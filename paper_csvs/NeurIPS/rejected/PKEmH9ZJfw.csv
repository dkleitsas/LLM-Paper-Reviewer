Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000819000819000819,"Learning causal representations from observational and interventional data in the
1"
ABSTRACT,0.001638001638001638,"absence of known ground-truth graph structures necessitates implicit latent causal
2"
ABSTRACT,0.002457002457002457,"representation learning. Implicit learning of causal mechanisms typically involves
3"
ABSTRACT,0.003276003276003276,"two categories of interventional data: hard and soft interventions. In real-world
4"
ABSTRACT,0.004095004095004095,"scenarios, soft interventions are often more realistic than hard interventions, as the
5"
ABSTRACT,0.004914004914004914,"latter require fully controlled environments. Unlike hard interventions, which di-
6"
ABSTRACT,0.005733005733005733,"rectly force changes in a causal variable, soft interventions exert influence indirectly
7"
ABSTRACT,0.006552006552006552,"by affecting the causal mechanism. However, the subtlety of soft interventions
8"
ABSTRACT,0.007371007371007371,"impose several challenges for learning causal models. One challenge is that soft
9"
ABSTRACT,0.00819000819000819,"intervention’s effects are ambiguous, since parental relations remain intact. In this
10"
ABSTRACT,0.009009009009009009,"paper, we tackle the challenges of learning causal models using soft interventions
11"
ABSTRACT,0.009828009828009828,"while retaining implicit modeling. Our approach models the effects of soft inter-
12"
ABSTRACT,0.010647010647010647,"ventions by employing a causal mechanism switch variable designed to toggle
13"
ABSTRACT,0.011466011466011465,"between different causal mechanisms. In our experiments, we consistently observe
14"
ABSTRACT,0.012285012285012284,"improved learning of identifiable, causal representations, compared to baseline
15"
ABSTRACT,0.013104013104013105,"approaches.
16"
INTRODUCTION,0.013923013923013924,"1
Introduction
17"
INTRODUCTION,0.014742014742014743,"Soft 
Intervention"
INTRODUCTION,0.015561015561015561,"Hard 
Intervention"
INTRODUCTION,0.01638001638001638,Observations
INTRODUCTION,0.0171990171990172,"Object’s class
Object’s color
Causal Graph"
INTRODUCTION,0.018018018018018018,"Fish
Bird
Bee
Cat
Spider
Bug
Classes"
INTRODUCTION,0.018837018837018837,"Figure 1: Difference between hard interventions
and soft interventions: As seen in the middle row,
hard interventions sever connections with parents.
Therefore, an object’s class cannot have any effect
on the object’s color when we intervene on color.
On the other hand, soft interventions, as shown in
the bottom row, allow for such effects."
INTRODUCTION,0.019656019656019656,"One of the long-standing challenges in causal
18"
INTRODUCTION,0.020475020475020474,"representation learning is how to recover the
19"
INTRODUCTION,0.021294021294021293,"ground-truth causal graph of a system solely
20"
INTRODUCTION,0.022113022113022112,"from observations. Termed the identifiability
21"
INTRODUCTION,0.02293202293202293,"of causal models problem, this endeavor is cru-
22"
INTRODUCTION,0.02375102375102375,"cial. Without achieving identifiability, we risk
23"
INTRODUCTION,0.02457002457002457,"erroneously attributing causal relationships to
24"
INTRODUCTION,0.025389025389025387,"learned representations. Furthermore, statisti-
25"
INTRODUCTION,0.02620802620802621,"cal models can masquerade as Directed Acyclic
26"
INTRODUCTION,0.02702702702702703,"Graphs (DAGs) where edges lack causal signif-
27"
INTRODUCTION,0.027846027846027847,"icance, further complicating our pursuit.
28"
INTRODUCTION,0.028665028665028666,"When considering the challenge of identifying
29"
INTRODUCTION,0.029484029484029485,"causal models, it is known that the Markov con-
30"
INTRODUCTION,0.030303030303030304,"dition in graphs is insufficient for this task [26].
31"
INTRODUCTION,0.031122031122031123,"Thus, without additional assumptions or data,
32"
INTRODUCTION,0.03194103194103194,"we find ourselves limited to learning only a
33"
INTRODUCTION,0.03276003276003276,"Markov Equivalence Class (MEC) of the causal
34"
INTRODUCTION,0.03357903357903358,"model.
Existing works have made different
35"
INTRODUCTION,0.0343980343980344,"assumptions about availability of ground-truth
36"
INTRODUCTION,0.03521703521703522,"causal variables labels [34], model parameters
37"
INTRODUCTION,0.036036036036036036,"[1], availability of paired interventional data [3, 31], and availability of intervention targets [17] to
38"
INTRODUCTION,0.036855036855036855,"ensure identifiability of causal models.
39"
INTRODUCTION,0.03767403767403767,"Interventional data are usually obtained through soft or hard interventions. Hard interventions
40"
INTRODUCTION,0.03849303849303849,"usually involve controlled experiments and they severe the connection of an intervened variable
41"
INTRODUCTION,0.03931203931203931,"with its parents [24]. In terms of Structural Causal Models (SCM), hard interventions set the causal
42"
INTRODUCTION,0.04013104013104013,"mechanism relating a causal variable to its parents, to a constant. Due to ethical or safety reasons, it
43"
INTRODUCTION,0.04095004095004095,"may not be possible to perform hard interventions in many real-world applications. On the other hand,
44"
INTRODUCTION,0.04176904176904177,"the effects of soft interventions are more subtle since parent variables can still affect their children.
45"
INTRODUCTION,0.042588042588042586,"These effects can be modeled by a change in the set of parents, the causal mechanisms, and the
46"
INTRODUCTION,0.043407043407043405,"exogenous variables [7]. Consequently, hard interventions can also be seen as a special case of soft
47"
INTRODUCTION,0.044226044226044224,"interventions where the causal mechanism is set to a constant. Illustrated in Figure 1, a prominent
48"
INTRODUCTION,0.04504504504504504,"challenge in causal representation learning lies in dealing with the ambiguity surrounding the effects
49"
INTRODUCTION,0.04586404586404586,"of soft interventions. The observed alterations in object colors fail to distinctly elucidate whether
50"
INTRODUCTION,0.04668304668304668,"they stem from parental influences or the applied interventions.
51"
INTRODUCTION,0.0475020475020475,"Additionally, a lack of comprehension regarding causal graphs can pose significant challenges in
52"
INTRODUCTION,0.04832104832104832,"causal representation learning. In certain applications, the causal graph can be constructed using
53"
INTRODUCTION,0.04914004914004914,"domain knowledge, allowing us to subsequently learn the causal variables [2, 18, 20]. However, this is
54"
INTRODUCTION,0.049959049959049956,"not universally applicable, necessitating the direct learning of the causal graph itself. In a Variational
55"
INTRODUCTION,0.050778050778050775,"AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning:
56"
INTRODUCTION,0.051597051597051594,"Explicit Latent Causal Models (ELCMs) [34, 1, 35, 37, 17, 15] and Implicit Latent Causal Models
57"
INTRODUCTION,0.05241605241605242,"(ILCMs) [3]. In ELCMs, the latents are the causal variables and the adjacency matrix of the causal
58"
INTRODUCTION,0.05323505323505324,"graph is parameterized and integrated into the prior of the latents such that the prior of latents is
59"
INTRODUCTION,0.05405405405405406,"factorized according to the Causal Markov Condition [27]. This approach to causal representation
60"
INTRODUCTION,0.054873054873054876,"learning is highly susceptible to becoming stuck in local minima as it is hard to learn representations
61"
INTRODUCTION,0.055692055692055695,"without knowing the graph, and it is hard to learn the graph without knowing the representations.
62"
INTRODUCTION,0.056511056511056514,"ILCMs [3] were introduced to circumvent this “chicken-and-egg” problem by using solution functions,
63"
INTRODUCTION,0.05733005733005733,"which can implicitly model edges in the causal graph rather than explicitly modeling the entire
64"
INTRODUCTION,0.05814905814905815,"adjacency matrix of the causal model. In ILCMs the latents are the exogenous variables and the there
65"
INTRODUCTION,0.05896805896805897,"is no explicit parameterization for the graph.
66"
INTRODUCTION,0.05978705978705979,"In implicit causal representation learning, the task involves recovering the exogenous variables E
67"
INTRODUCTION,0.06060606060606061,"from observed variables X and learning solution functions. In [3], interventions are assumed to
68"
INTRODUCTION,0.06142506142506143,"be hard, but this is often unrealistic and does not align with real-world problems. In this paper,
69"
INTRODUCTION,0.062244062244062245,"we propose a novel approach for Implicit Causal Representation Learning via Switchable
70"
INTRODUCTION,0.06306306306306306,"Mechanisms (ICRL-SM). We will introduce the causal mechanism switch variable as a way of
71"
INTRODUCTION,0.06388206388206388,"modeling the effect of soft interventions and identifying the causal variables. Our experiments on
72"
INTRODUCTION,0.0647010647010647,"both synthetic and large real-world datasets, highlight the efficacy of proposed method in identifying
73"
INTRODUCTION,0.06552006552006552,"causal variables and promising future directions in implicit causal representation learning. Our key
74"
INTRODUCTION,0.06633906633906633,"contributions can be summarized as follows:
75"
INTRODUCTION,0.06715806715806716,"I. A novel approach for implicit causal representation learning with soft interventions.
76"
INTRODUCTION,0.06797706797706797,"II. Employing causal mechanisms switch variable to model the effect of soft interventions.
77"
INTRODUCTION,0.0687960687960688,"III. Theory for identifiability up to reparameterization from soft interventions.
78 79"
RELATED WORK,0.06961506961506962,"2
Related Work
80"
RELATED WORK,0.07043407043407043,"Causal representation learning has recently garnered significant attention [27, 14]. The primary
81"
RELATED WORK,0.07125307125307126,"challenge in this problem lies in achieving identifiability beyond the Markov equivalence class [26].
82"
RELATED WORK,0.07207207207207207,"Solely relying on observational data necessitates additional assumptions regarding causal mechanisms,
83"
RELATED WORK,0.0728910728910729,"decoders, latent structure, and the availability of interventional data [22, 28, 36, 25, 15, 1, 40, 13,
84"
RELATED WORK,0.07371007371007371,"34]. Recent works have focused on identifying causal models from collected interventional data
85"
RELATED WORK,0.07452907452907453,"instead of making strong assumptions about functions of the causal model. Interventional data
86"
RELATED WORK,0.07534807534807535,"facilitates identifiability based on relatively weak assumptions [1, 6, 3, 39, 33]. This type of data
87"
RELATED WORK,0.07616707616707617,"can be further categorized based on whether it involves soft or hard interventions, and whether the
88"
RELATED WORK,0.07698607698607698,"manipulated variables are observed and specified or latent. Our focus in this paper is on examining
89"
RELATED WORK,0.07780507780507781,"soft interventions, encompassing both observed and unobserved variables.
90"
RELATED WORK,0.07862407862407862,"2.1
Explicit models vs. Implicit models
91"
RELATED WORK,0.07944307944307945,"Table 1 presents a comparison of the assumptions and identifiability results between our proposed
92"
RELATED WORK,0.08026208026208026,"theory and other related works on causal representation learning with interventions. In causal repre-
93"
RELATED WORK,0.08108108108108109,"sentation learning with interventions, one approach assumes a given causal graph and concentrates
94"
RELATED WORK,0.0819000819000819,"on identifying causal mechanisms and mixing functions. For instance, Causal Component Analysis
95"
RELATED WORK,0.08271908271908272,"(CauCA) [33] explores soft interventions with a known graph. Alternatively, when the graph is
96"
RELATED WORK,0.08353808353808354,"Table 1: Comparison of proposed method with other recent related work on causal learning from
interventional data"
RELATED WORK,0.08435708435708436,"Methods
Causal Mechanisms
Mixing functions
Interventions
Explicit/Implicit
Identifiability"
RELATED WORK,0.08517608517608517,"CausalDiscrepancy [38]
Nonlinear
Full row rank polynomial
Soft
Explicit
Permutation and Affine
CauCA [33]
Nonlinear
Diffeomorphism
Soft
Explicit
Different based on assumptions
Linear-CD [29]
Linear
Linear
Hard
Explicit
Permutation
Scale-I [30]
Nonlinear
Linear
Hard/Soft
Explicit
Scale/Mixed
ILCM [3]
Nonlinear
Diffeomorphism
Hard
Implicit
Permutation and reparameterization
dVAE [21]
Nonlinear
Diffeomorphism
Hard
Implicit
Permutation and reparameterization
ICRL-SM (ours)
Nonlinear
Diffeomorphism
Soft
Implicit
Reparameterization"
RELATED WORK,0.085995085995086,"not provided, explicit models seek to reconstruct it from interventional data [6, 17], potentially
97"
RELATED WORK,0.08681408681408681,"resulting in a chicken-and-egg problem in causal representation learning [3]. Current methods face
98"
RELATED WORK,0.08763308763308764,"the challenge of simultaneously learning the causal graph and other network parameters, especially
99"
RELATED WORK,0.08845208845208845,"in the absence of information about causal variables or the graph. Addressing these challenges, [3]
100"
RELATED WORK,0.08927108927108927,"recently introduced ILCM, which performs implicit causal representation learning exclusively using
101"
RELATED WORK,0.09009009009009009,"hard intervention data. In contrast, our approach introduces a novel method for learning an implicit
102"
RELATED WORK,0.09090909090909091,"model from soft interventions. [3] describes methods for extracting a causal graph from a learned
103"
RELATED WORK,0.09172809172809172,"implicit model, which could be applied to our method as well. In our experiments, we will compare
104"
RELATED WORK,0.09254709254709255,"our method with ILCM and dVAE [21], given their implicit nature and similar experimental settings
105"
RELATED WORK,0.09336609336609336,"and assumptions. Additionally, to showcase the superiority of our method over explicit models, we
106"
RELATED WORK,0.09418509418509419,"will employ explicit causal model discovery methods like ENCO [16] and DDS [5], in conjunction
107"
RELATED WORK,0.095004095004095,"with various variants of β-VAE.
108"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.09582309582309582,"2.2
Hard interventions vs Soft interventions
109"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.09664209664209664,"The identification of explicit causal models from hard interventions has been extensively ex-
110"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.09746109746109746,"plored. [29] investigate causal disentanglement in linear causal models with linear mixing functions
111"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.09828009828009827,"under hard interventions. Similarly, [4] focus on identifying causal models with linear causal mecha-
112"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.0990990990990991,"nisms and nonlinear mixing functions, also utilizing hard interventions. In a more general setting
113"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.09991809991809991,"with non-parametric causal mechanisms and mixing functions, [32] examine the identifiability of
114"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10073710073710074,"causal models, utilizing multi-environment data from unknown interventions. Similarly, [2] explore
115"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10155610155610155,"identifiability of causal models using multi-environment data from unknown interventions. [30]
116"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10237510237510238,"investigate the identifiability of causal models with nonlinear causal mechanisms and linear mixing
117"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10319410319410319,"functions, considering both hard and soft interventions.
118"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10401310401310401,"Recent work has expanded the concept of explicit hard interventions to include soft interventions. In
119"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10483210483210484,"their study, [38] address the identification of causal models from soft interventions, leveraging the
120"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10565110565110565,"sparsity of the adjacency matrix as an inductive bias. However, when dealing with implicit models,
121"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10647010647010648,"soft interventions introduce new complexities. Identifiability becomes more challenging, as the
122"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10728910728910729,"causal effect of variables on observed variables is less apparent. This ambiguity arises from the dual
123"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10810810810810811,"possibility of effects originating from interventions or influences from parent variables on the causal
124"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10892710892710893,"variables. Moreover, in scenarios where implicit modeling is retained, the absence of knowledge about
125"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.10974610974610975,"parent variables further complicates identifiability. While [3] theoretically establishes identifiability
126"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.11056511056511056,"for hard interventions, practical experiments involving complex causal models with over 10 variables
127"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.11138411138411139,"reveal increased ambiguity and confounding factors. Consequently, model identification becomes
128"
HARD INTERVENTIONS VS SOFT INTERVENTIONS,0.1122031122031122,"less straightforward.
129"
METHODOLOGY,0.11302211302211303,"3
Methodology
130"
DATA GENERATING PROCESS,0.11384111384111384,"3.1
Data Generating Process
131"
DATA GENERATING PROCESS,0.11466011466011466,"A structural causal model (Definition A1.1) is used to understand and describe the relationships
132"
DATA GENERATING PROCESS,0.11547911547911548,"between different variables and how they influence each other through causal mechanisms. A decoder
133"
DATA GENERATING PROCESS,0.1162981162981163,"function, g(z) = x, maps a vector of causal values z to observed values x. The causal variables
134"
DATA GENERATING PROCESS,0.11711711711711711,"Z are unobserved and the goal is to infer them from interventional data. For each causal variable,
135"
DATA GENERATING PROCESS,0.11793611793611794,"a diffeomorphic solution function, si : Ei →Zi, deterministically maps a value for exogenous
136"
DATA GENERATING PROCESS,0.11875511875511875,"variable Ei to a value for causal variable Zi. In implicit modeling, we learn the solution functions si
137"
DATA GENERATING PROCESS,0.11957411957411958,"directly, rather than defining them through local mechanisms fi. We write S for the set of all solution
138"
DATA GENERATING PROCESS,0.12039312039312039,"functions si ∈S, so S : E →Z.
139"
DATA GENERATING PROCESS,0.12121212121212122,"Identifying causal models from data can be complex and is often studied within classes of models
140"
DATA GENERATING PROCESS,0.12203112203112203,"such as those identifiable up to affine transformations. For example, in the context of nonlinear
141"
DATA GENERATING PROCESS,0.12285012285012285,"Independent Component Analysis (ICA), the generative process also involves a mixture function g of
142"
DATA GENERATING PROCESS,0.12366912366912367,"latent causal variables Z ∈Rn, resulting in observations X ∈Rn [15, 41]. However, a significant
143"
DATA GENERATING PROCESS,0.12448812448812449,"distinction between causal representation learning and nonlinear-ICA is that in the former, the causal
144"
DATA GENERATING PROCESS,0.12530712530712532,"variables Z may have complex dependencies. Our objective in this paper is to recover E from X and
145"
DATA GENERATING PROCESS,0.12612612612612611,"eventually map E to Z using solution functions.
146"
DATA GENERATING PROCESS,0.12694512694512694,"Identifying a causal model from observational data is not trivial and requires assumptions on the
147"
DATA GENERATING PROCESS,0.12776412776412777,"parameters of the model [1]. Adding information about interventions in addition to observations,
148"
DATA GENERATING PROCESS,0.1285831285831286,"helps to identify causal variables by exhibiting the effect of changing a causal variable on the observed
149"
DATA GENERATING PROCESS,0.1294021294021294,"variables. An interventional data point (x, ˜x, i) includes the pre-intervention observation x, the post-
150"
DATA GENERATING PROCESS,0.13022113022113022,"intervention observation ˜x, and intervention target i ∈I where I is the set of intervention targets
151"
DATA GENERATING PROCESS,0.13104013104013104,"selected from the causal variables. The post-intervention data ˜x is generated by a soft intervention
152"
DATA GENERATING PROCESS,0.13185913185913187,"that targets one of the causal variables in Z. To achieve identifiability up to reparametrization, we
153"
DATA GENERATING PROCESS,0.13267813267813267,"rely on a series of assumptions within the data generation process, outlined as follows:
154"
DATA GENERATING PROCESS,0.1334971334971335,"Assumption 3.1. (Data generating assumptions)
155"
DATA GENERATING PROCESS,0.13431613431613432,"1. Atomic Interventions: For every sample (x, ˜x, i), only one causal variable is targeted by an
156"
DATA GENERATING PROCESS,0.13513513513513514,"intervention.
157"
DATA GENERATING PROCESS,0.13595413595413594,"2. Known Targets: Targets of soft interventions are known.
158"
DATA GENERATING PROCESS,0.13677313677313677,"3. Post-intervention Exogenous Variables: The exogenous variables’ values change only for the
159"
DATA GENERATING PROCESS,0.1375921375921376,"corresponding intervened causal variable, while the others maintain their pre-intervention values,
160"
DATA GENERATING PROCESS,0.13841113841113842,"thus ei ̸= ˜ei if i ∈I ,and ei = ˜ei otherwise.
161"
DATA GENERATING PROCESS,0.13923013923013924,"4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability
162"
DATA GENERATING PROCESS,0.14004914004914004,"[15]. These interventions should modify causal mechanisms to ensure non-overlapping conditional
163"
DATA GENERATING PROCESS,0.14086814086814087,"distributions of causal variables (refer to Figure A1).
164"
DATA GENERATING PROCESS,0.1416871416871417,"5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss
165"
DATA GENERATING PROCESS,0.14250614250614252,"and avoids abrupt changes in the function’s image.
166"
DATA GENERATING PROCESS,0.14332514332514332,"The known targets assumption can be relaxed in applications where such data is not available
167"
DATA GENERATING PROCESS,0.14414414414414414,"and the same procedure in [3] can be used to infer the intervention targets. In fact, in our real-
168"
DATA GENERATING PROCESS,0.14496314496314497,"world experiments, intervention targets are not available and based on the nature of the datasets, we
169"
DATA GENERATING PROCESS,0.1457821457821458,"hypothesize our causal variables to be object attributes and actions to be intervention targets.
170"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1466011466011466,"3.2
Causal Mechanisms Switch Variable
171"
CAUSAL MECHANISMS SWITCH VARIABLE,0.14742014742014742,"The major difference of soft intervention with hard intervention is that post-intervention causal
172"
CAUSAL MECHANISMS SWITCH VARIABLE,0.14823914823914824,"variable ˜Zi is no longer disconnected from its parents and its causal mechanism ˜si is affected by the
173"
CAUSAL MECHANISMS SWITCH VARIABLE,0.14905814905814907,"intervention. This is why identifying the causal mechanisms is more difficult for soft interventions.
174"
CAUSAL MECHANISMS SWITCH VARIABLE,0.14987714987714987,"Soft intervention data yield fewer constraints on the causal graph structure than hard intervention
175"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1506961506961507,"data. For more details refer to string diagrams of soft and hard interventions depicted in Figure A5.
176"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15151515151515152,"Figure 2b shows our main generative model. It includes a data augmentation step that adds the
177"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15233415233415235,"intervention displacement ˜x −x as an observed feature that directly represents the effect of a soft
178"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15315315315315314,"intervention in observation space.
179"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15397215397215397,"Augmented implicit causal model To model the effect of soft interventions, we introduce the
180"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1547911547911548,"causal mechanism switch variable V [26]. By leveraging V, we can effectively switch to the pre-
181"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15561015561015562,"intervention causal mechanisms within post-intervention data. This facilitates the model’s ability to
182"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15642915642915642,"solely focus on discerning alterations in the intrinsic characteristics of each causal variable. These
183"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15724815724815724,"changes are encapsulated within their respective exogenous variables, aiding the model in learning
184"
CAUSAL MECHANISMS SWITCH VARIABLE,0.15806715806715807,"the causal relationships more accurately. We propose to use a modulated form of V to model the
185"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1588861588861589,"soft intervention effects on each causal variable as an additive effect with a nonlinear function hi
186"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1597051597051597,"such that ∀i, ˜Zi = ˜si( ˜Ei; ˜E/i) = si( ˜Ei; E/i, hi(V)). As the parental set for each causal variable is
187"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16052416052416052,"not known, we have to use a modulated form of V in every causal variable’s solution function and
188"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16134316134316135,"the inclusion of hi(V) enables the model to encompass variations in the parental sets of all causal
189"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16216216216216217,"variables in V. Therefore, there is a switch variable Vi for each causal variable Zi. Adding switch
190"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16298116298116297,"variables to solution functions leads to the concept of an augmented implicit causal model.
191"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1638001638001638,"Definition 3.2. (Augmented Implicit Causal Models) An Augmented Implicit Causal Models (AICMs)
192"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16461916461916462,"is defined as A = (S, Z, E, V) where V ∈Rn is the causal mechanism switch variable which models
193"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16543816543816545,"the effect of soft interventions on solution functions S:
194"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16625716625716624,"∀i, ˜Zi = ˜si( ˜Ei; ˜E/i) = si( ˜Ei; E/i, hi(V)),
(1)"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16707616707616707,"where ˜si is the new solution function resulting from the soft intervention, ˜E/i is the altered set of all
195"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1678951678951679,"exogenous variables except i, including the ancestral exogenous variables, due to intervention, and
196"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16871416871416872,"˜Ei is the post-intervention exogenous variable.
197"
CAUSAL MECHANISMS SWITCH VARIABLE,0.16953316953316952,"The usage of V in soft interventions is analogous to augmented networks in [23] which were mainly
198"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17035217035217035,"designed for hard interventions. Pearl [23] even foresaw this possibility by saying: ""One advantage
199"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17117117117117117,"of the augmented network representation is that it is applicable to any change in the functional
200"
CAUSAL MECHANISMS SWITCH VARIABLE,0.171990171990172,"relationship fi and not merely to the replacement of fi by a constant.""
201"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17280917280917282,"By using Taylor’s expansion, we can expand the solution functions as follows:
202"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17362817362817362,"si( ˜Ei; E/i, hi(V)) = si( ˜Ei; E/i, hi(v0)) + P∞
n=1
1
n!  ∂nsi ∂hn
i"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17444717444717445,"hi=hi(v0)
(hi(V) −hi(v0))n
!"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17526617526617527,"= si( ˜Ei; E/i, hi(v0)) + Ri (2)"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1760851760851761,"where we’ll use Ri as a short-hand for Equation 2. We define the separable dependence property
203"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1769041769041769,"for solution functions as ∃hi(v0) : si( ˜Ei; E/i, hi(v0)) = si( ˜Ei; E/i). An example of such a scenario
204"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17772317772317772,"could be in location-scale noise models such as, si(˜ei; e/i, hi(v)) = ˜ei + loc(e/i) + hi(v) =
205"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17854217854217855,"˜ei + loc(e/i) + v2 + v where v0 would be zero . By assuming the separable dependence property,
206"
CAUSAL MECHANISMS SWITCH VARIABLE,0.17936117936117937,"we can write the solution function in Equation 2 as:
207"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18018018018018017,"si( ˜Ei; E/i, hi(V)) = si( ˜Ei; E/i) + Ri = si( ˜Ei; E/i) + soft intervention effect
(3)"
CAUSAL MECHANISMS SWITCH VARIABLE,0.180999180999181,"As a result, we can switch to pre-intervention solution functions. Subsequently, by modeling soft
208"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18181818181818182,"intervention effects using hi(V), we can recover pre-intervention solution functions. During inference,
209"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18263718263718265,"we simply disregard the hi(V) term in the solution functions. Nonetheless, it is possible to train the
210"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18345618345618345,"prior p(V) to ensure that the separable dependence property is maintained for pre-intervention data.
211"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18427518427518427,"Observability of switch variable The intuition behind using V is to separate the effect of soft
212"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1850941850941851,"intervention on ˜Zi into two: (1) The effect on causal mechanisms and parents, and (2) The effect on
213"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18591318591318592,"exogenous variable Ei. For example, we can say that causal variables in images of objects are the
214"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18673218673218672,"objects’ attributes such as shape, color, and size, and performing actions like ""Fold"" change these
215"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18755118755118755,"attributes. Furthermore, it can be asserted that the camera angle within a given image may influence
216"
CAUSAL MECHANISMS SWITCH VARIABLE,0.18837018837018837,"the shape of the object. If the images were generated from a hard intervention, the camera angle
217"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1891891891891892,"remains fixed between pre and post intervention. However, the camera angle changes along with
218"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19000819000819,"the performed actions indicating that the interventions are soft. In this case, if we had a knowledge
219"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19082719082719082,"of how the camera angle affects the attributes of objects, then we could separate the effect of soft
220"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19164619164619165,"intervention. In other words, if V is observed, then we can extract the effect of the intervention that
221"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19246519246519248,"we are interested in (i.e., the effect on the causal variable itself). For more details, refer to Figure A4.
222"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19328419328419327,"Lacking an understanding of how soft intervention influences the causal model, a more complex
223"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1941031941031941,"model becomes necessary. Consequently, the term Ri in Equation 2 would involve a higher order of
224"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19492219492219492,"hi(V). Therefore, we assume the observability of V:
225"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19574119574119575,"Assumption 3.3. (Observability of V) Given an intervention sample (x, ˜x, i) and linear decoders,
226"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19656019656019655,"we can approximate the soft intervention effects hi(V) as follows:
227"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19737919737919737,"˜z −z = ∆ei + R
(using Equation 2),
˜x −x = g(˜z) −g(z) ≈g(˜z −z) = g(∆ei + R),"
CAUSAL MECHANISMS SWITCH VARIABLE,0.1981981981981982,"where R = [R0, R1, ..., Rn] and n is the number of causal variables. R and ∆ei are the vectors
228"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19901719901719903,"indicating the soft intervention effects and change in effect of the exogenous variable of the intervened
229"
CAUSAL MECHANISMS SWITCH VARIABLE,0.19983619983619982,"causal variable, respectively. Note that elements of R will be all zero except for the intervened causal
230"
CAUSAL MECHANISMS SWITCH VARIABLE,0.20065520065520065,"variable. Consequently, with linear mixing functions and some pre-processing on observed samples
231"
CAUSAL MECHANISMS SWITCH VARIABLE,0.20147420147420148,"(here subtraction), we can observe Ri.
232"
CAUSAL MECHANISMS SWITCH VARIABLE,0.2022932022932023,"Our synthetic data is generated using a linear decoder, however, the decoder for the real-world
233"
CAUSAL MECHANISMS SWITCH VARIABLE,0.2031122031122031,"datasets is not necessarily linear. Therefore, we do not observe V from ˜x −x in the real-world dataset.
234"
CAUSAL MECHANISMS SWITCH VARIABLE,0.20393120393120392,"Nevertheless, our findings suggest that incorporating soft interventions through V leads to superior
235"
CAUSAL MECHANISMS SWITCH VARIABLE,0.20475020475020475,"performance compared to other implicit modeling approaches. Clearly, understanding the impact of
236"
CAUSAL MECHANISMS SWITCH VARIABLE,0.20556920556920558,"soft interventions on the generative system of the dataset would result in improved outcomes.
237"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.20638820638820637,"3.3
Identifiability Theorem for Implicit SCMs with Soft Interventions
238"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2072072072072072,"In this paper, our focus lies in identifying the causal variables up to reparameterization through soft
239"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.20802620802620803,"interventions. We first define identifiability up to reparameterization (Definition 3.4) and subsequently
240"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.20884520884520885,"introduce the identifiability theorem 3.5. The proof of theorem is extensive and is available in full in
241"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.20966420966420968,"Appendix A1.
242"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21048321048321048,"We establish identifiability up to reparameterization, allowing for the mapping of causal variables Z
243"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2113022113022113,"and Z′ between two Latent Causal Models (M and M′) through component-wise transformations
244"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21212121212121213,"M
V
V = ""! ⋮
"""""
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21294021294021295,Encoder 𝑿
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21375921375921375,"Pre
Intervention"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21457821457821458,"Post
Intervention FC FC"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2153972153972154,"Causal 
Mechanism"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21621621621621623,Switch
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21703521703521703,Location Scale
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21785421785421785,Solution Function (S)
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.21867321867321868,"Encoder 𝑿"" −𝑿"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2194922194922195,"𝑋""  −𝑋"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2203112203112203,"M
V
! = !! ⋮
!"""
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22113022113022113,"M
V
!̃ = !̃! ⋮
!̃"""
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22194922194922195,Decoder
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22276822276822278,"Decoder 𝒁"" 𝑋 𝑋"" 𝑿"""
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22358722358722358,(a) General overview of ICRL-SM 𝑒
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2244062244062244,"𝑥#
𝑥"" −𝑥
𝑥"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22522522522522523,"𝑒̃
𝑣
𝑧
𝑧̃"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22604422604422605,(b) Generative model
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22686322686322685,"(Definition A1.2). Given our implicit modeling approach, lacking knowledge of the causal graph, we
245"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22768222768222768,"include all exogenous variables in the solution functions, as depicted in Equation 1. Notably, the
246"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2285012285012285,"causal graph remains unaltered during learning. To illustrate, we contrast hard interventions,
247"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.22932022932022933,"which neglect parent influences, with soft interventions that acknowledge parental effects in a simple
248"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23013923013923013,"example. Consider a basic causal model Z1 →Z2 alongside a location-scale noise model [12] for the
249"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23095823095823095,"solution function, given by ˜z2 = ˜e2−f
loc(e1)
]
scale(e1) . The distribution p( ˜Z2) mean is
1
]
scale(e1) × mean( ˜E2) −
250"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23177723177723178,"f
loc(e1)
]
scale(e1) In the context of hard interventions, we can assume p( ˜Z2|Z1) = p( ˜Z2) = N(0, 1) as there
251"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2325962325962326,"are no parental effects. Consequently, the location and scale networks within the solution function tend
252"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2334152334152334,"to dampen parental effects, given the absence of parental influence in the ground-truth data. Contrarily,
253"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23423423423423423,"soft interventions exhibit parental influence in the ground-truth data, thus p( ˜Z2|Z1) ̸= N(0, 1). Due
254"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23505323505323505,"to the lack of parental knowledge in implicit modeling, we model p( ˜Z2|Z1) = p( ˜Z2|E2), as E2
255"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23587223587223588,"is a known parent of ˜Z2. Consequently, parental effects are propagated to Ei (the corresponding
256"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23669123669123668,"exogenous variable of each causal variable), violating identifiability up to reparameterization. By
257"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2375102375102375,"leveraging V, we allow parental effects to propagate to V instead of Ei.
258"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23832923832923833,"Definition 3.4. (Equivalence up to component-wise reparameterization) Let M = (A, X, g, I)
259"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23914823914823916,"and M′ = (A′, X, g′, I) be two Latent Causal Models (LCM) based on AICMs A, A′ with shared
260"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.23996723996723995,"observation space X, shared intervention targets I, and respective decoders g and g′. We say that
261"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.24078624078624078,"M and M′ are equivalent up to component-wise reparameterization M ∼r M′ if there exists a
262"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.2416052416052416,"component-wise transformation (Definition A1.2) ϕZ from the causal variables Z to the causal
263"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.24242424242424243,"variables Z′ and a component-wise transformation ϕE between E and E′ such that:
264"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.24324324324324326,"1. Indices are preserved (i.e., ϕi(zi) = z′
i and ϕi(ei) = e′
i). Corresponding edges are preserved (i.e.,
265"
IDENTIFIABILITY THEOREM FOR IMPLICIT SCMS WITH SOFT INTERVENTIONS,0.24406224406224405,"Zi →Zj holds in G iff Z′
i →Z′
j holds in G′. Edges Ei →Zi should be preserved as well.)
266"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.24488124488124488,"2.
The exogenous transformation preserves the probability measure on exogenous variables
267"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.2457002457002457,"pE′ = (ϕE)∗pE (Definition A1.4).
268"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.24651924651924653,"3. The causal transformation preserves the probability measure on causal variables pZ′ = (ϕZ)∗pZ
269"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.24733824733824733,"(Definition A1.4).
270 271"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.24815724815724816,"Theorem 3.5. (Identifiability of latent causal models.)
Let M = (A, X, g, I) and M′ =
272"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.24897624897624898,"(A′, X, g′, I) be two LCMs with shared observation space X and shared intervention targets I.
273"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.2497952497952498,"Suppose the following conditions are satisfied:
274"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25061425061425063,"1. Data generating assumptions explained in Assumption 3.1.
275"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25143325143325146,"2. Soft interventions satisfy Assumption 3.3.
276"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25225225225225223,"3. The causal and exogenous variables are real-valued.
277"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25307125307125306,"4. The causal and exogenous variables follow a multivariate normal distribution.
278"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.2538902538902539,"Then the following statements are equivalent:
279"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.2547092547092547,"-Two LCMs M and M′ assign the same likelihood to interventional and observational data i.e.,
280"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25552825552825553,"pX,I
M (x, ˜x, i) = pX,I
M′ (x, ˜x, i).
281"
THE EXOGENOUS TRANSFORMATION PRESERVES THE PROBABILITY MEASURE ON EXOGENOUS VARIABLES,0.25634725634725636,"- M and M′ are disentangled, that is M ∼r M′ according to Definition 3.4.
282"
TRAINING OBJECTIVE,0.2571662571662572,"3.4
Training Objective
283"
TRAINING OBJECTIVE,0.257985257985258,"Consequently, there will be three latent variables in ICRL-SM:
284"
TRAINING OBJECTIVE,0.2588042588042588,"1. A causal mechanism switch variable V.
285"
TRAINING OBJECTIVE,0.2596232596232596,"2. The pre-intervention exogenous variables E.
286"
TRAINING OBJECTIVE,0.26044226044226043,"3. The post-intervention exogenous variables ˜E.
287"
TRAINING OBJECTIVE,0.26126126126126126,"As the data log-likelihood log p(x, ˜x, x −˜x) ≡log p(x, ˜x) is intractable, we utilize an ELBO
288"
TRAINING OBJECTIVE,0.2620802620802621,"approximation as training objective:
289"
TRAINING OBJECTIVE,0.2628992628992629,"log p(x, ˜x) ≥Eq(e,˜e,v|x,˜x)
h
log p(x, ˜x|e, ˜e, v)
i
−KLD(q(e, ˜e, v|x, ˜x)||p(e, ˜e, x))"
TRAINING OBJECTIVE,0.26371826371826373,"= Eq(v|˜x−x)·q(e|x)·q(˜e|˜x)
h
log(p(x|e)p(˜x|˜e)p(˜x −x|v))
i
−KLD(q(v|˜x −x) · q(e|x) · q(˜e|˜x)||p(˜e|e, v)p(v)p(e))."
TRAINING OBJECTIVE,0.26453726453726456,"(4)
The observations are encoded and decoded independently. The KLD term regularizes the encodings
290"
TRAINING OBJECTIVE,0.26535626535626533,"to share the latent intervention model p(˜e|e, v)p(v)p(e) that is shared across all data points. The
291"
TRAINING OBJECTIVE,0.26617526617526616,"components of this model can be interpreted as follows:
292"
TRAINING OBJECTIVE,0.266994266994267,"1. p(e) is the prior distribution over exogenous variables e.
293"
TRAINING OBJECTIVE,0.2678132678132678,"2. p(v) is the prior distribution over switch variables v.
294"
TRAINING OBJECTIVE,0.26863226863226863,"3. p(˜e|e, v) is a transition model that shows how the exogeneous variables change as a function of the
295"
TRAINING OBJECTIVE,0.26945126945126946,"intervention.
296"
TRAINING OBJECTIVE,0.2702702702702703,"We factorize the posterior with a mean-field approximation q(v, e, ˜e|x, ˜x) = q(v|˜x −x) · q(e|x) ·
297"
TRAINING OBJECTIVE,0.2710892710892711,"q(˜e|˜x) and, following our data generation model (Figure 2b), the reconstruction probability
298"
TRAINING OBJECTIVE,0.2719082719082719,"as p(x, ˜x|e, ˜e, v) = p(x|e)p(˜x|˜e)p(˜x −x|v).
The prior over latent variables is factorized as
299"
TRAINING OBJECTIVE,0.2727272727272727,"p(˜e, e, v) = p(˜e|e, v)p(v)p(e)(Figure 2b). Pre-intervention exogenous variables are mutually inde-
300"
TRAINING OBJECTIVE,0.27354627354627353,"pendent, hence, p(e) = Πip(ei) and p(v) = Πip(vi). We assume p(ei) and p(vi) to be standard
301"
TRAINING OBJECTIVE,0.27436527436527436,"Gaussian. Furthermore, as we assume ei = ˜ei for all non-intervened variables, the p(˜e|e, v) will be
302"
TRAINING OBJECTIVE,0.2751842751842752,"as follows:
303"
TRAINING OBJECTIVE,0.276003276003276,"p(˜e|e, v) = Πi/∈Iδ(˜ei −ei)Πi∈Ip(˜ei|e, v) = Πi/∈Iδ(˜ei −ei)Πi∈Ip(˜zi|ei)

∂˜zi
∂˜ei (5)"
TRAINING OBJECTIVE,0.27682227682227684,"The last equality is obtained from the Change of Variable Rule in probability theory, applied to the
304"
TRAINING OBJECTIVE,0.27764127764127766,"solution function ˜zi = si(˜ei; e/i, hi(v)). Furthermore, we write p(˜zi|e, v) = p(˜zi|ei) since only ei
305"
TRAINING OBJECTIVE,0.2784602784602785,"is a known parent of ˜zi in implicit modeling. We assume p(˜zi|ei) to be a Gaussian whose mean is
306"
TRAINING OBJECTIVE,0.27927927927927926,"determined by ei. We implement the solution function using a location-scale noise models [12] as
307"
TRAINING OBJECTIVE,0.2800982800982801,"also practiced in [3], which defines an invertible diffeomorphism. For simplicity, in our experiments,
308"
TRAINING OBJECTIVE,0.2809172809172809,"we are only going to change the loc network in post-intervention. Therefore, hi(v) will be used as:
309"
TRAINING OBJECTIVE,0.28173628173628174,"˜zi = ˜si(˜ei; e/i, hi(v)) = ˜ei −(loci(e/i) + hi(v))"
TRAINING OBJECTIVE,0.28255528255528256,"scalei(e/i)
,
(6)"
TRAINING OBJECTIVE,0.2833742833742834,"where loci : Rn−1 →R and scalei : Rn−1 →R are fully connected networks calculating the first
310"
TRAINING OBJECTIVE,0.2841932841932842,"and second moments, respectively. The general overview of the model is illustrated in Figure 2a.
311"
EXPERIMENTS AND RESULTS,0.28501228501228504,"4
Experiments and Results
312"
EXPERIMENTS AND RESULTS,0.2858312858312858,"The experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement
313"
EXPERIMENTS AND RESULTS,0.28665028665028663,"to identify the true causal graph from pairs of observations (x, ˜x, i), and (2) Action Inference to make
314"
EXPERIMENTS AND RESULTS,0.28746928746928746,"supervised inferences about actions generated from the post-intervention samples using information
315"
EXPERIMENTS AND RESULTS,0.2882882882882883,"about the values of the manipulated causal variables. Moreover, we conducted additional experiments
316"
EXPERIMENTS AND RESULTS,0.2891072891072891,"designed as an ablation study, the results of which are presented in A4. All models are trained using
317"
EXPERIMENTS AND RESULTS,0.28992628992628994,"the same setting and data with known intervention targets.
318"
DATASETS,0.29074529074529076,"4.1
Datasets
319"
DATASETS,0.2915642915642916,"Synthetic Dataset We generate simple synthetic datasets with X = Z = Rn. For each value of
320"
DATASETS,0.29238329238329236,"n, we generate ten random DAGs, a random location-scale SCM, then a random dataset from the
321"
DATASETS,0.2932022932022932,"parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order
322"
DATASETS,0.294021294021294,"from a Bernoulli distribution with probability 0.5. The pre-intervention and post-intervention causal
323"
DATASETS,0.29484029484029484,"variables are obtained as:
324"
DATASETS,0.29565929565929566,"zi = scale(zpai)ei + loc(zpai)
Soft-Intervention
−−−−−−−−−→˜zi = scale(zpai)˜ei + f
loc(zpai),
(7)"
DATASETS,0.2964782964782965,"where the loc and scale networks are changed in post intervention. The pre-intervention loc and
325"
DATASETS,0.2972972972972973,"post-intervention f
loc network weights are initialized with samples drawn from N(0, 1) and N(3, 1),
326"
DATASETS,0.29811629811629814,"respectively. The scale is constant 1 for both pre-intervention and post-intervention samples. Both
327"
DATASETS,0.2989352989352989,"ei and ˜ei are sampled from a standard Gaussian. The causal variables are mapped to the data space
328"
DATASETS,0.29975429975429974,"through a randomly sampled SO(n) rotation. For each dataset, we generate 100,000 training samples,
329"
DATASETS,0.30057330057330056,"10,000 validation samples, and 10,000 test samples.
330"
DATASETS,0.3013923013923014,"Action Datasets Causal-Triplet datasets tailored for actionable counterfactuals [19] feature paired
331"
DATASETS,0.3022113022113022,"images where several global scene properties may vary including camera view and object occlusions.
332"
DATASETS,0.30303030303030304,"Thus, the images can be viewed as outcomes of soft interventions, wherein actions affect objects
333"
DATASETS,0.30384930384930386,"alongside subtle alterations. These datasets [19] consist of: images obtained from a photo-realistic
334"
DATASETS,0.3046683046683047,"simulator of embodied agents, ProcTHOR [9], and the other contains images repurposed from a real-
335"
DATASETS,0.30548730548730546,"world video dataset of human-object interactions [8]. The former one contains 100 k images in which
336"
DATASETS,0.3063063063063063,"7 types of actions manipulate 24 types of objects in 10 k distinct ProcTHOR indoor environments.
337"
DATASETS,0.3071253071253071,"The latter consists of 2,632 image pairs, collected under a similar setup from the Epic-Kitchens
338"
DATASETS,0.30794430794430794,"dataset with 97 actions manipulating 277 objects.Based on the nature of actions in this dataset, the
339"
DATASETS,0.30876330876330876,"causal variables should represent attributes of objects such as shape and color. As the dataset consists
340"
DATASETS,0.3095823095823096,"of images we train all the methods with ResNet encoder and decoder. For the ProcThor dataset the
341"
DATASETS,0.3104013104013104,"number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from
342"
DATASETS,0.31122031122031124,"the dataset as 97 causal variables will be too complex in a VAE setup.
343"
METRICS,0.31203931203931207,"4.2
Metrics
344"
METRICS,0.31285831285831284,"For the causal disentanglement task, we are going to use the DCI scores [10]. Causal disentanglement
345"
METRICS,0.31367731367731366,"score quantifies the degree to which Zi factorises or disentangles the Z∗. Causal disentanglement Di
346"
METRICS,0.3144963144963145,"for Zi is calculated as Di = (1 −HK(Pi.)) = (1 + PK−1
k=0 Pik logK Pik) where Pij =
Rij
PK−1
k=0 Rik
347"
METRICS,0.3153153153153153,"and Rij denotes the probability of Zi being important for predicting Z∗
j . Total causal disentanglement
348"
METRICS,0.31613431613431614,is the weighted average P
METRICS,0.31695331695331697,i ρiDi where ρi = P
METRICS,0.3177723177723178,"j Rij
P"
METRICS,0.3185913185913186,"ij Rij . Causal Completeness quantifies the degree
349"
METRICS,0.3194103194103194,"to which each Z∗
i is captured by a single Zi. Causal completeness is calculated as Cj = (1 −
350"
METRICS,0.3202293202293202,"HD( ˜P.j)) = (1 + PD−1
d=0 ˜Pdj logD ˜Pij). D and K here are equal to the dimension of Z∗and Z
351"
METRICS,0.32104832104832104,"which is n. For the action inference task, we will use classification accuracy as a metric. As we
352"
METRICS,0.32186732186732187,"assume intervention targets are known, we train all models using known intervention targets for a fair
353"
METRICS,0.3226863226863227,"comparison.
354"
RESULTS,0.3235053235053235,"5
Results
355"
CAUSAL DISENTANGLEMENT,0.32432432432432434,"5.1
Causal Disentanglement
356"
CAUSAL DISENTANGLEMENT,0.32514332514332517,"We generated a dataset for the soft interventions and trained the models of ICRL-SM, ILCM, β-VAE
357"
CAUSAL DISENTANGLEMENT,0.32596232596232594,"and D-VAE for 10 different seeds, which generated 10 different causal graphs. We selected 4 causal
358"
CAUSAL DISENTANGLEMENT,0.32678132678132676,"variables to encompass complex causal structures, including forks, chains, and colliders. Table 2
359"
CAUSAL DISENTANGLEMENT,0.3276003276003276,"displays the Causal Disentanglement and Causal Completeness scores for all models, computed on
360"
CAUSAL DISENTANGLEMENT,0.3284193284193284,"the test data.
361"
CAUSAL DISENTANGLEMENT,0.32923832923832924,Table 2: Comparison of identifiability results
CAUSAL DISENTANGLEMENT,0.33005733005733007,"Graph
Causal Disentanglement
Causal Completeness"
CAUSAL DISENTANGLEMENT,0.3308763308763309,"Model
Name
β-VAE
d-VAE
ILCM
ICRL-SM
β-VAE
d-VAE
ILCM
ICRL-SM"
CAUSAL DISENTANGLEMENT,0.3316953316953317,"G1
0.38
0.54
0.71
0.82
0.51
0.69
0.78
0.87"
CAUSAL DISENTANGLEMENT,0.3325143325143325,"G2
0.30
0.72
0.75
0.83
0.49
0.77
0.80
0.87"
CAUSAL DISENTANGLEMENT,0.3333333333333333,"G3
0.28
0.51
0.68
0.98
0.49
0.56
0.78
0.98"
CAUSAL DISENTANGLEMENT,0.33415233415233414,"G4
0.16
0.50
0.65
0.68
0.38
0.69
0.77
0.78"
CAUSAL DISENTANGLEMENT,0.33497133497133497,"G5
0.27
0.44
0.53
0.42
0.45
0.54
0.66
0.50"
CAUSAL DISENTANGLEMENT,0.3357903357903358,"G6
0.52
0.62
0.71
0.98
0.66
0.69
0.86
0.98"
CAUSAL DISENTANGLEMENT,0.3366093366093366,"G7
0.39
0.49
0.71
0.75
0.70
0.73
0.89
0.89"
CAUSAL DISENTANGLEMENT,0.33742833742833744,"G8
0.47
0.54
0.50
0.59
0.6
0.63
0.62
0.68"
CAUSAL DISENTANGLEMENT,0.33824733824733827,"G9
0.30
0.68
0.83
0.85
0.40
0.76
0.86
0.87"
CAUSAL DISENTANGLEMENT,0.33906633906633904,"G10
0.39
0.39
0.52
0.32
0.53
0.56
0.82
0.70"
CAUSAL DISENTANGLEMENT,0.33988533988533987,"The results in Table 2 indicate that our method ICRL-SM can identify the true causal graph in most
362"
CAUSAL DISENTANGLEMENT,0.3407043407043407,"cases. The worst results are seen for graphs G5 and G10. As mentioned in [27, 25], causal graphs are
363"
CAUSAL DISENTANGLEMENT,0.3415233415233415,"sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify
364"
CAUSAL DISENTANGLEMENT,0.34234234234234234,"the causal variables well. Furthermore, in the next experiment we are going to examine the factors
365"
CAUSAL DISENTANGLEMENT,0.34316134316134317,"affecting causal disentanglement such as the number of edges in the graph and the intensity of soft
366"
CAUSAL DISENTANGLEMENT,0.343980343980344,"intervention effect. These findings can explain why ICRL-SM cannot identify causal variables in
367"
CAUSAL DISENTANGLEMENT,0.3447993447993448,"G10 despite its sparsity.
368"
CAUSAL DISENTANGLEMENT,0.34561834561834565,"Table 3: Table comparing action and object accuracy across various methods on Causal-Triplet
datasets under different settings. Z and zi show whether all causal variables (Z), or only the
intervened casual variable (zi) are used for the prediction task. R64 denote images with resolutions
64 × 64."
CAUSAL DISENTANGLEMENT,0.3464373464373464,"Epic-Kitchens
ProcTHOR"
CAUSAL DISENTANGLEMENT,0.34725634725634724,"Action Accuracy
Object Accuracy
Action Accuracy
Object Accuracy"
CAUSAL DISENTANGLEMENT,0.34807534807534807,"Method
Z;R64
zi;R64
Z;R64
zi;R64
Z;R64
zi;R64
Z;R64
zi;R64"
CAUSAL DISENTANGLEMENT,0.3488943488943489,"β −V AE [11]
0.27
0.18
0.19
0.06
0.39
0.30
0.44
0.37
d −V AE [21]
0.19
0.69
0.20
0.17
0.35
0.81
0.40
0.78
ILCM [3]
0.21
0.59
0.14
0.14
0.30
0.70
0.41
0.76
ICRL-SM (ours)
0.16
0.86
0.16
0.18
0.28
0.93
0.40
0.82"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3497133497133497,"5.2
Factors Affecting Causal Disentanglement
369"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35053235053235055,"In this experiment, we consider the graph G3, which has the best identifiability, and change the
370"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35135135135135137,"intensity of soft intervention and number of edges in its data generation process. To change the
371"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3521703521703522,"intensity, the post-intervention f
loc network weights are initialized with samples drawn from N(1, 1)
372"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35298935298935297,"(almost similar to loc) and N(10, 1) (significantly different from loc). To change the number of
373"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3538083538083538,"edges, we consider a chain and fully-connected graph.
374"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3546273546273546,"Table 4: Left table depicts the action and object accuracy of three explicit models, with experiments
conducted applying an image with resolution of R64 as the input to the Resnet50 encoder with the
intervened causal variable (zi). Right table shows the comparison of ICRL-SM performance on
different configurations of G5"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35544635544635544,"Datasets
Methods
Action Accuracy
Object Accuracy"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35626535626535627,"Epic-Kitchens
ENCO [16]
0.69
0.13
DDS [5]
0.44
0.09
Fixed-order
0.79
0.14
ICRL-SM (ours)
0.86
0.18"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3570843570843571,"ProcTHOR
ENCO [16]
0.45
0.53
DDS [5]
0.64
0.67
Fixed-order
0.65
0.54
ICRL-SM (ours)
0.93
0.82"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3579033579033579,"Edges
Post-intervention
Causal
Causal
causal mechanism
Disentanglement
Completeness"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.35872235872235875,"Chain
Default
0.98
0.98
Full
Default
0.89
0.89
Default
Significantly different
0.68
0.73
Default
Almost similar
0.85
0.86"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3595413595413595,"The results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is
375"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.36036036036036034,"much worse in the fully-connected graph than the default graph of G3. The result for significantly
376"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.36117936117936117,"different post-intervention causal mechanisms indicate that the switch variable cannot approximate
377"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.361998361998362,"intense effects of soft intervention and more supervision is required to observe V. Similar post-
378"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.3628173628173628,"intervention causal mechanisms also do not have sufficient variability to disentangle the causal
379"
FACTORS AFFECTING CAUSAL DISENTANGLEMENT,0.36363636363636365,"variables as mentioned in Theory 3.5.
380"
ACTION INFERENCE,0.36445536445536447,"5.3
Action Inference
381"
ACTION INFERENCE,0.3652743652743653,"In this experiment, we show the performance of ICRL-SM in the real-world Causal-Triplet datasets.
382"
ACTION INFERENCE,0.36609336609336607,"In these datasets V i.e., soft intervention effects, are not directly observable. Nevertheless, our findings
383"
ACTION INFERENCE,0.3669123669123669,"suggest that incorporating soft interventions through V leads to superior performance compared to
384"
ACTION INFERENCE,0.3677313677313677,"other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the
385"
ACTION INFERENCE,0.36855036855036855,"generative system of the dataset would result in improved outcomes.
386"
ACTION INFERENCE,0.36936936936936937,"The results in Table 3 indicate that when including all causal variables to predict actions, ICRL-SM
387"
ACTION INFERENCE,0.3701883701883702,"performs at par with the baseline methods. However, including all causal variables in the action
388"
ACTION INFERENCE,0.371007371007371,"or object inference may cause spurious correlations. Therefore, we have also experimented with
389"
ACTION INFERENCE,0.37182637182637185,"including only the related causal variable in action and object inference. In this setting, ICRL-
390"
ACTION INFERENCE,0.3726453726453726,"SM significantly outperforms the baseline methods which means that it can better disentangle the
391"
ACTION INFERENCE,0.37346437346437344,"causal variables. We have also compared ICRL-SM with explicit causal representation learning
392"
ACTION INFERENCE,0.37428337428337427,"methods. ENCO [16] and DDS [5] have variable topological order of causal variables during training.
393"
ACTION INFERENCE,0.3751023751023751,"Furthermore, we have included a specific setting where the topological order is fixed during training.
394"
ACTION INFERENCE,0.3759213759213759,"As shown in Table 4, our proposed method has superior performance to explicit models as well.
395"
CONCLUSION,0.37674037674037675,"6
Conclusion
396"
CONCLUSION,0.3775593775593776,"ICRL-SM, our novel model, enhances implicit causal representation learning during soft interventions
397"
CONCLUSION,0.3783783783783784,"by introducing a causal mechanism switch variable. Evaluations on synthetic and real-world datasets
398"
CONCLUSION,0.37919737919737917,"demonstrate ICRL-SM’s superiority over state-of-the-art methods, highlighting its practical effective-
399"
CONCLUSION,0.38001638001638,"ness. Our findings emphasize ICRL-SM’s ability to discern causal models from soft interventions,
400"
CONCLUSION,0.3808353808353808,"marking it as a promising avenue for future research.
401"
REFERENCES,0.38165438165438165,"References
402"
REFERENCES,0.3824733824733825,"[1] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation
403"
REFERENCES,0.3832923832923833,"learning. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine
404"
REFERENCES,0.3841113841113841,"Learning Research, pages 372–407. PMLR, 2023.
405"
REFERENCES,0.38493038493038495,"[2] Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, and Mark Crowley. Generative causal
406"
REFERENCES,0.3857493857493858,"representation learning for out-of-distribution motion forecasting. In International Conference on Machine
407"
REFERENCES,0.38656838656838655,"Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 31596–31612. PMLR,
408"
REFERENCES,0.38738738738738737,"2023.
409"
REFERENCES,0.3882063882063882,"[3] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation
410"
REFERENCES,0.389025389025389,"learning. In NeurIPS, 2022.
411"
REFERENCES,0.38984438984438985,"[4] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Schölkopf, and Pradeep
412"
REFERENCES,0.3906633906633907,"Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing,
413"
REFERENCES,0.3914823914823915,"2023.
414"
REFERENCES,0.3923013923013923,"[5] Bertrand Charpentier, Simon Kibler, and Stephan Günnemann. Differentiable DAG sampling. In The Tenth
415"
REFERENCES,0.3931203931203931,"International Conference on Learning Representations, ICLR. OpenReview.net, 2022.
416"
REFERENCES,0.3939393939393939,"[6] Gregory F. Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational
417"
REFERENCES,0.39475839475839475,"data, 2013.
418"
REFERENCES,0.3955773955773956,"[7] Juan D. Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results.
419"
REFERENCES,0.3963963963963964,"In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
420"
REFERENCES,0.3972153972153972,"editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
421"
REFERENCES,0.39803439803439805,"Processing Systems, NeurIPS, 2020.
422"
REFERENCES,0.3988533988533989,"[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma,
423"
REFERENCES,0.39967239967239965,"Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric
424"
REFERENCES,0.4004914004914005,"vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. Int. J. Comput. Vis., 130(1):33–55,
425"
REFERENCES,0.4013104013104013,"2022.
426"
REFERENCES,0.4021294021294021,"[9] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric
427"
REFERENCES,0.40294840294840295,"Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural
428"
REFERENCES,0.4037674037674038,"generation. Advances in Neural Information Processing Systems, 35:5982–5994, 2022.
429"
REFERENCES,0.4045864045864046,"[10] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled
430"
REFERENCES,0.40540540540540543,"representations. In 6th International Conference on Learning Representations, ICLR, 2018.
431"
REFERENCES,0.4062244062244062,"[11] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,
432"
REFERENCES,0.407043407043407,"Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained
433"
REFERENCES,0.40786240786240785,"variational framework. In 5th International Conference on Learning Representations, ICLR, 2017.
434"
REFERENCES,0.4086814086814087,"[12] Alexander Immer, Christoph Schultheiss, Julia E. Vogt, Bernhard Schölkopf, Peter Bühlmann, and Alexan-
435"
REFERENCES,0.4095004095004095,"der Marx. On the identifiability and estimation of causal location-scale noise models. In International
436"
REFERENCES,0.4103194103194103,"Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research,
437"
REFERENCES,0.41113841113841115,"pages 14316–14332. PMLR, 2023.
438"
REFERENCES,0.411957411957412,"[13] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft
439"
REFERENCES,0.41277641277641275,"interventions with unknown targets: Characterization and learning. In Advances in Neural Information
440"
REFERENCES,0.4135954135954136,"Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020.
441"
REFERENCES,0.4144144144144144,"[14] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A
442"
REFERENCES,0.4152334152334152,"survey and open problems. CoRR, abs/2206.15475, 2022.
443"
REFERENCES,0.41605241605241605,"[15] Sébastien Lachapelle, Pau Rodríguez, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre Lacoste,
444"
REFERENCES,0.4168714168714169,"and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for
445"
REFERENCES,0.4176904176904177,"nonlinear ICA. In 1st Conference on Causal Learning and Reasoning, CLeaR, volume 177 of Proceedings
446"
REFERENCES,0.41850941850941853,"of Machine Learning Research, pages 428–484. PMLR, 2022.
447"
REFERENCES,0.41932841932841936,"[16] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity
448"
REFERENCES,0.4201474201474201,"constraints. In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net,
449"
REFERENCES,0.42096642096642095,"2022.
450"
REFERENCES,0.4217854217854218,"[17] Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS:
451"
REFERENCES,0.4226044226044226,"causal identifiability from temporal intervened sequences. In International Conference on Machine
452"
REFERENCES,0.42342342342342343,"Learning, ICML, volume 162 of Proceedings of Machine Learning Research, pages 13557–13603. PMLR,
453"
REFERENCES,0.42424242424242425,"2022.
454"
REFERENCES,0.4250614250614251,"[18] Chang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, and Tie-Yan Liu.
455"
REFERENCES,0.4258804258804259,"Learning causal semantic representation for out-of-distribution prediction. In M. Ranzato, A. Beygelzimer,
456"
REFERENCES,0.4266994266994267,"Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing
457"
REFERENCES,0.4275184275184275,"Systems, volume 34, pages 6155–6170. Curran Associates, Inc., 2021.
458"
REFERENCES,0.4283374283374283,"[19] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard Schölkopf, and
459"
REFERENCES,0.42915642915642915,"Francesco Locatello. Causal triplet: An open challenge for intervention-centric causal representation
460"
REFERENCES,0.42997542997543,"learning. In Conference on Causal Learning and Reasoning, CLeaR, volume 213 of Proceedings of
461"
REFERENCES,0.4307944307944308,"Machine Learning Research, pages 553–573. PMLR, 2023.
462"
REFERENCES,0.43161343161343163,"[20] Yuejiang Liu, Riccardo Cadei, Jonas Schweizer, Sherwin Bahmani, and Alexandre Alahi. Towards robust
463"
REFERENCES,0.43243243243243246,"and adaptive motion forecasting: A causal representation perspective. In IEEE/CVF Conference on
464"
REFERENCES,0.4332514332514332,"Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages
465"
REFERENCES,0.43407043407043405,"17060–17071. IEEE, 2022.
466"
REFERENCES,0.4348894348894349,"[21] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael
467"
REFERENCES,0.4357084357084357,"Tschannen. Weakly-supervised disentanglement without compromises. In Proceedings of the 37th
468"
REFERENCES,0.43652743652743653,"International Conference on Machine Learning,ICML, volume 119 of Proceedings of Machine Learning
469"
REFERENCES,0.43734643734643736,"Research, pages 6348–6359. PMLR, 2020.
470"
REFERENCES,0.4381654381654382,"[22] Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal
471"
REFERENCES,0.438984438984439,"representation learning for out-of-distribution generalization. In The Tenth International Conference on
472"
REFERENCES,0.4398034398034398,"Learning Representations, ICLR, 2022.
473"
REFERENCES,0.4406224406224406,"[23] Judea Pearl. Causality, cambridge university press (2000). Artif. Intell., 169(2):174–179, 2005.
474"
REFERENCES,0.44144144144144143,"[24] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal inference in statistics: A primer. John
475"
REFERENCES,0.44226044226044225,"Wiley and Sons, 2016.
476"
REFERENCES,0.4430794430794431,"[25] Ronan Perry, Julius von Kügelgen, and Bernhard Schölkopf. Causal discovery in heterogeneous environ-
477"
REFERENCES,0.4438984438984439,"ments under the sparse mechanism shift hypothesis. In NeurIPS, 2022.
478"
REFERENCES,0.44471744471744473,"[26] Bernhard Schölkopf. Causality for machine learning. CoRR, abs/1911.10500, 2019.
479"
REFERENCES,0.44553644553644556,"[27] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh
480"
REFERENCES,0.44635544635544633,"Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612–
481"
REFERENCES,0.44717444717444715,"634, 2021.
482"
REFERENCES,0.447993447993448,"[28] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Weakly supervised
483"
REFERENCES,0.4488124488124488,"disentangled generative causal representation learning. J. Mach. Learn. Res., 23:241:1–241:55, 2022.
484"
REFERENCES,0.44963144963144963,"[29] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via
485"
REFERENCES,0.45045045045045046,"interventions, 2023.
486"
REFERENCES,0.4512694512694513,"[30] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based
487"
REFERENCES,0.4520884520884521,"causal representation learning with interventions, 2023.
488"
REFERENCES,0.45290745290745293,"[31] Julius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf, Michel Besserve,
489"
REFERENCES,0.4537264537264537,"and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from
490"
REFERENCES,0.45454545454545453,"style. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances
491"
REFERENCES,0.45536445536445536,"in Neural Information Processing Systems, volume 34, pages 16451–16467. Curran Associates, Inc., 2021.
492"
REFERENCES,0.4561834561834562,"[32] Julius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki´c, Elias Bareinboim,
493"
REFERENCES,0.457002457002457,"David M. Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal representations from
494"
REFERENCES,0.45782145782145783,"unknown interventions, 2023.
495"
REFERENCES,0.45864045864045866,"[33] Liang Wendong, Armin Keki´c, Julius von Kügelgen, Simon Buchholz, Michel Besserve, Luigi Gresele,
496"
REFERENCES,0.4594594594594595,"and Bernhard Schölkopf. Causal component analysis, 2023.
497"
REFERENCES,0.46027846027846026,"[34] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentan-
498"
REFERENCES,0.4610974610974611,"gled representation learning via neural structural causal models. In IEEE Conference on Computer Vision
499"
REFERENCES,0.4619164619164619,"and Pattern Recognition, CVPR, pages 9593–9602. Computer Vision Foundation / IEEE, 2021.
500"
REFERENCES,0.46273546273546273,"[35] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li. Learning causal representations for
501"
REFERENCES,0.46355446355446356,"robust domain adaptation. IEEE Transactions on Knowledge and Data Engineering, pages 1–1, 2021.
502"
REFERENCES,0.4643734643734644,"[36] Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, and Xindong Wu. Causality-based
503"
REFERENCES,0.4651924651924652,"feature selection: Methods and evaluations. ACM Comput. Surv., 53(5), 2020.
504"
REFERENCES,0.46601146601146604,"[37] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks.
505"
REFERENCES,0.4668304668304668,"In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of
506"
REFERENCES,0.46764946764946763,"Machine Learning Research, pages 7154–7163. PMLR, 2019.
507"
REFERENCES,0.46846846846846846,"[38] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and
508"
REFERENCES,0.4692874692874693,"Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023.
509"
REFERENCES,0.4701064701064701,"[39] Jiaqi Zhang, Chandler Squires, Kristjan H. Greenewald, Akash Srivastava, Karthikeyan Shanmugam,
510"
REFERENCES,0.47092547092547093,"and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. CoRR,
511"
REFERENCES,0.47174447174447176,"abs/2307.06250, 2023.
512"
REFERENCES,0.4725634725634726,"[40] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with NO TEARS: continuous
513"
REFERENCES,0.47338247338247336,"optimization for structure learning. In Advances in Neural Information Processing Systems 31: Annual
514"
REFERENCES,0.4742014742014742,"Conference on Neural Information Processing Systems NeurIPS, pages 9492–9503, 2018.
515"
REFERENCES,0.475020475020475,"[41] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ICA: sparsity and beyond. In
516"
REFERENCES,0.47583947583947583,"NeurIPS, 2022.
517"
REFERENCES,0.47665847665847666,"Appendix
518"
REFERENCES,0.4774774774774775,"A1
Proof of Identifiability Theorem
519"
REFERENCES,0.4782964782964783,"In order to prove our model is identifiable we need a two additional definitions and some previously
520"
REFERENCES,0.47911547911547914,"stated assumptions.
521"
REFERENCES,0.4799344799344799,"Definition A1.1. Structural Causal Models
522"
REFERENCES,0.48075348075348073,"A structural causal model (SCM) is a tuple C = (F, Z, E, G) with the following components:
523"
REFERENCES,0.48157248157248156,"1. The domain of causal variables Z = Z1 × Z2 × . . . × Zn.
524"
REFERENCES,0.4823914823914824,"2. The domain of exogenous variables E = E1 × E2 × . . . × En.
525"
REFERENCES,0.4832104832104832,"3. A directed acyclic graph G(C) over the causal and exogenous variables.
526"
REFERENCES,0.48402948402948404,"4. A causal mechanism fi ∈F which maps an assignment of parent values for the parents Zpai plus
527"
REFERENCES,0.48484848484848486,"an exogenous variable value for Ei to a value of causal variable Zi.
528"
REFERENCES,0.4856674856674857,"Definition A1.2. (Component-wise Transformation) Let ϕ be a transformation (1-1 onto mapping)
529"
REFERENCES,0.4864864864864865,"between product spaces ϕ : Πn
i=1Xi →Πn
i=1Yi. If there exist local transformations ϕi such that
530"
REFERENCES,0.4873054873054873,"∀i, j, ∀x, ϕ(x1, x2, ..., xn)i = ϕi(xj), then ϕ is a component-wise transformation.
531"
REFERENCES,0.4881244881244881,"Definition A1.3. (Diffeomorphism) A diffeomorphism between smooth manifolds M and N is a
532"
REFERENCES,0.48894348894348894,"bijective map f : M →N, which is smooth and has a smooth inverse. Diffeomorphisms preserve
533"
REFERENCES,0.48976248976248976,"information as they are invertible transformations without discontinuous changes in their image.
534"
REFERENCES,0.4905814905814906,"Definition A1.4. (Pushforward measure) Given a measurable function f : A →B between two
535"
REFERENCES,0.4914004914004914,"measurable spaces A and B, and a measure p defined on A, the pushforward measure f∗p on B is
536"
REFERENCES,0.49221949221949224,"defined for measurable sets E in B as:
537"
REFERENCES,0.49303849303849306,"(f∗p)(E) = p(f −1(E))
538"
REFERENCES,0.49385749385749383,"where ∗denotes the pushforward operation. In other words, the pushforward measure f∗p assigns a
539"
REFERENCES,0.49467649467649466,"measure to a set in B by measuring the pre-image of that set under f in the space A.
540"
REFERENCES,0.4954954954954955,"Lemma A1.5. The transformation ϕZ : Z →Z′ between the causal variable of two LCMs M
541"
REFERENCES,0.4963144963144963,"and M′ defined in Definition 3.4 is a component-wise transformation, if ∀i, j, i ̸= j
˜E′
i ⊥⊥˜E′
j and
542"
REFERENCES,0.49713349713349714,"the causal variables follow a multivariate normal distribution conditional on the pre-intervention
543"
REFERENCES,0.49795249795249796,"exogenous variables where ˜E′
i denote the post-intervention exogenous variable of causal variable i
544"
REFERENCES,0.4987714987714988,"in M′.
545"
REFERENCES,0.4995904995904996,"proof: We consider the case where the exogenous variables are mapped to causal variables by a
546"
REFERENCES,0.5004095004095004,"location-scale noise model such that ˜zi =
˜ei−f
loc(e/i)
]
scale(e/i) .
547"
REFERENCES,0.5012285012285013,"∀i, j, i ̸= j
˜E′
i ⊥⊥˜E′
j →E[ ˜E′
i ˜E′
j] = E[ ˜E′
i]E[ ˜E′
j]"
REFERENCES,0.502047502047502,"let’s add these three constants −E[ ˜E′
i]g
loc′
j(e′
/j), −E[ ˜E′
j]g
loc′
i(e′
/i), g
loc′
i(e′
/i)g
loc′
j(e′
/j) to the both
548"
REFERENCES,0.5028665028665029,"sides of the equality and then divide both sides by ^
scale′
i(e′
/i) ^
scale′
j(e′
/j):
549 E "
REFERENCES,0.5036855036855037,"
˜E′
i ˜E′
j −˜E′
i g
loc′
j(e′
/j) −˜E′
j g
loc′
i(e′
/i) + g
loc′
i(e′
/i)g
loc′
j(e′
/j)"
REFERENCES,0.5045045045045045,"^
scale′
i(e′
/i) ^
scale′
j(e′
/j)  ="
REFERENCES,0.5053235053235053,"E[ ˜E′
i]E[ ˜E′
j] −E[ ˜E′
i]g
loc′
j(e′
/j) −E[ ˜E′
j]g
loc′
i(e′
/i) + g
loc′
i(e′
/i)g
loc′
j(e′
/j)"
REFERENCES,0.5061425061425061,"^
scale′
i(e′
/i) ^
scale′
j(e′
/j) →E "
REFERENCES,0.506961506961507,"(
˜E′
i −g
loc′
i(e′
/i)"
REFERENCES,0.5077805077805078,"^
scale′
i(e′
/i)
)(
˜E′
j −g
loc′
j(e′
/j)"
REFERENCES,0.5085995085995086,"^
scale′
j(e′
/j)
) "
REFERENCES,0.5094185094185094,"= (
E[ ˜E′
i] −g
loc′
i(e′
/i)"
REFERENCES,0.5102375102375102,"^
scale′
i(e′
/i)
)(
E[ ˜E′
j] −g
loc′
j(e′
/j)"
REFERENCES,0.5110565110565111,"^
scale′
j(e′
/j)
)"
REFERENCES,0.5118755118755118,"→E[ ˜Z′
i ˜
Z′
j|E′] = E[ ˜Z′
i|E′]E[ ˜
Z′
j|E′]"
REFERENCES,0.5126945126945127,"→E[ ˜Z′
i ˜
Z′
j|E′] −E[ ˜Z′
i|E]E[ ˜
Z′
j|E′] = 0"
REFERENCES,0.5135135135135135,"→E[ ˜Z′
i ˜
Z′
j|E′] −E[ ˜Z′
i|E′]E[ ˜
Z′
j|E′] −E[ ˜Z′
i|E′]E[ ˜
Z′
j|E′] + E[ ˜Z′
i|E′]E[ ˜
Z′
j|E′] = 0"
REFERENCES,0.5143325143325144,"→E[ ˜Z′
i ˜
Z′
j|E′] −E[ ˜
Z′
jE[ ˜Z′
i|E′]|E′] −E[ ˜Z′
iE[ ˜
Z′
j|E′]|E′] + E[ ˜Z′
i|E′]E[ ˜
Z′
j|E′] = 0"
REFERENCES,0.5151515151515151,"→E
h
( ˜Z′
i −E[ ˜Z′
i|E′])( ˜
Z′
j −E[ ˜
Z′
j|E′])|E′i
= 0"
REFERENCES,0.515970515970516,"→cov( ˜Z′
i, ˜
Z′
j|E′) = 0"
REFERENCES,0.5167895167895168,"Typically, the aforementioned equalities would be valid for any diffeomorphic solution function
550"
REFERENCES,0.5176085176085176,"˜si : ˜Ei →˜Zi. However, in this paper, we specifically focus on solution functions represented by a
551"
REFERENCES,0.5184275184275184,"location-scale noise model.
552"
REFERENCES,0.5192465192465192,"Assuming that the causal variables follow a multivariate normal distribution conditional on the
553"
REFERENCES,0.5200655200655201,"pre-intervention exogenous variables, cov( ˜Z′
i, ˜
Z′
j|E′) = 0 would imply that ˜Z′
i ⊥⊥˜
Z′
j|E′. Let’s
554"
REFERENCES,0.5208845208845209,"define ϕE = g′−1 ◦g : E →E′ where g and g′ are the decoders in M and M′. As stated in
555"
REFERENCES,0.5217035217035217,"Assumption 3.1, the decoders are diffeomorphism, hence, ϕE is a diffeomorphism. Furthermore, let’s
556"
REFERENCES,0.5225225225225225,"denote ˜s as the set of all solution functions in post-intervention which are also diffeomorphism as
557"
REFERENCES,0.5233415233415234,"stated in Assumption 3.1. Consequently:
558"
REFERENCES,0.5241605241605242,"(ϕ−1
E
is diffeomorphic) ∀i, j, i ̸= j
˜Z′
i ⊥⊥˜
Z′
j|E′ →˜Z′
i ⊥⊥˜
Z′
j|ϕ−1
E (E′) →˜Z′
i ⊥⊥˜
Z′
j|E"
REFERENCES,0.5249795249795249,"→p( ˜Z′
i|E)p( ˜
Z′
j|E) = p( ˜Z′
i, ˜
Z′
j|E)"
REFERENCES,0.5257985257985258,"(all functions in ˜s are diffeomorphism) →p( ˜Z′
i|˜s(E))p( ˜
Z′
j|˜s(E)) = p( ˜Z′
i, ˜
Z′
j|˜s(E))"
REFERENCES,0.5266175266175266,"→p( ˜Z′
i| ˜Z)p( ˜
Z′
j| ˜Z) = p( ˜Z′
i, ˜
Z′
j| ˜Z)"
REFERENCES,0.5274365274365275,"The association between ˜Z′ and ˜Z arises from their shared observation space. We know that every
559"
REFERENCES,0.5282555282555282,"causal variable in M′ depends at least on one of the causal variables in M. If one of the causal
560"
REFERENCES,0.5290745290745291,"variables in M′ depended on more than one causal variable in M, it would create dependency
561"
REFERENCES,0.5298935298935299,"between two variables in M′ and violate the above equality. Therefore, no variable in M′ depends
562"
REFERENCES,0.5307125307125307,"on more than one causal variable in M. Consequently, the transformation ϕZ is a component-wise
563"
REFERENCES,0.5315315315315315,"transformation.
564"
REFERENCES,0.5323505323505323,"Theorem A1.6. (Identifiability of latent causal models.)
Let M = (A, X, g, I) and M′ =
565"
REFERENCES,0.5331695331695332,"(A′, X, g′, I) be two LCMs with shared observation space X and shared intervention targets I.
566"
REFERENCES,0.533988533988534,"Suppose the following conditions are satisfied:
567"
REFERENCES,0.5348075348075348,"1. Identical correspondence assumptions explained in 3.1.
568"
REFERENCES,0.5356265356265356,"2. Soft interventions satisfy Assumption 3.3.
569"
REFERENCES,0.5364455364455365,"3. The causal and exogenous variables are real-valued.
570"
REFERENCES,0.5372645372645373,"4. The causal and exogenous variables follow a multivariate normal distribution.
571"
REFERENCES,0.538083538083538,"Then the following statements are equivalent:
572"
REFERENCES,0.5389025389025389,"-Two LCMs M and M′ assign the same likelihood to interventional and observational data i.e.,
573"
REFERENCES,0.5397215397215397,"pX
M(x, ˜x) = pX ′
M′(x, ˜x).
574"
REFERENCES,0.5405405405405406,"- M and M′ are disentangled, that is M ∼r M′ according to Definition 3.4.
575"
REFERENCES,0.5413595413595413,"Proof We will proceed to prove the equivalence between statements 1 and 2 by showing the implica-
576"
REFERENCES,0.5421785421785422,"tion is true in each direction.
577"
REFERENCES,0.542997542997543,"A1.1
M ∼r M′ ⇒pX
M(x, ˜x) = pX
M′(x, ˜x)
578"
REFERENCES,0.5438165438165438,"This direction is fairly straightforward. According to Definition 3.4, the fact that M ∼r M ′ implies
579"
REFERENCES,0.5446355446355446,"that ϕE is measure preserving. Therefore, pE
M′(e′, ˜e′) = (ϕE)∗pE
M(e, ˜e). Furthermore, considering
580"
REFERENCES,0.5454545454545454,"that ancestry is preserved, ϕZ is measure preserving, and that causal variables are obtained from their
581"
REFERENCES,0.5462735462735463,"ancestral exogenous variables in implicit models, we have pZ
M′(z′, ˜z′) = (ϕZ)∗pZ
M(z, ˜z). Since
582"
REFERENCES,0.5470925470925471,"models are trained to maximize the log likelihood of p(x, ˜x, ˜x −x) and the latent spaces in M
583"
REFERENCES,0.547911547911548,"and M ′ have the same distribution, the decoders should yield the same observational distributions
584"
REFERENCES,0.5487305487305487,"pX
M(x, ˜x) = pX
M′(x, ˜x).
585"
REFERENCES,0.5495495495495496,"A1.2
pX
M(x, ˜x) = pX
M′(x, ˜x) ⇒M ∼r M′
586"
REFERENCES,0.5503685503685504,"Let’s define ϕE = g′−1 ◦g : E →E′. Since we can express e = s−1(z), we can now define ϕZ as
587"
REFERENCES,0.5511875511875511,"ϕZ = s′ ◦g′−1 ◦g ◦s−1 : Z →Z′.
(8)"
REFERENCES,0.552006552006552,"Therefore, ϕE = s′−1◦ϕZ◦s. Because g and g′ are diffeomorphisms, ϕE is a diffeomorphism as well.
588"
REFERENCES,0.5528255528255528,"Furthermore, since pX
M = pX
M′ and ϕE is a diffeomorphism, then pE
M′ = (ϕE)∗pE
M. Consequently,
589"
REFERENCES,0.5536445536445537,"ϕE is measure-preserving. Similarly, ϕE is measure-preserving as well since causal mechanisms are
590"
REFERENCES,0.5544635544635544,"diffeomorphisms.
591"
REFERENCES,0.5552825552825553,"Step 1: Identical correspondence of edges and nodes Let’s define the set U as U = {E ×E|∀I, J ∈
592"
REFERENCES,0.5561015561015561,"I : supp pE,I
M (e, ˜e|I) ∩supp pE,I
M (e, ˜e|J)}. Then, assuming atomic interventions and counterfac-
593"
REFERENCES,0.556920556920557,"tual exogenous variables, pE,I
M (U|I) = pE,I
M (U|J) = 0. Therefore, we can say that pE
M(e, ˜e) =
594
P"
REFERENCES,0.5577395577395577,"I∈I pE,I
M (e, ˜e|I)pI
M(I) is a discrete mixture of non-overlapping distributions pE,I
M (e, ˜e|I). Sim-
595"
REFERENCES,0.5585585585585585,"ilarly, we can say that pE
M′(e, ˜e) is a discrete mixture of non-overlapping distributions. It can be
596"
REFERENCES,0.5593775593775594,"concluded that as ϕE must map between these distributions, there exists a bijection that also induces
597"
REFERENCES,0.5601965601965602,"a permutation ψ : [n] →[n]. Note: If we had non-atomic interventions or non-counterfactual exoge-
598"
REFERENCES,0.561015561015561,"nous variables, then these distributions would have some overlapping. With overlapping distributions,
599"
REFERENCES,0.5618345618345618,"we can no longer claim there is a bijection mapping between these distributions.
600"
REFERENCES,0.5626535626535627,"In space Z, the interventions should also be sufficiently variable in order to have non-overlapping
601"
REFERENCES,0.5634725634725635,"pZ,I
M (z, ˜z|I) distributions. In the case of soft interventions, ˜z is affected by all ancestral exogenous
602"
REFERENCES,0.5642915642915642,"variables which could be ancestors of other causal variables as well. Consequently, if the changes in
603"
REFERENCES,0.5651105651105651,"causal mechanisms are not sufficient, the effect of ancestral exogenous variables on causal variables
604"
REFERENCES,0.5659295659295659,"will share some similarities and create overlapping distributions. Similar to pE
M(e, ˜e|I), we can say
605"
REFERENCES,0.5667485667485668,"that there is a permutation between pZ
M(z, ˜z|I) as well. Furthermore, as we assume the target of
606"
REFERENCES,0.5675675675675675,"interventions are known we have:
607"
REFERENCES,0.5683865683865684,"∀I ∈I : pZ
M(z, ˜z|I) = pZ
M′(z, ˜z|I)
(9)"
REFERENCES,0.5692055692055692,"Consequently, the permutation ψ is an identity transformation. The effect of soft intervention with
608"
REFERENCES,0.5700245700245701,"known targets on these conditional distributions is shown in Figure A1.
609"
REFERENCES,0.5708435708435708,"Step 2: Component-wise ϕZ
610"
REFERENCES,0.5716625716625716,"According to Lemma A1.5, in order to prove that ϕZ is a component-wise transformation, we need
611"
REFERENCES,0.5724815724815725,"to prove that ˜E′
i and ˜E′
j are independent ∀i, j, i ̸= j. In implicit modeling we do not know the parents
612"
REFERENCES,0.5733005733005733,"of each causal variable, hence, we assume the distribution of ˜Z′
i to be conditioned only on E′
i as in
613"
REFERENCES,0.5741195741195741,"Equation 5 since E′
i is a known parent of ˜Z′
i. The mean of a conditional distribution can be calculated
614"
REFERENCES,0.5749385749385749,"as:
615"
REFERENCES,0.5757575757575758,"E[˜z′
i|e′
i] = µ˜z′
i + ρσ˜z′
i
σe′
i
(e′
i −µe′
i)
(10)"
REFERENCES,0.5765765765765766,"where ρ and σ are the correlation coefficient and variance of the random variables, respectively. On
616"
REFERENCES,0.5773955773955773,"the other hand, we model ˜Z′
i using switch mechanisms as:
617"
REFERENCES,0.5782145782145782,0.6 0.4 0.20.0 0.2 0.4 0.6 0.8 1.0 X1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 X2
REFERENCES,0.579033579033579,Observed samples
REFERENCES,0.5798525798525799,"Intervention on E1
Intervention on E2 Z1"
REFERENCES,0.5806715806715806,"10 0
10
20
30
40 Z2 10 0 10 20 30 40 P"
REFERENCES,0.5814905814905815,"0.00
0.01 0.02 0.03 0.04"
M,0.5823095823095823,"0.05
M
M'"
M,0.5831285831285832,Pre-Intervention
M,0.583947583947584,"(a)
(b) Z1"
M,0.5847665847665847,"10 0
10
20
30
40 Z2 10 0 10 20 30 40 P"
M,0.5855855855855856,"0.00
0.01 0.02 0.03 0.04 0.05 M
M'"
M,0.5864045864045864,Intervention on Z1 Z1
M,0.5872235872235873,"10 0
10
20
30
40 Z2 10 0 10 20 30 40 P"
M,0.588042588042588,"0.00
0.01 0.02 0.03 0.04 0.05 M
M'"
M,0.5888615888615889,Intervention on Z2
M,0.5896805896805897,"(c)
(d)"
M,0.5904995904995906,"Figure A1: The distribution of observed and causal variables in two causal models M and M′,
which belong to the equivalence class up to reparameterization. (a) There are 10 observed samples
in which Z1 or Z2 has been intervened on. (b) The distribution of causal variables when I = 0 (no
intervention) is identical to each other but the range of value of causal variables are different and can
be mapped to each other using ϕZ. (c) The intervention on Z1 (I = 1). (d) The intervention on Z2
(I = 2). For I = 1 and I = 2 the distributions are again identical to each other but are different for
different targets of intervention as soft interventions change the conditional distribution (condition on
parents) of causal variables. Also, for each value of I, the distributions of M and M′ should move
in one direction as targets are known."
M,0.5913185913185913,"˜z′
i = si(˜e′
i; e′
/i, h(v′))"
M,0.5921375921375921,"By using Taylor’s expansion we can write above equation as:
618"
M,0.592956592956593,"si(˜e′
i; e′
/i, hi(v′)) = si(˜e′
i; e′
/i, hi(v′
0)) + + ∞
X n=1 1
n! ∂nsi ∂hn
i"
M,0.5937755937755937,"hi=hi(v′
0)
(hi(v′) −hi(v′
0))n
!"
M,0.5945945945945946,"= si(˜e′
i; e′
/i, hi(v′
0)) + Ri"
M,0.5954135954135954,"Furthermore, we assume separable dependence such that:
619"
M,0.5962325962325963,"∃v′
0 such that ∀i
si(˜e′
i; e′
/i, hi(v′
0)) = si(˜e′
i; e′
/i)"
M,0.597051597051597,"An example of such a scenario could be in location-scale noise models, where a soft intervention
620"
M,0.5978705978705978,"changes the location parameter of the model as:
621"
M,0.5986895986895987,"si(e′
i; e′
/i) = e′
i + loc(e′
/i) →˜si(˜e′
i; e′
/i) = si(˜e′
i; e′
/i, hi(v′))"
M,0.5995085995085995,"= ˜e′
i + loc(e′
/i) + hi(v′) = ˜e′
i + loc(e′
/i) + v′2 + v′"
M,0.6003276003276004,"In this example, for v′
0 = 0, si(˜e′
i; e′
/i, hi(v′
0)) = si(˜e′
i; e′
/i).
622"
M,0.6011466011466011,"Consequently, we can write the following equality from Equation 10:
623"
M,0.601965601965602,"E[ ˜Z′
i|e′
i] = E[si( ˜E′
i; E′
/i) + Ri|e′
i] = µ ˜
Z′
i + ρ
σ ˜
Z′
i
σE′
i
(e′
i −µE′
i)"
M,0.6027846027846028,"By taking the partial derivative of both side with respect to ˜E′
j we have:
624"
M,0.6036036036036037,"∀j ̸= i
E[
∂si( ˜E′
i; E′
/i)"
M,0.6044226044226044,"∂˜E′
i
· ∂˜E′
i
∂˜E′
j
+
∂si( ˜E′
i; E′
/i)"
M,0.6052416052416052,"∂E′
/i
·
∂E′
/i
∂˜E′
j
+ ∂Ri"
M,0.6060606060606061,"∂˜E′
j
|e′
i] = 0"
M,0.6068796068796068,"If we did not have the causal mechanism switch variable (hi(V′)), the equation above would only
625"
M,0.6076986076986077,"hold if si was constant in parents, which is not the case due to the presence of soft interventions, or if
626"
M,0.6085176085176085,"∂si( ˜E′
i;E′
/i)"
M,0.6093366093366094,"∂˜E′
i
· ∂˜E′
i
∂˜
E′
j = −
∂si( ˜E′
i;E′
/i)
∂E′
/i
·
∂E′
/i
∂˜
E′
j . The latter scenario would imply that ∂˜
E′
i
∂˜
E′
j ̸= 0, hence, ˜E′
i ̸⊥⊥˜E′
j.
627"
M,0.6101556101556102,"However, by introducing the causal mechanism switch variable V and assuming it is observed, we
628"
M,0.6109746109746109,"can account for the effects of soft interventions through hi(V′). In this case, ∂˜
E′
i
∂˜
E′
j = 0 as exogenous
629"
M,0.6117936117936118,"variables are commonly assumed to be independent in practice. Consequently:
630"
M,0.6126126126126126,"∀i, j
˜E′
i ⊥⊥˜E′
j"
M,0.6134316134316135,"→∀i, j
p( ˜Z′
i, ˜
Z′
j| ˜Zi, ˜
Zj) = p( ˜Z′
i| ˜Zi)p( ˜
Z′
j| ˜
Zj)"
M,0.6142506142506142,"→ϕZ is a component-wise transformation. 𝑧̃! "" 𝑧̃! 𝑧 𝑧̃! "" 𝑧
𝑰
𝑰 𝜑!! 𝑧̃#"" = 𝑧̃#"" 𝜑!"" 𝑓"" 𝑓# 𝑓#$ 𝜑! 𝑓"" $ 𝑽
𝑽"" (a) 𝑧̃! "" 𝑧̃! 𝑧 𝑧̃! "" 𝑰
𝑰 𝜑!! = 𝑓"" 𝑓#
𝑓"" $ (b)"
M,0.6150696150696151,"Figure A2: (a) String diagram of the causal variables Z and Z′. The triangle indicates sampling I
from its distribution. The left-hand side diagram is when ϕZ is applied last and the right-hand side
diagram is when ϕZ is applied first. I is the intervention which affects intervened causal variable’s
mechanism variable. V is used to model the effect of intervention on mechanisms and parents. (b)
String diagrams after discarding ˜
Z′o and the disentangled effect of soft intervention on ˜Zi modeled by
V ."
M,0.6158886158886159,"Step 3: Component-wise ϕE
631"
M,0.6167076167076168,"Using the result from previous step that ϕZ is a component-wise transformation, the string diagrams
632"
M,0.6175266175266175,"for connections between E and E′ will be as shown in Figure A3. ϕEi will only depend on EA,
633"
M,0.6183456183456183,"where A = anci is the ancestors of variable i, and ei. Because s(e)anci, s(e)i, and s′−1(z′)i only
634"
M,0.6191646191646192,"depend on ancestors and ϕZ is a component-wise transformation. The first equality in Figure A3
635"
M,0.61998361998362,"follows from the definition of ϕEi. The second equality holds when we first apply ϕZA and then apply
636"
M,0.6208026208026208,"the causal mechanisms. It can be concluded from the most right-hand side diagram in Figure A3
637"
M,0.6216216216216216,"that the transformation from E′
i × EA →E′
i is constant in EA. Therefore, ϕEi is a component-wise
638"
M,0.6224406224406225,"transformation.
639 𝑓! ""#$ 𝑓! 𝑠"" 𝑓! ""#$ 𝑓! # 𝑠"" =
= 𝜑$! 𝜀! 𝜀"" 𝜀"" # 𝜀!
𝜀"" 𝑧! 𝑧"""
M,0.6232596232596233,"𝜑%""
𝜑%! 𝑧"" #
𝑧! # 𝜀"" #
𝜀"" # 𝑧"" # 𝑧! # 𝜀"" #"
M,0.6240786240786241,"𝜑%"" 
𝑧! 𝜀!"
M,0.6248976248976249,"Figure A3: String diagrams for connections between E and E′. The triangle indicates sampling
variables from their corresponding distributions."
M,0.6257166257166257,"(a) Pre-Epic-Kitchens
(b) Pre-Epic-Kitchens
(c) Pre-Epic-Kitchens
(d) Pre-Epic-Kitchens"
M,0.6265356265356266,"(a) Post: Valve-locked
(b) Post: Bread-Inserted
(c)
Post: Clothes-Gathered
(d) Post: Juice-Poured"
M,0.6273546273546273,"(e) Pre-ProcTHOR
(f) Pre-ProcTHOR
(g) Pre-ProcTHOR
(h) Pre-ProcTHOR"
M,0.6281736281736282,"(e) Post: Cabinet-Open
(f) Post: Box-Open
(g) Post: TV-Broken
(h) Post: TV-On"
M,0.628992628992629,"Figure A4: In the Causal-Triplet dataset [19], visual representations capture both pre and post-
intervention scenarios. The first two rows showcase data samples from Epic-Kitchens, while the third
and fourth rows feature samples from ProcTHOR. Each image in the post-intervention condition
is accompanied by labels specifying the corresponding action and intervened object. In the images
in the first two rows, the agent is performing an action on an object but the camera angle has also
changed. So we can say that for example the distribution of causal variables conditioned on the
camera angle has been changed due to soft intervention."
M,0.6298116298116299,"A2
Soft vs. Hard intervention
640"
M,0.6306306306306306,"In a causal model, an intervention refers to a deliberate action taken to manipulate or change one or
641"
M,0.6314496314496314,"more variables in order to observe its impact on other variables within the causal model. Interventions
642"
M,0.6322686322686323,"help to study how changes in one variable directly cause changes in another, thereby revealing causal
643"
M,0.633087633087633,"relationships.
644 𝑧̃! 𝑰 𝑧̃"" 𝑠! 𝑒/! 𝑠̃"" (a) 𝑧̃! 𝑒/""
𝑰"
M,0.6339066339066339,"𝑠̃!
𝑠"" 𝑧̃$ (b)"
M,0.6347256347256347,"Figure A5: Causal graph models in the presence of Hard (a) and Soft (b) interventions. There are no
connections from parents to ˜Zi in hard interventions (a). Whereas, parents are connected to ˜Zi in soft
interventions (b).Let’s consider an implicit model and use /i to denote all variables except variable
i. The major difference of soft intervention (b) with hard intervention (a) is that ˜Zi is no longer
disconnected from its parents and its causal mechanism ˜si is affected by the intervention. Thus, with
a hard intervention, we know the post-intervention parents of a node ˜Zi (there are none), whereas
with soft interventions, the parents themselves may not change."
M,0.6355446355446356,"Based on the levels of control and manipulation in a causal intervention, we can have soft vs. hard
645"
M,0.6363636363636364,"interventions. A hard intervention involves directly manipulating the variables of interest in a
646"
M,0.6371826371826372,"controlled manner such as Randomized Controlled Trials (RCTs). In other words, a hard intervention
647"
M,0.638001638001638,"sets the value of a causal variable Z to a certain value denoted as do(Z = z) [24].
648"
M,0.6388206388206388,"On the other hand, soft intervention involves more subtle or less controlled manipulation of variables
649"
M,0.6396396396396397,"and changes the conditional distribution of the causal variable p(Z|Zpa) →˜p(Z|Zpa) which can be
650"
M,0.6404586404586404,"modeled as ˜zi = ˜fi(zpai, ˜ei) [7].
651"
M,0.6412776412776413,"Looking at interventions from a graphical standpoint, a hard intervention entails that the intervened
652"
M,0.6420966420966421,"node is solely impacted by the intervention itself, with no influence coming from its ancestral nodes.
653"
M,0.642915642915643,"Conversely, in the context of a soft intervention, the representation of the intervened node can be
654"
M,0.6437346437346437,"influenced not only by the intervention but also by its parent nodes.
655"
M,0.6445536445536445,"As an example, suppose we are trying to understand the causal relationship between different types
656"
M,0.6453726453726454,"of diets and weight loss. The soft intervention in this scenario could be a switch from a regular diet to
657"
M,0.6461916461916462,"a low-carb diet. Switching to a low-carb diet is a voluntary choice made by the individual and there
658"
M,0.647010647010647,"are no external forces or regulations compelling them to make this change (non-coercive).
659"
M,0.6478296478296478,"The intervention involves a modification of the individual’s diet rather than a complete disruption
660"
M,0.6486486486486487,"since they are adjusting the proportion of macronutrients (fats, proteins, and carbs) they consume,
661"
M,0.6494676494676495,"which is less disruptive than a radical change in eating habits (gradual modification). The individual
662"
M,0.6502866502866503,"has autonomy to choose and tailor their diet according to their preferences and health goals so they
663"
M,0.6511056511056511,"are empowered to make informed decisions about their dietary choices (behavioural empowerment).
664"
M,0.6519246519246519,"Conversely, if the government or an authority were to intervene and enforce a mandatory low-carb
665"
M,0.6527436527436528,"diet through legal means, this would constitute a hard intervention. In this scenario, regulations would
666"
M,0.6535626535626535,"be implemented, prohibiting the consumption of specific carbohydrate-containing foods. Regulatory
667"
M,0.6543816543816544,"agencies would be established to oversee and ensure adherence to the low-carb diet mandate, taking
668"
M,0.6552006552006552,"actions such as removing prohibited foods from the market, restricting their import and production,
669"
M,0.6560196560196561,"and so on. Individuals caught consuming banned foods would be subject to fines, legal repercussions,
670"
M,0.6568386568386568,"or other penalties.
671"
M,0.6576576576576577,"A3
Experiments
672"
M,0.6584766584766585,"This section contains additional details about ICRL-SM design architectures, datasets, and experi-
673"
M,0.6592956592956593,"ments settings.
674"
M,0.6601146601146601,"A3.1
Datasets
675"
M,0.6609336609336609,"A3.1.1
Synthetic
676"
M,0.6617526617526618,"We generate simple synthetic datasets with X = Z = Rn. For each value of n, we generate ten
677"
M,0.6625716625716626,"random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM.
678"
M,0.6633906633906634,"To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoulli
679"
M,0.6642096642096642,"distribution with probability 0.5. The pre-intervention and post-intervention causal variables are
680"
M,0.665028665028665,"obtained as:
681"
M,0.6658476658476659,"zi = scale(zpai)ei + loc(zpai)
Soft-Intervention
−−−−−−−−−→˜zi = scale(zpai)˜ei + f
loc(zpai),
(11)"
M,0.6666666666666666,"where the loc and scale networks are changed in post intervention. The pre-intervention loc and
682"
M,0.6674856674856675,"post-intervention f
loc network weights are initialized with samples drawn from N(0, 1) and N(3, 1),
683"
M,0.6683046683046683,"respectively. For ablation studies, we change the mean of these Normal distributions. The scale is
684"
M,0.6691236691236692,"constant 1 for both pre-intervention and post-intervention samples. Both ei and ˜ei are sampled from
685"
M,0.6699426699426699,"a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled
686"
M,0.6707616707616708,"SO(n) rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples,
687"
M,0.6715806715806716,"and 10,000 test samples.
688"
M,0.6723996723996724,"A3.1.2
Causal-Triplet
689"
M,0.6732186732186732,"The Causal-Triplet datasets are consisted of images containing objects in which an action is manipu-
690"
M,0.674037674037674,"lating the objects shown in Figure A4. Examples of actions and objects in these datasets are given in
691"
M,0.6748566748566749,"Table A1 and A2.
692"
M,0.6756756756756757,Table A1: Actions and objects present in the Causal-Triplet images (ProcTHOR Dataset).
M,0.6764946764946765,ProcTHOR Dataset
M,0.6773136773136773,"Object
Television
Bed
Bed
Television
Laptop
Book
Box
Action
Break
Clean
Dirty
Turn off
Turn on
Open
Close"
M,0.6781326781326781,Table A2: Actions and objects present in the Causal-Triplet images (Epic-Kitchens Dataset).
M,0.678951678951679,Epic-Kitchens Dataset
M,0.6797706797706797,"Object
Tofu
Rice
Hob
Bag
Cupboard
Garlic
Tap
Wrap
Rice
Cheese
Action
Insert
Pour
Wash
Fold
Open
Pat
Move
Check
Transition
Stretch"
M,0.6805896805896806,"Object
Wrap
Skin
Button
Lid
Plate
Egg
Sponge
Oil
Water
Dough
Action
Flip
Gather
Press
Lock
Wrap
Drop
Water
Carry
Smell
Mark"
M,0.6814086814086814,"Based on the actions and objects, we treat our causal variables as attributes of objects which can be
693"
M,0.6822276822276823,"changed by actions. Therefore, actions in these datasets are considered as interventions. Assume that
694"
M,0.683046683046683,"z1 corresponds to the attributes of an object, e.g. a door, the target of opening or closing (action’s
695"
M,0.6838656838656839,"target) is z1.
696"
M,0.6846846846846847,"We use actions’ labels in these datasets to detect the targets of interventions to determine which causal
697"
M,0.6855036855036855,"variable has been intervened upon. Note that informing the model about the target of intervention is
698"
M,0.6863226863226863,"not same as informing about the action itself (See Table 3). We use 5000 images of these datasets to
699"
M,0.6871416871416871,"train all models.
700"
M,0.687960687960688,"A3.2
Architecture Design
701"
M,0.6887796887796888,"Based on the ICRL-SM architecture depicted in Figure 2a, we devised a location-scale solution
702"
M,0.6895986895986896,"function (Equation 6) in which the loci and scalei, and hi networks each comprise of fully connected
703"
M,0.6904176904176904,"networks. These networks consist of two layers each, with 64 hidden units per layer and ReLU
704"
M,0.6912366912366913,"activation functions. The encoder and decoder parameters for latents E and ˜E are shared and we use a
705"
M,0.6920556920556921,"separate encoder and decoder with the same architecture for the latent V. For our synthetic dataset
706"
M,0.6928746928746928,"experiments, the encoder and decoder are consisted of fully connected networks with 2 hidden layers
707"
M,0.6936936936936937,"and 64 units in each hidden layer. For the Causal-Triplet datasets, we utilized ResNet-based networks.
708"
M,0.6945126945126945,"The same encoder and decoder architectures are used for all baseline models in the experiments.
709"
M,0.6953316953316954,"ResNet50 encoder, ResNet50 decoder, and classifiers with 1 hidden layer and 64 hidden units are
710"
M,0.6961506961506961,"used for predicting actions and objects for experiments in Table 4 and Table 3. ResNet18 encoder,
711"
M,0.696969696969697,"ResNet18 decoder, and classifiers with 2 hidden layer and 2 hidden units are used for predicting
712"
M,0.6977886977886978,"actions and objects for experiments in Table A4 and Table A3.
713"
M,0.6986076986076986,"A3.3
Training
714"
M,0.6994266994266994,"To enforce the condition described in Equation 5 for i /∈I, we assign the post-intervention exogenous
715"
M,0.7002457002457002,"variables the same value as the pre-intervention exogenous variables. In mathematical terms, this
716"
M,0.7010647010647011,"translates to ∀i /∈I, we set ˜ei = ei.
717"
M,0.7018837018837019,"In our experiments, we do not pretrain the networks, however, for the baseline models we follow the
718"
M,0.7027027027027027,"training procedure in [3]. We also use consistency in our experiments to ensure that the encoder and
719"
M,0.7035217035217035,decoder are inverse of each other. Consistency regularizer is used as P
M,0.7043407043407044,"i Eˆx∼p(ˆx|e),x∼p(x)[(x −ˆx)2]
720"
M,0.7051597051597052,"where ˆx are the reconstructed samples.
721"
M,0.7059787059787059,"For optimization, Adam optimizer is used with default hyperparamters. In the synthetic experiments,
722"
M,0.7067977067977068,"learning rate changes from 3e−4 to 1e−8 with a cosine scheduler. In the Causal-Triplet experiments
723"
M,0.7076167076167076,"in Table 4 and Table 3 learning rate changes from 0.002 to 1e −8 with a cosine scheduler. For Table
724"
M,0.7084357084357085,"A4 and Table A3 experiments earning rate changes from 0.0001 to 1e −8 with a cosine scheduler. In
725"
M,0.7092547092547092,"all experiments the batch size is set to 64. In the main Causal-Triplet experiments we train the models
726"
M,0.7100737100737101,"for 400 epochs, in the appendix Causal-Triplet experiments we train the models for 2000 epochs, and
727"
M,0.7108927108927109,"in the synthetic experiments we train the models for 100 epochs. In the appendix experiments, the
728"
M,0.7117117117117117,"graph parameters for explicit models are frozen after 1000 epochs.
729"
M,0.7125307125307125,"All models are trained using Nvidia GeForce RTX4090 GPUs. Each of the Causal-Triplet experiments
730"
M,0.7133497133497133,"takes 3-8 hours to train the models and each of the synthetic experiments takes 2-3 hours to train the
731"
M,0.7141687141687142,"models.
732"
M,0.714987714987715,"We save the models’ weights with best validation loss and evaluate them using those weights with
733"
M,0.7158067158067158,"test data.
734"
M,0.7166257166257166,"A4
Ablation study
735"
M,0.7174447174447175,"A4.1
Scalability
736"
M,0.7182637182637183,"While our primary research objective centered on addressing identifiability challenges in implicit
737"
M,0.719082719082719,"causal models under soft interventions, we also conducted an investigation into the scalability of our
738"
M,0.7199017199017199,"proposed model. To comprehensively assess its performance, we designed experiments covering a
739"
M,0.7207207207207207,"range of causal graphs, featuring 5 to 10 variables, with 10 different seeds for each variable, following
740"
M,0.7215397215397216,"a similar experimental setup as our 4-variable causal graph experiments. The outcomes of these
741"
M,0.7223587223587223,"experiments, comparing ICRL-SM and ILCM, are presented in Figure A6. By increasing the number
742"
M,0.7231777231777232,"of variables in the graph, confounding factors and ambiguities of causal relations increase as well.
743"
M,0.723996723996724,"Consequently, more supervision on V is required to better separate the effect of causal variables
744"
M,0.7248157248157249,"themselves on the observed variables.
745"
M,0.7256347256347256,"D4
D5
D6
D7
D8
D9
D10
Causal Variables 0.4 0.5 0.6 0.7 0.8 0.9"
M,0.7264537264537264,Causal Disentanglement Score
M,0.7272727272727273,Means with Standard Deviations
M,0.7280917280917281,"ICRL-SM (mean)
ICRL-SM (std)
ILCM (mean)
ILCM (Std)"
M,0.7289107289107289,Figure A6: Causal disentanglement for different number of variables
M,0.7297297297297297,"A4.2
Backbone model
746"
M,0.7305487305487306,"We trained the models using a simpler backbone model, ResNet18, to see how it affects performance.
747"
M,0.7313677313677314,"The input image resolution is 64 × 64 and we use the intervened causal variables to predict action
748"
M,0.7321867321867321,"and object classes. The results are shown in Table A4 and A3. It can be seen from the results that the
749"
M,0.733005733005733,"proposed method outperforms other explicit and implicit models even with a simpler model.
750"
M,0.7338247338247338,"Table A3: Table comparing action and object accuracy across various methods on Causal-Triplet
datasets using ResNet18 model."
M,0.7346437346437347,"Epic-Kitchens
ProcTHOR
Method
Action Accuracy
Object Accuracy
Action Accuracy
Object Accuracy"
M,0.7354627354627354,"β −V AE [11]
0.15
0.04
0.20
0.36
d −V AE [21]
0.16
0.02
0.15
0.38
ILCM [3]
0.19
0.04
0.15
0.42
ICRL-SM (ours)
0.35
0.04
0.40
0.69"
M,0.7362817362817363,"Table A4: Action and object accuracy of three explicit models are compared with ICRL-SM. Exper-
iments are conducted applying image with resolution of R64 as the input to the Resnet18 encoder
with the intervened casual variable (zi)."
M,0.7371007371007371,"Datasets
Methods
Action Accuracy
Object Accuracy"
M,0.737919737919738,"Epic-Kitchens
ENCO [16]
0.14
0.03
DDS [5]
0.16
0.05
Fixed-order
0.14
0.05
ICRL-SM (ours)
0.35
0.04"
M,0.7387387387387387,"ProcTHOR
ENCO [16]
0.16
0.28
DDS [5]
0.34
0.35
Fixed-order
0.34
0.38
ICRL-SM (ours)
0.40
0.69"
M,0.7395577395577395,"NeurIPS Paper Checklist
751"
CLAIMS,0.7403767403767404,"1. Claims
752"
CLAIMS,0.7411957411957412,"Question: Do the main claims made in the abstract and introduction accurately reflect the
753"
CLAIMS,0.742014742014742,"paper’s contributions and scope?
754"
CLAIMS,0.7428337428337428,"Answer: [Yes]
755"
CLAIMS,0.7436527436527437,"Justification: Our contributions include identifiability of causal models with soft inter-
756"
CLAIMS,0.7444717444717445,"ventions. In the proposed methods section we give the theory and assumptions for the
757"
CLAIMS,0.7452907452907452,"identifiability result and in our experiments we evaluate our method using datasets generated
758"
CLAIMS,0.7461097461097461,"by soft interventions.
759"
CLAIMS,0.7469287469287469,"Guidelines:
760"
CLAIMS,0.7477477477477478,"• The answer NA means that the abstract and introduction do not include the claims
761"
CLAIMS,0.7485667485667485,"made in the paper.
762"
CLAIMS,0.7493857493857494,"• The abstract and/or introduction should clearly state the claims made, including the
763"
CLAIMS,0.7502047502047502,"contributions made in the paper and important assumptions and limitations. A No or
764"
CLAIMS,0.7510237510237511,"NA answer to this question will not be perceived well by the reviewers.
765"
CLAIMS,0.7518427518427518,"• The claims made should match theoretical and experimental results, and reflect how
766"
CLAIMS,0.7526617526617526,"much the results can be expected to generalize to other settings.
767"
CLAIMS,0.7534807534807535,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
768"
CLAIMS,0.7542997542997543,"are not attained by the paper.
769"
LIMITATIONS,0.7551187551187551,"2. Limitations
770"
LIMITATIONS,0.7559377559377559,"Question: Does the paper discuss the limitations of the work performed by the authors?
771"
LIMITATIONS,0.7567567567567568,"Answer: [Yes]
772"
LIMITATIONS,0.7575757575757576,"Justification: We have some strict assumptions on data generation process and model
773"
LIMITATIONS,0.7583947583947583,"which are given in Assumptions 3.3 and 3.1 which may not be plausible to satisfy in some
774"
LIMITATIONS,0.7592137592137592,"applications.
775"
LIMITATIONS,0.76003276003276,"Guidelines:
776"
LIMITATIONS,0.7608517608517609,"• The answer NA means that the paper has no limitation while the answer No means that
777"
LIMITATIONS,0.7616707616707616,"the paper has limitations, but those are not discussed in the paper.
778"
LIMITATIONS,0.7624897624897625,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
779"
LIMITATIONS,0.7633087633087633,"• The paper should point out any strong assumptions and how robust the results are to
780"
LIMITATIONS,0.7641277641277642,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
781"
LIMITATIONS,0.764946764946765,"model well-specification, asymptotic approximations only holding locally). The authors
782"
LIMITATIONS,0.7657657657657657,"should reflect on how these assumptions might be violated in practice and what the
783"
LIMITATIONS,0.7665847665847666,"implications would be.
784"
LIMITATIONS,0.7674037674037674,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
785"
LIMITATIONS,0.7682227682227682,"only tested on a few datasets or with a few runs. In general, empirical results often
786"
LIMITATIONS,0.769041769041769,"depend on implicit assumptions, which should be articulated.
787"
LIMITATIONS,0.7698607698607699,"• The authors should reflect on the factors that influence the performance of the approach.
788"
LIMITATIONS,0.7706797706797707,"For example, a facial recognition algorithm may perform poorly when image resolution
789"
LIMITATIONS,0.7714987714987716,"is low or images are taken in low lighting. Or a speech-to-text system might not be
790"
LIMITATIONS,0.7723177723177723,"used reliably to provide closed captions for online lectures because it fails to handle
791"
LIMITATIONS,0.7731367731367731,"technical jargon.
792"
LIMITATIONS,0.773955773955774,"• The authors should discuss the computational efficiency of the proposed algorithms
793"
LIMITATIONS,0.7747747747747747,"and how they scale with dataset size.
794"
LIMITATIONS,0.7755937755937756,"• If applicable, the authors should discuss possible limitations of their approach to
795"
LIMITATIONS,0.7764127764127764,"address problems of privacy and fairness.
796"
LIMITATIONS,0.7772317772317773,"• While the authors might fear that complete honesty about limitations might be used by
797"
LIMITATIONS,0.778050778050778,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
798"
LIMITATIONS,0.7788697788697788,"limitations that aren’t acknowledged in the paper. The authors should use their best
799"
LIMITATIONS,0.7796887796887797,"judgment and recognize that individual actions in favor of transparency play an impor-
800"
LIMITATIONS,0.7805077805077805,"tant role in developing norms that preserve the integrity of the community. Reviewers
801"
LIMITATIONS,0.7813267813267813,"will be specifically instructed to not penalize honesty concerning limitations.
802"
THEORY ASSUMPTIONS AND PROOFS,0.7821457821457821,"3. Theory Assumptions and Proofs
803"
THEORY ASSUMPTIONS AND PROOFS,0.782964782964783,"Question: For each theoretical result, does the paper provide the full set of assumptions and
804"
THEORY ASSUMPTIONS AND PROOFS,0.7837837837837838,"a complete (and correct) proof?
805"
THEORY ASSUMPTIONS AND PROOFS,0.7846027846027847,"Answer: [Yes]
806"
THEORY ASSUMPTIONS AND PROOFS,0.7854217854217854,"Justification: We give the full set of our assumptions in the proposed method section and the
807"
THEORY ASSUMPTIONS AND PROOFS,0.7862407862407862,"detailed proof in Appendix A1.
808"
THEORY ASSUMPTIONS AND PROOFS,0.7870597870597871,"Guidelines:
809"
THEORY ASSUMPTIONS AND PROOFS,0.7878787878787878,"• The answer NA means that the paper does not include theoretical results.
810"
THEORY ASSUMPTIONS AND PROOFS,0.7886977886977887,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
811"
THEORY ASSUMPTIONS AND PROOFS,0.7895167895167895,"referenced.
812"
THEORY ASSUMPTIONS AND PROOFS,0.7903357903357904,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
813"
THEORY ASSUMPTIONS AND PROOFS,0.7911547911547911,"• The proofs can either appear in the main paper or the supplemental material, but if
814"
THEORY ASSUMPTIONS AND PROOFS,0.7919737919737919,"they appear in the supplemental material, the authors are encouraged to provide a short
815"
THEORY ASSUMPTIONS AND PROOFS,0.7927927927927928,"proof sketch to provide intuition.
816"
THEORY ASSUMPTIONS AND PROOFS,0.7936117936117936,"• Inversely, any informal proof provided in the core of the paper should be complemented
817"
THEORY ASSUMPTIONS AND PROOFS,0.7944307944307945,"by formal proofs provided in appendix or supplemental material.
818"
THEORY ASSUMPTIONS AND PROOFS,0.7952497952497952,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7960687960687961,"4. Experimental Result Reproducibility
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7968877968877969,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7977067977067978,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7985257985257985,"of the paper (regardless of whether the code and data are provided or not)?
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7993447993447993,"Answer:[Yes]
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8001638001638002,"Justification: We provide the full details of our model architecture and training settings in
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.800982800982801,"Appendix A3 and in Section 5.
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8018018018018018,"Guidelines:
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8026208026208026,"• The answer NA means that the paper does not include experiments.
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8034398034398035,"• If the paper includes experiments, a No answer to this question will not be perceived
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042588042588042,"well by the reviewers: Making the paper reproducible is important, regardless of
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8050778050778051,"whether the code and data are provided or not.
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8058968058968059,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8067158067158067,"to make their results reproducible or verifiable.
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8075348075348076,"• Depending on the contribution, reproducibility can be accomplished in various ways.
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8083538083538083,"For example, if the contribution is a novel architecture, describing the architecture fully
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8091728091728092,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80999180999181,"be necessary to either make it possible for others to replicate the model with the same
837"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8108108108108109,"dataset, or provide access to the model. In general. releasing code and data is often
838"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8116298116298116,"one good way to accomplish this, but reproducibility can also be provided via detailed
839"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8124488124488124,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
840"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8132678132678133,"of a large language model), releasing of a model checkpoint, or other means that are
841"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.814086814086814,"appropriate to the research performed.
842"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8149058149058149,"• While NeurIPS does not require releasing code, the conference does require all submis-
843"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8157248157248157,"sions to provide some reasonable avenue for reproducibility, which may depend on the
844"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165438165438166,"nature of the contribution. For example
845"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8173628173628174,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
846"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"to reproduce that algorithm.
847"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.819000819000819,"(b) If the contribution is primarily a new model architecture, the paper should describe
848"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8198198198198198,"the architecture clearly and fully.
849"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8206388206388207,"(c) If the contribution is a new model (e.g., a large language model), then there should
850"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8214578214578214,"either be a way to access this model for reproducing the results or a way to reproduce
851"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8222768222768223,"the model (e.g., with an open-source dataset or instructions for how to construct
852"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8230958230958231,"the dataset).
853"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.823914823914824,"(d) We recognize that reproducibility may be tricky in some cases, in which case
854"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8247338247338247,"authors are welcome to describe the particular way they provide for reproducibility.
855"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8255528255528255,"In the case of closed-source models, it may be that access to the model is limited in
856"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8263718263718264,"some way (e.g., to registered users), but it should be possible for other researchers
857"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8271908271908271,"to have some path to reproducing or verifying the results.
858"
OPEN ACCESS TO DATA AND CODE,0.828009828009828,"5. Open access to data and code
859"
OPEN ACCESS TO DATA AND CODE,0.8288288288288288,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
860"
OPEN ACCESS TO DATA AND CODE,0.8296478296478297,"tions to faithfully reproduce the main experimental results, as described in supplemental
861"
OPEN ACCESS TO DATA AND CODE,0.8304668304668305,"material?
862"
OPEN ACCESS TO DATA AND CODE,0.8312858312858313,"Answer: [Yes]
863"
OPEN ACCESS TO DATA AND CODE,0.8321048321048321,"Justification: We provide our anonymized codes which contains the necessary scripts and
864"
OPEN ACCESS TO DATA AND CODE,0.8329238329238329,"instructions to run the experiments.
865"
OPEN ACCESS TO DATA AND CODE,0.8337428337428338,"Guidelines:
866"
OPEN ACCESS TO DATA AND CODE,0.8345618345618345,"• The answer NA means that paper does not include experiments requiring code.
867"
OPEN ACCESS TO DATA AND CODE,0.8353808353808354,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
868"
OPEN ACCESS TO DATA AND CODE,0.8361998361998362,"public/guides/CodeSubmissionPolicy) for more details.
869"
OPEN ACCESS TO DATA AND CODE,0.8370188370188371,"• While we encourage the release of code and data, we understand that this might not be
870"
OPEN ACCESS TO DATA AND CODE,0.8378378378378378,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
871"
OPEN ACCESS TO DATA AND CODE,0.8386568386568387,"including code, unless this is central to the contribution (e.g., for a new open-source
872"
OPEN ACCESS TO DATA AND CODE,0.8394758394758395,"benchmark).
873"
OPEN ACCESS TO DATA AND CODE,0.8402948402948403,"• The instructions should contain the exact command and environment needed to run to
874"
OPEN ACCESS TO DATA AND CODE,0.8411138411138411,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
875"
OPEN ACCESS TO DATA AND CODE,0.8419328419328419,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
876"
OPEN ACCESS TO DATA AND CODE,0.8427518427518428,"• The authors should provide instructions on data access and preparation, including how
877"
OPEN ACCESS TO DATA AND CODE,0.8435708435708436,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
878"
OPEN ACCESS TO DATA AND CODE,0.8443898443898444,"• The authors should provide scripts to reproduce all experimental results for the new
879"
OPEN ACCESS TO DATA AND CODE,0.8452088452088452,"proposed method and baselines. If only a subset of experiments are reproducible, they
880"
OPEN ACCESS TO DATA AND CODE,0.846027846027846,"should state which ones are omitted from the script and why.
881"
OPEN ACCESS TO DATA AND CODE,0.8468468468468469,"• At submission time, to preserve anonymity, the authors should release anonymized
882"
OPEN ACCESS TO DATA AND CODE,0.8476658476658476,"versions (if applicable).
883"
OPEN ACCESS TO DATA AND CODE,0.8484848484848485,"• Providing as much information as possible in supplemental material (appended to the
884"
OPEN ACCESS TO DATA AND CODE,0.8493038493038493,"paper) is recommended, but including URLs to data and code is permitted.
885"
OPEN ACCESS TO DATA AND CODE,0.8501228501228502,"6. Experimental Setting/Details
886"
OPEN ACCESS TO DATA AND CODE,0.8509418509418509,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
887"
OPEN ACCESS TO DATA AND CODE,0.8517608517608518,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
888"
OPEN ACCESS TO DATA AND CODE,0.8525798525798526,"results?
889"
OPEN ACCESS TO DATA AND CODE,0.8533988533988534,"Answer: [Yes]
890"
OPEN ACCESS TO DATA AND CODE,0.8542178542178542,"Justification: We provide the full details of our model architecture and training settings in
891"
OPEN ACCESS TO DATA AND CODE,0.855036855036855,"Appendix A3 and in Section 5.
892"
OPEN ACCESS TO DATA AND CODE,0.8558558558558559,"Guidelines:
893"
OPEN ACCESS TO DATA AND CODE,0.8566748566748567,"• The answer NA means that the paper does not include experiments.
894"
OPEN ACCESS TO DATA AND CODE,0.8574938574938575,"• The experimental setting should be presented in the core of the paper to a level of detail
895"
OPEN ACCESS TO DATA AND CODE,0.8583128583128583,"that is necessary to appreciate the results and make sense of them.
896"
OPEN ACCESS TO DATA AND CODE,0.8591318591318591,"• The full details can be provided either with the code, in appendix, or as supplemental
897"
OPEN ACCESS TO DATA AND CODE,0.85995085995086,"material.
898"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8607698607698607,"7. Experiment Statistical Significance
899"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8615888615888616,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8624078624078624,"information about the statistical significance of the experiments?
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8632268632268633,"Answer: [Yes]
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.864045864045864,"Justification: In our synthetic experiments we initialized the causal graph in the dataests
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8648648648648649,"with different seeds. The results of these different seeds are provided in Table 2 and Figure
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8656838656838657,"A6.
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8665028665028665,"Guidelines:
906"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8673218673218673,"• The answer NA means that the paper does not include experiments.
907"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8681408681408681,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
908"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.868959868959869,"dence intervals, or statistical significance tests, at least for the experiments that support
909"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8697788697788698,"the main claims of the paper.
910"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8705978705978706,"• The factors of variability that the error bars are capturing should be clearly stated (for
911"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8714168714168714,"example, train/test split, initialization, random drawing of some parameter, or overall
912"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8722358722358723,"run with given experimental conditions).
913"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8730548730548731,"• The method for calculating the error bars should be explained (closed form formula,
914"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8738738738738738,"call to a library function, bootstrap, etc.)
915"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8746928746928747,"• The assumptions made should be given (e.g., Normally distributed errors).
916"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8755118755118755,"• It should be clear whether the error bar is the standard deviation or the standard error
917"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763308763308764,"of the mean.
918"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771498771498771,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
919"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.877968877968878,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
920"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8787878787878788,"of Normality of errors is not verified.
921"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8796068796068796,"• For asymmetric distributions, the authors should be careful not to show in tables or
922"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8804258804258804,"figures symmetric error bars that would yield results that are out of range (e.g. negative
923"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8812448812448812,"error rates).
924"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8820638820638821,"• If error bars are reported in tables or plots, The authors should explain in the text how
925"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8828828828828829,"they were calculated and reference the corresponding figures or tables in the text.
926"
EXPERIMENTS COMPUTE RESOURCES,0.8837018837018837,"8. Experiments Compute Resources
927"
EXPERIMENTS COMPUTE RESOURCES,0.8845208845208845,"Question: For each experiment, does the paper provide sufficient information on the com-
928"
EXPERIMENTS COMPUTE RESOURCES,0.8853398853398854,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
929"
EXPERIMENTS COMPUTE RESOURCES,0.8861588861588862,"the experiments?
930"
EXPERIMENTS COMPUTE RESOURCES,0.8869778869778869,"Answer: [Yes]
931"
EXPERIMENTS COMPUTE RESOURCES,0.8877968877968878,"Justification: The details are given in Appendix A3.
932"
EXPERIMENTS COMPUTE RESOURCES,0.8886158886158886,"Guidelines:
933"
EXPERIMENTS COMPUTE RESOURCES,0.8894348894348895,"• The answer NA means that the paper does not include experiments.
934"
EXPERIMENTS COMPUTE RESOURCES,0.8902538902538902,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
935"
EXPERIMENTS COMPUTE RESOURCES,0.8910728910728911,"or cloud provider, including relevant memory and storage.
936"
EXPERIMENTS COMPUTE RESOURCES,0.8918918918918919,"• The paper should provide the amount of compute required for each of the individual
937"
EXPERIMENTS COMPUTE RESOURCES,0.8927108927108927,"experimental runs as well as estimate the total compute.
938"
EXPERIMENTS COMPUTE RESOURCES,0.8935298935298935,"• The paper should disclose whether the full research project required more compute
939"
EXPERIMENTS COMPUTE RESOURCES,0.8943488943488943,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
940"
EXPERIMENTS COMPUTE RESOURCES,0.8951678951678952,"didn’t make it into the paper).
941"
CODE OF ETHICS,0.895986895986896,"9. Code Of Ethics
942"
CODE OF ETHICS,0.8968058968058968,"Question: Does the research conducted in the paper conform, in every respect, with the
943"
CODE OF ETHICS,0.8976248976248976,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
944"
CODE OF ETHICS,0.8984438984438985,"Answer: [Yes]
945"
CODE OF ETHICS,0.8992628992628993,"Justification:
946"
CODE OF ETHICS,0.9000819000819,"Guidelines:
947"
CODE OF ETHICS,0.9009009009009009,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
948"
CODE OF ETHICS,0.9017199017199017,"• If the authors answer No, they should explain the special circumstances that require a
949"
CODE OF ETHICS,0.9025389025389026,"deviation from the Code of Ethics.
950"
CODE OF ETHICS,0.9033579033579033,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
951"
CODE OF ETHICS,0.9041769041769042,"eration due to laws or regulations in their jurisdiction).
952"
BROADER IMPACTS,0.904995904995905,"10. Broader Impacts
953"
BROADER IMPACTS,0.9058149058149059,"Question: Does the paper discuss both potential positive societal impacts and negative
954"
BROADER IMPACTS,0.9066339066339066,"societal impacts of the work performed?
955"
BROADER IMPACTS,0.9074529074529074,"Answer: [NA]
956"
BROADER IMPACTS,0.9082719082719083,"Justification:
957"
BROADER IMPACTS,0.9090909090909091,"Guidelines:
958"
BROADER IMPACTS,0.9099099099099099,"• The answer NA means that there is no societal impact of the work performed.
959"
BROADER IMPACTS,0.9107289107289107,"• If the authors answer NA or No, they should explain why their work has no societal
960"
BROADER IMPACTS,0.9115479115479116,"impact or why the paper does not address societal impact.
961"
BROADER IMPACTS,0.9123669123669124,"• Examples of negative societal impacts include potential malicious or unintended uses
962"
BROADER IMPACTS,0.9131859131859131,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
963"
BROADER IMPACTS,0.914004914004914,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
964"
BROADER IMPACTS,0.9148239148239148,"groups), privacy considerations, and security considerations.
965"
BROADER IMPACTS,0.9156429156429157,"• The conference expects that many papers will be foundational research and not tied
966"
BROADER IMPACTS,0.9164619164619164,"to particular applications, let alone deployments. However, if there is a direct path to
967"
BROADER IMPACTS,0.9172809172809173,"any negative applications, the authors should point it out. For example, it is legitimate
968"
BROADER IMPACTS,0.9180999180999181,"to point out that an improvement in the quality of generative models could be used to
969"
BROADER IMPACTS,0.918918918918919,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
970"
BROADER IMPACTS,0.9197379197379197,"that a generic algorithm for optimizing neural networks could enable people to train
971"
BROADER IMPACTS,0.9205569205569205,"models that generate Deepfakes faster.
972"
BROADER IMPACTS,0.9213759213759214,"• The authors should consider possible harms that could arise when the technology is
973"
BROADER IMPACTS,0.9221949221949222,"being used as intended and functioning correctly, harms that could arise when the
974"
BROADER IMPACTS,0.923013923013923,"technology is being used as intended but gives incorrect results, and harms following
975"
BROADER IMPACTS,0.9238329238329238,"from (intentional or unintentional) misuse of the technology.
976"
BROADER IMPACTS,0.9246519246519247,"• If there are negative societal impacts, the authors could also discuss possible mitigation
977"
BROADER IMPACTS,0.9254709254709255,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
978"
BROADER IMPACTS,0.9262899262899262,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
979"
BROADER IMPACTS,0.9271089271089271,"feedback over time, improving the efficiency and accessibility of ML).
980"
SAFEGUARDS,0.9279279279279279,"11. Safeguards
981"
SAFEGUARDS,0.9287469287469288,"Question: Does the paper describe safeguards that have been put in place for responsible
982"
SAFEGUARDS,0.9295659295659295,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
983"
SAFEGUARDS,0.9303849303849304,"image generators, or scraped datasets)?
984"
SAFEGUARDS,0.9312039312039312,"Answer: [NA]
985"
SAFEGUARDS,0.9320229320229321,"Justification:
986"
SAFEGUARDS,0.9328419328419328,"Guidelines:
987"
SAFEGUARDS,0.9336609336609336,"• The answer NA means that the paper poses no such risks.
988"
SAFEGUARDS,0.9344799344799345,"• Released models that have a high risk for misuse or dual-use should be released with
989"
SAFEGUARDS,0.9352989352989353,"necessary safeguards to allow for controlled use of the model, for example by requiring
990"
SAFEGUARDS,0.9361179361179361,"that users adhere to usage guidelines or restrictions to access the model or implementing
991"
SAFEGUARDS,0.9369369369369369,"safety filters.
992"
SAFEGUARDS,0.9377559377559378,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
993"
SAFEGUARDS,0.9385749385749386,"should describe how they avoided releasing unsafe images.
994"
SAFEGUARDS,0.9393939393939394,"• We recognize that providing effective safeguards is challenging, and many papers do
995"
SAFEGUARDS,0.9402129402129402,"not require this, but we encourage authors to take this into account and make a best
996"
SAFEGUARDS,0.941031941031941,"faith effort.
997"
LICENSES FOR EXISTING ASSETS,0.9418509418509419,"12. Licenses for existing assets
998"
LICENSES FOR EXISTING ASSETS,0.9426699426699426,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
999"
LICENSES FOR EXISTING ASSETS,0.9434889434889435,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1000"
LICENSES FOR EXISTING ASSETS,0.9443079443079443,"properly respected?
1001"
LICENSES FOR EXISTING ASSETS,0.9451269451269452,"Answer: [Yes]
1002"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"Justification:
1003"
LICENSES FOR EXISTING ASSETS,0.9467649467649467,"Guidelines:
1004"
LICENSES FOR EXISTING ASSETS,0.9475839475839476,"• The answer NA means that the paper does not use existing assets.
1005"
LICENSES FOR EXISTING ASSETS,0.9484029484029484,"• The authors should cite the original paper that produced the code package or dataset.
1006"
LICENSES FOR EXISTING ASSETS,0.9492219492219492,"• The authors should state which version of the asset is used and, if possible, include a
1007"
LICENSES FOR EXISTING ASSETS,0.95004095004095,"URL.
1008"
LICENSES FOR EXISTING ASSETS,0.9508599508599509,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1009"
LICENSES FOR EXISTING ASSETS,0.9516789516789517,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1010"
LICENSES FOR EXISTING ASSETS,0.9524979524979525,"service of that source should be provided.
1011"
LICENSES FOR EXISTING ASSETS,0.9533169533169533,"• If assets are released, the license, copyright information, and terms of use in the
1012"
LICENSES FOR EXISTING ASSETS,0.9541359541359541,"package should be provided. For popular datasets, paperswithcode.com/datasets
1013"
LICENSES FOR EXISTING ASSETS,0.954954954954955,"has curated licenses for some datasets. Their licensing guide can help determine the
1014"
LICENSES FOR EXISTING ASSETS,0.9557739557739557,"license of a dataset.
1015"
LICENSES FOR EXISTING ASSETS,0.9565929565929566,"• For existing datasets that are re-packaged, both the original license and the license of
1016"
LICENSES FOR EXISTING ASSETS,0.9574119574119574,"the derived asset (if it has changed) should be provided.
1017"
LICENSES FOR EXISTING ASSETS,0.9582309582309583,"• If this information is not available online, the authors are encouraged to reach out to
1018"
LICENSES FOR EXISTING ASSETS,0.959049959049959,"the asset’s creators.
1019"
NEW ASSETS,0.9598689598689598,"13. New Assets
1020"
NEW ASSETS,0.9606879606879607,"Question: Are new assets introduced in the paper well documented and is the documentation
1021"
NEW ASSETS,0.9615069615069615,"provided alongside the assets?
1022"
NEW ASSETS,0.9623259623259623,"Answer: [Yes]
1023"
NEW ASSETS,0.9631449631449631,"Justification: We only have a code repository for replicating experiments and we have
1024"
NEW ASSETS,0.963963963963964,"submitted the anonymized zip file with our submission.
1025"
NEW ASSETS,0.9647829647829648,"Guidelines:
1026"
NEW ASSETS,0.9656019656019657,"• The answer NA means that the paper does not release new assets.
1027"
NEW ASSETS,0.9664209664209664,"• Researchers should communicate the details of the dataset/code/model as part of their
1028"
NEW ASSETS,0.9672399672399672,"submissions via structured templates. This includes details about training, license,
1029"
NEW ASSETS,0.9680589680589681,"limitations, etc.
1030"
NEW ASSETS,0.9688779688779688,"• The paper should discuss whether and how consent was obtained from people whose
1031"
NEW ASSETS,0.9696969696969697,"asset is used.
1032"
NEW ASSETS,0.9705159705159705,"• At submission time, remember to anonymize your assets (if applicable). You can either
1033"
NEW ASSETS,0.9713349713349714,"create an anonymized URL or include an anonymized zip file.
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721539721539721,"14. Crowdsourcing and Research with Human Subjects
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737919737919738,"include the full text of instructions given to participants and screenshots, if applicable, as
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746109746109746,"well as details about compensation (if any)?
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754299754299754,"Answer: [NA]
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762489762489762,"Justification:
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9770679770679771,"Guidelines:
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9778869778869779,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787059787059788,"human subjects.
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795249795249795,"• Including this information in the supplemental material is fine, but if the main contribu-
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9803439803439803,"tion of the paper involves human subjects, then as much detail as possible should be
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811629811629812,"included in the main paper.
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819819819819819,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828009828009828,"or other labor should be paid at least the minimum wage in the country of the data
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9836199836199836,"collector.
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844389844389845,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852579852579852,"Subjects
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860769860769861,"Question: Does the paper describe potential risks incurred by study participants, whether
1052"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868959868959869,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1053"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877149877149877,"approvals (or an equivalent approval/review based on the requirements of your country or
1054"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885339885339886,"institution) were obtained?
1055"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893529893529893,"Answer: [NA]
1056"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901719901719902,"Justification:
1057"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990990990990991,"Guidelines:
1058"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918099918099919,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1059"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926289926289926,"human subjects.
1060"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934479934479934,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1061"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942669942669943,"may be required for any human subjects research. If you obtained IRB approval, you
1062"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995085995085995,"should clearly state this in the paper.
1063"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959049959049959,"• We recognize that the procedures for this may vary significantly between institutions
1064"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967239967239967,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1065"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975429975429976,"guidelines for their institution.
1066"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983619983619983,"• For initial submissions, do not include any information that would break anonymity (if
1067"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991809991809992,"applicable), such as the institution conducting the review.
1068"
