Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033112582781456954,"Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However, pre-
trained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is necessary
for reasoning. While knowledge graphs (KG) are often used to augment LMs with
structured representations of world knowledge, it remains an open question how
to effectively fuse and reason over the KG representations and the language con-
text, which provides situational constraints and nuances. In this work, we propose
GREASELM, a new model that fuses encoded representations from pretrained
LMs and graph neural networks over multiple layers of modality interaction op-
erations. Information from both modalities propagates to the other, allowing lan-
guage context representations to be grounded by structured world knowledge, and
allowing linguistic nuances (e.g., negation, hedging) in the context to inform the
graph representations of knowledge. Our results on three benchmarks in the com-
monsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical ques-
tion answering (i.e., MedQA-USMLE) domains demonstrate that GREASELM
can more reliably answer questions that require reasoning over both situational
constraints and structured knowledge, even outperforming models 8√ó larger.1"
INTRODUCTION,0.006622516556291391,"1
INTRODUCTION"
INTRODUCTION,0.009933774834437087,"Question answering is a challenging task that requires complex reasoning over both explicit con-
straints described in the textual context of the question, as well as unstated, relevant knowledge
about the world (i.e., knowledge about the domain of interest). Recently, large pretrained language
models Ô¨Åne-tuned on QA datasets have become the dominant paradigm in NLP for question answer-
ing tasks (Khashabi et al., 2020). After pretraining on an extreme-scale collection of general text
corpora, these language models learn to implicitly encode broad knowledge about the world, which
they are able to leverage when Ô¨Åne-tuned on a domain-speciÔ¨Åc downstream QA task. However,
despite the strong performance of this two-stage learning procedure on common benchmarks, these
models struggle when given examples that are distributionally different from examples seen during
Ô¨Åne-tuning (McCoy et al., 2019). Their learned behavior often relies on simple (at times spurious)
patterns to offer shortcuts to an answer, rather than robust, structured reasoning that effectively fuses
the explicit information provided by the context and implicit external knowledge (Marcus, 2018)."
INTRODUCTION,0.013245033112582781,"On the other hand, massive knowledge graphs (KG), such as Freebase (Bollacker et al., 2008), Wiki-
data (VrandeÀáci¬¥c & Kr¬®otzsch, 2014), ConceptNet (Speer et al., 2017), and Yago (Suchanek et al.,
2007) capture such external knowledge explicitly using triplets that capture relationships between
entities. Previous research has demonstrated the signiÔ¨Åcant role KGs can play in structured rea-
soning and query answering (Ren et al., 2020; 2021; Ren & Leskovec, 2020). However, extending
these reasoning advantages to general QA (where questions and answers are expressed in natural
language and not easily mapped to strict logical queries) requires Ô¨Ånding the right integration of
knowledge from the KG with the information and constraints provided by the QA example. Prior"
INTRODUCTION,0.016556291390728478,"1All code, data and pretrained models are available at https://github.com/snap-stanford/
GreaseLM."
INTRODUCTION,0.019867549668874173,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023178807947019868,"KG Retrieval
[INT] If it is not used for hair, a 
round brush is an example of 
what?  [SEP] art supplies  [SEP]"
INTRODUCTION,0.026490066225165563,painting
INTRODUCTION,0.029801324503311258,AtLocation
INTRODUCTION,0.033112582781456956,"round
brush"
INTRODUCTION,0.03642384105960265,"hair
brush"
INTRODUCTION,0.039735099337748346,RelatedTo
INTRODUCTION,0.04304635761589404,AtLocation
INTRODUCTION,0.046357615894039736,UsedFor
INTRODUCTION,0.04966887417218543,UsedFor
INTRODUCTION,0.052980132450331126,Choice
INTRODUCTION,0.056291390728476824,"Entity
Question"
INTRODUCTION,0.059602649006622516,Entity
INTRODUCTION,0.06291390728476821,"art
supply"
INTRODUCTION,0.06622516556291391,Interaction Node hair
INTRODUCTION,0.0695364238410596,LM Layer
INTRODUCTION,0.0728476821192053,Cross-modal Fuser
INTRODUCTION,0.076158940397351,GreaseLM Layer
INTRODUCTION,0.07947019867549669,"GreaseLM 
Layer √óùëÄ"
INTRODUCTION,0.08278145695364239,Uni-modal
INTRODUCTION,0.08609271523178808,"Encoder
LM Layer MInt"
INTRODUCTION,0.08940397350993377,GNN Layer ‚Ñé!
INTRODUCTION,0.09271523178807947,"(#$!)
‚Ñé&"
INTRODUCTION,0.09602649006622517,"(#$!)
‚Ñé'()"
INTRODUCTION,0.09933774834437085,"(#$!)
¬∑¬∑¬∑ ‚Ñé!"
INTRODUCTION,0.10264900662251655,"(#)
‚Ñé& (#) $‚Ñé'() (#)"
INTRODUCTION,0.10596026490066225,"¬∑¬∑¬∑
‚Ñé'() (#) ùëí'()"
INTRODUCTION,0.10927152317880795,"(#$!)
ùëí!"
INTRODUCTION,0.11258278145695365,"(#$!)
ùëí*"
INTRODUCTION,0.11589403973509933,"(#$!)
¬∑¬∑¬∑ ÃÉùëí'() (#) ùëí!"
INTRODUCTION,0.11920529801324503,"(#)
ùëí*"
INTRODUCTION,0.12251655629139073,"(#)
¬∑¬∑¬∑
ùëí'() (#)"
INTRODUCTION,0.12582781456953643,"LM Layer
GNN Layer ‚Ñé!"
INTRODUCTION,0.1291390728476821,"(#+!)
‚Ñé& (#+!) $‚Ñé'() (#+!)"
INTRODUCTION,0.13245033112582782,"¬∑¬∑¬∑
‚Ñé'() (#+!) ÃÉùëí'() (#+!) ùëí!"
INTRODUCTION,0.1357615894039735,"(#+!)
ùëí*"
INTRODUCTION,0.1390728476821192,"(#+!)
¬∑¬∑¬∑
ùëí'() (#+!) Int"
INTRODUCTION,0.1423841059602649,Pooling
INTRODUCTION,0.1456953642384106,Answer Selection MLP √óùëÅ
INTRODUCTION,0.1490066225165563,"¬∑¬∑¬∑
¬∑¬∑¬∑
[INT]
If
it
[SEP]"
INTRODUCTION,0.152317880794702,GreaseLM Layer ‚†á ‚†á
INTRODUCTION,0.15562913907284767,"[INT]
Int MInt"
INTRODUCTION,0.15894039735099338,"Figure 1: GREASELM Architecture. The textual context is appended with a special interaction
token and passed through N LM-based unimodal encoding layers. Simultaneously, a local KG of
relevant knowledge is extracted and connected to an interaction node. In the later GREASELM lay-
ers, the language representation continues to be updated through LM layers and the KG is processed
using a GNN, simulating reasoning over its knowledge. In each layer, after each modality‚Äôs repre-
sentation is updated, the representations of the interaction token and node are pulled, concatenated,
and passed through a modality interaction (MInt) unit to mix their representations. In subsequent
layers, the mixed information from the interaction elements mixes with their respective modalities,
allowing knowledge from the KG to affect the representations of individual tokens, and context from
language to affect Ô¨Åne-grained entity knowledge representations in the GNN."
INTRODUCTION,0.16225165562913907,"methods propose various ways to leverage both modalities (i.e., expressive large language models
and structured KGs) for improved reasoning (Mihaylov & Frank, 2018; Lin et al., 2019; Feng et al.,
2020). However, these methods typically fuse the two modalities in a shallow and non-interactive
manner, encoding both separately and fusing them at the output for a prediction, or using one to
augment the input of the other. Consequently, previous methods demonstrate restricted capacity to
exchange useful information between the two modalities. It remains an open question how to effec-
tively fuse the KG and LM representations in a truly uniÔ¨Åed manner, where the two representations
can interact in a non-shallow way to simulate structured, situational reasoning."
INTRODUCTION,0.16556291390728478,"In this work, we present GREASELM, a new model that enables fusion and exchange of informa-
tion from both the LM and KG in multiple layers of its architecture (see Figure 1). Our proposed
GREASELM consists of an LM that takes as input the natural language context, as well as a graph
neural network (GNN) that reasons over the KG. After each layer of the LM and GNN, we design
an interactive scheme to bidirectionally transfer the information from each modality to the other
through specially initialized interaction representations (i.e., interaction token for the LM; interac-
tion node for the GNN). In such a way, all the tokens in the language context receive information
from the KG entities through the interaction token and the KG entities indirectly interact with the
tokens through the interaction node. By such a deep integration across all layers, GREASELM en-
ables joint reasoning over both the language context and the KG entities under a uniÔ¨Åed framework
agnostic to the speciÔ¨Åc language model or graph neural network, so that both modalities can be
contextualized by the other."
INTRODUCTION,0.16887417218543047,"GREASELM demonstrates signiÔ¨Åcant performance gains across different LM architectures. We
perform experiments on several standard QA benchmarks: CommonsenseQA, OpenbookQA and
MedQA-USMLE, which require external knowledge across different domains (commonsense rea-
soning and medical reasoning) and use different KGs (ConceptNet and Disease Database). Across
both domains, GREASELM outperforms comparably-sized prior QA models, including strong Ô¨Åne-"
INTRODUCTION,0.17218543046357615,Published as a conference paper at ICLR 2022
INTRODUCTION,0.17549668874172186,"tuned LM baselines (by 5.5%, 6.6%, and 1.3%, respectively) and state-of-the-art KG+LM models
(by 0.9%, 1.8%, and 0.5%, respectively) on the three competitive benchmarks. Furthermore, with
the deep fusion of both modalities, GREASELM exhibits strong performance over baselines on ques-
tions that exhibit textual nuance, such as resolving multiple constraints, negation, and hedges, and
which require effective reasoning over both language context and KG."
RELATED WORK,0.17880794701986755,"2
RELATED WORK"
RELATED WORK,0.18211920529801323,"Integrating KG information has become a popular research area for improving neural QA systems.
Some works explore using two-tower models to answer questions, where a graph representation
of knowledge and language representation are fused with no interaction between them (Wang et al.,
2019). Other works seek to use one modality to ground the other, such as using an encoded represen-
tation of a linked KG to augment the textual representation of a QA example (e.g., Knowledgeable
Reader, Mihaylov & Frank, 2018; KagNet, Lin et al., 2019; KT-NET, Yang et al., 2019). Others re-
verse the Ô¨Çow of information and use a representation of the text (e.g., Ô¨Ånal layer of LM) to provide
an augmentation to a graph reasoning model over an extracted KG for the example (e.g., MHGRN,
Feng et al., 2020; Lv et al., 2020). In all of these settings, however, the interaction between both
modalities is limited as information between them only Ô¨Çows one way."
RELATED WORK,0.18543046357615894,"More recent approaches explore deeper integrations of both modalities. Certain approaches learn to
access implicit knowledge encoded in LMs (Bosselut et al., 2019; Petroni et al., 2019; Hwang et al.,
2021) by training on structured KG data, and then use the LM to generate local KGs that can be
used for QA (Wang et al., 2020; Bosselut et al., 2021). However, these approaches discard the static
KG once they train the LM on its facts, losing important structure that can guide reasoning. More
recently, QA-GNN (Yasunaga et al., 2021) proposed to jointly update the LM and GNN represen-
tations via message passing. However, they use a single pooled representation of the LM to seed
the textual component of this joint structure, limiting the updates that can be made to the textual
representation. In contrast to prior works, we propose to make individual token representations in
the LM and node representations in the GNN mix for multiple layers, enabling representations of
both modalities to reÔ¨Çect particularities of the other (e.g., knowledge grounds language; language
nuances speciÔ¨Åes which knowledge is important). Simultaneously, we retain the individual structure
of both modalities, which we demonstrate improves QA performance substantially (¬ß5)."
RELATED WORK,0.18874172185430463,"Additionally, some works explore integrating knowledge graphs with language models in the pre-
training stage. However, much like for QA, the modality interaction is typically limited to knowl-
edge feeding language (Zhang et al., 2019; Shen et al., 2020; Yu et al., 2020), rather than designing
interactions across multiple layers. Sun et al. (2020)‚Äôs work is perhaps most similar, but they do not
use the same interaction bottleneck, requiring high-precision entity mention spans for linking, and
they limit expressivity through shared modality parameters for the LM and KG."
RELATED WORK,0.19205298013245034,"3
PROPOSED APPROACH: GREASELM"
RELATED WORK,0.19536423841059603,"In this work, we augment large-scale language models (Devlin et al., 2019; Liu et al., 2019; Lan
et al., 2020; Liu et al., 2021) with graph reasoning modules over KGs. Our method, GREASELM
(depicted in Figure 1), consists of two stacked components: (1) a set of unimodal LM layers which
learn an initial representation of the input tokens, and (2) a set of upper cross-modal GREASELM
layers which learn to jointly represent the language sequence and linked knowledge graph, allowing
textual representations formed from the underlying LM layers and a graph representation of the KG
to mix with one another. We denote the number of LM layers as N, and the number of GREASELM
layers as M. The total number of layers in our model is N + M."
RELATED WORK,0.1986754966887417,"Notation. In the task of multiple choice question answering (MCQA), a generic MCQA-type dataset
consists of examples with a context paragraph c, a question q and a candidate answer set A, all
expressed in text. In this work, we also assume access to an external knowledge graph (KG) G that
provides background knowledge that is relevant to the content of the multiple choice questions."
RELATED WORK,0.20198675496688742,"Given a QA example (c, q, A), and the KG G as input, our goal is to identify which answer a ‚ààA is
correct. Without loss of generality, when an operation is applied to an arbitrary answer, we refer to
that answer as a. We denote a sequence of tokens in natural language as {w1, . . . , wT }, where T is"
RELATED WORK,0.2052980132450331,Published as a conference paper at ICLR 2022
RELATED WORK,0.20860927152317882,"the total number of tokens, and the representation of a token wt from the ‚Ñì-th layer of the model as
h(‚Ñì)
t . We denote a set of nodes from the KG as {e1, . . . , eJ}, where J is the total number of nodes,
and the representation of a node ej in the ‚Ñì-th layer of the model as e(‚Ñì)
j ."
INPUT REPRESENTATION,0.2119205298013245,"3.1
INPUT REPRESENTATION"
INPUT REPRESENTATION,0.2152317880794702,"We concatenate our context paragraph c, question q, and candidate answer a with separator tokens
to get our model input [c; q; a] and tokenize the combined sequence into {w1, . . . , wT }. Second, we
use the input sequence to retrieve a subgraph of the KG G (denoted Gsub), which provides knowledge
from the KG that is relevant to this QA example. We denote the set of nodes in Gsub as {e1, . . . , eJ}."
INPUT REPRESENTATION,0.2185430463576159,"KG Retrieval. Given each QA context, we follow the procedure from Yasunaga et al. (2021) to
retrieve the subgraph Gsub from G. We describe this procedure in Appendix B.1. Each node in Gsub
is assigned a type based on whether its corresponding entity was linked from the context c, question
q, answer a, or as a neighbor to these nodes. In the rest of the paper, we use ‚ÄúKG‚Äù to refer to Gsub."
INPUT REPRESENTATION,0.22185430463576158,"Interaction Bottlenecks. In the cross-modal GREASELM layers, information is fused between both
modalities, for which we deÔ¨Åne a special interaction token wint and a special interaction node eint
whose representations serve as the bottlenecks through which the two modalities interact (¬ß3.3). We
prepend wint to the token sequence and connect eint to all the linked nodes Vlinked in Gsub."
LANGUAGE PRE-ENCODING,0.2251655629139073,"3.2
LANGUAGE PRE-ENCODING"
LANGUAGE PRE-ENCODING,0.22847682119205298,"In the unimodal encoding component, given the sequence of tokens {wint, w1, . . . , wT }, we Ô¨Årst
sum the token, segment, and positional embeddings for each token to compute its ‚Ñì=0 input repre-
sentation {h(0)
int, h(0)
1 , . . . , h(0)
T }, and then compute an output representation for each layer ‚Ñì:"
LANGUAGE PRE-ENCODING,0.23178807947019867,"{h(‚Ñì)
int, h(‚Ñì)
1 , . . . , h(‚Ñì)
T } = LM-Layer({h(‚Ñì‚àí1)
int
, h(‚Ñì‚àí1)
1
, . . . , h(‚Ñì‚àí1)
T
})
(1)
for ‚Ñì= 1, . . . , N"
LANGUAGE PRE-ENCODING,0.23509933774834438,"where LM-Layer(¬∑) is a single LM encoder layer, whose parameters are initialized using a pretrained
model (¬ß4.1). We refer readers to Vaswani et al. (2017) for technical details of these layers."
GREASELM,0.23841059602649006,"3.3
GREASELM"
GREASELM,0.24172185430463577,"GREASELM uses a cross-modal fusion component to inject information from the KG into language
representations and information from language into KG representations. The GREASELM layer
is designed to separately encode information from both modalities, and fuse their representations
using the bottleneck of the special interaction token and node. It is comprised of three components:
(1) a transformer LM encoder block which continues to encode the language context, (2) a GNN
layer that reasons over KG entities and relations, and (3) a modality interaction layer that takes the
unimodal representations of the interaction token and interaction node and exchanges information
through them. We discuss these three components below."
GREASELM,0.24503311258278146,"Language Representation.
In the ‚Ñì-th GREASELM layer, the input token embeddings
{h(N+‚Ñì‚àí1)
int
, h(N+‚Ñì‚àí1)
1
, . . . , h(N+‚Ñì‚àí1)
T
} are fed into additional transformer LM encoder blocks that
continue to encode the textual context based on the LM‚Äôs pretrained representations:"
GREASELM,0.24834437086092714,"{Àúh(N+‚Ñì)
int
, Àúh(N+‚Ñì)
1
, . . . , Àúh(N+‚Ñì)
T
} = LM-Layer({h(N+‚Ñì‚àí1)
int
, h(N+‚Ñì‚àí1)
1
, . . . , h(N+‚Ñì‚àí1)
T
})
(2)
for ‚Ñì= 1, . . . , M"
GREASELM,0.25165562913907286,"where Àúh corresponds to pre-fused embeddings of the language modality. As we will discuss below,
because hN+‚Ñì‚àí1
int
will encode information received from the knowledge graph representation, these
late language encoding layers will also allow the token representations to mix with KG knowledge."
GREASELM,0.25496688741721857,"Graph Representation. The GREASELM layers also encode a representation of the local KG Gsub
linked from the QA example. To represent the graph, we Ô¨Årst compute initial node embeddings
{e(0)
1 , . . . , e(0)
J } for the retrieved entities using pretrained KG embeddings for these nodes (¬ß4.1).
The initial embedding of the interaction node e0
int is initialized randomly."
GREASELM,0.2582781456953642,Published as a conference paper at ICLR 2022
GREASELM,0.26158940397350994,"Then,
in each layer of the GNN, the current representation of the node embeddings
{e(‚Ñì‚àí1)
int
, e(‚Ñì‚àí1)
1
, . . . , e(‚Ñì‚àí1)
J
} is fed into the layer to perform a round of information propagation
between nodes in the graph and yield pre-fused node embeddings for each entity:"
GREASELM,0.26490066225165565,"{Àúe(‚Ñì)
int, Àúe(‚Ñì)
1 , . . . , Àúe(‚Ñì)
J } = GNN({e(‚Ñì‚àí1)
int
, e(‚Ñì‚àí1)
1
, . . . , e(‚Ñì‚àí1)
J
})
(3)
for ‚Ñì= 1, . . . , M"
GREASELM,0.2682119205298013,"where GNN corresponds to a variant of graph attention networks (VeliÀáckovi¬¥c et al., 2018) that is a
simpliÔ¨Åcation of the method of Yasunaga et al. (2021). The GNN computes node representations
Àúe(‚Ñì)
j
for each node ej ‚àà{e1, . . . , eJ} via message passing between neighbors on the graph."
GREASELM,0.271523178807947,"Àúe(‚Ñì)
j
= fn X"
GREASELM,0.27483443708609273,"es‚ààNej ‚à™{ej}
Œ±sjmsj !"
GREASELM,0.2781456953642384,"+ e(‚Ñì‚àí1)
j
(4)"
GREASELM,0.2814569536423841,"where Nej represents the neighborhood of an arbitrary node ej, msj denotes the message one of its
neighbors es passes to ej, Œ±sj is an attention weight that scales the message msj, and fn is a 2-layer
MLP. The messages msj between nodes allow entity information from a node to affect the model‚Äôs
representation of its neighbors, and are computed in the following manner:"
GREASELM,0.2847682119205298,"rsj = fr(Àúrsj, us, uj)
(5)
msj = fm(e(‚Ñì‚àí1)
s
, us, rsj)
(6)"
GREASELM,0.28807947019867547,"where us, uj are node type embeddings, Àúrsj is a relation embedding for the relation connecting
es and ej, fr is a 2-layer MLP, and fm is a linear transformation. The attention weights Œ±sj
scale the contribution of each neighbor‚Äôs message by its importance, and are computed as follows:
qs = fq(e(‚Ñì‚àí1)
s
, us)
(7)
kj = fk(e(‚Ñì‚àí1)
j
, uj, rsj)
(8)"
GREASELM,0.2913907284768212,"Œ≥sj = q‚ä§
s kj
‚àö"
GREASELM,0.2947019867549669,"D
(9)
Œ±sj =
exp(Œ≥sj)
P"
GREASELM,0.2980132450331126,"es‚ààNej ‚à™{ej} exp(Œ≥sj)
(10)"
GREASELM,0.30132450331125826,"where fq and fk are linear transformations and us, uj, rsj are deÔ¨Åned the same as above."
GREASELM,0.304635761589404,"As discussed in the following paragraph, message passing between the interaction node eint and the
nodes from the retrieved subgraph will allow information from text that eint receives from wint to
propagate to the other nodes in the graph."
GREASELM,0.3079470198675497,"Modality Interaction. Finally, after using a transformer LM layer and a GNN layer to update token
embeddings and node embeddings respectively, we use a modality interaction layer (MInt) to let
the two modalities fuse information through the bottleneck of the interaction token wint and the
interaction node eint. We concatenate the pre-fused embeddings of the interaction token Àúh(i)
int and
interaction node Àúe(i)
int, pass the joint representation through a mixing operation (MInt), and then split
the output post-fused embeddings into h(i)
int and e(i)
int:"
GREASELM,0.31125827814569534,"[h(‚Ñì)
int; e(‚Ñì)
int] = MInt([Àúh(‚Ñì)
int; Àúe(‚Ñì)
int]),
(11)"
GREASELM,0.31456953642384106,"We use a two-layer MLP as our MInt operation, though other fusion operators could be used to mix
the representation. All the tokens other than the interaction token wint and all the nodes other than
the interaction node eint are not involved in the modality interaction process: w(‚Ñì) = Àúw(‚Ñì) for w ‚àà
{w1, . . . , wT } and e(‚Ñì) = Àúe(‚Ñì) for e ‚àà{e1, . . . , eJ}. However, they receive information from the in-
teraction representations h(‚Ñì)
int and e(‚Ñì)
int in the next layers of their respective modal propagation (i.e.,
Eqs. 2, 3). Consequently, across multiple GREASELM layers, information propagates between both
modalities (see Fig. 1 for visual depiction), grounding language representations to KG knowledge,
and knowledge representations to contextual constraints."
GREASELM,0.31788079470198677,"Learning & Inference.
For the MCQA task, given a question q and an answer a from all
the candidates A, we compute the probability of a being the correct answer as p(a | q, c) ‚àù
exp(MLP(h(N+M)
int
, e(M)
int , g)), where g denotes attention-based pooling of {e(M)
j
| ej
‚àà"
GREASELM,0.3211920529801324,"{e1, . . . , eJ}} using h(N+M)
int
as a query. We optimize the whole model end-to-end using the cross
entropy loss. At inference time, we predict the most plausible answer as arg maxa‚ààA p(a | q, c)."
GREASELM,0.32450331125827814,Published as a conference paper at ICLR 2022
GREASELM,0.32781456953642385,"Dataset
Example"
GREASELM,0.33112582781456956,"CommonsenseQA
A weasel has a thin body and short legs to easier burrow after prey in a what?
(A) tree (B) mulberry bush (C) chicken coop (D) viking ship (E) rabbit warren"
GREASELM,0.3344370860927152,"OpenbookQA
Which of these would let the most heat travel through?
(A) a new pair of jeans
(B) a steel spoon in a cafeteria
(C) a cotton candy at a store (D) a calvin klein cotton hat"
GREASELM,0.33774834437086093,MedQA-USMLE
GREASELM,0.34105960264900664,"A 57-year-old man presents to his primary care physician with a 2-month
history of right upper and lower extremity weakness. He noticed the weakness
when he started falling far more frequently while running errands. Since then,
he has had increasing difÔ¨Åculty with walking and lifting objects. His past
medical history is signiÔ¨Åcant only for well-controlled hypertension, but he says
that some members of his family have had musculoskeletal problems. His right
upper extremity shows forearm atrophy and depressed reÔ¨Çexes while his right
lower extremity is hypertonic with a positive Babinski sign. Which of the
following is most likely associated with the cause of this patients symptoms?
(A) HLA-B8 haplotype
(B) HLA-DR2 haplotype
(C) Mutation in SOD1
(D) Mutation in SMN1"
GREASELM,0.3443708609271523,Table 1: Examples of the MCQA task for each of the datasets evaluated in this work.
EXPERIMENTAL SETUP,0.347682119205298,"4
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.3509933774834437,"We evaluate GREASELM on three diverse multiple-choice question answering datasets across two
domains: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018) as com-
monsense reasoning benchmarks, and MedQA-USMLE (Jin et al., 2021) as a clinical QA task."
EXPERIMENTAL SETUP,0.3543046357615894,"CommonsenseQA is a 5-way multiple-choice question answering dataset of 12,102 questions that
require background commonsense knowledge beyond surface language understanding. We perform
our experiments using the in-house data split of Lin et al. (2019) to compare to baseline methods."
EXPERIMENTAL SETUP,0.3576158940397351,"OpenbookQA is a 4-way multiple-choice question answering dataset that tests elementary scientiÔ¨Åc
knowledge. It contains 5,957 questions along with an open book of scientiÔ¨Åc facts. We use the
ofÔ¨Åcial data splits from Mihaylov & Frank (2018)."
EXPERIMENTAL SETUP,0.3609271523178808,"MedQA-USMLE is a 4-way multiple-choice question answering dataset, which requires biomed-
ical and clinical knowledge. The questions are originally from practice tests for the United States
Medical License Exams (USMLE). The dataset contains 12,723 questions. We use the original data
splits from Jin et al. (2021)."
IMPLEMENTATION & TRAINING DETAILS,0.36423841059602646,"4.1
IMPLEMENTATION & TRAINING DETAILS"
IMPLEMENTATION & TRAINING DETAILS,0.3675496688741722,"Language Models.
We seed GREASELM with RoBERTa-Large (Liu et al., 2019) for our exper-
iments on CommonsenseQA, AristoRoBERTa (Clark et al., 2019) for our experiments on Open-
bookQA, and SapBERT (Liu et al., 2021) for our experiments on MedQA-USMLE, demonstrating
GREASELM‚Äôs generality with respect to language model initializations. Hyperparameters for train-
ing these models can be found in Appendix Table 7."
IMPLEMENTATION & TRAINING DETAILS,0.3708609271523179,"Knowledge Graphs. We use ConceptNet (Speer et al., 2017), a general-domain knowledge graph,
as our external knowledge source G for both CommonsenseQA and OpenbookQA. It has 799,273
nodes and 2,487,810 edges in total. For MedQA-USMLE, we use a self-constructed knowledge
graph that integrates the Disease Database portion of the UniÔ¨Åed Medical Language System (UMLS;
Bodenreider, 2004) and DrugBank (Wishart et al., 2018). The knowledge graph contains 9,958
nodes and 44,561 edges. Additional information about node initialization and hyperparameters for
preprocessing these KGs can be found in Appendix B.2."
BASELINE METHODS,0.3741721854304636,"4.2
BASELINE METHODS"
BASELINE METHODS,0.37748344370860926,"Fine-tuned LMs.
To study the effect of using KGs as external knowledge sources, we compare
our method with vanilla Ô¨Åne-tuned LMs, which are knowledge-agnostic. We Ô¨Åne-tune RoBERTa-"
BASELINE METHODS,0.38079470198675497,Published as a conference paper at ICLR 2022
BASELINE METHODS,0.3841059602649007,"Table 2: Performance comparison on CommonsenseQA in-house split (controlled experiments).
As the ofÔ¨Åcial test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy,
following the data split of Lin et al. (2019). Experiments are controlled using same seed LM."
BASELINE METHODS,0.38741721854304634,"Methods
IHdev-Acc. (%)
IHtest-Acc. (%)"
BASELINE METHODS,0.39072847682119205,"RoBERTa-Large (w/o KG)
73.1 (¬±0.5)
68.7 (¬±0.6)"
BASELINE METHODS,0.39403973509933776,"RGCN (Schlichtkrull et al., 2018)
72.7 (¬±0.2)
68.4 (¬±0.7)
GconAttn (Wang et al., 2019)
72.6 (¬±0.4)
68.6 (¬±1.0)
KagNet (Lin et al., 2019)
73.5 (¬±0.2)
69.0 (¬±0.8)
RN (Santoro et al., 2017)
74.6 (¬±0.9)
69.1 (¬±0.2)
MHGRN (Feng et al., 2020)
74.5 (¬±0.1)
71.1 (¬±0.8)
QA-GNN (Yasunaga et al., 2021)
76.5 (¬±0.2)
73.4 (¬±0.9)"
BASELINE METHODS,0.3973509933774834,"GREASELM (Ours)
78.5 (¬±0.5)
74.2 (¬±0.4)"
BASELINE METHODS,0.40066225165562913,"Table 3: Test Accuracy comparison
on OpenBookQA. Experiments are
controlled using the same seed LM
for all LM+KG methods."
BASELINE METHODS,0.40397350993377484,"Model
Acc."
BASELINE METHODS,0.40728476821192056,"AristoRoBERTa (no KG)
78.4"
BASELINE METHODS,0.4105960264900662,"+ RGCN
74.6
+ GconAttn
71.8
+ RN
75.4
+ MHGRN
80.6
+ QA-GNN
82.8"
BASELINE METHODS,0.4139072847682119,"GREASELM (Ours)
84.8"
BASELINE METHODS,0.41721854304635764,"Table 4:
Test accuracy comparison to public Open-
BookQA model implementations.
‚àóUniÔ¨ÅedQA (11B
params) and T5 (3B) are 30x and 8x larger than our model."
BASELINE METHODS,0.4205298013245033,"Model
Acc.
# Params"
BASELINE METHODS,0.423841059602649,"ALBERT (Lan et al., 2020) + KB
81.0
‚àº235M
HGN (Yan et al., 2020)
81.4
‚â•355M
AMR-SG (Xu et al., 2021)
81.6
‚àº361M
ALBERT + KPG (Wang et al., 2020)
81.8
‚â•235M
QA-GNN (Yasunaga et al., 2021)
82.8
‚àº360M
T5* (Raffel et al., 2020)
83.2
‚àº3B
T5 + KB (Pirtoaca)
85.4
‚â•11B
UniÔ¨ÅedQA* (Khashabi et al., 2020)
87.2
‚àº11B"
BASELINE METHODS,0.4271523178807947,"GREASELM (Ours)
84.8
‚àº359M"
BASELINE METHODS,0.4304635761589404,"Large (Liu et al., 2019) for CommonsenseQA, and AristoRoBERTa2 (Clark et al., 2019) for Open-
bookQA. For MedQA-USMLE, we use a state-of-the-art biomedical language model, SapBERT (Liu
et al., 2021), which is an augmentation of PubmedBERT (Gu et al., 2022) that is trained with entity
disambiguation objectives to allow the model to better understand entity knowledge."
BASELINE METHODS,0.4337748344370861,"LM+KG models. We also evaluate GREASELM‚Äôs ability to exploit its knowledge graph augmen-
tation by comparing with existing LM+KG methods: (1) Relation Network (RN; Santoro et al.,
2017), (2) RGCN (Schlichtkrull et al., 2018), (3) GconAttn (Wang et al., 2019), (4) KagNet (Lin
et al., 2019), (5) MHGRN (Feng et al., 2020), and (6) QA-GNN (Yasunaga et al., 2021). QA-GNN
is the existing top-performing model under this LM+KG paradigm. The key difference between
GREASELM and these baseline methods is that they do not fuse the representations of both modal-
ities across multiple interaction layers, allowing the representation of both modalities to affect the
other (¬ß3.3). For fair comparison, we use the same LM to initialize these baselines as for our model."
EXPERIMENTAL RESULTS,0.4370860927152318,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.44039735099337746,"Our results in Tables 2 and 3 demonstrate a consistent improvement on the CommonsenseQA and
OpenbookQA datasets. On CommonsenseQA, our model‚Äôs test performance improves by 5.5% over
Ô¨Åne-tuned LMs and 0.9% over existing LM+KG models. On OpenbookQA, these improvements are
magniÔ¨Åed, with 6.4% over raw LMs, and 2.0% over the prior best LM+KG system, QA-GNN. The
boost over QA-GNN suggests that GREASELM‚Äôs multi-layer fusion component that passes infor-
mation between the text and KG representations is more expressive than LM+KG methods which do"
EXPERIMENTAL RESULTS,0.44370860927152317,"2OpenbookQA provides an extra corpus of scientiÔ¨Åc facts in a textual form. AristoRoBERTa is based off
RoBERTa-Large, but uses the facts corresponding to each question, prepared by Clark et al. (2019), as an
additional input along with the QA context."
EXPERIMENTAL RESULTS,0.4470198675496689,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.4503311258278146,"Table 5: Performance of GREASELM on the CommonsenseQA IH-dev set on complex questions
with semantic nuance such as prepositional phrases, negation terms, and hedge terms."
EXPERIMENTAL RESULTS,0.45364238410596025,"Model
# Prepositional Phrases
Negation
Hedge
0
1
2
3
4
Term
Term"
EXPERIMENTAL RESULTS,0.45695364238410596,"n
210
429
316
171
59
83
167"
EXPERIMENTAL RESULTS,0.4602649006622517,"RoBERTa-Large
66.7
72.3
76.3
74.3
69.5
63.8
70.7
QA-GNN
76.7
76.2
79.1
74.9
81.4
66.2
76.0"
EXPERIMENTAL RESULTS,0.46357615894039733,"GREASELM (Ours)
75.7
79.3
80.4
77.2
84.7
69.9
78.4"
EXPERIMENTAL RESULTS,0.46688741721854304,"not integrate such sustained interaction between both modalities. We also achieve competitive re-
sults to other systems on the leaderboard of OpenbookQA (Table 4), posting the third highest score.
However, we note that the T5 (Raffel et al., 2020) and UniÔ¨ÅedQA (Khashabi et al., 2020) models are
pretrained models with 8√ó and 30√ó more parameters, respectively, than our model. Among models
with comparable parameter counts, GREASELM achieves the highest score. An ablation study on
different model components and hyperparameters is reported in Appendix C.1."
EXPERIMENTAL RESULTS,0.47019867549668876,"Quantitative Analysis. Given these overall performance improvements, we investigated whether
GREASELM‚Äôs improvements were reÔ¨Çected in questions that required more complex reasoning.
Because we had no gold structures from these datasets to categorize the reasoning complexity of
different questions, we deÔ¨Åned three proxies: the number of prepositional phrases in the questions,
the presence of negation terms, and the presence of hedging terms. We use the number of preposi-
tional phrases as a proxy for the number of explicit reasoning constraints being set in the questions.
For example, the CommonsenseQA question in Table 1, ‚ÄúA weasel has a thin body and short legs to
easier burrow after prey in a what?‚Äù has three prepositional phrases: to easier burrow, after prey, in
a what, which each provide an additional search constraint for the answer (n.b., in certain cases, the
prepositional phrases do not provide constraints that are needed for selecting the correct answer).
The presence of negation and hedging terms stratiÔ¨Åes our evaluation to questions that have explicit
negation mentions (e.g., no, never) and terms indicating uncertainty (e.g., sometimes; maybe)."
EXPERIMENTAL RESULTS,0.4735099337748344,"Our results in Table 5 demonstrate that GREASELM generally outperforms RoBERTa-Large and
QA-GNN for both questions with negation terms and hedge terms, indicating GREASELM handles
contexts with nuanced constraints. Furthermore, we also note that GREASELM performs better than
the baselines across all questions with prepositional phrases, our measure for reasoning complexity.
QA-GNN and GREASELM perform comparably on questions with no prepositional phrases, but
the increasing complexity of questions requires deeper cross-modal fusion between language and
knowledge representations. While QA-GNN‚Äôs end fusion approach of initializing a node in the
GNN from the LM‚Äôs Ô¨Ånal representation of the context is an effective approach, it compresses the
language context to a single vector before allowing interaction with the KG, potentially limiting the
cross-relationships between language and knowledge that can be captured (see example in Figure 2).
Interestingly, we note that both GREASELM and QA-GNN signiÔ¨Åcantly outperform RoBERTa-
Large even when no prepositional phrases are in the question. We hypothesize that some of these
questions may require less reasoning, but require speciÔ¨Åc commonsense knowledge that RoBERTa
may not have learned during pretraining (e.g., ‚ÄúWhat is a person considered a bully known for?‚Äù)."
EXPERIMENTAL RESULTS,0.4768211920529801,"Qualitative Analysis. In Figure 2, we examine GREASELM‚Äôs node-to-node attention weights in-
duced by the GNN layers of the model, and analyze whether they reÔ¨Çect more expressive reasoning
steps compared to QA-GNN. Figure 2 shows an example from the CommonsenseQA IH-dev set. In
this example, GREASELM correctly predicts that the answer is ‚Äúairplane‚Äù while QA-GNN makes
an incorrect prediction, ‚Äúmotor vehicle‚Äù. For both models, we perform Best First Search (BFS) on
the retrieved KG subgraph Gsub to trace high attention weights from the interaction node (purple)."
EXPERIMENTAL RESULTS,0.48013245033112584,"For GREASELM, we observe that the attention by the interaction node increases on the ‚Äúbug‚Äù entity
in the intermediate GNN layers, but drops again by the Ô¨Ånal layer, resembling a suitable intuition
surrounding the hedge term ‚Äúunlikely‚Äù. Meanwhile, the attention on ‚Äúwindshield‚Äù consistently in-
creases across all layers. For QA-GNN, the attention on ‚Äúbug‚Äù increases over multiple layers. As
‚Äúbug‚Äù is mentioned multiple times in the context, it may be well-represented in QA-GNN‚Äôs context
node initialization, which is never reformulated by language representations, unlike in GREASELM."
EXPERIMENTAL RESULTS,0.48344370860927155,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.4867549668874172,What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?
EXPERIMENTAL RESULTS,0.4900662251655629,A. airplane ‚úÖE. motor vehicle car
EXPERIMENTAL RESULTS,0.49337748344370863,vehicle
EXPERIMENTAL RESULTS,0.4966887417218543,GNN 1st Layer
EXPERIMENTAL RESULTS,0.5,windshield bug
EXPERIMENTAL RESULTS,0.5033112582781457,airplane
EXPERIMENTAL RESULTS,0.5066225165562914,(a) GreaseLM Int car
EXPERIMENTAL RESULTS,0.5099337748344371,vehicle
EXPERIMENTAL RESULTS,0.5132450331125827,GNN Middle Layer
EXPERIMENTAL RESULTS,0.5165562913907285,windshield bug
EXPERIMENTAL RESULTS,0.5198675496688742,airplane Int car
EXPERIMENTAL RESULTS,0.5231788079470199,vehicle
EXPERIMENTAL RESULTS,0.5264900662251656,GNN Final Layer
EXPERIMENTAL RESULTS,0.5298013245033113,windshield bug
EXPERIMENTAL RESULTS,0.5331125827814569,airplane Int
EXPERIMENTAL RESULTS,0.5364238410596026,What is unlikely to get bugs on its windshield due to bugs' inability to reach it when it is moving?
EXPERIMENTAL RESULTS,0.5397350993377483,A. airplane E. motor vehicle ‚ùå car
EXPERIMENTAL RESULTS,0.543046357615894,vehicle
EXPERIMENTAL RESULTS,0.5463576158940397,GNN 1st Layer
EXPERIMENTAL RESULTS,0.5496688741721855,windshield bug
EXPERIMENTAL RESULTS,0.5529801324503312,airplane
EXPERIMENTAL RESULTS,0.5562913907284768,(b) QA-GNN Int car
EXPERIMENTAL RESULTS,0.5596026490066225,vehicle
EXPERIMENTAL RESULTS,0.5629139072847682,GNN Middle Layer
EXPERIMENTAL RESULTS,0.5662251655629139,windshield bug
EXPERIMENTAL RESULTS,0.5695364238410596,airplane Int car
EXPERIMENTAL RESULTS,0.5728476821192053,vehicle
EXPERIMENTAL RESULTS,0.5761589403973509,GNN Final Layer
EXPERIMENTAL RESULTS,0.5794701986754967,windshield bug
EXPERIMENTAL RESULTS,0.5827814569536424,airplane Int
EXPERIMENTAL RESULTS,0.5860927152317881,"Figure 2: Qualitative analysis of GREASELM‚Äôs graph attention weight changes across multiple
layers of message passing compared with QA-GNN. GREASELM demonstrates attention change
patterns that more closely resemble the expected change in focus on the ‚Äúbug‚Äù entity."
EXPERIMENTAL RESULTS,0.5894039735099338,"Table 6: Performance on MedQA-USMLE
Methods
Acc. (%)"
EXPERIMENTAL RESULTS,0.5927152317880795,"Baselines (Jin et al., 2021)
CHANCE
25.0
PMI
31.1
IR-ES
35.5
IR-CUSTOM
36.1
CLINICALBERT-BASE
32.4
BIOROBERTA-BASE
36.1
BIOBERT-BASE
34.1
BIOBERT-LARGE
36.7"
EXPERIMENTAL RESULTS,0.5960264900662252,"Baselines (Our implementation)
SapBERT-Base (w/o KG)
37.2
QA-GNN
38.0"
EXPERIMENTAL RESULTS,0.5993377483443708,"GREASELM (Ours)
38.5"
EXPERIMENTAL RESULTS,0.6026490066225165,"Domain generality
Our reported results thus
far demonstrate the viability of our method in
the general commonsense reasoning domain. In
this section, we explore whether GREASELM
could be adapted to other domains by evaluat-
ing on the MedQA-USMLE dataset. Our results
in Table 6 demonstrate that GREASELM out-
performs state-of-the-art Ô¨Åne-tuned LMs (e.g.,
SapBERT; Liu et al., 2021) and a QA-GNN
augmentation of SapBERT. Additionally, we
note the improved performance over all clas-
sical methods and LM methods Ô¨Årst reported
in Jin et al. (2021). Additional results in Ap-
pendix C show that our approach is also ag-
nostic to the language model used with im-
provements recorded by GREASELM when it is
seeded with other LMs, such as PubmedBERT
(Gu et al., 2022), and BioBERT (Lee et al.,
2020). While these results are promising as they suggest that GREASELM is an effective aug-
mentation of pretrained LMs for different domains and KGs (i.e., the medical domain with the DDB
+ Drugbank KG), there is still ample room for improvement on this task."
CONCLUSION,0.6059602649006622,"6
CONCLUSION"
CONCLUSION,0.609271523178808,"In this paper, we introduce GREASELM, a new model that enables interactive fusion through joint
information exchange between knowledge from language models and knowledge graphs. Experi-
mental results demonstrate superior performance compared to prior KG+LM and LM-only baselines
across standard datasets from multiple domains (commonsense and medical). Our analysis shows
improved capability modeling questions exhibiting textual nuances, such as negation and hedging."
CONCLUSION,0.6125827814569537,Published as a conference paper at ICLR 2022
CONCLUSION,0.6158940397350994,ACKNOWLEDGMENT
CONCLUSION,0.6192052980132451,"We thank Rok Sosic, Maria Brbic, Jordan Troutman, Rajas Bansal, and our anonymous review-
ers for discussions and for providing feedback on our manuscript. We thank Xiaomeng Jin for
help with data preprocessing. We also gratefully acknowledge the support of DARPA under Nos.
HR00112190039 (TAMI), N660011924033 (MCS); ARO under Nos. W911NF-16-1-0342 (MURI),
W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598 (CINES), OAC-1934578 (HDR),
CCF-1918940 (Expeditions), IIS-2030477 (RAPID), NIH under No. R56LM013365; Stanford Data
Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMorgan
Chase, Docomo, Hitachi, Intel, JD.com, KDDI, Toshiba, NEC, and UnitedHealth Group. J. L. is
a Chan Zuckerberg Biohub investigator. The content is solely the responsibility of the authors and
does not necessarily represent the ofÔ¨Åcial views of the funding entities."
REFERENCES,0.6225165562913907,REFERENCES
REFERENCES,0.6258278145695364,"Olivier Bodenreider. The uniÔ¨Åed medical language system (UMLS): Integrating biomedical termi-
nology. Nucleic acids research, 2004."
REFERENCES,0.6291390728476821,"Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabo-
ratively created graph database for structuring human knowledge. In SIGMOD, 2008."
REFERENCES,0.6324503311258278,"Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in Neural Information
Processing Systems (NeurIPS), 2013."
REFERENCES,0.6357615894039735,"Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli C¬∏ elikyilmaz, and Yejin
Choi. Comet: Commonsense transformers for automatic knowledge graph construction. In Asso-
ciation for Computational Linguistics (ACL), 2019."
REFERENCES,0.6390728476821192,"Antoine Bosselut, Ronan Le Bras, and Yejin Choi. Dynamic neuro-symbolic knowledge graph con-
struction for zero-shot commonsense question answering. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence, 2021."
REFERENCES,0.6423841059602649,"Peter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson,
Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Niket Tandon, et al. From ‚Äòf‚Äô to ‚Äòa‚Äôon the
NY Regents science exams: An overview of the Aristo project. arXiv preprint arXiv:1909.01958,
2019."
REFERENCES,0.6456953642384106,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019."
REFERENCES,0.6490066225165563,"Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable
multi-hop relational reasoning for knowledge-aware question answering. In Empirical Methods
in Natural Language Processing (EMNLP), 2020."
REFERENCES,0.652317880794702,"Yuxian Gu, Robert Tinn, Hao Cheng, Michael R. Lucas, Naoto Usuyama, Xiaodong Liu, Tris-
tan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-speciÔ¨Åc language model pretraining
for biomedical natural language processing. ACM Transactions on Computing for Healthcare
(HEALTH), 3:1 ‚Äì 23, 2022."
REFERENCES,0.6556291390728477,"Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosse-
lut, and Yejin Choi. Comet-atomic 2020: On symbolic and neural commonsense knowledge
graphs. In AAAI, 2021."
REFERENCES,0.6589403973509934,"Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-
ease does this patient have? a large-scale open domain question answering dataset from medical
exams. Applied Sciences, 2021."
REFERENCES,0.6622516556291391,"Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Ha-
jishirzi. UniÔ¨Åedqa: Crossing format boundaries with a single qa system. In Findings of EMNLP,
2020."
REFERENCES,0.6655629139072847,Published as a conference paper at ICLR 2022
REFERENCES,0.6688741721854304,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.6721854304635762,"Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-
woo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text
mining. Bioinformatics, 36:1234 ‚Äì 1240, 2020."
REFERENCES,0.6754966887417219,"Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph
networks for commonsense reasoning. In Empirical Methods in Natural Language Processing
(EMNLP), 2019."
REFERENCES,0.6788079470198676,"Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. Self-alignment
pretraining for biomedical entity representations. In NAACL, 2021."
REFERENCES,0.6821192052980133,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.6854304635761589,"Shangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin
Jiang, Guihong Cao, and Songlin Hu. Graph-based reasoning over heterogeneous external knowl-
edge for commonsense question answering. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, 2020."
REFERENCES,0.6887417218543046,"G. Marcus. Deep learning: A critical appraisal. ArXiv, abs/1801.00631, 2018."
REFERENCES,0.6920529801324503,"R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. In ACL, 2019."
REFERENCES,0.695364238410596,"Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, and A. G. Galstyan. Lawyers
are dishonest? quantifying representational harms in commonsense knowledge resources. ArXiv,
abs/2103.11320, 2021."
REFERENCES,0.6986754966887417,"Todor Mihaylov and Anette Frank. Knowledgeable reader: Enhancing cloze-style reading compre-
hension with external commonsense knowledge. In Association for Computational Linguistics
(ACL), 2018."
REFERENCES,0.7019867549668874,"Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? A new dataset for open book question answering. In Empirical Methods in Natural
Language Processing (EMNLP), 2018."
REFERENCES,0.7052980132450332,"Fabio Petroni, Tim Rockt¬®aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. Language models as knowledge bases? In Empirical Methods in Natural
Language Processing (EMNLP), 2019."
REFERENCES,0.7086092715231788,"George Sebastian Pirtoaca. Ai2 leaderboard. URL https://leaderboard.allenai.org/
open_book_qa/submission/brhieieqaupc4cnddfg0."
REFERENCES,0.7119205298013245,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniÔ¨Åed text-to-text
transformer. Journal of Machine Learning Research (JMLR), 2020."
REFERENCES,0.7152317880794702,"Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge
graphs. In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.7185430463576159,"Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in
vector space using box embeddings. In International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.7218543046357616,"Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuur-
mans, Jure Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop
question answering on knowledge graphs. In International Conference on Machine Learning
(ICML), 2021."
REFERENCES,0.7251655629139073,Published as a conference paper at ICLR 2022
REFERENCES,0.7284768211920529,"Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Advances in Neural Information Processing Systems (NeurIPS), 2017."
REFERENCES,0.7317880794701986,"Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference, 2018."
REFERENCES,0.7350993377483444,"Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, and Weizhu Chen. Exploiting
structured knowledge in text via graph-guided representation learning. In EMNLP, 2020."
REFERENCES,0.7384105960264901,"Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
Towards controllable
biases in language generation. In the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP)-Findings, long, 2020."
REFERENCES,0.7417218543046358,"Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of
general knowledge. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, 2017."
REFERENCES,0.7450331125827815,"Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: A Core of Semantic Knowledge.
In 16th International Conference on the World Wide Web, pp. 697‚Äì706, 2007."
REFERENCES,0.7483443708609272,"Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng
Zhang. Colake: Contextualized language and knowledge embedding. In COLING, 2020."
REFERENCES,0.7516556291390728,"Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-
tion answering challenge targeting commonsense knowledge. In North American Chapter of the
Association for Computational Linguistics (NAACL), 2019."
REFERENCES,0.7549668874172185,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998‚Äì6008, 2017."
REFERENCES,0.7582781456953642,"Petar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ."
REFERENCES,0.7615894039735099,"Denny VrandeÀáci¬¥c and Markus Kr¬®otzsch. Wikidata: A free collaborative knowledgebase. Commun.
ACM, 57(10):78‚Äì85, September 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https:
//doi.org/10.1145/2629489."
REFERENCES,0.7649006622516556,"Peifeng Wang, Nanyun Peng, Pedro Szekely, and Xiang Ren. Connecting the dots: A knowledgeable
path generator for commonsense question answering. arXiv preprint arXiv:2005.00691, 2020."
REFERENCES,0.7682119205298014,"Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz,
Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, et al. Improving natural language
inference using external knowledge in the science questions domain. In Proceedings of the AAAI
Conference on ArtiÔ¨Åcial Intelligence, 2019."
REFERENCES,0.7715231788079471,"David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir
Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, et al. Drugbank 5.0: a major update to the
drugbank database for 2018. Nucleic acids research, 2018."
REFERENCES,0.7748344370860927,"Weiwen Xu, Huihui Zhang, Deng Cai, and Wai Lam. Dynamic semantic graph construction and rea-
soning for explainable multi-hop science question answering. arXiv preprint arXiv:2105.11776,
2021."
REFERENCES,0.7781456953642384,"Jun Yan, Mrigank Raman, Aaron Chan, Tianyu Zhang, Ryan Rossi, Handong Zhao, Sungchul Kim,
Nedim Lipka, and Xiang Ren. Learning contextualized knowledge structures for commonsense
reasoning. arXiv preprint arXiv:2010.12873, 2020."
REFERENCES,0.7814569536423841,"An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, and Sujian Li.
Enhancing pre-trained language representations with rich knowledge for machine reading com-
prehension. In Association for Computational Linguistics (ACL), 2019."
REFERENCES,0.7847682119205298,Published as a conference paper at ICLR 2022
REFERENCES,0.7880794701986755,"Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.
QA-
GNN: Reasoning with language models and knowledge graphs for question answering. ArXiv,
abs/2104.06378, 2021."
REFERENCES,0.7913907284768212,"Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. Jaket: Joint pre-training of knowl-
edge graph and language understanding. ArXiv, abs/2010.00796, 2020."
REFERENCES,0.7947019867549668,"Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced
language representation with informative entities. In ACL, 2019."
REFERENCES,0.7980132450331126,Published as a conference paper at ICLR 2022
REFERENCES,0.8013245033112583,"A
ETHICS STATEMENT"
REFERENCES,0.804635761589404,"We outline potential ethical issues with our work below. First, GREASELM is a method to fuse
language representations and knowledge graph representations for effective reasoning about textual
situations. Consequently, GREASELM could reÔ¨Çect many of the same biases and toxic behaviors
exhibited by language models and knowledge graphs that are used to initialize it. For example,
prior large-scale language models have been shown to encode biases about race, gender, and other
demographic attributes (Sheng et al., 2020). Because GREASELM is seeded with pretrained lan-
guage models that often learn these patterns, it is possible to reÔ¨Çect them in open-world settings.
Second, the ConceptNet knowledge graph (Speer et al., 2017) used in this work has been shown to
encode stereotypes (Mehrabi et al., 2021), rather than completely clean commonsense knowledge.
If GREASELM were used outside these standard benchmarks in conjunction with ConceptNet as a
KG, it might rely on unethical relationships in its knowledge resource to arrive at conclusions. Con-
sequently, while GREASELM could be used for applications outside these standard benchmarks,
we would encourage implementers to use the same precautions they would apply to other language
models and methods that use noisy knowledge sources."
REFERENCES,0.8079470198675497,"Another source of ethical concern is the use of the MedQA-USMLE evaluation. While we Ô¨Ånd
clinical reasoning using language models and knowledge graphs to be an interesting testbed for
GREASELM and for joint language and reasoning models in general, we do not encourage users to
use these models for real world clinical prediction, particularly at these performance levels."
REFERENCES,0.8112582781456954,"B
EXPERIMENTAL SETUP DETAILS"
REFERENCES,0.8145695364238411,"B.1
ENTITY LINKING"
REFERENCES,0.8178807947019867,"Given each QA context, we follow the procedure from Yasunaga et al. (2021) to retrieve the subgraph
Gsub from G. First, we perform entity linking to G to retrieve an initial set of nodes Vlinked. Second,
we add any bridge entities that are in a 2-hop path between any pair of linked entities in Vlinked to get
the set of retrieved entities Vretrieved. Then we prune the set of nodes Vretrieved using a relevance score
computed for each node. To compute the relevance score, we follow the procedure of Yasunaga et al.
(2021) ‚Äì we concatenate the node name with the context of the QA example, and pass it through a
pre-trained LM, using the output score of the node name as the relevance score. We only retain the
top 200 scores nodes and prune the remaining ones. Finally, we retrieve all the edges that connect
any two nodes in Vsub, forming the retrieved subgraph Gsub. Each node in Gsub is assigned a type
according to whether its corresponding entity was linked from the context c, question q, answer a,
or from a bridge path."
REFERENCES,0.8211920529801324,"B.2
GRAPH INITIALIZATION"
REFERENCES,0.8245033112582781,"To compute initial node embeddings (¬ß3.3) for entities retrieved in Gsub from ConceptNet, we follow
the method of MHGRN (Feng et al., 2020). We convert knowledge triples in the KG into sentences
using pre-deÔ¨Åned templates for each relation. Then, these sentences are fed into a BERT-large LM
to compute embeddings for each sentence. Finally, for all sentences containing an entity, we extract
all token representations of the entity‚Äôs mention spans in these sentences, mean pool over these
representations and project this mean-pooled representation."
REFERENCES,0.8278145695364238,"For MedQA-USMLE, node embeddings are initialized similarly using the pooled token output em-
beddings of the entity name from the SapBERT model (described in ¬ß4.2; Liu et al., 2021). For
MedQA, 5% of examples do not yield a retrieved entity. In these cases, we represent the graph using
a dummy node initialized with 0. In essence, GreaseLM backs off to only using LM representations
as the graph propagates no information."
REFERENCES,0.8311258278145696,"B.3
HYPERPARAMETERS"
REFERENCES,0.8344370860927153,Published as a conference paper at ICLR 2022
REFERENCES,0.8377483443708609,Table 7: Hyperparameter settings for models and experiments
REFERENCES,0.8410596026490066,"Category
Hyperparameter
Dataset"
REFERENCES,0.8443708609271523,"CommonsenseQA
OpenbookQA
MedQA-USMLE"
REFERENCES,0.847682119205298,Model architecture
REFERENCES,0.8509933774834437,"Number of GREASELM layers M
5
6
3"
REFERENCES,0.8543046357615894,"Number of Unimodal LM layers N
19
18
9"
REFERENCES,0.8576158940397351,"Number of attention heads in GNN
2
2
2"
REFERENCES,0.8609271523178808,"Dimension of node embeddings and the messages in GNN
200
200
200"
REFERENCES,0.8642384105960265,"Dimension of MLP hidden layers (except MInt operator)
200
200
200"
REFERENCES,0.8675496688741722,"Number of hidden layers of MLPs
1
1
1"
REFERENCES,0.8708609271523179,"Dimension of MInt operator hidden layer
400
200
400"
REFERENCES,0.8741721854304636,"Regularization
Dropout rate of the embedding layer, GNN layers and fully-connected layers
0.2
0.2
0.2"
REFERENCES,0.8774834437086093,Optimization
REFERENCES,0.8807947019867549,"Learning rate of parameters in LM
1.00E-05
1.00E-05
5.00E-05"
REFERENCES,0.8841059602649006,"Learning rate of parameters not in LM
1.00E-03
1.00E-03
1.00E-03"
REFERENCES,0.8874172185430463,"Number of epochs in which LM‚Äôs parameters are kept frozen
4
4
0"
REFERENCES,0.890728476821192,"Optimizer
RAdam
RAdam
RAdam"
REFERENCES,0.8940397350993378,"Learning rate schedule
constant
constant
constant"
REFERENCES,0.8973509933774835,"Batch size
128
128
128"
REFERENCES,0.9006622516556292,"Number of epochs
30
70
20"
REFERENCES,0.9039735099337748,"Max gradient norm (gradient clipping)
1.0
1.0
1.0"
REFERENCES,0.9072847682119205,"Data
Max number of nodes
200
200
200"
REFERENCES,0.9105960264900662,"Max number of tokens
100
100
512"
REFERENCES,0.9139072847682119,"C
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.9172185430463576,"C.1
ABLATION STUDIES"
REFERENCES,0.9205298013245033,"In Table 8, we summarize an ablation study conducted using the CommonsenseQA IHdev set."
REFERENCES,0.9238410596026491,"Modality interaction. A key component of GREASELM is the connection of the LM to the GNN
via the modality interaction module (Eq. 11). If we remove modality interaction, the performance
drops signiÔ¨Åcantly, from 78.5% to 76.5% (approximately the performance of QA-GNN). Integrating
the modality interaction in every other layer instead of consecutive layers also hurts performance. A
possible explanation is that skipping layers could impede learning consistent representations across
layers for both the LM and the GNN, a property which may be desirable given we initialize the
model using a pretrained LM‚Äôs weights (e.g., RoBERTa). We also Ô¨Ånd that sharing parameters
between modality interaction layers (Eq. 11) outperforms not sharing, possibly because our datasets
are not very large (e.g., 10k for CommonsenseQA), and sharing parameters helps prevent overÔ¨Åtting."
REFERENCES,0.9271523178807947,"Table 8: Ablation study of our model components, using the CommonsenseQA IH-dev set."
REFERENCES,0.9304635761589404,"Ablation Type
Ablation
Dev Acc.
GREASELM
-
78.5"
REFERENCES,0.9337748344370861,"Modality Interaction
No interaction
76.5
Interaction in every other layer
76.3"
REFERENCES,0.9370860927152318,"Interaction Layer Parameter Sharing
No parameter sharing
77.1"
REFERENCES,0.9403973509933775,"Number of GREASELM layers (M)
M = 4
77.7
M = 6
78.0
M = 7
76.2"
REFERENCES,0.9437086092715232,"Graph Connectivity
Interaction node connected to all
77.6
nodes in Vsub, not only Vlinked"
REFERENCES,0.9470198675496688,"Node Initialization
Random
60.8
TransE (Bordes et al., 2013)
77.7"
REFERENCES,0.9503311258278145,"Number of GREASELM layers. We Ô¨Ånd that M = 5 GREASELM layers achieves the highest
performance. However, both the results for M = 4 and M = 6 are relatively close to the top
performance, indicating our method is not overly sensitive to this hyperparameter."
REFERENCES,0.9536423841059603,Published as a conference paper at ICLR 2022
REFERENCES,0.956953642384106,"Graph connectivity. The interaction node eint is a key component of GREASELM that bridges the
interaction between the KG and the text. Selecting which nodes in the KG are directly connected
to eint affects the rate at which information from different portions of the KG can reach the text
representations. We Ô¨Ånd that connecting eint KG nodes explicitly linked to the input text performs
best. Connecting eint to all nodes in the subgraph (e.g., bridge entities) hurts performance (-0.9%),
possibly because the interaction node is overloaded by having to attend to all nodes in the graph (up
to 200). By connecting the interaction node only to linked entities, each linked entity serves as a
Ô¨Ålter for relevant information that reaches the interaction node."
REFERENCES,0.9602649006622517,"KG node embedding initialization. Effectively initializing KG node representations is critical.
When we initialize nodes randomly instead of using the BERT-based initialization method from
Feng et al. (2020), the performance drops signiÔ¨Åcantly (78.5%‚Üí60.8%). While using standard KG
embeddings (e.g., TransE; Bordes et al., 2013) recovers much of the performance drop (77.7%), we
still Ô¨Ånd that using BERT-based entity embeddings performs best."
REFERENCES,0.9635761589403974,"C.2
EFFECT OF LM INITIALIZATION ON GREASELM"
REFERENCES,0.9668874172185431,"Table 9: Performance on the in-house splits of Com-
monsenseQA for different LM initializations of our
method, GREASELM."
REFERENCES,0.9701986754966887,"Methods
IHdev-Acc.
IHtest-Acc."
REFERENCES,0.9735099337748344,"ROBERTA-LARGE
73.1
68.7
+ GREASELM (Ours)
78.5
74.2"
REFERENCES,0.9768211920529801,"ROBERTA-BASE
65.1
59.8
+ GREASELM (Ours)
69.3
65.0"
REFERENCES,0.9801324503311258,"Table 10: Initialization on MedQA-
USMLE"
REFERENCES,0.9834437086092715,"Methods
Acc. (%)"
REFERENCES,0.9867549668874173,"SAPBERT-BASE
37.2
+ GREASELM (Ours)
38.5"
REFERENCES,0.9900662251655629,"BIOBERT-BASE
34.1
+ GREASELM (Ours)
34.6"
REFERENCES,0.9933774834437086,"PUBMEDBERT-BASE
38.0
+ GREASELM (Ours)
38.7"
REFERENCES,0.9966887417218543,"To evaluate whether our method is agnostic to the LM used to seed the GreaseLM layers, we replace
the LMs we use in previous experiments (RoBERTa-large for CommonsenseQA and SapBERT for
MedQA-USMLE) with RoBERTa-base for CommonsenseQA, and BioBERT and PubmedBERT
for MedQA-USMLE. Across multiple LM initializations in two domains, our results demonstrate
that GREASELM can provide a consistent improvement for multiple LMs when used as a modality
junction between KGs and language."
