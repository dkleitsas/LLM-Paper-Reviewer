Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011560693641618498,"We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method
for data augmentation that combines the best of interpolation based training and
noise injection schemes. Rather than training with convex combinations of pairs of
examples and their labels, we use noise-perturbed convex combinations of pairs
of data points in both input and feature space. This method includes mixup and
manifold mixup as special cases, but it has additional advantages, including better
smoothing of decision boundaries and enabling improved model robustness. We
provide theory to understand this as well as the implicit regularization effects of
NFM. Our theory is supported by empirical results, demonstrating the advantage
of NFM, as compared to mixup and manifold mixup. We show that residual
networks and vision transformers trained with NFM have favorable trade-offs
between predictive accuracy on clean data and robustness with respect to various
types of data perturbation across a range of computer vision benchmark datasets."
INTRODUCTION,0.0023121387283236996,"1
INTRODUCTION"
INTRODUCTION,0.003468208092485549,"Mitigating over-ﬁtting and improving generalization on test data are central goals in machine learning.
One approach to accomplish this is regularization, which can be either data-agnostic or data-dependent
(e.g., explicitly requiring the use of domain knowledge or data). Noise injection is a typical example
of data-agnostic regularization (Bishop, 1995), where noise can be injected into the input data
(An, 1996), or the activation functions (Gulcehre et al., 2016), or the hidden layers of deep neural
networks (Camuto et al., 2020; Lim et al., 2021). Data augmentation constitutes a different class
of regularization methods (Baird, 1992; Chapelle et al., 2001; DeCoste & Sch¨olkopf, 2002), which
can also be either data-agnostic or data-dependent. Data augmentation involves training a model
with not just the original data, but also with additional data that is properly transformed, and it has
led to state-of-the-art results in image recognition (Cires¸an et al., 2010; Krizhevsky et al., 2012).
The recently-proposed data-agnostic method, mixup (Zhang et al., 2017), trains a model on linear
interpolations of a random pair of examples and their corresponding labels, thereby encouraging
the model to behave linearly in-between training examples. Both noise injection and mixup have
been shown to impose smoothness and increase model robustness to data perturbations (Zhang
et al., 2020; Carratino et al., 2020; Lim et al., 2021), which is critical for many safety and sensitive
applications (Goodfellow et al., 2018; Madry et al., 2017)."
INTRODUCTION,0.004624277456647399,"In this paper, we propose and study a simple yet effective data augmentation method, which we call
Noisy Feature Mixup (NFM). This method combines mixup and noise injection, thereby inheriting
the beneﬁts of both methods, and it can be seen as a generalization of input mixup (Zhang et al.,
2017) and manifold mixup (Verma et al., 2019). When compared to noise injection and mixup, NFM
imposes regularization on the largest natural region surrounding the dataset (see Fig. 1), which may
help improve robustness and generalization when predicting on out of distribution data. Conveniently,
NFM can be implemented on top of manifold mixup, introducing minimal computation overhead."
INTRODUCTION,0.005780346820809248,∗Equal contribution
INTRODUCTION,0.006936416184971098,"Published as a conference paper at ICLR 2022 x1
x2"
INTRODUCTION,0.008092485549132947,"λx1 + (1 −λ)x2 x′
1"
INTRODUCTION,0.009248554913294798,"x′
2
λx′
1 + (1 −λ)x′
2"
INTRODUCTION,0.010404624277456647,"Figure 1:
An illustration of how two
data points, x1 and x2, are transformed
in mixup (top) and noisy feature mixup
(NFM) with S := {0} (bottom)."
INTRODUCTION,0.011560693641618497,"Contributions.
Our main contributions are as follows."
INTRODUCTION,0.012716763005780347,"• We study NFM via the lens of implicit regulariza-
tion, showing that NFM ampliﬁes the regularizing
effects of manifold mixup and noise injection, im-
plicitly reducing the feature-output Jacobians and
Hessians according to the mixing level and noise
levels (see Theorem 1).
• We provide mathematical analysis to show that
NFM can improve model robustness when com-
pared to manifold mixup and noise injection. In
particular, we show that, under appropriate assump-
tions, NFM training approximately minimizes an
upper bound on the sum of an adversarial loss and
feature-dependent regularizers (see Theorem 2)."
INTRODUCTION,0.013872832369942197,"• We provide empirical results in support of our theoretical ﬁndings, showing that NFM improves
robustness with respect to various forms of data perturbation across a wide range of state-of-the-
art architectures on computer vision benchmark tasks."
INTRODUCTION,0.015028901734104046,"In the Supplementary Materials (SM), we provide proofs for our theorems along with additional
theoretical and empirical results to gain more insights into NFM. In particular, we show that NFM
can implicitly increase classiﬁcation margin (see Proposition 1 in SM C) and the noise injection
procedure in NFM can robustify manifold mixup in a probabilistic sense (see Theorem 5 in SM D).
We also provide and discuss generalization bounds for NFM (see Theorem 6 and 7 in SM E)."
INTRODUCTION,0.016184971098265895,"Notation. I denotes identity matrix, [K] := {1, . . . , K}, the superscript T denotes transposition, ◦
denotes composition, ⊙denotes Hadamard product, 1 denotes the vector with all components equal
one. For a vector v, vk denotes its kth component and ∥v∥p denotes its lp norm for p > 0. conv(X)
denote the convex hull of X. Mλ(a, b) := λa + (1 −λ)b, for random variables a, b, λ. δz denotes the
Dirac delta function, deﬁned as δz(x) = 1 if x = z and δz(x) = 0 otherwise. 1A denotes indicator
function of the set A. For α, β > 0, ˜Dλ :=
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α) denotes
a uniform mixture of two Beta distributions. For two vectors a, b, cos(a, b) := ⟨a, b⟩/∥a∥2∥b∥2
denotes their cosine similarity. N(a, b) is a Gaussian distribution with mean a and covariance b."
RELATED WORK,0.017341040462427744,"2
RELATED WORK"
RELATED WORK,0.018497109826589597,"Regularization. Regularization refers to any technique that reduces overﬁtting in machine learning;
see (Mahoney & Orecchia, 2011; Mahoney, 2012) and references therein, in particular for a discussion
of implicit regularization, a topic that has received attention recently in the context of stochastic
gradient optimization applied to neural network models. Traditional regularization techniques such
as ridge regression, weight decay and dropout do not make use of the training data to reduce the
model capacity. A powerful class of techniques is data augmentation, which constructs additional
examples from the training set, e.g., by applying geometric transformations to the original data
(Shorten & Khoshgoftaar, 2019). A recently proposed technique is mixup (Zhang et al., 2017), where
the examples are created by taking convex combinations of pairs of inputs and their labels. Verma
et al. (2019) extends mixup to hidden representations in deep neural networks. Subsequent works
by Greenewald et al. (2021); Yin et al. (2021); Engstrom et al. (2019); Kim et al. (2020a); Yun et al.
(2019); Hendrycks et al. (2019) introduce different variants and extensions of mixup. Regularization
is also intimately connected to robustness (Hoffman et al., 2019; Sokoli´c et al., 2017; Novak et al.,
2018; Elsayed et al., 2018; Moosavi-Dezfooli et al., 2019). Adding to the list is NFM, a powerful
regularization method that we propose to improve model robustness."
RELATED WORK,0.019653179190751446,"Robustness. Model robustness is an increasingly important issue in modern machine learning.
Robustness with respect to adversarial examples (Kurakin et al., 2016) can be achieved by adversarial
training (Goodfellow et al., 2014; Madry et al., 2017; Utrera et al., 2020). Several works present
theoretical justiﬁcations to observed robustness and how data augmentation can improve it (Hein &
Andriushchenko, 2017; Yang et al., 2020b; Couellan, 2021; Pinot et al., 2019a; 2021; Zhang et al.,
2020; 2021; Carratino et al., 2020; Kimura, 2020; Dao et al., 2019; Wu et al., 2020; Gong et al.,
2020; Chen et al., 2020). Relatedly, Fawzi et al. (2016); Franceschi et al. (2018); Lim et al. (2021)"
RELATED WORK,0.020809248554913295,Published as a conference paper at ICLR 2022
RELATED WORK,0.021965317919075144,"investigate how noise injection can be used to improve robustness. Parallel to this line of work, we
provide theory to understand how NFM can improve robustness. Also related is the study of the
trade-offs between robustness and accuracy (Min et al., 2020; Zhang et al., 2019; Tsipras et al., 2018;
Schmidt et al., 2018; Su et al., 2018; Raghunathan et al., 2020; Yang et al., 2020a)."
NOISY FEATURE MIXUP,0.023121387283236993,"3
NOISY FEATURE MIXUP"
NOISY FEATURE MIXUP,0.024277456647398842,"Noisy Feature Mixup is a generalization of input mixup (Zhang et al., 2017) and manifold mixup
(Verma et al., 2019). The main novelty of NFM against manifold mixup lies in the injection of noise
when taking convex combinations of pairs of input and hidden layer features. Fig. 1 illustrates, at
a high level, how this modiﬁcation alters the region in which the resulting augmented data resides.
Fig. 2 shows that NFM is most effective at smoothing the decision boundary of the trained classiﬁers;
compared to noise injection and mixup alone, it imposes the strongest smoothness on this dataset."
NOISY FEATURE MIXUP,0.025433526011560695,"Formally, we consider multi-class classiﬁcation with K labels. Denote the input space by X ⊂Rd
and the output space by Y = RK. The classiﬁer, g, is constructed from a learnable map f : X →RK,
mapping an input x to its label, g(x) = arg maxk f k(x) ∈[K]. We are given a training set,
Zn := {(xi, yi)}n
i=1, consisting of n pairs of input and one-hot label, with each training pair
zi := (xi, yi) ∈X × Y drawn i.i.d. from a ground-truth distribution D. We consider training a deep
neural network f := fk ◦gk, where gk : X →gk(X) maps an input to a hidden representation at
layer k, and fk : gk(X) →gL(X) := Y maps the hidden representation to a one-hot label at layer L.
Here, gk(X) ⊂Rdk for k ∈[L], dL := K, g0(x) = x and f0(x) = f(x)."
NOISY FEATURE MIXUP,0.026589595375722544,Training f using NFM consists of the following steps:
NOISY FEATURE MIXUP,0.027745664739884393,"1. Select a random layer k from a set, S ⊂{0} ∪[L], of eligible layers in the neural network.
2. Process two random data minibatches (x, y) and (x′, y′) as usual, until reaching layer k. This
gives us two immediate minibatches (gk(x), y) and (gk(x′), y′).
3. Perform mixup on these intermediate minibatches, producing the mixed minibatch:"
NOISY FEATURE MIXUP,0.028901734104046242,"(˜gk, ˜y) := (Mλ(gk(x), gk(x′)), Mλ(y, y′)),
(1)"
NOISY FEATURE MIXUP,0.03005780346820809,"where the mixing level λ ∼Beta(α, β), with the hyper-parameters α, β > 0.
4. Produce noisy mixed minibatch by injecting additive and multiplicative noise:"
NOISY FEATURE MIXUP,0.03121387283236994,"(˜˜gk, ˜y) := ((1 + σmultξmult
k
) ⊙Mλ(gk(x), gk(x′)) + σaddξadd
k
, Mλ(y, y′)),
(2)"
NOISY FEATURE MIXUP,0.03236994219653179,"where the ξadd
k
and ξmult
k
are Rdk-valued independent random variables modeling the additive
and multiplicative noise respectively, and σadd, σmult ≥0 are pre-speciﬁed noise levels."
NOISY FEATURE MIXUP,0.03352601156069364,"5. Continue the forward pass from layer k until the output using the noisy mixed minibatch (˜˜gk, ˜y).
6. Compute the loss and gradients that update all the parameters of the network."
NOISY FEATURE MIXUP,0.03468208092485549,"Baseline (85.5%).
Dropout (87.0%).
Weight decay (88.0%).
Noise injections (87.0%)."
NOISY FEATURE MIXUP,0.035838150289017344,"Mixup (84.5%).
Manifold mixup (88.5%).
Noisy mixup (89.0%).
NFM (90.0%).
Figure 2: The decision boundaries and test accuracy (in parenthesis) for different training schemes on
a toy dataset in binary classiﬁcation (see Subsection F.2 for details)."
NOISY FEATURE MIXUP,0.03699421965317919,Published as a conference paper at ICLR 2022
NOISY FEATURE MIXUP,0.03815028901734104,"At the level of implementation, following (Verma et al., 2019), we backpropagate gradients through
the entire computational graph, including those layers before the mixup layer k."
NOISY FEATURE MIXUP,0.03930635838150289,"In the case where σadd = σmult = 0, NFM reduces to manifold mixup (Verma et al., 2019). If in
addition S = {0}, it reduces to the original mixup method (Zhang et al., 2017). The main difference
between NFM and manifold mixup lies in the noise injection of the fourth step above. Note that
NFM is equivalent to injecting noise into gk(x), gk(x′) ﬁrst, then performing mixup on the resulting
pair, i.e., the order that the third and fourth steps occur does not change the resulting noisy mixed
minibatch. For simplicity, we have used the same mixing level, noise distribution, and noise levels
for all layers in S in our formulation."
NOISY FEATURE MIXUP,0.04046242774566474,"Within the above setting, we consider the expected NFM loss:"
NOISY FEATURE MIXUP,0.04161849710982659,"LNF M(f) = E(x,y),(x′,y′)∼DEk∼SEλ∼Beta(α,β)Eξk∼Ql(fk(Mλ,ξk(gk(x), gk(x′))), Mλ(y, y′)),"
NOISY FEATURE MIXUP,0.04277456647398844,"where l : RK × RK →[0, ∞) is a loss function (note that here we have suppressed the dependence
of both l and f on the learnable parameter θ in the notation), ξk := (ξadd
k
, ξmult
k
) are drawn from
some probability distribution Q with ﬁnite ﬁrst two moments, and"
NOISY FEATURE MIXUP,0.04393063583815029,"Mλ,ξk(gk(x), gk(x′)) := (1 + σmultξmult
k
) ⊙Mλ(gk(x), gk(x′)) + σaddξadd
k
."
NOISY FEATURE MIXUP,0.04508670520231214,"NFM seeks to minimize a stochastic approximation of LNF M(f) by sampling a ﬁnite number of
k, λ, ξk values and using minibatch gradient descent to minimize this loss approximation."
THEORY,0.046242774566473986,"4
THEORY"
THEORY,0.047398843930635835,"In this section, we provide mathematical analysis to understand NFM. We begin with formulating
NFM in the framework of vicinal risk minimization and interpreting NFM as a stochastic learning
strategy in Subsection 4.1. Next, we study NFM via the lens of implicit regularization in Subsection
4.2. Our key contribution is Theorem 1, which shows that minimizing the NFM loss function is
approximately equivalent to minimizing a sum of the original loss and feature-dependent regularizers,
amplifying the regularizing effects of manifold mixup and noise injection according to the mixing and
noise levels. In Subsection 4.3, we focus on demonstrating how NFM can enhance model robustness
via the lens of distributionally robust optimization. The key result of Theorem 2 shows that NFM
loss is approximately the upper bound on a regularized version of an adversarial loss, and thus
training with NFM not only improves robustness but can also mitigate robust over-ﬁtting, a dominant
phenomenon where the robust test accuracy starts to decrease during training (Rice et al., 2020)."
THEORY,0.048554913294797684,"4.1
NFM: BEYOND EMPIRICAL RISK MINIMIZATION"
THEORY,0.04971098265895954,"The standard approach in statistical learning theory (Bousquet et al., 2003) is to select a hypothesis
function f : X →Y from a pre-deﬁned hypothesis class F to minimize the expected risk with
respect to D and to solve the risk minimization problem: inff∈F R(f) := E(x,y)∼D[l(f(x), y)], for
a suitable choice of loss function l. In practice, we do not have access to the ground-truth distribution.
Instead, we ﬁnd an approximate solution by solving the empirical risk minimization (ERM) problem,
in which case D is approximated by the empirical distribution Pn = 1"
THEORY,0.05086705202312139,"n
Pn
i=1 δzi. In other words, in
ERM we solve the problem: inff∈F Rn(f) := 1"
THEORY,0.05202312138728324,"n
Pn
i=1 l(f(xi), yi)."
THEORY,0.05317919075144509,"However, when the training set is small or the model capacity is large (as is the case for deep neural
networks), ERM may suffer from overﬁtting. Vicinal risk minimization (VRM) is a data augmentation
principle introduced in (Vapnik, 2013) that goes beyond ERM, aiming to better estimate expected
risk and reduce overﬁtting. In VRM, a model is trained not simply on the training set, but on samples
drawn from a vicinal distribution, that smears the training data to their vicinity. With appropriate
choices for this distribution, the VRM approach has resulted in several effective regularization
schemes (Chapelle et al., 2001). Input mixup (Zhang et al., 2017) can be viewed as an example of
VRM, and it turns out that NFM can be constructed within a VRM framework at the feature level (see
Section A in SM). On a high level, NFM can be interpreted as a random procedure that introduces
feature-dependent noise into the layers of the deep neural network. Since the noise injections are
applied only during training and not inference, NFM is an instance of a stochastic learning strategy.
Note that the injection strategy of NFM differs from those of An (1996); Camuto et al. (2020); Lim"
THEORY,0.05433526011560694,Published as a conference paper at ICLR 2022
THEORY,0.055491329479768786,"et al. (2021). Here, the structure of the injected noise differs from iteration to iteration (based on the
layer chosen) and depends on the training data in a different way. We expect NFM to amplify the
beneﬁts of training using either noise injection or mixup alone, as will be shown next."
IMPLICIT REGULARIZATION OF NFM,0.056647398843930635,"4.2
IMPLICIT REGULARIZATION OF NFM"
IMPLICIT REGULARIZATION OF NFM,0.057803468208092484,"We consider loss functions of the form l(f(x), y) := h(f(x)) −yf(x), which includes standard
choices such as the logistic loss and the cross-entropy loss, and recall that f := fk ◦gk. Denote
Lstd
n
:= 1"
IMPLICIT REGULARIZATION OF NFM,0.058959537572254334,"n
Pn
i=1 l(f(xi), yi) and let Dx be the empirical distribution of training samples {xi}i∈[n].
We shall show that NFM exhibits a natural form of implicit regularization, i.e., regularization imposed
implicitly by the stochastic learning strategy, without explicitly modifying the loss."
IMPLICIT REGULARIZATION OF NFM,0.06011560693641618,"Let ϵ > 0 be a small parameter. In the sequel, we rescale 1 −λ 7→ϵ(1 −λ), σadd 7→ϵσadd,
σmult 7→ϵσmult, and denote ∇kf and ∇2
kf as the ﬁrst and second directional derivative of fk with
respect to gk respectively, for k ∈S. By working in the small parameter regime, we can relate the
NFM empirical loss LNF M
n
to the original loss Lstd
n
and identify the regularizing effects of NFM.
Theorem 1. Let ϵ > 0 be a small parameter, and assume that h and f are twice differentiable. Then,
LNF M
n
= Ek∼SLNF M(k)
n
, where"
IMPLICIT REGULARIZATION OF NFM,0.06127167630057803,"LNF M(k)
n
= Lstd
n
+ ϵR(k)
1
+ ϵ2 ˜R(k)
2
+ ϵ2 ˜R(k)
3
+ ϵ2ϕ(ϵ),
(3)"
IMPLICIT REGULARIZATION OF NFM,0.06242774566473988,"with ˜R(k)
2
= R(k)
2 +σ2
addRadd(k)
2
+σ2
multRmult(k)
2
and ˜R(k)
3
= R(k)
3 +σ2
addRadd(k)
3
+σ2
multRmult(k)
3
,
where"
IMPLICIT REGULARIZATION OF NFM,0.06358381502890173,"Radd(k)
2
= 1"
N,0.06473988439306358,"2n n
X"
N,0.06589595375722543,"i=1
h′′(f(xi))∇kf(gk(xi))T Eξk[ξadd
k
(ξadd
k
)T ]∇kf(gk(xi)),
(4)"
N,0.06705202312138728,"Rmult(k)
2
= 1"
N,0.06820809248554913,"2n n
X"
N,0.06936416184971098,"i=1
h′′(f(xi))∇kf(gk(xi))T (Eξk[ξmult
k
(ξmult
k
)T ] ⊙gk(xi)gk(xi)T )∇kf(gk(xi)), (5)"
N,0.07052023121387284,"Radd(k)
3
= 1"
N,0.07167630057803469,"2n n
X"
N,0.07283236994219654,"i=1
(h′(f(xi)) −yi)Eξk[(ξadd
k
)T ∇2
kf(gk(xi))ξadd
k
],
(6)"
N,0.07398843930635839,"Rmult(k)
3
= 1"
N,0.07514450867052024,"2n n
X"
N,0.07630057803468208,"i=1
(h′(f(xi)) −yi)Eξk[(ξmult
k
⊙gk(xi))T ∇2
kf(gk(xi))(ξmult
k
⊙gk(xi))].
(7)"
N,0.07745664739884393,"Here, Rk
1, Rk
2 and Rk
3 are the regularizers associated with the loss of manifold mixup (see Theorem 3
in SM for their explicit expression), and ϕ is some function such that limϵ→0 ϕ(ϵ) = 0."
N,0.07861271676300578,"Theorem 1 implies that, when compared to manifold mixup, NFM introduces additional smoothness,
regularizing the directional derivatives, ∇kf(gk(xi)) and ∇2
kf(gk(xi)), with respect to gk(xi),
according to the noise levels σadd and σmult, and amplifying the regularizing effects of manifold
mixup and noise injection. In particular, making ∇2f(xi) small can lead to smooth decision
boundaries (at the input level), while reducing the conﬁdence of model predictions. On the other hand,
making the ∇kf(gk(xi)) small can lead to improvement in model robustness, which we discuss next."
ROBUSTNESS OF NFM,0.07976878612716763,"4.3
ROBUSTNESS OF NFM"
ROBUSTNESS OF NFM,0.08092485549132948,"We show that NFM improves model robustness. We do this by considering the following three lenses:
(1) implicit regularization and classiﬁcation margin; (2) distributionally robust optimization; and (3)
a probabilistic notion of robustness. We focus on (2) in the main paper. See Section C-D in SM and
the last paragraph in this subsection for details on (1) and (3)."
ROBUSTNESS OF NFM,0.08208092485549133,"We now demonstrate how NFM helps adversarial robustness. By extending the analysis of Zhang
et al. (2017); Lamb et al. (2019), we can relate the NFM loss function to the one used for adversarial
training, which can be viewed as an instance of distributionally robust optimization (DRO) (Kwon
et al., 2020; Kuhn et al., 2019; Rahimian & Mehrotra, 2019) (see also Proposition 3.1 in (Staib &
Jegelka, 2017)). DRO provides a framework for local worst-case risk minimization, minimizing
supremum of the risk in an ambiguity set, such as in the vicinity of the empirical data distribution."
ROBUSTNESS OF NFM,0.08323699421965318,Published as a conference paper at ICLR 2022
ROBUSTNESS OF NFM,0.08439306358381503,"Following (Lamb et al., 2019), we consider the binary cross-entropy loss, setting h(z) = log(1 + ez),
with the labels y taking value in {0, 1} and the classiﬁer model f : Rd →R. In the following, we
assume that the model parameter θ ∈Θ := {θ : yif(xi) + (yi −1)f(xi) ≥0 for all i ∈[n]}. Note
that this set contains the set of all parameters with correct classiﬁcations of training samples (before
applying NFM), since {θ : 1{f(xi)≥0} = yi for all i ∈[n]} ⊂Θ. Therefore, the condition of θ ∈Θ
is satisﬁed when the model classiﬁes all labels correctly for the training data before applying NFM.
Since, in practice, the training error often becomes zero in ﬁnite time, we study the effect of NFM on
model robustness in the regime of θ ∈Θ."
ROBUSTNESS OF NFM,0.08554913294797688,"Working in the data-dependent parameter space Θ, we have the following result.
Theorem 2. Let θ ∈Θ := {θ : yif(xi) + (yi −1)f(xi) ≥0 for all i ∈[n]} such that ∇kf(gk(xi))
and ∇2
kf(gk(xi)) exist for all i ∈[n], k ∈S. Assume that fk(gk(xi)) = ∇kf(gk(xi))T gk(xi),
∇2
kf(gk(xi)) = 0 for all i ∈[n], k ∈S. In addition, suppose that ∥∇f(xi)∥2 > 0 for all i ∈[n],
Er∼Dx[gk(r)] = 0 and ∥gk(xi)∥2 ≥c(k)
x
√dk for all i ∈[n], k ∈S. Then,"
ROBUSTNESS OF NFM,0.08670520231213873,"LNF M
n
≥1 n n
X"
ROBUSTNESS OF NFM,0.08786127167630058,"i=1
max
∥δi∥2≤ϵmix
i
l(f(xi + δi), yi) + Lreg
n
+ ϵ2φ(ϵ),
(8)"
ROBUSTNESS OF NFM,0.08901734104046242,"where
ϵmix
i
:=
ϵEλ∼˜
Dλ[1 −λ] · Ek∼S
h
r(k)
i
c(k)
x
∥∇kf(gk(xi))∥2"
ROBUSTNESS OF NFM,0.09017341040462427,"∥∇f(xi)∥2
√dk
i
and
Lreg
n
:="
ROBUSTNESS OF NFM,0.09132947976878612,"1
2n
Pn
i=1 |h′′(f(xi))|(ϵreg
i
)2, with r(k)
i
:= | cos(∇kf(gk(xi)), gk(xi))| and"
ROBUSTNESS OF NFM,0.09248554913294797,"(ϵreg
i
)2 := ϵ2∥∇kf(gk(xi))∥2
2"
ROBUSTNESS OF NFM,0.09364161849710982,"
Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2 cos(∇kf(gk(xi)), gk(xr))2]"
ROBUSTNESS OF NFM,0.09479768786127167,"+ σ2
addEξk[∥ξadd
k
∥2
2 cos(∇kf(gk(xi)), ξadd
k
)2]"
ROBUSTNESS OF NFM,0.09595375722543352,"+ σ2
multEξk[∥ξmult
k
⊙gk(xi)∥2
2 cos(∇kf(gk(xi)), ξmult
k
⊙gk(xi))2]

,
(9)"
ROBUSTNESS OF NFM,0.09710982658959537,and φ is some function such that limϵ→0 φ(ϵ) = 0.
ROBUSTNESS OF NFM,0.09826589595375723,"The second assumption stated in Theorem 2 is similar to the one made in Lamb et al. (2019); Zhang
et al. (2020), and is satisﬁed by linear models and deep neural networks with ReLU activation function
and max-pooling. Theorem 2 shows that the NFM loss is approximately an upper bound of the
adversarial loss with l2 attack of size ϵmix = mini∈[n] ϵmix
i
, plus a feature-dependent regularization
term Lreg
n
(see SM for further discussions). Therefore, we see that minimizing the NFM loss not
only results in a small adversarial loss, while retaining the robustness beneﬁts of manifold mixup, but
it also imposes additional smoothness, due to noise injection, on the adversarial loss. The latter can
help mitigate robust overﬁtting and improve test performance (Rice et al., 2020; Rebufﬁet al., 2021)."
ROBUSTNESS OF NFM,0.09942196531791908,"NFM can also implicitly increase the classiﬁcation margin (see Section C of SM). Moreover, since
the main novelty of NFM lies in the introduction of noise injection, it would be insightful to isolate
the robustness boosting beneﬁts of injecting noise on top of manifold mixup. We demonstrate these
advantages via the lens of probabilistic robustness in Section D of SM."
EMPIRICAL RESULTS,0.10057803468208093,"5
EMPIRICAL RESULTS"
EMPIRICAL RESULTS,0.10173410404624278,"In this section, we study the test performance of models trained with NFM, and examine to what
extent NFM can improve robustness to input perturbations. We demonstrate the tradeoff between
predictive accuracy on clean and perturbed test sets. We consider input perturbations that are common
in the literature: (a) white noise; (b) salt and pepper; and (c) adversarial perturbations (see Section F)."
EMPIRICAL RESULTS,0.10289017341040463,"We evaluate the average performance of NFM with different model architectures on CIFAR-
10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), ImageNet (Deng et al., 2009), and CIFAR-
10c (Hendrycks & Dietterich, 2019). We use a pre-activated residual network (ResNet) with depth
18 (He et al., 2016) on small scale tasks. For more challenging tasks, we consider the performance of
wide ResNet-18 (Zagoruyko & Komodakis, 2016) and ResNet-50 architectures, respectively."
EMPIRICAL RESULTS,0.10404624277456648,"Baselines. We evaluate against related data augmentation schemes that have shown performance
improvements in recent years: mixup (Zhang et al., 2017); manifold mixup (Verma et al., 2019);"
EMPIRICAL RESULTS,0.10520231213872833,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.10635838150289018,"cutmix (Yun et al., 2019); puzzle mixup (Kim et al., 2020b); and noisy mixup (Yang et al., 2020b).
Further, we compare to vanilla models trained without data augmentation (baseline), models trained
with label smoothing, and those trained on white noise perturbed inputs."
EMPIRICAL RESULTS,0.10751445086705202,"Experimental details. All hyperparameters are consistent with those of the baseline model across
the ablation experiments. In the models trained on the different data augmentation schemes, we keep
α ﬁxed, i.e., the parameter deﬁning Beta(α, α), from which the λ parameter controlling the convex
combination between data point pairs is sampled. Across all models trained with NFM, we control
the level of noise injections by ﬁxing the additive noise level to σadd = 0.4 and multiplicative noise
to σmult = 0.2. To demonstrate the signiﬁcant improvements on robustness upon the introduction of
these small input perturbations, we show a second model (‘*’) that was injected with higher noise
levels (i.e., σadd = 1.0, σmult = 0.5). See SM (Section F.5) for further details and comparisons
against NFM models trained on various other levels of noise injections."
EMPIRICAL RESULTS,0.10867052023121387,"5.1
CIFAR10"
EMPIRICAL RESULTS,0.10982658959537572,"Pre-activated ResNet-18. Table 1 summarizes the performance improvements and indicates a
consistent robustness across different α values. The model trained with NFM outperforms the
baseline model on the clean test set, while being more robust to input perturbations (Fig. 3; left). This
advantage is also displayed in the models trained with mixup and manifold mixup, though in a less
pronounced way. Notably, the NFM model is also robust to salt and pepper perturbations and could
be signiﬁcantly more so by further increasing the noise levels (Fig. 3; right)."
EMPIRICAL RESULTS,0.11098265895953757,"5.2
CIFAR-100"
EMPIRICAL RESULTS,0.11213872832369942,"Wide ResNet-18. Previous work indicates that data augmentation has a positive effect on performance
for this dataset (Zhang et al., 2017). Fig. 4 (left) conﬁrms that mixup and manifold mixup improve
the generalization performance on clean data and highlights the advantage of data augmentation.
The NFM training scheme is also capable of further improving the generalization performance. In"
EMPIRICAL RESULTS,0.11329479768786127,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
75 80 85 90 95"
EMPIRICAL RESULTS,0.11445086705202312,"Baseline
Mixup
CutMix
PuzzleMix
Noisy Mixup
Manifold Mixup
NFM (*)
NFM"
EMPIRICAL RESULTS,0.11560693641618497,Test Accuracy
EMPIRICAL RESULTS,0.11676300578034682,White Noise (σ)
EMPIRICAL RESULTS,0.11791907514450867,"0.00
0.02
0.04
0.06
0.08
65 70 75 80 85 90 95"
EMPIRICAL RESULTS,0.11907514450867052,"Baseline
Mixup
CutMix
PuzzleMix
Noisy Mixup
Manifold Mixup
NFM (*)
NFM"
EMPIRICAL RESULTS,0.12023121387283237,Salt and Pepper Noise (γ)
EMPIRICAL RESULTS,0.12138728323699421,"Figure 3: Pre-actived ResNet-18 evaluated on CIFAR-10 with different training schemes. Shaded
regions indicate one standard deviation about the mean. Averaged across 5 random seeds."
EMPIRICAL RESULTS,0.12254335260115606,"Table 1: Robustness of ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-10. The results are averaged over 5 models trained with different seed values."
EMPIRICAL RESULTS,0.12369942196531791,"Scheme
Clean (%)
σ (%)
γ (%)
0.1
0.2
0.3
0.02
0.04
0.1"
EMPIRICAL RESULTS,0.12485549132947976,"Baseline
94.6
90.4
76.7
56.3
86.3
76.1
55.2
Baseline + Noise
94.4
94.0
87.5
71.2
89.3
82.5
64.9
Baseline + Label Smoothing
95.0
91.3
77.5
56.9
87.7
79.2
60.0"
EMPIRICAL RESULTS,0.1260115606936416,"Mixup (α = 1.0) Zhang et al. (2017)
95.6
93.2
85.4
71.8
87.1
76.1
55.2
CutMix Yun et al. (2019)
96.3
86.7
60.8
32.4
90.9
81.7
54.7
PuzzleMix Kim et al. (2020b)
96.3
91.7
78.1
59.9
91.4
81.8
54.4
Manifold Mixup (α = 1.0) Verma et al. (2019)
95.7
92.7
82.7
67.6
88.9
80.2
57.6
Noisy Mixup (α = 1.0) Yang et al. (2020b)
78.9
78.6
66.6
46.7
66.6
53.4
25.9"
EMPIRICAL RESULTS,0.12716763005780346,"Noisy Feature Mixup (α = 1.0)
95.4
95.0
91.6
83.0
91.9
87.4
73.3"
EMPIRICAL RESULTS,0.1283236994219653,Published as a conference paper at ICLR 2022
EMPIRICAL RESULTS,0.12947976878612716,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
40 50 60 70 80"
EMPIRICAL RESULTS,0.130635838150289,"Baseline
Mixup
CutMix
PuzzleMix
Noisy Mixup
Manifold Mixup
NFM (*)
NFM"
EMPIRICAL RESULTS,0.13179190751445086,Test Accuracy
EMPIRICAL RESULTS,0.1329479768786127,White Noise (σ)
EMPIRICAL RESULTS,0.13410404624277455,"0.00
0.02
0.04
0.06
0.08
40 50 60 70 80"
EMPIRICAL RESULTS,0.1352601156069364,Salt and Pepper Noise (γ)
EMPIRICAL RESULTS,0.13641618497109825,Figure 4: Wide ResNets evaluated on CIFAR-100. Averaged across 5 random seeds.
EMPIRICAL RESULTS,0.1375722543352601,"Table 2: Robustness of Wide-ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-100. The results are averaged over 5 models trained with different seed values."
EMPIRICAL RESULTS,0.13872832369942195,"Scheme
Clean (%)
σ (%)
γ (%)
0.1
0.2
0.3
0.02
0.04
0.1"
EMPIRICAL RESULTS,0.13988439306358383,"Baseline
76.9
64.6
42.0
23.5
58.1
39.8
15.1
Baseline + Noise
76.1
75.2
60.5
37.6
64.9
51.3
23.0"
EMPIRICAL RESULTS,0.14104046242774568,"Mixup (α = 1.0) Zhang et al. (2017)
80.3
72.5
54.0
33.4
62.5
43.8
16.2
CutMix Yun et al. (2019)
77.8
58.3
28.1
13.8
70.3
58.
24.8
PuzzleMix (200 epochs) Kim et al. (2020b)
78.6
66.2
41.1
22.6
69.4
56.3
23.3
PuzzleMix (1200 epochs) Kim et al. (2020b)
80.3
53.0
19.1
6.2
69.3
51.9
15.7
Manifold Mixup (α = 1.0) Verma et al. (2019)
79.7
70.5
45.0
23.8
62.1
42.8
14.8
Noisy Mixup (α = 1.0) Yang et al. (2020b)
78.9
78.6
66.6
46.7
66.6
53.4
25.9"
EMPIRICAL RESULTS,0.14219653179190753,"Noisy Feature Mixup (α = 1.0)
80.9
80.1
72.1
55.3
72.8
62.1
34.4"
EMPIRICAL RESULTS,0.14335260115606938,"Table 3: Robustness of ResNet-50 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on ImageNet. Here, the NFM training scheme improves both the predictive accuracy on
clean data and robustness with respect to data perturbations."
EMPIRICAL RESULTS,0.14450867052023122,"Scheme
Clean (%)
σ (%)
γ (%)
0.1
0.25
0.5
0.06
0.1
0.15"
EMPIRICAL RESULTS,0.14566473988439307,"Baseline
76.0
73.5
67.0
50.1
53.2
50.4
45.0
Manifold Mixup (α = 0.2) Verma et al. (2019)
76.7
74.9
70.3
57.5
58.1
54.6
49.5"
EMPIRICAL RESULTS,0.14682080924855492,"Noisy Feature Mixup (α = 0.2)
77.0
76.5
72.0
60.1
58.3
56.0
52.3
Noisy Feature Mixup (α = 1.0)
76.8
76.2
71.7
60.0
60.9
58.8
54.4"
EMPIRICAL RESULTS,0.14797687861271677,"addition, we see that the model trained with NFM is less sensitive to both white noise and salt and
pepper perturbations. These results are surprising, as robustness is often thought to be at odds with
accuracy (Tsipras et al., 2018). However, we demonstrate NFM has the ability to improve both
accuracy and robustness. Table 2 indicates that for the same α, NFM can achieve an average test
accuracy of 80.9% compared to only 80.3% in the mixup setting."
IMAGENET,0.14913294797687862,"5.3
IMAGENET"
IMAGENET,0.15028901734104047,"ResNet-50. Table 3 similarly shows that NFM improves both the generalization and robustness
capacities with respect to data perturbations. Although less pronounced in comparison to previous
datasets, NFM shows a favorable trade-off without requiring additional computational resources.
Note that due to computational costs, we do not average across multiple seeds and only compare
NFM to the baseline and manifold mixup models."
IMAGENET,0.15144508670520232,"5.4
CIFAR-10C"
IMAGENET,0.15260115606936417,"In Figure 6 we use the CIFAR-10C dataset (Hendrycks & Dietterich, 2019) to demonstrate that models
trained with NFM are more robust to a range of perturbations on natural images. Figure 6 (left) shows"
IMAGENET,0.15375722543352602,Published as a conference paper at ICLR 2022
IMAGENET,0.15491329479768787,"0.00
0.02
0.04
0.06
0.08
0.10
0.12
50 60 70 80 90"
IMAGENET,0.15606936416184972,"Baseline
Mixup
CutMix
PuzzleMix
Noisy Mixup
Manifold Mixup
NFM (*)
NFM"
IMAGENET,0.15722543352601157,Test Accuracy
IMAGENET,0.15838150289017341,Adverserial Noise (ϵ)
IMAGENET,0.15953757225433526,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
20 30 40 50 60 70 80"
IMAGENET,0.1606936416184971,Adverserial Noise (ϵ)
IMAGENET,0.16184971098265896,"Figure 5: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated on
CIFAR-100 (right) with respect to adversarially perturbed inputs."
IMAGENET,0.1630057803468208,"1
2
3
4
5 40 50 60 70 80 90"
"BASELINE
CUTMIX",0.16416184971098266,"100
Baseline
CutMix
M. Mixup"
"BASELINE
CUTMIX",0.1653179190751445,"Mixup
Puzz
Mix
NFM
NFM (*) le"
"BASELINE
CUTMIX",0.16647398843930636,Test Accuracy
"BASELINE
CUTMIX",0.1676300578034682,Severities
"BASELINE
CUTMIX",0.16878612716763006,"gaussian jpeg
impulse
shot
snow
speckle
20 40 60 80 100"
"BASELINE
CUTMIX",0.1699421965317919,Noise type
"BASELINE
CUTMIX",0.17109826589595376,Figure 6: Pre-actived ResNet-18 evaluated on CIFAR-10c.
"BASELINE
CUTMIX",0.1722543352601156,"the average test accuracy across six selected perturbations and demonstrates the advantage of NFM
being particularly pronounced with the progression of severity levels. The right ﬁgure shows the
performance on the same set of six perturbations for the median severity level 3. NFM excels on
Gaussian, impulse, speckle and shot noise, and is competitive with the rest on the snow perturbation."
ROBUSTNESS TO ADVERSARIAL EXAMPLES,0.17341040462427745,"5.5
ROBUSTNESS TO ADVERSARIAL EXAMPLES"
ROBUSTNESS TO ADVERSARIAL EXAMPLES,0.1745664739884393,"So far we have only considered white noise and salt and pepper perturbations. We further consider
adversarial perturbations. Here, we use projected gradient decent (Madry et al., 2017) with 7 iterations
and various ϵ levels to construct the adversarial perturbations. Fig. 5 highlights the improved resilience
of ResNets trained with NFM to adversarial input perturbations and shows this consistently on both
CIFAR-10 (left) and CIFAR-100 (right). Models trained with both mixup and manifold mixup do not
show a substantially increased resilience to adversarial perturbations."
ROBUSTNESS TO ADVERSARIAL EXAMPLES,0.17572254335260115,"In Section F.6, we compare NFM to models that are adversarially trained. There, we see that
adversarially trained models are indeed more robust to adversarial attacks, while at the same time
being less accurate on clean data. However, models trained with NFM show an advantage compared
to adversarially trained models when faced with salt and pepper perturbations."
CONCLUSION,0.176878612716763,"6
CONCLUSION"
CONCLUSION,0.17803468208092485,"We introduce Noisy Feature Mixup, an effective data augmentation method that combines mixup and
noise injection. We identify the implicit regularization effects of NFM, showing that the effects are
ampliﬁcations of those of manifold mixup and noise injection. Moreover, we demonstrate the beneﬁts
of NFM in terms of superior model robustness, both theoretically and experimentally. Our work
inspires a range of interesting future directions, including theoretical investigations of the trade-offs
between accuracy and robustness for NFM and applications of NFM beyond computer vision tasks.
Further, it will be interesting to study whether NFM may also lead to better model calibration by
extending the analysis of Thulasidasan et al. (2019); Zhang et al. (2021)."
CONCLUSION,0.1791907514450867,Published as a conference paper at ICLR 2022
CONCLUSION,0.18034682080924855,CODE OF ETHICS
CONCLUSION,0.1815028901734104,We acknowledge that we have read and commit to adhering to the ICLR Code of Ethics.
CONCLUSION,0.18265895953757225,REPRODUCIBILITY
CONCLUSION,0.1838150289017341,"The codes that can be used to reproduce the empirical results, as well as description of the data
processing steps, presented in this paper are available as a zip ﬁle in Supplementary Material at
OpenReview.net. The codes are also available at https://github.com/erichson/NFM. For
the theoretical results, all assumptions, proofs and the related discussions are provided in SM."
CONCLUSION,0.18497109826589594,ACKNOWLEDGMENTS
CONCLUSION,0.1861271676300578,"S. H. Lim would like to acknowledge the WINQ Fellowship and the Knut and Alice Wallenberg
Foundation for providing support of this work. N. B. Erichson and M. W. Mahoney would like to
acknowledge IARPA (contract W911NF20C0035), NSF, and ONR for providing partial support of
this work. Our conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no
ofﬁcial endorsement should be inferred. We are also grateful for the generous support from Amazon
AWS."
REFERENCES,0.18728323699421964,REFERENCES
REFERENCES,0.1884393063583815,"Guozhong An. The effects of adding noise during backpropagation training on a generalization
performance. Neural Computation, 8(3):643–674, 1996."
REFERENCES,0.18959537572254334,"Henry S Baird. Document image defect models. In Structured Document Image Analysis, pp.
546–556. Springer, 1992."
REFERENCES,0.1907514450867052,"Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural Computation,
7(1):108–116, 1995."
REFERENCES,0.19190751445086704,"Olivier Bousquet, St´ephane Boucheron, and G´abor Lugosi. Introduction to statistical learning theory.
In Summer school on machine learning, pp. 169–207. Springer, 2003."
REFERENCES,0.1930635838150289,"Alexander Camuto, Matthew Willetts, Umut S¸ims¸ekli, Stephen Roberts, and Chris Holmes. Explicit
regularisation in Gaussian noise injections. arXiv preprint arXiv:2007.07368, 2020."
REFERENCES,0.19421965317919074,"Luigi Carratino, Moustapha Ciss´e, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regulariza-
tion. arXiv preprint arXiv:2006.06049, 2020."
REFERENCES,0.19537572254335261,"Olivier Chapelle, Jason Weston, L´eon Bottou, and Vladimir Vapnik. Vicinal risk minimization.
Advances in Neural Information Processing Systems, pp. 416–422, 2001."
REFERENCES,0.19653179190751446,"Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation.
Journal of Machine Learning Research, 21(245):1–71, 2020."
REFERENCES,0.1976878612716763,"Dan Claudiu Cires¸an, Ueli Meier, Luca Maria Gambardella, and J¨urgen Schmidhuber. Deep, big,
simple neural nets for handwritten digit recognition. Neural Computation, 22(12):3207–3220,
2010."
REFERENCES,0.19884393063583816,"Nicolas Couellan. Probabilistic robustness estimates for feed-forward neural networks. Neural
Networks, 142:138–147, 2021."
REFERENCES,0.2,"Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R´e. A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528–1537. PMLR, 2019."
REFERENCES,0.20115606936416186,"Dennis DeCoste and Bernhard Sch¨olkopf. Training invariant support vector machines. Machine
Learning, 46(1):161–190, 2002."
REFERENCES,0.2023121387283237,Published as a conference paper at ICLR 2022
REFERENCES,0.20346820809248556,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.2046242774566474,"Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018."
REFERENCES,0.20578034682080926,"Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211–407, 2014."
REFERENCES,0.2069364161849711,"Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classiﬁcation. arXiv preprint arXiv:1803.05598, 2018."
REFERENCES,0.20809248554913296,"Logan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry,
Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric
Wallace. A discussion of ’adversarial examples are not bugs, they are features’. Distill, 2019."
REFERENCES,0.2092485549132948,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. ArXiv preprint
arXiv:1906.00945, 2020."
REFERENCES,0.21040462427745665,"Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classiﬁers:
from adversarial to random noise. arXiv preprint arXiv:1608.08967, 2016."
REFERENCES,0.2115606936416185,"Jean-Yves Franceschi, Alhussein Fawzi, and Omar Fawzi. Robustness of classiﬁers to uniform
lp and Gaussian noise. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
1280–1288. PMLR, 2018."
REFERENCES,0.21271676300578035,"Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International
Statistical Review, 70(3):419–435, 2002."
REFERENCES,0.2138728323699422,"Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: A simple way to improve
generalization of neural network training. arXiv preprint arXiv:2002.09024, 2020."
REFERENCES,0.21502890173410405,"Ian Goodfellow, Patrick McDaniel, and Nicolas Papernot. Making machine learning robust against
adversarial inputs. Communications of the ACM, 61(7):56–66, 2018."
REFERENCES,0.2161849710982659,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.21734104046242775,"Kristjan Greenewald, Anming Gu, Mikhail Yurochkin, Justin Solomon, and Edward Chien. k-mixup
regularization for deep learning via optimal transport. arXiv preprint arXiv:2106.02933, 2021."
REFERENCES,0.2184971098265896,"Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions.
In International Conference on Machine Learning, pp. 3059–3068. PMLR, 2016."
REFERENCES,0.21965317919075145,"Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi.
Escaping the big data paradigm with compact transformers. arXiv preprint arXiv:2104.05704,
2021."
REFERENCES,0.2208092485549133,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630–645. Springer, 2016."
REFERENCES,0.22196531791907514,"Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiﬁer
against adversarial manipulation. arXiv preprint arXiv:1705.08475, 2017."
REFERENCES,0.223121387283237,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corrup-
tions and perturbations. Proceedings of the International Conference on Learning Representations,
2019."
REFERENCES,0.22427745664739884,"Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781, 2019."
REFERENCES,0.2254335260115607,Published as a conference paper at ICLR 2022
REFERENCES,0.22658959537572254,"Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with Jacobian regularization. arXiv
preprint arXiv:1908.02729, 2019."
REFERENCES,0.2277456647398844,"Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning, pp. 5275–5285.
PMLR, 2020a."
REFERENCES,0.22890173410404624,"Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In International Conference on Machine Learning, 2020b."
REFERENCES,0.2300578034682081,"Masanari Kimura. Mixup training as the complexity reduction. arXiv preprint arXiv:2006.06231,
2020."
REFERENCES,0.23121387283236994,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009."
REFERENCES,0.2323699421965318,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. Advances in Neural Information Processing Systems, 25:1097–1105,
2012."
REFERENCES,0.23352601156069364,"Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shaﬁeezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
Operations Research & Management Science in the Age of Analytics, pp. 130–166. INFORMS,
2019."
REFERENCES,0.23468208092485549,"Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,
2016."
REFERENCES,0.23583815028901733,"Yongchan Kwon, Wonyoung Kim, Joong-Ho Won, and Myunghee Cho Paik. Principled learning
method for Wasserstein distributionally robust optimization with local perturbations. In Interna-
tional Conference on Machine Learning, pp. 5567–5576. PMLR, 2020."
REFERENCES,0.23699421965317918,"Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacriﬁcing too much accuracy. In Proceedings of the
12th ACM Workshop on Artiﬁcial Intelligence and Security, pp. 95–103, 2019."
REFERENCES,0.23815028901734103,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656–672. IEEE, 2019."
REFERENCES,0.23930635838150288,"Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy recurrent
neural networks. arXiv preprint arXiv:2102.04877, 2021."
REFERENCES,0.24046242774566473,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.24161849710982658,"M. W. Mahoney. Approximate computation and implicit regularization for very large-scale data
analysis. In Proceedings of the 31st ACM Symposium on Principles of Database Systems, pp.
143–154, 2012."
REFERENCES,0.24277456647398843,"M. W. Mahoney and L. Orecchia. Implementing regularization implicitly via approximate eigenvector
computation. In International Conference on Machine Learning, pp. 121–128, 2011."
REFERENCES,0.24393063583815028,"Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More data
can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020."
REFERENCES,0.24508670520231213,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9078–9086, 2019."
REFERENCES,0.24624277456647398,"Roman Novak, Yasaman Bahri, Daniel A Abolaﬁa, Jeffrey Pennington, and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760, 2018."
REFERENCES,0.24739884393063583,Published as a conference paper at ICLR 2022
REFERENCES,0.24855491329479767,"Sayak Paul and Pin-Yu Chen.
Vision transformers are robust learners.
arXiv preprint
arXiv:2105.07581, 2021."
REFERENCES,0.24971098265895952,"Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, C´edric Gouy-
Pailler, and Jamal Atif. Theoretical evidence for adversarial robustness through randomization.
arXiv preprint arXiv:1902.01148, 2019a."
REFERENCES,0.2508670520231214,"Rafael Pinot, Florian Yger, C´edric Gouy-Pailler, and Jamal Atif. A uniﬁed view on differential
privacy and robustness to adversarial examples. arXiv preprint arXiv:1906.07982, 2019b."
REFERENCES,0.2520231213872832,"Rafael Pinot, Laurent Meunier, Florian Yger, C´edric Gouy-Pailler, Yann Chevaleyre, and Jamal
Atif.
On the robustness of randomized classiﬁers to adversarial examples.
arXiv preprint
arXiv:2102.10875, 2021."
REFERENCES,0.25317919075144507,"Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. arXiv preprint arXiv:2002.10716,
2020."
REFERENCES,0.2543352601156069,"Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659, 2019."
REFERENCES,0.25549132947976877,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
arXiv preprint
arXiv:2103.01946, 2021."
REFERENCES,0.2566473988439306,"Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In
International Conference on Machine Learning, pp. 8093–8104. PMLR, 2020."
REFERENCES,0.25780346820809247,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. arXiv preprint arXiv:1804.11285, 2018."
REFERENCES,0.2589595375722543,"Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness
of visual transformers. arXiv preprint arXiv:2103.15670, 2021."
REFERENCES,0.26011560693641617,"Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1–48, 2019."
REFERENCES,0.261271676300578,"Jure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep
neural networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, 2017."
REFERENCES,0.26242774566473986,"Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of
adversarial training. In NIPS workshop on Machine Learning and Computer Security, volume 3,
pp. 4, 2017."
REFERENCES,0.2635838150289017,"Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness the
cost of accuracy?–a comprehensive study on the robustness of 18 deep image classiﬁcation models.
In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631–648, 2018."
REFERENCES,0.26473988439306356,"Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, and Sarah Michalak.
On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
arXiv preprint arXiv:1905.11001, 2019."
REFERENCES,0.2658959537572254,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018."
REFERENCES,0.26705202312138726,"Francisco Utrera, Evan Kravitz, N Benjamin Erichson, Rajiv Khanna, and Michael W Mahoney.
Adversarially-trained deep nets transfer better. arXiv preprint arXiv:2007.05869, 2020."
REFERENCES,0.2682080924855491,"Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media,
2013."
REFERENCES,0.26936416184971096,"Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-Paz,
and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In
International Conference on Machine Learning, pp. 6438–6447. PMLR, 2019."
REFERENCES,0.2705202312138728,Published as a conference paper at ICLR 2022
REFERENCES,0.27167630057803466,"Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via Lipschitz
augmentation. arXiv preprint arXiv:1905.03684, 2019a."
REFERENCES,0.2728323699421965,"Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classiﬁcation
via an all-layer margin. arXiv preprint arXiv:1910.04284, 2019b."
REFERENCES,0.27398843930635836,"Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R´e. On the generalization effects of
linear transformations in data augmentation. In International Conference on Machine Learning,
pp. 10410–10420. PMLR, 2020."
REFERENCES,0.2751445086705202,"Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness. arXiv preprint arXiv:2003.02460, 2020a."
REFERENCES,0.27630057803468205,"Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, Kannan
Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 6223–6234, 2020b."
REFERENCES,0.2774566473988439,"Wenpeng Yin, Huan Wang, Jin Qu, and Caiming Xiong. BatchMixup: Improving training by interpo-
lating hidden states of the entire mini-batch. In Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021, pp. 4908–4912, 2021."
REFERENCES,0.2786127167630058,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In International
Conference on Computer Vision, pp. 6023–6032, 2019."
REFERENCES,0.27976878612716766,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016."
REFERENCES,0.2809248554913295,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472–7482. PMLR, 2019."
REFERENCES,0.28208092485549136,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017."
REFERENCES,0.2832369942196532,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020."
REFERENCES,0.28439306358381505,"Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves
calibration. arXiv preprint arXiv:2102.06289, 2021."
REFERENCES,0.2855491329479769,Published as a conference paper at ICLR 2022
REFERENCES,0.28670520231213875,Supplementary Material (SM) for “Noisy Feature Mixup”
REFERENCES,0.2878612716763006,Organizational Details. This SM is organized as follows.
REFERENCES,0.28901734104046245,"• In Section A, we study the regularizing effects of NFM within the vicinal risk minimization
framework, relating the effects to those of mixup and noise injection.
• In Section B, we restate the results presented in the main paper and provide their proof.
• In Section C, we study robutsness of NFM through the lens of implicit regularization,
showing that NFM can implicitly increase the classiﬁcation margin.
• In Section D, we study robustness of NFM via the lens of probabilistic robustness, showing
that noise injection can improve robustness on top of manifold mixup while keeping track
of maximal loss in accuracy incurred under attack by tuning the noise levels.
• In Section E, we provide results on generalization bounds for NFM and their proofs,
identifying the mechanisms by which NFM can lead to improved generalization bound.
• In Section F, we provide additional experimental results and their details."
REFERENCES,0.2901734104046243,We recall the notation that we use in the main paper as well as this SM.
REFERENCES,0.29132947976878615,"Notation. I denotes identity matrix, [K] := {1, . . . , K}, the superscript T denotes transposition, ◦
denotes composition, ⊙denotes Hadamard product, 1 denotes the vector with all components equal
one. For a vector v, vk denotes its kth component and ∥v∥p denotes its lp norm for p > 0. conv(X)
denote the convex hull of X. Mλ(a, b) := λa + (1 −λ)b, for random variables a, b, λ. δz denotes the
Dirac delta function, deﬁned as δz(x) = 1 if x = z and δz(x) = 0 otherwise. 1A denotes indicator
function of the set A. For α, β > 0, ˜Dλ :=
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α), a uniform
mixture of two Beta distributions. For two vectors a, b, cos(a, b) := ⟨a, b⟩/∥a∥2∥b∥2 denotes their
cosine similarity. N(a, b) denotes the Gaussian distribution with mean a and covariance b."
REFERENCES,0.292485549132948,"A
NFM THROUGH THE LENS OF VICINAL RISK MINIMIZATION"
REFERENCES,0.29364161849710985,"In this section, we shall show that NFM can be constructed within a vicinal risk minimization (VRM)
framework at the level of both input and hidden layer representations."
REFERENCES,0.2947976878612717,"To begin with, we deﬁne a class of vicinal distributions and then relate NFM to such distributions.
Deﬁnition 1 (Randomly perturbed feature distribution). Let Zn = {z1, . . . , zn} be a feature set. We
say that P′
n is an ei-randomly perturbed feature distribution if there exists a set {z′
1, . . . , z′
n} such
that P′
n = 1"
REFERENCES,0.29595375722543354,"n
Pn
i=1 δz′
i, with z′
i = zi + ei, for some random variable ei (possibly dependent on Zn)
drawn from a probability distribution."
REFERENCES,0.2971098265895954,Note that the support of an ei-randomly perturbed feature distribution may be larger than that of Z.
REFERENCES,0.29826589595375724,"If Zn is an input dataset and the ei are bounded variables such that ∥ei∥≤β for some β ≥0, then P′
n
is a β-locally perturbed data distribution according to Deﬁnition 2 in (Kwon et al., 2020). Examples
of β-locally perturbed data distribution include that associated with denoising autoencoder, input
mixup, and adversarial training (see Example 1-3 in (Kwon et al., 2020)). Deﬁnition 1 can be viewed
as an extension of the deﬁnition in (Kwon et al., 2020), relaxing the boundedness condition on the
ei to cover a wide families of perturbed feature distribution. One simple example is the Gaussian
distribution, i.e., when ei ∼N(0, σ2
i ), which models Gaussian noise injection into the features.
Another example is the distribution associated with NFM, which we now discuss."
REFERENCES,0.2994219653179191,"To keep the randomly perturbed distribution close to the original distribution, the amplitude of
the perturbation should be small. In the sequel, we let ϵ > 0 be a small parameter and rescale
1 −λ 7→ϵ(1 −λ), σadd 7→ϵσadd and σmult 7→ϵσmult."
REFERENCES,0.30057803468208094,Let Fk be the family of mappings from gk(X) to Y and consider the VRM:
REFERENCES,0.3017341040462428,"inf
fk∈Fk Rn(fk) := E(g′
k(x),y′)∼P(k)
n [l(fk(g′
k(x))), y′)],
(10)"
REFERENCES,0.30289017341040464,"where P(k)
n
= 1"
REFERENCES,0.3040462427745665,"n
Pn
i=1 δ(g′
k(xi),y′
i), with g′
k(xi) = gk(xi) + ϵeNF M(k)
i
and y′
i = yi + ϵey
i , for some"
REFERENCES,0.30520231213872834,"random variables eNF M(k)
i
and ey
i ."
REFERENCES,0.3063583815028902,Published as a conference paper at ICLR 2022
REFERENCES,0.30751445086705204,"In NFM, we approximate the ground-truth distribution D using the family of distributions {P(k)
n }k∈S,
with a particular choice of (eNF M(k)
i
, ey
i ). In the sequel, we denote NFM at the level of kth layer as
NFM(k) (i.e., the particular case when S := {k})."
REFERENCES,0.3086705202312139,"The following lemma identiﬁes the (eNF M(k)
i
, ey
i ) associated with NFM(k) and relates the effects
of NFM(k) to those of mixup and noise injection, for any perturbation level ϵ > 0."
REFERENCES,0.30982658959537573,"Lemma 1. Let ϵ > 0 and denote zi(k) := gk(xi). Learning the neural network map f using
NFM(k) is a VRM with the (ϵeNF M(k)
i
, ϵey
i )-randomly perturbed feature distribution, P(k)
n
=
1
n
Pn
i=1 δ(z′
i(k),y′
i), with z′
i(k) := zi(k)+ϵeNF M(k)
i
, y′
i := yi +ϵey
i , as the vicinal distribution. Here,
ey
i = (1 −λ)(˜yi −yi),"
REFERENCES,0.3109826589595376,"eNF M(k)
i
= (1 + ϵσmultξmult) ⊙emixup(k)
i
+ enoise(k)
i
,
(11)"
REFERENCES,0.31213872832369943,"where emixup(k)
i
= (1 −λ)(˜zi(k) −zi(k)), and enoise(k)
i
= σmultξmult ⊙zi(k) + σaddξadd, with
zi(k), ˜zi(k) ∈gk(X), λ ∼Beta(α, β) and yi, ˜yi ∈Y. Here, (˜zi(k), ˜yi) are drawn randomly from
the training set."
REFERENCES,0.3132947976878613,"Therefore, the random perturbation associated to NFM is data-dependent, and it consists of a
randomly weighted sum of that from injecting noise into the feature and that from mixing pairs
of feature samples. As a simple example, one can take ξadd, ξmult to be independent standard
Gaussian random variables, in which case we have enoise(k)
i
∼N(0, σ2
addI + σ2
multdiag(zi(k))2),
and ei ∼N(0, σ2
add + σ2
multMλ(zi(k), ˜zi(k))2) in Lemma 1."
REFERENCES,0.31445086705202313,We now prove Lemma 1.
REFERENCES,0.315606936416185,"Proof of Lemma 1. Let k be given and set ϵ = 1 without loss of generality. For every i ∈[n],
NFM(k) injects noise on top of a mixed sample z′
i(k) and outputs:"
REFERENCES,0.31676300578034683,"z′′
i (k) = (1 + σmultξmult) ⊙z′
i(k) + σaddξadd
(12)
= (1 + σmultξmult) ⊙(λzi(k) + (1 −λ)˜zi(k)) + σaddξadd
(13)"
REFERENCES,0.3179190751445087,"= zi(k) + eNF M(k)
i
,
(14)"
REFERENCES,0.3190751445086705,"where eNF M(k)
i
= (1 −λ)(˜zi(k) −zi(k)) + σmultξmult ⊙(λzi(k) + (1 −λ)˜zi(k)) + σaddξadd."
REFERENCES,0.3202312138728324,"Now, note that applying mixup to the pair (zi(k), ˜zi(k)) results in z′
i(k) = zi(k) + emixup(k)
i
, with
emixup(k)
i
= (1 −λ)(˜zi(k) −zi(k)), where zi(k), ˜zi(k) ∈gk(X) and λ ∼Beta(α, β), whereas
applying noise injection to zi(k) results in (1+σmultξmult)⊙zi(k)+σaddξadd = zi(k)+enoise(k)
i
,
with enoise(k)
i
= σmultξmult ⊙zi(k) + σaddξadd. Rewriting eNF M(k)
i
in terms of emixup(k)
i
and
enoise(k)
i
gives"
REFERENCES,0.3213872832369942,"eNF M(k)
i
= (1 + σmultξmult) ⊙emixup(k)
i
+ enoise(k)
i
.
(15)"
REFERENCES,0.3225433526011561,"Similarly, we can derive the expression for ey
i using the same argument. The results in the lemma
follow upon applying the rescaling 1 −λ 7→ϵ(1 −λ), σadd 7→ϵσadd and σmult 7→ϵσmult, for
ϵ > 0."
REFERENCES,0.3236994219653179,"B
STATEMENTS AND PROOF OF THE RESULTS IN THE MAIN PAPER"
REFERENCES,0.3248554913294798,"B.1
COMPLETE STATEMENT OF THEOREM 1 IN THE MAIN PAPER AND THE PROOF"
REFERENCES,0.3260115606936416,We ﬁrst state the complete statement of Theorem 1 in the main paper.
REFERENCES,0.32716763005780347,"Theorem 3 (Theorem 1 in the main paper). Let ϵ > 0 be a small parameter, and assume that h and
f are twice differentiable. Then, LNF M
n
= Ek∼SLNF M(k)
n
, where"
REFERENCES,0.3283236994219653,"LNF M(k)
n
= Lstd
n
+ ϵR(k)
1
+ ϵ2 ˜R(k)
2
+ ϵ2 ˜R(k)
3
+ ϵ2ϕ(ϵ),
(16)"
REFERENCES,0.32947976878612717,Published as a conference paper at ICLR 2022 with
REFERENCES,0.330635838150289,"˜R(k)
2
= R(k)
2
+ σ2
addRadd(k)
2
+ σ2
multRmult(k)
2
,
(17)"
REFERENCES,0.33179190751445087,"˜R(k)
3
= R(k)
3
+ σ2
addRadd(k)
3
+ σ2
multRmult(k)
3
,
(18) where"
REFERENCES,0.3329479768786127,"R(k)
1
= Eλ∼˜
Dλ[1 −λ] n n
X"
REFERENCES,0.33410404624277457,"i=1
(h′(f(xi) −yi)∇kf(gk(xi))T Exr∼Dx[gk(xr) −gk(xi)],
(19)"
REFERENCES,0.3352601156069364,"R(k)
2
= Eλ∼˜
Dλ[(1 −λ)2]"
N,0.33641618497109826,"2n n
X"
N,0.3375722543352601,"i=1
h′′(f(xi))∇kf(gk(xi))T"
N,0.33872832369942196,"× Exr∼Dx[(gk(xr) −gk(xi))(gk(xr) −gk(xi))T ]∇kf(gk(xi)),
(20)"
N,0.3398843930635838,"R(k)
3
= Eλ∼˜
Dλ[(1 −λ)2]"
N,0.34104046242774566,"2n n
X"
N,0.3421965317919075,"i=1
(h′(f(xi)) −yi)"
N,0.34335260115606936,"× Exr∼Dx[(gk(xr) −gk(xi))T ∇2
kf(gk(xi))(gk(xr) −gk(xi))],
(21)"
N,0.3445086705202312,"Radd(k)
2
= 1"
N,0.34566473988439306,"2n n
X"
N,0.3468208092485549,"i=1
h′′(f(xi))∇kf(gk(xi))T Eξk[ξadd
k
(ξadd
k
)T ]∇kf(gk(xi)),
(22)"
N,0.34797687861271676,"Rmult(k)
2
= 1"
N,0.3491329479768786,"2n n
X"
N,0.35028901734104045,"i=1
h′′(f(xi))∇kf(gk(xi))T (Eξk[ξmult
k
(ξmult
k
)T ] ⊙gk(xi)gk(xi)T )∇kf(gk(xi)), (23)"
N,0.3514450867052023,"Radd(k)
3
= 1"
N,0.35260115606936415,"2n n
X"
N,0.353757225433526,"i=1
(h′(f(xi)) −yi)Eξk[(ξadd
k
)T ∇2
kf(gk(xi))ξadd
k
],
(24)"
N,0.35491329479768785,"Rmult(k)
3
= 1"
N,0.3560693641618497,"2n n
X"
N,0.35722543352601155,"i=1
(h′(f(xi)) −yi)Eξk[(ξmult
k
⊙gk(xi))T ∇2
kf(gk(xi))(ξmult
k
⊙gk(xi))], (25)"
N,0.3583815028901734,"and ϕ(ϵ) = Eλ∼˜
DλExr∼DxEξk∼Q[ϕ(ϵ)], with ϕ some function such that limϵ→0 ϕ(ϵ) = 0."
N,0.35953757225433525,"Following the setup of Zhang et al. (2020), we provide empirical results to show that the second order
Taylor approximation for the NFM loss function is generally accurate (see Figure 7)."
N,0.3606936416184971,"Recall from the main paper that the NFM loss function to be minimized is LNF M
n
= Ek∼SLNF M(k)
n
,
where"
N,0.36184971098265895,"LNF M(k)
n
= 1 n2 n
X i=1 n
X"
N,0.3630057803468208,"j=1
Eλ∼Beta(α,β)Eξk∼Ql(fk(Mλ,ξk(gk(xi), gk(xj))), Mλ(yi, yj)),
(26)"
N,0.36416184971098264,"Figure 7: Comparison of the original NFM loss with the approximate loss function during training
and testing for a two layer ReLU neural network trained on the toy dataset of Subsection F.2."
N,0.3653179190751445,Published as a conference paper at ICLR 2022
N,0.36647398843930634,"where l : RK × RK →[0, ∞) is a loss function of the form l(f(x), y) = h(f(x)) −yf(x),
ξk := (ξadd
k
, ξmult
k
) are drawn from some probability distribution Q with ﬁnite ﬁrst two moments
(with zero mean), and
Mλ,ξk(gk(x), gk(x′)) := (1 + σmultξmult
k
) ⊙Mλ(gk(x), gk(x′)) + σaddξadd
k
.
(27)"
N,0.3676300578034682,"Before proving Theorem 3, we note that, following the argument of the proof of Lemma 3.1 in Zhang
et al. (2020), the loss function minimized by NFM can be written as follows. For completeness, we
provide all details of the proof."
N,0.36878612716763004,"Lemma 2. The NFM loss (26) can be equivalently written as LNF M
n
= Ek∼SLNF M(k)
n
, where"
N,0.3699421965317919,"LNF M(k)
n
= 1 n n
X"
N,0.37109826589595374,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q[h(fk(gk(xi)+ϵeNF M(k)
i
))−yifk(gk(xi)+ϵeNF M(k)
i
)],"
N,0.3722543352601156,"(28)
with
eNF M(k)
i
= (1 + ϵσmultξmult
k
) ⊙emixup(k)
i
+ enoise(k)
i
.
(29)"
N,0.37341040462427744,"Here emixup(k)
i
= (1 −λ)(gk(xr) −gk(xi)) and enoise(k)
i
= σmultξmult
k
⊙gk(xi) + σaddξadd
k
, with
gk(xi), gk(xr) ∈gk(X) and λ ∼Beta(α, β)."
N,0.3745664739884393,"Proof of Lemma 2. From (26), we have:"
N,0.37572254335260113,"LNF M(k)
n
= 1 n2 n
X i=1 n
X"
N,0.376878612716763,"j=1
Eλ∼Beta(α,β)Eξk∼Ql(fk(Mλ,ξk(gk(xi), gk(xj))), Mλ(yi, yj)).
(30)"
N,0.37803468208092483,"We can rewrite:
Eλ∼Beta(α,β)l(fk(Mλ,ξk(gk(xi), gk(xj))), Mλ(yi, yj))"
N,0.3791907514450867,"= Eλ∼Beta(α,β)[h(fk(Mλ,ξk(gk(xi), gk(xj)))) −Mλ(yi, yj)fk(Mλ,ξk(gk(xi), gk(xj)))]
(31)"
N,0.38034682080924853,"= Eλ∼Beta(α,β)[λ(h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yifk(Mλ,ξk(gk(xi), gk(xj))))"
N,0.3815028901734104,"+ (1 −λ)(h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yjfk(Mλ,ξk(gk(xi), gk(xj))))]
(32)
= Eλ∼Beta(α,β)EB∼Bern(λ)[B(h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yifk(Mλ,ξk(gk(xi), gk(xj))))"
N,0.38265895953757223,"+ (1 −B)(h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yjfk(Mλ,ξk(gk(xi), gk(xj))))],
(33)
where Bern(λ) denotes the Bernoulli distribution with parameter λ (i.e., P[B = 1] = λ and
P[B = 0] = 1 −λ)."
N,0.3838150289017341,"Note that λ ∼Beta(α, β) and B|λ ∼Bern(λ). By conjugacy, we can switch their order:"
N,0.38497109826589593,"B ∼Bern

α
α + β"
N,0.3861271676300578,"
, λ|B ∼Beta(α + B, β + 1 −B),
(34)"
N,0.3872832369942196,"and arrive at:
Eλ∼Beta(α,β)l(fk(Mλ,ξk(gk(xi), gk(xj))), Mλ(yi, yj))"
N,0.3884393063583815,"= EB∼Bern(
α
α+β)Eλ∼Beta(α+B,β+1−B)[B(h(fk(Mλ,ξk(gk(xi), gk(xj))))"
N,0.3895953757225434,"−yifk(Mλ,ξk(gk(xi), gk(xj))))
+ (1 −B)(h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yjfk(Mλ,ξk(gk(xi), gk(xj))))]
(35)"
N,0.39075144508670523,"=
α
α + β Eλ∼Beta(α+1,β)[h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yifk(Mλ,ξk(gk(xi), gk(xj)))]"
N,0.3919075144508671,"+
β
α + β Eλ∼Beta(α,β+1)[h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yjfk(Mλ,ξk(gk(xi), gk(xj)))]. (36)"
N,0.3930635838150289,"Using the facts that Beta(β + 1, α) and 1 −Beta(α, β + 1) are of the same distribution and
M1−λ(xi, xj) = Mλ(xj, xi), we have:
X"
N,0.3942196531791908,"i,j
Eλ∼Beta(α,β+1)[h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yjfk(Mλ,ξk(gk(xi), gk(xj)))] =
X"
N,0.3953757225433526,"i,j
Eλ∼Beta(β+1,α)[h(fk(Mλ,ξk(gk(xi), gk(xj)))) −yifk(Mλ,ξk(gk(xi), gk(xj)))].
(37)"
N,0.3965317919075145,Published as a conference paper at ICLR 2022
N,0.3976878612716763,"Therefore, denoting ˜Dλ :=
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α) and Dx := 1"
N,0.3988439306358382,"n
Pn
j=1 δxj the
empirical distribution induced by the training samples {xj}j∈[n], we have:"
N,0.4,"LNF M(k)
n
= 1 n n
X"
N,0.40115606936416187,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q[h(fk(Mλ,ξk(gk(xi), gk(xr))))"
N,0.4023121387283237,"−yifk(Mλ,ξk(gk(xi), gk(xr)))].
(38)"
N,0.40346820809248557,"The statement of the lemma follows upon substituting the fact that Mλ,ξk(gk(xi), gk(xr)) = gk(xi)+
ϵeNF M(k)
i
into the above equation."
N,0.4046242774566474,"With this lemma in hand, we now prove Theorem 3."
N,0.40578034682080927,"Proof of Theorem 3. Denote ψi(ϵ) := h(fk(gk(xi) + ϵeNF M(k)
i
)) −yifk(gk(xi) + ϵeNF M(k)
i
),
where eNF M(k)
i
is given in (29). Since h and fk are twice differentiable by assumption, ψi is twice
differentiable in ϵ, and"
N,0.4069364161849711,"ψi(ϵ) = ψi(0) + ϵψ′
i(0) + ϵ2"
N,0.40809248554913297,"2 ψ′′
i (0) + ϵ2ϕi(ϵ),
(39)"
N,0.4092485549132948,"where ϕi is some function such that limϵ→0 ϕi(ϵ) = 0. Therefore, by Lemma 2, LNF M
n
=
Ek∼SLNF M(k)
n
, where"
N,0.41040462427745666,"LNF M(k)
n
= 1 n n
X"
N,0.4115606936416185,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q[ψi(ϵ)]
(40) = 1 n n
X"
N,0.41271676300578036,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q"
N,0.4138728323699422,"
ψi(0) + ϵψ′
i(0) + ϵ2"
N,0.41502890173410406,"2 ψ′′
i (0) + ϵ2ϕi(ϵ)

(41) = 1 n n
X"
N,0.4161849710982659,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q"
N,0.41734104046242776,"
ψi(0) + ϵψ′
i(0) + ϵ2"
N,0.4184971098265896,"2 ψ′′
i (0)

+ ϵ2ϕ(ϵ)
(42)"
N,0.41965317919075146,"=: Lstd
n
+ ϵR(k)
1
+ ϵ2( ˜R(k)
2
+ ˜R(k)
3 ) + ϵ2ϕ(ϵ),
(43)"
N,0.4208092485549133,where ϕ(ϵ) = 1
N,0.42196531791907516,"n
Pn
i=1 Eλ∼˜
DλExr∼DxEξk∼Q[ϕi(ϵ)]."
N,0.423121387283237,"It remains to compute ψ′
i(0) and ψ′′
i (0) in order to arrive at the expression for the R(k)
1 , ˜R(k)
2
and
˜R(k)
3
presented in Theorem 3."
N,0.42427745664739885,"Denoting ˜gk(xi) := gk(xi) + ϵeNF M(k)
i
, we compute, applying chain rule:"
N,0.4254335260115607,"ψ′
i(ϵ) = h′(fk(˜gk(xi)))∇kfk(˜gk(xi))T ∂˜gk(xi)"
N,0.42658959537572255,"∂ϵ
−yi∇kfk(˜gk(xi))T ∂˜gk(xi)"
N,0.4277456647398844,"∂ϵ
(44)"
N,0.42890173410404625,= (h′(fk(˜gk(xi))) −yi)∇kfk(˜gk(xi))T ∂˜gk(xi)
N,0.4300578034682081,"∂ϵ
(45)"
N,0.43121387283236995,"= (h′(fk(˜gk(xi))) −yi)∇kfk(˜gk(xi))T eNF M(k)
i
(46)"
N,0.4323699421965318,"= (h′(fk(˜gk(xi))) −yi)∇kfk(˜gk(xi))T [(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi) + ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))],
(47)"
N,0.43352601156069365,where we have used ∂˜gk(xi)
N,0.4346820809248555,"∂ϵ
= eNF M(k)
i
in the second last line and substituted the expression for
eNF M(k)
i
from (29) in the last line above."
N,0.43583815028901735,"Therefore,"
N,0.4369942196531792,"ψ′
i(0) = (h′(fk(gk(xi))) −yi)∇kfk(gk(xi))T [(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi)],
(48)"
N,0.43815028901734104,Published as a conference paper at ICLR 2022 and
N,0.4393063583815029,"Eξk∼Qψ′
i(0) = (h′(fk(gk(xi))) −yi)∇kfk(gk(xi))T [(1 −λ)(gk(xr) −gk(xi))],
(49)"
N,0.44046242774566474,"where we have used the assumptions that Eξk∼Qξadd
k
= 0 and Eξk∼Qξmult
k
= 0. The expression for
the R(k)
1
in the theorem then follows from substituting (49) into (42)."
N,0.4416184971098266,"Next, using chain rule, we have:"
N,0.44277456647398844,"ψ′′
i (ϵ) = ∂ ∂ϵ"
N,0.4439306358381503,"
(h′(fk(˜gk(xi))) −yi)∇kfk(˜gk(xi))T ∂˜gk(xi) ∂ϵ"
N,0.44508670520231214,"
(50) =
 ∂"
N,0.446242774566474,"∂ϵ(h′(fk(˜gk(xi))) −yi)

∇kfk(˜gk(xi))T ∂˜gk(xi) ∂ϵ"
N,0.44739884393063584,+ (h′(fk(˜gk(xi))) −yi) ∂ ∂ϵ
N,0.4485549132947977,"
∇kfk(˜gk(xi))T ∂˜gk(xi) ∂ϵ"
N,0.44971098265895953,"
.
(51)"
N,0.4508670520231214,"Note that, applying chain rule, ∂
∂ϵ"
N,0.45202312138728323,"
∇kfk(˜gk(xi))T ∂˜gk(xi) ∂ϵ 
= ∂ ∂ϵ"
N,0.4531791907514451,"
∇kfk(˜gk(xi))T eNF M(k)
i

(52) = ∂ ∂ϵ"
N,0.45433526011560693,"
(eNF M(k)
i
)T ∇kfk(˜gk(xi))

(53)"
N,0.4554913294797688,"= (eNF M(k)
i
)T ∇2
kfk(˜gk(xi))∂˜gk(xi)"
N,0.45664739884393063,"∂ϵ
(54)"
N,0.4578034682080925,"= (eNF M(k)
i
)T ∇2
kfk(˜gk(xi))eNF M(k)
i
.
(55)"
N,0.45895953757225433,"Also, using chain rule again,  ∂"
N,0.4601156069364162,"∂ϵ(h′(fk(˜gk(xi))) −yi)

= h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T ∂˜gk(xi)"
N,0.461271676300578,"∂ϵ
(56)"
N,0.4624277456647399,"= h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T eNF M(k)
i
.
(57)"
N,0.4635838150289017,"Therefore, we have:"
N,0.4647398843930636,"ψ′′
i (ϵ) = h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T eNF M(k)
i
(eNF M(k)
i
)T ∇kfk(˜gk(xi))"
N,0.4658959537572254,"+ (h′(fk(˜gk(xi))) −yi)(eNF M(k)
i
)T ∇2
kfk(˜gk(xi))eNF M(k)
i
(58)"
N,0.46705202312138727,"= h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T [(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi) + ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]"
N,0.4682080924855491,"× [(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi)"
N,0.46936416184971097,"+ ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]T ∇kfk(˜gk(xi))"
N,0.4705202312138728,"+ (h′(fk(˜gk(xi))) −yi)[(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi)"
N,0.47167630057803467,"+ ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]T ∇2
kfk(˜gk(xi))[(1 −λ)(gk(xr) −gk(xi))"
N,0.4728323699421965,"+ σaddξadd
k
+ σmultξmult
k
⊙gk(xi) + ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]
(59)"
N,0.47398843930635837,"=: h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T P1(ϵ)∇kfk(˜gk(xi)) + (h′(fk(˜gk(xi))) −yi)P2(ϵ),
(60)"
N,0.4751445086705202,"where we have substituted the expression for the eNF M(k)
i
into the ﬁrst line to arrive at the last line
above."
N,0.47630057803468207,Published as a conference paper at ICLR 2022
N,0.4774566473988439,"Note that,"
N,0.47861271676300576,Eξk∼QP1(ϵ)
N,0.4797687861271676,"= Eξk∼Q[(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi)"
N,0.48092485549132946,"+ ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))] × [(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi) + ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]T
(61)"
N,0.4820809248554913,"= (1 −λ)2(gk(xr) −gk(xi))(gk(xr) −gk(xi))T + σ2
addEξk∼Q[ξadd
k
(ξadd
k
)T ]"
N,0.48323699421965316,"+ σ2
multEξk∼Q[(ξmult
k
⊙gk(xi))(ξmult
k
⊙gk(xi))T ] + o(ϵ)
(62)"
N,0.484393063583815,"= (1 −λ)2(gk(xr) −gk(xi))(gk(xr) −gk(xi))T + σ2
addEξk∼Q[ξadd
k
(ξadd
k
)T ]"
N,0.48554913294797686,"+ σ2
multEξk∼Q[(ξmult
k
(ξmult
k
)T ) ⊙gk(xi))gk(xi)T ] + o(ϵ),
(63)"
N,0.4867052023121387,"as ϵ →0, where we have used the assumption that Eξk∼Qξadd
k
= 0 and Eξk∼Qξmult
k
= 0 in the
second last line above."
N,0.48786127167630056,"Similarly,"
N,0.4890173410404624,Eξk∼QP2(ϵ)
N,0.49017341040462425,"= Eξk∼Q[(1 −λ)(gk(xr) −gk(xi)) + σaddξadd
k
+ σmultξmult
k
⊙gk(xi)"
N,0.4913294797687861,"+ ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]T ∇2
kfk(˜gk(xi))[(1 −λ)(gk(xr) −gk(xi))"
N,0.49248554913294795,"+ σaddξadd
k
+ σmultξmult
k
⊙gk(xi) + ϵ(1 −λ)σmultξmult
k
⊙(gk(xr) −gk(xi))]
(64)"
N,0.4936416184971098,"= (1 −λ)2(gk(xr) −gk(xi))T ∇2
kfk(˜gk(xi))(gk(xr) −gk(xi))"
N,0.49479768786127165,"+ σ2
addEξk∼Q[(ξadd
k
)T ∇2
kfk(˜gk(xi))ξadd
k
]"
N,0.4959537572254335,"+ σ2
multEξk∼Q[(ξmult
k
⊙gk(xi))T ∇2
kfk(˜gk(xi))(ξmult
k
⊙gk(xi))] + o(ϵ),
(65)"
N,0.49710982658959535,as ϵ →0.
N,0.4982658959537572,"Now, recall from Eq. (42) that we have"
N,0.49942196531791905,"LNF M(k)
n
= 1 n n
X"
N,0.500578034682081,"i=1
Eλ∼˜
DλExr∼DxEξk∼Q"
N,0.5017341040462427,"
ψi(0) + ϵψ′
i(0) + ϵ2"
N,0.5028901734104047,"2 ψ′′
i (0)

+ ϵ2ϕ(ϵ)
(66)"
N,0.5040462427745664,"=: Lstd
n
+ ϵR(k)
1
+ ϵ2( ˜R(k)
2
+ ˜R(k)
3 ) + ϵ2ϕ(ϵ),
(67)"
N,0.5052023121387283,"where ψi(0) = h(fk(gk(xi))) −yifk(gk(xi)). Also, we have:"
N,0.5063583815028901,"Eξk∼Q[ψ′′
i (ϵ)]"
N,0.507514450867052,= h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T Eξk∼Q[P1(ϵ)]∇kfk(˜gk(xi))
N,0.5086705202312138,"+ (h′(fk(˜gk(xi))) −yi)Eξk∼Q[P2(ϵ)]
(68)"
N,0.5098265895953757,= h′′(fk(˜gk(xi)))∇kfk(˜gk(xi))T [(1 −λ)2(gk(xr) −gk(xi))(gk(xr) −gk(xi))T
N,0.5109826589595375,"+ σ2
addEξk∼Q[ξadd
k
(ξadd
k
)T ] + σ2
multEξk∼Q[(ξmult
k
(ξmult
k
)T ) ⊙gk(xi))gk(xi)T ] + o(ϵ)]
× ∇kfk(˜gk(xi))"
N,0.5121387283236994,"+ (h′(fk(˜gk(xi))) −yi)[(1 −λ)2(gk(xr) −gk(xi))T ∇2
kfk(˜gk(xi))(gk(xr) −gk(xi))"
N,0.5132947976878612,"+ σ2
addEξk∼Q[(ξadd
k
)T ∇2
kfk(˜gk(xi))ξadd
k
]"
N,0.5144508670520231,"+ σ2
multEξk∼Q[(ξmult
k
⊙gk(xi))T ∇2
kfk(˜gk(xi))(ξmult
k
⊙gk(xi))] + o(ϵ)].
(69)"
N,0.5156069364161849,Published as a conference paper at ICLR 2022
N,0.5167630057803468,"Therefore, setting ϵ = 0,"
N,0.5179190751445086,"Eξk∼Q[ψ′′
i (0)]"
N,0.5190751445086705,= h′′(fk(gk(xi)))∇kfk(gk(xi))T [(1 −λ)2(gk(xr) −gk(xi))(gk(xr) −gk(xi))T
N,0.5202312138728323,"+ σ2
addEξk∼Q[ξadd
k
(ξadd
k
)T ] + σ2
multEξk∼Q[(ξmult
k
(ξmult
k
)T ) ⊙gk(xi))gk(xi)T ]]
× ∇kfk(gk(xi))"
N,0.5213872832369942,"+ (h′(fk(gk(xi))) −yi)[(1 −λ)2(gk(xr) −gk(xi))T ∇2
kfk(gk(xi))(gk(xr) −gk(xi))"
N,0.522543352601156,"+ σ2
addEξk∼Q[(ξadd
k
)T ∇2
kfk(gk(xi))ξadd
k
]"
N,0.5236994219653179,"+ σ2
multEξk∼Q[(ξmult
k
⊙gk(xi))T ∇2
kfk(gk(xi))(ξmult
k
⊙gk(xi))]].
(70)"
N,0.5248554913294797,"The expression for the ˜R(k)
2
and ˜R(k)
3
in the theorem follows upon substituting (70) into (66)."
N,0.5260115606936416,"B.2
THEOREM 2 IN THE MAIN PAPER AND THE PROOF"
N,0.5271676300578034,"We ﬁrst restate Theorem 2 in the main paper and then provide the proof. Recall that we consider the
binary cross-entropy loss, setting h(z) = log(1 + ez), with the labels y taking value in {0, 1} and
the classiﬁer model f : Rd →R.
Theorem 4 (Theorem 2 in the main paper). Let θ ∈Θ := {θ : yif(xi)+(yi−1)f(xi) ≥0 for all i ∈
[n]} be a point such that ∇kf(gk(xi)) and ∇2
kf(gk(xi)) exist for all i ∈[n], k ∈S. Assume that
fk(gk(xi)) = ∇kf(gk(xi))T gk(xi), ∇2
kf(gk(xi)) = 0 for all i ∈[n], k ∈S. In addition, suppose
that ∥∇f(xi)∥2 > 0 for all i ∈[n], Er∼Dx[gk(r)] = 0 and ∥gk(xi)∥2 ≥c(k)
x
√dk for all i ∈[n],
k ∈S. Then,"
N,0.5283236994219653,"LNF M
n
≥1 n n
X"
N,0.5294797687861271,"i=1
max
∥δi∥2≤ϵmix
i
l(f(xi + δi), yi) + Lreg
n
+ ϵ2φ(ϵ),
(71) where"
N,0.530635838150289,"ϵmix
i
= ϵEλ∼˜
Dλ[1 −λ] · Ek∼S"
N,0.5317919075144508,"
r(k)
i
c(k)
x
∥∇kf(gk(xi))∥2"
N,0.5329479768786127,∥∇f(xi)∥2 p dk
N,0.5341040462427745,"
,
(72)"
N,0.5352601156069364,"r(k)
i
= | cos(∇kf(gk(xi)), gk(xi))|,
(73)"
N,0.5364161849710982,"Lreg
n
= 1"
N,0.5375722543352601,"2n n
X"
N,0.5387283236994219,"i=1
|h′′(f(xi))|(ϵreg
i
)2,
(74) with"
N,0.5398843930635838,"(ϵreg
i
)2 = ϵ2∥∇kf(gk(xi))∥2
2"
N,0.5410404624277456,"
Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2 cos(∇kf(gk(xi)), gk(xr))2]"
N,0.5421965317919075,"+ σ2
addEξ[∥ξadd∥2
2 cos(∇kf(gk(xi)), ξadd)2]"
N,0.5433526011560693,"+ σ2
multEξ[∥ξmult ⊙gk(xi)∥2
2 cos(∇kf(gk(xi)), ξmult ⊙gk(xi))2]

,
(75)"
N,0.5445086705202312,and φ is some function such that limϵ→0 φ(ϵ) = 0.
N,0.545664739884393,"Theorem 4 says that LNF M
n
is approximately an upper bound of sum of an adversarial loss with l2-
attack of size ϵmix = mini ϵmix
i
and a feature-dependent regularizer with the strength of mini(ϵreg
i
)2.
Therefore, minimizing the NFM loss would result in a small regularized adversarial loss. We note
that both ϵmix
i
and ϵreg
i
depend on the cosine similarities between the directional derivatives and the
features at which the derivatives are evaluated at, whereas the ϵreg
i
additionally depend on the cosine
similarities between the directional derivatives and the injected noise."
N,0.5468208092485549,"Before proving Theorem 4, we remark that the assumption that fk(gk(xi)) = ∇kf(gk(xi))T gk(xi),
∇2
kf(gk(xi)) = 0 for all i ∈[n], k ∈S is satisﬁed by fully connected neural networks with ReLU
activation function or max-pooling. For a proof of this, we refer to Section B.2 in Zhang et al. (2020).
The assumption that Er∼Dx[gk(r)] = 0 could be relaxed at the cost of obtaining a more complicated
formula (see Remark 1 for the formula) for the ϵreg
i
in the bound, which could be derived in a
straightforward manner."
N,0.5479768786127167,Published as a conference paper at ICLR 2022
N,0.5491329479768786,"Proof of Theorem 4. For h(z) = log(1 + ez), we have h′(z) =
ez
1+ez =: S(z) ≥0 and h′′(z) =
ez
(1+ez)2 = S(z)(1 −S(z)) ≥0. Substituting these expressions into the equation of Theorem 3 and
using the assumptions that fk(gk(xi)) = ∇kf(gk(xi))T gk(xi) and Er∼Dx[gk(r)] = 0, we have, for
k ∈S,"
N,0.5502890173410404,"R(k)
1
= Eλ∼˜
Dλ[1 −λ] n n
X"
N,0.5514450867052023,"i=1
(yi −S(f(xi)))fk(gk(xi)),
(76)"
N,0.5526011560693641,and we compute:
N,0.553757225433526,"R(k)
2
= Eλ∼˜
Dλ[(1 −λ)2]"
N,0.5549132947976878,"2n n
X"
N,0.5560693641618497,"i=1
S(f(xi))(1 −S(f(xi)))∇kf(gk(xi))T"
N,0.5572254335260116,"× Exr∼Dx[(gk(xr) −gk(xi))(gk(xr) −gk(xi))T ]∇kf(gk(xi))
(77)"
N,0.5583815028901734,"≥Eλ∼˜
Dλ[(1 −λ)]2"
N,0.5595375722543353,"2n n
X"
N,0.5606936416184971,"i=1
|S(f(xi))(1 −S(f(xi)))|∇kf(gk(xi))T"
N,0.561849710982659,"× Exr∼Dx[(gk(xr) −gk(xi))(gk(xr) −gk(xi))T ]∇kf(gk(xi))
(78)"
N,0.5630057803468208,"= Eλ∼˜
Dλ[(1 −λ)]2"
N,0.5641618497109827,"2n n
X"
N,0.5653179190751445,"i=1
|S(f(xi))(1 −S(f(xi)))|∇kf(gk(xi))T"
N,0.5664739884393064,"× (Exr∼Dx[(gk(xr)gk(xr)T ] + gk(xi)gk(xi)T ])∇kf(gk(xi))
(79)"
N,0.5676300578034682,"= Eλ∼˜
Dλ[(1 −λ)]2"
N,0.5687861271676301,"2n n
X"
N,0.5699421965317919,"i=1
|S(f(xi))(1 −S(f(xi)))|(∇kf(gk(xi))T gk(xi))2"
N,0.5710982658959538,"+ Eλ∼˜
Dλ[(1 −λ)]2"
N,0.5722543352601156,"2n n
X"
N,0.5734104046242775,"i=1
|S(f(xi))(1 −S(f(xi)))|Exr∈Dx[(∇kf(gk(xi))T gk(xr))2] (80)"
N,0.5745664739884393,"= Eλ∼˜
Dλ[(1 −λ)]2"
N,0.5757225433526012,"2n n
X"
N,0.576878612716763,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2∥gk(xi)∥2
2"
N,0.5780346820809249,"× (cos(∇kf(gk(xi)), gk(xi)))2 + 1"
N,0.5791907514450867,"2n n
X"
N,0.5803468208092486,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2"
N,0.5815028901734104,"× Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2 cos(∇kf(gk(xi)), gk(xr))2]
(81) ≥1"
N,0.5826589595375723,"2n n
X"
N,0.5838150289017341,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2Eλ∼˜
Dλ[(1 −λ)]2dk(r(k)
i
c(k)
x )2 + 1"
N,0.584971098265896,"2n n
X"
N,0.5861271676300578,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2 · Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2"
N,0.5872832369942197,"× cos(∇kf(gk(xi)), gk(xr))2]
(82) = 1"
N,0.5884393063583815,"2n n
X"
N,0.5895953757225434,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇f(xi)∥2
2"
N,0.5907514450867052,"×

Eλ∼˜
Dλ[(1 −λ)]2 ∥∇kf(gk(xi))∥2
2
∥∇f(xi)∥2
2
dk(r(k)
i
c(k)
x )2
 + 1"
N,0.5919075144508671,"2n n
X"
N,0.5930635838150289,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2 · Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2"
N,0.5942196531791908,"× cos(∇kf(gk(xi)), gk(xr))2].
(83)"
N,0.5953757225433526,"In the above, we have used the facts that E[Z2] = E[Z]2 + V ar(Z) ≥E[Z]2 and S, S(1 −S) ≥0 to
obtain (78), the assumption that Er∼Dx[gk(r)] = 0 to arrive at (79), the assumption that ∥gk(xi)∥2 ≥
c(k)
x
√dk for all i ∈[n], k ∈S to arrive at (82), and the assumption that ∥∇f(xi)∥2 > 0 for all
i ∈[n] to justify the last equation above."
N,0.5965317919075145,Published as a conference paper at ICLR 2022
N,0.5976878612716763,"Next, we bound R(k)
1 , using the assumption that θ ∈Θ. Note that from our assumption on θ, we
have yif(xi) + (yi −1)f(xi) ≥0, which implies that f(xi) ≥0 if yi = 1 and f(xi) ≤0 if
yi = 0. Thus, if yi = 1, then (yi −S(f(xi)))fk(gk(xi)) = (1 −S(f(xi)))fk(gk(xi)) ≥0, since
f(xi) ≥0 and (1 −S(f(xi))) ≥0 due to the fact that S(f(xi)) ∈(0, 1). A similar argument leads
to (yi −S(f(xi)))fk(gk(xi)) ≥0 if yi = 0. So, we have (yi −S(f(xi)))fk(gk(xi)) ≥0 for all
i ∈[n]."
N,0.5988439306358382,"Therefore, noting that Eλ∼˜
Dλ[1 −λ] ≥0, we compute:"
N,0.6,"R(k)
1
= Eλ∼˜
Dλ[1 −λ] n n
X"
N,0.6011560693641619,"i=1
|yi −S(f(xi))||fk(gk(xi))|
(84)"
N,0.6023121387283237,"= Eλ∼˜
Dλ[1 −λ] n n
X"
N,0.6034682080924856,"i=1
|S(f(xi)) −yi|∥∇kf(gk(xi))∥2∥gk(xi)∥2| cos(∇kf(gk(xi)), gk(xi))| (85) ≥1 n n
X"
N,0.6046242774566474,"i=1
|S(f(xi)) −yi|∥∇kf(gk(xi))∥2(Eλ∼˜
Dλ[1 −λ]r(k)
i
c(k)
x
p"
N,0.6057803468208093,"dk)
(86) = 1 n n
X"
N,0.6069364161849711,"i=1
|S(f(xi)) −yi|∥∇f(xi)∥2"
N,0.608092485549133,"
Eλ∼˜
Dλ[1 −λ]∥∇kf(gk(xi))∥2"
N,0.6092485549132948,"∥∇f(xi)∥2
r(k)
i
c(k)
x
p dk"
N,0.6104046242774567,"
. (87)"
N,0.6115606936416185,"Note that R(k)
3
= 0 as a consequence of our assumption that ∇2
kf(gk(xi)) = 0 for all i ∈[n], k ∈S,
and similar argument leads to:"
N,0.6127167630057804,"Radd(k)
2
= 1"
N,0.6138728323699422,"2n n
X"
N,0.6150289017341041,"i=1
|S(f(xi))(1 −S(f(xi)))|∇kf(gk(xi))T Eξk[ξadd
k
(ξadd
k
)T ]∇kf(gk(xi)) (88) = 1"
N,0.6161849710982659,"2n n
X"
N,0.6173410404624278,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2"
N,0.6184971098265896,"× Eξk[∥ξadd
k
∥2
2 cos(∇kf(gk(xi)), ξadd
k
)2]
(89)"
N,0.6196531791907515,"Rmult(k)
2
= 1"
N,0.6208092485549133,"2n n
X"
N,0.6219653179190752,"i=1
|S(f(xi))(1 −S(f(xi)))|∇kf(gk(xi))T (Eξk[ξadd
k
(ξadd
k
)T ] ⊙gk(xi)gk(xi)T )"
N,0.623121387283237,× ∇kf(gk(xi)) = 1
N,0.6242774566473989,"2n n
X"
N,0.6254335260115607,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2"
N,0.6265895953757226,"× Eξk[∥ξmult
k
⊙gk(xi)∥2
2 cos(∇kf(gk(xi)), ξmult ⊙gk(xi))2].
(90)"
N,0.6277456647398844,Published as a conference paper at ICLR 2022
N,0.6289017341040463,"Using Theorem 3 and the above results, we obtain:"
N,0.630057803468208,"LNF M
n
−1 n n
X"
N,0.63121387283237,"i=1
l(f(xi), yi)"
N,0.6323699421965318,"≥Ek[ϵR(k)
1
+ ϵ2R(k)
2
+ ϵ2Radd(k)
2
+ ϵ2Rmult(k)
2
+ ϵ2ϕ(ϵ)]
(91) ≥1 n n
X"
N,0.6335260115606937,"i=1
|S(f(xi)) −yi|∥∇f(xi)∥2ϵmix
i
(92) + 1"
N,0.6346820809248555,"2n n
X"
N,0.6358381502890174,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇f(xi)∥2
2(ϵmix
i
)2 + 1"
N,0.6369942196531792,"2n n
X"
N,0.638150289017341,"i=1
|S(f(xi))(1 −S(f(xi)))|∥∇kf(gk(xi))∥2
2 · Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2"
N,0.6393063583815028,"× cos(∇kf(gk(xi)), gk(xr))2]
(93) + 1"
N,0.6404624277456648,"2n n
X"
N,0.6416184971098265,"i=1
|S(f(xi))(1 −S(f(xi)))|(ϵnoise
i
)2 + ϵ2ϕ(ϵ),
(94)"
N,0.6427745664739885,"where ϵmix
i
:= ϵEλ∼˜
Dλ[1 −λ]Ek
h
∥∇kf(gk(xi))∥2"
N,0.6439306358381502,"∥∇f(xi)∥2
r(k)
i
c(k)
x
√dk
i
and"
N,0.6450867052023121,"(ϵnoise
i
)2 = ϵ2∥∇kf(gk(xi))∥2
2"
N,0.6462427745664739,"
σ2
addEξk[∥ξadd
k
∥2
2 cos(∇kf(gk(xi)), ξadd
k
)2]"
N,0.6473988439306358,"+ σ2
multEξk[∥ξmult
k
⊙gk(xi)∥2
2 cos(∇kf(gk(xi)), ξmult
k
⊙gk(xi))2]

.
(95)"
N,0.6485549132947976,"On the other hand, for any small parameters ϵi > 0 and any inputs z1, . . . , zn, we can, using a
second-order Taylor expansion and then applying our assumptions, compute:"
N,0.6497109826589595,"1
n n
X"
N,0.6508670520231213,"i=1
max
∥δi∥2≤ϵi l(f(zi + δi), yi) −1 n n
X"
N,0.6520231213872832,"i=1
l(f(zi), yi) ≤1 n n
X"
N,0.653179190751445,"i=1
|S(f(zi)) −yi|∥∇f(zi)∥2ϵi + 1"
N,0.6543352601156069,"2n n
X"
N,0.6554913294797687,"i=1
|S(f(zi))(1 −S(f(zi)))|∥∇f(zi)∥2
2ϵ2
i + 1 n n
X"
N,0.6566473988439306,"i=1
max
∥δi∥2≤ϵi ∥δi∥2
2ϕ′
i(δi)
(96) ≤1 n n
X"
N,0.6578034682080924,"i=1
|S(f(zi)) −yi|∥∇f(zi)∥2ϵi + 1"
N,0.6589595375722543,"2n n
X"
N,0.6601156069364161,"i=1
|S(f(zi))(1 −S(f(zi)))|∥∇f(zi)∥2
2ϵ2
i + 1 n n
X"
N,0.661271676300578,"i=1
ϵ2
i ϕ′′
i (ϵi),
(97)"
N,0.6624277456647398,"where the ϕ′
i are functions such that limz→0 ϕ′
i(z) = 0, ϕ′′
i (ϵi) := max∥δi∥2≤ϵi ϕ′
i(δi) and
limz→0 ϕ′′
i (z) = 0."
N,0.6635838150289017,"Combining (94) and (97), we see that"
N,0.6647398843930635,"LNF M
n
≥1 n n
X"
N,0.6658959537572254,"i=1
max
∥δmix
i
∥2≤ϵmix
i
l(f(xi + δmix
i
), yi) + Lreg
n
+ ϵ2ϕ(ϵ) −1 n n
X"
N,0.6670520231213873,"i=1
(ϵmix
i
)2ϕ′′
i (ϵmix
i
) (98) =: 1 n n
X"
N,0.6682080924855491,"i=1
max
∥δmix
i
∥2≤ϵmix
i
l(f(xi + δmix
i
), yi) + Lreg
n
+ ϵ2φ(ϵ),
(99)"
N,0.669364161849711,"where Lreg
n
is deﬁned in the theorem. Noting that limϵ→0 φ(ϵ) = 0, the proof is done."
N,0.6705202312138728,Published as a conference paper at ICLR 2022
N,0.6716763005780347,"Remark 1. Had we assumed that Er∼Dx[gk(r)] ̸= 0, then the statements of Theorem 4 remain
unchanged, but with (ϵreg
i
)2 replaced by"
N,0.6728323699421965,"(ϵreg
i
)2 = ϵ2∥∇kf(gk(xi))∥2
2"
N,0.6739884393063584,"
Eλ[(1 −λ)]2Exr[∥gk(xr)∥2
2 cos(∇kf(gk(xi)), gk(xr))2]"
N,0.6751445086705202,"+ σ2
addEξ[∥ξadd∥2
2 cos(∇kf(gk(xi)), ξadd)2]"
N,0.6763005780346821,"+ σ2
multEξ[∥ξmult ⊙gk(xi)∥2
2 cos(∇kf(gk(xi)), ξmult ⊙gk(xi))2]
"
N,0.6774566473988439,"−ϵ2Eλ[(1 −λ)]2∇kf(gk(xi))T [Ergk(r)gk(xi)T + gk(xi)Ergk(r)T ]∇kf(gk(xi)).
(100)"
N,0.6786127167630058,"C
NFM THROUGH THE LENS OF IMPLICIT REGULARIZATION AND
CLASSIFICATION MARGIN"
N,0.6797687861271676,"First, we deﬁne classiﬁcation margin at the input level. We shall show that minimizing the NFM
loss can lead to an increase in the classiﬁcation margin, and therefore improve model robustness in
this sense.
Deﬁnition 2 (Classiﬁcation Margin). The classiﬁcation margin of a training input-label sample
si := (xi, ci) measured by the Euclidean metric d is deﬁned as the radius of the largest d-metric ball
in X centered at xi that is contained in the decision region associated with the class label ci, i.e., it
is: γd(si) = sup{a : d(xi, x) ≤a ⇒g(x) = ci ∀x}."
N,0.6809248554913295,"Intuitively, a larger classiﬁcation margin allows a classiﬁer to associate a larger region centered
on a point xi in the input space to the same class. This makes the classiﬁer less sensitive to input
perturbations, and a perturbation of xi is still likely to fall within this region, keeping the classiﬁer
prediction. In this sense, the classiﬁer becomes more robust. In the typical case, the networks are
trained by a loss (cross-entropy) that promotes separation of different classes in the network output.
This, in turn, maximizes a certain notion of score of each training sample (Sokoli´c et al., 2017).
Deﬁnition 3 (Score). For an input-label training sample si = (xi, ci), we deﬁne its score as
o(si) = minj̸=ci
√"
N,0.6820809248554913,"2(eci −ej)T f(xi) ≥0, where ei ∈RK is the Kronecker delta vector (one-hot
vector) with ei
i = 1 and ej
i = 0 for i ̸= j."
N,0.6832369942196532,"A positive score implies that at the network output, classes are separated by a margin that corresponds
to the score. A large score may not imply a large classiﬁcation margin, but score can be related to
classiﬁcation margin via the following bound.
Proposition 1. Assume that the score o(si) > 0 and let k ∈S. Then, the classiﬁcation margin for
the training sample si can be lower bounded as:"
N,0.684393063583815,"γd(si) ≥
C(si)
supx∈conv(X) ∥∇kf(gk(x))∥2
,
(101)"
N,0.6855491329479769,where C(si) = o(si)/ supx∈conv(X) ∥∇gk(x)∥2.
N,0.6867052023121387,"Since NFM implicitly reduces the feature-output Jacobians ∇kf (including the input-output Jacobian)
according to the mixup level and noise levels (see Proposition 3), this, together with Theorem 1,
suggests that applying NFM implicitly increases the classiﬁcation margin, thereby making the model
more robust to input perturbations. We note that a similar, albeit more involved, bound can also be
obtained for the all-layer margin, a more reﬁned version of classiﬁcation margin introduced in (Wei
& Ma, 2019b), and the conclusion that applying NFM implicitly increases the margin also holds."
N,0.6878612716763006,We now prove the proposition.
N,0.6890173410404624,"Proof of Proposition 1. Note that, for any k ∈S, ∇f(x) = ∇kf(gk(x))∇gk(x) by the chain rule,
and so
∥∇f(x)∥2 ≤∥∇kf(gk(x))∥2∥∇gk(x)∥2
(102) ≤ "
N,0.6901734104046243,"sup
x∈conv(X)
∥∇kf(gk(x))∥2 !"
N,0.6913294797687861,"sup
x∈conv(X)
∥∇gk(x)∥2 !"
N,0.692485549132948,".
(103)"
N,0.6936416184971098,Published as a conference paper at ICLR 2022
N,0.6947976878612717,"The statement in the proposition follows from a straightforward application of Theorem 4 in (Sokoli´c
et al., 2017) together with the above bound."
N,0.6959537572254335,"D
NFM THROUGH THE LENS OF PROBABILISTIC ROBUSTNESS"
N,0.6971098265895954,"Since the main novelty of NFM lies in the introduction of noise injection, it would be insightful
to isolate the robustness boosting beneﬁts of injecting noise on top of manifold mixup. We shall
demonstrate the isolated beneﬁt in this section."
N,0.6982658959537572,"The key idea is based on the observation that manifold mixup produces minibatch outputs that lie in
the convex hull of the feature space at each iteration. Therefore, for k ∈S, NFM(k) can be viewed
as injecting noise to the layer k features sampled from some distribution over conv(gk(X)), and so
the NFM(k) neural network Fk can be viewed as a probabilistic mapping from conv(gk(X)) to
P(Y), the space of probability distributions on Y."
N,0.6994219653179191,"To isolate the beneﬁt of noise injection, we adapt the approach of (Pinot et al., 2019a; 2021) to our
setting to show that the Gaussian noise injection procedure in NFM robustiﬁes manifold mixup in a
probabilistic sense. At its core, this probabilistic notion of robustness amounts to making the model
locally Lipschitz with respect to some distance on the input and output space, ensuring that a small
perturbation in the input will not lead to large changes (as measured by some probability metric) in
the output. Interestingly, it is related to a notion of differential privacy (Lecuyer et al., 2019; Dwork
et al., 2014), as formalized in (Pinot et al., 2019b)."
N,0.7005780346820809,We now formalize this probabilistic notion of robustness.
N,0.7017341040462428,"Let p > 0. We say that a standard model f : X →Y is αp-robust if for any (x, y) ∼D such that
f(x) = y, one has, for any data perturbation τ ∈X,"
N,0.7028901734104046,"∥τ∥p ≤αp =⇒f(x) = f(x + τ).
(104)"
N,0.7040462427745665,Analogous deﬁnition can be formulated when output of the model is distribution-valued.
N,0.7052023121387283,"Deﬁnition 4 (Probabilistic robustness). A probabilistic model F : X →P(Y) is called (αp, ϵ)-robust
with respect to D if, for any x, τ ∈X, one has"
N,0.7063583815028902,"∥τ∥p ≤αp =⇒D(F(x), F(x + τ)) ≤ϵ,
(105)"
N,0.707514450867052,where D is a metric or divergence between two probability distributions.
N,0.7086705202312139,"We refer to the probabilistic model (built on top of a manifold mixup classiﬁer) that injects Gaus-
sian noise to the layer k features as probabilistic FM model, and we denote it by F noisy(k) :
conv(gk(X)) →P(Y). We denote G as the classiﬁer constructed from F noisy(k), i.e., G : x 7→
arg maxj∈[K][F noisy(k)]j(x)."
N,0.7098265895953757,"In the sequel, we take D to be the total variation distance DT V , deﬁned as:"
N,0.7109826589595376,"DT V (P, Q) := sup
S⊂X
|P(S) −Q(S)|,
(106)"
N,0.7121387283236994,"for any two distributions P and Q over X. Recall that if P and Q have densities ρp and ρq respectively,
then the total variation distance is half of the L1 distance, i.e., DT V (P, Q) = 1"
R,0.7132947976878613,"2
R"
R,0.7144508670520231,"X |ρp(x)−ρq(x)|dx.
The choice of the distance depends on the problem on hand and will give rise to different notions of
robustness. One could also consider other statistical distances such as the Wasserstein distance and
Renyi divergence, which can be related to total variation (see (Pinot et al., 2021; Gibbs & Su, 2002)
for details)."
R,0.715606936416185,"Before presenting our main result in this section, we need the following notation. Let Σ(x) :=
σ2
addI + σ2
multxxT . For x, τ ∈X, let Πx be a dk by dk −1 matrix whose columns form a basis
for the subspace orthogonal to gk(x + τ) −gk(x), and {ρi(gk(x), τ)}i∈[dk−1] be the eigenvalues of
(ΠT
x Σ(gk(x))Πx)−1ΠT
x Σ(gk(x + τ))Πx −I. Also, let [F]topk(x) denote the kth highest value of
the entries in the vector F(x)."
R,0.7167630057803468,"Viewing an NFM(k) classiﬁer as a probabilistic FM classiﬁer, we have the following result."
R,0.7179190751445087,Published as a conference paper at ICLR 2022
R,0.7190751445086705,"Theorem 5 (Gaussian noise injection robustiﬁes FM classiﬁers). Let k ∈S, dk > 1, and assume
that gk(x)gk(x)T ≥β2
kI > 0 for all x ∈conv(X) for some constant βk. Then, F noisy(k) is
(αp, ϵk(p, d, αp, σadd, σmult))-robust with respect to DT V against lp adversaries, with"
R,0.7202312138728324,"ϵk(p, d, αp, σadd, σmult) = 9"
R,0.7213872832369942,"2 min{1, max{A, B}},
(107) where"
R,0.7225433526011561,"A = Ap(αp)
σ2
mult
σ2
add + σ2
multβ2
k  Z 1"
R,0.7236994219653179,"0
∇gk(x + tτ)dt 2"
R,0.7248554913294798,"2
+ 2∥gk(x)∥2  Z 1"
R,0.7260115606936416,"0
∇gk(x + tτ)dt

2 
, (108)"
R,0.7271676300578035,"B = Bk(τ)αp(1p∈(0,2] + d1/2−1/p1p∈(2,∞) +
√"
R,0.7283236994219653,"d1p=∞)
p"
R,0.7294797687861272,"σ2
add + σ2
multβ2
k
,
(109) with"
R,0.730635838150289,"Ap(αp) = 
 "
R,0.7317919075144509,"αp1αp<1 + α2
p1αp≥1,
if p ∈(0, 2],
d1/2−1/p(αp1αp<1 + α2
p1αp≥1),
if p ∈(2, ∞),
√"
R,0.7329479768786127,"d(αp1αp<1 + α2
p1αp≥1),
if p = ∞,
(110) and"
R,0.7341040462427746,"Bk(τ) =
sup
x∈conv(X)  Z 1"
R,0.7352601156069364,"0
∇gk(x + tτ)dt

2
·"
R,0.7364161849710983,"v
u
u
t"
R,0.7375722543352601,"dk−1
X"
R,0.738728323699422,"i=1
ρ2
i (gk(x), τ)

.
(111)"
R,0.7398843930635838,"Moreover, if x ∈X is such that [F noisy(k)]top1(x) ≥[F noisy(k)]top2(x) + 2ϵ(p, d, αp, σadd, σmult),
then for any τ ∈X, we have"
R,0.7410404624277457,"∥τ∥p ≤α =⇒G(x) = G(x + τ),
(112)"
R,0.7421965317919075,for any p > 0.
R,0.7433526011560694,"Theorem 5 implies that we can inject Gaussian noise into the feature mixup representation to improve
robustness of FM classiﬁers in the sense of Deﬁnition 4, while keeping track of maximal loss in
accuracy incurred under attack, by tuning the noise levels σadd and σmult. To illustrate this, suppose
that σmult = 0 and consider the case of p = 2, in which case A = 0, B ∼α2/σadd and so injecting
additive Gaussian noise can help controlling the change in the model output, keeping the classiﬁer’s
prediction, when the data perturbation is of size α2."
R,0.7445086705202312,"We now prove Theorem 5. Before this, we need the following lemma.
Lemma 3. Let x1 := z ∈Rdk and x2 := z + τ ∈Rdk, with τ > 0 and dk > 1, and Σ(x) :=
σ2
addI +σ2
multxxT ≥(σ2
add +σ2
multβ2)I > 0, for some constant β, for all x. Let Π be a dk by dk −1
matrix whose columns form a basis for the subspace orthogonal to τ, and let ρ1(z, τ), . . . , ρdk−1(z, τ)
denote the eigenvalues of (ΠT Σ(x1)Π)−1ΠT Σ(x2)Π −I."
R,0.7456647398843931,"Deﬁne the function C(x1, x2, Σ) := max{A, B}, where"
R,0.7468208092485549,"A =
σ2
mult
σ2
add + σ2
multβ2 (∥τ∥2
2 + 2τ T z),
(113)"
R,0.7479768786127168,"B =
∥τ∥2
p"
R,0.7491329479768786,"σ2
add + σ2
multβ2"
R,0.7502890173410405,"v
u
u
t"
R,0.7514450867052023,"dk−1
X"
R,0.7526011560693642,"i=1
ρ2
i (z, τ).
(114)"
R,0.753757225433526,"Then, the total variation distance between N(x1, Σ(x1)) and N(x2, Σ(x2)) admits the following
bounds:
1
200 ≤DT V (N(x1, Σ(x1)), N(x2, Σ(x2)))"
R,0.7549132947976879,"min{1, C(x1, x2, Σ)}
≤9"
R,0.7560693641618497,"2.
(115)"
R,0.7572254335260116,"Proof of Lemma 3. The result follows from a straightforward application of Theorem 1.2 in (Devroye
et al., 2018), which provides bounds on the total variation distance between Gaussians with different
means and covariances."
R,0.7583815028901734,Published as a conference paper at ICLR 2022
R,0.7595375722543353,"With this lemma in hand, we now prove Theorem 5."
R,0.7606936416184971,"Proof of Theorem 5. We denote the noise injection procedure by the map I : x →N(x, Σ(x)),
where Σ(x) = σ2
addI + σ2
multxxT ."
R,0.761849710982659,Let x ∈X be a test datapoint and τ ∈X be a data perturbation such that ∥τ∥p ≤αp for p > 0.
R,0.7630057803468208,Note that
R,0.7641618497109827,"DT V (Fk(I(gk(x))), Fk(I(gk(x + τ)))) ≤DT V (I(gk(x)), I(gk(x + τ)))
(116)
≤DT V (I(gk(x)), I(gk(x) + gk(x + τ) −gk(x)))
(117)"
R,0.7653179190751445,"= DT V (I(gk(x)), I (gk(x) + τk))
(118) ≤9"
R,0.7664739884393064,"2 min{1, Φ(gk(x), τk, σadd, σmult, βk)},
(119)"
R,0.7676300578034682,"where τk := gk(x + τ) −gk(x) =
R 1
0 ∇gk(x + tτ)dt

τ by the generalized fundamental theorem
of calculus, and"
R,0.7687861271676301,"Φ(gk(x), τk, σadd, σmult, βk)"
R,0.7699421965317919,":= max 
"
R,0.7710982658959538,"
σ2
mult
σ2
add + σ2
multβ2
k
(∥τk∥2
2 + 2⟨τk, gk(x)⟩),
∥τk∥2
p"
R,0.7722543352601156,"σ2
add + σ2
multβ2
k"
R,0.7734104046242775,"v
u
u
t"
R,0.7745664739884393,"dk−1
X"
R,0.7757225433526012,"i=1
ρ2
i (gk(x), τ) 
 , (120)"
R,0.776878612716763,"where the ρi(gk(x), τ) are the eigenvalues given in the theorem."
R,0.7780346820809249,"In the ﬁrst line above, we have used the data preprocessing inequality (Theorem 6 in (Pinot
et al., 2021)), and the last line follows from applying Lemma 3 together with the assumption
that gk(x)gk(x)T ≥β2
k > 0 for all x."
R,0.7791907514450868,Using the bounds
R,0.7803468208092486,∥τk∥2 ≤ Z 1
R,0.7815028901734105,"0
∇gk(x + tτ)dt

2
∥τ∥2
(121) and"
R,0.7826589595375723,"|⟨τk, gk(x)⟩| ≤∥gk(x)∥2  Z 1"
R,0.7838150289017342,"0
∇gk(x + tτ)dt

2
∥τ∥2,
(122)"
R,0.784971098265896,we have
R,0.7861271676300579,"Φ(gk(x), τk, σadd, σmult, βk) ≤max {A, B} ,
(123) where"
R,0.7872832369942196,"A =
σ2
mult
σ2
add + σ2
multβ2
k  Z 1"
R,0.7884393063583816,"0
∇gk(x + tτ)dt 2"
R,0.7895953757225433,"2
∥τ∥2
2 + 2∥gk(x)∥2  Z 1"
R,0.7907514450867053,"0
∇gk(x + tτ)dt

2
∥τ∥2 "
R,0.791907514450867,"(124)
and B ="
R,0.793063583815029,"R 1
0 ∇gk(x + tτ)dt

2 ∥τ∥2
p"
R,0.7942196531791907,"σ2
add + σ2
multβ2
k"
R,0.7953757225433526,"v
u
u
t"
R,0.7965317919075144,"dk−1
X"
R,0.7976878612716763,"i=1
ρ2
i (gk(x), τ)
(125)"
R,0.7988439306358381,"≤
sup
x∈conv(X)  Z 1"
R,0.8,"0
∇gk(x + tτ)dt

2
·"
R,0.8011560693641618,"v
u
u
t"
R,0.8023121387283237,"dk−1
X"
R,0.8034682080924855,"i=1
ρ2
i (gk(x), τ)

∥τ∥2
p"
R,0.8046242774566474,"σ2
add + σ2
multβ2
k
(126)"
R,0.8057803468208092,"=: Bk(τ)
∥τ∥2
p"
R,0.8069364161849711,"σ2
add + σ2
multβ2
k
.
(127)"
R,0.8080924855491329,Published as a conference paper at ICLR 2022
R,0.8092485549132948,"The ﬁrst statement of the theorem then follows from the facts that ∥τ∥2 ≤∥τ∥p ≤αp for p ∈(0, 2],
∥τ∥2 ≤d1/2−1/q∥τ∥q ≤d1/2−1/qαq for q > 2, and ∥τ∥2 ≤
√"
R,0.8104046242774566,"d∥τ∥∞≤
√"
R,0.8115606936416185,"dα∞for any τ ∈Rd.
In particular, these imply that A ≤CAp, where Ap = 
 "
R,0.8127167630057803,"αp1αp<1 + α2
p1αp≥1,
if p ∈(0, 2],
d1/2−1/p(αp1αp<1 + α2
p1αp≥1),
if p ∈(2, ∞),
√"
R,0.8138728323699422,"d(αp1αp<1 + α2
p1αp≥1),
if p = ∞,
(128) and"
R,0.815028901734104,"C :=
σ2
mult
σ2
add + σ2
multβ2
k  Z 1"
R,0.8161849710982659,"0
∇gk(x + tτ)dt 2"
R,0.8173410404624277,"2
+ 2∥gk(x)∥2  Z 1"
R,0.8184971098265896,"0
∇gk(x + tτ)dt

2"
R,0.8196531791907514,"
.
(129)"
R,0.8208092485549133,"The last statement in the theorem essentially follows from Proposition 3 in (Pinot et al., 2021)."
R,0.8219653179190751,"E
ON GENERALIZATION BOUNDS FOR NFM"
R,0.823121387283237,"Let F be the family of mappings x 7→f(x) and Zn := ((xi, yi))i∈[n]. Given a loss function l, the
Rademacher complexity of the set l ◦F := {(x, y) 7→l(f(x), y) : f ∈F} is deﬁned as:"
R,0.8242774566473988,"Rn(l ◦F) := EZn,σ """
R,0.8254335260115607,"sup
f∈F"
N,0.8265895953757225,"1
n n
X"
N,0.8277456647398844,"i=1
σil(f(xi), yi) #"
N,0.8289017341040462,",
(130)"
N,0.8300578034682081,"where σ := (σ1, . . . , σn), with the σi independent uniform random variables taking values in
{−1, 1}."
N,0.8312138728323699,"Following (Lamb et al., 2019), we can derive the following generalization bound for the NFM
loss function, i.e., the upper bound on the difference between the expected error on unseen data
and the NFM loss. This bound shows that NFM can reduce overﬁtting and give rise to improved
generalization.
Theorem 6 (Generalization bound for the NFM loss). Assume that the loss function l satisﬁes
|l(x, y) −l(x′, y)| ≤M for all x, x′ and y. Then, for every δ > 0, with probability at least 1 −δ
over a draw of n i.i.d. samples {(xi, yi)}n
i=1, we have the following generalization bound: for all
maps f ∈F,"
N,0.8323699421965318,"Ex,y[l(f(x), y)] −LNF M
n
≤2Rn(l ◦F) + 2M r"
N,0.8335260115606936,ln(1/δ)
N,0.8346820809248555,"2n
−Qϵ(f),
(131)"
N,0.8358381502890173,"where
Qϵ(f) = E[ϵR(k)
1
+ ϵ2 ˜R(k)
2
+ ϵ2 ˜R(k)
3 ] + ϵ2ϕ(ϵ),
(132)
for some function ϕ such that limx→∞ϕ(x) = 0."
N,0.8369942196531792,"To compare the generalization behavior of NFM with that without using NFM, we also need the
following generalization bound for the standard loss function.
Theorem 7 (Generalization bound for the standard loss). Assume that the loss function l satisﬁes
|l(x, y) −l(x′, y)| ≤M for all x, x′ and y. Then, for every δ > 0, with probability at least 1 −δ
over a draw of n i.i.d. samples {(xi, yi)}n
i=1, we have the following generalization bound: for all
maps f ∈F,"
N,0.838150289017341,"Ex,y[l(f(x), y)] −Lstd
n
≤2Rn(l ◦F) + 2M r"
N,0.8393063583815029,ln(1/δ)
N,0.8404624277456647,"2n
.
(133)"
N,0.8416184971098266,"By comparing the above two theorems and following the argument of (Lamb et al., 2019), we see
that the generalization beneﬁt of NFM comes from two mechanisms. The ﬁrst mechanism is based
on the term Qϵ(f). Assuming that the Rademacher complexity term is the same for both methods,
then NFM has a better generalization bound than that of standard method if Qϵ(f) > 0. The second
mechanism is based on the Rademacher complexity term Rn(l ◦F). For certain families of neural
networks, this term can be bounded by the norms of the hidden layers of the network and the norms
of the Jacobians of each layer with respect to all previous layers (Wei & Ma, 2019a;b). Therefore,"
N,0.8427745664739884,Published as a conference paper at ICLR 2022
N,0.8439306358381503,"this term differs for the case of training using NFM and the case of standard training. Since NFM
implicitly reduces the feature-output Jacobians (see Theorem 3), we can argue that NFM leads to a
smaller Rademacher complexity term and hence a better generalization bound."
N,0.8450867052023121,We now prove Theorem 6. The proof of Theorem 7 follows the same argument as that of Theorem 6.
N,0.846242774566474,"Proof of Theorem 6. Let Zn := {(xi, yi)}i∈[n] and Z′
n := {(x′
i, y′
i)}i∈[n] be two test datasets, where
Z′
n differs from Zn by exactly one point of an arbitrary index i0."
N,0.8473988439306358,"Denote GE(Zn) := supf∈F Ex,y[l(f(x), y)] −LNF M
n
, where LNF M
n
is computed using the dataset
Zn, and likewise for GE(Z′
n). Then,"
N,0.8485549132947977,"GE(Z′
n) −GE(Zn) ≤M(2n −1)"
N,0.8497109826589595,"n2
≤2M"
N,0.8508670520231214,"n ,
(134)"
N,0.8520231213872832,"where we have used the fact that LNF M
n
has n2 terms and there are 2n −1 different terms for Zn
and Z′
n. Similarly, we have GE(Zn) −GE(Z′
n) ≤2M n ."
N,0.8531791907514451,"Therefore, by McDiarmid’s inequality, for any δ > 0, with probability at least 1 −δ,"
N,0.8543352601156069,GE(Zn) ≤EZn[GE(Zn)] + 2M r
N,0.8554913294797688,ln(1/δ)
N,0.8566473988439306,"2n
.
(135)"
N,0.8578034682080925,"Applying Theorem 3, we have"
N,0.8589595375722543,"GE(Zn) ≤EZn """
N,0.8601156069364162,"sup
f∈F
EZ′n ""
1
n n
X"
N,0.861271676300578,"i=1
l(f(x′
i), y′
i) #"
N,0.8624277456647399,"−LNF M
n # + 2M r"
N,0.8635838150289017,ln(1/δ)
N,0.8647398843930636,"2n
(136) = EZn """
N,0.8658959537572254,"sup
f∈F
EZ′n ""
1
n n
X"
N,0.8670520231213873,"i=1
l(f(x′
i), y′
i) # −1 n n
X"
N,0.8682080924855491,"i=1
l(f(xi), yi) #"
N,0.869364161849711,−Qϵ(f) + 2M r
N,0.8705202312138728,ln(1/δ)
N,0.8716763005780347,"2n
(137)"
N,0.8728323699421965,"≤EZn,Z′n """
N,0.8739884393063584,"sup
f∈F"
N,0.8751445086705202,"1
n n
X"
N,0.8763005780346821,"i=1
(l(f(x′
i), y′
i) −l(f(xi), yi)) #"
N,0.8774566473988439,−Qϵ(f) + 2M r
N,0.8786127167630058,ln(1/δ)
N,0.8797687861271676,"2n
(138)"
N,0.8809248554913295,"≤EZn,Z′n,σ """
N,0.8820809248554913,"sup
f∈F"
N,0.8832369942196532,"1
n n
X"
N,0.884393063583815,"i=1
σi(l(f(x′
i), y′
i) −l(f(xi), yi)) #"
N,0.8855491329479769,−Qϵ(f) + 2M r
N,0.8867052023121387,ln(1/δ)
N,0.8878612716763006,"2n
(139)"
N,0.8890173410404625,"≤2EZn,σ """
N,0.8901734104046243,"sup
f∈F"
N,0.8913294797687862,"1
n n
X"
N,0.892485549132948,"i=1
σil(f(xi), yi) #"
N,0.8936416184971099,−Qϵ(f) + 2M r
N,0.8947976878612717,ln(1/δ)
N,0.8959537572254336,"2n
(140)"
N,0.8971098265895954,= 2Rn(l ◦F) −Qϵ(f) + 2M r
N,0.8982658959537573,ln(1/δ)
N,0.8994219653179191,"2n
,
(141)"
N,0.900578034682081,"where (136) uses the deﬁnition of GE(Zn), (137) uses ± 1"
N,0.9017341040462428,"n
Pn
i=1 l(f(xi), yi) inside the expectation
and the linearity of expectation, (138) follows from the Jensen’s inequality and the convexity of
the supremum, (139) follows from the fact that σi(l(f(x′
i), y′
i) −l(f(xi), yi)) and l(f(x′
i), y′
i) −
l(f(xi), yi) have the same distribution for each σi ∈{−1, 1} (since Zn, Z′
n are drawn i.i.d. with the
same distribution), and (140) follows from the subadditivity of supremum."
N,0.9028901734104047,The bound in the theorem then follows from the above bound.
N,0.9040462427745665,Published as a conference paper at ICLR 2022
N,0.9052023121387284,"F
ADDITIONAL EXPERIMENTS AND DETAILS"
N,0.9063583815028902,"F.1
INPUT PERTURBATIONS"
N,0.9075144508670521,We consider the following three types of data perturbations during inference time:
N,0.9086705202312139,"• White noise perturbations are constructed as ˜x = x + ∆x, where the additive noise is sampled
from a Gaussian distribution ∆x ∼N(0, σ). This perturbation strategy emulates measurement
errors that can result from data acquisition with poor sensors (where σ corresponds to the severity
of these errors)."
N,0.9098265895953758,"• Salt and pepper perturbations emulate defective pixels that result from converting analog signals to
digital signals. The noise model takes the form P( ˜X = X) = 1 −γ, and P( ˜X = max) = P( ˜X =
min) = γ/2, where ˜X(i, j) denotes the corrupted image and min, max denote the minimum and
maximum pixel values, respectively. γ parameterizes the proportion of defective pixels."
N,0.9109826589595376,"• Adversarial perturbations are “worst-case” non-random perturbations that maximize the loss
ℓ(gδ(X + ∆X), y) subject to the constraint ∥∆X∥≤r on the norm of the perturbation. We
consider the projected gradient decent for constructing these perturbations (Madry et al., 2017)."
N,0.9121387283236995,"F.2
ILLUSTRATION OF THE EFFECTS OF NFM ON TOY DATASETS"
N,0.9132947976878613,"We consider a binary classiﬁcation task for the noise corrupted 2D dataset whose data points form
two concentric circles. Points on the same circle corresponds to the same label class. We generate
500 samples, setting the scale factor between inner and outer circle to be 0.05 and adding Gaussian
noise with zero mean and standard deviation of 0.3 to the samples. Fig. 8 shows the training and
test data points. We train a fully connected feedforward neural network that has four layers with the
ReLU activation functions on these data, using 300 points for training and 200 for testing. All models
are trained with Adam and learning rate 0.1, and the seed is ﬁxed across all experiments. Note that
the learning rate can be considered as a temperature parameter which introduces some amount of
regularization itself. Hence, we choose a learning rate that is large for this problem to better illustrate
the regularization effects imposed by the different schemes that we consider."
N,0.9144508670520232,"Fig. 2 illustrates how different regularization strategies affect the decision boundaries of the neural
network classiﬁer. The decision boundaries and the test accuracy indicate that white noise injections
and dropout (we explore dropout rates in the range [0.0, 0.9] and we ﬁnds that 0.2 yields the best
performance) introduce a favorable amount of regularization. Most notably is the effect of weight
decay (we use 9e−3), i.e., the decision boundary is nicely smoothed and the test accuracy is improved.
In contrast, the simple mixup data augmentation scheme shows no beneﬁts here, whereas manifold
mixup is improving the predictive accuracy considerably. Combining mixup (manifold mixup) with
noise injections yields the best performance in terms of both smoothness of the decision boundary
and predictive accuracy. Indeed, NFM is outperforming all other methods here."
N,0.915606936416185,"The performance could be further improved by combining NFM with weight decay or dropout. This
shows that there are interaction effects between different regularization schemes. In practice, when"
N,0.9167630057803469,"(a) Data points for training.
(b) Data points for testing."
N,0.9179190751445087,Figure 8: The toy dataset in R2 that we use for binary classiﬁcation.
N,0.9190751445086706,Published as a conference paper at ICLR 2022
N,0.9202312138728324,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35 60 70 80 90"
N,0.9213872832369943,"Baseline
Mixup (
= 0.1)"
N,0.922543352601156,"Noisy Mixup (
= 0.1)
Manifold Mixup (
= 0.1)
Noisy Feature Mixup (*)
Noisy Feature Mixup (
= 0.1)"
N,0.923699421965318,Test Accuracy
N,0.9248554913294798,White Noise (σ)
N,0.9260115606936417,"0.00
0.05
0.10
0.15
0.20 60 70 80 90"
N,0.9271676300578034,"Baseline
Mixup (
= 0.1)"
N,0.9283236994219654,"Noisy Mixup (
= 0.1)
Manifold Mixup (
= 0.1)
Noisy Feature Mixup (*)
Noisy Feature Mixup (
= 0.1)"
N,0.9294797687861271,Salt and Pepper Noise (γ)
N,0.930635838150289,Figure 9: Vision transformers evaluated on CIFAR-10 with different training schemes.
N,0.9317919075144508,"Table 4: Robustness of Wide-ResNet-18 w.r.t. white noise (σ) and salt and pepper (γ) perturbations
evaluated on CIFAR-100. The results are averaged over 5 models trained with different seed values."
N,0.9329479768786128,"Scheme
Clean (%)
σ (%)
γ (%)
0.1
0.2
0.3
0.08
0.12
0.2"
N,0.9341040462427745,"Baseline
91.3
89.4
77.0
56.7
83.2
74.6
48.6
Mixup (α = 0.1) Zhang et al. (2017)
91.2
89.5
77.6
57.7
82.9
74.6
48.6
Mixup (α = 0.2) Zhang et al. (2017)
91.2
89.2
77.8
58.9
82.6
74.5
47.9
Noisy Mixup (α = 0.1) Yang et al. (2020b)
90.9
90.4
87.5
80.2
84.0
79.4
63.8
Noisy Mixup (α = 0.2) Yang et al. (2020b)
90.9
90.4
87.4
79.8
83.8
79.3
63.4
Manifold Mixup (α = 0.1) Verma et al. (2019)
91.2
89.2
77.2
56.9
83.0
74.3
47.1
Manifold Mixup (α = 1.0) Verma et al. (2019)
90.2
88.4
76.0
55.1
81.3
71.4
42.7
Manifold Mixup (α = 2.0) Verma et al. (2019)
89.0
87.0
74.3
53.7
79.8
70.3
41.9
Noisy Feature Mixup (α = 0.1)
91.4
90.2
88.2
84.8
84.4
81.2
74.4
Noisy Feature Mixup (α = 1.0)
89.8
89.1
86.6
82.7
82.5
79.0
71.4
Noisy Feature Mixup (α = 2.0)
88.4
87.6
84.6
80.1
80.4
76.5
68.6"
N,0.9352601156069364,"one trains deep neural networks, different regularization strategies are considered as knobs that are
ﬁne-tuned. From this perspective, NFM provides additional knobs to further improve a model."
N,0.9364161849710982,"F.3
ADDITIONAL RESULTS FOR VISION TRANSFORMERS"
N,0.9375722543352601,"Here we consider compact vision transformer (ViT-lite) with 7 attention layers and 4 heads (Hassani
et al., 2021). Fig. 9 (left) compares vision transformers trained with different data augmentation
strategies. Again, NFM improves the robustness of the models while achieving state-of-the-art
accuracy when evaluated on clean data. However, mixup and manifold mixup do not boost the
robustness. Further, Fig. 9 (right) shows that that the vision transformer is less sensitive to salt and
pepper perturbations as compared to the ResNet model. These results are consistent with the high
robustness properties of transformers recently reported in Shao et al. (2021); Paul & Chen (2021).
Table 4 provides additional results for different α values."
N,0.9387283236994219,"Table 4 shows results for vision transformers trained with different data augmentation schemes and
different values of α. It can be seen that NFM with α = 0.1 helps to improve the predictive accuracy
on clean data while also improving the robustness of the models. For example, the model trained
with NFM shows about a 25% improvement compared to the baseline model when faced with salt
and paper perturbations (γ = 0.2). Further, our results indicate that larger values of α have a negative
effect on the generalization performance of vision transformer."
N,0.9398843930635838,"F.4
ABLATION STUDY"
N,0.9410404624277456,"In Table 5 we provide a detailed ablation study where we vary several knobs. First, we can see that
just injecting noise helps to improve robustness, but the test accuracy is only marginally improving.
On the other hand, just mixing inputs and hidden features improves the testing performance of the
model, but it does not signiﬁcantly improve the robustness of a model. In contrast, the NFM scheme
combines best of both worlds and shows that both accuracy and robustness can be increased. Varying
the noise levels indicate that there is a trade-off between test accuracy on clean data and robustness to"
N,0.9421965317919075,Published as a conference paper at ICLR 2022
N,0.9433526011560693,"perturbations. We also vary the mixup parameter α to show that the good performance is consistent
across a range of different values."
N,0.9445086705202312,Table 5: Ablation study using Wide-ResNet-18 trained and evaluated on CIFAR-100.
N,0.945664739884393,"Mixup
Manifold
Noise Injections
α
Noise Levels
Clean (%)
σ (%)
γ (%)
σadd
σmult
0.1
0.25
0.5
0.06
0.1
0.15"
N,0.9468208092485549,"


-
0
0
76.9
64.6
42.0
23.5
58.1
39.8
15.1



-
0.4
0.2
78.1
76.2
65.7
46.6
70.0
58.8
28.4"
N,0.9479768786127167,"


1
0
0
80.3
72.5
54.0
33.4
62.5
43.8
16.2



1
0.4
0.2
78.9
78.6
66.6
46.7
66.6
53.4
25.9"
N,0.9491329479768786,"


0.2
0
0
79.7
70.6
46.6
25.3
62.1
43.0
15.2



1
0
0
79.7
70.5
45.0
23.8
62.1
42.8
14.8



2
0
0
79.2
69.3
43.8
23.0
62.8
44.2
16.0"
N,0.9502890173410404,"


1
0.1
0.1
81.0
76.2
56.6
36.4
66.8
49.7
21.4



0.2
0.4
0.2
80.6
79.2
70.2
51.7
71.5
60.4
30.3



1
0.4
0.2
80.9
80.1
72.1
55.3
72.8
62.1
34.4



2
0.4
0.2
80.7
80.0
71.5
53.9
72.7
62.7
36.6



1
0.8
0.4
80.3
80.1
75.5
66.4
74.3
66.5
44.6"
N,0.9514450867052023,"F.5
ADDITIONAL RESULTS FOR RESNETS WITH HIGHER LEVELS OF NOISE INJECTIONS"
N,0.9526011560693641,"In the experiments in Section 5, we considered models trained with NFM that use noise injection
levels σadd = 0.4 and σmult = 0.2, whereas the ablation model uses σadd = 1.0 and σmult = 0.5.
Here, we want to better illustrate the trade-off between accuracy and robustness. We saw that there
exists a potential sweet-spot where we are able to improve both the predictive accuracy and the
robustness of the model. However, if the primary aim is to push the robustness of the model, then we
need to sacriﬁce some amount of accuracy."
N,0.953757225433526,"Fig. 10 is illustrating this trade-off for pre-actived ResNet-18s trained on CIFAR-10. We can see
that increased levels of noise injections considerably improve the robustness, while the accuracy on
clean data points drops. In practice, the amount of noise injection that the user chooses depend on the
situation. If robustness is critical, than higher noise levels can be used. If adversarial examples are the
main concern, than other training strategies such as adversarial training might be favorable. However,
the advantage of NFM over adversarial training is that (a) we have a more favorable trade-off between
robustness and accuracy in the small noise regime, and (b) NFM is computationally inexpensive,
when compared to most adversarial training schemes. This is further illustrated in the next section."
N,0.9549132947976878,"F.6
COMPARISON WITH ADVERSARIAL TRAINED MODELS"
N,0.9560693641618497,"Here, we compare NFM to adversarial training in the small noise regime, i.e., the situation where
models do not show a signiﬁcant drop on the clean test set. Speciﬁcally, we consider the projected
gradient decent (PGD) method (Madry et al., 2017) using 7 attack iterations and varying l2 per-"
N,0.9572254335260115,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
70 75 80 85 90 95"
N,0.9583815028901734,"add = 0.4,
mult = 0.2"
N,0.9595375722543352,"add = 0.6,
mult = 0.3"
N,0.9606936416184971,"add = 0.8,
mult = 0.4"
N,0.9618497109826589,"add = 1.0,
mult = 0.5"
N,0.9630057803468208,"add = 1.2,
mult = 0.6"
N,0.9641618497109826,Test Accuracy
N,0.9653179190751445,White Noise (σ)
N,0.9664739884393063,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
70 75 80 85 90 95"
N,0.9676300578034682,Salt and Pepper Noise (γ)
N,0.96878612716763,"Figure 10: Pre-actived ResNet-18 evaluated on CIFAR-10 trained with NFM and varying levels of
additive (σadd) and multiplicative (σmult) noise injections. Shaded regions indicate one standard
deviation about the mean. Averaged across 5 random seeds."
N,0.9699421965317919,Published as a conference paper at ICLR 2022
N,0.9710982658959537,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
60 65 70 75 80 85 90 95"
N,0.9722543352601156,"= 0.001
= 0.002
= 0.005
= 0.01
NFM (
add = 0.4,
mult = 0.2)
NFM (
add = 1.2,
mult = 0.6)"
N,0.9734104046242774,Test Accuracy
N,0.9745664739884393,Adverserial Noise (ϵ)
N,0.9757225433526011,"0.00
0.02
0.04
0.06
0.08
0.10
0.12
60 65 70 75 80 85 90 95"
N,0.976878612716763,Adverserial Noise (ϵ)
N,0.9780346820809248,"Figure 11: Pre-actived ResNet-18 evaluated on CIFAR-10 (left) and Wide ResNet-18 evaluated on
CIFAR-100 (right) with respect to adversarial perturbed inputs. Shaded regions indicate one standard
deviation about the mean. Averaged across 5 random seeds."
N,0.9791907514450867,"turbation levels ϵ to train adversarial robust models. First, we compare how resilient the different
models are with respect to adversarial input perturbations during inference time (Fig. 11; left). Again
the adversarial examples are constructed using the PGD method with 7 attack iterations. Not very
surprisingly, the adversarial trained model with ϵ = 0.01 features the best resilience while sacriﬁcing
about 0.5% accuracy as compared to the baseline model (here not shown). In contrast, the models
trained with NFM are less robust, while being about 1 −1.5% more accurate on clean data."
N,0.9803468208092485,"Next, we compare in (Fig. 11; right) the robustness with respect to salt and pepper perturbations, i.e.,
perturbations that both models have not seen before. Interestingly, here we see an advantage of the
NFM scheme with high noise injection levels as compared to the adversarial trained models."
N,0.9815028901734104,"F.7
FEATURE VISUALIZATION COMPARISON"
N,0.9826589595375722,"In this subsection, we concern ourselves with comparing the features learned by three ResNet-
50 models trained on Restricted Imagenet (Tsipras et al., 2018): without mixup, manifold mixup
(Verma et al., 2019), and NFM. We can compare features by maximizing randomly chosen pre-logit
activations of each model with respect to the input, as described by Engstrom et al. (2020). We do so
for all models with Projected Gradient Ascent over 200 iterations, a step size of 16, and an ℓ2 norm
constraint of 2,000. Both the models trained with manifold mixup and NFM use an α = 0.2, and the
NFM model uses in addition σadd = 2.4 and σmult = 1.2. The result, as shown in Fig. 12, is that the
features learned by the model trained with NFM are slightly stronger (i.e., different from random
noise) than the clean model."
N,0.9838150289017341,"F.8
TRAIN AND TEST ERROR FOR CIFAR-100"
N,0.9849710982658959,"Figure 13 shows models trained with different training schemes on CIFAR-100. Compared to the
baseline model, the models trained with manifold mixup and NFM have a similar convergence
behavior. However, they are able to achieve a smaller test error. This shows that both manifold mixup
and NFM have a favorable implicit regularization effect, where the effect is more pronounced for the
NFM scheme."
N,0.9861271676300578,Published as a conference paper at ICLR 2022
N,0.9872832369942196,"Seed
No Mixup
Manifold Mixup
NFM (Ours)"
N,0.9884393063583815,"Figure 12: The features learned by the NFM classiﬁer are slightly stronger (i.e., different from
random noise) than the clean model. See Subsection F.7 for more details."
N,0.9895953757225433,"0
50
100
150
200
Number of epochs 0 20 40 60 80 100"
N,0.9907514450867052,Train error
N,0.991907514450867,"Baseline
Manifold Mixup (
= 1.0)
NFM (
= 1.0)"
N,0.9930635838150289,(a) Train error.
N,0.9942196531791907,"0
50
100
150
200
Number of epochs 0 20 40 60 80 100"
N,0.9953757225433526,Test error
N,0.9965317919075144,"Baseline
Manifold Mixup (
= 1.0)
NFM (
= 1.0)"
N,0.9976878612716763,(b) Test error.
N,0.9988439306358381,Figure 13: Train (a) and test (b) error for a pre-actived Wide-ResNet-18 trained on CIFAR-100.
