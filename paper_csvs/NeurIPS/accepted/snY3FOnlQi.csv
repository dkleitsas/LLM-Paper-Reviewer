Section,Section Appearance Order,Paragraph
UNIVERSITY OF ROCHESTER,0.0,"1University of Rochester
2Meta Reality Labs Research"
ABSTRACT,0.003816793893129771,Abstract
ABSTRACT,0.007633587786259542,"Can machines recording an audio-visual scene produce realistic, matching audio-
visual experiences at novel positions and novel view directions? We answer it by
studying a new task‚Äîreal-world audio-visual scene synthesis‚Äîand a first-of-its-
kind NeRF-based approach for multimodal learning. Concretely, given a video
recording of an audio-visual scene, the task is to synthesize new videos with spatial
audios along arbitrary novel camera trajectories in that scene. We propose an
acoustic-aware audio generation module that integrates prior knowledge of audio
propagation into NeRF, in which we implicitly associate audio generation with the
3D geometry and material properties of a visual environment. Furthermore, we
present a coordinate transformation module that expresses a view direction relative
to the sound source, enabling the model to learn sound source-centric acoustic
fields. To facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our
method on this real-world dataset and the simulation-based SoundSpaces dataset.
We recommend that readers visit our project page for convincing comparisons:
https://liangsusan-git.github.io/project/avnerf/."
INTRODUCTION,0.011450381679389313,"1
Introduction"
INTRODUCTION,0.015267175572519083,"We study a new task, real-world audio-visual scene synthesis, to generate target videos and audios
along novel camera trajectories from source audio-visual recordings of known trajectories. By
learning from real-world source videos with binaural audio, we aim to generate target video frames
and spatial audios that exhibit consistency with the given camera trajectory visually and acoustically.
This consistency ensures perceptual realism and immersion, enriching the overall user experience."
INTRODUCTION,0.019083969465648856,"As far as we know, attempts in the audio-visual learning literature [1‚Äì11] have yet to succeed in
solving this challenging task thus far. Although there are similar works [12‚Äì15], these methods have
constraints that limit their ability to solve this new task. Luo et al. [12] propose neural acoustic
fields to model sound propagation in a room. Su et al. [13] introduce representing audio scenes
by disentangling the scene‚Äôs geometry features. These methods are tailored for estimating room
impulse response signals in a simulation environment that are difficult to obtain in a real-world scene.
Concurrent to our work, ViGAS proposed by Chen et al. [15] learns to synthesize new sounds by
inferring the audio-visual cues. However, ViGAS is limited to a few viewpoints for audio generation."
INTRODUCTION,0.022900763358778626,"We introduce AV-NeRF, a novel NeRF-based method of synthesizing real-world audio-visual scenes.
AV-NeRF enables the generation of videos and spatial audios, following arbitrary camera trajectories.
It utilizes source videos and camera poses as references. AV-NeRF consists of two branches: A-NeRF,
which learns the acoustic fields of an environment, and V-NeRF, which models color and density
fields. We represent a static audio field as a continuous function using A-NeRF, which takes the
listener‚Äôs position and head direction as input. A-NeRF effectively models the energy decay of sound
as the sound travels from the source to the listener by correlating the listener‚Äôs position with the"
INTRODUCTION,0.026717557251908396,"Render Images
Render Video w/"
INTRODUCTION,0.030534351145038167,"Binaural Audio
Input Images
Input Videos"
INTRODUCTION,0.03435114503816794,"NeRF
Ours"
INTRODUCTION,0.03816793893129771,"Figure 1: NeRF learns to render visual scenes from novel viewpoints. Beyond visual rendering, we
introduce AV-NeRF, a method for synthesizing audio-visual scenes with video frames and binaural
audios along new camera trajectories. Integrating coherent sight and sound creates an immersive and
realistic perceptual experience for users."
INTRODUCTION,0.04198473282442748,"energy reduction effect. Moreover, A-NeRF accounts for the impact of the listener‚Äôs orientation
on sound perception by establishing a correlation between the audio channel difference and the
listener‚Äôs direction. Consequently, A-NeRF facilitates distance-sensitive and direction-aware audio
synthesis, resulting in the generation of realistic binaural sound. We utilize the vanilla NeRF [16] as
our V-NeRF for vision rendering since the rendering of the purely visual world has been extensively
researched [17, 16, 18‚Äì20] and is not the primary focus of our work."
INTRODUCTION,0.04580152671755725,"To further enhance synthesis quality, we introduce an acoustic-aware audio generation method and a
coordinate transformation mechanism. Considering that the 3D geometry and material properties of
an environment determine sound propagation [21, 22, 4, 23], we propose an acoustic-aware audio
generation module called AV-Mapper. AV-Mapper extracts geometry and material information from
V-NeRF and feed them to A-NeRF, enabling A-NeRF to generate audios with acoustic awareness. In
visual space, NeRF [16] expresses the viewing direction (Œ∏, œï) using an absolute coordinate system,
where the directions of two parallel light rays are represented identically. However, in auditory space,
this expression method is unsuitable as it ignores the spatial relationship between the sound emitter
and receiver. Human perception of sound source relies on the relative direction of the sound source
to the head orientation. Given this observation, we propose a coordinate transform mechanism that
expresses the viewing direction relative to the sound source. This approach encourages our model to
learn sound source-centric acoustic fields."
INTRODUCTION,0.04961832061068702,"Our work represents the initial step towards addressing the audio-visual scene synthesis task in
real-world settings. As such, there is currently no publicly available dataset that fulfills our specific
requirements. To facilitate our study, we curated a high-quality audio-visual dataset called RWAVS
(Real-World Audio-Visual Synthesis), which encompasses multimodal data, including camera poses,
video frames, and realistic binaural (stereo) audios. In order to enhance the diversity of our dataset,
we collected data from various environments, ranging from offices and apartments to houses and even
outdoor spaces. We will release this dataset to the research community. Experiments on the RWAVS
and the synthetic SoundSpaces datasets can validate the effectiveness of our proposed approach. It
demonstrates AV-NeRF‚Äôs capability of synthesizing novel-view audio-visual scenes in a range of
environments, including both real-world and synthetic, as well as indoor and outdoor settings."
INTRODUCTION,0.05343511450381679,"In summary, our contributions include: (1) proposing a novel approach to synthesize audio-visual
scenes in real-world environments at novel camera poses; (2) introducing a new acoustic-aware audio
generation method that incorporates our prior knowledge of sound propagation; (3) suggesting a
coordinate transformation mechanism to effectively express sound direction; (4) curating a high-
quality audio-visual dataset for benchmarking this task; and (5) demonstrating the advantages of our
method through quantitative and qualitative evaluations."
RELATED WORK,0.05725190839694656,"2
Related Work"
RELATED WORK,0.061068702290076333,"Our work is closely related to several areas, including novel-view synthesis, vision-guided audio
spatialization, simulation-based acoustic synthesis, and learning-based audio generation. We elaborate
on each of these topics in the following section."
RELATED WORK,0.0648854961832061,"Neural Fields. Our method is based on neural fields, particularly Neural Radiance Fields (NeRF) [16].
NeRF employs MLPs to learn an implicit and continuous representation of visual scenes, enabling
the synthesis of novel views. Building upon NeRF‚Äôs foundation, several works have extended its
applicability to broader domains, including in-the-wild images [24], (silent) video [25‚Äì27], audio"
RELATED WORK,0.06870229007633588,"[12, 13], and audio-visual [14] content. Luo et al. [12] propose neural acoustic fields (NAF) to
capture the sound propagation in an environment, and Su et al. [13] introduce disentangling a scene‚Äôs
geometry features for audio scene representation (INRAS). Despite the compelling results achieved
by NAF and INRAS, their application in the real world is hindered by their reliance on ground-truth
impulse response signals that are simulated in synthetic environments. Furthermore, the discretization
of position and direction in these approaches restricts the range of camera poses they can handle,
whereas our model can handle continuous position and direction queries. Du et al. [14] proposed a
manifold learning method that maps vectors from a latent space to audio and image spaces. Although
the learned manifold allows for audio-visual interpolation, the model lacks support for controllable
audio-visual generation. In contrast, our method focuses on learning audio-visual representations that
are explicitly conditioned on spatial coordinates, enabling controllable generation."
RELATED WORK,0.07251908396946564,"Visually Informed Spatial Audio Generation. Considering that images often reveal the position of a
sound source and/or the structure of a scene, many visually informed audio spatialization approaches
[7, 28‚Äì31, 15] have been proposed. Among them, Gao and Grauman [7] focus on normal field-of-
view videos and binaural audios. Zhou et al. [28] propose a unified framework to solve the sound
separation and stereo sound generation at the same time. Xu et al. [29] propose a Pseudo2Binaural
pipeline that augments audio data using HRIR function [32]. Most recently, Chen et al. [15] proposed
the Visually-Guided Acoustic Synthesis (ViGAS) method for generating novel-view audios. However,
AV-NeRF differs from this method in two ways: (1) ViGAS relies on ground-truth images for audio
transformation, whereas AV-NeRF addresses the absence of ground-truth images by utilizing rendered
images from learned vision fields; (2) ViGAS only supports a limited number of viewpoints for audio
synthesis, while AV-NeRF has the capability to render novel videos at arbitrary camera poses."
RELATED WORK,0.07633587786259542,"Geometry and Material Based Acoustic Simulation. Several works [4, 33, 22, 34, 23] focus on sim-
ulating acoustic environments by modeling their 3D geometry and material properties. SoundSpaces,
proposed by Chen et al. [4], is an audio platform that calculates the room impulse response signals
for discrete listener and sound source positions. Extending this work, Chen et al. [33] introduce
SoundSpaces 2.0, which enables acoustic generation for arbitrary microphone locations. Tang et al.
[23] propose calibrating geometric acoustic ray-tracing with a finite-difference time-domain wave
solver to compute high-quality impulse responses. Additionally, Li et al. [22] propose a method
that blends the early reverberation portion, modeled using geometric acoustic simulation and fre-
quency modulation, with a late reverberation tail extracted from recorded impulse responses. In
contrast to these simulation methods, our approach implicitly leverages learned geometry and material
information for audio-visual scene synthesis."
RELATED WORK,0.08015267175572519,"Learning Based Audio Generation. Learning-based methods exploit the powerful modeling ability
of neural networks to synthesize audios. Tang et al. [34] propose the use of neural networks to estimate
reverberation time (T60) and equalization (EQ). Ratnarajah et al. [35, 36, 37] employ generative
adversarial networks (GANs) to supervise realistic room impulse response generation. Richard et al.
[38] introduce a binauralization network that models the sound propagation linearly and nonlinearly.
Recent research has also investigated learning from noisy data [39] and few-shot data [40]."
TASK DEFINITION,0.08396946564885496,"3
Task Definition"
TASK DEFINITION,0.08778625954198473,"The real-world audio-visual scene synthesis task is to generate visual frames and corresponding
binaural audios for arbitrary camera trajectories. Given a static environment E and multiple observa-
tions O = {O1, O2, . . . , ON} of this environment, where N is the number of training samples and
each observation Oi includes the camera pose p = (x, y, z, Œ∏, œï), the mono source audio clip as, the
recorded binaural audio at, and the image I, this task aims to synthesize a new binaural audio a‚àó
t and
a novel view I‚àóbased on a camera pose query p‚àóand a source audio a‚àó
s, where p‚àóis distinct from
the camera poses in all observations. This process can be formatted as:"
TASK DEFINITION,0.0916030534351145,"(a‚àó
t , I‚àó) = f(p‚àó, a‚àó
s|O, E) ,
(1)"
TASK DEFINITION,0.09541984732824428,"where f is a mapping function. Combining predicted a‚àó
t and I‚àóof all poses in the queried camera
trajectory, we can synthesize a realistic video with spatial audio. This task poses several challenges:
(1) p‚àóis a novel viewpoint distinct from all observed viewpoints, (2) the synthesized binaural audio
a‚àó
t is expected to exhibit rich spatial effects contributing to its realism, (3) during inference, no
observation O is accessible and the mapping function f relies solely on the learned fields for scene
synthesis. It should be noted that the position of the sound source is known in the environment E."
TASK DEFINITION,0.09923664122137404,"Direction (ùúÉ, ùúô)"
TASK DEFINITION,0.10305343511450382,"Position (ùë•, ùë¶, ùëß)"
TASK DEFINITION,0.10687022900763359,V-NeRF
TASK DEFINITION,0.11068702290076336,"Density ùúé
Color ùëê"
TASK DEFINITION,0.11450381679389313,A-NeRF
TASK DEFINITION,0.1183206106870229,"Mix ùëö!
Diff ùëö"" ùêà!""#"
TASK DEFINITION,0.12213740458015267,ùêà$%&'(
TASK DEFINITION,0.12595419847328243,A-V Mapper
TASK DEFINITION,0.1297709923664122,Input Audio
TASK DEFINITION,0.13358778625954199,Synthesized Audio L R
TASK DEFINITION,0.13740458015267176,"Encoder
Supervision"
TASK DEFINITION,0.14122137404580154,Direction ùúÉ
TASK DEFINITION,0.1450381679389313,"Position (ùë•, ùë¶)"
TASK DEFINITION,0.14885496183206107,"Figure 2: The pipeline of our method. Given the position (x, y, z) and viewing direction (Œ∏, œï) of a
listener, our method can render an image the listener would see and the corresponding binaural audio
the listener would hear. Our model consists of V-NeRF, A-NeRF, and AV-Mapper. A-NeRF learns to
generate acoustic masks, V-NeRF learns to generate visual frames, and AV-Mapper is optimized to
integrate geometry and material information extracted from V-NeRF into A-NeRF."
METHOD,0.15267175572519084,"4
Method"
METHOD,0.15648854961832062,"Our method aims to learn neural fields that can synthesize real-world audio-visual scenes from novel
poses. The entire pipeline is illustrated in Figure 2. Our model comprises three trainable modules:
V-NeRF, A-NeRF, and AV-Mapper. A-NeRF is responsible for generating acoustic masks, V-NeRF
focuses on generating visual frames, and AV-Mapper is optimized to extract geometry and material
information from V-NeRF, integrating this information into A-NeRF."
V-NERF,0.16030534351145037,"4.1
V-NeRF"
V-NERF,0.16412213740458015,"NeRF [16] uses a Multi-Layer Perceptron (MLP) to implicitly and continuously represent a visual
scene. It effectively learns a mapping from camera poses to colors and densities:"
V-NERF,0.16793893129770993,"NeRF : (x, y, z, Œ∏, œï) ‚Üí(c, œÉ) ,
(2)"
V-NERF,0.1717557251908397,"where X = (x, y, z) is the 3D position, d = (Œ∏, œï) is the direction, c = (r, g, b) is the color, and œÉ is
the density. In order to achieve view-dependent color rendering c and ensure multi-view consistency,
[16] introduces two MLP components within NeRF. The first MLP maps a 3D coordinate (x, y, z)
to density œÉ and a corresponding feature vector. The second MLP then takes the feature vector and
2D direction (Œ∏, œï) as inputs, producing the color c. This process is illustrated in Fig. 3. NeRF then
uses the volume rendering method [41] to generate the color of any ray r(t) = o + td as it traverses
through the visual scene, bounded by near tn and far tf:"
V-NERF,0.17557251908396945,"C(r) =
Z tf"
V-NERF,0.17938931297709923,"tn
T(t)œÉ(r(t))c(r(t), d)dt ,
(3)"
V-NERF,0.183206106870229,"where T(t) = exp(‚àí
R t
tn œÉ(r(s))ds). The depth of any ray r can be calculated similarly by replacing
c(r(t), d) with t:"
V-NERF,0.18702290076335878,"D(r) =
Z tf"
V-NERF,0.19083969465648856,"tn
T(t)œÉ(r(t))tdt .
(4)"
V-NERF,0.1946564885496183,"These continuous integrals are estimated by quadrature in practice [42]. Subsequently, RGB and
depth images can be rendered for any given camera pose using V-NeRF (Fig. 3). We apply positional
encoding to all input coordinates to preserve the high-frequency information of the generated images."
V-NERF,0.1984732824427481,V-NeRF
V-NERF,0.20229007633587787,"Position (ùë•, ùë¶, ùëß)"
V-NERF,0.20610687022900764,"Direction (ùúÉ, ùúô)"
V-NERF,0.2099236641221374,Density ùúé
V-NERF,0.21374045801526717,"Color ùëê ùêà!""#"
V-NERF,0.21755725190839695,ùêà$%&'(
V-NERF,0.22137404580152673,"Figure 3: The pipeline of V-NeRF. Given the 3D position (x, y, z) and direction (Œ∏, œï) of a camera
pose, V-NeRF can render corresponding RGB and depth images. V-NeRF consists of two MLPs with
the first MLP estimating density œÉ and the second one predicting color c."
A-NERF,0.22519083969465647,"4.2
A-NeRF"
A-NERF,0.22900763358778625,"The goal of A-NeRF is to learn a neural acoustic representation capable of mapping 5D coordinates
(x, y, z, Œ∏, œï) to corresponding acoustic masks mm, md ‚ààRF , where mm quantifies the change
in audio magnitude with respect to the position (x, y, z) while md characterizes the impact of the
direction (Œ∏, œï) on the channel difference of binaural audios, and F is the number of frequency bins:"
A-NERF,0.23282442748091603,"NeRF : (x, y, z, Œ∏, œï) ‚Üí(mm, md) .
(5)"
A-NERF,0.2366412213740458,"After training A-NeRF, we can synthesize binaural sound that contains rich spatial and acoustic
information by fusing the magnitude of any input audio with predicted mm and md."
A-NERF,0.24045801526717558,"In practice, we simplify the problem of audio synthesis by discarding the z axis and the œï direction,
which represents coordinates related to height, and instead focus on the 2D space. On RWAVS and
SoundSpaces datasets, we observe that the sound received by the listener exhibits more distinct
variations along the horizontal plane, as opposed to the vertical axis. Fig. 4 depicts the pipeline of
audio synthesis. Similar to V-NeRF, A-NeRF consists of two MLP components that parameterize
different aspects of acoustic fields. The first MLP takes as input the 2D position (x, y) and frequency
query f ‚àà[0, F], producing a mixture mask mm for frequency f and a feature vector. The mask mm
quantifies the change in audio magnitude based on the distance between the sound source and the
sound receiver. The feature vector serves as an implicit embedding of the input information and the
acoustic scene. Then we concatenate the feature vector with the transformed direction Œ∏‚Ä≤ (explained
in Sec. 4.4) and pass them to the second MLP. The second MLP associates the direction Œ∏‚Ä≤ with
the channel difference of binaural audios. Given a fixed sound emitter, the difference between two
channels of the received audio changes as the direction Œ∏‚Ä≤ of the sound receiver varies. For instance,
when the receiver faces the emitter, the energy of the two channels will be approximately equal.
Conversely, when the emitter is on the left side of the receiver, the left channel will exhibit higher
energy compared to the right channel. The second MLP is optimized to generate a difference mask
md that characterizes such direction influence. We query A-NeRF with all frequencies f within
[0, F] to generate the complete masks mm and md."
A-NERF,0.24427480916030533,"After obtaining the mixture mask mm and the difference mask md, we can synthesize binaural audios
by composing these masks and input audios (Fig. 4). For an input source audio as, we initially
employ the short-time Fourier transform (STFT) to calculate the magnitude ss ‚ààRF √óW , where W
represents the number of time frames and F denotes the number of frequency bins. We then multiply
ss with mask mm to obtain the mixed magnitude sm. Additionally, we multiply sm and mask md to
predict the difference magnitude sd. Then we add mixture magnitude sm and difference magnitude
sd to compute the magnitude of left channel sl, and subtract sd from sm to compute the magnitude
of right channel sr. We further refine sl and sr using 2D convolution. Finally, we apply Inverse
STFT to the predicted sl and sr, respectively, to synthesize the binaural audio at. Because A-NeRF
operates on the magnitude, we use the phase information of the source audio for the Inverse STFT
process. Instead of estimating two channels of the target audio directly, we predict the mixture and
the difference between the two channels of the target audio. Gao and Grauman [7] suggest that direct
two-channel predictions can lead to shortcut learning in the spatialization network."
AV-MAPPER,0.2480916030534351,"4.3
AV-Mapper"
AV-MAPPER,0.25190839694656486,"Given the fact that 3D geometry and material property determine sound propagation in an environment
(prior knowledge), we propose an acoustic-aware audio generation method that integrates color and
depth information estimated by V-NeRF with A-NeRF. When V-NeRF learns to represent visual
scenes, it can learn the color c and density œÉ function of the environment, thanks to the multi-view ùë†!"
AV-MAPPER,0.25572519083969464,A-NeRF
AV-MAPPER,0.2595419847328244,"Position (ùë•, ùë¶) ùëö!"
AV-MAPPER,0.2633587786259542,Direction ùúÉ
AV-MAPPER,0.26717557251908397,Frequency ùëì
AV-MAPPER,0.27099236641221375,"A-V Mapper ùëö"""
AV-MAPPER,0.2748091603053435,"(ùë•, ùë¶)"
AV-MAPPER,0.2786259541984733,"(ùëÜ!, ùëÜ"") 
Sound Source
ùúÉ
ùúÉ‚Ä≤"
AV-MAPPER,0.2824427480916031,Direction Transformation
AV-MAPPER,0.2862595419847328,Input Audio ùëé# STFT ‚ûï ‚ûñ
AV-MAPPER,0.2900763358778626,Conv2D ISTFT
AV-MAPPER,0.29389312977099236,Synthesized
AV-MAPPER,0.29770992366412213,"Audio ùëé$ L R
ùë†"" ùë†"" ùë†! ùëí"
AV-MAPPER,0.3015267175572519,Color & Depth
AV-MAPPER,0.3053435114503817,": Audio-visual flow
: Audio flow"
AV-MAPPER,0.30916030534351147,: Input/Output flow ùë†! ùë†#
AV-MAPPER,0.31297709923664124,ùëö? : Mask
AV-MAPPER,0.31679389312977096,"ùë†? : Magnitude
‚ûï/‚ûñ: Add/Subtract"
AV-MAPPER,0.32061068702290074,Conv2D ISTFT
AV-MAPPER,0.3244274809160305,"Figure 4: The pipeline of A-NeRF. Given the 2D position (x, y) and direction Œ∏ of a camera pose,
A-NeRF can render mixture mask mm, and difference mask md, which then be used for audio
synthesis. A-NeRF is also composed of two MLPs, with the first predicting mixture mask mm and
the second estimating difference mask md. The direction Œ∏ is transformed relatively to the sound
source prior to inputting into A-NeRF."
AV-MAPPER,0.3282442748091603,"consistency constraint. Utilizing volume rendering [16, 41], we can synthesize the RGB and depth
image for any given camera pose. The RGB image captures the semantics and category of each
object, implicitly indicating their material properties. And the depth image depicts the geometric
structure of the environment. By providing A-NeRF with a combination of RGB and depth images,
we can offer rich environmental knowledge to A-NeRF."
AV-MAPPER,0.3320610687022901,"Specifically, when synthesizing the binaural audio for a given camera pose, our process begins
by rendering a pair of RGB and depth images with the same camera pose using Eq. 3 and Eq. 4,
respectively. We exploit a pre-trained encoder (e.g., ResNet-18 [43] pre-trained on ImageNet dataset
[44]) to extract color and depth features from the RGB and depth images. The extracted features can
serve as important indicators of the environment. Next, we develop an AV-Mapper, implemented as
an MLP network, to project the color and depth features into a latent space. Our goal for the A-V
Mapper is two-fold: (1) to extract meaningful embeddings while discarding unnecessary features,
and (2) to align the features in the original space of the pre-trained image encoder with the space
relevant to our specific problem. The output embedding of the AV-Mapper, denoted as e ‚ààRc, with
c representing the width of each linear layer in A-NeRF, is then added to the input of A-NeRF for
controlling the audio synthesis. Ablation studies (Sec. 5.2) demonstrate the slight advantage of this
fusion method over others."
COORDINATE TRANSFORMATION,0.33587786259541985,"4.4
Coordinate Transformation"
COORDINATE TRANSFORMATION,0.33969465648854963,"Viewing direction (Œ∏, œï) in V-NeRF is expressed in an absolute coordinate system. This is a natural
practice in visual space such that directions of two parallel light rays are expressed identically.
However, this expression method in audio space is less suitable because the human perception of
the sound direction is based on the relative direction to the sound source instead of the absolute
direction. To address this limitation, we propose expressing the viewing direction of a camera pose
relative to the sound source. This coordinate transformation encourages A-NeRF learning a sound
source-centric acoustic field, enhancing spatial audio generation."
COORDINATE TRANSFORMATION,0.3435114503816794,"Given the 2D position (x, y) and the direction Œ∏ of the camera, as well as the position of the sound
source (Sx, Sy), we obtain two direction vectors: V1 = (Sx ‚àíx, Sy ‚àíy) and V2 = (cos(Œ∏), sin(Œ∏)).
V1 represents the direction from the camera to the sound source, and V2 represents the camera‚Äôs
direction in the absolute coordinate system. By calculating the angle between V1 and V2, denoted as
the relative direction Œ∏‚Ä≤ = ‚à†(V1, V2), we obtain the rotation angle relative to the sound source. This
angle Œ∏‚Ä≤ allows different camera poses to share the same direction encoding if they face the sound
source at the same angle."
COORDINATE TRANSFORMATION,0.3473282442748092,"After computing the relative angle Œ∏‚Ä≤, we choose learnable embeddings instead of positional encoding
[16, 45] to project Œ∏‚Ä≤ into a high-frequency space. We use the embedding Œò ‚ààR4√óc to represent four
discrete directions, namely, 0‚ó¶, 90‚ó¶, 180‚ó¶, and 270‚ó¶, where 4 is the number of directions and c is
the width of A-NeRF. Given a relative angle Œ∏‚Ä≤ ‚àà[0‚ó¶, 360‚ó¶), we linearly interpolate the direction
embedding Œò‚Ä≤ ‚ààRc according to the angle between Œ∏‚Ä≤ and four discrete directions. We add the"
COORDINATE TRANSFORMATION,0.3511450381679389,"(a) Office
(b) House
(c) Apartment
(d) Outdoors"
COORDINATE TRANSFORMATION,0.3549618320610687,"Figure 5: Example scenes of RWAVS dataset. RWAVS dataset consists of diverse audio-visual scenes
indoors and outdoors. Each environment possesses distinct acoustics and structures."
COORDINATE TRANSFORMATION,0.35877862595419846,"interpolated embedding Œ∏‚Ä≤ to the input of each linear layer in A-NeRF, thereby providing A-NeRF
with direction information. Ablation studies (Sec. 5.2) show that this encoding method performs best."
LEARNING OBJECTIVE,0.36259541984732824,"4.5
Learning Objective"
LEARNING OBJECTIVE,0.366412213740458,The loss function of V-NeRF is the same as [16]:
LEARNING OBJECTIVE,0.3702290076335878,"LV = ||C(r) ‚àíÀÜC(r)||2 ,
(6)"
LEARNING OBJECTIVE,0.37404580152671757,"where C(r) is the ground-truth color along the ray r and ÀÜC(r) is the color rendered by V-NeRF. After
training V-NeRF, we optimize A-NeRF and AV-Mapper together with the L2 loss function:"
LEARNING OBJECTIVE,0.37786259541984735,"LA = ||sm ‚àíÀÜsm||2 + ||sl ‚àíÀÜsl||2 + ||sr ‚àíÀÜsr||2 ,
(7)"
LEARNING OBJECTIVE,0.3816793893129771,"where sm,l,r are the predicted magnitudes, ÀÜsm,l,r are the ground-truth magnitudes, and subscript
m, l, r are mixture, left, and right, respectively. The first term of LA encourages A-NeRF to predict
masks that represent spatial effects caused by distance, while the second and third term encourages
A-NeRF to generate masks that capture the differences between two channels."
EXPERIMENTS,0.38549618320610685,"5
Experiments"
DATASETS,0.3893129770992366,"5.1
Datasets"
DATASETS,0.3931297709923664,"To the best of our knowledge, our method is the first NeRF-based system capable of synthesizing real-
world videos with perceptually realistic binaural audios at arbitrary poses. However, existing datasets
do not meet the specific requirements of our experiments, particularly in terms of simultaneously
providing camera poses, high-quality binaural audios, and images. Therefore, we curated a high-
quality audio-visual scene dataset (real) to address this gap and facilitate further research on this
problem. Additionally, we utilize (synthetic) SoundSpaces dataset [4] to validate our method."
DATASETS,0.3969465648854962,"(1) RWAVS Dataset. We collected the Real-World Audio-Visual Scene (RWAVS) dataset to bench-
mark our method. In order to increase the diversity of our dataset, we recorded data across different
scenarios. Fig. 5 shows the example scenarios we used for data recording, including both indoor and
outdoor environments, which we believe represent most daily settings. RWAVS dataset comprises
multimodal data, including camera poses, high-quality binaural audios, and videos. Unlike Replay-
NVAS dataset [15], where the environment and the recording viewpoint are constant, RWAVS dataset
contains various viewpoints in diverse environments. During data recording, we randomly moved
around the environment while holding the device, capturing various acoustic and visual signals.
RWAVS dataset encompasses all positions and directions (360‚ó¶) within an environment."
DATASETS,0.40076335877862596,"In detail, we employed a 3Dio Free Space XLR binaural microphone for capturing high-quality stereo
audio, a TASCAM DR-60DMKII for recording and storing audio, and a GoPro Max for capturing
accompanying videos. Additionally, an LG XBOOM 360 omnidirectional speaker was used as the
sound source. For each environment and sound source combination, we collected data ranging from
10 to 25 minutes, resulting in a total collection of 232 minutes (3.8 hours) of data from diverse
environments with varying source positions."
DATASETS,0.40458015267175573,"We extract key frames at 1 fps from recorded videos and use COLMAP [46] to estimate the corre-
sponding camera pose. Each key frame is accompanied by one-second binaural audio and one-second"
DATASETS,0.4083969465648855,Table 1: Comparison with state-of-the-art methods on RWAVS dataset.
DATASETS,0.4122137404580153,"Methods
Office
House
Apartment
Outdoors
Overall
MAG
ENV
MAG
ENV
MAG
ENV
MAG
ENV
MAG
ENV"
DATASETS,0.41603053435114506,"Mono-Mono
9.269
0.411
11.889
0.424
15.120
0.474
13.957
0.470
12.559
0.445
Mono-Energy
1.536
0.142
4.307
0.180
3.911
0.192
1.634
0.127
2.847
0.160
Stereo-Energy
1.511
0.139
4.301
0.180
3.895
0.191
1.612
0.124
2.830
0.159"
DATASETS,0.4198473282442748,"INRAS [13]
1.405
0.141
3.511
0.182
3.421
0.201
1.502
0.130
2.460
0.164
NAF [12]
1.244
0.137
3.259
0.178
3.345
0.193
1.284
0.121
2.283
0.157
ViGAS [15]
1.049
0.132
2.502
0.161
2.600
0.187
1.169
0.121
1.830
0.150
Ours
0.930
0.129
2.009
0.155
2.230
0.184
0.845
0.111
1.504
0.145"
DATASETS,0.42366412213740456,"Table 2: Ablation studies. We break down AV-NeRF to analyze the contribution of each component.
Left: the inclusion of AV-Mapper (AV) and Coordinate Transformation (CT), Middle: different
multimodal fusion methods, and Right: different direction encoding methods."
DATASETS,0.42748091603053434,"Methods
Overall
MAG
ENV"
DATASETS,0.4312977099236641,"Baseline
2.287
0.157
Ours w/o AV
1.791
0.150
Ours w/o CT
1.701
0.149
Ours
1.504
0.145"
DATASETS,0.4351145038167939,"Methods
Overall
MAG
ENV"
DATASETS,0.4389312977099237,"Concat Input
1.507
0.145
Add Input
1.504
0.145
Add All Layers
1.505
0.145"
DATASETS,0.44274809160305345,"Methods
Overall
MAG
ENV"
DATASETS,0.44656488549618323,"Absolute Direction
1.701
0.149
Relative Direction
1.508
0.145
Relative Embedding
1.504
0.145"
DATASETS,0.45038167938931295,"source audio, forming a complete data sample. For audio clips with noticeable background noise, we
perform noise suppression using Adobe Audition [47]. We split 80% data as training samples and the
rest as validation samples. After pre-processing, we obtain 9850 and 2469 samples for training and
validation, respectively. This dataset is challenging because of the diverse environments and various
camera poses. We will release this dataset to the research community."
DATASETS,0.4541984732824427,"(2) SoundSpaces Dataset. While RWAVS offers realistic training samples, its realism restricts
its scale because it is time-consuming to record high-quality multimodal data in the real world.
Therefore, we use the synthetic SoundSpaces dataset to augment our experiments. To evaluate our
method on SoundSpaces dataset, we modify AV-NeRF to estimate impulse responses instead of
the acoustic mask while keeping all other components intact. We follow NAF [12] selecting six
representative indoor scenes, consisting of two single rooms with rectangular walls, two single rooms
with non-rectangular walls, and two multi-room layouts. In each scene, SoundSpaces dataset provides
an extensive collection of impulse response signals for sound source and sound receiver pairs, which
are densely sampled from a 2D room grid. Each pair includes four discrete head orientations (0‚ó¶, 90‚ó¶,
180‚ó¶, and 270‚ó¶), and each orientation is associated with two-channel binaural RIRs. We render RGB
and depth images for each sound receiver pose using Habitat-Sim simulator [48, 49]. We maintain
the same training/test split as NAF, allocating 90% data for training and 10% data for testing."
RESULTS ON RWAVS DATASET,0.4580152671755725,"5.2
Results on RWAVS Dataset"
RESULTS ON RWAVS DATASET,0.4618320610687023,"Comparison with State-of-the-art. We compare AV-NeRF with the following baselines: (1) Mono-
Mono duplicates the source audio as twice to generate a fake binaural audio without modifying the
source audio; (2) Mono-Energy assumes that the average energy of the target audio at is known,
scales the energy of the input audio to match the target, and duplicates the scaled audio to generate
a stereo audio; (3) Stereo-Energy assumes that the energy of the two channels of the target audio
at is known, separately scales the energy of the input audio to match the target, and combines the
two scaled channels to generate a stereo audio; (4) IRNAS [13] learns representing audio scenes by
disentangling scene‚Äôs geometry features with implicit neural fields, and we adapt INRAS to predict
wave masks on RWAVS dataset; (5) NAF [12] designs local feature grids and an implicit decoder to
capture the sound propagation in a physical scene, and we modify NAF to predict magnitude masks
on RWAVS dataset; (6) ViGAS [15] achieves novel-view acoustic synthesis by analyzing audio-visual
cues from source viewpoints. We select magnitude distance (MAG) [29], which measures the audio
quality in the time-frequency domain, and envelope distance (ENV) [30], which measures the audio
quality in the time domain, to evaluate various methods. Please refer to the supplementary material
for implementation details."
RESULTS ON RWAVS DATASET,0.46564885496183206,"Rendered Image
Rendered Image
Rendered Audio
GT Audio
Rendered Audio
GT Audio L R L R"
RESULTS ON RWAVS DATASET,0.46946564885496184,"Figure 6: Visualization of synthesized audio-visual scene. We present the rendered image, synthesized
binaural audio, and ground-truth audio."
RESULTS ON RWAVS DATASET,0.4732824427480916,"AV-NeRF outperforms all baselines across different environments, including office, house, apartment,
and outdoors, by a significant margin (Table 1). AV-NeRF outruns INRAS with 0.956 on the overall
MAG metric (39% relative improvement) and 0.019 on the average ENV metric (11.6%). AV-NeRF
surpasses NAF with 0.779 on MAG metric (34%) and 0.012 on ENV metric (8%). Our approach
is better than ViGAS in terms of both MAG (1.504 compared to ViGAS‚Äôs 1.830) and ENV (0.145
compared to ViGAS‚Äôs 0.150)."
RESULTS ON RWAVS DATASET,0.4770992366412214,"Ablation Studies. We conduct ablation studies on AV-NeRF to analyze the contribution of different
components. We combine A-NeRF and V-NeRF as our baseline, which does not contain AV-Mapper
(Sec. 4.3) or coordinate transformation (Sec. 4.4). The ablation results are as follows: (1) AV-Mapper
(AV) and coordinate transformation (CT) play important roles in learning audio scenes (Table 2 left).
The exclusion of either component degrades the generation quality; (2) adding visual information to
the input of A-NeRF is the most effective multimodal fusion method compared with concatenation
and adding visual information to all layers of A-NeRF (Table 2 middle); (3) using embeddings to
represent relative angles outperforms applying positional encoding to either absolute or relative
angles (Table 2 right). ""Absolute Direction"" represents applying positional encoding to the absolute
angle, ""Relative Direction"" means transforming the relative angle with the positional encoding, and
""Relative Embedding"" is the embedding method."
RESULTS ON RWAVS DATASET,0.48091603053435117,"Visualization. We visualize the synthesized audio-visual scenes in Fig. 6 to intuitively assess the
generation quality of our model. AV-NeRF can synthesize realistic binaural audios that have the same
signal envelope and channel difference as the ground-truth audios."
RESULTS ON SOUNDSPACES DATASET,0.4847328244274809,"5.3
Results on SoundSpaces Dataset
Table 3: Comparison with state-of-the-art. We
report the performance on the SonudSpaces dataset
using T60, C50, and EDT metrics. The lower score
indicates a higher RIR generation quality. Opus
is an open audio codec [50], and AAC is a multi-
channel audio coding standard [51]."
RESULTS ON SOUNDSPACES DATASET,0.48854961832061067,"Methods
T60 (%) ‚Üì
C50 (dB) ‚Üì
EDT (sec) ‚Üì
Opus-nearest
10.10
3.58
0.115
Opus-linear
8.64
3.13
0.097
AAC-nearest
9.35
1.67
0.059
AAC-linear
7.88
1.68
0.057
NAF [12]
3.18
1.06
0.031
INRAS [13]
3.14
0.60
0.019
Ours
2.47
0.57
0.016"
RESULTS ON SOUNDSPACES DATASET,0.49236641221374045,"We compare AV-NeRF with traditional audio
coding methods [50, 51] and advanced learning-
based neural field methods [12, 13] using T60,
C50, and EDT metrics [13]. Please refer to our
supplementary material for implementation de-
tails. Table 3 shows that AV-NeRF outruns both
traditional and advanced methods, achieving
21% relative improvement on T60 metric com-
pared with the previous state-of-the-art method
INRAS, 5% on C50, and 16% on EDT."
DISCUSSION,0.4961832061068702,"6
Discussion"
DISCUSSION,0.5,"In this work, we propose a first-of-its-kind NeRF system capable of synthesizing real-world audio-
visual scenes. Our model can generate audios with rich spatial information at novel camera poses.
We demonstrate the effectiveness of our method on real RWAVS and synthetic SoundSpaces datasets."
DISCUSSION,0.5038167938931297,"Limitation. Firstly, we currently focus on static scenes with a single fixed sound source in our
study. However, it is worth exploring the challenge of learning implicit neural representations for
audio-visual scenes with multiple dynamic sound sources. Second, while our model successfully
generates audio-visual scenes, it does not account for reverberation effects present in the real world.
Reverberation is essential for humans to perceive scene size and structure. Third, similar to the
original NeRF, AV-NeRF must represent and render each scene separately. Developing a neural field"
DISCUSSION,0.5076335877862596,"that can either learn to represent all scenes or transfer knowledge to new environments is a crucial
problem, particularly for industrial applications."
DISCUSSION,0.5114503816793893,"Broader Impact. Although AV-NeRF is designed to synthesize audio-visual scenes, it is important
to acknowledge its potential application in generating scenes involving artificial humans. The misuse
of AV-NeRF could lead to the production of deceptive and misleading media."
REFERENCES,0.5152671755725191,References
REFERENCES,0.5190839694656488,"[1] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisen-
sory features. In ECCV, pages 631‚Äì648, 2018."
REFERENCES,0.5229007633587787,"[2] Shentong Mo and Pedro Morgado. Localizing visual sounds the easy way. In Shai Avidan,
Gabriel J. Brostow, Moustapha Ciss√©, Giovanni Maria Farinella, and Tal Hassner, editors, ECCV,
volume 13697, pages 218‚Äì234, 2022."
REFERENCES,0.5267175572519084,"[3] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event
localization in unconstrained videos. In ECCV, pages 247‚Äì263, 2018."
REFERENCES,0.5305343511450382,"[4] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah,
Vamsi Krishna Ithapu, Philip Robinson, and Kristen Grauman. Soundspaces: Audio-visual
navigation in 3d environments. In ECCV, 2020."
REFERENCES,0.5343511450381679,"[5] Changan Chen, Ziad Al-Halah, and Kristen Grauman. Semantic audio-visual navigation. In
CVPR, pages 15516‚Äì15525, 2021."
REFERENCES,0.5381679389312977,"[6] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio
Torralba. The sound of pixels. In ECCV, pages 570‚Äì586, 2018."
REFERENCES,0.5419847328244275,"[7] Ruohan Gao and Kristen Grauman. 2.5d visual sound. In CVPR, 2019."
REFERENCES,0.5458015267175572,"[8] Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, and Ziwei Liu. Sep-stereo: Visually
guided stereophonic audio generation by associating source separation. In ECCV, pages 52‚Äì69.
Springer, 2020."
REFERENCES,0.549618320610687,"[9] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf:
Audio driven neural radiance fields for talking head synthesis. In ICCV, pages 5784‚Äì5794,
2021."
REFERENCES,0.5534351145038168,"[10] Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, and
Xilin Chen. Unicon: Unified context network for robust active speaker detection. In ACM MM,
pages 3964‚Äì3972, 2021."
REFERENCES,0.5572519083969466,"[11] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan
Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. In ECCV,
2022."
REFERENCES,0.5610687022900763,"[12] Andrew Luo, Yilun Du, Michael J Tarr, Joshua B Tenenbaum, Antonio Torralba, and Chuang
Gan. Learning neural acoustic fields. NeurIPS, 2022."
REFERENCES,0.5648854961832062,"[13] Kun Su, Mingfei Chen, and Eli Shlizerman. Inras: Implicit neural representation for audio
scenes. In NeurIPS, 2022."
REFERENCES,0.5687022900763359,"[14] Yilun Du, M. Katherine Collins, B. Joshua Tenenbaum, and Vincent Sitzmann. Learning
signal-agnostic manifolds of neural fields. In NeurIPS, 2021."
REFERENCES,0.5725190839694656,"[15] Changan Chen, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia
Neverova, Kristen Grauman, and Andrea Vedaldi. Novel-view acoustic synthesis. arXiv
preprint arXiv:2301.08730, 2023."
REFERENCES,0.5763358778625954,"[16] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,
and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV,
2020."
REFERENCES,0.5801526717557252,"[17] Vincent Sitzmann, Michael Zollh√∂fer, and Gordon Wetzstein. Scene representation networks:
Continuous 3d-structure-aware neural scene representations. NeurIPS, 32, 2019."
REFERENCES,0.583969465648855,"[18] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable
volumetric rendering: Learning implicit 3d representations without 3d supervision. In CVPR,
pages 3504‚Äì3515, 2020."
REFERENCES,0.5877862595419847,"[19] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron
Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance.
NeurIPS, 33:2492‚Äì2502, 2020."
REFERENCES,0.5916030534351145,"[20] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang,
Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and
Angjoo Kanazawa. Nerfstudio: A modular framework for neural radiance field development.
arXiv preprint arXiv:2302.04264, 2023."
REFERENCES,0.5954198473282443,"[21] Carl Schissler, Christian Loftin, and Dinesh Manocha. Acoustic classification and optimization
for multi-modal rendering of real-world scenes. IEEE TVCG, 24(3):1246‚Äì1259, 2017."
REFERENCES,0.5992366412213741,"[22] Dingzeyu Li, Timothy R. Langlois, and Changxi Zheng. Scene-aware audio for 360¬∞ videos.
ACM TOG, 37(4), 2018."
REFERENCES,0.6030534351145038,"[23] Zhenyu Tang, Rohith Aralikatti, Anton Jeran Ratnarajah, and Dinesh Manocha. Gwa: A large
high-quality acoustic dataset for audio processing. In SIGGRAPH, pages 1‚Äì9, 2022."
REFERENCES,0.6068702290076335,"[24] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Doso-
vitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained
Photo Collections. In CVPR, 2021."
REFERENCES,0.6106870229007634,"[25] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for
space-time view synthesis of dynamic scenes. In CVPR, 2021."
REFERENCES,0.6145038167938931,"[26] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from
dynamic monocular video. In ICCV, 2021."
REFERENCES,0.6183206106870229,"[27] Tianye Li, Mira Slavcheva, Michael Zollh√∂fer, Simon Green, Christoph Lassner, Changil Kim,
Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard A. Newcombe, and Zhaoyang
Lv. Neural 3d video synthesis from multi-view video. In CVPR, pages 5511‚Äì5521, 2022."
REFERENCES,0.6221374045801527,"[28] Hang Zhou, Xudong Xu, Dahua Lin, Xiaogang Wang, and Ziwei Liu. Sep-stereo: Visually
guided stereophonic audio generation by associating source separation. In ECCV, 2020."
REFERENCES,0.6259541984732825,"[29] Xudong Xu, Hang Zhou, Ziwei Liu, Bo Dai, Xiaogang Wang, and Dahua Lin. Visually informed
binaural audio generation without binaural audios. In CVPR, 2021."
REFERENCES,0.6297709923664122,"[30] Pedro Morgado, Nuno Vasconcelos, Timothy R. Langlois, and Oliver Wang. Self-supervised
generation of spatial audio for 360¬∞ video. In NeurIPS, pages 360‚Äì370, 2018."
REFERENCES,0.6335877862595419,"[31] Changan Chen, Ruohan Gao, Paul Calamia, and Kristen Grauman. Visual acoustic matching.
In CVPR, 2022."
REFERENCES,0.6374045801526718,"[32] V.R. Algazi, R.O. Duda, D.M. Thompson, and C. Avendano. The cipic hrtf database. In
Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and
Acoustics (Cat. No.01TH8575), pages 99‚Äì102, 2001. doi: 10.1109/ASPAA.2001.969552."
REFERENCES,0.6412213740458015,"[33] Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia,
Dhruv Batra, Philip W Robinson, and Kristen Grauman. Soundspaces 2.0: A simulation
platform for visual-acoustic learning. arXiv, 2022."
REFERENCES,0.6450381679389313,"[34] Zhenyu Tang, Nicholas J Bryan, Dingzeyu Li, Timothy R Langlois, and Dinesh Manocha.
Scene-aware audio rendering via deep acoustic analysis. IEEE TVCG, 2020."
REFERENCES,0.648854961832061,"[35] Anton Ratnarajah, Shi-Xiong Zhang, Meng Yu, Zhenyu Tang, Dinesh Manocha, and Dong Yu.
Fast-rir: Fast neural diffuse room impulse response generator. In ICASSP, pages 571‚Äì575, 2022."
REFERENCES,0.6526717557251909,"[36] Anton Ratnarajah, Zhenyu Tang, and Dinesh Manocha. Ir-gan: Room impulse response
generator for far-field speech recognition. In INTERSPEECH, pages 286‚Äì290, 2021."
REFERENCES,0.6564885496183206,"[37] Anton Ratnarajah, Zhenyu Tang, Rohith Aralikatti, and Dinesh Manocha. Mesh2ir: Neural
acoustic impulse response generator for complex 3d scenes. In ACM MM, pages 924‚Äì933, 2022."
REFERENCES,0.6603053435114504,"[38] Alexander Richard, Dejan Markovic, Israel D Gebru, Steven Krenn, Gladstone Butler, Fernando
de la Torre, and Yaser Sheikh. Neural synthesis of binaural speech from mono audio. In ICLR,
2021."
REFERENCES,0.6641221374045801,"[39] Alexander Richard, Peter Dodds, and Vamsi Krishna Ithapu. Deep impulse responses: Estimat-
ing and parameterizing filters with deep networks. In ICASSP, pages 3209‚Äì3213, 2022."
REFERENCES,0.6679389312977099,"[40] Sagnik Majumder, Changan Chen, Ziad Al-Halah, and Kristen Grauman. Few-shot audio-visual
learning of environment acoustics. In NeurIPS, 2022."
REFERENCES,0.6717557251908397,"[41] James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. SIGGRAPH, 18(3):
165‚Äì174, 1984."
REFERENCES,0.6755725190839694,"[42] Nelson Max. Optical models for direct volume rendering. IEEE TVCG, 1(2):99‚Äì108, 1995."
REFERENCES,0.6793893129770993,"[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, pages 770‚Äì778, 2016."
REFERENCES,0.683206106870229,"[44] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, pages 248‚Äì255, 2009."
REFERENCES,0.6870229007633588,"[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998‚Äì6008,
2017."
REFERENCES,0.6908396946564885,"[46] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR,
pages 4104‚Äì4113, 2016."
REFERENCES,0.6946564885496184,"[47] Adobe Inc. Adobe audition. Software, 2023. URL https://www.adobe.com/products/
audition.html."
REFERENCES,0.6984732824427481,"[48] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah
Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan,
Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt
Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training
home assistants to rearrange their habitat. In NeurIPS, 2021."
REFERENCES,0.7022900763358778,"[49] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana
Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.
Habitat: A Platform for Embodied AI Research. In ICCV, 2019."
REFERENCES,0.7061068702290076,"[50] Xiph.Org Foundation. Xiph opus. https://opus-codec.org/, 2012."
REFERENCES,0.7099236641221374,"[51] International Organization for Standardization. Advanced audio coding (aac). ISO/IEC 13818-
7:2006, 2006."
REFERENCES,0.7137404580152672,"[52] Manik Varma and Andrew Zisserman. Classifying images of materials: Achieving viewpoint
and illumination independence. In ECCV, pages 255‚Äì271, 2002."
REFERENCES,0.7175572519083969,"[53] Zhoutong Zhang, Jiajun Wu, Qiujia Li, Zhengjia Huang, James Traer, Josh H McDermott,
Joshua B Tenenbaum, and William T Freeman. Generative modeling of audible shapes for
object perception. In ICCV, pages 1251‚Äì1260, 2017."
REFERENCES,0.7213740458015268,"[54] Changan Chen, Wei Sun, David Harwath, and Kristen Grauman. Learning audio-visual derever-
beration. In ICASSP, pages 1‚Äì5, 2023."
REFERENCES,0.7251908396946565,"[55] Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb:
Cross-modal reverb impulse response synthesis. In ICCV, pages 286‚Äì295, 2021."
REFERENCES,0.7290076335877863,"[56] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF‚àí‚àí:
Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064,
2021."
REFERENCES,0.732824427480916,"[57] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting
neural radiance fields. In ICCV, 2021."
REFERENCES,0.7366412213740458,"[58] Thomas M√ºller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics
primitives with a multiresolution hash encoding. ACM TOG, 41(4):1‚Äì15, 2022."
REFERENCES,0.7404580152671756,"[59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. NeurIPS, 32, 2019."
REFERENCES,0.7442748091603053,"[60] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, ICLR, 2015."
REFERENCES,0.7480916030534351,"[61] Julius Orion Smith. Mathematics of the discrete Fourier transform (DFT): with audio applica-
tions. 2008."
REFERENCES,0.7519083969465649,"A
Additional Visualization Results"
REFERENCES,0.7557251908396947,"We present additional results in Figure 7 that demonstrate AV-NeRF‚Äôs capability of generating
distance-aware auditory effects. For each scene, we select a fixed source audio and synthesize new
binaural audio at varying positions and distances from the sound source. The first row displays the
rendered images, while the second row showcases the corresponding rendered audio. As illustrated
in the figure, the amplitude of the rendered audio diminishes as the distance between the source and
the camera increases, and conversely, it increases as the camera approaches the source."
REFERENCES,0.7595419847328244,"Approach Sound Source
Leave Sound Source"
REFERENCES,0.7633587786259542,Rendered Image
REFERENCES,0.767175572519084,Rendered Audio L R
REFERENCES,0.7709923664122137,Approach Sound Source
REFERENCES,0.7748091603053435,Rendered Image
REFERENCES,0.7786259541984732,Rendered Audio L R
REFERENCES,0.7824427480916031,Leave Sound Source
REFERENCES,0.7862595419847328,Rendered Image
REFERENCES,0.7900763358778626,Rendered Audio L R
REFERENCES,0.7938931297709924,"Figure 7: Distance-aware audio rendering. We show some example scenes where AV-NeRF can
successfully synthesize consistent binaural audio with the camera movements."
REFERENCES,0.7977099236641222,"B
Failure Cases"
REFERENCES,0.8015267175572519,"We include several failure cases in Figure 8, where we present the rendered images, synthesized
binaural audio, and ground-truth audio. AV-NeRF makes wrong acoustic predictions when the
audio-visual scene involves noticeable noise or the AV-Mapper can not extract reliable material and
geometry information from the visual space."
REFERENCES,0.8053435114503816,"Because we work for real-world audio-visual learning, there inevitably exist various types of noise
in a scene, e.g., sounds from refrigerators, air conditioning, wind, or even workers make noise
when collecting data. These ambient noises are recorded by the microphone and are present in the
ground-truth binaural audio, which hinders AV-NeRF from accurately learning the acoustic field. In
Figure 8 (a), the ground-truth audio (used for training) marked with red boxes contains noticeable
noise, resulting in inaccurate audio rendering."
REFERENCES,0.8091603053435115,"AV-NeRF relies on the AV-Mapper to extract reliable material and geometry information from input
images, enabling a comprehensive understanding of the audio-visual environment. However, in cases
where the AV-Mapper fails to extract meaningful information from certain images (e.g., images of
plain walls or whiteboards), AV-NeRF predicts erroneous acoustic masks. In Figure 8 (b), the rendered
audio marked with black boxes displays inconsistent waveforms compared to the ground-truth audio,
indicating a model failure due to meaningless material and geometry information."
REFERENCES,0.8129770992366412,"Rendered Image
Rendered Image
Rendered Audio
GT Audio
Rendered Audio
GT Audio L R L R"
REFERENCES,0.816793893129771,(a) Noisy environment.
REFERENCES,0.8206106870229007,"Rendered Image
Rendered Image
Rendered Audio
GT Audio
Rendered Audio
GT Audio L R L R"
REFERENCES,0.8244274809160306,(b) Meaningless visual input.
REFERENCES,0.8282442748091603,"Figure 8: Failure cases. We present failure cases in which AV-NeRF makes wrong acoustic predictions.
For each case, we show the rendered image, the rendered audio, and the recorded ground-truth audio."
REFERENCES,0.8320610687022901,"C
Rationality of AV-Mapper"
REFERENCES,0.8358778625954199,"In our paper, we leverage RGB and depth images as implicit indicators of environmental material
properties and geometry. While these images enable the model to perceive the environment implicitly,
it is important to note that achieving a precise one-to-one mapping from images to material properties
or geometry is not guaranteed for AV-Mapper. Certain corner cases, such as a black desk and object
occlusion, can lead to the failure of AV-Mapper."
REFERENCES,0.8396946564885496,"Nevertheless, existing studies have demonstrated the feasibility of inferring material and geometry
information from images. For instance, back in 2002, Varma and Zisserman [52] proposed a filter-
based approach for material classification in images. More recently, in 2017, Zhang et al. [53]
successfully recognized material and shape attributes using visual data. Moreover, there is a body
of audio-visual research indicating that extracted geometry and material information via ResNet
networks can contribute to audio synthesis [54, 55]. Our paper further supports this concept. As
illustrated in Table 2 (left), the inclusion of AV-Mapper in AV-NeRF can improve the MAG score by
16% from 1.791 to 1.504 and enhance the ENV score by 3% from 0.150 to 0.145."
REFERENCES,0.8435114503816794,"D
Necessity of Distance and Direction Coordinates"
REFERENCES,0.8473282442748091,Table 4: Necessity of distance and direction information.
REFERENCES,0.851145038167939,"Methods
Overall
Position
Direction
MAG
ENV"
REFERENCES,0.8549618320610687,"1.822
0.156
‚úì
1.817
0.155
‚úì
1.688
0.148
‚úì
‚úì
1.504
0.145"
REFERENCES,0.8587786259541985,"To validate the necessity of distance and direction coordinates when modeling audio fields, we
conduct additional ablation studies. We intentionally exclude positional and directional coordinates
when representing an auditory scene and evaluate the performance without these inputs. The results
are presented in Table 4. AV-NeRF achieves a MAG metric score of 1.504 and an ENV metric score
of 0.145 when both position and direction information are utilized. Removing either positional or
directional input leads to a performance drop between 2% to 21%. In the absence of both positional
and directional inputs, performance degrades further to 1.822 on the MAG metric and 0.156 on"
REFERENCES,0.8625954198473282,"the ENV metric. These outcomes distinctly affirm the indispensable role of distance and direction
coordinates in acoustic modeling."
REFERENCES,0.8664122137404581,"E
Multiple Sound Sources"
REFERENCES,0.8702290076335878,"Given that scenes with a single sound source may not fully represent real-world complexities, we
extend both the RWAVS dataset and the AV-NeRF model to support multi-source scenes."
REFERENCES,0.8740458015267175,"Specifically, we collect two multi-source scenes, adhering to the recording settings outlined in Section
5.1. The only modification we make is placing two sound sources instead of a single one in the
environment, thus creating multiple sound sources. We extend AV-NeRF by stacking multiple equal
A-NeRF modules to support the parameterization of multi-source acoustic fields. AV-NeRF generates
acoustic masks for each sound source separately. We conduct experiments in these multi-source
scenes and present the performance of AV-NeRF and other baselines in Table 5. As presented in
the table, AV-NeRF outperforms other baselines in both MAG (0.282) and ENV (0.063) metrics.
The experimental results clearly demonstrate the proposed method‚Äôs capability to effectively handle
multiple sound sources."
REFERENCES,0.8778625954198473,Table 5: Comparison with state-of-the-art methods on multi-source scenes.
REFERENCES,0.8816793893129771,"Methods
Multi-source
MAG
ENV"
REFERENCES,0.8854961832061069,"Mono-Mono
1.949
0.172
Mono-Energy
0.533
0.075
Stereo-Energy
0.527
0.073"
REFERENCES,0.8893129770992366,"INRAS [13]
0.472
0.078
NAF [12]
0.401
0.080
Ours
0.282
0.063"
REFERENCES,0.8931297709923665,"F
Architectures"
REFERENCES,0.8969465648854962,"A-NeRF. A-NeRF consists of two Multilayer Perceptrons (MLPs), each comprising four linear layers
with an additional residual connection. The width of each linear layer, denoted as c, is set to 128 for
the RWAVS dataset and 256 for the SoundSpaces dataset. In A-NeRF, all linear layers are followed
by ReLU activation layers, except for the last layer, where the ReLU activation is replaced with the
Sigmoid function. The first MLP takes the listener‚Äôs position (x, y) and the frequency f ‚àà[0, F] as
input, where F represents the number of frequency bins. It predicts a mixture mask mm ‚ààR for
the given frequency f and generates a feature vector with c channels. Prior to feeding them into the
MLP, we apply positional encoding to the listener‚Äôs position (x, y) and the frequency f. We set the
maximum frequency used for positional encoding as 10."
REFERENCES,0.9007633587786259,"Then, we adopt relative transformation (Sec. 4.4) to project the listener‚Äôs direction Œ∏ into a high-
frequency space. We concatenate the transformed listener‚Äôs direction and the feature vector, and
feed it into the second MLP. The second MLP is appended with a Sigmoid layer and a scaling layer,
ensuring that the difference mask md estimated by the second MLP falls within the range of [‚àí1, 1].
For each frequency query f, A-NeRF estimates two masks: mm and md, both of which are scalars.
We iterate over all frequencies f ‚àà[0, F] to obtain the complete masks mm and md. After computing
the masks, we synthesize the target audio at according to the procedure discussed in Sec. 4.2."
REFERENCES,0.9045801526717557,"V-NeRF. We utilize the nerfacto model provided by nerf-studio [20] as the V-NeRF. This model
combines several well-established and successful methods, including camera pose refinement [56, 57],
image appearance conditioning [24], hash encoding, and proposal sampling [58]. Due to its robust and
effective performance on real-world data, we utilize the default settings of the nerfacto model without
making any architectural modifications. For more detailed information regarding the architecture of
V-NeRF, please refer to the documentation provided by nerf-studio."
REFERENCES,0.9083969465648855,"AV-Mapper. For each camera pose, we render both RGB and depth images using V-NeRF. We resize
images to 256 √ó 256 and center-crop to 224 √ó 224 prior to feeding them into a frozen ResNet-18 [43]"
REFERENCES,0.9122137404580153,"image encoder pre-trained on ImageNet-1K dataset [44]. ResNet-18 embeds the input image as a
512-dimension feature vector. We concatenate the RGB and the depth feature vectors, and input them
into the AV-Mapper to learn environmental knowledge of the sound acoustics. AV-Mapper projects
the input feature vectors to a latent embedding of c channels. We implement the AV-Mapper as a
3-layer MLP, with each intermediate linear layer followed by a ReLU activation function."
REFERENCES,0.916030534351145,"G
Implementation Details"
REFERENCES,0.9198473282442748,"RWAVS Dataset. We implement our method using the PyTorch framework [59]. We employ Adam
optimizer [60] with Œ≤1 = 0.9 and Œ≤2 = 0.999 for model optimization. The initial learning rate is set
to 5e‚àí4 and exponentially decreased to 5e‚àí6. We train the model for 100 epochs with a batch size
of 32."
REFERENCES,0.9236641221374046,"Before feeding the camera position (x, y) to A-NeRF, we normalize it within the range [‚àí1, 1] √ó
[‚àí1, 1] and apply positional encoding [16]. Additionally, we resample all audios to a frequency of
22050 Hz and utilize the Short-Time Fourier Transform (STFT) to convert waveform audios into the
time-frequency domain. For this transformation, we set the number of ffts as 512, the window length
as 512, and the hop length as 128. A Hanning window is applied during the process. Finally, we
compute both the magnitude and the phase from the spectrogram."
REFERENCES,0.9274809160305344,"We use magnitude distance (MAG) [29] and envelope distance (ENV) [30] as evaluation metrics for
audio quality. The MAG metric quantifies audio quality in the time-frequency domain and is defined
as follows:
MAG(mprd, mgt) = ||mprd ‚àímgt||2 ,
(8)
where mprd is the predicted magnitude, and mgt is the ground-truth magnitude. ENV metric that
measures the audio quality in the time domain is formatted as:
ENV(aprd, agt) = ||hilbert(aprd) ‚àíhilbert(agt)||2 ,
(9)
where aprd is the predicted audio, agt is the ground-truth audio, and hilbert is the Hilbert transfor-
mation function [61]."
REFERENCES,0.9312977099236641,"SoundSpaces Dataset. Our model is trained on the SoundSpaces dataset using the same training
settings as RWAVS dataset. We resample the impulse responses to 22050 Hz following INRAS [13].
The 2D position is normalized to [‚àí1, 1] √ó [‚àí1, 1] prior to positional encoding."
REFERENCES,0.9351145038167938,"We tailor A-NeRF for impulse response prediction with some minor modifications: (1) The input
frequency query f is replaced by a time query t ‚àà[0, T], where T represents the length of an impulse
response signal; (2) the first MLP only generates a feature vector while discarding the mixture mask
mm; (3) the second MLP predicts impulse response signals instead of difference mask md."
REFERENCES,0.9389312977099237,"Since the generated impulse responses are in the time domain, we employ STFT to convert them into
the time-frequency domain and calculate their magnitudes. We utilize an STFT configuration with
512 FFTs, a sliding window width of 512, a hop stride of 128, and a Hanning window. We supervise
the model training using the L2 distance between the ground-truth magnitudes and the predicted
magnitudes."
REFERENCES,0.9427480916030534,"For performance evaluation, we choose three metrics: T60, C50, and EDT [13]. T60 characterizes
the reverberation effects in an audio signal by measuring the time it takes for the audio‚Äôs energy to
attenuate by 60 dB. The T60 distance is calculated as follows:"
REFERENCES,0.9465648854961832,"T60(aprd, agt) = |T60(aprd) ‚àíT60(agt)|"
REFERENCES,0.950381679389313,"T60(agt)
,
(10)"
REFERENCES,0.9541984732824428,"where aprd and agt are the predicted and ground-truth impulse responses, respectively. C50 quantifies
the energy ratio between early reflections and late reverberation, allowing it to represent the clarity
and loudness of the audio. We format the C50 distance as:
C50(aprd, agt) = |C50(aprd) ‚àíC50(agt)| .
(11)
The EDT metric shares similarities with T60 but places greater emphasis on capturing the early
reflections of impulse responses. The EDT distance is defined as follows:
EDT(aprd, agt) = |EDT(aprd) ‚àíEDT(agt)| .
(12)
With these three metrics, we can evaluate the generation quality of impulse responses from different
aspects, including clarity, energy, and reverberation."
REFERENCES,0.9580152671755725,"H
Setup of RWAVS Dataset"
REFERENCES,0.9618320610687023,"Recording Devices. We have assembled a recording system, as depicted in Fig. 9, to capture high-
quality audio-visual scenes in real-world environments. Our system comprises a 3Dio Free Space
XLR binaural microphone for capturing stereo audio, a TASCAM DR-60DMKII for recording and
storing audio, and a GoPro Max for capturing accompanying videos."
REFERENCES,0.9656488549618321,"Figure 9: Recording system. It comprises a profes-
sional binaural microphone, a sports camera, and a
recorder."
REFERENCES,0.9694656488549618,"This system is portable, allowing us to posi-
tion it flexibly and capture scenes from different
camera poses. In addition, we utilized an LG
XBOOM 360 omnidirectional speaker to serve
as a sound source, which plays music repeatedly.
Figure 5 in the main paper illustrates the setup
used to record data in four distinct environments:
office, house, apartment, and outdoors. Within
each environment, we positioned the speaker at
multiple locations to capture diverse acoustic
effects. Each combination of environment and
sound source represents an audio-visual scene.
We collected data ranging from 10 to 25 min-
utes for each scene, resulting in a total collection
of 232 minutes (3.8 hours) of diverse data, en-
compassing various environments and source
positions."
REFERENCES,0.9732824427480916,"Example Scenes. In Fig.10, we present four
example scenes from RWAVS dataset along with the corresponding camera pose distributions. The
first column showcases images of the example scenes. The second column displays the camera poses
used for video recording: the black dot represents the sound source, and each blue triangle represents
a camera pose. We normalize all camera poses to the range of [‚àí1, 1] and visualize them in an x-y
plane from a top-down view. The third column contains 2D density heatmaps, which illustrate the
distribution of camera poses in each unit area: each pixel represents a unit area and its color shows the
number of camera poses in this area. As shown in the figure, RWAVS dataset encompasses densely
covered camera poses for each environment. We also analyze the distribution of camera directions
(shown in the last column of Fig.10). We present the direction distribution in a polar coordinate
system with the angle representing the viewing direction and the radius meaning the number of
camera poses in this region. RWAVS dataset consists of various viewpoints that approximately cover
a 360‚ó¶range of viewing directions. In summary, the RWAVS dataset comprises diverse environments
with a wide range of camera poses."
REFERENCES,0.9770992366412213,"Example Scenes
Camera Poses
Position Distribution
Direction Distribution"
REFERENCES,0.9809160305343512,"Sound
Source"
REFERENCES,0.9847328244274809,"Sound
Source"
REFERENCES,0.9885496183206107,"Sound
Source"
REFERENCES,0.9923664122137404,"Sound
Source"
REFERENCES,0.9961832061068703,"Figure 10: Example scenes. We present several example scenes along with their corresponding
camera pose distributions. We display the position density heatmap and the direction distribution
map. RWAVS dataset is composed of diverse environments with various camera poses."
