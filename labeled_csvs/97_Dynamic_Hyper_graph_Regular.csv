Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003676470588235294,"Recent advances in dynamic unigraph methods make predictions about links
between nodes at the current time, based on previous observations. The most
powerful of these approaches are based on regularising over previous graph
laplacians with a greater emphasis placed on more recent observations as opposed
to older observations. Concurrently, researchers have identified domains in
which hypergraph formulations of data provide more detailed information about
relationships between entities when those relationships can be multi-factored.
This work presents a natural synthesis of these two strands of work, extending
regularisation based on dynamic observations to hypergraphs. We present a
modelling framework for dynamic hypergraphs, an algorithm for 1-step ahead
prediction of the dynamic adjacency matrix, and experiments demonstrating the
improved accuracy of this algorithm compared to more conventional approaches."
INTRODUCTION,0.007352941176470588,"1
Introduction"
INTRODUCTION,0.011029411764705883,"Hypergraphs are a powerful way to represent, and reason about, data. Unlike unigraphs (graphs with
pairwise links), hypergraphs capture higher-order relationships between nodes, and these represen-
tations can lead to more expressive models that capture higher-order correlations [1]. Numerous
pre-existing data-sets can be readily represented as hypergraphs [1], and a number of applications
already exploit this capability, including: image retrieval [2], person re-identification [3], gene
selection [4, 5], disease prediction [6, 7], sub-type identification [8], functional network analysis
[9], recommendation systems [10] and link prediction in social networks [11]. The core feature of
hypergraphs that support this improved representational capability is the hyperedge, a generalisation
of a unigraph edge with arbitrary arity, which captures far more complicated relationships than
pairwise relations. We argue in this paper that above applications (and others) utilise data where
time information could be better exploited. For instance, time-stamped MRI data might facilitate
pre-emptive disease prediction, while the accuracy of person re-identification may benefit from a
greater emphasis on more recent images."
INTRODUCTION,0.014705882352941176,"Motivating example.
Consider a dataset of supermarket baskets each a collection of products
bought together. This data can be effectively captured by a hypergraph, where each product is a node,
and each basket is a hyperedge. Most simply, a hyperedge is a set of nodes, and here an intersection
of hyperedges indicates products common to the corresponding baskets. If we were to reduce this
hypergraph to a unigraph of products, such that two products share an edge if they share a basket, we
would lose some of this information. For instance, it may be that two baskets have more than two
products in common and the proposed unigraph does not capture this2. If time information is given"
INTRODUCTION,0.01838235294117647,"∗Equal contribution.
2We can alternatively model any hypergraph as a bipartite graph [12]. For this example both products and
baskets would then be nodes of different types, with a basket linking to any product it contains. However, this
inflates the size of the graph and it is still unclear how we might model or predict higher-order correlations
between products."
INTRODUCTION,0.022058823529411766,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
INTRODUCTION,0.025735294117647058,"for baskets, these data can be readily modelled as a dynamic hypergraph, where we partition shopping
baskets by time period (e.g. months), each period captured by a hypergraph, in order to capture or
analyse how product choices change over time. We argue that the ability to predict characteristics
of the hypergraph at the next time-step would have wide applicability in exploiting such dynamic
datasets. It is not clear how unigraph based methods would simulaneously capture higher order
relationships and this additional layer of time complexity."
INTRODUCTION,0.029411764705882353,"Non-negative matrix factorization.
Non-negative matrix factorization (NMF) aims to find two
non-negative matrices whose product provides a good approximation to some target matrix. The
non-negative constraints lead to an efficient, distributed parts-based representation that can aid in
the discovery of causal structure within data [13, 14]. Seminal work on Graph regularised NMF
[15] uses a regularization term based on the graph laplacian to enforce a NMF factorization that
respects the graph structure, namely that two node embeddings are sufficiently close to each other if
the corresponding nodes are connected in the graph. This idea is extended in [16] for 1-step ahead
prediction for dynamic (time-sequence of) unigraphs based on NMF methods with a recency weighted
regularization over previous graph laplacians. A method for hypergraph regularized NMF prediction
is presented in [17] to improve manifold learning used in image clustering."
INTRODUCTION,0.03308823529411765,"Contribution.
Here we present a method for 1-step ahead prediction for dynamic hypergraphs
using NMF, and with regularization based on the time-series of hyper-laplacians (hypergraph version
of laplacians). Previous works show that incorporating evolving information into feature extraction
can greatly enhance dynamic network analysis [16, 18, 19], and we argue that the same is true for
dynamic hypergraph analysis. Following ideas developed for ensemble manifold regularization
(EMR) [20], we assume that the graph regularization from various time points is located in the
convex hull of the previously given manifold candidates; for us, previous hyper-laplacians. Hence,
regularization by the time-dependent hyper-laplacians integrates the intrinsic geometrical structure of
the data space into the next step prediction, analogous to the dynamic unigraph method in [16]."
IMPLEMENTATION/METHODS,0.03676470588235294,"2
Approach"
IMPLEMENTATION/METHODS,0.04044117647058824,"Our approach operates on a dynamic hypergraph, a sequence of hypergraphs for times t = 1, . . . T:
Definition 1. A discrete dynamic hypergraph is time-indexed set G∗= {G1, ..., GT } with Gt =
(V, Et, Wt) the hypergraph at time t and V the static ordered vertex set (cardinality N). The
hyperedge set at time t is denoted Et = {ei,t}Mt
i=1 (cardinality Mt), where an arbitrary hyperedge is
an ordered tuple of nodes ei,t = (ν1t, ..., νnitt) i.e. for j < j′, νjt < νj′t (nit ≤N). The hyperedge
weight matrix at time t is a diagonal matrix Wt ∈RMt×Mt
+
with non-zero entries3 [Wt]ii > 0."
IMPLEMENTATION/METHODS,0.04411764705882353,"We extend the concepts of incidence, node degree, edge degree, adjacency and hyper-laplacian
matrices from [17] to our time dependent domain in the natural way to give the following definition.
Definition 2. At time t: the incidence matrix Ht ∈{0, 1}N×Mt denotes node membership of
hyperedges, i.e. [Ht]ij = 1 iff νi ∈ej,t; the node degree matrix Dν,t ∈RN×N
+
is a diagonal matrix
of node degrees, i.e. [Dν,t]ii = P
k[Wt]kk[Ht]ik; the edge degree matrix De,t ∈RMt×Mt
+
is a
diagonal matrix of edge degrees, i.e. [De,t]jj = P
k[Ht]kj; and the adjacency matrix is given by:"
IMPLEMENTATION/METHODS,0.04779411764705882,"At = HtWtHT
t −Dν,t ∈RV×V
+
The hyper-laplacian at time t (or laplacian for brevity) is given by"
IMPLEMENTATION/METHODS,0.051470588235294115,"Lt = Dν,t −At = 2Dν,t −HtWtHT
t ∈RV×V
+"
IMPLEMENTATION/METHODS,0.05514705882352941,"Given G∗, we aim to predict properties of the hypergraph GT +1. This problem can be cast in a few
different ways, this paper presents a method to predict AT +1, the adjacency matrix at time T + 1."
IMPLEMENTATION/METHODS,0.058823529411764705,"Approximating Dynamic Hypergraphs.
Here we define our dynamic hypergraph regularized
NMF (DyHGrNMF) method, which combines aspects of dynamic graph regularized NMF [16]
with hypergraph regularized NMF [17], to approximate AT +1 ≈BF, using positive low-rank real
matrices B, FT ∈RN×K
+
."
IMPLEMENTATION/METHODS,0.0625,"3We denote the (i, j)th element of matrix M by [M]ij"
IMPLEMENTATION/METHODS,0.0661764705882353,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
IMPLEMENTATION/METHODS,0.06985294117647059,"Definition 3. Given a dynamic hypergraph G∗, regularization strength λ > 0 and feature dimension
K ≤N, DyHGrNMF seeks to minimize the following objective:"
IMPLEMENTATION/METHODS,0.07352941176470588,"ODyHGrNMF (B, F, θ) = ∥AT −BF∥2
F + λ"
IMPLEMENTATION/METHODS,0.07720588235294118,"T −1
X"
IMPLEMENTATION/METHODS,0.08088235294117647,"t=1
θT −tTr(FLtFT ) ! (1)"
IMPLEMENTATION/METHODS,0.08455882352941177,where ∥·∥F denotes the Frobenius norm. This gives the following optimisation problem:
IMPLEMENTATION/METHODS,0.08823529411764706,"min
{B,F,θ}{ODyHGrNMF (B, F, θ)}
s.t.
B ≥0; F ≥0; θ ≥0"
IMPLEMENTATION/METHODS,0.09191176470588236,"The first term in Equation (1) ensures that BF is a good approximation for AT , the adjacency matrix
for the current time-step, while the second term regularizes this in terms of the hyper-laplacians of all
prior hypergraphs Gt, t ≤T −1 geometrically weighted according to their recency. As argued in
[16], this has the effect of encouraging two nodes νi, νj ∈V to be embedded4 close to one another,
i.e. P"
IMPLEMENTATION/METHODS,0.09558823529411764,"k([F]ik −[F]jk)2 is small, if they share hyperedges in previous time-steps and with a greater
emphasis on more recent graphs."
IMPLEMENTATION/METHODS,0.09926470588235294,"Update rules.
To make the approximation, we begin by an arbitrary initialisation of the basis
matrix, B ≥0 and feature matrix F ≥0, then iteratively update elements of these with the following
update rules, for all i = 1, . . . , N, j = 1, . . . , N and k = 1, . . . , K as follows"
IMPLEMENTATION/METHODS,0.10294117647058823,[B]ik ←[AT FT ]ik
IMPLEMENTATION/METHODS,0.10661764705882353,"[BFFT ]ik
· [B]ik
(2)"
IMPLEMENTATION/METHODS,0.11029411764705882,"[F]kj ←
[BT AT ]kj
h
BT BF + λ PT −1
t=1 θT −tFLt
i kj"
IMPLEMENTATION/METHODS,0.11397058823529412,"· [F]kj
(3)"
IMPLEMENTATION/METHODS,0.11764705882352941,"A detailed derivation of these update rules can be found in the Appendix. We also define an approach
to learn a symmetric approximation named Symmetric DyHGrNMF (SDyHGrNMF) the objective
function and update rules for which can also be found in the Appendix."
RESULTS/EXPERIMENTS,0.1213235294117647,"3
Data-sets and Evaluations"
RESULTS/EXPERIMENTS,0.125,"In order to evaluate the 1 step ahead adjacency matrix approximations defined in Section 2, we apply
our algorithms to the following data-sets:"
RESULTS/EXPERIMENTS,0.12867647058823528,1. Co-authorship: 20 years of time-stamped co-authorship data from [21].
RESULTS/EXPERIMENTS,0.1323529411764706,2. Email: 20 years of data from email-Enron dataset (sets of email addresses on emails) [22]
RESULTS/EXPERIMENTS,0.13602941176470587,"3. Stocks: Novel data-set derived from adjusted closing prices of top 9 blue-chip stocks from 16
December 2020 to 16 December 2022 [23], processed to give a dynamic hypergraph. Processing
used the time sequence of correlation matrices, {Σt}T
t=1, from adjusted closing prices for the
9 stocks over T time periods, to construct {Ht}T
t=1, where [Ht]ij = 1 iff [Σt]ij ≥c for some
c ∈(0, 1)."
RESULTS/EXPERIMENTS,0.13970588235294118,"Further details regarding the datasets are in the Appendix. For each dataset, there are Tmax time-
dependent hypergraphs {G1, ..., GTmax}, our experiments consider one prediction at time Tmax and
predict the adjacency matrix the next step ahead, with approximation bAT +1 = BF. The evaluation
score is the RMSE, i.e. ∥AT +1 −bAT +1∥0.5
F . This is consistent with that used for dynamic unilink
prediction in [16]. Note that the choice to predict the adjacency matrix is motivated by a number of
applications in of hypergraph learning that rely on good quality prediction of the adjacency matrix
for the target hypergraph, e.g. see [1]. In particular, link prediction and graph clustering can both
benefit from a node’s features (rows of F) being ‘close’ to the mean feature vector of hyperedges it
belongs to, and this ‘closeness’ is what the regularization term in Equation (1) controls."
RESULTS/EXPERIMENTS,0.14338235294117646,4We treat the ith column of F as the embedding for νi.
RESULTS/EXPERIMENTS,0.14705882352941177,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
RESULTS/EXPERIMENTS,0.15073529411764705,"Baselines.
We compare our DyHGrNMF (asymmetric) and SDyHGrNMF (symmetric) algorithms
with a selection of baselines. The HGrNMF-T and SHGrNMF-T algorithms are respectively asym-
metric and symmetric variants of the GrNMF algorithm in [16], where the regularizer is taken to
be LT −1, the laplacian at time T −1. Thus, to predict AT +1, DyHGrNMF and SDyHGrNMF
respectively seek to minimise the following objective functions:"
RESULTS/EXPERIMENTS,0.15441176470588236,"OHGrNMF −T (B, F, θ) = ∥AT −BF∥2
F + λTr(FLT −1FT )"
RESULTS/EXPERIMENTS,0.15808823529411764,"OSHGrNMF −T (F, θ) = ∥AT −FT F∥2
F + λTr(FLT −1FT )
HGrNMF-T does not incorporate the sequence of laplacians in an auto-regressive manner, only taking
a single laplacian from time T −1 into account. Thus comparison with DyHGrNMF evaluates the
relative benefits of incorporating information from previous timesteps (before T −1). A similar
comparison can be made between SHGrNMF-T and SDyGrNMF, but for symmetric approximations."
RESULTS/EXPERIMENTS,0.16176470588235295,"Table 1: RMSE over time (Score) between actual
(AT +1) and predicted ( bAT +1) adjacency matrices
for a variety of datasets and models. Lower scores
better. Best scores shown in bold."
RESULTS/EXPERIMENTS,0.16544117647058823,"Data-set
Model
Score"
RESULTS/EXPERIMENTS,0.16911764705882354,Co-authorship
RESULTS/EXPERIMENTS,0.17279411764705882,"DyHGrNMF (ours)
3.635
HGrNMF-T
3.909
SDyHGrNMF (ours)
18.89
SHGrNMF-T
18.89
Martingale
4.272
Collapsed Tensor
109.9 Email"
RESULTS/EXPERIMENTS,0.17647058823529413,"DyHGrNMF (ours)
1.144
HGrNMF-T
1.150
SDyHGrNMF (ours)
12.93
SHGrNMF-T
16.18
Martingale
1.150
Collapsed Tensor
5.629"
RESULTS/EXPERIMENTS,0.1801470588235294,Stocks (c = 0.3)
RESULTS/EXPERIMENTS,0.18382352941176472,"DyHGrNMF (ours)
0.4092
HGrNMF-T
0.5612
SDyHGrNMF (ours)
2.504
SHGrNMF-T
2.504
Collapsed Tensor
20.38"
RESULTS/EXPERIMENTS,0.1875,Stocks (c = 0.4)
RESULTS/EXPERIMENTS,0.19117647058823528,"DyHGrNMF (ours)
2.366
HGrNMF-T
3.566
SDyHGrNMF (ours)
10.94
SHGrNMF-T
10.94
Collapsed Tensor
78.57"
RESULTS/EXPERIMENTS,0.1948529411764706,Stocks (c = 0.5)
RESULTS/EXPERIMENTS,0.19852941176470587,"DyHGrNMF (ours)
1.423
HGrNMF-T
1.553
SDyHGrNMF (ours)
4.522
SHGrNMF-T
4.522
Collapsed Tensor
25.87"
RESULTS/EXPERIMENTS,0.20220588235294118,"We also include two simpler baselines. The
first is a matrix version of martingale predic-
tion, which predicts the adjacency matrix at time
T + 1 from the previous adjacency matrix, i.e.
bAT +1 = AT
[24]. The second is the Col-
lapsed Tensor method from [16], where the
next adjacency matrix is predicted as the mean
adjacency matrix from previous time-steps, i.e.
bAT +1 = 1"
RESULTS/EXPERIMENTS,0.20588235294117646,"T
PT
t=1 At."
RESULTS/EXPERIMENTS,0.20955882352941177,"Analysis.
Table 1 shows RMSE over time for
the proposed novel models and baselines. All
hyperparameters were optimised by grid-search
over feasible values (see appendix for details).
For each NMF based method, hyperparameters
include: feature dimension K – the number of
rows of F, regularisation strength λ, and num-
ber of outer iterations of algorithm (max 3). Dy-
namic hypergraph methods have an additional
hyperparameter θ, the geometric weighting pa-
rameter.
DyHGrNMF outperforms all other
models, and is the only model to consistently
outperform simple baselines. Symmetric NMF
models perform substantially worse than others,
including simple baselines."
CONCLUSION/DISCUSSION ,0.21323529411764705,"4
Discussion"
CONCLUSION/DISCUSSION ,0.21691176470588236,"This paper presents a new framework for mod-
elling dynamic hypergraphs, and presents 2
new models for approximating adjacency ma-
trices from past observations. Our algorithms,
namely DyHGrNMF and SDyHGrNMF, repre-
sent asymmetric and symmetric variants of hy-
pergraph regularized non-negative matrix fac-
torization for such dynamic hypergraphs, and
models combine the dynamic characteristics of
hypergraph models with the ability to exploit higher order relationships in NMF. Evaluations show
that our asymmetric model consistently predicts adjacency matrices more accurately than a competi-
tive baseline, HGrNMF-T, that does not integrate the entire history of observations. This shows that
the dynamic hypergraph structure as a whole captures useful information about future observations
and we argue that this could be effectively exploited in novel algorithms. For future work, we aim to
identify good applications to utilise these approximations, and explore how node embeddings can
be effectively exploited. Two promising methodological directions include 1) methods to predict
missing nodes from partial hyper-edges (extending link prediction beyond binary relationships) and 2)
methods to predict incidence matrices – higher order grouping of nodes beyond pairwise association."
CONCLUSION/DISCUSSION ,0.22058823529411764,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
REFERENCES,0.22426470588235295,References
REFERENCES,0.22794117647058823,"[1] Yue Gao, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing Zou. Hyper-
graph learning: Methods and practices. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(5):2548–2566, 2020. 1, 3
[2] Yuchi Huang, Qingshan Liu, and Dimitris Metaxas. ] video object segmentation by hypergraph
cut. In 2009 IEEE conference on computer vision and pattern recognition, pages 1738–1745.
IEEE, 2009. 1
[3] Wei Zhao, Shulong Tan, Ziyu Guan, Boxuan Zhang, Maoguo Gong, Zhengwen Cao, and Quan
Wang. Learning to map social network users by unified manifold alignment on hypergraph.
IEEE transactions on neural networks and learning systems, 29(12):5834–5846, 2018. 1
[4] Ze Tian, TaeHyun Hwang, and Rui Kuang. A hypergraph-based learning algorithm for classify-
ing gene expression and arraycgh data with prior knowledge. Bioinformatics, 25(21):2831–2838,
2009. 1
[5] Xiao Zheng, Wenyang Zhu, Chang Tang, and Minhui Wang. Gene selection for microarray data
classification via adaptive hypergraph embedded dictionary learning. Gene, 706:188–200, 2019.
1
[6] Yue Gao, Chong-Yaw Wee, Minjeong Kim, Panteleimon Giannakopoulos, Marie-Louise Mon-
tandon, Sven Haller, and Dinggang Shen. Mci identification by joint learning on multiple mri
data. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part II 18, pages
78–85. Springer, 2015. 1
[7] Wei Shao, Yao Peng, Chen Zu, Mingliang Wang, Daoqiang Zhang, Alzheimer’s Disease Neu-
roimaging Initiative, et al. Hypergraph based multi-task feature selection for multimodal
classification of alzheimer’s disease. Computerized Medical Imaging and Graphics, 80:101663,
2020. 1
[8] Emad Ramadan, Sudhir Perincheri, and David Tuck. A hyper-graph approach for analyzing
transcriptional networks in breast cancer. In Proceedings of the First ACM International
Conference on Bioinformatics and Computational Biology, pages 556–562, 2010. 1
[9] Li Xiao, Junqi Wang, Peyman H Kassani, Yipu Zhang, Yuntong Bai, Julia M Stephen, Tony W
Wilson, Vince D Calhoun, and Yu-Ping Wang. Multi-hypergraph learning-based brain functional
connectivity analysis in fmri data. IEEE transactions on medical imaging, 39(5):1746–1758,
2019. 1
[10] Quan Fang, Jitao Sang, Changsheng Xu, and Yong Rui. Topic-sensitive influencer mining in
interest-based social media networks via hypergraph learning. IEEE Transactions on Multimedia,
16(3):796–812, 2014. 1
[11] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. Revisiting user mobility
and social relationships in lbsns: a hypergraph embedding approach. In The world wide web
conference, pages 2147–2157, 2019. 1
[12] Qionghai Dai and Yue Gao. Mathematical foundations of hypergraph. In Hypergraph Computa-
tion, pages 19–40. Springer, 2023. 1
[13] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999. 2
[14] David A Ross and Richard S Zemel. Learning parts-based representations of data. Journal of
Machine Learning Research, 7(11), 2006. 2
[15] Deng Cai, Xiaofei He, Jiawei Han, and Thomas S Huang. Graph regularized nonnegative
matrix factorization for data representation. IEEE transactions on pattern analysis and machine
intelligence, 33(8):1548–1560, 2010. 2
[16] Xiaoke Ma, Penggang Sun, and Yu Wang. Graph regularized nonnegative matrix factorization
for temporal link prediction in dynamic networks. Physica A: Statistical mechanics and its
applications, 496:121–136, 2018. 2, 3, 4
[17] Kun Zeng, Jun Yu, Cuihua Li, Jane You, and Taisong Jin. Image clustering by hyper-graph
regularized non-negative matrix factorization. Neurocomputing, 138:209–217, 2014. 2
[18] Petter Holme and Jari Saramäki. Temporal networks. Physics reports, 519(3):97–125, 2012. 2"
REFERENCES,0.23161764705882354,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
REFERENCES,0.23529411764705882,"[19] Petter Holme. Modern temporal network theory: a colloquium. The European Physical Journal
B, 88:1–30, 2015. 2
[20] Bo Geng, Dacheng Tao, Chao Xu, Linjun Yang, and Xian-Sheng Hua. Ensemble manifold
regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(6):1227–
1233, 2012. 2
[21] Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Kleinberg. Simpli-
cial closure and higher-order link prediction. Proceedings of the National Academy of Sciences,
115(48):E11221–E11230, 2018. 3, 12
[22] Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simpli-
cial closure and higher-order link prediction. Proceedings of the National Academy of Sciences,
2018. ISSN 0027-8424. doi: 10.1073/pnas.1800683115. 3, 12
[23] Yahoo finance. https://uk.finance.yahoo.com/. 3, 12
[24] Emile Richard, Stéphane Gaïffas, and Nicolas Vayatis. Link prediction in graphs with autore-
gressive features. The Journal of Machine Learning Research, 15(1):565–593, 2014. 4
[25] Da Kuang, Chris Ding, and Haesun Park. Symmetric nonnegative matrix factorization for graph
clustering. In Proceedings of the 2012 SIAM international conference on data mining, pages
106–117. SIAM, 2012. 8
[26] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback
datasets. In 2008 Eighth IEEE international conference on data mining, pages 263–272. Ieee,
2008. 7
[27] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical
University of Denmark, 7(15):510, 2008. 7, 11
[28] Cornell arb coauth-dblp. https://www.cs.cornell.edu/~arb/data/coauth-DBLP/, . 12
[29] Cornell arb email-enron. https://www.cs.cornell.edu/~arb/data/email-Enron/, . 12"
OTHER,0.23897058823529413,"A
Appendix"
OTHER,0.2426470588235294,"Note 1. We sometimes define the basis and feature matrices at time T as B and F i.e. we remove the
time T subscript for brevity and reduced clutter of notation.
Definition 4. Sequence of degree matrices for dynamic hyperedges in a dynamic hypergraph:
The (time-ordered) sequence of degree matrices for dynamic hyperedges in a dynamic hypergraph"
OTHER,0.24632352941176472,"can be represented as {De,t}T
t=1 where De,t ∈RMt×Mt
+
with entries [De,t]ij."
OTHER,0.25,"The scalar [De,t]ij, corresponding to the i-th row and the j-th column entry of De,t, can be written
as:"
OTHER,0.2536764705882353,"[De,t]ij = De,t(ei,t, ej,t) =
δe,t(ei,t)
if i = j
0
if i ̸= j , ∀ei,t ∈Et"
OTHER,0.25735294117647056,"where δe,t(ei,t) is the degree for some arbitrary hyperedge ei,t ∈Et, that can be written in terms of
column sums of the incidence matrix in the following way:"
OTHER,0.2610294117647059,"δe,t(ei,t) = P|V|
k=1[Ht]kj = P|V|
k=1 1{νk ∈ej,t}.
Definition 5. Objective function and constrained optimisation formulation for SDyHGrNMF (1-step):
The loss function to minimise for SDyHGrNMF (1-step) is:"
OTHER,0.2647058823529412,"OSDyHGrNMF (F, θ) = ∥AT ∥2
F + ∥FT F∥2
F −2Tr{AT (FT F)T } + λ PT −1
t=1 θT −tTr(FLtFT )
where λ controls the relative importance of the regulariser. Thus, we get the following optimisation
problem:"
OTHER,0.26838235294117646,"min
{F,θ,λ}{OSDyHGrNMF (F, θ)}
s.t.
F ≥0; θ ≥0, λ > 0"
OTHER,0.27205882352941174,"Update rules.
To make the approximation for SDyHGrNMF (1-step), we begin by an arbitrary
initialisation of the feature matrix F ≥0, then iteratively update elements of these with the following
update rules, for all i = 1, . . . , N, j = 1, . . . , N and k = 1, . . . , K as follows"
OTHER,0.2757352941176471,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.27941176470588236,"[F]kj ←
2[FAT ]kj
2[FFT F]kj + λ PT −1
t=1 θT −t[FLt]kj
· [F]kj
(4)"
OTHER,0.28308823529411764,"Using the above update rule to find F, the hyper-links at time T + 1 (1-step) are predicted as:
ˆ
AT +1 = FT F."
OTHER,0.2867647058823529,Algorithm 1: DyHGrNMF (1-step)
OTHER,0.29044117647058826,"1 function dyhgrnmf_1_step(Inputs) ;
Input
:seq_adj_matrices: Sequence of adjacency matrices {At}T
t=1
Input
:seq_laplacian_matrices: Sequence of laplacian matrices {Lt}T −1
t=1
Input
:dimensions: The number of components in the initial step of non-hypergraph
regularised NMF
Input
:_lambda: Explicit regularisation parameter λ
Input
:theta: Implicit regularisation parameter θ
Input
:K: Number of iterations of the algorithm to perform
Output :Updated basis, feature matrices at time T −1 using which we predict
AT = FT
T −1FT −1"
OTHER,0.29411764705882354,2 BT −1 ←sklearn.decomposition.NMF(dimensions).fit_transform(AT −1)
OTHER,0.2977941176470588,3 FT −1 ←sklearn.decomposition.NMF(dimensions).components_
OTHER,0.3014705882352941,4 for _ in range(K) do
OTHER,0.30514705882352944,"5
for i in range(#{Bij,T −1}N
i=1) do"
OTHER,0.3088235294117647,"6
for j in range(#{Bij,T −1}K
j=1) do"
OTHER,0.3125,"7
Bij,T −1 ←(
(AT −1F T
T −1)ij
(BT −1FT −1F T
T −1)ij )(Bij,T −1)}end"
OTHER,0.3161764705882353,"8
end"
OTHER,0.31985294117647056,"9
for i in range(#{Fij,T −1}K
i=1) do"
OTHER,0.3235294117647059,"10
for j in range(#{Fij,T −1}N
j=1) do"
OTHER,0.3272058823529412,"11
(Fij,T −1) ←"
OTHER,0.33088235294117646,"(
(BT
T −1AT −1)ij
(BT
T −1BT −1FT −1)ij+(_lambda) PT −1
t=1 (theta)T −t(FT −1Lt)ij )(Fij,T −1)end"
OTHER,0.33455882352941174,"12
end"
OTHER,0.3382352941176471,"13
end"
OTHER,0.34191176470588236,"Derivation of update rules for DyHGrNMF (1-step).
In the constrained optimisation problem
for DyHGrNMF, the objective function is to be minimised, subject to a certain set of inequality
constraints."
OTHER,0.34558823529411764,"So let’s attempt to apply the Karush-Kuhn-Tucker (KKT) approach to constrained optimisation
in preliminaries, to ODyHGrNMF . First, note that the direct constrained optimisation procedure
cannot be applied to ODyHGrNMF , since the Lagrangian for this problem does not allow for direct,
closed-form solutions of B and F. By fixing θ, λ, an iterative 2-step strategy as done in GNMF,
GrNMF and HNMF is used. At each iteration, either B or F is optimised while the other remains
fixed. Iterations are repeated until the algorithm converges or the maximum number of iterations is
reached. This is called the Alternating Non-negative Least Squares (ALS) method [26]."
OTHER,0.3492647058823529,"As part of the derivation, let’s first prove a Lemma that expresses ODyHGrNMF just in terms of traces
of matrices, so that the KKT optimisation procedure can be applied to the optimisation procedure in
DyHGrNMF (in particular, first order conditions of the Lagrangian can be more easily evaluated, by
using properties of derivatives of traces of matrices [27])."
OTHER,0.35294117647058826,"Lemma 1.
The optimisation problem for DyHGrNMF (1-step) can be reformulated as:"
OTHER,0.35661764705882354,"min
{B,F}{ODyHGrNMF (B, F)}
s.t.
B ≥0; F ≥0"
OTHER,0.3602941176470588,"ODyHGrNMF (B, F) = Tr(BFFT BT ) −2Tr(AT FT BT ) + λ PT −1
t=1 θT −tTr(FLtFT )"
OTHER,0.3639705882352941,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.36764705882352944,Algorithm 2: SDyHGrNMF (1-step)
OTHER,0.3713235294117647,"1 function sdyhgrnmf_1_step(Inputs) ;
Input
:seq_adj_matrices: Sequence of adjacency matrices {At}T
t=1
Input
:seq_laplacian_matrices: Sequence of laplacian matrices {Lt}T −1
t=1
Input
:dimensions: The number of components in the initial step of non-hypergraph
regularised SNMF
Input
:_lambda: Explicit regularisation parameter λ
Input
:theta: Implicit regularisation parameter θ
Input
:K: Number of iterations of the algorithm to perform
Output :Updated feature matrix at time T −1 using which we predict bAT = FT
T −1FT −1"
OTHER,0.375,"2 FT −1 ←SymmNMF(AT −1, dimensions)"
OTHER,0.3786764705882353,"3 # The SymmNMF function is an implementation of non-hypergraph regularised symmetric NMF
from [25]."
OTHER,0.38235294117647056,4 for _ in range(K) do
OTHER,0.3860294117647059,"5
for i in range(#{Fij,T −1}K
i=1) do"
OTHER,0.3897058823529412,"6
for j in range(#{Fij,T −1}N
j=1) do"
OTHER,0.39338235294117646,"7
(Fij,T −1) ←(
2(FT −1AT −1)ij
2(FT −1F T
T −1FT −1)ij+_lambda PT −1
t=1 θT −t(FT −1Lt)ij )(FT −1)ijend"
OTHER,0.39705882352941174,"8
end"
OTHER,0.4007352941176471,"9
end"
OTHER,0.40441176470588236,"Proof of Lemma 1.
By the optimisation problem for DyHGrNMF (1-step ahead):"
OTHER,0.40808823529411764,"min
{B,F,θ,λ}{ODyHGrNMF (B, F, θ)}
s.t.
B ≥0; F ≥0; θ ≥0, λ > 0 ="
OTHER,0.4117647058823529,"min
{B,F,θ,λ}{∥AT ∥2
F + ∥BF∥2
F −2Tr{AT (BF)T } + λ"
OTHER,0.41544117647058826,"T −1
X"
OTHER,0.41911764705882354,"t=1
θT −tTr(FLtFT )}"
OTHER,0.4227941176470588,"s.t.
B ≥0; F ≥0; θ ≥0, λ > 0 ="
OTHER,0.4264705882352941,"min
{B,F,θ,λ}{∥AT ∥2
F + ∥BF∥2
F −2Tr{AT (BF)T } + λ"
OTHER,0.43014705882352944,"T −1
X"
OTHER,0.4338235294117647,"t=1
θT −tTr(FLtFT )}"
OTHER,0.4375,"s.t.
B ≥0; F ≥0"
OTHER,0.4411764705882353,"(since θ, λ are assumed to be fixed in the iterative 2-step optimisation procedure) ="
OTHER,0.44485294117647056,"min
{B,F,θ,λ}{Tr(AT AT
T ) + Tr(BFFT BT ) −2Tr{AT FT BT } + λ"
OTHER,0.4485294117647059,"T −1
X"
OTHER,0.4522058823529412,"t=1
θT −tTr(FLtFT )}"
OTHER,0.45588235294117646,"s.t.
B ≥0; F ≥0"
OTHER,0.45955882352941174,"(by definition of ∥.∥2
F and (AB)T = BT AT ) ="
OTHER,0.4632352941176471,"min
{B,F,θ,λ}{Tr(BFFT BT ) −2Tr{AT FT BT } + λ"
OTHER,0.46691176470588236,"T −1
X"
OTHER,0.47058823529411764,"t=1
θT −tTr(FLtFT )}"
OTHER,0.4742647058823529,"s.t.
B ≥0; F ≥0"
OTHER,0.47794117647058826,"since Tr(AT AT
T ) is not a function of B or F, that are the arguments with respect to which the
objective function is minimised. (Q.E.D.)"
OTHER,0.48161764705882354,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.4852941176470588,"Back to derivation of update rules for DyHGrNMF (1-step).
The KKT optimisation procedure
can now be applied to the optimisation problem for DyHGrNMF (1-step). The Lagrangian for this
problem is:"
OTHER,0.4889705882352941,"L(B, F, ΨT , ΦT ) = Tr(BFFT BT )−2Tr(AT FT BT )+λ PT −1
t=1 θT −tTr(FLtFT )+Tr(ΦT B)+
Tr(ΨT F). Since the trace is only defined for square matrices, hence ΦT = [ΦT ]ij ∈RK×N and
ΨT = [ΨT ]ij ∈RN×K are the Lagrange multipliers for the constraints [B]ij, [F]ij respectively (at
time T)."
OTHER,0.49264705882352944,The partial derivatives of L with respect to B and F are:
OTHER,0.4963235294117647,"∂L
∂B =
∂
∂B{Tr(BFFT BT )} −2 ∂"
OTHER,0.5,"∂B{Tr(AT FT BT )} +
∂
∂B{Tr(ΦT B)} (the terms independent of
B are trivially dropped)"
OTHER,0.5036764705882353,"Note that:
∂
∂B{Tr(BFFT BT )} = B(FFT )T + B(FFT ) = 2BFFT since
∂
∂X(Tr(XBXT )) =
XBT + XB for some matrices B, X and (AB)T = BT AT"
OTHER,0.5073529411764706,"∂
∂B{Tr(AT FT BT )} = AT FT since
∂
∂X(Tr(BXT )) = B"
OTHER,0.5110294117647058,"and
∂
∂B{Tr(ΦT B)} = ΦT. Hence,"
OTHER,0.5147058823529411,"∂L
∂B = 2BFFT −2AT FT + ΦT. Now,"
OTHER,0.5183823529411765,"∂L
∂F
=
∂
∂F{Tr(BFFT BT )} −2 ∂"
OTHER,0.5220588235294118,"∂F{Tr(AT FT BT )} + λ PT −1
t=1 θT −t ∂"
OTHER,0.5257352941176471,"∂F{Tr(FLtFT )} +
∂
∂F{Tr(ΨT F)} (the terms independent of F are trivially dropped)"
OTHER,0.5294117647058824,"Note that:
∂
∂F{Tr(BFFT BT )} = BT BF + BT BF = 2BT BF (Since
∂
∂X{Tr(AXBXT C)} =
AT CT XBT + CAXB. Set B = IN where IN is the (N × N)-identity matrix which gives:
∂
∂X{Tr(AXXT C)} = AT CT X + CAX)"
OTHER,0.5330882352941176,"∂
∂F{Tr(AT FT BT )} =
∂
∂F{Tr(AT (BF)T )} =
∂
∂F{Tr((BF)T AT )} since Tr(AB) = Tr(BA)"
OTHER,0.5367647058823529,"=
∂
∂F{Tr(FT BT AT )} = BT AT since
∂
∂X(Tr(XT B)) = B"
OTHER,0.5404411764705882,"∂
∂F{Tr(FLtFT )} = FLT
t + FLt since
∂
∂X(Tr(XBXT )) = XBT + XB"
OTHER,0.5441176470588235,"Recall that the un-normalised dynamic hyper-laplacian is Lt = Dν,t −At, so LT
t = (Dν,t −At)T =
DT
ν,t −AT
t = Dν,t −At since the degree matrix is diagonal hence symmetric and the adjacency
matrix is symmetric. Hence the hyper-laplacian is symmetric and so we can write:"
OTHER,0.5477941176470589,"∂
∂F{Tr(FLtFT )} = 2FLt"
OTHER,0.5514705882352942,"and
∂
∂F{Tr(ΨT F)} = ΨT
Therefore the partial derivative of L with respect to feature matrix F is:"
OTHER,0.5551470588235294,"∂L
∂F = 2BT BF −2BT AT + λ PT −1
t=1 θT −t2FLt + ΨT"
OTHER,0.5588235294117647,"By the Karush-Kuhn-Tucker (KKT) conditions in the KKT optimisation procedure, we have: [ΦT]ij ·
[B]ij = 0 and [ΨT]ij · [F]ij = 0. Therefore, we can write the 2 partial derivatives evaluated above,
in scalar form (the ij-th element of the matrix expression) as (where i = 1, ..., N, j = 1, ..., K in
(1); i = 1, ..., K, j = 1, ..., N in (2)): [ ∂L"
OTHER,0.5625,∂B]ij = 2[BFFT ]ij −2[AT FT ]ij + [ΦT ]ij (1) [ ∂L
OTHER,0.5661764705882353,"∂F]ij = 2[BT BF]ij −2[BT AT ]ij + λ PT −1
t=1 2θT −t[FLt]ij + [ΨT ]ij (2)"
OTHER,0.5698529411764706,"where Xijt = [Xt]ij for some matrix X. As part of the KKT optimisation procedure, the critical
points of the Lagrangian are found by setting ∂L"
OTHER,0.5735294117647058,∂B = 0 and ∂L
OTHER,0.5772058823529411,"∂F = 0. Let’s multiply (1) by [B]ij and
(2) by [F]ij, which gives:"
OTHER,0.5808823529411765,2[BFFT ]ij · [B]ij −2[AT FT ]ij · [B]ij + [ΦT ]ij · [B]ij = 0 (1)
OTHER,0.5845588235294118,"2[BT BF]ij · [F]ij −2[BT AT ]ij · [F]ij + λ PT −1
t=1 2θT −t[FLt]ij · [F]ij + [ΨT ]ij · [F]ij = 0 (2)"
OTHER,0.5882352941176471,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.5919117647058824,"Setting [ΦT ]ij · [B]ij = [ΨT ]ij · [F]ij = 0 by the KKT-conditions, and dividing both sides of (1)
and (2) by 2:"
OTHER,0.5955882352941176,[BFFT ]ij · [B]ij −[AT FT ]ij · [B]ij = 0 (1)
OTHER,0.5992647058823529,"[BT BF]ij · [F]ij −[BT AT ]ij · [F]ij + λ PT −1
t=1 θT −t[FLt]ij · [F]ij = 0 (2)"
OTHER,0.6029411764705882,⇒[BFFT ]ij · [B]ij = [AT FT ]ij · [B]ij (1)
OTHER,0.6066176470588235,"⇒[BT BF]ij · [F]ij + λ PT −1
t=1 θT −t[FLt]ij · [F]ij = [BT AT ]ij · [F]ij (2)"
OTHER,0.6102941176470589,"It can be seen from the first order conditions that it is difficult to obtain an analytic, closed form
solution for BijT and FijT . Inspired from notation and derivation for GNMF, GrNMF and HNMF, it
is easy to see from (1), (2) that the update rules can be written as:"
OTHER,0.6139705882352942,[B]ij ←[AT FT ]ij
OTHER,0.6176470588235294,[BFFT ]ij · [B]ij (1)
OTHER,0.6213235294117647,"[F]ij ←
[BT AT ]ij
[BT BF+λ PT −1
t=1 θT −tFLt]ij
· [F]ij (2)"
OTHER,0.625,(Q.E.D.)
OTHER,0.6286764705882353,"Derivation of update rules for SDyHGrNMF (1-step).
As part of this derivation, let’s prove the
following Lemma:"
OTHER,0.6323529411764706,"Lemma 2.
The (1-step ahead) objective function OSDyHGrNMF in the relevant optimisation
problem can be expressed as:"
OTHER,0.6360294117647058,"OSDyHGrNMF (F, θ) = Tr(FT FFT F) −2Tr(AT FT F) + λ PT −1
t=1 θT −tTr(FLtFT ) s.t. F ≥0."
OTHER,0.6397058823529411,"Proof of Lemma 2.
Using the definition for the objective function and constrained optimisation
problem formulation for SDyHGrNMF (1-step ahead), the optimisation problem for SDyHGrNMF
(1-step ahead) can be written as:"
OTHER,0.6433823529411765,"min
{F,θ,λ}{OSDyHGrNMF (F, θ)}
s.t.
F ≥0; θ ≥0, λ > 0 ="
OTHER,0.6470588235294118,"min
{F,θ,λ}{∥AT ∥2
F + ∥FT F∥2
F −2Tr(AT (FT F)T ) + λ"
OTHER,0.6507352941176471,"T −1
X"
OTHER,0.6544117647058824,"t=1
θT −tTr(FLtFT )}"
OTHER,0.6580882352941176,"s.t.
F ≥0; θ ≥0, λ > 0 ="
OTHER,0.6617647058823529,"min
{F,θ,λ}{Tr(AT AT
T ) + Tr((FT F)(FT F)T ) −2Tr(AT (FT F)T ) + λ"
OTHER,0.6654411764705882,"T −1
X"
OTHER,0.6691176470588235,"t=1
θT −tTr(FLtFT )}"
OTHER,0.6727941176470589,"s.t.
F ≥0; θ ≥0, λ > 0"
OTHER,0.6764705882352942,"(since ∥A∥2
F = Tr(AAT )) ="
OTHER,0.6801470588235294,"min
{F,θ,λ}{Tr(AT AT
T ) + Tr(FT FFT F) −2Tr(AT FT F) + λ"
OTHER,0.6838235294117647,"T −1
X"
OTHER,0.6875,"t=1
θT −tTr(FLtFT )}"
OTHER,0.6911764705882353,"s.t.
F ≥0; θ ≥0, λ > 0"
OTHER,0.6948529411764706,(since (AB)T = BT AT ) =
OTHER,0.6985294117647058,"min
{F,θ,λ}{Tr(AT AT
T ) + Tr(FT FFT F) −2Tr(AT FT F) + λ"
OTHER,0.7022058823529411,"T −1
X"
OTHER,0.7058823529411765,"t=1
θT −tTr(FLtFT )}"
OTHER,0.7095588235294118,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.7132352941176471,"s.t.
F ≥0; θ ≥0, λ > 0"
OTHER,0.7169117647058824,"(since Tr(AT AT
T ) is not a function of F, θ or λ."
OTHER,0.7205882352941176,(Q.E.D.)
OTHER,0.7242647058823529,"Back to derivation of update rules for SDyHGrNMF (1-step).
Using Lemma 2, we can write up
the Lagrangian for this optimisation problem as:"
OTHER,0.7279411764705882,"L(F, ΦT ) = Tr(AT AT
T ) + Tr(FT FFT F) −2Tr(AT FT F) + λ PT −1
t=1 θT −tTr(FLtFT ) +
Tr(ΦT F)"
OTHER,0.7316176470588235,"It is easy to see that this problem now only has 1 first order condition. Let’s now find the partial
derivative of L with respect to F:"
OTHER,0.7352941176470589,"∂L
∂F =
∂
∂F{Tr(FT FFT F)} −2 ∂"
OTHER,0.7389705882352942,"∂F{AT FT F} + λ PT −1
t=1 θT −t ∂"
OTHER,0.7426470588235294,"∂F{FLtFT } +
∂
∂F{Tr(ΦT F)}"
OTHER,0.7463235294117647,Let’s individually evaluate each of the partial derivatives in the expression above. Note that:
OTHER,0.75,"∂
∂F{Tr(FT FFT F)} = 4FFT F. We get this by the following result for the derivative of trace of a
higher order matrix [27]:"
OTHER,0.7536764705882353,"∂
∂X{Tr(BT XT CXXT CXB)} = CXXT CXBBT +CT XBBT XT CT X+CXBBT XT CX+
CT XXT CT XBBT
for arbitrary matrices B, C, X with suitable dimensions such that
BT XT CXXT CXB is a square matrix."
OTHER,0.7573529411764706,"Applying this identity to the partial derivative at hand, set B = IN and C = IK, which gives:"
OTHER,0.7610294117647058,"∂
∂X{Tr(XT XXT X)} = XXT X + XXT X + XXT X + XXT X = 4XXT X"
OTHER,0.7647058823529411,Going to the next derivatives in ∂L ∂F:
OTHER,0.7683823529411765,"∂
∂F{AT FT F} = FAT
T + FAT = F(AT
T + AT ) (since
∂
∂X(Tr(BXT X)) = XBT + XB)"
OTHER,0.7720588235294118,"∂
∂F{Tr(FLtFT )} = FLT
t + FLt = F(LT
t + Lt) (since
∂
∂X(Tr(XBXT )) = XBT + XB)"
OTHER,0.7757352941176471,"∂
∂F{Tr(ΦT F)} = ΦT"
OTHER,0.7794117647058824,"Hence, ∂L"
OTHER,0.7830882352941176,∂F can be written as:
OTHER,0.7867647058823529,"∂L
∂F = 4FFT F −2F(AT
T + AT ) + λ PT −1
t=1 θT −tF(LT
t + Lt) + ΦT"
OTHER,0.7904411764705882,"= 4FFT F −4FAT + λ PT −1
t=1 θT −tF((Dν,t −At)T + (Dν,t −At)) + ΦT (by definition of the
un-normalised dynamic hyper-laplacian and symmetry of the adjacency matrix)"
OTHER,0.7941176470588235,"= 4FFT F −4FAT + 2λ PT −1
t=1 θT −tF(Dν,t −At) + ΦT"
OTHER,0.7977941176470589,"= 4FFT F −4FAT + 2λ PT −1
t=1 θT −tFLt + ΦT (by definition of Lt)"
OTHER,0.8014705882352942,"By the KKT-conditions, we have [ΦT ]ij · [F]ij = [ΦT ]ij · [F]ij = 0∀i, j. Hence, ∂L"
OTHER,0.8051470588235294,"∂F can be written
in scalar notation as (where i = 1, ..., K, j = 1, ..., N): ( ∂L"
OTHER,0.8088235294117647,"∂F)ij = 4[FFT F]ij −4[FAT ]ij + 2λ PT −1
t=1 θT −t[FLt]ij + [ΦT ]ij
As done for the derivation of update rules for DyHGrNMF (1-step), to find the critical point/s of the
Lagrangian for solving the constrained optimisation problem, we have:"
OTHER,0.8125,"∂L
∂F = 0K,N ⇒( ∂L"
OTHER,0.8161764705882353,"∂F)ij = 0 ∀i = 1, ..., K and ∀j = 1, ..., N, since it is easy to see that the R.H.S.
of ∂L"
OTHER,0.8198529411764706,∂F is a (K × N) matrix expression. ⇒( ∂L
OTHER,0.8235294117647058,"∂F)ij = 4[FFT F]ij −4[FAT ]ij + 2λ PT −1
t=1 θT −t[FLt]ij + [ΦT ]ij = 0."
OTHER,0.8272058823529411,Multiplying both sides by [F]ij:
OTHER,0.8308823529411765,"⇒4[FFT F]ij · [F]ij −4[FAT ]ij · [F]ij + 2λ PT −1
t=1 θT −t[FLt]ij · [F]ij + [ΦT ]ij · [F]ij = 0."
OTHER,0.8345588235294118,Applying the KKT-conditions and dividing both sides by 2:
OTHER,0.8382352941176471,"⇒2[FFT F]ij · [F]ij −2[FAT ]ij · [F]ij + λ PT −1
t=1 θT −t[FLt]ij · [F]ij = 0"
OTHER,0.8419117647058824,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.8455882352941176,"⇒2[FFT F]ij ·[F]ij +λ PT −1
t=1 θT −t[FLt]ij ·[F]ij = 2[FAT ]ij ·[F]ij. Taking [F]ij common from
the right on both sides of the equation:"
OTHER,0.8492647058823529,"⇒2[FFT F + λ PT −1
t=1 θT −tFLt]ij · [F]ij = 2[FAT ]ij · [F]ij."
OTHER,0.8529411764705882,"It can be seen from the first order conditions that it is difficult to obtain an analytic, closed form
solution for [F]ij. Inspired from notation and derivation for GNMF, GrNMF, HNMF and DyHGrNMF
stated previously, and using the equation for the first order condition above (dividing both sides of it
by 2[FFT F + λ PT −1
t=1 θT −tFLt]ij), the update rule for [F]ij can be written as:"
OTHER,0.8566176470588235,"[F]ij ←
2[FAT ]ij
[2FFT F+λ PT −1
t=1 θT −tFLt]ij · [F]ij for i = 1, ...K, j = 1, ..., N."
OTHER,0.8602941176470589,(Q.E.D.)
OTHER,0.8639705882352942,"B
Description of the datasets"
OTHER,0.8676470588235294,"Co-authorship.
The dataset, used in [21], was downloaded from [28]. As stated in [28], this is
a temporal higher-order network dataset, which here means a sequence of timestamped simplices
where each simplex is a set of nodes. In this dataset, nodes are authors and a simplex is a publication
recorded on DBLP. Timestamps are the year of publication. This dataset has been restricted to
simplices that consist of at most 25 nodes. The number of nodes in this dataset are 1,924,991, the
number of timestamped simplices are 3,700,067 and the number of unique simplices are 2,599,087.
As part of the dynamic hyper-graph construction for this dataset, we only take data for 20 years."
OTHER,0.8713235294117647,"Email.
The dataset, used in [22], was downloaded from [29]. As stated in [29], this is a temporal
higher-order network dataset, which here means a sequence of timestamped simplices where each
simplex is a set of nodes. In email communication, messages can be sent to multiple recipients. In
this dataset, nodes are email addresses at Enron and a simplex is comprised of the sender and all
recipients of the email. Timestamps are in millisecond resolution and only email addresses from a
core set of employees are included in this dataset. The dataset has been restricted to simplices that
consist of at most 25 nodes, and this is the version of the dataset used. The number of nodes are 143,
the number of timestamped simplices are 10,883 and the number of unique simplices are 1,542. As
part of the dynamic hyper-graph construction for this dataset, we only take data for 20 years."
OTHER,0.875,"Stocks.
The data was downloaded from [23]. The top 9 blue chip stocks from 16 December
2020 to 16 December 2022 were Apple (stock ticker: AAPL), Berkshire Hathaway (stock ticker:
BRK-B), Coca-Cola Co (stock ticker: KO), Johnson and Johnson (stock ticker: JNJ), American
Express Company (stock ticker: AXP), AbbVie Inc. (stock ticker: ABBV), Nike Inc (stock ticker:
NKE), Lockheed Martin Corp (stock ticker: LMT), Honeywell International Inc (stock ticker: HON).
There are 50 time periods, and the number of nodes is the number of blue-chip stocks taken into
account."
OTHER,0.8786764705882353,"Methodology of dynamic hyper-graph construction for Co-authorship and Email datasets.
For each year of data, construct a dictionary of list of lists where each element in the nested list
consists of a timestamped (not necessarily unique) simplex. Each key in this dictionary corre-
sponds to a unique year and each value corresponds to a list of lists. We define this dictionary
as _constructed_timeslices. Using this dictionary, we define a new variable: num_time_slices =
⌈(prop_timeslice) ∗(num_years_data)⌉where prop_timeslice is set to
1
100 for both datasets and
num_years_data is the number of years of data taken into account (i.e 20 years as stated previ-
ously). Next, we define a new variable: avg_time_slice = num_years_data"
OTHER,0.8823529411764706,"num_time_slices. Next, we define a new
list: time_slice_boundaries where the first element is the first year of data to take into account i.e.
start_time. We update elements in this list in the following way:"
OTHER,0.8860294117647058,"for i in range(num_time_slices + 1), do"
OTHER,0.8897058823529411,time_slice_boundaries.append(round(start_time + i*avg_time_slice))
OTHER,0.8933823529411765,end for
OTHER,0.8970588235294118,"Finally we make the dynamic hyper-graph, defined as dyhy_lol in the following way:"
OTHER,0.9007352941176471,dyhy_lol = []
OTHER,0.9044117647058824,Dynamic Hypergraph Regularized Non-negative Matrix Factorization
OTHER,0.9080882352941176,num_timeslices = len(time_slice_boundaries) - 1
OTHER,0.9117647058823529,"for i in range(num_timeslices), do"
OTHER,0.9154411764705882,dyhy_lol_subset = []
OTHER,0.9191176470588235,"for key in _constructed_timeslices.keys(), do"
OTHER,0.9227941176470589,"if key in range(time_slice_boundaries[i], time_slice_boundaries[i+1]), do"
OTHER,0.9264705882352942,dyhy_lol_subset.append(_constructed_timeslices[key])
OTHER,0.9301470588235294,end if
OTHER,0.9338235294117647,end for
OTHER,0.9375,dyhy_lol.append(dyhy_lol_subset)
OTHER,0.9411764705882353,end for
OTHER,0.9448529411764706,"Methodology of dynamic hyper-graph construction for Stocks dataset.
Let’s write down some
pseudo-code used to generate the dyanmic hyper-graph for this dataset. We have defined T, which is
equivalent to the T in our mathematical notation, to be 50 for this dataset. Define seq_inc_matrices
as the time-dependent sequence of incidence matrices to construct. Define stock to be the data-frame
of data loaded for some stock in stock_list = [AAPL, BRK-B, KO, JNJ, AXP, ABBV, NKE, LMT,
HON]. Here, we assume some Pythonic and Pandas (package in Python) indexing:"
OTHER,0.9485294117647058,seq_inc_matrices = []
OTHER,0.9522058823529411,dyhy_lol = []
OTHER,0.9558823529411765,"for t in [1, 2, ..50], do"
OTHER,0.9595588235294118,Step 1: Define the adjusted closing prices across different time periods for each stock
OTHER,0.9632352941176471,stock_adj_close = stock.AdjClose[0:t + 454] for stock in stock_list
OTHER,0.9669117647058824,Step 2: Concatenate the adjusted closing prices across different time periods to obtain time-variant
OTHER,0.9705882352941176,(450 × 9) matrices for stock data (the columns dictate the stocks)
OTHER,0.9742647058823529,Step 3: Compute the returns and correlation matrices for each stock matrix
OTHER,0.9779411764705882,"Step 4: For each correlation matrix corresponding to the constructed stock matrix at time t,"
OTHER,0.9816176470588235,construct the incidence matrix inc_matrix_timepoint (corresponding to Ht) at time t where
OTHER,0.9852941176470589,"[Ht]ij = 1 iff [Σt]ij ≥c for some c ∈(0, 1)."
OTHER,0.9889705882352942,seq_inc_matrices.append(inc_matrix_timepoint)
OTHER,0.9926470588235294,end for
OTHER,0.9963235294117647,"Hyper-parameter optimisation.
The feasible values across which we have done grid search to find
values giving the smallest score, are: dimensions = [4, |V|//2, |V| −3]; λ, θ = log_space(0, 0.2)
where log_space(0, 0.2) is an even distribution of values on a log scale from 0 to 0.2."
