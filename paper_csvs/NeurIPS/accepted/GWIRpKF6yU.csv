Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0029069767441860465,"We study the optimal control of multiple-input and multiple-output dynamical
systems via the design of neural network-based controllers with stability and output
tracking guarantees. While neural network-based nonlinear controllers have shown
superior performance in various applications, their lack of provable guarantees has
restricted their adoption in high-stake real-world applications. This paper bridges
the gap between neural network-based controllers and the need for stabilization
guarantees. Using equilibrium-independent passivity, a property present in a wide
range of physical systems, we propose neural Proportional-Integral (PI) controllers
that have provable guarantees of stability and zero steady-state output tracking error.
The key structure is the strict monotonicity on proportional and integral terms,
which is parameterized as gradients of strictly convex neural networks (SCNN). We
construct SCNN with tunable softplus-β activations, which yields universal approx-
imation capability and is also useful in incorporating communication constraints.
In addition, the SCNNs serve as Lyapunov functions, providing end-to-end stability
and tracking guarantees. Experiments on traffic and power networks demonstrate
that the proposed approach improves both transient and steady-state performances,
while unstructured neural networks lead to unstable behaviors."
INTRODUCTION,0.005813953488372093,"1
Introduction"
INTRODUCTION,0.00872093023255814,"Learning-based methods have the potential to solve difficult problems in control and have received
significant attention from both the machine learning and control communities. A common problem
in many applications is to design controllers that–with as little control effort as possible–stabilize a
system at a prescribed output. A canonical example is the LQR problem and its variants, which finds
optimal linear controllers for linear systems [1, 2]."
INTRODUCTION,0.011627906976744186,"For nonlinear systems, the problem is considerably harder. Neither of the two control goals, system
stabilization and output tracking at the steady state, is easy to achieve. Their combination, optimizing
controller costs while guaranteeing stability and output tracking at the steady state, is even more
challenging. For example, vehicles in a platoon should stay in formation (stability) while cruising at
the desired speed (output tracking) [3]. The optimization is then done over the set of all controllers
that achieves both of these goals, e.g., reaching a platoon while consuming the least amount of fuel.
Another pertinent application is the stability of electric grids with high amounts of renewables. Unlike
systems with conventional generators, the power electronic interfaces of the inverters need to be
actively controlled such that the grid synchronizes to the same frequency (output tracking), at the
minimum operational costs. Currently, learning becomes a popular tool to parameterize nonlinear
controllers as neural networks and train them to optimize performance, but provable guarantees on
stability and steady-state error for these controllers are nontrivial to obtain [4–7]."
INTRODUCTION,0.014534883720930232,"Many real-world applications have multiple (possibly a continuous set of) equilibria that may be
of interest. For example, a group of vehicles may need to cruise at different speeds. Moreover, the"
INTRODUCTION,0.01744186046511628,"internal states may not be directly accessible and sometimes there are communication limit. These
constraints make it difficult to enforce stability by middle steps including learning a Lyapunov function
and learning a model of the system (for details, see related work in Section 1.1). Therefore, we seek to
achieve “end-to-end” guarantees: the neural network-based controller should guarantee stability and
steady-state error by constructions, for a range of possible tracking points. There are some works that
showed a class of monotone controllers can provide stability guarantees for varying equilibria [8–10],
but they rely on tailor-made Lyapunov functions that are found in specific applications. Especially,
these results are limited to single-input and single-output (SISO) control. In practice, however, lots of
complex systems need controllers that are multiple-input and multiple-output (MIMO), sometimes
with high input and output dimensions."
INTRODUCTION,0.020348837209302327,"Moreover, current learning-based approaches typically focus on optimizing transient performance
(the cost and speed at which a system reaches an equilibrium), often overlooking the steady-state
behavior (cost at the equilibrium state) [11–13]. In classical control terminology, these controller is
proportional. Steady-state requirements are difficult to incorporate since training can only occur over
a finite horizon. To enforce output tracking at the steady-state, an integral term is commonly needed
to drive the system outputs to the desired value at the steady state [14–17]. This reasoning underlies
the widespread use of linear Proportional-Integral (PI) controllers in practical applications [18–20].
However, linear parameterization inherently limits the controllers’ degrees of freedom, potentially
leading to suboptimal performance. This work addresses the following question: Can we learn
nonlinear controllers that guarantee transient stability and zero steady-state error for MIMO systems?"
INTRODUCTION,0.023255813953488372,"Contributions. Clearly, it is not possible to design a controller for all nonlinear systems and the
answer to the question above depends on the class of systems under consideration. This paper focuses
on systems that satisfy the equilibrium independent passivity (EIP) [21, 22], which is present in many
critical societal-scale systems including transportation [23], power systems [24], and communication
network [25]. This abstraction allows us to design generalized controllers without considering the
detailed physical system dynamics. We propose a structured Neural-PI controller that has provable
guarantees of stability and zero steady-state output tracking error. The key structure we use is strictly
monotone functions with vector-valued inputs and outputs. Experiments on traffic and power systems
demonstrate that Neural-PI control can reduce the transient cost by at least 30% compared to the best
linear controllers and maintain stability guarantees. Unstructured neural networks, on the other hand,
lead to unstable behaviors. We summarize our major contributions as follows."
INTRODUCTION,0.02616279069767442,"1) We propose a generalized PI structure for neural network-based controllers in MIMO systems,
with proportional and integral terms being strictly monotone functions constructed through the
gradient of strictly convex neural networks (SCNN). We construct SCNN with a tunable softplus-β
activation function and prove their universal approximation capability in Theorem 1.
2) For a multi-agent system with an underlying communication graph, we show how to restrict the
controllers to respect the communication constraints through the composition of SCNNs.
3) Using EIP and the monotone functions structured by SCNNs, we design a generalized Lyapunov
function that works for a range of equilibria. This provides end-to-end guarantees on asymptotic
stability and zero steady-state output tracking error proved in Theorem 2. The structured neural
networks can be trained for transient optimization without jeopardizing the guarantees, establishing
a framework for coordinating transient optimization and steady-state requirement."
RELATED WORKS,0.029069767441860465,"1.1
Related works"
RELATED WORKS,0.03197674418604651,"Learning-based control with stability guarantees. Recently, there has been a large interest in
enforcing stability in learning-based controller [26–30]. Many works add soft penalties on the
violation of stability conditions in the cost function, but it is nontrivial to certify stability for all the
possible initial states in a compact set [30]. Some recent works learn a Lyapunov function and use it
to certify stability through a satisfiability modulo theories (SMT) solver [26–29]. But this approach
is difficult to scale to high-dimensional systems and the learned Lyapunov function only works for a
single equilibrium. For control-affine systems, the work in [31] designs feedback linearizing policy
with integral control to guarantee stabilizing to a range of equilibria. But it requires that all the state
are accessible and many systems are not control affine. Our proposed controller guarantees stability
for a set of equilibria by construction, and only needs access to the outputs (not the full states)."
RELATED WORKS,0.03488372093023256,"Optimizing long-term behavior. To regulate long-term behavior, existing works train neural
networks using a cost function defined over a long time horizon [32, 33]. However, it is difficult"
RELATED WORKS,0.0377906976744186,"to quantify how long the trajectory is enough to reach the steady state, thus enforcing steady-state
tracking performance by adding a long-term cost is challenging. Our proposed controller enforces
steady-state tracking by construction, instead of relying on training."
RELATED WORKS,0.040697674418604654,"Algorithm to tune MIMO PI controller. Classical Proportional Integral (PI) control structures are
widely used in industrial applications, while tuning PI controller parameters in MIMO systems is
tedious and relies on heuristic rules [34, 35]. Learning-based methods become popular to tune PI
parameters, although still restricted to linear control gains [34, 35]. Our contribution is the more
generalized PI control and the MIMO neural network for parameterization."
RELATED WORKS,0.0436046511627907,"Monotone neural network.
Even though scalar-valued monotone functions have been studied in
the learning community [36, 37], their generalization to the vector-valued case has not. Numerous
papers studied vector-valued functions that are monotone in every input, that is, q(x) ≤q(x′) if
x ≤x′ [38–41]. This is only a small subclass of vector-valued monotone functions that defined as
functions satisfying (q(x) −q(x′))⊤(x −x′) ≥0. In this paper, we construct the general class of
monotone functions (often called monotone operators) [42] that satisfy the inner product inequality."
BACKGROUND AND PRELIMINARIES,0.046511627906976744,"2
Background and Preliminaries"
BACKGROUND AND PRELIMINARIES,0.04941860465116279,We consider a dynamic system described by:
BACKGROUND AND PRELIMINARIES,0.05232558139534884,"˙x = f(x, u),
y = h(x),
(1)"
BACKGROUND AND PRELIMINARIES,0.055232558139534885,"with state x(t) ∈Rn, control action u(t) ∈Rm, and output y(t) ∈Rv. We also sometimes omit the
time index t for simplicity1. In many practical applications, not all of the internal states are directly
accessible. Hence, we consider control actions that follow output-feedback control laws of the form
u = g(y), which is a static function of the output y and not the state x."
BACKGROUND AND PRELIMINARIES,0.05813953488372093,"A state x∗such that f(x∗,u∗) = 0n, y∗= h(x∗), u∗= g(y∗) is called an equilibrium since the
system stops changing at this state. Throughout this paper, the superscript ∗indicates the equilibrium
values of variables.
Definition 1 (Local asymptotic stability [16], Definition 4.1). The system (1) is asymptotically stable
around an equilibrium x∗if, ∀ϵ > 0, ∃δ > 0 such that ∥x(0) −x∗∥< δ ensures ∥x(t) −x∗∥< ϵ,
∀t ≥0, and ∃δ′ > 0 such that ∥x(0) −x∗∥< δ′ ensures limt→∞∥x(t) −x∗∥= 0."
BACKGROUND AND PRELIMINARIES,0.061046511627906974,"One of the main tools to prove asymptotic stability is the Lyapunov’s direct method [27, 16]:
Lemma 1 (Lyapunov functions and asymptotic stability [16], Theorem 4.1). Consider the system (1)
with an equilibrium x∗∈X ⊂Rn. Suppose there exists a continuously differentiable function
V : X 7→R that satisfies the following conditions: 1) V (x∗) = 0, V (x) > 0, ∀x ∈X\x∗; 2)
˙V (x) = (∇xV (x))⊤˙x ≤0, ∀x ∈X with the equality holds when x = x∗. Then the system is
asymptotically stable around x∗."
BACKGROUND AND PRELIMINARIES,0.06395348837209303,"The key challenge to using Lyapunov theory is in constructing such a function and verifying the
satisfaction of the conditions in Lemma 1. An important part of this paper is to show how to
systematically use neural networks to construct Lyapunov functions for a class of nonlinear systems."
BACKGROUND AND PRELIMINARIES,0.06686046511627906,"Besides stability, we are often interested in achieving a specific equilibrium such that the output
converges to a prescribed setpoint.
Definition 2 (Output tracking to ¯y). The dynamical system (1) is said to track a setpoint ¯y if
limt→∞y(t) = ¯y."
BACKGROUND AND PRELIMINARIES,0.06976744186046512,"For output tracking, the dimension of the input u should be at least the dimension of the output
y, otherwise there is not enough degrees of freedom in the input to track all the outputs. If the
dimensions of the input are strictly larger than the output, it is always possible to define “dummy”
outputs (e.g., by appending zeros) to match the input dimension. Therefore, it is common to assume
the same input and output dimensions [22]. In the remainder of this paper, we make the following
assumption.
Assumption 1 (Identical input and output dimension). The input u and the output y have the same
dimension, namely, u(t) and y(t) are both vectors in Rm."
BACKGROUND AND PRELIMINARIES,0.07267441860465117,"1Throughout this paper, vectors are denoted in lower case bold and matrices are denoted in upper case bold,
unless otherwise specified. Vectors of all ones and zeros are denoted as 1n, 0n ∈Rn, respectively."
BACKGROUND AND PRELIMINARIES,0.0755813953488372,"Figure 1: (a) The controller aims to improve transient performances after disturbances while guaranteeing
stabilization to the desired steady-state output ¯y. (b) Two examples of physical systems with output tracking
tasks. (c) We provide end-to-end stability and output tracking guarantees by enforcing stabilizing PI structure in
the design of neural networks. The key structure is strictly monotone functions, which are parameterized by the
gradient of SCNNs. (d) The transient performance is optimized by training the neural networks."
BACKGROUND AND PRELIMINARIES,0.07848837209302326,"We will show that strictly monotone functions play an important role in guaranteeing stability and
output tracking in a range of systems. Using neural networks for constructing monotone functions
from R to R has been well-studied in [36, 37]. However, since we consider MIMO systems in this
paper, we use a generalized notion of monotonicity for vector-valued functions as defined in [42].
Definition 3 (Strictly monotone function [42]). A continuous function q : Rm 7→Rm is strictly
monotone on D ⊂Rm if (q(η) −q(ξ))⊤(η −ξ) ≥0, ∀η, ξ ∈D, with the equality holds if and only
if η = ξ."
BACKGROUND AND PRELIMINARIES,0.08139534883720931,"In this paper, we will show a way to construct (strictly) monotone functions using the gradient of
(strictly) convex neural networks."
PROBLEM STATEMENT,0.08430232558139535,"3
Problem Statement"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.0872093023255814,"3.1
Transient and steady-state requirements"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.09011627906976744,"We aim to optimize the controller in u to improve the transient response after disturbances while
guaranteeing steady-state performance, as shown in Figure 1(a). By steady-state performance, we
mean the system should settle at the desired setpoint ¯y. By transient performance, we want the system
to quickly reaching the steady state with small control efforts."
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.09302325581395349,"The inverted pendulum in Figure 1(b) serves as a good motivating example. The input u is the force
on the pendulum. The objectives include 1) the angle y should reach a setpoint ¯y (e.g., 5◦) and stays
there; 2) minimizing the control cost and deviation of y from ¯y before reaching the steady state.
These objectives are common in many real-world applications, such as vehicle platooning (Fig. 1(b))
and power system control in our experiments.
Transient performance optimization. During the transient period, our goal is to quickly stabilize
the system to the desired setpoint ¯y, while trading off with using large control efforts in u. Let J(·)
be the cost function of the system. 2 The transient optimization problem up to time T is, min
u T
X"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.09593023255813954,"t=0
J(y(t) −¯y, u(t)),
(2a)"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.09883720930232558,"s.t. dynamics in (1) : ˙x = f(x, u),
y = h(x) ,
(2b)
stability and output tracking in Definition 1-2 : lim
t→∞y(t) = ¯y
(2c)"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.10174418604651163,"Note that in (2a) we sum up to time T, which should be long enough to cover the transient period.
Since we require tracking in (2c), the exact value of T is not critical in the optimization problem. In"
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.10465116279069768,2Including other forms of differentiable cost functions do not change the analysis.
TRANSIENT AND STEADY-STATE REQUIREMENTS,0.10755813953488372,"practice, the system dynamics (1) (and sometimes even the cost function) can be highly nonlinear
and nonconvex. Therefore, the current state-of-the-art is to learn u by parameterizing it as a neural
network and training it by minimizing the cost in (2a) [43, 44]. But the key challenge with applying
these neural network-based controllers is guaranteeing stability and output tracking [45, 26, 30].
Even if the learned policy may appear “stable” during training, it is not necessarily stable during
testing. This can be observed in both the vehicle and power system experiments in Section 6.
Stability for a range of tracking points. We emphasize that the setpoint ¯y may vary in practice
(e.g., the setpoint velocity of vehicles may change), and thus the controller is required to guarantee
stability and output tracking to a range of equilibria corresponding to the setpoints. This is difficult to
achieve through existing works that enforce stability by learning a Lyapunov function [26, 27, 30],
since the learned Lyapunov function is for a single equilibrium (normally setup as x∗= 0n). These
methods also require that all the states x are observed, which may not be achieved in practice.
Communication requirement. A typical constraint in a large system is limits on communications.
For example, vehicles in a group may only be able to measure the output of their neighbors (line-of-
sight) or nodes in a power system may only have real-time communication from a subset of other
nodes. Therefore, the control action ui may be constrained to be a function of a subset of the outputs
y. We show later in Subsection 4.3 about how these communication constraints can be naturally
accommodated in our controller design."
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.11046511627906977,"3.2
Bridging controller design and stability through passivity analysis"
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.11337209302325581,"As shown in Figure 1(a), optimizing transient and steady-state performances are two problems in two
different time-scales. Therefore, coordinating them is a central challenge."
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.11627906976744186,"End-to-end performance guarantees. We propose to overcome this challenge through a structured
controller design that provides end-to-end stability and tracking guarantees, as shown in Figure 1(c-d).
""End-to-end"" means that the guarantees are provided by construction, and do not depend on how the
controller is trained. Thus, the neural networks can be trained to optimize the transient performance
without jeopardizing the stability and steady-state guarantees. In particular, the construction works
for a range of equilibria and can conveniently incorporate communication characteristics."
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.11918604651162791,"Instead of working on specific systems individually, we seek to find a family of physical systems
that allows us to derive a generalized controller design. It turns out that the notion of equilibrium
independent passivity (EIP) [21, 22] provides a concise and useful abstraction of physical systems
for stability analysis. This paper thus focuses on systems satisfying the following assumptions:"
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.12209302325581395,"Assumption 2 (Equilibrium-Independent Passivity [21] ). The system (1) is strictly equilibrium-
independent passive (EIP), which satisfies: (i) there exists a nonempty set U∗⊂Rm such that for
every equilibrium u∗∈U∗, there is a unique x∗∈Rn such that f(x∗, u∗) = 0n, and (ii) there
exists a positive definite storage function S (x, x∗) and a positive scalar ρ such that"
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.125,"S (x∗, x∗) = 0
and
˙S (x, x∗) ≤−ρ ∥y −y∗∥2 + (y −y∗)⊤(u −u∗) .
(3)"
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.12790697674418605,"The EIP property in Assumption 2 has been found in a large class of physical systems, including
transportation [23], power systems [24, 46], robotics [47], communication [25], and others. We will
conduct experiments on vehicle platoons and power systems to show how EIP can be validated."
BRIDGING CONTROLLER DESIGN AND STABILITY THROUGH PASSIVITY ANALYSIS,0.1308139534883721,"The condition (3) of EIP systems provides us a generalized approach to construct Lyapunov functions
without knowing the specifies of the dynamics f(·). In turn, we are able to find the right structure
for the controllers. In Section 4, we show what these structures are for PI controllers and how to
enforce such structures in the design of neural networks. Then Section 5 formally demonstrates the
theoretical guarantees and the training procedure for transient optimization."
STRUCTURED NEURAL-PI CONTROL,0.13372093023255813,"4
Structured Neural-PI Control"
STRUCTURED NEURAL-PI CONTROL,0.13662790697674418,"In this section, we construct controllers with a proportional and an integral term, which are both
vector-valued strictly monotone functions parameterized by the gradient of strictly convex functions.
Then, we present a neural network architecture that is strictly convex by construction and can
conveniently incorporate communication constraints."
GENERALIZED PI CONTROL,0.13953488372093023,"4.1
Generalized PI control"
GENERALIZED PI CONTROL,0.14244186046511628,"To realize output tracking, we introduce an integral variable ˙s = ¯y −y. Intuitively, s is the
accumulated tracking error and will remain unchanged at the steady-state when y = ¯y. On this basis,
we consider a generalized PI controller u = p(¯y −y)+r(s). The first component is the proportional
term, where p(¯y −y) is a function of the tracking error ¯y −y between the current output y and
desired output ¯y. The second component is the integral term r(s) as a function of the integral of
historical tracking errors. Intuitively, the proportional term drives y close to ¯y, and the integral term
ensures the tracking error equals zero at the steady state."
GENERALIZED PI CONTROL,0.14534883720930233,"Generalized PI Controller. Compactly, the control law is"
GENERALIZED PI CONTROL,0.14825581395348839,"u =
p(¯y −y)
|
{z
}
Proportional control"
GENERALIZED PI CONTROL,0.1511627906976744,"+
r(s)
|{z}
Integral control (4a)"
GENERALIZED PI CONTROL,0.15406976744186046,"˙s = ¯y −y.
(4b)"
GENERALIZED PI CONTROL,0.1569767441860465,"Remark 1. The above controller can be envisioned as a nonlinear generalization of widely adopted
linear Proportional-Integral (PI) controllers, where p(¯y −y) := KP (¯y −y), r(s) := KI(s) with
KP and KI being the proportional and integral coefficients (scalar for SISO control). Linear PI
control laws are almost always used in existing works [14–16], but the performance of linear PI
controllers can be poor for large-scale nonlinear systems[6, 48]."
GENERALIZED PI CONTROL,0.15988372093023256,"To achieve provable stability guarantees with output tracking, we need to construct a Lyapunov
function that works for the closed-loop system formed by (1) and (4). Therefore, we further design
structures in the proportional function p(·) and the integral function r(·) that can be utilized in
Lyapunov stability analysis. The structures are strictly monotone functions, which generalizes
conventional linear functions. In the next subsection, we will show how to parameterize strictly
monotone functions. In section 5.1, we will show how the PI controllers structured with monotone
functions provide end-to-end stability and output tracking guarantees."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.16279069767441862,"4.2
Strictly monotone function through gradients of strictly convex neural networks"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.16569767441860464,"It is not trivial to parameterize strictly monotone functions since this is an infinite-dimensional
function space. Scalar-valued monotone functions mapping from R to R [36, 37] has been proposed,
but it is difficult to extend these designs to MIMO systems."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.1686046511627907,"In this paper, we propose a new parameterization of strictly monotone functions by leveraging the
fact in Proposition 1 that the gradient of a strictly convex function is strictly monotone."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.17151162790697674,"Proposition 1 (Gradients of strictly convex functions). Let g : Rm 7→R be a strictly convex function,
then (∇ηg(η) −∇ξg(ξ))⊤(η −ξ) ≥0 ∀η, ξ ∈Rm, with equality holds if and only if η = ξ.
Namely, ∇g : Rm 7→Rm is a strictly monotone increasing function."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.1744186046511628,"Figure 2: Strictly monotone function constructed by
strictly convex neural networks (SCNN)."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.17732558139534885,"Hence, the strictly monotone property of p(·)
and r(·) can be guaranteed by design if they
are the gradient of a strictly convex function,
as shown by Figure 2. The next proposition
shows how to construct a strictly convex neural
network (SCNN)."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.18023255813953487,"Proposition 2 (Strictly convex neural networks).
Consider a function g(z; θ) : Rm 7→R pa-
rameterized by k-layer, fully connected neural
network with the input z ∈Rm. The output ol of layer l = 0, . . . , k −1 and the function g(z; θ) is
represented as"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.18313953488372092,"ol+1 = σl

W (o)
l
ol + W (z)
l
z + bl

,
g(z; θ) = ok
(5)"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.18604651162790697,"where o0, W (o)
0
≡0 , θ =
n
W (z)
0:k−1, W (o)
1:k−1, b0:k−1
o
are trainable weights and biases, and σi are"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.18895348837209303,"non-linear activation functions. The function g(z; θ) is strictly convex in z provided that all W (o)
1:k−1
are positive, W (z)
0
is nonzero, and all functions σl are strictly convex and increasing."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.19186046511627908,"Figure 3: Communication embedded controller. Take m = 4 where z = (z1, z2, z3, z4) and P network
p = ∇zg(P )(z; θ(P ), β(P )) as an example. (a) Global communication, where V = {(1, 2, 3, 4)} and p can
be a function of the full z. (b) Decentralized, V = {(1), · · · , (4)} and pi is a function of zi, ∀i. (c) Partial
communication, V = {(1, 2), (2, 3, 4)}, where there exists communication within (1, 2) and (2, 3, 4)."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.19476744186046513,"The construction of SCNN follows the general structure of input-convex neural networks [49, 50],
where the conditions on activation functions and weights are modified to achieve strictly convexity.
The proof follows from the fact that positive sums of strictly convex functions are also strictly convex
and that the composition of a strictly convex and convex increasing function is also strictly convex."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.19767441860465115,"The next theorem demonstrates the universal approximation property of SCNN in Proposition 2.
Theorem 1 (Universal approximation ). Let Z be a compact domain in Rm and q(z) : Z 7→R
be a Lipschitz continuous and strictly convex function. For any ϵ > 0, there exists a function
g(z; θ) : Z 7→R constructed by (5) where the activation function is the softplus-β function"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.2005813953488372,"σSoftplusβ
l
(x) := 1"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.20348837209302326,"β log(1 + eβx),
(6)"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.2063953488372093,such that |q(z) −g(z; θ)| < ϵ for all z ∈Z.
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.20930232558139536,"The proof is given in Appendix A.1. We sketch the proof as follows. We leverage the results
in [49, 50] that the structure (5) with ReLU activation is a universal approximation for any convex
function. However, ReLU activations are not strictly convex, and thus we design the Softplus-β
activation. By showing that the structure (5) with Softplus-β activation can approximate neural
networks with the ReLU activations arbitrarily closely when β is sufficiently large, we then prove
that the structure (5) with Softplus-β can universally approximate any strictly convex function."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.21220930232558138,"Notably, β in the activation function is a tunable parameter. Hence, we write down the SCNN (5)
with activation function being Softplus-β function in (6) as g(z; θ, β), where β is an extra trainable
parameter."
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.21511627906976744,"Thus, we parameterize the structured Neural-PI control law u = p(¯y −y) + r(s) in (4) as"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.2180232558139535,"p(¯y −y) = ∇zg(P )(z; θ(P ), β(P ))|z=¯y−y,"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.22093023255813954,"r(s) = ∇zg(I)(z; θ(I), β(I))|z=s .
(7)"
STRICTLY MONOTONE FUNCTION THROUGH GRADIENTS OF STRICTLY CONVEX NEURAL NETWORKS,0.2238372093023256,"This way, p(·) and r(·) by construction are strictly monotone. In particular, the convex functions
g(P )(z; θ(P ), β(P )), g(I)(z; θ(I), β(I)) play a vital role in constructing a generalized Lyapunov
function and showing the satisfaction of the Lyapunov condition in Subsection 5.1. This subsequently
provides end-to-end stability guarantees that do not dependent on the training algorithm."
EMBEDDING COMMUNICATION CONSTRAINTS,0.22674418604651161,"4.3
Embedding Communication Constraints"
EMBEDDING COMMUNICATION CONSTRAINTS,0.22965116279069767,"For some large-scale physical systems, not all of the control inputs have access to all of the output
measurements at real-time. Therefore, some ui’s cannot be a function of the full vector y and s
because of this lack of global communication, as elaborated in Subsection 3.1."
EMBEDDING COMMUNICATION CONSTRAINTS,0.23255813953488372,"Constructing the controllers as the gradient of convex functions provides us with a convenient
way to embed these communication constraints. Let V be the set of indexes with communications.
Take m = 4 where z = (z1, z2, z3, z4) and the proportional term p = ∇zg(P )(z; θ(P ), β(P )) as
an example. Figure 3(a) shows the case with global communication, where V = {(1, 2, 3, 4)}
and p can be a function of the full z. Figure 3(b) shows the case without communication where"
EMBEDDING COMMUNICATION CONSTRAINTS,0.23546511627906977,"V = {(1), · · · , (4)} and pi can only be a function of zi for all i. Figure 3(c) shows the case V =
{(1, 2), (2, 3, 4)}, where there exists communication within the indexes (1, 2) and within (2, 3, 4). By
defining SCNN g(zvj; θ(P )
j
, β(P )
j
) for each group in vj ∈V, the gradient ∇zg(zvj; θ(P )
j
, β(P )
j
) will
only be a function of zvj and thus satisfying the commutation constraints. Therefore, we construct
the functions in (7) as"
EMBEDDING COMMUNICATION CONSTRAINTS,0.23837209302325582,"∇zg(P )(z; θ(P ), β(P )) = ∇z
X"
EMBEDDING COMMUNICATION CONSTRAINTS,0.24127906976744187,"vj∈V
g(P )(zvj; θ(P )
j
, β(P )
j
),
(8)"
EMBEDDING COMMUNICATION CONSTRAINTS,0.2441860465116279,"where θ(P ) := {θ(P )
1
, · · · , θ(P )
|V| }, β(P ) := {β(P )
1
, · · · , β(P )
|V| }, and θ(P )
j
, θ(I)
j
are parameters of the
SCNNs for group vj within the communication network."
EMBEDDING COMMUNICATION CONSTRAINTS,0.24709302325581395,"The function g(I)(z; θ(I), β(I)) is also constructed in a similar way as g(P )(z; θ(P ), β(P )) in (8) to
incorporate the communication constraints in V. Strict convexity still holds since a sum of strictly
convex functions is also strictly convex."
TRAINING WITH END-TO-END GUARANTEES,0.25,"5
Training with End-to-End Guarantees"
END-TO-END STABILITY AND OUTPUT-TRACKING GUARANTEES,0.25290697674418605,"5.1
End-to-end stability and output-tracking guarantees"
END-TO-END STABILITY AND OUTPUT-TRACKING GUARANTEES,0.2558139534883721,"The convex function g(I)(s; θ(I), β(I)) constructed from SCNNs can be utilized to construct a Lya-
punov function for proving stability using the Bregman distance defined in the following Lemma [51]:
Lemma 2 (Bregman distance). The Bregman distance associated with the convex function
g(I)(s; θ(I), β(I)) is defined by"
END-TO-END STABILITY AND OUTPUT-TRACKING GUARANTEES,0.25872093023255816,"B(s, s∗; θ(I), β(I)) = g(I)(s; θ(I), β(I))−g(I)(s∗; θ(I), β(I))−∇sg(I)(s∗; θ(I), β(I))⊤(s −s∗) ,
which is positive definite with equality holds if and only if s = s∗."
END-TO-END STABILITY AND OUTPUT-TRACKING GUARANTEES,0.2616279069767442,"The Bregman distance allows us to construct Lyapunov functions without specifying the equilibrium
s∗. Combining the storage function in Assumption 2 and B(s, s∗; θ(I), β(I)) in Lemma 2, next
theorem shows that the Neural-PI controller stabilizes the system to the desired output ¯y.
Theorem 2.
Suppose Assumption 2 holds and the input u follows the structured PI control law
u = p(¯y −y) + r(s) where p(·) and r(·) are constructed as the gradients of strictly convex neural
networks in (7). If the system (1) has a feasible equilibrium, then the system is locally asymptotically
stable around it. In particular, the steady-state outputs track the setpoint ¯y, namely y∗= ¯y."
END-TO-END STABILITY AND OUTPUT-TRACKING GUARANTEES,0.26453488372093026,"Theorem 2 shows that Neural-PI control has provable guarantees on stability and zero steady-state
output tracking error from the structures in (7). Detailed proof could be found in Appendix A.2 and we
sketch the proof below. At an equilibrium, the right side of (4b) equals to zero gives y∗= ¯y. We show
that an equilibrium is asymptotically stable by constructing a Lyapunov function V (x, s)|x∗,s∗=
S(x, x∗) + B(s, s∗; θ(I), β(I)). Since r(s) = ∇sg(I)(s; θ(I), β(I)) by construction in (7), the time
derivative of the Bregman distance term is ˙B(s, s∗; θ(I), β(I)) = (r(s) −r (s∗))⊤(−(y −y∗)).
Combining with the fact ˙S (x, x∗) ≤−ρ ∥y −y∗∥2 + (y −y∗)⊤(u −u∗) (from the EIP assump-
tion), and u = p(¯y −y) + r(s), we can conclude that the Lyapunov stability condition holds.
Remark 2. The satisfaction of the Lyapunov condition does not depend on the specifics of f(·) (as
long as it is EIP), making the stability certification robust to parameter changes for systems satisfying
Assumption 2. We will demonstrate this in the experiment on power system control."
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.26744186046511625,"5.2
Training to improve transient performances"
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.2703488372093023,"The Neural-PI controller can be trained by most model-based or model-free algorithms, since
the stability and output-tracking are guaranteed by construction. Fig 4 visualizes the detailed
construction and the computation graph in the dynamical system in (1). The trainable parameters
Θ := {θ(P ), β(P ), θ(I), β(I)} are contained in p(·) and r(·) functions, where both are parameterized
as the gradients of SCNNs. u = ∇¯y−yg(P )(¯y −y; θ(P ), β(P )) + ∇sg(I)(s; θ(I), β(I)) serves as
control signal in the system defined in (1) that evolves through time. The loss function is defined as"
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.27325581395348836,"Loss(Θ) = T
X"
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.2761627906976744,"t=0
J(y(t) −¯y, u(t)),
(9)"
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.27906976744186046,"where J(·) is the transient cost function defined in (2a). The parameters Θ are optimized via gradient
descent to minimize this loss function (9)."
TRAINING TO IMPROVE TRANSIENT PERFORMANCES,0.2819767441860465,Figure 4: The computation graph for training the Neural-PI controllers.
EXPERIMENTS,0.28488372093023256,"6
Experiments"
EXPERIMENTS,0.2877906976744186,"We end the paper with case studies demonstrating the effectiveness of the proposed Neural-PI control
in two large-scale systems: vehicle platooning and power system frequency control. Detailed problem
formulation, verification of assumptions, simulation setting, and results are provided in Appendix B.1
and B.2 in the supplementary material. All experiments are run with an NVIDIA Tesla T4 GPU with
16GB memory. Code is available at this link."
VEHICLE PLATOONING,0.29069767441860467,"6.1
Vehicle platooning
Experiment setup. We conduct experiments on the vehicle platooning problem in Figure 1(b) with
20 vehicles (m = 20), where u ∈Rm is the control signal to adjust the velocities of vehicles and
the output y ∈Rm is their actual velocities. The state is x = (ζ, y), where ζ ∈Rm is the relative
position of vehicles. The dynamic model is ˙ζ = Γy, ˙y = ˆκ(−(y −λ0) + ˆρ(u −E ˆDE⊤ζ)),
where ˆκ, ˆρ, ˆD, E, Γ are constant matrices with their physical meaning given in Appendix B.1.1.
The vector λ0 ∈Rm reflects the default velocity of vehicles. In Appendix B.1.2, We verify
that this system is EIP (i.e., satisfying Assumption 2) using the storage function S (x, x∗) =
1
2(y −y∗)⊤ˆκ−1 ˆρ−1(y −y∗) + 1"
VEHICLE PLATOONING,0.2936046511627907,"2ζ⊤E ˆDE⊤ζ. The objective is for the vehicles to reach the same
setpoint velocity quickly with acceptable control effort. We train for 400 epochs, where each epoch
trains with the loss (9) averaged on 300 trajectories, and each trajectory evolves 6s from random
initial velocities."
VEHICLE PLATOONING,0.29651162790697677,"Controller performance. We compare the performance of 1) Neural-PI: the learned structured
Neural-PI controllers parametrized by (7) with three layers and 20 neurons in each hidden layer. 2)
DenseNN: Dense neural networks (5) the same as Neural-PI with unconstrained weights. Both of
them have no communication constriants. 3) Linear-PI: linear PI control where p(¯y −y) := KP (¯y −
y), r(s) := KI(s) with KP and KI being the trainable proportional and integral coefficients.
Figure 5(a) shows the transient and steady-state costs on 100 testing trajectories starting from
randomly generated initial states. Neural-PI attains much lower costs even though the weights in
DenseNN are not constrained. Compared with Linear-PI, Neural-PI achieves similar steady-state cost
(since it retains all the stability guarantees of classical linear PI control), but reduces the transient
cost by approximately 30%. Given ¯y = 5m/s, Figure 5(b) and 5(c) show the dynamics of selected
nodes under DenseNN and Neural-PI, respectively. All the outputs track under Neural-PI but they are
unstable under DenseNN (even though the training cost was not prohibitively high). In particular,
DenseNN appears to be stable until about 10s, but states blows up quickly after that. Therefore,
enforcing stabilizing structures is essential."
POWER SYSTEMS FREQUENCY CONTROL,0.29941860465116277,"6.2
Power systems frequency control"
POWER SYSTEMS FREQUENCY CONTROL,0.3023255813953488,"Experiment Setup. The second experiment is the power system frequency control on the IEEE
39-bus New England system [52], where u ∈Rm is the control signal to adjust the power injection
from generators and the output y ∈Rm is the rotating speed (i.e., frequency) of generators. The
objective is to stabilize generators at the required frequency ¯y = 60Hz at the steady state while
minimizing the transient control cost. The state is x = (δ, y), where δ ∈Rm is the rotating angle of"
POWER SYSTEMS FREQUENCY CONTROL,0.30523255813953487,"(a) Transient and steady-state cost
(b) Velocities under DenseNN
(c) Velocities under Neural-PI
Figure 5: (a) The average transient cost and steady-state cost with error bar on 100 testing trajectories starting
from randomly generated initial states. (b) The dynamics of DenseNN. (c) The dynamics of Neural-PI."
POWER SYSTEMS FREQUENCY CONTROL,0.3081395348837209,"generators. The dynamics of the system is ˙δ = Γy, ˆ
M ˙y = −ˆD(y −¯y) −d + u −Eˆb sin(E⊤δ),
where ˆ
M, ˆD ˆb, E, Γ are constant matrices with their physical meaning given in Appendix B.2.1.
The vector d is the net load of the system. In Appendix B.2.2, We verify that this system is EIP
(i.e., satisfying Assumption 2) using the storage function S (x, x∗) = 1"
POWER SYSTEMS FREQUENCY CONTROL,0.311046511627907,"2(y −y∗)⊤ˆ
M(y −y∗) −
1⊤
e ˆb(cos(E⊤δ) −cos(E⊤δ∗)) −(Eˆb sin(E⊤δ∗))⊤(δ −δ∗)). Below we show the impact of
communication constraints on the performance of Neural-PI control. More simulation results can
be found in Appendix B.2.3 to demonstrate 1) Neural-PI control is robust to parameter changes,
disturbances and noises. 2) Neural-PI significantly reduces the number of sampled trajectories to
train well compared with DenseNN."
POWER SYSTEMS FREQUENCY CONTROL,0.313953488372093,"Impact of communication constraints. Most systems do not have fully connected real-time
communication capabilities, so the controller needs to respect the communication constraints and we
show the flexibility of Neural-PI control under different communication structures. We compare the
performance of Neural-PI controller where 1) all the nodes can communicate 2) half of the nodes
can communicate and 3) none of the nodes can communicate (thus the controller is decentralized),
respectively. The transient and steady-state costs are illustrated in Figure 6(a). Neural-PI-Full achieves
the lowest transient and steady-state cost. Notably, the steady-state cost significantly decreases with
increased communication capability. The reason is that communication serves to better allocated
control efforts such that they can maintain output tracking with smaller control costs. The frequency
dynamics are shown in Figure 6(b)-(d), where under all settings Neural-PI controllers can stabilize to
the setpoint (60Hz) and it converges the fastest under full communication."
POWER SYSTEMS FREQUENCY CONTROL,0.3168604651162791,"(a) Transient and steady cost
(b) Full Comm
(c) Half Comm
(d) Decentralized
Figure 6: (a) The average transient and steady-state cost with error bar on 100 testing trajectories for Neural-
PI controller with different communication constraints. The dynamics of Neural-PI where (b) all nodes can
communicate, (c) half of nodes can communicate, (d) none nodes can communicate."
CONCLUSIONS,0.31976744186046513,"7
Conclusions"
CONCLUSIONS,0.3226744186046512,"This paper proposes structured Neural-PI controllers for multiple-input multiple-output dynamic
systems. We parameterize the P and I terms as the gradients of strictly convex neural networks. For a
wide range of dynamic systems, we show that this controller structure provides end-to-end stability
and zero output tracking error guarantees. Experiments demonstrate that the Neural-PI control law
retains all the stability guarantees of classical linear PI control, but achieves much lower transient
cost. Unstructured neural networks, however, lead to unstable behavior and much higher costs. The
theoretic guarantees of Neural-PI control also significantly reduce the amount of data required to
train well. Since classical PI control is widely utilized in real-world applications, we expect that
the controllers can be transferred to real-world scenarios. Potential barriers to the application in
real-world scenarios include the verification of EIP when a storage function is difficult to find and
provable guarantees on the robustness to noises. These are all important future directions for us."
CONCLUSIONS,0.32558139534883723,Acknowledgments and Disclosure of Funding
CONCLUSIONS,0.32848837209302323,"The authors are partially supported by the NSF grants ECCS-1930605, 1942326, 2200692, and
2153937."
REFERENCES,0.3313953488372093,References
REFERENCES,0.33430232558139533,"[1] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning, pages 1467–1476. PMLR, 2018."
REFERENCES,0.3372093023255814,"[2] Yingying Li, Tianpeng Zhang, Subhro Das, Jeff Shamma, and Na Li. Safe adaptive learning for
linear quadratic regulators with constraints. Technical report, Technical report, 2023."
REFERENCES,0.34011627906976744,"[3] Samuel Coogan and Murat Arcak. A dissipativity approach to safety verification for intercon-
nected systems. IEEE Transactions on Automatic Control, 60(6):1722–1727, 2014."
REFERENCES,0.3430232558139535,"[4] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent reinforcement learning: A
selective overview of theories and algorithms. Handbook of Reinforcement Learning and
Control, pages 321–384, 2021."
REFERENCES,0.34593023255813954,"[5] Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learn-
ing for networked systems with average reward. Advances in Neural Information Processing
Systems, 33:2074–2086, 2020."
REFERENCES,0.3488372093023256,"[6] Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent
reinforcement learning for active voltage control on power distribution networks. Advances in
Neural Information Processing Systems, 34:3271–3284, 2021."
REFERENCES,0.35174418604651164,"[7] Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for
networked system control. In International Conference on Learning Representations, 2019."
REFERENCES,0.3546511627906977,"[8] Wenqi Cui, Yan Jiang, and Baosen Zhang. Reinforcement learning for optimal primary frequency
control: A Lyapunov approach. IEEE Transactions on Power Systems, 2022."
REFERENCES,0.35755813953488375,"[9] Yuanyuan Shi, Guannan Qu, Steven Low, Anima Anandkumar, and Adam Wierman. Stability
constrained reinforcement learning for real-time voltage control. American Control Conference
(ACC), 2022."
REFERENCES,0.36046511627906974,"[10] Jie Feng, Yuanyuan Shi, Guannan Qu, Steven H Low, Anima Anandkumar, and Adam Wierman.
Stability constrained reinforcement learning for real-time voltage control in distribution systems.
IEEE Transactions on Control of Network Systems, 2023."
REFERENCES,0.3633720930232558,"[11] Charles Dawson, Sicun Gao, and Chuchu Fan. Safe control with learned certificates: A survey
of neural lyapunov, barrier, and contraction methods for robotics and control. IEEE Transactions
on Robotics, 2023."
REFERENCES,0.36627906976744184,"[12] Guannan Qu, Yuanyuan Shi, Sahin Lale, Anima Anandkumar, and Adam Wierman. Stable
online control of linear time-varying systems. In Learning for Dynamics and Control, pages
742–753. PMLR, 2021."
REFERENCES,0.3691860465116279,"[13] Christian Fiedler, Carsten W Scherer, and Sebastian Trimpe. Learning-enhanced robust con-
troller synthesis with rigorous statistical and control-theoretic guarantees. In 2021 60th IEEE
Conference on Decision and Control (CDC), pages 5122–5129. IEEE, 2021."
REFERENCES,0.37209302325581395,"[14] Changhong Zhao, Enrique Mallada, and Florian Dörfler. Distributed frequency control for
stability and economic dispatch in power networks. In Proc. of American Control Conference,
pages 2359–2364, July 2015. doi: 10.1109/ACC.2015.7171085."
REFERENCES,0.375,"[15] Martin Andreasson, Dimos V Dimarogonas, Henrik Sandberg, and Karl Henrik Johansson.
Distributed control of networked dynamical systems: Static feedback, integral action and
consensus. IEEE Transactions on Automatic Control, 59(7):1750–1764, 2014."
REFERENCES,0.37790697674418605,"[16] Hassan K Khalil. Nonlinear control, volume 406. Pearson New York, 2015."
REFERENCES,0.3808139534883721,"[17] Yan Jiang, Wenqi Cui, Baosen Zhang, and Jorge Cortés. Stable reinforcement learning for
optimal frequency control: A distributed averaging-based integral approach. IEEE Open Journal
of Control Systems, 1:194–209, 2022."
REFERENCES,0.38372093023255816,"[18] Sönke Thomsen, Nils Hoffmann, and Friedrich Wilhelm Fuchs. Pi control, pi-based state space
control, and model-based predictive control for drive systems with elastically coupled loads—a
comparative study. IEEE transactions on industrial electronics, 58(8):3647–3657, 2010."
REFERENCES,0.3866279069767442,"[19] Jian Wang, Xin Xu, Daxue Liu, Zhenping Sun, and Qingyang Chen. Self-learning cruise
control using kernel-based least squares policy iteration. IEEE Transactions on Control Systems
Technology, 22(3):1078–1087, 2013."
REFERENCES,0.38953488372093026,"[20] Deany Putri Aulia, Alfiyah Shaldzabila Yustin, Amien Marzuq Hilman, Aulia Rahma Annisa,
and Eng Wahyu Kunto Wibowo. Fuzzy gain scheduling for cascaded pi-control for dc motor. In
2021 IEEE Conference on Energy Conversion (CENCON), pages 158–162. IEEE, 2021."
REFERENCES,0.39244186046511625,"[21] Murat Arcak, Chris Meissen, and Andrew Packard. Networks of dissipative systems: composi-
tional certification of stability, performance, and safety. Springer, 2016."
REFERENCES,0.3953488372093023,"[22] George H Hines, Murat Arcak, and Andrew K Packard. Equilibrium-independent passivity: A
new definition and numerical certification. Automatica, 47(9):1949–1956, 2011."
REFERENCES,0.39825581395348836,"[23] Mathias Bürger, Daniel Zelazo, and Frank Allgöwer. Duality and network theory in passivity-
based cooperative control. Automatica, 50(8):2051–2061, 2014."
REFERENCES,0.4011627906976744,"[24] Wenqi Cui and Baosen Zhang. Equilibrium-independent stability analysis for distribution
systems with lossy transmission lines. IEEE Control Systems Letters, 6:3349–3354, 2022."
REFERENCES,0.40406976744186046,"[25] John W Simpson-Porco. Equilibrium-independent dissipativity with quadratic supply rates.
IEEE Transactions on Automatic Control, 64(4):1440–1455, 2018."
REFERENCES,0.4069767441860465,"[26] Ya-Chien Chang, Nima Roohi, and Sicun Gao. Neural lyapunov control. Advances in neural
information processing systems, 32, 2019."
REFERENCES,0.40988372093023256,"[27] Ruikun Zhou, Thanin Quartz, Hans De Sterck, and Jun Liu. Neural lyapunov control of
unknown nonlinear systems with stability guarantees. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems,
2022. URL https://openreview.net/forum?id=QvlcRh8hd8X."
REFERENCES,0.4127906976744186,"[28] Charles Dawson, Zengyi Qin, Sicun Gao, and Chuchu Fan. Safe nonlinear control using
robust neural lyapunov-barrier functions.
In Aleksandra Faust, David Hsu, and Gerhard
Neumann, editors, Proceedings of the 5th Conference on Robot Learning, volume 164 of
Proceedings of Machine Learning Research, pages 1724–1735. PMLR, 08–11 Nov 2022. URL
https://proceedings.mlr.press/v164/dawson22a.html."
REFERENCES,0.41569767441860467,"[29] Qiuhua Huang, Renke Huang, Weituo Hao, Jie Tan, Rui Fan, and Zhenyu Huang. Adaptive
power system emergency control using deep reinforcement learning. IEEE Transactions on
Smart Grid, 2019."
REFERENCES,0.4186046511627907,"[30] Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Neural certificates for safe
control policies. arXiv preprint arXiv:2006.08465, 2020."
REFERENCES,0.42151162790697677,"[31] Armin Lederer, Jonas Umlauft, and Sandra Hirche. Uniform error bounds for gaussian process
regression with application to safe control. Advances in Neural Information Processing Systems,
32, 2019."
REFERENCES,0.42441860465116277,"[32] Andreas Doerr, Christian Daniel, Duy Nguyen-Tuong, Alonso Marco, Stefan Schaal, Toussaint
Marc, and Sebastian Trimpe. Optimizing long-term predictions for model-based policy search.
In Conference on Robot Learning, pages 227–238. PMLR, 2017."
REFERENCES,0.4273255813953488,"[33] Nathan Lambert, Albert Wilcox, Howard Zhang, Kristofer SJ Pister, and Roberto Calandra.
Learning accurate long-term dynamics for model-based reinforcement learning. In 2021 60th
IEEE Conference on Decision and Control (CDC), pages 2880–2887. IEEE, 2021."
REFERENCES,0.43023255813953487,"[34] Andreas Doerr, Duy Nguyen-Tuong, Alonso Marco, Stefan Schaal, and Sebastian Trimpe.
Model-based policy search for automatic tuning of multivariate pid controllers. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pages 5295–5301. IEEE, 2017."
REFERENCES,0.4331395348837209,"[35] Ignacio Carlucho, Mariano De Paula, and Gerardo G Acosta. An adaptive deep reinforcement
learning approach for mimo pid control of mobile robots. ISA transactions, 102:280–294, 2020."
REFERENCES,0.436046511627907,"[36] Joseph Sill. Monotonic networks. In Proceedings of the 10th International Conference on
Neural Information Processing Systems, pages 661–667, 1997."
REFERENCES,0.438953488372093,"[37] Antoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks. Advances in
neural information processing systems, 32, 2019."
REFERENCES,0.4418604651162791,"[38] Xingchao Liu, Xing Han, Na Zhang, and Qiang Liu. Certified monotonic neural networks.
Advances in Neural Information Processing Systems, 33:15427–15438, 2020."
REFERENCES,0.44476744186046513,"[39] Hennie Daniels and Marina Velikova. Monotone and partially monotone neural networks. IEEE
Transactions on Neural Networks, 21(6):906–917, 2010."
REFERENCES,0.4476744186046512,"[40] Aishwarya Sivaraman, Golnoosh Farnadi, Todd Millstein, and Guy Van den Broeck.
Counterexample-guided learning of monotonic neural networks. Advances in Neural Informa-
tion Processing Systems, 33:11936–11948, 2020."
REFERENCES,0.45058139534883723,"[41] Hong Zhang and Zhen Zhang. Feedforward networks with monotone constraints. In IJCNN’99.
International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339),
volume 3, pages 1820–1823. IEEE, 1999."
REFERENCES,0.45348837209302323,"[42] Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. comput. math,
15(1):3–43, 2016."
REFERENCES,0.4563953488372093,"[43] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network
dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018
IEEE International Conference on Robotics and Automation (ICRA), pages 7559–7566. IEEE,
2018."
REFERENCES,0.45930232558139533,"[44] Lucian Bu¸soniu, Tim de Bruin, Domagoj Toli´c, Jens Kober, and Ivana Palunko. Reinforcement
learning for control: Performance, stability, and deep approximators. Annual Reviews in Control,
46:8–28, 2018."
REFERENCES,0.4622093023255814,"[45] Priya L Donti, Melrose Roderick, Mahyar Fazlyab, and J Zico Kolter. Enforcing robust
control guarantees within neural network policies. In International Conference on Learning
Representations, 2020."
REFERENCES,0.46511627906976744,"[46] Pulkit Nahata, Raffaele Soloperto, Michele Tucci, Andrea Martinelli, and Giancarlo Ferrari-
Trecate. A passivity-based approach to voltage stabilization in dc microgrids with zip loads.
Automatica, 113:108770, 2020."
REFERENCES,0.4680232558139535,"[47] Chris Meissen, Kristian Klausen, Murat Arcak, Thor I Fossen, and Andrew Packard. Passivity-
based formation control for uavs with a suspended load. IFAC-PapersOnLine, 50(1):13150–
13155, 2017."
REFERENCES,0.47093023255813954,"[48] Wei He, Hejia Gao, Chen Zhou, Chenguang Yang, and Zhijun Li. Reinforcement learning
control of a flexible two-link manipulator: an experimental investigation. IEEE Transactions on
Systems, Man, and Cybernetics: Systems, 51(12):7326–7336, 2020."
REFERENCES,0.4738372093023256,"[49] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International
Conference on Machine Learning, pages 146–155. PMLR, 2017."
REFERENCES,0.47674418604651164,"[50] Yize Chen, Yuanyuan Shi, and Baosen Zhang. Optimal control via neural networks: A convex
approach. In International Conference on Learning Representations, 2019."
REFERENCES,0.4796511627906977,"[51] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,
2004."
REFERENCES,0.48255813953488375,"[52] T Athay, Robin Podmore, and Sudhir Virmani. A practical method for the direct analysis of
transient stability. IEEE Transactions on Power Apparatus and Systems, PAS-98(2):573–584,
1979."
REFERENCES,0.48546511627906974,"[53] Ján Drgona, Aaron Tuor, and Draguna Vrabie. Learning constrained adaptive differentiable
predictive control policies with guarantees. arXiv preprint arXiv:2004.11184, 2020."
REFERENCES,0.4883720930232558,"[54] Erik Weitenberg, Claudio De Persis, and Nima Monshizadeh. Exponential convergence under
distributed averaging integral frequency control. Automatica, 98:103–113, Dec. 2018."
REFERENCES,0.49127906976744184,"[55] Erik Weitenberg, Yan Jiang, Changhong Zhao, Enrique Mallada, Claudio De Persis, and Florian
Dörfler. Robust decentralized secondary frequency control in power systems: Merits and
tradeoffs. IEEE Transactions on Automatic Control, 64(10):3967–3982, Oct. 2019."
REFERENCES,0.4941860465116279,"[56] Florian Dörfler and Sergio Grammatico. Gather-and-broadcast frequency control in power
systems. Automatica, 79:296–305, May 2017. ISSN 0005-1098. doi: https://doi.org/10.1016/j.
automatica.2017.02.003."
REFERENCES,0.49709302325581395,"[57] Peter W Sauer, Mangalore A Pai, and Joe H Chow. Power system dynamics and stability: with
synchrophasor measurement and power system toolbox. John Wiley & Sons, 2017."
REFERENCES,0.5,"A
Proof"
REFERENCES,0.502906976744186,"A.1
Proof of Theorem 1"
REFERENCES,0.5058139534883721,"We leverage the results in [49, 50] that the structure (5) with ReLU activation is a universal approx-
imation for any convex function. However, ReLU activations are not strictly convex, and thus we
design the Softplus-β activation. By showing that the structure (5) with Softplus-β activation can
approximate neural networks with the ReLU activations arbitrarily closely when β is sufficiently
large, we then prove that the structure (5) with Softplus-β can universally approximate any strictly
convex function."
REFERENCES,0.5087209302325582,"To prepare for the proof of Theorem 1, we first derive the following Lemma about the difference
between ReLU activations and Softplus-β activation."
REFERENCES,0.5116279069767442,"Lemma 3. Consider the ReLU activation σReLU
l
(x) := max(x, 0). For all x ∈R and ∆> 0, we"
REFERENCES,0.5145348837209303,"have 0 <

σSoftplusβ
l
(x + ∆) −σReLU
l
(x)

< ∆+ 1"
REFERENCES,0.5174418604651163,β log(2).
REFERENCES,0.5203488372093024,"Proof. Note that

σSoftplusβ
l
(x + ∆) −σReLU
l
(x)

=

σSoftplusβ
l
(x + ∆) −σSoftplusβ
l
(x)

+

σSoftplusβ
l
(x) −σReLU
l
(x)

,"
REFERENCES,0.5232558139534884,"we prove the lemma by deriving the bound for

σSoftplusβ
l
(x + ∆) −σSoftplusβ
l
(x)

and

σSoftplusβ
l
(x) −σReLU
l
(x)

as follows"
REFERENCES,0.5261627906976745,"(i) Since dσSoftplusβ
l
(x)
dx
= eβx/(1 + eβx) ∈(0, 1), then 0 <

σSoftplusβ
l
(x + ∆) −σSoftplusβ
l
(x)

<
∆for ∆> 0.
(ii) Explicitly represent σSoftplusβ
l
(x) −σReLU
l
(x) yields"
REFERENCES,0.5290697674418605,"σSoftplusβ
l
(x) −σReLU
l
(x) =
 1"
REFERENCES,0.5319767441860465,"β log(1 + eβx) −x,
x ≥0
1
β log(1 + eβx),
x < 0"
REFERENCES,0.5348837209302325,"Thus,
d

σSoftplusβ
l
(x) −σReLU
l
(x)
"
REFERENCES,0.5377906976744186,"dx
:=

−1/(1 + eβx),
x ≥0
eβx/(1 + eβx),
x < 0"
REFERENCES,0.5406976744186046,"and therefore

σSoftplusβ
l
(x) −σReLU
l
(x)

≤

σSoftplusβ
l
(0) −σReLU
l
(0)

=
1
β log(2). Note"
REFERENCES,0.5436046511627907,that 1
REFERENCES,0.5465116279069767,β log(1 + eβx) > 1
REFERENCES,0.5494186046511628,"β log(eβx) = x, then 0 <

σSoftplusβ
l
(x) −σReLU
l
(x)

≤1"
REFERENCES,0.5523255813953488,β log(2).
REFERENCES,0.5552325581395349,"Combining (i) and (ii), 0 <

σSoftplusβ
l
(x + ∆) −σReLU
l
(x)

< ∆+ 1"
REFERENCES,0.5581395348837209,β log(2).
REFERENCES,0.561046511627907,The proof of Theorem 1 is given below.
REFERENCES,0.563953488372093,"Proof. Previous works [49, 50] have shown that the structure (5) with ReLU activation is a universal
approximation for any convex function. Hence, for any q(z) : Z 7→R , there exists g(z; θ)ReLU
constructed by (5) where the activation function is ReLU and satisfying |g(z; θ)ReLU −q(z)| < 1"
REFERENCES,0.5668604651162791,"2ϵ.
Note that"
REFERENCES,0.5697674418604651,"|g(z; θ)Softplusβ −q(z)| ≤|g(z; θ)Softplusβ −g(z; θ)ReLU| + |g(z; θ)ReLU −q(z)|,"
REFERENCES,0.5726744186046512,"then it suffices to prove |g(z; θ)Softplusβ −q(z)| < ϵ by showing the existence of g(z; θ)Softplusβ such
that |g(z; θ)Softplusβ −g(z; θ)ReLU| < 1"
REFERENCES,0.5755813953488372,2ϵ for all z ∈Z.
REFERENCES,0.5784883720930233,"Next, we compute the difference of the structure (5) with softplus-β and ReLU activations by tracing
through the first layer to the last layer, under the same weights θ =
n
W (z)
0:k−1, W (o)
1:k−1, b0:k−1
o
."
REFERENCES,0.5813953488372093,The difference between the output of the first layer in g(z; θ)Softplusβ and g(z; θ)ReLU is
REFERENCES,0.5843023255813954,"oSoftplusβ
1
−oReLU
1
= σSoftplusβ
1

W (z)
0
z + b0

−σReLU
1

W (z)
0
z + b0

,
(10)"
REFERENCES,0.5872093023255814,"which yields 0 < oSoftplusβ
1
−oReLU
1
≤1"
REFERENCES,0.5901162790697675,β log(2)1 by Lemma 3.
REFERENCES,0.5930232558139535,The difference of the second layer is
REFERENCES,0.5959302325581395,"oSoftplusβ
2
−oReLU
2
= σSoftplusβ
2

W (o)
1
oSoftplusβ
1
+ W (z)
2
z + b2

−σReLU
2

W (o)
1
oReLU
1
+ W (z)
2
z + b2
"
REFERENCES,0.5988372093023255,"= σSoftplusβ
2

W (o)
1

oSoftplusβ
1
−oReLU
1

+ W (o)
1
oReLU
1
+ W (z)
2
z + b2
"
REFERENCES,0.6017441860465116,"−σReLU
2

W (o)
1
oReLU
1
+ W (z)
2
z + b2

. (11)"
REFERENCES,0.6046511627906976,"Since all the element of W (o)
1
is positive, we have 0 < W (o)
1

oSoftplusβ
1
−oReLU
1

≤1"
REFERENCES,0.6075581395348837,"β log(2)W (o)
1
1."
REFERENCES,0.6104651162790697,"Applying Lemma 3 element-wise yields 0 ≤oSoftplusβ
2
−oReLU
2
≤1"
REFERENCES,0.6133720930232558,"β log(2)W (o)
1
1 + 1"
REFERENCES,0.6162790697674418,β log(2)1.
REFERENCES,0.6191860465116279,"Similarily 0 ≤oSoftplusβ
3
−oReLU
3
≤1"
REFERENCES,0.622093023255814,"β log(2)W (o)
2
W (o)
1
1+ 1"
REFERENCES,0.625,"β log(2)W (o)
2
1+ 1"
REFERENCES,0.627906976744186,β log(2)1. By induction
REFERENCES,0.6308139534883721,"0 ≤oSoftplusβ
l+1
−oReLU
l+1 ≤1"
REFERENCES,0.6337209302325582,"β log(2)  1 +  
l
X i=1 lY"
REFERENCES,0.6366279069767442,"j=i
W (o)
j
1   "
REFERENCES,0.6395348837209303,"
(12)"
REFERENCES,0.6424418604651163,"Hence,"
REFERENCES,0.6453488372093024,0 ≤g(z; θ)Softplusβ −g(z; θ)ReLU ≤1
REFERENCES,0.6482558139534884,β log(2)  1 + 
REFERENCES,0.6511627906976745,"
k−1
X i=1 k−1
Y"
REFERENCES,0.6540697674418605,"j=i
W (o)
j
1   "
REFERENCES,0.6569767441860465,",
(13)"
REFERENCES,0.6598837209302325,"where Qk−1
j=i W (o)
j
:= W (o)
k−1W (o)
k−2 · · · W (o)
i
."
REFERENCES,0.6627906976744186,Let β > 2
REFERENCES,0.6656976744186046,"ϵ log(2)

1 +
Pk−1
i=1
Qk−1
j=i W (o)
j

1

, then 0 ≤g(z; θ)Softplusβ −g(z; θ)ReLU ≤1"
REFERENCES,0.6686046511627907,"2ϵ. We
complete the proof using"
REFERENCES,0.6715116279069767,|g(z; θ)Softplusβ −q(z)| ≤|g(z; θ)Softplusβ −g(z; θ)ReLU| + |g(z; θ)ReLU −q(z)| < 1
REFERENCES,0.6744186046511628,2ϵ + 1
REFERENCES,0.6773255813953488,2ϵ = ϵ.
REFERENCES,0.6802325581395349,"A.2
Proof of Theorem 2"
REFERENCES,0.6831395348837209,"Proof. At an equilibrium, the right side of (4b) equals to zero gives y∗= ¯y and the corresponding
set of equlibrium E = {x∗, s∗|f(x∗, r(s∗)) = 0, y∗= ¯y, h(x∗) = y∗}. We construct a Lyapunov
function to prove that if there is a feasible equilibrium in E, then the system is locally asymptotically
stable around it."
REFERENCES,0.686046511627907,"We construct a Lyapunov function using the storage function in Assumption 2 and the Bregman
distance in Lemma 2 as"
REFERENCES,0.688953488372093,"V (x, s)|x∗,s∗= S(x, x∗) + B(s, s∗; θ(I), β(I)),
(14)"
REFERENCES,0.6918604651162791,"where the functions by construction satisfy S(x, x∗) ≥0, B(s, s∗; θ(I), β(I)) ≥0 with equality
holds only when x = x∗and s = s∗, respectively. Hence, V (x, s)|x∗,s∗is a well-defined function
that is positive definite and equals to zero at the equilibrium."
REFERENCES,0.6947674418604651,"To prepare for the calculation of the time derivative of the Lyapunov function, we start by calculating
the time derivative of the function B(s, s∗; θ(I), β(I)):"
REFERENCES,0.6976744186046512,"˙B(s, s∗; θ(I), β(I)) =

∇sg(I)(s; θ(I), β(I)) −∇sg(I)(s∗; θ(I), β(I))
⊤
˙s"
REFERENCES,0.7005813953488372,"1= (r(s) −r (s∗))⊤(−(y −y∗)) , (15)"
REFERENCES,0.7034883720930233,"where 1 follows from ∇sg(I)(s; θ(I), β(I)) = r (s) and ˙s = (−(y −¯y)) = (−(y −y∗))."
REFERENCES,0.7063953488372093,The time derivative of the Lyapunov function is
REFERENCES,0.7093023255813954,"˙V (x, s)|x∗,s∗= ˙S(x, x∗) + ˙B(s, s∗; θ, β)"
REFERENCES,0.7122093023255814,"1
≤−ρ ∥y −y∗∥2 + (y −y∗)⊤(u −u∗) + (r(s) −r (s∗))⊤(−(y −y∗))"
REFERENCES,0.7151162790697675,2= −ρ ∥y −y∗∥2 + (y −y∗)⊤(p(−y + ¯y) −p(−y∗+ ¯y))
REFERENCES,0.7180232558139535,"3
≤−ρ ∥y −y∗∥2 (16)"
REFERENCES,0.7209302325581395,"where 1 follows from the strict EIP property and equations derived in (15). The equality 2 is
derived by plugging in the controller design in (4a) where u = p(−y + ¯y) + r(s) and u∗=
p(−y∗+ ¯y) + r(s∗). The inequality 3 uses strictly monotone property of p(·) ."
REFERENCES,0.7238372093023255,"Therefore, ˙V (x, s)|x∗,s∗≤0 with equality only holds at the equilibrium. By Lyapunov stability
theory in Proposition 1, the system is locally asymptotically stable around the equilibrium."
REFERENCES,0.7267441860465116,"B
Experiments"
REFERENCES,0.7296511627906976,"We demonstrate the effectiveness of the proposed neural-PI control in two dynamical systems: vehicle
platooning and power system frequency control. All experiments are run with an NVIDIA Tesla T4
GPU with 16GB memory. For completeness, the figures highlighted in Section 6 are also shown
below with more thorough discussions. Code is available at this link."
REFERENCES,0.7325581395348837,"B.1
Vehicle platooning"
REFERENCES,0.7354651162790697,"B.1.1
Problem statement"
REFERENCES,0.7383720930232558,"The first experiment is the vehicle platoon control in [3, 23] with m vehicles, where u ∈Rm is the
control signal to adjust the velocities of vehicles and the output y ∈Rm is their actual velocities. The
state is x = (ζ, y), where ζ ∈Rm is the relative position of vehicles with ζ(0) ⊥Im(1m) (namely,
the vehicles are not in the same position at the time step 0). The dynamic model is"
REFERENCES,0.7412790697674418,"˙ζ = Γy,"
REFERENCES,0.7441860465116279,"˙y = ˆκ

−(y −λ0) + ˆρ

u −E ˆDE⊤ζ

,
(17)"
REFERENCES,0.747093023255814,"where ˆκ = diag(κ1, · · · , κm), ˆρ = diag(ρ1, · · · , ρm) ∈Rm×m are constant diagonal matrices
and κi > 0, ρi > 0 for all i = 1, · · · , m. The vector λ0 = (λ0
1, · · · , λ0
m) ∈Rm reflects the
default velocity of vehicles. The matrix E ∈Rm×e is the incidence matrix that indicates the
neighbouring relations for e pairs of neighbouring vehicles and satisfy ker(E⊤) = Im(1m). The
matrix Γ := Im −1"
REFERENCES,0.75,"m1m1⊤
m extracts the relative velocities of vehicles by Γy. The diagonal matrix
ˆD ∈Rm×m represents the sensitivity to the relative distance of neighbouring vehicles."
REFERENCES,0.752906976744186,"B.1.2
Verification of Assumption 2"
REFERENCES,0.7558139534883721,"We start by verifying the uniqueness of x∗for any u∗∈Rm. At the equilibrium, the right side
of (17) equals zero gives

−(y∗−λ0) + ˆρ

u∗−E ˆDE⊤ζ∗
= 0m and Γy∗= 0m.
(18)"
REFERENCES,0.7587209302325582,"For a given u∗∈Rm, suppose there exists x∗
a = (ζ∗
a, y∗
a) and x∗
b = (ζ∗
b , y∗
b), x∗
a ̸= x∗
b such
that (18) holds. Plugging in (18) gives"
REFERENCES,0.7616279069767442,"(y∗
a −y∗
b) + ˆρE ˆDE⊤(ζ∗
a −ζ∗
b ) = 0m
(19a)
Γ(y∗
a −y∗
b) = 0m.
(19b)"
REFERENCES,0.7645348837209303,"Note that ΓE = E. Left multiplying (19a) with (E ˆDE⊤(ζ∗
a −ζ∗
b ))⊤Γ yields (E ˆDE⊤(ζ∗
a −
ζ∗
b ))⊤ˆρE ˆDE⊤(ζ∗
a −ζ∗
b ) = 0, which holds if and only if E ˆDE⊤(ζ∗
a −ζ∗
b ) = 0m since ˆρ ≻0."
REFERENCES,0.7674418604651163,"Hence, (y∗
a −y∗
b) = −ˆρE ˆDE⊤(ζ∗
a −ζ∗
b ) = 0m and therefore y∗
a = y∗
b. Note that ker(E ˆDE⊤) =
Im(1m) and Im(Γ) ⊥Im(1m), thus (ζ∗
a −ζ∗
b ) ⊥Im(1m). Hence, E ˆDE⊤(ζ∗
a −ζ∗
b ) = 0m if and
only if ζ∗
a = ζ∗
b . Theorefore, for every equilibrium u∗∈Rm, there is a unique x∗= (ζ∗, y∗) ∈Rn
such that f(x∗, u∗) = 0n."
REFERENCES,0.7703488372093024,"Let the storage function be S (x, x∗) = 1"
REFERENCES,0.7732558139534884,2(y −y∗)⊤ˆκ−1 ˆρ−1(y −y∗) + 1
REFERENCES,0.7761627906976745,2ζ⊤E ˆDE⊤ζ. Then
REFERENCES,0.7790697674418605,"˙S (x, x∗) = (y −y∗)⊤ˆρ−1ˆκ−1 ˙y + ζ⊤E ˆDE⊤˙ζ"
REFERENCES,0.7819767441860465,"= (y −y∗)⊤ˆρ−1 
−(y −λ0) + ˆρ

u −E ˆDE⊤ζ

+ ζ⊤E ˆDE⊤Γy"
REFERENCES,0.7848837209302325,"1= −ˆρ−1||y −y∗||2
2 + (y −y∗)⊤(u −u∗)"
REFERENCES,0.7877906976744186,"2
≤−(min
i
ρ−1
i )||y −y∗||2
2 + (y −y∗)⊤(u −u∗)"
REFERENCES,0.7906976744186046,"where 1 follows from

−(y∗−λ0) + ˆρ

u∗−E ˆDE⊤ζ∗
= 0m and E⊤y∗= E⊤Γy∗= 0e
by definition of equilibrium. The inequality 2 follows from ˆρ > 0. Therefore, the dynamics in (17)
satisfy conditions in Assumption 2."
REFERENCES,0.7936046511627907,"B.1.3
Simulation and Visualization"
REFERENCES,0.7965116279069767,"Simulation and training setup
We adopt the model setup and parameters in [3, 23]. The number of
vehicles is m = 20. The sensitivity parameter is κi = 1 for all vehicles. The parameters λ0
i and ρi are
randomly generated by λ0
i ∼uniform[5, 6] and ρi ∼uniform[1, 2], respectively. We train for 400
epochs, where each epoch trains with 300 trajectories with initial velocities yi(0) ∼uniform[5, 6]
and initial integral variable si(0) = 0. The stepsize in time is set as ∆t = 0.02s and for K = 300
steps in a trajectory (Namely, each trajectory evolves 6s)."
REFERENCES,0.7994186046511628,"We implement control law in u to realize a specific output agreement at ¯y and reduce the transient
cost. The transient cost is set to be J(y, u) = PK
k=1 ||y(k∆t) −¯y||1 + ˆc||u(k∆t)||2
2, where
ˆc = diag(c1, · · · , cm) with ci ∼uniform[0.025, 0.075]. The loss function in training is set to be the
same as J(y, u), such that neural networks are optimized to reduce transient cost through training.
The neural PI controller can be trained by most model-based or model-free algorithms, and we use
the model-based framework in [8, 53] by embedding the system dynamic model in the computation
graph shown in Figure 4 and training Neural-PI by gradient descent through J(y, u)."
REFERENCES,0.8023255813953488,"Controller performances.
We compare the performance of 1) Neural-PI: the learned structured
Neural-PI controllers parametrized by (7) with three layers and 20 neurons in each hidden layer. The
neural networks are updated using Adam with learning rate initializes at 0.05 and decays every 50
steps with a base of 0.7. 2) DenseNN-PI: The proportional and integral terms are parameterized
by dense neural networks (5) with three layers, 20 neurons in each hidden layer, and unconstrained
weights. The neural networks are updated using Adam with learning rate initializes at 0.035 and
decays every 50 steps with a base of 0.7. 3) Linear-PI: linear PI control where p(¯y−y) := KP (¯y−y),
r(s) := KI(s) with KP and KI being the trainable proportional and integral coefficients. The
coefficients are updated using Adam with learning rate initializes at 0.03 and decays every 50 steps
with a base of 0.7. All of them have no communication constraints. All of the controllers are trained
using 5 random seeds. The training time is shown in Table 1."
REFERENCES,0.8052325581395349,Table 1: Training time for vehicle platoon control
REFERENCES,0.8081395348837209,"Method
Average Training time (s)
Standard Deviation (s)"
REFERENCES,0.811046511627907,"Neural-PI
5232.36
30.55
DenseNN-PI
3567.01
16.28
Linear-PI
1836.93
10.09"
REFERENCES,0.813953488372093,"The average batch loss during epochs of training with 5 seeds is shown in Figure 7(a). All of the
three methods converge, with the Neural-PI achieves the lowest cost. Figure 7(b) shows the transient"
REFERENCES,0.8168604651162791,"(a) Training Loss
(b) Average transient and steady cost"
REFERENCES,0.8197674418604651,"Figure 7: (a) The average batch loss during epochs of training with 5 seeds. All converge, with the Neural-PI
achieving the lowest cost. (b) The average transient cost and steady-state cost with error bar on 100 testing
trajectories starting from randomly generated initial states. Neural-PI achieves a transient cost that is much lower
than others. DenseNN without structured design has both high costs in transient and steady-state performances."
REFERENCES,0.8226744186046512,"and steady-state cost on 100 testing trajectories starting from randomly generated initial states. The
steady-state cost is C(y, u) = ||y(15) −¯y||1 + ˆc||u(15)||2
2, where we use the variables at the time
t = 15s since the dynamics approximately enter the steady state after t = 15s as we will show later
in simulation. Neural-PI and Linear-PI have the lowest steady-state cost, and the output reaches ¯y as
guaranteed by Theorem 2. Neural-PI also achieves a transient cost that is much lower than others.
By contrast, DenseNN-PI without structured design has both high costs in transient and steady-state
performances."
REFERENCES,0.8255813953488372,(a) Neural-PI
REFERENCES,0.8284883720930233,(b) Linear-PI
REFERENCES,0.8313953488372093,(c) DenseNN-PI
REFERENCES,0.8343023255813954,"Figure 8: Dynamics of velocity y and control action u with ¯y = 5m/s. (a) Neural-PI stabilizes to ¯y quickly. (b)
Linear-PI achieves output tracking with high control effort. (c) DenseNN leads to unstable behavior."
REFERENCES,0.8372093023255814,"Given ¯y = 5m/s, Figure 8 shows the dynamics of velocity y and control action u on 8 nodes under
the three methods. As guaranteed by Theorem 2, Neural-PI in Figure 8(a) and Linear-PI in Figure 8(b)
reaches the required speed ¯y = 5m/s. However, Linear-PI has slower convergence and much larger
control efforts compared with Neural-PI. Even though DenseNN-PI achieves finite loss both in
training and testing, Figure 8(c) actually exhibits unstable behaviors. In particular, DenseNN-PI
appears to be stable until about 10s, but states blows up quickly after that. Therefore, enforcing
stabilizing structures is essential."
REFERENCES,0.8401162790697675,"B.2
Power systems frequency control"
REFERENCES,0.8430232558139535,"B.2.1
Problem statement"
REFERENCES,0.8459302325581395,"The second experiment is the power system frequency control on the IEEE 39-bus New England
system [52] shown in Figure 9, where u ∈Rm is the control signal to adjust the power injection
from generators and the output y ∈Rm is the rotating speed (i.e., frequency) of generators. The
objective is to stabilize generators at the required frequency ¯y = 60Hz at the steady state while
minimizing the transient control cost. The state is x = (δ, y), where δ ∈Rm is the rotating angle
of generators in the center-of-inertia coordinates with δ(0) ⊥Im(1m) [54]. The model of power
systems reflects the transmission of electricity from generators to loads through power transmission
lines and is represented as follows:"
REFERENCES,0.8488372093023255,"˙δ = Γy,
ˆ
M ˙y = −ˆD(y −¯y) −d + u −Eˆb sin(E⊤δ),
(20)"
REFERENCES,0.8517441860465116,"where
ˆ
M = diag(M1, · · · , Mm), ˆD = diag(D1, · · · , Dm) with Mj > 0 and Dj > 0 being
the inertia and damping constant of generator j, respectively. The vector d is the net load of
the system. The matrix E ∈Rm×e is the incidence matrix corresponding to the topology of
the power network with e transmission lines and satisfying ker(E⊤) = Im(1m). The matrix
Γ := Im −1"
REFERENCES,0.8546511627906976,"m1m1⊤
m extracts the relative rotating speed of generators by Γy. The diagonal matrix
ˆb = diag(b1, · · · , be) ∈Re×e with bj > 0 being the susceptance of the j-th transmission line."
REFERENCES,0.8575581395348837,Figure 9: IEEE 39-bus test system [52]
REFERENCES,0.8604651162790697,"We adopt a common assumption in literature that the power system operates with δ satisfying
H =

δ|[E⊤δ]j ∈(−π/2, π/2) ∀j = 1, · · · , e
	
, where [E⊤δ]j is the angle difference between the
generators in head and tail of the j-th transmission line [55–57]. This range is sufficiently large to
include almost all practical scenarios [55–57]."
REFERENCES,0.8633720930232558,"B.2.2
Verification of Assumption 2"
REFERENCES,0.8662790697674418,"At the equilibrium, the right side of (17) equals zero gives"
REFERENCES,0.8691860465116279,"−ˆD(y∗−¯y) −d + u∗−Eˆb sin(E⊤δ∗) = 0m and Γy∗= 0m.
(21)"
REFERENCES,0.872093023255814,"We start by verifying the uniqueness of x∗for any u∗∈U where (21) has a feasible solution such
that δ ∈H. For a given u∗∈U, suppose there exists x∗
a = (δ∗
a, y∗
a) and x∗
b = (δ∗
b, y∗
b), x∗
a ̸= x∗
b
such that (21) holds. Plugging in (21) gives"
REFERENCES,0.875,"ˆD(y∗
a −y∗
b) + Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

= 0m
(22a)"
REFERENCES,0.877906976744186,"Γ(y∗
a −y∗
b) = 0m.
(22b)"
REFERENCES,0.8808139534883721,"Note that ΓE = E. Left multiplying (22a) with (Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

)⊤Γ ˆD−1 yields
(Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

)⊤ˆD−1(Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

) = 0, which holds if and
only if (Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

) = 0m since ˆD−1 ≻0. Plugging in (22a) gives ˆD(y∗
a −
y∗
b) = 0m, which holds if and only if y∗
a = y∗
b since Di > 0 for all i = 1, · · · , m."
REFERENCES,0.8837209302325582,"Left multiplying (Eˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)

) = 0m with (δ∗
a −δ∗
b))⊤yields"
REFERENCES,0.8866279069767442,"0 =
 
E⊤δ∗
a −E⊤δ∗
b
⊤ˆb
 
sin(E⊤δ∗
a) −sin(E⊤δ∗
b)
 = e
X"
REFERENCES,0.8895348837209303,"j=1
bj
 
[E⊤δ∗
a]j −[E⊤δ∗
b)]j
  
sin([E⊤δ∗
a]j) −sin([E⊤δ∗
b)]j)

.
(23)"
REFERENCES,0.8924418604651163,"Since bj > 0 and sin(·) is strictly increasing in (−π/2, π/2), (23) holds if and only if E⊤(δ∗
a−δ∗
a) =
0e. Note that Im(Γ) ⊥Im(1m), thus (δ∗
a −δ∗
b) ⊥Im(1m). Hence, (23) holds if and only if
δ∗
a = δ∗
b. Therefore, for every equilibrium u∗∈U, there is a unique x∗= (δ∗, y∗) ∈Rn such that
f(x∗, u∗) = 0n."
REFERENCES,0.8953488372093024,"Let the storage function be S (x, x∗) = 1"
REFERENCES,0.8982558139534884,"2(y −y∗)⊤ˆ
M(y −y∗)−1⊤
e ˆb(cos(E⊤δ)−cos(E⊤δ∗))−
(Eˆb sin(E⊤δ∗))⊤(δ −δ∗)). Note that −1⊤
e ˆb(cos(E⊤δ) is strictly convex in H, thus the Bregman
distance −1⊤
e ˆb(cos(E⊤δ) −cos(E⊤δ∗)) −(Eˆb sin(E⊤δ∗))⊤(δ −δ∗)) ≥0 with equality holds
only when δ = δ∗."
REFERENCES,0.9011627906976745,"The time derivative is
˙S (x, x∗) = (y −y∗)⊤ˆ
M ˙y + (Eˆb sin(E⊤δ) −Eˆb sin(E⊤δ∗))⊤˙δ"
REFERENCES,0.9040697674418605,= (y −y∗)⊤(−ˆD(y −¯y) −d + u −Eˆb sin(E⊤δ))
REFERENCES,0.9069767441860465,+ (Eˆb sin(E⊤δ) −Eˆb sin(E⊤δ∗))⊤Γy
REFERENCES,0.9098837209302325,"−(y −y∗)⊤(−ˆD(y∗−¯y) −d + u∗−Eˆb sin(E⊤δ∗))
|
{z
}
=0m
1= −(y −y∗)⊤ˆD(y −y∗) + (y −y∗)⊤(u −u∗)"
REFERENCES,0.9127906976744186,−(y∗)⊤(Eˆb sin(E⊤δ) −Eˆb sin(E⊤δ∗))
REFERENCES,0.9156976744186046,"2
≤−(min
i
Di)||y −y∗||2
2 + (y −y∗)⊤(u −u∗)"
REFERENCES,0.9186046511627907,"where 1 follows from (−ˆD(y∗−¯y)−d+u∗−Eˆb sin(E⊤δ∗)) = 0m by definition of equilibrium.
The relation 2 follows from E⊤y∗= E⊤Γy∗= 0e and Di > 0 for all i = 1, · · · , m. Therefore,
the dynamics (20) of the power system frequency control satisfies conditions in Assumption 2."
REFERENCES,0.9215116279069767,"B.2.3
Simulation and Visualization"
REFERENCES,0.9244186046511628,"Simulation Setup
We conduct experiments on the IEEE New England 10-machine 39-bus (NE39)
power network with parameters given in [52, 8]. We implement control law for power output u
of generators to realize the track of frequency at 60Hz and reduce the power generation cost. The
state δ is initialized as the solution of power flow at the nominal frequency and s is initialized as 0."
REFERENCES,0.9273255813953488,"The number of epochs and batch size are 400 and 300, respectively. The step-size in time is set as
∆t = 0.01s and the number of time stages in a trajectory for training is K = 400."
REFERENCES,0.9302325581395349,"Apart from the accumulated frequency deviation, an important metric for the frequency control
problem is the maximum frequency deviation (also known as the frequency nadir) after a distur-
bance [8]. Hence, the transient cost is set to be J(y, u) = Pn
i=1
 
maxk=1,··· ,K |yi(k∆t) −¯y| +
0.05 PK
k=1 |yi(k∆t) −¯y| + 0.005 PK
k=1(ui(k∆t))2
. The loss function in training is J(y, u), such
that neural networks are optimized to reduce transient cost. The neural PI controller can be trained
by most model-based or model-free algorithms, and we use the model-based framework in [8, 53]
by embedding the system dynamic model in the computation graph shown in Figure 4 and training
Neural-PI by gradient descent through J(y, u)."
REFERENCES,0.9331395348837209,Two major goals of this experiment is
REFERENCES,0.936046511627907,"1) Verifies the robustness of the controller under parameter changes. Note that the load d is a
parameter in the dynamics (20). In particular, power system operator emphasizes on the ability
of the system to withstand a big disturbance such as a step load change. To this end, we train
and test controllers by randomly picking at most three generators to have a step load change
uniformly distributed in uniform[−1, 1] p.u., where 1p.u.=100 MW is the base unit of power for
the IEEE-NE39 test system.
2) Verifies the performances under communication constraints. Most systems do not have fully
connected real-time communication capabilities, so the controller needs to respect the communi-
cation constraints and we show the flexibility of Neural-PI control under different communication
structures."
REFERENCES,0.938953488372093,"Controller Performances.
We compare the performance of Neural-PI controller where 1) all
the nodes can communicate 2) half of the nodes can communicate and 3) none of the nodes can
communicate (thus the controller is decentralized), respectively. All neural-PI controllers are pa-
rameterized by (7) and (8) where each SCNN has three layers and 20 neurons in each hidden layer.
The neural networks are updated using Adam with the learning rate initializes at 0.05 and decays
every 50 steps with a base of 0.7. We compare against the following two benchmarks where all
the nodes can communicate: 4) DenseNN-PI-Full: Dense neural networks (5) with three layers, 20
neurons in each hidden layer, and unconstrained weights. The neural networks are updated using
Adam with a learning rate initializes at 0.01 and decays every 50 steps with a base of 0.7. Note that
DenseNN needs such a small learning rate to let the training converge, the reason is that DenseNN
may lead to unstable behaviors that we will see later. 5) Linear-PI-Full: linear PI control where
p(¯y −y) := KP (¯y −y), r(s) := KI(s) with KP and KI being the trainable proportional and
integral coefficients. The coefficients are updated using Adam with the learning rate initializes at
0.08 and decays every 50 steps with a base of 0.7. All of the controllers are trained using 5 random
seeds. The training time is shown in Table 2."
REFERENCES,0.9418604651162791,Table 2: Training time for power system frequency control
REFERENCES,0.9447674418604651,"Method
Average Training time (s)
Standard Deviation (s)"
REFERENCES,0.9476744186046512,"Neural-PI-Full
4373.52
64.58
Neural-PI-Half
8034.92
115.26
Neural-PI-Dec
23549.34
300.95
DenseNN-PI-Full
2193.84
21.22
Linear-PI-Full
981.65
11.19"
REFERENCES,0.9505813953488372,"The average batch loss during epochs of training with 5 seeds is shown in Figure 10(a). All converge,
with the Neural-PI-Full achieving the lowest cost. Figure 10(b) shows the average transient cost and
steady-state cost with error bar on 100 testing trajectories subject to random step load changes. The
steady-state cost is C(y, u) = 0.05||y(15) −¯y||1 + 0.005||u(15)||2
2, where we use the variables at
the time t = 15s since the dynamics approximately enter the steady state after t = 15s as we will
show later in simulation. Neural-PI-Full achieves the lowest transient and steady-state cost. Notably,
the steady-state cost significantly decreases with increased communication capability. The reason
is that communication serves to better allocated control efforts such that they can maintain output"
REFERENCES,0.9534883720930233,"(a) Training Loss
(b) Transient and steady cost"
REFERENCES,0.9563953488372093,"Figure 10: (a) Average batch loss during epochs of training with 5 seeds. All converge, with the Neural-PI
achieving the lowest cost. (b)The average transient cost and steady-state cost with error bar on 100 testing
trajectories subject to random step load changes. Neural-PI achieves a transient cost that is much lower than
others. The steady-state cost significantly decreases with increased communication capability. DenseNN without
structured design has both high costs in transient and steady-state performances."
REFERENCES,0.9593023255813954,"Table 3: The average transient cost on 100 testing trajectories starting from randomly generated initial
states"
REFERENCES,0.9622093023255814,"Number of training trajectories
Neural-PI
Linear-PI
DenseNN"
REFERENCES,0.9651162790697675,"5
0.1328
0.1915
1.0
10
0.1300
0.1865
0.9833
50
0.1257
0.1838
0.9624
100
0.1234
0.1816
0.9214
300
0.1233
0.1815
0.5347"
REFERENCES,0.9680232558139535,"tracking with smaller control costs. Again, DenseNN without structured design has high costs both in
transient and in steady state."
REFERENCES,0.9709302325581395,"With a step load change at 0.5s, Figure 11 shows the dynamics of frequency y and control action
u on 7 nodes under the five methods. Again, DenseNN-PI-Full in Figure 11(e) exhibits unstable
behavior with large oscillations. As guaranteed by Theorem 2, Neural-PI in Figure 11(a-c) reaches
the required frequency ¯y = 60Hz, but the speed of convergence is lower for reduced communication
capabilities. Hence, the guarantees provided by the structured Neural-PI controllers are robust to
parameter changes and communication constraints, which have significant practical importance."
REFERENCES,0.9738372093023255,"Performance with different numbers of training trajectories.
Table 3 compares the transient
cost attained by different controllers trained with different numbers of trajectories. For both Linear-PI
and Neural-PI, training with 5 trajectories for each epoch has already achieved a similar cost as
training with 300 trajectories. By contrast, unstructured DenseNN requires a much larger amount
of training data to reduce transient costs on testing trajectories. Therefore, the stabilizing structure
significantly reduces the requirement for the number of samples to learn well."
REFERENCES,0.9767441860465116,"The impact of disturbances and noises.
The satisfaction of the Lyapunov condition is robust
to disturbances in the system parameters and does not need to know how large the disturbances are,
as shown in the proof Theorem 2 and Remark 2. Therefore, if there is a sudden change in the load
levels, the proposed controller design still stabilizes the system and tracks the required frequency
at 60Hz. In Figure 12(a), we demonstrate the system dynamics after two disturbances in load. In
Figure 12(b)-(c), we add noises in both data measurement and dynamics with the signal-to-noise
ratio being 5 dB (much larger than typical measurement noises). The results show that the systems
are input-to-state stable, i.e., that bounded noise will lead to bounded states. Incorporating noise in
rigorous theoretical analysis is an important future direction for us."
REFERENCES,0.9796511627906976,(a) NeuralPI-Full
REFERENCES,0.9825581395348837,(b) NeuralPI-Half
REFERENCES,0.9854651162790697,(c) NeuralPI-Dec
REFERENCES,0.9883720930232558,(d) Linear-PI-Full
REFERENCES,0.9912790697674418,"(e) DenseNN-Full
Figure 11: Dynamics of frequency y, control action u and accumulated cost on 7 nodes with ¯y = 60Hz and a
step load change at 0.5s. (a) Neural-PI when all nodes can communicate (b) Neural-PI when half of nodes can
communicate, (c) Neural-PI when none nodes can communicate. The control with different communication
capability all stabilize the system to the required ¯y = 60Hz. (d) Linear-PI-Full is stable but has slower
convergence. (e) DenseNN-PI-Full leads to large frequency deviations and oscillations."
REFERENCES,0.9941860465116279,(a) There is a step increase of load on 0.5s and a step decrease of load on 3.5s.
REFERENCES,0.997093023255814,(b) Neural-PI with noise on data measurement
