Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.006060606060606061,"To better understand complexity in neural networks, we theoretically investigate
1
the idealised phenomenon of lossless network compressibility, whereby an iden-
2
tical function can be implemented with a smaller network. We give an efﬁcient
3
formal algorithm for optimal lossless compression in the setting of single-hidden-
4
layer hyperbolic tangent networks. To measure lossless compressibility, we deﬁne
5
the rank of a parameter as the minimum number of hidden units required to imple-
6
ment the same function. Losslessly compressible parameters are atypical, but their
7
existence has implications for nearby parameters. We deﬁne the proximate rank
8
of a parameter as the rank of the most compressible parameter within a small L∞
9
neighbourhood. Unfortunately, detecting nearby losslessly compressible parame-
10
ters is not so easy: we show that bounding the proximate rank is an NP-complete
11
problem, using a reduction from Boolean satisﬁability via a novel abstract clus-
12
tering problem involving covering points with small squares. These results un-
13
derscore the computational complexity of measuring neural network complexity,
14
laying a foundation for future theoretical and empirical work in this direction.
15"
INTRODUCTION,0.012121212121212121,"1
Introduction
16"
INTRODUCTION,0.01818181818181818,"Learned neural networks often generalise well, depite the excessive expressive capacity of their ar-
17
chitectures (Zhang et al., 2017, 2021). Moreover, learned neural networks are often approximately
18
compressible, in that smaller networks can be found implementing similar functions (via, e.g., model
19
distillation, Buciluˇa et al., 2006; Hinton et al., 2014; see, e.g., Sanh et al., 2019 for a large-scale ex-
20
ample). In other words, learned neural networks are often simpler than they might seem. Advancing
21
our understanding of neural network complexity is key to understanding deep learning.
22"
INTRODUCTION,0.024242424242424242,"We propose studying the idealised phenomenon of lossless compressibility, whereby an identical
23
function can be implemented with a smaller network.1
Classical functional equivalence results
24
imply that, in many architectures, almost all parameters are incompressible in this lossless, unit-
25
based sense (e.g., Sussmann, 1992; Chen et al., 1993; Fefferman, 1994; Phuong and Lampert, 2020).
26
However, these results speciﬁcally exclude measure zero sets of parameters with more complex
27
functional equivalence classes (Anonymous, 2023), some of which are losslessly compressible.
28"
INTRODUCTION,0.030303030303030304,"We argue that, despite their atypicality, losslessly compressible parameters may be highly relevant
29
to deep learning. The learning process exerts a non-random selection pressure on parameters, and
30
losslessly compressible parameters are appealing solutions due to parsimony. Moreover, losslessly
31
compressible parameters are a source of information singularities (cf. Fukumizu, 1996), highly rele-
32
vant to statistical theories of deep learning (Watanabe, 2009; Wei et al., 2022).
33"
INTRODUCTION,0.03636363636363636,"1We measure the size of a neural network for compression purposes by the number of units. Other conven-
tions are possible, such as counting the number of weights, or the description length of speciﬁc weights."
INTRODUCTION,0.04242424242424243,"Even if losslessly compressible parameters themselves are rare, their aggregate parametric neigh-
34
bourhoods have nonzero measure. These neighbourhoods have a rich structure that reaches through-
35
out the parameter space (Anonymous, 2023). The parameters in these neighbourhoods implement
36
similar functions to their losslessly compressible neighbours, so they are necessarily approximately
37
compressible. Their proximity to information singularities also has implications for local learning
38
dynamics (Amari et al., 2006; Wei et al., 2008; Cousseau et al., 2008; Amari et al., 2018).
39"
INTRODUCTION,0.048484848484848485,"In this paper, we study losslessly compressible parameters and their neighbours in the setting of
40
single-hidden-layer hyperbolic tangent networks. While this architecture is not immediately relevant
41
to modern deep learning, parts of the theory are generic to feed-forward architecture components. A
42
comprehensive investigation of this simple and concrete case is a ﬁrst step towards studying more
43
modern architectures. To this end, we offer the following theoretical contributions.
44"
INTRODUCTION,0.05454545454545454,"1. In Section 4, we give efﬁcient formal algorithms for optimal lossless compression of single-
45
hidden-layer hyperbolic tangent networks, and for computing the rank of a parameters—the
46
minimum number of hidden units required to implement the same function.
47
2. In Section 5, we deﬁne the proximate rank—the rank of the most compressible parameter
48
within a small L∞neighbourhood. We give a greedy algorithm for bounding this value.
49
3. In Section 6, we show that bounding the proximate rank below a given value (that is, de-
50
tecting proximity to parameters with a given maximum rank), is an NP-complete decision
51
problem. The proof involves a reduction from Boolean satisﬁability via a novel abstract
52
decision problem involving clustering points in the plane into small squares.
53"
INTRODUCTION,0.06060606060606061,"These results underscore the computational complexity of measuring neural network complexity:
54
we show that while lossless network compression is easy, detecting highly-compressible networks
55
near a given parameter can be very hard indeed (embedding any computational problem in NP).
56
Our contributions lay a foundation for future theoretical and empirical work detecting proximity to
57
losslessly compressible parameters in learned networks using modern architectures. In Section 7,
58
we discuss these research directions, and limitations of the lossless compressibility framework.
59"
INTRODUCTION,0.06666666666666667,"2
Related work2
60"
INTRODUCTION,0.07272727272727272,"Two neural network parameters are functionally equivalent if they implement the same function.
61
In single-hidden-layer hyperbolic tangent networks, Sussmann (1992) showed that, for almost all
62
parameters, two parameters are functionally equivalent if and only if they are related by simple oper-
63
ations of exchanging and negating the weights of hidden units. Similar operations have been found
64
for various architectures, including different nonlinearities (e.g., Albertini et al., 1993; K˚urková and
65"
INTRODUCTION,0.07878787878787878,"Kainen, 1994), multiple hidden layers (e.g., Fefferman and Markel, 1993; Fefferman, 1994; Phuong
66"
INTRODUCTION,0.08484848484848485,"and Lampert, 2020), and more complex connection graphs (Vlaˇci´c and Bölcskei, 2021, 2022).
67"
INTRODUCTION,0.09090909090909091,"Lossless compressibility requires functionally equivalent parameters in smaller architectures. In
68
all architectures where functional equivalence has been studied (cf. above), the simple operations
69
identiﬁed do not change the number of units. However, all of these studies explicitly exclude from
70
consideration certain measure zero subsets of parameters with richer functional equivalence classes.
71
The clearest example of this crucial assumption comes from Sussmann (1992), whose result holds
72
exactly for “minimal networks” (in our parlance, losslessly incompressible networks).
73"
INTRODUCTION,0.09696969696969697,"Anonymous (2023) relaxes this assumption, studying functional equivalence for non-minimal single-
74
hidden-layer hyperbolic tangent networks. Anonymous (2023) gives an algorithm for ﬁnding canon-
75
ical equivalent parameters using various opportunities for eliminating or merging redundant units.3
76
This algorithm implements optimal lossless compression as a side-effect. We give a more direct and
77
efﬁcient lossless compression algorithm using similar techniques.
78"
INTRODUCTION,0.10303030303030303,"Beyond lossless compression, there is a signiﬁcant empirical literature on approximate compress-
79
ibility and compression techniques in neural networks, including via network pruning, weight quan-
80
tisation, and student–teacher learning (or model distillation). Approximate compressibility has also
81"
INTRODUCTION,0.10909090909090909,"2We discuss related work in computational complexity throughout the paper (Section 6 and Appendix B).
3Patterns of unit redundancies have also been studied by Fukumizu and Amari (2000), Fukumizu et al.
(2019), and ¸Sim¸sek et al. (2021), though from a dual perspective of cataloguing various ways of adding hidden
units to a neural network while preserving the implemented function (lossless expansion, so to speak)."
INTRODUCTION,0.11515151515151516,"been proposed as a learning objective (see, e.g., Hinton and van Camp, 1993; Aytekin et al., 2019)
82
and used as a basis for generalisation bounds (Suzuki et al., 2020a,b). For an overview, see Cheng
83"
INTRODUCTION,0.12121212121212122,"et al. (2018, 2020) or Choudhary et al. (2020). Of particular interest is a recent empirical study of
84
network pruning from Casper et al. (2021), who, while investigating the structure of learned neu-
85
ral networks, found many instances of units with weak or correlated outputs. Casper et al. (2021)
86
found that these units could be removed without a large effect on performance, using elimination
87
and merging operations bearing a striking resemblance to those discussed by Anonymous (2023).
88"
PRELIMINARIES,0.12727272727272726,"3
Preliminaries
89"
PRELIMINARIES,0.13333333333333333,"We consider a family of fully-connected, feed-forward neural network architectures with one input
90
unit, one biased output unit, and one hidden layer of h ∈N biased hidden units with the hyperbolic
91
tangent nonlinearity tanh(z) = (ez −e−z)/(ez + e−z). The weights and biases of the network
92
are encoded in a parameter vector in the format w = (a1, b1, c1, . . . , ah, bh, ch, d) ∈Wh = R3h+1,
93
where for each hidden unit i = 1, . . . , h there is an outgoing weight ai ∈R, an incoming weight
94
bi ∈R, and a bias ci ∈R; and d ∈R is the output unit bias. Thus each parameter w ∈Wh indexes
95
a mathematical function fw : R →R such that fw(x) = d + Ph
i=1 ai tanh(bix + ci). All of our
96
results generalise to networks with multi-dimensional inputs and outputs.
97"
PRELIMINARIES,0.1393939393939394,"Two parameters w ∈Wh, w′ ∈Wh′ are functionally equivalent if fw = fw′ as functions on R (∀x ∈
98
R, fw(x) = fw′(x)). A parameter w ∈Wh is (losslessly) compressible (or non-minimal) if and
99
only if w is functionally equivalent to some w′ ∈Wh′ with fewer hidden units h′ < h (otherwise,
100
w is incompressible or minimal). Sussmann (1992) showed that a simple condition, reducibility, is
101
necessary and sufﬁcient for lossless compressibility. A parameter (a1, b1, c1, . . . , ah, bh, ch, d) ∈
102
Wh is reducible if and only if it satisﬁes any of the following reducibility conditions:
103"
PRELIMINARIES,0.14545454545454545,"(i) ai = 0 for some i, or
104
(ii) bi = 0 for some i, or
105
(iii) (bi, ci) = (bj, cj) for some i ̸= j, or
106
(iv) (bi, ci) = (−bj, −cj) for some i ̸= j.
107"
PRELIMINARIES,0.15151515151515152,"Each reducibility condition suggests a simple operation to remove a hidden unit while preserving the
108
function (Sussmann, 1992; Anonymous, 2023): (i) units with zero outgoing weight do not contribute
109
to the function; (ii) units with zero incoming weight contribute a constant that can be incorporated
110
into the output bias; and (iii), (iv) unit pairs with identical (negative) incoming weight and bias
111
contribute in proportion (since the hyperbolic tangent is odd), and can be merged into a single unit
112
with the sum (difference) of their outgoing weights.
113"
PRELIMINARIES,0.15757575757575756,"Deﬁne the uniform norm (or L∞norm) of a vector v ∈Rp as ∥v∥∞= maxp
i=1 abs(vi), the largest
114
absolute component of v. Deﬁne the uniform distance between v and u ∈Rp as ∥u −v∥∞. Given
115
a positive scalar ε ∈R+, deﬁne the closed uniform neighbourhood of v with radius ε, ¯B∞(v; ε), as
116
the set of vectors of distance at most ε from v: ¯B∞(v; ε) = { u ∈Rp : ∥u −v∥∞≤ε }.
117"
PRELIMINARIES,0.16363636363636364,"A decision problem4 is a tuple (I, J) where I is a set of instances and J ⊆I is a subset of afﬁrmative
118
instances. A solution is a deterministic algorithm that determines if any given instance i ∈I is
119
afﬁrmative (i ∈J). A reduction from one decision problem X = (I, J) to another Y = (I′, J′) is a
120
deterministic polytime algorithm implementing a mapping φ : I →I′ such that φ(i) ∈J′ ⇔i ∈J.
121
If such a reduction exists, say X is reducible5 to Y and write X →Y . Reducibility is transitive.
122"
PRELIMINARIES,0.1696969696969697,"P is the class of decision problems with polytime solutions (polynomial in the instance size). NP
123
is the class of decision problems for which a deterministic polytime algorithm can verify afﬁrmative
124
instances given a certiﬁcate. A decision problem Y is NP-hard if all problems in NP are reducible
125
to Y (∀X ∈NP, X →Y ). Y is NP-complete if Y ∈NP and Y is NP-hard. Boolean satisﬁabil-
126
ity is a well-known NP-complete decision problem (Cook, 1971; Levin, 1973; see also Garey and
127"
PRELIMINARIES,0.17575757575757575,"Johnson, 1979). NP-complete decision problems have no known polytime exact solutions.
128"
PRELIMINARIES,0.18181818181818182,"4We informally review several basic notions from computational complexity theory. Consult Garey and
Johnson (1979) for a rigorous introduction (in terms of formal languages, encodings, and Turing machines).
5Context should sufﬁce to distinguish reducibility between decision problems and of network parameters."
LOSSLESS COMPRESSION AND RANK,0.18787878787878787,"4
Lossless compression and rank
129"
LOSSLESS COMPRESSION AND RANK,0.19393939393939394,"We consider the problem of lossless neural network compression: ﬁnding, given a compressible
130
parameter, a functionally equivalent but incompressible parameter. The following algorithm solves
131
this problem by eliminating units meeting reducibility conditions (i) and (ii), and merging unit pairs
132
meeting reducibility conditions (iii) and (iv) in ways preserving functional equivalence.
133
Algorithm 4.1 (Lossless neural network compression). Given h ∈N, proceed:
134
1: procedure COMPRESS(w = (a1, b1, c1, . . . , ah, bh, ch, d) ∈Wh)
135
2:
▷Stage 1: Eliminate units with incoming weight zero (incorporate into new output bias δ) ◁
136
3:
I ←{ i ∈{1, . . . , h} : bi ̸= 0 }
137
4:
δ ←d + P
i/∈I tanh(ci) · ai
138
5:
▷Stage 2: Partition and merge remaining units by incoming weight and bias
◁
139
6:
Π1, . . . , ΠJ ←partition I by the value of sign(bi) · (bi, ci)
140
7:
for j ←1, . . . , J do
141
8:
αj ←P
i∈Πj sign(bi) · ai
142
9:
βj, γj ←sign(bmin Πj) · (bmin Πj, cmin Πj)
143
10:
end for
144
11:
▷Stage 3: Eliminate merged units with outgoing weight zero
◁
145
12:
k1, . . . , kr ←{ j ∈{1, . . . , J} : αj ̸= 0 }
146
13:
▷Construct a new parameter with the remaining merged units
◁
147
14:
return (αk1, βk1, γk1, . . . , αkr, βkr, γkr, δ) ∈Wr
148
15: end procedure
149"
LOSSLESS COMPRESSION AND RANK,0.2,"Theorem 4.1 (Algorithm 4.1 correctness). Given w ∈Wh, compute w′ = COMPRESS(w) ∈Wr.
150
(i) fw′ = fw, and (ii) w′ is incompressible.
151
Proof sketch (Full proof in Appendix A). For (i), note that units eliminated in Stage 1 contribute
152
a constant ai tanh(ci), units merged in Stage 2 have proportional contributions (tanh is odd), and
153
merged units eliminated in Stage 3 do not contribute. For (ii), by construction, w′ satisﬁes no
154
reducibility conditions, so w′ is not reducible and thus incompressible by Sussmann (1992).
♦
155"
LOSSLESS COMPRESSION AND RANK,0.20606060606060606,"We deﬁne the rank6 of a neural network parameter w ∈Wh, denoted rank(w), as the minimum num-
156
ber of hidden units required to implement fw: rank(w) = min { h′ ∈N : ∃w′ ∈Wh′; fw = fw′ }.
157
The rank is also the number of hidden units in COMPRESS(w), since Algorithm 4.1 produces an
158
incompressible parameter, which is minimal by deﬁnition. Computing the rank is therefore a trivial
159
matter of counting the units, after performing lossless compression. The following is a streamlined
160
algorithm, following Algorithm 4.1 but removing steps that don’t inﬂuence the ﬁnal count.
161
Algorithm 4.2 (Rank of a neural network parameter). Given h ∈N, proceed:
162
1: procedure RANK(w = (a1, b1, c1, . . . , ah, bh, ch, d) ∈Wh)
163
2:
▷Stage 1: Identify units with incoming weight nonzero
◁
164
3:
I ←{ i ∈{1, . . . , h} : bi ̸= 0 }
165
4:
▷Stage 2: Partition and compute outgoing weights for merged units
◁
166
5:
Π1, . . . , ΠJ ←partition I by the value of sign(bi) · (bi, ci)
167
6:
αj ←P"
LOSSLESS COMPRESSION AND RANK,0.21212121212121213,"i∈Πj sign(bi) · ai for j ←1, . . . , J
168
7:
▷Stage 3: Count merged units with outgoing weight nonzero
◁
169
8:
return |{ j ∈{1, . . . , J} : αj ̸= 0 }|
▷|S| denotes set cardinality
170
9: end procedure
171
Theorem 4.2 (Algorithm 4.2 correctness). Given w ∈Wh, rank(w) = RANK(w).
172
Proof. Let r be the number of hidden units in COMPRESS(w). Then r = rank(w) by Theorem 4.1.
173
Moreover, comparing Algorithms 4.1 and 4.2, observe RANK(w) = r.
174
Remark 4.3. Both Algorithms 4.1 and 4.2 require O(h log h) time if the partitioning step is per-
175
formed by ﬁrst sorting the units by lexicographically non-decreasing sign(bi) · (bi, ci).
176"
LOSSLESS COMPRESSION AND RANK,0.21818181818181817,"6In the multi-dimensional case, our notion of rank generalises the familiar notion from linear algebra, where
the rank of a linear transformation corresponds to the minimum number of hidden units required to implement
the transformation with an unbiased linear neural network (cf. Piziak and Odell, 1999). Unlike in the linear
case, our non-linear rank is not bound by the input and output dimensionalities."
PROXIMITY TO LOW-RANK PARAMETERS,0.22424242424242424,"5
Proximity to low-rank parameters
177"
PROXIMITY TO LOW-RANK PARAMETERS,0.23030303030303031,"Given a neural network parameter w ∈Wh and a positive radius ε ∈R+, we deﬁne the proximate
178
rank of w at radius ε, denoted prankε(w), as the rank of the lowest-rank parameter within a closed
179
uniform (L∞) neighbourhood of w with radius ε. That is,
180"
PROXIMITY TO LOW-RANK PARAMETERS,0.23636363636363636,"prankε(w) = min

rank(u) ∈N : u ∈¯B∞(w; ε)
	
."
PROXIMITY TO LOW-RANK PARAMETERS,0.24242424242424243,"The proximate rank measures the proximity of w to the set of parameters with a given rank bound,
181
that is, sufﬁciently losslessly compressible parameters.
182"
PROXIMITY TO LOW-RANK PARAMETERS,0.24848484848484848,"The following greedy algorithm computes an upper bound on the proximate rank. The algorithm
183
replaces each of the three stages of Algorithm 4.2 with a relaxed version, as follows.
184"
PROXIMITY TO LOW-RANK PARAMETERS,0.2545454545454545,"1. Instead of eliminating units with zero incoming weight, eliminate units with near zero
185
incoming weight (there is a nearby parameter where these are zero).
186
2. Instead of partitioning the remaining units by sign(bi) · (bi, ci), cluster them by nearby
187
sign(bi) · (bi, ci) (there is a nearby parameter where they have the same sign(bi) · (bi, ci)).
188
3. Instead of eliminating merged units with zero outgoing weight, eliminate merged units with
189
near zero outgoing weight (there is a nearby parameter where these are zero).
190"
PROXIMITY TO LOW-RANK PARAMETERS,0.2606060606060606,"Step (2) is non-trivial, we use a greedy approach, described separately as Algorithm 5.2.
191
Algorithm 5.1 (Greedy bound for proximate rank). Given h ∈N, proceed:
192
1: procedure BOUND(ε ∈R+, w = (a1, b1, c1, . . . , ah, bh, ch, d) ∈Wh)
193
2:
▷Stage 1: Identify units with incoming weight not near zero
◁
194
3:
I ←{ i ∈{1, . . . , h} : abs(bi) > ε }
195
4:
▷Stage 2: Compute outgoing weights for nearly-mergeable units
◁
196
5:
Π1, . . . , ΠJ ←APPROXPARTITION(ε, sign(bi) · (bi, ci) for i ∈I)
▷Algorithm 5.2
197
6:
αj ←P"
PROXIMITY TO LOW-RANK PARAMETERS,0.26666666666666666,"i∈Πj sign(bi) · ai for j ←1, . . . , J
198
7:
▷Stage 3: Count nearly-mergeable units with outgoing weight not near zero
◁
199
8:
return |{ j ∈{1, . . . , J} : abs(αj) > ε · |Πj| }|
▷|S| denotes set cardinality
200
9: end procedure
201
Algorithm 5.2 (Greedy approximate partition). Given h ∈N, proceed:
202
1: procedure APPROXPARTITION(ε ∈R+, u1, . . . , uh ∈R2)
203
2:
J ←0
204
3:
for i ←1, . . . , h do
205
4:
if for some j ∈{1, . . . , J}, ∥ui −vj∥∞≤ε then
206
5:
Πj ←Πj ∪{i}
▷If near a group-starter, join that group.
207
6:
else
208
7:
J, vJ+1, ΠJ+1 ←J + 1, ui, {i}
▷Else, start a new group with this vector.
209
8:
end if
210
9:
end for
211
10:
return Π1, . . . , ΠJ
212
11: end procedure
213"
PROXIMITY TO LOW-RANK PARAMETERS,0.2727272727272727,"Theorem 5.1 (Algorithm 5.1 correctness). For w ∈Wh and ε ∈R+, prankε(w) ≤BOUND(ε, w).
214"
PROXIMITY TO LOW-RANK PARAMETERS,0.2787878787878788,"Proof sketch (Full proof in Appendix A). Trace the algorithm to construct a parameter u ∈¯B∞(w; ε)
215
with rank(u) = BOUND(ε, w). During Stage 1, set the nearly-eliminable incoming weights to
216
zero. Use the group-starting vectors v1, . . . , vJ from Algorithm 5.2 to construct mergeable incoming
217
weights and biases during Stage 2. During Stage 3, subtract or add a fraction of the merged unit
218
outgoing weight from the outgoing weights of the original units.
♦
219
Remark 5.2. Both Algorithms 5.1 and 5.2 have worst-case runtime complexity O(h2).
220
Remark 5.3. Algorithm 5.1 does not compute the proximate rank—merely an upper bound. There
221
may exist a more efﬁcient approximate partition than the one found by Algorithm 5.2. It turns out
222
that this suboptimality is fundamental—computing a smallest approximate partition is NP-hard,
223
and can be reduced to computing the proximate rank. We formally prove this observation below.
224"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.28484848484848485,"6
Computational complexity of proximate rank
225"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.2909090909090909,"Remark 5.3 alludes to an essential difﬁculty in computing the proximate rank: grouping units with
226
similar (up to sign) incoming weight and bias pairs for merging. The following abstract decision
227
problem, Problem UPC, captures the related task of clustering points in the plane into groups with a
228
ﬁxed maximum uniform radius.7
229"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.296969696969697,"Given h source points x1, . . . , xh ∈R2, deﬁne an (r, ε)-cover, a collection of r covering points
230
y1, . . . , yr ∈R2 such that the uniform distance between each source point and its nearest covering
231
point is at most ε (that is, ∀i ∈{1, . . . , h} , ∃j ∈{1, . . . , r} , ∥xi −yj∥∞≤ε).
232
Problem UPC. Uniform point cover, or UPC, is a decision problem. The instances are tuples of the
233
form (h, r, ε, X) where h, r ∈N; ε ∈R+; and X is a list of h source points in R2. The afﬁrmative
234
instances are all tuples (h, r, ε, X) for which there exists an (r, ε)-cover of the h points in X.
235"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.30303030303030304,"Theorem 6.1. Problem UPC is NP-complete.
236"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3090909090909091,"Proof sketch (Full proof in Appendix C). The main task is to show that UPC is NP-hard (∀X ∈NP,
237
X →UPC). Since reducibility is transitive, it sufﬁces to give a reduction from the well-known NP-
238
complete problem Boolean satisﬁability (Cook, 1971; Levin, 1973). Actually, to simplify the proof,
239
we consider an NP-complete variant of Boolean satisﬁability, restricted to formulas with (i) two or
240
three literals per clause, (ii) one negative occurrence and one or two positive occurrences per literal,
241
and (iii) a planar bipartite clause–variable incidence graph.
242"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3151515151515151,"From such a formula we must construct a UPC instance, afﬁrmative if and only if the formula is
243
satisﬁable. Due to the restrictions, the bipartite clause–variable is planar with maximum degree 3,
244
and can be embedded onto an integer grid (Valiant, 1981, §IV). We divide the embedded graph into
245
unit-width tiles of ﬁnitely many types, and we replace each tile with an arrangement of source points
246
based on its type. The aggregate collection of source points mirrors the structure of the original for-
247
mula. The variable tile arrangements can be covered essentially in either of two ways, corresponding
248
to “true” and “false” in a satisfying assignment. The edge tile arrangements transfer these assign-
249
ments to the clause tiles, where the cover can only be completed if all clauses have at least one true
250
positive literal or false negative literal. Figure 1 shows one example of this construction.
♦
251"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3212121212121212,"(a)
(b)
(c)"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.32727272727272727,"variables: v1, . . . , v6"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3333333333333333,"clauses:
(¯v1 ∨¯v3 ∨v4)
(v1 ∨¯v2 ∨v5)
(v3 ∨¯v4 ∨v6)"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3393939393939394,"(v2 ∨¯v5)
(v5 ∨v6)
(v4 ∨¯v6) + − + −
+ − + − + + + − + + − + − + −
+ − + − + + + − + + − + − + −
+ − + − + + + − + + − ε"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.34545454545454546,"(d)
(e)
(f)"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3515151515151515,"Figure 1: Example of reduction from restricted Boolean satisﬁability to Problem UPC. (a) A satisﬁ-
able restricted Boolean formula. (b) The formula’s planar bipartite variable–clause invidence graph
(circles: variables, squares: clauses, edges: ± literals). (c) The graph embedded onto an integer grid.
(d) The embedding divided into unit tiles of various types. (e) The h = 68 source points aggregated
from each of the tiles. (f) Existence of a (34, 1/8)-cover of the source points (coloured points are
covering points, with uniform neighbourhoods of radius 1/8 shown). General case in Appendix C."
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3575757575757576,"7Problem UPC is reminiscent of known hard clustering problems such as planar k-means (Mahajan et al.,
2012) and vertex k-center (Hakimi, 1964; Kariv and Hakimi, 1979). Supowit (1981, §4.3.2) showed that a
Euclidean-distance version is NP-complete. Problem UPC is also related to clique partition on unit disk graphs,
which is NP-complete (Cerioli et al., 2004, 2011). We discuss these and other relations in Appendix B."
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.36363636363636365,"The following decision problem formalises the task of bounding the proximate rank, or equivalently,
252
detecting nearby low-rank parameters. It is NP-complete by reduction from Problem UPC.
253
Problem PR. Bounding proximate rank, or PR, is a decision problem. Each instance comprises a
254
number of hidden units h ∈N, a parameter w ∈Wh, a uniform radius ε ∈R+, and a maximum
255
rank r ∈N. The afﬁrmative instances are those instances where prankε(w) ≤r.
256
Theorem 6.2. Problem PR is NP-complete.
257
Proof. Since UPC is NP-complete (Theorem 6.1), it sufﬁces to show UPC →PR and PR ∈NP.
258"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3696969696969697,"(UPC →PR, the reduction): Given an instance of Problem UPC, allocate one hidden unit per source
259
point, and construct a parameter using the source point coordinates as incoming weights and biases.
260
Actually, to avoid issues with zeros and signs, ﬁrst translate the source points well into the positive
261
quadrant. Likewise, set the outgoing weights to a positive value. Figure 2 gives an example.
262"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.37575757575757573,"Formally, let h, r ∈N, ε ∈R+, and x1, . . . , xh ∈R2. In linear time construct a PR instance with h
263
hidden units, uniform radius ε, maximum rank r, and parameter w ∈Wh as follows.
264"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.38181818181818183,"1. Deﬁne xmin =
 
minh
i=1 xi,1, minh
i=1 xi,2

∈R2, containing the minimum ﬁrst and second
265
coordinates among all source points (minimising over each dimension independently).
266"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3878787878787879,"2. Deﬁne a translation T : R2 →R2 such that T(x) = x −xmin + (2ε, 2ε).
267"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.3939393939393939,"3. Translate the source points x1, . . . , xh to x′
1, . . . , x′
h where x′
i = T(xi). Note (for later)
268
that all components of the translated source points are at least 2ε by step (1).
269"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4,"4. Construct the neural network parameter w = (2ε, x′
1,1, x′
1,2, . . . , 2ε, x′
h,1, x′
h,2, 0) ∈Wh.
270
In other words, for i = 1, . . . , h, set ai = 2ε, bi = x′
i,1, and ci = x′
i,2; and set d = 0.
271"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.40606060606060607,"(UPC →PR, equivalence): It remains to show that the constructed instance of PR is afﬁrmative if
272
and only if the given instance of UPC is afﬁrmative, that is, there exists an (r, ε)-cover of the source
273
points if and only if the constructed parameter has prankε(w) ≤r.
274"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4121212121212121,"(⇒): If there is a small cover of the source points, then the hidden units can be perturbed so that
275
they match up with the (translated) covering points. Since there are few covering points, many units
276
can now be merged, so the original parameter has low proximate rank.
277"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.41818181818181815,"Formally, suppose there exists an (r, ε)-cover y1, . . . , yr. Deﬁne ρ : {1, . . . , h} →{1, . . . , r} such
278
that the nearest covering point to each source point xi is yρ(i) (breaking ties arbitrarily). Then for
279
j = 1, . . . , r, deﬁne y′
j = T(yj) where T is the translation deﬁned in step (2) of the construction.
280
Finally, deﬁne a parameter w⋆= (2ε, y′
ρ(1),1, y′
ρ(1),2, . . . , 2ε, y′
ρ(h),1, y′
ρ(h),2, 0) ∈Wh (in other
281
words, for i = 1, . . . , h, a⋆
i = 2ε, b⋆
i = y′
ρ(i),1, and c⋆
i = y′
ρ(i),2; and d⋆= 0).
282"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.42424242424242425,"Then rank(w⋆) ≤r, since there are at most r distinct incoming weight and bias pairs (namely
283
y′
1, . . . , y′
r). Moreover, ∥w −w⋆∥∞≤ε, since both parameters have the same output bias and
284
outgoing weights, and, by the deﬁning property of the cover, for i = 1, . . . , h,
285"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4303030303030303,"∥(bi, ci) −(b⋆
i , c⋆
i )∥∞=
x′
i −y′
ρ(i)

∞=
T(xi) −T(yρ(i))

∞=
xi −yρ(i)

∞≤ε."
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.43636363636363634,"Therefore prankε(w) ≤rank(w⋆) ≤r.
286 ε x1 x2 x3 x4 x5 x6 x7 x8 x9 xmin 2ε 2ε x′
i xi T"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.44242424242424244,"(bi, ci)"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4484848484848485,"bi
2ε
ci"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.45454545454545453,"(a)
(b)
(c)
(d)"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.46060606060606063,"Figure 2: Illustrative example of the parameter construction. (a) A set of source points x1, . . . , x9.
(b) Transformation T translates all points into the positive quadrant by a margin of 2ε. (c,d) The
coordinates of the transformed points become the incoming weights and biases of the parameter."
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4666666666666667,"(⇐): Conversely, since all of the weights and biases are at least 2ε, any nearby low-rank parameter
287
implies the approximate mergeability of some units. Therefore, if the parameter has low proximate
288
rank, there is a small cover of the translated points, and, in turn, of the original points.
289"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4727272727272727,"Formally, suppose prankε(w) ≤r, with w⋆∈¯B∞(w; ε) such that rank(w⋆) = r⋆≤r. In general,
290
the only ways that w⋆could have reduced rank compared to w are the following (cf. Algorithm 4.1):
291"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.47878787878787876,"1. Some incoming weight bi could be perturbed to zero, allowing its unit to be eliminated.
292"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.48484848484848486,"2. Two units i, j with (bi, ci) and (bj, cj) within 2ε could be perturbed to have identical in-
293
coming weight and bias, allowing them to be merged.
294"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.4909090909090909,"3. Two units i, j with (bi, ci) and −(bj, cj) within 2ε could be perturbed to have identically
295
negative weight and bias, again allowing them to be merged.
296"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.49696969696969695,"4. Some group of m ≥1 units, merged through the above options, with total outgoing weight
297
within mε of zero, could have their outgoing weights perturbed to make the total zero.
298"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.503030303030303,"By construction, all ai, bi, ci ≥2ε > 0, immediately ruling out (1) and (3). Option (4) is also ruled
299
out because any such total outgoing weight is 2mε > mε. This leaves option (2) alone responsible.
300
Thus, there are exactly r⋆distinct incoming weight and bias pairs among the units of w⋆. Denote
301
these pairs y′
1, . . . , y′
r⋆—they constitute an (r⋆, ε)-cover of the incoming weight and bias vectors of
302
w, x′
1, . . . , x′
h (as w⋆∈¯B∞(w; ε)). Finally, invert T to produce an (r⋆, ε)-cover of x1, . . . , xh, and
303
add r −r⋆arbitrary covering points to extend this to the desired (r, ε)-cover.
304"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.509090909090909,"(PR ∈NP): We must show that an afﬁrmative instance of PR can be veriﬁed in polynomial time,
305
given a certiﬁcate. Consider an instance h, r ∈N, ε ∈R+, and w = (a1, b1, c1, . . . , ah, bh, ch, d) ∈
306
Wh. Use as a certiﬁcate a partition8 Π1, . . . , ΠJ of { i ∈{1, . . . , h} : abs(bi) > ε }, such that
307
(1) for each Πj, for each i, k ∈Πj, ∥sign(bi) · (bi, ci) −sign(bk) · (bk, ck)∥∞≤2ε; and (2) at
308
most r of the Πj satisfy P"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5151515151515151,"i∈Πj sign(bi) · ai > ε · |Πj|. The validity of such a certiﬁcate can be
309
veriﬁed in polynomial time by checking each of these conditions directly.
310"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5212121212121212,"It remains to show that such a certiﬁcate exists if and only if the instance is afﬁrmative.
If
311
prankε(w) ≤r, then there exists a parameter w⋆∈¯B∞(w; ε) with rank(w⋆) ≤r. The partition
312
computed from Stage 2 of COMPRESS(w⋆) satisﬁes the required properties for w ∈¯B∞(w⋆; ε).
313"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5272727272727272,"Conversely, given such a partition, for each Πj, deﬁne vj ∈R2 as the centroid of the bounding
314
rectangle of the set of points { sign(bi) · (bi, ci) : i ∈Πj }, that is,
315"
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5333333333333333,vj = 1 2
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5393939393939394,"
max
i∈Πj abs(bi) + min
i∈Πj abs(bi), max
i∈Πj sign(bi) · ci + min
i∈Πj sign(bi) · ci 
."
COMPUTATIONAL COMPLEXITY OF PROXIMATE RANK,0.5454545454545454,"All of the points within these bounding rectangles are at most uniform distance ε from their centroids.
316
To construct a nearby low-rank parameter, follow the proof of Theorem 5.1 using Π1, . . . , ΠJ and
317
v1, . . . , vJ in place of their namesakes from Algorithms 5.1 and 5.2. Thus prankε(w) ≤r.
318"
DISCUSSION,0.5515151515151515,"7
Discussion
319"
DISCUSSION,0.5575757575757576,"In this paper, we have studied losslessly compressible neural network parameters, measuring the
320
size of a network by the number of hidden units. Losslessly compressible parameters comprise a
321
measure zero subset of the parameter space, but this is a rich subset that stretches throughout the
322
entire parameter space (Anonymous, 2023). Moreover, the neighbourhood of this region has nonzero
323
measure and comprises approximately compressible parameters.
324"
DISCUSSION,0.5636363636363636,"It’s possible that part of the empirical success of deep learning can be explained by the proximity
325
of learned neural networks to losslessly compressible parameters. Our theoretical and algorithmic
326
contributions, namely the notions of rank and proximate rank and their associated algorithms, serve
327
as a foundation for future research in this direction. In this section, we outline promising next steps
328
for future work and discuss limitations of our approach.
329"
DISCUSSION,0.5696969696969697,"8It would seem simpler to use a nearby low-rank parameter itself as the certiﬁcate, which exists exactly in
afﬁrmative cases by deﬁnition of the proximate rank. Unfortunately, an arbitrary nearby low-rank parameter is
unsuitable because the parameter could have unbounded description length, leading to the certiﬁcate not being
veriﬁable in polynomial time. By using instead this partition we essentially establish that in such cases there is
always also a nearby low-rank parameter with polynomial description length."
DISCUSSION,0.5757575757575758,"Limitations of the lossless compressibility framework.
Section 4 offers efﬁcient algorithms for
330
optimal lossless compression and computing the rank of neural network parameters. However, the
331
rank is an idealised notion, serving as a basis for the theory of proximate rank. One would not
332
expect to ﬁnd compressible parameters in practice, since numerical imprecision is likely to prevent
333
the observation of identically equal, negative, or zero weights in practice. Moreover, the number
334
of units is not the only measure of a network’s description length. For example, the sparsity and
335
precision of weights may be relevant axes of parsimony in neural network modelling.
336"
DISCUSSION,0.5818181818181818,"Returning to the deep learning context—there is a gap between lossless compressibility and phe-
337
nomena of approximate compressibility. In practical applications and empirical investigations, the
338
neural networks in question are only approximately preserved the function, and moreover the degree
339
of approximation may deteriorate for unlikely inputs. Considering the neighbourhoods of losslessly
340
compressible parameters helps bridge this gap, but there are approximately compressible neural
341
networks beyond the proximity of losslessly compressible parameters, which are not accounted for
342
in this approach. More broadly, a comprehensive account of neural network compressibility must
343
consider architectural redundancy as well as redundancy in the parameter.
344"
DISCUSSION,0.5878787878787879,"Tractable detection of proximity to low-rank parameters.
An important direction for future
345
work is to empirically investigate the proximity of low-rank neural networks to the neural networks
346
that arise during the course of successful deep learning. Unfortunately, our main result (Theo-
347
rem 6.2) suggests that detecting such proximity is computationally intractable in general, due to
348
the complex structure of the neighbourhoods of low-rank parameters.
349"
DISCUSSION,0.593939393939394,"There is still hope for empirically investigating the proximate rank of learned networks. Firstly,
350
NP-completeness does not preclude efﬁcient approximation algorithms, and approximations are
351
still useful as a one-sided test of proximity to low-rank parameters. Algorithm 5.1 provides a naive
352
approximation, with room for improvement in future work. Secondly, Theorem 6.2 is a worst-case
353
analysis—Section 6 essentially constructs pathological parameters poised between nearby low-rank
354
regions such that choosing the optimal direction of perturbation involves solving (a hard instance of)
355
Boolean satisﬁability. Such instances might be rare in practice (cf. the related problem of k-means
356
clustering; Daniely et al., 2012). As an extreme example, detecting proximity to merely compress-
357
ible parameters (r = h −1) permits a polytime solution based on the reducibility conditions.
358"
DISCUSSION,0.6,"Towards lossless compressibility theory in modern architectures.
We have studied lossless
359
compressibility in the simple, concrete setting of single-hidden-layer hyperbolic tangent networks.
360
Several elements of our approach will be useful for future work on more modern architectures. At
361
the core of our analysis are structural redundancies arising from zero, constant, or proportional units
362
(cf. reducibility conditions (i)–(iii)). In particular, the computational difﬁculty of bounding the prox-
363
imate rank is due to the approximate merging embedding a hard clustering problem. These features
364
are not due to the speciﬁcs of the hyperbolic tangent, rather they are generic features of any layer in
365
a feed-forward network component.
366"
DISCUSSION,0.6060606060606061,"In more complex architectures there will be additional or similar opportunities for compression.
367
While unit negation symmetries are characteristic of odd nonlinearities, other nonlinearities will
368
exhibit their own afﬁne symmetries which can be handled analogously. Further redundancies will
369
arise from interactions between layers or from specialised computational structures.
370"
CONCLUSION,0.6121212121212121,"8
Conclusion
371"
CONCLUSION,0.6181818181818182,"Towards a better understanding of complexity and compressibility in learned neural networks, we
372
have developed a theoretical and algorithmic framework for lossless compressibility in single-
373
hidden-layer hyperbolic tangent networks. The rank is a measure of a parameter’s lossless com-
374
pressibility. Section 4 offers efﬁcient algorithms for performing optimal lossless compression and
375
computing the rank. The proximate rank is a measure of proximity to low-rank parameters. Sec-
376
tion 5 offers an efﬁcient algorithm for approximately bounding the proximate rank. In Section 6, we
377
show that optimally bounding the proximate rank, or, equivalently, detecting proximity to low-rank
378
parameters, is NP-complete, by reduction from Boolean satisﬁability via a novel hard clustering
379
problem. These results underscore the complexity of losslessly compressible regions of the param-
380
eter space and lay a foundation for future theoretical and empirical work on detecting losslessly
381
compressibile parameters arising while learning with more complex architectures.
382"
REFERENCES,0.6242424242424243,"References
383"
REFERENCES,0.6303030303030303,"Francesca Albertini, Eduardo D. Sontag, and Vincent Maillot. Uniqueness of weights for neural
384
networks. In Artiﬁcial Neural Networks for Speech and Vision, pages 113–125. Chapman & Hall,
385
London, 1993. Proceedings of a workshop held at Rutgers University in 1992. Access via Eduardo
386"
REFERENCES,0.6363636363636364,"D. Sontag. Cited on page 2.
387"
REFERENCES,0.6424242424242425,"Shun-ichi Amari, Hyeyoung Park, and Tomoko Ozeki. Singularities affect dynamics of learning in
388
neuromanifolds. Neural Computation, 18(5):1007–1065, 2006. Access via Crossref. Cited on
389
page 2.
390"
REFERENCES,0.6484848484848484,"Shun-ichi Amari, Tomoko Ozeki, Ryo Karakida, Yuki Yoshida, and Masato Okada. Dynamics of
391
learning in MLP: Natural gradient and singularity revisited. Neural Computation, 30(1):1–33,
392
2018. Access via Crossref. Cited on page 2.
393"
REFERENCES,0.6545454545454545,"Anonymous. Functional equivalence and path connectivity of reducible hyperbolic tangent networks.
394
2023. Anonymised article included with supplementary material. Cited on pages 1, 2, 3, 8, and 26.
395"
REFERENCES,0.6606060606060606,"Caglar Aytekin, Francesco Cricri, and Emre Aksu. Compressibility loss for neural network weights.
396
2019. Preprint arXiv:1905.01044 [cs.LG]. Cited on page 3.
397"
REFERENCES,0.6666666666666666,"Piotr Berman, Alex D. Scott, and Marek Karpinski.
Approximation hardness and satisﬁability
398
of bounded occurrence instances of SAT. Technical Report IHES/M/03/25, Institut des Hautes
399
Études Scientiﬁques [Institute of Advanced Scientiﬁc Studies], 2003. Access via CERN. Cited
400
on page 18.
401"
REFERENCES,0.6727272727272727,"Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceed-
402
ings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data
403
Mining, pages 535–541. ACM, 2006. Access via Crossref. Cited on page 1.
404"
REFERENCES,0.6787878787878788,"Stephen Casper, Xavier Boix, Vanessa D’Amario, Ling Guo, Martin Schrimpf, Kasper Vinken, and
405
Gabriel Kreiman. Frivolous units: Wider networks are not really that wide. In Proceedings of
406
the Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, volume 8, pages 6921–6929. AAAI
407
Press, 2021. Access via Crossref. Cited on page 3.
408"
REFERENCES,0.6848484848484848,"Márcia R. Cerioli, Luerbio Faria, Talita O. Ferreira, and Fábio Protti. On minimum clique partition
409
and maximum independent set on unit disk graphs and penny graphs: Complexity and approxi-
410
mation. Electronic Notes in Discrete Mathematics, 18:73–79, 2004. Access via Crossref. Cited
411
on pages 6, 15, 18, and 26.
412"
REFERENCES,0.6909090909090909,"Márcia R. Cerioli, Luerbio Faria, Talita O. Ferreira, and Fábio Protti. A note on maximum indepen-
413
dent sets and minimum clique partitions in unit disk graphs and penny graphs: Complexity and
414
approximation. RAIRO: Theoretical Informatics and Applications, 45(3):331–346, 2011. Access
415
via Crossref. Cited on pages 6, 15, 18, and 26.
416"
REFERENCES,0.696969696969697,"An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On the geometry of feedforward neural
417
network error surfaces. Neural Computation, 5(6):910–927, 1993. Access via Crossref. Cited on
418
page 1.
419"
REFERENCES,0.703030303030303,"Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for deep
420
neural networks: The principles, progress, and challenges. IEEE Signal Processing Magazine,
421
35(1):126–136, 2018. Access via Crossref. Cited on pages 3 and 10.
422"
REFERENCES,0.7090909090909091,"Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
423
for deep neural networks. 2020. Preprint arXiv:1710.09282v9 [cs.LG]. Updated version of Cheng
424"
REFERENCES,0.7151515151515152,"et al. (2018). Cited on page 3.
425"
REFERENCES,0.7212121212121212,"Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagannathan Sarangapani. A comprehen-
426
sive survey on model compression and acceleration. Artiﬁcial Intelligence Review, 53(7):5113–
427
5155, 2020. Access via Crossref. Cited on page 3.
428"
REFERENCES,0.7272727272727273,"Brent N. Clark, Charles J. Colbourn, and David S. Johnson. Unit disk graphs. Discrete Mathematics,
429
86(1-3):165–177, 1990. Access via Crossref. Cited on page 15.
430"
REFERENCES,0.7333333333333333,"Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third
431
Annual ACM Symposium on Theory of Computing, pages 151–158. ACM, 1971.
Access via
432"
REFERENCES,0.7393939393939394,"Crossref. Cited on pages 3, 6, 17, and 18.
433"
REFERENCES,0.7454545454545455,"Florent Cousseau, Tomoko Ozeki, and Shun-ichi Amari. Dynamics of learning in multilayer percep-
434
trons near singularities. IEEE Transactions on Neural Networks, 19(8):1313–1328, 2008. Access
435
via Crossref. Cited on page 2.
436"
REFERENCES,0.7515151515151515,"Amit Daniely, Nati Linial, and Michael Saks. Clustering is difﬁcult only when it does not matter.
437
2012. Preprint arXiv:1205.4891 [cs.LG]. Cited on page 9.
438"
REFERENCES,0.7575757575757576,"Charles Fefferman. Reconstructing a neural net from its output. Revista Matemática Iberoameri-
439
cana, 10(3):507–555, 1994. Access via Crossref. Cited on pages 1 and 2.
440"
REFERENCES,0.7636363636363637,"Charles Fefferman and Scott Markel. Recovering a feed-forward net from its output. In Advances
441
in Neural Information Processing Systems 6, pages 335–342. Morgan Kaufmann, 1993. Access
442
via NeurIPS. Cited on page 2.
443"
REFERENCES,0.7696969696969697,"Kenji Fukumizu. A regularity condition of the information matrix of a multilayer perceptron net-
444
work. Neural Networks, 9(5):871–879, 1996. Access via Crossref. Cited on pages 1 and 26.
445"
REFERENCES,0.7757575757575758,"Kenji Fukumizu and Shun-ichi Amari.
Local minima and plateaus in hierarchical structures of
446
multilayer perceptrons. Neural Networks, 13(3):317–327, 2000. Access via Crossref. Cited on
447
page 2.
448"
REFERENCES,0.7818181818181819,"Kenji Fukumizu, Shoichiro Yamaguchi, Yoh-ichi Mototake, and Mirai Tanaka. Semi-ﬂat minima
449
and saddle points by embedding neural networks to overparameterization. In Advances in Neural
450
Information Processing Systems 32, pages 13868–13876. Curran Associates, 2019. Access via
451"
REFERENCES,0.7878787878787878,"NeurIPS. Cited on page 2.
452"
REFERENCES,0.793939393939394,"Jesus Garcia-Diaz, Jairo Sanchez-Hernandez, Ricardo Menchaca-Mendez, and Rolando Menchaca-
453
Mendez. When a worse approximation factor gives better performance: A 3-approximation algo-
454
rithm for the vertex k-center problem. Journal of Heuristics, 23(5):349–366, 2017. Access via
455"
REFERENCES,0.8,"Crossref. Cited on page 15.
456"
REFERENCES,0.806060606060606,"Michael R. Garey and David S. Johnson. Computers and Intractability: A Guide to the Theory of
457
NP-Completeness. W. H. Freeman and Company, 1979. Cited on pages 3 and 26.
458"
REFERENCES,0.8121212121212121,"S. L. Hakimi. Optimum locations of switching centers and the absolute centers and medians of a
459
graph. Operations Research, 12(3):450–459, 1964. Access via Crossref. Cited on pages 6 and 15.
460"
REFERENCES,0.8181818181818182,"Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the de-
461
scription length of the weights. In Proceedings of the Sixth Annual Conference on Computational
462
Learning Theory, pages 5–13. ACM, 1993. Access via Crossref. Cited on page 3.
463"
REFERENCES,0.8242424242424242,"Geoffrey E. Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. Pre-
464
sented at Twenty-eighth Conference on Neural Information Processing Systems, Deep Learning
465
workshop, 2014. Preprint arXiv:1503.02531 [stat.ML]. Cited on page 1.
466"
REFERENCES,0.8303030303030303,"Klaus Jansen and Haiko Müller. The minimum broadcast time problem for several processor net-
467
works. Theoretical Computer Science, 147(1-2):69–85, 1995. Access via Crossref. Cited on
468
page 18.
469"
REFERENCES,0.8363636363636363,"O. Kariv and S. L. Hakimi. An algorithmic approach to network location problems. I: the p-centers.
470
SIAM Journal on Applied Mathematics, 37(3):513–538, 1979. Access via Crossref. Cited on
471
pages 6 and 15.
472"
REFERENCES,0.8424242424242424,"Richard M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Compu-
473
tations, pages 85–103. Springer, 1972. Access via Crossref. Cited on pages 15 and 26.
474"
REFERENCES,0.8484848484848485,"Vˇera K˚urková and Paul C. Kainen. Functionally equivalent feedforward neural networks. Neural
475
Computation, 6(3):543–558, 1994. Access via Crossref. Cited on page 2.
476"
REFERENCES,0.8545454545454545,"Leonid A. Levin. Universal sequential search problems. Problemy Peredachi Informatsii [Problems
477
of Information Transmission], 9(3):115–116, 1973. In Russian. Translated into English in ?. Cited
478
on pages 3, 6, and 17.
479"
REFERENCES,0.8606060606060606,"David Lichtenstein. Planar formulae and their uses. SIAM Journal on Computing, 11(2):329–343,
480
1982. Access via Crossref. Cited on page 18.
481"
REFERENCES,0.8666666666666667,"Yanpei Liu, Aurora Morgana, and Bruno Simeone. A linear algorithm for 2-bend embeddings of
482
planar graphs in the two-dimensional grid. Discrete Applied Mathematics, 81(1-3):69–91, 1998.
483
Access via Crossref. Cited on pages 19 and 24.
484"
REFERENCES,0.8727272727272727,"Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan. The planar k-means problem is NP-
485
hard. Theoretical Computer Science, 442:13–21, 2012. Access via Crossref. Cited on pages 6
486
and 15.
487"
REFERENCES,0.8787878787878788,"Mary Phuong and Christoph H. Lampert. Functional vs. parametric equivalence of ReLU networks.
488
In 8th International Conference on Learning Representations. OpenReview, 2020. Access via
489"
REFERENCES,0.8848484848484849,"OpenReview. Cited on pages 1 and 2.
490"
REFERENCES,0.8909090909090909,"R. Piziak and P. L. Odell. Full rank factorization of matrices. Mathematics Magazine, 72(3):193–
491
201, 1999. Access via Crossref. Cited on page 4.
492"
REFERENCES,0.896969696969697,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version
493
of BERT: Smaller, faster, cheaper and lighter. Presented at the Fifth Workshop on Energy Efﬁcient
494
Machine Learning and Cognitive Computing, 2019. Preprint arXiv:1910.01108 [cs.CL]. Cited on
495
page 1.
496"
REFERENCES,0.9030303030303031,"Berﬁn ¸Sim¸sek, François Ged, Arthur Jacot, Francesco Spadaro, Clément Hongler, Wulfram Gerst-
497
ner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks:
498
Symmetries and invariances. In Proceedings of the 38th International Conference on Machine
499
Learning, pages 9722–9732. PMLR, 2021. Access via PMLR. Cited on page 2.
500"
REFERENCES,0.9090909090909091,"Kenneth J. Supowit. Topics in Computational Geometry. Ph.D. thesis, University of Illinois at
501
Urbana-Champaign, 1981. Access via ProQuest. Cited on pages 6 and 15.
502"
REFERENCES,0.9151515151515152,"Héctor J. Sussmann. Uniqueness of the weights for minimal feedforward nets with a given input-
503
output map. Neural Networks, 5(4):589–593, 1992. Access via Crossref. Cited on pages 1, 2, 3,
504"
REFERENCES,0.9212121212121213,"4, and 26.
505"
REFERENCES,0.9272727272727272,"Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi, Kotaro Ito, Tokuma Wachi, So Hi-
506
rai, Masatoshi Yukishima, and Tomoaki Nishimura. Spectral pruning: Compressing deep neural
507
networks via spectral analysis and its generalization error. In Proceedings of the Twenty-Ninth In-
508
ternational Joint Conference on Artiﬁcial Intelligence, pages 2839–2846. IJCAI, 2020a. Access
509
via Crossref. Cited on page 3.
510"
REFERENCES,0.9333333333333333,"Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura. Compression based bound for non-compressed
511
network: Uniﬁed generalization error analysis of large compressible deep neural network. In 8th
512
International Conference on Learning Representations. OpenReview, 2020b. Access via Open-
513"
REFERENCES,0.9393939393939394,"Review. Cited on page 3.
514"
REFERENCES,0.9454545454545454,"Craig A. Tovey. A simpliﬁed NP-complete satisﬁability problem. Discrete Applied Mathematics,
515
8(1):85–89, 1984. Access via Crossref. Cited on page 18.
516"
REFERENCES,0.9515151515151515,"Leslie G. Valiant. Universality considerations in VLSI circuits. IEEE Transactions on Computers,
517
100(2):135–140, 1981. Access via Crossref. Cited on pages 6 and 19.
518"
REFERENCES,0.9575757575757575,"Verner Vlaˇci´c and Helmut Bölcskei. Afﬁne symmetries and neural network identiﬁability. Advances
519
in Mathematics, 376:107485, 2021. Access via Crossref. Cited on page 2.
520"
REFERENCES,0.9636363636363636,"Verner Vlaˇci´c and Helmut Bölcskei. Neural network identiﬁability for a family of sigmoidal non-
521
linearities. Constructive Approximation, 55(1):173–224, 2022. Access via Crossref. Cited on
522
page 2.
523"
REFERENCES,0.9696969696969697,"Sumio Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University
524
Press, 2009. Cited on page 1.
525"
REFERENCES,0.9757575757575757,"Haikun Wei, Jun Zhang, Florent Cousseau, Tomoko Ozeki, and Shun-ichi Amari. Dynamics of learn-
526
ing near singularities in layered networks. Neural Computation, 20(3):813–843, 2008. Access
527
via Crossref. Cited on page 2.
528"
REFERENCES,0.9818181818181818,"Susan Wei, Daniel Murfet, Mingming Gong, Hui Li, Jesse Gell-Redman, and Thomas Quella. Deep
529
learning is singular, and that’s good. IEEE Transactions on Neural Networks and Learning Sys-
530
tems, 2022. Access via Crossref. To appear in an upcoming volume. Cited on page 1.
531"
REFERENCES,0.9878787878787879,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
532
deep learning requires rethinking generalization. In 5th International Conference on Learning
533
Representations. OpenReview, 2017. Access via OpenReview. Cited on pages 1 and 13.
534"
REFERENCES,0.9939393939393939,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
535
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
536
115, 2021. Access via Crossref. Republication of Zhang et al. (2017). Cited on page 1.
537"
