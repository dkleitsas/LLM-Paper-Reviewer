Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020833333333333333,"Face recognition (FR) has been applied to nearly every aspect of daily life, but
1"
ABSTRACT,0.004166666666666667,"it is always accompanied by the underlying risk of leaking private information.
2"
ABSTRACT,0.00625,"At present, almost all attack models against FR rely heavily on the presence of
3"
ABSTRACT,0.008333333333333333,"a classification layer. However, in practice, the FR model can obtain complex
4"
ABSTRACT,0.010416666666666666,"feature embedding of the input via the model backbone, and then compare it
5"
ABSTRACT,0.0125,"with the target for inference, which does not explicitly involve the outputs of the
6"
ABSTRACT,0.014583333333333334,"classification layer adopting logit or other losses. In this work, we advocate a
7"
ABSTRACT,0.016666666666666666,"novel inference attack composed of two stages for practical FR models without a
8"
ABSTRACT,0.01875,"classification layer. The first stage is the membership inference attack. Specifically,
9"
ABSTRACT,0.020833333333333332,"We analyze the distances between the intermediate features and batch normalization
10"
ABSTRACT,0.022916666666666665,"(BN) parameters. The results indicate that this distance is a critical metric for
11"
ABSTRACT,0.025,"membership inference. We thus design a simple but effective attack model that can
12"
ABSTRACT,0.027083333333333334,"determine whether a face image is from the training data set or not. The second
13"
ABSTRACT,0.029166666666666667,"stage is the model inversion attack, where sensitive private data is reconstructed
14"
ABSTRACT,0.03125,"using a pre-trained generative adversarial network (GAN) guided by the attack
15"
ABSTRACT,0.03333333333333333,"model in the first stage. To the best of our knowledge, the proposed attack model is
16"
ABSTRACT,0.035416666666666666,"the very first in the literature developed for FR models without a classification layer.
17"
ABSTRACT,0.0375,"We illustrate the application of the proposed attack model in the establishment of
18"
ABSTRACT,0.03958333333333333,"privacy-preserving FR techniques.
19"
INTRODUCTION,0.041666666666666664,"1
Introduction
20"
INTRODUCTION,0.04375,"Face recognition (FR) [5, 31, 18, 21, 13] technology has improved steadily over the past few years.
21"
INTRODUCTION,0.04583333333333333,"It has been widely applied in a large number of personal or commercial scenarios for enhancing
22"
INTRODUCTION,0.04791666666666667,"user experience. Recent studies [3, 27, 36] indicated that existing FR models can remember the
23"
INTRODUCTION,0.05,"information from the training data, making them vulnerable to some privacy attacks such as model
24"
INTRODUCTION,0.052083333333333336,"extraction attacks [29], model inversion attacks [6], attribute inference attacks [7] (also known as
25"
INTRODUCTION,0.05416666666666667,"property inference attacks) and membership inference attacks [26]. As a result, malicious attackers
26"
INTRODUCTION,0.05625,"may be able to obtain users‚Äô private information through the use of certain attacks, which could cause
27"
INTRODUCTION,0.058333333333333334,"significant damage. In order to facilitate the development of privacy-preserving FR methods, it is
28"
INTRODUCTION,0.06041666666666667,"essential to evaluate the leakage of the private information quantitatively. This motivates this study
29"
INTRODUCTION,0.0625,"which leads to the establishment of novel inference attacks to quantify such privacy leakage.
30"
INTRODUCTION,0.06458333333333334,"Inference attacks on machine learning algorithms can be roughly categorized into membership
31"
INTRODUCTION,0.06666666666666667,"inference attacks and model inversion attacks [24]. We shall focus on both types of attacks against
32"
INTRODUCTION,0.06875,"FR models. The goal of a membership inference attack is to infer whether a record is included in
33"
INTRODUCTION,0.07083333333333333,"the training data set or not, which is usually formulated as a binary classification problem. Model
34"
INTRODUCTION,0.07291666666666667,"inversion attacks attempt to recover the input of the target model. For example, some inversion attacks
35"
INTRODUCTION,0.075,"can recover the identity information of the training data from the FR model.
36"
INTRODUCTION,0.07708333333333334,"Backbone
Head"
INTRODUCTION,0.07916666666666666,Classifier
INTRODUCTION,0.08125,Logits Label
INTRODUCTION,0.08333333333333333,Similarity
INTRODUCTION,0.08541666666666667,metric Train
INTRODUCTION,0.0875,Embedding
INTRODUCTION,0.08958333333333333,"Inference
‚àö √ó"
INTRODUCTION,0.09166666666666666,Traditional attack target
INTRODUCTION,0.09375,Our attack target
INTRODUCTION,0.09583333333333334,"Figure 1: The schematic diagram of the training and inference process of the FR model. Our attack
target is the backbone, while traditional attacks focus on the output of the classification layer which
can be removed in the generalized FR process."
INTRODUCTION,0.09791666666666667,"At present, both types of attacks against FR usually rely on the presence of a classification layer. In
37"
INTRODUCTION,0.1,"other words, the performance of the attack and the corresponding defense algorithms [33] is heavily
38"
INTRODUCTION,0.10208333333333333,"dependent on utilizing the output from logits in the classification layer or the labels themselves
39"
INTRODUCTION,0.10416666666666667,"[4]. Nevertheless, FR has completely different training and inference paradigms, as shown in Fig.1.
40"
INTRODUCTION,0.10625,"Existing face recognition techniques do include a classifier after the feature extraction network
41"
INTRODUCTION,0.10833333333333334,"(backbone) in training, but the classifier is not used during inference. Instead, they extract feature
42"
INTRODUCTION,0.11041666666666666,"embeddings from the image pair using the trained backbone, then calculate their similarity based
43"
INTRODUCTION,0.1125,"on certain metrics to determine whether the two images are from the same person to achieve FR.
44"
INTRODUCTION,0.11458333333333333,"Therefore, in a realistic setup, the attacker can only acquire the feature embedding of the query
45"
INTRODUCTION,0.11666666666666667,"image and has no access to the discarded classification layer. The aforementioned difference in the
46"
INTRODUCTION,0.11875,"paradigms of training and inference for FR brings significant challenges to the current attack methods,
47"
INTRODUCTION,0.12083333333333333,"because there is evidence [8] indicating that compared with logits and losses, the feature embedding,
48"
INTRODUCTION,0.12291666666666666,"as a more general representation, contains less information about the training data. Moreover, the
49"
INTRODUCTION,0.125,"training stage of an FR system is essentially a closed-set classification problem, while the inference
50"
INTRODUCTION,0.12708333333333333,"phase becomes a more complicated open-set problem. In summary, performing inference attacks
51"
INTRODUCTION,0.12916666666666668,"against FR without exploiting the classification layer is non-trivial but practically important.
52"
INTRODUCTION,0.13125,"In this paper, we shall propose a novel two-stage attack algorithm, and its diagram is shown in Fig.2.
53"
INTRODUCTION,0.13333333333333333,"The first stage is the membership inference attack. Compared to previous attack methods that focus on
54"
INTRODUCTION,0.13541666666666666,"the classification layer of the target model, we explore the inherent relationship between the backbone
55"
INTRODUCTION,0.1375,"output of the target model and the training data set. Specifically, we first analyze the distribution of
56"
INTRODUCTION,0.13958333333333334,"the distances between the intermediate features of the member/non-member samples and statistics
57"
INTRODUCTION,0.14166666666666666,"stored in the Batch Normalization layer in the trained backbone, and found it critical for membership
58"
INTRODUCTION,0.14375,"inference. We then design an attack model to determine whether a sample belongs to the training data
59"
INTRODUCTION,0.14583333333333334,"set or not, through utilizing this distance distribution. We conduct experiments under different levels
60"
INTRODUCTION,0.14791666666666667,"of prior knowledge including partial access to training data set and no access to training data set. In
61"
INTRODUCTION,0.15,"both cases, the proposed membership attack is efficient and can provide state-of-the-art performance.
62"
INTRODUCTION,0.15208333333333332,"The second stage is the model inversion attack. Previous model inversion attacks against FR such as
63"
INTRODUCTION,0.15416666666666667,"those from [28, 1, 17] are heavily dependent on the classifier of the target model. They often guide
64"
INTRODUCTION,0.15625,"the optimization of the attack network using the logit of the classifier‚Äôs output or some well-designed
65"
INTRODUCTION,0.15833333333333333,"loss functions based on the logit. For the FR models in consideration, none of these approaches are
66"
INTRODUCTION,0.16041666666666668,"applicable. We thus put forward a novel inversion attack approach, where our attack model in the first
67"
INTRODUCTION,0.1625,"stage guides the synthesis network, StyleGAN, to optimize the latent code such that the synthesized
68"
INTRODUCTION,0.16458333333333333,"face becomes close to a member sample in the training data set as much as possible. Specifically, we
69"
INTRODUCTION,0.16666666666666666,"adopt the PPA [28] paradigm, preferring to sample a random batch of samples, filtering out these
70"
INTRODUCTION,0.16875,"samples that are more likely to be considered members of the training data set by the first-stage attack
71"
INTRODUCTION,0.17083333333333334,Conv2d
INTRODUCTION,0.17291666666666666,Classifier
INTRODUCTION,0.175,Batch Normalization
INTRODUCTION,0.17708333333333334,"Mean
Var"
INTRODUCTION,0.17916666666666667,Batch Normalization
INTRODUCTION,0.18125,"Mean
Var"
INTRODUCTION,0.18333333333333332,Backbone
INTRODUCTION,0.18541666666666667,Distance
INTRODUCTION,0.1875,Batch Normalization
INTRODUCTION,0.18958333333333333,"Mean
Var"
INTRODUCTION,0.19166666666666668,"Distance
Distance"
INTRODUCTION,0.19375,Attack model
INTRODUCTION,0.19583333333333333,"Member/Non-member
z ‚ààùëÅ(0,1)"
INTRODUCTION,0.19791666666666666,Generator
INTRODUCTION,0.2,Mapping
INTRODUCTION,0.20208333333333334,network ùë§‚ààùí≤
INTRODUCTION,0.20416666666666666,Synthesis
INTRODUCTION,0.20625,network
INTRODUCTION,0.20833333333333334,"Stage 1
Stage 2"
INTRODUCTION,0.21041666666666667,"Figure 2: The diagram of the proposed two-stage inference attack against FR models without
classification layers. The first stage is the membership inference attack. The attack model utilizes
the parameters from the Batch Normalization layers to determine whether a sample belongs to the
training data set or not. The second stage is the model inversion attack. StyleGAN is adopted to
synthesize images and optimize the results from the output of the first-stage attack."
INTRODUCTION,0.2125,"model, and then further optimizing the remaining samples to make them better fit the membership
72"
INTRODUCTION,0.21458333333333332,"distribution. Experiments demonstrate that the proposed attack algorithm against FR can recover
73"
INTRODUCTION,0.21666666666666667,"some sensitive private information in the training data set.
74"
INTRODUCTION,0.21875,"Our contributions can be summarized as follows:
75"
INTRODUCTION,0.22083333333333333,"‚Ä¢ We extend existing inference attacks against FR models by relaxing the assumption of the
76"
INTRODUCTION,0.22291666666666668,"existence of a classification layer in the inference process. The proposed attack method can
77"
INTRODUCTION,0.225,"thus make the defense techniques in literature targeting the FR classification layer subject to
78"
INTRODUCTION,0.22708333333333333,"the risk of private information leakage again.
79"
INTRODUCTION,0.22916666666666666,"‚Ä¢ We analyze the distance between the intermediate features of member/non-member samples
80"
INTRODUCTION,0.23125,"of the training data set and the parameters of the BN layers. We design a simple and effective
81"
INTRODUCTION,0.23333333333333334,"attack model accordingly, which is applicable to the trained FR models without classification
82"
INTRODUCTION,0.23541666666666666,"layers. Experiments demonstrate that our method outperforms the state-of-the-art methods
83"
INTRODUCTION,0.2375,"with the same prior knowledge.
84"
INTRODUCTION,0.23958333333333334,"‚Ä¢ We propose a model inversion attack algorithm that does not require the classification layer.
85"
INTRODUCTION,0.24166666666666667,"It makes the synthesized samples better fit the membership distribution, and its effectiveness
86"
INTRODUCTION,0.24375,"is verified by our experiments.
87"
RELATED WORK,0.24583333333333332,"2
Related Work
88"
MEMBERSHIP INFERENCE ATTACK,0.24791666666666667,"2.1
Membership Inference Attack
89"
MEMBERSHIP INFERENCE ATTACK,0.25,"Member inference attacks aim to speculate whether a record has been involved in the training of the
90"
MEMBERSHIP INFERENCE ATTACK,0.2520833333333333,"target model. In recent years, there have been significant efforts devoted to membership inference
91"
MEMBERSHIP INFERENCE ATTACK,0.25416666666666665,"attacks and defenses. Specifically, [26] first proposed the membership inference attack, where the
92"
MEMBERSHIP INFERENCE ATTACK,0.25625,"attacker achieves membership inference through multiple attack models. [25] relaxed the assumptions
93"
MEMBERSHIP INFERENCE ATTACK,0.25833333333333336,"in membership inference attacks; other works [4, 20] proposed membership inference attacks for
94"
MEMBERSHIP INFERENCE ATTACK,0.2604166666666667,"scenarios where only labels are available. In addition, [2] analyzed the membership inference
95"
MEMBERSHIP INFERENCE ATTACK,0.2625,"attack from a causal perspective. Meanwhile, some works focused on defenses against membership
96"
MEMBERSHIP INFERENCE ATTACK,0.26458333333333334,"inference attacks, and the available algorithms can be broadly categorized into two types: overfitting
97"
MEMBERSHIP INFERENCE ATTACK,0.26666666666666666,"reduction [30, 26, 25] and perturbation model prediction [23, 15, 33]. On the contrary, the attack
98"
MEMBERSHIP INFERENCE ATTACK,0.26875,"method presented in this paper does not require the use of labels and model predictions. It not only
99"
MEMBERSHIP INFERENCE ATTACK,0.2708333333333333,"achieves good attack performance, and more importantly, it renders the existing defense methods that
100"
MEMBERSHIP INFERENCE ATTACK,0.27291666666666664,"are based on the presence of the classification layer vulnerable again.
101"
MEMBERSHIP INFERENCE ATTACK,0.275,"Recently, attack techniques [19, 8] with no demand of the classification layers attracted great attention,
102"
MEMBERSHIP INFERENCE ATTACK,0.27708333333333335,"which were developed mainly for pedestrian re-identification (Re-ID). FR is a more challenging task
103"
MEMBERSHIP INFERENCE ATTACK,0.2791666666666667,"compared with pedestrian re-identification in the sense that the face data set contains only face-related
104"
MEMBERSHIP INFERENCE ATTACK,0.28125,"information but has more identities and samples than the Re-ID data set. Moreover, [19] requires
105"
MEMBERSHIP INFERENCE ATTACK,0.2833333333333333,"acquiring multiple samples of the same class, while [8] needs to obtain partial training data. These
106"
MEMBERSHIP INFERENCE ATTACK,0.28541666666666665,"conditions are difficult to fulfill in realistic scenarios. The developed method in this paper outperforms
107"
MEMBERSHIP INFERENCE ATTACK,0.2875,"the existing membership inference attack algorithms in terms of improved success rate under the
108"
MEMBERSHIP INFERENCE ATTACK,0.28958333333333336,"same amount of available information.
109"
MODEL INVERSION ATTACK,0.2916666666666667,"2.2
Model Inversion Attack
110"
MODEL INVERSION ATTACK,0.29375,"The model inversion attack is a particular type of privacy attacks on machine learning models,
111"
MODEL INVERSION ATTACK,0.29583333333333334,"which aims at reconstructing the data used in training. Many recent model inversion techniques
112"
MODEL INVERSION ATTACK,0.29791666666666666,"[17, 28, 1, 10] have been implemented with the help of the generative power of GAN. [17] formulated
113"
MODEL INVERSION ATTACK,0.3,"model inversion as the problem of finding the maximum a posteriori (MAP) estimate given a target
114"
MODEL INVERSION ATTACK,0.3020833333333333,"label, and limited the search space to the generative space of GAN. [28] proposed a more robust cross-
115"
MODEL INVERSION ATTACK,0.30416666666666664,"domain attack with a pre-trained GAN, which can generate realistic training samples independent
116"
MODEL INVERSION ATTACK,0.30625,"of the pre-trained data. MIRROR [1], on the other hand, explored a new space P over StyleGAN,
117"
MODEL INVERSION ATTACK,0.30833333333333335,"and regularize latent vectors in P to obtain good performance. [10] cast the latent space search as a
118"
MODEL INVERSION ATTACK,0.3104166666666667,"Markov decision process (MDP) and solved it using reinforcement learning.
119"
MODEL INVERSION ATTACK,0.3125,"Meanwhile, there are some works [33, 32, 22] focusing on defenses against model inversion attacks.
120"
MODEL INVERSION ATTACK,0.3145833333333333,"[33] proposed a unified purification framework to resist both model inversion and membership
121"
MODEL INVERSION ATTACK,0.31666666666666665,"inference attacks by reducing the dispersion of confidence score vectors. DuetFace [22] utilized
122"
MODEL INVERSION ATTACK,0.31875,"a framework that converts RGB images into their frequency-domain representations and makes it
123"
MODEL INVERSION ATTACK,0.32083333333333336,"difficult for attackers to recover the training data by splitting them. PPFR-FD [32] investigated
124"
MODEL INVERSION ATTACK,0.3229166666666667,"the impact of visualization components on recognition networks based on the privacy-accuracy
125"
MODEL INVERSION ATTACK,0.325,"trade-off analysis and finally proposed an image masking method, which can effectively remove the
126"
MODEL INVERSION ATTACK,0.32708333333333334,"visualization part of the images without affecting the FR accuracy evidently.
127"
METHOD,0.32916666666666666,"3
Method
128"
METHOD,0.33125,"In this section, we present the proposed attack method in detail, whose diagram is shown in Fig.2.
129"
METHOD,0.3333333333333333,"We first briefly review the existing training and inference paradigms of FR in Section 3.2. Next,
130"
METHOD,0.33541666666666664,"we introduce the difference in the distance distribution of member/non-member compared to BN
131"
METHOD,0.3375,"layer parameters in Section 3.3, which will guide the member inference attack. In Section 3.3,
132"
METHOD,0.33958333333333335,"we will formally introduce the first stage of the inference attack, i.e. membership inference attack
133"
METHOD,0.3416666666666667,"algorithm. Furthermore, we will introduce our second stage - the model inversion attack method
134"
METHOD,0.34375,"without classification layers in Section 3.4.
135"
BACKGROUND,0.3458333333333333,"3.1
Background
136"
BACKGROUND,0.34791666666666665,"Commonly adopted training and inference paradigms of FR are shown in Fig.1. Given a face data
137"
BACKGROUND,0.35,"set D, an image sampled from it is denoted as x ‚ààRc√óh√ów, where c, h and w represent the number
138"
BACKGROUND,0.35208333333333336,"of channels, height and width of the sample. Its identity label can be represented as a one-hot
139"
BACKGROUND,0.3541666666666667,"vector y ‚ààRn√ó1, where n is the number of classes. In the training phase, a backbone network B
140"
BACKGROUND,0.35625,"with model parameters collected in Œ∏b takes an input sample and extracts the corresponding high-
141"
BACKGROUND,0.35833333333333334,"dimensional feature embedding v = B(x) ‚ààRf√ó1, where f is the dimensionalities of the embedding.
142"
BACKGROUND,0.36041666666666666,"The backbone network B employs a classification layer C, whose parameters are collected in Œ∏c,
143"
BACKGROUND,0.3625,"to predict the class ÀÜy = C(B(x)) for the obtained embedding v. The cross-entropy, denoted as
144"
BACKGROUND,0.3645833333333333,"LCE(ÀÜy, y), is usually used as the loss function in training, and the training process can be formulated
145"
BACKGROUND,0.36666666666666664,"as the following minimization problem with respect to Œ∏b and Œ∏c:
146"
BACKGROUND,0.36875,"min
Œ∏b,Œ∏c E(x,y)‚àºDLCE[C(B(x)), y].
(1)"
BACKGROUND,0.37083333333333335,"During inference, the goal is to decide whether the two face images come from the same person or
147"
BACKGROUND,0.3729166666666667,"not. The trained backbone B is utilized to extract the embeddings of the two face images without
148"
BACKGROUND,0.375,"utilizing the classification layer anymore. The similarity of the obtained two embeddings is evaluated
149"
BACKGROUND,0.3770833333333333,"based on certain metrics. If the computed similarity is higher than a well-designed threshold, they are
150"
BACKGROUND,0.37916666666666665,"considered to be from the same person.
151"
DISTANCE DISTRIBUTION COMPARISON,0.38125,"3.2
Distance Distribution Comparison
152"
DISTANCE DISTRIBUTION COMPARISON,0.38333333333333336,"Current studies [19, 8] on the distributions of feature embeddings of samples from the training data set
153"
DISTANCE DISTRIBUTION COMPARISON,0.3854166666666667,"(i.e., member samples) and other samples (i.e., non-member samples) were carried out. In particular,
154"
DISTANCE DISTRIBUTION COMPARISON,0.3875,"[19] requires a large number of inter-class samples to calculate the class centers, while ASSD [8]
155"
DISTANCE DISTRIBUTION COMPARISON,0.38958333333333334,"demands access to some training samples as reference samples to calculate their Euclidean distances.
156"
DISTANCE DISTRIBUTION COMPARISON,0.39166666666666666,"These assumptions may be too strict and are not always available to attackers. Thus, we shall look for
157"
DISTANCE DISTRIBUTION COMPARISON,0.39375,"information on the training samples stored in the trained backbone network B itself.
158"
DISTANCE DISTRIBUTION COMPARISON,0.3958333333333333,"Specifically, we consider replacing the reference samples required in [19, 8] with the statistics stored
159"
DISTANCE DISTRIBUTION COMPARISON,0.39791666666666664,"in the Batch Normalization (BN) [14] layer of the trained backbone network. A BN layer normalizes
160"
DISTANCE DISTRIBUTION COMPARISON,0.4,"the feature maps during the training process to mitigate covariate shifts [14], and it implicitly captures
161"
DISTANCE DISTRIBUTION COMPARISON,0.40208333333333335,"the channel-wise means and variances [35]. To gain more insights, we compare the distances between
162"
DISTANCE DISTRIBUTION COMPARISON,0.4041666666666667,"the intermediate features of the member/non-member samples and the corresponding ""running mean""
163"
DISTANCE DISTRIBUTION COMPARISON,0.40625,"and ""running variance (var)"" in several BN layers, and the results are shown in Appendix.
164"
DISTANCE DISTRIBUTION COMPARISON,0.4083333333333333,"There is a clear boundary separating the distance distribution of member and non-member samples
165"
DISTANCE DISTRIBUTION COMPARISON,0.41041666666666665,"in some BN layers. This indicates that the BN layer parameters may be utilized for membership
166"
DISTANCE DISTRIBUTION COMPARISON,0.4125,"inference. This observation is fundamentally different from the findings in [19, 8], as they completely
167"
DISTANCE DISTRIBUTION COMPARISON,0.41458333333333336,"ignored the information stored in the trained model itself on the training data set. The performance of
168"
DISTANCE DISTRIBUTION COMPARISON,0.4166666666666667,"the techniques from [19, 8] heavily depends on the number of samples used. If the available samples
169"
DISTANCE DISTRIBUTION COMPARISON,0.41875,"are insufficient, they will not perform as well as expected. Our observation, on the other hand, relax
170"
DISTANCE DISTRIBUTION COMPARISON,0.42083333333333334,"these constraints on the following membership inference attack.
171"
INFERENCE MEMBERSHIP ATTACK,0.42291666666666666,"3.3
Inference Membership Attack
172"
INFERENCE MEMBERSHIP ATTACK,0.425,"The proposed membership inference attack algorithm is designed based on the observation presented
173"
INFERENCE MEMBERSHIP ATTACK,0.4270833333333333,"in the previous subsection. It is also the first step of our attack model, as shown as Stage 1 in Fig. 2.
174"
INFERENCE MEMBERSHIP ATTACK,0.42916666666666664,"Specifically, we consider the ""running mean"" parameters in certain BN layers, denoted by ui ‚ààRbi√ó1,
175"
INFERENCE MEMBERSHIP ATTACK,0.43125,"where bi represents the number of channels in the ith BN layer, and thus we obtain a set of vectors
176"
INFERENCE MEMBERSHIP ATTACK,0.43333333333333335,"u = {u1, u2, ¬∑ ¬∑ ¬∑ , un}, where n represents the number of BN layers selected. It is worthwhile to
177"
INFERENCE MEMBERSHIP ATTACK,0.4354166666666667,"point out that since the number of channels in the BN layers can be different, the vectors ui in u do
178"
INFERENCE MEMBERSHIP ATTACK,0.4375,"not necessarily have the same dimensionality.
179"
INFERENCE MEMBERSHIP ATTACK,0.4395833333333333,"Given the input image x, we extract the intermediate features vi ‚ààRci√óhi√ówi before a particular BN
180"
INFERENCE MEMBERSHIP ATTACK,0.44166666666666665,"layer, and then we obtain vi ‚ààRci√ó1 by normalizing along both the height and width dimensions
181"
INFERENCE MEMBERSHIP ATTACK,0.44375,"following the BN operation. We then compute the Euclidean distance between the extracted and
182"
INFERENCE MEMBERSHIP ATTACK,0.44583333333333336,"normalized feature vi, and the reference ui using
183"
INFERENCE MEMBERSHIP ATTACK,0.4479166666666667,di = 1
INFERENCE MEMBERSHIP ATTACK,0.45,"bi
||vi ‚àíui||2
2.
(2)"
INFERENCE MEMBERSHIP ATTACK,0.45208333333333334,"We transmit the distance vector d = {d1, d2, ¬∑ ¬∑ ¬∑ , dn} into the classification network A. Due to the
184"
INFERENCE MEMBERSHIP ATTACK,0.45416666666666666,"good distinguishability of the features we extract, we do not need to design a complex network for
185"
INFERENCE MEMBERSHIP ATTACK,0.45625,"membership inference. Here, we only need to use a fully connected layer and a sigmoid function to
186"
INFERENCE MEMBERSHIP ATTACK,0.4583333333333333,"compose our attack model, which predicts the probability that the sample is a member. If it is from
187"
INFERENCE MEMBERSHIP ATTACK,0.46041666666666664,"the training data set, then the attack model should output 1, otherwise we expect it to output 0.
188"
INFERENCE MEMBERSHIP ATTACK,0.4625,"More empirical experiments show that the use of both the original face image and its horizontally
189"
INFERENCE MEMBERSHIP ATTACK,0.46458333333333335,"flipped version is capable of enhancing the performance. Therefore, in the implemented membership
190"
INFERENCE MEMBERSHIP ATTACK,0.4666666666666667,"inference scheme, we first flip horizontally an image x to obtain x‚Ä≤, and then extract v‚Ä≤
i by following
191"
INFERENCE MEMBERSHIP ATTACK,0.46875,"the same procedure used to find vi. The distance di is now computed as
192"
INFERENCE MEMBERSHIP ATTACK,0.4708333333333333,di = 1
INFERENCE MEMBERSHIP ATTACK,0.47291666666666665,"bi
||vi + v‚Ä≤
i
2
‚àíui||2
2.
(3)"
INFERENCE MEMBERSHIP ATTACK,0.475,"The classification network A is trained through solving
193"
INFERENCE MEMBERSHIP ATTACK,0.47708333333333336,"min
Œ∏A LCE[A(d), s]
(4)"
INFERENCE MEMBERSHIP ATTACK,0.4791666666666667,"where Œ∏A denotes the parameters of the classifier A in our attack model, and s is the binary label (0
194"
INFERENCE MEMBERSHIP ATTACK,0.48125,"or 1). The training procedure of the membership inference attack is shown in Algorithm 1.
195"
INFERENCE MEMBERSHIP ATTACK,0.48333333333333334,"Algorithm 1: The training procedure of the membership inference attack (Stage 1)
Input: Member of training data xm and non-member data xn, number of training iterations M.
Output: The optimal parameters of the attack model Œ∏best."
INFERENCE MEMBERSHIP ATTACK,0.48541666666666666,"1 Extract the parameters of Batch Normalization layers in the target FR model, denoted as u."
INFERENCE MEMBERSHIP ATTACK,0.4875,"2 Set Œ∏best = Œ∏0 and Accbest = 0, where Œ∏0 denotes the initial parameters of the attack model A
and Accbest records the best performance on the test set."
INFERENCE MEMBERSHIP ATTACK,0.4895833333333333,"3 for i = 0, 1, 2, ¬∑ ¬∑ ¬∑ , M ‚àí1 do"
INFERENCE MEMBERSHIP ATTACK,0.49166666666666664,"4
Horizontally flip the member data xm and non-member data xn, and obtain x‚Ä≤
m and x‚Ä≤
n,
respectively."
INFERENCE MEMBERSHIP ATTACK,0.49375,"5
Feed the target model with xm, x‚Ä≤
m, xn and x‚Ä≤
n, and extract the corresponding intermediate
features vm, v‚Ä≤
m, vn and v‚Ä≤
n."
INFERENCE MEMBERSHIP ATTACK,0.49583333333333335,"6
Compute the distance using Eq.3 and obtain the distance vectors dm and dn."
INFERENCE MEMBERSHIP ATTACK,0.4979166666666667,"7
Update the classifier parameters using the following equation:"
INFERENCE MEMBERSHIP ATTACK,0.5,"8
Œ∏i+1 ‚ÜêŒ∏i ‚àíŒ±‚àáŒ∏i (LCE[A(dm), 1] + LCE[A(dn), 0])
where Œ± denotes the optimization step."
INFERENCE MEMBERSHIP ATTACK,0.5020833333333333,"9
Evaluate the model on the test set and obtain the accuracy Acci+1, and update Œ∏best =
Œ∏i+1, Accbest = Acci+1 if Acci+1 > Accbest."
MODEL INVERSION ATTACK,0.5041666666666667,"3.4
Model Inversion Attack
196"
MODEL INVERSION ATTACK,0.50625,"Existing model inversion attacks require that target labels are given in order to improve the prediction
197"
MODEL INVERSION ATTACK,0.5083333333333333,"score of the target identity. In this work, we consider a model inversion attack on the FR models
198"
MODEL INVERSION ATTACK,0.5104166666666666,"without the classification layer. As such, our goal is not to reconstruct the original image when the
199"
MODEL INVERSION ATTACK,0.5125,"target identity is known. Instead, we focus on recovering the identity of the training data set using the
200"
MODEL INVERSION ATTACK,0.5145833333333333,"trained backbone model as much as possible.
201"
MODEL INVERSION ATTACK,0.5166666666666667,"The model inversion attack is Stage 2 of the proposed attack model in Fig. 2. The procedure of model
202"
MODEL INVERSION ATTACK,0.51875,"inversion attack is shown in Algorithm 2. In particular, we utilize the powerful generative model
203"
MODEL INVERSION ATTACK,0.5208333333333334,"StyleGAN [16] to help restore the identity information of the training data. We first collect samples
204"
MODEL INVERSION ATTACK,0.5229166666666667,"from a normal distribution z ‚ààN(0, 1), and these samples are fed into StyleGAN‚Äôs mapping network
205"
MODEL INVERSION ATTACK,0.525,"to obtain a more disentangled subspace W. All our subsequent optimizations will be performed in W
206"
MODEL INVERSION ATTACK,0.5270833333333333,"space.
207"
MODEL INVERSION ATTACK,0.5291666666666667,"[28, 1] have pointed out that the selection of the initial latent vectors to optimize has a strong impact
208"
MODEL INVERSION ATTACK,0.53125,"on the effectiveness of model inversion attacks and its importance should not be underestimated.
209"
MODEL INVERSION ATTACK,0.5333333333333333,"As the classification layer is absent in our case, it is not feasible to obtain the confidence of the
210"
MODEL INVERSION ATTACK,0.5354166666666667,"target label, and we have to utilize the attack model in Stage 1 to select samples that are closer to
211"
MODEL INVERSION ATTACK,0.5375,"the distribution of the training set. For this purpose, we feed all the initial latent vectors into the
212"
MODEL INVERSION ATTACK,0.5395833333333333,"StyleGAN and produce a set of initial face images. After proper resizing, these images with their
213"
MODEL INVERSION ATTACK,0.5416666666666666,"horizontally flipped versions are processed following Stage 1 and we can get the prediction of the
214"
MODEL INVERSION ATTACK,0.54375,"developed attack model A. Those images with high membership classification scores are retained as
215"
MODEL INVERSION ATTACK,0.5458333333333333,"the ‚Äôfinal‚Äô set of initial vectors.
216"
MODEL INVERSION ATTACK,0.5479166666666667,"Next, we optimize the latent vectors as follows. In particular, we perform some operations on the
217"
MODEL INVERSION ATTACK,0.55,"images synthesized using the latent vectors, and propagate the transformed images through the
218"
MODEL INVERSION ATTACK,0.5520833333333334,"developed membership inference attack model in Stage 1. The membership classification network
219"
MODEL INVERSION ATTACK,0.5541666666666667,"predicts the probability that the generated images belong to the training set. The latent vectors will
220"
MODEL INVERSION ATTACK,0.55625,"then be iteratively updated to further increase the membership prediction probability. We desire to
221"
MODEL INVERSION ATTACK,0.5583333333333333,"enhance the robustness through data augmentations and finally find a good set of latent vectors.
222"
MODEL INVERSION ATTACK,0.5604166666666667,"Algorithm 2: The procedure of the model inversion attack (Stage 2)
Input: Initial N points sampled from a normal distribution z ‚ààN(0, 1), number of iteration M.
Output: The generated candidate set q (the size of q is Int (0.1N), Int (0.1N) is the integer part
of 0.1N), and the realistic training image set r."
MODEL INVERSION ATTACK,0.5625,1 Generate N images from initial sample points z by the generator.
MODEL INVERSION ATTACK,0.5645833333333333,"2 Extract the parameters of Batch Normalization layers in the target FR model, denoted as u."
MODEL INVERSION ATTACK,0.5666666666666667,"3 Feed the target model with the generated images and their horizontally flipped versions, and
compute the distance using Eq.3, and then predict the probability of membership. (We simplify
this step as MI, i.e., the membership inference.)"
MODEL INVERSION ATTACK,0.56875,"4 Select Top-n (n= Int(0.1N)) sampling points in descending probability order as the candidates
to optimize their intermediate representations wj
0(j = 1, 2, ¬∑ ¬∑ ¬∑ , Int(0.1N)). The relationship
between z and w is shown in Fig.2."
MODEL INVERSION ATTACK,0.5708333333333333,5 Set q = {} and append optimized candidates to it as follows:
MODEL INVERSION ATTACK,0.5729166666666666,"6 for j = 1, 2, ¬∑ ¬∑ ¬∑ , Int(0.1N) do"
GENERATE THE IMAGE XJ,0.575,"7
Generate the image xj
0 from the selected sample point wj
0 by the generator."
GENERATE THE IMAGE XJ,0.5770833333333333,"xj
0 = G(wj
0)"
GENERATE THE IMAGE XJ,0.5791666666666667,"8
for i = 0, 1, 2, ¬∑ ¬∑ ¬∑ , M ‚àí1 do"
"PERFORM SOME DATA AUGMENTATION OPERATIONS ON THE IMAGE XJ
I AND GET THE COUNTERPARTS
XJ",0.58125,"9
Perform some data augmentation operations on the image xj
i and get the counterparts
xj
i1,xj
i2,¬∑ ¬∑ ¬∑ ,xj
im, where m denotes the number of the data augmentation types."
"PERFORM SOME DATA AUGMENTATION OPERATIONS ON THE IMAGE XJ
I AND GET THE COUNTERPARTS
XJ",0.5833333333333334,"10
Make membership inference and compute the average probability:"
"PERFORM SOME DATA AUGMENTATION OPERATIONS ON THE IMAGE XJ
I AND GET THE COUNTERPARTS
XJ",0.5854166666666667,"pj
i =
1
m + 1[MI(xj
i) + m
X"
"PERFORM SOME DATA AUGMENTATION OPERATIONS ON THE IMAGE XJ
I AND GET THE COUNTERPARTS
XJ",0.5875,"k=1
(MI(xj
ik))]"
OPTIMIZE THE SAMPLE POINT WJ,0.5895833333333333,"11
Optimize the sample point wj
i by minimizing the loss function:"
OPTIMIZE THE SAMPLE POINT WJ,0.5916666666666667,"wj
i+1 ‚Üêwj
i ‚àíŒ±w‚àáwLCE[pj
i, 1]"
OPTIMIZE THE SAMPLE POINT WJ,0.59375,where Œ±w denotes the optimization step size.
OPTIMIZE THE SAMPLE POINT WJ,0.5958333333333333,"12
Generate the updated images: xj
i+1 = G(wj
i+1)"
OPTIMIZE THE SAMPLE POINT WJ,0.5979166666666667,"13
Append the optimized candidate to the set q = q ‚à™{xj
M}"
FOR EACH CANDIDATE XJ,0.6,"14 For each candidate xj
M ‚ààq, search the similar image in the training data based on the cosine
similarity, and obtain the image pairs."
FOR EACH CANDIDATE XJ,0.6020833333333333,"15 Select Top-n (n = Int(0.01N)) image pairs in descending similaritiy order and obtain the
realistic training image set r."
EXPERIMENT,0.6041666666666666,"4
Experiment
223"
EXPERIMENT,0.60625,"In this section, we will describe our experimental setup and results in detail. First, for the membership
224"
EXPERIMENT,0.6083333333333333,"inference attack of Stage 1, we perform two different settings based on different prior information:
225"
EXPERIMENT,0.6104166666666667,"the partial training data Dp and the auxiliary data Ds. For the model inversion attack of Stage 2, we
226"
EXPERIMENT,0.6125,"only use the auxiliary data Ds.
227"
DATASET AND TARGET MODEL,0.6145833333333334,"4.1
Dataset and Target Model
228"
DATASET AND TARGET MODEL,0.6166666666666667,"We use two datasets for inference attack, CASIA-WebFace [34] and MS1M-ArcFace [5]. The
229"
DATASET AND TARGET MODEL,0.61875,"CASIA-WebFace dataset is collected in a semi-automatical way from the Internet, and is usually used
230"
DATASET AND TARGET MODEL,0.6208333333333333,"for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575
231"
DATASET AND TARGET MODEL,0.6229166666666667,"real identities. MS1M-ArcFace is obtained by cleansing on MS1M [9], and it contains 5.8M images
232"
DATASET AND TARGET MODEL,0.625,"from 85k different celebs.
233"
DATASET AND TARGET MODEL,0.6270833333333333,"We use IR-SE-50 as the backbone, which combines an improved version of the vanilla ResNet-50
234"
DATASET AND TARGET MODEL,0.6291666666666667,"[11] with SENet [12], and use ArcFace [5] as the margin-based measurements (head). The target
235"
DATASET AND TARGET MODEL,0.63125,"model is trained for 50 epochs with an initial learning rate of 0.1 and step scheduling at 10, 20, 30
236"
DATASET AND TARGET MODEL,0.6333333333333333,"Table 1: The attack success rate of the membership inference attack in the case 1. We also consider
the case where the target models are trained with randomly flipped images, denoted as ‚ÄôFR(flip)‚Äô."
DATASET AND TARGET MODEL,0.6354166666666666,"Dp
Proportion"
DATASET AND TARGET MODEL,0.6375,"1%
5%
10%
1%+FR(flip)
5%+FR(flip)
10%+FR(flip)"
DATASET AND TARGET MODEL,0.6395833333333333,"ASSD [8]
57.20
57.32
65.42
55.31
56.28
60.45
Amean
77.94
78.03
79.75
73.83
74.84
78.27
Amean&var
77.22
76.52
78.58
68.24
69.34
78.18
Amean&flip
91.94
91.74
91.94
97.37
97.46
97.42"
DATASET AND TARGET MODEL,0.6416666666666667,Table 2: The attack success rate of the membership inference attack in the case 2.
DATASET AND TARGET MODEL,0.64375,"Ds
Target Model(Backbone+Head)"
DATASET AND TARGET MODEL,0.6458333333333334,"IR-SE-50+ArcFace
IR-SE-50+CosFace
IR-SE-101+ArcFace"
DATASET AND TARGET MODEL,0.6479166666666667,"Amean
63.37
62.56
71.19
Amean&var
64.43
62.25
71.29
Amean&flip
87.30
82.93
85.04"
DATASET AND TARGET MODEL,0.65,"and 40 epochs, using the SGD optimizer with a momentum of 0.9, weight decay of 0.0001. Previous
237"
DATASET AND TARGET MODEL,0.6520833333333333,"researches [25, 2] have proved ‚Äôif a model is overfitted, then it is vulnerable to membership inference
238"
DATASET AND TARGET MODEL,0.6541666666666667,"attack.‚Äô In this experiment, to avoid overfitting, we choose the FR model that has the best test accuracy
239"
DATASET AND TARGET MODEL,0.65625,"in all epochs during training as the target model.
240"
MEMBERSHIP INFERENCE ATTACK,0.6583333333333333,"4.2
Membership Inference Attack
241"
MEMBERSHIP INFERENCE ATTACK,0.6604166666666667,"As above mentioned, we give two different cases. Case 1: access to part of the training dataset. Case
242"
MEMBERSHIP INFERENCE ATTACK,0.6625,"2: access to the auxiliary dataset. In the case 1, we choose CASIA-WebFace as the training data set.
243"
MEMBERSHIP INFERENCE ATTACK,0.6645833333333333,"We use different proportions of training data for training the attack model, which uses ASSD [8]
244"
MEMBERSHIP INFERENCE ATTACK,0.6666666666666666,"as the baseline. In the case 2, MS1M-ArcFace is used as the training data set. We train a shadow
245"
MEMBERSHIP INFERENCE ATTACK,0.66875,"model to mimic the behavior of the target model [25]. Specifically, we first split the dataset Ds by
246"
MEMBERSHIP INFERENCE ATTACK,0.6708333333333333,"half into Dshadow and Dtarget. Then we split Dshadow by half into Dmember
shadow and Dnon‚àímember
shadow
.
247"
MEMBERSHIP INFERENCE ATTACK,0.6729166666666667,"Dtarget is used for the attack evaluation, it is also split into Dmember
target
and Dnon‚àímember
target
. All the
248"
MEMBERSHIP INFERENCE ATTACK,0.675,"Dmember serve as the members of the (shadow or target) model‚Äôs training data, while the other serves
249"
MEMBERSHIP INFERENCE ATTACK,0.6770833333333334,"as the non-member data. For evaluation, we sample 60,000 images from members and non-members
250"
MEMBERSHIP INFERENCE ATTACK,0.6791666666666667,"separately in both cases and use the attack success rate (ASR) as the evaluation metric.
251"
MEMBERSHIP INFERENCE ATTACK,0.68125,"Furthermore, we perform experiments with different settings for ablation study. First, we conduct
252"
MEMBERSHIP INFERENCE ATTACK,0.6833333333333333,"an experiment case where the images are not flipped and the ""mean distance"" is fed to the attack
253"
MEMBERSHIP INFERENCE ATTACK,0.6854166666666667,"model, denoted as Amean. And then we consider replacing the ""mean distance"" with ""mean and
254"
MEMBERSHIP INFERENCE ATTACK,0.6875,"variance distances"", which both are input to the attack model, denoted as Amean&var. Finally, we
255"
MEMBERSHIP INFERENCE ATTACK,0.6895833333333333,"use ""mean distances"" of the normal image and the horizontally flipped one as fusion features to input
256"
MEMBERSHIP INFERENCE ATTACK,0.6916666666666667,"the attack model, denoted as Amean&flip. Specially in case 2, we use different combinations of the
257"
MEMBERSHIP INFERENCE ATTACK,0.69375,"backbones and heads as the target models to validate the generalization of our methods. All results
258"
MEMBERSHIP INFERENCE ATTACK,0.6958333333333333,"of the membership inference attack are shown in Tab.1 and Tab.2 As Tab.1 shows, in case 1, the
259"
MEMBERSHIP INFERENCE ATTACK,0.6979166666666666,"performance of our attack significantly outperforms the baseline. We believe this is due to the fact
260"
MEMBERSHIP INFERENCE ATTACK,0.7,"that our selected BN-based features characterize the membership of the training set better than the
261"
MEMBERSHIP INFERENCE ATTACK,0.7020833333333333,"reference sample selected in [8], despite our network design is more lightweight than the latter. And
262"
MEMBERSHIP INFERENCE ATTACK,0.7041666666666667,"we are also able to achieve relatively good results in the Ds experiments in case 2, which validates
263"
MEMBERSHIP INFERENCE ATTACK,0.70625,"the effectiveness of our method.
264"
MODEL INVERSION ATTACK,0.7083333333333334,"4.3
Model Inversion Attack
265"
MODEL INVERSION ATTACK,0.7104166666666667,"To our knowledge, this is the first time that the model inversion attack is launched against an FR
266"
MODEL INVERSION ATTACK,0.7125,"model without the classification layer. Previous metrics always judge the accuracy of the generated
267"
MODEL INVERSION ATTACK,0.7145833333333333,"images given a target label, which are obviously not applicable in this scenario. Therefore, we use
268"
MODEL INVERSION ATTACK,0.7166666666666667,"a new metric compatible with our proposed scenario. To be specific, we will use the target model
269"
MODEL INVERSION ATTACK,0.71875,Origin
MODEL INVERSION ATTACK,0.7208333333333333,Result
MODEL INVERSION ATTACK,0.7229166666666667,Origin
MODEL INVERSION ATTACK,0.725,Result
MODEL INVERSION ATTACK,0.7270833333333333,"Figure 3: Some results of the model inversion attack in case 2. The first and third rows show the
original training images, while the second and fourth rows show the synthesis images generated by
our proposed algorithm without using the classification layer."
MODEL INVERSION ATTACK,0.7291666666666666,"to extract the embedding of the final generated results, and obtain the most likely class index by
270"
MODEL INVERSION ATTACK,0.73125,"means of the classification layer trained for FR. Then we search the most similar image pairs as
271"
MODEL INVERSION ATTACK,0.7333333333333333,"mentioned in Algorithm 2. Some attack results are provided in Fig.3. The first and third rows show
272"
MODEL INVERSION ATTACK,0.7354166666666667,"the original training images, while the second and fourth rows show the synthesis images generated
273"
MODEL INVERSION ATTACK,0.7375,"by our proposed algorithm without using the classification layer. It can be seen that the reconstructed
274"
MODEL INVERSION ATTACK,0.7395833333333334,"image is very similar to the original image (more results can be found in the appendix). This also
275"
MODEL INVERSION ATTACK,0.7416666666666667,"confirms the effectiveness of our algorithm to some extent.
276"
LIMITATION,0.74375,"5
Limitation
277"
LIMITATION,0.7458333333333333,"Although we propose an inference attack algorithm against face recognition models without classi-
278"
LIMITATION,0.7479166666666667,"fication layers, there are actually some inherent assumptions that need to be considered. First, our
279"
LIMITATION,0.75,"membership inference attack is implemented based on the backbone‚Äôs internal BN layer parameters,
280"
LIMITATION,0.7520833333333333,"which means that our algorithm currently can only be applied to white-box attacks in such a scenario.
281"
LIMITATION,0.7541666666666667,"Therefore, it is a future challenge to explore further attacks in a completely black-box scenario.
282"
LIMITATION,0.75625,"In addition, since we are not able to operate a delicate control on the attack, we cannot guarantee the
283"
LIMITATION,0.7583333333333333,"expected results given the target class. And the identity characteristics of our final synthesis images
284"
LIMITATION,0.7604166666666666,"still need to be improved, which is another major challenge in this scenario due to the fact that we
285"
LIMITATION,0.7625,"cannot optimize the latent vector for a specific target label.
286"
CONCLUSION,0.7645833333333333,"6
Conclusion
287"
CONCLUSION,0.7666666666666667,"In this paper, we propose a new scenario where the inference attack against face recognition models
288"
CONCLUSION,0.76875,"can be implemented without the classification layers. This is a more realistic and more challenging
289"
CONCLUSION,0.7708333333333334,"attack, where the adversary cannot obtain information about the classification layer on the stage of
290"
CONCLUSION,0.7729166666666667,"training, and all the defenses based on the classification layer will be ineffective in this scenario.
291"
CONCLUSION,0.775,"Considering the internal parameters of the model, we theoretically analyze the distance distributions
292"
CONCLUSION,0.7770833333333333,"between the member/non-member and the BN layer parameters, and accordingly design a simple and
293"
CONCLUSION,0.7791666666666667,"efficient attack model that is compatible with this novel scenario. Experiments demonstrate that our
294"
CONCLUSION,0.78125,"method outperforms state-of-the-art similar works under the same prior. Further, we propose a model
295"
CONCLUSION,0.7833333333333333,"inversion attack algorithm in this scenario. We utilize the classifier in our attack model to free model
296"
CONCLUSION,0.7854166666666667,"inversion attacks from dependence on the classification layer, and make the generated samples closer
297"
CONCLUSION,0.7875,"to the membership distribution. The final experimental results prove that our proposed method is able
298"
CONCLUSION,0.7895833333333333,"to recover the identities of some training members. We hope that this scenario and the algorithm we
299"
CONCLUSION,0.7916666666666666,"proposed will encourage more researchers to focus on face recognition attacks in real scenarios, and
300"
CONCLUSION,0.79375,"to attach further importance to privacy security protection.
301"
REFERENCES,0.7958333333333333,"References
302"
REFERENCES,0.7979166666666667,"[1] Shengwei An, Guanhong Tao, Qiuling Xu, Yingqi Liu, Guangyu Shen, Yuan Yao, Jingwei Xu, and Xiangyu
303"
REFERENCES,0.8,"Zhang. Mirror: Model inversion for deep learning network with high fidelity. In Proceedings of the 29th
304"
REFERENCES,0.8020833333333334,"Network and Distributed System Security Symposium, 2022.
305"
REFERENCES,0.8041666666666667,"[2] Teodora Baluta, Shiqi Shen, S Hitarth, Shruti Tople, and Prateek Saxena. Membership inference attacks
306"
REFERENCES,0.80625,"and generalization: A causal perspective. arXiv preprint arXiv:2209.08615, 2022.
307"
REFERENCES,0.8083333333333333,"[3] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating
308"
REFERENCES,0.8104166666666667,"and testing unintended memorization in neural networks. In USENIX Security Symposium, volume 267,
309"
REFERENCES,0.8125,"2019.
310"
REFERENCES,0.8145833333333333,"[4] Christopher A Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only
311"
REFERENCES,0.8166666666666667,"membership inference attacks. In International conference on machine learning, pages 1964‚Äì1974. PMLR,
312"
REFERENCES,0.81875,"2021.
313"
REFERENCES,0.8208333333333333,"[5] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss
314"
REFERENCES,0.8229166666666666,"for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern
315"
REFERENCES,0.825,"recognition, pages 4690‚Äì4699, 2019.
316"
REFERENCES,0.8270833333333333,"[6] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
317"
REFERENCES,0.8291666666666667,"information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer
318"
REFERENCES,0.83125,"and communications security, pages 1322‚Äì1333, 2015.
319"
REFERENCES,0.8333333333333334,"[7] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fully
320"
REFERENCES,0.8354166666666667,"connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM
321"
REFERENCES,0.8375,"SIGSAC conference on computer and communications security, pages 619‚Äì633, 2018.
322"
REFERENCES,0.8395833333333333,"[8] Junyao Gao, Xinyang Jiang, Huishuai Zhang, Yifan Yang, Shuguang Dou, Dongsheng Li, Duoqian Miao,
323"
REFERENCES,0.8416666666666667,"Cheng Deng, and Cairong Zhao. Similarity distribution based membership inference attack on person
324"
REFERENCES,0.84375,"re-identification. arXiv preprint arXiv:2211.15918, 2022.
325"
REFERENCES,0.8458333333333333,"[9] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and
326"
REFERENCES,0.8479166666666667,"benchmark for large-scale face recognition. In Computer Vision‚ÄìECCV 2016: 14th European Conference,
327"
REFERENCES,0.85,"Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 87‚Äì102. Springer,
328"
REFERENCES,0.8520833333333333,"2016.
329"
REFERENCES,0.8541666666666666,"[10] Gyojin Han, Jaehyun Choi, Haeil Lee, and Junmo Kim. Reinforcement learning-based black-box model
330"
REFERENCES,0.85625,"inversion attacks. arXiv preprint arXiv:2304.04625, 2023.
331"
REFERENCES,0.8583333333333333,"[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
332"
REFERENCES,0.8604166666666667,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016.
333"
REFERENCES,0.8625,"[12] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference
334"
REFERENCES,0.8645833333333334,"on computer vision and pattern recognition, pages 7132‚Äì7141, 2018.
335"
REFERENCES,0.8666666666666667,"[13] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue
336"
REFERENCES,0.86875,"Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the
337"
REFERENCES,0.8708333333333333,"IEEE/CVF conference on computer vision and pattern recognition, pages 5901‚Äì5910, 2020.
338"
REFERENCES,0.8729166666666667,"[14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
339"
REFERENCES,0.875,"internal covariate shift. In International conference on machine learning, pages 448‚Äì456. pmlr, 2015.
340"
REFERENCES,0.8770833333333333,"[15] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong.
Memguard:
341"
REFERENCES,0.8791666666666667,"Defending against black-box membership inference attacks via adversarial examples. In Proceedings of
342"
REFERENCES,0.88125,"the 2019 ACM SIGSAC conference on computer and communications security, pages 259‚Äì274, 2019.
343"
REFERENCES,0.8833333333333333,"[16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
344"
REFERENCES,0.8854166666666666,"improving the image quality of StyleGAN. In Proc. CVPR, 2020.
345"
REFERENCES,0.8875,"[17] Mahdi Khosravy, Kazuaki Nakamura, Yuki Hirose, Naoko Nitta, and Noboru Babaguchi. Model inversion
346"
REFERENCES,0.8895833333333333,"attack by integration of deep generative models: Privacy-sensitive face generation from a face recognition
347"
REFERENCES,0.8916666666666667,"system. IEEE Transactions on Information Forensics and Security, 17:357‚Äì372, 2022.
348"
REFERENCES,0.89375,"[18] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition.
349"
REFERENCES,0.8958333333333334,"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18750‚Äì
350"
REFERENCES,0.8979166666666667,"18759, 2022.
351"
REFERENCES,0.9,"[19] Guoyao Li, Shahbaz Rezaei, and Xin Liu. User-level membership inference attack against metric embed-
352"
REFERENCES,0.9020833333333333,"ding learning. arXiv preprint arXiv:2203.02077, 2022.
353"
REFERENCES,0.9041666666666667,"[20] Zheng Li and Yang Zhang. Membership leakage in label-only exposures. In Proceedings of the 2021 ACM
354"
REFERENCES,0.90625,"SIGSAC Conference on Computer and Communications Security, pages 880‚Äì895, 2021.
355"
REFERENCES,0.9083333333333333,"[21] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: A universal representation for face
356"
REFERENCES,0.9104166666666667,"recognition and quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and
357"
REFERENCES,0.9125,"Pattern Recognition, pages 14225‚Äì14234, 2021.
358"
REFERENCES,0.9145833333333333,"[22] Yuxi Mi, Yuge Huang, Jiazhen Ji, Hongquan Liu, Xingkun Xu, Shouhong Ding, and Shuigeng Zhou.
359"
REFERENCES,0.9166666666666666,"Duetface: Collaborative privacy-preserving face recognition via channel splitting in the frequency domain.
360"
REFERENCES,0.91875,"In Proceedings of the 30th ACM International Conference on Multimedia, pages 6755‚Äì6764, 2022.
361"
REFERENCES,0.9208333333333333,"[23] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with membership privacy using
362"
REFERENCES,0.9229166666666667,"adversarial regularization. In Proceedings of the 2018 ACM SIGSAC conference on computer and commu-
363"
REFERENCES,0.925,"nications security, pages 634‚Äì646, 2018.
364"
REFERENCES,0.9270833333333334,"[24] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
365"
REFERENCES,0.9291666666666667,"Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE
366"
REFERENCES,0.93125,"symposium on security and privacy (SP), pages 739‚Äì753. IEEE, 2019.
367"
REFERENCES,0.9333333333333333,"[25] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. Ml-leaks:
368"
REFERENCES,0.9354166666666667,"Model and data independent membership inference attacks and defenses on machine learning models.
369"
REFERENCES,0.9375,"arXiv preprint arXiv:1806.01246, 2018.
370"
REFERENCES,0.9395833333333333,"[26] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
371"
REFERENCES,0.9416666666666667,"against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 3‚Äì18.
372"
REFERENCES,0.94375,"IEEE, 2017.
373"
REFERENCES,0.9458333333333333,"[27] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine learning models that remember too
374"
REFERENCES,0.9479166666666666,"much. In Proceedings of the 2017 ACM SIGSAC Conference on computer and communications security,
375"
REFERENCES,0.95,"pages 587‚Äì601, 2017.
376"
REFERENCES,0.9520833333333333,"[28] Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correia, Antonia Adler, and Kristian Kersting.
377"
REFERENCES,0.9541666666666667,"Plug & play attacks: Towards robust and flexible model inversion attacks. In Proceedings of the 39th
378"
REFERENCES,0.95625,"International Conference on Machine Learning (ICML), Proceedings of Machine Learning Research, pages
379"
REFERENCES,0.9583333333333334,"20522‚Äì20545. PMLR, 2022.
380"
REFERENCES,0.9604166666666667,"[29] Florian Tram√®r, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning
381"
REFERENCES,0.9625,"models via prediction apis. In USENIX security symposium, volume 16, pages 601‚Äì618, 2016.
382"
REFERENCES,0.9645833333333333,"[30] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Towards demystifying membership
383"
REFERENCES,0.9666666666666667,"inference attacks. arXiv preprint arXiv:1807.09173, 2018.
384"
REFERENCES,0.96875,"[31] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu.
385"
REFERENCES,0.9708333333333333,"Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on
386"
REFERENCES,0.9729166666666667,"computer vision and pattern recognition, pages 5265‚Äì5274, 2018.
387"
REFERENCES,0.975,"[32] Yinggui Wang, Jian Liu, Man Luo, Le Yang, and Li Wang. Privacy-preserving face recognition in the
388"
REFERENCES,0.9770833333333333,"frequency domain. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages
389"
REFERENCES,0.9791666666666666,"2558‚Äì2566, 2022.
390"
REFERENCES,0.98125,"[33] Ziqi Yang, Bin Shao, Bohan Xuan, Ee-Chien Chang, and Fan Zhang. Defending model inversion and
391"
REFERENCES,0.9833333333333333,"membership inference attacks via prediction purification. arXiv preprint arXiv:2005.03915, 2020.
392"
REFERENCES,0.9854166666666667,"[34] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv
393"
REFERENCES,0.9875,"preprint arXiv:1411.7923, 2014.
394"
REFERENCES,0.9895833333333334,"[35] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha,
395"
REFERENCES,0.9916666666666667,"and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the
396"
REFERENCES,0.99375,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8715‚Äì8724, 2020.
397"
REFERENCES,0.9958333333333333,"[36] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
398"
REFERENCES,0.9979166666666667,"learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107‚Äì115, 2021.
399"
