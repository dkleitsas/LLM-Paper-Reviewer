Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007763975155279503,"In contrast to standard statistical wisdom, modern learning algorithms typically
ﬁnd their best performance in the overparameterized regime in which the model has
many more parameters than needed to ﬁt the training data. A growing number of
recent works have shown that random feature models can offer a detailed theoretical
explanation for this unexpected behavior, but typically these analyses have utilized
isotropic distributional assumptions on the underlying data generation process,
thereby failing to provide a realistic characterization of real-world models that are
designed to identify and harness the structure in natural data. In this work, we
examine the high-dimensional asymptotics of random feature regression in the
presence of structured data, allowing for arbitrary input correlations and arbitrary
alignment between the data and the weights of the target function. We deﬁne a
partial order on the space of weight-data alignments and prove that generalization
performance improves in response to stronger alignment. We also clarify several
previous observations in the literature by distinguishing the behavior of the sample-
wise and parameter-wise learning curves, ﬁnding that sample-wise multiple descent
can occur at scales dictated by the eigenstructure of the data covariance, but that
parameter-wise multiple descent is limited to double descent, although strong
anisotropy can induce additional signatures such as wide plateaus and steep cliffs.
Finally, these signatures are related to phase transitions in the spectrum of the
feature kernel matrix, and unlike the double descent peak, persist even under
optimal regularization."
INTRODUCTION,0.0015527950310559005,"1
INTRODUCTION"
INTRODUCTION,0.002329192546583851,"While deep learning races ahead with its impressive list of practical successes, it has often done so
by building larger models and training on ever larger datasets. For example, recent large language
models leverage billions (Brown et al., 2020; Raffel et al., 2019) or even trillions (Fedus et al.,
2021) of parameters. As the state-of-the-art pushes the computational frontier, model development,
experimentation, and exploration become increasingly costly, both in terms of the computational
resources and person-hours."
INTRODUCTION,0.003105590062111801,"To facilitate the development of improved models, it is becoming ever more necessary to develop
theoretical models that can help guide model development in these high-dimensional scenarios.
Indeed, our theoretical understanding is so limited that even linear regression, the most basic machine
learning method of all, is still yielding surprises in high dimensions (Hastie et al., 2019)."
INTRODUCTION,0.0038819875776397515,"While linear regression can capture many salient features of high-dimensional data, it is not a
reasonable model for high-dimensional models. In particular, there is no natural data-independent
notion of model size: the number of parameters in the model is intimately tied (usually equal) to the
dimensionality of the datapoints. This limitation means that linear regression may not accurately"
INTRODUCTION,0.004658385093167702,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005434782608695652,"capture the effect of overparameterization, which appears to be a crucial ingredient in modern deep
learning models (Zhang et al., 2016)."
INTRODUCTION,0.006211180124223602,"Beyond linear regression, the next simplest model is arguably the random feature model of Rahimi
et al. (2007), which turns out to be tractable in high dimensions while also allowing for a natural
data-independent lever to vary model size – the number of random features. Random feature models
have also gained independent interest through their relationship to neural kernels and Gaussian
processes (Neal, 1996; Lee et al., 2017; Novak et al., 2018; Lee et al., 2019; Jacot et al., 2018). While
recent works have examined the asymptotic performance of random feature models, most have done so
in the restricted setting of isotropic covariates and target functions (Mei & Montanari, 2019; d’Ascoli
et al., 2020; Adlam & Pennington, 2020a;b). Based on the conjectured Gaussian equivalence principle
of Goldt et al. (2020b), a few recent works have begun relaxing these simplifying assumptions in an
effort to describe more natural data distributions (Loureiro et al., 2021; d’Ascoli et al., 2021). In this
work, we extend the high-dimensional random feature linearization technique of Tripuraneni et al.
(2021a;b) to allow for anisotropic target functions, revealing a host of novel phenomena related to the
alignment of the data and the target weights."
CONTRIBUTIONS,0.006987577639751553,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.007763975155279503,Our primary contributions are to:
CONTRIBUTIONS,0.008540372670807454,"1. Derive theoretical predictions for the test error, bias, and variance of random feature re-
gression for anisotropic Gaussian covariates under general linear target functions in the
high-dimensional asymptotic limit (Section 3.1);"
CONTRIBUTIONS,0.009316770186335404,"2. Prove that overparameterization reduces the total test error, bias, and variance, even in the
presence of anisotropy (Section 3.2);"
CONTRIBUTIONS,0.010093167701863354,"3. Deﬁne a partial order on the space of weight-data alignments and prove that generalization
performance improves in response to stronger alignment (Deﬁnition 2.1, Section 3.3);"
CONTRIBUTIONS,0.010869565217391304,"4. Demonstrate that anisotropy can induce sample-wise multiple descent, but prove that
parameter-wise multiple descent is limited to double descent (Section 3.4);"
PROVIDE A THEORETICAL EXPLANATION FOR STEEP CLIFFS OBSERVED IN PARAMETER-WISE LEARNING CURVES,0.011645962732919254,"5. Provide a theoretical explanation for steep cliffs observed in parameter-wise learning curves
that persist under optimal regularization, and relate them to phase transitions in the spectrum
of the feature kernel matrix (Section 3.4)."
RELATED WORK,0.012422360248447204,"1.2
RELATED WORK"
RELATED WORK,0.013198757763975156,"Our analysis builds on a series of works that have studied the exact high-dimensional asymptotics of
the test error for a growing class of model families and data distributions. Early work in this direction
examined minimum-norm interpolated least squares and ridge regression in the high-dimensional
random design setting (Belkin et al., 2019; Dobriban et al., 2018; Hastie et al., 2019), ﬁnding
surprising phenomena such as double descent and a negative optimal ridge constant (Kobak et al.,
2020). These analyses were generalized in (Richards et al., 2021; Wu & Xu, 2020; Hastie et al., 2019)
to allow for anisotropic covariance matrices and weights that generate the targets. This anisotropic
conﬁguration was studied in further detail in (Mel & Ganguli, 2021), uncovering nuanced behavior
such as sample-wise multiple descent and a sequence of phase transitions in the performance and
optimal regularization."
RELATED WORK,0.013975155279503106,"While linear regression can provide insight into generalization performance in high dimensions, it can-
not faithfully reproduce the features of non-linear models, such as the effects of overparameterization.
Random feature models, on the other hand, can capture many such features while still maintaining
analytical tractability. Recent results have developed a detailed understanding of isotropic random
feature regression in the high-dimensional asymptotic limit, providing a precise characterization of
peaks in the test error and the beneﬁts of overparameterization (Mei & Montanari, 2019; d’Ascoli
et al., 2020; Adlam & Pennington, 2020a). The origin of these peaks was explained by means
of a ﬁne-grained decomposition of the bias and variance in (Adlam & Pennington, 2020b; Lin &
Dobriban, 2020). While most prior work examining the high-dimensional asymptotics of random
feature models have leveraged isotropy of the covariate distribution, this assumption was recently
relaxed in (Liao et al., 2020) for random Fourier feature models, in (Tripuraneni et al., 2021a;b) in the"
RELATED WORK,0.014751552795031056,Published as a conference paper at ICLR 2022
RELATED WORK,0.015527950310559006,"study of covariate shift, and in several works based on the Gaussian equivalence principle of Goldt
et al. (2020a;b) (Gerace et al., 2020; Loureiro et al., 2021)."
RELATED WORK,0.016304347826086956,"Though most of these works did not pursue a detailed investigation of the anisotropy itself, while
ﬁnalizing this manuscript we became aware of concurrent work (d’Ascoli et al., 2021) which does
focus on the interplay of the anisotropic input features and target functions, as we do here. Whereas
d’Ascoli et al. (2021) leverage the Gaussian equivalence principle and the replica method from
statistical physics, we use a completely different set of techniques stemming from the literature
on random matrix theory and operator-valued free probability (Pennington & Worah, 2018; 2019;
Adlam et al., 2019; Adlam & Pennington, 2020a; Louart et al., 2018; Péché et al., 2019; Far et al.,
2006; Mingo & Speicher, 2017), which provides a complementary perspective that could be made
completely rigorous. d’Ascoli et al. (2021) investigates the role of anisotropy not just under squared
loss, as we do here, but also under logistic loss, and obtains good numerical support for the Gaussian
equivalence conjecture in that setting. In contrast, our narrower and deeper focus on squared loss
allows us not only to uncover a host of novel phenomena, including multiple descent, steep error cliffs,
and spectral phase changes, but also to rigorously prove several propositions about their existence
and properties. We provide a more in-depth discussion of these and other related works in App. A."
RELATED WORK,0.017080745341614908,"Our work relies heavily on the concept of alignment (see Def. 2.1) between the data covariance and
the target vector coefﬁcients. An analogous deﬁnition appears in (Tripuraneni et al., 2021a;b) in the
context of covariate shift, leading to a superﬁcial similarity of results. However, the implications
and interpretations are otherwise quite different, so we refrain from repeatedly commenting on these
recurring connections."
PRELIMINARIES,0.017857142857142856,"2
PRELIMINARIES"
PROBLEM SETUP AND NOTATION,0.018633540372670808,"2.1
PROBLEM SETUP AND NOTATION"
PROBLEM SETUP AND NOTATION,0.019409937888198756,"We study random feature regression (Rahimi et al., 2007) as a model for learning an unknown linear
function of the data,
y(xi) = β⊤xi/√n0 + ϵi ,
(1)
from m independent samples (xi, yi) ∈Rn0 × R, i = 1, . . . , m, where the covariates are Gaussian,
xi ∼N(0, Σ), and where ϵi ∼N(0, σ2
ϵ ) is additive noise (present on the training samples only)."
PROBLEM SETUP AND NOTATION,0.020186335403726708,"We focus on the high-dimensional proportional asymptotics in which the input feature dimension
n0, the hidden layer size n1, and the number of samples m all tend to inﬁnity at the same rate, with
φ := n0/m and ψ := n0/n1 held constant. The overparameterization ratio φ/ψ = n1/m is a
measure of the normalized complexity of the random feature model."
PROBLEM SETUP AND NOTATION,0.02096273291925466,"Interestingly, under this model in our high-dimensional setup, any nonlinear component of the
signal to be learned behaves like additive noise, so the linear function in Eq. (1) does not sacriﬁce
generality (Mei & Montanari, 2019; Adlam & Pennington, 2020a)."
PROBLEM SETUP AND NOTATION,0.021739130434782608,"Following many works on high-dimensional regression (see e.g. Dobriban et al. (2018)), we assume
the coefﬁcient vector β is random, β ∼N(0, Σβ), though allowing for deterministic β would be a
straightforward extension and would simply involve replacing Σβ →ββ⊤throughout."
PROBLEM SETUP AND NOTATION,0.02251552795031056,"Denoting the training dataset as X = [x1, . . . , xm] and the test point as x, the random features are,
F := σ(WX/√n0)
and
f := σ(Wx/√n0) ,
(2)"
PROBLEM SETUP AND NOTATION,0.023291925465838508,"where W ∈Rn1×n0 is a random weight matrix with i.i.d. standard Gaussian entries, and σ(·) : R →
R is an activation function that is applied elementwise. These random features deﬁne the kernel"
PROBLEM SETUP AND NOTATION,0.02406832298136646,"K(x1, x2) := 1"
PROBLEM SETUP AND NOTATION,0.024844720496894408,"n1
σ(Wx1/√n0)⊤σ(Wx2/√n0) ,
(3)"
PROBLEM SETUP AND NOTATION,0.02562111801242236,which can be used to compute the model’s predictions on a point x as ˆy(x) = Y K−1Kx.
PROBLEM SETUP AND NOTATION,0.026397515527950312,"Here we have deﬁned the training labels as Y := [y(x1), . . . , y(xm)], and the regularized kernel
matrix as K := K(X, X) + γIm, as well as Kx := K(X, x) for γ ≥0."
PROBLEM SETUP AND NOTATION,0.02717391304347826,"The training error is given by,"
PROBLEM SETUP AND NOTATION,0.027950310559006212,"EΣ
train = EβE[(y(X) −ˆy(X))2] = E[(β⊤X/√n0 −Y K−1K(X, X))2] ,
(4)"
PROBLEM SETUP AND NOTATION,0.02872670807453416,Published as a conference paper at ICLR 2022
PROBLEM SETUP AND NOTATION,0.029503105590062112,and the test error (without label noise on the test point) can be written as
PROBLEM SETUP AND NOTATION,0.030279503105590064,"EΣ = EβExE[(y(x) −ˆy(x))2] −σ2
ϵ = ExE[(β⊤x/√n0 −Y K−1Kx)2] ,
(5)"
PROBLEM SETUP AND NOTATION,0.031055900621118012,"where the outermost expectation over β has been suppressed since the quantity concentrates sharply
around its mean and the inner expectation is computed over all the randomness from training, i.e. W,
X, and ϵ. The test error can be decomposed into its bias and variance components as"
PROBLEM SETUP AND NOTATION,0.03183229813664596,"EΣ = ExE[(E[ˆy(x)] −y(x))2]
|
{z
}
BΣβ"
PROBLEM SETUP AND NOTATION,0.03260869565217391,"+ Ex[V[ˆy(x)]]
|
{z
}
VΣβ .
(6)"
PROBLEM SETUP AND NOTATION,0.033385093167701864,"The bias and variance are computed with respect to all the randomness from training, i.e. W, X, ϵ,
which differs from common practice in the statistics literature, but which is necessary to obtain a
decomposition that is intuitive and unambiguous when the predictive function relies on randomness
from multiple sources (Adlam & Pennington, 2020b; Lin & Dobriban, 2020)."
PROBLEM SETUP AND NOTATION,0.034161490683229816,"As noted by (Hastie et al., 2019; Wu & Xu, 2020; Mel & Ganguli, 2021), the test error of linear
regression depends on the geometry of (Σ, Σβ) (or (Σ, β) in the case of nonrandom β), and we will
ﬁnd the same to be true for random feature regression. As such, we decompose the training and β"
PROBLEM SETUP AND NOTATION,0.03493788819875776,"covariance matrices into eigenbases as Σ = Pn0
i=1 λivivi⊤and Σβ = Pn0
i=1 λβ
i vβ
i vβ
i
⊤, where the
eigenvalues are in nondecreasing magnitude, i.e. λ1 ≤λ2 ≤. . . ≤λn0 and λβ
1 ≤λβ
2 ≤. . . ≤λβ
n0.
Following (Tripuraneni et al., 2021a;b), we then deﬁne the overlap coefﬁcients as"
PROBLEM SETUP AND NOTATION,0.03571428571428571,"qi := v⊤
i Σβvi = n0
X"
PROBLEM SETUP AND NOTATION,0.036490683229813664,"j=1
(vβ
j · vi)2λβ
j ,
(7)"
PROBLEM SETUP AND NOTATION,0.037267080745341616,"to measure the alignment of Σβ with the ith eigendirection of Σ. Note that for deterministic β this
deﬁnition simply gives the square of the component of β in the ith eigendirection, i.e. qi = (v⊤
i β)2."
ASSUMPTIONS,0.03804347826086957,"2.2
ASSUMPTIONS"
ASSUMPTIONS,0.03881987577639751,"In order to deﬁne the limiting behavior of the test error in Eq. (5), it is necessary to impose some
regularity conditions on Σ and Σβ.1 As in (Wu & Xu, 2020; Tripuraneni et al., 2021a;b), the limiting
spectra of these matrices cannot be considered independently because their eigenspaces may align.
This alignment is most conveniently described in an eigenbasis of Σ."
ASSUMPTIONS,0.039596273291925464,Assumption 1. We deﬁne the joint spectral distribution (JSD) as
ASSUMPTIONS,0.040372670807453416,"µn0 := 1 n0 n0
X"
ASSUMPTIONS,0.04114906832298137,"i=1
δ(λi,qi)
(8)"
ASSUMPTIONS,0.04192546583850932,"and assume it converges in distribution to some µ, a distribution on R2
+ as n0 →∞. We refer to µ
as the limiting joint spectral distribution (LJSD), and emphasize that this deﬁnes the relevant limiting
properties of the covariate and weight distributions2."
ASSUMPTIONS,0.042701863354037264,"Often we use (λ, q) for random variables sampled jointly from µ and denote the marginal of λ under
µ with µdata. Since the conditional expectation E[q|λ] is an important object in our study, we assume
the following for simplicity."
ASSUMPTIONS,0.043478260869565216,"Assumption 2. µ is either absolutely continuous or a ﬁnite sum of delta masses and the expectations
of λ and q are ﬁnite. Moreover, Eµ[λq] = 1."
ASSUMPTIONS,0.04425465838509317,"The normalization condition Eµ[λq] = 1 is not necessary for our analysis, but, as in (Mel & Ganguli,
2021), it enforces consistent signal-to-noise ratios across different target functions and thereby
facilitates meaningful comparisons of the test error."
ASSUMPTIONS,0.04503105590062112,"1In fact, the necessary assumptions are identical to those of Tripuraneni et al. (2021a;b) since Σβ plays an
analogous role to the test covariance Σ∗of that work.
2The JSD depends not only on Σ and Σβ but also on a choice of eigendecomposition for Σ when it has
repeated eigenvalues; however, as in (Tripuraneni et al., 2021a;b), all possible choices lead to the same formulas
and conclusions."
ASSUMPTIONS,0.04580745341614907,Published as a conference paper at ICLR 2022
ASSUMPTIONS,0.046583850931677016,"As we will eventually consider the high-dimensional limit, it is convenient to deﬁne the asymp-
totic covariance scale as s = limn0→∞
1
n0 tr(Σ) = Eµ[λ] under the limiting behavior speciﬁed in
Assumption 1."
ASSUMPTIONS,0.04736024844720497,"One important case is when Σβ = In0, in which case the LJSD degenerates to µ∅deﬁned by"
ASSUMPTIONS,0.04813664596273292,"µ∅(λ, q) := µdata(λ)δ1(q) .
(9)"
ASSUMPTIONS,0.04891304347826087,"Following Tripuraneni et al. (2021a;b), we also enforce the following standard regularity assumptions
on the activation functions to ensure the existence of the moments and derivatives we compute."
ASSUMPTIONS,0.049689440993788817,"Assumption 3. The activation function σ : R →R is assumed to be differentiable almost everywhere.
We assume there exists a universal constant C such that, |σ(x)|, |σ′(x)| = O(exp(Cx))."
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05046583850931677,"2.3
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05124223602484472,"The assumptions above allow for a wide class of possible asymptotic covariance structures and
spectra. To allow for concrete examples that have intuitive interpretations, we introduce the following
simple family of distributions that capture multi-scale structure in the input, which we refer to as
d-scale LJSDs. In particular, for d ∈N and real 0 < α ≤1 and θ, we deﬁne"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05201863354037267,"µd-scale
α,θ
:= d−1
X j=0"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.052795031055900624,"1
dδ(Cαj,Dαθj),
(10)"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05357142857142857,"where C :=

1
d
1−αd"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05434782608695652,"1−α
−1
and D :=
1
C

1
d
1−α(θ+1)d"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.05512422360248447,"1−α(θ+1)
−1
enforce the normalization conditions"
A SIMPLE FAMILY OF ANISOTROPIC DISTRIBUTIONS,0.055900621118012424,"s = Eµd-scale
α,θ [λ] = 1 and Eµd-scale
α,θ [qλ] = 1. These simple covariance models capture the hierarchy of
scales that often characterize the structure of natural datasets. Note that by setting α = 1 and s = 1,
the distribution reduces to the isotropic case with identity covariance. For the nontrivial setting in
which α < 1, the data exhibits a sequence of d distinct scales where each scale is a factor α smaller
than the previous one. The exponent θ parameterizes the strength of the weight-data alignment in
an intuitive way. When θ = 0, there is no alignment, and Σβ is proportional to the identity. When
θ > 0, there is strong alignment, as the large eigendirections of the training distribution correspond
to large eigendirections of Σβ. When θ < 0, there is anti-alignment, as the large eigendirections of
the training distribution correspond to small eigendirections of Σβ."
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.056677018633540376,"2.4
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT"
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.05745341614906832,"As the preceding discussion has suggested and as we will see explicitly in the following section,
the LJSD µ captures all of the information about the pair of covariance matrices (Σ, Σβ) that is
relevant for describing the asymptotic test error, bias, and variance. As the alignment between Σ and
Σβ can vary in strength among the different eigendirections, the concept of alignment is inherently
multi-dimensional. Nevertheless, by requiring strong alignment along the larger eigendirections, it is
possible to deﬁne the following natural partial order on the space of possible weight-data alignments3"
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.05822981366459627,"Deﬁnition 2.1. Let µ1 and µ2 be LJSDs with the same marginal distribution of λ. If the asymptotic
overlap coefﬁcients are such that Eµ1 [λq|λ] /Eµ2 [λq|λ] = Eµ1 [q|λ] /Eµ2 [q|λ] is nondecreasing
in λ, we say that µ1 is more strongly aligned than µ2 and write µ1 ≤µ2. Comparing against the
case of isotropic weight distribution, µ∅, we say µ1 is aligned when µ1 ≤µ∅and anti-aligned when
µ1 ≥µ∅."
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.059006211180124224,"The parameter θ of the d-scale model in Eq. (10) provides a quantitative measure of alignment
under Deﬁnition 2.1, formalizing the intuition given in Section 2.3. As we will see in Section 3.3, the
asymptotic generalization performance of random feature regression is strictly ordered under this
deﬁnition of alignment strength."
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.059782608695652176,"3Note that this deﬁnition is nearly identical to that of Tripuraneni et al. (2021a;b), with the concept of
hardness replaced by alignment."
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.06055900621118013,Published as a conference paper at ICLR 2022
DEFINITION OF THE STRENGTH OF WEIGHT-DATA ALIGNMENT,0.06133540372670807,"Figure 1: Test error, bias, and variance as a function of the overparameterization ratio (φ/ψ = n1/m)
and the alignment θ for the 2-scale LJSD (Eq. (10)) with φ = n0/m = 10/9, σ = tanh, γ = 10−8
and σ2
ε = 1/100. (a) The total test error exhibits the characteristic double descent behavior for all
shift powers. (b) The bias is a nonincreasing function of φ/ψ for all shift powers, as in Proposition
3.1. (c) The variance is the source of the peak, and is a nonincreasing function of φ/ψ for all shift
powers in the overparameterized regime, as in Proposition 3.2. Blue, orange, and green lines in (a,b,c)
indicate locations of vertical and horizontal slices shown in (d,e,f). (d) 1D horizontal slices of (a,b,c)
more clearly demonstrate the monotonicity in φ/ψ predicted by Propositions 3.1 and 3.2. (e) 1D
vertical slices demonstrate the monotonicity in θ of the test error and the bias predicted by Proposition
3.3, and illustrate that the variance may not decrease in response to stronger alignment (solid green
trace). (f) Test error, bias, and variance normalized by the training error (ie.
Eµ+σ2
ε
Etrain/Etrain(θ=0); similarly
for Bµ, Vµ). Same legend as (e). Constancy of the blue traces illustrates how the train and test error
respond identically to alignment. Simulations for m = 4000 (d,e; crosses) agree well with formulas."
MAIN RESULTS,0.062111801242236024,"3
MAIN RESULTS"
MAIN RESULTS,0.06288819875776397,"Our main result characterizes the high-dimensional asymptotic limits of the test error, bias, and
variance of the nonlinear random feature model of Section 2. Before stating the result, we introduce
some additional notation that captures the effect of the nonlinearity σ,"
MAIN RESULTS,0.06366459627329192,"η := Vz∼N(0,s)[σ(z)] , ρ:= ( 1"
MAIN RESULTS,0.06444099378881987,"sEz∼N(0,s)[zσ(z)])2 , ζ := sρ , ω:= s(η/ζ −1) .
(11)"
MAIN RESULTS,0.06521739130434782,"where, as above, s = limn0→∞
1
n0 tr(Σ) = Eµ[λ]. The constant ω ≥0 is a measure of the degree of
nonlinearity, with ω = 0 corresponding to linear activation functions (see Lemma B.1). Analogously
to Tripuraneni et al. (2021a;b), we also introduce two functionals of µ, which capture all the relevant
spectral information needed for describing the test loss,"
MAIN RESULTS,0.06599378881987578,"Ia,b := φ Eµ

λa (φ + xλ)−b
and
Iβ
a,b := φ Eµ

qλa (φ + xλ)−b
.
(12)"
MAIN RESULTS,0.06677018633540373,"3.1
FORMULAS FOR ASYMPTOTIC BIAS, VARIANCE, AND TEST ERROR"
MAIN RESULTS,0.06754658385093168,"Theorem 3.1. Under Assumptions 1-3, as n0, n1, m →∞with φ = n0/m and ψ = n0/n1 held
constant, the training error EΣ
train tends toward the value of Eµ
train where"
MAIN RESULTS,0.06832298136645963,"Eµ
train = −γ2 
∂γ(τ1Iβ
1,1) + σ2
ε∂γτ1

,
(13)"
MAIN RESULTS,0.06909937888198758,Published as a conference paper at ICLR 2022
MAIN RESULTS,0.06987577639751552,"Figure 2: Sample-wise multiple descent for anisotropic data as a function of nonlinearity strength
ω. All panels refer to a 3-scale covariance model with α = 103, and ψ = 1"
MAIN RESULTS,0.07065217391304347,"2, s = ρ = 1, γ = 10−13"
MAIN RESULTS,0.07142857142857142,"and σ2
ε = 10. (a) Error exhibits multiple peaks as a function of the the sampling density φ−1. As
ω is increased, peaks are attenuated and eventually disappear in sequence starting from those due
to the weakest data scales (at m = n0) to those due to the strongest data scales (at m = n0/3). (b)
Limiting spectral density of the kernel matrix K(X, X) = F T F/n1. Vertical slices correspond to
the spectral density at the corresponding value of φ−1 for ω = 10−10. Peaks in the error curves
in (a) occur at critical values m = 1"
MAIN RESULTS,0.07220496894409938,"3n0, 2"
MAIN RESULTS,0.07298136645962733,"3n0, n0 where a new spectral component ﬁrst appears. (c)
Consistent with this, sample-wise learning curves exhibiting multiple descent at low ω pass through
several transitions in the number of spectral components, while those at higher ω with fewer peaks
pass through fewer component transitions."
MAIN RESULTS,0.07375776397515528,"and the bias, variance, and test error BΣβ, VΣβ, EΣβ tend toward Bµ, Vµ, Eµ with"
MAIN RESULTS,0.07453416149068323,"Eµ = Eµ
train
γ2τ 2
1
−σ2
ε ,
Bµ = φIβ
1,2 ,
and
Vµ = Eµ −Bµ ,
(14)"
MAIN RESULTS,0.07531055900621118,"where ρ, ω are deﬁned in (11), Ia,b, Iβ
a,b are deﬁned in (12), and x is the unique nonnegative real"
MAIN RESULTS,0.07608695652173914,root of x = 1−γτ1
MAIN RESULTS,0.07686335403726709,"ω+I1,1 with τ1 =
√"
MAIN RESULTS,0.07763975155279502,"(ψ−φ)2+4xψφγ/ρ+ψ−φ 2ψγ
."
MAIN RESULTS,0.07841614906832298,"Remark 3.1. As noted in (Adlam & Pennington, 2020a; Hastie et al., 2019) for isotropic covariates,
the test error is simply related to the training error through the generalized cross-validation (GCV)
formula (Golub et al., 1979). Curiously, because τ1 is independent of β, it follows that the test
error depends on β entirely through its effect on the training error; in contrast, the bias-variance
decomposition retains explicit β dependence, even conditional on the training error."
MAIN RESULTS,0.07919254658385093,"The results in Theorem 3.1 depend on a single scalar self-consistent equation for x, x =
1−γτ1
ω+I1,1 ,
which is in fact the same as the one appearing in Tripuraneni et al. (2021a;b), and which signiﬁcantly
simpliﬁes the expressions relative to those recently obtained using the replica method (d’Ascoli
et al., 2021). Owing to its simplicity, this equation admits straightforward analysis that we pursue
in the Appendix, yielding numerous inequalities and bounds that ultimately prove the propositions
presented throughout this section. We will occasionally refer to the ridgeless limit of Theorem 3.1,
which is given in Corollary G.1. Finally, as a consistency check, taking σ(x) = x and ψ →0 in
Theorem 3.1 yields an expression for the test error, bias, and variance for linear regression that agrees
with the results of (Wu & Xu, 2020; Mel & Ganguli, 2021) (see Section E)."
THE BENEFITS OF OVERPARAMETERIZATION,0.07996894409937888,"3.2
THE BENEFITS OF OVERPARAMETERIZATION"
THE BENEFITS OF OVERPARAMETERIZATION,0.08074534161490683,"While there is abundant empirical evidence that overparameterization can improve the generalization
performance of practical models (see e.g. (Zhang et al., 2016)), rigorous explanations for this behavior
have been offered solely in the setting of isotropic covariates and target functions. It is therefore
natural to wonder whether the beneﬁts of overparmeterization persist in the presence of anisotropy,
and indeed recent theoretical work has provided numerical evidence hinting that overparameterization
is beneﬁcial (d’Ascoli et al., 2021). The following two results prove this to be the case."
THE BENEFITS OF OVERPARAMETERIZATION,0.08152173913043478,"First, we show that the bias decreases (or stays constant) in response to an increase in the number of
random features, regardless of any anisotropy in the covariates or target function weights."
THE BENEFITS OF OVERPARAMETERIZATION,0.08229813664596274,Published as a conference paper at ICLR 2022
THE BENEFITS OF OVERPARAMETERIZATION,0.08307453416149069,"Proposition 3.1. In the setting of Theorem 3.1, the bias Bµ is a nonincreasing function of the
overparameterization ratio φ/ψ."
THE BENEFITS OF OVERPARAMETERIZATION,0.08385093167701864,"The same is true for the variance in the overparameterized regime. Note that our proof requires the
ridgeless setting, but numerical simulations suggest that the result may hold generally (see Fig. 1).
Proposition 3.2. In the ridgeless limit and in the overparameterized regime (ψ < φ), the variance
Vµ is a nonincreasing function of overparameterization ratio φ/ψ."
THE BENEFITS OF OVERPARAMETERIZATION,0.08462732919254658,"In Fig. 1, Propositions 3.1 and 3.2 are illustrated. Moving left to right in panels (a)-(d) leads to models
with more parameters. The monotonicity of the bias across the whole range of parameterization is
evident, as is the monotonicity of the variance in the overparmaeterized regime."
WEIGHT-DATA ALIGNMENT REDUCES THE BIAS AND TEST ERROR,0.08540372670807453,"3.3
WEIGHT-DATA ALIGNMENT REDUCES THE BIAS AND TEST ERROR"
WEIGHT-DATA ALIGNMENT REDUCES THE BIAS AND TEST ERROR,0.08618012422360248,"A number of recent works have conﬁrmed the intuition that generalization performance should
improve if the weights of the target function align with the large eigendirections of the training
covariance, with formal theoretical arguments in the case of linear regression (Hastie et al., 2019;
Mel & Ganguli, 2021), and with informal numerical simulations for random feature models (d’Ascoli
et al., 2021). The following result proves that this informal observation holds in full generality, not
only for the total test error, but for the bias and the bias-to-variance ratio as well.
Proposition 3.3. Let µ1, µ2 be two LJSDs such that µ1 ≤µ2 (see Deﬁnition 2.1). Then Bµ1 ≤Bµ2,
Eµ1 ≤Eµ2, and Bµ1/Vµ1 ≤Bµ2/Vµ2."
WEIGHT-DATA ALIGNMENT REDUCES THE BIAS AND TEST ERROR,0.08695652173913043,"We illustrate Proposition 3.3 in Fig. 1 for the 2-scale LJSD of Eq. (10). Following vertical lines
upward in panels (a-c) or the x-axis left-to-right in (e,f) yields stronger alignment and a corresponding
decrease in the bias and test error. Panel (f) shows the alignment-dependence of the bias, variance,
and test error when normalized by the training error. As suggested by Remark 3.1, the normalized
test error is independent of the alignment strength, and as intuition would suggest and as predicted
by Proposition 3.3, the normalized bias decreases in response to stronger alignment."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.08773291925465838,"3.4
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES"
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.08850931677018634,"Strong anisotropy has been observed to induce structured learning curves in the context of linear
regression (Mel & Ganguli (2021)). It is natural to wonder whether these effects generalize to the case
of nonlinear random features. Figure 2 shows that this is indeed the case for sample-wise learning
curves, where φ−1 = m/n0 is varied while ψ = n0/n1 is held ﬁxed. Horizontal slices through Fig.
2 (a) exhibit sample-wise multiple descent. Fig. 2 (b) and (c) show that these peaks are associated
with phase transitions at critical values of φ−1 where the spectrum of the kernel matrix
1
n1 F T F
acquires a new component. Increased nonlinearity ω attenuates this effect."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.08928571428571429,"In contrast, the following proposition shows that parameter-wise multiple descent does not occur
even in the presence of strong anisotropy, as long as µ is aligned.
Proposition 3.4. If µ is aligned (see Deﬁnition 2.1), then, in the ridgeless limit, the test error has at
most two interior critical points as a function of the overparameterization ratio φ/ψ.
Remark 3.2. Since k-fold descent requires at least k critical points, we conclude that multiple
(k > 2) descent does not occur, even in the presence of covariate anisotropy, so long as µ is aligned."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09006211180124224,"Fig. 3(a,b) shows parameter-wise learning curves for various values of θ in the 2- and 3-scale models.
As predicted by Proposition 3.4, the curves for aligned LJSDs (θ ≥0) have at most two critical points
(not shown). In fact, even for anti-aligned LJSDs (θ < 0), most curves also have at most two critical
points; however, for small enough θ, an additional critical point emerges, and indeed the upper-most
traces in Fig. 3(b) showcase a second peak in the learning curves."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09083850931677019,"Even though parameter-wise multiple descent is not possible for aligned LJSDs, strong anisotropy
can nevertheless induce other signatures, including wide plateaus and steep cliffs. To understand the
origin of these effects, consider the ridgeless limit, for which the self-consistent equation for x is, ω x"
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09161490683229814,"φ + Eµ
λ
λ + φ x
= 1"
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09239130434782608,"φ min

1, φ"
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09316770186335403,"ψ

.
(15)"
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09394409937888198,Published as a conference paper at ICLR 2022
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09472049689440994,"Figure 3: Strong anisotropy causes steep cliffs in the test error as a function of the overparameteri-
zation ratio φ/ψ = n1/m. φ = 5/6, γ = 10−22, s = ρ = 1, ω = 10−16 and σ2
ε = 10−4. Top row:
d = 2-scale model with α = 10−5; bottom row: 3-scale model with α = 10−6. (a,b) Test error
exhibits the characteristic peak at the interpolation threshold, but additionally displays steep cliffs
at critical values of n1 corresponding to multiples of n0/d (b, dashed lines; m also shown). The
θ value associated to each trace is indicated by the colored ticks in (a). (c) Unlike the peak at the
interpolation threshold, cliffs in the error traces in (b) persist under optimal regularization (error
values from analytical formulas were numerically optimized over γ). (d) The spectrum of the kernel
matrix K(X, X) =
1
n1 F ⊤F undergoes a phase transition at each of the critical values of n1 where a
cliff occurs (dashed lines). The phase transition associated to the interpolation threshold at n1 = m
is also visible in the lower panel, but is out of range (at ≈10−17) in the upper panel."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09549689440993789,"For small ω, the left side becomes relatively insensitive to changes in x whenever λ−≪φ/x ≪λ+
for some wide spectral gap with boundary (λ−, λ+). It follows that, in the underparameterized regime
(φ < ψ), the derivative ∂(φ/ψ)/∂x is small, or, equivalently, ∂x/∂(φ/ψ) is large. This strong
sensitivity of x to changes in the overparameterization ratio φ/ψ induces similarly strong changes
to the error because ∂Eµ/∂x is bounded. As a result, the parameter-wise learning curves exhibit
sharp downward cliffs in the underparameterized regime for sufﬁciently small ω and sufﬁciently large
spectral gaps λ+/λ−. We make this argument more precise in Appendix F."
ANISOTROPY INDUCES STRUCTURED LEARNING CURVES,0.09627329192546584,"Fig. 3(a) illustrates these cliffs in the context of the d = 2- and 3-scale LJSD. Along horizontal slices,
the error turns sharply downward as a function of n1 ∝φ/ψ at integer multiples of n0/d. As can be
seen in Fig. 3(b), when the alignment is increased and β overlaps more with large λs, the ﬁrst error
cliff, corresponding to the largest scales in the data, strengthens relative to others (cliff at φ/ψ = 1/2
in the 2-scale model and φ/ψ = 1/3 in the 3-scale model). Fig. 3(c) shows that, unlike the peak at the
interpolation threshold, cliffs persist under optimal regularization. Finally, Fig. 3(d) illustrates how
steep decreases in the error are associated with the appearance of a new component in the spectrum
of the kernel matrix at critical values of φ/ψ."
CONCLUSIONS,0.09704968944099379,"4
CONCLUSIONS"
CONCLUSIONS,0.09782608695652174,"We presented an exact calculation of the limiting test error, bias, and variance for random feature
kernel regression with anisotropic covariates and target function weights. We deﬁned a partial order
over weight-data alignments and proved that stronger alignment decreases the bias and test error.
We also proved that the beneﬁts of overparameterization persist in the anisotropic setting, and that
weight-data alignment limits parameter-wise multiple descent to double descent. In contrast, we
demonstrated that anisotropy can induce sample-wise multiple descent, and parameter-wise cliffs,
and that their structure is dictated by the eigenstructure of the data covariance. Future directions
include extending our results to the non-asymptotic regime, accommodating feature learning and more
general neural network models, and investigating the impact of anisotropy for other loss functions."
CONCLUSIONS,0.0986024844720497,Published as a conference paper at ICLR 2022
REFERENCES,0.09937888198757763,REFERENCES
REFERENCES,0.10015527950310558,"Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74–84. PMLR, 2020a."
REFERENCES,0.10093167701863354,"Ben Adlam and Jeffrey Pennington. Understanding double descent requires a ﬁne-grained bias-
variance decomposition. arXiv preprint arXiv:2011.03321, 2020b."
REFERENCES,0.10170807453416149,"Ben Adlam, Jake Levinson, and Jeffrey Pennington. A random matrix perspective on mixtures of
nonlinearities for deep learning. arXiv preprint arXiv:1912.00827, 2019."
REFERENCES,0.10248447204968944,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854, 2019."
REFERENCES,0.10326086956521739,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.10403726708074534,"Stéphane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overﬁtting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020."
REFERENCES,0.1048136645962733,"Stéphane d’Ascoli, Marylou Gabrié, Levent Sagun, and Giulio Biroli. On the interplay between data
structure and loss function in classiﬁcation problems, 2021."
REFERENCES,0.10559006211180125,"Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classiﬁcation. The Annals of Statistics, 46(1):247–279, 2018."
REFERENCES,0.1063664596273292,"Reza Rashidi Far, Tamer Oraby, Wlodzimierz Bryc, and Roland Speicher. Spectra of large block
matrices. arXiv preprint cs/0610045, 2006."
REFERENCES,0.10714285714285714,"William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021."
REFERENCES,0.10791925465838509,"Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mézard, and Lenka Zdeborová. Generali-
sation error in learning with random features and the hidden manifold model, 2020."
REFERENCES,0.10869565217391304,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks, 2019."
REFERENCES,0.10947204968944099,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods?, 2021."
REFERENCES,0.11024844720496894,"Sebastian Goldt, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. Modeling the inﬂuence of
data structure on learning in neural networks: The hidden manifold model. Physical Review X, 10
(4):041044, 2020a."
REFERENCES,0.1110248447204969,"Sebastian Goldt, Galen Reeves, Marc Mézard, Florent Krzakala, and Lenka Zdeborová. The gaussian
equivalence of generative models for learning with two-layer neural networks. arXiv e-prints, pp.
arXiv–2006, 2020b."
REFERENCES,0.11180124223602485,"Gene H Golub, Michael Heath, and Grace Wahba. Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics, 21(2):215–223, 1979."
REFERENCES,0.1125776397515528,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.11335403726708075,"J. William Helton, Scott A. McCullough, and Victor Vinnikov. Noncommutative convexity arises
from linear matrix inequalities. Journal of Functional Analysis, 240:105–191, 2006."
REFERENCES,0.11413043478260869,"Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features,
2021."
REFERENCES,0.11490683229813664,Published as a conference paper at ICLR 2022
REFERENCES,0.11568322981366459,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.11645962732919254,"Noureddine El Karoui. The spectrum of kernel random matrices. Annals of Statistics, 38:1–50, 2010."
REFERENCES,0.1172360248447205,"Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world
high-dimensional data can be zero or negative due to the implicit ridge regularization. J. Mach.
Learn. Res., 21:169–1, 2020."
REFERENCES,0.11801242236024845,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017."
REFERENCES,0.1187888198757764,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572–8583, 2019."
REFERENCES,0.11956521739130435,"Zhenyu Liao, Romain Couillet, and Michael W Mahoney. A random matrix analysis of random
fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding
double descent. arXiv preprint arXiv:2006.05013, 2020."
REFERENCES,0.1203416149068323,"Licong Lin and Edgar Dobriban. What causes the test error? going beyond bias-variance via anova.
arXiv preprint arXiv:2010.05170, 2020."
REFERENCES,0.12111801242236025,"Cosme Louart, Zhenyu Liao, Romain Couillet, et al. A random matrix approach to neural networks.
The Annals of Applied Probability, 28(2):1190–1248, 2018."
REFERENCES,0.12189440993788819,"Bruno Loureiro, Cédric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mézard,
and Lenka Zdeborová. Learning curves of generic features maps for realistic datasets with a
teacher-student model, 2021."
REFERENCES,0.12267080745341614,"Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019."
REFERENCES,0.1234472049689441,"Gabriel Mel and Surya Ganguli. A theory of high dimensional regression with arbitrary correlations
between input features and target functions: sample complexity, multiple descent curves and a
hierarchy of phase transitions. In International Conference on Machine Learning, pp. 7578–7587.
PMLR, 2021."
REFERENCES,0.12422360248447205,"James A Mingo and Roland Speicher. Free probability and random matrices, volume 35. Springer,
2017."
REFERENCES,0.125,"Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks, pp. 29–53.
Springer, 1996."
REFERENCES,0.12577639751552794,"Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolaﬁa,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018."
REFERENCES,0.1265527950310559,"S Péché et al. A note on the pennington-worah distribution. Electronic Communications in Probability,
24, 2019."
REFERENCES,0.12732919254658384,"Jeffrey Pennington and Pratik Worah. The spectrum of the ﬁsher information matrix of a single-
hidden-layer neural network. In NeurIPS, pp. 5415–5424, 2018."
REFERENCES,0.1281055900621118,"Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019."
REFERENCES,0.12888198757763975,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.1296583850931677,"Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007."
REFERENCES,0.13043478260869565,Published as a conference paper at ICLR 2022
REFERENCES,0.13121118012422361,"Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression
under general source condition. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings
of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130 of
Proceedings of Machine Learning Research, pp. 3889–3897. PMLR, 13–15 Apr 2021. URL
http://proceedings.mlr.press/v130/richards21b.html."
REFERENCES,0.13198757763975155,"Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Covariate shift in high-dimensional random
feature regression. arXiv preprint arXiv:2111.08234, 2021a."
REFERENCES,0.1327639751552795,"Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization improves robustness
to covariate shift in high dimensions. Advances in Neural Information Processing Systems, 34,
2021b."
REFERENCES,0.13354037267080746,"Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear
regression. arXiv preprint arXiv:2006.05800, 2020."
REFERENCES,0.1343167701863354,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."
REFERENCES,0.13509316770186336,Published as a conference paper at ICLR 2022
REFERENCES,0.1358695652173913,"Supplementary Material: Anisotropic Random Feature Regression in High
Dimensions"
REFERENCES,0.13664596273291926,TABLE OF CONTENTS: SUPPLEMENTARY MATERIAL
REFERENCES,0.1374223602484472,"A Additional discussion of related work
2"
REFERENCES,0.13819875776397517,"A.1
Asymptotic error formulas and Theorem 3.1 . . . . . . . . . . . . . . . . . . . . .
2"
REFERENCES,0.1389751552795031,"A.2
Gaussian equivalents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2"
REFERENCES,0.13975155279503104,"A.3
Weight-data alignment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3"
REFERENCES,0.140527950310559,"B
Useful inequalities
3"
REFERENCES,0.14130434782608695,"B.1
Basic properties of the self-consistent equation for x
. . . . . . . . . . . . . . . .
3"
REFERENCES,0.1420807453416149,"B.2
I and Iβ inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5"
REFERENCES,0.14285714285714285,"C Weight-data alignment is a partial order
6"
REFERENCES,0.14363354037267081,"D Proofs of propositions
6"
REFERENCES,0.14440993788819875,"D.1
Proposition 3.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
REFERENCES,0.14518633540372672,"D.2
Proposition 3.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
REFERENCES,0.14596273291925466,"D.3
Proposition 3.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
REFERENCES,0.14673913043478262,"D.4
Proposition 3.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
REFERENCES,0.14751552795031056,"E
Linear regression limit
10"
REFERENCES,0.1482919254658385,"E.1
Comparison to Mel & Ganguli (2021) . . . . . . . . . . . . . . . . . . . . . . . .
10"
REFERENCES,0.14906832298136646,"E.2
Comparison to Wu & Xu (2020) . . . . . . . . . . . . . . . . . . . . . . . . . . .
11"
REFERENCES,0.1498447204968944,"F
Structured learning curves
12"
REFERENCES,0.15062111801242237,"F.1
Effect of spectral gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12"
REFERENCES,0.1513975155279503,"F.2
Analysis of the D-scale model in the separated limit . . . . . . . . . . . . . . . . .
14"
REFERENCES,0.15217391304347827,"G Proof of Theorem 3.1
15"
REFERENCES,0.1529503105590062,"G.1
Decomposition of the test loss
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
15"
REFERENCES,0.15372670807453417,"G.2
Decomposition of the bias and total variance . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.1545031055900621,"G.3
Summary of linearized trace terms . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.15527950310559005,"G.4
Calculation of error terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
REFERENCES,0.15605590062111802,"G.5
Final result for bias, variance, and test error . . . . . . . . . . . . . . . . . . . . .
31"
REFERENCES,0.15683229813664595,Published as a conference paper at ICLR 2022
REFERENCES,0.15760869565217392,"A
ADDITIONAL DISCUSSION OF RELATED WORK"
REFERENCES,0.15838509316770186,"Since preparing the initial version of this manuscript, we became aware of several related recent
and concurrent works. Our main contribution relative to these works is the detailed phenomenology
explored throughout the main text, but here we provide a more detailed discussion of some of
the connections between the current paper and other work, highlighting the main similarities and
differences."
REFERENCES,0.15916149068322982,"A.1
ASYMPTOTIC ERROR FORMULAS AND THEOREM 3.1"
REFERENCES,0.15993788819875776,"Alternative asymptotic formulas for the test error of anisotropic random feature regression have
been presented in a set of concurrent works, yielding overlapping results with Theorem 3.1. The
result that is most closely related is that of d’Ascoli et al. (2021), who also study the random feature
model with anisotropic input data and target function. They derive formulas for arbitrary convex loss
function (generalizing our setup) and then focus on two specialized learning scenarios: (1) linear
target function with additive noise and quadratic loss, corresponding to the case studied here; and
(2) discrete class labels sign(βT x/√n0) with label-ﬂipping noise and logistic cost function. Their
main results rely on a Gaussian equivalence theorem for anisotropic data, which they derive assuming
the Gaussian equivalence principle of Goldt et al. (2020a;b), and then proceed via a standard replica
calculation. Another salient paper is that of Loureiro et al. (2021), who study generic feature maps for
student-teacher models. A Gaussian covariate model is proposed and rigorous asymptotic solutions
are derived for it using Gaussian comparison inequalities, which are shown to agree with calculations
from the replica method. The model is general enough to facilitate comparisons to realistic datasets,
and numerical evidence and universality arguments support the utility of the model for exactly
describing the random feature model we examine here, among many other applications."
REFERENCES,0.16071428571428573,"Our technical approach proceeds in a substantially different manner, using tools from random matrix
theory and operator-valued free probability, rather than statistical physics techniques or the replica
method. The results could be made entirely rigorous, though here we simply present the pertinent
calculations and defer justiﬁcation of the underlying linearization techniques to future work and
to (Tripuraneni et al., 2021a;b). Our analysis ultimately yields ﬁnal expressions with a relatively
simple form, involving only a single scalar self-consistent equation, which lends itself to more
straightforward downstream calculations and analysis (e.g. Propositions 3.1-3.4 and Corollary
G.1). Finally, beyond the total error, we also derive formulas for the bias and variance, which aid
signiﬁcantly in the interpretation of the phenomenology, and are novel results. Interestingly, the
order parameter Q from d’Ascoli et al. (2021) (and others) is interpreted as the variance of the
student’s outputs, but actually differs from the variance deﬁned in Eq. (6). The reason is that the
bias-variance decomposition is deﬁned conditionally on x. Because the conditional mean is nonzero,
i.e. E[ˆy|x] ̸= 0, Q actually corresponds to an uncentered second moment, and corresponds to our
term E3 (deﬁned in Eq. (S147)) which differs from the total variance by the non-trivial additive term
E4 = Ex[E[ˆy|x]2] (deﬁned in Eq. (S162)). A more thorough discussion of these and related concepts
is given by Adlam & Pennington (2020b)."
REFERENCES,0.16149068322981366,"A.2
GAUSSIAN EQUIVALENTS"
REFERENCES,0.1622670807453416,"Our calculations utilize the concept of Gaussian equivalents, in particular a linear-signal-plus-noise
surrogate F lin for the random feature matrix F. This approach originates from Karoui (2010) in the
context of kernel random matrices of the form Kij = σ(X⊤
i Xj/n0) or Kij = σ(∥Xi −Xj∥2/n0)
and from Pennington & Worah (2019); Adlam et al. (2019); Péché et al. (2019) for the covariance
matrices F ⊤F/n1 studied here. This linearization technique was further developed by Adlam et al.
(2019) for anisotropic covariates (and in the presence of bias), where it was shown to be sufﬁcient for
predicting the training error for random feature ridge regression with isotropic linear target functions.
In the setting of spherical data and weights, Mei & Montanari (2019) extended these results to cover
the test error as well. In this case, many of the main technical results could build directly from Karoui
(2010) owing to a decomposition of kernel inner product matrices (Mei & Montanari, 2019, Section
C.4) that ultimately relies on the orthogonality of Gegenbauer polynomials and cannot (immediately)
be extended to the Gaussian settings studied here and elsewhere. For random feature regression
with the neural tangent kernel (which subsumes the standard random feature setting), a proof of the
linearization was outlined for the test error in the setting of isototropic Gaussian covariates by Adlam"
REFERENCES,0.16304347826086957,Published as a conference paper at ICLR 2022
REFERENCES,0.1638198757763975,"& Pennington (2020a). As mentioned previously, an extension to anisotropic Gaussian covariates
was later developed by Tripuraneni et al. (2021a;b), which is the basis for our analysis in this work."
REFERENCES,0.16459627329192547,"In a parallel and largely independent line of work stemming from Goldt et al. (2020a), a nearly
identical approach is developed under the name of the Gaussian equivalence property, under which
a possibly nonlinear target function and/or prediction are replaced with simple linear Gaussian
equivalents. This is a crucial step in the analysis in Goldt et al. (2020b); Gerace et al. (2020); d’Ascoli
et al. (2021) and related work. For example, Gerace et al. (2020) use this principle (in fact, a stronger
version they refer to as “replicated Gaussian equivalence"") in order to perform their replica analysis
of the isotropic random feature model. Subsequently, in the isotropic setting, this principle was
rigorously justiﬁed using the Lindeburg exchange method under a variety of technical assumptions
on the data distribution, weight distributions, nonlinear activation function, and target function (Hu
& Lu, 2021). Goldt et al. (2020b) relaxes some of these conditions and provides extensive tests of
the resulting formulas on real-world datasets. To perform the analysis for anisotropic input data
and target function weights, as is pursued in d’Ascoli et al. (2021), an anisotropic extension of the
Gaussian equivalence theorem is required. Substantial numerical evidence and theoretical arguments
are presented by d’Ascoli et al. (2021); Loureiro et al. (2021) for the validity of this extension, but to
the best of our knowledge a rigorous proof in this context has not been established."
REFERENCES,0.1653726708074534,"A.3
WEIGHT-DATA ALIGNMENT"
REFERENCES,0.16614906832298137,"One of the basic conclusions of our study of anisotropy is that weight-data alignment generally
improves performance. Similar observations appear in several recent works, albeit in slightly different
contexts. For example, Ghorbani et al. (2019) study the random feature model with isotropic inputs,
but anisotropic weights, in the case of a ﬁxed quadratic target function and derives an asymptotic
formula for the test error in the population limit (ie. m ≫n0, n1). For wide networks, n1 ≫n0, the
error simpliﬁes and is exactly proportional to a simple measure of alignment between the random
feature weights and the target, that is loosely related to the measure we propose in Deﬁnition 2.1."
REFERENCES,0.1669254658385093,"Ghorbani et al. (2021) also study the random feature model in the population limit, and makes the
assumption that the target function is sensitive to a much lower dimensional subspace of the input by
positing sub-linear scaling of the dimensionality of the relevant subspace. They show that increasing
the power of the input data in this subspace generally decreases test error and the number of random
features required to learn a function of ﬁxed complexity. Although the learning contexts and the ﬁnal
scaling limits for m, n0, n1 are distinct, these phenomena parallel our main result on alignment (see
e.g. Fig. 3b for illustration in the context of the d-scale model)."
REFERENCES,0.16770186335403728,"A main contribution of the current paper is the partial order on the space of weight-data alignments,
which allows us to prove that the total error and the bias decrease in response to stronger alignment
(Proposition 3.3). Our results in this vein are most directly related to those of d’Ascoli et al. (2021),
who informally observe a basic relationship between weight-data alignment and performance, though
the impact of alignment is also investigated elsewhere, e.g. Loureiro et al. (2021, Fig. 2). While
these works informally examine concept of alignment, the conclusions about it derive from numerical
evaluation of the formulas, and as such the generality of some of the results remains unclear and some
of the underlying phenomena are partially obfuscated. For example, it is not clear why the “isotropic”
and “misaligned” curves cross each other in of d’Ascoli et al. (2021, Fig. 2c): naively, one might
expect the misaligned model to always perform worse. Our results provide a nice perspective on this
behavior: owing to the differing covariate distributions, the two forms of alignment are incomparable
under the partial order."
REFERENCES,0.16847826086956522,"B
USEFUL INEQUALITIES"
REFERENCES,0.16925465838509315,"Here we include the statements and proofs of several auxiliary inequalities that we use throughout
the Supplementary Material."
REFERENCES,0.17003105590062112,"B.1
BASIC PROPERTIES OF THE SELF-CONSISTENT EQUATION FOR x"
REFERENCES,0.17080745341614906,"We begin by reviewing the basic inequalities, ﬁrst given in (Tripuraneni et al., 2021a;b). The
deﬁnitions of the following quantities can be found in Theorem 3.1."
REFERENCES,0.17158385093167702,Published as a conference paper at ICLR 2022
REFERENCES,0.17236024844720496,"Lemma B.1 (Adapted from
(Tripuraneni et al., 2021a;b)). We have the following bounds:
ω, τ1, ¯τ1, x, Ia,b, Iβ
a,b ≥0 and ∂x"
REFERENCES,0.17313664596273293,∂γ ≤0.
REFERENCES,0.17391304347826086,"Proof. As shown in (Pennington & Worah, 2018) for the unit-variance case, a simple Hermite
expansion argument establishes the relation η ≥ζ, which implies ω = s(η/ζ −1) ≥0. From
Appendix G.4.1, τ1 and ¯τ1 are traces of positive semi-deﬁnite matrices and are therefore nonnegative.
From the same equations, it follows that x = γρτ1¯τ1 ≥0. Nonnegativity of x implies Ia,b ≥0 and
Iβ
a,b ≥0 from their deﬁnitions in (12). Finally, using the nonnegativity of ω, τ1, ¯τ1, x, and Ia,b, the
expression for ∂x"
REFERENCES,0.17468944099378883,"∂γ in Theorem 3.1 immediately gives,"
REFERENCES,0.17546583850931677,"∂x
∂γ = −
x
γ + ργ( ψ"
REFERENCES,0.17624223602484473,"φ τ1 + ¯τ1)(ω + φI1,2)
≤0.
(S1)"
REFERENCES,0.17701863354037267,Next we show that the self-consistent equation x = 1−γτ1
REFERENCES,0.1777950310559006,"ω+I1,1 appearing in Theorem 3.1 and deﬁned in
(S237) admits a unique positive real solution for x."
REFERENCES,0.17857142857142858,"Lemma B.2 (Adapted from (Tripuraneni et al., 2021a;b)). There is a unique real x ≥0 satisfying
x = 1−γτ1"
REFERENCES,0.1793478260869565,"ω+I1,1 ."
REFERENCES,0.18012422360248448,"Proof. Let t = 1/x ≥0 and deﬁne,"
REFERENCES,0.18090062111801242,"h(t) = t
ρ(ψ −φ) +
p"
REFERENCES,0.18167701863354038,ρ2(ψ −φ)2 + 4γρφψ/t
REFERENCES,0.18245341614906832,"2ρψ
−1

+ ω + I1,1(1/t) ,
(S2)"
REFERENCES,0.18322981366459629,"which is a rewriting of eqn. (S237), so it sufﬁces to show that h admits a unique real positive root. To
that end, ﬁrst observe that limt→0 I1,1(1/t) = 0 and limt→∞I1,1(1/t) = s so that"
REFERENCES,0.18400621118012422,"h(0) = ω > 0
and
lim
t→∞h(t)/t = −min{1, φ/ψ} < 0 ,
(S3)"
REFERENCES,0.18478260869565216,"which together imply that h has an odd number of positive real roots. Next, we show that h is concave
for t ≥0:"
REFERENCES,0.18555900621118013,h′′(t) = −2φ t3
REFERENCES,0.18633540372670807,"
γ2ρφψ
(ρ2(ψ −φ)2 + 4γρφψ/t)3/2 + I2,3(1/t)

(S4)"
REFERENCES,0.18711180124223603,"≤0 ,
(S5)"
REFERENCES,0.18788819875776397,"which implies that h has at most two positive real roots. Therefore, we conclude that h has exactly
one positive real root. To provide a bounding interval for this root, we ﬁrst observe that,"
REFERENCES,0.18866459627329193,"lim
t→∞h(t) −
 
−min{1, φ/ψ}t + ω + s +
γφ
ρ|ψ −φ|

= 0 ,
(S6)"
REFERENCES,0.18944099378881987,"so that h(t) can be upper- and lower-bounded by linear functions ,"
REFERENCES,0.19021739130434784,"ω −min{1, φ/ψ}t ≤h(t) ≤ω + s +
γφ
ρ|ψ −φ| −min{1, φ/ψ}t .
(S7)"
REFERENCES,0.19099378881987578,"The roots of these linear functions bound the root of h, so we have"
REFERENCES,0.19177018633540371,"min{1, φ/ψ}"
REFERENCES,0.19254658385093168,"γφ
ρ|ψ−φ| + ω + s
≤x ≤min{1, φ/ψ}"
REFERENCES,0.19332298136645962,"ω
.
(S8)"
REFERENCES,0.19409937888198758,Published as a conference paper at ICLR 2022
REFERENCES,0.19487577639751552,"B.2
I AND Iβ INEQUALITIES"
REFERENCES,0.1956521739130435,"We now establish some useful properties of the I and Iβ functionals deﬁned in (12). To begin, we
note that simple algebraic manipulations establish the following raising and lowering identities:"
REFERENCES,0.19642857142857142,"Ia−1,b−1 = φIa−1,b + xIa,b
and
Iβ
a−1,b−1 = φIβ
a−1,b + xIβ
a,b .
(S9)"
REFERENCES,0.1972049689440994,"Next, we consider how the partial order of LJSDs given in Deﬁnition 2.1 leads to inequalities on the
Iβ functionals. Letting (Iβ
a,b)1 and (Iβ
a,b)2 to denote the corresponding functionals with the LJSDs
µ1 and µ2 respectively, we can establish the following useful lemma."
REFERENCES,0.19798136645962733,"Lemma B.3 (Adapted from (Tripuraneni et al., 2021a;b)). Let µ1 ≤µ2, so µ1 is more strongly
aligned than µ2 (recall Deﬁnition 2.1). Suppose the functions f, g, h : R →R are such that
f(λ) = g(λ)h(λ) and h(λ) is nonincreasing for all λ > 0, then"
REFERENCES,0.19875776397515527,"Eµ1[qf(λ)]
Eµ2[qf(λ)] ≤Eµ1[qg(λ)]"
REFERENCES,0.19953416149068323,"Eµ2[qg(λ)].
(S10)"
REFERENCES,0.20031055900621117,"Proof. By the law of iterated expectation, we have"
REFERENCES,0.20108695652173914,Eµ1[qf(λ)] = Eµ2[qg(λ)]Eλ
REFERENCES,0.20186335403726707,Eµ2[qg(λ)|λ]
REFERENCES,0.20263975155279504,"Eµ2[qg(λ)]
Eµ1[q|λ]
Eµ2[q|λ]h(λ)

.
(S11)"
REFERENCES,0.20341614906832298,"Note that the expectation Eλ in (S11) over λ is the same under µ1 and µ2 by assumption. More-
over, the function h(λ) is nonincreasing in λ by assumption. Finally, observe that the factor
Eµ2[rg(λ)|λ]/Eµ2[rg(λ)] deﬁnes a change in distribution for the random variable λ, since tak-
ing its expectation over λ yields 1. Denote a new random with this distribution by ˜λ. Then, we may
apply the Harris inequality to see"
REFERENCES,0.20419254658385094,Eµ1[qf(λ)] = Eµ2[qg(λ)]E˜λ
REFERENCES,0.20496894409937888,"""
Eµ1[q|˜λ]
Eµ2[q|˜λ]
h(˜λ) # (S12)"
REFERENCES,0.20574534161490685,≤Eµ2[qg(λ)]E˜λ
REFERENCES,0.20652173913043478,"""
Eµ1[q|˜λ]
Eµ2[q|˜λ] #"
REFERENCES,0.20729813664596272,"E˜λ
h
h(˜λ)
i
(S13)"
REFERENCES,0.2080745341614907,≤Eµ2[qg(λ)]Eλ
REFERENCES,0.20885093167701863,Eµ2[qg(λ)|λ]
REFERENCES,0.2096273291925466,"Eµ2[qg(λ)]
Eµ1[q|λ]
Eµ2[q|λ] 
Eλ"
REFERENCES,0.21040372670807453,Eµ2[qg(λ)|λ]
REFERENCES,0.2111801242236025,"Eµ2[qg(λ)] h(λ)

(S14)"
REFERENCES,0.21195652173913043,= Eµ1[qg(λ)]
REFERENCES,0.2127329192546584,"Eµ2[qg(λ)]Eµ2 [qf(λ)] .
(S15)"
REFERENCES,0.21350931677018634,"Corollary B.1. Let µ1 ≤µ2 and (Iβ
a,b)i := φ Eµi

qλa (φ + xλ)−b
. Then, for a ≤1 and b ≥0,"
REFERENCES,0.21428571428571427,"1
Eµ2 [λq]"
REFERENCES,0.21506211180124224,"
Iβ
a,b
"
REFERENCES,0.21583850931677018,"2 −
1
Eµ1 [λq]"
REFERENCES,0.21661490683229814,"
Iβ
a,b
"
REFERENCES,0.21739130434782608,"1 ≥0.
(S16)"
REFERENCES,0.21816770186335405,"Proof. Note that h : λ 7→φλa−1(φ + xλ)−b is a nonincreasing function of λ ≥0. Then, setting
g = λ and f = gh in Lemma B.3 gives the desired result."
REFERENCES,0.21894409937888198,"Lemma B.4. Suppose the functions f, g, h : R →R are such that f(λ) = λg(λ)h(λ) and h(λ) is
nonincreasing for all λ > 0. Then, if the LJSD µ is aligned (see Deﬁnition 2.1), then Eλ[g]Eµ[qf] ≤
Eµ[qg]Eλ[f]."
REFERENCES,0.21972049689440995,Published as a conference paper at ICLR 2022
REFERENCES,0.2204968944099379,Proof.
REFERENCES,0.22127329192546583,"Eµ[qf] = Eµ[qλgh]
(S17)
= Eλ[Eµ[qλ|λ]g(λ)h(λ)]
(S18)"
REFERENCES,0.2220496894409938,= Eµ[g]Eλ
REFERENCES,0.22282608695652173,"
Eµ[qλ|λ]h(λ) g(λ) Eµ[g]"
REFERENCES,0.2236024844720497,"
(S19)"
REFERENCES,0.22437888198757763,≤Eµ[g]Eλ
REFERENCES,0.2251552795031056,"
Eµ[qλ|λ] g(λ) Eµ[g] 
Eλ"
REFERENCES,0.22593167701863354,"
h(λ) g(λ) Eµ[g]"
REFERENCES,0.2267080745341615,"
(S20)"
REFERENCES,0.22748447204968944,"=
1
Eλ[g]Eµ[qλg]Eλ[f] ,
(S21)"
REFERENCES,0.22826086956521738,"where Eµ[qλ|λ] is nondecreasing in λ because µ is aligned, so the inequality again follows from the
Harris inequality."
REFERENCES,0.22903726708074534,"Corollary B.2. If µ is aligned, Ia,bIβ
a,b ≤Ia−1,bIβ
a+1,b."
REFERENCES,0.22981366459627328,Proof. Take g : λ →φλa(φ + xλ)−b and h : λ →1/λ in Lemma B.4.
REFERENCES,0.23059006211180125,"C
WEIGHT-DATA ALIGNMENT IS A PARTIAL ORDER"
REFERENCES,0.23136645962732919,"We restate Deﬁnition 2.1 for reference, and prove that it deﬁnes a partial order. The deﬁnition and
proof are identical to those of Tripuraneni et al. (2021a;b), but differ in notation so we repeat them
here for clarity.
Deﬁnition C.1 (Restatement of Deﬁnition 2.1). Let µ1 and µ2 be LJSDs with the same marginal
distribution of λ. If the asymptotic overlap coefﬁcients are such that Eµ1 [λq|λ] /Eµ2 [λq|λ] =
Eµ1 [q|λ] /Eµ2 [q|λ] is nondecreasing in λ, we say that µ1 is more strongly aligned than µ2 and write
µ1 ≤µ2. Comparing against the case of isotropic weight distribution, µ∅, we say µ1 is aligned when
µ1 ≤µ∅and anti-aligned when µ1 ≥µ∅.
Proposition C.1. Deﬁnition 2.1 is a partial order over over weight-data alignments µ."
REFERENCES,0.23214285714285715,Proof. Reﬂexivity is satisﬁed as Eµ[q|λ]/Eµ[q|λ] = 1 is nondecreasing for all µ.
REFERENCES,0.2329192546583851,"For antisymmetry, we see µ1 ≤µ2 and µ2 ≤µ1 imply Eµ1[q|λ]/Eµ2[q|λ] is constant in λ as it is
nonincreasing and nondecreasing. However, setting Eµ1[q|λ] = cEµ2[q|λ] and taking expectation
over λ and rearranging yields 1 = Eµ1[q]/Eµ2[q] = c, so in fact Eµ1[q|λ] = Eµ2[q|λ]. Assuming
that µ1 and µ2 are absolutely continuous (the case where they are a sum of point masses is similar),
we can write their densities as pi(λ, q) = pi(λ)pi(q|λ). By assumption p1(λ) = p2(λ), so it sufﬁces
to show p1(q|λ) = p2(q|λ) almost everywhere. Next note"
REFERENCES,0.23369565217391305,"0 = Eµ1[q|λ] −Eµ2[q|λ] =
Z"
REFERENCES,0.234472049689441,"R+ q (p1(q|λ) −p2(q|λ)) dq,
(S22)"
REFERENCES,0.23524844720496896,we have that p1(q|λ) −p2(q|λ) = 0 almost everywhere.
REFERENCES,0.2360248447204969,"Finally, for transitivity assume µ1 ≤µ2 and µ2 ≤µ3, then"
REFERENCES,0.23680124223602483,"Eµ1[q|λ]
Eµ3[q|λ] = Eµ1[q|λ]"
REFERENCES,0.2375776397515528,Eµ2[q|λ] · Eµ2[q|λ]
REFERENCES,0.23835403726708074,"Eµ3[q|λ],
(S23)"
REFERENCES,0.2391304347826087,"so Eµ1[q|λ]/Eµ3[q|λ] is the product of two nondecreasing, positive functions and is thus also nonde-
creasing."
REFERENCES,0.23990683229813664,"D
PROOFS OF PROPOSITIONS"
REFERENCES,0.2406832298136646,"D.1
PROPOSITION 3.1"
REFERENCES,0.24145962732919254,"Proposition D.1 (Restatement of Proposition 3.1). In the setting of Theorem 3.1, the bias Bµ is a
nonincreasing function of overparameterization ratio φ/ψ."
REFERENCES,0.2422360248447205,Published as a conference paper at ICLR 2022
REFERENCES,0.24301242236024845,Proof. Recall from Theorem 3.1 that the bias is given by
REFERENCES,0.24378881987577639,"Bµ = φIβ
1,2 ,
(S24)"
REFERENCES,0.24456521739130435,"where x is the unique positive real root of the self-consistent equation,"
REFERENCES,0.2453416149068323,x = 1 −γτ1
REFERENCES,0.24611801242236025,"ω + I1,1
.
(S25)"
REFERENCES,0.2468944099378882,"Differentiating (S24) with respect to φ/ψ gives,"
REFERENCES,0.24767080745341616,"∂Bµ
∂(φ/ψ) = −ψ2 φ
∂Bµ"
REFERENCES,0.2484472049689441,∂ψ = 2ψ2 ∂x
REFERENCES,0.24922360248447206,"∂ψ Iβ
1,3 .
(S26)"
REFERENCES,0.25,"Since Lemma B.1 gives Iβ
a,b ≥0, it sufﬁces to show ∂x"
REFERENCES,0.25077639751552794,"∂ψ ≤0, which immediately follows by
implicitly differentiating (S25)) and simplifying the expression,"
REFERENCES,0.2515527950310559,"∂x
∂ψ = −
ρxτ1(ω + I1,1)
φ
 
1 + ρ(¯τ1 + ψ"
REFERENCES,0.25232919254658387,"φ τ1)(ω + φI1,2)
 ≤0 ,
(S27)"
REFERENCES,0.2531055900621118,"where the inequality also follows from Lemma B.1. Therefore we conclude that
∂Bµ
∂(φ/ψ) ≤0."
REFERENCES,0.25388198757763975,"D.2
PROPOSITION 3.2"
REFERENCES,0.2546583850931677,"Proposition D.2 (Restatement of Proposition 3.2). In the setting of Corollary G.1 and in the overpa-
rameterized regime (ψ < φ), the variance Vµ is a nonincreasing function of overparameterization
ratio φ/ψ."
REFERENCES,0.2554347826086957,"Proof. In the overparameterized regime, Corollary G.1 gives the expression for the variance as,"
REFERENCES,0.2562111801242236,"Vµ =
ψ
φ −ψ (σ2
ε + Iβ
1,1) +
xI2,2
ω + φI1,2
(σ2
ε + Iβ
1,2) ,
(S28)"
REFERENCES,0.25698757763975155,"and, since the self-consistent equation x =
1
ω+I1,1 is independent of ψ, we have ∂x"
REFERENCES,0.2577639751552795,"∂ψ = 0 and, ∂Vµ"
REFERENCES,0.25854037267080743,"∂ψ =
φ
(φ −ψ)2 (σ2
ε + Iβ
1,1) ≥0 ,
(S29)"
REFERENCES,0.2593167701863354,which implies that the variance is nonincreasing in the overparameterized regime.
REFERENCES,0.26009316770186336,"D.3
PROPOSITION 3.3"
REFERENCES,0.2608695652173913,"Proposition D.3 (Restatement of Proposition 3.3). Let µ1, µ2 be two LJSDs such that µ1 ≤µ2 (see
Deﬁnition 2.1). Then Bµ1 ≤Bµ2, Eµ1 ≤Eµ2, and Bµ1/Vµ1 ≤Bµ2/Vµ2."
REFERENCES,0.26164596273291924,"Proof. For the bias, Corollary B.1 implies (Iβ
1,2)1 ≤(Iβ
1,2)2 and therefore Bµ1 ≤Bµ2."
REFERENCES,0.26242236024844723,"For the test error, we use the explicit expression for the variance from Eq. (S378) and the identity
Iβ
2,2 = 1"
REFERENCES,0.26319875776397517,"xIβ
1,1 −φ"
REFERENCES,0.2639751552795031,"xIβ
1,2 which follows from Eq. (S9) to write,"
REFERENCES,0.26475155279503104,"Eµ = C0 + C1Iβ
1,1 + C2Iβ
1,2 ,
(S30)"
REFERENCES,0.265527950310559,Published as a conference paper at ICLR 2022
REFERENCES,0.266304347826087,"where the Ci ≥0 and depend on µ only through the marginal λ (i.e. they are independent of the
weight distribution):"
REFERENCES,0.2670807453416149,C0 = −ρψ
REFERENCES,0.26785714285714285,"φ
∂x
∂γ σ2
ε

(ω + φI1,2)(ω + I1,1) + φ"
REFERENCES,0.2686335403726708,"ψ γ¯τ1I2,2

≥0
(S31)"
REFERENCES,0.2694099378881988,C1 = −ρψ
REFERENCES,0.2701863354037267,"φ
∂x
∂γ"
REFERENCES,0.27096273291925466,"
(ω + φI1,2)(ω + I1,1) + γτ1"
REFERENCES,0.2717391304347826,"x (ω + φI1,2)

≥0
(S32)"
REFERENCES,0.27251552795031053,C2 = φ −ρψ
REFERENCES,0.2732919254658385,"φ
∂x
∂γ φ2"
REFERENCES,0.27406832298136646,"ψ γ¯τ1I2,2 −φγτ1"
REFERENCES,0.2748447204968944,"x
(ω + φI1,2)

(S33) = −ρψ"
REFERENCES,0.27562111801242234,"φ
∂x
∂γ φ2"
REFERENCES,0.27639751552795033,"ψ γ¯τ1I2,2 −φγτ1"
REFERENCES,0.27717391304347827,"x
(ω + φI1,2) −
φ2 ρψ ∂x ∂γ ! (S34)"
REFERENCES,0.2779503105590062,= −ργ ∂x ∂γ
REFERENCES,0.27872670807453415,"
φ¯τ1I2,2 −ψτ1"
REFERENCES,0.2795031055900621,"x (ω + φI1,2) + φ"
REFERENCES,0.2802795031055901,"ρx(1 + ρ(τ1ψ/φ + ¯τ1)(ω + φI1,2))

(S35)"
REFERENCES,0.281055900621118,= −ργ ∂x ∂γ
REFERENCES,0.28183229813664595,"
φ¯τ1I2,2 + φ"
REFERENCES,0.2826086956521739,"ρx(1 + ρ¯τ1(ω + φI1,2))

(S36)"
REFERENCES,0.2833850931677019,"≥0 .
(S37)"
REFERENCES,0.2841614906832298,"It is now straightforward to write,"
REFERENCES,0.28493788819875776,"Eµ2 −Eµ1 = C1(Iβ
1,1)2 + C2(Iβ
1,2)2 −C1(Iβ
1,1)1 + C2(Iβ
1,2)1
(S38)"
REFERENCES,0.2857142857142857,"= C1
 
(Iβ
1,1)2 −(Iβ
1,1)1

+ C2
 
(Iβ
1,2)2 −(Iβ
1,2)1

(S39)"
REFERENCES,0.2864906832298137,"≥0 ,
(S40)"
REFERENCES,0.28726708074534163,"where the inequality follows from Corollary B.1. Similarly, we can write,"
REFERENCES,0.28804347826086957,"Bµ1
Bµ2
Eµ2 −Eµ1 = C0
(Iβ
1,2)1
(Iβ
1,2)2
+ C1
(Iβ
1,2)1
(Iβ
1,2)2
(Iβ
1,1)2 + C2(Iβ
1,2)1 −C0 −C1(Iβ
1,1)1 −C2(Iβ
1,2)1 (S41)"
REFERENCES,0.2888198757763975,"= C0
 (Iβ
1,2)1
(Iβ
1,2)2
−1

+ C1
 (Iβ
1,2)1
(Iβ
1,2)2
(Iβ
1,1)2 −(Iβ
1,1)1

(S42)"
REFERENCES,0.28959627329192544,"≤0 ,
(S43)"
REFERENCES,0.29037267080745344,"where the inequality follows from Corollary B.1 and from Lemma B.3 with g : λ →φλ(φ + λx)−1
and h : λ →(φ + λx)−1. Finally, using Eµi = Bµi + Vµi, the above implies Bµ1/Vµ1 ≤Bµ2/Vµ2."
REFERENCES,0.2911490683229814,"D.4
PROPOSITION 3.4"
REFERENCES,0.2919254658385093,"Proposition D.4 (Restatement of Proposition 3.4). If the LJSD is aligned (see Deﬁnition 2.1), then,
in the setting of Corollary G.1, the test error has at most two interior critical points as a function of
the overparameterization ratio φ/ψ."
REFERENCES,0.29270186335403725,"Proof. From Corollary G.1, there is a critical point at the interpolation threshold φ/ψ = 1. Therefore
it sufﬁces to show that there is at most one additional interior critical point. Focusing ﬁrst on the
overparameterized regime φ > ψ, the test error reads,"
REFERENCES,0.29347826086956524,"Eµ = φIβ
1,2 +
ψ
φ −ψ (σ2
ε + Iβ
1,1) +
xI2,2
ω + φI1,2
(σ2
ε + Iβ
1,2) ,
(S44)"
REFERENCES,0.2942546583850932,"and, since ∂x"
REFERENCES,0.2950310559006211,"∂ψ = 0, ∂E"
REFERENCES,0.29580745341614906,"∂ψ =
φ
(φ −ψ)2 (σ2
ε + Iβ
1,1) > 0 ,
(S45)"
REFERENCES,0.296583850931677,which implies that the test error is monotone decreasing in the overparameterized regime.
REFERENCES,0.297360248447205,Published as a conference paper at ICLR 2022
REFERENCES,0.2981366459627329,"Next, let us consider the case φ < ψ. In this case,"
REFERENCES,0.29891304347826086,"Eµ = φIβ
1,2 +
φ
ψ −φ(σ2
ε + Iβ
1,1) + xIβ
2,2 ,
(S46)"
REFERENCES,0.2996894409937888,"so that,
∂Eµ"
REFERENCES,0.3004658385093168,∂ψ = φ ∂x
REFERENCES,0.30124223602484473,"∂ψ
∂
∂xIβ
1,2 −
φ
(ψ −φ)2 (σ2
ε + Iβ
1,1) +
φ
ψ −φ
∂x
∂ψ
∂
∂xIβ
1,1 + ∂x"
REFERENCES,0.30201863354037267,"∂ψ
∂
∂x(xIβ
2,2)
(S47)"
REFERENCES,0.3027950310559006,"= −
φ
(ψ −φ)2 (σ2
ε + Iβ
1,1) + ∂x ∂ψ 
φ ∂"
REFERENCES,0.30357142857142855,"∂xIβ
1,2 +
φ
ψ −φ
∂
∂xIβ
1,1 + ∂"
REFERENCES,0.30434782608695654,"∂x(xIβ
2,2)

(S48)"
REFERENCES,0.3051242236024845,"= −
φ
(ψ −φ)2 (σ2
ε + Iβ
1,1) + ∂x ∂ψ"
REFERENCES,0.3059006211180124,"
−2φIβ
2,3 −
φ
ψ −φIβ
2,2 + Iβ
2,2 −2xIβ
3,3"
REFERENCES,0.30667701863354035,"
(S49)"
REFERENCES,0.30745341614906835,"= −
φ
(ψ −φ)2 (σ2
ε + Iβ
1,1) −∂x"
REFERENCES,0.3082298136645963,"∂ψ
ψ
ψ −φIβ
2,2
(S50)"
REFERENCES,0.3090062111801242,"= −
φ
(ψ −φ)2 (σ2
ε + Iβ
1,1) +
φ
ψ(ψ −φ)
Iβ
2,2
ω + φI1,2
.
(S51)"
REFERENCES,0.30978260869565216,Therefore we see that ∂E
REFERENCES,0.3105590062111801,∂ψ = 0 implies
REFERENCES,0.3113354037267081,"φ
ψ = x(ω + I1,1) = 1 −(ω + φI1,2)σ2
ε + Iβ
1,1
Iβ
2,2
,
(S52)"
REFERENCES,0.31211180124223603,"or, equivalently, g(x) = 0 for"
REFERENCES,0.31288819875776397,"g(x) = 1 −(ω + φI1,2)σ2
ε + Iβ
1,1
Iβ
2,2
−x(ω + I1,1) .
(S53)"
REFERENCES,0.3136645962732919,"First we note that g has at most one real root since its derivative is never positive,"
REFERENCES,0.3144409937888199,"g′(x) = 2φI2,3
σ2
ε + Iβ
1,1
Iβ
2,2
+ (ω + φI1,2) "
REFERENCES,0.31521739130434784,"1 −2σ2
ε + Iβ
1,1
Iβ
3,3 !"
REFERENCES,0.3159937888198758,"−(ω + φI1,1) + xI2,2
(S54)"
REFERENCES,0.3167701863354037,"= 2φI2,3
σ2
ε + Iβ
1,1
Iβ
2,2
−2(ω + φI1,2)σ2
ε + Iβ
1,1
Iβ
3,3
(S55)"
REFERENCES,0.31754658385093165,"= 2σ2
ε + Iβ
1,1
(Iβ
2,2)2"
REFERENCES,0.31832298136645965,"
φI2,3Iβ
2,2 −(ω + φI1,2)Iβ
3,3

(S56)"
REFERENCES,0.3190993788819876,"= 2σ2
ε + Iβ
1,1
(Iβ
2,2)2"
REFERENCES,0.3198757763975155,"
φ2I2,3Iβ
2,3 −(ω + φI1,2 −xφI2,3)Iβ
3,3

(S57)"
REFERENCES,0.32065217391304346,"= 2σ2
ε + Iβ
1,1
(Iβ
2,2)2"
REFERENCES,0.32142857142857145,"
φ2I2,3Iβ
2,3 −(ω + φ2I1,3)Iβ
3,3

(S58)"
REFERENCES,0.3222049689440994,"≤2φ2 σ2
ε + Iβ
1,1
(Iβ
2,2)2"
REFERENCES,0.32298136645962733,"
I2,3Iβ
2,3 −I1,3Iβ
3,3

(S59)"
REFERENCES,0.32375776397515527,"≤0 ,
(S60)"
REFERENCES,0.3245341614906832,where the last line follows from Corollary B.2 since we are assuming µ is aligned.
REFERENCES,0.3253105590062112,"Next, regarding x as a function of φ/ψ, we consider the interval (x−, x+) for x−= x(φ/ψ =
0) and x+ = x(φ/ψ = 1). From the self-consistent equation for x, we immediately see that
x+(ω + I1,1(x+)) = 1 and x−= 0 so that"
REFERENCES,0.32608695652173914,"g(x+) = −(ω + φI1,2)σ2
ε + Iβ
1,1
Iβ
2,2
(S61)"
REFERENCES,0.3268633540372671,"< 0 .
(S62)"
REFERENCES,0.327639751552795,Published as a conference paper at ICLR 2022 and
REFERENCES,0.328416149068323,"g(x−) = 1 −(ω + φ2Eµ[λ])σ2
ε + Eµ[qλ])"
REFERENCES,0.32919254658385094,"Eµ[qλ2])
.
(S63)"
REFERENCES,0.3299689440993789,"Observe that,"
REFERENCES,0.3307453416149068,"g(x−) > 0
⇔
σ2
ε < σ2
c ≡
Eµ[qλ2]
ω + φEµ[λ] −Eµ[qλ] .
(S64)"
REFERENCES,0.33152173913043476,"Therefore, from the intermediate value theorem, we conclude that g has no real roots in (x−, x+) for
σ2
ε > σ2
c, and exactly one real root if σ2
ε < σ2
c."
REFERENCES,0.33229813664596275,"E
LINEAR REGRESSION LIMIT"
REFERENCES,0.3330745341614907,"To reduce to the linear case, we need to take ψ →0 and σ(x) →x, in which case we have that
η = ζ = ρ →1 and ω →0, so that"
REFERENCES,0.3338509316770186,"τ1 →x
and
¯τ1 →1"
REFERENCES,0.33462732919254656,"γ ,
(S65)"
REFERENCES,0.33540372670807456,so that γ = 1
REFERENCES,0.3361801242236025,"x −I1,1
(S66) = 1"
REFERENCES,0.33695652173913043,"x −φEs2∼µdata
s2"
REFERENCES,0.33773291925465837,"φ + xs2 .
(S67)"
REFERENCES,0.3385093167701863,"E.1
COMPARISON TO MEL & GANGULI (2021)"
REFERENCES,0.3392857142857143,"To compare with (Mel & Ganguli, 2021), note that φ = 1/α, γ = 1/φλ, x = τ1 = φ/˜λ, so we have λ = φ ˜λ"
REFERENCES,0.34006211180124224,"φ −φEs2∼µdata
s2˜λ/φ
˜λ + s2 ! (S68)"
REFERENCES,0.3408385093167702,"= ˜λ −φEs2∼µdata
s2˜λ
˜λ + s2 ,
(S69)"
REFERENCES,0.3416149068322981,"which is the expression appearing in Eq. (8) in (Mel & Ganguli, 2021). To compare expressions for
the test error, note that"
REFERENCES,0.3423913043478261,"∂x
∂γ →−
x
γ + φI1,2
,
(S70)"
REFERENCES,0.34316770186335405,"and so,"
REFERENCES,0.343944099378882,ρf = ∂˜λ
REFERENCES,0.3447204968944099,"∂λ
(S71)"
REFERENCES,0.3454968944099379,= ∂φ/x
REFERENCES,0.34627329192546585,"∂φγ
(S72) = −1"
REFERENCES,0.3470496894409938,"x2
∂x
∂γ
(S73)"
REFERENCES,0.34782608695652173,"=
1
x(γ + φI1,2) ,
(S74)"
REFERENCES,0.34860248447204967,Published as a conference paper at ICLR 2022
REFERENCES,0.34937888198757766,"so that,"
REFERENCES,0.3501552795031056,"E = φIβ
1,2 + 1 ρf"
REFERENCES,0.35093167701863354,"
φIβ
1,2 + σ2
ε

x2I2,2
(S75)"
REFERENCES,0.3517080745341615,"= φIβ
1,2 + 1"
REFERENCES,0.35248447204968947,"ρf
φIβ
1,2(xI1,1 −xφI1,2) + σ2
ε
ρf
x2I2,2
(S76)"
REFERENCES,0.3532608695652174,"= φIβ
1,2 + 1"
REFERENCES,0.35403726708074534,"ρf
φIβ
1,2(1 −x(γ + φI1,2)) + σ2
ε
ρf
x2I2,2
(S77)"
REFERENCES,0.3548136645962733,"= φIβ
1,2 + 1"
REFERENCES,0.3555900621118012,"ρf
φIβ
1,2(1 −ρf) + σ2
ε
ρf
x2I2,2
(S78) = 1 ρf"
REFERENCES,0.3563664596273292,"
φIβ
1,2 + σ2
εx2I2,2

.
(S79)"
REFERENCES,0.35714285714285715,"In contrast to our conventions, the error F in (Mel & Ganguli, 2021) does include an additive constant
induced by the label noise, and also normalizes by the total output variance i.e. F = E+σ2
ε
Var[y] . Taking
this relation into account and using the deﬁnitions of I and Iβ, and ﬁnally translating the notation
via the substitutions φ →1/α, λq →v = (SUT w)2,
σ2
ε
Var[y] →fn, |v|2"
REFERENCES,0.3579192546583851,"Var[y] →fs, we ﬁnd"
REFERENCES,0.358695652173913,"F = E + σ2
ε
Var [y]
(S80)"
REFERENCES,0.359472049689441,"=
σ2
ε
Var [y] + 1 ρf "
REFERENCES,0.36024844720496896,"
1
Var [y]Eµ  qλ"
REFERENCES,0.3610248447204969,"˜λ
˜λ + λ !2"
REFERENCES,0.36180124223602483,"+ φ
σ2
ε
Var [y]Eµ"
REFERENCES,0.36257763975155277,"""
λ
˜λ + λ 2#"
REFERENCES,0.36335403726708076,"
(S81)"
REFERENCES,0.3641304347826087,= fn + 1 ρf  fsEµ 
REFERENCES,0.36490683229813664,"ˆv2
 
˜λ
˜λ + λ !2"
REFERENCES,0.3656832298136646,"+ fn
1
αEµ"
REFERENCES,0.36645962732919257,"""
λ
˜λ + λ 2#"
REFERENCES,0.3672360248447205,"
(S82)"
REFERENCES,0.36801242236024845,"which is Eq. (6) of (Mel & Ganguli, 2021)."
REFERENCES,0.3687888198757764,"E.2
COMPARISON TO WU & XU (2020)"
REFERENCES,0.3695652173913043,"Wu & Xu (2020) study the case of anisotropic regularizer:
ˆβλ =
 
X⊤X + λΣw
−1 X⊤y
(S83)"
REFERENCES,0.3703416149068323,"with n samples, p features, X ∈Rn×p and p/n →γ. After simplifying the error expression they
arrive at eq. 3.1:"
REFERENCES,0.37111801242236025,"E

ˆy −˜x⊤ˆβλ
2
= ˜σ2

1 + 1"
REFERENCES,0.3718944099378882,"ntr

Σx/w

X⊤
/wX/w + λI
−1
−λΣx/w

X⊤
/wX/w + λI
−2 (S84) + λ2"
REFERENCES,0.37267080745341613,"n tr

Σx/w

X⊤
/wX/w + λI
−1
Σwβ

X⊤
/wX/w + λI
−1
(S85)"
REFERENCES,0.3734472049689441,"Setting Σw →I must give the expression for isotropic regularization, thus the effect of the weighting
matrix Σw can be accounted for by just changing the parameters of the isotropic model. The effective
feature covariance is Σ →Σx/w and the effective weight covariance is Σβ →Σwβ."
REFERENCES,0.37422360248447206,The error expression given in eqs. 4.1-4.3 is
REFERENCES,0.375,"E

˜y −˜x⊤ˆβλ
2
→m′ (−λ)"
REFERENCES,0.37577639751552794,"m2 (−λ) ·  γE
gh"
REFERENCES,0.3765527950310559,"(h · m (−λ) + 1)2 + ˜σ2
! (S86) where"
REFERENCES,0.37732919254658387,"λ =
1
m (−λ) −γE
h
1 + h · m (−λ)
(S87) 1 ="
REFERENCES,0.3781055900621118,"1
m2 (−λ) −γE
h2"
REFERENCES,0.37888198757763975,(h · m (−λ) + 1)2 !
REFERENCES,0.3796583850931677,"m′ (−λ)
(S88)"
REFERENCES,0.3804347826086957,Published as a conference paper at ICLR 2022
REFERENCES,0.3812111801242236,"In our notation, the predicted output on a new input x is"
REFERENCES,0.38198757763975155,"ˆy =

1
√n0
β⊤X + ϵtr   1"
REFERENCES,0.3827639751552795,"n1
F ⊤F + γIm"
REFERENCES,0.38354037267080743,−1  1
REFERENCES,0.3843167701863354,"n1
F ⊤f (x)

(S89)"
REFERENCES,0.38509316770186336,"→

1
√n0
β⊤X + ϵtr   1"
REFERENCES,0.3858695652173913,"n0
X⊤X + γIm −1 1"
REFERENCES,0.38664596273291924,"n0
X⊤x
(S90)"
REFERENCES,0.38742236024844723,"=

1
√n0
β⊤X + ϵtr"
REFERENCES,0.38819875776397517,"
X⊤
 1"
REFERENCES,0.3889751552795031,"n0
XX⊤+ γIn0 −1 1"
REFERENCES,0.38975155279503104,"n0
x
(S91)"
REFERENCES,0.390527950310559,"= ˆy⊤˜X⊤
˜X ˜X⊤+ φγIn0
−1
˜x
(S92)"
REFERENCES,0.391304347826087,"where ˜X has
1
√m =
1
√"
REFERENCES,0.3920807453416149,"samples normalization. Thus translating our notation involves setting φ →γ,"
REFERENCES,0.39285714285714285,"γ →λ/γ, Σ →Σx/w, λ →h, Σβ →γΣwβ, and q →γg. In this new notation, our equation for x
reads
λ = γ"
REFERENCES,0.3936335403726708,"x −γE
h"
REFERENCES,0.3944099378881988,"1 + h ·

x
γ

(S93)"
REFERENCES,0.3951863354037267,"which shows x →γm (−λ), and therefore ∂x"
REFERENCES,0.39596273291925466,∂γ →∂γm(−λ)
REFERENCES,0.3967391304347826,"∂λ/γ
= −γ2m′ (−λ). Next, note that −∂x"
REFERENCES,0.39751552795031053,"∂γ I2,2 =
x
γ + φI1,2
I2,2
(S94)"
REFERENCES,0.3982919254658385,"= I1,1 −φI1,2"
REFERENCES,0.39906832298136646,"γ + φI1,2
(S95)"
REFERENCES,0.3998447204968944,"= I1,1 + γ"
REFERENCES,0.40062111801242234,"γ + φI1,2
−1
(S96)"
REFERENCES,0.40139751552795033,"=
1/x
γ + φI1,2
−1
(S97) = −1"
REFERENCES,0.40217391304347827,"x2
∂x
∂γ −1
(S98)"
REFERENCES,0.4029503105590062,So the full error is
REFERENCES,0.40372670807453415,"E = φIβ
1,2 −∂x ∂γ"
REFERENCES,0.4045031055900621,"
φIβ
1,2I2,2 + σ2
eI2,2"
REFERENCES,0.4052795031055901,"
(S99)"
REFERENCES,0.406055900621118,"= φ

1 −∂x"
REFERENCES,0.40683229813664595,"∂γ I2,2"
REFERENCES,0.4076086956521739,"
Iβ
1,2 −σ2
e
∂x
∂γ I2,2
(S100)"
REFERENCES,0.4083850931677019,"=

−1"
REFERENCES,0.4091614906832298,"x2
∂x
∂γ"
REFERENCES,0.40993788819875776,"
φIβ
1,2 + σ2
e 
−1"
REFERENCES,0.4107142857142857,"x2
∂x
∂γ −1

(S101)"
REFERENCES,0.4114906832298137,"=

−1"
REFERENCES,0.41226708074534163,"x2
∂x
∂γ"
REFERENCES,0.41304347826086957," 
φIβ
1,2 + σ2
e

−σ2
e
(S102)"
REFERENCES,0.4138198757763975,→m′(−λ)
REFERENCES,0.41459627329192544,"m2(−λ)  γE
hg"
REFERENCES,0.41537267080745344,"(1 + m(−λ)h)2 + ˜σ2
!"
REFERENCES,0.4161490683229814,"−˜σ2
(S103)"
REFERENCES,0.4169254658385093,"which, after removing the additive shift, matches the expressions given in (Wu & Xu, 2020) eq. 4.1."
REFERENCES,0.41770186335403725,"F
STRUCTURED LEARNING CURVES"
REFERENCES,0.41847826086956524,"F.1
EFFECT OF SPECTRAL GAP"
REFERENCES,0.4192546583850932,"Here we demonstrate that a large gap in the spectrum of Σ can induce steep cliffs in the learning
curves as a function of the overparameterization φ/ψ:"
REFERENCES,0.4200310559006211,"Suppose there is a gap in the spectrum of Σ of size g. That is, there are λ−< λ+ such that there is
no eigenvalue λ ∈(λ−, λ+) and λ+"
REFERENCES,0.42080745341614906,"λ−= g. Assuming φ < 1 and µ is aligned, and working in the"
REFERENCES,0.421583850931677,Published as a conference paper at ICLR 2022
REFERENCES,0.422360248447205,"noiseless ridgeless limit, we will show the slope of the learning curve ∂log Eµ"
REFERENCES,0.4231366459627329,"∂(φ/ψ) becomes arbitrarily
negative for small ω."
REFERENCES,0.42391304347826086,"From Theorem (3.1), x, τ1 satisfy"
REFERENCES,0.4246894409937888,"x =
1 −γτ1
ω + φE
λ
φ+xλ
(S104) τ1 = q"
REFERENCES,0.4254658385093168,(ψ −φ)2 + 4xψφγ/ρ + ψ −φ
REFERENCES,0.42624223602484473,"2ψγ
(S105)"
REFERENCES,0.42701863354037267,"Since x ≤min{1,φ/ψ}"
REFERENCES,0.4277950310559006,"ω
(Eq. (S8)), for ω > 0, x stays ﬁnite in the ridgeless limit γ →0, so"
REFERENCES,0.42857142857142855,γτ1 →|ψ −φ| + ψ −φ
REFERENCES,0.42934782608695654,"2ψ
.
(S106)"
REFERENCES,0.4301242236024845,"We have the numerator 1 −γτ1 →min(1, φ/ψ), and"
REFERENCES,0.4309006211180124,"x

ω + φE
λ
φ + xλ"
REFERENCES,0.43167701863354035,"
= min

1, φ ψ"
REFERENCES,0.43245341614906835,"
.
(S107)"
REFERENCES,0.4332298136645963,"Since x = 0 is not a solution for 0 < ψ, φ < ∞, we can change variables to ˜γ = φ"
REFERENCES,0.4340062111801242,"x, giving ω 1"
REFERENCES,0.43478260869565216,"˜γ + E
λ
˜γ + λ = 1"
REFERENCES,0.4355590062111801,"φ min

1, φ ψ"
REFERENCES,0.4363354037267081,"
(S108)"
REFERENCES,0.43711180124223603,"which implies ˜γ is a continuous decreasing function of φ/ψ (keeping φ ﬁxed). Taking the limit of
(S108) directly shows that ˜γmax := limφ/ψ→0 ˜γ = ∞, while ˜γmin := limφ/ψ→∞˜γ satisﬁes"
REFERENCES,0.43788819875776397,"ω
1
˜γmin
+ E
λ
˜γmin + λ = 1"
REFERENCES,0.4386645962732919,"φ
(S109)"
REFERENCES,0.4394409937888199,"By the intermediate value theorem, ˜γ takes all values in the interval (˜γmin, ∞). For φ < 1, using
E
λ
˜γmin+λ ≤1 we obtain ˜γmin ≤ω
φ
1−φ."
REFERENCES,0.44021739130434784,"We assume that ω
φ
1−φ ≤λ−, so the previous bound gives ˜γmin ≤λ−and thus ˜γ attains all values
in (λ−, λ+). In particular, there is some 0 < φ/ψ < 1 such that ˜γ (φ/ψ) =
p"
REFERENCES,0.4409937888198758,"λ−λ+. At this point,
differentiating (S108) gives −˜γ ∂"
REFERENCES,0.4417701863354037,"∂˜γ
1
ψ = ω 1"
REFERENCES,0.44254658385093165,"˜γ + ˜γE
λ"
REFERENCES,0.44332298136645965,"(˜γ + λ)2
(S110) ≤ω 1"
REFERENCES,0.4440993788819876,˜γ + ˜γ
REFERENCES,0.4448757763975155,"λ+
(˜γ + λ+)2 p (λ ≥λ+) +
λ−
(˜γ + λ−)2 p (λ ≤λ−) !"
REFERENCES,0.44565217391304346,(S111) = ω 1
REFERENCES,0.44642857142857145,"˜γ +
√g
 √g + 1
2
(S112)"
REFERENCES,0.4472049689440994,Since −˜γ ∂
REFERENCES,0.44798136645962733,"∂˜γ
1
ψ = 1"
REFERENCES,0.44875776397515527,"φ

∂log x
∂(φ/ψ)
−1
, we get 1
φ
1 ω 1"
REFERENCES,0.4495341614906832,"˜γ +
√g
(√g+1)
2
≤∂log x"
REFERENCES,0.4503105590062112,"∂(φ/ψ)
(S113)"
REFERENCES,0.45108695652173914,"For large spectral gap g this tends toward 1
φ p λ+λ−"
REFERENCES,0.4518633540372671,"ω
≤∂log x"
REFERENCES,0.452639751552795,"∂(φ/ψ)
(S114)"
REFERENCES,0.453416149068323,Published as a conference paper at ICLR 2022
REFERENCES,0.45419254658385094,"If the nonlinearity ω is small compared to the middle of the spectral gap
p"
REFERENCES,0.4549689440993789,"λ+λ−, x undergoes large
fractional change as a function of the overparameterization ratio φ/ψ."
REFERENCES,0.4557453416149068,"To see how this affects the test error, we can use the lowering identity Iβ
a−1,b−1 = φIβ
a−1,b + xIβ
a,b
to write the ridgeless error expression from Eq. (S46) as"
REFERENCES,0.45652173913043476,"Eµ = φIβ
1,2 +
φ
ψ −φ"
REFERENCES,0.45729813664596275,"
σ2
ϵ + Iβ
1,1

+ xIβ
2,2
(S115)"
REFERENCES,0.4580745341614907,"=
φ
ψ −φσ2
ϵ +
ψ
ψ −φIβ
1,1.
(S116)"
REFERENCES,0.4588509316770186,"So we can write
∂
∂(φ/ψ) log

Eµ −
φ
ψ −φσ2
ϵ"
REFERENCES,0.45962732919254656,"
=
∂
∂(φ/ψ) log
ψ
ψ −φIβ
1,1
(S117)"
REFERENCES,0.46040372670807456,"=
ψ
ψ −φ +
∂
∂(φ/ψ) log Iβ
1,1
(S118)"
REFERENCES,0.4611801242236025,"For general a, b, we have"
REFERENCES,0.46195652173913043,"∂
∂(φ/ψ) log Iβ
a,b = −b
 ∂log x"
REFERENCES,0.46273291925465837,∂(φ/ψ)
REFERENCES,0.4635093167701863," E
h
λa+1"
REFERENCES,0.4642857142857143,"(˜γ+λ)b+1 q
i"
REFERENCES,0.46506211180124224,"E
h
λa
(˜γ+λ)b q
i
(S119)"
REFERENCES,0.4658385093167702,"Specializing to a = b = 1, and using the fact that
λ
˜γ+λEq [q|λ] is a nondecreasing function of λ
(guaranteed since q is aligned), we may apply the Harris inequality to obtain −∂"
REFERENCES,0.4666149068322981,"∂ψ log Iβ
1,1 =
 ∂log x"
REFERENCES,0.4673913043478261,∂(φ/ψ)
REFERENCES,0.46816770186335405," Eλ
h
λ
˜γ+λ

λ
˜γ+λEq [q|λ]
i"
REFERENCES,0.468944099378882,"Eλ
h
λ
(˜γ+λ)Eq [q|λ]
i
(S120)"
REFERENCES,0.4697204968944099,"≥
 ∂log x"
REFERENCES,0.4704968944099379,"∂(φ/ψ) 
Eλ"
REFERENCES,0.47127329192546585,"
λ
˜γ + λ"
REFERENCES,0.4720496894409938,"
(S121)"
REFERENCES,0.47282608695652173,"g→∞
−−−→
 ∂log x"
REFERENCES,0.47360248447204967,∂(φ/ψ)
REFERENCES,0.47437888198757766,"
p (λ > λ+)
(S122) ≥1 φ p λ+λ−"
REFERENCES,0.4751552795031056,"ω
p (λ > λ+) ,
(S123)"
REFERENCES,0.47593167701863354,which implies
REFERENCES,0.4767080745341615,"−
∂
∂(φ/ψ) log

Eµ −
φ
ψ −φσ2
ϵ"
REFERENCES,0.47748447204968947,"
= −
ψ
ψ −φ −
∂
∂(φ/ψ) log Iβ
1,1
(S124)"
REFERENCES,0.4782608695652174,"≥−
ψ
ψ −φ + 1 φ p λ+λ−"
REFERENCES,0.47903726708074534,"ω
p (λ > λ+)
(S125)"
REFERENCES,0.4798136645962733,"In particular, if σ2
ε = 0, then"
REFERENCES,0.4805900621118012,−∂log Eµ
REFERENCES,0.4813664596273292,"∂(φ/ψ) ≥−
ψ
ψ −φ + 1 φ p λ+λ−"
REFERENCES,0.48214285714285715,"ω
p (λ > λ+)
(S126)"
REFERENCES,0.4829192546583851,"Thus as ω →0 the learning curve becomes arbitrarily steep at the critical value x = φ/
p λ+λ−."
REFERENCES,0.483695652173913,"F.2
ANALYSIS OF THE D-SCALE MODEL IN THE SEPARATED LIMIT"
REFERENCES,0.484472049689441,We will consider the d-scale covariance model:
REFERENCES,0.48524844720496896,"λn = Cαn,
pn = 1"
REFERENCES,0.4860248447204969,"d,
n = 0, 1, · · · , d −1
(S127)"
REFERENCES,0.48680124223602483,where C is chosen so that
REFERENCES,0.48757763975155277,"1 = s = ¯tr [Σ] = 1 d d−1
X"
REFERENCES,0.48835403726708076,"n=0
Cαn = C 1"
REFERENCES,0.4891304347826087,"d
1 −αd"
REFERENCES,0.48990683229813664,"1 −α
(S128)"
REFERENCES,0.4906832298136646,Published as a conference paper at ICLR 2022
REFERENCES,0.49145962732919257,"We will obtain expressions for x in the limit of small λ. Consider the ridgeless limit of ˜γ := φ/x:
1
˜γ ω +
X"
REFERENCES,0.4922360248447205,"n
pn
λn
˜γ + λn
=
1
max (φ, ψ)
(S129)"
REFERENCES,0.49301242236024845,"Suppose, ω sits between the scales Cαj, Cαj+1. To enforce this constraint, we will take ω = ˆωαj+ 1"
REFERENCES,0.4937888198757764,"2
where ˆω is a constant independent of α."
REFERENCES,0.4945652173913043,"The α scaling of ˜γ will depend on the value of max(φ, ψ). Discarding the second term in (S129) we
obtain max (φ, ψ) ω ≤˜γ, and thus the lowest possible scaling for ˜γ is ˜γ = Cj+ 1"
REFERENCES,0.4953416149068323,2 αj+ 1
REFERENCES,0.49611801242236025,"2 . Substituting
this ansatz into (S129) and taking the limit α →0, we obtain
1
max (φ, ψ) = 1"
REFERENCES,0.4968944099378882,˜γ ω + 1 d X n Cαn Cj+ 1
REFERENCES,0.49767080745341613,2 αj+ 1
REFERENCES,0.4984472049689441,"2 + Cαn
(S130)"
REFERENCES,0.49922360248447206,"α→0
−−−→1"
REFERENCES,0.5,˜γ ω + j + 1
REFERENCES,0.5007763975155279,"d
(S131)"
REFERENCES,0.5015527950310559,"Solving for ˜γ gives ˜γ =
max(φ,ψ)
1−max(φ,ψ) j+1"
REFERENCES,0.5023291925465838,"d ω. For other values of max(φ, ψ), ˜γ may have higher scaling,"
REFERENCES,0.5031055900621118,"ie. ˜γ = Ckαk with k ≤j. Substituting and solving for ˜γ we obtain ˜γ = max(φ,ψ) k+1"
REFERENCES,0.5038819875776398,"d −1
1−max(φ,ψ) k"
REFERENCES,0.5046583850931677,"d λk. Thus we
obtain the following self-consistent solutions for ˜γ: ˜γ = 
 "
REFERENCES,0.5054347826086957,"max(φ,ψ)
1−max(φ,ψ) j+1"
REFERENCES,0.5062111801242236,"d ω
max (φ, ψ) <
d
j+1
max(φ,ψ) k+1"
REFERENCES,0.5069875776397516,"d −1
1−max(φ,ψ) k"
REFERENCES,0.5077639751552795,"d λk
d
k+1 < max (φ, ψ) < d"
REFERENCES,0.5085403726708074,"k
(S132)"
REFERENCES,0.5093167701863354,"Thus ˜γ takes on the scale of a single eigenvalue λk for a range of overparameterization ratios
corresponding to
d
k+1 < ψ max

φ
ψ, 1

< d"
REFERENCES,0.5100931677018633,"k. To understand what happens at the transitions between
these regimes, we can apply the results from the previous subsection F.1 for generic Σ with large
spectral gap. In the notation of F.1, the D-scale model has a spectral gap between each pair of
consecutive scales of size g = λj/λj+1 = Cαj/Cαj+1 = 1/α and as a consequence, ˜γ will exhibit
near inﬁnite slop as it passes through the middle of a gap
p"
REFERENCES,0.5108695652173914,λj+1λj = Cαj+ 1
REFERENCES,0.5116459627329193,"2 . Comparing to the
self-consistent solutions (S132) these transitions must happen at the critical values max(φ, ψ) =
d
k+1
for k ≤j. At these transition points, the error exhibits steep cliffs in the parameter regime descried
in F.1."
REFERENCES,0.5124223602484472,"G
PROOF OF THEOREM 3.1"
REFERENCES,0.5131987577639752,"The proof closely follows the methods described in (Adlam et al., 2019; Adlam & Pennington,
2020a;b; Tripuraneni et al., 2021a;b). Indeed, precisely the same techniques from operator-valued
free probability used in those works apply here. The main and only difference is the anisotropic
weight covariance Σβ, which changes the details of the computations but not the arguments justifying
the linearized Gaussian equivalents and the application of operator-valued free probability. We
therefore refer the reader to those previous works for an in-depth discussion of methods and merely
focus here on the details of the requisite calculations. Throughout this section, we use ¯tr to denote
the dimension-normalized trace, i.e. ¯tr(A) = 1"
REFERENCES,0.5139751552795031,ntr(A) for a matrix A ∈Rn×n.
REFERENCES,0.514751552795031,"G.1
DECOMPOSITION OF THE TEST LOSS"
REFERENCES,0.515527950310559,"The test loss can be written as,
EΣ∗= E(x,y)(y −ˆy(x))2 = E1 + E2 + E3
(S133)
with
E1 = E(x,ε)tr(y(x)y(x)⊤)
(S134)"
REFERENCES,0.5163043478260869,"E2 = −2E(x,ε)tr(K⊤
x K−1Y ⊤y(x))
(S135)"
REFERENCES,0.5170807453416149,"E3 = E(x,ε)tr(K⊤
x K−1Y ⊤Y K−1Kx) .
(S136)"
REFERENCES,0.5178571428571429,Published as a conference paper at ICLR 2022
REFERENCES,0.5186335403726708,"Recall the kernels K = K(X, X) and Kx = K(X, x) are given by,"
REFERENCES,0.5194099378881988,K = F ⊤F
REFERENCES,0.5201863354037267,"n1
+ γIm
and
Kx = 1"
REFERENCES,0.5209627329192547,"n1
F ⊤f .
(S137)"
REFERENCES,0.5217391304347826,"Using the cyclicity and linearity of the trace, the expectation over x requires the computation of"
REFERENCES,0.5225155279503105,"ExKxK⊤
x ,
Exy(x)K⊤
x ,
Exy(x)y(x)⊤.
(S138)"
REFERENCES,0.5232919254658385,"As described in detail in (Tripuraneni et al., 2021a;b; Adlam et al., 2019; Adlam & Pennington,
2020a; Mei & Montanari, 2019), asympotically the trace terms E1, E2, and E3 are invariant to a
linearization of the random feature vector f,"
REFERENCES,0.5240683229813664,"f →f lin =
√ρ
√n0
Wx +
p"
REFERENCES,0.5248447204968945,"η −ζθ ,
(S139)"
REFERENCES,0.5256211180124224,"where θ ∈Rn1 is a vector of iid standard normal variates. Similarly, we will take the linearization of
the training features to be
√ρ
√n0 WX +√η −ζΘ where Θ ∈Rn1×m has standard normal components.
The expectations over x are now trivial and we readily ﬁnd,"
REFERENCES,0.5263975155279503,"ExKxK⊤
x = 1"
REFERENCES,0.5271739130434783,"n2
1
F ⊤  ρ"
REFERENCES,0.5279503105590062,"n0
WΣW ⊤+ (η −ζ)In1

F
(S140)"
REFERENCES,0.5287267080745341,"Exy(x)K⊤
x =
√ρ
n0n1
β⊤ΣW ⊤F
(S141)"
REFERENCES,0.5295031055900621,Exy(x)y(x)⊤= 1
REFERENCES,0.53027950310559,"n0
βΣβ⊤
(S142)"
REFERENCES,0.531055900621118,"Next, we recall the deﬁnition, Y = β⊤X/√n0 + ϵ, and, using the above substitution, we ﬁnd"
REFERENCES,0.531832298136646,"Eϵ

Y ⊤Y

= 1"
REFERENCES,0.532608695652174,"n0
X⊤ΣβX + σ2
εIm
(S143)"
REFERENCES,0.5333850931677019,"Eϵ

Y ⊤Exy(x)K⊤
x

=
√ρ"
REFERENCES,0.5341614906832298,"n3/2
0
n1
X⊤ΣβΣW ⊤F .
(S144)"
REFERENCES,0.5349378881987578,"Putting these pieces together, we have"
REFERENCES,0.5357142857142857,E1 = tr(ΣβΣ)
REFERENCES,0.5364906832298136,"n0
(S145)"
REFERENCES,0.5372670807453416,"E2 = E21
(S146)
E3 = E31 + E32 ,
(S147)"
REFERENCES,0.5380434782608695,"where,"
REFERENCES,0.5388198757763976,"E21 = −2
√ρ"
REFERENCES,0.5395962732919255,"n3/2
0
n1
Etr
 
X⊤ΣβΣW ⊤FK−1
(S148)"
REFERENCES,0.5403726708074534,"E31 = σ2
εEtr
 
K−1Σ3K−1
(S149)"
REFERENCES,0.5411490683229814,E32 = 1
REFERENCES,0.5419254658385093,"n0
Etr
 
K−1Σ3K−1X⊤ΣβX

(S150) and,"
REFERENCES,0.5427018633540373,"Σ3 =
ρ
n0n2
1
F ⊤WΣW ⊤F + η −ζ"
REFERENCES,0.5434782608695652,"n2
1
F ⊤F .
(S151)"
REFERENCES,0.5442546583850931,"G.2
DECOMPOSITION OF THE BIAS AND TOTAL VARIANCE"
REFERENCES,0.5450310559006211,"Note that it is sufﬁcient to calculate the bias term given the total test loss, since the total variance
can be obtained as VΣ = EΣ −BΣ. Following the total multivariate bias-variance decomposition"
REFERENCES,0.5458074534161491,Published as a conference paper at ICLR 2022
REFERENCES,0.546583850931677,"of (Adlam & Pennington, 2020b), for each random variable in question we introduce an iid copy of it
denoted by either the subscript 1 or 2. We can then write,
BΣ = E(x,y)(y −E(W,X,ε)ˆy(x; W, X, ε))2
(S152)"
REFERENCES,0.547360248447205,"= E(x,y)E(W1,X1,ε1)E(W2,X2,ϵ2)(y −ˆy(x; W1, X1, ε1))(y −ˆy(x; W2, X2, ϵ2))
(S153)"
REFERENCES,0.5481366459627329,= tr(ΣβΣ)
REFERENCES,0.5489130434782609,"n0
+ E21 + H000 ,
(S154)"
REFERENCES,0.5496894409937888,"where an expression for E21 was given previously and H000 satisﬁes
H000 = Eˆy(x; W1, X1, ε1)ˆy(x; W2, X2, ϵ2) ,
(S155)
where the expectations are over x, W1, X1, ε1, W2, X2, and ϵ2. Recalling the deﬁnition of ˆy,
ˆy(x; W, X, ε) := Y (X, ϵ)K(X, X; W)−1K(X, x; W)
(S156)
and the techniques described in the previous section, it is straightforward to analyze the above term.
First note we can write,"
REFERENCES,0.5504658385093167,"ExK(X1, x; W1)K(x, X2; W2) =
ρ
n0n2
1
F ⊤
11W1ΣW ⊤
2 F22 .
(S157)"
REFERENCES,0.5512422360248447,"Here we have deﬁned F11 ≡F(W1, X1) and F22 ≡F(W2, X2). Now we proceed to calculate H000
as
H000 = Eˆy(x; W1, X1, ε)ˆy(x; W2, X2, ϵ2)
(S158)"
REFERENCES,0.5520186335403726,"= EK(x, X2; W2)K(X2, X2; W2)−1Y (X2, ϵ2)⊤Y (X1, ε1)K(X1, X1; W1)−1K(X1, x; W)
(S159)"
REFERENCES,0.5527950310559007,"= Etr
 
K(X2, X2; W2)−1X⊤
2 X1K(X1, X1; W1)−1K(X1, x; W)K(x, X2; W2)

(S160)"
REFERENCES,0.5535714285714286,"=
ρ
n2
0n2
1
Etr
 
K−1
22 X⊤
2 ΣβX1K−1
11 F ⊤
11W1ΣW ⊤
2 F22

(S161)"
REFERENCES,0.5543478260869565,"≡E4 ,
(S162)
where in the second-to-last line we have deﬁned K11
≡
K(X1, X1; W1) and K22
≡
K(X2, X2; W2)."
REFERENCES,0.5551242236024845,"G.3
SUMMARY OF LINEARIZED TRACE TERMS"
REFERENCES,0.5559006211180124,"We now summarize the requisite terms needed to compute the total test error, bias, and variance after
using cyclicity of the trace to rearrange several of them. In the following, we slightly change notation
in order to make explicit the dependence on the covariance matrix Σ. To be speciﬁc, whereas above
we assumed that the columns of X1 and X2 were drawn from multivariate Gaussians with covariance
Σ, below we assume that they are drawn from multivariate Gausssians with identity covariance. This
change is equivalent to replacing X1 →Σ1/2X1 and X2 →Σ1/2X2 in the above expressions. We
utilize this deﬁnition so that X1, X2, W1, W2, and Θ all have iid standard Gaussian entries. From
the previous computations, we can now write the requisite terms as,"
REFERENCES,0.5566770186335404,"Σ3
=
ρ
n0n2
1
F ⊤
11W1ΣW ⊤
1 F11 + η −ζ"
REFERENCES,0.5574534161490683,"n2
1
F ⊤
11F11
(S163)"
REFERENCES,0.5582298136645962,"E21
=
−2
√ρ"
REFERENCES,0.5590062111801242,"n3/2
0
n1
tr

X⊤
1 Σ1/2ΣβΣW ⊤
1 F11K−1
11
"
REFERENCES,0.5597826086956522,(S164)
REFERENCES,0.5605590062111802,"E31
=
σ2
ϵ tr
 
K−1
11 Σ3K−1
11
"
REFERENCES,0.5613354037267081,(S165)
REFERENCES,0.562111801242236,"E32
=
1
n0
tr

K−1
11 Σ3K−1
11 X⊤
1 Σ1/2ΣβΣ1/2X1
"
REFERENCES,0.562888198757764,(S166)
REFERENCES,0.5636645962732919,"E4
=
ρ
n2
0n2
1
tr

F22K−1
22 X⊤
2 Σ1/2ΣβΣ1/2X1K−1
11 F ⊤
11W1ΣW ⊤
2
"
REFERENCES,0.5644409937888198,(S167)
REFERENCES,0.5652173913043478,"EΣ
=
1
n0
tr (ΣΣβ) + E21 + E31 + E32
(S168)"
REFERENCES,0.5659937888198758,"BΣ
=
1
n0
tr (ΣΣβ) + E21 + E4
(S169)"
REFERENCES,0.5667701863354038,"VΣ
=
EΣ −BΣ
(S170)"
REFERENCES,0.5675465838509317,Published as a conference paper at ICLR 2022
REFERENCES,0.5683229813664596,"G.4
CALCULATION OF ERROR TERMS"
REFERENCES,0.5690993788819876,"To compute the test error, bias, and total variance, we need to evaluate the asymptotic trace objects
appearing in the expressions for E21, E31, E32, and E4, deﬁned in the previous section. As these
expressions are essentially rational functions of the random matrices X, W, Θ, Σ, and Σβ, these
computations can be accomplished by representing the rational functions as single blocks of a suitably-
deﬁned block matrix inverse - the so-called linear pencil method (see eg . Far et al., 2006) - and then
applying the theory of operator-valued free probability (Mingo & Speicher, 2017). These techniques
and their application to problems of this type have been well-established elsewhere (Adlam et al.,
2019; Adlam & Pennington, 2020a;b), we only lightly sketch the mathematical details, referring the
reader to the literature for a more pedagogical overview. Instead, we focus on presenting the details
of the requisite calculations."
REFERENCES,0.5698757763975155,"Relative to prior work, the main challenge in the current setting is generalizing the calculations to
include an arbitrary weight covariance matrix Σβ. This generalization is facilitated by the general
theory of operator-valued free probability, and in particular through the subordinated form of the
operator-valued self-consistent equations that we ﬁrst present in eqn. (S201). The form of this
equation enables the simple computation of the operator-valued R-transform of the remaining random
matrices, W, X, and Θ, which are all iid Gaussian and can therefore be obtained simply by using
the methods of (Far et al., 2006). The remaining complication amounts to performing the trace in
eqn. (S201), which asymptotically becomes an integral over the LJSD µ. While this might in general
lead to a complicated coupling of many transcendental equations, it turns out that the trascendentality
can be entirely factored into a single scalar ﬁxed-point equation, whose solution we denote by x (see
eqn. (S237)), and the remaining equations are purely algebraic given x. To facilitate this particular
simpliﬁcation, it is necessary to ﬁrst compute all of the entries in the operator-valued Stieltjes
transform of the kernel matrix K, which we do in Sec. G.4.1. Using these results, we compute the
remaining error terms in the subsequent sections."
REFERENCES,0.5706521739130435,"As a matter of notation, note that throughout this entire section whenever a matrix X, X1, or X2
appears it is composed of iid N(0, 1) entries as in Appendix G.3. This differs from the notation
of the main paper, but we follow this prescription to ease the already cumbersome presentation.
This deﬁnition of X allows us to explicitly extract and represent the training covariance Σ in our
calculations."
REFERENCES,0.5714285714285714,"G.4.1
K−1"
REFERENCES,0.5722049689440993,"The NCAlgebra Mathematica package (NCRealization method; algorithm described in Helton et al.,
2006) was used to generate the following matrix pencil QK−1:"
REFERENCES,0.5729813664596274,QK−1 = 
REFERENCES,0.5737577639751553,"














 Im"
REFERENCES,0.5745341614906833,√η−ζΘ⊤ γ√n1 √ρX⊤
REFERENCES,0.5753105590062112,"γ√n0
0
0
0
0
0
0"
REFERENCES,0.5760869565217391,"−Θ√η−ζ
√n1
In1
0
0
−
√ρW
√n1
0
0
0
0
0
0
In0
−Σ1/2
0
0
0
0
0
0
−W ⊤"
REFERENCES,0.5768633540372671,"√n1
0
In0
0
0
Σβ
√ρ
0
0
0
0
0
0
In0
−Σ1/2
0
0
0
−X
√n0
0
0
0
0
In0
0
0
0
0
0
0
0
0
0
In0
−Σ1/2
0
0
0
0
0
0
0
0
In0
−X
√n0
0
0
0
0
0
0
0
0
Im "
REFERENCES,0.577639751552795,"














 ."
REFERENCES,0.578416149068323,"(S171)
This matrix is speciﬁcally chosen so that inverting [QK−1]⊤and taking the normalized trace of its
ﬁrst block gives exactly γ ¯trK−1, the quantity of interest. Computing the full inverse of [QK−1]⊤
via repeated applications of the Schur complement formula and taking block-wise traces shows that"
REFERENCES,0.5791925465838509,"GK−1
1,1
=
γ ¯tr(K−1)
(S172)"
REFERENCES,0.5799689440993789,"GK−1
9,1
=
φ ¯tr
 
ΣβΣ1/2XK−1X⊤Σ1/2 n0"
REFERENCES,0.5807453416149069,(S173)
REFERENCES,0.5815217391304348,"GK−1
2,2
=
γ ¯tr( ˆK−1)
(S174)"
REFERENCES,0.5822981366459627,Published as a conference paper at ICLR 2022
REFERENCES,0.5830745341614907,"GK−1
3,3
=
GK−1
6,6
= 1 −
√ρ ¯tr
 
Σ1/2W ⊤FK−1X⊤ √n0n1"
REFERENCES,0.5838509316770186,(S175)
REFERENCES,0.5846273291925466,"GK−1
4,3
=
GK−1
6,5
= ¯tr(Σ1/2) −
√ρ ¯tr
 
ΣW ⊤FK−1X⊤ √n0n1"
REFERENCES,0.5854037267080745,(S176)
REFERENCES,0.5861801242236024,"GK−1
5,3
=
GK−1
6,4
=
γ√ρ ¯tr

Σ1/2W ⊤ˆK−1W
 n1"
REFERENCES,0.5869565217391305,(S177)
REFERENCES,0.5877329192546584,"GK−1
6,3
=
γ√ρ ¯tr

ΣW ⊤ˆK−1W
 n1"
REFERENCES,0.5885093167701864,(S178)
REFERENCES,0.5892857142857143,"GK−1
7,3
=
¯tr
 
ΣβΣ1/2W ⊤FK−1X⊤Σ1/2"
REFERENCES,0.5900621118012422,"√n0n1
−
¯tr(ΣβΣ1/2)
√ρ
(S179)"
REFERENCES,0.5908385093167702,"GK−1
8,3
=
¯tr
 
ΣβΣW ⊤FK−1X⊤Σ1/2"
REFERENCES,0.5916149068322981,"√n0n1
−
¯tr(ΣβΣ)
√ρ
(S180)"
REFERENCES,0.592391304347826,"GK−1
3,4
=
GK−1
5,6
= −
√ρ ¯tr
 
FK−1X⊤W ⊤"
REFERENCES,0.593167701863354,"√n0n1ψ
(S181)"
REFERENCES,0.593944099378882,"GK−1
4,4
=
GK−1
5,5
= 1 −
√ρ ¯tr
 
Σ1/2W ⊤FK−1X⊤ √n0n1"
REFERENCES,0.59472049689441,(S182)
REFERENCES,0.5954968944099379,"GK−1
5,4
=
γ√ρ ¯tr

ˆK−1WW ⊤"
REFERENCES,0.5962732919254659,"n1ψ
(S183)"
REFERENCES,0.5970496894409938,"GK−1
7,4
=
¯tr

ΣβΣ1/2XF ⊤ˆK−1W
"
REFERENCES,0.5978260869565217,"√n0n1
−
¯tr(Σβ)
√ρ
(S184)"
REFERENCES,0.5986024844720497,"GK−1
8,4
=
¯tr
 
ΣβΣ1/2W ⊤FK−1X⊤Σ1/2"
REFERENCES,0.5993788819875776,"√n0n1
−
¯tr(ΣβΣ1/2)
√ρ
(S185)"
REFERENCES,0.6001552795031055,"GK−1
3,5
=
GK−1
4,6
= −
√ρ ¯tr
 
Σ1/2XK−1X⊤ n0"
REFERENCES,0.6009316770186336,(S186)
REFERENCES,0.6017080745341615,"GK−1
4,5
=
−
√ρ ¯tr
 
ΣXK−1X⊤ n0"
REFERENCES,0.6024844720496895,(S187)
REFERENCES,0.6032608695652174,"GK−1
7,5
=
¯tr
 
ΣβΣ1/2XK−1X⊤Σ1/2 n0"
REFERENCES,0.6040372670807453,(S188)
REFERENCES,0.6048136645962733,"GK−1
8,5
=
¯tr
 
ΣβΣXK−1X⊤Σ1/2 n0"
REFERENCES,0.6055900621118012,(S189)
REFERENCES,0.6063664596273292,"GK−1
3,6
=
−
√ρ ¯tr
 
K−1X⊤X
"
REFERENCES,0.6071428571428571,"n0φ
(S190)"
REFERENCES,0.6079192546583851,"GK−1
7,6
=
¯tr
 
ΣβΣ1/2XK−1X⊤ n0"
REFERENCES,0.6086956521739131,(S191)
REFERENCES,0.609472049689441,"GK−1
8,6
=
¯tr
 
ΣβΣ1/2XK−1X⊤Σ1/2 n0"
REFERENCES,0.610248447204969,(S192)
REFERENCES,0.6110248447204969,"GK−1
7,7
=
GK−1
8,8
= GK−1
9,9
= 1
(S193)"
REFERENCES,0.6118012422360248,"GK−1
8,7
=
¯tr(Σ1/2) ,
(S194)"
REFERENCES,0.6125776397515528,"where GK−1 := id9 ⊗¯tr [(QK−1)⊤]−1 ∈M9(C) is a scalar 9 × 9 matrix whose i, j entry GK−1
i,j
is the normalized trace of the (i, j)-block of the inverse of [QK−1]⊤. We have also deﬁned ˆK =
1
n1 FF ⊤+ γIn1 (note that K is m × m while ˆK is n1 × n1). It is straightforward to verify that when"
REFERENCES,0.6133540372670807,Published as a conference paper at ICLR 2022
REFERENCES,0.6141304347826086,"the n0, n1, m →∞limit is eventually taken, each entry of GK−1 is properly scaled and will tend
toward a ﬁnite value."
REFERENCES,0.6149068322981367,"We aim to compute the limiting values of these trace terms as n0, n1, m →∞, as they will be related
to the error terms of interest. To proceed, recall that the asymptotic block-wise traces of the inverse
of QK−1 can be determined from its operator-valued Stieltjes transform (Mingo & Speicher, 2017).
The simplest way to apply the results of (Far et al., 2006; Mingo & Speicher, 2017) is to augment
QK−1 to form the the self-adjoint matrix ¯QK−1,"
REFERENCES,0.6156832298136646,¯QK−1 =
REFERENCES,0.6164596273291926,"0
[QK−1]
⊤"
REFERENCES,0.6172360248447205,"QK−1
0 !"
REFERENCES,0.6180124223602484,",
(S195)"
REFERENCES,0.6187888198757764,"and observe that we can write ¯QK−1 as,"
REFERENCES,0.6195652173913043,"¯QK−1 = ¯Z −¯QK−1
W,X,Θ −¯QK−1
Σ"
REFERENCES,0.6203416149068323,"=

0
I9
I9
0 
−"
REFERENCES,0.6211180124223602,"0
[QK−1
W,X,Θ]
⊤"
REFERENCES,0.6218944099378882,"QK−1
W,X,Θ
0 ! −"
REFERENCES,0.6226708074534162,"0
[QK−1
Σ
]
⊤"
REFERENCES,0.6234472049689441,"QK−1
Σ
0 !"
REFERENCES,0.6242236024844721,",
(S196) where"
REFERENCES,0.625,"QK−1
W,X,Θ
=
− "
REFERENCES,0.6257763975155279,"













"
REFERENCES,0.6265527950310559,"0
√η−ζΘ⊤ γ√n1 √ρX⊤"
REFERENCES,0.6273291925465838,"γ√n0
0
0
0
0
0
0"
REFERENCES,0.6281055900621118,"−Θ√η−ζ
√n1
0
0
0
−
√ρW
√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−W ⊤"
REFERENCES,0.6288819875776398,"√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−X
√n0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−X
√n0
0
0
0
0
0
0
0
0
0 "
REFERENCES,0.6296583850931677,"













"
REFERENCES,0.6304347826086957,(S197)
REFERENCES,0.6312111801242236,"QK−1
Σ
=
− "
REFERENCES,0.6319875776397516,"











"
REFERENCES,0.6327639751552795,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0
0
0
0
0
0
0
0
0
Σβ
√ρ
0
0
0
0
0
0
0
−Σ1/2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 "
REFERENCES,0.6335403726708074,"











"
REFERENCES,0.6343167701863354,",
(S198)"
REFERENCES,0.6350931677018633,"and the addition in (S196) is performed block-wise. Note that we have separated the iid Gaussian
matrices W, X, Θ from the constant terms and from the Σ-dependent terms. Denote by ¯GK−1 ∈
M18(C) the block matrix"
REFERENCES,0.6358695652173914,¯GK−1 =
REFERENCES,0.6366459627329193,"0
[GK−1]
⊤"
REFERENCES,0.6374223602484472,"GK−1
0 !"
REFERENCES,0.6381987577639752,"= id18 ⊗¯tr

¯QK−1−1
,
(S199)"
REFERENCES,0.6389751552795031,"and by ¯GK−1
Σ
∈M18(C) the operator-valued Stieltjes transform of ¯QK−1
Σ
. Using (S196) and the
deﬁnition of the operator-valued Stieltjes transform G ¯
QK−1
W,X,Θ+ ¯
QK−1
Σ
, we can write"
REFERENCES,0.639751552795031,"¯GK−1 = id18 ⊗¯tr

¯Z −¯QK−1
W,X,Θ −¯QK−1
Σ
−1
= G ¯
QK−1
W,X,Θ+ ¯
QK−1
Σ
( ¯Z)
(S200)"
REFERENCES,0.640527950310559,"Thus using the subordinated form of the equations for addition of free variables (Mingo & Speicher,
2017; section 9.2 Thm. 11), and the deﬁning equation for ¯GK−1
Σ
, the operator-valued theory of
free probability shows that in the limit n0, n1, m →∞, the Stieltjes transform ¯GK−1 satisﬁes the"
REFERENCES,0.6413043478260869,Published as a conference paper at ICLR 2022
REFERENCES,0.6420807453416149,following 18 × 18 matrix equation:
REFERENCES,0.6428571428571429,"¯GK−1 = ¯GK−1
Σ
( ¯Z −¯RK−1
W,X,Θ( ¯GK−1))"
REFERENCES,0.6436335403726708,"= id ⊗¯tr

¯Z −¯RK−1
W,X,Θ( ¯GK−1) −¯QK−1
Σ
−1
,
(S201)"
REFERENCES,0.6444099378881988,"where ¯RK−1
W,X,Θ( ¯GK−1) ∈M18(C) is the operator-valued R-transform of ¯QK−1
W,X,Θ. Note that (S201)
is a coupled set of 18×18 scalar equations and thus eliminates all reference to large random matrices.
To see this, note that ¯Z, ¯GK−1, ¯RK−1
W,X,Θ( ¯GK−1) are all scalar-entried 18 × 18 matrices. The right-
hand side of (S201) is deﬁned by expanding the inverse to obtain an 18 × 18 block matrix whose
blocks involve various rational functions of Σ, Σβ and the scalar entries of ¯Z, ¯GK−1, ¯RK−1
W,X,Θ( ¯GK−1).
Finally one computes the normalized traces of these blocks, giving scalar values and eliminating all
reference to random matrices. Below, when writing out these equations explicitly, we will use the
fact that traces of rational functions of Σ, Σβ tend toward expectations of the corresponding rational
functions over the LJSD µ. Both here and in the sequel, to ease the already cumbersome presentation,
we use GK−1 to also denote the limiting value satisfying (S201)."
REFERENCES,0.6451863354037267,"As described in (Adlam & Pennington, 2020a;b), since ¯QK−1
W,X,Θ is a block matrix whose blocks are iid"
REFERENCES,0.6459627329192547,"Gaussian matrices (and their transposes), an explicit expression for ¯RK−1
W,X,Θ( ¯GK−1) can be obtained
through a covariance map, denoted by η (Far et al., 2006). In particular, η : Md(C) →Md(C) is
deﬁned by,
[η(D)]ij =
X"
REFERENCES,0.6467391304347826,"kl
σ(i, k; l, j)αkDkl ,
(S202)"
REFERENCES,0.6475155279503105,"where αk is dimensionality of the kth block and σ(i, k; l, k) denotes the covariance between the
entries of the blocks ij block of ¯QK−1
W,X,Θ and entries of the kl block of ¯QK−1
W,X,Θ. Here d = 18 is the
number of blocks. When the constituent blocks are iid Gaussian matrices and their transposes, as
is the case here, then ¯RK−1
W,X,Θ = η (Mingo & Speicher, 2017; section 9.1 and 9.2 Thm. 11), and"
REFERENCES,0.6482919254658385,"therefore the entries of ¯RK−1
W,X,Θ can be read off from eqn. (S195). To simplify the presentation, we
only report the entries of ¯RK−1
W,X,Θ(GK−1) that are nonzero, given the speciﬁc sparsity pattern of"
REFERENCES,0.6490683229813664,"GK−1. The latter follows from eqn. (S201) in the manner described in (Mingo & Speicher, 2017; Far
et al., 2006). Practically speaking, the sparsity pattern can be obtained by iterating an eqn. (S201),
starting with an ansatz sparsity pattern determined by ¯Z, and stopping when the iteration converges to
a ﬁxed sparsity pattern. In this case (and all cases that follow in the subsequent sections), the number
of necessary iterations is small and can be done explicitly. We omit the details and instead simply
report the following results for the nonzero entries:"
REFERENCES,0.6498447204968945,"¯RK−1
W,X,Θ( ¯GK−1) ="
REFERENCES,0.6506211180124224,"0
RK−1
W,X,Θ(GK−1)⊤"
REFERENCES,0.6513975155279503,"RK−1
W,X,Θ(GK−1)
0 !"
REFERENCES,0.6521739130434783,",
(S203)"
REFERENCES,0.6529503105590062,"where,"
REFERENCES,0.6537267080745341,"[RK−1
W,X,Θ(GK−1)]1,1
=
GK−1
2,2 (ζ −η) −√ρGK−1
6,3
γ
(S204)"
REFERENCES,0.6545031055900621,"[RK−1
W,X,Θ(GK−1)]1,9
=
−
√ρGK−1
8,3
γ
(S205)"
REFERENCES,0.65527950310559,"[RK−1
W,X,Θ(GK−1)]2,2
=
ψGK−1
1,1 (ζ −η)"
REFERENCES,0.656055900621118,"γφ
+ √ρψGK−1
4,5
(S206)"
REFERENCES,0.656832298136646,"[RK−1
W,X,Θ(GK−1)]4,5
=
√ρGK−1
2,2
(S207)"
REFERENCES,0.657608695652174,"[RK−1
W,X,Θ(GK−1)]6,3
=
−
√ρGK−1
1,1
γφ
(S208)"
REFERENCES,0.6583850931677019,"[RK−1
W,X,Θ(GK−1)]8,3
=
−
√ρGK−1
1,9
γφ
,
(S209)"
REFERENCES,0.6591614906832298,Published as a conference paper at ICLR 2022
REFERENCES,0.6599378881987578,"and the remaining entries of RK−1
W,X,Θ(GK−1) are zero. Owing to the large degree of sparsity, the
matrix inverse in (S201) can be performed explicitly and yields relatively simple expressions that
depend on the entries of GK−1 and the matrices Σ and Σβ. For example, the (16, 4) entry of the
self-consistent equation reads,"
REFERENCES,0.6607142857142857,"GK−1
7,4
=

id ⊗¯tr

¯Z −¯RK−1
W,X,Θ( ¯GK−1) −¯QK−1
Σ
−1"
REFERENCES,0.6614906832298136,"16,4
(S210)"
REFERENCES,0.6622670807453416,"=
¯tr
h
−1
√ρΣβ
 
In0 + ρ"
REFERENCES,0.6630434782608695,"φγ GK−1
1,1 GK−1
2,2 Σ)−1i
(S211)"
REFERENCES,0.6638198757763976,"n0→∞
=
−Eµ
h q/√ρ 1 + x φλ"
REFERENCES,0.6645962732919255,"i
(S212)"
REFERENCES,0.6653726708074534,"=
−Iβ
0,1
√ρ ,
(S213)"
REFERENCES,0.6661490683229814,"where to compute the asymptotic normalized trace we moved to an eigenbasis of Σ and recalled the
deﬁnition of the LJSD µ and the deﬁnition of Iβ in Eq. (12). The remaining entries of the (S201)
can be obtained in a similar manner and together yield the following set of coupled equations for the
entries of GK−1,"
REFERENCES,0.6669254658385093,"GK−1
1,1
=
−
γ
−GK−1
2,2 (−ζ + η + ρ) + ρGK−1
2,2
−√ρGK−1
6,3
−γ
(S214)"
REFERENCES,0.6677018633540373,"GK−1
2,2
=
γφ
ψGK−1
1,1 (η −ζ) −γφ
 √ρψGK−1
4,5
−1

(S215)"
REFERENCES,0.6684782608695652,"GK−1
3,6
=
Eµ
h
√ρGK−1
1,1
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.6692546583850931,(S216)
REFERENCES,0.6700310559006211,"GK−1
4,5
=
Eµ
h
λ√ρGK−1
1,1
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.6708074534161491,(S217)
REFERENCES,0.671583850931677,"GK−1
5,4
=
Eµ
h
−
γ√ρφGK−1
2,2
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.672360248447205,(S218)
REFERENCES,0.6731366459627329,"GK−1
6,3
=
Eµ
h
−
γλ√ρφGK−1
2,2
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.6739130434782609,(S219)
REFERENCES,0.6746894409937888,"GK−1
7,4
=
Eµ
h
−
qγφ
√ρ
 
λρGK−1
1,1 GK−1
2,2
+ γφ

i"
REFERENCES,0.6754658385093167,(S220)
REFERENCES,0.6762422360248447,"GK−1
7,6
=
Eµ
h
q
√"
REFERENCES,0.6770186335403726,"λGK−1
1,1
λρGK−1
1,1 GK−1
2,2
+ γφ i"
REFERENCES,0.6777950310559007,(S221)
REFERENCES,0.6785714285714286,"GK−1
8,3
=
Eµ
h
−
qγλφ
√ρ
 
λρGK−1
1,1 GK−1
2,2
+ γφ

i"
REFERENCES,0.6793478260869565,(S222)
REFERENCES,0.6801242236024845,"GK−1
8,5
=
Eµ
h
qλ3/2GK−1
1,1
λρGK−1
1,1 GK−1
2,2
+ γφ i"
REFERENCES,0.6809006211180124,(S223)
REFERENCES,0.6816770186335404,"GK−1
8,7
=
Eµ
h√ λ
i"
REFERENCES,0.6824534161490683,(S224)
REFERENCES,0.6832298136645962,"GK−1
9,1
=
√ρGK−1
8,3
−GK−1
2,2 (−ζ + η + ρ) + ρGK−1
2,2
−√ρGK−1
6,3
−γ
(S225)"
REFERENCES,0.6840062111801242,"GK−1
3,4
=
GK−1
5,6
= Eµ
h
√"
REFERENCES,0.6847826086956522,"λρGK−1
1,1 GK−1
2,2
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.6855590062111802,(S226)
REFERENCES,0.6863354037267081,"GK−1
3,5
=
GK−1
4,6
= Eµ
h
− √"
REFERENCES,0.687111801242236,"λ√ρGK−1
1,1
λρGK−1
1,1 GK−1
2,2
+ γφ i"
REFERENCES,0.687888198757764,(S227)
REFERENCES,0.6886645962732919,Published as a conference paper at ICLR 2022
REFERENCES,0.6894409937888198,"GK−1
4,3
=
GK−1
6,5
= Eµ
h
−
γ
√"
REFERENCES,0.6902173913043478,"λφ
−λρGK−1
1,1 GK−1
2,2
−γφ i"
REFERENCES,0.6909937888198758,(S228)
REFERENCES,0.6917701863354038,"GK−1
5,3
=
GK−1
6,4
= Eµ
h
γ
√"
REFERENCES,0.6925465838509317,"λ√ρφGK−1
2,2
λρGK−1
1,1 GK−1
2,2
+ γφ i"
REFERENCES,0.6933229813664596,(S229)
REFERENCES,0.6940993788819876,"GK−1
7,3
=
GK−1
8,4
= Eµ
h
−
qγ
√"
REFERENCES,0.6948757763975155,"λφ
√ρ
 
λρGK−1
1,1 GK−1
2,2
+ γφ

i"
REFERENCES,0.6956521739130435,(S230)
REFERENCES,0.6964285714285714,"GK−1
7,5
=
GK−1
8,6
= Eµ
h
qλGK−1
1,1
λρGK−1
1,1 GK−1
2,2
+ γφ i"
REFERENCES,0.6972049689440993,(S231)
REFERENCES,0.6979813664596274,"GK−1
7,7
=
GK−1
8,8
= GK−1
9,9
= 1
(S232)"
REFERENCES,0.6987577639751553,"GK−1
3,3
=
GK−1
4,4
= GK−1
5,5
= GK−1
6,6
= Eµ
h
−
γφ
−λρGK−1
1,1 GK−1
2,2
−γφ"
REFERENCES,0.6995341614906833,"i
,
(S233)"
REFERENCES,0.7003105590062112,"where we have used the fact that, asymptotically, the normalized trace becomes equivalent to an ex-
pectation over µ. After eliminating GK−1
6,3
and GK−1
4,5
from the ﬁrst two equations, it is straightforward
to show that"
REFERENCES,0.7010869565217391,"τ1
≡
¯tr(K−1) = 1"
REFERENCES,0.7018633540372671,"γ GK−1
1,1
= p"
REFERENCES,0.702639751552795,(ψ −φ)2 + 4xψφγ/ρ + ψ −φ
REFERENCES,0.703416149068323,"2ψγ
(S234)"
REFERENCES,0.7041925465838509,"¯τ1
≡
¯tr( ˆK−1) = 1"
REFERENCES,0.7049689440993789,"γ GK−1
2,2
= 1 γ + ψ"
REFERENCES,0.7057453416149069,"φ
 
τ1 −1 γ
"
REFERENCES,0.7065217391304348,(S235)
REFERENCES,0.7072981366459627,"τ2
=
¯tr( 1"
REFERENCES,0.7080745341614907,"n0
X⊤Σ1/2ΣβΣ1/2XK−1) = τ1Iβ
1,1
(S236)"
REFERENCES,0.7088509316770186,"where we have used the notation τ1 and τ2 from (Adlam & Pennington, 2020a;b), and ¯τ1 is the
companion transform of τ1, and where x satisﬁes the self-consistent equation,"
REFERENCES,0.7096273291925466,x = 1 −γτ1
REFERENCES,0.7104037267080745,"ω + I1,1
=
1 −
√"
REFERENCES,0.7111801242236024,(ψ−φ)2+4xψφγ/ρ+ψ−φ
REFERENCES,0.7119565217391305,"2ψ
ω + I1,1
.
(S237)"
REFERENCES,0.7127329192546584,"Here we utilized the two-index set of functionals of µ, Ia,b deﬁned in Eq. (12)."
REFERENCES,0.7135093167701864,"Note that the product τ1¯τ1 is simply related to x,"
REFERENCES,0.7142857142857143,"x = γρτ1¯τ1 ,
(S238)"
REFERENCES,0.7150621118012422,"so that, given x, the equations for the remaining entries of GK−1 completely decouple. In particular,"
REFERENCES,0.7158385093167702,"GK−1
3,6
=
−
√ρGK−1
1,1 I0,1
γφ
(S239)"
REFERENCES,0.7166149068322981,"GK−1
4,5
=
−
√ρGK−1
1,1 I1,1
γφ
(S240)"
REFERENCES,0.717391304347826,"GK−1
5,4
=
√ρGK−1
2,2 I0,1
(S241)"
REFERENCES,0.718167701863354,"GK−1
6,3
=
√ρGK−1
2,2 I1,1
(S242)"
REFERENCES,0.718944099378882,"GK−1
7,4
=
−Iβ
0,1
√ρ
(S243)"
REFERENCES,0.71972049689441,"GK−1
7,6
=
Iβ
1
2 ,1GK−1
1,1
γφ
(S244)"
REFERENCES,0.7204968944099379,"GK−1
8,3
=
−Iβ
1,1
√ρ
(S245)"
REFERENCES,0.7212732919254659,Published as a conference paper at ICLR 2022
REFERENCES,0.7220496894409938,"GK−1
8,5
=
Iβ
3
2 ,1GK−1
1,1
γφ
(S246)"
REFERENCES,0.7228260869565217,"GK−1
8,7
=
I 1"
REFERENCES,0.7236024844720497,"2 ,0
φ
(S247)"
REFERENCES,0.7243788819875776,"GK−1
9,1
=
−
√ρGK−1
1,1 GK−1
8,3
γ
(S248)"
REFERENCES,0.7251552795031055,"GK−1
3,4
=
GK−1
5,6
= −
xI 1"
REFERENCES,0.7259316770186336,"2 ,1
φ
(S249)"
REFERENCES,0.7267080745341615,"GK−1
3,5
=
GK−1
4,6
= −"
REFERENCES,0.7274844720496895,"√ρGK−1
1,1 I 1"
REFERENCES,0.7282608695652174,"2 ,1
γφ
(S250)"
REFERENCES,0.7290372670807453,"GK−1
4,3
=
GK−1
6,5
= I 1"
REFERENCES,0.7298136645962733,"2 ,1
(S251)"
REFERENCES,0.7305900621118012,"GK−1
5,3
=
GK−1
6,4
= √ρGK−1
2,2 I 1"
REFERENCES,0.7313664596273292,"2 ,1
(S252)"
REFERENCES,0.7321428571428571,"GK−1
7,3
=
GK−1
8,4
= −
Iβ
1
2 ,1
√ρ
(S253)"
REFERENCES,0.7329192546583851,"GK−1
7,5
=
GK−1
8,6
= Iβ
1,1GK−1
1,1
γφ
(S254)"
REFERENCES,0.7336956521739131,"GK−1
7,7
=
GK−1
8,8
= GK−1
9,9
= 1
(S255)"
REFERENCES,0.734472049689441,"GK−1
3,3
=
GK−1
4,4
= GK−1
5,5
= GK−1
6,6
= I0,1 ,
(S256)"
REFERENCES,0.735248447204969,which will be important intermediate results for the subsequent sections.
REFERENCES,0.7360248447204969,"Finally, we note that these results are sufﬁcient to compute the training error. The expected training
loss can be written as,"
REFERENCES,0.7368012422360248,Etrain = 1
REFERENCES,0.7375776397515528,"mEtr
 
(Y −ˆy(X))(Y −ˆy(X))⊤
(S257) = γ2"
REFERENCES,0.7383540372670807,"m Etr
 
Y ⊤Y K−2
(S258) = γ2"
REFERENCES,0.7391304347826086,"m Etr
  1"
REFERENCES,0.7399068322981367,"n0
(X⊤Σ1/2ΣβΣ1/2X + σ2
εIm)K−2
(S259)"
REFERENCES,0.7406832298136646,"= −γ2  
∂γτ2 + σ2
ε∂γτ1

(S260)"
REFERENCES,0.7414596273291926,"= −γ2 
∂γ(τ1Iβ
1,1) + σ2
ε∂γτ1

.
(S261)"
REFERENCES,0.7422360248447205,"G.4.2
E21"
REFERENCES,0.7430124223602484,"The calculation of E21 proceeds exactly as in (Tripuraneni et al., 2021a;b) with the simple modiﬁcation
of including an additional factor Σβ inside the ﬁnal trace term, yielding"
REFERENCES,0.7437888198757764,E21 = −2x
REFERENCES,0.7445652173913043,"φIβ
2,1 .
(S262)"
REFERENCES,0.7453416149068323,"G.4.3
E31"
REFERENCES,0.7461180124223602,"The calculation of E31 proceeds exactly as in (Tripuraneni et al., 2021a;b) with no modiﬁcations
since there is no dependence on Σβ. The result is,"
REFERENCES,0.7468944099378882,E31 = −ρψ
REFERENCES,0.7476708074534162,"φ
∂x
∂γ"
REFERENCES,0.7484472049689441,"
σ2
ε

(ω + φI1,2)(ω + I1,1) + φ"
REFERENCES,0.7492236024844721,"ψ γ¯τ1I2,2

,
(S263)"
REFERENCES,0.75,Published as a conference paper at ICLR 2022
REFERENCES,0.7507763975155279,"G.4.4
E32"
REFERENCES,0.7515527950310559,"Deﬁne the block matrix QE32 ≡[QE32
1
QE32
2
] by,"
REFERENCES,0.7523291925465838,"QE32
1
= "
REFERENCES,0.7531055900621118,"































 Im"
REFERENCES,0.7538819875776398,√η−ζΘ⊤ γ√n1 √ρX⊤
REFERENCES,0.7546583850931677,"γ√n0
0
0
0
√η−ζΘ⊤(ζ−η)"
REFERENCES,0.7554347826086957,"γ√n1
0"
REFERENCES,0.7562111801242236,"−Θ√η−ζ
√n1
In1
0
0
−
√ρW
√n1
0
0
0"
REFERENCES,0.7569875776397516,"0
0
In0
−Σ1/2
0
0
0
Σ1/2 (η −ζ)"
REFERENCES,0.7577639751552795,"0
−W ⊤"
REFERENCES,0.7585403726708074,"√n1
0
In0
0
0
0
0"
REFERENCES,0.7593167701863354,"0
0
0
0
In0
−Σ1/2
0
n1Σρ
n0
√ρ
−
X
√n0
0
0
0
0
In0
0
0
0
0
0
0
0
0
In1
0"
REFERENCES,0.7600931677018633,"0
0
0
0
0
0
−W ⊤"
REFERENCES,0.7608695652173914,"√n1
In0"
REFERENCES,0.7616459627329193,"0
0
0
0
0
0
√η−ζΘ⊤"
REFERENCES,0.7624223602484472,"γ√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.7631987577639752,"0
0
0
0
0
0
−W ⊤"
REFERENCES,0.7639751552795031,"√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 "
REFERENCES,0.764751552795031,"































 ,"
REFERENCES,0.765527950310559,"(S264)
and,"
REFERENCES,0.7663043478260869,"QE32
2
= "
REFERENCES,0.7670807453416149,"




























"
REFERENCES,0.7678571428571429,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Θ√η−ζ
√n1
−
√ρW
√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.7686335403726708,"Im
0
0
√ρX⊤"
REFERENCES,0.7694099378881988,"γ√n0
0
0
0
0"
REFERENCES,0.7701863354037267,"0
In0
−Σ1/2
0
0
0
0
0
−
X
√n0
0
In0
0
0
0
0
0"
REFERENCES,0.7709627329192547,"0
0
0
In0
−Σ1/2
0
0
0"
REFERENCES,0.7717391304347826,"0
0
0
0
In0
Σβ
√ρ
0
0"
REFERENCES,0.7725155279503105,"0
0
0
0
0
In0
−Σ1/2
0
0
0
0
0
0
0
In0
−
X
√n0
0
0
0
0
0
0
0
Im "
REFERENCES,0.7732919254658385,"




























"
REFERENCES,0.7740683229813664,".
(S265)"
REFERENCES,0.7748447204968945,"Then block matrix inversion (i.e. repeated applications of the Schur complement formula) shows that,"
REFERENCES,0.7756211180124224,"GE32
8,8
=
GE32
14,14 = GE32
15,15 = GE32
16,16 = 1
(S266)"
REFERENCES,0.7763975155279503,"GE32
1,1
=
GE32
9,9 = GK−1
1,1
(S267)"
REFERENCES,0.7771739130434783,"GE32
2,2
=
GE32
7,7 = GK−1
2,2
(S268)"
REFERENCES,0.7779503105590062,"GE32
13,8
=
GK−1
3,3
−1
(S269)"
REFERENCES,0.7787267080745341,"GE32
3,3
=
GE32
6,6 = GE32
11,11 = GE32
12,12 = GE32
4,4 = GE32
5,5 = GE32
10,10 = GE32
13,13 = GK−1
3,3 (S270)"
REFERENCES,0.7795031055900621,"GE32
3,4
=
GE32
5,6 = GE32
10,11 = GE32
12,8 = GE32
12,13 = GK−1
3,4
(S271)"
REFERENCES,0.78027950310559,"GE32
3,5
=
GE32
4,6 = GE32
12,10 = GE32
13,11 = GK−1
3,5
(S272)"
REFERENCES,0.781055900621118,"GE32
3,6
=
GE32
12,11 = GK−1
3,6
(S273)"
REFERENCES,0.781832298136646,"GE32
4,3
=
GE32
6,5 = GE32
11,10 = GE32
13,12 = GK−1
4,3
(S274)"
REFERENCES,0.782608695652174,"GE32
4,5
=
GE32
13,10 = GK−1
4,5
(S275)"
REFERENCES,0.7833850931677019,"GE32
5,3
=
GE32
6,4 = GE32
10,12 = GE32
11,8 = GE32
11,13 = GK−1
5,3
(S276)"
REFERENCES,0.7841614906832298,"GE32
5,4
=
GE32
10,8 = GE32
10,13 = GK−1
5,4
(S277)"
REFERENCES,0.7849378881987578,"GE32
6,3
=
GE32
11,12 = GK−1
6,3
(S278)"
REFERENCES,0.7857142857142857,"GE32
14,12
=
GE32
15,13 = GK−1
7,3
(S279)"
REFERENCES,0.7864906832298136,Published as a conference paper at ICLR 2022
REFERENCES,0.7872670807453416,"GE32
14,13
=
GK−1
7,4
(S280)"
REFERENCES,0.7880434782608695,"GE32
14,11
=
GK−1
7,6
(S281)"
REFERENCES,0.7888198757763976,"GE32
15,12
=
GK−1
8,3
(S282)"
REFERENCES,0.7895962732919255,"GE32
15,10
=
GK−1
8,5
(S283)"
REFERENCES,0.7903726708074534,"GE32
15,14
=
GK−1
8,7
(S284)"
REFERENCES,0.7911490683229814,"GE32
16,9
=
GK−1
9,1
(S285)"
REFERENCES,0.7919254658385093,"GE32
14,10
=
GE32
15,11 = GK−1
9,1"
REFERENCES,0.7927018633540373,"φ
(S286)"
REFERENCES,0.7934782608695652,"GE32
16,1
=
φ
ψ E32 ,
(S287)"
REFERENCES,0.7942546583850931,"where GE32
i,j denotes the normalized trace of the (i, j)-block of the inverse of
 
QE32⊤. For brevity,
we have suppressed the expressions for the other non-zero blocks."
REFERENCES,0.7950310559006211,"To compute the limiting values of these traces, we require the asymptotic block-wise traces of QE32,
which may be determined from the operator-valued Stieltjes transform. To proceed, we ﬁrst augment
QE32 to form the the self-adjoint matrix ¯QE32,"
REFERENCES,0.7958074534161491,"¯QE32 =

0
[QE32]
⊤"
REFERENCES,0.796583850931677,"QE32
0"
REFERENCES,0.797360248447205,"
.
(S288)"
REFERENCES,0.7981366459627329,"and observe that we can write ¯QE32 as,
¯QE32 = ¯Z −¯QE32
W,X,Θ −¯QE32
Σ"
REFERENCES,0.7989130434782609,"=

0
I16
I16
0 
−"
REFERENCES,0.7996894409937888,"0
[QE32
W,X,Θ]
⊤"
REFERENCES,0.8004658385093167,"QE32
W,X,Θ
0 ! −"
REFERENCES,0.8012422360248447,"0
[QE32
Σ ]
⊤"
REFERENCES,0.8020186335403726,"QE32
Σ
0 !"
REFERENCES,0.8027950310559007,",
(S289)"
REFERENCES,0.8035714285714286,"where QE32
W,X,θ ≡[[QE32
W,X,θ]1 [QE32
W,X,θ]2] and,"
REFERENCES,0.8043478260869565,"[QE32
W,X,θ]1 = − "
REFERENCES,0.8051242236024845,"



















"
REFERENCES,0.8059006211180124,"0
√η−ζΘ⊤ γ√n1 √ρX⊤"
REFERENCES,0.8066770186335404,"γ√n0
0
0
0
√η−ζΘ⊤(ζ−η)"
REFERENCES,0.8074534161490683,"γ√n1
0"
REFERENCES,0.8082298136645962,"−Θ√η−ζ
√n1
0
0
0
−
√ρW
√n1
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.8090062111801242,"0
−W ⊤"
REFERENCES,0.8097826086956522,"√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−
X
√n0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.8105590062111802,"0
0
0
0
0
0
−W ⊤ √n1
0"
REFERENCES,0.8113354037267081,"0
0
0
0
0
0
√η−ζΘ⊤"
REFERENCES,0.812111801242236,"γ√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.812888198757764,"0
0
0
0
0
0
−W ⊤"
REFERENCES,0.8136645962732919,"√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 "
REFERENCES,0.8144409937888198,"



















"
REFERENCES,0.8152173913043478,(S290)
REFERENCES,0.8159937888198758,"[QE32
W,X,θ]2 = − "
REFERENCES,0.8167701863354038,"
















"
REFERENCES,0.8175465838509317,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Θ√η−ζ
√n1
−
√ρW
√n1
0
0
0
0
0
0
0
0
0
0
0
0
0
0"
REFERENCES,0.8183229813664596,"0
0
0
√ρX⊤"
REFERENCES,0.8190993788819876,"γ√n0
0
0
0
0
0
0
0
0
0
0
0
0
−
X
√n0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−
X
√n0
0
0
0
0
0
0
0
0 "
REFERENCES,0.8198757763975155,"
















"
REFERENCES,0.8206521739130435,(S291)
REFERENCES,0.8214285714285714,Published as a conference paper at ICLR 2022
REFERENCES,0.8222049689440993,"QE32
Σ
= − "
REFERENCES,0.8229813664596274,"
















"
REFERENCES,0.8237577639751553,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0
Σ1/2 (η −ζ)
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
n1Σρ
n0
√ρ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0"
REFERENCES,0.8245341614906833,"0
0
0
0
0
0
0
0
0
0
0
0
0
Σβ
√ρ
0
0"
REFERENCES,0.8253105590062112,"0
0
0
0
0
0
0
0
0
0
0
0
0
0
−Σ1/2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0 "
REFERENCES,0.8260869565217391,"
















 ."
REFERENCES,0.8268633540372671,"(S292)
The operator-valued Stieltjes transforms satisfy,
¯GE32 = ¯GE32
Σ ( ¯Z −¯RE32
W,X,Θ( ¯GE32))"
REFERENCES,0.827639751552795,"= id ⊗¯tr

¯Z −¯RE32
W,X,Θ( ¯GE32) −¯QE32
Σ
−1
,
(S293)"
REFERENCES,0.828416149068323,"where ¯RE32
W,X,Θ( ¯GE32) is the operator-valued R-transform of ¯QE32
W,X,Θ. As discussed above, since
¯QE32
W,X,Θ is a block matrix whose blocks are iid Gaussian matrices (and their transposes), an explicit
expression for ¯RE32
W,X,Θ( ¯GE32) can be obtained from the covariance map η, which can be read off
from eqn. (S288). As above, we utilize the speciﬁc sparsity pattern for GE32 that is induced by
Eq. (S293), to obtain,"
REFERENCES,0.8291925465838509,"¯RE32
W,X,Θ( ¯GE32) ="
REFERENCES,0.8299689440993789,"0
RE32
W,X,Θ(GE32)⊤"
REFERENCES,0.8307453416149069,"RE32
W,X,Θ(GE32)
0 !"
REFERENCES,0.8315217391304348,",
(S294)"
REFERENCES,0.8322981366459627,"where,"
REFERENCES,0.8330745341614907,"[RE32
W,X,θ(GE32)]1,1 = GE32
2,2 (ζ −η)"
REFERENCES,0.8338509316770186,"γ
−
√ρGE32
6,3
γ
+ GE32
2,7 (ζ −η) (ζ −η)"
REFERENCES,0.8346273291925466,"γ
(S295)"
REFERENCES,0.8354037267080745,"[RE32
W,X,θ(GE32)]1,9 = GE32
7,2 (ζ −η)"
REFERENCES,0.8361801242236024,"γ
−
√ρGE32
11,3
γ
+ GE32
7,7 (ζ −η) (ζ −η)"
REFERENCES,0.8369565217391305,"γ
(S296)"
REFERENCES,0.8377329192546584,"[RE32
W,X,θ(GE32)]1,16 = −
√ρGE32
15,3
γ
(S297)"
REFERENCES,0.8385093167701864,"[RE32
W,X,θ(GE32)]2,2 = ψGE32
1,1 (ζ −η)"
REFERENCES,0.8392857142857143,"γφ
+ √ρψGE32
4,5
(S298)"
REFERENCES,0.8400621118012422,"[RE32
W,X,θ(GE32)]2,7 = ψGE32
9,1 (ζ −η)"
REFERENCES,0.8408385093167702,"γφ
+ √ρψGE32
8,5 + √ρψGE32
13,5 + ψGE32
1,1 (ζ −η) (ζ −η)"
REFERENCES,0.8416149068322981,"γφ
(S299)"
REFERENCES,0.842391304347826,"[RE32
W,X,θ(GE32)]4,5 = √ρGE32
2,2
(S300)"
REFERENCES,0.843167701863354,"[RE32
W,X,θ(GE32)]4,10 = √ρGE32
7,2
(S301)"
REFERENCES,0.843944099378882,"[RE32
W,X,θ(GE32)]6,3 = −
√ρGE32
1,1
γφ
(S302)"
REFERENCES,0.84472049689441,"[RE32
W,X,θ(GE32)]6,12 = −
√ρGE32
9,1
γφ
(S303)"
REFERENCES,0.8454968944099379,"[RE32
W,X,θ(GE32)]7,2 = ψGE32
1,9 (ζ −η)"
REFERENCES,0.8462732919254659,"γφ
+ √ρψGE32
4,10
(S304)"
REFERENCES,0.8470496894409938,"[RE32
W,X,θ(GE32)]7,7 = ψGE32
9,9 (ζ −η)"
REFERENCES,0.8478260869565217,"γφ
+ √ρψGE32
8,10 + √ρψGE32
13,10 + ψGE32
1,9 (ζ −η) (ζ −η)"
REFERENCES,0.8486024844720497,"γφ
(S305)"
REFERENCES,0.8493788819875776,Published as a conference paper at ICLR 2022
REFERENCES,0.8501552795031055,"[RE32
W,X,θ(GE32)]8,5 = √ρGE32
2,7
(S306)"
REFERENCES,0.8509316770186336,"[RE32
W,X,θ(GE32)]8,10 = √ρGE32
7,7
(S307)"
REFERENCES,0.8517080745341615,"[RE32
W,X,θ(GE32)]9,1 = GE32
2,7 (ζ −η)"
REFERENCES,0.8524844720496895,"γ
−
√ρGE32
6,12
γ
(S308)"
REFERENCES,0.8532608695652174,"[RE32
W,X,θ(GE32)]9,9 = GE32
7,7 (ζ −η)"
REFERENCES,0.8540372670807453,"γ
−
√ρGE32
11,12
γ
(S309)"
REFERENCES,0.8548136645962733,"[RE32
W,X,θ(GE32)]9,16 = −
√ρGE32
15,12
γ
(S310)"
REFERENCES,0.8555900621118012,"[RE32
W,X,θ(GE32)]11,3 = −
√ρGE32
1,9
γφ
(S311)"
REFERENCES,0.8563664596273292,"[RE32
W,X,θ(GE32)]11,12 = −
√ρGE32
9,9
γφ
(S312)"
REFERENCES,0.8571428571428571,"[RE32
W,X,θ(GE32)]13,5 = √ρGE32
2,7
(S313)"
REFERENCES,0.8579192546583851,"[RE32
W,X,θ(GE32)]13,10 = √ρGE32
7,7
(S314)"
REFERENCES,0.8586956521739131,"[RE32
W,X,θ(GE32)]15,3 = −
√ρGE32
1,16
γφ
(S315)"
REFERENCES,0.859472049689441,"[RE32
W,X,θ(GE32)]15,12 = −
√ρGE32
9,16
γφ
,
(S316)"
REFERENCES,0.860248447204969,"and the remaining entries of RE32
W,X,θ(GE32) are zero.
As above, plugging these expressions into
eqn. (S293) and explicitly performing the block-matrix inverse yields the following set of coupled
equations,"
REFERENCES,0.8610248447204969,"GE32
7,2 = γ2√ρ¯τ 2
1 ψGE32
8,5 + γ2√ρ¯τ 2
1 ψGE32
13,5 + γ¯τ 2
1 ψGE32
9,1 (ζ −η)"
REFERENCES,0.8618012422360248,"φ
+ γ2τ1¯τ 2
1 ψ(ζ −η) (ζ −η)"
REFERENCES,0.8625776397515528,"φ
(S317)"
REFERENCES,0.8633540372670807,"GE32
8,3 = I 1"
REFERENCES,0.8641304347826086,"2 ,1ζ −I 1"
REFERENCES,0.8649068322981367,"2 ,1η −
γ¯τ1I 3 2 ,1ρ"
REFERENCES,0.8656832298136646,"ψ
(S318)"
REFERENCES,0.8664596273291926,"GE32
8,4 = −γ¯τ1I1,1 (ρτ1ψ (ζ −η) + φρ)"
REFERENCES,0.8672360248447205,"ψφ
(S319)"
REFERENCES,0.8680124223602484,"GE32
8,5 = −I1,1 (ρτ1ψ (ζ −η) + φρ)
√ρψφ
(S320)"
REFERENCES,0.8687888198757764,"GE32
8,6 = −"
REFERENCES,0.8695652173913043,"√ρτ1

ψI 1"
REFERENCES,0.8703416149068323,"2 ,1ζ −ψI 1"
REFERENCES,0.8711180124223602,"2 ,1η −γ¯τ1I 3"
REFERENCES,0.8718944099378882,"2 ,1ρ
"
REFERENCES,0.8726708074534162,"ψφ
(S321)"
REFERENCES,0.8734472049689441,"GE32
9,1 = γτ 2
1 GE32
7,2 (ζ −η) −γ√ρτ 2
1 GE32
11,3 + γ2τ 2
1 ¯τ1(ζ −η) (ζ −η)
(S322)"
REFERENCES,0.8742236024844721,"GE32
10,3 = √ρφGE32
7,2 I 1"
REFERENCES,0.875,"2 ,2 −γρ3/2¯τ 2
1 GE32
9,1 I 3"
REFERENCES,0.8757763975155279,"2 ,2 −
γ√ρ¯τ1φ

−ψI 1"
REFERENCES,0.8765527950310559,"2 ,2ζ + ψI 1"
REFERENCES,0.8773291925465838,"2 ,2η + γ¯τ1I 3"
REFERENCES,0.8781055900621118,"2 ,2ρ
"
REFERENCES,0.8788819875776398,"ψ
(S323)"
REFERENCES,0.8796583850931677,"GE32
10,4 = √ρφGE32
7,2 I0,2 −γρ3/2¯τ 2
1 GE32
9,1 I1,2 −γ2√ρ¯τ 2
1 I1,2 (ρτ1ψ (ζ −η) + φρ)"
REFERENCES,0.8804347826086957,"ψ
(S324)"
REFERENCES,0.8812111801242236,"GE32
10,5 = −ρτ1GE32
7,2 I1,2 −ρ¯τ1GE32
9,1 I1,2 −I1,2 (γ¯τ1φρ + xψζ −xψη)"
REFERENCES,0.8819875776397516,"ψ
(S325)"
REFERENCES,0.8827639751552795,"GE32
10,6 = −ρτ1GE32
7,2 I 1"
REFERENCES,0.8835403726708074,"2 ,2 −ρ¯τ1GE32
9,1 I 1"
REFERENCES,0.8843167701863354,"2 ,2 +
γ2ρτ1¯τ 2
1 I 3 2 ,2ρ"
REFERENCES,0.8850931677018633,"ψ
+ xI 1"
REFERENCES,0.8858695652173914,"2 ,2 (η −ζ)
(S326)"
REFERENCES,0.8866459627329193,Published as a conference paper at ICLR 2022
REFERENCES,0.8874223602484472,"GE32
11,3 = √ρφGE32
7,2 I1,2 −γρ3/2¯τ 2
1 GE32
9,1 I2,2 −γ√ρ¯τ1φ (−ψI1,2ζ + ψI1,2η + γ¯τ1I2,2ρ)"
REFERENCES,0.8881987577639752,"ψ
(S327)"
REFERENCES,0.8889751552795031,"GE32
11,4 = √ρφGE32
7,2 I 1"
REFERENCES,0.889751552795031,"2 ,2 −γρ3/2¯τ 2
1 GE32
9,1 I 3"
REFERENCES,0.890527950310559,"2 ,2 −
γ2√ρ¯τ 2
1 I 3"
REFERENCES,0.8913043478260869,"2 ,2 (ρτ1ψ (ζ −η) + φρ)"
REFERENCES,0.8920807453416149,"ψ
(S328)"
REFERENCES,0.8928571428571429,"GE32
11,5 = −ρτ1GE32
7,2 I 3"
REFERENCES,0.8936335403726708,"2 ,2 −ρ¯τ1GE32
9,1 I 3"
REFERENCES,0.8944099378881988,"2 ,2 −
I 3"
REFERENCES,0.8951863354037267,"2 ,2 (γ¯τ1φρ + xψζ −xψη)"
REFERENCES,0.8959627329192547,"ψ
(S329)"
REFERENCES,0.8967391304347826,"GE32
12,4 = −ρτ1GE32
7,2 I 1"
REFERENCES,0.8975155279503105,"2 ,2 −ρ¯τ1GE32
9,1 I 1"
REFERENCES,0.8982919254658385,"2 ,2 + I 3 2 ,2"
REFERENCES,0.8990683229813664,"γ2ρτ1¯τ 2
1 ρ
ψ
+ x2 (ζ −η) φ"
REFERENCES,0.8998447204968945,"
(S330)"
REFERENCES,0.9006211180124224,"GE32
12,5 = −"
REFERENCES,0.9013975155279503,"√ρGE32
9,1 I 1"
REFERENCES,0.9021739130434783,"2 ,2
γ
+
ρ3/2τ 2
1 GE32
7,2 I 3"
REFERENCES,0.9029503105590062,"2 ,2
φ
+
I 3"
REFERENCES,0.9037267080745341,"2 ,2
 
γρ2τ 2
1 ¯τ1ψ (ζ −η) + xφρ
"
REFERENCES,0.9045031055900621,"√ρψφ
(S331)"
REFERENCES,0.90527950310559,"GE32
12,6 = −
√ρGE32
9,1 I0,2"
REFERENCES,0.906055900621118,"γ
+ ρ3/2τ 2
1 GE32
7,2 I1,2
φ
+
γρ2τ 2
1 ¯τ1I1,2 (ζ −η) −x2I2,2ρ"
REFERENCES,0.906832298136646,"ψ
√ρφ
(S332)"
REFERENCES,0.907608695652174,"GE32
13,3 = −ρτ1GE32
7,2 I 3"
REFERENCES,0.9083850931677019,"2 ,2 −ρ¯τ1GE32
9,1 I 3"
REFERENCES,0.9091614906832298,"2 ,2 +
γ2ρτ1¯τ 2
1 I 5 2 ,2ρ"
REFERENCES,0.9099378881987578,"ψ
+ xI 3"
REFERENCES,0.9107142857142857,"2 ,2 (η −ζ)
(S333)"
REFERENCES,0.9114906832298136,"GE32
13,4 = −ρτ1GE32
7,2 I1,2 −ρ¯τ1GE32
9,1 I1,2 + I2,2"
REFERENCES,0.9122670807453416,"γ2ρτ1¯τ 2
1 ρ
ψ
+ x2 (ζ −η) φ"
REFERENCES,0.9130434782608695,"
(S334)"
REFERENCES,0.9138198757763976,"GE32
13,5 = −
√ρGE32
9,1 I1,2"
REFERENCES,0.9145962732919255,"γ
+ ρ3/2τ 2
1 GE32
7,2 I2,2
φ
+ I2,2
 
γρ2τ 2
1 ¯τ1ψ (ζ −η) + xφρ
"
REFERENCES,0.9153726708074534,"√ρψφ
(S335)"
REFERENCES,0.9161490683229814,"GE32
13,6 = −"
REFERENCES,0.9169254658385093,"√ρGE32
9,1 I 1"
REFERENCES,0.9177018633540373,"2 ,2
γ
+
ρ3/2τ 2
1 GE32
7,2 I 3"
REFERENCES,0.9184782608695652,"2 ,2
φ
+
γρ2τ 2
1 ¯τ1I 3"
REFERENCES,0.9192546583850931,"2 ,2 (ζ −η) −
x2I 5 2 ,2ρ"
REFERENCES,0.9200310559006211,"ψ
√ρφ
(S336)"
REFERENCES,0.9208074534161491,"GE32
13,8 = −xI1,1"
REFERENCES,0.921583850931677,"φ
(S337)"
REFERENCES,0.922360248447205,"GE32
14,3 = √ρτ1Iβ
3
2 ,2GE32
7,2 + √ρ¯τ1Iβ
3
2 ,2GE32
9,1 +
xψIβ
3
2 ,2 (ζ −η) −γ2ρτ1¯τ 2
1 Iβ
5
2 ,2ρ
√ρψ
(S338)"
REFERENCES,0.9231366459627329,"GE32
14,4 = √ρτ1Iβ
1,2GE32
7,2 + √ρ¯τ1Iβ
1,2GE32
9,1 −Iβ
2,2
 
γ2ρτ1¯τ 2
1 φρ + ψx2ζ −ψx2η
"
REFERENCES,0.9239130434782609,"√ρψφ
(S339)"
REFERENCES,0.9246894409937888,"GE32
14,5 = Iβ
1,2GE32
9,1
γ
−ρτ 2
1 Iβ
2,2GE32
7,2
φ
+
Iβ
2,2

γρ2τ 2
1 ¯τ1(η−ζ) φ
−xρ ψ
"
REFERENCES,0.9254658385093167,"ρ
(S340)"
REFERENCES,0.9262422360248447,"GE32
14,6 =
Iβ
1
2 ,2GE32
9,1
γ
−
ρτ 2
1 Iβ
3
2 ,2GE32
7,2
φ
+
γρ2τ 2
1 ¯τ1Iβ
3
2 ,2 (η −ζ) +
x2Iβ
5
2 ,2ρ"
REFERENCES,0.9270186335403726,"ψ
ρφ
(S341)"
REFERENCES,0.9277950310559007,"GE32
14,8 = xIβ
1,1
√ρφ
(S342)"
REFERENCES,0.9285714285714286,"GE32
14,11 =
τ1Iβ
1
2 ,1
φ
(S343)"
REFERENCES,0.9293478260869565,"GE32
14,13 = −Iβ
0,1
√ρ
(S344)"
REFERENCES,0.9301242236024845,"GE32
15,3 = √ρτ1Iβ
2,2GE32
7,2 + √ρ¯τ1Iβ
2,2GE32
9,1 + xψIβ
2,2 (ζ −η) −γ2ρτ1¯τ 2
1 Iβ
3,2ρ
√ρψ
(S345)"
REFERENCES,0.9309006211180124,"GE32
15,4 = √ρτ1Iβ
3
2 ,2GE32
7,2 + √ρ¯τ1Iβ
3
2 ,2GE32
9,1 −
Iβ
5
2 ,2
 
γ2ρτ1¯τ 2
1 φρ + ψx2ζ −ψx2η
"
REFERENCES,0.9316770186335404,"√ρψφ
(S346)"
REFERENCES,0.9324534161490683,Published as a conference paper at ICLR 2022
REFERENCES,0.9332298136645962,"GE32
15,5 =
Iβ
3
2 ,2GE32
9,1
γ
−
ρτ 2
1 Iβ
5
2 ,2GE32
7,2
φ
−
Iβ
5
2 ,2
 
γρ2τ 2
1 ¯τ1ψ (ζ −η) + xφρ
"
REFERENCES,0.9340062111801242,"ρψφ
(S347)"
REFERENCES,0.9347826086956522,"GE32
15,6 = Iβ
1,2GE32
9,1
γ
−ρτ 2
1 Iβ
2,2GE32
7,2
φ
+
γρ2τ 2
1 ¯τ1Iβ
2,2 (η −ζ) +
x2Iβ
3,2ρ
ψ
ρφ
(S348)"
REFERENCES,0.9355590062111802,"GE32
15,8 =
xIβ
3
2 ,1
√ρφ
(S349)"
REFERENCES,0.9363354037267081,"GE32
15,10 =
τ1Iβ
3
2 ,1
φ
(S350)"
REFERENCES,0.937111801242236,"GE32
15,12 = −Iβ
1,1
√ρ
(S351)"
REFERENCES,0.937888198757764,"GE32
15,14 =
I 1"
REFERENCES,0.9386645962732919,"2 ,0
φ
(S352)"
REFERENCES,0.9394409937888198,"GE32
16,1 = τ 2
1 Iβ
1,1GE32
7,2 (ζ −η) −√ρτ 2
1 Iβ
1,1GE32
11,3 + γτ 2
1 ¯τ1Iβ
1,1(ζ −η) (ζ −η) −√ρτ1GE32
15,3
(S353)"
REFERENCES,0.9402173913043478,"GE32
16,9 = τ1Iβ
1,1
(S354)"
REFERENCES,0.9409937888198758,"GE32
1,1 = GE32
9,9 = γτ1
(S355)"
REFERENCES,0.9417701863354038,"GE32
2,2 = GE32
7,7 = γ¯τ1
(S356)"
REFERENCES,0.9425465838509317,"GE32
3,6 = GE32
12,11 = −
√ρτ1I0,1"
REFERENCES,0.9433229813664596,"φ
(S357)"
REFERENCES,0.9440993788819876,"GE32
4,5 = GE32
13,10 = −
√ρτ1I1,1"
REFERENCES,0.9448757763975155,"φ
(S358)"
REFERENCES,0.9456521739130435,"GE32
6,3 = GE32
11,12 = γ√ρ¯τ1I1,1
(S359)"
REFERENCES,0.9464285714285714,"GE32
11,6 = GE32
12,3 = −ρτ1GE32
7,2 I1,2 −ρ¯τ1GE32
9,1 I1,2 + γ2ρτ1¯τ 2
1 I2,2ρ
ψ
+ xI1,2 (η −ζ)
(S360)"
REFERENCES,0.9472049689440993,"GE32
14,10 = GE32
15,11 = τ1Iβ
1,1
φ
(S361)"
REFERENCES,0.9479813664596274,"GE32
14,12 = GE32
15,13 = −
Iβ
1
2 ,1
√ρ
(S362)"
REFERENCES,0.9487577639751553,"GE32
5,4 = GE32
10,8 = GE32
10,13 = γ√ρ¯τ1I0,1
(S363)"
REFERENCES,0.9495341614906833,"GE32
3,5 = GE32
4,6 = GE32
12,10 = GE32
13,11 = −"
REFERENCES,0.9503105590062112,√ρτ1I 1
REFERENCES,0.9510869565217391,"2 ,1
φ
(S364)"
REFERENCES,0.9518633540372671,"GE32
4,3 = GE32
6,5 = GE32
11,10 = GE32
13,12 = I 1"
REFERENCES,0.952639751552795,"2 ,1
(S365)"
REFERENCES,0.953416149068323,"GE32
8,8 = GE32
14,14 = GE32
15,15 = GE32
16,16 = 1
(S366)"
REFERENCES,0.9541925465838509,"GE32
3,4 = GE32
5,6 = GE32
10,11 = GE32
12,8 = GE32
12,13 = −
xI 1"
REFERENCES,0.9549689440993789,"2 ,1
φ
(S367)"
REFERENCES,0.9557453416149069,"GE32
5,3 = GE32
6,4 = GE32
10,12 = GE32
11,8 = GE32
11,13 = γ√ρ¯τ1I 1"
REFERENCES,0.9565217391304348,"2 ,1
(S368)"
REFERENCES,0.9572981366459627,"GE32
3,3 = GE32
4,4 = GE32
5,5 = GE32
6,6 = GE32
10,10 = GE32
11,11 = GE32
12,12 = GE32
13,13 = I0,1 ,
(S369)"
REFERENCES,0.9580745341614907,"Here we have used the relations in eqns. (S266)-(S287), the deﬁnition of Iβ
a,b, as well as the results in
Sec. G.4.1 to simplify the expressions. It is straightforward algebra to solve these equations for the
undetermined entries of GE32 and thereby obtain the following expression for E32,"
REFERENCES,0.9588509316770186,E32 = (η −ζ)A32 + ρB32
REFERENCES,0.9596273291925466,"D32
,
(S370)"
REFERENCES,0.9604037267080745,Published as a conference paper at ICLR 2022
REFERENCES,0.9611801242236024,"where,"
REFERENCES,0.9619565217391305,"A32 = −ρ3τ1ψ2x4I1,1I2,2Iβ
2,2 + ρ2τ1ψx3I2,2Iβ
2,2(ρφ + xψ(ζ −η))"
REFERENCES,0.9627329192546584,"−ρ3τ1ψ2x3φI1,1I1,2Iβ
2,2 + ρ2τ1ψ2x2I1,1Iβ
1,1(η −ζ)"
REFERENCES,0.9635093167701864,"+ ρ2τ1ψ2x2I1,1Iβ
2,2(ρ + x(ζ −η)) + ρ2τ1ψx2φI1,2Iβ
2,2(ρφ + xψ(ζ −η))"
REFERENCES,0.9642857142857143,"+ ρ3τ1ψ2x2φI1,1I1,2Iβ
1,1 −ρ2τ1ψxφI1,2Iβ
1,1(ρφ + xψ(ζ −η))"
REFERENCES,0.9650621118012422,"+ ρτ1ψxIβ
1,1(ζ −η)(ρφ + xψ(ζ −η))"
REFERENCES,0.9658385093167702,"−ρτ1ψxIβ
2,2(ρ + x(ζ −η))(ρφ + xψ(ζ −η))
(S371)"
REFERENCES,0.9666149068322981,"B32 = −ρ2ψx6I2
2,2Iβ
3,2 −2ρ2ψx5φI2
2,2Iβ
2,2 + 2ρψx4φI1,2Iβ
3,2(η −ζ)"
REFERENCES,0.967391304347826,"−2ρ2ψx4φ2I1,2I2,2Iβ
2,2 + ρ2ψx4φ2I2
1,2Iβ
3,2 + ρ2ψx4φI2
2,2Iβ
1,1"
REFERENCES,0.968167701863354,"+ ρ2ψx4φI1,1I2,2Iβ
2,2 + ρ2x4I2,2Iβ
3,2(ψ + φ)"
REFERENCES,0.968944099378882,"+ ρx3φI2,2Iβ
2,2(ρ(ψ + φ) + 2xψ(ζ −η))"
REFERENCES,0.96972049689441,"+ ρ2ψx3φ2I1,2I2,2Iβ
1,1 + ρ2ψx3φ2I1,1I1,2Iβ
2,2 + ρψx2φI1,1Iβ
1,1(ζ −η)"
REFERENCES,0.9704968944099379,"−ρx2φI2,2Iβ
1,1(ρφ + xψ(ζ −η)) −ρψx2φI1,1Iβ
2,2(ρ + x(ζ −η))"
REFERENCES,0.9712732919254659,"−ρ2ψx2φ2I1,1I1,2Iβ
1,1 + Iβ
3,2
 
x4ψ(ζ −η)2 −ρ2x2φ

(S372)"
REFERENCES,0.9720496894409938,"D32 = −ρ3ψx4φI2
2,2 + 2ρ2ψx2φ2I1,2(η −ζ)"
REFERENCES,0.9728260869565217,"+ ρ3ψx2φ3I2
1,2 + ρ3x2φI2,2(ψ + φ) + ρφ
 
x2ψ(ζ −η)2 −ρ2φ

.
(S373)"
REFERENCES,0.9736024844720497,"Further simpliﬁcations are possible using the raising and lowering identities in eqn. (S9), as well as
the results in Sec. G.4.1, to obtain,"
REFERENCES,0.9743788819875776,E32 = x2
REFERENCES,0.9751552795031055,"φ Iβ
3,2 −ρψ"
REFERENCES,0.9759316770186336,"φ
∂x
∂γ"
REFERENCES,0.9767080745341615,"
Iβ
1,1(ω + φI1,2)(ω + I1,1) + φ2"
REFERENCES,0.9774844720496895,"ψ γ¯τ1Iβ
1,2I2,2 + γτ1Iβ
2,2(ω + φI1,2)

,"
REFERENCES,0.9782608695652174,(S374)
REFERENCES,0.9790372670807453,"where
∂x
∂γ = −
x
γ + ργ(τ1ψ/φ + ¯τ1)(ω + φI1,2) .
(S375)"
REFERENCES,0.9798136645962733,"G.4.5
E4"
REFERENCES,0.9805900621118012,"The calculation of E4 proceeds exactly as in (Tripuraneni et al., 2021a;b) with the simple modiﬁcation
of including an additional factor Σβ inside the ﬁnal trace term, yielding"
REFERENCES,0.9813664596273292,E4 = x2
REFERENCES,0.9821428571428571,"φ Iβ
3,2 .
(S376)"
REFERENCES,0.9829192546583851,"G.5
FINAL RESULT FOR BIAS, VARIANCE, AND TEST ERROR"
REFERENCES,0.9836956521739131,"Putting the above pieces together, we have,"
REFERENCES,0.984472049689441,"Bµ = φIβ
1,2
(S377)"
REFERENCES,0.985248447204969,Vµ = −ρψ
REFERENCES,0.9860248447204969,"φ
∂x
∂γ"
REFERENCES,0.9868012422360248,"
Iβ
1,1(ω + φI1,2)(ω + I1,1) + φ2"
REFERENCES,0.9875776397515528,"ψ γ¯τ1Iβ
1,2I2,2 + γτ1Iβ
2,2(ω + φI1,2)"
REFERENCES,0.9883540372670807,"+ σ2
ε

(ω + φI1,2)(ω + I1,1) + φ"
REFERENCES,0.9891304347826086,"ψ γ¯τ1I2,2

.
(S378)"
REFERENCES,0.9899068322981367,(S379)
REFERENCES,0.9906832298136646,Some algebra shows that
REFERENCES,0.9914596273291926,"Eµ = Bµ + Vµ
(S380)"
REFERENCES,0.9922360248447205,Published as a conference paper at ICLR 2022
REFERENCES,0.9930124223602484,"= −∂γ(τ1(σ2
ε + Iβ
1,1))
τ 2
1
−σ2
ε
(S381)"
REFERENCES,0.9937888198757764,= Etrain
REFERENCES,0.9945652173913043,"γ2τ 2
1
−σ2
ε .
(S382)"
REFERENCES,0.9953416149068323,"Corollary G.1. In the setting of Theorem 3.1, as the ridge regularization constant γ →0, Eµ =
Bµ + Vµ with Bµ = φIβ
1,2 and Vµ given by"
REFERENCES,0.9961180124223602,"Vµ
γ→0
−→min(φ, ψ)"
REFERENCES,0.9968944099378882,"|φ −ψ| (σ2
ε + Iβ
1,1) +"
REFERENCES,0.9976708074534162,"(
xIβ
2,2
if φ < ψ
xI2,2
ω+φI1,2 (σ2
ε + Iβ
1,2)
otherwise ,
(S383)"
REFERENCES,0.9984472049689441,"where x is the unique positive real root of x = min(1,φ/ψ)"
REFERENCES,0.9992236024844721,"ω+I1,1
."
