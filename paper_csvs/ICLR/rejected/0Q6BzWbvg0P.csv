Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016891891891891893,"Designing deep networks robust to adversarial examples remains an open problem.
Likewise, recent zeroth-order hard-label attacks on image classiﬁcation models
have shown comparable performance to their ﬁrst-order, gradient-level alternatives.
It was recently shown in the gradient-level setting that regular adversarial examples
leave the data manifold, while their on-manifold counterparts are in fact generaliza-
tion errors. In this paper, we argue that query efﬁciency in the zeroth-order setting
is connected to an adversary’s traversal through the data manifold. To explain this
behavior, we propose an information-theoretic argument based on a noisy manifold
distance oracle, which leaks manifold information through the adversary’s gradient
estimate. Through numerical experiments of manifold-gradient mutual information,
we show this behavior acts as a function of the effective problem dimensional-
ity. On high-dimensional real-world datasets and multiple zeroth-order attacks
using dimension reduction, we observe the same behavior to produce samples
closer to the data manifold. This can result in up to 4x decrease in the manifold
distance measure, regardless of the model robustness. Our results suggest that
taking the manifold-gradient mutual information into account can thus inform
better robust model design in the future, and avoid leakage of the sensitive data
manifold information."
INTRODUCTION,0.0033783783783783786,"1
INTRODUCTION"
INTRODUCTION,0.005067567567567568,"Adversarial examples against deep learning models were originally investigated as blind spots in
classiﬁcation (Szegedy et al., 2013; Goodfellow et al., 2014). Formal methods for discovering these
blind spots emerged, which we denote as gradient-level attacks, and became the ﬁrst techniques to
reach widespread attention within the deep learning community (Papernot et al., 2016; Moosavi-
Dezfooli et al., 2015; Carlini & Wagner, 2016; 2017; Chen et al., 2018). In order to compute the
necessary gradient information, such techniques required access to the model parameters and a
sizeable query budget. These shortcomings were addressed by the creation of score-level attacks,
which only require the conﬁdence values output by the deep learning models (Fredrikson et al.,
2015; Tram`er et al., 2016; Chen et al., 2017; Ilyas et al., 2018). However, these attacks still rely on
models to divulge information that would be impractical to receive in real-world systems. By contrast,
hard-label attacks make no assumptions about receiving side information, and only the predicted
class is observable, thus providing the weakest, yet most realistic adversarial threat model. These
methods, which originated from a random-walk on the decision boundary (Brendel et al., 2017), have
been carefully reﬁned to offer convergence guarantees (Cheng et al., 2019), query efﬁciency (Chen
et al., 2019; Cheng et al., 2020), and capability in the physical world Feng et al. (2020)."
INTRODUCTION,0.006756756756756757,"Despite the steady improvements of hard-label attacks, open questions persist about their behavior,
and adversarial machine learning (AML) attacks at large. Adversarial examples were originally
assumed to lie in rare pockets of the input space (Goodfellow et al., 2014), but this conventional
wisdom was later challenged by the boundary tilting assumption (Tanay & Grifﬁn, 2016; Gilmer
et al., 2018), which adopts a “data-geometric” view of the input space living on a lower-dimensional
manifold. This is supported by Stutz et al. (2019), who suggest that regular adversarial examples
leave the data manifold, while on-manifold adversarial examples are generalization errors. From a
data-geometric perspective, an adversarial example’s distance to the manifold primarily describes"
INTRODUCTION,0.008445945945945946,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010135135135135136,"the amount of semantic features preserved during the attack process. This makes it advantageous to
produce on-manifold adversarial examples, since the adversary can exploit the inherent generalization
error of the model while producing samples that are semantically similar for humans. However, the
true data manifold is either difﬁcult or impossible to describe, and relying solely on approximations
of the manifold can lead to the creation of crude adversarial examples (Stutz et al., 2019)."
INTRODUCTION,0.011824324324324325,"In this paper, we adopt the boundary-tilting assumption and demonstrate an unexpected beneﬁt of
query-efﬁcient zeroth-order attacks, i.e., attacks enabled by the use of dimensionality reduction
techniques. These attacks are more likely to discover on-manifold examples, which we theoretically
demonstrate is the result of manifold-gradient mutual information. Our results suggest that this
quantity can increase as a function of the data dimensionality. This information leakage leads to
adversarial examples that are on-manifold generalization errors. With this knowledge, we empirically
demonstrate how to improve hard-label attacks in a generic yet principled way, and potentially
re-think their interaction with model robustness and public-facing systems in the near future."
INTRODUCTION,0.013513513513513514,"For clarity, we provide a block diagram of our claims and experiments in the Appendix (Section A.3).
Our speciﬁc contributions are as follows:"
INTRODUCTION,0.015202702702702704,"• Introduction of manifold distance oracle. To create on-manifold examples, the adversary must
(implicitly) leverage manifold information during the attack phase. We thus propose an information-
theoretic formulation of the noisy manifold distance (NMD) oracle, which can explain how zeroth-
order attacks craft on-manifold examples. We theoretically demonstrate on a Gaussian data model
that manifold-gradient mutual information can increase as a function of data dimensionality. We
empirically show this is true even on large-scale image datasets such as CIFAR-10 and ImageNet.
This ﬁnding relates to known behavior in the gradient-level setting, where semantic manifold priors
(e.g., shapes and textures) can be leaked from robust models (Engstrom et al., 2019)."
INTRODUCTION,0.016891891891891893,"• Reveal new insights of manifold feedback during query-efﬁcient zeroth-order search. In
practice, the data manifold is difﬁcult to characterize. We propose the use of three proxies for
manifold distance, which all show consistent results in terms of an adversary’s ability to search
near the manifold. This methodology allows us to empirically demonstrate the connection between
dimension reduction, model robustness, and manifold feedback from the model, beyond the known
convergence rates tied to dimensionality (Nesterov & Spokoiny, 2017). Our ﬁndings inform how to
search closer to the manifold (Table 1), reduce gradient deviation (Table 2), and improve query
efﬁciency (Figure 2) in a simple and generic way for hard-label attacks."
INTRODUCTION,0.018581081081081082,"• Attack-agnostic method for super-pixel grouping. We show that spatial dimension reduction of
a decision-based gradient estimate acts as an attack- and knowledge-agnostic method for searching
over super-pixels of an image. More importantly, this helps an attacker exploit a model’s reaction
to salient input changes, leading to samples closer to the manifold compared to the attack on full
dimension. As a result, we demonstrate up to 200% and 340% success rate improvement for
state-of-the-art hard-label attacks HSJA (Chen et al., 2019) and Sign-OPT attack (Cheng et al.,
2020), respectively."
RELATED WORK,0.02027027027027027,"2
RELATED WORK"
RELATED WORK,0.02195945945945946,"Since the original discovery of adversarial samples against deep models (Szegedy et al., 2013;
Goodfellow et al., 2014), the prevailing question was why such examples existed. The original
assumption was that adversarial examples lived in low-probability pockets of the input space, and were
never encountered during parameter optimization (Szegedy et al., 2013). This effect was believed to
be ampliﬁed by the linearity of weight activations in the presence of small perturbations (Goodfellow
et al., 2014). These assumptions were later challenged by the boundary tilting assumption, which in
summary 1) asserts that the train and test sets of a model only occupy a sub-manifold of the true data,
while the decision boundary lies close to samples on and beyond the sub-manifold (Tanay & Grifﬁn,
2016), and 2) supports the “data geometric“ view, where high-dimensional geometry of the true data
manifold enables a low-probability error set to exist (Gilmer et al., 2018). Likewise the boundary
tilting assumption describes adversarial samples as leaving the manifold, which has inspired defenses
based on projecting such samples back to the data manifold (Jalal et al., 2019; Samangouei et al.,
2018). However, these approaches were later defeated by adaptive attacks (Carlini et al., 2019; Carlini
& Wagner, 2017; Tramer et al., 2020)."
RELATED WORK,0.02364864864864865,Under review as a conference paper at ICLR 2022
RELATED WORK,0.02533783783783784,"We investigate the scenario where an adversary uses zeroth-order information (i.e., top-1 label feed-
back) to estimate the desired gradient direction (Cheng et al., 2020; Chen et al., 2019). Contemporary
attacks in this setting are variants of random gradient-free method (RGF) (Nesterov & Spokoiny,
2017), and rely on formulations which convert the top-1 (hard) label, which is a step function, into a
continuous real-valued function g : Rd →R, which takes search direction θ ∈Rd and outputs the
distance to the nearest adversarial example (Cheng et al., 2018). The gradient estimate is conceived
as a function of the gradient ∇g and can be estimated with either two samples of information (Sign-
OPT) (Cheng et al., 2020), or a single point (HopSkipJumpAttack) (Chen et al., 2019). Details of
speciﬁc formulations for each attack are provided in Section A.2 of the Appendix."
RELATED WORK,0.02702702702702703,"Query efﬁciency is a persistent desire in the study of hard-label attacks. One clue for achieving
efﬁciency comes from the theory of gradient estimation error and convergence, which shows that the
estimation cost is polynomial in d, the dimension of the optimized variable, thus motivating the use
of standard dimension-reduction techniques (Tu et al., 2019). However, to date it is not completely
understood how this relates to traversal through the data manifold. We leverage previous results of
the gradient-level setting (Stutz et al., 2019; Engstrom et al., 2019) to formulate an explanation of
manifold leakage during hard-label adversarial attacks."
NOISY MANIFOLD DISTANCE ORACLE,0.028716216216216218,"3
NOISY MANIFOLD DISTANCE ORACLE"
NOISY MANIFOLD DISTANCE ORACLE,0.030405405405405407,"Santurkar et al. (2019) demonstrate that the gradients of robust models have higher visual semantic
alignment with the data compared to gradients of standard models. We build on this ﬁnding by ﬁrst
assuming that the benign observable data generates from a true lower-dimension distribution. Under
the boundary-tilting assumption, this lower-dimension distribution forms a manifold onto which new
observations, either benign or adversarial, can be encoded (Tanay & Grifﬁn, 2016). Likewise, we
assume that deep learning models will learn a lower-dimension representation of the observable data,
e.g., feature layers of convolutional neural networks learn to encode training observations onto a
low dimension approximate manifold (Zhang et al., 2018). When an adversary creates adversarial
samples, they are leveraging a pathway that shadows the model gradient, not the true manifold.
Thus there is the possibility that adversarial samples are considered “off-manifold”, e.g., cannot be
expected to generate naturally from the true manifold. However, it is critical for adversarial samples
to be as close to the manifold as possible, since on-manifold adversarial examples can exploit the
fundamental generalization error of the model (Stutz et al., 2019). More formally, we deﬁne the
notion of manifold distance as follows."
NOISY MANIFOLD DISTANCE ORACLE,0.03209459459459459,"Deﬁnition 3.1 (Manifold Distance). Consider the benign sample x0 and adversarial counterpart
x. Assuming a perfect encoding back to the true manifold φ, the manifold distance is deﬁned as
d(φ(x0), φ(x)), where d is a distance function with the domain of the true manifold."
NOISY MANIFOLD DISTANCE ORACLE,0.033783783783783786,"Unfortunately, unless the true manifold for a dataset is known, it is impossible to deﬁne φ. Instead,
a proxy d′ can be used such that d′(x, x′) ∼d(φ(x), φ(x′)). In practice, one can implement d′
with any perceptual distance score, such as Learned Perceptual Image Patch Similarity (Zhang et al.,
2018). If relying on a distance measure d, such as the Lp-norm, an approximate encoder φ′(·) ∼φ(·)
can be learned using reconstruction-based training of autoencoders (Stutz et al., 2019), or leveraging
feature layers of convolutional neural networks (Zhang et al., 2018). We are interested in the class
of hard-label adversaries that implicitly minimize some proxy of the manifold distance. Given the
result of Santurkar et al. (2019), the robust model’s gradient could be treated as a manifold distance
oracle, because it leaks the direction towards its approximate manifold. As a result, the model acts
as an oracle responding to queries about manifold distance, or in other words, an implicit proxy
for manifold distance, d′. In the hard-label setting, the data manifold, true gradient, and model
parameters are not accessible. Thus we are interested in a decision-based version of the manifold
distance oracle, deﬁned as follows."
NOISY MANIFOLD DISTANCE ORACLE,0.03547297297297297,"Deﬁnition 3.2 (Noisy Manifold Distance Oracle). Consider a manifold distance oracle instantiating
d′, benign sample x0, and pair of adversarial samples (x′, x′′) such that d′(x0, x′) < d′(x0, x′′),
e.g., x′ is considered on-manifold while x′′ is not. In the hard-label setting, the noisy manifold
distance (NMD) oracle instantiates d′′ such that d′′(x0, x′) = 0 and d′′(x0, x′′) = 1."
NOISY MANIFOLD DISTANCE ORACLE,0.037162162162162164,"During a hard-label attack, the adversary searches in a direction that minimizes perceptual distance
to the original sample. Concurrently, the adversary can be said to implicitly minimize the expected"
NOISY MANIFOLD DISTANCE ORACLE,0.03885135135135135,Under review as a conference paper at ICLR 2022
NOISY MANIFOLD DISTANCE ORACLE,0.04054054054054054,"output of the NMD oracle, which is a binary indicator that a sample is on-manifold or not. Without
knowledge of the true (or approximate) manifold, this requires careful selection of the search direction
from the current sample. Since the search direction of contemporary hard-label attacks is synthesized
over expectation of a ball around the adversarial sample, we are interested in search directions such
as x0 −x′ which minimize the expected distance to the manifold."
NOISY MANIFOLD DISTANCE ORACLE,0.04222972972972973,"To formalize the entailed information in the NMD oracle, we turn to a standard result in data
processing, which states the following:"
NOISY MANIFOLD DISTANCE ORACLE,0.04391891891891892,"Deﬁnition 3.3 (Data Processing Inequality (DPI) (Beaudry & Renner, 2012)). If three random
variables form the Markov chain X →Y →Z, then their mutual information (MI) has the relation
I(X; Y ) ⩾I(X; Z)."
NOISY MANIFOLD DISTANCE ORACLE,0.04560810810810811,"We assume the data manifold M, the input gradient G, and the hard-label gradient estimate ¨G will
form the Markov chain M →G →¨G. This assumption is reasonable due to the observations by
Santurkar et al. (2019); modifying the sampled data manifold (e.g., by adding adversarial samples
through saddle-point optimization) causally induces a smoother loss surface, which imposes its
own gradient distribution. Likewise, the true gradient and gradient estimate of hard-label attack are
causally linked due to the estimate’s bounded variance (Cheng et al., 2020)."
NOISY MANIFOLD DISTANCE ORACLE,0.0472972972972973,"If I(M, G) is larger for adversarially robust models, by Deﬁnition 3.3 the upper bound on I(M, ¨G)
is larger, which means more manifold information could be leaked in the noisy gradient. This
information could be used to search in the direction where d′′ is minimized in expectation, leading
towards on-manifold examples. However, DPI only offers an upper bound, thus the distance decrease
is not guaranteed, only suggested. In the information theoretic sense, does this mean the gradients
of models robust in an ϵ-ball around each sample can reveal more information about the distance to
training data than standard models? An immediate follow-up concern is whether other factors can
inﬂuence the model to reveal this information, such as the problem dimensionality. As a ﬁrst step we
posit the following hypothesis:"
NOISY MANIFOLD DISTANCE ORACLE,0.048986486486486486,"Hypothesis 1. Consider the manifold distribution M which can generate data to train a natural
model with gradient distribution G, and train robust model with smoothed gradient distribution G′.
We posit that their manifold-gradient mutual information I has the relation I(M, G′) ≥I(M, G)."
NOISY MANIFOLD DISTANCE ORACLE,0.05067567567567568,"In order to empirically verify Hypothesis 1, we must parameterize the notion of model robustness
while solving for I(M, G), given an arbitrary gradient distribution G and manifold distribution M.
Schmidt et al. (2018) have shown that robust training requires additional data as a function of the
data dimensionality. We leverage the data model and results from Schmidt et al. (2018) to derive an
analytical solution for I(M, G), since we can parameterize model robustness as a function of data
size and dimensionality. Consequently, the remainder of our theoretical analysis assumes a Gaussian
mixture data model."
NOISY MANIFOLD DISTANCE ORACLE,0.052364864864864864,"Deﬁnition 3.4 (Data model and optimal weights (Schmidt et al., 2018)). Let µ ∈Rd be the per-class
centers (means) and let σ > 0 be the variance parameter. Then the (µ, σI)-Gaussian model is deﬁned
by the following distribution over (x, y) ∈Rd × {±1}: First, draw a label y ∈{±1} uniformly at
random. Then sample the data point x ∈Rd from N(y · µ, σI)."
NOISY MANIFOLD DISTANCE ORACLE,0.05405405405405406,"Deﬁnition 3.5 (Optimal classiﬁcation weight (Schmidt et al., 2018)). Fix σ ≤c1d
1
4 for the universal
constant c1, and samples (x1, y1), · · · , (xn, yn) drawn i.i.d from the (µ, σI)-Gaussian model with
||µ|| =
√"
NOISY MANIFOLD DISTANCE ORACLE,0.05574324324324324,"d (i.e., µk = 1 for all dimensions k ∈{0, . . . , d}). Schmidt et al. (2018) prove that the
weight setting bw = 1"
NOISY MANIFOLD DISTANCE ORACLE,0.057432432432432436,"n
Pn
i=1 yixi yields an lϵ
∞-robust classiﬁcation error of at most 1% for the linear
classiﬁer f bw : Rd →{±1} instantiated as f bw(x) = sign(bwT x) if n ≥"
NOISY MANIFOLD DISTANCE ORACLE,0.05912162162162162,"(
1,
for ϵ ≤1 4d−1 4 c2ϵ2√"
NOISY MANIFOLD DISTANCE ORACLE,0.060810810810810814,"d,
for
1
4d−1"
NOISY MANIFOLD DISTANCE ORACLE,0.0625,4 ≤ϵ ≤1
NOISY MANIFOLD DISTANCE ORACLE,0.06418918918918919,"4
,
(1)"
NOISY MANIFOLD DISTANCE ORACLE,0.06587837837837837,for a universal constant c2.
NOISY MANIFOLD DISTANCE ORACLE,0.06756756756756757,"Note that the instantiation of bw must change with choice of ϵ and d. We can leverage the weight
settings as a function of n and d to give a deﬁnition of manifold-gradient mutual information."
NOISY MANIFOLD DISTANCE ORACLE,0.06925675675675676,Under review as a conference paper at ICLR 2022
NOISY MANIFOLD DISTANCE ORACLE,0.07094594594594594,"101
102
103 d 0.0 0.1 0.2"
NOISY MANIFOLD DISTANCE ORACLE,0.07263513513513513,"I(
, ) , k (bits)"
NOISY MANIFOLD DISTANCE ORACLE,0.07432432432432433,c2 = 1
NOISY MANIFOLD DISTANCE ORACLE,0.07601351351351351,"= 0.000
 = 0.180
 = 0.250"
NOISY MANIFOLD DISTANCE ORACLE,0.0777027027027027,"101
102
103 d"
NOISY MANIFOLD DISTANCE ORACLE,0.07939189189189189,c2 = 102
NOISY MANIFOLD DISTANCE ORACLE,0.08108108108108109,"= 0.000
 = 0.180
 = 0.250"
NOISY MANIFOLD DISTANCE ORACLE,0.08277027027027027,"Figure 1: a) Average per-dimension mutual information (I) over dimension d for values of c2 and ϵ
in Equation 13, log-scale d-axis with d ∈[5, 2000), average over ten seeds. The approximate mutual
information is higher for robust and standard models at lower d regardless of c2 and choice of ϵ."
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.08445945945945946,"3.1
MANIFOLD-GRADIENT MUTUAL INFORMATION"
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.08614864864864864,"Notice the classiﬁer sgn(·) in Deﬁnition 3.5 is discontinuous at xk = 0 for any dimension k. Instead
we consider the sub-gradient of the classiﬁer at xk < 0 and xk > 0. In either case (non-robust
or robust), the input sub-gradient for f b
w(x′
k) is deﬁned dimension-wise for our isotropic Gaussian
as ∇x′
kf c
wk = sign wk. Since the weight of each dimension is Gaussian distributed with c
wk ∼
N(µk, σ2), we can deﬁne the distribution of gradients as G ∼Rademacher (P c
wk∼N [ c
wk ≥0]).
Using this fact, we deﬁne manifold-gradient mutual information in three parts: 1) deﬁning the
manifold-gradient point-wise joint probabilities between gk and xk at each dimension k for the
sub-gradient cases where xk > 0 and xk < 0, 2) deﬁning the manifold-gradient marginal probability
under the gradient, and 3) the marginal probability under the manifold. The complete derivation of
the joint and marginal probabilities can be found in Section A.1 of the Appendix. The three parts are
used in the standard deﬁnition of mutual information (Cover & Thomas, 2006)."
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.08783783783783784,"Notation.
Fix σ = c1d
1
4 for both cases. We denote the sub-manifold sampled from the positive
(y = 1) and negative (y = −1) classes as M+ and M−, respectively. For brevity we label xk > 0
as x+ and xk < 0 as x−."
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.08952702702702703,"Deﬁnition 3.6 (Manifold-Gradient Mutual Information). We deﬁne the manifold-gradient mutual
information, based on the standard deﬁnition of mutual information from information theory (Cover
& Thomas, 2006), as"
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.09121621621621621,"I(M, G)ϵ,k = 2
Z"
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.0929054054054054,"M+ p(1, x+) log(
p(1, x+)
pG(1)pM(x+)) dx+ + 2
Z"
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.0945945945945946,"M+ p(−1, x+) log(
p(−1, x+)
pG(−1)pM(x+)) dx+. (2)"
MANIFOLD-GRADIENT MUTUAL INFORMATION,0.09628378378378379,"with the total unnormalized mutual information deﬁned as the summation over dimensions (due to
dimension co-independence) I(M, G)ϵ = Pd
k=1 I(M, G)ϵ,k."
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.09797297297297297,"3.2
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY"
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.09966216216216216,"To provide numerical support for Hypothesis 1, we run experiments using the Riemann approximation
of Equation 13, provided in the Appendix as Equation 15. We estimate the average per-dimension
mutual information, I(M, G)ϵ,k = I(M,G)ϵ"
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.10135135135135136,"d
, for the case where x ∈Rd while varying the dimen-
sionality term d ∈[5, 2000) against values of c2 ∈{1, 100} and ϵ ∈{0.000, 0.180, 0.250}. The
values of c2 represent two multiplicative factors for number of samples in robust models (Equation 1).
In our experiments, we target an error within 10−1 (e.g., 0.9 ≤pG(1) + pG(−1) ≤1.0). Thus we
multiply each branch of Equation 1 by a large constant (104). We run the approximation over ten
different random seeds and show the average with standard error shaded."
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.10304054054054054,"The estimation result is shown in Figure 1 with log-scale x-axis. Regardless of c2 and ϵ, lower values
of the dimensionality evidence a higher mutual information. We minimize variance of the estimate
when c2 = 102 (right plot shaded area), which follows intuition due to the higher sample count in the
estimate."
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.10472972972972973,Under review as a conference paper at ICLR 2022
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.10641891891891891,"Observation 1. Given reduced data dimensionality, a robust model could increase I(M, G)ϵ,k and
lead to leaking better search direction through the gradient (e.g., act as manifold distance oracle).
This supports Hypothesis 1."
MUTUAL INFORMATION AS A FUNCTION OF DIMENSIONALITY,0.10810810810810811,"This can theoretically explain the high visual alignment observed empirically by Engstrom et al.
(2019) and Santurkar et al. (2019) on robust models. From the security perspective, the NMD oracle
acts as a side channel leaking sensitive information as a factor of the model robustness and data
dimensionality."
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE,0.1097972972972973,"4
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE"
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE,0.11148648648648649,"According to Observation 1, the true gradient and manifold of a robust model have higher mutual
information, and this is exacerbated by reducing the data dimensionality. Under our Markov chain
assumption, this means an attack algorithm can act as a noisy manifold distance oracle, and this
oracle could be upper bounded by the true gradient-manifold mutual information. Although the
data dimensionality and robustness are controlled by the model designer, an attacker can search in
arbitrarily lower dimensionality through dimension-reduction techniques, such as autoencoder-based
attacks (Tu et al., 2019). In fact, in the image domain the intrinsic dimensionality of data can be lower
than the true dimension (Amsaleg et al., 2017). In order to connect the notion of manifold-gradient
mutual information with on-manifold adversarial samples of real datasets, we posit the following."
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE,0.11317567567567567,"Hypothesis 2. Consider Observation 1 and Deﬁnition 3.3 (DPI), then due to the higher upper
bound on I(M, ¨G) and leaking better search directions, a hard-label adversary can minimize d′′ in
expectation on robust models when the gradient estimate dimensionality is reduced."
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE,0.11486486486486487,"In the most common problem setting, the adversary is interested in attacking a K-way multi-
class classiﬁcation model f : Rd →{1, . . . , K}. Given an original example x0, the goal is to
generate adversarial example x such that x is close to x0 and f(x) ̸= f(x0), where closeness is
often approximated by the Lp-norm of x −x0. In the gradient-level setting, we require the gradient
∇f(·). However, in the hard-label setting we are forced to estimate ∂f(x)"
ZEROTH-ORDER SEARCH THROUGH THE MANIFOLD DISTANCE ORACLE,0.11655405405405406,"∂x
without access to ∇f(·),
only decision evaluations of f. Rather than optimizing the step function f, hard-label attacks minimize
the continuous function g(θ), which is an estimate of the distance to the nearest decision boundary
in the direction θ. We evaluate the effect of dimension reduction on Sign-OPT attack (Cheng et al.,
2020) and HopSkipJumpAttack (HSJA) (Chen et al., 2019), as both are considered state-of-the-art in
the literature, and rely on minimization of g(·). We provide a brief overview of their formulation in
Section A.2 of the Appendix, and leave details to the respective authors’ work. Alternative hard-label
attacks, such as RayS by Chen & Gu (2020), do not rely on the explicit zeroth-order gradient estimate
from the model. This style of attack behaves differently since it can adapt to the problem dimension
independent of the true gradient, which we demonstrate in Section A.5.6 of the Appendix."
DIMENSION-REDUCED ZEROTH-ORDER SEARCH,0.11824324324324324,"4.1
DIMENSION-REDUCED ZEROTH-ORDER SEARCH"
DIMENSION-REDUCED ZEROTH-ORDER SEARCH,0.11993243243243243,"To test Hypothesis 2, we modify existing hard-label attacks to produce dimension-reduced variants.
This scheme enables dynamic scaling of the effective dimensionality regardless of speciﬁc attack
formulation. In practice we implement the reduction through an encoding map E : Rd →Rd′ for
reduced dimension d′ and decoding map D : Rd′ →Rd. In general the adversarial sample is created
by x = x0 + g (D(θ′))
D(θ′)
||D(θ′)||, where θ′ ∈Rd′ and is optimized depending on the respective attack
(e.g., Sign-OPT and HSJA), and as before, g is a measure of distance to the decision boundary in
direction D(θ′). The mapping functions can be initialized with either an autoencoder (AE), or a
pair of channel-wise bilinear transform functions (henceforth referred to as BiLN) which simply
scales the spatial dimension of the input up or down. This represents two distinct methods to search
over super-pixels of the image, which either rely on an approximate description of the manifold
(AE), or instead exploit the known spatial co-dependence of images (BiLN). The implementation
and training details of the AE variant can be found in Section A.4.2 of the Appendix. To study
the effect of dimension-reduction without semantic information, we implement a random variant of
BiLN (Rand) which samples a subset of coordinates uniform-randomly from the source image as the
dimension-reduced version, then replaces the pixels at these coordinates with those from the gradient"
DIMENSION-REDUCED ZEROTH-ORDER SEARCH,0.12162162162162163,Under review as a conference paper at ICLR 2022
DIMENSION-REDUCED ZEROTH-ORDER SEARCH,0.12331081081081081,"estimate. This is meant to show the effect of discarding some semantic information (e.g., spatial
correlation) in the update."
ESTIMATING MANIFOLD DISTANCE,0.125,"4.2
ESTIMATING MANIFOLD DISTANCE"
ESTIMATING MANIFOLD DISTANCE,0.1266891891891892,"We leverage three proxies of manifold distance in order to test Hypothesis 2. The Learned Perceptual
Image Patch Similarity (LPIPS) acts as a proxy for manifold distance, d′, and computes a distance
that correlated well with human perception in human studies (Zhang et al., 2018; Laidlaw et al.,
2021). We use the same LPIPS code and checkpoint provided by the authors. Fr´echet Inception
Distance (FID) (Heusel et al., 2018) is similar to LPIPS, and leverages the internal representations
of deep networks as an approximate encoding onto the manifold. Although FID lacks human
studies, Heusel et al. (2018) show it is viable for scoring the visual quality of synthetically generated
images, which offers us a comparison against LPIPS. In addition to LPIPS and FID, we create an
approximate encoding φ′ by taking the encoder of trained autoencoders for each dataset, which can
be used to compute L∞distance between encoded samples. In other words, this lets us compute
||φ′(x0) −φ′(x)||∞for benign sample x0 and adversarial sample x. The results on FID and our
trained autoencoder were consistent with LPIPS, so they are described in Section A.5.9 of the
Appendix."
ESTIMATING MANIFOLD DISTANCE,0.12837837837837837,"Finally, if hard-label gradient estimates on real-world data resulted in a sample close to the approx-
imate manifold, we could say the gradient estimates leveraged noisy mutual information, which
may be upper bounded by the clean mutual information (Hypothesis 1). This would manifest in
a lower gradient deviation, or in other words, the distance between the true gradient and gradient
estimate at the ﬁrst attack step. We can further infer that the adversarial training effectively smooths
the sampled data manifold (which generates from true manifold) by augmenting perturbed data
samples during training. The smoothing yields a well-deﬁned boundary that aligns with salient input
changes (Santurkar et al., 2019), and should further lower variance of the gradient estimate compared
to natural models, which improves the baseline performance of an attack. We test this by calculating
per-pixel gradient deviation ||g−ˆg||2"
ESTIMATING MANIFOLD DISTANCE,0.13006756756756757,"H×W
for true gradient g (in the direction of the adversarial label), ﬁrst
gradient estimate ˆg, estimate height H, and estimate width W. When taking the true input gradient
in the direction of the adversarial label, we use the victim model’s original criterion to calculate the
gradient, which was cross-entropy for all models in our evaluation."
RESULTS & DISCUSSION,0.13175675675675674,"5
RESULTS & DISCUSSION"
RESULTS & DISCUSSION,0.13344594594594594,"We test Hypothesis 2 by comparing two SotA hard-label attacks with their compatible dimension-
reduced variants, against both natural and robust models. First we show empirical evidence of
the relationship between manifold distance and dimension-reduced attacks in Section 5.1. Next in
Section 5.2, we investigate the result of Section 5.1 from the perspective of reducing error in the
gradient estimate. Finally in Section 5.3, we show how these observations inform better attack design."
RESULTS & DISCUSSION,0.13513513513513514,"Setup. We perform experiments using CIFAR-10 (Krizhevsky, 2009) and ImageNet (Krizhevsky
et al., 2012) for RGB image data. The natural CIFAR-10 network is the same implementation open-
sourced by Cheng et al. (2020). The architecture for ImageNet is the Resnet50 network taken from
the PyTorch Torchvision library, and the accompanying pre-trained weights act as the natural model.1
In addition, we leverage the representative adversarial training technique proposed by Madry et al.
(2017) (and their ϵ =
8
255 = 0.031 checkpoints for L∞setting) as the robust models for CIFAR-10
and ImageNet. The BiLN variants downscale to 16 × 16 for CIFAR-10, and 32 × 32 for ImageNet.
We use L∞-norm versions of attacks for all experiments, and the same ϵ values for natural models
as the robust CIFAR-10 and robust ImageNet (hereafter referred to as Madry CIFAR-10 and Madry
ImageNet). All attacks run for 25k queries without early stopping on correctly classiﬁed samples.
For brevity, we only show results for the untargeted case. Additional implementation details, such
as hyperparameters and hardware used, can be found in the Appendices (Section A.4). Code for
experiments is provided in the supplementary materials."
RESULTS & DISCUSSION,0.13682432432432431,1https://pytorch.org/docs/stable/torchvision/models.html
RESULTS & DISCUSSION,0.13851351351351351,Under review as a conference paper at ICLR 2022
RESULTS & DISCUSSION,0.14020270270270271,"Attack Variant
Natural
CIFAR-10
Madry
CIFAR-10
Natural
ImageNet
Madry
ImageNet"
RESULTS & DISCUSSION,0.14189189189189189,"HSJA
0.132 ± 0.098⋆
1.335 ± 0.611
0.257 ± 0.378
1.249 ± 0.652
HSJA+BiLN
0.252 ± 0.165↑
1.147 ± 0.535↓⋆
0.170 ± 0.143↓⋆
1.205 ± 0.711↓⋆"
RESULTS & DISCUSSION,0.14358108108108109,"HSJA+Rand
1.433 ± 0.747↑
2.384 ± 0.503↑
1.276 ± 0.649↑
1.183 ± 0.596↓"
RESULTS & DISCUSSION,0.14527027027027026,"Sign-OPT
0.105 ± 0.081
0.768 ± 0.408
0.768 ± 0.872
1.229 ± 0.771
Sign-OPT+BiLN
0.225 ± 0.146↑
0.849 ± 0.397↑
0.176 ± 0.204↓
0.708 ± 0.461↓
Sign-OPT+Rand
0.440 ± 0.464↑
1.021 ± 0.593↑
0.356 ± 0.385↓
0.367 ± 0.361↓
Sign-OPT+AE
0.331 ± 0.389↑
0.660 ± 0.302↓
1.034 ± 0.571↑
1.658 ± 0.638↑"
RESULTS & DISCUSSION,0.14695945945945946,"Table 1: Average LPIPS scores for each attack’s set of 200 adversarial samples on CIFAR-10 and
ImageNet (lower is better). Arrows denote higher or lower score compared to baseline variant, and
starred items indicate highest success rate."
RESULTS & DISCUSSION,0.14864864864864866,"Med. Benign Local ID
0.469
0.224
1.039
2.013"
RESULTS & DISCUSSION,0.15033783783783783,"Attack Variant
Natural
CIFAR-10
Madry
CIFAR-10
Natural
ImageNet
Madry
ImageNet"
RESULTS & DISCUSSION,0.15202702702702703,"HSJA
6.65 ± 0.61⋆
5.46 ± 0.06
77.35 ± 0.04
77.32 ± 0.00
HSJA+BiLN
5.37 ± 0.69↓
3.86 ± 0.10↓⋆
55.12 ± 1.37↓⋆
56.14 ± 0.12↓⋆"
RESULTS & DISCUSSION,0.15371621621621623,"HSJA+Rand
11.33 ± 7.41↑
2.01 ± 1.65↓
72.19 ± 59.98↓
3.22 ± 2.73↓"
RESULTS & DISCUSSION,0.1554054054054054,"Sign-OPT
3.72 ± 0.99
0.71 ± 0.38
1.70 ± 1.01
0.55 ± 0.18
Sign-OPT+BiLN
3.71 ± 1.02↓
0.78 ± 0.35↑
1.83 ± 0.97↑
1.74 ± 0.56↑
Sign-OPT+Rand
8.21 ± 6.67↑
2.32 ± 2.07↑
37.54 ± 46.20↑
6.72 ± 1.54↑
Sign-OPT+AE
4.66 ± 0.86↑
2.48 ± 0.32↑
36.83 ± 0.15↑
36.87 ± 0.31↑"
RESULTS & DISCUSSION,0.1570945945945946,"Table 2: Average per-pixel gradient deviation on natural and robust CIFAR-10 (unit of 10−2) and
ImageNet (unit of 10−4) over 200 samples. Top row lists the median Local Intrinsic Dimensionality
(LID) of benign samples from the dataset. Arrows denote higher or lower deviation compared to
baseline variant, and starred items indicate highest success rate."
MANIFOLD DISTANCE,0.15878378378378377,"5.1
MANIFOLD DISTANCE"
MANIFOLD DISTANCE,0.16047297297297297,"LPIPS results are shown in Table 1, with colored arrows denoting either lower distance than baseline
variant (green arrow), or a higher distance (red arrow). Generally, the dimension-reduced variants
lower the proxy of manifold distance on ImageNet more often than on CIFAR-10 (green arrows). The
random sampling variant (⋆-Rand) discards the semantic priors of the estimate, and in fact it achieved
the lowest SR AUC scores, despite having lower scores. Our results using LPIPS are consistent
with L∞distance of the manifold approximation (Section A.5.9), and Fr´echet Inception Distance
(Section A.5.8), which all demonstrate a tendency to be lower with dimension-reduced attacks.
Observation 2. Dimension-reduced hard-label attacks can have lower LPIPS score, L∞approxi-
mated distance, and Fr´echet Inception Distance (and thus lower manifold distance) on robust models
if they preserve semantic priors in the update, which supports Hypothesis 2."
GRADIENT DEVIATION,0.16216216216216217,"5.2
GRADIENT DEVIATION"
GRADIENT DEVIATION,0.16385135135135134,"The results for gradient deviation are shown in Table 2. Notably, an attack can have high gradient
deviation despite low LPIPS score (AE case, bottom row). Likewise, low deviation does not imply
successful attack, as we show later with the Rand variant (rows three and six). We investigated why
Madry ImageNet did not always have lower gradient deviation, which we posit is due to having a
higher true dimensionality. For the benign samples of each dataset we estimated the Local Intrinsic
Dimensionality (LID), which was proposed to estimate true data dimensionality in a region around
samples (Amsaleg et al., 2017). In the top row of Table 2 we ﬁnd the median LID is similar between
natural and robust CIFAR-10, but much higher on robust ImageNet than natural. Since our results
of Section 3 suggested that higher problem dimension reduced mutual information, we suspect
the Madry ImageNet model reduces the leakage through the NMD oracle through higher true data
dimensionality. We leave a deeper analysis of this direction for future work. Results on additional"
GRADIENT DEVIATION,0.16554054054054054,Under review as a conference paper at ICLR 2022 0.00 0.25 0.50 0.75 1.00
GRADIENT DEVIATION,0.16722972972972974,Success Rate
GRADIENT DEVIATION,0.16891891891891891,Natural CIFAR-10
GRADIENT DEVIATION,0.17060810810810811,ε = 0.031
GRADIENT DEVIATION,0.17229729729729729,Madry CIFAR-10
GRADIENT DEVIATION,0.17398648648648649,ε = 0.031
GRADIENT DEVIATION,0.17567567567567569,"HSJA
BiLN+HSJA 16
Rand+HSJA 16"
GRADIENT DEVIATION,0.17736486486486486,"0
10000
20000"
GRADIENT DEVIATION,0.17905405405405406,# of queries 0.00 0.25 0.50 0.75 1.00
GRADIENT DEVIATION,0.18074324324324326,Success Rate
GRADIENT DEVIATION,0.18243243243243243,"0
10000
20000"
GRADIENT DEVIATION,0.18412162162162163,# of queries
GRADIENT DEVIATION,0.1858108108108108,"Sign-OPT
Rand+Sign-OPT 16
BiLN+Sign-OPT 16
AE+Sign-OPT (a) 0.00 0.25 0.50 0.75 1.00"
GRADIENT DEVIATION,0.1875,Success Rate
GRADIENT DEVIATION,0.1891891891891892,Natural ImageNet
GRADIENT DEVIATION,0.19087837837837837,ε = 0.031
GRADIENT DEVIATION,0.19256756756756757,Madry ImageNet
GRADIENT DEVIATION,0.19425675675675674,ε = 0.031
GRADIENT DEVIATION,0.19594594594594594,"HSJA
BiLN+HSJA 32
Rand+HSJA 32"
GRADIENT DEVIATION,0.19763513513513514,"0
10000
20000"
GRADIENT DEVIATION,0.19932432432432431,# of queries 0.00 0.25 0.50 0.75 1.00
GRADIENT DEVIATION,0.20101351351351351,Success Rate
GRADIENT DEVIATION,0.20270270270270271,"0
10000
20000"
GRADIENT DEVIATION,0.20439189189189189,# of queries
GRADIENT DEVIATION,0.20608108108108109,"Sign-OPT
BiLN+Sign-OPT 32
Rand+Sign-OPT 32
AE+Sign-OPT (b)"
GRADIENT DEVIATION,0.20777027027027026,Figure 2: Success rates across attacks over 200 samples on CIFAR-10 (a) and ImageNet (b).
GRADIENT DEVIATION,0.20945945945945946,"robust CIFAR-10 models are provided in Section A.5.2 of the Appendix, which exhibited a similar
trend of lower gradient deviation. Sign-OPT has a universally lower gradient deviation than HSJA,
which aligns with ﬁndings of Liu et al. (2020)."
GRADIENT DEVIATION,0.21114864864864866,"Observation 3. The gradient deviation is universally lower on the robust CIFAR-10 model for BiLN
attacks (rows two and ﬁve). For ImageNet, deviation on robust models is either lower or similar
(rows one, two, four, and seven)."
INFORMING PRACTICE,0.21283783783783783,"5.3
INFORMING PRACTICE"
INFORMING PRACTICE,0.21452702702702703,"We have shown that dimension reduction has unexpected consequences in terms of manifold distance,
and on CIFAR-10 and some ImageNet cases, leads to a lower gradient deviation on the robust model.
We ﬁnalize our contribution by providing a comprehensive evaluation of the attack success rates
in Figure 2 against number of queries. The plots are quantiﬁed by taking their max-normalized
Trapezoid rule area-under-curve (AUC).2 For comparison, the highest AUC scores are starred in the
previous tables. Our dimension-reduced HSJA+BiLN variant (yellow line) surpasses the previous
SotA hard-label attack for ImageNet, HSJA, on both natural and robust models. This variant also
exhibited the lowest LPIPS score across attack variants. However, lowest LPIPS score does not imply
highest SR, evidenced with HSJA+Rand on natural ImageNet (brown line, AUC= 0.077) and Sign-
OPT variants on either dataset (e.g., yellow line in Madry ImageNet, AUC= 0.215). Low gradient
deviation does not imply higher attack success, evidenced by Sign-OPT+BiLN in Table 2 for Madry
CIFAR-10 (AUC= 0.156) or HSJA+Rand and Sign-OPT+Rand (AUC= 0.088 and AUC= 0.092,
respectively). The Rand variants, combined with our ﬁndings so far, allow us to say the following."
INFORMING PRACTICE,0.21621621621621623,"Observation 4. Successful attacks exhibit preservation of leaked semantic priors. Measures of
manifold distance such as LPIPS tend to be lower on dimension-reduced attacks, independent of
variance in the gradient estimate."
INFORMING PRACTICE,0.2179054054054054,"We posit that minimizing gradient deviation through correction of estimator bias alone could be
misleading, since the semantic information provided by a better NMD oracle (due to dimension
reduction) can potentially improve the gradient deviation. Although our theoretical analysis focuses
on robust models, we suspect future hard-label attacks may treat ϵ as a useful prior, which carries
with it implications about when to deploy robust models in society. On the contrary, natural models
will respond to any input changes, even if they are semantically meaningless (Santurkar et al., 2019),
so depending on the adversary’s goal (e.g., evasion or information leakage), they could be less useful
in the hard-label setting."
INFORMING PRACTICE,0.2195945945945946,2Tables with all normalized SR AUC scores are provided in Section A.5.3 of the Appendix.
INFORMING PRACTICE,0.22128378378378377,Under review as a conference paper at ICLR 2022
CONCLUSION,0.22297297297297297,"6
CONCLUSION"
CONCLUSION,0.22466216216216217,"Despite the recent progress in zeroth-order attack methods, open questions remain about their precise
behavior. We develop an information-theoretic analysis that sheds light on their ability to produce
on-manifold adversarial examples. Through experiments on real-world datasets, we show an over
two-fold increase in attack success rates by leveraging new ﬁndings about manifold distance and
gradient deviation. With knowledge of the manifold-gradient relationship, it is possible to further
reﬁne hard-label attacks, and inform a better evaluation of model robustness. Given the availability of
larger datasets in the future, our method may turn the strength of deep learning, which is efﬁciently
extracting patterns in large-scale data, into a weakness."
REFERENCES,0.22635135135135134,REFERENCES
REFERENCES,0.22804054054054054,"L. Amsaleg, J. Bailey, D. Barbe, S. Erfani, M. E. Houle, V. Nguyen, and M. Radovanovi´c. The
vulnerability of learning to adversarial perturbation increases with intrinsic dimensionality. In
2017 IEEE Workshop on Information Forensics and Security (WIFS), pp. 1–6, December 2017.
doi: 10.1109/WIFS.2017.8267651."
REFERENCES,0.22972972972972974,"Normand J. Beaudry and Renato Renner. An intuitive proof of the data processing inequality.
arXiv:1107.0740 [quant-ph], September 2012. URL http://arxiv.org/abs/1107.0740.
arXiv: 1107.0740."
REFERENCES,0.23141891891891891,"Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-Based Adversarial Attacks: Reliable
Attacks Against Black-Box Machine Learning Models. arXiv:1712.04248 [cs, stat], December
2017. URL http://arxiv.org/abs/1712.04248. arXiv: 1712.04248."
REFERENCES,0.23310810810810811,"Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. In
Security and Privacy (SP), pp. 582–597, 2016. ISBN 978-1-5090-5533-3. doi: 10.1109/SP.2017.49.
arXiv: 1608.04644 ISSN: 10816011."
REFERENCES,0.23479729729729729,"Nicholas Carlini and David Wagner. Adversarial Examples Are Not Easily Detected: Bypassing
Ten Detection Methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security - AISec ’17, pp. 3–14, Dallas, Texas, USA, 2017. ACM Press. ISBN 978-1-4503-5202-
4. doi: 10.1145/3128572.3140444. URL http://dl.acm.org/citation.cfm?doid=
3128572.3140444."
REFERENCES,0.23648648648648649,"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras,
Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On Evaluating Adversarial Robustness.
arXiv:1902.06705 [cs, stat], February 2019. URL http://arxiv.org/abs/1902.06705.
arXiv: 1902.06705."
REFERENCES,0.23817567567567569,"Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright. HopSkipJumpAttack: A Query-Efﬁcient
Decision-Based Attack. arXiv:1904.02144 [cs, math, stat], April 2019. URL http://arxiv.
org/abs/1904.02144. arXiv: 1904.02144."
REFERENCES,0.23986486486486486,"Jinghui Chen and Quanquan Gu. RayS: A Ray Searching Method for Hard-label Adversarial Attack.
arXiv:2006.12792 [cs, stat], June 2020. URL http://arxiv.org/abs/2006.12792.
arXiv: 2006.12792."
REFERENCES,0.24155405405405406,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In ACM Workshop on Artiﬁcial Intelligence and Security, pp. 15–26, 2017."
REFERENCES,0.24324324324324326,"Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to
deep neural networks via adversarial examples. In Thirty-second AAAI conference on artiﬁcial
intelligence, 2018."
REFERENCES,0.24493243243243243,"Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-Efﬁcient
Hard-label Black-box Attack:An Optimization-based Approach. arXiv:1807.04457 [cs, stat], July
2018. URL http://arxiv.org/abs/1807.04457. arXiv: 1807.04457."
REFERENCES,0.24662162162162163,Under review as a conference paper at ICLR 2022
REFERENCES,0.2483108108108108,"Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-
efﬁcient hard-label black-box attack: An optimization-based approach. International Conference
on Learning Representations, 2019."
REFERENCES,0.25,"Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. SIGN-
OPT: A QUERY-EFFICIENT HARD-LABEL ADVERSARIAL ATTACK. The International
Conference on Learning Representations (ICLR), pp. 16, 2020. URL https://openreview.
net/forum?id=SklTQCNtvS."
REFERENCES,0.2516891891891892,"Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed Adversarial Robustness via Ran-
domized Smoothing. arXiv:1902.02918 [cs, stat], February 2019. URL http://arxiv.org/
abs/1902.02918. arXiv: 1902.02918."
REFERENCES,0.2533783783783784,"Thomas M Cover and Joy A Thomas. Elements of Information Theory. Wiley-Interscience. John
Wiley & Sons, 2nd edition, 2006."
REFERENCES,0.25506756756756754,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry.
Learning Perceptually-Aligned Representations via Adversarial Robustness.
arXiv:1906.00945 [cs, stat], June 2019. URL http://arxiv.org/abs/1906.00945.
arXiv: 1906.00945."
REFERENCES,0.25675675675675674,"Ryan Feng, Jiefeng Chen, Nelson Manohar, Earlence Fernandes, Somesh Jha, and Atul
Prakash.
Query-Efﬁcient Physical Hard-Label Attacks on Deep Learning Visual Classiﬁca-
tion. arXiv:2002.07088 [cs], February 2020. URL http://arxiv.org/abs/2002.07088.
arXiv: 2002.07088."
REFERENCES,0.25844594594594594,"Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks that Exploit Con-
ﬁdence Information and Basic Countermeasures. Proceedings of the 22nd ACM SIGSAC Con-
ference on Computer and Communications Security - CCS ’15, pp. 1322–1333, 2015. ISSN
15437221. doi: 10.1145/2810103.2813677. URL http://dl.acm.org/citation.cfm?
doid=2810103.2813677. ISBN: 9781450338325."
REFERENCES,0.26013513513513514,"Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. The Relationship Between High-Dimensional Geometry and Adversarial
Examples. arXiv:1801.02774 [cs], September 2018. URL http://arxiv.org/abs/1801.
02774. arXiv: 1801.02774."
REFERENCES,0.26182432432432434,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. 2014. ISSN 0012-7183. URL http://arxiv.org/abs/1412.6572. arXiv:
1412.6572 ISBN: 1412.6572."
REFERENCES,0.2635135135135135,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.
arXiv:1706.08500 [cs, stat], January 2018. URL http://arxiv.org/abs/1706.08500.
arXiv: 1706.08500."
REFERENCES,0.2652027027027027,"Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box Adversarial Attacks
with Limited Queries and Information. arXiv:1804.08598 [cs, stat], July 2018. URL http:
//arxiv.org/abs/1804.08598. arXiv: 1804.08598."
REFERENCES,0.2668918918918919,"Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G. Dimakis. The Robust Manifold
Defense: Adversarial Training using Generative Models. arXiv:1712.09196 [cs, stat], July 2019.
URL http://arxiv.org/abs/1712.09196. arXiv: 1712.09196."
REFERENCES,0.2685810810810811,"Kim Jungeum and Xiao Wang.
Sensible Adversarial Learning.
2020.
URL https://
openreview.net/pdf?id=rJlf_RVKwr."
REFERENCES,0.2702702702702703,"Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. pp. 60, 2009."
REFERENCES,0.2719594594594595,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
ImageNet Classiﬁcation with
Deep Convolutional Neural Networks.
In F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp.
1097–1105. Curran Associates, Inc., 2012.
URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf."
REFERENCES,0.27364864864864863,Under review as a conference paper at ICLR 2022
REFERENCES,0.27533783783783783,"Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual Adversarial Robustness: Defense Against
Unseen Threat Models. arXiv:2006.12655 [cs, stat], July 2021. URL http://arxiv.org/
abs/2006.12655. arXiv: 2006.12655."
REFERENCES,0.27702702702702703,"Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred Hero, and Pramod K. Varsh-
ney.
A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning.
arXiv:2006.06224 [cs, eess, stat], June 2020. URL http://arxiv.org/abs/2006.06224.
arXiv: 2006.06224."
REFERENCES,0.27871621621621623,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv:1706.06083 [cs, stat],
June 2017. URL http://arxiv.org/abs/1706.06083. arXiv: 1706.06083."
REFERENCES,0.28040540540540543,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple
and accurate method to fool deep neural networks. 2015. ISSN 10636919. doi: 10.1109/
CVPR.2016.282. URL http://arxiv.org/abs/1511.04599. arXiv: 1511.04599 ISBN:
9781467388511."
REFERENCES,0.28209459459459457,"Yurii Nesterov and Vladimir Spokoiny. Random Gradient-Free Minimization of Convex Functions.
Foundations of Computational Mathematics, 17(2):527–566, April 2017. ISSN 1615-3375, 1615-
3383. doi: 10.1007/s10208-015-9296-2. URL http://link.springer.com/10.1007/
s10208-015-9296-2."
REFERENCES,0.28378378378378377,"Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. Proceedings - 2016 IEEE European
Symposium on Security and Privacy, EURO S and P 2016, pp. 372–387, 2016. doi: 10.1109/
EuroSP.2016.36. URL http://arxiv.org/abs/1511.07528. arXiv: 1511.07528 ISBN:
9781509017515."
REFERENCES,0.28547297297297297,"Pouya Samangouei, Maya Kabkab, and Rama Chellappa.
Defense-gan: Protecting classiﬁers
against adversarial attacks using generative models. In International Conference on Learning
Representations, 2018."
REFERENCES,0.28716216216216217,"Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander
Madry. Image Synthesis with a Single (Robust) Classiﬁer. arXiv:1906.09453 [cs, stat], June 2019.
URL http://arxiv.org/abs/1906.09453. arXiv: 1906.09453."
REFERENCES,0.28885135135135137,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially Robust Generalization Requires More Data. arXiv:1804.11285 [cs, stat], May 2018. URL
http://arxiv.org/abs/1804.11285. arXiv: 1804.11285."
REFERENCES,0.2905405405405405,"David Stutz, Matthias Hein, and Bernt Schiele. Disentangling Adversarial Robustness and General-
ization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12. IEEE
Computer Society, 2019."
REFERENCES,0.2922297297297297,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. pp. 1–10, 2013. ISSN 15499618. doi:
10.1021/ct2009208. URL http://arxiv.org/abs/1312.6199. arXiv: 1312.6199 ISBN:
1549-9618."
REFERENCES,0.2939189189189189,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.2956081081081081,"Thomas Tanay and Lewis Grifﬁn. A Boundary Tilting Persepective on the Phenomenon of Adversarial
Examples. arXiv:1608.07690 [cs, stat], August 2016. URL http://arxiv.org/abs/1608.
07690. arXiv: 1608.07690."
REFERENCES,0.2972972972972973,"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On Adaptive Attacks
to Adversarial Example Defenses. arXiv:2002.08347 [cs, stat], February 2020. URL http:
//arxiv.org/abs/2002.08347. arXiv: 2002.08347."
REFERENCES,0.2989864864864865,Under review as a conference paper at ICLR 2022
REFERENCES,0.30067567567567566,"Florian Tram`er, Fan Zhang, Floriantra Er Epﬂ, Ari Juels, Michael K Reiter, and Thomas
Ristenpart.
Stealing Machine Learning Models via Prediction APIs.
2016.
URL https:
//www.usenix.org/conference/usenixsecurity16/technical-sessions/
presentation/tramer. ISBN: 978-1-931971-32-4."
REFERENCES,0.30236486486486486,"Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh,
and Shin-Ming Cheng. AutoZOOM: Autoencoder-Based Zeroth Order Optimization Method
for Attacking Black-Box Neural Networks. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 33:742–749, July 2019. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai.v33i01.
3301742. URL https://aaai.org/ojs/index.php/AAAI/article/view/3852."
REFERENCES,0.30405405405405406,"Haichao Zhang and Jianyu Wang. Defense Against Adversarial Attacks Using Feature Scattering-
based Adversarial Training. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle
Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32,
pp. 1831–1841. Curran Associates, Inc., 2019."
REFERENCES,0.30574324324324326,"Haichao Zhang and Wei Xu. Adversarial Interpolation Training: A simple approach for improving
model robustness. 2020. URL https://openreview.net/pdf?id=Syejj0NYvr."
REFERENCES,0.30743243243243246,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically Principled Trade-off between Robustness and Accuracy. PMLR, pp. 11, 2019."
REFERENCES,0.3091216216216216,"Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable
Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 586–595, Salt Lake City, UT, June 2018. IEEE. ISBN 978-1-
5386-6420-9. doi: 10.1109/CVPR.2018.00068. URL https://ieeexplore.ieee.org/
document/8578166/."
REFERENCES,0.3108108108108108,"Xiao Zhang, Jinghui Chen, Quanquan Gu, and David Evans. Understanding the Intrinsic Robustness
of Image Distributions using Conditional Generative Models. arXiv:2003.00378 [cs, stat], February
2020. URL http://arxiv.org/abs/2003.00378. arXiv: 2003.00378."
REFERENCES,0.3125,Under review as a conference paper at ICLR 2022
REFERENCES,0.3141891891891892,"A
APPENDIX"
REFERENCES,0.3158783783783784,"A.1
DERIVATION OF MANIFOLD-GRADIENT MUTUAL INFORMATION (MI)"
REFERENCES,0.31756756756756754,"We deﬁne the manifold-gradient point-wise joint probability in a case-wise manner, for the respective
values under g ∈{−1, 1}d and x ∈Rd. We are concerned with the sub-gradient cases where x > 0
(denoted x+) and x < 0 (denoted x−) which correspond to ﬁxed values of g based on class means
y · µ with y ∈{−1, 1}. This gives for each dimension k,"
REFERENCES,0.31925675675675674,"p(gk = 1, x+
k ) =
1
2σ
√"
REFERENCES,0.32094594594594594,2π exp  −1 2
REFERENCES,0.32263513513513514,"x+
k −µk σ 2!"
REFERENCES,0.32432432432432434,"p(gk = 1, x−
k ) =
1
2σ
√"
REFERENCES,0.3260135135135135,2π exp  −1 2
REFERENCES,0.3277027027027027,"x+
k + µk σ"
REFERENCES,0.3293918918918919,"2!
(3)"
REFERENCES,0.3310810810810811,"p(gk = −1, x+
k ) =
1
2σ
√"
REFERENCES,0.3327702702702703,2π exp  −1 2
REFERENCES,0.3344594594594595,"x+
k + µk σ 2!"
REFERENCES,0.33614864864864863,"p(gk = −1, x−
k ) =
1
2σ
√"
REFERENCES,0.33783783783783783,2π exp  −1 2
REFERENCES,0.33952702702702703,"x+
k −µk σ 2! . (4)"
REFERENCES,0.34121621621621623,"Since the Schmidt et al. Gaussian mixture is created symmetrically (the probability mass is evenly
split between the two classes i.e., the mixture comprises one Gaussian offset by µk and mirrored at
xk = 0) we can simplify to"
REFERENCES,0.34290540540540543,"p(gk = 1, x+
k ) =
1
2σ
√"
REFERENCES,0.34459459459459457,2π exp  −1 2
REFERENCES,0.34628378378378377,"x+
k −µk σ 2! ,
(5)"
REFERENCES,0.34797297297297297,"p(gk = −1, x+
k ) =
1
2σ
√"
REFERENCES,0.34966216216216217,2π exp  −1 2
REFERENCES,0.35135135135135137,"x+
k + µk σ 2! ,
(6)"
REFERENCES,0.3530405405405405,"where x ∼N(y · µ, σI). In words, Equation 6 is the symmetrical tail of the Gaussian mixture
marginal while Equation 5 is the remainder of the mixture."
REFERENCES,0.3547297297297297,"Similarly, a point-wise gradient is given as the Rademacher outcome gk ∈{±1}. The choice of ϵ
directly inﬂuences the marginal probability over the manifold. The marginal probability over the
manifold can be given as the Riemann approximations"
REFERENCES,0.3564189189189189,"pG(gk = 1)ϵ =
1
2σ
√ 2π n
X"
REFERENCES,0.3581081081081081,"i=1
exp  −1 2"
REFERENCES,0.3597972972972973,"x∗
i,k −µk σ 2!"
REFERENCES,0.3614864864864865,"∆i
(7) and"
REFERENCES,0.36317567567567566,"pG(gk = −1)ϵ =
1
2σ
√ 2π n
X"
REFERENCES,0.36486486486486486,"i=1
exp  −1 2"
REFERENCES,0.36655405405405406,"x∗
i,k + µk σ 2!"
REFERENCES,0.36824324324324326,"∆i,
(8)"
REFERENCES,0.36993243243243246,"with ∆i = x+
i,k−x+
i−1,k for arbitrary x∗
i,k ∈[x+
i−1,k, x+
i,k], and n is controlled by the hyper-parameter
ϵ."
REFERENCES,0.3716216216216216,The marginal for the manifold under the gradient is given similarly as
REFERENCES,0.3733108108108108,"pM(xk) = p(gk = 1, x+
k ) + p(gk = −1, x+
k )"
REFERENCES,0.375,"=
1
σ
√"
REFERENCES,0.3766891891891892,2π exp  −1 2
REFERENCES,0.3783783783783784,"x+
k −µk σ 2!"
REFERENCES,0.38006756756756754,"+
1
σ
√"
REFERENCES,0.38175675675675674,2π exp  −1 2
REFERENCES,0.38344594594594594,"x+
k + µk σ 2! ,
(9)"
REFERENCES,0.38513513513513514,Under review as a conference paper at ICLR 2022
REFERENCES,0.38682432432432434,"where x+
k > 0 for all dimensions k. Denote the sub-manifold sampled from the positive (y = 1) and
negative (y = −1) classes as M+ and M−, respectively. Our deﬁnition for manifold-gradient mutual
information is based on the standard deﬁnition of mutual information from information theory (Cover
& Thomas, 2006),"
REFERENCES,0.3885135135135135,"I(M, G)ϵ,k =
Z M Z"
REFERENCES,0.3902027027027027,"G
p(gk, xk) log(
p(gk, xk)
pG(gk)pM(xk)) dgk dxk,
(10)"
REFERENCES,0.3918918918918919,"where ϵ is treated as a hyper-parameter controlling the value of n in pG(gk). By substitution into
Equation 10 we have"
REFERENCES,0.3935810810810811,"I(M, G)ϵ,k =
Z"
REFERENCES,0.3952702702702703,"M
p(1, xk) log(
p(1, xk)
pG(1)pM(xk)) dxk +
Z"
REFERENCES,0.3969594594594595,"M
p(−1, xk) log(
p(−1, xk)
pG(−1)pM(xk)) dxk.
(11)"
REFERENCES,0.39864864864864863,"This is split further similar to true positive, true negative, false positive, and false negative, as"
REFERENCES,0.40033783783783783,"I(M, G)ϵ,k =
Z"
REFERENCES,0.40202702702702703,"M+ p(1, x+
k ) log(
p(1, x+
k )
pG(1)pM(x+
k )) dx+
k +
Z"
REFERENCES,0.40371621621621623,"M−p(1, x−
k ) log(
p(1, x−
k )
pG(1)pM(x−
k )) dx−
k +
Z"
REFERENCES,0.40540540540540543,"M+ p(−1, x+
k ) log(
p(−1, x+
k )
pG(−1)pM(x+
k )) dx+
k +
Z"
REFERENCES,0.40709459459459457,"M−p(−1, x−
k ) log(
p(−1, x−
k )
pG(−1)pM(x−
k )) dx−
k , (12)"
REFERENCES,0.40878378378378377,and simpliﬁed due to symmetry at 0 as
REFERENCES,0.41047297297297297,"I(M, G)ϵ,k = 2
Z"
REFERENCES,0.41216216216216217,"M+ p(1, x+
k ) log(
p(1, x+
k )
pG(1)pM(x+
k )) dx+
k + 2
Z"
REFERENCES,0.41385135135135137,"M+ p(−1, x+
k ) log(
p(−1, x+
k )
pG(−1)pM(x+
k )) dx+
k . (13)"
REFERENCES,0.4155405405405405,"The total un-normalized mutual information is given by the summation over dimensions I(M, G)ϵ =
Pd
k=1 I(M, G)ϵ,k. Notably the cases for each possible scenario under detection theory are repre-
sented. Each case is bounded by the results of Schmidt et al. (2018). By substitution from each
marginal and joint probability in Equations 8, 3, and 4 respectively, we have the closed form solution
for mutual information."
REFERENCES,0.4172297297297297,"This leads to the Riemann approximation of Equation 13,"
REFERENCES,0.4189189189189189,"I(M, G)ϵ,k = 2 n
X"
REFERENCES,0.4206081081081081,"i=1
p(1, x∗
i,k) log(
p(1, x∗
i,k)
pG(1)pM(x∗
i,k)) ∆i + 2 n
X"
REFERENCES,0.4222972972972973,"i=1
p(−1, x∗
i,k) log(
p(−1, x∗
i,k)
pG(−1)pM(x∗
i,k)) ∆i. (14)"
REFERENCES,0.4239864864864865,"with ∆i = x+
i,k −x+
i−1,k for arbitrary positive x∗
i,k ∈[x+
i−1,k, x+
i,k]. Since x+ is a standard multi-
variate Gaussian (Cover & Thomas, 2006), the ﬁnal mutual information is the summation over each
dimension,"
REFERENCES,0.42567567567567566,Under review as a conference paper at ICLR 2022
REFERENCES,0.42736486486486486,"I(M, G)ϵ = 2 d
X k=1 n
X"
REFERENCES,0.42905405405405406,"i=1
p(1, x∗
i,k) log(
p(1, x∗
i,k)
pG(1)pM(x∗
i,k)) ∆i + 2 d
X k=1 n
X"
REFERENCES,0.43074324324324326,"i=1
p(−1, x∗
i,k) log(
p(−1, x∗
i,k)
pG(−1)pM(x∗
i,k)) ∆i. (15)"
REFERENCES,0.43243243243243246,"A.2
HARD-LABEL ATTACK FORMULATION"
REFERENCES,0.4341216216216216,"Contemporary hard-label attacks are variants of random gradient-free method (RGF) (Nesterov &
Spokoiny, 2017), a gradient estimator which yields the estimate ˆg over q random directions {ui}q
i=1."
REFERENCES,0.4358108108108108,"OPT-Attack For benign example x0, true label y0, and hard-label black-box function f : Rd →
{1, . . . , K}, Cheng et al. (2019) deﬁne the objective function g : Rd →R as a function of search
direction θ, where the optimal solution is g(θ∗), the minimum distance from x0 to the nearest
adversarial example along the direction θ∗. For the untargeted attack, g(θ) is the distance to any
decision boundary along direction θ, and allows for estimating the gradient as"
REFERENCES,0.4375,"ˆg = 1 q q
X i=0"
REFERENCES,0.4391891891891892,g(θ + βui) −g(θ)
REFERENCES,0.4408783783783784,"β
· ui,
(16)"
REFERENCES,0.44256756756756754,"where β is a small smoothing parameter. Notably, g(θ) is continuous even if f is a non-continuous
step function."
REFERENCES,0.44425675675675674,"Sign-OPT Cheng et al. (2020) later improved the query efﬁciency by only considering the sign of
the gradient estimate,"
REFERENCES,0.44594594594594594,"ˆ∇g(θ) ≈ˆg := q
X"
REFERENCES,0.44763513513513514,"i=1
sign (g(θ + βui) −g(θ)) ui.
(17)"
REFERENCES,0.44932432432432434,"We focus on the Sign-OPT variant, since the ﬁndings are more relevant to the current state-of-the-art."
REFERENCES,0.4510135135135135,"HopSkipJumpAttack Similar to Sign-OPT, HopSkipJumpAttack (HSJA) (Chen et al., 2019) uses
a zeroth-order sign oracle to improve Boundary Attack (Brendel et al., 2017). HSJA lacks the
convergence analysis of Sign-OPT and relies on one-point gradient estimate. Regardless, HSJA is
competitive and can excel in the L∞setting."
REFERENCES,0.4527027027027027,"Dimension-reduced Sign-OPT & HSJA. In general, for attacks relying on the Cheng et al. (2019)
formulation, the update in Equation 16 becomes"
REFERENCES,0.4543918918918919,"ˆg = 1 q q
X i=0"
REFERENCES,0.4560810810810811,"g(θ′ + βu′
i) −g(θ′)
β
· u′
i
(18)"
REFERENCES,0.4577702702702703,"for the reduced-dimension Gaussian vectors {u′
i ∈Rd′}q
i=0 for integer d′ < d and direction θ′ ∈Rd′.
The reduced-dimension direction θ′ is initialized randomly with θ′ ∼N(0, 1) for the untargeted
case, or for the targeted case as θ′ = E(xt), where xt is a test sample correctly classiﬁed as target
class t by the victim model. This scheme also applies to HSJA, since HSJA performs a single-point
sign estimate. As in the normal variants, ˆg is used to update θ′."
REFERENCES,0.4594594594594595,"A.3
MAIN PAPER BLOCK DIAGRAM"
REFERENCES,0.46114864864864863,"A block diagram of assumptions, claims, and observations is shown in Figure 3."
REFERENCES,0.46283783783783783,Under review as a conference paper at ICLR 2022
REFERENCES,0.46452702702702703,"External Result:
Semantic leakage"
REFERENCES,0.46621621621621623,in robust model
REFERENCES,0.46790540540540543,"gradients 
[Santurkar’19,"
REFERENCES,0.46959459459459457,Engstrom’19]
REFERENCES,0.47128378378378377,"Assumption: 
Models learn a 
lower dimension 
representation of 
data, and encode"
REFERENCES,0.47297297297297297,it as  priors.
REFERENCES,0.47466216216216217,External Result:
REFERENCES,0.47635135135135137,"Data 
Processing 
Inequality (DPI)"
REFERENCES,0.4780405405405405,[Beaudry and
REFERENCES,0.4797297297297297,Renner’12]
REFERENCES,0.4814189189189189,"Assumption: 
Data Manifold,"
REFERENCES,0.4831081081081081,Gradient and
REFERENCES,0.4847972972972973,"gradient 
estimate form a"
REFERENCES,0.4864864864864865,Markov Chain
REFERENCES,0.48817567567567566,External Result:
REFERENCES,0.48986486486486486,Robust models
REFERENCES,0.49155405405405406,require more
REFERENCES,0.49324324324324326,"data 
[Schmidt‘18]"
REFERENCES,0.49493243243243246,"Hypothesis 1: 
Manifold-Gradient"
REFERENCES,0.4966216216216216,"MI can be higher 
on robust models."
REFERENCES,0.4983108108108108,Observation 1: MI
REFERENCES,0.5,can increase with
REFERENCES,0.5016891891891891,lower dimension.
REFERENCES,0.5033783783783784,Hypothesis 2:
REFERENCES,0.5050675675675675,"A hard-label 
adversary can 
sample closer 
to an unknown"
REFERENCES,0.5067567567567568,data manifold.
REFERENCES,0.5084459459459459,"Observation 2: Dim. 
reduced hard-label attacks"
REFERENCES,0.5101351351351351,"have lower LPIPS, 
approximated manifold 
distance (Lp-norm), and 
FID score if they preserve"
REFERENCES,0.5118243243243243,semantic priors.
REFERENCES,0.5135135135135135,"External Result: 
LPIPS correlates"
REFERENCES,0.5152027027027027,"to human 
perception 
[Zhang et al. ’18, 
Laidlaw et al.’21]"
REFERENCES,0.5168918918918919,External Result:
REFERENCES,0.518581081081081,Robust model
REFERENCES,0.5202702702702703,"decision 
boundary aligns"
REFERENCES,0.5219594594594594,"with salient 
input changes 
[Santukar’19]."
REFERENCES,0.5236486486486487,Observation 3: Dim.
REFERENCES,0.5253378378378378,reduced hard label
REFERENCES,0.527027027027027,"attacks decrease 
estimate variance"
REFERENCES,0.5287162162162162,"(rows 2, 5)."
REFERENCES,0.5304054054054054,"Observation 4: 
Successful attacks"
REFERENCES,0.5320945945945946,"preserve the 
semantic priors, and 
tend to exhibit lower"
REFERENCES,0.5337837837837838,LPIPS score.
REFERENCES,0.535472972972973,"Figure 1
Section 3.2"
REFERENCES,0.5371621621621622,Section 4
REFERENCES,0.5388513513513513,Section 3
REFERENCES,0.5405405405405406,Section 5.1
REFERENCES,0.5422297297297297,Table 1
REFERENCES,0.543918918918919,Table 2
REFERENCES,0.5456081081081081,Section 5.3
REFERENCES,0.5472972972972973,Figure 2
REFERENCES,0.5489864864864865,Section 3
REFERENCES,0.5506756756756757,"Figure 3: Block diagram summarizing the assumptions, claims, and observations of the main paper."
REFERENCES,0.5523648648648649,Under review as a conference paper at ICLR 2022
REFERENCES,0.5540540540540541,"A.4
IMPLEMENTATION DETAILS"
REFERENCES,0.5557432432432432,"A.4.1
HARDWARE AND ATTACK HYPERPARAMETERS"
REFERENCES,0.5574324324324325,"All experiments in the main paper were performed on an internal high-performance compute cluster
equipped with NVIDIA Tesla V100 Tensor Core GPUs and high-speed non-volatile ﬂash storage. In
total 16 GPUs, 1TB main system memory, and 40 Intel Xeon CPU cores were used to run experiments
completely."
REFERENCES,0.5591216216216216,"Depending on dataset dimension, HSJA requires tuning of parameter γ for best performance. On
CIFAR-10 we used γ = 10.0. For ImageNet, it was necessary to set γ ≥1000.0 to re-create the
published results of the regular variant (Chen et al., 2019). Due to similar performance we use
γ = 1000.0 for regular and dimension-reduced variants. We note that the dimension-reduced variants
like HSJA+BiLN were less sensitive to γ, performing similarly regardless of the setting."
REFERENCES,0.5608108108108109,"A.4.2
ADVERSARY AUTOENCODER"
REFERENCES,0.5625,"We are primarily interested in the effect of reduced search resolution on attack behavior. Thus in this
work, given a candidate direction θ′ and magnitude (or radius) r, the adversarial sample in the AE
case is the blending (1 −r)x0 + rD (E(x0) + θ′).3"
REFERENCES,0.5641891891891891,"For AE attack variants, we implement the same architecture described by Tu et al. (2019). Speciﬁcally
it leverages a fully convolutional network for the encoder and decoder. Every AE is trained using the
held out test set, as we assume disjoint data between adversary and victim."
REFERENCES,0.5658783783783784,"The adversary’s AE is tuned to minimize reconstruction error of input images, so the output quality
of the AE will depend on the adversary’s ability to collect data. We assume the adversary only has
access to the test set, which tends to be considerably less informative than the training set. This crude
manifold approximation can manifest as an additional layer of distortion on top of adversarial noise.
With BiLN, no additional training is required, so it synthesizes search directions independent of the
adversary’s manifold description (i.e., possible extracted knowledge about test samples)."
REFERENCES,0.5675675675675675,"ImageNet samples are downsized to 128x128 before passing to the AE, and the output of the AE is
scaled back to 224x224, as described by Tu et al. (2019)."
REFERENCES,0.5692567567567568,"A.4.3
DATA SAMPLING"
REFERENCES,0.5709459459459459,"Original samples are chosen from the test set using the technique from Chen et al. (2019): on
CIFAR-10, ﬁve random samples are taken from each of ten uniform-randomly chosen classes (i.e.,
50 total samples). On the ImageNet dataset, ten random classes are uniform-randomly chosen and
ten random samples taken from each (100 total samples)."
REFERENCES,0.5726351351351351,"A.5
SUPPLEMENTAL RESULTS"
REFERENCES,0.5743243243243243,"A.5.1
QUERY VS. DISTORTION PLOTS"
REFERENCES,0.5760135135135135,"We show the model queries against attack distortion measurement in Figure 4 to accompany the
results in the main paper. The distortion is much higher and stays higher with Rand variants, due
to discarding important semantic information. The plots evidence that BiLN variants (yellow lines)
offer a simple yet effective way to improve the query efﬁciency of the hard-label attacks."
REFERENCES,0.5777027027027027,"A.5.2
GRADIENT DEVIATION ON ROBUST CIFAR-10"
REFERENCES,0.5793918918918919,"In Table 3 we show supplementary gradient deviation results for CIFAR-10 using different defense
mechanisms or robust models. In general they exhibit the same trend as our main paper results, which
is that dimension-reduced attacks manage to reduce gradient deviation across each robust model."
REFERENCES,0.581081081081081,Under review as a conference paper at ICLR 2022 0.0 0.1 0.2 0.3
REFERENCES,0.5827702702702703,Distortion (L∞)
REFERENCES,0.5844594594594594,Natural CIFAR-10
REFERENCES,0.5861486486486487,"HSJA
BiLN+HSJA 16
Rand+HSJA 16"
REFERENCES,0.5878378378378378,Madry CIFAR-10
REFERENCES,0.589527027027027,"0
10000
20000
# of queries 0.0 0.1 0.2 0.3"
REFERENCES,0.5912162162162162,Distortion (L∞)
REFERENCES,0.5929054054054054,"Sign-OPT
BiLN+Sign-OPT 16
Rand+Sign-OPT 16
AE+Sign-OPT"
REFERENCES,0.5945945945945946,"0
10000
20000
# of queries i. (a) 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.5962837837837838,Distortion (L∞)
REFERENCES,0.597972972972973,Natural ImageNet
REFERENCES,0.5996621621621622,"HSJA
Rand+HSJA 32
BiLN+HSJA 32"
REFERENCES,0.6013513513513513,Madry ImageNet
REFERENCES,0.6030405405405406,"0
10000
20000
# of queries 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.6047297297297297,Distortion (L∞)
REFERENCES,0.606418918918919,"Sign-OPT
BiLN+Sign-OPT 32
Rand+Sign-OPT 32
AE+Sign-OPT"
REFERENCES,0.6081081081081081,"0
10000
20000
# of queries i."
REFERENCES,0.6097972972972973,"(b)
Figure 4: Query vs. distortion plots for a) CIFAR-10 and b) ImageNet, corresponding to the success
rate plots in the main text. Dashed lines denote the value of ϵ."
REFERENCES,0.6114864864864865,"Attack Variant
TRADES
(Zhang et al., 2019)
Interpolation
(Zhang & Xu, 2020)
Feat. Scattering
(Zhang & Wang, 2019)
SENSE
(Jungeum & Wang, 2020)"
REFERENCES,0.6131756756756757,"HSJA
0.0542±0.0001
0.0542±0.0001
0.0541±0.0000
0.0556±0.0045
HSJA+BiLN
0.0395±0.0001
0.0393±0.0001
0.0401±0.0004
0.0389±0.0056
HSJA+Rand
0.008±0.004
0.002±0.005
0.216±0.000
0.222±0.017"
REFERENCES,0.6148648648648649,"Sign-OPT
0.0042±0.0005
0.0039±0.0007
0.0019±0.0004
0.0083±0.0104
Sign-OPT+BiLN
0.0026±0.0007
0.0023±0.0009
0.0020±0.0004
0.0075±0.0110
Sign-OPT+Rand
0.007±0.002
0.004±0.005
0.006±0.002
0.025±0.048
Sign-OPT+AE
0.0257±0.0002
0.0282±0.0002
0.0259±0.0000
0.0278±0.0069"
REFERENCES,0.6165540540540541,Table 3: Per-pixel gradient deviation measured across additional robust CIFAR-10 models
REFERENCES,0.6182432432432432,"A.5.3
SUCCESS RATE NORMALIZED AUC SCORES"
REFERENCES,0.6199324324324325,"Tables 5 and 4 show the max-normalized Trapezoid rule area-under-curve (AUC) measurements for
the success rate plots of the main text. Highest scores are bolded. Notably, the HSJA+BiLN variant
earns the highest score in almost all cases."
REFERENCES,0.6216216216216216,"A.5.4
SUCCESS RATE SCORES"
REFERENCES,0.6233108108108109,We provide the success rates over all samples at speciﬁc query intervals in Tables 6 and 7.
REFERENCES,0.625,"A.5.5
ATTACKING A SMOOTHED MODEL"
REFERENCES,0.6266891891891891,"Gaussian smoothing is a technique of performing adversarial training with sampled affected by
Gaussian noise. At test time, inference is achieved via a Monte Carlo search over many Gaussian-
perturbed versions of the sample under test. The SotA at time of writing, randomized smoothing
proposed by Cohen et al. (2019), is a good candidate for hard-label attacks since the true gradient
of the smoothed model is undeﬁned. We use the checkpoint corresponding to smoothing parameter
σ = 0.5 and ϵ ≃1.0. These results are shown in Figure 5. In general, the BiLN variant exceeds
all other variants in the natural ImageNet case, with small improvement on the smoothed model.
Although it can ﬁnd samples closer to the smoothed ϵ, only a fraction are within the bound."
REFERENCES,0.6283783783783784,"3We observed that it is detrimental to set x = D(E(x0) + rθ′) or x = D(rθ′) directly. Despite remaining
on the data manifold by attacking it directly, the approximation of the data manifold is crude, which results in
large distortion (Stutz et al., 2019)"
REFERENCES,0.6300675675675675,Under review as a conference paper at ICLR 2022
REFERENCES,0.6317567567567568,"Attack Variant
Natural
CIFAR-10
Madry
CIFAR-10"
REFERENCES,0.6334459459459459,"HSJA
1.000
0.650
HSJA+BiLN
0.968
1.000
HSJA+Rand
0.033
0.088"
REFERENCES,0.6351351351351351,"Sign-OPT
0.763
0.171
Sign-OPT+BiLN
0.310
0.156
Sign-OPT+Rand
0.144
0.092
Sign-OPT+AE
0.312
0.300"
REFERENCES,0.6368243243243243,"Table 4: Success Rate (SR) Normalized AUC scores for CIFAR-10 SR plots of the main text. Higher
is better."
REFERENCES,0.6385135135135135,"Attack Variant
Natural
ImageNet
Madry
ImageNet"
REFERENCES,0.6402027027027027,"HSJA
0.867
0.470
HSJA+BiLN
1.000
1.000
HSJA+Rand
0.077
0.211"
REFERENCES,0.6418918918918919,"Sign-OPT
0.364
0.153
Sign-OPT+BiLN
0.376
0.215
Sign-OPT+Rand
0.070
0.033
Sign-OPT+AE
0.018
0.105"
REFERENCES,0.643581081081081,"Table 5: Success Rate (SR) Normalized AUC scores for ImageNet SR plots of the main text. Higher
is better. 0 20 40 60 80"
REFERENCES,0.6452702702702703,Distortion (L2)
REFERENCES,0.6469594594594594,Natural ImageNet
REFERENCES,0.6486486486486487,"HSJA
BiLN+HSJA 32
Rand+HSJA 32"
REFERENCES,0.6503378378378378,Smoothing ImageNet
REFERENCES,0.652027027027027,"0
10000
20000
# of queries 0 20 40 60 80"
REFERENCES,0.6537162162162162,Distortion (L2)
REFERENCES,0.6554054054054054,"Sign-OPT
BiLN+Sign-OPT 32
Rand+Sign-OPT 32
AE+Sign-OPT"
REFERENCES,0.6570945945945946,"0
10000
20000
# of queries (a) 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6587837837837838,Success Rate
REFERENCES,0.660472972972973,Natural ImageNet
REFERENCES,0.6621621621621622,ε = 3.0
REFERENCES,0.6638513513513513,Smoothing ImageNet
REFERENCES,0.6655405405405406,ε = 1.0
REFERENCES,0.6672297297297297,"HSJA
BiLN+HSJA 32
Rand+HSJA 32"
REFERENCES,0.668918918918919,"0
10000
20000"
REFERENCES,0.6706081081081081,# of queries 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.6722972972972973,Success Rate
REFERENCES,0.6739864864864865,"0
10000
20000"
REFERENCES,0.6756756756756757,# of queries
REFERENCES,0.6773648648648649,"Sign-OPT
BiLN+Sign-OPT 32
Rand+Sign-OPT 32
AE+Sign-OPT"
REFERENCES,0.6790540540540541,"(b)
Figure 5: Results of attacking Smoothed ImageNet Cohen et al. (2019) in the L2 setting for a) query
vs. distortion and b) query vs. success rate, Dashed lines denote the value of ϵ."
REFERENCES,0.6807432432432432,"A.5.6
ATTACKING WITHOUT GRADIENT ESTIMATE"
REFERENCES,0.6824324324324325,"We perform additional experiments with an attack that does not perform an explicit gradient estimate.
Chen & Gu (2020) propose an alternative hard-label attack method which is to search for the minimum
decision boundary radius r from a sample x0, along a ray direction θ. Instead of searching over Rd
to minimize g(θ), Chen et al. propose to perform ray search over directions θ ∈{−1, 1}d, resulting
in 2d maximum possible directions. This reduction of the search resolution enables SotA query"
REFERENCES,0.6841216216216216,Under review as a conference paper at ICLR 2022
REFERENCES,0.6858108108108109,"Attack Variant
Natural
@ 4k
Madry
@ 4k"
REFERENCES,0.6875,"Natural
@ 11k
Madry
@ 11k"
REFERENCES,0.6891891891891891,"Natural
@ 25k
Madry
@ 25k"
REFERENCES,0.6908783783783784,"HSJA
0.905
0.100
0.995
0.145
1.000
0.180
HSJA+BiLN
0.850
0.165
0.970
0.225
0.985
0.255
HSJA+Rand
0.040
0.020
0.020
0.020
0.040
0.000
Sign-OPT
0.515
0.030
0.795
0.040
0.890
0.040
Sign-OPT+BiLN
0.235
0.035
0.310
0.035
0.355
0.035
Sign-OPT+Rand
0.060
0.020
0.200
0.020
0.180
0.020
Sign-OPT+AE
0.210
0.055
0.325
0.065
0.345
0.070"
REFERENCES,0.6925675675675675,"Table 6: CIFAR-10 succcess rate values at query intervals 4k, 11k, and 25k, for setting ϵ =
8
255.."
REFERENCES,0.6942567567567568,"Attack Variant
Natural
@ 4k
Madry
@ 4k"
REFERENCES,0.6959459459459459,"Natural
@ 11k
Madry
@ 11k"
REFERENCES,0.6976351351351351,"Natural
@ 25k
Madry
@ 25k"
REFERENCES,0.6993243243243243,"HSJA
0.550
0.105
0.850
0.130
0.965
0.165
HSJA+BiLN
0.835
0.240
0.965
0.290
1.000
0.335
HSJA+Rand
0.070
0.060
0.070
0.060
0.070
0.060
Sign-OPT
0.210
0.045
0.335
0.045
0.485
0.045
Sign-OPT+BiLN
0.240
0.055
0.345
0.065
0.445
0.065
Sign-OPT+Rand
0.050
0.010
0.070
0.010
0.070
0.010
Sign-OPT+AE
0.015
0.030
0.015
0.030
0.020
0.040"
REFERENCES,0.7010135135135135,"Table 7: ImageNet succcess rate values at query intervals 4k, 11k, and 25k, for setting ϵ =
8
255."
REFERENCES,0.7027027027027027,"0
10000
20000
# of queries 0.0 0.1 0.2 0.3"
REFERENCES,0.7043918918918919,Distortion (L∞)
REFERENCES,0.706081081081081,Natural
REFERENCES,0.7077702702702703,"RayS
BiLN+RayS a=2
BiLN+RayS b=2
BiLN+RayS b=4"
REFERENCES,0.7094594594594594,"0
10000
20000
# of queries"
REFERENCES,0.7111486486486487,"i.
ii."
REFERENCES,0.7128378378378378,Madry Adv.Tr.
REFERENCES,0.714527027027027,"Figure 6: Results for RayS on the CIFAR-10 dataset, corresponding to distortion against query usage
(dotted red line denotes the value of ϵ, shaded areas mark standard deviation)."
REFERENCES,0.7162162162162162,"efﬁciency in the L∞setting with proof of convergence. The search resolution is further reduced by
the hierarchical variant of RayS, which performs on-the-ﬂy upscaling of image super-pixels."
REFERENCES,0.7179054054054054,"The intuition behind RayS attack is to perform a discrete search in at most 2d directions. Chen
et al. also perform a hierarchical search over progressively larger super-pixels of the image. This
has the effect of already upscaling on-the-ﬂy (Chen & Gu, 2020). RayS has the unique behavior
of performing a discrete search for the decision boundary, rather than an explicit gradient estimate.
To achieve an appropriate reduced-dimension version of RayS, we modify the calculation of s in
Algorithm 3 of Chen & Gu (2020), which either speeds up upscaling by a factor a (i.e., s = s + a),
or extends the search through a speciﬁc block index by a factor b (increase block level at k = 2sb
instead of k = 2s)."
REFERENCES,0.7195945945945946,"The result of attacking CIFAR-10 with RayS is shown in Figure 6. The BiLN variants of RayS each
have minimal effect on overall query efﬁciency (Insets 6.i and 6.ii). This is a result of RayS not
relying on explicit gradient estimation. When comparing the FID-64 score, the dimension-reduced
variants of RayS do not have a large variation between them (Inset 6.i), a side-effect of the adaptive
super-pixel search, which can automatically scale the super-pixel size as the search progresses."
REFERENCES,0.7212837837837838,Under review as a conference paper at ICLR 2022
REFERENCES,0.722972972972973,"Natural
CIFAR-10
Madry
CIFAR-10
Natural
ImageNet
Madry
ImageNet"
REFERENCES,0.7246621621621622,"Benign
0.787 ± 0.830
0.564 ± 1.724
1.206 ± 0.803
2.623 ± 2.383"
REFERENCES,0.7263513513513513,"HSJA
8.014 ± 5.829
62.709 ± 112.416
4.798 ± 2.578
3.342 ± 2.263
HSJA+BiLN
7.497 ± 5.811
50.467 ± 100.057
4.787 ± 2.550
4.290 ± 4.524
HSJA+Rand
6.156 ± 6.053
15.745 ± 24.195
5.191 ± 1.948
3.132 ± 2.489"
REFERENCES,0.7280405405405406,"Sign-OPT
7.240 ± 5.006
51.491 ± 100.080
4.747 ± 1.988
3.707 ± 3.660
Sign-OPT+BiLN
6.308 ± 4.079
47.355 ± 119.958
4.547 ± 2.118
4.808 ± 4.873
Sign-OPT+Rand
5.576 ± 4.178
12.792 ± 13.546
5.364 ± 1.757
4.867 ± 3.369
Sign-OPT+AE
6.700 ± 4.735
51.380 ± 103.355
4.891 ± 2.299
3.791 ± 3.598"
REFERENCES,0.7297297297297297,Table 8: Measurement of Local Intrinsic Dimensionality (LID) averaged over 200 samples.
REFERENCES,0.731418918918919,"A.5.7
LOCAL INTRINSIC DIMENSIONALITY"
REFERENCES,0.7331081081081081,"In Table 8 we show the average Local Intrinsic Dimensionality Amsaleg et al. (2017) for each dataset
and attack combination."
REFERENCES,0.7347972972972973,"A.5.8
FR´ECHET INCEPTION DISTANCE"
REFERENCES,0.7364864864864865,"Unfortunately, the data manifold of real-world datasets is difﬁcult to describe. This is an open problem
in the study of Generative Adversarial Networks (GANs), since designers require that generator
images are on-manifold (i.e., in-distribution Zhang et al. (2020)) to preserve semantic relationships
between images. This has motivated the recently proposed Fr´echet Inception Distance (FID) that acts
as a surrogate measure of the manifold distance over a set of RGB image samples (Heusel et al., 2018).
As an additional proxy for manifold distance, we run experiments that assume adversarial samples
are synthetically generated images from the data manifold, which can later be compared to their
unmodiﬁed counterparts on the true manifold using FID. As a result, this estimation process is only
available from the defender’s perspective. Since FID uses an Inception-V3 coding layer (Szegedy
et al., 2016) to encode images, the estimation correlates with distortion of semantic high-level
features. Thus sampling closer to the data manifold will result in a lower FID score. The attacks
in our experiments do not target the Inception-V3 network, so the FID metric will not rely on any
internal aspects of the victim models."
REFERENCES,0.7381756756756757,"FID score is calculated using the 64-dimensional max pooling layer of the Inception-V3 deep
network for coding (denoted as FID-64 in this supplementary material), taken from an open-source
implementation.4 The choice of the 64-dimensional feature layer allows to calculate full-rank FID
without the full 2,048 sample count of original FID, which is prohibitive based on the scale of our
analysis. Since the coding layer differs slightly from the original FID-2048 implementation, the
magnitudes will differ from those published by Heusel et al. (2018)."
REFERENCES,0.7398648648648649,"The comparison of FID scores is shown in Table 9 for natural and robust models. The scores for
ImageNet on dimension-reduced attack variants (italicized) are universally lower (as low as 0.014,
bold), while on CIFAR-10 the regular variants did not exhibit the behavior. We posit that the higher
dimensionality of ImageNet (224 × 224) enables dimension reduction to be more effective than the
lower dimension CIFAR-10 (32 × 32). In general, attacks have a higher FID score on robust models
than natural models. This can be explained by the fact that robust models are more secure in a region
around the original sample, as a result the adversarial sample discovery is further away from the
true manifold. The random variant (Rand) in rows three and six evidences that the preservation of
semantic priors is important during the update, otherwise samples have high manifold distance. The
regular variants of HSJA and Sign-OPT are capable of high FID scores on robust models. However,
dimension-reduced variants have a universal behavior to reduce the score in the robust setting, similar
to the natural setting for ImageNet. AE variants exhibit higher FID score than BiLN, since BiLN can
rescale invariant of the adversary’s manifold knowledge (e.g., only having knowledge of test set)."
REFERENCES,0.7415540540540541,4https://github.com/mseitzer/pytorch-fid
REFERENCES,0.7432432432432432,Under review as a conference paper at ICLR 2022
REFERENCES,0.7449324324324325,"Attack Variant
Natural
CIFAR-10
Madry
CIFAR-10
Natural
ImageNet
Madry
ImageNet"
REFERENCES,0.7466216216216216,"HSJA
0.005
1.622
1.026
29.756
HSJA+BiLN
0.006↑
0.373↓
0.012↓
4.646 ↓
HSJA+Rand
2.198↑
8.256↑
3.404↑
2.354↓"
REFERENCES,0.7483108108108109,"Sign-OPT
0.001
0.305
20.969
38.505
Sign-OPT+BiLN
0.002↑
0.045↓
0.009↓
0.062↓
Sign-OPT+Rand
0.141↑
0.210↓
0.234↓
0.156↓
Sign-OPT+AE
0.333↑
0.008↓
1.514↓
7.869↓"
REFERENCES,0.75,"Table 9: Fr´echet Inception Distance (FID) scores for each attack’s set of 200 adversarial samples on
CIFAR-10 and ImageNet (lower is better). ∗denotes highest success rate (SR) AUC. Arrows denote
higher or lower score compared to baseline variant."
REFERENCES,0.7516891891891891,"Attack Variant
Natural
CIFAR-10
Madry
CIFAR-10
Natural
ImageNet
Madry
ImageNet"
REFERENCES,0.7533783783783784,"HSJA
0.016 ± 0.012⋆
0.162 ± 0.099
0.030 ± 0.046
0.170 ± 0.119
HSJA+BiLN
0.033 ± 0.024↑
0.156 ± 0.096↓⋆
0.019 ± 0.017↓⋆
0.169 ± 0.122↓⋆"
REFERENCES,0.7550675675675675,"HSJA+Rand
0.334 ± 0.176↑
0.457 ± 0.101↑
0.309 ± 0.136↑
0.308 ± 0.141↑"
REFERENCES,0.7567567567567568,"Sign-OPT
0.015 ± 0.013
0.137 ± 0.088
0.096 ± 0.118
0.152 ± 0.112
Sign-OPT+BiLN
0.048 ± 0.039↑
0.191 ± 0.103↑
0.040 ± 0.044↓
0.171 ± 0.105↑
Sign-OPT+Rand
0.084 ± 0.092↑
0.214 ± 0.100↑
0.082 ± 0.077↓
0.087 ± 0.059↓
Sign-OPT+AE
0.058 ± 0.123↑
0.094 ± 0.068↓
0.235 ± 0.200↑
0.586 ± 0.299↑"
REFERENCES,0.7584459459459459,"Table 10: L∞distance between adversarial and benign samples projected to approximated manifold
(using autoencoder trained on training data) for each attack’s set of 200 adversarial samples on
CIFAR-10 and ImageNet (lower is better). Arrows denote higher or lower distance compared to
baseline variant, and starred items indicate highest success rate."
REFERENCES,0.7601351351351351,"A.5.9
L∞-NORM OVER APPROXIMATE MANIFOLD"
REFERENCES,0.7618243243243243,"We re-use the setup described in Section A.4.2, but train the autoencoders using the training data
(defender’s perspective) instead of test data (attacker’s perspective). The results are shown in Table 10.
The HSJA+BiLN attack variants were successful in lowering distance for both natural and robust
ImageNet. Generally, Sign-OPT variants were most successful for lowering distance from baseline
variant for both CIFAR-10 and ImageNet. The primary factor is the dataset dimensionality, with
dimension reduction having a bigger impact on ImageNet than CIFAR-10 (green arrows in ImageNet
are more widespread). Likewise, robust models always exhibit a higher distance than natural. This
can be explained by the fact that adversarially trained models are more robust in a region around the
benign sample, thus the successful adversarial sample will be farther away."
REFERENCES,0.7635135135135135,"A.5.10
VISUAL RESULTS - CIFAR-10"
REFERENCES,0.7652027027027027,We provide visual qualitative results for each attack on CIFAR-10 in Figure 7.
REFERENCES,0.7668918918918919,"A.5.11
VISUAL RESULTS - IMAGENET"
REFERENCES,0.768581081081081,We provide visual qualitative results for each attack on ImageNet in Figure 8.
REFERENCES,0.7702702702702703,Under review as a conference paper at ICLR 2022
REFERENCES,0.7719594594594594,"Queries:
Original"
REFERENCES,0.7736486486486487,"917
L∞=0.10"
REFERENCES,0.7753378378378378,"13991
L∞=0.01"
REFERENCES,0.777027027027027,"25409
L∞=0.01"
REFERENCES,0.7787162162162162,Final θ (x50)
REFERENCES,0.7804054054054054,(a) Sign-OPT on CIFAR10
REFERENCES,0.7820945945945946,"Queries:
Original"
REFERENCES,0.7837837837837838,"878
L∞=0.69"
REFERENCES,0.785472972972973,"13928
L∞=0.29"
REFERENCES,0.7871621621621622,"25067
L∞=0.31"
REFERENCES,0.7888513513513513,Final θ (x50)
REFERENCES,0.7905405405405406,(b) Sign-OPT on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.7922297297297297,"Queries:
Original"
REFERENCES,0.793918918918919,"945
L∞=0.20"
REFERENCES,0.7956081081081081,"13867
L∞=0.05"
REFERENCES,0.7972972972972973,"25128
L∞=0.05"
REFERENCES,0.7989864864864865,Final θ (x50)
REFERENCES,0.8006756756756757,(c) BiLN+Sign-OPT on CIFAR10
REFERENCES,0.8023648648648649,"Queries:
Original"
REFERENCES,0.8040540540540541,"871
L∞=0.64"
REFERENCES,0.8057432432432432,"13949
L∞=0.67"
REFERENCES,0.8074324324324325,"25383
L∞=0.67"
REFERENCES,0.8091216216216216,Final θ (x50)
REFERENCES,0.8108108108108109,(d) BiLN+Sign-OPT on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.8125,"Queries:
Original"
REFERENCES,0.8141891891891891,"972
L∞=0.32"
REFERENCES,0.8158783783783784,"14021
L∞=0.07"
REFERENCES,0.8175675675675675,"25356
L∞=0.07"
REFERENCES,0.8192567567567568,Final θ (x50)
REFERENCES,0.8209459459459459,(e) AE+Sign-OPT on CIFAR10
REFERENCES,0.8226351351351351,"Queries:
Original"
REFERENCES,0.8243243243243243,"969
L∞=0.77"
REFERENCES,0.8260135135135135,"13778
L∞=0.21"
REFERENCES,0.8277027027027027,"25084
L∞=0.20"
REFERENCES,0.8293918918918919,Final θ (x50)
REFERENCES,0.831081081081081,(f) AE Sign-OPT on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.8327702702702703,"Queries:
Original"
REFERENCES,0.8344594594594594,"32
L∞=0.17"
REFERENCES,0.8361486486486487,"9898
L∞=0.01"
REFERENCES,0.8378378378378378,"25417
L∞=0.01"
REFERENCES,0.839527027027027,Final θ (x50)
REFERENCES,0.8412162162162162,(g) HSJA on CIFAR10
REFERENCES,0.8429054054054054,"Queries:
Original"
REFERENCES,0.8445945945945946,"32
L∞=0.46"
REFERENCES,0.8462837837837838,"9898
L∞=0.09"
REFERENCES,0.847972972972973,"25417
L∞=0.08"
REFERENCES,0.8496621621621622,Final θ (x50)
REFERENCES,0.8513513513513513,(h) HSJA on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.8530405405405406,"Queries:
Original"
REFERENCES,0.8547297297297297,"28
L∞=0.21"
REFERENCES,0.856418918918919,"9794
L∞=0.01"
REFERENCES,0.8581081081081081,"25217
L∞=0.01"
REFERENCES,0.8597972972972973,Final θ (x50)
REFERENCES,0.8614864864864865,(i) BiLN+HSJA on CIFAR10
REFERENCES,0.8631756756756757,"Queries:
Original"
REFERENCES,0.8648648648648649,"28
L∞=0.52"
REFERENCES,0.8665540540540541,"9794
L∞=0.08"
REFERENCES,0.8682432432432432,"25217
L∞=0.07"
REFERENCES,0.8699324324324325,Final θ (x50)
REFERENCES,0.8716216216216216,(j) BiLN+HSJA on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.8733108108108109,"Queries:
Original"
REFERENCES,0.875,"11
L∞=0.98"
REFERENCES,0.8766891891891891,"11955
L∞=0.01"
REFERENCES,0.8783783783783784,"25001
L∞=0.00"
REFERENCES,0.8800675675675675,Final θ (x50)
REFERENCES,0.8817567567567568,(k) RayS on CIFAR10
REFERENCES,0.8834459459459459,"Queries:
Original"
REFERENCES,0.8851351351351351,"18
L∞=0.94"
REFERENCES,0.8868243243243243,"11327
L∞=0.07"
REFERENCES,0.8885135135135135,"25006
L∞=0.06"
REFERENCES,0.8902027027027027,Final θ (x50)
REFERENCES,0.8918918918918919,(l) RayS on CIFAR10 (Madry Adv. Tr.)
REFERENCES,0.893581081081081,Figure 7: Visual selection of attack trajectories on CIFAR-10.
REFERENCES,0.8952702702702703,Under review as a conference paper at ICLR 2022
REFERENCES,0.8969594594594594,"Queries:
Original"
REFERENCES,0.8986486486486487,"934
L∞=0.27"
REFERENCES,0.9003378378378378,"18257
L∞=0.06"
REFERENCES,0.902027027027027,"25330
L∞=0.03"
REFERENCES,0.9037162162162162,"Final θ
(x100)"
REFERENCES,0.9054054054054054,(a) Sign-OPT on ImageNet
REFERENCES,0.9070945945945946,"Queries:
Original"
REFERENCES,0.9087837837837838,"917
L∞=0.10"
REFERENCES,0.910472972972973,"13723
L∞=0.08"
REFERENCES,0.9121621621621622,"25429
L∞=0.07"
REFERENCES,0.9138513513513513,"Final θ
(x100)"
REFERENCES,0.9155405405405406,(b) Sign-OPT on ImageNet (Madry Adv. Tr.)
REFERENCES,0.9172297297297297,"Queries:
Original"
REFERENCES,0.918918918918919,"944
L∞=0.14"
REFERENCES,0.9206081081081081,"14016
L∞=0.06"
REFERENCES,0.9222972972972973,"25119
L∞=0.05"
REFERENCES,0.9239864864864865,"Final θ
(x100)"
REFERENCES,0.9256756756756757,(c) BiLN+Sign-OPT on ImageNet
REFERENCES,0.9273648648648649,"Queries:
Original"
REFERENCES,0.9290540540540541,"915
L∞=0.18"
REFERENCES,0.9307432432432432,"13956
L∞=0.07"
REFERENCES,0.9324324324324325,"25425
L∞=0.08"
REFERENCES,0.9341216216216216,"Final θ
(x100)"
REFERENCES,0.9358108108108109,(d) BiLN+Sign-OPT on ImageNet (Madry Adv. Tr.)
REFERENCES,0.9375,"Queries:
Original"
REFERENCES,0.9391891891891891,"828
L∞=0.38"
REFERENCES,0.9408783783783784,"13447
L∞=0.31"
REFERENCES,0.9425675675675675,"25230
L∞=0.29"
REFERENCES,0.9442567567567568,"Final θ
(x100)"
REFERENCES,0.9459459459459459,(e) AE+Sign-OPT on ImageNet
REFERENCES,0.9476351351351351,"Queries:
Original"
REFERENCES,0.9493243243243243,"845
L∞=0.14"
REFERENCES,0.9510135135135135,"13609
L∞=0.09"
REFERENCES,0.9527027027027027,"25425
L∞=0.08"
REFERENCES,0.9543918918918919,"Final θ
(x100)"
REFERENCES,0.956081081081081,(f) AE+Sign-OPT on ImageNet (Madry Adv. Tr.)
REFERENCES,0.9577702702702703,"Queries:
Original"
REFERENCES,0.9594594594594594,"37
L∞=0.23"
REFERENCES,0.9611486486486487,"10028
L∞=0.01"
REFERENCES,0.9628378378378378,"25667
L∞=0.00"
REFERENCES,0.964527027027027,"Final θ
(x100)"
REFERENCES,0.9662162162162162,(g) HSJA on ImageNet
REFERENCES,0.9679054054054054,"Queries:
Original"
REFERENCES,0.9695945945945946,"37
L∞=0.07"
REFERENCES,0.9712837837837838,"10028
L∞=0.02"
REFERENCES,0.972972972972973,"25667
L∞=0.02"
REFERENCES,0.9746621621621622,"Final θ
(x100)"
REFERENCES,0.9763513513513513,(h) HSJA on ImageNet (Madry Adv. Tr.)
REFERENCES,0.9780405405405406,"Queries:
Original"
REFERENCES,0.9797297297297297,"26
L∞=0.13"
REFERENCES,0.981418918918919,"9742
L∞=0.00"
REFERENCES,0.9831081081081081,"25117
L∞=0.00"
REFERENCES,0.9847972972972973,"Final θ
(x100)"
REFERENCES,0.9864864864864865,(i) BiLN+HSJA on ImageNet
REFERENCES,0.9881756756756757,"Queries:
Original"
REFERENCES,0.9898648648648649,"26
L∞=0.10"
REFERENCES,0.9915540540540541,"9742
L∞=0.01"
REFERENCES,0.9932432432432432,"25117
L∞=0.01"
REFERENCES,0.9949324324324325,"Final θ
(x100)"
REFERENCES,0.9966216216216216,(j) BiLN+HSJA on ImageNet (Madry Adv. Tr.)
REFERENCES,0.9983108108108109,Figure 8: Visual selection of attack trajectories on ImageNet.
