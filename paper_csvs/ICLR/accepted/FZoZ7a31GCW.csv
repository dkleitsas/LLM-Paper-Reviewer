Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0034602076124567475,"We introduce a deep generative model for representation learning of
biological sequences that, unlike existing models, explicitly represents
the evolutionary process.
The model makes use of a tree-structured
Ornstein-Uhlenbeck process, obtained from a given phylogenetic tree, as
an informative prior for a variational autoencoder.
We show the model
performs well on the task of ancestral sequence reconstruction of single
protein families.
Our results and ablation studies indicate that the
explicit representation of evolution using a suitable tree-structured prior
has the potential to improve representation learning of biological sequences
considerably. Finally, we brieﬂy discuss extensions of the model to genomic-
scale data sets and the case of a latent phylogenetic tree."
INTRODUCTION,0.006920415224913495,"1
Introduction"
INTRODUCTION,0.010380622837370242,"Representation learning of biological sequences is important for data exploration and
downstream tasks such as protein design (Detlefsen et al., 2020; Alley et al., 2019). Deep
generative models such as variational autoencoders (VAEs) (Kingma & Welling, 2013;
2019) have been especially useful for this purpose (Riesselman et al., 2018; Greener et al.,
2018). However, current models do not take evolutionary information fully into account,
i.e., by relating the sequences belonging to a protein family in a phylogenetic tree and
incorporating parameterized evolutionary models (Durbin et al., 1998).
To address this
problem, we replace the standard multivariate Gaussian prior of a conventional VAE with
a tree-structured prior that takes into account a given evolutionary tree. We propose a
prior based on the Ornstein-Uhlenbeck Gaussian process on a tree (Hansen, 1997; Jones &"
INTRODUCTION,0.01384083044982699,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01730103806228374,"Moriarty, 2013). We apply the model to a classic problem in phylogenetics, namely the
inference of ancestral sequences."
INTRODUCTION,0.020761245674740483,"Ancestral sequence reconstruction (ASR), i.e., the inference of ancestral sequences given
their descendants or leaf sequences (Pauling et al., 1963; Yang et al., 1995; Koshi &
Goldstein, 1996; Joy et al., 2016; Hochberg & Thornton, 2017; Selberg et al., 2021), has
important applications including protein engineering (Cole & Gaucher, 2011; Spence et al.,
2021), modeling tumour evolution (El-Kebir et al., 2015), evaluating virus diversity and
vaccine design (Gaschen et al., 2002), understanding drug mechanisms (Wilson et al.,
2015) and reconstructing ancient proteins in vitro (Chang et al., 2002; Wilson et al., 2015;
Hochberg & Thornton, 2017)."
INTRODUCTION,0.02422145328719723,"As input, we assume a set of nS known, aligned leaf sequences and their phylogenetic tree.
The task we want to address is the inference of the nA ≤nS−1 unknown, ancestral sequences
(Joy et al., 2016). We show that our probabilistic model, called Draupnir, is about on par
with or better than the accuracy of established ASR methods for a standard experimentally-
derived data set (Alieva et al., 2008; Randall et al., 2016) and several simulated data sets.
In addition, we show that Draupnir is capable of capturing coevolution among sequence
positions, unlike conventional ASR methods."
INTRODUCTION,0.02768166089965398,"The paper is organised as follows. In Background, we brieﬂy discuss evolution of biological
sequences, ancestral sequence reconstruction and the tree-structured Ornstein-Uhlenbeck
process.
In Related Work, we discuss deep generative models of biological sequences.
In Methods, we describe the Draupnir model, the inference of ancestral sequences and
the setup of the benchmarking experiments.
In Results, we discuss the quality of the
latent representations, compare the accuracy of Draupnir with state-of-the-art phylogenetic
methods for ASR, and present the results of ablation experiments. We end with a brief
discussion of future work, including extending the method to genomic-scale data sets and
the case of a latent phylogenetic tree."
BACKGROUND,0.031141868512110725,"2
Background"
PROTEIN SEQUENCES AND EVOLUTION,0.03460207612456748,"2.1
Protein sequences and evolution"
PROTEIN SEQUENCES AND EVOLUTION,0.03806228373702422,"Biological molecules such as proteins and nucleic acids (DNA, RNA) can be characterised
by sequences of characters from an alphabet of size nC, where typically nC = 21 for proteins
and nC = 5 for nucleic acids (Durbin et al., 1998). These alphabets include one character
that represents a gap, which is useful in aligning related sequences in a multiple sequence
alignment (MSA). In the course of evolution, mutations arise that cause changes in these
sequences, including character substitutions, deletions and insertions. A set of nS known,
homologous, extant sequences and their evolutionary relationships are naturally represented
as nS leaf nodes in a binary tree or phylogeny, where the nA internal or ancestral nodes
represent unknown, ancestral sequences (Joy et al., 2016). Among the internal nodes, the
root node is the most ancient node."
PROTEIN SEQUENCES AND EVOLUTION,0.04152249134948097,"Edges between two nodes in the tree are labelled by positive real numbers that represent
the time diﬀerence or the amount of change between them. Such a labelled binary tree
naturally deﬁnes a (nS + nA) × (nS + nA) matrix containing the pairwise distances between
all nodes, called the patristic distance matrix, T. In the context of biological sequences, the
ﬁeld of phylogenetics is concerned with the inference of the tree topology, the labels of the
tree’s edges and the composition of the ancestral sequences, making use of methods based on
heuristics (such as maximum parsimony) or probabilistic, evolutionary models (Joy et al.,
2016)."
ANCESTRAL PROTEIN RECONSTRUCTION,0.04498269896193772,"2.2
Ancestral protein reconstruction"
ANCESTRAL PROTEIN RECONSTRUCTION,0.04844290657439446,"The ASR problem amounts to inferring the composition of the nA ancestral sequences from
the nS extant sequences, making use of a tractable model of evolution (Joy et al., 2016).
Typically, the phylogenetic tree that relates the sequences is assumed known. Standard
methods to do this typically assume independent (factorized) evolution of the characters"
ANCESTRAL PROTEIN RECONSTRUCTION,0.05190311418685121,Published as a conference paper at ICLR 2022
ANCESTRAL PROTEIN RECONSTRUCTION,0.05536332179930796,"in the sequence, which is a computationally convenient but unrealistic assumption. For
example, in proteins, amino acids are involved in an intricate 3-dimensional network of
interactions that can lead to strong dependencies between amino acids far part in the
sequence. This phenomenon is called epistasis (Hochberg & Thornton, 2017), which requires
coevolutionary models that go beyond the factorized assumption. Nonetheless, it has been
possible to infer ancestral sequences and subsequently resurrect functional ancient, ancestral
proteins in vitro (Hochberg & Thornton, 2017).
The aim of this work is to go beyond
the assumption of independent, factorized evolution by using a model of evolution that
features continuous, latent vector representations of the protein sequences. This allows us
to formulate the ASR problem in the context of a deep generative model."
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.058823529411764705,"2.3
The Ornstein-Uhlenbeck process on a phylogenetic tree"
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.06228373702422145,"Typically, ASR of biological sequences is done using factorised evolutionary models that
represent substitutions, insertions and deletions of the discrete characters in the sequences
(Joy et al., 2016). In contrast, Draupnir aims to model the evolution of latent, continuous
representations or underlying traits of the sequences. A simple diﬀusive process allowing for
an equilibrium distribution is the Ornstein-Uhlenbeck (OU) process (Hansen, 1997; Jones
& Moriarty, 2013). As the OU process is a Gaussian process, it has a Gaussian equilibrium
distribution, as well as Gaussian marginal distributions."
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.0657439446366782,"We use an OU process on a phylogenetic tree (TOU process) (Hansen, 1997; Jones &
Moriarty, 2013) to put the latent representations under the control of a parameterized
evolutionary model. Apart from the mean, which for our purposes can be assumed to be
zero, the TOU process has three parameters: the variation unattributable to the phylogeny
or the intensity of speciﬁc variation σn, the characteristic length scale of the evolutionary
dynamics λ, and the intensity of inherited variation σf. The covariance function for the
corresponding multivariate Gaussian distribution is then given by (Hadjipantelis et al., 2012;
Jones & Moriarty, 2013),"
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.06920415224913495,"Σk,l = σ2
f exp
−Tk,l λ"
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.0726643598615917,"
+ σ2
nδk,l
(1)"
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.07612456747404844,"where Tk,l is the patristic distance between nodes k and l in the tree, and the Kronecker
delta δk,l = 1 if k = l, and 0 otherwise."
THE ORNSTEIN-UHLENBECK PROCESS ON A PHYLOGENETIC TREE,0.07958477508650519,"The TOU process and related diﬀusive processes on trees are well-established evolutionary
models that have been used to model the evolution of continuous traits, such as body mass or
length (Joy et al., 2016). For example, Lartillot (2014) proposes a phylogenetic Kalman ﬁlter
for ancestral trait reconstruction of low-dimensional, continuous traits; Tolkoﬀet al. (2018)
propose phylogenetic factor analysis, in which a latent variable under the control of a small
number independent univariate Brownian diﬀusion processes is related to observed traits
through a loading matrix; Horta et al. (2021) use a multivariate TOU process and Markov
chain Monte Carlo to model both continuous traits and sequences of discrete characters. To
represent the latter, they make use of a pairwise Potts model."
RELATED WORK,0.08304498269896193,"3
Related work"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.08650519031141868,"3.1
Representation learning of biological sequences"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.08996539792387544,"A VAE (Kingma & Welling, 2013; 2019) is a probabilistic, generative model featuring latent
vectors or representations, {z}N
n=1, that are independently sampled from a prior distribution,
zn ∼π(zn). The latent vectors are passed to a neural network (the decoder) with parameters
θ, leading to a likelihood, xn ∼pθ(xn | NNθ(zn)), for the data, {x}N
n=1.
The prior is
typically a standard multivariate Gaussian distribution, but other priors have been used,
such as distributions on the Poincar´e ball to recover hierarchical structures (Mathieu et al.,
2019). The posterior distribution p(zn | xn) is intractable, but can be approximated with
a variational distribution or guide, qφ(zn | NNφ(xn)), involving a second neural network
(the encoder). Point estimates of the parameters θ and φ are obtained by maximizing the"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.09342560553633218,Published as a conference paper at ICLR 2022
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.09688581314878893,"evidence lower bound (ELBO),"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.10034602076124567,"Lθ,φ(x) = Eq"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.10380622837370242,"
log
 pθ(x, z)"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.10726643598615918,"qφ(z | x) 
,"
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.11072664359861592,"using stochastic gradient ascent (Hoﬀman et al., 2013)."
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.11418685121107267,"VAEs are increasingly used for representation learning of biological sequences (Detlefsen
et al., 2020).
Riesselman et al. (2018) use a VAE with biologically motivated priors to
evaluate the stability of mutants and to explore new regions of sequence space. Greener et al.
(2018) use autoencoders to design metal-binding proteins and novel protein folds. Ding et al.
(2019) show that the latent representations obtained with a VAE can capture evolutionary
relationships between sequences. The above models do not represent the phylogenetic tree
explicitly, but typically aim to condition on some evolutionary information by training
on pre-computed MSAs - an approach that has been called evo-tuning (Rao et al., 2019;
Detlefsen et al., 2020). Hawkins-Hooker et al. (2021) use a VAE with a convolutional encoder
and decoder, combining upsampling and autoregression, without relying on a MSA."
REPRESENTATION LEARNING OF BIOLOGICAL SEQUENCES,0.11764705882352941,"The above models assume that the latent vectors factor independently,
which is
computationally convenient but unrealistic if the sequences are related to each other in a
phylogeny. A more realistic approach thus uses a prior π({z}N
n=1 | τ, κ)π(κ) that conditions
the latent vectors on a given phylogenetic tree, τ, and an evolutionary model with latent
parameters, κ. Because the latent vectors do not factor independently anymore, mini-batch
training can include the sequences but not the latent vectors, which limits the possible size
of the data sets. Nonetheless, we show here that such a model is both computationally
tractable and practically useful for realistic data sets concerning single protein families."
METHODS,0.12110726643598616,"4
Methods"
THE DRAUPNIR MODEL,0.1245674740484429,"4.1
The Draupnir model"
THE DRAUPNIR MODEL,0.12802768166089964,"The pseudocode of the Draupnir model is given in Algorithm 1; Figure 1 shows the
corresponding graphical model.
A summary of the variables, their dimensions and the
notation is given in Tables 1 and 2 in Appendix A.1."
THE DRAUPNIR MODEL,0.1314878892733564,"As inputs, we assume (a) a set of nS aligned sequences, each with length nL, organized in
the nS × nL matrix, S, and (b) information on their phylogenetic tree in the form of their
(nS + nA) × (nS + nA) patristic distance matrix, T."
THE DRAUPNIR MODEL,0.13494809688581316,"The latent matrix Z of the model is a matrix with nS rows (one for each leaf sequence)
and nZ columns, where nZ is the size of the latent representation of the sequences. In all
experiments, nZ = 30. Each column of Z (with nS elements) is sampled from a univariate
OU process on the phylogenetic tree, representing the evolution of a hidden trait underlying
the sequences along the tree. Each row of Z corresponds to the latent vector of a standard
VAE. However, unlike a standard VAE, the latent vectors do not factorize independently."
THE DRAUPNIR MODEL,0.1384083044982699,"For each of the nZ TOU processes, the parameters of the TOU process, corresponding to κ
in Section 3.1, are sampled from a suitable prior distribution. The TOU process has three
parameters: σf, λ and σn. As we use nZ tree OU processes (one for each column of Z), we
need to sample nZ sets of these three parameters."
THE DRAUPNIR MODEL,0.14186851211072665,"For each of the three parameters, σf, λ, σn, we sample a hyperparameter (α1, α2, α3) from
a half-normal distribution with scale parameter equal to one. These hyperparameters serve
as scale parameter for the half-normal priors over σf, λ and σn. Given the parameters of
the TOU process obtained from the prior described above, an nS × nS covariance matrix
can be calculated based on the patristic distance matrix of the leaves, T(S,S). We need one
such covariance matrix for each of the nZ columns of the matrix of latent representations,
Z. The element k, l, with k, l ∈1, ..., nS, of covariance matrix h, with h ∈1, ..., nZ, is given
by (Hadjipantelis et al., 2012; Jones & Moriarty, 2013),"
THE DRAUPNIR MODEL,0.1453287197231834,"Ch,k,l = σ2
f,h exp(−Tk,l/λh) + σ2
n,hδk,l.
(2)"
THE DRAUPNIR MODEL,0.14878892733564014,Published as a conference paper at ICLR 2022
THE DRAUPNIR MODEL,0.1522491349480969,"As decoder, we use a bidirectional gated recurrent unit (GRU, Cho et al. (2014)) with length
equal to the alignment length, nL. The input at each position i of the GRU for sequence Sk,:
is a concatenated vector, consisting of the latent vector Zk,: representing sequence k, and the
BLOSUM embedding Ei,:, which is the result of applying a fully connected neural network,
NN(1)
θ , to the BLOSUM vector Vi,: describing the amino acid preferences at position i in
the MSA (see Section 4.2). For each of the nS sequences and for each position i, the GRU
states are mapped to a logit vector that speciﬁes the probabilities of the nC characters using
another fully connected neural network, NN(2)
θ . The architecture of the networks is given in
Appendix A.2."
THE DRAUPNIR MODEL,0.15570934256055363,"h ∈ {1,...,nZ}"
THE DRAUPNIR MODEL,0.15916955017301038,"k ∈ {1,...,nS}"
THE DRAUPNIR MODEL,0.16262975778546712,"i ∈ {1,...,nL} σ=1 α1 α2 α3 σn,h σf,h"
THE DRAUPNIR MODEL,0.16608996539792387,"λh
Σh,:,: Z:,h"
THE DRAUPNIR MODEL,0.1695501730103806,μ=0(nS)
THE DRAUPNIR MODEL,0.17301038062283736,"T(S,S)
Yk,i,:←cat(Zk,:,NN(Vi,:))
V"
THE DRAUPNIR MODEL,0.17647058823529413,"Sk,:
GRU/NN"
THE DRAUPNIR MODEL,0.17993079584775087,"Figure 1: Draupnir as a graphical model. For notation and information on variables and
their dimensions see Tables 1 and 2 in Appendix A.1 and A.2. Random variables are shown
as ellipses, while deterministic quantities are shown as rounded boxes and observed random
variables are shown as shaded ellipses. Parameters of priors and other given quantities are
shown without boxes. The model contains three plates, respectively corresponding to the
number of dimensions of the leaf sequence-speciﬁc latent vector Zk,: (nZ=30), the number
of leaf sequences (nS) and the alignment length (nL). “cat” indicates the concatenation
of two vectors. α are the hyperparameters and σn, σf, λ are the parameters of the TOU
processes. Σh,:,: is the covariance matrix that is used to sample component h of the nS
latent vectors from a multivariate Gaussian distribution (with mean 0(nS)). T(S,S) is the
patristic distance matrix containing the distances between the leaf sequences. Yk,: is the
input vector for the GRU that produces the likelihood parameters for leaf sequence k, Sk,:.
V represents the MSA as an nL × nC matrix of averaged BLOSUM vectors. NN denotes a
fully connected neural network."
BLOSUM EMBEDDINGS,0.18339100346020762,"4.2
BLOSUM embeddings"
BLOSUM EMBEDDINGS,0.18685121107266436,"A BLOSUM matrix B is an nC ×nC substitution matrix used for sequence alignment, where
each row contains the log-odds scores of replacing a given character with any of the other
characters (Henikoﬀ& Henikoﬀ, 1992). Each position (column) in the MSA is represented
by the weighted average of the BLOSUM vectors of the characters in that column (see
Algorithm 2). The averaged BLOSUM vectors only need to be precomputed once. In the
model, the BLOSUM vectors are processed into BLOSUM embeddings by a neural network
to provide position-speciﬁc information on the MSA, while the latent variables provide
sequence-speciﬁc information (see Algorithm 2)."
MODEL IMPLEMENTATION AND TRAINING,0.1903114186851211,"4.3
Model implementation and training"
MODEL IMPLEMENTATION AND TRAINING,0.19377162629757785,"Draupnir was implemented in the deep probabilistic programming language Pyro (Bingham
et al., 2019) and trained using stochastic variational inference with Pyro’s AutoDelta guide
by optimizing the ELBO (Kingma & Welling, 2019), resulting in maximum a posteriori
(MAP) estimates for all parameters. We use Adam (Kingma & Ba, 2014) as the optimizer
using the default values. From the MAP estimates, we can sample the latent representations
of the ancestral nodes (see Section 4.4 and equation 5 in the Appendix).
These latent
representations are then subsequently decoded to their respective ancestral sequences. We"
MODEL IMPLEMENTATION AND TRAINING,0.1972318339100346,Published as a conference paper at ICLR 2022
MODEL IMPLEMENTATION AND TRAINING,0.20069204152249134,"Algorithm 1 The Draupnir model
Require: Multiple sequence alignment S, patristic distance matrix T"
MODEL IMPLEMENTATION AND TRAINING,0.2041522491349481,"for j ∈[1, 2, 3] do
▷Hyperpriors over the TOU process parameters
αj ∼HN(1)"
MODEL IMPLEMENTATION AND TRAINING,0.20761245674740483,"for h ∈[1, ..., nZ] do
▷Priors over the parameters of the nZ TOU processes
σf,h ∼HN(α0)
σn,h ∼HN(α1)"
MODEL IMPLEMENTATION AND TRAINING,0.21107266435986158,λh ∼HN(α2)
MODEL IMPLEMENTATION AND TRAINING,0.21453287197231835,"for h ∈[1, ..., nZ] do
▷Kernels for the nZ TOU processes
for k, l ∈{1, ..., nS} do"
MODEL IMPLEMENTATION AND TRAINING,0.2179930795847751,"Ch,k,l ←σ2
f,h exp(−T (S,S)
k,l
/λh) + σ2
n,hδk,l"
MODEL IMPLEMENTATION AND TRAINING,0.22145328719723184,"for h ∈[1, . . . , nZ] do
▷Prior over tree-strucured latent matrix Z
Z:,h ∼MVN(0(nS), Ch,:,:)"
MODEL IMPLEMENTATION AND TRAINING,0.22491349480968859,"for i ∈[1, . . . , nL] do
▷BLOSUM embeddings
Ei,: ←NN(1)
θ (Vi,:)"
MODEL IMPLEMENTATION AND TRAINING,0.22837370242214533,"for k ∈[1, . . . , nS] do
▷Input vector Y for GRU
for i ∈[1, . . . , nL] do"
MODEL IMPLEMENTATION AND TRAINING,0.23183391003460208,"Yk,i,: ←cat(Zk,:, Ei,:)
▷Concatenate sequence- and position-speciﬁc vectors"
MODEL IMPLEMENTATION AND TRAINING,0.23529411764705882,"for k ∈[1, . . . , nS] do
▷Likelihood parameters (logits) L from GRU
Hk,:,: ←GRUθ(Yk,:,:)
▷Bidirectional GRU states
for i ∈[1, . . . , nL] do"
MODEL IMPLEMENTATION AND TRAINING,0.23875432525951557,"Lk,i,: ←NN(2)
θ (Hk,i,:)
Sk,i ∼Categorical(Lk,i,:)
▷Likelihood at position i in sequence k"
MODEL IMPLEMENTATION AND TRAINING,0.2422145328719723,"Algorithm 2 Pre-computation of weighted averaged BLOSUM vectors
Require: Multiple sequence alignment S"
MODEL IMPLEMENTATION AND TRAINING,0.24567474048442905,"V ←0(nL×nc)
▷Initialize BLOSUM weighted average V
for i ∈[1, . . . , nL] do
▷Position in sequence alignment
for k ∈[1, . . . , nS] do
▷Index of leaf sequence
r ←Sk,i
▷Character at position i in leaf sequence k
Vi,: ←Vi,: + Br,:
▷Add BLOSUM vector corresponding to the amino acid
Vi,: ←
1
nS Vi,:
▷Average"
MODEL IMPLEMENTATION AND TRAINING,0.2491349480968858,"also use a custom guide to calculate a variational posterior (Draupnir-variational, see
Appendix A.6). Training details can be found in Appendix A.4. All programs were executed
on an Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz machine with a Quadro RTX 6000 GPU."
INFERENCE OF THE ANCESTRAL SEQUENCES,0.25259515570934254,"4.4
Inference of the ancestral sequences"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2560553633217993,"In this section, for ease of notation, let z ≡Z(A,S)
:,h
denote one of the h ∈{1, . . . , nz}
columns of the latent representation matrix for both ancestral and leaf sequences, Z(A,S).
First, note that z can be partitioned as z = (z(S), z(A)), where z(S) and z(A) denote the
latent representations of the leaf and ancestral sequences, respectively. The prior p(z) is a
multivariate Gaussian distribution with parameters,"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.25951557093425603,Published as a conference paper at ICLR 2022
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2629757785467128,"µ =

µ(S) µ(A)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2664359861591695,"
=

0(nZ) 0(nA)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2698961937716263,"
, Σ =

Σ(S,S)
Σ(S,A)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.27335640138408307,"Σ(A,S)
Σ(A,A)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2768166089965398,"
, Λ =

Λ(S,S)
Λ(S,A)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.28027681660899656,"Λ(A,S)
Λ(A,S) 
,"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2837370242214533,"where Λ ≡Σ−1.
The covariance matrices Σ(S,S), Σ(A,A), Σ(A,S)
≡
 
Σ(S,A)T are
respectively obtained from the distance matrices concerning distances within the leaves,
within the ancestors and between the ancestors and the leaves, T(S,S), T(A,A) and T(A,S),
and the TOU process parameters (see Eq. 2)."
INFERENCE OF THE ANCESTRAL SEQUENCES,0.28719723183391005,"As p(z) is a multivariate Gaussian distribution, we can easily obtain the conditional
distribution of z(A) given z(S) as follows (see Bishop (2006), page 689),"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2906574394463668,"p(z) = MVN (z | Σ) ⇒p

z(A) | z(S)
= MVN

z(A) | µA|S,

Λ(A,A)−1
, with"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.29411764705882354,"µA|S = µA −

Λ(A,A)−1
Λ(A,S) 
z(S) −µ(S)
= −

Λ(A,A)−1
Λ(A,S)z(S)."
INFERENCE OF THE ANCESTRAL SEQUENCES,0.2975778546712803,"As µA|S corresponds to the MAP of z(A) for any given values of z(S) and the TOU process
parameters when the ancestral sequences are not observed, the MAP estimate of the latent
representation of the ancestral sequences is given by,"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.30103806228373703,"z(A),MAP = −

Λ(A,A)−1
Λ(A,S)z(S),MAP."
INFERENCE OF THE ANCESTRAL SEQUENCES,0.3044982698961938,"In addition, a Gaussian approximation of the posterior of z(A) based on the MAP estimates
is given by,"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.3079584775086505,"z(A) ∼MVN

z(A) | z(A),MAP,

Λ(A,A)−1
.
(3)"
INFERENCE OF THE ANCESTRAL SEQUENCES,0.31141868512110726,"The reconstructed ancestral sequences are subsequently obtained by applying the GRU
decoder to the MAP estimates of their latent representations, as explained in Section 4.1."
BENCHMARKING,0.314878892733564,"4.5
Benchmarking"
BENCHMARKING,0.31833910034602075,"In order to assess the accuracy of the ASR we benchmark Draupnir (MAP and Marginal)
against state-of-the-art phylogenetic methods.
We selected methods that perform ASR
using a given tree topology and given patristic distances, including one Bayesian method
(PhyloBayes, Lartillot et al. (2013)) and three maximum likelihood based methods (PAML,
Yang (2007); FastML, Ashkenazy et al. (2012); and IQTree, Nguyen et al. (2015)). We
apply the ASR methods to both protein sequences, and to their corresponding DNA
sequences followed by subsequent translation to protein sequences. For Draupnir, we use
the protein sequences, which is the harder problem. We use eleven data sets with diﬀerent
numbers of leaves (from 19 to 800), diﬀerent alignment lengths (from 63 to 558) and with
or without gaps (10 and 1 data set respectively). The data sets include eight simulated
data sets generated using the software EvolveAGene (Hall, 2016) and three data sets with
experimentally determined ancestral sequences.
Note that the simulated data sets were
obtained from factored evolution models. We included a large data set with 800 leaves. For
prediction with Draupnir, we either use i) the most likely sequence (Draupnir-MAP in Fig.
3), using Equation 4, ii) samples from the marginal distribution (Draupnir-Marginal in Fig.
3; we report the average identity of 50 samples), using Equation 5 or iii) samples from the
variational posterior using an amortised guide (Draupnir-Variational, see Appendix A.6),
using Equation 6. Details on the data sets and training can be found in Appendix A.3 and
A.4, respectively."
BENCHMARKING,0.3217993079584775,Published as a conference paper at ICLR 2022
RESULTS,0.32525951557093424,"5
Results"
LATENT REPRESENTATIONS AND COEVOLUTION,0.328719723183391,"5.1
Latent representations and coevolution"
LATENT REPRESENTATIONS AND COEVOLUTION,0.33217993079584773,"In order to inspect the quality of the latent representations of the sequences, we use the
β-lactamase family with 32 leaf sequences. We visualize t-SNE projections (Van der Maaten
& Hinton, 2008) of the latent representations and compare the results with the structure
of the phylogenetic tree (see Figure 2). The result indicates that the latent representations
represent the structure of the tree and its diﬀerent clades (subtrees) well, indicating that
the TOU process performs well as an informative prior on evolutionary relationships. In
Appendix A.6, we show how marginalizing over the latent representations allows Draupnir
(marginal and variational) to model coevolution among sites."
LATENT REPRESENTATIONS AND COEVOLUTION,0.3356401384083045,"Figure 2: Results for the β-lactamase family with 32 leaves. Left: t-SNE projection of the
latent representations of the ancestral and leaf nodes. Right: The phylogenetic tree. Both
plots are coloured according to clade membership."
BENCHMARKING RESULTS,0.3391003460207612,"5.2
Benchmarking Results"
BENCHMARKING RESULTS,0.34256055363321797,"In Figure 3, we compare the accuracy of Draupnir (MAP and marginal) with state-of-the-art
ASR methods by plotting the average percent identity between the ancestral sequences as
reconstructed by Draupnir and the true sequences. The true ancestral sequences were either
experimentally determined or simulated. The description and origin of the data sets can be
found in Appendix A.3. Tables with benchmark results are shown in Appendix A.5. 19*"
BENCHMARKING RESULTS,0.3460207612456747,"32      
35* 50 71* 100 150 200 300 400 800 0 20 40 60 80 100 225"
BENCHMARKING RESULTS,0.3494809688581315,314 261 71 272 63 477 128 77 558 99 DNA
BENCHMARKING RESULTS,0.35294117647058826,"IQTree
PAML-CodeML
PhyloBayes 19*"
BENCHMARKING RESULTS,0.356401384083045,"32      
35* 50 71* 100 150 200 300 400 800 225"
BENCHMARKING RESULTS,0.35986159169550175,314 261 71 272 63 477 128 77 558 99
BENCHMARKING RESULTS,0.3633217993079585,Protein
BENCHMARKING RESULTS,0.36678200692041524,"FastML
Draupnir MAP
Draupnir marginal samples"
BENCHMARKING RESULTS,0.370242214532872,Number of leaves
BENCHMARKING RESULTS,0.3737024221453287,Percentage of Identity
BENCHMARKING RESULTS,0.3771626297577855,"IQTree
PAML-CodeML
PhyloBayes"
BENCHMARKING RESULTS,0.3806228373702422,"Figure 3: Comparison of the average percentage identity (y-axis) between predicted and true
ancestral sequences for Draupnir (MAP and marginal) and ASR methods for data sets with
diﬀerent number of leaves (x-axis; experimental data sets are indicated with an asterisk).
Missing points indicate that the ASR method failed to produce results on the given hardware.
The alignment size is shown on the dotted lines. We compare with ASR methods using the
DNA sequences (left) and the corresponding Protein sequences, subsequently translated to
protein sequences (right). Tables with detailed results for Draupnir-marginal and Draupnir-
MAP can be found in Appendix A.5. For the benchmarking settings see Appendix A.8."
BENCHMARKING RESULTS,0.38408304498269896,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.3875432525951557,"5.3
Ablation studies"
ABLATION STUDIES,0.39100346020761245,"In the ﬁrst ablation study, we investigate the inﬂuence of the BLOSUM embeddings by
removing them as input to the GRU. Overall, the absence of the BLOSUM embeddings
slows down convergence and sometimes make the learning process unstable, but ultimately
does not strongly aﬀect accuracy (see Figure 5). 0 2000 4000 6000 8000 10000 12000"
ABLATION STUDIES,0.3944636678200692,Number of epochs 0 20 40 60 80 100
ABLATION STUDIES,0.39792387543252594,Percent identity
ABLATION STUDIES,0.4013840830449827,"(a) Leaves, stable case 0 2000 4000 6000 8000 10000 12000"
ABLATION STUDIES,0.40484429065743943,Number of epochs 0 20 40 60 80
ABLATION STUDIES,0.4083044982698962,Percent identity
ABLATION STUDIES,0.4117647058823529,"(b) Ancestors, stable case 0 2000 4000 6000 8000 10000 12000"
ABLATION STUDIES,0.41522491349480967,Number of epochs 0 20 40 60 80
ABLATION STUDIES,0.4186851211072664,Percent identity
ABLATION STUDIES,0.42214532871972316,"(c) Ancestors, unstable case."
ABLATION STUDIES,0.42560553633217996,"Figure 5: BLOSUM embedding ablation study for the 800 leaves data set. For every 100
training epochs, the average percent identity and standard deviation are plotted for all leaves
(training set) or ancestors (test set), respectively. The results obtained with the BLOSUM
embedding are shown in dark green (MAP) and light green (marginal, see Equation 3). The
results without the BLOSUM embeddings are shown in pink (MAP) and purple (marginal)."
ABLATION STUDIES,0.4290657439446367,"In the second ablation study, we investigate the inﬂuence of the tree-structured prior by
comparing with a standard VAE with a Gaussian prior. We do this by using diagonal unit
covariance matrices for each of the nZ columns of the latent matrix Z. The rest of the model
was identical. We then compare the latent representations obtained for the leaf nodes. The
results (see Figure 7) indicate that the standard VAE is not capable of reconstructing the
evolutionary relationships well: sequences belonging to the same clade often end up far apart
in latent space. This indicates that the inﬂuence of the tree-structures prior is substantial.
A quantitative analysis of this ablation study can be found in Appendix A.7."
ABLATION STUDIES,0.43252595155709345,(a) Phylogenetic tree coloured by clade membership
ABLATION STUDIES,0.4359861591695502,"(b) Standard VAE, leaves
(c) Leaves
(d) Leaves and ancestors"
ABLATION STUDIES,0.43944636678200694,"Figure 7: Bottom: t-SNE projections of the latent representations for the SRC-Kinase SH3
domain with 100 leaves, obtained from a standard VAE (left) and Draupnir-marginal (center
and right), colored by clade. Note that only the latter model can be used to infer the latent
representations of the ancestral sequences. Top: the corresponding tree."
ABLATION STUDIES,0.4429065743944637,Published as a conference paper at ICLR 2022
DISCUSSION AND FUTURE WORK,0.4463667820069204,"6
Discussion and future work"
DISCUSSION AND FUTURE WORK,0.44982698961937717,"Draupnir demonstrates the potential value of incorporating evolutionary information and
evolutionary models explicitly in deep generative models for representation learning of
biological sequences. We point out that it is possible to extend the model with additional
information beyond sequences, for example backbone angles describing protein structure
(Golden et al., 2017) or measurements of protein stability. In future work, extending the
model to genomic-size data can be done using inducing points for Gaussian processes, as
explored in Jazbec et al. (2021) and Vikram et al. (2019). The case of a latent phylogenetic
tree can be addressed using a coalescent point process prior (Lambert & Stadler, 2013;
Vikram et al., 2019). Finally, in the large data case, the current simple network architectures
can be improved with more expressive compositions such as an MSA transformer (Rao et al.,
2021) or a deconvolutional model for sequences (Hawkins-Hooker et al., 2021).
Finally,
we point out that the learned parameters of the TOU processes might oﬀer interpretable
information on the evolutionary process."
DISCUSSION AND FUTURE WORK,0.4532871972318339,Acknowledgments
DISCUSSION AND FUTURE WORK,0.45674740484429066,"LSM acknowledges support from the Independent Research Fund Denmark under the grant
“Resurrecting ancestral proteins in silico to understand how cancer drugs work”.
OR
and ASA acknowledge support from the Independent Research Fund Denmark under the
grant “Deep probabilistic programming for protein structure prediction”. We thank Robert
Schenck for technical support and contribution of computational resources. We thank the
anonymous reviewers for their suggestions and comments."
DISCUSSION AND FUTURE WORK,0.4602076124567474,Availability
DISCUSSION AND FUTURE WORK,0.46366782006920415,"Draupnir can be found at https://github.com/LysSanzMoreta/DRAUPNIR_ASR and
installed as a python library."
REFERENCES,0.4671280276816609,References
REFERENCES,0.47058823529411764,"Naila O Alieva, Karen A Konzen, Steven F Field, Ella A Meleshkevitch, Marguerite E Hunt,
Victor Beltran-Ramirez, David J Miller, J¨org Wiedenmann, Anya Salih, and Mikhail V
Matz. Diversity and evolution of coral ﬂuorescent proteins. PLoS ONE, 3(7):e2680, 2008."
REFERENCES,0.4740484429065744,"Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M
Church. Uniﬁed rational protein engineering with sequence-based deep representation
learning. Nature methods, 16(12):1315–1322, 2019."
REFERENCES,0.47750865051903113,"Haim Ashkenazy, Osnat Penn, Adi Doron-Faigenboim, Oﬁr Cohen, Gina Cannarozzi, Oren
Zomer, and Tal Pupko. FastML: a web server for probabilistic reconstruction of ancestral
sequences. Nucleic acids research, (W1):W580–W584, 2012."
REFERENCES,0.4809688581314879,"Ahmet Bakan, Anindita Dutta, Wenzhi Mao, Ying Liu, Chakra Chennubhotla, Timothy R
Lezon, and Ivet Bahar.
Evol and prody for bridging protein sequence evolution and
structural dynamics. Bioinformatics, 30:2681–2683, 2014."
REFERENCES,0.4844290657439446,"Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan,
Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman.
Pyro:
Deep universal probabilistic programming.
The journal of machine learning
research, 20(1):973–978, 2019."
REFERENCES,0.48788927335640137,"Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006."
REFERENCES,0.4913494809688581,"Belinda SW Chang, Karolina J¨onsson, Manija A Kazmi, Michael J Donoghue, and Thomas P
Sakmar. Recreating a functional ancestral archosaur visual pigment. Molecular biology
and evolution, (9):1483–1489, 2002."
REFERENCES,0.49480968858131485,Published as a conference paper at ICLR 2022
REFERENCES,0.4982698961937716,"Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the
properties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259, 2014."
REFERENCES,0.5017301038062284,"Megan F Cole and Eric A Gaucher. Utilizing natural diversity to evolve protein function:
applications towards thermostability. Current opinion in chemical biology, (3):399–406,
2011."
REFERENCES,0.5051903114186851,"Nicki Skafte Detlefsen, Søren Hauberg, and Wouter Boomsma.
What is a meaningful
representation of protein sequences? arXiv preprint arXiv:2012.02679, 2020."
REFERENCES,0.5086505190311419,"Xinqiang Ding, Zhengting Zou, and Charles L Brooks III. Deciphering protein evolution
and ﬁtness landscapes with latent space models. Nature communications, (1):1–13, 2019."
REFERENCES,0.5121107266435986,"Richard Durbin, Sean R Eddy, Anders Krogh, and Graeme Mitchison. Biological sequence
analysis: probabilistic models of proteins and nucleic acids. Cambridge university press,
1998."
REFERENCES,0.5155709342560554,"Mohammed El-Kebir, Layla Oesper, Hannah Acheson-Field, and Benjamin J Raphael.
Reconstruction of clonal trees and tumor composition from multi-sample sequencing data.
Bioinformatics, (12):i62–i70, 2015."
REFERENCES,0.5190311418685121,"Brian Gaschen, Jesse Taylor, Karina Yusim, Brian Foley, Feng Gao, Dorothy Lang, Vladimir
Novitsky, Barton Haynes, Beatrice H Hahn, Tanmoy Bhattacharya, et al.
Diversity
considerations in HIV-1 vaccine selection. Science, (5577):2354–2360, 2002."
REFERENCES,0.5224913494809689,"Michael Golden, Eduardo Garc´ıa-Portugu´es, Michael Sørensen, Kanti V Mardia, Thomas
Hamelryck, and Jotun Hein. A generative angular model of protein structure evolution.
Molecular biology and evolution, 34(8):2085–2100, 2017."
REFERENCES,0.5259515570934256,"Joe G Greener, Lewis Moﬀat, and David T Jones. Design of metalloproteins and novel
protein folds using variational autoencoders. Scientiﬁc reports, pp. 1–12, 2018."
REFERENCES,0.5294117647058824,"Pantelis Z Hadjipantelis, Nick S Jones, John Moriarty, David Springate, and Christopher G
Knight.
Ancestral inference from functional data: statistical methods and numerical
examples. arXiv preprint arXiv:1208.0628, 2012."
REFERENCES,0.532871972318339,"Barry G Hall. Comparison of the accuracies of several phylogenetic methods using protein
and DNA sequences. Molecular biology and evolution, 22(3):792–802, 2005."
REFERENCES,0.5363321799307958,"Barry G Hall.
Eﬀects of sequence diversity and recombination on the accuracy of
phylogenetic trees estimated by kSNP. Cladistics, (1):90–99, 2016."
REFERENCES,0.5397923875432526,"Thomas F Hansen.
Stabilizing selection and the comparative analysis of adaptation.
Evolution, 51(5):1341–1351, 1997."
REFERENCES,0.5432525951557093,"Alex Hawkins-Hooker, Florence Depardieu, Sebastien Baur, Guillaume Couairon, Arthur
Chen, and David Bikard.
Generating functional protein variants with variational
autoencoders. PLoS computational biology, (2):e1008736, 2021."
REFERENCES,0.5467128027681661,"Steven Henikoﬀand Jorja G Henikoﬀ. Amino acid substitution matrices from protein blocks.
Proceedings of the national academy of sciences, (22):10915–10919, 1992."
REFERENCES,0.5501730103806228,"Georg KA Hochberg and Joseph W Thornton.
Reconstructing ancient proteins to
understand the causes of structure and function. Annual Review of Biophysics, pp. 247–
269, 2017."
REFERENCES,0.5536332179930796,"Matthew D Hoﬀman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
inference. Journal of machine learning research, 14(5), 2013."
REFERENCES,0.5570934256055363,"Edwin Rodr´ıguez Horta, Alejandro Lage-Castellanos, and Roberto Mulet.
Ancestral
sequence reconstruction for co-evolutionary models.
arXiv preprint arXiv:2108.03801,
2021."
REFERENCES,0.5605536332179931,Published as a conference paper at ICLR 2022
REFERENCES,0.5640138408304498,"Metod Jazbec, Matt Ashman, Vincent Fortuin, Michael Pearce, Stephan Mandt, and
Gunnar R¨atsch. Scalable Gaussian process variational autoencoders. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 3511–3519. PMLR, 2021."
REFERENCES,0.5674740484429066,"Nick S Jones and John Moriarty. Evolutionary inference for function-valued traits: Gaussian
process regression on phylogenies. Journal of the Royal Society Interface, 10(78):20120616,
2013."
REFERENCES,0.5709342560553633,"Jeﬀrey B Joy, Richard H Liang, Rosemary M McCloskey, T Nguyen, and Art FY Poon.
Ancestral reconstruction. PLoS computational biology, 12(7):e1004763, 2016."
REFERENCES,0.5743944636678201,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014."
REFERENCES,0.5778546712802768,"Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.5813148788927336,"Diederik P Kingma and Max Welling.
An introduction to variational autoencoders.
Foundations and trends in machine learning, 12:307–392, 2019."
REFERENCES,0.5847750865051903,"Jeﬀrey M Koshi and Richard A Goldstein. Probabilistic reconstruction of ancestral protein
sequences. Journal of molecular evolution, 42(2):313–320, 1996."
REFERENCES,0.5882352941176471,"Amaury Lambert and Tanja Stadler. Birth–death models and coalescent point processes:
The shape and probability of reconstructed phylogenies. Theoretical population biology,
90:113–128, 2013."
REFERENCES,0.5916955017301038,"Nicolas Lartillot.
A phylogenetic Kalman ﬁlter for ancestral trait reconstruction using
molecular data. Bioinformatics, pp. 488–496, 2014."
REFERENCES,0.5951557093425606,"Nicolas Lartillot, Nicolas Rodrigue, Daniel Stubbs, and Jacques Richer. PhyloBayes MPI:
Phylogenetic reconstruction with inﬁnite mixtures of proﬁles in a parallel environment.
Systematic biology, (4):611–615, 2013."
REFERENCES,0.5986159169550173,"Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh.
Continuous hierarchical representations with Poincar´e variational auto-encoders. arXiv
preprint arXiv:1901.06033, 2019."
REFERENCES,0.6020761245674741,"Jaina Mistry, Sara Chuguransky, Lowri Williams, Matloob Qureshi, Gustavo A Salazar,
Erik LL Sonnhammer, Silvio CE Tosatto, Lisanna Paladin, Shriya Raj, Lorna J
Richardson, et al. Pfam: The protein families database in 2021. Nucleic Acids Research,
49(D1):D412–D419, 2021."
REFERENCES,0.6055363321799307,"Faruck Morcos, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S Marks, Chris
Sander, Riccardo Zecchina, Jos´e N Onuchic, Terence Hwa, and Martin Weigt. Direct-
coupling analysis of residue coevolution captures native contacts across many protein
families. Proceedings of the National Academy of Sciences, 108:E1293–E1301, 2011."
REFERENCES,0.6089965397923875,"Lam-Tung Nguyen, Heiko A Schmidt, Arndt Von Haeseler, and Bui Quang Minh. IQ-TREE:
a fast and eﬀective stochastic algorithm for estimating maximum-likelihood phylogenies.
Molecular biology and evolution, 32(1):268–274, 2015."
REFERENCES,0.6124567474048442,"Linus Pauling, Emile Zuckerkandl, T Henriksen, and R L¨ovstad. Chemical paleogenetics.
Acta chemica Scandinavica, 17:S9–S16, 1963."
REFERENCES,0.615916955017301,"Ryan N Randall, Caelan E Radford, Kelsey A Roof, Divya K Natarajan, and Eric A
Gaucher. An experimental phylogeny to benchmark ancestral sequence reconstruction.
Nature communications, 7(1):1–6, 2016."
REFERENCES,0.6193771626297578,"Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter
Abbeel, and Yun S Song. Evaluating protein transfer learning with TAPE. Advances in
neural information processing systems, 32:9689, 2019."
REFERENCES,0.6228373702422145,Published as a conference paper at ICLR 2022
REFERENCES,0.6262975778546713,"Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F Canny, Pieter Abbeel, Tom
Sercu, and Alexander Rives. MSA transformer. bioRxiv, 2021."
REFERENCES,0.629757785467128,"Adam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of
genetic variation capture the eﬀects of mutations. Nature methods, 15(10):816–822, 2018."
REFERENCES,0.6332179930795848,"Avery GA Selberg,
Eric A Gaucher,
and David A Liberles.
Ancestral sequence
reconstruction:
From chemical paleogenetics to maximum likelihood algorithms and
beyond. Journal of Molecular Evolution, (3):157–164, 2021."
REFERENCES,0.6366782006920415,"Matthew A Spence, Joe A Kaczmarski, Jake W Saunders, and Colin J Jackson. Ancestral
sequence reconstruction for protein engineers. Current Opinion in Structural Biology, pp.
131–141, 2021."
REFERENCES,0.6401384083044983,"Max R Tolkoﬀ, Michael E Alfaro, Guy Baele, Philippe Lemey, and Marc A Suchard.
Phylogenetic factor analysis. Systematic biology, (3):384–399, 2018."
REFERENCES,0.643598615916955,"Laurens Van der Maaten and Geoﬀrey Hinton. Visualizing data using t-SNE. Journal of
machine learning research, (11), 2008."
REFERENCES,0.6470588235294118,"Sharad Vikram, Matthew D Hoﬀman, and Matthew J Johnson. The LORACs prior for
VAEs: Letting the trees speak for the data. In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pp. 3292–3301. PMLR, 2019."
REFERENCES,0.6505190311418685,"C Wilson, RV Agafonov, M Hoemberger, S Kutter, A Zorba, J Halpin, V Buosi, R Otten,
D Waterman, DL Theobald, et al. Using ancient protein kinases to unravel a modern
cancer drug’s mechanism. Science, 347(6224):882–886, 2015."
REFERENCES,0.6539792387543253,"Ziheng Yang. PAML 4: Phylogenetic analysis by maximum likelihood. Molecular biology
and evolution, 24(8):1586–1591, 2007."
REFERENCES,0.657439446366782,"Ziheng Yang, Sudhir Kumar, and Masatoshi Nei. A new method of inference of ancestral
nucleotide and amino acid sequences. Genetics, 141(4):1641–1650, 1995."
REFERENCES,0.6608996539792388,Published as a conference paper at ICLR 2022
REFERENCES,0.6643598615916955,"A
Appendix"
REFERENCES,0.6678200692041523,"A.1
Notation and variables"
REFERENCES,0.671280276816609,"Name
Description
nZ
Number of TOU processes, length of latent vector (30)
nL
Alignment length
nS
Number of leaf sequences
nA
Number of ancestral sequences (nA = nS −1)
nC
Number of character types
S
Sequence alignment matrix of leaf sequences
Z(S) ≡Z
Matrix of latent representations of the leaf sequences
Note: We use Z for notational convenience where possible.
Z(A)
Matrix of latent representations of the ancestral sequences"
REFERENCES,0.6747404844290658,"Z(A,S)
Matrix of latent representations of the leaf and ancestral
sequences
T
Patristic distance matrix (given)"
REFERENCES,0.6782006920415224,"T(S,S)
Patristic distance submatrix of distances between leaf
sequences (given)"
REFERENCES,0.6816608996539792,"T(A,A)
Patristic distance submatrix of distances between ancestral
sequences (given)"
REFERENCES,0.6851211072664359,"T(A,S)
Patristic distance submatrix of distances between leaf and
ancestral sequences (given)
B
BLOSUM matrix (given)
α
Vector of OU hyperprior parameters
σf
Vector of intensities of inherited variation (TOU process)
σn
Vector of intensities of speciﬁc variation (TOU process)
λ
Vector of characteristic lenght-scales (TOU process)
C
Tensor of nZ TOU process covariance matrices
V
Matrix of weighted BLOSUM vectors
E
Matrix of BLOSUM embeddings in the model
F
Tensor of BLOSUM embeddings in the guide
Y
Input tensor to GRU
H
State tensor of GRU
L
Tensor of logits of the nC sequence characters
θ
Parameters of the neural networks and the GRU
0(d)
d-dimensional vector of zeros
0(m×n)
(m × n)-dimensional matrix of zeros
GRU
Gated recurrent unit
NN
Fully connected neural network
HN
Half-normal distribution
MVN
Multivariate Gaussian distribution
cat
Concatenation of two vectors."
REFERENCES,0.6885813148788927,Table 1: Variables and notation used for the Draupnir model.
REFERENCES,0.6920415224913494,Published as a conference paper at ICLR 2022
REFERENCES,0.6955017301038062,"Name
Dimensions
α
3
σf, σn, λ
nZ
T
(nS + nA) × (nS + nA)
T(A,A)
nA × nA
T(S,S)
nS × nS
T(A,S)
nA × nS
C
nZ × nS × nS
Z(S) ≡Z
nS × nZ
Z(A)
nA × nZ
Z(A,S)
(nS + nA) × nZ
Y
nS × nL × (nZ + nC)
E
nL × nC
F
nS × nL × nC
H
nS × nL × 60
L
nS × nL × nC
B
nC × nC
S
nS × nL
V
nL × nC"
REFERENCES,0.698961937716263,Table 2: Dimensions of variables
REFERENCES,0.7024221453287197,"A.2
Draupnir settings"
REFERENCES,0.7058823529411765,"Neural network architecture
The Draupnir model contains three neural networks (see
Algorithm 1, Figure 1 and Figure 8): a fully connected network, NN(1)
θ , that maps the pre-
computed BLOSUM vectors, V, to BLOSUM embeddings, E; a bidirectional GRUθ (Cho
et al., 2014) with a single layer that takes as input the BLOSUM embeddings and the latent
vector of the k−th leaf sequence, Zk,:, and a second fully connected network, NN(2)
θ , that
maps the GRUθ states to the logit vectors of the sequence characters. The dimensionality
of the state of the bidirectional GRUθ is 2 × 60."
REFERENCES,0.7093425605536332,"In the guide, we re-use the neural network architectures described above: NN(2)
φ
and GRUφ
are identical to NN(2)
θ
and GRUθ, respectively, except that the output size of NN(2)
φ
is (2×nZ)
instead of nC, corresponding to the length of the mean vector and standard deviation vector
of the latent representation. NN(1)
φ
is identical to NN(1)
θ ."
REFERENCES,0.71280276816609,"−−→
←−−
2x60"
REFERENCES,0.7162629757785467,"fc1
120x60"
REFERENCES,0.7197231833910035,"fc2
60x21"
REFERENCES,0.7231833910034602,LogSoftmax
REFERENCES,0.726643598615917,"fc3
21x50"
REFERENCES,0.7301038062283737,"fc4
50x21"
REFERENCES,0.7335640138408305,"Figure 8: Architectures and dimensions of the neural networks used in Draupnir. Left:
Architecture of the bidirectional GRUθ (red) and NN(2)
θ . Right: Architecture of NN(1)
θ . “fc”
indicates a fully connected layer."
REFERENCES,0.7370242214532872,Published as a conference paper at ICLR 2022
REFERENCES,0.740484429065744,"Additional Draupnir settings
We chose BLOSUM62 as the base substitution matrix,
except for the CFP datasets (71 and 35 leaves) where we use PAM70 due to the presence
of special amino acids."
REFERENCES,0.7439446366782007,"A.3
Data sets"
REFERENCES,0.7474048442906575,"Table 3: Descriptions of the eleven data sets used for benchmarking and the leaves-only
data set used in the co-evolutionary analysis. All data sets contain insertions and deletions
(gaps), except the one in italic (top), which only contains substitutions. Data sets labelled
with an asterisk only contain the sequence of the root node."
REFERENCES,0.7508650519031141,"Dataset
Number of
leaves
Alignment
length
Source"
REFERENCES,0.754325259515571,Datasets with experimentally determined ancestral sequences
REFERENCES,0.7577854671280276,"Randall’s Coral ﬂuorescent proteins (CFP) benchmark
19
225
Randall et al. (2016)
*Coral ﬂuorescent proteins (CFP) Faviina subclade
35
261
allfav root node sequence from Alieva et al. (2008)
*Coral ﬂuorescent proteins (CFP) subclade
71
272
allcor root node sequence from Alieva et al. (2008)"
REFERENCES,0.7612456747404844,"EvolveAGene4 (Hall, 2005) simulations"
REFERENCES,0.7647058823529411,"Simulation β-Lactamase
32
314
GenBank accession no. AF309824
Simulation Calcitonin
50
71
NBCI CCDS7820.1
Simulation SRC-Kinase SH3 domain
100
63
GenBank BC011566.1
Simulation Sirtuin
150
477
NBCI CCDS44412.1
Simulation SRC-Kinase SH3 domain
200
128
GenBank BC011566.1
Simulation PIGBOS
300
77
NBCI CCDS81884.1
Simulation Insulin
400
558
NCBI BC011566.1
Simulation SRC-Kinase SH3 domain
800
99
GenBank BC011566.1"
REFERENCES,0.7681660899653979,Leaves only data set
REFERENCES,0.7716262975778547,"PF00400
125
138
PFAM family no. PF00400"
REFERENCES,0.7750865051903114,Published as a conference paper at ICLR 2022
REFERENCES,0.7785467128027682,"A.4
Training"
REFERENCES,0.7820069204152249,"Table 4: Training settings and running times (Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz,
Quadro RTX 6000 GPU). On the largest dataset (800 leaves), we make use of a Reduce on
plateau learning scheduler combined with plating to further improve the accuracy results."
REFERENCES,0.7854671280276817,"Dataset
Epochs
Plate
size"
REFERENCES,0.7889273356401384,"Learning
rate
scheduler"
REFERENCES,0.7923875432525952,"Model
parameters
Running times"
REFERENCES,0.7958477508650519,Datasets with experimentally determined ancestral sequences
REFERENCES,0.7993079584775087,"Randall’s Coral ﬂuorescent proteins (CFP) benchmark, 19 leaves
16600
19
No
52055
53 min 39 s
Coral ﬂuorescent proteins (CFP) Faviina subclade, 35 leaves
23000
35
No
55181
1 h 36 min 1 s
Coral ﬂuorescent proteins (CFP) clade, 71 leaves
23000
71
No
52535
1 h 4 min"
REFERENCES,0.8027681660899654,"EvolveAGene4 (Hall, 2005) simulations"
REFERENCES,0.8062283737024222,"Simulation β-Lactamase, 32 leaves
15000
32
No
52445
1 h 41 min 39 s
Simulation Calcitonin, 50 leaves
18700
50
No
52985
2 h 10 min 1 s
Simulation SRC-Kinase SH3 domain, 100 leaves
21600
100
No
54485
2 h 42 min 10 s
Simulation Sirtuin, 150 leaves
20000
150
No
55985
4 h 6 min 23 s
Simulation SRC-Kinase SH3 domain, 200 leaves
22000
200
No
57485
49 min 21 s
Simulation PIGBOS, 300 leaves
18000
300
No
60485
44 min 1 s
Simulation Insulin, 400 leaves
18400
400
No
63485
3 h 32 min 9 s
Simulation SRC-Kinase SH3 domain, 800 leaves
25000
50
Yes
141005
3 h 39 min 29 s"
REFERENCES,0.8096885813148789,Leaves only data set
REFERENCES,0.8131487889273357,"PF00400, 125 leaves - Marginal
23000
125
No
55235
52 min 48 s
PF00400, 125 leaves - Variational
23000
125
No
137487
1 h 21 min 39 s"
REFERENCES,0.8166089965397924,Published as a conference paper at ICLR 2022
REFERENCES,0.8200692041522492,"A.5
Benchmarking tables"
REFERENCES,0.8235294117647058,"Table 5: Benchmarking results using protein sequences. The table shows the means and
the standard deviation of the percent identity for all the predicted ancestral sequences and
their corresponding true sequences. In the case of Draupnir-MAP, the means and standard
deviations are calculated using the most likely sequence of each ancestral node. In the case
of Draupnir-marginal samples, they are calculated using 50 samples per ancestral node.
“Not available” indicates the ASR method failed to produce results for that data set on the
given hardware. The results for the standard ASR methods were obtained using the amino
acid sequences."
REFERENCES,0.8269896193771626,"Number of leaves
Alignment length
Draupnir MAP
Draupnir marginal
samples
PAML-CodeML
PhyloBayes
FastML
IQTree"
REFERENCES,0.8304498269896193,"Randall’s Coral ﬂuorescent proteins (CFP)
19
225
95.03±1.29
93.67±0.85
98.14±1.3
98.09±1.03
98.17±1.31
98.27±1.07
Coral ﬂuorescent proteins (CFP) clade
71
272
74.49±1.07
74.78±1.09
60.96±0.8
61.17±0.85
74.94±1.07
60.96±0.8
Coral ﬂuorescent proteins (CFP) Faviina subclade
35
261
86.49±0.98
86.46±1.11
74.56±1.14
74.33±0.86
86.76±1.27
76.01±1.16
Simulation Beta-Lactamase
32
314
81.92±7.97
77.37±7.08
57.91±22.39
83.12±6.13
87.52±6.28
83.07±6.01
Simulation Calcitonin, 50
50
71
80.77±9.22
77.73±7.33
41.94±21.21
69.45±8.88
86.52±6.07
69.54±8.83
Simulation SRC-Kinase SH3 domain, 100
100
63
74.94±10.46
67.27±8.92
33.24±17.33
66.78±9.35
83.21±8.71
66.76±9.17
Simulation Sirtuin SIRT1, 150
150
477
86.59±4.9
77.78±3.98
12.87±7.16
17.36±1.15
Not available
16.09±1.36
Simulation SRC-Kinase SH3 domain, 200
200
128
86.92±5.84
82.65±4.85
9.69±7.99
9.10±1.64
91.09±4.54
10.44±2.71
Simulation PIGB Opposite Strand regulator
300
77
92.69±4.08
86.34±3.43
33.57±10.39
55.20±6.12
Not available
55.53±5.95
Simulation Insulin Factor like
400
558
86.48±4.06
73.81±3.16
16.17±8.37
23.30±1.74
Not available
25.22±1.61
Simulation SRC-Kinase SH3 domain, 800
800
99
91.57±4.3
90.24±3.63
Not available
Not available
Not available
49.63±3.6"
REFERENCES,0.8339100346020761,"Table 6: Benchmarking results using DNA sequences. The table is similar to Table 5 but the
reconstructions for the standard ASR methods were obtained using DNA instead of amino
acid sequences. The DNA sequences were subsequently translated into protein sequences
before comparison."
REFERENCES,0.8373702422145328,"Number of leaves
Alignment length
Draupnir MAP
Draupnir marginal
samples
PAML-CodeML
PhyloBayes
FastML
IQTree"
REFERENCES,0.8408304498269896,"Randall’s Coral ﬂuorescent proteins (CFP)
19
225
95.03±1.29
93.67±0.85
98.69±0.82
98.82±0.83
98.59±0.77
98.69±0.76
Coral ﬂuorescent proteins (CFP) clade
71
272
74.49±1.07
74.78±1.09
60.88±0.85
54.65±0.71
74.94±1.07
61.10±0.79
Coral ﬂuorescent proteins (CFP) Faviina subclade
35
261
86.49±0.98
86.46±1.11
76.01±1.16
76.67±1.13
86.76±1.27
76.01±1.16
Simulation Beta-Lactamase
32
314
81.92±7.97
77.37±7.08
58.21±22.68
83.61±6.15
87.66±6.3
83.65±6.13
Simulation Calcitonin, 50
50
71
80.77±9.22
77.73±7.33
41.88±21.14
70.01±8.92
86.55±6.29
69.42±8.89
Simulation SRC-Kinase SH3 domain, 100
100
63
74.94±10.46
67.27±8.92
33.06±17.56
68.90±9.24
82.31±9.9
67.90±9.31
Simulation Sirtuin SIRT1, 150
150
477
86.59±4.9
77.78±3.98
13.02±7.25
22.54±2.08
87.92±8.4
68.98±0.79
Simulation SRC-Kinase SH3 domain, 200
200
128
86.92±5.84
82.65±4.85
9.72±8.17
17.60±2.53
84.48±8.5
61.62±2.71
Simulation PIGB Opposite Strand regulator
300
77
92.69±4.08
86.34±3.43
33.64±10.35
55.79±5.23
Not available
54.10±5.03
Simulation Insulin Factor like
400
558
86.48±4.06
73.81±3.16
16.23±8.4
26.02±2.22
Not available
24.63±3.94
Simulation SRC-Kinase SH3 domain, 800
800
99
91.57±4.3
90.24±3.63
Not available
Not available
Not available
49.66±3.68"
REFERENCES,0.8442906574394463,"A.6
Coevolution analysis"
REFERENCES,0.8477508650519031,"Conventional ASR methods use models that assume factorized evolution (Horta et al., 2021),
that is, they assume that each site evolves independently of all other sites in the sequence.
In reality, some sites are coupled in evolution, resulting in dependencies between sites. This
phenomenon is called epistasis (Hochberg & Thornton, 2017). Draupnir is a model that
goes beyond factorized evolution, and thus potentially models coevolving sites. Here, we
analyze to what extend this is indeed the case."
REFERENCES,0.8512110726643599,"In order to evaluate modelling of coevolution, we make use of direct coupling analysis (DCA)
(Morcos et al., 2011). DCA identiﬁes coevolving pairs of residues that directly inﬂuence each
other by calculating a quantity called Direct Information (DI), which is obtained by ﬁtting
a Markov random ﬁeld. We calculated the DI using ProDy ((Bakan et al., 2014)). As data
set, we used 125 sequences from the WB40 domain of the PF00400 family from the PFAM
data base Mistry et al. (2021)."
REFERENCES,0.8546712802768166,"The DIs of the leaf sequences serve as ground truth and are compared with sequences
sampled from Draupnir at the root node in three diﬀerent ways (see below). If coevolution
is at least partially modelled, the DIs of the leaf sequences will be similar (but not completely
identical) to the DIs of the sampled sequences at the root node. We sample sequences from
Draupnir using three diﬀerent methods."
REFERENCES,0.8581314878892734,"The ﬁrst method (MAP) simply uses the MAP estimates of the probability vectors at each
position:"
REFERENCES,0.8615916955017301,"Published as a conference paper at ICLR 2022 a ∼ nL
Y"
REFERENCES,0.8650519031141869,"i=1
p

ai | θ(MAP)
i

,
(4)"
REFERENCES,0.8685121107266436,"where a is an ancestral sequence and θ(MAP)
i
is the MAP estimate of the amino acid
probability vector at position i for that sequence.
This baseline results in independent
sites. Picking the most likely amino acid at each position according to the above expression
corresponds to Draupnir -MAP in Section 5.2."
REFERENCES,0.8719723183391004,"The second method (Draupnir-marginal) makes use of the MAP estimates of the leaf
representations but marginalizes over the ancestral representations (see Section 4.4):"
REFERENCES,0.8754325259515571,"a ∼
Z  nL
Y"
REFERENCES,0.8788927335640139,"i=1
p

ai |
h
θ

z(a)i i !"
REFERENCES,0.8823529411764706,"p(z(a) | Z(MAP))dz(a),
(5)"
REFERENCES,0.8858131487889274,"where the probability vectors θi are obtained from a GRU applied to the latent
representation z(a) of the ancestral sequence a (see Algorithm 1). The last factor involves
conditional Gaussian distributions (see Equation 3 and Section 4.4). Integrating over z(a)
introduces correlations along the sequence. Therefore, this method is in principle capable
to model some coevolution as we integrate over the latent representations of the ancestors
(while using a point estimate for the latent representations of the leaves). This corresponds
to the “Draupnir marginal samples” in Section 5.2."
REFERENCES,0.889273356401384,"In the third method (Variational), we make use of a guide qφ(Z | S) to obtain a variational
posterior over the latent representations Z (see Algorithm 3):"
REFERENCES,0.8927335640138409,"a ∼
Z  nL
Y"
REFERENCES,0.8961937716262975,"i=1
p

ai |
h
θ

z(a)i i !"
REFERENCES,0.8996539792387543,"p(z(a) | Z)qφ(Z | S)dz(a)dZ.
(6)"
REFERENCES,0.903114186851211,"In this case, we marginalize over the latent representations of both leaves and ancestors.
This method should capture the coevolutionary signal to a greater extent than the second
method."
REFERENCES,0.9065743944636678,"The results are shown in Figure 9. As expected the third method, (Variational) does best,
while the ﬁrst method (MAP) does worst. The Variational method improves considerably
upon the Marginal method, indicating that replacing the MAP estimate with a variational
distribution for the latent representations of the leaves has signiﬁcant impact. The results
indicate that Draupnir indeed to a signiﬁcant extent can capture coevolutionary information."
REFERENCES,0.9100346020761245,Published as a conference paper at ICLR 2022
REFERENCES,0.9134948096885813,"Algorithm 3 Architecture of the variational guide, qφ(Z | S). We use point estimates
for the hyperparameters and parameters of the TOU processes, and multivariate diagonal
Gaussian distributions for the latent representations. An asterisk indicates the value of a
point estimate. δ(.) is the Dirac delta function."
REFERENCES,0.916955017301038,Require: Multiple sequence alignment S
REFERENCES,0.9204152249134948,"for j ∈{1, 2, 3} do
▷Point estimates of TOU hyperprior parameters
αj ∼δ(α∗
j)"
REFERENCES,0.9238754325259516,"for h ∈{1, ..., nZ} do
▷Point estimates of the parameters of the nZ TOU processes
σf,h ∼δ(σ∗
f,h)
σn,h ∼δ(σ∗
n,h)
λh ∼δ(λ∗
h)"
REFERENCES,0.9273356401384083,"for k ∈[1, . . . , nS] do"
REFERENCES,0.9307958477508651,"for i ∈[1, . . . , nL] do"
REFERENCES,0.9342560553633218,"r ←Sk,i
▷Amino acid at position i in leaf sequence k
Fk,i,: ←NN(1)
φ (Br,:)
▷BLOSUM embedding"
REFERENCES,0.9377162629757786,"for k ∈[1, . . . , nS] do"
REFERENCES,0.9411764705882353,"Hk,:,: ←GRUφ(Fk,:,:)
▷Bidirectional GRU states
m, c ←NN(2)
φ (Hk,−1,:)
▷Mean and standard deviations of Zk,:
Zk,: ∼MVN(m, diag(c))"
REFERENCES,0.9446366782006921,"Figure 9: DI values for all position pairs of the WB40 data set obtained from the leaf
sequences (upper left) and sequences sampled at the root node (other plots) using the
MAP, Marginal and Variational methods. We sampled 125 root sequences, which is equal
to the number of leaves. The correlation coeﬃcients between the DIs of the leaves and the
sampled sequences are 0.05 (MAP), 0.66 (Marginal) and 0.79 (Variational)."
REFERENCES,0.9480968858131488,Published as a conference paper at ICLR 2022
REFERENCES,0.9515570934256056,"A.7
Quantitative analysis of the latent space representations"
REFERENCES,0.9550173010380623,"In order to compare the standard VAE with Draupnir in a quantitative way (see Figure
7), we analyze the correlation between (a) the Euclidean distances between the latent
representations of the leaves and (b) the corresponding branch lengths in the phylogenetic
tree for both models. The results are shown in Figure 10."
REFERENCES,0.9584775086505191,"Figure 10: Comparison of the Euclidean distances between the latent representations of the
leaves (y-axis) and the corresponding branch lengths in the phylogenetic tree (x-axis). We
use the same color scheme as in Figure 2. We traverse the tree in level order and assign the
colour of the clade of the ﬁrst leaf. (Left) The standard VAE. The correlation coeﬃcient
is 0.79; the Spearman correlation coeﬃcient is 0.85. (Right) Draupnir. The correlation
coeﬃcient is 0.91; the Spearman correlation coeﬃcient is 0.94."
REFERENCES,0.9619377162629758,Published as a conference paper at ICLR 2022
REFERENCES,0.9653979238754326,"A.8
Benchmark settings"
REFERENCES,0.9688581314878892,"PAML-CodeML settings
PAML-CodeML (provided by Biopython 1.78) was used with
the following settings:"
REFERENCES,0.972318339100346,"verbose
2 (includes detailed information of the posterior probabilities per node)
runmode
0 (utilize given tree)
seqtype
2 (amino acids)
clock
0 (no molecular clock, genes are evolving at diﬀerent rates)
aaDist
0
getSE
0
RateAncestor
1
aaRateFile
WAG
method
1
model
2 (more dn/ds (selection coeﬃcient) ratios per branch)
ﬁx alpha
0 (estimate gamma)
alpha
0.5 (initial alpha value for gamma distribution)
ﬁx blength
0 (keep given branch lengths)"
REFERENCES,0.9757785467128027,PhyloBayes 4.1 settings
REFERENCES,0.9792387543252595,pd -s -f -T treeﬁle -cat -gtr -d alignmentﬁle chainname
REFERENCES,0.9826989619377162,"In the case of PhyloBayes, as recommended, we run 2 Markov chains until the convergence
criteria are met. The recommended convergence criteria are minimum eﬀective sample size
above 300 and max diﬀamong the chains below 0.1. Both for evaluation of the convergence
of the chains and for sampling the ancestral sequences we use 100 samples."
REFERENCES,0.986159169550173,FastML v3.11 settings
REFERENCES,0.9896193771626297,"perl fastml –MSA-ﬁle alignmentﬁle –seqType aa or nuc –SubMatrix WAG or GTR
–indelReconstruction ML –Tree treeﬁle"
REFERENCES,0.9930795847750865,"IQ-Tree v2.0.3 settings
For IQ-Tree (multicore version 2.0.3), model choice and settings are automatically
optimized by using the options “-m TEST” and “-nt AUTO”."
REFERENCES,0.9965397923875432,iqtree -s alignmentﬁle -m TEST -asr -te treeﬁle -nt AUTO
