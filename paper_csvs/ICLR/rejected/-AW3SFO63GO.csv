Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003663003663003663,"Adversarial examples have attracted signiﬁcant attention over the years, yet a suf-
ﬁcient understanding is in lack, especially when analyzing their performances in
combination with adversarial training. In this paper, we revisit some properties of
adversarial examples from both frequency and spatial perspectives: 1) the special
high-frequency components of adversarial examples tend to mislead naturally-
trained models while have little impact on adversarially-trained ones, and 2)
adversarial examples show disorderly perturbations on naturally-trained models
and locally-consistent (image shape related) perturbations on adversarially-trained
ones. Motivated by these, we analyze the fragile tendency of models with the gen-
erated adversarial perturbations, and propose a connection with model vulnerabil-
ity and local intermediate response. That is, a smaller local intermediate response
comes along with better model adversarial robustness. To be speciﬁc, we demon-
strate that: 1) DNNs are naturally fragile at least for large enough local response
differences between adversarial/natural examples, 2) and smoother adversarially-
trained models can alleviate local response differences with enhanced robustness."
INTRODUCTION,0.007326007326007326,"1
INTRODUCTION"
INTRODUCTION,0.01098901098901099,"Despite deep neural networks (DNNs) perform well in many ﬁelds (He et al., 2016; Devlin et al.,
2019), their counter-intuitive vulnerability attracts increasing attention, both for safety-critical ap-
plications (Sharif et al., 2016) and the black-box mechanism of DNNs (Fazlyab et al., 2019). DNNs
have been found vulnerable to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015),
where small perturbations on the input can easily change the predictions of a well-trained DNN with
high conﬁdence. In computer vision, adversarial examples exhibit their destructiveness both in the
digital world and the physical world (Kurakin et al., 2017)."
INTRODUCTION,0.014652014652014652,"Since then, how to alleviate the vulnerability of DNN so as to narrow the performance gap between
adversarial/natural examples is another key issue. Existing methods including defensive distilla-
tion (Papernot et al., 2016) and pixel denoising (Liao et al., 2018) have shown their limitations
due to follow-up attack strategies (Carlini & Wagner, 2017) or gradient masking (Athalye et al.,
2018). Amongst them, adversarial training (Goodfellow et al., 2015; Madry et al., 2018) and its
variants (Zhang et al., 2019; Wang et al., 2020b) indicate their reliable robustness and outperform.
Moreover, as a data augmentation method, adversarial training currently seems to rely on additional
data (Schmidt et al., 2018; Rebufﬁet al., 2021) to further improve robustness, while is sensitive to
some basic model hyper-parameters, e.g., weight decay (Pang et al., 2021). Apart from these, the
effect of simply early stopping (Rice et al., 2020) even exceeds some promotion methods according
to recent benchmarks (Croce & Hein, 2020; Chen & Gu, 2020). These studies arise our curiosity
to further explore the relationship between adversarial examples and adversarial training, hoping to
provide some new understanding."
INTRODUCTION,0.018315018315018316,"Recalling that high-frequency components can be potentially linked to adversarial examples (Wang
et al., 2020a; Yin et al., 2019; Harder et al., 2021), however, few explorations discuss the relation-
ship between high-frequency components and the destructiveness of adversarial examples. In this
paper, we ﬁrst demonstrate that high-frequency components of adversarial examples tend to mislead
the standard DNNs, yet little impact on the adversarially robust models. We further show that ad-
versarial examples statistically have more high-frequency components than natural ones, indicating
relatively drastic local changes among pixels of adversarial examples. Since adversarial examples"
INTRODUCTION,0.02197802197802198,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02564102564102564,"exhibit more semantically meaningful on robust models (Tsipras et al., 2019), we further notice that
adversarial examples show locally-consistent perturbations related to image shapes on adversarially-
trained models, in contrast to disorderly perturbations on standard models. Both explorations on the
frequency and spatial domain emphasize local properties of adversarial examples, and motivated
by the local receptive ﬁeld of the convolution kernels, we propose a locally intermediate response
perspective to rethink the vulnerability of DNNs. Different from the existing global activation per-
spective (Bai et al., 2021; Xu et al., 2019), our local perspective reﬂects the joint effect of local
features and the intermediate layers of the model. Based on the local perspective, we emphasize
that large enough local response differences make it difﬁcult for the network to treat an image and
its potentially adversarial examples as one category, and demonstrate DNN models are naturally
fragile at least attributed to it. Motivated by adversarially-trained models tend to have ‘smooth’ ker-
nels (Wang et al., 2020a), we simply use the smoother kernels to alleviate local response differences
on adversarially-trained models, which in turn affects the model robustness and reduces the robust
overﬁtting (Rice et al., 2020). To a certain extent, this explains why weight decay effectively affects
model robustness (Pang et al., 2021)."
INTRODUCTION,0.029304029304029304,Our main contributions are summarized as follows:
INTRODUCTION,0.03296703296703297,"• We ﬁrst reveal some properties of adversarial examples in the frequency and spatial domain:
1) the high-frequency components of adversarial examples tend to mislead naturally-trained
DNNs, yet have little impact on adversarially-trained models, and 2) adversarial examples
have locally-consistent perturbations on adversarially-trained models, compared with dis-
orderly local perturbations on naturally-trained models."
INTRODUCTION,0.03663003663003663,"• Then we introduce local response and emphasize its importance in the model adversarial
robustness. That is, naturally-trained DNNs are often fragile, at least for non-ignorable lo-
cal response differences through the same layer between potentially adversarial examples
and natural ones. In contrast, adversarially-trained models effectively alleviate the local re-
sponse differences. And the smoother adversarially-trained models show better adversarial
robustness as they can reduce local response differences."
INTRODUCTION,0.040293040293040296,"• Finally we empirically study local response with generated adversarial examples. We fur-
ther show that, compared with failed attacks, adversarial examples (successful attacks)
statistically show larger local response differences with natural examples. Moreover, com-
pared with adversarial examples generated by the model itself, those transferred by other
models show markedly smaller local response differences."
RELATED WORK,0.04395604395604396,"2
RELATED WORK"
RELATED WORK,0.047619047619047616,"Understandings of model vulnerability. Since the discovery of adversarial examples (Szegedy
et al., 2014), a number of understandings on model vulnerability has been developed. For in-
stance, linear property of DNNs (Goodfellow et al., 2015), submanifold (Tanay & Grifﬁn, 2016)
and geometry of the manifold (Gilmer et al., 2018) were considered from the high-dimensional
perspective; the computing of Lipschitz constant (Szegedy et al., 2014; Fazlyab et al., 2019) and
lower/upper bounds (Fawzi et al., 2018; Weng et al., 2018) were considered from the deﬁnition of
model robustness; non-robust features (Ilyas et al., 2019), high-frequency components (Wang et al.,
2020a; Yin et al., 2019), high-rank features (Jere et al., 2020) and high-order interactions among
pixels (Ren et al., 2021) were explored adversarial examples from the different perspectives on im-
ages, which imply our local perspective; feature denosing (Xie et al., 2019), robust pruning (Madaan
& Ju Hwang., 2020) and activation suppressing (Bai et al., 2021) were focused on global intermedi-
ate activations of models. On the other hand, taken adversarial training into consideration, Tsipras
et al. (2019), Schmidt et al. (2018) and Zhang et al. (2019) explored the trade-off between robust-
ness and accuracy; Wang et al. (2020a) found adversarially-trained models tend to show smooth
kernels; Tsipras et al. (2019) and Zhang & Zhu (2018) argued adversarially robust models learned
more shape-biased representations."
RELATED WORK,0.05128205128205128,"Different from these studies, we characterize the adversarial examples from both frequency and spa-
tial domain to emphasize local properties of adversarial examples, and propose a locally intermediate
response to rethink the vulnerability of DNNs."
RELATED WORK,0.054945054945054944,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05860805860805861,Adversarial training. Adversarial training can be seen as a min-max optimization problem:
RELATED WORK,0.06227106227106227,"min
θ
1
n n
X"
RELATED WORK,0.06593406593406594,"i=1
max
x′
i∈B(x) L(fθ(x′
i), yi)"
RELATED WORK,0.0695970695970696,"where f denotes a DNN model with parameters θ, and (xi, yi) denotes a pair of a natural example
xi and its ground-truth label yi. Given a classiﬁcation loss L, the inner maximization problem can
be regarded as searching for suitable perturbations in boundary B to maximize loss, while the outer
minimization problem is to optimize model parameters on adversarial examples {x′
i}n
i=1 generated
from the inner maximization."
DIFFERENT PERTURBATIONS AFFECT THE VULNERABILITY OF THE MODEL,0.07326007326007326,"3
DIFFERENT PERTURBATIONS AFFECT THE VULNERABILITY OF THE MODEL"
DIFFERENT PERTURBATIONS AFFECT THE VULNERABILITY OF THE MODEL,0.07692307692307693,"In this section, we ﬁrst investigate adversarial examples in the frequency and spatial domain, and
show the connections between models and their adversarial examples. Note that our threat model is
a white-box model, and the fact that the adversarial example is only deﬁned by the misleading result
under the legal threat model, our ﬁndings suggest that the properties of the sampled examples can
broadly reﬂect the fragile tendency of the model."
DIFFERENT PERTURBATIONS AFFECT THE VULNERABILITY OF THE MODEL,0.08058608058608059,"Setup. We generate ℓ∞bounded adversarial examples by PGD-10 (maximum perturbation ϵ =
8/255 and step size 2/255) with random start for the robust model (Madry et al., 2018). Speciﬁcally,
we use ResNet-18 (He et al., 2016) as the backbone to train the standard and adversarially-trained
models for 100 epochs on CIFAR-10 (Krizhevsky, 2009). Following (Pang et al., 2021), we use
the SGD optimizer with momentum 0.9, weight decay 5 × 10−4 and initial learning rate 0.1, with
a three-stage learning rate divided by 10 at 50 and 75 epoch respectively. The robustness of both
models is evaluated by PGD-20 (step size 1/255)."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.08424908424908426,"3.1
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.08791208791208792,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.09157509157509157,Test Accuracy on Filtered Images
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.09523809523809523,"natural examples
adversarial examples"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.0989010989010989,(a) ﬁltered images (STD)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.10256410256410256,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.10622710622710622,Test Accuracy on Filtered Images
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.10989010989010989,"natural examples
adversarial examples"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.11355311355311355,(b) ﬁltered images (ADV)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.11721611721611722,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.12087912087912088,Test Accuracy on Merged Images
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.12454212454212454,"merged natural examples
merged adversarial examples"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1282051282051282,(c) merged images (STD)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.13186813186813187,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.13553113553113552,Test Accuracy on Merged Images
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1391941391941392,"merged natural examples
merged adversarial examples"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.14285714285714285,(d) merged images (ADV)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.14652014652014653,"Figure 1: The destructiveness of high-frequency components from natural and adversarial examples
on both standard (STD) and adversarially-trained (ADV) models. Shown above are well-trained
models tested with (a)-(b) images through low-pass ﬁlter and (c)-(d) frequency-swapped images."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.15018315018315018,"Inspired by (Wang et al., 2020a), we are naturally curious whether the adversarial examples cause
considerable damage to the model mainly because of their high-frequency components. To answer
this question, Figure 1 illustrates the trend of model performance and robustness on the test set with
the high-frequency components increased (the increase of the ﬁltering scale denotes that more high-
frequency components are added to the ﬁltered images). Figure 1(a) shows that, for standard models,
the high-frequency components of natural examples promote classiﬁcation and reach the model per-
formance (green line); on the contrary, the performance of the ﬁltered adversarial examples ﬁrst rises
to get the highest accuracy 47.5%, and then drops rapidly to reach 0.0% (red line). Obviously, in the
low-frequency range, the performance of natural and adversarial examples are quite close, yet more
high-frequency components widen the difference. That is, the special high-frequency components
caused by adversarial perturbations exhibit a clear destructive effect on standard models, and simply
ﬁlter out them can effectively alleviate the destructiveness of adversaries even on standard models."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.15384615384615385,"However, for robust models, we show that the prediction performance ﬁnally reaches robustness
without a rapid drop in Figure 1(b). But surprisingly, we ﬁnd that the performance of ﬁltered adver-
sarial examples in some range exceeds the ﬁnal robustness 47.5% (red line), reaching a maximum of
51.2%. That is, although these high-frequency components do not exhibit a clear destructive effect,
simply ﬁltering out them has a positive impact on alleviating robust overﬁtting (Rice et al., 2020)."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1575091575091575,Under review as a conference paper at ICLR 2022
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.16117216117216118,"We then swap their high-frequency components between both examples controlled by a frequency
threshold in Figure 1(c)-1(d). For merged natural examples with high-frequency components from
adversaries, the increase of the frequency threshold controls the accuracy increasing from the model
robustness (red line) to the model performance (green line), the opposite occurs on merged adversar-
ial examples. These clearly illustrate the boost effect of the high-frequency components from natural
examples and the destructive effect of the high-frequency components from adversarial examples."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.16483516483516483,"0
5
10
15
20
25
30 0 5 10 15 20 25 30 6 4 2 0 2 4 6"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1684981684981685,(a) ori. images
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.17216117216117216,"0
5
10
15
20
25
30 0 5 10 15 20 25 30 6 4 2 0 2 4 6"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.17582417582417584,(b) adv (STD)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1794871794871795,"0
5
10
15
20
25
30 0 5 10 15 20 25 30 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.18315018315018314,(c) diff (STD)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.18681318681318682,"0
5
10
15
20
25
30 0 5 10 15 20 25 30 6 4 2 0 2 4 6"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.19047619047619047,(d) adv (ADV)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.19413919413919414,"0
5
10
15
20
25
30 0 5 10 15 20 25 30 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.1978021978021978,(e) diff (ADV)
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.20146520146520147,"Figure 2: The average logarithmic amplitude spectrum of (a) 1000 three-channel images and (b) their
adversarial examples generated by the standard (STD) model, where the corners represent high-freq
range, and the colorbars represent the logarithmic amplitude log(| · |) (the redder the larger). And
(c) denotes the difference between (b) and (a), log(|adv|) −log(|nat|) = log(|adv|/|nat|), where
the write color of (c) represents equivalent, and the red represents |adv| > |nat|. (d) and (e) are on
the adversarially-trained (ADV) model, and (e) denotes the difference between (d) and (a)."
THE DESTRUCTIVENESS OF HIGH-FREQUENCY COMPONENTS FROM ADVERSARIES,0.20512820512820512,"To further illustrate, we ﬁnd that statistically, the main difference in the frequency domain between
natural and adversarial examples is concentrated in the high-frequency region in Figure 2. We visu-
alize the logarithmic amplitude spectrum of both examples. Figure 2(a) and 2(b) show that, compare
with natural examples’, the high-frequency components of the adversaries are hard to ignore. Figure
2(c) further emphasizes that adversarial examples markedly show more high-frequency components,
indicating relatively drastic local changes among pixels. This statistical difference explains the high
detection rate of using Magnitude Fourier Spectrum (Harder et al., 2021) to detect adversarial ex-
amples. Furthermore, Figure 2(d) and 2(e) show that the high-frequency components of adversarial
examples generated by robust models are less than those from standard models, yet still more than
natural examples’. Besides, the analysis of ﬁltering out low-frequency components in Figure 6
(Appendix A) also emphasizes our statement. That is, compared with natural examples’, the spe-
cial high-frequency components of adversarial examples show their serious misleading effects on
standard models, yet are not enough to be fully responsible for the destructiveness to robust models."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.2087912087912088,"3.2
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL"
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.21245421245421245,"(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)"
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.21611721611721613,"Figure 3: Visualisation of adversarial perturbations (PGD) generated by Left: adversarially-trained
(ADV) model and Right: standard (STD) model. The columns from left to right are represented
by (a)-(m) respectively: (a) natural example; (b) adversarial example and (c)-(g) its perturbations
(overall, average of channels, and three channels) generated by ADV model; (h) adversarial example
and (i)-(m) its perturbations generated by STD model. Shown above are all successfully attacked,
the ﬁrst row is attacked as a bird from a dog, the second a cat from a frog, the third a car from a ship."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.21978021978021978,"To answer the different effects of high-frequency components, we directly explore the adversarial
examples themselves, e.g., in the spatial domain. We ﬁrst get well-trained models as above and
visualize adversarial perturbations in Figure 3, including the overall perturbations scaled to [0,255],
the average perturbations of channels, and perturbations of three channels."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.22344322344322345,Under review as a conference paper at ICLR 2022
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.2271062271062271,"As shown in Figure 3(Right), the perturbations generated by the standard models tend to be locally
inconsistent and disordered. This observation is different from Figure 3(Left) that adversarial ex-
amples show locally-consistent perturbations related to image shapes on the adversarially-trained
models, that is, perturbations tend to be locally co-increasing or co-decreasing on each channel. In
more detail, the perturbation of each pixel on a single channel tend to reach the perturbation bound,
i.e., +8/255 (the reddest), -8/255 (the bluest), which is counter-intuitive under the iterative attack of
PGD-20 and naturally associated with the one-step attack FGSM in Figure 7 (Appendix B). In fact,
both attacks show similar perturbations and similar model robustness1, while the former produces
more detailed perturbations. Besides, compared with failed attacks in Figure 8 (Appendix B), ad-
versarial examples of successful attacks in Figure 3 show more misleading local perturbations, e.g.,
the perturbations in the ﬁrst row are more like a bird (bird wings are added), and the third like a car."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.23076923076923078,"Perturbations in frequency vs. spatial domain. Similar to the different destructive effects of
high-frequency components, the perturbations in the spatial domain exhibit a more intuitive differ-
ence related to the models. Since more high-frequency components indicate images have relatively
drastic local changes, we count that adversarial examples generated by adversarially-trained models
show fewer high-frequency components mainly due to their locally-consistent perturbations. These
perturbations with smooth local changes imply that simply ﬁltering out high-frequency components
has little promotion on robust models, while locally-disordered perturbations imply the effectiveness
of ﬁltering out high-frequency components on standard models."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.23443223443223443,"Perturbations vs. fragile trend of models. For a given model, optimization-based attacks attempt
to search for special perturbations to maximize the classiﬁcation loss of adversarial examples. We
note that for adversarially-trained models with smooth kernels, the perturbations that tend to max-
imize loss exhibit a locally-consistent tendency, and for standard models with non-smooth kernels,
the perturbations tend to be locally-disordered to maximize loss. This implies a potential connection
between the fragile trend of models and their potentially adversarial examples."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.23809523809523808,"4
LOCAL RESPONSES: FURTHER IMPACT OF PERTURBATIONS ON MODELS"
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.24175824175824176,"In this section, motivated by local properties of adversarial examples and the local receptive ﬁeld
of convolution kernels, we ﬁrst introduce a locally intermediate response perspective to rethink the
vulnerability of the model, and then empirically show the relationship between local responses and
the vulnerability of the model."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.2454212454212454,"Different from the existing idea of searching for adversarial examples, we consider under what
circumstances the potential examples, refer to all legal variants in boundary B regardless of whether
destructive or not, would show their destructive effects on a well-trained model, that is, the reason
why some macro-similar potential examples present prediction results far away from their natural
examples. Inspired by local properties of adversarial examples and kernels, we naturally consider
whether the destructiveness of potential examples can be viewed from local responses, which reﬂects
the combined effect of the local features and the model property."
DIFFERENT PERTURBATIONS AND THE FRAGILE TENDENCY OF THE MODEL,0.2490842490842491,"Note that under ideal circumstances, given a well-trained model, if the local responses of any ex-
amples on a certain layer are completely consistent, then the ﬁnal predictions of these examples
are exactly the same. To relax the condition in reality, we hypothesize as follows (referred to as
Assumption 1): If macro-similar features through the same layer exhibit a sufﬁciently small differ-
ence in local responses, with sufﬁciently small differences in the subsequent layers, then the ﬁnal
responses of the network are relatively close; otherwise, the large enough local differences make
the network difﬁcult to treat them as the same category. To illustrate this, we take the convolutional
layer as an example."
LOCAL RESPONSES,0.25274725274725274,"4.1
LOCAL RESPONSES"
LOCAL RESPONSES,0.2564102564102564,"Macroscopically, we denote a DNN model f for classiﬁcation, the l-th layer of activation feature
maps f l, and f 0 to represent the input features. Macro-similar image and its potential example
pass through the l-th layer to get f l and ˆf l respectively. We also denote the (l+1)-th weight com-"
LOCAL RESPONSES,0.2600732600732601,"1The adversarially-trained model (the best checkpoint) reaches 51.9% robustness on the PGD-20 attack and
57.4% robustness on the FGSM attack."
LOCAL RESPONSES,0.26373626373626374,Under review as a conference paper at ICLR 2022
LOCAL RESPONSES,0.2673992673992674,"ponent M l+1 ∈RH×W ×K, where H, W, K represent height, weight and number of kernels re-
spectively. From the locally intermediate response perspective, we ﬁrst get one of the convolution
kernels ml+1 ∈RH×W , and then capture the l-th layer of local features centered at (i,j) position xl
i,j
and ˆxl
i,j corresponding to its local receptive ﬁeld. Formally, for the difference of local responses,"
LOCAL RESPONSES,0.27106227106227104,"∆l+1
i,j := ˆxl
i,j ⊗ml+1 −xl
i,j ⊗ml+1 = xd
l ⊗ml+1"
LOCAL RESPONSES,0.27472527472527475,"where ∆l+1
i,j denotes the (i,j)-th response difference of local features through the same kernel ml+1,
and xdl := ˆxl
i,j −xl
i,j denotes the difference of local feature maps between its potential example and
natural example. The second equation is based on the linear property of the convolution operation."
LOCAL RESPONSES,0.2783882783882784,"Ideally, if the differences are all equal to zeros at any positions in the (l+1)-th layer, that is, the
intermediate layer has the same utility for both examples, then the potential example after the (l+1)-
th layer shows exactly the same responses as the natural example’s. However, in most cases the local
differences are difﬁcult to be exactly zeros everywhere, then based on Assumption 1, our aim is to
make the absolute difference of local responses |∆l+1
i,j | as small as possible to approach the model’s
cognition of potential example and natural example."
LOCAL RESPONSES,0.28205128205128205,"The difference of local responses ∆l+1
i,j composed of xdl and ml+1 can be further expressed as:"
LOCAL RESPONSES,0.2857142857142857,"xd
l ⊗ml+1 = H
X i=1 W
X"
LOCAL RESPONSES,0.2893772893772894,"j=1
cl
ijml+1
ij"
LOCAL RESPONSES,0.29304029304029305,"where cl
ij and ml+1
ij
represent the (i,j)-th elements of xdl and ml+1 respectively, thus the absolute
local difference |∆l+1
i,j | is affected jointly by both local features and kernel parameters."
LOCAL RESPONSES,0.2967032967032967,"Note that what we care about is under what circumstances are more likely to produce large enough
response differences. Though the speciﬁc impact of cl
ij and ml+1
ij
on ∆l+1
i,j
is quite complicated,
we consider statistical circumstances as numerous convolution kernels and various local features are
combined to affect the response differences in real networks. Statistically, the larger amplitude of
ml+1 with ﬁxed xdl, or larger amplitude of xdl with ﬁxed ml+1, tends to produce larger |∆l+1
i,j |.
Besides, if a kernel ml+1 is relatively non-smooth, then non-smooth local features xdl , rather than
smooth local features, are more likely to produce large enough local response differences."
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.30036630036630035,"4.2
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.304029304029304,"Recalling that local properties of adversarial examples in Section 3, we further explore how different
local perturbations affect the models from the locally intermediate response perspective. To illustrate
the effect of perturbations, we assume two convolution kernels with the same mean but different
variances, one is disordered, and the other is smoother. STD ADV"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3076923076923077,Adversarial
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.31135531135531136,"examples
Local perturbations
Kernels"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.315018315018315,Locally-disordered
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.31868131868131866,Locally-consistent
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.32234432234432236,Disordered kernel
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.326007326007326,Smooth kernel
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.32967032967032966,Local response
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3333333333333333,differences 8/255 8/255
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.336996336996337,79/255
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.34065934065934067,0.11/255 … image
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3443223443223443,"-1
-1
-1 -1"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.34798534798534797,"-1
-1
-1 -1
9"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3516483516483517,"1/9
1/9
1/9"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3553113553113553,"1/9
1/9
1/9"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.358974358974359,"1/9
1/9
1/9"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3626373626373626,"8/255
8/255
8/255"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3663003663003663,"8/255
8/255
8/255"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.36996336996337,"8/255
8/255
8/255"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.37362637362637363,"-1/255
8/255
0"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3772893772893773,"-4/255
4/255"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.38095238095238093,"-8/255
-2/255
8/255"
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.38461538461538464,-4/255
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3882783882783883,"Figure 4: Further impact of local perturbations on different convolutional kernels. Shown above is
adversarial perturbations with same amplitude (single channel) act on kernels with different smooth-
ness and then get the local response differences of adversarial example and natural example."
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.39194139194139194,Under review as a conference paper at ICLR 2022
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3956043956043956,"Local responses vs. locally-disordered perturbations. Since adversarial perturbations generated
by standard models tend to be locally-disordered, we discuss these perturbations affect the local
responses. Locally-disordered perturbations indicate relatively drastic local changes among pixels
of potential example, in other words, local input difference xd0 of potential example and natural ex-
ample (also refers to local perturbations) show its great variance. These perturbations are convolved
on numerous kernels. As shown in Figure 4, suppose convolution kernels with the same mean but
greater variance, these locally-disordered perturbations are more likely to yield large enough abso-
lute response differences |∆1
i,j| in the ﬁrst layer, since the non-smooth kernels tend to emphasize
the relationship between different pixels. On the other hand, suppose smoother kernels with rela-
tively small variance, these locally-disordered perturbations tend to show smaller absolute response
differences since smoother kernels tend to focus on the average situation within their local receptive
ﬁelds. That is, locally-disordered perturbations pass through different types of convolution kernels
tend to produce different response differences, which can be accumulated in the subsequent layers."
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.3992673992673993,"Local responses vs. locally-consistent perturbations. Since adversarial perturbations generated
by adversarially robust models tend to be locally-consistent, we count that such perturbations are
more destructive to smoother kernels, comparing with locally-disordered perturbations in Figure 4.
Locally-consistent perturbations xd0 reaching the perturbation bound tend to produce larger absolute
response differences in the ﬁrst layer since they tend to show a larger absolute average, yet locally-
disordered perturbations show smaller response differences due to a smaller absolute average."
FURTHER IMPACT OF LOCAL PERTURBATIONS ON MODELS,0.40293040293040294,"Both different local perturbations have different impacts on the local response differences of poten-
tial examples and natural examples, implying the fragile trend of models and the transferability of
adversarial examples."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4065934065934066,"4.3
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.41025641025641024,"Motivated by adversarially-trained models that tend to show smoother kernels (Wang et al., 2020a),
we ﬁrst provide a further understanding of local responses and then give empirical understandings."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4139194139194139,"Local responses vs. model smoothness. Given a non-smooth convolution kernel ml+1 with great
variance, it is more likely to cause huge impact on an absolute response difference |∆l+1
i,j | within its
local receptive ﬁeld when ﬁxing xdl. Considering numerous non-smooth kernels and various local
features are combined in the same layer, it is quite easy to yield large enough response differences.
More complicated is the subsequent layers act on the current local response differences and get in-
tricate accumulation effects. That is, for a standard model with non-smooth kernels, the model itself
tends to amplify the local response differences between potential examples and natural examples.
On the other hand, since an adversarially-trained model show smoother kernels, the model has a ten-
dency to weaken the local response differences and narrow the ﬁnal responses of potential examples
and natural examples, indicating a trade-off between model robustness and accuracy."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4175824175824176,"Setup. We get both standard and adversarially-trained models in Section 3. Considering complex
functional layers, including linear and nonlinear ones in DNNs, based on Assumption 1, we use the
maximum and the total absolute differences of local responses in some layers to show their effects.
Taking ResNet-18 as an example, we investigate some layers including the ﬁrst convolutional layer
as Conv 1, the feature maps before layer 1 as Layer 0, and Layer 1 to Layer 4 respectively1."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.42124542124542125,"To verify above analysis, we ﬁrst compare the local response differences of potential examples and
natural examples between standard and robust models. As Table 1 shows, the standard model ex-
hibits signiﬁcantly larger differences in local responses layer by layer, especially from the perspec-
tive of total differences yielded by per pairs. These large enough response differences accumulate,
making the model’s cognition of potential examples far away from their natural ones and ultimately
leading the model to express a high vulnerability. On the other hand, the adversarially-trained model
signiﬁcantly shortens local response differences in corresponding layers, yet still some differences
exist. In other words, how to further reduce the local response differences is a perspective of ap-
proaching model robustness to performance. We also note that, due to the complex effect of nonlin-
ear layers (e.g., BatchNorm, ReLu), the maximum absolute difference of local responses does not
strictly increase monotonically but increases in trend."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4249084249084249,"1Feature map size of Conv 1, Layer 0 and Layer 1: 64 × 32 × 32, Layer 2: 128 × 16 × 16, Layer 3:
256 × 8 × 8, and Layer 4: 512 × 4 × 4."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.42857142857142855,Under review as a conference paper at ICLR 2022
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.43223443223443225,"Table 1: The absolute difference of local responses (per image) between test set images and their
potentially adversarial examples on different layers. Left/Right denote standard/robust model."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4358974358974359,"Model
Conv 1
Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.43956043956043955,"Max
0.12/0.05
0.30/0.15
1.92/0.52
1.46/0.36
1.45/0.41
3.72/1.02
Total
370/556
443/489
4276/1246
2093/430
581/164
2624/526"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4432234432234432,"Local responses vs. destructive effect. Naturally, we wonder whether a clear difference between
potential examples of successful and failed attacks on a certain model exists. We ﬁrst select the
natural examples that are classiﬁed correctly, and count their potential examples of successful and
failed attacks. As Table 2 shows1, for the standard model, the successful ones show similar differ-
ences with the failed ones in the front layers, but greater differences especially in Layer 4 close to
the outputs, leading to ﬁnally destructive effects. However, for the adversarially-trained model in
Table 4 (Appendix C), similar differences even in Layer 4 may be due to the smoother kernels and
locally smoother perturbations, leading to the ﬁnal responses of both close to natural examples’."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4468864468864469,Table 2: Left/Right denote successfully-attacked/failed adversarial examples on standard model.
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.45054945054945056,"Model
Conv 1
Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4542124542124542,"Max
0.16/0.15
0.379/0.378
2.50/2.51
1.38/1.37
1.16/1.08
2.11/1.38
Total
558/554
676/669
5955/6001
2363/2339
522/494
1481/757"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.45787545787545786,Table 3: Left/Right denote original/transferred adversarial examples on adversarially-trained model.
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.46153846153846156,"Model
Conv 1
Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4652014652014652,"Max
0.05/0.04
0.15/0.12
0.52/0.44
0.36/0.14
0.41/0.08
1.02/0.10
Total
556/321
489/302
1246/767
430/138
164/38
526/57"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.46886446886446886,"Local responses vs. transferability. Different local perturbations related to models imply the difﬁ-
culty of adversarial examples’ transferability, then we further explore whether the transferability can
be understood from the locally intermediate response perspective. To verify the analysis of Section
4.2, we exchange the adversaries obtained from the standard and robust model. Table 3 shows,
locally-disordered perturbations, combined with a smoother adversarially-trained model, exhibit
fairly small local response differences layer by layer, making the model’s cognition close enough to
the natural examples and leading to 83.6% robustness close enough to 84.9% model performance.
Similar situations occur when locally-consistent perturbations combined with a non-smooth stan-
dard model in Table 5 (Appendix C). These results indicate that the searched perturbations related to
models tend to amplify response differences as much as possible to enlarge the model’s cognition of
potential examples and natural examples, and the weak transferability of adversarial examples may
be due to transferred perturbations that are difﬁcult to enlarge the model’s cognition."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4725274725274725,"4.4
SMOOTHER KERNELS: ALLEVIATE LOCAL RESPONSE DIFFERENCES"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.47619047619047616,"To further exhibit the effect of shortening local response differences, we simply show smoother ad-
versarially robust models can alleviate local response differences and then improve their robustness.
As shown in Figure 5, adversarially-trained models with different smoothness (i.e., larger weight
decay parameters show the smaller magnitude of the kernels (Loshchilov & Hutter, 2019)) show
different local response differences. Figures 5(b)-5(g) for the maximum differences and Figure 9 for
the total differences (Appendix C) indicate that smoother kernels slightly weaken the local response
differences between the potential examples and natural examples layer by layer, and ﬁnally tend to
narrow the robustness and performance in Figure 5(h). This to some extent explains adversarially-
trained models are more sensitive to weight decay (Pang et al., 2021). On the other hand, we ﬁnd
that the increase of weight decay is quite difﬁcult to further reduce the magnitude of parameters
and the local response differences in each layer, which may be one of the reasons for the current
bottleneck in the robustness of the adversarial training. Besides, the increase of weight decay can
effectively weaken the robust overﬁtting (Rice et al., 2020) in Figure 5(h) and Figure 9(g)."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.47985347985347987,"1Note that under the PGD-20 attack, the standard model hardly yields examples of failed attacks, then we
use the FGSM attack to illustrate the problem."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4835164835164835,Under review as a conference paper at ICLR 2022
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.48717948717948717,"0
25
50
75
100
Epochs 2000 4000"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4908424908424908,Quadratic Sum of Parameters
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4945054945054945,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.4981684981684982,(a) Parameters
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5018315018315018,"0
25
50
75
100
Epochs 0.1 0.2 0.3"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5054945054945055,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5091575091575091,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5128205128205128,(b) Conv 1
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5164835164835165,"0
25
50
75
100
Epochs 0.2 0.4 0.6"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5201465201465202,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5238095238095238,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5274725274725275,(c) Layer 0
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5311355311355311,"0
25
50
75
100
Epochs 0.5 1.0 1.5"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5347985347985348,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5384615384615384,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5421245421245421,(d) Layer 1
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5457875457875457,"0
25
50
75
100
Epochs 0.5 1.0 1.5"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5494505494505495,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5531135531135531,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5567765567765568,(e) Layer 2
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5604395604395604,"0
25
50
75
100
Epochs 0.5 1.0 1.5 2.0"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5641025641025641,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5677655677655677,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5714285714285714,(f) Layer 3
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.575091575091575,"0
25
50
75
100
Epochs 0.50 0.75 1.00 1.25"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5787545787545788,Maximum Differences
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5824175824175825,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5860805860805861,(g) Layer 4
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5897435897435898,"0
25
50
75
100
Epochs 40 60 80"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5934065934065934,Test Accuracy (%)
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.5970695970695971,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.6007326007326007,(h) Test Accuracy
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.6043956043956044,"Figure 5: The maximum local response differences on robust models with different smoothness. (a)
shows different model smoothness affected by weight decay, (b)-(g) denote the impact of weight
decay on the local response differences, and (h) shows the model robustness and performance."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.608058608058608,"4.5
DISCUSSION: LOCAL RESPONSES AND MODEL ROBUSTNESS"
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.6117216117216118,"The above suggests that the model robustness is related to the model itself and the property of
potential examples, while they are combined through the local responses. Due to enough differences
in local responses, some potential examples are not regarded as similar to natural examples (or their
predictions are different from the natural ones with correct predictions), so they eventually show
their destructive effects as adversarial examples. That is, if a model tends to weaken the local
response differences of potential and natural examples, then the model exhibits great robustness as
the ﬁnal responses of the potential examples are more likely to be close to the natural ones."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.6153846153846154,"On the other hand, though small differences of local responses make the network more inclined to
treat both examples as the same categories, but the existing differences, especially the intricate dif-
ferences after multi-layer accumulation emphasize the difﬁculty of approaching model robustness
to performance, and demonstrate that DNN models are naturally fragile. Essentially, these response
differences come from whether a non-zero convolution kernel acts on all legal perturbations in
boundary B to obtain differences that are all zeros or sufﬁciently small without further accumula-
tion. In other words, it tells the model robustness is that, given legal potential examples from any
attacks, the accumulated response differences can be alleviated or removed. For instance, feature
denosing (Xie et al., 2019) and activation suppressing (Bai et al., 2021) can be viewed as shortening
local response differences of both examples to improve the model robustness."
EMPIRICAL UNDERSTANDING OF LOCAL RESPONSES,0.6190476190476191,"Besides, shortening local response differences does not directly mean an improvement of the model’s
robustness, in fact, it expresses the closeness of the model’s cognition on both examples. That is,
a poorly-trained model may also treat both as relatively close, but give a bad model performance.
To further improve the model robustness, a more realistic idea is to ﬁnd proper parameters (whether
theoretical parameters to minimize the local response differences exist) or a new method (nonlinear
layers, loss functions, model structures) to shorten the response differences as much as possible,
while not overly weaken the classiﬁcation performance of the model."
CONCLUSION,0.6227106227106227,"5
CONCLUSION"
CONCLUSION,0.6263736263736264,"In this paper, we investigate the local properties of adversarial examples generated by different mod-
els and rethink the vulnerability of DNNs from a novel locally intermediate response perspective. We
ﬁnd that the high-frequency components of adversarial examples tend to mislead standard DNNs,
but have little impact on adversarially-trained models. Furthermore, locally-disordered perturbations
are shown on standard models, but locally-consistent perturbations on adversarially-trained models.
Both explorations emphasize the local perspective and the potential relationship between models
and adversarial examples, then we explore how different local perturbations affect the models. We
demonstrate DNN models are naturally fragile at least for large enough local response differences
between potentially adversarial examples and natural examples, and empirically show smoother
adversarially-trained models can alleviate local response differences to improve robustness."
CONCLUSION,0.63003663003663,Under review as a conference paper at ICLR 2022
REFERENCES,0.6336996336996337,REFERENCES
REFERENCES,0.6373626373626373,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML. 2018."
REFERENCES,0.6410256410256411,"Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang.
Improving
adversarial robustness via channel-wise activation suppressing. In ICLR. 2021."
REFERENCES,0.6446886446886447,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy (SP). 2017."
REFERENCES,0.6483516483516484,"Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack. In
KDD. 2020."
REFERENCES,0.652014652014652,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML. 2020."
REFERENCES,0.6556776556776557,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL. 2019."
REFERENCES,0.6593406593406593,"Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classiﬁers’ robustness to adversarial
perturbations. In Machine Learning. 2018."
REFERENCES,0.663003663003663,"Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. Efﬁ-
cient and accurate estimation of lipschitz constants for deep neural networks. In NeurIPS. 2019."
REFERENCES,0.6666666666666666,"Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu, Martin Watten-
berg, and Ian Goodfellow. Adversarial spheres. In arXiv preprint arXiv:1801.02774. 2018."
REFERENCES,0.6703296703296703,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR. 2015."
REFERENCES,0.673992673992674,"Paula Harder, Franz-Josef Pfreundt, Margret Keuper, and Janis Keuper. Spectraldefense: Detecting
adversarial attacks on cnns in the fourier domain. In arXiv preprint arXiv:2103.03000. 2021."
REFERENCES,0.6776556776556777,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR. 2016."
REFERENCES,0.6813186813186813,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In NeurIPS. 2019."
REFERENCES,0.684981684981685,"Malhar Jere, Maghav Kumar, and Farinaz Koushanfar. A singular value perspective on model ro-
bustness. In arXiv preprint arXiv:2012.03516. 2020."
REFERENCES,0.6886446886446886,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.6923076923076923,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In
ICLR. 2017."
REFERENCES,0.6959706959706959,"Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In CVPR. 2018."
REFERENCES,0.6996336996336996,Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR. 2019.
REFERENCES,0.7032967032967034,"Divyam Madaan and Sung Ju Hwang. Adversarial neural pruning with latent vulnerability suppres-
sion. In ICML. 2020."
REFERENCES,0.706959706959707,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR. 2018."
REFERENCES,0.7106227106227107,"Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial
training. In ICLR. 2021."
REFERENCES,0.7142857142857143,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as
a defense to adversarial perturbations against deep neural networks.
In IEEE Symposium on
Security and Privacy (SP). 2016."
REFERENCES,0.717948717948718,Under review as a conference paper at ICLR 2022
REFERENCES,0.7216117216117216,"Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann.
Fixing data augmentation to improve adversarial robustness.
In arXiv preprint
arXiv:2103.01946. 2021."
REFERENCES,0.7252747252747253,"Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang,
Meng Zhou, Jie Shi, and Quanshi Zhang. Towards a uniﬁed game-theoretic view of adversarial
perturbations and robustness. In arXiv preprint arXiv:2103.07364. 2021."
REFERENCES,0.7289377289377289,"Leslie Rice, Eric Wong, and Zico Kolter. Overﬁtting in adversarially robust deep learning. In ICML.
2020."
REFERENCES,0.7326007326007326,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. In NeurIPS. 2018."
REFERENCES,0.7362637362637363,"Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In SIGSAC. 2016."
REFERENCES,0.73992673992674,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR. 2014."
REFERENCES,0.7435897435897436,"Thomas Tanay and Lewis Grifﬁn. A boundary tilting persepective on the phenomenon of adversarial
examples. In arXiv preprint arXiv:1608.07690. 2016."
REFERENCES,0.7472527472527473,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In ICLR. 2019."
REFERENCES,0.7509157509157509,"Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain
the generalization of convolutional neural networks. In CVPR. 2020a."
REFERENCES,0.7545787545787546,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassiﬁed examples. In ICLR. 2020b."
REFERENCES,0.7582417582417582,"Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
In ICLR. 2018."
REFERENCES,0.7619047619047619,"Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In CVPR. 2019."
REFERENCES,0.7655677655677655,"Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan, Chuang Gan, and Xue
Lin. Interpreting adversarial examples by activation promotion and suppression. In arXiv preprint
arXiv:1904.02057. 2019."
REFERENCES,0.7692307692307693,"Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, and Justin Gilmer. A fourier
perspective on model robustness in computer vision. In NeurIPS. 2019."
REFERENCES,0.7728937728937729,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML. 2019."
REFERENCES,0.7765567765567766,"Tianyuan Zhang and Zhanxing Zhu. Interpreting adversarially trained convolutional neural net-
works. In ICML. 2018."
REFERENCES,0.7802197802197802,Under review as a conference paper at ICLR 2022
REFERENCES,0.7838827838827839,"A
THE DESTRUCTIVENESS OF ADVERSARIAL EXAMPLES ON FREQUENCY
DOMAIN"
REFERENCES,0.7875457875457875,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.7912087912087912,Test Accuracy on Filtered Images
REFERENCES,0.7948717948717948,"natural examples
adversarial examples"
REFERENCES,0.7985347985347986,(a) low-frequency components (STD)
REFERENCES,0.8021978021978022,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency scale 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.8058608058608059,Test Accuracy on Filtered Images
REFERENCES,0.8095238095238095,"natural examples
adversarial examples"
REFERENCES,0.8131868131868132,(b) low-frequency components (ADV)
REFERENCES,0.8168498168498168,"Figure 6: The destructiveness of only high-frequency components from natural and adversarial ex-
amples on both standard (STD) and adversarially-trained (ADV) models. Shown above are well-
trained models tested with images through high-pass ﬁlter."
REFERENCES,0.8205128205128205,"Here, we further investigate the contribution of only high-frequency components to the destructive-
ness of both examples. We get both standard and adversarially-trained models as above. Figure
6 illustrates the trend of model performance and robustness on the test set with the low-frequency
components decreased (the increase of the ﬁltering scale denotes that the less low-frequency com-
ponents are added to the ﬁltered images). As the increase of ﬁltering scale, only natural examples on
standard models keep certain classiﬁcation performance and then decrease to reach 10% (the accu-
racy of random classiﬁcation). That is, the high-frequency components of natural examples to some
extent can promote classiﬁcation. On the other hand, the high-frequency components of adversarial
examples on standard models show almost no promotion but destructive effect, and ﬁnally reach
10% as well. For robust models, the high-frequency components of both examples show similar but
little performance."
REFERENCES,0.8241758241758241,"B
THE LOCAL PROPERTIES OF ADVERSARIAL EXAMPLES ON SPATIAL
DOMAIN"
REFERENCES,0.8278388278388278,"We further explore the local properties of adversarial examples generated by the FGSM attack. For
adversarially-trained models, compared with the PGD attack in Figure 3, the adversarial examples
generated by the FGSM attack in Figure 7 show similar perturbations and similar model robust-
ness (51.9% robustness for PGD-20 and 57.4% robustness for FGSM), yet the latter produces less
detailed perburbations. For standard models, though both attacks show locally-disordered perturba-
tions, the latter exhibits less disordered perturbations since less perturbation values can be achieved,
i.e., +8/255, 0, and -8/255."
REFERENCES,0.8315018315018315,"(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)"
REFERENCES,0.8351648351648352,"Figure 7: Visualisation of adversarial perturbations (FGSM) generated by Left: adversarially-trained
(ADV) model and Right: standard (STD) model."
REFERENCES,0.8388278388278388,Under review as a conference paper at ICLR 2022
REFERENCES,0.8424908424908425,"Similar to adversarial examples of successful attacks in Figure 3, the failed attacks in Figure 8 show
locally-consistent perturbations related to image shapes on the adversarially-trained models as well.
The difference is that, these examples show less misleading local perturbations since the shape of
their perturbations is closer to the shape of the original image."
REFERENCES,0.8461538461538461,"(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)"
REFERENCES,0.8498168498168498,"Figure 8: Visualisation of adversarial perturbations (PGD) generated by Left: adversarially-trained
(ADV) model and Right: standard (STD) model. Shown above are all unsuccessfully attacked on
ADV model."
REFERENCES,0.8534798534798534,"C
LOCAL RESPONSES"
REFERENCES,0.8571428571428571,"As mentioned in Section 4.3, we report destructive effects on the adversarially-trained model in
Table 4. We ﬁrst select the natural examples that are classiﬁed correctly, and count their potential
examples of successful and failed attacks. Similar to the standard model, the successful ones show
similar differences with the failed ones in the front layers, but greater differences in Layer 4 close to
the outputs. The difference is that, due to the smoother kernels and locally smoother perturbations,
the robust model exhibits a closer local response differences in Layer 4 between the successful ones
and the failed ones. Besides, compared with the PGD attack, the adversarial examples generated by
the FGSM attack show similar differences in the front layers, but less local response differences in
Layer 4, leading to higher model robustness."
REFERENCES,0.8608058608058609,"Table 4: The absolute difference of local responses (per image) between test set images and their
potentially adversarial examples on different layers. Left/Right denote successfully-attacked/failed
adversarial examples on adversarially-trained model respectively."
REFERENCES,0.8644688644688645,"Model
Conv 1
Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
REFERENCES,0.8681318681318682,"Max (PGD-20)
0.048/0.047
0.15/0.15
0.51/0.53
0.36/0.37
0.40/0.42
1.10/0.99
Total (PGD-20)
557/555
488/490
1264/1242
445/420
169/160
578/491
Max (FGSM)
0.048/0.047
0.16/0.15
0.53/0.54
0.35/0.36
0.37/0.38
0.90/0.86
Total (FGSM)
580/576
510/511
1292/1272
436/414
161/153
478/428"
REFERENCES,0.8717948717948718,"We further report the transferability of adversarial examples on the standard model, as shown in
Table 5. We get the potentially adversarial examples obtained from the robust model to attack the
standard model. That is, locally-consistent perturbations, combined with a non-smooth standard
model, exhibit small local response differences layer by layer, making the model’s cognition close
enough to the natural examples and leading to 78.4% robustness close to 94.4% model performance."
REFERENCES,0.8754578754578755,"Table 5: The absolute difference of local responses (per image) between test set images and their po-
tentially adversarial versions on different layers of ResNet-18. Left/Right denote original/transferred
adversarial examples on standard model respectively."
REFERENCES,0.8791208791208791,"Model
Conv 1
Layer 0
Layer 1
Layer 2
Layer 3
Layer 4"
REFERENCES,0.8827838827838828,"Max
0.12/0.15
0.30/0.29
1.92/0.85
1.46/0.88
1.45/0.67
3.72/0.88
Total
370/532
443/509
4276/2639
2093/1157
581/280
2624/525"
REFERENCES,0.8864468864468864,Under review as a conference paper at ICLR 2022
REFERENCES,0.8901098901098901,"As mentioned in Section 4.4, Figure 9 is a supplement to Figure 5 from the total absolute local
response differences perspective, which indicates that smaller local response differences tend to
approach model robustness to performance as well."
REFERENCES,0.8937728937728938,"0
25
50
75
100
Epochs 1000 2000"
REFERENCES,0.8974358974358975,Total Differences
REFERENCES,0.9010989010989011,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9047619047619048,(a) Conv 1
REFERENCES,0.9084249084249084,"0
25
50
75
100
Epochs 1000 2000"
REFERENCES,0.9120879120879121,Total Differences
REFERENCES,0.9157509157509157,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9194139194139194,(b) Layer 0
REFERENCES,0.9230769230769231,"0
25
50
75
100
Epochs 2000 4000 6000"
REFERENCES,0.9267399267399268,Total Differences
REFERENCES,0.9304029304029304,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9340659340659341,(c) Layer 1
REFERENCES,0.9377289377289377,"0
25
50
75
100
Epochs 1000 2000 3000 4000"
REFERENCES,0.9413919413919414,Total Differences
REFERENCES,0.945054945054945,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9487179487179487,(d) Layer 2
REFERENCES,0.9523809523809523,"0
25
50
75
100
Epochs 0 1000 2000"
REFERENCES,0.9560439560439561,Total Differences
REFERENCES,0.9597069597069597,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9633699633699634,(e) Layer 3
REFERENCES,0.967032967032967,"0
25
50
75
100
Epochs 200 300 400 500"
REFERENCES,0.9706959706959707,Total Differences
REFERENCES,0.9743589743589743,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.978021978021978,(f) Layer 4
REFERENCES,0.9816849816849816,"0
25
50
75
100
Epochs 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.9853479853479854,Robust Loss
REFERENCES,0.989010989010989,"weight decay 1e-4
weight decay 3e-4
weight decay 5e-4
weight decay 7e-4
weight decay 10e-4"
REFERENCES,0.9926739926739927,(g) Robust Loss
REFERENCES,0.9963369963369964,"Figure 9: The total absolute local response differences (per image) of some layers on adversarially-
trained models with different smoothness. Among them, (a)-(f) denote the inﬂuence of different
weight decay parameters on the local response differences in each layer, and (g) shows the robust
loss of training set and test set."
