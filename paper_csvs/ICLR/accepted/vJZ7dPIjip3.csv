Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002849002849002849,"End-to-end (geometric) deep learning has seen ﬁrst successes in approximating
the solution of combinatorial optimization problems. However, generating data in
the realm of NP-hard/-complete tasks brings practical and theoretical challenges,
resulting in evaluation protocols that are too optimistic. Speciﬁcally, most datasets
only capture a simpler subproblem and likely suffer from spurious features. We
investigate these effects by studying adversarial robustness–a local generalization
property–to reveal hard, model-speciﬁc instances and spurious features. For this
purpose, we derive perturbation models for SAT and TSP. Unlike in other ap-
plications, where perturbation models are designed around subjective notions of
imperceptibility, our perturbation models are efﬁcient and sound, allowing us to
determine the true label of perturbed samples without a solver. Surprisingly, with
such perturbations, a sufﬁciently expressive neural solver does not suffer from the
limitations of the accuracy-robustness trade-off common in supervised learning.
Although such robust solvers exist, we show empirically that the assessed neural
solvers do not generalize well w.r.t. small perturbations of the problem instance."
INTRODUCTION,0.005698005698005698,"1
INTRODUCTION"
INTRODUCTION,0.008547008547008548,"Combinatorial Optimization covers some of the most studied computational problems.
Well-
known examples are the NP-complete SATisﬁability problem for boolean statements or the Trav-
eling Salesperson Problem (TSP). These problems can be solved efﬁciently with approximate
solvers that have been crafted over the previous decades (Festa, 2014). As an alternative to en-
gineered, application-speciﬁc heuristics, learning seems to be a good candidate (Bengio et al., 2021)"
INTRODUCTION,0.011396011396011397,"Figure 1: Adversarial examples can enhance the
coverage of problem space X and ﬁnd model-
speciﬁc regions with difﬁcult examples.
Left,
shows the asymptotic coverage of an efﬁcient,
incomplete data generator (Yehuda et al., 2020).
Right, shows the important case (X is large) of a
sparse sample of a possibly complete generator."
INTRODUCTION,0.014245014245014245,"and was studied as a component in traditional
solvers (e.g. Haim & Walsh (2009)). Despite
deep learning for combinatorial optimization
gaining attention recently, it is still an open
question if and to what extent deep learning can
effectively approximate NP-hard problems."
INTRODUCTION,0.017094017094017096,"Moreover, there is a “Catch-22“; even if neural
networks could solve NP-hard problems, gen-
erating the training data is either (a) incomplete
but efﬁcient or (b) complete but inefﬁcient (or
approximate). An incomplete data generator (a)
crafts the problem instances s.t. their labels are
known and a complete data generator (b) ob-
tains the labels via (approximately) solving the
random instances. Additionally, a dense sample
is intractable even for moderate problem sizes
due to the large problem space X."
INTRODUCTION,0.019943019943019943,∗equal contribution
INTRODUCTION,0.022792022792022793,Published as a conference paper at ICLR 2022
INTRODUCTION,0.02564102564102564,"For the special case of (a) exact polynomial-time data-generators, Yehuda et al. (2020) detail two
challenges as a result of the NP-hardness: (1) Easier subproblem: a dataset from an efﬁcient data
generator only captures a strictly easier subproblem. (2) Spurious features: due to the lack of
completeness, the resulting dataset can be trivially solvable due to superﬁcial/spurious features.
These ﬁndings extrapolate to case (b) for a sufﬁciently sparse sample (i.e. data could have been
generated by an efﬁcient data generator). Thus, it is concerning that often the same (potentially
ﬂawed) data generator is used for training and evaluation."
INTRODUCTION,0.02849002849002849,"Adversarial robustness is a challenging local generalization property that offers a way of ﬁxing the
too optimistic model performance estimates that are the result of evaluations on incomplete datasets.
We illustrate this in Fig. 1 for two relevant data generation schemes: (a) to the left we discuss an
incomplete, efﬁcient data generator and (b) to the right we discuss a sparse sample of a potentially
complete data generator. With a suitable choice of perturbation model, we (a) possibly extend the
reachable space by an efﬁcient data generator or (b) cover the space around each instance in the
sparse dataset. In other words, such adversarial attacks aim to ﬁnd the intersection of the uncovered
regions and hard, model-speciﬁc samples. Therefore, it is possible to detect the described defects.
Adversarial robustness more realistically evaluates a model’s generalization ability instead of simply
testing on the same data generation procedure or a sparse external dataset."
INTRODUCTION,0.03133903133903134,"Adversarial robustness is a desirable property for neural combinatorial optimization, since, in con-
trast to general learning tasks, in combinatorial optimization we do not have an accuracy robustness
trade-off in the sense of Suggala et al. (2019). This means, there exists a model with high accuracy
and high robustness. One key factor to avoid the accuracy robustness trade-off is choosing a pertur-
bation model that guarantees the correct label of the perturbed sample (we call them sound). This is
in stark contrast to other domains where one relies on imperceptible perturbations."
INTRODUCTION,0.03418803418803419,"We instantiate our adversarial attack framework for the NP-complete SAT and TSP. Nevertheless,
most of the principles can be transferred to other combinatorial optimization problems. Note that
often such problems can be even reduced onto one another, as is the case for e.g. SAT and the
maximum independent set. Having said this, we select SAT because of its general applicability and
the notoriously challenging TSP due to its practical importance (e.g. for supply chain optimization).
We then use our attacks to show that the evaluated neural SAT and TSP solvers are highly non-robust."
INTRODUCTION,0.037037037037037035,"Contributions. (1) We bring the study of adversarial robustness to the ﬁeld of neural combinatorial
solvers to tackle fundamental problems in the evaluation of neural solvers. (2) We propose pertur-
bation models for SAT and TSP s.t. we efﬁciently determine the updated solution. (3) We show that
the models for SAT and TSP can be easily fooled with small perturbations of the problem instance
and (4) that adversarial training can improve the robustness and generalization."
BACKGROUND ON NEURAL SOLVERS,0.039886039886039885,"2
BACKGROUND ON NEURAL SOLVERS"
BACKGROUND ON NEURAL SOLVERS,0.042735042735042736,"Intuitively, combinatorial optimization is the task of ﬁnding an optimal element from the ﬁnite set
of possible solutions (e.g. the truth assignment for a boolean statement). We formalize this as Y =
arg minY ′∈g(x) c(x, Y ′) where x is a problem instance, g(x) = Y the ﬁnite set of feasible solutions,
and c(·) a cost function. Typically, there is also an associated binary decision problem y, such as
ﬁnding the optimal route vs. checking whether a route of at most cost c0 exists (see § 4 and § 5).
Then, for example, a neural solver ˆy = fθ(x) learns a mapping fθ : X →{0, 1} to approximate
the decision problem. In this work, θ are the parameters, x ∈X is the problem instance, and ˆy (or
ˆY ) the prediction. In case of supervised learning, we then optimize the parameters θ w.r.t. a loss
ℓ(fθ(x), y) over a ﬁnite set of labeled training instances (x, y). However, to obtain the exact labels
y for a given x is intractable for larger problem instances due to the exponential or worse runtime.
Two ways to generate data pairs are mentioned in the introduction and visualized in Fig. 1: (a) an
efﬁcient but incomplete data generator (b) using a solver to obtain the labels for random samples."
ADVERSARIAL ROBUSTNESS,0.045584045584045586,"3
ADVERSARIAL ROBUSTNESS"
ADVERSARIAL ROBUSTNESS,0.04843304843304843,"Adversarial robustness refers to the robustness of a machine learning model to a small perturbation
of the input instance (Szegedy et al., 2014). We deﬁne an adversarial attack in Eq. 1, where the
parameters θ are constant and G denotes the perturbation model that describes the possible perturbed"
ADVERSARIAL ROBUSTNESS,0.05128205128205128,Published as a conference paper at ICLR 2022
ADVERSARIAL ROBUSTNESS,0.05413105413105413,"instances ˜x around the clean sample x (i.e. the perturbation space) given the original solution Y .
Since ˜Y ̸= Y in the general case, we introduce ˜Y = h(˜x, x, Y ) to model how the solution changes."
ADVERSARIAL ROBUSTNESS,0.05698005698005698,"ℓadv,G(x, Y ) = max
˜x
ℓ(fθ(˜x), ˜Y )
s.t.
˜x ∈G(x, Y ) ∧˜Y = h(˜x, x, Y )
(1)"
ADVERSARIAL ROBUSTNESS,0.05982905982905983,"Sound and efﬁcient perturbation model. Our framework for a neural combinatorial solver fθ
stands out from many other works on adversarial robustness since we choose the perturbation model
G s.t. we provably know a solution ˜Y = h(˜x, x, Y ) for all possible ˜x. We call such a perturbation
model sound. This stands in contrast to other domains, where we usually hope to preserve the label
using the subjective concept of imperceptible/unnoticable perturbations (Szegedy et al., 2014)."
ADVERSARIAL ROBUSTNESS,0.06267806267806268,"While we can naively obtain a sound perturbation model for combinatorial optimization using a
solver, this is intractable for realistic problem sizes. We therefore propose to use perturbation models
that are efﬁcient and sound. That is, we can determine the updated solution ˜Y without applying a
solver on the perturbed instance ˜x. For example, if we add a node to a TSP instance, the optimal
route including the new node will change, but we can efﬁciently determine ˜Y for the chosen G."
ADVERSARIAL ROBUSTNESS,0.06552706552706553,"Important technical details arise due to (a) the potentially non-unique Y and (b) non-constant ˜Y
while perturbing the input. One way to handle both effects is through the choice of the loss ℓ. (a) We
can deal with the ambiguity in Y if the loss is equal for any two optimal solutions/predictions. This
can be achieved naturally by incorporating the cost c(Y ) of the combinatorial optimization problem.
(b) Since the solution ˜Y can change throughout the optimization, it is important to choose a loss that
assesses the difference between prediction fθ(˜x) and ground truth ˜Y . For example, a viable loss for
TSP is the optimality gap ℓOG( ˆY , Y ) = |c( ˆY )−c(Y )|/c(Y )) that is normalized by c(Y )."
ADVERSARIAL ROBUSTNESS,0.06837606837606838,"Perturbation strength. With a sound perturbation model, all generated instances ˜x are valid prob-
lem instances regardless of how much they differ from x. Hence, in the context of combinatorial
optimization, the perturbation strength/budget models the severity of a potential distribution shift
between training data and test data. This again highlights the differences to other domains. For ex-
ample in image classiﬁcation with the common Lp perturbation model ∥x −˜x∥p ≤r, the instance
changes its true label or becomes meaningless (e.g. a gray image) for a large enough r."
ADVERSARIAL ROBUSTNESS,0.07122507122507123,"Generalization. Speciﬁcally, adversarial robustness is one way to measure the generalization over
perturbed instances ˜x in the proximity of x. Adversarial robustness is important in the context
of neural combinatorial solvers since training and validation/test distribution differ from the actual
data distribution p(x). First, the data distribution p(x) is typically unknown and highly application-
speciﬁc. Second, due to theoretical limitations of the data generation process the train and valida-
tion/test distribution likely captures a simpler sub-problem suffering from spurious features (Yehuda
et al., 2020). Third, we ultimately desire a general-purpose solver that performs well regardless of
p(x) (in the limits of a polynomial approximation)."
ADVERSARIAL ROBUSTNESS,0.07407407407407407,"We stress that in the context of combinatorial optimization, adversarial examples are neither anoma-
lous nor statistical defects since all generated instances correspond to valid problem instances. In
contrast to other domains, the set of valid problems is not just a low-dimensional manifold in a
high-dimensional space. Thus, the so-called manifold hypothesis (Stutz et al., 2019) does not ap-
ply for combinatorial optimization. In summary, it is critical for neural solvers to perform well on
adversarial examples when striving for generalization."
ADVERSARIAL ROBUSTNESS,0.07692307692307693,"Accuracy robustness trade-off. A trade-off between adversarial robustness and standard general-
ization was reported for many learning tasks (Tsipras et al., 2019). That is, with increasing robust-
ness the accuracy on the test data decreases. Interestingly, with a sound perturbation model and the
purely deterministic labels in combinatorial optimization (the solution is either optimal or not), no
such trade-off exists. Hence, if the model was expressive enough and we had sufﬁcient compute,
there would exist a model with high accuracy and robustness (see § A for more details)."
ADVERSARIAL ROBUSTNESS,0.07977207977207977,"Adversarial training. In adversarial training, we leverage adversarially perturbed instances with the
desire of training a robust model with improved generalization. For this, adversarial attacks reveal
the regions that are both difﬁcult for the model and not covered by training samples (see Fig. 1).
Hence, adversarial training can be understood as a powerful data augmentation using hard model-
speciﬁc samples. Though it is not the main focus of this work, in § 6, we show that adversarial
training can be used to improve the robustness and generalization of a neural combinatorial solver."
ADVERSARIAL ROBUSTNESS,0.08262108262108261,Published as a conference paper at ICLR 2022
ADVERSARIAL ROBUSTNESS,0.08547008547008547,"Remarks on decision problems. For the binary decision problems, we typically are not required to
know ˜Y ; it sufﬁces to know ˜y. Moreover, for such binary problems, we keep the solution constant
y = ˜y, but there also exist practical perturbations that change the label of the decision problem. For
example for SAT, we can add a set of clauses that are false in isolation which makes ˜y = 0."
ADVERSARIAL ROBUSTNESS,0.08831908831908832,"Requirements for neural solvers. We study neural combinatorial solvers fθ that are often a Graph
Neural Network (GNN). We then solve Eq. 1 using different variants of Projected Gradient Descent
(PGD) and therefore assume the model to be differentiable w.r.t. its inputs (see § D). For non-
differentiable models, one can use derivative-free optimization (Yang & Long, 2021)."
SAT,0.09116809116809117,"4
SAT"
SAT,0.09401709401709402,"We ﬁrst introduce the problem as well as notation and then propose the perturbation models (§ 4.1).
Last, we discuss the attacks for a speciﬁc neural solver (§ 4.2)."
SAT,0.09686609686609686,"Problem statement. The goal is to determine if a boolean expression, e.g. (v1∨v2∨¬v3)∧(v1∨v3),
is satisﬁable y = 1 or not y = 0. Here, we represent the boolean expressions in Conjunctive
Normal Form (CNF) that is a conjunction of multiple clauses k(v1, . . . , vn) and each clause is
a disjunction of literals li. Each literal is a potentially negated boolean variable li ∈{¬vi, vi}
and w.l.o.g. we assume that a clause may not contain the same variable multiple times. In our
standard notation, a problem instance x represents such a boolean expression in CNF. A solution
Y ∈{l∗
1, . . . , l∗
n | l∗
i ∈{¬vi, vi}} provides truth assignments for every variable. Hence, in the
example above x = (v1 ∨v2 ∨¬v3)∧(v1 ∨v3), y = 1, and a possible solution is Y = {v1, ¬v2, v3}.
Note that multiple optimal Y exist but for our attacks it sufﬁces to know one."
SOUND PERTURBATION MODEL,0.09971509971509972,"4.1
SOUND PERTURBATION MODEL"
SOUND PERTURBATION MODEL,0.10256410256410256,"We now introduce a sound and efﬁcient perturbation model for SAT which we then use for an
adversarial attack on a neural (decision) SAT solver. Recall that the perturbation model is sound
since we provably obtain the correct label ˜y and it is efﬁcient since we achieve this without using a
solver. Instead of using a solver, we leverage invariances of the SAT problem."
SOUND PERTURBATION MODEL,0.10541310541310542,"Proposition 1 Let x = k1(v1, . . . , vn) ∧. . . km(v1, . . . , vn) be a boolean statement in Conjunctive
Normal Form (CNF) with m clauses and n variables. Then ˜x, a perturbed version of x, has the
same label y = ˜y in the following cases:"
SOUND PERTURBATION MODEL,0.10826210826210826,"• SAT: x is satisﬁable y = 1 with truth assignment Y . Then, we can arbitrarily remove or add
literals in x to obtain ˜x, as long as one literal in Y remains in each clause."
SOUND PERTURBATION MODEL,0.1111111111111111,"• DEL: x is unsatisﬁable y = 0. Then, we can obtain ˜x from x through arbitrary removals of
literals, as long as one literal per clause remains."
SOUND PERTURBATION MODEL,0.11396011396011396,"• ADC: x is unsatisﬁable y = 0. Then, we can arbitrarily remove, add, or modify clauses in x to
obtain ˜x, as long as there remains a subset of clauses that is unsatisﬁable in isolation."
NEURAL SAT SOLVER,0.1168091168091168,"4.2
NEURAL SAT SOLVER"
NEURAL SAT SOLVER,0.11965811965811966,"Selsam et al. (2019) propose NeuroSAT, a neural solver for satisﬁability-problems that uses a
message-passing architecture (Gilmer et al., 2017) on the graph representation of the boolean ex-
pressions. The SAT problem is converted into a bipartite graph consisting of clause nodes and literal
nodes. For each variable there exist two literal nodes; one represents the variable and the other its
negation. If a literal is contained in a clause its node is connected to the respective clause node.
NeuroSAT then recursively updates the node embeddings over the message-passing steps using this
graph, and in the last step, the ﬁnal vote ˆy ∈{0, 1} is aggregated over the literal nodes."
NEURAL SAT SOLVER,0.1225071225071225,"Attacks. We then use these insights to craft perturbed problem instances ˜x guided by the maximiza-
tion of the loss ℓadvG (see Eq. 1). Speciﬁcally, for SAT and DEL, two of admissible perturbations
deﬁned in Proposition 1, we optimize over a subset of edges connecting the literal and clause nodes
where we set the budget ∆relatively to the number of literals/edges in x. For ADC we additionally
concatenate d additional clauses and optimize over their edges obeying ∆but keep the remaining x
constant. If not reported separately, we decide for either DEL and ADC randomly with equal odds."
NEURAL SAT SOLVER,0.12535612535612536,Published as a conference paper at ICLR 2022
NEURAL SAT SOLVER,0.1282051282051282,"L0-PGD. The addition and removal of a limited number of literals is essentially a perturbation with
budget ∆over a set of discrete edges connecting literals and clauses. Similarly to the L0-PGD attack
of Xu et al. (2019), we continuously relax the edges in {0, 1} to [0, 1] during optimization. We then
determine the edge weights via projected gradient descent s.t. the weights are within [0, 1] and that
we obey the budget ∆. After the attack, we use these weights to sample the discrete perturbations in
{0, 1}. In other words, the attack continuously/softly adds as well as removes literals from x during
the attack and afterward we sample the discrete perturbations to obtain ˜x. For additional details
about the attacks, we refer to § E."
TSP,0.13105413105413105,"5
TSP"
TSP,0.1339031339031339,"We ﬁrst introduce the TSP including the necessary notation. Then, we propose a perturbation that
adds new nodes s.t. we know the optimal route afterward (§ 5.1). In § 5.2, we detail the attack for a
neural decision TSP solver and, in § 5.3, we describe the attack for a model that predicts the optimal
TSP route Y ."
TSP,0.13675213675213677,"Problem statement. We are given a weighted graph G = (V, M) that consist of a ﬁnite set of nodes
V as well as edges M ⊆V2 and a weight ω(e) for each possible edge e ∈V2. We use the elements
in V as indices or nodes interchangeably. The goal is then to ﬁnd a permutation σ of the nodes V s.t.
the cost of traversing all nodes exactly once is minimized (i.e. the Hamiltonian path of lowest cost):"
TSP,0.1396011396011396,"σ∗= arg min
σ′∈S c(σ′, G) = arg min
σ′∈S ω(σ′
1(V), σ′
n(V)) + n−1
X"
TSP,0.14245014245014245,"i=1
ω(σ′
i(V), σ′
i+1(V))
(2)"
TSP,0.1452991452991453,"where S is the set of all permutations and n = |V| is the number of nodes. Although multiple σ∗
might exist here it sufﬁces to know one. An important special case is the “metric TSP”, where the
nodes represent coordinates in a space that obeys the triangle inequality (e.g. euclidean distance).
For notational ease, we interchangeably use σ as a permutation or the equivalent list of nodes.
Moreover, we say σ contains edge (I, J) ∈|M| if I and J are consecutive or the ﬁrst and last
element. In our standard notation x = G, Y = σ∗, and the respective decision problem solves the
question if there exist c(σ∗) ≤c0 of at most c0 cost."
SOUND PERTURBATION MODEL,0.14814814814814814,"5.1
SOUND PERTURBATION MODEL"
SOUND PERTURBATION MODEL,0.150997150997151,"Adversarially perturbing the TSP such that we know the resulting solution seems more challenging
than SAT. However, assuming we would know the optimal route σ∗for graph x = G, then under
certain conditions we can add new nodes s.t. we are guaranteed to know the perturbed optimal route
˜σ∗. Note that this does not imply that we are able to solve the TSP in sub-exponential time in the
worst case. We solely derive an efﬁcient special case through leveraging the properties of the TSP."
SOUND PERTURBATION MODEL,0.15384615384615385,"Proposition 2 Let σ∗be the optimal route over the nodes V in G, let Z ̸∈V be an additional node,
and P, Q are any two neighbouring nodes on σ∗. Then, the new optimal route ˜σ∗(including Z) is
obtained from σ∗through inserting Z between P and Q if ̸ ∃(A, B) ∈V2 \ {(P, Q)} with A ̸= B
s.t. ω(A, Z) + ω(B, Z) −ω(A, B) ≤ω(P, Z) + ω(Q, Z) −ω(P, Q)."
SOUND PERTURBATION MODEL,0.15669515669515668,"Corollary 1 We can add multiple nodes to G and obtain the optimal route ˜σ∗as long as the condi-
tion of Proposition 2 (including the other previously added nodes) is fulﬁlled."
SOUND PERTURBATION MODEL,0.15954415954415954,"Corollary 2 For the metric TSP, it is sufﬁcient if the condition of Proposition 2 holds for (A, B) ∈
V2 \ ({(P, Q)} ∪H) with A ̸= B where H denotes the pairs of nodes both on the Convex Hull
H ∈CH(V)2 that are not a line segment of the Convex Hull."
NEURAL DECISION TSP SOLVER,0.1623931623931624,"5.2
NEURAL DECISION TSP SOLVER"
NEURAL DECISION TSP SOLVER,0.16524216524216523,"Prates et al. (2019) propose a GNN (called DTSP) to solve the decision variant of the TSP for an
input pair x = (G, c0) with graph G and a cost c0. DTSP predicts whether there exists a Hamiltonian
cycle in G of cost c0 or less (y = 1 if the cycle exists)."
NEURAL DECISION TSP SOLVER,0.16809116809116809,"Based on our perturbation model, we inject adversarial nodes. For the metric TSP, we determine
their coordinates by maximizing the binary cross-entropy–a continuous optimization problem. This"
NEURAL DECISION TSP SOLVER,0.17094017094017094,Published as a conference paper at ICLR 2022
NEURAL DECISION TSP SOLVER,0.1737891737891738,"is easy to generalize to the non-metric TSP (omitting Corollary 2), if e.g. the triangle equality does
not hold or there is no “simple” cost function concerning the node’s coordinates/attributes. Then,
the optimization is performed over the edge weights, but depending on what the weights represent
we might need to enforce further requirements."
NEURAL DECISION TSP SOLVER,0.17663817663817663,"Unfortunately, the constraint in Proposition 2 is non-convex and it is also not clear how to ﬁnd a
relaxation that is still sufﬁciently tight and can be solved in closed form. For this reason, when the
constraint for a node is violated, we use vanilla gradient descent with the constraint as objective:
ω(P, Z) + ω(Q, Z) −ω(P, Q) −[minA,B ω(A, Z) + ω(B, Z) −ω(A, B)]. This penalizes if a
constraint is violated. We stop as soon as the node fulﬁlls the requirement/constraint again and limit
the maximum number of iterations to three. Since some adversarial nodes might still violate the
constraint after this projection, we only include valid nodes in each evaluation of the neural solver.
Moreover, for optimizing over multiple adversarial nodes jointly and in a vectorized implementation,
we assign them an order and also consider previous nodes while evaluating the constraint. Ordering
the nodes allows us to parallelize the constraint evaluation for multiple nodes, despite the sequential
nature, since we can ignore the subsequent nodes."
NEURAL TSP SOLVER,0.1794871794871795,"5.3
NEURAL TSP SOLVER"
NEURAL TSP SOLVER,0.18233618233618235,"Joshi et al. (2019) propose a Graph Convolutional Network (ConvTSP) to predict which edges of
the euclidean TSP graph are present in the optimal route. The probability map over the edges is then
decoded into a permutation over the nodes via a greedy search or beam search. We use the same
attack as for the TSP decision problem (see § 5.2) with the exception of having a different objective
with changing label ˜Y . Although the optimality gap ℓ( ˆY , Y ) = c( ˆY )−c(Y )/c(Y ) is a natural choice
and common in the TSP literature (Kool et al., 2019), it proved to be tough to backpropagate through
the decoding of the ﬁnal solution from the soft prediction. Hence, for ConvTSP we maximize the
cross-entropy over the edges. Hence, we perturb the input s.t. the predicted route is maximally
different from the optimal solution ˜Y and then report the optimality gap."
EMPIRICAL RESULTS,0.18518518518518517,"6
EMPIRICAL RESULTS"
EMPIRICAL RESULTS,0.18803418803418803,"In this section, we show that the assessed SAT and TSP neural solvers are not robust w.r.t. small
perturbations of the input using the sound perturbation models introduced in § 4 and 5. We ﬁrst
discuss SAT in § 6.1 and then TSP in § 6.2. We use the published hyperparameters by the respective
works for training the models. We run the experiments for at least ﬁve randomly selected seeds.
We compare the accuracy on the clean and perturbed problem instances (i.e. clean vs. adversarial
accuracy). Since no directly applicable prior work exists, we compare to the random baseline that
randomly selects the perturbation s.t. the budget is exhausted. Moreover, we use Adam (Kingma &
Ba, 2015) and early stopping for our attacks. For further details we refer to § E and § F as well as
the code https://www.daml.in.tum.de/robustness-combinatorial-solvers."
EMPIRICAL RESULTS,0.1908831908831909,"Clean
SAT 0.2 0.4 0.6 0.8"
EMPIRICAL RESULTS,0.19373219373219372,Accuracy
EMPIRICAL RESULTS,0.19658119658119658,"Rand
Adv"
EMPIRICAL RESULTS,0.19943019943019943,"Clean
ADC
DEL 0.7 0.8 0.9 1.0"
EMPIRICAL RESULTS,0.2022792022792023,(a) SAT 10-40
EMPIRICAL RESULTS,0.20512820512820512,"Clean
SAT
0.0 0.2 0.4 0.6"
EMPIRICAL RESULTS,0.20797720797720798,Accuracy
EMPIRICAL RESULTS,0.21082621082621084,"Clean
ADC
DEL 0.4 0.6 0.8 1.0"
EMPIRICAL RESULTS,0.21367521367521367,"Rand
Adv"
EMPIRICAL RESULTS,0.21652421652421652,(b) SAT 50-100
EMPIRICAL RESULTS,0.21937321937321938,"Figure 2: Efﬁcacy of adversarial attacks as introduced in § 4 and assessment of (un)robustness of
the NeuroSAT model on the 10-40 dataset. Recall that SAT is the attack on the satisﬁable problem
instances and ADC as well as DEL are the attacks on unsatisﬁable problem instances."
EMPIRICAL RESULTS,0.2222222222222222,Published as a conference paper at ICLR 2022
SAT,0.22507122507122507,"6.1
SAT"
SAT,0.22792022792022792,"1
5
50
Attack Steps 0.25 0.50 0.75"
SAT,0.23076923076923078,Accuracy
SAT,0.2336182336182336,"Figure 3: SAT Attack on
NeuroSAT"
SAT,0.23646723646723647,"Setup.
Following
Selsam et al. (2019), we train NeuroSAT for 60
epochs using the ofﬁcial parameters and data generation. The random
data generator for the training/validation data greedily adds clauses until
the problem becomes unsatisﬁable which is determined by an exact SAT
solver (Ignatiev et al., 2018; S¨orensson & Een, 2005). We are then left
with an unsatisﬁable problem instance and a satisﬁable problem instance
if we omit the last clause (i.e. the dataset is balanced). For each instance
pair, the number of variables is sampled uniformly within a speciﬁed
range and then the number of literals in each clause is drawn from a geometric distribution. We
name the dataset accordingly to the range of numbers of variables. For example, the training set
10-40 consists of problem instances with 10 to 40 variables. For our attacks, we use the budgets of
∆DEL = 5% as well as ∆SAT = 5% relatively to the number of literals in x and for ADC we add
an additional 25% of clauses and enforce the average number of literals within the new clauses."
SAT,0.23931623931623933,"0.01
0.05
0.10
Ratio of pert. literals 0.25 0.50 0.75"
SAT,0.24216524216524216,Accuracy
SAT,0.245014245014245,"Adv
Rand"
SAT,0.24786324786324787,"Figure 4:
Rob. on satisﬁable
problems (SAT) over budgets ∆."
SAT,0.25071225071225073,"Attack efﬁcacy and model (un)robustness. From the results of
our adversarial attacks presented in Fig. 2 it is apparent that the
studied model NeuroSAT is not robust w.r.t. small perturbation of
the input. Additionally, Fig. 3 shows that for the SAT attack, one
gradient update step already sufﬁces to decrease the accuracy to
26% (see also § I). All this shows the efﬁcacy of our attacks and
perturbation model, it also highlights that the standard accuracy
gives a too optimistic impression of the model’s performance.
We hypothesize that the model likely suffers from challenges (1)
easier subproblem and/or (2) spurious features, while it is also
possible that the fragility is due to a lack of expressiveness."
SAT,0.2535612535612536,"0
10
20
Message passing steps 0.5 1.0"
SAT,0.2564102564102564,Probability SAT
SAT,0.25925925925925924,"Clean
Pert."
SAT,0.2621082621082621,"Figure 5: NeuroSAT’s prediction
over the message passing steps."
SAT,0.26495726495726496,"Difﬁculty imbalance. It is much harder for the model to spot
satisﬁable instances than unsatisﬁable ones. This is apparent
from the clean accuracy and even more obvious from the ad-
versarial accuracy. Even with moderate budgets, we are able to
lower the adversarial accuracy to values below 20% for satis-
ﬁable instances while for unsatisﬁable instances we barely get
below 50% even on the larger problem instances 50-100. An
intuitive explanation is given by the fact that it is impossible
to ﬁnd a solution for an unsatisﬁable instance (and it is cheap
to verify a candidate solution). Similarly, Selsam et al. (2019)
hypothesize that NeuroSAT only changes its prediction if it ﬁnds a solution. Thus, it is even more
remarkable how we are still able to fool the neural solver in 30% of the cases (DEL attack)."
SAT,0.2678062678062678,"Qualitative insights. We show in Fig. 5 how NeuroSAT’s decision evolves over the message-
passing steps for satisﬁable instances. NeuroSAT comes to its conclusion typically after 15 message-
passing steps for the clean samples. For the perturbed samples, NeuroSAT almost never comes to
the right conclusion. For the instances where NeuroSAT predicts the right label, it has a hard time
doing so since it converges slower. For a speciﬁc adversarial example see § J."
SAT,0.2706552706552707,"Table 1: Accuracy comparison of regular train-
ing with a 10% larger training set and adversar-
ial ﬁnetuning of 10 extra epochs (17%)."
SAT,0.27350427350427353,"Data
Regular
Extra data
Adv. train. 10-40"
SAT,0.27635327635327633,"Train
89.0 ± 0.06
89.1 ± 0.05
88.8 ± 0.06
Test
89.1 ± 0.10
89.1 ± 0.07
89.6 ± 0.06
Random
86.4 ± 0.09
86.3 ± 0.11
87.3 ± 0.07
Attack
50.0 ± 1.16
49.6 ± 1.52
54.0 ± 0.48"
SAT,0.2792022792022792,50-100
SAT,0.28205128205128205,"Test
81.1 ± 0.64
81.2 ± 0.89
82.7 ± 0.50
Random
78.6 ± 0.80
79.0 ± 1.02
80.8 ± 0.46
Attack
39.4 ± 3.15
37.9 ± 2.62
44.4 ± 1.19
3-10
92.3 ± 0.57
92.7 ± 0.30
93.0 ± 0.33
100-300
64.4 ± 1.53
65.3 ± 1.63
67.0 ± 0.74
SATLIB
66.1 ± 3.07
66.2 ± 4.71
63.7 ± 1.88
UNI3SAT
86.0 ± 0.75
85.1 ± 0.80
86.9 ± 0.39"
SAT,0.2849002849002849,"Attack budget.
We study the inﬂuence of the
budget ∆and, hence, the similarity of x vs. ˜x,
in Fig. 4. It sufﬁces to perturb 0.2% of the lit-
erals for the recall to drop below 50% using our
SAT perturbation model. This stands in stark con-
trast to the random attack, where the accuracy for
the satisﬁable instances is almost constant over the
plotted range of budgets."
SAT,0.28774928774928776,"Hard model-speciﬁc samples. The surprisingly
weak performance (Fig. 2 and 4) of the random
baselines shows how much more effective our at-
tacks are and justiﬁes their minimally larger cost
(see § H). In § G, we show the difﬁculty is indeed"
SAT,0.2905982905982906,Published as a conference paper at ICLR 2022
SAT,0.2934472934472934,"model-speciﬁc. Assuming that hard model-speciﬁc instances are the instances that are important
to improve the model’s performance, we can lower the amount of labeled data (potentially expen-
sive). Of course, we cannot do anything about the NP-completeness of the problem, but adversarial
robustness opens the possibility to use the expensively generated examples as effectively as possible."
SAT,0.2962962962962963,"Adversarial training for SAT. We conjecture that if the models were expressive enough, we would
now be able to leverage the adversarial examples for an improved training procedure. Therefore,
similar to Jeddi et al. (2020), we perform an adversarial ﬁne-tuning. That is, we train the models for
another 10 epochs including perturbed problem instances. We use the same setup as for the attacks
presented above but we observed that too severe perturbations harm NeuroSAT’s performance (e.g.
a budget of 5% sufﬁces to push the accuracy below 20% for the satisﬁable instances). We therefore
lower the budget of the satisﬁable instances to 1% and perturb 5% of the training instances. For a fair
comparison, we also compare to a model that was trained on a 10% larger training set. To compare
the solvers’ capability to generalize for the different training strategies, we choose datasets of differ-
ent problem sizes and also include the external benchmark datasets SATLIB and UNI3SAT (Hoos
& St¨utzle, 2000). We consistently outperform the regularly trained models in terms of robustness as
well as generalization (with the exceptions SATLIB). We report the results in Table 1."
TSP,0.29914529914529914,"6.2
TSP"
TSP,0.301994301994302,"Setup. In our setup we follow Prates et al. (2019) and generate the training data by uniformly
sampling n ∼U(20, 40) nodes/coordinates from the 2D unit square. This is converted into a fully
connected graph where the edge weights represent the L2-distances between two nodes. A near-
optimal solution Y for training and attacks is obtained with the Concorde solver (Applegate et al.,
2006). For the decision-variant of the TSP, we produce two samples from every graph: a graph
with y = 1 and cost-query cy=1
0
= c(σ∗) · (1 + d) and a second graph with y = 0 and cost-query
cy=0
0
= c(σ∗) · (1 −d), where d = 2% (Prates et al., 2019). For predicting the TSP solution we use
ConvTSP (Joshi et al., 2019) but keep the setup identical to its decision equivalent. We attack these
models via adding ﬁve adversarial nodes and adjust ˜c0 as well as ˜Y accordingly."
TSP,0.30484330484330485,"Decision TSP Solver. If a route of target cost exists, our attack successfully fools the neural solver
in most of the cases. This low adversarial accuracy highlights again that the clean accuracy is
far too optimistic and that the model likely suffers from challenges (1) easier subproblem and/or (2)
spurious features. In Fig. 7, we see that the changes are indeed rather small and that the perturbations
lead to practical problem instances. For further examples see § K. 0.4 0.6 0.8"
TSP,0.3076923076923077,Accuracy
TSP,0.31054131054131057,Label: Route Exists 0.7 0.8 0.9
TSP,0.31339031339031337,Label: No Route Exists
TSP,0.3162393162393162,"Clean
Rand
Adv"
TSP,0.3190883190883191,"Figure 6: DecisionTSP for problems with n ∼
U(20, 40) nodes and ﬁve adversarial nodes.
Figure 7: Examples of the optimal route Y
and perturbed routes ˜Y for DecisionTSP."
TSP,0.32193732193732194,"Difﬁculty imbalance. For the TSP, we also observe an imbalance in performance between both
classes. This is naturally explained with a look at the non-decision TSP version where a solver
constructs a potential route ˆY . Since c( ˆY ) ≥c(Y ) such a network comes with the optimal precision
of 1 by design."
TSP,0.3247863247863248,"Attacking TSP Solver. For ConvTSP ﬁve new adversarial nodes sufﬁce to exceed an optimality gap
of 2%. Note that naive baselines such as the “farthest insertion” achieve an optimality gap of 2.3%
on the clean dataset (Kool et al., 2019). Moreover, for the class ”route exists” we can compare the
performance on the decision TSP. Even though the model performs better than DTSP, our attacks
degrade the performance relatively by 10%. In Fig. 9, we can also view the predicted routes and
observe that the prediction can differ severely between the clean and perturbed problem instance."
TSP,0.32763532763532766,Published as a conference paper at ICLR 2022 0.01 0.02 0.03 0.04
TSP,0.33048433048433046,Optimality Gap
TSP,0.3333333333333333,Route Prediction
TSP,0.33618233618233617,"Clean
Rand
Adv 0.75 0.80 0.85 0.90"
TSP,0.33903133903133903,Accuracy
TSP,0.3418803418803419,Route Exists
TSP,0.34472934472934474,"Figure 8: ConvTSP for problems with n = 20
nodes and ﬁve adversarial nodes. We plot the opti-
mality gap (left) and the decision TSP performance
(right)."
TSP,0.3475783475783476,"Concorde
Perturbed
Clean
Adv. Points"
TSP,0.3504273504273504,"Figure 9:
Exemplary problem instances
where the attack successfully changed the op-
timal route for ConvTSP that show drastic
changes of the prediction."
RELATED WORK,0.35327635327635326,"7
RELATED WORK"
RELATED WORK,0.3561253561253561,"Combinatorial Optimization. Further important works about neural SAT solvers are (Amizadeh
et al., 2019; Yolcu & P´oczos, 2019; Kurin et al., 2019; Cameron et al., 2020) and for neural TSP
solvers (Khalil et al., 2017; Deudon et al., 2018; Bresson & Laurent, 2021; Wang et al., 2021; Bello
et al., 2016; Kool et al., 2019). We refer to the recent surveys surveys (Bengio et al., 2021; Cappart
et al., 2021; Vesselinova et al., 2020) for a detailed overview and discussion."
RELATED WORK,0.358974358974359,"Generalization. There are only very few works on generalization of neural combinatorial solvers.
One exception are Franc¸ois et al. (2019) and Joshi et al. (2021). They empirically assess the im-
pact of different model and training pipeline design choices on the generalization (for TSP) while
we discuss the generation of hard model-speciﬁc instances. A work by Selsam & Bjørner (2019)
studies generalization for their NeuroSAT model (Selsam et al., 2019) and proposes a data augmen-
tation technique relying on traditional solvers. Moreover, they study a hybrid model consisting of
a simpliﬁed NeuroSAT and a traditional solver. In summary, previous works about generalization
of neural combinatorial solvers analyze speciﬁc tasks while we propose a general framework for
combinatorial optimization and study TSP and SAT to show its importance."
RELATED WORK,0.36182336182336183,"Adversarial Robustness. Adversarial robustness has been studied in various domains including
computer vision (Szegedy et al., 2014) and graphs (Z¨ugner et al., 2018; Dai et al., 2018). We refer to
G¨unnemann (2021) for a broad overview of adversarial robustness of GNNs. Speciﬁcally, for TSP
we optimize the continuous input coordinates (or edge weights) but use our own approach due to the
non-convex constraints. For attacking the SAT model we need to perturb the discrete graph structure
and rely on L0-PGD by Xu et al. (2019) proposed in the context of GNNs."
RELATED WORK,0.3646723646723647,"Hard sample mining. Deriving adversarial examples might appear similar to hard sample min-
ing (Sung, 1995). However, hard sample mining aims in spotting hard problem instances in the train
data unlike we who also perturb the problem instances (of the training data or any other dataset).
Moreover, if we combine augmentations with hard sample mining, we only create randomly per-
turbed instances and their generation is not guided by the model. The surprisingly weak random
baseline in our experiments gives an impression about how effective such an approach might be."
DISCUSSION,0.36752136752136755,"8
DISCUSSION"
DISCUSSION,0.37037037037037035,"We bring the study of adversarial robustness to the ﬁeld of neural combinatorial optimization. In
contrast to general learning tasks, we show that there exists a model with both high accuracy and
robustness. A key ﬁnding of our work is that the assessed neural combinatorial solvers are all sen-
sitive w.r.t. small perturbations of the input. For example, we can fool NeuroSAT (Selsam et al.,
2019) for the overwhelming majority of instances from the training data distribution with moderate
perturbations (5% of literals). We show that adversarial training can be used to improve robust-
ness. However, strong perturbations can still fool the model, indicating a lack of expressiveness.
In summary, contemporary supervised neural solvers seem to be very fragile, and adversarial ro-
bustness is an insightful research direction to reveal as well as address neural solver deﬁciencies for
combinatorial optimization."
DISCUSSION,0.3732193732193732,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.37606837606837606,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3789173789173789,"We provide the source code and conﬁguration for the key experiments including instructions on how
to generate data and train the models. All proofs are stated in the appendix with explanations and
underlying assumptions. We thoroughly checked the implementation and also veriﬁed empirically
that the proposed sound perturbation models hold."
ETHICS STATEMENT,0.3817663817663818,ETHICS STATEMENT
ETHICS STATEMENT,0.38461538461538464,"Solving combinatorial optimization problems effectively and efﬁciently would beneﬁt a wide range
of applications. For example, it could further improve supply chain optimization or auto-routing
electric circuits. Since combinatorial optimization is such a fundamental building block it is needless
to detail how big of an impact this line of research could have. Unfortunately, this also includes
applications with negative implications. Speciﬁcally, the examples about supply chain optimization
and electric circuits also apply to military applications. Nevertheless, we believe that the positive
impact can be much greater than the negative counterpart."
ETHICS STATEMENT,0.38746438746438744,"Unarguably, studying adversarial robustness in the context of combinatorial optimization comes
with the possibility of misuse. However, not studying this topic and, therefore, being unaware of the
model’s robustness imposes an even greater risk. Moreover, since we study white-box attacks we
leave the practitioner with a huge advantage over a possible real-world adversary that e.g. does not
know the weights of the model. Aside from robustness, we did not conduct dedicated experiments
on the consequences on e.g. fairness for our methods or the resulting models."
REFERENCES,0.3903133903133903,REFERENCES
REFERENCES,0.39316239316239315,"Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsu-
pervised differentiable approach. In 7th International Conference on Learning Representations,
ICLR, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019."
REFERENCES,0.396011396011396,"David Applegate, Ribert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006."
REFERENCES,0.39886039886039887,"Gilles Audemard and Laurent Simon. Predicting learnt clauses quality in modern sat solvers. In
Proceedings of the 21st International Jont Conference on Artiﬁcal Intelligence, IJCAI’09, San
Francisco, CA, USA, 2009. Morgan Kaufmann Publishers Inc."
REFERENCES,0.4017094017094017,"Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016."
REFERENCES,0.4045584045584046,"Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimiza-
tion: A methodological tour d’horizon. European Journal of Operational Research, 290(2), 2021.
Publisher: Elsevier."
REFERENCES,0.4074074074074074,"Xavier Bresson and Thomas Laurent. The transformer network for the traveling salesman problem.
CoRR, abs/2103.03012, 2021."
REFERENCES,0.41025641025641024,"Chris Cameron, Rex Chen, Jason Hartford, and Kevin Leyton-Brown. Predicting propositional satis-
ﬁability via end-to-end learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
2020."
REFERENCES,0.4131054131054131,"Quentin Cappart,
Didier Ch´etelat,
Elias Khalil,
Andrea Lodi,
Christopher Morris,
and
Petar Veliˇckovi´c.
Combinatorial optimization and reasoning with graph neural networks.
arXiv:2102.09544 [cs, math, stat], Apr. 2021. arXiv: 2102.09544."
REFERENCES,0.41595441595441596,"M. Cutler. Efﬁcient special case algorithms for the n-line planar traveling salesman problem. Net-
works, 10(3), 1980. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/net.3230100302."
REFERENCES,0.4188034188034188,"Hanjun Dai, Hui Li, Tian Tian, Huang Xin, Lin Wang, Zhu Jun, and Song Le. Adversarial attack on
graph structured data. 35th International Conference on Machine Learning, ICML, 3, 2018."
REFERENCES,0.42165242165242167,Published as a conference paper at ICLR 2022
REFERENCES,0.42450142450142453,"Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin
Rousseau. Learning heuristics for the tsp by policy gradient. In Willem-Jan van Hoeve, editor,
Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations Research, Cham,
2018. Springer International Publishing."
REFERENCES,0.42735042735042733,"P. Festa. A brief introduction to exact, approximation, and heuristic algorithms for solving hard
combinatorial optimization problems. In 2014 16th International Conference on Transparent
Optical Networks (ICTON), July 2014. ISSN: 2161-2064."
REFERENCES,0.4301994301994302,"Antoine Franc¸ois, Quentin Cappart, and Louis-Martin Rousseau. How to evaluate machine learning
approaches for combinatorial optimization: Application to the travelling salesman problem. arXiv
preprint arXiv:1909.13121, 2019."
REFERENCES,0.43304843304843305,"Simon Geisler, Tobias Schmidt, Hakan S¸irin, Daniel Z¨ugner, Aleksandar Bojchevski, and Stephan
G¨unnemann. Robustness of graph neural networks at scale. In Neural Information Processing
Systems, NeurIPS, 2021."
REFERENCES,0.4358974358974359,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. 34th International Conference on Machine Learning,
ICML, 3, 2017."
REFERENCES,0.43874643874643876,"Stephan G¨unnemann. Graph neural networks: Adversarial robustness. In Lingfei Wu, Peng Cui, Jian
Pei, and Liang Zhao, editors, Graph Neural Networks: Foundations, Frontiers, and Applications,
chapter 8, . Springer, Singapore, 2021."
REFERENCES,0.4415954415954416,"Shai Haim and Toby Walsh. Restart Strategy Selection Using Machine Learning Techniques. In
Oliver Kullmann, editor, Theory and Applications of Satisﬁability Testing - SAT 2009, Lecture
Notes in Computer Science, Berlin, Heidelberg, 2009. Springer."
REFERENCES,0.4444444444444444,"Holger Hoos and Thomas St¨utzle. SATLIB: An online resource for research on SAT. In SAT 2000.
Apr. 2000. Journal Abbreviation: SAT 2000."
REFERENCES,0.4472934472934473,"Alexey Ignatiev, Antonio Morgado, and Joao Marques-Silva. PySAT: A Python toolkit for prototyp-
ing with SAT oracles. In SAT, 2018."
REFERENCES,0.45014245014245013,"Ahmadreza Jeddi, Mohammad Javad Shaﬁee, and Alexander Wong. A Simple Fine-tuning Is All
You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning. arXiv:2012.13628 [cs],
Dec. 2020. arXiv: 2012.13628."
REFERENCES,0.452991452991453,"Chaitanya K. Joshi, Thomas Laurent, and Xavier Bresson. An Efﬁcient Graph Convolutional Net-
work Technique for the Travelling Salesman Problem. arXiv:1906.01227 [cs, stat], Oct. 2019.
arXiv: 1906.01227."
REFERENCES,0.45584045584045585,"Chaitanya K. Joshi, Quentin Cappart, Louis-Martin Rousseau, and Thomas Laurent. Learning TSP
Requires Rethinking Generalization. arXiv:2006.07054 [cs, stat], Sept. 2021. arXiv: 2006.07054."
REFERENCES,0.4586894586894587,"Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial opti-
mization algorithms over graphs. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.46153846153846156,"Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In 3rd Interna-
tional Conference on Learning Representations, {ICLR} 2015, 2015."
REFERENCES,0.46438746438746437,"Wouter Kool, Herke van Hoof, and Max Welling. Attention, Learn to Solve Routing Problems! In
7th International Conference on Learning Representations, ICLR 2019, 2019."
REFERENCES,0.4672364672364672,"Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. Improving SAT solver heuristics
with graph networks and reinforcement learning. CoRR, abs/1909.11830, 2019."
REFERENCES,0.4700854700854701,"Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional
networks and guided tree search. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, 2018."
REFERENCES,0.47293447293447294,"Marcelo O. R. Prates, Pedro H. C. Avelar, Henrique Lemos, Lu´ıs C. Lamb, and Moshe Y. Vardi.
Learning to solve np-complete problems: A graph neural network for decision TSP.
In The
Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019. AAAI Press, 2019."
REFERENCES,0.4757834757834758,Published as a conference paper at ICLR 2022
REFERENCES,0.47863247863247865,"G¨unter Rote. The N-line Traveling Salesman Problem, 1991."
REFERENCES,0.48148148148148145,"Daniel Selsam and Nikolaj Bjørner. Guiding High-Performance SAT Solvers with Unsat-Core Pre-
dictions. arXiv:1903.04671 [cs], July 2019. arXiv: 1903.04671."
REFERENCES,0.4843304843304843,"Daniel Selsam, Matthew Lamm, Benedikt B¨unz, Percy Liang, Leonardo de Moura, and David L.
Dill. Learning a SAT solver from single-bit supervision. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,
2019."
REFERENCES,0.48717948717948717,"David Stutz, Matthias Hein, and Bernt Schiele. Disentangling Adversarial Robustness and General-
ization. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, 2019."
REFERENCES,0.49002849002849,"Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. Revisiting Adver-
sarial Risk. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics. PMLR,
Apr. 2019. ISSN: 2640-3498."
REFERENCES,0.4928774928774929,"Kah Kay Sung. Learning and example selection for object and pattern detection. PhD Thesis,
Massachusetts Institute of Technology, Cambridge, MA, USA, 1995."
REFERENCES,0.49572649572649574,"Christian Szegedy, W. Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, Ian J. Goodfellow, and R. Fer-
gus. Intriguing properties of neural networks. 2nd International Conference on Learning Repre-
sentations, ICLR, 2014."
REFERENCES,0.4985754985754986,"Niklas S¨orensson and Niklas Een. Minisat v1.13-a sat solver with conﬂict-clause minimization.
International Conference on Theory and Applications of Satisﬁability Testing, 01 2005."
REFERENCES,0.5014245014245015,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness May Be at Odds with Accuracy. In 7th International Conference on Learning Repre-
sentations, ICLR 2019, 2019."
REFERENCES,0.5042735042735043,"N. Vesselinova, R. Steinert, D. F. Perez-Ramirez, and M. Boman. Learning Combinatorial Optimiza-
tion on Graphs: A Survey With Applications to Networking. IEEE Access, 8, 2020. Conference
Name: IEEE Access."
REFERENCES,0.5071225071225072,"Runzhong Wang, Zhigang Hua, Gan Liu, Jiayi Zhang, Junchi Yan, Feng Qi, Shuang Yang, Jun Zhou,
and Xiaokang Yang. A bi-level framework for learning to solve combinatorial optimization on
graphs. CoRR, abs/2106.04927, 2021."
REFERENCES,0.50997150997151,"Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K. Jain. Ad-
versarial Attacks and Defenses in Images, Graphs and Text: A Review. International Journal of
Automation and Computing, 17(2), Apr. 2020."
REFERENCES,0.5128205128205128,"Kaidi Xu, Hongge Chen, Sijia Liu, Pin Yu Chen, Tsui Wei Weng, Mingyi Hong, and Xue Lin.
Topology attack and defense for graph neural networks: An optimization perspective. IJCAI
International Joint Conference on Artiﬁcial Intelligence, 2019-Augus, 2019."
REFERENCES,0.5156695156695157,"Runze Yang and Teng Long. Derivative-free optimization adversarial attacks for graph convolutional
networks. PeerJ Computer Science, 7, Aug. 2021. Publisher: PeerJ Inc."
REFERENCES,0.5185185185185185,"Gal Yehuda, Moshe Gabel, and Assaf Schuster. It’s Not What Machines Can Learn, It’s What We
Cannot Teach. In International Conference on Machine Learning. PMLR, Nov. 2020. ISSN:
2640-3498."
REFERENCES,0.5213675213675214,"Emre Yolcu and Barnab´as P´oczos. Learning local search heuristics for boolean satisﬁability. In
NeurIPS, 2019."
REFERENCES,0.5242165242165242,"Daniel Z¨ugner, Amir Akbarnejad, and Stephan G¨unnemann. Adversarial attacks on neural networks
for graph data. International Conference on Knowledge Discovery and Data Mining, KDD, 2018."
REFERENCES,0.5270655270655271,Published as a conference paper at ICLR 2022
REFERENCES,0.5299145299145299,"A
ACCURACY ROBUSTNESS TRADEOFF FOR COMBINATORIAL SOLVERS"
REFERENCES,0.5327635327635327,"If the accuracy robustness trade-off existed for neural combinatorial optimization, it would imply
that it is hopeless to strive for an accurate general-purpose neural combinatorial solver. Fortunately,
this is not the case. To show this we built upon the ideas of Suggala et al. (2019) but we only consider
the special case of combinatorial decision problems where each possible prediction ˆY is either true
or false (i.e. no stochasticity of the true label as long as we account for the ambiguity in Y )."
REFERENCES,0.5356125356125356,"For general learning tasks, Suggala et al. (2019) reﬁned the classical deﬁnition of adversarial ro-
bustness s.t. there is provably no such trade-off in general classiﬁcation tasks if either the dataset
is “margin separable” or when we restrict the perturbation space to be label-preserving for a Bayes
optimal classiﬁer."
REFERENCES,0.5384615384615384,"For combinatorial optimization with a sound perturbation model G we can argue both ways. First,
due to the soundness we never encourage a wrong prediction (i.e. the margin condition is natu-
rally fulﬁlled). This is contrast to e.g. image classiﬁcation where an instance close to the decision
boundary with the common Lp perturbation model ∥x −˜x∥p ≤r changes its true label or becomes
meaningless (e.g. a gray image) for a large enough r. Second, the Bayes optimal classiﬁer for combi-
natorial optimization solves the optimization problem perfectly on p(x) (with support(p(x)) ⊆X).
For these reasons, we do not need to include the Bayes optimal classiﬁer in the deﬁnition of the
attack (Eq. 1)."
REFERENCES,0.5413105413105413,"More formally, we show by contradiction that any minimizer f ∗= arg minf∈F Radv,G(f) is optimal
w.r.t. minf∈F R(f). Let ˆf be an optimal classiﬁer w.r.t. R(f) for a combinatorial decision problem."
REFERENCES,0.5441595441595442,"We assume the loss ℓis equal for all possible optimal Y , to account for the multiple possible Y and
deﬁne the adversarial risk"
REFERENCES,0.5470085470085471,"Radv,G(f) = Ex∼p(x)"
REFERENCES,0.5498575498575499,"
max
˜x∈G(x,Y ) ℓ(fθ(˜x), h(˜x, x, Y ))
"
REFERENCES,0.5527065527065527,"for any data distribution p(x) and the standard risk R(f) = Ex∼p(x) [ℓ(fθ(x), Y )]. W.l.o.g. we
assume the best possible risk is zero R∗= 0 (i.e. the prediction is always true)."
REFERENCES,0.5555555555555556,Suppose f ∗and ˆf differ in their prediction f ∗(x) ̸= ˆf(x) over a non-empty subset of support(p(x)).
REFERENCES,0.5584045584045584,"Since f ∗is optimal and there is no stochasticity in the true label Y , it is always correct with standard
risk R(f ∗) = 0 and, analogously, Radv,G(f ∗) = 0."
REFERENCES,0.5612535612535613,"Thus, ˆf cannot be an optimizer of ˆf = arg minf∈F R(f) if its predictions differs over a non-empty
subset of support(p(x))."
REFERENCES,0.5641025641025641,"Note that this also holds for support(p(x)) ⊆support(G(p(x))) ⊆X which is indeed the interesting
case for contemporary data generators and models."
REFERENCES,0.5669515669515669,"B
PROOF OF PROPOSITION 1"
REFERENCES,0.5698005698005698,"Proposition 1 Let x = k1(v1, . . . , vn) ∧. . . km(v1, . . . , vn) be a boolean statement in Conjunctive
Normal Form (CNF) with m clauses and n variables. Then ˜x, a perturbed version of x, has the
same label y = ˜y in the following cases:"
REFERENCES,0.5726495726495726,"• SAT: x is satisﬁable y = 1 with truth assignment Y . Then, we can arbitrarily remove or add
literals in x to obtain ˜x, as long as one literal in Y remains in each clause."
REFERENCES,0.5754985754985755,"• DEL: x is unsatisﬁable y = 0 and we obtain ˜x from x through arbitrary removals of literals, as
long as one literal per clause remains."
REFERENCES,0.5783475783475783,"• ADC: x is unsatisﬁable y = 0. Then, we can arbitrarily remove, add, or modify clauses in x to
obtain ˜x, as long as there remains a subset of clauses that is unsatisﬁable in isolation."
REFERENCES,0.5811965811965812,"W.l.o.g. we assume that an empty clause is true. That is, we evaluate the expression if the empty
clauses were not there. Moreover, we assume to know one possible Y (multiple might exist)."
REFERENCES,0.584045584045584,Published as a conference paper at ICLR 2022
REFERENCES,0.5868945868945868,"• SAT: Every disjunctive clause evaluates to one, if one literal is true. Since we keep at least one
literal in Y in each clause, every clause evaluates to one. The statement evaluates to one since
the conjunction of true statement is also true."
REFERENCES,0.5897435897435898,"• DEL: Since every clause is a disjunction of literals, the removal of one/some of the literals
strictly reduces the number of speciﬁable assignments. That is, removing any literal li from its
clause removes the possibility to satisfy this clause through li = 1."
REFERENCES,0.5925925925925926,"• ADC: Since the clauses are a conjunctive, all clauses need to evaluate to true. If a subset of
clauses it not satisﬁable by themselves (i.e. not all can be true at the same time), the expression
necessarily resolves to 0."
REFERENCES,0.5954415954415955,"Hence, all conditions in Proposition 1 do not change the satisﬁability y, but might alter Y . It is
apparent that the updated Y can be obtained efﬁciently. □"
REFERENCES,0.5982905982905983,"C
PROOF OF PROPOSITION 2"
REFERENCES,0.6011396011396012,"Proposition 2 Let σ∗be the optimal route over the nodes V in G, let Z ̸∈V be an additional node,
and P, Q are any two neighbouring nodes on σ∗. Then, the new optimal route ˜σ∗(including Z) is
obtained from σ∗through inserting Z between P and Q if ̸ ∃(A, B) ∈V2 \ {(P, Q)} with A ̸= B
s.t. ω(A, Z) + ω(B, Z) −ω(A, B) ≤ω(P, Z) + ω(Q, Z) −ω(P, Q)."
REFERENCES,0.603988603988604,"Corollary 1 We can add multiple nodes to G and obtain the optimal route ˜σ∗as long as the condi-
tion of Proposition 2 (including the other previously added nodes) is fulﬁlled."
REFERENCES,0.6068376068376068,"Corollary 2 For the metric TSP, it is sufﬁcient if the condition of Proposition 2 holds for (A, B) ∈
V2 \ ({(P, Q)} ∪H) with A ̸= B where H denotes the pairs of nodes both on the Convex Hull
H ∈CH(V)2 that are not a line segment of the Convex Hull."
REFERENCES,0.6096866096866097,"Proof. We proof by contradiction. We deﬁne (R, S) ∈V2 \ {(P, Q)} to be the two neighboring
nodes of Z on ˜σ∗. Suppose ω(P, Z) + ω(Q, Z) −ω(P, Q) < ω(R, Z) + ω(S, Z) −ω(R, S) and
˜σ∗would not contain the edges ω(P, Z) as well as ω(Q, Z)."
REFERENCES,0.6125356125356125,We know by optimality of σ∗that
REFERENCES,0.6153846153846154,"c(˜σ∗) −ω(R, Z) −ω(S, Z) + ω(R, S) ≥c(σ∗)"
REFERENCES,0.6182336182336182,and by optimality of ˜σ∗that
REFERENCES,0.6210826210826211,"c(σ∗) + ω(P, Z) + ω(Q, Z) −ω(P, Q) ≥c(˜σ∗). Thus,"
REFERENCES,0.6239316239316239,"c(σ∗) + ω(P, Z) + ω(Q, Z) −ω(P, Q) ≥c(˜σ∗) ≥c(σ∗) + ω(R, Z) + ω(S, Z) −ω(R, S)"
REFERENCES,0.6267806267806267,and equivalently
REFERENCES,0.6296296296296297,"ω(P, Z) + ω(Q, Z) −ω(P, Q) ≥ω(R, Z) + ω(S, Z) −ω(R, S)"
REFERENCES,0.6324786324786325,which leads to a contradiction.
REFERENCES,0.6353276353276354,"Since we do not know what edges are contained in ˜σ∗(i.e. what nodes could be R and S) we state
the stricter condition ̸ ∃(A, B) ∈V2 \ {(P, Q)} with A ̸= B s.t. ω(A, Z) + ω(B, Z) −ω(A, B) ≤
ω(P, Z) + ω(Q, Z) −ω(P, Q). □"
REFERENCES,0.6381766381766382,"If multiple σ∗exist, then this statement holds for any optimal route that has a direct connection
between P and Q. Corollary 1 follows by induction and Corollary 2 is due to the fact that the in
metric space the optimal route ˜σ∗must be a simple polygon (i.e. no crossings are allowed). This
was ﬁrst stated for an euclidean space as “the intersection theorem” by Cutler (1980) and is a direct
consequence of the triangle inequality. Also note, alternatively to the proof presented here, one can
also unfold the dynamic program proposed by Rote (1991) to end up at Proposition 2."
REFERENCES,0.6410256410256411,Published as a conference paper at ICLR 2022
REFERENCES,0.6438746438746439,"D
PROJECTED GRADIENT DESCENT (PGD)"
REFERENCES,0.6467236467236467,"Algorithm D.1: Projected Gradient Descent
Data: Problem (x, Y ) and possibly y, Solver fθ(·),
Loss ℓ, budget ∆, attack steps s, learn rate α"
REFERENCES,0.6495726495726496,"1 ˜x0 ←initialize(x, Y, ∆)"
REFERENCES,0.6524216524216524,"2 for t ∈{0, 1, . . . , s −1} do"
REFERENCES,0.6552706552706553,"3
˜x′
t+1 ←update(˜xt, αt, ∇ℓ(fθ(˜xt), h(˜x, x, Y )))"
REFERENCES,0.6581196581196581,"4
˜xt+1 ←project(˜x′
t+1, x, ∆)"
END,0.6609686609686609,5 end
END,0.6638176638176638,"6 ˜x ←postprocess(˜xE, ∆)"
END,0.6666666666666666,"7 return ˜x, h(˜x, x, Y )"
END,0.6695156695156695,"PGD is one of the most success-
ful and widely studied approaches
to craft adversarial examples.
For
a further techniques and a broader
overview of adversarial robustness in
various domains, we refer to Xu et al.
(2020)."
END,0.6723646723646723,"All our attacks roughly match the
framework of Algorithm D.1. First,
we initialize the perturbed instance,
or, alternatively, we can use some
variable that models the difference to
the clean instance x (line 1). Initial-
ization strategies that we consider are
random initialization or initializing to the clean instance. Then we perform the attack for s steps and
in each step update the perturbed instance through a gradient descent step (line 3). For faster con-
vergence we additionally use Adam as optimizer (Kingma & Ba, 2015). After the gradient update
we perform a projection step that ensures we stay within the budget ∆or satisfy other constraints.
For simplicity, we omit the fact that we use early stopping in all our algorithms. Speciﬁcally, in each
attack step we check if the current perturbed instance ˜xt+1 comes with the best loss so far. Then, af-
ter s attack steps we assign the best possible ˜xs ←arg maxt∈{1,...,s} ℓ(fθ(˜xt), h(˜xt, x, Y )). This
happens right before we (optionally) perform a postprocessing. Finally, we return the perturbed
instance ˜x with solution Y or decision label y."
END,0.6752136752136753,"Limitations. As discussed in § 3, we require the model to be differentiable w.r.t. its input. For-
tunately, most neural combinatorial solvers rely on GNNs and therefore this does not impose an
issue. However, even if assessing a non-differentiable model one could revert to derivative-free
optimization (Yang & Long, 2021)."
END,0.6780626780626781,"For some combinatorial optimization problems the number of variables we need to optimize over
can be very large. For example, when attacking a Maximum Independent Set neural solver (Li et al.,
2018) using our perturbation models for SAT, we need to construct a graph that blows up quickly.
Nevertheless, this could be done with derivative-free optimization (Yang & Long, 2021) or a scalable
variant of of L0-PGD called projected randomized block coordinate descent (Geisler et al., 2021)."
END,0.6809116809116809,"E
SAT ATTACK DETAILS"
END,0.6837606837606838,"SAT model description. Selsam et al. (2019) propose to model the input problem as a bipartite
graph as described in § 4.2. We instantiate the model as described in their paper: over 26 message
passing steps the GNN updates its embeddings of size 128. Clause nodes and literal nodes have
separate LSTMs to update their embeddings with messages produced by 3-layer MLPs. After the
last message passing steps, the output vote on whether the problem is satisﬁable or not is obtained
by transforming the clause embeddings with an additional vote-MLP and averaging over the ﬁnal
votes of the literal nodes. The paper also notes that the number of message passing steps has to
be adapted to the problem size. As no speciﬁc values are provided, we use 64 steps for the50-100
dataset and 128 steps for the SATLIB and uni3sat data."
END,0.6866096866096866,"Selsam et al. (2019) trained their model in a single epoch on a large dataset consisting of “millions
of samples”. However, since Selsam et al. (2019) did not publish their dataset we used a total of
60,000 samples with the very same data generation strategy. We then use 50,000 of the samples to
train the model for 60 epochs. With this strategy we closely match the reported performance. For the
larger train set in Table 1 we generate an additional 5,000 samples. During training we use the same
hyperparameters as described in the paper. Please use the referenced code for exactly reproducing
the dataset."
END,0.6894586894586895,Published as a conference paper at ICLR 2022
END,0.6923076923076923,"Optimization Problem.
We restate the optimization problem for an adversarial attack on a
SAT decision problem. The SAT attack tries to ﬁnd a perturbed instance ˜x that maximizes the
loss s.t. every clause contains at least one literal lj that is present in the solution assignments
Y = {l∗
1, . . . , l∗
n | l∗
i ∈{¬vi, vi}}. Here we omit that we additionally constrain the number of
inserted/removed literals relatively to the number of literals in the clean instance x via budget ∆."
END,0.6951566951566952,"max
˜x
ℓ(fθ(˜x), y = 1)
s.t.
∀˜ki ∈˜x : (∃lj ∈˜ki with lj = l∗
j)
(E.1)"
END,0.698005698005698,"The ADC attack maximizes the loss by adding clauses to the problem, meaning that every clause ki
in the original problem x has to be present also in the perturbed instance ˜x, as these clauses ensure
that the problem remains unsatisﬁable:"
END,0.7008547008547008,"max
˜x
ℓ(fθ(˜x), y = 0)
s.t.
∀ki ∈x : ki ∈˜x
(E.2)"
END,0.7037037037037037,"Lastly, the DEL attack optimizes over what literals to delete from the problem’s clauses, as long as
no clause is removed completely. This results in the following optimization problem:"
END,0.7065527065527065,"max
˜x
ℓ(fθ(˜x), y = 0)
s.t.
∀˜ki ∈˜x : ˜ki ⊆ki
∧
nonempty(˜ki)
(E.3)"
END,0.7094017094017094,"Attack Details. The attack on SAT problems modiﬁes the literals that are contained in clauses. This
means speciﬁcally that we optimize over the edges represented by the literals-clauses adjacency
matrix x = A ∈{0, 1}2n×m. We implemented these attacks such that they can operate on batches
of problems, however we omit this at this point in the following for an improved readability."
END,0.7122507122507122,"Algorithm E.1 describes in detail SAT and DEL (see Proposition 1). The difference of the adja-
cency matrix A to the adversarially perturbed version ˜
A is modelled via the perturbation matrix
M: (˜x = ˜
A = A ⊕M). We then optimize over M. During the SAT attack, we allow deletions
and additions of edges, as long as one solution-preserving truth assignment per clause, described
by the indicator T = onehot(Y ), is preserved (line 7). For the DEL attack only deletions are al-
lowed, under the constraint that no clause can be fully deleted (lines 9 & 10). Additionally, a global
budget is enforced (line 6). For details on the budget as well as other hyperparameters of the at-
tack, we refer to Table E.1. For the ADC attack described in Algorithm E.2, the perturbation matrix
M ∈{0, 1}2n× ˜m describes additional clauses appended to the original problem (line 4). The attack
can freely optimize over M but the number of literals/edges. ∆is enforced s.t. on average each
clause contains as many literals as a clause in A (line 6)."
END,0.7150997150997151,Table E.1: Hyperparameters for the attacks on NeuroSAT proposed in § 4
END,0.717948717948718,"SAT
DEL
ADC"
END,0.7207977207977208,"attack steps
500
500
500
learning rate
0.1
0.1
0.1
fraction of perturbed literals ∆
5% of edges
5% of edges
25% of clauses
# ﬁnal samples
20
20
20
temperature scaling
5
5
5"
END,0.7236467236467237,"Because the attack optimizes over a set of discrete edges with a gradient based method, similarly
to (Xu et al., 2019), we relax the edge weights to [0, 1] during the attack. Before generating the
perturbed problem instance ˜
A, we sample the discrete M ′ from a Bernoulli distribution where the
entries of the matrix M represent the probability of success. In contrast to (Xu et al., 2019) we
sample 19 instead of 20 times but add an additional sample that chooses the top elements in M.
Thereafter, we take the sample that maximizes the loss. Moreover, our projection differs slightly
from the one proposed by Xu et al. (2019) since we iteratively enforce the budget instead of per-
forming a bisection search."
END,0.7264957264957265,Published as a conference paper at ICLR 2022
END,0.7293447293447294,"Algorithm E.1: SAT & DEL Attack
Data: Adjacency A ∈{0, 1}2n×m,
edge budget ∆, steps s, learning
rate α, solution T ∈{0, 1}2n×m,
SAT model fθ, label y
Result: Perturbed Adjacency ˜
A"
END,0.7321937321937322,"1 Initialize M ←02n×m,"
END,0.7350427350427351,"2 for t ∈{0, 1, . . . , s −1} do"
IF Y THEN,0.7378917378917379,"3
if y then
˜
A ←A ⊕M"
ELSE,0.7407407407407407,"4
else
˜
A ←A −M"
ELSE,0.7435897435897436,"5
M ←update(M, α, ∇ℓ(fθ( ˜
A), y)"
ELSE,0.7464387464387464,"6
M ←project-budget(M, ∆)"
ELSE,0.7492877492877493,"7
if y then M ←M ∗T"
ELSE,0.7521367521367521,"8
else"
ELSE,0.7549857549857549,"9
M ←M ∗A"
ELSE,0.7578347578347578,"10
M ←ensure-no-del(M)"
END,0.7606837606837606,"11
end"
END,0.7635327635327636,12 end
END,0.7663817663817664,13 M ′ ←sample(M)
END,0.7692307692307693,"14
˜
A ←A ⊕M ′"
END,0.7720797720797721,"Algorithm E.2: ADC Attack
Data: Adjacency A ∈{0, 1}2n×m,
clause budget ω, steps s, learning
rate α, SAT model fθ
Result: Perturbed Adjacency ˜
A"
END,0.7749287749287749,"1 Initialize M ←02n× ˜m,"
END,0.7777777777777778,"2 ∆←avg(A, axis = 0) ∗˜m"
END,0.7806267806267806,"3 for t ∈{0, 1, . . . , s −1} do"
END,0.7834757834757835,"4
˜
A ←append(A, M)"
END,0.7863247863247863,"5
M ←update(M, α, ∇fθ( ˜
A), y))"
END,0.7891737891737892,"6
M ←project-budget(M, ∆)"
END,0.792022792022792,7 end
END,0.7948717948717948,8 M ′ ←sample(M)
END,0.7977207977207977,"9
˜
A ←append(A, M ′)"
END,0.8005698005698005,"Computational complexity. All three attacks operate for s steps. Under the assumption that the
models are linear w.r.t. the number of edges in the graph representation of the problem (for forward
and backward pass), each step has a time complexity of O(mn). Therefore, the overall attack has
a time complexity of O(nms). Under the same assumption for the memory requirements of ∇Mℓ,
the total space complexity is O(nm)."
END,0.8034188034188035,"F
TSP ATTACK DETAILS"
END,0.8062678062678063,"For simplicity we only discuss the case of metric TSP. This is also what the used neural sovers are
mainly intended for. The TSP attack is actually implemented in a batched fashion. However, for an
improved readability we omit the details here. We refer to table Table F.1 for details on the attack
hyperparameters."
END,0.8091168091168092,"DTSP model. The DTSP model employs a GNN to predict whether there exists a route of certain
cost on an input graph (Prates et al., 2019). The model sequentially updates the embeddings of
both nodes and edges that were initialized by a 3-layer MLP with the coordinates as input. After
multiple message passing steps, the ﬁnal prediction is then based on the aggregation of the resulting
edge embeddings. We follow the training guidelines described in the paper and train on 220 graph
samples. The graphs are generated by sampling from the unit square (x ∈[0, 1]n×2) and solutions
are obtained with the concorde solver (Applegate et al., 2006). Dual problem sets are built from
each graph for training, validation and test data by increasing and decreasing the true cost by a small
factor."
END,0.811965811965812,"ConvTSP model. The ConvTSP model aims at predicting the optimal route Y = σ over a given
graph via a GCN (Joshi et al., 2019). It predicts a probability map over the edges indicating the
likelihood of an edge being present in an optimal solution. These lay the basis for different solution
decoding procedures. We employ their greedy search method, where the graph is traversed based
on the edges with the highest probabilities. We follow the training procedure described in the paper
as well as the data generation technique. Data is generated the same way as for the DTSP model,
however the target represents binary indicators whether an edge is present in the optimal solution.
During training, the binary cross entropy between the target and the probability maps over the edges
is minimized, and a solution-decoding technique is only applied during inference."
END,0.8148148148148148,"Optimization problem. We again restate the optimization problem from Eq. 1 for the speciﬁc case
of TSP. We add ∆nodes to the original problem x ∈[0, 1]n×2 to create a larger, perturbed problem"
END,0.8176638176638177,Published as a conference paper at ICLR 2022
END,0.8205128205128205,Table F.1: Hyperparameters for the attacks on TSP proposed in § 5
END,0.8233618233618234,"DTSP
ConvTSP"
END,0.8262108262108262,"maximum number of adv. nodes ∆
5
5
attack steps
200
500
learning rate
0.001
0.01
gradient project learning rate
0.002
0.002
gradient project steps
3
3"
END,0.8290598290598291,"instance x ∈[0, 1](n+∆)×2 which maximizes the loss:"
END,0.8319088319088319,"max
˜x
ℓ(fθ(˜x), Y )
s.t.
∀˜xi with i > n Proposition 2 holds
(F.1)"
END,0.8347578347578347,"Attack details. The input of the TSP is represented by the coordinates x ∈[0, 1]n×2 for the n
nodes. Additionally, we know the near-optimal route Y obtained with the Concorde solver which
we use as ground truth. During the TSP attack we add additional adversarial nodes Z ∈[0, 1]∆×2
to the input problem x. The adversarial nodes are initialized by randomly sampling nodes until they
fulﬁll the constraint from Proposition 2 (line 1). For the attack, as described in Algorithm F.1, we
operate solely on the coordinates of the adversarial nodes and assume that the model converts the
coordinates into a weighted graph. For the DTSP model, we additionally calculate the updated route
cost c0 (accordingly to y) and append it to the input. For simplicity, we omitted this special case in
Algorithm F.1. Moreover, for the DTSP we do not pass ˜Y to the loss; instead, we use the decision
label y that is kept constant throughout the optimization. We then obtain the updated coordinates Z
for the perturbed solution ˜Y (lines 5-6). In the project step (line 7), we only consider the coordinates
in Z that violate the constraint. As discussed in § 5.2, we perform gradient descent on the constraint
due to its non-convexity. We decide against optimizing the Lagrangian since this would require
evaluating the neural solver fθ and, therefore, is less efﬁcient. For the projection, we update the
adversarial node i that violates the constraint with"
END,0.8376068376068376,Zi ←Zi −η∇Zi
END,0.8404558404558404,"
ω(P, Zi) + ω(Q, Zi) −ω(P, Q) −

min
A,B∈x ω(A, Z) + ω(B, Z) −ω(A, B)
"
END,0.8433048433048433,until the constrain is fulﬁlled again but for at most three consecutive steps.
END,0.8461538461538461,"Algorithm F.1: TSP Attack
Data: Node Coords x ∈[0, 1]n×2, steps s,
learning rate α, TSP model fθ,
Adversarial Coords Z ∈[0, 1]∆×2
Result: Perturbed Node Coords
˜x ∈[0, 1](n+∆)×2"
END,0.8490028490028491,1 Initialize Z ←random-allowed-point()
END,0.8518518518518519,"2 for t ∈{0, 1, . . . , s −1} do"
END,0.8547008547008547,"3
mask ←is-constraint-fulﬁlled(Z, x)"
END,0.8575498575498576,"4
˜x ←append(x, Z[mask])"
END,0.8603988603988604,"5
˜Y ←update-solution(˜x, Y )"
END,0.8632478632478633,"6
Z ←update(Z, α, ∇ℓ(fθ(W ), ˜Y ))"
END,0.8660968660968661,"7
Z ←project(Z, x);"
END,0.8689458689458689,8 end
END,0.8717948717948718,"9 return ˜x, ˜Y"
END,0.8746438746438746,"Computational complexity. We again assume that the model has a linear time and space complex-
ity. This time it is linear w.r.t. the O(n2) number of elements in the distance matrix between the
input coordinates. The most costly operation we add, is the check if the constraint is violated or
not for each of the potentially added nodes ∆. This operation has a time and space complexity of"
END,0.8774928774928775,Published as a conference paper at ICLR 2022
END,0.8803418803418803,"Table G.1: Runtime in milliseconds for the Glucose and MiniSAT solver on 100 clean and perturbed
problem instances from 50-100"
END,0.8831908831908832,"Perturbed
DEL
ADC
SAT"
END,0.886039886039886,"Glucose
0.1897 ± 0.20
0.1944 ± 0.13
0.1315 ± 0.06
✓
0.0335 ± 0.03
0.0816 ± 0.11
0.1391 ± 0.08"
END,0.8888888888888888,"MiniSAT
0.1351 ± 0.07
0.1510 ± 0.11
0.0980 ± 0.05
✓
0.0189 ± 0.02
0.0587 ± 0.09
0.1383 ± 0.31"
END,0.8917378917378918,"O(n2) since we need to evaluate the distances to all nodes (except the special case where both nodes
are on the convex hull but are non-adjacent, see Corollary 2). However, checking the constraint has
the same complexity as the neural solver. Therefore, the overall space complexity turns out to be
O(∆n2) = O(n2) and the time complexity is O(n2s) with the number of attack steps s."
END,0.8945868945868946,"G
OFF-THE-SHELF SAT SOLVERS: TIME COST OF PERTURBED INSTANCES"
END,0.8974358974358975,"Comparison SAT Solvers. We test two SAT-solver’s runtime on all attacks for both clean an per-
turbed samples from the 50-100 dataset to better understand the difﬁculty of the samples for non-
neural SAT solvers. Table G.1 shows the mean as well as the standard deviation of the Glucose
(Audemard & Simon, 2009) as well as the MiniSAT (S¨orensson & Een, 2005) solver. The results
suggest that both solvers can more quickly ﬁnd that a sample is unsatisﬁable for perturbed samples
from both the DEL and the ADC attacks. For perturbed satisﬁable samples, the runtime increases
compared to the clean sample for the MiniSAT solver while Glucose solver needs about the same
time as for the clean sample to ﬁnd a solution."
END,0.9002849002849003,"H
CONVERGENCE NEUROSAT"
END,0.9031339031339032,"Complementary to Fig. 5, we also present the convergence for 100 randomly chosen satisﬁable
instances from the 10-40 dataset. We see that the observations drawn from Fig. 5 also hold here: (1)
rarely an instance is predicted as SAT despite the moderate budget of perturbing 5% of the literals
and (2) if NeuroSAT identiﬁes the sample as satisﬁable it requires more message passing steps to do
so."
END,0.905982905982906,"0
5
10
15
20
25
Message Passing Steps 0.0 0.5 1.0"
END,0.9088319088319088,Probability SAT
END,0.9116809116809117,"Clean
Perturbed"
END,0.9145299145299145,Figure H.1: Clean vs. perturbed: NeuroSAT’s prediction over the message passing steps.
END,0.9173789173789174,"I
EFFICIENCY NEUROSAT ATTACKS"
END,0.9202279202279202,"To better understand the efﬁciency of the NeuroSAT attacks and how many gradient update steps are
needed to ﬂip the model’s prediction, we show additional results for the attack strength over several
attack step budgets s in Fig. I.1. The indicated number of gradient update steps s are taken during
the attack, while retaining the learning rate from Table E.1 and early stopping on an instance level.
The results in Fig. I.1 show that only 1 gradient update step sufﬁces for the SAT attack to decrease
the accuracy from 79% to 26%. Compared to the results from Fig. 2, where the random attack draws
a single sample and only decreases the accuracy to 74%, the importance of the guidance through the
gradient update becomes apparent. For the DEL attack on the NeuroSAT model, the attack strength"
END,0.9230769230769231,Published as a conference paper at ICLR 2022
END,0.9259259259259259,"converges around 200 steps, showing again the imbalance between the labels with regards to how
many misclassiﬁcations the attack can force."
END,0.9287749287749287,"1
5
50
Attack Steps 0.2 0.4 0.6 0.8"
END,0.9316239316239316,Accuracy
END,0.9344729344729344,(a) SAT Attack
END,0.9373219373219374,"1
5
50
Attack Steps 0.8 0.9 1.0"
END,0.9401709401709402,Accuracy
END,0.9430199430199431,(b) DEL Attack
END,0.9458689458689459,Figure I.1: Attacks on NeuroSAT with varying number of attack steps s
END,0.9487179487179487,"Table I.1: Number of random
samples to match optimized
loss"
END,0.9515669515669516,"∆=
0.01
0.05"
END,0.9544159544159544,"SAT
1257
9532
DEL
749
4854"
END,0.9572649572649573,"In Table I.1 we further compare the efﬁciency of the attacks to the
random baseline. Random samples are drawn until the loss matches
or exceeds that of the adversarially perturbed sample. We introduce
a cut-off at 20000 samples and report the mean number of samples
for the SAT and DEL attack on the budgets ∆= 0.01 and ∆=
0.05."
END,0.9601139601139601,"We can observe that the additional computational cost of generat-
ing adversarial samples is justiﬁed, given the random baseline per-
forms comparably well only with several thousand random samples
drawn. While the efﬁciency of the proposed approach in general is important to consider for practical
relevance, it is not sufﬁcient to only compare to the random baseline in terms of sample efﬁciency.
There are several other aspects to consider, like additional computational and storage overhead for
a larger, denser dataset or to what extent a larger dataset of random or adversarial examples can
improve generalization."
END,0.9629629629629629,"J
QUALITATIVE RESULTS SAT"
END,0.9658119658119658,"To illustrate the changes to SAT problems, we provide an example for a successful attack on a small
and fairly simple problem (from SAT-3-10). While the model recognizes the problem below as
satisﬁable with 100% conﬁdence, the SAT attack perturbes it (modiﬁed clauses highlighted blue)
such that the model votes ’satisﬁable’ with only 0.59% conﬁdence."
END,0.9686609686609686,"Clean SAT Problem:
(¬4∨¬3∨1∨2)∧(1∨2∨3∨4)∧(¬4∨¬3∨¬2∨¬1)∧(¬3∨2)∧((¬2∨¬1∨3∨4)∧(¬3 ∨1 ∨2 ∨4)∧
(¬4 ∨¬2 ∨1 ∨3) ∧(¬4 ∨¬3 ∨¬2 ∨¬1) ∧((¬4 ∨¬2 ∨1 ∨3) ∧(¬2 ∨1 ∨4) ∧(¬4 ∨¬2 ∨1 ∨3) ∧
(¬3 ∨¬2 ∨¬1 ∨4) ∧(¬4 ∨¬1) ∧((¬2 ∨¬1 ∨3 ∨4) ∧(¬3 ∨¬2 ∨¬1) ∧(1 ∨2 ∨3 ∨4) ∧(1 ∨2 ∨3 ∨4) ∧
(¬4 ∨¬2 ∨1) ∧(¬4 ∨¬3 ∨¬2 ∨¬1) ∧(1 ∨2 ∨3 ∨4) ∧(¬3 ∨¬1 ∨2 ∨4) ∧((¬4 ∨¬3 ∨¬1 ∨2) ∧
(¬4 ∨1 ∨3) ∧(¬4 ∨¬2 ∨3) ∧(¬2 ∨¬1 ∨3 ∨4) ∧(¬3 ∨¬1 ∨2 ∨4) ∧(¬4 ∨¬1 ∨3) ∧
(¬4 ∨¬2 ∨¬1 ∨3) ∧(¬3 ∨¬2 ∨1) ∧(1 ∨2 ∨3 ∨4) ∧(¬4 ∨¬3 ∨2) ∧(¬3 ∨¬1 ∨4) ∧(¬3 ∨¬1)"
END,0.9715099715099715,"Perturbed SAT Problem:
(¬4 ∨¬3 ∨1 ∨2) ∧(1 ∨2 ∨3 ∨4) ∧(¬4 ∨¬3 ∨¬2 ∨¬1) ∧(¬3 ∨2) ∧(¬2 ∨¬1 ∨3 ∨4) ∧(¬3 ∨2 ∨4) ∧
(¬4 ∨¬2 ∨1 ∨3) ∧(¬4 ∨¬3 ∨¬2 ∨¬1) ∧(¬4 ∨¬2 ∨1 ∨3) ∧(¬2 ∨4) ∧(¬4 ∨¬2 ∨1 ∨3) ∧
(¬3 ∨¬2 ∨¬1 ∨4) ∧(¬4 ∨¬1) ∧(¬2 ∨¬1 ∨3 ∨4) ∧(¬3 ∨¬1) ∧(1 ∨2 ∨3 ∨4) ∧(1 ∨2 ∨3 ∨4) ∧
(¬4 ∨¬2 ∨1) ∧(¬4 ∨¬3 ∨¬2 ∨¬1) ∧(1 ∨2 ∨3 ∨4) ∧(¬3 ∨¬1 ∨2 ∨4) ∧(¬4 ∨¬3 ∨¬1 ∨2) ∧
(¬4 ∨3) ∧(¬4 ∨¬2 ∨3) ∧(¬2 ∨¬1 ∨3 ∨4) ∧(¬3 ∨¬1 ∨2 ∨4) ∧(¬4 ∨¬1 ∨3) ∧
(¬4 ∨¬2 ∨¬1 ∨3) ∧(¬3 ∨¬2 ∨1) ∧(1 ∨2 ∨3 ∨4) ∧(¬4 ∨¬3 ∨2) ∧(¬3 ∨¬1 ∨4) ∧(¬3 ∨¬1)"
END,0.9743589743589743,Published as a conference paper at ICLR 2022
END,0.9772079772079773,"K
QUALITATIVE RESULTS TSP"
END,0.98005698005698,"In this section we complement the ﬁgures Fig. 7 and Fig. 9 with further examples. In Fig. K.1, we plot further
examples for the attack on DTSP and further examples for ConvTSP in Fig. K.2."
END,0.9829059829059829,"(a)
(b)
(c)"
END,0.9857549857549858,Figure K.1: Examples of the optimal route Y and perturbed routes ˜Y for DecisionTSP.
END,0.9886039886039886,"Concorde
Perturbed
Clean
Adv. Points"
END,0.9914529914529915,"(a)
(b)
(c)"
END,0.9943019943019943,"(d)
(e)
(f)"
END,0.9971509971509972,"Figure K.2: Exemplary problem instances where the attack successfully changed the optimal route
for ConvTSP that show drastic changes of the prediction."
