Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003952569169960474,"Numerous physical systems are described by ordinary or partial differential equa-
tions whose solutions are given by holomorphic or meromorphic functions in the
complex domain. In many cases, only the magnitude of these functions are ob-
served on various points on the purely imaginary jω-axis since coherent mea-
surement of their phases is often expensive. However, it is desirable to retrieve
the lost phases from the magnitudes when possible. To this end, we propose a
physics-infused deep neural network based on the Blaschke products for phase re-
trieval. Inspired by the Helson and Sarason Theorem, we recover coefﬁcients of a
rational function of Blaschke products using a Blaschke Product Neural Network
(BPNN), based upon the magnitude observations as input. The resulting rational
function is then used for phase retrieval. We compare the BPNN to conventional
deep neural networks (NNs) on several phase retrieval problems, comprising both
synthetic and contemporary real-world problems (e.g., metamaterials for which
data collection requires substantial expertise and is time consuming). On each
phase retrieval problem, we compare against a population of conventional NNs
of varying size and hyperparameter settings. Even without any hyper-parameter
search, we ﬁnd that BPNNs consistently outperform the population of optimized
NNs in scarce data scenarios, and do so despite being much smaller models. The
results can in turn be applied to calculate the refractive index of metamaterials,
which is an important problem in emerging areas of material science."
INTRODUCTION,0.007905138339920948,"1
INTRODUCTION"
INTRODUCTION,0.011857707509881422,"Numerous physical systems are described by ordinary or partial differential equations whose solu-
tions are given by holomorphic or meromorphic functions f(z) = |f(z)| exp(jθ(z)) in the complex
domain, where |f(z)| and θ(z) are respectively the magnitude and phase of f(z). In many cases,
only |f(jω)|, the magnitude of f(z), is observed on a ﬁnite set of points (Ω) of the purely imaginary
jω-axis1, since the coherent measurement of their phases may be expensive (see Ch. 13 of Stern
(2016)) or require signiﬁcant expertise (King, 2009). However, it is desirable to retrieve the lost
phase θ(jω) from measurements of the magnitudes |f(jω)| for ω ∈Ω, since the phase often en-
codes important information. For example in both spectroscopy and optical imaging, estimation of
θ(jω) is often not possible. However, phase information determines the optical constants in the spec-
troscopy of materials (Lucarini, 2005), and is more important than |f(jω)| in optical imaging (Stern,
2016). The phase retrieval problem is also important in a number of other disciplines, including
holography (Fienup, 1982), x-ray crystallography (Nugent, 2007), astronomy (Stark, 1987), blind
deconvolution (Shechtman et al., 2015), and coherent diffractive imaging (Latychevskaia, 2018)."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.015810276679841896,"1For readers unfamiliar with phase retrieval problems and have questions about why function values are
only observed on the purely imaginary line, please refer to the explanation we have included in the Appendix."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.019762845849802372,Published as a conference paper at ICLR 2022
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.023715415019762844,"Many techniques for phase retrieval have been developed in the literature. The Kramers–Kronig
formula, see Lucarini (2005) (also known as Sokhotski–Plemelj formula), is noteworthy and is de-
rived by the application of the Hilbert Transform to log f(jω) = log |f(jω)| + jθ(jω). Unfortu-
nately, for phase retrieval, this elegant formula requires knowledge of the magnitude |f(jω)| along
the entire jω-axis. However, as mentioned above, acquiring these values is often impossible since
Ωis typically a ﬁnite set with a small number of elements. This severely limits the application of
Kramers–Kronig’s formula in many applications. Another approach is based on compressed sensing,
but it requires sparsity assumptions, and stringent assumptions on the design or projection matrix
(i.e., Restricted Isometry Property (RIP) (Candes, 2008), mutual incoherence (Foucart & Rauhut,
2013), etc) that often do not hold for meromorphic functions encountered in practice."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.02766798418972332,"To overcome the above limitations, phase retrieval can also be cast as a data-driven regression prob-
lem, where we wish to learn a mapping from the magnitude of a signal, X ∈RN, to its phase,
Θ ∈CN. If the number of frequencies (|Ω|) measured is N then the phase can be represented as
a vector of complex numbers Θ = [exp(θ(jω1)), · · · , exp(θ(jωN))] ∈CN, where exp(θ(jωi)) is
the phase at i-th frequency. Similarly, the signal’s corresponding magnitude vector can be given by
|f(jωi)|, i = 1, 2, · · · , N, and the goal is to infer Θ based upon X. We can then learn this mapping
using a data-driven model (see Figure 1 below for an example)."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.03162055335968379,"Figure 1: Example of the deep neural network approach to phase retrieval problem. FCNN refers
fully-connected neural network here but can be replaced by any machine learning model. The mag-
nitude is encoded into an input of size N while the output is an N-dimensional complex vector of
corresponding phases."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.03557312252964427,"With a sufﬁcient amount of training data, it is likely that a universal function approximator (i.e.
neural network), can learn the underlying function well within a tolerance range. For instance, deep
learning has shown great success on phase retrieval problems in the imaging area where abundant
training data is available (Sinha et al., 2017; Metzler et al., 2018). But sufﬁcient training data can be
expensive to acquire in many practical applications, or sometimes even impossible."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.039525691699604744,"In this paper, we depart from the existing literature by exploring the applications of physics-infused
neural networks to this setting. The infusion of physics reduces the number of required training
examples to a small enough amount that is practical for many measurements and design applications
of interest. The main idea of the approach is described next. Given the fact that any phase value
has a magnitude of 1 (i.e.| exp(jθ(jω))| = 1 for all ω), we observe that conditions of a Theorem of
Helson and Sarason are then satisﬁed indicating that exp(jθ(jω)) can be approximated by a rational
function whose numerator and denominator are both Blaschke products (Garcia et al., 2016). This
is then used to design a neural network for calculating coefﬁcients of these Blaschke products which
may be be used for phase retrieval."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.043478260869565216,"In Section 2, we brieﬂy review conventional phase retrieval methods, and existing applications of
the neural network to phase retrieval problems. In Section 3, we brieﬂy review Blaschke products
and the Helson-Sarason Theorem. In Section 4, we propose our approach for baking these results
into neural networks resulting in Blaschke Product Neural Networks. In Section 5, we provide
experimental results on various synthetic and metamaterials datasets along with a inclusive set of
baselines to demonstrate the performance improvements of our construction."
FOR READERS UNFAMILIAR WITH PHASE RETRIEVAL PROBLEMS AND HAVE QUESTIONS ABOUT WHY FUNCTION VALUES ARE,0.04743083003952569,Published as a conference paper at ICLR 2022
RELATED WORK,0.05138339920948617,"2
RELATED WORK"
RELATED WORK,0.05533596837944664,"We ﬁrst brieﬂy review existing methods of phase retrieval applicable to phase retrieval of analytic
and meromorphic functions of interest."
RELATED WORK,0.05928853754940711,"Perhaps the most important result in this area is the elegant calculation of Kramers and Kronig
(also known as Sokhotski–Plemelj formula) obtained by the application of the Hilbert Transform to
log f(jω) = log |f(jω)| + jθ(jω) and is given by"
RELATED WORK,0.06324110671936758,θ(jω) = 2ω
RELATED WORK,0.06719367588932806,"π P.V.
Z ∞ 0"
RELATED WORK,0.07114624505928854,log |f(jβ)|
RELATED WORK,0.07509881422924901,"β2 −ω2 dβ,"
RELATED WORK,0.07905138339920949,"where P.V. denotes Cauchy’s principal value. We note that this can also be derived by invoking the
Cauchy integration formula along a path containing the imaginary jω-axis."
RELATED WORK,0.08300395256916997,"However, there is a major obstacle in the application of this elegant formula. In many important
practical scenarios, the set Ωof frequencies (points on jω-axis) that we have measurements of
|f(jω)| has a small cardinal number. In such cases, the approximation of Kramers-Kronig integral
by a Riemann Sum produces unsatisfactory results. Other approaches used to analytically extend or
extrapolate |f(jω)| have also demonstrated limited success (Chalashkanov et al., 2012)."
RELATED WORK,0.08695652173913043,"Another approach to phase retrieval is by sparse representation. This approach assumes that it
is a priori known that f(z) has a sparse representation in a known dictionary of functions D =
{Φi(z), i = 1, 2, · · · , M} i.e. it can be written as a linear sum of elements of D with only a small
number of non-zero coefﬁcients. Under this assumption, many techniques are developed to recover
these coefﬁcients and the phase of f(z). Again, this is an interesting proposal but the underlying
assumptions may not hold in many problems of interest. In particular, neither a dictionary D may
be known nor classes of functions of interest may be sparse in any ﬁxed dictionary. Recently, some
new techniques based on deep learning have been developed to learn the structure of the underlying
signal which we want to recover its phase. That is, deep neural networks are used to encode the prior
knowledge on the structure of the signal instead of hard-coded assumptions such as sparsity (Jagatap
& Hegde, 2019). However, these methods require a signiﬁcant number of training samples from the
magnitude of the underlying signal, which limits the applicability of these approaches on material
science applications."
RELATED WORK,0.09090909090909091,"Recently, DL has demonstrated its capability to solve the phase-retrieval problem, especially in
the imaging area: Sinha et al. (2017) achieved lensless imaging for phase objects utilizing CNN.
Xue et al. (2019) constructed a bayesian convolution neural network to achieve phase imaging while
quantifying the uncertainty of the network output. Metzler et al. (2018) combined regularization-by-
denoising framework with a convolution neural network for noisy phase retrieval for images. Goy
et al. (2018) achieved low photon count phase retrieval with a Convolutional Neural Network(CNN)
as well. To avoid the difﬁculty of getting a large set of paired phase and intensity measurements,
Zhang et al. Zhang et al. (2021) used an unpaired dataset for phase-retrieval. Other works of Tayal
et al. (2020); Houhou et al. (2020) have also used deep learning for end-to-end phase recovery.
None of these works use the underlying physics in their deep learning approach or they require a
large number of training samples; hence, are not directly applicable to the physics applications we
are interested in this paper. We took a different path of applying physical prior knowledge to our
phase retrieval process."
RELATED WORK,0.09486166007905138,"One closely related line of work to ours is solving partial differential equations using neural net-
works (Dissanayake & Phan-Thien, 1994; Berg & Nystr¨om, 2018; Zhang & Bilige, 2019; Chen
et al., 2018). However, these methods typically need a lot of data and require careful selection of
hyper-parameters; additionally, they do not consider directly solving the phase retrieval problem.
Our approach, on the other hand, needs no speciﬁc hyper-parameter search algorithm, and achieves
high-accuracy result with a very limited training samples."
MATHEMATICAL BACKGROUND,0.09881422924901186,"3
MATHEMATICAL BACKGROUND"
MATHEMATICAL BACKGROUND,0.10276679841897234,"We brieﬂy review the mathematical background required in the rest of this paper. In particular, we
review Blaschke products and the Helson-Sarason Theorem. We note that the standard presentation
of the Blaschke products is motivated by inner functions in Hardy spaces presented over the unit disk"
MATHEMATICAL BACKGROUND,0.1067193675889328,Published as a conference paper at ICLR 2022
MATHEMATICAL BACKGROUND,0.11067193675889328,"whose boundary is the unit circle (Rudin, 1987; Garcia et al., 2016). In our case, we are interested in
the right hand half plane whose boundary is the jω-axis. These two are equivalent using the complex
Mobius transformation (z + 1)/(1 −z). In this light, our exposition is closer to Akutowicz (1956)."
MATHEMATICAL BACKGROUND,0.11462450592885376,"Let a1, a2, a3, · · · be an inﬁnite sequence of numbers for which ℜ(ak) ≥0 for all k for which ∞
X j=1"
MATHEMATICAL BACKGROUND,0.11857707509881422,"ℜ(aj)
|aj|2 + 1 < ∞,"
MATHEMATICAL BACKGROUND,0.1225296442687747,"then for any integer n, any complex function given by"
MATHEMATICAL BACKGROUND,0.12648221343873517,"B(z) =
1 + z 1 −z n Y j"
MATHEMATICAL BACKGROUND,0.13043478260869565,|aj −1|
MATHEMATICAL BACKGROUND,0.13438735177865613,"aj −1
|aj + 1|"
MATHEMATICAL BACKGROUND,0.1383399209486166,"aj + 1
z −aj
z + ¯aj"
MATHEMATICAL BACKGROUND,0.1422924901185771,"is referred to as a inﬁnite Blaschke Product. If the set {a1, a2, · · · , ak} is ﬁnite, B(z) is referred to
as a ﬁnite Blaschke product. For a ﬁnite Blaschke Product, it is clear that Pk
j=1
ℜ(aj)
|aj|2+1 < ∞since
there are only ﬁnite number of coefﬁcients aj, and therefore this condition can be dropped. Note
that roots with ℜ(ak) = 0 do not contribute to the product."
MATHEMATICAL BACKGROUND,0.14624505928853754,"For both ﬁnite and inﬁnite Blaschke products, we can see that |B(z)| = 1 for z = jω on the purely
imaginary axis 2. In this paper, we are only interested in ﬁnite Blaschke products because of the
following approximation Theorem of Helson and Sarason."
MATHEMATICAL BACKGROUND,0.15019762845849802,"Theorem 1 Let u(jω) be any continuous function on the jω-axis for which |u(jω)| = 1 for all ω.
Let ϵ > 0. Then there exists ﬁnite Blaschke products B1(jω) and B2(jω) such that"
MATHEMATICAL BACKGROUND,0.1541501976284585,∥u(jω) −B1(jω)
MATHEMATICAL BACKGROUND,0.15810276679841898,"B2(jω)∥∞< ϵ,"
MATHEMATICAL BACKGROUND,0.16205533596837945,where the ∥· ∥∞denotes the standard L∞norm computed over the jω axis.
MATHEMATICAL BACKGROUND,0.16600790513833993,"Let b(jω) = exp(jθ(jω)). We know that |b(jω)| = 1 because |b(jω)| = | exp(jθ(jω))| =
| cos(θ(jω)) + j sin(θ(jω))| =
p"
MATHEMATICAL BACKGROUND,0.16996047430830039,"cos(θ(jω))2 + sin(θ(jω))2 = 1. Applying the Helson-Sarason
Theorem, for analytic or meromorphic functions f(jω) (with no poles on the imaginary axis), the
phase b(jω) can be approximated by a rational function of ﬁnite Blaschke products. The main theme
of this paper is that these Blaschke products can be in turn computed using neural networks with a
small amount of data."
MATHEMATICAL BACKGROUND,0.17391304347826086,"From the expression of ﬁnite Blaschke products, we can see that Bi(jω) = AiPi(jω)/P ∗
i (jω) for
i = 1, 2 where Ai a complex number with |Ai| = 1 and Pi(jω) is a complex polynomial of ω
and P ∗
i (z) = ¯Pi(−¯z), where ¯Pi(jω) is constructed from Pi(jω) by replacing all its coefﬁcients
with their respective conjugates. Note that [P ∗(jω)]∗= P(jω). Let A = A1/A2 and represent
the Blaschke products B1 and B2 in the above theorem with the complex polynomials, we have the
approximating rational function written as AP1(jω)P ∗
2 (jω)/P ∗
1 (jω)P2(jω) with |A| = 1. Thus we
have the estimation for phase function b(jw) as:"
MATHEMATICAL BACKGROUND,0.17786561264822134,"b(jw) ≈AP1(jw)P ∗
2 (jw)
P ∗
1 (jw)P2(jw) = A P(jw)"
MATHEMATICAL BACKGROUND,0.18181818181818182,"P ∗(jw),
(1)"
MATHEMATICAL BACKGROUND,0.1857707509881423,"where P(z) = P1(z)P ∗
2 (z) and P1(z) and P ∗
2 (z) correspond to respectively factors of P(z) with
roots with positive and negative real parts."
BLASCHKE PRODUCT NEURAL NETWORK,0.18972332015810275,"3.1
BLASCHKE PRODUCT NEURAL NETWORK"
BLASCHKE PRODUCT NEURAL NETWORK,0.19367588932806323,"Inspired by the above, we model the phase function by equation 1, and propose the Blaschke Prod-
uct Neural Network (BPNN). A BPNN is a neural network with the magnitude of observations as
the input and the coefﬁcients of the rational function of Blaschke products as the output. In this
paper, we only present BPNNs which are fully connected neural networks (FCNNs), although such
a restriction is not necessary. We next describe the operation of BPNNs by discussing the forward
and backward pass mechanisms."
BLASCHKE PRODUCT NEURAL NETWORK,0.1976284584980237,2Details proving |B(jw)| = 1 can be found in the appendix
BLASCHKE PRODUCT NEURAL NETWORK,0.2015810276679842,Published as a conference paper at ICLR 2022
BLASCHKE PRODUCT NEURAL NETWORK,0.20553359683794467,"Forward Pass
The output of BPNN are Blaschke coefﬁcients of the complex polynomials that
are in turn used to construct predicted phase function ˜b(jω) according to equation 1. Without loss
of generality, let Ω= {ω1, ω2, · · · , ωN} be the set of frequencies for which experimental results
measuring the magnitude of signals |f(jω)| is performed, and their corresponding phases must be
retrieved. It can be assumed that ω0 < ω1 < · · · < ωN. The BPNN then produces estimates ˜b(jωi)
of the phase b(jωi). Combining with the known magnitude at ωi, this gives an estimate"
BLASCHKE PRODUCT NEURAL NETWORK,0.20948616600790515,˜f(jωi) = |f(jωi)|˜b(jωi).
BLASCHKE PRODUCT NEURAL NETWORK,0.2134387351778656,"Backward Pass
After predictions are generated with forward pass , the prediction error is com-
puted as L = 1 N N
X"
BLASCHKE PRODUCT NEURAL NETWORK,0.21739130434782608,"i=1
|f(jωi) −˜f(jωi)|2."
BLASCHKE PRODUCT NEURAL NETWORK,0.22134387351778656,"The training seeks to minimize L over all training samples in a standard manner using the stochas-
tic gradient descent algorithm and its variants. Cross-validation and testing can also be done in a
standard format."
BLASCHKE PRODUCT NEURAL NETWORK,0.22529644268774704,"An important hyper-parameter of interest is number of degrees for complex polynomials P1(jω) and
P2(jω). This hyper-parameter can either be cross-validated or chosen based on the practitioner’s
knowledge about the complexity of the speciﬁc phase retrieval problem. In our experiments, we
found that complex phase retrieval problems may require larger degree polynomials, and calculating
the value of these high degree polynomials over large frequency ranges (100THz - 1500THz and
above) may lead to the overﬂow problem. To overcome this problem, we propose a piece-wise
implementation of BPNN."
BLASCHKE PRODUCT NEURAL NETWORK,0.22924901185770752,"Piece-wise Implementation of BPNN
In this approach, given a large frequency range, the whole
frequency band is partitioned into non-overlapping contiguous segments. The Blaschke method
is then applied to each segment (see ﬁgure2b). The BPNN is then trained to output the respec-
tive Blaschke coefﬁcients for each segment. During training, the predictions of phases on all seg-
ments are concatenated. Then the following training is the same as non-piecewise BPNN. The exact
method of partitioning and degrees of corresponding Blaschke polynomials for each sequence are
hyper-parameters of choice. We have two example applications of piece-wise BPNNs to metamate-
rial datasets in our numerical experiments."
BLASCHKE PRODUCT NEURAL NETWORK,0.233201581027668,(a) Illustration of BPNNs
BLASCHKE PRODUCT NEURAL NETWORK,0.23715415019762845,(b) Illustration of Piece-wise Implementation of BPNN
BLASCHKE PRODUCT NEURAL NETWORK,0.24110671936758893,"Figure 2: Structure of BPNN (and piecewise BPNN): BPNN is an end-to-end model. Predictions
are constructed by the combination of the Blaschke coefﬁcients output of FCNN and values of input
magnitude. In the piece-wise BPNN case, FCNN generates the sets of Blaschke coefﬁcients for all
segments (partitions) of the frequency range, and predictions are made for each segment separately
and the results are concatenated."
BLASCHKE PRODUCT NEURAL NETWORK,0.2450592885375494,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2490118577075099,"4
EXPERIMENTS"
EXPERIMENTS,0.25296442687747034,"We conduct simulations on ﬁve phase retrieval datasets, two of which are synthetic, while the re-
maining three are metamaterial datasets. A detailed listing of the datasets is shown in Table 1. We
endeavor to test the BPNN on diverse, yet complete, datasets, and thus we choose datasets that
include both holomorphic and meromorphic complex functions. Additionally, the selected metama-
terial datasets are comprehensive, and well represent the discipline."
DATASETS,0.25691699604743085,"4.1
DATASETS"
DATASETS,0.2608695652173913,"Table 1: List of phase retrieval problems in Experiments section. (# Frequency) shows total number
of frequency points considered for the problem. Please see relevant sections for detailed descrip-
tions."
DATASETS,0.2648221343873518,"PHASE RETRIEVAL PROBLEM
TYPE
# FREQUENCY"
DATASETS,0.26877470355731226,"Polynomial ODE (4.1.1)
Synthetic
200
Lorentzian response function (A.7)
Synthetic
1000
Metamaterial absorber (4.1.3)
Metamaterial
1000
Huygens’ metasurface (4.1.4)
Metamaterial
2001
Membrane metasurface (4.1.5)
Metamaterial
2001"
DATASETS,0.2727272727272727,"4.1.1
SYNTHETIC I: POLYNOMIAL ODE"
DATASETS,0.2766798418972332,"We ﬁrst select points ω0 < ω1 < ω2 < · · · < ωN on the jω axis and ﬁx these values. The ﬁrst
synthetic dataset is the solution to a ordinary differential equation (ODE) given by:"
DATASETS,0.28063241106719367,"df
dz = p(z),"
DATASETS,0.2845849802371542,"where p(z) is a complex polynomial of degree m = 4 with its roots lying on the unit circle. We
uniformly sample roots of p(z) on the unit circle, then solve the above ODE on the jω-axis using the
Euler method with the real and imaginary part of the initial condition f(ω0) uniformly selected in
[−1, 1] for 1050 randomly generated initial values. The values (f(jω1), f(jω2), · · · , f(jωn)) are
then calculated and their magnitudes and phases are used in this experiment, of which 50 are used
for training and 1000 for testing."
DATASETS,0.2885375494071146,"4.1.2
SYNTHETIC II: LORENTZIAN RESPONSE FUNCTION"
DATASETS,0.2924901185770751,"The second synthetic dataset is generated by considering a causal model of a non-magnetic ma-
terial, with a frequency-dependent complex dielectric permittivity deﬁned as a sum of Lorentzian
oscillators. A simple Lorentzian model of the dielectric function is applicable to a wide range of res-
onant material systems with response functions that must obey Kramers-Kronig relations. Using the
transfer-matrix method, Markoˇs & Soukoulis (2008), the complex transmission coefﬁcient t(ω) can
be computed directly from the dielectric function via closed form equations.(Please see appendix
for more details). We generate a dataset of complex transmission coefﬁcients t by uniformly sam-
pling Lorentzian parameters for the frequency range 100-500 THz. We consider a ﬁxed set of four
Lorentzian oscillators, and generate 50 sample transmission spectra for training, and 2000 spectra
for the test dataset."
DATASETS,0.2964426877470356,"4.1.3
METAMATERIAL I: METAMATERIAL ABSORBER"
DATASETS,0.30039525691699603,"Metamaterials are structured materials consisting of periodic arrays of resonant elements that derive
their spectral properties from their unit cell geometry Smith et al. (2000). With effective material
properties that can consist of resonances in both the electric permittivity and magnetic permeability,
and hence more unknown material parameters, metamaterials often represent a more complicated
phase-retrieval problem than many conventional systems."
DATASETS,0.30434782608695654,Published as a conference paper at ICLR 2022
DATASETS,0.308300395256917,"The ﬁrst metamaterial dataset considers a metal-based absorber geometry with a metallic ground
plane backing a dielectric spacer layer, designed to operate in the millimeter wave (50-200 GHz)
frequency range. This metamaterial absorber (MMA) dataset has 55,000 pairs of MMA geometry
and complex frequency dependent reﬂection coefﬁcient r(ω) (t(ω) is zero due to the metal backing).
The data are obtained with commercial computational electromagnetic simulation software (CST
Microwave Studios)."
DATASETS,0.31225296442687744,"4.1.4
METAMATERIAL II: HUYGENS’ METASURFACE"
DATASETS,0.31620553359683795,"Huygens’ metasurfaces made of dielectric materials represent another class of metamaterials Decker
et al. (2015), with underlying properties that are different than their metallic counterparts, including
for example the lack of Ohmic losses Liu et al. (2017). When individual designs are combined
into supercells, all-dielectric metasurfaces can exhibit very rich spectral phenomena. The Huygens’
metasurface dataset consists of the complex reﬂection and transmission response derived from a
supercell of elliptical resonators adopted from the work of Deng et al. (2021). The phase information
can be inferred directly from the complex reﬂection or transmission. Out of a total of 1000 samples,
we randomly sample 80 for training and reserve the rest for testing."
DATASETS,0.3201581027667984,"4.1.5
METAMATERIAL III: MEMBRANE METASURFACE"
DATASETS,0.3241106719367589,"The membrane metasurfaces are similar to the Huygens’ metasurfaces. However, instead of having
four stand-alone elliptical resonators in the supercell, it consists of a thin slab of SiC with regions
of empty holes. The SiC slab ﬁlling the supercell has a thickness deﬁned by h, and a periodicity
p. In the SiC slab, we placed four elliptical holes where the elliptical SiC resonators were placed.
The four holes thus have the same geometrical parameters rma, rmi, and θ. The fourteen geomet-
rical parameters share the same range as the elliptical resonators dataset. Similarly, the input of
these datasets contains fourteen geometry parameters, and the outputs are complex reﬂection and
transmission with 2001 frequency points range from 100 THz to 500 THz. Out of a total of 7331
samples, we randomly sample 80 for training and reserve the rest for testing."
BENCHMARKING NEURAL NETWORKS,0.32806324110671936,"4.2
BENCHMARKING NEURAL NETWORKS"
BENCHMARKING NEURAL NETWORKS,0.33201581027667987,"Comparison is done on each dataset with training datasets of increasing size. For each phase retrieval
problem and each training dataset, we compare BPNN to a family of NNs of various sizes and
structures. For each structure, we search its hyper-parameters of training to guarantee we have NNs
close to optimal. And we record test error at each training epoch and report the best test error. We
note that recording test error is uncommon but this guarantees that the reported performance is close
to its limit."
BENCHMARKING NEURAL NETWORKS,0.3359683794466403,"We use NNs of various structures and all hidden layers of each of the NNs have the same width. The
range of hidden layer width in search is [32,64,128,256] and the number of hidden layers in search
is [1,2,3,4]. For hyper-parameters of training, we search dropout rate in the list of [0., 0.05, 0.1,
0.15, 0.2] and learning rate in the list of [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]. Each NN is trained with 6000
epochs after which training losses of all neural networks have converged. The training is performed
with 3 random initialization and we report the best test error. Huygens’ metasurface dataset is
more complex than the other datasets, so the range of hidden layer size in search is increased to
[64,128,256,512] while keeping the ranges of the other searched parameters the same. Unlike the
extensive searching for NNs, we do not search hyper-parameters of BPNN and only use one BPNN
for each phase retrieval problem. We also train BPNN with 3 random initialization on each dataset
and report its best test error. We use ReLU as activation function for both NNs and BPNNs while
Adam(Kingma & Ba, 2017) is used for training of all NNs and BPNNs."
RESULTS,0.33992094861660077,"4.3
RESULTS"
RESULTS,0.3438735177865613,"For each dataset, we compare the family of regular fully-connected neural networks, each with its
best searched hyper-parameters to BPNN without hyper-parameters search. For three meta-material
datasets, we further include four baselines: (1)The Kramers-Kronig method (KK-method): a con-
ventional phase retrieval method (Lucarini, 2005), (2)AAA-network algorithm: a established ratio-
nal approximation technique, similar to BPNN we used its functional form to approximate the phase"
RESULTS,0.34782608695652173,Published as a conference paper at ICLR 2022
RESULTS,0.35177865612648224,"Figure 3: Performance comparison of a number of neural networks of different structures, each with
its best hyper-parameters, and BPNN on two synthetic phase retrieval problems with training set of
various size."
RESULTS,0.3557312252964427,"function (Nakatsukasa et al., 2018b), (3)MUSIC algorithm: a data-driven function approximation
method that estimates functions as sum of exponentials (Schmidt, 1986a), and (4) Linear+BP: using
linear regressor instead of neural networks in BPNN.(Please see Appendix for more details about
KK-method, AAA algorithm and MUSIC algorithm.) All y-axes are plotted with a dB scale in base
10 (10 log10(e) where e is the MSE error) for visual clarity. For the legends in Fig. 3 we use entries
of the form (A, B), where A refers to the width (i.e., number of neurons) in each hidden layer of
the neural network being evaluated, and B refers to the depth of the neural network (i.e., number of
hidden layers)."
RESULTS,0.35968379446640314,"Performance of Baselines We note that both the KK and MUSIC methods exhibit poor perfor-
mance, because the KK method requires a large number of magnitude measurements to achieve ac-
curate estimates, while the MUSIC method expects that the target function is sparse in some basis,
which cannot be assumed for our problems. The Linear+BP model also performs substantially worse
than the the BPNN, suggesting that the relationship between the phase magnitude an the Blaschke
Product coefﬁcients is non-linear. Although representing the phase function with a Blaschke Product
decreases the complexity of the problem, mapping from magnitude to the Blaschke coefﬁcients still
appears to be beyond the capability of a linear regressor. In the following paragraphs we describe
results of each dataset in detail."
RESULTS,0.36363636363636365,"Synthetic Datasets We use BPNN of 4 roots with a fully-connected neural network of 2 hidden
layers, each of size 64 and no dropout. It is obvious from ﬁgure[3] that BPNN outperforms a family
of optimized neural networks by a large margin. This supports our theoretical justiﬁcation of using
BPNN when the phase functions are precisely meromorphic functions. The outstanding performance
of BPNN on the Lorentzian dataset implies its value on phase retrieval problems in metamaterials.
To conﬁrm this, we conduct more experiments on real metamaterial datasets."
RESULTS,0.3675889328063241,"MMA, ADM, Membrane For the MMA dataset, similar to synthetic datasets, we also use BPNN
with 4 roots while using a fully-connected neural network with 2 hidden layers of size 64. No
dropout nor weight-decay was added. Although without any regularization techniques for neu-
ral networks, BPNN still demonstrate superior performance to all the optimized neural networks.
Given that ADM dataset and Membrane dataset are much more complex than the synthetic datasets
and MMA dataset, we use a Piecewise BPNN with frequencies partitioned into 20 equal-length se-
quences and we assign each sequence with 3 roots. We note that this is the most naive split for
Piecewise BPNN and a more reasonable split can lead to improved performance. We ﬁnd that the
best errors of regular neural networks are close to BPNN’s best error. However, we also ﬁnd that
the median error of BPNN is much smaller than the median error of the best neural network (the
neural network with smallest test MSE error across all architectures). This shows that the BPNN is
making continued good predictions through the sacriﬁce of small amounts of poor predictions. This
property can be valuable when the prediction problem at hand is the on-or-off type due to BPNN’s
higher probability of making good predictions."
RESULTS,0.3715415019762846,Published as a conference paper at ICLR 2022
RESULTS,0.37549407114624506,"Figure 4: Performance comparison of family of neural networks, each with its best hyper-parameters
and BPNN (without hyper-parameter search) on MMA dataset along with four other baselines: the
KK method, linear regressor+Blaschke Product, MUSIC algorithm and the AAA method."
RESULTS,0.3794466403162055,"Figure 5: On the left is the performance comparison of family of neural networks, each with its best
hyper-parameters and BPNN along with four other baseline methods. On the right is the comparison
of median MSE loss of the best neural network in the large family and median MSE loss of BPNN.
(On top are performance comparisons on Huygens’ metasurface dataset; At bottom are performance
comparisons on membrane metasurface dataset.)"
CONCLUSION,0.383399209486166,"5
CONCLUSION"
CONCLUSION,0.38735177865612647,"We presented a physics-infused neural network referred to as Blaschke Product Neural Network
(BPNN) for phase retrieval problem of holomorphic and meromorphic functions. Through extensive
experiments on synthetic and metamaterial datasets, we have found that the data required for training
neural networks is signiﬁcantly less than expected when the underlying functions are holomorphic
or meromorphic. By baking the physics into neural networks, even less training data is required and
the performance is further improved."
CONCLUSION,0.391304347826087,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.3952569169960474,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.39920948616600793,"We upload four datasets. Due to restriction of maximum supplementary size of 100MB, we include
three datasets in supplementary: two synthetic datasets and Huygens’ metasurface dataset. Mem-
brane metasurface can be accessed with this link (https://ﬁgshare.com/s/966f196d1847269a447c).
Details about hyper-parameter search space and the process of searching can be found in the exper-
iment section. Included in the supplementary we also have our implementation of the BPNN and
neural networks by Pytorch with easy option of the hyper-parameters."
REPRODUCIBILITY STATEMENT,0.4031620553359684,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.40711462450592883,"This work was supported in part by the Ofﬁce of Naval Research (ONR) under grant number
N00014-21-1-2590. YD, OK, SR, JM, and WJP acknowledge funding from the Department of
Energy under U.S. Department of Energy (DOE) (DESC0014372)."
REFERENCES,0.41106719367588934,REFERENCES
REFERENCES,0.4150197628458498,"Edwin J. Akutowicz. On the determination of the phase of a fourier integral, i. Transactions of the
American Mathematical Society, 83(1):179–192, 1956. ISSN 00029947. URL http://www.
jstor.org/stable/1992910."
REFERENCES,0.4189723320158103,"Jens Berg and Kaj Nystr¨om. A uniﬁed deep artiﬁcial neural network approach to partial differential
equations in complex geometries. Neurocomputing, 317:28–41, 2018."
REFERENCES,0.42292490118577075,"Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.
Comptes rendus mathematique, 346(9-10):589–592, 2008."
REFERENCES,0.4268774703557312,"N. M. Chalashkanov, S. J. Dodd, and J. C. Fothergill. Limitations of kramers-kronig transform for
calculation of the DC conductance magnitude from dielectric measurements. In 2012 Annual
Report Conference on Electrical Insulation and Dielectric Phenomena. IEEE, October 2012.
doi: 10.1109/ceidp.2012.6378768.
URL https://doi.org/10.1109/ceidp.2012.
6378768."
REFERENCES,0.4308300395256917,"Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 6572–6583, 2018."
REFERENCES,0.43478260869565216,"Manuel Decker, Isabelle Staude, Matthias Falkner, Jason Dominguez, Dragomir N Neshev, Igal
Brener, Thomas Pertsch, and Yuri S Kivshar. High-efﬁciency dielectric huygens’ surfaces. Ad-
vanced Optical Materials, 3(6):813–820, 2015."
REFERENCES,0.43873517786561267,"Yang Deng, Simiao Ren, Kebin Fan, Jordan M Malof, and Willie J Padilla. Neural-adjoint method
for the inverse design of all-dielectric metasurfaces. Optics Express, 29(5):7526–7534, 2021."
REFERENCES,0.4426877470355731,"MWMG Dissanayake and Nhan Phan-Thien. Neural-network-based approximations for solving
partial differential equations. communications in Numerical Methods in Engineering, 10(3):195–
201, 1994."
REFERENCES,0.44664031620553357,"T. A Driscoll, N. Hale, and L. N. Trefethen. Chebfun Guide. Pafnuty Publications, 2014. URL
http://www.chebfun.org/docs/guide/."
REFERENCES,0.4505928853754941,"J. R. Fienup. Phase retrieval algorithms: a comparison. Applied Optics, 21(15):2758, August 1982.
doi: 10.1364/ao.21.002758. URL https://doi.org/10.1364/ao.21.002758."
REFERENCES,0.45454545454545453,"Simon Foucart and Holger Rauhut. An invitation to compressive sensing. In A mathematical intro-
duction to compressive sensing, pp. 1–39. Springer, 2013."
REFERENCES,0.45849802371541504,"Stephan Ramon Garcia, Javad Mashreghi, and William T. Ross. Finite blaschke products: a survey,
2016."
REFERENCES,0.4624505928853755,"Alexandre Goy, Kwabena Arthur, Shuai Li, and George Barbastathis. Low photon count phase
retrieval using deep learning. Physical review letters, 121(24):243902, 2018."
REFERENCES,0.466403162055336,Published as a conference paper at ICLR 2022
REFERENCES,0.47035573122529645,"P Grosse and V Offermann. Analysis of reﬂectance data using the kramers-kronig relations. Applied
Physics A, 52(2):138–144, 1991."
REFERENCES,0.4743083003952569,"Rola Houhou, Parijat Barman, Micheal Schmitt, Tobias Meyer, Juergen Popp, and Thomas Bocklitz.
Deep learning as phase retrieval tool for cars spectra. Optics Express, 28(14):21002–21024, 2020."
REFERENCES,0.4782608695652174,"Gauri Jagatap and Chinmay Hegde. Phase retrieval using untrained neural network priors. NeurIPS
Workshop on Deep Inverse Blind Submission, 2019."
REFERENCES,0.48221343873517786,"Frederick King. Hilbert transforms. Cambridge University Press, Cambridge, UK New York, 2009.
ISBN 978-0-521-51720-1."
REFERENCES,0.48616600790513836,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.4901185770750988,"Tatiana Latychevskaia. Iterative phase retrieval in coherent diffractive imaging: practical issues.
Applied Optics, 57(25):7187, August 2018. doi: 10.1364/ao.57.007187. URL https://doi.
org/10.1364/ao.57.007187."
REFERENCES,0.49407114624505927,"Xinyu Liu, Kebin Fan, Ilya V Shadrivov, and Willie J Padilla. Experimental realization of a terahertz
all-dielectric metasurface absorber. Optics express, 25(1):191–201, 2017."
REFERENCES,0.4980237154150198,"Valerio Lucarini. Kramers-Kronig relations in optical materials research. Springer, Berlin New
York, 2005. ISBN 3-540-23673-2."
REFERENCES,0.5019762845849802,"Peter Markoˇs and Costas M. Soukoulis. Wave Propagation: From Electrons to Photonic Crystals
and Left-Handed Materials. Princeton University Press, stu - student edition edition, 2008. ISBN
9780691130033. URL http://www.jstor.org/stable/j.ctt7s2wj."
REFERENCES,0.5059288537549407,"Christopher Metzler, Phillip Schniter, Ashok Veeraraghavan, et al. prdeep: robust phase retrieval
with a ﬂexible deep network. In International Conference on Machine Learning, pp. 3501–3510.
PMLR, 2018."
REFERENCES,0.5098814229249012,"Yuji Nakatsukasa, Olivier S`ete, and Lloyd N Trefethen. The aaa algorithm for rational approxima-
tion. SIAM Journal on Scientiﬁc Computing, 40(3):A1494–A1522, 2018a."
REFERENCES,0.5138339920948617,"Yuji Nakatsukasa, Olivier S`ete, and Lloyd N. Trefethen. The aaa algorithm for rational approxima-
tion. SIAM Journal on Scientiﬁc Computing, 40(3):A1494–A1522, Jan 2018b. ISSN 1095-7197.
doi: 10.1137/16m1106122. URL http://dx.doi.org/10.1137/16M1106122."
REFERENCES,0.5177865612648221,"Keith A. Nugent. X-ray noninterferometric phase imaging: a uniﬁed picture. Journal of the Optical
Society of America A, 24(2):536, February 2007. doi: 10.1364/josaa.24.000536. URL https:
//doi.org/10.1364/josaa.24.000536."
REFERENCES,0.5217391304347826,"Walter Rudin.
Real and Complex Analysis, 3rd Ed.
McGraw-Hill, Inc., USA, 1987.
ISBN
0070542341."
REFERENCES,0.525691699604743,"R. Schmidt.
Multiple emitter location and signal parameter estimation.
IEEE Transactions on
Antennas and Propagation, 34(3):276–280, 1986a. doi: 10.1109/TAP.1986.1143830."
REFERENCES,0.5296442687747036,"Ralph Schmidt. Multiple emitter location and signal parameter estimation. IEEE transactions on
antennas and propagation, 34(3):276–280, 1986b."
REFERENCES,0.5335968379446641,"Yoav Shechtman, Yonina C. Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and
Mordechai Segev. Phase retrieval with application to optical imaging: A contemporary overview.
IEEE Signal Processing Magazine, 32(3):87–109, May 2015. doi: 10.1109/msp.2014.2352673.
URL https://doi.org/10.1109/msp.2014.2352673."
REFERENCES,0.5375494071146245,"Ayan Sinha, Justin Lee, Shuai Li, and George Barbastathis. Lensless computational imaging through
deep learning. Optica, 4(9):1117–1125, 2017."
REFERENCES,0.541501976284585,"David R Smith, Willie J Padilla, DC Vier, Syrus C Nemat-Nasser, and Seldon Schultz. Composite
medium with simultaneously negative permeability and permittivity. Physical review letters, 84
(18):4184, 2000."
REFERENCES,0.5454545454545454,"Henry Stark. Image recovery. Academic Press, Orlando, 1987. ISBN 9780323145978."
REFERENCES,0.549407114624506,Published as a conference paper at ICLR 2022
REFERENCES,0.5533596837944664,"Adrian Stern (ed.). Optical Compressive Imaging. CRC Press, November 2016. doi: 10.1201/
9781315371474. URL https://doi.org/10.1201/9781315371474."
REFERENCES,0.5573122529644269,"Kshitij Tayal, Chieh-Hsin Lai, Vipin Kumar, and Ju Sun. Inverse problems, deep learning, and
symmetry breaking. arXiv preprint arXiv:2003.09077, 2020."
REFERENCES,0.5612648221343873,"Yujia Xue, Shiyi Cheng, Yunzhe Li, and Lei Tian. Reliable deep-learning-based phase imaging with
uncertainty quantiﬁcation. Optica, 6(5):618–629, 2019."
REFERENCES,0.5652173913043478,"Run-Fa Zhang and Sudao Bilige. Bilinear neural network method to obtain the exact analytical solu-
tions of nonlinear partial differential equations and its application to p-gbkp equation. Nonlinear
Dynamics, 95(4):3041–3048, 2019."
REFERENCES,0.5691699604743083,"Yuhe Zhang, Mike Andreas Noack, Patrik Vagovic, Kamel Fezzaa, Francisco Garcia-Moreno, To-
bias Ritschel, and Pablo Villanueva-Perez. Phasegan: a deep-learning phase-retrieval approach
for unpaired datasets. Optics Express, 29(13):19593–19604, 2021."
REFERENCES,0.5731225296442688,Published as a conference paper at ICLR 2022
REFERENCES,0.5770750988142292,"A
APPENDIX"
REFERENCES,0.5810276679841897,"A.1
OBSERVATIONS OF PHASE FUNCTION VALUES"
REFERENCES,0.5849802371541502,"Here we provide a short explanation for the reason why the function values are only available on the
imaginary line. In physics, electrical engineering and other ﬁelds, solutions to many linear PDEs
with constant coefﬁcients, and also many other measurements are done in the frequency domain.
This in principle is achieved by multiplying a real signal x(t) by sinusoidals cos(ωt) and sin(ωt),
and integrating over time (Calculating Cosine and Sine Transforms) for various values of ω. If x(t)
is a real physical signal, these cosine and sine transforms are real and imaginary parts of X(jω), the
complex Fourier transform of x(t). Thus X(jω) gives the value of x(·) over the imaginary axis."
REFERENCES,0.5889328063241107,"A.2
|B(jw)| = 1"
REFERENCES,0.5928853754940712,"We provide the details proving the norm of Blashke product is 1 if z = jw. To show that it has a
norm of one, we should show that all component (that are multiplied) would have a norm of one
individually:"
REFERENCES,0.5968379446640316,"B(z) =
1 + z 1 −z n Y j"
REFERENCES,0.6007905138339921,|aj −1|
REFERENCES,0.6047430830039525,"aj −1
|aj + 1|"
REFERENCES,0.6086956521739131,"aj + 1
z −aj
z + ¯aj"
REFERENCES,0.6126482213438735,"B(z) = B1(z) ∗B2(z) ∗B3(z)
|B(z)| = |B1(z)| ∗|B2(z)| ∗|B3(z)|"
REFERENCES,0.616600790513834,"B1(z) =
1 + z 1 −z"
REFERENCES,0.6205533596837944,"n
; B2(z) =
Y j"
REFERENCES,0.6245059288537549,|aj −1|
REFERENCES,0.6284584980237155,"aj −1
|aj + 1|"
REFERENCES,0.6324110671936759,"aj + 1 ; B3(z) =
Y j"
REFERENCES,0.6363636363636364,"z −aj
z + ¯aj"
REFERENCES,0.6403162055335968,"|B1(jw)| = |
1 + jw 1 −jw n
|"
REFERENCES,0.6442687747035574,"= |
1 + jw 1 −jw 
|n"
REFERENCES,0.6482213438735178,"=
|1 + jw|"
REFERENCES,0.6521739130434783,|1 −jw| n
REFERENCES,0.6561264822134387,"=
(1 + w2)0.5"
REFERENCES,0.6600790513833992,(1 + w2)0.5 n = 1
REFERENCES,0.6640316205533597,"|B2(jw)| = |
Y j"
REFERENCES,0.6679841897233202,|aj −1|
REFERENCES,0.6719367588932806,"aj −1
|aj + 1|"
REFERENCES,0.6758893280632411,"aj + 1 | =
Y j"
REFERENCES,0.6798418972332015,||aj −1||
REFERENCES,0.6837944664031621,"|aj −1|
||aj + 1||"
REFERENCES,0.6877470355731226,"|aj + 1| =
Y"
REFERENCES,0.691699604743083,"j
1 ∗1 = 1"
REFERENCES,0.6956521739130435,"|B3(jw)| = |
Y j"
REFERENCES,0.6996047430830039,"z −aj
z + ¯aj
| =
Y j"
REFERENCES,0.7035573122529645,"|jw −aj|
|jw + ¯aj| =
Y j"
REFERENCES,0.7075098814229249,"|jw −aj −bjj|
|jw + aj −bjj|(assume aj = aj + bjj) =
Y j"
REFERENCES,0.7114624505928854,"(a2
j + (bj −w)2)0.5"
REFERENCES,0.7154150197628458,"(a2
j + (w −bj)2)0.5"
REFERENCES,0.7193675889328063,Published as a conference paper at ICLR 2022 = 1
REFERENCES,0.7233201581027668,"A.3
HYPER-PARAMETERS OF PIECEWISE BPNN"
REFERENCES,0.7272727272727273,"In order to provide analysis and some insights on hyper-parameters of Piecewise BPNN, we conduct
experiments on the two most complex datasets, Hyugens’ metasurface dataset and membrane meta-
surface dataset, to have empirical results regarding the effect of hyper-parameters (e.g number of
segments and number of roots per segment) on performance. We use training dataset of size 50 for
both datasets and ﬁx BPNN’s neural networks to have 2 hidden layers and hidden size of 64 for each
layer. From ﬁgure[6] we can see that Piecewise BPNN’s performance is robust to hyper-parameters:
a wide range of different settings of hyper-parameters can generate satisfactory results as long as the
total number of roots is above the threshold necessary. Similarly, the Piecewise BPNN is also robust
to overﬁtting caused by high number of roots: while the optimal total number of roots appears in the
range of 60-80, overshooting total number of roots to 160 or even 200 doesn’t cause a sharp increase
in test error. We believe this is a strong indication that the Piecewise BPNN learns the necessary
complexity and thus zeros out the unnecessary degrees"
REFERENCES,0.7312252964426877,"Figure 6: On the left is the heatmap of minimum test MSE (mean squared error) of Piecewise BPNN
with different hyper-parameter conﬁgurations on Hyugen’s metasurface dataset. On the right is the
heatmap of minimum test MSE of Piecewise BPNN with different hyper-parameter conﬁgurations
on membrane metasurface dataset. Each minimum MSE is calculated with 3 runs of training from
different initialization."
REFERENCES,0.7351778656126482,"A.4
BASELINE MODEL 1: KRAMERS–KRONIG"
REFERENCES,0.7391304347826086,"The Kramers–Kronig relations, implied by causality of a stable physics system, are often used to
convert between imaginary parts and real parts of physical functions such as dielectric functions,
susceptibility, and reﬂection coefﬁcient. We adopt the notation from Grosse & Offermann (1991),
since the metamaterials problems included in the main text focus on the phase retrieval tasks on
spectral data, including reﬂectance. In general, the Kramers–Kronig relations for complex reﬂection
coefﬁcient are given by:"
REFERENCES,0.7430830039525692,r′(ω) = 1
REFERENCES,0.7470355731225297,"π P
Z −∞ ∞"
REFERENCES,0.7509881422924901,"r′′(ω′)
ω′ −ω dω′"
REFERENCES,0.7549407114624506,r′′(ω) = −1
REFERENCES,0.758893280632411,"π P
Z −∞ ∞"
REFERENCES,0.7628458498023716,"r′(ω′)
ω′ −ω dω′"
REFERENCES,0.766798418972332,which r′(ω) and r′′(ω) are real and complex part of reﬂection coefﬁcient r(ω) respectively.
REFERENCES,0.7707509881422925,"In practice, r′(ω) and r′′(ω) are often unknown. The reﬂectance R(ω) = |r(ω)|2 is the physical
quantity that is directly measurable by intensity measurement, but the phase information is lost."
REFERENCES,0.7747035573122529,Published as a conference paper at ICLR 2022
REFERENCES,0.7786561264822134,"Therefore, it is beneﬁcial to derive phase φ(ω) from reﬂectance R(ω). A general form of complex
reﬂection coefﬁcient can be written as r(ω) =
p"
REFERENCES,0.782608695652174,R(ω) exp iφ(ω). We can infer that:
REFERENCES,0.7865612648221344,ln r(ω) = 0.5 ln R(ω) + iφ(ω)
REFERENCES,0.7905138339920948,"If Kramers-Kronig relations are valid for ln r(ω), we can ﬁrst measure R(ω) from zero frequency
to the highest measurable frequency. Then we can apply Kramers-Kronig relations on 0.5 ln R(ω)
as real part of ln r(ω) to infer the imaginary part φ(ω). In our implementation, we follow the
above procedure to derive ln r(ω), and therefore can get the Kramers-Kronig reconstructed complex
reﬂection coefﬁcient ˆr(ω). Since the loss metric in the main text is the averaged mean square error
of the real and imaginary parts of the reﬂection coefﬁcient, ˆr
′(ω) and ˆr
′′(ω) are further taken from
ˆr(ω) to be compared with r′(ω) and r′′(ω)."
REFERENCES,0.7944664031620553,"The above procedure is built on the premise that there is causality for ln r(ω), but ln r(ω) is not a real
physical function that is proven to be governed by causality. Thus, we cannot prove that Kramers-
Kronig relations is valid on ln r(ω). Furthermore, the reﬂection spectrum in the metamaterials
dataset are typically in a ﬁnite frequency range that degrades the performance of the Kramers-Kronig
relations. Nonetheless, we provided the Kramers-Kronig relations as a baseline to perform phase
retrieval."
REFERENCES,0.7984189723320159,"A.5
BASELINE MODEL 2: AAA-NETWORK ALGORITHM DETAILS"
REFERENCES,0.8023715415019763,"Here we supply the details of the Adaptive Antoulas-Anderson (AAA) -network baseline we in-
cluded in our main text. Following the notation of Nakatsukasa et al. (2018a), it uses rational
barycentric representation of rational apprixmation, which approximate an arbitrary function as ra-
tional function of type (m, m):"
REFERENCES,0.8063241106719368,r(z) = n(z)
REFERENCES,0.8102766798418972,"d(z) = m
X j=1"
REFERENCES,0.8142292490118577,"wjfj
z −zj m
X j=1"
REFERENCES,0.8181818181818182,"wj
z −zj"
REFERENCES,0.8221343873517787,"where zj, wj, fj are support points, weights and value parameters of this approximation. It approx-
imates a complex function over a set of complex support points using an iterative greedy selection
and uses linear least-squares to solve for weights."
REFERENCES,0.8260869565217391,"Although AAA also utilizes rational approximation, there are a couple of fundamental differences
between AAA and our Blashke product approximation. (1) The functional form difference: AAA
uses the barycentric representation, which although limited to type (m, m) rational, covers a much
larger functional space compared to our Blashke product, that is specialized to approximate the phase
function on the unit circle according to the Helson-Sarason Theorem. (2) The ﬁtting process: AAA
uses a greedy solution to pick the support point ﬁrst and then solve the least square for weights and
values. Our Blashke product ﬁts aj simultaneously and does not require iterative steps internally
(although the actual ﬁtting is done using gradient-based methods, which requires iterative steps).
This means the AAA is difﬁcult to build into a neural network and get gradient feedback from the
ﬁt, unlike our BPNN approach."
REFERENCES,0.8300395256916996,"With AAA, we can successfully approximate the phase function using three sets of barycentric
parameters (also known as AAA parameters). However, this process requires knowledge of the
phase function or at least the ability to sample points on the phase function. Therefore a naive AAA
alone is not able to be applied to our application of phase retrieval, which not only requires the
approximation of phase function but also the extrapolation of the mapping between magnitude and
the approximation parameters."
REFERENCES,0.83399209486166,"For programming implementation, we are using the existing package Driscoll et al. (2014)3. Setting
the number of support points comparable to our BP (so that they have similar degrees of freedom),
we ﬁrst run the AAA algorithm for all of our training sets (number of points ranging from 1 to
50) and get the training AAA parameters. Then we used a neural network to model the relation-
ship between the measured magnitude to the AAA parameters for the training data. The network
structure we used was 64 neurons with 2 hidden layers with Relu activation. After the network is"
REFERENCES,0.8379446640316206,3We used this public Python version: https://github.com/c-f-h/baryrat
REFERENCES,0.841897233201581,Published as a conference paper at ICLR 2022
REFERENCES,0.8458498023715415,"properly trained (reach convergence for 300 epochs), we input the testing spectra magnitude and
get the inferred AAA parameters for the test spectra. Then we simply reconstructed the barycentric
representation and compared the reconstructed phase function with the ground truth to get the MSE
measure."
REFERENCES,0.849802371541502,"As graphs in the main text suggested, all of the AAA networks performed poorly compared to BP
and to naive NN results. This is expected due to following reasons:"
REFERENCES,0.8537549407114624,"1. The functional form of barycentric rational functions lacks actual physics meaning:
Encompassing all (m, m) rational functions, they are too large and not restrictive enough
for the phase function. The lack of actual physical meaning in the rational approximation
unlike our Blashke product (which takes into account the unit circle property of phase
functions) is the most important reason we suspect this not working"
REFERENCES,0.857707509881423,"2. The two-stage process error: There are two stages in our AAA-network baseline, a AAA
parameter ﬁtting stage and a network mapping learning stage. The error of each stage
would compound and damage the overall performance"
REFERENCES,0.8616600790513834,"3. One-to-many issue: Unlike the Blashke product where the parameters form an ordered
set, the parameters of the barycentric representation of AAA form an unordered set. The
order is important for neural networks to learn a meaningful mapping as it would not have
one-to-many problems, which damages the training performance"
REFERENCES,0.8656126482213439,"A.6
BASELINE MODEL 3: MUSIC ALGORITHM DETAILS"
REFERENCES,0.8695652173913043,"Multiple Signal Classiﬁcation (MUSIC) algorithm Schmidt (1986b) is a signal processing algorithm
popular in frequency ﬁnding and direction ﬁnding where the signal is linear combination of multiple
signal source of different frequencies and/or directions. MUSIC assumes that the signal vector is
consist of p complex exponentials whose frequencies w are unknown."
REFERENCES,0.8735177865612648,"Since MUSIC is not originally applied to phase retrieval problems and there is no widely single way
to do phase retrieval, we would describe fully the process of our MUSIC phase retrieval algorithm
here. First, we would assume that our signal x is consist of the sum of p complex exponentials with
unknown frequency w and noise term n:"
REFERENCES,0.8774703557312253,x = As + n A = 
REFERENCES,0.8814229249011858,"


"
REFERENCES,0.8853754940711462,"1
1
...
1
ejw1
ejw2
...
ejwp
ej2w1
ej2w2
...
ej2wp
...
...
...
...
ej(M−1)w1
ej(M−1)w2
...
ej(M−1)wp "
REFERENCES,0.8893280632411067,"


, s = [s1, ..., sp]T"
REFERENCES,0.8932806324110671,"where A is the Vandermonde matrix of steering vectors and s is the amplitude vector. In the assump-
tion the actual number of sources is much lower than the observation, therefore p ¡ M. With this
assumption, we would make another assumption that the signal comes from the same distribution,
namely their autocorrelation matrix (M by M) should be a constant. Therefore, we would estimate
this autocorrelation using the average of the sampled correlation matrix:"
REFERENCES,0.8972332015810277,"ˆ
Rx = 1 N XXH"
REFERENCES,0.9011857707509882,"Now with the estimate of autocorrelation, we estimate the frequency of this matrix using the
eigenspace method, where the eigenvectors associated with a small eigenvalue would be treated as
the noise subspace and would be used to recover the signal frequency. Speciﬁcally, we decomposite
the ˆ
Rx into eigenvalues and eigenvectors, took the smallest (M-p) eigenvalues and their correspond-
ing eigenvectors to form noise subspace UN. Since the eigenvectors from a Hermitian matrix should
be orthogonal to each other, meaning any pure signal from signal space would be orthogonal to all
the vectors that span the noise subspace. Using a squared norm distance, for a signal e, the distance:"
REFERENCES,0.9051383399209486,"d2 = ||U H
N e||2 = eHUNU H
N e = M
X"
REFERENCES,0.9090909090909091,"i=p+1
|eHvi|2"
REFERENCES,0.9130434782608695,Published as a conference paper at ICLR 2022
REFERENCES,0.9169960474308301,"should be zero for all pure signals. Or, the inverse of d2 should be the peaks in the w space when
we sweep through a range of possible w values. Now, we sweep the w space with a ﬁne grid and"
REFERENCES,0.9209486166007905,the peaks in the 1
REFERENCES,0.924901185770751,"d2 would be our w values. Therefore, we can construct the A matrix by the top-p
peaks in the curve."
REFERENCES,0.9288537549407114,"The above is a standard MUSIC algorithm to ﬁnd the signal frequency (assuming the signal is a
sum of exponential), now with the frequency of the signal found, ﬁnding the actual signal x would
be equivalent to ﬁnding the corresponding amplitude vector s. Therefore we constructed our phase
retrieval problem as an optimization problem:"
REFERENCES,0.932806324110672,"x∗= A arg min
s"
REFERENCES,0.9367588932806324," 
||ˆx| −|xgt||2
= A arg min
s"
REFERENCES,0.9407114624505929," 
||As| −|xgt||22
"
REFERENCES,0.9446640316205533,"Where the |xgt| is the ground truth magnitude or experimental measurements. For each test phase
retrieval problem (each test magnitude spectra), we would need to run the optimization again using
the estimated A matrix. We tried the Newton raphson method for this operation but it did not
converge even if we set a larger tolerance. Therefore we used the PyTorch framework (without any
neural network) and utilized the parallel, GPU-based gradient optimization ability. We constructed
the computational graph using the above equation and randomly initialized 1024 initial guesses
and used Adam optimizer for 300 epoch till the loss converges. Afterward we would take the best
performing single ﬁt among the 1024 candidates as our ﬁnal answer for this single magnitude. Due
to time constraints (300 epoch with 1024 initialization for each of our test cases), we did 500 test
cases instead of the full dataset like other methods."
REFERENCES,0.9486166007905138,"A.7
DETAILS ON METAMATERIALS DATASET"
REFERENCES,0.9525691699604744,"Metamaterial absorber:
The input parameters include the four geometric dimensions of the top
metal layer a, w1, w2, g, material properties of a top dielectric layer ϵ, tan δ, the conductivity of
the metal σmetal, and thicknesses for all layers dcap, dsub, dm1, dm2. The output is the complex
reﬂection coefﬁcient r(ω), with 1000 frequency points for each of the real and imaginary part of
r(ω) evaluated from 0 – 200 GHz."
REFERENCES,0.9565217391304348,"Membrane Metasurface:
The metasurface supercell geometry has a periodicity p, including four
elliptical SiC resonators with consistent height h. Each elliptical resonator has a major axis radius
rma, a minor axis radius rmi, and a rotational angle θ. The fourteen geometry parameters deﬁne
inputs of computational simulations and yield complex reﬂection and transmission responses. This
ADM dataset is particularly interested in the frequency-dependent reﬂection and transmission range
from 100 - 500 THz."
REFERENCES,0.9604743083003953,"Synthetic II: Lorentzian Response Function
The second synthetic dataset is generated by con-
sidering a causal model of a non-magnetic material, with a frequency-dependent complex dielectric
permittivity deﬁned as a sum of Lorentzian oscillators:"
REFERENCES,0.9644268774703557,"ϵr(ω) = ϵ∞+ N
X i"
REFERENCES,0.9683794466403162,"ω2
ϵ,p,i
ω2
ϵ,o,i −ω2 −j ∗ωϵ,s,iω ."
REFERENCES,0.9723320158102767,"A simple Lorentzian model of the dielectric function is applicable to a wide range of resonant ma-
terial systems with response functions that must obey Kramers-Kronig relations. Using the transfer-
matrix method, Markoˇs & Soukoulis (2008), the complex transmission coefﬁcient t(ω) can be com-
puted directly from the dielectric function via the closed form equation:"
REFERENCES,0.9762845849802372,"t(ω) =

cos
nωl c 
−i 2"
REFERENCES,0.9802371541501976,"
z + 1 z"
REFERENCES,0.9841897233201581,"
sin
nωl c"
REFERENCES,0.9881422924901185,"−1
,"
REFERENCES,0.9920948616600791,"where n = √ϵr, z =
p"
REFERENCES,0.9960474308300395,"1/ϵr, and c, l are pre-deﬁned physical constants. We generate a dataset of
complex transmission coefﬁcients t by uniformly sampling Lorentzian parameters for ϵr optimized
for the frequency range 100-500 THz. We consider a ﬁxed set of four Lorentzian oscillators, and
generate 50 sample transmission spectra for training, and 2000 spectra for the test dataset."
