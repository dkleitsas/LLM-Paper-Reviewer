Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012903225806451613,"Continual learning of partially similar tasks poses a challenge for artificial neural
networks, as task similarity presents both an opportunity for knowledge transfer and
a risk of interference and catastrophic forgetting. However, it remains unclear how
task similarity in input features and readout patterns influences knowledge transfer
and forgetting, as well as how they interact with common algorithms for continual
learning. Here, we develop a linear teacher-student model with latent structure
and show analytically that high input feature similarity coupled with low readout
similarity is catastrophic for both knowledge transfer and retention. Conversely,
the opposite scenario is relatively benign. Our analysis further reveals that task-
dependent activity gating improves knowledge retention at the expense of transfer,
while task-dependent plasticity gating does not affect either retention or transfer
performance at the over-parameterized limit. In contrast, weight regularization
based on the Fisher information metric significantly improves retention, regardless
of task similarity, without compromising transfer performance. Nevertheless, its
diagonal approximation and regularization in the Euclidean space are much less
robust against task similarity. We demonstrate consistent results in a permuted
MNIST task with latent variables. Overall, this work provides insights into when
continual learning is difficult and how to mitigate it."
INTRODUCTION,0.0025806451612903226,"1
Introduction"
INTRODUCTION,0.003870967741935484,"Artificial neural networks surpass human capabilities in various domains, yet struggle with continual
learning. These networks tend to forget previously learned tasks when trained sequentially—a
problem known as catastrophic forgetting [44, 16, 23, 32]. This phenomenon affects not only
supervised training of feedforward networks but also extends to recurrent neural networks [10],
reinforcement learning tasks [30], and fine-tuning of large language models [41]. Many algorithms
for mitigating catastrophic forgetting have been developed previously, including rehearsal techniques
[48, 49, 59], weight regularization [30, 36, 67], and activity-gating methods [15, 54, 42, 56], among
others [57, 50, 63, 21]. However, these methods often hinder forward and backward knowledge
transfer [28, 39, 29], and thus it remains unclear how to achieve knowledge transfer and retention
simultaneously."
INTRODUCTION,0.005161290322580645,"A key factor for continual learning is task similarity. If two subsequent tasks are similar, there is a
potential for a knowledge transfer from one task to another, but the risk of interference also becomes
high [47, 28, 35, 11, 39]. The impact of task similarity on transfer and retention performance is
particularly complicated because two tasks can be similar in different manners [65, 32]. Sometimes
familiar input features need to be associated with novel output patterns, but at other times, novel
input features need to be associated with familiar output patterns. Previous works observed that these"
INTRODUCTION,0.0064516129032258064,"two scenarios influence continual learning differently [35], yet the impact of the input and output
similarity on knowledge transfer and retention has not been well understood. Moreover, it remains
unknown how the task similarity interacts with algorithms for continual learning such as activity
gating or weight regularization."
INTRODUCTION,0.007741935483870968,"To gain insight into these questions, in this work, we investigate how transfer and retention perfor-
mance depend on task similarity, task-dependent gating, and weight regularization in analytically
tractable teacher-student models. Teacher-student models are simple, typically linear, neural networks
in which the generative model of data is specified explicitly by the teacher network [18, 66, 3].
These models have provided tremendous insights into generalization property [55, 45, 19, 1], con-
vergence rate [61, 58, 38], and learning dynamics [51, 52, 4, 25] of neural networks, due to their
analytical tractability. Several works also studied continual learning using teacher-student settings
[2, 35, 27, 11, 20, 37, 13] (see Related works section for details)."
INTRODUCTION,0.00903225806451613,"We develop a linear teacher-student model with a low-dimensional latent structure and analyze how
the similarity of input features and readout patterns between tasks affect continual learning. We
show analytically that a combination of low feature similarity and high readout similarity is relatively
benign for continual learning, as the retention performance remains high and the transfer performance
remains non-negative. However, the opposite, a combination of high feature similarity and low
readout similarity is harmful. In this regime, both transfer and retention performance become below
the chance level even when the two subsequent tasks are positively correlated. Furthermore, transfer
performance depends on the feature similarity non-monotonically, such that, beyond a critical point,
the higher the feature similarity is, the lower the transfer performance becomes."
INTRODUCTION,0.01032258064516129,"We further analyze how common algorithms for continual learning, activity and plasticity gating
[15, 42, 46], activity sparsification [57], and weight regularization [30, 36, 67], interact with task
similarity in our problem setting, deriving several non-trivial conclusions. Activity gating improves
retention at the cost of transfer when the gating highly sparsifies the activity, but helps both transfer and
retention on average if the activity is kept relatively dense. Plasticity gating and activity sparsification,
by contrast, do not influence either transfer or retention performance at the over-parameterized
limit. Lastly, weight regularization in the Fisher information metric helps retention without affecting
knowledge transfer and achieves perfect retention regardless of task similarity in the presence of
low-dimensional latent. However, its diagonal approximation and the regularization in the Euclidean
metric are much less robust against both task similarity and regularizer amplitude."
INTRODUCTION,0.011612903225806452,"Furthermore, we test our key predictions numerically in a permuted MNIST task with a latent structure.
When the input pixels are permuted from one task to the next, the retention performance remains high.
However, when the mapping from the latent to the target output is changed, both the retention and
transfer performance go below the chance level, as predicted. Random task-dependent gating of input
and hidden layer activity improves retention at the cost of knowledge transfer, but adaptive gating
mitigates this tradeoff. We also show that in a fully-connected feedforward network, there exists an
efficient layer-wise approximation of the weight regularization in the Fisher information metric, which
outperforms its diagonal approximation and the regularization in the Euclidean metric. Nevertheless,
the performance of the diagonal approximation is much closer to the layer-wise approximation of the
Fisher information metric than to the Euclidean weight regularization."
INTRODUCTION,0.012903225806451613,"Our theory thus reveals when continual learning is difficult, and how different algorithms mitigate
these challenging situations, providing a basic framework for analyzing continual learning in artificial
and biological neural networks."
RELATED WORKS,0.014193548387096775,"2
Related works"
RELATED WORKS,0.015483870967741935,"Previous works on continual learning in linear teacher-student models found that forgetting is most
prominent at the intermediate task similarity [11, 40, 13] as observed empirically [47]. However,
these works did not address the tradeoff between forgetting and knowledge transfer, and these simple
settings did not disentangle the effect of the similarity in input feature and readout pattern. Forward
and backward transfer performance in continual learning were also analyzed in linear and deep-linear
networks [33, 8], yet its relationship with catastrophic forgetting has not been well characterized. Lee
et al. [35] analyzed dynamics of both forgetting and forward transfer in one-hidden layer nonlinear
network under a multi-head continual learning setting. However, their analysis of readout similarity"
RELATED WORKS,0.016774193548387096,"0
1
2
3
4
5
6
7
8
9"
RELATED WORKS,0.01806451612903226,"0
1
2
3
4
5
6
7
8
9
9
1
3
6
4
5
0
7
8
2 B"
RELATED WORKS,0.01935483870967742,Task 2
RELATED WORKS,0.02064516129032258,"0
1
2
3
4
5
6
7
8
9"
RELATED WORKS,0.02193548387096774,Task 1
RELATED WORKS,0.023225806451612905,Task 2
RELATED WORKS,0.024516129032258065,Feature similarity !!
RELATED WORKS,0.025806451612903226,"Readout similarity !"""
RELATED WORKS,0.027096774193548386,"0.0
1.0 1.0 0.0"
RELATED WORKS,0.02838709677419355,"C
A
Task 1
D"
RELATED WORKS,0.02967741935483871,"Figure 1: (A) Schematic representation of the continual linear regression model with low-dimensional
latent variables. (B-D) Examples of continual learning with two tasks in the MNIST setting. The
tasks have low feature similarity in panel C (input pixels are partially permuted) and low readout
similarity in panel D (output labels are partially permuted). Panels C and D correspond to the green
and orange points in panel B, respectively."
RELATED WORKS,0.03096774193548387,"and its comparison to feature similarity were conducted numerically and it did not address the effect
of common heuristics, such as gating or weight regularization, either."
RELATED WORKS,0.03225806451612903,"By comparison, we introduce a low-dimensional latent structure into a linear teacher-student model,
which enables us to decouple the influence of feature and readout similarity on knowledge transfer
and retention. Moreover, this low-dimensionality assumption on the latent enables us to evaluate the
transfer and retention performance analytically even in the presence of gating or weight regularization
in the Fisher information metric."
RELATED WORKS,0.03354838709677419,"Continual learning has been studied from many theoretical frameworks beyond teacher-student
modeling, including neural tangent kernel [5, 9, 27], PAC learning [6, 60], and computational
complexity [31]. Learning of low-dimensional latent representation has also been studied in the
context of multi-task learning [43, 62]. In addition, several works investigated the effect of weight
regularization on continual learning in analytically tractable settings [30, 12, 24]. In particular, Evron
et al. [12] investigated effect of weight-regularization in Fisher-information metric in continual linear
regression scenario."
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.03483870967741935,"3
Teacher-student model with low-dimensional latent variables"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.03612903225806452,"Let us consider task-incremental continual learning of regression tasks. For analytical tractability, we
consider a teacher-student setting where the target outputs are generated by the teacher network (Fig.
1A). We define the student network, which learns the task, as a linear projection from a potentially
nonlinear transformation of the input, y = Wψ(x), where x ∈RNx is the input, y ∈RNy is
the output, W ∈RNy×Nx is the trainable weight matrix, and ψ(x) : RNx →RNx is an input
transformation. We use ψ(x) = x for the vanilla and weight regularization models, ψ(x) = g ⊙x
for the task-dependent gating model, and ψ(x) = sgn(x)⊙max{0, |x|−h} for the soft-thresholding
model, where g and h are gating and thresholding vectors, respectively. Here, g ⊙x is an element-
wise multiplication, and sgn(x) is a function that returns the sign of the input. Throughout the paper,
we use bold-italic letters for vectors, capital-italic letters for matrices."
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.03741935483870968,We generate the input x and target output y∗of the µ-th task using a latent variable s ∈RNs:
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.03870967741935484,"s ←N (0, Is) ,
x = Aµs,
y∗= Bµs,
(1)"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04,"for µ = 1, 2, where Aµ ∈RNx×Ns and Bµ ∈RNy×Ns are mixing matrices hidden from the student
network, and Is is the size Ns identity matrix. We introduce this latent structure, s, to decouple the
effect of feature and readout similarity on the transfer and retention performance. Below, we set the
latent space to be low-dimensional compared to the input space (i.e., Ns ≪Nx). This is motivated
by the presence of low-dimensional latent structure in many machine learning datasets [64, 7] and
the tasks used in neuroscience experiments [17, 26], but also aids analytical tractability."
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04129032258064516,"We generate the mixing matrices for the first task, A1 and B1, by sampling elements independently
from a Gaussian distribution with mean zero and variance
1
Ns . The subsequent task matrices A2 and
B2 are also generated randomly, but with element-wise correlation with the previous matrices A1
and B1 (see Appendix A for the details). We denote the element-wise correlation between A1 and"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04258064516129032,"Figure 2: Transfer and retention performance of the vanilla model. (A) Illustration of ∆ϵT F and
∆ϵRT . Red and blue lines represent the error on task 1 and task 2, respectively. Here, the model
was trained on task 1 for 100 iterations and then trained on task 2 for another 100 iterations. (B, C)
Transfer performance under various task similarity. Points in panel B are numerical results (the means
and the standard deviations over ten random seeds), while solid lines are analytical results (Eq. 4).
(D-G) Retention performance under various task similarity. Panel G magnifies the 0.9 ≤ρb ≤1.0
region of panel E, and the white dashed line in panel G represents local minima/maxima."
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04387096774193548,"A2 by ρa and refer it as feature correlation, because A1 and A2 specify the input features. Similarly,
we denote the correlation between B1 and B2, the readout correlation, by ρb. Figs. 1B-D illustrate
the difference between ρa and ρb in an image classification setting. When ρa < 1.0 and ρb = 1.0,
input features are partially modified in the second task compared to the first task (Green point on
Fig. 1B and Fig. 1C). For example, permuted MNIST task [22] corresponds to this low feature
similarity setting with ρa = 0.0 and ρb = 1.0. By contrast, when ρa = 1 and ρb < 1, the readouts
are changed partially (Orange point on Fig. 1B and Fig. 1D). When ρa = 1 and ρb = −1, the same
input features are associated with the opposite readout in the new task, which is known as the reversal
learning paradigm. In this reversal scenario, the network fails to achieve knowledge transfer rather
trivially. Thus, we focus on the scenario when tasks have non-negative correlation in terms of both
input features and readout (i.e., 0 ≤ρa, ρb ≤1) below."
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04516129032258064,"We measure the performance of the student network with weight W on the µ-th task by mean-squared
error ϵµ[W] ≡
1
Ny"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04645161290322581,"D
∥Bµs −Wψ(Aµs)]∥2E"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04774193548387097,"s, where ⟨·⟩s is the expectation over latent variable s.
The transfer and retention performance are defined by"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.04903225806451613,"∆ϵT F ≡⟨ϵ2[Wo] −ϵ2[W1]⟩A,B ,
∆ϵRT ≡⟨ϵ1[Wo] −ϵ1[W2]⟩A,B .
(2)"
TEACHER-STUDENT MODEL WITH LOW-DIMENSIONAL LATENT VARIABLES,0.05032258064516129,"Here, ⟨·⟩A,B is the expectation over randomly generated task matrices A1, A2, B1 and B2 under
a given feature and readout correlation ρa, ρb. As illustrated in Fig. 2A, the transfer performance
∆ϵT F measures how much performance the model achieves on task 2 by learning task 1, whereas the
retention performance ∆ϵRT measures how well the model can perform task 1 after learning task 2
(here, the task switch occurs at the 100th iteration). Below, we study how knowledge transfer and
retention, the two key objectives of continual learning, depend on task similarity, and how to optimize
the performance through gating and weight regularization."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05161290322580645,"4
Impact of task similarity on knowledge transfer and retention"
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05290322580645161,"Let us first investigate the vanilla model without gating or weight regularization, to examine how task
similarity affects knowledge transfer and retention. Given ψ(x) = x, at the infinite sample limit,
learning by gradient descent follows ˙W = −(WAµ −Bµ)AT
µ. The fixed point of this dynamics, as
detailed in Appendix B.1, is:"
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05419354838709677,"Wµ = Wµ−1(I −UµU T
µ ) + BµA+
µ ,
(3)"
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05548387096774193,"where Wµ−1 is the weight after learning of the previous task, Uµ is the semi-orthogonal matrix from
singular value decomposition (SVD) of Aµ = UµΛµV T
µ , and A+
µ is the pseudo-inverse of matrix Aµ."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.0567741935483871,"Inserting Eq. 3 into Eq. 2 and taking the expectation over randomly generated tasks A1, B1, A2, B2,
the transfer and retention performance are written as (see Appendix B.1)"
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05806451612903226,"∆ϵT F = ρa(2ρb −ρa),
∆ϵRT = 1 −ρ2
a(ρ2
a −2ρaρb + 1).
(4)"
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.05935483870967742,"Recall that ρa is the feature similarity defined by the correlation between A1 and A2, while ρb
is the readout similarity, representing the correlation between B1 and B2. The derivation of the
above equations relies on (correlated) random generation of A1, B1, A2, B2 and the low-rank latent
assumption: Ns ≪Nx. These two equations, despite their simplicity, capture the transfer and
retention performance in numerical simulations well (Figs. 2B, D, and F; see Appendix E.1 for the
details of numerical estimation). Furthermore, they reveal asymmetric and non-monotonic impact of
the feature and readout similarity on the performance."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.06064516129032258,"Fig. 2B depicts the knowledge transfer from task 1 to task 2, ∆ϵT F , under various (ρa, ρb) conditions.
As expected, ∆ϵT F = 0 when the two tasks are independent (i.e., (ρa, ρb) = (0, 0)), and ∆ϵT F = 1
when the tasks are identical (i.e., (ρa, ρb) = (1, 1)). At intermediate levels of similarity, there is clear
asymmetry in the influence of feature and readout similarities on transfer performance; particularly, a
combination of high feature similarity and low readout similarity leads to negative transfer, while the
opposite scenario results in a modest positive transfer (lower-right vs upper-left of Fig. 2B)."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.06193548387096774,"Notably, under a fixed readout similarity ρb, the knowledge transfer depends non-monotonically
on the feature similarity ρa. When ρa < ρb, the higher the feature similarity the better transfer
becomes, as implied from Eq. 4. However, once the feature similarity exceeds the readout similarity,
counter-intuitively, high feature similarity worsens the transfer performance (Figs. 2B and C). This is
because, when the feature similarity is high, inputs are aligned with learned features, resulting in a
large output regardless of readout similarity. Particularly, under a low readout similarity, performance
becomes below zero because extracted features are mostly projected to the incorrect directions."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.06322580645161291,"The retention performance also exhibits asymmetric dependence on feature and readout similarity.
When feature similarity ρa is low, the network barely forgets the previous task regardless of readout
similarity (Fig. 2D left). By contrast, when the feature similarity is high, the retention performance
can either be positive or negative depending on the readout similarity. Moreover, in the high readout
similarity regime, the retention performance is the lowest at an intermediate feature similarity (Figs.
2E and F). This is because high retention is possible when either interference is low, or similarity
between two tasks is high. Note that, this last point on the non-monotonic dependence on feature
similarity under ρb = 1 has been investigated both empirically [47] and analytically [35, 11, 13].
Indeed, at ρb = 1, ∆ϵRT in Eq. 4 coincides with Eq. (5) in [13]."
IMPACT OF TASK SIMILARITY ON KNOWLEDGE TRANSFER AND RETENTION,0.06451612903225806,"Therefore, the knowledge transfer and retention performance depend on feature and readout simi-
larities in an asymmetric and non-monotonic manner. Specifically, a combination of high feature
similarity and low readout similarity is detrimental to continual learning, resulting in negative transfer
and retention performance, even in the presence of a positive correlation between the two tasks in
both input features and readout patterns. Moreover, under a fixed readout similarity, the knowledge
transfer performance depends on the feature similarity in a non-monotonic manner. Thus, the con-
tinual learning in the vanilla model is sensitive to task similarity and limited in performance. These
results make us wonder whether we can mitigate the sensitivity to task similarity and improve the
overall transfer and retention performance by modifying the learning algorithm. To this end, we next
investigate how task-dependent gating and weight regularization methods, two popular strategies in
continual learning, alleviate knowledge transfer and retention."
TASK-DEPENDENT GATING,0.06580645161290323,"5
Task-dependent gating"
RANDOM ACTIVITY GATING,0.06709677419354838,"5.1
Random activity gating"
RANDOM ACTIVITY GATING,0.06838709677419355,"One popular method for mitigating forgetting in continual learning is activity gating [15, 54, 42, 21,
56]. With gating of input activity, the network is written as y = W(gµ ⊙x), where gµ ∈{0, 1}Nx
is a binary gating vector for task µ. We first consider a random gating scenario where elements of gµ
are randomly sampled from a Bernoulli distribution with rate α which we denote as the gating level.
All units are active at α = 1, while no units are active at α →0 limit."
RANDOM ACTIVITY GATING,0.0696774193548387,"Figure 3: Random task-dependent activity gating model. (A) Knowledge transfer performance under
ρa = 1.0. The gating level α is defined as the fraction of active input neurons (i.e., α = Pr[gi = 1]).
(B) The transfer performance under the optimal gating level α∗= min{ ρb"
RANDOM ACTIVITY GATING,0.07096774193548387,"ρa , 1}. (C) Retention
performance under ρa = 1.0. (D) Average transfer and retention performance over uniform prior on
0 ≤ρa, ρb ≤1. Horizontal dashed lines are the performance of the vanilla model, while solid lines
are the performance of the random gating model. Points are numerical estimations."
RANDOM ACTIVITY GATING,0.07225806451612904,"Figure 4: Transfer and retention performance of the adaptive activity gating (A, B), random plasticity
gating (C), and input soft-thresholding (D) models. Dashed and solid lines in panels A and B
represent the performance of the random and adaptive activity gating models, respectively."
RANDOM ACTIVITY GATING,0.07354838709677419,"From a parallel argument with the vanilla model, when Ns ≪αNx, the transfer and retention
performance are estimated as (see Appendix B.1):"
RANDOM ACTIVITY GATING,0.07483870967741936,"∆ϵT F = αρa(2ρb −αρa),
(5a)"
RANDOM ACTIVITY GATING,0.07612903225806451,"∆ϵRT = 1 −α2ρ2
a(α2ρ2
a −2αρaρb + 1).
(5b)"
RANDOM ACTIVITY GATING,0.07741935483870968,"Thus, random gating scales the feature similarity from ρa to αρa. This scaling lowers the knowledge
transfer if ρb ≥ρa because random gating reduces the fraction of input neurons active in two
subsequent tasks (lime line in Fig. 3A). However, if ρb < ρa, gating with α ≥ρb"
RANDOM ACTIVITY GATING,0.07870967741935483,"ρa enhances the
transfer by reducing the effective feature similarity (blue lines in Fig. 3A; also compare Fig. 3B with
Fig. 2C). The optimal gating level α∗for knowledge transfer is min{ ρb"
RANDOM ACTIVITY GATING,0.08,"ρa , 1}, indicating that the input
activity typically needs to be dense. By contrast, the retention performance is optimized at α →0
limit where tasks do not interfere with each others (Fig. 3C). At this limit, we have ∆ϵRT →1. Note
that, in reality, α has to be non-zero to optimize the retention (Eq. 5b holds only when α ≫Ns Nx )."
RANDOM ACTIVITY GATING,0.08129032258064516,"These results indicate that random activity gating improves the retention at the cost of forward knowl-
edge transfer, as suggested previously [28, 39, 29]. This tradeoff between knowledge transfer and
retention is especially critical when the network doesn’t know the task similarities. Suppose that the
task similarity is distributed uniformly over 0 ≤ρa, ρb ≤1. Then, the average transfer performance is
maximized at α = 3"
RANDOM ACTIVITY GATING,0.08258064516129032,"4, while the average retention performance decreases monotonically as a function
of α (Fig. 3D). Thus, a moderate gating (α ≈3"
RANDOM ACTIVITY GATING,0.08387096774193549,"4) benefits both transfer and retention on average.
However, retention performance in this regime is significantly below one, implying that the network
cannot reliably achieve high retention performance without sacrificing the transfer performance."
ADAPTIVE ACTIVITY GATING,0.08516129032258064,"5.2
Adaptive activity gating"
ADAPTIVE ACTIVITY GATING,0.08645161290322581,"One way to overcome the tradeoff between transfer and retention performance is to consider adaptive
gating. Let us introduce a probe trial at the beginning of the task 2, in which the model tests how
well it performs if it uses the gating for task 1, g1, for task 2 as well. If the error in the probe trial is
small, the model should keep using the same gating vectors to achieve a good knowledge transfer.
Otherwise, it should resample the gating vector for retaining the knowledge on task 1. Using the"
ADAPTIVE ACTIVITY GATING,0.08774193548387096,"Figure 5: Performance of weight regularization in Euclidean metric. (A,B) Transfer (A) and retention
(B) performance. The amplitude of the weight regularization scales with 1"
ADAPTIVE ACTIVITY GATING,0.08903225806451613,"γ −1. (C) Regularizer
coefficient γ that optimizes the retention performance. (D) Average performance over uniform task
similarity distribution in 0 ≤ρa, ρb ≤1. Horizontal dashed lines are the performance of the vanilla
model."
ADAPTIVE ACTIVITY GATING,0.09032258064516129,"probe error ϵpb, we set the probability of using the same gating as ρg = 1 −ϵpb/ϵo. This adaptive
gating indeed improves the average transfer performance compared to the random gating (solid vs
dashed lines in Fig. 4A) with only a relatively small reduction in the retention performance (Fig. 4B)."
PLASTICITY GATING AND INPUT SOFT-THRESHOLDING,0.09161290322580645,"5.3
Plasticity gating and input soft-thresholding"
PLASTICITY GATING AND INPUT SOFT-THRESHOLDING,0.09290322580645162,"Previous works have also explored other gating mechanisms, such as plasticity gating [46] and activity
sparsification [57]. However, their benefits over the activity gating discussed above have not been
fully understood. We implement task-dependent plasticity gating into our problem setting by making
only the synaptic weights projected from a subset of input neurons plastic. Unexpectedly, we found
that both transfer and retention performance are independent of the fraction of plastic synapses in the
Ns ≪Nx limit, potentially due to over-parameterization (Fig. 4C; see Appendix B.2 for details)."
PLASTICITY GATING AND INPUT SOFT-THRESHOLDING,0.09419354838709677,"Similarly, when the input activity is sparsified using soft-thresholding φ(x) = sgn(x) max{0, |x| −
h}, with a fixed threshold h =
√"
PLASTICITY GATING AND INPUT SOFT-THRESHOLDING,0.09548387096774194,"2erfc−1(α), the transfer and retention performance remain inde-
pendent of the resultant input sparsity α (Fig. 4D; see Appendix B.3 for the details). These results
imply that plasticity gating and input sparsification are not effective in our model, which operates in
an over-parameterized regime (i.e., Ns ≪Nx), but do not exclude their potential benefits for many
continual learning tasks."
WEIGHT REGULARIZATION,0.0967741935483871,"6
Weight regularization"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.09806451612903226,"6.1
Weight regularization in Euclidean metric"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.09935483870967741,"Another popular method is weight regularization, which keeps the synaptic weights close to those
learned from previous tasks [30, 36, 67]. Let us first consider regularization of the Euclidean distance
between the current weight W and the weight learned in the previous task Wµ−1:"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10064516129032258,ℓµ = 1
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10193548387096774,"2 ∥Bµ −WAµ∥2
F + Nx"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.1032258064516129,2Ns ( 1
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10451612903225807,"γ −1) ∥W −Wµ−1∥2
F .
(6)"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10580645161290322,"Here, we parameterize the regularizer amplitude by Nx"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10709677419354839,Ns ( 1
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10838709677419354,"γ −1) using a non-negative parameter γ
for brevity. γ = 1 corresponds to zero regularization, while γ = 0 is the infinite regularization limit.
Under this parameterization, if Ns ≪Nx, the transfer and retention performance follow simple
expressions as below (see Appendix C.1):"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.10967741935483871,"∆ϵT F = γρa(2ρb −γρa),
(7a)"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.11096774193548387,"∆ϵRT = 1 −γ2ρ2
a(1 −2γρaρb + γ2ρ2
a) + 2γρa(1 −γ)(ρb −γρa) −(1 −γ)2.
(7b)"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.11225806451612903,"The expression of the transfer performance is equivalent to Eq. 5a if we change γ to α. Thus, the
Euclidean weight regularization with amplitude Nx"
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.1135483870967742,Ns ( 1
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.11483870967741935,"γ −1) is mathematically equivalent with random
activity gating with sparsity γ, in terms of knowledge transfer (compare Fig. 5A with Fig. 3A). By
contrast, the expression of the retention performance contains two additional terms compared to 5b.
In particular, the last term, (1 −γ)2, indicates that strong weight regularization not only prevent
forgetting, but also impairs task acquisition. Thus, the retention performance is optimized at an
intermediate regularizer strength (Figs. 5B and C)."
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.11612903225806452,"Figure 6: Weight regularization in the Fisher information metric. (A,B) The retention performance
under various task similarities and regularizer coefficients. (C,D) Average transfer and retention
performance under the regularization with the exact Fisher information metric (C) and its diagonal
approximation (D)."
WEIGHT REGULARIZATION IN EUCLIDEAN METRIC,0.11741935483870967,"Because strong regularization (i.e., small γ) impairs both retention and transfer, unlike the random
activity gating model, there isn’t a tradeoff between knowledge transfer and retention in terms of
the average performance over 0 ≤ρa, ρb ≤1 (Fig. 5D). However, the retention performance at the
optimal parametric regime is relatively low (Fig. 5D with Fig. 4B)."
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.11870967741935484,"6.2
Weight regularization in Fisher information metric"
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.12,"Previous works proposed that the synaptic weights should be regularized in the Fisher information
metric to allow flexibility in the non-overlapping weight space [30, 67]. Applying it to our model
setting, the regularizer term in Eq. 6 instead becomes 1 2( 1"
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.12129032258064516,"γ −1) ∥(W −Wµ−1)Aµ−1∥2
F (see Appendix
C.2). If ρa, γ < 1 and Ns ≪Nx, optimization of the weight under this regularization yields"
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.12258064516129032,"Wµ = Wµ−1

I −
Ns
Nx(1−ρ2a)Aµ

AT
µ −ρaAT
µ−1

+
Ns
Nx(1−ρ2a)Bµ
 
AT
µ −ρaAT
µ−1

,
(8)"
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.12387096774193548,"where Wµ−1 is the weight after the previous task learning. Notably, the weight becomes independent
of the regularizer’s amplitude γ. Furthermore, the retention performance under arbitrary task similarity
(ρa, ρb) is derived as ∆ϵRT = 1 −O(
Ns
Nx(1−ρ2a)). Thus, under the Fisher information metric, the
retention performance is perfect as long as the condition Ns ≪Nx(1 −ρ2
a) is satisfied (Figs. 6A-C).
Intuitively, assuming over-parameterization (Ns ≪Nx), the network can freeze the weight changes
in a low-dimensional subspace, while maintaining sufficient plasticity for new tasks, unless the two
tasks share the same feature. Notably, if the first task is learned with weight regularization in an
orthogonal direction, the transfer performance remains the same with the vanilla model (Fig. 6C)."
WEIGHT REGULARIZATION IN FISHER INFORMATION METRIC,0.12516129032258064,"Importantly, this invariance no longer holds when the Fisher information metric is approximated
by its diagonal component (Figs. 6D vs 6C), as is done in the elastic weight methods [30]. This
is because the diagonal approximation makes the metric full-rank even when the true metric has
a low-rank structure. Thus, weight regularization in the Fisher information metric robustly helps
retention without harming transfer, but diagonal approximation attenuates its robustness."
NUMERICAL EXPERIMENTS,0.12645161290322582,"7
Numerical experiments"
NUMERICAL EXPERIMENTS,0.12774193548387097,"Our theory so far has revealed when continual learning is difficult and when activity gating and weight
regularization can ameliorate the transfer and retention in a simple problem setting. Let us next
examine how much these insights are applicable to more realistic datasets and neural architectures."
NUMERICAL EXPERIMENTS,0.12903225806451613,"To this end, we consider the permuted MNIST dataset [34, 22], but with a latent structure (see
Appendix E.2). For each output label, we constructed a four-dimensional latent variable s using the
binary representation of the corresponding digit (e.g., s9 = [1, 0, 0, 1]T ). The target output was then
generated by a random fixed projection of the latent variable: y∗= Bµ(s −1"
NUMERICAL EXPERIMENTS,0.13032258064516128,"2). Note that, although
here we introduced an explicit latent structure for a direct comparison with the theory, qualitatively
similar results holds in the vanilla permuted MNIST setting depicted in Figs. 1B-D (see Fig. 12).
We controlled the readout similarity between tasks using the element-wise correlation between two
matrices B1 and B2. The feature similarity was controlled by permuting a subset of input pixels, as
in the previous permuted MNIST tasks. We used a one-hidden-layer fully-connected network with
rectified-linear nonlinearity at the hidden layer, and trained the network using stochastic gradient
descent."
NUMERICAL EXPERIMENTS,0.13161290322580646,"Figure 7: Permuted MNIST with latent variables. (A,B) Transfer and retention performance of the
vanilla model. (C,D) Performance of random (dashed lines) and adaptive (solid lines) activity gating
models. (E-H) Performance of the weight regularization in the Euclidean metric, and approximated
Fisher information metrics (red: layer-wise approximation; orange: synapse-wise/diagonal approxi-
mation). Here, lines are linear interpolations and error bars are standard errors over random seeds,
not standard deviations."
NUMERICAL EXPERIMENTS,0.1329032258064516,"As predicted from our theory, both transfer and retention performance, measured by the test error,
exhibited asymmetric and non-monotonic dependence on the task similarity. When (ρa, ρb) =
(1.0, 0.0), both transfer and retention performance were below zero (lower-right of Figs. 7A and B vs.
Figs. 2C and E). By contrast, the model achieved high retention and zero transfer under (ρa, ρb) =
(0.0, 1.0) as expected (upper-left). Notably, around ρb = 0.5, the transfer performance exhibited a
non-monotonic dependence on ρa in accordance with the theory. The retention performance also
exhibited non-monotonic dependence on ρa under ρb ≈1, but the dependence became monotonic
after a long training (see Figs. 9C and D in Appendix). We observe consistent results even when
feature and readout similarity are adjusted by permuting input pixels and output labels, respectively,
provided the loss is measured by cross-entropy (Fig. 12B, corresponding to the examples depicted in
Figs. 1C and 1D)."
NUMERICAL EXPERIMENTS,0.13419354838709677,"Random activity gating, implemented in both input and hidden layers, improved the retention
performance at the cost of low knowledge transfer as predicted (compare Fig. 7D and Fig. 4B).
Moreover, adaptive gating based on a probe trial improved the transfer performance under a low
gating level, especially when the readout similarity is high (solid vs dashed lines in Figs. 7C and D)."
NUMERICAL EXPERIMENTS,0.13548387096774195,"In a deep neural network, weight regularization using the Fisher information metric is typically
computationally expensive. However, an efficient layer-wise approximation exists for fully-connected
networks (see Appendix E.2). This layer-wise approximation enabled high retention performance
across a wide range of regularization amplitudes and consistently outperformed weight regularization
in the Euclidean metric under various task similarities (red vs. gray lines in Figs. 7F-H). Nevertheless,
the retention performance under the diagonal approximation was closer to that of the layer-wise
approximation of the Fisher information metric and was slightly better when both methods performed
poorly (red vs. orange lines). This is because the sparsity of hidden layer weights and activity makes
the diagonal components sparse."
DISCUSSION,0.1367741935483871,"8
Discussion"
DISCUSSION,0.13806451612903226,"Here, we analyzed the impact of task similarity on continual learning in a linear teacher-student model
with low-dimensional latent structures. We showed that, a combination of high feature similarity
and low readout similarity lead to poor outcomes in both retention and transfer, unlike the opposite
combination. We further explored how continual learning algorithms such as gating mechanisms,
activity sparsification, and weight regularization interact with task similarity. Results indicate that
weight regularization in the Fisher information metric significantly aids retention regardless of task
similarity. Numerical experiments in a permuted MNIST task with latent supported these findings."
DISCUSSION,0.1393548387096774,"Our findings on the interaction between task similarity and continual learning algorithms have several
implications. Firstly, when tasks are known to have high feature similarity and low readout similarity,
adaptive activity gating and weight regularization in the Fisher information metric help retention
without sacrificing transfer (Figs. 4, 6, 7). In the context of neuroscience, our results indicate that
simple non-adaptive activity or plasticity gating mechanisms are not sufficient for good continual
learning performance, especially when familiar sensory stimuli need to be associated with novel
motor actions. Indeed, a previous study reported catastrophic forgetting among rats learning timing
estimation tasks [14]. Lastly, it also implies the potential importance of studying wider benchmarks
beyond permuted image recognition tasks for continual learning, because image permutation belongs
to a class of relatively harmless task similarity according to our theory. As a simple extension, here
we developed an image recognition task with a latent variable and showed that a vanilla deep network
exhibits more forgetting under readout remapping than under input permutation (Figs. 7A and B)."
DISCUSSION,0.1406451612903226,"Limitations
Our theoretical results from a teacher-student model come with limitation in direct
applicability. The presence of low-dimensional latent generating both input and target output is a
reasonable assumption for many real-world tasks [7, 26], but the linear projection assumption is
not. While we replicated most key results in a deep nonlinear network solving permuted MNIST
task, we found that the diagonal approximation of Fisher information metric performs much better
than the prediction from the teacher-student model. This is potentially because sparse activity at the
hidden layer effectively makes the regularization low-rank even under the diagonal approximation.
More generally, the presence of hidden layers should enable a distinctive adaptation for feature and
readout similarity, which is an important future direction. It is also important to apply our theoretical
framework for analyzing continual learning in more complicated neural architectures and datasets."
ACKNOWLEDGMENTS,0.14193548387096774,"9
Acknowledgments"
ACKNOWLEDGMENTS,0.1432258064516129,"The author thanks Liu Ziyin and Ziyan Li for discussion. This work was partially supported by The
Swartz Foundation."
REFERENCES,0.14451612903225808,References
REFERENCES,0.14580645161290323,"[1] M. S. Advani, A. M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization error in
neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.14709677419354839,"[2] H. Asanuma, S. Takagi, Y. Nagano, Y. Yoshida, Y. Igarashi, and M. Okada. Statistical mechanical analysis
of catastrophic forgetting in continual learning with teacher and student networks. Journal of the Physical
Society of Japan, 90(10):104001, 2021."
REFERENCES,0.14838709677419354,"[3] Y. Bahri, J. Kadmon, J. Pennington, S. S. Schoenholz, J. Sohl-Dickstein, and S. Ganguli. Statistical
mechanics of deep learning. Annual Review of Condensed Matter Physics, 11:501–528, 2020."
REFERENCES,0.14967741935483872,"[4] M. Baity-Jesi, L. Sagun, M. Geiger, S. Spigler, G. B. Arous, C. Cammarota, Y. LeCun, M. Wyart, and
G. Biroli. Comparing dynamics: Deep neural networks versus glassy systems. In International Conference
on Machine Learning, pages 314–323. PMLR, 2018."
REFERENCES,0.15096774193548387,"[5] M. A. Bennani, T. Doan, and M. Sugiyama. Generalisation guarantees for continual learning with
orthogonal gradient descent. arXiv preprint arXiv:2006.11942, 2020."
REFERENCES,0.15225806451612903,"[6] X. Chen, C. Papadimitriou, and B. Peng. Memory bounds for continual learning. In 2022 IEEE 63rd
Annual Symposium on Foundations of Computer Science (FOCS), pages 519–530. IEEE, 2022."
REFERENCES,0.15354838709677418,"[7] U. Cohen, S. Chung, D. D. Lee, and H. Sompolinsky. Separability and geometry of object manifolds in
deep neural networks. Nature communications, 11(1):746, 2020."
REFERENCES,0.15483870967741936,"[8] O. Dhifallah and Y. M. Lu. Phase transitions in transfer learning for high-dimensional perceptrons. Entropy,
23(4):400, 2021."
REFERENCES,0.15612903225806452,"[9] T. Doan, M. A. Bennani, B. Mazoure, G. Rabusseau, and P. Alquier. A theoretical analysis of catastrophic
forgetting through the ntk overlap matrix. In International Conference on Artificial Intelligence and
Statistics, pages 1072–1080. PMLR, 2021."
REFERENCES,0.15741935483870967,"[10] B. Ehret, C. Henning, M. R. Cervera, A. Meulemans, J. Von Oswald, and B. F. Grewe. Continual learning
in recurrent neural networks. arXiv preprint arXiv:2006.12109, 2020."
REFERENCES,0.15870967741935485,"[11] I. Evron, E. Moroshko, R. Ward, N. Srebro, and D. Soudry. How catastrophic can catastrophic forgetting
be in linear regression? In Conference on Learning Theory, pages 4028–4079. PMLR, 2022."
REFERENCES,0.16,"[12] I. Evron, E. Moroshko, G. Buzaglo, M. Khriesh, B. Marjieh, N. Srebro, and D. Soudry. Continual
learning in linear classification on separable data. In International Conference on Machine Learning, pages
9440–9484. PMLR, 2023."
REFERENCES,0.16129032258064516,"[13] I. Evron, D. Goldfarb, N. Weinberger, D. Soudry, and P. Hand. The joint effect of task similarity and
overparameterization on catastrophic forgetting–an analytical model. arXiv preprint arXiv:2401.12617,
2024."
REFERENCES,0.1625806451612903,"[14] R. French and A. Ferrara. Modelling time perception in rats: evidence for catastrophic interference in
animal learning. In 21st Congress of Cognitive Sciences, 1999."
REFERENCES,0.1638709677419355,"[15] R. M. French. Using semi-distributed representations to overcome catastrophic forgetting in connectionist
networks. In Proceedings of the 13th annual cognitive science society conference, volume 1, pages
173–178, 1991."
REFERENCES,0.16516129032258065,"[16] R. M. French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):
128–135, 1999."
REFERENCES,0.1664516129032258,"[17] P. Gao, E. Trautmann, B. Yu, G. Santhanam, S. Ryu, K. Shenoy, and S. Ganguli. A theory of multineuronal
dimensionality, dynamics and measurement. BioRxiv, page 214262, 2017."
REFERENCES,0.16774193548387098,"[18] E. Gardner and B. Derrida. Three unfinished works on the optimal storage capacity of networks. Journal
of Physics A: Mathematical and General, 22(12):1983, 1989."
REFERENCES,0.16903225806451613,"[19] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers neural
network. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.1703225806451613,"[20] D. Goldfarb and P. Hand. Analysis of catastrophic forgetting for random orthogonal transformation tasks in
the overparameterized regime. In International Conference on Artificial Intelligence and Statistics, pages
2975–2993. PMLR, 2023."
REFERENCES,0.17161290322580644,"[21] S. Golkar, M. Kagan, and K. Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476,
2019."
REFERENCES,0.17290322580645162,"[22] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio. An empirical investigation of catastrophic
forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013."
REFERENCES,0.17419354838709677,"[23] R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu. Embracing change: Continual learning in deep neural
networks. Trends in cognitive sciences, 24(12):1028–1040, 2020."
REFERENCES,0.17548387096774193,"[24] R. Heckel. Provable continual learning via sketched jacobian approximations. In International Conference
on Artificial Intelligence and Statistics, pages 10448–10470. PMLR, 2022."
REFERENCES,0.1767741935483871,"[25] N. Hiratani, Y. Mehta, T. Lillicrap, and P. E. Latham. On the stability and scalability of node perturbation
learning. Advances in Neural Information Processing Systems, 35:31929–31941, 2022."
REFERENCES,0.17806451612903226,"[26] M. Jazayeri and S. Ostojic. Interpreting neural computations by examining intrinsic and embedding
dimensionality of neural activity. Current opinion in neurobiology, 70:113–120, 2021."
REFERENCES,0.17935483870967742,"[27] R. Karakida and S. Akaho. Learning curves for continual learning in neural networks: Self-knowledge
transfer and forgetting. In International Conference on Learning Representations, 2021."
REFERENCES,0.18064516129032257,"[28] Z. Ke, B. Liu, and X. Huang. Continual learning of a mixed sequence of similar and dissimilar tasks.
Advances in neural information processing systems, 33:18493–18504, 2020."
REFERENCES,0.18193548387096775,"[29] Z. Ke, B. Liu, W. Xiong, A. Celikyilmaz, and H. Li. Sub-network discovery and soft-masking for continual
learning of mixed tasks. arXiv preprint arXiv:2310.09436, 2023."
REFERENCES,0.1832258064516129,"[30] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ra-
malho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings
of the national academy of sciences, 114(13):3521–3526, 2017."
REFERENCES,0.18451612903225806,"[31] J. Knoblauch, H. Husain, and T. Diethe. Optimal continual learning has perfect memory and is np-hard. In
International Conference on Machine Learning, pages 5327–5337. PMLR, 2020."
REFERENCES,0.18580645161290324,"[32] D. Kudithipudi, M. Aguilar-Simon, J. Babb, M. Bazhenov, D. Blackiston, J. Bongard, A. P. Brna,
S. Chakravarthi Raja, N. Cheney, J. Clune, et al. Biological underpinnings for lifelong learning ma-
chines. Nature Machine Intelligence, 4(3):196–210, 2022."
REFERENCES,0.1870967741935484,"[33] A. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning in
deep linear networks. arXiv preprint arXiv:1809.10374, 2018."
REFERENCES,0.18838709677419355,"[34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.1896774193548387,"[35] S. Lee, S. Goldt, and A. Saxe. Continual learning in the teacher-student setup: Impact of task similarity. In
International Conference on Machine Learning, pages 6109–6119. PMLR, 2021."
REFERENCES,0.19096774193548388,"[36] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang. Overcoming catastrophic forgetting by incremental
moment matching. Advances in neural information processing systems, 30, 2017."
REFERENCES,0.19225806451612903,"[37] C. Li, Z. Huang, W. Zou, and H. Huang. Statistical mechanics of continual learning: Variational principle
and mean-field potential. Physical Review E, 108(1):014309, 2023."
REFERENCES,0.1935483870967742,"[38] Y. Li, T. Ma, and H. R. Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In
Conference on learning theory, pages 2613–2682. PMLR, 2020."
REFERENCES,0.19483870967741934,"[39] S. Lin, L. Yang, D. Fan, and J. Zhang. Beyond not-forgetting: Continual learning with backward knowledge
transfer. Advances in Neural Information Processing Systems, 35:16165–16177, 2022."
REFERENCES,0.19612903225806452,"[40] S. Lin, P. Ju, Y. Liang, and N. Shroff. Theory on forgetting and generalization of continual learning. In
International Conference on Machine Learning, pages 21078–21100. PMLR, 2023."
REFERENCES,0.19741935483870968,"[41] Y. Luo, Z. Yang, F. Meng, Y. Li, J. Zhou, and Y. Zhang. An empirical study of catastrophic forgetting in
large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023."
REFERENCES,0.19870967741935483,"[42] N. Y. Masse, G. D. Grant, and D. J. Freedman. Alleviating catastrophic forgetting using context-dependent
gating and synaptic stabilization. Proceedings of the National Academy of Sciences, 115(44):E10467–
E10475, 2018."
REFERENCES,0.2,"[43] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. Journal of
Machine Learning Research, 17(81):1–32, 2016."
REFERENCES,0.20129032258064516,"[44] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential
learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989."
REFERENCES,0.20258064516129032,"[45] A. Ndirango and T. Lee. Generalization in multitask deep neural classifiers: a statistical physics approach.
Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.20387096774193547,"[46] S. Özgün, A.-M. Rickmann, A. G. Roy, and C. Wachinger. Importance driven continual learning for
segmentation across domains. In Machine Learning in Medical Imaging: 11th International Workshop,
MLMI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4, 2020, Proceedings 11, pages
423–433. Springer, 2020."
REFERENCES,0.20516129032258065,"[47] V. V. Ramasesh, E. Dyer, and M. Raghu. Anatomy of catastrophic forgetting: Hidden representations and
task semantics. arXiv preprint arXiv:2007.07400, 2020."
REFERENCES,0.2064516129032258,"[48] A. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123–146,
1995."
REFERENCES,0.20774193548387096,"[49] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning.
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.20903225806451614,"[50] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and
R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016."
REFERENCES,0.2103225806451613,"[51] D. Saad and S. A. Solla. On-line learning in soft committee machines. Physical Review E, 52(4):4225,
1995."
REFERENCES,0.21161290322580645,"[52] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in
deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.2129032258064516,"[53] A. M. Saxe, J. L. McClelland, and S. Ganguli. A mathematical theory of semantic development in deep
neural networks. Proceedings of the National Academy of Sciences, 116(23):11537–11546, 2019."
REFERENCES,0.21419354838709678,"[54] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention
to the task. In International conference on machine learning, pages 4548–4557. PMLR, 2018."
REFERENCES,0.21548387096774194,"[55] H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples. Physical
review A, 45(8):6056, 1992."
REFERENCES,0.2167741935483871,"[56] E. Sezener, A. Grabska-Barwi´nska, D. Kostadinov, M. Beau, S. Krishnagopal, D. Budden, M. Hutter,
J. Veness, M. Botvinick, C. Clopath, et al. A rapid and efficient learning rule for biological neural circuits.
BioRxiv, pages 2021–03, 2021."
REFERENCES,0.21806451612903227,"[57] R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmidhuber. Compete to compute. Advances
in neural information processing systems, 26, 2013."
REFERENCES,0.21935483870967742,"[58] Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in
convergence and critical point analysis. In International conference on machine learning, pages 3404–3413.
PMLR, 2017."
REFERENCES,0.22064516129032258,"[59] G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias. Brain-inspired replay for continual learning with
artificial neural networks. Nature communications, 11(1):4069, 2020."
REFERENCES,0.22193548387096773,"[60] L. Wang, X. Zhang, Q. Li, M. Zhang, H. Su, J. Zhu, and Y. Zhong. Incorporating neuro-inspired adaptability
for continual learning in artificial intelligence. Nature Machine Intelligence, 5(12):1356–1368, 2023."
REFERENCES,0.2232258064516129,"[61] J. Werfel, X. Xie, and H. Seung. Learning curves for stochastic gradient descent in linear feedforward
networks. Advances in neural information processing systems, 16, 2003."
REFERENCES,0.22451612903225807,"[62] Z. Xu and A. Tewari. On the statistical benefits of curriculum learning. In International Conference on
Machine Learning, pages 24663–24682. PMLR, 2022."
REFERENCES,0.22580645161290322,"[63] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks. arXiv
preprint arXiv:1708.01547, 2017."
REFERENCES,0.2270967741935484,"[64] X. Yu, T. Liu, X. Wang, and D. Tao. On compressing deep models by low rank and sparse decomposition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7370–7379,
2017."
REFERENCES,0.22838709677419355,"[65] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling task
transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 3712–3722, 2018."
REFERENCES,0.2296774193548387,"[66] L. Zdeborová and F. Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in
Physics, 65(5):453–552, 2016."
REFERENCES,0.23096774193548386,"[67] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In International
conference on machine learning, pages 3987–3995. PMLR, 2017."
REFERENCES,0.23225806451612904,"A
Problem setting: Linear teacher-student model with a latent variable"
REFERENCES,0.2335483870967742,"Let us consider a continuous learning of Nsess tasks. We generate the input x ∈RNx and the target
output y∗∈RNy of the µ-th task by"
REFERENCES,0.23483870967741935,"s ←N (0, Is) ,
x = Aµs,
y∗= Bµs,
(9)"
REFERENCES,0.2361290322580645,"where s ∈RNs is the latent variable, Is is the size Ns identity matrix, and Aµ ∈RNx×Ns and
Bµ ∈RNy×Ns are mixing matrices that map the latent variable s into the input space and output
space, respectively. Throughout the paper, the vector x is defined as a column vector, and its transpose
xT is a row vector."
REFERENCES,0.23741935483870968,"We consider sequential learning of these tasks with a neural network defined by y = Wψ(x), where
W ∈RNy×Nx is the plastic weight. We set the function ψ : RNx →RNx to ψ(x) = x in the
vanilla and weight regularization models, ψ(x) = g ⊙x in activity and plasticity gating models,
and ψ(x) = sgn(x) ⊙max{0, |x| −h} in the soft-thresholding model. Here, g ⊙x represents an
element-wise multiplication of two vectors, and sgn(x) is a function that returns the sign of the input
x. The goal of the student network, y = Wψ(x), is to mimic the teacher network that generates both
input and target output [18, 66, 3]. Below, we focus on learning of Nsess = 2 tasks. To analyze the
transfer and retention performance, we introduce the following two assumptions on task structure."
REFERENCES,0.23870967741935484,"Assumption I: Random task assumption
We consider random generation of task matrices Aµ, Bµ
but with correlation between subsequent tasks. We sample elements of the mixing matrices for the
first task, A1 and B1, from a Gaussian distribution with mean zero and variance
1
Ns independently.
For the subsequent tasks, we generate mixing matrices Aµ by"
REFERENCES,0.24,"Aµ,ij ="
REFERENCES,0.24129032258064517,"(
Aµ−1,ij
(with probability ρa)
eAµ,ij
(otherwise)
(10)"
REFERENCES,0.24258064516129033,"where eAµ is a random matrix whose elements are sampled independently from a Gaussian distribution
with mean zero and variance
1
Ns . We generate Bµ in the same manner, but with correlation ρb.
Although this hidden weight generation doesn’t exactly follow the correlated Gaussian assumption,
the difference appears only in higher-order terms, which are negligible under Ns ≫Nx assumption
we introduce below."
REFERENCES,0.24387096774193548,"Assumption II: Low-dimensional latent assumption
The second assumption is the relative low
dimensionality of the latent space with respect to the input space: Ns ≪Nx. This assumption of
over-parameterization in the input space simplifies the analysis (see Appendix D)."
REFERENCES,0.24516129032258063,"Evaluation of the transfer and retention performance
We evaluate the task performance of a
student model with weight W on task µ by mean-squared error:"
REFERENCES,0.24645161290322581,ϵµ[W] ≡1 Ny
REFERENCES,0.24774193548387097,"D
∥Bµs −Wψ(Aµs)]∥2E"
REFERENCES,0.24903225806451612,"s .
(11)"
REFERENCES,0.2503225806451613,"Here, ⟨·⟩s is the expectation over latent s ∼N(0, Is). Using ϵµ[W], the degree of forward knowledge
transfer and retention are evaluated by"
REFERENCES,0.25161290322580643,"∆ϵT F
µ
≡⟨ϵµ[Wo] −ϵµ[Wµ−1]⟩A,B ,
(12a)"
REFERENCES,0.25290322580645164,"∆ϵRT
µ
≡⟨ϵµ[Wo] −ϵµ[Wµ+1]⟩A,B ,
(12b)"
REFERENCES,0.2541935483870968,"where Wµ is the weight after training on the µ-th task. Note that, the retention performance is defined
by the difference with the initial error ϵµ[Wo]. This means that, if the network fails to learn the task
in the first place (i.e., if ϵµ[Wµ] > 0), the retention performance becomes sub-optimal even if the
network doesn’t forget the learned task. We used this definition for our problem setting because the
network is able to learn the task perfectly unless strong regularization is added to the network."
REFERENCES,0.25548387096774194,"Learning procedures
We consider the gradient descent learning from infinite samples at the
gradient flow limit and analyze the performance at the convergence. Below, we investigate how
∆ϵT F
µ
and ∆ϵRT
µ
depend on the task similarity and hyper-parameters under various algorithms for
continual learning."
REFERENCES,0.2567741935483871,"B
Task-dependent gating model"
REFERENCES,0.25806451612903225,"B.1
Activity gating"
REFERENCES,0.2593548387096774,"Let us first consider task-dependent activity gating model. The student network (the network that
learns the task) is given by:"
REFERENCES,0.26064516129032256,"y = W[gµ ⊙x],
(13)"
REFERENCES,0.26193548387096777,"where gµ is the gating vector that depends on the task id µ, but independent of input x. Note that, by
setting gµ = 1, we recover the vanilla model y = Wx."
REFERENCES,0.2632258064516129,The performance of this network on the µ-th task is written as
REFERENCES,0.2645161290322581,ϵµ[W] ≡1 Ny
REFERENCES,0.2658064516129032,"D
∥Bµs −W[gµ ⊙(Aµs)]∥2E s = 1"
REFERENCES,0.2670967741935484,"Ny
∥Bµ −WDµAµ∥2
F ,
(14)"
REFERENCES,0.26838709677419353,"where Dµ ≡diag[gµ] is a diagonal matrix whose (i, i)-th element is gµ,i, and ∥·∥F is the Frobenius
norm."
REFERENCES,0.2696774193548387,"Solution of gradient descent learning
Under the gradient descent learning, the weight dynamics
follows"
REFERENCES,0.2709677419354839,˙W(t) = −η ∂ϵµ[W]
REFERENCES,0.27225806451612905,"∂W
= −2η"
REFERENCES,0.2735483870967742,"Ny
(WDµAµ −Bµ) (DµAµ)T .
(15)"
REFERENCES,0.27483870967741936,"From singular value decomposition (SVD), DµAµ is rewritten as"
REFERENCES,0.2761290322580645,"DµAµ = UµΣµV T
µ ,
(16)"
REFERENCES,0.27741935483870966,"where Uµ ∈RNx×No and Vµ ∈RNs×No are semi-orthonormal matrices (i.e., U T
µ Uµ = V T
µ Vµ = Io),
Σµ ∈RNo×No is a positive diagonal matrix, No is the number of non-zero singular values of DµAµ.
Using this decomposition, the gradient descent dynamics is rewritten as"
REFERENCES,0.2787096774193548,˙W(t) = −2η
REFERENCES,0.28,"Ny
(WUµΣµ −BµVµ) ΣµU T
µ .
(17)"
REFERENCES,0.2812903225806452,"Thus, the weight matrix W(t) at arbitrary t is written as"
REFERENCES,0.28258064516129033,"W(t) = W(t = 0) + Q(t)U T
µ ,
(18)"
REFERENCES,0.2838709677419355,"where Q ∈RNy×No is a time-dependent matrix. At the fixed point of this learning dynamics, we
have
 
Wµ−1 + QU T
µ

UµΣµ −BµVµ

ΣµU T
µ = O,
(19)"
REFERENCES,0.28516129032258064,"where O is the zero matrix and Wµ−1 ≡W(t = 0) is the weight after learning of the previous task.
Because Σµ is a positive diagonal matrix, the equation above has a unique solution:"
REFERENCES,0.2864516129032258,"Q = (BµVµ −Wµ−1UµΣµ) Σ−1
µ ,
(20a)"
REFERENCES,0.28774193548387095,"W = Wµ−1
 
Ix −UµU T
µ

+ Bµ(DµAµ)+,
(20b)"
REFERENCES,0.28903225806451616,"where (DµAµ)+ = VµΣ−1
µ U T
µ is the pseudo-inverse of DµAµ, and Ix is the size Nx identity matrix."
REFERENCES,0.2903225806451613,"From Eqs. 20b and 14, we have"
REFERENCES,0.29161290322580646,ϵµ[Wµ] = 1 Ny
REFERENCES,0.2929032258064516,"Bµ
 
VµV T
µ −Is
2"
REFERENCES,0.29419354838709677,"F .
(21)"
REFERENCES,0.2954838709677419,"Moreover, by setting the initial weight Wo (the weight before the first tasks) to zero, we have"
REFERENCES,0.2967741935483871,ϵµ[Wo] = 1
REFERENCES,0.29806451612903223,"Ny
∥Bµ∥2
F ,
(22)"
REFERENCES,0.29935483870967744,and W1 and W2 follow
REFERENCES,0.3006451612903226,"W1 = B1(D1A1)+,
W2 = B1(D1A1)+(Ix −U2U T
2 ) + B2(D2A2)+.
(23)"
REFERENCES,0.30193548387096775,The error in task 2 after learning of task 1 is thus written as
REFERENCES,0.3032258064516129,ϵ2[W1] = 1 Ny
REFERENCES,0.30451612903225805,"B2 −B1(D1A1)+D2A2
2
F .
(24)"
REFERENCES,0.3058064516129032,"Similarly, the error on task 1 after learning of task 2 is"
REFERENCES,0.30709677419354836,ϵ1[W2] = 1 Ny
REFERENCES,0.30838709677419357,"B1
 
Is −(D1A1)+(Ix −U2U T
2 )D1A1

−B2(D2A2)+D1A1
2
F
(25)"
REFERENCES,0.3096774193548387,"Note that the results so far does not rely on Assumptions I and II, making them applicable to arbitrary
task matrices (A, B)."
REFERENCES,0.3109677419354839,"The weight matrix after learning depends on the pseudo-inverse, (DA)+ = V Σ−1U T , which is
typically a complicated function of the original matrix DA. However, when αNx ≫Ns, it can be
approximated by a scaled transpose,
Ns
αNx (DA)T (see Eq. 101)."
REFERENCES,0.31225806451612903,"Random and adaptive activity gating
In the adaptive activity gating model, we generate input
gating units {gµ
i } randomly, but with correlations between subsequent tasks. We sample the gating
units for the first task with g1
i ←Bernoulli(α), where α is the sparsity of the gating units. The gating
units for the µ + 1 task, {gµ+1
i
}, are then generated by"
REFERENCES,0.3135483870967742,"gµ+1
i
=
gµ
i
(with probability ρg
µ+1)
Bernoulli(α)
(otherwise)
(26)"
REFERENCES,0.31483870967741934,"Here, ρg
µ+1 is the correlation between the gating units for the µ-th and µ + 1-th tasks. The gating
correlation ρg
µ is set to zero in the case of random task-dependent gating, whereas ρg
µ is adjusted
based on the model performance right after a model switch in the adaptive task-dependent gating. To
this end, we introduce a probe trial where the model solves the new task using the gating vector for
the old task. The error on the probe trial is"
REFERENCES,0.3161290322580645,ϵpb ≡1
REFERENCES,0.3174193548387097,"Ny
∥B2 −W1D1A2∥2
F .
(27)"
REFERENCES,0.31870967741935485,We then set the probability of using the same gating elements by
REFERENCES,0.32,"ρg = max
n
0, 1 −ϵpb ϵo"
REFERENCES,0.32129032258064516,"o
(28)"
REFERENCES,0.3225806451612903,"where ϵo ≡
1
Ny ∥B2∥2
F is the baseline error on the task 2. It keeps the gating the same if ϵpb = 0,
while it resamples all gating elements in case ϵpb ≥ϵo."
REFERENCES,0.32387096774193547,"Transfer performance
The average transfer performance ∆ϵT F from the first to the second tasks
is:"
REFERENCES,0.3251612903225806,∆ϵT F = 1 Ny
REFERENCES,0.32645161290322583,"D
∥B2∥2
F
E
−1 Ny"
REFERENCES,0.327741935483871,"B2 −B1 (D1A1)+ D2A2

2 F  = 1 Ny"
REFERENCES,0.32903225806451614,"D
2tr

BT
2 B1(D1A1)+D2A2

−
B1(D1A1)+D2A2
2
F E = 2ρb Ns"
REFERENCES,0.3303225806451613,"tr

(D1A1)+D2A2

−1 Ns"
REFERENCES,0.33161290322580644,"D(D1A1)+D2A2
2
F"
REFERENCES,0.3329032258064516,"E
.
(29)"
REFERENCES,0.33419354838709675,"In the last line, we took the expectation over the correlated random matrices B1 and B2. Using the
approximation (DA)+ ≈
Ns
αNx (DA)T (Eq. 101), and then taking the expectation over A1, A2, D1,
and D2, the first term becomes

tr[(D1A1)+D2A2]

≈
Ns
αNx

tr[AT
1 D1D2A2]"
REFERENCES,0.33548387096774196,"=
Ns
αNx Ns
X i=1 Nx
X j=1"
REFERENCES,0.3367741935483871,"D
a(1)
ji g(1)
j g(2)
j a(2)
ji
E
= ˜αρaNs,
(30)"
REFERENCES,0.33806451612903227,"where a(1)
ji represents the (j, i)-th element of A1, and ˜α is defined by"
REFERENCES,0.3393548387096774,"˜α ≡ρg + (1 −ρg)α.
(31)"
REFERENCES,0.3406451612903226,The second term of Eq. 29 follows
REFERENCES,0.3419354838709677,"D(D1A1)+D2A2
2
F"
REFERENCES,0.3432258064516129,"E
≈

Ns
αNx"
REFERENCES,0.3445161290322581,"2 DAT
1 D1D2A2
2
F E"
REFERENCES,0.34580645161290324,"=

Ns
αNx"
REFERENCES,0.3470967741935484,"2
Ns
X i,k=1 Nx
X j,l=1"
REFERENCES,0.34838709677419355,"D
g(1)
j g(2)
j g(1)
l
g(2)
l
a(1)
ji a(2)
jk a(2)
lk a(1)
li
E"
REFERENCES,0.3496774193548387,"=
1
α2N 2x Ns
X i,k=1 Nx
X j,l=1"
REFERENCES,0.35096774193548386,"D
g(1)
j g(2)
j g(1)
l
g(2)
l
[δikρ2
a + δjl + δikδjlρ2
a]
E = Ns"
REFERENCES,0.352258064516129,"
˜α2ρ2
a + ˜αNs αNx"
REFERENCES,0.3535483870967742,"
1 + ρ2
a
Ns"
REFERENCES,0.3548387096774194,"
.
(32)"
REFERENCES,0.3561290322580645,"The third line follows from Isserlis’ theorem, from which we can decompose the higher-order
correlation as below:
D
a(1)
ji a(2)
jk a(2)
lk a(1)
li
E
=
D
a(1)
ji a(2)
jk
E D
a(2)
lk a(1)
li
E
+
D
a(1)
ji a(1)
li
E D
a(2)
jk a(2)
lk
E
+
D
a(1)
ji a(2)
lk
E D
a(2)
jk a(1)
li
E"
REFERENCES,0.3574193548387097,"= ρ2
a
N2
s δik +
1
N 2
s δjl + ρ2
a
N2
s δikδjl.
(33)"
REFERENCES,0.35870967741935483,"Summing up the equations above, at
Ns
αNx →0 limit, we have"
REFERENCES,0.36,"∆ϵT F = ˜αρa (2ρb −˜αρa) .
(34)"
REFERENCES,0.36129032258064514,"Note that when A2 is generated by random replacement, instead of sampling from a corre-
lated Gaussian distribution,
D
∥(D1a1)+D2A2∥2
F
E
term instead follows
D
∥(D1a1)+D2A2∥2
F
E
≈"
REFERENCES,0.36258064516129035,"Ns

˜α2ρ2
a + ˜αNs αNx"
REFERENCES,0.3638709677419355,"h
1 + ρa"
REFERENCES,0.36516129032258066,"Ns (2 −ρaα˜α)
i
, which converges to Eq. 32 at Ns ≪Nx limit."
REFERENCES,0.3664516129032258,"Retention
performance
Let
us
next
analyze
the
retention
performance,
∆ϵRT
≡
⟨ϵ1[Wo] −ϵ1[W2]⟩A,B, which characterizes how well the network performs the task 1 after learning
task 2. At αNx ≫Ns regime, ⟨ϵ1[Wo]⟩A,B = 1. Inserting Eq. 23 into Eq. 14, we get"
REFERENCES,0.36774193548387096,ϵ1[W2] = 1 Ny
REFERENCES,0.3690322580645161,"B1 −

B1(D1A1)+(Ix −U2U T
2 ) + B2(D2A2)+
D1A1
2
F = 1 Ny"
REFERENCES,0.37032258064516127,"B1(D1A1)+U2U T
2 D1A1 −B2(D2A2)+D1A1
2
F .
(35)"
REFERENCES,0.3716129032258065,"Thus, ∆ϵRT follows"
REFERENCES,0.37290322580645163,∆ϵRT = 1 Ny
REFERENCES,0.3741935483870968,"D
∥B1∥2
F
E
−1 Ny"
REFERENCES,0.37548387096774194,"DB1(D1A1)+U2U T
2 D1A1
2
F E
−1 Ny"
REFERENCES,0.3767741935483871,"DB2(D2A2)+D1A1
2
F E + 2 Ny"
REFERENCES,0.37806451612903225,"D
tr
h
BT
2 B1(D1A1)+U2U T
2 D1A1(D1A1)T  
(D2A2)+T iE"
REFERENCES,0.3793548387096774,= 1 −1 Ns
REFERENCES,0.38064516129032255,"D(D1A1)+U2U T
2 D1A1
2
F E
−1 Ns"
REFERENCES,0.38193548387096776,"D(D2A2)+D1A1
2
F E + 2ρb Ns"
REFERENCES,0.3832258064516129,"D
tr
h
(D1A1)+U2U T
2 D1A1(D1A1)T  
(D2A2)+T iE
.
(36)"
REFERENCES,0.38451612903225807,"Because D2A2AT
2 D2 = U2Σ2
2U T
2 , at the Ns ≪αNx limit, U2U T
2 is approximated by (see Eq. 102)"
REFERENCES,0.3858064516129032,"U2U T
2 ≈Ns"
REFERENCES,0.3870967741935484,"αNx
D2A2AT
2 D2.
(37)"
REFERENCES,0.38838709677419353,"Figure 8: The gating level dependence of the transfer and retention performance (A) Phase diagram
of the gating level dependence. (B-D) Transfer and retention performance as a function of the gating
level at a representative point of each phase."
REFERENCES,0.3896774193548387,"Using this approximation and (DA)+ ≈
Ns
αNx (DA)T (Eq. 101), we can obtain an analytical
expression for Eq. 36. Taking the expectation over D and A, the second term of Eq. 36 becomes"
NS,0.3909677419354839,"1
Ns  Ns αNx"
NS,0.39225806451612905,"4 DAT
1 D1D2A2AT
2 D2D1A1
2
F E =
X ijkl X mnpq"
NS,0.3935483870967742,"D
g(1)
j g(1)
l
g(1)
n g(1)
q g(2)
j g(2)
l
g(2)
n g(2)
q a(1)
ji a(2)
jk a(2)
lk a(1)
lma(1)
nma(2)
np a(2)
qp a(1)
qi
E"
NS,0.39483870967741935,"=
1
α4NsN 4x X ijkl X mnpq"
NS,0.3961290322580645,"D
g(1)
j g(1)
l
g(1)
n g(1)
q g(2)
j g(2)
l
g(2)
n g(2)
q
E"
NS,0.39741935483870966,"×
 
[δikδkmδmp + δikδlqδmp + δkmδjnδip] ρ4
a + [δikδkmδnq + δkmδmpδjq + δmpδpiδjl + δpiδikδln] ρ2
a
"
NS,0.3987096774193548,"+ O

1
αNx "
NS,0.4,"= ˜α4ρ4
a +
Ns
αNx ˜α3(4ρ2
a + 2ρ4
a) + O

1
αNx"
NS,0.4012903225806452,"
,
(38)"
NS,0.40258064516129033,"Here, we applied Isserlis’ theorem again, then retained the terms up to the next-to-leading order with
respect to
Ns
αNx . Similarly, the third term of Eq. 36 becomes"
NS,0.4038709677419355,"1
Ns  Ns αNx"
NS,0.40516129032258064,"2 
tr

AT
1 D1D2A2AT
2 D2D1A1

= ˜α2ρ2
a + ˜αNs αNx"
NS,0.4064516129032258,"
1 + ρ2
a
Ns"
NS,0.40774193548387094,"
.
(39)"
NS,0.40903225806451615,"Lastly, the cross-term is evaluated as"
NS,0.4103225806451613,"1
Ns  Ns αNx"
NS,0.41161290322580646,"3 
tr

AT
2 D2D1A1AT
1 D1D2A2AT
2 D2D1A1
"
NS,0.4129032258064516,"=
1
α3NsN 3x X ijk X lmn"
NS,0.41419354838709677,"D
g(1)
j g(1)
l
g(1)
n g(2)
j g(2)
l
g(2)
n
E  
δikδkmρ3
a + δikδlnρa + δimδjlρa + δmkδjnρ3
a

+ O

1
αNx "
NS,0.4154838709677419,"= ˜α3ρ3
a +
Ns
αNx ˜α2(2ρa + ρ3
a) + O

1
αNx"
NS,0.4167741935483871,"
.
(40)"
NS,0.4180645161290323,"Therefore, up to the leading order, the retention performance follows"
NS,0.41935483870967744,"∆ϵRT = 1 −˜α2ρ2
a
 
˜α2ρ2
a −2˜αρaρb + 1

+ O

Ns
αNx"
NS,0.4206451612903226,"
.
(41)"
NS,0.42193548387096774,"Vanilla model
In the vanilla model, we have ˜α = 1. Therefore, at Ns ≪Nx limit, from Eqs. 34
and 41, the transfer and retention performance follow"
NS,0.4232258064516129,"∆ϵT F = ρa(2ρb −ρa),
(42a)"
NS,0.42451612903225805,"∆ϵRT = 1 −ρ2
a(ρ2
a −2ρaρb + 1).
(42b)"
NS,0.4258064516129032,"Random activity gating
Under the random task-dependent activity gating, the gating level ˜α = α
is a constant. If Ns ≪αNx, the transfer and retention performance are written by"
NS,0.4270967741935484,"∆ϵT F = αρa(2ρb −αρa),
(43a)"
NS,0.42838709677419357,"∆ϵRT = 1 −α2ρ2
a(α2ρ2
a −2αρaρb + 1).
(43b)"
NS,0.4296774193548387,"Note that, this expression does not hold at the sparse limit α < Ns Nx ."
NS,0.4309677419354839,"Depending on (ρa, ρb) combination, the gating level influences the transfer and retention performance
in three different manners (Fig. 8). When ρb > ρa, unless both ρa and ρb are large, there is a
monotonic tradeoff between the transfer and retention performance (Fig. 8B). In the red region
where ρb ≥2ρa"
NS,0.432258064516129,"3 +
1
3ρa or ρb ≥2
√"
NS,0.4335483870967742,"2
3
∧ρa ≥
1
√"
NS,0.43483870967741933,"2, a large gating level above a threshold improves the
retention performance (Fig. 8C). When ρb < ρa, high gating level, α > ρb"
NS,0.43612903225806454,"ρa , lower both transfer and
retention performance, thus there is no benefit of choosing large α (Fig. 8D)."
NS,0.4374193548387097,"Adaptive activity gating
In the adaptive task-dependent activity gating model, we introduced a
probe trial at the beginning of the task 2 to test the model performance for the new task. Under an
adaptive gating with ρg = max
n
0, 1 −ϵpb ϵo"
NS,0.43870967741935485,"o
, the probe error ϵpb follows:"
NS,0.44,"⟨ϵpb⟩A,B,g =
 1"
NS,0.44129032258064516,"Ny
∥B2 −W1D1A2∥2
F  A,B,g ≈"
NS,0.4425806451612903,"*
1
Ny"
NS,0.44387096774193546,B2 −Ns
NS,0.44516129032258067,"αNx
B1(D1A1)T D1A2  2 F + A,B,g"
NS,0.4464516129032258,"= 1 −2ρaρb + ρ2
a + O
 Ns αNx"
NS,0.447741935483871,"
.
(44)"
NS,0.44903225806451613,"Thus, at Ns ≪αNx region, ρg = max {0, ρa(2ρb −ρa)}, and the effective gating level becomes"
NS,0.4503225806451613,"eα =
α + (1 −α)ρa (2ρb −ρa)
(if 2ρb ≥ρa)
α
(otherwise)
(45)"
NS,0.45161290322580644,"meaning that if the transfer performance is positive in the vanilla model, there should be an overlap
in the gating, but otherwise, the gating vector for the next task should be sampled independently."
NS,0.4529032258064516,"Average error under a uniform prior on task similarity
Gating influences the performance
differently depending on the feature and readout similarity. Thus, ideally the model should adjust the
gating level based on the task similarity, but in most practical settings, the model doesn’t know the
similarity a priori. Therefore, it is important to analyze how it performs on average under a randomly
chosen task similarity. To this end, here we measure the average transfer and retention performance
assuming a uniform prior on 0 ≤ρa, ρb ≤1."
NS,0.4541935483870968,"In the vanilla model, we have"
NS,0.45548387096774196,"∆¯ϵT F =
Z 1 0
dρa Z 1"
NS,0.4567741935483871,"0
dρb (ρa(2ρb −ρa)) = 1"
NS,0.45806451612903226,"6,
(46a)"
NS,0.4593548387096774,"∆¯ϵRT =
Z 1 0
dρa Z 1"
NS,0.46064516129032257,"0
dρb
 
1 −ρ2
a

ρ2
a −2ρaρb + 1

= 43"
NS,0.4619354838709677,"60.
(46b)"
NS,0.4632258064516129,"Red and blue dash lines in Figs. 3D, 4C, 4D, 5D, 6C, and 6D represent these baseline performance of
the vanilla model: ∆¯ϵT F = 1"
NS,0.4645161290322581,6 and ∆¯ϵRT = 43
NS,0.46580645161290324,"60. By contrast, under a random task-dependent gating,"
NS,0.4670967741935484,∆¯ϵT F = α 2 −α2
NS,0.46838709677419355,"3 ,
∆¯ϵRT = 1 −α2"
NS,0.4696774193548387,3 + α3 4 −α4
NS,0.47096774193548385,"5 .
(47)"
NS,0.472258064516129,"Solid lines in Fig. 3D and dashed lines in Fig. 4B plot the result above. In the case of adaptive gating,
the expression of the average performance becomes complicated, yet still numerically tractable."
NS,0.4735483870967742,"Note that the average transfer performance is expected to be lower than the retention performance,
because forward knowledge transfer requires a high similarity. Even if we use the optimal gating
level for transfer, α∗(ρa, ρb) = min(1, ρb"
NS,0.47483870967741937,"ρa ), the average transfer performance becomes Z 1 0
dρa Z 1"
NS,0.4761290322580645,"0
dρbα∗(ρa, ρb)ρa(2ρb −α∗(ρa, ρb)ρa) = 1"
NS,0.4774193548387097,"4.
(48)"
NS,0.47870967741935483,"B.2
Plasticity gating"
NS,0.48,"An alternative strategy is to gate plasticity at certain synapses in a task-dependent manner while
keeping the activity intact. We implement this method by making synapses from only a subset of
input neurons plastic for each task. Given a linear regression model y = Wx, the learning dynamics
follows:
˙W(t) = −η(WAµ −Bµ)(DµAµ)T ,
(49)"
NS,0.48129032258064514,"where Dµ = diag(gµ) is a diagonal matrix specifying which synapses are gated. As in the activity
gating, gµ is a gating vector whose elements take either one or zero in a task-dependent manner. We
sample elements of gµ from a Bernoulli distribution with probability α as before."
NS,0.48258064516129034,"Let us denote SVD of Aµ by Aµ = UµΛµV T
µ . Then, from a parallel argument with Eqs. 18 and 19,
we have"
NS,0.4838709677419355,"Wµ = Wµ−1 + (BµVµΛ−1
µ
−Wµ−1Uµ)(U T
µ DµUµ)−1U T
µ Dµ.
(50)"
NS,0.48516129032258065,"Notably, unlike Eq. 20b, gating term Dµ appears both inside and outside of the inverse term
((U T
µ D uUµ)−1 and U T
µ Dµ). Thus, up to the leading order, gating level α does not affect the transfer
and retention performance."
NS,0.4864516129032258,"B.3
Input soft-thresholding"
NS,0.48774193548387096,"Input-dependent gating is an another method proposed for efficient continual learning. In a regression
setting, it corresponds to soft-thresholding of the inputs. Let us introduce a soft-thresholding function
φ(x) by"
NS,0.4890322580645161,"φ(x) = sgn(x) max{0, |x| −h},
(51)"
NS,0.49032258064516127,"where sgn(x) represents the sign of x. This nonlinearity makes the analysis difficult, but if Ns ≫1
in addition to Assumptions I and II, the error becomes tractable. Note that Ns ≫1 assumption is
required only in this subsection. Under the student network defined by"
NS,0.4916129032258065,"ˆy = Wφ(x),
(52)"
NS,0.49290322580645163,the mean-squared error follows
NS,0.4941935483870968,ϵ[W] = 1 Ny
NS,0.49548387096774194,"D
∥Bs∥2E s + 1 Ny"
NS,0.4967741935483871,"D
∥Wφ(As)∥2E s −2 Ny"
NS,0.49806451612903224,"tr

Bsφ(As)T W T "
NS,0.4993548387096774,"s .
(53)"
NS,0.5006451612903225,"Let us denote mi ≡PNs
j=1 Aijsj, then we have"
NS,0.5019354838709678,"⟨mi⟩= 0,

m2
i

= Ns
X"
NS,0.5032258064516129,"j=1
A2
ij ≈1,
⟨sjmi⟩= Aij.
(54)"
NS,0.5045161290322581,"Thus, sj and mi approximately follows a joint Gaussian distribution:

sj
mi"
NS,0.5058064516129033,"
∼N

0
0"
NS,0.5070967741935484,"
,

1
Aji
Aji
1"
NS,0.5083870967741936,"
.
(55)"
NS,0.5096774193548387,"Denoting Aji = ρ for brevity, we have"
NS,0.5109677419354839,"⟨sjφ(mi)⟩ =
Z ∞ −∞
ds Z −h"
NS,0.512258064516129,"−∞
dm(m + h) +
Z ∞"
NS,0.5135483870967742,"h
dm(m −h) !
s 2π
p"
NS,0.5148387096774194,"1 −ρ2 exp

−
1
2(1 −ρ2)

s2 + m2 −2ρsm
 =
Z ∞ −∞
ds Z −h"
NS,0.5161290322580645,"−∞
dm(m + h) +
Z ∞"
NS,0.5174193548387097,"h
dm(m −h)"
NS,0.5187096774193548,"!
s
2π"
NS,0.52,"h
1 + ρsm + ρ2"
NS,0.5212903225806451,"2 (s2 −1)(m2 −1) + O(ρ3)
i"
NS,0.5225806451612903,"× exp

−s2 + m2 2 "
NS,0.5238709677419355,"= erfc
h
h
√ 2"
NS,0.5251612903225806,"i
ρ + O(ρ3).
(56)"
NS,0.5264516129032258,"In the third line, we performed a Taylor expansion around ρ = 0. Similarly, the expectation of
φ(mj)φ(mi) over s is written as"
NS,0.5277419354838709,⟨φ(mj)φ(mi)⟩= 1 2π Z −h
NS,0.5290322580645161,"−∞
dm(m + h) +
Z ∞"
NS,0.5303225806451612,"h
dm(m −h)"
NS,0.5316129032258065,!  Z −h
NS,0.5329032258064517,"−∞
dm′(m′ + h) +
Z ∞"
NS,0.5341935483870968,"h
dm′(m′ −h) !"
NS,0.535483870967742,"×
h
1 + ρcmm′ + ρ2
c
2 (m2 −1)(m′2 −1) + O(ρ3
c)
i
exp

−m2 + m′2 2 "
NS,0.5367741935483871,"=

erfc
h
h
√ 2"
NS,0.5380645161290323,"i2
ρc + O(ρ3
c),
(57)"
NS,0.5393548387096774,"where ρc = PNs
k=1 AikAjk. Therefore, if |ρ| ≪1 and |ρc| ≪1, the error ϵ[W] is written as"
NS,0.5406451612903226,ϵ[W] = 1
NS,0.5419354838709678,"Ny
∥B∥2
F + α2"
NS,0.5432258064516129,"Ny
tr

WAAT W T 
−2α"
NS,0.5445161290322581,"Ny
tr[BAT W T ],
(58)"
NS,0.5458064516129032,"where the input activity sparseness α follows α = erfc
h
h
√ 2"
NS,0.5470967741935484,"i
. Under this loss function, gradient
descent from W = Wo converges to"
NS,0.5483870967741935,W = Wo(I −UU T ) + 1
NS,0.5496774193548387,"αBA+,
(59)"
NS,0.5509677419354839,"where A = UΛV T is the singular value decomposition of A. Therefore, from Wo = O, the weights
after task 1 and task 2 become"
NS,0.552258064516129,W1 = 1
NS,0.5535483870967742,"α
f
W1,
W2 = 1"
NS,0.5548387096774193,"α
f
W2,
(60)"
NS,0.5561290322580645,"where
f
W1 ≡B1A+
1 ,
f
W2 ≡B1A+
1

I −U2U T
2

+ B2A+
2 .
(61)"
NS,0.5574193548387096,"This means that for both tasks (µ, ν = 1, 2)"
NS,0.5587096774193548,ϵµ[Wν] = 1
NS,0.56,"Ny
∥Bµ∥2
F + 1"
NS,0.5612903225806452,"Ny
tr
h
f
WνAµAT
µ f
W T
ν
i
−2"
NS,0.5625806451612904,"Ny
tr[BµAT
µ f
W T
ν ],
(62)"
NS,0.5638709677419355,"implying that the error is invariant against input sparseness α. Thus, in our problem setting, input
sparsification via soft-thresholding doesn’t influence knowledge transfer or retention. It also implies
that from energy efficiency perspective, soft-thresholding is beneficial because it reduces the number
of active neurons while preserving its learning proficiency."
NS,0.5651612903225807,"C
Weight regularization"
NS,0.5664516129032258,"C.1
Weight regularization in Euclidean metric"
NS,0.567741935483871,"Let us next consider a weight regularization approach by introducing L2 regularization with respect
to the weight learned in the previous task. The loss function ℓµ for the µ-th task is then given by"
NS,0.5690322580645162,ℓµ = 1
NS,0.5703225806451613,"2 ∥Bµ −WAµ∥2
F + λ"
NS,0.5716129032258065,"2 ∥W −Wµ−1∥2
F .
(63)"
NS,0.5729032258064516,"When λ > 0, there exists a unique solution:"
NS,0.5741935483870968,"W = (BµAT
µ + λWµ−1)(AµAT
µ + λIx)−1.
(64)"
NS,0.5754838709677419,"Thus, from zero-initialization, W1 and W2 become"
NS,0.5767741935483871,"W1 = B1AT
1
 
A1AT
1 + λIx
−1
(65a)"
NS,0.5780645161290323,"W2 =
 
B2AT
2 + λB1AT
1 [A1AT
1 + λIx]−1
(A2AT
2 + λIx)−1.
(65b)"
NS,0.5793548387096774,"Under Nx ≫Ns, the inverse term is approximated by"
NS,0.5806451612903226," 
AAT + λIx
−1 = 1"
NS,0.5819354838709677,λIx −1
NS,0.5832258064516129,"λ2 A

Is + 1"
NS,0.584516129032258,"λAT A
−1
AT ≈1 λ"
NS,0.5858064516129032,"
Ix −
Ns
Nx + λNs
AAT

.
(66)"
NS,0.5870967741935483,"Figure 9: Weight regularization in Euclidean metric. (A,B) Optimal regularizer coefficient γ that
maximizes the transfer performance (A), and the maximum performance at the optimal γ (B) under
various (ρa, ρb) pairs. (C,D) Optimal regularizer coefficient for retention (C), and the resultant
performance (D). Panel C is the same with Fig. 5C (replicated for completeness)."
NS,0.5883870967741935,"In the first line, we used Woodbury matrix identity, and in the second line, we used AT A ≈Nx"
NS,0.5896774193548387,"Ns Is
(see Eq. 99). Let us introduce normalized regularization amplitude γ and ˜γ by"
NS,0.5909677419354838,"γ ≡
Nx
Nx + λNs
,
eγ ≡
Ns
Nx + λNs
.
(67)"
NS,0.5922580645161291,"Under these approximations, W1 and W2 simplify to W1 ≈1"
NS,0.5935483870967742,"λB1AT
1
 
1 −eγA1AT
1

≈eγB1AT
1 ,
(68a) W2 ≈1"
NS,0.5948387096774194,"λ
 
B2AT
2 + λeγB1AT
1
  
Ix −eγA2AT
2

≈eγ
 
B1AT
1 + B2AT
2

−eγ2B1AT
1 A2AT
2 .
(68b)"
NS,0.5961290322580645,"Here, we further applied AT A ≈Nx"
NS,0.5974193548387097,"Ns Is. Note that, unlike Eq. 20b, the result above holds only under
Assumptions I & II."
NS,0.5987096774193549,"Transfer performance
The transfer performance ∆ϵT F follows"
NS,0.6,∆ϵT F = 1 Ny
NS,0.6012903225806452,"D
∥B2∥2
F
E
−1 Ny"
NS,0.6025806451612903,"DB2 −eγB1AT
1 A2
2
F E = 2eγ Ny"
NS,0.6038709677419355,"tr[BT
2 B1AT
1 A2]

−eγ2 Ny"
NS,0.6051612903225806,"DB1AT
1 A2
2
F E"
NS,0.6064516129032258,= 2eγρb Ns
NS,0.607741935483871,"tr[AT
1 A2]

−eγ2 Ns"
NS,0.6090322580645161,"DAT
1 A2
2
F E"
NS,0.6103225806451613,"= γρa (2ρb −γρa) + O

Ns
Nx"
NS,0.6116129032258064,"
.
(69)"
NS,0.6129032258064516,"In the last line, we used Eq. 33 to estimate
DAT
1 A2
2
F"
NS,0.6141935483870967,"E
. Notably, the expression of the transfer
performance, ∆ϵT F , coincides with Eq. 5a under γ →α, indicating that in terms of transfer
performance, the weight regularization with amplitude Nx"
NS,0.6154838709677419,"Ns
  1"
NS,0.6167741935483871,"α −1

is equivalent to random activity
gating with gating level α. Figs. 9A and B show the optimal regularizer coefficient γ that maximizes
transfer performance and the resultant performance."
NS,0.6180645161290322,"Retention performance
The average error on task 1 after learning task 2 is"
NS,0.6193548387096774,"⟨ϵ1[W2]⟩A,B =
 1 Ny"
NS,0.6206451612903225,"B1 −
 
eγ[B1AT
1 + B2AT
2 ] −eγ2B1AT
1 A2AT
2

A1
2
F  = 1 Ny"
NS,0.6219354838709678,"DB1(I −eγAT
1 A1)
2
F"
NS,0.6232258064516129,"E
+ 2eγ Ny"
NS,0.6245161290322581,"tr

(I −eγAT
1 A1)BT
1 (eγB1AT
1 A2 −B2)AT
2 A1
 + eγ2 Ny"
NS,0.6258064516129033,"D(eγB1AT
1 A2 −B2)AT
2 A1
2
F"
NS,0.6270967741935484,"E
.
(70)"
NS,0.6283870967741936,"Taking the expectation over A1, A2, B1, B2, up to the leading order with respect to Ns"
NS,0.6296774193548387,"Nx , we have"
NY,0.6309677419354839,"1
Ny"
NY,0.632258064516129,"DB1(I −eγAT
1 A1)
2
F E
= 1 Ny"
NY,0.6335483870967742,"D
∥B1∥2
F −2eγtr[BT
1 B1AT
1 A1] + eγ2 B1AT
1 A1
2
F E"
NY,0.6348387096774194,= 1 −2eγ Ns
NY,0.6361290322580645,"tr[AT
1 A1]

+ eγ2 Ns"
NY,0.6374193548387097,"DAT
1 A1
2
F E"
NY,0.6387096774193548,"= (1 −γ)2 + O

Ns
Nx"
NY,0.64,"
.
(71)"
NY,0.6412903225806451,"Similarly, we have eγ
Ny"
NY,0.6425806451612903,"tr

(I −eγAT
1 A1)BT
1 (eγB1AT
1 A2 −B2)AT
2 A1
 = eγ Ns"
NY,0.6438709677419355,"tr

(I −eγAT
1 A1)(eγAT
1 A2 −ρbI)AT
2 A1
"
NY,0.6451612903225806,"= ρaγ(1 −γ)(γρa −ρb) + O

Ns
Nx"
NY,0.6464516129032258,"
,
(72)"
NY,0.6477419354838709,"and
1
Ny
eγ2 D(eγB1AT
1 A2 −B2)AT
2 A1
2
F E = eγ2 Ns"
NY,0.6490322580645161,"tr[AT
1 A2AT
2 A1] −2eγρbtr[AT
1 A2AT
1 A2AT
2 A1] + eγ2tr[AT
1 A2AT
2 A1AT
1 A2AT
2 A1]"
NY,0.6503225806451612,"= ρ2
aγ2  
1 −2ρaρbγ + ρ2
aγ2
+ O

Ns
Nx"
NY,0.6516129032258065,"
.
(73)"
NY,0.6529032258064517,"Therefore, the error is written as"
NY,0.6541935483870968,"⟨ϵ1[W2]⟩= (1 −γ)2 −2γρa(1 −γ)(ρb −γρa) + γ2ρ2
a(1 −2γρaρb + γ2ρ2
a).
(74)"
NY,0.655483870967742,"The optimal γ for each task similarity (ρa, ρb) and the resultant retention performance are plotted in
Figs. 9C and D."
NY,0.6567741935483871,"C.2
Elastic weight regularization"
NY,0.6580645161290323,"In the previous section, we regularized the change in the weight matrix in the Euclidean space.
However, previous works indicate that the weight should be regularized using the Fisher information
matrix (FIM) as the metric [30, 67]. Let us construct an inference model by y = Wx + σξ, where ξ
is a zero mean Gaussian random variable. Given input-target pair x, y, the likelihood of weight W is"
NY,0.6593548387096774,"p(W|x, y) ∝p(x, y|W)p(W) ∝exp

−1"
NY,0.6606451612903226,"2σ2 ∥Wx −y∥2

.
(75)"
NY,0.6619354838709678,"Thus, (ij, kl)-th component of FIM becomes

∂2"
NY,0.6632258064516129,"∂wijwkl
(−log p (W|x, y))"
NY,0.6645161290322581,"s
=
1
2σ2 δik *X m X"
NY,0.6658064516129032,"n
ajmalnsmsn +"
NY,0.6670967741935484,"s
=
1
2σ2 δik
X"
NY,0.6683870967741935,"n
ajnaln, (76)"
NY,0.6696774193548387,"and the quadratic weight regularization in the Fisher information metric follows X ij X kl  δik
X"
NY,0.6709677419354839,"n
ajnaln !"
NY,0.672258064516129,"(wij −wo
ij)(wkl −wo
kl) = ∥(W −Wo)A∥2
F .
(77)"
NY,0.6735483870967742,"Therefore, the loss function for this weight regularizer is written as"
NY,0.6748387096774193,ℓµ = 1
NY,0.6761290322580645,"2 ∥Bµ −WAµ∥2
F + λ"
NY,0.6774193548387096,"2 ∥(W −Wµ−1)Aµ−1∥2
F .
(78)"
NY,0.6787096774193548,"As we will see, this loss function has different forms of solution depending on whether ρa < 1 or
ρa = 1. We thus consider these two conditions separately below."
NY,0.68,"C.2.1
Variable features (ρa < 1)"
NY,0.6812903225806451,"Taking gradient with respect to W, we have
∂ℓµ
∂W = −(Bµ −WAµ)AT
µ + λ(W −Wµ−1)Aµ−1AT
µ−1,
(79)"
NY,0.6825806451612904,"indicating that W is written as W = Wµ−1 + Q eAT where Q is a Ny × 2Ns matrix and eA ≡
[Aµ−1, Aµ] is an Nx × 2Ns matrix. Solving ∂ℓµ"
NY,0.6838709677419355,"∂W = 0, we get"
NY,0.6851612903225807,"Q
λAT
µ−1Aµ−1
AT
µ−1Aµ
λAT
µAµ−1
AT
µAµ"
NY,0.6864516129032258," AT
µ−1
AT
µ"
NY,0.687741935483871,"
= (O
Bµ −Wµ−1Aµ)
AT
µ−1
AT
µ"
NY,0.6890322580645162,"
,
(80)"
NY,0.6903225806451613,"Multiplying both sides with (Aµ−1 Aµ) from the right side,"
NY,0.6916129032258065,"Q
λAT
µ−1Aµ−1
AT
µ−1Aµ
λAT
µAµ−1
AT
µAµ"
NY,0.6929032258064516," AT
µ−1Aµ−1
AT
µ−1Aµ
AT
µAµ−1
AT
µAµ "
NY,0.6941935483870968,"= (O
Bµ −Wµ−1Aµ)
AT
µ−1Aµ−1
AT
µ−1Aµ
AT
µAµ−1
AT
µAµ"
NY,0.6954838709677419,"
(81)"
NY,0.6967741935483871,"If λ ̸= 0 and ρa < 1, two square matrices in the above equations are almost surely invertible under
assumptions I & II. Therefore, we have"
NY,0.6980645161290323,"Q = (O
Bµ −Wµ−1Aµ)
λAT
µ−1Aµ−1
AT
µ−1Aµ
λAT
µAµ−1
AT
µAµ"
NY,0.6993548387096774,"−1
(82)"
NY,0.7006451612903226,"Under Nx ≫Ns, from Eq. 99, the inverse matrix is approximated by
λAT
µ−1Aµ−1
AT
µ−1Aµ
λAT
µAµ−1
AT
µAµ"
NY,0.7019354838709677,"−1
≈Ns Nx"
NY,0.7032258064516129,"
λIs
ρaIs
λρaIs
Is"
NY,0.704516129032258,"−1
=
Ns
Nx(1 −ρ2a)"
NY,0.7058064516129032,"
1
λIs
−ρa"
NY,0.7070967741935484,"λ Is
−ρaIs
Is  (83)"
NY,0.7083870967741935,"Therefore, W = Wµ−1 + Q eAT follows"
NY,0.7096774193548387,W = Wµ−1
NY,0.7109677419354838,"
I −
Ns
Nx(1 −ρ2a)Aµ

AT
µ −ρaAT
µ−1

+
Ns
Nx(1 −ρ2a)Bµ
 
AT
µ −ρaAT
µ−1

. (84)"
NY,0.712258064516129,"Notably, the equation above doesn’t depend on the regularizer amplitude λ, except for λ ̸= 0
condition. At λ →0 limit, the inverse matrix term in Eq. 82 effectively becomes singular, and thus
the equation above no longer holds."
NY,0.7135483870967742,"Let us suppose the first task is learned without any weight regularization, or learned with weight
regularization in the Fisher information metric imposed by an uncorrelated task. Then, we have
W1 = Ns"
NY,0.7148387096774194,"Nx B1AT
1 . Hence, the transfer performance becomes the same with the vanilla model. The
weight after the second task becomes"
NY,0.7161290322580646,W2 = Ns
NY,0.7174193548387097,"Nx
B1AT
1"
NY,0.7187096774193549,"
I −
Ns
Nx(1 −ρ2a)A2[AT
2 −ρaAT
1 ]

+
Ns
Nx(1 −ρ2a)B2(AT
2 −ρaAT
1 ).
(85)"
NY,0.72,"Thus, the retention performance under W2 follows"
NY,0.7212903225806452,∆ϵRT = 1 Ny
NY,0.7225806451612903,"D
∥B1∥2
F
E −1 Ny"
NY,0.7238709677419355,"B1

I −Ns"
NY,0.7251612903225807,"Nx AT
1 A1

+
Ns
Nx(1−ρ2a)

Ns
Nx B1AT
1 A2 −B2

(AT
2 −ρaAT
1 )A1

2 F ."
NY,0.7264516129032258,"(86)
Taking expectation over B1 and B2, ∆ϵRT is rewritten as"
NY,0.727741935483871,"∆ϵRT = 1 −
1
Ns I −Ns"
NY,0.7290322580645161,"Nx AT
1 A1

2 F "
NY,0.7303225806451613,"−
2
Nx(1−ρ2a)
D
tr
h
(I −Ns"
NY,0.7316129032258064,"Nx AT
1 AT
1 )

Ns
Nx AT
1 A2 −ρbI
  
AT
2 −ρaAT
1

A1
iE"
NY,0.7329032258064516,"−
1
Ns"
NY,0.7341935483870968,"
Ns
Nx(1−ρ2
a)
2  Ns"
NY,0.7354838709677419,"Nx AT
1 A2(AT
2 −ρaAT
1 )A1

2"
NY,0.7367741935483871,"F +
(AT
2 −ρaAT
1 )A1
2
F  + 2ρb Nx"
NY,0.7380645161290322,"
Ns
Nx(1−ρ2
a)
2 
tr

AT
1 (A2 −ρaA1)AT
2 A1(AT
2 −ρaAT
1 )A1

.
(87)"
NY,0.7393548387096774,The second term is rewritten as
NS,0.7406451612903225,"1
Ns I −Ns"
NS,0.7419354838709677,"Nx AT
1 A1

2 F"
NS,0.743225806451613,"= 1 −
2
Nx

tr[AT
1 A1]

+
1
Ns"
NS,0.7445161290322581,"
Ns
Nx"
NS,0.7458064516129033,"2 
tr[AT
1 A1AT
1 A1]

= Ns+1"
NS,0.7470967741935484,"Nx .
(88)"
NS,0.7483870967741936,"Moreover, (AT
2 −ρaAT
1 )A1 term also cancels out up to the leading order term because"
NS,0.7496774193548387,"1
Ns"
NS,0.7509677419354839,"
Ns
Nx(1−ρ2a)
2 D(AT
2 −ρaAT
1 )A1
2
F E"
NS,0.752258064516129,"=
1
Ns"
NS,0.7535483870967742,"
Ns
Nx(1−ρ2a)
2 X ijkl"
NS,0.7548387096774194,"D
a(1)
ji (a(2)
jk −ρaa(1)
jk )(a(2)
lk −ρaa(1)
lk )a(1)
li
E"
NS,0.7561290322580645,"=
1
Ns"
NS,0.7574193548387097,"
Ns
Nx(1−ρ2a)
2 X ijkl"
NS,0.7587096774193548,"D
a(1)
ji a(1)
li
E D
(a(2)
jk −ρaa(1)
jk )(a(2)
lk −ρaa(1)
lk )
E"
NS,0.76,"=
1
Ns"
NS,0.7612903225806451,"
Ns
Nx(1−ρ2
a)
2 X"
NS,0.7625806451612903,"ijkl
δjl
1−ρ2
a
N2
s
=
Ns
Nx(1−ρ2
a)
(89)"
NS,0.7638709677419355,"Similarly, the trace terms are also cancelled out up to the leading order term:"
NX,0.7651612903225806,"1
Nx"
NX,0.7664516129032258,"
Ns
Nx(1−ρ2a)
2 
tr[AT
1 (AT
2 −ρaA1)AT
2 A1(AT
2 −ρaAT
1 )A1]"
NX,0.7677419354838709,"=
1
Nx"
NX,0.7690322580645161,"
Ns
Nx(1−ρ2a)
2 X"
NX,0.7703225806451612,ijklmn
NX,0.7716129032258064,"D
a(1)
ji (a(2)
jk −ρaa(1)
jk )a(2)
lk a(1)
lm(a(2)
nm −ρaa(1)
nm)a(1)
ni
E"
NX,0.7729032258064517,"=
1
Nx"
NX,0.7741935483870968,"
Ns
Nx(1−ρ2a)
2 X"
NX,0.775483870967742,ijklmn
NX,0.7767741935483871,"D
(a(2)
jk −ρaa(1)
jk )(a(2)
nm −ρaa(1)
nm)
E D
a(1)
ji a(2)
lk a(1)
lma(1)
ni
E"
NX,0.7780645161290323,"=
1
Nx"
NX,0.7793548387096774,"
Ns
Nx(1−ρ2a)
2 X"
NX,0.7806451612903226,ijklmn
NX,0.7819354838709678,"1
Ns δjnδkm(1 −ρ2
a) 1"
NX,0.7832258064516129,"N 2
s (ρa + δjlδik2ρa)"
NX,0.7845161290322581,"=
Nsρa
Nx(1−ρ2a)

1 +
2
NxNs"
NX,0.7858064516129032,"
,
(90) and"
NX,0.7870967741935484,"1
Nx(1−ρ2a)
D
tr[(I −Ns"
NX,0.7883870967741935,"Nx AT
1 A1)( Ns"
NX,0.7896774193548387,"Nx AT
1 A2 −ρbI)(AT
2 −ρaAT
1 )A2]
E"
NX,0.7909677419354839,"=
1
Nx(1−ρ2a)
X ijkl *"
NX,0.792258064516129,"δij −Ns Nx
X"
NX,0.7935483870967742,"m
a(1)
mia(1)
mj !"
NX,0.7948387096774193,"Ns
Nx
X"
NX,0.7961290322580645,"n
a(1)
nj a(2)
nk −ρbδjk"
NX,0.7974193548387096,"! 
a(2)
lk −ρaa(1)
lk

a(2)
li + = 0. (91)"
NX,0.7987096774193548,"Thus, we get ∆ϵRT = 1 −O

Ns
Nx(1−ρ2a)

. Therefore, if Ns ≪(1 −ρ2
a)Nx, we have ∆ϵRT ≈1
regardless of task similarity ρa and ρb."
NX,0.8,"C.2.2
Fixed features (ρa = 1)"
NX,0.8012903225806451,"When ρa = 1, Aµ−1 = Aµ = A. The gradient follows"
NX,0.8025806451612904,"∂ℓµ
∂W = [−(Bµ −WA) + λ(W −Wµ−1)A] AT ,
(92)"
NX,0.8038709677419354,"and the weight W is written as W = Wµ−1 + QU T , where U is defined by SVD of A: A = UΛV T .
Solving ∂ℓµ"
NX,0.8051612903225807,"∂W = 0, we get"
NX,0.8064516129032258,W = Wµ−1
NX,0.807741935483871,"
I −
1
1 + λUU T

+
1
1 + λBµA+
(93)"
NX,0.8090322580645162,"Thus, the weight after task 1 and 2 follow"
NX,0.8103225806451613,"W1 =
1
1 + λB1A+,
W2 =
1
1 + λB1A+

I −
1
1 + λUU T

+
1
1 + λB2A+.
(94)"
NX,0.8116129032258065,"Therefore, the transfer performance is"
NX,0.8129032258064516,∆ϵT F = 1 Ny
NX,0.8141935483870968,"D
∥B2∥2E
−1 Ny"
NX,0.8154838709677419,"D
∥B2 −W1A2∥2E"
NX,0.8167741935483871,"=
2ρb
1 + λ −
1
(1 + λ)2 = γf (2ρb −γf) .
(95)"
NX,0.8180645161290323,"In the last line, we defined γf by γf ≡
1
1+λ. Similarly, the retention performance is written as"
NX,0.8193548387096774,∆ϵRT = 1 Ny
NX,0.8206451612903226,"D
∥B1∥2
F
E
−1 Ny"
NX,0.8219354838709677,"D
∥B1 −W2A∥2
F
E = 1 Ny"
NX,0.8232258064516129,"D
∥B1∥2
F
E
−1 Ny *"
NX,0.824516129032258,"
1 −
λ
(1 + λ)2"
NX,0.8258064516129032,"
B1 −
1
1 + λB2  2 F +"
NX,0.8270967741935484,"= 2γf(1 −γf)2 −γ4
f + 2γf

(1 −γf)2 −γ2
f

ρb.
(96)"
NX,0.8283870967741935,"If the first task is learned without weight regularization, the retention performance instead becomes:"
NX,0.8296774193548387,∆ϵRT = 1 Ny
NX,0.8309677419354838,"D
∥B1∥2
F
E
−1 Ny"
NX,0.832258064516129,"*
1
1 + λ(B1 −B2) 2 F +"
NX,0.8335483870967741,"= 1 −2γ2
f(1 −ρb).
(97)"
NX,0.8348387096774194,"D
Properties of very tall zero-mean random matrices"
NX,0.8361290322580646,"Our simple analytical results rely on the assumption that matrix A ∈RNx×Ns is very tall (i.e.,
Nx ≫Ns), and each element of A is sampled independently from a normal distribution with mean
zero and a finite variance."
NX,0.8374193548387097,"Given Aij ∼N(0,
1
Ns ), we have
D
1
Nx AT A
E"
NX,0.8387096774193549,"A =
1
Ns Is. The element-wise deviation from the mean
is evaluated as
*h
1
Nx AT A −
1
Ns Is
i ij 2+"
NX,0.84,"A
= 1 + δij"
NX,0.8412903225806452,"NxN 2s
,
(98)"
NX,0.8425806451612903,"implying that
 1"
NX,0.8438709677419355,"Nx AT A −
1
Ns Is

2 F "
NX,0.8451612903225807,"A
=
1
Nx"
NX,0.8464516129032258,"
1 +
1
Ns"
NX,0.847741935483871,"
. Therefore, at Nx ≫Ns limit,
1
Nx AT A"
NX,0.8490322580645161,approximately follows
NX,0.8503225806451613,"1
Nx AT A ≈
1
Ns Is.
(99)"
NX,0.8516129032258064,"Even in the presence of random gating, if the effective matrix is still tall (i.e., αNx ≫Ns), sim-
ilar approximations hold. Given a diagonal matrix D whose diagonal components are sampled
independently from a Bernoulli distribution with rate α, at αNx ≫Ns limit, we have"
NX,0.8529032258064516,"1
αNx (DA)T DA ≈
1
Ns Is.
(100)"
NX,0.8541935483870968,"This result implies that,"
NX,0.8554838709677419,"(DA)+ ≈
Ns
αNx (DA)T .
(101)"
NX,0.8567741935483871,"Moreover, denoting the SVD of DA by DA = UΣV T , at αNx ≫Ns limit, we have"
NX,0.8580645161290322,"UU T ≈
Ns
αNx DAAT D.
(102)"
NX,0.8593548387096774,"This is because, at αNx ≫Ns limit, from Eq. 100, the eigenspectrum of matrix
1
αNx DA(DA)T is
concentrated at λ =
1
Ns , and thus,"
NX,0.8606451612903225,"Ns
αNx DAAT D =
Ns
αNx UΣ2U T ≈UU T .
(103)"
NX,0.8619354838709677,"Figure 10: Permuted MNIST with a latent variable. (A) Learning curves under (ρa, ρb) = (0.8, 0.2).
(B) Knowledge transfer under various task similarities. (C) Comparison of the retention performance
measured after 10 epochs (black) and 100 epochs (gray) of training. (D) Retention performance
after 100 epochs of training with task 2. After 100 epochs of training, the system still exhibits strong
asymmetric dependence on the feature and readout similarities, but the non-monotonic dependence
on the feature similarity under ρb ≈1 region disappears. (E,F) Retention performance under the
weight regularization in the Fisher information metric (FIM) with a layer-wise approximation (panel
E) and a diagonal approximation (panel F), under (ρa, ρb) = (0.5, 0.5). Unlike Figs. 7F-H in the
main figure, we adjusted the regularizer amplitude of two layers independently. As in Fig. 7, error
bars in panel A-C represent standard errors over 10 random seeds."
NX,0.863225806451613,"E
Numerical methods"
NX,0.864516129032258,"Numerical experiments were conducted in standard laboratory GPUs and CPUs. Source codes for all
numerical results are made publicly available at https://github.com/nhiratani/transfer_
retention_model."
NX,0.8658064516129033,"E.1
Implementation of linear teacher-student models"
NX,0.8670967741935484,"Numerical results in Figs. 2-6 and 8-9 were implemented as below. We set the latent variable
dimensionality Ns = 30, the input width Nx = 3000, and the output width Ny = 10. The student
weight W was initialized as the zero matrix, and updated with the full gradient descent with learning
rate η = 0.001. We trained the network with the first task for Nepoch epochs, and then retrained it
with the second task for another Nepoch epochs. We used Nepoch = 100 for the vanilla model and
the activity gating models, but we used Nepoch = 500 for other models. Unless stated otherwise,
error bars represent standard deviation over 10 random seeds. Average performances in Figs. 3D,
4B-D, 5D, and 6CD were estimated by taking average over 100 pairs of (ρa, ρb), sampled uniformly
from ρa, ρb ∈[0, 1]."
NX,0.8683870967741936,"In the input soft-thresholding model, we estimated the gradient in a sample-based manner because
the exact gradient is not tractable due to nonlinearity. We estimated the gradient from 10000 random
samples at each iterations, then updated the model for 5000 iterations with learning η = 0.01."
NX,0.8696774193548387,"E.2
Implementation of permuted MNIST with latent"
NX,0.8709677419354839,"Data generation
We used permuted MNIST dataset [34, 22], a common benchmark for continual
learning, but with addition of the latent space. We constructed a four dimensional latent space s using
the binary representation of digits, s0 = [0, 0, 0, 0]T , s1 = [0, 0, 0, 1]T , and so on. The target output
was generated by a projection of the latent variable s to a ten-dimensional space, y∗= B(s −1"
NX,0.8722580645161291,"21),
where B is a 10 × 4 matrix generated randomly. The elements of matrix B1 for the first task were
sampled independently from a Gaussian distribution with mean zero and variance one. For the second
task, we introduced readout similarity by keeping some of the elements while resampling other
elements. We defined the readout similarity ρb by the fraction of the elements kept the same between
two tasks."
NX,0.8735483870967742,"Figure 11: Effect of layer-wise weight regularization in deep feedforward networks solving permuted
MNIST. (A) The transfer performance under three forms of weight regularization as a function of
the regularization amplitude. (B-D) The same as A, but the retention performance were plotted.
The panels are parallel to Fig. 7E-H, but here, we used a network with 3 hidden layers instead of
a one-hidden layer network used for Fig. 7. When the regularizer amplitude is larger than 102, we
observed instability in learning dynamics."
NX,0.8748387096774194,"Feature similarity was introduced by permuting the input pixels as done previously [22]. We used the
vanilla MNIST images for the first task and permuted some of the pixels in the second task depending
on the feature similarity ρa."
NX,0.8761290322580645,"Model implementation
We used one hidden layer neural network with ReLU non-linearity in the
hidden layer: y = b2 + W2ReLU [b1 + W1x]. We set the hidden layer width to Nh = 1500. The
input and output widths were set to Nx = 784 and Ny = 10. We initialized the first weight W1 and
the bias b1 by sampling each weight independently from a Gaussian distribution with mean zero
and variance
1
N 2
h . We initialized the second weight W2 and the bias b2 in the same manner, but with"
NX,0.8774193548387097,"variance
1
N2
y . Here, we set the amplitude of the initial weights small to operate the learning in the
rich regime [53]. We set the mini-batch size to 300 and the learning rate to η = 0.01, and trained
the network for 100 epochs per task. The retention performance was measured after 10 epochs of
training with task 2 except for Figs. 10 C, 10 D and 11 A-D where the retention was evaluated after
100 epochs. In the learning of task 2, the test error typically drops significantly in the first 10 epochs
and shifts to gradual improvement afterward (red line in Fig. 10A). Notably, the asymmetric task
similarity dependence of the retention performance was observed consistently both after a short and
long training on the second task (Figs. 7B and 10D)."
NX,0.8787096774193548,"Weight regularization in the Fisher information metric
From the same argument made in section
C.2, FIM of a noise-free model is approximated by the Hessian of the mean squared error. Let us
consider a vanilla feedforward neural network with depth K, where hidden layer activity h1, ..., hK−1
and the output y follow"
NX,0.88,"hk = ϕ (bk + Wkhk−1) , for k = 1, ..., K −1
(104)
y = bK + WKhk−1,
(105)"
NX,0.8812903225806452,"where ϕ is an element-wise rectified linear function. For brevity, we denote ∇ϕµ as a diagonal matrix
where the diagonal components represent element-wise derivative ϕ′(bk + Wkhk−1). Given loss
function ℓ= 1"
NX,0.8825806451612903,"2 ∥y −y∗∥2, the Hessian is estimated as ∂2ℓ"
NX,0.8838709677419355,"∂w(µ)
ij ∂w(µ)
kl
=
∂ℓ"
NX,0.8851612903225806,"∂w(µ)
ij"
NX,0.8864516129032258," 
∇ϕνW T
ν+1...∇ϕK−1W T
K(y −y∗)
"
NX,0.8877419354838709,"k hν−1,l
"
NX,0.8890322580645161,"≈

∇ϕνW T
ν+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµ
"
NX,0.8903225806451613,"ki hµ−1,jhν−1,l"
NX,0.8916129032258064,"+ [µ > ν]+

∇ϕνW T
ν+1...∇ϕµ−1
"
NX,0.8929032258064517,"kj

∇ϕµW T
µ+1...W T
K(y −y∗)
"
NX,0.8941935483870967,"i hν−1,l"
NX,0.895483870967742,"+ [µ < ν]+

∇ϕµW T
µ+1...∇ϕν−1
"
NX,0.896774193548387,"il

∇ϕνW T
ν+1...W T
K(y −y∗)
"
NX,0.8980645161290323,"k hµ−1,j"
NX,0.8993548387096775,"≡eH(µ,ν)
ijkl .
(106)"
NX,0.9006451612903226,"In the second line, we omitted the second-order derivative ϕ′′ as this term is effectively negligible
under ReLU activation function."
NX,0.9019354838709678,"Thus, distance between two weights δW = W −Wo in the metric defined by this approximated
Hessian is written as
X µ,ν X"
NX,0.9032258064516129,"ijkl
δw(µ)
ij δw(ν)
kl eH(µ,ν)
ijkl =
X"
NX,0.9045161290322581,"µ,ν
hT
µ−1δW T
µ ∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wν+1∇ϕνδWνhν−1 + 2
X"
NX,0.9058064516129032,"µ>ν
hT
ν−1δW T
ν ∇ϕνW T
ν+1...∇ϕµ−1δW T
µ ∇ϕµ+1W T
µ+1...W T
L (y −y∗).
(107)"
NX,0.9070967741935484,"We define a layer-wise approximation of the weight regularization in the Fisher information metric
by taking µ = ν components of the distance defined above:"
NX,0.9083870967741936,"Rlw[W] = 1 2 K
X µ=1"
NX,0.9096774193548387,"D
∥WK∇ϕK−1...Wµ+1∇ϕµδWµϕµ−1∥2E
.
(108)"
NX,0.9109677419354839,"Here, the expectation is taken over training data (x, y∗) of the previous task (i.e., the task the network
shouldn’t forget). Taking the derivative with respect to δWk for k = 1, .., K, we have ∂Rlw"
NX,0.912258064516129,"∂δWµ
=

∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµδWµhµ−1hT
µ−1"
NX,0.9135483870967742,"≈

∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµ

δWµ

hµ−1hT
µ−1

.
(109)"
NX,0.9148387096774193,"This layer-wise approximation captures the true Fisher information metric more accurately than
element-wise approximation implemented previously, while computationally less heavy than the
estimation of the true metric. In the numerical estimations, we scaled the regularization terms at the
k-the layer by"
NX,0.9161290322580645,"Zk =

∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµ

2

hµ−1hT
µ−1

2 ,
(110)"
NX,0.9174193548387096,where ∥·∥2 is the spectral norm. We did not impose weight regularization on the bias parameters bk.
NX,0.9187096774193548,"In the case of synapse-wise approximation (i.e., elastic regularization), the regularizer is instead
written as"
NX,0.92,Rsw[W] = 1 2 X µ X ij ∂2ℓ
NX,0.9212903225806451,"∂(w(µ)
ij )2
 
δwµ
ij
2 = 1 2"
NX,0.9225806451612903,"D
∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµ
"
NX,0.9238709677419354,"ii h2
µ−1,j
E 
δw(µ)
ij
2
. (111)"
NX,0.9251612903225807,"Thus, the gradient of Rsw with respect to δWµ follows ∂Rsw"
NX,0.9264516129032258,"∂(δWµ) =
D
diag

∇ϕµW T
µ+1...∇ϕK−1W T
KWK∇ϕK−1...Wµ+1∇ϕµ

(hµ−1 ⊙hµ−1)T E
⊙δWµ. (112)"
NX,0.927741935483871,We scaled the regularization term by the largest coefficient in a layer-wise manner.
NX,0.9290322580645162,"In the case of one-hidden layered networks, the layer-wise regularizer is written as"
NX,0.9303225806451613,"Rlw[W1, W2] =
1
2Z1"
NX,0.9316129032258065,"D
∥W2∇ϕδW1x∥2E
+
1
2Z2"
NX,0.9329032258064516,"D
∥δW2ϕ∥2E
,
(113)"
NX,0.9341935483870968,"where the normalization constants follow Z1 =
W2W T
2

2

xxT 
2, Z2 =

ϕϕT 
2. By
contrast, the synapse-wise regularizer is given as"
NX,0.9354838709677419,"Rsw[W1, W2] =
1
2Z1 X ij X k"
NX,0.9367741935483871,"
w(2)
ki ϕ′
ixj
2 
δw(1)
ij
2
+
1
2Z2 X"
NX,0.9380645161290323,"ij
ϕ2
j

δw(2)
ij
2
,
(114)"
NX,0.9393548387096774,"where Z1 = maxij
DP"
NX,0.9406451612903226,"k(w(2)
ki ϕ′
ixj)2E
and Z2 = maxj

ϕ2
j

. To see the potential effect of different
normalization methods used for the layer-wise and diagonal approximations, in Figs. 10 E and F, we"
NX,0.9419354838709677,"Figure 12: Permuted MNIST with pixel and label permutations. (A,B) Transfer and classification
performance measured by the classification accuracy (A) and the cross-entropy loss (B). (C) Transfer
performance of random (dashed lines) and adaptive (solid lines) activity gating models. (D-F) Perfor-
mance of the weight regularization in the Euclidean metric, and approximated Fisher information
metrics."
NX,0.9432258064516129,"independently changed the regularizer amplitude for W1 and W2 and measured the retention perfor-
mance under the two approximation methods. Even in this setting, the layer-wise approximation of
the Fisher-information metric robustly outperformed the diagonal approximation, when the regulazier
amplitude for W2 was sufficiently large."
NX,0.944516129032258,"In Fig. 11, we implemented this method to a feedforward network with three all-to-all hidden layers,
where hidden layer widths are set to be 784-1000-300-100-10. We used the learning rate η = 0.01,
mini-batch size 300, and trained the network for 100 epochs per task. The weight regularization
based on a layer-wise approximation of the Fisher-information metric, robustly outperformed both
the diagonal approximation of the metric and weight regularization in the Euclidean space even in
this deep network (Fig. 11B-D)."
NX,0.9458064516129032,"E.3
Implementation of permuted MNIST"
NX,0.9470967741935484,"In Fig. 12, we implemented permuted MNIST without any explicit latent structure. We modulated
the input feature similarity by permuting a subset of input pixels between the first and the second
tasks, and modulated the readout similarity by permuting a subset of the output labels as shown in
Figs. 1C and 1D. We used the same one-hidden layer network with the model above, but introduced
the softmax at the output units and estimated the loss using cross-entropy loss. We set the mini-batch
size to be 300, and the epoch per task as 100. The retention performance was evaluated at the end of
the training of the second task using Eq. 2."
NX,0.9483870967741935,"When transfer and retention performance were assessed based on differences in classification accuracy,
both measures exhibited a positive monotonic relationship with feature and readout similarity (Fig.
12A). However, when evaluated using cross-entropy loss, the training loss function, transfer and
retention performance showed a non-monotonic dependence on feature and readout similarities (Fig.
12B). Specifically, under low readout similarity, increasing feature similarity negatively impacted
transfer performance."
NX,0.9496774193548387,"Adaptive gating with a probe trial improved transfer performance (Fig. 12C vs. Figs. 7C and
4A). Additionally, weight regularization in the layer-wise approximation of the Fisher information
metric yielded better retention performance compared to its diagonal approximation and weight
regularization in Euclidean space. (Figs. 12D-F)."
NX,0.9509677419354838,NeurIPS Paper Checklist
CLAIMS,0.952258064516129,1. Claims
CLAIMS,0.9535483870967741,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9548387096774194,Answer: [Yes]
CLAIMS,0.9561290322580646,"Justification: The claims made in the fourth, fifth, and sixth paragraphs of Introduction were
supported by sections 4, 5 and 6, and 7, respectively."
LIMITATIONS,0.9574193548387097,2. Limitations
LIMITATIONS,0.9587096774193549,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.96,Answer: [Yes]
LIMITATIONS,0.9612903225806452,"Justification: Please see the paragraph titled ""Limitations"" in section 8."
THEORY ASSUMPTIONS AND PROOFS,0.9625806451612903,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9638709677419355,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9651612903225807,"Answer: [Yes]
Justification: All assumptions and derivations of the theoretical results are provided in
Appendix A-D."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9664516129032258,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.967741935483871,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9690322580645161,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9703225806451613,Justification: Please see Appendix E.
OPEN ACCESS TO DATA AND CODE,0.9716129032258064,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9729032258064516,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9741935483870968,"Answer: [Yes]
Justification:
Source codes are made publicly available at https://github.com/
nhiratani/transfer_retention_model."
OPEN ACCESS TO DATA AND CODE,0.9754838709677419,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9767741935483871,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9780645161290322,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9793548387096774,Justification: Please see Appendix E.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9806451612903225,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9819354838709677,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.983225806451613,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.984516129032258,Justification: Please see the figures and the legends of Figs. 1 and 6.
EXPERIMENTS COMPUTE RESOURCES,0.9858064516129033,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9870967741935484,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9883870967741936,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9896774193548387,Justification: Please see Appendix E
CODE OF ETHICS,0.9909677419354839,9. Code Of Ethics
CODE OF ETHICS,0.9922580645161291,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research does not contain any element that requires ethical concern.
10. Broader Impacts"
CODE OF ETHICS,0.9935483870967742,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The work does not have any direct societal impact.
11. Safeguards"
CODE OF ETHICS,0.9948387096774194,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The paper does not contain any data or models that have a risk for misuse.
12. Licenses for existing assets"
CODE OF ETHICS,0.9961290322580645,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing licensed assets
13. New Assets"
CODE OF ETHICS,0.9974193548387097,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14. Crowdsourcing and Research with Human Subjects"
CODE OF ETHICS,0.9987096774193548,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects."
