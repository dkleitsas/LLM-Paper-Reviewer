Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0037313432835820895,"Unsupervised learning of latent variable models (LVMs) is widely used to represent
data in machine learning. When such models reﬂect the ground truth factors and
the mechanisms mapping them to observations, there is reason to expect that they
allow generalization in downstream tasks. It is however well known that such
identiﬁability guaranties are typically not achievable without putting constraints on
the model class. This is notably the case for nonlinear Independent Component
Analysis, in which the LVM maps statistically independent variables to observations
via a deterministic nonlinear function. Several families of spurious solutions ﬁtting
perfectly the data, but that do not correspond to the ground truth factors can be
constructed in generic settings. However, recent work suggests that constraining
the function class of such models may promote identiﬁability. Speciﬁcally, function
classes with constraints on their partial derivatives, gathered in the Jacobian matrix,
have been proposed, such as orthogonal coordinate transformations (OCT), which
impose orthogonality of the Jacobian columns. In the present work, we prove that
a subclass of these transformations, conformal maps, is identiﬁable and provide
novel theoretical results suggesting that OCTs have properties that prevent families
of spurious solutions to spoil identiﬁability in a generic setting."
INTRODUCTION,0.007462686567164179,"1
Introduction"
INTRODUCTION,0.011194029850746268,"Unsupervised representation learning methods can ﬁt Latent Variables Models (LVM) to complex
real world data. While those latent representations allow to create realistic novel samples or represent
the data in a compact way [32, 16], they are a priori not related to the ground truth generative factors
of the data. It is highly desirable to recover the true underlying source distribution because those are
expected to help with various downstream tasks, e.g., out of distribution generalization [45, 2]."
INTRODUCTION,0.014925373134328358,"One principled framework for representation learning is Independent Component Analysis (ICA)
where one tries to recover unobserved sources s 2 Rd from observations x = f(s) and one assumes
that the components si are independent. An important result is that for linear functions f it is
possible to recover s from observations x up to certain symmetries, i.e., the model is identiﬁable
[8]. In contrast, for general non-linear models f is highly non-identiﬁable [25]. This has important
consequences for representation learning, in particular the learning of disentangled representations is
also unidentiﬁable without some access to the underlying sources [37]. Notably, this makes theoretical
analysis of a large body of methods (see, e.g., [20, 30, 42]) that enforces disentanglement difﬁcult."
INTRODUCTION,0.018656716417910446,"Several additional assumptions were suggested to make the ICA problem identiﬁable. Broadly, there
are two directions. First, some works imposed additional or different restrictions on the distribution of
the sources. One line of research adds temporal structure by considering time series data [19, 23, 24].
More recently, Hyvärinen et al. [26] proposed to introduce an observed auxiliary variable u, e.g., a
class label, such that the source distribution has independent components conditional on the auxiliary"
INTRODUCTION,0.022388059701492536,"variable. They show that under suitable assumptions on the distribution of u and s arbitrary nonlinear
mixing functions can be identiﬁed. Several recent works extended this approach [29, 48, 53]."
INTRODUCTION,0.026119402985074626,"Another possibility is to restrict the class of admissible functions by considering more ﬂexible classes
than just linear functions, but not allowing arbitrary non-linear functions. The general aim of this
approach is to ﬁnd sufﬁciently “small” function classes such that ICA is identiﬁable within this class
while making them as large as possible to allow ﬂexible representation of complex data and being
applicable to real world problems. So far results in this direction are rather limited. It was shown that
the post-nonlinear model is identiﬁable [49]. Moreover, it has been shown that ICA with conformal
maps in dimension 2 is almost identiﬁable [25]. More recently, identiﬁability of volume preserving
transformations was investigated in the auxiliary variable case in [53] (combining the two possible
restrictions) and identiﬁability based on sparsity of the mixing function was studied in [54]."
INTRODUCTION,0.029850746268656716,"In this work, we extend the previous works by proving new identiﬁability results for unconditional
ICA. We consider conformal maps (i.e., maps that locally preserve angles) and Orthogonal Coordinate
Transforms (OCT) (i.e., maps where Df >Df is a diagonal matrix where Df denotes the derivative
of f). OCTs, that we will also call orthogonal maps for simplicity, were recently introduced in the
context of representation learning in [17], motivated by the independence of mechanisms assumption
from the causality literature. The main focus of this work is to prove new identiﬁability and partial
identiﬁability results for this class of functions. An overview of our results can be found in Table 1.
We summarize the main contributions and the structure of this paper as follows."
INTRODUCTION,0.033582089552238806,• We prove that ICA with conformal maps is identiﬁable in d ≥3 and extend the earlier
INTRODUCTION,0.03731343283582089,"results in dimension 2 (Theorems 2-3 in Section 3).
• We deﬁne a new notion of local identiﬁability (Deﬁnition 5) and prove that ICA with"
INTRODUCTION,0.041044776119402986,"orthogonal maps is locally identiﬁable (Theorem 4 in Section 4).
• On the contrary we show that ICA with volume preserving maps is not identiﬁable not even"
INTRODUCTION,0.04477611940298507,"in the local sense (Theorem 6 in Section 5).
• We introduce new tools to the ICA ﬁeld: our results are based on connections to rigidity"
INTRODUCTION,0.048507462686567165,"theory (see Section 6), restricting the global structure of functions based on local constraints.
Moreover, we exploit the global structure of partial differential equations related to the
identiﬁability problem while most earlier results argue locally using linear algebraic tools."
SETTING,0.05223880597014925,"2
Setting"
SETTING,0.055970149253731345,"Independent component analysis investigates the problem of identifying underlying sources when
observing a mixture of them. We will consider the following general setting: there exists some
random hidden vector of sources s 2 Rd and the observed data is generated by"
SETTING,0.05970149253731343,"x = f(s),
ps(s) = Yd"
SETTING,0.06343283582089553,"i=1 pi(si) = P
(1)"
SETTING,0.06716417910447761,"where f : Rd ! Rd is smooth and invertible. The condition on s means that its coordinates (often
referred to as factors of variation) are independent. Formally this means that the distribution P of s
satisﬁes P 2 M1(R)⌦n where M1(R) denotes the probability measures on R. The goal of ICA is
to ﬁnd an unmixing g : Rd ! Rd such that g(x) has independent components. Ideally, this should
recover the true underlying factors of variation and achieve Blind Source Separation (BSS), i.e.,
g = f −1 up to certain symmetries. Identiﬁcation of the true generative factors of variations of the
observations is of interest since these provide a causal and interventional understanding of the data."
SETTING,0.0708955223880597,"An important observation was that, in the generality stated above, identiﬁcation of s is not possible. In
[25] two general constructions of spurious solutions were given: the well known Darmois construction
and a construction based on measure preserving transformations. The latter one is closer to our
work here and we will discuss those in more detail in Section 4 and Appendix C. In a nutshell it is
based on the observation that for measures P with smooth density one can construct smooth Measure
Preserving Transformations (MPT), m : Rd ! Rd (that mix the different coordinates), i.e., maps that
leave P invariant, such that m(s)"
SETTING,0.07462686567164178,"D= s if s ⇠P.1 This implies that all functions (f ◦m)−1 recover
independent sources since (f ◦m)−1(x)"
SETTING,0.07835820895522388,D= s making BSS impossible.
WE USE THE NOTATION X,0.08208955223880597,1We use the notation X
WE USE THE NOTATION X,0.08582089552238806,D= Y to indicate that the two random variables X and Y follow the same distribution
WE USE THE NOTATION X,0.08955223880597014,"Figure 1: Illustration of the considered function classes. (a) shows a standard coordinate frame,
(b) a conformal map applied to this frame which preserves angles, (c) an orthogonal map (polar
coordinates) that preserve the orthogonality of lines parallel to the coordinate axes but not all angles
(see red line), (d) a volume preserving map."
WE USE THE NOTATION X,0.09328358208955224,"Thus it is a natural question whether additional assumptions on the mixing function f or distribution
of s allow us to identify f. Let us deﬁne a framework for identiﬁability. We assume data is generated
according to (1) where f belongs to some function class F(A, B) of invertible functions A ! B,
which we will always assume to be diffeomorphisms.2 and P satisﬁes P 2 P for some set of
probability distributions P ⇢M1(R)⌦d. Finally, let S be a group of transformations g : Rd ! Rd
that encodes the allowed symmetries up to which the sources can be identiﬁed as follows. The
function class will be simply denoted F when domain/codomain information is irrelevant."
WE USE THE NOTATION X,0.09701492537313433,"Deﬁnition 1. (Identiﬁability) We say that independent component analysis in (F, P) is identiﬁable
up to S if for functions f, f 0 2 F and distributions P, P0 2 P the relation f(s)"
WE USE THE NOTATION X,0.10074626865671642,"D= f 0(s0)
where s ⇠P and s0 ⇠P0
(2)"
WE USE THE NOTATION X,0.1044776119402985,implies that there is h 2 S such that h = f 0−1 ◦f on the support of P.
WE USE THE NOTATION X,0.10820895522388059,"Note that we require the identity h = f 0−1 ◦f only to hold on the support of P because, for complex
classes F, there is in general no unique extension of f beyond the support of P and without data the
extension cannot be identiﬁed. We do not always make this explicit in the following. Put differently,
identiﬁability means that given observations of x = f(s) and knowledge of (F, P), we can ﬁnd g
such that g ◦f 2 S, in particular the reconstructed sources s0 = g(x) and the true sources s are
related by a symmetry transformation in S. We discuss in Appendix C how to identify the set S and
how spurious solutions to the identiﬁcation problem can be constructed. In the following, it will
be convenient to use the notation f⇤P which denotes the push-forward of the measure P along the
function f. We refer to Appendix A for a formal deﬁnition, but we note here that the distribution of
f(s) equals f⇤P whenever s ⇠P. Therefore, (2) can be equivalently written as f⇤P = f 0 ⇤P0."
WE USE THE NOTATION X,0.11194029850746269,We illustrate Deﬁnition 1 through the well known example of linear maps
WE USE THE NOTATION X,0.11567164179104478,"Flin = {f : Rd ! Rd : f is linear and invertible},
(3)"
WE USE THE NOTATION X,0.11940298507462686,"i.e., x = As for some invertible matrix A 2 Rn⇥n. We further deﬁne"
WE USE THE NOTATION X,0.12313432835820895,"Plin = {P 2 M1(R)⌦d : at most one component of P is Gaussian},
(4)
Slin = {P⇤: P is a permutation matrix and ⇤is a diagonal matrix}.
(5)"
WE USE THE NOTATION X,0.12686567164179105,It is easy to check that S is a group. Then the following identiﬁability result for Flin is well known.
WE USE THE NOTATION X,0.13059701492537312,"Theorem 1. (Theorem 11 in [8]) The pair (Flin, Plin) is identiﬁable up to Slin."
WE USE THE NOTATION X,0.13432835820895522,"This result is optimal as the ordering and scale of the si is unidentiﬁable and the restriction to at most
one Gaussian component is required to avoid linear MPTs of multivariate Gaussians. We provide a
proof of this result in Appendix D as this serves as a preparation for the more involved Theorem 2
below."
WE USE THE NOTATION X,0.13805970149253732,"An important observation which was also made in [22] is that with minor differences one can
also consider the case where f : Rd ! M maps to a d-dimensional Riemannian manifold M. An"
WE USE THE NOTATION X,0.1417910447761194,2A diffeomorphism is a differentiable bijective map with differentiable inverse.
WE USE THE NOTATION X,0.1455223880597015,"Table 1: Overview of new identiﬁability results. Note that Identiﬁable implies Locally identiﬁable
and if Locally identiﬁable does not hold neither of the other two properties can hold."
WE USE THE NOTATION X,0.14925373134328357,"Function class
Identiﬁable"
WE USE THE NOTATION X,0.15298507462686567,(Def. 1)
WE USE THE NOTATION X,0.15671641791044777,Locally identiﬁable
WE USE THE NOTATION X,0.16044776119402984,(Def. 5)
WE USE THE NOTATION X,0.16417910447761194,"Gaussian only
spurious solution
Linear
3
3
3
Conformal
3
(Thm. 2 & 3)
3
3
Orthogonal
?
3
(Thm. 4)
7
(Prop. 1)
Volume preserving
7
7
(Thm. 6)
7
General nonlinear
7
7
(Lemma 1)
7"
WE USE THE NOTATION X,0.16791044776119404,"important example for this setting is the case where M ⇢Rn is a submanifold of a higher dimensional
Euclidean space. This covers the standard setting of unsupervised representation learning where
high dimensional observations (often images) are created from low dimensional factors of variation
mirroring the well known manifold hypothesis [50]. Note that this setting essentially covers the case
of undercomplete ICA, where we consider f : Rd ! Rn with n > d. The only difference is that we
assume that we already know the submanifold M that f maps to. This manifold can, however, be
identiﬁed from the observations x = f(s) under minor regularity assumption on f and the support of
the data distribution. To avoid technical difﬁculties we assume that the manifold is already known.
Note that we restrict our attention to the case where the factors of variations are parametrized by a
Euclidean space. An extension to product manifolds and a combination with the approach in [21] is
an interesting question left to future work."
WE USE THE NOTATION X,0.17164179104477612,"In the next sections we discuss our results on identiﬁability of ICA for different function classes.
An illustration of the considered classes can be found in Figure 1. They are all characterized by a
local condition on their gradient. Previously, in [22] it was shown that the function class of local
isometries is identiﬁable. Local isometries have been used frequently in machine learning, and more
speciﬁcally in representation learning [44, 50, 11]. Our main results consider two generalizations of
these function classes, conformal maps and orthogonal coordinate transformations. Conformal maps
preserve angles locally and have been used in computer vision [46, 51, 18]. For d = 2, conformal
maps essentially consist of all biholomorphic mappings of simply connected open domains of the
complex plane, and thus constitute a “large”, non-parametric family, as a consequence of the Riemann
Mapping Theorem [38]. For d ≥3, Liouville’s Theorem implies that this class contains relatively
“few” functions in ﬁxed dimension (i.e., mapping Rd ! Rd), in the sense that it is a parametric family,"
WE USE THE NOTATION X,0.17537313432835822,"with parameter dimension quadratic in d (see Theorem 7). However, this is a rich class when the
target space is higher dimensional than the domain (i.e., Rd ! Rn, d < n). OCT are an even more
general class that were motivated based on the principle of independent causal mechanisms in [17].
Notably, it contains all conformal maps precomposed with nonlinear entrywise reparametrizations
of the source components (see Corollary 1). It is however much larger, as one can for example
concatenate arbitrary functions from the large family of 2d conformal mappings to obtain higher
dimensional OCTs. Moreover, many works showed that training VAEs promotes orthogonality of
the columns of the input Jacobian [43, 55, 33, 40, 47] and this has been empirically shown to be a
good inductive bias for disentanglement. Indeed, these algorithms are widely used in representation
learning and often recover semantically meaningful representations [34, 5, 30, 20]."
RESULTS FOR CONFORMAL MAPS,0.1791044776119403,"3
Results for conformal maps"
RESULTS FOR CONFORMAL MAPS,0.1828358208955224,"Our ﬁrst main result is an extension of Theorem 1 to conformal maps. A conformal map is a map
that locally preserves angles, i.e. locally it looks like a scaled rotation. It can be shown that this is
equivalent to the following deﬁnition.
Deﬁnition 2. (Conformal map) We deﬁne for domains ⌦⇢Rd the set of conformal maps by
Fconf = {f 2 C1(⌦, Rd) : Df(x) = λ(x)O(x)} where λ : ⌦! R \ {0} is a scalar function and
O : ⌦! O(d) is a map to orthogonal matrices (i.e., O(x)−1 = O(x)>)."
RESULTS FOR CONFORMAL MAPS,0.1865671641791045,"All our results also hold for the more general class of conformal maps f : ⌦! M where M is a
Riemannian manifold. The complete deﬁnition can be found in Appendix B. For convenience we
deﬁne signed permutation matrices by"
RESULTS FOR CONFORMAL MAPS,0.19029850746268656,"Perm±(d) = {P 2 Rd⇥d : Q 2 Rd⇥d, such that Qij = |Pij|, is a permutation matrix},
(6)"
RESULTS FOR CONFORMAL MAPS,0.19402985074626866,"i.e. the set of matrices whose entry-wise absolute value is a permutation. Later we will also use the
notation Diag(d) and Perm(d) for d ⇥d diagonal and permutation matrices, respectively. We deﬁne"
RESULTS FOR CONFORMAL MAPS,0.19776119402985073,"Sconf = {x ! Px + a where P 2 Perm±(d), a 2 Rd, 2 R \ {0}}
(7)
and"
RESULTS FOR CONFORMAL MAPS,0.20149253731343283,Pconf = P⌦n
RESULTS FOR CONFORMAL MAPS,0.20522388059701493,"1
\ Plin,
where"
RESULTS FOR CONFORMAL MAPS,0.208955223880597,"P1 = {µ 2 M1(R), there is ; 6= O ⇢R open, s.t. µ has positive C2 density on O}. (8)"
RESULTS FOR CONFORMAL MAPS,0.2126865671641791,"While this condition might appear a bit technical it actually only rules out pathological cases like
the cantor measure or densities which are nowhere differentiable and probably it could be relaxed
further. In particular P1 contains all probability measures with piecewise smooth densities. Then the
following identiﬁability for conformal maps in dimension d > 2 holds.
Theorem 2. For d > 2, ICA with respect to the pair (Fconf, Pconf) is identiﬁable up to Sconf."
RESULTS FOR CONFORMAL MAPS,0.21641791044776118,"This means that we can identify conformal maps up to three symmetries, namely constant shifts of
the distributions, rescaling of all coordinates by the same constant factor, and permutations of the
coordinates. The proof is in Appendix E. The main ingredient in the proof is that conformal maps in
dimension d > 2 are very rigid and can be characterized explicitly, as we will discuss in Section 6."
RESULTS FOR CONFORMAL MAPS,0.22014925373134328,"We remark that it might be more natural to not ﬁx the scale of the sources and allow arbitrary
coordinate-wise rescaling. The result can be easily extended to accommodate this. We deﬁne"
RESULTS FOR CONFORMAL MAPS,0.22388059701492538,Sreparam = {g : Rd ! Rd| g = P ◦h where P 2 Perm±(d) and h : Rd ! Rd with
RESULTS FOR CONFORMAL MAPS,0.22761194029850745,"h(x) = (h1(x1), . . . , hd(xd))> for some hi 2 C1(R, R) with h0"
RESULTS FOR CONFORMAL MAPS,0.23134328358208955,i > 0}. (9)
RESULTS FOR CONFORMAL MAPS,0.23507462686567165,"It is easy to see that Sreparam is a group. We deﬁne the class of parameterized conformal maps by
Fr−conf = {f ◦h |f 2 Fconf, h 2 Sreparam} \ C3(⌦, M) and then get the following Corollary.
Corollary 1. For d > 2, ICA with respect to the pair (Fr−conf, Pconf) is identiﬁable up to Sreparam
if we assume in addition that the observational distribution cannot be expressed as f⇤P for some
f 2 Fconf and P 2 M1(R)⌦d which has at least two Gaussian components."
RESULTS FOR CONFORMAL MAPS,0.23880597014925373,"The additional restriction on the observational distribution is clearly necessary to exclude the non-
identiﬁability of Gaussian distributions."
RESULTS FOR CONFORMAL MAPS,0.24253731343283583,"For dimension d = 2 it was shown [25] that conformal maps can be identiﬁed up to a rotation when
ﬁxing one point of the conformal map (setting f(0) = 0). The authors also claim, without proof,
that the remaining ambiguity can be removed for typical probability distributions. We extend their
result by removing the condition that one point is ﬁxed and prove full identiﬁability with a minor
assumption on the involved densities. We deﬁne the following set of probability measures on R2"
RESULTS FOR CONFORMAL MAPS,0.2462686567164179,Pconf2 = {P = P1 ⌦P2 2 M(R)2| s.t. supp(Pi) is a bounded interval Ii and
RESULTS FOR CONFORMAL MAPS,0.25,"Pi has density bounded above and below on Ii}
(10)"
RESULTS FOR CONFORMAL MAPS,0.2537313432835821,"Then we get the following result.
Theorem 3. For d = 2, ICA with respect to the pair (Fconf, Pconf2) is identiﬁable up to Sconf."
RESULTS FOR CONFORMAL MAPS,0.2574626865671642,"This means that we can identify conformal maps on compact domains in dimension 2 up to shifts,
permutations of coordinates, and scale. Note that we can also identify conformal maps if P has full
support R2 using the same proof as for d > 2 (see Lemma 2 in the supplement) and an extension as
in Corollary 1 is possible. The proof of this result is in Appendix E."
RESULTS FOR ORTHOGONAL MAPS,0.26119402985074625,"4
Results for orthogonal maps"
RESULTS FOR ORTHOGONAL MAPS,0.26492537313432835,"Recently, in [17], the more general class of OCTs was considered in the context of ICA. They referred
to orthogonal coordinates as IMA maps, referencing to independent mechanisms. This nomenclature
was motivated by the causality literature and we refer to their paper for an extensive motivation and
further results. As we focus on theoretical results for this function class we stick to the more common
term of OCTs. Orthogonal coordinate transformations are deﬁned as the set of functions whose
derivative have orthogonal columns, i.e., the vectors @if and @jf are orthogonal for i 6= j."
RESULTS FOR ORTHOGONAL MAPS,0.26865671641791045,"Deﬁnition 3. (OCT maps) We deﬁne for domains ⌦⇢Rd the set of OCT maps (orthogonal
coordinates) by FOCT = {f 2 C1(⌦, Rd) : Df(x)>Df(x) 2 Diag(d)}."
RESULTS FOR ORTHOGONAL MAPS,0.27238805970149255,"OCTs constitute a rich class of functions.
The study of OCTs has a long history and already in
the 19th century the structure of all OCTs deﬁned in a neighbourhood of a point were characterized
[10, 4]. Later, also the set of global orthogonal coordinate systems on Rd was characterized [28]. As
those results are not easily accessible we will provide here a simple argument showing that OCTs
constitute a rich class of functions. We ﬁrst note that FOCT contains the above Fr−conf, as functions
in the later class have a Df(x) that takes the form of a Jacobian of a conformal map whose columns
are rescaled by derivatives of the entry-wise reparametrizations, such that they remain orthogonal.
However, FOCT is much bigger than Fr−conf. For example, take a n-tuple (f 1, ..., f n) of arbitrary
injective 2D conformal maps f k : ⌦k ! R2 2 Fconf where ⌦k ⇢R2 and build the “concatenated”
map fconc : ⌦1 ⇥· · · ⇥⌦n ! R2n given by"
RESULTS FOR ORTHOGONAL MAPS,0.27611940298507465,fconc(s) = (f 1
RESULTS FOR ORTHOGONAL MAPS,0.2798507462686567,"1 (s1, s2), f 1"
RESULTS FOR ORTHOGONAL MAPS,0.2835820895522388,"2 (s1, s2), . . . , f n"
RESULTS FOR ORTHOGONAL MAPS,0.2873134328358209,"1 (s2n−1, s2n), f n"
RESULTS FOR ORTHOGONAL MAPS,0.291044776119403,"2 (s2n−1, s2n))> .
(11)"
RESULTS FOR ORTHOGONAL MAPS,0.2947761194029851,"The Jacobian of fconc is block diagonal, such that columns associated to different diagonal blocks
are obviously orthogonal, and columns pertaining to the same k-th diagonal block are orthogonal
by conformality of fk. With such a construction, that we can also further post-compose with
transformations in Fconf on R2n, we can thus build a large non-parametric subclass of FOCT on R2n.
This construction can be easily adapted to the case of odd dimensions."
RESULTS FOR ORTHOGONAL MAPS,0.29850746268656714,"Setting for identiﬁability with OCTs.
OCTs can also be generalized to maps whose target is a
d-dimensional manifold (see deﬁnition in Appendix B), and the following results will also apply
to such case. First, we note that we can only hope to identify a mechanism f 2 FOCT up to
coordinate-wise transformations and permutations, i.e., maps in Sreparam. Indeed, if f 2 FOCT and
g 2 Sreparam then f ◦g 2 FOCT. Thus, in particular f⇤P = (f ◦g)⇤(g−1)⇤P. This implies that
given observations from f⇤P we can identify f and P only up to g 2 Sreparam. More precisely, for
any (sufﬁciently smooth) P0 there is f 0 such that f⇤P = f 0"
RESULTS FOR ORTHOGONAL MAPS,0.30223880597014924,⇤P0 where we pick g such that P0 = g−1 ⇤P. 3
RESULTS FOR ORTHOGONAL MAPS,0.30597014925373134,"As the distribution of the si is not identiﬁable, we map it to a ﬁxed reference distribution that we
choose to be the uniform distribution on (0, 1)d. We introduce the shorthand Cd = (0, 1)d for the
standard open unit cube (exclusion of the boundary will be important for our result) and denote by ⌫
the uniform (Lebesgue) measure on Cd. For ﬁxed base measure ⌫the symmetry group is reduced to
permutations and reﬂections, i.e., maps in P 2 Perm±(d)."
RESULTS FOR ORTHOGONAL MAPS,0.30970149253731344,"We conjecture that for ’typical’ pairs (f, P) 2 FOCT ⇥POCT, ICA is identiﬁable with respect to
Sreparam (with a suitable deﬁnition of POCT, e.g., POCT = Pconf). However, we leave a precise
statement for future work. Below we will show a weaker notion of identiﬁability for OCTs, but,
before that, we ﬁrst exhibit exceptional classes of spurious solutions for ICA with OCTs."
RESULTS FOR ORTHOGONAL MAPS,0.31343283582089554,"Spurious solutions for ICA with OCTs.
We ﬁrst note that just as for linear maps and conformal
maps (see Thm. 1) Gaussian distributions can hamper identiﬁcation. This is because arbitrary
measures with a factorized density can be pushed forward into multivariate Gaussians using a suitable
coordinate-wise transformation.
Fact 1. Let P be a probability measure on R with bounded density p and cumulative distribution"
RESULTS FOR ORTHOGONAL MAPS,0.31716417910447764,"function FP. Denote the cumulative distribution function of the standard normal by FN . Let
hP = F −1"
RESULTS FOR ORTHOGONAL MAPS,0.3208955223880597,N ◦FP. Then (hP)⇤P has a standard normal distribution.
RESULTS FOR ORTHOGONAL MAPS,0.3246268656716418,"This implies that if P = P1 ⌦. . . ⌦Pd and x = f(s) = (hP1(s1), . . . , hPd(sd)) then f⇤P follows a
standard normal distribution. In particular, for every A 2 O(d) the distribution of Ax is standard
normal and its components are independent. Note that, in contrast to conformal and linear maps, it is
not sufﬁcient to exclude Gaussian source distributions: due to the ﬂexibility of the function class, we
have to exclude that the pair (f, P) has a Gaussian observational distribution f⇤P. Next we show that
a more general construction using OCTs is possible.
Proposition 1. Let P be a rotation invariant distribution on Rd with smooth density. Then there is a
smooth and invertible (on its image) function f : Cd ! Rd with f 2 FOCT such that f⇤⌫= P."
RESULTS FOR ORTHOGONAL MAPS,0.3283582089552239,"The proof of this result can be found in Appendix F. The main idea in the proof is that d-dimensional
polar coordinates do the trick up to coordinate-wise rescaling. This proposition implies that the
entire family {fR = R ◦f| R 2 O(n)} satisﬁes (fR)⇤⌫= f⇤⌫and, by deﬁnition of FOCT we have"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.332089552238806,"3This is possible if both distributions have compact connected support where they have a smooth positive
density. We ignore difﬁculties associated with unbounded support or non-regular measures here"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3358208955223881,"Figure 2: Illustration of radius dependent ro-
tations as deﬁned in Lemma 1. The left ﬁgure
shows the initial sources. In the right ﬁgure a
radius dependent volume preserving transfor-
mation was applied (see Appendix C)."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.33955223880597013,"Figure 3: Smooth invariant deformations.
The blue grid indicates the transformation f,
while the green grid shows the deformed map
Φt. Outside the red box Φt = ft holds (see
Deﬁnition 5)."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.34328358208955223,fR 2 FOCT because f 2 FOCT implies R ◦f 2 FOCT. In particular all inverses gR = f −1
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.34701492537313433,"R
recover
independent sources because (gR)⇤f⇤⌫= ⌫and BSS is not possible in a meaningful way in this
(special) case. The construction in Proposition 1 gives spurious solutions for substantially more
observational distributions than just the Gaussian (this is indicated in Table 1). Nevertheless we do
not view this as a general obstacle to identiﬁability results for OCTs for two reasons. Firstly the
spurious solutions only apply to carefully chosen pairs of function and base measure such that we
obtain a still very non-generic (radial) observational distribution. Moreover, the function constructed
in Proposition 1 cannot be extended to Cd = [0, 1]d such that it remains invertible (in the language of
differential geometry: f is only an immersion not an embedding of a submanifold). In this sense the
main message of Proposition 1 is that an identiﬁability result for FOCT needs to contain assumptions
ruling out those spurious solutions (just like Gaussians are excluded for linear ICA)."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.35074626865671643,"Local Stability of OCTs.
We now give partial results towards identiﬁability of OCTs. While
we do not prove general identiﬁability for this class, we demonstrate their local rigidity: OCTs
cannot be continuously deformed to obtain spurious solutions. This is in stark contrast to the general
nonlinear case, which we will discuss for comparison below. Actually, we will show the following
slightly stronger statement. Suppose we know some initial data generating mechanism f0 because
we have, e.g., access to samples (s, f0(s)). Now we assume that the mixing ft depends smoothly
on some parameter t which could be, e.g., time or an environment. Then we can identify ft if
ft 2 FOCT for all t and given access to samples ft(s). This would not be true if ft’s would be
unconstrained nonlinear mixing functions. We now make these statements precise. We ﬁrst deﬁne
smooth deformations of a mixing function."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.35447761194029853,"Deﬁnition 4. (Smooth invariant deformations) Let F be some function class. Consider a family
of differentiable transformations Φ 2 C1((−T, T) ⇥Rd, M) for some T > 0 and a smooth, d-
dimensional manifold M such that Φt = Φ(t, ·) is a diffeomorphism onto its image. We call Φt a
smooth invariant deformation if Φt(·) 2 F for all t."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3582089552238806,"An illustration of this deﬁnition can be found in Figure 3. Based on invariant deformations we can
now deﬁne a local identiﬁability property of ICA in a given function class."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3619402985074627,"Deﬁnition 5. (Local identiﬁability of ICA). Consider a function class F. Let ft be a smooth invariant
deformation in F that is analytic in t. Let Φt be another smooth invariant deformation in F analytic
in t such that Φ0 = f0 and (Φt)⇤⌫= (ft)⇤⌫for all t. Assume that there is "" > 0 such that
Φt(s) = ft(s) if dist(s, @Cd) < "". Then we say that ICA in F is locally identiﬁable at (ft, ⌫) if
these assumptions imply Φt = ft for all t. We call F locally identiﬁable if it is locally identiﬁable at
(ft, ⌫) for all analytic local deformations ft."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3656716417910448,"We remark that our notion of local identiﬁability is indeed similar to the concept of local identiﬁability
in the context of parameter identiﬁcation of neural networks [3]. Local identiﬁability of a function
class means that we can identify smooth deformations ft 2 F from some initial mixing f0 2 F if
f0 is known on the whole latent domain, and the behaviour of ft close to the domain’s boundary is
known for all t. This can be interpreted as plain identiﬁability in the concept drift setting [52, 15].
Formulated differently, it means that an adversary cannot smoothly deform the function f0 in a subset
whose boundary is away from the boundary of the domain, such that the outcome is ambiguous given"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3694029850746269,"the resulting observational distributions. An experimental illustration of this setting and Theorem 4
below can be found in Appendix H. The notion of locality in Deﬁnition 5 should be understood as
“non-global” and notably does not imply restrictions to a small neighbourhood, as local properties"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.373134328358209,"often do. The non-globality manifests itself in two ways: we consider smooth transformations of
the ground truth, i.e., small changes of the data generating function f0 and in addition we assume
that the changes are not everywhere in s, i.e., sources s close to the boundary are kept invariant. An
extension to other source distributions beyond ﬁxed ⌫(and possibly changing with t) is possible but
not necessary in the context of OCTs as explained above. We can show that local identiﬁability holds
true in FOCT.
Theorem 4. The function class FOCT is locally identiﬁable."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.376865671641791,"The proof, in Appendix F, introduces new tools to the ﬁeld of ICA. The main idea is to consider
the vector ﬁeld X that generates the deformation Φt and then rewrite the assumption as systems of
partial differential equations for X. The proof is then completed by showing that the only solution of
this system vanishes. Let us state one simple consequence of this theorem.
Corollary 2. Let Φt be a smooth analytic invariant deformation in FOCT such that (Φt)⇤⌫= f⇤⌫
for all t and there is "" > 0 such that Φt(s) = f(s) if dist(s, @Cd) < "". Then Φt = f for all t."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3805970149253731,"Let us reiterate what this corollary shows: we cannot smoothly and locally transform the function f
such that (1) the observational distribution remains invariant, i.e., equal to f⇤⌫, and (2) the deformed
functions remain OCTs."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3843283582089552,"At a high level this result suggests that OCTs can be identiﬁed if we know f close to the boundary of
the support of s, e.g., by having, in addition to unlabelled data f(s), labelled data (s, f(s)) for those
s where one coordinate si is extremal. Note that we actually do not show this result as there might be
further solutions which are not connected by smooth transformations. We expect that those results
can be generalized substantially. In particular, we conjecture that for “most” functions f the boundary
condition can be removed thus giving a stronger local identiﬁability result up to the boundary of the
support of s. As a partial result in this direction we prove the following theorem.
Theorem 5. Let f : Cd ! Rd be given by f(x) = RDx, with R 2 O(d) and D = Diag(µ1, . . . , µd)
where µi are i.i.d. samples from a distribution supported on the positive reals R+ which has a density.
Suppose that Φt is a smooth invariant deformation in FOCT such that Φ0 = f, (Φt)⇤⌫= f⇤⌫, and
Φt is analytic in t. Then for almost all µi (i.e., with probability one) this implies Φt = f on Cd, i.e.,
Φt is constant in time."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3880597014925373,"In Appendix F we show that this theorem follows from a slightly stronger result stated as Theorem 10
which has a similar proof as Theorem 4. We do expect that the conclusion of the Theorem actually
holds for all µi not just almost all, but we are unable to show this."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.3917910447761194,"Comparison with ICA for general nonlinear functions.
Let us emphasize that those results are
non-trivial as they establish a large difference between ICA with generic nonlinear maps and ICA
with OCTs. To clarify this, we state that no result similar to Theorem 4 holds without the assumption
that Φt 2 FOCT. Put differently, the function class Fnonlinear is not locally identiﬁable.
Fact 2. Suppose f : Cd ! Rd is a diffeomorphism on its image. Then there are uncountably many"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.39552238805970147,"smooth deformations Φt of (f, ⌫) such that (i) (Φt)⇤⌫= f⇤⌫and (ii) there is an "" > 0 such that
Φt(s) = f(s) whenever dist(s, @Cd) < ""."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.39925373134328357,"For completeness, we provide a general construction that is close to our proof of Theorem 4 in
Appendix C in the supplement. A very clear construction for this result was given in [25].
Lemma 1 (Smoothly varying radius dependent rotations (see [25])). Let R : R ⇥R+ ! O(d) be
a smooth function mapping to orthogonal matrices and let a 2 Cd. Assume that R(t, r) = id for
r ≥dist(a, @Cd). Then the map s ! hR,a(t, s) = R(|s −a|, t)(s −a) + a preserves the uniform"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.40298507462686567,"measure ⌫on Cd for all t so that f ◦hR,a(t, s)"
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.40671641791044777,D= f(s) for all t if s is distributed according to ⌫.
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.41044776119402987,"An illustration of this construction is shown in Figure 2 (see App. C for details). Clearly, by
concatenation this allows us to create a vast family of spurious solutions. Note that those solutions
are excluded when restricting to OCTs which is a corollary of Theorem 4.
Corollary 3. Suppose f 2 FOCT. Let Φt be the smooth invariant deformation deﬁned by Φt =
f ◦hR,a(t, ·) where hR,a is as in Lemma 1. If Φt 2 FOCT for all t this implies that hR,a(t, s) = s
and Φt = f for all t."
THIS IS POSSIBLE IF BOTH DISTRIBUTIONS HAVE COMPACT CONNECTED SUPPORT WHERE THEY HAVE A SMOOTH POSITIVE,0.4141791044776119,"We now summarize our view on the results of this section informally (we do not claim that the
statements below regarding (inﬁnite dimensional) manifolds can be made rigorous). For a given
data generating mechanism (f0, ⌫) we expect that typically the set of all solutions MOCT = {f 2
FOCT| f⇤⌫= (f0)⇤⌫} ⇢FOCT is a zero dimensional submanifold, i.e., consists of isolated spurious
solutions and we prove this when ﬁxing the boundary (see Theorem 4) while the corresponding
submanifold of general nonlinear spurious solutions Mnonlinear = {f : Cd ! Rd| f⇤⌫= (f0)⇤⌫} is
inﬁnite dimensional even when requiring f(s) = f0(s) close to the boundary of Cd."
RESULTS FOR VOLUME PRESERVING MAPS,0.417910447761194,"5
Results for volume preserving maps"
RESULTS FOR VOLUME PRESERVING MAPS,0.4216417910447761,"Let us ﬁnally consider volume preserving transformations. For ⌦⇢Rd, those are deﬁned as the set
of functions Fvol = {f : ⌦! Rd | det Df(x) = 1 for all x 2 ⌦}. Invertible volume preserving
deformations have the property that they preserve the standard (Lebesgue)-measure λ in the sense that
f⇤λ⌦= λf(⌦). Recently it was proposed that volume preserving functions are a suitable function
class for ICA. Here we show that those functions are not sufﬁciently rigid to allow identiﬁability
of ICA in the unconditional case. Note that Lemma 1 and Fact 2 already show how to construct
spurious solutions for the case that the base distribution is the uniform measure ⌫. However, for an
arbitrary distribution P this is slightly more difﬁcult because we need to ﬁnd maps g that preserve P,
i.e., g⇤P = P, and are volume preserving, i.e., preserve the standard measure.
Theorem 6. Let p be a twice differentiable probability density with bounded gradient. Suppose that
x = f(s) where the distribution P of s has density p and f is a diffeomorphism with det Df(x) = 1
for x 2 Rd. Then there is a family of functions ft : R ⇥Rd ! Rd with f0 = f and ft 6= f0 for t 6= 0
such that det Dft(x) = 1 and (ft)⇤P = f⇤P."
RESULTS FOR VOLUME PRESERVING MAPS,0.4253731343283582,"The proof and an illustration are in Appendix G. It is based on the ﬂows generated by suitable explicit
vector ﬁelds. As those ﬂows can be concatenated we obtain a large family of spurious solutions.
We think that the approach used here is a powerful technique to construct counter-examples to
identiﬁability in ICA. As local identiﬁability is weaker than identiﬁability we conclude that ICA in
Fvol is not identiﬁable. Note that this is even true when we know the distribution of s. A rigorous
version of this statement is that if (Fvol, Pvol) is identiﬁable with respect to Svol then Svol contains
functions mixing coordinates si, sj for i 6= j (i.e., there is h 2 Svol such that @i@jh 6= 0)."
RELATION TO RIGIDITY THEORY,0.4291044776119403,"6
Relation to rigidity theory"
RELATION TO RIGIDITY THEORY,0.43283582089552236,"Our results rely on rigidity properties of certain function classes. Rigidity refers to the property
that a local constraint on the derivative of a function implies global restrictions on the shape of the
function. These type of results are of interest in the ﬁeld of continuum mechanics [6] where, e.g., the
condition Df 2 SO(d) is used to describe deformation of rigid solids and the condition Det Df = 1
to describe incompressible ﬂuids."
RELATION TO RIGIDITY THEORY,0.43656716417910446,"We now state a well known rigidity result for conformal maps that is the main input in the proof of
Theorem 2. This result shows that there are very few conformal maps in dimension d > 2.
Theorem 7 (Liouville). Let d > 2, ⌦⇢Rd open and connected, f : ⌦! Rd conformal. Then"
RELATION TO RIGIDITY THEORY,0.44029850746268656,"f(x) = b + ↵A(x −a)/|x −a|""
(12)"
RELATION TO RIGIDITY THEORY,0.44402985074626866,"where b, a 2 Rd, ↵2 R, A 2 O(d), and "" 2 {0, 2}."
RELATION TO RIGIDITY THEORY,0.44776119402985076,"Originally this result was shown by Liouville [36], a modern treatment is [27]. In particular, this
shows that conformal maps are (up to translations) rotations or rotations followed by an inversion."
RELATION TO RIGIDITY THEORY,0.45149253731343286,"To illustrate the strength of Theorem 7 we compare it to the setting of volume preserving maps which
satisfy no similar rigidity property. Intuitively the different rigidity properties are already apparent
from the connection to solids, which can merely be rotated and shifted, and ﬂuids which can also
be stirred leading to chaotic deformations. Rigorously the different behaviours can be clariﬁed by
the observation that conformal maps have a ﬁnite number of parameters and thus a ﬁnite number of
constraints (e.g., of the form f(x) = y) allows to identify them. In contrast volume preserving maps
cannot be identiﬁed from ﬁnitely many constraints as the following proposition shows.
Proposition 2. For d > 2 and {x1, . . . , xn, y1, . . . , yn} ⇢Rd, all pairwise different, there is a
volume preserving diffeomorphism f : Rd ! Rd, f 2 Fvol such that f(xi) = yi."
RELATION TO RIGIDITY THEORY,0.4552238805970149,"Let us emphasize that the different rigidity properties are not at all surprising when arguing based
on degrees of freedom or numbers of constraints. While volume preserving maps enforce only a
single scalar constraint on the Jacobian Df the condition for conformal maps gives n(n + 1)/2 −1
constraints on the Jacobian."
RELATION TO RIGIDITY THEORY,0.458955223880597,"Let us ﬁnally comment on OCTs where the picture is not as well understood. As discussed in
Section 4 it is known [10, 4] that OCTs constitute a rich, non-parametric class of functions and
therefore OCTs are much more ﬂexible than conformal maps. We illustrated this with example OCT
constructions in Section 4 leveraging 2D conformal maps. Nevertheless, it is not known if and what
rigidity properties can be derived for OCTs. However, our results suggest that the additional measure
preservation condition in the context of ICA gives enough rigidity to (almost) give identiﬁability of
ICA. In this sense OCTs might be a good function class for ICA as it is rich enough to allow complex
representations of data while at the same time being sufﬁciently rigid to still provide a notion of
identiﬁability whose strength remains to be determined."
DISCUSSION,0.4626865671641791,"7
Discussion"
DISCUSSION,0.4664179104477612,"ICA is long known to be identiﬁable for linear maps, baring pathological cases, and highly non-
identiﬁable for general nonlinear ones. Surprisingly, similar results for function classes of intermediate
complexity remain scarce. In this work we address this question with several identiﬁability results for
different function classes. Our ﬁrst main result is that ICA is identiﬁable in the class of conformal
maps (up to classical ambiguities). This considerably extends previous claims, limited to a speciﬁc
2D setting [25], and ruling out several families of spurious solutions [17]. On the negative side we
show that the ICA problem for volume preserving maps admits a large class of spurious solutions.
Finally, we show that OCTs satisfy certain weaker notions of local identiﬁability."
DISCUSSION,0.4701492537313433,"In our proofs, we draw connections to methods and techniques that, to the best of our knowledge,
have not been used in the context of ICA before. We relate the identiﬁability problem in ICA to
the rigidity of the considered function class F and use tools from the theory of partial differential
equations. These techniques have been applied very successfully to the analysis of elastic solids
[7, 6] and we believe that there are many applications of these methods in the ﬁeld of ICA."
DISCUSSION,0.47388059701492535,"While the main focus of current research after the seminal work of Hyvärinen et al. [26] is on
the auxiliary variable case, there are three reasons to consider unconditional ICA. Firstly, it is a
fundamental research question that is, as illustrated by our results, deeply rooted in functional
analysis. Secondly there is high application potential for completely unsupervised learning without
any auxiliary variables, as the corresponding datasets do not require labelling or speciﬁc experimental
settings. Thirdly, it is very likely that the techniques can be generalized to the auxiliary variable case."
DISCUSSION,0.47761194029850745,"Another important open problem is assessing the type of constraints on ground truth mechanisms,
encoded by function classes, that are relevant for real world data. It is plausible that those mech-
anisms are typically much more regular than generic nonlinear functions. Recently, Gresele et al.
[17] suggested, based on arguments from the causality literature that FOCT is a natural class for
representation learning (and our results show it also has favourable theoretical properties), but this
will require experimental conﬁrmation on real world data."
DISCUSSION,0.48134328358208955,"Finally, a central question from a machine learning perspective is the ability to design learning
algorithms that can train LVMs with identiﬁable function class constraints. Interestingly, Gresele et al.
[17] showed that OCT maps can be learnt using a closed from regularized likelihood loss, thereby
providing, supported by our result, a full-ﬂedged identiﬁable nonlinear ICA framework."
DISCUSSION,0.48507462686567165,Acknowledgments and Disclosure of Funding
DISCUSSION,0.48880597014925375,"We thank Luigi Gresele and Julius von Kügelgen for helpful discussions and feedback on an earlier
version of this paper. We thank the anonymous reviewers whose comments helped us to make
the paper more accessible. This work was in part supported by the German Federal Ministry of
Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B) and the
Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645."
REFERENCES,0.4925373134328358,References
REFERENCES,0.4962686567164179,[1] L. Ahlfors. Complex Analysis: An Introduction to the Theory of Analytic Functions of One
REFERENCES,0.5,"Complex Variable, Third Edition. AMS Chelsea Publishing Series. American Mathematical
Society, 1979. ISBN 9781470467678."
REFERENCES,0.503731343283582,"[2] Y. Bengio, A. C. Courville, and P. Vincent. Representation learning: A review and new"
REFERENCES,0.5074626865671642,"perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, 2013. doi: 10.1109/
TPAMI.2013.50. URL https://doi.org/10.1109/TPAMI.2013.50."
REFERENCES,0.5111940298507462,"[3] J. Bona-Pellissier, F. Malgouyres, and F. Bachoc. Local identiﬁability of deep reLU neural"
REFERENCES,0.5149253731343284,"networks: the theory. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in
Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=
-3cHWtrbLYq."
REFERENCES,0.5186567164179104,"[4] E. Cartan.
La géométrie des espaces de Riemann.
Gauthier-Villars, 1925.
URL http:
//eudml.org/doc/192543."
REFERENCES,0.5223880597014925,"[5] T. Q. Chen, X. Li, R. B. Grosse, and D. Duvenaud. Isolating sources of disentanglement in"
REFERENCES,0.5261194029850746,"variational autoencoders. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8,
2018, Montréal, Canada, pages 2615–2625, 2018. URL https://proceedings.neurips.
cc/paper/2018/hash/1ee3dfcd8a0645a25a35977997223d22-Abstract.html."
REFERENCES,0.5298507462686567,"[6] P. G. Ciarlet. Mathematical Elasticity: Volume II: Theory of Plates. ISSN. Elsevier Science,"
REFERENCES,0.5335820895522388,1997. ISBN 9780080535913.
REFERENCES,0.5373134328358209,"[7] P. G. Ciarlet. Mathematical Elasticity, Volume I: Three-Dimensional Elasticity. Classics in"
REFERENCES,0.5410447761194029,"Applied Mathematics Series. Society for Industrial and Applied Mathematics, 2021. ISBN
9781611976779."
REFERENCES,0.5447761194029851,"[8] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287–314,"
REFERENCES,0.5485074626865671,"1994. ISSN 0165-1684. doi: https://doi.org/10.1016/0165-1684(94)90029-9. URL https:
//www.sciencedirect.com/science/article/pii/0165168494900299. Higher Order"
REFERENCES,0.5522388059701493,Statistics.
REFERENCES,0.5559701492537313,[9] B. Dacorogna and J. Moser. On a partial differential equation involving the jacobian determinant.
REFERENCES,0.5597014925373134,"Annales de l’I.H.P. Analyse non linéaire, 7(1):1–26, 1990. URL http://www.numdam.org/"
REFERENCES,0.5634328358208955,item/AIHPC_1990__7_1_1_0/.
REFERENCES,0.5671641791044776,[10] G. Darboux. Leçons sur la théorie générale des surfaces. 1896.
REFERENCES,0.5708955223880597,[11] D. L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for
REFERENCES,0.5746268656716418,"high-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591–5596,
2003. doi: 10.1073/pnas.1031596100. URL https://www.pnas.org/doi/abs/10.1073/
pnas.1031596100."
REFERENCES,0.5783582089552238,[12] D. R. Dunninger and E. C. Zachmanoglou. The condition for uniqueness of solutions of
REFERENCES,0.582089552238806,"the dirichlet problem for the wave equation in coordinate rectangles.
Journal of Mathe-
matical Analysis and Applications, 20(1):17–21, 1967. ISSN 0022-247X. doi: https://doi.
org/10.1016/0022-247X(67)90103-5. URL https://www.sciencedirect.com/science/
article/pii/0022247X67901035."
REFERENCES,0.585820895522388,"[13] C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. nﬂows: normalizing ﬂows in PyTorch,"
REFERENCES,0.5895522388059702,Nov. 2020. URL https://doi.org/10.5281/zenodo.4296287.
REFERENCES,0.5932835820895522,"[14] L. C. Evans. Partial differential equations. American Mathematical Society, Providence, R.I.,"
REFERENCES,0.5970149253731343,2010. ISBN 9780821849743 0821849743.
REFERENCES,0.6007462686567164,"[15] J. a. Gama, I. Žliobaitundeﬁned, A. Bifet, M. Pechenizkiy, and A. Bouchachia. A survey"
REFERENCES,0.6044776119402985,"on concept drift adaptation. ACM Comput. Surv., 46(4), mar 2014. ISSN 0360-0300. doi:
10.1145/2523813. URL https://doi.org/10.1145/2523813."
REFERENCES,0.6082089552238806,"[16] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,"
REFERENCES,0.6119402985074627,"and Y. Bengio. Generative Adversarial Networks. arXiv:1406.2661 [cs, stat], June 2014. URL
http://arxiv.org/abs/1406.2661. arXiv: 1406.2661."
REFERENCES,0.6156716417910447,"[17] L. Gresele, J. von Kügelgen, V. Stimper, B. Schölkopf, and M. Besserve. Independent mecha-"
REFERENCES,0.6194029850746269,"nism analysis, a new concept? In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W.
Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,
pages 28233–28248, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
edc27f139c3b4e4bb29d1cdbc45663f9-Abstract.html."
REFERENCES,0.6231343283582089,"[18] X. Gu, Y. Wang, T. F. Chan, P. M. Thompson, and S. Yau. Genus zero surface conformal"
REFERENCES,0.6268656716417911,"mapping and its application to brain surface mapping. IEEE Trans. Medical Imaging, 23(8):
949–958, 2004. doi: 10.1109/TMI.2004.831226. URL https://doi.org/10.1109/TMI.
2004.831226."
REFERENCES,0.6305970149253731,"[19] S. Harmeling, A. Ziehe, M. Kawanabe, and K. Müller. Kernel-based nonlinear blind source"
REFERENCES,0.6343283582089553,"separation. Neural Comput., 15(5):1089–1124, 2003. doi: 10.1162/089976603765202677.
URL https://doi.org/10.1162/089976603765202677."
REFERENCES,0.6380597014925373,"[20] I. Higgins, L. Matthey, A. Pal, C. P. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed,"
REFERENCES,0.6417910447761194,"and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL"
REFERENCES,0.6455223880597015,https://openreview.net/forum?id=Sy2fzU9gl.
REFERENCES,0.6492537313432836,"[21] I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. Rezende, and A. Lerchner. Towards"
REFERENCES,0.6529850746268657,"a deﬁnition of disentangled representations, 2018. URL https://arxiv.org/abs/1812.
02230."
REFERENCES,0.6567164179104478,"[22] D. Horan, E. Richardson, and Y. Weiss. When is unsupervised disentanglement possible?"
REFERENCES,0.6604477611940298,"In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Ad-
vances in Neural Information Processing Systems, volume 34, pages 5150–5161. Curran
Associates, Inc., 2021.
URL https://proceedings.neurips.cc/paper/2021/file/
29586cb449c90e249f1f09a0a4ee245a-Paper.pdf."
REFERENCES,0.664179104477612,[23] A. Hyvärinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning
REFERENCES,0.667910447761194,"and nonlinear ICA. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pages 3765–3773, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
d305281faf947ca7acade9ad5c8c818c-Abstract.html."
REFERENCES,0.6716417910447762,[24] A. Hyvärinen and H. Morioka. Nonlinear ICA of temporally dependent stationary sources.
REFERENCES,0.6753731343283582,"In A. Singh and X. J. Zhu, editors, Proceedings of the 20th International Conference on
Artiﬁcial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL,
USA, volume 54 of Proceedings of Machine Learning Research, pages 460–469. PMLR, 2017.
URL http://proceedings.mlr.press/v54/hyvarinen17a.html."
REFERENCES,0.6791044776119403,[25] A. Hyvärinen and P. Pajunen. Nonlinear independent component analysis: Existence and
REFERENCES,0.6828358208955224,"uniqueness results. Neural Networks, 12(3):429–439, 1999. doi: 10.1016/S0893-6080(98)
00140-3. URL https://doi.org/10.1016/S0893-6080(98)00140-3."
REFERENCES,0.6865671641791045,"[26] A. Hyvärinen, H. Sasaki, and R. E. Turner. Nonlinear ICA using auxiliary variables and gener-"
REFERENCES,0.6902985074626866,"alized contrastive learning. In K. Chaudhuri and M. Sugiyama, editors, The 22nd International
Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha,
Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pages 859–868.
PMLR, 2019. URL http://proceedings.mlr.press/v89/hyvarinen19a.html."
REFERENCES,0.6940298507462687,[27] T. Iwaniec and G. J. Martin. Geometric Function Theory and Non-linear Analysis. Oxford
REFERENCES,0.6977611940298507,"mathematical monographs. Oxford University Press, 2001. ISBN 9780198509295. URL
https://books.google.de/books?id=fN3WKfPPTHAC."
REFERENCES,0.7014925373134329,[28] E. G. Kalnins and W. Miller. Separation of variables on n-dimensional riemannian manifolds. i.
REFERENCES,0.7052238805970149,"the n-sphere sn and euclidean n-space rn. Journal of Mathematical Physics, 27(7):1721–1736,
1986. doi: 10.1063/1.527088. URL https://doi.org/10.1063/1.527088."
REFERENCES,0.7089552238805971,"[29] I. Khemakhem, D. P. Kingma, R. P. Monti, and A. Hyvärinen. Variational autoencoders"
REFERENCES,0.7126865671641791,"and nonlinear ICA: A unifying framework. In S. Chiappa and R. Calandra, editors, The
23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2020, 26-28
August 2020, Online [Palermo, Sicily, Italy], volume 108 of Proceedings of Machine Learning
Research, pages 2207–2217. PMLR, 2020. URL http://proceedings.mlr.press/v108/
khemakhem20a.html."
REFERENCES,0.7164179104477612,"[30] H. Kim and A. Mnih. Disentangling by factorising. In J. G. Dy and A. Krause, editors, Proceed-"
REFERENCES,0.7201492537313433,"ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research,
pages 2654–2663. PMLR, 2018.
URL http://proceedings.mlr.press/v80/kim18b.
html."
REFERENCES,0.7238805970149254,[31] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and
REFERENCES,0.7276119402985075,"Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.7313432835820896,"[32] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. arXiv:1312.6114 [cs, stat],"
REFERENCES,0.7350746268656716,May 2014. URL http://arxiv.org/abs/1312.6114. arXiv: 1312.6114.
REFERENCES,0.7388059701492538,[33] A. Kumar and B. Poole. On implicit regularization in β-vaes. In Proceedings of the 37th
REFERENCES,0.7425373134328358,"International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,
volume 119 of Proceedings of Machine Learning Research, pages 5480–5490. PMLR, 2020.
URL http://proceedings.mlr.press/v119/kumar20d.html."
REFERENCES,0.746268656716418,"[34] A. Kumar, P. Sattigeri, and A. Balakrishnan. Variational inference of disentangled latent concepts"
REFERENCES,0.75,"from unlabeled observations. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=H1kG7GZAW."
REFERENCES,0.753731343283582,"[35] J. M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer, 2003."
REFERENCES,0.7574626865671642,ISBN 9780387954486.
REFERENCES,0.7611940298507462,[36] J. Liouville. Extension au cas des trois dimensions de la question du tracé géographique. Note
REFERENCES,0.7649253731343284,"VI, pages 609–617, 1850."
REFERENCES,0.7686567164179104,"[37] F. Locatello, S. Bauer, M. Lucic, G. Rätsch, S. Gelly, B. Schölkopf, and O. Bachem. Chal-"
REFERENCES,0.7723880597014925,"lenging common assumptions in the unsupervised learning of disentangled representations.
In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Con-
ference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pages 4114–4124. PMLR, 2019.
URL http://proceedings.mlr.press/v97/locatello19a.html."
REFERENCES,0.7761194029850746,[38] W. F. Osgood. On the existence of the green’s function for the most general simply connected
REFERENCES,0.7798507462686567,"plane region. Transactions of the American Mathematical Society, 1(3):310–314, 1900."
REFERENCES,0.7835820895522388,"[39] G. Papamakarios, E. T. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan."
REFERENCES,0.7873134328358209,"Normalizing ﬂows for probabilistic modeling and inference. J. Mach. Learn. Res., 22:57:1–
57:64, 2021. URL http://jmlr.org/papers/v22/19-1028.html."
REFERENCES,0.7910447761194029,"[40] P. Reizinger, L. Gresele, J. Brady, J. V. Kügelgen, D. Zietlow, B. Schölkopf, G. Martius,"
REFERENCES,0.7947761194029851,"W. Brendel, and M. Besserve. Embrace the gap: VAEs perform independent mechanism analysis.
In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information
Processing Systems, 2022. URL https://openreview.net/forum?id=G4GpqX4bKAH."
REFERENCES,0.7985074626865671,[41] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In F. R. Bach and
REFERENCES,0.8022388059701493,"D. M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference"
REFERENCES,0.8059701492537313,"Proceedings, pages 1530–1538. JMLR.org, 2015. URL http://proceedings.mlr.press/
v37/rezende15.html."
REFERENCES,0.8097014925373134,[42] K. Ridgeway and M. C. Mozer. Learning deep disentangled embeddings with the f-statistic
REFERENCES,0.8134328358208955,"loss. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada, pages 185–194, 2018. URL https://proceedings.neurips.cc/paper/2018/
hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html."
REFERENCES,0.8171641791044776,"[43] M. Rolínek, D. Zietlow, and G. Martius.
Variational autoencoders pursue PCA direc-
tions (by accident).
In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 12406–12415. Com-
puter Vision Foundation / IEEE, 2019.
doi: 10.1109/CVPR.2019.01269.
URL http:
//openaccess.thecvf.com/content_CVPR_2019/html/Rolinek_Variational_
Autoencoders_Pursue_PCA_Directions_by_Accident_CVPR_2019_paper.html."
REFERENCES,0.8208955223880597,[44] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
REFERENCES,0.8246268656716418,"Science, 290(5500):2323–2326, 2000. doi: 10.1126/science.290.5500.2323. URL https:
//www.science.org/doi/abs/10.1126/science.290.5500.2323."
REFERENCES,0.8283582089552238,"[45] B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio."
REFERENCES,0.832089552238806,"Toward causal representation learning. Proc. IEEE, 109(5):612–634, 2021. doi: 10.1109/
JPROC.2021.3058954. URL https://doi.org/10.1109/JPROC.2021.3058954."
REFERENCES,0.835820895522388,"[46] E. Sharon and D. Mumford. 2d-shape analysis using conformal mapping. Int. J. Comput. Vis.,"
REFERENCES,0.8395522388059702,"70(1):55–75, 2006. doi: 10.1007/s11263-006-6121-z. URL https://doi.org/10.1007/
s11263-006-6121-z."
REFERENCES,0.8432835820895522,"[47] J. Sliwa, S. Ghosh, V. Stimper, L. Gresele, and B. Schölkopf. Probing the robustness of"
REFERENCES,0.8470149253731343,"independent mechanism analysis for representation learning, 2022. URL https://arxiv.
org/abs/2207.06137."
REFERENCES,0.8507462686567164,"[48] P. Sorrenson, C. Rother, and U. Köthe. Disentanglement by nonlinear ICA with general"
REFERENCES,0.8544776119402985,"incompressible-ﬂow networks (GIN). In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=rygeHgSFDH."
REFERENCES,0.8582089552238806,[49] A. Taleb and C. Jutten. Source separation in post-nonlinear mixtures. IEEE Trans. Signal
REFERENCES,0.8619402985074627,"Process., 47(10):2807–2820, 1999. doi: 10.1109/78.790661. URL https://doi.org/10.
1109/78.790661."
REFERENCES,0.8656716417910447,"[50] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear"
REFERENCES,0.8694029850746269,"dimensionality reduction. Science, 290(5500):2319–2323, Dec. 2000."
REFERENCES,0.8731343283582089,"[51] S. Wang, Y. Wang, M. Jin, X. D. Gu, and D. Samaras. Conformal geometry and its applications"
REFERENCES,0.8768656716417911,"on 3d shape matching, recognition, and stitching. IEEE Trans. Pattern Anal. Mach. Intell., 29
(7):1209–1220, 2007. doi: 10.1109/TPAMI.2007.1050. URL https://doi.org/10.1109/
TPAMI.2007.1050."
REFERENCES,0.8805970149253731,[52] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts.
REFERENCES,0.8843283582089553,"Machine Learning, 23(1):69–101, 1996. doi: 10.1007/BF00116900. URL https://doi.org/
10.1007/BF00116900."
REFERENCES,0.8880597014925373,"[53] X. Yang, Y. Wang, J. Sun, X. Zhang, S. Zhang, Z. Li, and J. Yan. Nonlinear ICA using volume-"
REFERENCES,0.8917910447761194,"preserving transformations. In International Conference on Learning Representations, 2022.
URL https://openreview.net/forum?id=AMpki9kp8Cn."
REFERENCES,0.8955223880597015,"[54] Y. Zheng, I. Ng, and K. Zhang. On the identiﬁability of nonlinear ICA: sparsity and beyond."
REFERENCES,0.8992537313432836,"CoRR, abs/2206.07751, 2022. doi: 10.48550/arXiv.2206.07751. URL https://doi.org/10.
48550/arXiv.2206.07751."
REFERENCES,0.9029850746268657,"[55] D. Zietlow, M. Rolínek, and G. Martius. Demystifying inductive biases for (beta-)vae based"
REFERENCES,0.9067164179104478,"architectures.
In M. Meila and T. Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139
of Proceedings of Machine Learning Research, pages 12945–12954. PMLR, 2021. URL
http://proceedings.mlr.press/v139/zietlow21a.html."
REFERENCES,0.9104477611940298,Checklist
REFERENCES,0.914179104477612,"1.
For all authors..."
REFERENCES,0.917910447761194,(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
REFERENCES,0.9216417910447762,"contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See discussion (Section 7) and"
REFERENCES,0.9253731343283582,"assumptions of our results
(c) Did you discuss any potential negative societal impacts of your work? [N/A] Due to"
REFERENCES,0.9291044776119403,"the theoretical focus of this work no direct negative impacts are expected.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to"
REFERENCES,0.9328358208955224,"them? [Yes]
2.
If you are including theoretical results..."
REFERENCES,0.9365671641791045,(a) Did you state the full set of assumptions of all theoretical results? [Yes] All results are
REFERENCES,0.9402985074626866,"stated including all required assumptions
(b) Did you include complete proofs of all theoretical results? [Yes] The proofs can be"
REFERENCES,0.9440298507462687,"found in the supplementary material (Appendix D to Appendix G)
3.
If you ran experiments..."
REFERENCES,0.9477611940298507,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-"
REFERENCES,0.9514925373134329,"mental results (either in the supplemental material or as a URL)? [Yes] See Appendix H
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they"
REFERENCES,0.9552238805970149,"were chosen)? [Yes] See end of Appendix H
(c) Did you report error bars (e.g., with respect to the random seed after running experi-"
REFERENCES,0.9589552238805971,"ments multiple times)? [Yes] See Figure 7 and 8
(d) Did you include the total amount of compute and the type of resources used (e.g., type"
REFERENCES,0.9626865671641791,"of GPUs, internal cluster, or cloud provider)? [Yes] See end of Appendix H
4.
If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9664179104477612,"(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9701492537313433,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9738805970149254,(d) Did you discuss whether and how consent was obtained from people whose data you’re
REFERENCES,0.9776119402985075,"using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable"
REFERENCES,0.9813432835820896,"information or offensive content? [N/A]
5.
If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9850746268656716,"(a) Did you include the full text of instructions given to participants and screenshots, if"
REFERENCES,0.9888059701492538,"applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review"
REFERENCES,0.9925373134328358,"Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount"
REFERENCES,0.996268656716418,spent on participant compensation? [N/A]
