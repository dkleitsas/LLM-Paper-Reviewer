Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022988505747126436,"Exceptional text-to-image (T2I) generation results of Stable Diffusion models
1"
ABSTRACT,0.004597701149425287,"(SDMs) come with substantial computational demands. To resolve this issue, re-
2"
ABSTRACT,0.006896551724137931,"cent research on efficient SDMs has prioritized reducing the number of sampling
3"
ABSTRACT,0.009195402298850575,"steps and utilizing network quantization. Orthogonal to these directions, this study
4"
ABSTRACT,0.011494252873563218,"highlights the power of classical architectural compression for general-purpose T2I
5"
ABSTRACT,0.013793103448275862,"synthesis by introducing a block-removed knowledge-distilled SDM (BK-SDM).
6"
ABSTRACT,0.016091954022988506,"We eliminate several residual and attention blocks from the U-Net of SDMs, obtain-
7"
ABSTRACT,0.01839080459770115,"ing over a 30% reduction in the number of parameters, MACs per sampling step,
8"
ABSTRACT,0.020689655172413793,"and latency. We conduct distillation-based pretraining with only 0.22M LAION
9"
ABSTRACT,0.022988505747126436,"pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite
10"
ABSTRACT,0.02528735632183908,"being trained with limited resources, our compact models can imitate the original
11"
ABSTRACT,0.027586206896551724,"SDM by benefiting from transferred knowledge and achieve competitive results
12"
ABSTRACT,0.029885057471264367,"against larger multi-billion parameter models on the zero-shot MS-COCO bench-
13"
ABSTRACT,0.03218390804597701,"mark. Moreover, we demonstrate the applicability of our lightweight pretrained
14"
ABSTRACT,0.034482758620689655,"models in personalized generation with DreamBooth finetuning.
15"
ABSTRACT,0.0367816091954023,"Figure 1: Our compressed stable diffusion enables efficient (a) zero-shot general-purpose text-to-
image generation and (b) personalized synthesis. Selected samples from our lightest BK-SDM-Small
with 36% reduced parameters and latency are shown."
INTRODUCTION,0.03908045977011494,"1
Introduction
16"
INTRODUCTION,0.041379310344827586,"Large diffusion models [44, 51, 38, 47] have showcased groundbreaking results in text-to-image (T2I)
17"
INTRODUCTION,0.04367816091954023,"synthesis tasks, which aim to create photorealistic images from textual descriptions. Stable Diffusion
18"
INTRODUCTION,0.04597701149425287,"models (SDMs) [46, 47] are one of the most renowned open-source models, and their exceptional
19"
INTRODUCTION,0.04827586206896552,"capability has begun to be leveraged as a backbone in several text-guided vision applications, e.g.,
20"
INTRODUCTION,0.05057471264367816,"text-driven image editing [2, 23] and 3D object creation [67], text-to-video generation [1, 68], and
21"
INTRODUCTION,0.052873563218390804,"subject-driven [50, 25] and controllable [37, 71] T2I.
22"
INTRODUCTION,0.05517241379310345,"Figure 2: Computation of the major components
in Stable Diffusion v1. The denoising U-Net is the
main processing bottleneck. THOP [75] is used to
measure MACs in generating a 512×512 image."
INTRODUCTION,0.05747126436781609,"SDMs are T2I-specialized latent diffusion mod-
23"
INTRODUCTION,0.059770114942528735,"els (LDMs) [47], which employ diffusion oper-
24"
INTRODUCTION,0.06206896551724138,"ations [17, 59, 30] in a latent space to improve
25"
INTRODUCTION,0.06436781609195402,"compute efficiency. Within a SDM, a U-Net
26"
INTRODUCTION,0.06666666666666667,"[49, 6] conducts an iterative sampling procedure
27"
INTRODUCTION,0.06896551724137931,"to gradually eliminate noise from random la-
28"
INTRODUCTION,0.07126436781609195,"tents and is assisted by a text encoder [42] and
29"
INTRODUCTION,0.0735632183908046,"an image decoder [9, 64] to produce text-aligned
30"
INTRODUCTION,0.07586206896551724,"images. This inference process still involves ex-
31"
INTRODUCTION,0.07816091954022988,"cessive computational requirements (see Figure
32"
INTRODUCTION,0.08045977011494253,"2), which often hinder the utilization of SDMs
33"
INTRODUCTION,0.08275862068965517,"despite their rapidly growing usage.
34"
INTRODUCTION,0.08505747126436781,"To alleviate this issue, numerous approaches
35"
INTRODUCTION,0.08735632183908046,"toward efficient SDMs have been introduced.
36"
INTRODUCTION,0.0896551724137931,"Meng et al. [35, 34] reduce the number of denoising steps by distilling a pretrained diffusion model
37"
INTRODUCTION,0.09195402298850575,"to guide an identically architectured model with fewer sampling steps. Li et al. [28], Hou and Asghar
38"
INTRODUCTION,0.09425287356321839,"[19], Shen et al. [57] employ post-training quantization techniques, and Chen et al. [4] enhance the
39"
INTRODUCTION,0.09655172413793103,"implementation of SDMs for better compatibility with GPUs. However, the removal of architectural
40"
INTRODUCTION,0.09885057471264368,"elements in diffusion models has not been investigated in spite of the established efficacy of structured
41"
INTRODUCTION,0.10114942528735632,"pruning across discriminative models [26, 69] and generative adversarial networks (GANs) [31, 24].
42"
INTRODUCTION,0.10344827586206896,"This study unlocks the immense potential of classical architectural compression in attaining smaller
43"
INTRODUCTION,0.10574712643678161,"and faster diffusion models. We eliminate multiple residual and attention blocks from the U-Net of a
44"
INTRODUCTION,0.10804597701149425,"SDM and pretrain it with feature-level knowledge distillation (KD) [48, 13] for general-purpose T2I
45"
INTRODUCTION,0.1103448275862069,"synthesis. Despite being trained with only 0.22M LAION pairs (less than 0.1% of the entire training
46"
INTRODUCTION,0.11264367816091954,"pairs) [55] on a single A100 GPU, our compact models can mimic the original SDM by leveraging
47"
INTRODUCTION,0.11494252873563218,"transferred knowledge. On the popular zero-shot MS-COCO benchmark [29], our work achieves a
48"
INTRODUCTION,0.11724137931034483,"FID [15] score of 15.76 with 0.76B parameters and 16.98 with 0.66B parameters, which are on par
49"
INTRODUCTION,0.11954022988505747,"with multi-billion parameter models [43, 7, 8]. Furthermore, we present the practical application of
50"
INTRODUCTION,0.12183908045977011,"our lightweight pretrained models in customized T2I with DreamBooth finetuning [50].
51"
INTRODUCTION,0.12413793103448276,"Our contributions are summarized as follows:
52"
INTRODUCTION,0.12643678160919541,"◦To the best of our knowledge, this is the first study to architecturally compress large-scale
53"
INTRODUCTION,0.12873563218390804,"diffusion models. Our work is orthogonal to prior directions for efficient diffusion, e.g., enabling
54"
INTRODUCTION,0.1310344827586207,"less sampling steps and employing quantization, and can be readily integrated with them.
55"
INTRODUCTION,0.13333333333333333,"◦We compress SDMs by removing architectural blocks from the U-Net and achieve more than
56"
INTRODUCTION,0.135632183908046,"30% reduction in model size and inference speed. We also introduce an interesting finding on
57"
INTRODUCTION,0.13793103448275862,"the minor role of innermost blocks.
58"
INTRODUCTION,0.14022988505747128,"◦We demonstrate the advantage of distillation-based pretraining, which allows us to attain com-
59"
INTRODUCTION,0.1425287356321839,"petitive zero-shot T2I results even with very limited training resources.
60"
INTRODUCTION,0.14482758620689656,"◦We highlight the capability of our light pretrained backbones in customized generation. Our
61"
INTRODUCTION,0.1471264367816092,"models can lower the finetuning cost by 30% while retaining 97% scores of the original SDM.
62"
RELATED WORK,0.14942528735632185,"2
Related work
63"
RELATED WORK,0.15172413793103448,"Large T2I diffusion models. By gradually removing noise from corrupted data, diffusion-based
64"
RELATED WORK,0.15402298850574714,"generative models [18, 59, 6] enable high-fidelity synthesis with broad mode coverage. Integrating
65"
RELATED WORK,0.15632183908045977,"these merits with the advancement of pretrained language models [42, 41, 5] has significantly
66"
RELATED WORK,0.15862068965517243,"improved the quality of T2I synthesis. In GLIDE [38] and Imagen [51], a text-conditional diffusion
67"
RELATED WORK,0.16091954022988506,"Figure 3: U-Net architectures of SDMs and KD-based pretraining process. The compact U-Net
student is built by eliminating several residual and attention blocks from the original U-Net teacher.
Through the feature and output distillation from the teacher, the student can be trained effectively yet
rapidly. See Appendix for the details of block components."
RELATED WORK,0.1632183908045977,"model generates a 64×64 image, which is upsampled via super-resolution modules. In DALL·E-2 [44],
68"
RELATED WORK,0.16551724137931034,"a text-conditional prior network produces an image embedding, which is transformed into a 64×64
69"
RELATED WORK,0.167816091954023,"image via a diffusion decoder and further upscaled into higher resolutions. SDMs [46, 47] perform
70"
RELATED WORK,0.17011494252873563,"the diffusion modeling in a 64×64 latent space constructed through a pixel-space autoencoder. We use
71"
RELATED WORK,0.1724137931034483,"SDM as our baseline because of its open-access and gaining popularity over numerous downstream
72"
RELATED WORK,0.17471264367816092,"tasks [2, 67, 1, 50].
73"
RELATED WORK,0.17701149425287357,"Efficient diffusion models. Several studies have addressed the slow sampling process of diffusion
74"
RELATED WORK,0.1793103448275862,"models. Diffusion-tailored distillation approaches [35, 34, 52] progressively transfer knowledge from
75"
RELATED WORK,0.18160919540229886,"a pretrained diffusion model to a fewer-step model with the same architecture. Fast high-order solvers
76"
RELATED WORK,0.1839080459770115,"[32, 33, 73] for diffusion ordinary differential equations boost the sampling speed. Orthogonal
77"
RELATED WORK,0.18620689655172415,"to these directions for less sampling steps, our network compression approach reduces per-step
78"
RELATED WORK,0.18850574712643678,"computation and can be easily integrated with them. Leveraging quantization techniques [28, 19, 57]
79"
RELATED WORK,0.19080459770114944,"and implementation optimizations [4] has been applied for SDMs and also can be combined with our
80"
RELATED WORK,0.19310344827586207,"models for further efficiency gains.
81"
RELATED WORK,0.19540229885057472,"Distillation-based compression. KD enhances the performance of small-size models by exploiting
82"
RELATED WORK,0.19770114942528735,"output-level [16, 39] and feature-level [48, 13, 70] information of large source models. Although
83"
RELATED WORK,0.2,"this classical distillation has been actively used toward efficient GANs [27, 45, 31, 22, 72], its power
84"
RELATED WORK,0.20229885057471264,"has not been explored for structurally compressed diffusion models. Distillation-based pretraining
85"
RELATED WORK,0.2045977011494253,"enables small yet capable general-purpose language models [54, 61, 21] and vision transformers
86"
RELATED WORK,0.20689655172413793,"[63, 11]. Beyond such models, we show that its success can be extended to diffusion models with
87"
RELATED WORK,0.20919540229885059,"iterative sampling steps. Concurrently with our study, a recently released small SDM without paper
88"
RELATED WORK,0.21149425287356322,"evidence [40] similarly utilizes KD pretraining for a block-eliminated architecture, but it relies on
89"
RELATED WORK,0.21379310344827587,"significantly more training resources along with multi-stage distillation. In contrast, our lightest
90"
RELATED WORK,0.2160919540229885,"model achieves further reduced computation, and we show that competitive results can be obtained
91"
RELATED WORK,0.21839080459770116,"even with much less data and single-stage distillation.
92"
RELATED WORK,0.2206896551724138,"3
BK-SDM: block-removed knowledge-distilled SDM
93"
RELATED WORK,0.22298850574712645,"We compress the U-Net [49] of a SDM [46, 47], which is the most compute-heavy component (see
94"
RELATED WORK,0.22528735632183908,"Figure 2). Conditioned on the text and time-step embeddings, the U-Net performs multiple denoising
95"
RELATED WORK,0.22758620689655173,"steps on latent representations. At each denoising step, the U-Net produces the noise residual to
96"
RELATED WORK,0.22988505747126436,"compute the latent for the next step (see the top part of Figure 3). We reduce this per-step computation
97"
RELATED WORK,0.23218390804597702,"by exploiting block-level elimination and feature distillation.
98"
COMPRESSED U-NET ARCHITECTURE,0.23448275862068965,"3.1
Compressed U-Net architecture
99"
COMPRESSED U-NET ARCHITECTURE,0.2367816091954023,"The proposed models are referred to as:
100"
COMPRESSED U-NET ARCHITECTURE,0.23908045977011494,"◦BK-SDM-Base (0.76B parameters) obtained with Section 3.1.1 (fewer blocks in outer stages).
101"
COMPRESSED U-NET ARCHITECTURE,0.2413793103448276,"◦BK-SDM-Small (0.66B) with Section 3.1.1 (fewer blocks) and Section 3.1.2 (mid-stage removal).
102"
COMPRESSED U-NET ARCHITECTURE,0.24367816091954023,"Table 1: Minor impact of eliminating the mid-stage
from the U-Net of SDM on zero-shot MS-COCO per-
formance. Any retraining is not performed for the mid-
stage removed model. For evaluation details, see Sec-
tion 5.1.1."
COMPRESSED U-NET ARCHITECTURE,0.24597701149425288,"Model
Performance
# Params
FID ↓
IS ↑
U-Net
Whole
SDM-v1.4 [46]
13.05
36.76
859.5M
1032.1M
Mid-Stage
Removal
15.60
32.33
762.5M
(-11.3%)
935.1M
(-9.4%)"
COMPRESSED U-NET ARCHITECTURE,0.2482758620689655,"Figure 4: Visual results of the mid-stage
removed U-Net without retraining."
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.25057471264367814,"3.1.1
Fewer blocks in the down and up stages
103"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.25287356321839083,"Our design philosophy is closely aligned with that of DistilBERT [54] which halves the number of
104"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.25517241379310346,"layers for improved computational efficiency and initializes the compact model with the original
105"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2574712643678161,"weights by benefiting from the shared dimensionality. In the original U-Net, each stage with a
106"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2597701149425287,"common spatial size consists of multiple blocks, and most stages contain pairs of residual (R) [12]
107"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2620689655172414,"and cross-attention (A) [65, 20] blocks. We hypothesize the existence of some unnecessary pairs and
108"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.26436781609195403,"use the following removal strategies, as shown in Figure 3.
109"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.26666666666666666,"For the down stages, we maintain the first R-A pairs while eliminating the second pairs, because the
110"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2689655172413793,"first pairs process the changed spatial information and would be more important than the second pairs.
111"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.271264367816092,"This design choice does not harm the dimensionality of the original U-Net, enabling the use of the
112"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2735632183908046,"corresponding pretrained weights for initialization [54].
113"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.27586206896551724,"For the up stages, while adhering to the aforementioned scheme, we retain the third R-A pairs. This
114"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.27816091954022987,"allows us to utilize the output feature maps at the end of each down stage and the corresponding skip
115"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.28045977011494255,"connections between the down and up stages. The same process is applied to the innermost down and
116"
FEWER BLOCKS IN THE DOWN AND UP STAGES,0.2827586206896552,"up stages that contain only R blocks.
117"
REMOVAL OF THE ENTIRE MID-STAGE,0.2850574712643678,"3.1.2
Removal of the entire mid-stage
118"
REMOVAL OF THE ENTIRE MID-STAGE,0.28735632183908044,"Surprisingly, removing the entire mid-stage from the original U-Net (marked with red in Figure 3)
119"
REMOVAL OF THE ENTIRE MID-STAGE,0.2896551724137931,"does not noticeably degrade the generation quality for many text prompts while effectively reducing
120"
REMOVAL OF THE ENTIRE MID-STAGE,0.29195402298850576,"the number of parameters (see Table 1 and Figure 4). This observation is consistent with the minor
121"
REMOVAL OF THE ENTIRE MID-STAGE,0.2942528735632184,"role of inner layers in the U-Net generator of GANs [24].
122"
REMOVAL OF THE ENTIRE MID-STAGE,0.296551724137931,"Integrating the mid-stage removal with fewer blocks in Section 3.1.1 further decreases computational
123"
REMOVAL OF THE ENTIRE MID-STAGE,0.2988505747126437,"burdens (Table 3) at the cost of a slight decline in performance (Table 2). Therefore, we offer
124"
REMOVAL OF THE ENTIRE MID-STAGE,0.30114942528735633,"this mid-stage elimination as an option, depending on the priority between compute efficiency and
125"
REMOVAL OF THE ENTIRE MID-STAGE,0.30344827586206896,"generation quality.
126"
DISTILLATION-BASED PRETRAINING,0.3057471264367816,"3.2
Distillation-based pretraining
127"
DISTILLATION-BASED PRETRAINING,0.3080459770114943,"For general-purpose T2I generation, we train the compact U-Net to mimic the behavior of the original
128"
DISTILLATION-BASED PRETRAINING,0.3103448275862069,"U-Net. Following Rombach et al. [47], we use the pretrained-and-frozen encoders to obtain the inputs
129"
DISTILLATION-BASED PRETRAINING,0.31264367816091954,"of the U-Net.
130"
DISTILLATION-BASED PRETRAINING,0.31494252873563217,"Given the latent representation z of an image and its paired text embedding y, the task loss for the
131"
DISTILLATION-BASED PRETRAINING,0.31724137931034485,"reverse denoising process [18, 47] is computed as:
132"
DISTILLATION-BASED PRETRAINING,0.3195402298850575,"LTask = Ez,ϵ,y,t
h
||ϵ −ϵS(zt, y, t)||2
2
i
,
(1)"
DISTILLATION-BASED PRETRAINING,0.3218390804597701,"where ϵ∼N(0, I) and t∼Uniform(1, T) denote the noise and time step sampled from the diffusion
133"
DISTILLATION-BASED PRETRAINING,0.32413793103448274,"process, respectively, and ϵS(◦) indicates the output of our compact U-Net student. For brevity, we
134"
DISTILLATION-BASED PRETRAINING,0.3264367816091954,"omit the subscripts of Ez,ϵ,y,t[◦] in the following notations.
135"
DISTILLATION-BASED PRETRAINING,0.32873563218390806,"The compact student is also trained to imitate the outputs of the original U-Net teacher, ϵT(◦) ,with
136"
DISTILLATION-BASED PRETRAINING,0.3310344827586207,"the following output-level KD objective [16]:
137"
DISTILLATION-BASED PRETRAINING,0.3333333333333333,"LOutKD = E
h
||ϵT(zt, y, t) −ϵS(zt, y, t)||2
2
i
.
(2)"
DISTILLATION-BASED PRETRAINING,0.335632183908046,"A key to our approach is the utilization of feature-level KD [48, 13] that provides abundant guidance
138"
DISTILLATION-BASED PRETRAINING,0.33793103448275863,"for the student’s training:
139"
DISTILLATION-BASED PRETRAINING,0.34022988505747126,"LFeatKD = E
h X"
DISTILLATION-BASED PRETRAINING,0.3425287356321839,"l
||f l
T(zt, y, t) −f l
S(zt, y, t)||2
2
i
,
(3)"
DISTILLATION-BASED PRETRAINING,0.3448275862068966,"where f l
T(◦) and f l
S(◦) represent the feature maps of the l-th layer in a predefined set of distilled layers
140"
DISTILLATION-BASED PRETRAINING,0.3471264367816092,"from the teacher and the student, respectively. While learnable regressors (e.g., 1×1 convolutions
141"
DISTILLATION-BASED PRETRAINING,0.34942528735632183,"to match the number of channels) have been commonly used in existing studies [58, 45, 48], our
142"
DISTILLATION-BASED PRETRAINING,0.35172413793103446,"approach circumvents this requirement. By applying distillation at the end of each stage in both
143"
DISTILLATION-BASED PRETRAINING,0.35402298850574715,"models, we ensure that the dimensionality of the feature maps already matches, thus eliminating the
144"
DISTILLATION-BASED PRETRAINING,0.3563218390804598,"need for additional regressors.
145"
DISTILLATION-BASED PRETRAINING,0.3586206896551724,"The final objective is formalized as below, and we simply set the loss weights λOutKD and λFeatKD
146"
DISTILLATION-BASED PRETRAINING,0.36091954022988504,"as 1. Without any hyperparameter tuning, our approach is effective in empirical validation.
147"
DISTILLATION-BASED PRETRAINING,0.3632183908045977,"L = LTask + λOutKDLOutKD + λFeatKDLFeatKD.
(4)"
DISTILLATION-BASED PRETRAINING,0.36551724137931035,"3.3
Application: faster and smaller personalized SDMs
148"
DISTILLATION-BASED PRETRAINING,0.367816091954023,"To emphasize the benefit of our lightweight pretrained SDMs, we use a popular finetuning scenario
149"
DISTILLATION-BASED PRETRAINING,0.3701149425287356,"for personalized generation. DreamBooth [50] enables T2I diffusion models to create contents about
150"
DISTILLATION-BASED PRETRAINING,0.3724137931034483,"a particular subject using just a few input images. Our compact models not only accelerate inference
151"
DISTILLATION-BASED PRETRAINING,0.37471264367816093,"speed but also reduce finetuning cost. Moreover, they produce high-quality images based on the
152"
DISTILLATION-BASED PRETRAINING,0.37701149425287356,"inherited capability of the original SDM.
153"
EXPERIMENTAL SETUP,0.3793103448275862,"4
Experimental setup
154"
DATASETS AND EVALUATION METRICS,0.3816091954022989,"4.1
Datasets and evaluation metrics
155"
DATASETS AND EVALUATION METRICS,0.3839080459770115,"Pretraining. We train our compact SDM with only 0.22M image-text pairs from LAION-Aesthetics
156"
DATASETS AND EVALUATION METRICS,0.38620689655172413,"V2 6.5+ [55, 56], which are significantly fewer than the original training data used for SDM-v1.4
157"
DATASETS AND EVALUATION METRICS,0.38850574712643676,"[46] (i.e., 600M pairs of LAION-Aesthetics V2 5+ [55] for the resumed training).
158"
DATASETS AND EVALUATION METRICS,0.39080459770114945,"Zero-shot T2I evaluation. Following the popular protocol [43, 47, 51] to assess general-purpose T2I
159"
DATASETS AND EVALUATION METRICS,0.3931034482758621,"with pretrained models, we use 30K prompts from the MS-COCO validation split [29] and compare
160"
DATASETS AND EVALUATION METRICS,0.3954022988505747,"the generated images to the whole validation set. We compute Fréchet Inception Distance (FID) [15]
161"
DATASETS AND EVALUATION METRICS,0.39770114942528734,"and Inception Score (IS) [53] to assess visual quality. Moreover, we measure CLIP score [42, 14]
162"
DATASETS AND EVALUATION METRICS,0.4,"with CLIP-ViT-g/14 model to assess text-image correspondence.
163"
DATASETS AND EVALUATION METRICS,0.40229885057471265,"Finetuning for personalized generation. We use the DreamBooth dataset [50] that covers 30
164"
DATASETS AND EVALUATION METRICS,0.4045977011494253,"subjects, each of which is associated with 25 prompts and 4∼6 images. Through individual finetuning
165"
DATASETS AND EVALUATION METRICS,0.4068965517241379,"for each subject, 30 personalized models are obtained. For evaluation, we follow the protocol of Ruiz
166"
DATASETS AND EVALUATION METRICS,0.4091954022988506,"et al. [50] based on four synthesized images per subject and per prompt. We consider CLIP-I and
167"
DATASETS AND EVALUATION METRICS,0.4114942528735632,"DINO scores to measure how well subject details are maintained in generated images (i.e., subject
168"
DATASETS AND EVALUATION METRICS,0.41379310344827586,"fidelity) and CLIP-T scores to measure text-image alignment (i.e., text fidelity). We use ViT-S/16
169"
DATASETS AND EVALUATION METRICS,0.4160919540229885,"embeddings [3] for DINO scores and CLIP-ViT-g/14 embeddings for CLIP-I and CLIP-T.
170"
IMPLEMENTATION,0.41839080459770117,"4.2
Implementation
171"
IMPLEMENTATION,0.4206896551724138,"We use the released version v1.4 of SDM [46] as our compression target. We remark that our approach
172"
IMPLEMENTATION,0.42298850574712643,"is also applicable to other versions in v1.1–v1.5 with the same architecture and to SDM-v2 with a
173"
IMPLEMENTATION,0.42528735632183906,"similarly designed architecture.
174"
IMPLEMENTATION,0.42758620689655175,"Table 2: Zero-shot results on 30K prompts from MS-COCO validation set [29] at 256×256 resolution.
Despite being trained with a smaller dataset and having fewer parameters, our compressed models
achieve results on par with prior approaches for general-purpose T2I. For our models, the results with
the minimum FID and the final 50K-th iteration are reported (see Section 5.1.3 for detailed analysis)."
IMPLEMENTATION,0.4298850574712644,"Model
Type
FID ↓
IS ↑
# Params
Data Size
SDM-v1.4 [47]
DF
13.05
36.76
1.04B
600M
Small Stable Diffusion [40]
DF
12.76
32.33
0.76B
229M
BK-SDM-Base (Ours) @ Min FID
DF
13.57
29.22
0.76B
0.22M
BK-SDM-Base (Ours) @ Final Iter
DF
15.76
33.79
0.76B
0.22M
BK-SDM-Small (Ours) @ Min FID
DF
15.93
29.61
0.66B
0.22M
BK-SDM-Small (Ours) @ Final Iter
DF
16.98
31.68
0.66B
0.22M
DALL·E†⋆[43]
AR
27.5
17.9
12B
250M
CogView‡⋆[7]
AR
27.1
18.2
4B
30M
CogView2†⋆[8]
AR
24.0
22.4
6B
30M
Make-A-Scene‡ [10]
AR
11.84
-
4B
35M
LAFITE‡♯[74]
GAN
26.94
26.02
0.23B
3M
GALIP (CC3M)† [62]
GAN
16.12
-
0.32B
3M
GALIP (CC12M)† [62]
GAN
12.54
-
0.32B
12M
GLIDE‡ [38]
DF
12.24
-
5B
250M
LDM-KL-8-G‡♯[47]
DF
12.63
30.29
1.45B
400M
DALL·E-2† [44]
DF
10.39
-
5.2B
250M"
IMPLEMENTATION,0.432183908045977,"† and ‡: FID from [62] and [47], respectively. ⋆and ♯: IS from [8] and [47], respectively. DF and AR:
diffusion and autoregressive models. ↓and ↑: lower and higher values are better."
IMPLEMENTATION,0.43448275862068964,"We adjust the codes in Diffusers library [66] for pretraining our models and those in PEFT library
175"
IMPLEMENTATION,0.4367816091954023,"[60] for DreamBooth-finetuning, both of which adopt the training process of DDPM [18] in latent
176"
IMPLEMENTATION,0.43908045977011495,"spaces. We use a single NVIDIA A100 80G GPU for 50K-iteration pretraining with a constant
177"
IMPLEMENTATION,0.4413793103448276,"learning rate of 5e-5. For DreamBooth, we use a single NVIDIA GeForce RTX 3090 GPU to finetune
178"
IMPLEMENTATION,0.4436781609195402,"each personalized model for 800 iterations with a constant learning rate of 1e-6.
179"
IMPLEMENTATION,0.4459770114942529,"Following the default inference setup, we use PNDM scheduler [30] for zero-shot T2I generation
180"
IMPLEMENTATION,0.4482758620689655,"and DPM-Solver [32, 33] for DreamBooth results. For compute efficiency, we always opt for 25
181"
IMPLEMENTATION,0.45057471264367815,"denoising steps of the U-Net at the inference phase. The classifier-free guidance scale [17, 51] is set
182"
IMPLEMENTATION,0.4528735632183908,"to the default value of 7.5, except the analysis in Figure 7.
183"
RESULTS,0.45517241379310347,"5
Results
184"
RESULTS,0.4574712643678161,"5.1
General-purpose T2I generation
185"
MAIN RESULTS,0.45977011494252873,"5.1.1
Main results
186"
MAIN RESULTS,0.46206896551724136,"Table 2 shows the zero-shot T2I results on 30K samples from the MS-COCO 256×256 validation
187"
MAIN RESULTS,0.46436781609195404,"set. Despite being trained with only 0.22M samples and having fewer than 1B parameters, our
188"
MAIN RESULTS,0.4666666666666667,"compressed models demonstrate competitive performance on par with previous large pretrained
189"
MAIN RESULTS,0.4689655172413793,"models. Despite the absence of a paper support, we include the model [40] that is identical in
190"
MAIN RESULTS,0.47126436781609193,"structure to BK-SDM-Base for comparison. This model benefits from far more training resources,
191"
MAIN RESULTS,0.4735632183908046,"i.e., two-stage KD relied on two teachers (SDM-v1.4 and v1.5) and a much larger volume of data
192"
MAIN RESULTS,0.47586206896551725,"with significantly longer iterations.
193"
MAIN RESULTS,0.4781609195402299,"Figure 5 depicts synthesized images of different models with some MS-COCO captions. Our
194"
MAIN RESULTS,0.4804597701149425,"compressed models inherit the superior ability of SDM and produce more photorealistic images
195"
MAIN RESULTS,0.4827586206896552,"compared to the AR-based [8] and GAN-based [74, 62] baselines. Noticeably, the same latent code
196"
MAIN RESULTS,0.4850574712643678,"results in a shared visual style between the original and our compact SDMs (4th–6th columns in
197"
MAIN RESULTS,0.48735632183908045,"Figure 5), similar to the observation in transfer learning for GANs [36].
198"
MAIN RESULTS,0.4896551724137931,"Table 3 summarizes how the computational reduction for each sampling step of the U-Net impacts the
199"
MAIN RESULTS,0.49195402298850577,"overall compute of the entire SDM. The per-step reduction effectively decreases MACs and inference
200"
MAIN RESULTS,0.4942528735632184,"time by more than 30% as well as the number of parameters.
201"
MAIN RESULTS,0.496551724137931,"Figure 5: Visual comparison on zero-shot MS-COCO benchmark. The results of previous studies
[8, 74, 62] were obtained with their official codes and released models. We do not apply any CLIP-
based reranking for SDM and our models."
MAIN RESULTS,0.49885057471264366,"Table 3: The impact of per-step compute reduction of the U-Net on the entire SDM. The number of
sampling steps is indicated with the parentheses, e.g., U-Net (1) for one step. The full computation
(denoted by “Whole”) covers the text encoder, U-Net, and image decoder. All corresponding values
are obtained on the generation of a single 512×512 image with 25 denoising steps. The latency was
measured on Xeon Silver 4210R CPU 2.40GHz and NVIDIA GeForce RTX 3090 GPU."
MAIN RESULTS,0.5011494252873563,"# Params
MACs
CPU Latency
GPU Latency
Model
U-Net
Whole
U-Net (1)
U-Net (25)
Whole
U-Net (1)
U-Net (25)
Whole
U-Net (1)
U-Net (25)
Whole
SDM-v1.4 [46]
860M
1033M
339G
8469G
9716G
5.63s
146.28s
153.02s
0.049s
1.28s
1.41s
BK-SDM-
Base (Ours)"
M,0.503448275862069,"580M
(-32.6%)
752M
(-27.1%)"
G,0.5057471264367817,"224G
(-33.9%)
5594G
(-33.9%)
6841G
(-29.5%)"
S,0.5080459770114942,"3.84s
(-31.8%)
99.95s
(-31.7%)
106.62s
(-30.3%)"
S,0.5103448275862069,"0.032s
(-34.6%)
0.83s
(-35.2%)
0.96s
(-31.9%)
BK-SDM-
Small (Ours)"
M,0.5126436781609195,"483M
(-43.9%)
655M
(-36.5%)"
G,0.5149425287356322,"218G
(-35.7%)
5444G
(-35.7%)
6690G
(-31.1%)"
S,0.5172413793103449,"3.45s
(-38.7%)
89.78s
(-38.6%)
96.52s
(-36.9%)"
S,0.5195402298850574,"0.030s
(-38.7%)
0.77s
(-39.8%)
0.90s
(-36.1%)"
ABLATION STUDY,0.5218390804597701,"5.1.2
Ablation study
202"
ABLATION STUDY,0.5241379310344828,"Table 4 presents the ablation study with the zero-shot MS-COCO benchmark dataset. The common
203"
ABLATION STUDY,0.5264367816091954,"default settings for the models N1–N7 involve the usage of fewer blocks in the down and up stages
204"
ABLATION STUDY,0.5287356321839081,"(Section 3.1.1) and the denoising task loss (Eq. 1). All the models are drawn at the 50K-th training
205"
ABLATION STUDY,0.5310344827586206,"iteration. We made the following observations.
206"
ABLATION STUDY,0.5333333333333333,"N1 vs. N2. Importing the pretrained weights for initialization clearly improves the performance of
207"
ABLATION STUDY,0.535632183908046,"block-removed SDMs. Transferring knowledge from well-trained models, a popularized practice in
208"
ABLATION STUDY,0.5379310344827586,"machine learning, is also beneficial for T2I generation with SDMs.
209"
ABLATION STUDY,0.5402298850574713,"N2 vs. N3 vs. N4. Exploiting output-level KD (Eq. 2) effectively boosts the generation quality
210"
ABLATION STUDY,0.542528735632184,"compared to using only the denoising task loss. Leveraging feature-level KD (Eq. 3) further improves
211"
ABLATION STUDY,0.5448275862068965,"the performance by offering sufficient guidance over multiple stages in the student.
212"
ABLATION STUDY,0.5471264367816092,"N4 vs. N5. An increased batch size leads to a better IS and CLIP score but with a minor drop in FID.
213"
ABLATION STUDY,0.5494252873563218,"We opt for a batch size of 256 based on the premise that more samples per batch would enhance the
214"
ABLATION STUDY,0.5517241379310345,"model’s understanding ability.
215"
ABLATION STUDY,0.5540229885057472,"N6 and N7. Despite slight performance drop, the models N6 and N7 with the mid-stage removal
216"
ABLATION STUDY,0.5563218390804597,"have fewer parameters (0.66B) than N4 and N5 (0.76B), offering improved compute efficiency.
217"
ABLATION STUDY,0.5586206896551724,"Table 4: Ablation study on zero-shot MS-COCO 256×256 30K. The common settings include fewer
blocks in the down and up stages and the denoising task loss. N5 and N7 correspond to BK-SDM-
Base and BK-SDM-Small, respectively"
ABLATION STUDY,0.5609195402298851,"Model
Performance"
ABLATION STUDY,0.5632183908045977,"No.
Initialize
Weights"
ABLATION STUDY,0.5655172413793104,"Output
KD
Feature
KD"
ABLATION STUDY,0.5678160919540229,"Batch
Size"
ABLATION STUDY,0.5701149425287356,"Remove
Mid
FID ↓
IS ↑
CLIP
score ↑
N1
Random
✗
✗
64
✗
43.80
13.61
0.1622
N2
Pretrained
✗
✗
64
✗
20.45
22.68
0.2444
N3
Pretrained
✓
✗
64
✗
16.48
27.30
0.2620
N4
Pretrained
✓
✓
64
✗
14.61
31.44
0.2826
N5
Pretrained
✓
✓
256
✗
15.76
33.79
0.2878
N6
Pretrained
✓
✓
64
✓
16.87
29.51
0.2644
N7
Pretrained
✓
✓
256
✓
16.98
31.68
0.2677
Original SDM-v1.4 [46, 47]
13.05
36.76
0.2958"
ABLATION STUDY,0.5724137931034483,"Figure 6: Results on zero-shot MS-COCO 256×256 30K over
training progress. For our models, the architecture size, usage
of KD, and batch size are denoted."
ABLATION STUDY,0.5747126436781609,"Figure 7:
Effect of different
classifier-free guidance scales on
MS-COCO 512×512 5K."
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5770114942528736,"5.1.3
Impact of distillation on pretraining phase
218"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5793103448275863,"We further analyze the merits of transferred knowledge via distillation, with the models from the
219"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5816091954022988,"pretrained weight initialization. Figure 6 shows zero-shot T2I performance over training iterations.
220"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5839080459770115,"Compared to the absence of KD (indicated with green), distillation (purple and pink) accelerates the
221"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5862068965517241,"training process and leads to improved generation scores, demonstrating the benefits of providing
222"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5885057471264368,"sufficient hints for training guidance. Notably, our small-size model trained with KD (yellow)
223"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5908045977011495,"outperforms the bigger base-size model without KD (green). Additionally, while the best FID score
224"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.593103448275862,"is observed early on for our models, IS and CLIP score exhibit ongoing improvement, implying that
225"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5954022988505747,"judging models solely with FID may be suboptimal.
226"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.5977011494252874,"Figure 7 shows the trade-off curves from different classifier-free guidance scales [17, 51]
227"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6,"{2.0, 2.5, 3.0, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5}. For the analysis, we use 5K samples from the MS-
228"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6022988505747127,"COCO validation set and our base-size models from the 50K-th iteration. Higher guidance scales
229"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6045977011494252,"lead to better text-aligned images at the cost of less diversity. Compared to the baseline trained only
230"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6068965517241379,"with the denoising task loss, distillation-based pretraining leads to much better trade-off curves.
231"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6091954022988506,"5.2
Personalized T2I with DreamBooth
232"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6114942528735632,"Table 5 compares the results of DreamBooth finetuning [50] with different pretrained models. BK-
233"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6137931034482759,"SDM-Small can preserve over 97% performance of the original SDM with the reduced finetuning
234"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6160919540229886,"time and number of parameters. Figure 8 depicts that our models can accurately capture the subject
235"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6183908045977011,"details and generate various scenes. Over the models pretrained with a batch size of 64, we observe
236"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6206896551724138,"the impact of KD pretraining on personalized synthesis. The baselines without KD fail to generate
237"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6229885057471264,"the subjects entirely or cannot maintain the identity details.
238"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6252873563218391,"Table 5: Personalized generation with finetuning over different pretrained models. Our compact
models can preserve subject fidelity (DINO and CLIP-I) and prompt fidelity (CLIP-T) of the original
SDM with reduced finetuning (FT) time and fewer parameters."
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6275862068965518,"Pretrained Model
DINO ↑
CLIP-I ↑
CLIP-T ↑
FT Time†
# Params
SDM-v1.4 [46, 47]
0.728
0.725
0.263
881.3s
1.04B
BK-SDM-Base (Ours)
0.723
0.717
0.260
622.3s
0.76B
BK-SDM-Small (Ours)
0.720
0.705
0.259
603.6s
0.66B
BK-SDM-Base, Batch Size 64
0.718
0.708
0.262
622.3s
0.76B
- Without KD & Random Init.
0.594
0.465
0.191
622.3s
0.76B
- Without KD & Pretrained Init.
0.716
0.669
0.258
622.3s
0.76B"
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.6298850574712643,† Per-subject finetuning time for 800 iterations on NVIDIA GeForce RTX 3090 GPU.
IMPACT OF DISTILLATION ON PRETRAINING PHASE,0.632183908045977,"Figure 8: Visual results of personalized generation. Each subject is marked as “a [identifier] [class
noun]” (e.g., “a [V] dog""). Similar to the original SDM, our compact models can synthesize the
images of input subjects in different backgrounds while preserving their appearance."
CONCLUSION AND DISCUSSION,0.6344827586206897,"6
Conclusion and discussion
239"
CONCLUSION AND DISCUSSION,0.6367816091954023,"This study uncovers the potential of architectural compression for general-purpose text-to-image
240"
CONCLUSION AND DISCUSSION,0.639080459770115,"synthesis with a renowned model, Stable Diffusion. Our block-removed lightweight models are
241"
CONCLUSION AND DISCUSSION,0.6413793103448275,"effective for zero-shot generation, achieving competitive performance against large-scale baselines.
242"
CONCLUSION AND DISCUSSION,0.6436781609195402,"Distillation is a key aspect of our method, leading to effective pretraining even under very constrained
243"
CONCLUSION AND DISCUSSION,0.6459770114942529,"resources. Moreover, our smaller and faster pretrained models are successfully applied in personalized
244"
CONCLUSION AND DISCUSSION,0.6482758620689655,"generation. Our work is orthogonal to previous directions for efficient diffusion models, e.g., enabling
245"
CONCLUSION AND DISCUSSION,0.6505747126436782,"fewer sampling steps, and can be readily combined with them. We hope our study can facilitate future
246"
CONCLUSION AND DISCUSSION,0.6528735632183909,"research on structural compression of large diffusion models.
247"
CONCLUSION AND DISCUSSION,0.6551724137931034,"Limitations and future works. Our compact models inherit the capability of the source model for
248"
CONCLUSION AND DISCUSSION,0.6574712643678161,"high-fidelity image generation, but they have shortcomings such as inaccurate generation of full-body
249"
CONCLUSION AND DISCUSSION,0.6597701149425287,"human appearance. While we show that distillation pretraining is powerful even with very limited
250"
CONCLUSION AND DISCUSSION,0.6620689655172414,"resources, increasing the volume of data and analyzing its effects would be promising.
251"
CONCLUSION AND DISCUSSION,0.664367816091954,"Negative social impacts. Because recent large generative models are capable of creating high-quality
252"
CONCLUSION AND DISCUSSION,0.6666666666666666,"plausible content, they also involve potential risks of malicious use. To avoid causing unintended
253"
CONCLUSION AND DISCUSSION,0.6689655172413793,"social bias, researchers should take steps to ensure the appropriateness of training data. Moreover,
254"
CONCLUSION AND DISCUSSION,0.671264367816092,"the release of resulting models should be accompanied by strong and reliable safeguards.
255"
REFERENCES,0.6735632183908046,"References
256"
REFERENCES,0.6758620689655173,"[1] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents:
257"
REFERENCES,0.6781609195402298,"High-resolution video synthesis with latent diffusion models. In CVPR, 2023.
258"
REFERENCES,0.6804597701149425,"[2] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions.
259"
REFERENCES,0.6827586206896552,"In CVPR, 2023.
260"
REFERENCES,0.6850574712643678,"[3] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in
261"
REFERENCES,0.6873563218390805,"self-supervised vision transformers. In ICCV, 2021.
262"
REFERENCES,0.6896551724137931,"[4] Y.-H. Chen, R. Sarokin, J. Lee, J. Tang, C.-L. Chang, A. Kulik, and M. Grundmann. Speed is all you need:
263"
REFERENCES,0.6919540229885057,"On-device acceleration of large diffusion models via gpu-aware optimizations. In CVPR Workshop, 2023.
264"
REFERENCES,0.6942528735632184,"[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
265"
REFERENCES,0.696551724137931,"for language understanding. In NAACL, 2019.
266"
REFERENCES,0.6988505747126437,"[6] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021.
267"
REFERENCES,0.7011494252873564,"[7] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, et al. Cogview:
268"
REFERENCES,0.7034482758620689,"Mastering text-to-image generation via transformers. In NeurIPS, 2021.
269"
REFERENCES,0.7057471264367816,"[8] M. Ding, W. Zheng, W. Hong, and J. Tang. Cogview2: Faster and better text-to-image generation via
270"
REFERENCES,0.7080459770114943,"hierarchical transformers. In NeurIPS, 2022.
271"
REFERENCES,0.7103448275862069,"[9] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In CVPR,
272"
REFERENCES,0.7126436781609196,"2021.
273"
REFERENCES,0.7149425287356321,"[10] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-based
274"
REFERENCES,0.7172413793103448,"text-to-image generation with human priors. In ECCV, 2022.
275"
REFERENCES,0.7195402298850575,"[11] Z. Hao, J. Guo, D. Jia, K. Han, Y. Tang, C. Zhang, H. Hu, and Y. Wang. Learning efficient vision
276"
REFERENCES,0.7218390804597701,"transformers via fine-grained manifold distillation. In NeurIPS, 2022.
277"
REFERENCES,0.7241379310344828,"[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
278"
REFERENCES,0.7264367816091954,"[13] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y. Choi. A comprehensive overhaul of feature distillation.
279"
REFERENCES,0.728735632183908,"In ICCV, 2019.
280"
REFERENCES,0.7310344827586207,"[14] J. Hessel, A. Holtzman, M. Forbes, R. Le Bras, and Y. Choi. CLIPScore: A reference-free evaluation
281"
REFERENCES,0.7333333333333333,"metric for image captioning. In EMNLP, 2021.
282"
REFERENCES,0.735632183908046,"[15] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale
283"
REFERENCES,0.7379310344827587,"update rule converge to a local nash equilibrium. In NeurIPS, 2017.
284"
REFERENCES,0.7402298850574712,"[16] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NeurIPS Workshop,
285"
REFERENCES,0.7425287356321839,"2014.
286"
REFERENCES,0.7448275862068966,"[17] J. Ho and T. Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021.
287"
REFERENCES,0.7471264367816092,"[18] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.
288"
REFERENCES,0.7494252873563219,"[19] J. Hou and Z. Asghar. World’s first on-device demonstration of stable diffusion on an android phone.
289"
REFERENCES,0.7517241379310344,"https://www.qualcomm.com/news, 2023.
290"
REFERENCES,0.7540229885057471,"[20] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception
291"
REFERENCES,0.7563218390804598,"with iterative attention. In ICML, 2021.
292"
REFERENCES,0.7586206896551724,"[21] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. Tinybert: Distilling bert for
293"
REFERENCES,0.7609195402298851,"natural language understanding. In Findings of EMNLP, 2020.
294"
REFERENCES,0.7632183908045977,"[22] Q. Jin, J. Ren, O. J. Woodford, J. Wang, G. Yuan, Y. Wang, and S. Tulyakov. Teachers do more than teach:
295"
REFERENCES,0.7655172413793103,"Compressing image-to-image models. In CVPR, 2021.
296"
REFERENCES,0.767816091954023,"[23] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani. Imagic: Text-based real
297"
REFERENCES,0.7701149425287356,"image editing with diffusion models. In CVPR, 2023.
298"
REFERENCES,0.7724137931034483,"[24] B.-K. Kim, S. Choi, and H. Park. Cut inner layers: A structured pruning strategy for efficient u-net gans.
299"
REFERENCES,0.774712643678161,"In ICML Workshop, 2022.
300"
REFERENCES,0.7770114942528735,"[25] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-concept customization of text-to-image
301"
REFERENCES,0.7793103448275862,"diffusion. In CVPR, 2023.
302"
REFERENCES,0.7816091954022989,"[26] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. In ICLR,
303"
REFERENCES,0.7839080459770115,"2017.
304"
REFERENCES,0.7862068965517242,"[27] M. Li, J. Lin, Y. Ding, Z. Liu, J.-Y. Zhu, and S. Han. Gan compression: Efficient architectures for
305"
REFERENCES,0.7885057471264367,"interactive conditional gans. In CVPR, 2020.
306"
REFERENCES,0.7908045977011494,"[28] X. Li, L. Lian, Y. Liu, H. Yang, Z. Dong, D. Kang, S. Zhang, and K. Keutzer. Q-diffusion: Quantizing
307"
REFERENCES,0.7931034482758621,"diffusion models. arXiv preprint arXiv:2302.04304, 2023.
308"
REFERENCES,0.7954022988505747,"[29] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft
309"
REFERENCES,0.7977011494252874,"coco: Common objects in context. In ECCV, 2014.
310"
REFERENCES,0.8,"[30] L. Liu, Y. Ren, Z. Lin, and Z. Zhao. Pseudo numerical methods for diffusion models on manifolds. In
311"
REFERENCES,0.8022988505747126,"ICLR, 2022.
312"
REFERENCES,0.8045977011494253,"[31] Y. Liu, Z. Shu, Y. Li, Z. Lin, F. Perazzi, and S.-Y. Kung. Content-aware gan compression. In CVPR, 2021.
313"
REFERENCES,0.8068965517241379,"[32] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic
314"
REFERENCES,0.8091954022988506,"model sampling in around 10 steps. In NeurIPS, 2022.
315"
REFERENCES,0.8114942528735632,"[33] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver++: Fast solver for guided sampling of
316"
REFERENCES,0.8137931034482758,"diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.
317"
REFERENCES,0.8160919540229885,"[34] C. Meng, R. Gao, D. P. Kingma, S. Ermon, J. Ho, and T. Salimans. On distillation of guided diffusion
318"
REFERENCES,0.8183908045977012,"models. In NeurIPS Workshop, 2022.
319"
REFERENCES,0.8206896551724138,"[35] C. Meng, R. Gao, D. P. Kingma, S. Ermon, J. Ho, and T. Salimans. On distillation of guided diffusion
320"
REFERENCES,0.8229885057471265,"models. In CVPR, 2023.
321"
REFERENCES,0.825287356321839,"[36] S. Mo, M. Cho, and J. Shin. Freeze the discriminator: a simple baseline for fine-tuning gans. In CVPR
322"
REFERENCES,0.8275862068965517,"Workshop, 2020.
323"
REFERENCES,0.8298850574712644,"[37] C. Mou, X. Wang, L. Xie, J. Zhang, Z. Qi, Y. Shan, and X. Qie. T2i-adapter: Learning adapters to dig out
324"
REFERENCES,0.832183908045977,"more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.
325"
REFERENCES,0.8344827586206897,"[38] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide:
326"
REFERENCES,0.8367816091954023,"Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2022.
327"
REFERENCES,0.8390804597701149,"[39] W. Park, D. Kim, Y. Lu, and M. Cho. Relational knowledge distillation. In CVPR, 2019.
328"
REFERENCES,0.8413793103448276,"[40] J.
Pinkney.
Small
stable
diffusion.
https://huggingface.co/OFA-Sys/
329"
REFERENCES,0.8436781609195402,"small-stable-diffusion-v0, 2023.
330"
REFERENCES,0.8459770114942529,"[41] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
331"
REFERENCES,0.8482758620689655,"multitask learners. OpenAI blog, 2019.
332"
REFERENCES,0.8505747126436781,"[42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
333"
REFERENCES,0.8528735632183908,"J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.
334"
REFERENCES,0.8551724137931035,"[43] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot
335"
REFERENCES,0.8574712643678161,"text-to-image generation. In ICML, 2020.
336"
REFERENCES,0.8597701149425288,"[44] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation
337"
REFERENCES,0.8620689655172413,"with clip latents. arXiv preprint arXiv:2204.06125, 2022.
338"
REFERENCES,0.864367816091954,"[45] Y. Ren, J. Wu, X. Xiao, and J. Yang. Online multi-granularity distillation for gan compression. In ICCV,
339"
REFERENCES,0.8666666666666667,"2021.
340"
REFERENCES,0.8689655172413793,"[46] R. Rombach and P. Esser.
Stable diffusion v1-4.
https://huggingface.co/CompVis/
341"
REFERENCES,0.871264367816092,"stable-diffusion-v1-4, 2022.
342"
REFERENCES,0.8735632183908046,"[47] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with
343"
REFERENCES,0.8758620689655172,"latent diffusion models. In CVPR, 2022.
344"
REFERENCES,0.8781609195402299,"[48] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep
345"
REFERENCES,0.8804597701149425,"nets. In ICLR, 2015.
346"
REFERENCES,0.8827586206896552,"[49] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation.
347"
REFERENCES,0.8850574712643678,"In MICCAI, 2015.
348"
REFERENCES,0.8873563218390804,"[50] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning
349"
REFERENCES,0.8896551724137931,"text-to-image diffusion models for subject-driven generation. In CVPR, 2023.
350"
REFERENCES,0.8919540229885058,"[51] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,
351"
REFERENCES,0.8942528735632184,"B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language
352"
REFERENCES,0.896551724137931,"understanding. In NeurIPS, 2022.
353"
REFERENCES,0.8988505747126436,"[52] T. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022.
354"
REFERENCES,0.9011494252873563,"[53] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for
355"
REFERENCES,0.903448275862069,"training gans. In NeurIPS, 2016.
356"
REFERENCES,0.9057471264367816,"[54] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper
357"
REFERENCES,0.9080459770114943,"and lighter. In NeurIPS Workshop, 2019.
358"
REFERENCES,0.9103448275862069,"[55] C. Schuhmann and R. Beaumont. Laion-aesthetics. https://laion.ai/blog/laion-aesthetics,
359"
REFERENCES,0.9126436781609195,"2022.
360"
REFERENCES,0.9149425287356322,"[56] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta,
361"
REFERENCES,0.9172413793103448,"C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation
362"
REFERENCES,0.9195402298850575,"image-text models. In NeurIPS Workshop, 2022.
363"
REFERENCES,0.9218390804597701,"[57] H. Shen, P. Cheng, X. Ye, W. Cheng, and H. Abidi. Accelerate stable diffusion with intel neural compressor.
364"
REFERENCES,0.9241379310344827,"https://medium.com/intel-analytics-software, 2022.
365"
REFERENCES,0.9264367816091954,"[58] C. Shu, Y. Liu, J. Gao, Z. Yan, and C. Shen. Channel-wise knowledge distillation for dense prediction. In
366"
REFERENCES,0.9287356321839081,"ICCV, 2021.
367"
REFERENCES,0.9310344827586207,"[59] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In ICLR, 2021.
368"
REFERENCES,0.9333333333333333,"[60] L. D. Y. B. S. P. Sourab Mangrulkar, Sylvain Gugger. Peft: State-of-the-art parameter-efficient fine-tuning
369"
REFERENCES,0.9356321839080459,"methods. https://github.com/huggingface/peft, 2022.
370"
REFERENCES,0.9379310344827586,"[61] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou. Mobilebert: a compact task-agnostic bert for
371"
REFERENCES,0.9402298850574713,"resource-limited devices. In ACL, 2020.
372"
REFERENCES,0.9425287356321839,"[62] M. Tao, B.-K. Bao, H. Tang, and C. Xu. Galip: Generative adversarial clips for text-to-image synthesis. In
373"
REFERENCES,0.9448275862068966,"CVPR, 2023.
374"
REFERENCES,0.9471264367816092,"[63] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou. Training data-efficient image
375"
REFERENCES,0.9494252873563218,"transformers amp; distillation through attention. In ICML, 2021.
376"
REFERENCES,0.9517241379310345,"[64] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.
377"
REFERENCES,0.9540229885057471,"[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
378"
REFERENCES,0.9563218390804598,"Attention is all you need. In NeurIPS, 2017.
379"
REFERENCES,0.9586206896551724,"[66] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, and T. Wolf. Diffusers:
380"
REFERENCES,0.960919540229885,"State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.
381"
REFERENCES,0.9632183908045977,"[67] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d
382"
REFERENCES,0.9655172413793104,"diffusion models for 3d generation. In CVPR, 2023.
383"
REFERENCES,0.967816091954023,"[68] J. Z. Wu, Y. Ge, X. Wang, W. Lei, Y. Gu, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot
384"
REFERENCES,0.9701149425287356,"tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022.
385"
REFERENCES,0.9724137931034482,"[69] Z. Xie, L. Zhu, L. Zhao, B. Tao, L. Liu, and W. Tao. Localization-aware channel pruning for object
386"
REFERENCES,0.9747126436781609,"detection. Neurocomputing, 403:400–408, 2020.
387"
REFERENCES,0.9770114942528736,"[70] S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of
388"
REFERENCES,0.9793103448275862,"convolutional neural networks via attention transfer. In ICLR, 2017.
389"
REFERENCES,0.9816091954022989,"[71] L. Zhang and M. Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint
390"
REFERENCES,0.9839080459770115,"arXiv:2302.05543, 2023.
391"
REFERENCES,0.9862068965517241,"[72] L. Zhang, X. Chen, X. Tu, P. Wan, N. Xu, and K. Ma. Wavelet knowledge distillation: Towards efficient
392"
REFERENCES,0.9885057471264368,"image-to-image translation. In CVPR, 2022.
393"
REFERENCES,0.9908045977011494,"[73] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. In ICLR, 2023.
394"
REFERENCES,0.993103448275862,"[74] Y. Zhou, R. Zhang, C. Chen, C. Li, C. Tensmeyer, T. Yu, J. Gu, J. Xu, and T. Sun. Towards language-free
395"
REFERENCES,0.9954022988505747,"training for text-to-image generation. In CVPR, 2022.
396"
REFERENCES,0.9977011494252873,"[75] L. Zhu. Thop: Pytorch-opcounter. https://github.com/Lyken17/pytorch-OpCounter, 2018.
397"
