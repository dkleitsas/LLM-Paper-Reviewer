Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0028089887640449437,"Deep graph neural networks (GNNs) have gained increasing popularity, while
usually suffer from unaffordable computations for real-world large-scale appli-
cations. Hence, pruning GNNs is of great need but largely unexplored. A re-
cent work (Chen et al., 2021) studies lottery ticket learning for GNNs, aiming
to ﬁnd a subset of model parameters and graph structure that can best maintain
the GNN performance. However, it is tailed for the transductive setting, failing
to generalize to unseen graphs, which are common in inductive tasks like graph
classiﬁcation. In this work, we propose a simple and effective learning paradigm,
Inductive Co-Pruning of GNNs (ICPG), to endow graph lottery tickets with in-
ductive pruning capacity. To prune the input graphs, we design a predictive model
to predict importance scores for each edge based on the input; to prune the model
parameters, it views the weight’s magnitude as their importance scores. Then we
design an iterative co-pruning strategy to trim the graph edges and GNN weights
based on their importance scores. Although it might be strikingly simple, ICPG
surpasses the existing pruning method and can be universally applicable in both
inductive and transductive learning settings. On ten graph-classiﬁcation and two
node-classiﬁcation benchmarks, ICPG achieves the same performance level with
14.26%∼43.12% sparsity for graphs and 48.80%∼91.41% sparsity for the model."
INTRODUCTION,0.0056179775280898875,"1
INTRODUCTION"
INTRODUCTION,0.008426966292134831,"Graph neural networks (GNNs) (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017; Zhou et al., 2018)
have become a prevalent solution for machine learning tasks on graph-structured data. Such success
is usually ascribed to the powerful representation learning of GNN, which incorporates the graph
structure into the representations, such as aggregating neural messages from the neighboring nodes
to update the ego node’s representation (Veliˇckovi´c et al., 2017; Kipf & Welling, 2016)."
INTRODUCTION,0.011235955056179775,"As the ﬁeld grows, there is an increasing need of building deeper GNN architectures (Li et al.,
2020a; 2021) on larger-scale graphs (Hu et al., 2020). While deepening GNNs shows potentials on
large-scale graphs, it also brings expensive computations due to the increased scale of graph data
and model parameters, limiting their deployment in resource-constrained applications. Taking fraud
detection in a transaction network as an example, the scale of user nodes easily reaches millions or
even larger, making a GNN-detector model prohibitive to stack deep layers and predict the malicious
behaviors in real time. Hence, pruning over-parameterized GNNs is of great need, which aims to
answer the question: Can we co-sparsify the input graphs and model parameters, while preserving
or even improving the performance?"
INTRODUCTION,0.014044943820224719,"Recently, the pruning approach for GNNs, UGS, (Chen et al., 2021), is proposed to ﬁnd graph lottery
tickets (GLTs) — smaller subsets of model parameters and input graphs. At its core is Lottery Ticket
Hypothesis (LTH) (Frankle & Carbin, 2018) speculating that any dense, randomly-initialized neural
network contains a sparse subnetwork, which can be trained independently to achieve a matching
performance as the dense network. Speciﬁcally, UGS employs trainable masks on each edge in the
input graph and each weight in the model parameters, to specify their importance. When training the
model with the masks, the strategy of iterative magnitude-based pruning (IMP) (Frankle & Carbin,
2018) is used to discard the edges and weights with the lowest mask values at each iteration."
INTRODUCTION,0.016853932584269662,"Despite effectiveness, there exist the following limitations: (1) UGS focuses solely on providing
the transductive graph masks by generating a painstakingly customized mask for a single edge"
INTRODUCTION,0.019662921348314606,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02247191011235955,"individually and independently. That is, the edge masks are limited to the given graph, making
UGS infeasible to be applied in the inductive setting, since the edge masks hardly generalize to
unseen edges or entirely new graphs. (2) Applying a mask for each edge alone only provides a local
understanding of the edge, rather than the global view of the entire graph (e.g., in node classiﬁcation)
or multiple graphs (e.g., in graph classiﬁcation). Moreover, the way of creating trainable edge masks
will double the parameters of GNNs, which violates the purpose of pruning somehow. As a result,
these edge masks could be suboptimal to guide the pruning. (3) The unsatisfactory graph pruning
will negatively inﬂuence the pruning of model weights. Worse still, low-quality weight pruning will
amplify the misleading signal of edge masks in turn. They inﬂuence each other and form a vicious
circle. We ascribe all these limitations of UGS to its transductive nature. Hence, conducting the
combinatorial pruning in the inductive setting is crucial to high-quality winning tickets."
INTRODUCTION,0.025280898876404494,"In this work, we emphasize the inductive nature within the combinatorial pruning of input graphs and
GNN parameters and present our framework, Inductive Co-Pruning of GNNs (ICPG). It is an ex-
tremely simple but effective pruning framework that is applicable to any GNN in both inductive and
transductive settings. Speciﬁcally, for the input graphs, we design a generative probabilistic model
(termed AutoMasker), which learns to generate edge masks from the observed graphs. It is param-
eterized with an additional GNN-based encoder, whose parameters are shared across the population
of observed graphs. As a consequence, AutoMasker is naturally capable to specify the signiﬁcance
for each edge and extract core-subgraphs from a global view of the entire observations. For the
model parameters, we simply exploit the magnitude of a model weight to assess whether it should
be pruned, rather than training an additional mask (Chen et al., 2021). Having established the edge
masks and weight magnitudes, we can obtain high-quality GLTs by pruning the lowest-mask edges
and lowest-magnitude weights. Experiments on ten graph-classiﬁcation and two node-classiﬁcation
benchmarks consistently validate our framework ICPG by identifying high-quality GLTs. The visu-
alizations show that ICPG always retains decisive subgraphs, such as edges located on digital pixels
in MNIST graphs, which further illustrates the effectiveness and rationality. Moreover, we inspect
the graph- and GNN-level transferability of GLTs, which promises for deploying our ICPG in the
pre-training and ﬁne-tuning paradigm. Our main contributions can be summarized as follows:"
INTRODUCTION,0.028089887640449437,"• We proposed a simple but effective pruning framework, ICPG, to prune the GNN model and input
graphs simultaneously. It can identify high-quality GLTs in diverse tasks of graph representation
learning, under both inductive and transductive settings.
• For graph classiﬁcation tasks, ICPG can locate GLTs from small-scale (TUDataset), medium-
scale (Superpixel graphs) to challenging large-scale datasets (OGB) with graph sparsity from
22.62%∼43.12% and GNN sparsity from 67.23%∼91.41% with no degradation on performance.
• For node classiﬁcation tasks, ICPG can effectively develop on both transductive learning (Cora
dataset) and inductive learning (PPI dataset), which identiﬁes the GLTs with graph sparsity from
22.62%∼26.49% and GNN sparsity from 67.23%∼73.79% without sacriﬁcing performance.
• The proposed AutoMasker promises for both GNN-level and graph-level transferability, which
can achieve comparable or even better performance as compared with the original full graphs.
In-depth analyses with visual inspections further demonstrate the effectiveness and rationality."
RELATED WORK,0.03089887640449438,"2
RELATED WORK"
RELATED WORK,0.033707865168539325,"Graph Neural Networks (GNNs) (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017; Xu et al., 2019;
Ying et al., 2018) have emerged as a powerful tool for learning the representation of graph-structured
data. The great success mainly comes from the structure-aware learning, which follows the iterative
message-passing scheme (Veliˇckovi´c et al., 2017). Speciﬁcally, we denote an undirected graph by
G = (A, X) with the node set V and edge set E. A ∈{0, 1}|V|×|V| is the adjacency matrix, where
A[i, j] = 1 denotes the edge between node vi and node vj, otherwise A[i, j] = 0. X ∈R|V|×d
is the matrix of node features, where xi = X[i, :] is the d-dimensional feature of the node vi ∈V.
Given a K-layer GNN, its k-th layer generates the representation of node vi as:"
RELATED WORK,0.03651685393258427,"a(k)
i
= AGGREGATION(k)({h(k−1)
j
|j ∈N(i)}), h(k)
i
= COMBINE(k)(h(k−1)
i
, a(k)
i
),
(1)"
RELATED WORK,0.03932584269662921,"where h(k)
i
and a(k)
i
are the representation of node vi and the message aggregated from its neighbor
nodes set N(i), respectively; the AGGREGATION and COMBINE operators are the message"
RELATED WORK,0.042134831460674156,Under review as a conference paper at ICLR 2022
RELATED WORK,0.0449438202247191,"passing and update functions, respectively. After propagating through K layers, we get the ﬁnal
representations of nodes, which facilitate downstream node-level tasks, such as node classiﬁcation
and link prediction. As for graph-level tasks like graph classiﬁcation and graph matching, we further
hire the READOUT function to generate the representation of the whole graph G:"
RELATED WORK,0.047752808988764044,"ZG = READOUT({h(k)
i
|vi ∈V, k ∈{1, · · · , K}}).
(2)"
RELATED WORK,0.05056179775280899,"Various GNNs, such as GIN (Xu et al., 2019) and GAT (Veliˇckovi´c et al., 2017), implement different
AGGREGATION, COMBINE and READOUT functions, so as to reﬁne the desired information
from graph structures."
RELATED WORK,0.05337078651685393,"Graph Sparsiﬁcation or Sampling (Voudigari et al., 2016; Leskovec & Faloutsos, 2006) aims to
ﬁnd small core-subgraphs from the original graph, which can remain effective for graph learning
tasks. Numerous strategies (Zeng et al., 2019; Franceschi et al., 2019; Ying et al., 2019; Hamil-
ton et al., 2017; Chen et al., 2018) are proposed to achieve efﬁcient graph learning. For example,
GraphSAGE (Hamilton et al., 2017) samples and aggregates feature from a node’s local neighbor-
hood. FastGCN (Chen et al., 2018) adopts the global importance sampling, which is more efﬁcient
for training. DropEdge (Rong et al., 2019) randomly drops edges from the input graph, which can
be seen as a data augmenter. Another research line selects the core-subgraph in an optimization
way. SGCN (Li et al., 2020b) adopts the ADMM optimization algorithm to sparsify the adjacency
matrix. UGS (Chen et al., 2021) utilizes a trainable mask for each edge to remove the potential
task-irrelevant edges. Distinct from them, our AutoMasker predicts the importance score of each
edge from a global view, thus having better generalization ability in the inductive settings."
RELATED WORK,0.056179775280898875,"Lottery Ticket Hypothesis (LTH) (Frankle & Carbin, 2018) states that a sparse subnetwork exists in
a dense randomly-initialized network that can be trained to achieve comparable performance to the
full models. LTH is explored in many ﬁelds such as computer vision and natural language processing
(Chen et al., 2020; Ma et al., 2021; Yin et al., 2020; Liu et al., 2019; Wang et al., 2020; Savarese et al.,
2020). Recently, Chen et al. (2021) extends the LTH to the GNNs and proposes the Graph Lottery
Ticket (GLT), which includes subgraph and subnetwork pairs that can be trained independently
to reach comparable performance to the dense pairs. However, due to the transductive nature of
graph-speciﬁc masks, UGS (Chen et al., 2021) can not develop on inductive learning settings, such
as graph classiﬁcation tasks. To tackle the dilemma, we propose AutoMasker to globally learn the
signiﬁcance of each edge from training graphs and predict importance scores for new coming graphs,
which is graph-independent and inductive."
METHODOLOGY,0.05898876404494382,"3
METHODOLOGY"
METHODOLOGY,0.06179775280898876,"Here we ﬁrst formulate the task of learning graph lottery tickets, and then present our inductive
strategy of co-pruning the input graphs and model weights."
INDUCTIVE GRAPH LOTTERY TICKET,0.06460674157303371,"3.1
INDUCTIVE GRAPH LOTTERY TICKET"
INDUCTIVE GRAPH LOTTERY TICKET,0.06741573033707865,"Without loss of generality, we consider the inductive task of graph classiﬁcation as an example.
Given a GNN classiﬁer f(·, Θg0), it starts from the randomly-initialized parameters Θg0 before
training and arrives at the well-optimized parameters Θg after training. Once trained, it takes any
graph G = (A, X) as the input and yields a probability distribution over C classes ˆy = f(G, Θg).
Wherein, G is associated with the adjacency matrix A and the pre-existing node features X."
INDUCTIVE GRAPH LOTTERY TICKET,0.0702247191011236,"The goal of learning graph lottery tickets is to make the input graph G and the model weights Θg0
sparse to reduce the computational costs, while preserving the classiﬁcation performance. Formally,
it aims to generate two masks mG and mΘ, which are applied on G and Θg0 correspondingly, so
as to establish the sparser input graph G′ = (mG ⊙A, X) and initialized weights Θ′
g0 = mΘ ⊙
Θg0. Hereafter, through retraining the subnetwork f(·, Θ′
g0) on the sparse versions {G′} of training
graphs, we can get the new converged parameters Θ′
g. If the well-optimized subnetwork can achieve
comparable performance with full graphs and network, we term the pair of G′ and f(·, Θ′
g0) as graph
lottery tickets (GLTs)."
INDUCTIVE GRAPH LOTTERY TICKET,0.07303370786516854,"Although a very recent study, UGS (Chen et al., 2021), has proposed an approach to learn the GLTs,
it focuses solely on the transductive setting but leaves the inductive setting untouched. Speciﬁcally,"
INDUCTIVE GRAPH LOTTERY TICKET,0.07584269662921349,Under review as a conference paper at ICLR 2022
INDUCTIVE GRAPH LOTTERY TICKET,0.07865168539325842,"it assigns a trainable mask to each edge of the input graph and trains such graph-speciﬁc masks
individually and independently. As a consequence, these edge-dependent masks are limited to the
given graph, hardly generalizing to unseen edges or entirely new graphs. Distinct from UGS, we
aim to uncover GLTs in inductive learning settings."
AUTOMASKER,0.08146067415730338,"3.2
AUTOMASKER"
AUTOMASKER,0.08426966292134831,"Instead of assigning mask to single edge, our idea is extremely simple: we take a collection of graph
instances and design a trainable model to learn to mask edges collectively. The key ingredient to-
wards this model is an additional GNN-based model, termed AutoMasker, whose parameters are
shared across the population of observed graphs. Here we represent AutoMasker as the combina-
tion of a graph encoder and a subsequent scoring function. Formally, given a graph G = (A, X),
AutoMasker applies a GNN-based graph encoder g(·) to create representations of all nodes as:"
AUTOMASKER,0.08707865168539326,"H = g(A, X),
(3)"
AUTOMASKER,0.0898876404494382,"where H ∈R|V×d| stores d-dimension representations of all nodes, whose i-th row hi represents the
representation of node vi; g(·) is a GNN following the message-passing paradigm in Equation 1. To
assess the importance score of edge (i, j) between node vi and node vj, AutoMasker builds a multi-
layer perceptron (MLP) upon the concatenation of node representations hi and hj, which yields the
score αij. In what follows, the sigmoid function σ(·) projects αij into the range of (0, 1), which
represents the probability of edge (i, j) being the winning ticket. The scoring function is represented
as follows:"
AUTOMASKER,0.09269662921348315,"sij = σ(αij),
αij = MLP([hi, hj]).
(4)"
AUTOMASKER,0.09550561797752809,"By employing the scoring function over all possible edges, we are able to collect the matrix of
edge masks sG, where sG[i, j] = sij if edge (i, j) holds, otherwise sG[i, j] = 0. In a nutshell, we
summarize the AutoMasker function as follows:"
AUTOMASKER,0.09831460674157304,"sG = AutoMasker(G, Θa),
(5)"
AUTOMASKER,0.10112359550561797,"where Θa is the trainable parameters of AutoMasker, covering the parameters of the GNN encoder
and the MLP."
AUTOMASKER,0.10393258426966293,"Although the key ingredient of AutoMasker is simple, it has several conceptual advantages over
UGS: (1) Global view: Although edge masks derived from UGS might preserve the ﬁdelity to local
importance, they do not help to delineate the general picture of the whole graph population. Distinct
from UGS, our AutoMasker takes a global view of the graph population, which enables us to identify
the edge coalitions. Speciﬁcally, as edges usually collaborate with each other to make predictions,
rather than working individually, they form a coalition like the functional groups of a molecule
graph, the community of a social network. Considering such coalition effects, AutoMasker is able to
measure the importance of edges more accurately. (2) Lightweight edge masks: When using UGS
to prune graph data with millions of edges or nodes, the cost of assigning local edge masks one-
by-one will be prohibitive with such a large-scale dataset in real-world scenarios. Moreover, UGS
introduces additional parameters, whose scale remains the same as the edge numbers P
G |E| and
is much larger than the original parameters being pruned. Hence, it somehow violates the purpose
of pruning. In our AutoMasker, the additional parameters are i.e., Θa in Equation 5 only and
remain invariant across the change of data scale. (3) Generalization: AutoMasker can generalize
the mechanism of mask generation to new graphs without retraining, making it more efﬁcient to
prune unseen and large-scale graphs."
INDUCTIVE CO-PRUNING STRATEGY,0.10674157303370786,"3.3
INDUCTIVE CO-PRUNING STRATEGY"
INDUCTIVE CO-PRUNING STRATEGY,0.10955056179775281,"Here we present the framework of Inductive Co-Pruning of GNNs (ICPG) to localize GLTs induc-
tively. Figure 1 demonstrates its overview, which consists of the following two steps:"
INDUCTIVE CO-PRUNING STRATEGY,0.11235955056179775,"• Step 1: Co-training AutoMasker and the GNN model of interest. Given an input graph G =
(A, X), AutoMasker ﬁrst generates the edge mask sG via Equation 5. Then we apply sG to
the adjacency matrix A to create the masked graph Gs = (sG ⊙A, X), which fully reﬂects
AutoMasker’s decision for the importance of each edge, such that less important edges are prone
to have lower mask values. Finally, we feed the masked graphs into the GNN model to co-train the"
INDUCTIVE CO-PRUNING STRATEGY,0.1151685393258427,Under review as a conference paper at ICLR 2022
INDUCTIVE CO-PRUNING STRATEGY,0.11797752808988764,AutoMasker
INDUCTIVE CO-PRUNING STRATEGY,0.12078651685393259,"Magnitude
Pruning 0.15 0.95 0.61 0.23 0.38 0.26"
INDUCTIVE CO-PRUNING STRATEGY,0.12359550561797752,"0.13
0.69 0.68"
INDUCTIVE CO-PRUNING STRATEGY,0.12640449438202248,"0.91
0.37
0.56 0.18 0.25 0.18 0.52 0.77 0.18 0.67"
INDUCTIVE CO-PRUNING STRATEGY,0.12921348314606743,"0.63
0.45 0.74 0.63"
INDUCTIVE CO-PRUNING STRATEGY,0.13202247191011235,Training Graph(s)
INDUCTIVE CO-PRUNING STRATEGY,0.1348314606741573,"Rating
Scores"
INDUCTIVE CO-PRUNING STRATEGY,0.13764044943820225,AutoMasker
INDUCTIVE CO-PRUNING STRATEGY,0.1404494382022472,"Mask
Pruning"
INDUCTIVE CO-PRUNING STRATEGY,0.14325842696629212,Training/Testing Graph(s)
INDUCTIVE CO-PRUNING STRATEGY,0.14606741573033707,"Rating
Scores"
INDUCTIVE CO-PRUNING STRATEGY,0.14887640449438203,"GNN
Masked Graph(s)"
INDUCTIVE CO-PRUNING STRATEGY,0.15168539325842698,"Masked Graph(s)
Sparse GNN
Sparse Graph(s)"
INDUCTIVE CO-PRUNING STRATEGY,0.1544943820224719,Step1: Co-training the AutoMasker and GNN model.
INDUCTIVE CO-PRUNING STRATEGY,0.15730337078651685,Step2: Co-sparsifying the input graphs and GNN model. 0.88 0.11 0.97 0.44 0.96
INDUCTIVE CO-PRUNING STRATEGY,0.1601123595505618,"0.86
0.38 0.93"
INDUCTIVE CO-PRUNING STRATEGY,0.16292134831460675,"0.21
0.72
0.19 0.91 0.88 0.07 0.69 0.42 0.89 0.32"
INDUCTIVE CO-PRUNING STRATEGY,0.16573033707865167,"0.12
0.84 0.23 0.24 0.58"
INDUCTIVE CO-PRUNING STRATEGY,0.16853932584269662,Figure 1: The Inductive Co-Pruning of GNNs (ICPG) framework to ﬁnd the GLTs.
INDUCTIVE CO-PRUNING STRATEGY,0.17134831460674158,"AutoMasker and the model. The GNN model adopts the masked graph to learn the representation
and make predictions, which can be viewed as the supervision signals to guide the AutoMasker
to achieve a more accurate decision. The detailed co-training process is shown in Algorithm 1 of
Appendix A1.1. When the training is done, we conduct Step 2 to perform the pruning."
INDUCTIVE CO-PRUNING STRATEGY,0.17415730337078653,"• Step 2: Co-sparsifying the input graphs and GNN model. Having obtained the well-trained
AutoMasker and GNN model, we can apply the knowledge learned from numerous graphs to co-
sparsify the graphs and the model. For graphs, we adopt AutoMasker to predict the importance
of all the edges for each graph. Then the edges of a certain graph are sorted based on the mask
values, and the edges with 5% the lowest values are pruned to obtain the binary graph mask mG.
For GNN, we sort the parameters based on the weight magnitude and prune 20% the lowest-
magnitude parameters to obtain the binary model mask mΘ. Under the current sparsity level, we
now successfully obtain the sparsiﬁed graph G′ = (mG ⊙A, X) and the sparsiﬁed mask mΘ for
the model. Finally, we need to check whether the sparsity meets our condition. If the sparsity is
satisﬁed, the algorithm is completed; if not, we need to reuse the found GLT to update the original
graphs and GNN model, and iteratively use Step 1 and Step 2 (dotted arrow in Figure 1) until the
condition is met. In Appendix A1.1, Algorithm 2 offers the detailed algorithm of ICPG."
EXPERIMENTS,0.17696629213483145,"4
EXPERIMENTS"
EXPERIMENTS,0.1797752808988764,"In this section, we conduct extensive experiments on diverse benchmarks to validate the effective-
ness of the ICPG. We ﬁrst introduce the experimental settings in Section 4.1, and explore the exis-
tence of GLTs in graph classiﬁcation and node classiﬁcation in Sections 4.2 and 4.3, respectively.
We also validate the transferability of the AutoMasker in Section 4.4. More ablation studies and
visualizations are provided in Sections 4.5 and 4.6, respectively."
EXPERIMENTAL SETTINGS,0.18258426966292135,"4.1
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.1853932584269663,"Datasets. For graph classiﬁcation, we use two biological graphs (NCI1, MUTAG), four social
graphs (COLLAB, RED-B, RED-M5K, RED-M12K) (Morris et al., 2020), two superpixel graphs
(MNIST, CIFAR-10) (Knyazev et al., 2019), and two large-scale Open Graph Benchmark (ogbg-
ppa and ogbg-code2) (Hu et al., 2020). For node classiﬁcation, we choose the transductive learning
dataset: Cora, and the inductive learning dataset: PPI. More details are provided in Appendix A1."
EXPERIMENTAL SETTINGS,0.18820224719101122,"Models and Training Details. We adopt the same architecture for the GNN model and GNN
encoder for AutoMasker. For all graph classiﬁcation datasets and Cora dataset, we adopt the GCN
(Kipf & Welling, 2016) model with different layers and hiddens. For the PPI dataset, we choose the
GAT (Veliˇckovi´c et al., 2017) network to achieve a better baseline performance as work (Veliˇckovi´c
et al., 2017). More details about models and training are provided in Appendix A1.4 and A1.5."
EXPERIMENTAL SETTINGS,0.19101123595505617,Under review as a conference paper at ICLR 2022 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98
EXPERIMENTAL SETTINGS,0.19382022471910113,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
EXPERIMENTAL SETTINGS,0.19662921348314608,Graph Sparsity (%) 50 60 70 80
EXPERIMENTAL SETTINGS,0.199438202247191,Accuracy (%) NCI1
EXPERIMENTAL SETTINGS,0.20224719101123595,"RP
ICPG (Ours)
Baseline
RP-GLT (5.00%)
ICPG-GLT (26.49%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
EXPERIMENTAL SETTINGS,0.2050561797752809,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
EXPERIMENTAL SETTINGS,0.20786516853932585,Graph Sparsity (%) 60 70 80 90
MUTAG,0.21067415730337077,"100
MUTAG"
MUTAG,0.21348314606741572,"RP
ICPG (Ours)
Baseline
RP-GLT (18.55%)
ICPG-GLT (30.17%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.21629213483146068,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.21910112359550563,Graph Sparsity (%) 60 70 80
MUTAG,0.22191011235955055,COLLAB
MUTAG,0.2247191011235955,"RP
ICPG (Ours)
Baseline
RP-GLT (5.00%)
ICPG-GLT (33.66%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.22752808988764045,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.2303370786516854,Graph Sparsity (%) 60 80 RED-B
MUTAG,0.23314606741573032,"RP
ICPG (Ours)
Baseline
RP-GLT (9.75%)
ICPG-GLT (22.62%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.23595505617977527,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.23876404494382023,Graph Sparsity (%) 20 30 40 50 60
MUTAG,0.24157303370786518,Accuracy (%)
MUTAG,0.2443820224719101,RED-M5K
MUTAG,0.24719101123595505,"RP
ICPG (Ours)
Baseline
RP-GLT (14.26%)
ICPG-GLT (40.13%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.25,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.25280898876404495,Graph Sparsity (%) 20 30 40 50
MUTAG,0.2556179775280899,RED-M12K
MUTAG,0.25842696629213485,"RP
ICPG (Ours)
Baseline
RP-GLT (9.75%)
ICPG-GLT (51.23%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.2612359550561798,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.2640449438202247,Graph Sparsity (%) 20 40 60 80 MNIST
MUTAG,0.26685393258426965,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (43.12%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98"
MUTAG,0.2696629213483146,"40.13
43.12
45.96
48.67
51.23
53.67
55.99
58.19
60.28
62.26"
MUTAG,0.27247191011235955,Graph Sparsity (%) 20 30 40 50
MUTAG,0.2752808988764045,CIFAR-10
MUTAG,0.27808988764044945,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (14.26%)"
MUTAG,0.2808988764044944,Figure 2: Graph classiﬁcation performance on achieved graph sparsity.
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.28370786516853935,"4.2
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.28651685393258425,"We ﬁrst conduct experiments to ﬁnd the GLTs in graph classiﬁcation tasks. The results are displayed
in Figure 2 and Figure 3. We also plot the random pruning (RP) for better comparison. Stars denote
the extreme sparsity, which is the maximal sparsity-level without performance degradation. More
results about GNNs sparsity are provided in Appendix A2.1. We make the following Observations:"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.2893258426966292,"Obs.1. GLTs extensively exist in graph classiﬁcation tasks. Utilizing the ICPG, we can success-
fully locate the GLTs with different sparsity-levels from different types of graphs. For NCI1 and
MUTAG, we precisely identify GLTs with the extreme graph sparsity at 26.49% and 30.17%, GNN
sparsity at 73.79% and 79.03%, respectively. On four social network datasets, we ﬁnd the GLTs
with graph sparsity of 22.62%∼51.23% and GNN sparsity-level in 67.23%∼95.60%. For MNIST
and CIFAR-10, the GLTs are achieved with graphs sparsity of 43.13% and 14.26%, GNN sparsity
of 91.41% and 48.80%. These results show that ICPG can locate the high-quality GLTs in graph
classiﬁcation tasks with different graph types, and demonstrate the potential of efﬁcient training or
inference with sparser graphs and lightweight GNNs without sacriﬁcing performance."
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.29213483146067415,"Obs.2. AutoMasker has good generalization ability. The mainstream graph sparsiﬁcation tech-
niques (Chen et al., 2021; Zheng et al., 2020; Li et al., 2020b) cannot inductively prune unseen
graphs. However, the AutoMasker can ﬂexibly overcome this challenge. Compared with random
pruning (RP), our proposed ICPG can ﬁnd more sparse subgraphs and subnetworks and keep a large
gap with RP. For instance, the RED-M5K and RED-M12K graphs pruned by ICPG can achieve
40.13% and 51.23% extreme graph sparsity, improving 25.87% and 41.48% compared with RP,
which keeps an extremely large superiority. The excellent results indicate that AutoMasker can pre-
cisely capture more signiﬁcant core-patterns from the training graphs and have a good generalization
ability to predict the high-quality masks for unseen graphs. 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.2949438202247191,"51.23
53.67
55.99
58.19
60.28
62.26
64.15"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.29775280898876405,Graph Sparsity (%) 20 40 60
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.300561797752809,Accuracy (%)
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.30337078651685395,ogbg-ppa
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3061797752808989,"RP
ICPG (Ours)
Baseline
RP-GLT (5.00%)
ICPG-GLT (14.26%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3089887640449438,"48.67
51.23
53.67
55.99
58.19
60.28
62.26
64.15"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.31179775280898875,Graph Sparsity (%) 6 8 10 12 14
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3146067415730337,F1 score (%)
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.31741573033707865,ogbg-code2
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3202247191011236,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (18.55%)"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.32303370786516855,"Figure 3: Graph classiﬁcation performance over achieved
graph sparsity on large-scale datasets."
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3258426966292135,"Obs.3. The extreme sparsity of GLTs
depends on the property of graphs.
Although ICPG achieves higher spar-
sity than RP on most graphs, the im-
provements are not obvious on a small
part of the graphs, such as biochemical
molecule graphs: NCI1 or MUTAG. We
make the following conjectures: Firstly,
most of the edges in these graphs are
important, such as a certain edge may
correspond to a crucial chemical bond,
which may drastically affect the chemi-
cal properties of the molecule if pruned.
Secondly, the graph size is relatively small, which just includes a few dozen nodes and edges, so it
is more sensitive to pruning. On the contrary, the larger social network datasets, such as RED-M5K"
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.32865168539325845,Under review as a conference paper at ICLR 2022
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.33146067415730335,"or RED-M12K, contain hundreds or thousands of edges for each graph, so there may be plenty of
redundant edges that demonstrate the insensitivity to pruning."
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION,0.3342696629213483,"Obs.4. AutoMasker can well tackle larger-size and larger-quantity graphs. Figure 3 demon-
strates the results on the challenging OGB datasets, which are consist of larger-size graphs (2266.1
edges and 243.4 nodes on average per graph for ogbg-ppa) and larger-quantity graphs (452,741
graphs for ogbg-code2). We surprisingly ﬁnd the OGB datasets are so intractable that RP can only
locate 5% graph sparsity of GLT on the ogbg-ppa, and it is even impossible to ﬁnd any sparser GLTs
on the ogbg-code2. Despite this, the proposed ICPG can locate the GLTs with 14.26% and 18.55%
graph sparsity, 48.80% and 59.40% GNN sparsity on ogbg-ppa and ogbg-code2, respectively. The
superior performance further veriﬁes the strong scalability and generalization of the AutoMasker."
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.33707865168539325,"4.3
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.3398876404494382,"Since ICPG can achieve excellent performance on diverse types and scales of graphs, we also want
to explore that if it can also tackle node-level tasks. To answer this question, we conduct experiments
on Core and PPI datasets, which are commonly used in transductive and inductive node classiﬁcation
tasks. We also reproduce the recent work UGS (Chen et al., 2021) for Cora (cannot apply for
inductive setting) for better comparison. We can get the following Observations: 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.34269662921348315,"51.23
53.67
55.99
58.19
60.28
62.26
64.15"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.3455056179775281,Graph Sparsity (%) 40 60 80
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.34831460674157305,Accuracy (%) Cora
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.351123595505618,"RP
UGS
ICPG (Ours)
Baseline
RP-GLT (9.75%)
UGS-GLT (18.55%)
ICPG-GLT (26.49%) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.3539325842696629,"51.23
53.67
55.99
58.19
60.28
62.26
64.15"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.35674157303370785,Graph Sparsity (%) 60 70 80 90 100 PPI
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.3595505617977528,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (22.62%)"
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.36235955056179775,"Figure 4: Transductive and inductive node classiﬁcation
performance on achieved graph sparsity."
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION,0.3651685393258427,"Obs.5.
ICPG can achieve excellent
performance on node classiﬁcation
tasks. Firstly, for Cora, both ICPG and
UGS can ﬁnd GLTs that are sparser than
RP, which demonstrates that both are
applicable to transductive node classiﬁ-
cation. Secondly, ICPG can ﬁnd sparser
GLTs than UGS (↑7.94%), while the
performance drop faster than UGS in the
later stage, we give the following con-
jectures: (1) UGS just adopts simple
trainable masks for edges without con-
sidering the global topological structure
of the entire graph, while AutoMasker is constructed based on a GNN-encoder, which can provide
a global understanding of the whole graph. Hence, the AutoMasker can predict more high-quality
masks than UGS. (2) ICPG is worse than UGS in the later stage. The reason is that AutoMasker has
found the most signiﬁcant edges in advance, which consists of an extremely compact core-subgraph
at a certain critical point, so further pruning over that point will seriously degrade the performance.
Thirdly, as for the PPI dataset, the performance of ICPG still keeps a large gap with RP and can
achieve 22.62% graph sparsity and 67.23% GNN sparsity without sacriﬁcing performance, which
further demonstrates the effectiveness of the ICPG on inductive learning."
THE TRANSFERABILITY OF THE AUTOMASKER,0.36797752808988765,"4.4
THE TRANSFERABILITY OF THE AUTOMASKER"
THE TRANSFERABILITY OF THE AUTOMASKER,0.3707865168539326,"We consider two orthogonal perspectives to verify the transferability of the AutoMasker: GNN-level
transferability and graph-level transferability. From GNNs view, we transfer the sparse graphs found
by AutoMasker to the other two popular GNN models: GIN (Xu et al., 2019) and GAT (Veliˇckovi´c
et al., 2017). From graphs view, we ﬁrst pre-train the AutoMasker on larger-scale RED-M12K
graphs and then transfer the well-trained AutoMasker to other two smaller-scale graphs: RED-B
and RED-M5K. Please notes that we keep the GNN models unpruned on transferred tasks. The
experimental results are provided in Figure 5 and Table 1. We make the following Observations:"
THE TRANSFERABILITY OF THE AUTOMASKER,0.37359550561797755,"Obs.6. AutoMasker has both GNN-level and graph-level transferability. For GNN-level, we can
observe from Figure 5 that GIN and GAT can achieve ranging 9.75%∼45.96% and 18.55%∼22.62%
extreme sparsity on NCI1 and RED-M12K without sacriﬁcing performance. And AutoMasker also
outperforms RP and keeps a large gap. These results demonstrate that AutoMasker can effectively
extract the GNN-independent subgraphs. These sparse subgraphs represent signiﬁcant semantic in-
formation and can be universally transferred to any GNN architecture without performance degrada-
tion. For graph-level, the performance of random pruning decreases as the graph sparsity increases.
For RED-B and RED-M5K, when the pruning sparsity increases from 0 to 55.99%, the performance"
THE TRANSFERABILITY OF THE AUTOMASKER,0.37640449438202245,Under review as a conference paper at ICLR 2022 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23 53.67
THE TRANSFERABILITY OF THE AUTOMASKER,0.3792134831460674,"55.99
58.19
60.28
62.26
64.15"
THE TRANSFERABILITY OF THE AUTOMASKER,0.38202247191011235,Graph Sparsity (%) 70 75 80 85
THE TRANSFERABILITY OF THE AUTOMASKER,0.3848314606741573,Accuracy (%)
THE TRANSFERABILITY OF THE AUTOMASKER,0.38764044943820225,NCI1 (GIN)
THE TRANSFERABILITY OF THE AUTOMASKER,0.3904494382022472,"RP
AutoMasker (Ours)
Baseline 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23 53.67"
THE TRANSFERABILITY OF THE AUTOMASKER,0.39325842696629215,"55.99
58.19
60.28
62.26
64.15"
THE TRANSFERABILITY OF THE AUTOMASKER,0.3960674157303371,Graph Sparsity (%) 70 75 80
THE TRANSFERABILITY OF THE AUTOMASKER,0.398876404494382,NCI1 (GAT) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23 53.67
THE TRANSFERABILITY OF THE AUTOMASKER,0.40168539325842695,"55.99
58.19
60.28
62.26
64.15"
THE TRANSFERABILITY OF THE AUTOMASKER,0.4044943820224719,Graph Sparsity (%) 44 46 48 50 52
THE TRANSFERABILITY OF THE AUTOMASKER,0.40730337078651685,RED-M12K (GIN) 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23 53.67
THE TRANSFERABILITY OF THE AUTOMASKER,0.4101123595505618,"55.99
58.19
60.28
62.26
64.15"
THE TRANSFERABILITY OF THE AUTOMASKER,0.41292134831460675,Graph Sparsity (%) 44 46 48 50
THE TRANSFERABILITY OF THE AUTOMASKER,0.4157303370786517,RED-M12K (GAT)
THE TRANSFERABILITY OF THE AUTOMASKER,0.41853932584269665,Figure 5: Performance of diverse GNNs on the sparse graphs found by AutoMasker.
THE TRANSFERABILITY OF THE AUTOMASKER,0.42134831460674155,Table 1: Graph-level transferability performance. From RED-M12K to RED-B and RED-M5K
THE TRANSFERABILITY OF THE AUTOMASKER,0.4241573033707865,"Settings
Graph Sparsity"
THE TRANSFERABILITY OF THE AUTOMASKER,0.42696629213483145,"Dataset
Method
0% (No pruning)
9.75 %
18.55 %
33.66 %
45.96 %
55.99 %"
THE TRANSFERABILITY OF THE AUTOMASKER,0.4297752808988764,"RED-B
Random Pruning
92.15±1.59
90.60±1.22
89.75±1.75
86.75±2.41
85.15±3.92
85.34±1.67
AutoMasker
92.15±1.59
92.16±2.06
91.05±2.14
90.15±1.89
90.06±2.57
89.64±1.72"
THE TRANSFERABILITY OF THE AUTOMASKER,0.43258426966292135,"RED-M5K
Random Pruning
56.63±0.93
56.33±1.59
55.85±1.08
54.81±2.17
54.19±2.27
54.95±1.79
AutoMasker
56.63±0.93
56.89±2.16
56.69±2.59
57.01±3.90
56.97±2.86
56.09±4.44"
THE TRANSFERABILITY OF THE AUTOMASKER,0.4353932584269663,"decreases by 7.39% and 2.97%, respectively. While AutoMasker can achieve consistent improve-
ment within all sparsity levels. Furthermore, the GNN model trained with more sparse graphs even
outperforms the GNN trained with the original dense graphs, such as RED-B at 9.75% and RED-
M5K at 9.75%∼45.96%. It demonstrates that the pre-trained AutoMasker can transfer the learned
knowledge to small-scale downstream tasks, with lower computational cost and better performance.
In summary, AutoMasker can learn model-independent, general and signiﬁcant sparse subgraph
structures from the graphs, so that it has outstanding GNN-level and graph-level transferability."
ABLATION STUDY,0.43820224719101125,"4.5
ABLATION STUDY 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23"
ABLATION STUDY,0.4410112359550562,"53.67
55.99
58.19
60.28
62.26
64.15"
ABLATION STUDY,0.4438202247191011,Graph Sparsity (%) 20 30 40 50 60
ABLATION STUDY,0.44662921348314605,Accuracy (%)
ABLATION STUDY,0.449438202247191,RED-M5K
ABLATION STUDY,0.45224719101123595,"GCN
GIN
GAT"
ABLATION STUDY,0.4550561797752809,"MLP
RP
Baseline 0.00 5.00 9.75 14.26 18.55 22.62 26.49 30.17 33.66 36.98 40.13 43.12 45.96 48.67 51.23"
ABLATION STUDY,0.45786516853932585,"53.67
55.99
58.19
60.28
62.26
64.15"
ABLATION STUDY,0.4606741573033708,Graph Sparsity (%) 60 70 80 90 100 PPI
ABLATION STUDY,0.46348314606741575,"RPGM
RPG-PM
RPG
RPM"
ABLATION STUDY,0.46629213483146065,"ICPG
PG
PM
Baseline"
ABLATION STUDY,0.4691011235955056,"Figure 6: (Left): The performance of ICPG over achieved
graph sparsity with AutoMasker based on diverse encoders.
(Right): The comparison of different components in ICPG."
ABLATION STUDY,0.47191011235955055,"In this section, we investigate the
diverse encoder networks in Au-
toMasker and explore the two inde-
pendent components in ICPG. We
can make the following ﬁndings:"
ABLATION STUDY,0.4747191011235955,"Encoder networks. AutoMasker is
designed on a GNN-based encoder,
which leads to a global understanding
of each edge from the entire graphs.
So we extensively investigate the im-
pact of the diverse encoders, such as
GNN-based or MLP-based encoders.
We can observe the results from Fig-
ure 6 (Left) that, for all the GNN-based encoders, AutoMasker can achieve good performance:
45.96% extreme sparsity for GCN and 51.23% for GIN and GAT, while MLP-based encoder only
achieves 33.66% extreme sparsity. It indicates that the message passing scheme of the GNN encoder
naturally considers the graph structure from a global view, while the MLP-based encoder does not."
ABLATION STUDY,0.47752808988764045,"Co-sparsiﬁcation. To further study the effectiveness of each component in ICPG: mask-based prun-
ing for graphs (PG) and magnitude-based pruning for model (PM), we separate them and explore
their roles when applying on the graphs and the model independently. We also plot the performance
of random pruning only for graphs (RPG), only for models (RPM), both of all (RPGM), random
pruning for graphs with magnitude-based pruning for model (RPG-PM) for comparison. The results
are summarized in Figure 6 (Right). We can ﬁnd that: PG can also ﬁnd the matching subgraphs
(9.75% sparsity), which indicates that apply AutoMasker separately can also extract the signiﬁcant
edges from each graph. PM can also locate the matching subnetworks at 14.26% sparsity, which is
consistent with the LTH (Frankle & Carbin, 2018) in the computer vision ﬁeld. ICPG signiﬁcantly
outperforms RPGM and RPG-PM, and the gap gradually widens as the sparsity increases. We also
observe that ICPG is even better than PG (↑12.87%), we made the following conjectures: (1) As for"
ABLATION STUDY,0.4803370786516854,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.48314606741573035,"Original Image Original Graph
RP Graph
ICPG Graph"
ABLATION STUDY,0.4859550561797753,(a) MNIST
ABLATION STUDY,0.4887640449438202,"Original Image Original Graph
RP Graph
ICPG Graph"
ABLATION STUDY,0.49157303370786515,"(b) CIFAR-10
Figure 7: Visualization of the subgraphs extracted by AutoMasker from MNIST (a) and CIFAR-10
(b) superpixel graphs. Original images and graphs are displayed on the ﬁrst and second columns in
(a) and (b), respectively. The sparsity of RP and ICPG in (a) and (b) is 64.15%."
ABLATION STUDY,0.4943820224719101,"PG, with the sparsity gradually increasing, the graphs also become more simple. If we still train the
over-parameterized GNN model with simple graphs, it may cause over-ﬁtting. (2) Slightly pruning
the over-parameterized GNN through PM can be regarded as a kind of regularization, which will
improve the performance, and it is consistent with LTH (Frankle & Carbin, 2018). Further, the
regularized GNN can additionally provide AutoMasker with more precise supervision signals from
the gradient in backpropagation to make more wise decisions. To summary, we should co-train the
AutoMasker and GNN and co-sparsify the input graphs and model to achieve better performance."
VISUALIZATION AND ANALYSIS,0.49719101123595505,"4.6
VISUALIZATION AND ANALYSIS"
VISUALIZATION AND ANALYSIS,0.5,"To better illustrate the signiﬁcant subgraphs extracted by the AutoMasker, we visualize the matching
subgraphs in GLTs found by the ICPG. We select graphs with 64.15% sparsity level from the MNIST
and CIFAR-10 superpixel datasets. For better comparison, we also plot the original images, original
graphs, random pruning graphs, which are depicted in Figure 7. More wonderful visualizations are
provided in Figure A11 and Figure A12. We can make the following ﬁndings:"
VISUALIZATION AND ANALYSIS,0.5028089887640449,"For MNIST and CIFAR-10, the edges between nodes that locate on the digitals and objects pix-
els (the dark blue nodes) should be denser, which are conducive to the graph classiﬁcation tasks.
RP evenly prunes the signiﬁcant edges or structures without considering any important reference,
which makes the core-subgraphs destroyed and seriously deteriorates the performance. ICPG uti-
lizes AutoMasker to learn the signiﬁcance of each edge from a global view and can precisely prune
redundant edges. As the ICPG graph in Figure 7 (a) shows, the pruned edges are mainly located
on non-digital pixels, such as the upper-left, lower-right corners and the center part of the number
0; the lower-left corner of the number 8, while the remaining edges or nodes are mainly located on
digital pixels, which demonstrate that AutoMasker can indeed extract signiﬁcant patterns."
CONCLUSION,0.5056179775280899,"5
CONCLUSION"
CONCLUSION,0.5084269662921348,"In this work, we endow the graph lottery tickets with inductive pruning capacity. We propose a
simple and effective pruning framework ICPG, to co-sparsify the input graphs and GNN model. For
graphs, we propose a generative probabilistic model to generate the importance score for each edge.
It provides a global understanding of edge importance from the entire graph topological structure
to guarantee high-quality graph masks and has strong generalization ability and transferability in
inductive learning settings. For the model, we adopt the GNN weight’s magnitude to estimate their
importance scores. Then we co-sparsify the input graphs and GNN model based on their impor-
tant scores to ﬁnd the graph lottery tickets. Extensive experiments on diverse types (biochemical
molecules, social networks, superpixel graphs, citation networks) and scales (small, medium, large)
of graphs, diverse learning settings (inductive, transductive) and diverse tasks (graph classiﬁcation,
node classiﬁcation) consistently validate the effectiveness of the ICPG."
CONCLUSION,0.5112359550561798,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.5140449438202247,"6
ETHICS STATEMENT"
ETHICS STATEMENT,0.5168539325842697,"In recent years, graph neural networks (GNNs) have been widely applied in learning and process-
ing graph-structured data, such as recommendation systems and drug discovery. With the rapidly
growing size of graphs, deep graph neural networks will inevitably confront expensive computa-
tional costs and slow response times in the training and inference stage. Hence, it is challenging
to implement deep GNNs in real-world large-scale graphs. However, the current GNNs pruning
methods cannot solve the inductive learning settings, which are common in real-world scenes, such
as dynamic social networks learning. Fortunately, the proposed co-pruning framework provides an
effective and practical solution to reduce computational costs, which can be universally applied to
both transductive and inductive graph learning. Furthermore, the proposed AutoMasker also gives a
novel pre-training perspective to save the computational cost in downstream tasks."
REPRODUCIBILITY STATEMENT,0.5196629213483146,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.5224719101123596,"We promise that all results in this paper are reproducible. The models, datasets, and training and
inference settings in this paper have been given in the main text and Appendix A1. To help readers
easily reproduce our results, we provide a more detailed explanation. Codes available at https:
//anonymous.4open.science/r/Inductive_Lottery_Ticket_Learning-1419"
REPRODUCIBILITY STATEMENT,0.5252808988764045,"(1) Models. For simplicity, we keep the same GNN architecture for the proposed AutoMasker and
the following GNN model. Please note that AutoMasker usually requires fewer hidden units. The
GNN encoder network of AutoMasker can also be changed as needed and we have already given an
example of replacing GCN based encoder with GIN or GAT in the ablation study. All the results
in this paper are based on the GCN model, except for the PPI dataset. According to the suggestion
of (Veliˇckovi´c et al., 2017), GAT (Veliˇckovi´c et al., 2017) network can achieve a better baseline
performance on PPI dataset. More detailed model settings such as the number of layers or hidden
units have been summarized in Appendix A1.4 and Table A3."
REPRODUCIBILITY STATEMENT,0.5280898876404494,"(2) Datasets. We conduct all the experiments on common graph learning benchmarks (Dwivedi
et al., 2020; Xu et al., 2019; Veliˇckovi´c et al., 2017; Kipf & Welling, 2016; You et al., 2020; Hu
et al., 2020). We have summarized the detailed information of the datasets in Appendix A1.2, the
statistics of the datasets in Table A2, the method of the datasets splitting in Appendix A1.3."
REPRODUCIBILITY STATEMENT,0.5308988764044944,"(3) Baselines. We adopt the following two baselines for comparison: random pruning (RP) and UGS
(Chen et al., 2021). For random pruning, we simply adopt a completely random selection strategy
to prune the edges of the graph or the parameters of the GNN model. For UGS, we reproduce the
results of the original paper (Chen et al., 2021) based on the released code."
REPRODUCIBILITY STATEMENT,0.5337078651685393,"(4) Training and inference settings. We extensively refer to the training settings for numerous
literature in the ﬁeld of graph learning (Chen et al., 2019; Xu et al., 2019; Dwivedi et al., 2020;
Chen et al., 2021; Veliˇckovi´c et al., 2017), and conﬁgure the training hyperparameters according
to their suggestions. The detailed training settings have been given in the Appendix A1.5. As for
inference settings, we adopt the method consistent with (Chen et al., 2021; Xu et al., 2019; Hu et al.,
2020). For the TUDataset, we adopt the 10-fold cross validation and report the mean and standard
deviation. For the OGB datasets, we follow the ofﬁcial method (Hu et al., 2020) to report the results.
For other datasets, we report the test accuracy at the epoch with the best validation accuracy."
REFERENCES,0.5365168539325843,REFERENCES
REFERENCES,0.5393258426966292,"Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine
S¨usstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions
on pattern analysis and machine intelligence, 34(11):2274–2282, 2012."
REFERENCES,0.5421348314606742,"Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. In International Conference on Learning Representations, 2018."
REFERENCES,0.5449438202247191,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. arXiv preprint arXiv:2012.06908, 2020."
REFERENCES,0.547752808988764,Under review as a conference paper at ICLR 2022
REFERENCES,0.550561797752809,"Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A uniﬁed lottery
ticket hypothesis for graph neural networks. arXiv preprint arXiv:2102.06790, 2021."
REFERENCES,0.5533707865168539,"Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on
graph classiﬁcation. arXiv preprint arXiv:1905.04579, 2019."
REFERENCES,0.5561797752808989,"Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020."
REFERENCES,0.5589887640449438,"Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In International conference on machine learning, pp. 1972–1982.
PMLR, 2019."
REFERENCES,0.5617977528089888,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.5646067415730337,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024–1034, 2017."
REFERENCES,0.5674157303370787,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020."
REFERENCES,0.5702247191011236,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.5730337078651685,"Boris Knyazev, Graham W Taylor, and Mohamed R Amer. Understanding attention and generaliza-
tion in graph neural networks. arXiv preprint arXiv:1905.02850, 2019."
REFERENCES,0.5758426966292135,"Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636, 2006."
REFERENCES,0.5786516853932584,"Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020a."
REFERENCES,0.5814606741573034,"Guohao Li, Matthias M¨uller, Guocheng Qian, Itzel Carolina Delgadillo Perez, Abdulellah Abual-
shour, Ali Kassem Thabet, and Bernard Ghanem. Deepgcns: Making gcns go as deep as cnns.
TPAMI, 2021."
REFERENCES,0.5842696629213483,"Jiayu Li, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, and Reza Zafarani. Sgcn: A
graph sparsiﬁer based on graph convolutional networks. In Paciﬁc-Asia Conference on Knowledge
Discovery and Data Mining, pp. 275–287. Springer, 2020b."
REFERENCES,0.5870786516853933,"Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In 7th International Conference on Learning Representations, 2019."
REFERENCES,0.5898876404494382,"Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang. Good
students play big lottery better. arXiv, abs/2101.03255, 2021."
REFERENCES,0.5926966292134831,"Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint
arXiv:2007.08663, 2020."
REFERENCES,0.5955056179775281,"Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classiﬁcation. arXiv preprint arXiv:1907.10903, 2019."
REFERENCES,0.598314606741573,"Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsiﬁcation.
In Advances in Neural Information Processing Systems 33 pre-proceedings, 2020."
REFERENCES,0.601123595505618,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.6039325842696629,"Elli Voudigari, Nikos Salamanos, Theodore Papageorgiou, and Emmanuel J Yannakoudakis. Rank
degree: An efﬁcient algorithm for graph sampling. In 2016 IEEE/ACM International Conference
on Advances in Social Networks Analysis and Mining (ASONAM), pp. 120–129. IEEE, 2016."
REFERENCES,0.6067415730337079,Under review as a conference paper at ICLR 2022
REFERENCES,0.6095505617977528,"Chaoqi Wang, Guodong Zhang, and Roger Grosse.
Picking winning tickets before training by
preserving gradient ﬂow. In 8th International Conference on Learning Representations, 2020."
REFERENCES,0.6123595505617978,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks?
In International Conference on Learning Representations, 2019.
URL https:
//openreview.net/forum?id=ryGs6iA5Km."
REFERENCES,0.6151685393258427,"Shihui Yin, Kyu-Hyoun Kim, Jinwook Oh, Naigang Wang, Mauricio Serrano, Jae-Sun Seo, and
Jungwook Choi. The sooner the better: Investigating structure of early winning lottery tickets,
2020."
REFERENCES,0.6179775280898876,"Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnn explainer: A
tool for post-hoc explanation of graph neural networks. arXiv preprint arXiv:1903.03894, 2019."
REFERENCES,0.6207865168539326,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-
mation Processing Systems, pp. 4800–4810, 2018."
REFERENCES,0.6235955056179775,"Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. Advances in Neural Information Processing Systems,
33, 2020."
REFERENCES,0.6264044943820225,"Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019."
REFERENCES,0.6292134831460674,"Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen,
and Wei Wang. Robust graph representation learning via neural sparsiﬁcation. In International
Conference on Machine Learning, pp. 11458–11468. PMLR, 2020."
REFERENCES,0.6320224719101124,"Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018."
REFERENCES,0.6348314606741573,Under review as a conference paper at ICLR 2022
REFERENCES,0.6376404494382022,"A1
MORE IMPLEMENTATION DETAILS"
REFERENCES,0.6404494382022472,"A1.1
ALGORITHMS"
REFERENCES,0.6432584269662921,"We summarize the detailed implementation of the proposed ICPG in Algorithm 2. The Algorithm 1
represents the co-training and co-sparsifying for a single iteration in ICPG."
REFERENCES,0.6460674157303371,"Algorithm 1: Mask & Magnitude Pruning
Input: D, f(·, Θg0), AutoMasker(·, Θa0),
M, mΘ, Epoch T.
Output: Sparsiﬁed masks {m′
Gi}N
i=1, m′
Θ."
REFERENCES,0.648876404494382,1 for t = 0 to T −1 do
REFERENCES,0.651685393258427,"2
for Gi ∈D and mGi ∈M do"
REFERENCES,0.6544943820224719,"3
Gi ←(mGi ⊙Ai, Xi)"
REFERENCES,0.6573033707865169,"4
sGi ←AutoMasker(Gi, Θat)"
REFERENCES,0.6601123595505618,"5
Gi ←(sGi ⊙Ai, Xi)"
REFERENCES,0.6629213483146067,"6
Forward f(Gi, mΘ ⊙Θgt)"
REFERENCES,0.6657303370786517,"7
Backward to update Θat+1, Θgt+1"
END,0.6685393258426966,"8
end"
END,0.6713483146067416,9 end
END,0.6741573033707865,10 for Gi ∈D do
END,0.6769662921348315,"11
sGi ←AutoMasker(Gi, ΘaT )"
END,0.6797752808988764,"12
Set 5% of the lowest mask values in sGi
to 0 and others to 1, creating m′
Gi."
END,0.6825842696629213,13 end
END,0.6853932584269663,"14 Prune 20% of the lowest magnitude
parameters in ΘgT , creating m′
Θ."
END,0.6882022471910112,Algorithm 2: Finding GLTs by ICPG
END,0.6910112359550562,"Input: Graphs D = {Gi = (Ai, Xi)}N
i=1,
f(·, Θg0), AutoMasker(·, Θa0),
sparsity levels sd, sθ.
Output: GLT {G′
i = (mGi ⊙Ai, Xi)}N
i=1,
f(·; mΘ ⊙Θg0)."
END,0.6938202247191011,"1 Initialize masks set M ←{mGi ←Ai}N
i=1"
END,0.6966292134831461,2 Initialize GNN mask mΘ ←1 ∈R∥Θg0∥0
END,0.699438202247191,"3 while the sparsity of M < sd, mΘ < sθ do"
END,0.702247191011236,"4
Sparsify the GNN f(·; Θg0) with mΘ
and dataset D = {Gi = (Ai, Xi)}N
i=1
with the mask set M and get the new
masks as presented in Algorithm 1."
END,0.7050561797752809,"5
Update M ←{mGi ←m′
Gi}N
i=1"
END,0.7078651685393258,"6
Update mΘ ←m′
Θ
7
Rewind AutoMasker’s weight to Θa0."
END,0.7106741573033708,"8
Rewind GNN’s weight to Θg0."
END,0.7134831460674157,9 end
END,0.7162921348314607,"A1.2
DATASETS DETAILS"
END,0.7191011235955056,"As for graph classiﬁcation, we conduct the experiments with three scale levels: small-scale, medium-
scale and large-scale. We provide the following details:"
END,0.7219101123595506,"• Small-scale: We adpot the TUDataset (Morris et al., 2020), which is a collection of benchmark
datasets for graph classiﬁcation and regression. We choose two biological graphs: NCI1, MU-
TAG and four social graphs: COLLAB, Reddit-Binary (RED-B), Reddit-Multi-5K (RED-M5K),
Reddit-Multi-12K (RED-M12K) as numerous works (Xu et al., 2019; You et al., 2020; Dwivedi
et al., 2020) does.
• Medium-scale: We use MNIST and CIFAR-10 superpixel graphs. The original MNIST and
CIFAR-10 images are converted to graphs using superpixels, which represent small regions of
homogeneous intensity in images and can be extracted with the SLIC (Achanta et al., 2012) tech-
nique. These datasets are commonly used in numerous graph representation learning researches
(Dwivedi et al., 2020; You et al., 2020; Knyazev et al., 2019).
• Large-scale, we adopt the Open Graph Benchmark (OGB) (Hu et al., 2020), which is a collec-
tion of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. We
choose two commonly used large-scale graph property prediction dataset: ogbg-ppa and ogbg-
code2. The ogbg-ppa dataset is a set of undirected protein association neighborhoods extracted
from the protein-protein association networks of 1,581 different species that cover 37 broad taxo-
nomic groups and span the tree of life. The ogbg-code2 dataset is a collection of Abstract Syntax
Trees (ASTs) obtained from approximately 450 thousands Python method deﬁnitions."
END,0.7247191011235955,"As for node classiﬁcation, we use the commonly used citation network Cora dataset for semi-
supervised learning and the popular inductive learning dataset PPI. The task for PPT is classifying
protein functions across various biological protein-protein interaction (PPI) graphs. These datasets
are commonly used in transductive learning (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017) and in-
ductive learning (Hamilton et al., 2017; Zheng et al., 2020) researches. All the detail statistics about
theaforementioned datasets are summerized in Table A2."
END,0.7275280898876404,Under review as a conference paper at ICLR 2022
END,0.7303370786516854,"Table A2: Datasets statistics.
Datasets
Category
Graphs Avg. Nodes Avg. Edges Avg. Degree Classes
NCI1
Biochemical Molecules
4,110
29.87
32.30
1.08
2
MUTAG
Biochemical Molecules
188
17.93
19.79
1.10
2"
END,0.7331460674157303,"COLLAB
Social Networks
5,000
74.49
2457.78
32.99
3
RED-B
Social Networks
2,000
429.63
494.07
1.15
2
RED-M5K
Social Networks
4,999
508.52
594.87
1.17
5
RED-M12K
Social Networks
11,929
391.41
456.89
1.16
11"
END,0.7359550561797753,"MNIST
Superpixel Graphs
70,000
70.57
564.56
8.00
10
CIFAR-10
Superpixel Graphs
60,000
117.63
941.04
8.00
10"
END,0.7387640449438202,"ogbg-ppa
OGB Dataset
158,100
243.4
2,266.1
9.31
37
ogbg-code2
OGB Dataset
452,741
125.2
124.2
0.99
-"
END,0.7415730337078652,"Cora
Citation Network
1
2708
5429
2.00
7
PPI
Biological Protein
24
2372.67
34113.16
14.38
121"
END,0.7443820224719101,"A1.3
DATASETS SPLITTING"
END,0.7471910112359551,"TUDataset: We perform the commonly used 10-fold cross validation. Consistent with the work
(Chen et al., 2019; Xu et al., 2019), we select the epoch with the best cross-validation accuracy
averaged over the 10 folds and report the average and standard deviation of test accuracies at the
selected epoch over 10 folds."
END,0.75,"Superpixel dataset: Consistent with (Dwivedi et al., 2020), we split them to 55000 train/5000 vali-
dation/10000 test for MNIST, and 45000 train/5000 validation/10000 test for CIFAR10, respectively.
We report the test accuracy at the epoch with the best validation accuracy."
END,0.7528089887640449,"OGB dataset: We adopt the ofﬁcial dataset splitting method for ogbg-ppa and ogbg-code2 in
the following links: https://ogb.stanford.edu/docs/graphprop/#ogbg-ppa and
https://ogb.stanford.edu/docs/graphprop/#ogbg-code2."
END,0.7556179775280899,"Cora: Following the work (Chen et al., 2021), we use 140 labeled data for training, 500 nodes
for validation and 1000 nodes for testing and report the test accuracy at the epoch with the best
validation accuracy."
END,0.7584269662921348,"PPI: We adopt the same splitting as works (Hamilton et al., 2017; Veliˇckovi´c et al., 2017). The
dataset contains 20 graphs for training, 2 for validation and 2 for testing. Critically, testing graphs
remain completely unobserved during training."
END,0.7612359550561798,"A1.4
MODEL CONFIGURATIONS"
END,0.7640449438202247,"TUDataset: we adopt the ResGCN (Chen et al., 2019) with 3 layers and 128 hidden dimensions.
Superpixel dataset: we use GCN with 4 layers and 146 hidden dimensions as work (Dwivedi et al.,
2020). OGB dataset: we use ﬁve-layer GCN model with 300 hidden dimensions for all experiments.
Cora dataset: we adopt two-layer GCN network with 512 hidden dimensions as work (Chen et al.,
2021). PPI dataset: we adopt two-layer GAT network with 256 hidden and 4 attention heads as work
(Veliˇckovi´c et al., 2017). For GCN encoder in AutoMasker, we use the same structure with different
hidden dimensions as the GNN model. More details are summarized in Table A3."
END,0.7668539325842697,"A1.5
TRAINING DETAILS"
END,0.7696629213483146,"All training hyper-parameters such as epochs, learning rate for GNN model and AutoMasker, opti-
mizer, batch size, weight decay are summarized in Table A3."
END,0.7724719101123596,Under review as a conference paper at ICLR 2022
END,0.7752808988764045,Table A3: Implementation details of graph classiﬁcation and node classiﬁcation.
END,0.7780898876404494,"Task
Graph Classiﬁcation
Node Classiﬁcation"
END,0.7808988764044944,"Dataset
TUDataset
Superpixel
ogbg-ppa
ogbg-code2
Cora
PPI"
END,0.7837078651685393,"Epoch
100
100
100
25
200
100
Optimizer
Admm
Admm
Admm
Admm
Admm
Admm
Batch Size
128
128
32
128
1
1
Weight Decay
0
0
0
0
5e-4
0
Model Layer-hidden
3-128
4-146
5-300
5-300
2-512
2-256
AutoMasker Layer-hidden
3-64
4-146
5-300
5-300
2-128
2-128
Model LR
1e-3
1e-3
1e-3
1e-3
1e-2
5e-3
AutoMasker LR
1e-4
5e-3
1e-3
1e-3
1e-2
1e-3"
END,0.7865168539325843,"A2
MORE EXPERIMENTAL RESULTS"
END,0.7893258426966292,"A2.1
GRAPH LOTTERY TICKETS IN GRAPH CLASSIFICATION"
END,0.7921348314606742,"The graph classiﬁcation performance with different GNN sparsity levels is shown in Figure A8. The
performance on large-scale datasets is provided in Figure A9. 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
END,0.7949438202247191,"83.22
86.58 91.41"
END,0.797752808988764,"95.60
98.85"
END,0.800561797752809,GNN Sparsity (%) 50 60 70 80
END,0.8033707865168539,Accuracy (%) NCI1
END,0.8061797752808989,"RP
ICPG (Ours)
Baseline
RP-GLT (20.00%)
ICPG-GLT (73.79%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
END,0.8089887640449438,"83.22
86.58 91.41"
END,0.8117977528089888,"95.60
98.85"
END,0.8146067415730337,GNN Sparsity (%) 70 80 90
MUTAG,0.8174157303370787,"100
MUTAG"
MUTAG,0.8202247191011236,"RP
ICPG (Ours)
Baseline
RP-GLT (59.04%)
ICPG-GLT (79.03%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.8230337078651685,"83.22
86.58 91.41"
MUTAG,0.8258426966292135,"95.60
98.85"
MUTAG,0.8286516853932584,GNN Sparsity (%) 50 60 70 80
MUTAG,0.8314606741573034,COLLAB
MUTAG,0.8342696629213483,"RP
ICPG (Ours)
Baseline
RP-GLT (20.00%)
ICPG-GLT (83.22%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.8370786516853933,"83.22
86.58 91.41"
MUTAG,0.8398876404494382,"95.60
98.85"
MUTAG,0.8426966292134831,GNN Sparsity (%) 60 80 RED-B
MUTAG,0.8455056179775281,"RP
ICPG (Ours)
Baseline
RP-GLT (36.00%)
ICPG-GLT (67.23%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.848314606741573,"83.22
86.58 91.41"
MUTAG,0.851123595505618,"95.60
98.85"
MUTAG,0.8539325842696629,GNN Sparsity (%) 20 30 40 50 60
MUTAG,0.8567415730337079,Accuracy (%)
MUTAG,0.8595505617977528,RED-M5K
MUTAG,0.8623595505617978,"RP
ICPG (Ours)
Baseline
RP-GLT (48.80%)
ICPG-GLT (91.41%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.8651685393258427,"83.22
86.58 91.41"
MUTAG,0.8679775280898876,"95.60
98.85"
MUTAG,0.8707865168539326,GNN Sparsity (%) 20 30 40 50
MUTAG,0.8735955056179775,RED-M12K
MUTAG,0.8764044943820225,"RP
ICPG (Ours)
Baseline
RP-GLT (36.00%)
ICPG-GLT (95.60%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.8792134831460674,"83.22
86.58 91.41"
MUTAG,0.8820224719101124,"95.60
98.85"
MUTAG,0.8848314606741573,GNN Sparsity (%) 20 40 60 80 MNIST
MUTAG,0.8876404494382022,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (91.41%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79 79.03"
MUTAG,0.8904494382022472,"83.22
86.58 91.41"
MUTAG,0.8932584269662921,"95.60
98.85"
MUTAG,0.8960674157303371,GNN Sparsity (%) 20 30 40 50
MUTAG,0.898876404494382,CIFAR-10
MUTAG,0.901685393258427,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (48.80%)"
MUTAG,0.9044943820224719,Figure A8: Graph classiﬁcation performance over achieved GNN sparsity. 0.00 20.00 36.00 48.80 59.04 67.23 73.79
MUTAG,0.9073033707865169,"79.03
83.22
86.58"
MUTAG,0.9101123595505618,"91.41
95.60
98.85"
MUTAG,0.9129213483146067,GNN Sparsity (%) 20 40 60
MUTAG,0.9157303370786517,Accuracy (%)
MUTAG,0.9185393258426966,ogbg-ppa
MUTAG,0.9213483146067416,"RP
ICPG (Ours)
Baseline
RP-GLT (20.00%)
ICPG-GLT (48.80%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79"
MUTAG,0.9241573033707865,"79.03
83.22
86.58
91.41
95.60
98.85"
MUTAG,0.9269662921348315,GNN Sparsity (%) 8 10 12 14
MUTAG,0.9297752808988764,F1 score (%)
MUTAG,0.9325842696629213,ogbg-code2
MUTAG,0.9353932584269663,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (59.04%)"
MUTAG,0.9382022471910112,Figure A9: Graph classiﬁcation performance over achieved GNN sparsity on large-scale datasets.
MUTAG,0.9410112359550562,Under review as a conference paper at ICLR 2022
MUTAG,0.9438202247191011,"A2.2
GRAPH LOTTERY TICKETS IN NODE CLASSIFICATION"
MUTAG,0.9466292134831461,"More results about the transductive and inductive node classiﬁcation on achieved GNN sparsity are
shown in Figure A10. We can observe that ICPG can achieve 73.79% GNN sparsity for transductive
node classiﬁcation and 67.23% GNN sparsity for inductive node classiﬁcation. 0.00 20.00 36.00 48.80 59.04 67.23 73.79"
MUTAG,0.949438202247191,"79.03
83.22
86.58"
MUTAG,0.952247191011236,"91.41
95.60
98.85"
MUTAG,0.9550561797752809,GNN Sparsity (%) 30 40 50 60 70 80
MUTAG,0.9578651685393258,Accuracy (%) Cora
MUTAG,0.9606741573033708,"RP
UGS
ICPG (Ours)
Baseline
RP-GLT (36.00%)
UGS-GLT (59.04%)
ICPG-GLT (73.79%) 0.00 20.00 36.00 48.80 59.04 67.23 73.79"
MUTAG,0.9634831460674157,"79.03
83.22
86.58"
MUTAG,0.9662921348314607,"91.41
95.60
98.85"
MUTAG,0.9691011235955056,Graph Sparsity (%) 60 70 80 90 100 PPI
MUTAG,0.9719101123595506,"RP
ICPG (Ours)
Baseline
RP-GLT (0.00%)
ICPG-GLT (67.23%)"
MUTAG,0.9747191011235955,Figure A10: Transductive and inductive node classiﬁcation performance on achieved GNN sparsity.
MUTAG,0.9775280898876404,"A3
MORE VISUALIZATION RESULTS"
MUTAG,0.9803370786516854,"More visualization results about the MNIST and CIFAR-10 superpixel graphs are shown in Figure
A11 and Figure A12. For MNIST, the important subgraphs are mainly located on the digital pixel
area. We can observe that RP evenly prunes the connections between any two nodes without consid-
ering any important information. While the proposed AutoMasker mainly prunes the insigniﬁcant
edges that locate on non-digital pixel background. For CIFAR-10, objects are critical information
for classiﬁcation, so the edges between the nodes that locate on the objects (the dark blue nodes)
should be denser. We can observe that RP prunes on both the objects and the background, which also
makes all the vital edges become sparse. As a comparison, the subgraphs extracted by AutoMasker
remain most of the edges located on objects while the sparsify edges are located on the backgrounds.
In summary, the AutoMasker effectively extracts the signiﬁcant core patterns while ﬁlter out the re-
dundant edges from dense graphs, which necessarily leads to less degradation on performance even
the graphs are heavily pruned."
MUTAG,0.9831460674157303,Under review as a conference paper at ICLR 2022
MUTAG,0.9859550561797753,"Original Image Original Graph
RP Graph
ICPG Graph"
MUTAG,0.9887640449438202,Figure A11: Visualization of the subgraphs at 64.15% sparsity from MNIST superpixel graphs.
MUTAG,0.9915730337078652,Under review as a conference paper at ICLR 2022
MUTAG,0.9943820224719101,"Original Image Original Graph
RP Graph
ICPG Graph"
MUTAG,0.9971910112359551,Figure A12: Visualization of the subgraphs at 64.15% sparsity from CIFAR-10 superpixel graphs.
