Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010416666666666667,"We propose ZeroSARAH—a novel variant of the variance-reduced method
SARAH (Nguyen et al., 2017)—for minimizing the average of a large number of
nonconvex functions 1"
ABSTRACT,0.0020833333333333333,"n
Pn
i=1 fi(x). To the best of our knowledge, in this noncon-
vex ﬁnite-sum regime, all existing variance-reduced methods, including SARAH,
SVRG, SAGA and their variants, need to compute the full gradient over all n data
samples at the initial point x0, and then periodically compute the full gradient once
every few iterations (for SVRG, SARAH and their variants). Note that SVRG,
SAGA and their variants typically achieve weaker convergence results than vari-
ants of SARAH: n2/3/ϵ2 vs. n1/2/ϵ2. Thus we focus on the variant of SARAH.
The proposed ZeroSARAH and its distributed variant D-ZeroSARAH are the ﬁrst
variance-reduced algorithms which do not require any full gradient computations,
not even for the initial point. Moreover, for both standard and distributed set-
tings, we show that ZeroSARAH and D-ZeroSARAH obtain new state-of-the-art
convergence results, which can improve the previous best-known result (given by
e.g., SPIDER, SARAH, and PAGE) in certain regimes. Avoiding any full gradient
computations (which are time-consuming steps) is important in many applications
as the number of data samples n usually is very large. Especially in the distributed
setting, periodic computation of full gradient over all data samples needs to pe-
riodically synchronize all clients/devices/machines, which may be impossible or
unaffordable. Thus, we expect that ZeroSARAH/D-ZeroSARAH will have a prac-
tical impact in distributed and federated learning where full device participation is
impractical."
INTRODUCTION,0.003125,"1
INTRODUCTION"
INTRODUCTION,0.004166666666666667,"Nonconvex optimization is ubiquitous across many domains of machine learning (Jain & Kar, 2017),
especially in training deep neural networks. In this paper, we consider the nonconvex ﬁnite-sum
problems of the form"
INTRODUCTION,0.005208333333333333,"min
x∈Rd ("
INTRODUCTION,0.00625,"f(x) := 1 n n
X"
INTRODUCTION,0.007291666666666667,"i=1
fi(x) ) ,
(1)"
INTRODUCTION,0.008333333333333333,"where f : Rd →R is a differentiable and possibly nonconvex function. Problem (1) captures the
standard empirical risk minimization problems in machine learning (Shalev-Shwartz & Ben-David,
2014). There are n data samples and fi denotes the loss associated with i-th data sample. We
assume the functions fi : Rd →R for all i ∈[n] := {1, 2, . . . , n} are also differentiable and
possibly nonconvex functions."
INTRODUCTION,0.009375,"Beyond the standard/centralized problem (1), we further consider the distributed/federated noncon-
vex problems:"
INTRODUCTION,0.010416666666666666,"min
x∈Rd ("
INTRODUCTION,0.011458333333333333,"f(x) := 1 n n
X"
INTRODUCTION,0.0125,"i=1
fi(x) )"
INTRODUCTION,0.013541666666666667,",
fi(x) := 1 m m
X"
INTRODUCTION,0.014583333333333334,"j=1
fi,j(x),
(2)"
INTRODUCTION,0.015625,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016666666666666666,"where n denotes the number of clients/devices/machines, fi denotes the loss associated with m data
samples stored on client i, and all functions are differentiable and can be nonconvex. Avoiding any
full gradient computations is important especially in this distributed setting (2), periodic computation
of full gradient over all data samples needs to periodically synchronize all clients, which may be
impossible or very hard to achieve."
INTRODUCTION,0.017708333333333333,"There has been extensive research in designing ﬁrst-order (gradient-type) methods for solving cen-
tralized/distributed nonconvex problems (1) and (2) such as SGD, SVRG, SAGA, SCSG, SARAH
and their variants, e.g., (Ghadimi & Lan, 2013; Ghadimi et al., 2016; Allen-Zhu & Hazan, 2016;
Reddi et al., 2016; Lei et al., 2017; Li & Li, 2018; Zhou et al., 2018; Fang et al., 2018; Wang et al.,
2018; Ge et al., 2019; Pham et al., 2019; Li, 2019; Li & Richt´arik, 2020; Horv´ath et al., 2020; Li
et al., 2021). Note that SVRG and SAGA variants typically achieve weaker convergence results than
SARAH variants, i.e., n2/3/ϵ2 vs. √n/ϵ2. Thus the current best convergence results are achieved
by SARAH variants such as SPIDER (Fang et al., 2018), SARAH (Pham et al., 2019) and PAGE (Li
et al., 2021; Li, 2021)."
INTRODUCTION,0.01875,"However, all of these variance-reduced algorithms (no matter based on SVRG, SAGA or SARAH)
require full gradient computations (i.e., compute ∇f(x) = 1"
INTRODUCTION,0.019791666666666666,"n
Pn
i=1 ∇fi(x)) without assuming ad-
ditional assumptions except standard L-smoothness assumptions. We would like to point out that
under an additional bounded variance assumption (e.g., Ei[∥∇fi(x) −∇f(x)∥2] ≤σ2, ∀x ∈Rd),
some of them (such as SCSG (Lei et al., 2017), SVRG+ (Li & Li, 2018), PAGE (Li et al., 2021)) may
avoid full gradient computations by using a large minibatch of stochastic gradients instead (usually
the minibatch size is O(σ2/ϵ2)). Clearly, there exist some drawbacks: i) σ2 usually is not known;
ii) if the target error ϵ is very small (deﬁned as E[∥∇f(bx)∥2] ≤ϵ2 in Deﬁnition 1) or σ is very large,
then the minibatch size O(σ2/ϵ2) is still very large for replacing full gradient computations."
INTRODUCTION,0.020833333333333332,"In this paper, we only consider algorithms under the standard L-smoothness assumptions, with-
out assuming any other additional assumptions (such as bounded variance assumption mentioned
above). Thus, all existing variance-reduced methods, including SARAH, SVRG, SAGA and their
variants, need to compute the full gradient over all n data samples at the initial point x0, and then
periodically compute the full gradient once every few iterations (for SVRG, SARAH and their vari-
ants). However, full gradient computations are time-consuming steps in many applications as the
number of data samples n usually is very large. Especially in the distributed setting, periodic com-
putation of full gradient needs to periodically synchronize all clients/devices, which usually is im-
practical. Motivated by this, we focus on designing new algorithms which do not require any full
gradient computations for solving standard and distributed nonconvex problems (1)–(2)."
OUR CONTRIBUTIONS,0.021875,"2
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.022916666666666665,"In this paper, we propose the ﬁrst variance-reduced algorithm ZeroSARAH (and also its dis-
tributed variant D-ZeroSARAH) without computing any full gradients for solving both standard
and distributed nonconvex ﬁnite-sum problems (1)–(2). Moreover, ZeroSARAH and Distributed
D-ZeroSARAH can obtain new state-of-the-art convergence results which improve previous best-
known results (given by e.g., SPIDER, SARAH and PAGE) in certain regimes (see Tables 1–2 for
the comparison with previous algorithms). ZeroSARAH is formally described in Algorithm 2, which
is a variant of SARAH (Nguyen et al., 2017). See Section 4 for more details and comparisons
between ZeroSARAH and SARAH. Then, D-ZeroSARAH is formally described in Algorithm 3 of
Section 5, which is a distributed variant of our ZeroSARAH."
OUR CONTRIBUTIONS,0.023958333333333335,"Now, we highlight the following results achieved by ZeroSARAH and D-ZeroSARAH:"
OUR CONTRIBUTIONS,0.025,"• ZeroSARAH and D-ZeroSARAH are the ﬁrst variance-reduced algorithms which do not require
any full gradient computations, not even for the initial point (see Algorithms 2–3 or Tables 1–2).
Avoiding any full gradient computations is important in many applications as the number of data
samples n usually is very large. Especially in the distributed setting, periodic computation of full
gradient over all data samples stored in all clients/devices may be impossible or very hard to achieve.
We expect that ZeroSARAH/D-ZeroSARAH will have a practical impact in distributed and federated
learning where full device participation is impractical."
OUR CONTRIBUTIONS,0.026041666666666668,"• Moreover, ZeroSARAH can recover the previous best-known convergence result O(n +
√nL∆0"
OUR CONTRIBUTIONS,0.027083333333333334,"ϵ2
)
(see Table 1 or Corollary 1), and also provide new state-of-the-art convergence results without any"
OUR CONTRIBUTIONS,0.028125,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.029166666666666667,"Table 1: Stochastic gradient complexity for ﬁnding an ϵ-approximate solution of nonconvex prob-
lems (1), under Assumption 1"
OUR CONTRIBUTIONS,0.030208333333333334,"Algorithms
Stochastic gradient
complexity
Full gradient computation"
OUR CONTRIBUTIONS,0.03125,"GD (Nesterov, 2004)
O( nL∆0"
OUR CONTRIBUTIONS,0.03229166666666667,"ϵ2
)
Computed for every iteration"
OUR CONTRIBUTIONS,0.03333333333333333,"SVRG (Reddi et al., 2016;
Allen-Zhu & Hazan, 2016),
SCSG (Lei et al., 2017),
SVRG+ (Li & Li, 2018)"
OUR CONTRIBUTIONS,0.034375,"O

n + n2/3L∆0"
OUR CONTRIBUTIONS,0.035416666666666666,"ϵ2

Computed for the initial point
and periodically computed
for every l iterations"
OUR CONTRIBUTIONS,0.036458333333333336,"SNVRG (Zhou et al., 2018),
Geom-SARAH (Horv´ath et al., 2020)"
OUR CONTRIBUTIONS,0.0375,"eO

n +
√nL∆0"
OUR CONTRIBUTIONS,0.03854166666666667,"ϵ2

Computed for the initial point
and periodically computed
for every l iterations
SPIDER (Fang et al., 2018),
SpiderBoost (Wang et al., 2018),
SARAH (Pham et al., 2019),
SSRGD (Li, 2019),
PAGE (Li et al., 2021)"
OUR CONTRIBUTIONS,0.03958333333333333,"O

n +
√nL∆0"
OUR CONTRIBUTIONS,0.040625,"ϵ2

Computed for the initial point
and periodically computed
for every l iterations"
OUR CONTRIBUTIONS,0.041666666666666664,"ZeroSARAH
(this paper, Corollary 1)
O

n +
√nL∆0"
OUR CONTRIBUTIONS,0.042708333333333334,"ϵ2

Only computed once for
the initial point 1"
OUR CONTRIBUTIONS,0.04375,"ZeroSARAH
(this paper, Corollary 2)
O
 √n(L∆0+G0) ϵ2
"
OUR CONTRIBUTIONS,0.04479166666666667,Never computed 2
OUR CONTRIBUTIONS,0.04583333333333333,"1 In Corollary 1, ZeroSARAH only computes the full gradient ∇f(x0) =
1
n
Pn
i=1 ∇fi(x0)
once for the initial point x0, i.e., minibatch size b0 = n, and then bk ≡√n for all iterations
k ≥1 in Algorithm 2.
2 In Corollary 2, ZeroSARAH never computes full gradients, i.e., minibatch size bk ≡√n for
all iterations k ≥0."
OUR CONTRIBUTIONS,0.046875,"full gradient computations (see Table 1 or Corollary 2) which can improve the previous best result
in certain regimes."
OUR CONTRIBUTIONS,0.04791666666666667,"• Besides, for the distributed nonconvex setting (2), the distributed D-ZeroSARAH (Algorithm 3) en-
joys similar beneﬁts as our ZeroSARAH, i.e., D-ZeroSARAH does not need to periodically synchro-
nize all n clients to compute any full gradients, and also provides new state-of-the-art convergence
results. See Table 2 and Section 5 for more details."
OUR CONTRIBUTIONS,0.04895833333333333,"• Finally, the experimental results in Section 6 show that ZeroSARAH is slightly better than the
previous state-of-the-art SARAH. However, we should point out that ZeroSARAH does not compute
any full gradients while SARAH needs to periodically compute the full gradients for every l iter-
ations (here l = √n). Thus the experiments validate our theoretical results (can be slightly better
than SARAH (see Table 1)) and conﬁrm the practical superiority of ZeroSARAH (avoid any full gra-
dient computations). Similar experimental results of D-ZeroSARAH for the distributed setting are
provided in Appendix A.2."
PRELIMINARIES,0.05,"3
PRELIMINARIES"
PRELIMINARIES,0.051041666666666666,"Notation: Let [n] denote the set {1, 2, · · · , n} and ∥· ∥denote the Euclidean norm for a vector
and the spectral norm for a matrix. Let ⟨u, v⟩denote the inner product of two vectors u and v. We
use O(·) and Ω(·) to hide the absolute constant, and eO(·) to hide the logarithmic factor. We will
write ∆0 := f(x0) −f ∗, f ∗:= minx∈Rd f(x), G0 := 1"
PRELIMINARIES,0.052083333333333336,"n
Pn
i=1 ∥∇fi(x0)∥2, b∆0 := f(x0) −bf ∗,
bf ∗:= 1"
PRELIMINARIES,0.053125,"n
Pn
i=1 minx∈Rd fi(x) and G′
0 :=
1
nm
Pn,m
i,j=1,1 ∥∇fi,j(x0)∥2."
PRELIMINARIES,0.05416666666666667,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.05520833333333333,"Table 2: Stochastic gradient complexity for ﬁnding an ϵ-approximate solution of distributed non-
convex problems (2), under Assumption 2"
PRELIMINARIES,0.05625,"Algorithms
Stochastic gradient
complexity
Full gradient computation"
PRELIMINARIES,0.057291666666666664,DC-GD 1
PRELIMINARIES,0.058333333333333334,"(Khaled & Richt´arik, 2020; Li & Richt´arik, 2020)
O( mL∆0"
PRELIMINARIES,0.059375,"ϵ2
)
Computed for every iteration"
PRELIMINARIES,0.06041666666666667,D-SARAH 2
PRELIMINARIES,0.06145833333333333,"(Cen et al., 2020)
O

m +
√m log mL∆0"
PRELIMINARIES,0.0625,"ϵ2

Computed for the initial point
and periodically computed
across all n clients"
PRELIMINARIES,0.06354166666666666,D-GET 2
PRELIMINARIES,0.06458333333333334,"(Sun et al., 2020)
O

m +
√mL∆0"
PRELIMINARIES,0.065625,"ϵ2

Computed for the initial point
and periodically computed
across all n clients
SCAFFOLD 3"
PRELIMINARIES,0.06666666666666667,"(Karimireddy et al., 2020)
O

m +
m
n1/3
L∆0"
PRELIMINARIES,0.06770833333333333,"ϵ2

Only computed once for
the initial point"
PRELIMINARIES,0.06875,DC-LSVRG/DC-SAGA 1
PRELIMINARIES,0.06979166666666667,"(Li & Richt´arik, 2020)
O

m + m2/3"
PRELIMINARIES,0.07083333333333333,"n1/3
L∆0"
PRELIMINARIES,0.071875,"ϵ2

Computed for the initial point
and periodically computed
across all n clients"
PRELIMINARIES,0.07291666666666667,FedPAGE 3
PRELIMINARIES,0.07395833333333333,"(Zhao et al., 2021)
O

m +
m
√n
L∆0"
PRELIMINARIES,0.075,"ϵ2

Computed for the initial point
and periodically computed
across all n clients"
PRELIMINARIES,0.07604166666666666,(Distributed) SARAH/SPIDER/SSRGD 4
PRELIMINARIES,0.07708333333333334,"(Nguyen et al., 2017; Fang et al., 2018; Li, 2019)
O
 
m + p m n
L∆0"
PRELIMINARIES,0.078125,"ϵ2

Computed for the initial point
and periodically computed
across all n clients"
PRELIMINARIES,0.07916666666666666,"D-ZeroSARAH
(this paper, Corollary 3)
O
 
m + p m n
L∆0"
PRELIMINARIES,0.08020833333333334,"ϵ2

Only computed once for
the initial point"
PRELIMINARIES,0.08125,"D-ZeroSARAH
(this paper, Corollary 4)
O
p m"
PRELIMINARIES,0.08229166666666667,"n
L∆0+G′
0
ϵ2
"
PRELIMINARIES,0.08333333333333333,Never computed
PRELIMINARIES,0.084375,"1 Distributed compressed methods. Here we translate their results to this distributed setting (2).
2 Decentralized methods. Here we translate their results to this distributed setting (2).
3 Federated local methods. Here we translate their results to this distributed setting (2).
4 Distributed version of previous SARAH-type methods (see e.g., Algorithm 4 in Appendix A.2)."
PRELIMINARIES,0.08541666666666667,"Deﬁnition 1 A point bx is called an ϵ-approximate solution for nonconvex problems (1) and (2) if
E[∥∇f(bx)∥2] ≤ϵ2."
PRELIMINARIES,0.08645833333333333,"To show the convergence results, we assume the following standard smoothness assumption for
nonconvex problems (1)."
PRELIMINARIES,0.0875,"Assumption 1 (L-smoothness) A function fi : Rd →R is L-smooth if ∃L > 0, such that"
PRELIMINARIES,0.08854166666666667,"∥∇fi(x) −∇fi(y)∥≤L∥x −y∥,
∀x, y ∈Rd.
(3)"
PRELIMINARIES,0.08958333333333333,"It is easy to see that f(x) =
1
n
Pn
i=1 fi(x) is also L-smooth under Assumption 1. We can also
relax Assumption 1 by deﬁning Li-smoothness for each fi. Then if we further deﬁne the average
L2 := 1"
PRELIMINARIES,0.090625,"n
Pn
i=1 L2
i , we know that f(x) = 1"
PRELIMINARIES,0.09166666666666666,"n
Pn
i=1 fi(x) is also L-smooth. Here we use the same L
just for simple representation."
PRELIMINARIES,0.09270833333333334,"For the distributed nonconvex problems (2), we use the following Assumption 2 instead of Assump-
tion 1. Similarly, we can also relax it by deﬁning Li,j-smoothness for different fi,j. Here we use
the same L just for simple representation."
PRELIMINARIES,0.09375,"Assumption 2 (L-smoothness) A function fi,j : Rd →R is L-smooth if ∃L > 0, such that"
PRELIMINARIES,0.09479166666666666,"∥∇fi,j(x) −∇fi,j(y)∥≤L∥x −y∥,
∀x, y ∈Rd.
(4)"
PRELIMINARIES,0.09583333333333334,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.096875,"Algorithm 1 SARAH (Nguyen et al., 2017; Pham et al., 2019)"
PRELIMINARIES,0.09791666666666667,"Input: initial point x0, epoch length l, stepsize η, minibatch size b"
PRELIMINARIES,0.09895833333333333,1: ex = x0
PRELIMINARIES,0.1,"2: for s = 0, 1, 2, . . . do
3:
x0 = ex"
PRELIMINARIES,0.10104166666666667,"4:
v0 = ∇f(x0) = 1 n nP"
PRELIMINARIES,0.10208333333333333,"i=1
∇fi(x0)
// compute the full gradient once for every l iterations"
PRELIMINARIES,0.103125,"5:
x1 = x0 −ηv0"
PRELIMINARIES,0.10416666666666667,"6:
for k = 1, 2, . . . , l do
7:
Randomly sample a minibatch data samples Ib with |Ib| = b
8:
vk = 1 b
P i∈Ib"
PRELIMINARIES,0.10520833333333333," 
∇fi(xk) −∇fi(xk−1)

+ vk−1"
PRELIMINARIES,0.10625,"9:
xk+1 = xk −ηvk"
PRELIMINARIES,0.10729166666666666,"10:
end for
11:
ex randomly chosen from {xk}k∈[l] or ex = xl+1"
PRELIMINARIES,0.10833333333333334,12: end for
PRELIMINARIES,0.109375,Algorithm 2 SARAH without full gradient computations (ZeroSARAH)
PRELIMINARIES,0.11041666666666666,"Input: initial point x0, stepsize {ηk}, minibatch size {bk}, parameter {λk}"
PRELIMINARIES,0.11145833333333334,1: x−1 = x0
PRELIMINARIES,0.1125,"2: v−1 = 0, y−1
1
= y−1
2
= · · · = y−1
n
= 0
// no full gradient computation
3: for k = 0, 1, 2, . . . do
4:
Randomly sample a minibatch data samples Ik
b with |Ik
b | = bk"
PRELIMINARIES,0.11354166666666667,"5:
vk =
1
bk
P"
PRELIMINARIES,0.11458333333333333,"i∈Ik
b"
PRELIMINARIES,0.115625," 
∇fi(xk)−∇fi(xk−1)

+(1 −λk)vk−1 + λk

1
bk
P"
PRELIMINARIES,0.11666666666666667,"i∈Ik
b"
PRELIMINARIES,0.11770833333333333," 
∇fi(xk−1) −yk−1
i

+ 1 n nP"
PRELIMINARIES,0.11875,"j=1
yk−1
j
"
PRELIMINARIES,0.11979166666666667,"// no full gradient computations for vks
6:
xk+1 = xk −ηkvk"
PRELIMINARIES,0.12083333333333333,"7:
yk
i =
∇fi(xk)
for i ∈Ik
b
yk−1
i
for i /∈Ik
b
// the update of {yk
i } directly follows from the stochastic gradients computed in Line 5
8: end for"
ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.121875,"4
ZeroSARAH ALGORITHM AND ITS CONVERGENCE RESULTS"
ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.12291666666666666,"In this section, we consider the standard/centralized nonconvex problems (1). The distributed setting
(2) is considered in the following Section 5."
ZEROSARAH ALGORITHM,0.12395833333333334,"4.1
ZeroSARAH ALGORITHM"
ZEROSARAH ALGORITHM,0.125,"We ﬁrst describe the proposed ZeroSARAH in Algorithm 2, which is a variant of SARAH (Nguyen
et al., 2017). To better compare with SARAH and ZeroSARAH, we also recall the original SARAH
in Algorithm 1."
ZEROSARAH ALGORITHM,0.12604166666666666,"Now, we highlight some points for the difference between SARAH and our ZeroSARAH:"
ZEROSARAH ALGORITHM,0.12708333333333333,"• SARAH requires the full gradient computations for every epoch (see Line 4 of Algorithm 1).
However, ZeroSARAH combines the past gradient estimator vk−1 with another estimator to avoid
periodically computing the full gradient. See the difference between Line 8 of Algorithm 1 and Line
5 of Algorithm 2 (also highlighted with blue color)."
ZEROSARAH ALGORITHM,0.128125,"• The gradient estimator vk in ZeroSARAH (Line 5 of Algorithm 2) does not require more stochastic
gradient computations compared with vk in SARAH (Line 8 of Algorithm 1) if the minibatch size
bk = b."
ZEROSARAH ALGORITHM,0.12916666666666668,"• The new gradient estimator vk of ZeroSARAH also leads to simpler algorithmic structure, i.e.,
single-loop in ZeroSARAH vs. double-loop in SARAH."
ZEROSARAH ALGORITHM,0.13020833333333334,Under review as a conference paper at ICLR 2022
ZEROSARAH ALGORITHM,0.13125,"• Moreover, the difference of gradient estimator vk also leads to different results in expectation, i.e.,
1) for SARAH: Ek[vk −∇f(xk)] = vk−1 −∇f(xk−1); 2) for ZeroSARAH: Ek[vk −∇f(xk)] =
(1 −λk)(vk−1 −∇f(xk−1))."
CONVERGENCE RESULTS FOR ZEROSARAH,0.13229166666666667,"4.2
CONVERGENCE RESULTS FOR ZeroSARAH"
CONVERGENCE RESULTS FOR ZEROSARAH,0.13333333333333333,"Now, we present the main convergence theorem (Theorem 1) of ZeroSARAH (Algorithm 2) for solv-
ing nonconvex ﬁnite-sum problems (1). Subsequently, we formulate two corollaries which present
the detailed convergence results by specifying the choice of parameters. In particular, we list the
results of these two Corollaries 1–2 in Table 1 for comparing with convergence results of previous
works."
CONVERGENCE RESULTS FOR ZEROSARAH,0.134375,"Theorem 1 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
1"
CONVERGENCE RESULTS FOR ZEROSARAH,0.13541666666666666,"L
 
1+√"
CONVERGENCE RESULTS FOR ZEROSARAH,0.13645833333333332,"Mk+1
 for any k ≥0,"
CONVERGENCE RESULTS FOR ZEROSARAH,0.1375,"where Mk+1 :=
2
λk+1bk+1 + 8λk+1n2"
CONVERGENCE RESULTS FOR ZEROSARAH,0.13854166666666667,"b3
k+1
. Moreover, let λ0 = 1, γ0 ≥
η0
2λ1 and α0 ≥2nλ1η0"
CONVERGENCE RESULTS FOR ZEROSARAH,0.13958333333333334,"b2
1
. Then the
following equation holds for ZeroSARAH (Algorithm 2) for solving problem (1), for any iteration
K ≥0:"
CONVERGENCE RESULTS FOR ZEROSARAH,0.140625,"E[∥∇f(bxK)∥2] ≤
2∆0
PK−1
k=0 ηk
+ (n −b0)(4γ0 + 2α0b0)G0"
CONVERGENCE RESULTS FOR ZEROSARAH,0.14166666666666666,"nb0
PK−1
k=0 ηk
.
(5)"
CONVERGENCE RESULTS FOR ZEROSARAH,0.14270833333333333,"Remark: Note that we can upper bound both terms on the right-hand side of (5). It means that there
is no convergence neighborhood of ZeroSARAH and hence, ZeroSARAH can ﬁnd an ϵ-approximate
solution for any ϵ > 0."
CONVERGENCE RESULTS FOR ZEROSARAH,0.14375,"In the following, we provide two detailed convergence results in Corollaries 1 and 2 by specifying
two kinds of parameter settings. Note that the algorithm computes full gradient in iteration k if the
minibatch bk = n. Our convergence results show that without computing any full gradients actually
does not hurt the convergence performance of algorithms (see Table 1)."
CONVERGENCE RESULTS FOR ZEROSARAH,0.14479166666666668,"In particular, we note that the second term of (5) will be deleted if we choose minibatch size b0 = n
for the initial point x0 (see Corollary 1 for more details). Here Corollary 1 only needs to compute the
full gradient once for the initialization, and does not compute any full gradients later (i.e., bk ≡√n
for all k > 0)."
CONVERGENCE RESULTS FOR ZEROSARAH,0.14583333333333334,"Also note that even if we choose b0 < n, we can also upper bound the second term of (5). It means
that ZeroSARAH can ﬁnd an ϵ-approximate solution without computing any full gradients even for
the initial point, i.e., minibatch size bk < n for all iterations k ≥0. For instance, we choose
bk ≡√n for all k ≥0 in Corollary 2 , i.e., ZeroSARAH never computes any full gradients even for
the initial point."
CONVERGENCE RESULTS FOR ZEROSARAH,0.146875,"Corollary 1 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
1
(1+
√"
CONVERGENCE RESULTS FOR ZEROSARAH,0.14791666666666667,"8)L for any k ≥0,"
CONVERGENCE RESULTS FOR ZEROSARAH,0.14895833333333333,minibatch size bk ≡√n and parameter λk = bk
CONVERGENCE RESULTS FOR ZEROSARAH,0.15,"2n for any k ≥1. Moreover, let b0 = n and λ0 = 1.
Then ZeroSARAH (Algorithm 2) can ﬁnd an ϵ-approximate solution for problem (1) such that"
CONVERGENCE RESULTS FOR ZEROSARAH,0.15104166666666666,E[∥∇f(bxK)∥2] ≤ϵ2
CONVERGENCE RESULTS FOR ZEROSARAH,0.15208333333333332,and the number of stochastic gradient computations can be bounded by
CONVERGENCE RESULTS FOR ZEROSARAH,0.153125,"#grad := K−1
X"
CONVERGENCE RESULTS FOR ZEROSARAH,0.15416666666666667,"k=0
bk ≤n + 2(1 +
√"
CONVERGENCE RESULTS FOR ZEROSARAH,0.15520833333333334,"8)√nL∆0
ϵ2
= O

n +
√nL∆0 ϵ2 
."
CONVERGENCE RESULTS FOR ZEROSARAH,0.15625,"Remark: In Corollary 1, ZeroSARAH only computes the full gradient ∇f(x0) = 1"
CONVERGENCE RESULTS FOR ZEROSARAH,0.15729166666666666,"n
Pn
i=1 ∇fi(x0)
once for the initial point x0, i.e., minibatch size b0 = n, and then bk ≡√n for all iterations k ≥1
in Algorithm 2."
CONVERGENCE RESULTS FOR ZEROSARAH,0.15833333333333333,"In the following Corollary 2, we show that ZeroSARAH without computing any full gradients even
for the initial point does not hurt its convergence performance."
CONVERGENCE RESULTS FOR ZEROSARAH,0.159375,Under review as a conference paper at ICLR 2022
CONVERGENCE RESULTS FOR ZEROSARAH,0.16041666666666668,"Corollary 2 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
1
(1+
√"
CONVERGENCE RESULTS FOR ZEROSARAH,0.16145833333333334,"8)L for any k ≥0,"
CONVERGENCE RESULTS FOR ZEROSARAH,0.1625,"minibatch size bk ≡√n for any k ≥0, and parameter λ0 = 1 and λk = bk"
CONVERGENCE RESULTS FOR ZEROSARAH,0.16354166666666667,"2n for any k ≥1. Then
ZeroSARAH (Algorithm 2) can ﬁnd an ϵ-approximate solution for problem (1) such that"
CONVERGENCE RESULTS FOR ZEROSARAH,0.16458333333333333,E[∥∇f(bxK)∥2] ≤ϵ2
CONVERGENCE RESULTS FOR ZEROSARAH,0.165625,and the number of stochastic gradient computations can be bounded by
CONVERGENCE RESULTS FOR ZEROSARAH,0.16666666666666666,"#grad = O
√n(L∆0 + G0) ϵ2 
."
CONVERGENCE RESULTS FOR ZEROSARAH,0.16770833333333332,"Note that G0 can be bounded by G0 ≤2Lb∆0 via L-smoothness Assumption 1, then we also have"
CONVERGENCE RESULTS FOR ZEROSARAH,0.16875,"#grad = O
√n(L∆0 + Lb∆0) ϵ2 
."
CONVERGENCE RESULTS FOR ZEROSARAH,0.16979166666666667,"Remark: In Corollary 2, ZeroSARAH never computes any full gradients even for the initial point,
i.e., minibatch size bk ≡√n for all iterations k ≥0 in Algorithm 2. If we consider L, ∆0, G0 or
b∆0 as constant values then the stochastic gradient complexity in Corollary 2 is #grad = O(
√n"
CONVERGENCE RESULTS FOR ZEROSARAH,0.17083333333333334,"ϵ2 ),
i.e., full gradient computations do not appear in ZeroSARAH (Algorithm 2) and the term ‘n’ also
does not appear in its convergence result. Also note that the parameter settings (i.e., {ηk}, {bk} and
{λk} in Algorithm 2) of Corollaries 1 and 2 are exactly the same except for b0 = n (in Corollary 1)
and b0 = √n (in Corollary 2). Moreover, the parameter settings (i.e., {ηk}, {bk} and {λk}) for
Corollaries 1 and 2 only require the values of L and n, which is the same as all previous algorithms.
If one further allows other values, e.g., ϵ, G0 or b∆0, for setting the initial b0, then the gradient
complexity can be further improved (see Appendix D for more details)."
D-ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.171875,"5
D-ZeroSARAH ALGORITHM AND ITS CONVERGENCE RESULTS"
D-ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.17291666666666666,"Now, we consider the distributed nonconvex problems (2), i.e., minx∈Rd

f(x) := 1"
D-ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.17395833333333333,"n
Pn
i=1 fi(x)"
D-ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.175,with fi(x) := 1
D-ZEROSARAH ALGORITHM AND ITS CONVERGENCE RESULTS,0.17604166666666668,"m
Pm
j=1 fi,j(x), where n denotes the number of clients/devices/machines, fi denotes
the loss associated with m data samples stored on client i."
D-ZEROSARAH ALGORITHM,0.17708333333333334,"5.1
D-ZeroSARAH ALGORITHM"
D-ZEROSARAH ALGORITHM,0.178125,"To solve distributed nonconvex problems (2), we propose a distributed variant of ZeroSARAH (called
D-ZeroSARAH) and describe it in Algorithm 3. Same as our ZeroSARAH, D-ZeroSARAH also does
not need to compute any full gradients at all. Avoiding any full gradient computations is important
especially in this distributed setting, periodic computation of full gradient across all n clients may be
impossible or unaffordable. Thus, we expect the proposed D-ZeroSARAH (Algorithm 3) will have a
practical impact in distributed and federated learning where full device participation is impractical."
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.17916666666666667,"5.2
CONVERGENCE RESULTS FOR D-ZeroSARAH"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18020833333333333,"Similar to ZeroSARAH in Section 4.2, we also ﬁrst present the main convergence theorem (The-
orem 2) of D-ZeroSARAH (Algorithm 3) for solving distributed nonconvex problems (2). Subse-
quently, we formulate two corollaries which present the detailed convergence results by specifying
the choice of parameters. In particular, we list the results of these two Corollaries 3–4 in Table 2
for comparing with convergence results of previous works. Note that here we use the smoothness
Assumption 2 instead of Assumption 1 for this distributed setting (2)."
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18125,"Theorem 2 Suppose that Assumption 2 holds. Choose stepsize ηk ≤
1"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18229166666666666,"L
 
1+√"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18333333333333332,"Wk+1
 for any k ≥0,"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.184375,"where Wk+1 :=
2
λk+1sk+1bk+1 + 8λk+1n2m2"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18541666666666667,"s3
k+1b3
k+1 . Moreover, let λ0 = 1 and θ0 :=
nm
(nm−1)λ1 + 4nmλ1s0b0"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18645833333333334,"s2
1b2
1
.
Then the following equation holds for D-ZeroSARAH (Algorithm 3) for solving distributed problem
(2), for any iteration K ≥0:"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.1875,"E[∥∇f(bxK)∥2] ≤
2∆0
PK−1
k=0 ηk
+ (nm −s0b0)η0θ0G′
0
nms0b0
PK−1
k=0 ηk
.
(6)"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18854166666666666,Under review as a conference paper at ICLR 2022
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.18958333333333333,Algorithm 3 Distributed ZeroSARAH (D-ZeroSARAH)
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.190625,"Input: initial point x0, parameters {ηk}, {sk}, {bk}, {λk}"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19166666666666668,1: x−1 = x0
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19270833333333334,"2: v−1 = 0, y−1
1
= y−1
2
= · · · = y−1
n
= 0
// no full gradient computation
3: for k = 0, 1, 2, . . . do
4:
Randomly sample a subset of clients Sk from n clients with size |Sk| = sk
5:
for each client i ∈Sk do
6:
Sample the data minibatch Ik
bi (with size |Ik
bi| = bk) from the m data samples in client i
7:
Compute its local minibatch gradient information:
gk
i,curr =
1
bk
P"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19375,"j∈Ik
bi
∇fi,j(xk), gk
i,prev =
1
bk
P"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19479166666666667,"j∈Ik
bi
∇fi,j(xk−1), yk
i,prev =
1
bk
P"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19583333333333333,"j∈Ik
bi
yk−1
i,j"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.196875,"yk
i,j =
∇fi,j(xk)
for j ∈Ik
bi
yk−1
i,j
for j /∈Ik
bi
, yk
i = 1 m m
P"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19791666666666666,"j=1
yk
i,j"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.19895833333333332,"8:
end for
9:
vk =
1
sk
P i∈Sk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.2," 
gk
i,curr −gk
i,prev

+ (1 −λk)vk−1 + λk 1 sk
P i∈Sk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20104166666666667," 
gk
i,prev −yk
i,prev

+ λkyk−1"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20208333333333334,"// no full gradient computations for vks
10:
xk+1 = xk −ηkvk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.203125,"11:
yk = 1 n nP"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20416666666666666,"i=1
yk
i
// here yk
i = yk−1
i
for client i /∈Sk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20520833333333333,12: end for
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20625,"Corollary 3 Suppose that Assumption 2 holds. Choose stepsize ηk ≤
1
(1+
√"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20729166666666668,"8)L for any k ≥0,"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.20833333333333334,"clients subset size sk ≡√n, minibatch size bk ≡√m and parameter λk = skbk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.209375,"2nm for any k ≥1.
Moreover, let s0 = n, b0 = m, and λ0 = 1. Then D-ZeroSARAH (Algorithm 3) can ﬁnd an
ϵ-approximate solution for distributed problem (2) such that"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21041666666666667,E[∥∇f(bxK)∥2] ≤ϵ2
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21145833333333333,and the number of stochastic gradient computations for each client can be bounded by
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.2125,"#grad = O

m +
rm n
L∆0 ϵ2 
."
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21354166666666666,"Corollary 4 Suppose that Assumption 2 holds. Choose stepsize ηk ≤
1
(1+
√"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21458333333333332,"8)L for any k ≥0,"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.215625,"clients subset size sk ≡√n and minibatch size bk ≡√m for any k ≥0, and parameter λ0 = 1 and
λk = skbk"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21666666666666667,"2nm for any k ≥1. Then D-ZeroSARAH (Algorithm 3) can ﬁnd an ϵ-approximate solution for
distributed problem (2) such that
E[∥∇f(bxK)∥2] ≤ϵ2"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21770833333333334,and the number of stochastic gradient computations for each client can be bounded by
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21875,"#grad = O
rm"
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.21979166666666666,"n
L∆0 + G′
0
ϵ2 
."
CONVERGENCE RESULTS FOR D-ZEROSARAH,0.22083333333333333,"Remark: Similar discussions and remarks of Theorem 1 and Corollaries 1–2 for ZeroSARAH in
Section 4.2 also hold for the results of D-ZeroSARAH (i.e., Theorem 2 and Corollaries 3–4)."
EXPERIMENTS,0.221875,"6
EXPERIMENTS"
EXPERIMENTS,0.22291666666666668,"Now, we present the numerical experiments for comparing the performance of our ZeroSARAH/D-
ZeroSARAH with previous algorithms. In the experiments, we consider the nonconvex robust linear
regression and binary classiﬁcation with two-layer neural networks, which are used in (Wang et al.,
2018; Zhao et al., 2010; Tran-Dinh et al., 2019). All datasets used in our experiments are down-
loaded from LIBSVM (Chang & Lin, 2011). The detailed description of these objective functions
and datasets are provided in Appendix A.1."
EXPERIMENTS,0.22395833333333334,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.225,"0
20
40
60
80
100
#grad/n 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTS,0.22604166666666667,Gradient norm || f(x)||
EXPERIMENTS,0.22708333333333333,"abalone (stepsize 
= 0.01)"
EXPERIMENTS,0.228125,"SARAH
ZeroSARAH"
EXPERIMENTS,0.22916666666666666,"0
20
40
60
80
100
120
140
#grad/n 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
EXPERIMENTS,0.23020833333333332,Gradient norm || f(x)||
EXPERIMENTS,0.23125,"triazines (stepsize 
= 0.01)"
EXPERIMENTS,0.23229166666666667,"SARAH
ZeroSARAH"
EXPERIMENTS,0.23333333333333334,"0
10
20
30
40
50
#grad/n 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
EXPERIMENTS,0.234375,Gradient norm || f(x)||
EXPERIMENTS,0.23541666666666666,"mg (stepsize 
= 0.01)"
EXPERIMENTS,0.23645833333333333,"SARAH
ZeroSARAH"
EXPERIMENTS,0.2375,"0
20
40
60
80
100
#grad/n 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
EXPERIMENTS,0.23854166666666668,Gradient norm || f(x)||
EXPERIMENTS,0.23958333333333334,"pyrim (stepsize 
= 0.01)"
EXPERIMENTS,0.240625,"SARAH
ZeroSARAH"
EXPERIMENTS,0.24166666666666667,"0
20
40
60
80
100
#grad/n 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTS,0.24270833333333333,Gradient norm || f(x)||
EXPERIMENTS,0.24375,"abalone (stepsize 
= 0.1)"
EXPERIMENTS,0.24479166666666666,"SARAH
ZeroSARAH"
EXPERIMENTS,0.24583333333333332,"0
20
40
60
80
100
120
140
#grad/n 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.246875,Gradient norm || f(x)||
EXPERIMENTS,0.24791666666666667,"triazines (stepsize 
= 0.1)"
EXPERIMENTS,0.24895833333333334,"SARAH
ZeroSARAH"
EXPERIMENTS,0.25,"0
10
20
30
40
50
#grad/n 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
EXPERIMENTS,0.25104166666666666,Gradient norm || f(x)||
EXPERIMENTS,0.2520833333333333,"mg (stepsize 
= 0.1)"
EXPERIMENTS,0.253125,"SARAH
ZeroSARAH"
EXPERIMENTS,0.25416666666666665,"0
20
40
60
80
100
#grad/n 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.2552083333333333,Gradient norm || f(x)||
EXPERIMENTS,0.25625,"pyrim (stepsize 
= 0.1)"
EXPERIMENTS,0.25729166666666664,"SARAH
ZeroSARAH"
EXPERIMENTS,0.25833333333333336,"0
20
40
60
80
100
#grad/n 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTS,0.259375,Gradient norm || f(x)||
EXPERIMENTS,0.2604166666666667,"abalone (stepsize 
= 1)"
EXPERIMENTS,0.26145833333333335,"SARAH
ZeroSARAH"
EXPERIMENTS,0.2625,"0
20
40
60
80
100
120
140
#grad/n 0.2 0.3 0.4 0.5 0.6 0.7 0.8"
EXPERIMENTS,0.2635416666666667,Gradient norm || f(x)||
EXPERIMENTS,0.26458333333333334,"triazines (stepsize 
= 1)"
EXPERIMENTS,0.265625,"SARAH
ZeroSARAH"
EXPERIMENTS,0.26666666666666666,"0
10
20
30
40
50
#grad/n 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6"
EXPERIMENTS,0.2677083333333333,Gradient norm || f(x)||
EXPERIMENTS,0.26875,"mg (stepsize 
= 1)"
EXPERIMENTS,0.26979166666666665,"SARAH
ZeroSARAH"
EXPERIMENTS,0.2708333333333333,"0
20
40
60
80
100
#grad/n 0.2 0.4 0.6 0.8 1.0"
EXPERIMENTS,0.271875,Gradient norm || f(x)||
EXPERIMENTS,0.27291666666666664,"pyrim (stepsize 
= 1)"
EXPERIMENTS,0.27395833333333336,"SARAH
ZeroSARAH"
EXPERIMENTS,0.275,"Figure 1: Performance between SARAH and ZeroSARAH under different datasets (columns) with
respect to different stepsizes (rows).
In rows, we have stepsizes 0.01, 0.1, 1, respectively.
In
columns, we have LIBSVM datasates ‘abalone’, ‘triazines’, ‘mg’, ‘pyrim’, respectively."
EXPERIMENTS,0.2760416666666667,"In Figure 1, the x-axis and y-axis represent the number of stochastic gradient computations and
the norm of gradient, respectively. The numerical results presented in Figure 1 are conducted on
different datasets with different stepsizes. Regrading the parameter settings, we directly use the
theoretical values according to the theorems or corollaries of SARAH and ZeroSARAH, i.e., we do
not tune the parameters. Concretely, for SARAH (Algorithm 1), the epoch length l = √n and the
minibatch size b = √n (see Theorem 6 in Pham et al. (2019)). For ZeroSARAH (Algorithm 2),
the minibatch size bk ≡√n for any k ≥0, λ0 = 1 and λk =
bk
2n ≡
1
2√n for any k ≥1
(see our Corollary 2). Note that there is no epoch length l for ZeroSARAH since it is a loopless
(single-loop) algorithm while SARAH requires l for setting the length of its inner-loop (see Line 6
of Algorithm 1). For the stepsize η, both SARAH and ZeroSARAH adopt the same constant stepsize
η = O( 1"
EXPERIMENTS,0.27708333333333335,"L). However the smooth parameter L is not known in the experiments, thus here we use
three stepsizes, i.e., η = 0.01, 0.1, 1."
EXPERIMENTS,0.278125,"Remark: The experimental results validate our theoretical convergence results (our ZeroSARAH can
be slightly better than SARAH (see Table 1)) and conﬁrm the practical superiority of ZeroSARAH
(avoid any full gradient computations). To demonstrate the full gradient computations in Figure 1,
we point out that each circle marker in the curve of SARAH (blue curves) denotes a full gradient
computation in SARAH. We emphasize that our ZeroSARAH never computes any full gradients. Note
that in this section we only present the experiments for the standard/centralized setting (1). Similar
experiments in the distributed setting (2) are provided in Appendix A.2, e.g., Figure 2 demonstrates
similar performance between distributed SARAH and distributed ZeroSARAH."
CONCLUSION,0.2791666666666667,"7
CONCLUSION"
CONCLUSION,0.28020833333333334,"In this paper, we propose ZeroSARAH and its distributed variant D-ZeroSARAH algorithms for solv-
ing both standard and distributed nonconvex ﬁnite-sum problems (1) and (2). In particular, they are
the ﬁrst variance-reduced algorithms which do not require any full gradient computations, not even
for the initial point. Moreover, our new algorithms can achieve better theoretical results than previ-
ous state-of-the-art results in certain regimes. While the numerical performance of our algorithms
is also comparable/better than previous state-of-the-art algorithms, the main advantage of our algo-
rithms is that they do not need to compute any full gradients. This characteristic can lead to practical
signiﬁcance of our algorithms since periodic computation of full gradient over all data samples from
all clients usually is impractical and unaffordable."
CONCLUSION,0.28125,Under review as a conference paper at ICLR 2022
REFERENCES,0.28229166666666666,REFERENCES
REFERENCES,0.2833333333333333,"Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699–707, 2016."
REFERENCES,0.284375,"Shicong Cen, Huishuai Zhang, Yuejie Chi, Wei Chen, and Tie-Yan Liu. Convergence of distributed
stochastic variance reduced methods without sampling extra data. IEEE Transactions on Signal
Processing, 68:3976–3989, 2020."
REFERENCES,0.28541666666666665,"Chih-Chung Chang and Chih-Jen Lin.
LIBSVM: a library for support vector machines.
ACM
transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011."
REFERENCES,0.2864583333333333,"Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. In Advances in Neural Informa-
tion Processing Systems, pp. 687–697, 2018."
REFERENCES,0.2875,"Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized SVRG: Simple variance reduction
for nonconvex optimization. In Conference on Learning Theory, pp. 1394–1448, 2019."
REFERENCES,0.28854166666666664,"Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013."
REFERENCES,0.28958333333333336,"Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267–305, 2016."
REFERENCES,0.290625,"Samuel Horv´ath, Lihua Lei, Peter Richt´arik, and Michael I Jordan. Adaptivity of stochastic gradient
methods for nonconvex optimization. arXiv preprint arXiv:2002.05359, 2020."
REFERENCES,0.2916666666666667,"Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations
and Trends R⃝in Machine Learning, 10(3-4):142–336, 2017."
REFERENCES,0.29270833333333335,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020."
REFERENCES,0.29375,"Ahmed Khaled and Peter Richt´arik. Better theory for SGD in the nonconvex world. arXiv preprint
arXiv:2002.03329, 2020."
REFERENCES,0.2947916666666667,"Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex ﬁnite-sum optimization via
SCSG methods. In Advances in Neural Information Processing Systems, pp. 2345–2355, 2017."
REFERENCES,0.29583333333333334,"Zhize Li. SSRGD: Simple stochastic recursive gradient descent for escaping saddle points. In
Advances in Neural Information Processing Systems, pp. 1521–1531, 2019."
REFERENCES,0.296875,"Zhize Li. A short note of PAGE: Optimal convergence rates for nonconvex optimization. arXiv
preprint arXiv:2106.09663, 2021."
REFERENCES,0.29791666666666666,"Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5569–5579, 2018."
REFERENCES,0.2989583333333333,"Zhize Li and Peter Richt´arik.
A uniﬁed analysis of stochastic gradient methods for nonconvex
federated optimization. arXiv preprint arXiv:2006.07013, 2020."
REFERENCES,0.3,"Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt´arik. PAGE: A simple and optimal prob-
abilistic gradient estimator for nonconvex optimization. In International Conference on Machine
Learning, pp. 6286–6295. PMLR, arXiv:2008.10898, 2021."
REFERENCES,0.30104166666666665,"Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer, 2004."
REFERENCES,0.3020833333333333,"Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak´aˇc. SARAH: A novel method for ma-
chine learning problems using stochastic recursive gradient. In International Conference on Ma-
chine Learning, pp. 2613–2621, 2017."
REFERENCES,0.303125,Under review as a conference paper at ICLR 2022
REFERENCES,0.30416666666666664,"Nhan H Pham, Lam M Nguyen, Dzung T Phan, and Quoc Tran-Dinh.
ProxSARAH: An efﬁ-
cient algorithmic framework for stochastic composite nonconvex optimization. arXiv preprint
arXiv:1902.05679, 2019."
REFERENCES,0.30520833333333336,"Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnab´as P´oczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314–
323, 2016."
REFERENCES,0.30625,"Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: from theory to algo-
rithms. Cambridge University Press, 2014."
REFERENCES,0.3072916666666667,"Haoran Sun, Songtao Lu, and Mingyi Hong. Improving the sample and communication complexity
for decentralized non-convex optimization: Joint gradient estimation and tracking. In Interna-
tional Conference on Machine Learning, pp. 9217–9228. PMLR, 2020."
REFERENCES,0.30833333333333335,"Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. Hybrid stochastic gradi-
ent descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920,
2019."
REFERENCES,0.309375,"Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. SpiderBoost and momentum:
Faster stochastic variance reduction algorithms. arXiv preprint arXiv:1810.10690, 2018."
REFERENCES,0.3104166666666667,"Haoyu Zhao, Zhize Li, and Peter Richt´arik. FedPAGE: A fast local stochastic gradient method for
communication-efﬁcient federated learning. arXiv preprint arXiv:2108.04755, 2021."
REFERENCES,0.31145833333333334,"Lei Zhao, Musa Mammadov, and John Yearwood. From convex to nonconvex: a loss function anal-
ysis for binary classiﬁcation. In 2010 IEEE International Conference on Data Mining Workshops,
pp. 1281–1288. IEEE, 2010."
REFERENCES,0.3125,"Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 3925–3936, 2018."
REFERENCES,0.31354166666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.3145833333333333,"A
EXTRA EXPERIMENTS"
REFERENCES,0.315625,"In this appendix, we ﬁrst describe the details of the objective functions and datasets used in our
experiments in Appendix A.1. Then in Appendix A.2, we present the experimental results in the
distributed setting (2)."
REFERENCES,0.31666666666666665,"A.1
OBJECTIVE FUNCTIONS AND DATASETS IN EXPERIMENTS"
REFERENCES,0.3177083333333333,The nonconvex robust linear regression problem (used in Wang et al. (2018)) is:
REFERENCES,0.31875,"min
x∈Rd ("
REFERENCES,0.31979166666666664,"f(x) := 1 n n
X"
REFERENCES,0.32083333333333336,"i=1
ℓ(bi −xT ai) ) ,
(7)"
REFERENCES,0.321875,where the nonconvex loss function ℓ(x) := log( x2
REFERENCES,0.3229166666666667,2 + 1).
REFERENCES,0.32395833333333335,"The binary classiﬁcation with two-layer neural networks (used in Zhao et al. (2010); Tran-Dinh et al.
(2019)) is:"
REFERENCES,0.325,"min
x∈Rd ("
REFERENCES,0.3260416666666667,"f(x) := 1 n n
X"
REFERENCES,0.32708333333333334,"i=1
ℓ
 
aT
i x, bi

+ λ"
REFERENCES,0.328125,"2 ∥x∥2
) (8)"
REFERENCES,0.32916666666666666,"where {ai} ∈Rd, bi ∈{−1, 1}, λ ≥0 is an ℓ2-regularization parameter, and the function ℓis
deﬁned as"
REFERENCES,0.3302083333333333,"ℓ(x, y) :=

1 −
1
1 + exp(−xy) 2
."
REFERENCES,0.33125,"All datasets are downloaded from LIBSVM (Chang & Lin, 2011). The summary of datasets infor-
mation is provided in the following Table 3."
REFERENCES,0.33229166666666665,"Table 3: Metadata of datasets
Dataset
n (# of datapoints)
d (# of features)
a9a
32561
123
abalone
4177
8
mg
1385
6
mushrooms
8124
112
phishing
11055
68
pyrim
74
27
triazines
186
60
w8a
49749
300"
REFERENCES,0.3333333333333333,"A.2
EXPERIMENTS FOR THE DISTRIBUTED SETTING"
REFERENCES,0.334375,"Before presenting the experimental results in the distributed setting (2), we also need a distributed
variant of SARAH-type methods in order to compare with our distributed variant of ZeroSARAH.
Here we describe one possible version in Algorithm 4. Note that distributed SARAH also requires
to periodically computes full gradients (see Line 7 of Algorithm 4), but it is not required by our
D-ZeroSARAH (Algorithm 3)."
REFERENCES,0.33541666666666664,"In order to mimic distributed setup, we represented clients as parallel processes. We implement the
training process using Python 3.8.8, mpi4py library. We run it on the workstation with 48 Cores,
Intel(R) Xeon(R) Gold 6246 CPU @ 3.30GHz. We partition the dataset among 10 threads; having
M datapoints and n clients, k-thread gets datapoints in range k⌊M"
REFERENCES,0.33645833333333336,"n ⌋+ 1, . . . , (k + 1)⌊M"
REFERENCES,0.3375,"n ⌋. In case
of n⌊M"
REFERENCES,0.3385416666666667,"n ⌋+ 1 ≤M, datapoints n⌊M"
REFERENCES,0.33958333333333335,"n ⌋+ 1, . . . M are ignored."
REFERENCES,0.340625,Under review as a conference paper at ICLR 2022
REFERENCES,0.3416666666666667,Algorithm 4 Distributed SARAH-type methods (one possible version)
REFERENCES,0.34270833333333334,"Input: initial point x0, epoch length l, stepsize η, client minibatch size s, data minibatch size b"
REFERENCES,0.34375,1: x−1 = x0
REFERENCES,0.34479166666666666,"2: for k = 0, 1, 2, . . . do
3:
if k mod l = 0 then
4:
for each client i ∈{1, . . . , n} do"
REFERENCES,0.3458333333333333,"5:
Compute full gradient of each client: gk
i = 1 m m
P"
REFERENCES,0.346875,"j=1
∇fi,j(xk)
// = ∇fi(xk)"
REFERENCES,0.34791666666666665,"6:
end for"
REFERENCES,0.3489583333333333,"7:
vk = 1 n nP"
REFERENCES,0.35,"i=1
gk
i
// full gradient computations"
REFERENCES,0.35104166666666664,"8:
else
9:
Randomly sample a subset of clients Sk from n clients with size |Sk| = s
10:
for each client i ∈Sk do
11:
Sample minibatch Ik
i of size |Ik
i | = b (from the m data samples in client i)
12:
Compute the local minibatch gradient information:
gk
i,curr = 1 b
P"
REFERENCES,0.35208333333333336,"j∈Ik
i
∇fi,j(xk)
and
gk
i,prev = 1 b
P"
REFERENCES,0.353125,"j∈Ik
i
∇fi,j(xk−1)"
REFERENCES,0.3541666666666667,"13:
end for
14:
vk = 1 s
P i∈Sk"
REFERENCES,0.35520833333333335," 
gk
i,curr −gk
i,prev

+ vk−1"
REFERENCES,0.35625,"15:
end if
16:
xk+1 = xk −ηkvk"
REFERENCES,0.3572916666666667,17: end for
REFERENCES,0.35833333333333334,"0
2000
4000
6000
8000
10000
Iteration 10
5 10
4 10
3 10
2 10
1"
REFERENCES,0.359375,|| f(x)||2 w8a
REFERENCES,0.36041666666666666,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.3614583333333333,"0
2000
4000
6000
8000
10000
Iteration 10
4 10
3"
REFERENCES,0.3625,|| f(x)||2
REFERENCES,0.36354166666666665,phishing
REFERENCES,0.3645833333333333,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.365625,"0
2000
4000
6000
8000
10000
Iteration 10
3 10
2 10
1"
REFERENCES,0.36666666666666664,|| f(x)||2
REFERENCES,0.36770833333333336,mushrooms
REFERENCES,0.36875,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.3697916666666667,"0
100
200
300
400
Iteration 10
8 10
6 10
4 10
2"
REFERENCES,0.37083333333333335,|| f(x)||2 a9a
REFERENCES,0.371875,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.3729166666666667,"0
2
4
6
8
10
12
# epochs 10
5 10
4 10
3 10
2 10
1"
REFERENCES,0.37395833333333334,|| f(x)||2 w8a
REFERENCES,0.375,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.37604166666666666,"0
5
10
15
20
25
# epochs 10
4 10
3"
REFERENCES,0.3770833333333333,|| f(x)||2
REFERENCES,0.378125,phishing
REFERENCES,0.37916666666666665,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.3802083333333333,"0
5
10
15
20
25
30
# epochs 10
3 10
2 10
1"
REFERENCES,0.38125,|| f(x)||2
REFERENCES,0.38229166666666664,mushrooms
REFERENCES,0.38333333333333336,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.384375,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
# epochs 10
8 10
6 10
4 10
2"
REFERENCES,0.3854166666666667,|| f(x)||2 a9a
REFERENCES,0.38645833333333335,"Zero-SARAH
Zero-SARAH
Zero-SARAH
SARAH
SARAH
SARAH"
REFERENCES,0.3875,"Figure 2: Performance between distributed SARAH and distributed ZeroSARAH under different
datasets (columns). We show convergence using the theoretical stepsize in red lines; in blue lines,
we scale it by factor ×3; in green lines, we scale it by factor ×9."
REFERENCES,0.3885416666666667,"In Figure 2, we present numerical results for the distributed setting. The solid lines and dashed lines
denote the distributed ZeroSARAH and distributed SARAH, respectively. Regrading the parameter
settings, we also use the theoretical values according to the theorems or corollaries. In particular,
from Tran-Dinh et al. (2019), we know that the smoothness constant L ≈0.15405 maxi ∥ai∥2 + λ
for objective function (8). We choose the regularizer parameter to be λ = 0.15405·10−6 maxi ∥ai∥2.
In order to obtain comparable plots similar to the standard/centralized setting (Figure 1), we also
use multiple stepsizes. We choose the theoretical stepsize from Corollary 4 (i.e.,
1
(1+
√"
REFERENCES,0.38958333333333334,"8)L) scaled
by factors of ×1 (red curves), ×3 (blue curves), ×9 (green curves), respectively."
REFERENCES,0.390625,"Remark: Similar to Figure 1, the experimental results in Figure 2 also validate our theoretical
convergence results (distributed ZeroSARAH (D-ZeroSARAH) can be slightly better than distributed
SARAH (see Table 2)) and conﬁrm the practical superiority of D-ZeroSARAH (avoid any full gradi-
ent computations) for the distributed setting (2)."
REFERENCES,0.39166666666666666,Under review as a conference paper at ICLR 2022
REFERENCES,0.3927083333333333,"B
MISSING PROOFS FOR ZeroSARAH"
REFERENCES,0.39375,"In this appendix, we provide the missing proofs for the standard nonconvex setting (1). Concretely,
we provide the detailed proofs for Theorem 1 and Corollaries 1–2 of ZeroSARAH in Section 4."
REFERENCES,0.39479166666666665,"B.1
PROOF OF THEOREM 1"
REFERENCES,0.3958333333333333,"First, we need a useful lemma in Li et al. (2021) which describes the relation between the function
values after and before a gradient descent step."
REFERENCES,0.396875,"Lemma 1 (Li et al. (2021)) Suppose that function f is L-smooth and let xk+1 := xk −ηkvk. Then
for any vk ∈Rd and ηk > 0, we have"
REFERENCES,0.39791666666666664,f(xk+1) ≤f(xk) −ηk
REFERENCES,0.39895833333333336,"2 ∥∇f(xk)∥2 −
 1"
REFERENCES,0.4,"2ηk
−L 2"
REFERENCES,0.4010416666666667,"
∥xk+1 −xk∥2 + ηk"
REFERENCES,0.40208333333333335,"2 ∥vk −∇f(xk)∥2.
(9)"
REFERENCES,0.403125,"Then, we provide the following Lemma 2 to bound the last variance term of (9)."
REFERENCES,0.4041666666666667,"Lemma 2 Suppose that Assumption 1 holds. The gradient estimator vk is deﬁned in Line 5 of
Algorithm 2, then we have"
REFERENCES,0.40520833333333334,Ek[∥vk −∇f(xk)∥2] ≤(1 −λk)2∥vk−1 −∇f(xk−1)∥2 + 2L2
REFERENCES,0.40625,"bk
∥xk −xk−1∥2"
REFERENCES,0.40729166666666666,"+ 2λ2
k
bk"
N,0.4083333333333333,"1
n n
X"
N,0.409375,"j=1
∥∇fj(xk−1) −yk−1
j
∥2.
(10)"
N,0.41041666666666665,"Proof of Lemma 2. First, according to the gradient estimator vk of ZeroSARAH (see Line 5 of
Algorithm 2), we know that"
N,0.4114583333333333,vk = 1 bk X
N,0.4125,"i∈Ik
b"
N,0.41354166666666664," 
∇fi(xk) −∇fi(xk−1)

+ (1 −λk)vk−1 + λk
 1 bk X"
N,0.41458333333333336,"i∈Ik
b"
N,0.415625," 
∇fi(xk−1) −yk−1
i

+ 1 n n
X"
N,0.4166666666666667,"j=1
yk−1
j
 (11)"
N,0.41770833333333335,Now we bound the variance as follows:
N,0.41875,Ek[∥vk −∇f(xk)∥2]
N,0.4197916666666667,"(11)
= Ek"
N,0.42083333333333334,"""
1
bk X"
N,0.421875,"i∈Ik
b"
N,0.42291666666666666," 
∇fi(xk) −∇fi(xk−1)

+ (1 −λk)vk−1"
N,0.4239583333333333,"+ λk
 1 bk X"
N,0.425,"i∈Ik
b"
N,0.42604166666666665," 
∇fi(xk−1) −yk−1
i

+ 1 n n
X"
N,0.4270833333333333,"j=1
yk−1
j

−∇f(xk) 2# = Ek"
N,0.428125,"""
1
bk X"
N,0.42916666666666664,"i∈Ik
b"
N,0.43020833333333336," 
∇fi(xk) −∇fi(xk−1)

+ ∇f(xk−1) −∇f(xk) + (1 −λk)(vk−1 −∇f(xk−1))"
N,0.43125,"+ λk
 1 bk X"
N,0.4322916666666667,"i∈Ik
b"
N,0.43333333333333335," 
∇fi(xk−1) −yk−1
i

+ 1 n n
X"
N,0.434375,"j=1
yk−1
j
−∇f(xk−1)
 2# = Ek"
N,0.4354166666666667,"""
1
bk X"
N,0.43645833333333334,"i∈Ik
b"
N,0.4375," 
∇fi(xk) −∇fi(xk−1)

+ ∇f(xk−1) −∇f(xk)"
N,0.43854166666666666,"+ λk
 1 bk X"
N,0.4395833333333333,"i∈Ik
b"
N,0.440625," 
∇fi(xk−1) −yk−1
i

+ 1 n n
X"
N,0.44166666666666665,"j=1
yk−1
j
−∇f(xk−1)
 2#"
N,0.4427083333333333,+ (1 −λk)2∥vk−1 −∇f(xk−1)∥2
N,0.44375,Under review as a conference paper at ICLR 2022 ≤2Ek
N,0.44479166666666664,"""
1
bk X"
N,0.44583333333333336,"i∈Ik
b"
N,0.446875," 
∇fi(xk) −∇fi(xk−1)

+ ∇f(xk−1) −∇f(xk) 2# + 2Ek "" λ2
k"
BK,0.4479166666666667,"1
bk X"
BK,0.44895833333333335,"i∈Ik
b"
BK,0.45," 
∇fi(xk−1) −yk−1
i

+ 1 n n
X"
BK,0.4510416666666667,"j=1
yk−1
j
−∇f(xk−1) 2#"
BK,0.45208333333333334,+ (1 −λk)2∥vk−1 −∇f(xk−1)∥2 ≤2L2
BK,0.453125,"bk
∥xk −xk−1∥2 + 2λ2
k
bk"
N,0.45416666666666666,"1
n n
X"
N,0.4552083333333333,"j=1
∥∇fj(xk−1) −yk−1
j
∥2 + (1 −λk)2∥vk−1 −∇f(xk−1)∥2, (12)"
N,0.45625,"where (12) uses the L-smoothness Assumption 1 and the fact that E[∥x −Ex∥2] ≤E[∥x∥2], for any
random variable x.
□"
N,0.45729166666666665,"To deal with the last term of (10), we use the following Lemma 3."
N,0.4583333333333333,"Lemma 3 Suppose that Assumption 1 holds. The update of {yk
i } is deﬁned in Line 7 of Algorithm 2,
then we have, for ∀βk > 0, Ek ""
1
n n
X"
N,0.459375,"j=1
∥∇fj(xk) −yk
j ∥2
#"
N,0.46041666666666664,"≤
 
1 −bk"
N,0.46145833333333336,"n

(1 + βk) 1 n n
X"
N,0.4625,"j=1
∥∇fj(xk−1) −yk−1
j
∥2"
N,0.4635416666666667,"+
 
1 −bk"
N,0.46458333333333335,"n
 
1 + 1 βk"
N,0.465625,"
L2∥xk −xk−1∥2.
(13)"
N,0.4666666666666667,"Proof of Lemma 3. According to the update of {yk
i } (see Line 7 of Algorithm 2), we have Ek ""
1
n n
X"
N,0.46770833333333334,"j=1
∥∇fj(xk) −yk
j ∥2
#"
N,0.46875,"=
 
1 −bk n
 1 n n
X"
N,0.46979166666666666,"j=1
∥∇fj(xk) −yk−1
j
∥2
(14)"
N,0.4708333333333333,"=
 
1 −bk n
 1 n n
X"
N,0.471875,"j=1
∥∇fj(xk) −∇fj(xk−1) + ∇fj(xk−1) −yk−1
j
∥2"
N,0.47291666666666665,"≤
 
1 −bk"
N,0.4739583333333333,"n

(1 + βk) 1 n n
X"
N,0.475,"j=1
∥∇fj(xk−1) −yk−1
j
∥2 +
 
1 −bk"
N,0.47604166666666664,"n
 
1 + 1 βk"
N,0.47708333333333336,"
L2∥xk −xk−1∥2, (15)"
N,0.478125,"where (14) uses the update of {yk
j } (see Line 7 of Algorithm 2), and (15) uses Young’s inequality
and L-smoothness Assumption 1.
□"
N,0.4791666666666667,"Now we combine Lemmas 1–3 (i.e., (9), (10) and (13)) to prove Theorem 1."
N,0.48020833333333335,"Proof of Theorem 1. First, we take expectation to obtain E """
N,0.48125,"f(xk+1) −f ∗+

γk −ηk 2"
N,0.4822916666666667,"
∥vk −∇f(xk)∥2 +
 1"
N,0.48333333333333334,"2ηk
−L 2"
N,0.484375,"
∥xk+1 −xk∥2 + αk
1
n n
X"
N,0.48541666666666666,"j=1
∥∇fj(xk) −yk
j ∥2
# ≤E """
N,0.4864583333333333,f(xk) −f ∗−ηk
N,0.4875,2 ∥∇f(xk)∥2 + γk(1 −λk)2∥vk−1 −∇f(xk−1)∥2
N,0.48854166666666665,+ 2γkL2
N,0.4895833333333333,"bk
∥xk −xk−1∥2 + 2γkλ2
k
bk"
N,0.490625,"1
n n
X"
N,0.49166666666666664,"j=1
∥∇fj(xk−1) −yk−1
j
∥2"
N,0.49270833333333336,+ αk(1 −bk
N,0.49375,n )(1 + 1
N,0.4947916666666667,"βk
)L2∥xk −xk−1∥2 + αk(1 −bk"
N,0.49583333333333335,"n )(1 + βk) 1 n n
X"
N,0.496875,"j=1
∥∇f(xk−1) −yk−1
j
∥2
#"
N,0.4979166666666667,"Under review as a conference paper at ICLR 2022 = E """
N,0.49895833333333334,f(xk) −f ∗−ηk
N,0.5,2 ∥∇f(xk)∥2 + γk(1 −λk)2∥vk−1 −∇f(xk−1)∥2
N,0.5010416666666667,"+
2γkL2"
N,0.5020833333333333,"bk
+ αk(1 −bk"
N,0.503125,n )(1 + 1
N,0.5041666666666667,"βk
)L2
∥xk −xk−1∥2"
N,0.5052083333333334,"+
2γkλ2
k
bk
+ αk(1 −bk"
N,0.50625,"n )(1 + βk)
 1 n n
X"
N,0.5072916666666667,"j=1
∥∇fj(xk−1) −yk−1
j
∥2
#"
N,0.5083333333333333,".
(16)"
N,0.509375,Now we choose appropriate parameters. Let γk = ηk−1
N,0.5104166666666666,"2λk and γk ≤γk−1, then γk(1−λk)2 ≤γk−1− ηk−1"
N,0.5114583333333333,2 . Let βk = bk
N,0.5125,"2n, αk = 2nλkηk−1"
N,0.5135416666666667,"b2
k
and αk ≤αk−1, we have 2γkλ2
k
bk
+αk(1−bk"
N,0.5145833333333333,n )(1+βk) ≤αk−1.
N,0.515625,We also have 2γkL2
N,0.5166666666666667,"bk
+ αk(1 −bk"
N,0.5177083333333333,"n )(1 +
1
βk )L2 ≤
1
2ηk−1 −L"
BY FURTHER LETTING STEPSIZE,0.51875,2 by further letting stepsize
BY FURTHER LETTING STEPSIZE,0.5197916666666667,"ηk−1 ≤
1
L
 
1 + √Mk
,
(17)"
BY FURTHER LETTING STEPSIZE,0.5208333333333334,"where Mk :=
2
λkbk + 8λkn2"
BY FURTHER LETTING STEPSIZE,0.521875,"b3
k
."
BY FURTHER LETTING STEPSIZE,0.5229166666666667,"Summing up (16) from k = 1 to K −1, we get 0 ≤E """
BY FURTHER LETTING STEPSIZE,0.5239583333333333,"f(x1) −f ∗− K−1
X k=1 ηk"
BY FURTHER LETTING STEPSIZE,0.525,2 ∥∇f(xk)∥2 + γ1(1 −λ1)2∥v0 −∇f(x0)∥2
BY FURTHER LETTING STEPSIZE,0.5260416666666666,"+
2γ1L2"
BY FURTHER LETTING STEPSIZE,0.5270833333333333,"b1
+ α1(1 −b1"
BY FURTHER LETTING STEPSIZE,0.528125,n )(1 + 2n
BY FURTHER LETTING STEPSIZE,0.5291666666666667,"b1
)L2
∥x1 −x0∥2"
BY FURTHER LETTING STEPSIZE,0.5302083333333333,"+
2γ1λ2
1
b1
+ α1(1 −b1"
BY FURTHER LETTING STEPSIZE,0.53125,n )(1 + b1
BY FURTHER LETTING STEPSIZE,0.5322916666666667,"2n)
 1 n n
X"
BY FURTHER LETTING STEPSIZE,0.5333333333333333,"j=1
∥∇fj(x0) −y0
j ∥2
#"
BY FURTHER LETTING STEPSIZE,0.534375,".
(18)"
BY FURTHER LETTING STEPSIZE,0.5354166666666667,"For k = 0, we directly uses (9), i.e.,"
BY FURTHER LETTING STEPSIZE,0.5364583333333334,"E[f(x1) −f ∗] ≤E
h
f(x0) −f ∗−η0"
BY FURTHER LETTING STEPSIZE,0.5375,"2 ∥∇f(x0)∥2 −
 1"
BY FURTHER LETTING STEPSIZE,0.5385416666666667,"2η0
−L 2"
BY FURTHER LETTING STEPSIZE,0.5395833333333333,"
∥x1 −x0∥2 + η0"
BY FURTHER LETTING STEPSIZE,0.540625,"2 ∥v0 −∇f(x0)∥2i
. (19)"
BY FURTHER LETTING STEPSIZE,0.5416666666666666,"Now, we combine (18) and (19) to get E"
BY FURTHER LETTING STEPSIZE,0.5427083333333333,""" K−1
X k=0 ηk"
BY FURTHER LETTING STEPSIZE,0.54375,"2 ∥∇f(xk)∥2
#"
BY FURTHER LETTING STEPSIZE,0.5447916666666667,"≤E
h
f(x0) −f ∗+
 
γ1(1 −λ1)2 + η0"
BY FURTHER LETTING STEPSIZE,0.5458333333333333,"2

∥v0 −∇f(x0)∥2"
BY FURTHER LETTING STEPSIZE,0.546875,"+
2γ1λ2
1
b1
+ α1(1 −b1"
BY FURTHER LETTING STEPSIZE,0.5479166666666667,n )(1 + b1
BY FURTHER LETTING STEPSIZE,0.5489583333333333,"2n)
 1 n n
X"
BY FURTHER LETTING STEPSIZE,0.55,"j=1
∥∇fj(x0) −y0
j ∥2i
(20)"
BY FURTHER LETTING STEPSIZE,0.5510416666666667,"≤E
h
f(x0) −f ∗+ η0(1 −λ1(1 −λ1))"
BY FURTHER LETTING STEPSIZE,0.5520833333333334,"2λ1
∥v0 −∇f(x0)∥2 + 2nλ1η0 b2
1"
N,0.553125,"1
n n
X"
N,0.5541666666666667,"j=1
∥∇fj(x0) −y0
j ∥2i (21)"
N,0.5552083333333333,"≤E
h
f(x0) −f ∗+ γ0∥v0 −∇f(x0)∥2 + α0
1
n n
X"
N,0.55625,"j=1
∥∇fj(x0) −y0
j ∥2i
(22)"
N,0.5572916666666666,"≤f(x0) −f ∗+ γ0
n −b0
(n −1)b0"
N,0.5583333333333333,"1
n n
X"
N,0.559375,"j=1
∥∇fj(x0)∥2 + α0(1 −b0 n ) 1 n n
X"
N,0.5604166666666667,"j=1
∥∇fj(x0)∥2
(23)"
N,0.5614583333333333,"= f(x0) −f ∗+

γ0
n −b0
(n −1)b0
+ α0(1 −b0"
N,0.5625,"n )
 1 n n
X"
N,0.5635416666666667,"j=1
∥∇fj(x0)∥2,
(24)"
N,0.5645833333333333,Under review as a conference paper at ICLR 2022
N,0.565625,"where (20) follows from the deﬁnition of η0 in (17), (21) uses γ1 =
η0
2λ1 and α1 = 2nλ1η0"
N,0.5666666666666667,"b2
1
, (22)"
N,0.5677083333333334,"holds by choosing γ0 ≥
η0
2λ1 ≥
η0(1−λ1(1−λ1))"
N,0.56875,"2λ1
and α0 ≥
2nλ1η0"
N,0.5697916666666667,"b2
1
, and (23) uses λ0 = 1. By"
N,0.5708333333333333,"randomly choosing bxK from {xk}K−1
k=0 with probability ηk/ PK−1
t=0 ηt for xk, (24) turns to"
N,0.571875,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+
2
PK−1
k=0 ηk"
N,0.5729166666666666,"
γ0
n −b0
(n −1)b0
+ α0(1 −b0"
N,0.5739583333333333,"n )
 1 n n
X"
N,0.575,"j=1
∥∇fj(x0)∥2"
N,0.5760416666666667,"≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (n −b0)(4γ0 + 2α0b0)"
N,0.5770833333333333,"nb0
PK−1
k=0 ηk"
N,0.578125,"1
n n
X"
N,0.5791666666666667,"j=1
∥∇fj(x0)∥2.
(25) □"
N,0.5802083333333333,"B.2
PROOFS OF COROLLARIES 1 AND 2"
N,0.58125,"Now, we prove the detailed convergence results in Corollaries 1–2 with speciﬁc parameter settings."
N,0.5822916666666667,Proof of Corollary 1. First we know that (25) with b0 = n turns to
N,0.5833333333333334,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
.
(26)"
N,0.584375,Then if we set λk = bk
N,0.5854166666666667,"2n and bk ≡√n for any k ≥1, then we know that Mk :=
2
λkbk + 8λkn2"
N,0.5864583333333333,"b3
k
≡8"
N,0.5875,"and thus the stepsize ηk ≤
1"
N,0.5885416666666666,"L
 
1+√"
N,0.5895833333333333,"Mk+1
 ≡
1
(1+
√"
N,0.590625,"8)L for any k ≥0. By plugging ηk ≤
1
(1+
√"
N,0.5916666666666667,"8)L
into (26), we have"
N,0.5927083333333333,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
N,0.59375,8)L(f(x0) −f ∗)
N,0.5947916666666667,"K
= ϵ2,"
N,0.5958333333333333,"where the last equality holds by letting the number of iterations K = 2(1+
√"
N,0.596875,8)L(f(x0)−f ∗)
N,0.5979166666666667,"ϵ2
. Thus the
number of stochastic gradient computations is"
N,0.5989583333333334,"#grad = K−1
X"
N,0.6,"k=0
bk = b0 + K−1
X"
N,0.6010416666666667,"k=1
bk = n + (K −1)√n ≤n + 2(1 +
√"
N,0.6020833333333333,"8)√nL(f(x0) −f ∗) ϵ2
. □"
N,0.603125,Proof of Corollary 2. First we recall (25) here:
N,0.6041666666666666,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (n −b0)(4γ0 + 2α0b0)"
N,0.6052083333333333,"nb0
PK−1
k=0 ηk"
N,0.60625,"1
n n
X"
N,0.6072916666666667,"j=1
∥∇fj(x0)∥2.
(27)"
N,0.6083333333333333,"In this corollary, we do not compute any full gradients even for the initial point. We set the minibatch
size bk ≡√n for any k ≥0. So we need consider the second term of (27) since b0 = √n is
not equal to n. Similar to Corollary 1, if we set λk =
bk
2n for any k ≥1, then we know that
Mk :=
2
λkbk + 8λkn2"
N,0.609375,"b3
k
≡8 and thus the stepsize ηk ≤
1"
N,0.6104166666666667,"L
 
1+√"
N,0.6114583333333333,"Mk+1
 ≡
1
(1+
√"
N,0.6125,8)L for any k ≥0. For
N,0.6135416666666667,"the second term, we recall that γ0 ≥
η0
2λ1 =
√n
(1+
√"
N,0.6145833333333334,8)L and α0 ≥2nλ1η0
N,0.615625,"b2
1
=
1
(1+
√"
N,0.6166666666666667,8)L√n. It is easy to
N,0.6177083333333333,"see that γ0 ≥α0b0 since b0 = √n ≤n. Now, we can change (27) to"
N,0.61875,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
N,0.6197916666666666,8)L(f(x0) −f ∗)
N,0.6208333333333333,"K
+ 6(n −√n)"
N,0.621875,"nK
1
n n
X"
N,0.6229166666666667,"j=1
∥∇fj(x0)∥2"
N,0.6239583333333333,"≤2(1 +
√"
N,0.625,8)L(f(x0) −f ∗) + 6G0
N,0.6260416666666667,"K
(28) = ϵ2,"
N,0.6270833333333333,Under review as a conference paper at ICLR 2022
N,0.628125,where (28) is due to the deﬁnition G0 := 1
N,0.6291666666666667,"n
Pn
i=1 ∥∇fi(x0)∥2, and the last equality holds by letting"
N,0.6302083333333334,"the number of iterations K =
2(1+
√"
N,0.63125,8)L(f(x0)−f ∗)+6G0
N,0.6322916666666667,"ϵ2
. Thus the number of stochastic gradient
computations is"
N,0.6333333333333333,"#grad = K−1
X"
N,0.634375,"k=0
bk = √nK = √n2(1 +
√"
N,0.6354166666666666,8)L(f(x0) −f ∗) + 6G0
N,0.6364583333333333,"ϵ2
= O
√n(L∆0 + G0) ϵ2 
."
N,0.6375,"Note that G0 can be bounded by G0 ≤2L(f(x0) −bf ∗) via L-smoothness Assumption 1, then we
have"
N,0.6385416666666667,"#grad = O
√n(L∆0 + Lb∆0) ϵ2 
."
N,0.6395833333333333,"Note that ∆0 := f(x0) −f ∗, where f ∗:= minx f(x), and b∆0 := f(x0) −bf ∗, where bf ∗:=
1
n
Pn
i=1 minx fi(x).
□"
N,0.640625,"C
MISSING PROOFS FOR D-ZeroSARAH"
N,0.6416666666666667,"In this appendix, we provide the missing proofs for the distributed nonconvex setting (2). Concretely,
we provide the detailed proofs for Theorem 2 and Corollaries 3–4 of D-ZeroSARAH in Section 5."
N,0.6427083333333333,"C.1
PROOF OF THEOREM 2"
N,0.64375,"Similar to Appendix B.1, we ﬁrst recall the lemma in Li et al. (2021) which describes the change of
function value after a gradient update step."
N,0.6447916666666667,"Lemma 1 (Li et al. (2021)) Suppose that function f is L-smooth and let xk+1 := xk −ηkvk. Then
for any vk ∈Rd and ηk > 0, we have"
N,0.6458333333333334,f(xk+1) ≤f(xk) −ηk
N,0.646875,"2 ∥∇f(xk)∥2 −
 1"
N,0.6479166666666667,"2ηk
−L 2"
N,0.6489583333333333,"
∥xk+1 −xk∥2 + ηk"
N,0.65,"2 ∥vk −∇f(xk)∥2.
(29)"
N,0.6510416666666666,"Then, we provide the following Lemma 4 to bound the last variance term of (29)."
N,0.6520833333333333,"Lemma 4 Suppose that Assumption 2 holds. The gradient estimator vk is deﬁned in Line 9 of
Algorithm 3, then we have"
N,0.653125,Ek[∥vk −∇f(xk)∥2] ≤(1 −λk)2∥vk−1 −∇f(xk−1)∥2 + 2L2
N,0.6541666666666667,"skbk
∥xk −xk−1∥2"
N,0.6552083333333333,"+ 2λ2
k
skbk"
NM,0.65625,"1
nm n,m
X"
NM,0.6572916666666667,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2.
(30)"
NM,0.6583333333333333,"Proof of Lemma 4. First, according to the gradient estimator vk of D-ZeroSARAH (see Line 9 of
Algorithm 3), we know that"
NM,0.659375,vk = 1 sk X i∈Sk
NM,0.6604166666666667," 
gk
i,curr −gk
i,prev

+ (1 −λk)vk−1 + λk
1
sk X i∈Sk"
NM,0.6614583333333334," 
gk
i,prev −yk
i,prev

+ λkyk−1
(31)"
NM,0.6625,Now we bound the variance as follows:
NM,0.6635416666666667,Ek[∥vk −∇f(xk)∥2]
NM,0.6645833333333333,"(31)
= Ek"
NM,0.665625,"""
1
sk X i∈Sk"
NM,0.6666666666666666," 
gk
i,curr −gk
i,prev

+ (1 −λk)vk−1 + λk
 1 sk X i∈Sk"
NM,0.6677083333333333," 
gk
i,prev −yk
i,prev

+ yk−1
−∇f(xk) 2# = Ek"
NM,0.66875,"""
1
sk X i∈Sk"
NM,0.6697916666666667," 
gk
i,curr −gk
i,prev

+ ∇f(xk−1) −∇f(xk) + (1 −λk)(vk−1 −∇f(xk−1))"
NM,0.6708333333333333,Under review as a conference paper at ICLR 2022
NM,0.671875,"+ λk
 1 sk X i∈Sk"
NM,0.6729166666666667," 
gk
i,prev −yk
i,prev

+ yk−1 −∇f(xk−1)
 2# = Ek"
NM,0.6739583333333333,"""
1
sk X i∈Sk"
NM,0.675," 
gk
i,curr −gk
i,prev

+ ∇f(xk−1) −∇f(xk)"
NM,0.6760416666666667,"+ λk
 1 sk X i∈Sk"
NM,0.6770833333333334," 
gk
i,prev −yk
i,prev

+ yk−1 −∇f(xk−1)
 2#"
NM,0.678125,+ (1 −λk)2∥vk−1 −∇f(xk−1)∥2 ≤2Ek
NM,0.6791666666666667,"""
1
skbk X i∈Sk X"
NM,0.6802083333333333,"j∈Ik
bi"
NM,0.68125," 
∇fi,j(xk) −∇fi,j(xk−1)

+ ∇f(xk−1) −∇f(xk) 2# + 2Ek "" λ2
k"
SKBK,0.6822916666666666,"1
skbk X i∈Sk X"
SKBK,0.6833333333333333,"j∈Ik
bi"
SKBK,0.684375," 
∇fi,j(xk−1) −yk−1
i,j

+ yk−1 −∇f(xk−1) 2#"
SKBK,0.6854166666666667,+ (1 −λk)2∥vk−1 −∇f(xk−1)∥2 ≤2L2
SKBK,0.6864583333333333,"skbk
∥xk −xk−1∥2 + 2λ2
k
skbk"
NM,0.6875,"1
nm n,m
X"
NM,0.6885416666666667,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2 + (1 −λk)2∥vk−1 −∇f(xk−1)∥2, (32)"
NM,0.6895833333333333,"where (32) uses the L-smoothness Assumption 2, i.e., ∥∇fi,j(x) −∇fi,j(y)∥≤L∥x −y∥, and the
fact that E[∥x −Ex∥2] ≤E[∥x∥2] for any random variable x.
□"
NM,0.690625,"To deal with the last term in (30), we uses the following Lemma 5."
NM,0.6916666666666667,"Lemma 5 Suppose that Assumption 2 holds. The update of {yk
i,j} is deﬁned in Line 7 of Algo-
rithm 3, then we have, for ∀βk > 0, Ek"
NM,0.6927083333333334,"""
1
nm n,m
X"
NM,0.69375,"i,j=1,1
∥∇fi,j(xk) −yk
i,j∥2
#"
NM,0.6947916666666667,"≤
 
1 −skbk"
NM,0.6958333333333333,"nm

(1 + βk) 1 nm n,m
X"
NM,0.696875,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2"
NM,0.6979166666666666,"+
 
1 −skbk"
NM,0.6989583333333333,"nm
 
1 + 1 βk"
NM,0.7,"
L2∥xk −xk−1∥2.
(33)"
NM,0.7010416666666667,"Proof of Lemma 5. According to the update of {yk
i,j} (see Line 7 and Line 11 of Algorithm 3), we
have Ek"
NM,0.7020833333333333,"""
1
nm n,m
X"
NM,0.703125,"i,j=1,1
∥∇fi,j(xk) −yk
i,j∥2
#"
NM,0.7041666666666667,"=
 
1 −skbk"
NM,0.7052083333333333,"nm
 1 nm n,m
X"
NM,0.70625,"i,j=1,1
∥∇fi,j(xk) −yk−1
i,j ∥2
(34)"
NM,0.7072916666666667,"=
 
1 −skbk"
NM,0.7083333333333334,"nm
 1 nm n,m
X"
NM,0.709375,"i,j=1,1
∥∇fi,j(xk) −∇fi,j(xk−1) + ∇fi,j(xk−1) −yk−1
i,j ∥2"
NM,0.7104166666666667,"≤
 
1 −skbk"
NM,0.7114583333333333,"nm

(1 + βk) 1 nm n,m
X"
NM,0.7125,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2 +
 
1 −skbk"
NM,0.7135416666666666,"nm
 
1 + 1 βk"
NM,0.7145833333333333,"
L2∥xk −xk−1∥2, (35)"
NM,0.715625,"where (34) uses the update of {yk
i,j} in Algorithm 3, and (35) uses Young’s inequality and L-
smoothness Assumption 2.
□"
NM,0.7166666666666667,"Now we combine Lemmas 1, 4 and 5 (i.e., (29), (30) and (33)) to prove Theorem 2."
NM,0.7177083333333333,Under review as a conference paper at ICLR 2022
NM,0.71875,"Proof of Theorem 2. First, we take expectation to obtain E """
NM,0.7197916666666667,"f(xk+1) −f ∗+

γk −ηk 2"
NM,0.7208333333333333,"
∥vk −∇f(xk)∥2 +
 1"
NM,0.721875,"2ηk
−L 2"
NM,0.7229166666666667,"
∥xk+1 −xk∥2"
NM,0.7239583333333334,"+ αk
1
nm n,m
X"
NM,0.725,"i,j=1,1
∥∇fi,j(xk) −yk
i,j∥2
# ≤E """
NM,0.7260416666666667,f(xk) −f ∗−ηk
NM,0.7270833333333333,2 ∥∇f(xk)∥2 + γk(1 −λk)2∥vk−1 −∇f(xk−1)∥2
NM,0.728125,+ 2γkL2
NM,0.7291666666666666,"skbk
∥xk −xk−1∥2 + 2γkλ2
k
skbk"
NM,0.7302083333333333,"1
nm n,m
X"
NM,0.73125,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2"
NM,0.7322916666666667,"+ αk
 
1 −skbk"
NM,0.7333333333333333,"nm
 
1 + 1 βk"
NM,0.734375,"
L2∥xk −xk−1∥2"
NM,0.7354166666666667,"+ αk
 
1 −skbk"
NM,0.7364583333333333,"nm

(1 + βk) 1 nm n,m
X"
NM,0.7375,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2
# = E """
NM,0.7385416666666667,f(xk) −f ∗−ηk
NM,0.7395833333333334,2 ∥∇f(xk)∥2 + γk(1 −λk)2∥vk−1 −∇f(xk−1)∥2
NM,0.740625,"+
2γkL2"
NM,0.7416666666666667,"skbk
+ αk
 
1 −skbk"
NM,0.7427083333333333,"nm
 
1 + 1 βk"
NM,0.74375,"
L2
∥xk −xk−1∥2"
NM,0.7447916666666666,"+
2γkλ2
k
skbk
+ αk
 
1 −skbk"
NM,0.7458333333333333,"nm

(1 + βk)
 1 nm n,m
X"
NM,0.746875,"i,j=1,1
∥∇fi,j(xk−1) −yk−1
i,j ∥2
#"
NM,0.7479166666666667,".
(36)"
NM,0.7489583333333333,Now we choose appropriate parameters. Let γk = ηk−1
NM,0.75,"2λk and γk ≤γk−1, then γk(1−λk)2 ≤γk−1− ηk−1"
NM,0.7510416666666667,2 . Let βk = skbk
NM,0.7520833333333333,"2nm, αk = 2nmλkηk−1"
NM,0.753125,"s2
kb2
k
and αk ≤αk−1, we have 2γkλ2
k
skbk + αk
 
1 −skbk"
NM,0.7541666666666667,"nm

(1 + βk) ≤"
NM,0.7552083333333334,αk−1. We also have 2γkL2
NM,0.75625,"skbk + αk
 
1 −skbk"
NM,0.7572916666666667,"nm
 
1 +
1
βk

L2 ≤
1
2ηk−1 −L"
BY FURTHER LETTING STEPSIZE,0.7583333333333333,2 by further letting stepsize
BY FURTHER LETTING STEPSIZE,0.759375,"ηk−1 ≤
1
L
 
1 + √Wk
,
(37)"
BY FURTHER LETTING STEPSIZE,0.7604166666666666,"where Wk :=
2
λkskbk + 8λkn2m2"
BY FURTHER LETTING STEPSIZE,0.7614583333333333,"s3
kb3
k
."
BY FURTHER LETTING STEPSIZE,0.7625,"Summing up (36) from k = 1 to K −1, we get 0 ≤E """
BY FURTHER LETTING STEPSIZE,0.7635416666666667,"f(x1) −f ∗− K−1
X k=1 ηk"
BY FURTHER LETTING STEPSIZE,0.7645833333333333,2 ∥∇f(xk)∥2 + γ1(1 −λ1)2∥v0 −∇f(x0)∥2
BY FURTHER LETTING STEPSIZE,0.765625,"+
2γ1L2"
BY FURTHER LETTING STEPSIZE,0.7666666666666667,"s1b1
+ α1
 
1 −s1b1"
BY FURTHER LETTING STEPSIZE,0.7677083333333333,"nm
 
1 + 2nm s1b1"
BY FURTHER LETTING STEPSIZE,0.76875,"
L2
∥x1 −x0∥2"
BY FURTHER LETTING STEPSIZE,0.7697916666666667,"+
2γ1λ2
1
s1b1
+ α1
 
1 −s1b1"
BY FURTHER LETTING STEPSIZE,0.7708333333333334,"nm
 
1 + s1b1"
NM,0.771875,"2nm
 1 nm n,m
X"
NM,0.7729166666666667,"i,j=1,1
∥∇fi,j(x0) −y0
i,j∥2
#"
NM,0.7739583333333333,".
(38)"
NM,0.775,"For k = 0, we directly uses (29), i.e.,"
NM,0.7760416666666666,"E[f(x1) −f ∗] ≤E
h
f(x0) −f ∗−η0"
NM,0.7770833333333333,"2 ∥∇f(x0)∥2 −
 1"
NM,0.778125,"2η0
−L 2"
NM,0.7791666666666667,"
∥x1 −x0∥2 + η0"
NM,0.7802083333333333,"2 ∥v0 −∇f(x0)∥2i
. (39)"
NM,0.78125,"Now, we combine (38) and (39) to get E"
NM,0.7822916666666667,""" K−1
X k=0 ηk"
NM,0.7833333333333333,"2 ∥∇f(xk)∥2
#"
NM,0.784375,Under review as a conference paper at ICLR 2022
NM,0.7854166666666667,"≤E
h
f(x0) −f ∗+
 
γ1(1 −λ1)2 + η0"
NM,0.7864583333333334,"2

∥v0 −∇f(x0)∥2"
NM,0.7875,"+
2γ1λ2
1
s1b1
+ α1
 
1 −s1b1"
NM,0.7885416666666667,"nm
 
1 + s1b1"
NM,0.7895833333333333,"2nm
 1 nm n,m
X"
NM,0.790625,"i,j=1,1
∥∇fi,j(x0) −y0
i,j∥2i
(40)"
NM,0.7916666666666666,"≤E
h
f(x0) −f ∗+ η0(1 −λ1(1 −λ1))"
NM,0.7927083333333333,"2λ1
∥v0 −∇f(x0)∥2"
NM,0.79375,+ 2nmλ1η0
NM,0.7947916666666667,"s2
1b2
1"
NM,0.7958333333333333,"1
nm n,m
X"
NM,0.796875,"i,j=1,1
∥∇fi,j(x0) −y0
i,j∥2i
(41)"
NM,0.7979166666666667,≤f(x0) −f ∗+ η0 2λ1
NM,0.7989583333333333,"nm −s0b0
(nm −1)s0b0"
NM,0.8,"1
nm n,m
X"
NM,0.8010416666666667,"i,j=1,1
∥∇fi,j(x0)∥2"
NM,0.8020833333333334,+ 2nmλ1η0
NM,0.803125,"s2
1b2
1"
NM,0.8041666666666667,nm −s0b0
NM,0.8052083333333333,"nm
1
nm n,m
X"
NM,0.80625,"i,j=1,1
∥∇fi,j(x0)∥2
(42)"
NM,0.8072916666666666,= f(x0) −f ∗+ (nm −s0b0)η0θ0
NM,0.8083333333333333,"2nms0b0
G′
0,
(43)"
NM,0.809375,"where (40) follows from the deﬁnition of η0 in (37), (41) uses γ1 =
η0
2λ1 and α1 =
2nmλ1η0"
NM,0.8104166666666667,"s2
1b2
1
,"
NM,0.8114583333333333,"(42) uses λ0 = 1, and (43) uses the deﬁnitions θ0 :=
nm
(nm−1)λ1 + 4nmλ1s0b0"
NM,0.8125,"s2
1b2
1
and G′
0 :="
NM,0.8135416666666667,"1
nm
Pn,m
i,j=1,1 ∥∇fi,j(x0)∥2."
NM,0.8145833333333333,"By randomly choosing bxK from {xk}K−1
k=0 with probability ηk/ PK−1
t=0 ηt for xk, (43) turns to"
NM,0.815625,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (nm −s0b0)η0θ0G′
0
nms0b0
PK−1
k=0 ηk
(44) □"
NM,0.8166666666666667,"C.2
PROOFS OF COROLLARIES 3 AND 4"
NM,0.8177083333333334,"Now, we prove the detailed convergence results in Corollaries 3–4 with speciﬁc parameter settings."
NM,0.81875,Proof of Corollary 3. First we know that (44) with s0 = n and b0 = m turns to
NM,0.8197916666666667,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
.
(45)"
NM,0.8208333333333333,Then if we set λk = skbk
NM,0.821875,"2nm, sk ≡√n, and bk ≡√m for any k ≥1, then we know that Wk :="
NM,0.8229166666666666,"2
λkskbk + 8λkn2m2"
NM,0.8239583333333333,"b3
ks3
k
≡8 and thus the stepsize ηk ≤
1"
NM,0.825,"L
 
1+√"
NM,0.8260416666666667,"Wk+1
 ≡
1
(1+
√"
NM,0.8270833333333333,8)L for any k ≥0. By
NM,0.828125,"plugging ηk ≤
1
(1+
√"
NM,0.8291666666666667,"8)L into (45), we have"
NM,0.8302083333333333,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
NM,0.83125,8)L(f(x0) −f ∗)
NM,0.8322916666666667,"K
= ϵ2,"
NM,0.8333333333333334,"where the last equality holds by letting the number of iterations K = 2(1+
√"
NM,0.834375,8)L(f(x0)−f ∗)
NM,0.8354166666666667,"ϵ2
. Thus the
number of stochastic gradient computations for each client is"
NM,0.8364583333333333,"#grad = K−1
X"
NM,0.8375,"k=0
bk = m + (K −1)√m
√n
≤n +
rm"
NM,0.8385416666666666,"n
2(1 +
√"
NM,0.8395833333333333,"8)L(f(x0) −f ∗) ϵ2
. □"
NM,0.840625,Proof of Corollary 4. First we recall (44) here:
NM,0.8416666666666667,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (nm −s0b0)η0θ0G′
0
nms0b0
PK−1
k=0 ηk
.
(46)"
NM,0.8427083333333333,Under review as a conference paper at ICLR 2022
NM,0.84375,"In this corollary, we do not compute any full gradients even for the initial point. We set the client
sample size sk ≡√n and minibatch size bk ≡√m for any k ≥0. So we need consider the
second term of (46) since s0b0 = √nm is not equal to nm. Similar to Corollary 3, if we set
λk = skbk"
NM,0.8447916666666667,"2nm for any k ≥1, then we know that Wk :=
2
λkskbk + 8λkn2m2"
NM,0.8458333333333333,"b3
ks3
k
≡8 and thus the stepsize"
NM,0.846875,"ηk ≤
1"
NM,0.8479166666666667,"L
 
1+√"
NM,0.8489583333333334,"Wk+1
 ≡
1
(1+
√"
NM,0.85,"8)L for any k ≥0. Now, we can change (46) to"
NM,0.8510416666666667,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
NM,0.8520833333333333,8)L(f(x0) −f ∗)
NM,0.853125,"K
+ (nm −s0b0)θ0G′
0
nms0b0K"
NM,0.8541666666666666,"≤2(1 +
√"
NM,0.8552083333333333,"8)L(f(x0) −f ∗) + 4G′
0
K
(47) = ϵ2,"
NM,0.85625,"where (47) holds by plugging the initial values of the parameters into the last term, and the last
equality holds by letting the number of iterations K = 2(1+
√"
NM,0.8572916666666667,"8)L(f(x0)−f ∗)+4G′
0
ϵ2
. Thus the number
of stochastic gradient computations for each client is"
NM,0.8583333333333333,"#grad = K−1
X"
NM,0.859375,"k=0
bk = K√m
√n
=
rm"
NM,0.8604166666666667,"n
2(1 +
√"
NM,0.8614583333333333,"8)L(f(x0) −f ∗) + 4G′
0
ϵ2
= O
rm"
NM,0.8625,"n
L∆0 + G′
0
ϵ2 
."
NM,0.8635416666666667,"Note that ∆0 := f(x0) −f ∗where f ∗:= minx f(x).
□"
NM,0.8645833333333334,"D
FURTHER IMPROVEMENT FOR CONVERGENCE RESULTS"
NM,0.865625,"Note that all parameter settings, i.e., {ηk}, {bk} and {λk} in ZeroSARAH for Corollaries 1–2, only
require the values of L and n, and {ηk}, {sk}, {bk}, {λk} in D-ZeroSARAH for Corollaries 3–4
only require the values of L, n and m, both are the same as all previous algorithms. If one further
allows other values, e.g., ϵ, G0 or b∆0, for setting the initial b0, then the gradient complexity can be
further improved. See Appendices D.1 and D.2 for better results of ZeroSARAH and D-ZeroSARAH,
respectively."
NM,0.8666666666666667,"D.1
BETTER RESULT FOR ZeroSARAH"
NM,0.8677083333333333,"Corollary 5 Suppose that Assumption 1 holds. Choose stepsize ηk ≤
1
(1+
√"
NM,0.86875,"8)L for any k ≥0, mini-"
NM,0.8697916666666666,batch size bk ≡√n and parameter λk = bk
NM,0.8708333333333333,"2n for any k ≥1. Moreover, let b0 = min
nq nG0"
NM,0.871875,"ϵ2 , n
o"
NM,0.8729166666666667,"and λ0 = 1. Then ZeroSARAH (Algorithm 2) can ﬁnd an ϵ-approximate solution for problem (1)
such that
E[∥∇f(bxK)∥2] ≤ϵ2"
NM,0.8739583333333333,and the number of stochastic gradient computations can be bounded by
NM,0.875,"#grad = O
√n
L∆0"
NM,0.8760416666666667,"ϵ2
+ min
nr G0"
NM,0.8770833333333333,"ϵ2 , √n
o
."
NM,0.878125,"Similarly, G0 can be bounded by G0 ≤2Lb∆0 via Assumption 1. Let b0 = min
nq nLb∆0"
NM,0.8791666666666667,"ϵ2
, n
o
, then
we also have"
NM,0.8802083333333334,"#grad = O
√n
L∆0"
NM,0.88125,"ϵ2
+ min
n
s Lb∆0"
NM,0.8822916666666667,"ϵ2 , √n
o
."
NM,0.8833333333333333,"Remark: The result of Corollary 5 for ZeroSARAH is the best one compared with Corollaries 1–2.
In particular, it recovers Corollary 1 when b0 = n. In the case b0 < n (never computes any full"
NM,0.884375,"gradients even for the initial point), then #grad = O
√n
  L∆0"
NM,0.8854166666666666,"ϵ2
+
q G0"
NM,0.8864583333333333,"ϵ2

which is better than the"
NM,0.8875,"result O
 √n(L∆0+G0)"
NM,0.8885416666666667,"ϵ2

in Corollary 2. Similar to the Remark after Corollary 2, if we consider L,"
NM,0.8895833333333333,Under review as a conference paper at ICLR 2022
NM,0.890625,"∆0, G0 or b∆0 as constant values then the stochastic gradient complexity in Corollary 5 is #grad =
O(
√n"
NM,0.8916666666666667,"ϵ2 ), i.e., full gradient computations do not appear in ZeroSARAH and the term ‘n’ also does not
appear in its convergence result. If we further assume that loss functions fi’s are non-negative, i.e.,
∀x, fi(x) ≥0 (usually the case in practice), we can simply bound b∆0 := f(x0) −bf ∗≤f(x0) and"
NM,0.8927083333333333,"then b0 can be set as min
nq"
NM,0.89375,nLf(x0)
NM,0.8947916666666667,"ϵ2
, n
o
for Corollary 5."
NM,0.8958333333333334,Proof of Corollary 5. First we recall (25) here:
NM,0.896875,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (n −b0)(4γ0 + 2α0b0)"
NM,0.8979166666666667,"nb0
PK−1
k=0 ηk"
N,0.8989583333333333,"1
n n
X"
N,0.9,"j=1
∥∇fj(x0)∥2.
(48)"
N,0.9010416666666666,"Note that here we also need consider the second term of (48) since b0 may be less than n. Similar
to Corollary 2, if we set λk =
bk
2n and bk ≡√n for any k ≥1, then we know that Mk :="
N,0.9020833333333333,"2
λkbk + 8λkn2"
N,0.903125,"b3
k
≡8 and thus the stepsize ηk ≤
1"
N,0.9041666666666667,"L
 
1+√"
N,0.9052083333333333,"Mk+1
 ≡
1
(1+
√"
N,0.90625,8)L for any k ≥0. For the
N,0.9072916666666667,"second term, we recall that γ0 ≥
η0
2λ1 =
√n
(1+
√"
N,0.9083333333333333,8)L and α0 ≥2nλ1η0
N,0.909375,"b2
1
=
1
(1+
√"
N,0.9104166666666667,"8)L√n. It is easy to see
that γ0 ≥α0b0 since b0 ≤n. Now, we can change (48) to"
N,0.9114583333333334,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
N,0.9125,8)L(f(x0) −f ∗)
N,0.9135416666666667,"K
+ 6(n −b0)
√nb0K
1
n n
X"
N,0.9145833333333333,"j=1
∥∇fj(x0)∥2"
N,0.915625,"= 2(1 +
√"
N,0.9166666666666666,8)L(f(x0) −f ∗)
N,0.9177083333333333,"K
+ 6(n −b0)G0
√nb0K
(49) = ϵ2,"
N,0.91875,where (49) is due to the deﬁnition G0 := 1
N,0.9197916666666667,"n
Pn
i=1 ∥∇fi(x0)∥2, and the last equality holds by letting"
N,0.9208333333333333,"the number of iterations K =
2(1+
√"
N,0.921875,8)L(f(x0)−f ∗)
N,0.9229166666666667,"ϵ2
+ 6(n−b0)G0
√nb0ϵ2
. Thus the number of stochastic
gradient computations is"
N,0.9239583333333333,"#grad = K−1
X"
N,0.925,"k=0
bk = b0 + K−1
X"
N,0.9260416666666667,"k=1
bk"
N,0.9270833333333334,"= b0 + (K −1)√n ≤b0 + 2(1 +
√"
N,0.928125,8)√nL(f(x0) −f ∗)
N,0.9291666666666667,"ϵ2
+ 6(n −b0)G0"
N,0.9302083333333333,"b0ϵ2
."
N,0.93125,"By choosing b0 = min{
q nG0"
N,0.9322916666666666,"ϵ2 , n}, we have"
N,0.9333333333333333,"#grad ≤√n
2(1 +
√"
N,0.934375,8)L(f(x0) −f ∗)
N,0.9354166666666667,"ϵ2
+ min
n
7 r G0"
N,0.9364583333333333,"ϵ2 , √n
o"
N,0.9375,"= O
√n
L∆0"
N,0.9385416666666667,"ϵ2
+ min
nr G0"
N,0.9395833333333333,"ϵ2 , √n
o
."
N,0.940625,"Similarly, G0 can be bounded by G0 ≤2L(f(x0) −bf ∗) via Assumption 1 and let b0 ="
N,0.9416666666666667,"min{
q nLb∆0"
N,0.9427083333333334,"ϵ2
, n}, then we have"
N,0.94375,"#grad = O
√n
L∆0"
N,0.9447916666666667,"ϵ2
+ min
n
s Lb∆0"
N,0.9458333333333333,"ϵ2 , √n
o
."
N,0.946875,"Note that ∆0 := f(x0) −f ∗, where f ∗:= minx f(x), and b∆0 := f(x0) −bf ∗, where bf ∗:=
1
n
Pn
i=1 minx fi(x).
□"
N,0.9479166666666666,"D.2
BETTER RESULT FOR D-ZeroSARAH"
N,0.9489583333333333,"Corollary 6 Suppose that Assumption 2 holds. Choose stepsize ηk ≤
1
(1+
√"
N,0.95,"8)L for any k ≥0,"
N,0.9510416666666667,"clients subset size sk ≡√n, minibatch size bk ≡√m and parameter λk = skbk"
N,0.9520833333333333,2nm for any k ≥1.
N,0.953125,Under review as a conference paper at ICLR 2022
N,0.9541666666666667,"Moreover, let s0 = min
nq"
N,0.9552083333333333,"nG′
0
mϵ2 , n
o
and b0 = m (or b0 = min
nq"
N,0.95625,"mG′
0
nϵ2 , m
o
and s0 = n),
and λ0 = 1. Then D-ZeroSARAH (Algorithm 3) can ﬁnd an ϵ-approximate solution for distributed
problem (2) such that
E[∥∇f(bxK)∥2] ≤ϵ2"
N,0.9572916666666667,and the number of stochastic gradient computations for each client can be bounded by
N,0.9583333333333334,"#grad = O
rm n L∆0"
N,0.959375,"ϵ2
+ min
nr"
N,0.9604166666666667,"G′
0
ϵ2 , √nm
o
."
N,0.9614583333333333,Proof of Corollary 6. First we recall (44) here:
N,0.9625,"E[∥∇f(bxK)∥2] ≤2(f(x0) −f ∗)
PK−1
k=0 ηk
+ (nm −s0b0)η0θ0G′
0
nms0b0
PK−1
k=0 ηk
.
(50)"
N,0.9635416666666666,"Similar to Corollary 4, here we also need consider the second term of (50) since s0b0 may be less
than nm. Similarly, if we set λk = skbk"
N,0.9645833333333333,"2nm, sk ≡√n, and bk ≡√n for any k ≥1, then we know that
Wk :=
2
λkskbk + 8λkn2m2"
N,0.965625,"b3
ks3
k
≡8 and thus the stepsize ηk ≤
1"
N,0.9666666666666667,"L
 
1+√"
N,0.9677083333333333,"Wk+1
 ≡
1
(1+
√"
N,0.96875,8)L for any k ≥0.
N,0.9697916666666667,Then (50) changes to
N,0.9708333333333333,"E[∥∇f(bxK)∥2] ≤2(1 +
√"
N,0.971875,8)L(f(x0) −f ∗)
N,0.9729166666666667,"K
+ (nm −s0b0)θ0G′
0
nms0b0K"
N,0.9739583333333334,"= 2(1 +
√"
N,0.975,8)L(f(x0) −f ∗)
N,0.9760416666666667,"K
+ 6(nm −s0b0)G′
0
√nms0b0K
(51) = ϵ2,"
N,0.9770833333333333,"where (51) by ﬁguring out θ0 with the initial values of the parameters, and the last equality holds
by letting the number of iterations K = 2(1+
√"
N,0.978125,8)L(f(x0)−f ∗)
N,0.9791666666666666,"ϵ2
+ 6(nm−s0b0)G′
0
√nms0b0ϵ2
. Thus the number of
stochastic gradient computations for each client is"
N,0.9802083333333333,"#grad = K−1
X"
N,0.98125,"k=0
bk = s0"
N,0.9822916666666667,"n b0 + (K −1)√m
√n ≤
rm"
N,0.9833333333333333,"n
2(1 +
√"
N,0.984375,8)L(f(x0) −f ∗)
N,0.9854166666666667,"ϵ2
+ s0b0"
N,0.9864583333333333,"n
+ 6(nm −s0b0)G′
0
ns0b0ϵ2 ≤
rm n"
N,0.9875,"2(1 +
√"
N,0.9885416666666667,8)L(f(x0) −f ∗)
N,0.9895833333333334,"ϵ2
+ min
n
7 r"
N,0.990625,"G′
0
ϵ2 , √nm
o
(52) = O rm n L∆0"
N,0.9916666666666667,"ϵ2
+ min
r"
N,0.9927083333333333,"G′
0
ϵ2 , √nm
! ,"
N,0.99375,"where (52) holds by choosing s0b0 = min
nq"
N,0.9947916666666666,"nmG′
0
ϵ2
, nm
o
. It can be satisﬁed by letting s0 ="
N,0.9958333333333333,"min
nq"
N,0.996875,"nG′
0
mϵ2 , n
o
and b0 = m (or s0 = n and b0 = min
nq"
N,0.9979166666666667,"mG′
0
nϵ2 , m
o
). The last equation uses the"
N,0.9989583333333333,"deﬁnition ∆0 := f(x0) −f ∗where f ∗:= minx f(x).
□"
