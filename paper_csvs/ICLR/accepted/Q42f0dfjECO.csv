Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003745318352059925,"We give simpler, sparser, and faster algorithms for differentially private ﬁne-
tuning of large-scale pre-trained language models, which achieve the state-of-
the-art privacy versus utility tradeoffs on many standard NLP tasks. We pro-
pose a meta-framework for this problem, inspired by the recent success of highly
parameter-efﬁcient methods for ﬁne-tuning. Our experiments show that differen-
tially private adaptations of these approaches outperform previous private algo-
rithms in three important dimensions: utility, privacy, and the computational and
memory cost of private training. On many commonly studied datasets, the util-
ity of private models approaches that of non-private models. For example, on the
MNLI dataset we achieve an accuracy of 87.8% using RoBERTa-Large and 83.5%
using RoBERTa-Base with a privacy budget of ε = 6.7. In comparison, absent
privacy constraints, RoBERTa-Large achieves an accuracy of 90.2%. Our ﬁndings
are similar for natural language generation when privately ﬁne-tuning GPT-2. Our
experiments also show that larger models are better suited for private ﬁne-tuning:
while they are well known to achieve superior accuracy non-privately, we ﬁnd that
they also better maintain their accuracy when privacy is introduced."
INTRODUCTION,0.00749063670411985,"1
INTRODUCTION"
INTRODUCTION,0.011235955056179775,"Deep learning models are well known to leak sensitive information about the dataset when trained
using conventional methods (Shokri et al., 2017; Carlini et al., 2019; 2021). To combat this issue,
models can instead be trained to guarantee differential privacy (DP) (Dwork et al., 2006b), a strong
notion of data privacy which limits the inﬂuence of any individual training point on the ﬁnal model.
While DP is one of the few approaches capable of providing machine learning models with rigorous
privacy guarantees, it generally comes at a cost in terms of test accuracy. One oft-cited explanation
is that the constraint of DP necessitates much more training data (Tram`er & Boneh, 2021; Feldman,
2020; Brown et al., 2021). Unfortunately, more training data may be hard to acquire, particularly in
settings where privacy is a concern."
INTRODUCTION,0.0149812734082397,"Parallel to these developments, Transformer-based (Vaswani et al., 2017) large language models
(LLMs), including the BERT (Devlin et al., 2019; Liu et al., 2019) and GPT (Radford et al., 2018;
2019; Brown et al., 2020) families, have enabled signiﬁcant progress in natural language process-
ing, achieving state-of-the-art accuracy in almost every task considered. These models are ﬁrst
pre-trained on an extremely large and diverse public dataset. The weights are then ﬁne-tuned for"
INTRODUCTION,0.018726591760299626,"∗Authors contribute equally to this work and are listed in alphabetical order.
†The work of Da Yu is done while he was an intern at Microsoft Research Asia."
INTRODUCTION,0.02247191011235955,Published as a conference paper at ICLR 2022
INTRODUCTION,0.026217228464419477,"Figure 1: An illustration of our framework. First, the model is pre-trained on a large, public dataset.
Next, new parameters are introduced and privately ﬁne-tuned on a smaller, private, task-speciﬁc
dataset. The original parameters are frozen during this process. Finally, the ﬁne-tuned new pa-
rameters may be released publicly and plugged-in to the model for downstream tasks, while still
preserving privacy of the private dataset."
INTRODUCTION,0.0299625468164794,"Table 1: Accuracy of ﬁne-tuning for downstream tasks with RoBERTa-Large (in %). Our results
achieve accuracy comparable to full ﬁne-tuning non-privately, while simultaneously guaranteeing
differential privacy. We choose δ =1e-5 for SST-2 and QNLI and δ =1e-6 for MNLI and QQP due
to their dataset sizes. Implementation details are in Section 4.1."
INTRODUCTION,0.033707865168539325,"Method
MNLI
SST-2
QQP
QNLI
Avg.
Trained params
Non-private ﬁne-tuning
90.2
96.4
92.2
94.7
93.4
100%
Our results (ε = 6.7)
87.8
95.3
87.4
90.8
90.3
0.94%"
INTRODUCTION,0.03745318352059925,"each task of interest using a much smaller task-speciﬁc dataset. For example, a single pre-trained
GPT-family model may be ﬁne-tuned for various downstream tasks, such as email reply suggestion,
sentence completion in text editors, language translation, and more. This two-stage paradigm can
naturally be adapted to solve tasks in private learning, automatically addressing the aforementioned
data shortage issue via the massive scale of the public pre-training dataset. One may pre-train the
model on public data as usual,1 but then ﬁne-tune the model privately."
INTRODUCTION,0.04119850187265917,"Despite the success of these models, task-speciﬁc ﬁne-tuning introduces a number of technical chal-
lenges. In the non-private setting, the immense size of LLMs makes it impractical to ﬁne-tune the
full model and store a separate copy of the parameters for hundreds of downstream tasks. Things
only get worse with privacy, which leads to overheads in terms of running time, memory usage,
and most importantly, accuracy. The magnitude of noise introduced to a model due to DP grows
as the model size increases (Bassily et al., 2014; Abadi et al., 2016; Bun et al., 2014), which can
overwhelm any signal for larger models. A recent line of work in the non-private literature has
proposed parameter-efﬁcient methods to alleviate the issues of storage and computational cost for
ﬁne-tuning (Houlsby et al., 2019; Li & Liang, 2021; Aghajanyan et al., 2020; Hu et al., 2021; Ma-
habadi et al., 2021). The main focus of our work is to explore parameter-efﬁciency in the context of
private learning."
OUR CONTRIBUTIONS,0.0449438202247191,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.04868913857677903,"Our primary contribution is to show that advanced parameter-efﬁcient methods can lead to sim-
pler and signiﬁcantly improved algorithms for private ﬁne-tuning. Our framework is illustrated in
Figure 1. Our ﬁndings and contributions are summarized as follows:"
OUR CONTRIBUTIONS,0.052434456928838954,"State-of-the-art utility and privacy.
Empirical evaluation of our algorithms reveals that they
achieve state-of-the-art accuracy versus privacy tradeoffs, improving upon the previous best (Yu"
OUR CONTRIBUTIONS,0.056179775280898875,"1Despite the fact that the pre-training data is public, there may nonetheless be privacy concerns related
to personal or copyrighted data. However, since these pre-trained models have already been released, any
associated privacy loss has already been incurred."
OUR CONTRIBUTIONS,0.0599250936329588,Published as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.06367041198501873,"et al., 2021b). More importantly, for many ﬁne-tuning tasks, the utility of models trained with DP
approaches that of non-private models. For example, privately ﬁne-tuning RoBERTa-Large on the
MNLI data set (Williams et al., 2018), we achieve an accuracy of 87.8% with a privacy budget of
(ε = 6.7, δ = 1e-6). Without privacy guarantees, RoBERTa-Large achieves an accuracy of 90.2%
(GPT-3 is known to achieve 91.7% (Hu et al., 2021)); see Table 1 for a summary. We also explore
private natural language generation tasks, ﬁne-tuning GPT-2 models on the E2E dataset (Novikova
et al., 2017). Again, the utility approaches non-private levels: we achieve a ROUGE-L score of
0.6755 with GPT-2-Large and (ε = 5.4, δ = 1e-5), compared to 0.72 without privacy."
OUR CONTRIBUTIONS,0.06741573033707865,"Larger models are better.
Prior work has consistently shown that larger language models achieve
better accuracy for downstream tasks. Our results give evidence that this phenomenon extends to
the private setting. For example, on the MNLI dataset, RoBERTa-Base achieves an accuracy of
83.5% whereas RoBERTa-Large achieves an accuracy of 87.8%, both under a privacy budget of
(ε = 6.7, δ = 1e-6). Similarly, privately ﬁne-tuning with E2E, GPT-2-Small, GPT-2-Medium,
and GPT-2-Large achieve ROUGE-L scores of 0.6219, 0.6645 and 0.6755 respectively, all under
a privacy budget of (ε = 5.4, δ = 1e-5). While established in the non-private setting, we ﬁnd
this phenomenon quite surprising under DP. There is often a tension when choosing private model
architectures: larger models may have higher capacity, but necessitate the introduction of more
noise. Consequently, smaller and simpler private models achieve the better accuracy in several
settings (Papernot et al., 2019; Tram`er & Boneh, 2021). In contrast, our experiments show that
ﬁne-tuning the biggest models achieves the best accuracy.2"
OUR CONTRIBUTIONS,0.07116104868913857,"Simpler, sparser, and faster.
Beyond accuracy concerns, DP requirements also lead to signiﬁcant
overheads in terms of computation and memory usage. The large number of parameters contributes
to the high cost of training LLMs, and things get worse under privacy, which has been documented to
increase training time by up to two orders of magnitude (Carlini et al., 2019; Subramani et al., 2021).
The parameter-efﬁcient approaches we employ partially offset this issue: as we only update a small
fraction of the total number of parameters, training becomes considerably more computationally
and memory efﬁcient. Furthermore, as in the non-private setting, this framework leads to a modular
design, where a single large pre-trained model can be augmented with lightweight modiﬁcations for
each individual downstream task."
OUR CONTRIBUTIONS,0.0749063670411985,"To the best of our knowledge, we are the ﬁrst to ﬁne-tune GPT-2-Large using differential privacy,
the largest model trained thus far using DP. Given our state-of-the-art results for a variety of stan-
dard NLP tasks using advanced ﬁne-tuning techniques, we believe that our paper will serve as a
benchmark for further work in this direction. For example, the best average accuracy achieved by
the prior work of Yu et al. (2021b) on four standard NLP tasks in Table 1 is 83.9% using ε = 8 (and
the same δ as in Table 1), whereas we can achieve an average accuracy of 90.3% using ε = 6.7 by a
combination of better algorithms, larger models, and new privacy accounting techniques."
OUR CONTRIBUTIONS,0.07865168539325842,"Finally, though recently considered elsewhere (see Section 5), we put further focus on the framing of
public pre-training and private ﬁne-tuning as an important conceptual direction in DP deep learning."
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.08239700374531835,"2
PRELIMINARIES AND PRIOR ALGORITHM BASELINES"
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.08614232209737828,Recall the formal deﬁnition of differential privacy.
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.0898876404494382,"Deﬁnition 2.1 (Differential Privacy (DP) (Dwork et al., 2006b;a)). A randomized algorithm
A is (ε,δ)-differentially private if for any two neighboring datasets D and D′, which dif-
fer in exactly the data pertaining to a single user, and for all sets S of possible outputs:
Pr[A(D) ∈S] ≤eε Pr[A(D′) ∈S] + δ."
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.09363295880149813,We review prior techniques for private ﬁne-tuning.
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.09737827715355805,"2An alternative perspective is that what we currently think of as “large” language models are relatively
small, and we are yet to reach the point where the beneﬁts of model size on accuracy are outweighed by the
drawbacks."
PRELIMINARIES AND PRIOR ALGORITHM BASELINES,0.10112359550561797,Published as a conference paper at ICLR 2022
FULL FINE-TUNING VIA DPSGD,0.10486891385767791,"2.1
FULL FINE-TUNING VIA DPSGD"
FULL FINE-TUNING VIA DPSGD,0.10861423220973783,"To train a machine learning model with privacy, the most popular algorithm is the celebrated DP
stochastic gradient descent (DPSGD) (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016).
This optimization method serves as a drop-in replacement for SGD, augmenting it with the addition
of per-example gradient clipping and Gaussian noise addition steps. These two steps serve to limit
and mask the contribution of a single example. Two key points to note are that a) per-example
gradient clipping incurs signiﬁcant computational and memory overheads in most implementations,
and b) noise introduced due to privacy grows as the square-root as the number of model parameters.
With this tool in place, the most basic ﬁne-tuning strategy is to train all parameters using DPSGD."
REPARAMETRIZED GRADIENT PERTURBATION,0.11235955056179775,"2.2
REPARAMETRIZED GRADIENT PERTURBATION"
REPARAMETRIZED GRADIENT PERTURBATION,0.11610486891385768,"To mitigate the limitations of DPSGD, a recent work of Yu et al. (2021b) introduced an elegant
method called reparametrized gradient perturbation (RGP). RGP exploits the implicit low-rank
structure in the gradient updates of SGD to substantially improve upon DPSGD. Speciﬁcally, they
reparametrize each layer’s weight matrix W into LR + ˜W, where L and R are low-rank gradient-
carrier matrices and ˜W is the residual weight.
The authors show that one can obtain a low-
dimensional projection of W’s gradient by taking gradients only of the low-rank matrices L and R
(and not the high-rank ˜W). Privacy is introduced by clipping and noising these low-dimensional gra-
dients of L and R. While this low-dimensional projection loses some of the signal in W’s gradient, it
turns out to contain enough to still achieve high accuracy. At the same time, the low-dimensional gra-
dients alleviate the aforementioned issues related to privatization, signiﬁcantly reducing the memory
consumption and noise introduced."
OUR APPROACH,0.1198501872659176,"3
OUR APPROACH"
A META-FRAMEWORK,0.12359550561797752,"3.1
A META-FRAMEWORK"
A META-FRAMEWORK,0.12734082397003746,"We introduce our approach as a meta-framework for private deep learning, which abstracts the key
principles of recent ﬁne-tuning methods."
A META-FRAMEWORK,0.13108614232209737,"Suppose f(WPT; x) is a pre-trained model where WPT are the pre-trained weights and x is any
input. We create a new ﬁne-tuning model"
A META-FRAMEWORK,0.1348314606741573,"fFT(WPT, θ; x)
(1)"
A META-FRAMEWORK,0.13857677902621723,"which incorporates additional trainable parameters θ, where dim(θ) ≪dim(WPT). That is, the
number of new parameters in θ is a small fraction of the original number of parameters in the pre-
trained weights WPT. Fine-tuning is done by running DPSGD on the additional parameters θ, while
freezing the weights of pre-trained model WPT. The new parameters are initialized to θ0 such that"
A META-FRAMEWORK,0.14232209737827714,"fFT(WPT, θ0; x) = f(WPT; x).
(2)"
A META-FRAMEWORK,0.14606741573033707,"The initialization condition (2) is very important, as it ensures that ﬁne-tuning starts at the pre-trained
model and improves it by modifying the parameters θ. Most ﬁne-tuning methods are additive and
have the following special form:"
A META-FRAMEWORK,0.149812734082397,"fFT(WPT, θ; x) = f(WPT + π(θ); x),
(3)"
A META-FRAMEWORK,0.15355805243445692,"i.e., they modify the pre-trained weights by adding a correction term π(θ) parametrized by θ."
A META-FRAMEWORK,0.15730337078651685,"Recent work in the non-private literature has described concrete instantiations of this frame-
work (Houlsby et al., 2019; Mahabadi et al., 2021; Hu et al., 2021), which (crucially) are effective
when dim(θ) ≪dim(WPT). In the non-private setting, such reparametrizations are useful for re-
ducing the computation and memory required for ﬁne-tuning, and enable lightweight and plug-in
modiﬁcations to the base model for different downstream tasks. At the same time, they maintain (or
sometimes surpass) the accuracy achieved by full ﬁne-tuning."
A META-FRAMEWORK,0.16104868913857678,"We give some intuition as to why parameter-efﬁcient methods can to be more effective for private
ﬁne-tuning, especially on smaller datasets. For simplicity, we assume that the ﬁne-tuning method
is additive as in (3), such that the ﬁne-tuned weights WFT = WPT + π(θ). We can imagine that"
A META-FRAMEWORK,0.1647940074906367,Published as a conference paper at ICLR 2022
A META-FRAMEWORK,0.16853932584269662,"WFT lies on a manifold passing through WPT of very small dimension (equal to the dimension of
θ) compared to the dimension of WPT. Even if the parameters θ are very noisy due to the noise
added during DPSGD, we will always stay in this manifold. In particular, we are not disturbing
the pre-trained weights in most directions (those orthogonal to the manifold near WPT). If we run
DPSGD on all the weights instead, then we add noise in all directions, thus potentially unlearning
the knowledge learned during pre-training, especially in low data regimes. However, this intuition
may not always be true; see the remark at the end of our experiments on NLU tasks."
A META-FRAMEWORK,0.17228464419475656,"Besides substantial gains in the accuracy, the above method of reparametrization has several other
advantages:"
A META-FRAMEWORK,0.1760299625468165,"• A single pre-trained model such as BERT or GPT is generally applied to hundreds of down-
stream tasks via ﬁne-tuning. Private ﬁne-tuning using previous methods requires updating
all parameters and storing a different copy of the ﬁne-tuned model per task. This creates
substantial overheads for storing and deploying, and can be very expensive in practice. On
the other hand, the reparametrization (1) means that we only need to store a single pre-
trained model that can be shared across many downstream tasks. Each downstream task
requires only a small number of new parameters that can be plugged in."
A META-FRAMEWORK,0.1797752808988764,"• Differentially private training requires computing and storing per-example gradients, which
increases the memory footprint. In our approach, however, learning is done in a much lower
dimension, hence saving on the memory cost as compared to prior works."
A META-FRAMEWORK,0.18352059925093633,"• Finally, we expect that (1) also gives a more communication-efﬁcient method of ﬁne-tuning
in distributed settings such as federated learning, due to the signiﬁcantly smaller number
of parameters learned during ﬁne-tuning."
INSTANTIATING THE META-FRAMEWORK,0.18726591760299627,"3.2
INSTANTIATING THE META-FRAMEWORK"
INSTANTIATING THE META-FRAMEWORK,0.19101123595505617,"In this section, we discuss a few ways to instantiate our meta-framework. This list is non-exhaustive,
but covers the methods we employ in our experiments."
FINE-TUNING VIA LOW-RANK ADAPTATION,0.1947565543071161,"3.2.1
FINE-TUNING VIA LOW-RANK ADAPTATION"
FINE-TUNING VIA LOW-RANK ADAPTATION,0.19850187265917604,"Low-Rank Adaptation (LoRA) (Hu et al., 2021) is an additive ﬁne-tuning scheme as deﬁned in (3).
For each dense weight matrix W i
PT of size a × b in the pre-trained network, we add a low-rank
correction term LiRi, i.e.,"
FINE-TUNING VIA LOW-RANK ADAPTATION,0.20224719101123595,"W i = W i
PT + LiRi,
(4)"
FINE-TUNING VIA LOW-RANK ADAPTATION,0.20599250936329588,"where Li ∈Ra×r, Ri ∈Rr×b are new trainable parameters. Hu et al. (2021) apply this repa-
rameterization only to the Transformer attention weights (Wq, Wv), and freeze all other weights
(e.g., Wk and Wo and those in the feed-forward layers). The rank r is typically chosen to be small,
e.g., r = 4, 16, 64. Since most parameters in Transformer architectures are dense weight matrices,
choosing a small r results in a nearly square-root reduction in the number of parameters."
FINE-TUNING VIA ADAPTERS,0.20973782771535582,"3.2.2
FINE-TUNING VIA ADAPTERS"
FINE-TUNING VIA ADAPTERS,0.21348314606741572,"Houlsby et al. (2019) propose adapter-based ﬁne-tuning, in which we modify the architecture of
the pre-trained model by adding new “adapter” layers after each attention and feed-forward layer.
Adapter layers are bottleneck layers with residual connections. Speciﬁcally, given an input x, an
adapter layer A performs
A(x) = U(τ(D(x))) + x,
(5)"
FINE-TUNING VIA ADAPTERS,0.21722846441947566,"where U is an up-projection afﬁne linear map, D is a down-projection afﬁne linear map, and τ
is a non-linear activation function such as the Gaussian error Linear Unit (GeLU) (Hendrycks &
Gimpel, 2016). If x has dimension d, then U ∈Rd×r, D ∈Rr×d for some r ≪d. Thus, the num-
ber of introduced parameters is signiﬁcantly less than the number of parameters in the pre-trained
model. When ﬁne-tuning, the parameters of the original model are frozen, and only parameters of
the adapter layers, as well as layer normalizations, are modiﬁed. Note that ﬁne-tuning with adapters
is not an additive ﬁne-tuning framework as in (3), but is captured by the broader framework in (1)."
FINE-TUNING VIA ADAPTERS,0.2209737827715356,Published as a conference paper at ICLR 2022
FINE-TUNING VIA ADAPTERS,0.2247191011235955,"Table 2: Memory and speed comparison for RoBERTa-Large. The rank is chosen as r = 16 for
RGP and LoRA. The speed is measured by the wall-clock time for training one epoch of the SST-2
dataset on a single Tesla V100 GPU with gradient accumulation for batch size 2000."
FINE-TUNING VIA ADAPTERS,0.22846441947565543,"Method
Memory (GB)
Speed (seconds per epoch)
Full ﬁne-tuning (DPSGD)
27.9
715
RGP
9.1
296
DP LoRA
6.1
271"
FINE-TUNING VIA COMPACTER,0.23220973782771537,"3.2.3
FINE-TUNING VIA COMPACTER"
FINE-TUNING VIA COMPACTER,0.23595505617977527,"The recent work of Mahabadi et al. (2021) introduces Compacters (Compact adapters), a method
which further improves the parameter efﬁciency of adapters. This is done by replacing the dense
matrices in the up-projection U and down-projection D by tensor products of smaller matrices, thus
reducing the number of trainable parameters. Speciﬁcally, they replace the dense matrix Mℓin the
adapter layer ℓby a low-rank parameterized hypercomplex multiplication (LPHM) layer, i.e., each
dense matrix Mℓ∈Ra×b is expressed as Mℓ= n
X"
FINE-TUNING VIA COMPACTER,0.2397003745318352,"i=1
Ai ⊗
 
Sℓ
i T ℓ
i

(6)"
FINE-TUNING VIA COMPACTER,0.24344569288389514,"where Ai ∈Rn×n, Sℓ
i ∈Ra/n×k, T ℓ
i ∈Rk×b/n and ⊗is the matrix Kronecker product. Note
the matrices Ai are not indexed by the layer ℓbecause these matrices are shared among all the
adapter layers. Since each adapter layers has two dense matrices (one for up-projection and one for
down-projection), if there are L adapter layers, this reduces the number of parameters from L(2ab)
to L(2(a + b)k) + n3. In practice, a and b are chosen to be either the model dimension d or the
intermediate representation dimension r in the adapters, n is typically chosen to be a small constant
such as n = 2, 4, 8, 12 and k is chosen to be 1."
FINE-TUNING VIA COMPACTER,0.24719101123595505,"3.2.4
WHY DOES PARAMETER-EFFICIENT TUNING WORK?"
FINE-TUNING VIA COMPACTER,0.250936329588015,"Theoretical explanation of success of parameter-efﬁcient ﬁne-tuning methods is active area of re-
search in deep learning. Indeed, since trends have consistently shown that model accuracy increases
with size, how can one achieve competitive accuracy while ﬁne-tuning less than 1% of the param-
eters? One popular hypothesis is intrinsic dimensionality (Li et al., 2018), which posits that the
minimum number of parameters needed to train a machine learning model may be much less than
the total number of model parameters. Aghajanyan et al. (2020) explore this hypothesis in the con-
text of ﬁne-tuning LLMs, showing that one can achieve most of their accuracy by training only a very
small number of parameters (chosen via a random projection). Perhaps surprisingly, they ﬁnd that
as the model size increases, intrinsic dimension decreases, in the limit exhibiting zero-shot learning.
While we did not explore this hypothesis in the context of DP due to computational restrictions, we
believe it may be an interesting lens through which one can understand the effectiveness of private
parameter-efﬁcient ﬁne-tuning."
COMPARISION WITH BASELINE ALGORITHMS,0.2546816479400749,"3.3
COMPARISION WITH BASELINE ALGORITHMS"
COMPARISION WITH BASELINE ALGORITHMS,0.25842696629213485,"We highlight some key algorithmic differences between our proposed methods and the baselines of
full ﬁne-tuning and RGP."
COMPARISION WITH BASELINE ALGORITHMS,0.26217228464419473,"• DPSGD and RGP both require updating all parameters of the pre-trained model, whereas
our proposed methods update only a tiny fraction (between 0.05% and 1%). The rightmost
columns of Tables 3 and 4 list the number of parameters trained by these algorithms."
COMPARISION WITH BASELINE ALGORITHMS,0.26591760299625467,"• RGP performs a low-rank decomposition of weight matrices which is similar to LoRA,
though there are subtle differences. Recall that in RGP, at the beginning of each iteration
t, the historical weight matrix Wt−1 is decomposed to ﬁnd a low-rank product LR. The
gradients on L and R are then projected back to the full parameter space to perform the
descent step. Hence, RGP keeps modifying the pre-trained weights during learning.
LoRA can be viewed as a simpliﬁcation of RGP. LoRA reparametrizes WFT := WPT +LR,
where the pre-trained weight matrix WPT is frozen during training. Hence, compared to"
COMPARISION WITH BASELINE ALGORITHMS,0.2696629213483146,Published as a conference paper at ICLR 2022
COMPARISION WITH BASELINE ALGORITHMS,0.27340823970037453,"Table 3: Accuracy for ﬁne-tuning with RoBERTa-Base (in %). The privacy parameters are ε = 6.7,
and δ =1e-5 for SST-2 and QNLI and 1e-6 for MNLI and QQP. Bold indicates the best accuracy
with DP. Numbers for non-private ﬁne-tuning are from Liu et al. (2019) and Hu et al. (2021)."
COMPARISION WITH BASELINE ALGORITHMS,0.27715355805243447,"Method
MNLI
SST-2
QQP
QNLI
Avg.
Trained params"
COMPARISION WITH BASELINE ALGORITHMS,0.2808988764044944,"Full
w/o DP
87.6
94.8
91.9
92.8
91.8
100%
DP
53.1
82.6
74.4
63.9
68.5
LoRA
w/o DP
87.5
95.1
90.8
93.3
91.7
0.24%
RGP3
DP
80.1
91.6
85.5
87.2
86.1
100%
Adapter
DP
83.4
92.5
85.6
87.5
87.3
1.4% (r = 48)
Compacter
DP
82.6
92.3
84.7
85.1
86.2
0.055% (r = 96, n = 8)
LoRA
DP
83.5
92.2
85.7
87.3
87.2
0.94% (r = 16)"
COMPARISION WITH BASELINE ALGORITHMS,0.2846441947565543,"RGP, LoRA eliminates the decomposition and the projection to the full parameter space at
each iteration, simplifying the implementation and reducing the running time and memory
cost. This is summarized in Table 2. We observe that DP LoRA reduces the memory cost
by about 33% and the training speed by 8%. As we will see, this simpliﬁcation also results
in improved utility."
COMPARISION WITH BASELINE ALGORITHMS,0.2883895131086142,"• Neither full ﬁne-tuning nor RGP fall into our meta-framework described by (1). Thus, if a
pre-trained model is to be applied to several downstream tasks, one must store a separate
set of weights for each task, incurring a signiﬁcant memory cost and losing the plug-in
functionality. In contrast, our methods are much more lightweight."
EXPERIMENTS,0.29213483146067415,"4
EXPERIMENTS"
EXPERIMENTS,0.2958801498127341,"We experimentally evaluate our methods for DP ﬁne-tuning to demonstrate their utility, pri-
vacy, and parameter-efﬁciency.
We investigate both language understanding and text genera-
tion tasks to establish that our techniques are applicable to a variety of tasks and model ar-
chitectures.
Our code is publicly available at https://github.com/AnonymousAKES/
Differentially-Private-Fine-tuning-of-Language-Models."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.299625468164794,"4.1
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.30337078651685395,"We ﬁrst compare our methods with state-of-the-art ﬁne-tuning algorithms using models from the
BERT family, which was used in the prior work (Yu et al., 2021b). Speciﬁcally, we use RoBERTa
models (Liu et al., 2019), which are pre-trained on public data collected from the web. RoBERTa-
Base has 125M parameters and RoBERTa-Large has 355M parameters. We choose four downstream
tasks: MNLI, QQP, QNLI, and SST-2 from GLUE (Wang et al., 2018), following Yu et al. (2021b)."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.30711610486891383,"Implementation Details: For ﬁne-tuning with adapters, we may choose the intermediate repre-
sentation dimension r, shared across all adapter layers. For ﬁne-tuning with Compacter, we can
choose both r and the Kronecker product kernel dimension n in (6). For LoRA ﬁne-tuning, we add
bottleneck branches for both the attention layers and the feedforward layers, which differs slightly
from the addition of bottleneck branches for only the Wq and Wv matrices of the attention layers
as done by Hu et al. (2021). Given the same bottleneck representation dimension r in (4), our new
implementation uses twice as many trainable parameters as the original paper, and achieves some
improvements for learning with DP. We perform privacy accounting using the approach of Gopi
et al. (2021), which currently gives the tightest bounds."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.31086142322097376,"Hyperparameter choice: Given the large number of hyperparameter choices, e.g., the interme-
diate representation dimension, learning rate, weight decay, privacy parameter δ, and model size,
an exhaustive grid search over all hyperparameters is expensive. Our hyperparameter choices are
informed by prior work and are as follows. For privacy parameters, we use δ = 1e-5 for SST-2
and QNLI and δ = 1e-6 for QQP and MNLI due to their dataset sizes, and use noise multipliers
0.92, 0.83, 0.66 and 0.65 for SST-2, QNLI, QQP, and MNLI, respectively, which is the same as Yu
et al. (2021b). In Appendix B, we run experiments under different privacy parameters. The proposed
framework performs well under a wide range choices of ε and δ. The clipping threshold is 10 for"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3146067415730337,3Numbers are from https://github.com/dayu11/Differentially-Private-Deep-Learning.
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.31835205992509363,Published as a conference paper at ICLR 2022
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.32209737827715357,"Table 4: Accuracy for ﬁne-tuning with RoBERTa-Large (in %). The privacy parameters are ε = 6.7,
and δ =1e-5 for SST-2 and QNLI and δ =1e-6 for MNLI and QQP. Bold indicates the best accuracy
with DP. Numbers for non-private ﬁne-tuning are from Liu et al. (2019) and Hu et al. (2021)."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3258426966292135,"Method
MNLI
SST-2
QQP
QNLI
Avg.
Trained params
Full
w/o DP
90.2
96.4
92.2
94.7
93.4
100%
LoRA
w/o DP
90.6
96.2
91.6
94.9
93.3
0.23%
RGP
DP
86.1
93.0
86.7
90.0
88.9
100%
Adapter
DP
87.7
93.9
86.3
90.7
89.7
1.4% (r = 48)
Compacter
DP
87.5
94.2
86.2
90.2
89.5
0.053% (r = 96, n = 8)
LoRA
DP
87.8
95.3
87.4
90.8
90.3
0.94% (r = 16)"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3295880149812734,"all methods. The batch size is 2000. In Appendix D, we show the performance of the proposed al-
gorithm is stable across a wide range of choices of clipping thresholds and batch sizes. For adapters
and Compacter, we follow the original papers and choose r from {16, 48, 96} and n from {4, 8, 12}.
For LoRA, we choose the best-performing rank r from the set {4, 16, 48, 64}. The best performing
hyperparameters are noted in Tables 3 and 4. We train for 20 epochs using AdamW (Loshchilov &
Hutter, 2019) with weight decay 1e-2 and search over four learning rates {5e-4, 1e-3, 2e-3, 5e-3}."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3333333333333333,"Results: We report the prediction accuracy in Tables 3 and 4. Our experiments using RoBERTa-
Base serve as a direct comparison to Yu et al. (2021b) who only trained the base model, whereas
RoBERTa-Large experiments demonstrate the signiﬁcance of using larger models. Our key ﬁnd-
ings are: (1) On all datasets, our methods achieve the best accuracy while training a tiny fraction
of parameters; larger models give signiﬁcant improvements. (2) Noticeable improvements in ε ver-
sus Yu et al. (2021b) are primarily due to new privacy accountants based on Fourier-based numerical
composition (Koskela et al., 2020; 2021; Gopi et al., 2021); we use the accountant in Gopi et al.
(2021) since it is the most efﬁcient. (3) Private adapters provide the best average performance for
RoBERTa-Base, whereas LoRA outperforms all other methods for RoBERTa-Large."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.33707865168539325,"Remark: While our experiments indicate that full ﬁne-tuning does not achieve competitive perfor-
mance, there could be a choice of hyperparameters that improves upon the reported numbers, e.g.,
“mega” batch sizes (in the millions) in Anil et al. (2021). We note that our main message is that one
does not need to ﬁne-tune all parameters to achieve the best accuracy. Nevertheless, it is interesting
to wonder if full ﬁne-tuning with DPSGD can match the accuracy of parameter-efﬁcient methods. A
positive answer would imply that private and non-private ﬁne-tuning conceptually mirror each other."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3408239700374532,"Update: A concurrent work by Li et al (Li et al., 2022) show that using a larger batch size and
training with full-precision improves the performance of full ﬁne-tuning via DPSGD, and obtains
similar performance as our algorithms. Thus, poor performance of DPSGD in our experiments is
due to the suboptimal choice of hyperparameters and also due to precision issues, although we use
same hyperparameters for all the algorithms. We run new experiments with hyperparameters of (Li
et al., 2022) in full precision mode, and get improvements around 1% even for our algorithms. We
report these ﬁndings in Appendix C."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3445692883895131,"4.2
FINE-TUNING FOR NATURAL LANGUAGE GENERATION (NLG)"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.34831460674157305,"Next, we study private ﬁne-tuning for text generation problems using the GPT-2 series of models on
the End-2-End (E2E) NLG challenge (Novikova et al., 2017), one of the primary benchmarks used
in recent works on non-private ﬁne-tuning (Hu et al., 2021; Li & Liang, 2021). We use GPT-2-Small
(117M parameters), GPT-2-Medium (345M parameters), and GPT-2-Large (774M parameters).4 To
the best of our knowledge, we are the ﬁrst to privately ﬁne-tune for E2E or ﬁne-tune GPT-2-Large.
The purpose of this section is not to evaluate various ﬁne-tuning algorithms, but to show that private
ﬁne-tuning is competitive with non-private ﬁne-tuning for text generation problems. Due to the high
cost of training, we report experimental results only for ﬁne-tuning with LoRA."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.352059925093633,"In Appendix F, we present additional experiments on NLG that include private ﬁne-tuning of the
GPT-2-XL model with 1.5 billion parameters. Other noticeable points in the additional experiments"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.35580524344569286,4https://huggingface.co/transformers/model_doc/gpt2.html.
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3595505617977528,Published as a conference paper at ICLR 2022
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.36329588014981273,"Table 5: Metrics on the E2E NLG task (ε = 5.4, δ =1e-5). Non-DP results from Hu et al. (2021)."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.36704119850187267,"Method
BLEU
NIST
MET
ROUGE-L
CIDEr
GPT-2-Small + DP
59.26
6.13
36.6
64.1
1.63
GPT-2-Medium + DP
64.2
7.77
40.02
66.45
2.00
GPT-2-Large + DP
64.51
8.22
41.5
67.55
2.13
GPT-2-Medium
70.4
8.85
46.8
71.8
2.53
GPT-2-Large
70.4
8.89
46.8
72.0
2.47"
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3707865168539326,"include 1) we show improved performance using better hyperparameters; 2) we test different privacy
parameters; 3) we consider a new dataset DART (Nan et al., 2021)."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.37453183520599254,"E2E NLG challenge: The E2E dataset in Novikova et al. (2017) contains template-like information
in the restaurant domain to be mapped to natural language with end-to-end training. The dataset
consists of 42K training samples, 4.6K validation samples, and 4.6K test samples. We use standard
metrics such as BLUE, ROUGE-L, etc., used in (Hu et al., 2021) for evaluation."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.3782771535580524,"Hyperparameter choice: For LoRA, we choose the bottleneck rank r = 4 in (4) and ﬁne-tune Wq
and Wv matrices of the attention layers as in the original paper. We optimize using AdamW with
learning rate 2e-4, weight decay 1e-2 and train our models for 5 epochs using batch size 64. We take
the gradient clipping parameter to be 1.0 and set the noise multiplier as 0.5."
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS,0.38202247191011235,"Results: The results of our experiments are summarized in the Table 5, which reiterate the main
themes of this paper: private ﬁne-tuning with a parameter-efﬁcient approach performs close to their
non-private counterparts and show consistent improvement in the utility as the model size increases."
RELATED WORK,0.3857677902621723,"5
RELATED WORK"
RELATED WORK,0.3895131086142322,"Some work studies private language models on more traditional architectures such as
LSTMs (Hochreiter & Schmidhuber, 1997), either training with DPSGD (McMahan et al., 2018;
Carlini et al., 2019) or related heuristics (Ramaswamy et al., 2020). Though pre-training on public
data is suggested (McMahan et al., 2018), public data appears to only be used in one of these works
for honest hyperparameter selection (Ramaswamy et al., 2020). A few more recent works consider
training LLMs with DP. Anil et al. (2021) privately train BERT-Large from scratch, compared to
our work which focuses on private ﬁne-tuning.
(Hoory et al., 2021; Basu et al., 2021) perform
private full ﬁne-tuning of BERT models. Hoory et al. (2021) achieve accuracy which is comparable
to the non-private model, but additionally supplement the public pre-training data with additional
domain-relevant material, while we use off-the-shelf pre-trained models. Basu et al. (2021) observe
signiﬁcant drops in utility, compared to our parameter-efﬁcient methods which do not. While Kerri-
gan et al. (2020) consider public pre-training and private ﬁne-tuning, their experiments are on much
smaller architectures (i.e., feedforward networks with three hidden layers). A simultaneous work
of Ginart et al. (2022) investigates private prediction (rather than learning) for next-token predic-
tion. A subsequent work by Senge et al. (2021) also investigates the effect of private ﬁne-tuning on
various NLP tasks."
RELATED WORK,0.39325842696629215,"In a concurrent work, Li et al. (2022) also investigate DP ﬁne-tuning of LLMs. In several cases, their
results demonstrate qualitatively similar ﬁndings as ours. While our experiments focus primarily on
parameter-efﬁcient ﬁne-tuning methods, interestingly, they show that private full ﬁne-tuning can
also achieve comparable utility if the experimental setup is conﬁgured properly, e.g., using suitable
hyperparameters. In Appendix C, we run experiments under the setup in Li et al. (2022). We show
their setup can also improve the performance of our methods."
CONCLUSION,0.3970037453183521,"6
CONCLUSION"
CONCLUSION,0.40074906367041196,"So far, DP deep learning has focused on training models from scratch. The spectacular success of
transfer learning in real-world applications, however, shows that private ﬁne-tuning is an equally per-
tinent problem to study and deserves more attention. We show that by combining recent advances in
NLP, parameter-efﬁciency, privacy accounting, and using larger models, one can privately ﬁne-tune
models whose utility approaches that of non-private models. We hope our work inspires more study
on the core problem of private ﬁne-tuning, which we believe to be a central direction for research in
private machine learning, leading to more interaction between the LLM and DP communities."
CONCLUSION,0.4044943820224719,Published as a conference paper at ICLR 2022
CONCLUSION,0.40823970037453183,ACKNOWLEDGMENTS
CONCLUSION,0.41198501872659177,"The authors would like to thank Rabeeh Karimi Mahabadi for sharing hyperparameters based on
experiments in Mahabadi et al. (2021). Janardhan Kulkarni would like to thank Edward Hu for
sharing many ideas on ﬁne-tuning. Gautam Kamath is supported by an NSERC Discovery Grant."
REFERENCES,0.4157303370786517,REFERENCES
REFERENCES,0.41947565543071164,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM Conference
on Computer and Communications Security, CCS ’16, pp. 308–318, New York, NY, USA, 2016.
ACM."
REFERENCES,0.4232209737827715,"Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the ef-
fectiveness of language model ﬁne-tuning. arXiv preprint arXiv:2012.13255, 2020."
REFERENCES,0.42696629213483145,"Noga Alon, Raef Bassily, and Shay Moran. Limits of private learning with access to public data. In
Advances in Neural Information Processing Systems 32, NeurIPS ’19, pp. 10342–10352. Curran
Associates, Inc., 2019."
REFERENCES,0.4307116104868914,"Ehsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas Steinke,
Vinith M Suriyakumar, Om Thakkar, and Abhradeep Thakurta. Public data-assisted mirror de-
scent for private model training. arXiv preprint arXiv:2112.00193, 2021."
REFERENCES,0.4344569288389513,"Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differen-
tially private BERT. arXiv preprint arXiv:2108.01624, 2021."
REFERENCES,0.43820224719101125,"Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efﬁ-
cient algorithms and tight error bounds. In Proceedings of the 55th Annual IEEE Symposium on
Foundations of Computer Science, FOCS ’14, pp. 464–473, Washington, DC, USA, 2014. IEEE
Computer Society."
REFERENCES,0.4419475655430712,"Raef Bassily, Om Thakkar, and Abhradeep Guha Thakurta. Model-agnostic private learning. In
Advances in Neural Information Processing Systems 31, NeurIPS ’18, pp. 7102–7112. Curran
Associates, Inc., 2018."
REFERENCES,0.44569288389513106,"Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and Steven Wu.
Private query release assisted by public data. In Proceedings of the 37th International Conference
on Machine Learning, ICML ’20, pp. 695–703. JMLR, Inc., 2020a."
REFERENCES,0.449438202247191,"Raef Bassily, Shay Moran, and Anupama Nandi. Learning from mixtures of private and public
populations. In Advances in Neural Information Processing Systems 33, NeurIPS ’20, pp. 2947–
2957. Curran Associates, Inc., 2020b."
REFERENCES,0.45318352059925093,"Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib Singh, and Fatemehsadat
Mireshghallah. Benchmarking differential privacy and federated learning for BERT models. arXiv
preprint arXiv:2106.13973, 2021."
REFERENCES,0.45692883895131087,"Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approxi-
mate differential privacy. Theory of Computing, 12(1):1–61, 2016."
REFERENCES,0.4606741573033708,"Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning
for transformer-based masked language-models. arXiv preprint arXiv:, 2021."
REFERENCES,0.46441947565543074,"Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization
of irrelevant training data necessary for high-accuracy learning?
In Proceedings of the 53nd
Annual ACM Symposium on the Theory of Computing, STOC ’21, pp. 123–132, New York, NY,
USA, 2021. ACM."
REFERENCES,0.4681647940074906,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M."
REFERENCES,0.47191011235955055,Published as a conference paper at ICLR 2022
REFERENCES,0.4756554307116105,"Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners.
In Advances in
Neural Information Processing Systems 33, NeurIPS ’20. Curran Associates, Inc., 2020."
REFERENCES,0.4794007490636704,"Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate
differential privacy. In Proceedings of the 46th Annual ACM Symposium on the Theory of Com-
puting, STOC ’14, pp. 1–10, New York, NY, USA, 2014. ACM."
REFERENCES,0.48314606741573035,"Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for
efﬁcient on-device learning. In Advances in Neural Information Processing Systems 33, NeurIPS
’20, pp. 11285–11297. Curran Associates, Inc., 2020."
REFERENCES,0.4868913857677903,"Nicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:
Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security
Symposium, USENIX Security ’19, pp. 267–284. USENIX Association, 2019."
REFERENCES,0.49063670411985016,"Nicholas Carlini, Florian Tram`er, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raf-
fel. Extracting training data from large language models. In 30th USENIX Security Symposium,
USENIX Security ’21, pp. 2633–2650. USENIX Association, 2021."
REFERENCES,0.4943820224719101,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), NAACL-HLT ’19, pp. 4171–4186, 2019."
REFERENCES,0.49812734082397003,"Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Proceedings of the 24th Annual Interna-
tional Conference on the Theory and Applications of Cryptographic Techniques, EUROCRYPT
’06, pp. 486–503, Berlin, Heidelberg, 2006a. Springer."
REFERENCES,0.50187265917603,"Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography, TCC
’06, pp. 265–284, Berlin, Heidelberg, 2006b. Springer."
REFERENCES,0.5056179775280899,"Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings
of the 52nd Annual ACM Symposium on the Theory of Computing, STOC ’20, pp. 954–959, New
York, NY, USA, 2020. ACM."
REFERENCES,0.5093632958801498,"Antonio Ginart, Laurens van der Maaten, James Zou, and Chuan Guo. Submix: Practical private
prediction for large-scale language models. arXiv preprint arXiv:2201.00971, 2022."
REFERENCES,0.5131086142322098,"Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy.
arXiv preprint arXiv:2106.02848, 2021."
REFERENCES,0.5168539325842697,"Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
arXiv:1606.08415, 2016."
REFERENCES,0.5205992509363296,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735–1780, 1997."
REFERENCES,0.5243445692883895,"Shlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell, Itay Laish, Hootan Nakhost,
Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, and Yossi Matias. Learning and evaluating
a differentially private pre-trained language model. In Proceedings of the Third Workshop on
Privacy in Natural Language Processing, PrivateNLP ’21, pp. 21–29, 2021."
REFERENCES,0.5280898876404494,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.
In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019."
REFERENCES,0.5318352059925093,"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
2021."
REFERENCES,0.5355805243445693,Published as a conference paper at ICLR 2022
REFERENCES,0.5393258426966292,"Zhanglong Ji and Charles Elkan. Differential privacy based on importance weighting. Machine
Learning, 93(1):163–183, 2013."
REFERENCES,0.5430711610486891,"Peter Kairouz, M´onica Ribero, Keith Rush, and Abhradeep Thakurta. (nearly) dimension indepen-
dent private ERM with adagrad rates via publicly estimated subspaces. In Proceedings of the 34th
Annual Conference on Learning Theory, COLT ’21, pp. 2717–2746, 2021."
REFERENCES,0.5468164794007491,"Gavin Kerrigan, Dylan Slack, and Jens Tuyls. Differentially private language models beneﬁt from
public pre-training. arXiv preprint arXiv:2009.05886, 2020."
REFERENCES,0.550561797752809,"Antti Koskela, Joonas J¨alk¨o, and Antti Honkela. Computing tight differential privacy guarantees
using fft. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2560–2569.
PMLR, 2020."
REFERENCES,0.5543071161048689,"Antti Koskela, Joonas J¨alk¨o, Lukas Prediger, and Antti Honkela.
Tight differential privacy for
discrete-valued mechanisms and for the subsampled gaussian mechanism using fft. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pp. 3358–3366. PMLR, 2021."
REFERENCES,0.5580524344569289,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP), EMNLP ’21. Association for Computational Linguistics, 2021."
REFERENCES,0.5617977528089888,"Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimen-
sion of objective landscapes. In Proceedings of the 6th International Conference on Learning
Representations, ICLR ’18, 2018."
REFERENCES,0.5655430711610487,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
ACL-IJCNLP ’21, pp. 4582–4597, 2021."
REFERENCES,0.5692883895131086,"Xuechen Li, Florian Tram`er, Percy Liang, and Tatsunori Hashimoto. Large language models can
be strong differentially private learners. In Proceedings of the 10th International Conference on
Learning Representations, ICLR ’22, 2022."
REFERENCES,0.5730337078651685,"Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and Steven Wu. Leveraging
public data for practical private query release. In Proceedings of the 38th International Conference
on Machine Learning, ICML ’21, pp. 6968–6977. JMLR, Inc., 2021."
REFERENCES,0.5767790262172284,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.5805243445692884,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the
7th International Conference on Learning Representations, ICLR ’19, 2019."
REFERENCES,0.5842696629213483,"Zelun Luo, Daniel J Wu, Ehsan Adeli, and Li Fei-Fei. Scalable differential privacy with sparse
network ﬁnetuning. In Proceedings of the 2021 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, CVPR ’21, pp. 5059–5068, Washington, DC, USA, 2021. IEEE
Computer Society."
REFERENCES,0.5880149812734082,"Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank
hypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021."
REFERENCES,0.5917602996254682,"H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models.
In Proceedings of the 6th International Conference on Learning
Representations, ICLR ’18, 2018."
REFERENCES,0.5955056179775281,"Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740, 2017."
REFERENCES,0.599250936329588,Published as a conference paper at ICLR 2022
REFERENCES,0.602996254681648,"Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,
Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jes-
sica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu,
Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani.
DART: open-domain structured data record to text generation. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT ’21, pp. 432–447. Association for Computational Linguis-
tics, 2021."
REFERENCES,0.6067415730337079,"Anupama Nandi and Raef Bassily. Privately answering classiﬁcation queries in the agnostic PAC
model. In Proceedings of the 31st International Conference on Algorithmic Learning Theory,
ALT ’20, pp. 687–703. JMLR, Inc., 2020."
REFERENCES,0.6104868913857678,"Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser.
The E2E dataset: New challenges for
end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and
Dialogue, SIGDIAL ’17, pp. 201–206. Association for Computational Linguistics, 2017."
REFERENCES,0.6142322097378277,"Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.
In Proceedings of the Third Conference on Machine Translation (WMT), 2018."
REFERENCES,0.6179775280898876,"Nicolas Papernot, Mart´ın Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar.
Semi-
supervised knowledge transfer for deep learning from private training data. In Proceedings of
the 5th International Conference on Learning Representations, ICLR ’17, 2017."
REFERENCES,0.6217228464419475,"Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and ´Ulfar Er-
lingsson. Scalable private learning with PATE. In Proceedings of the 6th International Conference
on Learning Representations, ICLR ’18, 2018."
REFERENCES,0.6254681647940075,"Nicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making
the shoe ﬁt: Architectures, initializations, and tuning for learning with privacy.
https://
openreview.net/forum?id=rJg851rYwH, 2019."
REFERENCES,0.6292134831460674,"Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-
fusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Con-
ference of the European Chapter of the Association for Computational Linguistics: Main Volume,
EACL ’21, pp. 487–503. Association for Computational Linguistics, 2021."
REFERENCES,0.6329588014981273,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018."
REFERENCES,0.6367041198501873,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners, 2019."
REFERENCES,0.6404494382022472,"Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and
Franc¸oise Beaufays. Training production language models without memorizing user data. arXiv
preprint arXiv:2009.10031, 2020."
REFERENCES,0.6441947565543071,"Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and
Iryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers. arXiv preprint
arXiv:2010.11918, 2020."
REFERENCES,0.6479400749063671,"Manuel Senge, Timour Igamberdiev, and Ivan Habernal. One size does not ﬁt all: Investigating
strategies for differentially-private learning across nlp tasks. arXiv preprint arXiv:2112.08159,
2021."
REFERENCES,0.651685393258427,"Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In Proceedings of the 38th IEEE Symposium on Security
and Privacy, SP ’17, pp. 3–18, Washington, DC, USA, 2017. IEEE Computer Society."
REFERENCES,0.6554307116104869,"Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with dif-
ferentially private updates. In Proceedings of the 2013 IEEE Global Conference on Signal and
Information Processing, GlobalSIP ’13, pp. 245–248, Washington, DC, USA, 2013. IEEE Com-
puter Society."
REFERENCES,0.6591760299625468,Published as a conference paper at ICLR 2022
REFERENCES,0.6629213483146067,"Pranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private sgd
via just-in-time compilation and vectorization. In Advances in Neural Information Processing
Systems 34, NeurIPS ’21. Curran Associates, Inc., 2021."
REFERENCES,0.6666666666666666,"Zhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang, Nevin Zhang, and He He.
Seq-
PATE: Differentially private text generation via knowledge distillation, 2022.
URL https:
//openreview.net/forum?id=5sP_PUUS78v."
REFERENCES,0.6704119850187266,"Florian Tram`er and Dan Boneh. Differentially private learning needs better features (or much more
data). In Proceedings of the 9th International Conference on Learning Representations, ICLR
’21, 2021."
REFERENCES,0.6741573033707865,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30, NIPS ’17, pp. 5998–6008. Curran Associates, Inc., 2017."
REFERENCES,0.6779026217228464,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding.
In
International Conference on Learning Representations, 2018."
REFERENCES,0.6816479400749064,"Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), NAACL-HLT ’18, pp. 1112–1122. Association for Computational Lin-
guistics, 2018."
REFERENCES,0.6853932584269663,"Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient
embedding perturbation for private learning. In Proceedings of the 9th International Conference
on Learning Representations, ICLR ’21, 2021a."
REFERENCES,0.6891385767790262,"Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via
low-rank reparametrization. In Proceedings of the 38th International Conference on Machine
Learning, ICML ’21. JMLR, Inc., 2021b."
REFERENCES,0.6928838951310862,"Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Pri-
vate SGD with gradient subspace identiﬁcation. In Proceedings of the 9th International Confer-
ence on Learning Representations, ICLR ’21, 2021."
REFERENCES,0.6966292134831461,Published as a conference paper at ICLR 2022
REFERENCES,0.700374531835206,"Table 6: Test accuracy for ﬁne-tuning RoBERTa-Large with different privacy parameters. The num-
ber of training samples is denoted by n. The values of σ are noise multipliers. Numbers in the
brackets are the changes compared to the results in Table 4 (ε = 6.7, δ = Θ(1/n))."
REFERENCES,0.704119850187266,"Taks
σ
δ = 1/n
δ = 1/10n
δ = 1/100n
δ = 1/1000n
Accuracy (in %)
MNLI
1.88
ε = 1
ε = 1.35
ε = 1.49
ε = 1.61
86.8 (-1.0%)
QQP
1.88
ε = 1
ε = 1.40
ε = 1.54
ε = 1.67
85.2 (-2.2%)
QNLI
3.01
ε = 1
ε = 1.48
ε = 1.64
ε = 1.79
88.0 (-2.8%)
SST-2
3.63
ε = 1
ε = 1.47
ε = 1.64
ε = 1.80
93.1 (-2.2%)"
REFERENCES,0.7078651685393258,"MNLI
0.91
ε = 3
ε = 4.12
ε = 4.51
ε = 4.89
87.4 (-0.4%)
QQP
0.93
ε = 3
ε = 4.10
ε = 4.49
ε = 4.86
86.8 (-0.6%)
QNLI
1.29
ε = 3
ε = 4.45
ε = 4.90
ε = 5.33
89.9 (-0.9%)
SST-2
1.52
ε = 3
ε = 4.37
ε = 4.83
ε = 5.25
94.1 (-1.2%)"
REFERENCES,0.7116104868913857,"A
ADDITIONAL RELATED WORK"
REFERENCES,0.7153558052434457,"There exist other parameter-efﬁcient tuning methods which we did not evaluate in our work. Some
of these include random subspace projection (exploiting intrinsic dimensionality (Li et al., 2018;
Aghajanyan et al., 2020)), preﬁx and prompt tuning (Li & Liang, 2021; Lester et al., 2021), tuning
only biases (Cai et al., 2020; Ben Zaken et al., 2021), and other architecture variants including
Adapters (Pfeiffer et al., 2021; R¨uckl´e et al., 2020). An interesting direction for future work is
to see whether parameter-efﬁcient tuning approaches speciﬁcally designed for the private setting
can achieve higher utility. We also mention zero-shot learning, in which no task-speciﬁc dataset is
required and thus perfect privacy is achieved. Currently, zero-shot approaches achieve low utility
compared to ﬁne-tuning, though it is possible that future models may narrow this gap."
REFERENCES,0.7191011235955056,"Finally, our investigation ﬁts more broadly into a line of work employing public data for private
data analysis. Some works on image classiﬁcation consider pre-training on a large public dataset
and ﬁne-tuning on a smaller private dataset (Abadi et al., 2016; Papernot et al., 2019; Tram`er &
Boneh, 2021; Luo et al., 2021). In particular, Luo et al. (2021) investigate the role of parameter
efﬁciency in private ﬁne-tuning ResNet models, and propose strategies to choose which parameters
to ﬁne-tune. One line of work uses unlabeled public data to train a student model (Papernot et al.,
2017; 2018; Bassily et al., 2018), including one work simultaneous to our own for natural language
generation Tian et al. (2022). Another recent idea uses a small amount of public data to identify a
lower-dimensional subspace of the gradients in which to perform private descent (Zhou et al., 2021;
Yu et al., 2021a; Kairouz et al., 2021). A simultaneous work of Amid et al. (2021) uses public data
in the mirror map for a private mirror descent algorithm. Finally, other works (both theoretical and
experimental) investigate the role of public data in private query release, synthetic data generation,
and prediction (Ji & Elkan, 2013; Beimel et al., 2016; Alon et al., 2019; Nandi & Bassily, 2020;
Bassily et al., 2020a;b; Liu et al., 2021)."
REFERENCES,0.7228464419475655,"B
EXPERIMENTS WITH DIFFERENT PRIVACY PARAMETERS"
REFERENCES,0.7265917602996255,"Now we test our framework under different privacy constraints. Speciﬁcally, we run LoRA on the
language understanding tasks with various choices of privacy parameters ε and δ. We consider both
RoBERTa-Base and RoBERTa-Large."
REFERENCES,0.7303370786516854,"For the RoBERTa-Large model, we set ε = 1 and 3 with δ being the same as those in Section 4.
We use the PRV accountant (Gopi et al., 2021). After getting the noise multipliers, we also reduce
the value of δ and report the corresponding value of ε. The hyperparameters are the same as those
in Section 4. We run experiments on all four tasks, i.e., MNLI (n ∼392k), QQP (n ∼364k),
QNLI (n ∼104k), and SST-2 (n ∼67k). We report the results in Table 6. The performance
of our framework is decent even with very tight privacy budgets. For instance, with ε < 2 and
δ = 1/1000n, the accuracy gap between the non-private baseline is only 3.8 for MNLI and 2.1 for
SST-2."
REFERENCES,0.7340823970037453,Published as a conference paper at ICLR 2022 85 90 95
REFERENCES,0.7378277153558053,Accuracy SST-2
REFERENCES,0.7415730337078652,"0.1
0.5
1
3
5
8
12
50 51 52 53"
REFERENCES,0.7453183520599251,Accuracy
REFERENCES,0.7490636704119851,"Non-private
DP LoRA 75 80 85 90"
REFERENCES,0.7528089887640449,Accuracy MNLI
REFERENCES,0.7565543071161048,"0.1
0.5
1
3
5
8
12
53.0 53.5 54.0 54.5 55.0"
REFERENCES,0.7602996254681648,Accuracy
REFERENCES,0.7640449438202247,"Non-private
DP LoRA"
REFERENCES,0.7677902621722846,"Figure 2: Test accuracy (in %) of ﬁne-tuning the RoBERTa-Base model on MNLI and SST-2 with
various choices of ε."
REFERENCES,0.7715355805243446,"Table 7: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Base (in %). Experiments are
run with full-precision. We also scale up the batch size according to the dataset size compared to
SST-2. The privacy parameters are ε = 6.7, and δ =1e-5 for SST-2 and QNLI and 1e-6 for MNLI
and QQP."
REFERENCES,0.7752808988764045,"Method
MNLI
SST-2
QQP
QNLI
Average Accuracy"
REFERENCES,0.7790262172284644,"Full
w/o DP
87.6
94.8
91.9
92.8
91.8
DP
83.2
85.9
86.2
84.8
85.0
Adapter
DP
84.6
92.9
87.4
89.2
88.5
LoRA
DP
84.5
92.7
87.1
88.3
88.2"
REFERENCES,0.7827715355805244,"For the RoBERTa-Base model, we try various choices of ε. The values of ε are chosen from
[0.1, 0.5, 1, 3, 5, 8, 12]. All other settings are the same as those in Section 4. We run experiments on
the MNLI and SST-2 datasets. The results are presented in Figure 2. Our framework performs well
for a wide range of ε. We note that our algorithm achieves meaningful accuracy even for very tight
privacy parameters ε = 0.5 and 1. Such values of ε are rarely explored when training deep models
with differential privacy."
REFERENCES,0.7865168539325843,"C
FINE-TUNING FOR LANGUAGE UNDERSTANDING TASKS WITH LARGE
BATCH SIZE AND FULL-PRECISION"
REFERENCES,0.7902621722846442,"Li et al. (2022) show the performance of ﬁne-tuning the full model can be signiﬁcantly improved
with proper conﬁguration. In this section, we re-evaluate the tasks in Table 3 and 4 under the
conﬁguration in Li et al. (2022) and show such a conﬁguration also improves the performance of
our methods."
REFERENCES,0.7940074906367042,"The conﬁguration in Li et al. (2022) has two major differences compared to that in Section 4.1.
The ﬁrst difference is Li et al. (2022) run experiments with full-precision while the experiments"
REFERENCES,0.797752808988764,"Table 8: Accuracy for ﬁne-tuning downstream tasks with RoBERTa-Large (in %). Experiments are
run with full-precision. We also scale up the batch size according to the dataset size compared to
SST-2. The privacy parameters are ε = 6.7, and δ =1e-5 for SST-2 and QNLI and δ =1e-6 for
MNLI and QQP."
REFERENCES,0.8014981273408239,"Method
MNLI
SST-2
QQP
QNLI
Average Accuracy"
REFERENCES,0.8052434456928839,"Full
w/o DP
90.2
96.4
92.2
94.7
93.4
DP
86.4
90.9
87.5
89.4
88.6
Adapter
DP
88.6
94.5
87.8
91.6
90.6
LoRA
DP
89.0
95.3
88.4
92.4
91.3"
REFERENCES,0.8089887640449438,Published as a conference paper at ICLR 2022 200 500 1000 2000 4000
REFERENCES,0.8127340823970037,Batchsize 0.1 1.0 3.0 5.0 10.0
REFERENCES,0.8164794007490637,Clipping Threshold
REFERENCES,0.8202247191011236,"90.8
91.3
92.1
92.4
92.7"
REFERENCES,0.8239700374531835,"91.2
91.4
91.7
92.2
92.5"
REFERENCES,0.8277153558052435,"90.9
91.4
91.9
92.0
92.5"
REFERENCES,0.8314606741573034,"90.7
91.5
91.9
92.1
92.6"
REFERENCES,0.8352059925093633,"90.7
91.6
92.2
92.6
92.8"
REFERENCES,0.8389513108614233,Learning Rate 0.0005 200 500 1000 2000 4000
REFERENCES,0.8426966292134831,Batchsize 0.1 1.0 3.0 5.0 10.0
REFERENCES,0.846441947565543,Clipping Threshold
REFERENCES,0.850187265917603,"91.0
92.0
92.1
92.3
92.7"
REFERENCES,0.8539325842696629,"91.5
91.9
92.2
92.7
92.9"
REFERENCES,0.8576779026217228,"91.1
92.0
92.2
92.7
92.8"
REFERENCES,0.8614232209737828,"91.3
92.1
92.2
92.9
92.8"
REFERENCES,0.8651685393258427,"91.2
92.1
92.8
92.9
92.9"
REFERENCES,0.8689138576779026,Learning Rate 0.0010
REFERENCES,0.8726591760299626,"Figure 3: Test accuracy (in %) of ﬁne-tuning RoBERTa-Base with differentially private LoRA on
the SST-2 dataset. Our algorithm performs well on a wide range of hyperparameters."
REFERENCES,0.8764044943820225,"Table 9: Non-private metrics on the E2E NLG task, using full ﬁne-tuning."
REFERENCES,0.8801498127340824,"Method
BLEU
NIST
MET
ROUGE-L
CIDEr
GPT-2-Medium
68.2
8.62
46.2
71.0
2.47
GPT-2-Large
68.5
8.78
46.0
69.9
2.45"
REFERENCES,0.8838951310861424,"in Section 4.1 use half-precision. Using half-precision is a common approach to speed up NLP
experiments (Ott et al., 2018). However, half-precision may incur underﬂow issue which impacts
the model performance (Micikevicius et al., 2017). The second difference is they use larger batch
size for larger datasets. For example, the batch size for MNLI is roughly six times larger than the
batch size for SST-2 in Li et al. (2022). In Section 4.1, we use the same batch size for all datasets."
REFERENCES,0.8876404494382022,"We follow the above setup and re-evaluate DP-LoRA and DP-Adapter. The results are in Table 7
and 8. The results of full ﬁne-tuning with differential privacy are directly adopted from Li et al.
(2022). The conﬁguration in Li et al. (2022) further improves the strong results in Table 3 and 4.
For example, we achieve 89.0% accuracy on the MNLI dataset, which is only 1.2% lower than the
accuracy without DP constraint. Moreover, the beneﬁt of the proposed framework over full ﬁne-
tuning is still clear. The average accuracy of the proposed algorithms is ∼3% higher than that of full
ﬁne-tuning."
REFERENCES,0.8913857677902621,"D
ON THE INFLUENCE OF HYPERPARAMETERS"
REFERENCES,0.8951310861423221,"Here we demonstrate that our algorithms perform well for a wide range of hyperparameters. We
study two hyperparameters that are directly related to the variance of noise: clipping threshold and
batchsize. The clipping threshold is chosen from [0.1, 1.0, 3.0, 5.0, 10.0] and the batchsize is chosen
from [200, 500, 1000, 2000, 4000]. We note that we keep the number of updates the same as that in
Section 4 when the batchsize is changed. We ﬁne-tune the RoBERTa-Base model with differentially
private LoRA (r = 16) on the SST-2 dataset. The results are presented in Figure 3. DP LoRA
performs well for all the hyperparameters considered. The gap between the best accuracy and the
worst accuracy is only 2%."
REFERENCES,0.898876404494382,"E
FULL FINE-TUNING WITH GPT-2"
REFERENCES,0.9026217228464419,"All results in Table 5 (in the main body), both private and non-private, perform ﬁne-tuning using
LoRA. In Table 9, we additionally report utility of non-private full ﬁne-tuning. These numbers are
taken from Table 1 of Li & Liang (2021). In general, these numbers are slightly lower than those
obtained by performing non-private ﬁne-tuning with LoRA."
REFERENCES,0.9063670411985019,Published as a conference paper at ICLR 2022
REFERENCES,0.9101123595505618,"Table 10: Metrics on the E2E NLG task. Non-DP results from Hu et al. (2021), except for GPT-2-
XL, which was not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Hu
et al. (2021). Bold indicates the best accuracy with DP. DP parameters are (ε = 6.0, δ = 1e-5). Val
perp stands for validation perplexity."
REFERENCES,0.9138576779026217,"Method
Val perp
BLEU
NIST
MET
ROUGE-L
CIDEr
GPT-2-Small + DP
4.51
63.8
7.19
39.5
67.5
1.87
GPT-2-Medium + DP
4.02
65.5
8.45
42.7
67.9
2.23
GPT-2-Large + DP
3.87
66.7
8.63
44.0
67.8
2.33
GPT-2-XL + DP
3.79
66.1
8.53
43.0
68.1
2.28
GPT-2-Medium
3.19
70.4
8.85
46.8
71.8
2.53
GPT-2-Large
3.06
70.4
8.89
46.8
72.0
2.47
GPT-2-XL
3.01
69.4
8.78
46.2
71.5
2.49"
REFERENCES,0.9176029962546817,"Table 11: Metrics on the E2E NLG task. Bold indicates the best accuracy with DP. DP parameters
satisfy (ε = 3.0, δ = 1e-5), (ε = 3.4, δ = 1/10n), (ε = 3.9, δ = 1/100n) and (ε = 4.5, δ =
1/1000n). Val perp stands for validation perplexity."
REFERENCES,0.9213483146067416,"Method
Val perp
BLEU
NIST
MET
ROUGE-L
CIDEr
GPT-2-Small + DP
4.59
62.7
7.03
39.2
66.4
1.85
GPT-2-Medium + DP
4.08
65.2
8.31
42.2
68.1
2.22
GPT-2-Large + DP
3.92
66.7
8.60
43.6
68.1
2.29
GPT-2-XL + DP
3.85
67.6
8.64
44.9
68.6
2.36"
REFERENCES,0.9250936329588015,"F
ADDITIONAL EXPERIMENTS ON NATURAL LANGUAGE GENERATION"
REFERENCES,0.9288389513108615,"In this section, we perform additional experiments on private ﬁne-tuning for text generation prob-
lems using the GPT-2 series of models. This includes the private ﬁne-tuning of the GPT-2-XL model
with 1.5B parameters. There are three main points to note compared to our results in the main body:
1) We show an improved performance on E2E NLG challenge using better hyperparameters; 2) We
conduct experiments on E2E dataset with different privacy parameters to show that large language
models perform strong even with smaller privacy budgets; 3) Finally, we conduct new experiments
on DART dataset."
REFERENCES,0.9325842696629213,"F.1
IMPROVING THE PERFORMANCE ON E2E NLG CHALLENGE"
REFERENCES,0.9363295880149812,We improve the results of Table 5 with the following set of new hyperparameters.
REFERENCES,0.9400749063670412,"Hyperparameter choice: For LoRA, we choose the bottleneck rank r = 4 in (4) and ﬁne-tune Wq
and Wv matrices of the attention layers as in the original paper. We optimize using AdamW with
learning rate 4e-4, weight decay 1e-2 and train our models for 20 epochs. We use batch size 128. We
take the gradient clipping parameter to be 1.0 and the noise multiplier to be 0.6 for the accountant
in Gopi et al. (2021), achieving ε = 6.0, δ =1e-5."
REFERENCES,0.9438202247191011,Results: The results of our experiments are summarized in the Table 10.
REFERENCES,0.947565543071161,"F.2
EXPERIMENTS WITH DIFFERENT PRIVACY PARAMETERS"
REFERENCES,0.951310861423221,"On E2E dataset, we test our framework with smaller privacy budgets (ε < 5 and δ ≪1/n) where n
is the number of samples in the training data."
REFERENCES,0.9550561797752809,"Hyperparameter choice: The hyperparameter choices are similar as in Section F.1. The only
difference is that we increase the noise multiplier to be 0.71 for the accountant in Gopi et al. (2021),
achieving the following (ε, δ) pairs: (ε = 3.0, δ =1e-5), (ε = 3.4, δ = 1/10n), (ε = 3.9, δ =
1/100n) and (ε = 4.5, δ = 1/1000n)."
REFERENCES,0.9588014981273408,Results: The results of our experiments are summarized in the Table 11.
REFERENCES,0.9625468164794008,"There are a couple of interesting observations comparing Table 11 with Table 10. First, we observe
that although privacy budget is tight in Table 11, the results are quite similar to Table 10, which"
REFERENCES,0.9662921348314607,Published as a conference paper at ICLR 2022
REFERENCES,0.9700374531835206,"Table 12: Metrics on the DART dataset. Non-DP results from Hu et al. (2021), except for GPT-2-
XL, which was not reported in the paper. We ran GPT-2-XL with hyperparameters presented in Hu
et al. (2021). Bold indicates the best accuracy with DP. DP parameters are (ε = 6.8, δ = 1e-5). Val
perp stands for validation perplexity. Unlike all other metrics, the lower the TER metric is the better
for the performance of the model."
REFERENCES,0.9737827715355806,"Method
Val perp
BLEU
MET
TER
GPT-2-Small + DP
3.82
38.5
0.34
0.53
GPT-2-Medium + DP
3.30
42.0
0.36
0.51
GPT-2-Large + DP
3.10
43.1
0.36
0.5
GPT-2-XL + DP
3.00
43.8
0.37
0.5
GPT-2-Medium
2.67
47.1
0.39
0.46
GPT-2-Large
2.89
47.5
0.39
0.45
GPT-2-XL
2.83
48.1
0.39
0.46"
REFERENCES,0.9775280898876404,"shows that our methods also perform very well under stronger privacy guarantees. A more inter-
esting observation is that under smaller epsilon regimes, the performance for private ﬁne-tuning of
GPT-2-XL model improves. Observe that the performance improvement is more prominent going
from GPT-2-Small to GPT-2-XL in this setting, which may indicate that larger models can be even
more effective in private learning when the privacy budgets are tight."
REFERENCES,0.9812734082397003,"F.3
PERFORMING EXPERIMENTS ON DART DATASET"
REFERENCES,0.9850187265917603,"We study the DART dataset as a text generation problem for private ﬁne-tuning of GPT-2 series of
models."
REFERENCES,0.9887640449438202,"DART: DART was introduced as an open-domain data-to-text dataset by Nan et al. (2021). The
dataset consists of 62K training samples, 6.9K validation samples, and 12K test samples. In com-
parison to E2E, the dataset is larger and the task is more challenging."
REFERENCES,0.9925093632958801,"Hyperparameter choice: The hyperparameter choices are similar as in the previous setting. The
only difference is that we use batch size 256 for the experiments on DART. This achieves ε =
6.8, δ =1e-5 on DART using the accountant in Gopi et al. (2021)."
REFERENCES,0.9962546816479401,Results: The results of our experiments are summarized in the Table 12.
