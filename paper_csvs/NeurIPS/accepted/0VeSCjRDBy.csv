Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011668611435239206,"Knowledge distillation (KD) has been shown to be highly effective in guiding
a student model with a larger teacher model and achieving practical benefits in
improving the computational and memory efficiency for large language models
(LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit
distribution distance between teacher and student probability predictions. Instead
of optimizing these mandatory behavior cloning objectives, we explore an imitation
learning strategy for KD of LLMs. In particular, we minimize the imitation
gap by matching the action-value moments of the teacher’s behavior from both
on- and off-policy perspectives. To achieve this action-value moment-matching
goal, we propose an adversarial training algorithm to jointly estimate the moment-
matching distance and optimize the student policy to minimize it. Results from
both task-agnostic instruction-following experiments and task-specific experiments
demonstrate the effectiveness of our method and achieve new state-of-the-art
performance."
INTRODUCTION,0.002333722287047841,"1
Introduction"
INTRODUCTION,0.003500583430571762,"Large language models (LLMs) like GPT-4 [1] and LLaMA [35] have revolutionized natural language
processing, significantly enhancing the quality of text generation across various tasks. This success is
largely due to the extensive scale of training data and the substantial increase in model parameters
[19]. However, the high computational and memory requirements of these models present significant
challenges for practical deployment. To address these issues, knowledge distillation (KD) [16] has
emerged as a key technique. KD involves transferring knowledge from a large, complex teacher model
to a smaller, more efficient student model, thereby maintaining high performance while reducing
resource demands. Most distillation methods for auto-regressive text generation models, including
LLMs, employ metrics of probability distribution distance, such as Kullback-Leibler (KL) divergence
[20] and reverse KL divergence [14], aiming to align the token-level probability distributions between
the teacher and student models."
INTRODUCTION,0.004667444574095682,"The distribution matching-based distillation methods can be viewed as behavior cloning on a decision-
making problem from the perspective of imitation learning [24, 14, 2]. Based on this concept, early
works based on the teacher-generated outputs [20] or a supervised dataset [30] can be viewed as an off-
policy approach. Recent works further incorporate an on-policy approach, training the student on its
self-generated outputs [24], using KL-based divergence [14, 2, 21] and total variation (TV) distance
[39]. Accordingly, such distribution matching-based methods face the sub-optimality problem. The
objective functions aimed at aligning the probability distributions between the teacher and student
models can be straightforward but cannot fully capture the goal of distilling language knowledge.
First, intuitively, the correct output for an input can vary, and thus behavior cloning cannot capture the
full knowledge of a teacher. Besides, there is no standardized definition for the quality of a generated
output given an input, which makes it difficult to define the objective of knowledge distillation. This"
INTRODUCTION,0.005834305717619603,"��
...
��
..."
INTRODUCTION,0.007001166861143524,"��
...
��
... �’ ��
�∗ ℳ ~��"
INTRODUCTION,0.008168028004667444,"���(�≤�, ��+1)
���(�≤�, �’)"
INTRODUCTION,0.009334889148191364,"��
...
��
..."
INTRODUCTION,0.010501750291715286,"�≤�
��
...
��
... ��+1 �’ ��
�∗ ℳ ~�∗"
INTRODUCTION,0.011668611435239206,"��∗(�≤�, ��+1)
��∗(�≤�, �’)"
INTRODUCTION,0.012835472578763127,(a) On-policy distribution-matching distillation.
INTRODUCTION,0.014002333722287048,(c) On-policy Q-value moment-match. distillation (ours).
INTRODUCTION,0.015169194865810968,(b) Off-policy distribution-matching distillation.
INTRODUCTION,0.01633605600933489,"(d) Off-policy Q-value moment-match. distillation (ours). ��
�� ��
�� �∗
�∗ �∗
�∗ �’� �’�−1 ... �� ... ��−1 �’� ... �’�−1 �� ... ��−1 �≤�+1 ��+1"
INTRODUCTION,0.01750291715285881,"�≤�
�≤�+1
�≤�"
INTRODUCTION,0.01866977829638273,"�≤�
�≤�+1 �≤�+1 ��+1 ��+1"
INTRODUCTION,0.019836639439906652,"Figure 1: The comparison between the distribution-matching-based distillation and the action-value
moment-matching distillation is outlined. πθ and π∗denote the student policy and the teacher policy,
respectively. For both on-policy (using student-generated outputs) and off-policy (using teacher-
generated outputs) perspectives, our approach optimizes moment-matching of action-value functions
(Q-functions) instead of minimizing the distribution distance measured by M = KL, RKL, TV, etc."
INTRODUCTION,0.021003500583430573,"imposes a significant limitation on the generalization performance of the student model through
distillation."
INTRODUCTION,0.022170361726954493,"To address the aforementioned issues, we employ a reinforcement learning (RL) formulation for
the auto-regressive text generation problem and utilize the definition of imitation gap to describe
the high-level goal of knowledge distillation. Additionally, we address the imitation gap for KD by
matching moments of the action-value function, which reflects the quality of token-level predictions
for the entire output. In addressing the action-value function, we adopt the approach of Swamy et
al. [33], considering a two-player minimax game between the language policy and the action-value
functions, aiming to minimize an upper bound of the moment-matching objective. For this purpose,
we introduce an adversarial training algorithm based on the policy gradient to jointly optimize the
on-/off-policy objectives. Figure 1 illustrates the overall approach."
INTRODUCTION,0.023337222870478413,"Theoretically, we compare the moment-matching objective with other distribution-matching mea-
surements such as step-wise TV distance and analyze the convergence rate of our algorithm to
an ϵ-accurate stationary point for optimization. Empirically, we evaluate our approach on both
the instruction-following dataset and three task-specific datasets for text summarization, machine
translation, and commonsense reasoning. Results demonstrate that the proposed adversarial moment-
matching approach effectively optimizes the moment-matching distance of the imitation gap and
outperforms state-of-the-art KD methods and a range of distribution-matching-based methods. The
code and implementation are released at https://github.com/jiachenwestlake/MMKD."
RELATED WORK,0.024504084014002333,"2
Related Work"
RELATED WORK,0.025670945157526253,"Distillation of large language models. There has been an increasing interest in knowledge distillation
(KD) of auto-regressive LMs, especially concerning large language models (LLMs) [41, 42]. This
process effectively transfers elicited knowledge from teacher LLMs to smaller student models, aiming
to compress the large size of neural network parameters and make LLMs more efficient. Sequence-
level KD (SeqKD) [20] is a variation of supervised fine-tuning (SFT) in KD. It can be viewed as the
simplest method for distillation of black-box LLMs by fine-tuning the student model with teacher-
generated outputs. This method has been extensively used for LLMs and has achieved success [34, 6].
In contrast, distillation of white-box LLMs can make full use of internal information of the teacher
model, such as logits [30, 39] and hidden states [23], for distribution alignment, making it more
effective and efficient for KD. However, unlike previous work that explicitly clones the distribution
of teacher LLMs into student models, this work learns an auxiliary Q-value function to guide KD."
RELATED WORK,0.026837806301050177,"Distillation via distribution matching. Most promising results in the distillation of white-box LLMs
are achieved by minimizing divergence between the probability distributions of the teacher model"
RELATED WORK,0.028004667444574097,"and student models. Kullback-Leibler (KL) divergence, reverse Kullback-Leibler (RKL) divergence,
and Jensen–Shannon (JS) divergence are three widely used KD objectives for auto-regressive LMs
[39, 14, 2, 21, 41]. Wen et al. [39] have shown the equivalent formulations of sequence-level KL,
RKL, JS divergences, and the step-wise terms. Additionally, they also present the strong performance
of step-wise total variation (TV) distance for KD, which can upper bound the sequence-level term.
As a result, most recent works focus on on-policy approaches for KD [2] and combine the real-
time-generated outputs by students (on-policy) with the real-time-generated outputs by teachers (or
from supervised datasets) (off-policy). Following this line, Gu et al. [14] further propose a policy
gradient-based method to address the high variance issues of RKL-based methods while Ko et al.
[21] propose a more efficient and effective method using a skew KL divergence loss and an adaptive
off-policy approach. We also focus on a combination of on-policy and off-policy objectives for
KD, but we introduce a more sophisticated moment-matching approach instead of directly using the
well-studied distribution-matching metrics such as KL, RKL, JS divergences, and TV distance."
RELATED WORK,0.029171528588098017,"Distillation via reinforcement learning. In a common formulation of RL in text generation [44, 26,
15], an auto-regressive model can be viewed as a language policy, making decisions on the next token
(action) based on the currently generated sequence (state). From this perspective, KD corresponds
to behavior cloning in imitation learning [20, 7, 14, 2]. For imitation learning in text generation,
early works such as SeqGAN [44] and TextGAIL [40] utilize a generative adversarial framework to
balance between the reward model, optimized by discriminating generated/real-word text, and the
language policy, optimized by policy gradient-based methods using the reward model. Existing work
on KD via imitation learning refers to ImitKD [24], which optimizes the student policy by learning
from demonstrations of the teacher model. RL-based distillation can also be especially relevant for
leveraging the feedback from the teacher to train student models [4, 9], in which teacher models
are used to generate the feedback data for training a reward model. We build our method upon an
RL-based imitation learning framework. However, unlike previous work [20, 14, 2], we propose an
adversarial moment-matching approach to enhance behavior cloning."
METHOD,0.030338389731621937,"3
Method"
NOTATIONS AND DEFINITIONS,0.03150525087514586,"3.1
Notations and Definitions"
NOTATIONS AND DEFINITIONS,0.03267211201866978,"In this section, we consider the text generation task as a decision-making process and give a corre-
sponding reinforcement learning (RL) formulation."
NOTATIONS AND DEFINITIONS,0.0338389731621937,"Text generation. Given an input x, the auto-regressive generation task in our work aims to generate
a sequence of tokens as the output (y1, . . . , yT ), where yt comes from a vocabulary V. For simplicity,
we define y = (y0, y1, . . . , yT ) as the full input-output sequence, where y0 = x denotes the input.
The generator is modeled by a conditional probability distribution pθ(y|x) = ΠT −1
t=0 pθ(yt+1|y≤t),
where y≤t denotes the prefix (y0, y1, . . . , yt), t ∈{0, 1, . . . , T −1}."
NOTATIONS AND DEFINITIONS,0.03500583430571762,"RL formulation. We model text generation as a finite-horizon, time-independent Markov de-
cision process.
At each time step t ∈{0, . . . , T −1}, the policy πθ takes an action (t):
yt+1 ∈V based on the current state (t): y≤t ∈Y, transits to the next state (t + 1):
y≤t+1 ∈Y and receives a reward (t): r(y≤t, yt+1) by a reward function r : Y × V →R.
The policy corresponds to the generation model πθ(yt+1|y≤t) = pθ(yt+1|y≤t). We focus on
a (conditional) trajectory {y1, y≤1, y2, . . . , y≤T −1, yT } =: τ
∼πθ|x which refers to a se-
quence of state-action pairs generated by given an initial state y0 = x ∼px and then repeat-
edly sampling an action yt+1 ∼πθ(·|y≤t) and obtain the next state y≤t+1 ∼T(·|y≤t, yt+1)1
for T time steps.
In such case, the probability of a (conditional) trajectory is formally rep-
resented as p(τ|x, πθ) = ΠT −1
t=0 T(y≤t+1|y≤t, yt+1)πθ(yt+1|y≤t).
We also define our value"
NOTATIONS AND DEFINITIONS,0.03617269544924154,"function and Q-value function as V πθ(y≤t) = Eτ(t)∼πθ|y≤t
hPT −1
t′=t γt′−tr(y≤t′, yt′+1)
i
and"
NOTATIONS AND DEFINITIONS,0.03733955659276546,"Qπθ(y≤t, yt+1) = Eτ(t)∼πθ|y≤t,yt+1
hPT −1
t′=t γt′−tr(y≤t′, yt′+1)
i
, where γ ∈(0, 1) denotes the
discounting factor. We define the RL objective in our generation task to maximize the performance
J(πθ) = Ex∼pxEτ∼πθ|x
hPT −1
t=0 γtr(y≤t, yt+1)
i
."
NOTATIONS AND DEFINITIONS,0.038506417736289385,"1In text generation, the state-transition is commonly assumed to be deterministic [44, 26], i.e.,
T(y≤t+1|y≤t, yt+1) = 1."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.039673278879813305,"3.2
Knowledge Distillation as Moment-Matching Imitation Learning"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.040840140023337225,"Based on the RL formulation of auto-regressive generation, we can view the goal of knowledge
distillation at a high-level as to bridge the performance gap between the teacher policy and the student
policy."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.042007001166861145,"Definition 1 (Imitation gap). We define the imitation gap between the teacher policy and student
policy as:"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.043173862310385065,"J(π∗) −J(πθ) =
E
x∼px
τ∼π∗|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.044340723453908985,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.045507584597432905,"t=0
γtr(y≤t, yt+1) #"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.046674445740956826,"−
E
x∼px
τ∼πθ|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.047841306884480746,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.049008168028004666,"t=0
γtr(y≤t, yt+1) # ,
(1)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.050175029171528586,"From the perspective of imitation learning [33, 32], the objective of distillation from the teacher
policy π∗to the student policy πθ can be represented as to minimize the imitation gap of Eq. (1) w.r.t.
the parameters of student policy θ. A direct idea from Eq. (1) is to use moment matching over the
reward to optimize the imitation gap [33]. However, we actually care about the long-term reward,
at each time step, we should consider the accumulated reward in the future output rather than the
immediate reward to the fitness of previous tokens (prefix). To this end, we can alternatively use the
Q-value function (def. in §3.1) for each timestep to represent the overall reward from the current
timestep to the last timestep. Similar to [33], we can apply the Performance Difference Lemma (PDL)
[18, 3, 33] to expand the imitation gap in Eq. (1) into either off-policy or on-policy expressions."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.051341890315052506,"Proposition 1 (Off-policy bound of imitation gap [33]). Let FQ denote the set of Q-value functions
induced by sampling actions from πθ, then we have:"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.052508751458576426,"J(π∗) −J(πθ) ≤sup
f∈FQ
E
x∼px
τ∼π∗|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.05367561260210035,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.05484247374562427,"t=0
γt"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.056009334889148193,"f(y≤t, yt+1) −
E
y∼πθ(·|y≤t)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.057176196032672114,"
f(y≤t, y)

!#"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.058343057176196034,"|
{z
}
=:Loff(πθ,f) (2)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.059509918319719954,"In the following sections, we will use Loff(πθ, f) to represent the off-policy moment-matching
objective of mitation learning for KD."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.060676779463243874,"The off-policy moment-matching objective in Proposition 1 only requires a collected dataset of
teacher-generated trajectories to be evaluated and minimized."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.061843640606767794,"Proposition 2 (On-policy bound of imitation gap [33]). Let FQ∗denote the set of Q-value functions
induced by sampling actions from π∗, then we have:"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.06301050175029171,"J(π∗) −J(πθ) ≤
sup
f∈FQ∗
E
x∼px
τ∼πθ|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.06417736289381563,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.06534422403733955,"t=0
γt"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.06651108518086347,"E
y∼π∗(·|y≤t)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.0676779463243874,"
f(y≤t, y)

−f(y≤t, yt+1) !#"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.06884480746791131,"|
{z
}
=:Lon(πθ,f) (3)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07001166861143523,"In the following sections, we will use Lon(πθ, f) to represent the on-policy moment-matching
objective of an imitation learning for KD."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07117852975495916,"Proof. See Appendix A.1 and Appendix A.2 for the complete derivations of Proposition 1 and
Proposition 2, respectively."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07234539089848308,"It is notable from Proposition 2 that the on-policy moment-matching objective requires interactions
with the teacher to tell us what action they would take in any state visited by the student as well as
on-policy samples from the student’s current policy τ ∼πθ|x."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.073512252042007,"In the remaining content of this section, we will explore the relationship between the moment-
matching objectives and the existing distribution-matching objectives [39]. At the beginning, we
draw a general formulation of the state-of-the-art methods for distillation of LLMs [39, 14, 2, 21] that
rely on distribution-matching between the student’s and teacher’s predictions, through minimizing
the step-wise probability distribution distance between the teacher policy and student policy."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07467911318553092,"Definition 2 (Generalized step-wise distribution distance). The off-policy and on-policy versions
are defined as follows,"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07584597432905485,"doff
M(πθ, π∗) :=
E
x∼px
τ∼π∗|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07701283547257877,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07817969661610269,"t=0
γtM(π∗(·|y≤t), πθ(·|y≤t)) # ;
(4)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.07934655775962661,"don
M(πθ, π∗) :=
E
x∼px
τ∼πθ|x"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08051341890315053,"""T −1
X"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08168028004667445,"t=0
γtM(π∗(·|y≤t), πθ(·|y≤t)) # ,
(5)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08284714119019837,"where M(·, ·) denotes a distribution distance, consisting of total variation (TV) distance [39] and
Kullback-Leibler (KL)-based divergence [14, 2]. Detailed definitions for these distances refer to
Appendix A.3. For simplicity, we directly replace M with TV, KL, RKL, etc in the following sections."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08401400233372229,"It is notable from Wen et al. [39] that the sequence-level KL, RKL and JS divergences can be
equivalently represented as the step-wise terms, and the sequence-level TV distance can be upper
bounded by the step-wise terms, which can be actually implemented by algorithms. To make a
connection with the step-wise distribution distance (Definition 2), we use the following definition."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08518086347724621,"Definition 3 (Distribution-matching formulation of moment-matching objectives). Based on
Definition 2, we can re-formulate the off-policy and on-policy moment-matching (MM) objectives
(Proposition 1 and Proposition 2, respectively) via step-wise distribution-matching, which can be
defined as doff
MM(πθ, π∗) and don
MM(πθ, π∗) respectively, where the distance metric MM(·, ·) can be
defined as follows,"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08634772462077013,"MMoff(on)(π∗(·|y≤t), πθ(·|y≤t))=
E
y∼π∗(·|y≤t)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08751458576429405,"h
f off(on)
∗
(y≤t, y)
i
−
E
y∼πθ(·|y≤t)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08868144690781797,"h
f off(on)
∗
(y≤t, y)
i
,"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.08984830805134189,"Off-policy: f off
∗
= arg max
f∈FQ
Loff(πθ, f);
On-policy: f on
∗
= arg max
f∈FQ∗
Lon(πθ, f),
(6)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09101516919486581,"where Loff(πθ, f) and Lon(πθ, f) denote the off-policy and on-policy moment-matching objectives,
which are defined in Proposition 1 and Proposition 2, respectively."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09218203033838973,"Under Definition 3, we observe that the main difference between the moment-matching objectives
and other step-wise distribution distance, e.g., TV distance and KL-based divergences in formulation
comes from the optimal Q-value function f off(on)
∗
, aiming to maximize the discrepancy of its expec-
tations based on π∗(·|y≤t) v.s. πθ(·|y≤t) for each step t ∈{0, 1, . . . , T −1}. To look deeper, we
draw a connection between the moment-matching objectives and step-wise TV distance using the
following corollary."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09334889148191365,"Theorem 1 (Relationship between moment-matching objective and TV distance). Under a
constrain of uniform boundness on the class of Q-value functions for off-/on-policy learning: FQ =
FQ∗= {f : ∥f∥∞≤1}, the moment-matching objectives in Proposition 1 and Proposition 2 can be
upper-bounded by the step-wise TV distance, Formally, we have"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09451575262543757,"J(π∗) −J(πθ) ≤
sup
f:∥f∥∞≤1
Loff(πθ, f) ≤2doff
TV(πθ, π∗);
(7)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09568261376896149,"J(π∗) −J(πθ) ≤
sup
f:∥f∥∞≤1
Lon(πθ, f) ≤2don
TV(πθ, π∗),
(8)"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09684947491248541,"for the off-policy and on-policy perspectives, respectively."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09801633605600933,Proof. See Appendix A.4 for the complete derivation.
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.09918319719953325,"We can observe from Theorem 1 that minimizing the step-wise TV distance can achieve sub-
optimal results compared to optimizing the moment-matching objectives Loff(πθ, f), Lon(πθ, f)
for off-policy and on-policy imitation learning, which are defined in Proposition 1 and Proposition
2, respectively. Thus, optimizing the moment-matching objectives can potentially achieve better
optimization results for imitation learning."
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.10035005834305717,"Algorithm 1: Adversarial training procedure
Input: Dataset Dxy with inputs and ground-truth outputs
Teacher policy π∗; Student policy πθ with initial parameters θ pretrained on Dxy; Off-policy Q-value
function fϕ1 and on-policy Q-value function fϕ2 with initial parameters ϕ1 and ϕ2, respectively;
Step sizes K (outer), N (inner); Learning rate η; Controlling factor α; Off-/on-policy combination factor β
Output: The optimized student policy πθ∗
for k = 0, 1, 2, . . . , K −1 do"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.10151691948658109,"for n = 0, 1, 2, . . . , N −1 do"
KNOWLEDGE DISTILLATION AS MOMENT-MATCHING IMITATION LEARNING,0.10268378063010501,"Sample an input x ∼Dx and generate an trajectory τ off ∼π∗|x
ϕ1 ←ϕ1 + αβη∇ϕ1 ˆLoff(τ off, θk, fϕ1)
 maximize Loff(πθk, fϕ1) in Eq. (9)
Sample an input x ∼Dx and generate an trajectory τ on ∼πθ|x
ϕ2 ←ϕ2 + α(1 −β)η∇ϕ2 ˆLon(τ on, θk, fϕ2)
 maximize Lon(πθk, fϕ2) in Eq. (9)
end
Sample an input xk ∼Dx and generate trajectories τ off
k
∼π∗|xk and τ on
k
∼πθ|xk
θk+1 ←θk −η

−β ˆGoff(τ off
k , θk) + (1 −β) ˆGon(τ on
k , θk)

 minimize L(πθ, fϕ1, fϕ2) in Eq. (9) end"
ADVERSARIAL TRAINING ALGORITHM,0.10385064177362893,"3.3
Adversarial Training Algorithm"
ADVERSARIAL TRAINING ALGORITHM,0.10501750291715285,"Optimization objective. As shown in previous work [14, 2, 21] incorporating both the off-policy
and on-policy distillation benefits effectiveness and efficiency. We thus consider a training objective
to jointly minimize the off-policy moment-matching objective in Proposition 1 and the on-policy
moment-matching objective in Proposition 2. Both the off-/on-policy objectives can be optimized
by viewing the learning procedure as solving a game. More specifically, we consider a two-player
minimax game between the student policy and the Q-value functions. To this end, we initialize
two small networks of a single-layer MLP to estimate the off-/on-policy Q-value functions, respec-
tively. For example in a causal/seq-to-seq LM, the Q-value estimate module can be represented as
fϕ1(2)(y≤t, y) = (hπθ
t
+ voff(on)
y
)⊤woff(on)
y
for any action token y ∈V. This estimates the Q-value
function by taking the current t ∈{0, 1, . . . , T −1} hidden step of a policy network hπθ
t
∈RH (for
next token prediction) to combine with the feature vector of the token voff(on)
y
∈RH with a linear
transformation by woff(on)
y
∈RH for off(on)-policy learning. Here, H represents the hidden size
and the additional parameter cost is O(H|V|) for Q-value estimation. Finally, combining off- and
on-policy objectives with a factor β ∈(0, 1), the optimization problem can be represented as follows,"
ADVERSARIAL TRAINING ALGORITHM,0.10618436406067679,"min
θ∈Θ
max
ϕ1,ϕ2∈Φ βLoff(πθ, fϕ1) + (1 −β)Lon(πθ, fϕ2)
|
{z
}
=:L(πθ,fϕ1,fϕ2) ,
(9)"
ADVERSARIAL TRAINING ALGORITHM,0.1073512252042007,"where L(πθ, fϕ1, fϕ2) represents the overall training objective. To minimize the objective w.r.t the
policy parameters θ, we use a policy gradient approach and derive the policy gradient in Appendix
A.5, formally represented as follows,"
ADVERSARIAL TRAINING ALGORITHM,0.10851808634772463,"∇L(πθ, fϕ1, fϕ2) =
E
x∼px"
ADVERSARIAL TRAINING ALGORITHM,0.10968494749124855,"
−β
E
τ∼π∗|x"
ADVERSARIAL TRAINING ALGORITHM,0.11085180863477247," ˆGoff(τ, θ)

+ (1 −β)
E
τ ′∼πθ|x"
ADVERSARIAL TRAINING ALGORITHM,0.11201866977829639," ˆGon(τ ′, θ)
"
ADVERSARIAL TRAINING ALGORITHM,0.11318553092182031,"s.t.
ˆGoff(τ, θ) ="
ADVERSARIAL TRAINING ALGORITHM,0.11435239206534423,"T −1
X"
ADVERSARIAL TRAINING ALGORITHM,0.11551925320886815,"t=0
γt
E
y∼πθ(·|y≤t)"
ADVERSARIAL TRAINING ALGORITHM,0.11668611435239207,"
∇log πθ(y|y≤t)fϕ1(y≤t, y)

;"
ADVERSARIAL TRAINING ALGORITHM,0.11785297549591599,"ˆGon(τ ′, θ) ="
ADVERSARIAL TRAINING ALGORITHM,0.11901983663943991,"T −1
X"
ADVERSARIAL TRAINING ALGORITHM,0.12018669778296383,"t=0
γt∇log πθ(y′
t+1|y′
≤t) ˆQfϕ2(y′
≤t, y′
t+1), (10)"
ADVERSARIAL TRAINING ALGORITHM,0.12135355892648775,"where ˆQfϕ2 : Y × V →R denotes the empirical Q-value defined in Eq. (21). Besides, we use
stochastic gradient ascent (SGA) to maximize the objective of L(πθ, fϕ1, fϕ2) w.r.t. parameters of
the on-policy Q-value function ϕ1 and parameters of the off-policy Q-value function ϕ2."
ADVERSARIAL TRAINING ALGORITHM,0.12252042007001167,"Training procedure. The goal is to achieve an equilibrium between minimizing the objective w.r.t.
the parameters of student policy θ ∈Θ and maximizing the objective w.r.t. the parameters of on-policy
and off-policy Q-value functions ϕ1, ϕ2 ∈Φ, formally defined as minθ maxϕ1,ϕ2 L(πθ, fϕ1, fϕ2)"
ADVERSARIAL TRAINING ALGORITHM,0.12368728121353559,"(Eq. (9)). To this end, we use an adversarial training strategy in Algorithm 1, by starting from
a student model fine-tuned on a dataset Dxy. In the training algorithm, we iteratively maximize
the objective w.r.t. the parameters of Q-value functions fϕ1, fϕ2 and simultaneously minimize the
objective w.r.t. the parameters of student policy πθ. In each iteration of policy updating, we first
perform N steps of stochastic gradient ascent (SGA) w.r.t. the parameters of Q-value functions
ϕ1, ϕ2. Then, the parameters of student policy θ are updated by stochastic gradient descent (SGD)
with the estimated policy gradient with sampling policy gradients."
CONVERGENCE ANALYSIS,0.12485414235705951,"3.4
Convergence Analysis"
CONVERGENCE ANALYSIS,0.12602100350058343,"We further provide a convergence analysis for the algorithm proposed in §3.3. To deal with the
challenges of non-convexity by certain reward structures, the algorithm is expected to obtain an
ϵ-accurate stationary point of the policy parameters θ∗∈Θ, satisfying that E[∥∇L(θ∗)∥2] ≤ϵ. We
focus on policy optimization and directly use the optimized off-/on-policy Q-value functions in
each outer-loop iteration k ∈{0, 1, . . . , K −1}. We denote ϕ1(θk) = arg maxϕ1 Loff(πθk, fϕ1),
ϕ2(θk) = arg maxϕ2 Lon(πθk, fϕ2) as the inner-loop optimized functions and use L(θk) :=
L(πθk, fϕ1(θk), fϕ2(θk)) (def. in Eq. (9)) for simplicity in this section. We start with the following
standard assumption [45]."
CONVERGENCE ANALYSIS,0.12718786464410736,"Assumption 1. Suppose that the optimized Q-value functions and the parameterized policy πθ satisfy
the following conditions:"
CONVERGENCE ANALYSIS,0.12835472578763127,"(i) The uniformly boundness of off/on-policy Q-value functions optimized by Algorithm 1, i.e.,
∥fϕ1∥∞, ∥fϕ2∥∞≤1."
CONVERGENCE ANALYSIS,0.1295215869311552,"(ii) The B-Lipschitzness and the L-smoothness of the parameterized policy, i.e., for any state-
action pair (y≤t, yt+1) ∈Y × V at any time step t ∈{0, 1, . . . , T −1},"
CONVERGENCE ANALYSIS,0.1306884480746791,"∥∇log πθ(yt+1|y≤t)∥≤B, for any θ ∈Θ,
(11)"
CONVERGENCE ANALYSIS,0.13185530921820304,"∥∇log πθ1(yt+1|y≤t) −∇log πθ2(yt+1|y≤t)∥≤L∥θ1 −θ2∥, for any θ1, θ2 ∈Θ (12)"
CONVERGENCE ANALYSIS,0.13302217036172695,"Theorem 2 (Convergence rate of Algorithm 1 to stationary points). Let {θk}1≤k≤K be the
sequence of parameters of the policy πθk given by Algorithm 1.
Let the learning rate η ="
BL,0.13418903150525088,"2
BL q"
BL,0.1353558926487748,"1−γT
(1−γ)KLL . Under Assumption 1, we have"
BL,0.13652275379229872,"min
0≤k≤K−1 E

∥∇L(θk)∥2
≤O
 1
√ K"
BL,0.13768961493582263,"
(13)"
BL,0.13885647607934656,Proof. See Appendix A.6 for the complete derivation.
BL,0.14002333722287047,"Theorem 2 illustrates that the output gradient norm square by Algorithm 1 can converge to a
neighborhood around zero with the rate of 1/
√"
BL,0.1411901983663944,"K. Furthermore, leveraging a sufficient number of
training iterations O(ϵ−2), Algorithm 1 can obtain an ϵ-accurate stationary point. This leads to the
following corollary on the computational complexity of the training procedure.
Corollary 1 (Computational complexity of Algorithm 1). We formalize the policy as a softmax
function πθ with a linear transformation: softmax(θy≤t) for any y≤t ∈RH, where θ ∈R|V|×H
and H denotes the hidden size. Then, to obtain an ϵ-accurate stationary point by Algorithm 1, the
complexity of gradient computation is O(ϵ−2T|V|H(N + T + |V|))."
BL,0.1423570595099183,Proof. See Appendix A.7 for the complete derivation.
BL,0.14352392065344224,"Corollary 1 shows that Algorithm 1 has a polynomial computational complexity w.r.t ϵ−2, N, |V|, H
and T, to obtain an ϵ-accurate stationary point for optimizing the training objective in Eq. (9)."
EXPERIMENTS,0.14469078179696615,"4
Experiments"
EXPERIMENTS,0.14585764294049008,"We consider task-agnostic instruction-following experiments and task-specific experiments, including
text summarization, machine translation, and commonsense reasoning. We compare our approach"
EXPERIMENTS,0.147024504084014,"Table 1: Comparison with state-of-the-art KD methods on the instruction-following dataset using
fine-tuned OpenLLaMA-7B as the teacher and fine-tuned OpenLLaMA-3B as the student. We format
the best, the second best and worse than SFT results. The results based on GPT-2 are available in
Appendix C.1."
EXPERIMENTS,0.14819136522753792,"Method
DollyEval
SelfInst
VicunaEval
S-NI
UnNI"
EXPERIMENTS,0.14935822637106183,"GPT-4
R-L
GPT-4
R-L
GPT-4
R-L
R-L
R-L"
EXPERIMENTS,0.15052508751458576,"OpenLLaMA2-7B (teacher)
58.8±1.2
32.5±0.4
56.7±0.8
21.6±0.2
46.2±0.6
22.6±0.5
36.3±0.5
38.5±0.2
SFT (student)
46.8±0.7
26.7±0.6
40.8±1.1
16.3±0.7
34.8±0.8
17.3±0.2
30.4±0.4
28.6±0.3
KD [16]
43.9±0.8
22.4±0.4
43.5±0.5
17.4±0.5
33.7±0.3
16.4±0.2
29.3±0.6
23.4±0.3
SeqKD [20]
50.2±0.6
26.2±0.4
46.8±0.3
15.8±0.5
38.8±1.2
18.0±0.6
29.7±0.3
27.8±0.1
ImitKD [24]
53.7±1.6
25.3±0.3
45.0±0.7
18.4±0.4
41.7±1.2
19.1±0.2
33.1±0.7
28.7±0.5
MiniLLM [14]
58.7±1.2
28.4±0.3
51.8±1.5
20.2±0.6
44.2±1.1
20.7±0.5
37.4±0.4
37.5±0.2
GKD [2]
57.6±1.0
27.5±0.3
52.4±1.2
20.9±0.3
45.5±0.8
19.3±0.5
36.8±0.6
34.8±0.3
DistiLLM [21]
59.2±1.2
29.5±0.2
53.4±1.0
20.8±0.7
46.3±0.9
20.4±0.3
37.2±0.1
38.2±0.1
Ours
59.8±0.8
30.7±0.4
54.2±1.2
21.7±0.5
47.8±0.7
21.4±0.4
38.7±0.4
39.1±0.3"
EXPERIMENTS,0.1516919486581097,"with various KD baselines, including: SFT, which fine-tunes the student model on the supervised
dataset Dxy; KD [16], which uses KL divergence on the supervised dataset Dxy; SeqKD [20],
which applies SFT to the student model with teacher-generated outputs; ImitKD [24], which uses
KL divergence on the student-generated outputs; MiniLLM [14], which uses RKL divergence with a
policy gradient method; GKD [2], which uses JS divergence with an on-policy method; and DistiLLM
[21], which uses an adaptive training method for off-policy optimization of a skew KL divergence.
Additionally, we focus on step-wise distance optimization for KD and compare it with a range of
well-known methods, including KL divergence, RKL divergence, JS divergence, and TV distance, as
discussed by Wen et al. [39]. All the reported results are the average across three random seeds."
TASK-AGNOSTIC DISTILLATION,0.1528588098016336,"4.1
Task-Agnostic Distillation"
TASK-AGNOSTIC DISTILLATION,0.15402567094515754,"Experimental Setup. We follow the previous works [14, 21] for the implementation of the instruction-
following experiment, aiming to evaluate the distilled model’s ability to handle diverse tasks presented
in the form of instructions. We construct the training data from databricks-dolly-15k [8], where
we randomly select 15K samples for training and equally split 500 samples for validation and
testing. We evaluate the trained model on five instruction-following datasets: DollyEval, SelfInst [36],
VicunaEval [6], S-NI [37], and UnNI [17]. Following the previous works [14, 21], we also add the
OpenWebText [13] corpus, consisting of long-document plain text, for joint training with a language
modeling task. This has been shown to effectively improve the performance of instruction tuning
[14]. The evaluation metrics include ROUGE-L [25] and GPT-4 feedback with the same prompts as
in [21]. More details on experimental setup refer to Appendix B."
TASK-AGNOSTIC DISTILLATION,0.15519253208868145,"Main results. Table 1 illustrates the instruction-following performances. Compared with the
SFT baseline, which indicates the student model without KD, KD and SeqKD hardly improve the
performances. This indicates that using only supervised datasets or teacher-generated outputs does
not benefit the KD of large language models. In contrast, utilizing the student-generated outputs
with KL divergence [2], RKL divergence [14], and JS divergence [2] shows effectiveness for KD
in the instruction-following task. State-of-the-art methods [14, 2, 21] tend to combine the student-
generated outputs with the teacher-generated output or supervised dataset to further improve the
results of KD. This shows that a mixture optimization of both on-policy and off-policy objectives
can effectively improve the KD performance of large language models on the instruction-following
task. In particular, we use an adversarial moment-matching method and optimize both on-policy and
off-policy objectives for KD, thus achieving the best results on five test datasets with both GPT-4
feedback and ROUGE-L evaluations."
TASK-SPECIFIC DISTILLATION,0.15635939323220538,"4.2
Task-Specific Distillation"
TASK-SPECIFIC DISTILLATION,0.15752625437572929,"Experimental Setup. We evaluated the KD models on three tasks consisting of text summarization,
machine translation, and reasoning. For the text summarization task, we follow Ko et al. [21] to
conduct experiments on the SAMSum [12] dataset. For the machine translation tasks, we follow Ko et
al. [21] to conduct experiments on the IWSLT’17 (en-de) [5] dataset. For the commonsense reasoning
task, we conduct experiments on the StrategyQA dataset [11] with chain-of-thought augmentations"
TASK-SPECIFIC DISTILLATION,0.15869311551925322,"Table 2: Comparison with the state-of-the-art KD methods on text summarization, machine translation
and commonsense reasoning datasets. We report the ROUGE-L, BLEU and accuracy for SAMSum,
IWSLT’17 (en-de) and StrategyQA, respectively. We format the best, the second best and worse
than SFT results."
TASK-SPECIFIC DISTILLATION,0.15985997666277713,"Method
SAMSum
IWSLT’17 (en-de)
StrategyQA"
TASK-SPECIFIC DISTILLATION,0.16102683780630106,"T5-Small
T5-Base
T5-Large
T5-Small
T5-Base
T5-Large
T5-Small
T5-Base
T5-Large"
TASK-SPECIFIC DISTILLATION,0.16219369894982497,"T5-XL (teacher)
52.5±0.4
35.2±0.2
64.5±0.8
SFT (student)
40.6±0.2
47.3±0.3
49.8±0.2
21.5±0.1
30.1±0.0
33.7±0.1
52.4±0.5
57.5±0.8
60.7±0.8
KD [16]
39.2±0.4
46.5±0.3
47.4±0.3
21.7±0.1
29.8±0.2
31.7±0.1
49.7±0.3
55.3±0.1
59.2±0.5
SeqKD [20]
39.7±0.3
47.7±0.5
49.3±0.4
21.2±0.3
29.2±0.2
32.9±0.5
50.6±0.7
57.5±1.1
61.5±0.8
ImitKD [24]
41.8±0.3
48.6±0.7
51.2±0.5
22.2±0.3
28.7±0.6
34.1±0.2
53.8±0.8
59.7±0.5
61.7±0.6
GKD [2]
42.1±0.3
48.2±0.5
51.7±0.4
22.7±0.2
31.2±0.1
34.7±0.2
55.6±0.4
60.3±0.5
63.6±0.3
DistiLLM [21]
42.6±0.2
49.4±0.6
52.1±0.4
22.5±0.1
30.8±0.2
35.5±0.1
56.3±0.3
61.2±0.7
62.8±0.2
Ours
43.7±0.4
50.4±0.3
52.7±0.3
23.7±0.1
32.4±0.3
36.0±0.2
58.2±0.4
62.9±0.3
65.3±0.7"
TASK-SPECIFIC DISTILLATION,0.1633605600933489,"[38]. For all of the task-specific experiments, we use T5-XL [29] as the teacher model and T5-Large/-
Base/-Small as the student model. For the machine translation experiments, we employ a multilingual
pretrained model, mT5 [43], to build the methods. For evaluation, we use ROUGE-L [25], BLEU
[27], and accuracy as the performance metrics on SAMSum, IWSLT’17 (en-de), and StrategyQA,
respectively. More details about the experimental setup refer to Appendix B."
TASK-SPECIFIC DISTILLATION,0.1645274212368728,"Main results. Table 2 displays the performances on three task-specific datasets. Since the original
work of MiniLLM [14] does not consider these tasks, we thus do not make comparisons with
MiniLLM. The performance trend is similar to the instruct-following results, revealing that KD of
large language models for specific tasks also benefits from the combination of on-policy objectives
with student-generated outputs and off-policy objectives with teacher-generated outputs or supervised
datasets. Additionally, we observe that student models of different sizes all benefit from the KD
methods to improve performance. Overall, our approach achieves the best results on all three
task-specific datasets for student models of different sizes. This demonstrates the effectiveness
of an adversarial moment-matching approach for KD of large language models on specific tasks."
TASK-SPECIFIC DISTILLATION,0.16569428238039674,"KL
RKL
JS
TV
Ours
20 22 24 26 28 30 32"
TASK-SPECIFIC DISTILLATION,0.16686114352392065,ROUGE-L
TASK-SPECIFIC DISTILLATION,0.16802800466744458,"on-policy
off-policy
mixed"
TASK-SPECIFIC DISTILLATION,0.1691948658109685,(a) DollyEval.
TASK-SPECIFIC DISTILLATION,0.17036172695449242,"KL
RKL
JS
TV
Ours
43 44 45 46 47 48 49 50 51 52"
TASK-SPECIFIC DISTILLATION,0.17152858809801633,ROUGE-L
TASK-SPECIFIC DISTILLATION,0.17269544924154026,"on-policy
off-policy
mixed"
TASK-SPECIFIC DISTILLATION,0.17386231038506417,(b) SAMSum.
TASK-SPECIFIC DISTILLATION,0.1750291715285881,"KL
RKL
JS
TV
Ours
26 27 28 29 30 31 32 33 34 BLEU"
TASK-SPECIFIC DISTILLATION,0.176196032672112,"on-policy
off-policy
mixed"
TASK-SPECIFIC DISTILLATION,0.17736289381563594,(c) IWSLT’17 (en-de).
TASK-SPECIFIC DISTILLATION,0.17852975495915985,"KL
RKL
JS
TV
Ours
54 56 58 60 62 64"
TASK-SPECIFIC DISTILLATION,0.17969661610268378,Accuracy
TASK-SPECIFIC DISTILLATION,0.1808634772462077,"on-policy
off-policy
mixed"
TASK-SPECIFIC DISTILLATION,0.18203033838973162,(d) StrategyQA.
TASK-SPECIFIC DISTILLATION,0.18319719953325556,"Figure 2: Performance of difference step-wise distribution dis-
tances."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.18436406067677946,"4.3
Analysis on Step-Wise
Distance Optimization"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.1855309218203034,"Comparison with distribution
matching.
We make compar-
isons with different step-wise dis-
tribution distances with a uni-
form formulation of Definition
2, considering the on-policy, off-
policy objectives as well as the
joint form. Results on four tasks
with a default combination fac-
tor β = 0.5 are shown in Fig-
ure 2. More instruct-following
results are available in Appendix
C.2 and results with different val-
ues of off-/on-policy combina-
tion factor are available in Ap-
pendix C.5. Compared with the
KL divergence, RKL divergence,
JS divergence and total variation
distance, the proposed moment-matching distance achieves the best results under both the on-policy
and off-policy training objectives, which shows that the proposed moment-matching approach is
effective for KD of large language models. Besides, we observe that using a joint objective of both
on-policy and off-policy can further significantly improve the performances. This shows that both
on-policy and off-policy moment-matching objectives contribute to the minimization of the imitation
gap and can thus benefit the KD of large language models."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.1866977829638273,"0
2000
4000
6000
8000
10000
Step 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.18786464410735124,Loss (train)
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.18903150525087514,training loss
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19019836639439908,on-policy distance don
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19136522753792298,off-policy distance doff 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19253208868144692,Distance (eval)
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19369894982497082,"(a) Training loss and don
MM, doff
MM
against training step."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19486581096849476,DollyEval
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19603267211201866,SelfInst
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.1971995332555426,VicunaEval S-NI UnNI
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.1983663943990665,Average KL RKL JS TV Ours
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.19953325554259044,"2.65
4.87
3.21
3.67
3.02
3.48"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20070011668611434,"2.54
4.78
3.87
3.58
2.67
3.48"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20186697782963828,"2.31
4.52
3.67
3.32
2.43
3.25"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20303383897316218,"1.98
2.34
3.32
2.13
1.83
2.32"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20420070011668612,"0.96
1.85
2.11
0.85
0.92
1.34"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20536756126021002,don on five test sets 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20653442240373396,"(b) On-policy moment-matching
distance don
MM on the test sets."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.20770128354725786,DollyEval
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.2088681446907818,SelfInst
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.2100350058343057,VicunaEval S-NI UnNI
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21120186697782964,Average KL RKL JS TV Ours
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21236872812135357,"2.05
2.79
2.21
2.07
2.02
2.23"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21353558926487748,"1.84
3.28
2.37
2.18
1.67
2.27"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.2147024504084014,"1.78
2.62
2.15
1.68
1.43
1.93"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21586931155192532,"1.53
1.94
2.42
1.13
1.23
1.65"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21703617269544925,"0.75
1.45
1.11
0.72
0.62
0.93"
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.21820303383897316,doff on five test sets 1.0 1.5 2.0 2.5 3.0
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.2193698949824971,"(c) Off-policy moment-matching
distance don
MM on the test sets."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.220536756126021,"Figure 3: Adversarial training procedure for optimizing the on-policy and off-policy moment-
matching distances don
MM, doff
MM on the instruction-following dataset."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.22170361726954493,"Adversarial training procedure. We present the training loss and moment-matching distance against
the adversarial training steps. As depicted in Figure 3 (a), the training loss initially increases within
the first 0-1,000 steps, indicating that initially, the Q-value functions are stronger than the policy in
maximizing the loss function L(πθ, fϕ1, fϕ2) in Eq. (9). Concurrently, the policy gradient method
contributes to minimizing the training loss, which eventually converges to a much lower stable value.
Additionally, both the on-policy and off-policy moment-matching distances don
MM and doff
MM decrease
and eventually reach a low value with only minor fluctuations. For more results and details on
experimental setups, please refer to Appendix C.3."
"ANALYSIS ON STEP-WISE
DISTANCE OPTIMIZATION",0.22287047841306884,"Moment-matching distance optimization. We further illustrate the on-policy moment-matching
distance don
MM and the off-policy moment-matching distance doff
MM (defined in Definition 3) optimized
by different step-wise distances in Figure 3 (b) and (c), respectively. Interestingly, we observe that
the total variation (TV) distance obtains the second-best results on average for both on-policy and
off-policy distances. This finding suggests a similarity between the formulations of TV distance
and moment-matching distances to some extent, as supported by the theoretical result of Theorem
1. Across all instruction-following test sets, our approach effectively optimizes both on-policy and
off-policy moment-matching distances more than other step-wise distribution distances used in KD,
including KL divergence, RKL divergence, JS divergence, and TV distance. This observation also
underscores the effectiveness of our policy gradient methods. Extensive results on the task-specific
datasets are available in Appendix C.4."
CONCLUSION,0.22403733955659277,"5
Conclusion"
CONCLUSION,0.22520420070011668,"In this work, we investigated a moment-matching approach for knowledge distillation of large
language models. Specifically, we formulated knowledge distillation from a perspective of imitation
learning and derived both on-policy and off-policy bounds for the imitation gap between the teacher
model and student model via moment-matching distance. Additionally, we proposed an adversarial
training algorithm to simultaneously estimate and minimize the joint objective of on-policy and
off-policy moment-matching distances. In experiments, we evaluated the proposed algorithm on
four instruction-following datasets and three task-specific datasets, comparing it with a range of
state-of-the-art KD methods as well as four well-studied step-wise distribution distances for KD of
auto-regressive models. Results demonstrate that our approach can effectively leverage the policy
gradient method to optimize the moment-matching distance and achieve the best results across all
datasets."
CONCLUSION,0.22637106184364061,"Limitations and future work. The proposed adversarial training algorithm requires additional
computational steps for the inner-loop gradient ascent, which may result in increased time complexity.
Moreover, the proposed approach necessitates auxiliary networks to build the Q-value functions,
which may incur additional memory costs. Besides, the experiments are conducted with limited LLM
architectures, such as OpenLLaMA and T5. Therefore, in future work, we aim to enhance the time
and memory efficiency of our approach, and evaluate the proposed approach on a wider range of
architectures."
CONCLUSION,0.22753792298716452,Acknowledgements
CONCLUSION,0.22870478413068845,"We thank the anonymous reviewers for their helpful comments and suggestions. This work was
supported by SI-TECH Information Technology Co., Ltd."
REFERENCES,0.22987164527421236,References
REFERENCES,0.2310385064177363,"[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774, 2023."
REFERENCES,0.2322053675612602,"[2] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea,
Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from
self-generated mistakes. In The Twelfth International Conference on Learning Representations,
2024."
REFERENCES,0.23337222870478413,"[3] James Bagnell, Sham M Kakade, Jeff Schneider, and Andrew Ng. Policy search by dynamic
programming. Advances in Neural Information Processing Systems, 16, 2003."
REFERENCES,0.23453908984830804,"[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:
Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022."
REFERENCES,0.23570595099183198,"[5] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuitho
Sudoh, Koichiro Yoshino, and Christian Federmann. Overview of the iwslt 2017 evaluation
campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation,
pages 2–14, 2017."
REFERENCES,0.23687281213535588,"[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023."
REFERENCES,0.23803967327887982,"[7] Kamil Ciosek. Imitation learning by reinforcement learning. In International Conference on
Learning Representations, 2021."
REFERENCES,0.23920653442240372,"[8] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023."
REFERENCES,0.24037339556592766,"[9] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan
Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback.
arXiv preprint arXiv:2310.01377, 2023."
REFERENCES,0.24154025670945156,"[10] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama. URL: https://github.
com/openlm-research/open_llama, 2023."
REFERENCES,0.2427071178529755,"[11] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did
aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.
Transactions of the Association for Computational Linguistics, 9:346–361, 2021."
REFERENCES,0.24387397899649943,"[12] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A
human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd
Workshop on New Frontiers in Summarization, pages 70–79, 2019."
REFERENCES,0.24504084014002334,"[13] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex.
Openwebtext corpus.
http://Skylion007.github.io/OpenWebTextCorpus, 2019."
REFERENCES,0.24620770128354727,"[14] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large
language models. In The Twelfth International Conference on Learning Representations, 2024."
REFERENCES,0.24737456242707118,"[15] Yongchang Hao, Yuxin Liu, and Lili Mou. Teacher forcing recovers reward functions for text
generation. Advances in Neural Information Processing Systems, 35:12594–12607, 2022."
REFERENCES,0.2485414235705951,"[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015."
REFERENCES,0.24970828471411902,"[17] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning
language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409–14428,
2023."
REFERENCES,0.2508751458576429,"[18] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–
274, 2002."
REFERENCES,0.25204200700116686,"[19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.2532088681446908,"[20] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2016."
REFERENCES,0.2543757292882147,"[21] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined
distillation for large language models. In Forty-first International Conference on Machine
Learning, 2024."
REFERENCES,0.2555425904317386,"[22] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.25670945157526254,"[23] Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less
is more: Task-aware layer-wise distillation for language model compression. In International
Conference on Machine Learning, pages 20852–20867. PMLR, 2023."
REFERENCES,0.25787631271878647,"[24] Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei. Autoregressive knowledge
distillation through imitation learning. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, pages 6121–6133, 2020."
REFERENCES,0.2590431738623104,"[25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out, pages 74–81, 2004."
REFERENCES,0.2602100350058343,"[26] Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations. In 9th
International Conference on Learning Representations, 2021."
REFERENCES,0.2613768961493582,"[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311–318, 2002."
REFERENCES,0.26254375729288215,"[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI Blog, 1(8):1–24, 2019."
REFERENCES,0.2637106184364061,"[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020."
REFERENCES,0.26487747957992996,"[30] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."
REFERENCES,0.2660443407234539,"[31] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv
preprint arXiv:0901.2698, 2009."
REFERENCES,0.26721120186697783,"[32] Gokul Swamy, Sanjiban Choudhury, J Bagnell, and Steven Z Wu. Sequence model imitation
learning with unobserved contexts. Advances in Neural Information Processing Systems,
35:17665–17676, 2022."
REFERENCES,0.26837806301050177,"[33] Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and match-
ing: A game-theoretic framework for closing the imitation gap. In International Conference on
Machine Learning, pages 10022–10032. PMLR, 2021."
REFERENCES,0.26954492415402564,"[34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca, 2023."
REFERENCES,0.2707117852975496,"[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023."
REFERENCES,0.2718786464410735,"[36] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated in-
structions. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 13484–13508, 2023."
REFERENCES,0.27304550758459745,"[37] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
pages 5085–5109, 2022."
REFERENCES,0.2742123687281214,"[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems, 35:24824–24837, 2022."
REFERENCES,0.27537922987164526,"[39] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level
knowledge distillation. In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 10817–10834, 2023."
REFERENCES,0.2765460910151692,"[40] Qingyang Wu, Lei Li, and Zhou Yu. Textgail: Generative adversarial imitation learning for text
generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages
14067–14075, 2021."
REFERENCES,0.2777129521586931,"[41] Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. Rethinking kullback-
leibler divergence in knowledge distillation for large language models.
arXiv preprint
arXiv:2404.02657, 2024."
REFERENCES,0.27887981330221706,"[42] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng
Tao, and Tianyi Zhou. A survey on knowledge distillation of large language models. arXiv
preprint arXiv:2402.13116, 2024."
REFERENCES,0.28004667444574094,"[43] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,
Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text trans-
former. In Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 483–498, 2021."
REFERENCES,0.2812135355892649,"[44] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial
nets with policy gradient. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 31, 2017."
REFERENCES,0.2823803967327888,"[45] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient
methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization,
58(6):3586–3612, 2020."
REFERENCES,0.28354725787631274,"A
Proofs"
REFERENCES,0.2847141190198366,"A.1
Proof of Proposition 1"
REFERENCES,0.28588098016336055,"Proof. Similar to the proof of Performance Difference Lemma (PDL) [18, 3, 33], we have
J(π∗) −J(πθ)"
REFERENCES,0.2870478413068845,"=
E
x∼px
τ∼π∗|x"
REFERENCES,0.2882147024504084,"""T −1
X"
REFERENCES,0.2893815635939323,"t=0
γtr(y≤t, yt+1) #"
REFERENCES,0.29054842473745623,"−
E
x∼px [V πθ(x)]"
REFERENCES,0.29171528588098017,"=
E
x∼px
τ∼π∗|x"
REFERENCES,0.2928821470245041,"""T −1
X"
REFERENCES,0.294049008168028,"t=0
γt  
r(y≤t, yt+1) + V πθ(y≤t) −V πθ(y≤t)

#"
REFERENCES,0.2952158693115519,"−
E
x∼px [V πθ(x)]"
REFERENCES,0.29638273045507585,"=
E
x∼px
τ∼π∗|x"
REFERENCES,0.2975495915985998,"""T −1
X"
REFERENCES,0.29871645274212366,"t=0
γt  
r(y≤t, yt+1) + γV πθ(y≤t+1) −V πθ(y≤t)

#"
REFERENCES,0.2998833138856476,"=
E
x∼px
τ∼π∗|x"
REFERENCES,0.30105017502917153,"""T −1
X"
REFERENCES,0.30221703617269546,"t=0
γt 
r(y≤t, yt+1) + γEy≤t+1∼T (·|y≤t,yt+1)

V πθ(y≤t+1)

−V πθ(y≤t)
#"
REFERENCES,0.3033838973162194,"(i)
=
E
x∼px
τ∼π∗|x"
REFERENCES,0.3045507584597433,"""T −1
X"
REFERENCES,0.3057176196032672,"t=0
γt  
Qπθ(y≤t, yt+1) −V πθ(y≤t)

#"
REFERENCES,0.30688448074679114,"=
E
x∼px
τ∼π∗|x"
REFERENCES,0.3080513418903151,"""T −1
X"
REFERENCES,0.30921820303383896,"t=0
γt"
REFERENCES,0.3103850641773629,"Qπθ(y≤t, yt+1) −
E
y∼πθ(·|y≤t)"
REFERENCES,0.3115519253208868,"
Qπθ(y≤t, y)

!#"
REFERENCES,0.31271878646441076,"≤sup
f∈FQ
E
x∼px
τ∼π∗|x"
REFERENCES,0.31388564760793464,"""T −1
X"
REFERENCES,0.31505250875145857,"t=0
γt"
REFERENCES,0.3162193698949825,"f(y≤t, yt+1) −
E
y∼πθ(·|y≤t)"
REFERENCES,0.31738623103850644,"
f(y≤t, y)

!# ,"
REFERENCES,0.3185530921820303,"where (i) follows from Bellman equation and noting that the transition probability T(·|y≤t, yt+1) is
deterministic in an auto-regressive text generation problem. This completes the proof."
REFERENCES,0.31971995332555425,"A.2
Proof of Proposition 2"
REFERENCES,0.3208868144690782,"Proof. Similar to the proof of Proposition 1, we have
J(π∗) −J(πθ)"
REFERENCES,0.3220536756126021,"= −
E
x∼px
τ∼πθ|x"
REFERENCES,0.323220536756126,"""T −1
X"
REFERENCES,0.32438739789964993,"t=0
γtr(y≤t, yt+1) #"
REFERENCES,0.32555425904317387,"+
E
x∼px [V π∗(x)]"
REFERENCES,0.3267211201866978,"=
E
x∼px
τ∼πθ|x"
REFERENCES,0.3278879813302217,"""T −1
X"
REFERENCES,0.3290548424737456,"t=0
γt  
V π∗(y≤t) −
 
r(y≤t, yt+1) + V π∗(y≤t)

#"
REFERENCES,0.33022170361726955,"+
E
x∼px [V π∗(x)]"
REFERENCES,0.3313885647607935,"=
E
x∼px
τ∼πθ|x"
REFERENCES,0.33255542590431736,"""T −1
X"
REFERENCES,0.3337222870478413,"t=0
γt  
V π∗(y≤t) −
 
r(y≤t, yt+1) + γV π∗(y≤t+1)

#"
REFERENCES,0.3348891481913652,"=
E
x∼px
τ∼πθ|x"
REFERENCES,0.33605600933488916,"""T −1
X"
REFERENCES,0.3372228704784131,"t=0
γt 
V π∗(y≤t) −

r(y≤t, yt+1) + γEy≤t+1∼T (·|y≤t,yt+1)

V π∗(y≤t+1)
#"
REFERENCES,0.338389731621937,"=
E
x∼px
τ∼πθ|x"
REFERENCES,0.3395565927654609,"""T −1
X"
REFERENCES,0.34072345390898484,"t=0
γt  
V π∗(y≤t) −Qπ∗(y≤t, yt+1)

#"
REFERENCES,0.3418903150525088,"=
E
x∼px
τ∼πθ|x"
REFERENCES,0.34305717619603265,"""T −1
X"
REFERENCES,0.3442240373395566,"t=0
γt"
REFERENCES,0.3453908984830805,"E
y∼π∗(·|y≤t)"
REFERENCES,0.34655775962660446,"
Qπ∗(y≤t, y)

−Qπ∗(y≤t+1, yt+1) !#"
REFERENCES,0.34772462077012833,"≤sup
f∈FQ∗
E
x∼px
τ∼πθ|x"
REFERENCES,0.34889148191365227,"""T −1
X"
REFERENCES,0.3500583430571762,"t=0
γt"
REFERENCES,0.35122520420070014,"E
y∼π∗(·|y≤t)"
REFERENCES,0.352392065344224,"
f(y≤t, y)

−f(y≤t, yt+1) !# ,"
REFERENCES,0.35355892648774795,which completes the proof of Proposition 2.
REFERENCES,0.3547257876312719,"A.3
Existing Step-Wise Distribution Distance for Distillation"
REFERENCES,0.3558926487747958,"Definition 4 (Step-wise distribution distances for distillation [39]). Following Wen et al. [39], we
define four groups of well-studied probability distribution distances as follows,"
REFERENCES,0.3570595099183197,"• Total variation (TV) distance. The token-level TV distance between the probabilities of
teacher policy π∗and student policy πθ given the current state y≤t can be defined by the
ℓ2-norm as follows,"
REFERENCES,0.35822637106184363,"TV(πθ(·|y≤t), π∗(·|y≤t)) :=1 2 X y∈V"
REFERENCES,0.35939323220536756,"π∗(y|y≤t) −πθ(y|y≤t)

(14)"
REFERENCES,0.3605600933488915,"• Kullback–Leibler (KL) divergence. The token-level KL divergence between the probabili-
ties of teacher policy π∗and student policy πθ given the current state y≤t can be defined as
follows,"
REFERENCES,0.3617269544924154,"KL(πθ(·|y≤t), π∗(·|y≤t)) :=
X"
REFERENCES,0.3628938156359393,"y∈V
π∗(y|y≤t) log π∗(y|y≤t)"
REFERENCES,0.36406067677946324,"πθ(y|y≤t)
(15)"
REFERENCES,0.3652275379229872,"• Reverse Kullback–Leibler (RKL) divergence.
The token-level RKL divergence be-
tween the probabilities of teacher policy π∗and student policy πθ given the current state
boldsymboly≤t can be defined as follows,"
REFERENCES,0.3663943990665111,"RKL(πθ(·|y≤t), π∗(·|y≤t)) :=
X"
REFERENCES,0.367561260210035,"y∈V
πθ(y|y≤t) log πθ(y|y≤t)"
REFERENCES,0.3687281213535589,"π∗(y|y≤t)
(16)"
REFERENCES,0.36989498249708286,"• Jenson–Shannon (JS) divergence. The token-level JS divergence between the probabilities
of teacher policy π∗and student policy πθ given the current state boldsymboly≤t can be
defined based on the KL divergence and RKL divergence as follows,"
REFERENCES,0.3710618436406068,"JS(πθ(·|y≤t), π∗(·|y≤t)) :=1"
REFERENCES,0.37222870478413067,"2KL(π∗, πθ + π∗"
REFERENCES,0.3733955659276546,"2
) + 1"
REFERENCES,0.37456242707117854,"2RKL(πθ, πθ + π∗"
REFERENCES,0.3757292882147025,"2
)
(17)"
REFERENCES,0.37689614935822635,"A.4
Proof of Theorem 1"
REFERENCES,0.3780630105017503,"Proof. We first derive an upper bound for the on-policy moment-matching objective of Eq. (3). Set
FQ∗= {f : ∥f∥∞≤1}, and by the definition of L(πθ, f) in Eq. (3), we have"
REFERENCES,0.3792298716452742,"sup
f:∥f∥∞≤1
Lon(πθ, f)"
REFERENCES,0.38039673278879815,"=
sup
f:∥f∥∞≤1
E
x∼px
τ∼πθ|x"
REFERENCES,0.38156359393232203,"""T −1
X"
REFERENCES,0.38273045507584597,"t=0
γt"
REFERENCES,0.3838973162193699,"E
y∼π∗(·|y≤t)"
REFERENCES,0.38506417736289383,"
f(y≤t, y)

−f(y≤t, y) !#"
REFERENCES,0.3862310385064177,"=
sup
f:∥f∥∞≤1
E
x∼px
τ∼πθ|x"
REFERENCES,0.38739789964994165,"""T −1
X"
REFERENCES,0.3885647607934656,"t=0
γt"
REFERENCES,0.3897316219369895,"E
y∼π∗(·|y≤t)"
REFERENCES,0.3908984830805134,"
f(y≤t, y)

−
E
y∼πθ(·|y≤t)"
REFERENCES,0.3920653442240373,"
f(y≤t, y)

!#"
REFERENCES,0.39323220536756126,"Then, we have"
REFERENCES,0.3943990665110852,"sup
f:∥f∥∞≤1
Lon(πθ, f)"
REFERENCES,0.39556592765460913,"(i)
≤
E
x∼px
τ∼πθ|x"
REFERENCES,0.396732788798133,"""T −1
X"
REFERENCES,0.39789964994165694,"t=0
γt
sup
f:∥f∥∞≤1 "
REFERENCES,0.3990665110851809,"E
y∼π∗(·|y≤t)"
REFERENCES,0.4002333722287048,"
f(y≤t, y)

−
E
y∼πθ(·|y≤t))"
REFERENCES,0.4014002333722287,"
f(y≤t, y)

!#"
REFERENCES,0.4025670945157526,"(ii)
=
E
x∼px
τ∼πθ|x "
REFERENCES,0.40373395565927656,"
T −1
X"
REFERENCES,0.4049008168028005,"t=0
γt X y∈V"
REFERENCES,0.40606767794632437,π∗(y|y≤t) −πθ(y|y≤t) 
REFERENCES,0.4072345390898483,"(iii)
= 2don
TV(πθ, π∗) (Def. 2 & 4),"
REFERENCES,0.40840140023337224,"where (i) follows from Jensen’s inequality, (ii) follows from [31] and (iii) follows from the definition
of TV distance."
REFERENCES,0.40956826137689617,"Similarly, we can bound the off-policy version of Eq. (2) as follows,"
REFERENCES,0.41073512252042005,"sup
f:∥f∥∞≤1
E
x∼px
τ∼π∗|x"
REFERENCES,0.411901983663944,"
Loff(πθ, f)

≤
E
x∼px
τ∼π∗|x "
REFERENCES,0.4130688448074679,"
T −1
X"
REFERENCES,0.41423570595099185,"t=0
γt X y∈V"
REFERENCES,0.41540256709451573,π∗(y|y≤t) −πθ(y|y≤t)  
REFERENCES,0.41656942823803966,"= 2doff
TV(πθ, π∗) (Def. 4),"
REFERENCES,0.4177362893815636,which completes the proof of Theorem 1.
REFERENCES,0.41890315052508753,"A.5
Derivation of Policy Gradient in Eq. (10)"
REFERENCES,0.4200700116686114,"Based on the definition of training objective in Eq. (9), we have"
REFERENCES,0.42123687281213534,"∇L(πθ, fϕ1, fϕ2) = β∇Loff(πθ, fϕ1) + (1 −β)∇Lon(πθ, fϕ2)
(18)"
REFERENCES,0.4224037339556593,"Based on the definition of Loff(πθ, fϕ1) in Eq. (2), we have"
REFERENCES,0.4235705950991832,"∇Loff(πθ, fϕ1)"
REFERENCES,0.42473745624270715,"=∇
E
x∼px
τ∼π∗|x"
REFERENCES,0.425904317386231,"""T −1
X"
REFERENCES,0.42707117852975496,"t=0
γt"
REFERENCES,0.4282380396732789,"f(y≤t, yt+1) −
E
y∼πθ(·|y≤t)"
REFERENCES,0.4294049008168028,"
f(y≤t, y)

!#"
REFERENCES,0.4305717619603267,"= −
E
x∼px
τ∼π∗|x"
REFERENCES,0.43173862310385064,"T −1
X"
REFERENCES,0.4329054842473746,"t=0
γt∇
E
y∼πθ(·|y≤t)"
REFERENCES,0.4340723453908985,"
fϕ1(y≤t, y)
"
REFERENCES,0.4352392065344224,"= −
E
x∼px
τ∼π∗|x"
REFERENCES,0.4364060676779463,"T −1
X"
REFERENCES,0.43757292882147025,"t=0
γt X"
REFERENCES,0.4387397899649942,"y∈V
πθ(y|y≤t)∇log πθ(y|y≤t)fϕ1(y≤t, y)"
REFERENCES,0.43990665110851807,"= −
E
x∼px
τ∼π∗|x"
REFERENCES,0.441073512252042,"T −1
X"
REFERENCES,0.44224037339556593,"t=0
γt
E
y∼πθ(·|y≤t)"
REFERENCES,0.44340723453908987,"
∇log πθ(y|y≤t)fϕ1(y≤t, y)
 (19)"
REFERENCES,0.44457409568261375,"Then, based on the definition of Lon(πθ, fϕ2) in Eq. (3), we have"
REFERENCES,0.4457409568261377,"∇Lon(πθ, fϕ2)"
REFERENCES,0.4469078179696616,"=∇
E
x∼px
τ∼πθ|x"
REFERENCES,0.44807467911318555,"""T −1
X"
REFERENCES,0.4492415402567094,"t=0
γt"
REFERENCES,0.45040840140023336,"E
y∼π∗(·|y≤t)"
REFERENCES,0.4515752625437573,"
fϕ2(y≤t, y)

−fϕ2(y≤t, yt+1) !#"
REFERENCES,0.45274212368728123,"(i)
=
E
x∼px
τ∼πθ|x"
REFERENCES,0.4539089848308051,"""T −1
X"
REFERENCES,0.45507584597432904,"t=0
γt∇log πθ(yt|y≤t)"
REFERENCES,0.456242707117853,"T −1
X"
REFERENCES,0.4574095682613769,"t′=t
γt′−t"
REFERENCES,0.45857642940490084,"E
y∼π∗(·|y≤t′)"
REFERENCES,0.4597432905484247,"
fϕ2(y≤t′, y)

−fϕ2(y≤t′, yt′+1) !# , (20)"
REFERENCES,0.46091015169194866,"where (i) follows from a standard derivation of gradient policy (c.f. [22]). For simplicity, set"
REFERENCES,0.4620770128354726,"ˆQfϕ2(y≤t, yt+1) ="
REFERENCES,0.4632438739789965,"T −1
X"
REFERENCES,0.4644107351225204,"t′=t
γt′−t"
REFERENCES,0.46557759626604434,"E
y∼π∗(·|y≤t′)"
REFERENCES,0.46674445740956827,"
fϕ2(y≤t′, y)

−fϕ2(y≤t′, yt′+1) ! (21)"
REFERENCES,0.4679113185530922,"as the empirical Q-value given any draw of trajectory τ ∼πθ|y0 = x, x ∼px in Eq. (20)."
REFERENCES,0.4690781796966161,"Coming back to Eq. (18) and combining with Eq. (19) and Eq. (20), we have"
REFERENCES,0.47024504084014,"∇L(πθ, fϕ1, fϕ2) = −β
E
x∼px
τ∼π∗|x"
REFERENCES,0.47141190198366395,"""T −1
X"
REFERENCES,0.4725787631271879,"t=0
γt
E
y∼πθ(·|y≤t)"
REFERENCES,0.47374562427071176,"
∇log πθ(y|y≤t)fϕ1(y≤t, y)

#"
REFERENCES,0.4749124854142357,"+ (1 −β)
E
x∼px
τ∼πθ|x"
REFERENCES,0.47607934655775963,"""T −1
X"
REFERENCES,0.47724620770128356,"t=0
γt∇log πθ(yt+1|y≤t) ˆQfϕ2(y≤t, yt+1) # #"
REFERENCES,0.47841306884480744,"Then, using the law of iterated expectations, we obtain the final formulation of policy gradient,"
REFERENCES,0.4795799299883314,"∇L(πθ, fϕ1, fϕ2) =
E
x∼px"
REFERENCES,0.4807467911318553,"
−β
E
τ∼π∗|x"
REFERENCES,0.48191365227537925," T −1
X"
REFERENCES,0.4830805134189031,"t=0
γt
E
y∼πθ(·|y≤t)"
REFERENCES,0.48424737456242706,"
∇log πθ(y|y≤t)fϕ1(y≤t, y)
 "
REFERENCES,0.485414235705951,"+ (1 −β)
E
τ ′∼πθ|x"
REFERENCES,0.4865810968494749," T −1
X"
REFERENCES,0.48774795799299886,"t=0
γt∇log πθ(y′
t+1|y′
≤t) ˆQfϕ2(y′
≤t, y′
t+1)

,"
REFERENCES,0.48891481913652274,which completes the derivation of policy gradient in Eq. (10).
REFERENCES,0.49008168028004667,"A.6
Proof of Theorem 2"
REFERENCES,0.4912485414235706,"Lemma 1. Let ˆ∇L(θ) = −β ˆGoff(τ, θ) + (1 −β) ˆGon(τ ′, θ) denote the empirical policy gradient
given any trajectories x ∼px, τ ∼π∗|x, τ ′ ∼πθ|x, where L(θ) := L(πθ, fϕ1, fϕ2) (def. in Eq.
(9)) denote the objective w.r.t. the policy parameters θ given any off-/on-policy Q-value functions fϕ1
and fϕ2. Then, under Assumption 1, we have ∥ˆ∇L(θ)∥≤BL with"
REFERENCES,0.49241540256709454,BL = β(1 −γT )B
REFERENCES,0.4935822637106184,"1 −γ
+ 2(1 −β)(1 −γT )2B"
REFERENCES,0.49474912485414235,(1 −γ)2
REFERENCES,0.4959159859976663,"Proof. By triangle inequality, we have for any x ∼px, τ ∼π∗|x, τ ′ ∼πθ|x,"
REFERENCES,0.4970828471411902,"∥ˆ∇L(θ)∥≤β
 ˆGoff(τ, θ)
 + (1 −β)
 ˆGon(τ ′, θ)

(22)"
REFERENCES,0.4982497082847141,"By the formulation of off-policy gradient ∥ˆGoff(τ, θ)∥in Eq. (10) under the condition of optimized
off-policy Q-value functions fϕ1 by Algorithm 1, we have"
REFERENCES,0.49941656942823803,"ˆGoff(τ, θ)
 = "
REFERENCES,0.5005834305717619,"T −1
X"
REFERENCES,0.5017502917152858,"t=0
γt
E
y∼πθ(·|y≤t)"
REFERENCES,0.5029171528588098,"
∇log πθ(y|y≤t)fϕ1(y≤t, y)
"
REFERENCES,0.5040840140023337,"By Jensen’s inequality, we have
 ˆGoff(τ, θ)
 ≤"
REFERENCES,0.5052508751458576,"T −1
X"
REFERENCES,0.5064177362893816,"t=0
γt
E
y∼πθ(·|y≤t)"
REFERENCES,0.5075845974329055,"∇log πθ(y|y≤t)
 fϕ1(y≤t, y)
"
REFERENCES,0.5087514585764294,"By Assumption 1, we have
 ˆGoff(τ, θ)
 ≤B"
REFERENCES,0.5099183197199533,"T −1
X"
REFERENCES,0.5110851808634772,"t=0
γt = B(1 −γT )"
REFERENCES,0.5122520420070011,"1 −γ
(23)"
REFERENCES,0.5134189031505251,"Similarly, we can bound the on-policy gradient ∥ˆGon(τ ′, θ)∥by Jensen’s inequality as follows,
 ˆGon(τ ′, θ)
 ≤"
REFERENCES,0.514585764294049,"T −1
X"
REFERENCES,0.5157526254375729,"t=0
γt ∇log πθ(yt+1|y≤t)

 ˆQfϕ2(y≤t, yt+1)"
REFERENCES,0.5169194865810969,"Based on the definition of ˆQfϕ2(y≤t, yt+1) in Eq. (21) and by Jensen’s inequality, we have"
REFERENCES,0.5180863477246208,"ˆQfϕ2(y≤t, yt+1)
 = "
REFERENCES,0.5192532088681447,"T −1
X"
REFERENCES,0.5204200700116686,"t′=t
γt′−t"
REFERENCES,0.5215869311551925,"E
y∼π∗(·|y≤t′)"
REFERENCES,0.5227537922987164,"
fϕ2(y≤t′, y)

−fϕ2(y≤t′, yt′+1) ! ≤"
REFERENCES,0.5239206534422404,"T −1
X"
REFERENCES,0.5250875145857643,"t′=t
γt′−t"
REFERENCES,0.5262543757292882,"E
y∼π∗(·|y≤t′)"
REFERENCES,0.5274212368728122,"
|fϕ2(y≤t′, y)|

+ |fϕ2(y≤t′, yt′+1)| !"
REFERENCES,0.5285880980163361,"Then, by Assumption 1 (i) that ∥fϕ2∥∞≤1, we have"
REFERENCES,0.5297549591598599,"ˆQfϕ2(y≤t, yt+1)
 ≤2"
REFERENCES,0.5309218203033839,"T −1
X"
REFERENCES,0.5320886814469078,"t′=t
γt′−t ≤2"
REFERENCES,0.5332555425904317,"T −1
X"
REFERENCES,0.5344224037339557,"t′=0
γt′ = 2(1 −γT ) 1 −γ"
REFERENCES,0.5355892648774796,"Thus, we have"
REFERENCES,0.5367561260210035,"ˆGon(τ ′, θ)
 ≤2(1 −γT )2B"
REFERENCES,0.5379229871645275,"(1 −γ)2
(24)"
REFERENCES,0.5390898483080513,"Coming back to the bound of ∥ˆ∇L(θ)∥in Eq. (22), we combine it with Eq. (23) and Eq. (24). Then,
we have"
REFERENCES,0.5402567094515752,∥ˆ∇L(θ)∥≤β(1 −γT )B
REFERENCES,0.5414235705950992,"1 −γ
+ 2(1 −β)(1 −γT )2B"
REFERENCES,0.5425904317386231,"(1 −γ)2
|
{z
}
BL ,"
REFERENCES,0.543757292882147,which completes the proof of Lemma 1.
REFERENCES,0.544924154025671,"Lemma 2. Under Assumption 1, the objective function L(θ) is LL-smooth such that for any θ, θ′ ∈Θ,"
REFERENCES,0.5460910151691949,"L(θ) ≤L(θ′) + ⟨∇L(θ′), θ −θ′⟩+ 1"
REFERENCES,0.5472578763127188,"2LL∥θ −θ′∥2,"
REFERENCES,0.5484247374562428,with the constant
REFERENCES,0.5495915985997666,LL = β (1 −γT )(B2 + L)
REFERENCES,0.5507584597432905,"1 −γ
+ (1 −β)2(1 −γT )2"
REFERENCES,0.5519253208868145,(1 −γ)2  γB2
REFERENCES,0.5530921820303384,"1 −γ + L
"
REFERENCES,0.5542590431738623,"Proof. Under the definition of policy gradient in Eq. (10), for any θ1, θ2 ∈Θ, we have"
REFERENCES,0.5554259043173863,∥∇L(θ1) −∇L(θ2)∥
REFERENCES,0.5565927654609102,"=

E
x∼px"
REFERENCES,0.5577596266044341,"
−β
E
τ∼π∗|x"
REFERENCES,0.558926487747958,"
ˆGoff(τ, θ1) −ˆGoff(τ, θ2)
"
REFERENCES,0.5600933488914819,"+ (1 −β)

Eτ1∼πθ1|x"
REFERENCES,0.5612602100350058,"
ˆGon(τ1, θ1)

−Eτ2∼πθ2|x"
REFERENCES,0.5624270711785297,"
ˆGon(τ2, θ2)
 "
REFERENCES,0.5635939323220537,"Then, by Jensen’s inequality and triangle inequality, we have"
REFERENCES,0.5647607934655776,∥∇L(θ1) −∇L(θ2)∥
REFERENCES,0.5659276546091015,"≤
E
x∼px"
REFERENCES,0.5670945157526255,"
β
E
τ∼π∗|x"
REFERENCES,0.5682613768961493,"ˆGoff(τ, θ1) −ˆGoff(τ, θ2)

|
{z
}
I1"
REFERENCES,0.5694282380396732,"+ (1 −β)
Eτ1∼πθ1|x"
REFERENCES,0.5705950991831972,"
ˆGon(τ1, θ1)

−Eτ2∼πθ2|x"
REFERENCES,0.5717619603267211,"
ˆGon(τ2, θ2)

|
{z
}
I2"
REFERENCES,0.572928821470245,"
(25)"
REFERENCES,0.574095682613769,"Based on the definition of off-policy gradient in Eq. (10) and using Jensen’s inequality, we have for
any x ∼px, τ ∼π∗|x,"
REFERENCES,0.5752625437572929,"I1 =
 ˆGoff(τ, θ1) −ˆGoff(τ, θ2) ≤"
REFERENCES,0.5764294049008168,"T −1
X"
REFERENCES,0.5775962660443408,"t=0
γt

E
y∼πθ1(·|y≤t)"
REFERENCES,0.5787631271878646,"
∇log πθ1(y|y≤t)fϕ1(y≤t, y)

−
E
y∼πθ2(·|y≤t)"
REFERENCES,0.5799299883313885,"
∇log πθ2(y|y≤t)fϕ1(y≤t, y)


(26)"
REFERENCES,0.5810968494749125,"Then, by triangle inequality, we have for any t ∈{0, 1, . . . , T −1},

E
y∼πθ1(·|y≤t)"
REFERENCES,0.5822637106184364,"
∇log πθ1(y|y≤t)fϕ1(y≤t, y)

−
E
y∼πθ2(·|y≤t)"
REFERENCES,0.5834305717619603,"
∇log πθ2(y|y≤t)fϕ1(y≤t, y)
 =  X y∈V"
REFERENCES,0.5845974329054843,"
πθ1(y|y≤t)∇log πθ1(y|y≤t)fϕ1(y≤t, y) −πθ2(y|y≤t)∇log πθ2(y|y≤t)fϕ1(y≤t, y)
 ≤
X y∈V"
REFERENCES,0.5857642940490082,"fϕ1(y≤t, y)

πθ1(y|y≤t) −πθ2(y|y≤t)
∇log πθ1(y|y≤t)"
REFERENCES,0.5869311551925321,"+ πθ2(y|y≤t)
∇log πθ1(y|y≤t) −∇log πθ2(y|y≤t)

 (27)"
REFERENCES,0.588098016336056,"By Taylor expansion of πθ(y|y≤t), we have that for any t ∈{0, 1, . . . , T −1},"
REFERENCES,0.5892648774795799,"πθ1(y|y≤t) −πθ2(y|y≤t)
 =
(θ1 −θ2)⊤∇log π˜θ(y|y≤t)π˜θ(y|y≤t)"
REFERENCES,0.5904317386231038,"≤∥θ1 −θ2∥
∇log π˜θ(y|y≤t)
π˜θ(y|y≤t)"
REFERENCES,0.5915985997666278,"≤∥θ1 −θ2∥· B · π˜θ(y|y≤t),"
REFERENCES,0.5927654609101517,"where ˜θ is a vector lying between θ1 and θ2, i.e., there exists some λ ∈[0, 1] such that ˜θ =
λθ1 + (1 −λ)θ2. Then, combining with Eq. (27), yields

E
y∼πθ1(·|y≤t)"
REFERENCES,0.5939323220536756,"
∇log πθ1(y|y≤t)fϕ1(y≤t, y)

−
E
y∼πθ2(·|y≤t)"
REFERENCES,0.5950991831971996,"
∇log πθ2(y|y≤t)fϕ1(y≤t, y)
 ≤
X y∈V"
REFERENCES,0.5962660443407235,"
B2π˜θ(y|y≤t)∥θ1 −θ2∥+ πθ2(y|y≤t)L∥θ1 −θ2∥
"
REFERENCES,0.5974329054842473,=(B2 + L)∥θ1 −θ2∥
REFERENCES,0.5985997666277713,"Then, combining with Eq. (26) yields"
REFERENCES,0.5997666277712952,"I1 =
 ˆGoff(τ, θ1) −ˆGoff(τ, θ2)
 ≤(B2 + L)∥θ1 −θ2∥"
REFERENCES,0.6009334889148191,"T −1
X"
REFERENCES,0.6021003500583431,"t=0
γt ≤(1 −γT )(B2 + L)"
REFERENCES,0.603267211201867,"1 −γ
∥θ1 −θ2∥ (28)"
REFERENCES,0.6044340723453909,"In addition, we can first bound I2 using Jensen’s inequality and triangle inequality,"
REFERENCES,0.6056009334889149,"I2 =
Eτ1∼πθ1|x"
REFERENCES,0.6067677946324388,"
ˆGon(τ1, θ1)

−Eτ2∼πθ2|x"
REFERENCES,0.6079346557759626,"
ˆGon(τ2, θ2)
 ≤"
REFERENCES,0.6091015169194866,"T −1
X t=0"
REFERENCES,0.6102683780630105,"Z
γt| ˆQfϕ2(y≤t, yt+1)| t−1
Y"
REFERENCES,0.6114352392065344,"t′=0
πθ1(yt′+1|y≤t′)∇log πθ1(yt′+1|y≤t′) − t−1
Y"
REFERENCES,0.6126021003500584,"t′=0
πθ2(yt′+1|y≤t′)∇log πθ2(yt′+1|y≤t′)
dy≤1 · · · dy≤tdy1 · · · dyt"
REFERENCES,0.6137689614935823,"By triangle inequality and the boundess of | ˆQfϕ2(y≤t, yt+1)| ≤2(1−γT )"
REFERENCES,0.6149358226371062,"1−γ
, we further have,"
REFERENCES,0.6161026837806302,I2 ≤2(1 −γT ) 1 −γ
REFERENCES,0.617269544924154,"T −1
X t=0"
REFERENCES,0.6184364060676779,"Z
γt
 t−1
Y"
REFERENCES,0.6196032672112018,"t′=0
πθ1(yt′+1|y≤t′) − t−1
Y"
REFERENCES,0.6207701283547258,"t′=0
πθ2(yt′+1|y≤t′)
∥∇log πθ1(yt′+1|y≤t′)∥ + t−1
Y"
REFERENCES,0.6219369894982497,"t′=0
πθ2(yt′+1|y≤t′)∥∇log πθ1(yt′+1|y≤t′) −∇log πθ2(yt′+1|y≤t′)∥

dy≤1 · · · dy≤tdy1 · · · dyt (29)"
REFERENCES,0.6231038506417736,"By Taylor expansion of Qt−1
t′=0 πθ(yt′+1|y≤t′), we have  t−1
Y"
REFERENCES,0.6242707117852976,"t′=0
πθ1(yt′+1|y≤t′) − t−1
Y"
REFERENCES,0.6254375729288215,"t′=0
πθ2(yt′+1|y≤t′)"
REFERENCES,0.6266044340723453,"=
(θ1 −θ2)⊤
t−1
X"
REFERENCES,0.6277712952158693,"t′=0
∇π˜θ(yt′+1|y≤t′) t−1
Y"
REFERENCES,0.6289381563593932,"t′′=0,t′′̸=t′
π˜θ(yt′′+1|y≤t′′)"
REFERENCES,0.6301050175029171,"≤∥θ1 −θ2∥ t−1
X t′=0"
REFERENCES,0.6312718786464411,"∇log π˜θ(yt′+1|y≤t′)

t−1
Y"
REFERENCES,0.632438739789965,"t′′=0
π˜θ(yt′′+1|y≤t′′)"
REFERENCES,0.6336056009334889,"≤∥θ1 −θ2∥· t · B · t−1
Y"
REFERENCES,0.6347724620770129,"t′′=0
π˜θ(yt′′+1|y≤t′′),"
REFERENCES,0.6359393232205367,"where ˜θ denotes a vector lying between θ1 and θ2, i.e., there exists some λ such that ˜θ = λθ1 + (1 −
λ)θ2. Coming back to the boundness of I2 in Eq. (29), we have"
REFERENCES,0.6371061843640606,I2 ≤2(1 −γT ) 1 −γ
REFERENCES,0.6382730455075846,"T −1
X t=0 Z
γt B2t t−1
Y"
REFERENCES,0.6394399066511085,"t′′=0
π˜θ(yt′′+1|y≤t′′) + L t−1
Y"
REFERENCES,0.6406067677946324,"t′=0
πθ2(yt′+1|y≤t′) !"
REFERENCES,0.6417736289381564,∥θ1 −θ2∥dy≤1 · · · dy≤tdy1 · · · dyt
REFERENCES,0.6429404900816803,=2(1 −γT ) 1 −γ
REFERENCES,0.6441073512252042,"T −1
X"
REFERENCES,0.6452742123687282,"t=0
γt(B2t + L)∥θ1 −θ2∥≤2(1 −γT )2"
REFERENCES,0.646441073512252,(1 −γ)2  γB2
REFERENCES,0.6476079346557759,"1 −γ + L

∥θ1 −θ2∥, (30)"
REFERENCES,0.6487747957992999,where the last inequality follows from the fact that
REFERENCES,0.6499416569428238,"T −1
X"
REFERENCES,0.6511085180863477,"t=0
tγt = γ −TγT + (T −1)γT +1"
REFERENCES,0.6522753792298717,"(1 −γ)2
≤γ −TγT +1 + (T −1)γT +1"
REFERENCES,0.6534422403733956,"(1 −γ)2
= γ(1 −γT )"
REFERENCES,0.6546091015169195,(1 −γ)2
REFERENCES,0.6557759626604434,"Then, combining Eq. (25) with the boundness of I1 in Eq. (28) and the boundness of I2 in Eq. (30),
we obtain the final bound of"
REFERENCES,0.6569428238039673,∥∇L(θ1) −∇L(θ2)∥
REFERENCES,0.6581096849474912,"≤

β (1 −γT )(B2 + L)"
REFERENCES,0.6592765460910152,"1 −γ
+ (1 −β)2(1 −γT )2"
REFERENCES,0.6604434072345391,(1 −γ)2  γB2
REFERENCES,0.661610268378063,"1 −γ + L
"
REFERENCES,0.662777129521587,"|
{z
}
LL"
REFERENCES,0.6639439906651109,"∥θ1 −θ2∥
(31)"
REFERENCES,0.6651108518086347,"Next, we have for any θ, θ ∈Θ,"
REFERENCES,0.6662777129521587,"L(θ) −L(θ′) −⟨∇L(θ′), θ −θ′⟩"
REFERENCES,0.6674445740956826,"≤|L(θ) −L(θ′) −⟨∇L(θ′), θ −θ′⟩| ≤  Z"
REFERENCES,0.6686114352392065,"(0,1)
⟨∇L(θ′ + t(θ −θ′)), θ −θ′⟩dt −⟨∇L(θ′), θ −θ′⟩  ≤
Z"
REFERENCES,0.6697782963827305,"(0,1)
∥∇L(θ′ + t(θ −θ′)) −∇L(θ′)∥∥θ −θ′∥dt"
REFERENCES,0.6709451575262544,"Then, by Eq. (31) and set θ1 = θ′ + t(θ −θ′) and θ2 = θ′, we have"
REFERENCES,0.6721120186697783,"L(θ) −L(θ′) −⟨∇L(θ′), θ −θ′⟩ ≤
Z"
REFERENCES,0.6732788798133023,"(0,1)
LL∥θ −θ′∥2tdt = 1"
REFERENCES,0.6744457409568262,"2LL∥θ −θ′∥2,"
REFERENCES,0.67561260210035,which completes the proof of Lemma 2.
REFERENCES,0.676779463243874,We prove Theorem 2 as follows.
REFERENCES,0.6779463243873979,"Proof of Theorem 2. Let θt, θt+1, t ∈{0, 1, . . . , T −1} be adjacent parameters of policy πθt,
πθt+1 given by Algorithm 1. Then, using Lemma 2 by setting θ = θk+1, θ′ = θk for any k ∈
{0, 1, . . . , K −1}, we have"
REFERENCES,0.6791131855309218,"L(πθk+1, fϕ1(θk+1), fϕ2(θk+1))"
REFERENCES,0.6802800466744457,"≤L(πθk, fϕ1(θk), fϕ2(θk)) + ⟨∇L(πθk, fϕ1(θk), fϕ2(θk)), θk+1 −θk⟩+ 1"
REFERENCES,0.6814469078179697,2LL∥θk+1 −θk∥2
REFERENCES,0.6826137689614936,"Following from the updating rule θk+1 = θk −η ˆ∇L(θk) and Lemma 1, we have"
REFERENCES,0.6837806301050176,∥θk+1 −θk∥= η∥ˆ∇L(θk)∥≤ηBL
REFERENCES,0.6849474912485414,"Then, we have"
REFERENCES,0.6861143523920653,"L(πθk+1, fϕ1(θk+1), fϕ2(θk+1))"
REFERENCES,0.6872812135355892,"≤L(πθk, fϕ1(θk), fϕ2(θk)) −⟨∇L(πθk, fϕ1(θk), fϕ2(θk)), η ˆ∇L(θk)⟩+ 1"
REFERENCES,0.6884480746791132,"2η2B2
LLL"
REFERENCES,0.6896149358226371,"We introduce a probability measure space (Ω, F, P) and then θk : Ω→Θ, k ∈{0, 1, . . . , K −1}
can be viewed as a random variable on it. Let {σ(θk)}0≤k≤K−1 denote a sequence of increasing
sigma-algebras such that σ(θ0) ⊂σ(θ1) ⊂· · · σ(θK−1) ⊂F, we define the conditional expectation
E[ ˆ∇L(θk) | σ(θk)] as"
REFERENCES,0.690781796966161,"E[ ˆ∇L(θk) | σ(θk)] =Ex∼px
h
−βEτ∼π∗|x ˆGoff(τ, θk) + (1 −β)Eτ ′∼πθk |x ˆGon(τ ′, θk)
i"
REFERENCES,0.691948658109685,"=∇L(πθk, fϕ1(θk), fϕ2(θk)),"
REFERENCES,0.6931155192532089,"where the second equality follows from the unbiased estimation property in Eq. (10). Then, taking
the conditional expectation, we have"
REFERENCES,0.6942823803967327,"E[L(πθk+1, fϕ1(θk+1), fϕ2(θk+1))|σ(θk)]"
REFERENCES,0.6954492415402567,"≤L(πθk, fϕ1(θk), fϕ2(θk)) −η∥∇L(πθk, fϕ1(θk), fϕ2(θk))∥2 + 1"
REFERENCES,0.6966161026837806,"2η2B2
LLL"
REFERENCES,0.6977829638273045,"Taking total expectation, rearranging the terms and making average on k ∈{0, 1, . . . , K −1}, we
have"
K,0.6989498249708285,"1
K K−1
X"
K,0.7001166861143524,"k=0
E

∥∇L(πθk, fϕ1(θk), fϕ2(θk))∥2 ≤1 ηK K−1
X k=0"
K,0.7012835472578763," 
E[L(πθk, fϕ1(θk), fϕ2(θk))] −E[L(πθk+1, fϕ1(θk+1), fϕ2(θk+1))]

+ 1"
K,0.7024504084014003,"2ηB2
LLL = 1"
K,0.7036172695449242,"ηK
 
L(πθ0, fϕ1(θ0), fϕ2(θ0)) −E[L(πθK, fϕ1(θK), fϕ2(θK))]

+ 1"
K,0.704784130688448,"2ηB2
LLL"
K,0.705950991831972,≤2(1 −γT )
K,0.7071178529754959,η(1 −γ)K + 1
K,0.7082847141190198,"2ηB2
LLL"
K,0.7094515752625438,"Let η =
2
BL q"
K,0.7106184364060677,"1−γT
(1−γ)KLL and denote L(θk) = L(πθk, fϕ1(θk), fϕ2(θk)) for simpilicity for any k ∈
{0, 1, . . . , K −1}, then we have"
K,0.7117852975495916,"min
0≤k≤K−1 E

∥∇L(θk)∥2
≤1 K K−1
X"
K,0.7129521586931156,"k=0
E

∥∇L(θk)∥2
≤2BL s"
K,0.7141190198366394,(1 −γT )LL
K,0.7152858809801633,"(1 −γ)K
= O r"
K,0.7164527421236873,"1
K ! (32)"
K,0.7176196032672112,"A.7
Proof of Corollary 1"
K,0.7187864644107351,"Proof. Let the convergence rate in Eq. (32) satisfy that 2BL
q"
K,0.7199533255542591,(1−γT )LL
K,0.721120186697783,"(1−γ)K
≤ϵ, then we have"
K,0.7222870478413069,"K ≥4(1 −γT )B2
LLL
(1 −γ)ϵ2
,"
K,0.7234539089848308,"which indicates that when the iteration number of policy updating satisfies K := O(ϵ−2), it can
reach an ϵ-accurate stationary point to optimize the objective in Eq. (9), such that"
K,0.7246207701283547,"min
0≤k≤K−1 E

∥∇L(θk)∥2
≤ϵ"
K,0.7257876312718786,"For simplicity, we define the policy as a softmax function with a linear transformation of y≤t ∈RH"
K,0.7269544924154026,"with θ ∈R|V|×H. Formally, for any trajectory τ and any timestep t ∈{0, 1, . . . , T −1}, we have
the probability of any y ∈V,"
K,0.7281213535589265,"πθ(y|y≤t) =
exp(θyy≤t)
P"
K,0.7292882147024504,"y′∈V exp(θy′y≤t)
(33)"
K,0.7304550758459744,"In the following, we will analyze the computational complexity in each policy updating iteration. First,
we find that each inner-loop step of Q-value function updating has a gradient computation complexity
of O(T|V|H) given the linear formulation of Q-value functions. Accordingly, N inner-loop steps
in each policy updating iteration have a computational complexity of O(NT|V|H). Second, for
policy gradient computation, since computational complexity of ∇log πθ(y|y≤T ) is O(|V|H), the
computation complexity of policy gradient computation is O(T|V|H(T + |V|)). Overall, the total
gradient computational complexity is O(ϵ−2T|V|H(N + T + |V|)), which completes the proof of
Corollary 1."
K,0.7316219369894983,"B
Experimental Setup"
K,0.7327887981330222,We use NVIDIA A40 GPUs with 40GB RAM to conduct all the experiments.
K,0.733955659276546,"B.1
Instruction-Following Experiments"
K,0.73512252042007,"Base models. We conduct experiments on both GPT-2 [28] and OpenLLaMA [10]. For the GPT-2
experiments, we use GPT-2 XL2 with 1.5B parameters to construct the teacher policy and GPT-23
with 117M parameters to construct the student policy. For the OpenLLaMA experiments, we use
OpenLLaMA-7B4 with 6.7B parameters to construct the teacher policy and OpenLLaMA-3B5 with
2.7B parameters to construct the student model."
K,0.7362893815635939,"Training details. We fine-tune the OpenLLaMA-7B teacher model and the OpenLLaMA-3B student
models on the corresponding supervised dataset with 10,000 steps. The GPT-2 teacher and student
models use the fine-tuned checkpoints by Gu et al. [14]. For the implementation of compared
baselines, we use the code by Ko et al. [21] and re-run the results. The optimization protocol for KD
training largely follows the previous work [14, 21]. In particular, we search for the learning rates
among a finite set for each experiment to obtain the best result. The batch size for each experiments
is seleted to make full use of the 40GB RAM of an A40 GPU. To handle the adversarial training, we
choose the number of adversarial steps K = 5 and the adversarial control factor α = 0.1 based on
the development experiments. We use a default off-/on-policy combination factor β = 0.5 for main
experiments while exploring other values for analysis. The hyperparameters for training are listed in
Table 3."
K,0.7374562427071178,"2https://huggingface.co/openai-community/gpt2-xl
3https://huggingface.co/openai-community/gpt2
4https://huggingface.co/openlm-research/open_llama_7b
5https://huggingface.co/openlm-research/open_llama_7b"
K,0.7386231038506418,"Table 3: Hyperparameters for instruction-following experiments.
Hyperparameter
GPT-2
OpenLLaMA"
K,0.7397899649941657,"Max. Step Size (K)
10,000
10,000
Inner Step Size (N)
5
5
Batch Size (per GPU)
{8, 16, 32}
{4, 8}
Dropout Rate
0.1
0.1
Controlling Factor (α)
0.1
0.1
Discounting Factor (γ)
{0.90, 0.95, 0.99}
{0.90, 0.95, 0.99}
Combination Factor (β)
{0, 0.5, 0.9, 0.99, 1.0}
{0, 0.5, 0.9, 0.99, 1.0}
Learning Rate (η)
{5e−5, 1e−4, 5e−4}
{5e−6, 1e−5, 5e−5}
Warmup Steps
1,000
500
Weight Decay
1e−2
1e−2
Max Seq. Length
512
512
Sampling (top-p)
1.0
1.0
Sampling (temperature)
1.0
1.0
Evaluation
Greedy Sampling
Greedy Sampling
#GPUs
2
4"
K,0.7409568261376897,"B.2
Task-Specific Experiments"
K,0.7421236872812136,"Base models. For the text summarization and commonsense reasoning experiments, we use T5-
XL6 with 2.8B parameters to construct the teacher policy and construct the student policy with
T5-Large7 (770M parameters), T5-Base8 (220M parameters) and T5-Small9 (60M parameters).
For the machine translation experiments, we use mT5-XL [43] to construct the teacher policy and
mT5-Large/-Base/-Small to construct the student policy."
K,0.7432905484247374,"Training details. We initialize the corresponding teacher and student models using 10,000-step-fine-
tuning checkpoints on the SAMSum dataset, 80,000-step-fine-tuning checkpoints on the IWSLT’17
(en-de) dataset and 3,000-step-fine-tuning checkpoints on the StrategyQA dataset. We largely follow
Ko et al. [21] to set the hyperparameters for training. In particular, we search for the learning rate
from a preset range to obtain the best result for each baseline and our method. The batch size is
selected to make full use of the RAM of GPUs. We use a relatively larger maximum number of
training steps for IWSLT’17 (en-de) experiments to satisfy sufficient convergences for the machine
translation task. We use beam search for the evaluation of the IWSLT’17 (en-de) dataset."
K,0.7444574095682613,"Table 4: Hyperparameters for three task-specific experiments.
Hyperparameter
SAMSum
IWSLT’17 (en-de)
StrategyQA"
K,0.7456242707117853,"Max. Step Size (K)
10,000
80,000
3,000
Inner Step Size (N)
5
2
5
Batch Size (per GPU)
{16, 32, 64}
{16, 32, 64}
{16, 32, 64}
Dropout Rate
0.0
0.3
0.1
Controlling Factor (α)
0.1
0.1
0.1
Discounting Factor (γ)
{0.90, 0.95, 0.99}
{0.90, 0.95, 0.99}
{0.90, 0.95, 0.99}
Combination Factor (β)
{0, 0.5, 0.9, 0.99, 1.0}
{0, 0.5, 0.9, 0.99, 1.0}
{0, 0.5, 0.9, 0.99, 1.0}
Learning Rate (η)
{5e−5, 1e−4, 5e−4}
{1e−4, 5e−4, 1e−3}
{1e−4, 5e−4, 1e−3}
Warmup Steps
1,000
4,000
300
Weight Decay
1e−2
1e−4
1e−2
Max. Seq. Length
1,024
512
1,024
Sampling (top-p)
1.0
1.0
1.0
Sampling (temperature)
1.0
1.0
1.0
Evaluation
Greedy Sampling
Beam Search
Greedy Sampling
#GPUs
2
4
1"
K,0.7467911318553092,"6https://huggingface.co/google/t5-v1_1-xl
7https://huggingface.co/google/t5-v1_1-large
8https://huggingface.co/google/t5-v1_1-base
9https://huggingface.co/google/t5-v1_1-small"
K,0.7479579929988331,"Table 5: Comparison with state-of-the-art KD methods on the instruction-following dataset using
fine-tuned GPT-2 XL (1.5B) as the teacher model and fine-tuned GPT-2 (0.1B) as the student model.
We format the best, the second best and worse than SFT results."
K,0.7491248541423571,"Method
DollyEval
SelfInst
VicunaEval
S-NI
UnNI"
K,0.750291715285881,"GPT-4
R-L
GPT-4
R-L
GPT-4
R-L
R-L
R-L"
K,0.751458576429405,"GPT-2 XL (teacher)
45.5±0.7
28.2±0.8
34.7±1.6
14.3±0.2
32.7±1.6
16.2±0.3
27.6±0.3
32.2±0.3
SFT (student)
29.8±1.2
23.4±0.2
20.2±0.7
10.3±0.5
17.8±0.9
14.6±0.4
16.1±0.3
18.2±0.6
KD [16]
29.5±0.8
23.8±0.3
18.0±1.0
12.3±0.2
17.2±0.7
15.2±0.4
20.8±0.5
22.5±0.3
SeqKD [20]
29.8±0.5
24.2±0.2
18.2±0.8
11.6±0.4
18.2±0.7
15.5±0.3
15.5±0.6
20.1±0.1
ImitKD [24]
26.4±0.6
22.7±0.5
18.2±0.5
11.5±0.4
18.6±0.4
14.5±0.3
18.2±0.3
21.8±0.4
MiniLLM [14]
30.2±1.2
24.3±0.3
20.5±0.3
13.2±0.3
20.5±0.7
18.5±0.3
22.7±0.3
23.5±0.2
GKD [2]
29.2±0.6
23.6±0.2
20.7±0.5
12.7±0.2
20.2±0.6
17.7±0.2
25.1±0.3
25.9±0.1
DistiLLM [21]
31.2±0.4
25.2±0.4
21.7±0.5
12.5±0.3
22.5±1.2
19.2±0.5
27.7±0.2
27.6±0.4
Ours
31.7±0.5
26.1±0.3
22.7±0.5
14.2±0.3
23.6±0.8
20.5±0.2
28.6±0.2
29.9±0.5"
K,0.7526254375729288,"KL
RKL
JS
TV
Ours
20 22 24 26 28 30 32"
K,0.7537922987164527,ROUGE-L
K,0.7549591598599766,"on-policy
off-policy
mixed"
K,0.7561260210035006,(a) DollyEval.
K,0.7572928821470245,"KL
RKL
JS
TV
Ours
16 17 18 19 20 21 22 23"
K,0.7584597432905484,ROUGE-L
K,0.7596266044340724,"on-policy
off-policy
mixed"
K,0.7607934655775963,(b) SelfInst.
K,0.7619603267211202,"KL
RKL
JS
TV
Ours
15 16 17 18 19 20 21 22 23"
K,0.7631271878646441,ROUGE-L
K,0.764294049008168,"on-policy
off-policy
mixed"
K,0.7654609101516919,(c) VicunaEval.
K,0.7666277712952159,"KL
RKL
JS
TV
Ours
30 32 34 36 38 40"
K,0.7677946324387398,ROUGE-L
K,0.7689614935822637,"on-policy
off-policy
mixed"
K,0.7701283547257877,(d) S-NI.
K,0.7712952158693116,"KL
RKL
JS
TV
Ours 26 28 30 32 34 36 38 40"
K,0.7724620770128354,ROUGE-L
K,0.7736289381563594,"on-policy
off-policy
mixed"
K,0.7747957992998833,(e) UnNI.
K,0.7759626604434072,"Figure 4: Performance of difference step-wise distribution distances on five instruction-following
datasets using OpenLLaMA-7B →OpenLLaMA-3B."
K,0.7771295215869312,"C
Additional Results"
K,0.7782963827304551,"C.1
Results Based on GPT-2"
K,0.779463243873979,"In addition to the experimental results based on OpenLLaMA for instruction-following tasks, we
also conduct experiments based on GPT-2. Results are illustrated in Table 5. Compared with current
state-of-the-art KD approaches, our method achieves the best results on five datasets with both GPT-4
feedback and ROUGE-L evaluations."
K,0.780630105017503,"C.2
Comparisons on Step-Wise Distribution Distance"
K,0.7817969661610268,"Figure 4 and Figure 5 illustrate performance comparison with well-studied step-wise distribution
distance, including KL, RKL, JS divergences and TV distances. Results show that the optimization
of proposed moment-matching objectives outperforms other step-wise distribution distances via
either on-policy distillation or off-policy distillation. Besides, jointly using on-policy and off-policy
moment-matching further improves the performances and achieves the best results on five instruction-
following datasets with KD from the OpenLLaMA-7B to OpenLLaMA-3B model, and achieves the
best results on three task-specific datasets with KD from the (m)T5-XL to (m)T5-Base model."
K,0.7829638273045507,"KL
RKL
JS
TV
Ours
43 44 45 46 47 48 49 50 51 52"
K,0.7841306884480747,ROUGE-L
K,0.7852975495915986,"on-policy
off-policy
mixed"
K,0.7864644107351225,(a) SAMSum.
K,0.7876312718786465,"KL
RKL
JS
TV
Ours
26 27 28 29 30 31 32 33 34 BLEU"
K,0.7887981330221704,"on-policy
off-policy
mixed"
K,0.7899649941656943,(b) IWSLT’17 (en-de).
K,0.7911318553092183,"KL
RKL
JS
TV
Ours
54 56 58 60 62 64"
K,0.7922987164527421,Accuracy
K,0.793465577596266,"on-policy
off-policy
mixed"
K,0.79463243873979,(c) StrategyQA.
K,0.7957992998833139,"Figure 5: Performance of difference step-wise distribution distances on three task-specific datasets
using (m)T5-XL →(m)T5-Base."
K,0.7969661610268378,"0
2000
4000
6000
8000
10000
Step 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
K,0.7981330221703618,Loss (train)
K,0.7992998833138857,training loss
K,0.8004667444574096,on-policy distance don
K,0.8016336056009334,off-policy distance doff 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
K,0.8028004667444574,Distance (eval)
K,0.8039673278879813,(a) Instruct-following.
K,0.8051341890315052,"0
2000
4000
6000
8000
10000
Step 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1"
K,0.8063010501750292,Loss (train)
K,0.8074679113185531,training loss
K,0.808634772462077,on-policy distance don
K,0.809801633605601,off-policy distance doff 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2
K,0.8109684947491248,Distance (eval)
K,0.8121353558926487,(b) SAMSum.
K,0.8133022170361727,"0
10000
20000
30000
40000
50000
Step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60"
K,0.8144690781796966,Loss (train)
K,0.8156359393232205,training loss
K,0.8168028004667445,on-policy distance don
K,0.8179696616102684,off-policy distance doff 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
K,0.8191365227537923,Distance (eval)
K,0.8203033838973163,(c) IWSLT’17 (en-de).
K,0.8214702450408401,"0
250
500
750
1000
1250
1500
1750
2000
Step 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3"
K,0.822637106184364,Loss (train)
K,0.823803967327888,training loss
K,0.8249708284714119,on-policy distance don
K,0.8261376896149358,off-policy distance doff 1.0 1.2 1.4 1.6 1.8 2.0
K,0.8273045507584598,Distance (eval)
K,0.8284714119019837,(d) StrategyQA.
K,0.8296382730455076,"Figure 6: Training loss and don
MM, doff
MM against training step on four datasets."
K,0.8308051341890315,"C.3
Adversarial Training Procedure"
K,0.8319719953325554,"Figure 6 illustrates the training loss and on-/off-policy moment-matching distances against the training
steps on the instruction-following dataset and three task-specific datasets. We can observe that the
training losses on four datasets have a similar trend, increasing at the beginning and then converging
to a relatively lower level. The trend of loss function aligns with the characteristics of adversarial
training with gradient descent ascent. In contrast, both the on-policy moment-matching distance don
MM
and the off-policy moment-matching distance doff
MM reduce as the number of training steps increases,
which shows the effectiveness of our adversarial training approach for moment-matching."
K,0.8331388564760793,DollyEval
K,0.8343057176196033,SelfInst
K,0.8354725787631272,VicunaEval S-NI UnNI
K,0.8366394399066511,Average KL RKL JS TV Ours
K,0.8378063010501751,"2.65
4.87
3.21
3.67
3.02
3.48"
K,0.838973162193699,"2.54
4.78
3.87
3.58
2.67
3.48"
K,0.8401400233372228,"2.31
4.52
3.67
3.32
2.43
3.25"
K,0.8413068844807468,"1.98
2.34
3.32
2.13
1.83
2.32"
K,0.8424737456242707,"0.96
1.85
2.11
0.85
0.92
1.34"
K,0.8436406067677946,don on five test sets 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
K,0.8448074679113186,(a) On-policy moment-matching distance.
K,0.8459743290548425,DollyEval
K,0.8471411901983664,SelfInst
K,0.8483080513418904,VicunaEval S-NI UnNI
K,0.8494749124854143,Average KL RKL JS TV Ours
K,0.8506417736289381,"2.05
2.79
2.21
2.07
2.02
2.23"
K,0.851808634772462,"1.84
3.28
2.37
2.18
1.67
2.27"
K,0.852975495915986,"1.78
2.62
2.15
1.68
1.43
1.93"
K,0.8541423570595099,"1.53
1.94
2.42
1.13
1.23
1.65"
K,0.8553092182030338,"0.75
1.45
1.11
0.72
0.62
0.93"
K,0.8564760793465578,doff on five test sets 1.0 1.5 2.0 2.5 3.0
K,0.8576429404900817,(b) Off-policy moment-matching distance.
K,0.8588098016336057,Figure 7: Moment-matching via distribution-matching on the instruction-following dataset.
K,0.8599766627771295,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.8611435239206534,"2.85
2.58
2.35
2.52
1.67"
K,0.8623103850641773,"2.1
2.32
2.18
2.06
1.32"
K,0.8634772462077013,"2.02
2.25
1.98
1.78
1.24"
K,0.8646441073512252,don on five test sets
K,0.8658109684947491,"1.4
1.6
1.8
2.0
2.2
2.4
2.6
2.8"
K,0.8669778296382731,"(a) On-policy moment-matching
distance on SAMSum."
K,0.868144690781797,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.8693115519253208,"2.55
2.38
2.25
2.32
1.5"
K,0.8704784130688448,"2.22
2.23
2.08
1.85
1.02"
K,0.8716452742123687,"1.97
2.17
1.78
1.93
1.23"
K,0.8728121353558926,don on IWSLT 17 (en-de)
K,0.8739789964994166,"1.2
1.4
1.6
1.8
2.0
2.2
2.4"
K,0.8751458576429405,"(b) On-policy moment-matching
distance on IWSLT’17 (en-de)."
K,0.8763127187864644,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.8774795799299884,"2.61
2.31
2.52
2.18
1.87"
K,0.8786464410735122,"2.28
2.37
2.33
2.12
1.61"
K,0.8798133022170361,"2.42
2.16
2.25
1.87
1.39"
K,0.8809801633605601,don on StrategyQA
K,0.882147024504084,"1.4
1.6
1.8
2.0
2.2
2.4
2.6"
K,0.8833138856476079,"(c) On-policy moment-matching
distance on StrategyQA."
K,0.8844807467911319,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.8856476079346558,"2.85
2.78
3.17
2.77
1.82"
K,0.8868144690781797,"2.4
2.28
2.37
2.18
1.21"
K,0.8879813302217037,"2.21
2.32
2.27
2.02
1.34"
K,0.8891481913652275,doff on five test sets
K,0.8903150525087514,"1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00"
K,0.8914819136522754,"(d) Off-policy moment-matching
distance on SAMSum."
K,0.8926487747957993,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.8938156359393232,"2.08
2.51
2.14
1.95
1.12"
K,0.8949824970828472,"1.65
2.32
1.87
1.23
0.72"
K,0.8961493582263711,"1.87
2.1
1.72
1.32
0.68"
K,0.897316219369895,doff on IWSLT 17 (en-de)
K,0.8984830805134189,"0.75
1.00
1.25
1.50
1.75
2.00
2.25
2.50"
K,0.8996499416569428,"(e) Off-policy moment-matching
distance on IWSLT’17 (en-de)."
K,0.9008168028004667,"KL
RKL
JS
TV
Ours Small Base Large"
K,0.9019836639439907,"2.52
2.23
2.46
2.24
1.37"
K,0.9031505250875146,"1.95
2.18
2.32
2.03
1.28"
K,0.9043173862310385,"2.23
1.89
2.13
1.91
1.19"
K,0.9054842473745625,doff on StrategyQA
K,0.9066511085180864,"1.2
1.4
1.6
1.8
2.0
2.2
2.4"
K,0.9078179696616102,"(f) Off-policy moment-matching
distance on StrategyQA."
K,0.9089848308051341,Figure 8: Moment-matching via distribution-matching on three task-specific datasets.
K,0.9101516919486581,"C.4
Moment-Matching via Distribution Matching"
K,0.911318553092182,"We investigate how the distribution-matching methods via KL, RKL, JS divergences or TV distance
can optimize the moment-matching distance in Figure 7 and Figure 8. Results show that the proposed
adversarial training algorithm is more effective in minimizing the moment-matching distance than
the distribution-matching methods."
K,0.912485414235706,"C.5
Analysis on the Off-/On-Policy Combination Factor β"
K,0.9136522753792299,"We study the impact of on-policy and off-policy objectives with the combination factor β ∈
{0.00, 0.25, 0.50, 0.75, 1.00} in Eq. (9), which denotes a linear combination coefficient of the
on-policy and off-policy objectives. We observe that if β = 0.00, only the on-policy objective
contributes to policy learning. As it increases from 0 to 1, the influence of off-policy objective
increases while that of the on-policy objective decreases. Finally, when β = 1.00, only the off-policy
objective contributes to policy learning. We conduct experiments across four datasets. Specifically,
we evaluate ROUGE-L for OpenLLaMA2-3B on the DollyEval dataset, ROUGE-L for T5-base on
the SAMSum dataset, accuracy for T5-base on the IWSLT’17 dataset and accuracy for T5-base on"
K,0.9148191365227538,Table 6: Effects of the off-/on-policy combination factor β on four datasets.
K,0.9159859976662778,"β
0.00
0.25
0.50
0.75
1.00"
K,0.9171528588098017,"DollyEval
28.8±0.7
31.2±0.3
30.7±0.4
29.8±0.2
27.4±0.4
SAMSum
48.2±0.3
50.5±0.2
50.4±0.3
51.2±0.4
48.7±0.2
IWSLT’17 (en-de)
30.7±0.1
31.7±0.6
32.4±0.2
33.2±0.2
31.2±0.2
StrategyQA
59.7±0.4
61.4±0.2
62.9±0.4
62.7±0.4
60.8±0.3"
K,0.9183197199533255,"the StrategyQA dataset. Results in Table 6 show that a combination of on-policy and off-policy
objectives outperforms using either on-policy or off-policy objectives only across four datasets."
K,0.9194865810968494,NeurIPS Paper Checklist
CLAIMS,0.9206534422403734,1. Claims
CLAIMS,0.9218203033838973,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9229871645274212,Answer: [Yes] .
CLAIMS,0.9241540256709452,"Justification: The claim of contributions in the abstract and introduction have been fully
reflected in the sections of Methods and Experiments."
CLAIMS,0.9253208868144691,Guidelines:
CLAIMS,0.926487747957993,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9276546091015169,2. Limitations
LIMITATIONS,0.9288214702450408,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9299883313885647,Answer: [Yes] .
LIMITATIONS,0.9311551925320887,Justification: Limitations are discussed in section of the conclusion.
LIMITATIONS,0.9323220536756126,Guidelines:
LIMITATIONS,0.9334889148191365,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size."
LIMITATIONS,0.9346557759626605,"• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9358226371061844,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9369894982497082,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9381563593932322,"Answer: [Yes] .
Justification: Complete proofs for the theoretical results are available in Appendix A. All
proofs are based on Assumption 1."
THEORY ASSUMPTIONS AND PROOFS,0.9393232205367561,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.94049008168028,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.941656942823804,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9428238039673279,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9439906651108518,"Answer: [Yes] .
Justification: Detailed experimental setups such as datasets, models and hyperparameters
used in implementing proposed algorithms are all described in detail. See Appendix B."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9451575262543758,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9463243873978997,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9474912485414235,"(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9486581096849475,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9498249708284714,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9509918319719953,Answer: [Yes] .
OPEN ACCESS TO DATA AND CODE,0.9521586931155193,"Justification:
All the datasets used in this work are publicly available.
The code
and implementation details are released at this GitHub URL: https://github.com/
jiachenwestlake/MMKD."
OPEN ACCESS TO DATA AND CODE,0.9533255542590432,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9544924154025671,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9556592765460911,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9568261376896149,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9579929988331388,Answer: [Yes] .
OPEN ACCESS TO DATA AND CODE,0.9591598599766628,Justification: We provide the details of experimental settings in Appendix B.
OPEN ACCESS TO DATA AND CODE,0.9603267211201867,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9614935822637106,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9626604434072346,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9638273045507585,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9649941656942824,Answer: [Yes] .
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9661610268378062,Justification: All experimental results have error bars.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9673278879813302,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9684947491248541,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.969661610268378,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.970828471411902,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9719953325554259,Answer: [Yes] .
EXPERIMENTS COMPUTE RESOURCES,0.9731621936989499,Justification: Available in Appendix B.
EXPERIMENTS COMPUTE RESOURCES,0.9743290548424738,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9754959159859977,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9766627771295215,9. Code Of Ethics
CODE OF ETHICS,0.9778296382730455,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9789964994165694,Answer: [Yes] .
CODE OF ETHICS,0.9801633605600933,Justification: The research is conducted with the NeurIPS Code of Ethics.
CODE OF ETHICS,0.9813302217036173,Guidelines:
CODE OF ETHICS,0.9824970828471412,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9836639439906651,10. Broader Impacts
BROADER IMPACTS,0.9848308051341891,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA] .
Justification: Our work mainly focuses on algorithm design and performance improvement,
which has no relationship to societal impacts.
Guidelines:"
BROADER IMPACTS,0.9859976662777129,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9871645274212368,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA] .
Justification: The paper poses no such risks.
Guidelines:"
BROADER IMPACTS,0.9883313885647608,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9894982497082847,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes] .
Justification: The paper has cited the original papers that produced the models, code packages
or datasets."
BROADER IMPACTS,0.9906651108518086,Guidelines:
BROADER IMPACTS,0.9918319719953326,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
BROADER IMPACTS,0.9929988331388565,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA] .
Justification: The paper does not release new assets.
Guidelines:"
BROADER IMPACTS,0.9941656942823804,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
BROADER IMPACTS,0.9953325554259043,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
BROADER IMPACTS,0.9964994165694282,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] ."
BROADER IMPACTS,0.9976662777129521,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
BROADER IMPACTS,0.9988331388564761,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
