Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011723329425556857,"In Federated Learning, a common approach for aggregating local models across
clients is periodic averaging of the full model parameters. It is, however, known
that different layers of neural networks can have a different degree of model dis-
crepancy across the clients. The conventional full aggregation scheme does not
consider such a difference and synchronizes the whole model parameters at once,
resulting in inefﬁcient network bandwidth consumption. Aggregating the parame-
ters that are similar across the clients does not make meaningful training progress
while increasing the communication cost. We propose FedLAMA, a layer-wise
model aggregation scheme for scalable Federated Learning. FedLAMA adap-
tively adjusts the aggregation interval in a layer-wise manner, jointly considering
the model discrepancy and the communication cost. The layer-wise aggregation
method enables to ﬁnely control the aggregation interval to relax the aggregation
frequency without a signiﬁcant impact on the model accuracy. Our empirical study
shows that FedLAMA reduces the communication cost by up to 60% for IID data
and 70% for non-IID data while achieving a comparable accuracy to FedAvg."
INTRODUCTION,0.0023446658851113715,"1
INTRODUCTION"
INTRODUCTION,0.0035169988276670576,"In Federated Learning, periodic full model aggregation is the most common approach for aggregat-
ing local models across clients. Many Federated Learning algorithms, such as FedAvg (McMahan
et al. (2017)), FedProx (Li et al. (2018)), FedNova (Wang et al. (2020)), and SCAFFOLD (Karim-
ireddy et al. (2020)), assume the underlying periodic full aggregation scheme. However, it has been
observed that the magnitude of gradients can be signiﬁcantly different across the layers of neural net-
works (You et al. (2019)). That is, all the layers can have a different degree of model discrepancy.
The periodic full aggregation scheme does not consider such a difference and synchronizes the en-
tire model parameters at once. Aggregating the parameters that are similar across all the clients does
not make meaningful training progress while increasing the communication cost. Considering the
limited network bandwidth in usual Federated Learning environments, such an inefﬁcient network
bandwidth consumption can signiﬁcantly harm the scalability of Federated Learning applications."
INTRODUCTION,0.004689331770222743,"Many researchers have put much effort into addressing the expensive communication issue. Adap-
tive model aggregation methods adjust the aggregation interval to reduce the total communication
cost (Wang & Joshi (2018); Haddadpour et al. (2019)). Gradient (model) compression (Alistarh
et al. (2018); Albasyoni et al. (2020)), sparsiﬁcation (Wangni et al. (2017); Wang et al. (2018)),
low-rank approximation (Vogels et al. (2020); Wang et al. (2021)), and quantization (Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)) techniques directly reduce the local data size.
Employing heterogeneous model architectures across clients is also a communication-efﬁcient ap-
proach (Diao et al. (2020)). While all these works effectively tackle the expensive communication
issue from different angles, they commonly assume the underlying periodic full model aggregation."
INTRODUCTION,0.005861664712778429,"To break such a convention of periodic full model aggregation, we propose FedLAMA, a novel layer-
wise adaptive model aggregation scheme for scalable and accurate Federated Learning. FedLAMA
ﬁrst prioritizes all the layers based on their contributions to the total model discrepancy. We present
a metric for estimating the layer-wise degree of model discrepancy at run-time. The aggregation
intervals are adjusted based on the layer-wise model discrepancy such that the layers with a smaller
degree of model discrepancy is assigned with a longer aggregation interval than the other layers.
The above steps are repeatedly performed once the entire model is synchronized once."
INTRODUCTION,0.007033997655334115,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008206330597889801,"Our focus is on how to relax the model aggregation frequency at each layer, jointly considering the
communication efﬁciency and the impact on the convergence properties of federated optimization.
By adjusting the aggregation interval based on the layer-wise model discrepancy, the local mod-
els can be effectively synchronized while reducing the number of communications at each layer.
The model accuracy is marginally affected since the intervals are increased only at the layers that
have the smallest contribution to the total model discrepancy. Our empirical study demonstrates
that FedLAMA automatically ﬁnds the interval settings that make a practical trade-off between the
communication cost and the model accuracy. We also provide a theoretical convergence analysis of
FedLAMA for smooth and non-convex problems under non-IID data settings."
INTRODUCTION,0.009378663540445486,"We evaluate the performance of FedLAMA across three representative image classiﬁcation bench-
mark datasets: CIFAR-10 (Krizhevsky et al. (2009)), CIFAR-100, and Federated Extended MNIST
(Cohen et al. (2017)). Our experimental results deliver novel insights on how to aggregate the local
models efﬁciently consuming the network bandwidth. Given a ﬁxed number of training iterations, as
the aggregation interval increases, FedLAMA reduces the communication cost by up to 60% under
IID data settings and 70% under non-IID data settings, while having only a marginal accuracy drop."
RELATED WORKS,0.010550996483001172,"2
RELATED WORKS"
RELATED WORKS,0.011723329425556858,"Compression Methods – The communication-efﬁcient global model update methods can be catego-
rized into two groups: structured update and sketched update (Koneˇcn`y et al. (2016)). The structured
update indicates the methods that enforce a pre-deﬁned ﬁxed structure of the local updates, such as
low-rank approximation and random mask methods. The sketched update indicates the methods
that post-process the local updates via compression, sparsiﬁcation, or quantization. Both directions
are well studied and have shown successful results (Alistarh et al. (2018); Albasyoni et al. (2020);
Wangni et al. (2017); Wang et al. (2018); Vogels et al. (2020); Wang et al. (2021); Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)). The common principle behind these methods is
that the local updates can be replaced with a different data representation with a smaller size."
RELATED WORKS,0.012895662368112544,"These compression methods can be independently applied to our layer-wise aggregation scheme
such that the each layer’s local update is compressed before being aggregated. Since our focus is on
adjusting the aggregation frequency rather than changing the data representation, we do not directly
compare the performance between these two approaches. We leave harmonizing the layer-wise
aggregation scheme and a variety of compression methods as a promising future work."
RELATED WORKS,0.01406799531066823,"Similarity Scores – Canonical Correlation Analysis (CCA) methods are proposed to estimate the
representational similarity across different models (Raghu et al. (2017); Morcos et al. (2018)). Cen-
tered Kernel Alignment (CKA) is an improved extension of CCA (Kornblith et al. (2019)). While
these methods effectively quantify the degree of similarity, they commonly require expensive com-
putations. For example, SVCCA performs singular vector decomposition of the model and CKA
computes Hilbert-Schmidt Independence Criterion multiple times (Gretton et al. (2005)). In addi-
tion, the representational similarity does not deliver any information regarding the gradient differ-
ence that is strongly related to the convergence property. We will propose a practical metric for
estimating the layer-wise model discrepancy, which is cheap enough to be used at run-time."
RELATED WORKS,0.015240328253223915,"Layer-wise Model Freezing – Layer freezing (dropping) is the representative layer-wise technique
for neural network training (Brock et al. (2017); Kumar et al. (2019); Zhang & He (2020); Goutam
et al. (2020)). All these methods commonly stop updating the parameters of the layers in a bottom-
up direction. These empirical techniques are supported by the analysis presented in (Raghu et al.
(2017)). Since the layers converge from the input-side sequentially, the layer-wise freezing can re-
duce the training time without strongly affecting the accuracy. These previous works clearly demon-
strate the advantages of processing individual layers separately."
BACKGROUND,0.016412661195779603,"3
BACKGROUND"
BACKGROUND,0.017584994138335287,Federated Optimization – We consider federated optimization problems as follows.
BACKGROUND,0.01875732708089097,"min
x∈Rd """
BACKGROUND,0.01992966002344666,"F(x) := m
X"
BACKGROUND,0.021101992966002344,"i=1
piFi(x) # ,
(1)"
BACKGROUND,0.022274325908558032,Under review as a conference paper at ICLR 2022
BACKGROUND,0.023446658851113716,"Algorithm 1: FedLAMA: Federated Layer-wise Adaptive Model Aggregation.
Input: τ ′: base aggregation interval, φ: interval increasing factor, pi, i ∈{1, · · · , m}."
BACKGROUND,0.0246189917936694,"1 τl ←τ ′, ∀l ∈{1, · · · , L};"
BACKGROUND,0.02579132473622509,2 for k = 1 to K do
BACKGROUND,0.026963657678780773,"3
SGD step: xi
k = xi
k−1 −η∇f(wi
k−1, ξk);"
BACKGROUND,0.02813599062133646,"4
for l = 1 to L do"
BACKGROUND,0.029308323563892145,"5
if k mod τl is 0 then"
BACKGROUND,0.03048065650644783,"6
Synchronize layer l: u(l,k) ←Pm
i=1 pixi
(l,k);"
BACKGROUND,0.031652989449003514,"7
dl ←Pm
i=1

pi∥u(l,k) −xi
(l,k)∥2
/(τl(dim(u(l,k))) ;"
BACKGROUND,0.032825322391559206,"8
if k mod φτ ′ is 0 then"
BACKGROUND,0.03399765533411489,"9
Adjust aggregation interval at all L layers (Algorithm 2).;"
BACKGROUND,0.035169988276670575,10 Output: uK;
BACKGROUND,0.03634232121922626,"where pi = ni/n is the ratio of local data to the total dataset, and Fi(x) =
1
ni
P"
BACKGROUND,0.03751465416178194,"ξ∈D fi(x, ξ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size."
BACKGROUND,0.038686987104337635,"FedAvg is a basic algorithm that solves the above minimization problem. As the degree of data het-
erogeneity increases, FedAvg converges more slowly. Several variants of FedAvg, such as FedProx,
FedNova, and SCAFFOLD, tackle the data heterogeneity issue. All these algorithms commonly
aggregate the local solutions using the periodic full aggregation scheme."
BACKGROUND,0.03985932004689332,"Model Discrepancy – All local SGD-based algorithms allow the clients to independently train their
local models within each communication round. The variance of stochastic gradients and heteroge-
neous data distribution can lead the local models to different directions on parameter space during
the local update steps. We formally deﬁne such a discrepancy among the models as follows. m
X"
BACKGROUND,0.041031652989449004,"i=1
pi∥u −xi∥2,"
BACKGROUND,0.04220398593200469,"where m is the number of clients, u is the synchronized model, and xi is client i’s local model.
This quantity bounds the difference between the local gradients and the global gradients under a
smoothness assumption on objective functions."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04337631887456037,"4
LAYER-WISE ADAPTIVE MODEL AGGREGATION"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.044548651817116064,"Layer Prioritization – In theoretical analysis, it is common to assume the smoothness of objective
functions such that the difference between local gradients and global gradients is bounded by a
scaled difference of the corresponding sets of parameters. Motivated by this convention, we deﬁne
‘layer-wise unit model discrepancy’, a useful metric for prioritizing the layers as follows."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04572098475967175,"dl =
Pm
i=1 pi∥ul −xi
l∥2"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04689331770222743,"τl(dim(ul))
,
l ∈{1, · · · , L}
(2)"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.04806565064478312,"where L is the number of layers, l is the layer index, u is the global parameters, xi is the client i’s
local parameters, τ is the aggregation interval, and dim(·) is the number of parameters."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.0492379835873388,"This metric quantiﬁes how much each parameter contributes to the model discrepancy at each iter-
ation. The communication cost is proportional to the number of parameters. Thus, Pm
i=1 pi∥ul −
xi
l∥2/dim(ul) shows how much model discrepancy can be eliminated by synchronizing the layer
at a unit communication cost. This metric allows prioritizing the layers such that the layers with a
smaller dl value has a lower priority than the others."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05041031652989449,"Adaptive Model Aggregation Algorithm – We propose FedLAMA, a layer-wise adaptive model
aggregation scheme. Algorithm 1 shows FedLAMA algorithm. There are two input parameters: τ ′
is the base aggregation interval and φ is the interval increase factor. First, the parameters at layer
l are synchronized across the clients after every τl iterations (line 6). Then, the proposed metric"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05158264947245018,Under review as a conference paper at ICLR 2022
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05275498241500586,"Algorithm 2: Layer-wise Adaptive Interval Adjustment.
Input: d: the observed model discrepancy at all L layers, τ ′: the base aggregation interval, φ:
the interval increasing factor."
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.053927315357561546,1 Sorted model discrepancy: ˆd ←sort (d);
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05509964830011723,2 Sorted index of the layers: ˆi ←argsort (d);
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05627198124267292,"3 Total model size: λ ←PL
l=1 dim(ul);"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05744431418522861,"4 Total model discrepancy: δ ←PL
l=1 dl ∗dim(ul);"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.05861664712778429,5 for l = 1 to L do
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.059788980070339975,"6
δl ←(Pl
i=1 ˆdi ∗dim(ui))/δ;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06096131301289566,"7
λl ←(Pl
i=1 dim(ui))/λ;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06213364595545135,"8
Find the layer index: i ←ˆil ;"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06330597889800703,"9
if δl < λl then"
LAYER-WISE ADAPTIVE MODEL AGGREGATION,0.06447831184056271,"10
τi ←φτ ′;"
ELSE,0.06565064478311841,"11
else"
ELSE,0.0668229777256741,"12
τi ←τ ′;"
ELSE,0.06799531066822978,13 Output: τ: the adjusted aggregation intervals at all L layers.;
ELSE,0.06916764361078546,"dl is calculated using the synchronized parameters ul (line 7). At the end of every φτ ′ iterations,
FedLAMA adjusts the model aggregation interval at all the L layers. (line 9)."
ELSE,0.07033997655334115,"Algorithm 2 ﬁnds the layers that can be less frequently aggregated making a minimal impact on the
total model discrepancy. First, the layer-wise degree of model discrepancy is estimated as follows."
ELSE,0.07151230949589683,"δl =
Pl
i=1 ˆdi ∗dim(ui)
PL
i=1 ˆdi ∗dim(ui)
,
(3)"
ELSE,0.07268464243845252,"where ˆdi is the ith smallest element in the sorted list of the proposed metric d. Given l layers with
the smallest dl values, δl quantiﬁes their contribution to the total model discrepancy. Second, the
communication cost impact is estimated as follows."
ELSE,0.0738569753810082,"λl =
Pl
i=1 dim(ui)
PL
i=1 dim(ui)
(4)"
ELSE,0.07502930832356389,"λl is the ratio of the parameters at the l layers with the smallest dl values. Thus, 1 −λl estimates
the number of parameters that will be more frequently synchronized than the others. As l increases,
δl increases while 1 −λl decreases monotonically. Algorithm 2 loops over the L layers ﬁnding the
l value that makes δl and 1 −λl similar. In this way, it ﬁnds the aggregation interval setting that
slightly sacriﬁces the model discrepancy while remarkably reducing the communication cost."
ELSE,0.07620164126611957,"Figure 1 shows the δl and 1 −λl curves collected from a) CIFAR-10 (ResNet20) training and b)
CIFAR-100 (Wide-ResNet28-10) training. The x-axis is the number of layers to increase the aggre-
gation interval and the y-axis is the δl and 1 −λl values. The cross point of the two curves is much
lower than 0.5 on y-axis in both charts. For instance, in Figure 1.a), the two curves are crossed when
x value is 9, and the corresponding y value is near 0.2. That is, when the aggregation interval is
increased at those 9 layers, 20% of the total model discrepancy will increase by a factor of φ while
80% of the total communication cost will decrease by the same factor. Note that the cross points are
below 0.5 since the δl and 1 −λl are calculated using the dl values sorted in an increasing order."
ELSE,0.07737397420867527,"It is worth noting that FedLAMA can be easily extended to improve the convergence rate at the cost
of having minor extra communications. In this work, we do not consider ﬁnding such interval set-
tings because it can increase the latency cost, which is not desired in Federated Learning. However,
in the environments where the latency cost can be ignored, such as high-performance computing
platforms, FedLAMA can accelerate the convergence by adjusting the intervals based on the cross
point of 1 −δl and λl calculated using the list of dl values sorted in a decreasing order."
ELSE,0.07854630715123095,"Impact of Aggregation Interval Increasing Factor φ – In Federated Learning, the communication
latency cost is usually not negligible, and the total number of communications strongly affects the"
ELSE,0.07971864009378664,Under review as a conference paper at ICLR 2022
ELSE,0.08089097303634232,"a) CIFAR-10 (ResNet20) 
b) CIFAR-100 (WRN28-10)"
ELSE,0.08206330597889801,"0
5
10
15
20
25
0.0 0.2 0.4 0.6 0.8 1.0"
ELSE,0.08323563892145369,normalized magnitude
ELSE,0.08440797186400938,# of layers with increased interval
ELSE,0.08558030480656506,"delta
 lambda"
ELSE,0.08675263774912075,"0
5
10
15
20
0.0 0.2 0.4 0.6 0.8 1.0"
ELSE,0.08792497069167643,normalized magnitude
ELSE,0.08909730363423213,# of layers with increased interval
ELSE,0.09026963657678781,"delta
 lambda"
ELSE,0.0914419695193435,"Figure 1: The comparison between the model discrepancy increase factor δl and the communication
cost decrease factor 1 −λl for a) CIFAR-10 and b) CIFAR-100 training."
ELSE,0.09261430246189918,"scalability. When increasing the aggregation interval, Algorithm 2 multiplies a pre-deﬁned small
constant φ to the ﬁxed base interval τ ′ (line 10). This approach ensures that the communication
latency cost is not increased while the network bandwidth consumption is reduced by a factor of φ."
ELSE,0.09378663540445487,"FedAvg can be considered as a special case of FedLAMA where φ is set to 1. When φ > 1, Fed-
LAMA less frequently synchronize a subset of layers, and it results in reducing their communication
costs. When increasing the aggregation interval, FedLAMA multiplies φ to the base interval τ ′. So,
it is guaranteed that the whole model parameters are fully synchronized after φτ ′ iterations. Because
of the layers with the base aggregation interval τ ′, the total model discrepancy of FedLAMA after
φτ ′ iterations is always smaller than that of FedAvg with an aggregation interval of φτ ′."
CONVERGENCE ANALYSIS,0.09495896834701055,"5
CONVERGENCE ANALYSIS"
PRELIMINARIES,0.09613130128956623,"5.1
PRELIMINARIES"
PRELIMINARIES,0.09730363423212192,"Notations – All vectors in this paper are column vectors. x ∈Rd denotes the parameters of one
local model and m is the number of clients. The stochastic gradient computed from a single training
data point ξ is denoted by g(x, ξ). For convenience, we use g(x) instead. The full batch gradient is
denoted by ∇F(x). We use ∥·∥and ∥·∥op to denote l2 norm and matrix operator norm, respectively."
PRELIMINARIES,0.0984759671746776,Assumptions – We analyze the convergence rate of FedLAMA under the following assumptions.
PRELIMINARIES,0.09964830011723329,"1. (Smoothness). Each local objective function is L-smooth, that is, ∥∇Fi(x) −∇Fi(y)∥≤
L∥x −y∥, ∀i ∈{1, · · · , m}.
2. (Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: Eξ[gi(x, ξ)] = ∇Fi(x).
3. (Bounded Variance).
The stochastic gradient at each client has bounded variance:
Eξ[∥gi(x, ξ) −∇Fi(x)∥2 ≤σ2], ∀i ∈{1, · · · , m}, σ2 ≥0.
4. (Bounded Dissimilarity). For any sets of weights {pi ≥0}m
i=1, Pm
i=1 pi = 1, there exist
constants β2 ≥1 and κ2 ≥0 such that Pm
i=1 pi∥∇Fi(x)∥2 ≤β2∥Pm
i=1 pi∇Fi(x)∥2+κ2.
If local objective functions are identical to each other, β2 = 1 and κ2 = 0."
ANALYSIS,0.10082063305978899,"5.2
ANALYSIS"
ANALYSIS,0.10199296600234467,"We begin with showing two key lemmas. All the proofs can be found in Appendix.
Lemma 5.1. (Framework) Under Assumption 1 ∼3, if the learning rate satisﬁes η ≤
1
2L, FedLAMA
ensures"
K,0.10316529894490035,"1
K K
X"
K,0.10433763188745604,"k=1
E
h
∥∇F(uk)∥2i
≤
2
ηK E [F(u1) −F(u∗)] + 2ηLσ2
m
X"
K,0.10550996483001172,"i=1
(pi)2 + L2 K K
X k=1 m
X"
K,0.10668229777256741,"i=1
pi E
huk −xi
k
2i
. (5)"
K,0.10785463071512309,Under review as a conference paper at ICLR 2022
K,0.10902696365767878,"Lemma 5.2. (Model Discrepancy) Under Assumption 1 ∼4, if the learning rate satisﬁes η <
1
2(τmax−1)L, FedLAMA ensures"
K,0.11019929660023446,"1
K K
X k=1 m
X"
K,0.11137162954279015,"i=1
pi E
huk −xi
k
2i
≤2η2(τmax −1)σ2"
K,0.11254396248534584,"1 −A
+
Aκ2"
K,0.11371629542790153,"L2(1 −A) +
Aβ2"
K,0.11488862837045721,"KL2(1 −A) K
X"
K,0.1160609613130129,"k=1
E
h
∥∇F(uk)∥2i
, (6)"
K,0.11723329425556858,where A = 4η2(τmax −1)2L2 and τmax is the largest averaging interval across all the layers.
K,0.11840562719812427,"Based on Lemma 5.1 and 5.2, we analyze the convergence rate of FedLAMA as follows."
K,0.11957796014067995,"Theorem 5.3. Suppose all m local models are initialized to the same point u1.
Under As-
sumption 1
∼
4, if FedLAMA runs for K iterations and the learning rate satisﬁes η
≤"
K,0.12075029308323564,"min

1
2(τmax−1)L,
1
L√"
K,0.12192262602579132,2τmax(τmax−1)(2β2+1)
K,0.123094958968347,"
, FedLAMA ensures E ""
1
K K
X"
K,0.1242672919109027,"i=1
∥∇F(uk)∥2
#"
K,0.1254396248534584,"≤
4
ηK (E [F(u1) −F(u∗)]) + 4η m
X"
K,0.12661195779601406,"i=1
p2
i Lσ2
(7)"
K,0.12778429073856976,"+ 3η2(τmax −1)L2σ2 + 6η2τmax(τmax −1)L2κ2,"
K,0.12895662368112543,where u∗indicates a local minimum and τmax is the largest averaging interval across all the layers.
K,0.13012895662368112,"Remark 1. (Linear Speedup) With a sufﬁciently small diminishing learning rate and a large number
of training iterations, FedLAMA achieves linear speedup. If the learning rate is η =
√m
√"
K,0.13130128956623682,"K and
pi = 1"
K,0.1324736225087925,"m, ∀i ∈{1, · · · , m}, we have E ""
1
K K
X"
K,0.1336459554513482,"i=1
∥∇F(uk)∥2
#"
K,0.13481828839390386,"≤O

1
√ mK"
K,0.13599062133645956,"
+ O
m K 
(8)"
K,0.13716295427901523,"If K > m3, the ﬁrst term on the right-hand side becomes dominant and it achieves linear speedup."
K,0.13833528722157093,"Remark 2. (Impact of Interval Increase Factor φ) The worst-case model discrepancy depends on the
largest averaging interval across all the layers, τmax = φτ ′. The larger the interval increase factor φ,
the larger the model discrepancy terms in (7). In the meantime, as φ increases, the communication
frequency at the selected layers is proportionally reduced. So, φ should be appropriately tuned to
effectively reduce the communication cost while not much increasing the model discrepancy."
EXPERIMENTS,0.1395076201641266,"6
EXPERIMENTS"
EXPERIMENTS,0.1406799531066823,"Experimental Settings – We evaluate FedLAMA using three representative benchmark datasets:
CIFAR-10 (ResNet20 (He et al. (2016))), CIFAR-100 (WideResNet28-10 (Zagoruyko & Komodakis
(2016))), and Federated Extended MNIST (CNN (Caldas et al. (2018))). We use TensorFlow 2.4.3
for local training and MPI for model aggregation. All our experiments are conducted on 4 compute
nodes each of which has 2 NVIDIA v100 GPUs."
EXPERIMENTS,0.141852286049238,"Due to the limited compute resources, we simulate Federated Learning such that each process se-
quentially trains multiple models and then the models are aggregated across all the processes at
once. While it provides the same classiﬁcation results as the actual Federated Learning, the train-
ing time is serialized within each process. Thus, instead of wall-clock time, we consider the total
communication cost calculated as follows. C = L
X"
EXPERIMENTS,0.14302461899179367,"l=1
Cl = L
X"
EXPERIMENTS,0.14419695193434937,"l=1
dim(ul) ∗κl,
(9)"
EXPERIMENTS,0.14536928487690504,where κl is the total number of communications at layer l during the training.
EXPERIMENTS,0.14654161781946073,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.1477139507620164,"Table 1:
(IID data) CIFAR-10 classiﬁcation results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 300."
EXPERIMENTS,0.1488862837045721,"LR
Base aggregation interval: τ ′
Interval increase factor: φ
Validation acc.
Comm. cost
0.8
6
1 (FedAvg)
88.37 ± 0.02%
100%
0.8
12
1 (FedAvg)
84.74 ± 0.05%
50%
0.4
6
2 (FedLAMA)
88.41 ±0.01%
62.33%
0.6
24
1 (FedAvg)
80.34 ± 0.3%
25%
0.6
6
4 (FedLAMA)
86.21 ±0.1%
42.17%"
EXPERIMENTS,0.15005861664712777,"Table 2: (IID data) CIFAR-100 classiﬁcation results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 250."
EXPERIMENTS,0.15123094958968347,"LR
Base aggregation interval: τ ′
Interval increase factor: φ
Validation acc.
Comm. cost
0.6
6
1 (FedAvg)
76.50 ± 0.02%
100%
0.6
12
1 (FedAvg)
66.97 ± 0.9%
50%
0.5
6
2 (FedLAMA)
76.02 ±0.01%
66.01%
0.6
24
1 (FedAvg)
45.01 ± 1.1%
25%
0.5
6
4 (FedLAMA)
76.17 ±0.02%
39.91%"
EXPERIMENTS,0.15240328253223914,"Hyper-Parameter Settings – We use 128 clients in our experiments. The local batch size is set to 32
and the learning rate is tuned based on a grid search. For CIFAR-10 and CIFAR-100, we artiﬁcially
generate heterogeneous data distributions using Dirichlet’s distribution. When using Non-IID data,
we also consider partial device participation such that randomly chosen 25% of the clients participate
in training at every φτ ′ iterations. We report the average accuracy across at least three separate runs."
CLASSIFICATION PERFORMANCE ANALYSIS,0.15357561547479484,"6.1
CLASSIFICATION PERFORMANCE ANALYSIS"
CLASSIFICATION PERFORMANCE ANALYSIS,0.15474794841735054,"To evaluate the proposed model aggregation scheme, we keep all the other factors the same, such as
optimizer, the number of clients, the degree of heterogeneity, and compare the performance across
different model aggregation schemes. We compare the performance across three different model
aggregation settings as follows."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1559202813599062,• Periodic full aggregation with an interval of τ ′
CLASSIFICATION PERFORMANCE ANALYSIS,0.1570926143024619,• Periodic full aggregation with an interval of φτ ′
CLASSIFICATION PERFORMANCE ANALYSIS,0.15826494724501758,• Layer-wise adaptive aggregation with intervals of τ ′ and φ
CLASSIFICATION PERFORMANCE ANALYSIS,0.15943728018757328,"The ﬁrst setting provides the baseline communication cost, and we compare it to the other settings’
communication costs. The third setting is FedLAMA with the base aggregation interval τ ′ and the
interval increase factor φ. Due to the limited space, we present a part of experimental results that
deliver the key insights. More results can be found in Appendix."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16060961313012895,"Experimental Results with IID Data – We ﬁrst present CIFAR-10 and CIFAR-100 classiﬁcation
results under IID data settings. Table 1 and 2 show the CIFAR-10 and CIFAR-100 results, respec-
tively. Note that the learning rate is individually tuned for each setting using a grid search, and
we report the best settings. In both tables, the ﬁrst row shows the performance of FedAvg with
a short interval τ ′ = 6. As the interval increases, FedAvg signiﬁcantly loses the accuracy while
the communication cost is proportionally reduced. FedLAMA achieves a comparable accuracy to
FedAvg with τ ′ = 6 while its communication cost is similar to that of FedAvg with φτ ′. These
results demonstrate that Algorithm 2 effectively ﬁnds the layer-wise interval settings that maximize
the communication cost reduction while minimizing the model discrepancy increase."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16178194607268465,"Experimental Results with Non-IID Data – We now evaluate the performance of FedLAMA using
non-IID data. FEMNIST is inherently heterogeneous such that it contains the hand-written digit
pictures collected from 3, 550 different writers. We use random 10% of the writers’ training samples
in our experiments. Table 3 shows the FEMNIST classiﬁcation results. The base interval τ ′ is set
to 10. FedAvg (φ = 1) signiﬁcantly loses the accuracy as the aggregation interval increases. For
example, when the interval increases from 10 to 40, the accuracy is dropped by 2.1% ∼2.7%.
In contrast, FedLAMA maintains the accuracy when φ increases, while the communication cost
is remarkably reduced. This result demonstrates that FedLAMA effectively ﬁnds the best interval
setting that reduces the communication cost while maintaining the accuracy."
CLASSIFICATION PERFORMANCE ANALYSIS,0.16295427901524032,Under review as a conference paper at ICLR 2022
CLASSIFICATION PERFORMANCE ANALYSIS,0.16412661195779601,"Table 3:
(Non-IID data) FEMNIST classiﬁcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 2, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1652989449003517,"LR
Base aggregation interval: τ ′
Interval increase factor: φ
active ratio
Validation acc.
Comm. cost 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16647127784290738,"10
1 (FedAvg) 25%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16764361078546308,"86.04 ± 0.01%
100%
20
1 (FedAvg)
85.38 ± 0.02%
50%
10
2 (FedLAMA)
86.01 ±0.01%
52.83%
40
1 (FedAvg)
83.97 ± 0.02%
25%
10
4 (FedLAMA)
85.61 ±0.02%
29.97% 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16881594372801875,"10
1 (FedAvg) 50%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.16998827667057445,"86.59 ± 0.01%
100%
20
1 (FedAvg)
85.50 ± 0.02%
50%
10
2 (FedLAMA)
86.07 ±0.02%
53.32%
40
1 (FedAvg)
83.92 ± 0.02%
25%
10
4 (FedLAMA)
85.77 ±0.02%
29.98% 0.04"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17116060961313012,"10
1 (FedAvg) 100%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17233294255568582,"85.74 ± 0.03%
100%
20
1 (FedAvg)
85.08 ± 0.01%
50%
10
2 (FedLAMA)
85.40 ±0.02%
51.86%
40
1 (FedAvg)
83.62 ± 0.02%
25%
10
4 (FedLAMA)
84.67 ±0.02%
29.98%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1735052754982415,"Table 4:
(Non-IID data) CIFAR-10 classiﬁcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1746776084407972,"LR
Base aggregation interval: τ ′
Interval increase factor: φ
active ratio
Dirichlet’s coeff.
Validation acc.
Comm. cost 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17584994138335286,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17702227432590856,"25%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17819460726846426,"84.02 ± 0.1%
100%
24
1 (FedAvg)
76.27 ± 0.08%
25%
6
4 (FedLAMA)
83.06 ±0.1%
39.52% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.17936694021101993,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18053927315357562,"25%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1817116060961313,"87.59 ± 0.2%
100%
24
1 (FedAvg)
83.36 ± 0.4%
25%
6
4 (FedLAMA)
86.57 ±0.02%
42.40% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.182883939038687,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18405627198124266,"100%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18522860492379836,"89.52 ± 0.05%
100%
24
1 (FedAvg)
84.82 ± 0.06%
25%
6
4 (FedLAMA)
87.47 ±0.1%
42.49% 0.8"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18640093786635403,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18757327080890973,"100%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.18874560375146543,"90.53 ± 0.08%
100%
24
1 (FedAvg)
85.68 ± 0.1%
25%
6
4 (FedLAMA)
87.45 ±0.05%
42.73%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1899179366940211,"Table 5: (Non-IID data) CIFAR-100 classiﬁcation results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000."
CLASSIFICATION PERFORMANCE ANALYSIS,0.1910902696365768,"LR
Base aggregation interval: τ ′
Interval increase factor: φ
active ratio
Dirichlet’s coeff.
Validation acc.
Comm. cost 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19226260257913247,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19343493552168817,"25%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19460726846424384,"79.15 ± 0.02%
100%
12
1 (FedAvg)
76.16 ± 0.05%
50%
6
2 (FedLAMA)
78.63 ±0.03%
63.14% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19577960140679954,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1969519343493552,"25%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.1981242672919109,"78.81 ± 0.1%
100%
12
1 (FedAvg)
76.11 ± 0.05%
50%
6
2 (FedLAMA)
77.86 ±0.04%
63.20% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.19929660023446658,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20046893317702227,"100%
0.1"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20164126611957797,"79.77 ± 0.04%
100%
12
1 (FedAvg)
77.71 ± 0.08%
50%
6
2 (FedLAMA)
79.07 ±0.1%
60.48% 0.4"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20281359906213364,"6
1 (FedAvg)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20398593200468934,"100%
0.5"
CLASSIFICATION PERFORMANCE ANALYSIS,0.205158264947245,"80.19 ± 0.05%
100%
12
1 (FedAvg)
77.40 ± 0.06%
50%
6
2 (FedLAMA)
78.88 ±0.05%
61.73%"
CLASSIFICATION PERFORMANCE ANALYSIS,0.2063305978898007,"Table 4 and 5 show the non-IID CIFAR-10 and CIFAR-100 experimental results. We use Dirichlet’s
distribution to generate heterogeneous data across all the clients. The detailed settings regarding
Dirichlet’s distribution can be found in Appendix. The base aggregation interval τ ′ is set to 6. The
interval increase factor φ is set to 2 for FedLAMA. Likely to the IID data experiments, we observe
that the periodic full averaging signiﬁcantly loses the accuracy as the model aggregation interval
increases, while it has a proportionally reduced communication cost. For both datasets, FedLAMA
achieves a comparable accuracy to the periodic full averaging with the interval of τ ′ while having
the communication cost that is close to the periodic full averaging with the increased interval of
φτ ′. Especially, FedLAMA works effectively even when the Dirichlet’s coefﬁcient is set to 0.1.
The coefﬁcient of 0.1 represents an extremely high degree of data heterogeneity in terms of not
only the number of samples per client but also the balance of the classes assigned to each client.
These results imply that FedLAMA is a practical algorithm for Federated Learning applications
with highly heterogeneous data distributions."
CLASSIFICATION PERFORMANCE ANALYSIS,0.20750293083235638,Under review as a conference paper at ICLR 2022
CLASSIFICATION PERFORMANCE ANALYSIS,0.20867526377491208,"1
3
5
7
9
11 13 15 17 19 21 23
0 500 1000 1500"
CLASSIFICATION PERFORMANCE ANALYSIS,0.20984759671746775,"1
2
3
4
0 50 100 150 200"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21101992966002345,"1
3
5
7
9 11 13 15 17 19 21 23 25 27 29
0 500 1000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21219226260257915,"a) CIFAR-10 (ResNet20)
b) CIFAR-100 (WRN28-10)
c) FEMNIST (CNN)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21336459554513482,"layer ID
layer ID
layer ID"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21453692848769051,# of communications
CLASSIFICATION PERFORMANCE ANALYSIS,0.21570926143024619,"FedAvg
FedLAMA"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21688159437280188,"Figure 2: The number of communications at the individual layers. The communications are counted
during the whole training (non-IID data)."
CLASSIFICATION PERFORMANCE ANALYSIS,0.21805392731535755,"1
3
5
7
9 11 13 15 17 19 21 23 25 27 29
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.21922626025791325,1.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.22039859320046892,2.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.22157092614302462,3.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.2227432590855803,4.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.223915592028136,"1
3
5
7
9
11 13 15 17 19 21 23
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.2250879249706917,2.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22626025791324736,4.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22743259085580306,6.00E+007
CLASSIFICATION PERFORMANCE ANALYSIS,0.22860492379835873,"1
2
3
4
0.00E+000"
CLASSIFICATION PERFORMANCE ANALYSIS,0.22977725674091443,5.00E+008
CLASSIFICATION PERFORMANCE ANALYSIS,0.2309495896834701,1.00E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.2321219226260258,1.50E+009
CLASSIFICATION PERFORMANCE ANALYSIS,0.23329425556858147,"a) CIFAR-10 (ResNet20)
b) CIFAR-100 (WRN28-10)
c) FEMNIST (CNN)"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23446658851113716,"layer ID
layer ID
layer ID"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23563892145369286,total data size
CLASSIFICATION PERFORMANCE ANALYSIS,0.23681125439624853,"FedAvg
FedLAMA"
CLASSIFICATION PERFORMANCE ANALYSIS,0.23798358733880423,"Figure 3:
The total data size (communication cost) that correspond to Figure 2. The data size
comparison clearly shows where the performance gain of FedLAMA comes from."
COMMUNICATION EFFICIENCY ANALYSIS,0.2391559202813599,"6.2
COMMUNICATION EFFICIENCY ANALYSIS"
COMMUNICATION EFFICIENCY ANALYSIS,0.2403282532239156,"We analyze the total number of communications and the accumulated data size to evaluate the com-
munication efﬁciency of FedLAMA. Figure 2 shows the total number of communications at the
individual layers. The τ ′ is set to 6 and φ is 2 for FedLAMA. The key insight is that FedLAMA
increases the aggregation interval mostly at the output-side large layers. This means the dl value
shown in Equation (2) at the these layers are smaller than the others. Since these large layers take
up most of the total model parameters, the communication cost is remarkably reduced when their
aggregation intervals are increased. Figure 3 shows the layer-wise local data size shown in Equation
9. FedLAMA shows the signiﬁcantly smaller total data size than FedAvg. The extra computational
cost of FedLAMA is almost negligible since it calculates dl after each communication round only.
Therefore, given the virtually same computational cost, FedLAMA aggregates the local models at a
cheaper communication cost, and thus it improves the scalablity of Federated Learning."
COMMUNICATION EFFICIENCY ANALYSIS,0.24150058616647127,"We found that the amount of the reduced communication cost was not strongly affected by the
degree of data heterogeneity. As shown in Table 4 and 5, the reduced communication cost is similar
across different Dirichlet’s coefﬁcients and device participation ratios. That is, FedLAMA can be
considered as an effective model aggregation scheme regardless of the degree of data heterogeneity."
CONCLUSION,0.24267291910902697,"7
CONCLUSION"
CONCLUSION,0.24384525205158264,"We proposed a layer-wise model aggregation scheme that adaptively adjusts the model aggregation
interval at run-time. Breaking the convention of aggregating the whole model parameters at once,
this novel model aggregation scheme introduces a ﬂexible communication strategy for scalable Fed-
erated Learning. Furthermore, we provide a solid convergence guarantee of FedLAMA under the
assumptions on the non-convex objective functions and the non-IID data distribution. Our empirical
study also demonstrates the efﬁcacy of FedLAMA for scalable and accurate Federated Learning."
CONCLUSION,0.24501758499413834,Under review as a conference paper at ICLR 2022
CONCLUSION,0.246189917936694,"Harmonizing FedLAMA with other advanced optimizers, gradient compression, and low-rank ap-
proximation methods is a promising future work."
CONCLUSION,0.2473622508792497,Under review as a conference paper at ICLR 2022
CODE OF ETHICS,0.2485345838218054,"8
CODE OF ETHICS"
CODE OF ETHICS,0.24970691676436108,"Our work does not deliver potentially harmful insights or conﬂicts of interests. We also do not ﬁnd
any potential inappropriate application or privacy/security issues. The datasets we used in our study
are all public benchmark datasets, and our source code will be opened once the paper is accepted."
REPRODUCIBILITY STATEMENT,0.2508792497069168,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.25205158264947247,"The software versions, implementation details, hyper-parameter settings can be found in the ﬁrst
two paragraphs of Section 6. The entire source code used in our experiments will be published as
an open source once the paper is accepted. We believe one can exactly reproduce our experimental
results following the provided descriptions."
REFERENCES,0.2532239155920281,REFERENCES
REFERENCES,0.2543962485345838,"Alyazeed Albasyoni, Mher Safaryan, Laurent Condat, and Peter Richt´arik. Optimal gradient com-
pression for distributed and federated learning. arXiv preprint arXiv:2010.03246, 2020."
REFERENCES,0.2555685814771395,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
Qsgd:
Communication-efﬁcient sgd via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 30:1709–1720, 2017."
REFERENCES,0.2567409144196952,"Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and C´edric
Renggli. The convergence of sparsiﬁed gradient methods. arXiv preprint arXiv:1809.10505,
2018."
REFERENCES,0.25791324736225085,"Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Freezeout: Accelerate training
by progressively freezing layers. arXiv preprint arXiv:1706.04983, 2017."
REFERENCES,0.25908558030480655,"Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2018."
REFERENCES,0.26025791324736225,"Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921–2926. IEEE, 2017."
REFERENCES,0.26143024618991795,"Enmao Diao, Jie Ding, and Vahid Tarokh. Heteroﬂ: Computation and communication efﬁcient
federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020."
REFERENCES,0.26260257913247365,"Kelam Goutam, S Balasubramanian, Darshan Gera, and R Raghunatha Sarma. Layerout: Freezing
layers in deep neural networks. SN Computer Science, 1(5):1–9, 2020."
REFERENCES,0.2637749120750293,"Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017."
REFERENCES,0.264947245017585,"Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¨olkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63–77. Springer, 2005."
REFERENCES,0.2661195779601407,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck R Cadambe. Lo-
cal sgd with periodic averaging: Tighter analysis and adaptive synchronization. arXiv preprint
arXiv:1910.13598, 2019."
REFERENCES,0.2672919109026964,"Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Pra-
neeth Vepakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and benchmark
for federated machine learning. arXiv preprint arXiv:2007.13518, 2020."
REFERENCES,0.268464243845252,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.2696365767878077,Under review as a conference paper at ICLR 2022
REFERENCES,0.2708089097303634,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020."
REFERENCES,0.2719812426729191,"Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon.
Federated learning: Strategies for improving communication efﬁciency.
arXiv
preprint arXiv:1610.05492, 2016."
REFERENCES,0.2731535756154748,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, pp. 3519–
3529. PMLR, 2019."
REFERENCES,0.27432590855803046,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.27549824150058616,"Adarsh Kumar, Arjun Balasubramanian, Shivaram Venkataraman, and Aditya Akella. Accelerat-
ing deep learning inference via freezing. In 11th {USENIX} Workshop on Hot Topics in Cloud
Computing (HotCloud 19), 2019."
REFERENCES,0.27667057444314186,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018."
REFERENCES,0.27784290738569756,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli-
gence and statistics, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.2790152403282532,"Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. arXiv preprint arXiv:1806.05759, 2018."
REFERENCES,0.2801875732708089,"Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. arXiv preprint
arXiv:1706.05806, 2017."
REFERENCES,0.2813599062133646,"Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efﬁcient federated learning method with periodic averaging and quan-
tization. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 2021–2031.
PMLR, 2020."
REFERENCES,0.2825322391559203,"Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Practical low-rank communication com-
pression in decentralized deep learning. In NeurIPS, 2020."
REFERENCES,0.283704572098476,"Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and Dimitris Pa-
pailiopoulos. Atomo: Communication-efﬁcient learning via atomic sparsiﬁcation. arXiv preprint
arXiv:1806.04090, 2018."
REFERENCES,0.28487690504103164,"Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferﬁsh: Communication-efﬁcient
models at no extra cost. arXiv preprint arXiv:2103.03936, 2021."
REFERENCES,0.28604923798358733,"Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd. arXiv preprint arXiv:1810.08313, 2018."
REFERENCES,0.28722157092614303,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020."
REFERENCES,0.28839390386869873,"Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsiﬁcation for communication-
efﬁcient distributed optimization. arXiv preprint arXiv:1710.09854, 2017."
REFERENCES,0.2895662368112544,"Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
Tern-
grad: Ternary gradients to reduce communication in distributed deep learning. arXiv preprint
arXiv:1705.07878, 2017."
REFERENCES,0.29073856975381007,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019."
REFERENCES,0.29191090269636577,Under review as a conference paper at ICLR 2022
REFERENCES,0.29308323563892147,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.29425556858147717,"Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with
progressive layer dropping. arXiv preprint arXiv:2010.13369, 2020."
REFERENCES,0.2954279015240328,Under review as a conference paper at ICLR 2022
REFERENCES,0.2966002344665885,"A
APPENDIX"
REFERENCES,0.2977725674091442,"A.1
CONVERGENCE ANALYSIS"
REFERENCES,0.2989449003516999,"Herein, we provide the proofs of the lemmas and theorem shown in Section 5."
REFERENCES,0.30011723329425555,"A.1.1
PRELIMINARIES"
REFERENCES,0.30128956623681125,"FedLAMA periodically chooses a few layers that will be less frequently synchronized. We call these
layers Least Critical Layers (LCL) for short."
REFERENCES,0.30246189917936694,"Notations – All vectors in this paper are column vectors. x ∈Rd denotes the parameters of one
local model and m is the number of workers. The stochastic gradient computed from a single
training data point ξ is denoted by g(x, ξ). For convenience, we use g(x) instead. The full batch
gradient is denoted by ∇F(x). We use ∥· ∥and ∥· ∥op to denote l2 norm and matrix operator norm,
respectively."
REFERENCES,0.30363423212192264,"Objective Function – In this paper, we consider federated optimization problems as follows."
REFERENCES,0.3048065650644783,"min
x∈Rd """
REFERENCES,0.305978898007034,"F(x) := m
X"
REFERENCES,0.3071512309495897,"i=1
piFi(x) #"
REFERENCES,0.3083235638921454,",
(10)"
REFERENCES,0.3094958968347011,"where pi = ni/n is the ratio of local data to the total dataset, and Fi(x) =
1
ni
P"
REFERENCES,0.3106682297772567,"ξ∈D fi(x, ξ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size. Note
that, by deﬁnition, Pm
i=1 pi = 1."
REFERENCES,0.3118405627198124,"Averaging Matrix – We deﬁne a time-varying averaging matrix Wk ∈Rdm×dm as follows. Wk = 
 "
REFERENCES,0.3130128956623681,"P,
if k mod τmin is 0
J,
if k mod τmax is 0
I,
otherwise
(11)"
REFERENCES,0.3141852286049238,"I is an identity matrix, P is also a time-varying averaging matrix, and J is a full averaging matrix.
First, P1
i is a d × d diagonal matrix that has 1 for the diagonal elements that correspond to the LCL
parameters and pi for all the other diagonal elements. Likewise, P0
i is another d×d diagonal matrix
that has 0 for the diagonal elements that correspond to the LCL parameters and pi for all the other
diagonal elements. Then, P is deﬁned as follows."
REFERENCES,0.31535756154747946,"P =
P1, for m diagonal blocks
P0, for all the other blocks
(12)"
REFERENCES,0.31652989449003516,"The ith block column of P consists of P1
i and P0
i following the above deﬁnition."
REFERENCES,0.31770222743259086,"Here we present an example of P where m = 2 and d = 2. In this example, p0 = 1/3 and p1 = 2/3.
Saying the LCL is the second parameter, P is deﬁned as follows."
REFERENCES,0.31887456037514655,"P1
0 =
 1"
REFERENCES,0.32004689331770225,"3
0
0
1"
REFERENCES,0.3212192262602579,"
, P0
0 =
 1"
REFERENCES,0.3223915592028136,"3
0
0
0"
REFERENCES,0.3235638921453693,"
, P1
1 =
 2"
REFERENCES,0.324736225087925,"3
0
0
1"
REFERENCES,0.32590855803048063,"
, P0
1 =
 2"
REFERENCES,0.32708089097303633,"3
0
0
0"
REFERENCES,0.32825322391559203,"
(13)"
REFERENCES,0.32942555685814773,"P =

P1
0
P0
1
P0
0
P1
1 
=  "
REFERENCES,0.3305978898007034,"1
3
0
2
3
0
0
1
0
0
1
3
0
2
3
0
0
0
0
1 "
REFERENCES,0.33177022274325907,".
(14)"
REFERENCES,0.33294255568581477,"The full-averaging matrix J is deﬁned as follows. First, Ji is a d × d diagonal matrix that has pi for
the diagonal elements. Then, J consists of m × m blocks of Ji such that each column block is m of
Ji blocks. Here we present an example of J where m = 2 and d = 2 as follows."
REFERENCES,0.33411488862837047,"J0 =
 1"
REFERENCES,0.33528722157092616,"3
0
0
1
3"
REFERENCES,0.3364595545134818,"
, J1 =
 2"
REFERENCES,0.3376318874560375,"3
0
0
2
3"
REFERENCES,0.3388042203985932,"
(15)"
REFERENCES,0.3399765533411489,Under review as a conference paper at ICLR 2022
REFERENCES,0.34114888628370454,"J =

J0
J1
J0
J1 
=  "
REFERENCES,0.34232121922626024,"1
3
0
2
3
0
0
1
3
0
2
3
1
3
0
2
3
0
0
1
3
0
2
3 "
REFERENCES,0.34349355216881594,".
(16)"
REFERENCES,0.34466588511137164,The averaging matrix P and J have the following properties:
REFERENCES,0.34583821805392734,"1. P1dm = 1dm, J1dm = 1dm."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.347010550996483,"2. The product of any two averaging matrices consists only of diagonal block matrices because
all the blocks in P and J are diagonal."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3481828839390387,3. PJ = JP = J regardless of which layers are chosen as the LCL.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3493552168815944,4. PP = P regardless of which layers are chosen as the LCL.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3505275498241501,"Vectorization – We deﬁne a vectorized form of m local model parameters xk ∈Rdm, its stochastic
gradients gk ∈Rdm, and the full gradients fk ∈Rdm as follows"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3516998827667057,"xk = vec

x1
k, x2
k, · · · , xm
k"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3528722157092614,"gk = vec

g1(x1
k), g2(x2
k), · · · , gm(xm
k )"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3540445486518171,"fk = vec

∇F1(x1
k), ∇F2(x2
k), · · · , ∇Fm(xm
k )
	
. (17)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3552168815943728,"The full model aggregation can be written using the vectorized form of local models xk and the
averaging matrix J as follows. Jxk =  "
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3563892145369285,"1
3
0
2
3
0
0
1
3
0
2
3
1
3
0
2
3
0
0
1
3
0
2
3    "
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35756154747948415,"x(1,1)
k
x(1,2)
k
x(2,1)
k
x(2,2)
k  =  "
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35873388042203985,"(x(1,1)
k
+ 2x(2,1)
k
)/3
(x(1,2)
k
+ 2x(2,2)
k
)/3
(x(1,1)
k
+ 2x(2,1)
k
)/3
(x(1,2)
k
+ 2x(2,2)
k
)/3 "
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.35990621336459555,"
(18)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.36107854630715125,"where x(i,j)
k
is the jth model parameter of local model i at iteration k."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3622508792497069,"We also deﬁne the following additional vectorized forms of the weighted model parameters and
gradients for convenience."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3634232121922626,"ˆxk = vec
√p1x1
k, √p2x2
k, · · · , √pmxm
k"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3645955451348183,"ˆgk = vec
√p1g1(x1
k), √p2g2(x2
k), · · · , √pmgm(xm
k )"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.365767878077374,"ˆfk = vec
√p1∇F1(x1
k), √p2∇F2(x2
k), · · · , √pm∇Fm(xm
k )
	
(19)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3669402110199297,Assumptions – We analyze the convergence rate of FedLAMA under the following assumptions.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.36811254396248533,"1. (Smoothness). Each local objective function is L-smooth, that is, ∥∇Fi(x) −∇Fi(y)∥≤
L∥x −y∥, ∀i ∈{1, · · · , m}."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.369284876905041,"2. (Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: Eξ[gi(x, ξ)] = ∇Fi(x)."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3704572098475967,"3. (Bounded Variance).
The stochastic gradient at each client has bounded variance:
Eξ[∥gi(x, ξ) −∇Fi(x)∥2 ≤σ2], ∀i ∈{1, · · · , m}, σ2 ≥0."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3716295427901524,"4. (Bounded Dissimilarity). For any sets of weights {pi ≥0}m
i=1, Pm
i=1 pi = 1, there exist
constants β2 ≥1 and κ2 ≥0 such that Pm
i=1 pi∥∇Fi(x)∥2 ≤β2∥Pm
i=1 pi∇Fi(x)∥2+κ2.
If local objective functions are identical to each other, β2 = 1 and κ2 = 0."
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37280187573270807,Under review as a conference paper at ICLR 2022
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37397420867526376,"A.1.2
PROOFS"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37514654161781946,"Theorem 5.1.
Suppose all m local models are initialized to the same point u1.
Under As-
sumption 1
∼
4, if FedLAMA runs for K iterations and the learning rate satisﬁes η
≤"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37631887456037516,"min

1
2(τmax−1)L,
1
L√"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.37749120750293086,2τmax(τmax−1)(2β2+1)
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3786635404454865,"
, FedLAMA ensures E ""
1
K K
X"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3798358733880422,"i=1
∥∇F(uk)∥2
#"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3810082063305979,"≤
4
ηK (E [F(u1) −F(u∗)]) + 4η m
X"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.3821805392731536,"i=1
p2
i Lσ2"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38335287221570924,"+ 3η2(τmax −1)L2σ2 + 6η2τmax(τmax −1)L2κ2,
(20)"
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38452520515826494,where u∗indicates a local minimum.
THE PRODUCT OF ANY TWO AVERAGING MATRICES CONSISTS ONLY OF DIAGONAL BLOCK MATRICES BECAUSE,0.38569753810082064,"Proof. Based on Lemma 5.1 and 5.2, we have"
K,0.38686987104337633,"1
K K
X"
K,0.388042203985932,"k=1
E
h
∥∇F(uk)∥2i
≤
2
ηK (E [F(u1) −F(u∗)]) + 2η m
X"
K,0.3892145369284877,"i=1
p2
i Lσ2"
K,0.3903868698710434,"+ L2
 
η2(τmax −1)σ2
j
1 −A
+
Aβ2"
K,0.39155920281359907,"KL2(1 −A) K
X"
K,0.39273153575615477,"k=1
E
h
∥∇F(uk)∥2i
+
Aκ2"
K,0.3939038686987104,L2(1 −A) ! .
K,0.3950762016412661,"After re-writing the left-hand side and a minor rearrangement, we have"
K,0.3962485345838218,"1
K K
X"
K,0.3974208675263775,"k=1
E
h
∥∇F(uk)∥2i
≤
2
ηK (E [F(u1) −F(u∗)]) + 2η m
X"
K,0.39859320046893315,"i=1
p2
i Lσ2 + 1 K K
X k=1 Aβ2"
K,0.39976553341148885,"1 −A E
h
∥∇F(uk)∥2i"
K,0.40093786635404455,"+ L2
η2(τmax −1)σ2"
K,0.40211019929660025,"1 −A
+
Aκ2"
K,0.40328253223915594,"L2(1 −A) 
."
K,0.4044548651817116,"By moving the third term on the right-hand side to the left-hand side, we have"
K,0.4056271981242673,"1
K K
X k=1"
K,0.406799531066823,"
1 −Aβ2 1 −A "
K,0.4079718640093787,"E
h
∥∇jF(uk)∥2i
≤
2
ηK (E [F(u1) −F(u∗)]) + 2η m
X"
K,0.4091441969519343,"i=1
p2
i Lσ2"
K,0.41031652989449,"+ L2
η2(τmax −1)σ2"
K,0.4114888628370457,"1 −A
+
Aκ2"
K,0.4126611957796014,L2(1 −A)
K,0.4138335287221571,"
.
(21)"
K,0.41500586166471276,"If A ≤
1
2β2+1, then Aβ2"
K,0.41617819460726846,1−A ≤1
K,0.41735052754982416,"2. Therefore, (21) can be simpliﬁed as follows."
K,0.41852286049237986,"1
K K
X"
K,0.4196951934349355,"k=1
E
h
∥∇F(uk)∥2i
≤
4
ηK (E [F(u1) −F(u∗)]) + 4η m
X"
K,0.4208675263774912,"i=1
p2
i Lσ2
(22)"
K,0.4220398593200469,"+ 2L2
η2(τmax −1)σ2 1 −A"
K,0.4232121922626026,"
+ 2 Aκ2 1 −A."
K,0.4243845252051583,"The learning rate condition A ≤
1
2β2+1 also ensures that
1
1−A ≤1 +
1
2β2 . Based on Assumption 4,
1
2β2 ≤2"
K,0.42555685814771393,"3, and thus
1
1−A ≤2"
K,0.42672919109026963,"3. Therefore, we have"
K,0.42790152403282533,"1
K K
X"
K,0.42907385697538103,"k=1
E
h
∥∇F(uk)∥2i
≤
4
ηK (E [F(u1) −F(u∗)]) + 4η m
X"
K,0.43024618991793667,"i=1
p2
i Lσ2"
K,0.43141852286049237,+ 3η2(τmax −1)L2σ2 + 6η2τmax(τmax −1)L2κ2.
K,0.43259085580304807,We complete the proof.
K,0.43376318874560377,Under review as a conference paper at ICLR 2022
K,0.4349355216881594,"Learning Rate Constraints – In Theorem 5.3, we have two learning rate constraints, one from (22)
and the other from (51) as follows."
K,0.4361078546307151,"A <
1
2β2 + 1
from (22)"
K,0.4372801875732708,"A < 1
from (51)
After a minor rearrangement, we have a uniﬁed learning rate constraint as follows."
K,0.4384525205158265,η ≤min
K,0.4396248534583822,"(
1
2(τmax −1)L,
1 L
p"
K,0.44079718640093785,2τmax(τmax −1)(2β2 + 1) )
K,0.44196951934349354,"Lemma 5.1. (Framework) Under Assumption 1 ∼3, if the learning rate satisﬁes η ≤
1
2L, FedLAMA
ensures
1
K K
X"
K,0.44314185228604924,"k=1
E
h
∥∇F(uk)∥2i
≤
2
ηK E [F(u1) −F(u∗)] + 2ηLσ2
m
X"
K,0.44431418522860494,"i=1
(pi)2 + L2 K K
X k=1 m
X"
K,0.4454865181711606,"i=1
pi E
huk −xi
k
2i
. (23)"
K,0.4466588511137163,"Proof. Based on Assumption 1, we have"
K,0.447831184056272,"E [F(uk+1) −F(uk)] ≤−ηE """
K,0.4490035169988277,"⟨∇F(uk), m
X"
K,0.4501758499413834,"i=1
pigi(xi
k)⟩ #"
K,0.451348182883939,"|
{z
}
T1 + η2L"
E,0.4525205158264947,"2 E    m
X"
E,0.4536928487690504,"i=1
pigi(xi
k)  2 "
E,0.4548651817116061,"|
{z
}
T2 (24)"
E,0.45603751465416176,"First, T1 can be rewritten as follows."
E,0.45720984759671746,"T1 = E """
E,0.45838218053927315,"⟨∇F(uk), m
X"
E,0.45955451348182885,"i=1
pi
 
gi(xi
k) −∇Fi(xi
k)

⟩ # + E """
E,0.46072684642438455,"⟨∇F(uk), m
X"
E,0.4618991793669402,"i=1
pi∇Fi(xi
k)⟩ # = E """
E,0.4630715123094959,"⟨∇F(uk), m
X"
E,0.4642438452520516,"i=1
pi∇Fi(xi
k)⟩ # = 1"
E,0.4654161781946073,2 ∥∇F(uk)∥2 + 1
E,0.46658851113716293,"2 E    m
X"
E,0.46776084407971863,"i=1
pi∇Fi(xi
k)  2 −1"
E,0.46893317702227433,2 E  
E,0.47010550996483,"∇F(uk) − m
X"
E,0.4712778429073857,"i=1
pi∇Fi(xi
k)  2 ,"
E,0.47245017584994137,"(25)
where the last equality holds based on a basic equality: 2a⊤b = ∥a∥2 + ∥b∥2 −∥a −b∥2 ."
E,0.47362250879249707,"Then, T2 can be bounded as follows."
E,0.47479484173505276,"T2 = E    m
X"
E,0.47596717467760846,"i=1
pi
 
gi(xi
k) −E

gi(xi
k)

+ m
X"
E,0.4771395076201641,"i=1
pi E

gi(xi
k)
 2  = E    m
X"
E,0.4783118405627198,"i=1
pi
 
gi(xi
k) −∇Fi(xi
k)

+ m
X"
E,0.4794841735052755,"i=1
pi∇Fi(xi
k)  2  ≤2 E    m
X"
E,0.4806565064478312,"i=1
pi
 
gi(xi
k) −∇Fi(xi
k)
 2"
E,0.48182883939038684,"+ 2 E    m
X"
E,0.48300117233294254,"i=1
pi∇Fi(xi
k)  2  = 2 m
X"
E,0.48417350527549824,"i=1
p2
i E
hgi(xi
k) −∇Fi(xi
k)
2i
+ 2 E    m
X"
E,0.48534583821805394,"i=1
pi∇Fi(xi
k)  2 "
E,0.48651817116060964,"≤2σ2
m
X"
E,0.4876905041031653,"i=1
p2
i + 2 E    m
X"
E,0.488862837045721,"i=1
pi∇Fi(xi
k)  2"
E,0.4900351699882767,",
(26)"
E,0.4912075029308324,Under review as a conference paper at ICLR 2022
E,0.492379835873388,"where the last equality holds because gi(xi
k) −∇Fi(xi
k) has 0 mean and is independent across i,
and the last inequality follows Assumption 3."
E,0.4935521688159437,"By plugging in (25) and (26) into (24), we have the following."
E,0.4947245017584994,E [F(uk+1) −F(uk)] ≤−η
E,0.4958968347010551,2 ∥∇F(uk)∥2 −η
E,0.4970691676436108,"2 E    m
X"
E,0.49824150058616645,"i=1
pi∇Fi(xi
k)  2  + η"
E,0.49941383352872215,2 E  
E,0.5005861664712778,"∇F(uk) − m
X"
E,0.5017584994138335,"i=1
pi∇Fi(xi
k)  2"
E,0.5029308323563892,"+ η2Lσ2
m
X"
E,0.5041031652989449,"i=1
p2
i"
E,0.5052754982415005,"+ η2L E    m
X"
E,0.5064478311840562,"i=1
pi∇Fi(xi
k)  2  = −η"
E,0.5076201641266119,2 ∥∇F(uk)∥2 −η
E,0.5087924970691676,"2(1 −2ηL) E    m
X"
E,0.5099648300117233,"i=1
pi∇Fi(xi
k)  2  + η"
E,0.511137162954279,2 E  
E,0.5123094958968347,"∇F(uk) − m
X"
E,0.5134818288393904,"i=1
pi∇Fi(xi
k)  2"
E,0.5146541617819461,"+ η2Lσ2
m
X"
E,0.5158264947245017,"i=1
p2
i"
E,0.5169988276670574,"If η ≤
1
2L, it follows"
E,0.5181711606096131,"E [F(uk+1) −F(uk)] η
≤−1"
E,0.5193434935521688,"2 ∥∇F(uk)∥2 + ηLσ2
m
X"
E,0.5205158264947245,"i=1
p2
i + 1"
E,0.5216881594372802,2 E  
E,0.5228604923798359,"∇F(uk) − m
X"
E,0.5240328253223916,"i=1
pi∇Fi(xi
k)  2  ≤−1"
E,0.5252051582649473,"2 ∥∇F(uk)∥2 + ηLσ2
m
X"
E,0.5263774912075029,"i=1
p2
i
(27) + 1 2 m
X"
E,0.5275498241500586,"i=1
pi E
h∇Fi(uk) −∇Fi(xi
k)
2i ≤−1"
E,0.5287221570926143,"2 ∥∇F(uk)∥2 + ηLσ2
m
X"
E,0.52989449003517,"i=1
p2
i + L2 2 m
X"
E,0.5310668229777257,"i=1
pi E
huk −xi
k
2i
,"
E,0.5322391559202814,where (27) holds based on the convexity of ℓ2 norm and Jensen’s inequality.
E,0.5334114888628371,"By taking expectation and averaging across K iterations, we have."
K,0.5345838218053928,"1
K K
X k=1"
K,0.5357561547479485,"E [F(uk+1) −F(uk)] η
≤−1"
K,0.536928487690504,"2K K
X"
K,0.5381008206330598,"k=1
∥∇F(uk)∥2 + ηLσ2
m
X"
K,0.5392731535756154,"i=1
p2
i + L2"
K,0.5404454865181711,"2K K
X k−1 m
X"
K,0.5416178194607268,"i=1
pi E
huk −xi
k
2i
."
K,0.5427901524032825,Under review as a conference paper at ICLR 2022
K,0.5439624853458382,"After a minor rearrangement, we have a telescoping sum as follows."
K,0.5451348182883939,"1
K K
X"
K,0.5463071512309496,"k=1
E
h
∥∇F(uk)∥2i
≤
2
ηK E [F(u1) −F(uk+1)] + 2ηLσ2
m
X"
K,0.5474794841735052,"i=1
p2
i + L2 K K
X k=1 m
X"
K,0.5486518171160609,"i=1
pi E
huk −xi
k
2i"
K,0.5498241500586166,"≤
2
ηK E [F(u1) −F(u∗)] + 2ηLσ2
m
X"
K,0.5509964830011723,"i=1
p2
i + L2 K K
X k=1 m
X"
K,0.552168815943728,"i=1
pi E
huk −xi
k
2i
,"
K,0.5533411488862837,"where u∗indicates the local minimum. Here, we complete the proof."
K,0.5545134818288394,"Lemma 5.2. (Model Discrepancy) Under Assumption 1 ∼4, if the learning rate satisﬁes η <
1
2(τmax−1)L, FedLAMA ensures"
K,0.5556858147713951,"1
K K
X k=1 m
X"
K,0.5568581477139508,"i=1
pi E
huk −xi
k
2i
≤2η2(τmax −1)σ2"
K,0.5580304806565064,"1 −A
+
Aκ2"
K,0.5592028135990621,"L2(1 −A) +
Aβ2"
K,0.5603751465416178,"KL2(1 −A) K
X"
K,0.5615474794841735,"k=1
E
h
∥∇F(uk)∥2i
, (28)"
K,0.5627198124267292,where A = 4η2(τmax −1)2L2 and τmax is the largest averaging interval across all the layers.
K,0.5638921453692849,"Proof. We begin with rewriting the weighted average of the squared distance using the vectorized
form of the local models as follows. m
X"
K,0.5650644783118406,"i=1
pi
uk −xi
k
2 = m
X i=1"
K,0.5662368112543963,"√pi
 
uk −xi
k
2"
K,0.567409144196952,"= ∥Jˆxk −ˆxk∥2
(29)"
K,0.5685814771395076,"= ∥(J −I)ˆxk∥2 ,"
K,0.5697538100820633,where (29) holds by the commutative property of multiplication.
K,0.570926143024619,"Then, according to the parameter update rule, we have"
K,0.5720984759671747,"(J −I)ˆxk = (J −I)Wk−1(ˆxk−1 −ηˆgk−1)
= (J −I)Wk−1ˆxk−1 −(J −Wk−1)ηˆgk−1,
(30)"
K,0.5732708089097304,"where (30) holds because JW = J based on the averaging matrix property 3, and IW = W."
K,0.5744431418522861,"Then, expanding the expression of xk−1, we have"
K,0.5756154747948418,"(J −I)ˆxk = (J −I)Wk−1(Wk−2(ˆxk−2 −ηˆgk−2)) −(J −Wk−1)ηˆgk−1
= (J −I)Wk−1Wk−2ˆxk−2 −(J −Wk−1Wk−2)ηˆgk−2 −(J −Wk−1)ηˆgk−1."
K,0.5767878077373975,"Repeating the same procedure for ˆxk−2, ˆxk−3, · · · , ˆx2, we have"
K,0.5779601406799532,"(J −I)ˆxk = (J −I) k−1
Y"
K,0.5791324736225087,"s=1
Wsˆx1 −η k−1
X"
K,0.5803048065650644,"s=1
(J − k−1
Y"
K,0.5814771395076201,"l=s
Wl)ˆgs = −η k−1
X"
K,0.5826494724501758,"s=1
(J − k−1
Y"
K,0.5838218053927315,"l=s
Wl)ˆgs,
(31)"
K,0.5849941383352872,"where (31) holds because xi
1 is the same across all the workers and thus (J −I)ˆx1 = 0."
K,0.5861664712778429,Under review as a conference paper at ICLR 2022
K,0.5873388042203986,"Based on (31), we have"
K,0.5885111371629543,"1
K K
X k=1 m
X"
K,0.5896834701055099,"i=1
pi E
huk −xi
k
2i = 1 K K
X k=1"
K,0.5908558030480656,"
E
h
∥(J −I)ˆxk∥2i = 1 K K
X k=1  η2 E    k−1
X"
K,0.5920281359906213,"s=1
(J − k−1
Y"
K,0.593200468933177,"l=s
Wl)ˆgs  2    = 1 K K
X k=1  η2 E    k−1
X"
K,0.5943728018757327,"s=1
(J − k−1
Y"
K,0.5955451348182884,"l=s
Wl)(ˆgs −ˆfs) + k−1
X"
K,0.5967174677608441,"s=1
(J − k−1
Y"
K,0.5978898007033998,"l=s
Wl)ˆfs  2    ≤2η2 K "
K,0.5990621336459554,"




 K
X k=1
E    k−1
X"
K,0.6002344665885111,"s=1
(J − k−1
Y"
K,0.6014067995310668,"l=s
Wl)(ˆgs −ˆfs)  2 "
K,0.6025791324736225,"|
{z
}
T3 + K
X k=1
E    k−1
X"
K,0.6037514654161782,"s=1
(J − k−1
Y"
K,0.6049237983587339,"l=s
Wl)ˆfs  2 "
K,0.6060961313012896,"|
{z
}
T4 "
K,0.6072684642438453,"




 (32)"
K,0.608440797186401,"where (32) holds based on the convexity of ℓ2 norm and Jensen’s inequality. Now, we focus on
bounding T3 and T4, separately."
K,0.6096131301289566,"Bounding T3 K
X k=1
E    k−1
X"
K,0.6107854630715123,"s=1
(J − k−1
Y"
K,0.611957796014068,"l=s
Wl)(ˆgs −ˆfs)  2  = K
X k=1 k−1
X s=1
E   (J − k−1
Y"
K,0.6131301289566237,"l=s
Wl)(ˆgs −ˆfs)  2"
K,0.6143024618991794,"
(33) ≤ K
X k=1 k−1
X s=1
E "
K,0.6154747948417351,"
(ˆgs −ˆfs)

2
(J − k−1
Y"
K,0.6166471277842908,"l=s
Wl)  2 op "
K,0.6178194607268465,",
(34)"
K,0.6189917936694022,"where (33) holds because ˆgs −ˆfs has 0 mean and independent across s, and (34) holds based on
Lemma A.1."
K,0.6201641266119577,Under review as a conference paper at ICLR 2022
K,0.6213364595545134,"Without loss of generality, we replace k with aτmax + b, where a is the communication round index
and b is the iteration index within each communication round. Then, we have"
K,0.6225087924970691,"K/τmax−1
X a=0"
K,0.6236811254396248,"τmax
X b=1"
K,0.6248534583821805,"aτmax+b−1
X s=1
E "
K,0.6260257913247362,"
(ˆgs −ˆfs)

2
(J − k−1
Y"
K,0.6271981242672919,"l=s
Wl)  2 op   ="
K,0.6283704572098476,"K/τmax−1
X a=0"
K,0.6295427901524033,"τmax
X b=1 aτ
X s=1
E "
K,0.6307151230949589,"
(ˆgs −ˆfs)

2
(J −"
K,0.6318874560375146,"aτmax+b−1
Y"
K,0.6330597889800703,"l=s
Wl)  2 op   +"
K,0.634232121922626,"K/τmax−1
X a=0"
K,0.6354044548651817,"τmax
X b=1"
K,0.6365767878077374,"aτmax+b−1
X"
K,0.6377491207502931,"s=aτmax+1
E "
K,0.6389214536928488,"
(ˆgs −ˆfs)

2
(J −"
K,0.6400937866354045,"aτmax+b−1
Y"
K,0.6412661195779601,"l=s
Wl)  2 op   ="
K,0.6424384525205158,"K/τmax−1
X a=0"
K,0.6436107854630715,"τmax
X b=1"
K,0.6447831184056272,"aτmax+b−1
X"
K,0.6459554513481829,"s=aτmax+1
E "
K,0.6471277842907386,"
(ˆgs −ˆfs)

2
(J −"
K,0.6483001172332943,"aτmax+b−1
Y"
K,0.64947245017585,"l=s
Wl)  2 op "
K,0.6506447831184057,"
(35) ="
K,0.6518171160609613,"K/τmax−1
X a=0"
K,0.652989449003517,"τmax
X b=1"
K,0.6541617819460727,"aτ+b−1
X"
K,0.6553341148886284,"s=aτmax+1
E"
K,0.6565064478311841,"(ˆgs −ˆfs)

2
(36) ="
K,0.6576787807737398,"K/τmax−1
X a=0"
K,0.6588511137162955,"τmax
X b=1"
K,0.6600234466588512,"aτmax+b−1
X"
K,0.6611957796014069,"s=aτmax+1 m
X"
K,0.6623681125439624,"i=1
pi E
h(gi(xi
s) −∇Fi(xi
s))
2i ≤"
K,0.6635404454865181,"K/τmax−1
X a=0"
K,0.6647127784290738,"τmax
X b=1"
K,0.6658851113716295,"aτmax+b−1
X"
K,0.6670574443141852,"s=aτmax+1 m
X"
K,0.6682297772567409,"i=1
piσ2
(37) ="
K,0.6694021101992966,"K/τmax−1
X a=0"
K,0.6705744431418523,"τmax
X"
K,0.671746776084408,"b=1
(b −1)σ2 ="
K,0.6729191090269636,"K/τmax−1
X a=0"
K,0.6740914419695193,"τmax(τmax −1) 2
σ2"
K,0.675263774912075,≤K (τmax −1)
K,0.6764361078546307,"2
σ2.
(38)"
K,0.6776084407971864,"Remember FedLAMA synchronizes the whole parameters at least once after every τmax iterations.
Thus, (35) holds because Qaτmax+b−1
l=s
Wl is J when s ≤aτmax, and thus J −Qaτmax+b−1
l=s
Wl
becomes 0. (36) holds based on Lemma A.2. (37) holds based on Assumption 3."
K,0.6787807737397421,Bounding T4
K,0.6799531066822978,Under review as a conference paper at ICLR 2022
K,0.6811254396248535,"K−τmax
X k=1
E    k−1
X"
K,0.6822977725674091,"s=1
(J − k−1
Y"
K,0.6834701055099648,"l=s
Wl)ˆfs  2  ="
K,0.6846424384525205,"K/τmax−1
X a=0"
K,0.6858147713950762,"τmax
X b=1
E   "
K,0.6869871043376319,"aτ+b−1
X"
K,0.6881594372801876,"s=1
(J −"
K,0.6893317702227433,"aτmax+b−1
Y"
K,0.690504103165299,"l=s
Wl)ˆfs  2  ="
K,0.6916764361078547,"K/τmax−1
X a=0"
K,0.6928487690504103,"τmax
X b=1
E   "
K,0.694021101992966,"aτmax+b−1
X"
K,0.6951934349355217,"s=aτmax+1
(J −"
K,0.6963657678780774,"aτmax+b−1
Y"
K,0.6975381008206331,"l=s
Pl)ˆfs  2"
K,0.6987104337631888,"
(39) ≤"
K,0.6998827667057445,"K/τmax−1
X a=0"
K,0.7010550996483002,"τmax
X b=1 "
K,0.7022274325908558,(b −1)
K,0.7033997655334114,"aτmax+b−1
X"
K,0.7045720984759671,"s=aτmax+1
E   (J −"
K,0.7057444314185228,"aτmax+b−1
Y"
K,0.7069167643610785,"l=s
Pl)ˆfs  2  "
K,0.7080890973036342,"
(40) ≤"
K,0.7092614302461899,"K/τmax−1
X a=0"
K,0.7104337631887456,"τmax
X b=1 "
K,0.7116060961313013,(b −1)
K,0.712778429073857,"aτmax+b−1
X"
K,0.7139507620164126,"s=aτmax+1
E "
K,0.7151230949589683,"
ˆfs

2
(J −"
K,0.716295427901524,"aτmax+b−1
Y"
K,0.7174677608440797,"l=s
Pl)  2 op   "
K,0.7186400937866354,"
(41) ≤"
K,0.7198124267291911,"K/τmax−1
X a=0"
K,0.7209847596717468,"τmax
X b=1 "
K,0.7221570926143025,(b −1)
K,0.7233294255568582,"aτmax+b−1
X"
K,0.7245017584994138,"s=aτmax+1
E"
K,0.7256740914419695,"ˆfs

2! (42)"
K,0.7268464243845252,≤τmax(τmax −1) 2
K,0.7280187573270809,"K/τmax−1
X a=0"
K,0.7291910902696366,"aτmax+τmax−1
X"
K,0.7303634232121923,"s=aτmax+1
E"
K,0.731535756154748,"ˆfs

2!"
K,0.7327080890973037,"≤τmax(τmax −1) 2 K
X k=1
E"
K,0.7338804220398594,"ˆfk

2"
K,0.735052754982415,"= τmax(τmax −1) 2 K
X k=1 m
X"
K,0.7362250879249707,"i=1
pi E
h∇Fi(xi
k)
2i
,
(43)"
K,0.7373974208675264,"where (39) holds because J −Qaτmax+b−1
l=s
Pl becomes 0 when s ≤aτ. (40) holds based on the
convexity of ℓ2 norm and Jensen’s inequality. (41) holds based on Lemma A.1. (42) holds based on
Lemma A.2."
K,0.738569753810082,Final Result
K,0.7397420867526378,"By plugging in (38) and (43) into (32), we have"
K,0.7409144196951934,"1
K K
X k=1 m
X"
K,0.7420867526377491,"i=1
pi E
huk −xi
k
2i ≤2η2 K "
K,0.7432590855803048,K (τmax −1)
K,0.7444314185228605,"2
σ2 + τmax(τmax −1) 2 K
X k=1 m
X"
K,0.7456037514654161,"i=1
pi E
h∇Fi(xi
k)
2i!!"
K,0.7467760844079718,"= η2(τmax −1)σ2 + η2τmax(τmax −1) K K
X k=1 m
X"
K,0.7479484173505275,"i=1
pi E
h∇Fi(xi
k)
2i! (44)"
K,0.7491207502930832,The local gradient term on the right-hand side in (44) can be rewritten using the following inequality.
K,0.7502930832356389,"E
h∇Fi(xi
k)
2i
= E
h∇Fi(xi
k) −∇Fi(uk) + ∇Fi(uk)
2i"
K,0.7514654161781946,"≤2 E
h∇Fi(xi
k) −∇Fi(uk)
2i
+ 2 E
h
∥∇Fi(uk)∥2i
(45)"
K,0.7526377491207503,"≤2L2 E
huk −xi
k
2i
+ 2 E
h
∥∇Fi(uk)∥2i
,
(46)"
K,0.753810082063306,Under review as a conference paper at ICLR 2022
K,0.7549824150058617,where (45) holds based on the convexity of ℓ2 norm and Jensen’s inequality.
K,0.7561547479484173,"Plugging in (46) into (44), we have"
K,0.757327080890973,"1
K K
X k=1 m
X"
K,0.7584994138335287,"i=1
pi E
huk −xi
k
2i"
K,0.7596717467760844,"≤η2(τmax −1)σ2 + 2η2τmax(τmax −1)L2 K K
X k=1 m
X"
K,0.7608440797186401,"i=1
pi E
huk −xi
k
2i"
K,0.7620164126611958,"+ 2η2τmax(τmax −1) K K
X k=1 m
X"
K,0.7631887456037515,"i=1
pi E
h
∥∇Fi(uk)∥2i
(47)"
K,0.7643610785463072,"After a minor rearranging, we have"
K,0.7655334114888629,"1
K K
X k=1 m
X"
K,0.7667057444314185,"i=1
pi E
huk −xi
k
2i
≤
η2(τmax −1)σ2"
K,0.7678780773739742,1 −2η2τmax(τmax −1)L2
K,0.7690504103165299,"+
2η2τmax(τmax −1)
K(1 −2η2τmax(τmax −1)L2) K
X k=1 m
X"
K,0.7702227432590856,"i=1
pi E
h
∥∇Fi(uk)∥2i (48)"
K,0.7713950762016413,Let us deﬁne A = 2η2τmax(τmax −1)L2. Then (48) is simpliﬁed as follows.
K,0.772567409144197,"1
K K
X k=1 m
X"
K,0.7737397420867527,"i=1
pi E
huk −xi
k
2i"
K,0.7749120750293084,≤η2(τmax −1)σ2
K,0.776084407971864,"1 −A
+
A
KL2(1 −A) K
X k=1 m
X"
K,0.7772567409144197,"i=1
pi E
h
∥∇Fi(uk)∥2i"
K,0.7784290738569754,"Based on Assumption 4, we have"
K,0.779601406799531,"1
K K
X k=1 m
X"
K,0.7807737397420867,"i=1
pi E
huk −xi
k
2i"
K,0.7819460726846424,≤η2(τmax −1)σ2
K,0.7831184056271981,"1 −A
+
Aβ2"
K,0.7842907385697538,"KL2(1 −A) K
X k=1
E    m
X"
K,0.7854630715123095,"i=1
pi∇Fi(uk)  2"
K,0.7866354044548651,"+
Aκ2"
K,0.7878077373974208,"L2(1 −A)
(49)"
K,0.7889800703399765,= η2(τmax −1)σ2
K,0.7901524032825322,"1 −A
+
Aβ2"
K,0.7913247362250879,"KL2(1 −A) K
X"
K,0.7924970691676436,"k=1
E
h
∥∇F(uk)∥2i
+
Aκ2"
K,0.7936694021101993,"L2(1 −A),
(50)"
K,0.794841735052755,where (50) holds based on the deﬁnition of the objective function (10).
K,0.7960140679953107,"Note that (49) is true only when 1 −A > 0. Thus, after a minor rearrangement, we have a learning
rate constraint as follows."
K,0.7971864009378663,"η <
1
2(τmax −1)L
(51)"
K,0.798358733880422,"Here, we complete the proof."
K,0.7995310668229777,"A.1.3
PROOF OF OTHER LEMMAS"
K,0.8007033997655334,"Lemma A.1. Consider a real matrix A ∈Rmdj×mdj and a real vector b ∈Rmdj. If b ̸= 0mdj,
we have
∥Ab∥≤∥A∥op∥b∥
(52)"
K,0.8018757327080891,Under review as a conference paper at ICLR 2022
K,0.8030480656506448,Proof.
K,0.8042203985932005,∥Ab∥2 = ∥Ab∥2
K,0.8053927315357562,∥b∥2 ∥b∥2
K,0.8065650644783119,"≤∥A∥2
op∥b∥2
(53)"
K,0.8077373974208675,where (53) holds based on the deﬁnition of operator norm.
K,0.8089097303634232,"Lemma A.2. Suppose an md × md averaging matrix P and the full-averaging matrix J, then"
K,0.8100820633059789,"∥J −P∥2
op = 1.
(54)"
K,0.8112543962485346,regardless of which layers are chosen as the LCL.
K,0.8124267291910903,"Proof. First, by the deﬁnition of averaging matrix P, all the columns that do not correspond to the
LCL are zeroed out in J −P. Then, based on the averaging matrix property 1 and 2, the remaining
columns in P has 1 at all different rows. By the deﬁnition of J, all the non-zero elements in ith
column are the same pi, i ∈{1, · · · , m}. Consequently, the remaining columns in J −P are
always orthogonal regardless of which layers are chosen as the LCL, and thus the eigenvalues of
J −P are either 1 or −1. Finally, by the deﬁnition of the matrix operator norm, ∥J −P∥2
op =
max{|λ(J −P)|} = 1, where λ(·) indicates the eigenvalues of the input matrix."
K,0.813599062133646,Under review as a conference paper at ICLR 2022
K,0.8147713950762017,"0
100
200
300
0.2 0.4 0.6 0.8 1.0"
K,0.8159437280187574,Validation accuracy (%) Epoch
K,0.8171160609613131,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8182883939038686,"0
100
200
300
0.2 0.4 0.6 0.8 1.0"
K,0.8194607268464243,Validation accuracy (%) Epoch
K,0.82063305978898,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8218053927315357,"0
100
200
300 1.0 1.5 2.0 2.5"
K,0.8229777256740914,Training loss (softmax) Epoch
K,0.8241500586166471,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8253223915592028,"0
100
200
300 1.0 1.5 2.0 2.5"
K,0.8264947245017585,Training loss (softmax) Epoch
K,0.8276670574443142,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8288393903868698,"0
2000
4000
6000
0.2 0.4 0.6 0.8 1.0"
K,0.8300117233294255,Validation accuracy (%)
K,0.8311840562719812,Iteration
K,0.8323563892145369,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8335287221570926,"0
2000
4000
6000
0.2 0.4 0.6 0.8 1.0"
K,0.8347010550996483,Validation accuracy (%)
K,0.835873388042204,Iteration
K,0.8370457209847597,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8382180539273154,"0
2000
4000
6000 1.0 1.5 2.0 2.5"
K,0.839390386869871,Training loss (softmax)
K,0.8405627198124267,Iteration
K,0.8417350527549824,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8429073856975381,"0
2000
4000
6000 1.0 1.5 2.0 2.5"
K,0.8440797186400938,Training loss (softmax)
K,0.8452520515826495,Iteration
K,0.8464243845252052,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8475967174677609,a) CIFAR-10 IID setting learning curves
K,0.8487690504103166,b) CIFAR-10 non-IID setting learning curves
K,0.8499413833528722,"Figure 4: The learning curves of CIFAR-10 (ResNet20) training (128 clients). a): The curves for
IID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg (x) indicates
FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of x
and the interval increase factor of y. As the aggregation interval increases, FedAvg rapidly loses the
convergence speed, and it results in achieving a lower validation accuracy within the ﬁxed iteration
budget. In contrast, FedLAMA effectively increases the aggregation interval while maintaining the
convergence speed."
K,0.8511137162954279,"0
100
200 1 2 3 4"
K,0.8522860492379836,Training loss (softmax) Epoch
K,0.8534583821805393,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.854630715123095,"0
100
200 1 2 3 4"
K,0.8558030480656507,Training loss (softmax) Epoch
K,0.8569753810082064,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8581477139507621,"0
100
200 0.2 0.4 0.6 0.8"
K,0.8593200468933178,Validation accuracy (%) Epoch
K,0.8604923798358733,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.861664712778429,"0
100
200 0.2 0.4 0.6 0.8"
K,0.8628370457209847,Validation accuracy (%) Epoch
K,0.8640093786635404,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8651817116060961,"0
2000
4000
6000
0.2 0.4 0.6 0.8"
K,0.8663540445486518,Validation accuracy (%)
K,0.8675263774912075,Iteration
K,0.8686987104337632,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8698710433763188,"0
2000
4000
6000
0.2 0.4 0.6 0.8"
K,0.8710433763188745,Validation accuracy (%)
K,0.8722157092614302,Iteration
K,0.8733880422039859,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8745603751465416,"0
2000
4000
6000 1 2 3 4"
K,0.8757327080890973,Training loss (softmax)
K,0.876905041031653,Iteration
K,0.8780773739742087,"FedAvg (6)
 FedLAMA (6, 2)
 FedLAMA (6, 4)"
K,0.8792497069167644,"0
2000
4000
6000 1 2 3 4"
K,0.88042203985932,Training loss (softmax)
K,0.8815943728018757,Iteration
K,0.8827667057444314,"FedAvg (6)
 FedAvg (12)
 FedAvg (24)"
K,0.8839390386869871,a) CIFAR-100 IID setting learning curves
K,0.8851113716295428,b) CIFAR-100 non-IID setting learning curves
K,0.8862837045720985,"Figure 5:
The learning curves of CIFAR-100 (WideResNet28-10) training (128 clients). a): The
curves for IID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg
(x) indicates FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base
interval of x and the interval increase factor of y. While FedAvg signiﬁcantly loses the convergence
speed as the aggregation interval increases, FedLAMA has a marginl impact on it which results in a
higher validation accuracy."
K,0.8874560375146542,"A.2
ADDITIONAL EXPERIMENTAL RESULTS"
K,0.8886283704572099,"In this section, we provide extra experimental results with extensive hyper-parameter settings. We
commonly use 128 clients and a local batch size of 32 in all the experiments. The gradual learning
rate warmup (Goyal et al. (2017)) is also applied to the ﬁrst 10 epochs in all the experiments. Overall,
the learning curve charts and the validation accuracy tables deliver the key insight that FedLAMA
achieves a comparable convergence speed to the periodic full aggregation with the base interval"
K,0.8898007033997656,Under review as a conference paper at ICLR 2022
K,0.8909730363423212,"0
500
1000
1500
2000
0.2 0.4 0.6 0.8 1.0"
K,0.8921453692848769,Validation accuracy (%)
K,0.8933177022274326,Iteration
K,0.8944900351699883,"FedAvg (10)
 FedLAMA (10, 2)
 FedLAMA (10, 4)"
K,0.895662368112544,"0
500
1000
1500
2000 0.2 0.4 0.6 0.8 1.0"
K,0.8968347010550997,Validation accuracy (%)
K,0.8980070339976554,Iteration
K,0.8991793669402111,"FedAvg (10)
 FedAvg (20)
 FedAvg (40)"
K,0.9003516998827668,"0
500
1000
1500
2000 1 2 3 4"
K,0.9015240328253223,Training loss (softmax)
K,0.902696365767878,Iteration
K,0.9038686987104337,"FedAvg (10)
 FedAvg (20)
 FedAvg (40)"
K,0.9050410316529894,"0
500
1000
1500
2000 1 2 3 4"
K,0.9062133645955451,Training loss (softmax)
K,0.9073856975381008,Iteration
K,0.9085580304806565,"FedAvg (10)
 FedLAMA (10, 2)
 FedLAMA (10, 4)"
K,0.9097303634232122,"Figure 6:
The learning curves of FEMNIST (CNN) training. FedAvg (x) indicates FedAvg with
the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of x and the interval
increase factor of y. FedLAMA curves are not strongly affected by the increased aggregation interval
while FedAvg signiﬁcantly loses the convergence speed as well as the validation accuracy."
K,0.9109026963657679,"(τ’) while having the communication cost that is similar to the periodic full aggregation with the
increased interval (φτ ′)."
K,0.9120750293083235,"Artiﬁcial Data Heterogeneity – For CIFAR-10 and CIFAR-100, we artiﬁcially generate the het-
erogeneous data distribution using Dirichlet’s distribution. The concentration coefﬁcient α is set to
0.1, 0.5, and 1.0 to evaluate the performance of FedLAMA across a variety of degree of data hetero-
geneity. Note that the small concentration coefﬁcient represents the highly heterogeneous numbers
of local samples across clients as well as the balance of the samples across the labels. We used the
data distribution source code provided by FedML (He et al. (2020))."
K,0.9132473622508792,"CIFAR-10 – Figure 4 shows the full learning curves for IID and non-IID CIFAR-10 datasets. The
hyper-parameter settings correspond to Table 4 and 1. First, as the aggregation interval increases
from 6 to 24, FedAvg suffers from the slower convergence, and it results in achieving a lower val-
idation accuracy, regardless of the data distribution. In contrast, FedLAMA learning curves are
marginally affected by the increased aggregation interval. Table 6 and 7 show the CIFAR-10 classi-
ﬁcation performance of FedLAMA across different φ settings. As expected, the accuracy is reduced
as φ increases. The IID and non-IID data settings show the common trend. Depending on the system
network bandwidth, φ can be tuned to be an appropriate value. When φ = 2, the accuracy is almost
the same as or even slightly higher than FedAvg accuracy. If the network bandwidth is limited, one
can increase φ and slightly increase the epoch budget to achieve a good accuracy. Table 8 shows the
CIFAR-10 accuracy across different τ ′ settings. We see that the accuracy is signiﬁcantly dropped as
τ ′ increases."
K,0.9144196951934349,"CIFAR-100 – Figure 5 shows the learning curves for IID and non-IID CIFAR-100 datasets. Likely
to CIFAR-10 results, FedAvg learning curves are strongly affected as the aggregation interval in-
creases from 6 to 24 while FedLAMA learning curves are not strongly affected. Table 9 and 10
show the CIFAR-100 classiﬁcation performance of FedLAMA across different φ settings. Fed-
LAMA achieves a comparable accuracy to FedAvg with a short aggregation interval, even when
the degree of data heterogeneity is extreamly high (25% device sampling and Direchlet’s coefﬁcient
of 0.1). Table 11 shows the FedAvg accuracy with different τ ′ settings. Under the strongly het-
erogeneous data distributions, FedAvg with a large aggregation interval (τ ≥12) do not achieve a
reasonable accuracy."
K,0.9155920281359906,"FEMNIST – Figure 6 shows the learning curves of CNN training. Likely to the previous two
datasets, the periodic full aggregation suffers from the slower convergence as the aggregation in-
terval increases. FedLAMA learning curves are not much affected by the increased aggregation
interval, and it results in achieving a higher validation accuracy after the same number of iterations.
Table 12 shows the FEMNIST classiﬁcation performance of FedLAMA across different φ settings.
FedLAMA achieves a similar accuracy to the baseline (FedAvg with τ ′ = 10) even when using a
large interval increase factor φ ≥4. These results demonstrate the effectiveness of the proposed
layer-wise adaptive model aggregation method on the problems with heterogeneous data distribu-
tions."
K,0.9167643610785463,Under review as a conference paper at ICLR 2022
K,0.917936694021102,Table 6: (IID data) CIFAR-10 classiﬁcation results of FedLAMA with different φ settings.
K,0.9191090269636577,"# of clients
Local batch size
LR
Averaging interval: τ ′
Interval increase factor: φ
Validation acc."
K,0.9202813599062134,"128
32 0.8 6"
K,0.9214536928487691,"1 (FedAvg)
88.37 ± 0.1% 0.5"
K,0.9226260257913247,"2
88.41 ± 0.04%
4
86.33 ± 0.2%
8
85.08 ± 0.04%"
K,0.9237983587338804,Table 7: (Non-IID data) CIFAR-10 classiﬁcation results of FedLAMA with different φ settings.
K,0.9249706916764361,"# of clients
Local batch size
LR
τ ′
Active ratio
Dirichlet coeff.
φ
Validation acc."
K,0.9261430246189918,"128
32 0.8 6"
K,0.9273153575615475,"100%
1"
K,0.9284876905041032,"1 (FedAvg)
90.79 ± 0.1%
2
89.01 ± 0.04%
4
87.84 ± 0.01%"
K,0.9296600234466589,"100%
0.5"
K,0.9308323563892146,"1 (FedAvg)
90.53 ± 0.18%
2
89.21 ± 0.2%
4
86.68 ± 0.12%"
K,0.9320046893317703,"100%
0.1"
K,0.9331770222743259,"1 (FedAvg)
89.52 ± 0.11%
2
89.00 ± 0.1%
4
84.82 ± 0.08% 50%
1"
K,0.9343493552168816,"1 (FedAvg)
90.34 ± 0.12%
2
89.56 ± 0.13%
4
87.48 ± 0.21%"
K,0.9355216881594373,"50%
0.5"
K,0.936694021101993,"1 (FedAvg)
89.86 ± 0.13%
2
88.44 ± 0.15%
4
87.29 ± 0.18%"
K,0.9378663540445487,"50%
0.1"
K,0.9390386869871044,"1 (FedAvg)
87.83 ± 0.2%
2
87.40 ± 0.17%
4
85.92 ± 0.21% 0.6 25%
1"
K,0.94021101992966,"1 (FedAvg)
88.97 ± 0.03%
2
87.89 ± 0.2%
4
86.61 ± 0.1%"
K,0.9413833528722158,"25%
0.5"
K,0.9425556858147714,"1 (FedAvg)
87.59 ± 0.05%
2
87.12 ± 0.08%
4
86.57 ± 0.02%"
K,0.943728018757327,"0.3
25%
0.1"
K,0.9449003516998827,"1 (FedAvg)
84.02 ± 0.04%
2
83.55 ± 0.02%
4
83.06 ± 0.03%"
K,0.9460726846424384,Table 8: (Non-IID data) CIFAR-10 classiﬁcation results of FedAvg with different τ ′ settings.
K,0.9472450175849941,"# of clients
Local batch size
LR
τ ′
Active ratio
Dirichlet coeff.
φ
Validation acc."
K,0.9484173505275498,"128
32
0.8 6"
K,0.9495896834701055,"100%
0.1"
K,0.9507620164126612,"1 (FedAvg)
89.52 ± 0.11%
12
1 (FedAvg)
87.29 ± 0.05%
24
1 (FedAvg)
84.82 ± 0.1%"
K,0.9519343493552169,"128
32
0.3 6"
K,0.9531066822977726,"25%
0.1"
K,0.9542790152403282,"1 (FedAvg)
84.02 ± 0.1%
12
1 (FedAvg)
82.48 ± 0.2%
24
1 (FedAvg)
76.72 ± 0.1%"
K,0.9554513481828839,Table 9: (IID data) CIFAR-100 classiﬁcation results of FedLAMA with different φ settings.
K,0.9566236811254396,"# of clients
Local batch size
LR
Averaging interval: τ ′
Interval increase factor: φ
Validation acc."
K,0.9577960140679953,"128
32
0.6
6"
K,0.958968347010551,"1 (FedAvg)
76.50 ± 0.02%
2
75.99 ± 0.03%
4
76.17 ± 0.2%
8
76.15 ± 0.2%"
K,0.9601406799531067,Under review as a conference paper at ICLR 2022
K,0.9613130128956624,Table 10: (Non-IID data) CIFAR-100 classiﬁcation results of FedLAMA with different φ settings.
K,0.9624853458382181,"# of clients
Local batch size
LR
τ ′
Active ratio
Dirichlet coeff.
φ
Validation acc."
K,0.9636576787807737,"128
32 0.4 6"
K,0.9648300117233294,"100%
1"
K,0.9660023446658851,"1 (FedAvg)
80.34 ± 0.01%
2
78.92 ± 0.01%
4
77.16 ± 0.05%"
K,0.9671746776084408,"100%
0.5"
K,0.9683470105509965,"1 (FedAvg)
80.19 ± 0.02%
2
78.88 ± 0.1%
4
78.03 ± 0.08%"
K,0.9695193434935522,"0.2
100%
0.1"
K,0.9706916764361079,"1 (FedAvg)
79.78 ± 0.02%
2
79.07 ± 0.02%
4
79.32 ± 0.01% 0.4 50%
1"
K,0.9718640093786636,"1 (FedAvg)
79.94 ± 0.1%
2
78.98 ± 0.01%
4
77.50 ± 0.02%"
K,0.9730363423212193,"50%
0.5"
K,0.9742086752637749,"1 (FedAvg)
79.95 ± 0.05%
2
78.37 ± 0.05%
4
76.93 ± 0.1%"
K,0.9753810082063306,"0.2
50%
0.1"
K,0.9765533411488863,"1 (FedAvg)
79.62 ± 0.06%
2
78.76 ± 0.02%
4
77.44 ± 0.02%"
K,0.977725674091442,"0.4
25%
1"
K,0.9788980070339977,"1 (FedAvg)
78.78 ± 0.02%
2
78.10 ± 0.02%
0.2
4
76.84 ± 0.03% 0.4"
K,0.9800703399765534,"25%
0.5"
K,0.981242672919109,"1 (FedAvg)
78.81 ± 0.01%
2
77.86 ± 0.04%
4
77.01 ± 0.1%"
K,0.9824150058616647,"25%
0.1"
K,0.9835873388042204,"1 (FedAvg)
79.06 ± 0.03%
2
78.63 ± 0.02%
0.2
4
77.17 ± 0.01%"
K,0.984759671746776,Table 11: (Non-IID data) CIFAR-100 classiﬁcation results of FedAvg with different τ ′ settings.
K,0.9859320046893317,"# of clients
Local batch size
LR
τ ′
Active ratio
Dirichlet coeff.
φ
Validation acc."
K,0.9871043376318874,"128
32
0.4 6"
K,0.9882766705744431,"100%
0.1"
K,0.9894490035169988,"1 (FedAvg)
79.78 ± 0.02%
12
1 (FedAvg)
77.71 ± 0.1%
24
1 (FedAvg)
69.63 ± 0.1%"
K,0.9906213364595545,"128
32
0.4 6"
K,0.9917936694021102,"25%
0.1"
K,0.9929660023446659,"1 (FedAvg)
79.06 ± 0.03%
12
1 (FedAvg)
76.16 ± 0.05%
24
1 (FedAvg)
67.43 ± 0.1%"
K,0.9941383352872216,"Table 12: FEMNIST classiﬁcation results of FedLAMA with different φ settings.
# of clients
Local batch size
LR
Averaging interval: τ ′
Active ratio
Interval increase factor: φ
Validation acc."
K,0.9953106682297772,"128
32
0.04
12 100%"
K,0.9964830011723329,"1 (FedAvg)
85.74 ± 0.21%
2
85.40 ± 0.13%
4
84.67 ± 0.1%
8
84.15 ± 0.18% 50%"
K,0.9976553341148886,"1 (FedAvg)
86.59 ± 0.2%
2
86.07 ± 0.1%
4
85.77 ± 0.15%
8
85.31 ± 0.03% 25%"
K,0.9988276670574443,"1 (FedAvg)
86.04 ± 0.2%
2
86.01 ± 0.1%
4
85.62 ± 0.08%
8
85.23 ± 0.1%"
