Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024691358024691358,"Multimodal variational autoencoders (VAEs) have shown promise as efﬁcient gen-
erative models for weakly-supervised data. Yet, despite their advantage of weak
supervision, they exhibit a gap in generative quality compared to unimodal VAEs,
which are completely unsupervised. In an attempt to explain this gap, we un-
cover a fundamental limitation that applies to a large family of mixture-based
multimodal VAEs. We prove that the sub-sampling of modalities enforces an unde-
sirable upper bound on the multimodal ELBO and thereby limits the generative
quality of the respective models. Empirically, we showcase the generative quality
gap on both synthetic and real data and present the tradeoffs between different vari-
ants of multimodal VAEs. We ﬁnd that none of the existing approaches fulﬁlls all
desired criteria of an effective multimodal generative model when applied on more
complex datasets than those used in previous benchmarks. In summary, we identify,
formalize, and validate fundamental limitations of VAE-based approaches for mod-
eling weakly-supervised data and discuss implications for real-world applications."
INTRODUCTION,0.0049382716049382715,"1
INTRODUCTION"
INTRODUCTION,0.007407407407407408,"In recent years, multimodal VAEs have shown great potential as efﬁcient generative models for
weakly-supervised data, such as pairs of images or paired images and captions. Previous works (Wu
and Goodman, 2018; Shi et al., 2019; Sutter et al., 2020) demonstrate that multimodal VAEs leverage
weak supervision to learn generalizable representations, useful for downstream tasks (Dorent et al.,
2019; Minoura et al., 2021) and for the conditional generation of missing modalities (Lee and van der
Schaar, 2021). However, despite the advantage of weak supervision, state-of-the-art multimodal
VAEs consistently underperform when compared to simple unimodal VAEs in terms of generative
quality.1 This paradox serves as a starting point for our work, which aims to explain the observed lack
of generative quality in terms of a fundamental limitation that underlies existing multimodal VAEs."
INTRODUCTION,0.009876543209876543,"What is limiting the generative quality of multimodal VAEs? We ﬁnd that the sub-sampling of
modalities during training leads to a problem that affects all mixture-based multimodal VAEs—a
family of models that subsumes the MMVAE (Shi et al., 2019), MoPoE-VAE (Sutter et al., 2021), and
a special case of the MVAE (Wu and Goodman, 2018). We prove that modality sub-sampling enforces
an undesirable upper bound on the multimodal ELBO and thus prevents a tight approximation of the
joint distribution when there is modality-speciﬁc variation in the data. Our experiments demonstrate
that modality sub-sampling can explain the gap in generative quality compared to unimodal VAEs
and that the gap typically increases with each additional modality. Through extensive ablations on
three different datasets, we validate the generative quality gap between unimodal and multimodal
VAEs and present the tradeoffs between different approaches."
INTRODUCTION,0.012345679012345678,"Our results raise serious concerns about the utility of multimodal VAEs for real-world applications.
We show that none of the existing approaches fulﬁlls all desired criteria (Shi et al., 2019; Sutter et al.,
2020) of an effective multimodal generative model when applied to slightly more complex datasets
than used in previous benchmarks. In particular, we demonstrate that generative coherence (Shi et al.,
2019) cannot be guaranteed for any of the existing approaches, if the information shared between
modalities cannot be predicted in expectation across modalities. Our ﬁndings are particularly relevant
for applications on datasets with a relatively high degree of modality-speciﬁc variation, which is a
typical characteristic of many real-world datasets (Baltrušaitis et al., 2019)."
THE LACK OF GENERATIVE QUALITY CAN EVEN BE RECOGNIZED BY VISUAL INSPECTION OF THE QUALITATIVE RESULTS FROM,0.014814814814814815,"1The lack of generative quality can even be recognized by visual inspection of the qualitative results from
previous works; for instance, see the supplementaries of Sutter et al. (2021) or Shi et al. (2021)."
THE LACK OF GENERATIVE QUALITY CAN EVEN BE RECOGNIZED BY VISUAL INSPECTION OF THE QUALITATIVE RESULTS FROM,0.01728395061728395,Published as a conference paper at ICLR 2022
RELATED WORK,0.019753086419753086,"2
RELATED WORK"
RELATED WORK,0.022222222222222223,"First, to put multimodal VAEs into context, let us point out that there is a long line of research
focused on learning multimodal generative models based on a wide variety of methods. There are
several notable generative models with applications on pairs of modalities (e.g., Ngiam et al., 2011;
Srivastava and Salakhutdinov, 2014; Wu and Goodman, 2019; Lin et al., 2021; Ramesh et al., 2021),
as well as for the specialized task of image-to-image translation (e.g., Huang et al., 2018; Choi et al.,
2018; Liu et al., 2019). Moreover, generative models can use labels as side information (Ilse et al.,
2019; Tsai et al., 2019; Wieser et al., 2020); for example, to guide the disentanglement of shared
and modality-speciﬁc information (Tsai et al., 2019). In contrast, multimodal VAEs do not require
strong supervision and can handle a large and variable number of modalities efﬁciently. They learn
a joint distribution over multiple modalities, but also enable the inference of latent representations,
as well as the conditional generation of missing modalities, given any subset of modalities (Wu and
Goodman, 2018; Shi et al., 2019; Sutter et al., 2021)."
RELATED WORK,0.024691358024691357,"Multimodal VAEs are an extension of VAEs (Kingma and Welling, 2014) and they belong to the class
of multimodal generative models with encoder-decoder architectures (Baltrušaitis et al., 2019). The
ﬁrst multimodal extensions of VAEs (Suzuki et al., 2016; Hsu and Glass, 2018; Vedantam et al., 2018)
use separate inference networks for every subset of modalities, which quickly becomes intractable
as the number of inference networks required grows exponentially with the number of modalities.
Starting with the seminal work of Wu and Goodman (2018), multimodal VAEs were developed as an
efﬁcient method for multimodal learning. In particular, multimodal VAEs enable the inference of
latent representations, as well as the conditional generation of missing modalities, given any subset
of input modalities. Different types of multimodal VAEs were devised by decomposing the joint
encoder as a product (Wu and Goodman, 2018), mixture (Shi et al., 2019), or mixture of products
(Sutter et al., 2021) of unimodal encoders respectively. A commonality between these approaches
is the sub-sampling of modalities during training—a property we will use to deﬁne the family of
mixture-based multimodal VAEs. For the MMVAE and MoPoE-VAE, the sub-sampling is a direct
consequence of deﬁning the joint encoder as a mixture distribution over different subsets of modalities.
Further, our analysis includes a special case of the MVAE without ELBO sub-sampling, which can
be seen as another member of the family of mixture-based multimodal VAEs (Sutter et al., 2021).
The MVAE was originally proposed with “ELBO sub-sampling”, an additional training paradigm
that was later found to result in an incorrect bound on the joint distribution (Wu and Goodman, 2019).
While this training paradigm is also based on the sub-sampling of modalities, the objective differs
from mixture-based multimodal VAEs in that the MVAE does not reconstruct the missing modalities
from the set of sub-sampled modalities.2"
RELATED WORK,0.027160493827160494,"Table 1 provides an overview of the different variants of mixture-based multimodal VAEs and
the properties that one can infer from empirical results in previous works (Shi et al., 2019; 2021;
Sutter et al., 2021). Most importantly, there appears to be a tradeoff between generative quality and
generative coherence (i.e., the ability to generate semantically related samples across modalities). Our
work explains why the generative quality is worse for models that sub-sample modalities (Section 4)
and shows that a tighter approximation of the joint distribution can be achieved without sub-sampling
(Section 4.3). Through systematic ablations, we validate the proposed theoretical limitations and
showcase the tradeoff between generative quality and generative coherence (Section 5.1). Our
experiments also reveal that generative coherence cannot be guaranteed for more complex datasets
than those used in previous benchmarks (Section 5.2)."
RELATED WORK,0.02962962962962963,"3
MULTIMODAL VAES, IN DIFFERENT FLAVORS"
RELATED WORK,0.03209876543209877,"Let X
:=
{X1, . . . , XM} be a set of random vectors describing M modalities and let
x := {x1, . . . , xM} be a sample from the joint distribution p(x1, . . . , xM). For conciseness, denote
subsets of modalities by subscripts; for example, X{1,3} or x{1,3} respectively for modalities 1 and 3."
RELATED WORK,0.0345679012345679,"Throughout this work, we assume that all modalities are described by discrete random vectors (e.g.,
pixel values), so that we can assume non-negative entropy and conditional entropy terms. Deﬁnitions
for all required information-theoretic quantities are provided in Appendix A."
RELATED WORK,0.037037037037037035,"2For completeness, in Appendix C, we also analyze the effect of ELBO sub-sampling."
RELATED WORK,0.03950617283950617,Published as a conference paper at ICLR 2022
RELATED WORK,0.04197530864197531,"Table 1: Overview of multimodal VAEs. Entries for generative quality and generative coherence
denote properties that were observed empirically in previous works. The lightning symbol ( ) denotes
properties for which our work presents contrary evidence. This overview abstracts technical details,
such as importance sampling and ELBO sub-sampling, which we address in Appendix C."
RELATED WORK,0.044444444444444446,"Model
Decomposition of pθ(z | x)
Modality sub-sampling
Generative quality
Generative coherence"
RELATED WORK,0.04691358024691358,"MVAE (Wu and Goodman, 2018)
QM
i=1 pθ(z | xi)

good
poor"
RELATED WORK,0.04938271604938271,"MMVAE (Shi et al., 2019)
1
M
PM
i=1 pθ(z | xi)

limited
good"
RELATED WORK,0.05185185185185185,"MoPoE-VAE (Sutter et al., 2021)
1
|P(M)|
P
A∈P(M)
Q
i∈A pθ(z|xi)

limited
good"
THE MULTIMODAL ELBO,0.05432098765432099,"3.1
THE MULTIMODAL ELBO"
THE MULTIMODAL ELBO,0.056790123456790124,"Deﬁnition 1. Let pθ(z|x) be a stochastic encoder, parameterized by θ, that takes multiple modalities
as input. Let qφ(x | z) be a variational decoder (for all modalities), parameterized by φ, and let q(z)
be a prior. The multimodal evidence lower bound (ELBO) on Ep(x)[log p(x)] is deﬁned as"
THE MULTIMODAL ELBO,0.05925925925925926,"L(x; θ, φ) := Ep(x)pθ(z | x)[log qφ(x | z)] −Ep(x)[DKL(pθ(z | x) || q(z))] .
(1)"
THE MULTIMODAL ELBO,0.06172839506172839,"The multimodal ELBO (Deﬁnition 1), ﬁrst introduced by Wu and Goodman (2018), is the objective
maximized by all multimodal VAEs and it forms a variational lower bound on the expected log-
evidence.3 The ﬁrst term denotes the estimated log-likelihood of all modalities and the second term
is the KL-divergence between the stochastic encoder and the prior. We take an information-theoretic
perspective using the variational information bottleneck (VIB) from Alemi et al. (2017) and employ
the standard notation used in multiple previous works (Alemi et al., 2017; Poole et al., 2019). Similar
to the latent variable model approach, the VIB derives the ELBO as a variational lower bound on the
expected log-evidence, but, in addition, the VIB is a more general framework for optimization that
allows us to reason about the underlying information-theoretic quantities of interest (for details on
the VIB and its notation, please see Appendix B.1)."
THE MULTIMODAL ELBO,0.06419753086419754,"Note that the above deﬁnition of the multimodal ELBO requires that the complete set of modalities
is available. To overcome this limitation and to learn the inference networks for different subsets
of modalities, existing models use different decompositions of the joint encoder, as summarized in
Table 1. Recent work shows that existing models can be generalized by formulating the joint encoder
as a mixture of products of experts (Sutter et al., 2021). Analogously, in the following, we generalize
existing models to deﬁne the family of mixture-based multimodal VAEs."
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.06666666666666667,"3.2
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.0691358024691358,"Now we introduce the family of mixture-based multimodal VAEs, which subsumes the MMVAE,
MoPoE-VAE, and a special case of the MVAE without ELBO sub-sampling. We ﬁrst deﬁne an
encoder that generalizes the decompositions used by existing models:
Deﬁnition 2. Let S = {(A, ωA) | A ⊆{1, . . . , M}, A ̸= ∅, ωA ∈[0, 1]} be an arbitrary set of non-
empty subsets A of modalities and corresponding mixture coefﬁcients ωA, such that P"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.07160493827160494,"A∈S ωA = 1.
Deﬁne the stochastic encoder to be a mixture distribution: pS
θ (z | x) := P"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.07407407407407407,A∈S ωA pθ(z | xA).
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.07654320987654321,"In the above deﬁnition and throughout this work, we write A ∈S to abbreviate (A, ωA) ∈S. To
deﬁne the family of mixture-based multimodal VAEs, we restrict the family of models optimizing the
multimodal ELBO to the subfamily of models that use a mixture-based stochastic encoder.
Deﬁnition 3. The family of mixture-based multimodal VAEs is comprised of all models that maximize
the multimodal ELBO using a stochastic encoder pS
θ (z | x) that is consistent with Deﬁnition 2. In
particular, we deﬁne the family in terms of all models that maximize the following objective:"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.07901234567901234,"LS(x; θ, φ) =
X"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.08148148148148149,"A∈S
ωA

Ep(x)pθ(z | xA)[log qφ(x | z)] −Ep(x) [DKL (pθ(z | xA) || q(z))]
	
.
(2)"
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.08395061728395062,"3Even though we write the expectation over p(x), for the estimation of the ELBO we still assume that we
only have access to a ﬁnite sample from the training distribution p(x). The notation is used for consistency with
the well-established information-theoretic perspective on VAEs (Alemi et al., 2017; Poole et al., 2019)."
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.08641975308641975,Published as a conference paper at ICLR 2022
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.08888888888888889,"In Appendix B.2, we show that the objective LS(x; θ, φ) is a lower bound on L(x; θ, φ) (which makes
it an ELBO) and explain how, for different choices of the set of subsets S, the objective LS(x; θ, φ)
relates to the objectives of the MMVAE, MoPoE-VAE, and MVAE without ELBO sub-sampling."
THE FAMILY OF MIXTURE-BASED MULTIMODAL VAES,0.09135802469135802,"From a computational perspective, a characteristic of mixture-based multimodal VAEs is the sub-
sampling of modalities during training, which is a direct consequence of deﬁning the encoder as a
mixture distribution over subsets of modalities. The sub-sampling of modalities can be viewed as
the extraction of a subset xA ∈x, where A indexes one subset of modalities that is drawn from
the model-speciﬁc set of subsets S. The only member of the family of mixture-based multimodal
VAEs that forgoes sub-sampling, deﬁnes a trivial mixture over a single subset, the complete set of
modalities (Sutter et al., 2021)."
MODALITY SUB-SAMPLING LIMITS THE MULTIMODAL ELBO,0.09382716049382717,"4
MODALITY SUB-SAMPLING LIMITS THE MULTIMODAL ELBO"
AN INTUITION ABOUT THE PROBLEM,0.0962962962962963,"4.1
AN INTUITION ABOUT THE PROBLEM"
AN INTUITION ABOUT THE PROBLEM,0.09876543209876543,"Before we delve into the details, let us illustrate how modality sub-sampling affects the likelihood esti-
mation, and hence the multimodal ELBO. Consider the likelihood estimation using the objective LS:
X"
AN INTUITION ABOUT THE PROBLEM,0.10123456790123457,"A∈S
ωA Ep(x)pθ(z | xA)[log qφ(x | z)] ,
(3)"
AN INTUITION ABOUT THE PROBLEM,0.1037037037037037,"where A denotes a subset of modalities and ωA the respective mixture weight. Crucially, the stochastic
encoder pθ(z | xA) encodes a subset of modalities. What seems to be a minute detail, can have
a profound impact on the likelihood estimation, because the precise estimation of all modalities
depends on information from all modalities. In trying to reconstruct all modalities from incomplete
information, the model can learn an inexact, average prediction; however, it cannot reliably predict
modality-speciﬁc information, such as the background details in an image given a concise verbal
description of its content."
AN INTUITION ABOUT THE PROBLEM,0.10617283950617284,"In the following, we formalize the above intuition by showing that, in the presence of modality-
speciﬁc variation, modality sub-sampling enforces an undesirable upper bound on the multimodal
ELBO and therefore prevents a tight approximation of the joint distribution."
A FORMALIZATION OF THE PROBLEM,0.10864197530864197,"4.2
A FORMALIZATION OF THE PROBLEM"
A FORMALIZATION OF THE PROBLEM,0.1111111111111111,"Theorem 1 states our main theoretical result, which describes a non-trivial limitation of mixture-based
multimodal VAEs. Our result shows that the sub-sampling of modalities enforces an undesirable
upper bound on the approximation of the joint distribution when there is modality-speciﬁc variation
in the data. This limitation conﬂicts with the goal of modeling real-world multimodal data, which
typically exhibits a considerable degree of modality-speciﬁc variation.
Theorem 1. Each mixture-based multimodal VAE (Deﬁnition 3) approximates the expected log-
evidence up to an irreducible discrepancy ∆(X, S) that depends on the model-speciﬁc mixture
distribution S as well as on the amount of modality-speciﬁc information in X."
A FORMALIZATION OF THE PROBLEM,0.11358024691358025,"For the maximization of LS(x; θ, φ) and every value of θ and φ, the following inequality holds:"
A FORMALIZATION OF THE PROBLEM,0.11604938271604938,"Ep(x)[log p(x)] ≥LS(x; θ, φ) + ∆(X, S)
(4)"
A FORMALIZATION OF THE PROBLEM,0.11851851851851852,"where
∆(X, S) =
X"
A FORMALIZATION OF THE PROBLEM,0.12098765432098765,"A∈S
ωA H(X{1,...,M}\A | XA) .
(5)"
A FORMALIZATION OF THE PROBLEM,0.12345679012345678,"In particular, the generative discrepancy is always greater than or equal to zero and it is independent
of θ and φ and thus remains constant during the optimization."
A FORMALIZATION OF THE PROBLEM,0.1259259259259259,"A proof is provided in Appendix B.5 and it is based on Lemmas 1 and 2. Theorem 1 formalizes
the rationale that, in the general case, cross-modal prediction cannot recover information that is
speciﬁc to the target modalities that are unobserved due to modality sub-sampling. In general,
the conditional entropy H(X{1,...,M}\A | XA) measures the amount of information in one subset
of random vectors X{1,...,M}\A that is not shared with another subset XA. In our context, the
sub-sampling of modalities yields a discrepancy ∆(X, S) that is a weighted average of conditional"
A FORMALIZATION OF THE PROBLEM,0.12839506172839507,Published as a conference paper at ICLR 2022
A FORMALIZATION OF THE PROBLEM,0.1308641975308642,"entropies H(X{1,...,M}\A | XA) of the modalities X{1,...,M}\A unobserved by the encoder given an
observed subset XA. Hence, ∆(X, S) describes the modality-speciﬁc information that cannot be
recovered by cross-modal prediction, averaged over all subsets of modalities."
A FORMALIZATION OF THE PROBLEM,0.13333333333333333,"Theorem 1 applies to the MMVAE, MoPoE-VAE, and a special case of the MVAE without ELBO sub-
sampling, since all of these models belong to the class of mixture-based multimodal VAEs. However,
∆(X, S) can vary signiﬁcantly between different models, depending on the mixture distribution
deﬁned by the respective model and on the amount of modality-speciﬁc variation in the data. In the
following, we show that without modality sub-sampling ∆(X, S) vanishes, whereas for the MMVAE
and MoPoE-VAE, ∆(X, S) typically increases with each additional modality. In Section 5, we
provide empirical support for each of these theoretical statements."
A FORMALIZATION OF THE PROBLEM,0.13580246913580246,"4.3
IMPLICATIONS OF THEOREM 1"
A FORMALIZATION OF THE PROBLEM,0.1382716049382716,"First, we consider the case of no modality sub-sampling, for which it is easy to show that the
generative discrepancy vanishes.
Corollary 1. Without modality sub-sampling, ∆(X, S) = 0 ."
A FORMALIZATION OF THE PROBLEM,0.14074074074074075,"A proof is provided in Appendix B.6. The result from Corollary 1 applies to the MVAE without
ELBO sub-sampling and suggests that this model should yield a tighter approximation of the joint
distribution and hence a better generative quality compared to mixture-based multimodal VAEs that
sub-sample modalities. Note that this does not imply that a model without modality sub-sampling is
superior to one that uses sub-sampling and that there can be an inductive bias that favors sub-sampling
despite the approximation error it incurs. Especially, Corollary 1 does not imply that the variational
approximation is tight for the MVAE; for instance, the model can be underparameterized or simply
misspeciﬁed due to simplifying assumptions, such as the PoE-factorization (Kurle et al., 2019)."
A FORMALIZATION OF THE PROBLEM,0.14320987654320988,"Second, we consider how additional modalities might affect the generative discrepancy. Corollary 2
predicts an increased generative discrepancy (and hence, a decline of generative quality) when we
increase the number of modalities for the MMVAE and MoPoE-VAE.
Corollary 2 (informal). For the MMVAE and MoPoE-VAE, the generative discrepancy increases
with each additional modality, if the new modality is sufﬁciently diverse."
A FORMALIZATION OF THE PROBLEM,0.145679012345679,"A proof is provided in Appendix B.7. The notion of diversity requires a more formal treatment of
the underlying information-theoretic quantities, which we defer to Appendix B.7. Intuitively, a new
modality is sufﬁciently diverse, if it does not add too much redundant information with respect to
the existing modalities. In special cases when there is a lot of redundant information, ∆(X, S) can
decrease given an additional modality, but it does not vanish in any one of these cases. Only if there
is very little modality-speciﬁc information in all modalities, we have ∆(X, S) →0 for the MMVAE
and MoPoE-VAE. This condition requires modalities to be extremely similar, which does not apply
to most multimodal datasets, where ∆(X, S) typically represents a large part of the total variation."
A FORMALIZATION OF THE PROBLEM,0.14814814814814814,"In summary, Theorem 1 formalizes how the family of mixture-based multimodal VAEs is fundamen-
tally limited for the task of approximating the joint distribution, and Corollaries 1 and 2 connect this
result to existing models—the MMVAE, MoPoE-VAE, and MVAE without ELBO sub-sampling. We
now turn to the experiments, where we present empirical support for the limitations described by
Theorem 1 and its Corollaries."
EXPERIMENTS,0.1506172839506173,"5
EXPERIMENTS"
EXPERIMENTS,0.15308641975308643,"Figure 1 presents the three considered datasets.
PolyMNIST (Sutter et al., 2021) is a sim-
ple, synthetic dataset with ﬁve image modalities that allows us to conduct systematic ablations.
Translated-PolyMNIST is a new dataset that adds a small tweak—the downscaling and random
translation of digits—to demonstrate the limitations of existing methods when shared information
cannot be predicted in expectation across modalities. Finally, Caltech Birds (CUB; Wah et al., 2011;
Shi et al., 2019) is used to validate the limitations on a more realistic dataset with two modalities,
images and captions. Please note that we use CUB with real images and not the simpliﬁed version
based on precomputed ResNet-features that was used in Shi et al. (2019) and Shi et al. (2021). For a
more detailed description of the three considered datasets, please see Appendix C.1."
EXPERIMENTS,0.15555555555555556,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1580246913580247,"(a) PolyMNIST
(5 modalities)"
EXPERIMENTS,0.16049382716049382,"(b) Translated-PolyMNIST
(5 modalities)"
EXPERIMENTS,0.16296296296296298,"(c) Caltech Birds (CUB)
(2 modalities)"
EXPERIMENTS,0.1654320987654321,"Figure 1: The three considered datasets. Each subplot shows samples from the respective dataset.
The two PolyMNIST datasets are conceptually similar in that the digit label is shared between ﬁve
synthetic modalities. The Caltech Birds (CUB) dataset provides a more realistic application for which
there is no annotation on what is shared between paired images and captions."
EXPERIMENTS,0.16790123456790124,"In total, more than 400 models were trained, requiring approximately 1.5 GPU years of compute on a
single NVIDIA GeForce RTX 2080 Ti GPU. For the experiments in the main text, we use the publicly
available code from Sutter et al. (2021) and in Appendix C.3 we also include ablations using the
publicly available code from Shi et al. (2019), which implements importance sampling and alternative
ELBO objectives. To provide a fair comparison across methods, we use the same architectures and
similar capacities for all models. For each unimodal VAE, we make sure to decrease the capacity by
reducing the latent dimensionality proportionally with respect to the number of modalities. Additional
information on architectures, hyperparameters, and evaluation metrics is provided in Appendix C."
THE GENERATIVE QUALITY GAP,0.17037037037037037,"5.1
THE GENERATIVE QUALITY GAP"
THE GENERATIVE QUALITY GAP,0.1728395061728395,"We assume that an increase in the generative discrepancy ∆(X, S) is associated with a drop of
generative quality. However, we want to point out that there can also be an inductive bias that favors
modality sub-sampling despite the approximation error that it incurs. In fact, our experiments reveal a
fundamental tradeoff between generative quality and generative coherence when shared information
can be predicted in expectation across modalities."
THE GENERATIVE QUALITY GAP,0.17530864197530865,"We measure generative quality in terms of Fréchet inception distance (FID; Heusel et al., 2017), a
standard metric for evaluating the quality of generated images. Lower FID represents better generative
quality and the values typically correlate well with human perception (Borji, 2019). In addition,
in Appendix C we provide log-likelihood values, as well as qualitative results for all modalities
including captions, for which FID cannot be computed."
THE GENERATIVE QUALITY GAP,0.17777777777777778,"Figure 2 presents the generative quality across a range of β values.4 To relate different methods, we
compare models with the best FID respectively, because different methods can reach their optima at
different β values. As described by Theorem 1, mixture-based multimodal VAEs that sub-sample
modalities (MMVAE and MoPoE-VAE) exhibit a pronounced generative quality gap compared to
unimodal VAEs. When we compare the best models, we observe a gap of more than 60 points on both
PolyMNIST and Translated-PolyMNIST, and about 30 points on CUB images. Qualitative results
(Figure 9 in Appendix C.3) conﬁrm that this gap is clearly visible in the generated samples and that it
applies not only to image modalities, but also to captions. In contrast, the MVAE (without ELBO
sub-sampling) reaches the generative quality of unimodal VAEs, which is in line with our theoretical
result from Corollary 1. For completeness, in Appendix C.3, we also report joint log-likelihoods,
latent classiﬁcation performance, as well as additional FIDs for all modalities."
THE GENERATIVE QUALITY GAP,0.18024691358024691,"Figure 3 examines how the generative quality is affected when we vary the number of modalities.
Notably, for the MMVAE and MoPoE-VAE, the generative quality deteriorates almost continuously
with the number of modalities, which is in line with our theoretical result from Corollary 2. Interest-
ingly, for the MVAE, the generative quality on Translated-PolyMNIST also decreases slightly, but
the change is comparatively small. Figure 11 in Appendix C.3, shows a similar trend even when
we control for modality-speciﬁc differences by generating PolyMNIST using the same background
image for all modalities."
THE GENERATIVE QUALITY GAP,0.18271604938271604,"4The regularization coefﬁcient β weights the KL-divergence term of the multimodal ELBO (Deﬁnitions 1
and 3) and it is arguably the most impactful hyperparameter in VAEs (e.g., see Higgins et al., 2017)."
THE GENERATIVE QUALITY GAP,0.18518518518518517,Published as a conference paper at ICLR 2022
THE GENERATIVE QUALITY GAP,0.18765432098765433,"(a) PolyMNIST
(b) Translated-PolyMNIST
(c) Caltech Birds (CUB)"
THE GENERATIVE QUALITY GAP,0.19012345679012346,"Figure 2: Generative quality for one output modality over a range of β values. Points denote the FID
averaged over three seeds and bands show one standard deviation respectively. Due to numerical
instabilities, the MVAE could not be trained with larger β values."
THE GENERATIVE QUALITY GAP,0.1925925925925926,"(a) PolyMNIST
(b) Translated-PolyMNIST"
THE GENERATIVE QUALITY GAP,0.19506172839506172,"Figure 3: Generative quality as a function of the number of modalities. The results show the FID of
the same modality and therefore all values on the same scale. All models are trained with β = 1 on
PolyMNIST and β = 0.3 on Translated-PolyMNIST. The results are averaged over three seeds and
the bands show one standard deviation respectively. For the unimodal VAE, which uses only a single
modality, the average and standard deviation are plotted as a constant."
THE GENERATIVE QUALITY GAP,0.19753086419753085,"In summary, the results from Figure 2 and Figure 3 provide empirical support for the existence of
a generative quality gap between unimodal and mixture-based multimodal VAEs that sub-sample
modalities. The results verify that the approximation of the joint distribution improves for models
without sub-sampling, which manifests in better generative quality. In contrast, the gap increases
disproportionally with each additional modality for both the MMVAE and MoPoE-VAE. Hence, the
presented results support all of the theoretical statements from Sections 4.2 and 4.3."
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.2,"5.2
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.20246913580246914,"Apart from generative quality, another desired criterion (Shi et al., 2019; Sutter et al., 2020) for an
effective multimodal generative model is generative coherence, which measures a model’s ability to
generate semantically related samples across modalities. To be consistent with Sutter et al. (2021), we
compute the leave-one-out coherence (see Appendix C.2), which means that the input to each model
consists of all modalities except the one that is being conditionally generated. On CUB, we resort to
a qualitative evaluation of coherence, because there is no ground truth annotation of shared factors
and the proxies used in Shi et al. (2019) and Shi et al. (2021) do not yield meaningful estimates when
applied to the conditionally generated images from models that were trained on real images.5"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.20493827160493827,"In terms of generative coherence, Figure 4 reveals that the positive results from previous work do
not translate to more complex datasets. As a baseline, for PolyMNIST (Figure 4a) we replicate the
coherence results from Sutter et al. (2021) for a range of β values. Consistent with previous work
(Shi et al., 2019; 2021; Sutter et al., 2020; 2021), we ﬁnd that the MMVAE and MoPoE-VAE exhibit"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.2074074074074074,"5Please note that previous work (Shi et al., 2019; 2021) used a simpliﬁed version of the CUB dataset, where
images were replaced by precomputed ResNet-features."
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.20987654320987653,Published as a conference paper at ICLR 2022
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.2123456790123457,(a) PolyMNIST
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.21481481481481482,(b) Translated-PolyMNIST
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.21728395061728395,"MVAE, β = 9"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.21975308641975308,"MMVAE, β = 9"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.2222222222222222,"MoPoE-VAE, β = 9"
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.22469135802469137,(c) Caltech Birds (CUB)
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.2271604938271605,"Figure 4: Generative coherence for the conditional generation across modalities. For PolyMNIST
(Figures 4a and 4b), we plot the average leave-one-out coherence. Due to numerical instabilities, the
MVAE could not be trained with larger β values. For CUB (Figure 4c), we show qualitative results
for the conditional generation of images given captions. Best viewed zoomed and in color."
LACK OF GENERATIVE COHERENCE ON MORE COMPLEX DATA,0.22962962962962963,"superior coherence compared to the MVAE. Though, it was not apparent from previous work that
MVAE’s coherence can improve signiﬁcantly with increasing β values, which can be of independent
interest for future work. On Translated-PolyMNIST (Figure 4b), the stark decline of all models
makes it evident that coherence cannot be guaranteed when shared information cannot be predicted
in expectation across modalities. Our qualitative results (Figure 10 in Appendix C.3) conﬁrm that not
a single multimodal VAE is able to conditionally generate coherent examples and, for the most part,
not any digits at all. To verify that the lack of coherence is not an artifact of our implementation, we
have checked that the encoders and decoders have sufﬁcient capacity such that digits show up in most
self-reconstructions. On CUB (Figure 4c), for which coherence cannot be computed, the qualitative
results for conditional generation verify that none of the existing approaches generates images that are
both of sufﬁciently high quality and coherent with respect to the given caption. Overall, the negative
results on Translated-PolyMNIST and CUB showcase the limitations of existing approaches when
applied to more complex datasets than those used in previous benchmarks."
DISCUSSION,0.23209876543209876,"6
DISCUSSION"
DISCUSSION,0.2345679012345679,"Implications and scope
Our experiments lend empirical support to the proposed theoretical limi-
tations of mixture-based multimodal VAEs. On both synthetic and real data, our results showcase
the generative limitations of multimodal VAEs that sub-sample modalities. However, our results
also reveal that none of the existing approaches (including those without sub-sampling) fulﬁll all
desired criteria (Shi et al., 2019; Sutter et al., 2020) of an effective multimodal generative model.
More broadly, our results showcase the limitations of existing VAE-based approaches for modeling
weakly-supervised data in the presence of modality-speciﬁc information, and in particular when
shared information cannot be predicted in expectation across modalities. The Translated-PolyMNIST
dataset demonstrates this problem in a simple setting, while the results on CUB conﬁrm that similar
issues can be expected on more realistic datasets. For future work, it would be interesting to generate
simulated data where the discrepancy ∆(X, S) can be measured exactly and where it is gradually
increased by an adaptation of the dataset in a way that increases only the modality-speciﬁc variation.
Furthermore, it is worth noting that Theorem 1 applies to all multimodal VAEs that optimize Equa-"
DISCUSSION,0.23703703703703705,Published as a conference paper at ICLR 2022
DISCUSSION,0.23950617283950618,"tion (2), which is a lower bound on the multimodal ELBO for models that sub-sample modalities. Our
theory predicts the same discrepancy for models that optimize a tighter bound (e.g., via Equation (28)),
because the discrepancy ∆(X, S) derives from the likelihood term, which is equal for Equations (2)
and (28). In Appendix C.3 we verify that the discrepancy can also be observed for the MMVAE with
the original implementation from Shi et al. (2019) that uses a tighter bound. Further analysis of the
different bounds can be an interesting direction for future work."
DISCUSSION,0.2419753086419753,"Model selection and generalization
Our results raise fundamental questions regarding model
selection and generalization, as generative quality and generative coherence do not necessarily go
hand in hand. In particular, our experiments demonstrate that FIDs and log-likelihoods do not reﬂect
the problem of lacking coherence and without access to ground truth labels (on what is shared
between modalities) coherence metrics cannot be computed. As a consequence, it can be difﬁcult
to perform model selection on more realistic multimodal datasets, especially for less interpretable
types of modalities, such as DNA sequences. Hence, for future work it would be interesting to
design alternative metrics for generative coherence that can be applied when shared information
is not annotated. For the related topic of generalization, it can be illuminating to consider what
would happen, if one could arbitrarily “scale things up”. In the limit of inﬁnite i.i.d. data, perfect
generative coherence could be achieved by a model that memorizes the pairwise relations between
training examples from different modalities. However, would this yield a model that generalizes out
of distribution (e.g., under distribution shift)? We believe that for future work it would be worthwhile
to consider out-of-distribution generalization performance (e.g., Montero et al., 2021) in addition to
generative quality and coherence."
DISCUSSION,0.24444444444444444,"Limitations
In general, the limitations and tradeoffs presented in this work apply to a large family
of multimodal VAEs, but not necessarily to other types of generative models, such as generative
adversarial networks (Goodfellow et al., 2014). Where current VAEs are limited by the reconstruc-
tion of modality-speciﬁc information, other types of generative models might offer less restrictive
objectives. Similar to previous work, we have only considered models with simple priors, such as
Gauss and Laplace distributions with independent dimensions. Further, we have not considered
models with modality-speciﬁc latent spaces, which seem to yield better empirical results (Hsu and
Glass, 2018; Sutter et al., 2020; Daunhawer et al., 2020), but currently lack theoretical grounding.
Modality-speciﬁc latent spaces offer a potential solution to the problem of cross-modal prediction
by providing modality-speciﬁc context from the target modalities to each decoder. However, more
work is required to establish guarantees for the identiﬁability and disentanglement of shared and
modality-speciﬁc factors, which might only be possible for VAEs under relatively strong assumptions
(Locatello et al., 2019; 2020; Gresele et al., 2019; von Kügelgen et al., 2021)."
CONCLUSION,0.24691358024691357,"7
CONCLUSION"
CONCLUSION,0.24938271604938272,"In this work, we have identiﬁed, formalized, and demonstrated several limitations of multimodal
VAEs. Across different datasets, this work revealed a signiﬁcant gap in generative quality between
unimodal and mixture-based multimodal VAEs. We showed that this apparent paradox can be
explained by the sub-sampling of modalities, which enforces an undesirable upper bound on the
multimodal ELBO and therefore limits the generative quality of the respective models. While the
sub-sampling of modalities allows these models to learn the inference networks for different subsets
of modalities efﬁciently, there is a notable tradeoff in terms of generative quality. Finally, we studied
two failure cases—Translated-PolyMNIST and CUB—that demonstrate the limitations of multimodal
VAEs when applied to more complex datasets than those used in previous benchmarks."
CONCLUSION,0.2518518518518518,"For future work, we believe that it is crucial to be aware of the limitations of existing methods as
a ﬁrst step towards developing new methods that achieve more than incremental improvements for
multimodal learning. We conjecture that there are at least two potential strategies to circumvent the
theoretical limitations of multimodal VAEs. First, the sub-sampling of modalities can be combined
with modality-speciﬁc context from the target modalities. Second, cross-modal reconstruction terms
can be replaced with less restrictive objectives that do not require an exact prediction of modality-
speciﬁc information. Finally, we urge future research to design more challenging benchmarks and to
compare multimodal generative models in terms of both generative quality and coherence across a
range of hyperparameter values, to present the tradeoff between these metrics more transparently."
CONCLUSION,0.254320987654321,Published as a conference paper at ICLR 2022
CONCLUSION,0.25679012345679014,ACKNOWLEDGEMENTS
CONCLUSION,0.25925925925925924,"ID and KC were supported by the SNSF grant #200021_188466. Special thanks to Alexander
Marx, Nicolò Ruggeri, Maxim Samarin, Yuge Shi, and Mario Wieser for helpful discussions and/or
feedback on the manuscript."
REPRODUCIBILITY STATEMENT,0.2617283950617284,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.2641975308641975,"For all theoretical statements, we provide detailed derivations and state the necessary assumptions.
For our main theoretical results, we present empirical support on both synthetic and real data. To
ensure empirical reproducibility, the results of each experiment and every ablation were averaged
over multiple seeds and are reported with standard deviations. All of the used datasets are either
public or can be generated from publicly available resources using the code that we provide in the
supplementary material. Information about implementation details, hyperparameter settings, and
evaluation metrics are included in Appendix C."
REFERENCES,0.26666666666666666,REFERENCES
REFERENCES,0.2691358024691358,"Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. (2017). Deep variational information
bottleneck. In International Conference on Learning Representations."
REFERENCES,0.2716049382716049,"Baltrušaitis, T., Ahuja, C., and Morency, L.-P. (2019). Multimodal machine learning: a survey and
taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):423–443."
REFERENCES,0.2740740740740741,"Borji, A. (2019). Pros and cons of GAN evaluation measures. Computer Vision and Image Under-
standing, 179:41–65."
REFERENCES,0.2765432098765432,"Choi, Y., Choi, M., Kim, M., Ha, J.-W., Kim, S., and Choo, J. (2018). StarGAN: uniﬁed generative
adversarial networks for multi-domain image-to-image translation. In Conference on Computer
Vision and Pattern Recognition."
REFERENCES,0.27901234567901234,"Cover, T. M. and Thomas, J. A. (2012). Elements of information theory. John Wiley & Sons."
REFERENCES,0.2814814814814815,"Daunhawer, I., Sutter, T. M., Marcinkevics, R., and Vogt, J. E. (2020). Self-supervised disentangle-
ment of modality-speciﬁc and shared factors improves multimodal generative models. In German
Conference on Pattern Recognition."
REFERENCES,0.2839506172839506,"Dorent, R., Joutard, S., Modat, M., Ourselin, S., and Vercauteren, T. (2019). Hetero-modal variational
encoder-decoder for joint modality completion and segmentation. In Medical Image Computing
and Computer Assisted Intervention, pages 74–82. Springer."
REFERENCES,0.28641975308641976,"Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and
Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing
systems."
REFERENCES,0.28888888888888886,"Gresele, L., Rubenstein, P. K., Mehrjou, A., Locatello, F., and Schölkopf, B. (2019). The incomplete
Rosetta stone problem: identiﬁability results for multi-view nonlinear ICA. In Conference on
Uncertainty in Artiﬁcial Intelligence."
REFERENCES,0.291358024691358,"Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). GANs trained
by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems."
REFERENCES,0.2938271604938272,"Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner,
A. (2017). beta-VAE: learning basic visual concepts with a constrained variational framework. In
International Conference on Learning Representations."
REFERENCES,0.2962962962962963,"Hsu, W.-N. and Glass, J. (2018). Disentangling by partitioning: a representation learning framework
for multimodal sensory data. arXiv preprint arXiv:1805.11264."
REFERENCES,0.29876543209876544,"Huang, X., Liu, M., Belongie, S. J., and Kautz, J. (2018). Multimodal unsupervised image-to-image
translation. In European Conference on Computer Vision."
REFERENCES,0.3012345679012346,Published as a conference paper at ICLR 2022
REFERENCES,0.3037037037037037,"Ilse, M., Tomczak, J. M., Louizos, C., and Welling, M. (2019). DIVA: domain invariant variational
autoencoders. arXiv preprint arXiv:1905.10427."
REFERENCES,0.30617283950617286,"Kingma, D. P. and Ba, J. (2015). Adam: a method for stochastic gradient descent. International
Conference on Learning Representations."
REFERENCES,0.30864197530864196,"Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. In International Conference
on Learning Representations."
REFERENCES,0.3111111111111111,"Kurle, R., Guennemann, S., and van der Smagt, P. (2019). Multi-source neural variational inference.
In AAAI Conference on Artiﬁcial Intelligence."
REFERENCES,0.3135802469135803,"LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324."
REFERENCES,0.3160493827160494,"Lee, C. and van der Schaar, M. (2021). A variational information bottleneck approach to multi-omics
data integration. In International Conference on Artiﬁcial Intelligence and Statistics."
REFERENCES,0.31851851851851853,"Lin, J., Men, R., Yang, A., Zhou, C., Ding, M., Zhang, Y., Wang, P., Wang, A., Jiang, L., Jia, X., et al.
(2021). M6: a chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823."
REFERENCES,0.32098765432098764,"Liu, M., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J., and Kautz, J. (2019). Few-shot
unsupervised image-to-image translation. In International Conference on Computer Vision."
REFERENCES,0.3234567901234568,"Locatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S., Schölkopf, B., and Bachem, O. (2019).
Challenging common assumptions in the unsupervised learning of disentangled representations. In
International Conference on Machine Learning."
REFERENCES,0.32592592592592595,"Locatello, F., Poole, B., Rätsch, G., Schölkopf, B., Bachem, O., and Tschannen, M. (2020). Weakly-
supervised disentanglement without compromises. In International Conference on Machine
Learning."
REFERENCES,0.32839506172839505,"Minoura, K., Abe, K., Nam, H., Nishikawa, H., and Shimamura, T. (2021). A mixture-of-experts deep
generative model for integrated analysis of single-cell multiomics data. Cell Reports Methods."
REFERENCES,0.3308641975308642,"Montero, M. L., Ludwig, C. J., Costa, R. P., Malhotra, G., and Bowers, J. (2021). The role of
disentanglement in generalisation. In International Conference on Learning Representations."
REFERENCES,0.3333333333333333,"Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A. Y. (2011). Multimodal deep learning.
In International Conference on Machine Learning."
REFERENCES,0.3358024691358025,"Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. (2019). On variational bounds of
mutual information. In International Conference on Machine Learning."
REFERENCES,0.33827160493827163,"Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021).
Zero-shot text-to-image generation. In International Conference on Machine Learning."
REFERENCES,0.34074074074074073,"Shi, Y., Paige, B., Torr, P., and Siddharth, N. (2021). Relating by contrasting: a data-efﬁcient frame-
work for multimodal generative models. In International Conference on Learning Representations."
REFERENCES,0.3432098765432099,"Shi, Y., Siddharth, N., Paige, B., and Torr, P. (2019). Variational mixture-of-experts autoencoders for
multi-modal deep generative models. In Advances in Neural Information Processing Systems."
REFERENCES,0.345679012345679,"Srivastava, N. and Salakhutdinov, R. (2014). Multimodal learning with deep Boltzmann machines.
Journal of Machine Learning Research, 15(1):2949–2980."
REFERENCES,0.34814814814814815,"Sutter, T. M., Daunhawer, I., and Vogt, J. E. (2020). Multimodal generative learning utilizing
Jensen-Shannon-divergence. In Advances in Neural Information Processing Systems."
REFERENCES,0.3506172839506173,"Sutter, T. M., Daunhawer, I., and Vogt, J. E. (2021). Generalized multimodal ELBO. In International
Conference on Learning Representations."
REFERENCES,0.3530864197530864,"Suzuki, M., Nakayama, K., and Matsuo, Y. (2016). Joint multimodal learning with deep generative
models. arXiv preprint arXiv:1611.01891."
REFERENCES,0.35555555555555557,Published as a conference paper at ICLR 2022
REFERENCES,0.35802469135802467,"Tsai, Y.-H. H., Liang, P. P., Zadeh, A., Morency, L.-P., and Salakhutdinov, R. (2019). Learning
factorized multimodal representations. In International Conference on Learning Representations."
REFERENCES,0.36049382716049383,"Tucker, G., Lawson, D., Gu, S., and Maddison, C. J. (2019). Doubly reparameterized gradient
estimators for Monte Carlo objectives. In International Conference on Learning Representations."
REFERENCES,0.362962962962963,"Vedantam, R., Fischer, I., Huang, J., and Murphy, K. (2018). Generative models of visually grounded
imagination. In International Conference on Learning Representations."
REFERENCES,0.3654320987654321,"von Kügelgen, J., Sharma, Y., Gresele, L., Brendel, W., Schölkopf, B., Besserve, M., and Locatello, F.
(2021). Self-supervised learning with data augmentations provably isolates content from style. In
Advances in Neural Information Processing Systems."
REFERENCES,0.36790123456790125,"Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. (2011). The Caltech-UCSD Birds-
200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology."
REFERENCES,0.37037037037037035,"Wieser, M., Parbhoo, S., Wieczorek, A., and Roth, V. (2020). Inverse learning of symmetry transfor-
mations. In Advances in Neural Information Processing Systems."
REFERENCES,0.3728395061728395,"Wu, M. and Goodman, N. (2018). Multimodal generative models for scalable weakly-supervised
learning. In Advances in Neural Information Processing Systems."
REFERENCES,0.37530864197530867,"Wu, M. and Goodman, N. D. (2019). Multimodal generative models for compositional representation
learning. arXiv preprint arXiv:1912.05075."
REFERENCES,0.37777777777777777,Published as a conference paper at ICLR 2022
REFERENCES,0.3802469135802469,"A
DEFINITIONS"
REFERENCES,0.38271604938271603,"Let X, Y, and Z denote the support sets of three discrete random vectors X, Y , and Z respectively.
Let pX(x), pY (y), and pZ(z) denote the respective marginal distributions and note that we will
leave out the subscripts (e.g., p(x) instead of pX (x)) when it is clear from context which distribution
we are referring to. Analogously, we write shorthand p(y | x) for the conditional distribution of Y
given X and p(x, y) for the joint distribution of X and Y ."
REFERENCES,0.3851851851851852,The entropy of X is deﬁned as
REFERENCES,0.38765432098765434,"H(X) = −
X"
REFERENCES,0.39012345679012345,"x∈X
p(x) log p(x) .
(6)"
REFERENCES,0.3925925925925926,The conditional entropy of X given Y is deﬁned as
REFERENCES,0.3950617283950617,"H(X | Y ) = −
X"
REFERENCES,0.39753086419753086,"x∈X,y∈Y
p(x, y) log p(x | y) .
(7)"
REFERENCES,0.4,The joint entropy of X and Y is deﬁned as
REFERENCES,0.4024691358024691,"H(X, Y ) = −
X"
REFERENCES,0.4049382716049383,"x∈X,y∈Y
p(x, y) log p(x, y) .
(8)"
REFERENCES,0.4074074074074074,"The Kullback-Leibler divergence of the discrete probability distribution P from the discrete probabil-
ity distribution Q is deﬁned as"
REFERENCES,0.40987654320987654,"DKL(P || Q) =
X"
REFERENCES,0.4123456790123457,"x∈X
P(x) log P(x)"
REFERENCES,0.4148148148148148,"Q(x)
(9)"
REFERENCES,0.41728395061728396,assuming that P and Q are deﬁned on the same support set X.
REFERENCES,0.41975308641975306,"The cross-entropy of the discrete probability distribution Q from the discrete probability distribution
P is deﬁned as
CE(P, Q) = −
X"
REFERENCES,0.4222222222222222,"x∈X
P(x) log Q(x)
(10)"
REFERENCES,0.4246913580246914,assuming that P and Q are deﬁned on the same support set X.
REFERENCES,0.4271604938271605,The mutual information of X and Y is deﬁned as
REFERENCES,0.42962962962962964,"I(X; Y ) = DKL(p(x, y) || p(x)p(y)) .
(11)"
REFERENCES,0.43209876543209874,The conditional mutual information of X and Y given Z is deﬁned as
REFERENCES,0.4345679012345679,"I(X; Y | Z) =
X"
REFERENCES,0.43703703703703706,"z∈Z
p(z)DKL(p(x, y | z) || p(x | z)p(y | z)) .
(12)"
REFERENCES,0.43950617283950616,"Recall that we assume discrete random vectors (e.g., pixel values) and therefore can assume non-
negative entropy, conditional entropy and conditional mutual information terms (Cover and Thomas,
2012). For continuous random variables, all of the above sums can be replaced with integrals. The
only information-theoretic quantities for which in this work we use continuous random vectors are
the KL-divergence and mutual information, both of which are always non-negative."
REFERENCES,0.4419753086419753,Published as a conference paper at ICLR 2022
REFERENCES,0.4444444444444444,"B
PROOFS"
REFERENCES,0.4469135802469136,"B.1
INFORMATION-THEORETIC DERIVATION OF THE MULTIMODAL ELBO"
REFERENCES,0.44938271604938274,"Proposition 1 relates the multimodal ELBO (Deﬁnition 1) to the expected log-evidence, the quantity
that is being approximated by all likelihood-based generative models including VAEs. The derivation
is based on a straightforward extension of the variational information bottleneck (VIB; Alemi et al.,
2017). We include the result mainly for the purpose of illustration—to clarify the notation, as well as
the relation between the multimodal ELBO and the underlying information-theoretic quantities of
interest: the entropy, conditional entropy, and mutual information."
REFERENCES,0.45185185185185184,"Notation
Readers who are familiar with latent variable models, but may be less familiar with the
information-theoretic perspective on VAEs, please keep in mind the following notational differences.
In contrast to the latent variable model perspective, which deﬁnes a variational posterior (typically
denoted by the letter q) and a stochastic decoder (typically denoted by the letter p), the VIB deﬁnes
a stochastic encoder pθ(z | x) and variational decoder qφ(x | z). Moreover, the VIB makes no
assumptions about the true posterior. Also note that latent variable models tend to write the ELBO
with respect to the log-evidence log p(x), but information-theoretic approaches write the ELBO with
respect to the expected log-evidence Ep(x)[log p(x)]; though, it is still assumed that the estimation of
the ELBO is based on a ﬁnite sample from p(x).
Proposition 1. The multimodal ELBO forms a variational lower bound on the expected log-evidence:"
REFERENCES,0.454320987654321,"Ep(x)[log p(x)] ≥L(x; θ, φ) .
(13)"
REFERENCES,0.4567901234567901,"Proof. First,
notice
that
the
expected
log-evidence
is
equal
to
the
negative
entropy
−H(X) = Ep(x)[log p(x)]. Given any random variable Z, the entropy can be decomposed into
conditional entropy and mutual information terms: H(X) = H(X | Z) + I(X; Z)."
REFERENCES,0.45925925925925926,"The expected log-evidence relates to the multimodal ELBO as follows:
Ep(x)[log p(x)] = −H(X | Z) −I(X; Z)
(14)"
REFERENCES,0.4617283950617284,"≥Ep(x)pθ(z | x)[log qφ(x | z)] −Ep(x)[DKL(pθ(z | x) || q(z))]
(15)"
REFERENCES,0.4641975308641975,"= L(x; θ, φ)
(16)
where the inequality follows from the variational approximations of the respective terms. As in Alemi
et al. (2017), we can use the following variational bounds."
REFERENCES,0.4666666666666667,"For the conditional entropy, we have
−H(X | Z) = Ep(x)pθ(z | x) [log p(x | z)]
(17)"
REFERENCES,0.4691358024691358,"= Ep(x)pθ(z | x) [log qφ(x | z)] + Ep(z) [DKL(p(x | z) || qφ(x | z))]
(18)"
REFERENCES,0.47160493827160493,"≥Ep(x)pθ(z | x) [log qφ(x | z)]
(19)
where qφ(x | z) is a variational decoder that is parameterized by φ."
REFERENCES,0.4740740740740741,"For the mutual information, we have
−I(X; Z) = −Ep(x) [DKL(pθ(z | x) || p(z))]
(20)"
REFERENCES,0.4765432098765432,"= −Ep(x) [DKL(pθ(z | x) || q(z))] + DKL(p(z) || q(z))
(21)"
REFERENCES,0.47901234567901235,"≥−Ep(x) [DKL(pθ(z | x) || q(z))]
(22)
where q(z) is a prior."
REFERENCES,0.48148148148148145,"Hence, the multimodal ELBO forms a variational lower bound on the expected log-evidence:
Ep(x)[log p(x)] = L(x; θ, φ) + ∆VA(x, φ)
(23)"
REFERENCES,0.4839506172839506,"≥L(x; θ, φ)
(24)
where
∆VA(x, φ) = Ep(z) [DKL(p(x | z) || qφ(x | z))] + DKL(p(z) || q(z))
(25)"
REFERENCES,0.48641975308641977,denotes the (non-negative) variational approximation gap.
REFERENCES,0.4888888888888889,Published as a conference paper at ICLR 2022
REFERENCES,0.49135802469135803,"B.2
RELATION BETWEEN THE DIFFERENT OBJECTIVES"
REFERENCES,0.49382716049382713,"Proposition 2 relates the multimodal ELBO L from Deﬁnition 1 to the objective LS, which is a
general formulation of the objective maximized by all mixture-based multimodal VAEs. Compared
to previous mixture-based formulations (Shi et al., 2019; Sutter et al., 2020), our formulation is more
general in that it allows for arbitrary subsets with non-uniform mixture coefﬁcients. Further, the
derivation quantiﬁes the approximation gap between L and LS, where the latter corresponds to the
objectives that are actually being optimized in the implementations of the MMVAE, MoPoE-VAE,
and MVAE without sub-sampling.
Proposition 2. For every stochastic encoder pS
θ (z | x) that is consistent with Deﬁnition 2, the
following inequality holds:
L(x; θ, φ) ≥LS(x; θ, φ) .
(26)"
REFERENCES,0.4962962962962963,Proof. Recall the multimodal ELBO from Deﬁnition 1:
REFERENCES,0.49876543209876545,"L(x; θ, φ) = Ep(x)pθ(z | x)[log qφ(x | z)] −Ep(x)[DKL(pθ(z | x) || q(z))] .
(27)"
REFERENCES,0.5012345679012346,"For the encoder pθ(z | x), plug in the mixture-based encoder pS
θ (z | x) = P
A∈S ωA pθ(z | xA)
from Deﬁnition 2 and re-write as follows:"
REFERENCES,0.5037037037037037,"Ep(x)pS
θ (z | x)[log qφ(x | z)] −Ep(x)[DKL(pS
θ (z | x) || q(z))]
(28)"
REFERENCES,0.5061728395061729,= Ep(x) P
REFERENCES,0.508641975308642,"A∈S ωA pθ(z | xA)[log qφ(x | z)] −
(29)"
REFERENCES,0.5111111111111111,Ep(x) P
REFERENCES,0.5135802469135803,"A∈S ωA pθ(z | xA)[log pS
θ (z | x) −log q(z)] =
X"
REFERENCES,0.5160493827160494,"A∈S
ωA

Ep(x)pθ(z | xA)[log qφ(x | z)] −Ep(x)pθ(z | xA)[log pS
θ (z | x)] +
(30)"
REFERENCES,0.5185185185185185,"Ep(x)pθ(z | xA)[log q(z)] =
X"
REFERENCES,0.5209876543209877,"A∈S
ωA

Ep(x)pθ(z | xA)[log qφ(x | z)] + Ep(x)[CE(pθ(z | xA), pS
θ (z | x))] −
(31)"
REFERENCES,0.5234567901234568,"Ep(x)[CE(pθ(z | xA), q(z))] =
X"
REFERENCES,0.5259259259259259,"A∈S
ωA

Ep(x)pθ(z | xA)[log qφ(x | z)] + Ep(x)[DKL(pθ(z | xA) || pS
θ (z | x))] −
(32)"
REFERENCES,0.528395061728395,"Ep(x)[DKL(pθ(z | xA) || q(z))] ≥
X"
REFERENCES,0.5308641975308642,"A∈S
ωA

Ep(x)pθ(z | xA)[log qφ(x | z)] −Ep(x)[DKL(pθ(z | xA) || q(z))]
	
(33)"
REFERENCES,0.5333333333333333,"= LS(x; θ, φ)
(34)"
REFERENCES,0.5358024691358024,"In Equation (31), CE(p, q) denotes the cross-entropy between distributions p and q. For Equation (32),
decompose both cross-entropy terms using CE(p, q) = H(p) + DKL(p || q) and notice that the
respective entropy terms cancel out. The inequality (Equation (33)) follows from the non-negativity
of the KL-divergence. This concludes the proof that LS(x; θ, φ) forms a lower bound on L(x; θ, φ)."
REFERENCES,0.5382716049382716,"Objectives of individual models
Sutter et al. (2021) already showed that Equation (28) subsumes
the objectives of the MMVAE, MoPoE-VAE, and MVAE without ELBO sub-sampling. However,
in their actual implementation, all of these methods take the sum out of the KL-divergence term
(e.g., see Shi et al., 2019, Equation 3), which corresponds to the objective LS. To see how LS
recovers the objectives of the individual models, simply plug in the model-speciﬁc deﬁnition of S
into Equation (33) and use uniform mixture coefﬁcients ωA = 1/|S| for all subsets. For the MVAE
without ELBO sub-sampling, S is comprised of only one subset, the complete set of modalities
{x1, . . . , xM}. For the MMVAE, S is comprised of the set of unimodal subsets {{x1}, . . . , {xM}}.
For the MoPoE-VAE, S is comprised of the powerset P(M) \ {∅}. Further implementation details,
such as importance sampling and ELBO sub-sampling, are discussed in Appendix C.3."
REFERENCES,0.5407407407407407,Published as a conference paper at ICLR 2022
REFERENCES,0.5432098765432098,"B.3
OBJECTIVE LS IS A SPECIAL CASE OF THE VIB"
REFERENCES,0.5456790123456791,"Lemma 1. LS(x; θ, φ) is a special case of the variational information bottleneck (VIB) objective min
ψ X"
REFERENCES,0.5481481481481482,"A∈S
ωA {Hψ(X | ZA) + Iψ(XA; ZA)} ,
(35)"
REFERENCES,0.5506172839506173,"where the encoding ZA = fψ(XA) is a function of a subset XA, the terms Hψ(X | ZA) and
Iψ(XA; ZA) denote variational upper bounds of H(X | ZA) and I(XA; ZA) respectively, and ψ
summarizes the parameters of these variational estimators."
REFERENCES,0.5530864197530864,"Proof. We start from LS, the objective optimized by all mixture-based multimodal VAEs. Recall
from Deﬁnition 3:"
REFERENCES,0.5555555555555556,"LS(x; θ, φ) =
X"
REFERENCES,0.5580246913580247,"A∈S
ωA
n
Ep(x)pθ(z | xA)[log qφ(x | z)]
|
{z
}
(i)"
REFERENCES,0.5604938271604938,"−Ep(x) [DKL (pθ(z | xA) || q(z))]
|
{z
}
(ii)"
REFERENCES,0.562962962962963,"o
. (36)"
REFERENCES,0.5654320987654321,"Each term within the sum is comprised of two terms: (i) the log-likelihood estimation based on a
variational decoder qφ(x | z); (ii) the regularization of the stochastic encoder pθ(z | xA) with respect
to a variational prior q(z). The sampled encoding z ∼pθ(z | xA) can be viewed as the output of a
function ZA = fθ(XA) of a subset of modalities."
REFERENCES,0.5679012345679012,"To see the relation to the underlying information terms H(X | ZA) and I(XA; ZA), we undo the
variational approximation for (i) and (ii) by re-introducing the unobserved ground truth decoder
p(x | z) and the ground truth prior p(z)."
REFERENCES,0.5703703703703704,"For (i), we have"
REFERENCES,0.5728395061728395,"Ep(x)pθ(z | xA) [log qφ(x | z)] ≤Ep(x)pθ(z | xA) [log qφ(x | z)] +
(37)"
REFERENCES,0.5753086419753086,Ep(z) [DKL(p(x | z) || qφ(x | z))]
REFERENCES,0.5777777777777777,"= Ep(x)pθ(z | xA) [log p(x | z)]
(38)"
REFERENCES,0.5802469135802469,"= −H(X | ZA)
(39)"
REFERENCES,0.582716049382716,"For (ii), we have"
REFERENCES,0.5851851851851851,"Ep(x) [DKL(pθ(z | xA) || q(z))] ≥Ep(x) [DKL(pθ(z | xA) || q(z))] −DKL(p(z) || q(z))
(40)"
REFERENCES,0.5876543209876544,"= Ep(x) [DKL(pθ(z | xA) || p(z))]
(41)"
REFERENCES,0.5901234567901235,"= I(XA; ZA)
(42)"
REFERENCES,0.5925925925925926,"Since LS(x; θ, φ) is being maximized, (i) is being maximized, while (ii) is being minimized. The
maximization of (i) is equal to the minimization of a variational upper bound on H(X | ZA).
Similarly, the minimization of (ii) is equal to the minimization of a variational upper bound on
I(XA; ZA). Hence, we have established that LS(x; θ, φ) is a special case of the more general VIB
objective (Equation (35)) where the information terms are estimated with a mixture-based multimodal
VAE that is parameterized by ψ = {θ, φ}."
REFERENCES,0.5950617283950618,"B.4
DECOMPOSITION OF THE CONDITIONAL ENTROPY FOR SUBSETS OF MODALITIES"
REFERENCES,0.5975308641975309,"Lemma 2. Let XA ⊆X be some subset of modalites. If ZA = f(XA), where f is some function of
the subset XA, then the following equality holds:"
REFERENCES,0.6,"H(X | ZA) = H(X{1,...,M}\A | XA) + H(XA | ZA) .
(43)"
REFERENCES,0.6024691358024692,"Proof. When ZA is a function of a subset XA
⊆
X, we have the Markov chain
ZA ←XA −−X{1,...,M}\A, since ZA is a function of the (observed) subset of modalities and
depends on the remaining (unobserved) modalities only through XA."
REFERENCES,0.6049382716049383,Published as a conference paper at ICLR 2022
REFERENCES,0.6074074074074074,"We can re-write H(X | ZA) as follows:
H(X | ZA) = H(X | ZA, XA) + I(X; XA | ZA)
(44)
= H(X | XA) + I(X; XA | ZA)
(45)
= H(X{1,...,M}\A | XA) + I(X; XA | ZA)
(46)"
REFERENCES,0.6098765432098765,"= H(X{1,...,M}\A | XA) + H(XA | ZA)
(47)
Equation (44) applies the deﬁnition of the conditional mutual information. Equation (45) is based on
the conditional independence X |="
REFERENCES,0.6123456790123457,"ZA | XA implied by the Markov chain. Equation (46) removes
the “known” information that we condition on. Finally, Equation (47) follows from XA ⊆X, which
implies that I(X; XA) = H(XA) and I(X; XA | ZA) = H(XA | ZA)."
REFERENCES,0.6148148148148148,"B.5
PROOF OF THEOREM 1"
REFERENCES,0.6172839506172839,"Theorem 1. Each mixture-based multimodal VAE (Deﬁnition 3) approximates the expected log-
evidence up to an irreducible discrepancy ∆(X, S) that depends on the model-speciﬁc mixture
distribution S as well as on the amount of modality-speciﬁc information in X."
REFERENCES,0.6197530864197531,"For the maximization of LS(x; θ, φ) and every value of θ and φ, the following inequality holds:
Ep(x)[log p(x)] ≥LS(x; θ, φ) + ∆(X, S)
(4)
where
∆(X, S) =
X"
REFERENCES,0.6222222222222222,"A∈S
ωA H(X{1,...,M}\A | XA) .
(5)"
REFERENCES,0.6246913580246913,"In particular, the generative discrepancy is always greater than or equal to zero and it is independent
of θ and φ and thus remains constant during the optimization."
REFERENCES,0.6271604938271605,"Proof. Lemma 1 shows that all mixture-based multimodal VAEs approximate the expected log-
evidence via the more general VIB objective min
ψ X"
REFERENCES,0.6296296296296297,"A∈S
ωA {Hψ(X | ZA) + Iψ(XA; ZA)}
(48)"
REFERENCES,0.6320987654320988,where the encoding ZA = fψ(XA) is a function of a subset XA ⊆X.
REFERENCES,0.6345679012345679,"The fact that ZA is a function of a subset, permits the following decomposition of the conditional
entropy (see Lemma 2):
H(X | ZA) = H(X{1,...,M}\A | XA) + H(XA | ZA) .
(49)"
REFERENCES,0.6370370370370371,"In particular, Equation (49) holds for every ZA = fψ(XA) and thus for every value ψ. Further, notice
that H(X{1,...,M}\A | XA) is independent of the learned encoding ZA and thus remains constant
during the optimization with respect to ψ."
REFERENCES,0.6395061728395062,"Hence, for every value ψ, the following inequality holds:
Hψ(X | ZA) ≥H(X | ZA)
(50)
≥H(X{1,...,M}\A | XA)
(51)"
REFERENCES,0.6419753086419753,"which means that the minimization of Hψ(X | ZA) is lower-bound by H(X{1,...,M}\A | XA), even
if Hψ(X | ZA) is a tight estimator of H(X | ZA)."
REFERENCES,0.6444444444444445,"Analogously, for the optimization of the VIB objective (Equation (48)), for every value ψ, the
following inequality holds:
X"
REFERENCES,0.6469135802469136,"A∈S
ωA {Hψ(X | ZA) + Iψ(XA; ZA)}
(52) ≥
X"
REFERENCES,0.6493827160493827,"A∈S
ωA {H(X | ZA) + Iψ(XA; ZA)}
(53) =
X"
REFERENCES,0.6518518518518519,"A∈S
ωA {H(XA | ZA) + Iψ(XA; ZA)} +
X"
REFERENCES,0.654320987654321,"A∈S
ωA H(X{1,...,M}\A | XA)"
REFERENCES,0.6567901234567901,"|
{z
}
∆(X,S) (54)"
REFERENCES,0.6592592592592592,Published as a conference paper at ICLR 2022
REFERENCES,0.6617283950617284,"where ∆(X, S) is independent of ψ and thus remains constant during the optimization. Consequently,
∆(X, S) represents an irreducible error for the optimization of the VIB objective."
REFERENCES,0.6641975308641975,"For mixture-based multimodal VAEs, Lemma 1 shows that LS(x; θ, φ) is a special case of the VIB
objective with ψ = (θ, φ). Hence, for every value of θ and φ, the following inequality holds:"
REFERENCES,0.6666666666666666,"Ep(x)[log p(x)] ≥LS(x; θ, φ) + ∆(X, S) .
(55)"
REFERENCES,0.6691358024691358,"The exact value of ∆(X, S) depends on the deﬁnition of the mixture distribution S, as well as on the
amount of modality-speciﬁc variation in the data. In particular, ∆(X, S) > 0, if there is any subset
A ∈S with ωA > 0 for which H(X{1,...,M}\A | XA) > 0."
REFERENCES,0.671604938271605,"B.6
PROOF OF COROLLARY 1"
REFERENCES,0.674074074074074,"Corollary 1. Without modality sub-sampling, ∆(X, S) = 0 ."
REFERENCES,0.6765432098765433,"Proof. Without modality sub-sampling, S is comprised of only one subset, the complete set of
modalities {1, . . . , M}, and therefore XA = X and X{1,...,M}\A = ∅. It follows that ∆(X, S) =
H(X{1,...,M}\A | XA) = H(∅| X) = 0, since the conditional entropy of the empty set is zero."
REFERENCES,0.6790123456790124,"B.7
PROOF OF COROLLARY 2"
REFERENCES,0.6814814814814815,"Corollary 2. For the MMVAE and MoPoE-VAE, the generative discrepancy increases given an
additional modality XM+1, if the new modality is sufﬁciently diverse in the following sense:"
REFERENCES,0.6839506172839506,"
1
|S+| −1 |S|  X"
REFERENCES,0.6864197530864198,"A∈S
I(X{1,...,M}\A; XM+1 | XA) <
1
|S+||S| X"
REFERENCES,0.6888888888888889,"A∈S
H(XA | XM+1) +
(56)"
REFERENCES,0.691358024691358,"1
|S+| X"
REFERENCES,0.6938271604938272,"A∈S
H(XM+1 | X)
(57)"
REFERENCES,0.6962962962962963,"where S denotes the model-speciﬁc mixture distribution over the set of subsets of modalities given
modalities X1, . . . , XM and S+ is the respective mixture distribution over the extended set of subsets
of modalities given X1, . . . , XM+1."
REFERENCES,0.6987654320987654,"Proof. Let XM+1 be the new modality, let X+ := {X1, . . . , XM+1} denote the extended set of
modalities, and let S+ denote the new mixture distribution over subsets given X+. Note that all
subsets from S are still contained in S+, but that S+ contains new subsets in addition to those in S.
Further, due to the re-weighting of mixture coefﬁcients, S+ can have different mixture coefﬁcients for
the subsets it shares with S. We denote by S−:= {(A, ω+
A) ∈S+ : A ̸∈S} the set of new subsets
and let ω+
A denote the new mixture coefﬁcients, where typically ωA ̸= ω+
A due to the re-weighting."
REFERENCES,0.7012345679012346,"We are interested in the change of the generative discrepancy, when we add modality XM+1:"
REFERENCES,0.7037037037037037,"∆(X+, S+) −∆(X, S)
(58) =
X"
REFERENCES,0.7061728395061728,"B∈S+
ω+
B H(X{1,...,M+1}\B | XB) −
X"
REFERENCES,0.7086419753086419,"A∈S
ωA H(X{1,...,M}\A | XA) .
(59)"
REFERENCES,0.7111111111111111,Published as a conference paper at ICLR 2022
REFERENCES,0.7135802469135802,"Re-write the right hand side in terms of subsets that are contained in both S and S+ and subsets that
are only contained in S+. For this, we decompose the ﬁrst term as follows
X"
REFERENCES,0.7160493827160493,"B∈S+
ω+
B H(X{1,...,M+1}\B | XB)
(60) =
X"
REFERENCES,0.7185185185185186,"A∈S
ω+
A H(X{1,...,M+1}\A | XA) +
X"
REFERENCES,0.7209876543209877,"B∈S−
ω+
B H(X{1,...,M+1}\B | XB)
(61) =
X"
REFERENCES,0.7234567901234568,"A∈S
ω+
A H(X{1,...,M}\A | XA) +
X"
REFERENCES,0.725925925925926,"A∈S
ω+
A H(XM+1 | X) +
(62) X"
REFERENCES,0.7283950617283951,"B∈S−
ω+
B H(X{1,...,M+1}\B | XB)
(63)"
REFERENCES,0.7308641975308642,where the last equation follows from
REFERENCES,0.7333333333333333,"H(X{1,...,M+1}\A | XA) = H(X{1,...,M}\A | XA) + H(XM+1 | XA, X{1,...,M}\A)
(64)"
REFERENCES,0.7358024691358025,"= H(X{1,...,M}\A | XA) + H(XM+1 | X) .
(65)"
REFERENCES,0.7382716049382716,"We can use the decomposition from Equation (63) to re-write the right hand side of Equation (59) by
collecting the corresponding terms for H(X{1,...,M}\A | XA):
X"
REFERENCES,0.7407407407407407,"A∈S
(ω+
A −ωA)H(X{1,...,M}\A | XA) +
X"
REFERENCES,0.7432098765432099,"A∈S
ω+
A H(XM+1 | X) + X"
REFERENCES,0.745679012345679,"B∈S−
ω+
B H(X{1,...,M+1}\B | XB) .
(66)"
REFERENCES,0.7481481481481481,"Notice that in Equation (66) only the ﬁrst term can be negative, due to the re-weighting of mixture
coefﬁcients for terms that do not contain XM+1. Hence, in the general case, the generative discrepancy
can only decrease, if the mixture coefﬁcients change in such a way that the ﬁrst term in Equation (66)
dominates the other two terms."
REFERENCES,0.7506172839506173,"For the relevant special case of uniform mixture weights, which applies to both the MMVAE and
MoPoE-VAE, we can further decompose Equation (66) into (i) information shared between X and
XM+1, and (ii) information that is speciﬁc to X or XM+1."
REFERENCES,0.7530864197530864,"Using uniform mixture coefﬁcients ωA =
1
|S| and ω+
A =
1
|S+| for all subsets, we can factor out the
coefﬁcients and re-write Equation (66) as follows:

1
|S+| −1 |S|  X"
REFERENCES,0.7555555555555555,"A∈S
H(X{1,...,M}\A | XA) +
1
|S+| X"
REFERENCES,0.7580246913580246,"A∈S
H(XM+1 | X) +"
REFERENCES,0.7604938271604939,"1
|S+| X"
REFERENCES,0.762962962962963,"B∈S−
H(X{1,...,M+1}\B | XB)
(67)"
REFERENCES,0.7654320987654321,"where the second term already denotes information that is speciﬁc to XM+1. Hence, we decompose
the ﬁrst and last terms corresponding to (i) and (ii)."
REFERENCES,0.7679012345679013,"For the ﬁrst term from Equation (67), we have

1
|S+| −1 |S|  X"
REFERENCES,0.7703703703703704,"A∈S
H(X{1,...,M}\A | XA)
(68)"
REFERENCES,0.7728395061728395,"=

1
|S+| −1 |S|  X A∈S"
REFERENCES,0.7753086419753087,"n
H(X{1,...,M}\A | XA, XM+1) + I(X{1,...,M}\A; XM+1 | XA)
o
. (69)"
REFERENCES,0.7777777777777778,"For the last term from Equation (67), we have
1
|S+| X"
REFERENCES,0.7802469135802469,"B∈S−
H(X{1,...,M+1}\B | XB)
(70)"
REFERENCES,0.782716049382716,"=
1
|S+|"
REFERENCES,0.7851851851851852,"n
H(X | XM+1) +
X"
REFERENCES,0.7876543209876543,"A∈S
1{(A∪{M+1})∈S−}H(X{1,...,M}\A | XA, XM+1)
o
(71)"
REFERENCES,0.7901234567901234,Published as a conference paper at ICLR 2022
REFERENCES,0.7925925925925926,where we can further decompose
REFERENCES,0.7950617283950617,"1
|S+|H(X | XM+1) =
1
|S+|"
REFERENCES,0.7975308641975308,"n
H(X | XA, XM+1) + I(X; XA | XM+1)
o
(72)"
REFERENCES,0.8,"=
1
|S+|"
REFERENCES,0.8024691358024691,"n
H(X | XA, XM+1) + H(XA | XM+1)
o
(73)"
REFERENCES,0.8049382716049382,"=
1
|S+||S| X A∈S"
REFERENCES,0.8074074074074075,"n
H(X{1,...,M}\A | XA, XM+1) + H(XA | XM+1)
o
. (74)"
REFERENCES,0.8098765432098766,"Collecting all corresponding terms from Equations (69), (71) and (74), we can re-write Equation (67)
as follows:

1
|S+| −1"
REFERENCES,0.8123456790123457,"|S| +
1
|S+||S|  X"
REFERENCES,0.8148148148148148,"A∈S
H(X{1,...,M}\A | XA, XM+1) +
(75)"
REFERENCES,0.817283950617284,"
1
|S+| −1 |S|  X"
REFERENCES,0.8197530864197531,"A∈S
I(X{1,...,M}\A; XM+1 | XA) +
(76)"
REFERENCES,0.8222222222222222,"1
|S+| X"
REFERENCES,0.8246913580246914,"A∈S
1{(A∪{M+1})∈S−}H(X{1,...,M}\A | XA, XM+1) +
(77)"
REFERENCES,0.8271604938271605,"1
|S+||S| X"
REFERENCES,0.8296296296296296,"A∈S
H(XA | XM+1) +
(78)"
REFERENCES,0.8320987654320988,"1
|S+| X"
REFERENCES,0.8345679012345679,"A∈S
H(XM+1 | X).
(79)"
REFERENCES,0.837037037037037,"For both the MMVAE and MoPoE, the ﬁrst and last terms cancel out, which can see by plugging in
the respective deﬁnitions of S into the above equation. Recall that for the MMVAE, S is comprised of
the set of unimodal subsets {{x1}, . . . , {xM}} and thus S+ is comprised of {{x1}, . . . , {xM+1}}.
For the MoPoE-VAE, S is comprised of the powerset P(M) \ {∅} and thus S+ is comprised of
the powerset P(M + 1) \ {∅}. Hence, for the MMVAE and MoPoE-VAE, we have shown that
∆(X+, S+) −∆(X, S) is equal to the following expression:

1
|S+| −1 |S|  X"
REFERENCES,0.8395061728395061,"A∈S
I(X{1,...,M}\A; XM+1 | XA) +
(80)"
REFERENCES,0.8419753086419753,"1
|S+||S| X"
REFERENCES,0.8444444444444444,"A∈S
H(XA | XM+1) +
1
|S+| X"
REFERENCES,0.8469135802469135,"A∈S
H(XM+1 | X)
(81)"
REFERENCES,0.8493827160493828,where the information is decomposed into:
REFERENCES,0.8518518518518519,"(i) information shared between X and XM+1 (term (80)), and"
REFERENCES,0.854320987654321,"(ii) information that is speciﬁc to X or XM+1 (the ﬁrst and second terms in (81) respectively),"
REFERENCES,0.8567901234567902,"and where only (i) can be negative since |S+| > |S|. This concludes the proof of Corollary 2, showing
that ∆(X+, S+) −∆(X, S) > 0, if XM+1 is sufﬁciently diverse in the sense that (ii) > (i)."
REFERENCES,0.8592592592592593,Published as a conference paper at ICLR 2022
REFERENCES,0.8617283950617284,"C
EXPERIMENTS"
REFERENCES,0.8641975308641975,"C.1
DESCRIPTION OF THE DATASETS"
REFERENCES,0.8666666666666667,"PolyMNIST
The PolyMNIST dataset, introduced in Sutter et al. (2021), combines the MNIST
dataset (LeCun et al., 1998) with crops from ﬁve different background images to create ﬁve synthetic
image modalities. Each sample from the data is a set of ﬁve MNIST images (with digits of the
same class) overlayed on 28 × 28 crops from ﬁve different background images. Figure 1a shows 10
samples from the PolyMNIST dataset; each column represents one sample and each row represents
one modality. The dataset provides a convenient testbed for the evaluation of generative coherence,
because by design only the digit information is shared between modalities."
REFERENCES,0.8691358024691358,"Translated-PolyMNIST
This new dataset is conceptually similar to PolyMNIST in that a digit
label is shared between ﬁve synthetic image modalities. The difference is that in the creation of the
dataset, we change the size and position of the digit, as shown in Figure 1b. Technically, instead of
overlaying a full-sized 28 × 28 MNIST digit on a patch from the respective background image, we
downsample the MNIST digit by a factor of two and place it at a random (x, y)-coordinate within
the 28 × 28 background patch. Conceptually, these transformations leave the shared information
between modalities (i.e., the digit label) unaffected and only serve to make it more difﬁcult to predict
the shared information across modalities on expectation."
REFERENCES,0.8716049382716049,"Caltech Birds (CUB)
The extended CUB dataset from Shi et al. (2019) is comprised of two
modalities, images and captions. Each image from Caltech-Birds (CUB-200-2011 Wah et al., 2011)
is coupled with 10 crowdsourced descriptions of the respective bird. Figure 1c shows ﬁve samples
from the dataset. It is important to note that we use the CUB dataset with real images, instead of the
simpliﬁed version based on precomputed ResNet-features that was used in Shi et al. (2019; 2021)."
REFERENCES,0.8740740740740741,"C.2
IMPLEMENTATION DETAILS"
REFERENCES,0.8765432098765432,"Our experiments are based on the publicly available code from Sutter et al. (2021), which already
provides an implementation of PolyMNIST. A notable difference in our implementation is that
we employ ResNet architectures, because we found that the previously used convolutional neural
networks did not have sufﬁcient capacity for the more complex datasets we use. For internal
consistency, we use ResNets for PolyMNIST as well. We have veriﬁed that there is no signiﬁcant
difference compared to the results from Sutter et al. (2021) when we change to ResNets."
REFERENCES,0.8790123456790123,"Hyperparameters
All models were trained using the Adam optimizer (Kingma and Ba, 2015) with
learning rate 5e-4 and a batch size of 256. For image modalities we estimate likelihoods using Laplace
distributions and for captions we employ one-hot categorical distributions. Models were trained for
500, 1000, and 150 epochs on PolyMNIST, Translated-PolyMNIST, and CUB respectively. Similar
to previous work, we use Gaussian priors and a latent space with 512 dimensions for PolyMNIST
and 64 dimensions for CUB. For a fair comparison, we reduce the latent dimensionality of unimodal
VAEs proportionally (wrt. the number of modalities) to control for capacity. For the β-ablations, we
use β ∈{3e-4, 3e-3, 3e-1, 1, 3, 9} and, in addition, 32 for CUB."
REFERENCES,0.8814814814814815,"Evaluation metrics
For the evaluation of generative quality, we use the Fréchet inception distance
(FID; Heusel et al., 2017), a standard metric for evaluating the quality of generated images. In
Appendix C.3, we also provide log-likelihoods and qualitative results for both images and captions.
To compute generative coherence, we adopt the deﬁnitions from previous works (Shi et al., 2019;
Sutter et al., 2021). Generative coherence requires annotation on what is shared between modalities;
for example, in both PolyMNIST and Translated-PolyMNIST the digit label is shared by design.
For a single generated example ˆxm ∼qφ(xm | z) from modality m, the generative coherence is
computed as the following indicator:"
REFERENCES,0.8839506172839506,"Coherence(ˆxm, y, gm) = 1{gm(ˆxm) = y}
(82)"
REFERENCES,0.8864197530864197,"where y is a ground-truth class label and gm is a pretrained classiﬁer (learned on the training data from
modality m) that outputs a predicted class label. To compute the conditional coherence accuracy,
we average the coherence values over a set of N conditionally generated examples, where N is"
REFERENCES,0.8888888888888888,Published as a conference paper at ICLR 2022
REFERENCES,0.891358024691358,"typically the size of the test set. In particular, when ˆxm ∼qφ(xm | z) is conditionally generated
from z ∼pθ(z | xA) such that A = {1, . . . , M} \ m, the metric is speciﬁed as the leave-one-out
conditional coherence accuracy, because the input consists of all modalities except the one that is
being generated. When it is clear from context which metric is used, we refer to the (leave-one-
out) conditional coherence accuracy simply as generative coherence. For PolyMNIST, we use the
pretrained digit classiﬁers that are provided in the publicly available code from Sutter et al. (2021) and
for Translated-PolyMNIST we train the classiﬁers from scratch with the same architectures that are
used for the VAE encoders. Notably, the new pretrained digit classiﬁers have a classiﬁcation accuracy
between 93.5–96.9% on the test set of the respective modality, which means that it is possible to
predict the digits fairly well with the given architectures."
REFERENCES,0.8938271604938272,"C.3
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.8962962962962963,"Linear classiﬁcation
Shi et al. (2019) propose linear classiﬁcation as a measure of latent fac-
torization, to judge the quality of learned representations and to assess how well the information
decomposes into shared and modality-speciﬁc features. Figure 6 shows the linear classiﬁcation accu-
racy on the learned representations. The results suggest that not only does the generative coherence
decline when we switch from PolyMNIST to Translated-PolyMNIST, but also the quality of the
learned representations. While a low classiﬁcation accuracy does not imply that there is no digit
information encoded in the latent representation (after all, digits show up in most self-reconstructions),
the result demonstrates that a linear classiﬁer cannot extract the digit information."
REFERENCES,0.8987654320987655,"Log-likelihoods and qualitative results
Figure 7 shows the generative quality in terms of joint
log-likelihoods. We observe a similar ranking of models as with FID, but we notice that the gap
between MVAE and MoPoE-VAE appears less pronounced. The reason for this discrepancy is that,
to be consistent with Sutter et al. (2021), we estimate joint log-likelihoods given all modalities—a
procedure that resembles reconstruction more than it does unconditional generation. It can be of
independent interest that log-likelihoods might overestimate the generative quality for unconditional
generation for certain types of models. Qualitative results for unconditional generation (Figure 9)
support the hypothesis that the presented log-likelihoods do not reﬂect the visible lack of generative
quality for the MoPoE-VAE. Further, qualitative results for conditional generation (Figure 10)
indicate a lack of diversity for both the MMVAE and MoPoE-VAE: even though we draw different
samples from the posterior, the respective conditionally generated samples (i.e., the ten samples along
each column) show little diversity in terms of backgrounds or writing styles."
REFERENCES,0.9012345679012346,"Figure 5: PolyMNIST with
ﬁve “repeated” modalities."
REFERENCES,0.9037037037037037,"Repeated modalities
To check if the generative quality gap is also
present when modalities have similar modality-speciﬁc variation, we
use PolyMNIST with “repeated” modalities generated from the same
background image (illustrated in Figure 5). We vary the number of
modalities from 2 to 5, but in contrast to the results from Figure 3, we
now use repeated modalities. Figure 11 conﬁrms that the generative
quality of both the MVAE and MoPoE-VAE deteriorates with each addi-
tional modality, even in this simpliﬁed setting with repeated modalities.
In comparison, the generative quality of the MVAE is much closer to
the unimodal VAE for any number of modalities. These results lend
further support to the theoretical statements from Corollaries 1 and 2."
REFERENCES,0.9061728395061729,"MMVAE with the ofﬁcial implementation
The empirical results of the MMVAE in Section 5 are
based on a simpliﬁed version of the model that was proposed by Shi et al. (2019). In particular, we
use the re-implementation from Sutter et al. (2021), which optimizes the standard ELBO and not the
doubly reparameterized ELBO gradient estimator (DReG, Tucker et al., 2019) with importance sam-
pling that is used in the ofﬁcial implementation from Shi et al. (2019). Further, the re-implementation
does not parameterize the prior but uses a ﬁxed, standard normal prior instead."
REFERENCES,0.908641975308642,"To verify that these implementation differences do not affect the core results—the generative quality
gap and the lack of coherence—we conducted experiments using the MMVAE with the ofﬁcial
implementation from Shi et al. (2019). Figure 12 shows the β-ablation for PolyMNIST and it
conﬁrms that there is still a clear gap in generative quality between the unimodal VAE and the
MMVAE when we use the ofﬁcial implementation. For Translated-PolyMNIST (not shown) the"
REFERENCES,0.9111111111111111,Published as a conference paper at ICLR 2022
REFERENCES,0.9135802469135802,"results are similar; in particular, we have veriﬁed that generative coherence for cross generation is
random, even if we limit the dataset to two modalities."
REFERENCES,0.9160493827160494,"MVAE with ELBO sub-sampling
For the MVAE, Wu and Goodman (2018) introduce ELBO
sub-sampling as an additional training strategy to learn the inference networks for different subsets of
modalities. In our notation, ELBO sub-sampling can be described by the following objective:"
REFERENCES,0.9185185185185185,"L(x; θ, φ) +
X"
REFERENCES,0.9209876543209876,"A∈S
L(xA; θ, φ)
(83)"
REFERENCES,0.9234567901234568,"where S denotes some set of subsets of modalities. Wu and Goodman (2018) experiment with
different choices for S, but throughout all of their experiments they use at least the set of unimodal
subsets {{x1}, . . . , {xM}}, which yields the following objective:"
REFERENCES,0.9259259259259259,"L(x; θ, φ) + M
X"
REFERENCES,0.928395061728395,"i=1
L(xi; θ, φ) .
(84)"
REFERENCES,0.9308641975308642,"It is important to note that the above objective differs from the objective optimized by all mixture-
based multimodal VAEs (Deﬁnition 3) in that there are no cross-modal reconstructions in Equa-
tion (84). As a consequence, ELBO sub-sampling puts more weight on the approximation of the
marginal distributions compared to the conditionals and therefore does not optimize a proper bound
on the joint distribution (Wu and Goodman, 2019)."
REFERENCES,0.9333333333333333,"Figure 13 shows the PolyMNIST β-ablation comparing MVAE with and without ELBO sub-sampling.
MVAE+ denotes the model with ELBO sub-sampling using objective (84). Notably, MVAE+ achieves
signiﬁcantly better generative coherence, while both models perform similarly in terms of generative
quality (both in terms of FID and joint log-likelihood). Hence, even though the MVAE+ optimizes
an incorrect bound on the joint distribution (Wu and Goodman, 2019), our results suggest that the
learned models behave quite similar in practice, which can be of independent interest for future work."
REFERENCES,0.9358024691358025,"(a) PolyMNIST
(b) Translated-PolyMNIST"
REFERENCES,0.9382716049382716,"Figure 6: Linear classiﬁcation of latent representations. For each model, linear classiﬁers were trained
on the joint embeddings from 500 randomly sampled training examples. Points denote the average
digit classiﬁcation accuracy of the respective classiﬁers. The results are averaged over three seeds
and the bands show one standard deviation respectively. Due to numerical instabilities, the MVAE
could not be trained with larger β values. For CUB, classiﬁcation performance cannot be computed,
because shared factors are not annotated."
REFERENCES,0.9407407407407408,Published as a conference paper at ICLR 2022
REFERENCES,0.9432098765432099,"(a) PolyMNIST
(b) Translated-PolyMNIST
(c) Caltech Birds (CUB)"
REFERENCES,0.945679012345679,"Figure 7: Joint log-likelihoods over a range of β values. Each point denotes the estimated joint log-
likelihood averaged over three different seeds and the bands show one standard deviation respectively.
Due to numerical instabilities, the MVAE could not be trained with larger β values."
REFERENCES,0.9481481481481482,"(a) X1
(b) X2
(c) X3
(d) X4
(e) X5"
REFERENCES,0.9506172839506173,"Figure 8: FID for modalities X1, . . . , X5. The top row shows all FIDs for PolyMNIST and the
bottom row for Translated-PolyMNIST respectively. Points denote the FID averaged over three seeds
and bands show one standard deviation respectively. Due to numerical instabilities, the MVAE could
not be trained with larger β values."
REFERENCES,0.9530864197530864,Published as a conference paper at ICLR 2022
REFERENCES,0.9555555555555556,"(a) unimodal VAE, β = 1
(b) MVAE, β = 1
(c) MMVAE, β = 1
(d) MoPoE-VAE, β = 1"
REFERENCES,0.9580246913580247,"(e) unimodal VAE, β = 0.3
(f) MVAE, β = 0.3
(g) MMVAE, β = 0.3
(h) MoPoE-VAE, β = 0.3"
REFERENCES,0.9604938271604938,"(i) unimodal VAE, β = 9
(j) MVAE, β = 9
(k) MMVAE, β = 9
(l) MoPoE-VAE, β = 9"
REFERENCES,0.9629629629629629,"(m) unimodal VAE, β = 9
(n) MVAE, β = 9
(o) MMVAE, β = 9
(p) MoPoE-VAE, β = 9"
REFERENCES,0.9654320987654321,"Figure 9: Qualitative results for the unconditional generation using prior samples. For PolyMNIST
(Subﬁgures (a) to (d)) and Translated-PolyMNIST (Subﬁgures (e) to (h)), we show 20 samples for
each modality. For CUB, we show 100 generated images (Subﬁgures (i) to (l)) and 100 generated
captions (Subﬁgures (m) to (p)) respectively. Best viewed zoomed and in color."
REFERENCES,0.9679012345679012,Published as a conference paper at ICLR 2022
REFERENCES,0.9703703703703703,"(a) MVAE, β = 1
(b) MMVAE, β = 1
(c) MoPoE-VAE, β = 1"
REFERENCES,0.9728395061728395,"(d) MVAE, β = 0.3
(e) MMVAE, β = 0.3
(f) MoPoE-VAE, β = 0.3"
REFERENCES,0.9753086419753086,"(g) MVAE, β = 9.0
(h) MMVAE, β = 9.0
(i) MoPoE-VAE, β = 9.0"
REFERENCES,0.9777777777777777,"(j) MVAE, β = 9.0
(k) MMVAE, β = 9.0
(l) MoPoE-VAE, β = 9.0"
REFERENCES,0.980246913580247,"Figure 10: Qualitative results for the conditional generation across modalities. For PolyMNIST
(Subﬁgures (a) to (c)) and Translated-PolyMNIST (Subﬁgures (d) to (f)), we show 10 conditionally
generated samples of modality X1 given the sample from modality X2 that is shown in the ﬁrst row
of the respective subﬁgure. For CUB, we show the generation of images given captions (Subﬁgures
(g) to (i)), as well as the generation of captions given images (Subﬁgures (j) to (l)). Best viewed
zoomed and in color."
REFERENCES,0.9827160493827161,Published as a conference paper at ICLR 2022
REFERENCES,0.9851851851851852,"(a) PolyMNIST
(b) Translated-PolyMNIST"
REFERENCES,0.9876543209876543,"Figure 11: Generative quality as a function of the number of modalities. In contrast to Figure 3, here
we “repeat” the same modality, to verify that the generative quality also declines when the modality-
speciﬁc variation of all modalities is similar. All models are trained with β = 1 on PolyMNIST and
β = 0.3 on Translated-PolyMNIST. The results are averaged over three seeds and all modalities; the
bands show one standard deviation respectively. For the unimodal VAE, which uses only a single
modality, the average and standard deviation are plotted as a constant."
REFERENCES,0.9901234567901235,"(a) FID
(b) Joint log-likelihood"
REFERENCES,0.9925925925925926,"Figure 12: PolyMNIST β-ablation using the ofﬁcial implementation of the MMVAE. In particular,
for both the MMVAE and the unimodal VAE, we use the DReG objective, importance sampling, as
well as a learned prior. Points denote the value of the respective metric averaged over three seeds and
bands show one standard deviation respectively."
REFERENCES,0.9950617283950617,"(a) FID
(b) Joint log-likelihood
(c) Generative coherence"
REFERENCES,0.9975308641975309,"Figure 13: PolyMNIST β-ablation, comparing MVAE with and without additional ELBO sub-
sampling. MVAE+ denotes the model with additional ELBO sub-sampling. Points denote the value
of the respective metric averaged over three seeds and bands show one standard deviation respectively."
