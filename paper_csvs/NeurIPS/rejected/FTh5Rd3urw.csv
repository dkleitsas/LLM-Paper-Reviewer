Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015479876160990713,"Existing incentive solutions for traditional Federated Learning (FL) only consider
1"
ABSTRACT,0.0030959752321981426,"individual clients’ contributions to a single global model. They are unsuitable for
2"
ABSTRACT,0.0046439628482972135,"clustered personalization, where multiple cluster-level models can exist. Moreover,
3"
ABSTRACT,0.006191950464396285,"they focus solely on providing monetary incentives and fail to address the need
4"
ABSTRACT,0.007739938080495356,"for personalized FL, overlooking the importance of enhancing the personalized
5"
ABSTRACT,0.009287925696594427,"model’s appeal to individual clients as a motivating factor for consistent partici-
6"
ABSTRACT,0.010835913312693499,"pation. In this paper, we first propose to treat incentivization and personalization
7"
ABSTRACT,0.01238390092879257,"as interrelated challenges and solve them with an incentive mechanism that fos-
8"
ABSTRACT,0.01393188854489164,"ters personalized learning. Second, unlike existing approaches that rely on the
9"
ABSTRACT,0.015479876160990712,"aggregator to perform client clustering, we propose to involve clients by allowing
10"
ABSTRACT,0.017027863777089782,"them to provide incentive-driven preferences for joining clusters based on their
11"
ABSTRACT,0.018575851393188854,"data distributions. Our approach enhances the personalized and cluster-level model
12"
ABSTRACT,0.020123839009287926,"appeal for self-aware clients with high-quality data leading to their active and con-
13"
ABSTRACT,0.021671826625386997,"sistent participation. Through evaluation, we show that we achieve an 8–45% test
14"
ABSTRACT,0.02321981424148607,"accuracy improvement of the cluster models, 3–38% improvement in personalized
15"
ABSTRACT,0.02476780185758514,"model appeal, and 31–100% increase in the participation rate, compared to a wide
16"
ABSTRACT,0.02631578947368421,"range of FL modeling approaches, including those that tackle data heterogeneity
17"
ABSTRACT,0.02786377708978328,"and learn personalized models.
18"
INTRODUCTION,0.029411764705882353,"1
Introduction
19"
INTRODUCTION,0.030959752321981424,"Training high-quality models using traditional distributed machine learning requires massive data
20"
INTRODUCTION,0.032507739938080496,"transfer from the data sources to a central location, which raises various communication, computation,
21"
INTRODUCTION,0.034055727554179564,"and privacy challenges. In response, Federated Learning (FL) [1–4] has emerged as a solution
22"
INTRODUCTION,0.03560371517027864,"to train models at the source, reducing privacy issues and addressing the need for high-quality
23"
INTRODUCTION,0.03715170278637771,"models. However, the success of FL relies on resolving various new challenges related to statistical
24"
INTRODUCTION,0.03869969040247678,"heterogeneity [5–10], scheduling [11–13], and incentive distribution [14–18]. Recent works have
25"
INTRODUCTION,0.04024767801857585,"focused on training personalized models [9,19–22] to overcome data heterogeneity challenges.
26"
INTRODUCTION,0.04179566563467492,"Among personalized Federated Learning (pFL) techniques, similarity-based approaches that use
27"
INTRODUCTION,0.043343653250773995,"clustering of clients at the aggregator have gained popularity [23–27]. These personalization solutions
28"
INTRODUCTION,0.04489164086687306,"fulfill the primary goal of overcoming data heterogeneity for specific cases. However, existing pFL
29"
INTRODUCTION,0.04643962848297214,"solutions do not include any incentive mechanism, which is crucial in FL to motivate participants
30"
INTRODUCTION,0.047987616099071206,"to contribute their data and computation resources. Existing incentive mechanisms [14,16,28] for
31"
INTRODUCTION,0.04953560371517028,"traditional FL cannot be applied to pFL techniques because they only consider the performance
32"
INTRODUCTION,0.05108359133126935,"contribution of clients towards training a single objective. In contrast, clients in pFL can be con-
33"
INTRODUCTION,0.05263157894736842,"tributing towards multiple objectives simultaneously [7,8,24,25,29,30]. Furthermore, traditional
34"
INTRODUCTION,0.05417956656346749,"incentive solutions only provide monetary benefits and do not consider increasing personalized
35"
INTRODUCTION,0.05572755417956656,"models’ appeal as an incentive for encouraging active and reliable participation of clients. Without
36"
INTRODUCTION,0.05727554179566564,"incentives, participants may provide low-quality data [14,16,18] or opt-out from participation1 [31],
37"
INTRODUCTION,0.058823529411764705,"leading to poorly performing pFL models [10,32,33], as shown with empirical evaluations in the later
38"
INTRODUCTION,0.06037151702786378,"section. Collaboration fairness [34,35] can also be ensured by appropriately rewarding contributions
39"
INTRODUCTION,0.06191950464396285,"and accounting for data heterogeneity [18,36].
40"
INTRODUCTION,0.06346749226006192,"In addition, since existing pFL techniques assume voluntary and consistent participation from clients,
41"
INTRODUCTION,0.06501547987616099,"the aggregator controls the client selection and training with limited knowledge of clients’ training
42"
INTRODUCTION,0.06656346749226007,"capacity, availability, frequency of new incoming data, clustering preferences, and performance
43"
INTRODUCTION,0.06811145510835913,"requirements from the trained personalized models. These factors can directly influence the motivation
44"
INTRODUCTION,0.0696594427244582,"of self-conscious clients to participate consistently. Our evaluation shows that this causes frequent
45"
INTRODUCTION,0.07120743034055728,"opt-outs from uninterested clients due to uninformed clustering decisions by the server and low
46"
INTRODUCTION,0.07275541795665634,"personalized model appeal (PMA)2, which leads to reduced pFL performance. We also show that
47"
INTRODUCTION,0.07430340557275542,"solving personalization and incentivization as interrelated challenges yield better outcomes for pFL
48"
INTRODUCTION,0.07585139318885449,"than solving them as separate problems. However, this requires new paradigms for clustered pFL
49"
INTRODUCTION,0.07739938080495357,"using data distribution information available to clients via their preferences and designing incentive
50"
INTRODUCTION,0.07894736842105263,"mechanisms for increasing pFL appeal to reduce client opt-outs.
51"
INTRODUCTION,0.0804953560371517,"In this paper, we propose PI-FL that combines clustering-based pFL with token-based incentivization.
52"
INTRODUCTION,0.08204334365325078,"Unlike previous works that control clustering from the server side, PI-FL allows clients to estimate
53"
INTRODUCTION,0.08359133126934984,"the importance of each cluster and send their preferences for joining them to the aggregator as bids.
54"
INTRODUCTION,0.08513931888544891,"To identify a cluster’s importance to a client we use the importance weight of the cluster model
55"
INTRODUCTION,0.08668730650154799,"as defined by FedSoft [25]. Clients also use the importance weights to perform weighted local
56"
INTRODUCTION,0.08823529411764706,"aggregation for single-shot personalization. This client-driven clustering approach results in accurate
57"
INTRODUCTION,0.08978328173374613,"clustering because clients can attain a global perspective from their own local dataset which is only
58"
INTRODUCTION,0.0913312693498452,"accessible to them and the importance weights information of each cluster. This allows them to make
59"
INTRODUCTION,0.09287925696594428,"informed decisions that the server cannot make, resulting in improved PMA and reduced opt-outs. To
60"
INTRODUCTION,0.09442724458204334,"incentivize clients for consistent participation, PI-FL motivates clients to join clusters with the clients
61"
INTRODUCTION,0.09597523219814241,"that are most similar to them, maximizing their contribution to the cluster and, in turn, their rewards.
62"
INTRODUCTION,0.09752321981424149,"Good quality cluster-level models then produce more appealing personalized models for each client.
63"
INTRODUCTION,0.09907120743034056,"The incentive mechanism treats clients as both providers and consumers. As a consumer, the client
64"
INTRODUCTION,0.10061919504643962,"tries to attain a certain level of personalized model appeal, so it pays the provider to spend resources
65"
INTRODUCTION,0.1021671826625387,"to participate in training for the said model in each round. Whereas as a provider, the client earns
66"
INTRODUCTION,0.10371517027863777,"a profit based on its marginal contribution to training the cluster models. The marginal contribution
67"
INTRODUCTION,0.10526315789473684,"is calculated with a Shapley Value approximation due to the large computational overhead of the
68"
INTRODUCTION,0.10681114551083591,"original algorithm [38–41].
69"
INTRODUCTION,0.10835913312693499,"Contributions.
Existing pFL solutions fail to include PMA as an incentive to maintain consistent
70"
INTRODUCTION,0.10990712074303406,"participation, resulting in increased opt-outs. To address this issue, we propose PI-FL as the first
71"
INTRODUCTION,0.11145510835913312,"contribution, which provides contribution-based incentives to achieve collaborative fairness and
72"
INTRODUCTION,0.1130030959752322,"maintain the cluster-level and personalized models’ appeal for clients to prevent opt-outs. Additionally,
73"
INTRODUCTION,0.11455108359133127,"PI-FL has the added advantage of creating personalized models for unseen clients with unknown
74"
INTRODUCTION,0.11609907120743033,"data distributions that perform similarly to seen clients without the need for training. Secondly, we
75"
INTRODUCTION,0.11764705882352941,"provide theoretical analysis and empirical verification of the benefits of including incentives with
76"
INTRODUCTION,0.11919504643962849,"personalization. Lastly, we empirically evaluate the performance of PI-FL and other pFL models.
77"
RELATED WORK,0.12074303405572756,"2
Related work
78"
RELATED WORK,0.12229102167182662,"Cluster-based pFL: Among the cluster-based pFL works most related to PI-FL are FedSoft [25],
79"
RELATED WORK,0.1238390092879257,"FedGroup [24], and [29]. FedSoft utilizes soft clustering on the basis of matching data distributions
80"
RELATED WORK,0.12538699690402477,"in clients with cluster models while FedGroup quantifies the similarities between clients’ gradients by
81"
RELATED WORK,0.12693498452012383,"calculating the Euclidean distance of decomposed cosine similarity metric and [29] finds the optimal
82"
RELATED WORK,0.12848297213622292,"personalization-generalization trade-off from the cluster model by solving a bi-level optimization
83"
RELATED WORK,0.13003095975232198,"problem. This work incurs clustering overhead at each iteration and does not consider the overlap of
84"
RELATED WORK,0.13157894736842105,"distribution between clients wherein each client is restricted to one cluster for each training round.
85"
RELATED WORK,0.13312693498452013,"Other cluster-based pFL models include IFCA [42] which proposes a framework for loss-based
86"
RELATED WORK,0.1346749226006192,"clustering of clients and [23] which proposes three approaches for personalization using clustering,
87"
RELATED WORK,0.13622291021671826,"data interpolation, and model interpolation.
88"
RELATED WORK,0.13777089783281735,"Other pFL models: Some pFL models propose meta-learning techniques that provide methods for
89"
RELATED WORK,0.1393188854489164,"rapid training of a personalized model. These include fine-tuning methods such as Per-FedAvg [43]
90"
RELATED WORK,0.14086687306501547,"1By “opt-out” we mean the clients voluntarily leave FL due to the lack of incentivization.
2Akin to global model appeal [37], we propose a new metric to measure the personalized model appeal."
RELATED WORK,0.14241486068111456,"and regularization of local models [44,45]. Others works [8,46] including FedALA [6], Ditto [30]
91"
RELATED WORK,0.14396284829721362,"and pFedMe [47] propose multi-task learning and model-interpolation [48] pFL models. FedFomo [7]
92"
RELATED WORK,0.14551083591331268,"suggests an adaptive local aggregation approach for personalization. FedProx [5] proposes a proximal
93"
RELATED WORK,0.14705882352941177,"term to improve the stability of FL. As per our knowledge, all of these pFL works lack qualities for
94"
RELATED WORK,0.14860681114551083,"attracting or sustaining long-term participation from self-conscious clients leading to an increase
95"
RELATED WORK,0.15015479876160992,"in opt-outs and low PMA. Moreover, most of these works require either require further training or
96"
RELATED WORK,0.15170278637770898,"re-clustering to adapt the personalized models for new incoming clients.
97"
RELATED WORK,0.15325077399380804,"Incentivized FL: FAIR [14] integrates a quality-aware incentive mechanism with model aggregation
98"
RELATED WORK,0.15479876160990713,"to improve global model quality and encourage the participation of high-quality learning clients.
99"
RELATED WORK,0.1563467492260062,"FedFAIM [18] proposes a fairness-based incentive mechanism to prevent free-riding and reward
100"
RELATED WORK,0.15789473684210525,"fairness with Shapley value-based client contribution calculation. [31] proposes an approach based
101"
RELATED WORK,0.15944272445820434,"on reputation and reverse auction theory which selects and rewards participants by combining the
102"
RELATED WORK,0.1609907120743034,"reputation and bids of the participants under a limited budget. [16] proposes an approach where
103"
RELATED WORK,0.16253869969040247,"clients decide whether to participate based on their own utilities (reward minus cost) modeled as a
104"
RELATED WORK,0.16408668730650156,"minority game with incomplete information. Other incentivized FL works include [15,17,34,49,50].
105"
RELATED WORK,0.16563467492260062,"All of these works propose standalone solutions to attract clients, however, none of them fulfill the
106"
RELATED WORK,0.16718266253869968,"design requirements to be used with any pFL models.
107"
RELATED WORK,0.16873065015479877,"Why existing incentive mechanisms cannot be applied directly to pFL frameworks?
108"
RELATED WORK,0.17027863777089783,"Existing FL incentivization schemes designed for motivating clients to contribute to a single global
109"
RELATED WORK,0.17182662538699692,"goal [14,16,18,31] may not be applicable to pFL frameworks due to the multi-dimensional goals and
110"
RELATED WORK,0.17337461300309598,"objectives involved. In pFL frameworks, multiple objectives must be optimized simultaneously, such
111"
RELATED WORK,0.17492260061919504,"as cluster and personalized models per client in cluster-based pFL [24,25,29] or global and local
112"
RELATED WORK,0.17647058823529413,"models per client in multi-task learning [20,30,45,47]. To encourage clients to contribute towards
113"
RELATED WORK,0.1780185758513932,"the multiple objectives in pFL frameworks, new incentive mechanisms need to be developed that are
114"
RELATED WORK,0.17956656346749225,"specifically tailored to their multi-objective nature. PI-FL uses clustering for pFL wherein the clusters
115"
RELATED WORK,0.18111455108359134,"memberships are changed after every R training rounds. PI-FL is different from these as it forms
116"
RELATED WORK,0.1826625386996904,"clear boundaries between multiple cluster models and improves shared learning between cluster
117"
RELATED WORK,0.18421052631578946,"similarities through multiple participation at the client level. PI-FL incorporates maintaining PMA
118"
RELATED WORK,0.18575851393188855,"for consistent client participation with an incentive mechanism that directly motivates personalized
119"
RELATED WORK,0.1873065015479876,"training on the basis of Individual Rationality (IR) constraint of game theory [14,51].
120"
PROPOSED METHODOLOGY,0.18885448916408668,"3
Proposed Methodology
121"
PROPOSED METHODOLOGY,0.19040247678018576,"Profiler
Scheduler
Token
Manager"
PROPOSED METHODOLOGY,0.19195046439628483,"Tier
Preferences"
PROPOSED METHODOLOGY,0.19349845201238391,Create tiers
PROPOSED METHODOLOGY,0.19504643962848298,Token bids
PROPOSED METHODOLOGY,0.19659442724458204,Pay Provider
PROPOSED METHODOLOGY,0.19814241486068113,"Reimburse
Consumer bids"
PROPOSED METHODOLOGY,0.1996904024767802,"Participation History
& Marginal Contributions"
PROPOSED METHODOLOGY,0.20123839009287925,Figure 1: PI-FL design
PROPOSED METHODOLOGY,0.20278637770897834,"In this section, we introduce PI-FL, which has three main modules:
122"
PROPOSED METHODOLOGY,0.2043343653250774,"the profiler, the token manager, and the scheduler as shown by
123"
PROPOSED METHODOLOGY,0.20588235294117646,"the architecture diagram in Figure 1. The profiler calculates and
124"
PROPOSED METHODOLOGY,0.20743034055727555,"maintains the history of client contributions using Shapley Values
125"
PROPOSED METHODOLOGY,0.2089783281733746,"approximation (lines 24-27) of Algorithm 1. The profiler also aids
126"
PROPOSED METHODOLOGY,0.21052631578947367,"the scheduler in forming clusters using two different modes further
127"
PROPOSED METHODOLOGY,0.21207430340557276,"explained in section 3.1. The token manager orchestrates transactions, holds auctions, deducts
128"
PROPOSED METHODOLOGY,0.21362229102167182,"payments, and distributes rewards as given in lines (13 and 14). The scheduler selects clients based
129"
PROPOSED METHODOLOGY,0.21517027863777088,"on bids and contributions, grouping them for improved homogeneity shown in lines (20 and 27-29).
130"
PROPOSED METHODOLOGY,0.21671826625386997,"Individual clients calculate the importance weights of each aggregated cluster model and send their
131"
PROPOSED METHODOLOGY,0.21826625386996903,"preference bids to the Token Manager for joining clusters as shown in lines (23-28) in Algorithm 2.
132"
PROPOSED METHODOLOGY,0.21981424148606812,"Clients also generate a single-shot personalized model, shown in line 29. We assume that each client
133"
PROPOSED METHODOLOGY,0.22136222910216719,"will look to maximize their profits according to the principle of Individual Rationality (IR) [10,52]
134"
PROPOSED METHODOLOGY,0.22291021671826625,"and this will lead them to choose clusters in which they can contribute the most for maximum reward.
135"
PROFILER,0.22445820433436534,"3.1
Profiler
136"
PROFILER,0.2260061919504644,"At the start of pFL training, the scheduler module forms the initial clusters by randomly assigning
137"
PROFILER,0.22755417956656346,"clients. Then for each round, clients train the cluster-level model on their local data and calculate
138"
PROFILER,0.22910216718266255,"the importance weight of each aggregated cluster model Mk on their local dataset via Equation 1.
139"
PROFILER,0.2306501547987616,"Here υck is the normalized sum of correctly predicted data points nck on local dataset Dc of client c.
140"
PROFILER,0.23219814241486067,"The importance weights are used to generate a single-shot personalized model through the weighted
141"
PROFILER,0.23374613003095976,"aggregation of cluster-level models using Equation 2.
142"
PROFILER,0.23529411764705882,"υck = nck/nk ∈[0, 1] | k ∈[K]
(1) 143"
PROFILER,0.23684210526315788,"Algorithm 1 PI-FL (Server)
Input: R: Rounds, Pr: Pre-training rounds, K: Number of clusters, Mk: Cluster-level model of
cluster k ∈K, MG: Global model at aggregator, N: Number of clients, C: Number of classes
in dataset, ζa: Available Clients, Np: Number of clients to select on basis of performance,
Nr: Number of clients to select randomly for each cluster, ζk: Clients selected for training in
cluster k ∈K, FedAvg: [2], F1-Scores: [53], sort(): Python 3.7 Timsort implementation [54]"
PROFILER,0.23839009287925697,1 for each round r ∈R do
PROFILER,0.23993808049535603,"2
ζk = SelectClients(r) for each cluster k ∈K"
PROFILER,0.24148606811145512,"3
for cluster k ∈K do"
PROFILER,0.24303405572755418,"4
Server sends cluster-level model Mk for training to clients in ζk
5
Token Manager collects bid payments from all willing clients via Eqn. 4"
PROFILER,0.24458204334365324,"6
Token manager updates available tokens for round r via Eqn. 5"
PROFILER,0.24613003095975233,"7
Uk ←model updates received from clients in ζk
8
Mk = FedAvg(Uk)"
PROFILER,0.2476780185758514,9 Function SelectClients(r)
PROFILER,0.24922600619195046,"10
if r = 0 then"
PROFILER,0.25077399380804954,"11
for k = 1 to K do"
PROFILER,0.25232198142414863,"12
ζ∗
k ←Scheduler randomly assigns clients from ζa."
PROFILER,0.25386996904024767,"13
return ζ∗
k
14
else if r > 1 then"
PROFILER,0.25541795665634676,"15
for i = 1 to N do"
PROFILER,0.25696594427244585,"16
θi ←ClientPreferences(M1, ..., Mk) | ∀k ∈[K] // from Algorithm 2"
PROFILER,0.2585139318885449,"17
Server calculates marginal contributions ψki of each client within its cluster with Shapley
Values approximation in Algorithm 3 | ∀k ∈[K], ∀i ∈[N]"
PROFILER,0.26006191950464397,"18
//Profiler sorts clients on the basis of their marginal contributions and preference bids"
PROFILER,0.26160990712074306,"19
Sc = sort(θi, ψki)"
PROFILER,0.2631578947368421,"20
for k = 1 to K do"
PROFILER,0.2647058823529412,"21
ζ∗
k ←Np clients selected from Sc and Nr clients randomly from ζa by Scheduler."
PROFILER,0.26625386996904027,"22
return ζ∗
k Pck = K
X"
PROFILER,0.2678018575851393,"k=1
υck × (ωk)
(2)"
PROFILER,0.2693498452012384,"Here the Pck is the personalized model of client c in cluster k and ωk is the weight vector of k cluster
144"
PROFILER,0.2708978328173375,"model. Using this, clients generate single-shot personalized models offline according to their dynamic
145"
PROFILER,0.2724458204334365,"data needs. The client-centric clustering and participation method enhances the appeal of pFL for
146"
PROFILER,0.2739938080495356,"clients and in doing so also provides them the opportunity to customize their personalized model
147"
PROFILER,0.2755417956656347,"offline in case their requirements which are unknown to the server change during training. Clients can
148"
PROFILER,0.2770897832817337,"also make informed decisions on participating in training clusters based on their budget and past re-
149"
PROFILER,0.2786377708978328,"wards, using importance weights and knowledge of previous rounds. They convey their preferences to
150"
PROFILER,0.2801857585139319,"the aggregator by submitting bids for the cluster they wish to participate in for the next training round.
151"
PROFILER,0.28173374613003094,"The profiler calculates the marginal contributions of each client after every round using Shapley
152"
PROFILER,0.28328173374613,"Values approximation (Algorithm 3), aiding scheduling by providing data quality information to
153"
PROFILER,0.2848297213622291,"the scheduler. The Shapley Value approximation derivation from Appendix is used to avoid the
154"
PROFILER,0.28637770897832815,"computational expense of calculating Shapley Values for multiple clients. PI-FL also includes a mode
155"
PROFILER,0.28792569659442724,"to facilitate clients to form well-defined initial clusters. So the clients can avoid the decision-making
156"
PROFILER,0.2894736842105263,"process in the beginning and streamline their spending when the client contributions and cluster
157"
PROFILER,0.29102167182662536,"distributions are unclear. For this, the profiler and the scheduler module facilitate forming the initial
158"
PROFILER,0.29256965944272445,"clusters by training for some pre-training rounds. This is done as client contributions and similarity
159"
PROFILER,0.29411764705882354,"metrics that the clients use among other metrics to make decisions about joining clusters are initially
160"
PROFILER,0.29566563467492263,"unknown. After pre-training, the profiler calculates per-class F1-Scores ξ of all client local models
161"
PROFILER,0.29721362229102166,"on an IID test dataset [53]. Then the profiler with the help of scheduler clusters clients for the next
162"
PROFILER,0.29876160990712075,"training round using the K-Means clustering [55] algorithm with the most varying F1-scores VF 1
163"
PROFILER,0.30030959752321984,"from C total classes. Equation 3 shows the calculation of VF 1 where C is the number of total classes
164"
PROFILER,0.3018575851393189,"and N is the number of all available clients.
165"
PROFILER,0.30340557275541796,"VF 1 = var(ξi) ∈[1, C] | ∀i ∈N
(3)"
PROFILER,0.30495356037151705,"We perform all our evaluations for PI-FL without this feature, but this is an added feature that PI-FL in-
166"
PROFILER,0.3065015479876161,"cludes for faster convergence and to save clients’ costs. We also realize the constraints in choosing all
167"
PROFILER,0.3080495356037152,"the clients for training, which is why clients that reply within threshold time in pre-training rounds are
168"
PROFILER,0.30959752321981426,"used to calculate F1 scores. The remaining clients are considered unexplored and assigned to clusters
169"
PROFILER,0.3111455108359133,"randomly, they can later settle into appropriate clusters through preference and contribution selection.
170"
PROFILER,0.3126934984520124,"Algorithm 2 PI-FL (Client)
Input: Th: Importance weight threshold, K: Number of clusters, Mk: Cluster-level model of cluster
k ∈K, D: Local dataset of client,"
PROFILER,0.3142414860681115,"23 Function ClientPreferences(M1, ..., Mk)"
PROFILER,0.3157894736842105,"24
for each cluster k ∈K do"
PROFILER,0.3173374613003096,"25
for each data point d ∈D do"
PROFILER,0.3188854489164087,"26
The client computes υk importance weight of Mk model for each data point d via Eqn. 1"
PROFILER,0.3204334365325077,"27
if υk > Th then"
PROFILER,0.3219814241486068,"28
Client adds cluster k to client’s preference bids list θ∗
i"
PROFILER,0.3235294117647059,"29
The client generates personalized model Pck via Eqn. 2"
PROFILER,0.32507739938080493,"30
return θ∗
i"
TOKEN MANAGER,0.326625386996904,"3.2
Token Manager
171"
TOKEN MANAGER,0.3281733746130031,"The token manager acts as a bank to orchestrate and keep track of transactions between different
172"
TOKEN MANAGER,0.32972136222910214,"clients. At the start of each training round the token manager holds an auction for each cluster, and
173"
TOKEN MANAGER,0.33126934984520123,"the clients that want to participate in that cluster place their bids using tokens. The token manager
174"
TOKEN MANAGER,0.3328173374613003,"forwards the list of willing clients to the scheduler to select clients for training. It also deducts
175"
TOKEN MANAGER,0.33436532507739936,"payments from the willing clients/consumers via Equation 4. Here τi is the tokens owned by client i,
176"
TOKEN MANAGER,0.33591331269349844,"ζk are the clients willing to join cluster k, and τp in this and all following Equations is the per round
177"
TOKEN MANAGER,0.33746130030959753,"bid amount to be paid by each client for participation.
178"
TOKEN MANAGER,0.33900928792569657,"τi = τi −τp | i ∈ζ∗
k
(4)"
TOKEN MANAGER,0.34055727554179566,"The tokens collected as payments from clients/consumers are then added to the available pool of tokens
179"
TOKEN MANAGER,0.34210526315789475,"at the Token Manager as shown in Equation 5. Here τar are the total available pool of tokens at the
180"
TOKEN MANAGER,0.34365325077399383,"Token Manager. The term Np is the number of clients selected on basis of performance and Nr is the
181"
TOKEN MANAGER,0.34520123839009287,"number of clients selected randomly. The significance of using Np and Nr is explained in section 3.3.
182"
TOKEN MANAGER,0.34674922600619196,"τar = τar + (Np + Nr) × τp | r ∈[1, R]
(5)"
TOKEN MANAGER,0.34829721362229105,"The token manager handles the distribution of reimbursement and rewards to each provider/client.
183"
TOKEN MANAGER,0.3498452012383901,"Reimbursement penalizes degradation in the performance of providers and depends on the utility
184"
TOKEN MANAGER,0.35139318885448917,"function. The utility is calculated as the percentage of average accuracy improvement of the cluster
185"
TOKEN MANAGER,0.35294117647058826,"model Mk over the maximum achieved accuracy in past rounds on the local data of clients in cluster
186"
TOKEN MANAGER,0.3544891640866873,"k. The utility function is given in Equation 6 and reimbursement calculation is given in Equation
187"
TOKEN MANAGER,0.3560371517027864,"7, both metrics are calculated at the profiler which assists the token manager in reimbursement.
188"
TOKEN MANAGER,0.35758513931888547,"θ =
η × (γ −min(γ, max(0.0, (Acckr−Acckmax)"
TOKEN MANAGER,0.3591331269349845,"Acckmax
)))"
TOKEN MANAGER,0.3606811145510836,"γ
| η ∈[0, 1], γ ∈[0, 1]
(6) 189"
TOKEN MANAGER,0.3622291021671827,"τi = τi −τar × θ | θ ∈[0, γ], ∀i ∈[N], ∀r ∈[1, R]
(7)"
TOKEN MANAGER,0.3637770897832817,"In Equation 6, Acckr is the cluster-level model accuracy in the current round r and Acckmax is the
190"
TOKEN MANAGER,0.3653250773993808,"maximum cluster-level model accuracy achieved until the current round r. The term η represents the
191"
TOKEN MANAGER,0.3668730650154799,"maximum portion of tokens that can be returned and γ represents the maximum accuracy improvement
192"
TOKEN MANAGER,0.3684210526315789,"that leads to the use of one full token. In Equation 7, τar are the total number of tokens collected
193"
TOKEN MANAGER,0.369969040247678,"from consumers/clients for r training round. We have used a similar approach to [28], however, they
194"
TOKEN MANAGER,0.3715170278637771,"use the accuracy of the FedAvg model on an IID dataset. It is not practical to assume the presence of
195"
TOKEN MANAGER,0.37306501547987614,"an IID dataset that can correspond to the data distribution of clients within a cluster which is why we
196"
TOKEN MANAGER,0.3746130030959752,"rely on the local dataset of clients within that cluster to gather this information.
197"
TOKEN MANAGER,0.3761609907120743,"τi = τi + sort(ψki, Ωki) ×
τar
Nr × (Nr+1)"
TOKEN MANAGER,0.37770897832817335,"2
| ∀k ∈[K], ∀i ∈[N], ∀r ∈[R]
(8)"
TOKEN MANAGER,0.37925696594427244,"After reimbursement, the token manager uses the marginal contributions calculated by the profiler
198"
TOKEN MANAGER,0.38080495356037153,"and sorts providers/clients by their contributions and participation record in Equation 8. Here ψki
199"
TOKEN MANAGER,0.38235294117647056,"represents the marginal contributions and Ωki represents the participation records of all clients N
200"
TOKEN MANAGER,0.38390092879256965,"in K clusters. The term β is a normalizing term from Equation 8 in which Nr are the number of
201"
TOKEN MANAGER,0.38544891640866874,"providers selected for participation in round r. Using the ranks α of providers from sorting and
202"
TOKEN MANAGER,0.38699690402476783,"the normalization term β, the remaining available tokens are distributed between these providers
203"
TOKEN MANAGER,0.38854489164086686,"in Equation 8. Here τi represents the tokens owned by provider/client i and τar are the tokens
204"
TOKEN MANAGER,0.39009287925696595,"available for incentive distribution at the token manager. Through reimbursements to consumers and
205"
TOKEN MANAGER,0.39164086687306504,"payments to providers, the Token Manager ensures that each client receives an incentive according
206"
TOKEN MANAGER,0.3931888544891641,"to their contributions in training the pFL models. By doing so, PI-FL incentivizes improvement in
207"
TOKEN MANAGER,0.39473684210526316,"personalized learning, resulting in an enhancement of PMA and a decrease in opt-outs.
208"
SCHEDULER,0.39628482972136225,"3.3
Scheduler
209"
SCHEDULER,0.3978328173374613,"The scheduler selects clients for each round r by the SelectClients(r) function given in Algorithm
210"
SCHEDULER,0.3993808049535604,"1. The scheduler receives the preference bids θi from the token manager, the marginal contributions
211"
SCHEDULER,0.40092879256965946,"ψki from the profiler for each client i ∈N in cluster k ∈K, where N is the total number of clients
212"
SCHEDULER,0.4024767801857585,"and K are the total number of clusters. Using this information scheduler groups clients with similar
213"
SCHEDULER,0.4040247678018576,"preference bids and then sorts those clients by their marginal contributions. Then the scheduler
214"
SCHEDULER,0.4055727554179567,"selects Np number of clients from the sorted clients and Nr number of clients randomly. Both Np
215"
SCHEDULER,0.4071207430340557,"and Nr are tunable parameters. To reduce bias, a small portion of clients Nr are selected randomly
216"
SCHEDULER,0.4086687306501548,"which is a technique adopted from previous works [2,28,56,57]. By grouping clients with similar
217"
SCHEDULER,0.4102167182662539,"preferences the scheduler reduces the within-cluster bias improving the within-cluster homogeneity
218"
SCHEDULER,0.4117647058823529,"and a cluster model is produced that accurately represents the clients within it. Section 4 gives a
219"
SCHEDULER,0.413312693498452,"theoretical analysis of how this is an important factor in improving the PMA.
220"
THEORETICAL ANALYSIS,0.4148606811145511,"4
Theoretical Analysis
221"
THEORETICAL ANALYSIS,0.41640866873065013,"We study the following particular case to develop insights. Suppose there are m clients in total,
222"
THEORETICAL ANALYSIS,0.4179566563467492,"each observing a set of independent Gaussian observations zi,j ∼N(µi, σ2), j = 1, . . . , ni, with a
223"
THEORETICAL ANALYSIS,0.4195046439628483,"personalized task of estimating its unknown mean µ ∈R. The quality of the learning result, denoted
224"
THEORETICAL ANALYSIS,0.42105263157894735,"by ˆµ, will be assessed by the mean squared error Ei(ˆµ −µ)2, where the expectation Ei is taken with
225"
THEORETICAL ANALYSIS,0.42260061919504643,"respect to the distribution of client i.
226"
THEORETICAL ANALYSIS,0.4241486068111455,"It is conceivable that if clients’ underlying parameters µi’s are arbitrarily given, personalized FL
227"
THEORETICAL ANALYSIS,0.42569659442724456,"may not boost the local learning result. To highlight the potential benefit of cluster-based modeling,
228"
THEORETICAL ANALYSIS,0.42724458204334365,"we suppose that the m clients can be partitioned into two subsets: one with m1 clients, say T1 =
229"
THEORETICAL ANALYSIS,0.42879256965944273,"{1, . . . , m1}, and the other with m2 clients, say T2 = {m1+1, . . . , m}, whose underlying parameters
230"
THEORETICAL ANALYSIS,0.43034055727554177,"are randomly generated in the following way:
231"
THEORETICAL ANALYSIS,0.43188854489164086,"µi ∼N(β1, τ 2) |
i ∈T1,
µi ∼N(β2, τ 2) |
i ∈T2.
(9)
Here, β1 and β2 can be treated as the root cause of two underlying clusters. We will study how
232"
THEORETICAL ANALYSIS,0.43343653250773995,"the values of sample size ni, data variation σ, within-cluster similarity as quantified by τ, and
233"
THEORETICAL ANALYSIS,0.43498452012383904,"cross-cluster similarity as quantified by |β1 −β2| will influence the gain of a client in personalized
234"
THEORETICAL ANALYSIS,0.43653250773993807,"learning. To simplify the discussion, we will assess the learning quality (based on the mean squared
235"
THEORETICAL ANALYSIS,0.43808049535603716,"error) of any particular client i in the following three procedures:
236"
THEORETICAL ANALYSIS,0.43962848297213625,"Local training: Client i only performs local learning by minimizing the local loss Li(µ) =
237
Pni
j=1(µ −zi,j)2, and obtains ˆµi = n−1
i
Pni
j=1 zi,j. Thus, the corresponding error is
238"
THEORETICAL ANALYSIS,0.4411764705882353,e(ˆµi) = Ei(ˆµi −µ1)2 = σ2
THEORETICAL ANALYSIS,0.44272445820433437,"ni
.
(10)"
THEORETICAL ANALYSIS,0.44427244582043346,"Federated training:
Suppose the FL converges to the global minimum of the loss,
239
Pm
i=1
ni"
THEORETICAL ANALYSIS,0.4458204334365325,"n Li(µ),
n
∆= Pm
i=1 ni, which can be calculated to be ˆµFL = Pm
i=1
ni"
THEORETICAL ANALYSIS,0.4473684210526316,"n ˆµi. Consider
240"
THEORETICAL ANALYSIS,0.44891640866873067,"any particular client i. Without loss of generality, suppose it belongs to cluster 1, namely i ∈T1.
241"
THEORETICAL ANALYSIS,0.4504643962848297,"From the client i’s angle, conditional on its local µi and assuming a flat prior on β1 and β2, client j’s
242"
THEORETICAL ANALYSIS,0.4520123839009288,"µj follows µj | µi ∼N(µ1, 2τ 2) for j ∈T1 and j ̸= i, and µj | µi ∼N(µ1 + β2 −β1, 2τ 2) for
243"
THEORETICAL ANALYSIS,0.4535603715170279,"j ∈T2. Then, the corresponding error is
244"
THEORETICAL ANALYSIS,0.4551083591331269,"e(ˆµFL) = Ei(ˆµFL −µ1)2 =
 X j∈T2 nj"
THEORETICAL ANALYSIS,0.456656346749226,"n (β2 −β1)
2
+
X"
THEORETICAL ANALYSIS,0.4582043343653251,"j=1,...,m,j̸=i nj n 2σ2"
THEORETICAL ANALYSIS,0.45975232198142413,"nj
+ 2τ 2

+
ni n 2 σ2"
THEORETICAL ANALYSIS,0.4613003095975232,"ni
.
(11)"
THEORETICAL ANALYSIS,0.4628482972136223,"It can be seen that compared with (10), the above FL error can be non-vanishing if P
j∈T2
nj"
THEORETICAL ANALYSIS,0.46439628482972134,"n (β2−β1)
245"
THEORETICAL ANALYSIS,0.46594427244582043,"is away from zero, even if sample sizes go to infinity. In other words, in the presence of a significant dif-
246"
THEORETICAL ANALYSIS,0.4674922600619195,"ference between the two clusters, the FL may not bring additional gain compared with local learning.
247"
THEORETICAL ANALYSIS,0.46904024767801855,"Cluster-based personalized FL: Suppose our algorithm allows both clusters to be correctly identified
248"
THEORETICAL ANALYSIS,0.47058823529411764,"upon convergence. Consider any particular client i. Suppose it belongs to Cluster 1 and will use a
249"
THEORETICAL ANALYSIS,0.47213622291021673,"weighted average of Cluster-specific models. Specifically, the Cluster 1 model will be the minimum
250"
THEORETICAL ANALYSIS,0.47368421052631576,of the loss P
THEORETICAL ANALYSIS,0.47523219814241485,"j∈T1
nj
nT1 Lj(µ),
nT1
∆= P
j∈T1 nj, which can be calculated to be ˆµT1 = P
j∈T1
nj
nT1 ˆµi.
251"
THEORETICAL ANALYSIS,0.47678018575851394,"By a similar argument as in the derivation of (11), we can calculate
252"
THEORETICAL ANALYSIS,0.47832817337461303,"e(ˆµT1) =
X"
THEORETICAL ANALYSIS,0.47987616099071206,"j∈T1,j̸=i  nj nT1 2σ2"
THEORETICAL ANALYSIS,0.48142414860681115,"nj
+ 2τ 2

+
 ni nT1 2 σ2"
THEORETICAL ANALYSIS,0.48297213622291024,"ni
.
(12)"
THEORETICAL ANALYSIS,0.4845201238390093,"The above value can be smaller than that in (10). To see this, let us suppose the sample sizes ni’s are
253"
THEORETICAL ANALYSIS,0.48606811145510836,"all equal to, say n0, for simplicity. Then, we have
254"
THEORETICAL ANALYSIS,0.48761609907120745,"e(ˆµT1) = m1 −1 m2
1 σ2"
THEORETICAL ANALYSIS,0.4891640866873065,"n0
+ 2τ 2

+ 1 m2
σ2"
THEORETICAL ANALYSIS,0.4907120743034056,"n0
= m1 −1 m2
1 σ2"
THEORETICAL ANALYSIS,0.49226006191950467,"n0
+ 2τ 2

+ 1 m2
1 σ2 n0"
THEORETICAL ANALYSIS,0.4938080495356037,"=
1
m1 σ2"
THEORETICAL ANALYSIS,0.4953560371517028,"n0
+ m1 −1"
THEORETICAL ANALYSIS,0.4969040247678019,"m2
1
2τ 2,"
THEORETICAL ANALYSIS,0.4984520123839009,"which is smaller than (10) if and only if
255"
THEORETICAL ANALYSIS,0.5,τ 2 < m1σ2
THEORETICAL ANALYSIS,0.5015479876160991,"2n0
.
(13)"
THEORETICAL ANALYSIS,0.5030959752321982,"We derive the following intuitions from this analysis: R1. If the within-cluster bias is relatively small,
256"
THEORETICAL ANALYSIS,0.5046439628482973,"the number of cluster-specific clients is large, and data noise is large, a client will have personalized
257"
THEORETICAL ANALYSIS,0.5061919504643962,"gain from collaborating with others in the same cluster. R2. PI-FL’s incentive algorithm rewards
258"
THEORETICAL ANALYSIS,0.5077399380804953,"accuracy improvement reflected in PMA, which directly correlates with reducing within-cluster bias
259"
THEORETICAL ANALYSIS,0.5092879256965944,"as per Equation 13. R3. By association, the incentive algorithm motivates clients to join similar
260"
THEORETICAL ANALYSIS,0.5108359133126935,"clusters which increases cluster homogeneity and reduces the within-cluster bias. We show the impact
261"
THEORETICAL ANALYSIS,0.5123839009287926,"of change in performance with an ablation study of PI-FL incentive in section 5.5.
262"
EXPERIMENTAL STUDY,0.5139318885448917,"5
Experimental Study
263"
EXPERIMENTAL SETUP,0.5154798761609907,"5.1
Experimental Setup
264"
EXPERIMENTAL SETUP,0.5170278637770898,"We use NVIDIA GeForce RTX 3070 GPUs for all our experiments. To evaluate the performance of
265"
EXPERIMENTAL SETUP,0.5185758513931888,"PI-FL with other pFL models we use four datasets. A simple CNN model (32x64x64 convolutional
266"
EXPERIMENTAL SETUP,0.5201238390092879,"and 3136x128 linear layer parameters) is used that can be trained on client devices with limited
267"
EXPERIMENTAL SETUP,0.521671826625387,"system resources to map Cross-Device FL settings [10] for all pFL methods.
268"
EXPERIMENTAL SETUP,0.5232198142414861,"CIFAR10 Data. For comparison with FedSoft [25] we use the same CIFAR10 dataset provided in
269"
EXPERIMENTAL SETUP,0.5247678018575851,"their repository. This image dataset has images of dimension 32 × 32 × 3 and 10 output classes. We
270"
EXPERIMENTAL SETUP,0.5263157894736842,"copy different data heterogeneity conditions from [25], namely 10:90, 30:70, linear, and random. The
271"
EXPERIMENTAL SETUP,0.5278637770897833,"data classes are divided into two clusters DA and DB. In the 10:90 partition, 50 clients have 90% train-
272"
EXPERIMENTAL SETUP,0.5294117647058824,"ing data from DA and 10% from DB, while the other 50 have 10% training data from DA and 90%
273"
EXPERIMENTAL SETUP,0.5309597523219814,"from DB. The 30:70 partition is similar to 10:90 except that the distribution ratios are 30% and 70%.
274"
EXPERIMENTAL SETUP,0.5325077399380805,"EMNIST Data. This image dataset has images of dimension 28 x 28 and 52 output classes where
275"
EXPERIMENTAL SETUP,0.5340557275541795,"26 classes are lower case letters and 26 classes are upper case letters. Same as CIFAR10 data, we use
276"
EXPERIMENTAL SETUP,0.5356037151702786,"the 10:90 and 30:70 data partitions and also include linear, and random partitions. In linear partition,
277"
EXPERIMENTAL SETUP,0.5371517027863777,"client k has (0.5 + k)% training and testing data from DA and (99.5 −k)% training data and testing
278"
EXPERIMENTAL SETUP,0.5386996904024768,"data from DB. In the random partition, client k is assigned a mixture vector generated randomly by
279"
EXPERIMENTAL SETUP,0.5402476780185759,"dividing the [0, 1] range into S segments with S −1 points drawn from Uniform(0, 1). The training
280"
EXPERIMENTAL SETUP,0.541795665634675,"and testing data are then assigned based on this vector from DA and DB. Similar to [6,30,37], we
281"
EXPERIMENTAL SETUP,0.5433436532507739,"also divide the EMNIST dataset into K clusters, where K = Ct"
EXPERIMENTAL SETUP,0.544891640866873,"Cp , Ct are total classes and Cp are the
282"
EXPERIMENTAL SETUP,0.5464396284829721,"classes owned per party with no overlap of data between clusters.
283"
EXPERIMENTAL SETUP,0.5479876160990712,"Synthetic CIFAR10. This is a synthetic dataset created from the CIFAR10 dataset and contains
284"
EXPERIMENTAL SETUP,0.5495356037151703,"the same hetrogenous partitions of 10:90, 30:70, linear, and random. The only difference is that
285"
EXPERIMENTAL SETUP,0.5510835913312694,"the training and testing data distributions are different to simulate dynamic data at the clients. For
286"
EXPERIMENTAL SETUP,0.5526315789473685,"example, in 10:90 50 clients have 90% training data with 10% testing data from DA and 10% training
287"
EXPERIMENTAL SETUP,0.5541795665634675,"data with 90% testing data from DB and vice versa. Similar to this, all the other partitions also
288"
EXPERIMENTAL SETUP,0.5557275541795665,"have inverse training and testing data distributions. The reason for separate training and testing data
289"
EXPERIMENTAL SETUP,0.5572755417956656,"distributions are explained in further depth in Appendix.
290"
FOCUS OF EXPERIMENTAL STUDY,0.5588235294117647,"5.2
Focus of Experimental Study
291"
FOCUS OF EXPERIMENTAL STUDY,0.5603715170278638,"First, we compare the clustering ability of PI-FL with a recent clustering-based pFL algorithm [25].
292"
FOCUS OF EXPERIMENTAL STUDY,0.5619195046439629,"Second, we show how PI-FL compares with other non-clustering pFL models with a simple test
293"
FOCUS OF EXPERIMENTAL STUDY,0.5634674922600619,"accuracy comparison. Taking it one step further, we provide a comparison of PI-FL and other
294"
FOCUS OF EXPERIMENTAL STUDY,0.565015479876161,"clustering and non-clustering pFL models in terms of reduction in opt-outs and PMA maintenance
295"
FOCUS OF EXPERIMENTAL STUDY,0.56656346749226,"in section 5.3. Lastly, in section 5.5 we show that including client preferences while clustering yields
296"
FOCUS OF EXPERIMENTAL STUDY,0.5681114551083591,"better personalization results because clients can make decisions based on knowledge restricted to
297"
FOCUS OF EXPERIMENTAL STUDY,0.5696594427244582,"the aggregator server.
298"
FOCUS OF EXPERIMENTAL STUDY,0.5712074303405573,Table 1: Test accuracy on CIFAR10
FOCUS OF EXPERIMENTAL STUDY,0.5727554179566563,"PI-FL
FedSoft
10:90
30:70
10:90
30:70"
FOCUS OF EXPERIMENTAL STUDY,0.5743034055727554,"c0
c1
c0
c1
c0
c1
c0
c1"
FOCUS OF EXPERIMENTAL STUDY,0.5758513931888545,"θ0
63.7
41.3
58.0
57.7
48.9
49.5
48.0
48.4
θ1
43.7
63.8
58.6
58.8
50.7
49.6
50.0
50.0"
FOCUS OF EXPERIMENTAL STUDY,0.5773993808049536,Table 2: Test Accuracy of pFL methods on EMNIST
FOCUS OF EXPERIMENTAL STUDY,0.5789473684210527,"Partitions
Ditto
FedProx
FedALA
PerfFedAvg
FedProto
PI-FL"
FOCUS OF EXPERIMENTAL STUDY,0.5804953560371517,"10:90
85.78±4.84
75.15±4.81
75.54±4.65
87.5±3.79
71.95±1.39
87.5±3.66
30:70
75.96±4.54
79.74±4.01
78.42±3.21
76.63±3.94
59.7±4.71
85.07±3.36
Linear
75.3±5.08
82.84±2.7
82.04±3.61
80.82±3.53
62.63±4.93
83.4±4.85
Random
77.82±6.79
80.93±4.42
78.98±5.07
83.31±5.19
68.43±5.65
86.21±4.34"
FOCUS OF EXPERIMENTAL STUDY,0.5820433436532507,"5.3
Test Accuracy performance study.
299"
FOCUS OF EXPERIMENTAL STUDY,0.5835913312693498,"Effectiveness of clustering. We evaluate the performance of cluster-level models using holdout
300"
FOCUS OF EXPERIMENTAL STUDY,0.5851393188854489,"datasets sampled from the corresponding cluster distributions (DA and DB). To demonstrate the
301"
FOCUS OF EXPERIMENTAL STUDY,0.586687306501548,"effectiveness of our proposed PI-FL method, we compare it with a recent cluster-based pFL algorithm
302"
FOCUS OF EXPERIMENTAL STUDY,0.5882352941176471,"called FedSoft using CIFAR10 data. We use the same parameters as in [25], with N = 100 clients,
303"
FOCUS OF EXPERIMENTAL STUDY,0.5897832817337462,"batch size 128, and learning rate η = 0.01, and perform training for 300 rounds. Table 1 presents the
304"
FOCUS OF EXPERIMENTAL STUDY,0.5913312693498453,"test accuracy for the 10:90 and 30:70 partitions with PI-FL. We observe that PI-FL performs better
305"
FOCUS OF EXPERIMENTAL STUDY,0.5928792569659442,"for the 10:90 partition, where each cluster dominates one of the distributions. With PI-FL, clients that
306"
FOCUS OF EXPERIMENTAL STUDY,0.5944272445820433,"have a greater portion of data from θ0 prefer to train in cluster c0, achieving 63.68% accuracy, while
307"
FOCUS OF EXPERIMENTAL STUDY,0.5959752321981424,"clients with a greater portion of data from θ1 prefer to train in cluster c1, achieving 63.82% accuracy.
308"
FOCUS OF EXPERIMENTAL STUDY,0.5975232198142415,"FedSoft cluster-level models, on the other hand, achieve 50.7% and 49.6% for 10:90 data. It is worth
309"
FOCUS OF EXPERIMENTAL STUDY,0.5990712074303406,"noting that FedSoft is unable to cater to different partitions of data through its clustering mechanism,
310"
FOCUS OF EXPERIMENTAL STUDY,0.6006191950464397,"and the performance is adversely impacted by increased heterogeneity. Moreover, cluster-level
311"
FOCUS OF EXPERIMENTAL STUDY,0.6021671826625387,"models in FedSoft are unable to dominate a single distribution of data. As expected, the performance
312"
FOCUS OF EXPERIMENTAL STUDY,0.6037151702786377,"for the 30:70 partition is not as good as it is a less heterogeneous partition than the 10:90 partition.
313"
FOCUS OF EXPERIMENTAL STUDY,0.6052631578947368,"Neither cluster dominates a single distribution, and the clients with different distributions are not
314"
FOCUS OF EXPERIMENTAL STUDY,0.6068111455108359,"clearly differentiated for training with different clusters. Additionally, the cluster-level models c0 and
315"
FOCUS OF EXPERIMENTAL STUDY,0.608359133126935,"c1 have similar performance with either distribution (θ0 and θ1), as FedSoft promotes personalizing
316"
FOCUS OF EXPERIMENTAL STUDY,0.6099071207430341,"models when clients have a greater percentage of shared data. This generates cluster-level models
317"
FOCUS OF EXPERIMENTAL STUDY,0.6114551083591331,"that cannot represent a single distribution and do not perform as well as PI-FL with non-IID data.
318"
FOCUS OF EXPERIMENTAL STUDY,0.6130030959752322,"Comparison with non-clustering pFL models. Table 2 shows a test accuracy comparison of PI-FL
319"
FOCUS OF EXPERIMENTAL STUDY,0.6145510835913313,"with other recent pFL algorithms. This table shows that some pFL models are able to perform well
320"
FOCUS OF EXPERIMENTAL STUDY,0.6160990712074303,"for individual partitions such as Ditto for 10:90, FedProx and FedALA for Linear, and PerFedAvg for
321"
FOCUS OF EXPERIMENTAL STUDY,0.6176470588235294,"Random, however, PI-FL is able to maintain its performance for all partitions.
322"
FOCUS OF EXPERIMENTAL STUDY,0.6191950464396285,"5.4
Effectiveness of PI-FL in opt-outs reduction and PMA maintenance.
323"
FOCUS OF EXPERIMENTAL STUDY,0.6207430340557275,"Each client’s natural aim is to create a model that maximizes its test accuracy. Clients can have
324"
FOCUS OF EXPERIMENTAL STUDY,0.6222910216718266,"different thresholds of how much should be the least accuracy gain for it to participate in pFL, and
325"
FOCUS OF EXPERIMENTAL STUDY,0.6238390092879257,"we define this self-defined threshold as ρi, i ∈[N]. Since each client can have its own definition of
326"
FOCUS OF EXPERIMENTAL STUDY,0.6253869969040248,"the threshold requirement, we define ρi as the test accuracy achieved by client i if it used FedAvg. So
327"
FOCUS OF EXPERIMENTAL STUDY,0.6269349845201239,"PMAi shows the gain in performance from pFL compared to vanilla FL using FedAvg for client i in
328"
FOCUS OF EXPERIMENTAL STUDY,0.628482972136223,"N. PMA is similar to GMA from [37], however, creating a single global model may not be appealing
329"
FOCUS OF EXPERIMENTAL STUDY,0.6300309597523219,"for all clients as we show in section 4 and verify in section 5.4. We formally define PMA and opt-outs
330"
FOCUS OF EXPERIMENTAL STUDY,0.631578947368421,"in Equation 14 and 15 respectively, where fi(wk) is the test accuracy achieved by pFL.
331"
FOCUS OF EXPERIMENTAL STUDY,0.6331269349845201,"PMAi = fi(wk) −ρi | i ∈[N], k ∈[K]
(14)"
FOCUS OF EXPERIMENTAL STUDY,0.6346749226006192,"332
opt-outs = 1 N N
X"
FOCUS OF EXPERIMENTAL STUDY,0.6362229102167183,"i=1
fi(wk) < ρi | i ∈[N], k ∈[K]
(15)"
FOCUS OF EXPERIMENTAL STUDY,0.6377708978328174,0.0 0.2 0.4 0.6 0.8 1.0
FOCUS OF EXPERIMENTAL STUDY,0.6393188854489165,Number of clients 20 0 20 40
FOCUS OF EXPERIMENTAL STUDY,0.6408668730650154,PMA (%)
FOCUS OF EXPERIMENTAL STUDY,0.6424148606811145,(a) PI-FL & FedSoft
FOCUS OF EXPERIMENTAL STUDY,0.6439628482972136,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 40 20 0"
FOCUS OF EXPERIMENTAL STUDY,0.6455108359133127,(b) PI-FL & Ditto
FOCUS OF EXPERIMENTAL STUDY,0.6470588235294118,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 40 20 0 20"
FOCUS OF EXPERIMENTAL STUDY,0.6486068111455109,(c) PI-FL & FedALA
FOCUS OF EXPERIMENTAL STUDY,0.6501547987616099,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 40 20 0 20"
FOCUS OF EXPERIMENTAL STUDY,0.651702786377709,(d) PI-FL & FedFomo
FOCUS OF EXPERIMENTAL STUDY,0.653250773993808,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 40 20 0 20"
FOCUS OF EXPERIMENTAL STUDY,0.6547987616099071,PMA (%)
FOCUS OF EXPERIMENTAL STUDY,0.6563467492260062,(e) PI-FL & FedProto
FOCUS OF EXPERIMENTAL STUDY,0.6578947368421053,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 40 20 0 20"
FOCUS OF EXPERIMENTAL STUDY,0.6594427244582043,(f) PI-FL & FedAMP
FOCUS OF EXPERIMENTAL STUDY,0.6609907120743034,"0.0
0.2
0.4
0.6
0.8
1.0
Number of clients 0 20 40"
FOCUS OF EXPERIMENTAL STUDY,0.6625386996904025,PMA (%)
FOCUS OF EXPERIMENTAL STUDY,0.6640866873065016,"(g) 4 classes per client
Figure 2: CDF of clients’ PMA for different datasets and methods"
FOCUS OF EXPERIMENTAL STUDY,0.6656346749226006,"333
Figure 2 shows the empirical Cumulative Distribution Function (CDF) plot of PMA for all clients with
334"
FOCUS OF EXPERIMENTAL STUDY,0.6671826625386997,"CIFAR10 data using FedSoft and PIFL and with EMNIST dataset for all other pFL models. PI-FL
335"
FOCUS OF EXPERIMENTAL STUDY,0.6687306501547987,"particularly outperforms for the 10:90 partition in terms of PMA as this is the most heterogeneous
336"
FOCUS OF EXPERIMENTAL STUDY,0.6702786377708978,"data partition as can be seen in Figure 2a. The EMNIST dataset is less heterogeneous as it has more
337"
FOCUS OF EXPERIMENTAL STUDY,0.6718266253869969,"classes per client compared to CIFAR10 which is why FedAvg is able to perform relatively well and
338"
FOCUS OF EXPERIMENTAL STUDY,0.673374613003096,"there is less room for improvement with personalizing. PI-FL maintains the PMA and also improves
339"
FOCUS OF EXPERIMENTAL STUDY,0.6749226006191951,"it, particularly for the 10:90 and 30:70 partitions where other pFL solutions lack. We also test on a
340"
FOCUS OF EXPERIMENTAL STUDY,0.6764705882352942,"more heterogenous case where the dataset is divided into 52 clusters and each client owns 4 maximum
341"
FOCUS OF EXPERIMENTAL STUDY,0.6780185758513931,"classes. Figure 2g shows that while other pFL solutions perform better than FedAvg only Ditto and
342"
FOCUS OF EXPERIMENTAL STUDY,0.6795665634674922,"FedProto come relatively close to PI-FL, however, PI-FL outperforms them both by approximately
343"
FOCUS OF EXPERIMENTAL STUDY,0.6811145510835913,"15% in terms of PMA. The FedProx, FedALA, and PerFedAvg opt-out ratios are 0.64, 0.31, and 0.68,
344"
FOCUS OF EXPERIMENTAL STUDY,0.6826625386996904,"respectively. Ditto, FedFomo, and PI-FL have no opt-outs. This goes to show that PI-FL is not only
345"
FOCUS OF EXPERIMENTAL STUDY,0.6842105263157895,"able to reduce the opt-outs but also improves the PMA under all data heterogeneity conditions.
346"
FOCUS OF EXPERIMENTAL STUDY,0.6857585139318886,"5.5
Advantages of including client preferences in pFL.
347"
FOCUS OF EXPERIMENTAL STUDY,0.6873065015479877,"0
20
40
60
80
Accuracy (%) 0.0 0.2 0.4 0.6 0.8 1.0"
FOCUS OF EXPERIMENTAL STUDY,0.6888544891640866,Number of clients
FOCUS OF EXPERIMENTAL STUDY,0.6904024767801857,"PI-FL (10:90)
FedSoft (10:90)
PI-FL (30:70)
FedSoft (30:70)"
FOCUS OF EXPERIMENTAL STUDY,0.6919504643962848,"PI-FL (linear)
FedSoft (linear)
PI-FL (random)
FedSoft (random)"
FOCUS OF EXPERIMENTAL STUDY,0.6934984520123839,"Figure 3: PI-FL and FedSoft
with Synthetic CIFAR10 data"
FOCUS OF EXPERIMENTAL STUDY,0.695046439628483,"We show that PI-FL can maintain the test accuracy of personalized
348"
FOCUS OF EXPERIMENTAL STUDY,0.6965944272445821,"models even in case of dynamic data at the client or a new unseen
349"
FOCUS OF EXPERIMENTAL STUDY,0.6981424148606811,"client accidentally being added to the wrong cluster. Figure 3 shows
350"
FOCUS OF EXPERIMENTAL STUDY,0.6996904024767802,"the CDF of clients’ personalized model test accuracy after training
351"
FOCUS OF EXPERIMENTAL STUDY,0.7012383900928792,"for 500 rounds. PI-FL is robust to variations in clients’ local data,
352"
FOCUS OF EXPERIMENTAL STUDY,0.7027863777089783,"while FedSoft is less effective due to its clustering approach being
353"
FOCUS OF EXPERIMENTAL STUDY,0.7043343653250774,"based on the server’s perspective, which lacks access to clients’ pri-
354"
FOCUS OF EXPERIMENTAL STUDY,0.7058823529411765,"vate data and limits its ability to make accurate clustering decisions.
355"
FOCUS OF EXPERIMENTAL STUDY,0.7074303405572755,"0
20
40
60
80
Accuracy (%) 0.0 0.2 0.4 0.6 0.8 1.0"
FOCUS OF EXPERIMENTAL STUDY,0.7089783281733746,Number of clients
FOCUS OF EXPERIMENTAL STUDY,0.7105263157894737,"10:90 (I)
10:90 (NI)
30:70 (I)
30:70 (NI)
Random (I)
Random (NI)
Linear (I)
Linear (NI)"
FOCUS OF EXPERIMENTAL STUDY,0.7120743034055728,"Figure 4: PI-FL with and with-
out incentive (I/NI)"
FOCUS OF EXPERIMENTAL STUDY,0.7136222910216719,"Ablatian study with Incentive in PI-FL. To measure the impact
356"
FOCUS OF EXPERIMENTAL STUDY,0.7151702786377709,"of incentive provision on personalized model generation we test
357"
FOCUS OF EXPERIMENTAL STUDY,0.7167182662538699,"PI-FL with incentives enabled and disabled. Figure 4 shows the
358"
FOCUS OF EXPERIMENTAL STUDY,0.718266253869969,"CDF of clients’ personalized model test accuracy with the Synthetic
359"
FOCUS OF EXPERIMENTAL STUDY,0.7198142414860681,"CIFAR10 dataset. Except for the 30:70 partition, the accuracy for
360"
FOCUS OF EXPERIMENTAL STUDY,0.7213622291021672,"all other partitions is higher with the incentive enabled. We argue
361"
FOCUS OF EXPERIMENTAL STUDY,0.7229102167182663,"that the test accuracy for 30:70 is low in this case because it is a less
362"
FOCUS OF EXPERIMENTAL STUDY,0.7244582043343654,"heterogeneous data case and PI-FL performs best in cases where
363"
FOCUS OF EXPERIMENTAL STUDY,0.7260061919504643,"data is highly heterogeneous and requires personalized learning.
364"
FOCUS OF EXPERIMENTAL STUDY,0.7275541795665634,"Further details of the experimental setup and impact of incentive on
365"
FOCUS OF EXPERIMENTAL STUDY,0.7291021671826625,"clustering are discussed in the Appendix.
366"
CONCLUSION,0.7306501547987616,"6
Conclusion
367"
CONCLUSION,0.7321981424148607,"In this paper, we proposed PI-FL to address the challenges of incentive provision in pFL for increasing
368"
CONCLUSION,0.7337461300309598,"consistent participation by providing appealing personalized models to clients. PI-FL client-centric
369"
CONCLUSION,0.7352941176470589,"clustering approach ensures accurate clustering and improved performance even in case of dynamic
370"
CONCLUSION,0.7368421052631579,"data distribution shift of the client’s local data or inadvertently mistaken clustering decision by the
371"
CONCLUSION,0.7383900928792569,"client. Unlike prior works that consider incentivizing and personalization as separate problems,
372"
CONCLUSION,0.739938080495356,"PI-FL solves them as interrelated challenges yielding improvement in pFL performance. Extensive
373"
CONCLUSION,0.7414860681114551,"empirical evaluation shows its promising performance compared to other state-of-the-art works.
374"
REFERENCES,0.7430340557275542,"References
375"
REFERENCES,0.7445820433436533,"[1] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Federated
376"
REFERENCES,0.7461300309597523,"learning: Strategies for improving communication efficiency,” arXiv preprint arXiv:1610.05492,
377"
REFERENCES,0.7476780185758514,"2016.
378"
REFERENCES,0.7492260061919505,"[2] H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of deep
379"
REFERENCES,0.7507739938080495,"networks using model averaging,” CoRR, vol. abs/1602.05629, 2016. [Online]. Available:
380"
REFERENCES,0.7523219814241486,"http://arxiv.org/abs/1602.05629
381"
REFERENCES,0.7538699690402477,"[3] E. Diao, J. Ding, and V. Tarokh, “HeteroFL: Computation and communication efficient federated
382"
REFERENCES,0.7554179566563467,"learning for heterogeneous clients,” in International Conference on Learning Representations,
383"
REFERENCES,0.7569659442724458,"2021.
384"
REFERENCES,0.7585139318885449,"[4] ——, “Semifl: Semi-supervised federated learning for unlabeled clients with alternate training,”
385"
REFERENCES,0.760061919504644,"Advances in Neural Information Processing Systems, vol. 35, pp. 17 871–17 884, 2022.
386"
REFERENCES,0.7616099071207431,"[5] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization
387"
REFERENCES,0.7631578947368421,"in heterogeneous networks,” 2020.
388"
REFERENCES,0.7647058823529411,"[6] J. Zhang, Y. Hua, H. Wang, T. Song, Z. Xue, R. Ma, and H. Guan, “Fedala: Adaptive local
389"
REFERENCES,0.7662538699690402,"aggregation for personalized federated learning,” 12 2022.
390"
REFERENCES,0.7678018575851393,"[7] M. Zhang, K. Sapra, S. Fidler, S. Yeung, and J. M. Alvarez, “Personalized federated learning
391"
REFERENCES,0.7693498452012384,"with first order model optimization,” 2021.
392"
REFERENCES,0.7708978328173375,"[8] O. MARFOQ, G. Neglia, A. Bellet, L. Kameni, and R. Vidal, “Federated multi-task learning
393"
REFERENCES,0.7724458204334366,"under a mixture of distributions,” in Advances in Neural Information Processing Systems,
394"
REFERENCES,0.7739938080495357,"A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [Online]. Available:
395"
REFERENCES,0.7755417956656346,"https://openreview.net/forum?id=YCqx6zhEzRp
396"
REFERENCES,0.7770897832817337,"[9] V. Kulkarni, M. Kulkarni, and A. Pant, “Survey of personalization techniques for federated learn-
397"
REFERENCES,0.7786377708978328,"ing,” in 2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability
398"
REFERENCES,0.7801857585139319,"(WorldS4), 2020, pp. 794–797.
399"
REFERENCES,0.781733746130031,"[10] P. Kairouz, H. B. McMahan, A. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz,
400"
REFERENCES,0.7832817337461301,"C. Charles, G. Cormode, R. Cummings et al., “Advances and open problems in federated
401"
REFERENCES,0.7848297213622291,"learning,” Foundations and Trends in Machine Learning, vol. 12, no. 3-4, pp. 1–357, 2019.
402"
REFERENCES,0.7863777089783281,"[11] F. Lai, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Oort: Efficient federated learning via
403"
REFERENCES,0.7879256965944272,"guided participant selection,” 2021.
404"
REFERENCES,0.7894736842105263,"[12] J. Han, A. F. Khan, S. Zawad, A. Anwar, N. B. Angel, Y. Zhou, F. Yan, and A. R. Butt,
405"
REFERENCES,0.7910216718266254,"“Heterogeneity-aware adaptive federated learning scheduling,” in 2022 IEEE International
406"
REFERENCES,0.7925696594427245,"Conference on Big Data (Big Data), 2022, pp. 911–920.
407"
REFERENCES,0.7941176470588235,"[13] Z. Chai, A. Ali, S. Zawad, S. Truex, A. Anwar, N. Baracaldo, Y. Zhou, H. Ludwig, F. Yan, and
408"
REFERENCES,0.7956656346749226,"Y. Cheng, “Tifl: A tier-based federated learning system,” CoRR, vol. abs/2001.09249, 2020.
409"
REFERENCES,0.7972136222910217,"[Online]. Available: https://arxiv.org/abs/2001.09249
410"
REFERENCES,0.7987616099071208,"[14] Y. Deng, F. Lyu, J. Ren, Y.-C. Chen, P. Yang, Y. Zhou, and Y. Zhang, “Fair: Quality-aware
411"
REFERENCES,0.8003095975232198,"federated learning with precise user incentive and model aggregation,” in IEEE INFOCOM
412"
REFERENCES,0.8018575851393189,"2021 - IEEE Conference on Computer Communications, 2021, pp. 1–10.
413"
REFERENCES,0.8034055727554179,"[15] M. Tang and V. W. Wong, “An incentive mechanism for cross-silo federated learning: A public
414"
REFERENCES,0.804953560371517,"goods perspective,” in IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,
415"
REFERENCES,0.8065015479876161,"2021, pp. 1–10.
416"
REFERENCES,0.8080495356037152,"[16] M. Hu, D. Wu, Y. Zhou, X. Chen, and M. Chen, “Incentive-aware autonomous client partici-
417"
REFERENCES,0.8095975232198143,"pation in federated learning,” IEEE Transactions on Parallel and Distributed Systems, vol. 33,
418"
REFERENCES,0.8111455108359134,"no. 10, pp. 2612–2627, 2022.
419"
REFERENCES,0.8126934984520123,"[17] J. S. Ng, W. Y. B. Lim, Z. Xiong, X. Cao, D. Niyato, C. Leung, and D. I. Kim, “A hierarchical
420"
REFERENCES,0.8142414860681114,"incentive design toward motivating participation in coded federated learning,” IEEE Journal on
421"
REFERENCES,0.8157894736842105,"Selected Areas in Communications, vol. 40, no. 1, pp. 359–375, 2022.
422"
REFERENCES,0.8173374613003096,"[18] Z. Shi, L. Zhang, Z. Yao, L. Lyu, C. Chen, L. Wang, J. Wang, and X.-Y. Li, “Fedfaim: A model
423"
REFERENCES,0.8188854489164087,"performance-based fair incentive mechanism for federated learning,” IEEE Transactions on Big
424"
REFERENCES,0.8204334365325078,"Data, pp. 1–13, 2022.
425"
REFERENCES,0.8219814241486069,"[19] A. Z. Tan, H. Yu, L. Cui, and Q. Yang, “Towards personalized federated learning,” CoRR, vol.
426"
REFERENCES,0.8235294117647058,"abs/2103.00710, 2021. [Online]. Available: https://arxiv.org/abs/2103.00710
427"
REFERENCES,0.8250773993808049,"[20] H. Chen, J. Ding, E. W. Tramel, S. Wu, A. K. Sahu, S. Avestimehr, and T. Zhang, “Self-aware
428"
REFERENCES,0.826625386996904,"personalized federated learning,” Advances in Neural Information Processing Systems, vol. 35,
429"
REFERENCES,0.8281733746130031,"pp. 20 675–20 688, 2022.
430"
REFERENCES,0.8297213622291022,"[21] L. Collins, E. Diao, T. Roosta, J. Ding, and T. Zhang, “Perfedsi: A framework for personalized
431"
REFERENCES,0.8312693498452013,"federated learning with side information,” 2022.
432"
REFERENCES,0.8328173374613003,"[22] Q. Le, E. Diao, X. Wang, A. Anwar, V. Tarokh, and J. Ding, “Personalized federated
433"
REFERENCES,0.8343653250773994,"recommender systems with private and partially federated autoencoders,” arXiv preprint
434"
REFERENCES,0.8359133126934984,"arXiv:2212.08779, 2022.
435"
REFERENCES,0.8374613003095975,"[23] Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three approaches for personalization with
436"
REFERENCES,0.8390092879256966,"applications to federated learning,” ArXiv, vol. abs/2002.10619, 2020.
437"
REFERENCES,0.8405572755417957,"[24] M. Duan, D. Liu, X. Ji, R. Liu, L. Liang, X. Chen, and Y. Tan, “Fedgroup: Accurate federated
438"
REFERENCES,0.8421052631578947,"learning via decomposed similarity-based clustering,” 2021.
439"
REFERENCES,0.8436532507739938,"[25] Y. Ruan and C. Joe-Wong, “Fedsoft: Soft clustered federated learning with proximal local
440"
REFERENCES,0.8452012383900929,"updating,” in AAAI, 2022.
441"
REFERENCES,0.846749226006192,"[26] X. Tang, S. Guo, and J. Guo, “Personalized federated learning with contextualized
442"
REFERENCES,0.848297213622291,"generalization,” in Proceedings of the Thirty-First International Joint Conference on
443"
REFERENCES,0.8498452012383901,"Artificial Intelligence, IJCAI-22, L. D. Raedt, Ed.
International Joint Conferences on
444"
REFERENCES,0.8513931888544891,"Artificial Intelligence Organization, 7 2022, pp. 2241–2247, main Track. [Online]. Available:
445"
REFERENCES,0.8529411764705882,"https://doi.org/10.24963/ijcai.2022/311
446"
REFERENCES,0.8544891640866873,"[27] C. Ye, R. Ghanadan, and J. Ding, “Meta clustering for collaborative learning,” Journal of
447"
REFERENCES,0.8560371517027864,"Computational and Graphical Statistics, pp. 1–10, 2022.
448"
REFERENCES,0.8575851393188855,"[28] J. Han, A. F. Khan, S. Zawad, A. Anwar, N. B. Angel, Y. Zhou, F. Yan, and A. R. Butt, “Tiff:
449"
REFERENCES,0.8591331269349846,"Tokenized incentive for federated learning,” in 2022 IEEE 15th International Conference on
450"
REFERENCES,0.8606811145510835,"Cloud Computing (CLOUD), 2022, pp. 407–416.
451"
REFERENCES,0.8622291021671826,"[29] X. Tang, S. Guo, and J. Guo, “Personalized federated learning with clustered generalization,”
452"
REFERENCES,0.8637770897832817,"ArXiv, vol. abs/2106.13044, 2021.
453"
REFERENCES,0.8653250773993808,"[30] T. Li, S. Hu, A. Beirami, and V. Smith, “Federated multi-task learning for competing constraints,”
454"
REFERENCES,0.8668730650154799,"CoRR, vol. abs/2012.04221, 2020. [Online]. Available: https://arxiv.org/abs/2012.04221
455"
REFERENCES,0.868421052631579,"[31] J. Zhang, Y. Wu, and R. Pan, “Incentive mechanism for horizontal federated learning based on
456"
REFERENCES,0.8699690402476781,"reputation and reverse auction,” in Proceedings of the Web Conference 2021, ser. WWW ’21.
457"
REFERENCES,0.871517027863777,"New York, NY, USA: Association for Computing Machinery, 2021, p. 947–956. [Online].
458"
REFERENCES,0.8730650154798761,"Available: https://doi.org/10.1145/3442381.3449888
459"
REFERENCES,0.8746130030959752,"[32] T. Jahani-Nezhad, M. A. Maddah-Ali, S. Li, and G. Caire, “Swiftagg: Communication-efficient
460"
REFERENCES,0.8761609907120743,"and dropout-resistant secure aggregation for federated learning with worst-case security guaran-
461"
REFERENCES,0.8777089783281734,"tees,” in 2022 IEEE International Symposium on Information Theory (ISIT), 2022, pp. 103–108.
462"
REFERENCES,0.8792569659442725,"[33] J. So, B. Güler, and A. S. Avestimehr, “Turbo-aggregate: Breaking the quadratic aggregation
463"
REFERENCES,0.8808049535603715,"barrier in secure federated learning,” IEEE Journal on Selected Areas in Information Theory,
464"
REFERENCES,0.8823529411764706,"vol. 2, no. 1, pp. 479–489, 2021.
465"
REFERENCES,0.8839009287925697,"[34] L. Gao, L. Li, Y. Chen, W. Zheng, C. Xu, and M. Xu, “Fifl: A fair incentive mechanism for
466"
REFERENCES,0.8854489164086687,"federated learning,” in Proceedings of the 50th International Conference on Parallel Processing,
467"
REFERENCES,0.8869969040247678,"ser. ICPP ’21.
New York, NY, USA: Association for Computing Machinery, 2021. [Online].
468"
REFERENCES,0.8885448916408669,"Available: https://doi.org/10.1145/3472456.3472469
469"
REFERENCES,0.8900928792569659,"[35] Y. Shi, H. Yu, and C. Leung, “Towards fairness-aware federated learning,” IEEE Transactions
470"
REFERENCES,0.891640866873065,"on Neural Networks and Learning Systems, pp. 1–17, 2023.
471"
REFERENCES,0.8931888544891641,"[36] Z. Zhou, L. Chu, C. Liu, L. Wang, J. Pei, and Y. Zhang, “Towards fair federated learning,” in
472"
REFERENCES,0.8947368421052632,"Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, ser.
473"
REFERENCES,0.8962848297213623,"KDD ’21.
New York, NY, USA: Association for Computing Machinery, 2021, p. 4100–4101.
474"
REFERENCES,0.8978328173374613,"[Online]. Available: https://doi.org/10.1145/3447548.3470814
475"
REFERENCES,0.8993808049535603,"[37] Y. J. Cho, D. Jhunjhunwala, T. Li, V. Smith, and G. Joshi, “Maximizing global model appeal in
476"
REFERENCES,0.9009287925696594,"federated learning,” 2023.
477"
REFERENCES,0.9024767801857585,"[38] S. Wei, Y. Tong, Z. Zhou, and T. Song, Efficient and Fair Data Valuation for Horizontal
478"
REFERENCES,0.9040247678018576,"Federated Learning, 11 2020, pp. 139–152.
479"
REFERENCES,0.9055727554179567,"[39] A. Heuillet, F. Couthouis, and N. Díaz-Rodríguez, “Collective explainable ai: Explaining
480"
REFERENCES,0.9071207430340558,"cooperative strategies and agent contribution in multiagent reinforcement learning with shapley
481"
REFERENCES,0.9086687306501547,"values,” IEEE Computational Intelligence Magazine, vol. 17, no. 1, pp. 59–71, 2022.
482"
REFERENCES,0.9102167182662538,"[40] Z. Liu, Y. Chen, H. Yu, Y. Liu, and L. Cui, “Gtg-shapley: Efficient and accurate participant
483"
REFERENCES,0.9117647058823529,"contribution evaluation in federated learning,” ACM Trans. Intell. Syst. Technol., vol. 13, no. 4,
484"
REFERENCES,0.913312693498452,"may 2022. [Online]. Available: https://doi.org/10.1145/3501811
485"
REFERENCES,0.9148606811145511,"[41] L. Dong, Z. Liu, K. Zhang, A. Yassine, and M. S. Hossain, “Affordable federated edge learning
486"
REFERENCES,0.9164086687306502,"framework via efficient shapley value estimation,” Future Generation Computer Systems, 2023.
487"
REFERENCES,0.9179566563467493,"[Online]. Available: https://www.sciencedirect.com/science/article/pii/S0167739X23001826
488"
REFERENCES,0.9195046439628483,"[42] A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient framework for clustered
489"
REFERENCES,0.9210526315789473,"federated learning,” Advances in Neural Information Processing Systems, vol. 33, pp. 19 586–
490"
REFERENCES,0.9226006191950464,"19 597, 2020.
491"
REFERENCES,0.9241486068111455,"[43] A. Fallah, A. Mokhtari, and A. Ozdaglar, “Personalized federated learning with theoretical
492"
REFERENCES,0.9256965944272446,"guarantees: A model-agnostic meta-learning approach,” in Advances in Neural Information
493"
REFERENCES,0.9272445820433437,"Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.
494"
REFERENCES,0.9287925696594427,"Curran Associates, Inc., 2020, pp. 3557–3568. [Online]. Available: https://proceedings.neurips.
495"
REFERENCES,0.9303405572755418,"cc/paper_files/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf
496"
REFERENCES,0.9318885448916409,"[44] F. Hanzely and P. Richtárik, “Federated learning of a mixture of global and local models,” 2021.
497"
REFERENCES,0.93343653250774,"[45] F. Hanzely, B. Zhao, and M. Kolar, “Personalized federated learning: A unified framework and
498"
REFERENCES,0.934984520123839,"universal optimization techniques,” ArXiv, vol. abs/2102.09743, 2021.
499"
REFERENCES,0.9365325077399381,"[46] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task learning,” in
500"
REFERENCES,0.9380804953560371,"Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio,
501"
REFERENCES,0.9396284829721362,"H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30.
Curran Associates,
502"
REFERENCES,0.9411764705882353,"Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/
503"
REFERENCES,0.9427244582043344,"6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf
504"
REFERENCES,0.9442724458204335,"[47] C. T. Dinh, N. H. Tran, and T. D. Nguyen, “Personalized federated learning with moreau
505"
REFERENCES,0.9458204334365325,"envelopes,” 2022.
506"
REFERENCES,0.9473684210526315,"[48] P. P. Liang, T. Liu, L. Ziyin, N. B. Allen, R. P. Auerbach, D. Brent, R. Salakhutdinov, and L.-P.
507"
REFERENCES,0.9489164086687306,"Morency, “Think locally, act globally: Federated learning with local and global representations,”
508"
REFERENCES,0.9504643962848297,"2020.
509"
REFERENCES,0.9520123839009288,"[49] R. Zeng, S. Zhang, J. Wang, and X. Chu, “FMore: An incentive scheme of multi-dimensional
510"
REFERENCES,0.9535603715170279,"auction for federated learning in MEC,” in 2020 IEEE 40th International Conference
511"
REFERENCES,0.955108359133127,"on Distributed Computing Systems (ICDCS).
IEEE, nov 2020. [Online]. Available:
512"
REFERENCES,0.9566563467492261,"https://doi.org/10.1109%2Ficdcs47774.2020.00094
513"
REFERENCES,0.958204334365325,"[50] P. Sun, H. Che, Z. Wang, Y. Wang, T. Wang, L. Wu, and H. Shao, “Pain-fl: Personalized privacy-
514"
REFERENCES,0.9597523219814241,"preserving incentive for federated learning,” IEEE Journal on Selected Areas in Communications,
515"
REFERENCES,0.9613003095975232,"vol. 39, no. 12, pp. 3805–3820, 2021.
516"
REFERENCES,0.9628482972136223,"[51] L. Zhang, T. Zhu, P. Xiong, W. Zhou, and P. S. Yu, “A robust game-theoretical federated
517"
REFERENCES,0.9643962848297214,"learning framework with joint differential privacy,” IEEE Transactions on Knowledge and Data
518"
REFERENCES,0.9659442724458205,"Engineering, vol. 35, no. 4, pp. 3333–3346, 2023.
519"
REFERENCES,0.9674922600619195,"[52] L. Li, Q. Li, H. Chen, and Y. Chen, “Federated learning with strategic participants: A game
520"
REFERENCES,0.9690402476780186,"theoretic approach,” in Proceedings of the 37th International Conference on Machine Learning.
521"
REFERENCES,0.9705882352941176,"PMLR, 2020, pp. 8597–8606.
522"
REFERENCES,0.9721362229102167,"[53] F1-scores, “Sklearn.metrics.f1_score.” [Online]. Available: https://scikit-learn.org/stable/
523"
REFERENCES,0.9736842105263158,"modules/generated/sklearn.metrics.f1_score.html
524"
REFERENCES,0.9752321981424149,"[54] Python, “Cpython/functions.rst at main · python/cpython.” [Online]. Available:
https:
525"
REFERENCES,0.9767801857585139,"//github.com/python/cpython/blob/main/Doc/library/functions.rst
526"
REFERENCES,0.978328173374613,"[55] K-Means, “Sklearn.cluster.kmeans.” [Online]. Available: https://scikit-learn.org/stable/modules/
527"
REFERENCES,0.9798761609907121,"generated/sklearn.cluster.KMeans.html
528"
REFERENCES,0.9814241486068112,"[56] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon,
529"
REFERENCES,0.9829721362229102,"J. Koneˇcný, S. Mazzocchi, H. B. McMahan, T. Van Overveldt, D. Petrou, D. Ramage, and
530"
REFERENCES,0.9845201238390093,"J. Roselander, “Towards federated learning at scale: System design,” 2019. [Online]. Available:
531"
REFERENCES,0.9860681114551083,"https://arxiv.org/abs/1902.01046
532"
REFERENCES,0.9876160990712074,"[57] A. Khan, Y. Li, A. Anwar, Y. Cheng, T. Hoang, N. Baracaldo, and A. Butt, “A distributed and
533"
REFERENCES,0.9891640866873065,"elastic aggregation service for scalable federated learning systems,” 2022. [Online]. Available:
534"
REFERENCES,0.9907120743034056,"https://arxiv.org/abs/2204.07767
535"
REFERENCES,0.9922600619195047,"[58] A. E. Roth, “Introduction to the shapley value,” The Shapley value, pp. 1–27, 1988.
536"
REFERENCES,0.9938080495356038,"[59] G. D. P. Regulation, “General data protection regulation (gdpr),” Intersoft Consulting, Accessed
537"
REFERENCES,0.9953560371517027,"in October, vol. 24, no. 1, 2018.
538"
REFERENCES,0.9969040247678018,"[60] A. Act, “Health insurance portability and accountability act of 1996,” Public law, vol. 104, p.
539"
REFERENCES,0.9984520123839009,"191, 1996.
540"
