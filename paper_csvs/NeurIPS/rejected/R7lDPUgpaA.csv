Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002242152466367713,"Understanding generalization in deep neural networks is an active area of research.
1"
ABSTRACT,0.004484304932735426,"A promising avenue of exploration has been that of margin measurements: the
2"
ABSTRACT,0.006726457399103139,"shortest distance to the decision boundary for a given sample or its representation
3"
ABSTRACT,0.008968609865470852,"internal to the network. While margins have been shown to be correlated with
4"
ABSTRACT,0.011210762331838564,"the generalization ability of a model when measured at its hidden representations
5"
ABSTRACT,0.013452914798206279,"(hidden margins), no such link between large margins and generalization has been
6"
ABSTRACT,0.01569506726457399,"established for input margins. We show that while input margins are not gener-
7"
ABSTRACT,0.017937219730941704,"ally predictive of generalization, they can be if the search space is appropriately
8"
ABSTRACT,0.020179372197309416,"constrained. We develop such a measure based on input margins, which we refer
9"
ABSTRACT,0.02242152466367713,"to as ‘constrained margins’. The predictive power of this new measure is demon-
10"
ABSTRACT,0.02466367713004484,"strated on the ‘Predicting Generalization in Deep Learning’ (PGDL) dataset and
11"
ABSTRACT,0.026905829596412557,"contrasted with hidden representation margins. We find that constrained margins
12"
ABSTRACT,0.02914798206278027,"achieve highly competitive scores and outperform other margin measurements in
13"
ABSTRACT,0.03139013452914798,"general.
14"
INTRODUCTION,0.033632286995515695,"1
Introduction
15"
INTRODUCTION,0.03587443946188341,"Our understanding of the generalization ability of deep neural networks (DNNs) remains incomplete.
16"
INTRODUCTION,0.03811659192825112,"Various bounds on the generalization error for classical machine learning models have been proposed
17"
INTRODUCTION,0.04035874439461883,"based on the complexity of the hypothesis space [1, 2]. However, this approach paints an unfinished
18"
INTRODUCTION,0.042600896860986545,"picture when considering modern DNNs [3]. Generalization in DNNs is an active field of study and
19"
INTRODUCTION,0.04484304932735426,"updated bounds are proposed on an ongoing basis [4, 5, 6, 7].
20"
INTRODUCTION,0.04708520179372197,"A complementary approach to developing theoretical bounds is to develop empirical techniques that
21"
INTRODUCTION,0.04932735426008968,"are able to predict the generalization ability of certain families of DNN models. The ‘Predicting
22"
INTRODUCTION,0.0515695067264574,"Generalization in Deep Learning’ (PGDL) challenge, exemplifies such an approach. The challenge
23"
INTRODUCTION,0.053811659192825115,"was held at NeurIPS 2020 [8] and provides a useful test bed for evaluating complexity measures,
24"
INTRODUCTION,0.05605381165919283,"where a complexity measure is a scalar-valued function that relates a model’s training data and
25"
INTRODUCTION,0.05829596412556054,"parameters to its expected performance on unseen data. Such a predictive complexity measure would
26"
INTRODUCTION,0.06053811659192825,"not only be practically useful but could lead to new insights into how DNNs generalize.
27"
INTRODUCTION,0.06278026905829596,"In this work, we focus on classification margins in deep neural classifiers. It is important to note that
28"
INTRODUCTION,0.06502242152466367,"the term ‘margin’ is, often confusingly, used to refer to 1) output margins [9], 2) input margins [10],
29"
INTRODUCTION,0.06726457399103139,"and 3) hidden margins [11], interchangeably. Here (1) is a measure of the difference in class output
30"
INTRODUCTION,0.06950672645739911,"values, while (2) or (3) is concerned with measuring the distance from a sample to its nearest decision
31"
INTRODUCTION,0.07174887892376682,"boundary in either input or hidden representation space, respectively. In this work, we focus on input
32"
INTRODUCTION,0.07399103139013453,"and hidden margins.
33"
INTRODUCTION,0.07623318385650224,"While margins measured at the hidden representations of deep neural classifiers have been shown to
34"
INTRODUCTION,0.07847533632286996,"be predictive of a model’s generalization, this link has not been established for input space margins.
35"
INTRODUCTION,0.08071748878923767,"We show that, in several circumstances, the classical definition of input margin does not predict
36"
INTRODUCTION,0.08295964125560538,"generalization, but a direction-constrained version of this metric does: a quantity we refer to as
37"
INTRODUCTION,0.08520179372197309,"constrained margins. By measuring margins in directions of ‘high utility’, that is, directions that are
38"
INTRODUCTION,0.08744394618834081,"expected to be more useful to the classification task, we are able to better capture the generalization
39"
INTRODUCTION,0.08968609865470852,"ability of a trained DNN.
40"
INTRODUCTION,0.09192825112107623,"We make several contributions:
41"
INTRODUCTION,0.09417040358744394,"1. Demonstrate the first link between large input margins and generalisation performance, by
42"
INTRODUCTION,0.09641255605381166,"developing a new input margin-based complexity measure that achieves highly competitive
43"
INTRODUCTION,0.09865470852017937,"performance on the PGDL benchmark and outperforms several contemporary complexity
44"
INTRODUCTION,0.10089686098654709,"measures.
45"
SHOW THAT MARGINS DO NOT NECESSARILY NEED TO BE MEASURED AT MULTIPLE HIDDEN LAYERS TO BE,0.1031390134529148,"2. Show that margins do not necessarily need to be measured at multiple hidden layers to be
46"
SHOW THAT MARGINS DO NOT NECESSARILY NEED TO BE MEASURED AT MULTIPLE HIDDEN LAYERS TO BE,0.10538116591928251,"predictive of generalization, as suggested in [11].
47"
SHOW THAT MARGINS DO NOT NECESSARILY NEED TO BE MEASURED AT MULTIPLE HIDDEN LAYERS TO BE,0.10762331838565023,"3. Provide a new perspective on margin analysis and how it applies to DNNs, that of finding
48"
SHOW THAT MARGINS DO NOT NECESSARILY NEED TO BE MEASURED AT MULTIPLE HIDDEN LAYERS TO BE,0.10986547085201794,"high utility directions along which to measure the distance to the boundary instead of
49"
SHOW THAT MARGINS DO NOT NECESSARILY NEED TO BE MEASURED AT MULTIPLE HIDDEN LAYERS TO BE,0.11210762331838565,"focusing on finding the shortest distance.
50"
BACKGROUND,0.11434977578475336,"2
Background
51"
BACKGROUND,0.11659192825112108,"This section provides an overview of existing work on 1) measuring classification margins and their
52"
BACKGROUND,0.11883408071748879,"relationship to generalization, and 2) the PGDL challenge and related complexity measures.
53"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1210762331838565,"2.1
Classification Margins and Generalization
54"
CLASSIFICATION MARGINS AND GENERALIZATION,0.12331838565022421,"Considerable prior work exists on understanding classification margins in machine learning mod-
55"
CLASSIFICATION MARGINS AND GENERALIZATION,0.12556053811659193,"els [12, 13]. The relation between margin and generalization is well understood for classifiers such as
56"
CLASSIFICATION MARGINS AND GENERALIZATION,0.12780269058295965,"support vector machines (SVMs) under statistical learning theory [1]. However, the non-linearity and
57"
CLASSIFICATION MARGINS AND GENERALIZATION,0.13004484304932734,"high dimensionality of DNN decision boundaries complicate such analyses, and precisely measuring
58"
CLASSIFICATION MARGINS AND GENERALIZATION,0.13228699551569506,"these margins is considered intractable [14, 15].
59"
CLASSIFICATION MARGINS AND GENERALIZATION,0.13452914798206278,"A popular technique (which we revisit in this work) is to approximate the classification margin using
60"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1367713004484305,"a first-order Taylor approximation. Elsayed et al. [16] use this method in both the input and hidden
61"
CLASSIFICATION MARGINS AND GENERALIZATION,0.13901345291479822,"space, and then formulate a loss function that maximizes these margins. However, while this results
62"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1412556053811659,"in a measurable increase in margin, it does not result in any significant gains in test accuracy. In a
63"
CLASSIFICATION MARGINS AND GENERALIZATION,0.14349775784753363,"seminal paper, Jiang et al. [11] utilize the same approximation in order to predict the generalization
64"
CLASSIFICATION MARGINS AND GENERALIZATION,0.14573991031390135,"gap of a set of trained networks by training a linear regression model on a summary of their hidden
65"
CLASSIFICATION MARGINS AND GENERALIZATION,0.14798206278026907,"margin distributions. Natekar and Sharma [17] demonstrate that this measure can be further improved
66"
CLASSIFICATION MARGINS AND GENERALIZATION,0.15022421524663676,"if margins are measured using the representations of Mixup [18] or augmented training samples.
67"
CLASSIFICATION MARGINS AND GENERALIZATION,0.15246636771300448,"Similarly, Chuang et al. [6] introduce novel generalization bounds and slightly improve on this metric
68"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1547085201793722,"by proposing an alternative cluster-aware normalization scheme (k-variance [19]).
69"
CLASSIFICATION MARGINS AND GENERALIZATION,0.15695067264573992,"Input margins are generally considered from the point of view of adversarial robustness, and many
70"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1591928251121076,"techniques have been developed to generate adversarial samples on or near the decision boundary.
71"
CLASSIFICATION MARGINS AND GENERALIZATION,0.16143497757847533,"Examples include: the Carlini and Wagner Attack [20], Projected Gradient Descent [21], and
72"
CLASSIFICATION MARGINS AND GENERALIZATION,0.16367713004484305,"DeepFool [22]. Some of these studies have investigated the link between adversarial robustness
73"
CLASSIFICATION MARGINS AND GENERALIZATION,0.16591928251121077,"and generalization, often concluding that an inherent trade-off exists [23, 24, 25]. However, this
74"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1681614349775785,"conclusion and its intricacies are still being debated [26].
75"
CLASSIFICATION MARGINS AND GENERALIZATION,0.17040358744394618,"Yousefzadeh and O’Leary [14] formulate finding a point on the decision boundary as a constrained
76"
CLASSIFICATION MARGINS AND GENERALIZATION,0.1726457399103139,"minimization problem, which is solved using an off-the-shelf optimization method. While this method
77"
CLASSIFICATION MARGINS AND GENERALIZATION,0.17488789237668162,"is more precise, it comes at a great computational cost. To alleviate this, dimensionality reduction
78"
CLASSIFICATION MARGINS AND GENERALIZATION,0.17713004484304934,"techniques are used in the case of image data to reduce the number of input features. In this case, the
79"
CLASSIFICATION MARGINS AND GENERALIZATION,0.17937219730941703,"classification margin is used for the purpose of model interpretability.
80"
CLASSIFICATION MARGINS AND GENERALIZATION,0.18161434977578475,"In this work we propose a modification to the Taylor approximation of the input classification margin
81"
CLASSIFICATION MARGINS AND GENERALIZATION,0.18385650224215247,"(and its iterative alternative DeepFool) in order for it to be more predictive of generalization.
82"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.1860986547085202,"2.2
Predicting Generalization in Deep Learning
83"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.18834080717488788,"The objective of this challenge was to design a complexity measure to rank models according to their
84"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.1905829596412556,"generalization gap. More precisely, participants only had access to a set of trained models, along with
85"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.19282511210762332,"their parameters and training data, and were tasked with ranking the models within each set according
86"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.19506726457399104,"to their generalization gap. Each solution was then evaluated on how well its ranking aligns with the
87"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.19730941704035873,"true ranking on a held-out set of tasks, which was unknown to the competitors.
88"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.19955156950672645,"In total, there are 550 trained models across 8 different tasks and 6 different image classification
89"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.20179372197309417,"datasets, where each task refers to a set of models trained on the same dataset with varying hyperpa-
90"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2040358744394619,"rameters and subsequent test accuracy. Tasks 1, 2, 4, and 5 were available for prototyping and tuning
91"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2062780269058296,"complexity measures, while Task 6 to 9 were used as a held-out set. There is no task 3. The final
92"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2085201793721973,"average score on the test set was the only metric used to rank the competitors. Conditional mutual
93"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.21076233183856502,"information (CMI) is used as evaluation metric, which measures the conditional mutual information
94"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.21300448430493274,"between the complexity measure and true generalization gap, given that a set of hyperparameter
95"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.21524663677130046,"types are observed. This is done in order to prevent spurious correlations resulting from specific
96"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.21748878923766815,"hyperparameters, a step towards establishing whether a causal relationship exists.
97"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.21973094170403587,"All models were trained to approximately the same, near zero, training loss. Note that this implies that
98"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2219730941704036,"ranking models according to either their generalization gap or test accuracy is essentially equivalent.
99"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2242152466367713,"Several interesting solutions were developed during the challenge: In addition to the modification of
100"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.226457399103139,"hidden margins mentioned earlier, the winning team [17] developed several prediction methods based
101"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.22869955156950672,"on the internal representations of each model. Their best-performing method measures clustering
102"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.23094170403587444,"characteristics of hidden layers (using Davies-Bouldin Index [27]), and combines this with the
103"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.23318385650224216,"model’s accuracy on Mixup-augmented training samples. In a similar fashion, the runners-up based
104"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.23542600896860988,"their metrics on measuring the robustness of trained networks to augmentations of their training
105"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.23766816143497757,"data [28].
106"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2399103139013453,"After the competition’s completion, the dataset was made publicly available, inspiring further research:
107"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.242152466367713,"Schiff et al. [29] generated perturbation response curves that ‘capture the accuracy change of a given
108"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.24439461883408073,"network as a function of varying levels of training sample perturbation’ and develop statistical
109"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.24663677130044842,"measures from these curves. They produced eleven complexity measures with different types of
110"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.24887892376681614,"sample Mixup and statistical metrics.
111"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.25112107623318386,"While several of the methods rely on using synthetic samples (e.g. Mixup), Zhang et al. [30] take
112"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2533632286995516,"this to the extreme and generate an artificial test set using pretrained generative adversarial networks
113"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2556053811659193,"(GANs). They demonstrate that simply measuring the classification accuracy on this synthetic test set
114"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.257847533632287,"is very predictive of a model’s generalization. While practically useful, this method does not make a
115"
PREDICTING GENERALIZATION IN DEEP LEARNING,0.2600896860986547,"link between any characteristics of the model and its generalization ability.
116"
THEORETICAL APPROACH,0.2623318385650224,"3
Theoretical approach
117"
THEORETICAL APPROACH,0.2645739910313901,"This section provides a theoretical overview of the proposed complexity measure. We first explain our
118"
THEORETICAL APPROACH,0.26681614349775784,"intuition surrounding classification margins, before mathematically formulating constrained margins.
119"
INTUITION,0.26905829596412556,"3.1
Intuition
120"
INTUITION,0.2713004484304933,"A correctly classified training sample with a large margin can have more varied feature values,
121"
INTUITION,0.273542600896861,"potentially due to noise, and still be correctly classified. However, as we will show, input margins
122"
INTUITION,0.2757847533632287,"are not generally predictive of generalization. This observation is supported by literature regarding
123"
INTUITION,0.27802690582959644,"adversarial robustness, where it has been shown that adversarial retraining (which increases input
124"
INTUITION,0.2802690582959641,"margins) can negatively affect generalization [23, 25].
125"
INTUITION,0.2825112107623318,"Stutz et al. [26] provide a plausible reason for this counter-intuitive observation: Through the use
126"
INTUITION,0.28475336322869954,"of Variational Autoencoder GANs they show that the majority of adversarial samples leave the
127"
INTUITION,0.28699551569506726,"class-specific data manifold of the samples’ class. They offer the intuitive example of black border
128"
INTUITION,0.289237668161435,"pixels in the case of MNIST images, which are zero for all training samples. Samples found on
129"
INTUITION,0.2914798206278027,"the decision boundary which manipulate these border pixels have a zero probability under the data
130"
INTUITION,0.2937219730941704,"distribution, and they do not lie on the underlying manifold.
131"
INTUITION,0.29596412556053814,"We leverage this intuition and argue that any input margin measure that relates to generalization
132"
INTUITION,0.2982062780269058,"should measure distances along directions that do not rely on spurious features in the input space.
133"
INTUITION,0.3004484304932735,"The intuition is that, while nearby decision boundaries exist for virtually any given training sample,
134"
INTUITION,0.30269058295964124,"these nearby decision boundaries are likely in directions which are not inherently useful for test set
135"
INTUITION,0.30493273542600896,"classification, i.e. they diverge from the underlying data manifold.
136"
INTUITION,0.3071748878923767,"More specifically, we argue that margins should be measured in directions of ‘high utility’, that is,
137"
INTUITION,0.3094170403587444,"directions that are expected to be useful for characterising a given dataset, while ignoring those of
138"
INTUITION,0.3116591928251121,"lower utility. In our case, we approximate these directions by defining high utility directions as
139"
INTUITION,0.31390134529147984,"directions which explain a large amount of variance in the data. We extract these using Principal
140"
INTUITION,0.31614349775784756,"Component Analysis (PCA). While typically used as a dimensionality reduction technique, PCA can
141"
INTUITION,0.3183856502242152,"be interpreted as learning a low-dimensional manifold [31], albeit a linear one. In this way, the PCA
142"
INTUITION,0.32062780269058294,"manifold identifies subspaces that are thought to contain the variables that are truly relevant to the
143"
INTUITION,0.32286995515695066,"underlying data distribution, which the out-of-sample data is assumed to also be generated from. In
144"
INTUITION,0.3251121076233184,"the following section, we formalize such a measure.
145"
CONSTRAINED MARGINS,0.3273542600896861,"3.2
Constrained Margins
146"
CONSTRAINED MARGINS,0.3295964125560538,"We first formulate the classical definition of an input margin [14], before adapting it for our purpose.
147"
CONSTRAINED MARGINS,0.33183856502242154,"Let f : X →R|N| denote a classification model with a set of output classes N = {1 . . . n}, and
148"
CONSTRAINED MARGINS,0.33408071748878926,"fk(x) the output value of the model for input sample x and output class k. For a correctly classified
149"
CONSTRAINED MARGINS,0.336322869955157,"input sample x, the goal is to find the closest point ˆx on the decision boundary between the true
150"
CONSTRAINED MARGINS,0.33856502242152464,"class i (where i = arg maxk(fk(x))) and another class j ̸= i. Formally, ˆx is found by solving the
151"
CONSTRAINED MARGINS,0.34080717488789236,"constrained minimization problem:
152"
CONSTRAINED MARGINS,0.3430493273542601,"arg min
ˆx∈[L,U]
||x −ˆx||2
(1)"
CONSTRAINED MARGINS,0.3452914798206278,"with L and U the lower and upper bounds of the search space, respectively, such that
153"
CONSTRAINED MARGINS,0.3475336322869955,"fi(ˆx) = fj(ˆx)
(2)"
CONSTRAINED MARGINS,0.34977578475336324,"for i and j as above.
154"
CONSTRAINED MARGINS,0.35201793721973096,"The margin is then given by the Euclidean distance between the input sample, x, and its corresponding
155"
CONSTRAINED MARGINS,0.3542600896860987,"sample on the decision boundary, ˆx. We now adapt this definition in order to define a ‘constrained
156"
CONSTRAINED MARGINS,0.35650224215246634,"margin’. Let the set P = {p1, p2, ..., pm} denote the first m principal component vectors of the
157"
CONSTRAINED MARGINS,0.35874439461883406,"training dataset, that is, the m orthogonal principal components which explain the most variance.
158"
CONSTRAINED MARGINS,0.3609865470852018,"Such principal components are straightforward to extract by first standardizing (z normalizing) each
159"
CONSTRAINED MARGINS,0.3632286995515695,"feature individually, and then calculating the eigenvectors of the covariance matrix of the standardized
160"
CONSTRAINED MARGINS,0.3654708520179372,"training data.
161"
CONSTRAINED MARGINS,0.36771300448430494,"We now restrict ˆx to any point consisting of the original sample x plus a linear combination of these
162"
CONSTRAINED MARGINS,0.36995515695067266,"principal component vectors, that is, for some coefficient vector B = [β1, β2, ..., βm]
163"
CONSTRAINED MARGINS,0.3721973094170404,"ˆx ≜x + m
X"
CONSTRAINED MARGINS,0.3744394618834081,"i=1
βipi
(3)"
CONSTRAINED MARGINS,0.37668161434977576,"Substituting ˆx into the original objective function of Equation (1), the new objective becomes
164"
CONSTRAINED MARGINS,0.3789237668161435,"min
β || m
X"
CONSTRAINED MARGINS,0.3811659192825112,"i=1
βipi||2
(4)"
CONSTRAINED MARGINS,0.3834080717488789,"such that Equation (2) is approximated within a certain tolerance. For this definition of margin, the
165"
CONSTRAINED MARGINS,0.38565022421524664,"search space is constrained to a lower-dimensional subspace spanned by the principal components
166"
CONSTRAINED MARGINS,0.38789237668161436,"with point x as origin, and the optimization problem then simplifies to finding a point on the decision
167"
CONSTRAINED MARGINS,0.3901345291479821,"boundary within this subspace. By doing so, we ensure that boundary samples that rely on spurious
168"
CONSTRAINED MARGINS,0.3923766816143498,"features (that is, in directions of low utility) are not considered viable solutions to Equation (1). Note
169"
CONSTRAINED MARGINS,0.39461883408071746,"that this formulation does not take any class labels into account for identifying high utility directions.
170"
CONSTRAINED MARGINS,0.3968609865470852,"While it is possible to solve the constrained minimization problem using a constrained optimizer [14],
171"
CONSTRAINED MARGINS,0.3991031390134529,"we approximate the solution by adapting the previously mentioned first-order Taylor approxima-
172"
CONSTRAINED MARGINS,0.4013452914798206,"tion [16, 32], which greatly reduces the computational cost. The Taylor approximation of the
173"
CONSTRAINED MARGINS,0.40358744394618834,"constrained margin d(x) for a sample x between classes i and j when using an L2 norm is given by
174"
CONSTRAINED MARGINS,0.40582959641255606,"d(x) =
fi(x) −fj(x)
|| [ ∇xfi(x) −∇xfj(x) ] PT ||2
(5)"
CONSTRAINED MARGINS,0.4080717488789238,"where P is the m × n matrix formed by the top m principal components with n input features. The
175"
CONSTRAINED MARGINS,0.4103139013452915,"derivation of Equation (5) is included in the supplementary material.
176"
CONSTRAINED MARGINS,0.4125560538116592,"The value d(x) only approximates the margin and the associated discrepancy in Equation (2) can be
177"
CONSTRAINED MARGINS,0.4147982062780269,"large. In order to reduce this to within a reasonable tolerance, we apply Equation (5) in an iterative
178"
CONSTRAINED MARGINS,0.4170403587443946,"manner, using a modification of the well-known DeepFool algorithm [22]. DeepFool was defined
179"
CONSTRAINED MARGINS,0.4192825112107623,"in the context of generating adversarial samples with the smallest possible perturbation, which is in
180"
CONSTRAINED MARGINS,0.42152466367713004,"effect very similar to finding the nearest point on the decision boundary with the smallest violation of
181"
CONSTRAINED MARGINS,0.42376681614349776,"Equation (2).
182"
CONSTRAINED MARGINS,0.4260089686098655,"To extract the DeepFool constrained margin for some sample x, the Taylor approximation of the
183"
CONSTRAINED MARGINS,0.4282511210762332,"constrained margin is calculated between the true class i and all other classes j, individually. A small
184"
CONSTRAINED MARGINS,0.4304932735426009,"step (scaled by a set learning rate) is then taken in the lower-dimensional subspace in the direction
185"
CONSTRAINED MARGINS,0.4327354260089686,"corresponding to the class with smallest margin. This point is then transformed back to the original
186"
CONSTRAINED MARGINS,0.4349775784753363,"feature space and the process is repeated until the distance changes less than a given tolerance in
187"
CONSTRAINED MARGINS,0.437219730941704,"comparison to the previous iteration. The exact process to calculate a DeepFool constrained margin is
188"
CONSTRAINED MARGINS,0.43946188340807174,"described in Algorithm 1. Note that we also clip ˆx according to the minimum and maximum feature
189"
CONSTRAINED MARGINS,0.44170403587443946,"values of the dataset after each step, which ensures that the point stays within the bound constraints
190"
CONSTRAINED MARGINS,0.4439461883408072,"expressed in Equation 1. While this is likely superfluous when generating normal adversarial samples
191"
CONSTRAINED MARGINS,0.4461883408071749,"– they are generally very close to the original x – it is a consideration when the search space is
192"
CONSTRAINED MARGINS,0.4484304932735426,"constrained, with clipped margins performing better. (See the supplementary material for an ablation
193"
CONSTRAINED MARGINS,0.45067264573991034,"analysis of clipping.)
194"
CONSTRAINED MARGINS,0.452914798206278,"Algorithm 1 DeepFool constrained margin calculation
Input: Sample x, classifier f, principal components P
Parameter: Stopping tolerance δ, Learning rate γ, Maximum iterations max
Output: Distance dbest, Equality violation vbest"
CONSTRAINED MARGINS,0.4551569506726457,"1: ˆx ←x, i ←arg max fk(x), d ←0, vbest ←∞, c ←0
2: while c ≤max do
3:
for j ̸= i do
4:
oj ←fi(ˆx) −fj(ˆx)
5:
wj ←[∇fi(ˆx) −∇fj(ˆx)]PT"
CONSTRAINED MARGINS,0.45739910313901344,"6:
end for
7:
l ←arg minj̸=i
|oj|
||wj||2
8:
r ←
|ol|
||wl||2
2 wlP"
CONSTRAINED MARGINS,0.45964125560538116,"9:
ˆx ←ˆx + γr
10:
ˆx ←clip (ˆx)
11:
v ←|ol|
12:
d ←||x −ˆx||2
13:
if v ≥vbest or |d −dbest| < δ then
14:
return dbest, vbest
15:
else
16:
vbest ←v
17:
dbest ←d
18:
c ←c + 1
19:
end if
20: end while
21: return dbest, vbest"
RESULTS,0.4618834080717489,"4
Results
195"
RESULTS,0.4641255605381166,"We investigate the extent to which constrained margins are predictive of generalization by comparing
196"
RESULTS,0.4663677130044843,"the new method with current alternatives. In Section 4.1 we describe our experimental setup.
197"
RESULTS,0.46860986547085204,"Following this, we do a careful comparison between our metric and existing techniques based on
198"
RESULTS,0.47085201793721976,"standard input and hidden margins (Section 4.2) and, finally, we compare with other complexity
199"
RESULTS,0.4730941704035874,"measures (Section 4.3).
200"
EXPERIMENTAL SETUP,0.47533632286995514,"4.1
Experimental setup
201"
EXPERIMENTAL SETUP,0.47757847533632286,"For all margin-based measures our indicator of generalization (complexity measure) is the mean
202"
EXPERIMENTAL SETUP,0.4798206278026906,"margin over 5 000 randomly selected training samples, or alternatively the maximum number available
203"
EXPERIMENTAL SETUP,0.4820627802690583,"for tasks with less than 5 000 training samples. Only correctly classified samples are considered, and
204"
EXPERIMENTAL SETUP,0.484304932735426,"the same training samples are used for all models of the same task. To compare constrained margins
205"
EXPERIMENTAL SETUP,0.48654708520179374,"to input and hidden margins we rank the model test accuracies according to the resulting indicator
206"
EXPERIMENTAL SETUP,0.48878923766816146,"and calculate the Kendall’s rank correlation [33], as used in [34]. This allows for a more interpretable
207"
EXPERIMENTAL SETUP,0.4910313901345291,"comparison than CMI. (As CMI is used throughout the PGDL challenge, we also include the resulting
208"
EXPERIMENTAL SETUP,0.49327354260089684,"CMI scores in the supplementary material.) To compare constrained margins to published results of
209"
EXPERIMENTAL SETUP,0.49551569506726456,"other complexity measures, we measure CMI between the complexity measure and generalization
210"
EXPERIMENTAL SETUP,0.4977578475336323,"gap and contrast this with the reported scores of other methods.
211"
EXPERIMENTAL SETUP,0.5,"As a baseline we calculate the standard input margins (‘Input’) using the first order Taylor approxi-
212"
EXPERIMENTAL SETUP,0.5022421524663677,"mation (Equation 5 without the subspace transformation), as we find that it achieves better results
213"
EXPERIMENTAL SETUP,0.5044843049327354,"than the iterative DeepFool variant and is therefore the stronger baseline; see the supplementary
214"
EXPERIMENTAL SETUP,0.5067264573991032,"material for a full comparison.
215"
EXPERIMENTAL SETUP,0.5089686098654709,"Creating a complexity measure from hidden margins (‘Hidden’) raises the question of which hidden
216"
EXPERIMENTAL SETUP,0.5112107623318386,"layers to consider. Jiang et al. [11] consider three equally spaced layers, Natekar and Sharma [17]
217"
EXPERIMENTAL SETUP,0.5134529147982063,"consider all layers, and Chuang et al. [6] consider either the first or last layer only. We calculate
218"
EXPERIMENTAL SETUP,0.515695067264574,"the mean hidden margin (using the Taylor approximation) for all these variations and find that for
219"
EXPERIMENTAL SETUP,0.5179372197309418,"the tasks studied here, using the first layer performs best, while the mean over all layers comes in
220"
EXPERIMENTAL SETUP,0.5201793721973094,"second. We include both results here. (A full analysis is included in the supplementary material.) We
221"
EXPERIMENTAL SETUP,0.5224215246636771,"normalize each layer’s margin distribution by following [11], and divide each margin by the total
222"
EXPERIMENTAL SETUP,0.5246636771300448,"feature variance at that layer.
223"
EXPERIMENTAL SETUP,0.5269058295964125,"Our constrained margin complexity measure (‘Constrained’) is obtained using Algorithm 1, although
224"
EXPERIMENTAL SETUP,0.5291479820627802,"in practice we implement this in a batched manner. Empirically, we find that the technique is not
225"
EXPERIMENTAL SETUP,0.531390134529148,"very sensitive with regard to the selection of hyperparameters and a single learning rate (γ = 0.25),
226"
EXPERIMENTAL SETUP,0.5336322869955157,"tolerance (δ = 0.01), and max iterations (max = 100) is used across all experiments. The number of
227"
EXPERIMENTAL SETUP,0.5358744394618834,"principal components for each dataset is selected by plotting the explained variance (of the train data)
228"
EXPERIMENTAL SETUP,0.5381165919282511,"per principal component in decreasing order on a logarithmic scale and applying the elbow method
229"
EXPERIMENTAL SETUP,0.5403587443946188,"using the Kneedle algorithm from Satopaa et al [35]. This results in a very low-dimensional search
230"
EXPERIMENTAL SETUP,0.5426008968609866,"space, ranging from 3 to 8 principal components for the seven unique datasets considered.
231"
EXPERIMENTAL SETUP,0.5448430493273543,"In order to prevent biasing our metric to the PGDL test set (tasks 6 to 9) we did not perform any tuning
232"
EXPERIMENTAL SETUP,0.547085201793722,"or development of the complexity measure using these tasks, nor do we tune any hyperparameters
233"
EXPERIMENTAL SETUP,0.5493273542600897,"per task. The choice of principal component selection algorithm was done after a careful analysis of
234"
EXPERIMENTAL SETUP,0.5515695067264574,"Tasks 1 to 5 only, see additional details in the supplementary material. In terms of computational
235"
EXPERIMENTAL SETUP,0.5538116591928252,"expense, we find that calculating the entire constrained margin distribution only takes 1 to 2 minutes
236"
EXPERIMENTAL SETUP,0.5560538116591929,"per model on an Nvidia A30.
237"
MARGIN COMPLEXITY MEASURES,0.5582959641255605,"4.2
Margin complexity measures
238"
MARGIN COMPLEXITY MEASURES,0.5605381165919282,"In Table 1 we show the Kendall’s rank correlation obtained when ranking models according to
239"
MARGIN COMPLEXITY MEASURES,0.5627802690582959,"constrained margin, standard input margins, and hidden margins. It can be observed that standard
240"
MARGIN COMPLEXITY MEASURES,0.5650224215246636,"input margins are not predictive of generalization for most tasks and, in fact, show a negative
241"
MARGIN COMPLEXITY MEASURES,0.5672645739910314,"correlation for some. This unstable behaviour is supported by ongoing work surrounding adversarial
242"
MARGIN COMPLEXITY MEASURES,0.5695067264573991,"robustness and generalization [23, 24, 25]. Furthermore, we observe a very large performance gap
243"
MARGIN COMPLEXITY MEASURES,0.5717488789237668,"between constrained and standard input margins, and an increase from 0.24 to 0.66 average rank
244"
MARGIN COMPLEXITY MEASURES,0.5739910313901345,"correlation is observed by constraining the margin search. This strongly supports our initial intuitions.
245"
MARGIN COMPLEXITY MEASURES,0.5762331838565022,"In the case of hidden margins, performance is more competitive, however, constrained margins
246"
MARGIN COMPLEXITY MEASURES,0.57847533632287,"still outperform hidden margins on 6 out of 8 tasks. One also observes that the selection of hidden
247"
MARGIN COMPLEXITY MEASURES,0.5807174887892377,"layers can have a very large effect, and the discrepancy between the two hidden-layer selections is
248"
MARGIN COMPLEXITY MEASURES,0.5829596412556054,"significant. Given that our constrained margin measurement is limited to the input space, there are
249"
MARGIN COMPLEXITY MEASURES,0.5852017937219731,"several advantages: 1) no normalization is required, as all models share the same input space, and 2)
250"
MARGIN COMPLEXITY MEASURES,0.5874439461883408,"the method is more robust when comparing models with varying topology, as no specific layers need
251"
MARGIN COMPLEXITY MEASURES,0.5896860986547086,"to be selected.
252"
MARGIN COMPLEXITY MEASURES,0.5919282511210763,"Table 1: Kendall’s rank correlation between mean margin and test accuracy for constrained, standard
input, and hidden margins using the first or all layer(s). Models in Task 4 are trained with batch
normalization while models in Task 5 are trained without. There is no Task 3."
MARGIN COMPLEXITY MEASURES,0.594170403587444,"Task
Architecture
Dataset
Constrained
Input
Hidden (1st)
Hidden (all)
1
VGG
CIFAR10
0.8040
0.0265
0.5794
0.7825
2
NiN
SVHN
0.8672
0.6841
0.7037
0.8281
4
FCN
CINIC10
0.6651
0.6251
0.7958
0.2707
5
FCN
CINIC10
0.2292
0.3571
0.5427
0.1329
6
NiN
OxFlowers
0.8008
-0.1351
0.4427
0.2839
7
NiN
OxPets
0.5027
0.3215
0.3623
0.3481
8
VGG
FMNIST
0.6004
-0.1233
-0.0656
0.1859"
NIN,0.5964125560538116,"9
NiN
CIFAR10
(augmented)
0.8145
0.1573
0.7097
0.4556"
NIN,0.5986547085201793,"Average
0.6605
0.2392
0.5088
0.4110"
OTHER COMPLEXITY MEASURES,0.600896860986547,"4.3
Other complexity measures
253"
OTHER COMPLEXITY MEASURES,0.6031390134529148,"To further assess the predictive power of constrained margins, we compare our method to the reported
254"
OTHER COMPLEXITY MEASURES,0.6053811659192825,"CMI scores of several other complexity measures. We compare against three solutions from the
255"
OTHER COMPLEXITY MEASURES,0.6076233183856502,"winning team [17], as well as the best solutions from two more recent works [6, 29], where that of
256"
OTHER COMPLEXITY MEASURES,0.6098654708520179,"Schiff et al. [29] has the highest average test set performance we are aware of. We do not compare
257"
OTHER COMPLEXITY MEASURES,0.6121076233183856,"against pretrained GANs [30]. The original naming of each method is kept. Of particular relevance
258"
OTHER COMPLEXITY MEASURES,0.6143497757847534,"are the MM and AM columns, which are hidden margins applied to Mixup and Augmented samples,
259"
OTHER COMPLEXITY MEASURES,0.6165919282511211,"as well as kV-Margin and kV-GN-Margin which are output and hidden margins with k-Variance
260"
OTHER COMPLEXITY MEASURES,0.6188340807174888,"normalization, respectively. The results of this comparison are shown in Table 2.
261"
OTHER COMPLEXITY MEASURES,0.6210762331838565,"One observes that constrained margins achieve highly competitive scores, and in fact, outperform all
262"
OTHER COMPLEXITY MEASURES,0.6233183856502242,"other measures on 4 out of 8 tasks. It is also important to note that the MM and AM columns show
263"
OTHER COMPLEXITY MEASURES,0.625560538116592,"that hidden margins can be improved in some cases if they are measured using the representations of
264"
OTHER COMPLEXITY MEASURES,0.6278026905829597,"Mixup or augmented training samples. That said, these methods still underperform on average in
265"
OTHER COMPLEXITY MEASURES,0.6300448430493274,"comparison to constrained input margins, which do not rely on any form of data augmentation.
266"
OTHER COMPLEXITY MEASURES,0.6322869955156951,"Table 2: Conditional Mutual Information (CMI) scores for several complexity measures on the PGDL
dataset. Acronyms: DBI=Davies Bouldin Index, LWM=Label-wise Mixup, MM=Mixup Mar-
gins, AM=Augmented Margins, kV =k-Variance, GN=Gradient Normalized, Gi=Gini coefficient,
Mi=Mixup. Test set average is the average over tasks 6 to 9. There is no Task 3. †Indicates a
margin-based measure."
OTHER COMPLEXITY MEASURES,0.6345291479820628,"Task
Natekar and Sharma
Chuang et al.
Schiff et al.
Ours"
OTHER COMPLEXITY MEASURES,0.6367713004484304,"DBI*LWM
MM†
AM†
kV-
Margin 1st†
kV-GN-
Margin 1st†
PCA
Gi&Mi
Constrained
Margin†
1
00.00
01.11
05.73
05.34
17.95
0.04
39.37
2
32.05
47.33
44.60
26.78
44.57
38.08
51.12
4
31.79
43.22
47.22
37.00
30.61
33.76
21.48
5
15.92
34.57
22.82
16.93
16.02
20.33
05.12
6
43.99
11.46
08.67
06.26
04.48
40.06
30.52
7
12.59
21.98
11.97
02.07
03.92
13.19
12.60
8
09.24
01.48
01.28
01.82
00.61
10.30
13.54
9
25.86
20.78
15.25
15.75
21.20
33.16
51.46
Test set
average
22.92
13.93
09.29
06.48
07.55
23.62
27.03"
A CLOSER LOOK,0.6390134529147982,"5
A closer look
267"
A CLOSER LOOK,0.6412556053811659,"In this section we do a further analysis of constrained margins. In Section 5.1 we investigate how the
268"
A CLOSER LOOK,0.6434977578475336,"performance of constrained margins changes when lower utility subspaces are considered, whereafter
269"
A CLOSER LOOK,0.6457399103139013,"we discuss limitations of the method in Section 5.2.
270"
A CLOSER LOOK,0.647982062780269,"Figure 1: Comparison of high to low utility directions using subspaces spanned by 10 principal
components, x-axis indicates the first component in each set of principal components. Left: Kendall’s
rank correlation for Task 1 (blue solid line) and 4 (red dashed line). Right: Mean constrained margin
for models from Task 4."
HIGH TO LOW UTILITY,0.6502242152466368,"5.1
High to low utility
271"
HIGH TO LOW UTILITY,0.6524663677130045,"We examine how high utility directions compare to those of lower utility when calculating constrained
272"
HIGH TO LOW UTILITY,0.6547085201793722,"margins. This allows us to further test our approach, as one would expect that margins measured
273"
HIGH TO LOW UTILITY,0.6569506726457399,"using the lower-ranked principal components should be less predictive of a model’s performance.
274"
HIGH TO LOW UTILITY,0.6591928251121076,"We calculate the mean constrained margin using select subsets of 10 contiguous principal components
275"
HIGH TO LOW UTILITY,0.6614349775784754,"in descending order of explained variance. For example, we calculate the constrained margins using
276"
HIGH TO LOW UTILITY,0.6636771300448431,"components 1 to 10, then 100 to 109, etc. This allows us to calculate the distance to the decision
277"
HIGH TO LOW UTILITY,0.6659192825112108,"boundary using 10 dimensional subspaces of decreasing utility. We, once again, make use of 5 000
278"
HIGH TO LOW UTILITY,0.6681614349775785,"training samples. We restrict ourselves to analysing the training set of tasks (tasks 1-5) and consider
279"
HIGH TO LOW UTILITY,0.6704035874439462,"one task where constrained margins perform very well (Task 1) and one with poorer performance
280"
HIGH TO LOW UTILITY,0.672645739910314,"(Task 4). Figure 1 (left) shows the resulting Kendall’s rank correlation for each subset of principal
281"
HIGH TO LOW UTILITY,0.6748878923766816,"components indexed by the first component in each set (principal component index). The right-hand
282"
HIGH TO LOW UTILITY,0.6771300448430493,"side shows the mean margin of all models from Task 4 at each subset.
283"
HIGH TO LOW UTILITY,0.679372197309417,"As expected, the first principal components lead to margins that are more predictive of generalization.
284"
HIGH TO LOW UTILITY,0.6816143497757847,"We see a gradual decrease in predictive power when considering later principal components. Task
285"
HIGH TO LOW UTILITY,0.6838565022421524,"1 especially suffers this phenomenon, reaching negative correlations. This supports the idea that
286"
HIGH TO LOW UTILITY,0.6860986547085202,"utilizing the directions of highest utility is a necessary aspect of input margin measurements. Addi-
287"
HIGH TO LOW UTILITY,0.6883408071748879,"tionally, one observes that the mean margin also rapidly decreases after the first few sets of principal
288"
HIGH TO LOW UTILITY,0.6905829596412556,"components. After the point shown here (index 1 000), we find that the mean margin increases as
289"
HIGH TO LOW UTILITY,0.6928251121076233,"DeepFool struggles to find samples on the decision boundary within the bound constraints. Due
290"
HIGH TO LOW UTILITY,0.695067264573991,"to this, it is difficult to draw any conclusions from an investigation of the lower-ranked principal
291"
HIGH TO LOW UTILITY,0.6973094170403588,"components. This also points to the notion that the adversarial vulnerability of modern DNNs is in
292"
HIGH TO LOW UTILITY,0.6995515695067265,"part due to nearby decision boundaries in the directions of the mid-tier principal components (the
293"
HIGH TO LOW UTILITY,0.7017937219730942,"range of 100 to 1 000).
294"
LIMITATIONS,0.7040358744394619,"5.2
Limitations
295"
LIMITATIONS,0.7062780269058296,"It has been demonstrated that our proposed metric performs well and aligns with our intitial intuition.
296"
LIMITATIONS,0.7085201793721974,"However, there are also certain limitations that require explanation. Empirically we observe that, for
297"
LIMITATIONS,0.7107623318385651,"tasks where constrained margins perform well, they do so across all hyperparameter variations, with
298"
LIMITATIONS,0.7130044843049327,"the exception of depth. This is illustrated in Figure 2 (left), which shows the mean constrained margin
299"
LIMITATIONS,0.7152466367713004,"versus test accuracy for Task 1. We observe that sets of networks with two and six convolutional
300"
LIMITATIONS,0.7174887892376681,"layers, respectively, each exhibit a separate relationship between margin and test accuracy. This
301"
LIMITATIONS,0.7197309417040358,"discrepancy is not always as strongly present: for Task 6 all three depth configurations show a more
302"
LIMITATIONS,0.7219730941704036,"similar relationship, as observed on the right of Figure 2, although the discrepancy is still present. The
303"
LIMITATIONS,0.7242152466367713,"same trend holds for all tasks where it is observed (1, 2, 4, 6, 9). It appears that shallower networks
304"
LIMITATIONS,0.726457399103139,"model the input space in a distinctly different fashion than their deeper counterparts.
305"
LIMITATIONS,0.7286995515695067,"Figure 2: Mean constrained margin versus test accuracy for PGDL Task 1 (left) and 6 (right). Left:
Models with 2 (green circle) and 6 (blue star) convolutional layers. Right: Models with 6 (blue star),
9 (red square), and 12 (black diamond) convolutional layers."
LIMITATIONS,0.7309417040358744,"For tasks such as 5 and 7, where constrained margins perform more poorly, there is no single
306"
LIMITATIONS,0.7331838565022422,"hyperparameter that appears to be the culprit. We do note that the resulting scatter plots of margin
307"
LIMITATIONS,0.7354260089686099,"versus test accuracy never show points in the lower right (large margin but low generalization) or
308"
LIMITATIONS,0.7376681614349776,"upper left (small margin but high generalization) quadrants. It is therefore possible that a larger
309"
LIMITATIONS,0.7399103139013453,"constrained margin is always beneficial to a model’s generalization, even though it is not always
310"
LIMITATIONS,0.742152466367713,"fully descriptive of its performance. Finally, while our approach to selecting the number of principal
311"
LIMITATIONS,0.7443946188340808,"components is experimentally sound, the results can be further improved if the optimal number is
312"
LIMITATIONS,0.7466367713004485,"known, see the supplementary material for details.
313"
CONCLUSION,0.7488789237668162,"6
Conclusion
314"
CONCLUSION,0.7511210762331838,"We have shown that constraining input margins to high utility subspaces can significantly improve
315"
CONCLUSION,0.7533632286995515,"their predictive power i.t.o generalization. Specifically, we have used the principal components of the
316"
CONCLUSION,0.7556053811659192,"data as a proxy for identifying these subspaces, which can be considered a rough approximation of
317"
CONCLUSION,0.757847533632287,"the underlying data manifold.
318"
CONCLUSION,0.7600896860986547,"Constraining the search to a warped subspace and using Euclidean distance to measure closeness is
319"
CONCLUSION,0.7623318385650224,"equivalent to defining a new distance metric on the original space. We are therefore, in effect, seeking
320"
CONCLUSION,0.7645739910313901,"a relevant distance metric to measure the closeness of the decision boundary. Understanding the
321"
CONCLUSION,0.7668161434977578,"requirements for such a metric remains an open question. Unfortunately, current approximations and
322"
CONCLUSION,0.7690582959641256,"methods for finding points on the decision boundary are largely confined to Lp metrics. The positive
323"
CONCLUSION,0.7713004484304933,"results achieved with the current PCA-and-Euclidean-based approach provide strong motivation that
324"
CONCLUSION,0.773542600896861,"this is a useful avenue to pursue. Furthermore, we believe that constrained margins can be used
325"
CONCLUSION,0.7757847533632287,"as a tool to further probe generalization, similar to the large amount of work that has been done
326"
CONCLUSION,0.7780269058295964,"surrounding standard input margins and characterization of decision boundaries.
327"
CONCLUSION,0.7802690582959642,"In conclusion, we propose constraining input margins to make them more predictive of generalization
328"
CONCLUSION,0.7825112107623319,"in DNNs. It has been demonstrated that this greatly increases the predictive power of input margins,
329"
CONCLUSION,0.7847533632286996,"and also outperforms hidden margins and several other contemporary methods on the PGDL tasks.
330"
CONCLUSION,0.7869955156950673,"This method has the benefits of requiring no per-layer normalization, no arbitrary selection of hidden
331"
CONCLUSION,0.7892376681614349,"layers, and does not rely on any form of surrogate test set (e.g. data augmentation or synthetic
332"
CONCLUSION,0.7914798206278026,"samples).
333"
REFERENCES,0.7937219730941704,"References
334"
REFERENCES,0.7959641255605381,"[1] Vladimir N Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural
335"
REFERENCES,0.7982062780269058,"Networks, 10(5):988–999, 1999.
336"
REFERENCES,0.8004484304932735,"[2] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the
337"
REFERENCES,0.8026905829596412,"generalization error of combined classifiers. The Annals of Statistics, 30(1):1–50, 2002.
338"
REFERENCES,0.804932735426009,"[3] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
339"
REFERENCES,0.8071748878923767,"deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
340"
REFERENCES,0.8094170403587444,"115, 2021.
341"
REFERENCES,0.8116591928251121,"[4] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds
342"
REFERENCES,0.8139013452914798,"for deep nets via a compression approach. In International Conference on Machine Learning
343"
REFERENCES,0.8161434977578476,"(ICML), pages 254–263. PMLR, 2018.
344"
REFERENCES,0.8183856502242153,"[5] Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in Deep Learning.
345"
REFERENCES,0.820627802690583,"Mathematical Aspects of Deep Learning. Cambridge University Press, 2022.
346"
REFERENCES,0.8228699551569507,"[6] Ching-Yao Chuang, Youssef Mroueh, Kristjan Greenewald, Antonio Torralba, and Stefanie
347"
REFERENCES,0.8251121076233184,"Jegelka. Measuring generalization with optimal transport. Advances in Neural Information
348"
REFERENCES,0.827354260089686,"Processing Systems, 34:8294–8306, 2021.
349"
REFERENCES,0.8295964125560538,"[7] Sanae Lotfi, Marc Anton Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum,
350"
REFERENCES,0.8318385650224215,"and Andrew Gordon Wilson. PAC-Bayes compression bounds so tight that they can explain
351"
REFERENCES,0.8340807174887892,"generalization. In Advances in Neural Information Processing Systems, 2022.
352"
REFERENCES,0.8363228699551569,"[8] Yiding Jiang, Pierre Foret, Scott Yak, Daniel M Roy, Hossein Mobahi, Gintare Karolina
353"
REFERENCES,0.8385650224215246,"Dziugaite, Samy Bengio, Suriya Gunasekar, Isabelle Guyon, and Behnam Neyshabur. Neurips
354"
REFERENCES,0.8408071748878924,"2020 competition: Predicting generalization in deep learning. arXiv preprint arXiv:2012.07976,
355"
REFERENCES,0.8430493273542601,"2020.
356"
REFERENCES,0.8452914798206278,"[9] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds
357"
REFERENCES,0.8475336322869955,"for neural networks. Advances in Neural Information Processing Systems, 30, 2017.
358"
REFERENCES,0.8497757847533632,"[10] Jure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin
359"
REFERENCES,0.852017937219731,"deep neural networks. IEEE Transactions on Signal Processing, 65(16):4265–4280, 2017.
360"
REFERENCES,0.8542600896860987,"[11] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization
361"
REFERENCES,0.8565022421524664,"gap in deep networks with margin distributions. In International Conference on Learning
362"
REFERENCES,0.8587443946188341,"Representations, 2018.
363"
REFERENCES,0.8609865470852018,"[12] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm for optimal
364"
REFERENCES,0.8632286995515696,"margin classifiers. In Proceedings of the fifth annual workshop on Computational Learning
365"
REFERENCES,0.8654708520179372,"Theory, pages 144–152, 1992.
366"
REFERENCES,0.8677130044843049,"[13] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest
367"
REFERENCES,0.8699551569506726,"neighbor classification. Journal of Machine Learning Research, 10(9):207–244, 2009.
368"
REFERENCES,0.8721973094170403,"[14] Roozbeh Yousefzadeh and Dianne P. O’Leary.
Deep learning interpretation: Flip points
369"
REFERENCES,0.874439461883408,"and homotopy methods. In Jianfeng Lu and Rachel Ward, editors, Proceedings of The First
370"
REFERENCES,0.8766816143497758,"Mathematical and Scientific Machine Learning Conference, volume 107 of Proceedings of
371"
REFERENCES,0.8789237668161435,"Machine Learning Research, pages 1–26. PMLR, 20–24 Jul 2020.
372"
REFERENCES,0.8811659192825112,"[15] Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez,
373"
REFERENCES,0.8834080717488789,"Kannan Ramchandran, and Michael W Mahoney. Boundary thickness and robustness in learning
374"
REFERENCES,0.8856502242152466,"models. Advances in Neural Information Processing Systems, 33:6223–6234, 2020.
375"
REFERENCES,0.8878923766816144,"[16] Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
376"
REFERENCES,0.8901345291479821,"margin deep networks for classification. Advances in Neural Information Processing Systems,
377"
REFERENCES,0.8923766816143498,"31, 2018.
378"
REFERENCES,0.8946188340807175,"[17] Parth Natekar and Manik Sharma. Representation based complexity measures for predicting
379"
REFERENCES,0.8968609865470852,"generalization in deep learning. arXiv preprint arXiv:2012.02775, 2020.
380"
REFERENCES,0.899103139013453,"[18] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond
381"
REFERENCES,0.9013452914798207,"empirical risk minimization. In International Conference on Learning Representations, 2018.
382"
REFERENCES,0.9035874439461884,"[19] Justin Solomon, Kristjan Greenewald, and Haikady Nagaraja. k-variance: A clustered notion of
383"
REFERENCES,0.905829596412556,"variance. SIAM Journal on Mathematics of Data Science, 4(3):957–978, 2022.
384"
REFERENCES,0.9080717488789237,"[20] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
385"
REFERENCES,0.9103139013452914,"IEEE Symposium on Security and Privacy, pages 39–57. IEEE, 2017.
386"
REFERENCES,0.9125560538116592,"[21] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
387"
REFERENCES,0.9147982062780269,"Towards deep learning models resistant to adversarial attacks. In International Conference on
388"
REFERENCES,0.9170403587443946,"Learning Representations, 2018.
389"
REFERENCES,0.9192825112107623,"[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: a simple
390"
REFERENCES,0.92152466367713,"and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on
391"
REFERENCES,0.9237668161434978,"Computer Vision and Pattern Recognition, pages 2574–2582, 2016.
392"
REFERENCES,0.9260089686098655,"[23] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander
393"
REFERENCES,0.9282511210762332,"Madry. Robustness May Be at Odds with Accuracy. In International Conference on Learning
394"
REFERENCES,0.9304932735426009,"Representations, 2019.
395"
REFERENCES,0.9327354260089686,"[24] Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is Robustness
396"
REFERENCES,0.9349775784753364,"the Cost of Accuracy?–A Comprehensive Study on the Robustness of 18 Deep Image Classifi-
397"
REFERENCES,0.9372197309417041,"cation Models. In Proceedings of the European Conference on Computer Vision (ECCV), pages
398"
REFERENCES,0.9394618834080718,"631–648, 2018.
399"
REFERENCES,0.9417040358744395,"[25] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Adversarial
400"
REFERENCES,0.9439461883408071,"Training Can Hurt Generalization. In ICML Workshop on Identifying and Understanding Deep
401"
REFERENCES,0.9461883408071748,"Learning Phenomena, 2019.
402"
REFERENCES,0.9484304932735426,"[26] David Stutz, Matthias Hein, and Bernt Schiele. Disentangling Adversarial Robustness and
403"
REFERENCES,0.9506726457399103,"Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
404"
REFERENCES,0.952914798206278,"Recognition, pages 6976–6987, 2019.
405"
REFERENCES,0.9551569506726457,"[27] David L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on
406"
REFERENCES,0.9573991031390134,"Pattern Analysis and Machine Intelligence, (2):224–227, 1979.
407"
REFERENCES,0.9596412556053812,"[28] Dhruva Kashyap, Natarajan Subramanyam, et al. Robustness to augmentations as a generaliza-
408"
REFERENCES,0.9618834080717489,"tion metric. arXiv preprint arXiv:2101.06459, 2021.
409"
REFERENCES,0.9641255605381166,"[29] Yair Schiff, Brian Quanz, Payel Das, and Pin-Yu Chen. Predicting Deep Neural Network
410"
REFERENCES,0.9663677130044843,"Generalization with Perturbation Response Curves. Advances in Neural Information Processing
411"
REFERENCES,0.968609865470852,"Systems, 34:21176–21188, 2021.
412"
REFERENCES,0.9708520179372198,"[30] Yi Zhang, Arushi Gupta, Nikunj Saunshi, and Sanjeev Arora. On Predicting Generalization
413"
REFERENCES,0.9730941704035875,"using GANs. In International Conference on Learning Representations, 2022.
414"
REFERENCES,0.9753363228699552,"[31] Geoffrey E Hinton, Peter Dayan, and Michael Revow. Modeling the Manifolds of Images of
415"
REFERENCES,0.9775784753363229,"Handwritten Digits. IEEE Transactions on Neural Networks, 8(1):65–74, 1997.
416"
REFERENCES,0.9798206278026906,"[32] Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvári. Learning with a Strong
417"
REFERENCES,0.9820627802690582,"Adversary. arXiv preprint arXiv:1511.03034, 2015.
418"
REFERENCES,0.984304932735426,"[33] M. G. Kendall. A New Measure of Rank Correlation. Biometrika, 30(1-2):81–93, 06 1938.
419"
REFERENCES,0.9865470852017937,"[34] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
420"
REFERENCES,0.9887892376681614,"generalization measures and where to find them. In International Conference on Learning
421"
REFERENCES,0.9910313901345291,"Representations, 2019.
422"
REFERENCES,0.9932735426008968,"[35] Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. Finding a"" kneedle"" in a
423"
REFERENCES,0.9955156950672646,"haystack: Detecting knee points in system behavior. In 2011 31st international Conference on
424"
REFERENCES,0.9977578475336323,"Distributed Computing systems workshops, pages 166–171. IEEE, 2011.
425"
