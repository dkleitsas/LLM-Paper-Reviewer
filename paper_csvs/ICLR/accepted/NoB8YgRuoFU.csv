Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004219409282700422,"We propose a novel prediction interval (PI) method for uncertainty quantiﬁcation,
which addresses three major issues with the state-of-the-art PI methods. First,
existing PI methods require retraining of neural networks (NNs) for every given
conﬁdence level and suffer from the crossing issue in calculating multiple PIs.
Second, they usually rely on customized loss functions with extra sensitive hyper-
parameters for which ﬁne tuning is required to achieve a well-calibrated PI. Third,
they usually underestimate uncertainties of out-of-distribution (OOD) samples
leading to over-conﬁdent PIs. Our PI3NN method calculates PIs from linear com-
binations of three NNs, each of which is independently trained using the standard
mean squared error loss. The coefﬁcients of the linear combinations are computed
using root-ﬁnding algorithms to ensure tight PIs for a given conﬁdence level. We
theoretically prove that PI3NN can calculate PIs for a series of conﬁdence levels
without retraining NNs and it completely avoids the crossing issue. Additionally,
PI3NN does not introduce any unusual hyperparameters resulting in a stable per-
formance. Furthermore, we address OOD identiﬁcation challenge by introducing
an initialization scheme which provides reasonably larger PIs of the OOD samples
than those of the in-distribution samples. Benchmark and real-world experiments
show that our method outperforms several state-of-the-art approaches with respect
to predictive uncertainty quality, robustness, and OOD samples identiﬁcation."
INTRODUCTION,0.008438818565400843,"1
INTRODUCTION"
INTRODUCTION,0.012658227848101266,"Neural networks (NNs) are widely used in prediction tasks due to their unrivaled performance in
modeling complex functions. Although NNs provide accurate predictions, quantifying the uncertainty
of their predictions is a challenge. Uncertainty quantiﬁcation (UQ) is crucial for many real-world ap-
plications such as self-driving cars and autonomous experimental and operational controls. Moreover,
effectively quantiﬁed uncertainties are useful for interpreting conﬁdence, capturing out-of-distribution
(OOD) data, and realizing when the model is likely to fail."
INTRODUCTION,0.016877637130801686,"A diverse set of UQ approaches have been developed for NNs, ranging from fully Bayesian NNs
[1], to assumption-based variational inference [2; 3], and to empirical ensemble approaches [4; 5; 6].
These methods require either high computational demands or strong assumptions or large memory
costs. Another set of UQ methods is to calculate prediction intervals (PIs), which provides a lower
and upper bound for an NN’s output such that the value of the prediction falls between the bounds
for some target conﬁdence level γ (e.g., 95%) of the unseen data. The most common techniques
to construct the PI are the delta method (also known as analytical method) [7; 8], methods that
directly predict the variance (e.g., maximum likelihood methods and ensemble methods) [9; 10] and
quantile regression methods [11; 12]. Most recent PI methods are developed on the high-quality
principle—a PI should be as narrow as possible, whilst capturing a speciﬁed portion of data. Khosravi
et al. [13] developed the Lower Upper Bound Estimation method by incorporating the high-quality
principle directly into the NN loss function for the ﬁrst time. Inspired by [13], the Quality-Driven
(QD) prediction interval approach in [14] deﬁnes a loss function that can generate a high-quality PI"
INTRODUCTION,0.02109704641350211,∗Corresponding author.
INTRODUCTION,0.02531645569620253,Published as a conference paper at ICLR 2022
INTRODUCTION,0.029535864978902954,"and is able to optimize the loss using stochastic gradient descent as well. Built on QD, the Prediction
Intervals with speciﬁc Value prEdictioN (PIVEN) method in [15] adds an extra term in the loss to
enable the calculation of point estimates and the PI method in [16] further integrates a penalty function
to the loss to improve the training stability of QD. The Simultaneous Quantile Regression (SQR)
method [17] proposes a loss function to learn all quantiles of a target variable with one NN. Existing
PI methods suffer from one or more of the following limitations: (1) Requirement of NNs retraining
for every given conﬁdence level γ and suffering from the crossing issue [18] when calculating PIs for
differnt γ; (2) Requirement of hyper-parameter ﬁne tuning; (3) Lack of OOD identiﬁcation capability
resulting in unreasonably narrow PIs for OOD samples (See Section 2.1 for details)."
INTRODUCTION,0.03375527426160337,"To address these limitations, we develop PI3NN (prediction interval based on three neural networks)—
a novel method for calculating PIs. We ﬁrst lay out the theoretical foundation of the PI3NN in Section
3.1 by proving Lemma 1 that connects the ground-truth upper and lower bounds of a PI to a family of
models that are easy to approximate. Another advantage of the model family introduced in Lemma 1
is that it makes the NN training independent of the target conﬁdence level, which makes it possible to
calculate multiple PIs for a series of conﬁdence levels without retraining NNs. On the basis of the
theoretical foundation, we describe the main PI3NN algorithm in Section 3.2. Different from existing
PI methods [14; 15; 16; 17] that design complicated loss functions to obtain a well-calibrated PI by
ﬁne-tuning their loss hyper-parameters, our method simply uses the standard mean squared error
(MSE) loss for training. Additionally, we theoretically prove that PI3NN has a non-crossing property
in Section 3.2.1. Moreover, we address the OOD identiﬁcation challenge by proposing a simple yet
effective initialization scheme in Section 3.3, which provides larger PIs of the OOD samples than
those of the in-distribution (InD) samples."
INTRODUCTION,0.0379746835443038,The main contributions of this work are summarized as follows:
INTRODUCTION,0.04219409282700422,"1. Our PI3NN method can calculate PIs for a series of conﬁdence levels without retraining NNs;
and the calculated PIs completely avoid the crossing issue as proved in Section 3.2.1."
INTRODUCTION,0.046413502109704644,"2. The theoretical foundation in Section 3.1 enables PI3NN to use the standard MSE loss to train
three NNs without introducing extra hyper-parameters that need to be ﬁne-tuned for a good PI."
INTRODUCTION,0.05063291139240506,"3. We develop a simple yet effective initialization scheme and a conﬁdence score in Section 3.3 to
identify OOD samples and reasonably quantify their uncertainty."
RELATED WORK,0.05485232067510549,"1.1
RELATED WORK"
RELATED WORK,0.05907172995780591,"Non-PI approaches for UQ. Early and recent work was nicely summarized and reviewed in these three
survey papers [19; 20; 21]. The non-PI approaches use a distribution to quantify uncertainty, which
can be further divided into Bayesian [1] and non-Bayesian methods. Bayesian methods—including
Markov chain Monte Carlo [22] and Hamiltonian Monte Carlo [23]—place priors on NN weights and
then infer a posterior distribution from the training data. Non-Bayesian methods includes evidential
regression [6] that places priors directly over the likelihood function and some ensemble learning
methods that do not use priors. For example, the DER method proposed in [6] placed evidential
priors over the Gaussian likelihood function and training the NN to infer the hyperparameters of the
evidential distribution. Gal and Ghahramani [3] proposed using Monte Carlo dropout to estimate
predictive uncertainty by using Dropout (which can be interpreted as ensemble model combination) at
test time. Deep ensembles [4] employed a combination of ensembles of NNs learning and adversarial
training to quantify uncertainty with a Gaussian distributional assumption on the data. Pearce et
al. [5] proposed an anchored ensembling by using the randomized MAP sampling to increase the
diversity of NN training in the ensemble."
BACKGROUND,0.06329113924050633,"2
BACKGROUND"
BACKGROUND,0.06751054852320675,"We are interested in building PIs for the output of the regression problem y = f(x) + ε from a
training set Dtrain = {(xi, yi)}N
i=1, where x ∈Rd, y ∈R, and ε is the random noise. We do not
impose distributional assumption on the noise ε. Since the output of f(x) is polluted by the noise,
the output y of the regression model y = f(x) + ε is also a random variable. For a given conﬁdence
level γ ∈(0, 1), the ground-truth 100γ% PI, denoted by [Ltrue
γ
(x), U true
γ
(x)], is deﬁned by"
BACKGROUND,0.07172995780590717,"P[Ltrue
γ
(x) ≤y ≤U true
γ
(x)] = γ.
(1)"
BACKGROUND,0.0759493670886076,Published as a conference paper at ICLR 2022
BACKGROUND,0.08016877637130802,"Note that Ltrue
γ
(x) and U true
γ
(x) are not unique for a ﬁxed γ in the deﬁnition of Eq. (1), because
the probability of y outside the PI, i.e., P[y > U true
γ
(x) or y < Ltrue
γ
(x)], may be split in any way
between the two tails. In this work, we aim to approximate the PI that satisﬁes"
BACKGROUND,0.08438818565400844,"P[y > U true
γ
(x)] = (1 −γ)/2
and
P[y < Ltrue
γ
(x)] = (1 −γ)/2,
(2)"
BACKGROUND,0.08860759493670886,which is unique because the probability outside the PI is equally split between the two tails.
MOTIVATION,0.09282700421940929,"2.1
MOTIVATION"
MOTIVATION,0.0970464135021097,"Recent effort on PI methods, e.g., QD, SQR and PIVEN, tend to exploit the NNs to learn the upper
and lower bounds in Eq. (2). Despite their promising performance, these methods suffer from some
unsatisfactory drawbacks. This effort is motivated by their following limitations:"
MOTIVATION,0.10126582278481013,"• Requirement of retraining NNs for every given γ and the crossing issue when calculating multiple
PIs. Existing PI methods usually incorporate γ into their loss functions for training NNs, so that
each NN can only predict PI for a speciﬁc γ, which is not convenient for users. On the other
hand, even multiple NNs can be trained for PIs with multiple γ values, the approximate PIs often
encounter the crossing issue, e.g., the upper bounds for different γ values may cross each other,
which is not reasonable. To alleviate this issue, a non-crossing constraint is usually added to the
loss as a regularization to encourage non-crossing PIs. However, the non-crossing constraint may
deteriorate the quality of the approximate PI, because due to the trade-off between the original
loss and the non-crossing constraint."
MOTIVATION,0.10548523206751055,"• Requiring hyper-parameter ﬁne tuning. Recently developed PI methods [14; 15; 16] tend to
design complicated loss functions to obtain a well-calibrated PI. Although these work has achieved
promising results, their performance is sensitive to the unusual hyperparameters introduced into
their customized loss functions. Thus, hyperparameter ﬁne tuning is usually required for each
speciﬁc problem to achieve satisfactory upper and lower bounds."
MOTIVATION,0.10970464135021098,"• Lack of OOD identiﬁcation capability. OOD identiﬁcation is a critical metric to evaluate the
performance of an UQ method. It has been received signiﬁcant attention in recent UQ method
development in the machine learning community. However, the OOD identiﬁcation has not been
deeply studied for PI methods in solving the regression problem. Even though there are some
promising empirical results on OOD-aware PIs [15], the underlying mechanism is still not clear,
making it difﬁcult to extend the occasional success to a general setting."
MOTIVATION,0.11392405063291139,"3
THE PI3NN METHOD"
MOTIVATION,0.11814345991561181,"The main contribution is presented in this section. Section 3.1 shows a theoretical justiﬁcation of our
method, where Lemma 1 plays a critical role to connect the ground-truth upper and lower bounds to
Eq. (5) and Eq. (6) that are easier to approximate. Section 3.2 introduces the main PI3NN algorithm
inspired by Lemma 1. Section 3.3 describes how to turn on the OOD identiﬁcation capability in the
PI3NN method. The features of our methods are illustrated using a simple example in Section 3.4."
THEORETICAL JUSTIFICATION,0.12236286919831224,"3.1
THEORETICAL JUSTIFICATION"
THEORETICAL JUSTIFICATION,0.12658227848101267,"To proceed, we ﬁrst rewrite Eq. (2) to an equivalent form"
THEORETICAL JUSTIFICATION,0.1308016877637131,"E

1y>U true
γ
(x)

−(1 −γ)/2 = 0
and
E

1y<Ltrue
γ
(x)

−(1 −γ)/2 = 0,
(3)"
THEORETICAL JUSTIFICATION,0.1350210970464135,"where 1(·) is the indicator function, deﬁned by"
THEORETICAL JUSTIFICATION,0.13924050632911392,"1y>U true
γ
(x) ="
THEORETICAL JUSTIFICATION,0.14345991561181434,"(
1, if y > U true
γ
(x),"
THEORETICAL JUSTIFICATION,0.14767932489451477,"0, otherwise,
and
1y<Ltrue
γ
(x) ="
THEORETICAL JUSTIFICATION,0.1518987341772152,"(
1, if y < Ltrue
γ
(x),"
THEORETICAL JUSTIFICATION,0.15611814345991562,"0, otherwise.
(4)"
THEORETICAL JUSTIFICATION,0.16033755274261605,"For simplicity, we take U true
γ
(x) as an example in the rest of Section 3.1, and the same derivation
can be applied to Ltrue
γ
(x). In deﬁnition, U true
γ
(x) has the following three properties:"
THEORETICAL JUSTIFICATION,0.16455696202531644,"(P1) U true
γ
(x) ≥M[y] = f(x) + M[ε], where M[·] denotes the median of a random variable,"
THEORETICAL JUSTIFICATION,0.16877637130801687,Published as a conference paper at ICLR 2022
THEORETICAL JUSTIFICATION,0.1729957805907173,"(P2) U true
γ
(x) −f(x) is independent of x, because ε is independent of x,"
THEORETICAL JUSTIFICATION,0.17721518987341772,"(P3) U true
γ
(x) is unique for a given conﬁdence level γ ∈(0, 1)."
THEORETICAL JUSTIFICATION,0.18143459915611815,"Next, we show that the ground-truth U true
γ
(x) is a member of the following model family:"
THEORETICAL JUSTIFICATION,0.18565400843881857,"ˆU(x|α) = M[y] + α E

(y −M[y])1y−M[y]>0

,
(5)"
THEORETICAL JUSTIFICATION,0.189873417721519,"where α ≥0 is a scalar model parameter and M[·] denotes the median of y. To this end, we prove the
following lemma:"
THEORETICAL JUSTIFICATION,0.1940928270042194,"Lemma 1. For a given γ ∈(0, 1), there exists a unique α(γ), such that ˆU(x|α(γ)) = U true
γ
(x)."
THEORETICAL JUSTIFICATION,0.19831223628691982,"The proof of this lemma is given in Appendix A. The same result can be derived for the lower bound,
i.e., for a given γ ∈(0, 1), there exists a unique β(γ), such that ˆL(x|β(γ)) = Ltrue
γ
(x), where
ˆL(x|β) is deﬁned by"
THEORETICAL JUSTIFICATION,0.20253164556962025,"ˆL(x|β) = M[y] −β E

(M[y] −y)1M[y]−y>0

.
(6)"
THEORETICAL JUSTIFICATION,0.20675105485232068,"Lemma 1 connects the ground-truth upper and lower bounds to the model families ˆU(x|α) and ˆL(x|β),
such that the task of approximating U true
γ
(x) and Ltrue
γ
(x) can be decomposed into two sub-tasks:"
THEORETICAL JUSTIFICATION,0.2109704641350211,"• Approximate the models ˆU(x|α) and ˆL(x|β) by training NNs;
• Calculate the optimal values of α(γ) and β(γ) for any γ ∈(0, 1)."
THEORETICAL JUSTIFICATION,0.21518987341772153,"In above theoretical justiﬁcation we assume the noise ε is homoscedastic as commonly done in
regression problems. But note that we use this assumption just for simplifying the theoretical proof.
In practice, our method can be generally applied to problems with different forms of noise."
THE MAIN ALGORITHM,0.21940928270042195,"3.2
THE MAIN ALGORITHM"
THE MAIN ALGORITHM,0.22362869198312235,"PI3NN accomplishes the above two sub-tasks in four steps. Step 1-3 build approximations of ˆU(x|α)
and ˆL(x|β) by training three independent NNs, denoted by fω(x), uθ(x), lξ(x), which approximate
M[y], E[(y −M[y])1y−M[y]>0], and E[(M[y] −y)1M[y]−y>0], respectively. After the three NNs are
trained, we calculate the optimal values of α(γ) and β(γ) in Step 4 using root-ﬁnding techniques."
THE MAIN ALGORITHM,0.22784810126582278,"Step 1: Train fω(x) to approximate the mean E[y]. This step follows the standard NN-based
regression process using the standard MSE loss. The trained fω(x) will serve two purposes. The ﬁrst
is to provide a baseline to approximate M[y] in Step 2; the second is to provide a point estimate of
E[f]. In this step, we use the standard L1 and L2 regularization to avoid over-ﬁtting."
THE MAIN ALGORITHM,0.2320675105485232,"Step 2: Add a shift ν to fω(x) to approximate the median M[y]. A scalar ν is added to fω(x), such
that each of the two data sets Dupper and Dlower, deﬁned by"
THE MAIN ALGORITHM,0.23628691983122363,"Dupper =

(xi, yi −fω(xi) −ν)
 yi ≥fω(xi) + ν, i = 1, . . . , N
	
,"
THE MAIN ALGORITHM,0.24050632911392406,"Dlower =

(xi, fω(xi) + ν −yi)
 yi < fω(xi) + ν, i = 1, . . . , N
	
,
(7)"
THE MAIN ALGORITHM,0.24472573839662448,"contains 50% of the total training samples in Dtrain. Note that Dupper and Dlower include data
points above and below fω(x) + ν, respectively. The value of the shift ν is calculated using a
root-ﬁnding method [24] (e.g., the bisection method) to ﬁnd the root (i.e., the zero) of the function
Q(ν) = P"
THE MAIN ALGORITHM,0.2489451476793249,"(xi,yi)∈Dtrain 1yi>fω(xi)+ν −0.5N. This is similar to ﬁnding the roots of Eq. (10), so we
refer to Lemma 2 for the existence of the root."
THE MAIN ALGORITHM,0.25316455696202533,"Step 3: Train uθ(x) and lξ(x) to learn E[(y −M[y])1y−M[y]>0] and E[(M[y] −y)1M[y]−y>0]. We
use Dupper to train uθ(x), and use Dlower to train lξ(x). To ensure the outputs of uθ(x) and lξ(x)
are positive, we add the operation
p"
THE MAIN ALGORITHM,0.25738396624472576,"(·)2 to the output layer of both NNs. The two NNs are trained
separately using the standard MSE loss, i.e.,"
THE MAIN ALGORITHM,0.2616033755274262,"θ = arg min
θ X"
THE MAIN ALGORITHM,0.26582278481012656,"Dupper
(yi−fω(xi)−ν−uθ(xi))2,
ξ = arg min
ξ X"
THE MAIN ALGORITHM,0.270042194092827,"Dlower
(fω(xi)+ν−yi−lξ(xi))2. (8)"
THE MAIN ALGORITHM,0.2742616033755274,Published as a conference paper at ICLR 2022
THE MAIN ALGORITHM,0.27848101265822783,"Step 4: Calculate the PI via root-ﬁnding methods. Using the three NNs trained in Step 1-3, we build
approximations of ˆU(x|α) and ˆL(x|α), denoted by"
THE MAIN ALGORITHM,0.28270042194092826,"U(x|α) = fω(x) + ν + αuθ(x),
L(x|β) = fω(x) + ν −βlξ(x).
(9)"
THE MAIN ALGORITHM,0.2869198312236287,"For a sequence of conﬁdence levels Γ = {γk}K
k=1, we use the bisection method [24] to calculate the
optimal value of α(γk) and β(γk) by ﬁnding the roots (i.e., the zeros) of the following functions:"
THE MAIN ALGORITHM,0.2911392405063291,"Qupper(α)
=
X"
THE MAIN ALGORITHM,0.29535864978902954,"(xi,yi)∈Dupper
1yi>U(xi|α) −⌈N(1 −γk)/2⌉,"
THE MAIN ALGORITHM,0.29957805907172996,"Qlower(β)
=
X"
THE MAIN ALGORITHM,0.3037974683544304,"(xi,yi)∈Dlower
1yi<L(xi|β) −⌈N(1 −γk)/2⌉,
(10)"
THE MAIN ALGORITHM,0.3080168776371308,"for k = 1, . . . , K. Note that Eq. (10) can be reviewed as the discrete version of Eq. (3) and/or
Eq. (??). When Dupper and Dlower have ﬁnite number of data points, it is easy to ﬁnd α(γ) and β(γ)
such that Qupper(α(γ)) = Qlower(β(γ)) = 0 up to the machine precision (See Section 3.2.1 for
details). Then, the ﬁnal K PIs for the conﬁdence levels Γ = {γk}K
k=1 is given by

L(x|β(γk)), U(x|α(γk))

=

fω(x) + ν −β(γk)lξ(x), fωk(x) + ν + α(γk)uθ(x)

.
(11)"
THE MAIN ALGORITHM,0.31223628691983124,"Remark 1 (Generate PIs for multiple γ without re-training the NNs). Note that the NN training in
Step 1-3 is independent of the conﬁdence level γ, so the PIs for multiple γ can be obtained by only
performing the root ﬁnding algorithm in Step 4 without re-training the three NNs. In contrast, most
existing PI methods (e.g., QD and PIVEN) require re-training NNs when γ is changed. This feature,
ultimately enabled by Lemma 1, makes our method more efﬁcient and convenient to use in practice."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.31645569620253167,"3.2.1
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3206751054852321,"The goal of this subsection is to theoretically prove that our algorithm can completely avoid the
“crossing issue” [18] suffered by most existing PI methods without adding any non-crossing constraints
to the loss function. To proceed, we ﬁrst prove the existence of the roots of the functions in Eq. (10).
Lemma 2 (Existence of the roots). Given the trained models fω(x), uθ(x), lξ(x) and ν, if the
training set Dtrain does not have repeated samples (i.e., (xi, yi) ̸= (xj, yj) if i ̸= j), then, for any
ﬁxed γ > 0, there exist α(γ) and β(γ) such that Qupper(α(γ)) = Qlower(β(γ)) = 0."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.32489451476793246,"The proof of this lemma is given in Appendix B. In practice, the reason why it is easy to ﬁnd the
roots of Qupper and Qlower is that the number of training samples are ﬁnite, so that the bisection
method can vary α and β continuously in [0, ∞) to search for “gaps” among the data points to ﬁnd
the zeros of Qupper(α) and Qlower(β). Below is the theorem about the non-crossing property of the
PIs obtained by our method. Lemma 2 will be used to in the proof of the theorem."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3291139240506329,"Theorem 1 (Non-crossing property). Given the trained models fω(x), uθ(x), lξ(x) and ν, if γ > γ′
satisfying ⌈N(1 −γ)/2⌉< ⌈N(1 −γ′)/2⌉, then U(x|α(γ)) > U(x|α(γ′)) and L(x|β(γ)) <
L(x|β(γ′))."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3333333333333333,"The proof of this theorem is given in Appendix C. Theorem 1 tells us that the width of the PI obtained
by our method will monotonically increasing with γ, which completely avoids the crossing issue.
Again, this property is ultimately enabled by the equivalent representation of the PI in Lemma 1."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.33755274261603374,"3.3
IDENTIFYING OUT-OF-DISTRIBUTION (OOD) SAMPLES"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.34177215189873417,"When using the trained model fω(x) to make predictions for x ̸∈Dtrain, it is required that a UQ
method can identify OOD samples and quantify their uncertainties, i.e., for x ̸∈Dtrain, the PI’s width
increases with the distance between x and Dtrain. Inspired by the expressivity of ReLU networks, we
add the OOD identiﬁcation feature to the PI3NN method by initializing the bias of the output layer
of uθ(x) and lξ(x) to a relatively large value. Speciﬁcally, when the OOD identiﬁcation feature is
turned on, we perform the following initialization before training uθ(x) and lξ(x) in Step 3."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3459915611814346,"• Pretrain uθ and lξ using the training set Dtrain with default initialization and compute the mean
outputs µupper = PN
i=1 uθ(xi)/N and µlower = PN
i=1 lξ(xi)/N based on the training set."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.350210970464135,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.35443037974683544,"• Initialize the biases of the output layers of uθ(x) and lξ(x) to c µupper and c µlower, where c is
a large number (e.g., c = 10 in this work), such that the initial outputs of uθ(x) and lξ(x) are
signiﬁcantly larger than µupper and µ."
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.35864978902953587,• Re-train uθ(x) and lξ(x) following Step 3 using the MSE loss.
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3628691983122363,"The key in above initialization scheme is the increase of the bias of the output layer of uθ(x) and
lξ(x). It is known that a ReLU network provides a piecewise linear function. The weights and biases
of hidden layers deﬁnes how the input space is partitioned into a set of linear regions [25]; the weights
of the output layer determines how those linear regions are combined; and the bias of the output
layer acts as a shifting parameter. The weights and biases are usually initialized with some standard
distribution, e.g., uniform or Gaussian. Setting the biases to cµupper and cµlower with a large value of
c will signiﬁcantly increase the output of the initial uθ(x) and lξ(x) (See Figure 1 for demonstration).
During the training, the loss in Eq. (8) will encourage the decrease of uθ(x) and lξ(x) only for the
in-distribution (InD) samples (i.e., x ∈Dtrain), not for the OOD samples. Therefore, after training,
the PI will be wider in the OOD region than in the InD region. Additionally, due to continuity of the
ReLU network, the PI width (PIW) will increase with the distance between x and Dtrain showing a
decreasing conﬁdence. Moreover, we can deﬁne the following conﬁdence score by exploiting the
width of the PI, i.e.,"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3670886075949367,Λ(x) = min
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.37130801687763715,"(PN
i=1(U(xi|α(γ)) −L(xi|β(γ))/N"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.3755274261603376,"U(x|α(γ)) −L(x|β(γ))
, 1.0 )"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.379746835443038,",
(12)"
THEORETICAL ANALYSIS ON THE NON-CROSSING PROPERTY,0.38396624472573837,"where the numerator is the mean PI width (MPIW) of the training set Dtrain and the denominator is
the PIW of a testing sample x. If x is an InD sample, its conﬁdence score should be close to one. As
x moves away from Dtrain, the PIW gets larger and thus the conﬁdence score becomes smaller."
AN ILLUSTRATIVE EXAMPLE,0.3881856540084388,"3.4
AN ILLUSTRATIVE EXAMPLE"
AN ILLUSTRATIVE EXAMPLE,0.3924050632911392,"We use a one-dimensional non-Gaussian cubic regression dataset to illustrate PI3NN. We train models
on y = x3 + ε within [−4, 4] and test them on [−7, 7]. The noise ε is deﬁned by ε = s(ζ)ζ, where
ζ ∼N(0, 1), s(ζ) = 30 for ζ ≥0 and s(ζ) = 10 for ζ < 0. Top panels of Figure 1 illustrate the four
steps of the PI3NN algorithm. After Step 1-3 we ﬁnish the NNs training and obtain fω(x)+ν −lξ(x)
and fω(x)+ν +uθ(x). Then for a given series of conﬁdence levels γ, we use root-ﬁnding technique
to calculate the corresponding α and β in Step 4 and obtain the associated 100γ% PIs deﬁned in
Eq. (11). Figure 1 shows the 90%, 95% and 99% PIs. In calculation of the series of PIs for the multiple
conﬁdence levels, PI3NN only trains the three NNs once and the resulted PIs have no “crossing issue”.
Bottom panels of Figure 1 demonstrate the effectiveness of PI3NN’s OOD identiﬁcation capability
and illustrate that it is our bias initialization scheme (Section 3.3) enables PI3NN to identify the OOD
regions and reasonably quantify their uncertainty."
EXPERIMENTAL EVALUATION,0.39662447257383965,"4
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.4008438818565401,"We evaluate our PI3NN method by comparing its performance to four top-performing baselines—QD
[14], PIVEN [15], SQR [17], and DER [6] on four experiments. The ﬁrst experiment focuses on
evaluation of the accuracy and robustness of the methods in calculating PIs. The last three experiments
focus on the assessment of the OOD identiﬁcation capability. The code of PI3NN is avaliable at
https://github.com/liusiyan/PI3NN."
UCI REGRESSION BENCHMARKS,0.4050632911392405,"4.1
UCI REGRESSION BENCHMARKS"
UCI REGRESSION BENCHMARKS,0.4092827004219409,"We ﬁrst evaluate PI3NN to calculate 95% PIs on nine UCI datasets [26]. The quality of PI is measured
by the Prediction Interval Coverage Probability (PICP) which represents the ratio of data samples
that fall within their respective PIs. We are interested in calculating a well-calibrated PIs, i.e., the test
PICP is close to the desired value of 0.95 and meanwhile has a narrow width. In this experiment,
we focus on the metric of PICP, not the PI width (PIW) because it is meaningful to compare PIWs
only when the methods have the same PICPs. A small PICP certainly produces small PIWs, but the
small PICP may be much lower than the desired conﬁdence level causing unreasonable PIs. In this
experiment, for different model settings, some PI methods produce a wide range of PICPs while"
UCI REGRESSION BENCHMARKS,0.41350210970464135,Published as a conference paper at ICLR 2022
UCI REGRESSION BENCHMARKS,0.4177215189873418,"Figure 1: Top panels illustrate the four steps of our PI3NN algorithm. Bottom panels illustrate the effectiveness
of the OOD identiﬁcation feature. As shown in bottom left, when turning on the OOD identiﬁcation feature by
initializing the bias of the output layer of uθ and lξ to a large value, PI3NN can accurately identify the OOD
regions [−7, −4] ∪[4, 7] by giving them increasingly large PIs as their distance from the training data gets large.
In bottom right, if we turn off the OOD identiﬁcation by using the default initialization, PI3NN will not identify
the OOD regions by giving them a narrow uncertainty bound."
UCI REGRESSION BENCHMARKS,0.4219409282700422,"others result in a relatively narrow range; it is unfair to compare the PIWs between methods that
produce different ranges of PICPs."
UCI REGRESSION BENCHMARKS,0.42616033755274263,"We use a single hidden layer ReLU NN for all the methods. For each method, we test multiple
scenarios with different hyper-parameter conﬁgurations, different training-testing data splits. For
each scenario, we use the average of 5 runs (with 5 random seeds) to generate the result. All the
methods consider the variation of the hidden layer size. QD and PIVEN additionally consider the
variation of the extra hyper-parameters introduced in their loss functions. See Appendix for details
about the experimental setup."
UCI REGRESSION BENCHMARKS,0.43037974683544306,"Table 1 summarizes the results. It shows the mean, standard deviation (std), best, and worst PICP
(across hyper-parameter conﬁgurations) for all the methods on the testing datasets. DER, because of
the Gaussian assumption, tends to overestimate the PI resulting in PICPs close to 1.0 for most datasets
and produces the worst performance in terms of PICP. For the four PI methods, our PI3NN achieves
the best mean PICP (closest to 0.95) with the smallest std. PI3NN is also the top performer in terms
of the best and the worst PICP. This suggests that PI3NN can produce a well-calibrated PI and its
performance is less affected by the hyper-parameter conﬁguration. In contrast, QD, PIVEN, and SQR
require a careful hyper-parameter tuning to obtain the comparable best PICP as the PI3NN, but for
each hyper-parameter set, their PICPs vary a lot with a large std and the worst PICP can be much
lower than the desired 0.95 resulting in an unreasonable PI. Thus, this experiment demonstrates the
advantage of our method in producing a well-calibrated PI without time-consuming hyperparameter
ﬁne tuning, which makes it convenient for practical use."
UCI REGRESSION BENCHMARKS,0.4345991561181435,"4.2
OOD IDENTIFICATION IN A 10-DIMENSIONAL FUNCTION EXAMPLE"
UCI REGRESSION BENCHMARKS,0.4388185654008439,"We use a 10-dimensional cubic function f(x) =
1
10(x3
1 + · · · + x3
10) to demonstrate the effectiveness
of the proposed bias initialization strategy in Section 3.3 for OOD identiﬁcation. The training data of
x is generated by drawing 5,000 samples from the normal distribution N(0, 1); the corresponding
training data of outputs y is obtained from y = f(x) + ε with ε following a Gaussian distribution
N(0, 1). We deﬁne a test set with 1,000 OOD samples, where x are drawn from a shifted normal
distribution N(2, 1). We calculate the 90% PI for both the training (InD) and the testing (OOD) data
using all the ﬁve methods. For PI3NN, we consider two situations with the OOD identiﬁcation turned
on and turned off. We set the constant c in Section 3.3 to 10 to turn on PI3NN’s OOD identiﬁcation
feature, and use the standard initialization to turn the feature off. We use PI width (PIW) to evaluate"
UCI REGRESSION BENCHMARKS,0.4430379746835443,Published as a conference paper at ICLR 2022
UCI REGRESSION BENCHMARKS,0.4472573839662447,"Boston
Concrete Energy
Kin8nm
Naval
Power
Protein
Wine
Yacht PI3NN"
UCI REGRESSION BENCHMARKS,0.45147679324894513,"Mean
0.95
0.94
0.95
0.94
0.95
0.95
0.95
0.95
0.95
Std
0.03
0.02
0.03
0.006
0.006
0.004
0.003
0.02
0.02
Best
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.94
Worst
0.88
0.89
0.87
0.93
0.94
0.96
0.94
0.91
0.90 QD"
UCI REGRESSION BENCHMARKS,0.45569620253164556,"Mean
0.85
0.84
0.87
0.91
0.94
0.94
0.93
0.90
0.93
Std
0.06
0.05
0.04
0.02
0.03
0.01
0.02
0.03
0.06
Best
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.93
Worst
0.69
0.66
0.78
0.86
0.59
0.92
0.90
0.83
0.71 PIVEN"
UCI REGRESSION BENCHMARKS,0.459915611814346,"Mean
0.83
0.83
0.82
0.87
0.94
0.94
0.91
0.82
0.88
Std
0.07
0.05
0.05
0.02
0.01
0.01
0.02
0.04
0.08
Best
0.94
0.94
0.93
0.91
0.95
0.95
0.94
0.91
0.93
Worst
0.51
0.50
0.67
0.82
0.91
0.90
0.88
0.68
0.52 SQR"
UCI REGRESSION BENCHMARKS,0.4641350210970464,"Mean
0.76
0.83
0.83
0.86
0.87
0.88
0.87
0.85
0.82
Std
0.17
0.12
0.11
0.13
0.14
0.13
0.13
0.12
0.13
Best
0.94
0.95
0.96
0.95
0.95
0.95
0.95
0.95
0.94
Worst
0.39
0.52
0.58
0.56
0.54
0.56
0.51
0.42
0.48 DER"
UCI REGRESSION BENCHMARKS,0.46835443037974683,"Mean
0.87
1.0
0.98
1.0
1.0
1.0
1.0
0.98
0.83
Std
0.03
0.0
0.04
0.0
0.0
0.0
0.004
0.008
0.1
Best
0.94
1.0
0.94
1.0
1.0
1.0
0.98
0.97
0.93
Worst
0.80
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.61"
UCI REGRESSION BENCHMARKS,0.47257383966244726,"Table 1: Evaluation of 95% PI on testing data. We show the mean, standard deviation (std), best, and worst
PICP (across hyper-parameter conﬁgurations) for all methods. The best performer should produce PICP values
closest to the desired 0.95. DER tends to overestimate the PI resulting in PICPs close to 1.0 for most datasets
and produces the worst performance. For the four PI methods, our PI3NN shows the top performance by giving
the best mean PICP (closest to 0.95) with the smallest std across hyper-parameter conﬁgurations. QD, PIVEN,
and SQR require a careful hyper-parameter tuning to obtain the comparable best PICP as the PI3NN, but for
each hyper-parameter set, their PICP std is large and the worst PICP can be much lower than the desired 0.95."
UCI REGRESSION BENCHMARKS,0.4767932489451477,"the method’s capability in OOD identiﬁcation. A method having an OOD identiﬁcation capability
should produce larger PIWs of the OOD samples than those of the InD samples. Additionally, we use
the conﬁdence score deﬁned in Eq. (12) to quantitatively evaluate the method’s capability in OOD
identiﬁcation. InD samples should have a close-to-one conﬁdence score and the OOD samples should
have a remarkably smaller conﬁdence score than that of the InD datasets."
UCI REGRESSION BENCHMARKS,0.4810126582278481,"Figure 2: Probability density functions of the PI width for the training (InD) and testing (OOD) data for all
the ﬁve methods. When we use a probability density function (PDF) to ﬁt PIWs of the InD and OOD samples,
respectively, we should be able to see two separated PDFs with the PDF of OOD samples shifting to the right
having larger PIWs. If the two PDFs are overlapped to each other, then it indicates the method can not identify
the OOD samples by reasonably quantifying their uncertainty."
UCI REGRESSION BENCHMARKS,0.48523206751054854,"PI3NN (With
OOD)"
UCI REGRESSION BENCHMARKS,0.48945147679324896,"PI3NN (No
OOD)"
UCI REGRESSION BENCHMARKS,0.4936708860759494,"QD
PIVEN
SQR
DER"
UCI REGRESSION BENCHMARKS,0.4978902953586498,"Train (InD)
0.91 ± 0.15
0.92 ± 0.12
0.94 ± 0.08
0.94 ± 0.08
0.91 ± 0.12
0.95 ± 0.08
Test (OOD)
0.28 ± 0.08
0.80 ± 0.18
0.90 ± 0.10
0.87 ± 0.05
0.98 ± 0.07
0.94 ± 0.11"
UCI REGRESSION BENCHMARKS,0.5021097046413502,Table 2: Mean ± Standard Deviation of the conﬁdence score (Eq. (12) with γ = 0.9) for the 10D cubic example
UCI REGRESSION BENCHMARKS,0.5063291139240507,"Figure 2 shows the PDFs of the PIWs of the InD and OOD samples. The ﬁgures indicate that our
PI3NN method, with OOD identiﬁcation feature turned on, can effectively identify the OOD samples
by assigning them larger and well-separated PIWs than those of the InD dataset. The other methods
are not able to identify the OOD samples by mixing their PIWs into the InD data’s PIWs, showing"
UCI REGRESSION BENCHMARKS,0.510548523206751,Published as a conference paper at ICLR 2022
UCI REGRESSION BENCHMARKS,0.5147679324894515,"overlapped PDFs. These methods’ performance is similar to the case when the PI3NN turns off its
OOD identiﬁcation capability. Table 2 lists the mean and std of the conﬁdence scores for the InD and
OOD datasets. This table further shows that PI3NN with OOD identiﬁcation feature turned on can
effectively identify the OOD samples by giving them smaller conﬁdence scores than those of the InD
samples, while other methods produce over-conﬁdent (wider) PIs for the OOD samples and cannot
separate OOD samples from InD dataset."
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5189873417721519,"4.3
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET"
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5232067510548524,"We test the performance of our method in OOD detection using the Flight Delay dataset [27], which
contains about 2 million US domestic ﬂights in 2008 and their cases for delay. We use the PI3NN
method to predict the regression map from 6 input parameters, i.e., Day of Month, Day of Week,
AirTime, Flight Distance, TaxiIn (taxi in time), and TaxiOut (taxi out time), to the Arrival Delay.
The US federal aviation administration (FAA) categories general commercial airports into several
ranks, i.e., Rank 1 (Large Hub): 1% or more of the total departures per airport, Rank 2 (Medium
Hub): 0.25% - 1.0% of the total departures per airport, Rank 3 (Small Hub): 0.05% - 0.25% of the
total departures per airport, and Rank 4 (NonHub): <0.05% but more than 100 ﬂights per year."
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5274261603375527,"The training set consists of randomly select 18K data from Rank 4 (Nonhub) airports’ data. We also
randomly select another 20K data from each rank to form 4 testing sets. It is known that there is a
data shift from Rank 1 airports to Rank 4 airports, because the bigger an airport, the longer the Taxi
time, the averaged AirTime and the Flight Distance. Thus, an OOD-aware PI method should be able
to identify those data shift. The setup for PI3NN method and the baselines are given in Appendix."
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5316455696202531,"Table 3 shows the conﬁdence scores (i.e., Eq. (12) with γ = 0.9) computed by our method and the
baselines. Our method with the OOD identiﬁcation feature turned on successfully detected the data
shift by giving the Rank 1,2,3 testing data lower conﬁdence scores. In contrast, other methods as well
as our method (with OOD identiﬁcation turned off) fail to identify the data shift from the training set
to the Rank 1,2,3 testing sets."
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5358649789029536,"Data
Rank 1
Rank 2
Rank 3
Rank 4"
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.540084388185654,"PI3NN (With OOD)
0.24 ± 0.05
0.32 ± 0.09
0.72 ± 0.22
0.92 ± 0.24
PI3NN (No OOD)
0.72 ± 0.32
0.79 ± 0.32
0.88 ± 0.27
0.90 ± 0.26
QD
0.89 ± 0.24
0.90 ± 0.24
0.92 ± 0.16
0.83 ± 0.14
PIVEN
0.83 ± 0.23
0.87 ± 0.21
0.99 ± 0.04
0.99 ± 0.03
SQR
0.98 ± 0.06
0.96 ± 1.41
0.97 ± 0.09
0.94 ± 0.10
DER
0.98 ± 0.13
1.00 ± 0.01
1.00 ± 0.00
1.00 ± 0.01"
OOD IDENTIFICATION IN A FLIGHT DELAY DATASET,0.5443037974683544,Table 3: Mean ± Standard Deviation of the conﬁdence score (Eq. (12) with γ = 0.9) for the ﬂight delay data.
CONCLUSION AND DISCUSSION,0.5485232067510548,"5
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.5527426160337553,"The limitations of the PI3NN method include: (1) For a target function with multiple outputs, each
output needs to have its own PI and OOD conﬁdence score. The PI and the conﬁdence score cannot
oversee all the outputs. For example, this could make it challenging to apply PI3NN to NN models
having image as outputs, (e.g., autoencoders). (2) The effectiveness of the OOD detection approach
depends on that there are sufﬁciently many piecewise linear regions (of ReLU networks) in the
OOD area. So far, this is achieved by the standard random initialization (ensure uniform distributed
piecewise linear regions at the beginning of training) and L1/L2 regularization (ensure the linear
regions not collapse together around the training set). However, there is no guarantee of uniformly
distributed piecewise linear regions after training. Improvement of this requires signiﬁcant theoretical
work on how to manipulate the piecewise linear function deﬁned by the ReLU network. This work
is for purely research purpose and will have no negative social impact. (3) When the noise ε has
heavy tails, our method will struggle when the distance between tail quantiles becomes very large.
In this case, the number of training data needed to accurately capture those tail quntiles may be too
large to afford. It is a common issue for all distribution-free PI methods, and reasonable distributional
assumption may be needed to alleviate this issue."
CONCLUSION AND DISCUSSION,0.5569620253164557,Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENT,0.5611814345991561,"6
ACKNOWLEDGEMENT"
ACKNOWLEDGEMENT,0.5654008438818565,"This work was supported by the U.S. Department of Energy, Ofﬁce of Science, Ofﬁce of Advanced
Scientiﬁc Computing Research, Applied Mathematics program; and by the Artiﬁcial Intelligence
Initiative at the Oak Ridge National Laboratory (ORNL). ORNL is operated by UT-Battelle, LLC.,
for the U.S. Department of Energy under Contract DE-AC05-00OR22725. This manuscript has been
authored by UT-Battelle, LLC. The US government retains and the publisher, by accepting the article
for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevoca-
ble,worldwide license to publish or reproduce the published form of this manuscript, or allow others
to do so, for US government purposes. DOE will provide public access to these results of federally
sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-
public-access-plan). We would like to thank Paul Laiu at ORNL for helpful discussions."
REFERENCES,0.569620253164557,REFERENCES
REFERENCES,0.5738396624472574,"[1] D. J. C. MacKay, “A practical bayesian framework for backpropagation networks,” Neural
Comput., vol. 4, p. 448–472, May 1992."
REFERENCES,0.5780590717299579,"[2] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley, “Stochastic variational inference,” Journal
of Machine Learning Research, vol. 14, no. 4, pp. 1303–1347, 2013."
REFERENCES,0.5822784810126582,"[3] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing model un-
certainty in deep learning,” in Proceedings of The 33rd International Conference on Machine
Learning (M. F. Balcan and K. Q. Weinberger, eds.), vol. 48 of Proceedings of Machine Learning
Research, (New York, New York, USA), pp. 1050–1059, PMLR, 20–22 Jun 2016."
REFERENCES,0.5864978902953587,"[4] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive uncertainty
estimation using deep ensembles,” in Proceedings of the 31st International Conference on
Neural Information Processing Systems, NIPS’17, (Red Hook, NY, USA), p. 6405–6416,
Curran Associates Inc., 2017."
REFERENCES,0.5907172995780591,"[5] T. Pearce, F. Leibfried, and A. Brintrup, “Uncertainty in neural networks: Approximately
bayesian ensembling,” in Proceedings of the Twenty Third International Conference on Artiﬁcial
Intelligence and Statistics (S. Chiappa and R. Calandra, eds.), vol. 108 of Proceedings of
Machine Learning Research, pp. 234–244, PMLR, 26–28 Aug 2020."
REFERENCES,0.5949367088607594,"[6] A. Amini, W. Schwarting, A. Soleimany, and D. Rus, “Deep evidential regression,” in Advances
in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin, eds.), vol. 33, pp. 14927–14937, Curran Associates, Inc., 2020."
REFERENCES,0.5991561181434599,"[7] R. D. D. VlEAUX, J. Schumi, J. Schweinsberg, and L. H. Ungar, “Prediction intervals for neural
networks via nonlinear regression,” Technometrics, vol. 40, no. 4, pp. 273–282, 1998."
REFERENCES,0.6033755274261603,"[8] J. T. G. Hwang and A. A. Ding, “Prediction intervals for artiﬁcial neural networks,” Journal of
the American Statistical Association, vol. 92, no. 438, pp. 748–757, 1997."
REFERENCES,0.6075949367088608,"[9] J. Carney, P. Cunningham, and U. Bhagwan, “Conﬁdence and prediction intervals for neu-
ral network ensembles,” in IJCNN’99. International Joint Conference on Neural Networks.
Proceedings (Cat. No.99CH36339), vol. 2, pp. 1215–1218 vol.2, 1999."
REFERENCES,0.6118143459915611,"[10] T. Heskes, “Practical conﬁdence and prediction intervals,” in Proceedings of the 9th Inter-
national Conference on Neural Information Processing Systems, NIPS’96, (Cambridge, MA,
USA), p. 176–182, MIT Press, 1996."
REFERENCES,0.6160337552742616,"[11] R. Koenker and G. Bassett, “Regression quantiles,” Econometrica, vol. 46, no. 1, pp. 33–50,
1978."
REFERENCES,0.620253164556962,"[12] R. Koenker and K. F. Hallock, “Quantile regression,” Journal of Economic Perspectives, vol. 15,
pp. 143–156, December 2001."
REFERENCES,0.6244725738396625,Published as a conference paper at ICLR 2022
REFERENCES,0.6286919831223629,"[13] A. Khosravi, S. Nahavandi, D. Creighton, and A. F. Atiya, “Lower upper bound estimation
method for construction of neural network-based prediction intervals,” IEEE Transactions on
Neural Networks, vol. 22, no. 3, pp. 337–346, 2011."
REFERENCES,0.6329113924050633,"[14] T. Pearce, A. Brintrup, M. Zaki, and A. Neely, “High-quality prediction intervals for deep
learning: A distribution-free, ensembled approach,” in Proceedings of the 35th International
Conference on Machine Learning (J. Dy and A. Krause, eds.), vol. 80 of Proceedings of Machine
Learning Research, pp. 4075–4084, PMLR, 10–15 Jul 2018."
REFERENCES,0.6371308016877637,"[15] E. Simhayev, G. Katz, and L. Rokach, “Piven: A deep neural network for prediction intervals
with speciﬁc value prediction,” 2021."
REFERENCES,0.6413502109704642,"[16] T. S. Salem, H. Langseth, and H. Ramampiaro, “Prediction intervals: Split normal mixture
from quality-driven deep ensembles,” in Proceedings of the 36th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI) (J. Peters and D. Sontag, eds.), vol. 124 of Proceedings of Machine
Learning Research, pp. 1179–1187, PMLR, 03–06 Aug 2020."
REFERENCES,0.6455696202531646,"[17] N. Tagasovska and D. Lopez-Paz, “Single-model uncertainties for deep learning,” in Advances in
Neural Information Processing Systems (H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-
Buc, E. Fox, and R. Garnett, eds.), vol. 32, Curran Associates, Inc., 2019."
REFERENCES,0.6497890295358649,"[18] F. Zhou, J. Wang, and X. Feng, “Non-crossing quantile regression for distributional rein-
forcement learning,” in Advances in Neural Information Processing Systems (H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, eds.), vol. 33, pp. 15909–15919, Curran
Associates, Inc., 2020."
REFERENCES,0.6540084388185654,"[19] L. V. Jospin, W. L. Buntine, F. Boussaïd, H. Laga, and M. Bennamoun, “Hands-on bayesian
neural networks - a tutorial for deep learning users,” CoRR, vol. abs/2007.06823, 2020."
REFERENCES,0.6582278481012658,"[20] H. Wang and D.-Y. Yeung, “A survey on bayesian deep learning,” ACM Computing Surveys
(CSUR), vol. 53, no. 5, pp. 1–37, 2021."
REFERENCES,0.6624472573839663,"[21] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth,
X. Cao, A. Khosravi, U. R. Acharya, V. Makarenkov, and S. Nahavandi, “A review of uncertainty
quantiﬁcation in deep learning: Techniques, applications and challenges,” 2021."
REFERENCES,0.6666666666666666,"[22] R. M. Neal, Bayesian learning for neural networks, vol. 118. Springer Science & Business
Media, 2012."
REFERENCES,0.6708860759493671,"[23] J. T. Springenberg, A. Klein, S. Falkner, and F. Hutter, “Bayesian optimization with robust
bayesian neural networks,” in Advances in Neural Information Processing Systems (D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, eds.), vol. 29, Curran Associates, Inc.,
2016."
REFERENCES,0.6751054852320675,"[24] A. Quarteroni, R. Sacco, and F. Saleri, Numerical Mathematics (Texts in Applied Mathematics).
Berlin, Heidelberg: Springer-Verlag, 2006."
REFERENCES,0.679324894514768,"[25] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee, “Understanding Deep Neural Networks with
Rectiﬁed Linear Units,” arXiv e-prints, p. arXiv:1611.01491, Nov. 2016."
REFERENCES,0.6835443037974683,"[26] D. Dua and C. Graff, “UCI machine learning repository,” 2017."
REFERENCES,0.6877637130801688,"[27] J. Hensman, N. Fusi, and N. D. Lawrence, “Gaussian processes for big data,” in Proceedings
of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, UAI’13, (Arlington,
Virginia, USA), p. 282–290, AUAI Press, 2013."
REFERENCES,0.6919831223628692,Published as a conference paper at ICLR 2022
REFERENCES,0.6962025316455697,APPENDIX
REFERENCES,0.70042194092827,"A
THE PROOF OF LEMMA 1"
REFERENCES,0.7046413502109705,"Proof. Substitute y = f(x) + ε into Eq. (5), we can rewrite Eq. (5) as ˆU(x|α) = f(x) + M[ε] +
αE[(ε −M[ε])1ε−M[ε]>0], where E[(ε −M[ε])1ε−M[ε]>0] = E[(y −M[y])1y−M[y]>0] is strictly
positive and independent of x. Then, we have that ˆU(x|α) satisﬁes the property (P1)."
REFERENCES,0.7088607594936709,"For a ﬁxed x, there exists a unique value ˜α(γ, x), which depends on both γ and x, such that"
REFERENCES,0.7130801687763713,"E[1y> ˆU(x|˜α(γ,x))] −(1 −γ)/2 = 0.
(13)"
REFERENCES,0.7172995780590717,"Comparing Eq. (13) and the deﬁnition of the upper bound in Eq. (3), we can see that ˆU(x|˜α(γ, x))
is also a upper bound of the 100γ% PI for a given x. Due to the uniqueness of the upper bound, i.e.,
the property (P3), we have ˆU(x|˜α(γ, x)) = U true
γ
(x)."
REFERENCES,0.7215189873417721,"Because ˆU(x|˜α(γ, x)) is the upper bound of the 100γ% PI, it satisﬁes the property (P2), i.e.,
ˆU(x|˜α(γ, x)) −f(x) is independent of x. Then we have"
REFERENCES,0.7257383966244726,"ˆU(x|˜α(γ, x)) −f(x) = M[ε] + ˜α(γ, x)E[(ε −M[ε])1ε−M[ε]>0]"
REFERENCES,0.729957805907173,"is independent of x. Since both M[ε] and E[(ε −M[ε])1ε−M[ε]>0] are independent of x, we have
α(γ) := ˜α(γ, x) is also independent of x, which concludes the proof."
REFERENCES,0.7341772151898734,"B
THE PROOF OF LEMMA 2"
REFERENCES,0.7383966244725738,"Proof. We only need to prove Qupper(α(γ)) = 0, and the same derivation can be applied to
Qlower(β(γ)). Also, for notational simplicity, we assume N is an even number, so that N/2 is
an integer. We ﬁrst deﬁne"
REFERENCES,0.7426160337552743,"g(α) =
X"
REFERENCES,0.7468354430379747,"(xi,yi)∈Dupper
1yi>U(xi|α),
for α ∈[0, ∞),
(14)"
REFERENCES,0.7510548523206751,"based on the deﬁnition of Qupper in Eq. (10). Since N is a ﬁnite integer, g(α) can be written as a
step function, i.e.,"
REFERENCES,0.7552742616033755,"g(α) = N/2
X"
REFERENCES,0.759493670886076,"k=0
k 1Ak(α),
(15)"
REFERENCES,0.7637130801687764,where 1Ak(α) is deﬁned by
REFERENCES,0.7679324894514767,"1Ak(α) =
1, if α ∈Ak,
0, if α ̸∈Ak,
(16)"
REFERENCES,0.7721518987341772,with Ak deﬁned by
REFERENCES,0.7763713080168776,"Ak =
n
α ∈[0, ∞)
 #{yi > U(xi|α) for (xi, yi) ∈Dupper} = k
o
."
REFERENCES,0.7805907172995781,"In other words, Ak is a sub-interval of [0, ∞), such that the number of samples in Dupper satisfying
yi > U(xi|α) is equal to k for α ∈Ak."
REFERENCES,0.7848101265822784,"Since (xi, yi) ̸= (xj, yj) for i ̸= j, the Lebesgue measure of Ak, denoted by λ(Ak) is strictly
positive, i.e., λ(Ak) > 0. From classic measure theory, we know that for any set in R with strictly
positive Lebesgue measure, there are inﬁnite real numbers in the set. As such, there are inﬁnite real
numbers in each Ak for k = 0, . . . , N/2."
REFERENCES,0.7890295358649789,"Hence, for a given γ, any real number α(γ) ∈A⌈N(1−γ)/2⌉will satisfy that g(α(γ)) = ⌈N(1−γ)/2⌉,
i.e., Qupper(α(γ)) = 0, which concludes the proof."
REFERENCES,0.7932489451476793,Published as a conference paper at ICLR 2022
REFERENCES,0.7974683544303798,"C
THE PROOF OF THEOREM 1"
REFERENCES,0.8016877637130801,"Proof. We only need to prove U(x|α(γ)) > U(x|α(γ′)), and the same derivation can be applied to
L(x|β). Since uθ(x) ≥0, we have that U(x|α) is monotonically increasing with α for any x. Thus,
if α(γ) > α(γ′) then U(x|α(γ)) > U(x|α(γ′)). So we only need to prove α(γ) > α(γ′). We use
the proof by contradiction approach to prove α(γ) > α(γ′), i.e., we will derive a contradiction from
the assumption that α(γ) ≤α(γ′)."
REFERENCES,0.8059071729957806,It is easy to see that the function
REFERENCES,0.810126582278481,"g(α) =
X"
REFERENCES,0.8143459915611815,"(xi,yi)∈Dupper
1yi>U(xi|α),
for α ∈[0, ∞),
(17)"
REFERENCES,0.8185654008438819,"is a monotonically decreasing function of α. Thus, if α(γ) ≤α(γ′), we have g(α(γ)) ≥g(α(γ′)).
On the other hand, we know from Lemma 2 that α(γ) and α(γ′) satisfy"
REFERENCES,0.8227848101265823,"g(α(γ)) = ⌈N(1 −γ)/2⌉and g(α(γ′)) = ⌈N(1 −γ′)/2⌉,
(18)"
REFERENCES,0.8270042194092827,"which leads to ⌈N(1 −γ)/2⌉≥⌈N(1 −γ′)/2⌉. This is a contradiction with the condition of the
theorem that ⌈N(1 −γ)/2⌉< ⌈N(1 −γ′)/2⌉. Therefore, the assumption α(γ) ≤α(γ′) is incorrect,
such that we have α(γ) > α(γ′) ⇒U(x|α(γ)) > U(x|α(γ′)), which concludes the proof."
REFERENCES,0.8312236286919831,"D
EXPERIMENT SETUPS AND PARAMETERS"
REFERENCES,0.8354430379746836,"D.1
SETUP FOR THE UCI EXAMPLES AND EXTRA RESULTS"
REFERENCES,0.8396624472573839,"We evaluated the performance of the ﬁve method on 9 widely used UCI data sets, including Boston
housing (boston), Concrete compressive strength (concrete), Energy efﬁciency (energy), KINematics
8 inputs non-linear medium unpredictability/noise (kin8nm), Combined Cycle Power Plant (power),
Physicochemical Properties of Protein Tertiary Structure (Protein), Wine quality (wine), and yacht
Hydrodynamics (yacht). Since data splitting will affect the results for those problems, we pre-split
the each data set with ﬁxed splitting random seed, and generated 3 train/test (90%/10%) data pairs
used for all methods, so as to ensure a fair comparison. Even different methods are built on top of
different ML/DL platform, software versions, we intend to standardize the data pre-processing steps
(e.g. data normalization) to ensure same data is imported to all models."
REFERENCES,0.8438818565400844,"For each method, we test multiple scenarios with different hyper-parameter conﬁgurations, different
training-testing data splits and random seeds. All the methods consider the variation of the hidden
layer size. QD and PIVEN additionally consider the variation of the extra hyper-parameters introduced
in their loss functions."
REFERENCES,0.8481012658227848,"We summarized the modeling results in Table 1 of the main text. In below Table 4 we listed the mean
PI width (MPIW) for the cases having similar PICPs. As discussed, it is meaningful to compare
the PI width only when the methods have the same PICPs. So, for the methods having the similar
PICPs (i.e., the best PICP values highlighted in Table 1), we compared their MPIWs; those methods
giving the smaller MPIW perform better. Table 4 indicates that for the similar PICP values, our
PI3NN methods gave the smallest MPIWs for most of the datasets (7 out of 9). Please note that
PI3NN obtained small MPIWs using the standard MSE loss, while other PI methods such as QD
and PIVEN speciﬁcally minimized MPIWs in their loss function. This customized loss although
sometimes gives better MPIWs after careful hyper-parameter tuning, it also results in unreliable
prediction performance sensitive to the hyper-parameter conﬁguration (as discussed in Table 1)."
REFERENCES,0.8523206751054853,"In the following, we discuss the model setup for each method in detail."
REFERENCES,0.8565400843881856,"We use a single hidden layer ReLU NN for all the methods. For each method, we test multiple scenar-
ios with different hyper-parameter conﬁgurations, different training-testing data splits. Speciﬁcally,
we generate 3 pre-split data pairs for all experiments. The other universally applied hyperparameter
for all methods is the number of hidden neurons: [50, 100, 200]. QD and PIVEN additionally consider
the variation of the extra hyper-parameters introduced in their loss functions. For each scenario, we
use the average of 5 runs (with 5 random seeds) to generate the result."
REFERENCES,0.8607594936708861,"For PI3NN method, we use the hyper-parameters for all experiments: learning rate (0.01), Adam
optimizer with MSE loss for all three networks."
REFERENCES,0.8649789029535865,Published as a conference paper at ICLR 2022
REFERENCES,0.869198312236287,"Boston
Concrete Energy
Kin8nm
Naval
Power
Protein
Wine
Yacht"
REFERENCES,0.8734177215189873,"PI3NN
PICP
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.94
MPIW
0.26
0.31
0.17
0.26
0.15
0.2
0.8
0.47
0.05"
REFERENCES,0.8776371308016878,"QD
PICP
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.95
MPIW
0.26
0.22
0.23
0.44
0.97
0.2
0.8
0.42"
REFERENCES,0.8818565400843882,"PIVEN
PICP
0.94
0.95
0.95
MPIW
0.31
0.28
0.2"
REFERENCES,0.8860759493670886,"SQR
PICP
0.94
0.95
0.95
0.95
0.95
0.95
0.95
0.94
MPIW
0.54
0.64
0.64
0.99
0.55
0.83
0.52
0.66"
REFERENCES,0.890295358649789,"DER
PICP
0.94
MPIW
0.27"
REFERENCES,0.8945147679324894,"Table 4: Evaluation of 95% PI on testing data. We compare the mean PI width (MPIW) between methods
when they have the similar PICP values (i.e., the best PICP highlighted in Table 1). For the similar PICP, the
method producing the smaller MPIW performs better. Our PI3NN shows the top performance by giving the
smallest MPIW for 7 out of 9 datasets. QD and PIVEN can also produce small MPIW for some datasets, but
they obtained the small MPIW by speciﬁcally minimizing the MPIW in their customized loss function which
resulted in a prediction performance sensitive to the hyper-parameter conﬁguration."
REFERENCES,0.8987341772151899,"For QD method, we include additional two hyper-parameter combinations: soften parameter: [100.,
160., 200.] and lambda_in parameter: [5., 15., 25.] which are essential parameters for QD method.
In addition, we keep the default parameters in the original QD code for boston and concrete data
sets, and our tuned parameters for rest of 7 data. Speciﬁcally, the maximum epochs for all data is
300 except for concrete (800); similarly, the learning rate for concrete is 0.03 and 0.02 for other data
sets; the decay rate and sigma_in are set to 0.9 and 0.1 respectively for 7 data sets except for concrete
(0.98 and 0.2). The outer layer bias are set to [3., -3]. Adam optimizer is used in the experiments, and
we use 100 batch size from the original code."
REFERENCES,0.9029535864978903,"For DER method, the maximum epochs is set to 40. We follow the author tuned learning rate and
batch size provided in the original code. The learning rate and batch size are (1e-3, 8) for boston,
(5e-3, 1) for concrete, (2e-3, 1) for energy, (1e-3, 1) for kin8nm, (1e-3, 2) for power, (5e-4, 1) for
naval, (1e-4, 32) for wine, (1e-3, 64) for protein, and (5e-4, 1) for yacht."
REFERENCES,0.9071729957805907,"For PIVEN method, similar to QD, we include the lambda_in ([5.0, 15.0, 20.0]) and sigma_in ([0.05,
0.2, 0.4]) for the hyper-parameter combinations. The rest of the parameters are provided by the
original authors in the code, including: batch size (100), outer layer biases [3., -3.], soften parameter
(160.0). Meanwhile, we use the author tuned maximum epochs, learning rate, decay rate for boston
(300, 0.02, 0.9), concrete (800, 0.03, 0.98), energy (1200, 0.016, 0.97), kin8nm (800, 0.0134725,
0.993922), naval (1000, 0.006, 0.998), power (500, 0.014075, 0.987099), protein (600, 0.002, 0.999),
wine (1000, 0.01, 0.98) and yacht (2000, 0.005, 0.98)."
REFERENCES,0.9113924050632911,"For SQR method, we added three hyper-parameter combinations based on the original paper and
code, which are the learning rate [1e-2, 1e-3, 1e-4], dropout rate [0.1, 0.25, 0.5, 0.75] and weight
decay [0, 1e-3, 1e-2, 1e-1, 1]. maximum epochs is 5,000 for all data sets by default. Additional
20% validation data is split from the training data. These parameter combinations with the split seed,
random seed, number of neurons yield much more resutls than other methods, thus, we reduced the
results data size by ﬁxing the learning rate (1e-2), dropout rate (0.1) and weight decay (0.1) for the
ﬁnal evaluations."
REFERENCES,0.9156118143459916,"D.2
SETUP FOR THE 10D EXAMPLE"
REFERENCES,0.919831223628692,"The 10-dimensional cubic function is used for generating the synthetic data for demonstrating OOD
identiﬁcation capability. As mentioned in Section 4.2, we generate a training data with 10 input
features and size of 5,000, and 1,000 OOD testing data."
REFERENCES,0.9240506329113924,"For PI3NN method, we use three networks with single layer (100 hidden nodes) with ReLU activation,
the MSE loss is used with SGD optimizer (learning rate = 0.01). Maximum epochs is 3,000. We
use comparable setups for the baseline methods. Speciﬁcally, we take the hyperparameter values
(suggested by the authors of those methods) and perform hyperparameter tuning around those"
REFERENCES,0.9282700421940928,Published as a conference paper at ICLR 2022
REFERENCES,0.9324894514767933,"suggested values, and choose the best performance for each baseline method. The hyperparameters
we tune include the hidden layer width, the learning rate and the extra hyperparameters introduced
into the baseline method. For hidden layers, we use 100,200,300 hidden neurons as candidates. For
learning rates, we use 0.0001, 0.001, 0.01 as candidates. For the exclusive hyperparameters, we
choose ﬁve candidate values (including the suggested ones). For each hyperparameter conﬁguration,
we run an ensemble of 5 runs to get the results. After tuning, we have the following setup for the
baseline methods. For QD method, we use single hidden layer network (200 hidden nodes) with
Adam optimizer (lr=0.02). Activation function is ReLU, and batch size is 100. The soften parameter,
labmda_in, and sigma_in are set to 160.0, 15.0 and 0.4, respectively. For PIVEN method, sigma_in
(0.2), learning rate (0.01). It shares the same network structure, softer parameter and lambda_in with
QD method. For DER method, we use single hidden layer with 200 nodes network, 1e-4 learning
rate, 256 batch size. For SQR method, we use single layer network with 100 neurons network. We ﬁx
the learning rate, dropout rate, weight decay rate to 1e-3, 0.1 and 0, respectively. We also use the
default setting by taking 20% of the training data as validation set in this method."
REFERENCES,0.9367088607594937,"D.3
SETUP FOR THE FLIGHT DELAY EXAMPLE"
REFERENCES,0.9409282700421941,"The ﬂight delay data (www.kaggle.com/vikalpdongre/us-flights-data-2008) con-
tains the ﬂight information in the U.S. in the year 2008. We separate the data by airports into 4 ranks
based on the percentage of departure ﬂights, which including: Large Hub (Rank 1), receives 1%
or more of the annual departures; Medium Hub (Rank 2), which receives 0.25%-1% of the annual
departures; Small Hub (Rank 3) and Nonhub (rank4) receives 0.05%-0.25% and <0.05% of the
departures, respectively. We select 5% of the Rank 4 data as the training data, another 5% of the Rank
4 is picked as ’test 4’ data, and we consider this as in-distribution data. Rest of three testing data
(’test 3’, ’test 2’, and ’test 1’) are selected from the Rank 3, 2 and 1, respectively with the 5% total
amount of data. We pick the 6 input features (’DayofMonth’, ’DayOfWeek’, ’AirTime’, ’Distance’,
’DepDelay’, ’TaxiOut’), and select ’ArrDelay’ as the output feature."
REFERENCES,0.9451476793248945,"For PI3NN method, in the experiment, we use the same network architecture (i.e., one hidden layer
ReLU network containing 100 hidden neurons) and as the one used for the UCI datasets. The Adam
optimizer is used with learning rate being 0.01 and the maximum epochs being 50,000. Conventional
L1 and L2 regularization are implemented with both penalties set to 0.02. The scalar parameter (i.e.
the one controlling the initial bias of the output layer) is set to 10 (the same as the test on the 10D
function)."
REFERENCES,0.9493670886075949,"We also perform hyperparameter tuning for each of the baseline methods. The universal hyperpa-
rameter, i.e., the number of hidden neurons come from the pool [50,100,200]. For the exclusive
hyperparameters, we choose ﬁve candidate values (including the suggested ones). For each hyper-
parameter conﬁguration, we run an ensemble of 5 runs to get the results. After tuning, we have the
following setup for the baseline methods."
REFERENCES,0.9535864978902954,"For QD method, we use single layer network with 100 neurons, Adam optimizer with 0.02 learning
rate, 50 epochs, 160.0 soften parameter, 0.1 sigma_in, 15.0 lambda_in, 100 batch size, 0.9 decay rate."
REFERENCES,0.9578059071729957,"For DER method, we use single hidden layer with 100 neurons network structure, 512 batch size,
1e-4 learning rate and 100 maximum epochs for the experiments."
REFERENCES,0.9620253164556962,"For PIVEN method, lambda_in and sigma_in are set to 15.0 and 0.2. Same single layer 100 nodes
network structure is applied. Soften parameter is 160.0. Batch size and maximum epochs are 100 and
500, respectively. We use 0.01 learning rate and 0.99 decay rate"
REFERENCES,0.9662447257383966,"For SQR method, we use same network structure [100 neurons] and maximum epochs (500) with
PIVEN method. We ﬁxed the learning rate (1e-3), dropout rate (0.1), and weight decay rate (0) in the
ﬂight delay experiments."
REFERENCES,0.9704641350210971,"D.4
OOD IDENTIFICATION IN A WATERSHED STREAMFLOW DATASET"
REFERENCES,0.9746835443037974,"We test the performance of our method in OOD detection using the streamﬂow dataset measured
in East River Watershed, Colorado, United States. The dataset contains three years of daily data of
precipitation, maximum temperature, minimum temperature, and streamﬂow at the catchment outlet.
We use the Long Short-Term Memory (LSTM) network to learn the relationship between the three"
REFERENCES,0.9789029535864979,Published as a conference paper at ICLR 2022
REFERENCES,0.9831223628691983,"meteorological forcing (i.e., precipitation, maximum temperature and minimum temperature) and the
streamﬂow. The training data are from the ﬁrst two years (2015-2016) and the third year (2017) of
data form the testing set. Note that 2017 is a wet year with a relatively large amount of precipitation,
which causes the streamﬂow patterns in 2017 dramatically different from those in 2015-2016."
REFERENCES,0.9873417721518988,"In this experiment, the LSTM network consists of two sub-networks, i.e., the recurrent layers and the
dense layers. The recurrent layers extract the temporal feature information and the dense layers learn
the rainfall-runoff relationship. The prediction uncertainty quantiﬁcation focuses on the dense layer
training. In implementation, we ﬁrst train the entire LSTM network to get the mean and the median
prediction following Step 1 and 2 in Section 3.2. Next, we keep the recurrent layers unchanged and
extract their outputs for all the LSTM input training samples, and use these output samples as inputs
together with the streamﬂow data to train the dense layers and apply the UQ method to quantify the
prediction uncertainty. For PI3NN, we use these samples to train uθ and lξ as mappings from the
feature space (i.e., the input of uθ and lξ is the input of the dense layers of the LSTM network) to
the output of the LSTM network. For other UQ methods such as QD, PIVEN, SQR, and DER, we
use these samples to train the dense layers directly to obtain the uncertainty estimate. In this way,
we make a fair comparison between PI3NN and the baselines where they all use the same calibrated
recurrent layers and focus on the training of the dense layers."
REFERENCES,0.9915611814345991,"Figure 3 shows the predicted value and its 95% PI. PI3NN can accurately identify the OOD data (i.e.,
the extreme event in 2017) by giving their predictions a large PI, while other methods fail to identify
the OOD patterns by giving the testing data similarly narrow PIs as the training data. This example
demonstrates that our PI3NN method can show a consistent superior performance for a different
dataset (time-series data) and a different network architecture (LSTM network)."
REFERENCES,0.9957805907172996,"Figure 3: Streamﬂow prediction using LSTM network and the calculated 95% PIs from our PI3NN method and
the baselines. The testing period (2017) is a wet year (extreme event) having dramatically different streamﬂow
patterns than the training period. PI3NN identiﬁes this OOD patterns by giving the prediction a large PI while
other methods fail to identify the OOD patterns by giving the testing data similarly narrow PIs as the training."
