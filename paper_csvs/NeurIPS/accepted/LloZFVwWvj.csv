Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0030120481927710845,"Non-autoregressive Transformer(NAT) significantly accelerates the inference of
neural machine translation. However, conventional NAT models suffer from limited
expression power and performance degradation compared to autoregressive (AT)
models due to the assumption of conditional independence among target tokens.
To address these limitations, we propose a novel approach called PCFG-NAT,
which leverages a specially designed Probabilistic Context-Free Grammar (PCFG)
to enhance the ability of NAT models to capture complex dependencies among
output tokens. Experimental results on major machine translation benchmarks
demonstrate that PCFG-NAT further narrows the gap in translation quality between
NAT and AT models. Moreover, PCFG-NAT facilitates a deeper understanding of
the generated sentences, addressing the lack of satisfactory explainability in neural
machine translation.2"
INTRODUCTION,0.006024096385542169,"1
Introduction"
INTRODUCTION,0.009036144578313253,"Autoregressive machine translation models rely on sequential generation, leading to slow generation
speeds and potential errors due to the cascading nature of the process. In contrast, non-autoregressive
Transformer (NAT) models [13] have emerged as promising alternatives for machine translation. The
Vanilla NAT model assumes that the target tokens are independent of each other given the source
sentence. This assumption enables the parallel generation of all tokens. However, it neglects the
contextual dependencies among target language words, resulting in a severe multi-modality problem
[13] caused by the rich expressiveness and diversity inherent in natural languages. Consequently,
Vanilla NAT models exhibit significant performance degradation compared to their autoregressive
counterparts."
INTRODUCTION,0.012048192771084338,"Several approaches have been proposed to alleviate the multi-modality problem in NAT models
while maintaining their computational efficiency. CTC-based NAT [11, 22] depicted in Figure 1(b)
addresses this issue by implicitly modeling linear adjacent dependencies through token repetition
and blank probability. This approach helps alleviate the multi-modality of token distributions at
each position compared to Vanilla NAT. Similarly, DA-Transformer [14, 37, 26] shown in Figure
1(c) introduces a hidden Markov model and employs a directed acyclic graph to model sentence
probability, enabling the model to capture dependencies between adjacent tokens and reduce the
multi-modal nature of the distribution."
INTRODUCTION,0.015060240963855422,"∗Corresponding author: Yang Feng
2Code is publicly available at https://github.com/ictnlp/PCFG-NAT."
INTRODUCTION,0.018072289156626505,"(a) Vanilla NAT
(b) CTC-NAT"
INTRODUCTION,0.02108433734939759,"(c) DA-Transformer
(d) PCFG-NAT"
INTRODUCTION,0.024096385542168676,"Figure 1: Illustration of Vanilla NAT, CTC-NAT, DA-Transformer, and PCFG-NAT"
INTRODUCTION,0.02710843373493976,"However, despite these enhancements, the modeling of only adjacent and unidirectional dependencies
limits the model’s ability to capture the rich semantic structures with non-adjacent and bidirectional
dependencies present in natural language. For example, considering the token need as the previous
token, the distribution of the next token is still affected by the issue of multi-modality, as the word
boat can be modified by various rich prefixes in the corpus."
INTRODUCTION,0.030120481927710843,"To overcome this constraints, we propose PCFG-NAT, a method that leverages context-free grammar
to endow the model with the capability to capture the intricate syntactic and semantic structures
present in natural language. For instance, as illustrated in Figure 1(d), PCFG-NAT can directly model
the robust and deterministic correlation between non-adjacent tokens, such as You need boat. In the
parse tree generated by PCFG-NAT for the target sentence, the nodes connected to the tokens You
need boat constitute the main chain of the tree, capturing the left-to-right semantic dependencies
through linear connections. Additionally, PCFG-NAT can learn relationships between modifiers, such
as are gonna and a bigger, and the words they modify by constructing a local prefix tree for the nodes
on the main chain. This allows for the modeling of right-to-left dependencies between the modified
words and their modifiers."
INTRODUCTION,0.03313253012048193,"We conduct experiments on major WMT benchmarks for NAT (WMT14 En↔De, WMT17 Zh↔En,
WMT16 En↔Ro), which shows that our method substantially improves the translation performance
and achieves comparable performance to autoregressive Transformer [40] with only one-iteration par-
allel decoding. Moreover, PCFG-NAT allows for the generation of sentences in a more interpretable
manner, thus bridging the gap between performance and explainability in neural machine translation."
BACKGROUND,0.03614457831325301,"2
Background"
BACKGROUND,0.0391566265060241,"In this section, we will present the necessary background information to facilitate readers’ com-
prehension of the motivation and theoretical underpinnings of PCFG-NAT. In Section 2.1, we will
provide a formalization of the machine translation task and introduce the concepts of autoregressive
Transformer [40] and Vanilla NAT [13], which will serve as the basis for our proposed approach.
Subsequently, in Section 2.2, we will provide a precise and rigorous definition of PCFG, accompanied
by illustrative examples aimed at enhancing clarity and comprehension."
NON-AUTOREGRESSIVE TRANSLATION,0.04216867469879518,"2.1
Non-autoregressive Translation"
NON-AUTOREGRESSIVE TRANSLATION,0.045180722891566265,"Machine translation can be formally defined as a sequence-to-sequence generation problem. Given
source sentence X = {x1, x2, ..., xS}, the translation model is required to generate target sentence
Y = {y1, y2, ..., yT }. The autoregressive translation model predicts each token on the condition of
all previous tokens:"
NON-AUTOREGRESSIVE TRANSLATION,0.04819277108433735,"P(Y |X) = T
Y"
NON-AUTOREGRESSIVE TRANSLATION,0.05120481927710843,"t=1
p(yt|y<t, X).
(1) 0 3 1
5 0
4 7 9
13 8 0"
NON-AUTOREGRESSIVE TRANSLATION,0.05421686746987952,"<blank>
You
<blank>
are
gonna
need
<blank>
a
bigger
boat
.
<blank>"
NON-AUTOREGRESSIVE TRANSLATION,0.0572289156626506,"Figure 2: An example parse tree for You are
gonna need a bigger boat."
NON-AUTOREGRESSIVE TRANSLATION,0.060240963855421686,"Applied Rules
Sequence"
NON-AUTOREGRESSIVE TRANSLATION,0.06325301204819277,"Start
V1
V1 →V0You V5,V0 →ϵ
You V5
V5 →V3 need V9
You V3 need V9
V3 →V0are V4,V0 →ϵ
You are V4 need V9
V4 →gonna
You are gonna need V9
V9 →V7 boat V10
You are gonna need V7 boat V10
V7 →V0a V8,V0 →ϵ
You are gonna need a V8 boat V10
V8 →bigger
You are gonna need a bigger boat V10
V10 →.
You are gonna need a bigger boat ."
NON-AUTOREGRESSIVE TRANSLATION,0.06626506024096386,"Table 1: Derivation of the parse tree. Used produc-
tion rules for every step are listed, where ϵ stands
for an empty string."
NON-AUTOREGRESSIVE TRANSLATION,0.06927710843373494,"The autoregressive inference is time-consuming as it requires generating target tokens from left to
right. To reduce the decoding latency, Gu et al. [13] proposes non-autoregressive Translation, which
discards dependency among target tokens and predicts all the tokens independently:"
NON-AUTOREGRESSIVE TRANSLATION,0.07228915662650602,"P(Y |X) = T
Y"
NON-AUTOREGRESSIVE TRANSLATION,0.07530120481927711,"t=1
p(yt|X).
(2)"
NON-AUTOREGRESSIVE TRANSLATION,0.0783132530120482,"Since the prediction does not rely on previous translation history, the decoder inputs of NAT are
purely positional embeddings or the copy of source embeddings. Vanilla NAT trains a length predictor
to determine the sentence length when decoding. During the training, the golden target length is used.
During the inference, the predictor predicts the target length and the translation of the given length is
obtained by argmax decoding."
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.08132530120481928,"2.2
Probabilistic Context-free Grammar"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.08433734939759036,"Probabilistic Context-free Grammar is built upon Context-Free Grammar (CFG). A CFG is defined
as a 4-tuple:
G = (N, T , R, S),
where N is the set of non-terminal symbols, T is the set of terminal symbols, R is the set of
production rules and S is the start symbol. PCFG extends CFG by associating each production rule
r ∈R with a probability P(r). Production rules are in the form:"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.08734939759036145,"A →α,
(3)"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.09036144578313253,"where A must be a single non-terminal symbol, and α is a string of terminals and/or nonterminals,
α can also be empty. Starting with the sequence that only has start symbol S, we can replace any
non-terminal symbol A with α based on production rule A →α. The derivation process repeats until
the sequence only contains terminal symbols and the derived hierarchical structure can be seen as a
Parse Tree. Those production rules and their probabilities can be applied regardless of the contexts,
which provides Markov property for the joint probability of the parse tree. For every rule, A →α
applied in a derivation, all of the symbols in α are the children nodes of A in order. The probability
of a parse tree t is formulated as:"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.09337349397590361,"P(t) =
Y"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.0963855421686747,"r∈R(t)
(P(r))nr,
(4)"
PROBABILISTIC CONTEXT-FREE GRAMMAR,0.09939759036144578,"where R(t) is the set of production rules applied in t and nr is the number of times that r is applied.
For sentence You are gonna need a bigger boat., Figure 2 shows an example parse tree and how the
sentence is derived from the start symbol V1."
THE PROPOSED METHOD,0.10240963855421686,"3
The Proposed Method"
THE PROPOSED METHOD,0.10542168674698796,"In the following sections, we present the formal definition of a novel variant of PCFG designed for
PCFG-NAT in Section 3.1 and the architecture of the PCFG-NAT model in Section 3.2. Finally, we
describe the training algorithm and decoding algorithm for PCFG-NAT in Section 3.3 and Section
3.4."
RIGHT HEAVY PCFG,0.10843373493975904,"3.1
Right Heavy PCFG"
RIGHT HEAVY PCFG,0.11144578313253012,"Despite the strong expressive capabilities of PCFGs, the parameter induction process becomes
challenging due to the vast latent variable space encompassing the possible parse tree structures of
a single sentence. Moreover, training PCFGs incurs significant time complexity [35]. To address
these challenges, we propose a novel variant of PCFG called Right-Heavy PCFG (RH-PCFG). RH-
PCFG strikes a balance between expressive capabilities and computational complexity, mitigating the
difficulties associated with parameter induction and reducing training time."
DERIVED PARSE TREE OF RH-PCFG,0.1144578313253012,"3.1.1
Derived Parse Tree of RH-PCFG"
DERIVED PARSE TREE OF RH-PCFG,0.11746987951807229,"RH-PCFG enforces a distinct right-heavy binary tree structure in the generated parse tree. Specifically,
for each non-terminal symbol in the tree, the height of the right subtree increases proportionally with
the length of the derived sub-string of the non-terminal symbol, while the height of the left subtree
remains constant. This characteristic significantly reduces the number of possible parse trees for a
given sentence, thereby reducing training complexity."
DERIVED PARSE TREE OF RH-PCFG,0.12048192771084337,"In the context of a right-heavy binary tree, the traversal process begins with the root node and
proceeds recursively by selecting the right child of the current node as the subsequent node to visit.
This sequential path, known as the Main Chain, is guided by a design principle inspired by linguistic
observations in natural language. Specifically, it reflects the tendency for sentence structures to
display left-to-right connectivity, with local complements predominantly positioned to the left of the
central core structure. Within this framework, the left subtree of each node along the main chain is
referred to as its Local Prefix Tree. The parse tree in Figure 2 represents a right heavy tree. In this
tree, V1, V5, V9, V13 form the main chain, and their left subtrees correspond to their respective local
prefix trees."
SUPPORT TREE OF RH-PCFG,0.12349397590361445,"3.1.2
Support Tree of RH-PCFG"
SUPPORT TREE OF RH-PCFG,0.12650602409638553,"To make sure that all the parse trees are the right heavy tree, we first build up a Support Tree as the
backbone for the parse trees derived from RH-PCFG. The construction of the support tree follows
two steps: At first, given a source sentence with a length of Lx and an upsample ratio of λ, we
sequentially connect λLx + 1 nodes with right links, which become the candidates for the main chain
nodes in parse trees. Next, excluding the root node, we append a complete binary tree of depth l
as the left subtree to each node. For the root node, only one left child is added. The added nodes
represent local prefix tree candidates to the attached main chain candidates. Consequently, for a
source sentence of length Lx, the number of non-terminal symbols m of the RH-PCFG is"
SUPPORT TREE OF RH-PCFG,0.12951807228915663,"m = λLx · 2l + 2.
(5)"
SUPPORT TREE OF RH-PCFG,0.13253012048192772,"The nodes within the support tree are assigned sequential indexes through an in-order traversal, and
these indexes correspond to the corresponding non-terminal symbols sharing the same index. Detailed
construction algorithm can be found in Appendix A.1. Figure 3 shows an example of PCFG parse
tree with Lx = 3, λ = 1, l = 2."
SUPPORT TREE OF RH-PCFG,0.1355421686746988,"1
5
9
13"
SUPPORT TREE OF RH-PCFG,0.13855421686746988,"0
3
7
11"
SUPPORT TREE OF RH-PCFG,0.14156626506024098,"2
6
10
4
8
12"
SUPPORT TREE OF RH-PCFG,0.14457831325301204,"Main Chain 
Candidates"
SUPPORT TREE OF RH-PCFG,0.14759036144578314,"Local Prefix 
Tree Candidates"
SUPPORT TREE OF RH-PCFG,0.15060240963855423,Figure 3: An example support tree of Right Heavy PCFG
SUPPORT TREE OF RH-PCFG,0.1536144578313253,"In RH-PCFG, the set of non-terminals is N = {V0, ..., Vm−1} and the set of terminals T is equivalent
to the vocabulary. The collection of production rules is represented by R, and the starting symbol is
V1. The production rules in RH-PCFG have the following form:
V0 →ϵ,
(6)
Vi →a, a ∈T , i ̸= 0, IsLeaf(i)
(7)
Vi →VjaVk, Vi, Vj, Vk ∈N, a ∈T , (InLeftReach(j, i) ∨j = 0) ∧InRightReach(k, i)
(8)"
SUPPORT TREE OF RH-PCFG,0.1566265060240964,"ϵ stands for an empty string. IsLeaf(i) indicates whether the non-terminal symbol Vi corresponds
to a leaf node in the support tree. InLeftReach(j, i) denotes that, in the support tree, the node
corresponding to Vj is in the left subtree of the node corresponding to Vi. InRightReach(k, i)
signifies that the node corresponding to Vk is in the right subtree of the node corresponding to Vi in
the support tree, and if Vi corresponds to a main chain node, Vk has to correspond to a main chain
node at the same time."
ARCHITECTURE,0.15963855421686746,"3.2
Architecture"
ARCHITECTURE,0.16265060240963855,"The main architecture of PCFG-NAT is identical to that of the Transformer [40], with the additional
incorporation of structures to model the probabilities of the PCFG. The input sentence to be translated
is fed into the encoder of the model. Additionally, based on the length Lx of the source sentence,
we compute the number of corresponding non-terminal symbols m in the RH-PCFG. Then m
positional embeddings are used as inputs to the decoder. Through multiple layers of self-attention
modules with a global attention view, cross-attention modules that focus on distant information, and
subsequent feed-forward transformations, we obtain m hidden states h0, ..., hm−1. These hidden
states are associated with non-terminal symbols of the same index and are utilized to calculate the
rule probabilities related to each corresponding non-terminal symbol. 8 9 10 11 12 13 14 15
0 1 2 3 4 5 6 7 16 17"
ARCHITECTURE,0.16566265060240964,"bigger
ship
one
a
bigger
boat
.
.
<blank>
You
will
are
gonna
need
one 
a
.
."
ARCHITECTURE,0.1686746987951807,"8
9
10
11
12
13
14
15
0
1
2
3
4
5
6
7
16
17"
ARCHITECTURE,0.1716867469879518,"Linear + Softmax
Output Layer"
ARCHITECTURE,0.1746987951807229,"Right 
Heavy PCFG"
ARCHITECTURE,0.17771084337349397,"Attention From 
Encoder Output"
ARCHITECTURE,0.18072289156626506,Positional
ARCHITECTURE,0.18373493975903615,Embedding
ARCHITECTURE,0.18674698795180722,Decoder
ARCHITECTURE,0.1897590361445783,Output
ARCHITECTURE,0.1927710843373494,Stacked Decoder Layers
ARCHITECTURE,0.19578313253012047,"8
0
1
2
3
4
5
6
7"
ARCHITECTURE,0.19879518072289157,"。
你
需要
⼀
艘
更
⼤
的
船"
ARCHITECTURE,0.20180722891566266,Stacked Encoder Layers
ARCHITECTURE,0.20481927710843373,"Figure 4: The architecture of PCFG-NAT model. A PCFG is inserted between the decoder layers and
the output layer, which captures the dependency between different tokens. A parse tree is derived to
generate the sentence You are gonna need a bigger boat. and is highlighted with bold black lines."
ARCHITECTURE,0.20783132530120482,"The probability of the rule P(Vi →a) can be regarded as the word prediction probability at each
position, and we can directly calculate it with the softmax operation. We use hi to denote the hidden
state representation of the non-terminal symbol Vi and use Wo to denote the weight matrix of the
output layer. Formally, we have:"
ARCHITECTURE,0.21084337349397592,"P(Vi →a) = Softmax(Wohi).
(9)"
ARCHITECTURE,0.21385542168674698,"For the rule P(Vi →VjaVk), we can independently consider the prediction probability of word a
and the transition probability between the three non-terminal symbols. Formally, we decompose it
into two parts:
P(Vi →VjaVk) = P(<Vj, Vk > |Vi) · P(a|Vi).
(10)"
ARCHITECTURE,0.21686746987951808,"The prediction probability P(a|Vi) can be calculated with the softmax operation like in Equation 9.
To normalize the distribution of P(<Vj, Vk > |Vi), we have to enumerate all valid combinations of
j, k. Let Child(i) = {< o, u > |Vi →VoaVu ∈R} denotes the set of children combinations of the
non-terminal symbol Vi, we can formally define the transition probability as:"
ARCHITECTURE,0.21987951807228914,"qi = Wqhi,
qj = Wlhj,
qk = Wrhk,
(11)"
ARCHITECTURE,0.22289156626506024,"P(<Vj, Vk > |Vi) =
exp(qT
i qj + qT
i qk + qT
j qk)
P"
ARCHITECTURE,0.22590361445783133,"<o,u>∈Child(i) exp(qT
i qo + qT
i qu + qTo qu),
(12)"
ARCHITECTURE,0.2289156626506024,"where Wq, Wl, Wr are learnable model parameters."
TRAINING,0.2319277108433735,"3.3
Training"
CYK ALGORITHM FOR RH-PCFG,0.23493975903614459,"3.3.1
CYK Algorithm for RH-PCFG"
CYK ALGORITHM FOR RH-PCFG,0.23795180722891565,"We train PCFG-NAT with the maximum likelihood estimation, which sums up all the possible parse
trees for the given sentence Y conditioned on source sentence X and model parameters θ:"
CYK ALGORITHM FOR RH-PCFG,0.24096385542168675,"L(θ) = −log P(Y |X, θ) = −log P(V1 ⇒y0y1...yn−1).
(13)"
CYK ALGORITHM FOR RH-PCFG,0.24397590361445784,"In PCFG, the CYK algorithm [35] is employed to compute the probability of a given sentence derived
from the start symbol. The CYK algorithm follows a bottom-up dynamic programming approach to
fill in the table S. Each element Sa
i,j in the table represents the sum of probabilities of that Va derives
the consecutive sub-string yi...yj of the target sentence, denoted as P(Va ⇒yi...yj). Therefore the
size of table S is O(mn2). Based on the definition of Sa
i,j, we have recursion formula"
CYK ALGORITHM FOR RH-PCFG,0.2469879518072289,"Sa
i,j = j−1
X k=i+1 X"
CYK ALGORITHM FOR RH-PCFG,0.25,"<b,c>∈Child(a)
P(Va →VbykVc)Sb
i,k−1Sc
k+1,j
(14)"
CYK ALGORITHM FOR RH-PCFG,0.25301204819277107,"For every non-terminal symbol Va, the complexity filling all the Sa
i,j is the product of the choice of
span < i, j >, splitting point k, and children symbols < b, c >."
CYK ALGORITHM FOR RH-PCFG,0.2560240963855422,"Local Prefix Tree For the non-terminal symbol Va in a local prefix tree, the rules specify that all
values of b and c are strictly fewer than the size of the left attached tree in the support structure,
denoted as 2l −1 < d = 2l. Similarly, the variables i, j, and k are also strictly limited to values
fewer than d, since it is the max length of sub-strings that Va can derives. Please note that l represents
the max depth of the complete binary tree added as the left subtree to each node. Therefore the
complexity filling all the Sa
i,j is O(d5) for Va and all Va in Local Prefix Tree should be O(d5m)
where m is the number of non-terminal symbols."
CYK ALGORITHM FOR RH-PCFG,0.25903614457831325,"Main Chain For the non-terminal symbol Va that serves as a main chain, based on the rules of the
model, the variable b has strictly fewer than d choices, while c has a maximum of m/d choices. In a
right-heavy parse tree, the symbols in the main chain are responsible for deriving a complete suffix
of the target sentence as they are the right offspring of the root. Consequently, j is always equal to
n −1, and variable i has n choices. However, k has only d choices since the the length of derived
sub-string of Vb is less than d. Considering the filling of all Sa
i,j entries for the main chain symbols,
the time complexity is O(mdn). Summing up the complexities for all m/d main chain symbols, the
total complexity is O(m2n)."
CYK ALGORITHM FOR RH-PCFG,0.2620481927710843,"Based on the above analysis, the overall time complexity of the CYK algorithm for RH-PCFG is
O(d5n + m2n).Such complexity is within the acceptable range, while the training complexity of
general form PCFG, such as Chomsky normal form [5], is O(n3|R|). Detailed pseudocode for the
training algorithm can be found in the Appendix A.2."
GLANCING TRAINING,0.26506024096385544,"3.3.2
Glancing Training"
GLANCING TRAINING,0.2680722891566265,"Glancing training, as introduced in the work of Qian et al. [30], has been successfully applied in
various NAT architectures [12, 14]. In the case of PCFG-NAT, we also incorporate glancing training.
This training approach involves replacing a portion of the decoder inputs with the embeddings of the
target sentence Y . To determine the ratio of decoder inputs to be replaced with target embeddings, we
employ the masking strategy proposed by Qian et al. [30]. There is a mismatch between the decoder
length m and target length n, so we have to derive an alignment between them to enable the glancing.
To incorporate Glancing Training into PCFG-NAT, we adapt the approach by finding the parse tree
T ∗
Y = arg maxTY ∈Γ(Y ) P(Y, TY |X) with the highest probability that generates the target sentence
Y , where Γ(Y ) stands for all parse trees of Y . T ∗
Y can be traced by repalcing the sum opearation
with max in the CYK algorithm. We provide the pseudocode in the Appendix A.3."
INFERENCE,0.2710843373493976,"3.4
Inference"
INFERENCE,0.2740963855421687,"We propose a Viterbi [41] decoding framework for PCFG-NAT to find the optimal parse tree T ∗=
arg maxT P(T|X, L) under the constraint of target length L. For each non-terminal symbol Va, we"
INFERENCE,0.27710843373493976,"maintain a record M a
L that represents the highest probability among sub-strings of length L that can
be derived by Va. Similar to the CYK algorithm, we can compute M a
L in a bottom-up manner using
dynamic programming, considering all non-terminal symbols Va and lengths L."
INFERENCE,0.28012048192771083,"M a
L =
max
<b,c>∈Child(a),o∈T ,k=1..L−1 P(Va →VboVc)M b
L−1−kM c
k
(15)"
INFERENCE,0.28313253012048195,"To recover the parse tree with the maximum probability given L, we need to keep track of the values
of b, c, and k that lead to M a
L and start the construction from M 1
L. The detailed pseudocode is
provided in the Appendix A.4. According to Shao et al. [37], we rerank the output sentences with
probability and length factors and then output the sentence with the highest score."
EXPERIMENTS,0.286144578313253,"4
Experiments"
SETUP,0.2891566265060241,"4.1
Setup"
SETUP,0.2921686746987952,"Dataset
We conduct our experiments on WMT14 English↔German (En-De, 4.5M sentence
pairs),WMT17 Chinese↔English (Zh-En, 20M sentence pairs) and WMT16 English↔Romanian
(En-Ro, 610k sentence pairs). We use the same preprocessed data and train/dev/test splits as Kasai
et al. [18]. For all the datasets, we learn a joint BPE model [36] with 32K merge operations to
tokenize the data and share the vocabulary for source and target languages. The translation quality is
evaluated with sacreBLEU [29] for WMT17 En-Zh and tokenized BLEU [28] for other benchmarks."
SETUP,0.29518072289156627,"Models
Iter
WMT14
WMT17
WMT16
Average Gap
Speedup
En-De
De-En
Eh-Zh
Zh-En
En-Ro
Ro-En
Transformer [40]
N
27.66
31.59
34.89
23.89
34.26
33.87
0
1.0×
CMLM [8]
10
24.61
29.40
-
-
32.86
32.87
-
2.2×
SMART [10]
10
25.10
29.58
-
-
32.71
32.86
-
2.2×
DisCo [17]
≈4
25.64
-
-
-
-
32.25
-
3.5×
Imputer [34]
8
25.0
-
-
-
-
-
-
2.7×
CMLMC [15]
10
26.40
30.92
-
-
34.14
34.13
-
1.7×
Vanilla NAT [13]
1
11.79
16.27
18.92
8.69
19.93
24.71
14.31
15.3×
CTC [21]
1
17.68
19.80
26.84
12.23
-
-
-
14.6×
FlowSeq[25]
1
18.55
23.36
-
-
29.26
30.16
-
1.1×
AXE [9]
1
20.40
24.90
-
-
30.47
31.42
-
14.2×
OAXE [6]
1
22.4
26.8
-
-
-
-
-
14.2×
CTC + GLAT [30]
1
25.02
29.14
30.65
19.92
-
-
-
14.6×
DA-Transformer + Lookahead [14]
1
26.55
30.81
33.54
22.68
32.31*
32.73*
1.27
14.0×
DA-Transformer + Joint-Viterbi [37]
1
26.89
31.10
33.65
23.24
32.46*
32.84*
1.00
13.2×
FA-DAT [26]
1
27.47
31.44
34.49
24.22
-
-
0.41
13.2×
PCFG-NAT
1
27.02
31.29
33.60
23.40
32.72
33.07
0.84
12.6×"
SETUP,0.29819277108433734,"Table 2: Results on raw WMT14 En-De, WMT17 Zh-En and WMT16 En-Ro. ‘Iter’ means the
number of decoding iterations, and N is the length of the target sentence. The speedup is evaluated on
the WMT14 En-De test set with a batch size of 1. ‘*’ indicates the results of our re-implementation."
SETUP,0.30120481927710846,"Implementation Details For our method, we choose the l = 1, λ = 4 settings for Support Tree of
RH-PCFG and linearly anneal τ from 0.5 to 0.1 for the glancing training. For DA-Transformer [14],
we use λ = 8 for the graph size, which has comparable hidden states to our models. All models are
optimized with Adam [20] with β = (0.9, 0.98) and ϵ = 10−8. For all models, each batch contains
approximately 64K source words. All models are trained for 300K steps. For WMT14 En-De and
WMT17 Zh-En, we measure validation BLEU for every epoch and average the 5 best checkpoints as
the final model. For WMT16 En-Ro, we just use the best checkpoint on the valid dataset. We use the
NVIDIA Tesla V100S-PCIE-32GB GPU to measure the translation latency on the WMT14 En-De
test set with a batch size of 1. We implement our models based on the open-source framework of
fairseq [27]."
MAIN RESULTS,0.3042168674698795,"4.2
Main Results"
MAIN RESULTS,0.3072289156626506,Translation Quality
MAIN RESULTS,0.3102409638554217,"As demonstrated in Table 2, PCFG-NAT outperforms CTC-GLAT [30] and DA-Transformer [14] in
terms of translation quality, effectively reducing the performance gap between NAT and AT models.
This indicates its ability to capture the intricate translation distribution inherent in the raw training
data and efficiently learn from dependencies with reduced multi-modality. FA-DAT [26] builds upon"
MAIN RESULTS,0.3132530120481928,"Decoding Strategy
WMT14
WMT17
WMT16
Speedup
En-De
De-En
En-Zh
Zh-En
En-Ro
Ro-En
Greedy Decoding
26.01
31.38
32.82
22.50
32.05
32.65
1.00×
Viterbi Decoding
27.02
32.29
33.60
23.40
32.72
33.07
0.91×"
MAIN RESULTS,0.31626506024096385,"Table 3: Ablation Study on the effect of decoding strategies. We use the NVIDIA Tesla V100S-PCIE-
32GB GPU to measure the speedup on the WMT14 En-De test set with a batch size of 1."
MAIN RESULTS,0.3192771084337349,"DA-Transformer and trains the model to maximize a fuzzy alignment score between the graph and
reference, taking captured translations in all modalities into account. The same training objective
can also be implemented in PCFG-NAT. Experimental results show that FA-DAT [26] achieves
state-of-the-art NAT performance. In the future, we plan to incorporate the ideas from FA-DAT [26]
into PCFG-NAT, enabling a fairer comparison and demonstrating the superiority of PCFG modeling."
MAIN RESULTS,0.32228915662650603,Inference Latency
MAIN RESULTS,0.3253012048192771,"As shown Table 2, PCFG-NAT demonstrates a decoding speedup that is slightly lower than Vanilla
NAT but significantly higher than both AT models and iterative NAT models. This observation
indicates that PCFG-NAT effectively strikes a favorable balance between translation quality and
decoding latency, achieving a desirable trade-off between the two factors."
ABLATION STUDY,0.32831325301204817,"4.3
Ablation study"
STRUCTURE OF SUPPORT TREE,0.3313253012048193,"4.3.1
Structure of Support Tree"
STRUCTURE OF SUPPORT TREE,0.33433734939759036,"4.0
4.5
5.0
5.5
6.0
6.5
7.0
7.5
8.0
 * 2^l 26.2 26.4 26.6 26.8 27.0"
STRUCTURE OF SUPPORT TREE,0.3373493975903614,BLEU on WMT14 En-De
STRUCTURE OF SUPPORT TREE,0.34036144578313254,Different Local Prefix Tree Max Layer Size
STRUCTURE OF SUPPORT TREE,0.3433734939759036,"Max Layer Size 1
Max Layer Size 2"
STRUCTURE OF SUPPORT TREE,0.3463855421686747,"Figure 5: Ablation study on the max depth
l of local prefix tree and upsample rample
ration λ. Notice the number of the non-
terminal symbols m equals to λLx · 2l + 2,
where Lx is the length of the source sen-
tence."
STRUCTURE OF SUPPORT TREE,0.3493975903614458,"In this section, we examine the impact of the max depth
l of the local prefix tree and the upsample ratio λ in the
support tree of RH-PCFG. Figure 5 presents the results.
When the total number of non-terminal symbols is kept
constant, incorporating a local prefix tree with max
two layers leads to a significant decline in performance.
We posit that an excessively large value of l leads to
a significant increase in the number of potential parse
trees, thereby making it more challenging for the model
to learn complex structures from the data. As a result,
the translation performance of the model may decline."
DECODING STRATEGY,0.35240963855421686,"4.4
Decoding Strategy"
DECODING STRATEGY,0.35542168674698793,"To determine the optimal parse tree for a given sentence
length, we employ Viterbi decoding. However, this
approach incurs a certain computational cost in terms
of decoding speed. To demonstrate the necessity of
Viterbi decoding, we compare it with a greedy decoding
strategy that always selects the rule with the highest
probability. The experimental results, presented in Table 3, indicate that while Viterbi decoding
is only slightly slower than greedy decoding, it yields significantly better performance. Based on
these findings, we assert that Viterbi decoding is necessary and do not recommend the use of greedy
decoding."
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.35843373493975905,"4.5
Ablation Study for Distilled Data and GLAT"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3614457831325301,"We conducted ablation studies to examine the impact of Glancing Training and Knowledge Distillation
on our model’s performance.GLAT consistently improves the translation performance of all models,
while the gains from KD are more limited."
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3644578313253012,"WMT16 En-Ro
BLEU"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3674698795180723,"Transformer [40]
Baseline
34.26
+KD
34.72"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3704819277108434,"DA Transformer [14]
Baseline
31.98
+GLAT
32.46
+GLAT & KD
32.40"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.37349397590361444,"PCFG-NAT
Baseline
31.94
+GLAT
32.72
+GLAT & KD
32.75"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.37650602409638556,"Table 4: Comparison of WMT16 En-Ro translation performance (BLEU scores) for Transformer,
DA Transformer, and PCFG-NAT models with and without Glancing (GLAT) [30] and Knowledge
Distillation (KD) techniques. Baseline means the models are trained on raw train sets without GLAT
[30]."
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3795180722891566,"NN
IN
DT
NNP
JJ
NNS
.
VB
,
VBD
RB
VBZ
CC
VBN
TO
CD
PRP
VBP
MD
VBG
PRP
subword
0.0 0.1 0.2 0.3 0.4 0.5 0.6"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3825301204819277,Frequency
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3855421686746988,"Main Chain
Local Prefix Tree"
ABLATION STUDY FOR DISTILLED DATA AND GLAT,0.3885542168674699,"Figure 6: The frequency of part-of-speech and subwords generated from the main chain and local
prefix tree respectively."
SYNTAX ANALYSIS FOR GENERATED PARSE TREE,0.39156626506024095,"4.6
Syntax Analysis for Generated Parse Tree"
SYNTAX ANALYSIS FOR GENERATED PARSE TREE,0.39457831325301207,"To analyze the syntax information captured by PCFG-NAT, we conducted an analytical experiment
on the WMT14 De-En test set. We utilized the averaged perceptron tagger, a part-of-speech tagging
tool available in the nltk library [24], to assign part-of-speech tags to the translations. Subsequently,
we calculated the frequency of part-of-speech tags for tokens directly generated from non-terminal
symbols in both the main chain and the local prefix tree. Additionally, we counted the frequency of
unfinished subwords ending with the symbol @@. To ensure comparability, the frequency scores were
normalized by the number of tokens in the main chain and local prefix tree, respectively. The results
are presented in Figure 6."
SYNTAX ANALYSIS FOR GENERATED PARSE TREE,0.39759036144578314,"As depicted in Figure 6, the majority of tokens generated by non-terminal symbols in the local prefix
tree correspond to the part-of-speech tags RB (adverb), JJ (adjective), NNP (proper noun, singular),
and comma. Furthermore, the frequency of unfinished subwords in the local prefix tree symbols is
notably higher compared to that in the main chain. It is well-known that RB, JJ, NNP, comma, and
unfinished subwords are closely related to the local adjacent tokens. Hence, PCFG-NAT effectively
captures the connection between these elements with the local prefix tree serving as complementary
information to the main chain."
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.4006024096385542,"4.7
Probability Analysis of the Maximum Likelihood Path"
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.4036144578313253,"We calculated the proportion of the maximum probability parse tree (path) that generates the ref-
erences among all parse trees (paths) to investigate the benefits of the syntax tree learned by the
RH-PCFG in comparison to the path in the Directed Acyclic Graph (DAG). We observed that the
proportion of the highest probability parse tree for PCFG-NAT is generally higher than the proportion"
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.4066265060240964,"WMT14
WMT17
WMT16
Average
En-De
De-En
En-Zh
Zh-En
En-Ro
Ro-En"
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.40963855421686746,"DA-Transformer
0.6026
0.7565
0.4899
0.5611
0.7948
0.7124
0.6529
PCFG-NAT
0.6421
0.7718
0.4681
0.5886
0.7764
0.7160
0.6605"
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.4126506024096386,"Table 5: In the Valid Set, for the DA-Transformer model, we calculated P(Y |X, A∗)/P(Y |X), where
X, Y represents the source text and the target text in the dataset, A∗= argmaxAP(A|X, Y ), and
A represents a hidden state path. For the PCFG-NAT model, we calculated P(Y |X, T ∗)/P(Y |X),
where T ∗= argmaxT P(T|X, Y ), and T represents a parse tree."
PROBABILITY ANALYSIS OF THE MAXIMUM LIKELIHOOD PATH,0.41566265060240964,"of the highest probability path for DA-Transformer [14]. This demonstrates that in the trained
RH-PCFG, the probabilities of parse trees are more concentrated. Under the same training set and
learnable parameters, this indicates that the modeling approach of RH-PCFG is more in line with the
intrinsic structure of sentences compared to the flattened assumption of Directed Acyclic Graph."
RELATED WORKS,0.4186746987951807,"5
Related Works"
RELATED WORKS,0.42168674698795183,"Gu et al. [13] accelerates neural machine translation with a non-autoregressive Transformer, which
comes at the cost of translation quality. The major reason for the performance degradation is the
multi-modality problem that there may exist multiple translations for the same source sentence. Many
efforts have been devoted to enhancing the representation power of NAT. One thread of research
introduces latent variables to directly model the uncertainty in the translation process, with techniques
like vector quantization [16, 33, 3, 4], generative flow [25], and variational inference [38, 12]. Another
thread of research explores some statistical information for intermediate prediction, including word
fertility [13], word alignment [31, 39], and syntactic structures [2, 23]. Zhang et al. [43] explores
the syntactic multi-modality problem in non-autoregressive machine translation The most relevant
works to PCFG-NAT are CTC-based NAT [11, 22] and DA-Transformer [14, 37, 26], which enriches
the representation power of NAT with a longer decoder. They can simultaneously consider multiple
translation modalities by ignoring some tokens [11] or modeling transitions [14]. Fang et al. [7]
proposes a non-autoregressive direct speech-to-speech translation model, which achieves both high-
quality translations and fast decoding speeds by decomposing the generation process into two steps.
Compared to previous NAT approaches, PCFG-NAT mitigates the issue of multi-modality in NAT by
capturing structured non-adjacent semantic dependencies in the target language, leading to improved
translation quality. Our work is also related to the line of work on incorporating structured semantic
information into machine translation models[42, 19, 32, 1]. In contrast to these works, PCFG-NAT
learns structured information unsupervised from data and can generate target tokens in parallel."
CONCLUSION,0.4246987951807229,"6
Conclusion"
CONCLUSION,0.42771084337349397,"Non-autoregressive Transformer has higher inference speed but weaker expression power due to the
lack of dependency modeling. In this paper, we propose PCFG-NAT to model target-side dependency
by capturing the hierarchical structure of the sentence. Experimental results show that PCFG-NAT
further enhances the translation performance of NAT models while providing a more interpretable
approach for generating translations."
LIMITATIONS,0.4307228915662651,"7
Limitations"
LIMITATIONS,0.43373493975903615,"Further research and exploration are needed to overcome the limitations encountered when attempting
to induce complex PCFGs from data. Developing more advanced learning algorithms or exploring
alternative approaches for modeling and capturing the rich syntactic structures of language may be
necessary to unlock the full potential of PCFG-NAT and further enhance its translation capabilities."
ACKNOWLEDGEMENT,0.4367469879518072,"8
Acknowledgement"
ACKNOWLEDGEMENT,0.4397590361445783,"This work is partially supported by the National Key R&D Program of China(under Grant
2021ZD0110102), the NSF of China(under Grants 61925208), CAS Project for Young Scientists
in Basic Research(YSBR-029) and Xplore Prize. We thank all the anonymous reviewers for their
insightful and valuable comments."
REFERENCES,0.4427710843373494,References
REFERENCES,0.4457831325301205,"[1] R. Aharoni and Y. Goldberg. Towards string-to-tree neural machine translation. In Proceed-
ings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers), pages 132–140, Vancouver, Canada, July 2017. Association for Computational
Linguistics. doi: 10.18653/v1/P17-2021. URL https://aclanthology.org/P17-2021."
REFERENCES,0.44879518072289154,"[2] N. Akoury, K. Krishna, and M. Iyyer. Syntactically supervised transformers for faster neural
machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1269–1281, Florence, Italy, July 2019. Association for Computational
Linguistics. doi: 10.18653/v1/P19-1122. URL https://aclanthology.org/P19-1122."
REFERENCES,0.45180722891566266,"[3] Y. Bao, S. Huang, T. Xiao, D. Wang, X. Dai, and J. Chen. Non-autoregressive translation by
learning target categorical codes. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 5749–5759, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.naacl-main.458. URL https://aclanthology.org/2021.naacl-main.458."
REFERENCES,0.45481927710843373,"[4] Y. Bao, H. Zhou, S. Huang, D. Wang, L. Qian, X. Dai, J. Chen, and L. Li. latent-GLAT:
Glancing at latent variables for parallel text generation. In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
8398–8409, Dublin, Ireland, May 2022. Association for Computational Linguistics.
doi:
10.18653/v1/2022.acl-long.575. URL https://aclanthology.org/2022.acl-long.575."
REFERENCES,0.4578313253012048,"[5] N. Chomsky. On certain formal properties of grammars. Information and Control, 2(2):137–
167, 1959. ISSN 0019-9958. doi: https://doi.org/10.1016/S0019-9958(59)90362-6. URL
https://www.sciencedirect.com/science/article/pii/S0019995859903626."
REFERENCES,0.4608433734939759,"[6] C. Du, Z. Tu, and J. Jiang. Order-agnostic cross entropy for non-autoregressive machine
translation. CoRR, abs/2106.05093, 2021. URL https://arxiv.org/abs/2106.05093."
REFERENCES,0.463855421686747,"[7] Q. Fang, Y. Zhou, and Y. Feng. Daspeech: Directed acyclic transformer for fast and high-quality
speech-to-speech translation. In Advances in Neural Information Processing Systems, 2023."
REFERENCES,0.46686746987951805,"[8] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer. Mask-predict: Parallel decoding of
conditional masked language models. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pages 6112–6121, Hong Kong, China, Nov. 2019.
Association for Computational Linguistics. doi: 10.18653/v1/D19-1633. URL https://
aclanthology.org/D19-1633."
REFERENCES,0.46987951807228917,"[9] M. Ghazvininejad, V. Karpukhin, L. Zettlemoyer, and O. Levy. Aligned cross entropy for
non-autoregressive machine translation. CoRR, abs/2004.01655, 2020. URL https://arxiv.
org/abs/2004.01655."
REFERENCES,0.47289156626506024,"[10] M. Ghazvininejad, O. Levy, and L. Zettlemoyer. Semi-autoregressive training improves mask-
predict decoding. CoRR, abs/2001.08785, 2020. URL https://arxiv.org/abs/2001.
08785."
REFERENCES,0.4759036144578313,"[11] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification:
Labelling unsegmented sequence data with recurrent neural nets. In ICML ’06: Proceedings of
the International Conference on Machine Learning, 2006."
REFERENCES,0.4789156626506024,"[12] J. Gu and X. Kong. Fully non-autoregressive neural machine translation: Tricks of the trade.
CoRR, abs/2012.15833, 2020. URL https://arxiv.org/abs/2012.15833."
REFERENCES,0.4819277108433735,"[13] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher. Non-autoregressive neural machine
translation. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=B1l8BtlCb."
REFERENCES,0.48493975903614456,"[14] F. Huang, H. Zhou, Y. Liu, H. Li, and M. Huang. Directed acyclic transformer for non-
autoregressive machine translation. In Proceedings of the 39th International Conference on
Machine Learning, ICML 2022, 2022."
REFERENCES,0.4879518072289157,"[15] X. S. Huang, F. Perez, and M. Volkovs. Improving non-autoregressive translation models
without distillation. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=I2Hw58KHp8O."
REFERENCES,0.49096385542168675,"[16] L. Kaiser, S. Bengio, A. Roy, A. Vaswani, N. Parmar, J. Uszkoreit, and N. Shazeer. Fast
decoding in sequence models using discrete latent variables. In J. Dy and A. Krause, editors,
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Pro-
ceedings of Machine Learning Research, pages 2390–2399. PMLR, 10–15 Jul 2018. URL
http://proceedings.mlr.press/v80/kaiser18a.html."
REFERENCES,0.4939759036144578,"[17] J. Kasai, J. Cross, M. Ghazvininejad, and J. Gu. Parallel machine translation with disentangled
context transformer. CoRR, abs/2001.05136, 2020. URL https://arxiv.org/abs/2001.
05136."
REFERENCES,0.49698795180722893,"[18] J. Kasai, J. Cross, M. Ghazvininejad, and J. Gu. Non-autoregressive machine translation with
disentangled context transformer. In International conference on machine learning, pages
5144–5155. PMLR, 2020."
REFERENCES,0.5,"[19] Y. Kim. Sequence-to-sequence learning with latent neural grammars. Advances in Neural
Information Processing Systems, 34:26302–26317, 2021."
REFERENCES,0.5030120481927711,"[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and
Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980."
REFERENCES,0.5060240963855421,"[21] J. Libovický and J. Helcl. End-to-end non-autoregressive neural machine translation with
connectionist temporal classification. CoRR, abs/1811.04719, 2018. URL http://arxiv.
org/abs/1811.04719."
REFERENCES,0.5090361445783133,"[22] J. Libovický and J. Helcl. End-to-end non-autoregressive neural machine translation with
connectionist temporal classification. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 3016–3021, Brussels, Belgium, Oct.-Nov.
2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1336. URL https:
//www.aclweb.org/anthology/D18-1336."
REFERENCES,0.5120481927710844,"[23] Y. Liu, Y. Wan, J. Zhang, W. Zhao, and P. Yu. Enriching non-autoregressive transformer with
syntactic and semantic structures for neural machine translation. In Proceedings of the 16th
Conference of the European Chapter of the Association for Computational Linguistics: Main
Volume, pages 1235–1244, Online, Apr. 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.eacl-main.105. URL https://aclanthology.org/2021.eacl-main.
105."
REFERENCES,0.5150602409638554,"[24] E. Loper and S. Bird. Nltk: The natural language toolkit, 2002. URL https://arxiv.org/
abs/cs/0205028."
REFERENCES,0.5180722891566265,"[25] X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy. FlowSeq: Non-autoregressive conditional
sequence generation with generative flow. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pages 4282–4292, Hong Kong, China, Nov. 2019.
Association for Computational Linguistics. doi: 10.18653/v1/D19-1437. URL https://www.
aclweb.org/anthology/D19-1437."
REFERENCES,0.5210843373493976,"[26] Z. Ma, C. Shao, S. Gui, M. Zhang, and Y. Feng. Fuzzy alignments in directed acyclic graph for
non-autoregressive machine translation. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=LSz-gQyd0zE."
REFERENCES,0.5240963855421686,"[27] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq:
A fast, extensible toolkit for sequence modeling. CoRR, abs/1904.01038, 2019. URL http:
//arxiv.org/abs/1904.01038."
REFERENCES,0.5271084337349398,"[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evalua-
tion of machine translation.
In Proceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July
2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL
https://aclanthology.org/P02-1040."
REFERENCES,0.5301204819277109,"[29] M. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference
on Machine Translation: Research Papers, pages 186–191, Belgium, Brussels, Oct. 2018.
Association for Computational Linguistics. URL https://www.aclweb.org/anthology/
W18-6319."
REFERENCES,0.5331325301204819,"[30] L. Qian, H. Zhou, Y. Bao, M. Wang, L. Qiu, W. Zhang, Y. Yu, and L. Li. Glancing transformer
for non-autoregressive neural machine translation. In Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers), pages 1993–2003, Online, Aug.
2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.155. URL
https://aclanthology.org/2021.acl-long.155."
REFERENCES,0.536144578313253,"[31] Q. Ran, Y. Lin, P. Li, and J. Zhou. Guiding non-autoregressive neural machine translation
decoding with reordering information. In Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Virtual Event, February 2-9, 2021, pages 13727–13735. AAAI Press, 2021. URL
https://ojs.aaai.org/index.php/AAAI/article/view/17618."
REFERENCES,0.5391566265060241,"[32] P. Rastogi, R. Cotterell, and J. Eisner. Weighting finite-state transductions with neural context.
Association for Computational Linguistics, 2016."
REFERENCES,0.5421686746987951,"[33] A. Roy, A. Vaswani, A. Neelakantan, and N. Parmar. Theory and experiments on vector
quantized autoencoders. arXiv preprint arXiv:1805.11063, 2018."
REFERENCES,0.5451807228915663,"[34] C. Saharia, W. Chan, S. Saxena, and M. Norouzi. Non-autoregressive machine translation
with latent alignments. CoRR, abs/2004.07437, 2020. URL https://arxiv.org/abs/2004.
07437."
REFERENCES,0.5481927710843374,"[35] I. Sakai. Syntax in universal translation. In EARLYMT, 1961."
REFERENCES,0.5512048192771084,"[36] R. Sennrich, B. Haddow, and A. Birch.
Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany, Aug.
2016. Association for Computational Linguistics.
doi:
10.18653/v1/P16-1162.
URL
https://aclanthology.org/P16-1162."
REFERENCES,0.5542168674698795,"[37] C. Shao, Z. Ma, and Y. Feng.
Viterbi decoding of directed acyclic transformer for non-
autoregressive machine translation. In Findings of EMNLP 2022, 2022."
REFERENCES,0.5572289156626506,"[38] R. Shu, J. Lee, H. Nakayama, and K. Cho. Latent-variable non-autoregressive neural machine
translation with deterministic inference using a delta posterior. In AAAI, 2020."
REFERENCES,0.5602409638554217,"[39] J. Song, S. Kim, and S. Yoon. AligNART: Non-autoregressive neural machine translation by
jointly learning to estimate alignment and translate. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, pages 1–14, Online and Punta Cana,
Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/
2021.emnlp-main.1. URL https://aclanthology.org/2021.emnlp-main.1."
REFERENCES,0.5632530120481928,"[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and
I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.
cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf."
REFERENCES,0.5662650602409639,"[41] A. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory, 13(2):260–269, 1967. doi: 10.1109/TIT.
1967.1054010."
REFERENCES,0.5692771084337349,"[42] B. Wang, I. Titov, J. Andreas, and Y. Kim. Hierarchical phrase-based sequence-to-sequence
learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, pages 8211–8229, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for
Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.563."
REFERENCES,0.572289156626506,"[43] K. Zhang, R. Wang, X. Tan, J. Guo, Y. Ren, T. Qin, and T.-Y. Liu. A study of syntactic multi-
modality in non-autoregressive machine translation. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 1747–1757, Seattle, United States, July 2022. Association
for Computational Linguistics.
doi: 10.18653/v1/2022.naacl-main.126.
URL https://
aclanthology.org/2022.naacl-main.126."
REFERENCES,0.5753012048192772,"A
Appendix"
REFERENCES,0.5783132530120482,"A.1
Construction of Support Tree"
REFERENCES,0.5813253012048193,src_length stands for the length of source sentence Lx.
REFERENCES,0.5843373493975904,layer_size stands for max depth of local prefix tree l.
REFERENCES,0.5873493975903614,upsample_ratio stands for hyperparameter λ.
DEF,0.5903614457831325,"1 def
build_support_tree (src_length , layer_size , upsample_lambda ):"
DEF,0.5933734939759037,"2
main_chain_size = upsample_lambda * src_length + 1"
DEF,0.5963855421686747,"3
root = Node ()"
DEF,0.5993975903614458,"4
root.leftChild = Node ()"
DEF,0.6024096385542169,"5
now = root"
DEF,0.6054216867469879,"6
for i in range( main_chain_size ):"
DEF,0.608433734939759,"7
now.rightChild = Node ()"
DEF,0.6114457831325302,"8
now = now.rightChild"
DEF,0.6144578313253012,"9
now.leftChild = full_binary_tree (layer_size)"
"RETURN
ROOT",0.6174698795180723,"10
return
root"
"RETURN
ROOT",0.6204819277108434,Listing 1: Code for Constructing a Support Tree in Section 3.1.2
DEF,0.6234939759036144,"1 def
full_binary_tree (layer_size):"
DEF,0.6265060240963856,"2
if layer_size
<= 0:"
"RETURN
NONE",0.6295180722891566,"3
return
None"
"RETURN
NONE",0.6325301204819277,"4
root = Node ()"
"RETURN
NONE",0.6355421686746988,"5
root.leftChild = full_binary_tree (layer_size -1)"
"RETURN
NONE",0.6385542168674698,"6
root.rightChild = full_binary_tree (layer_size -1)"
"RETURN
ROOT",0.641566265060241,"7
return
root
Listing 2: Code for Constructing a Constructing a Full Binary Tree with deepth layer_size"
"RETURN
ROOT",0.6445783132530121,The function build_support_tree returns the root node of the support tree.
"RETURN
ROOT",0.6475903614457831,"A.2
CYK Algorithm for RH-PCFG"
"RETURN
ROOT",0.6506024096385542,"m, n stands for the numbers of non-terminal symbols and target sentence length."
"RETURN
ROOT",0.6536144578313253,layer_size stands for max depth of local prefix tree l.
"RETURN
ROOT",0.6566265060240963,OpProbs[a][i] represents P(yi|Va).
"RETURN
ROOT",0.6596385542168675,"RuleProb[a][b][c] represents P(< Vb, Vc > |Va)."
"RETURN
ROOT",0.6626506024096386,"S[a][i][j] represents P(Va ⇒yi, ..., yj). S are initialized to 0."
DEF,0.6656626506024096,"1 def
local_prefix_tree_cyk (S, OpProbs , RuleProb , m, n, layer_size):"
DEF,0.6686746987951807,"2
for i in range(0,n):"
DEF,0.6716867469879518,"3
for a in range(0,m):"
DEF,0.6746987951807228,"4
S[a][i][i] = OpProbs[a][i]"
DEF,0.677710843373494,"5
for span in range(2, 2** layer_size):"
DEF,0.6807228915662651,"6
for a in range(0,m):"
DEF,0.6837349397590361,"7
for i in range(0,n):"
DEF,0.6867469879518072,"8
for b,c in Child(a):"
DEF,0.6897590361445783,"9
for k in range(i, i+span):"
DEF,0.6927710843373494,"10
S[a][i][i+span] += RuleProb[a][b][c] \"
DEF,0.6957831325301205,"11
* OpProbs[a][k] \"
DEF,0.6987951807228916,"12
* S[b][i][k-1] * S[c][k+1][i+span]
Listing 3: Code for CYK Training of Local Prefix Tree in Section 3.3.1."
DEF,0.7018072289156626,"1 def
main_chain_cyk (S, OpProbs , RuleProb , m, n, layer_size):"
DEF,0.7048192771084337,"2
for a in range(0, m):"
DEF,0.7078313253012049,"3
S[a][n -1][n-1] = OpProbs[a][n-1]"
DEF,0.7108433734939759,"4
for start in range(n-2, -1,
-1):"
DEF,0.713855421686747,"5
for a in range(0, m):"
DEF,0.7168674698795181,"6
for i in range(0,n):"
DEF,0.7198795180722891,"7
for b,c in Child(a):"
DEF,0.7228915662650602,"8
for k in range(start , start +2** layer_size):"
DEF,0.7259036144578314,"9
S[a][ start ][n-1] += RuleProb[a][b][c] \"
DEF,0.7289156626506024,"10
* OpProbs[a][k] \"
DEF,0.7319277108433735,"11
* S[b][ start ][k-1] * S[c][k+1][n-1]
Listing 4: Code for CYK Training of Main Chain in Section 3.3.1."
DEF,0.7349397590361446,"After applying local_prefix_tree_cyk and main_chain_cyk, S[1][0][n-1] contains the
value of P(V1 ⇒y0, ..., yn−1)"
DEF,0.7379518072289156,"A.3
Best Parse Tree for Glancing Training"
DEF,0.7409638554216867,"m, n stands for the numbers of non-terminal symbols and target sentence length."
DEF,0.7439759036144579,layer_size stands for max depth of local prefix tree l.
DEF,0.7469879518072289,OpProbs[a][i] represents P(yi|Va).
DEF,0.75,"RuleProb[a][b][c] represents P(< Vb, Vc > |Va)."
DEF,0.7530120481927711,"S[a][i][j] represents P(Va ⇒yi, ..., yj)."
DEF,0.7560240963855421,"Trace[a][i][j]=True means that derivation P(Va ⇒yi, ..., yj) is part of the maximum derivation
of target sentence. Trace are initialized to False."
DEF,0.7590361445783133,"A[k]=a means that the token at position k in target sentence should be aligned to non-terminal
symbol Va. A are initialized to 0."
DEF,0.7620481927710844,"1 def
best_parse_tree (S, A, Trace , OpProbs , RuleProb , m, n, layer_size):"
DEF,0.7650602409638554,"2
Trace [1][0][n-1] = True"
DEF,0.7680722891566265,"3
for start in range(0, n-2):"
DEF,0.7710843373493976,"4
for a in range(0, m):"
DEF,0.7740963855421686,"5
if Trace[a][ start ][n -1]:"
DEF,0.7771084337349398,"6
max_b , max_c , max_k = argmax("
DEF,0.7801204819277109,"7
RuleProb[a][b][c] \"
DEF,0.7831325301204819,"8
* OpProbs[a][k] \"
DEF,0.786144578313253,"9
* S[b][ start ][k-1] * S[c][k+1][n-1] \"
DEF,0.7891566265060241,"10
for (b,c,k) in \"
DEF,0.7921686746987951,"11
zip(( Child(a), \"
DEF,0.7951807228915663,"12
range(start , start +2** layer_size))) 13
)"
DEF,0.7981927710843374,"14
Trace[max_b ][ start ][max_k -1] = True"
DEF,0.8012048192771084,"15
Trace[max_c ][ max_k +1][n-1] = True"
DEF,0.8042168674698795,"16
A[max_k] = a 17"
DEF,0.8072289156626506,"18
for span in range(2, 2** layer_size):"
DEF,0.8102409638554217,"19
for a in range(0, m):"
DEF,0.8132530120481928,"20
for i in range(0,n):"
DEF,0.8162650602409639,"21
if Trace[a][i][i+span ]:"
DEF,0.8192771084337349,"22
max_b , max_c , max_k = argmax("
DEF,0.822289156626506,"23
RuleProb[a][b][c] \"
DEF,0.8253012048192772,"24
* OpProbs[a][k] \"
DEF,0.8283132530120482,"25
* S[b][i][k-1] * S[c][k+1][i+span] \"
DEF,0.8313253012048193,"26
for (b,c,k) \"
DEF,0.8343373493975904,"27
in zip(Child(a), range(i, i+span)) 28
)"
DEF,0.8373493975903614,"29
Trace[max_b ][i][max_k -1] = True"
DEF,0.8403614457831325,"30
Trace[max_c ][ max_k +1][i+span] = True"
DEF,0.8433734939759037,"31
A[max_k] = a
Listing 5: Code for Glancing Training Best Parse Tree in Section 3.3.2."
DEF,0.8463855421686747,"After applying the algorithm, the tensor A contains the best alignment of non-terminal symbols and
target tokens."
DEF,0.8493975903614458,"A.4
Algorithm for Inference"
DEF,0.8524096385542169,"m, n stands for the numbers of non-terminal symbols and target sentence length."
DEF,0.8554216867469879,layer_size stands for max depth of local prefix tree l.
DEF,0.858433734939759,"S[a][i][j] represents P(Va ⇒yi, ..., yj)."
DEF,0.8614457831325302,MaxOpProbs[a] represents maxP(y|Va).
DEF,0.8644578313253012,MaxOpTokens[a] represents argmaxyP(y|Va).
DEF,0.8674698795180723,"RuleProb[a][b][c] represents P(< Vb, Vc > |Va)."
DEF,0.8704819277108434,"MAX_P[a][L] are the probability of M a
L of"
DEF,0.8734939759036144,"M a
L =
max
<b,c>∈Child(a),o∈T ,k=1..L−1 P(Va →VboVc)M b
L−1−kM c
k
(16)"
DEF,0.8765060240963856,"and MAX_K[a][L],MAX_B[a][L],MAX_C[a][L] are corresponding value of k, b, c."
DEF,0.8795180722891566,output is the target sentence generated by viterbi decoding algorithm.
DEF,0.8825301204819277,"1 def
viterbi_decoding (S, MaxOpProbs , RuleProb , m, n, layer_size , MAX_P ,"
DEF,0.8855421686746988,"MAX_K , MAX_B , MAX_C):"
DEF,0.8885542168674698,"2
for start in range(n-2, -1,
-1):"
DEF,0.891566265060241,"3
for a in range(0, m):"
DEF,0.8945783132530121,"4
max_value , max_b , max_c , max_k = argmax("
DEF,0.8975903614457831,"5
RuleProb[a][b][c] \"
DEF,0.9006024096385542,"6
* MaxOpProbs[a] \"
DEF,0.9036144578313253,"7
* MAX_P[b][k-1] * MAX_P[c][k+1][n-1] \"
DEF,0.9066265060240963,"8
for (b,c,k) in \"
DEF,0.9096385542168675,"9
zip(( Child(a), \"
DEF,0.9126506024096386,"10
range(start , start +2** layer_size))) 11
)"
DEF,0.9156626506024096,"12
MAX_P[a][n-start] = max_value"
DEF,0.9186746987951807,"13
MAX_K[a][n-start] = max_k"
DEF,0.9216867469879518,"14
MAX_B[a][n-start] = max_b"
DEF,0.9246987951807228,"15
MAX_C[a][n-start] = max_c 16"
DEF,0.927710843373494,"17
for span in range(2, 2** layer_size):"
DEF,0.9307228915662651,"18
for a in range(0, m):"
DEF,0.9337349397590361,"19
max_value , max_b , max_c , max_k = argmax("
DEF,0.9367469879518072,"20
RuleProb[a][b][c] \"
DEF,0.9397590361445783,"21
* MaxOpProbs[a] \"
DEF,0.9427710843373494,"22
* MAX_P[b][k-1] * MAX_P[c][span -k] \"
DEF,0.9457831325301205,"23
for (b,c,k) \"
DEF,0.9487951807228916,"24
in zip(Child(a), range(i, i+span)) 25
)"
DEF,0.9518072289156626,"26
MAX_P[a][ span] = max_value"
DEF,0.9548192771084337,"27
MAX_K[a][ span] = max_k"
DEF,0.9578313253012049,"28
MAX_B[a][ span] = max_b"
DEF,0.9608433734939759,"29
MAX_C[a][ span] = max_c"
DEF,0.963855421686747,Listing 6: Code for Viterbi Decoding Algorihtm in Section 3.4.
DEF,0.9668674698795181,"1 def
parse_tree(a, output , prefix_length , length , MaxOpTokens , MAX_B ,
MAX_C , MAX_K):"
DEF,0.9698795180722891,"2
max_b , max_c , max_k = MAX_B[a][ length], MAX_C[a][ length], MAX_K[a
][ length]"
DEF,0.9728915662650602,"3
output[prefix_length+max_k[length ]] = MaxOpTokens[a] 4"
DEF,0.9759036144578314,"5
parse_tree(max_b , prefix_length , max_k -1)"
DEF,0.9789156626506024,"6
parse_tree(max_c , prefix_length +max_k , length -max_k)"
DEF,0.9819277108433735,"First, we utilize the viterbi_decoding method to obtain the values of MAX_B, MAX_C, MAX_K.
Subsequently, considering the length constraint denoted as L, we employ the parse_tree func-
tion with parameters set as follows: starting index of 1 as V1, output to store the gener-"
DEF,0.9849397590361446,"ated target sentence, prefix_length with an initial value of 0, a maximum limit of L tokens,
MaxOpTokens,MAX_B,MAX_C,MAX_K. This process effectively generates the desired target sentence."
DEF,0.9879518072289156,"A.5
Case Study"
DEF,0.9909638554216867,"(a)
(b)"
DEF,0.9939759036144579,Figure 7: Two translation cases with generated parse tree from WMT14 De-En test set
DEF,0.9969879518072289,"To better understand the ability of PCFG-NAT to model structured semantic information, we select
two cases from the WMT14 De-En test set and take a close look at the generated parse tree of
PCFG-NAT. In Figure 7(a), we can see that in the sentence generated by PCFG-NAT, the content
words lie in the main chain of the generated parse tree, and the words like just, very, which are
supplements to words in the main chain, lie in the local prefix tree of the generated parse tree. In the
generated parsing tree in Figure 7(b), the unfinished subword bo@@ becomes the left node of its
suffix thers, which demonstrates that PCFG-NAT can learn the local structure of natural language
and enhances the interpretability of NMT to a certain extent."
