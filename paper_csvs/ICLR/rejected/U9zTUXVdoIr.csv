Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0018867924528301887,"The vulnerability of deep learning models to adversarial examples and seman-
tic transformations has limited the applications in risk-sensitive areas. The re-
cent development of certiﬁed defense approaches like randomized smoothing pro-
vides a promising direction towards building reliable machine learning systems.
However, current certiﬁed defenses cannot handle complex semantic transforma-
tions like rotational blur and defocus blur which are common in practical applica-
tions. In this paper, we propose a generalized randomized smoothing framework
(GSmooth) for certiﬁed robustness against semantic transformations. We provide
both a uniﬁed and rigorous theoretical framework and scalable algorithms for cer-
tiﬁed robustness on complex semantic transformations. Speciﬁcally, our key idea
is to use a surrogate image-to-image neural network to approximate a transforma-
tion which provides a powerful tool for studying the properties of semantic trans-
formations and certify the transformation based on this neural network. Experi-
ments on multiple types of semantic perturbations and corruptions using multiple
datasets demonstrate the effectiveness of our approach."
INTRODUCTION,0.0037735849056603774,"1
INTRODUCTION"
INTRODUCTION,0.005660377358490566,"Although deep learning models have achieved remarkable success on various applications (LeCun
et al., 2015), they are vulnerable to adversarial examples (Biggio et al., 2013; Szegedy et al., 2013;
Goodfellow et al., 2014) and semantic transformations (Hendrycks & Dietterich, 2019). The vulner-
ability of deep learning models can limit their applications on many important tasks. For example,
the autonomous driving system can be misled even by a small adversarial patch on the road mark
(Jing et al., 2021). Compared with the maliciously crafted adversarial examples, semantic transfor-
mations are more practical in real-world scenarios, such as rotation, translation, blur, bad weather,
and so on. Such transformations do not damage the semantic features of images and can be easily
recognized by humans, but they also degrade the performance of deep learning models. Therefore,
it is imperative to improve model robustness against semantic transformations."
INTRODUCTION,0.007547169811320755,"To develop more reliable machine learning systems, many efforts have been made to design defense
techniques against adversarial attacks or semantic transformations. The existing defense methods
can be categorized into empirical defenses and certiﬁed defenses. Adversarial training (AT) (Madry
et al., 2017; Zhang et al., 2019a) is one of the most effective empirical defenses against ℓp-norm
bounded adversarial examples. Moreover, methods based on data augmentation (Hendrycks et al.,
2019; Wang et al., 2019; Calian et al., 2021) have been proposed to empirically improve the perfor-
mance under semantic transformations. However, the performance of empirical defenses is difﬁcult
to be fully justiﬁed and these defenses can be further broken by new adaptive attacks (Athalye et al.,
2018; Tramer et al., 2020). In contrast, the certiﬁed defenses aim to theoretically provide a certiﬁed
region where the model is theoretically safe under any attack or perturbation (Wong & Kolter, 2018;
Cohen et al., 2019; Gowal et al., 2018; Zhang et al., 2019b). Along this line, developing certiﬁed
defense methods is a crucial step towards reliable machine learning systems."
INTRODUCTION,0.009433962264150943,"Although certiﬁed defenses have achieved great success, most of them are limited to defend against
ℓp-norm bounded attacks. However, the ℓp distance between the original image and its corrupted
counterpart by a semantic transformation (e.g., translation, rotation) would be large even when the"
INTRODUCTION,0.011320754716981131,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013207547169811321,"corruption is slight. Therefore, the current methods are incapable of certifying robustness against
such semantic perturbations. To solve this problem, several recent works (Fischer et al., 2020;
Mohapatra et al., 2020; Li et al., 2021) attempt to extend the certiﬁed defenses to several simple
semantic corruptions, including translation, rotation, and Gaussian blur. However, these works are
not scalable to certify robustness against complex and general semantic perturbations. First, deter-
ministic certiﬁed defenses (Mohapatra et al., 2020) based on convex relaxation for the activation
function require solving a complex optimization problem for computing bound which is computa-
tionally expensive. Second, probabilistic approaches based on randomized smoothing also demand
a handcrafted Lipschitz bound (Li et al., 2021), which is intractable for complicated semantic trans-
formations. For example, many semantic transformations such as glass blur and pixelate do not have
a closed form expression or they are black boxes and hard to be analyzed theoretically, but they are
common in real-world scenarios. Therefore, it is still highly challenging to certify robustness against
these complex and realistic semantic transformations."
INTRODUCTION,0.01509433962264151,"To address the aforementioned challenges, we propose a generalized randomized smoothing frame-
work (GSmooth). First, we provide a uniﬁed framework of GSmooth for certifying general seman-
tic transformations. Then we categorize the transformations into resolvable transformations (e.g.,
translation) and non-resolvable transformations (e.g., rotational blur) similar with Li et al. (2021).
As mentioned above, most non-resolvable transformations are complex and the existing methods
cannot provide their certiﬁed radius. To handle the challenge, we propose to use an image-to-image
translation neural network to approximate all these transformations. Due to the strong capacity of
neural networks, our method is ﬂexible and scalable to model these complex semantic transforma-
tions. By introducing an augmented noise in the layers of the surrogate model, we can theoretically
provide the certiﬁed radius for the proxy neural networks which can be used for certifying the orig-
inal transformations. Next, we provide theoretical analysis and error bounding for the approxima-
tion. Finally, we validate the effectiveness of our methods on several publicly available datasets.
Extensive experimental results demonstrate that our methods are effective for certifying complex
semantic transformations including different types of blur or image quality corruptions."
RELATED WORK,0.016981132075471698,"2
RELATED WORK"
ATTACKS AND DEFENSES FOR SEMANTIC TRANSFORMATIONS,0.018867924528301886,"2.1
ATTACKS AND DEFENSES FOR SEMANTIC TRANSFORMATIONS"
ATTACKS AND DEFENSES FOR SEMANTIC TRANSFORMATIONS,0.020754716981132074,"Unlike ℓp perturbation which adds small noise to every pixel of an image, semantic attacks or phys-
ical attacks are usually unrestricted. Brown et al. (2017); Song et al. (2018) use a small patch added
to the image to mislead the classiﬁer or the object detector. Engstrom et al. (2019; 2018); Xiao
et al. (2018) construct adversarial examples using spatial transformations like rotation or transla-
tion. Hendrycks & Dietterich (2019) show that a wide variety of semantic perturbations degrade
the performance for many deep learning models. Many works (Cubuk et al., 2019; Hendrycks et al.,
2019; 2020; Robey et al., 2020) propose diverse data augmentation techniques to enhance robustness
under semantic perturbations. Calian et al. (2021) propose adversarial data augmentation that can
be viewed as adversarial training for defending semantic perturbations. Beyond empirical defenses,
several works (Mohapatra et al., 2020; Madry et al., 2017; Singh et al., 2019; Balunovi´c et al.,
2019) attempt to certify some simple geometric transformations. However, all of them belong to
deterministic certiﬁcation approaches and their performance on realistic datasets are unsatisfactory."
RANDOMIZED SMOOTHING,0.022641509433962263,"2.2
RANDOMIZED SMOOTHING"
RANDOMIZED SMOOTHING,0.024528301886792454,"Randomized smoothing is a novel certiﬁcation method originated from differential privacy (Lecuyer
et al., 2019). Cohen et al. (2019) then improve the certiﬁed bound and apply it to large scale deep
neural networks and datasets. Yang et al. (2020) exhaustively analyze the robust radius by using
different noise distribution and norms. Hayes (2020); Yang et al. (2020) point out that randomized
smoothing suffers from curse of dimensionality for the l∞norm. Salman et al. (2019) adopt adver-
sarial training to train smoothed classiﬁers to obtain better robustness guarantees. Li et al. (2021);
Fischer et al. (2020) extend randomized smoothing to certify some simple semantic transforma-
tions, e.g., image translation and rotation. It shows that randomized smoothing could be generalized
to certify more diverse attacks or corruptions. However, their methods are limited to simple semantic
transformations, which are easy to analyze their mathematical properties."
RANDOMIZED SMOOTHING,0.026415094339622643,Under review as a conference paper at ICLR 2022
PROPOSED METHOD,0.02830188679245283,"3
PROPOSED METHOD"
PROPOSED METHOD,0.03018867924528302,"In this section, we present the framework and theoretical analyses of our Generalized Randomized
Smoothing (GSmooth). We ﬁrst introduce the basic notations. Then we divide the semantic trans-
formations into resolvable transformations and non-resolvable transformations similar with Li et al.
(2021). Next we introduce the details of our GSmooth for these two types of semantic transforma-
tions, respectively. Finally, we show the theoretical insight and proof sketch of our main results."
NOTATIONS,0.03207547169811321,"3.1
NOTATIONS"
NOTATIONS,0.033962264150943396,"We ﬁrst introduce the notations and formulation of the task. Given the input of x ∈Rn and the
labels of Y = {1, 2, . . . p}, we denote the classiﬁer as f(x) : Rn →[0, 1]p, which outputs predicted
probabilities over all p classes. The prediction of f is arg maxi∈Y f(x)i, where f(·)i denotes the
i-th element of f(·). Let τ(θ, x) : Rm ×Rn →Rn be a semantic transformation of raw input x with
parameter θ ∈Rm. We deﬁne the smoothed classiﬁer as"
NOTATIONS,0.035849056603773584,"G(x) = Eθ∼g[f(τ(θ, x))],
(1)"
NOTATIONS,0.03773584905660377,"which is the average prediction for the samples under a smoothing distribution g(θ) where g(θ) =
exp(−ψ(θ)) and ψ(θ) is a smooth function from Rm →R. Let ||u|| = 1 be any vector with unit
norm and a random variable γu = ⟨u, ∇ψ(δ)⟩where δ ∼g and ∇is the gradient operator of a
function. The complementary CDF is ϕu(c) = P[γu > c] and the inverse complementary CDF is
ϕ−1
u (p) = inf{c|P(γu > c) ⩽p}. Following Yang et al. (2020), we deﬁne a function Φ as"
NOTATIONS,0.03962264150943396,"Φ(p) = max||u||=1 E[γuI{γu > ϕ−1
u (p)}],
(2)"
NOTATIONS,0.04150943396226415,"which will be used to represent the certiﬁed radius. Let yA= arg maxi∈YG(x)i be the predicted
label by the smoothed classiﬁer G(x) and yB = arg maxi∈Y\yA G(x)i is the runner-up class. With-
out causing confusion, we use G(x)A to denote the probability of the top class G(x)yA; likewise for
G(x)B."
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.04339622641509434,"3.2
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.045283018867924525,"We ﬁrst discuss a class of transformations that are resolvable — the composition of two transforma-
tions with parameters belonging to a perturbation set θ, ξ ∈P ⊂Rm is still a transformation with
a new parameter γ(θ, ξ) ∈P ⊂Rm, where γ(·, ·) : P × P →P is a function depending on these
parameters. For resolvable semantic transformations, we have the following theorem.
Theorem 1. Let f(x) be any classiﬁer and G(x) be the smoothed classiﬁer deﬁned in Eq. (1). If
there exists a function M(·, ·) : P × P →R, the transformation τ(·, ·) satisﬁes"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.04716981132075472,"∂γ(θ, ξ)"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.04905660377358491,"∂ξ
= ∂γ(θ, ξ)"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.0509433962264151,"∂θ
M(θ, ξ),"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.052830188679245285,"and there exist two constants pA , pB satisfying"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.05471698113207547,"G(x)A ⩾pA ⩾pB ⩾G(x)B,"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.05660377358490566,"then yA = arg maxi∈Y G(τ(ξ, x))i holds for any ∥ξ∥⩽R where"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.05849056603773585,"R =
1
2M ∗ Z pA pB"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.06037735849056604,"1
Φ(p)dp,
(3)"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.062264150943396226,"and M ∗= maxξ,θ∈P ||M(ξ, θ)||."
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.06415094339622641,"Remark. The settings of Theorem 1 are similar with Li et al. (2021) for resolvable semantic transfor-
mations. But here we adopt a different presentation and proof for the theorem which could be easier
to extend to our GSmooth framework for general semantic transformations. Speciﬁcally, we show
two examples of the theorem which are additive transformations and commutable transformations.
A transformation is additive if τ(θ, τ(ξ, x)) = τ(ξ + θ, x) for any θ, ξ ∈P; or it is commutable if
τ(θ, τ(ξ, x)) = τ(ξ, τ(θ, x)) for any θ, ξ ∈P. For these two types of transformations, it is straight-
forward to verify that they satisfy the property proposed in Theorem 1. As an example, we simply
apply Theorem 1 for isotropic Gaussian distribution g(θ) = N(0, σ2I) and get the certiﬁed radius R = σ"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.0660377358490566,"2
 
Ψ
 
pA"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.06792452830188679,"
−Ψ (pB)

,
(4)"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.06981132075471698,Under review as a conference paper at ICLR 2022
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.07169811320754717,Transformation Parameters
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.07358490566037736,Input Image
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.07547169811320754,"F (   )
θ
1"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.07735849056603773,"F (   )
x
2"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.07924528301886792,"H ( )
."
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.08113207547169811,Augmented Noisy Images
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.0830188679245283,Base Classifier
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.08490566037735849,"~    g 
~    g"
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.08679245283018867,Augmented Noise Noise R
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.08867924528301886,Surrogate Model
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.09056603773584905,Certified Radius
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.09245283018867924,"Figure 1: A graphical illustration of our GSmooth. We use a surrogate image-to-image translation
network to accurately ﬁt a semantic transformation. Then we add a new augmented noise into the
surrogate model and construct the GSmooth classiﬁer. The augmented noise are sampled to ensure
the transformation to be resolvable in the semantic space. We theoretically calculate the certiﬁcation
bound for the surrogate model to certify the original semantic transformation."
CERTIFIED BOUND FOR RESOLVABLE SEMANTIC TRANSFORMATIONS,0.09433962264150944,"where Ψ is the inverse CDF of the standard Gaussian distribution. These two kinds of transforma-
tions include image translation and Gaussian blur, which are basic semantic transformations and
widely discussed in previous works (Li et al., 2021; Fischer et al., 2020). The certiﬁcation of these
simple transformations only requires applying translation or Gaussian blur to the sample and gets
the average classiﬁcation score under the noise distribution."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.09622641509433963,"3.3
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.09811320754716982,"Translation and Gaussian blur are two speciﬁc cases of semantic transformations. In practice, most
semantic transformations are not commutable or even not resolvable. Therefore, we need to de-
velop better methods for certifying more types of semantic transformations. However, the existing
methods like Semanify-NN (Mohapatra et al., 2020) based on convex relaxation and TSS (Li et al.,
2021) based on randomized smoothing require to develop a speciﬁc algorithm or bound for each in-
dividual semantic transformation. This is not scalable and might be infeasible for more complicated
transformations without explicit mathematical forms."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1,"To address the challenge, we draw inspiration from the fact that neural networks are able to ap-
proximate functions including a complex and unknown semantic transformation (Zhu et al., 2017).
First, we propose to use a surrogate image-to-image translation model to accurately ﬁt the semantic
transformation. Then we theorectially show that by introducing an augmented noise in the layers
of the surrogate model, as shown in Fig 1, randomized smoothing can be extended to handle these
transformations. Speciﬁcally, we deﬁne the surrogate model as the following form that will lead to
a simple certiﬁcation bound as we shall see:"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1018867924528302,"τ(θ, x) = H(F1(θ) + F2(x)),
(5)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.10377358490566038,"where F1(·) : Rm →Rd, F2(·) : Rn →Rd, and H(·) : Rd →Rn are three individual neural
networks. F1(·) and F2(·) are the encoders for transformation parameters and images respectively,
and their encodings are added together in the semantic space which is critical for its theoretical
certiﬁcation. We ﬁnd that the surrogate neural network is much easier to analyze and can be certiﬁed
by introducing a dimensional augmentation strategy for both noise parameters and input images."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.10566037735849057,"As illustrated in Fig. 1, an augmented noise is added to the semantic layers H(·) in the surrogate
model. Our key insight is that the transformation could be viewed as the superposition of a resolvable
part and a non-resolvable residual part in the augmented semantic space. Then we could use the
augmented noise to control the non-resolvable residual part if the augmented dimension d ⩾m+n.
This dimension augmentation is the key step of our technique. The augmentation for noise is from
Rm to Rd. To keep the dimension consistent, we also augment data x to Rd by padding 0 entries."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.10754716981132076,Under review as a conference paper at ICLR 2022
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.10943396226415095,"By certifying the transformation based on the surrogate model, we are able to certify the original
transformation if the approximation error is within an acceptable region (detailed analysis is in
Theorem 3). Our method is ﬂexible and scalable since the surrogate neural network has a uniform
mathematical form for theoretical analysis and they are trained automatically. Next, we discuss the
details of GSmooth."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.11132075471698114,"Speciﬁcally, we introduce the augmented data ˜x ∈Rd and the augmented parameter ˜θ ∈Rd as"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.11320754716981132,"˜x =

x′
x"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.11509433962264151,"
, ˜θ =

θ
θ′"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1169811320754717,"
,
(6)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.11886792452830189,"where the additional parameters θ′ ∈Rn are sampled from g′(θ′), and the joint distribution of θ′"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.12075471698113208,"and θ is ˜θ ∼˜g where ˜g(˜θ) = g′(θ′)g(θ). Moreover, the augmented data x′ ∈Rm. We deﬁne the
generalized smoothed classiﬁer as"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.12264150943396226,"˜G(˜x) = E˜θ∼˜g(˜θ)
h
˜f(˜τ(˜θ, ˜x))
i
,
(7)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.12452830188679245,"where ˜f is the “augmented target classiﬁer” that equals the original classiﬁer when constrained on
the original input x, which means ˜f(˜x) = f(x). This can be achieved by setting the weights of
additional dimensions to 0. Note that now all the functions are augmented for a d dimensional
input. We further augment our surrogate neural network to represent the augmented transformation
˜τ : Rd →Rd,
˜τ(˜θ, ˜x) = ˜H( ˜F1(˜θ) + ˜F2(˜x)),
(8)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.12641509433962264,"where ˜H(·), ˜F1(·), ˜F2(·) : Rd →Rd are parts of the augmented surrogate model. By carefully
designing the interaction between the augmented parameters and the original parameters, we could
turn the transformation to a resolvable one and it does not change the original surrogate model when
constraining to the original input x and θ. Speciﬁcally, we design the function ˜F1 and ˜F2 as follows:"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.12830188679245283,"˜F1(˜θ) =

θ
F1(θ) + θ′"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.13018867924528302,"
, ˜F2(˜x) =

x′
F2(x)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1320754716981132,"
, ˜H(˜x) =

Id−n
H(x)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1339622641509434,"
.
(9)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.13584905660377358,"Before stating our main theorem, we introduce several notations ˜zξ = ˜F1(˜ξ)+ ˜F2(˜x), ˜zθ = ˜F1(˜θ)+
˜F2(˜x), ˜yξ = (y′
ξ, yξ)T = ˜H( ˜F1(˜ξ)+ ˜F2(˜x)) and ˜yθ = (y′
θ, yθ)T = ˜H( ˜F1(˜θ)+ ˜F2(˜x)) for simplicity.
Then we theoretically prove that the GSmooth classiﬁer is certiﬁably robust within a given range:"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.13773584905660377,"Theorem 2. Suppose f(x) is a classiﬁer and ˜G(˜x) is the smoothed classiﬁer deﬁned in Eq. (7), if
there exist pA and pB satisfying"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.13962264150943396,"˜G(˜x)A ⩾pA ⩾pB ⩾˜G(˜x)B,"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.14150943396226415,"then yA = arg maxi∈Y ˜G(˜τ(˜ξ, ˜x))i for any ∥ξ∥2 ⩽R, where"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.14339622641509434,"R =
1
2M ∗ Z pA pB"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.14528301886792452,"1
Φ(p)dp,
(10)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1471698113207547,and the coefﬁcient M ∗is deﬁned as
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1490566037735849,"M ∗= max
ξ,θ∈P s"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1509433962264151,"1 +

∂F2(yξ)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.15283018867924528,"∂ξ
−∂F1(θ) ∂θ  2"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.15471698113207547,"2
.
(11)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.15660377358490565,"As the main result of our GSmooth, we have several observations about it. First, we see that the
certiﬁed radius is similar to the result in Theorem 1. Second, compared with resolvable transforma-
tions, we need to add a new type of noise when constructing the GSmooth classiﬁer. This isotropic
noise has the same dimension as the data and is added to the intermediate layers of surrogate neural
networks. The theoretical explanation behind this is that this isotropic noise makes the Jacobian
matrix of the semantic transformation to be invertible which is crucial for the proof. Third, we ob-
serve that the coefﬁcient M ∗depends on the norm of the difference of two Jacobian matrices and is
independent with the target classiﬁer, later we will discuss the meaning of this term in detail."
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.15849056603773584,"Before diving into our theoretical insight and proof of the theorem, we introduce a speciﬁc case
of Theorem 2 which is more convenient for practical usage. We empirically found that taking a"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.16037735849056603,Under review as a conference paper at ICLR 2022
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.16226415094339622,"linear transformation as F1(θ) = A1θ + b1 where A1 ∈Rn×m, b1 ∈Rn does not sacriﬁce the
precision of the surrogate network and we have ∂F1(θ)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1641509433962264,"∂θ
= A1. After substituting the term in Eq. (44)
we only need to optimize ξ for calculating M ∗and we make the bound tighter. Additionally, we
use two gaussian distributions for the noise distribution and the augmented noise distribution, i.e.
g(θ) = N(0, σ2
1I) and g′(θ′) = N(0, σ2
2I). Formally, we have the following corollary,"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1660377358490566,"Corollary 1. Suppose f(x) is a classiﬁer and ˜G(˜x) is the smoothed classiﬁer deﬁned in Eq. (7), if
the layer F1(θ) in the surrogate neural network has the following form:"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.16792452830188678,"F1(θ) = A1θ + b1
(12)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.16981132075471697,"where A1∈Rn×m, b1 ∈Rn are the parameters; and if there exists pA and pB satisfying"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.17169811320754716,"˜G(˜x)A ⩾pA ⩾pB ⩾˜G(˜x)B,"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.17358490566037735,"then yA = arg maxi∈Y ˜G(˜τ(˜ξ, ˜x))i for any ∥ξ∥2 ⩽R where"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.17547169811320754,"R =
1
2M ∗
 
Ψ
 
pA"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.17735849056603772,"
−Ψ (pB)

,
(13)"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1792452830188679,"where Ψ(·) is the inverse CDF of standard Gaussian distribution, and the coefﬁcient M ∗is deﬁned
as,"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1811320754716981,"M ∗= max
ξ∈P s"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.1830188679245283,"1
σ2
1
+ 1 σ2
2"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.18490566037735848,∂F2(yξ)
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.18679245283018867,"∂ξ
−A1  2"
CERTIFIED BOUND FOR GENERAL SEMANTIC TRANSFORMATIONS,0.18867924528301888,"2
.
(14)"
PROOF SKETCH AND THEORETICAL ANALYSIS,0.19056603773584907,"4
PROOF SKETCH AND THEORETICAL ANALYSIS"
PROOF SKETCH OF OUR MAIN THEOREM,0.19245283018867926,"4.1
PROOF SKETCH OF OUR MAIN THEOREM"
PROOF SKETCH OF OUR MAIN THEOREM,0.19433962264150945,"In this section, we brieﬂy summarize the main idea for proving the Theorem 2 and the theoretical
insight of our GSmooth. The key idea is to prove that the gradient of the smoothed classiﬁer can be
bounded by a function of the classiﬁcation conﬁdence and the parameters of the noise distribution.
Formally, we calculate the gradient to the perturbation parameter ξ for our augmented smoothed
classiﬁer as
∇˜ξ ˜G(˜τ(˜ξ, ˜x)) = ∇˜ξE˜θ∼˜g(˜θ)[ ˜f(˜τ(˜θ, ˜τ(˜ξ, ˜x)))].
(15)"
PROOF SKETCH OF OUR MAIN THEOREM,0.19622641509433963,We expand the expectation into integral and see that
PROOF SKETCH OF OUR MAIN THEOREM,0.19811320754716982,"∇˜ξ ˜G(˜τ(˜ξ, ˜x)) =
Z"
PROOF SKETCH OF OUR MAIN THEOREM,0.2,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ))"
PROOF SKETCH OF OUR MAIN THEOREM,0.2018867924528302,"∂˜ξ
˜g(˜θ)d˜θ.
(16)"
PROOF SKETCH OF OUR MAIN THEOREM,0.2037735849056604,"The key step is to eliminate the gradient of ∂˜
F (˜τ(˜θ,˜yξ))"
PROOF SKETCH OF OUR MAIN THEOREM,0.20566037735849058,"∂˜ξ
and replace it with ∂˜
f(˜τ(˜θ,˜yξ))"
PROOF SKETCH OF OUR MAIN THEOREM,0.20754716981132076,"∂˜θ
. Then we
integrate it by parts to get the following obejective,"
PROOF SKETCH OF OUR MAIN THEOREM,0.20943396226415095,"∇˜ξ ˜G(˜τ(˜ξ, ˜x)) = −
Z"
PROOF SKETCH OF OUR MAIN THEOREM,0.21132075471698114,"Rn+d
˜F(˜τ(˜θ, ˜yξ)) ∂"
PROOF SKETCH OF OUR MAIN THEOREM,0.21320754716981133,"∂˜θ
( ˜
M(˜ξ, ˜θ)˜g(˜θ))d˜θ.
(17)"
PROOF SKETCH OF OUR MAIN THEOREM,0.21509433962264152,"After that, we could bound the gradient of the GSmooth classiﬁer using the technique similar to
randomized smoothing (Yang et al., 2020). More details of the proof can be found in Appendix A."
THEORETICAL INSIGHT,0.2169811320754717,"4.2
THEORETICAL INSIGHT"
THEORETICAL INSIGHT,0.2188679245283019,"Next, we provide the theoretical insight for our augmentation scheme on transformation parameters
and data. First, the key is to expand the transformation space by adding additional dimensions to
form a closed space. In the augmented space, the Jacobian matrix of the semantic transformation
became invertible which is crucial for our proof. Second, as we can see in Eq. (14) that M ∗is
inﬂuenced by two factors. One is the standard deviation of two noise distributions. The other is the
norm of the Jacobian matrix ∂F2(yξ)"
THEORETICAL INSIGHT,0.22075471698113208,"∂ξ
−A1. It can be viewed as the residual of the non-resolvable
part of the transformation. Along this line, our method decomposes the unknown semantic transfor-
mation into a resolvable part and a residual part. The non-resolvable residual part could be handled
by introducing additional noise with standard deviation σ2."
THEORETICAL INSIGHT,0.22264150943396227,Under review as a conference paper at ICLR 2022
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.22452830188679246,"4.3
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.22641509433962265,"In this subsection, we theoretically analyze the effectiveness of certifying real semantic transforma-
tion due to the existence of approximation error of surrogate neural networks.
Theorem 3. Suppose the simulation of the semantic transformation has an small enough error"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.22830188679245284,"∥˜τ(˜ξ, ˜x) −τ(˜ξ, ˜x)∥2 < ε,"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.23018867924528302,"Then there exists a constant ratio A = A(∥F ′
1(˜ξ)∥2, ∥F ′
2(˜yξ)∥2, ∥F ′
2(˜zξ)∥2) > 0 does not depend
on the target classiﬁer, we have the certiﬁed radius for the real semantic transformation satisﬁes
that
Rr > R(1 −Aε)
where R is the certiﬁed radius for surrogate the neural network in Theorem 2 and"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.2320754716981132,"R =
1
2M ∗ Z pA pB"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.2339622641509434,"1
Φ(p)dp,"
ERROR ANALYSIS FOR SURROGATE MODEL APPROXIMATION,0.2358490566037736,"We ﬁnd that the reduction of the certiﬁed radius is inﬂuenced by two factors. The ﬁrst one is the
approximation error ϵ between the surrogate transformation and the real semantic transformation.
The second one the ratio A is about the norm of the Jacobian matrix for some layers of the surrogate
model which is also an inherent property of the semantic transformation itself and does not depend
on the target classiﬁer."
EXPERIMENTS,0.23773584905660378,"5
EXPERIMENTS"
EXPERIMENTAL SETUP AND EVALUATION METRICS,0.23962264150943396,"5.1
EXPERIMENTAL SETUP AND EVALUATION METRICS"
EXPERIMENTAL SETUP AND EVALUATION METRICS,0.24150943396226415,"In this section, we conduct extensive experiments to show the effectiveness of our GSmooth on
various types of semantic transformations. We use MNIST, CIFAR-10 and CIFAR-100 (Krizhevsky
et al., 2009) datasets to verify our methods. We train a resnet with 110 layers (He et al., 2016a)
from scratch. Similar to prior works, we apply moderate data augmentation (Cohen et al., 2019)
to improve the generalization of the classiﬁer. For the surrogate image-to-image translation model
for simulating semantic transformations, we adopt the U-Net architecture (Ronneberger et al., 2015)
for ˜H(·) layers and several simple convolutional or linear layers for ˜F1(·) and ˜F2(·). All models
are trained using Adam optimizer with an initial learning rate of 0.001 that decays every 50 epochs
until convergence. The algorithms for calculating M ∗and other details in our experiments are listed
in Appendix B due to limited space. The evaluation metric is the certiﬁed accuracy, which is the
percentage of samples that are correctly classiﬁed and has a larger certiﬁed radius than the given
range. We use ∥α∥to indicate the preset certiﬁed radius."
MAIN RESULTS,0.24339622641509434,"5.2
MAIN RESULTS"
MAIN RESULTS,0.24528301886792453,"To demonstrate the effectiveness of our GSmooth on certifying complex semantic transforma-
tions, we measure the certiﬁed correct accuracy for different semantic transformations on differ-
ent datasets in Table 1. We compare the results of our GSmooth with several baselines, including
randomized smoothing for some simple semantic transformation of TSS (Li et al., 2021) and Indi-
vSPT/distSPT (Fischer et al., 2020), and our GSmooth is a natural and powerful extension of their
methods. We also compare our method with the deterministic certiﬁcation approaches, including
DeepG (Balunovi´c et al., 2019) that uses linear relaxations similar to Wong & Kolter (2018), Inter-
val (Singh et al., 2019) that is based on interval bound propagation, VeriVis (Pei et al., 2017) that
enumerates all possible outcomes for semantic transformations with ﬁnite values of parameters, and
Semanify-NN (Mohapatra et al., 2020) which uses a new preprocessing layer to turn the problem
into a ℓp norm certiﬁcation."
MAIN RESULTS,0.24716981132075472,"We have the following observations for the experimental results. First, only our method achieves
non-zero accuracy on certifying some complex semantic transformations and the results verify our
Theorem 2. This is a breakthrough that greatly extends the boundary of randomized smoothing
based methods. Second, we see the performance of GSmooth is similar to the state-of-the-art ran-
domized smoothing approaches like TSS on several simple semantic transformations like Gaussian"
MAIN RESULTS,0.2490566037735849,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.2509433962264151,"Cert Acc(%)
Type
Dataset
Attack Range"
MAIN RESULTS,0.2528301886792453,"Certiﬁed Accuracy(%)
GSmooth TSS DeepG Interval VeriVis Semanify- IndivSPT/"
MAIN RESULTS,0.25471698113207547,"(Ours)
NN
distSPT"
MAIN RESULTS,0.25660377358490566,"Gaussian Blur
Additive
MNIST
∥α∥2 < 6
91.0
90.6
–
–
–
–
–
CIFAR-10
∥α∥2 < 4
67.4
63.6
–
–
–
–
–
CIFAR-100
22.1
21.0
–
–
–
–
–"
MAIN RESULTS,0.25849056603773585,"Translation
Additive
MNIST
∥α∥2 < 8
98.7
99.6
0.0
0.0
98.8
98.8
99.6
CIFAR-10
∥α∥2 < 20
82.2
80.8
0.0
0.0
65.0
65.0
78.8
CIFAR-100
42.2
41.3
–
–
24.2
24.2
–"
MAIN RESULTS,0.26037735849056604,"Brightness, Contrast Resolvable
MNIST
∥α∥∞< 0.5
97.7
97.6
⩽0.4
0.0
–
⩽74
–
CIFAR-10
∥α∥∞< 0.4
82.5
82.4
0.0
0.0
–
–
–
CIFAR-100
42.3
41.4
0.0
0.0
–
–
–"
MAIN RESULTS,0.2622641509433962,"Rotation
Non-resolvable
MNIST
∥α∥2 < 50◦
95.7
97.4
⩽85.8
⩽6.0
–
⩽92.48
⩽76
CIFAR-10
∥α∥2 < 10◦
64.6
70.6
62.5
20.2
–
⩽49.37
⩽34
CIFAR-100
33.2
36.7
0.0
0.0
–
⩽21.7
⩽18"
MAIN RESULTS,0.2641509433962264,"Scaling
Non-resolvable
MNIST
∥α∥2 < 0.3
95.9
97.2
85.0
16.4
–
–
–
CIFAR-10
∥α∥2 < 0.3
54.3
58.8
0.0
0.0
–
–
–
CIFAR-100
31.2
37.8
0.0
0.0
–
–
–"
MAIN RESULTS,0.2660377358490566,"Rotational Blur
Non-resolvable
MNIST
∥α∥2 < 10
95.9
–
–
–
–
–
–
CIFAR-10
∥α∥2 < 10
39.7
–
–
–
–
–
–
CIFAR-100
27.2
–
–
–
–
–
–"
MAIN RESULTS,0.2679245283018868,"Defocus Blur
Non-resolvable
MNIST
∥α∥2 < 5
89.2
–
–
–
–
–
–
CIFAR-10
∥α∥2 < 5
25.0
–
–
–
–
–
–
CIFAR-100
13.1
–
–
–
–
–
–"
MAIN RESULTS,0.269811320754717,"Zoom Blur
Non-resolvable
MNIST
∥α∥2 < 0.5
93.9
–
–
–
–
–
–
CIFAR-10
∥α∥2 < 0.5
44.6
–
–
–
–
–
–
CIFAR-100
14.2
–
–
–
–
–
–"
MAIN RESULTS,0.27169811320754716,"Pixelate
Non-resolvable
MNIST
∥α∥2 < 0.5
87.1
–
–
–
–
–
–
CIFAR-10
∥α∥2 < 0.5
45.3
–
–
–
–
–
–
CIFAR-100
30.2
–
–
–
–
–
–"
MAIN RESULTS,0.27358490566037735,"Table 1: Our main results of certiﬁcation accuracy on several datasets and multiple types of semantic
transformations. – or 0.0% means the method fails to certify this type of semantic transformation."
MAIN RESULTS,0.27547169811320754,"Figure 2: Results of ablation experiments on the inﬂuence of smoothing distribution for zoomed
blur on CIFAR-10 dataset. The horizontal axis ∥α∥2 is the certiﬁed raidus."
MAIN RESULTS,0.27735849056603773,"blur, translation. This is a natural result since our method works similarly for resolvable transfor-
mations. For two speciﬁc non-resolvable transformations, i.e. rotation and scaling, our accuracy
is slightly lower. The possible reason is that in TSS (Li et al., 2021) they derive more elaborate
Lipschitz bound for rotation which is better than us. Third, those inherently non-resolvable transfor-
mations like image blurring (except Gaussian blur) and pixelate are more difﬁcult than resolvable or
approximately resolvable (rotation) transformations. Thus their certiﬁed accuracy is also lower."
ABLATION STUDY,0.2792452830188679,"5.3
ABLATION STUDY"
ABLATION STUDY,0.2811320754716981,"Ablation study on the inﬂuence of noise distribution. The choice of noise distribution is important
for randomized smoothing based methods. Since our GSmooth could certify different types of
semantic transformations that exhibit different properties. Understanding the inﬂuence of different
smoothing distributions for different semantic transformations is necessary. We choose (folded)
Gaussian, uniform, and exponential distribution and compare the certiﬁed accuracy on zoom blur
transformation for both CIFAR-10 and CIFAR-100 datasets. As shown in Fig. 2, We found that"
ABLATION STUDY,0.2830188679245283,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.2849056603773585,"Cert Acc
CIFAR-10
CIFAR-100"
ABLATION STUDY,0.28679245283018867,Rotational Blur
ABLATION STUDY,0.28867924528301886,"σ1
σ2
0.05
0.10
0.15
0.25
σ1
σ2
0.05
0.10
0.15
0.20"
ABLATION STUDY,0.29056603773584905,"0.1
44.3
46.4
35.5
16.1
0.1
23.1
26.2
17.2
10.4
0.25
45.7
47.1
38.3
18.9
0.25
23.2
27.2
20.3
11.3
0.5
46.5
48.4
38.8
18.3
0.5
22.2
26.1
18.6
11.8
0.75
42.1
45.5
36.6
17.0
0.75
24.0
25.5
18.3
13.2"
ABLATION STUDY,0.29245283018867924,"Table 2: Results of ablation study on the inﬂuence of standard deviation of smoothing distributions,
i.e. transformation noise σ1 and augmented noise σ2 for certiﬁcation accuracy on rotational blur."
ABLATION STUDY,0.2943396226415094,"Figure 3: Visualization of difference between the augmented noise in the semantic layers and the
noise on raw images. Left: original images from CIFAR-10. Middle: images with augmented noise
of σ2 = 0.2. Right: images with additive Gaussian noise σ = 0.2."
ABLATION STUDY,0.2962264150943396,"the impact of smoothing distributions depends on datasets. Uniform distribution is better for small
radius certiﬁcation while exponential distribution is more suitable for certifying large radius on
average."
ABLATION STUDY,0.2981132075471698,"Ablation study on the inﬂuence of noise variance for certiﬁcation. Since our GSmooth contains
two different variances for controlling the resolvable part and the residual part for a non-resolvable
semantic transformation. Here we investigate the effect of different noise variance on the certiﬁed
accuracy. The results are shown in Table 2. We found that using medium transformation noise and
augmented noise achieves the best certiﬁed accuracy. The fact is consistent with results in (Cohen
et al., 2019). An explanation is that there is a trade-off since higher the noise variance decreases the
coefﬁcient M ∗but it might also degrade the clean accuracy."
ABLATION STUDY,0.3,"Visulization experiments: comparsion between augmented semantic noise and image noise.
Our GSmooth needs to add a new noise in the semantic layers of the surrogate model. Here we
compare the difference between these two types of noise and visualize them. We random sample
images from CIFAR-10 and add gaussian noise with σ = 0.2 to both the semantic layer of the
surrogate model simulating zoomed blur transformation and raw images. Results are shown in Fig.
3. Both two types of noise severely blur the images. But we found that the augmented semantic
noise is more placid which can therefore keep the holistic semantic features better, e.g., shapes."
CONCLUSIONS,0.3018867924528302,"6
CONCLUSIONS"
CONCLUSIONS,0.30377358490566037,"In this paper, we proposed a generalized randomized smoothing framework (GSmooth) for certifying
robustness against general semantic transformations. We proposed a novel idea that using a surrogate
neural network to ﬁt semantic transformations. Then we prove tight certiﬁed robustness bound for
the surrogate model and use it for certifying semantic transformations. Extensive experiments verify
the effectiveness of our method and we achieved state-of-the-art performance on various types of
semantic transformations. In the future, we plan to extend our method to real-world scenarios on
more diverse semantic transformations."
CONCLUSIONS,0.30566037735849055,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.30754716981132074,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.30943396226415093,"We ensure the reproducibility of our paper from three aspects. (1) Experiment: The implementation
of our experiment is described in Sec. 5.1. Ablation study for our experiments is in Sec. 5.3. Further
details are in Appendix B. (2) Code: Our code is included in supplementary materials. (3) Theory
and Method: A complete proof of the theoretical results described is provided in Appendix A."
ETHICS STATEMENT,0.3113207547169811,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.3132075471698113,"Machine learning models are easily attacked by adversarial examples and semantic transformations.
Thus it is fundamental problem to develop certiﬁed robust machine learning methods. This paper
proposed GSmooth to certify against semantic transformations. It may promote the development of
safe and reliable machine learning models in the future."
REFERENCES,0.3150943396226415,REFERENCES
REFERENCES,0.3169811320754717,"Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning (ICML), pp. 274–283, 2018."
REFERENCES,0.31886792452830187,"Mislav Balunovi´c, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin Vechev. Certify-
ing geometric robustness of neural networks. Advances in Neural Information Processing Systems
32, 2019."
REFERENCES,0.32075471698113206,"Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. Evasion attacks against machine learning at test time. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases, pp. 387–402, 2013."
REFERENCES,0.32264150943396225,"Tom B Brown, Dandelion Man´e, Aurko Roy, Mart´ın Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017."
REFERENCES,0.32452830188679244,"Dan A Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise Rebufﬁ, Andras Gyorgy, Timothy
Mann, and Sven Gowal. Defending against image corruptions through adversarial augmentations.
arXiv preprint arXiv:2104.01086, 2021."
REFERENCES,0.3264150943396226,"Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019."
REFERENCES,0.3283018867924528,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113–123, 2019."
REFERENCES,0.330188679245283,"Foivos I Diakogiannis, Franc¸ois Waldner, Peter Caccetta, and Chen Wu. Resunet-a: A deep learning
framework for semantic segmentation of remotely sensed data. ISPRS Journal of Photogrammetry
and Remote Sensing, 162:94–114, 2020."
REFERENCES,0.3320754716981132,"Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation sufﬁce: Fooling cnns with simple transformations. 2018."
REFERENCES,0.3339622641509434,"Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Ex-
ploring the landscape of spatial robustness. In International Conference on Machine Learning,
pp. 1802–1811. PMLR, 2019."
REFERENCES,0.33584905660377357,"Marc Fischer, Maximilian Baader, and Martin Vechev. Certiﬁed defense to image transformations
via randomized smoothing. arXiv preprint arXiv:2002.12463, 2020."
REFERENCES,0.33773584905660375,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.33962264150943394,"Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715, 2018."
REFERENCES,0.34150943396226413,Under review as a conference paper at ICLR 2022
REFERENCES,0.3433962264150943,"Jamie Hayes. Extensions and limitations of randomized smoothing for robustness guarantees. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Work-
shops, pp. 786–787, 2020."
REFERENCES,0.3452830188679245,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016a."
REFERENCES,0.3471698113207547,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b."
REFERENCES,0.3490566037735849,"Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019."
REFERENCES,0.35094339622641507,"Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781, 2019."
REFERENCES,0.35283018867924526,"Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020."
REFERENCES,0.35471698113207545,"Pengfei Jing, Qiyi Tang, Yuefeng Du, Lei Xue, Xiapu Luo, Ting Wang, Sen Nie, and Shi Wu. Too
good to be safe: Tricking lane detection in autonomous driving with crafted perturbations. In 30th
{USENIX} Security Symposium ({USENIX} Security 21), 2021."
REFERENCES,0.35660377358490564,Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.3584905660377358,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015."
REFERENCES,0.360377358490566,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656–672. IEEE, 2019."
REFERENCES,0.3622641509433962,"Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Bhavya Kailkhura, Tao Xie, Ce Zhang, and
Bo Li. Tss: Transformation-speciﬁc smoothing for robustness certiﬁcation, 2021."
REFERENCES,0.3641509433962264,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep resid-
ual networks for single image super-resolution. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition workshops, pp. 136–144, 2017."
REFERENCES,0.3660377358490566,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.36792452830188677,"Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel.
Towards verifying
robustness of neural networks against a family of semantic perturbations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 244–252, 2020."
REFERENCES,0.36981132075471695,"Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Towards practical veriﬁcation of machine
learning: The case of computer vision systems. arXiv preprint arXiv:1712.01785, 2017."
REFERENCES,0.37169811320754714,"Alexander Robey, Hamed Hassani, and George J Pappas. Model-based robust deep learning: Gen-
eralizing to natural, out-of-distribution data. arXiv preprint arXiv:2005.10247, 2020."
REFERENCES,0.37358490566037733,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.3754716981132076,"Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck.
Provably robust deep learning via adversarially trained smoothed classiﬁers.
arXiv
preprint arXiv:1906.04584, 2019."
REFERENCES,0.37735849056603776,Under review as a conference paper at ICLR 2022
REFERENCES,0.37924528301886795,"Gagandeep Singh, Timon Gehr, Markus P¨uschel, and Martin Vechev. An abstract domain for cer-
tifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1–30,
2019."
REFERENCES,0.38113207547169814,"Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian
Tramer, Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors.
In 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018."
REFERENCES,0.38301886792452833,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.3849056603773585,"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.3867924528301887,"Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Gao Huang, and Cheng Wu. Implicit semantic
data augmentation for deep networks. Advances in Neural Information Processing Systems, 32:
12635–12644, 2019."
REFERENCES,0.3886792452830189,"Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR,
2018."
REFERENCES,0.3905660377358491,"Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3–19, 2018."
REFERENCES,0.39245283018867927,"Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. arXiv preprint arXiv:1801.02612, 2018."
REFERENCES,0.39433962264150946,"Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693–
10705. PMLR, 2020."
REFERENCES,0.39622641509433965,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472–7482. PMLR, 2019a."
REFERENCES,0.39811320754716983,"Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efﬁcient training of veriﬁably robust neural networks.
arXiv preprint arXiv:1906.06316, 2019b."
REFERENCES,0.4,"Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223–2232, 2017."
REFERENCES,0.4018867924528302,Under review as a conference paper at ICLR 2022
REFERENCES,0.4037735849056604,"A
PROOF OF THEOREMS"
REFERENCES,0.4056603773584906,"In this section, we will provide detailed proofs of theorems in our paper."
REFERENCES,0.4075471698113208,"First, we restate the theorem of randomized smoothing for additive noise and binary classiﬁers
f(·) : Rn →[0, 1],
G(x) = Eθ∼g[f(x + θ)]
(18)
Theorem 4. Let f(x) be any classiﬁer and G(x) be the smoothed classiﬁer deﬁned in Eq. (18), if
G(x) < 1"
REFERENCES,0.40943396226415096,"2, then G(x + δ) < 1"
FOR ANY,0.41132075471698115,2 for any
FOR ANY,0.41320754716981134,"∥δ∥<
Z
1
2 G(x)"
FOR ANY,0.41509433962264153,"1
Φ(p)dp
(19)"
FOR ANY,0.4169811320754717,where Φ(·) is a function about smoothing distribution deﬁned in Eq. (2).
FOR ANY,0.4188679245283019,Proof. We ﬁrst calculate the gradient of the smoothed classiﬁer
FOR ANY,0.4207547169811321,"∇G(x)
=
∂
∂x Z"
FOR ANY,0.4226415094339623,"Rn f(x + θ)g(θ)dθ =
Z"
FOR ANY,0.42452830188679247,"Rn
∂
∂xf(x + θ)g(θ)dθ =
Z"
FOR ANY,0.42641509433962266,"Rn
∂
∂θf(x + θ)g(θ)dθ"
FOR ANY,0.42830188679245285,"Then we multiple any vector with unit norm u ∈Bn(1) = {u : ∥u∥= 1, u ∈Rn},"
FOR ANY,0.43018867924528303,"|⟨∇G(x), u⟩|
= Z"
FOR ANY,0.4320754716981132,"Rn
∂
∂θf(x + θ)g(θ)dθ, u = Z Rn ∂"
FOR ANY,0.4339622641509434,"∂θf(x + θ), u

g(θ)dθ =  X i Z"
FOR ANY,0.4358490566037736,"Rn
∂f(x + θ)"
FOR ANY,0.4377358490566038,"∂θi
uig(θ)dθ  =  X i Z Rn−1 Z +∞ −∞"
FOR ANY,0.439622641509434,∂f(x + θ)
FOR ANY,0.44150943396226416,"∂θi
uig(θ)dθi  Y"
FOR ANY,0.44339622641509435,"j̸=i
dθj  = −
X i Z Rn−1 Z +∞"
FOR ANY,0.44528301886792454,"−∞
f(x + θ)ui
∂g(θ)"
FOR ANY,0.44716981132075473,"∂θi
dθi  Y"
FOR ANY,0.4490566037735849,"j̸=i
dθj  =  X i Z"
FOR ANY,0.4509433962264151,Rn f(x + θ)∂g(θ)
FOR ANY,0.4528301886792453,"∂θi
uidθ  = Z"
FOR ANY,0.4547169811320755,"Rn f(x + θ)⟨∇g(θ), u⟩dθ = Z"
FOR ANY,0.45660377358490567,"Rn f(x + θ)g(θ)⟨∇ψ(θ), u⟩dθ"
FOR ANY,0.45849056603773586,"=
|Eθ∼g[f(x + θ)⟨∇ψ(θ), u⟩]|
(20)"
FOR ANY,0.46037735849056605,"To bound the gradient of the smoothed classiﬁer, we use the following inequality,"
FOR ANY,0.46226415094339623,"|⟨∇G(x), u⟩| ⩽
sup
b
f: ˆ
G(x)=G(x)
Eθ∼g
h
ˆf(x + θ)⟨ψ(θ), u⟩
i
(21)"
FOR ANY,0.4641509433962264,"As shown in Yang et al. (2020), the optimal ˆf(x) achieves at,"
FOR ANY,0.4660377358490566,"ˆf(x + θ) =

1, if⟨u, ψ(θ)⟩> ϕ−1
u (G(x))
0, else
(22)"
FOR ANY,0.4679245283018868,"Under review as a conference paper at ICLR 2022 Then,"
FOR ANY,0.469811320754717,"Eθ∼g
h
ˆf(x + θ)⟨u, ψ(θ)⟩
i
=
E

γuI{γu > ϕ−1
u (G(x))}
"
FOR ANY,0.4716981132075472,"⩽
Φ(G(x))
(23)"
FOR ANY,0.47358490566037736,"which means that,
|⟨∇G(x), u⟩| ⩽Φ(G(x))
(24)"
FOR ANY,0.47547169811320755,"and this is true for all u ∈Bn(1), so we have,"
FOR ANY,0.47735849056603774,"max
u∈Bn(1)⟨∇G(x), u⟩⩽Φ(G(x))
(25)"
FOR ANY,0.47924528301886793,"Consider a path from ξt : [0, ∥δ∥] →Rd with ξ0 = x and ξ∥δ∥= x + δ and ξ′
t =
δ
∥δ∥, we have"
FOR ANY,0.4811320754716981,dG(ξt)
FOR ANY,0.4830188679245283,"dt
= ⟨∇G(ξt), u⟩⩽Φ(G(ξt)).
(26)"
FOR ANY,0.4849056603773585,"If the norm of δ satisﬁes that,"
FOR ANY,0.4867924528301887,"∥δ∥<
Z
1
2 G(x)"
FOR ANY,0.48867924528301887,"1
Φ(p)dp,
(27)"
FOR ANY,0.49056603773584906,"if right hand side exists, we name it ∥δ0∥=
R 1"
FOR ANY,0.49245283018867925,"2
G(x) 1/Φ(p)dp. Without loss of generality, we assume
that G(ξt) is increasing in t, then we cound calculate the minimal twhen G(ξt) increase to 1 2,"
FOR ANY,0.49433962264150944,"T =
Z
1
2 G(ξ0)"
FOR ANY,0.4962264150943396,"1
Φ(p)dp = ∥δ0∥.
(28)"
FOR ANY,0.4981132075471698,"By the generality of δ we have,"
FOR ANY,0.5,G(x + δ) < 1
FOR ANY,0.5018867924528302,"2
(29)"
FOR ANY,0.5037735849056604,for any δ < ∥δ0∥.
FOR ANY,0.5056603773584906,"This theorem is naturally extended to problems with p > 2 classes by considering the two top classes
which are G(x)A and G(x)B. This turns the problem into a binary classiﬁcation problem."
FOR ANY,0.5075471698113208,"A.1
THE PROOF OF THEOREM 1"
FOR ANY,0.5094339622641509,"Theorem 1. Let f(x) be any classiﬁer and G(x) be the smoothed classiﬁer deﬁned in Eq. (1), if
there exists a function M(·) , and the transformation τ(·, ·) satisﬁes that"
FOR ANY,0.5113207547169811,"∂γ(θ, ξ)"
FOR ANY,0.5132075471698113,"∂ξ
= ∂γ(θ, ξ)"
FOR ANY,0.5150943396226415,"∂θ
M(θ, ξ),"
FOR ANY,0.5169811320754717,"and there exists two constants pA , pB that satisﬁes that"
FOR ANY,0.5188679245283019,"G(x)A ⩾pA ⩾pB ⩾G(x)B,"
FOR ANY,0.5207547169811321,"then yA = arg maxi∈Y G(τ(ξ, x))i holds for any ∥ξ∥⩽R where"
FOR ANY,0.5226415094339623,"R =
1
2M ∗ Z pA pB"
FOR ANY,0.5245283018867924,"1
Φ(p)dp,
(30)"
FOR ANY,0.5264150943396226,"here M ∗= maxξ,θ||M(ξ, θ)||."
FOR ANY,0.5283018867924528,Under review as a conference paper at ICLR 2022
FOR ANY,0.530188679245283,"Proof. WLOG, we only prove it for binary cases that f(·) : Rn →[0, 1]."
FOR ANY,0.5320754716981132,"∇ξG(τ(γ(θ, ξ), x))"
FOR ANY,0.5339622641509434,"=
Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.5358490566037736,"∂τ(γ(θ, ξ), x)
· ∂τ(γ(θ, ξ), x)"
FOR ANY,0.5377358490566038,"∂ξ
g(θ)dθ"
FOR ANY,0.539622641509434,"=
Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.5415094339622641,"∂τ(γ(θ, ξ), x)
· ∂τ(γ(θ, ξ), x)"
FOR ANY,0.5433962264150943,"∂γ(θ, ξ)
∂γ(θ, ξ)"
FOR ANY,0.5452830188679245,"∂ξ
g(θ)dθ"
FOR ANY,0.5471698113207547,"=
Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.5490566037735849,"∂τ(γ(θ, ξ), x)
· ∂τ(γ(θ, ξ), x)"
FOR ANY,0.5509433962264151,"∂γ(θ, ξ)
∂γ(θ, ξ)"
FOR ANY,0.5528301886792453,"∂θ
M(θ, ξ)g(θ)dθ"
FOR ANY,0.5547169811320755,"=
Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.5566037735849056,"∂θ
M(θ, ξ)g(θ)dθ (31)"
FOR ANY,0.5584905660377358,"For u ∈Rd and ∥u∥= 1, we have:"
FOR ANY,0.560377358490566,"|⟨∇ξG(γ(ξ, x)), u⟩|
="
FOR ANY,0.5622641509433962,"Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.5641509433962264,"∂θ
M(θ, ξ)ug(θ)dθ

(32)"
FOR ANY,0.5660377358490566,"⩽
M ∗max
∥v∥=1 "
FOR ANY,0.5679245283018868,"Z ∂f(τ(γ(θ, ξ), x))"
FOR ANY,0.569811320754717,"∂θ
vg(θ)dθ

(33)"
FOR ANY,0.5716981132075472,"⩽
M ∗max
∥v∥=1 "
FOR ANY,0.5735849056603773,"Z
f(τ(γ(θ, ξ), x))∂g(θ)"
FOR ANY,0.5754716981132075,"∂θ vdθ

(34) (35)"
FOR ANY,0.5773584905660377,"here M ∗= maxθ,ξ ∥M(θ, ξ)∥and we assume that g(θ) = exp(−ψ(θ)):"
FOR ANY,0.5792452830188679,"|⟨∇ξG(γ(ξ, x)), u⟩|
⩽
M ∗max
∥v∥=1 "
FOR ANY,0.5811320754716981,"Z
f(τ(γ(θ, ξ), x))∂g(θ)"
FOR ANY,0.5830188679245283,"∂θ vdθ

(36)"
FOR ANY,0.5849056603773585,"=
M ∗max
∥v∥=1 "
FOR ANY,0.5867924528301887,"Z
f(τ(γ(θ, ξ), x))g(θ)∇ψ(θ)vdθ

(37)"
FOR ANY,0.5886792452830188,"=
M ∗max
∥v∥=1 |Eθ∼g [f(τ(γ(θ, ξ), x))⟨∇ψ(θ), v⟩]|
(38)"
FOR ANY,0.590566037735849,"⩽
M ∗max
∥v∥=1
sup
b
f: ˆ
G(τ(ξ,x))=G(τ(ξ,x))
Eθ∼g[ ˆf(τ(γ(θ, ξ), x))⟨ψ(θ), u⟩](39)"
FOR ANY,0.5924528301886792,"Similar with Theorem 4, the optimal ˆf achieves at"
FOR ANY,0.5943396226415094,"ˆf(τ(γ(θ, ξ), x)) =

1, if⟨ψ(θ), u⟩> ϕ−1
u (G(τ(ξ, x))))
0, else
(40)"
FOR ANY,0.5962264150943396,"Then we have
|⟨∇ξG(τ(ξ, x)), u⟩| ⩽Φ(G(τ(ξ, x))).
(41)"
FOR ANY,0.5981132075471698,"Consider a path from ζt : [0, ∥δ∥] →Rd with ζ0 = x and ζ∥δ∥= τ(ξ, x) and ζ′
t =
δ
∥δ∥, we have"
FOR ANY,0.6,dG(ξt)
FOR ANY,0.6018867924528302,"dt
= ⟨∇G(ξt), u⟩⩽Φ(G(ξt)).
(42)"
FOR ANY,0.6037735849056604,The last part of proof is the same with Theorem 4.
FOR ANY,0.6056603773584905,"A.2
THE PROOF OF THEOREM 2"
FOR ANY,0.6075471698113207,"Theorem 2. Suppose f(x) is any classiﬁer and ˜G(˜x) is the smoothed classiﬁer deﬁned in Eq. (7),
if there exists pA , pB that satisﬁes that"
FOR ANY,0.6094339622641509,"˜G(˜x)A ⩾pA ⩾pB ⩾˜G(˜x)B,"
FOR ANY,0.6113207547169811,Under review as a conference paper at ICLR 2022
FOR ANY,0.6132075471698113,"then yA = arg maxi∈Y ˜G(˜τ(˜ξ, ˜x))i for any ∥ξ∥2 ⩽R where"
FOR ANY,0.6150943396226415,"R =
1
2M ∗ Z pA pB"
FOR ANY,0.6169811320754717,"1
Φ(p)dp,
(43)"
FOR ANY,0.6188679245283019,and the coefﬁcient M ∗is deﬁned as
FOR ANY,0.620754716981132,"M ∗= max
ξ,θ∈P s"
FOR ANY,0.6226415094339622,"1 +

∂F2(yξ)"
FOR ANY,0.6245283018867924,"∂ξ
−∂F1(θ) ∂θ  2"
FOR ANY,0.6264150943396226,"2
.
(44)"
FOR ANY,0.6283018867924528,"Proof. In this part, we will prove Theorem 2, which is the main result in this paper. WLOG, we
prove it for binary cases where f(·) : Rn →[0, 1]. First, we will calculate the gradient of ˜G(˜τ(˜ξ, ˜x))
to ˜ξ:"
FOR ANY,0.630188679245283,"∇˜ξ ˜G(˜τ(˜ξ, ˜x)) = ∇˜ξE˜θ∼˜g(˜θ)[ ˜f(˜τ(˜θ, ˜τ(˜ξ, ˜x)))].
(45)"
FOR ANY,0.6320754716981132,We expand the expectation into integral and use chain rule to see that
FOR ANY,0.6339622641509434,"∇˜ξ ˜G(˜τ(˜ξ, ˜x)) =
Z"
FOR ANY,0.6358490566037736,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ))"
FOR ANY,0.6377358490566037,"∂˜τ(˜θ, ˜yξ)
· ∂˜τ(˜θ, ˜yξ)"
FOR ANY,0.6396226415094339,"∂˜yξ
· ∂˜yξ"
FOR ANY,0.6415094339622641,"∂˜ξ
˜g(˜θ)d˜θ.
(46)"
FOR ANY,0.6433962264150943,"The key step is to eliminate the gradient of ∂˜
f(˜τ(˜θ,˜yξ))"
FOR ANY,0.6452830188679245,"∂˜τ(˜θ,˜yξ)
and replace it with ∂˜
f(˜τ(˜θ,˜yξ))"
FOR ANY,0.6471698113207547,"∂˜θ
. Since:"
FOR ANY,0.6490566037735849,"∂˜τ(˜θ, ˜yξ)"
FOR ANY,0.6509433962264151,"∂˜yξ
=

σ′
1(z′
θ)
σ′
2(zθ)"
FOR ANY,0.6528301886792452," F ′
21(y′
ξ)
F ′
22(yξ)"
FOR ANY,0.6547169811320754,"
,
(47) ∂˜yξ"
FOR ANY,0.6566037735849056,"∂˜ξ
=
H′
1(z′
ξ)
H′
2(zξ)"
FOR ANY,0.6584905660377358," 
Id
F ′
1(ξ)
In"
FOR ANY,0.660377358490566,"
.
(48)"
FOR ANY,0.6622641509433962,We have:
FOR ANY,0.6641509433962264,"∇˜ξ ˜G(˜τ(˜ξ, ˜x))
=
Z"
FOR ANY,0.6660377358490566,"Rn+d
∂˜F(˜τ(˜θ, ˜yξ))"
FOR ANY,0.6679245283018868,"∂˜τ(˜θ, ˜yξ)
·

H′
1(z′
θ)
H′
2(zθ)"
FOR ANY,0.6698113207547169," F ′
21(y′
ξ)
F ′
22(yξ) "
FOR ANY,0.6716981132075471,"H′
1(z′
ξ)
H′
2(zξ)"
FOR ANY,0.6735849056603773," 
Id
F ′
1(ξ)
In"
FOR ANY,0.6754716981132075,"
˜g(˜θ)d˜θ
(49) =
Z"
FOR ANY,0.6773584905660377,"Rn+d
∂˜F(˜τ(˜θ, ˜yξ))"
FOR ANY,0.6792452830188679,"∂˜τ(˜θ, ˜yξ)
·

H′
1(z′
θ)
H′
2(zθ)"
FOR ANY,0.6811320754716981," 
Id
F ′
1(θ)
In"
FOR ANY,0.6830188679245283," 
Id
−F ′
1(θ)
In "
FOR ANY,0.6849056603773584,"F ′
21(y′
ξ)
F ′
22(yξ)"
FOR ANY,0.6867924528301886," H′
1(z′
ξ)
H′
2(zξ)"
FOR ANY,0.6886792452830188," 
Id
F ′
1(ξ)
In"
FOR ANY,0.690566037735849,"
˜g(˜θ)d˜θ
(50) =
Z"
FOR ANY,0.6924528301886792,"Rn+d
∂˜F(˜τ(˜θ, ˜yξ)) ∂˜θ"
FOR ANY,0.6943396226415094,"
Id
−F ′
1(θ)
In"
FOR ANY,0.6962264150943396," F ′
21(y′
ξ)
F ′
22(yξ) "
FOR ANY,0.6981132075471698,"H′
1(z′
ξ)
H′
2(zξ)"
FOR ANY,0.7," 
Id
F ′
1(ξ)
In"
FOR ANY,0.7018867924528301,"
˜g(˜θ)d˜θ
(51) =
Z"
FOR ANY,0.7037735849056603,"Rn+d
∂˜F(˜τ(˜θ, ˜yξ))"
FOR ANY,0.7056603773584905,"∂˜θ
˜
M(˜ξ, ˜θ)g(˜θ)d˜θ,
(52)"
FOR ANY,0.7075471698113207,here we deﬁne
FOR ANY,0.7094339622641509,"˜
M(˜ξ, ˜θ) ≜

Id
−F ′
1(θ)
In"
FOR ANY,0.7113207547169811," F ′
21(y′
ξ)
F ′
22(yξ)"
FOR ANY,0.7132075471698113," H′
1(z′
ξ)
H′
2(zξ)"
FOR ANY,0.7150943396226415," 
Id
F ′
1(ξ)
In"
FOR ANY,0.7169811320754716,"
(53)"
FOR ANY,0.7188679245283018,"We consider the Unit enlargement, which means:"
FOR ANY,0.720754716981132,Under review as a conference paper at ICLR 2022
FOR ANY,0.7226415094339622,"H1(z′) = z′, F21(x′) = x′
(54) thus:"
FOR ANY,0.7245283018867924,"˜
M(˜ξ, ˜θ) =

Id
Od×n
F ′
21(yξ) −F ′
1(θ)
F ′
22(yξ)H′
2(zξ)"
FOR ANY,0.7264150943396226,"
(55)"
FOR ANY,0.7283018867924528,"Since θ′ is the virtual parameter introduced, which can be taken as 0 in case of actual disturbance.
Thus we only need to consider the projection of ∇˜ξ ˜G(˜yξ) in the space of ξ. Thus we set"
FOR ANY,0.730188679245283,"˜u =

u
On×1"
FOR ANY,0.7320754716981132,"
,
(56)"
FOR ANY,0.7339622641509433,here u ∈Rd and ∥u∥= 1. Assume
FOR ANY,0.7358490566037735,"˜g(˜θ) = exp(−˜ψ(˜θ))
(57)"
FOR ANY,0.7377358490566037,∂˜g(˜θ)
FOR ANY,0.7396226415094339,"∂˜θ
= −˜g(˜θ) · ∇˜ψ(˜θ).
(58)"
FOR ANY,0.7415094339622641,And we have
FOR ANY,0.7433962264150943,"⟨∇˜ξ ˜G(˜yξ), ˜u⟩

=  Z"
FOR ANY,0.7452830188679245,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ))"
FOR ANY,0.7471698113207547,"∂˜θ
˜
M(˜ξ, ˜θ)˜u˜g(˜θ)d˜θ (59) =  Z"
FOR ANY,0.7490566037735849,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ)) ∂˜θ"
FOR ANY,0.7509433962264151,"
Id,
Od×n
F ′
22(yξ) −F ′
1(θ),
On×n"
FOR ANY,0.7528301886792453,"
˜u˜g(˜θ)d˜θ (60) =  Z"
FOR ANY,0.7547169811320755,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ))"
FOR ANY,0.7566037735849057,"∂˜θ
M(ξ, θ)˜u˜g(˜θ)d˜θ (61)"
FOR ANY,0.7584905660377359,"⩽
M ∗max
∥˜v∥2=1  Z"
FOR ANY,0.7603773584905661,"Rn+d
∂˜f(˜τ(˜θ, ˜yξ))"
FOR ANY,0.7622641509433963,"∂˜θ
˜v˜g(˜θ)d˜θ (62)"
FOR ANY,0.7641509433962265,"=
M ∗max
∥˜v∥2=1  Z"
FOR ANY,0.7660377358490567,"Rn+d
˜f(˜τ(˜θ, ˜yξ))∂˜g(˜θ)"
FOR ANY,0.7679245283018868,"∂˜θ
˜vd˜θ (63)"
FOR ANY,0.769811320754717,"=
M ∗max
∥˜v∥2=1  Z"
FOR ANY,0.7716981132075472,"Rn+d
˜f(˜τ(˜θ, ˜yξ))˜g(˜θ)∇˜ψ(˜θ)˜vd˜θ

(64)"
FOR ANY,0.7735849056603774,"=
M ∗max
∥˜v∥2=1"
FOR ANY,0.7754716981132076,"Eθ∼g
h
˜f(˜τ(˜θ, ˜τ(˜ξ, x)))⟨∇˜ψ(˜θ), ˜v⟩
i
(65)"
FOR ANY,0.7773584905660378,"We bound the right hand side by
⟨∇˜ξ ˜G(˜yξ), ˜u⟩
 ⩽
sup
b
f: ˆ
G(τ(ξ,x))=G(τ(ξ,x))
M ∗max
∥˜v∥2=1"
FOR ANY,0.779245283018868,"Eθ∼g
h
ˆf(˜τ(˜θ, ˜τ(˜ξ, x)))⟨∇˜ψ(˜θ), ˜v⟩
i
(66)"
FOR ANY,0.7811320754716982,"and the optimal ˆf is as follows,"
FOR ANY,0.7830188679245284,"ˆf(˜τ(˜θ, ˜τ(˜ξ, ˜x)) =

1, if⟨˜ψ(˜θ), ˜u⟩> ϕ−1
u ( ˜G(˜τ(˜ξ, ˜x))))
0, else
(67) here"
FOR ANY,0.7849056603773585,"M(ξ, θ) =

Id,
Od×n
F ′
22(yξ) −F ′
1(θ),
On×n  (68)"
FOR ANY,0.7867924528301887,Under review as a conference paper at ICLR 2022
FOR ANY,0.7886792452830189,and M ∗is
FOR ANY,0.7905660377358491,"M ∗= max
ξ,θ∈P "
FOR ANY,0.7924528301886793,"
Id,
Od×n
F ′
22(yξ) −F ′
1(θ),
On×n"
FOR ANY,0.7943396226415095,"
2"
FOR ANY,0.7962264150943397,"= max
ξ,θ∈P "
FOR ANY,0.7981132075471699,"
Id
∂F22(yξ)"
FOR ANY,0.8,"∂ξ
−∂F1(θ) ∂θ"
FOR ANY,0.8018867924528302,"
2"
FOR ANY,0.8037735849056604,"= max
ξ,θ∈P s 1 +"
FOR ANY,0.8056603773584906,∂F22(yξ)
FOR ANY,0.8075471698113208,"∂ξ
−∂F1(θ) ∂θ  2 2
. (69)"
FOR ANY,0.809433962264151,"Notice that here F22(·) is the same as F2(·) the notations in the main text. Then we could apply the
techniques used in Theorem 1 and Theorem 4, we have:"
FOR ANY,0.8113207547169812,"R =
1
2M ∗ Z pA pB"
FOR ANY,0.8132075471698114,"1
Φ(p)dp,
(70)"
FOR ANY,0.8150943396226416,Thus we have proven this Theorem.
FOR ANY,0.8169811320754717,"A.3
THE PROOF OF THEOREM 3"
FOR ANY,0.8188679245283019,Theorem 3. Suppose the simulation of the semantic transformation has an small enough error
FOR ANY,0.8207547169811321,"∥˜τ(˜ξ, ˜x) −τ(˜ξ, ˜x)∥< ε,"
FOR ANY,0.8226415094339623,"Then there exists a constant ratio A = A(∥F ′
1(˜ξ)∥, ∥F ′
2(˜yξ)∥, ∥F ′
2(˜zξ)∥) > 0 does not depend on
the target classiﬁer, we have the certiﬁed radius for the real semantic transformation satisﬁes that"
FOR ANY,0.8245283018867925,Rr > R(1 −Aε)
FOR ANY,0.8264150943396227,where R is the certiﬁed radius for surrogate the neural network in Theorem 2 and
FOR ANY,0.8283018867924529,"R =
1
2M ∗ Z pA pB"
FOR ANY,0.8301886792452831,"1
Φ(p)dp."
FOR ANY,0.8320754716981132,Proof. We set
FOR ANY,0.8339622641509434,"˜u =

u
On×1"
FOR ANY,0.8358490566037736,"
,
(71)"
FOR ANY,0.8377358490566038,here u ∈Rd and ∥u∥= 1. Then we have
FOR ANY,0.839622641509434,"D
∇˜ξ ˜G(˜τ(˜ξ, ˜x)) −∇˜ξ ˜G(¯τ(˜ξ, ˜x)), ˜u
E
=
Z  
∂˜f(˜τ(˜θ, ˜τ(˜ξ, ˜x)))"
FOR ANY,0.8415094339622642,"∂˜ξ
−∂˜f(˜τ(˜θ, ¯τ(˜ξ, ˜x))) ∂˜ξ !"
FOR ANY,0.8433962264150944,˜u˜g(˜θ)d˜θ
FOR ANY,0.8452830188679246,"=
Z 
˜τ(˜ξ, ˜x) −¯τ(˜ξ, ˜x)
 ∂2 ˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.8471698113207548,"∂˜ξ∂ˆτ
˜u˜g(˜θ)d˜θ (72)"
FOR ANY,0.8490566037735849,"Set L = ∂2 ˜
f(˜τ(˜θ,¯τ(˜ξ,˜x)))"
FOR ANY,0.8509433962264151,"∂˜ξ∂ˆτ
, we have L = ∂ ∂˜ξ"
FOR ANY,0.8528301886792453,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x))) ∂ˆτ ! = ∂ ∂˜ξ"
FOR ANY,0.8547169811320755,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.8566037735849057,"∂˜τ(˜θ, ˆτ(˜ξ, ˜x))
∂˜τ(˜θ, ˆτ(˜ξ, ˜x)) ∂ˆτ !"
FOR ANY,0.8584905660377359,".
(73)"
FOR ANY,0.8603773584905661,"Set y˜ξ = ˜τ(˜θ, ˆτ(˜ξ, ˜x)), we have:"
FOR ANY,0.8622641509433963,Under review as a conference paper at ICLR 2022
FOR ANY,0.8641509433962264,"∂˜τ(˜θ, ˆτ(˜ξ, ˜x))"
FOR ANY,0.8660377358490566,"∂ˆτ
= ∂˜H( ˜F1(˜θ) + ˜F2(ˆτ(˜ξ, ˜x))) ∂ˆτ"
FOR ANY,0.8679245283018868,"= ˜H′ 
˜F1(˜θ) + ˜F2(ˆτ(˜ξ, ˜x))
 ∂˜F2(ˆτ(˜ξ, ˜x)) ∂ˆτ"
FOR ANY,0.869811320754717,"= ˜H′ 
˜F1(˜θ) + ˜F2(ˆτ(˜ξ, ˜x))
 
Id
F ′
1(θ)
In"
FOR ANY,0.8716981132075472," 
Id
−F ′
1(θ)
In"
FOR ANY,0.8735849056603774," ∂˜F2(ˆτ(˜ξ, ˜x)) ∂ˆτ"
FOR ANY,0.8754716981132076,"= ∂˜τ(˜θ, ˆτ(˜ξ, ˜x)) ∂˜θ"
FOR ANY,0.8773584905660378,"
Id
−F ′
1(θ)
In"
FOR ANY,0.879245283018868,"
˜F2
′(ˆτ(˜ξ, ˜x))"
FOR ANY,0.8811320754716981,"= ∂˜τ(˜θ, ˆτ(˜ξ, ˜x))"
FOR ANY,0.8830188679245283,"∂˜θ
A1 (74)"
FOR ANY,0.8849056603773585,"here A1 =

Id
−F ′
1(θ)
In"
FOR ANY,0.8867924528301887,"
˜F2
′(ˆτ(˜ξ, ˜x)). By the proof above, we have
∂
∂˜ξ =
∂
∂˜θA2, thus we have: L = ∂ ∂˜ξ"
FOR ANY,0.8886792452830189,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.8905660377358491,"∂˜τ(˜θ, ˆτ(˜ξ, ˜x))
∂˜τ(˜θ, ˆτ(˜ξ, ˜x)) ∂ˆτ ! = ∂ ∂˜θ"
FOR ANY,0.8924528301886793,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.8943396226415095,"∂˜τ(˜θ, ˆτ(˜ξ, ˜x))
∂˜τ(˜θ, ˆτ(˜ξ, ˜x))"
FOR ANY,0.8962264150943396,"∂˜θ
A1 ! A2 = ∂ ∂˜θ"
FOR ANY,0.8981132075471698,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.9,"∂˜θ
A1 ! A2. (75)"
FOR ANY,0.9018867924528302,"D
∇˜ξ ˜G(˜τ(˜ξ, ˜x)) −∇˜ξ ˜G(¯τ(˜ξ, ˜x)), ˜u
E"
FOR ANY,0.9037735849056604,"=
Z 
˜τ(˜ξ, ˜x) −¯τ(˜ξ, ˜x)
 ∂2 ˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.9056603773584906,"∂˜ξ∂ˆτ
˜u˜g(˜θ)d˜θ"
FOR ANY,0.9075471698113208,"=
Z 
˜τ(˜ξ, ˜x) −¯τ(˜ξ, ˜x)
 ∂ ∂˜θ"
FOR ANY,0.909433962264151,"∂˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))"
FOR ANY,0.9113207547169812,"∂˜θ
A1 !"
FOR ANY,0.9132075471698113,A2˜u˜g(˜θ)d˜θ
FOR ANY,0.9150943396226415,"⩽˜Aϵ · |
Z
˜f(˜τ(˜θ, ˆτ(˜ξ, ˜x)))˜g(˜θ)⟨∇˜ψ(˜θ), ˜u⟩d ˜θ|, (76)"
FOR ANY,0.9169811320754717,"where ˜A is a constant depending on ∥F ′
1(˜ξ)∥, ∥F ′
2(˜yξ)∥, ∥F ′
2(˜zξ)∥. Then there exists A and we have"
FOR ANY,0.9188679245283019,"Rr ⩾R(1 −ϵA),
(77) where"
FOR ANY,0.9207547169811321,"R =
1
2M ∗ Z pA pB"
FOR ANY,0.9226415094339623,"1
Φ(p)dp"
FOR ANY,0.9245283018867925,"B
IMPLEMENTATION DETAILS AND EXPERIMENTAL SETTINGS"
FOR ANY,0.9264150943396227,"B.1
PRACTICAL ALGORITHMS FOR CALCULATING M ∗"
FOR ANY,0.9283018867924528,"For resolvable transformations in Theorem 1, the M ∗is deﬁned as"
FOR ANY,0.930188679245283,"M ∗= max
ξ,θ∈P ∥M(ξ, θ)∥.
(78)"
FOR ANY,0.9320754716981132,"Since if we have veriﬁed that the semantic transformation is resolvable, most of time we have a
closed form of M ∗like contrast/brightness transformation and we are able to calculate it analytically
as shown in Li et al. (2021)."
FOR ANY,0.9339622641509434,Under review as a conference paper at ICLR 2022
FOR ANY,0.9358490566037736,"For non-resolvable transformations in Corollary 1, M ∗is deﬁned as"
FOR ANY,0.9377358490566038,"M ∗= max
ξ∈P s"
FOR ANY,0.939622641509434,"1
σ2
1
+ 1 σ2
2"
FOR ANY,0.9415094339622642,∂F2(yξ)
FOR ANY,0.9433962264150944,"∂ξ
−A1  2"
FOR ANY,0.9452830188679245,"2
.
(79)"
FOR ANY,0.9471698113207547,"This ratio is similar with the Lipschitz bound for a semantic transformation in Li et al. (2021).For low
dimensional semantic transformations, we are able to interpolate the domain to ﬁnd a maximum M ∗
and corresponding ξ. But this remains a challenge for high dimensional semantic transformations.
Speciﬁcally, for a given ξ, we need to compute the norm of ∂F2(yξ)"
FOR ANY,0.9490566037735849,"∂ξ
−A1. The Jacobian matrix is
n × n. Caculating it requires n times of backpropagation. Thus it is inefﬁcient to store the matrix or
directly compute its norm. To solve the problem, we noitice that

∂F2(yξ)"
FOR ANY,0.9509433962264151,"∂ξ
−A1  2"
FOR ANY,0.9528301886792453,"2
= max
∥u∥2=1 "
FOR ANY,0.9547169811320755,∂F2(yξ)
FOR ANY,0.9566037735849057,"∂ξ
−A1 
u 2"
FOR ANY,0.9584905660377359,"2
.
(80)"
FOR ANY,0.960377358490566,"And then we have
∂F2(yξ)"
FOR ANY,0.9622641509433962,"∂ξ
−A1"
FOR ANY,0.9641509433962264,"T
u = ∂"
FOR ANY,0.9660377358490566,"∂ξ ⟨F2(yξ) −A1, u⟩
(81)"
FOR ANY,0.9679245283018868,"Since tranposing a matrix does not change its norm, we could calculate its norm by optimizing u
that,"
FOR ANY,0.969811320754717,"max
∥u∥2=1"
FOR ANY,0.9716981132075472,"∂
∂ξ ⟨F2(yξ) −A1, u⟩ 2"
FOR ANY,0.9735849056603774,"2
.
(82)"
FOR ANY,0.9754716981132076,"Using this formulation, we only need to multiply the output with an unit vector and perform one
backpropagation. This is a simple convex optimization problem. Then we could use any iterative
algorithm to ﬁnd its solution which is very fast to compute. This trick is crucial and it makes the
matrix norm computation to be scalable in practice."
FOR ANY,0.9773584905660377,"B.2
OTHER DETAILS FOR EXPERIMENTS."
FOR ANY,0.9792452830188679,"Our GSmooth requires to train two neural networks. First we randomly generate corrupted images
to train a image-to-image neural network. The training process of classiﬁers and certiﬁcation for
semantic transformations are done on 2080Ti GPUs. We use a U-Net (Ronneberger et al., 2015) for
the surrogate model and replace all BatchNorm layers with GroupNorm (Wu & He, 2018) since we
might use the model in low bacthsize settings. The U-Net could be replace by other networks used
in image segmentation or superresolution like Res-UNet (Diakogiannis et al., 2020) or EDSR (Lim
et al., 2017). We use L1-loss to train the surrogate model which achieves better accuracy than others
which is also reported (Lim et al., 2017)."
FOR ANY,0.9811320754716981,"After train a surrogate model to simulate the semantic transformation. Then we train the base clas-
siﬁer for certiﬁcation with a moderate data augmentation like (Li et al., 2021; Cohen et al., 2019) to
ensure that training and testing of the classiﬁer is performed on the same distribution. There are two
types of data augmentation, one is the semantic transformation and the other is the augmented noise
introduced only in our work. Data augmentation based semantic transformation could be done using
both the surrogate model or the raw semantic transformation. We can only use the surrogate model
to add the augmented noise because this noise is a type of semantic noise in the layers of surrogate
model. In our experiments the standard deviation of the augmented noise is chosen from 0.1 ∼0.4
depending on the performance. The basic network architectures for these datasets are kept the same
with (Li et al., 2021). On CIFAR-100 daatsets, we use a PreResNet (He et al., 2016b) re-implement
the method by Li et al. (2021). We also adopt the progressive sampling trick mentioned in TSS (Li
et al., 2021) which is useful to reduce computational cost and certify larger radius. The details could
also be found in Li et al. (2021)."
FOR ANY,0.9830188679245283,"C
SUPPLEMENTARY EXPERIMENTS"
FOR ANY,0.9849056603773585,"In this section, we report the results of our GSmooth under adaptive attacks to verify the tightness
of our certiﬁed bound. The experiments are conducted on CIFAR-10 dataset. We use expectation
of transformation to calculate the gradient of the model. Then we apply projected gradient descent"
FOR ANY,0.9867924528301887,Under review as a conference paper at ICLR 2022
FOR ANY,0.9886792452830189,"Cert Acc(%)
EoT attacks(%)
Gaussian blur
67.4
68.1
Tanslation
82.2
87.5
Rotation
64.6
68.4
Rotational blur
39.7
45.0
Defocus blur
25.0
25.0
Pixelate
45.3
49.2"
FOR ANY,0.9905660377358491,"Table 3: Accuracy of our GSmooth under adaptive attacks (PGD using expectation over transforma-
tions) on CIFAR-10 dataset."
FOR ANY,0.9924528301886792,"Type/Acc(%)
Augmix
TSS
Ours
Zoom blur
70.8
75.2
77.1
Defocus blur
72.2
75.6
76.8
Pixelate
50.9
76.0
76.7
Brightness
82.4
71.8
72.1
Motion blur
68.6
70.2
70.5
Gaussian blur
67.4
75.8
75.2"
FOR ANY,0.9943396226415094,Table 4: Empirical accuracy on subsets of CIFAR-10-C.
FOR ANY,0.9962264150943396,"to ﬁnd adversarial examples until it converged. Then we report the accuracy of our model on the
corrupted dataset. The result is listed in the following table. We found that the empirical attack is
no less than the certiﬁed accuracy. This shows that the bound for our model is effective. Additional,
some empirical results are much higher than the certiﬁed accuracy, which might indicate the bound
still has space for improvement."
FOR ANY,0.9981132075471698,"We have conducted some experiments under the common benchmark CIFAR-10-C to show the
empirical robustness under these corruptions. We conducted the experiments on some subsets of
CIFAR-10-C. The corruption types of these subsets are related to the experiments in our paper. The
settings of these experiments and the results of baselines are from TSS(Li et al 2021)."
