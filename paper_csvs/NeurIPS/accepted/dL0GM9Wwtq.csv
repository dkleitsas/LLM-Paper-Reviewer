Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003194888178913738,"Motivated by a recent literature on the double-descent phenomenon in machine
learning, we consider highly over-parameterized models in causal inference, in-
cluding synthetic control with many control units. In such models, there may be so
many free parameters that the model fits the training data perfectly. We first inves-
tigate high-dimensional linear regression for imputing wage data and estimating
average treatment effects, where we find that models with many more covariates
than sample size can outperform simple ones. We then document the performance
of high-dimensional synthetic control estimators with many control units. We find
that adding control units can help improve imputation performance even beyond
the point where the pre-treatment fit is perfect. We provide a unified theoretical
perspective on the performance of these high-dimensional models. Specifically, we
show that more complex models can be interpreted as model-averaging estimators
over simpler ones, which we link to an improvement in average performance. This
perspective yields concrete insights into the use of synthetic control when control
units are many relative to the number of pre-treatment periods."
INTRODUCTION,0.006389776357827476,"1
Introduction"
INTRODUCTION,0.009584664536741214,"Motivated by a recent literature on the double-decent phenomenon in machine learning, we investigate
the properties of common econometric estimators when we increase their complexity. For high-
dimensional linear regression and for synthetic control with many control units, we document
empirical applications where extremely over-parameterized models impute missing out-of-sample
and out-of-time outcomes well. We then provide a common explanation for the returns to complexity
in high-dimensional linear regression and synthetic control in terms of a simple model-averaging
property, which we link to an improvement in imputation performance."
INTRODUCTION,0.012779552715654952,"We often conceptualize the effect of complexity on econometric models in terms of a bias–variance
trade-off: Adding complexity makes models more expressive and reduces bias, while increased
overfitting leads to additional variance. In this view, an overly complex model fits overly well in the
training sample, but fails to recover true parameters or generalize to new data. For example, linear
regression with too many variables or synthetic control with many control units may perform poorly
because of overfitting and excess variance. Consistency results for high-dimensional econometric
models therefore usually assume that the expressiveness of models is limited relative to the available
sample size, and practical advice often highlights choosing simple models or regularizing complex
ones, in a way that balances bias and variance optimally to achieve good estimation and prediction
performance."
INTRODUCTION,0.01597444089456869,Replication code is available at github.com/amarvenu/causal-descent.
INTRODUCTION,0.019169329073482427,"A recent literature in statistics and machine learning has highlighted the surprisingly good prediction
performance of extremely high-dimensional models that fit the data perfectly. That literature has
documented a so-called double-descent phenomenon for deep neural networks and other high-
dimensional regression models: increasing complexity beyond the interpolation threshold at which
the training sample error is zero can lead to a gradual reduction in variance and improvement in
out-of-sample performance. In such cases, there are often two complexity regimes: below the
interpolation threshold, the usual bias–variance trade-off leads to a decrease (first descent) and then an
increase in out-of-sample loss, while beyond the interpolation threshold, out-of-sample loss decreases
again (second descent)."
INTRODUCTION,0.022364217252396165,"In order to investigate the properties of highly over-parameterized models in causal inference, we
first demonstrate a double-descent curve for linear regression in imputing wage outcomes and
estimating average-treatment effects. In the LaLonde (1986) sample of the Current Population
Survey (CPS) drawn from Dehejia and Wahba (1999, 2002), we generate over 8,000 variables
from binning and interacting the original eight demographic and employment-related variables.
We then fit a linear regression model on 3,000 training units and an increasing, randomly chosen
subset of these variables, choosing the norm-minimizing solution once the model fits perfectly.
Evaluated on the LaLonde (1986) control sample from the National Supported Work Demonstration
(NSW) experiment, we observe the usual bias–variance trade-off for a low and moderate number
of included covariates, where performance first increases slightly, before deteriorating substantially
when approaching the interpolation threshold. Beyond the interpolation threshold, however, out-of-
sample performance increases again. Ultimately, an extremely over-parameterized model on over
8,000 variables even outperforms less complex regressions with a randomly chosen set of covariates
and achieves performance comparable to a linear regression on the original, unmanipulated set of
covariates."
INTRODUCTION,0.025559105431309903,"Having demonstrated the returns to complexity in high-dimensional linear regression, we document
the performance of synthetic control estimators with many control units. In the California smoking
data (Abadie et al., 2010), we impute missing smoking rates based on a small number of pre-treatment
periods and an increasingly large number of control states. As in the case of linear regression, we see
returns to increasing model complexity, even beyond the point where some of the synthetic-control
models fit the training data perfectly. However, for synthetic control we do not observe an initial
trade-off between bias and variance: in our empirical example, the performance on future periods is
always better for the more complex models, with no intermittent increase in errors. Unlike the double-
descent relationship for linear regression, the performance of synthetic control in our application
yields a single-descent curve that only improves with complexity, no matter whether there are a few
or many control states."
INTRODUCTION,0.02875399361022364,"We then connect the returns to complexity in high-dimensional linear regression and in synthetic
control in terms of a simple model-averaging property. While both estimators are constructed
differently and represent different regressions, they both share a common feature: More complex
models can be represented as convex averages over simpler models. We show that this model-
averaging property applies to linear regression in the interpolation regime, as well as to synthetic
control in general. The property holds purely mechanically and does not depend on the training or
target distributions. It relates to other model-agnostic properties of linear regression, for which we
also show a reduction in (conditional) variance for minimal-norm least-squares estimators beyond the
interpolation threshold."
INTRODUCTION,0.03194888178913738,"Having established a model-averaging property for interpolation linear regression and for synthetic
control, we provide high-level assumptions under which this property translates to better imputation
performance. A direct consequence of model averaging is that the (convex) prediction loss of a more
complex model cannot be worse than the corresponding average prediction loss of simpler models,
when the same weights are used to average. When the complex model on average also outperforms
comparable models of the same complexity, then we show that the model-averaging property translates
into a reduction in average out-of-sample error relative to a randomly selected simpler model. These
results only put high-level, largely model-agnostic assumptions on the data-generating process, and
are driven by mechanical properties of the underlying estimators."
INTRODUCTION,0.03514376996805112,"Our results have practical implications for the use of synthetic control with many control units.
Conventional wisdom may indicate that synthetic control with a very large number of donor units
relative to the number of pre-treatment periods is problematic, and that selecting among many, ex-ante"
INTRODUCTION,0.038338658146964855,"exchangeable units in this case may represent a challenge. However, our results imply that this is not
the case: as long as ties are broken by a suitable regularization procedure (such as the minimum-norm
solution in our case), making an ex-ante choice among many control units is not necessary. That said,
if ex-ante information is available about which units are particularly suitable as controls, then using
this information can still be helpful and improve imputation."
INTRODUCTION,0.04153354632587859,"We build upon results on over-parameterized regression in machine learning and statistics, where
double-descent curves for kernel and linear regression and the performance of norm-minimizing
interpolating solutions have been studied extensively (including Liang and Rakhlin, 2020; Liang
et al., 2020; Liang and Recht, 2023; Bartlett et al., 2020; Hastie et al., 2022) as part of a broader
literature on double descent and interpolation in deep learning (Zhang et al., 2016; Belkin et al.,
2018; Belkin, 2021; Mei and Montanari, 2022). Kelly et al. (2022) documents the benefits of over-
parameterized models in asset return prediction both in theory and empirically. Kato and Imaizumi
(2022) provides results on the estimation of conditional average causal effects using over-parametrized
linear regression. Relative to this work on interpolating regression, we show connections to synthetic
control based on simple mechanical properties that are largely agnostic about the true data-generating
process. Thereby, we also relate to work on high-dimensional and regularized synthetic control
(Doudchenko and Imbens, 2016; Abadie and L’Hour, 2021; Ben-Michael et al., 2021) and the
connections between synthetic control and linear regression (Athey et al., 2021; Agarwal et al.,
2021; Shen et al., 2022; Bruns-Smith et al., 2023).1 Finally, we connect to a literature on model
averaging in econometrics and statistics (e.g. Hansen, 2007; Claeskens and Hjort, 2008). More
specifically, Wilson and Izmailov (2020) consider Bayesian model averaging in deep learning, and
discuss relationships to double descent. While our linear-regression solutions are closely related
to available results on norm-minimizing regression, we are not aware that the results on synthetic
control were noted previously."
INTRODUCTION,0.04472843450479233,"The remaining note is structured as follows. Section 2 provides an empirical example of double
descent for linear regression and discusses some properties of the norm-minimizing linear-regression
estimator in the interpolating case. Section 3 discusses the relationship of complexity and imputation
performance of synthetic control in an empirical example, and makes connections to the properties of
interpolating linear regression. Section 4 discusses high-level consequences of the model-averaging
property of interpolating linear regression and synthetic control. Section 5 concludes by discussing
some limitations and open questions."
DOUBLE DESCENT FOR LINEAR REGRESSION,0.04792332268370607,"2
Double Descent for Linear Regression"
DOUBLE DESCENT FOR LINEAR REGRESSION,0.051118210862619806,"In this section, we consider imputation by high-dimensional linear regression as an illustration of the
performance of highly over-parameterized models. We start with an empirical illustration of wage
imputation in CPS data with a varying number of randomly ordered covariates, which we evaluate in
terms of its ability to estimate an average treatment effect. We then discuss theoretical properties of
the interpolating linear-regression estimator, followed by a graphical illustration. These results are
closely related to prior work on interpolating linear and kernel regression (including Bartlett et al.,
2020; Liang et al., 2020; Hastie et al., 2022; Kato and Imaizumi, 2022)."
SETUP AND ESTIMATOR,0.054313099041533544,"2.1
Setup and Estimator"
SETUP AND ESTIMATOR,0.05750798722044728,"We consider a linear-regression estimator in data (yi, xi)n
i=1 ∈R × Rk from n observations of a
scalar outcome and k scalar covariates. We write X = (x′
i)n
i=1 ∈Rn×k and Y = (yi)n
i=1 ∈Rn. For
a subset J ⊆{1, . . . , k} of the covariates, we denote by BJ = arg minβ∈Rk;βj=0∀j /∈J
Pn
i=1(yi −
x′
iβ)2 the set of all least-squares linear-regression estimates on J.
Among these, we choose
the norm-minimizing estimate ˆβJ = arg minβ∈BJ ∥β∥.
Throughout, we assume that XJ =
(xij)i∈{1,...,n},j∈J ∈Rn×|J| is of full row rank. Hence, there are multiple least-squares solu-
tions, |BJ| > 1, if and only if |J| > n. In that case, Y = X ˆβJ and ˆβJ is the minimal-norm
interpolating solution (cf. Liang et al., 2020)."
SETUP AND ESTIMATOR,0.06070287539936102,"1For example, Bruns-Smith et al. (2023) shows that synthetic-control-type balancing estimators with non-
negative weights can be related to outcome regressions, which connects our OLS setup in Section 2 to the
synthetic-control setup in Section 3."
EMPIRICAL MOTIVATION,0.06389776357827476,"2.2
Empirical Motivation"
EMPIRICAL MOTIVATION,0.0670926517571885,"We impute wages in the non-experimental LaLonde (1986) sample of the Current Population Survey
(CPS) using linear regression on a large number of covariates, based on data drawn from Dehejia
and Wahba (1999, 2002). From the original eight covariates, we obtain k = 8408 explanatory
variables by binning and interacting the available features. We train minimal-norm least-squares
regression models on a training sample of n = 3000 randomly chosen observations, and evaluate
their mean-squared error in imputing average wages for subsets (of various sizes) of the 260 LaLonde
(1986) National Supported Work Demonstration (NSW) experimental controls as provided through
Dehejia and Wahba (1999, 2002). To these covariates we add a small amount of iid Gaussian noise
to avoid rank deficiency when estimating linear regression. When fitting wage imputation models,
we vary the set of covariates included in the regression. Specifically, we order all features randomly.
For a complexity ℓ∈{1, . . . , k}, we then choose the first ℓcovariates, and obtain the estimate ˆβJ
for J = {1, . . . , ℓ}. Our presented results reflect an average over five such random orderings of
covariates."
EMPIRICAL MOTIVATION,0.07028753993610223,"In order to assess the viability of this approach for applications to average treatment effect (ATE)
estimation, we assess each model by its ability to accurately predict the average outcome on a new
dataset. In particular, we consider various subsets of size m of the NSW experimental control set. For
each m, we draw 1000 samples of size m without replacement from the NSW experimental control
dataset. For each sample, we average the m observation-level outcome predictions and compare the
result to the true mean outcome of the given sample. We then take the RMSE across the 1000 draws
of size m to obtain our evaluation metric.2"
EMPIRICAL MOTIVATION,0.07348242811501597,"In Figure 1a, we consider CPS data and plot the average root-mean-squared error (RMSE) for
pointwise prediction of the wages, reporting the in-sample performance as well. The vertical dashed
line denotes the interpolation threshold where ℓ= n. Figure 1b shows the ATE RMSE metric,
once again averaged over five random orderings of the covariates, for various subset sizes. The
horizontal dashed lines show the RMSE of a simple linear regression on the original, unmanipulated
(low-dimensional) features, colored according to corresponding sample size m. A zoomed-in version
of Figure 1b focusing on the highly overparametrized regime can be found in Figure 7."
EMPIRICAL MOTIVATION,0.07667731629392971,"0
2000
4000
6000
8000
Number of Covariates 0 10 20 30 40 50"
EMPIRICAL MOTIVATION,0.07987220447284345,RMSE (thousands $)
EMPIRICAL MOTIVATION,0.08306709265175719,"Out-of-sample
In-sample"
EMPIRICAL MOTIVATION,0.08626198083067092,"(a) Average out-of-sample (blue) and in-sample
(orange) pointwise RMSE for CPS data."
EMPIRICAL MOTIVATION,0.08945686900958466,"0
2000
4000
6000
8000
Number of covariates 0 5 10 15 20 25 30"
EMPIRICAL MOTIVATION,0.0926517571884984,ATE RMSE (thousands $)
EMPIRICAL MOTIVATION,0.09584664536741214,"m = 1
m = 5
m = 50
m = 100"
EMPIRICAL MOTIVATION,0.09904153354632587,"(b) Average counterfactual prediction RMSE
for varying subset sizes on NSW controls."
EMPIRICAL MOTIVATION,0.10223642172523961,Figure 1: Average RMSE for linear regression for a varying number of covariates.
EMPIRICAL MOTIVATION,0.10543130990415335,"The out-of-sample losses in these illustrations show a descent in loss right of the interpolation
threshold (denoted by the vertical dashed line), at which point in-sample loss is zero. For NSW
experimental controls, out-of-sample error initially decreases (first descent), while for CPS non-
experimental controls it remains initially flat. For both out-of-sample datasets, loss then goes on
to increase and peaks sharply at ℓ= n, at which point the linear models start to fit perfectly. As
complexity increases further, loss decreases again (second descent). In both cases, loss continues
to decrease throughout. For NSW experimental controls, loss ultimately reaches a minimum that is
below the lowest error achieved left of the interpolation threshold, while the loss achieved in the CPS
case is similar between the right tails and the minimum on the left. Furthermore, as the sample size
m increases in Figure 1b, the highly complex interpolating models outperform a simple model based"
EMPIRICAL MOTIVATION,0.10862619808306709,"2For m = 1, taking 1000 draws would necessarily yield duplication; in this case, we instead simply consider
the full set of 260 NSW experimental control observations. Note that results for m = 1 therefore correspond to
a standard (observation-level) RMSE calculation, analogous to the out-of-sample curve in Figure 1a."
EMPIRICAL MOTIVATION,0.11182108626198083,"on the original set of provided features; the m = 50 and m = 100 curves have minima below their
corresponding dashed lines."
EMPIRICAL MOTIVATION,0.11501597444089456,"In this setting, highly over-parameterized linear models constructed via discretizing and interacting
available features exhibit performance improvements over a linear model fitted on the original
features. This result appears remarkable, since the heavily over-parameterized models do not include
the original non-binary covariates, but only indicators obtained from quantile binning (see Section B.1
for details). Further improvement could be achieved by always including the original covariates in
the model."
EMPIRICAL MOTIVATION,0.1182108626198083,"We note that our illustration is extreme in that it artificially creates a large number of low-signal
covariates, for which the best model only slightly outperforms a small, hand-curated model based
on the original covariates. Nevertheless, this simple empirical exercise demonstrates the non-
monotonicity in the relationship of complexity to variance and loss that has been studied by the
literature on interpolating regression and double descent, and extends it to causal target parameters
like the average treatment effect."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.12140575079872204,"2.3
Theoretical Properties and Geometric Illustration"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.12460063897763578,"Motivated by the empirical illustration, we note some theoretical properties that are direct conse-
quences of the structure of the norm-minimizing linear least-squares estimator, and will later serve as
a comparison point for synthetic control. We note that formal results on the bias–variance properties
in terms of more primitive properties of the data-generating process are available, including in Bartlett
et al. (2020); Liang et al. (2020); Hastie et al. (2022). Here, we focus on illustrating properties that
follow mechanically from the construction of the estimator. Our focus is on comparing more complex
models, with covariates J, to slightly simpler (more sparse) ones, with covariates J \ {j}, where the
j-th covariate is dropped. Throughout, we assume that the covariate matrices are of full row rank for
the more and less complex models"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.12779552715654952,"Assumption 1 (Full rank). The covariate matrix XJ ∈Rn×|J| with columns J as well as the
covariate matrices XJ\{j} ∈Rn×(|J|−1) for all j ∈J are of full row rank."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.13099041533546327,"We first consider the geometry of interpolating solutions, for which we note that more complex model
can be expressed by an average over simpler models."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.134185303514377,"Proposition 1 (Model averaging for interpolating linear least-squares regression). For every X and
J with |J| > n such that Assumption 1 holds there exists"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.13738019169329074,"λ ∈[0, 1]J,
X"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.14057507987220447,"j∈J
λj = 1
such that
ˆβJ =
X"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.14376996805111822,"j∈J
λj ˆβJ\{j}."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.14696485623003194,We can choose the weights in Proposition 1 as a function of the covariate matrix XJ only. Specif-
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.1501597444089457,"ically, weights can be chosen as λj =
1−X′
j(XJX′
J)−1Xj
|J|−n
, where Xj is the j-th column of X and
X′
j(XJX′
J)−1Xj can be seen as the “leverage” of feature j ∈J, analogously to the leverage of an
observation in the usual (low-dimensional) linear regression case.3 β1 β2"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.15335463258785942,"ˆβ{1,2} ˆβ{2} ˆβ{1}"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.15654952076677317,(a) Non-interpolating case with n = 2. β1 β2 ˆβ{2} ˆβ{1}
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.1597444089456869,"ˆβ{1,2}"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.16293929712460065,(b) Interpolating case with n = 1.
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.16613418530351437,"Figure 2: Minimal-norm least-squares solutions for linear regression with J = {1, 2}, where n varies."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.16932907348242812,"3We thank Tengyuan Liang for pointing out this connection to us, as well as for suggesting a direct proof via
the Sherman–Morrison–Woodbury formula."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.17252396166134185,"The model averaging property of interpolating linear regression is illustrated in the right panel of
Figure 2 for the simple case of two covariates (J = {1, 2}) and a single data point (n = 1). Here, the
models β ∈RJ that for the model perfectly lie on the line through ˆβ{1} and ˆβ{2}, with the norm-
minimizing solution ˆβ{1,2} lies between the two. In contrast, if ˆβ{1} and ˆβ{2} are not interpolating
(such as in the case of the left panel of Figure 2, where n = 2), then the more complex model ˆβ{1,2}
does not generally lie in the convex hull of the simpler ones."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.1757188498402556,"As an immediate consequence, any interpolating model can in this case be expressed as a convex
average of simpler interpolating models, ˆβJ = P"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.17891373801916932,"L⊆J;|L|=ℓλj ˆβL for all ℓ∈{n, . . . , |J|} (provided
all XL are of full row rank). A particularly interesting special case is ℓ= n, for which we express
ˆβJ as a model average of just-interpolating models ˆβL with ˆβL
L = X−1
L Y ."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.18210862619808307,"In addition to the model-averaging property, the geometry of interpolating solutions also implies
that the variance of the norm-minimizing linear least-squares estimator generically decreases in the
interpolation regime |J| > n, for which the complex estimator ˆβJ along with the simpler estimator
ˆβJ\{j} both fit the training data perfectly.
Proposition 2 (Variance reduction for linear least-squares regression). Suppose Assumption 1 and
that Y has a second moment. If |J| > n then a.s. tr Var(ˆβJ|X) ≤minj∈J tr Var(ˆβJ\{j}|X)."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.1853035143769968,"The reduction in variance is a direct consequence of a more general property of the least-squares
solution. Specifically, the next proposition shows that if we redraw new outcome data in the
interpolation regime, then the distance between more complex solutions is smaller than the distance
between less complex models.
Proposition 3 (Variation hierarchy for linear least-squares regression). For fixed X and J for which
Assumption 1 holds consider two draws YA and YB yielding minimal-norm least-squares estimates
ˆβJ
A, ˆβJ\{j}
A
and ˆβJ
B, ˆβJ\{j}
B
, respectively."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.18849840255591055,"1. If |J| ≤n, then ∥ˆβJ
A −ˆβJ
B∥X′X ≥maxj∈J ∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥X′X."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.19169329073482427,"2. If |J| > n, then ∥ˆβJ
A −ˆβJ
B∥≤minj∈J ∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.19488817891373802,"Here, we write ∥β∥M = √β′Mβ for some positive semi-definite symmetric matrix M ∈Rk×k."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.19808306709265175,"In words, the variation of models across draws of the outcome data increases with complexity on
the left of the interpolation threshold, while it decreases on the right. Here, the choice of norm is
essential for these results to hold uniformly across simpler models.4"
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.2012779552715655,"Figure 3 depicts this graphically. In Figure 3a, for the non-interpolating case we observe that the norm
of the difference between the model coefficient vectors making use of both covariates (ˆβ{1,2}, ˜β{1,2})
is larger than the norm of the differences between the coefficient vectors taking into consideration
just a single covariate at a time. In Figure 3b for the interpolating case, we observe that the reverse is
true; in this case, the norm of the difference between the complex models is smaller than the norms
of the differences between the simpler models."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.20447284345047922,"In practice, we may care about model properties beyond variance, and consider imputation loss
beyond the above norms in the parameters. In Section 4, we will leverage the model-averaging
properties from Proposition 1 to establish such bounds on more general imputation errors."
THEORETICAL PROPERTIES AND GEOMETRIC ILLUSTRATION,0.20766773162939298,"We believe that these variance and geometric properties of linear regression are well understood
in the literature and likely not new, although we are not aware of an explicit statement of the
model-averaging connection between more and less complex interpolating linear-regression models."
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.2108626198083067,"3
Single Descent for Synthetic Control"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.21405750798722045,"We next consider imputation using synthetic control with many control units. As in the case of linear
regression, we start with an empirical illustration. In the Abadie et al. (2010) California smoking"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.21725239616613418,"4The second result still holds for an alternative norm ∥β∥M = √β′Mβ with M positive definite and sym-
metric, provided that the same norm is used when selecting the norm-minimizing estimator in the interpolation
regime. β1 β2"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.22044728434504793,"ˆβ{1,2} ˆβ{2} ˆβ{1}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.22364217252396165,"˜β{1,2}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.2268370607028754,"˜β{2}
˜β{1}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.23003194888178913,(a) Non-interpolating case with n = 2. β1 β2
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.23322683706070288,"ˆβ{1,2}
ˆβ{2}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.2364217252396166,"ˆβ{1}
˜β{2} ˜β{1}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.23961661341853036,"˜β{1,2}"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.24281150159744408,(b) Interpolating case with n = 1.
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.24600638977635783,"Figure 3: Minimal-norm least-squares solutions for draws YA (black) and YB (orange) for linear
regression with J = {1, 2}, where n varies"
SINGLE DESCENT FOR SYNTHETIC CONTROL,0.24920127795527156,"dataset, we impute smoking rates for a target state for a varying number of control states. We then
discuss theoretical properties of the synthetic control estimator, which we connect to its imputation
quality in the following Section 4."
SETUP AND ESTIMATOR,0.2523961661341853,"3.1
Setup and Estimator"
SETUP AND ESTIMATOR,0.25559105431309903,"We consider a panel of N + 1 units observed over T time periods, Y = (yit)i∈{0,...,N},t∈{1,...,T } ∈
R(N+1)×T , where i = 0 denotes the target unit. Our goal is to impute y0t for t ∈{T +1, . . . , T +S}
given yit for i ∈{1, . . . , N}, t ∈{T + 1, . . . , T + S} by the synthetic-control estimator ˆy0t =
PN
i=1 ˆwiyit with convex weights ˆw ∈W = {w ∈[0, 1]N; Pn
i=1 wi = 1}. Specifically, for a subset
J ⊆{1, . . . , N} of control units, we consider the synthetic control weights"
SETUP AND ESTIMATOR,0.25878594249201275,ˆwJ = arg min
SETUP AND ESTIMATOR,0.26198083067092653,"w∈c
WJ
∥w∥
c
WJ =
arg min
w∈W;wj=0∀j /∈J T
X"
SETUP AND ESTIMATOR,0.26517571884984026,"t=1
(y0t − N
X"
SETUP AND ESTIMATOR,0.268370607028754,"i=1
wiyit)2.
(1)"
SETUP AND ESTIMATOR,0.2715654952076677,"Here, we choose the (unique) norm-minimizing synthetic control weights whenever there is more
than one empirical risk minimizer. We can also interpret this solution as the limit ˆwJ of a ridge
penalized synthetic control estimator ˆwJ
η ,"
SETUP AND ESTIMATOR,0.2747603833865815,"ˆwJ = lim
η→0 ˆwJ
η
ˆwJ
η =
arg min
w∈W;wj=0∀j /∈J T
X t=1  y0t − N
X"
SETUP AND ESTIMATOR,0.2779552715654952,"i=1
wiyit !2"
SETUP AND ESTIMATOR,0.28115015974440893,"+ η∥w∥2,
(2)"
SETUP AND ESTIMATOR,0.28434504792332266,"where ˆwJ
η puts a penalty on the Euclidean norm ∥w∥2 of the weights, multiplied by a factor η > 0.
This form of the penalized synthetic-control estimator is also considered by Shen et al. (2022)."
SETUP AND ESTIMATOR,0.28753993610223644,"We note that, unlike in the linear-regression case, we can now end up with non-interpolating solutions
even in the case of high model complexity (many control units). The reason is that the convexity
restriction ˆw ∈W allows for interpolation only if the target outcomes are in the convex hull of the
control outcomes."
EMPIRICAL MOTIVATION,0.29073482428115016,"3.2
Empirical Motivation"
EMPIRICAL MOTIVATION,0.2939297124600639,"To illustrate some properties of the synthetic control estimator, we impute California smoking rates
in the Abadie et al. (2010) dataset. In that dataset, California experiences the introduction of smoking
legislation in 1989, for which Abadie et al. (2010) provides a causal effect estimate by imputing
counterfactual smoking rates for those years when the legislation is in effect. We instead consider
only the time before the legislation is introduced, giving us access to observed control outcomes in
all years, even for California. Specifically, we fit synthetic control models for California on T = 3
years of data (1984–1986), and evaluate their imputation performance on the following S = 2 years
(1987–1988) in terms of mean-squared error."
EMPIRICAL MOTIVATION,0.2971246006389776,"When fitting synthetic control models, we vary how many of the N = 20 control states we include in
the estimation process. Specifically, for a given complexity ℓ∈{1, . . . , N}, we average out-of-time"
EMPIRICAL MOTIVATION,0.3003194888178914,"root-mean squared error (RMSE) across all
 N
ℓ

combinations of control states. We report results in
Figure 4."
EMPIRICAL MOTIVATION,0.3035143769968051,"0
5
10
15
20
Number of control states 0 5 10 15 20"
EMPIRICAL MOTIVATION,0.30670926517571884,RMSE (daily cigarette packs)
EMPIRICAL MOTIVATION,0.30990415335463256,"Out-of-time
Training"
EMPIRICAL MOTIVATION,0.31309904153354634,"Figure 4: Average out-of-time (blue) and training (orange) RMSE for synthetic control for a varying
number of control units."
EMPIRICAL MOTIVATION,0.31629392971246006,"Unlike the linear-regression case, the loss in the synthetic-control case changes monotonically: as we
increase the number of control units, average RMSE decreases. We therefore observe a single-descent
curve in the relationship of complexity and loss, with no notable change in regimes when the number
of control states surpasses the number T = 3 of training periods."
EMPIRICAL MOTIVATION,0.3194888178913738,"As before, our illustration is extreme: by using only three training periods and a random selection of
control states, we can provide a stark illustration of the difference in behavior between the synthetic
control and linear-regression estimators."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3226837060702875,"3.3
Theoretical Properties and Graphical Illustration"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3258785942492013,"While linear regression and synthetic control behave differently in terms of their double- vs single-
descent behavior, we note that both exhibit continuing returns to increasing complexity, with no limit.
In our empirical illustration, that return to complexity kicks in in the interpolation regime for linear
regression and throughout for synthetic control. We now connect this commonality in returns to
complexity to a corresponding theoretical connection."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.329073482428115,"Proposition 4 (Model averaging for synthetic control). For all J with |J| > 1 and data Y there
exists"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.33226837060702874,"ˆλ ∈[0, 1]J,
X"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3354632587859425,"j∈J
ˆλj = 1
such that
ˆwJ =
X"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.33865814696485624,"j∈J
ˆλj ˆwJ\{j}."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.34185303514376997,"Hence, synthetic control has the same model-averaging property as interpolating linear regression
(Proposition 1). Now, however, the model-averaging property also holds without interpolation, and is
instead driven by the convexity of synthetic control weights. Furthermore, the weights can depend on
outcome data, although only for pre-treatment outcomes. We further note that the result extends to
penalized synthetic control with some fixed penalty parameter η."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3450479233226837,"We illustrate the model-averaging property for synthetic control in Figure 5, where we show that
synthetic California with |J| = 2 (left) and |J| = 3 (right) control states is a convex combination of
synthetic California with fewer control states. Here, we focus on the fit in the training data, and do
not explicitly show the underlying synthetic-control weights. We note that Figure 5 covers both cases
where the synthetic estimator for California has perfect training fit (right) and cases where it does not
(left)."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.34824281150159747,"Unlike in the case of linear regression (Proposition 3) we do not, however, obtain an immediate bound
on the variation of synthetic control models. Indeed, as the case of |J| = 2 controls in Figure 5
clarifies, the complex model can have more variance in weights than the simple ones (which here do
not vary at all), despite the model-averaging property. In the next section, we will therefore connect
model averaging directly to improvements of imputation quality, without relying on explicit variation
results. MA IL yi1 yi2 CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3514376996805112,"ˆy{MA,IL}
CA
ˆy{MA}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3546325878594249,"ˆy{IL}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.35782747603833864,"(a) Non-interpolating case, |J| = 2. MA IL NJ yi1 yi2 CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3610223642172524,"ˆy{MA,IL}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.36421725239616615,"ˆy{IL,NJ}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.36741214057507987,"ˆy{NJ,MA}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3706070287539936,"ˆy{MA,IL,NJ}
CA"
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3738019169329074,"(b) Interpolating case, |J| = 3."
THEORETICAL PROPERTIES AND GRAPHICAL ILLUSTRATION,0.3769968051118211,"Figure 5: Synthetic-control examples for T = 2, where the set J of included units varies."
MODEL-AVERAGING BASED RISK BOUNDS,0.3801916932907348,"4
Model-Averaging Based Risk Bounds"
MODEL-AVERAGING BASED RISK BOUNDS,0.38338658146964855,"Above, we argued that more complex models are model averages over simpler models in two cases
that are relevant to causal inference: interpolating linear regression and synthetic control. In this
section, we discuss conditions under which model averaging leads to better imputation quality."
MODEL-AVERAGING BASED RISK BOUNDS,0.3865814696485623,"To unify the above cases, we now consider generic estimated functions ˆf ∗: X →R that can be
related to simpler models ˆf j : X →R for an index set j ∈J by the model-averaging property"
MODEL-AVERAGING BASED RISK BOUNDS,0.38977635782747605,"ˆf ∗=
X"
MODEL-AVERAGING BASED RISK BOUNDS,0.3929712460063898,"j∈J
ˆλj ˆf j
for some
ˆλ ∈[0, 1]J,
X"
MODEL-AVERAGING BASED RISK BOUNDS,0.3961661341853035,"j∈J
ˆλj = 1.
(MA)"
MODEL-AVERAGING BASED RISK BOUNDS,0.3993610223642173,"We see this model-averaging property as a purely mechanical property of estimators, which we
applies to interpolating linear regression (Proposition 1) and to synthetic control (Proposition 4) by
our results above"
MODEL-AVERAGING BASED RISK BOUNDS,0.402555910543131,"Model averaging provides some insurance against excess loss. Intuitively, being able to represent a
more complex model ˆf ∗in terms of a model average over simpler models diversifies the risk of a
bad imputation fit. When considering convex loss functions, we can make this intuition precise via a
simple application of Jensen’s inequality, which yields for the case of squared error that"
MODEL-AVERAGING BASED RISK BOUNDS,0.4057507987220447,"(y −ˆf ∗(x))2 ≤
X"
MODEL-AVERAGING BASED RISK BOUNDS,0.40894568690095845,"j∈J
ˆλj(y −ˆf j(x))2.
(3)"
MODEL-AVERAGING BASED RISK BOUNDS,0.41214057507987223,"for any target point (y, x) ∈R × X. Hence, imputation loss using the complex model is at most a
weighted average over the loss of simpler models. The relationship also extends directly to estimating
averages of outcomes y by averages of predictions ˆf(x), as in the case of average treatment effects in
Section 2."
MODEL-AVERAGING BASED RISK BOUNDS,0.41533546325878595,"In principle, the simple portfolio bound in (3) leaves open the possibility that the more complex
model ˆf ∗performs as poorly as the worst of the simpler models ˆf j. However, for this to occur, the
weights would have to be positively correlated with bad performance. A condition on imputation
quality we can therefore consider imposing is that the selection of weights is not, on average, working
against imputation quality. The following result formalizes this idea on a (very) high level.
Proposition 5 (Model-agnostic risk bound). Assume that (MA) holds and that for some distribution
over training and target data we have that for all permutations π : J →J"
MODEL-AVERAGING BASED RISK BOUNDS,0.4185303514376997,"E[(y −ˆf ∗(x))2] ≤E[(y −ˆf ∗
π(x))2]
where
ˆf ∗
π =
X"
MODEL-AVERAGING BASED RISK BOUNDS,0.4217252396166134,"j∈J
ˆλπ(j) ˆf j.
(P)"
MODEL-AVERAGING BASED RISK BOUNDS,0.4249201277955272,"Then we obtain the bound E[(y −ˆf ∗(x))2] ≤
1
|J|
P"
MODEL-AVERAGING BASED RISK BOUNDS,0.4281150159744409,j∈J E[(y −ˆf j(x))2].
MODEL-AVERAGING BASED RISK BOUNDS,0.43130990415335463,"In words, if the model chosen by the data on average over some distribution is not worse than a model
where we mix up weights, then the imputation performance of the complex model is not worse than
the average of the imputation performances of simple models, leading to observations like those
in Figures 1b and 4 where increased model complexity leads to improved imputation quality for
randomly-ordered models."
MODEL-AVERAGING BASED RISK BOUNDS,0.43450479233226835,"We make two comments on the formal conditions of the proposition. First, it is enough to assume
that (P) holds on average over permutations chosen uniformly at random. Second, the distribution
behind the expectation E can incorporate prior distributions over underlying parameters, in which
case the resulting bound holds on average over the same distribution."
MODEL-AVERAGING BASED RISK BOUNDS,0.43769968051118213,"In the examples of linear regression and synthetic control, Figure 6 illustrates the respective permuted
models. In those cases, the permutation assumption (P) assumes that, on average, the model chosen
by the data is not worse than a model where we mix up the weights (in gray). β1 β2 ˆβ{2} ˆβ{1}"
MODEL-AVERAGING BASED RISK BOUNDS,0.44089456869009586,"ˆβ{1,2}"
MODEL-AVERAGING BASED RISK BOUNDS,0.4440894568690096,"ˆβ{1,2}
π"
MODEL-AVERAGING BASED RISK BOUNDS,0.4472843450479233,(a) Interpolating linear regression MA IL yi1 yi2 CA
MODEL-AVERAGING BASED RISK BOUNDS,0.4504792332268371,"ˆy{MA,IL}
CA
ˆy{MA,IL}
π,CA"
MODEL-AVERAGING BASED RISK BOUNDS,0.4536741214057508,"ˆy{MA}
CA"
MODEL-AVERAGING BASED RISK BOUNDS,0.45686900958466453,"ˆy{IL}
CA"
MODEL-AVERAGING BASED RISK BOUNDS,0.46006389776357826,(b) Synthetic control
MODEL-AVERAGING BASED RISK BOUNDS,0.46325878594249204,Figure 6: Illustration of permutation bound based on the examples from Figures 2 and 5.
MODEL-AVERAGING BASED RISK BOUNDS,0.46645367412140576,"While more primitive conditions may be helpful to judge when a condition like (P) holds, we note
two attractive properties. First, the assumption complements the model-averaging property (MA)
in an important way: While model averaging relates more complex to less complex models, the
permutation property only compares models of comparable complexity (assuming that there are
no systematic ex-ante differences between the ˆf j). To violate this property would thus amount
to assuming that selection among models with comparable complexity is disadvantageous, which
may be unreasonable to expect on average. Second, we can formulate this condition on the level of
estimators, without explicit reference to the underlying data-generating process."
CONCLUSION,0.4696485623003195,"5
Conclusion"
CONCLUSION,0.4728434504792332,"We study the imputation performance of interpolating linear regression and synthetic control, and
provide a unified perspective on returns to complexity in both cases: More complex models can be
expressed as model averages over simpler ones. While we provide some high-level assumptions
on when this model-averaging property improves average imputation risk, more work is needed to
establish primitive sufficient conditions. This includes, in particular, studying how the bias changes
as models become more complex. In addition, we limit our analysis to comparing more complex
to simpler models when features or control units are randomly ordered, but in practice, we may
have knowledge about which simple models are more plausible than others. However, our results
show that highly over-parameterized models that achieve perfect in-sample fit can yield measurable
performance improvements over non-random simple models in causal settings. Future research could
explore conditions under which this phenomenon holds, namely where complex models can beat
non-random simple ones."
CONCLUSION,0.476038338658147,Acknowledgments and Disclosure of Funding
CONCLUSION,0.4792332268370607,"We thank Tengyuan Liang, Sendhil Mullainathan, Ashesh Rambachan, audiences at Emory University,
the 2023 “Econometrics in the Era of Machine Learning” conference at the University of Chicago, the
2023 “HDMetrics: Big Data, High-Dimensional Methods, and Machine Learning” workshop at the
University of Illinois at Urbana-Champaign, the 2023 CEME Conference for Young Econometricians
at Georgetown University, and the 2023 California Econometrics Conference at the University of
Washington, as well as four anonymous reviewers for helpful comments and discussions."
CONCLUSION,0.48242811501597443,"Computational support was provided by the Data, Analytics, and Research Computing (DARC) group
at the Stanford Graduate School of Business (RRID:SCR_022938)."
REFERENCES,0.48562300319488816,References
REFERENCES,0.48881789137380194,"Abadie, Alberto, Alexis Diamond, and Jens Hainmueller (2010). Synthetic control methods for
comparative case studies: Estimating the effect of california’s tobacco control program. Journal of
the American statistical Association, 105(490):493–505. (Cited on pages 2, 6, 7, and 17.)"
REFERENCES,0.49201277955271566,"Abadie, Alberto and Jérémy L’Hour (2021). A penalized synthetic control estimator for disaggregated
data. Journal of the American Statistical Association, 116(536):1817–1834. (Cited on page 3.)"
REFERENCES,0.4952076677316294,"Agarwal, Anish, Munther Dahleh, Devavrat Shah, and Dennis Shen (2021). Causal matrix completion.
arXiv preprint arXiv:2109.15154. (Cited on page 3.)"
REFERENCES,0.4984025559105431,"Athey, Susan, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi (2021).
Matrix completion methods for causal panel data models. Journal of the American Statistical
Association, 116(536):1716–1730. (Cited on page 3.)"
REFERENCES,0.5015974440894568,"Bartlett, Peter L, Philip M Long, Gábor Lugosi, and Alexander Tsigler (2020). Benign overfitting
in linear regression. Proceedings of the National Academy of Sciences of the United States of
America, 117(48):30063–30070. (Cited on pages 3 and 5.)"
REFERENCES,0.5047923322683706,"Belkin, Mikhail (2021). Fit without fear: remarkable mathematical phenomena of deep learning
through the prism of interpolation. Acta Numerica, 30:203–248. (Cited on page 3.)"
REFERENCES,0.5079872204472844,"Belkin, Mikhail, Siyuan Ma, and Soumik Mandal (2018). To understand deep learning we need to
understand kernel learning. In International Conference on Machine Learning, pages 541–549.
PMLR. (Cited on page 3.)"
REFERENCES,0.5111821086261981,"Ben-Michael, Eli, Avi Feller, and Jesse Rothstein (2021). The augmented synthetic control method.
Journal of the American Statistical Association, 116(536):1789–1803. (Cited on page 3.)"
REFERENCES,0.5143769968051118,"Bruns-Smith, David, Oliver Dukes, Avi Feller, and Elizabeth L Ogburn (2023). Augmented balancing
weights as undersmoothed regressions. (Cited on page 3.)"
REFERENCES,0.5175718849840255,"Claeskens, Gerda and Nils Lid Hjort (2008). Model selection and model averaging. (Cited on page 3.)"
REFERENCES,0.5207667731629393,"Dehejia, Rajeev H and Sadek Wahba (1999). Causal effects in nonexperimental studies: Reevaluating
the evaluation of training programs. Journal of the American statistical Association, 94(448):1053–
1062. (Cited on pages 2, 4, and 16.)"
REFERENCES,0.5239616613418531,"Dehejia, Rajeev H and Sadek Wahba (2002). Propensity score-matching methods for nonexperimental
causal studies. Review of Economics and statistics, 84(1):151–161. (Cited on pages 2, 4, and 16.)"
REFERENCES,0.5271565495207667,"Doudchenko, Nikolay and Guido W Imbens (2016). Balancing, regression, difference-in-differences
and synthetic control methods: A synthesis. Technical report, National Bureau of Economic
Research. (Cited on page 3.)"
REFERENCES,0.5303514376996805,"Hansen, Bruce E (2007). Least squares model averaging. Econometrica, 75(4):1175–1189. (Cited on
page 3.)"
REFERENCES,0.5335463258785943,"Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani (2022). Surprises in
high-dimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949–986.
(Cited on pages 3 and 5.)"
REFERENCES,0.536741214057508,"Kato, Masahiro and Masaaki Imaizumi (2022). Benign-overfitting in conditional average treatment
effect prediction with linear regression. arXiv preprint arXiv:2202.05245. (Cited on page 3.)"
REFERENCES,0.5399361022364217,"Kelly, Bryan T, Semyon Malamud, and Kangying Zhou (2022). The virtue of complexity in return
prediction. Technical report, National Bureau of Economic Research. (Cited on page 3.)"
REFERENCES,0.5431309904153354,"LaLonde, Robert J (1986). Evaluating the econometric evaluations of training programs with
experimental data. The American economic review, pages 604–620. (Cited on pages 2, 4, and 16.)"
REFERENCES,0.5463258785942492,"Liang, Tengyuan and Alexander Rakhlin (2020). Just interpolate: Kernel “Ridgeless” regression can
generalize. The Annals of Statistics, 48(3):1329–1347. (Cited on page 3.)"
REFERENCES,0.549520766773163,"Liang, Tengyuan, Alexander Rakhlin, and Xiyu Zhai (2020). On the Multiple Descent of Minimum-
Norm Interpolants and Restricted Lower Isometry of Kernels. In Abernethy, Jacob and Shivani
Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory, volume 125 of
Proceedings of Machine Learning Research, pages 2683–2711. PMLR. (Cited on pages 3 and 5.)"
REFERENCES,0.5527156549520766,"Liang, Tengyuan and Benjamin Recht (2023). Interpolating Classifiers Make Few Mistakes. Journal
of machine learning research: JMLR. (Cited on page 3.)"
REFERENCES,0.5559105431309904,"Mei, Song and Andrea Montanari (2022). The generalization error of random features regression: Pre-
cise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
75(4):667–766. (Cited on page 3.)"
REFERENCES,0.5591054313099042,"Shen, Dennis, Peng Ding, Jasjeet Sekhon, and Bin Yu (2022). Same Root Different Leaves: Time
Series and Cross-Sectional Methods in Panel Data. (Cited on pages 3 and 7.)"
REFERENCES,0.5623003194888179,"Wilson, Andrew G and Pavel Izmailov (2020). Bayesian deep learning and a probabilistic perspective
of generalization. Advances in neural information processing systems, 33:4697–4708. (Cited on
page 3.)"
REFERENCES,0.5654952076677316,"Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals (2016). Under-
standing deep learning requires rethinking generalization. (Cited on page 3.)"
REFERENCES,0.5686900958466453,"A
Proofs"
REFERENCES,0.5718849840255591,"Proof of Proposition 1. We provide a direct proof for the choice λj =
1−X′
j(XJX′
J)−1Xj
|J|−n
via the
Sherman–Morrison–Woodbury formula. We first note that, for k ≥n, A ∈Rn×k of full row rank n,
a ∈Rn, and ˆα = arg minα∈Rk;Aα=a ∥α∥we have that ˆα = A′(AA′)−1a. Indeed, Aˆα = a, and for
any α ∈Rk with Aα = a and α ̸= ˆα, for Π = A′(AA′)−1A we have that"
REFERENCES,0.5750798722044729,∥α∥2 = ∥Πα∥2 + ∥(I −Π)α∥2 = ∥Πˆα∥2 + ∥(I −Π)(α −ˆα)∥2 = ∥ˆα∥2 + ∥α −ˆα∥2 > ∥ˆα∥2.
REFERENCES,0.5782747603833865,"We next write XJ ∈Rn×k for the matrix with columns XJ
j = Xj for j ∈J and XJ
j = 0 for j /∈J.
Applying the above result to ˆβJ and ˆβJ\{j} for all j ∈J, we find"
REFERENCES,0.5814696485623003,"ˆβJ = XJ′(XJX′
J)−1Y,
ˆβJ\{j} = XJ\{j}′(XJ\{j}X′
J\{j})−1Y."
REFERENCES,0.5846645367412141,"Using that XJ\{j}X′
J\{j} = XJX′
J −XjX′
j, which is invertible by the assumption that XJ\{j} is
of full row rank, we find by the Sherman–Morrison–Woodbury that"
REFERENCES,0.5878594249201278,"(XJ\{j}XJ\{j})−1 = (XJX′
J)−1 + (XJX′
J)−1Xj(1 −X′
j(XJX′
J)−1Xj)−1X′
j(XJX′
J)−1 (4)"
REFERENCES,0.5910543130990416,"with X′
j(XJX′
J)−1Xj ̸= 1. Plugging in,"
REFERENCES,0.5942492012779552,ˆβJ\{j} = (XJ −X{j})′(XJ\{j}XJ\{j})−1Y
REFERENCES,0.597444089456869,"= ˆβJ −X{j}′(XJX′
J)−1Y −X{j}′(XJX′
J)−1Y
X′
j(XJX′
j)−1Xj
1 −X′
j(XJX′
j)−1Xj"
REFERENCES,0.6006389776357828,"+ XJ′(XJX′
J)−1XjX′
j(XJX′
J)−1Y
1
1 −X′
j(XJX′
j)−1Xj"
REFERENCES,0.6038338658146964,"= ˆβJ +

XJ′(XJX′
J)−1XjX′
j(XJX′
J)−1 −X{j}′(XJX′
J)−1
Y
1
1 −X′
j(XJX′
j)−1Xj
."
REFERENCES,0.6070287539936102,Since P
REFERENCES,0.610223642172524,"j∈J XjX′
j = XJX′
J and P"
REFERENCES,0.6134185303514377,"j∈J X{j} = XJ, we have that
X"
REFERENCES,0.6166134185303515,"j∈J
ˆβJ\{j}λj = ˆβJ X"
REFERENCES,0.6198083067092651,"j∈J
λj+(|J|−n)(XJ′(XJX′
J)−1XJX′
J(XJX′
J)−1Y −XJ′(XJX′
J)−1Y ) = ˆβJ X"
REFERENCES,0.6230031948881789,"j∈J
λj."
REFERENCES,0.6261980830670927,"Finally, X′
j(XJX′
J)−1Xj
≥0 since XJX′
J positive definite, X′
j(XJX′
J)−1Xj
≤1 since
XJ\{j}X′
J\{j}
=
XJX′
J −XjX′
j
⪯
XJX′
J in (4), and P"
REFERENCES,0.6293929712460063,"j∈J X′
j(XJX′
J)−1Xj
= tr
P"
REFERENCES,0.6325878594249201,"j∈J XjX′
jXJX′
J

= n, so λj ∈[0, 1] for all j ∈J and PJ
j=1 λj = 1."
REFERENCES,0.6357827476038339,"Proof of Proposition 2. The result follows from Proposition 3 by noting that, for two indpendent
draws YA and YB for fixed X, we have that"
REFERENCES,0.6389776357827476,"E[∥ˆβJ
A −ˆβJ
B∥2|X] = E[∥(ˆβJ
A −E[ˆβJ|X]) −(ˆβJ
B −E[ˆβJ|X])∥2|X]"
REFERENCES,0.6421725239616614,"= E[∥ˆβJ
A −E[ˆβJ|X]∥2|X] + E[∥ˆβJ
B −E[ˆβJ|X]∥2|X] = 2 tr Var(ˆβJ|X)"
REFERENCES,0.645367412140575,"(and the same for J \ {j}), and thus"
REFERENCES,0.6485623003194888,tr Var(ˆβJ|X) = 1
REFERENCES,0.6517571884984026,"2 E[∥ˆβJ
A −ˆβJ
B∥2|X] ≤1"
E,0.6549520766773163,"2 E

min
j∈J ∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥2
X
"
E,0.65814696485623,"≤min
j
1
2 E[∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥2|X] = tr Var(ˆβJ\{j}|X)."
E,0.6613418530351438,"Proof of Proposition 3. Consider first the case |J| ≤n. Under Assumption 1, we define the projec-
tion matrices"
E,0.6645367412140575,"ΠJ = XJ(X′
JXJ)−1X′
J ∈Rn×n,
ΠJ\{j} = XJ\{j}(X′
J\{j}XJ\{j})−1X′
J\{j} ∈Rn×n."
E,0.6677316293929713,"Since ΠJ\{j} = ΠJ\{j}ΠJ, we have that X ˆβJ\{j} = ΠJ\{j}Y = ΠJ\{j}ΠJY = ΠJ\{j} ˆβJ.
Therefore,"
E,0.670926517571885,"∥ˆβJ
A −ˆβJ
B∥2
X′X = ∥X ˆβJ
A −X ˆβJ
B∥2 = ∥ΠJ\{j}(X ˆβJ
A −X ˆβJ
B)∥2 + ∥(I −ΠJ\{j})(X ˆβJ
A −X ˆβJ
B)∥2"
E,0.6741214057507987,"≥∥ΠJ\{j}(X ˆβJ
A −X ˆβJ
B)∥2 = ∥X ˆβJ\{j}
A
−X ˆβJ\{j}
B
∥2 = ∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥2
X′X."
E,0.6773162939297125,"Consider now the case |J| > n. Using the notation from the proof of Proposition 1, under As-
sumption 1 we have that XJ ˆβJ = X ˆβJ = Y = X ˆβJ\{j} = XJ ˆβJ\{j} and thus ΠˆβJ = ΠˆβJ\{j}"
E,0.6805111821086262,"(as well as (I −Π)ˆβJ = 0) for the projection matrix Π = XJ′(XJX′
J)−1XJ ∈Rk×k. As a
consequence,"
E,0.6837060702875399,"∥ˆβJ
A −ˆβJ
B∥2 = ∥Π(ˆβJ
A −ˆβJ
B)∥2 + ∥(I −Π)(ˆβJ
A −ˆβJ
B)∥2 = ∥Π(ˆβJ\{j}
A
−ˆβJ\{j}
B
)∥2"
E,0.6869009584664537,"≤∥Π(ˆβJ\{j}
A
−ˆβJ\{j}
B
)∥2 + ∥(I −Π)(ˆβJ\{j}
A
−ˆβJ\{j}
B
)∥2 = ∥ˆβJ\{j}
A
−ˆβJ\{j}
B
∥2."
E,0.6900958466453674,"Proof of Proposition 4. Building upon the notation from Section 3.1, for J ⊂{1, . . . , N} write
WJ = {w ∈W; wj = 0 for all j /∈J} (where W = {w ∈[0, 1]N; PN
i=1 wi = 1} is the N −1-
simplex) and let ∂WJ = S"
E,0.6932907348242812,"j∈J WJ\{j} ⊆WJ be the boundary of WJ. For outcomes, it will also
be convenient to write X = (yit)t∈{1,...,T },i∈{1,...,N} ∈RT ×N for the pre-treatment outcomes of the
control units (with columns representing units), and y = (y0t)t∈{1,...,T } ∈RT for the pre-treatment
outcomes of the treated unit."
E,0.6964856230031949,"As the first step, we note that we can express the quality of synthetic control weights w ∈WJ as"
E,0.6996805111821086,"∥Xw −y∥2 = ∥Xw −yJ∥2 + ∥yJ −y∥2
(5)"
E,0.7028753993610224,"in terms of the fitted values yJ = XwJ for the solution wJ to a relaxed problem that drops the
non-negativity constraint. That solution with weights in W∗= {w ∈RN; Pn
i=1 wi = 1} is defined,
analogously to the synthetic-control solution in (1), as"
E,0.7060702875399361,wJ = arg min
E,0.7092651757188498,"w∈W
J ∥w∥∈W∗,
W
J =
arg min
w∈W∗;wj=0∀j /∈J
∥Xw −y∥⊆W∗."
E,0.7124600638977636,"For this solution, we note that ∥Xw −y∥2 = ∥X(w −wJ)∥2 + 2(w −wJ)′X′(XwJ −y) +
∥XwJ −y∥2. Assume now that (w −wJ)′X′(XwJ −y) ̸= 0. Then there is some ε ̸= 0 such that
wJ(ε) = wJ −(w −wJ) ε ∈W∗with wj = 0 for j /∈J fulfills ∥XwJ(ε) −y∥< ∥XwJ −y∥,
contradicting the choice of wJ. Hence we must have that ∥Xw −y∥2 = ∥X(w −wJ)∥2 + ∥XwJ −
y∥2 = ∥Xw −yJ∥2 + ∥yJ −y∥2, which establishes (5)."
E,0.7156549520766773,"As the second step, we note that we can therefore define the synthetic control solution in (1) in terms
of the fitted values yJ of the relaxed solution as"
E,0.7188498402555911,ˆwJ = arg min
E,0.7220447284345048,"w∈c
WJ
∥w∥∈WJ,
c
WJ = arg min
w∈WJ ∥Xw −yJ∥⊆WJ."
E,0.7252396166134185,"This follows immediately from (5) since ∥yJ −y∥is not affected by the choice of w ∈WJ. Similarly,
for the constrained solutions with index set J \ {j} for j ∈J, we have that"
E,0.7284345047923323,ˆwJ\{j} = arg min
E,0.731629392971246,"w∈c
WJ\{j}
∥w∥∈WJ\{j},
c
WJ\{j} = arg min
w∈WJ\{j} ∥Xw −yJ∥⊆WJ\{j}"
E,0.7348242811501597,since WJ\{j} ⊆WJ for all j ∈J.
E,0.7380191693290735,"As the third (and central) step, we use Farkas’ lemma to argue that there exist λ ∈RJ with λj ≥0
for all j ∈J such that XwJ = P"
E,0.7412140575079872,j∈J λjXwJ\{j}.
E,0.744408945686901,"Assume first that X ˆwJ ̸= yJ. Then we must have that ˆwJ ∈∂WJ. Indeed, if ˆwJ ∈WJ \∂WJ then
there exists some ε > 0 such that ˆwJ(ε) = ˆwJ (1 −ε) + wJ ε ∈WJ, for which ∥X ˆwJ(ε) −yJ∥=
∥X( ˆwJ(ε) −wJ)∥= (1 −ε)∥X( ˆwJ −wJ)∥< ∥X ˆwJ −yJ∥, contradicting the choice of c
W J and"
E,0.7476038338658147,"ˆwJ. Hence ˆwJ ∈∂WJ, so there is some j with ˆwJ ∈WJ\{j}, which implies that ˆwJ\{j} = ˆwJ"
E,0.7507987220447284,and X ˆwJ = X ˆwJ\{j}. This means that we can choose λ as the indicator for component j.
E,0.7539936102236422,"Assume now that X ˆwJ = yJ, and that there exists no such λ. Then, by Farkas’ lemma, there exists
v ∈RT \ {0} such that v′X ˆwJ < 0 and v′X ˆwJ\{j} ≥0 for all j ∈J. Define the projection
matrix Π = vv′"
E,0.7571884984025559,"v′v ∈RT ×T , and let W∗= arg minw∈WJ;ΠX(w−ˆ
wJ)=X(w−ˆ
wJ) v′Xw ⊆WJ. Then
the minimum is attained at a boundary point w∗∈W∗∩∂WJ of the feasible set. Indeed, the
feasible set is non-empty since it includes ˆwJ, and it is compact and convex. The minimum of
the linear function is therefore attained at a boundary point, which is in ∂WJ. As a consequence,
w∗∈WJ\{j} for some j ∈J. Since ˆwJ ∈WJ, we have that v′Xw∗≤v′X ˆwJ < v′X ˆwJ\{j}.
Hence there is some ε ∈(0, 1] such that ˆwJ\{j}(ε) = ˆwJ\{j} (1 −ε) + w∗ε ∈WJ\{j} fulfills
v′X ˆwJ\{j}(ε) = v′X ˆwJ. Since we therefore have ΠX ˆwJ\{j}(ε) = ΠX ˆwJ, as well as (I −
Π)X ˆwJ\{j}(ε) = (I −Π)X( ˆwJ\{j} (1 −ε) + ε ˆwJ) since ΠX(w∗−ˆwJ) = X(w∗−ˆwJ), we have
that"
E,0.7603833865814696,∥X ˆwJ\{j}(ε) −yJ∥2 = ∥X( ˆwJ\{j}(ε) −ˆwJ)∥2
E,0.7635782747603834,= ∥ΠX( ˆwJ\{j}(ε) −ˆwJ)∥2 + ∥(I −Π)X( ˆwJ\{j}(ε) −ˆwJ)∥2 = 0 + (1 −ε)2∥(I −Π)X( ˆwJ\{j} −ˆwJ)∥2
E,0.7667731629392971,"< ∥ΠX( ˆwJ\{j} −ˆwJ)∥2 + ∥(I −Π)X( ˆwJ\{j} −ˆwJ)∥2 = ∥X( ˆwJ\{j} −ˆwJ)∥2 = ∥X ˆwJ\{j} −yJ∥2,"
E,0.7699680511182109,"contradicting the choice of ˆwJ\{j}. Hence, such λ must exist."
E,0.7731629392971247,"As the fourth step, we expand the previous result on fitted values to the weights themselves in the case
of penalized synthetic control, and show that the weights sum to one in that case. To this end, note
that we can write the penalized synthetic control estimator from (2) as ˆwJ
η = arg minw∈WJ ∥Xw −
y∥2 +η∥w∥2. Write now ˜XJ
η = (X′
JXJ +ηI)1/2 ∈RJ×J for the symmetric positive-definite matrix
square root of the symmetric positive-definite X′
JXJ + ηI, where XJ is a matrix of the columns of
X with index in J, and ˜yJ
η = ( ˜XJ
η )−1X′
Jy ∈RJ. For wJ the entries of w ∈WJ corresponding to
the index set J, we find"
E,0.7763578274760383,∥Xw −y∥2 + η∥w∥2 = ∥XJwJ −y∥2 + η∥wJ∥2
E,0.7795527156549521,"= w′
JX′
JXJwJ −2w′
JX′
Jy + y′y + ηw′
JwJ = w′
J(X′
JXJ + ηI)wJ −2w′
JX′
Jy + y′y"
E,0.7827476038338658,"= w′
J ˜XJ′
η ˜XJ
η wJ −2w′
J ˜XJ′
η

( ˜XJ
η )−1X′
Jy

+ y′y = ∥˜XJ
η wJ −˜yJ
η ∥2 −∥˜yJ
η ∥2 + ∥y∥2."
E,0.7859424920127795,"Hence, we can write (noting that WJ\{j} ⊆WJ)"
E,0.7891373801916933,"ˆwJ
η = arg min
w∈WJ ∥˜XJ
η wJ −˜yJ
η ∥,
wJ\{j}
η
= arg min
w∈WJ\{j} ∥˜XJ
η wJ −˜yJ
η ∥,"
E,0.792332268370607,"so we can interpret penalized synthetic control on units J and J \ {j} with time periods {1, . . . , T}
and the original outcomes as non-penalized synthetic control on units J and J \{j} with time periods
J and transformed outcomes, where we note that the synthetic-control solutions are unique in this
case. Hence, we can apply the previous result to conclude that there exists λη ∈RJ with λη,j ≥0
for all j ∈J such that ˜XJ
η ˜wJ
η = P"
E,0.7955271565495208,"j∈J λη,j ˜XJ
η ˜wJ\{j}
η
. Since ˜XJ
η is invertible, it now also follows"
E,0.7987220447284346,"that ˜wJ
η = P"
E,0.8019169329073482,"j∈J λη,j ˜wJ\{j}
η
. Since also ˜wJ
η ∈W and ˜wJ\{j}
η
∈W for all j ∈J, we have that
P"
E,0.805111821086262,"j∈J λη,j = P"
E,0.8083067092651757,"j∈J λη,j1′ ˜wJ\{j}
η
= 1′ ˜wJ
η = 1. This establishes the main claim of the proposition
for penalized synthetic control."
E,0.8115015974440895,"As the fifth and final step, we derive the main result on minimum-norm synthetic control from the
above results on penalized synthetic control. Consider some sequence (ηι)∞
ι=1 in (0, ∞) with ηι →0,
and for every ι apply the previous step to the penalized synthetic control estimator with penalty ηι
to obtain a weight vector ληι ∈ΛJ =

λ ∈[0, 1]J; PJ
j=1 λj = 1
	
. Since ΛJ is compact, (ληι)∞
ι=1
must have a converging subsequence with some limit λ ∈ΛJ. Using the limit along this subsequence,
we have that"
E,0.8146964856230032,"ˆwJ = lim
ι→∞ˆwJ
ηι = lim
ι→∞ X"
E,0.8178913738019169,"j∈J
ληι,j ˜wJ\{j}
ηι
=
X j∈J"
E,0.8210862619808307," 
lim
ι→∞ληι,j
 
lim
ι→∞˜wJ\{j}
ηι

=
X"
E,0.8242811501597445,"j∈J
λj ˜wJ\{j}."
E,0.8274760383386581,"Proof of Proposition 5. By Jensen’s inequality applied to an average over the bounds in (3),"
E,0.8306709265175719,"E[(y −ˆf ∗(x))2] ≤
1
|J|! X"
E,0.8338658146964856,"π
E[(y −ˆf ∗
π(x))2] ≤
X j∈J
E"
E,0.8370607028753994,"""
1
|J|! X"
E,0.8402555910543131,"π
ˆλπ(j)
|
{z
}
= 1 |J|"
E,0.8434504792332268,"(y −ˆf j(x))2
# ."
E,0.8466453674121406,"B
Details of the Empirical Illustrations"
E,0.8498402555910544,"B.1
Many-Regressor Linear Least-Squares on CPS Data"
E,0.853035143769968,"We utilize the publicly available5 CPS control and NSW experimental control datasets, drawn from
the study presented in LaLonde (1986) as used by Dehejia and Wahba (1999, 2002). The resulting
data has 15,992 observations for CPS and 260 for NSW, with both datasets containing an identical
set of variables, detailed in Table 1."
E,0.8562300319488818,"Variable
Data Type
Description"
E,0.8594249201277955,"age
Discrete
Age
education
Discrete
Years of education
black
Dummy
Black
hispanic
Dummy
Hispanic
married
Dummy
Marital status
nodegree
Dummy
Lack of college degree
re74
Continuous
Income in 1974
re75
Continuous
Income in 1975
re78
Continuous
Income in 1978"
E,0.8626198083067093,Table 1: CPS and NSW dataset variables
E,0.865814696485623,"We use re78 as the outcome variable and all other variables as covariates. In order to achieve high
dimensionality, we first discretize the continuous income covariates into 50 bins via quantile binning.
We then construct a series of dummies for each discrete variable, corresponding to indicators for
each discretized value. We then interact all these dummy variables, as well as those covariates
which were originally dummies, taking care not to interact those which are mutually exclusive (e.g.
originating from the same original covariate or corresponding to race). We then drop any interactions
that are zero for all observations in the data. The resulting transformed dataset contains 8,408 dummy
covariates, as well as the unmodified outcome variable. In order to ensure that the covariate matrix is
full row rank for an arbitrary subset of columns, we go on to add iid N(0, 0.0004) noise to each of
the covariate values (again leaving the outcome variable unaffected). We then select a random subset
of 3,000 observations from the CPS dataset as our in-sample set, using the 260 NSW observations as
our out-of-sample set."
E,0.8690095846645367,"For fitting models of varying complexity, we randomly permute the order of the columns of the
covariate matrix; denote the resulting matrix as X. We then add an intercept and iterate over varying
levels of complexity ℓ, ranging from 1 to 8,409, corresponding to the number of covariates that we
will use for estimation. We then estimate the OLS coefficient vector ˆβℓ= Xℓ†y, where † denotes the
Moore–Penrose pseudoinverse."
E,0.8722044728434505,"To evaluate performance, we first select a sample size m and then draw 1000 samples of size m
from the NSW set. We then take define our evaluation metric to be the RMSE across these 1000
samples, with error defined as the difference between the average predicted outcome and the true
average outcome for each sample:"
E,0.8753993610223643,"RMSE(ℓ, m) ="
E,0.8785942492012779,"v
u
u
t
1
1000"
X,0.8817891373801917,"1000
X j=1"
X,0.8849840255591054,""" 
1
m m
X"
X,0.8881789137380192,"i=1
y∗
ji ! −"
M,0.8913738019169329,"1
m m
X"
M,0.8945686900958466,"i=1
x∗⊤
ji ˆβℓ
!#2"
M,0.8977635782747604,"5users.nber.org/~rdehejia/data/.nswdata2.html.
We
use
the
files
corresponding
to
cps_controls.txt and nswre74_control.txt."
M,0.9009584664536742,"This metric assesses the ability of the given model to accurately predict mean outcomes, a quantity of
direct relevance to ATE estimaiton. Where X∗, y∗denote the out-of-sample covariate and outcome
variables, respectively, and can correspond to either the CPS or NSW held-out samples. The subscript
ji refers to the ith observation of the jth sample of size m. In order to smooth out the effects of the
random ordering of columns, we repeat this exercise for five different random orderings and take a
pointwise average to obtain smooth RMSE vs. complexity curves (Figure 1b). A zoomed-in version
of this plot, focusing on the highly overparametrized regime and emphasizing the ability of some of
the depicted models to outperform simple baselines can be found in Figure 7."
M,0.9041533546325878,"5000
5500
6000
6500
7000
7500
8000
8500
Number of covariates 0 1 2 3 4 5"
M,0.9073482428115016,ATE RMSE (thousands $)
M,0.9105431309904153,"m = 5
m = 50
m = 100"
M,0.9137380191693291,Figure 7: Average RMSE for linear regression for a varying number of covariates (ℓ> 5000).
M,0.9169329073482428,"As a further illustration of the effects of increasing complexity, Figure 8 shows the average norm of the
model coefficients across different model complexities, with the average taken over random covariate
orderings as before. Starting small, model coefficients initially grow (in terms of their Euclidean
norm), reaching their peak at the interpolation threshold. To the right of the interpolation threshold,
the norm of the model coefficients decreases mechanically, since the estimator now minimizes the
norm among all interpolating solutions with fewer and fewer sparsity constraints."
M,0.9201277955271565,"0
2000
4000
6000
8000
Number of covariates 0 250 500 750 1000 1250 1500 1750 2000"
M,0.9233226837060703,Coefficient norm
M,0.9265175718849841,Figure 8: Average of the coefficient norm ∥ˆβJ∥for varying size of the set J of covariates.
M,0.9297124600638977,"B.2
Many-Unit Synthetic Control on Smoking Data"
M,0.9329073482428115,"For our synthetic-control exercise, we utilize public data from the Centers for Disease Control and
Prevention6 containing annual cost, revenue, tax, and quantity data for cigarette sales by state for
the years 1970 to 2019. We follow the approach of Abadie et al. (2010) in using synthetic control
to estimate per-capita cigarette pack consumption for the target state, California, as a function of
the other 49 states and Washington, D.C. For our evaluation, we utilize two years of data (1987 and
1988) as a hold-out sample and fit the model on three years (1984 to 1986). All of our data precedes
the year in which anti-smoking legislation took effect in California (1989)."
M,0.9361022364217252,"We begin by selecting a random subset of 20 states to serve as our donor pool, for computational
tractability. We then select a level of complexity ℓand select a subset of ℓstates from the chosen
20. Using that subset, we then estimate synthetic control weights based on the in-sample period,"
M,0.939297124600639,6chronicdata.cdc.gov/Policy/The-Tax-Burden-on-Tobacco-1970-2019/7nwe-3aj9.
M,0.9424920127795527,"choosing convex weight vector ˆwℓ∈W = {w ∈[0, 1]N; Pn
i=1 wi = 1} as described in Section 3.1:"
M,0.9456869009584664,ˆwℓ= arg min
M,0.9488817891373802,"w∈c
Wℓ
∥w∥
c
Wℓ=
arg min
w∈W;wj=0∀j /∈J"
X,0.952076677316294,"1986
X"
X,0.9552715654952076,"t=1984
(y0t − ℓ
X"
X,0.9584664536741214,"i=1
wiyit)2."
X,0.9616613418530351,"Here, y0 denotes the target state, California. We then compute the out-of-sample prediction error as:"
X,0.9648562300319489,RMSE(ℓ) =
X,0.9680511182108626,"v
u
u
t1 2"
X,0.9712460063897763,"1988
X"
X,0.9744408945686901,"t=1987
(y0t − ℓ
X"
X,0.9776357827476039,"i=1
ˆwℓ
iyit)2"
X,0.9808306709265175,"We then iterate over all
 20
ℓ

possible combinations of donor units for the given complexity level and
take the average RMSE value to be the predictive error for the given complexity level. We vary ℓ
from 1 to 20 to trace out the curve of synthetic control prediction risk vs. complexity (Figure 4)."
X,0.9840255591054313,"As a robustness check, we also consider synthetic control with 10 pre-treatment periods, where the
included control states are chosen among all 50 available donors (49 other states and Washington,
D.C.). In particular, at each complexity level ℓwe consider min{
 50
ℓ

, 10000} combinations of donor
units. The results are qualitatively similar, and provided in Figure 9."
X,0.987220447284345,"0
10
20
30
40
50
Number of control states 0 5 10 15 20"
X,0.9904153354632588,RMSE (daily cigarette packs)
X,0.9936102236421726,"Out-of-time
Training"
X,0.9968051118210862,"Figure 9: Average out-of-time (blue) and training (orange) RMSE for synthetic control for a varying
number of control units as in Figure 4, but with ten pre-treatment periods and control units chosen
randomly among all available donors."
