Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015923566878980893,"Multitask learning for face forgery detection has experienced impressive successes
1"
ABSTRACT,0.0031847133757961785,"in recent years. Nevertheless, the semantic relationships among different forgery
2"
ABSTRACT,0.004777070063694267,"detection tasks are generally overlooked in previous methods, which weakens
3"
ABSTRACT,0.006369426751592357,"knowledge transfer across tasks. Moreover, previously adopted multitask learning
4"
ABSTRACT,0.007961783439490446,"schemes require human intervention on allocating model capacity to each task and
5"
ABSTRACT,0.009554140127388535,"computing the loss weighting, which is bound to be suboptimal. In this paper,
6"
ABSTRACT,0.011146496815286623,"we aim at automated multitask learning for face forgery detection from a joint
7"
ABSTRACT,0.012738853503184714,"embedding perspective. We first define a set of coarse-to-fine face forgery detection
8"
ABSTRACT,0.014331210191082803,"tasks based on face attributes at different semantic levels. We describe the ground-
9"
ABSTRACT,0.01592356687898089,"truth for each task via a textural template, and train two encoders to jointly embed
10"
ABSTRACT,0.01751592356687898,"visual face images and textual descriptions in the shared feature space. In such a
11"
ABSTRACT,0.01910828025477707,"manner, the semantic closeness between two tasks is manifested as the distance
12"
ABSTRACT,0.020700636942675158,"in the learned feature space. Moreover, the capacity of the image encoder can be
13"
ABSTRACT,0.022292993630573247,"automatically allocated to each task through end-to-end optimization. Through joint
14"
ABSTRACT,0.02388535031847134,"embedding, face forgery detection can be performed by maximizing the feature
15"
ABSTRACT,0.025477707006369428,"similarity between the test face image and candidate textual descriptions. Extensive
16"
ABSTRACT,0.027070063694267517,"experiments show that the proposed method improves face forgery detection in
17"
ABSTRACT,0.028662420382165606,"terms of generalization to novel face manipulations. In addition, our multitask
18"
ABSTRACT,0.030254777070063694,"learning method renders some degree of model interpretation by providing human-
19"
ABSTRACT,0.03184713375796178,"understandable explanations.
20"
INTRODUCTION,0.03343949044585987,"1
Introduction
21"
INTRODUCTION,0.03503184713375796,"The emergence of deep generative models [1, 34, 67, 71] has significantly simplified and automated
22"
INTRODUCTION,0.03662420382165605,"the process of generating realistic counterfeit face images, popularly known as DeepFake. The
23"
INTRODUCTION,0.03821656050955414,"prevalence of falsified face images can erode the reliability and credibility of digital visual information.
24"
INTRODUCTION,0.03980891719745223,"Additionally, the exploitation and manipulation of such technologies pose a threat to individual rights
25"
INTRODUCTION,0.041401273885350316,"and national security.
26"
INTRODUCTION,0.042993630573248405,"Traditional DeepFake detectors were largely influenced by classic photo forensics [21] to expose
27"
INTRODUCTION,0.044585987261146494,"forgery traces by examining statistical anomalies [51, 58], visual artifacts [32, 46, 50, 51, 59],
28"
INTRODUCTION,0.04617834394904458,"and physical and geometric inconsistencies [15, 33, 35, 56]. With the rapid development of deep
29"
INTRODUCTION,0.04777070063694268,"learning, there has recently been a growing consensus on exploiting multitask learning for face
30"
INTRODUCTION,0.04936305732484077,"forgery detection [8, 10, 19, 41, 55, 80, 81]. The underlying assumption is that the primary task
31"
INTRODUCTION,0.050955414012738856,"(i.e., global face forgery classification) is likely to benefit from other highly relevant auxiliary tasks
32"
INTRODUCTION,0.052547770700636945,"through knowledge transfer. Representative auxiliary tasks include manipulation type (and degree)
33"
INTRODUCTION,0.054140127388535034,"classification [10], manipulation parameter estimation [75], blending boundary detection [41], spatial
34"
INTRODUCTION,0.05573248407643312,"forgery localization [28], face reconstruction [8], and face segmentation [55].
35"
INTRODUCTION,0.05732484076433121,"The prevailing multitask learning paradigm for face forgery detection follows a discriminative
36"
INTRODUCTION,0.0589171974522293,"approach, predicting multiple target outputs, one for each task, directly from the input face image.
37"
INTRODUCTION,0.06050955414012739,"Such a paradigm suffers from two main drawbacks. First, semantic relationships across tasks are
38"
INTRODUCTION,0.06210191082802548,"overlooked, which weakens knowledge transfer. For example, irrelevant information (e.g., every
39"
INTRODUCTION,0.06369426751592357,"detail of the face image in face reconstruction [8]) may be transferred across tasks. Second, extensive
40"
INTRODUCTION,0.06528662420382166,"human expertise should be involved, when determining task-agnostic (and task-specific) model
41"
INTRODUCTION,0.06687898089171974,"parameters and the loss weightings.
42"
INTRODUCTION,0.06847133757961783,"In this paper, we explore multitask learning for face forgery detection from a joint embedding
43"
INTRODUCTION,0.07006369426751592,"perspective [38]. In the joint embedding architecture, both the input and the target output are encoded
44"
INTRODUCTION,0.07165605095541401,"into latent representations in the shared feature space such that the irrelevant information can be
45"
INTRODUCTION,0.0732484076433121,"discarded from feature encoding. More importantly, the semantic closeness between two tasks can
46"
INTRODUCTION,0.07484076433121019,"be naturally modeled as the distance in the learned feature space, which is subsequently end-to-
47"
INTRODUCTION,0.07643312101910828,"end optimized to facilitate knowledge transfer across multiple tasks. Meanwhile, joint embedding
48"
INTRODUCTION,0.07802547770700637,"gives us a great opportunity to automate multitask learning in terms of allocating model capacity
49"
INTRODUCTION,0.07961783439490445,"(i.e., specifying task-agnostic and task-specific model parameters). In the context of face forgery
50"
INTRODUCTION,0.08121019108280254,"detection, the parameters of the face image encoder are shared across all tasks, whose capacity is
51"
INTRODUCTION,0.08280254777070063,"dynamically adjusted through end-to-end optimization. In addition, the multitask loss weightings can
52"
INTRODUCTION,0.08439490445859872,"be automatically computed in either theoretical [45, 65] or empirical [13, 36, 47] ways.
53"
INTRODUCTION,0.08598726114649681,"More concretely, we first introduce three coarse-to-fine face forgery detection tasks based on face
54"
INTRODUCTION,0.0875796178343949,"(2) A photo of a face with the global
attribute of expression altered"
INTRODUCTION,0.08917197452229299,(1) A photo of a fake face
INTRODUCTION,0.09076433121019108,"(3) A photo of a face with the local
attribute of mouth altered"
INTRODUCTION,0.09235668789808917,"Figure 1: Illustration of a fake face image
with its textural descriptions of three coarse-
to-fine face forgery detection tasks at different
semantic levels."
INTRODUCTION,0.09394904458598727,"attributes at different semantic levels. Leveraging
55"
INTRODUCTION,0.09554140127388536,"the recent advances in vision-language correspon-
56"
INTRODUCTION,0.09713375796178345,"dence as joint embedding [61], we encode the binary
57"
INTRODUCTION,0.09872611464968153,"labels of the three tasks via textural prompts, and
58"
INTRODUCTION,0.10031847133757962,"thus the semantic dependencies among tasks can be
59"
INTRODUCTION,0.10191082802547771,"represented with the textual embeddings in the rep-
60"
INTRODUCTION,0.1035031847133758,"resentation space. Fig. 1 shows an example, in which
61"
INTRODUCTION,0.10509554140127389,"we describe a fake face image with a set of coarse-
62"
INTRODUCTION,0.10668789808917198,"to-fine textual descriptions: 1) “A photo of a fake
63"
INTRODUCTION,0.10828025477707007,"face,” 2) “A photo of a face with the global attribute
64"
INTRODUCTION,0.10987261146496816,"of expression altered,” and 3) “A photo of a face with
65"
INTRODUCTION,0.11146496815286625,"the local attribute of mouth altered.” By jointly embedding the face image and all its associated
66"
INTRODUCTION,0.11305732484076433,"textural prompts through a popular vision-language model - CLIP [61], face forgery detection can
67"
INTRODUCTION,0.11464968152866242,"then be performed by maximizing the vision-language correspondence.
68"
INTRODUCTION,0.11624203821656051,"Our contributions are threefold. First, we formulate multitask face forgery detection from a joint
69"
INTRODUCTION,0.1178343949044586,"embedding perspective. Second, we define a set of coarse-to-fine face forgery detection tasks with
70"
INTRODUCTION,0.11942675159235669,"corresponding textural templates to describe (fake) face images. Compared to previous multitask
71"
INTRODUCTION,0.12101910828025478,"learning schemes, our instantiation gives rise to a more interpretable face forgery detector. Third,
72"
INTRODUCTION,0.12261146496815287,"we conduct extensive experiments on five popular face forgery detection datasets, and show that our
73"
INTRODUCTION,0.12420382165605096,"method performs favorably against state-of-the-art (SOTA) detectors in terms of generalization to
74"
INTRODUCTION,0.12579617834394904,"novel face manipulations.
75"
RELATED WORK,0.12738853503184713,"2
Related Work
76"
RELATED WORK,0.12898089171974522,"In this section, we briefly review the literature on face forgery detection, multitask learning, and joint
77"
RELATED WORK,0.1305732484076433,"embedding architectures.
78"
FACE FORGERY DETECTION,0.1321656050955414,"2.1
Face Forgery Detection
79"
FACE FORGERY DETECTION,0.1337579617834395,"Many face forgery detection methods usually explore the specific clues to detect the forgery inspired
80"
FACE FORGERY DETECTION,0.13535031847133758,"by the traditional photo forensics [15, 32, 33, 35, 46, 50, 51, 56], in which they detect eye blink-
81"
FACE FORGERY DETECTION,0.13694267515923567,"ing [42], head pose [77], pupil shape [24], lipreading [26], statistical anomalies [43, 60, 66, 81],
82"
FACE FORGERY DETECTION,0.13853503184713375,"corneal specularity [29], and idiosyncratic behavioral patterns of a well-known person [3]. In
83"
FACE FORGERY DETECTION,0.14012738853503184,"recent years, there is a growing consensus of exploiting multitask learning on face forgery detec-
84"
FACE FORGERY DETECTION,0.14171974522292993,"tion [8, 10, 41, 55, 81]. Besides the main face forgery classification task, these methods include
85"
FACE FORGERY DETECTION,0.14331210191082802,"auxiliary tasks to get performance improvement by knowledge transfer across tasks, such as manipula-
86"
FACE FORGERY DETECTION,0.1449044585987261,"tion type (and degree) classification [10], manipulation parameter estimation [75], blending boundary
87"
FACE FORGERY DETECTION,0.1464968152866242,"detection [41], spatial forgery localization [28], face reconstruction [8], and face segmentation [55].
88"
FACE FORGERY DETECTION,0.1480891719745223,"With the development of deep learning, some advanced networks are employed to facilitate the face
89"
FACE FORGERY DETECTION,0.14968152866242038,"forgery detection based on multiple tasks, such as two-stream CNN [82], self-attention model [80],
90"
FACE FORGERY DETECTION,0.15127388535031847,"and vision transformers [19]. Additionally, more advanced training strategies are also utilized to
91"
FACE FORGERY DETECTION,0.15286624203821655,"enhance the forgery detectors, including adversarial learning [10], reconstruction learning [8], and
92"
FACE FORGERY DETECTION,0.15445859872611464,"meta learning [11]. However, the previous learning paradigm and human intervention are sub-optimal
93"
FACE FORGERY DETECTION,0.15605095541401273,"for multitask learning on face forgery detection. In this paper, we explore an automated multitask
94"
FACE FORGERY DETECTION,0.15764331210191082,"learning method for face forgery detection from the joint embedding perspective, where multiple
95"
FACE FORGERY DETECTION,0.1592356687898089,"tasks are encoded into the language prompts, and vision-language correspondence is transferred
96"
FACE FORGERY DETECTION,0.160828025477707,"across tasks as the primary knowledge.
97"
MULTITASK LEARNING,0.1624203821656051,"2.2
Multitask Learning
98"
MULTITASK LEARNING,0.16401273885350318,"Multitask learning aims to jointly learn multiple related tasks to improve the generalization perfor-
99"
MULTITASK LEARNING,0.16560509554140126,"mance of all tasks by leveraging the knowledge contained in each [79]. Two main groups are model
100"
MULTITASK LEARNING,0.16719745222929935,"parameter sharing and loss weighting. The former involves both manual specifications of shared pa-
101"
MULTITASK LEARNING,0.16878980891719744,"rameters [4, 22, 37, 54] and learning to determine parameters for specific tasks [52, 64, 68, 74]. Loss
102"
MULTITASK LEARNING,0.17038216560509553,"weighting is typically divided as follows: Pareto Optimization (PO) methods and weight adaption
103"
MULTITASK LEARNING,0.17197452229299362,"methods. PO methods formulate multitask learning as a multi-objective optimization [45, 65], and
104"
MULTITASK LEARNING,0.1735668789808917,"find a Pareto stationary solution for the optimal loss weighting. Weight adoption methods adaptively
105"
MULTITASK LEARNING,0.1751592356687898,"adjust the loss weights during training based on pre-defined heuristics, such as uncertainty [36],
106"
MULTITASK LEARNING,0.1767515923566879,"gradient normalization [13], and loss descending rate [47]. In this paper, we consider multitask
107"
MULTITASK LEARNING,0.17834394904458598,"learning from the joint embedding perspective, in which the semantic closeness between tasks can be
108"
MULTITASK LEARNING,0.17993630573248406,"manifested as the distance in the learned feature space. Moreover, we assume all parameters in the
109"
MULTITASK LEARNING,0.18152866242038215,"image encoder are shared, whose capacity is dynamically allocated to each task during end-to-end
110"
MULTITASK LEARNING,0.18312101910828024,"optimization. We also adopt the method in [47] for dynamic loss weighting.
111"
JOINT EMBEDDING ARCHITECTURES,0.18471337579617833,"2.3
Joint Embedding Architectures
112"
JOINT EMBEDDING ARCHITECTURES,0.18630573248407642,"Joint embedding architectures (JEA) [38] aim at learning to output similar embeddings for compatible
113"
JOINT EMBEDDING ARCHITECTURES,0.18789808917197454,"inputs, x and y, and dissimilar embeddings for incompatible inputs, which is different from the
114"
JOINT EMBEDDING ARCHITECTURES,0.18949044585987262,"discriminative approaches that predict y directly from x. Becker et al. [6] propose the first JEA for
115"
JOINT EMBEDDING ARCHITECTURES,0.1910828025477707,"maximizing mutual information between representations from two views of the same scene. Later on,
116"
JOINT EMBEDDING ARCHITECTURES,0.1926751592356688,"Bromley et al. [7] propose a contrastive method of JEA for signatures verification. After a long hiatus,
117"
JOINT EMBEDDING ARCHITECTURES,0.1942675159235669,"JEA has been re-explored in face verification [14] and recognition [69], dimensionality reduction [25],
118"
JOINT EMBEDDING ARCHITECTURES,0.19585987261146498,"and video feature learning [70]. With the emergence of self-supervised learning, the use of JEA has
119"
JOINT EMBEDDING ARCHITECTURES,0.19745222929936307,"explored in recent years with methods training on contrastively (e.g., PIRL [53], MoCo [27], and
120"
JOINT EMBEDDING ARCHITECTURES,0.19904458598726116,"SimCLR [12]) or non-contrastively (e.g., BYOL [23], Barlow Twins [78], and I-JEPA [5]). More
121"
JOINT EMBEDDING ARCHITECTURES,0.20063694267515925,"recently, the emerging vision-language foundation models [30, 61] can also be grouped into JEA,
122"
JOINT EMBEDDING ARCHITECTURES,0.20222929936305734,"in which two separate encoders encode the compatible visual (i.e., x) and textual (i.e., y) inputs
123"
JOINT EMBEDDING ARCHITECTURES,0.20382165605095542,"into similar embeddings and contrast incompatible visual and textual embeddings. In this paper, we
124"
JOINT EMBEDDING ARCHITECTURES,0.2054140127388535,"use CLIP [61], a joint vision-language model pretrained on massive image-text pairs, to implement
125"
JOINT EMBEDDING ARCHITECTURES,0.2070063694267516,"the JEA to aid DeepFake detection by vision-language correspondence in the embedding space.
126"
JOINT EMBEDDING ARCHITECTURES,0.2085987261146497,"Moreover, we end-to-end fine-tune the CLIP in the context of automated multitask learning.
127"
METHOD,0.21019108280254778,"3
Method
128"
METHOD,0.21178343949044587,"In this section, we present multitask learning for face forgery detection using a joint embedding
129"
METHOD,0.21337579617834396,"approach, including preliminaries of the problem formulation, language prompts over multiple tasks,
130"
METHOD,0.21496815286624205,"and specifications of loss functions. The main joint embedding framework for face forgery detection
131"
METHOD,0.21656050955414013,"is shown in Fig. 2.
132"
PRELIMINARIES,0.21815286624203822,"3.1
Preliminaries
133"
PRELIMINARIES,0.2197452229299363,"Given a face image x ∈RN, a face forgery detector fθ : RN 7→R aims to predict a binary label
134"
PRELIMINARIES,0.2213375796178344,"y for the authenticity of x, i.e., 0 as the real or 1 as the fake. Considering that existing forged face
135"
PRELIMINARIES,0.2229299363057325,"images are mainly generated by modifying face components/attributes, we include two other related
136"
PRELIMINARIES,0.22452229299363058,"tasks - global face manipulation detection and local face manipulation detection. We consider three
137"
PRELIMINARIES,0.22611464968152867,"A photo of a {b} face. 
A photo of a {b} face."
PRELIMINARIES,0.22770700636942676,A photo of a face with
PRELIMINARIES,0.22929936305732485,{a} attribute
PRELIMINARIES,0.23089171974522293,modified.
PRELIMINARIES,0.23248407643312102,Contrastive Prompt Pairs
PRELIMINARIES,0.2340764331210191,"A photo of a {b} face. 
A photo of a {b} face."
PRELIMINARIES,0.2356687898089172,A photo of a face with
PRELIMINARIES,0.2372611464968153,{a} attribute
PRELIMINARIES,0.23885350318471338,modified.
PRELIMINARIES,0.24044585987261147,"Text
Encoder
Textual 
Embeddings"
PRELIMINARIES,0.24203821656050956,"Textual 
Embeddings"
PRELIMINARIES,0.24363057324840764,"Textual 
Embeddings"
PRELIMINARIES,0.24522292993630573,"Textual 
Embeddings"
PRELIMINARIES,0.24681528662420382,Input Face Image
PRELIMINARIES,0.2484076433121019,"Image 
Encoder"
PRELIMINARIES,0.25,Binary Category
PRELIMINARIES,0.2515923566878981,Probability
PRELIMINARIES,0.2531847133757962,Global Category
PRELIMINARIES,0.25477707006369427,Probability
PRELIMINARIES,0.25636942675159236,Local Category
PRELIMINARIES,0.25796178343949044,Probability
PRELIMINARIES,0.25955414012738853,Fidelity Loss
PRELIMINARIES,0.2611464968152866,Softmax
PRELIMINARIES,0.2627388535031847,"Visual
Embedding"
PRELIMINARIES,0.2643312101910828,A photo of a  face with
PRELIMINARIES,0.2659235668789809,the global attribute of
PRELIMINARIES,0.267515923566879,{g} altered
PRELIMINARIES,0.26910828025477707,"A photo of a face with 
the global attribute of"
PRELIMINARIES,0.27070063694267515,{g} unaltered
PRELIMINARIES,0.27229299363057324,"Textual 
Embeddings"
PRELIMINARIES,0.27388535031847133,"Cosine 
Similarity"
PRELIMINARIES,0.2754777070063694,Figure 2: Proposed joint embedding paradigm for multitask face forgery detection.
PRELIMINARIES,0.2770700636942675,"face attributes (i.e., expression, identity, and physical consistency1) for global face manipulations, and
138"
PRELIMINARIES,0.2786624203821656,"four face attributes (i.e., eye, illumination, mouth, and nose) for local face manipulations. Notably, a
139"
PRELIMINARIES,0.2802547770700637,"face image may contain multiple attribute labels.
140"
MULTITASK LANGUAGE PROMPTS,0.2818471337579618,"3.2
Multitask Language Prompts
141"
MULTITASK LANGUAGE PROMPTS,0.28343949044585987,"For each face attribute label from multiple tasks, we encode the ground-truth labels via language
142"
MULTITASK LANGUAGE PROMPTS,0.28503184713375795,"prompts. In specific, we design textual templates as follows. 1) binary level: a photo of a {c} face,
143"
MULTITASK LANGUAGE PROMPTS,0.28662420382165604,"where c ∈C = {real, fake}; 2) global-attribute level: A photo of a face with the global attribute of
144"
MULTITASK LANGUAGE PROMPTS,0.28821656050955413,"{g} altered, where g ∈G = {expression, identity, physical consistency}; and 3) local-attribute level:
145"
MULTITASK LANGUAGE PROMPTS,0.2898089171974522,"A photo of a face with the local attribute of {l} altered, where l ∈L = {eye, illumination, mouth,
146"
MULTITASK LANGUAGE PROMPTS,0.2914012738853503,"nose}. Inspired by contrastive methods [27, 53] in the joint embedding architecture, we also introduce
147"
MULTITASK LANGUAGE PROMPTS,0.2929936305732484,"contrastive language prompts, which are opposite in meaning to the original textual templates. Thus,
148"
MULTITASK LANGUAGE PROMPTS,0.2945859872611465,"we can have a contrastive prompts pair for each attribute label, as follows: global-attribute level:
149"
MULTITASK LANGUAGE PROMPTS,0.2961783439490446,"{(1) A photo of a face with the global attribute of {g} altered, (2) A photo of a face with the global
150"
MULTITASK LANGUAGE PROMPTS,0.29777070063694266,"attribute of {g} unaltered}; local-attribute level: {(1) A photo of a face with the local attribute of
151"
MULTITASK LANGUAGE PROMPTS,0.29936305732484075,"{l} altered, (2) A photo of a face with the local attribute of {l} unaltered}. Notably, the binary level
152"
MULTITASK LANGUAGE PROMPTS,0.30095541401273884,"prompts naturally have the property of contrastive prompt pairing. In this way, multiple tasks are
153"
MULTITASK LANGUAGE PROMPTS,0.30254777070063693,"encoded into a text corpus T , where each language prompt represents a ground-truth label y of the
154"
MULTITASK LANGUAGE PROMPTS,0.304140127388535,"corresponding task, and their semantic closeness can be learned through joint embedding.
155"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3057324840764331,"3.3
Multitask Learning via Joint Embedding
156"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3073248407643312,"Joint Embedding Formulation. Given the input face image x and the set of possible outputs Y,
157"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3089171974522293,"we predict the output by minimizing an energy-based model [39], i.e., ˆy = arg miny∈Y E(x, y), in
158"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3105095541401274,"the joint embedding architecture. In this paper, we construct E by two encoders: one image encoder
159"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.31210191082802546,"fϕ : RN 7→RK for encoding the face image and one text encoder gφ : T 7→RK for encoding the
160"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.31369426751592355,"language prompts, parameterized by ϕ and φ, respectively.
161"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.31528662420382164,"The ideal energy landscape of joint embedding satisfies that the energy is low for similar embeddings
162"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.31687898089171973,"of compatible inputs, while energy is high for dissimilar embeddings [39]. Thus, we calculate
163"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3184713375796178,"the probability of similarity ˆp(·|x) between the visual embedding and textual embeddings for the
164"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3200636942675159,"following optimization. Let u ∈RK be the visual embedding, and let v ∈RK and ¯v ∈RK be the
165"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.321656050955414,"textual embeddings from the two prompts opposing in meaning, we then estimate ˆp(·|x) as
166"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3232484076433121,"ˆp(·|x) =
1
1 + e−(s−¯s) ,
(1)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3248407643312102,"where
167"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.32643312101910826,"s = ⟨u, v⟩"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.32802547770700635,"∥u∥∥v∥
and
¯s = ⟨u, ¯v⟩"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.32961783439490444,"∥u∥∥¯v∥.
(2)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.33121019108280253,1We refer the interested readers to the Appendix for the detailed explanations.
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3328025477707006,"⟨·, ·⟩denotes the inner product and ∥· ∥represents the ℓ2-norm. The probability ˆp(·|x) is the
168"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3343949044585987,"abbreviation of ˆp(c|x), ˆp(g|x), and ˆp(l|x) according to a specific task, and a larger probability
169"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3359872611464968,"indicates a closer match to the corresponding semantic meaning of v.
170"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3375796178343949,"Losses for Multitask Learning. We use the statistical distance measure in the form of fidelity
171"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.339171974522293,"loss [73] to calculate the losses for multitask learning. Given the predicted category probability
172"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.34076433121019106,"ˆp(c|x), we design the loss at the binary level as
173"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.34235668789808915,"ℓ1(x; θ) = 1 −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.34394904458598724,"p(c|x)ˆp(c|x) −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.34554140127388533,"(1 −p(c|x))(1 −ˆp(c|x)),
(3)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3471337579617834,"where θ = {ϕ, φ} indicates the learnable parameters in image and language encoders, and p(c|x) = 1
174"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3487261146496815,"if x belongs to the c category or otherwise we have p(c|x) = 0. In our setting, a face image can be
175"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3503184713375796,"assigned with labels regarding one or more global face attribute manipulations, which forms a typical
176"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3519108280254777,"multi-label classification problem. Therefore, the averaged loss at the global-attribute level can be
177"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3535031847133758,"defined as follows,
178"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.35509554140127386,ℓ2(x; θ) = 1 |G| X g∈G
MULTITASK LEARNING VIA JOINT EMBEDDING,0.35668789808917195,"
1 −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.35828025477707004,"p(g|x)ˆp(g|x) −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.35987261146496813,"(1 −p(g|x))(1 −ˆp(g|x))

,
(4)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3614649681528662,"where p(g|x) = 1 if x belongs to the g category, otherwise we have p(g|x) = 0. Since the
179"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3630573248407643,"manipulations over different local face attributes may appear in one face image, we also consider it
180"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3646496815286624,"as a multi-label classification task, and the loss at the local-attribute level is:
181"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3662420382165605,ℓ3(x; θ) = 1 |L| X l∈L
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3678343949044586,"
1 −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.36942675159235666,"p(l|x)ˆp(l|x) −
p"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37101910828025475,"(1 −p(l|x))(1 −ˆp(l|x))

,
(5)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37261146496815284,"where p(l|x) = 1 if x belongs to the l category.
182"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37420382165605093,"Given a minibatch of training data B at the t-th iteration, we evaluate the overall loss function via the
183"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37579617834394907,"weighted sum of the individual losses in different levels as follows,
184"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37738853503184716,"ℓ(B, t; θ) = 1 |B| X x∈B"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.37898089171974525," 
λ1(t)ℓ1(x; θ) + λ2(t)ℓ2(x; θ) + λ3(t)ℓ3(x; θ)

.
(6)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.38057324840764334,"Here, the weighting vector λ(t) = [λ1(t), λ2(t), λ3(t)]⊺at the t-th iteration is automatically com-
185"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3821656050955414,"puted according to the relative descending rate [47]:
186"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3837579617834395,"λi(t) =
3 exp (wi(t −1)/τ)
P3
j=1 exp (wj(t −1)/τ)
, where wi(t −1) = ℓi(t −1)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3853503184713376,"ℓi(t −2),
(7)"
MULTITASK LEARNING VIA JOINT EMBEDDING,0.3869426751592357,"and τ is a fixed temperature parameter.
187"
EXPERIMENTS,0.3885350318471338,"4
Experiments
188"
EXPERIMENTAL SETUP,0.39012738853503187,"4.1
Experimental Setup
189"
EXPERIMENTAL SETUP,0.39171974522292996,"Datasets. We adopt the widely used FF++ [63] dataset for training. It contains 1, 000 real videos,
190"
EXPERIMENTAL SETUP,0.39331210191082805,"among which 720 and 140 are used for training and validation, respectively, and the remaining
191"
EXPERIMENTAL SETUP,0.39490445859872614,"140 are reserved for testing. All videos are manipulated by four face forgery methods, including
192"
EXPERIMENTAL SETUP,0.3964968152866242,"Deepfakes [1], Face2Face [72], FaceSwap [2], and NeuralTexures [71], with three compression levels,
193"
EXPERIMENTAL SETUP,0.3980891719745223,"i.e., no compression (denoted as Raw), slight compression with quantization parameter QP = 23
194"
EXPERIMENTAL SETUP,0.3996815286624204,"(denoted as C23), and severe compression with QP = 40 (denoted as C40). Following [10, 11, 26],
195"
EXPERIMENTAL SETUP,0.4012738853503185,"C23 version is adopted by default in our experiments. We evaluate the generalizability of the
196"
EXPERIMENTAL SETUP,0.4028662420382166,"proposed method on four popular DeepFake benchmarks, including FaceShifter (FSh) [40], Celeb-DF
197"
EXPERIMENTAL SETUP,0.40445859872611467,"(CDF) [44], DeeperForensics-1.0 (DF-1.0) [31], and DeepFake Detection Challenge (DFDC) [18].
198"
EXPERIMENTAL SETUP,0.40605095541401276,"Implementation Details. To facilitate the multitask learning via joint embedding paradigm, we need
199"
EXPERIMENTAL SETUP,0.40764331210191085,"face images associated with the proposed textual templates. In this paper, we adopt FF++ [63] to
200"
EXPERIMENTAL SETUP,0.40923566878980894,"enrich the training data. Following the general generation procedures (i.e., detecting face and then
201"
EXPERIMENTAL SETUP,0.410828025477707,"blending two faces according to the region-of-interest mask) in [10, 41], we focus on supplementing
202"
EXPERIMENTAL SETUP,0.4124203821656051,"the tampering of “expression” on “eye” and individual face attribute that is linked to “physical
203"
EXPERIMENTAL SETUP,0.4140127388535032,"consistency”, i.e., “eye”, “illumination”, “mouth”, and “nose”. Face attribute manipulations associated
204"
EXPERIMENTAL SETUP,0.4156050955414013,"with other textual prompts are already included in FF++.
205"
EXPERIMENTAL SETUP,0.4171974522292994,"As for face pre-processing, we use RetinaFace [17] to detect faces and save the aligned face images
206"
EXPERIMENTAL SETUP,0.41878980891719747,"as input with a size of 317 × 317. As in [63], we only extract the largest face and use an enlarged
207"
EXPERIMENTAL SETUP,0.42038216560509556,"crop, 1.3× the tight crop produced by the face detector.
208"
EXPERIMENTAL SETUP,0.42197452229299365,"As for the training, we use CLIP [61] to implement the joint embedding architecture, where we
209"
EXPERIMENTAL SETUP,0.42356687898089174,"adopt ViT-B/32 [20] as the visual encoder and GPT-2 [62] with a base size of 63M-parameter as the
210"
EXPERIMENTAL SETUP,0.4251592356687898,"text encoder. We then train the model by minimizing the loss using AdamW [49] with a decoupled
211"
EXPERIMENTAL SETUP,0.4267515923566879,"weight decay of 1 × 10−3. The initial learning rate is set to 1 × 10−7, which changes following a
212"
EXPERIMENTAL SETUP,0.428343949044586,"cosine annealing schedule [48]. The model is optimized for 36 epochs with mini-batches of 32. Data
213"
EXPERIMENTAL SETUP,0.4299363057324841,"augmentation strategy is also applied during training, which is a common trick in the face forgery
214"
EXPERIMENTAL SETUP,0.4315286624203822,"detection [41, 76, 80], and details can be found in Sec. 4.3. A single NVIDIA RTX 3090 GPU is
215"
EXPERIMENTAL SETUP,0.43312101910828027,"used during training.
216"
COMPARISON WITH SOTA METHODS,0.43471337579617836,"4.2
Comparison with SOTA Methods
217"
COMPARISON WITH SOTA METHODS,0.43630573248407645,"We compare our method with the several SOTA methods, including Face X-ray [41], PCL [81],
218"
COMPARISON WITH SOTA METHODS,0.43789808917197454,"MADD [80], LipForensics [26], RECCE [8], SBI [66], ICT [19], SLADD [10], and OST [11],
219"
COMPARISON WITH SOTA METHODS,0.4394904458598726,"to demonstrate its superiority.
The test performance on five datasets are listed in Table 1.
220"
COMPARISON WITH SOTA METHODS,0.4410828025477707,"Table 1: Comparison results with the SOTA. All models are developed using
the training set of FF++ (or its augmented versions) and tested on the test
set of FF++ and other four independent datasets. The evaluation metric we
adopt is AUC (%). In the last column are the mean AUC numbers over datasets
including / excluding the FF++ test set to emphasize cross-dataset generalization
performance. The best results are highlighted in bold."
COMPARISON WITH SOTA METHODS,0.4426751592356688,"Method
FF++
CDF
FSh
DF-1.0
DFDC
Mean AUC
Face X-ray [41]
98.37
80.43
92.80
86.80
65.50
84.78 / 81.38
PCL [81]
99.11
81.80
–
99.40
67.50
86.95 / 82.90
MADD [80]
98.97
77.44
97.17
66.58
67.94
81.62 / 77.28
LipForensics [26]
99.90
82.40
97.10
97.60
73.50
89.54 / 87.65
RECCE [8]
99.32
68.71
70.58
74.10
69.06
76.35 / 70.61
SBI [66]
99.64
93.18
97.40
77.70
72.42
88.07 / 85.18
ICT [19]
90.22
85.71
95.97
93.57
76.74
88.44 / 88.00
SLADD [10]
98.40
79.70
–
77.80
76.05
82.99 / 77.85
OST [11]
98.20
74.80
–
93.08
77.73
84.95 / 81.87
Ours
98.49
89.02
98.68
93.38
82.06
92.33 / 90.79"
COMPARISON WITH SOTA METHODS,0.4442675159235669,"Table 1 shows that
221"
COMPARISON WITH SOTA METHODS,0.445859872611465,"many
methods
222"
COMPARISON WITH SOTA METHODS,0.44745222929936307,"do
not
perform
223"
COMPARISON WITH SOTA METHODS,0.44904458598726116,"satisfactorily
on
224"
COMPARISON WITH SOTA METHODS,0.45063694267515925,"face
forgery
de-
225"
COMPARISON WITH SOTA METHODS,0.45222929936305734,"tection, while the
226"
COMPARISON WITH SOTA METHODS,0.4538216560509554,"proposed method
227"
COMPARISON WITH SOTA METHODS,0.4554140127388535,"outperforms
all
228"
COMPARISON WITH SOTA METHODS,0.4570063694267516,"the recent SOTA,
229"
COMPARISON WITH SOTA METHODS,0.4585987261146497,"achieving 92.33%
230"
COMPARISON WITH SOTA METHODS,0.4601910828025478,"of
AUC
aver-
231"
COMPARISON WITH SOTA METHODS,0.46178343949044587,"aged
from
five
232"
COMPARISON WITH SOTA METHODS,0.46337579617834396,"test datasets and
233"
COMPARISON WITH SOTA METHODS,0.46496815286624205,"surpassing
the
234"
COMPARISON WITH SOTA METHODS,0.46656050955414013,"second best, i.e.,
235"
COMPARISON WITH SOTA METHODS,0.4681528662420382,"LipForensics,
by
236"
COMPARISON WITH SOTA METHODS,0.4697452229299363,"2.79% in the term
237"
COMPARISON WITH SOTA METHODS,0.4713375796178344,"of Mean AUC over
238"
COMPARISON WITH SOTA METHODS,0.4729299363057325,"datasets including
239"
COMPARISON WITH SOTA METHODS,0.4745222929936306,"FF++ [63].
For
240"
COMPARISON WITH SOTA METHODS,0.47611464968152867,"cross-dataset generalizability comparison, the proposed method also surpasses the second best (i.e.,
241"
COMPARISON WITH SOTA METHODS,0.47770700636942676,"ICT) and third best (i.e., LipForensics) by 2.79% and 3.14%, respectively. In addition, we also
242"
COMPARISON WITH SOTA METHODS,0.47929936305732485,"have several interesting observations. First, all the methods can achieve saturated performance in
243"
COMPARISON WITH SOTA METHODS,0.48089171974522293,"FF++ [63], while underperform in the rest datasets, such as CDF [44] and DFDC [18]. This suggests
244"
COMPARISON WITH SOTA METHODS,0.482484076433121,"that the forgery cues in FF++ are easier to spot and overfit by these forgery detectors. Second, SBI
245"
COMPARISON WITH SOTA METHODS,0.4840764331210191,"reports a very high AUC of 93.18% on CDF, while performing unsatisfactorily on DF-1.0 [31] and
246"
COMPARISON WITH SOTA METHODS,0.4856687898089172,"DFDC. Similar results are also demonstrated by PCL, which exhibits an exceedingly high AUC
247"
COMPARISON WITH SOTA METHODS,0.4872611464968153,"of 99.40% on DF-1.0 but underperforms in DFDC. This may arise due to the overfitting on the
248"
COMPARISON WITH SOTA METHODS,0.4888535031847134,"low-level features, such as statistical inconsistency (e.g., landmark and color mismatch). Third, all
249"
COMPARISON WITH SOTA METHODS,0.49044585987261147,"methods obtain relatively low scores on DFDC, which we attribute to the domain shift caused by
250"
COMPARISON WITH SOTA METHODS,0.49203821656050956,"significantly different filming conditions. However, our method achieves a relative satisfactory result
251"
COMPARISON WITH SOTA METHODS,0.49363057324840764,"with a score of 82.06%, surpassing the second best by 4.33%. In summary, the remarkable results
252"
COMPARISON WITH SOTA METHODS,0.49522292993630573,"validate the effectiveness and superiority of the proposed joint-embedding-based multitask learning
253"
COMPARISON WITH SOTA METHODS,0.4968152866242038,"for DeepFake detection.
254"
ROBUSTNESS ANALYSIS,0.4984076433121019,"4.3
Robustness Analysis
255"
ROBUSTNESS ANALYSIS,0.5,"In this subsection, we study the robustness performance of the proposed method. Following [31], we
256"
ROBUSTNESS ANALYSIS,0.5015923566878981,"consider four popular perturbations (i.e., Patch Substitution (Patch-Sub), additive white Gaussian
257"
ROBUSTNESS ANALYSIS,0.5031847133757962,"Noise contamination (Noise), Gaussian Blurring (Blur), and pixelation), and only four severity levels
258"
ROBUSTNESS ANALYSIS,0.5047770700636943,"Table 2: Robustness results to low-level image perturbations, including patch substitution (Patch-
Sub), Gaussian noise contamination (Noise), Gaussian blurring (Blur), and pixelation. We constrain
the robustness evaluation on the perturbation levels that do not noticeably distort the main face
semantics."
ROBUSTNESS ANALYSIS,0.5063694267515924,"Method
Clean AUC
Patch-Sub
Noise
Blur
Pixelation
Mean AUC
Drop Rate
Face X-ray [41]
98.37
97.72
51.13
88.98
92.33
82.54
-16.09%
CNND [76]
99.56
96.25
57.25
92.61
90.10
84.05
-15.58%
LipForensics [26]
99.90
88.63
80.00
96.62
96.63
90.47
-9.44%
Ours (w/o Aug)
98.66
92.47
73.12
55.20
57.17
69.49
-29.57%
Ours
98.49
97.65
82.85
87.31
90.70
89.63
-8.99%"
ROBUSTNESS ANALYSIS,0.5079617834394905,"(i.e., from level 1 to level 4) are considered in the experiments2. Two different models are evaluated
259"
ROBUSTNESS ANALYSIS,0.5095541401273885,"in this section, i.e., our model training without data augmentation (denoted as Ours (w/o Aug)) and
260"
ROBUSTNESS ANALYSIS,0.5111464968152867,"our model training with data augmentation strategy (denoted as Ours). In specific, when training with
261"
ROBUSTNESS ANALYSIS,0.5127388535031847,"0
1
2
3
4 50 60 70 80 90 100"
ROBUSTNESS ANALYSIS,0.5143312101910829,AUC (%)
ROBUSTNESS ANALYSIS,0.5159235668789809,Patch Substitution
ROBUSTNESS ANALYSIS,0.517515923566879,"0
1
2
3
4 50 60 70 80 90 100"
ROBUSTNESS ANALYSIS,0.5191082802547771,Gaussian Noise
ROBUSTNESS ANALYSIS,0.5207006369426752,"0
1
2
3
4
Severity Level 50 60 70 80 90 100"
ROBUSTNESS ANALYSIS,0.5222929936305732,AUC (%)
ROBUSTNESS ANALYSIS,0.5238853503184714,Gaussian Blur
ROBUSTNESS ANALYSIS,0.5254777070063694,"0
1
2
3
4
Severity Level 50 60 70 80 90 100"
ROBUSTNESS ANALYSIS,0.5270700636942676,Pixelation
ROBUSTNESS ANALYSIS,0.5286624203821656,"Ours
LipForensics"
ROBUSTNESS ANALYSIS,0.5302547770700637,"CNND
FaceX-ray"
ROBUSTNESS ANALYSIS,0.5318471337579618,"Xception
PatchForensics"
ROBUSTNESS ANALYSIS,0.5334394904458599,Chance
ROBUSTNESS ANALYSIS,0.535031847133758,"Figure 3: Robustness results in terms of AUC. Models
are trained on the train set of FF++ and tested on
perturbed test sets. Zoom in for clearer comparison."
ROBUSTNESS ANALYSIS,0.5366242038216561,"data augmentation strategy, each training data
262"
ROBUSTNESS ANALYSIS,0.5382165605095541,"is augmented with a probability of 0.3 by one
263"
ROBUSTNESS ANALYSIS,0.5398089171974523,"randomly chosen perturbation during train-
264"
ROBUSTNESS ANALYSIS,0.5414012738853503,"ing, in which severity level is randomly ap-
265"
ROBUSTNESS ANALYSIS,0.5429936305732485,"plied at level 1 or 2.
266"
ROBUSTNESS ANALYSIS,0.5445859872611465,"To begin, we first evaluate the robustness for
267"
ROBUSTNESS ANALYSIS,0.5461783439490446,"the model without data augmentation. We
268"
ROBUSTNESS ANALYSIS,0.5477707006369427,"find that the CLIP-based model is sensitive to
269"
ROBUSTNESS ANALYSIS,0.5493630573248408,"the perturbations to images, which we argue
270"
ROBUSTNESS ANALYSIS,0.5509554140127388,"that the vision-language correspondence is
271"
ROBUSTNESS ANALYSIS,0.552547770700637,"corrupted by perturbations. We then evaluate
272"
ROBUSTNESS ANALYSIS,0.554140127388535,"the model training with data augmentation.
273"
ROBUSTNESS ANALYSIS,0.5557324840764332,"In Table 2, we find that training with a slight
274"
ROBUSTNESS ANALYSIS,0.5573248407643312,"data augmentation can alleviate the model
275"
ROBUSTNESS ANALYSIS,0.5589171974522293,"sensitivity to the perturbations, and achieve
276"
ROBUSTNESS ANALYSIS,0.5605095541401274,"a satisfactory performance on average. More-
277"
ROBUSTNESS ANALYSIS,0.5621019108280255,"over, the model of Ours also maintains a sat-
278"
ROBUSTNESS ANALYSIS,0.5636942675159236,"isfactory performance on pixelation and Blur.
279"
ROBUSTNESS ANALYSIS,0.5652866242038217,"It is noteworthy that CNND [76] and Face
280"
ROBUSTNESS ANALYSIS,0.5668789808917197,"X-ray [41] also augment their training data
281"
ROBUSTNESS ANALYSIS,0.5684713375796179,"by compression and blurring during training,
282"
ROBUSTNESS ANALYSIS,0.5700636942675159,"thus leading to good robustness to perturba-
283"
ROBUSTNESS ANALYSIS,0.571656050955414,"tions of pixelation and Blur. Fig. 3 demon-
284"
ROBUSTNESS ANALYSIS,0.5732484076433121,"strates the effect of increasing the severity for
285"
ROBUSTNESS ANALYSIS,0.5748407643312102,"each perturbation, where we compare with
286"
ROBUSTNESS ANALYSIS,0.5764331210191083,"Xception [63], CNND, PatchForensics [9],
287"
ROBUSTNESS ANALYSIS,0.5780254777070064,"Face X-ray, and LipForensics [26]. It can be observed that the proposed method maintains a good
288"
ROBUSTNESS ANALYSIS,0.5796178343949044,"performance against the perturbations by Patch-Sub and Noise, while other methods suffer from the
289"
ROBUSTNESS ANALYSIS,0.5812101910828026,"Noise, and LipForensics also suffers from the Patch-Sub.
290"
ABLATION STUDIES,0.5828025477707006,"4.4
Ablation Studies
291"
ABLATION STUDIES,0.5843949044585988,"Joint Embedding Framework. We conducted a series of ablations to verify the instantiated joint
292"
ABLATION STUDIES,0.5859872611464968,"embedding framework by CLIP [61]. We first (1) evaluate the pretrained CLIP, and then (2) fine-tune
293"
ABLATION STUDIES,0.5875796178343949,"it with the frozen text encoder on FF++ [63]. The following ablations adopt the same training
294"
ABLATION STUDIES,0.589171974522293,"procedure, while differing in two alternatives: (3) using equal task weights for multiple tasks instead
295"
ABLATION STUDIES,0.5907643312101911,"of dynamic loss weighting; (4) training without the contrastive prompt pairs, i.e., no contrastive
296"
ABLATION STUDIES,0.5923566878980892,"textual descriptions are used during training. From Table 3, we can observe that freezing language
297"
ABLATION STUDIES,0.5939490445859873,"encoder negatively affects the generalization performance, which we believe is because forgery-
298"
ABLATION STUDIES,0.5955414012738853,"related concepts have not been sufficiently captured during the pretraining stage of CLIP. We also
299"
ABLATION STUDIES,0.5971337579617835,"find that utilizing contrastive prompts can improve generalization, further indicating the contrasting
300"
ABLATION STUDIES,0.5987261146496815,"2The perturbations on severity level 5 often make the face semantically unrecognized, leading meaningless to
detect its authenticity."
ABLATION STUDIES,0.6003184713375797,"operation can benefit the joint embedding methods [12, 27]. Moreover, including the dynamic loss
301"
ABLATION STUDIES,0.6019108280254777,"weighting scheme is advantageous as it not only yields a slight improvement compared to using equal
302"
ABLATION STUDIES,0.6035031847133758,"task weights but also frees us from the burdensome task of hyper-parameter tuning.
303"
ABLATION STUDIES,0.6050955414012739,"Textual Templates. In this subsection, we investigate how the textual template design affects the
304"
ABLATION STUDIES,0.606687898089172,"model performance. We try three different alternatives from single task to three tasks: (5) binary-level
305"
ABLATION STUDIES,0.60828025477707,"text templates, i.e., single task formulation only considering the label of real or fake; (6) two-level
306"
ABLATION STUDIES,0.6098726114649682,"Table 3: Ablation Studies. Baseline denotes the single-task formulation
w/o contrastive textual pairing nor data augmentation, optimized for the
BCE loss."
ABLATION STUDIES,0.6114649681528662,"Model Variant
CDF
FSh
DF-1.0
DFDC
Mean AUC
(1) Pretrained CLIP
65.38
51.04
53.38
55.56
56.34
(2) Frozen gφ
90.56
98.92
91.22
80.19
90.22
(3) Equal Weights
88.32
98.77
92.93
82.27
90.57
(4) w/o Contrastive Pair
87.89
98.34
93.30
81.27
90.20
(5) Binary Templates
85.03
98.42
93.33
81.58
89.59
(6) Two-Levels
87.57
98.47
93.74
80.81
90.15
(7) Joint Templates
88.05
98.42
94.21
81.31
90.50
(8) ViT-B/16
88.13
99.62
93.30
82.30
90.84
(9) ViT-L/14
90.78
99.95
98.60
86.22
93.89
(10) BCE Loss
86.45
98.35
93.40
80.81
89.75
(11) Probabilistic Loss
87.81
98.41
91.55
81.18
89.74
Ours (Baseline)
71.63
98.19
89.94
74.02
83.44
Ours (w/o Aug)
85.53
98.82
93.95
80.41
89.68
Ours (Default)
89.02
98.68
93.38
82.06
90.79"
ABLATION STUDIES,0.6130573248407644,"separate text templates,
307"
ABLATION STUDIES,0.6146496815286624,"i.e., two-level-task formu-
308"
ABLATION STUDIES,0.6162420382165605,"lation,
where we con-
309"
ABLATION STUDIES,0.6178343949044586,"sider the separate tem-
310"
ABLATION STUDIES,0.6194267515923567,"plates describing the over-
311"
ABLATION STUDIES,0.6210191082802548,"all authenticity and global
312"
ABLATION STUDIES,0.6226114649681529,"face attributes; and (7)
313"
ABLATION STUDIES,0.6242038216560509,"the joint text templates
314"
ABLATION STUDIES,0.6257961783439491,"putting
together
labels
315"
ABLATION STUDIES,0.6273885350318471,"from three tasks, e.g., “A
316"
ABLATION STUDIES,0.6289808917197452,"photo of a {fake} face
317"
ABLATION STUDIES,0.6305732484076433,"with the global attribute
318"
ABLATION STUDIES,0.6321656050955414,"of {expression} and the lo-
319"
ABLATION STUDIES,0.6337579617834395,"cal attribute of {mouth}
320"
ABLATION STUDIES,0.6353503184713376,"are altered”.
The joint
321"
ABLATION STUDIES,0.6369426751592356,"probability over multiple
322"
ABLATION STUDIES,0.6385350318471338,"tasks can be computed
323"
ABLATION STUDIES,0.6401273885350318,"from the similarities be-
324"
ABLATION STUDIES,0.64171974522293,"tween the image embed-
325"
ABLATION STUDIES,0.643312101910828,"ding and all candidate tex-
326"
ABLATION STUDIES,0.6449044585987261,"tual embeddings. Then, we marginalize the joint distribution to obtain the marginal probability for
327"
ABLATION STUDIES,0.6464968152866242,"each task. From Table 3, we can observe that the performance of the model using joint templates is
328"
ABLATION STUDIES,0.6480891719745223,"inferior to that of the model using separate templates (i.e., Ours (Default)), indicating that separate
329"
ABLATION STUDIES,0.6496815286624203,"templates for each task are more conducive for learning the semantic closeness between two face
330"
ABLATION STUDIES,0.6512738853503185,"forgery detection tasks in joint embedding. On the other hand, less tasks (i.e., single task and two
331"
ABLATION STUDIES,0.6528662420382165,"tasks) result in the inferior performance. Notably, benefiting from the joint embedding, the model
332"
ABLATION STUDIES,0.6544585987261147,"using binary templates also achieves comparable results on generalization, though it only classifies
333"
ABLATION STUDIES,0.6560509554140127,"the overall authenticity of the face.
334"
ABLATION STUDIES,0.6576433121019108,"Encoder Architecture. In this subsection, we investigate other visual encoders with different settings
335"
ABLATION STUDIES,0.6592356687898089,"and model sizes. In specific, we choose (8) ViT-B/16 [20] and (9) ViT-L/14 [20]. As shown in Table 3,
336"
ABLATION STUDIES,0.660828025477707,"two alternative ViT-based architectures achieve better results on generalization. However, the larger
337"
ABLATION STUDIES,0.6624203821656051,"model will result in both computationally more expensive and time-consuming.
338"
ABLATION STUDIES,0.6640127388535032,"Multitask Objective. In this subsection, we study how different optimization objectives affect the
339"
ABLATION STUDIES,0.6656050955414012,"performance. As a reference, we first replace the fidelity loss functions with (10) binary cross entropy
340"
ABLATION STUDIES,0.6671974522292994,"loss (BCE Loss). We also adopt the (11) hierarchical probabilistic loss [16] to jointly formulate
341"
ABLATION STUDIES,0.6687898089171974,"multi-level classification tasks under a hierarchical label semantic graph. The relative similarity score
342"
ABLATION STUDIES,0.6703821656050956,"(i.e., s −¯s), as a raw score, for each node in the label hierarchy, will be converted into marginal
343"
ABLATION STUDIES,0.6719745222929936,"probabilities for loss computation. From Table 3, we observed that the proposed method outperforms
344"
ABLATION STUDIES,0.6735668789808917,"the variant trained with BCE loss, thus providing evidence for the effectiveness of the designed
345"
ABLATION STUDIES,0.6751592356687898,"fidelity losses. Furthermore, Table 3 shows that fidelity loss yields better performance than the
346"
ABLATION STUDIES,0.6767515923566879,"hierarchical probabilistic loss, suggesting that implicitly learning the semantic dependencies may be
347"
ABLATION STUDIES,0.678343949044586,"better than explicitly encoding the prior knowledge in the label hierarchy graph in advance.
348"
ABLATION STUDIES,0.6799363057324841,"4.5
Discussion: Vision-Language Correspondence
349"
ABLATION STUDIES,0.6815286624203821,"Human-Understandable Interpretation. The proposed joint embedding approach enjoys the
350"
ABLATION STUDIES,0.6831210191082803,"vision-language correspondence, which naturally provides model interpretations by providing human-
351"
ABLATION STUDIES,0.6847133757961783,"understandable explanations. Fig. 4 shows some examples of FF++ [63], in which Deepfakes [1]
352"
ABLATION STUDIES,0.6863057324840764,"indicate the identity swap, leading all local parts of the face are fake; and NeuralTextures [71] modify
353"
ABLATION STUDIES,0.6878980891719745,"the expression in the mouth part. Take an example of NeuralTextures, the texts with a probability
354"
ABLATION STUDIES,0.6894904458598726,"over 50% include “fake”, “expression”, and “mouth”. Hence, we consider this face image to be fake
355"
ABLATION STUDIES,0.6910828025477707,"Figure 4: Bar charts of the similarity scores between the visual image and the textual descriptions a
form of human-understandable explanations."
ABLATION STUDIES,0.6926751592356688,"because the model’s prediction relies on the following three textual prompts: “a photo of a fake face”,
356"
ABLATION STUDIES,0.6942675159235668,"“a photo of a face with the global attribute of expression altered”, and “a photo of a face with the
357"
ABLATION STUDIES,0.695859872611465,"local attribute of mouth altered”. More examples can be found in Appendix.
358"
ABLATION STUDIES,0.697452229299363,"Semantic Closeness across Tasks. We show the semantic closeness across tasks by a correlation
359"
ABLATION STUDIES,0.6990445859872612,"matrix in Fig. 5, in which each entry is represented by the cosine similarity between two textual
360"
ABLATION STUDIES,0.7006369426751592,"embeddings from the language prompts depicting the specific tasks. From Fig. 5, we can observe
361"
ABLATION STUDIES,0.7022292993630573,"that the text encoder of the pretrained CLIP has not sufficiently captured the semantic closeness
362"
ABLATION STUDIES,0.7038216560509554,"across tasks and treats most tasks equally, further verifying the results of the variant with frozen text
363"
ABLATION STUDIES,0.7054140127388535,"encoder in Table 3. After joint embedding learning on the forged faces, the semantic closeness across
364"
ABLATION STUDIES,0.7070063694267515,"tasks can be sufficiently learned, e.g., the concept of “identity” forgery is more related to the “nose”,
365"
ABLATION STUDIES,0.7085987261146497,"“mouth”, and “eye”, thus improving the performance of multitask learning for face forgery detection."
ABLATION STUDIES,0.7101910828025477,"(a) By Text Encoder of Pretrained CLIP
(b) By Text Encoder of Fine-tuned CLIP"
ABLATION STUDIES,0.7117834394904459,"Figure 5: Illustration of semantic closeness across tasks before and after fine-tuning.
366"
CONCLUSION AND LIMITATIONS,0.7133757961783439,"5
Conclusion and Limitations
367"
CONCLUSION AND LIMITATIONS,0.714968152866242,"Conclusion. In this paper, we consider multitask learning for face forgery detection from the joint
368"
CONCLUSION AND LIMITATIONS,0.7165605095541401,"embedding perspective. We have designed a set of coarse-to-fine language prompts to represent
369"
CONCLUSION AND LIMITATIONS,0.7181528662420382,"multiple tasks for face forgery detection. We then take an automated multitask learning scheme to train
370"
CONCLUSION AND LIMITATIONS,0.7197452229299363,"two encoders to joint embed visual face images and textual descriptions. Thus, semantic closeness
371"
CONCLUSION AND LIMITATIONS,0.7213375796178344,"across tasks is manifested as the distance in the learned feature space, thus improving multitask
372"
CONCLUSION AND LIMITATIONS,0.7229299363057324,"learning. From extensive experiments, vision-language correspondence after joint embedding shows
373"
CONCLUSION AND LIMITATIONS,0.7245222929936306,"great promise to support better face forgery detection by maximizing the feature similarity between the
374"
CONCLUSION AND LIMITATIONS,0.7261146496815286,"face image and candidate textual prompts, verifying the effectiveness and superiority of the proposed
375"
CONCLUSION AND LIMITATIONS,0.7277070063694268,"method. Moreover, the joint embedding scheme also renders some degree of model interpretation in
376"
CONCLUSION AND LIMITATIONS,0.7292993630573248,"a human-friendly way.
377"
CONCLUSION AND LIMITATIONS,0.7308917197452229,"Limitations. The proposed method relies on the assumption that the forged faces are generated with
378"
CONCLUSION AND LIMITATIONS,0.732484076433121,"the blending operation [41]. Thus, it may perform unsatisfactorily when fake face images are totally
379"
CONCLUSION AND LIMITATIONS,0.7340764331210191,"synthesized by GAN- or diffusion-model-based methods. Additionally, our model is image-based,
380"
CONCLUSION AND LIMITATIONS,0.7356687898089171,"though it can handle video-based DeepFake by sampling frames for prediction, it may fail when
381"
CONCLUSION AND LIMITATIONS,0.7372611464968153,"encountering the fake video manipulated by only lowering the frame rate [57].
382"
REFERENCES,0.7388535031847133,"References
383"
REFERENCES,0.7404458598726115,"[1] Deepfakes. https://github.com/deepfakes/faceswap.
384"
REFERENCES,0.7420382165605095,"[2] FaceSwap. https://github.com/MarekKowalski/FaceSwap.
385"
REFERENCES,0.7436305732484076,"[3] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li. Protecting world leaders against deep fakes. In
386"
REFERENCES,0.7452229299363057,"CVPRW, pages 38–45, 2019.
387"
REFERENCES,0.7468152866242038,"[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In NIPS, pages 1–13, 2006.
388"
REFERENCES,0.7484076433121019,"[5] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas.
389"
REFERENCES,0.75,"Self-supervised learning from images with a joint-embedding predictive architecture. arXiv preprint
390"
REFERENCES,0.7515923566878981,"arXiv:2301.08243, 2023.
391"
REFERENCES,0.7531847133757962,"[6] S. Becker and G. E. Hinton. Self-organizing neural network that discovers surfaces in random-dot
392"
REFERENCES,0.7547770700636943,"stereograms. Nature, 355(6356):161–163, 1992.
393"
REFERENCES,0.7563694267515924,"[7] J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature verification using a “Siamese” time
394"
REFERENCES,0.7579617834394905,"delay neural network. In NIPS, pages 737–744, 1993.
395"
REFERENCES,0.7595541401273885,"[8] J. Cao, C. Ma, T. Yao, S. Chen, S. Ding, and X. Yang. End-to-end reconstruction-classification learning for
396"
REFERENCES,0.7611464968152867,"face forgery detection. In CVPR, pages 4113–4122, 2022.
397"
REFERENCES,0.7627388535031847,"[9] L. Chai, D. Bau, S.-N. Lim, and P. Isola. What makes fake images detectable? Understanding properties
398"
REFERENCES,0.7643312101910829,"that generalize. In ECCV, pages 103–120, 2020.
399"
REFERENCES,0.7659235668789809,"[10] L. Chen, Y. Zhang, Y. Song, L. Liu, and J. Wang. Self-supervised learning of adversarial example: Towards
400"
REFERENCES,0.767515923566879,"good generalizations for DeepFake detection. In CVPR, pages 18710–18719, 2022.
401"
REFERENCES,0.7691082802547771,"[11] L. Chen, Y. Zhang, Y. Song, J. Wang, and L. Liu. OST: Improving generalization of DeepFake detection
402"
REFERENCES,0.7707006369426752,"via one-shot test-time training. In NIPS, pages 1–14, 2022.
403"
REFERENCES,0.7722929936305732,"[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual
404"
REFERENCES,0.7738853503184714,"representations. In ICML, pages 1597–1607, 2020.
405"
REFERENCES,0.7754777070063694,"[13] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. GradNorm: Gradient normalization for
406"
REFERENCES,0.7770700636942676,"adaptive loss balancing in deep multitask networks. In ICML, pages 794–803, 2018.
407"
REFERENCES,0.7786624203821656,"[14] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to
408"
REFERENCES,0.7802547770700637,"face verification. In CVPR, pages 539–546, 2005.
409"
REFERENCES,0.7818471337579618,"[15] V. Conotter, J. F. O’Brien, and H. Farid. Exposing digital forgeries in ballistic motion. IEEE TIFS,
410"
REFERENCES,0.7834394904458599,"7(1):283–296, 2011.
411"
REFERENCES,0.785031847133758,"[16] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Neven, and H. Adam. Large-scale
412"
REFERENCES,0.7866242038216561,"object classification using label relation graphs. In ECCV, pages 48–64, 2014.
413"
REFERENCES,0.7882165605095541,"[17] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou. RetinaFace: Single-shot multi-level face
414"
REFERENCES,0.7898089171974523,"localisation in the wild. In CVPR, pages 5203–5212, 2020.
415"
REFERENCES,0.7914012738853503,"[18] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and C. C. Ferrer. The DeepFake detection
416"
REFERENCES,0.7929936305732485,"challenge (DFDC) dataset. arXiv preprint arXiv:2006.07397, 2020.
417"
REFERENCES,0.7945859872611465,"[19] X. Dong, J. Bao, D. Chen, T. Zhang, W. Zhang, N. Yu, D. Chen, F. Wen, and B. Guo. Protecting celebrities
418"
REFERENCES,0.7961783439490446,"from DeepFake with identity consistency transformer. In CVPR, pages 9468–9478, 2022.
419"
REFERENCES,0.7977707006369427,"[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
420"
REFERENCES,0.7993630573248408,"derer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers
421"
REFERENCES,0.8009554140127388,"for image recognition at scale. In ICLR, pages 1–12, 2020.
422"
REFERENCES,0.802547770700637,"[21] H. Farid. Image forgery detection: A survey. IEEE SPM, 26(2):16–25, 2009.
423"
REFERENCES,0.804140127388535,"[22] Y. Gao, J. Ma, M. Zhao, W. Liu, and A. L. Yuille. NDDR-CNN: Layerwise feature fusing in multi-task
424"
REFERENCES,0.8057324840764332,"cnns by neural discriminative dimensionality reduction. In CVPR, pages 3205–3214, 2019.
425"
REFERENCES,0.8073248407643312,"[23] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo,
426"
REFERENCES,0.8089171974522293,"M. Gheshlaghi Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A
427"
REFERENCES,0.8105095541401274,"new approach to self-supervised learning. In NIPS, pages 21271–21284, 2020.
428"
REFERENCES,0.8121019108280255,"[24] H. Guo, S. Hu, X. Wang, M.-C. Chang, and S. Lyu. Eyes tell all: Irregular pupil shapes reveal GAN-
429"
REFERENCES,0.8136942675159236,"generated faces. In ICASSP, pages 2904–2908, 2022.
430"
REFERENCES,0.8152866242038217,"[25] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In
431"
REFERENCES,0.8168789808917197,"CVPR, pages 1735–1742, 2006.
432"
REFERENCES,0.8184713375796179,"[26] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic. Lips don’t lie: A generalisable and robust approach
433"
REFERENCES,0.8200636942675159,"to face forgery detection. In CVPR, pages 5039–5049, 2021.
434"
REFERENCES,0.821656050955414,"[27] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation
435"
REFERENCES,0.8232484076433121,"learning. In CVPR, pages 9729–9738, 2020.
436"
REFERENCES,0.8248407643312102,"[28] Y. He, B. Gan, S. Chen, Y. Zhou, G. Yin, L. Song, L. Sheng, J. Shao, and Z. Liu. ForgeryNet: A versatile
437"
REFERENCES,0.8264331210191083,"benchmark for comprehensive forgery analysis. In CVPR, pages 4360–4369, 2021.
438"
REFERENCES,0.8280254777070064,"[29] S. Hu, Y. Li, and S. Lyu. Exposing GAN-generated faces using inconsistent corneal specular highlights. In
439"
REFERENCES,0.8296178343949044,"ICASSP, pages 2500–2504, 2021.
440"
REFERENCES,0.8312101910828026,"[30] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up
441"
REFERENCES,0.8328025477707006,"visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904–4916,
442"
REFERENCES,0.8343949044585988,"2021.
443"
REFERENCES,0.8359872611464968,"[31] L. Jiang, R. Li, W. Wu, C. Qian, and C. C. Loy. DeeperForensics-1.0: A large-scale dataset for real-world
444"
REFERENCES,0.8375796178343949,"face forgery detection. In CVPR, pages 2889–2898, 2020.
445"
REFERENCES,0.839171974522293,"[32] M. K. Johnson and H. Farid. Exposing digital forgeries through chromatic aberration. In ACM MM&Sec,
446"
REFERENCES,0.8407643312101911,"pages 48–55, 2006.
447"
REFERENCES,0.8423566878980892,"[33] M. K. Johnson and H. Farid. Exposing digital forgeries in complex lighting environments. IEEE TIFS,
448"
REFERENCES,0.8439490445859873,"2(3):450–461, 2007.
449"
REFERENCES,0.8455414012738853,"[34] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image
450"
REFERENCES,0.8471337579617835,"quality of StyleGAN. In CVPR, pages 8110–8119, 2020.
451"
REFERENCES,0.8487261146496815,"[35] E. Kee, J. F. O’brien, and H. Farid. Exposing photo manipulation from shading and shadows. ACM TOG,
452"
REFERENCES,0.8503184713375797,"33(5):165:1–165:21, 2014.
453"
REFERENCES,0.8519108280254777,"[36] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry
454"
REFERENCES,0.8535031847133758,"and semantics. In CVPR, pages 7482–7491, 2018.
455"
REFERENCES,0.8550955414012739,"[37] I. Kokkinos. UberNet: Training a universal convolutional neural network for low-, mid-, and high-level
456"
REFERENCES,0.856687898089172,"vision using diverse datasets and limited memory. In CVPR, pages 6129–6138, 2017.
457"
REFERENCES,0.85828025477707,"[38] Y. LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022.
458"
REFERENCES,0.8598726114649682,"[39] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning.
459"
REFERENCES,0.8614649681528662,"Predicting Structured Data, 1(0):1–59, 2006.
460"
REFERENCES,0.8630573248407644,"[40] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen. Advancing high fidelity identity swapping for forgery
461"
REFERENCES,0.8646496815286624,"detection. In CVPR, pages 5074–5083, 2020.
462"
REFERENCES,0.8662420382165605,"[41] L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo. Face X-ray for more general face forgery
463"
REFERENCES,0.8678343949044586,"detection. In CVPR, pages 5001–5010, 2020.
464"
REFERENCES,0.8694267515923567,"[42] Y. Li, M.-C. Chang, and S. Lyu. In Ictu Oculi: Exposing AI created fake videos by detecting eye blinking.
465"
REFERENCES,0.8710191082802548,"In WIFS, pages 1–7, 2018.
466"
REFERENCES,0.8726114649681529,"[43] Y. Li and S. Lyu. Exposing DeepFake videos by detecting face warping artifacts. In CVPRW, pages 46–52,
467"
REFERENCES,0.8742038216560509,"2019.
468"
REFERENCES,0.8757961783439491,"[44] Y. Li, X. Yang, P. Sun, H. Qi, and S. Lyu. Celeb-DF: A large-scale challenging dataset for DeepFake
469"
REFERENCES,0.8773885350318471,"forensics. In CVPR, pages 3207–3216, 2020.
470"
REFERENCES,0.8789808917197452,"[45] X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong. Pareto multi-task learning. In NIPS, pages
471"
REFERENCES,0.8805732484076433,"12037–12047, 2019.
472"
REFERENCES,0.8821656050955414,"[46] Z. Lin, R. Wang, X. Tang, and H.-Y. Shum. Detecting doctored images using camera response normality
473"
REFERENCES,0.8837579617834395,"and consistency. In CVPR, pages 1087–1092, 2005.
474"
REFERENCES,0.8853503184713376,"[47] S. Liu, E. Johns, and A. J. Davison. End-to-end multi-task learning with attention. In CVPR, pages
475"
REFERENCES,0.8869426751592356,"1871–1880, 2019.
476"
REFERENCES,0.8885350318471338,"[48] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, pages 1–13,
477"
REFERENCES,0.8901273885350318,"2017.
478"
REFERENCES,0.89171974522293,"[49] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, pages 1–10, 2019.
479"
REFERENCES,0.893312101910828,"[50] S. Lyu. Estimating vignetting function from a single image for image authentication. In ACM MM&Sec,
480"
REFERENCES,0.8949044585987261,"pages 3–12, 2010.
481"
REFERENCES,0.8964968152866242,"[51] S. Lyu, X. Pan, and X. Zhang. Exposing region splicing forgeries with blind local noise estimation. IJCV,
482"
REFERENCES,0.8980891719745223,"110(2):202–221, 2014.
483"
REFERENCES,0.8996815286624203,"[52] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning
484"
REFERENCES,0.9012738853503185,"to mask weights. In ECCV, pages 72–88, 2018.
485"
REFERENCES,0.9028662420382165,"[53] I. Misra and L. v. d. Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, pages
486"
REFERENCES,0.9044585987261147,"6707–6717, 2020.
487"
REFERENCES,0.9060509554140127,"[54] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks for multi-task learning. In CVPR,
488"
REFERENCES,0.9076433121019108,"pages 3994–4003, 2016.
489"
REFERENCES,0.9092356687898089,"[55] H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen. Multi-task learning for detecting and segmenting
490"
REFERENCES,0.910828025477707,"manipulated facial images and videos. In BTAS, pages 1–8, 2019.
491"
REFERENCES,0.9124203821656051,"[56] J. F. O’brien and H. Farid. Exposing photo manipulation with inconsistent reflections. ACM TOG,
492"
REFERENCES,0.9140127388535032,"31(1):4:1–4:11, 2012.
493"
REFERENCES,0.9156050955414012,"[57] D. O’Sullivan. Doctored videos shared to make Pelosi sound drunk viewed millions of times on social me-
494"
REFERENCES,0.9171974522292994,"dia. https://edition.cnn.com/2019/05/23/politics/doctored-video-pelosi/index.html,
495"
REFERENCES,0.9187898089171974,"2019. Date of access: May 12, 2023.
496"
REFERENCES,0.9203821656050956,"[58] A. C. Popescu and H. Farid. Exposing digital forgeries by detecting traces of resampling. IEEE TSP,
497"
REFERENCES,0.9219745222929936,"53(2):758–767, 2005.
498"
REFERENCES,0.9235668789808917,"[59] A. C. Popescu and H. Farid. Exposing digital forgeries in color filter array interpolated images. IEEE TSP,
499"
REFERENCES,0.9251592356687898,"53(10):3948–3959, 2005.
500"
REFERENCES,0.9267515923566879,"[60] Y. Qian, G. Yin, L. Sheng, Z. Chen, and J. Shao. Thinking in frequency: Face forgery detection by mining
501"
REFERENCES,0.928343949044586,"frequency-aware clues. In ECCV, pages 86–103, 2020.
502"
REFERENCES,0.9299363057324841,"[61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
503"
REFERENCES,0.9315286624203821,"J. Clark, G. Krueger, and I. Sutskever.
Learning transferable visual models from natural language
504"
REFERENCES,0.9331210191082803,"supervision. In ICML, pages 8748–8763, 2021.
505"
REFERENCES,0.9347133757961783,"[62] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
506"
REFERENCES,0.9363057324840764,"multitask learners. 2019.
507"
REFERENCES,0.9378980891719745,"[63] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner. FaceForensics++: Learning to
508"
REFERENCES,0.9394904458598726,"detect manipulated facial images. In ICCV, pages 1–11, 2019.
509"
REFERENCES,0.9410828025477707,"[64] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard. Latent multi-task architecture learning. In AAAI, pages
510"
REFERENCES,0.9426751592356688,"4822–4829, 2019.
511"
REFERENCES,0.9442675159235668,"[65] O. Sener and V. Koltun. Multi-task learning as multi-objective optimization. In NIPS, pages 525–536,
512"
REFERENCES,0.945859872611465,"2018.
513"
REFERENCES,0.947452229299363,"[66] K. Shiohara and T. Yamasaki. Detecting deepfakes with self-blended images. In CVPR, pages 18720–18729,
514"
REFERENCES,0.9490445859872612,"2022.
515"
REFERENCES,0.9506369426751592,"[67] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In NIPS, page
516"
REFERENCES,0.9522292993630573,"11918–11930, 2019.
517"
REFERENCES,0.9538216560509554,"[68] X. Sun, R. Panda, R. Feris, and K. Saenko. AdaShare: Learning what to share for efficient deep multi-task
518"
REFERENCES,0.9554140127388535,"learning. In NIPS, pages 8728–8740, 2020.
519"
REFERENCES,0.9570063694267515,"[69] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace: Closing the gap to human-level performance
520"
REFERENCES,0.9585987261146497,"in face verification. In CVPR, pages 1701–1708, 2014.
521"
REFERENCES,0.9601910828025477,"[70] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invariance through imitation. In CVPR, pages
522"
REFERENCES,0.9617834394904459,"2729–2736, 2011.
523"
REFERENCES,0.9633757961783439,"[71] J. Thies, M. Zollhöfer, and M. Nießner. Deferred neural rendering: Image synthesis using neural textures.
524"
REFERENCES,0.964968152866242,"ACM TOG, 38(4):1–12, 2019.
525"
REFERENCES,0.9665605095541401,"[72] J. Thies, M. Zollhöfer, M. Stamminger, C. Theobalt, and M. Nießner. Face2Face: Real-time face capture
526"
REFERENCES,0.9681528662420382,"and reenactment of RGB videos. In CVPR, pages 2387–2395, 2016.
527"
REFERENCES,0.9697452229299363,"[73] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. Ma. FRank: A ranking method with fidelity loss. In
528"
REFERENCES,0.9713375796178344,"ACM SIGIR, pages 383–390, 2007.
529"
REFERENCES,0.9729299363057324,"[74] M. Wallingford, H. Li, A. Achille, A. Ravichandran, C. Fowlkes, R. Bhotika, and S. Soatto. Task adaptive
530"
REFERENCES,0.9745222929936306,"parameter sharing for multi-task learning. In CVPR, pages 7561–7570, 2022.
531"
REFERENCES,0.9761146496815286,"[75] S.-Y. Wang, O. Wang, A. Owens, R. Zhang, and A. A. Efros. Detecting Photoshopped faces by scripting
532"
REFERENCES,0.9777070063694268,"Photoshop. In ICCV, pages 10072–10081, 2019.
533"
REFERENCES,0.9792993630573248,"[76] S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros. CNN-generated images are surprisingly easy
534"
REFERENCES,0.9808917197452229,"to spot...for now. In CVPR, pages 8695–8704, 2020.
535"
REFERENCES,0.982484076433121,"[77] X. Yang, Y. Li, and S. Lyu. Exposing Deep Fakes using inconsistent head poses. In ICASSP, pages
536"
REFERENCES,0.9840764331210191,"8261–8265, 2019.
537"
REFERENCES,0.9856687898089171,"[78] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow Twins: Self-supervised learning via redundancy
538"
REFERENCES,0.9872611464968153,"reduction. In ICML, pages 12310–12320, 2021.
539"
REFERENCES,0.9888535031847133,"[79] Y. Zhang and Q. Yang. A survey on multi-task learning. IEEE TKDE, 34(12):5586–5609, 2021.
540"
REFERENCES,0.9904458598726115,"[80] H. Zhao, W. Zhou, D. Chen, T. Wei, W. Zhang, and N. Yu. Multi-attentional deepfake detection. In CVPR,
541"
REFERENCES,0.9920382165605095,"pages 2185–2194, 2021.
542"
REFERENCES,0.9936305732484076,"[81] T. Zhao, X. Xu, M. Xu, H. Ding, Y. Xiong, and W. Xia. Learning self-consistency for deepfake detection.
543"
REFERENCES,0.9952229299363057,"In ICCV, pages 15023–15033, 2021.
544"
REFERENCES,0.9968152866242038,"[82] P. Zhou, X. Han, V. I. Morariu, and L. S. Davis. Two-stream neural networks for tampered face detection.
545"
REFERENCES,0.9984076433121019,"In CVPRW, pages 1831–1839, 2017.
546"
