Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007722007722007722,"Multi-agent reinforcement learning has made substantial empirical progresses in
solving games with a large number of players. However, theoretically, the best
known sample complexity for ﬁnding a Nash equilibrium in general-sum games
scales exponentially in the number of players due to the size of the joint action
space, and there is a matching exponential lower bound. This paper investigates
what learning goals admit better sample complexities in the setting of m-player
general-sum Markov games with H steps, S states, and Ai actions per player.
First, we design algorithms for learning an ε-Coarse Correlated Equilibrium (CCE)
in e
O(H5S maxi≤m Ai/ε2) episodes, and an ε-Correlated Equilibrium (CE) in
e
O(H6S maxi≤m A2
i /ε2) episodes. This is the ﬁrst line of results for learning CCE
and CE with sample complexities polynomial in maxi≤m Ai. Our algorithm for
learning CE integrates an adversarial bandit subroutine which minimizes a weighted
swap regret, along with several novel designs in the outer loop. Second, we consider
the important special case of Markov Potential Games, and design an algorithm
that learns an ε-approximate Nash equilibrium within e
O(S P"
ABSTRACT,0.0015444015444015444,"i≤m Ai/ε3) episodes
(when only highlighting the dependence on S, Ai, and ε), which only depends
linearly in P"
ABSTRACT,0.0023166023166023165,"i≤m Ai and signiﬁcantly improves over existing efﬁcient algorithms
in the ε dependence. Overall, our results shed light on what equilibria or structural
assumptions on the game may enable sample-efﬁcient learning with many players."
INTRODUCTION,0.003088803088803089,"1
INTRODUCTION"
INTRODUCTION,0.003861003861003861,"Multi-agent reinforcement learning (RL) has achieved substantial recent successes in solving artiﬁcial
intelligence challenges such as GO (Silver et al., 2016; 2018), multi-player games with team play
such as Starcraft (Vinyals et al., 2019) and Dota2 (Berner et al., 2019), behavior learning in social
interactions (Baker et al., 2019), and economic simulation (Zheng et al., 2020; Trott et al., 2021). In
many applications, multi-agent RL is able to yield high quality policies for multi-player games with a
large number of players (Wang et al., 2016; Yang et al., 2018)."
INTRODUCTION,0.004633204633204633,"Despite these empirical progresses, theoretical understanding of when we can sample-efﬁciently
solve multi-player games with a large number of players remains elusive, especially in the setting of
multi-player Markov games. A main bottleneck here is the exponential blow-up of the joint action
space—The total number of joint actions in a generic game with simultaneous plays is equal to
the product of the number of actions for each player, which scales exponentially in the number of
players. Such an exponential dependence is indeed known to be unavoidable in the worst-case for
certain standard problems. For example, for learning an approximate Nash equilibrium from payoff
queries in an one-step multi-player general-sum game, the query complexity lower bound of Chen
et al. (2015) and Rubinstein (2016) shows that at least exponentially many queries (samples) is"
INTRODUCTION,0.005405405405405406,Published as a conference paper at ICLR 2022
INTRODUCTION,0.006177606177606178,"required, even when each player only has two possible actions and the query is noiseless. Moreover,
for learning Nash equilibrium in Markov games, the best existing sample complexity upper bound
also scales with the size of the joint action space (Liu et al., 2021)."
INTRODUCTION,0.0069498069498069494,"Nevertheless, these exponential lower bounds do not completely rule out interesting theoretical
inquiries—there may well be other notions of equilibria or additional structures within the game that
allow us to learn with a better sample complexity. This motivates us to ask the following"
INTRODUCTION,0.007722007722007722,"Question: When can we solve general-sum Markov games with sample complexity
milder than exponential in the number of players?"
INTRODUCTION,0.008494208494208495,"This paper makes steps towards answering the above question by considering multi-player general-
sum Markov games (MGs) with m players, H steps, S states, and Ai actions per player. We make two
lines of investigations: (1) Can we learn alternative notions of equilibria with better sample complexity
than learning Nash; (2) Can the Nash equilibrium be learned with better sample complexity under
additional structural assumptions on the game. This paper makes contributions on both ends, which
we summarize as follows."
INTRODUCTION,0.009266409266409266,"• We ﬁrst design an algorithm that learns the ε-approximate Coarse Correlated Equilibrium
(CCE) with e
O(H5S maxi∈[m] Ai/ε2) episodes of play (Section 3). Our algorithm CCE-V-
LEARNING is a multi-player adaptation of the Nash V-Learning algorithm of Bai et al. (2020)."
INTRODUCTION,0.010038610038610039,"• We design an algorithm CE-V-LEARNING which learns the stricter notion of ε-approximate
Correlated Equilibrium (CE) with e
O(H6S maxi∈[m] A2
i /ε2) episodes of play (Section 4). For
Markov games, these are the ﬁrst line of sample complexity results for learning CE and CCE that
only scales polynomially with maxi∈[m] Ai, and improves signiﬁcantly in the Ai dependency
over the current best algorithm which scales with Q"
INTRODUCTION,0.010810810810810811,i∈[m] Ai.
INTRODUCTION,0.011583011583011582,"• Technically, our algorithm CE-V-LEARNING makes several major modiﬁcations over CCE-
V-LEARNING in order to learn the CE (Section 4.2). Notably, inspired by the connection
between CE and low swap-regret learning, we use a mixed-expert Follow-The-Regularized
Leader algorithm within its inner loop to achieve low swap-regret for a particular adversarial
bandit problem. Our analysis also contains new results for adversarial bandits on weighted swap
regret and weighted regret with predicable weights, which may be of independent interest."
INTRODUCTION,0.012355212355212355,"• Finally, we consider learning Nash equilibrium in Markov Potential Games (MPGs), an important
subclass of general-sum Markov games. By a reduction to single-agent RL, we design an
algorithm NASH-CA that achieves e
O(ΦmaxH3S P"
INTRODUCTION,0.013127413127413128,"i∈[m] Ai/ε3) sample complexity, where
Φmax ≤Hm is the bound on the potential function (Section 5). Compared with the recent result
of Leonardos et al. (2021), we signiﬁcantly improves the ε dependence from their 1/ε6."
RELATED WORK,0.013899613899613899,"1.1
RELATED WORK"
RELATED WORK,0.014671814671814672,"Learning equilibria in general-sum games
The sample (query) complexity of learning Nash,
CE, and CCE from samples in one-step (i.e. normal form) general-sum games with m players
and Ai actions per player has been studied extensively in literature (Hart & Mas-Colell, 2000;
Hart, 2005; Stoltz, 2005; Cesa-Bianchi & Lugosi, 2006; Blum & Mansour, 2007; Fearnley et al.,
2015; Babichenko & Barman, 2015; Chen et al., 2015; Fearnley & Savani, 2016; Goldberg & Roth,
2016; Babichenko, 2016; Rubinstein, 2016; Hart & Nisan, 2018). It is known that learning Nash
equilibrium requires exponential in m samples in the worst case (Rubinstein, 2016), whereas CE and
CCE admit efﬁcient poly(m, maxi≤m Ai)-sample complexity algorithms by independent no-regret
learning (Hart & Mas-Colell, 2000; Hart, 2005; Syrgkanis et al., 2015; Goldberg & Roth, 2016;
Chen & Peng, 2020; Daskalakis et al., 2021). Our results for learning CE and CCE can be seen as
extension of these works into Markov games. We remark that even when the game is fully known, the
computational complexity for ﬁnding Nash in general-sum games is PPAD-hard (Daskalakis, 2013)."
RELATED WORK,0.015444015444015444,"Markov games
Markov games (Shapley, 1953; Littman, 1994) is a widely used framework for
game playing with sequential decision making, e.g. in multi-agent reinforcement learning. Algorithms
with asymptotic convergence have been proposed in the early works of Hu & Wellman (2003); Littman
(2001); Hansen et al. (2013). A recent line of work studies the non-asymptotic sample complexity for
learning Nash in two-player zero-sum Markov games (Bai & Jin, 2020; Xie et al., 2020; Bai et al.,"
RELATED WORK,0.016216216216216217,Published as a conference paper at ICLR 2022
RELATED WORK,0.01698841698841699,"2020; Zhang et al., 2020; Liu et al., 2021; Chen et al., 2021; Jin et al., 2021; Huang et al., 2021)
and learning various equilibria in general-sum Markov games (Liu et al., 2021; Bai et al., 2021),
building on techniques for learning single-agent Markov Decision Processes sample-efﬁciently (Azar
et al., 2017; Jin et al., 2018). Learning the Nash equilibrium in general-sum Markov games are much
harder than that in zero-sum Markov games. Liu et al. (2021) present the ﬁrst line of results for
learning Nash, CE, and CCE in general-sum Markov games; however their sample complexity scales
with Q"
RELATED WORK,0.01776061776061776,"i≤m Ai due to the model-based nature of their algorithm. Algorithms for computing CE in
extensive-form games has been widely studied (Von Stengel & Forges, 2008; Celli et al., 2020; Farina
et al., 2021; Morrill et al., 2021), though we remark Markov games and extensive-form games are
different frameworks and our results do not imply each other."
RELATED WORK,0.018532818532818532,"Markov potential games
Lastly, a recent line of works considers Markov potential games (Macua
et al., 2018; Leonardos et al., 2021; Zhang et al., 2021), a subset of general-sum Markov games
in which the Nash equilibrium admits more efﬁcient algorithms. Leonardos et al. (2021) gives a
sample-efﬁcient algorithm based on the policy gradient method (Agarwal et al., 2021). The special
case of Markov cooperative games is studied empirically in e.g. Lowe et al. (2017); Yu et al. (2021).
For one step potential games, Kleinberg et al. (2009); Palaiopanos et al. (2017); Cohen et al. (2017a)
show the convergence to Nash equilibria of no-regret dynamics."
PRELIMINARIES,0.019305019305019305,"2
PRELIMINARIES"
PRELIMINARIES,0.020077220077220077,"We present preliminaries for multi-player general-sum Markov games as well as the solution concept
of (approximate) Nash equilibrium. Alternative solution concepts and other concrete subclasses of
Markov games considered in this paper will be deﬁned in the later sections."
PRELIMINARIES,0.02084942084942085,"Markov games
A multi-player general sum Markov game (MG; Shapley (1953); Littman (1994))
with m players can be described by a tuple MG(H, S, {Ai}m
i=1 , P, {ri}m
i=1), where H is the episode
length, S is the state space with |S| = S, Ai is the action space for the ith player with |Ai| = Ai.
Without loss of generality, we assume Ai = [Ai]. We let a := (a1, · · · , am) denote the vector
of joint actions taken by all the players and A = A1 × · · · × Am denote the joint action space.
Throughout this paper we assume that S and Ai are ﬁnite. The transition probability P = {Ph}h∈[H]
is the collection of transition matrices, where Ph(·|s, a) ∈∆S denotes the distribution of the next
state when actions a are taken at state s at step h. The rewards ri = {rh,i}h∈[H],i∈[m] is the collection
of reward functions for the ith player, where rh,i(s, a) ∈[0, 1] gives the deterministic1 reward of ith
player if actions a are taken at state s at step h. Without loss of generality, we assume the initial state
s1 is deterministic. A key feature of general-sum games is that the rewards ri are in general different
for each player i, and the goal of each player is to maximize her own cumulative reward."
PRELIMINARIES,0.021621621621621623,"Markov product policy, value function
A product policy is a collection of m policies π :=
{πi}i∈[m] where πi is the general (potentially history-dependent) policy for the i-th player. We
ﬁrst focus on the case of Markov product policies, in which πi = {πh,i : S →∆Ai}h∈[H], and
πh,i(ai|s) is the probability for the ith player to take action ai at state s at step h. For a policy π
and i ∈[m], we use π−i := {πj}j∈[m],j̸=i to denote the policy of all but the ith player. The value
function V π
h,i(s) : S →R is deﬁned as the expected cumulative reward for the ith player when policy
π is taken starting from state s and step h:"
PRELIMINARIES,0.022393822393822392,"V π
h,i(s) := Eπ "" H
X"
PRELIMINARIES,0.023166023166023165,"h′=h
rh′,i(sh′, ah′)"
PRELIMINARIES,0.023938223938223938,"sh = s # .
(1)"
PRELIMINARIES,0.02471042471042471,"Best response & Nash equilibrium
For any product policy π = {πi}i∈[m], the best response for"
PRELIMINARIES,0.025482625482625483,"the ith player against π−i is deﬁned as any policy π† such that V π†,π−i
1,i
(s1) = supπ′
i V π′
i,π−i
1,i
(s1).
For any Markov product policy, this best response is guaranteed to exist (and be Markov) as the above
maximization problem is equivalent to solving a Markov Decision Process (MDP) for the ith player.
We will also use the notation V †,π−i
1,i
(s1) to denote the above value function V π†,π−i
1,i
(s1)."
PRELIMINARIES,0.026254826254826256,1Our results can be straightforwardly generalized to Markov games with stochastic rewards.
PRELIMINARIES,0.02702702702702703,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.027799227799227798,"We say π is a Nash equilibrium (e.g. Nash (1951); Pérolat et al. (2017)) if all players play the best
response against other players, i.e., for all i ∈[m],"
PRELIMINARIES,0.02857142857142857,"V π
1,i(s1) = V †,π−i
1,i
(s1).
Note that in general-sum MGs, there may exist multiple Nash equilibrium policies with different
value functions, unlike in two-player zero-sum MGs (Shapley, 1953). To measure the suboptimality
of any policy π, we deﬁne the NE-gap as"
PRELIMINARIES,0.029343629343629343,"NE-gap(π) := max
i∈[m]"
PRELIMINARIES,0.030115830115830116,"
sup
µi
V µi,π−i
1,i
(s1) −V π
1,i(s1)

."
PRELIMINARIES,0.03088803088803089,"For any ε ≥0, we say π is ε-approximate Nash equilibrium (ε-Nash) if NE-gap(π) ≤ε."
PRELIMINARIES,0.03166023166023166,"General correlated policy & Its best response
A general correlated policy π is a set of H maps
π := {πh : Ω× (S × A)h−1 × S →∆A}h∈[H]. The ﬁrst argument of πh is a random variable
ω ∈Ωsampled from some underlying distribution, and the other arguments contain all the history
information and the current state information (unlike Markov policies in which the policies only
depend on the current state information). The output of πh is a general distribution of actions in
A = A1 ×· · ·×Am (unlike product policies in which the action distribution is a product distribution)."
PRELIMINARIES,0.032432432432432434,"For any correlated policy π = {πh}h∈[H] and any player i, we can deﬁne a marginal policy π−i
as a set of H maps π−i := {πh,−i : Ω× (S × A)h−1 × S →∆A−i}h∈[H] where A−i :=
A1 × · · · × Ai−1 × Ai+1 × · · · × Am, and the output of πh,−i is deﬁned as the marginal distribution
of the output of πh restricted to the space A−i. For any general correlated policy π, we can deﬁne its
initial state value function V π
1,i(s1) similar as (1). The best response value of the ith player against π"
PRELIMINARIES,0.033204633204633204,"is V †,π−i
1,i
(s1) = supµi V µi,π−i
1,i
(s1), where V µi,π−i
1,i
(s1) is the value function of the policy (µi, π−i)
(the ith player plays according to general policy µi, and all other players play according to π−i), and
the supremum is taken over all general policy µi of the ith player."
PRELIMINARIES,0.03397683397683398,"Learning setting
Throughout this paper we consider the interactive learning (i.e. exploration)
setting where algorithms are able to play episodes within the MG and observe the realized transitions
and rewards. Our focus is on the PAC sample complexity (i.e. number of episodes of play) for any
learning algorithm to output an approximate equilibrium."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.03474903474903475,"2.1
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.03552123552123552,"The focus of this paper is the setting where the number of players m is large. Intuitively, as the
joint action space has size |A| = Qm
i=1 Ai which scales exponentially in m (if each Ai ≥2), naive
algorithms for learning Nash equilibrium may learn all ri(a) by enumeratively querying all a ∈A,
and this costs exponential in m samples. Unfortunately, recent work shows that such exponential in
m dependence is unavoidable in the worst-case for any algorithm—there is an exp(Ω(m)) sample
complexity lower bound for learning approximate Nash, even in one-step general-sum games (Chen
et al., 2015; Rubinstein, 2016) (see Proposition A.2 for formal statement)."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.036293436293436294,"This suggests that the Nash equilibrium as a solution concept may be too hard to learn efﬁciently
for MGs with a large number of players, and calls for alternative solution concepts or additional
structural assumptions on the game in order to achieve an improved m dependence."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.037065637065637064,"3
EFFICIENT LEARNING OF COARSE CORRELATED EQUILIBRIA (CCE)"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.03783783783783784,"Given the difﬁculty of learning Nash when the number of players m is large , we consider learning
other relaxed notions of equilibria for general-sum MGs. Two standard notions of equilibria for
games are the Correlated Equilibrium (CE) and Coarse Correlated Equilibrium (CCE), and they
satisfy {Nash} ⊂{CE} ⊂{CCE} for general-sum MGs (Nisan et al., 2007)."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.03861003861003861,"We begin by considering learning CCE (most relaxed notion above) for Markov games.
Deﬁnition 1 (ε-approximate CCE for general-sum MGs). We say a (general) correlated policy π is
an ε-approximate Coarse Correlated Equilibrium (ε-CCE) if"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.039382239382239385,"max
i∈[m]"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.040154440154440155,"
V †,π−i
1,i
(s1) −V π
1,i(s1)

≤ε."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.040926640926640924,Published as a conference paper at ICLR 2022
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.0416988416988417,We say π is an (exact) CCE if the above is satisﬁed with ε = 0.
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.04247104247104247,"The following result shows that there exists an algorithm that can learn an ε-approximate CCE in
general-sum Markov games within e
O(H5S maxi∈[m] Ai/ε2) episodes of play."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.043243243243243246,"Theorem 2 (Learning ε-approximate CCE for general-sum MGs). Suppose we run the CCE-V-
LEARNING algorithm (Algorithm 4) for all m players and"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.044015444015444015,"K ≥O
H5S maxi∈[m] Aiι"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.044787644787644784,"ε2
+ H4Sι3 ε "
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.04555984555984556,"episodes (ι = log(m maxi∈[m] AiHSK/(pε)) is a log factor). Then with probability at least 1 −p,"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.04633204633204633,"the certiﬁed policy bπ deﬁned in Algorithm 2 is an ε-CCE, i.e. maxi∈[m](V †,bπ−i
1,i
(s1) −V bπ
1,i(s1)) ≤ε."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.047104247104247106,"Mild dependence on action space
For small enough ε, the sample complexity featured in Theo-
rem 2 scales as e
O(H5S maxi∈[m] Ai/ε2). Most notably, this is the ﬁrst algorithm that scales with
maxi∈[m] Ai, and exhibits a sharp difference in learning Nash and learning CCE in view of the
exp(Ω(m)) lower bound for learning Nash in Proposition A.2. Indeed, existing algorithms such as
Multi-Nash-VI Algorithm with CCE subroutine (Liu et al., 2021) does require e
O(H4S2 Qm
i=1 Ai/ε2)
episodes of play, which scales with Q"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.047876447876447875,"i∈[m] Ai due to its model-based nature. We achieve signiﬁcantly
better dependence on Ai and also S, though slightly worse H dependence."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.04864864864864865,"Overview of algorithm and techniques
Our CCE-V-LEARNING algorithm (deferred to Ap-
pendix B.1 due to space limit) is a multi-player adaptation of the Nash V-Learning algorithm
of Bai et al. (2020); Tian et al. (2021) for learning Nash equilibria in two-player zero-sum MGs.
Similar as Bai et al. (2020), we show that this algorithm enjoys a “no-regret” like guarantee for each
player at each (h, s) (Lemma B.3). We also adopted the choice of hyperparameters in Tian et al.
(2021) so that the sample complexity has a slightly better dependence in H. When combined with the
“certiﬁed correlated policy” procedure (Algorithm 2), our algorithm outputs a policy that is ε-CCE.
Our certiﬁed policy procedure is adapted from the certiﬁed policy of Bai et al. (2020), and differs in
that ours output a correlated policy for all the players whereas Bai et al. (2020) outputs a product
policy. The key feature enabling this maxi∈[m] Ai dependence is that this algorithm uses decentral-
ized learning for each player to learn the value function (V ), instead of learning the Q function (as
in Liu et al. (2021)) that requires sample size scales as Q
i∈[m] Ai. The proof of Theorem 2 is in
Appendix B."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.04942084942084942,"4
EFFICIENT LEARNING OF CORRELATED EQUILIBRIA (CE)"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.05019305019305019,"In this section, we move on to considering the harder problem of learning Correlated Equilibria (CE).
We ﬁrst present the deﬁnition of CE in Markov games."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.050965250965250966,"Deﬁnition 3 (Strategy modiﬁcation for ith player). A strategy modiﬁcation φ := {φh,s}(h,s)∈[H]×S
for player i is a set of H × S functions φh,s : (S × A)h−1 × Ai →Ai. A strategy modiﬁcation φ
can be composed with any policy π to give a modiﬁed policy φ ⋄π deﬁned as follows: At any step
h and state s with the history information τh−1 = (s1, a1, · · · , sh−1, ah−1), if π chooses to play
a = (a1, . . . , am), the modiﬁed policy φ ⋄π will play (a1, . . . , ai−1, φh,s(τh−1, ai), ai+1, . . . , am).
We use Φi denote the set of all possible strategy modiﬁcations for player i."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.051737451737451735,"Deﬁnition 4 (ε-approximate CE for general-sum MGs). We say a (general) correlated policy π is an
ε-approximate CE (ε-CE) if"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.05250965250965251,"max
i∈[m] sup
φ∈Φi"
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.05328185328185328,"
V φ⋄π
1,i (s1) −V π
1,i(s1)

≤ε."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.05405405405405406,We say π is an (exact) CE if the above is satisﬁed with ε = 0.
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.054826254826254826,"Our deﬁnition of CE follows (Liu et al., 2021) and is a natural generalization of the CE for the
well-studied special case of one-step (i.e. normal form) games (Nisan et al., 2007)."
EXPONENTIAL LOWER BOUND FOR LEARNING APPROXIMATE NASH EQUILIBRIUM,0.055598455598455596,Published as a conference paper at ICLR 2022
ALGORITHM DESCRIPTION,0.05637065637065637,"4.1
ALGORITHM DESCRIPTION"
ALGORITHM DESCRIPTION,0.05714285714285714,"Our algorithm CE-V-LEARNING (Algorithm 1) builds further on top of CCE-V-LEARNING and
Nash V-Learning, and makes several novel modiﬁcations in order to learn the CE. The key feature of
CE-V-LEARNING is that it uses a weighted swap regret algorithm (mixed-expert FTRL) for every
(s, h, i). At a high-level, CE-V-LEARNING consists of the following steps:"
ALGORITHM DESCRIPTION,0.05791505791505792,"• Line 6-11 (Sample action using mixed-expert FTRL): For each (h, s) we maintain Ai “sub-
experts” indexed by b′ ∈[Ai] (Each sub-expert represents an independent “expert” that runs
her own FTRL algorithm). Sub-expert b′ ﬁrst computes an action distribution qb′(·) ∈∆Ai via
Follow-the-Regularized-Leader (FTRL; Line 8). Then we employ a two-step sampling procedure
to obtain the action: First sample a sub-expert b from a suitable distribution µ computed from
{qb′}b′∈[Ai], then sample the actual action ah,i from qb.
• Line 13-17 (Take action and record observations): Player i takes action ah,i and observes other
player’s actions, the reward, and the next state. Sub-expert b then computes a loss estimator and
weight according to the observations, which will be used in future FTRL updates.
• Line 19 (Optimistic value update): Updates the optimistic estimate of the value V h,i using
step-size αt and bonus βt."
ALGORITHM DESCRIPTION,0.05868725868725869,"Finally, after executing Algorithm 1 for K episodes, we use the certiﬁed correlated policy procedure
(Algorithm 2) to obtain our ﬁnal output policy bπ. This procedure is a direct modiﬁcation of the
certiﬁed policy procedure of (Bai et al., 2020) and outputs a correlated policy (because the randomly
sampled k and l in line 1 and line 4 of Algorithm 2 are used by all the players) instead of product
policy. The same procedure is also used for learning CCEs earlier in Section 3."
ALGORITHM DESCRIPTION,0.05945945945945946,Here we specify the hyperparameters used in Algorithm 1:
ALGORITHM DESCRIPTION,0.06023166023166023,"αt = (H + 1)/(H + t),
ηt =
p"
ALGORITHM DESCRIPTION,0.061003861003861,"ι/(Ait),
βt = cH2Ai
p"
ALGORITHM DESCRIPTION,0.06177606177606178,"ι/t + 2cH2ι/t.
(2)"
ALGORITHM DESCRIPTION,0.06254826254826255,"The constants αj
t used in Algorithm 2 is deﬁned as"
ALGORITHM DESCRIPTION,0.06332046332046332,"α0
t := Qt
k=1 (1 −αk) ,
αj
t := αj
Qt
k=j+1 (1 −αk) .
(3)"
ALGORITHM DESCRIPTION,0.06409266409266409,"Note that for any t ≥1, {αj
t}1≤j≤t sums to one and deﬁnes a distribution over [t]."
OVERVIEW OF TECHNIQUES,0.06486486486486487,"4.2
OVERVIEW OF TECHNIQUES"
OVERVIEW OF TECHNIQUES,0.06563706563706563,Here we brieﬂy overview the techniques used in Algorithm 1.
OVERVIEW OF TECHNIQUES,0.06640926640926641,"Minimizing swap regret via mixed-expert FTRL The key technical advance in our Algorithm 1
over CCE-V-LEARNING and Nash V-Learning is the use of mixed-expert FTRL (Line 6-11). The
purpose of this is to allow the algorithm to achieve low swap regret at each (h, s) in a suitable
sense—For one-step (normal form) games, it is known that combining low-swap-regret learning for
each player leads to an approximate CE (Stoltz, 2005; Cesa-Bianchi & Lugosi, 2006). To integrate
this into Markov games, we utilize a celebrated reduction from low-swap-regret learning to usual
low-regret learning (Blum & Mansour, 2007), which for any bandit problem with Ai actions maintains
Ai sub-experts each running its own FTRL algorithm. Our particular application builds upon the
two-step randomization scheme of Ito (2020) which ﬁrst samples a sub-expert b and the action from
this sub-expert. The distribution µ(·) for sampling the sub-expert is carefully chosen by solving a
linear system (Line 10) so that µ also coincides with the (marginal) distribution of the sampled action,
from which the reduction follows."
OVERVIEW OF TECHNIQUES,0.06718146718146718,"FTRL with predictable weights Applied naively, the above reduction does not directly work for
our purpose, as our analysis requires minimizing the weighted swap regret with weights αi
t, whereas
the reduction of Ito (2020) relies crucially on the vanilla (average) regret. We address this challenge
by using a slightly modiﬁed FTRL algorithm for each sub-expert that takes in random but predictable
weights (i.e. depending fully on prior information and “external” randomness). We present the
analysis for such FTRL algorithm in Appendix F.4, and the consequent analysis for the weighted
swap regret in Appendix F.1-F.3, both of which may be of independent interest."
OVERVIEW OF TECHNIQUES,0.06795366795366796,"Proposal distributions Finally, a nuanced but important new design in CE-V-LEARNING is that all
sub-experts compute a proposal action distribution to sample the sub-expert and the associated action."
OVERVIEW OF TECHNIQUES,0.06872586872586872,Published as a conference paper at ICLR 2022
OVERVIEW OF TECHNIQUES,0.0694980694980695,Algorithm 1 CE-V-LEARNING for general-sum MGs (i-th player’s version)
OVERVIEW OF TECHNIQUES,0.07027027027027027,"Require: Hyperparameters: {αj
t}1≤j≤t≤K, {αt}1≤t≤K, {ηt}1≤t≤K, {¯βt}1≤t≤K.
1: Initialize: For any (s, a, h), set V h,i(s) ←H, Nh(s) ←0. Set µh(a|s) ←1/Ai, qb′
h (a|s) ←
1/Ai, ℓb′
h,t(a|s) ←0, N b′
h (s) ←0 for all (b′, a, h, s, t) ∈[Ai] × [Ai] × [H] × S × [K].
2: for episode k = 1, . . . , K do
3:
Receive s1.
4:
for step h = 1, . . . , H do
5:
// Compute proposal action distributions by FTRL
6:
Update accumulator t := Nh(sh) ←Nh(sh) + 1. Set ut ←αt
t/α1
t.
7:
Let tb′ ←N b′
h (sh) for all b′ ∈[Ai] for shorthand.
8:
Compute the action distribution for all sub-experts b′ ∈[Ai]:"
OVERVIEW OF TECHNIQUES,0.07104247104247104,"qb′
h (a|sh) ∝a exp
 
−(ηtb′/ut) Ptb′
τ=1 wh,τ(b′|sh)ℓb′
h,τ(a|sh)

."
OVERVIEW OF TECHNIQUES,0.07181467181467181,"9:
// Sample sub-expert b and action from qb(·)
10:
Compute µh(·|sh) ∈∆[Ai] by solving µh(·|sh) = PAi
b′=1 µh(b′|sh)qb′
h (·|sh).
11:
Sample sub-expert b ∼µh(·|sh), and then action ah,i ∼qb
h(·|sh).
12:
// Take action and feed the observations to sub-expert b
13:
Take action ah,i, observe the actions ah,−i from all other players.
14:
Observe reward rh,i = rh,i(sh, ah,i, ah,−i) and the next state sh+1 from the environment.
15:
Update accumulator for the sampled sub-expert: tb := N b
h(sh) ←N b
h(sh) + 1.
16:
Compute and update loss estimator"
OVERVIEW OF TECHNIQUES,0.07258687258687259,"ℓb
h,tb(a|sh) ←"
OVERVIEW OF TECHNIQUES,0.07335907335907337,"
H −h + 1 −(rh,i + min{V h+1,i(sh+1), H −h})

/H · 1 {ah,i = a}
qb
h(a|sh) + ηtb
."
OVERVIEW OF TECHNIQUES,0.07413127413127413,"17:
Set wh,tb(b|sh) ←ut.
18:
// Optimistic value update
19:
V h,i (sh) ←(1 −αt) V h,i (sh) + αt
 
rh,i (sh, ah,i, ah,−i) + V h+1,i (sh+1) + βt

."
OVERVIEW OF TECHNIQUES,0.0749034749034749,Algorithm 2 Certiﬁed correlated policy bπ for general-sum MGs
OVERVIEW OF TECHNIQUES,0.07567567567567568,"1: Sample k ←Uniform([K]).
2: for step h = 1, . . . , H do
3:
Observe sh, and set t ←N k
h(sh) (the value of Nh(sh) at the beginning of the k’th episode).
4:
Sample l ∈[t] with P(l = j) = αj
t (c.f. Eq. (3)).
5:
Update k ←kl
h(sh) (the episode at the end of which the state sh is observed exactly l times).
6:
Jointly take action (ah,1, ah,2, . . . , ah,m) ∼Qm
i=1 µk
h,i(·|sh), where µk
h,i(·|sh) is the policy
µh,i(·|sh) at the beginning of the k’th episode."
OVERVIEW OF TECHNIQUES,0.07644787644787644,"Then, only the sampled sub-expert takes this action, and all other proposal distributions are discarded.
This is different from the original algorithms of (Blum & Mansour, 2007; Ito, 2020) in which the
FTRL update come after the sub-expert sampling and only happens for the sampled sub-expert. Our
design is required here as otherwise the sub-experts are required to predict the next time when it is
sampled in order to compute the weighted FTRL update, which is impossible."
THEORETICAL GUARANTEE,0.07722007722007722,"4.3
THEORETICAL GUARANTEE"
THEORETICAL GUARANTEE,0.077992277992278,We are now ready to present the theoretical guarantee for our CE-V-LEARNING algorithm.
THEORETICAL GUARANTEE,0.07876447876447877,"Theorem 5 (Learning ε-approximate CE for general-sum MGs). Suppose we run the CE-V-
LEARNING algorithm (Algorithm 1) for all m players for"
THEORETICAL GUARANTEE,0.07953667953667953,"K ≥O
H6S maxi∈[m] A2
i ι
ε2
+ H4S maxi∈[m] Aiι3 ε "
THEORETICAL GUARANTEE,0.08030888030888031,Published as a conference paper at ICLR 2022
THEORETICAL GUARANTEE,0.08108108108108109,"episodes (ι
=
log(m maxi∈[m] AiHSK/(pε)) is a log factor).
Then with probability
at least 1 −p, the certiﬁed correlated policy bπ deﬁned in Algorithm 2 is an ε-CE, i.e.
maxi∈[m] supφ∈Φi(V φ⋄bπ
1,i (s1) −V bπ
1,i(s1)) ≤ε."
THEORETICAL GUARANTEE,0.08185328185328185,"Discussions
To the best of our knowledge, Theorem 5 presents the ﬁrst result for learning CE that
scales polynomially with maxi∈[m] Ai, which is signiﬁcantly better than the best known existing
algorithm of Multi-Nash-VI with CE subroutine (Liu et al., 2021) whose sample complexity scales
with Q"
THEORETICAL GUARANTEE,0.08262548262548262,"i∈[m] Ai. Similar as in Theorem 2, this follows as our CE-V-LEARNING uses decentralized
learning for each player to learn the value function (V ) . We also observe that our sample complexity
for learning CE is higher than for learning CCE by a factor of e
O(H maxi∈[m] Ai); the additional
maxi∈[m] Ai factor is expected as CE is a strictly harder notion of equilibrium. The proof of
Theorem 5 can be found in Appendix C."
LEARNING NASH EQUILIBRIA IN MARKOV POTENTIAL GAMES,0.0833976833976834,"5
LEARNING NASH EQUILIBRIA IN MARKOV POTENTIAL GAMES"
LEARNING NASH EQUILIBRIA IN MARKOV POTENTIAL GAMES,0.08416988416988418,"In this section, we consider learning Nash equilibria in Markov Potential Games (MPGs), an important
subclass of general-sum MGs. Despite the curse of number of players of learning Nash in general-sum
MGs, recent work shows that learning Nash in MPGs does not require sample size exponential in m,
by using stochastic policy gradient based algorithms (Leonardos et al., 2021; Zhang et al., 2021). In
this section, we provide an alternative algorithm NASH-CA that also achieves a mild dependence on
m and an improved dependence on ε by a simple reduction to single-agent learning."
MARKOV POTENTIAL GAMES,0.08494208494208494,"5.1
MARKOV POTENTIAL GAMES"
MARKOV POTENTIAL GAMES,0.08571428571428572,"We ﬁrst present the deﬁnition of MPGs. Our deﬁnition is the ﬁnite-horizon variant2 of the deﬁnitions
of Macua et al. (2018); Leonardos et al. (2021); Zhang et al. (2021) and is slightly more general as
we only require (4) on the total return. Throughout this section, π denotes a Markov product policy.
Deﬁnition 6. (Markov potential games) A general-sum Markov game is a Markov potential game if
there exists a potential function Φ mapping any product policy to a real number in [0, Φmax], such
that for any i ∈[m], any two policies πi, π′
i of the ith player, and any policy π−i of other players, the
difference of the value functions of the ith player with policies (πi, π−i) and (π′
i, π−i) is equals the
difference of the potential function on the same policies, i.e.,"
MARKOV POTENTIAL GAMES,0.08648648648648649,"V πi,π−i
1,i
(s1) −V π′
i,π−i
1,i
(s1) = Φ(πi, π−i) −Φ(π′
i, π−i).
(4)"
MARKOV POTENTIAL GAMES,0.08725868725868725,"Note that the range of the potential function Φmax admits a trivial upper bound Φmax ≤mH (this
can be seen by varying πi for one i at a time). An important example of MPGs is Markov Cooperative
Games (MCGs) where all players share the same reward ri ≡r."
ALGORITHM AND THEORETICAL GUARANTEE,0.08803088803088803,"5.2
ALGORITHM AND THEORETICAL GUARANTEE"
ALGORITHM AND THEORETICAL GUARANTEE,0.0888030888030888,"We present a simple algorithm NASH-CA (Nash Coordinate Ascent) for learning an ε-Nash in MPGs.
As its name suggests, the algorithm operates by solving single-agent Markov Decision Processes
(MDPs) one player at a time, and intrinsically performing coordinate ascent on the potential function
of the Markov game. Due to the potential structure of MPGs and the boundedness of the potential
function, the local improvements of players across the steps can have an accumulative effect on the
potential function, and the algorithm is guaranteed to stop after a bounded number of steps. We
give the full description of the NASH-CA in Algorithm 3. We remark that NASH-CA is additionally
guaranteed to output a pure-strategy Nash equilibrium (cf. Appendix D for deﬁnition).
Theorem 7 (Sample complexity for NASH-CA). For Markov potential games, with probability
at least 1 −p, Algorithm 3 terminates within 4Φmax/ε steps of the while loop, and outputs an
ε-approximate (pure-strategy) Nash equilibrium. The total episodes of play is at most"
ALGORITHM AND THEORETICAL GUARANTEE,0.08957528957528957,"K = O
ΦmaxH3S Pm
i=1 Aiι
ε3
+ ΦmaxH3S2 Pm
i=1 Aiι2 ε2 
,"
ALGORITHM AND THEORETICAL GUARANTEE,0.09034749034749034,where ι = log( mHSK max1≤i≤m Ai
ALGORITHM AND THEORETICAL GUARANTEE,0.09111969111969112,"εp
) is a log factor."
ALGORITHM AND THEORETICAL GUARANTEE,0.0918918918918919,2Our results can easily adapted to the discounted inﬁnite time horizon setup.
ALGORITHM AND THEORETICAL GUARANTEE,0.09266409266409266,Published as a conference paper at ICLR 2022
ALGORITHM AND THEORETICAL GUARANTEE,0.09343629343629344,"Algorithm 3 NASH-CA for Markov potential games
Require: Error tolerance ε"
ALGORITHM AND THEORETICAL GUARANTEE,0.09420849420849421,"1: Initialize: π = {πi}i∈[m], where πi = {πh,i}(h,i)∈[H]×[m] for some deterministic policy πh,i.
2: while true do
3:
Execute policy π for N = Θ( H2ι"
ALGORITHM AND THEORETICAL GUARANTEE,0.09498069498069497,"ε2 ) episodes and obtain bV1,i(π) which is the empirical average
of the total return under policy π.
4:
for player i = 1, . . . , m do"
ALGORITHM AND THEORETICAL GUARANTEE,0.09575289575289575,"5:
Fix π−i, and let the ith player run UCBVI-UPLOW (Algorithm 7) for Ki = Θ( H3SAiι ε2
+"
ALGORITHM AND THEORETICAL GUARANTEE,0.09652509652509653,H3S2Aiι2
ALGORITHM AND THEORETICAL GUARANTEE,0.0972972972972973,"ε
) episodes and get a new deterministic policy bπi."
ALGORITHM AND THEORETICAL GUARANTEE,0.09806949806949807,"6:
Execute policy (bπi, π−i) for N = Θ( H2ι"
ALGORITHM AND THEORETICAL GUARANTEE,0.09884169884169884,"ε2 ) episodes and obtain bV1,i(bπi, π−i) which is the
empirical average of the total return under policy (bπi, π−i).
7:
Set ∆i ←bV1,i(bπi, π−i) −bV1,i(π).
8:
if maxi∈[m] ∆i > ε/2 then
9:
Update πj ←bπj where j = arg maxi∈[m] ∆i.
10:
else
11:
return π"
ALGORITHM AND THEORETICAL GUARANTEE,0.09961389961389962,"Discussions
For small enough ε, the sample complexity for the NASH-CA algorithm in the above
theorem is e
O(ΦmaxH3S P"
ALGORITHM AND THEORETICAL GUARANTEE,0.10038610038610038,"i≤m Ai/ε3). As Φmax ≤mH, this at most scales with the number of
players as m P"
ALGORITHM AND THEORETICAL GUARANTEE,0.10115830115830116,"i≤m Ai, which is much better than the exponential in m sample complexity for
general-sum MGs without additional structures. Compared with recent results on learning Nash via
policy gradients (Leonardos et al., 2021; Zhang et al., 2021), the NASH-CA algorithm also achieves
poly(m, maxi≤m Ai) dependence, and signiﬁcantly improves on the ε dependence from their ε−6
to ε−3. In addition, our algorithm does not require assumptions on bounded distribution mismatch
coefﬁcient as they do, due to the exploration nature of our single-agent MDP subroutine."
ALGORITHM AND THEORETICAL GUARANTEE,0.10193050193050193,"Also, compared with the sample complexity bound e
O(H4S2 Qm
i=1 Ai/ε2) of the Nash-VI algo-
rithm (Liu et al., 2021) for general-sum MGs (not restricted to MPGs), our NASH-CA algo-
rithm doesn’t suffer from the exponential dependence on m thanks to the MPG structure. We
do achieve a looser in the dependence on ε, yet overall our sample complexity is still better un-
less ε < (Pm
i=1 Ai)/(Qm
i=1 Ai) is exponentially small. The proof of Theorem 7 can be found in
Appendix D."
ALGORITHM AND THEORETICAL GUARANTEE,0.10270270270270271,"A lower bound
To accompany Theorem 7, we establish a sample complexity lower bound of
Ω(H2 Pm
i=1 Ai/ε2) for learning pure-strategy Nash in MCGs and hence MPGs (Theorem E.1 in
Appendix E). This lower bound improves in the Ai dependence over the naive reduction to single-
player MDPs (Domingues et al., 2021), which gives Ω(H3S maxi∈[m] Ai/ε2), though is loose on
the S, H dependence. The improved Ai dependence is achieved by constructing a novel class of hard
instances of on one-step games (Lemma E.2), which may be of further technical interest. However,
there is still a large gap between these lower bounds and the best current upper bound of either our
e
O(Pm
i=1 Ai/ε3) or the e
O(Qm
i=1 Ai/ε2) of Liu et al. (2021), which we leave as future work."
CONCLUSION,0.10347490347490347,"6
CONCLUSION"
CONCLUSION,0.10424710424710425,"This paper investigates the question of when can we solve general-sum Markov games (MGs) sample-
efﬁciently with a mild dependence on the number of players. Our results show that this is possible
for learning approximate (Coarse) Correlated Equilibria in general-sum MGs, as well as learning
approximate Nash equilibrium in Markov potential games. In both cases, our sample complexity
bounds improve over existing results in many aspects. Our work opens up many interesting directions
for future work, such as sharper algorithms for both problems, sample complexity lower bounds,
or how to perform sample-efﬁcient learning in general-sum MGs with function approximations. In
addition to Markov potential games, it would also be interesting to explore alternative structural
assumptions that permit sample-efﬁcient learning."
CONCLUSION,0.10501930501930502,Published as a conference paper at ICLR 2022
CONCLUSION,0.10579150579150579,ACKNOWLEDGEMENT
CONCLUSION,0.10656370656370656,"Ziang Song is partially supported by the elite undergraduate training program of School of Mathe-
matical Sciences in Peking University."
REFERENCES,0.10733590733590734,REFERENCES
REFERENCES,0.10810810810810811,"Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1–76, 2021."
REFERENCES,0.10888030888030888,"Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pp. 263–272. PMLR,
2017."
REFERENCES,0.10965250965250965,"Yakov Babichenko. Query complexity of approximate nash equilibria. Journal of the ACM (JACM),
63(4):1–24, 2016."
REFERENCES,0.11042471042471043,"Yakov Babichenko and Siddharth Barman. Query complexity of correlated equilibrium. ACM
Transactions on Economics and Computation (TEAC), 3(4):1–9, 2015."
REFERENCES,0.11119691119691119,"Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In
International Conference on Machine Learning, pp. 551–560. PMLR, 2020."
REFERENCES,0.11196911196911197,"Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.11274131274131274,"Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efﬁcient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021."
REFERENCES,0.11351351351351352,"Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor
Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,
2019."
REFERENCES,0.11428571428571428,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.11505791505791506,"Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning
Research, 8(6), 2007."
REFERENCES,0.11583011583011583,"Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for
extensive-form correlated equilibrium. arXiv preprint arXiv:2004.00603, 2020."
REFERENCES,0.1166023166023166,"Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006."
REFERENCES,0.11737451737451737,"Xi Chen and Binghui Peng. Hedging in games: Faster convergence of external and swap regrets.
arXiv preprint arXiv:2006.04953, 2020."
REFERENCES,0.11814671814671815,"Xi Chen, Yu Cheng, and Bo Tang. Well-supported versus approximate nash equilibria: Query
complexity of large games. arXiv preprint arXiv:1511.00785, 2015."
REFERENCES,0.11891891891891893,"Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021."
REFERENCES,0.11969111969111969,"Johanne Cohen, Amélie Héliou, and Panayotis Mertikopoulos. Learning with bandit feedback in
potential games. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 6372–6381, 2017a."
REFERENCES,0.12046332046332046,"Michael B Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup B Rao, Aaron Sidford, and
Adrian Vladu. Almost-linear-time algorithms for markov chains and new spectral primitives
for directed graphs. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, pp. 410–419, 2017b."
REFERENCES,0.12123552123552124,Published as a conference paper at ICLR 2022
REFERENCES,0.122007722007722,"Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforcement
learning. arXiv preprint arXiv:1510.08906, 2015."
REFERENCES,0.12277992277992278,"Constantinos Daskalakis. On the complexity of approximating a nash equilibrium. ACM Transactions
on Algorithms (TALG), 9(3):1–35, 2013."
REFERENCES,0.12355212355212356,"Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning
in general games. arXiv preprint arXiv:2108.06924, 2021."
REFERENCES,0.12432432432432433,"Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. Episodic rein-
forcement learning in ﬁnite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory, pp. 578–598. PMLR, 2021."
REFERENCES,0.1250965250965251,"Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret
learning dynamics for extensive-form correlated equilibrium. arXiv preprint arXiv:2104.01520,
2021."
REFERENCES,0.12586872586872586,"John Fearnley and Rahul Savani. Finding approximate nash equilibria of bimatrix games via payoff
queries. ACM Transactions on Economics and Computation (TEAC), 4(4):1–19, 2016."
REFERENCES,0.12664092664092663,"John Fearnley, Martin Gairing, Paul W Goldberg, and Rahul Savani. Learning equilibria of games
via payoff queries. J. Mach. Learn. Res., 16:1305–1344, 2015."
REFERENCES,0.1274131274131274,"Paul W Goldberg and Aaron Roth. Bounds for the query complexity of approximate equilibria. ACM
Transactions on Economics and Computation (TEAC), 4(4):1–25, 2016."
REFERENCES,0.12818532818532818,"Richard W Hamming. Error detecting and error correcting codes. The Bell system technical journal,
29(2):147–160, 1950."
REFERENCES,0.12895752895752896,"Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly
polynomial for 2-player turn-based stochastic games with a constant discount factor. Journal of
the ACM (JACM), 60(1):1–16, 2013."
REFERENCES,0.12972972972972974,"Sergiu Hart. Adaptive heuristics. Econometrica, 73(5):1401–1430, 2005."
REFERENCES,0.1305019305019305,"Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127–1150, 2000."
REFERENCES,0.13127413127413126,"Sergiu Hart and Noam Nisan. The query complexity of correlated equilibria. Games and Economic
Behavior, 108:401–410, 2018."
REFERENCES,0.13204633204633204,"Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039–1069, 2003."
REFERENCES,0.13281853281853281,"Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approxima-
tion in zero-sum markov games. arXiv preprint arXiv:2107.14702, 2021."
REFERENCES,0.1335907335907336,"Shinji Ito. A tight lower bound and efﬁcient reduction for swap regret. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.13436293436293437,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efﬁcient?
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 4868–4878, 2018."
REFERENCES,0.13513513513513514,"Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large
state spaces. arXiv preprint arXiv:2106.03352, 2021."
REFERENCES,0.13590733590733592,"Robert Kleinberg, Georgios Piliouras, and Éva Tardos. Multiplicative updates outperform generic
no-regret learning in congestion games. In Proceedings of the forty-ﬁrst annual ACM symposium
on Theory of computing, pp. 533–542, 2009."
REFERENCES,0.13667953667953667,"Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.13745173745173744,"Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence
of multi-agent policy gradient in markov potential games. arXiv preprint arXiv:2106.01969, 2021."
REFERENCES,0.13822393822393822,Published as a conference paper at ICLR 2022
REFERENCES,0.138996138996139,"Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994."
REFERENCES,0.13976833976833977,"Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322–328,
2001."
REFERENCES,0.14054054054054055,"Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, pp. 7001–7010. PMLR,
2021."
REFERENCES,0.14131274131274132,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017."
REFERENCES,0.14208494208494207,"Sergio Valcarcel Macua, Javier Zazo, and Santiago Zazo. Learning parametric closed-loop policies
for markov potential games. In International Conference on Learning Representations, 2018."
REFERENCES,0.14285714285714285,"Dustin Morrill, Ryan D’Orazio, Marc Lanctot, James R Wright, Michael Bowling, and Amy R
Greenwald. Efﬁcient deviation types and learning for hindsight rationality in extensive-form games.
In International Conference on Machine Learning, pp. 7818–7828. PMLR, 2021."
REFERENCES,0.14362934362934363,"John Nash. Non-cooperative games. Annals of mathematics, pp. 286–295, 1951."
REFERENCES,0.1444015444015444,"Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.
arXiv preprint arXiv:1506.03271, 2015."
REFERENCES,0.14517374517374518,"Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic Game Theory.
Cambridge University Press, 2007."
REFERENCES,0.14594594594594595,"Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. arXiv preprint
arXiv:1703.01138, 2017."
REFERENCES,0.14671814671814673,"Julien Pérolat, Florian Strub, Bilal Piot, and Olivier Pietquin. Learning nash equilibrium for general-
sum markov games from batch data. In Artiﬁcial Intelligence and Statistics, pp. 232–241. PMLR,
2017."
REFERENCES,0.14749034749034748,"Aviad Rubinstein. Settling the complexity of computing approximate two-player nash equilibria. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 258–265.
IEEE, 2016."
REFERENCES,0.14826254826254825,"Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095–1100, 1953."
REFERENCES,0.14903474903474903,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.1498069498069498,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, 2018."
REFERENCES,0.15057915057915058,"Gilles Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD
thesis, Université Paris Sud-Paris XI, 2005."
REFERENCES,0.15135135135135136,"Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of
regularized learning in games. arXiv preprint arXiv:1507.00407, 2015."
REFERENCES,0.15212355212355214,"Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games.
In International Conference on Machine Learning, pp. 10279–10288. PMLR, 2021."
REFERENCES,0.15289575289575288,"Alexander Trott, Sunil Srinivasa, Douwe van der Wal, Sebastien Haneuse, and Stephan Zheng.
Building a foundation for data-driven, interpretable, and robust policy design using the ai economist.
arXiv preprint arXiv:2108.02904, 2021."
REFERENCES,0.15366795366795366,Published as a conference paper at ICLR 2022
REFERENCES,0.15444015444015444,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019."
REFERENCES,0.1552123552123552,"Bernhard Von Stengel and Françoise Forges. Extensive-form correlated equilibrium: Deﬁnition and
computational complexity. Mathematics of Operations Research, 33(4):1002–1022, 2008."
REFERENCES,0.155984555984556,"Jun Wang, Weinan Zhang, and Shuai Yuan. Display advertising with real-time bidding (rtb) and
behavioural targeting. arXiv preprint arXiv:1610.03013, 2016."
REFERENCES,0.15675675675675677,"Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory, pp. 3674–3682. PMLR, 2020."
REFERENCES,0.15752895752895754,"Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy ﬁnetuning: Bridging
sample-efﬁcient ofﬂine and online reinforcement learning. arXiv preprint arXiv:2106.04895, 2021."
REFERENCES,0.1583011583011583,"Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean ﬁeld multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571–5580.
PMLR, 2018."
REFERENCES,0.15907335907335907,"Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021."
REFERENCES,0.15984555984555984,"Kaiqing Zhang, Sham M Kakade, Tamer Ba¸sar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020."
REFERENCES,0.16061776061776062,"Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in multi-agent markov stochastic games:
Stationary points and convergence. arXiv preprint arXiv:2106.00198, 2021."
REFERENCES,0.1613899613899614,"Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes,
and Richard Socher. The ai economist: Improving equality and productivity with ai-driven tax
policies. arXiv preprint arXiv:2004.13332, 2020."
REFERENCES,0.16216216216216217,"A
EXPONENTIAL IN m LOWER BOUND FOR LEARNING NASH IN
GENERAL-SUM MGS"
REFERENCES,0.16293436293436295,"In this section, we give a sample complexity lower bound for computing approximate Nash equi-
librium in one-step binary-action general-sum MGs (H = 1, S = 1 and Ai = 2) which has an
exponential dependence in m, the number of players. The result is built on the lower bound of query
complexity in Rubinstein (2016)."
REFERENCES,0.1637065637065637,"We use G to denote the one-step Markov game (H = 1 and S = 1), in which there are m players and
A = 2 actions for each player. We index the players by [m] = {1, . . . , m} and denote the actions
space of each player by [A] = {1, 2}. Since we restricted attention to binary-action games (i.e.
A = 2), the total number of joint actions is 2m."
REFERENCES,0.16447876447876447,"We deﬁne a (exact) query as the procedure where the algorithm queries a joint action a ∈[A]m and
observes the (deterministic) reward ri(a) ∈[0, 1]. We deﬁne the query complexity (Chen et al., 2015)
for learning ε-approximate Nash equilibrium (ANE) as the following."
REFERENCES,0.16525096525096525,"Deﬁnition A.1 (Query complexity). The query complexity QCp(ANE(m, ε)) for learning ε-ANE is
deﬁned as the smallest n such that there exists a randomized oracle algorithm A satisfying the
following: for any binary-action, m-player game G, the algorithm A can use no more than n
sequential queries of the reward to output an ε-ANE with probability at least 1 −p."
REFERENCES,0.16602316602316602,"In one-step MGs with deterministic reward, the query complexity is equivalent to the sample com-
plexity, since each query obtains a reward entry. The following result in Rubinstein (2016) gives a
2Ω(m) query complexity lower bound for learning ε0-ANE in m-player binary action games."
REFERENCES,0.1667953667953668,Published as a conference paper at ICLR 2022
REFERENCES,0.16756756756756758,"Algorithm 4 CCE-V-Learning for General-sum MGs (i-th player’s version)
Require: Hyperparameters: {αt}1≤t≤K, {ηt}1≤t≤K, {¯βt}1≤t≤K."
REFERENCES,0.16833976833976835,"1: Initialize: For any (s, a, h), set V h,i(s) ←H, V h,i(s) ←0, Lh,i ←0, µh(a|s) ←1/Ai,
Nh(s) ←0.
2: for episode k = 1, . . . , K do
3:
Receive s1.
4:
for step h = 1, . . . , H do
5:
Take action ah ∼µh(·|sh), observe the action ah,−i from the other players.
6:
Observe reward rh,i = rh,i(sh, ah, ah,−i) and next state sh+1 from the environment.
7:
Update accumulators: t := Nh(sh) ←Nh(sh) + 1.
8:
V h,i (sh) ←(1 −αt) V h,i (sh) + αt
 
rh,i (sh, ah, ah,−i) + V h+1,i (sh+1) + βt
"
REFERENCES,0.1691119691119691,"9:
V h,i (sh) ←(1 −αt) V h,i (sh) + αt
 
rh,i (sh, ah, ah,−i) + V h+1,i (sh+1) −βt
"
REFERENCES,0.16988416988416988,"10:
for all a ∈Ai do
11:
ℓh,i(sh, a) ←
1
H [H −h+1−rh,i −min{V h+1,i(sh+1), H −h}]1 {ah = a} /[µh(ah|sh)+ηt]."
REFERENCES,0.17065637065637065,"12:
Lh,i(sh, a) ←(1 −αt)Lh,i(sh, a) + αtLh,i(sh, a)
13:
Set µh(·|sh) ∝exp[−(ηt/αt)Lh,i(sh, ·)]."
REFERENCES,0.17142857142857143,"Proposition A.2 (Corollary 4.5, (Rubinstein, 2016)). There exists absolute constants ε0 > 0 and
c > 0, such that for all m,"
REFERENCES,0.1722007722007722,"QCp(ANE(m, ε0)) = 2Ω(m), where p = 2−cm."
REFERENCES,0.17297297297297298,"This result shows that it is impossible for any algorithm to learn an ε0-ANE for every binary action
game with probability at least (1 −p) using poly(m, log(1/p)) samples: such an algorithm with
p = 2−cm would only use poly(m, log(2cm)) = poly(m) samples, yet the sample complexity lower
bound in Proposition A.2 requires at least 2Ω(m) = exp(Ω(m)) samples. Since Proposition A.2
allows ε0 = Θ(1), this also rules out the possibility of learning ε-ANE with poly(m, log(1/p), 1/ε)
samples for all small ε."
REFERENCES,0.17374517374517376,"B
PROOFS FOR SECTION 3"
REFERENCES,0.1745173745173745,"B.1
ALGORITHM FOR LEARNING CCE IN GENERAL-SUM MARKOV GAMES"
REFERENCES,0.17528957528957528,"Our algorithm used to learn CCE in general-sum MGs is a combination of Algorithm 4 and Algorithm
2. In particular, Algorithm 4 computes a set of policies and plays these policies in each episode.
Algorithm 2 used the full history in Algorithm 4 to produce a certiﬁed, general correlated policy
which we will show to be a CCE (we will also use the same Algorithm 2 to produce the certiﬁed
policy in the algorithm of learning CE). During the execution of Algorithm 2, if the index t is 0 at
some step h, the certiﬁed policy can choose any action at and after step h."
REFERENCES,0.17606177606177606,"In Algorithm 4, we choose the hyper-parameters as follows:"
REFERENCES,0.17683397683397684,αt = H + 1
REFERENCES,0.1776061776061776,"H + t ,
ηt = r"
REFERENCES,0.1783783783783784,"Hι
Ait,
βt = c r H3Aiι"
REFERENCES,0.17915057915057914,"t
+ 2cH2ι t ,"
REFERENCES,0.1799227799227799,"where c > 0 is some absolute constant, and ι = log(
m maxi∈[m] AiHSK"
REFERENCES,0.1806949806949807,"pε
) is a log factor. The choice
of ηt follows the V-OL algorithm in Tian et al. (2021) which helps to shave off an H factor in the
sample complexity compared with the original Nash V-Learning algorithm in Bai et al. (2020)."
REFERENCES,0.18146718146718147,"Here, we have a short comment on the log factor ι. In fact, we need ι to be C log(
m maxi∈[m] AiHSK"
REFERENCES,0.18223938223938224,"pε
)
for some absolute constant C. For the cleanness of the results, in this paper, we ignore this difference
since this would not harm the correctness of all the results we present."
REFERENCES,0.18301158301158302,"B.2
PROOF OF THEOREM 2"
REFERENCES,0.1837837837837838,"We begin with an auxiliary lemma on αj
t (its deﬁnition is in (3))."
REFERENCES,0.18455598455598454,Published as a conference paper at ICLR 2022
REFERENCES,0.18532818532818532,"Lemma B.1 (Lemma 4.1 in Jin et al. (2018)). The following properties hold for αj
t:"
REFERENCES,0.1861003861003861,"1.
1
√"
REFERENCES,0.18687258687258687,"t ≤Pt
j=1
αj
t
√j ≤
2
√"
REFERENCES,0.18764478764478765,t for every t ≥1 .
REFERENCES,0.18841698841698842,"2. maxj∈[t] αj
t ≤2H"
REFERENCES,0.1891891891891892,"t and Pt
j=1

αj
t
2
≤2H"
REFERENCES,0.18996138996138995,t for every t ≥1 .
REFERENCES,0.19073359073359072,"3. P∞
t=j αj
t = 1 + 1"
REFERENCES,0.1915057915057915,H for every j ≥1.
PT,0.19227799227799228,"4. Pt
j=1
αj
t
j ≥1"
PT,0.19305019305019305,2t for every t ≥1.
PT,0.19382239382239383,"Property 4 above does not appear in (Jin et al., 2018), for which we provide a quick proof here: t
X j=1"
PT,0.1945945945945946,"αj
t
j ≥ t
X"
PT,0.19536679536679535,j=[t/2]
PT,0.19613899613899613,"αj
t
j ≥1/t t
X"
PT,0.1969111969111969,"j=[t/2]
αj
t"
PT,0.19768339768339768,"(i)
≥1/(2t) t
X"
PT,0.19845559845559846,"j=1
αj
t = 1/(2t)."
PT,0.19922779922779923,"Here, (i) uses αj
t is increasing in j for ﬁxed t."
PT,0.2,"Some notations
The following notations will be used repeatedly (throughout this section and the
next section). At the beginning of any episode k for a particular state sh, we denote k1
h(sh) < · · · <"
PT,0.20077220077220076,"kNk
h(sh)
h
(sh) < k to be all the episodes that the state sh was visited, where N k
h(sh) is the times the
state sh has been visited before the start of k-th episode. When there is no confusion, we sometimes
will write kj = kj
h(sh) in short. For player i, we let V k
h,i, V
k
h,i, µk
i denote the values and policies
maintained by Algorithm 4 at the beginning of k-th episode, and ak
h denote taken action at step h and
episode k. For any joint policy µh (over all players), reward function r and value function V , we
deﬁne the operators Ph and Dµh as"
PT,0.20154440154440154,"[PhV ](s, a) := Es′∼Ph(·|s,a)V (s′),
(5)"
PT,0.2023166023166023,"Dµh[r + PhV ](s) := Eah∼µh[r(s, ah) + Esh+1∼Ph(·|s,ah)V (sh+1)].
(6)"
PT,0.2030888030888031,"Towards proving Theorem 2, we begin with a simple consequence of the update rule in Algorithm 4,
which will be used several times later."
PT,0.20386100386100386,"Lemma B.2 (Update rule). Fix a state s in time step h and ﬁx an episode k, let t = N k
h(s) and
suppose s was previously visited at episodes k1 < · · · < kt < k at the h-th step. The update rules in
Algorithm 4 gives the following equations:"
PT,0.20463320463320464,"V
k
h,i(s) = α0
tH + t
X"
PT,0.20540540540540542,"j=1
αj
t"
PT,0.20617760617760617,"
rh,i

s, akj
h , akj
h,−i

+ V
kj"
PT,0.20694980694980694,"h+1,i

skj
h+1

+ βj 
,"
PT,0.20772200772200772,"V k
h,i(s) = t
X"
PT,0.2084942084942085,"j=1
αj
t
h
rh,i

s, akj
h , akj
h,−i

+ V kj
h+1,i

skj
h+1

−βj
i
."
PT,0.20926640926640927,"We next present and prove the following lemma which helps to explain why our choice of the bonus
term is βt. The constant c in βt is actually the same with the constant c in this lemma."
PT,0.21003861003861005,"Lemma B.3 (Per-state guarantee). Fix a state s in time step h and ﬁx an episode k, let t = N k
h(s)
and suppose s was previously visited at episodes k1 < · · · < kt < k at the h-th step. With probability
at least 1 −p"
PT,0.21081081081081082,"2, for any (i, s, h, t) ∈[m] × S × [H] × [K], there exist a constant c s.t."
PT,0.21158301158301157,"max
µ∈∆Ai t
X"
PT,0.21235521235521235,"j=1
αj
tDµ×µkj
h,−i"
PT,0.21312741312741312,"
rh,i + Ph min{V
kj"
PT,0.2138996138996139,"h+1,i, H −h}

(s) − t
X"
PT,0.21467181467181468,"j=1
αj
t"
PT,0.21544401544401545,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
PT,0.21621621621621623,"h+1,i(skj
h+1), H −h}

≤c
p"
PT,0.21698841698841698,H3Aiι/t + cH2ι/t.
PT,0.21776061776061775,Published as a conference paper at ICLR 2022
PT,0.21853281853281853,"Proof of Lemma B.3
First, we decompose max
µ t
X"
PT,0.2193050193050193,"j=1
αj
tDµ×µkj
h,−i"
PT,0.22007722007722008,"
rh,i + Ph min{V
kj"
PT,0.22084942084942086,"h+1,i, H −h}

(s) − t
X"
PT,0.22162162162162163,"j=1
αj
t"
PT,0.22239382239382238,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
PT,0.22316602316602316,"h+1,i(skj
h+1), H −h}
"
PT,0.22393822393822393,"into R⋆(i, s, h, t) + U(i, s, h, t) where"
PT,0.2247104247104247,"R⋆(i, s, h, t) := max
µ t
X"
PT,0.2254826254826255,"j=1
αj
tDµ×µkj
h,−i"
PT,0.22625482625482626,"
rh,i + Ph min{V
kj"
PT,0.22702702702702704,"h+1,i, H −h}

(s) − t
X"
PT,0.2277992277992278,"j=1
αj
tDµkj
h,i×µkj
h,−i"
PT,0.22857142857142856,"
rh,i + Ph min{V
kj"
PT,0.22934362934362934,"h+1,i, H −h}

(s), and"
PT,0.23011583011583012,"U(i, s, h, t) := t
X"
PT,0.2308880308880309,"j=1
αj
tDµkj
h,i×µkj
h,−i"
PT,0.23166023166023167,"
rh,i + Ph min{V
kj"
PT,0.23243243243243245,"h+1,i, H −h}

(s) − t
X"
PT,0.2332046332046332,"j=1
αj
t"
PT,0.23397683397683397,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
PT,0.23474903474903475,"h+1,i(skj
h+1), H −h}

."
PT,0.23552123552123552,"We ﬁrst bound U(i, s, h, t). Deﬁne Fl as the σ-algebra generated by all the random variables up
to the time when sh is observed at the l-th episode. Recall that for j ≥1, kj = kj
h(s) = inf{l >
kj−1 : s is visited at step h in episode l} (with convention k0 = 0). Then {kj}j≥1 is a sequence of
increasing stopping times w.r.t. {Fl}l≥1. Deﬁne Gj = Fkj+1. So {Gj}j≥0 is also a ﬁltration. Under
Gj−1 (= Fkj), by the deﬁnition of operator D and P, we have"
PT,0.2362934362934363,"E

rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
PT,0.23706563706563707,"h+1,i(skj
h+1), H −h}
 Gj−1 "
PT,0.23783783783783785,"=Dµkj
h,i×µkj
h,−i"
PT,0.2386100386100386,"
rh,i + Ph min{V
kj"
PT,0.23938223938223938,"h+1,i, H −h}

(s)."
PT,0.24015444015444015,"So we can apply Azuma-Hoeffding inequality. Note that Pt
j=1(αj
t)2 ≤2H/t by Lemma B.1. Using
Azuma-Hoeffding inequality, we have with probability at least 1 −
p
4mHSK"
PT,0.24092664092664093,"U(i, s, h, t) = t
X"
PT,0.2416988416988417,"j=1
αj
tDµkj
h,i×µkj
h,−i"
PT,0.24247104247104248,"
rh,i + Ph min{V
kj"
PT,0.24324324324324326,"h+1,i, H −h}

(s) − t
X"
PT,0.244015444015444,"j=1
αj
t"
PT,0.24478764478764478,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
PT,0.24555984555984556,"h+1,i(skj
h+1), H −h}
 ≤"
PT,0.24633204633204633,"v
u
u
t2H2 log(4mHSK/p) t
X"
PT,0.2471042471042471,"j=1
(αj
t)2 ≤2
p"
PT,0.2478764478764479,H3ι/t.
PT,0.24864864864864866,"After taking a union bound, we have the following statement is true with probability at least 1 −p/4,"
PT,0.2494208494208494,"U(i, s, h, t) ≤2
p"
PT,0.2501930501930502,"H3ι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]."
PT,0.25096525096525096,"Then we bound R⋆(i, s, h, t). For ﬁxed (i, s, h), if we deﬁne the loss function"
PT,0.2517374517374517,ℓj(a) = 1
PT,0.2525096525096525,"H Eai=a,a−i∼µkj
h,−i[H −h + 1 −rh,i(s, a) −Ph min{V
kj"
PT,0.25328185328185326,"h+1,i, H −h}(s)] ∈[0, 1],"
PT,0.25405405405405407,Published as a conference paper at ICLR 2022
PT,0.2548262548262548,"Algorithm 5 Correlated policy bπk
h for general-sum Markov games"
PT,0.2555984555984556,"1: for step h′ = h, . . . , H do
2:
Observe sh′, and set t ←N k
h′(sh′).
3:
Sample l ∈[t] with P(l = j) = αj
t.
4:
Update k ←kl
t(sh′).
5:
Jointly take action (ah′,1, ah′,2, . . . , ah′,m) ∼Qm
i=1 µk
h′,i(·|sh′)."
PT,0.25637065637065637,"then R⋆(i, s, h, t) = H maxµ
Pt
j=1 αj
t
D
µ −µkj
h,i, ℓj
E
becomes the weighted regret with weight"
PT,0.2571428571428571,"αj
t. Note that the update rule for µkj
h,i(·|s) is essentially performing Follow-the-Regularized-Leader
(FTRL) algorithm with changing step size for each state s and each step h to solve an adversarial
bandit problem. Lemma 17 in (Bai et al., 2020)3 bounds the weight regret with high probability.
Using that lemma, we have with probability at least 1 −
p
4mHS ,"
PT,0.2579150579150579,"R⋆(i, s, h, t) ≤Hαt
t log Ai"
PT,0.25868725868725867,"ηt
+ 3HAi 2 t
X"
PT,0.2594594594594595,"j=1
ηjαj
t + H"
PT,0.2602316602316602,"v
u
u
t2ι t
X"
PT,0.261003861003861,"j=1
(αj
t)2 + H"
MAX,0.2617760617760618,"2 max
j≤t αj
tι + H max
j≤t αj
tι/ηt"
MAX,0.2625482625482625,"simultaneously for all t ∈[K] . By Lemma B.1 and ηt =
q"
MAX,0.2633204633204633,"Hι
Ait, we have with probability at least
1 −
p
4mHS"
MAX,0.2640926640926641,"R⋆(i, s, h, t) ≤2
p"
MAX,0.2648648648648649,"H3Aiι/t + 3
p"
MAX,0.26563706563706563,"H3Aiι/t + 2
p"
MAX,0.26640926640926643,"H3ι/t + H2ι/t + 2
p"
MAX,0.2671814671814672,"H3Aiι/t ≤10
p"
MAX,0.26795366795366793,H3Aiι/t + 10H2ι/t for all t ∈[K].
MAX,0.26872586872586873,"Again, taking a union bound in all (i, s, h) ∈[m] × S × [H], we have with probability at least
1 −p/4,"
MAX,0.2694980694980695,"R⋆(i, s, h, t) ≤10
p"
MAX,0.2702702702702703,"H3Aiι/t + 10H2ι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]."
MAX,0.27104247104247103,"Finally, we concluded that with probability at least 1 −p/2, we have"
MAX,0.27181467181467184,"U(i, s, h, t) + R⋆(i, s, h, t) ≤c
p"
MAX,0.2725868725868726,"H3Aiι/t + cH2ι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]"
MAX,0.27335907335907333,for some absolute constant c.
MAX,0.27413127413127414,"Recall that the certiﬁed policy bπ as in Algorithm 2 is a nested mixture of policies. We further deﬁne
policies {bπk
h}h∈[H],k∈[K] in Algorithm 5. By construction, the relationship between bπ and bπk
h is
that when players jointly play policy the bπ, they ﬁrst sample k from Uniform([K]), then they play
together the policy bπk
1 (Algorithm 5 for h = 1) with the same sampled k. As a result, we have the
following relationship:"
MAX,0.2749034749034749,"V bπ
1,i(s1) = 1 K K
X"
MAX,0.2756756756756757,"k=1
V bπk
1
1,i (s1).
(7)"
MAX,0.27644787644787644,"Deﬁnition B.4 (Policy starting from the h-th step). We deﬁne the policy starting from the h-th step
for player i as π≥h,i := {πh′,i : Ω× (S × A)h′−h × S →∆Ai}H
h′=h. At each step h′ ≥h, π≥h,i
samples action based on current state, the history starting from the h-th step and a random number
ω ∈Ω. We use Π≥h,i to denote all policies for player i starting from the h-th step. Similar to Section
2, we can deﬁne general correlated policy starting from the h-th step (where the random numbers
may be correlated for different players), and we use Π≥h to denote all such general correlated policy
starting from the h-th step."
MAX,0.27722007722007724,"3A very similar result is Lemma F.3 in our paper. However, here we need Lemma 17 in (Bai et al., 2020) to
get the optimal H dependency."
MAX,0.277992277992278,Published as a conference paper at ICLR 2022
MAX,0.27876447876447874,"For π ∈Π≥h, we can deﬁne the value function starting from the h-th step as:"
MAX,0.27953667953667954,"V π
h,i(s) := Eπ "" H
X"
MAX,0.2803088803088803,"h′=h
rh′,i|sh = s # .
(8)"
MAX,0.2810810810810811,We also deﬁne the value function of the best response as:
MAX,0.28185328185328185,"V †,πh,−i
h,i
(s) :=
max
µi∈Π≥h,i V µi×πh,−i
h,i
(s)."
MAX,0.28262548262548265,"One example of a policy starting from the h-th step is bπk
h deﬁned in Algorithm 5, so that we can"
MAX,0.2833976833976834,"deﬁne V bπk
h
h,i (s) and V
†,bπk
h,−i
h,i
(s)."
MAX,0.28416988416988415,Lemma B.5 (Valid upper and lower bounds). We have
MAX,0.28494208494208495,"V
k
h,i(s) ≥V
†,bπk
h,−i
h,i
(s),
V k
h,i(s) ≤V bπk
h
h,i (s)"
MAX,0.2857142857142857,"for all (i, s, h, k) ∈[m] × S × [H] × [K] with probability at least 1 −p."
MAX,0.2864864864864865,"Proof of Lemma B.5
We prove this lemma by backward induction over h ∈[H + 1]. The base
case of h = H + 1 is true as all the value functions equal 0 by deﬁnition. Suppose the claim is true"
MAX,0.28725868725868725,"for h + 1. We begin with upper bounding V
†,bπk
h,−i
h,i
(s). Let t = N k
h(s) and kj = kj
h(s) for 1 ≤j ≤t
to be the j’th time that s is previously visited. By the deﬁnition of certiﬁed policies bπk
h,i and by the
value iteration formula of MGs, we have for any policy µi ∈Π≥h,i,"
MAX,0.28803088803088805,"V
µi,bπk
h,−i
h,i
(s) = t
X"
MAX,0.2888030888030888,"j=1
αj
tEa∼µh,i×µkj
h,−i"
MAX,0.28957528957528955,"
rh,i(s, a) + Es′∼Ph(·|s,a)V
(µh+1:H,i|s,a),bπkj
h+1,−i
h+1,i
(s′)
 ≤ t
X"
MAX,0.29034749034749036,"j=1
αj
tEa∼µh,i×µkj
h,−i"
MAX,0.2911196911196911,"
rh,i(s, a) + Es′∼Ph(·|s,a)V
†,bπkj
h+1,−i
h+1,i
(s′)

."
MAX,0.2918918918918919,"Here, µh,i is the policy µi at the h-th step, and (µh+1:H,i|s, a) is the policy µi from time h + 1 to H
with history information at h-th step to be sh = s and ah = a. By the deﬁnition of Π≥h,i, we have
(µh+1:H,i|s, a) ∈Π≥h+1,i which implies the inequality in the equation above. By taking supremum
over µi ∈Π≥h,i and using the deﬁnition of the operator D in (6), we have"
MAX,0.29266409266409266,"V
†,bπk
h,−i
h,i
(s) ≤
sup
µi∈∆Ai t
X"
MAX,0.29343629343629346,"j=1
αj
tDµi×µkj
h,−i[rh,i + PhV
†,bπkj
h+1,−i
h+1,i
](s)."
MAX,0.2942084942084942,"Conditional on the high probability event in Lemma B.3, we use the inductive hypothesis to obtain"
MAX,0.29498069498069496,"V
†,bπk
h,−i
h,i
(s) ≤sup
µi t
X"
MAX,0.29575289575289576,"j=1
αj
tDµi×µkj
h,−i[rh,i + Ph min{V
kj"
MAX,0.2965250965250965,"h+1,i, H −h}](s) ≤ t
X"
MAX,0.2972972972972973,"j=1
αj
t"
MAX,0.29806949806949806,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
MAX,0.29884169884169887,"h+1,i(skj
h+1), H −h}

+ c
p"
MAX,0.2996138996138996,"H3Aiι/t + cH2ι/t (i)
≤ t
X"
MAX,0.30038610038610036,"j=1
αj
t"
MAX,0.30115830115830117,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
MAX,0.3019305019305019,"h+1,i(skj
h+1), H −h} + βj  ≤ t
X"
MAX,0.3027027027027027,"j=1
αj
t"
MAX,0.30347490347490347,"
rh,i

s, akj
h , akj
h,−i

+ V
kj"
MAX,0.30424710424710427,"h+1,i

skj
h+1

+ ¯βj "
MAX,0.305019305019305,"= V
k
h,i(s)."
MAX,0.30579150579150577,"Here, (i) uses our choice of βj = c
q H3Aiι"
MAX,0.3065637065637066,"j
+ 2c H2ι"
MAX,0.3073359073359073,"j
and
1
√"
MAX,0.3081081081081081,"t ≤Pt
j=1
αj
t
√j , 1"
MAX,0.3088803088803089,"t ≤Pt
j=1
2αj
t
j ."
MAX,0.3096525096525097,Published as a conference paper at ICLR 2022
MAX,0.3104247104247104,"Meanwhile, for V k
h,i(s), by the deﬁnition of certiﬁed policy and inductive hypothesis,"
MAX,0.3111969111969112,"V bπk
h
h,i (s) = t
X"
MAX,0.311969111969112,"j=1
αj
tDµkj
h [rh,i + PhV bπkj"
MAX,0.3127413127413127,"h+1,i](s) ≥ t
X"
MAX,0.31351351351351353,"j=1
αj
tDµkj
h [rh,i + Ph max{V kj
h+1,i, 0}](s)."
MAX,0.3142857142857143,"Then we note that

Dµkj
h [rh,i + Ph max{V kj
h+1,i, 0}](s) −

rh,i"
MAX,0.3150579150579151,"
s, a
kj
h
h , a
kj
h
h,−i"
MAX,0.31583011583011583,"
+ max{V"
MAX,0.3166023166023166,"kj
h
h+1,i(s
kj
h
h+1), 0}
"
MAX,0.3173745173745174,"j≥1
is a martingale difference w.r.t. the ﬁltration {Gj}j≥0, which is deﬁned in the proof of Lemma B.3.
So by Azuma-Hoeffding inequality, with probability at least 1 −
p
2mSKH t
X"
MAX,0.31814671814671813,"j=1
αj
tDµkj
h [rh,i + Ph max{V kj
h+1,i, 0}](s) ≥ t
X"
MAX,0.31891891891891894,"j=1
αj
t"
MAX,0.3196911196911197,"
rh,i"
MAX,0.3204633204633205,"
s, a
kj
h
h , a
kj
h
h,−i"
MAX,0.32123552123552124,"
+ max{V"
MAX,0.322007722007722,"kj
h
h+1,i(s
kj
h
h+1), 0}

−2 r H3ι t . (9)"
MAX,0.3227799227799228,"On this event, we have t
X"
MAX,0.32355212355212354,"j=1
αj
tDµkj
h [rh,i + Ph max
n
V kj
h+1,i, 0
o
](s) (i)
≥ t
X"
MAX,0.32432432432432434,"j=1
αj
t"
MAX,0.3250965250965251,"
rh,i"
MAX,0.3258687258687259,"
s, a
kj
h
h , a
kj
h
h,−i"
MAX,0.32664092664092664,"
+ max

V"
MAX,0.3274131274131274,"kj
h
h+1,i(s
kj
h
h+1), 0

−βj "
MAX,0.3281853281853282,"= V k
h,i(s)."
MAX,0.32895752895752894,"Here, (i) uses Pt
j=1 αj
tβj ≥2 Pt
j=1 αj
t
p"
MAX,0.32972972972972975,"H3ι/j ≥2
p"
MAX,0.3305019305019305,H3ι/t.
MAX,0.3312741312741313,"As a result, the backward induction would work well for all h as long as the inequalities in
Lemma B.3 and (9) hold for all (i, s, h, k) ∈[m] × S × [H] × [K]. Taking a union bound in all
(i, s, h, k) ∈[m] × S × [H] × [K], we have with probability at least 1 −p/2, the inequality in (9) is
true simultaneously for all (i, s, h, k) ∈[m] × S × [H] × [K]. Therefore the inequalities in Lemma
B.3 and (9) hold simultaneously for all (i, s, h, k) ∈[m] × S × [H] × [K] with probability at least
1 −p. This ﬁnishes the proof of this lemma."
MAX,0.33204633204633205,"Equipped with these lemmas, we are ready to prove Theorem 2."
MAX,0.3328185328185328,"Proof of Theorem 2
Conditional on the high probability event in Lemma B.5 (this happens with
probability at least 1 −p), we have"
MAX,0.3335907335907336,"V
k
h,i(s) ≥V
†,bπk
h,−i
h,i
(s),
V k
h,i(s) ≤V bπk
h
h,i (s)"
MAX,0.33436293436293435,"for all (i, s, h, k) ∈[m] × S × [H] × [K]. Then, choosing h = 1 and s = s1, we have"
MAX,0.33513513513513515,"V
†,bπk
1,−i
1,i
(s1) −V bπk
1
1,i (s1) ≤V
k
1,i(s1) −V k
1,i(s1)."
MAX,0.3359073359073359,"Moreover, by (7), value function of certiﬁed policy can be decomposed as"
MAX,0.3366795366795367,"V bπ
1,i(s) = 1 K K
X"
MAX,0.33745173745173745,"k=1
V bπk
1
1,i (s),
V µ1,bπ−i
1,i
(s1) = 1 K K
X"
MAX,0.3382239382239382,"k=1
V
µ1,bπk
1,−i
1,i
(s1)"
MAX,0.338996138996139,where the decomposition is due to the ﬁrst line in the Algorithm 2: sample k ←Uniform([K]).
MAX,0.33976833976833976,Published as a conference paper at ICLR 2022
MAX,0.34054054054054056,So we have
MAX,0.3413127413127413,"V †,bπ−i
1,i
(s1) −V bπ
1,i(s1) ≤1 K K
X k=1"
MAX,0.3420849420849421,"
V
†,bπk
1,−i
1,i
(s1) −V bπk
1
1,i (s1)
 ≤1 K K
X k=1 "
MAX,0.34285714285714286,"V
k
1,i(s1) −V k
1,i(s1)

."
MAX,0.3436293436293436,"To prove bπ is an approximate CCE, we only need to bound PK
k=1
"
MAX,0.3444015444015444,"V
k
1,i(s1) −V k
1,i(s1)

. Letting"
MAX,0.34517374517374516,"δk
h,i := V
k
h,i(sk
h) −V k
h,i(sk
h) and t = N k
h(sk
h). Suppose sk
h was previously visited at episodes
k1, . . . , kt at the h-th step. By the update rule,"
MAX,0.34594594594594597,"δk
h,i = V
k
h,i(sk
h) −V k
h,i(sk
h)"
MAX,0.3467181467181467,"= α0
tH + t
X"
MAX,0.3474903474903475,"j=1
αj
t[V
k
h+1,i(skj
h+1) −V k
h+1,i(skj
h+1) + 2βj]"
MAX,0.34826254826254827,"= α0
tH + t
X"
MAX,0.349034749034749,"j=1
αj
tδkj
h+1,i + t
X"
MAX,0.3498069498069498,"j=1
2αj
tβj"
MAX,0.35057915057915057,"= α0
tH + t
X"
MAX,0.35135135135135137,"j=1
αj
tδkj
h+1,i + 2c t
X"
MAX,0.3521235521235521,"j=1
αj
t s AiH3ι"
MAX,0.3528957528957529,"j
+ 4c t
X"
MAX,0.35366795366795367,"j=1
αj
t
H2ι j
."
MAX,0.3544401544401544,"We can use Lemma B.1 which gives Pt
j=1 αj
t/√j ≤2/t and maxj≤t αj
t ≤2H/t to get"
MAX,0.3552123552123552,"δk
h,i ≤α0
tH + t
X"
MAX,0.355984555984556,"j=1
αj
tδkj
h+1,i + 4c
p"
MAX,0.3567567567567568,"H3Aiι/t + 8cH3ι(1 + log t)/t,"
MAX,0.3575289575289575,"where we also uses Pt
j=1 1/j ≤1 + log t."
MAX,0.3583011583011583,"Taking the summation w.r.t. k, we begin by the ﬁrst two terms; K
X"
MAX,0.3590733590733591,"k=1
α0
tH = K
X"
MAX,0.3598455598455598,"k=1
H1 {t = 0} = K
X"
MAX,0.36061776061776063,"k=1
H1

N k
h(sk
h) = 0
	
≤SH. K
X k=1"
MAX,0.3613899613899614,"Nk
h(sk
h)
X"
MAX,0.3621621621621622,"j=1
αj
Nk
h(sk
h)δ
kj
h(sk
h)
h+1,i (i)
≤ K
X"
MAX,0.36293436293436293,"k′=1
δk′
h+1,i ∞
X"
MAX,0.3637065637065637,"j=Nk′
h (sk′
h )+1
αNk′
h (sk′
h )
j"
MAX,0.3644787644787645,"(ii)
≤(1 + 1 H ) K
X"
MAX,0.36525096525096523,"k′=1
δk′
h+1,i,"
MAX,0.36602316602316604,"where (i) is by changing the order of summation and (ii) is by Lemma B.1. So K
X"
MAX,0.3667953667953668,"k=1
δk
h,i ≤SH +

1 + 1 H  K
X"
MAX,0.3675675675675676,"k=1
δk
h+1,i + 4c
p H3Aiι K
X k=1"
Q,0.36833976833976834,"1
q"
Q,0.3691119691119691,"N k
h(s)
+ 8cH3ι K
X k=1"
Q,0.3698841698841699,"1 + log N k
h(s)
N k
h(s)
."
Q,0.37065637065637064,"By pigeonhole argument, K
X k=1"
Q,0.37142857142857144,"1
q"
Q,0.3722007722007722,"N k
h(s)
=
X s∈S"
Q,0.372972972972973,"N K
h (s)
X n=1"
Q,0.37374517374517374,"1
√n ≤O(1)
√ SK."
Q,0.3745173745173745,"Similarly, K
X k=1"
Q,0.3752895752895753,"1 + log N k
h(s)
N k
h(s)
≤O(1)(1 + log(K)S(1 + log(K/S))) ≤O(1)S(ι + ι2) ≤O(1)Sι2,"
Q,0.37606177606177604,Published as a conference paper at ICLR 2022
Q,0.37683397683397685,"where we assume ι ≥1. So we have K
X"
Q,0.3776061776061776,"k=1
δk
h,i ≤SH +

1 + 1 H  K
X"
Q,0.3783783783783784,"k=1
δk
h+1,i + O(1)
p"
Q,0.37915057915057915,H3AiSKι + O(1)SH3ι3.
Q,0.3799227799227799,"Recursing this argument for h ∈[H] gives K
X"
Q,0.3806949806949807,"k=1
δk
1,i ≤eH2S + O(1)
p"
Q,0.38146718146718145,H5AiSKι + O(1)SH4ι3.
Q,0.38223938223938225,"To conclude,"
Q,0.383011583011583,"V †,bπ−i
1,i
(s1) −V bπ
1,i(s1) ≤1 K K
X k=1 "
Q,0.3837837837837838,"V
k
1,i(s1) −V k
1,i(s1)

= 1 K K
X"
Q,0.38455598455598455,"k=1
δk
1,i"
Q,0.3853281853281853,≤O(1)SH4ι3/K + O r
Q,0.3861003861003861,"H5S max
i∈[m] Aiι/K ! ."
Q,0.38687258687258685,"Therefore, K ≥O(
H5S maxi∈[m] Aiι"
Q,0.38764478764478766,"ε2
+ SH4ι3"
Q,0.3884169884169884,"ε
) guarantees that we have V †,bπ−i
1,i
(s1) −V bπ
1,i(s1) ≤ε
for all i ∈[m]. This complete the proof of Theorem 2."
Q,0.3891891891891892,"C
PROOFS FOR SECTION 4"
Q,0.38996138996138996,"In this section we prove Theorem 5. We ﬁrst deﬁne a set of lower value estimates V k
h,i(s) (along
with the upper estimates used in Algorithm 1) via the following update rule:"
Q,0.3907335907335907,"V h,i (sh) ←(1 −αt) V h,i (sh) + αt
 
rh,i (sh, ah, ah,−i) + V h+1,i (sh+1) −βt

.
(10)"
Q,0.3915057915057915,"We emphasize that V k
h,i(s) are analyses quantities only for simplifying the proof, and are not used by
the algorithm."
Q,0.39227799227799226,We restate and use several notations we introduced in the last section. At the beginning of any episode
Q,0.39305019305019306,"k for a particular state sh, we denote k1
h(sh) < · · · < kNk
h(sh)
h
(sh) < k to be all the episodes that the
state sh was visited, where N k
h(sh) is the times the state sh has been visited before the start of k-th
episode. When there is no confusion, we sometimes will write kj = kj
h(sh) in short. For player i,"
Q,0.3938223938223938,"we let V k
h,i, V
k
h,i, µk
i denote the values and policies maintained by Algorithm 4 at the beginning of
k-th episode, and ak
h denote taken action at step h and episode k. For any joint policy µh (over all
players), reward function r and value function V , we deﬁne the operators Ph and Dµh as"
Q,0.3945945945945946,"[PhV ](s, a) := Es′∼Ph(·|s,a)V (s′),"
Q,0.39536679536679536,"Dµh[r + PhV ](s) := Eah∼µh[r(s, ah) + Esh+1∼Ph(·|s,ah)V (sh+1)]."
Q,0.3961389961389961,The following lemma is the same as Lemma B.2 in the CCE case (except that for a different algorithm).
Q,0.3969111969111969,"Lemma C.1 (Update rule). Fix a state s in time step h and ﬁx an episode k, let t = N k
h(s) and
suppose s was previously visited at episodes k1 < · · · < kt < k at the h-th step. The update rule for
V h,i(s) and V h,i(s) in Algorithm 1 and (10) gives the following equations:"
Q,0.39768339768339767,"V
k
h,i(s) = α0
tH + t
X"
Q,0.39845559845559847,"j=1
αj
t"
Q,0.3992277992277992,"
rh,i

s, akj
h , akj
h,−i

+ V
kj"
Q,0.4,"h+1,i + βj 
,"
Q,0.40077220077220077,"V k
h,i(s) = t
X"
Q,0.4015444015444015,"j=1
αj
t
h
rh,i

s, akj
h , akj
h,−i

+ V kj
h+1,i

skj
h+1

−βj
i
."
Q,0.4023166023166023,Published as a conference paper at ICLR 2022
Q,0.40308880308880307,"We next prove the following lemma which helps explain our choice of the bonus term βt. The constant
c in βt is the same with the constant c in this lemma. For any policy modiﬁcation ϕi : Ai →Ai
for the ith player and one-step policy πh : S →∆A for any h, the modiﬁed policy ϕi ⋄πh is
deﬁned as follows: if πh chooses to play a = (a1, . . . , am), the modiﬁed policy ϕi ⋄πh will play
(a1, . . . , ai−1, ϕi(ai), ai+1, . . . , am). Moreover, for πh,i : S →∆Ai, policy ϕi ⋄πh,i chooses ϕi(a)
when πh,i chooses a."
Q,0.4038610038610039,"Lemma C.2 (Per-state guarantee). Fix a state s in time step h and ﬁx an episode k, let t = N k
h(s)
and suppose s was previously visited at episodes k1 < · · · < kt < k at the h-th step. With probability
1 −p"
Q,0.4046332046332046,"2, for any (i, s, h, t) ∈[m] × S × [H] × [K], there exist a constant c s.t."
Q,0.40540540540540543,"sup
ϕi:Ai→Ai t
X"
Q,0.4061776061776062,"j=1
αj
tDϕi⋄µkj
h"
Q,0.4069498069498069,"
rh,i + Ph min{V
kj"
Q,0.40772200772200773,"h+1,i, H −h}

(s) − t
X"
Q,0.4084942084942085,"j=1
αj
t"
Q,0.4092664092664093,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
Q,0.41003861003861003,"h+1,i(skj
h+1), H −h}

≤cH2Ai
p"
Q,0.41081081081081083,2ι/t + cH2Aiι/t.
Q,0.4115830115830116,"Proof of Lemma C.2
First, like the prove in Lemma B.3, we decompose"
Q,0.41235521235521233,"sup
ϕi:Ai→Ai t
X"
Q,0.41312741312741313,"j=1
αj
tDϕi⋄µkj
h"
Q,0.4138996138996139,"
rh,i + Ph min{V
kj"
Q,0.4146718146718147,"h+1,i, H −h}

(s) − t
X"
Q,0.41544401544401544,"j=1
αj
t"
Q,0.41621621621621624,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
Q,0.416988416988417,"h+1,i(skj
h+1), H −h}
"
Q,0.41776061776061774,"into R⋆(i, s, h, t) + U(i, s, h, t) where"
Q,0.41853281853281854,"R⋆(i, s, h, t) :=
sup
ϕi:Ai→Ai t
X"
Q,0.4193050193050193,"j=1
αj
tDϕi⋄µkj
h"
Q,0.4200772200772201,"
rh,i + Ph min{V
kj"
Q,0.42084942084942084,"h+1,i, H −h}

(s) − t
X"
Q,0.42162162162162165,"j=1
αj
tEa−i∼µkj
h,−i"
Q,0.4223938223938224,"
rh,i

s, akj
h , a−i

+ Esh+1∼Ph(·|s,akj
h ,a−i) min{V
kj"
Q,0.42316602316602314,"h+1,i(sh+1), H −h}

, and"
Q,0.42393822393822395,"U(i, s, h, t) := t
X"
Q,0.4247104247104247,"j=1
αj
tEa−i∼µkj
h,−i"
Q,0.4254826254826255,"
rh,i

s, akj
h , a−i

+ Esh+1∼Ph(·|s,akj
h ,a−i) min{V
kj"
Q,0.42625482625482625,"h+1,i(sh+1), H −h}
 − t
X"
Q,0.42702702702702705,"j=1
αj
t"
Q,0.4277992277992278,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
Q,0.42857142857142855,"h+1,i(skj
h+1), H −h}

."
Q,0.42934362934362935,"We ﬁrst bound U(i, s, h, t). By the same reason in proof of Lemma B.3, we can apply Azuma-
Hoeffding inequality. Note that Pt
j=1(αj
t)2 ≤2H/t by Lemma B.1. Using Azuma-Hoeffding
inequality, we have with probability at least 1 −
p
4mHSK ,"
Q,0.4301158301158301,"U(i, s, h, t) ≤"
Q,0.4308880308880309,"v
u
u
t2H2 log(4mHSK/p) t
X"
Q,0.43166023166023165,"j=1
(αj
t)2 ≤2
p"
Q,0.43243243243243246,H3ι/t.
Q,0.4332046332046332,"After taking a union bound, the following statement is true with probability at least 1 −p/4,"
Q,0.43397683397683395,"U(i, s, h, t) ≤2
p"
Q,0.43474903474903476,"H3ι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]."
Q,0.4355212355212355,"Then we bound R⋆(i, s, h, t). For ﬁxed (i, s, h), we deﬁne loss function"
Q,0.4362934362934363,ℓj(a) = 1
Q,0.43706563706563706,"H Eai=a,a−i∼µkj
h,−i[H −h + 1 −rh,i(s, a) −Esh+1∼Ph(·|s,a,a−i) min{V
kj"
Q,0.43783783783783786,"h+1,i(sh+1), H −h}]."
Q,0.4386100386100386,Published as a conference paper at ICLR 2022
Q,0.43938223938223936,"Algorithm 6 Correlated policy bπk
h for general-sum Markov games"
Q,0.44015444015444016,"1: for step h′ = h, . . . , H do
2:
Observe sh′, and set t ←N k
h′(sh′).
3:
Sample l ∈[t] with P(l = j) = αj
t.
4:
Update k ←kl
t(sh′).
5:
Jointly take action (ah′,1, ah′,2, . . . , ah′,m) ∼Qm
i=1 µk
h′,i(·|sh′)."
Q,0.4409266409266409,"Then we have ℓj(a) ∈[0, 1] for all a and the realized loss function:"
Q,0.4416988416988417,"˜ℓj(akj
h ) = 1"
Q,0.44247104247104246,"H [H −h + 1 −rh,i(s, akj
h , akj
−i) −min{V
kj"
Q,0.44324324324324327,"h+1,i(skj
h+1), H −h}] ∈[0, 1]"
Q,0.444015444015444,"is an unbiased estimator of ℓj(akj
h ). Then R⋆(i, s, h, t) can be written as"
Q,0.44478764478764476,"R⋆(i, s, h, t) = H
sup
ϕi:Ai→Ai t
X"
Q,0.44555984555984557,"j=1
αj
t[ℓj(akj
h,i) −⟨ϕi ⋄µkj
h,i, ℓj⟩]."
Q,0.4463320463320463,"Now, for any ﬁxed step h and state s, the distributions

qb
h(·|s)"
Q,0.4471042471042471,"b and visitation counts

N b
h(s)"
Q,0.44787644787644787,"b
are only updated at episodes k1
h(s), . . . , kt
h(s). Further, these updates are exactly equivalent to the
mixed-expert FTRL update algorithm which we describe in Algorithm 8. Therefore, the R⋆(i, s, h, t)
above can be bounded by the weighted swap regret bound of Lemma F.1 (choosing the log term as
ι = 4 log 10mSAHK"
Q,0.4486486486486487,"p
) to yield that"
Q,0.4494208494208494,"H
sup
ϕi:Ai→Ai t
X"
Q,0.45019305019305017,"j=1
αj
t[ℓj(akj
h,i) −⟨ϕi ⋄µkj
h,i, ℓj⟩] ≤40H2Ai
p"
Q,0.450965250965251,ι/t + 40H2Aiι/t for all t ∈[K]
Q,0.4517374517374517,"with probability at least 1 −p/(4mSH). Taking a union bound over all (i, s, h) ∈[m] × S × [H],
we have with probability at least 1 −p/4,"
Q,0.4525096525096525,"R⋆(i, s, h, t) ≤40H2Ai
p"
Q,0.4532818532818533,"ι/t + 40H2Aiι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]."
Q,0.4540540540540541,"Finally, we conclude that with probability at least 1 −p/2, we have"
Q,0.4548262548262548,"U(i, s, h, t) + R⋆(i, s, h, t) ≤cH2Ai
p"
Q,0.4555984555984556,"ι/t + cH2Aiι/t for all (i, s, h, t) ∈[m] × S × [H] × [K]"
Q,0.4563706563706564,for some absolute constant c.
Q,0.45714285714285713,"We deﬁne the auxiliary certiﬁed policies bπk
h in Algorithm 6 (same as Algorithm 5 for the CCE case
but repeated here for clarity). Again, we have the following relationship:"
Q,0.45791505791505793,"V bπ
1,i(s) = 1 K K
X"
Q,0.4586872586872587,"k=1
V bπk
1
1,i (s).
(11)"
Q,0.4594594594594595,"Deﬁnition C.3 (Policy modiﬁcation starting from the h-th step). A strategy modiﬁcation starting
from the h-th step φ≥h := {φh′,s}(h′,s)∈{h,h+1,...,H}×S for player i is a set of S × (H −h + 1)"
Q,0.46023166023166023,"functions φh′,s : (S × A)h′−h × Ai →Ai. This strategy modiﬁcation φ≥h can be composed
with any policy π ∈Π≥h (as in Deﬁnition B.4) to give a modiﬁed policy φ≥h ⋄π deﬁned as
follows: At any step h′ ≥h and state s with the history information starting from the h-th step
τh:h′−1 = (sh, ah, · · · , sh′−1, ah′−1), if π chooses to play a = (a1, . . . , am), the modiﬁed policy
φ ⋄π will play (a1, . . . , ai−1, φh′,s(τh:h′−1, ai), ai+1, . . . , am). We use Φ≥h,i denote the set of all
such possible strategy modiﬁcations for player i."
Q,0.461003861003861,"For any φ ∈Φ≥h,i, φ ⋄bπk
h also doesn’t depend on the history before the h-th step, so φ ⋄bπk
h ∈Π≥h,"
Q,0.4617760617760618,"which implies that V φ⋄bπk
h
h,i
(s) is well-deﬁned in (8)."
Q,0.46254826254826253,Published as a conference paper at ICLR 2022
Q,0.46332046332046334,Lemma C.4 (Valid upper and lower bounds). We have
Q,0.4640926640926641,"V
k
h,i(s) ≥
sup
φ∈Φ≥h,i
V φ⋄bπk
h
h,i
(s),
V k
h,i(s) ≤V bπk
h
h,i (s)"
Q,0.4648648648648649,"for all (i, k, h, s) ∈[m] × [K] × [H] × S with probability at least 1 −p/2."
Q,0.46563706563706564,"Proof of Lemma C.4
We prove this lemma by backward induction over h ∈[H + 1]. The base
case of h = H + 1 is true as all the value functions equal 0. Suppose the claim is true for h + 1. We
begin with upper bounding supφ∈Φ≥h,i V φ⋄bπk
h
h,i
(s). Let t = N k
h(s) and kj = kj
h(s) for 1 ≤j ≤t to
be the j’th time that s is previously visited. By the deﬁnition of certiﬁed policies bπk
h,i and by the
value iteration formula of MGs, we have for any φ ∈Φ≥h,i,"
Q,0.4664092664092664,"V φ⋄bπk
h
h,i
(s) = t
X"
Q,0.4671814671814672,"j=1
αj
tEa∼φh,i⋄µkj
h"
Q,0.46795366795366794,"
rh,i(s, a) + Es′∼Ph(·|s,a)V
(φ≥h+1|s,a)⋄bπkj
h+1
h+1,i
(s′)
 ≤ t
X"
Q,0.46872586872586874,"j=1
αj
tEa∼φh,i⋄µkj
h """
Q,0.4694980694980695,"rh,i(s, a) + Es′∼Ph(·|s,a)
sup
φ∈Φ≥h+1,i
V
φ⋄bπkj
h+1
h+1,i
(s′) # ."
Q,0.4702702702702703,"Here, φh,i is the strategy modiﬁcation function φ at the h-th step, and (φ≥h+1|s, a) is the modiﬁcation
φ from the h + 1-th step with history information to be sh = s and ah = a. By the deﬁnition of
Φ≥h,i, we have (φ≥h+1|s, a) ∈Φ≥h+1,i which implies the above inequality. By taking supremum
over φ ∈Φ≥h,i and using the deﬁnition of the operator D in (6), we have"
Q,0.47104247104247104,"sup
φ∈Φ≥h,i
V φ⋄bπk
h
h,i
(s) ≤
sup
ϕi:Ai→Ai t
X"
Q,0.4718146718146718,"j=1
αj
tDϕi⋄µkj
h [rh,i + Ph
sup
φ∈Φ≥h+1,i
V
φ⋄bπkj
h+1
h+1,i
](s)."
Q,0.4725868725868726,"Condition on the high probability event (with probability at least 1 −p/2) in Lemma C.2, we can use
the inductive hypothesis to obtain"
Q,0.47335907335907335,"sup
ϕi:Ai→Ai t
X"
Q,0.47413127413127415,"j=1
αj
tDϕi⋄µkj
h [rh,i + Ph
sup
φ∈Φ≥h+1,i
V
φ⋄bπkj
h+1
h+1,i
](s)"
Q,0.4749034749034749,"≤
sup
ϕi:Ai→Ai t
X"
Q,0.4756756756756757,"j=1
αj
tDϕi⋄µkj
h [rh,i + Ph min{V
kj"
Q,0.47644787644787645,"h+1,i, H −h}](s) ≤ t
X"
Q,0.4772200772200772,"j=1
αj
t"
Q,0.477992277992278,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
Q,0.47876447876447875,"h+1,i(skj
h+1), H −h}

+ cH2Ai
p"
Q,0.47953667953667956,"ι/t + cH2Aiι/t (i)
≤ t
X"
Q,0.4803088803088803,"j=1
αj
t"
Q,0.4810810810810811,"
rh,i

s, akj
h , akj
h,−i

+ min{V
kj"
Q,0.48185328185328186,"h+1,i(skj
h+1), H −h} + βj  ≤ t
X"
Q,0.4826254826254826,"j=1
αj
t"
Q,0.4833976833976834,"
rh,i

s, akj
h , akj
h,−i

+ V
kj"
Q,0.48416988416988416,"h+1,i + ¯βj "
Q,0.48494208494208496,"=V
k
h,i(s)."
Q,0.4857142857142857,"Here, (i) uses our choice of βj = cH2Ai
q"
Q,0.4864864864864865,"ι
j + 2c H2Aiι"
Q,0.48725868725868726,"j
and
1
√j ≤Pt
j=1
αj
t
√j , 1"
Q,0.488030888030888,"t ≤Pt
j=1
2αj
t
j , so"
Q,0.4888030888030888,"that cH2Ai
p"
Q,0.48957528957528956,"ι/t + cH2Aiι/t ≤P αj
tβj."
Q,0.49034749034749037,"Meanwhile, for V k
h,i(s), by the deﬁnition of certiﬁed policy and inductive hypothesis,"
Q,0.4911196911196911,"V bπk
h
h,i (s) = t
X"
Q,0.4918918918918919,"j=1
αj
tDµkj
h [rh,i + PhV
bπkj
h+1
h+1,i](s) ≥ t
X"
Q,0.49266409266409267,"j=1
αj
tDµkj
h [rh,i + Ph max{V kj
h+1,i, 0}](s)."
Q,0.4934362934362934,Published as a conference paper at ICLR 2022
Q,0.4942084942084942,"Note that

Dµkj
h [rh,i + Ph max
n
V kj
h+1,i, 0
o
](s) −

rh,i"
Q,0.49498069498069497,"
s, a
kj
h
h , a
kj
h
h,−i"
Q,0.4957528957528958,"
+ max{V"
Q,0.4965250965250965,"kj
h
h+1,i(s
kj
h
h+1), 0}
"
Q,0.4972972972972973,"j≥1
is a martingale difference sequence w.r.t. the ﬁltration {Gj}j≥0, which is deﬁned in the proof of
Lemma B.3. So by Azuma-Hoeffding inequality, with probability at least 1 −
p
2mSKH , t
X"
Q,0.4980694980694981,"j=1
αj
tDµkj
h [rh,i + Ph max{V kj
h+1,i, 0}](s) ≥ t
X"
Q,0.4988416988416988,"j=1
αj
t"
Q,0.4996138996138996,"
rh,i"
Q,0.5003861003861004,"
s, a
kj
h
h , a
kj
h
h,−i"
Q,0.5011583011583012,"
+ max{V"
Q,0.5019305019305019,"kj
h
h+1,i(s
kj
h
h+1), 0}

−2 r H3ι t . (12)"
Q,0.5027027027027027,"On this event, we have t
X"
Q,0.5034749034749034,"j=1
αj
tDµkj
h [rh,i + Ph max
n
V kj
h+1,i, 0
o
](s) (i)
≥ t
X"
Q,0.5042471042471043,"j=1
αj
t"
Q,0.505019305019305,"
rh,i"
Q,0.5057915057915058,"
s, a
kj
h
h , a
kj
h
h,−i"
Q,0.5065637065637065,"
+ max

V"
Q,0.5073359073359074,"kj
h
h+1,i(s
kj
h
h+1), 0

−βj  ≥ t
X"
Q,0.5081081081081081,"j=1
αj
t"
Q,0.5088803088803089,"
rh,i"
Q,0.5096525096525096,"
s, a
kj
h
h , a
kj
h
h,−i 
+ V"
Q,0.5104247104247104,"kj
h
h+1,i(s
kj
h
h+1) −βj "
Q,0.5111969111969112,"= V k
h,i(s)."
Q,0.511969111969112,"Here, (i) uses Pt
j=1 αj
tβj ≥2 Pt
j=1 αj
t
p"
Q,0.5127413127413127,"H3ι/j ≥2
p"
Q,0.5135135135135135,H3ι/t.
Q,0.5142857142857142,"As a result, the backward induction would work well for all h as long as the inequalities in
Lemma C.2 and (12) hold for all (i, s, h, k) ∈[m] × S × [H] × [K]. Taking a union bound in all
(i, s, h, k) ∈[m] × S × [H] × [K], we have with probability at least 1 −p/2, the inequality in (12) is
true simultaneously for all (i, s, h, k) ∈[m] × S × [H] × [K]. Therefore the inequalities in Lemma
B.3 and (12) hold simultaneously for all (i, s, h, k) ∈[m] × S × [H] × [K] with probability at least
1 −p. This ﬁnishes the proof of this lemma."
Q,0.5150579150579151,"Equipped with these lemmas, we are ready to prove Theorem 5."
Q,0.5158301158301158,"Proof of Theorem 5
Conditional on the high probability event in Lemma C.4 (this happens with
probability at least 1 −p), we have"
Q,0.5166023166023166,"V
k
h,i(s) ≥
sup
φ∈Φ≥h,i
V φ⋄bπk
h
h,i
(s),
V k
h,i(s) ≤V bπk
h
h,i (s)"
Q,0.5173745173745173,"for all (i, s, h, k) ∈[m] × S × [H] × [K]. Then, choosing h = 1 and s = s1, we have"
Q,0.5181467181467182,"sup
φ∈Φi
V φ⋄bπk
1
1,i
(s1) −V bπk
1
1,i (s1) ≤V
k
1,i(s1) −V k
1,i(s1)."
Q,0.518918918918919,"Moreover, by (11), value function of certiﬁed policy can be decomposed as"
Q,0.5196911196911197,"V bπ
1,i(s1) = 1 K K
X"
Q,0.5204633204633204,"k=1
V bπk
1
1,i (s1),
V φ⋄bπ
1,i (s1) = 1 K K
X"
Q,0.5212355212355212,"k=1
V φ⋄bπk
1
1,i
(s1),"
Q,0.522007722007722,"where the decomposition is due to the ﬁrst line in the Algorithm 2: sample k ←Uniform([K]).
Therefore we have the following bound on supφ∈Φi V φ⋄bπ
1,i (s1) −V bπ
1,i(s1):"
Q,0.5227799227799228,"sup
φ∈Φi
V φ⋄bπ
1,i (s1) −V bπ
1,i(s1) = sup
φ∈Φi"
K,0.5235521235521235,"1
K K
X"
K,0.5243243243243243,"k=1
V φ⋄bπk
1
1,i
(s1) −V bπ
1,i(s1)"
K,0.525096525096525,"Published as a conference paper at ICLR 2022 ≤1 K K
X k=1 "
K,0.5258687258687259,"sup
φ∈Φi
V φ⋄bπk
1
1,i
(s1) −V bπk
1
1,i (s1) ! ≤1 K K
X k=1 "
K,0.5266409266409267,"V
k
1,i(s1) −V k
1,i(s1)

."
K,0.5274131274131274,"By Lemma C.4 Letting δk
h,i := V
k
h,i(sk
h) −V k
h,i(sk
h) and t = N k
h(sk
h). By the update rule, we have"
K,0.5281853281853282,"δk
h,i = V
k
h,i(sk
h) −V k
h,i(sk
h)"
K,0.528957528957529,"= α0
tH + t
X"
K,0.5297297297297298,"j=1
αj
t[V
k
h+1,i(skj
h+1) −V k
h+1,i(skj
h+1) + 2βj]"
K,0.5305019305019305,"= α0
tH + t
X"
K,0.5312741312741313,"j=1
αj
tδkj
h+1,i + t
X"
K,0.532046332046332,"j=1
2αj
tβj"
K,0.5328185328185329,"= α0
tH + t
X"
K,0.5335907335907336,"j=1
αj
tδkj
h+1,i + 2cAiH2
t
X"
K,0.5343629343629344,"j=1
αj
t r ι"
K,0.5351351351351351,"j + 4c t
X"
K,0.5359073359073359,"j=1
αj
t
H2Aiι j
."
K,0.5366795366795367,"Taking the summation w.r.t. k, by the same argument in the proof of Theorem 2, we can get"
K,0.5374517374517375,"max
φ∈Φ V φ⋄bπ
1,i (s1) −V bπ
1,i(s1) = 1 K K
X k=1 "
K,0.5382239382239382,"V
k
1,i(s1) −V k
1,i(s1)

= 1 K K
X"
K,0.538996138996139,"k=1
δk
1,i"
K,0.5397683397683398,"≤O(1)SH4 max
i∈[m] Aiι3/K + O r"
K,0.5405405405405406,"H6S max
i∈[m] A2
i ι/K ! ."
K,0.5413127413127413,"Therefore, if K
≥O(
H6S maxi∈[m] A2
i ι
ε2
+
H4S maxi∈[m] Aiι3"
K,0.5420849420849421,"ε
), we have maxφ∈Φ V φ⋄bπ
1,i (s1) −
V bπ
1,i(s1) ≤ε holds for all i ∈[m], which means bπ is an ε-approximate CE. This completes the proof
of Theorem 2."
K,0.5428571428571428,"D
PROOFS FOR SECTION 5"
K,0.5436293436293437,"Here, we ﬁrst deﬁne pure-strategy Nash equilibrium. We say policy π is a pure strategy (deterministic
policy) if and only if for any (h, i, s) ∈[H]×[m]×S, πh,i(ai|s) = Iai=a′
h,i(s) for some a′
h,i(s). We
say π a pure-strategy Nash equilibrium if π is a pure-strategy and is a Nash equilibrium. Similarly, we
say π is a pure strategy ε-approximate Nash equilibrium if π is a pure strategy and NE-gap(π) ≤ε.
Pure-strategy Nash equilibrium does not always exist in general-sum MGs, but is guaranteed to exist
(as we will see) in Markov Potential Games."
K,0.5444015444015444,"D.1
EXISTENCE OF PURE-STRATEGY NASH EQUILIBRIA IN MARKOV POTENTIAL GAMES"
K,0.5451737451737452,"A particular property of MPGs is that, there always exists a pure-strategy Nash equilibrium. Such
a property does not hold for every general-sum MG. Pure-strategy Nash equilibria are preferred in
many scenarios since each player can take deterministic actions.
Proposition D.1. For any Markov potential games, there exists a pure-strategy Nash equilibrium."
K,0.5459459459459459,"See Theorem 3.1 in Leonardos et al. (2021) or Proposition 1 in Zhang et al. (2021) for a proof of
Proposition D.1."
K,0.5467181467181467,"D.2
THE UCBVI-UPLOW SUB-ROUTINE"
K,0.5474903474903475,"In this subsection we consider the problem of learning a near optimal policy in the ﬁxed horizon
stochastic reward RL problem MDP(H, S, A, P, r). The setting is standard (c.f. Jin et al. (2018))
and is a special case of Markov games (c.f. Section 2) by setting the number of agents m = 1. We
will use the same notations including policies and value functions as that of the Markov games as"
K,0.5482625482625483,Published as a conference paper at ICLR 2022
K,0.549034749034749,Algorithm 7 UCB-VI with Upper and Lower Conﬁdence Bounds (UCBVI-UPLOW)
K,0.5498069498069498,"1: Initialize: For any (s, a, h, s′): Qh(s, a) ←H, Qh(s, a) ←0, Nh(s) = Nh(s, a) =
Nh(s, a, s′) ←0.
2: for episode k = 1, . . . , K do
3:
for step h = H, . . . , 1 do
4:
for (s, a) ∈S × A do
5:
Set t ←Nh(s, a).
6:
if t > 0 then
7:
β ←BONUS(t, bVh[(V h+1 + V h+1)/2](s, a)) (c.f. Eq. (13))"
K,0.5505791505791506,"8:
γ ←(c/H) · bPh(V h+1 −V h+1)(s, a)."
K,0.5513513513513514,"9:
Qh(s, a) ←min
n
(rh + bPhV h+1)(s, a) + γ + β, H
o
."
K,0.5521235521235521,"10:
Qh(s, a) ←max
n
(rh + bPhV h+1)(s, a) −γ −β, 0
o"
K,0.5528957528957529,"11:
for s ∈S do
12:
πh(s) ←arg maxa∈A Qh(s, a).
13:
V h(s) ←Qh(s, πh(s)); V h(s) ←Qh(s, πh(s)).
14:
Receive the initial state s1 from the MDP.
15:
for step h = 1, . . . , H do
16:
Take action ah = πh(sh), observe reward rh and next state sh+1.
17:
Increment Nh(sh), Nh(sh, ah), and Nh(sh, ah, sh+1) by 1.
18:
bPh(·|sh, ah) ←Nh(sh, ah, ·)/Nh(sh, ah)."
K,0.5536679536679536,"19: Let (V
k
h, V k
h, πk) denote the value estimates and policy at the beginning of episode k."
K,0.5544401544401545,"20: return Policy πk⋆where k⋆= arg mink∈[K](V
k
1(s1) −V k
1(s1))."
K,0.5552123552123552,"introduced in Section 2, except that we will omit the sub-scripts i’s since here we just have a single
agent."
K,0.555984555984556,"We consider the UCBVI-UPLOW algorithm (Algorithm 7), which is adapted from (Xie et al., 2021;
Liu et al., 2021), for learning approximate optimal policy in reinforcement learning problems. Such an
algorithm is used as a sub-routine in Algorithm 3 to learn approximate pure-strategy Nash equilibria
in MPGs. We remark that although in Algorithm 3 we propose to use the UCBVI-UPLOW algorithm
to search for a near optimal policy, many alternative algorithms can be used to ﬁnd the near optimal
policy (e.g., UCBVI (Azar et al., 2017) or Q-learning (Jin et al., 2018)). Here we choose the UCBVI-
UPLOW algorithm because 1) it has a tight sample complexity bound; 2) it outputs a deterministic
policy which can be used to ﬁnd a pure-strategy approximate Nash equilibrium."
K,0.5567567567567567,"In the description of Algorithm 7, the bPh quantity appeared in lines 8,9,10, and 18 can be viewed
either as a set of empirical probability distributions or as an operator: for any ﬁxed (sh, ah), we can
view bPh(·|sh, ah) as a probability distribution over S; for any given function V : S →R, we can
view bPh as an operator by deﬁning (bPhV )(s, a) := Es′∼bPh(·|s,a)[V (s′)]. The bVh operator in line 7 is"
K,0.5575289575289575,"the empirical variance operator deﬁned as bVhV = bPhV 2 −(bPhV )2. The BONUS function in the
algorithm is chosen to be the Bernstein type bonus function"
K,0.5583011583011583,"BONUS(t, bσ2) = c(
p"
K,0.5590733590733591,"bσ2ι/t + H2Sι/t).
(13)"
K,0.5598455598455598,"We have the following sample complexity guarantee for the UCBVI-UPLOW algorithm returning
an ε-approximate optimal policy.
Lemma D.2. The UCBVI-UPLOW algorithm always returns a deterministic policy πk⋆. Moreover,
for any p ∈(0, 1], letting ι = log(SAHK/p) and taking the number of episodes"
K,0.5606177606177606,"K ≥O(H3SAι/ε2 + H3S2Aι2/ε),"
K,0.5613899613899614,"then with probability at least 1 −p, the returned policy πk⋆is ε-approximate optimal, i.e.,
supµ V µ
1 (s1) −V πk⋆
1
(s1) ≤ε."
K,0.5621621621621622,"Proof of Lemma D.2
First, πk⋆is obviously deterministic from line 12 of Algorithm 7."
K,0.5629343629343629,Published as a conference paper at ICLR 2022
K,0.5637065637065637,"The sample complexity guarantee of the UCBVI-UPLOW algorithm is a consequence of the sample
complexity guarantee of the Nash-VI algorithm for learning Nash in zero-sum Markov games as
proved in Liu et al. (2021)."
K,0.5644787644787644,"More speciﬁcally, we denote the rewards and transition matrices by {rh(s, a)}h∈[H] and P =
{Ph(sh+1|sh, ah)}h∈[H] for the MDP(H, S, A, P, r). Such a MDP can be viewed as a zero-sum
Markov game MG(H, S, A, B, P, ¯r), in which (H, S, A) are the same as that of the MDP, the action
space for the min-player is a singleton B = 1 so that the rewards ¯rh(s, a, b) ≡rh(s, a) do not depend
on the action of the min-player and the transition matrices Ph(·|s, a, b) = Ph(·|s, a1) do not depend
on the action of the min-player either. Then, for any ε-approximate Nash equilibrium (µ, ν) of the
associated zero-sum Markov game, the max-player’s policy µ must be ε-approximate optimal for the
original MDP."
K,0.5652509652509653,"By this correspondence, the UCBVI-UPLOW algorithm is actually a speciﬁc version of the Nash-VI
algorithm in Liu et al. (2021), and Line 12 in UCBVI-UPLOW is actually a speciﬁc version of
line 12 in Nash-VI in Liu et al. (2021): this is because in this speciﬁc Markov game, Q(s, a, b) and
Q(s, a, b) only depend on s and a, so that πh(s) = arg maxa∈A Qh(s, a) is actually in the CCE set
CCE(Qh(s, a, b), Qh(s, a, b))."
K,0.566023166023166,"By this reduction and by Theorem 4 in Liu et al. (2021), this lemma is proved."
K,0.5667953667953668,"D.3
PROOF OF THEOREM 7"
K,0.5675675675675675,We use superscript t to represent variables at the t-th step (before π is updated) of the while loop.
K,0.5683397683397683,"Because we can choose the log factor as ι = 4 log(
mHSK maxi∈[m] Ai"
K,0.5691119691119692,"εp
) (this doesn’t affect the
correctness of the theorem), for each execution of UCBVI-UPLOW, by Lemma D.2, it return a
ε/4-optimal deterministic policy with probability at least 1 −
pε
8m2H . Taking a union bound, we have"
K,0.5698841698841699,"max
µi V1,i(µi, πt
−i) −V1,i(bπt
i, πt
−i) ≤ε/4
(14)"
K,0.5706563706563706,"simultaneously for all i ∈[m] and t ≤4mH/ε with probability at least 1 −p/2. For the empirical
estimator bV t
1,i, it’s bounded in [0, H]. Thus by Hoeffding’s inequality, for ﬁxed i ∈[m] and t"
K,0.5714285714285714,"P(|bV t
1,i −V t
1,i| ≥ε/8) ≤2 exp

−Nε2 32H2 
."
K,0.5722007722007721,"Choosing N = CH2ι/ε2 for some large constant C, we have"
K,0.572972972972973,"P(|bV t
1,i −V t
1,i| ≥ε/8) ≤
εp
16m2H ."
K,0.5737451737451738,"Apply this inequality to bV t
1,i(bπt
i, πt
−i) and bV1,i(πt) and taking a union bound, we have"
K,0.5745173745173745,"|bV t
1,i(bπt
i, πt
−i) −V1,i(bπt
i, πt
−i)| ≤ε/8,
|bV t
1,i(πt) −V1,i(πt)| ≤ε/8
(15)"
K,0.5752895752895753,simultaneously for all i ∈[m] and t ≤4mH
K,0.5760617760617761,"ε
with probability at least 1 −p/2. As a result, by (14)
and (15), we have
max
µi V1,i(µi, πt
−i) −V1,i(bπt
i, πt
−i) ≤ε/4"
K,0.5768339768339769,"|bV t
1,i(bπt
i, πt
−i) −V1,i(bπt
i, πt
−i)| ≤ε/8"
K,0.5776061776061776,"|bV t
1,i(πt) −V1,i(πt)| ≤ε/8 (16)"
K,0.5783783783783784,"simultaneously for all i ∈[m] and t ≤4mH/ε with probability at least 1 −p. On this event,"
K,0.5791505791505791,"∆t
i = bV t
1,i(bπt
i, πt
−i) −bV t
1,i(πt)"
K,0.57992277992278,"≤V1,i(bπt
i, πt
−i) −V1,i(πt) + ε/4."
K,0.5806949806949807,Published as a conference paper at ICLR 2022
K,0.5814671814671815,"If the while loop doesn’t end after the t-th iteration and t ≤4mH/ε, there exists jt s.t. ∆t
jt ≥ε/2,
so we have"
K,0.5822393822393822,"Φ(πt+1) −Φ(πt) = Φ(bπt
jt, πt
−jt) −Φ(πt)"
K,0.583011583011583,"(i)
= V1,jt(bπt
jt, πt
−jt) −V1,jt(πt)"
K,0.5837837837837838,"≥∆t
jt −ε/4 = ε/4."
K,0.5845559845559846,"Here, (i) follows the deﬁnition of potential function. Because Φ is bounded, the while loop ends
within 4Φmax/ε ≤4mH/ε steps. Therefore, (16) holds simultaneously for all i ∈[m] and t before
the end of while loop with probability at least 1 −p. Again, on this event, if the while loop stops at
the end of t-th step, we have maxi∈[m] ∆t
i ≤ε/2, then"
K,0.5853281853281853,"max
µi V1,i(µi, πt
−i) −V1,i(πt) = max
µi V1,i(µi, πt
−i) −V1,i(bπt
i, πt
−i) + V1,i(bπt
i, πt
−i) −V1,i(πt)"
K,0.5861003861003861,"≤ε/4 + bV t
1,i(bπt
i, πt
−i) −bV t
1,i(πt) + 2ε/8"
K,0.5868725868725869,"= ε/2 + ∆t
i
≤ε."
K,0.5876447876447877,"So the returned policy πt is a ε-approximate Nash equilibrium. Moreover, since UCBVI-UPLOW
outputs a pure-strategy policy and our initial policy is also a pure-strategy policy, we can conclude
that with probability at least 1 −p, within 4Φmax/ε steps of the while loop, Algorithm 3 outputs an
ε-approximate (pure-strategy) Nash equilibrium."
K,0.5884169884169884,"Finally, the number of episodes within each step of the while loop is N + m
X"
K,0.5891891891891892,"i=1
(Ki + N) = O
H3S Pm
i=1 Aiι
ε2
+ H3S2 Pm
i=1 Aiι2"
K,0.5899613899613899,"ε
+ H2mι ε2 "
K,0.5907335907335908,"= O
H3S Pm
i=1 Aiι
ε2
+ H3S2 Pm
i=1 Aiι2 ε 
."
K,0.5915057915057915,So the total sample complexity (episodes) is at most
K,0.5922779922779923,"K = O
ΦmaxH3S Pm
i=1 Aiι
ε3
+ ΦmaxH3S2 Pm
i=1 Aiι2 ε2 
."
K,0.593050193050193,This concludes the proof.
K,0.5938223938223938,"E
LOWER BOUND OF FINDING APPROXIMATE PURE-STRATEGY NASH
EQUILIBRIUM"
K,0.5945945945945946,"In this section, we present an result on the sample complexity lower bound for learning a pure-strategy
Nash equilibrium in MPGs (a harder task than learning Nash as pure-strategy Nash is a stricter notion).
We remark that our lower bounds are actually constructed on Markov Cooperative Games (MCGs)
which is a subset of MPGs. Note that for MCGs, the potential function Φ is bounded in [0, H], so by
Theorem 7, the sample complexity of Nash-CA (Algorithm 3) is e
O(Pm
i=1 Ai/ε3) highlighting the
dependency on ε, m and Ai, i = 1, . . . , m. We would show this Pm
i=1 Ai dependency is inevitable
for learning ε-approximate pure-strategy Nash equilibrium in MCGs by proving an Ω(Pm
i=1 Ai/ε2)
lower bound."
K,0.5953667953667954,We ﬁrst present our main theorem:
K,0.5961389961389961,"Theorem E.1 (Lower bound for learning pure-strategy Nash in MCGs). Suppose Ai = 2k, i =
1, . . . , m , H ≥2, S ≥3 and m ≥4. Then, there exists an absolute constant c0 such that for any
ε ≤0.4 and any online ﬁnetuning algorithm M that outputs a pure-strategy policy bπ = (bπ1, . . . , bπm),
if the number of episodes"
K,0.5969111969111969,"K ≤c0
H2 Pm
i=1 Ai
ε2
= c0
2kmH2 ε2
,"
K,0.5976833976833977,Published as a conference paper at ICLR 2022
K,0.5984555984555985,"then there exists general-sum Markov cooperative game MG on which the algorithm M suffers from
ε/4-suboptimality, i.e.
EMNE-gap(bπ) ≥ε/4,
where the expectation EM is w.r.t. the randomness during the algorithm’s execution within Markov
game MG."
K,0.5992277992277992,"This theorem can be viewed as a corollary of the following lemma by a simple reduction. We would
prove this theorem in the next subsection. One-step (general-sum) game is a game with only one state
and one step. In a one-step game, each player chooses an action simultaneously and then receive it’s
own reward. The Nash equilibrium and NE-gap can be deﬁned similarly in one-step games.
Lemma E.2 (Lower bound for one-step game). Suppose Ai = 2k, i = 1, . . . , m and m ≥4. Then,
there exists an absolute constant c0 such that for any ε ≤0.4 and any online ﬁnetuning algorithm
that outputs a pure strategy bπ = (bπ1, . . . , bπm), if the number of samples n ≤c0"
K,0.6,"Pm
i=1 Ai"
K,0.6007722007722007,"ε2
= c0
2km ε2 ,"
K,0.6015444015444016,"then there exists a one-step game M with stochastic reward, on which the algorithm suffers from
ε/4-suboptimality, i.e."
K,0.6023166023166023,"EMNE-Gap(bπ) = EM max
i"
K,0.6030888030888031,"
max
πi ui(πi, bπ−i) −ui(bπ)

≥ε/4,"
K,0.6038610038610038,"where the expectation EM is w.r.t. the randomness during the algorithm’s execution within game M.
ui(π) is the expected reward of the ith player when strategy π are taken for each player."
K,0.6046332046332046,"The proof of this lemma is also in the next subsection. In the proof, we ﬁrst construct a class of
one-step games which reward is Bernoulli( 1"
K,0.6054054054054054,2) or Bernoulli( 1
K,0.6061776061776062,"2 +ε) depending on the taken joint-action.
The proportion of joint-actions with reward Bernoulli( 1"
K,0.6069498069498069,"2 + ε) is relatively small. Most importantly,
every pure-strategy ε-approximate Nash equilibrium has reward Bernoulli( 1"
K,0.6077220077220077,"2 + ε). So in order to
ﬁnd an ε-approximate pure-strategy Nash equilibrium, we must explore sufﬁcient joint-actions. The
number of the joint-actions with reward Bernoulli( 1"
K,0.6084942084942085,"2 + ε) can be bounded by the covering number of
[2k]m under Hamming distance. Then we use KL divergence decomposition (Lemma E.5) to argue
rigorously that we need to explore sufﬁcient joint-actions to get an ε-approximate pure-strategy Nash
equilibrium."
K,0.6092664092664093,"The rest of this section is organized as follows: We ﬁrst prove Lemma E.2 in Section E.1, and then
prove the main Theorem E.1 in Section E.2."
K,0.61003861003861,"Discussions of Theorem E.1
There’s Pm
i=1 Ai dependency4 in the lower bound of sample com-
plexity for ﬁnding a pure-strategy ε-approximate Nash equilibrium in MCGs. This bound is novel
and improves the existing result. The existing proof in sample complexity’s lower bound of Markov
games (Bai & Jin (2020)) relies on an reduction from Markov games to single-agent MDPs, so the
existing lower bound’s dependency on Ai (i = 1, . . . , m) is maxi∈[m] Ai ."
K,0.6108108108108108,"Here, we don’t include S factor in our lower bound. The difﬁculty is that the NE-gap only depends on
the player with the most suboptimality. For a single-agent MDP, if the player can change the policy
at each state to improve the expected cumulative reward by ε, then the player can change policy at all
state to improve the expected cumulative reward to the utmost extent. In general-sum Markov games,
at different state, maybe different players can change the policy for this state to improve his expected
cumulative reward by ε. However, the deﬁnition NE-gap only allows one player to change the policy.
This difference in nature makes S and Pm
i=1 Ai incompatible in the lower bound."
K,0.6115830115830115,"If we consider another notion of suboptimality, i.e., changing maximum to summation:"
K,0.6123552123552124,"NE-gap′(π) :=
X i∈[m]"
K,0.6131274131274131,"
sup
µi
V µi,π−i
1,i
(s1) −V π
1,i(s1)

."
K,0.6138996138996139,"This deﬁnition of NE-gap is different from the previous deﬁnition. With NE-gap′, if each player
can change his policy to improve his expected cumulative reward by ε, the NE-gap′ would be at"
K,0.6146718146718146,4We only prove this lower bound when Ai all equal to 2k. This case is representative.
K,0.6154440154440154,Published as a conference paper at ICLR 2022
K,0.6162162162162163,"least mε. Then we can similarly deﬁne ε-approximate Nash equilibrium as the policy π such that
NE-gap′(π) ≤ε. We simply point out that with this new deﬁnition of NE-gap′ and ε-approximate
Nash equilibrium, mimicking the proof of Theorem 2 in Dann & Brunskill (2015), we can prove the
sample complexity’s lower bound for learning a pure-strategy ε-approximate Nash equilibrium in
Markov (cooperative) games is Ω(H2S Pm
i=1 Ai/ε2)."
K,0.616988416988417,"E.1
PROOF OF LEMMA E.2"
K,0.6177606177606177,"For convenience, we call the joint-action (in one-step game) that is a Nash equilibrium a Nash strategy.
We begin with a special case of Lemma E.2, i.e. the case when Ai = 2 for all i ∈[m].
Lemma E.3. Suppose Ai = 2, i = 1, . . . , m and m ≥4. Then, there exists an absolute constant c0
such that for any ε ≤0.4 and any algorithm that outputs a pure strategy bπ = (bπ1, . . . , bπm), if the
number of samples"
K,0.6185328185328185,"n ≤c0
2m ε2 ,"
K,0.6193050193050194,"then there exists a one step game M with stochastic reward on which the algorithm suffers from
ε/4-suboptimality, i.e.,"
K,0.6200772200772201,"EMNE-gap(bπ) = EM max
i"
K,0.6208494208494209,"
max
πi ui(πi, bπ−i) −ui(bπ)

≥ε/4,"
K,0.6216216216216216,"where the expectation EM is w.r.t. the randomness during the algorithm’s execution within game M.
ui(π) is the expected reward of the ith player when strategy π are taken for each player."
K,0.6223938223938223,"The proof of this lemma further relies on the following lemma.
Lemma E.4. There exists a one-step game for m players where each player has two actions. The
deterministic reward is 0 or 1 and the number of joint actions that have 1 is at most 2m+1"
K,0.6231660231660232,"m . Moreover,
the only pure-strategy Nash equilibria are these joint actions which have reward 1."
K,0.623938223938224,"Proof of Lemma E.4
We use r(a) to denote the reward of (joint) actions a ∈{1, 2}m and deﬁne
hamming distance d(a, a′) = #{i : ai ̸= a′
i}. To ensure that pure-strategy Nash equilibria must
have reward 1, we only need to ensure that for a a ∈{1, 2}m, there exists one a′ ∈{1, 2}m such that"
K,0.6247104247104247,"r(a′) = 1,
d(a, a′) ≤1."
K,0.6254826254826255,"In other words, the set {a : r(a) = 1} is a 1-net of {1, 2}m under the distance d(·, ·). By the
deﬁnition of covering number, we only need to prove"
K,0.6262548262548262,"N({1, 2}m, d, 1) ≤2m+1 m
."
K,0.6270270270270271,"Deﬁne K(m, 1) = N({1, 2}m, d, 1). By hamming code (Hamming (1950)), we know that for any
integer k ≥1,
K(2k −1, 1) = 22k−k−1.
Moreover, we also have K(n, 1) ≤2K(n −1, 1) by adding 0 and 1 behind the 1-net of {0, 1}n−1.
Taking largest k such that 2k −1 ≤n and iterating this construction on the Hamming code we get
K(m, 1) ≤2m−[log2(m+1)] ≤2m+1"
K,0.6277992277992278,m . This ends the proof.
K,0.6285714285714286,"The next lemma is KL divergence decomposition (Lemma 15.1 of (Lattimore & Szepesvári, 2020)]),
we restate it in one-step games.
Lemma E.5 (KL divergence decomposition in one-step games). For any one-step games with
stochastic reward and any algorithm. Let X = (a1, r1, a2, r2, . . . , an, rn), where ak is the action
(adaptively) chosen by the algorithm at the kth round and rk is the reward received at the kth round
after ak is taken. P and Q are two probability measure for the stochastic reward. Let N(a) be the
total number of actions a in the ﬁrst n rounds. Then"
K,0.6293436293436293,"KL(X|P∥X|Q) =
X"
K,0.6301158301158302,"a
EP[N(a)]KL(P(·|a)∥Q(·|a))."
K,0.6308880308880309,Published as a conference paper at ICLR 2022
K,0.6316602316602317,"Then we return to the proof of Lemma E.3. Suppose a game M satisﬁes the condition in Lemma
E.4, by permuting the actions of M, we get 2m games. They all satisfy the condition in Lemma E.4.
Suppose the reward of the i-th game is r(i)(·) (i = 1, · · · , 2m) ."
K,0.6324324324324324,"We consider the following family of one-step games with stochastic reward:
Let a
=
(a1, a2, . . . , am)."
K,0.6332046332046332,"M(ε) = {M(i) ∈R{1,2}m with M(i)(a) = 1"
K,0.633976833976834,"2 + r(i)(a)ε, i = 1, 2, . . . , 2m},"
K,0.6347490347490348,"where in one-step game M(i), the reward is sampled from Bernoulli(M(i)(a)) if the joint action
is a = (a1, a2, . . . , am). Moreover, we deﬁne M(0) as a game whose reward is sampled from
Bernoulli( 1"
K,0.6355212355212355,2) independent of the action.
K,0.6362934362934363,"We further let ν denote the uniform distribution on {1, 2, . . . , 2m}."
K,0.637065637065637,"Proof of Lemma E.3
Fix a one-step game M(i) ∈M(ε), by Lemma E.4, it’s clear that pure-
strategy Nash equilibria form a set D(i) := a ∈{a : r(i)(a) = 1}. We have #D(i) ≤2m+1/m.
For any online ﬁnetuning algorithm A that outputs a pure strategy bπ. Suppose bπ takes joint-action
ba = (ba1, ba2, · · · , bam). From the structure of the M(ε), we know"
K,0.6378378378378379,"NE-Gap(bπ) = εPi(ba ̸∈D(i)),"
K,0.6386100386100386,"where Pi is w.r.t. the randomness of the algorithm and game M(i). So we only need to use n, the
number of samples to give a lower bound of Pi(ba ̸∈D(i))."
K,0.6393822393822394,"Since X = {a1, r1, · · · , an, rn} is a sufﬁcient statistics for the posterior distribution D(i), we have"
K,0.6401544401544401,"Ei∼νPi(ba ̸∈D(i)) ≥inf
g Ei∼νPi(g(X) ̸∈D(i))"
K,0.640926640926641,"(i)
≥inf
g Ei∼νP0(g(X) ̸∈D(i)) −Ei∼νTV(X|P0, X|Pi)"
K,0.6416988416988417,"(ii)
≥1"
K,0.6424710424710425,"2 −Ei∼νTV(X|P0, X|Pi)"
K,0.6432432432432432,"(iii)
≥1"
K,0.644015444015444,2 −Ei∼ν r
K,0.6447876447876448,"1
2KL(X|P0∥X|Pi)"
K,0.6455598455598456,"(iv)
= 1"
K,0.6463320463320463,"2 −Ei∼ν s 1
2 X"
K,0.6471042471042471,"a
EP0[N(a)]KL(P0(·|a)∥Pi(·|a))."
K,0.6478764478764478,"Above, (i) follows from the deﬁnition of total variation distance; (ii) uses the fact that under P0, we
can’t get any information about a⋆, so Ei∼νP0(g(X) ̸∈D(i)) = 1 −|D(1)|"
M,0.6486486486486487,"2m
≥1"
M,0.6494208494208494,"2; (iii) uses Pinsker’s
inequality; (iv) uses KL divergence decomposition (Lemma E.5)."
M,0.6501930501930502,"For a ∈D(i), we have"
M,0.6509652509652509,"KL(P0(·|a)∥Pi(·|a)) = KL
1 2, 1 2"
M,0.6517374517374518,"
∥
1"
M,0.6525096525096525,"2 −ε, 1"
M,0.6532818532818533,"2 + ε
 = 1"
LOG,0.654054054054054,"2 log
1
1 −4ε2 ≤4ε2,"
LOG,0.6548262548262548,"if ε ∈(0, 0.4]. For a ̸∈D(i), KL(P0(·|a)∥Pi(·|a)) = 0. So"
LOG,0.6555984555984556,Ei∼νPi(ba ̸∈D(i)) ≥1
LOG,0.6563706563706564,2 −Ei∼ν s
X,0.6571428571428571,"2
X"
X,0.6579150579150579,"a∈D(i)
EP0[N(a)]ε2"
X,0.6586872586872586,"(i)
≥1 2 −
s"
X,0.6594594594594595,"2Ei∼ν
X"
X,0.6602316602316602,"a∈D(i)
EP0[N(a)]ε2"
X,0.661003861003861,"(ii)
= 1 2 − r"
X,0.6617760617760617,2n|D(i)|
M,0.6625482625482626,"2m
ε2"
M,0.6633204633204633,"Published as a conference paper at ICLR 2022 ≥1 2 −
r"
N,0.6640926640926641,4 n mε2.
N,0.6648648648648648,"Above, (i) uses Jensen’s inequality, (ii) uses the fact that EP0[N(a)] is independent of i which gives"
N,0.6656370656370656,"Ei∼ν
X"
N,0.6664092664092665,"a∈D(i)
EP0[N(a)] = 1"
"M
X",0.6671814671814672,"2m
X"
"M
X",0.667953667953668,"a
EP0[N(a)]
X"
"M
X",0.6687258687258687,"i:a∈D(i)
1 = 1"
"M
X",0.6694980694980694,"2m n|D(1)|,"
"M
X",0.6702702702702703,where P
"M
X",0.6710424710424711,"i:a∈D(i) 1 = |D(1)| is because of the permutation. Finally, we choose c0 =
1
32, if n ≤
1
32
2m"
"M
X",0.6718146718146718,"ε2 ,
then
Ei∼νPi(ba ̸∈D(i)) ≥1 4."
"M
X",0.6725868725868726,So there’s a game instance M(i) on which the algorithm suffer from ε/4 sub-optimality.
"M
X",0.6733590733590734,"The difference between Lemma E.3 and E.2 is the size of action space. To prove the Ai = 2k case,
we need to generalized E.4 to the case each player have 2k actions."
"M
X",0.6741312741312742,"Lemma E.6. For all positive integers m and k. There exists a one-step game for m players where
each player has 2k actions. The deterministic reward is 0 or 1 and the number of joint actions that
have 1 is at most 2·(2k)m"
"M
X",0.6749034749034749,"km
. Moreover, the only pure-strategy Nash equilibria are these joint actions
which has reward 1."
"M
X",0.6756756756756757,"Proof of Lemma E.6
We would prove this lemma by induction. Without loss of generality, we
suppose the action for each player is 1, 2, . . . , 2k. First, we deﬁne the hamming distance between
two vectors.
d(x, y) := #{i : x(i) ̸= y(i)}."
"M
X",0.6764478764478764,"The joint action space is [2k]m = {1, 2, . . . , 2k}m. Suppose we have an 1-net of [2k]m, D, which
means for every y ∈[2k]m, we can ﬁnd a x ∈D, s.t. d(x, y) ≤1. If the reward of a game satisfy:"
"M
X",0.6772200772200773,"r(a) = 1 for all a ∈D,
and
r(a) = 0 for all a ̸∈D."
"M
X",0.677992277992278,"Then for every pure strategy a which has reward 0, we can ﬁnd another pure strategy a′ s.t. r(a′) = 1
and d(a, a′) = 1. This means that one player can change to obtain higher reward. So a is not a
pure-strategy Nash equilibrium."
"M
X",0.6787644787644788,"As a result, we can construct a game based on the 1-net. The only thing left to be veriﬁed is that the
number of joint actions that have reward 1 is at most 2·(2k)m"
"M
X",0.6795366795366795,"km
. Note that the number of joint actions
that have reward 1 is actually |D|, so we need to prove"
"M
X",0.6803088803088803,"N([2k]m, d, 1) ≤2 · (2k)m"
"M
X",0.6810810810810811,"km
,
(17)"
"M
X",0.6818532818532819,"where N(·, d, ε) denotes the covering number."
"M
X",0.6826254826254826,"For k = 1, from the proof of Lemma E.4, (17) is true. For k > 1, we ﬁrst decompose [2k]m into km
smaller blocks, i.e."
"M
X",0.6833976833976834,"Ω(l1, l2, . . . , lm) = {2l1 −1, 2l1} × {2l2 −1, 2l2} × · · · , ×{2lm −1, 2lm},"
"M
X",0.6841698841698842,"where {l1, l2, . . . , lm} ∈[k]m. Among these km blocks, we choose the blocks which satisfy
k|l1 +l2 +· · ·+lm. Apparently, there are km−1 blocks satisfying m|l1 +l2 +· · ·+lm. Because (17)
is true for k = 1, we can pick a 1-net of Ω(1, 1, . . . , 1) with at most 2·2m"
"M
X",0.684942084942085,"m
elements. By a translation,
(all elements add a constant vector), we can pick a 1-net of each block with at most 2·2m"
"M
X",0.6857142857142857,"m
elements.
Totally there are at most 2·2mkm−1"
"M
X",0.6864864864864865,"m
= 2·(2k)m"
"M
X",0.6872586872586872,"km
elements. These elements form a set P. We would
prove P is a 1-net of [2k]m."
"M
X",0.6880308880308881,"In fact, for any x ∈[2k]m, suppose x is in Ω(l1, l2, . . . , lm). After translating this block to
Ω(1, 1, . . . , 1), x is coincided with x0."
"M
X",0.6888030888030888,Published as a conference paper at ICLR 2022
"M
X",0.6895752895752896,Figure 1: A class of hard-to-learn MDPs where s+ and s−are absorbing state.
"M
X",0.6903474903474903,"If x0 ∈P, change l1 to l′
1 such that k|l′
1 + l2 + · · · + lm. Suppose x′ is the vector in block
Ω(l′
1, l2, . . . , lm) that corresponds to x0 in Ω(1, 1, . . . , 1). This gives x′ ∈P. Moreover, d(x, x′) ≤
1 because the translation from Ω(l1, l2, . . . , lm) to Ω(l′
1, l2, . . . , lm) only changes the ﬁrst component."
"M
X",0.6911196911196911,"If x0 ̸∈P, we can ﬁnd x′
0 in P ∩Ω(1, 1, . . . , 1) such that d(x0, x′
0) = 1 because P|Ω(1,1,...,1)
is a 1-net. Suppose x0 and x′
0 differs at the j-th component. We can change lj to l′
j s.t. k|l1 +
l2 + · · · + lj−1 + l′
j + lj+1 + · · · + lm. Suppose x′
0 corresponds to x′ in Ω(l1, . . . , lm) and x′′ in
Ω(l1, . . . , lj−1, l′
j, lj+1, . . . , lm). By the deﬁnition of P, we know x′′ ∈P. Moreover, x and x′ only
differ at the j-th component because of translation and the fact that x0 and x′
0 only differ at the j-th
component. x′ and x′′ also only differ at the j-th component because of translation. So we have x
and x′′ may only differ at the j-th component, i.e. d(x, x′′) ≤1."
"M
X",0.6918918918918919,"Recall that x′′ ∈P. To conclude, we can always ﬁnd another vector y ∈P such that d(x, y) ≤1,
this means P is a 1-net of [2k]m. So (17) is proved."
"M
X",0.6926640926640927,"Using this fundamental lemma and suppose a game M satisﬁes the condition in Lemma E.6, by
permuting the actions of M, we get (2k)m games. They all satisfy the condition in Lemma E.6.
Suppose the reward of the i-th game is r(i)(·) (i = 1, · · · , (2k)m) ."
"M
X",0.6934362934362934,"We consider the following family of one-step games with stochastic reward:
Let a
=
(a1, a2, . . . , am)."
"M
X",0.6942084942084942,M(ε) = {M(i) ∈R[2k]m with M(i)(a) = 1
"M
X",0.694980694980695,"2 + r(i)(a)ε, i = 1, 2, . . . , (2k)m},
(18)"
"M
X",0.6957528957528958,"where in one-step game M(i), the reward is sampled from Bernoulli(M(i)(a)) if the joint action
is a = (a1, a2, . . . , am). Moreover, we deﬁne M(0) as a game whose reward is sampled from
Bernoulli( 1"
"M
X",0.6965250965250965,2) independent of the action.
"M
X",0.6972972972972973,"Then by the same argument in proof of Lemma E.3, we can easily prove Lemma E.2."
"M
X",0.698069498069498,"E.2
PROOF OF THEOREM E.1"
"M
X",0.6988416988416989,We are now ready to prove Theorem E.1 based on Lemma E.2.
"M
X",0.6996138996138996,"Because S ≥3, we construct a class of general-sum Markov games as follows (see ﬁgure 1): the set
of all joint-actions A can be divided into two sets A+ an A−."
"M
X",0.7003861003861004,"Transition
Starting from s1, for a ∈A+, P1(s+|s1, a) =
1
2 +
ε
2(H−1) and P1(s−|s1, a) ="
"M
X",0.7011583011583011,"1
2 −
ε
2(H−1). For a ∈A−, P1(s+|s1, a) = P1(s−|s1, a) =
1
2. For all h ≥1 and a ∈A,
Ph(s+|s+, a) = 1 and Ph(s−|s−, a) = 1."
"M
X",0.7019305019305019,"Rewards
For any a and i ∈[m], r1,i(s, a) = 0 rh,i(s+, a) = 1 and rh,i(s−, a) = 0, where h > 1.
This also implies the Markov game is cooperative."
"M
X",0.7027027027027027,If we choose A+ as the joint action with reward Bernoulli( 1
"M
X",0.7034749034749035,"2 + ε) in M(i) deﬁned in (18). By the
construction, for any pure-strategy policy π, if the joint actions taken at step 1 is a ̸∈A+, we have"
"M
X",0.7042471042471042,Published as a conference paper at ICLR 2022
"M
X",0.705019305019305,Algorithm 8 Mixed-expert FTRL for weighted adversarial bandits
"M
X",0.7057915057915058,"Require: Hyper-parameters {ηt}1≤t≤T , {γt}1≤t≤T . Sequence

αi
t"
"M
X",0.7065637065637066,1≤i≤t≤T .
"M
X",0.7073359073359073,"1: Initialize: Accumulators tb′ ←0 for all b′ ∈[A].
2: for t = 1, 2, . . . , T do
3:
Compute weight ut ←αt
t/α1
t.
4:
Compute action distributions for all sub-experts b′ ∈[A] by FTRL:"
"M
X",0.7081081081081081,qb′(a) ∝a exp 
"M
X",0.7088803088803088,"−(ηtb′/ut) · tb′
X"
"M
X",0.7096525096525097,"τ=1
wτ(b′)bℓb′
τ (a) ! ."
"M
X",0.7104247104247104,"5:
Compute pt ∈∆[A] by solving the linear system pt(·) = PA
b′=1 pt(b′)qb′(·).
6:
Sample sub-expert b ∼pt.
7:
Sample action at ∼qb from sub-expert b.
8:
Play the action at, and observe the (bandit-feedback) loss ˜ℓt(at).
9:
Update accumulator for sub-expert b: tb ←tb + 1.
10:
Compute the loss estimate bℓb
tb(·) ∈RA
≥0 as"
"M
X",0.7111969111969112,"bℓb
tb(a) ←
˜ℓtb(a)1 {a = at}"
"M
X",0.711969111969112,"qb(a) + γtb
."
"M
X",0.7127413127413127,"11:
Set wtb(b) ←ut."
"M
X",0.7135135135135136,"NE-gap(π) ≥ε. The only useful information in each episode is the transition at step 1. Transition
from s1 to s+ can be viewed as getting a 1 reward and transition from s1 to s−can be viewed as
getting a 0 reward. So learning pure-strategy Nash equilibrium of this class of Markov games is
equivalent to learning pure-strategy Nash equilibrium of a game for m players with stochastic reward.
As a result, by Lemma E.2, if the number of episodes K ≤c0"
"M
X",0.7142857142857143,"Pm
i=1 Ai
ε2/(H −1)2 ≤4c0
H2 Pm
i=1 Ai
ε2
,"
"M
X",0.715057915057915,"then there exists general-sum Markov cooperative game MG on which the algorithm suffers from
ε/4-suboptimality, i.e.
EMNE-gap(bπ) ≥ε/4."
"M
X",0.7158301158301158,This ﬁnish the proof of Theorem E.1.
"M
X",0.7166023166023165,"F
ADVERSARIAL BANDIT WITH LOW WEIGHTED SWAP REGRET"
"M
X",0.7173745173745174,"In this section, we describe and analyze our main algorithm for adversarial bandits with low weighted
swap regret. Our Algorithm, Mixed-expert Follow-The-Regularized-Leader (FTRL) for weighted
adversarial bandits, is described in Algorithm 8."
"M
X",0.7181467181467182,"Problem setting
Throughout this section, we consider a standard (adversarial) bandit problem
with action space is [A] = {1, . . . , A} for some A ≥1. We assume that the realized loss values
˜ℓt ∈[0, 1]A, and let ℓt(a) := Et[˜ℓt(a)] denote the expected loss conditioned on (as usual) all
information before the sampling of the action at in Line 6 & 7. The hyperparameters in Algorithm 8
are chosen as"
"M
X",0.7189189189189189,"ηt = γt =
p"
"M
X",0.7196911196911197,"ι/(At),"
"M
X",0.7204633204633205,where the log factor
"M
X",0.7212355212355213,"ι := 4 log(8HAT/p).
(19)"
"M
X",0.722007722007722,Published as a conference paper at ICLR 2022
"M
X",0.7227799227799228,"F.1
MAIN RESULT FOR ADVERSARIAL BANDIT WITH LOW WEIGHTED SWAP REGRET"
"M
X",0.7235521235521235,"We ﬁrst deﬁne a strategy modiﬁcation. A strategy modiﬁcation is a function F : [A] →[A] which
can also be applied to any action distribution µ ∈∆A, such that F ⋄µ gives the swap distribution
which takes action F(a) with probability µ(a)."
"M
X",0.7243243243243244,"The swap regret (Blum & Mansour, 2007; Ito, 2020) measures the difference between the cumulative
realized loss for the algorithm and that for swapped action sequences generated by an arbitrary strategy
modiﬁcation F. Here, we consider a weighted version of the swap regret with some non-negative
weights

αi
t"
"M
X",0.7250965250965251,"1≤i≤t, deﬁned as"
"M
X",0.7258687258687259,"Rswap(t) :=
max
F :[A]→[A] t
X"
"M
X",0.7266409266409266,"i=1
αi
t[ℓi(ai) −ℓi(F(ai))].
(20)"
"M
X",0.7274131274131274,"We will also consider a slightly modiﬁed version of the swap regret used in our analyses for learning
CE, deﬁned as"
"M
X",0.7281853281853282,"eRswap(t) :=
max
F :[A]→[A] t
X"
"M
X",0.728957528957529,"i=1
αi
t[ℓi(ai) −

F ⋄pi, ℓi

],
(21)"
"M
X",0.7297297297297297,where pt is t-th action distribution (from which the action at is sampled from) played by the algorithm.
"M
X",0.7305019305019305,"We now state our main result of this section.
Lemma F.1 (Bound on weighted swap regret). If we execute Algorithm 8 for T rounds and the
weights

αi
t
	
are chosen according to (2), then with probability at least 1 −p/2, we have the
following bounds on the swap regret simultaneously for all t ∈[T]:"
"M
X",0.7312741312741313,"Rswap(t) =
max
F :[A]→[A] t
X"
"M
X",0.7320463320463321,"i=1
αi
t[ℓi(ai) −ℓi(F(ai))] ≤C(HA
p"
"M
X",0.7328185328185328,"ι/t + HAι/t),"
"M
X",0.7335907335907336,"eRswap(t) =
max
F :[A]→[A] t
X"
"M
X",0.7343629343629343,"i=1
αi
t[ℓi(ai) −

F ⋄pi, ℓi

] ≤C(HA
p"
"M
X",0.7351351351351352,"ι/t + HAι/t),"
"M
X",0.7359073359073359,where C > 0 is some absolute constant.
"M
X",0.7366795366795367,"The rest of this section is devoted to proving Lemma F.1, organized as follows. We ﬁrst present some
important properties of Algorithm 8 in Section F.2. We then prove Lemma F.1 in Section F.3 using
new auxiliary results on weighted adversarial bandits with predictable weights. Lastly, these auxiliary
results are stated and proved in Appendix F.4."
"M
X",0.7374517374517374,"F.2
PROPERTIES OF ALGORITHM 8"
"M
X",0.7382239382239382,"Solution of the linear system
In Line 5, we need to compute pt ∈∆[A] by solving the linear
system pt(·) = PA
b′=1 pt(b′)qb′(·). Such pt(·) can be interpreted as the stationary distribution of a
Markov chain over [A] with transition probability P(b|a) = qa(b). This guarantees the existence of
pt ∈∆[A] such that pt(·) = PA
b′=1 pt(b′)qb′(·). Moreover, pt(·) can be computed efﬁciently (see
e.g. Cohen et al. (2017b))."
"M
X",0.738996138996139,"FTRL update for each sub-expert
Here we show that, the updates for any particular sub-expert
b ∈[A] in Algorithm 8 is exactly equivalent to an FTRL update (with loss sequence being the ones
only over the episodes in which b is sampled) which we summarize in Algorithm 9. This is important
as our proof relies on reducing the weighted swap regret to a linear combination of weighted regrets
for each sub-expert b ∈[A]."
"M
X",0.7397683397683398,"To see this, ﬁx any b ∈[A]. When b is sampled in Line 6 at episode t, we have accumulator tb and
weight wtb(b) = ut = αt
t/α1
t at the end of episode t. Action at is chosen from distribution"
"M
X",0.7405405405405405,qb(a) ∝a exp 
"M
X",0.7413127413127413,−(ηtb/wtb) ·
"M
X",0.7420849420849421,"tb−1
X"
"M
X",0.7428571428571429,"τ=1
wτ(b′)bℓb
τ(a) !"
"M
X",0.7436293436293436,Published as a conference paper at ICLR 2022
"M
X",0.7444015444015444,"after wtb(b) = ut = αt
t/α1
t is observed. The loss estimate bℓtb is"
"M
X",0.7451737451737451,"bℓb
tb(a) ←
˜ℓtb(a)1 {a = at}"
"M
X",0.745945945945946,"qb(a) + γtb
."
"M
X",0.7467181467181467,"So from the sub-expert b’s perspective, she is performing FTRL (follow the regularized leader) with
changing step size and random weight (We summarize this in Algorithm 9). Suppose the sub-expert b
is chosen at episode t = k1, k2, . . . , ktb, then the weighted regret for sub-expert b becomes"
"M
X",0.7474903474903475,"Rt(b) :=
sup
θ∗∈∆A"
"M
X",0.7482625482625482,""" tb
X"
"M
X",0.749034749034749,"τ=1
wτ(b)(ℓkτ (akτ ) −⟨θ∗, ℓkτ ⟩) #"
"M
X",0.7498069498069498,".
(22)"
"M
X",0.7505791505791506,"Bounded weight wi(b)
Recall

αi
t
	
is chosen as in (2). We have the following bound:"
"M
X",0.7513513513513513,"αt
t
α1
t
≤1"
"M
X",0.7521235521235521,"α1
t
=
1
α1(1 −α2) · · · (1 −αt) = (H + 2)(H + 3) · · · (H + t)"
"M
X",0.752895752895753,"1 · 2 · · · (t −1)
≤(H + t)H+1."
"M
X",0.7536679536679537,"So for any t ≤T, we have
αt
t/α1
t ≤(H + T)2H ≡W.
(23)
The weight wnb(b) = αt
t/α1
t have a (non-random) upper bound W."
"M
X",0.7544401544401544,"F.3
PROOF OF LEMMA F.1"
"M
X",0.7552123552123552,"We use bi to denote the sampled sub-expert b at the i-th episode. Deﬁne Gi as the σ-algebra generated
by all the random variables observed up to the end of the i-th episode."
"M
X",0.7559845559845559,First observe that for Rswap(t) we have the bound
"M
X",0.7567567567567568,"Rswap(t) =
max
F :[A]→[A] t
X"
"M
X",0.7575289575289575,"i=1
αi
t[ℓi(ai) −ℓi(F(ai))]"
"M
X",0.7583011583011583,"≤
max
F :[A]→[A] t
X"
"M
X",0.759073359073359,"i=1
αi
t[ℓi(ai) −ℓi(F(bi))]"
"M
X",0.7598455598455598,"|
{z
}
I"
"M
X",0.7606177606177607,"+
max
F :[A]→[A] t
X"
"M
X",0.7613899613899614,"i=1
αi
t[ℓi(F(bi)) −ℓi(F(ai))]"
"M
X",0.7621621621621621,"|
{z
}
II ,"
"M
X",0.7629343629343629,"also, for eRswap(t) we have the bound"
"M
X",0.7637065637065638,"eRswap(t) =
max
F :[A]→[A] t
X"
"M
X",0.7644787644787645,"i=1
αi
t[ℓi(ai) −

F ⋄pi, ℓi

]"
"M
X",0.7652509652509653,"≤
max
F :[A]→[A] t
X"
"M
X",0.766023166023166,"i=1
αi
t[ℓi(ai) −ℓi(F(bi))]"
"M
X",0.7667953667953668,"|
{z
}
I"
"M
X",0.7675675675675676,"+
max
F :[A]→[A] t
X"
"M
X",0.7683397683397684,"i=1
αi
t[ℓi(F(bi)) −

F ⋄pi, ℓi

]"
"M
X",0.7691119691119691,"|
{z
}
eII ,"
"M
X",0.7698841698841699,"Term II and eII can be bounded by concentration in a similar fashion; here we ﬁrst focus on term II.
Observe that at the i-th episode, as pi obtained in Line 5 solves the equation"
"M
X",0.7706563706563706,"pi(a) = A
X"
"M
X",0.7714285714285715,"b′=1
pi(b′)qb′(a),"
"M
X",0.7722007722007722,"we have bi ∼pi(·) and ai ∼qbi(·) has the same marginal distribution conditioned on Gi−1, where
Gi−1 denote the σ-algebra generated by ℓi and all the random variables in the ﬁrst i−1 episodes. There-
fore, ﬁxing any strategy modiﬁcation F : [A] →[A], we have

αi
t[ℓi(F(bi)) −ℓi(F(ai))]"
"M
X",0.772972972972973,"1≤i≤t is
a bounded martingale difference sequence w.r.t. the ﬁltration {Gi}, so by Azuma-Hoeffding inequality
and Pt
i=1(αj
t)2 ≤2H/t, P t
X"
"M
X",0.7737451737451737,"i=1
αi
t[ℓi(F(bi)) −ℓi(F(ai))] ≥2 r"
"M
X",0.7745173745173746,H log 1/p t ! ≤p.
"M
X",0.7752895752895753,Published as a conference paper at ICLR 2022
"M
X",0.7760617760617761,"As there is at most AA strategy modiﬁcations, we can substitute p with p/(4AAT), and take a union
bound to get"
"M
X",0.7768339768339768,"II =
max
F :[A]→[A] t
X"
"M
X",0.7776061776061776,"i=1
αi
t[ℓi(F(bi)) −ℓi(F(ai))] ≤2
p"
"M
X",0.7783783783783784,"HA log(AT/p)/t ≤C
p"
"M
X",0.7791505791505792,"HAι/t
(24)"
"M
X",0.7799227799227799,"simultaneously for all t ∈[T] with probability at least 1 −p/4. We also note that, by a similar
argument (as bi is also distributed according to pi conditioned on the past), we have that"
"M
X",0.7806949806949807,"eII =
max
F :[A]→[A] t
X"
"M
X",0.7814671814671814,"i=1
αi
t[ℓi(F(bi)) −

F ⋄pi, ℓi

] ≤C
p"
"M
X",0.7822393822393823,"HAι/t.
(25)"
"M
X",0.783011583011583,"Therefore for bounding both Rswap(t) and eRswap(t), it sufﬁces to bound term I."
"M
X",0.7837837837837838,"We next bound term I. Deﬁne Ub :=

i ∈[t] : bi = b
	
and let nt
b be the value of tb at the end of
the t-th episode, i.e. nt
b = #

i : bi = b
	
. We also suppose the sub-expert b was chosen at episode
t = kb
1, kb
2, . . . , kb
nt
b up to episode t. Then we have t
X"
"M
X",0.7845559845559845,"i=1
αi
t[ℓi(ai) −ℓi(F(bi))] =
X b∈A X"
"M
X",0.7853281853281854,"i∈Ub
αi
t[ℓi(ai) −ℓi(F(b))] =
X b∈A"
"M
X",0.7861003861003861,"nt
b
X"
"M
X",0.7868725868725869,"τ=1
α1
t
αkb
τ
t
α1
t
[ℓkbτ (akb
τ ) −ℓkbτ (F(b))] =
X b∈A"
"M
X",0.7876447876447876,"nt
b
X"
"M
X",0.7884169884169884,"τ=1
α1
twτ(b)[ℓkbτ (akb
τ ) −ℓkbτ (F(b))]."
"M
X",0.7891891891891892,"Here the last equation is because our choice of wτ(b) = αkb
τ
kbτ /α1
kbτ = αkb
τ
t /α1
t which is a simple
corollary from the deﬁnition of αi
t in Eq. (3)."
"M
X",0.78996138996139,Then because
"M
X",0.7907335907335907,"nt
b
X"
"M
X",0.7915057915057915,"τ=1
wτ(b)[ℓkbτ (akb
τ ) −ℓkbτ (F(b))] ≤max
θ∗"
"M
X",0.7922779922779922,"nt
b
X"
"M
X",0.7930501930501931,"τ=1
wτ(b)[ℓkbτ (akb
τ ) −

ℓkbτ , θ∗
] = Rt(b),"
"M
X",0.7938223938223938,"where Rt(b) deﬁned in (22) is the weighted regret Rt(b) for sub-expert b , we can use our result on
weighted adversarial bandits with predictable weights (Lemma F.2) to bound this term (The upper
bound W of the weight wτ(b) can be taken as (H + T)2H by the calculation of (23). Moreover,"
"M
X",0.7945945945945946,"wτ(b) = αkb
τ
t /α1
t and {αj
t}t
j=1 is increasing, so {wτ(b)}τ≥1 is non-decreasing.). Recall that our"
"M
X",0.7953667953667953,choice of log term is ι = 4 log 4HAT
"M
X",0.7961389961389962,"p
= log AT ⌈log2 W ⌉"
"M
X",0.7969111969111969,"p′
where p′ ≤p/(4A). Thus by Lemma F.2,
with probability at least 1 −p/(4A),"
"M
X",0.7976833976833977,"Rt(b) ≤15 max
τ≤nt
b
wτ(b)
q"
"M
X",0.7984555984555984,"Ant
bι + ι

for all t ∈[T]."
"M
X",0.7992277992277992,"Taking a union bound, we get"
"M
X",0.8,"Rt(b) ≤15 max
τ≤nt
b
wτ(b)
q"
"M
X",0.8007722007722008,"Ant
bι + ι

for all (t, b) ∈[T] × [A]
(26)"
"M
X",0.8015444015444015,with probability at least 1 −p/4.
"M
X",0.8023166023166023,"On this event, we have"
"M
X",0.803088803088803,"max
F :[A]→[A] t
X"
"M
X",0.8038610038610039,"i=1
αi
t[ℓi(ai) −ℓi(F(bi))] =
max
F :[A]→[A] X b∈A"
"M
X",0.8046332046332046,"nt
b
X"
"M
X",0.8054054054054054,"τ=1
α1
twτ(b)[ℓkbτ (akb
τ ) −ℓkbτ (F(b))]"
"M
X",0.8061776061776061,"Published as a conference paper at ICLR 2022 ≤
X"
"M
X",0.806949806949807,"b∈A
max
F :[A]→[A]"
"M
X",0.8077220077220078,"nt
b
X"
"M
X",0.8084942084942085,"τ=1
α1
twτ(b)[ℓkbτ (akb
τ ) −ℓkbτ (F(b))] ≤
X"
"M
X",0.8092664092664092,"b∈A
α1
tRt(b)"
"M
X",0.81003861003861,"(i)
≤
X"
"M
X",0.8108108108108109,"b∈A
α1
t max
τ≤nt
b
wτ(a)(15
q"
"M
X",0.8115830115830116,"Ant
bι + 15ι)"
"M
X",0.8123552123552124,"(ii)
=
X"
"M
X",0.8131274131274131,"b∈A
α1
t
α
kb
nt
b
t
α1
t
(15
q"
"M
X",0.8138996138996138,"Ant
bι + 15ι)"
"M
X",0.8146718146718147,"(iii)
≤
X a∈A"
H,0.8154440154440155,2H
H,0.8162162162162162,"t (15
q"
H,0.816988416988417,"Ant
bι + 15ι)."
H,0.8177606177606178,"Here, (i) uses (26), (ii) uses the αt′
t is increasing w.r.t. t′ and (iii) uses max0≤t′≤t αt′
t ≤2H/t.
Finally, because P"
H,0.8185328185328186,"b∈A nt
b = t, by the concavity of x 7→√x, we have with probability at least
1 −p/4,"
H,0.8193050193050193,"I =
max
F :[A]→[A] t
X"
H,0.8200772200772201,"i=1
αi
t[ℓi(ai) −ℓi(F(bi))] ≤30H t
· X"
H,0.8208494208494208,"b∈A
(
q"
H,0.8216216216216217,"Ant
bι + ι) ! ≤30H"
H,0.8223938223938224,"t
· (A
√"
H,0.8231660231660232,tι + Aι)
H,0.8239382239382239,"= 30HA
p"
H,0.8247104247104247,ι/t + 30HAι/t.
H,0.8254826254826255,"Combining this with (24) and (25), we ﬁnish the proof."
H,0.8262548262548263,"F.4
AUXILIARY LEMMAS FOR WEIGHTED ADVERSARIAL BANDIT WITH PREDICTABLE
WEIGHTS"
H,0.827027027027027,"In this subsection, we consider the Follow the Regularized Leader (FTRL) algorithm (Lattimore &
Szepesvári, 2020) with"
H,0.8277992277992278,"1. changing step size,"
H,0.8285714285714286,"2. weighted regret with Ft−1-measurable weights and loss distributions,"
H,0.8293436293436294,3. high probability regret bound.
H,0.8301158301158301,"We present these results because the predictable weights we would use are potentially unbounded
from above; if weights are predictable and also bounded, then there may be an easier analysis."
H,0.8308880308880309,"Interaction protocol
We ﬁrst describe the interaction protocol between the environment and the
player for this problem. At each episode t, the environment adversarially choose a weight wt which
takes values in R>0, and distributions (Lat)a∈[A], where each Lat is a distribution that takes values
in P([0, 1]) (the space of distributions supported on [0, 1]). Then the player receives the weight
wt, chooses an action at, and observes a loss ˜ℓt(at) ∼Lat,t. The action chosen at can be based
on all the information in Dt ≡{(ai, wi, ˜ℓi(ai))i≤t−1, wt}. Then, the environment can observe the
player’s action at and the incurred loss realization ˜ℓt(at), and use these information and some external
randomness zt to choose the weight and distributions (wt+1, (La,t+1)a∈[A]) of the next episode. We
will consider the following variant of FTRL algorithm for the player."
H,0.8316602316602316,"We denote ℓt ≡(E˜ℓ∼Lat[˜ℓ])a∈[A] ∈[0, 1]A which is a random vector but is σ((Lat)a∈[A])-
measurable. For some θ∗∈RA, the regret of the player up to episode t is deﬁned as Rt ≡ t
X"
H,0.8324324324324325,"i=1
wi(ℓi(ai) −⟨θ∗, ℓi⟩)."
H,0.8332046332046332,Published as a conference paper at ICLR 2022
H,0.833976833976834,Algorithm 9 FTRL for Weighted Regret with Changing Step Size and Predictable Weights
H,0.8347490347490347,"1: for episode t = 1, . . . , T do
2:
Observe wt > 0.
3:
θt(a) ∝a exp(−(ηt/wt) Pt−1
i=1 wibℓi(a)).
4:
Take action at ∼θt(·), and observe loss ˜ℓt(at).
5:
bℓt(a) ←˜ℓt(a)1 {at = a} /(θt(a) + γt) for all a."
H,0.8355212355212355,"Let (zt)t≥1 be a sequence of external random variables which are identically and independently
distributed as Unif([0, 1]) and are independent of all other random variables. We deﬁne Ft =
σ({ai, ˜ℓi, zi}i≤t) to be the sigma algebra generated by the random chosen action ai, the random loss
˜ℓi(ai), and the external random variables zi by episode t. We assume that the random weights and
the random loss distributions (wt, (Lat)a∈[A])t≥1 are a (Ft)t≥1-predictable sequence, in the sense
that (wt, (Lat)a∈[A]) is Ft−1-measurable. Then Ft contains all information (random chosen action,
random observed loss, random weight, and random loss distributions) before action at+1 is taken."
H,0.8362934362934363,"We assume the predictable sequence (wt)1≤t≤T have a global (non-random) upper bound W. Then
we deﬁne the log term ι = log(AT⌈log2 W⌉/p). We set"
H,0.8370656370656371,"ηt = γt =
r ι At."
H,0.8378378378378378,"F.4.1
REGRET BOUND"
H,0.8386100386100386,"In the following, we consider to give a high probability weighted regret bound for Algorithm 9.
Lemma F.2. Let (wt, (Lat)a∈[A])t≥1 be any Ft-predictable sequence satisfying 1 ≤mini≤t wi ≤
maxi≤t wi ≤W for some constant (non-random) W > 0 almost surely. Moreover, suppose wi is
non-decreasing. Then, following Algorithm 9, with probability at least 1 −4p, for any θ∗∈∆A and
t ≤T we have t
X"
H,0.8393822393822394,"i=1
wi(ℓi(ai) −⟨θ∗, ℓi⟩) ≤15 max
i≤t wi ·
h√"
H,0.8401544401544402,"Atι + ι
i
,"
H,0.8409266409266409,where ι = log(AT⌈log2 W⌉/p).
H,0.8416988416988417,"Proof. This lemma follows from the bound in Lemma F.3 and a concentration step that we establish
below."
H,0.8424710424710424,"Deﬁne M = ⌈log2 W⌉, and wi(k) = wi1

wi ≤2k	
. Then wi(k) is also Fi−1 measurable. We
Consider sequence

wi(k)(ℓi(ai) −⟨θi, ℓi⟩)"
H,0.8432432432432433,"i≥1. Since wi is Fi−1-measurable, E[wi(ℓi(ai) −
⟨θi, ℓi⟩)|Fi−1] = 0, which means

wi(k)(ℓi(ai) −⟨θi, ℓi⟩)"
H,0.844015444015444,"i≥1 is a martingale difference sequence
w.r.t. ﬁltration (Fi). So by Azuma-Hoeffding inequality, P t
X"
H,0.8447876447876448,"i=1
wi(k)(ℓi(ai) −⟨θi, ℓi⟩) ≤
q"
H,0.8455598455598455,"2 · ιt(2k)2
!"
H,0.8463320463320463,"≥1 −
p
TM ."
H,0.8471042471042471,"Taking a union bound, we have P "
H,0.8478764478764479,"∀k ∈[M], t
X"
H,0.8486486486486486,"i=1
wi(k)(ℓi(ai) −⟨θi, ℓi⟩) ≤2k√ 2tι !"
H,0.8494208494208494,"≥1 −p/T.
(27)"
H,0.8501930501930502,"Denote k′ = ⌈log2 maxi≤t wi⌉, then we have"
H,0.850965250965251,"n
∀k ∈[M], t
X"
H,0.8517374517374517,"i=1
wi(k)(ℓi(ai) −⟨θi, ℓi⟩) ≤2k√ 2tι
o"
H,0.8525096525096525,"⊆
n
t
X"
H,0.8532818532818532,"i=1
wi(k′)(ℓi(ai) −⟨θi, ℓi⟩) ≤2k√"
H,0.8540540540540541,"2tι
o
⊆
n
t
X"
H,0.8548262548262548,"i=1
wi(ℓi(ai) −⟨θi, ℓi⟩) ≤2 max
i≤t wi
√"
H,0.8555984555984556,"2tι
o
."
H,0.8563706563706563,Published as a conference paper at ICLR 2022
H,0.8571428571428571,"Then probability bound gives P t
X"
H,0.857915057915058,"i=1
wi(ℓi(ai) −⟨θi, ℓi⟩) ≤2 max
i≤t wi
√ 2tι !"
H,0.8586872586872587,≥1 −p/T.
H,0.8594594594594595,"Taking a union bound, we get t
X"
H,0.8602316602316602,"i=1
wi(ℓi(ai) −⟨θi, ℓi⟩) ≤2 max
i≤t wi
√"
H,0.861003861003861,2tι for all t ∈[T]
H,0.8617760617760618,"with probability at least 1 −p. Summing the above and the regret bound shown in Lemma F.3, we
ﬁnish the proof."
H,0.8625482625482626,"Lemma F.3. Let (wt, (Lat)a∈[A])t≥1 be any Ft-predictable sequence satisfying 1 ≤mini≤t wi ≤
maxi≤t wi ≤W for some constant (non-random) W > 0 almost surely. Moreover, suppose wi is
non-decreasing. Then, following Algorithm 9, with probability at least 1 −3p, for any θ∗∈∆A and
t ≤T we have t
X"
H,0.8633204633204633,"i=1
wi ⟨θi −θ∗, ℓi⟩≤10 max
i≤t wi ·
h√"
H,0.864092664092664,"Atι + ι
i
,"
H,0.8648648648648649,where ι = log(AT⌈log2 W⌉/p).
H,0.8656370656370657,Proof. The regret Rt(θ∗) can be decomposed into three terms
H,0.8664092664092664,"Rt (θ∗) = t
X"
H,0.8671814671814672,"i=1
wi ⟨θi −θ∗, ℓi⟩ = t
X"
H,0.8679536679536679,"i=1
wi
D
θi −θ∗, bℓi
E"
H,0.8687258687258688,"|
{z
}
(A) + t
X"
H,0.8694980694980695,"i=1
wi
D
θi, ℓi −bℓi
E"
H,0.8702702702702703,"|
{z
}
(B) + t
X"
H,0.871042471042471,"i=1
wi
D
θ∗, bℓi −ℓi
E"
H,0.8718146718146718,"|
{z
}
(C)"
H,0.8725868725868726,"and we bound (A) in Lemma F.5, (B) in Lemma F.6 and (C) in Lemma F.7."
H,0.8733590733590734,Setting ηt = γt = p ι
H,0.8741312741312741,"At, the conditions in Lemma F.5 and Lemma F.7 are satisﬁed. Putting them
together and take union bound, we have with probability 1 −3p"
H,0.8749034749034749,Rt (θ∗) ≤wt log A
H,0.8756756756756757,"ηt
+ A 2 t
X"
H,0.8764478764478765,"i=1
ηiwi + max
i≤t wiι + A t
X"
H,0.8772200772200772,"i=1
γiwi + 2 max
i≤t wi
√"
H,0.877992277992278,"2tι + 2 max
i≤t wiι/γt"
H,0.8787644787644787,"≤max
i≤t wi ""
√"
H,0.8795366795366796,"Aιt + 3
√ Aι
2 t
X i=1 1
√"
H,0.8803088803088803,"t + ι + 2
√"
H,0.8810810810810811,"2tι + 2
√ Atι #"
H,0.8818532818532818,"≤10 max
i≤t wi
h√"
H,0.8826254826254826,"Atι + ι
i
for all t ∈[T]"
H,0.8833976833976834,This proves the lemma.
H,0.8841698841698842,"The rest of this section is devoted to the proofs of the Lemmas used in the proofs of Lemma F.3.
We begin the following useful lemma adapted from Lemma 1 in Neu (2015), which is crucial in
constructing high probability guarantees."
H,0.8849420849420849,"Lemma F.4. For any predictable sequence of coefﬁcients c1, c2, . . . , ct s.t. ci ∈[0, 2γi]A w.r.t.
(Fi)i≥1 and ﬁxing t, we have with probability at least 1 −p/(AT), t
X"
H,0.8857142857142857,"i=1
wi
D
ci, bℓi −ℓi
E
≤2 max
i≤t wiι"
H,0.8864864864864865,Published as a conference paper at ICLR 2022
H,0.8872586872586873,"Proof. Deﬁne M = ⌈log2 W⌉, and wi(k) = wi1

wi ≤2k	
. By deﬁnition,"
H,0.888030888030888,wi(k)bℓi (a) =wi(k)˜ℓi (a) 1 {ai = a}
H,0.8888030888030888,"θi (a) + γi
≤
wi(k)˜ℓi (a) 1 {ai = a}"
H,0.8895752895752895,θi (a) + wi(k)˜ℓi(a)1{ai=a}
K,0.8903474903474904,"2k
γi = 2k 2γi"
K,0.8911196911196911,2γiwi(k)˜ℓi(a)1{ai=a}
K,0.8918918918918919,2kθi(a)
K,0.8926640926640926,1 + γiwi(k)˜ℓi(a)1{ai=a}
K,0.8934362934362934,2kθi(a)
K,0.8942084942084942,"(i)
≤2k"
K,0.894980694980695,"2γi
log "
K,0.8957528957528957,1 + 2γiwi(k)˜ℓi (a) 1 {ai = a}
K,0.8965250965250965,2kθi (a) !
K,0.8972972972972973,"where (i) is because
z
1+z/2 ≤log (1 + z) for all z ≥0."
K,0.8980694980694981,Deﬁning the sum
K,0.8988416988416988,bSi = wi(k)
K,0.8996138996138996,2k
K,0.9003861003861003,"D
ci, bℓi
E
, Si = wi(k)"
K,0.9011583011583012,"2k
⟨ci, ℓi⟩,"
K,0.901930501930502,"Then Si is Fi−1-measurable since wi, ci, ℓi are Fi−1-measurable. Using Ei[·] to denote the condi-
tional expectation E[·|Fi], we have"
K,0.9027027027027027,"Ei−1
h
exp

bSi
i
≤Ei−1 "" exp X a"
K,0.9034749034749034,ci (a)
K,0.9042471042471042,"2γi
log "
K,0.905019305019305,1 + 2γiwi(k)˜ℓi (a) 1 {ai = a}
K,0.9057915057915058,2kθi (a) !!#
K,0.9065637065637066,"(i)
≤Ei−1 ""Y a "
K,0.9073359073359073,1 + ci (a) wi(k)˜ℓi (a) 1 {ai = a}
K,0.9081081081081082,2kθi (a) !#
K,0.9088803088803089,"= Ei−1 "" 1 +
X a"
K,0.9096525096525097,ci (a) wi(k)˜ℓi (a) 1 {ai = a}
K,0.9104247104247104,2kθi (a) #
K,0.9111969111969112,= 1 + Si ≤exp (Si)
K,0.911969111969112,"where (i) is because z1 log (1 + z2) ≤log (1 + z1z2) for any 0 ≤z1 ≤1 and z2 ≥−1. Here we
are using the condition ci (a) ≤2γi to guarantee the condition is satisﬁed."
K,0.9127413127413128,"Equipped with the above bound, we can now prove the concentration result. P ""
t
X i=1"
K,0.9135135135135135,"
bSi −Si

≥ι # ≤P "" exp ""
t
X i=1"
K,0.9142857142857143,"
bSi −Si
# ≥ATM p #"
K,0.915057915057915,"≤
p
ATM Et−1 "" exp ""
t
X i=1"
K,0.9158301158301159,"
bSi −Si
##"
K,0.9166023166023166,"≤
p
ATM Et−2 "" exp"
K,0.9173745173745174,"""t−1
X i=1"
K,0.9181467181467181,"
bSi −Si
#"
K,0.918918918918919,"Et−1
h
exp

bSt −St
i#"
K,0.9196911196911197,"≤
p
ATM Et−2 "" exp"
K,0.9204633204633205,"""t−1
X i=1"
K,0.9212355212355212,"
bSi −Si
##"
K,0.922007722007722,"≤· · · ≤
p
ATM ."
K,0.9227799227799228,So we have
K,0.9235521235521236,"P

t
X"
K,0.9243243243243243,"t=1
wi(k)
D
ci, bℓi −ℓi
E
≤2kι

≥1 −p/(ATM)."
K,0.9250965250965251,"Taking a union bound,"
K,0.9258687258687258,"P

∀k ∈[M], t
X"
K,0.9266409266409267,"t=1
wi(k)
D
ci, bℓi −ℓi
E
≤2kι

≥1 −p/(AT).
(28)"
K,0.9274131274131274,Published as a conference paper at ICLR 2022
K,0.9281853281853282,"Denote k′ = ⌈log2 maxi≤t wi⌉, and note that"
K,0.9289575289575289,"n
∀k ∈[M], t
X"
K,0.9297297297297298,"t=1
wi(k)
D
ci, bℓi −ℓi
E
≤2kι
o"
K,0.9305019305019305,"⊆
n
t
X"
K,0.9312741312741313,"t=1
wi(k′)
D
ci, bℓi −ℓi
E
≤2k′ι
o
⊆
n
t
X"
K,0.932046332046332,"t=1
wi
D
ci, bℓi −ℓi
E
≤2 max
i≤t wiι
o
."
K,0.9328185328185328,"Therefore, we have P t
X"
K,0.9335907335907336,"i=1
wi
D
ci, bℓi −ℓi
E
≤2 max
i≤t wiι !"
K,0.9343629343629344,≥1 −p/(AT).
K,0.9351351351351351,This proves the lemma.
K,0.9359073359073359,"Using Lemma F.4, we can bound the (A)(B)(C) separately as below."
K,0.9366795366795366,"Lemma F.5. If ηi ≤2γi for all i ≤t and {ηi/wi}i≥1 is non-increasing, with probability 1 −p, for
any t ∈[T] and θ∗∈∆A, t
X"
K,0.9374517374517375,"i=1
wi
D
θi −θ∗, bℓi
E
≤wt log A"
K,0.9382239382239382,"ηt
+ A 2 t
X"
K,0.938996138996139,"i=1
ηiwi + 1"
MAX,0.9397683397683397,"2 max
i≤t wiι."
MAX,0.9405405405405406,"Proof. We use the standard analysis of FTRL with changing step size, see for example Exercise
28.12(b) in Lattimore & Szepesvári (2020). Notice the essential step size is ηt/wt which is non-
increasing and the essential loss vector is wtbℓt, we have t
X"
MAX,0.9413127413127413,"i=1
wi
D
θi −θ∗, bℓi
E
≤wt log A ηt
+ t
X"
MAX,0.9420849420849421,"i=1
wi"
MAX,0.9428571428571428,"
⟨θi −θi+1, ℓi⟩−KL(θi+1∥θi) ηi 
."
MAX,0.9436293436293436,"We claim that
D
θi −θi+1, bℓi
E
−KL(θi+1∥θi)"
MAX,0.9444015444015444,"ηi
≤ηi 2"
MAX,0.9451737451737452,"D
θi, bℓ2
i
E
.
(29)"
MAX,0.9459459459459459,"In fact, applying Theorem 26.13 in Lattimore & Szepesvári (2020) gives that
D
θi −θi+1, bℓi
E
−KL(θi+1∥θi)"
MAX,0.9467181467181467,"ηi
≤ηi 2"
MAX,0.9474903474903474,"D
uiθi + (1 −ui)θi+1, bℓ2
i
E
,"
MAX,0.9482625482625483,"for some ui ∈[0, 1]. Note that our bℓi only have at most one non-zero entry, i.e. bℓi(a) > 0 if and only
if a = ai. If θi(ai) ≥θi+1(ai), we have ηi 2"
MAX,0.949034749034749,"D
uiθi + (1 −ui)θi+1, bℓ2
i
E
= ηi"
MAX,0.9498069498069498,2 [uiθi(ai) + (1 −ui)θi+1(ai)]bℓi(ai)2 ≤ηi
MAX,0.9505791505791505,2 θi(ai)bℓi(ai)2 = ηi 2
MAX,0.9513513513513514,"D
θi, bℓ2
i
E
."
MAX,0.9521235521235522,"Otherwise, θi(ai) < θi+1(ai), so we have
D
θi −θi+1, bℓi
E
= [θi(ai) −θi+1(ai)]bℓi(ai) ≤0 and"
MAX,0.9528957528957529,"−KL(θi+1∥θi)/ηi ≤0. In both cases, (29) is correct. As a result, t
X"
MAX,0.9536679536679536,"i=1
wi
D
θi −θ∗, bℓi
E
≤wt log A"
MAX,0.9544401544401544,"ηt
+ 1 2 t
X"
MAX,0.9552123552123553,"i=1
ηiwi
D
θi,bl2
i
E"
MAX,0.955984555984556,≤wt log A
MAX,0.9567567567567568,"ηt
+ 1 2 t
X i=1 X"
MAX,0.9575289575289575,"a∈A
ηiwibℓi (a)"
MAX,0.9583011583011583,"(i)
≤wt log A"
MAX,0.9590733590733591,"ηt
+ 1 2 t
X i=1 X"
MAX,0.9598455598455599,"a∈A
ηiwiℓi (a) + max
i≤t wiι"
MAX,0.9606177606177606,Published as a conference paper at ICLR 2022
MAX,0.9613899613899614,≤wt log A
MAX,0.9621621621621622,"ηt
+ A 2 t
X"
MAX,0.962934362934363,"i=1
ηiwi + max
i≤t wiι"
MAX,0.9637065637065637,"where (i) is by using Lemma F.4 with ci(a) = ηi. The any-time guarantee is justiﬁed by taking union
bound."
MAX,0.9644787644787645,"Lemma F.6. With probability 1 −p, for any t ∈[T], t
X"
MAX,0.9652509652509652,"i=1
wi
D
θi, ℓi −bℓi
E
≤A t
X"
MAX,0.9660231660231661,"i=1
γiwi + 2 max
i≤t wi
√ 2tι."
MAX,0.9667953667953668,"Proof. We further decompose the left side into t
X"
MAX,0.9675675675675676,"i=1
wi
D
θi, ℓi −bℓi
E
= t
X"
MAX,0.9683397683397683,"i=1
wi
D
θi, ℓi −Ei−1[bℓi]
E
+ t
X"
MAX,0.9691119691119691,"i=1
wi
D
θi, Ei−1[bℓi] −bℓi
E
."
MAX,0.9698841698841699,"The ﬁrst term is bounded by t
X"
MAX,0.9706563706563707,"i=1
wi
D
θi, ℓi −Ei−1[bℓi]
E
= t
X"
MAX,0.9714285714285714,"i=1
wi"
MAX,0.9722007722007722,"θi, ℓi −
θi
θi + γi
ℓi  = t
X"
MAX,0.972972972972973,"i=1
wi"
MAX,0.9737451737451738,"θi,
γi
θi + γi
ℓi ≤A t
X"
MAX,0.9745173745173745,"i=1
γiwi."
MAX,0.9752895752895753,"To bound the second term, we use similar argument in the proof of Lemma F.4 , we deﬁne w =
maxi≤t wi, M = ⌈log2 W⌉. and wi(k) = wi1

wi ≤2k	
, notice"
MAX,0.976061776061776,"D
θi, bℓi
E
≤
X"
MAX,0.9768339768339769,"a∈A
θi (a)1 {at = a}"
MAX,0.9776061776061776,"θi(a) + γi
≤
X"
MAX,0.9783783783783784,"a∈A
1 {ai = a} = 1,"
MAX,0.9791505791505791,"thus {wi(k)
D
θi, Ei−1[bℓi] −bℓi
E
}t
i=1 is a bounded martingale difference sequence w.r.t. the ﬁltration"
MAX,0.9799227799227799,"{Fi}t
i=1. By Azuma-Hoeffding inequality, t
X"
MAX,0.9806949806949807,"i=1
wi(k)
D
θi, Ei−1[bℓi] −bℓi
E
≤"
MAX,0.9814671814671815,"v
u
u
t2ι t
X"
MAX,0.9822393822393822,"i=1
wi(k)2 ≤
q"
MAX,0.983011583011583,2ι · t(2k)2
MAX,0.9837837837837838,"with probability at least 1 −p/TM. Taking a union bound, we get t
X"
MAX,0.9845559845559846,"i=1
wi(k)
D
θi, Ei−1[bℓi] −bℓi
E
≤
√"
MAX,0.9853281853281853,"2ι · t22k ,
for all (t, k) ∈[T] × [M]"
MAX,0.9861003861003861,"with probability at least 1 −p. On this event, choosing k′ = ⌈log2 w⌉, we have t
X"
MAX,0.9868725868725868,"i=1
wi
D
θi, Ei−1[bℓi] −bℓi
E
≤ t
X"
MAX,0.9876447876447877,"i=1
wi(k′)
D
θi, Ei−1[bℓi] −bℓi
E ≤2k′√"
MAX,0.9884169884169884,"2ι · t ≤2 max
i
wi
√"
MAX,0.9891891891891892,2ι · t.
MAX,0.9899613899613899,This ends the proof.
MAX,0.9907335907335907,"Lemma F.7. With probability 1 −p, for any t ∈[T] and any θ∗∈∆A, if γi is non-increasing in i, t
X"
MAX,0.9915057915057915,"i=1
wi
D
θ∗, bℓi −ℓi
E
≤2 max
i≤t wiι/γt."
MAX,0.9922779922779923,Published as a conference paper at ICLR 2022
MAX,0.993050193050193,"Proof. Deﬁne a basis {ej}A
j=1 of RA by"
MAX,0.9938223938223938,"ej (a) =
1 if a = j,
0 otherwise."
MAX,0.9945945945945946,"Then for all the j ∈[A], apply Lemma F.4 with ci = γtej. Since now ci(a) ≤γt ≤γi, the condition
in Lemma F.4 is satisﬁed. As a result, for any t ∈[T] and j ∈[A], we have with probability at least
1 −p/(TA) that
t
X"
MAX,0.9953667953667954,"i=1
wi
D
ej, bℓi −ℓi
E
≤2 max
i≤t wiι/γt."
MAX,0.9961389961389961,"Taking a union bound, we have with probability at least 1 −p, t
X"
MAX,0.9969111969111969,"i=1
wi
D
ej, bℓi −ℓi
E
≤2 max
i≤t wiι/γt for all (j, t) ∈[m] × [T]."
MAX,0.9976833976833976,"Since any θ∗is a convex combination of {ej}A
j=1, on this event, we also have t
X"
MAX,0.9984555984555985,"i=1
wi
D
θ∗, bℓi −ℓi
E
≤2 max
i≤t wiι/γt for all t ∈[T]."
MAX,0.9992277992277993,This ﬁnishes the proof.
