Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017667844522968198,"Continual learning aims to sequentially learn from different tasks without catas-
1"
ABSTRACT,0.0035335689045936395,"trophic forgetting. With no assumptions of task dependence, the knowledge learned
2"
ABSTRACT,0.00530035335689046,"from observed tasks may not align with that required for future tasks. This may re-
3"
ABSTRACT,0.007067137809187279,"sult in models’ disruptive updates for learning future tasks, causing abrupt changes
4"
ABSTRACT,0.0088339222614841,"to previously learned knowledge (e.g. representation drift [7]) which induces
5"
ABSTRACT,0.01060070671378092,"catastrophic forgetting. To reduce such disruptive updates, we connect knowledge
6"
ABSTRACT,0.012367491166077738,"for observed and unknown tasks by learning task data representations properly
7"
ABSTRACT,0.014134275618374558,"related to a set of global prototypes, which have general-purpose connections and
8"
ABSTRACT,0.015901060070671377,"are shared across all tasks. We derive global prototypes and the corresponding
9"
ABSTRACT,0.0176678445229682,"objective for NLP tasks. For those tasks, the correlated global prototypes can be
10"
ABSTRACT,0.019434628975265017,"obtained from a model pre-trained by masked language modeling. And the data
11"
ABSTRACT,0.02120141342756184,"representations that have proper relationships to global prototypes can be learned
12"
ABSTRACT,0.022968197879858657,"by specific adaptations of the pre-trained model. We investigate existing adaptation
13"
ABSTRACT,0.024734982332155476,"models and propose a neighbor attention model which combines different advan-
14"
ABSTRACT,0.026501766784452298,"tages of existing models for our objective. Experiments show that models learning
15"
ABSTRACT,0.028268551236749116,"data representations well related to global prototypes can induce significantly less
16"
ABSTRACT,0.030035335689045935,"catastrophic forgetting, without memorizing information from past tasks.
17"
INTRODUCTION,0.03180212014134275,"1
Introduction
18"
INTRODUCTION,0.03356890459363958,"In the continual learning paradigm, models progressively learn a sequence of tasks. This paradigm
19"
INTRODUCTION,0.0353356890459364,"supports real-world applications which face continuous streams of data and tasks [35, 20]. In practice,
20"
INTRODUCTION,0.037102473498233215,"models may be under storage constraints to use a fixed structure and under privacy considerations that
21"
INTRODUCTION,0.038869257950530034,"restrict revisiting of previous tasks’ data. These introduce the challenge of catastrophic forgetting,
22"
INTRODUCTION,0.04063604240282685,"where models lose knowledge of previously learned tasks after learning new tasks.
23"
INTRODUCTION,0.04240282685512368,"Most prior works address catastrophic forgetting using models that integrate the knowledge of the
24"
INTRODUCTION,0.044169611307420496,"past and present tasks, i.e. the observed tasks. For example, regularization-based models constrain
25"
INTRODUCTION,0.045936395759717315,"the deviation of current parameters from the previous ones [27, 56, 2, 29]; replay-based models
26"
INTRODUCTION,0.04770318021201413,"memorize samples from past tasks and rehearse when learning present tasks [35, 9, 46, 26]. However,
27"
INTRODUCTION,0.04946996466431095,"since there are no assumptions on task dependence in continual learning, models learned from a
28"
INTRODUCTION,0.05123674911660778,"set of observed tasks may not contain knowledge needed for unknown future tasks [28, 16]. To
29"
INTRODUCTION,0.053003533568904596,"learn such a future task, these models may have disruptive changes on previously learned knowledge
30"
INTRODUCTION,0.054770318021201414,"(e.g. representation drift [7]), which still induces catastrophic forgetting. One way to reduce such
31"
INTRODUCTION,0.05653710247349823,"disruptive updates is to make models consider potential knowledge connections to future tasks.
32"
INTRODUCTION,0.05830388692579505,"Our key idea is to build connections between observed and unknown tasks by connecting task-specific
33"
INTRODUCTION,0.06007067137809187,"data representations to a general-purpose representation base that is shared across all tasks. In many
34"
INTRODUCTION,0.061837455830388695,"domains, task-specific information about classes can be represented by specific combinations of
35"
INTRODUCTION,0.0636042402826855,"general units. For example, consider the data instance ‘A boy in a red hooded top is smiling. The
36"
INTRODUCTION,0.06537102473498234,"Figure 1: Representations learned with or without global prototypes. The shaded regions cover data
representations for each class. In (a), with knowledge only learned for observed supervised tasks,
models may have disruptive updates that cause data representation drift when learning a new task. In
(b), with reference to correlated global prototypes (dots) in each task learning, representations for
different tasks (shaded regions) can properly connect to each other which reduces representation drift."
INTRODUCTION,0.06713780918727916,"boy is upset.’ from ‘contradiction’ class in an entailment classification task. The set {smiling, upset}
37"
INTRODUCTION,0.06890459363957598,"conveys the task-specific information of ‘contradiction’ using the general (i.e. not task-specific)
38"
INTRODUCTION,0.0706713780918728,"semantics of the token units ‘smiling’ and ‘upset’. Based on this, we construct a general-purpose
39"
INTRODUCTION,0.07243816254416961,"representation base consisting a set of unit representations, which we call global prototypes. These
40"
INTRODUCTION,0.07420494699646643,"global prototypes are pre-learned to reflect semantic connections between them. Then we learn
41"
INTRODUCTION,0.07597173144876325,"data representations with appropriate task-specific connections to global prototypes. This allows
42"
INTRODUCTION,0.07773851590106007,"knowledge learned from observed tasks to connect to that of future tasks via the interconnection
43"
INTRODUCTION,0.07950530035335689,"of global prototypes, which is beyond the scope of task supervision from observed tasks. Our idea
44"
INTRODUCTION,0.0812720848056537,"mimics mechanism in the brain, a biological continual learning system [56] which rewires existing
45"
INTRODUCTION,0.08303886925795052,"neurons instead of creating new neurons to learn new tasks [17]. Here, global prototypes mimic
46"
INTRODUCTION,0.08480565371024736,"the neurons, and learning different connections between data representations and global prototypes
47"
INTRODUCTION,0.08657243816254417,"mimic the rewiring process. A figure of the idea is shown in Figure 1.
48"
INTRODUCTION,0.08833922261484099,"We address two main challenges in realizing this idea: (1). constructing the representation base with
49"
INTRODUCTION,0.09010600706713781,"correlated global prototypes; (2). learning data representations with task-specific connections to
50"
INTRODUCTION,0.09187279151943463,"global prototypes. We investigate the above challenges for NLP tasks. For text, the non-contextual
51"
INTRODUCTION,0.09363957597173145,"token representations are a natural choice for global prototypes, as any text information can be
52"
INTRODUCTION,0.09540636042402827,"represented by sets of tokens from a fixed vocabulary. For the first challenge, we obtain the global
53"
INTRODUCTION,0.09717314487632508,"prototypes from a pre-trained language model which learns semantic connections between tokens
54"
INTRODUCTION,0.0989399293286219,"through self-supervised learning [11]. For the second challenge, we learn data representations by
55"
INTRODUCTION,0.10070671378091872,"lightly adapting a pre-trained model to obtain task-specific connections to the global prototypes
56"
INTRODUCTION,0.10247349823321555,"(Section 3). We investigate existing adaptation models with learnable projections (Adapters [21]),
57"
INTRODUCTION,0.10424028268551237,"learnable embeddings (Prompt Tuning [30]), and propose a neighbor attention module combining
58"
INTRODUCTION,0.10600706713780919,"properties of these two (Section 4). Results show that catastrophic forgetting can be significantly
59"
INTRODUCTION,0.10777385159010601,"mitigated with models that can learn representations well connected to global prototypes. In addition,
60"
INTRODUCTION,0.10954063604240283,"our neighbor attention model combines the advantages of existing adaptation models, and achieves
61"
INTRODUCTION,0.11130742049469965,"superior performance in both vanilla and replay settings.
62"
INTRODUCTION,0.11307420494699646,"In conclusion, our contributions in this paper are:
63"
WE PROPOSE TO LEARN TASK-SPECIFIC INFORMATION OVER A GENERAL-PURPOSE BASE WITH GLOBAL,0.11484098939929328,"1. We propose to learn task-specific information over a general-purpose base with global
64"
WE PROPOSE TO LEARN TASK-SPECIFIC INFORMATION OVER A GENERAL-PURPOSE BASE WITH GLOBAL,0.1166077738515901,"prototypes to address general task connections in continual learning. Specifically, we derive
65"
WE PROPOSE TO LEARN TASK-SPECIFIC INFORMATION OVER A GENERAL-PURPOSE BASE WITH GLOBAL,0.11837455830388692,"the construction of the base and the corresponding objective for NLP tasks.
66"
WE INVESTIGATE EXISTING ADAPTATION MODELS AND PROPOSE A NEW NEIGHBOR ATTENTION MODEL TO,0.12014134275618374,"2. We investigate existing adaptation models and propose a new neighbor attention model to
67"
WE INVESTIGATE EXISTING ADAPTATION MODELS AND PROPOSE A NEW NEIGHBOR ATTENTION MODEL TO,0.12190812720848057,"learn data representations that have proper relationships to global prototypes.
68"
WE INVESTIGATE EXISTING ADAPTATION MODELS AND PROPOSE A NEW NEIGHBOR ATTENTION MODEL TO,0.12367491166077739,"3. We conduct experiments on different adaptation models and continual learning frameworks.
69"
WE INVESTIGATE EXISTING ADAPTATION MODELS AND PROPOSE A NEW NEIGHBOR ATTENTION MODEL TO,0.1254416961130742,"Results show our model can significantly reduce forgetting without replay.
70"
RELATED WORK,0.127208480565371,"2
Related Work
71"
RELATED WORK,0.12897526501766785,"Continual Learning Continual learning aims to sequentially learn new tasks while not forgetting
72"
RELATED WORK,0.13074204946996468,"previously learned tasks. Models for continual learning can be divided into three main categories: (1).
73"
RELATED WORK,0.13250883392226148,"regularization-based models which constrain the deviation of new parameters from the older ones
74"
RELATED WORK,0.13427561837455831,"[27, 56, 2, 29]; (2) replay-based models which reduce catastrophic forgetting by rehearsing on real or
75"
RELATED WORK,0.13604240282685512,"pseudo samples from previous tasks [35, 9] or generative models [46, 26]; (3). architecture-based
76"
RELATED WORK,0.13780918727915195,"models which learn evolving architectures for sequential tasks, with their capacities for each task
77"
RELATED WORK,0.13957597173144876,"carefully assigned [44, 53]. Most works above focus on knowledge based on observed tasks.
78"
RELATED WORK,0.1413427561837456,"Some recent works show that knowledge only from observed task supervision is insufficient for
79"
RELATED WORK,0.1431095406360424,"continual learning. Knoblauch et al. [28] claim that optimal continual learning requires perfect
80"
RELATED WORK,0.14487632508833923,"memory and is NP-hard; Guo et al. [16] suggest preservingholistic information which may not benefit
81"
RELATED WORK,0.14664310954063603,"current task but help future tasks. With biased knowledge, models can have disruptive updating
82"
RELATED WORK,0.14840989399293286,"when learning a new task, causing problems like representation drift [7, 36, 24]. In this paper, we
83"
RELATED WORK,0.1501766784452297,"propose to consider knowledge beyond observed task supervision through a general-purpose base
84"
RELATED WORK,0.1519434628975265,"with pre-learned global prototypes. Unlike previous works [39, 4] which use a pre-defined classifier
85"
RELATED WORK,0.15371024734982333,"to help class separation [38], our global prototypes are pre-learned with general semantic connections
86"
RELATED WORK,0.15547703180212014,"and thus can build connections between tasks. Some works use self-supervised learning to learn more
87"
RELATED WORK,0.15724381625441697,"general representations for continual learning [36, 14, 15]. However, those representations do not
88"
RELATED WORK,0.15901060070671377,"necessarily connect to specific global prototypes, which is different from our objective.
89"
RELATED WORK,0.1607773851590106,"Continual learning for NLP is an emerging area [3]. Liu et al. [32] introduce a sentence encoder
90"
RELATED WORK,0.1625441696113074,"with matrix conceptors; MBPA++ [10] uses an episodic memory with replay and local adaptation to
91"
RELATED WORK,0.16431095406360424,"mitigate catastrophic forgetting; LAMOL [49] learns to generate training samples for replay based
92"
RELATED WORK,0.16607773851590105,"on pre-trained knowledge; IDBR [23] disentangles hidden spaces to distinguish task-agnostic and
93"
RELATED WORK,0.16784452296819788,"task-specific information. Most of them require a memory of past task information, or converting
94"
RELATED WORK,0.1696113074204947,"data to a question-answering format along with text-to-text models [6, 42]. Our model does not have
95"
RELATED WORK,0.17137809187279152,"such restriction. There are also works [25] focusing on knowledge transfer in continual learning.
96"
RELATED WORK,0.17314487632508835,"Adaptation Models In this work, we use adaptation models to learn representations connected to
97"
RELATED WORK,0.17491166077738515,"global prototypes. Prior works using pre-trained model with light adaptation for target tasks were
98"
RELATED WORK,0.17667844522968199,"originally aimed at parameter efficient tuning. Different methods include adding limited trainable
99"
RELATED WORK,0.1784452296819788,"parameters on the frozen transformer layer [21, 40, 18, 22]; or selectively updating existing parameters
100"
RELATED WORK,0.18021201413427562,"during training [41, 55]. Recent prompt tuning works [31, 30, 34] learn target tasks by trainable
101"
RELATED WORK,0.18197879858657243,"prompt embeddings for generalization purposes as well.
102"
RELATED WORK,0.18374558303886926,"Most closely related work are adaptation models used for continual learning [51, 13, 43]. However,
103"
RELATED WORK,0.1855123674911661,"most use the models’ parameter efficiency to construct progressive memory. Whether utilizing the
104"
RELATED WORK,0.1872791519434629,"pre-trained knowledge can help continual learning, why and how they help remain unexplored. Our
105"
RELATED WORK,0.18904593639575973,"approach is based on a fixed model without progressive memory of parameters. We use the adaptation
106"
RELATED WORK,0.19081272084805653,"model for our desiderata, which also provides a metric to interpret whether the model can benefit
107"
RELATED WORK,0.19257950530035337,"continual learning. We believe our work can inspire further utilization of adaptation models for CL.
108"
LEARNING OVER GLOBAL PROTOTYPES,0.19434628975265017,"3
Learning over Global Prototypes
109"
LEARNING OVER GLOBAL PROTOTYPES,0.196113074204947,"We consider the following continual learning setting: the model learns from a sequence of tasks,
110"
LEARNING OVER GLOBAL PROTOTYPES,0.1978798586572438,"where each task consists of data Dτ = {(x(i)
τ , y(i)
τ )nτ
i=1}. xτ is the input data and yτ is the class label.
111"
LEARNING OVER GLOBAL PROTOTYPES,0.19964664310954064,"A task identifier τ is provided at the training time. We consider two scenarios: task-incremental and
112"
LEARNING OVER GLOBAL PROTOTYPES,0.20141342756183744,"class-incremental learning, where models are task-aware or task-agnostic at the inference time [37].
113"
LEARNING OVER GLOBAL PROTOTYPES,0.20318021201413428,"Without replay, we use the same training objective for both task-incremental and class-incremental
114"
LEARNING OVER GLOBAL PROTOTYPES,0.2049469964664311,"learning while evaluating them in different ways.
115"
LEARNING OVER GLOBAL PROTOTYPES,0.2067137809187279,"Notation
Cτ represents a set of all classes for each task τ, C = [C1, ...Cτ, ...] represents all
116"
LEARNING OVER GLOBAL PROTOTYPES,0.20848056537102475,"classes for all tasks. For NLP tasks, V represents the set of tokens with global prototypes in the
117"
LEARNING OVER GLOBAL PROTOTYPES,0.21024734982332155,"representation base. wi is the i-th column of a matrix w. Our main model consists of two components:
118"
LEARNING OVER GLOBAL PROTOTYPES,0.21201413427561838,"an encoder fθ to generate representation fθ(xτ) for each data instance xτ; and a classifier with matrix
119"
LEARNING OVER GLOBAL PROTOTYPES,0.2137809187279152,"wγ ∈Rd×|C| for class prediction, where d represents the dimension of data representations. At the
120"
LEARNING OVER GLOBAL PROTOTYPES,0.21554770318021202,"inference time, the class label is predicted by arg maxi∈Ccandidate fθ(xτ) · wi
γ. For task-incremental
121"
LEARNING OVER GLOBAL PROTOTYPES,0.21731448763250882,"inference we have Ccandidate = Cτ, while for class-incremental inference we have Ccandidate = C1:τ.
122"
THE LEARNING OBJECTIVE,0.21908127208480566,"3.1
The Learning Objective
123"
THE LEARNING OBJECTIVE,0.22084805653710246,"Classification Loss For a task τ, the typical classification objective is to minimize the cross-entropy
124"
THE LEARNING OBJECTIVE,0.2226148409893993,"loss Lc(xτ; θ, γ) over the training data for the task, as shown below:
125"
THE LEARNING OBJECTIVE,0.22438162544169613,"Lc(xτ; θ, γ) = −log
exp
 
wyτ
γ · fθ(xτ)
 P"
THE LEARNING OBJECTIVE,0.22614840989399293,"c∈Cτ exp
 
wcγ · fθ(xτ)
.
(1)"
THE LEARNING OBJECTIVE,0.22791519434628976,"After learning task τ, models have knowledge about data x1:τ and class vectors wc∈C1:τ
γ
from
126"
THE LEARNING OBJECTIVE,0.22968197879858657,"observed tasks 1 : τ. However, the knowledge may not align with that required for the unknown
127"
THE LEARNING OBJECTIVE,0.2314487632508834,"future task (τ + 1). Specifically, after adjusting θ in task (τ + 1), the alignment between wyτ
γ learned
128"
THE LEARNING OBJECTIVE,0.2332155477031802,"from task τ and fθ(xτ) with adjusted θ may shift and degrade. In other words, to learn a future task,
129"
THE LEARNING OBJECTIVE,0.23498233215547704,"models may have disruptive updates which make abrupt changes to previously learned knowledge
130"
THE LEARNING OBJECTIVE,0.23674911660777384,"(e.g. representation drift [7]), and induce forgetting.
131"
THE LEARNING OBJECTIVE,0.23851590106007067,"Prototype Loss To mitigate models’ disruptive updates, we consider potential connections between
132"
THE LEARNING OBJECTIVE,0.24028268551236748,"observed and unknown tasks. The connection is built by learning task-specific data representations
133"
THE LEARNING OBJECTIVE,0.2420494699646643,"connected to a general-purpose representation base, which is shared across all tasks. The base consists
134"
THE LEARNING OBJECTIVE,0.24381625441696114,"of global token prototypes (denoted proto[v] for token v) which reflect semantic connections between
135"
THE LEARNING OBJECTIVE,0.24558303886925795,"them. In particular, we want the data representation fθ(xτ) to be connected to the task-relevant global
136"
THE LEARNING OBJECTIVE,0.24734982332155478,"prototypes. Given a reference probability distribution p(v|xτ, yτ) which indicates the strength of
137"
THE LEARNING OBJECTIVE,0.24911660777385158,"connection between data representation and proto[v], we push the data representations towards the
138"
THE LEARNING OBJECTIVE,0.2508833922261484,"prototypes in proportion to their reference probability. Formally, we define the prototype loss as:
139"
THE LEARNING OBJECTIVE,0.25265017667844525,"Lv(xτ; θ) = −
X"
THE LEARNING OBJECTIVE,0.254416961130742,"v∈V p(v|xτ, yτ) log
exp
 
proto[v] · fθ(xτ)
 P"
THE LEARNING OBJECTIVE,0.25618374558303886,"v′∈V exp
 
proto[v′] · fθ(xτ)
.
(2)"
THE LEARNING OBJECTIVE,0.2579505300353357,"In Eq.(2), the softmax is calculated over all global prototypes, i.e. proto[v] for any v ∈V , regardless
140"
THE LEARNING OBJECTIVE,0.2597173144876325,"of task difference. Such calculation is task-agnostic, while the referenced probability p(v|xτ, yτ)
141"
THE LEARNING OBJECTIVE,0.26148409893992935,"gives task-specific guidance for representation learning. By doing this, Eq. (2) learns representations
142"
THE LEARNING OBJECTIVE,0.26325088339222613,"with task-specific connections to global prototypes. Since global prototypes are pre-learned to reflect
143"
THE LEARNING OBJECTIVE,0.26501766784452296,"semantic connections, representations learned by Eq. (2) can connect across tasks via connections of
144"
THE LEARNING OBJECTIVE,0.2667844522968198,"global prototypes. This can reduce abrupt representation change caused by disruptive updating.
145"
THE LEARNING OBJECTIVE,0.26855123674911663,"The reference probability p(v|xτ, yτ) gives task-specific guidance for representation learning, where
146"
THE LEARNING OBJECTIVE,0.2703180212014134,"tokens with task-specific information of xτ should have high probabilities. Considering both task-
147"
THE LEARNING OBJECTIVE,0.27208480565371024,"specific and holistic information of the data [16, 36], we set p(v|xτ, yτ) = 1/rτ when v is one of
148"
THE LEARNING OBJECTIVE,0.27385159010600707,"data’s rτ rationale tokens, i.e. tokens in the data that are essential for class prediction [8], otherwise
149"
THE LEARNING OBJECTIVE,0.2756183745583039,"p(v|xτ, yτ) = 0. Using multiple rationale tokens as task-specific guidance brings extra benefits to
150"
THE LEARNING OBJECTIVE,0.2773851590106007,"the expressiveness of data representations and global prototypes. First, different data representations
151"
THE LEARNING OBJECTIVE,0.2791519434628975,"from the same class have different guidance. Second, a small number of global prototypes can convey
152"
THE LEARNING OBJECTIVE,0.28091872791519434,"rich information when connecting representations to different sets of global prototypes.
153"
THE LEARNING OBJECTIVE,0.2826855123674912,"Learning Objective Based on the above analysis, our learning objective is to learn data representa-
154"
THE LEARNING OBJECTIVE,0.284452296819788,"tions that can correctly predict class labels (Eq. (1)); and properly connect to global prototypes (Eq.
155"
THE LEARNING OBJECTIVE,0.2862190812720848,"(2)). The optimal parameters θ∗, γ∗for task τ should satisfy the desiderata below:
156"
THE LEARNING OBJECTIVE,0.2879858657243816,"r Task performance. Lc(xτ; θ∗, γ∗) ≤Lc(xτ; θ, γ) for any θ ̸= θ∗, γ ̸= γ∗
(3)
r Global alignment. Lv(xτ; θ∗) ≤aτ
(4)"
THE LEARNING OBJECTIVE,0.28975265017667845,"where aτ > 0 is a threshold value of the prototype loss. Task performance desiderata (Eq. (3)) can be
157"
THE LEARNING OBJECTIVE,0.2915194346289753,"satisfied by optimization on classification loss in Eq. (1). In the rest of this section, we discuss two
158"
THE LEARNING OBJECTIVE,0.29328621908127206,"questions that are necessary for our desiderata: (1). How to get the semantically connected global
159"
THE LEARNING OBJECTIVE,0.2950530035335689,"prototype proto[v] for Eq. (2)? (2). How to get feasible models for the second desiderata in Eq. (4)?
160"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.2968197879858657,"3.2
Pre-trained Models for Prototypes and Data Representations
161"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.29858657243816256,"To get correlated global prototypes and learn data representations with reference to them, we utilize a
162"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3003533568904594,"model pre-trained by masked language modeling (MLM). The MLM objective is to predict masked
163"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.30212014134275617,Figure 2: Layers of the transformer and different adaptation models. Shaded blocks are learnable.
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.303886925795053,"token vm from a masked input ˜x, with the following loss:
164"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.30565371024734983,"Lm(˜x; δ, ϕ) = −
X"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.30742049469964666,"v∈V p(v|˜x) log
exp
 
wv
δ · fϕ(˜x)
"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.30918727915194344,"P
v′∈V exp
 
wv′
δ · fϕ(˜x)
,
(5)"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.31095406360424027,"where fϕ denotes the encoder for MLM, wδ consists of the token vector wv
δ for each token v. The
165"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3127208480565371,"probability p(v|˜x) = 1 if v is the masked token vm, and 0 otherwise.
166"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.31448763250883394,"Pre-Trained Model for Global Prototypes The MLM objective learns token vectors wδ that reflect
167"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.31625441696113077,"semantic connections between tokens, which suits our requirement for global prototypes. Therefore,
168"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.31802120141342755,"we can get the global prototype proto[v] as the v-th token vector (proto[v] = wv
δ) from a model
169"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3197879858657244,"pre-trained by MLM. Extending to cases when pre-trained models are unavailable, we can first train
170"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3215547703180212,"a model by self-supervised learning which learns global prototypes. Global prototypes are fixed once
171"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.32332155477031804,"learned. We leave improving them during continual task learning for future study.
172"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3250883392226148,"Adapting Pre-Trained Models for Feasibility To get feasible models for the desiderata in Eq.(4),
173"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.32685512367491165,"we have two options: (a). learning with the prototype loss in Eq.(2); (b). designing a model which
174"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3286219081272085,"can satisfy the desiderata without direct supervision of probabilities p(v|xτ, yτ). Option (a) needs
175"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3303886925795053,"rationale tokens to get p(v|xτ, yτ), which requires expensive human annotations. In this work, we
176"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3321554770318021,"investigate models for option (b). Specifically, we investigate whether adapting a pre-trained model
177"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3339222614840989,"where we get global prototypes can satisfy our desiderata. Comparing Eq.(5) and Eq.(2), when
178"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.33568904593639576,"having proto[v] = wv
δ, models for Eq.(5) learn representations that have task-agnostic connections
179"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3374558303886926,"to global prototypes, which is a variant of Eq.(2). When lightly adapting a pre-trained encoder fϕ to
180"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3392226148409894,"task encoder fθ, data representations are learned with reference to those task-agnostic connections.
181"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3409893992932862,"Therefore, the adapted representations may have better connections to global prototypes.
182"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.34275618374558303,"In general, our learning includes two stages: first training a model by self-supervised learning
183"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.34452296819787986,"for global prototypes (can be skipped if starting from a pre-trained language model); then lightly
184"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3462897526501767,"adapting this model for target tasks while satisfying the desiderata in Eq. (4). We investigate different
185"
PRE-TRAINED MODELS FOR PROTOTYPES AND DATA REPRESENTATIONS,0.3480565371024735,"adaptation models and whether they satisfy our desiderata in the following sections.
186"
ADAPTATION MODELS FOR GLOBAL ALIGNMENT,0.3498233215547703,"4
Adaptation Models for Global Alignment
187"
ADAPTATION MODELS FOR GLOBAL ALIGNMENT,0.35159010600706714,"We investigate the potential of different adaptation models for our desiderata of global alignment in
188"
ADAPTATION MODELS FOR GLOBAL ALIGNMENT,0.35335689045936397,"Eq.(4). In this section, we first introduce existing adaptation models (Section 4.1) and propose a new
189"
ADAPTATION MODELS FOR GLOBAL ALIGNMENT,0.3551236749116608,"neighbor attention model for the desiderata (Section 4.2). A comparison of models is shown in Fig. 2.
190"
EXISTING ADAPTATION MODELS,0.3568904593639576,"4.1
Existing Adaptation Models
191"
EXISTING ADAPTATION MODELS,0.3586572438162544,"For a transformer model, representations are calculated by the self-attention mechanism. Given input
192"
EXISTING ADAPTATION MODELS,0.36042402826855124,"representations H = [h1, ..., hn], each output representation oi after self-attention is:
193"
EXISTING ADAPTATION MODELS,0.3621908127208481,"oi = f
 
MHA
 
Qϕ(hi), Kϕ(H), Vϕ(H)

,
(6)"
EXISTING ADAPTATION MODELS,0.36395759717314485,"where MHA is the multi-head attention function (Appendix A), f is the feed-forward function,
194"
EXISTING ADAPTATION MODELS,0.3657243816254417,"Qϕ, Kϕ, Vϕ are linear functions for query, key and value. Adaptation models utilize pre-trained
195"
EXISTING ADAPTATION MODELS,0.3674911660777385,"parameters for self-attentions, while adding extra components to adapt the model for target tasks.
196"
EXISTING ADAPTATION MODELS,0.36925795053003535,"According to He et al. [18], different adaptations can be viewed as combining different modification
197"
EXISTING ADAPTATION MODELS,0.3710247349823322,"vectors ∆θoi to pre-trained representation oi. We investigate two types of modifications below.
198"
EXISTING ADAPTATION MODELS,0.37279151943462896,"Learnable Projections Models like Adapters [21] insert adaptation modules between transformer
199"
EXISTING ADAPTATION MODELS,0.3745583038869258,"layers. The module applies linear projections to the self-attention output oi, with the non-linear
200"
EXISTING ADAPTATION MODELS,0.3763250883392226,"activation between them. With a residual connection [19], the adapted output o(new)
i
is:
201"
EXISTING ADAPTATION MODELS,0.37809187279151946,"o(new)
i
←oi + ∆θoi,
∆θoi := Wθoi.
(7)"
EXISTING ADAPTATION MODELS,0.37985865724381623,"Wθ ∈Rd×d represents the linear projections. (We omit the non-linear activation for simplicity).
202"
EXISTING ADAPTATION MODELS,0.38162544169611307,"Learnable Embeddings Models like Prompt Tuning [30] add learnable embeddings in the input.
203"
EXISTING ADAPTATION MODELS,0.3833922261484099,"Then self-attention is performed based on the input with prompts. The adapted output is [18]:
204"
EXISTING ADAPTATION MODELS,0.38515901060070673,"o(new)
i
←(1 −λ(hi))oi + λ(hi)∆θoi,
∆θoi := MHA
 
Qϕ(hi), Kϕ(Pθ), Vϕ(Pθ)

.
(8)"
EXISTING ADAPTATION MODELS,0.3869257950530035,"Pθ are learnable prompt embeddings in Rp×d, p is the number of prompts. λ(hi) is a gate value
205"
EXISTING ADAPTATION MODELS,0.38869257950530034,"computed from self-attention which decides the ratio of pre-trained and modified representations.
206"
EXISTING ADAPTATION MODELS,0.39045936395759717,"Choices for Global Alignment Both of the adaptations show effectiveness in single-task perfor-
207"
EXISTING ADAPTATION MODELS,0.392226148409894,"mance for our desiderata Eq. (3) [21, 31]. For global alignment in Eq. (4), Prompt Tuning has a gate
208"
EXISTING ADAPTATION MODELS,0.39399293286219084,"λ(hi) to mix pre-trained and modified representations. With a small gate value, this may generate
209"
EXISTING ADAPTATION MODELS,0.3957597173144876,"representations close to pre-trained representations, and thus better connect to global prototypes.
210"
EXISTING ADAPTATION MODELS,0.39752650176678445,"However, the gate λ(hi) in Eq. (8) is decided by self attention over inputs and prompts, thus can lean
211"
EXISTING ADAPTATION MODELS,0.3992932862190813,"to modified representations ∆θoi. Also, the learned prompts Pθ may convey information far away
212"
EXISTING ADAPTATION MODELS,0.4010600706713781,"from the original data. These may degrade the models’ capacity for global alignment. Because of
213"
EXISTING ADAPTATION MODELS,0.4028268551236749,"this, we propose a model that has a controlled gate value and relies on neighbors of tokens instead of
214"
EXISTING ADAPTATION MODELS,0.4045936395759717,"searching from random prompts for task adaptation. In addition, the training for prompt embeddings
215"
EXISTING ADAPTATION MODELS,0.40636042402826855,"is not as easy as that for linear projections [30, 22], which may cause efficiency issues when adapting
216"
EXISTING ADAPTATION MODELS,0.4081272084805654,"multiple tasks. We also introduce learnable projections in our model for fast adaptations.
217"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4098939929328622,"4.2
Transformer with Neighbor Attentions
218"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.411660777385159,"We design a neighbor attention module added to the pre-trained model for task adaptations. The mod-
219"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4134275618374558,"ule has three properties: (1). utilizing learnable linear projections to learn modified representations;
220"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.41519434628975266,"(2). acquiring neighbor representations for extra information; (3). using a controlled gate to mix
221"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4169611307420495,"pre-trained and modified representations. The adapted output of the neighbor attention module is:
222"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.41872791519434627,"o(new)
i
←(1 −λ)oi + λ∆θoi, ∆θoi := MHA
 
Qϕ(hi), Kθ(Mi||hi), Vθ(Mi||hi)

.
(9)"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4204946996466431,"where λ is the ratio of modified representations in the mix-up, || denotes the concatenation operation.
223"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.42226148409893993,"Kθ, Vθ are learnable linear functions for key and value. Mi = [mi1, ..., mik] are k neighbor
224"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.42402826855123676,"representations of the input representation hi.
225"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.42579505300353354,"Comparing Eq. (9) to Eq. (8), neighbor attention has learnable linear functions for key and value.
226"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4275618374558304,"Moreover, we manually control the gate by setting λ = 0.1 to push the module to focus more on the
227"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4293286219081272,"pre-trained representations. This is for our desiderata to have representations close to pre-trained
228"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.43109540636042404,"ones which are trained over global prototypes. Finally, we introduce neighbor representations Mi for
229"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.43286219081272087,"information out of the inputs, which can improve the model’s expressivity. Details are shown below.
230"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.43462897526501765,"Neighbor Representations Before the first neighbor attention layer, we find the initial neighbor
231"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4363957597173145,"representations Mi for a hidden representation hi. Neighbors of hi can be obtained by comparing the
232"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4381625441696113,"dot product between hi and token embeddings from the pre-trained embedding layer, then selecting
233"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.43992932862190814,"k tokens which have top-K scores as neighbors. K decides the range of the neighborhood.
234"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4416961130742049,"Then we transform neighbor embeddings to the space of hi. We disentangle hi’s j-th neighbor
235"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.44346289752650175,"representation mij into two parts: one related to the hidden representation hi; and the other related to
236"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4452296819787986,"neighbor information out of hi. The latter can be obtained by deviating neighbor embedding eij from
237"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4469964664310954,"hi’s token embedding ei. Then the transformed neighbor representation is: mij = α(eij −ei)+βhi,
238"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.44876325088339225,"where 0 < α, β < 1 are scalars. In this paper, we set α = β = 0.2.
239"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.450530035335689,"After that, the neighbor representation Mi is updated at each neighbor attention layer. For the j-th
240"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.45229681978798586,"neighbor representation mij, the updated representation m(new)
ij
for the next layer is:
241"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4540636042402827,"m(new)
ij
←mij + ∆θmij, ∆θmij := f
 
MHA(Qϕ(mij), Kθ(Mi||hi), Vθ(Mi||hi))

."
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4558303886925795,"Adding neighbor attention on more layers will increase the model capacity, but also cause more risk
242"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.4575971731448763,"of over-smoothing [45], i.e., neighbor tokens all have the same representations. In practice, we add
243"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.45936395759717313,"neighbor attention to less than half of the transformer layers, and leave the last layer untouched for
244"
TRANSFORMER WITH NEIGHBOR ATTENTIONS,0.46113074204946997,"guidance. In continual learning, the optimal layer selections for different tasks may vary.
245"
EXPERIMENTAL SETTINGS,0.4628975265017668,"5
Experimental Settings
246"
EXPERIMENTAL SETTINGS,0.46466431095406363,"Single Task Evaluation for Desiderata We first evaluate the models’ capacities for our desiderata Eq.
247"
EXPERIMENTAL SETTINGS,0.4664310954063604,"(3) and Eq. (4) on single tasks. We test classification accuracies for desiderata of task performance on
248"
EXPERIMENTAL SETTINGS,0.46819787985865724,"tasks from the GLUE benchmark [50] and SNLI data [5]. For the desiderata of global alignment, we
249"
EXPERIMENTAL SETTINGS,0.46996466431095407,"predict top-20 tokens from the learned representation by the pre-trained decoder (global prototypes),
250"
EXPERIMENTAL SETTINGS,0.4717314487632509,"and compute the ratio of rationle tokens in the top-20 predictions (i.e. Recall@20). We evaluate this
251"
EXPERIMENTAL SETTINGS,0.4734982332155477,"on e-SNLI dataset [8], where data’s rationale tokens [5] are highlighted by human annotators.
252"
EXPERIMENTAL SETTINGS,0.4752650176678445,"Continual Learning (CL) Evaluation We evaluate four sequences of tasks: (1) Yahoo 1: a split of
253"
EXPERIMENTAL SETTINGS,0.47703180212014135,"Yahoo dataset for news question-answer categorization [57] with 5 disjoint tasks containing 2 classes
254"
EXPERIMENTAL SETTINGS,0.4787985865724382,"each; (2) Yahoo 2: a Yahoo sequence with the same split as (1) but with more data; (3) DB: a split of
255"
EXPERIMENTAL SETTINGS,0.48056537102473496,"DBPedia data for Wikipedia article classification [57] with 7 disjoint tasks containing 2 classes each;
256"
EXPERIMENTAL SETTINGS,0.4823321554770318,"(4) News Series: a sequence of tasks on news-related data, including AG_news (news classification,
257"
EXPERIMENTAL SETTINGS,0.4840989399293286,"4 classes), MRPC (paraphrase detection, 2 classes) [12], RTE (text entailment, 2 classes) [52] and
258"
EXPERIMENTAL SETTINGS,0.48586572438162545,"SST (sentiment analysis, 2 classes) [47]. For the above sequences except (2), we randomly sample
259"
EXPERIMENTAL SETTINGS,0.4876325088339223,"1245 samples per class, which is the least number of class samples in our datasets. For (2), we
260"
EXPERIMENTAL SETTINGS,0.48939929328621906,"sample 10000 samples per class. We measure the average accuracy and forgetting (Appendix C) with
261"
EXPERIMENTAL SETTINGS,0.4911660777385159,"standard deviations. For each sequence, we test five random orders of tasks.
262"
EXPERIMENTAL SETTINGS,0.4929328621908127,"We evaluate for both task-incremental and class-incremental learning. Task identifiers are available
263"
EXPERIMENTAL SETTINGS,0.49469964664310956,"at inference time for task-incremental learning but not for class-incremental learning [37]. For
264"
EXPERIMENTAL SETTINGS,0.49646643109540634,"class-incremental learning, the original cross-entropy loss over all seen classes will cause significant
265"
EXPERIMENTAL SETTINGS,0.49823321554770317,"forgetting [54, 1]. Since our work does not focus on the problem of cross-entropy, we apply the
266"
EXPERIMENTAL SETTINGS,0.5,"asymmetric strategy (ACE) [7]: the current task’s classification loss is calculated over in-task classes,
267"
EXPERIMENTAL SETTINGS,0.5017667844522968,"while the replay loss is calculated over all seen classes in the memory (if applicable).
268"
EXPERIMENTAL SETTINGS,0.5035335689045937,"Models and CL Frameworks
We compare different adaptation models on BERT-base. Data
269"
EXPERIMENTAL SETTINGS,0.5053003533568905,"representation is from a [MASK] token added to the beginning of input to match the pre-training
270"
EXPERIMENTAL SETTINGS,0.5070671378091873,"format. Models for comparison are: (1) NeiAttn: our standard neighbor attention model. (2) NeiReg:
271"
EXPERIMENTAL SETTINGS,0.508833922261484,"our neighbor attention model with extra regularization for holistic information (Appendix B). (3)
272"
EXPERIMENTAL SETTINGS,0.5106007067137809,"Fine-tuning (FT): a model in which all parameters are learnable. (4) Prompt Tuning (ProT) [30]:
273"
EXPERIMENTAL SETTINGS,0.5123674911660777,"the model adding learnable embeddings only to data inputs. (5) Prefix Tuning v2 (PT2) [33]: an
274"
EXPERIMENTAL SETTINGS,0.5141342756183745,"adaptation model adding learnable embeddings to inputs of all attention layers. (6) Adapter [21]:
275"
EXPERIMENTAL SETTINGS,0.5159010600706714,"an adaptation model with learnable linear projections injected in each layer. (7) BitFit [55]: an
276"
EXPERIMENTAL SETTINGS,0.5176678445229682,"adaptation model tuning only bias terms in the pre-trained model. More settings are in the appendix.
277"
EXPERIMENTAL SETTINGS,0.519434628975265,"We consider different frameworks (methods) for continual learning: (1) Vanilla: the vanilla online
278"
EXPERIMENTAL SETTINGS,0.5212014134275619,"learning framework; (2) MBPA: an episodic memory framework retrieving stored samples to locally
279"
EXPERIMENTAL SETTINGS,0.5229681978798587,"adapt the model at inference time [48]. (3) ER: an episodic memory framework storing all seen
280"
EXPERIMENTAL SETTINGS,0.5247349823321554,"examples and performs sparse (1%) experience replay; (4) A-GEM: an episodic memory framework
281"
EXPERIMENTAL SETTINGS,0.5265017667844523,"constraining on gradients to prevent degrading performance of previous tasks [9]; (5) Probing: a
282"
EXPERIMENTAL SETTINGS,0.5282685512367491,"framework which learns the encoder with Vanilla setting while tunes the classifier for each task
283"
EXPERIMENTAL SETTINGS,0.5300353356890459,"using all task data. This is used to evaluate the discrimination of data representations; (6). MTL: a
284"
EXPERIMENTAL SETTINGS,0.5318021201413428,"muti-task framework that jointly trains on all tasks (not continual learning). For class-incremental
285"
EXPERIMENTAL SETTINGS,0.5335689045936396,"cases, we have the above replay-based methods combined with the ACE strategy. The baseline
286"
EXPERIMENTAL SETTINGS,0.5353356890459364,"performance for each continual learning framework is that on FT model.
287"
EXPERIMENTAL RESULTS,0.5371024734982333,"6
Experimental Results
288"
EXPERIMENTAL RESULTS,0.5388692579505301,"Models for Desiderata in Eq.(3) and Eq.(4)
Figure 3 shows models’ capacities for our desiderata.
289"
EXPERIMENTAL RESULTS,0.5406360424028268,"We compare the classification accuracy for desiderata in Eq.(3) and Recall@20 of rationale tokens for
290"
EXPERIMENTAL RESULTS,0.5424028268551236,"desiderata in Eq.(4). The higher scores on both metrics, the better model capacities for our desiderata.
291"
EXPERIMENTAL RESULTS,0.5441696113074205,"Overall, NeiAttn and PT2 consistently achieve a superior balance between classification and recall
292"
EXPERIMENTAL RESULTS,0.5459363957597173,"scores on different NLI tasks. However, Adapter and FT achieve high classification scores but do
293"
EXPERIMENTAL RESULTS,0.5477031802120141,"not generate representations well related to global prototypes (low recall scores). This supports our
294"
EXPERIMENTAL RESULTS,0.549469964664311,"intuition that mixing pre-trained and modified representations with a gate can result representations
295"
EXPERIMENTAL RESULTS,0.5512367491166078,"better connected to global prototypes. With explicit regularization on holistic information, NeiReg
296"
EXPERIMENTAL RESULTS,0.5530035335689046,"Figure 3: Results for single-task learning. Dashed lines split figure regions based on scores of NeiAttn.
Results with higher accuracy and recall (upper right corner) are better. We test on three random seeds.
Table 1: Results for task-incremental learning. We report average accuracy (Acc) and forgetting
(Forget) with their standard deviations (std) on five random seeds. Bold scores are the best scores and
underline scores are the second best. Models in blue have prototype loss larger than the threshold.
Models in red satisfy the desiderata Eq. (4). Models with (*) are baselines for each CL framework."
EXPERIMENTAL RESULTS,0.5547703180212014,"CL Framework
Model
Yahoo 1
Yahoo 2
DB
News Series"
EXPERIMENTAL RESULTS,0.5565371024734982,"Acc std
Forget std
Acc std
Forget std
Acc std
Forget std
Acc std
Forget std"
EXPERIMENTAL RESULTS,0.558303886925795,"Pretrained
82.95 3.64
7.34 3.64
83.70 4.16
7.71 4.15
95.38 2.34
4.08 2.37
66.66 4.47
5.35 3.06
Vanilla
FT (*)
73.07 5.32
18.67 5.41
79.82 4.29
13.27 4.25
73.15 5.36
24.90 5.17
59.98 8.94
21.13 7.44
Adapter
79.85 1.83
11.86 1.83
71.90 2.45
20.92 2.47
98.70 1.10
1.19 1.10
65.43 4.73
15.53 4.29
PT2
88.62 0.80
3.04 0.79
90.64 0.76
2.38 0.71
99.83 0.04
0.07 0.04
75.03 0.97
6.13 0.98
NeiAttn
88.96 1.14
2.80 1.12
89.84 0.70
3.24 0.69
97.34 3.41
2.54 3.41
71.95 2.20
9.89 2.29 MBPA"
EXPERIMENTAL RESULTS,0.5600706713780919,"FT (*)
72.40 4.42
19.34 4.49
78.71 3.29
14.38 3.26
73.01 5.45
25.04 5.27
60.60 8.30
20.52 6.67
Adapter
78.50 2.12
13.13 2.09
73.66 2.95
19.15 2.95
99.09 1.10
0.80 1.10
65.28 4.74
15.67 4.11
PT2
90.69 0.78
0.97 0.75
91.70 0.51
1.33 0.58
99.90 0.06
-0.01 0.06
76.16 0.81
4.99 1.46
NeiAttn
90.69 1.36
1.67 1.35
91.18 0.90
1.90 0.88
97.53 3.28
2.35 3.28
73.28 2.53
8.56 2.11 ER"
EXPERIMENTAL RESULTS,0.5618374558303887,"FT (*)
70.77 6.72
20.92 6.72
90.31 0.72
2.67 0.67
91.05 8.74
8.75 8.69
70.44 5.87
10.93 4.85
Adapter
77.44 3.39
14.13 3.42
75.79 3.44
17.08 3.39
98.92 1.54
0.97 1.54
68.11 1.96
13.16 2.10
PT2
88.91 0.42
2.76 0.35
91.02 0.50
2.19 0.78
99.84 0.04
0.03 0.03
69.60 3.06
11.58 2.96
NeiAttn
84.02 3.10
7.87 3.12
91.54 0.22
1.52 0.24
99.68 0.18
0.20 0.18
75.05 0.94
7.31 0.48 A-GEM"
EXPERIMENTAL RESULTS,0.5636042402826855,"FT (*)
87.56 1.32
4.11 1.40
89.98 0.71
3.17 0.68
84.45 10.16
15.34 10.12
75.06 6.17
5.48 4.01
Adapter
80.86 2.36
10.65 2.26
77.47 3.20
15.37 3.24
99.52 0.23
0.38 0.24
73.80 1.16
6.72 1.61
PT2
90.40 0.21
1.39 0.16
90.84 0.19
2.22 0.21
99.88 0.01
0.01 0.01
73.31 0.73
4.29 1.02
NeiAttn
90.47 0.26
1.38 0.21
91.35 0.43
1.81 0.47
98.22 3.48
1.66 3.49
77.07 1.56
4.43 0.85"
EXPERIMENTAL RESULTS,0.5653710247349824,"Probing
(classifier non-CL)"
EXPERIMENTAL RESULTS,0.5671378091872792,"FT (*)
90.18 0.41
1.56 0.49
92.16 0.14
0.93 0.14
97.73 3.58
0.31 0.04
77.17 2.09
3.94 1.98
Adapter
91.11 0.25
0.51 0.25
88.98 7.25
3.84 7.28
99.87 0.01
0.02 0.01
78.47 0.76
2.49 1.70
PT2
91.49 0.12
0.17 0.09
92.81 0.11
0.21 0.11
99.89 0.01
0.01 0.01
77.62 0.32
3.53 1.06
NeiAttn
91.47 0.16
0.29 0.16
92.72 0.11
0.37 0.11
99.87 0.01
0.01 0.02
78.83 0.51
3.01 1.02"
EXPERIMENTAL RESULTS,0.568904593639576,"MTL (non-CL)
FT (*)
91.69 0.26
—
92.67 0.71
—
99.61 0.41
—
79.67 1.99
—"
EXPERIMENTAL RESULTS,0.5706713780918727,"performs best in in-task (SNLI→E-SNLI) rationale recalls, while losing its superiority in cross-task
297"
EXPERIMENTAL RESULTS,0.5724381625441696,"(GLUE→E-SNLI) rationale recalls. This may suggest the explicit regularization may not generalize
298"
EXPERIMENTAL RESULTS,0.5742049469964664,"well across tasks. With prompts only in the input, ProT has insufficient capacity for task performance.
299"
EXPERIMENTAL RESULTS,0.5759717314487632,"For desiderata Eq.(4), NeiAttn and PT2 perform much better than Adapter and FT. We set aτ to make
300"
EXPERIMENTAL RESULTS,0.5777385159010601,"NeiAttn and PT2 satisfy Eq.(4) while Adapter and FT fail to, then we evaluate them for CL scenarios.
301"
EXPERIMENTAL RESULTS,0.5795053003533569,"Task-Incremental Learning We test models’ capacities for task-incremental learning under different
302"
EXPERIMENTAL RESULTS,0.5812720848056537,"CL frameworks. Results are shown in Table 1. Models are split into two categories according to our
303"
EXPERIMENTAL RESULTS,0.5830388692579506,"desiderata (Eq.(4)) experiment above: (NeiAttn, PT2) which satisfy it and (FT, Adapters) in opposite.
304"
EXPERIMENTAL RESULTS,0.5848056537102474,"In the vanilla setting, both PT2 and NeiAttn significantly outperform other models with minor
305"
EXPERIMENTAL RESULTS,0.5865724381625441,"forgetting. Adapter on most CL frameworks performs worse than PT2 and NeiAttn, marginally
306"
EXPERIMENTAL RESULTS,0.588339222614841,"better than FT. This supports our claim that models learning representations better connected to
307"
EXPERIMENTAL RESULTS,0.5901060070671378,"global prototypes perform better in continual learning. Combined with ER and A-GEM, NeiAttn can
308"
EXPERIMENTAL RESULTS,0.5918727915194346,"improve more than PT2 in most cases. FT has significant improvement with replay but can also suffer
309"
EXPERIMENTAL RESULTS,0.5936395759717314,"from overfitting to the replay buffer (ER for Yahoo 1). We also evaluate on a probing framework with
310"
EXPERIMENTAL RESULTS,0.5954063604240283,"only the classifier retrained over task data to evaluate whether the forgetting will cause representations
311"
EXPERIMENTAL RESULTS,0.5971731448763251,"to lose separation. PT2 and NeiAttn also preserve the most separation of representations in this case.
312"
EXPERIMENTAL RESULTS,0.598939929328622,"In general, (NeiAttn, PT2) consistently outperform (FT, Adapter) under different CL frameworks.
313"
EXPERIMENTAL RESULTS,0.6007067137809188,"This supports that our desiderata Eq. (4) helps improve models’ continual learning ability. NeiAttn
314"
EXPERIMENTAL RESULTS,0.6024734982332155,"performs better with replay. The capacity of models also depends on different data distributions in
315"
EXPERIMENTAL RESULTS,0.6042402826855123,"the sequence. On News Series, when with replay, FT can even outperform PT2. This may happen
316"
EXPERIMENTAL RESULTS,0.6060070671378092,"because News Series includes data from similar distributions related to the news. And models should
317"
EXPERIMENTAL RESULTS,0.607773851590106,"have the capacity to deal with knowledge transfer besides catastrophic forgetting.
318"
EXPERIMENTAL RESULTS,0.6095406360424028,"Figure 4: Results on class incremental learning. Dashed lines
show scores of a pre-trained model in the vanilla setting."
EXPERIMENTAL RESULTS,0.6113074204946997,"Results
for
Class-Incremental
319"
EXPERIMENTAL RESULTS,0.6130742049469965,"Learning
Figure 4 shows models’
320"
EXPERIMENTAL RESULTS,0.6148409893992933,"performance on class-incremental
321"
EXPERIMENTAL RESULTS,0.6166077738515902,"learning. PT2 and NeiAttn perform
322"
EXPERIMENTAL RESULTS,0.6183745583038869,"well in the vanilla case, where the
323"
EXPERIMENTAL RESULTS,0.6201413427561837,"training is the same as that for task-
324"
EXPERIMENTAL RESULTS,0.6219081272084805,"incremental learning. This indicates
325"
EXPERIMENTAL RESULTS,0.6236749116607774,"that they can address connections
326"
EXPERIMENTAL RESULTS,0.6254416961130742,"between classes from different tasks
327"
EXPERIMENTAL RESULTS,0.627208480565371,"even without supervision.
On the
328"
EXPERIMENTAL RESULTS,0.6289752650176679,"other side, Adapter and FT perform much worse in this case. Then we evaluate three frameworks
329"
EXPERIMENTAL RESULTS,0.6307420494699647,"with replay: one is the full ER-ACE [7] with experience replay at each step; one is the ER-ACE
330"
EXPERIMENTAL RESULTS,0.6325088339222615,"(sparse) with sparse experience replay; the other is the ACE strategy with only previous task’s data
331"
EXPERIMENTAL RESULTS,0.6342756183745583,"stored in the replay (AGEM-ACE). We observe that performance on class-incremental learning
332"
EXPERIMENTAL RESULTS,0.6360424028268551,"heavily relies on the quality of replay. In most cases, FT, Adapter and NeiAttn can benefit more from
333"
EXPERIMENTAL RESULTS,0.6378091872791519,"the replay. We hypothesize that it is related to the fast adaptation ability related to linear projections.
334"
EXPERIMENTAL RESULTS,0.6395759717314488,"Table 2: The ratio of models’ learnable
parameters compared to FT."
EXPERIMENTAL RESULTS,0.6413427561837456,"Models
FT Bitfit Adapter ProT PT2 NeiAttn"
EXPERIMENTAL RESULTS,0.6431095406360424,"Parameters (%)
1
0.5
2.3
0.5
0.8
4.9"
EXPERIMENTAL RESULTS,0.6448763250883393,"Influence of Parameter-Efficiency With limited param-
335"
EXPERIMENTAL RESULTS,0.6466431095406361,"eters, adaptation models have less risk of deviating fast
336"
EXPERIMENTAL RESULTS,0.6484098939929329,"from previously learned knowledge compared to FT, and
337"
EXPERIMENTAL RESULTS,0.6501766784452296,"thus may perform better in CL. However, different models’
338"
EXPERIMENTAL RESULTS,0.6519434628975265,"improvements come not just from having fewer trainable
339"
EXPERIMENTAL RESULTS,0.6537102473498233,"parameters. Table 2 shows the comparison of parameters in each model. NeiAttn has better perfor-
340"
EXPERIMENTAL RESULTS,0.6554770318021201,"mance in most cases compared to Adapter and Pre-trained models, which have fewer or no trainable
341"
EXPERIMENTAL RESULTS,0.657243816254417,"parameters in the encoder. Even with more parameters, NeiAttn performs on par with PT2 with
342"
EXPERIMENTAL RESULTS,0.6590106007067138,"Vanilla and outperform PT2 with replay. NeiAttn also requires much less time to train (5 vs 20
343"
EXPERIMENTAL RESULTS,0.6607773851590106,"epochs). These suggest the adaptation model structure will highly influence its performance on CL.
344"
EXPERIMENTAL RESULTS,0.6625441696113075,"Figure 5: T-SNE plot of FT, NeiAttn rep-
resentations. Triangles are class vectors."
EXPERIMENTAL RESULTS,0.6643109540636042,"Visualization of Representations
In Figure 5, we vi-
345"
EXPERIMENTAL RESULTS,0.666077738515901,"sualize NeiAttn and FT’s data representations for class-
346"
EXPERIMENTAL RESULTS,0.6678445229681979,"incremental DB under Vanilla and ER-ACE frameworks.
347"
EXPERIMENTAL RESULTS,0.6696113074204947,"Even trained with in-task classes, Vanilla NeiAttn can well
348"
EXPERIMENTAL RESULTS,0.6713780918727915,"disperse data representations. Learning a model includes
349"
EXPERIMENTAL RESULTS,0.6731448763250883,"learning the encoder (representations) and classifier (class
350"
EXPERIMENTAL RESULTS,0.6749116607773852,"vectors). The learned class vectors may not well align with
351"
EXPERIMENTAL RESULTS,0.676678445229682,"representations even with replay (left bottom). We hypoth-
352"
EXPERIMENTAL RESULTS,0.6784452296819788,"esize this may result from different training paces for the
353"
EXPERIMENTAL RESULTS,0.6802120141342756,"encoder and classifier. For FT, the encoder quickly learns
354"
EXPERIMENTAL RESULTS,0.6819787985865724,"representations close to single class centroids, which may
355"
EXPERIMENTAL RESULTS,0.6837455830388692,"degrade the function of the classifier. However, with con-
356"
EXPERIMENTAL RESULTS,0.6855123674911661,"nections to multiple different global prototypes, NeiAttn representations may not quickly move to
357"
EXPERIMENTAL RESULTS,0.6872791519434629,"one centroid. Therefore, it can better balance the training of the encoder and classifier (right bottom).
358"
CONCLUSION,0.6890459363957597,"7
Conclusion
359"
CONCLUSION,0.6908127208480566,"In this paper, we investigate models which consider potential connections between observed and
360"
CONCLUSION,0.6925795053003534,"unknown tasks to reduce disruptive updating in CL. Specifically, we learn task-specific data repre-
361"
CONCLUSION,0.6943462897526502,"sentations appropriately connected to a general-purpose representation base with global prototypes.
362"
CONCLUSION,0.696113074204947,"For NLP tasks, the global prototypes can be obtained from a pre-trained language model. And the
363"
CONCLUSION,0.6978798586572438,"representation connected to global prototypes can be obtained by lightly adapting the pre-trained
364"
CONCLUSION,0.6996466431095406,"model. We investigate existing adaptation models and propose a neighbor attention model which
365"
CONCLUSION,0.7014134275618374,"combines advantages of existing models. Experimental results show that models learning representa-
366"
CONCLUSION,0.7031802120141343,"tions appropriately connected to global prototypes have significantly less catastrophic forgetting in
367"
CONCLUSION,0.7049469964664311,"CL, even without using experience replay. Specifically, when neighbor attention is used, we suffer
368"
CONCLUSION,0.7067137809187279,"from less catastrophic forgetting than FT and Adapter, and surpass PT2 when experience replay is
369"
CONCLUSION,0.7084805653710248,"applied. We consider the main limitations of our work as: (1) requiring extra memory to compute
370"
CONCLUSION,0.7102473498233216,"neighbor attentions; (2) the optimal number of neighbor attention layers may vary for different tasks.
371"
REFERENCES,0.7120141342756183,"References
372"
REFERENCES,0.7137809187279152,"[1] Ahn, H., Kwak, J., Lim, S., Bang, H., Kim, H., and Moon, T. Ss-il: Separated softmax for
373"
REFERENCES,0.715547703180212,"incremental learning. In Proceedings of the IEEE/CVF International conference on computer
374"
REFERENCES,0.7173144876325088,"vision, pp. 844–853, 2021.
375"
REFERENCES,0.7190812720848057,"[2] Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T. Memory aware
376"
REFERENCES,0.7208480565371025,"synapses: Learning what (not) to forget. In Proceedings of the European Conference on
377"
REFERENCES,0.7226148409893993,"Computer Vision (ECCV), pp. 139–154, 2018.
378"
REFERENCES,0.7243816254416962,"[3] Biesialska, M., Biesialska, K., and Costa-jussà, M. R. Continual lifelong learning in natural
379"
REFERENCES,0.726148409893993,"language processing: A survey. In Proceedings of the 28th International Conference on Compu-
380"
REFERENCES,0.7279151943462897,"tational Linguistics, pp. 6523–6541, Barcelona, Spain (Online), December 2020. International
381"
REFERENCES,0.7296819787985865,"Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.574. URL
382"
REFERENCES,0.7314487632508834,"https://www.aclweb.org/anthology/2020.coling-main.574.
383"
REFERENCES,0.7332155477031802,"[4] Biondi, N., Pernici, F., Bruni, M., Mugnai, D., and Bimbo, A. D. Cl2r: Compatible lifelong
384"
REFERENCES,0.734982332155477,"learning representations. ACM Transactions on Multimedia Computing, Communications and
385"
REFERENCES,0.7367491166077739,"Applications, 18(2s):1–22, 2023.
386"
REFERENCES,0.7385159010600707,"[5] Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning
387"
REFERENCES,0.7402826855123675,"natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in
388"
REFERENCES,0.7420494699646644,"Natural Language Processing, pp. 632–642, Lisbon, Portugal, September 2015. Association for
389"
REFERENCES,0.7438162544169611,"Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://www.aclweb.org/
390"
REFERENCES,0.7455830388692579,"anthology/D15-1075.
391"
REFERENCES,0.7473498233215548,"[6] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,
392"
REFERENCES,0.7491166077738516,"Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint
393"
REFERENCES,0.7508833922261484,"arXiv:2005.14165, 2020.
394"
REFERENCES,0.7526501766784452,"[7] Caccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau, J., and Belilovsky, E. New in-
395"
REFERENCES,0.7544169611307421,"sights on reducing abrupt representation change in online continual learning. arXiv preprint
396"
REFERENCES,0.7561837455830389,"arXiv:2203.03798, 2022.
397"
REFERENCES,0.7579505300353356,"[8] Camburu, O.-M., Rocktäschel, T., Lukasiewicz, T., and Blunsom, P. e-snli: Natural language
398"
REFERENCES,0.7597173144876325,"inference with natural language explanations. In Bengio, S., Wallach, H., Larochelle, H.,
399"
REFERENCES,0.7614840989399293,"Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information
400"
REFERENCES,0.7632508833922261,"Processing Systems, volume 31. Curran Associates, Inc., 2018.
401"
REFERENCES,0.765017667844523,"[9] Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efficient lifelong learning
402"
REFERENCES,0.7667844522968198,"with a-GEM. In International Conference on Learning Representations, 2019. URL https:
403"
REFERENCES,0.7685512367491166,"//openreview.net/forum?id=Hkf2_sC5FX.
404"
REFERENCES,0.7703180212014135,"[10] d’Autume, C. d. M., Ruder, S., Kong, L., and Yogatama, D. Episodic memory in lifelong
405"
REFERENCES,0.7720848056537103,"language learning. arXiv preprint arXiv:1906.01076, 2019.
406"
REFERENCES,0.773851590106007,"[11] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional
407"
REFERENCES,0.7756183745583038,"transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
408"
REFERENCES,0.7773851590106007,"[12] Dolan, B. and Brockett, C. Automatically constructing a corpus of sentential paraphrases. In
409"
REFERENCES,0.7791519434628975,"Third International Workshop on Paraphrasing (IWP2005), 2005.
410"
REFERENCES,0.7809187279151943,"[13] Ermis, B., Zappella, G., Wistuba, M., Rawal, A., and Archambeau, C. Memory efficient
411"
REFERENCES,0.7826855123674912,"continual learning with transformers. Advances in Neural Information Processing Systems, 35:
412"
REFERENCES,0.784452296819788,"10629–10642, 2022.
413"
REFERENCES,0.7862190812720848,"[14] Fini, E., Da Costa, V. G. T., Alameda-Pineda, X., Ricci, E., Alahari, K., and Mairal, J. Self-
414"
REFERENCES,0.7879858657243817,"supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on
415"
REFERENCES,0.7897526501766784,"Computer Vision and Pattern Recognition, pp. 9621–9630, 2022.
416"
REFERENCES,0.7915194346289752,"[15] Gomez-Villa, A., Twardowski, B., Yu, L., Bagdanov, A. D., and van de Weijer, J. Continually
417"
REFERENCES,0.7932862190812721,"learning self-supervised representations with projected functional regularization. In Proceedings
418"
REFERENCES,0.7950530035335689,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3867–3877,
419"
REFERENCES,0.7968197879858657,"2022.
420"
REFERENCES,0.7985865724381626,"[16] Guo, Y., Liu, B., and Zhao, D. Online continual learning through mutual information maximiza-
421"
REFERENCES,0.8003533568904594,"tion. In International Conference on Machine Learning, pp. 8109–8126. PMLR, 2022.
422"
REFERENCES,0.8021201413427562,"[17] Gurbuz, M. B. and Dovrolis, C. Nispa: Neuro-inspired stability-plasticity adaptation for
423"
REFERENCES,0.803886925795053,"continual learning in sparse networks. arXiv preprint arXiv:2206.09117, 2022.
424"
REFERENCES,0.8056537102473498,"[18] He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of
425"
REFERENCES,0.8074204946996466,"parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.
426"
REFERENCES,0.8091872791519434,"[19] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In
427"
REFERENCES,0.8109540636042403,"Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778,
428"
REFERENCES,0.8127208480565371,"2016.
429"
REFERENCES,0.8144876325088339,"[20] Hou, S., Pan, X., Loy, C. C., Wang, Z., and Lin, D. Learning a unified classifier incrementally
430"
REFERENCES,0.8162544169611308,"via rebalancing. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern
431"
REFERENCES,0.8180212014134276,"Recognition, pp. 831–839, 2019.
432"
REFERENCES,0.8197879858657244,"[21] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A.,
433"
REFERENCES,0.8215547703180212,"Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International
434"
REFERENCES,0.823321554770318,"Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.
435"
REFERENCES,0.8250883392226148,"[22] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora:
436"
REFERENCES,0.8268551236749117,"Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
437"
REFERENCES,0.8286219081272085,"[23] Huang, Y., Zhang, Y., Chen, J., Wang, X., and Yang, D. Continual learning for text classification
438"
REFERENCES,0.8303886925795053,"with information disentanglement based regularization. arXiv preprint arXiv:2104.05489, 2021.
439"
REFERENCES,0.8321554770318021,"[24] Javed, K. and White, M. Meta-learning representations for continual learning. Advances in
440"
REFERENCES,0.833922261484099,"Neural Information Processing Systems, 32, 2019.
441"
REFERENCES,0.8356890459363958,"[25] Ke, Z., Liu, B., Ma, N., Xu, H., and Shu, L. Achieving forgetting prevention and knowledge
442"
REFERENCES,0.8374558303886925,"transfer in continual learning. Advances in Neural Information Processing Systems, 34:22443–
443"
REFERENCES,0.8392226148409894,"22456, 2021.
444"
REFERENCES,0.8409893992932862,"[26] Kemker, R. and Kanan, C. Fearnet: Brain-inspired model for incremental learning. arXiv
445"
REFERENCES,0.842756183745583,"preprint arXiv:1711.10563, 2017.
446"
REFERENCES,0.8445229681978799,"[27] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K.,
447"
REFERENCES,0.8462897526501767,"Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in
448"
REFERENCES,0.8480565371024735,"neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.
449"
REFERENCES,0.8498233215547704,"[28] Knoblauch, J., Husain, H., and Diethe, T. Optimal continual learning has perfect memory and is
450"
REFERENCES,0.8515901060070671,"NP-hard. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on
451"
REFERENCES,0.8533568904593639,"Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5327–5337.
452"
REFERENCES,0.8551236749116607,"PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/knoblauch20a.
453"
REFERENCES,0.8568904593639576,"html.
454"
REFERENCES,0.8586572438162544,"[29] Lee, S.-W., Kim, J.-H., Jun, J., Ha, J.-W., and Zhang, B.-T. Overcoming catastrophic forgetting
455"
REFERENCES,0.8604240282685512,"by incremental moment matching. arXiv preprint arXiv:1703.08475, 2017.
456"
REFERENCES,0.8621908127208481,"[30] Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt
457"
REFERENCES,0.8639575971731449,"tuning. arXiv preprint arXiv:2104.08691, 2021.
458"
REFERENCES,0.8657243816254417,"[31] Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv
459"
REFERENCES,0.8674911660777385,"preprint arXiv:2101.00190, 2021.
460"
REFERENCES,0.8692579505300353,"[32] Liu, T., Ungar, L., and Sedoc, J. Continual learning for sentence representations using conceptors.
461"
REFERENCES,0.8710247349823321,"arXiv preprint arXiv:1904.09187, 2019.
462"
REFERENCES,0.872791519434629,"[33] Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J. P-tuning v2: Prompt tuning can be
463"
REFERENCES,0.8745583038869258,"comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602,
464"
REFERENCES,0.8763250883392226,"2021.
465"
REFERENCES,0.8780918727915195,"[34] Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. Gpt understands, too.
466"
REFERENCES,0.8798586572438163,"arXiv preprint arXiv:2103.10385, 2021.
467"
REFERENCES,0.8816254416961131,"[35] Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. Advances in
468"
REFERENCES,0.8833922261484098,"neural information processing systems, 30, 2017.
469"
REFERENCES,0.8851590106007067,"[36] Madaan, D., Yoon, J., Li, Y., Liu, Y., and Hwang, S. J. Representational continuity for
470"
REFERENCES,0.8869257950530035,"unsupervised continual learning. In International Conference on Learning Representations,
471"
REFERENCES,0.8886925795053003,"2021.
472"
REFERENCES,0.8904593639575972,"[37] Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A. D., and van de Weijer, J.
473"
REFERENCES,0.892226148409894,"Class-incremental learning: survey and performance evaluation on image classification. IEEE
474"
REFERENCES,0.8939929328621908,"Transactions on Pattern Analysis and Machine Intelligence, 2022.
475"
REFERENCES,0.8957597173144877,"[38] Pernici, F., Bruni, M., Baecchi, C., and Del Bimbo, A. Fix your features: Stationary and
476"
REFERENCES,0.8975265017667845,"maximally discriminative embeddings using regular polytope (fixed classifier) networks. arXiv
477"
REFERENCES,0.8992932862190812,"preprint arXiv:1902.10441, 2019.
478"
REFERENCES,0.901060070671378,"[39] Pernici, F., Bruni, M., Baecchi, C., Turchini, F., and Del Bimbo, A. Class-incremental learn-
479"
REFERENCES,0.9028268551236749,"ing with pre-allocated fixed classifiers. In 2020 25th International Conference on Pattern
480"
REFERENCES,0.9045936395759717,"Recognition (ICPR), pp. 6259–6266. IEEE, 2021.
481"
REFERENCES,0.9063604240282686,"[40] Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., and Gurevych, I. AdapterFusion: Non-destructive
482"
REFERENCES,0.9081272084805654,"task composition for transfer learning. In Proceedings of the 16th Conference of the European
483"
REFERENCES,0.9098939929328622,"Chapter of the Association for Computational Linguistics: Main Volume, pp. 487–503, Online,
484"
REFERENCES,0.911660777385159,"April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.39.
485"
REFERENCES,0.9134275618374559,"[41] Radiya-Dixit, E. and Wang, X. How fine can fine-tuning be? learning efficient language
486"
REFERENCES,0.9151943462897526,"models. In Chiappa, S. and Calandra, R. (eds.), Proceedings of the Twenty Third International
487"
REFERENCES,0.9169611307420494,"Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine
488"
REFERENCES,0.9187279151943463,"Learning Research, pp. 2435–2443. PMLR, 26–28 Aug 2020.
489"
REFERENCES,0.9204946996466431,"[42] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and
490"
REFERENCES,0.9222614840989399,"Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv
491"
REFERENCES,0.9240282685512368,"preprint arXiv:1910.10683, 2019.
492"
REFERENCES,0.9257950530035336,"[43] Razdaibiedina, A., Mao, Y., Hou, R., Khabsa, M., Lewis, M., and Almahairi, A. Progressive
493"
REFERENCES,0.9275618374558304,"prompts: Continual learning for language models. arXiv preprint arXiv:2301.12314, 2023.
494"
REFERENCES,0.9293286219081273,"[44] Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K.,
495"
REFERENCES,0.931095406360424,"Pascanu, R., and Hadsell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671,
496"
REFERENCES,0.9328621908127208,"2016.
497"
REFERENCES,0.9346289752650176,"[45] Shi, H., GAO, J., Xu, H., Liang, X., Li, Z., Kong, L., Lee, S. M. S., and Kwok, J. Revisiting over-
498"
REFERENCES,0.9363957597173145,"smoothing in BERT from the perspective of graph. In International Conference on Learning
499"
REFERENCES,0.9381625441696113,"Representations, 2022.
500"
REFERENCES,0.9399293286219081,"[46] Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learning with deep generative replay. arXiv
501"
REFERENCES,0.941696113074205,"preprint arXiv:1705.08690, 2017.
502"
REFERENCES,0.9434628975265018,"[47] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive
503"
REFERENCES,0.9452296819787986,"deep models for semantic compositionality over a sentiment treebank. In Proceedings of the
504"
REFERENCES,0.9469964664310954,"2013 conference on empirical methods in natural language processing, pp. 1631–1642, 2013.
505"
REFERENCES,0.9487632508833922,"[48] Sprechmann, P., Jayakumar, S. M., Rae, J. W., Pritzel, A., Badia, A. P., Uria, B., Vinyals, O.,
506"
REFERENCES,0.950530035335689,"Hassabis, D., Pascanu, R., and Blundell, C. Memory-based parameter adaptation. arXiv preprint
507"
REFERENCES,0.9522968197879859,"arXiv:1802.10542, 2018.
508"
REFERENCES,0.9540636042402827,"[49] Sun, F.-K., Ho, C.-H., and Lee, H.-Y. Lamol: Language modeling for lifelong language learning.
509"
REFERENCES,0.9558303886925795,"arXiv preprint arXiv:1909.03329, 2019.
510"
REFERENCES,0.9575971731448764,"[50] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-
511"
REFERENCES,0.9593639575971732,"task benchmark and analysis platform for natural language understanding. In International
512"
REFERENCES,0.9611307420494699,"Conference on Learning Representations, 2019. URL https://openreview.net/forum?
513"
REFERENCES,0.9628975265017667,"id=rJ4km2R5t7.
514"
REFERENCES,0.9646643109540636,"[51] Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy, J., and Pfister,
515"
REFERENCES,0.9664310954063604,"T. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on
516"
REFERENCES,0.9681978798586572,"Computer Vision and Pattern Recognition, pp. 139–149, 2022.
517"
REFERENCES,0.9699646643109541,"[52] Williams, A., Nangia, N., and Bowman, S. A broad-coverage challenge corpus for sentence
518"
REFERENCES,0.9717314487632509,"understanding through inference. In Proceedings of the 2018 Conference of the North American
519"
REFERENCES,0.9734982332155477,"Chapter of the Association for Computational Linguistics: Human Language Technologies,
520"
REFERENCES,0.9752650176678446,"Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for
521"
REFERENCES,0.9770318021201413,"Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.org/
522"
REFERENCES,0.9787985865724381,"anthology/N18-1101.
523"
REFERENCES,0.980565371024735,"[53] Yoon, J., Yang, E., Lee, J., and Hwang, S. J. Lifelong learning with dynamically expandable
524"
REFERENCES,0.9823321554770318,"networks. arXiv preprint arXiv:1708.01547, 2017.
525"
REFERENCES,0.9840989399293286,"[54] Yu, L., Twardowski, B., Liu, X., Herranz, L., Wang, K., Cheng, Y., Jui, S., and Weijer, J. v. d.
526"
REFERENCES,0.9858657243816255,"Semantic drift compensation for class-incremental learning. In Proceedings of the IEEE/CVF
527"
REFERENCES,0.9876325088339223,"conference on computer vision and pattern recognition, pp. 6982–6991, 2020.
528"
REFERENCES,0.9893992932862191,"[55] Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning for
529"
REFERENCES,0.991166077738516,"transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.
530"
REFERENCES,0.9929328621908127,"[56] Zenke, F., Poole, B., and Ganguli, S. Continual learning through synaptic intelligence. In
531"
REFERENCES,0.9946996466431095,"International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.
532"
REFERENCES,0.9964664310954063,"[57] Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification.
533"
REFERENCES,0.9982332155477032,"Advances in neural information processing systems, 28, 2015.
534"
