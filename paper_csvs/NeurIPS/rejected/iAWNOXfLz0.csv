Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019267822736030828,"Time series anomaly detection is a task that determines whether an unseen signal is
1"
ABSTRACT,0.0038535645472061657,"normal or abnormal, and it is a crucial function in various real-world applications.
2"
ABSTRACT,0.005780346820809248,"Typical approach is to learn normal data representation using generative models,
3"
ABSTRACT,0.007707129094412331,"like Generative Adversarial Network (GAN), to discriminate between normal and
4"
ABSTRACT,0.009633911368015413,"abnormal signals. Recently, a few studies actively adopt transformer to model
5"
ABSTRACT,0.011560693641618497,"time series data, but there is no transformer-based GAN framework for time
6"
ABSTRACT,0.01348747591522158,"series anomaly detection. As a pioneer work, we propose a new transformer-
7"
ABSTRACT,0.015414258188824663,"based GAN framework, called AnoFormer, and its effective training strategy for
8"
ABSTRACT,0.017341040462427744,"better representation learning. Specifically, we improve the detection ability of
9"
ABSTRACT,0.019267822736030827,"our model by introducing two-step masking strategies. The first step is Random
10"
ABSTRACT,0.02119460500963391,"masking: we design a random mask pool to hide parts of the signal randomly. This
11"
ABSTRACT,0.023121387283236993,"allows our model to learn the representation of normal data. The second step is
12"
ABSTRACT,0.025048169556840076,"Exclusive and Entropy-based Re-masking: we propose a novel refinement step
13"
ABSTRACT,0.02697495183044316,"to provide feedback to accurately model the exclusive and uncertain parts in the
14"
ABSTRACT,0.028901734104046242,"first step. We empirically demonstrate the effectiveness of re-masking step that
15"
ABSTRACT,0.030828516377649325,"our model generates more normal-like signals robustly. Extensive experiments on
16"
ABSTRACT,0.03275529865125241,"various datasets show that AnoFormer significantly outperforms the state-of-the-art
17"
ABSTRACT,0.03468208092485549,"methods in time series anomaly detection.
18"
INTRODUCTION,0.036608863198458574,"1
Introduction
19"
INTRODUCTION,0.038535645472061654,"Time series anomaly detection is a crucial technology to prevent potential risks and financial losses
20"
INTRODUCTION,0.04046242774566474,"in a variety of areas, such as detecting anomalies on sensor data of large-scale plants [1], ECG
21"
INTRODUCTION,0.04238921001926782,"monitoring [2], and the network traffic analysis [3]. To deal with this task, from the classic methods
22"
INTRODUCTION,0.04431599229287091,"[4, 5, 6] to the recent deep learning-based methods [2, 7, 8, 9, 10, 11, 12, 13], many studies have
23"
INTRODUCTION,0.046242774566473986,"focused on unsupervised learning methods due to the lack of labeled anomalies and highly nonlinear
24"
INTRODUCTION,0.04816955684007707,"temporal dependencies.
25"
INTRODUCTION,0.05009633911368015,"One of major deep learning-based approaches is a reconstruction-based method. It typically uses an
26"
INTRODUCTION,0.05202312138728324,"autoencoder (AE) or Generative Adversarial Network (GAN) to learn the representation of normal
27"
INTRODUCTION,0.05394990366088632,"data and to reconstruct a normal-like signal from an input always. As a backbone network, the
28"
INTRODUCTION,0.055876685934489405,"existing studies widely utilize CNN (Convolutional Neural Networks) [2] or RNN (Recurrent Neural
29"
INTRODUCTION,0.057803468208092484,"Networks) [8, 9, 10]. More recently, there have been attempts to apply transformer [14] to time
30"
INTRODUCTION,0.05973025048169557,"series anomaly detection, and it shows remarkable performances [11]. In this work, we also adopt
31"
INTRODUCTION,0.06165703275529865,"transformer to embed time series representation, but design an adversarial framework for anomaly
32"
INTRODUCTION,0.06358381502890173,"detection.
33"
INTRODUCTION,0.06551059730250482,"If we devise GAN using a transformer encoder, we expect that the model learns normal time series
34"
INTRODUCTION,0.0674373795761079,"data and eventually generates real normal-like signals. However, there is a major issue. Unlike the
35"
INTRODUCTION,0.06936416184971098,"AE structure, a pure transformer encoder-based generator does not have a compressed latent space,
36"
INTRODUCTION,0.07129094412331406,"i.e., it makes the model find the trivial solution, just copying an input and pasting to the output for the
37"
INTRODUCTION,0.07321772639691715,"reconstruction. Therefore, we need a new training method for the generator to learn the distribution
38"
INTRODUCTION,0.07514450867052024,"of normal time series data. To address this issue, we introduce a novel two-step masking strategy.
39"
INTRODUCTION,0.07707129094412331,"From this approach, the next question is where to mask an input signal to detect anomalies effectively.
40"
INTRODUCTION,0.0789980732177264,"Understandably, in order to make the normal-like output, the best masking positions are abnormal
41"
INTRODUCTION,0.08092485549132948,"points in the input signal. It is a challenging to mask the abnormal areas selectively because we do
42"
INTRODUCTION,0.08285163776493257,"not know where the abnormal parts are in advance.
43"
INTRODUCTION,0.08477842003853564,"In this paper, we propose AnoFormer, which is a novel transformer-based GAN utilizing a pure
44"
INTRODUCTION,0.08670520231213873,"transformer encoder only. To learn data representation effectively, we adopt a masking strategy. We
45"
INTRODUCTION,0.08863198458574181,"first train transformer-based GAN with random masking (Step 1) for representation learning of the
46"
INTRODUCTION,0.0905587668593449,"normal time series data. While filling the randomly masked parts of the input at Step 1, the model
47"
INTRODUCTION,0.09248554913294797,"learns the distribution of normal data effectively. In Step 1 alone, all parts of the input signal cannot
48"
INTRODUCTION,0.09441233140655106,"be considered, and this randomness is a big problem in anomaly detection. Therefore, we solve this
49"
INTRODUCTION,0.09633911368015415,"problem by re-masking the exclusive parts of Step 1. Also, to find the best masking positions, we
50"
INTRODUCTION,0.09826589595375723,"calculate entropy from the attention maps of transformer blocks and re-mask the parts with high
51"
INTRODUCTION,0.1001926782273603,"entropy that is likely to be abnormal points with high uncertainty. This exclusive and entropy-based
52"
INTRODUCTION,0.10211946050096339,"re-masking (Step 2) provides feedback for better representation learning, eventually improving the
53"
INTRODUCTION,0.10404624277456648,"anomaly detection performance. We experimentally prove that the proposed two-step masking is
54"
INTRODUCTION,0.10597302504816955,"essential for AnoFormer to solve anomaly detection problem successfully.
55"
INTRODUCTION,0.10789980732177264,"Our contributions can be summarized as follows:
56"
INTRODUCTION,0.10982658959537572,"‚Ä¢ We propose a simple yet effective transformer-based GAN framework having a generator
57"
INTRODUCTION,0.11175337186897881,"and a discriminator for unsupervised time series anomaly detection, called AnoFormer.
58"
INTRODUCTION,0.11368015414258188,"Moreover, we present pre-processing and embedding methods for our framework to deal
59"
INTRODUCTION,0.11560693641618497,"with time series data effectively.
60"
INTRODUCTION,0.11753371868978806,"‚Ä¢ We introduce a new two-step masking method to encode the distribution of normal time
61"
INTRODUCTION,0.11946050096339114,"series data. A newly proposed entropy-based re-masking helps our model to provide
62"
INTRODUCTION,0.12138728323699421,"the feedback to the uncertain parts based on entropy. From the extensive ablations, we
63"
INTRODUCTION,0.1233140655105973,"empirically verify that our two-step masking makes our model robust and successfully
64"
INTRODUCTION,0.1252408477842004,"embed the representation of normal time series data.
65"
INTRODUCTION,0.12716763005780346,"‚Ä¢ AnoFormer achieves new state-of-the-art results with significant improvements on various
66"
INTRODUCTION,0.12909441233140656,"unsupervised time series anomaly detection datasets: NeurIPS-TS, MIT-BIH, 2D-gesture,
67"
INTRODUCTION,0.13102119460500963,"and Power-demand.
68"
RELATED WORK,0.1329479768786127,"2
Related Work
69"
RELATED WORK,0.1348747591522158,"Generative models using transformer have been proposed and applied to diverse domains, e.g.,
70"
RELATED WORK,0.13680154142581888,"computer vision [15, 16, 17, 18], natural language processing [19, 20], and sequence modeling
71"
RELATED WORK,0.13872832369942195,"[21, 22]. In particular, these models are used to solve various tasks in the image domain, such
72"
RELATED WORK,0.14065510597302505,"as scene generation [15, 16, 23], saliency prediction [18], semantic segmentation [24], and sketch
73"
RELATED WORK,0.14258188824662812,"synthesis [25]. Moreover, transformer is presented to solve graph-to-sequence transduction task
74"
RELATED WORK,0.14450867052023122,"using graph neural network [26], text generation task [27], and time series forecasting task with
75"
RELATED WORK,0.1464354527938343,"the modified self-attention mechanism [28]. We also utilize transformer to construct a generative
76"
RELATED WORK,0.14836223506743737,"framework, i.e., having both a generator and a discriminator. In this framework, we propose an
77"
RELATED WORK,0.15028901734104047,"appropriate embedding method and loss form to effectively solve the anomaly detection problems.
78"
RELATED WORK,0.15221579961464354,"Many studies have used masking to the transformer architecture for effective representation learning.
79"
RELATED WORK,0.15414258188824662,"Including BERT [29], which proposes the Masked Language Model (MLM) technique to pretrain
80"
RELATED WORK,0.15606936416184972,"the language representation, many studies also adopt the masking methods, like [30] for action
81"
RELATED WORK,0.1579961464354528,"recognition, [30] for text classification task, [31] for text log anomaly detection, [32, 33] for visual
82"
RELATED WORK,0.1599229287090559,"representation learning. In [34], the CNN-based model learns the semantic context features by using
83"
RELATED WORK,0.16184971098265896,"a multi-scale mask across the whole image with different scales for anomaly detection in image
84"
RELATED WORK,0.16377649325626203,"domain. We also use masking in our transformer-based GAN for time series anomaly detection, but
85"
RELATED WORK,0.16570327552986513,"unlike the above studies, we propose the two-step masking strategy for training and test to provide
86"
RELATED WORK,0.1676300578034682,"feedback that boosts the model to generate the uncertain parts successfully.
87 [CLS]"
RELATED WORK,0.16955684007707128,Transformer-based Critic
RELATED WORK,0.17148362235067438,Transformer-based Generator
RELATED WORK,0.17341040462427745,"Positional 
Embedding"
RELATED WORK,0.17533718689788053,Embedding Table ùëæùëí
RELATED WORK,0.17726396917148363,Output ‡∑°ùëø1
RELATED WORK,0.1791907514450867,"Inverse Embedding
Pre-Processed Input ‡∑©ùëø min max"
RELATED WORK,0.1811175337186898,"Pre-Processing
(Norm. & Quant.)"
RELATED WORK,0.18304431599229287,"Critic Scores ùíõ
‡∑úùíõ"
RELATED WORK,0.18497109826589594,"‡∑°ùëø1,2 ‚àà‚Ñùùëá"
RELATED WORK,0.18689788053949905,Soft-Argmax
RELATED WORK,0.18882466281310212,Original Input ùëø
RELATED WORK,0.1907514450867052,ùíë‚Ä≤ ‚àà‚Ñùùëá√óùëë
RELATED WORK,0.1926782273603083,‡∑ùùíë‚àà‚Ñùùëá√óùêæ 0 K-1
RELATED WORK,0.19460500963391136,Cosine Similarity
RELATED WORK,0.19653179190751446,Element-wise Addition
RELATED WORK,0.19845857418111754,Train Only
RELATED WORK,0.2003853564547206,Masked Input ‡∑©ùëøùëö
RELATED WORK,0.2023121387283237,"1,2 ‚àà‚Ñùùëá"
RELATED WORK,0.20423892100192678,Embedding
RELATED WORK,0.20616570327552985,Transformer
RELATED WORK,0.20809248554913296,Encoder Block ùüè
RELATED WORK,0.21001926782273603,Transformer
RELATED WORK,0.2119460500963391,Encoder Block ùë≥ ‚ãØ
RELATED WORK,0.2138728323699422,Random Masking
RELATED WORK,0.21579961464354527,Exclusive & Entropy-based
RELATED WORK,0.21772639691714837,"Re-masking ‡∑©ùëøùëö
1"
RELATED WORK,0.21965317919075145,Embedding
RELATED WORK,0.22157996146435452,Transformer
RELATED WORK,0.22350674373795762,Encoder Block ùüè
RELATED WORK,0.2254335260115607,Transformer
RELATED WORK,0.22736030828516376,Encoder Block ùë≥ ‚ãØ
RELATED WORK,0.22928709055876687,Linear Classifier
RELATED WORK,0.23121387283236994,Mask Pool
RELATED WORK,0.23314065510597304,Entropy
RELATED WORK,0.2350674373795761,Exclusive Avg. ‚ãØ
RELATED WORK,0.23699421965317918,‚Ñìm: length ‚äï ‚ãØ
RELATED WORK,0.23892100192678228,Attention Map
RELATED WORK,0.24084778420038536,"‡∑©ùëøùëö
2
ùíîùíé: stride of"
RELATED WORK,0.24277456647398843,"sliding window
Random
Selection"
RELATED WORK,0.24470134874759153,ùíèùíé: # of masks
RELATED WORK,0.2466281310211946,Softmax ‚ãØ
RELATED WORK,0.24855491329479767,Softmax
RELATED WORK,0.2504816955684008,Entropy ùêæùëá ùëÑ Heads
RELATED WORK,0.2524084778420039,"Token
Embedding"
RELATED WORK,0.2543352601156069,ùíõ‚Ä≤ ‚àà‚Ñùùëá√óùëë
RELATED WORK,0.25626204238921,"N: Length of Sequence
K: Number of tokens"
RELATED WORK,0.2581888246628131,Output ‡∑°ùëø2
RELATED WORK,0.26011560693641617,Output ‡∑°ùëø
RELATED WORK,0.26204238921001927,Input ‡∑©ùëø
RELATED WORK,0.26396917148362237,Output ‡∑°ùëø
RELATED WORK,0.2658959537572254,"Top 25% Avg. 0 K-1 0 K-1
0 K-1 ùìõùíÇùíÖùíó ùìõùíìùíÜùíÑ"
RELATED WORK,0.2678227360308285,"Figure 1: Overview of the proposed AnoFormer. For simplicity, this figure shows the univariate case. In Step
1 (random masking), a pre-processed input ÀúX is masked with a randomly selected mask from a predefined
mask pool. After passing the masked input ÀúX
1
m to the generator, ÀÜX1 is generated as the output by passing
through embedding, transformer encoder, and inverse embedding layers. In Step 2 (exclusive and entropy-based
re-masking), based on the entropy calculated from attention maps of all layers in Step 1, ÀúX is re-masked and
ÀÜX2 is generated again from the generator. Final output ÀÜX is constructed via the combination of the masked
parts of Step 1 and Step 2. With an aid of a critic, the generator is able to generate more normal-like signals.
Here, Ladv is the adversarial loss, including Lg
adv and Lc
adv. Note that the critic is used only for the train time."
ANOFORMER,0.2697495183044316,"3
AnoFormer
88"
ANOFORMER,0.27167630057803466,"In this section, we propose AnoFormer for unsupervised time series anomaly detection. We first
89"
ANOFORMER,0.27360308285163776,"define the target task including an algorithm procedure briefly in Section 3.1. We then describe how
90"
ANOFORMER,0.27552986512524086,"to construct a transformer-based GAN framework based on a transformer encoder in Section 3.2.
91"
ANOFORMER,0.2774566473988439,"Next, we introduce two different masking steps for our model to encode time series data effectively
92"
ANOFORMER,0.279383429672447,"in Section 3.3. Finally, we present the whole training scheme of AnoFormer in Section 3.4. Figure 1
93"
ANOFORMER,0.2813102119460501,"shows the overall architecture of AnoFormer.
94"
PROBLEM DEFINITION,0.2832369942196532,"3.1
Problem Definition
95"
PROBLEM DEFINITION,0.28516377649325625,"Let X
=
{x1, x2, ¬∑ ¬∑ ¬∑ , xT }
‚àà
RT √ón be an input signal of T lengths, where xt
=
96

x1
t, x2
t, ¬∑ ¬∑ ¬∑ , xn
t
	
‚ààRn at time step t is a vector of dimension n. Since it is easier to get nor-
97"
PROBLEM DEFINITION,0.28709055876685935,"mal time series data compared to abnormal ones, we train a generator G and a discriminator D using
98"
PROBLEM DEFINITION,0.28901734104046245,"only normal data without any label in an unsupervised manner. After training, for each unseen signal
99"
PROBLEM DEFINITION,0.2909441233140655,"X, which can be normal or abnormal, the generator G generates a normal-like signal ÀÜX. From the
100"
PROBLEM DEFINITION,0.2928709055876686,"generated signal, we can determine whether the observed signal X is normal or not based on the
101"
PROBLEM DEFINITION,0.2947976878612717,"reconstruction errors between the given signal X and the generated signal ÀÜX.
102"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.29672447013487474,"3.2
Transformer-based GAN for Time Series Data
103"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.29865125240847784,"Pre-Processing. To deal with an input signal for a transformer encoder, we need a pre-processing
104"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.30057803468208094,"step that makes the input signal discrete tokens. To this end, we normalize each time series input
105"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.302504816955684,"X between -1 and 1 by using the min-max scaling. Then, we quantize the normalized real value
106"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3044315992292871,"within a specific range [0, K), where the integer K is a hyperparameter controlling the quantization
107"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3063583815028902,"resolution, and use the corresponding integer value as a token. Let ÀúX ‚ààRT √ón be the pre-processed
108"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.30828516377649323,"signal. We set K = 400 for all the experiments, in which the pre-processed signal ÀúX looks almost
109"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.31021194605009633,"like the input X. In total of K tokens (quantization levels), we add a [MASK] token to utilize it for
110"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.31213872832369943,"both training and test. To sum up, the input signal X is pre-processed to be ÀúX by applying scaling
111"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3140655105973025,"and quantization sequentially.
112"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3159922928709056,"Embedding. An embedding step embeds discrete tokens into the embedding vectors. Here we use
113"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3179190751445087,"a token embedding layer to map each token to the corresponding entry in an embedding weight
114"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3198458574181118,"W e ‚ààR(K+1)√ód. We denote z‚Ä≤ ‚ààRT √ód as the output token embedding, where d is an embedding
115"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3217726396917148,"dimension. We add a sinusoidal positional embedding p‚Ä≤ to the token embedding z‚Ä≤ to allow the
116"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3236994219653179,"model to attend relative positions as follows:
117"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.325626204238921,"z = z‚Ä≤ + p‚Ä≤.
(1)"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.32755298651252407,"Transformer Encoder. A transformer encoder uses the embedding z ‚ààRT √ód as the input, and
118"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.32947976878612717,"outputs ÀÜz ‚ààRT √ód. Each block of the transformer encoder contains a multi-head self-attention layer
119"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.33140655105973027,"and a feed-forward network, followed by a residual connection and a layer normalization. Through
120"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3333333333333333,"the self-attention mechanism, it is possible to attend the relevant information of each time step at
121"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3352601156069364,"once, while multiple attention heads can consider different periodicities in time series data [35].
122"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3371868978805395,"Inverse Embedding. We need to invert the output ÀÜz into the original form of time series, ÀÜX ‚ààRT √ón.
123"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.33911368015414256,"To this end, we introduce an inverse embedding layer to our model. We calculate the cosine similarity
124"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.34104046242774566,"between the output embedding ÀÜz and the embedding weight W e taken from the token embedding
125"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.34296724470134876,"layer, and apply the softmax operation as follows:
126"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3448940269749518,ÀÜp = softmax Ô£´
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3468208092485549,"Ô£≠
ÀÜz ¬∑ W e‚ä§"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.348747591522158,"‚à•ÀÜz‚à•
W e‚ä§ Ô£∂"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.35067437379576105,"Ô£∏.
(2)"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.35260115606936415,"From the above equation, we obtain the probability distribution ÀÜp ‚ààRT √óK, where ÀÜpt,k means the
127"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.35452793834296725,"probability that k will be selected in the range of [0, K) except the [MASK] token at the position t.
128"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.35645472061657035,"We then extract an index ÀÜxt of the maximum probability for each time step t ‚àà[1, 2, ¬∑ ¬∑ ¬∑ , T], using
129"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3583815028901734,"the soft-argmax operation as follows:
130"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3603082851637765,"ÀÜxt = soft-argmax(ÀÜpt) = PK‚àí1
i=0
eŒ≤ÀÜpt,i
PK‚àí1
j=0 eŒ≤ÀÜpt,j i,"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3622350674373796,"where Œ≤ is a sufficiently large value, such as 1000. Then, the indices in all time steps are concatenated
131"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.36416184971098264,"to reconstruct the quantized output ÀÜX as follows:
132"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.36608863198458574,"ÀÜX = {ÀÜx1, ÀÜx2, ¬∑ ¬∑ ¬∑ , ÀÜxT } .
(3)"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.36801541425818884,"Transformer-based GAN Framework. To enhance the generation quality of ÀÜX, we design an
133"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3699421965317919,"adversarial framework using transformer encoders. Following the notation of WGAN-GP [36], from
134"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.371868978805395,"now on we use the term critic C instead of the discriminator D. Same as the generator G, we
135"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3737957610789981,"construct the critic C using the transformer encoder, but in the critic C, a [CLS] token is added in
136"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.37572254335260113,"front of the input tokens for classification. After passing through the transformer encoder, the linear
137"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.37764932562620424,"classifier outputs the critic score using only the [CLS] token. While classifying the real input ÀúX and
138"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.37957610789980734,"the fake output ÀÜX, the critic C guides the generator G to reconstruct more normal-like signal ÀÜX. As
139"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3815028901734104,"a result, our model can distinguish ÀúX whether it is normal or abnormal according to the difference
140"
TRANSFORMER-BASED GAN FOR TIME SERIES DATA,0.3834296724470135,"between the input signal ÀúX and the reconstructed signal ÀÜX from the generator G at test time.
141"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3853564547206166,"3.3
Two-Step Masking for Time Series Encoding
142"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3872832369942196,"In the previous section, we introduce the transformer-based GAN framework for time series data.
143"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3892100192678227,"However, we empirically find that the representation learning of the proposed transformer-based
144"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3911368015414258,"GAN is not possible because the generator G just copies the input as the output always. Inspired by
145"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3930635838150289,"recent studies [35, 32, 33] that effectively learn the representation through masking in transformer,
146"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.394990366088632,"we propose two different masking steps during training and test time: 1) random masking and 2)
147"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3969171483622351,"exclusive and entropy-based re-masking. We experimentally demonstrate that the proposed two-step
148"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.3988439306358382,"masking is essential for our framework to learn the distribution of normal time series data successfully.
149"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4007707129094412,"In the following content, we describe how to mask the input effectively in each step with details.
150"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4026974951830443,"Step 1: Random Masking. As the first step, we partially hide the input signal ÀúX using a randomly
151"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4046242774566474,"selected mask from a mask pool. To construct the mask pool, we design a single mask in which the
152"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.40655105973025046,"mask and the non-mask sections alternately appear. We then generate multiple masks by applying
153"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.40847784200385356,"sliding window to the single mask, and group them as the mask pool. The composition of the mask
154"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.41040462427745666,"pool depends on a length lm of a single mask section, a ratio rm of all mask parts, and a stride sm for
155"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4123314065510597,"the sliding window. The number of masks nm in the predefined mask pool is determined as follows:
156"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4142581888246628,"nm = 2 √ó
 lm sm"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4161849710982659,"
.
(4)"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.41811175337186895,"Using the above equation, we generate the enough number of masks in the pool to cover all sections
157"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.42003853564547206,"of the signal. During the train and test time, the mask is randomly selected in the predefined mask
158"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.42196531791907516,"pool per each signal, and the generator G reconstructs ÀÜX1 from the masked input ÀúX
1
m.
159"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4238921001926782,"Step 2: Exclusive and Entropy-based Re-Masking. After ÀÜX1 is generated from Step 1, we again
160"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4258188824662813,"mask the exclusive parts that are not covered in Step 1 for our model to consider all parts of the
161"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4277456647398844,"input. To avoid the error accumulation, here we re-mask the input ÀúX, instead of the first output ÀÜX1.
162"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4296724470134875,"In addition, we provide feedback to our model by re-masking the parts that the model considers
163"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.43159922928709055,"uncertain during Step 1. To this end, we get an attention map from each layer of the generator as
164"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.43352601156069365,"follows:
165"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.43545279383429675,"Al,h = softmax QhKhT ‚àö d ! ,"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4373795761078998,"Al = 1 H H
X"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4393063583815029,"h=1
Al,h,"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.441233140655106,"where l ‚àà[1, 2, ¬∑ ¬∑ ¬∑ , L] and Al is the attention map in the l-th layer, calculated by the average of all
166"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.44315992292870904,"attention maps for individual heads, Al,h. This layer-wise attention map determines how much a
167"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.44508670520231214,"specific time step focuses on the other parts of the input per signal. In this context, the uniformly
168"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.44701348747591524,"distributed attention means that the model does not know which connections are valuable [37], i.e.,
169"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4489402697495183,"the prediction is uncertain. To quantify the uncertainty, we calculate an entropy H ÀÜ
X1 of the masked
170"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4508670520231214,"input ÀÜX1 as follows:
171"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4527938342967245,"H(t) = ‚àí1 L L
X l=1 T
X"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.45472061657032753,"j=1
Al
t,j log Al
t,j,"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.45664739884393063,"H ÀÜ
X1 = {H(1), H(2), ¬∑ ¬∑ ¬∑ , H(T)}."
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.45857418111753373,"To provide feedback on the high entropy parts, we re-mask 50% of the parts already masked in Step
172"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4605009633911368,"1. Then the generator G re-generates the second output ÀÜX2 from the masked signal ÀúX2. Finally,
173"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4624277456647399,"we combine the masked parts generated from Step 1 and the ones from Step 2 to construct the final
174"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.464354527938343,"output ÀÜX. If there are overlapped parts between Step 1 and Step 2, the parts of Step 2 are used. From
175"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4662813102119461,"this re-masking step, we experimentally prove that our model becomes robust to unexplored and
176"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4682080924855491,"uncertain parts within a fixed model size. We also use the same random masking and re-masking
177"
TWO-STEP MASKING FOR TIME SERIES ENCODING,0.4701348747591522,"strategies at test time.
178"
TRAINING ANOFORMER,0.4720616570327553,"3.4
Training AnoFormer
179"
TRAINING ANOFORMER,0.47398843930635837,"To train AnoFormer, we apply the cross-entropy loss to reconstruct the same input ÀúX from the final
180"
TRAINING ANOFORMER,0.47591522157996147,"output ÀÜX as follows:
181"
TRAINING ANOFORMER,0.47784200385356457,"Lrec = ‚àí T
X i=1 K
X"
TRAINING ANOFORMER,0.4797687861271676,"j=1
ÀúXi,j ¬∑ log
 ÀÜpi,j

,
(5)"
TRAINING ANOFORMER,0.4816955684007707,"where ÀúXi,j denotes the one-hot label vector from the input and ÀÜpi,j denotes the probability distribu-
182"
TRAINING ANOFORMER,0.4836223506743738,"tion of the final output ÀÜX. Using ÀÜX from the generator G during two-step masking, the critic C tries
183"
TRAINING ANOFORMER,0.48554913294797686,"to minimize the following loss function:
184"
TRAINING ANOFORMER,0.48747591522157996,"X‚Ä≤ = œµ ÀúX + (1 ‚àíœµ) ÀÜX,
(6)"
TRAINING ANOFORMER,0.48940269749518306,"LC,adv =

E
h
C

ÀÜX
i
‚àíE
h
C

ÀúX
i
+ ŒªEX‚Ä≤‚àºPX‚Ä≤

(
‚àáX‚Ä≤C
 
X‚Ä≤
2 ‚àí1)2
,
(7)"
TRAINING ANOFORMER,0.4913294797687861,"where œµ is randomly chosen between zero and one. The first term measures the Wasserstein distance
185"
TRAINING ANOFORMER,0.4932562620423892,"and the second term is the gradient penalty, where X‚Ä≤ is a random sample from PX‚Ä≤ to enforce the
186"
TRAINING ANOFORMER,0.4951830443159923,"Lipschitz constraint. The coefficient is a harmonic parameter to balance the Wasserstein distance and
187"
TRAINING ANOFORMER,0.49710982658959535,"the gradient penalty, where we use the value of 10. The loss function of the generator G is as follows:
188 189"
TRAINING ANOFORMER,0.49903660886319845,"Lg
adv = ‚àíE
h
C

ÀÜX
i
,
(8)"
TRAINING ANOFORMER,0.5009633911368016,"Table 1: Quantitative comparisons in four datasets. For all of the metrics, a higher value indicates a better
performance."
TRAINING ANOFORMER,0.5028901734104047,"Metric
Base
Architecture
Method
NeurIPS-TS
MIT-BIH
2D-gesture
Power-demand
Global
Contextual
Shapelet
Seasonal
Trend
Average AUROC"
TRAINING ANOFORMER,0.5048169556840078,"CNN
BeatGAN
0.9753
0.6128
0.7398
0.9742
1.0000
0.8372
0.9475
0.7256
0.5796
RNN
TadGAN
1.0000
0.4285
0.9834
0.9744
0.9327
0.9726
0.8256
0.5294
0.8438
RAE-ensemble
0.5226
0.9348
0.9244
0.9625
0.7246
0.8138
-
0.7808
0.6587
RAMED
0.5265
0.9325
0.9084
0.9628
0.7259
0.8112
-
0.7839
0.6787
Transformer
Anomaly Transformer
0.9931
0.6224
0.7407
0.9332
0.9976
0.8400
0.8108
0.7868
0.7739
AnoFormer (Ours)
1.0000
0.9758
0.9900
0.9985
0.9985
0.9911
0.9552
0.8407
0.8667 AUPRC"
TRAINING ANOFORMER,0.5067437379576107,"CNN
BeatGAN
0.9855
0.7051
0.6817
0.9748
1.0000
0.9634
0.9143
0.4952
0.1228
RNN
TadGAN
1.0000
0.3603
0.9565
0.9754
0.8731
0.9806
0.4621
0.4367
0.3098
RAE-ensemble
0.0453
0.8297
0.8159
0.9191
0.1378
0.5496
-
0.5287
0.1400
RAMED
0.0443
0.8223
0.6873
0.9109
0.1291
0.5188
-
0.5331
0.1627
Transformer
Anomaly Transformer
0.9959
0.6957
0.6630
0.9364
0.9978
0.9639
0.5603
0.5607
0.4967
AnoFormer (Ours)
1.0000
0.9854
0.9901
0.9985
0.9987
0.9982
0.9187
0.6142
0.5584"
TRAINING ANOFORMER,0.5086705202312138,F1 score
TRAINING ANOFORMER,0.5105973025048169,"CNN
BeatGAN
0.9345
0.7348
0.6136
0.9487
1.0000
0.9008
0.8015
0.4941
0.2266
RNN
TadGAN
1.0000
0.3590
0.9331
0.9844
0.8170
0.9380
0.5289
0.4138
0.5714
RAE-ensemble
0.0853
0.8343
0.7750
0.9181
0.3889
0.6003
-
0.5511
0.2678
RAMED
0.0838
0.8272
0.6203
0.8782
0.4040
0.5627
-
0.5633
0.2934
Transformer
Anomaly Transformer
0.9751
0.7358
0.6115
0.8730
0.9958
0.9014
0.5446
0.6486
0.6053
AnoFormer (Ours)
1.0000
0.9400
0.9696
0.9913
0.9974
0.9798
0.8410
0.6667
0.6226"
TRAINING ANOFORMER,0.51252408477842,"which makes the critic C not be able to classify the generated ÀÜX. To sum up, the proposed AnoFormer
190"
TRAINING ANOFORMER,0.5144508670520231,"is trained via the following loss functions for the generator G and the critic C:
191"
TRAINING ANOFORMER,0.5163776493256262,"LG = ŒªrecLrec + ŒªadvLg
adv,
(9)
LC = Lc
adv,
(10)"
TRAINING ANOFORMER,0.5183044315992292,"where we set Œªrec and Œªadv as 1.
192"
EXPERIMENTS,0.5202312138728323,"4
Experiments
193"
EXPERIMENTS,0.5221579961464354,"Datasets. We evaluated AnoFormer on four real-world benchmarks: 1) MIT-BIH 1 contains 48 ECG
194"
EXPERIMENTS,0.5240847784200385,"records of test subjects from Beth Israel Hospital, 2) 2D-gesture contains time series of X and Y
195"
EXPERIMENTS,0.5260115606936416,"coordinates of an actor‚Äôs right hand, 3) Power-demand is a dataset measuring the power comsumption
196"
EXPERIMENTS,0.5279383429672447,"for the Dutch research facility, and 4) NeurIPS-TS 2 [38] is a synthetic dataset including five different
197"
EXPERIMENTS,0.5298651252408478,"time series anomaly scenarios as point-global, point-contextual, pattern-shapelet, pattern-seasonal,
198"
EXPERIMENTS,0.5317919075144508,"and pattern-trend. More details on each dataset are summarized in Appendix A.
199"
EXPERIMENTS,0.5337186897880539,"Baselines. We compared our model with various baselines, including CNN, RNN, and transformer-
200"
EXPERIMENTS,0.535645472061657,"based reconstruction models. BeatGAN [2] and TadGAN [8] are CNN and LSTM-based GAN
201"
EXPERIMENTS,0.5375722543352601,"models, respectively. RAE-ensemble [9] is an ensemble of RNNs with sparse skip connections in
202"
EXPERIMENTS,0.5394990366088632,"autoencoder. RAMED [10] additionally uses the multiresolution decoding based on RAE-ensemble.
203"
EXPERIMENTS,0.5414258188824663,"Anomaly Transformer [11] develops the transformer architecture to utilize association information.
204"
EXPERIMENTS,0.5433526011560693,"Implementation Details. For both the generator G and the critic C, we utilized the basic transformer
205"
EXPERIMENTS,0.5452793834296724,"encoders with 9 and 6 layers for MIT-BIH, and 4 and 2 layers for other datasets, respectively. The
206"
EXPERIMENTS,0.5472061657032755,"embedding dimension and the number of heads are 128 and 8, respectively. The mask length lm is
207"
EXPERIMENTS,0.5491329479768786,"about 10% of the sequence length T, and the mask stride sm is about half of the mask length lm.
208"
EXPERIMENTS,0.5510597302504817,"We used Adam optimizer with initial learning rate, momentum Œ≤1, and Œ≤2 as 0.0001, 0.5, and 0.999,
209"
EXPERIMENTS,0.5529865125240848,"respectively. We implemented our model using PyTorch and trained on a NVIDIA RTX 3090 GPU.
210"
QUANTITATIVE RESULTS,0.5549132947976878,"4.1
Quantitative Results
211"
QUANTITATIVE RESULTS,0.5568400770712909,"Table 1 shows the anomaly detection performances of each baseline on three different real-world
212"
QUANTITATIVE RESULTS,0.558766859344894,"datasets (i.e., MIT-BIH, 2D-gesture, and Power-demand), and a synthetic dataset (i.e., NeurIPS-TS
213"
QUANTITATIVE RESULTS,0.5606936416184971,"[38]). Overall, RNN or transformer-based models showed high performances except MIT-BIH. In
214"
QUANTITATIVE RESULTS,0.5626204238921002,"MIT-BIH, BeatGAN showed the second-best performances among all the benchmarks. In case of
215"
QUANTITATIVE RESULTS,0.5645472061657033,"the proposed AnoFormer, this model outperformed all the baselines in four different datasets. In
216"
QUANTITATIVE RESULTS,0.5664739884393064,"1https://physionet.org/content/mitdb/1.0.0/
2https://github.com/datamllab/tods/tree/benchmark"
QUANTITATIVE RESULTS,0.5684007707129094,"(a)
(b)
(c)"
QUANTITATIVE RESULTS,0.5703275529865125,"(d)
(e)
(f)"
QUANTITATIVE RESULTS,0.5722543352601156,"Figure 2: Output visualization in Point-Contextual (NeurIPS-TS) and MIT-BIH datasets. Left: visualization of
abnormal input and the normal-like output. Middle: reconstruction results of random masking (blue). Right:
reconstruction results of exclusive (green) and entropy-based (red) re-masking. Vest viewed in color."
QUANTITATIVE RESULTS,0.5741811175337187,"particular, our model performed well on NeurIPS-TS containing five types of outliers, and it means
217"
QUANTITATIVE RESULTS,0.5761078998073218,"AnoFormer is robust to the various types of outliers. AnoFormer achieved the state-of-the-art results
218"
QUANTITATIVE RESULTS,0.5780346820809249,"from small datasets (e.g., 2D-gesture and Power-demand) with about 1,000 training sets to large
219"
QUANTITATIVE RESULTS,0.5799614643545279,"datasets (e.g., NeurIPS-TS and MIT-BIH) with about tens of thousands of training sets, and from
220"
QUANTITATIVE RESULTS,0.581888246628131,"univariate to multivariate cases. The experimental results demonstrate that the proposed transformer-
221"
QUANTITATIVE RESULTS,0.5838150289017341,"based GAN framework with the two-step masking strategy is effective to reconstruct normal time
222"
QUANTITATIVE RESULTS,0.5857418111753372,"series data for anomaly detection.
223"
QUALITATIVE RESULTS,0.5876685934489403,"4.2
Qualitative Results
224"
QUALITATIVE RESULTS,0.5895953757225434,"Figure 2 shows the qualitative examples of AnoFormer. First column (Figure 2(a) and Figure 2(d))
225"
QUALITATIVE RESULTS,0.5915221579961464,"shows the abnormal examples of point-contextual of NeurIPS-TS and MIT-BIH datasets, respectively.
226"
QUALITATIVE RESULTS,0.5934489402697495,"The other columns show that the proposed rnadom masking and re-masking strategies actually provide
227"
QUALITATIVE RESULTS,0.5953757225433526,"feedback to our framework. For example, the incorrectly copied parts in Step 1 was refined by the
228"
QUALITATIVE RESULTS,0.5973025048169557,"entropy-based re-masking (please see the black circles in the figure). As shown in the figure, when the
229"
QUALITATIVE RESULTS,0.5992292870905588,"abnormal inputs were received, the model generated the normal-like outputs. Therefore, AnoFormer
230"
QUALITATIVE RESULTS,0.6011560693641619,"can detect the abnormal points through the difference between the input and the output.
231"
ABLATION STUDY,0.603082851637765,"4.3
Ablation Study
232"
ABLATION STUDY,0.605009633911368,"We conducted various ablation studies to analyze the effectiveness of the proposed transformer-
233"
ABLATION STUDY,0.6069364161849711,"based GAN framework and two-step masking. All of the ablation studies were performed on the
234"
ABLATION STUDY,0.6088631984585742,"Point-Contextual dataset of NeurIPS-TS, since it is the most difficult task to detect the anomalies
235"
ABLATION STUDY,0.6107899807321773,"out of the five types of outliers. Figure 2 shows an example of Point-Contextual dataset, which has
236"
ABLATION STUDY,0.6127167630057804,"the small glitches as the outliers. In Appendix B, we additionally examined the sensitivity of each
237"
ABLATION STUDY,0.6146435452793835,"hyperparameter newly adopted in our model.
238"
TRANSFORMER-BASED GAN FRAMEWORK,0.6165703275529865,"4.3.1
Transformer-based GAN Framework
239"
TRANSFORMER-BASED GAN FRAMEWORK,0.6184971098265896,"We first investigated the effectiveness of the transformer-based adversarial framework in our model.
240"
TRANSFORMER-BASED GAN FRAMEWORK,0.6204238921001927,"In this experiment, we used BeatGAN as a CNN-based baseline. Table 2 shows the ablation results
241"
TRANSFORMER-BASED GAN FRAMEWORK,0.6223506743737958,"when the generator and the critic use different backbone networks, such as CNN, and transformer. As
242"
TRANSFORMER-BASED GAN FRAMEWORK,0.6242774566473989,"shown in the table, the transformer-based generator showed higher performances on all of metrics
243"
TRANSFORMER-BASED GAN FRAMEWORK,0.626204238921002,"with large margins than the CNN-based generator. Interestingly, we empirically found that there was
244"
TRANSFORMER-BASED GAN FRAMEWORK,0.628131021194605,"no synergy when using CNN-based critic with the transformer-based generator. It means, it is not
245"
TRANSFORMER-BASED GAN FRAMEWORK,0.630057803468208,"helpful for the transformer-based generator to construct the critic with an inappropriate baseline. On
246"
TRANSFORMER-BASED GAN FRAMEWORK,0.6319845857418112,Table 2: Ablation study of the proposed transformer-based GAN.
TRANSFORMER-BASED GAN FRAMEWORK,0.6339113680154143,"Generator
Critic
AUROC
AUPRC
F1 score
CNN
CNN
0.6128
0.7051
0.7348
Transformer
-
0.9572
0.9735
0.9093
Transformer
CNN
0.9510
0.9675
0.9026
Transformer
Transformer
0.9758
0.9854
0.9400"
TRANSFORMER-BASED GAN FRAMEWORK,0.6358381502890174,Table 3: Ablation study of the proposed two-step masking.
TRANSFORMER-BASED GAN FRAMEWORK,0.6377649325626205,"Step 1
Step 2
AUROC
AUPRC
F1 score
-
-
0.5000
0.3602
0.2386
Random
-
0.8557
0.7959
0.7548
Mask pool
-
0.9109
0.8651
0.8200
Mask pool
Mask pool (50%)
0.9277
0.8590
0.7808
Mask pool
Exclusive (50%)
0.9489
0.9622
0.9057
Mask pool
Exclusive + Random (75%)
0.9709
0.9466
0.9004
Mask pool
Exclusive + Anomaly score (75%)
0.9747
0.9533
0.9119
Mask pool
Exclusive + Entropy (75%)
0.9758
0.9854
0.9400"
TRANSFORMER-BASED GAN FRAMEWORK,0.6396917148362236,"the other hand, the transformer-based critic showed better performances than the baseline without
247"
TRANSFORMER-BASED GAN FRAMEWORK,0.6416184971098265,"the critic, which means it encourages the generated output to be close to the normal signal. From
248"
TRANSFORMER-BASED GAN FRAMEWORK,0.6435452793834296,"this result, we demonstrate that our transformer-based GAN framework trained with the proposed
249"
TRANSFORMER-BASED GAN FRAMEWORK,0.6454720616570327,"masking strategy is effective to reconstruct normal time series data for anomaly detection.
250"
TWO-STEP MASKING,0.6473988439306358,"4.3.2
Two-Step Masking
251"
TWO-STEP MASKING,0.649325626204239,"As shown in Table 3, we investigated the effect of two-step masking in our model. The first row
252"
TWO-STEP MASKING,0.651252408477842,"means a naive form of the transformer-based GAN without any masking. The result was 0.5 of
253"
TWO-STEP MASKING,0.653179190751445,"AUROC, which means the naive transformer-based GAN cannot distinguish between normal and
254"
TWO-STEP MASKING,0.6551059730250481,"abnormal signals at all. To overcome this critical issue, we adopted various masking strategies. First,
255"
TWO-STEP MASKING,0.6570327552986512,"we investigated the masking for Step 1. Here, Random means a fully random masking without any
256"
TWO-STEP MASKING,0.6589595375722543,"predefined mask pool. Mask Pool means our predefined mask pool defined in Section 3.3. The results
257"
TWO-STEP MASKING,0.6608863198458574,"showed that regardless of the masking strategy, masking itself during training and test enabled the
258"
TWO-STEP MASKING,0.6628131021194605,"transformer-based GAN to effectively learn the distribution of normal time series data. Moreover, we
259"
TWO-STEP MASKING,0.6647398843930635,"confirmed that Mask Pool is much better than Random masking, because each mask in the mask pool
260"
TWO-STEP MASKING,0.6666666666666666,"definitely covers the different parts from each other, providing a complementary effect.
261"
TWO-STEP MASKING,0.6685934489402697,"Next, we conducted in-depth experiments to evaluate and compare different re-masking strategies in
262"
TWO-STEP MASKING,0.6705202312138728,"Step 2. Mask pool method in Step 1 can be also used for re-masking. Exclusive method re-masks the
263"
TWO-STEP MASKING,0.6724470134874759,"exclusive parts of the random mask selected in Step 1. From the results, we found that re-masking
264"
TWO-STEP MASKING,0.674373795761079,"improved the detection ability of our model, and especially, exclusive masking strategy was really
265"
TWO-STEP MASKING,0.6763005780346821,"effective. This is because the model can consider the characteristic of whole signal during two-step
266"
TWO-STEP MASKING,0.6782273603082851,"masking. To provide more feedback to our model, we additionally re-masked the masked parts in
267"
TWO-STEP MASKING,0.6801541425818882,"Step 1. We experimented the following three cases: 1) Random method re-masks the signal randomly,
268"
TWO-STEP MASKING,0.6820809248554913,"2) Anomaly score method re-masks the parts with high anomaly scores, and 3) Entropy method
269"
TWO-STEP MASKING,0.6840077071290944,"re-masks the parts with high entropy values. The results showed that masking the uncertain parts
270"
TWO-STEP MASKING,0.6859344894026975,"provided the proper feedback to our model, resulting in the highest scores among all the baselines.
271"
TWO-STEP MASKING,0.6878612716763006,"Therefore, we confirmed that the entropy-based re-masking is more effective than the other additional
272"
TWO-STEP MASKING,0.6897880539499036,"masking methods.
273"
DISCUSSION,0.6917148362235067,"5
Discussion
274"
DISCUSSION,0.6936416184971098,"We further conducted analysis to demonstrate the effectiveness of the proposed two-step masking
275"
DISCUSSION,0.6955684007707129,"strategy. To confirm the importance of Step 2, we compared our method with the absence of Step 2.
276"
DISCUSSION,0.697495183044316,"Here, we also used Point-Contextual dataset in NeurIPS-TS for analysis.
277"
DISCUSSION,0.6994219653179191,"(a)
(b)
(c)"
DISCUSSION,0.7013487475915221,"Figure 3: (a) AUROC and std of all masks in the predefined mask pool. (b) Analysis on the entropy-based
re-masking strategy (normal-case). (c) Analysis on the entropy-based re-masking strategy (abnormal-case)."
DISCUSSION,0.7032755298651252,"The Effectiveness of Re-Masking. First, we investigated the validity of re-masking step. We reported
278"
DISCUSSION,0.7052023121387283,"the average AUROC and std for all masks in the predefined mask pool in Figure 3. In Step 1, there
279"
DISCUSSION,0.7071290944123314,"was a problem that the standard deviation (std) was too high because both training and test time
280"
DISCUSSION,0.7090558766859345,"had randomness in selecting the mask parts. By re-masking through Step 2, the randomness of the
281"
DISCUSSION,0.7109826589595376,"masked parts was eliminated and the std was significantly reduced. The performance of anomaly
282"
DISCUSSION,0.7129094412331407,"detection also increased with a large margin by referring to the entire signal.
283"
DISCUSSION,0.7148362235067437,"Analysis on Entropy-based Re-Masking. To understand the effectiveness of entropy-based re-
284"
DISCUSSION,0.7167630057803468,"masking intuitively, we visualized the relation between entropy and anomaly score in Figure 3(b) and
285"
DISCUSSION,0.7186897880539499,"Figure 3(c) for both cases of normal and abnormal. Since the entropy-based method re-masked the
286"
DISCUSSION,0.720616570327553,"parts selected in Step 1, we measured the anomaly score only in the parts corresponding to Step 1. We
287"
DISCUSSION,0.7225433526011561,"found two meaningful insights through the analysis. First, the higher the entropy, the more incorrect
288"
DISCUSSION,0.7244701348747592,"signal the model generates. In training phase, the high anomaly score means that the model generates
289"
DISCUSSION,0.7263969171483622,"output incorrectly, because only normal data is used. From the result of Figure 3(b), the entropy was
290"
DISCUSSION,0.7283236994219653,"also high in the parts with high anomaly score, which means that the model did not generate signals
291"
DISCUSSION,0.7302504816955684,"well in the parts with high entropy during Step 1. This is because in the high entropy the attention is
292"
DISCUSSION,0.7321772639691715,"uniformly distributed and the meaningful connection is not learned. By re-masking these parts in Step
293"
DISCUSSION,0.7341040462427746,"2, anomaly score was significantly reduces, which means that the model reconstructed the normal
294"
DISCUSSION,0.7360308285163777,"data well in the training process. Second, the entropy-based masking improves the discriminative
295"
DISCUSSION,0.7379576107899807,"ability between normal and abnormal in test time. As shown in Figure 3(c), likewise in the case of
296"
DISCUSSION,0.7398843930635838,"normal, the higher the entropy, the higher the anomaly score in abnormal case. However, there was
297"
DISCUSSION,0.7418111753371869,"also a part with a high entropy and a low anomaly score. These parts mean that the model copied
298"
DISCUSSION,0.74373795761079,"the abnormal input as it was without making it normal. It is possible to provide feedback in both
299"
DISCUSSION,0.7456647398843931,"cases with entropy-based re-masking. By re-masking the parts with high entropy, anomaly score was
300"
DISCUSSION,0.7475915221579962,"considerably increased, and it means the parts that were not well generated due to copying in Step 1
301"
DISCUSSION,0.7495183044315993,"were well re-generated. This makes it possible to further discriminate between normal and abnormal
302"
DISCUSSION,0.7514450867052023,"through anomaly score. In fact, a large anomaly score does not mean getting close to normal data. We
303"
DISCUSSION,0.7533718689788054,"used a NeurIPS-TS dataset to see if the output gets closer to normal data through re-masking. Since
304"
DISCUSSION,0.7552986512524085,"we synthesized abnormal datasets by injecting sporadic outliers in an additive manner, we could
305"
DISCUSSION,0.7572254335260116,"easily get the original normal version of the abnormal data. From this, we confirmed that the output
306"
DISCUSSION,0.7591522157996147,"was correctly getting closer to the original normal through Step 2. We further experimented about
307"
DISCUSSION,0.7610789980732178,"which layer‚Äôs entropy information should be used? From the results in Appendix C, we calculated
308"
DISCUSSION,0.7630057803468208,"entropy from all layers and averaged them.
309"
CONCLUSION,0.7649325626204239,"6
Conclusion
310"
CONCLUSION,0.766859344894027,"In this paper, we introduce AnoFormer, a novel transformer-based GAN for time series anomaly
311"
CONCLUSION,0.7687861271676301,"detection. To learn time series data directly with our model, we propose pre-processing and embedding
312"
CONCLUSION,0.7707129094412332,"methods suitable for time series data. A new training scheme based on two-step masking enables
313"
CONCLUSION,0.7726396917148363,"AnoFormer to embed the representation of normal signals. Especially, the exclusive and entropy-
314"
CONCLUSION,0.7745664739884393,"based re-masking method significantly improves the anomaly detection performances on several
315"
CONCLUSION,0.7764932562620424,"benchmark datasets. From the extensive experiments, we empirically demonstrate that our model
316"
CONCLUSION,0.7784200385356455,"is really effective to solve time series anomaly detection. As future work, we plan to study novel
317"
CONCLUSION,0.7803468208092486,"techniques for shorter inference time, and deal with time series data longer than an hour or a day.
318"
REFERENCES,0.7822736030828517,"References
319"
REFERENCES,0.7842003853564548,"[1] Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate
320"
REFERENCES,0.7861271676300579,"time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages
321"
REFERENCES,0.7880539499036608,"4027‚Äì4035, 2021.
322"
REFERENCES,0.789980732177264,"[2] Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. Beatgan: Anomalous rhythm
323"
REFERENCES,0.791907514450867,"detection using adversarially generated time series. In IJCAI, pages 4433‚Äì4439, 2019.
324"
REFERENCES,0.7938342967244701,"[3] Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and Xiuzhen Cheng. Learning graph
325"
REFERENCES,0.7957610789980732,"structures with transformer for multivariate time series anomaly detection in iot. IEEE Internet
326"
REFERENCES,0.7976878612716763,"of Things Journal, 2021.
327"
REFERENCES,0.7996146435452793,"[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J√∂rg Sander. Lof: identifying
328"
REFERENCES,0.8015414258188824,"density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference
329"
REFERENCES,0.8034682080924855,"on Management of data, pages 93‚Äì104, 2000.
330"
REFERENCES,0.8053949903660886,"[5] Bernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.
331"
REFERENCES,0.8073217726396917,"Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443‚Äì
332"
REFERENCES,0.8092485549132948,"1471, 2001.
333"
REFERENCES,0.8111753371868978,"[6] David MJ Tax and Robert PW Duin. Support vector data description. Machine learning,
334"
REFERENCES,0.8131021194605009,"54(1):45‚Äì66, 2004.
335"
REFERENCES,0.815028901734104,"[7] Lifeng Shen, Zhuocong Li, and James Kwok. Timeseries anomaly detection using temporal
336"
REFERENCES,0.8169556840077071,"hierarchical one-class network. Advances in Neural Information Processing Systems, 33:13016‚Äì
337"
REFERENCES,0.8188824662813102,"13026, 2020.
338"
REFERENCES,0.8208092485549133,"[8] Alexander Geiger, Dongyu Liu, Sarah Alnegheimish, Alfredo Cuesta-Infante, and Kalyan
339"
REFERENCES,0.8227360308285164,"Veeramachaneni. Tadgan: Time series anomaly detection using generative adversarial networks.
340"
REFERENCES,0.8246628131021194,"In 2020 IEEE International Conference on Big Data (Big Data), pages 33‚Äì43. IEEE, 2020.
341"
REFERENCES,0.8265895953757225,"[9] Tung Kieu, Bin Yang, Chenjuan Guo, and Christian S Jensen. Outlier detection for time series
342"
REFERENCES,0.8285163776493256,"with recurrent autoencoder ensembles. In IJCAI, pages 2725‚Äì2732, 2019.
343"
REFERENCES,0.8304431599229287,"[10] Lifeng Shen, Zhongzhong Yu, Qianli Ma, and James T Kwok. Time series anomaly detection
344"
REFERENCES,0.8323699421965318,"with multiresolution ensemble decoding. In Proceedings of the AAAI Conference on Artificial
345"
REFERENCES,0.8342967244701349,"Intelligence, volume 35, pages 9567‚Äì9575, 2021.
346"
REFERENCES,0.8362235067437379,"[11] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series
347"
REFERENCES,0.838150289017341,"anomaly detection with association discrepancy. In International Conference on Learning
348"
REFERENCES,0.8400770712909441,"Representations, 2022.
349"
REFERENCES,0.8420038535645472,"[12] Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu, Wei Cheng,
350"
REFERENCES,0.8439306358381503,"Jingchao Ni, Bo Zong, Haifeng Chen, and Nitesh V Chawla. A deep neural network for
351"
REFERENCES,0.8458574181117534,"unsupervised anomaly detection and diagnosis in multivariate time series data. In Proceedings
352"
REFERENCES,0.8477842003853564,"of the AAAI conference on artificial intelligence, volume 33, pages 1409‚Äì1416, 2019.
353"
REFERENCES,0.8497109826589595,"[13] Julien Audibert, Pietro Michiardi, Fr√©d√©ric Guyard, S√©bastien Marti, and Maria A Zuluaga.
354"
REFERENCES,0.8516377649325626,"Usad: unsupervised anomaly detection on multivariate time series. In Proceedings of the
355"
REFERENCES,0.8535645472061657,"26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
356"
REFERENCES,0.8554913294797688,"3395‚Äì3404, 2020.
357"
REFERENCES,0.8574181117533719,"[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
358"
REFERENCES,0.859344894026975,"≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
359"
REFERENCES,0.861271676300578,"processing systems, 30, 2017.
360"
REFERENCES,0.8631984585741811,"[15] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Nie√üner. Transformerfusion:
361"
REFERENCES,0.8651252408477842,"Monocular rgb scene reconstruction using transformers. Advances in Neural Information
362"
REFERENCES,0.8670520231213873,"Processing Systems, 34, 2021.
363"
REFERENCES,0.8689788053949904,"[16] Dor Arad Hudson and Larry Zitnick. Compositional transformers for scene generation. Advances
364"
REFERENCES,0.8709055876685935,"in Neural Information Processing Systems, 34, 2021.
365"
REFERENCES,0.8728323699421965,"[17] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make
366"
REFERENCES,0.8747591522157996,"one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34,
367"
REFERENCES,0.8766859344894027,"2021.
368"
REFERENCES,0.8786127167630058,"[18] Jing Zhang, Jianwen Xie, Nick Barnes, and Ping Li. Learning generative vision transformer with
369"
REFERENCES,0.8805394990366089,"energy-based latent space for saliency prediction. Advances in Neural Information Processing
370"
REFERENCES,0.882466281310212,"Systems, 34, 2021.
371"
REFERENCES,0.884393063583815,"[19] Yufei Wang, Can Xu, Huang Hu, Chongyang Tao, Stephen Wan, Mark Dras, Mark Johnson,
372"
REFERENCES,0.8863198458574181,"and Daxin Jiang. Neural rule-execution tracking machine for transformer-based text generation.
373"
REFERENCES,0.8882466281310212,"Advances in Neural Information Processing Systems, 34, 2021.
374"
REFERENCES,0.8901734104046243,"[20] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory
375"
REFERENCES,0.8921001926782274,"transformer for image captioning. In Proceedings of the IEEE/CVF Conference on Computer
376"
REFERENCES,0.8940269749518305,"Vision and Pattern Recognition, pages 10578‚Äì10587, 2020.
377"
REFERENCES,0.8959537572254336,"[21] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
378"
REFERENCES,0.8978805394990366,"Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
379"
REFERENCES,0.8998073217726397,"via sequence modeling. Advances in neural information processing systems, 34, 2021.
380"
REFERENCES,0.9017341040462428,"[22] Jiehui Xu, Jianmin Wang, Mingsheng Long, et al. Autoformer: Decomposition transform-
381"
REFERENCES,0.9036608863198459,"ers with auto-correlation for long-term series forecasting. Advances in Neural Information
382"
REFERENCES,0.905587668593449,"Processing Systems, 34, 2021.
383"
REFERENCES,0.9075144508670521,"[23] Dor Arad Hudson and Larry Zitnick. Compositional transformers for scene generation. Advances
384"
REFERENCES,0.9094412331406551,"in Neural Information Processing Systems, 34, 2021.
385"
REFERENCES,0.9113680154142582,"[24] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.
386"
REFERENCES,0.9132947976878613,"Segformer: Simple and efficient design for semantic segmentation with transformers. Advances
387"
REFERENCES,0.9152215799614644,"in Neural Information Processing Systems, 34, 2021.
388"
REFERENCES,0.9171483622350675,"[25] Mingrui Zhu, Changcheng Liang, Nannan Wang, Xiaoyu Wang, Zhifeng Li, and Xinbo Gao. A
389"
REFERENCES,0.9190751445086706,"sketch-transformer network for face photo-sketch synthesis. In International Joint Conference
390"
REFERENCES,0.9210019267822736,"on Artificial Intelligence, 2021.
391"
REFERENCES,0.9229287090558767,"[26] Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In Proceedings of
392"
REFERENCES,0.9248554913294798,"the AAAI Conference on Artificial Intelligence, volume 34, pages 7464‚Äì7471, 2020.
393"
REFERENCES,0.9267822736030829,"[27] Zhidong Liu, Junhui Li, and Muhua Zhu. Improving text generation with dynamic masking and
394"
REFERENCES,0.928709055876686,"recovering. In IJCAI, 2021.
395"
REFERENCES,0.930635838150289,"[28] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai
396"
REFERENCES,0.9325626204238922,"Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In
397"
REFERENCES,0.9344894026974951,"Proceedings of AAAI, 2021.
398"
REFERENCES,0.9364161849710982,"[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
399"
REFERENCES,0.9383429672447013,"deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
400"
REFERENCES,0.9402697495183044,"ence of the North American Chapter of the Association for Computational Linguistics: Human
401"
REFERENCES,0.9421965317919075,"Language Technologies, Volume 1 (Long and Short Papers), pages 4171‚Äì4186, Minneapolis,
402"
REFERENCES,0.9441233140655106,"Minnesota, June 2019. Association for Computational Linguistics.
403"
REFERENCES,0.9460500963391136,"[30] Yi-Bin Cheng, Xipeng Chen, Dongyu Zhang, and Liang Lin.
Motion-transformer: self-
404"
REFERENCES,0.9479768786127167,"supervised pre-training for skeleton-based action recognition. In Proceedings of the 2nd
405"
REFERENCES,0.9499036608863198,"ACM International Conference on Multimedia in Asia, pages 1‚Äì6, 2021.
406"
REFERENCES,0.9518304431599229,"[31] Haixuan Guo, Shuhan Yuan, and Xintao Wu. Logbert: Log anomaly detection via bert. In 2021
407"
REFERENCES,0.953757225433526,"International Joint Conference on Neural Networks (IJCNN), pages 1‚Äì8, 2021.
408"
REFERENCES,0.9556840077071291,"[32] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng,
409"
REFERENCES,0.9576107899807321,"Liwei Wu, Rui Zhao, Ming Tang, et al. Mst: Masked self-supervised transformer for visual
410"
REFERENCES,0.9595375722543352,"representation. Advances in Neural Information Processing Systems, 34, 2021.
411"
REFERENCES,0.9614643545279383,"[33] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image
412"
REFERENCES,0.9633911368015414,"transformers. In International Conference on Learning Representations, 2022.
413"
REFERENCES,0.9653179190751445,"[34] Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning
414"
REFERENCES,0.9672447013487476,"semantic context from normal samples for unsupervised anomaly detection. In Proceedings of
415"
REFERENCES,0.9691714836223507,"the AAAI Conference on Artificial Intelligence, volume 35, pages 3110‚Äì3118, 2021.
416"
REFERENCES,0.9710982658959537,"[35] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten
417"
REFERENCES,0.9730250481695568,"Eickhoff. A transformer-based framework for multivariate time series representation learning.
418"
REFERENCES,0.9749518304431599,"In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,
419"
REFERENCES,0.976878612716763,"pages 2114‚Äì2124, 2021.
420"
REFERENCES,0.9788053949903661,"[36] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
421"
REFERENCES,0.9807321772639692,"Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
422"
REFERENCES,0.9826589595375722,"R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
423"
REFERENCES,0.9845857418111753,"Systems, volume 30. Curran Associates, Inc., 2017.
424"
REFERENCES,0.9865125240847784,"[37] Edward Choi, Zhen Xu, Yujia Li, Michael Dusenberry, Gerardo Flores, Emily Xue, and Andrew
425"
REFERENCES,0.9884393063583815,"Dai. Learning the graphical structure of electronic health records with graph convolutional
426"
REFERENCES,0.9903660886319846,"transformer. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages
427"
REFERENCES,0.9922928709055877,"606‚Äì613, 2020.
428"
REFERENCES,0.9942196531791907,"[38] Kwei-Herng Lai, Daochen Zha, Junjie Xu, Yue Zhao, Guanchu Wang, and Xia Hu. Revisiting
429"
REFERENCES,0.9961464354527938,"time series outlier detection: Definitions and benchmarks. In Thirty-fifth Conference on Neural
430"
REFERENCES,0.9980732177263969,"Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.
431"
