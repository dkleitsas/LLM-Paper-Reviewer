Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0025974025974025974,"Integer linear programs (ILPs) are commonly employed to model diverse practical
problems such as scheduling and planning. Recently, machine learning techniques
have been utilized to solve ILPs. A straightforward idea is to train a model via
supervised learning, with an ILP as the input and an optimal solution as the
label. An ILP is symmetric if its variables can be permuted without changing
the problem structure, resulting in numerous equivalent and optimal solutions.
Randomly selecting an optimal solution as the label can introduce variability in the
training data, which may hinder the model from learning stable patterns. In this
work, we incorporate the intrinsic symmetry of ILPs and propose a novel training
framework called SymILO. Specifically, we modify the learning task by introducing
solution permutation along with neural network weights as learnable parameters
and then design an alternating algorithm to jointly optimize the loss function. We
conduct extensive experiments on ILPs involving different symmetries and the
computational results demonstrate that our symmetry-aware approach significantly
outperforms three existing methods‚Äî-achieving 50.3%, 66.5%, and 45.4% average
improvements, respectively."
INTRODUCTION,0.005194805194805195,"1
Introduction"
INTRODUCTION,0.007792207792207792,"Integer linear programs (ILPs) are optimization problems with integer variables and a linear objective,
and have a wide range of practical uses in various fields, such as production planning (Pochet &
Wolsey, 2006; Chen, 2010), resource allocation (Liu & Fan, 2018; Watson & Woodruff, 2011), and
transportation management (Luathep et al., 2011; Sch√∂bel, 2001). An important property that often
arises in ILPs is symmetry (Margot, 2003), which refers to a situation where permuting variables does
not change the structure of an ILP."
INTRODUCTION,0.01038961038961039,"Recently, there emerges many approaches equipping machine learning methods, supervised learning
in particular, to help efficient solution identification for ILPs (Zhang et al., 2023). Among these
approaches, an important category derived from the idea of predicting the optimal solution has
demonstrated significant improvements (Han et al., 2023; Ding et al., 2020; Khalil et al., 2022;
Nair et al., 2020). In this paper, we consider a classic supervised learning task that aims to train
an ML model to predict an optimal solution for an ILP. Specifically, given a training dataset D =
{(si, yi)}N
i=1 with yi denoting an optimal solution to instance si, we hope to train a neural network"
INTRODUCTION,0.012987012987012988,*Corresponding author: Akang Wang <wangakang@sribd.cn>
INTRODUCTION,0.015584415584415584,"model fŒ∏(¬∑) to approximate the mapping from ILP instances to their optimal solutions, via minimizing
the empirical risk defined on fŒ∏(si) and yi."
INTRODUCTION,0.01818181818181818,"However, for an ILP si with symmetry, there exist multiple optimal solutions including yi and its
symmetric counterparts, any of which has an equal probability of being returned as a label. Training
neural networks without taking symmetry into account is basically learning a model supervised by
random outputs, leading to prediction models of inferior performance."
INTRODUCTION,0.02077922077922078,"To address this issue, we propose to leverage the symmetry of ILPs to improve the model performance
of predicting an optimal solution. Specifically, given input si, we define a new empirical risk using
fŒ∏(si) and œÄi(yi), where œÄi(¬∑) denotes the operation of permuting elements in yi into its symmetric
counterpart. Along with ML model parameters, the permutation operators will also be optimized
during training. To achieve this, we further develop a computationally affordable algorithm that
alternates between optimization of model parameters and optimization of permutation operation. The
distinct contributions of our work can be summarized as follows."
INTRODUCTION,0.023376623376623377,"‚Ä¢ We propose a symmetry-aware framework (called SymILO) that introduces permutation
operators as extra optimization variables to the classic training procedure."
INTRODUCTION,0.025974025974025976,"‚Ä¢ We devise an alternating algorithm to solve the newly proposed problem, with a specific
focus on updating the permutation operator for different symmetries."
INTRODUCTION,0.02857142857142857,"‚Ä¢ We conduct comprehensive numerical studies on four typical benchmark datasets involving
symmetries, and the results show that our proposed approach significantly outperforms
existing methods."
RELATED WORKS,0.03116883116883117,"2
Related works"
RELATED WORKS,0.033766233766233764,"Previous works on identifying high-quality solutions to ILPs via machine learning techniques mainly
focus on reducing problem sizes. For example, Ding et al. (2020) propose to identify and predict
a subset of decision variables that stay unchanged within the collected solutions. Li & Wu (2022)
formulate MILPs as Markov decision processes and learn to reduce problem sizes via early-fixing."
RELATED WORKS,0.03636363636363636,"It is noteworthy that the emergence of GNNs has had a significant impact on solving ILPs. Gasse et al.
(2019) are the first to propose a bipartite-graph representation of ILPs and pass it to GNNs. Nair et al.
(2020) adopt the same representation scheme and train GNNs to predict the conditional distribution
of solutions, from which they further sample solutions. Rather than directly fixing variables, Han
et al. (2023) conduct search algorithms in a neighborhood centered around an initial point generated
from the predicted distribution. Other works based on GNNs (Sonnerat et al., 2022; Lin et al., 2019;
Khalil et al., 2022; Wu et al., 2021) also illustrate great potential in improving the solving efficiency."
RELATED WORKS,0.03896103896103896,"Limitations of the existing GNN-based approaches are also noticed. Nair et al. (2020); Han et al.
(2023) try to address the multiple solution problem by learning the conditional distribution. Chen
et al. (2022) introduce random features into the bipartite graph representation to differentiate variable
nodes involving symmetries."
RELATED WORKS,0.04155844155844156,"However, none of the existing learning-based approaches explicitly leverage the inherent symmetries
in ILPs to achieve improvements. In contrast, works from mathematical optimization perspectives
suggest that symmetry-handling algorithms exhibit great abilities in solving symmetry-involving
ILPs (Pfetsch & Rehn, 2019). To name a few, such algorithms include orbital fixing (Ostrowski et al.,
2011), tree pruning (Margot, 2002), and lexicographical ordering (Kaibel & Pfetsch, 2008)."
BACKGROUND AND PRELIMINARIES,0.04415584415584416,"3
Background and preliminaries"
ILPS,0.046753246753246755,"3.1
ILPs"
ILPS,0.04935064935064935,An integer linear program (ILP) has a formulation as follows:
ILPS,0.05194805194805195,"min
x
{c‚ä§x|Ax ‚â§b, x ‚ààZn}
(1)"
ILPS,0.05454545454545454,"where x ‚ààZn are integer decision variables, and c ‚ààRn, A ‚ààRm√ón, b ‚ààRm are given coefficients."
SYMMETRY GROUP,0.05714285714285714,"3.2
Symmetry Group"
SYMMETRY GROUP,0.05974025974025974,"Symmetry of ILPs is typically represented by groups. We start with some basic notations and most
of which follow Margot (2009). Denoting the index set by In = {1, 2, . . . , n}, a permutation on In
is a bijective (one-to-one and onto) mapping œÄ : In ‚àí‚ÜíIn. For example, an identity permutation
maps the index set to itself as {œÄ(i) = i}n
i=1 and a cyclic permutation has rotational mapping rules
{œÄ(i) = i + 1}n‚àí1
i=1 and œÄ(n) = 1. Schematic diagrams of these two permutations and other ones are
shown in Figure 1. For brevity, we abuse the notation œÄ a little bit and denote the permutation acting
on a vector y ‚ààRn by rearranging its coordinates, namely œÄ(y) =

yœÄ(1), yœÄ(2), . . . , yœÄ(n)
‚ä§. Let Q
be the set of all feasible solutions of (1) and Sn the set of all permutations on In. Note that Sn is
referred to as the symmetric group, which should not be confused with the symmetry group discussed
as follows."
SYMMETRY GROUP,0.06233766233766234,"Definition 3.1. A symmetry group of (1) is defined as the set of all permutations œÄ that map Q onto
itself, such that each feasible solution is mapped to another feasible solution with the same objective
value, i.e.,
G = {œÄ ‚ààSn : c‚ä§¬Øy = c‚ä§œÄ(¬Øy) and œÄ(¬Øy) ‚ààQ, ‚àÄ¬Øy ‚ààQ}.
(2)"
SYMMETRY GROUP,0.06493506493506493,"ùëõ
‚Ä¶
2
1"
SYMMETRY GROUP,0.06753246753246753,(c) Cyclic permutation
SYMMETRY GROUP,0.07012987012987013,"ùëõ‚àí1
‚Ä¶
2
1
ùëõ"
SYMMETRY GROUP,0.07272727272727272,(d) Reflective permutation
SYMMETRY GROUP,0.07532467532467532,(b) Swapping permutation
SYMMETRY GROUP,0.07792207792207792,"ùëõ
‚Ä¶
2
1"
SYMMETRY GROUP,0.08051948051948052,(a) Identity permutation
SYMMETRY GROUP,0.08311688311688312,"ùëõ‚àí1
‚Ä¶
2
1
ùëõ"
SYMMETRY GROUP,0.08571428571428572,"Figure 1: Permutation examples with directed
edges denoting mapping rules."
SYMMETRY GROUP,0.08831168831168831,"Next, we will delve into three commonly en-
countered symmetries, accompanied by typical
example problems. Since not all variables in an
ILP involve symmetry, we use q ‚â§n to indicate
the size of the symmetry group."
SYMMETRY GROUP,0.09090909090909091,"Symmetric group
The symmetric group, de-
noted by Sq, is the group that consists of all per-
mutations (q! in total) on Iq. Problems with this
kind of symmetry include bin packing (John-
son, 1974) and optimal job scheduling Graham
et al. (1979), etc. An example is illustrated in
Appendix B.0.1."
SYMMETRY GROUP,0.09350649350649351,"Cyclic and dihedral groups
As its name sug-
gests, cyclic symmetry allows elements to be
permuted to their right neighbors, cycling the right-most variables back to the left, e.g., a cyclic (or
rotational) permutation œÅ in Figure 1 (c). The elements of a cyclic group Cq are powers of œÅ, and
|Cq| = q. Problems with cyclic group often have characteristics of rotations or cycles, e.g., periodic
event scheduling problem (Serafini & Ukovich, 1989)."
SYMMETRY GROUP,0.09610389610389611,"Compared to the cyclic group, a dihedral group (denoted as Dq) additionally includes reflective
permutations, which is illustrated in Figure 1 (d). Consequently, Dq comprises a total of 2q distinct
permutations. A typical problem with such symmetry is the circular (or modular) golomb ruler
problem (see Appendix B.0.2)."
CLASSIC SUPERVISED LEARNING FOR SOLUTION PREDICTION,0.0987012987012987,"3.3
Classic supervised learning for solution prediction"
CLASSIC SUPERVISED LEARNING FOR SOLUTION PREDICTION,0.1012987012987013,"A classic solution prediction task based on supervised learning is formulated as follows. Let S be the
space of ILP instances and Y be the label (i.e., optimal solution) space. A model function fŒ∏ : S ‚àí‚ÜíY
parameterized by Œ∏ ‚ààŒò is used to learn a mapping from instances to optimal solutions. Let P(S, Y )
be a distribution over S √ó Y. The performance of the model function is measured by a criterion
called true risk : R(fŒ∏) := EP(S,Y ) [‚Ñì(fŒ∏(s), y)], where ‚Ñì: Y √ó Y ‚àí‚ÜíR+ is a given loss function,
e.g., mean squared error or cross-entropy loss. An intuitive way to improve the model performance is
to minimize the true risk. However, one cannot access all data from distribution P(S, Y ), which
makes it impossible to calculate the true risk. Practically, one can obtain a set of (instance, solution)
pairs called training data D = {(si, yi)}N
i=1 ‚äÜ(S √ó Y)N sampled from P(S, Y ), based on which
define the empirical risk as"
CLASSIC SUPERVISED LEARNING FOR SOLUTION PREDICTION,0.1038961038961039,"r(fŒ∏; D) := 1 N N
X"
CLASSIC SUPERVISED LEARNING FOR SOLUTION PREDICTION,0.10649350649350649,"i=1
‚Ñì(fŒ∏(si), yi) .
(3)"
CLASSIC SUPERVISED LEARNING FOR SOLUTION PREDICTION,0.10909090909090909,"By minimizing the empirical risk, i.e., minŒ∏‚ààŒò r, one aims to approximate the minimization of the
true risk, under the assumption that the training data is a representative sample of the overall data
distribution."
METHODOLOGY,0.11168831168831168,"4
Methodology"
REFORMULATION OF THE LEARNING TASK,0.11428571428571428,"4.1
Reformulation of the learning task"
REFORMULATION OF THE LEARNING TASK,0.11688311688311688,"In Section 3.3, we introduce a classic supervised learning task for general ILPs, which aims at
learning a mapping fŒ∏ from instances to optimal solutions. In this task, a dataset D = {(si, yi)}
is given, and the mapping fŒ∏ is learned by minimizing (3) with Œ∏ as decisions. However, for ILPs
with symmetry, an ILP instance has multiple solutions (let Yi be the set of optimal solutions of i-th
instance). As a consequence, the labels in this task have multiple choices, thus datasets choosing
different optimal solutions as labels {D‚Ä≤ = {(si, y‚Ä≤
i)}N
i=1, ‚àÄy‚Ä≤
i ‚ààYi} are all valid for the learning
task. Empirically, we observe that different D‚Ä≤ can lead to distinct performance, which motivates us
to consider the selection of labels for ILPs with symmetry."
REFORMULATION OF THE LEARNING TASK,0.11948051948051948,"We reformulate the learning task as follows. Firstly, we augment dataset D to dataset Ds =
{(si, yi, Gi)}, where Gi is the symmetry group of i-th instance and œÄi ‚ààGi. Secondly, we define
the symmetry-aware empirical risk as"
REFORMULATION OF THE LEARNING TASK,0.12207792207792208,"rs(fŒ∏, {œÄi}N
i=1; Ds) := 1 N N
X"
REFORMULATION OF THE LEARNING TASK,0.12467532467532468,"i=1
‚Ñì(fŒ∏(si), œÄi(yi)) .
(4)"
REFORMULATION OF THE LEARNING TASK,0.12727272727272726,"Then, the mapping fŒ∏ is learned by minimizing the symmetry-aware risk as minŒ∏,œÄ rs (both Œ∏ and œÄ
as decisions). In contrast to the original task, the symmetry-aware task uses symmetry information by
introducing extra decisions {œÄi}N
i=1, so as to dynamically selecting proper optimal solutions as labels.
There are important differences between the symmetry-aware empirical risk and the classic one:
Proposition 4.1. Let r‚àóand r‚àó
s be the global minimal values of minŒ∏ r and minŒ∏,œÄ rs, respectively.
Then, the following claims hold:"
REFORMULATION OF THE LEARNING TASK,0.12987012987012986,"(i) r‚àó
s ‚â§r‚àó,"
REFORMULATION OF THE LEARNING TASK,0.13246753246753246,"(ii) r‚àó
s < r‚àó, if there exist i, j ‚àà{1, . . . , N}, such that si = sj and yi Ã∏= yj."
REFORMULATION OF THE LEARNING TASK,0.13506493506493505,"Claim (i) always holds since minŒ∏ r is a special case of minŒ∏,œÄ rs when œÄ1, . . . , œÄN are all identity
permutations. Claim (ii) shows a significant advantage of rs compared to r. A non-rigorous proof is
available in Appendix A.1."
AN ALTERNATING MINIMIZATION ALGORITHM,0.13766233766233765,"4.2
An alternating minimization algorithm"
AN ALTERNATING MINIMIZATION ALGORITHM,0.14025974025974025,"The minimization of (4) is challenging due to the discrete nature of œÄ. Motivated by the well-known
block coordinate minimization algorithms (Mangasarian, 1994), we update Œ∏ and œÄ alternately, i.e.,"
AN ALTERNATING MINIMIZATION ALGORITHM,0.14285714285714285,"{œÄk+1
i
}N
i=1 ‚Üêarg min
œÄi‚ààGi rs(fŒ∏k, {œÄi}N
i=1; Ds),
(5)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.14545454545454545,"Œ∏k+1 ‚Üêarg min
Œ∏‚ààŒò rs(fŒ∏, {œÄk+1
i
}N
i=1; Ds).
(6)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.14805194805194805,"Such an alternating mechanism divides the minimization of (4) into two sub-problems: a discrete
optimization in (5) over sets {Gi}N
i=1 and a classic empirical risk minimization in (6). Repeatedly
solving (6) to optimal is unrealistic, thus it is more practical to update Œ∏ by several gradient steps
instead."
AN ALTERNATING MINIMIZATION ALGORITHM,0.15064935064935064,"The sub-problem in (5) is further specified as shown in Section 4.2.1, according to the symmetry
structures in the ILP instances."
AN ALTERNATING MINIMIZATION ALGORITHM,0.15324675324675324,"We summarize the proposed alternating minimization algorithm in Algorithm 1. In the main loop,
{œÄi}N
i=1 are updated first (line 5), after which an inner loop (lines 6-10) is operated to update Œ∏
through a gradient-based method GD, e.g., Adam (Kingma & Ba, 2014). These two updates alternate
until a preset maximum number of epochs K is reached. We finally note that Algorithm 1 can be
easily adapted to a mini-batch version, in which the data can be randomly sampled from Ds."
AN ALTERNATING MINIMIZATION ALGORITHM,0.15584415584415584,"ILPs
ùë†1, ùë†2, ‚Ä¶ , ùë†ùëõ"
AN ALTERNATING MINIMIZATION ALGORITHM,0.15844155844155844,"Bipartite 
representation"
AN ALTERNATING MINIMIZATION ALGORITHM,0.16103896103896104,Graph Neural
AN ALTERNATING MINIMIZATION ALGORITHM,0.16363636363636364,"Networks ùëìùúÉ
Prediction"
AN ALTERNATING MINIMIZATION ALGORITHM,0.16623376623376623,Symmetry-aware empirical risk
AN ALTERNATING MINIMIZATION ALGORITHM,0.16883116883116883,"ùëöùëñùëõ
ùúÉ‚ààŒò,ùúãùëñ‚ààùê∫ùëñ 1
ùëÅ‡∑ç ùëñ=1 ùëÅ"
AN ALTERNATING MINIMIZATION ALGORITHM,0.17142857142857143,"ùìÅùëìùúÉùë†ùëñ, ùúãùëñùë¶ùëñ"
AN ALTERNATING MINIMIZATION ALGORITHM,0.17402597402597403,"Solutions
ùë¶1, ùë¶2, ‚Ä¶ , ùë¶ùëõ"
AN ALTERNATING MINIMIZATION ALGORITHM,0.17662337662337663,"Symmetries
ùê∫1, ùê∫2, ‚Ä¶ , ùê∫ùëõ"
AN ALTERNATING MINIMIZATION ALGORITHM,0.17922077922077922,Update ùúã
AN ALTERNATING MINIMIZATION ALGORITHM,0.18181818181818182,Update ùúÉ
AN ALTERNATING MINIMIZATION ALGORITHM,0.18441558441558442,Alternating updates Data
AN ALTERNATING MINIMIZATION ALGORITHM,0.18701298701298702,Training
AN ALTERNATING MINIMIZATION ALGORITHM,0.18961038961038962,"Model
Downstream Tasks"
AN ALTERNATING MINIMIZATION ALGORITHM,0.19220779220779222,Fix and optimize
AN ALTERNATING MINIMIZATION ALGORITHM,0.19480519480519481,Node Selection
AN ALTERNATING MINIMIZATION ALGORITHM,0.1974025974025974,"Local Branching ‚Ä¶ ‚Ä¶ ‚Ä¶ ùíóùüè
ùíóùíè ùíÑùüè
ùíÑùíé ùë®ùüèùüè ùë®ùíéùüè ùë®ùüèùíè ùë®ùíéùíè 0.9 0.1 ‚Ä¶ 0.2"
AN ALTERNATING MINIMIZATION ALGORITHM,0.2,Figure 2: An overview of the SymILO framework.
AN ALTERNATING MINIMIZATION ALGORITHM,0.2025974025974026,"4.2.1
Optimization over symmetry groups in (5)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.2051948051948052,Algorithm 1 Alternating optimization
AN ALTERNATING MINIMIZATION ALGORITHM,0.2077922077922078,"1: Input: Dataset Ds = {(si, yi, Gi)}N
i=1, K, T
2: Output: Œ∏K"
AN ALTERNATING MINIMIZATION ALGORITHM,0.21038961038961038,"3: Initialize ML model parameter Œ∏1;
4: for k ‚Üê1 to K ‚àí1 do
5:
{œÄk+1
i
}N
i=1 ‚Üêarg minœÄi‚ààGi rs(fŒ∏k, {œÄi}N
i=1; Ds);
6:
ÀúŒ∏1 ‚ÜêŒ∏k;
7:
for t ‚Üê1 to T ‚àí1 do
8:
ÀúŒ∏t+1 ‚ÜêGD(ÀúŒ∏t, ‚àáÀúŒ∏trs(ÀúŒ∏t, {œÄk+1
i
}N
i=1; Ds)) ;
9:
end for
10:
Œ∏k+1 ‚ÜêÀúŒ∏T ;
11: end for"
AN ALTERNATING MINIMIZATION ALGORITHM,0.21298701298701297,"In this section, we investigate the concrete
formulations of the sub-problem in (5) for
the symmetry groups mentioned in Section
3.2 (symmetric group, cyclic and dihedral
groups), and devise algorithms to solve
them."
AN ALTERNATING MINIMIZATION ALGORITHM,0.21558441558441557,"Cyclic and dihedral groups
The cardi-
nality of a cyclic group Cq is q, and it is
2q for a dihedral group Dq. For symmetry
groups with such reasonably small size, a
straightforward and effective way to solve
(5) is to evaluate all possible permutations
and select the one that yields the minimum rs."
AN ALTERNATING MINIMIZATION ALGORITHM,0.21818181818181817,"Symmetric group
The cardinality of a symmetric group is factorially large, |Sq| = q!, so it is
impractical to traverse all permutations. Since œÄ1, . . . , œÄN are not coupled, we can separate them and
solve the N sub-problems individually:"
AN ALTERNATING MINIMIZATION ALGORITHM,0.22077922077922077,"min
œÄi ‚Ñì(fŒ∏(si), œÄi(yi)) , ‚àÄi = 1, . . . , N.
(7)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.22337662337662337,"Without loss of generality, consider an ILP whose variables have a matrix form (e.g., see Appendix
B.0.1), denoted by X ‚ààZp√óq(p ¬∑ q < n), and a symmetric group Sq acting on its column coordinates.
In this case, (7) is equivalent to solve the following binary linear program (BLP),"
AN ALTERNATING MINIMIZATION ALGORITHM,0.22597402597402597,"min
P
‚Ñì

ÀÜX, XP

s.t.
P ‚àà{0, 1}q√óq, P ‚ä§1 = 1, P1 = 1,
(8)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.22857142857142856,"where P is a permutation matrix, ÀÜX is the matrix form of fŒ∏(si), and 1 is an all-one vector. We relax
P to take continuous values between 0 and 1, and get a linear program (LP),"
AN ALTERNATING MINIMIZATION ALGORITHM,0.23116883116883116,"min
P
‚Ñì

ÀÜX, XP

s.t.
P ‚àà[0, 1]q√óq , P ‚ä§1 = 1, P1 = 1
(9)"
AN ALTERNATING MINIMIZATION ALGORITHM,0.23376623376623376,"According to Proposition 4.2, one can solve (9), to get the optimal permutations for the original
problem in (8). It can be done quite efficiently with the aid of off-the-shelf LP solvers, such as Gurobi
Optimization, LLC (2023), CPLEX IBM (2020), etc.
Proposition 4.2. When ‚Ñìis the squared error or binary cross-entropy loss, the optimal solution to (9)
is also an optimal solution to (8). (See the proof in A.2.)"
AN OVERVIEW OF THE SYMILO FRAMEWORK,0.23636363636363636,"4.3
An overview of the SymILO framework"
AN OVERVIEW OF THE SYMILO FRAMEWORK,0.23896103896103896,"In this section, we summarize a novel learning framework (SymILO) that utilizes symmetry for
solving ILPs. An overview is depicted in Figure 2, which consists of two parts: the upper row
connected by green arrows delineates a graph neural network (GNN)-based workflow, and the lower
row connected by red arrows outlines the training process."
AN OVERVIEW OF THE SYMILO FRAMEWORK,0.24155844155844156,"For the GNN-based workflow, an ILP is first converted to a bipartite graph (see appendix C for
details), which is then fed to a GNN model fŒ∏ (see appendix D for details), producing a predicted
solution. Notably, the predicted solution is finally used in downstream tasks for refinement. Due
to the complexity of solving ILPs, existing methods, such as Nair et al. (2020); Ding et al. (2020);
Khalil et al. (2022); Han et al. (2023), often include a post-processing module taking the predicted
solution as an initial point to identify higher-quality solutions. Our approach follows this routine and
integrates certain downstream techniques. Section 5.1 specifies three downstream tasks."
AN OVERVIEW OF THE SYMILO FRAMEWORK,0.24415584415584415,"For the training process, the data used to minimize the symmetry-aware empirical risk rs include
the collected solution yi and the symmetry group Gi of each instance si. Both parameters Œ∏ of the
GNN model and permutations {œÄi}N
i=1 of each solution are optimized via an alternating algorithm
mentioned in Algorithm 1. Given a trained model fŒ∏K, the prediction fŒ∏K(s‚Ä≤) for an unseen instance
s‚Ä≤ is used to guide the downstream tasks in identifying feasible solutions. Note that {œÄi}N
i=1 are
utilized only in the training phase but not in the inference phase."
EXPERIMENTAL SETTINGS,0.24675324675324675,"5
Experimental settings"
EXPERIMENTAL SETTINGS,0.24935064935064935,"In this section, the experimental settings are presented. The corresponding source code is available at
https://github.com/NetSysOpt/SymILO."
DOWNSTREAM TASKS AND BASELINES,0.2519480519480519,"5.1
Downstream tasks and baselines"
DOWNSTREAM TASKS AND BASELINES,0.2545454545454545,"In our experiments, we pass the predictions of GNN models to three downstream tasks, namely fix
and optimize, local branching, and node selection, to identify feasible solutions. For each downstream
task, we choose one existing method as a baseline. The downstream tasks and their corresponding
baselines (in parentheses) are shown below."
DOWNSTREAM TASKS AND BASELINES,0.2571428571428571,"Fix and optimize (ND):
‚ÄúFix and optimize‚Äù refers to a strategy where one first ‚Äúfix‚Äù or set some
variables to specific values and then ‚Äúoptimize‚Äù the remaining variables to find better solutions. The
baseline we choose is ‚ÄúNeural Diving‚Äù (ND) proposed by Nair et al. (2020), a technique using a
graph neural network to generate partial assignments for ILPs, which creates smaller sub-ILPs with
the unassigned variables."
DOWNSTREAM TASKS AND BASELINES,0.2597402597402597,"Local branching (PS):
Local branching is a heuristic method that constructs a linear constraint
based on a given initial solution to the original ILP instance. This constraint restrains the search
space in a region around the initial solution. It can help guide the optimization process toward better
solutions while balancing computational efficiency. Approaches based on this idea include Ding et al.
(2020); Han et al. (2023); Chen et al. (2023) and we select the ‚Äúpredict-and-search‚Äù (PS) framework
proposed by Han et al. (2023) as a baseline."
DOWNSTREAM TASKS AND BASELINES,0.2623376623376623,"Node selection (MIP-GNN):
In branch and bound algorithms, node selection is a process of
choosing the proper nodes to explore next. Effective node selection is crucial for the algorithm‚Äôs
success in solving optimization problems. ‚ÄúMIP-GNN‚Äù (MIP-GNN) proposed by Khalil et al. (2022)
uses GNN prediction to guide node selection and warm-starting, and is selected as another baseline."
BENCHMARK DATASETS,0.2649350649350649,"5.2
Benchmark datasets"
BENCHMARK DATASETS,0.2675324675324675,"We evaluate the proposed framework on four ILP benchmarks with certain symmetry, which consists
of (i) two problems with symmetric groups: the item placement problem (IP) and the steel mill slab
problem (SMSP), (ii) the periodic event scheduling problem (PESP) with cyclic group, and (iii) a
modified variant of PESP (PESPD) which has a dihedral group."
BENCHMARK DATASETS,0.2701298701298701,"The first benchmark IP is from the NeurIPS ML4CO 2021 competition (Gasse et al., 2022). We use
their source code to randomly generate instances with binary variables ranging from 208 to 1050.
Each instance has a symmetric group S4 ‚àºS10. We use 500 instances for our experiments, taking
400 as the training set and the remaining 100 for testing. The SMSP benchmark is from Schaus et al.
(2011), and contains 380 problem instances. We randomly select 304 of them as training data and take
the others as testing data. The instances of this benchmark have 22k‚àº24k binary variables and nearly
10k constraints, with each of them having a symmetric group S111. The last two benchmarks are from"
BENCHMARK DATASETS,0.2727272727272727,"PESPlib Goerigk (2012), a collection of periodic timetabling problems inspired by real-world railway
timetabling settings. Since PESPlib only provides a few instances, which are not sufficient to support
neural network training, we randomly perturb the weights of the provided instances to generate more
data (see Appendix G.3.1 for details). We respectively generate 500 instances for PESP and PESPD,
taking 400 of them as training sets and 100 as testing sets. The symmetry groups of these two datasets
are cyclic groups C5 ‚àºC15 and dihedral groups D5 ‚àºD15, respectively. For all training sets, 30%
instances are used for validation. The average numbers of variables and constraints, as well as the
symmetry groups of each benchmark problem, are summarized in Appendix F.1. Besides, more
details about their ILP formulations and corresponding symmetries are supplemented in Appendix G."
BENCHMARK DATASETS,0.2753246753246753,"These benchmarks only include problem instances. We collect the corresponding solutions using an
ILP solver CPLEX (IBM, 2020). However, solving ILP instances even with moderate sizes to optimal
is extremely expensive. It is more practical to use high-quality solutions as the labels. Therefore, we
run single-thread CPLEX for a time limit of 3,600 seconds and record the best solutions."
TRAINING SETTINGS,0.2779220779220779,"5.3
Training settings"
TRAINING SETTINGS,0.2805194805194805,"All models are trained with a batch size 16 for 50 epochs. The Adam optimizer with a learning rate of
0.001 is used, and other hyperparameters of the optimizer are set to their default values. The model
with the smallest loss on the validation set is used for subsequent evaluations. Other training settings,
such as the loss function and neural architectures, follow the configurations in Han et al. (2023).
More details about the hyper-parameter tuning for the downstream tasks and software resources are
shown in Section E."
EVALUATION METRICS,0.2831168831168831,"5.4
Evaluation metrics"
EVALUATION METRICS,0.2857142857142857,"To compare the prediction performance of the model trained on r and rs, we define the Top-m%
error for evaluation. In addition, another criterion relative primal gap is used to evaluate the final
performance in identifying feasible solutions in different downstream tasks."
EVALUATION METRICS,0.2883116883116883,"Top-m% error:
We use the distance between a rounded prediction and its nearest equivalent
solution as the error. Specifically, given a prediction ÀÜy and its label y, we define the equivalent
solution closest to ÀÜy as Àúy = œÄ‚Ä≤(y), where œÄ‚Ä≤ = arg minœÄ ‚à•ÀÜy ‚àíœÄ(y)‚à•. Then, the Top-m% error is
defined as"
EVALUATION METRICS,0.2909090909090909,"E(m) =
X"
EVALUATION METRICS,0.2935064935064935,"i‚ààM
|Round(ÀÜyi) ‚àíÀúyi|,
(10)"
EVALUATION METRICS,0.2961038961038961,"where M is the index set of m% variables with largest values of |Round(ÀÜyj) ‚àíÀÜyj|. This error
measures the minimum distance between the prediction and all solutions equivalent to the label.
Compared to naive use of the distance P"
EVALUATION METRICS,0.2987012987012987,"i‚ààM |Round(ÀÜyi) ‚àíyi|, (10) can more accurately represent
how close a prediction is to a feasible solution. Since for the naive distance, when Round(ÀÜy) equals
any equivalent solution œÄ(y) Ã∏= y, the distance is greater than 0, while that of (10) is 0."
EVALUATION METRICS,0.3012987012987013,"Relative primal gap:
We also feed the outputs of the models trained through rs to the downstream
tasks mentioned in Section 5.1 to evaluate the quality of the predictions. All the three downstream
approaches incorporate ILP solvers to search for solutions. We run these ILP solvers on a single
thread for a maximum of 800 seconds. Since all the problems used in the experiments are NP-hard,
identifying optimality is highly time-consuming. Thus the metric used in our experiments is relative
primal gap"
EVALUATION METRICS,0.3038961038961039,PG(Àúy) = |c‚ä§Àúy ‚àíc‚ä§y‚àó|
EVALUATION METRICS,0.3064935064935065,"|c‚ä§y‚àó| + œµ ,
(11)"
EVALUATION METRICS,0.3090909090909091,"which measures the relative gap in the objective value of a feasible solution Àúy to that of the best-known
solution y‚àó, and œµ is a small positive value to avoid the numerical issue. Additionally, let Œ≥r and Œ≥rs
respectively be the primal gaps of models trained through r and rs, then an improvement gain of our
approach is calculated as (Œ≥r ‚àíŒ≥rs)/Œ≥r."
NUMERICAL RESULTS,0.3116883116883117,"6
Numerical results"
NUMERICAL RESULTS,0.3142857142857143,"In this section, we present the comparison results on empirical risk r and symmetry-aware one rs. In
addition, primal gaps of SymILO and baselines on three downstream tasks are reported."
NUMERICAL RESULTS,0.3168831168831169,"6.1
On empirical risks and Top-m% error"
NUMERICAL RESULTS,0.3194805194805195,"We denote training and test risks by rtr(¬∑) = r(¬∑; Dtr) and rte = r(¬∑; Dte), respectively, and similarly
use rtr
s and rte
s for symmetry-aware risk. Let f (k) and f (k)
s
be the best classic model and symmetry-
aware model obtained at k-th epoch by training with rtr and rtr
s , respectively. We plot both the
training and test risks versus the number of epochs in Figure 3. As predicted in Proposition 4.1, when
algorithms converge, the classic empirical risk rtr is always greater than symmetry-aware risk rtr
s ."
NUMERICAL RESULTS,0.3220779220779221,"0
50
epoch"
NUMERICAL RESULTS,0.3246753246753247,(a) IP 0.25 0.31 0.38 0.44
NUMERICAL RESULTS,0.32727272727272727,empirical risk
NUMERICAL RESULTS,0.32987012987012987,"0
50
epoch
(b) SMSP 0.01 0.05 0.09 0.14"
NUMERICAL RESULTS,0.33246753246753247,empirical risk
NUMERICAL RESULTS,0.33506493506493507,"0
50
epoch
(c) PESP 0.33 1.22 2.11 3.00"
NUMERICAL RESULTS,0.33766233766233766,empirical risk
NUMERICAL RESULTS,0.34025974025974026,"0
50
epoch
(d) PESPD 1.33 1.89 2.44 3.00"
NUMERICAL RESULTS,0.34285714285714286,empirical risk
NUMERICAL RESULTS,0.34545454545454546,Figure 3: The training and test risks v.s. the number of epochs on four benchmark problems.
NUMERICAL RESULTS,0.34805194805194806,"As shown in Table 1, the symmetry-aware model f (K)
s
always predicts smaller Top-m% errors
in (10) compared to the classic model f (K), demonstrating the usefulness of proposed empirical
symmetry-aware risk in predicting solutions correctly."
NUMERICAL RESULTS,0.35064935064935066,"Table 1: Top-m% errors (‚Üì) of model f (K) and f (K)
s
averaged over different datasets."
NUMERICAL RESULTS,0.35324675324675325,"m%
IP
SMSP
PESP
PESPD"
NUMERICAL RESULTS,0.35584415584415585,"f (K)
f (K)
s
f (K)
f (K)
s
f (K)
f (K)
S
f (K)
f (K)
s"
NUMERICAL RESULTS,0.35844155844155845,"10%
0.8 ¬±0.8
0.4 ¬±0.6
0.6¬± 0.7
0.0¬± 0.0
7.5¬±17
0.1¬±0.2
87.4¬±41
12.7¬±4.8
30%
3.9 ¬±1.5
2.9 ¬±1.3
5.3¬± 2.6
0.1¬± 0.1
44.2¬±35
0.1¬±0.5
275¬±73
81.3¬±24
50%
17.0 ¬±2.4
5.1 ¬±1.7
19.5¬± 5.5
0.6¬± 2.5
52.7¬±35
0.3¬±0.8
422¬±102
223¬±77
70%
46.5 ¬±2.8
36.3 ¬±4.3
47.5¬± 9.8
17.8¬± 6.6
122¬±26
23¬±5.5
638¬±93
486¬±114
90%
82.9 ¬±1.5
76.1 ¬±3.0
103¬± 15
47.0¬± 9.1
1.6k¬±30
212¬±23
854¬±69
848¬±99"
NUMERICAL RESULTS,0.36103896103896105,"Table 2: Time cost for minimizing differ-
ent empirical risks (in seconds)."
NUMERICAL RESULTS,0.36363636363636365,"IP
SMSP
PESP
PESPD
r
5.54
69.43
14.97
16.17
rs
6.01
71.5
15.14
16.46
t
0.029
0.129
0.011
0.018"
NUMERICAL RESULTS,0.36623376623376624,"Moreover, the time costs of minimizing different em-
pirical risks r and rs for a mini batch are shown in
Table 2. Here, t denotes the average time of solving
the permutation decisions per instance. The reported
times for rs include the optimization time t. The ta-
ble illustrates that the alternate training strategy does
not significantly increase the training duration, and
the optimization step over œÄ is executed efficiently."
DOWNSTREAM RESULTS,0.36883116883116884,"6.2
Downstream results"
DOWNSTREAM RESULTS,0.37142857142857144,"The relative primal gaps of different downstream tasks at different solving time are shown in Figure 4,
and the final values at 800 seconds are listed in Table 3. As Figure 4 shows, our proposed empirical
risk significantly improves the performance of different downstream tasks over the primal gap in 800
seconds."
DOWNSTREAM RESULTS,0.37402597402597404,"Note that the node selection task exhibits modest performance in comparison to other tasks; a possible
reason is that it requires runtime interaction to call the callback functions provided by the CPLEX"
DOWNSTREAM RESULTS,0.37662337662337664,"10
200
400
600
800
Solving time (s)"
DOWNSTREAM RESULTS,0.37922077922077924,(a) IP 0.1 0.15 0.2 0.25 0.3 0.35
DOWNSTREAM RESULTS,0.38181818181818183,Primal gap
DOWNSTREAM RESULTS,0.38441558441558443,"10
200
400
600
800
Solving time (s)"
DOWNSTREAM RESULTS,0.38701298701298703,(b) SMSP 0.2 0.4 0.6 0.8 1 1.2
DOWNSTREAM RESULTS,0.38961038961038963,Primal gap
DOWNSTREAM RESULTS,0.3922077922077922,"10
200
400
600
800
Solving time (s)"
DOWNSTREAM RESULTS,0.3948051948051948,(c) PESP 0 1 2 3 4
DOWNSTREAM RESULTS,0.3974025974025974,Primal gap
DOWNSTREAM RESULTS,0.4,"10
200
400
600
800
Solving time (s)"
DOWNSTREAM RESULTS,0.4025974025974026,(d) PESPD 0 2 4 6 8 10
DOWNSTREAM RESULTS,0.4051948051948052,Primal gap
DOWNSTREAM RESULTS,0.4077922077922078,"Figure 4: Relative primal gaps at different times. Three downstream tasks, i.e., fix-and-optimize,
local branching, and node selection, are evaluated with a time limit of 800 seconds. The results of the
same downstream task use the same color. In addition, the relative primal gap of the Tuned CPLEX
running on a single thread is also reported as the blue dashed line."
DOWNSTREAM RESULTS,0.4103896103896104,"Python APIs, which can slow down the whole solving process. However, such a flaw does not affect
the demonstration of the effectiveness of our proposed method."
DOWNSTREAM RESULTS,0.412987012987013,"For the primal gap at 800 seconds shown in Table 3, the models trained through rs significantly
improve all downstream tasks. The performance gain of the model trained through rs is calculated
by computing the relative gaps between our approach‚Äôs gap improvements and that of the baselines.
Average gains over the three downstream tasks are 50.3%, 66.5% and 45.4%, respectively. The
overall results demonstrate the effectiveness of the proposed empirical risk rs. We also provide the
corresponding p-values for the significance of improvements in Appendix F.2."
DOWNSTREAM RESULTS,0.4155844155844156,"Table 3: Average relative primal gaps (‚Üì) of different downstream tasks at 800 second. The values in
this table are averaged over primal gaps of all test data for each benchmark problem. ‚ÄúTuned CPLEX‚Äù
is the result of the tuned CPLEX running on a single thread."
DOWNSTREAM RESULTS,0.41818181818181815,"Dataset
Tuned CPLEX
Fix&optimize
Local branching
Node Selection"
DOWNSTREAM RESULTS,0.42077922077922075,"ND
SymILO
gain(‚Üë)
PS
SymILO
gain(‚Üë)
MIP-GNN
SymILO
gain(‚Üë)"
DOWNSTREAM RESULTS,0.42337662337662335,"IP
0.188
0.201
0.124
38.4%
0.168
0.102
39.4%
0.312
0.190
39.2%
SMSP
0.190
0.300
0.180
40.0%
0.230
0.160
30.4%
1.180
0.740
37.3%
PESP
0.056
0.084
0.050
39.8%
0.306
0.000
100%
1.899
0.280
85.3%
PESPD
3.194
2.389
0.404
83.1%
3.442
0.127
96.3%
3.755
3.006
20%"
DOWNSTREAM RESULTS,0.42597402597402595,"Avg.
50.3%
66.5%
45.4%"
LIMITATIONS AND CONCLUSIONS,0.42857142857142855,"7
Limitations and conclusions"
LIMITATIONS AND CONCLUSIONS,0.43116883116883115,"In conclusion, we propose SymILO, a novel symmetry-aware learning framework for enhancing the
prediction of solutions for integer linear programs by incorporating symmetry into the training process.
Our approach shows significant performance improvements over symmetry-agnostic methods on
benchmark datasets. Despite the significant advancements presented in our symmetry-aware learning
framework, SymILO, several limitations must be acknowledged. Firstly, while we provide realizations
for three commonly encountered symmetry groups‚Äîsymmetric, cyclic, and dihedral‚Äîthe framework
requires specific formulations for optimizing permutations, which limits its immediate applicability
to other symmetry groups not discussed in this work. Secondly, for large-scale problem instances
with extensive and complex symmetry groups, the sub-problems involved in optimizing permutations
can significantly slow down the training process. Enhancing the computational efficiency of our
alternating optimization algorithm for these cases remains a challenge and an area for future research."
LIMITATIONS AND CONCLUSIONS,0.43376623376623374,Acknowledgments
LIMITATIONS AND CONCLUSIONS,0.43636363636363634,"This work was supported by the National Key R&D Program of China under grant 2022YFA1003900.
Akang Wang also acknowledges support from the National Natural Science Foundation of
China (Grant No.
12301416), the Shenzhen Science and Technology Program (Grant No.
RCBS20221008093309021), the Guangdong Basic and Applied Basic Research Foundation (Grant
No. 2024A1515010306) and the Longgang District Special Funds for Science and Technology Innova-
tion (LGKCSDPT2023002). Ruoyu Sun also acknowledges support from the Hetao Shenzhen-Hong
Kong Science and Technology Innovation Cooperation Zone Project (No. HZQSWS-KCCYB-
2024016), the University Development Fund (UDF01001491) at the Chinese University of Hong
Kong, Shenzhen, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artifi-
cial Intelligence (2023B1212010001), and the Guangdong Major Project of Basic and Applied Basic
Research (2023B0303000001). Tsung-Hui Chang acknowledges support from the Shenzhen Science
and Technology Program (Grant No. ZDSYS20230626091302006)."
REFERENCES,0.43896103896103894,References
REFERENCES,0.44155844155844154,"Chen, Y., Gao, W., Ge, D., and Ye, Y. Pre-trained mixed integer optimization through multi-variable
cardinality branching. arXiv preprint arXiv:2305.12352, 2023."
REFERENCES,0.44415584415584414,"Chen, Z., Liu, J., Wang, X., Lu, J., and Yin, W. On representing mixed-integer linear programs by
graph neural networks, 2022."
REFERENCES,0.44675324675324674,"Chen, Z.-L. Integrated production and outbound distribution scheduling: review and extensions.
Operations research, 58(1):130‚Äì148, 2010."
REFERENCES,0.44935064935064933,"Dantzig, G. B. Linear inequalities and related systems. Number 38. Princeton university press, 1956."
REFERENCES,0.45194805194805193,"Ding, J.-Y., Zhang, C., Shen, L., Li, S., Wang, B., Xu, Y., and Song, L. Accelerating primal solution
findings for mixed integer programs based on solution prediction. In Proceedings of the aaai
conference on artificial intelligence, volume 34, pp. 1452‚Äì1459, 2020."
REFERENCES,0.45454545454545453,"Gargani, A. and Refalo, P. An efficient model and strategy for the steel mill slab design problem.
In International Conference on Principles and Practice of Constraint Programming, pp. 77‚Äì89.
Springer, 2007."
REFERENCES,0.45714285714285713,"Gasse, M., Chetelat, D., Ferroni, N., Charlin, L., and Lodi, A. Exact combinatorial optimization with
graph convolutional neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch√©-Buc,
F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/pap
er/2019/file/d14c2267d848abeb81fd590f371d39bd-Paper.pdf."
REFERENCES,0.4597402597402597,"Gasse, M., Bowly, S., Cappart, Q., Charfreitag, J., Charlin, L., Ch√©telat, D., Chmiela, A., Du-
mouchelle, J., Gleixner, A., Kazachkov, A. M., et al. The machine learning for combinatorial
optimization competition (ml4co): Results and insights. In NeurIPS 2021 Competitions and
Demonstrations Track, pp. 220‚Äì231. PMLR, 2022."
REFERENCES,0.4623376623376623,"Goerigk, M. Pesplib‚Äìa benchmark library for periodic event scheduling, 2012."
REFERENCES,0.4649350649350649,"Graham, R. L., Lawler, E. L., Lenstra, J. K., and Kan, A. R. Optimization and approximation in
deterministic sequencing and scheduling: a survey. In Annals of discrete mathematics, volume 5,
pp. 287‚Äì326. Elsevier, 1979."
REFERENCES,0.4675324675324675,"Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.guro
bi.com."
REFERENCES,0.4701298701298701,"Han, Q., Yang, L., Chen, Q., Zhou, X., Zhang, D., Wang, A., Sun, R., and Luo, X. A gnn-guided
predict-and-search framework for mixed-integer linear programming. In The Eleventh International
Conference on Learning Representations, 2023."
REFERENCES,0.4727272727272727,"IBM, I. I. C. O. V20. 1: User‚Äôs manual for cplex. IBM Corp, 2020."
REFERENCES,0.4753246753246753,"Johnson, D. S. Fast algorithms for bin packing. Journal of Computer and System Sciences, 8(3):
272‚Äì314, 1974."
REFERENCES,0.4779220779220779,"Kaibel, V. and Pfetsch, M. Packing and partitioning orbitopes. Mathematical Programming, 114(1):
1‚Äì36, 2008."
REFERENCES,0.4805194805194805,"Khalil, E. B., Morris, C., and Lodi, A. Mip-gnn: A data-driven framework for guiding combinatorial
solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10219‚Äì
10227, 2022."
REFERENCES,0.4831168831168831,"Kingma, D. P. and Ba, J.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.4857142857142857,"Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly,
2(1-2):83‚Äì97, 1955."
REFERENCES,0.4883116883116883,"Li, L. and Wu, B. Learning to accelerate approximate methods for solving integer programming via
early fixing. arXiv preprint arXiv:2207.02087, 2022."
REFERENCES,0.4909090909090909,"Lin, X., Hou, Z. J., Ren, H., and Pan, F. Approximate mixed-integer programming solution with ma-
chine learning technique and linear programming relaxation. In 2019 3rd International Conference
on Smart Grid and Smart Cities (ICSGSC), pp. 101‚Äì107, 2019. doi: 10.1109/ICSGSC.2019.00-11."
REFERENCES,0.4935064935064935,"Liu, L. and Fan, Q. Resource allocation optimization based on mixed integer linear programming in
the multi-cloudlet environment. IEEE Access, 6:24533‚Äì24542, 2018."
REFERENCES,0.4961038961038961,"Luathep, P., Sumalee, A., Lam, W. H., Li, Z.-C., and Lo, H. K. Global optimization method for
mixed transportation network design problem: a mixed-integer linear programming approach.
Transportation Research Part B: Methodological, 45(5):808‚Äì827, 2011."
REFERENCES,0.4987012987012987,"Mangasarian, O. L. Nonlinear programming. SIAM, 1994."
REFERENCES,0.5012987012987012,"Margot, F. Pruning by isomorphism in branch-and-cut. Mathematical Programming, 94:71‚Äì90, 2002."
REFERENCES,0.5038961038961038,"Margot, F. Exploiting orbits in symmetric ilp. Mathematical Programming, 98:3‚Äì21, 2003."
REFERENCES,0.5064935064935064,"Margot, F. Symmetry in integer linear programming. 50 Years of Integer Programming 1958-2008:
From the Early Years to the State-of-the-Art, pp. 647‚Äì686, 2009."
REFERENCES,0.509090909090909,"Nair, V., Bartunov, S., Gimeno, F., Von Glehn, I., Lichocki, P., Lobov, I., O‚ÄôDonoghue, B., Sonnerat,
N., Tjandraatmadja, C., Wang, P., et al. Solving mixed integer programs using neural networks.
arXiv preprint arXiv:2012.13349, 2020."
REFERENCES,0.5116883116883116,"Ostrowski, J., Linderoth, J., Rossi, F., and Smriglio, S. Orbital branching. Mathematical Programming,
126:147‚Äì178, 2011."
REFERENCES,0.5142857142857142,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,
S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024‚Äì8035.
Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-a
n-imperative-style-high-performance-deep-learning-library.pdf."
REFERENCES,0.5168831168831168,"Pfetsch, M. E. and Rehn, T. A computational comparison of symmetry handling methods for mixed
integer programs. Mathematical Programming Computation, 11:37‚Äì93, 2019."
REFERENCES,0.5194805194805194,"Pochet, Y. and Wolsey, L. A. Production planning by mixed integer programming, volume 149.
Springer, 2006."
REFERENCES,0.522077922077922,"Schaus, P., Van Hentenryck, P., Monette, J.-N., Coffrin, C., Michel, L., and Deville, Y. Solving steel
mill slab problems with constraint-based techniques: Cp, lns, and cbls. Constraints, 16:125‚Äì147,
2011."
REFERENCES,0.5246753246753246,"Sch√∂bel, A. A model for the delay management problem based on mixed-integer-programming.
Electronic notes in theoretical computer science, 50(1):1‚Äì10, 2001."
REFERENCES,0.5272727272727272,"Serafini, P. and Ukovich, W. A mathematical model for periodic scheduling problems. SIAM Journal
on Discrete Mathematics, 2(4):550‚Äì581, 1989."
REFERENCES,0.5298701298701298,"Sonnerat, N., Wang, P., Ktena, I., Bartunov, S., and Nair, V. Learning a large neighborhood search
algorithm for mixed integer programs, 2022."
REFERENCES,0.5324675324675324,"Watson, J.-P. and Woodruff, D. L. Progressive hedging innovations for a class of stochastic mixed-
integer resource allocation problems. Computational Management Science, 8(4):355‚Äì370, 2011."
REFERENCES,0.535064935064935,"Wu, Y., Song, W., Cao, Z., and Zhang, J. Learning large neighborhood search policy for integer
programming. Advances in Neural Information Processing Systems, 34:30075‚Äì30087, 2021."
REFERENCES,0.5376623376623376,"Zhang, J., Liu, C., Li, X., Zhen, H.-L., Yuan, M., Li, Y., and Yan, J. A survey for solving mixed
integer programming via machine learning. Neurocomputing, 519:205‚Äì217, 2023."
REFERENCES,0.5402597402597402,"A
Theoretical proofs"
REFERENCES,0.5428571428571428,"A.1
Proposition 4.1"
REFERENCES,0.5454545454545454,"Non-rigorous Proof: Consider the case where s1 = s2, y1 Ã∏= y2, and ‚Ñìis the mean squared error,
and let ÀÜy = fŒ∏(s1) = fŒ∏(s2). Then,"
REFERENCES,0.548051948051948,"r‚àó‚â•min
ÀÜy
1
2
 
‚à•ÀÜy ‚àíy1‚à•2 + ‚à•ÀÜy ‚àíy2‚à•2
= 1"
REFERENCES,0.5506493506493506,4‚à•y1 ‚àíy2‚à•2 > 0.
REFERENCES,0.5532467532467532,"While for the symmetry-aware risk, since s1 and s2 corresponds to identical instances, there must
exist permutations œÄ‚Ä≤
1 and œÄ‚Ä≤
2, such that œÄ‚Ä≤
1(y1) = œÄ‚Ä≤
2(y2). Consequently,"
REFERENCES,0.5558441558441558,"r‚àó
s = min
ÀÜy
‚à•ÀÜy ‚àíœÄ‚Ä≤
1(y1)‚à•2 + ‚à•ÀÜy ‚àíœÄ‚Ä≤
2(y2)‚à•2 = 0 < r‚àó."
REFERENCES,0.5584415584415584,"A.2
Proposition 4.2"
REFERENCES,0.561038961038961,"Proof: When the loss function ‚Ñì(¬∑, ¬∑) is the squared error (SE) or the binary cross-entropy loss (BCE),"
REFERENCES,0.5636363636363636,"‚ÑìSE( ÀÜX, XP) = ‚à•ÀÜX ‚àíXP‚à•2
F
(12)"
REFERENCES,0.5662337662337662,"= tr( ÀÜX‚ä§ÀÜX ‚àí2 ÀÜX‚ä§XP + P ‚ä§X‚ä§XP)
(13)"
REFERENCES,0.5688311688311688,"= tr( ÀÜX‚ä§ÀÜX ‚àí2 ÀÜX‚ä§XP + X‚ä§X),
(14)"
REFERENCES,0.5714285714285714,"‚ÑìBCE( ÀÜX, XP) = ‚àí
X"
REFERENCES,0.574025974025974,"j,k
([XP]jk log ÀÜXjk + (1 ‚àí[XP]jk) log(1 ‚àíÀÜXjk)),
(15)"
REFERENCES,0.5766233766233766,"equations (13) to (14) hold for permutations of a matrix‚Äôs rows and columns don not change its
Frobenius norm, i.e., tr(P ‚ä§X‚ä§XP) = ‚à•XP‚à•2
F = ‚à•X‚à•2
F = tr(X‚ä§X). (14) and (15) show that
these two loss functions are linear w.r.t P, with which (8) becomes a linear assignment problem
Kuhn (1955). It is easy to verify that the constriant matrix of a linear assignment problem is totally
unimodular‚Äî-it satisfies the four conditions of Hoffman and Gale (see Page 252 in Dantzig (1956)),
thus an optimal solution of the relaxed problem (9) must be integral as well, namely an optimal
solution to problem (8)."
REFERENCES,0.5792207792207792,"B
ILP examples with different symmetry group"
REFERENCES,0.5818181818181818,"Example B.0.1. Consider a bin packing problem, in which there are three items I = {1, 2, 3} with
sizes {a1 = 1, a2 = 2, a3 = 3} and three identical bins J = {1, 2, 3} with capacity B = 3. Items
are packed into bins, and it is required to use a minimum number of bins without exceeding the
capacity. The specific formulation is as follows:
min
xij,yj‚àà{0,1} y1 + y2 + y3"
REFERENCES,0.5844155844155844,"a1x1j + a2x2j + a3x3j ‚â§Byj,
‚àÄj ‚ààJ
(16a)
xi1 + xi2 + xi3 = 1,
‚àÄi ‚ààI
(16b)
where yj = 1 denotes j-th bin is used and xij = 1 denotes i-th item is placed in j-th bin. 3 1 2
3
1 2
3 1 2
‚Ä¶"
REFERENCES,0.587012987012987,Figure 5: Equivalent solutions of Example B.0.1.
REFERENCES,0.5896103896103896,"Since all bins are identical, arbitrarily swapping them does not change the feasibility and the objective
value, e.g., the different assignments shown in Figure 5 are all equivalent. Specifically, assume X ‚âú Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.5922077922077922,"y1
y2
y3
x11
x12
x13
x21
x22
x23
x31
x32
x33 Ô£π Ô£∫Ô£ª= Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.5948051948051948,"1
1
0
0
1
0
0
1
0
1
0
0 Ô£π Ô£∫Ô£ª"
REFERENCES,0.5974025974025974,"is an optimal solution to the problem (16), then X = Ô£±
Ô£¥
Ô£≤ Ô£¥
Ô£≥ Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.6,"1
0
1
0
0
1
0
0
1
1
0
0 Ô£π Ô£∫Ô£ª, Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.6025974025974026,"1
1
0
1
0
0
1
0
0
0
1
0 Ô£π Ô£∫Ô£ª, Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.6051948051948052,"1
0
1
1
0
0
1
0
0
0
0
1 Ô£π Ô£∫Ô£ª, Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.6077922077922078,"0
1
1
0
0
1
0
0
1
0
1
0 Ô£π Ô£∫Ô£ª, Ô£Æ Ô£ØÔ£∞"
REFERENCES,0.6103896103896104,"0
1
1
0
1
0
0
1
0
0
0
1 Ô£π Ô£∫Ô£ª Ô£º
Ô£¥
Ô£Ω Ô£¥
Ô£æ"
REFERENCES,0.612987012987013,"are all equivalent solutions to X.
Formally, this problem has a symmetric group S3
=
{(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)} w.r.t. its bin numbers J.
Example B.0.2. Given a circle with circumference 8, place 3 ticks at integer points around the circle
such that all distances between inter-ticks along the circumference are distinct. The formulation of
this problem is as follows:"
REFERENCES,0.6155844155844156,"min
x1,x2,x30"
REFERENCES,0.6181818181818182,"s.t. yij = |xi ‚àíxj|,
‚àÄ(i, j) ‚ààS,
(17a)
dij = min{yij, 8 ‚àíyij},
‚àÄ(i, j) ‚ààS,
(17b)
d12 Ã∏= d13, d12 Ã∏= d23, d13 Ã∏= d23
‚àÄ(i, j) ‚ààS,
(17c)"
REFERENCES,0.6207792207792208,"where S = {(1, 2), (1, 3), (2, 3)}, x1, x2, x3 ‚àà{1, 2, 3, 4, 5, 6, 7, 8} denote the positions of each
tick, dij are distances between ticks i and j with auxiliary variables yij. The constraints in this
formulation are nonlinear, we linearize them by big-M methods. Equalities (17a) can be linearized by
introducing auxiliary variables aij ‚àà{0, 1}, ‚àÄ(i, j) ‚ààS as
yij ‚â•xi ‚àíxj,
‚àÄ(i, j) ‚ààS,
(18a)
yij ‚â•xj ‚àíxi,
‚àÄ(i, j) ‚ààS,
(18b)
yij ‚â§xi ‚àíxj + 8 ¬∑ aij,
‚àÄ(i, j) ‚ààS,
(18c)
yij ‚â§xj ‚àíxi + 8 ¬∑ (1 ‚àíaij),
(18d)
when xi ‚â•xj, aij = 0, otherwise aij = 1. Similarly, equalities (17b) are equivalent to
dij ‚â§yij,
‚àÄ(i, j) ‚ààS,
(19a)
dij ‚â§8 ‚àíyij,
‚àÄ(i, j) ‚ààS,
(19b)
dij ‚â•yij ‚àí8 ¬∑ mij,
‚àÄ(i, j) ‚ààS,
(19c)
dij ‚â•8 ‚àíyij ‚àí8 ¬∑ (1 ‚àímij),
‚àÄ(i, j) ‚ààS,
(19d)
where mij ‚àà{0, 1}, ‚àÄ(i, j) ‚ààS are auxiliary variables, with mij = 0 when yij ‚â§8 ‚àíyij."
REFERENCES,0.6233766233766234,The not-equal constraints (17c) can be linearized by
REFERENCES,0.625974025974026,"dij ‚â•dk‚Ñì+ 1 ‚àí8 ¬∑ tijk‚Ñì,
‚àÄ(i, j, k, ‚Ñì) ‚ààK,
(20a)
dk‚Ñì‚â•dij + 1 ‚àí8 ¬∑ (1 ‚àítijk‚Ñì),
‚àÄ(i, j, k, ‚Ñì) ‚ààK,
(20b)"
REFERENCES,0.6285714285714286,"where K = {(1, 2, 1, 3), (1, 2, 2, 3), (1, 3, 2, 3)}.
By introducing auxiliary variables tijk‚Ñì‚àà
{0, 1}, ‚àÄ(i, j, k, ‚Ñì) ‚ààK, we have dij ‚â•dk‚Ñì+ 1 if tijk‚Ñì= 1, otherwise dij ‚â§dk‚Ñì‚àí1, i.e.,
dij Ã∏= dk‚Ñì."
REFERENCES,0.6311688311688312,"Assume {x1 = ¬Øx1, x2 = ¬Øx2, x3 = ¬Øx3} is a feasible solution of this problem and let [¬∑]T denote
the mod ‚àíT operation, then it‚Äôs easy to verify that {xi = [¬Øxi + b]8}3
i=1 (rotation) and its reverse
{xi = [(8 ‚àí¬Øxi) + b]8}3
i=1, ‚àÄb ‚ààZ (reflection of the rotation) are both equivalent feasible solutions.
It is more intuitive to see the illustration in Figure 6, rotation and reflection acting on the ticks do not
change their corresponding distances."
REFERENCES,0.6337662337662338,"When representing x1, x2, x3 by binary variables zip ‚àà{0, 1}, ‚àÄi ‚àà1, 2, 3, ‚àÄp ‚àà{1, . . . , 8}: xi ="
X,0.6363636363636364,"8
X"
X,0.638961038961039,"p
p ¬∑ zip,
‚àÄi ‚àà{1, 2, 3},
(21a)"
X,0.6415584415584416,"8
X"
X,0.6441558441558441,"p
zip = 1,
‚àÄi ‚àà{1, 2, 3}
(21b)"
X,0.6467532467532467,"the modulo symmetry leads to a dihedral group D8 along the p dimension of zip. Specifically, let Z
be a feasible solution with its (i, p)-th entry as the value of zip, then any permutation œÄ ‚ààD8 acting
on the columns of Z yields another equivalent solution

Z:œÄ(1), . . . , Z:œÄ(8)

. 1 2 3 4
5
6 7 8 1 2 3 4
5
6 7 8 1 2 3 4
5
6 7"
"ROTATION
REFLECTION",0.6493506493506493,"8
rotation
reflection"
"ROTATION
REFLECTION",0.6519480519480519,Figure 6: Equivalent solutions of Example B.0.2.
"ROTATION
REFLECTION",0.6545454545454545,"C
Bipartite graph representation for MILP"
"ROTATION
REFLECTION",0.6571428571428571,"Gasse et al. (2019) proposed to represent a MILP (also works for ILP) by a bipartite graph G =
(V, C, E) with two disjoint sets of nodes, V = {v1, v2, . . . , vn} and C = {c1, c2, . . . , cm}, denoting
the decision variables and constraints in Problem (1), respectively. And E = {Ajk|Ajk Ã∏= 0, cj ‚àà
C, vk ‚ààV } is the set of weighted edges connecting variable nodes and constraint nodes, where
A is the coefficient matrix in Problem (1). Each node has a feature vector vk or cj describing the
information of the variables or constraints. For example, in our experiments, these features include
variable types (continuous or binary), variable positions, lower and upper bounds, right-hand side
coefficients, constraint types (=, ‚â§, ‚â•), etc."
"ROTATION
REFLECTION",0.6597402597402597,"D
Graph convolutional neural network"
"ROTATION
REFLECTION",0.6623376623376623,"In the graph convolutional neural network (GCNN)-based approach proposed by Gasse et al. (2019),
a bipartite graph G = (V, C, E) (with node features c(0)
j
= cj, v(0)
k
= vk, and edge features Ajk) is
taken as the input. Stacked layers are applied to aggregate information from neighbors and update
node embeddings. Each layer has two consecutive half convolutions computed as"
"ROTATION
REFLECTION",0.6649350649350649,"c(l)
j
= f (l)
c Ô£´"
"ROTATION
REFLECTION",0.6675324675324675,"Ô£≠c(l‚àí1)
j
,
X"
"ROTATION
REFLECTION",0.6701298701298701,"j:(j,k)‚ààE
g(l)
c

c(l‚àí1)
j
, v(l‚àí1)
k
, Ajk

Ô£∂"
"ROTATION
REFLECTION",0.6727272727272727,"Ô£∏,
(22)"
"ROTATION
REFLECTION",0.6753246753246753,"v(l)
k
= f (l)
v Ô£´"
"ROTATION
REFLECTION",0.6779220779220779,"Ô£≠v(l‚àí1)
k
,
X"
"ROTATION
REFLECTION",0.6805194805194805,"k:(j,k)‚ààE
g(l)
v

c(l)
j , v(l‚àí1)
k
, Ajk

Ô£∂"
"ROTATION
REFLECTION",0.6831168831168831,"Ô£∏,
(23)"
"ROTATION
REFLECTION",0.6857142857142857,"where l ‚àà{0, ¬∑ ¬∑ ¬∑ , L} denotes the layer index, f (l)
c
and g(l)
c
are non-linear transformations gathering
information from variable nodes and update on constraint nodes, f (l)
v
and g(l)
v
are on the contrary. All
of these four transformations are two-layer perceptrons with ReLU activations. Lastly, another two-
layer perceptron fout with sigmoid activation is used to convert the final embeddings to the predictions
of integer variables by ÀÜxk = Sigmoid(fout(v(L)
k
)). We denote fŒ∏ as the GNN parameterized by Œ∏,
and the output vector for the discrete part as ÀÜx = fŒ∏(G) in the remaining sections."
"ROTATION
REFLECTION",0.6883116883116883,"E
Detailed experimental settings"
"ROTATION
REFLECTION",0.6909090909090909,"E.1
Hyper-parameter tuning"
"ROTATION
REFLECTION",0.6935064935064935,"The experiments involved three downstream tasks: fix&optimize, local branching, and node selection.
For the first two, we utilized grid search for hyperparameter tuning. In fix&optimize, we adjusted Œ±,
the fraction of variables to fix, exploring values from 0.1 to 0.9. For local branching, we varied Œ≤, the
percentage of variables in the local branching constraint, within the same range. For node selection,
we adhered to default settings as stated in Khalil et al. (2022). In Table E.1, we report the optimal"
"ROTATION
REFLECTION",0.6961038961038961,"hyperparameters for each task and dataset, ensuring clarity and aiding in the reproducibility of our
work."
"ROTATION
REFLECTION",0.6987012987012987,Table 4: Hyper-parameters for different down-stream tasks
"ROTATION
REFLECTION",0.7012987012987013,"Dataset
r
rs
Œ±
Œ≤
Œ±
Œ≤"
"ROTATION
REFLECTION",0.7038961038961039,"IP
0.1
0.2
0.1
0.1
SMSP
0.5
0.5
0.5
0.6
PESP
0.1
0.1
0.3
0.4
PESPD
0.1
0.1
0.3
0.2"
"ROTATION
REFLECTION",0.7064935064935065,"E.2
Computational resources and software"
"ROTATION
REFLECTION",0.7090909090909091,"All evaluations are performed under the same configuration. The evaluation machine has one AMD
EPYC 7H12 64-Core Processor @ 2.60GHz, 256GB RAM, and one NVIDIA GeForce RTX 3080.
CPLEX 22.2.0 and PyTorch 2.0.1 (Paszke et al., 2019) are utilized in our experiments. The time
limit for running each experiment is set to 800 seconds since a tail-off of solution qualities was often
observed after that."
"ROTATION
REFLECTION",0.7116883116883117,"F
Other supplements"
"ROTATION
REFLECTION",0.7142857142857143,"F.1
Dataset details"
"ROTATION
REFLECTION",0.7168831168831169,"Table 5: More information about benchmark problems include average number of variables (‚Äúbin.‚Äù
for bianry while ‚Äúint.‚Äù for integer) and constraints, as well as symmetry groups."
"ROTATION
REFLECTION",0.7194805194805195,"problem
# of Var.
# of Cons.
symmetry
IP
208 ‚àº1050 bin.
46 ‚àº196
S4 ‚àºS10
SMSP
22k ‚àº24k bin.
20k ‚àº22k
S111
PESP
5k ‚àº15k int.
7k ‚àº21k
C5 ‚àºC15
PESPD
5k ‚àº15k int.
10k ‚àº30k
D5 ‚àºD15"
"ROTATION
REFLECTION",0.7220779220779221,"F.2
p-values for the significance of improvement"
"ROTATION
REFLECTION",0.7246753246753247,"We use the paired-sample T-test in MATLAB and report p-values in the following table. A p-value
less than 0.05 means the mean difference (improvement) is significant. It shows that our proposed
framework is effective in finding a better solution."
"ROTATION
REFLECTION",0.7272727272727273,Table 6: p-values of paired-sample T-tests for the difference of means of the relative primal gaps
"ROTATION
REFLECTION",0.7298701298701299,"p-value
Fix and Optimize
Local Branching
Node Selection"
"ROTATION
REFLECTION",0.7324675324675325,"IP
0.0012352
0.0007626
0.0044869
SMSP
0.0000441
0.0007752
0.0000026
PESP
0.0589491
0.0585871
0.0012326
PESPD
0.0000002
0.0000054
0.0088221"
"ROTATION
REFLECTION",0.7350649350649351,"G
Problem formulation and their corresponding symmetry group"
"ROTATION
REFLECTION",0.7376623376623377,"G.1
IP"
"ROTATION
REFLECTION",0.7402597402597403,"There are I items, J bins, and K resource types. Each item i has a fixed resource requirement aik
for each resource type k. Each bin j has a fixed capacity bk for each resource type k. The goal is to"
"ROTATION
REFLECTION",0.7428571428571429,"place all items in bins, while minimizing the imbalance of the resources used across all bins. This
problem has a formulation as follows:"
"ROTATION
REFLECTION",0.7454545454545455,"min
x,y,z X j‚ààJ X"
"ROTATION
REFLECTION",0.7480519480519481,"k‚ààK
Œ±kyjk +
X"
"ROTATION
REFLECTION",0.7506493506493507,"k‚ààK
Œ≤kzk"
"ROTATION
REFLECTION",0.7532467532467533,"s.t.
X"
"ROTATION
REFLECTION",0.7558441558441559,"j‚ààJ
xij = 1
‚àÄi ‚ààI
(24a) X"
"ROTATION
REFLECTION",0.7584415584415585,"i‚ààI
aikxij ‚â§bk
‚àÄj ‚ààJ, ‚àÄk ‚ààK
(24b) X"
"ROTATION
REFLECTION",0.7610389610389611,"i‚ààI
dikxij + yjk ‚â•1
‚àÄj ‚ààJ, ‚àÄk ‚ààK
(24c)"
"ROTATION
REFLECTION",0.7636363636363637,"yjk ‚â§zk
‚àÄj ‚ààJ, ‚àÄk ‚ààK
(24d)
xij ‚àà{0, 1}
‚àÄi ‚ààI, ‚àÄj ‚ààJ
(24e)
yjk ‚â•0
‚àÄj ‚ààJ, ‚àÄk ‚ààK
(24f)"
"ROTATION
REFLECTION",0.7662337662337663,"where xij = 1 denotes assigning item i to bin j, dik is normalized resource requirement for each
item, yjk and zk are implicit decision variables to track the imbalance of the resources. Since each
bin in this problem is identical (i.e., with the same capacity), reordering bins would not change a
feasible solution‚Äôs feasibility and objective value. This problem naturally has a symmetric group
S|J| w.r.t. the ordering of bins J. Specifically, let X ‚àà{0, 1}|I|√ó|J| be a feasible solution of an IP
instance with its (i, j)-th entry as the value of variable xij. Then arbitrary permutation œÄ ‚ààS|J|
acting on its columns {X:j, ‚àÄj ‚ààJ} yields an equivalent solution

X:œÄ(1), X:œÄ(2), . . . , X:œÄ(|J|)

."
"ROTATION
REFLECTION",0.7688311688311689,"G.2
SMSP"
"ROTATION
REFLECTION",0.7714285714285715,"Given order set O, and slab set S. Color set C, and slab weights Q = {u0 = 0, u1, u2, ..., uk}. u0
denotes unused slab. The ILP formulation of SMSP used in our experiments is from (Gargani &
Refalo, 2007) as"
"ROTATION
REFLECTION",0.7740259740259741,"min
x,y,z X"
"ROTATION
REFLECTION",0.7766233766233767,"s‚ààS,q‚ààQ
q √ó yqs"
"ROTATION
REFLECTION",0.7792207792207793,"s.t.
X"
"ROTATION
REFLECTION",0.7818181818181819,"o‚ààO
xos = 1
‚àÄs ‚ààS,
(25a) X"
"ROTATION
REFLECTION",0.7844155844155845,"q‚ààQ
yqs = 1
‚àÄs ‚ààS,
(25b) X"
"ROTATION
REFLECTION",0.787012987012987,"o‚ààO
woxos ‚â§
X"
"ROTATION
REFLECTION",0.7896103896103897,"q‚ààQ
q √ó yqs
‚àÄs ‚ààS,
(25c)"
"ROTATION
REFLECTION",0.7922077922077922,"xos ‚â§zcos
‚àÄo ‚ààO, s ‚ààS,
(25d)
X"
"ROTATION
REFLECTION",0.7948051948051948,"c‚ààC
zcs ‚â§2
‚àÄs ‚ààS,
(25e)"
"ROTATION
REFLECTION",0.7974025974025974,"xos, yqs, zcs ‚àà{0, 1}
‚àÄo ‚ààO, s ‚ààS, c ‚ààC, q ‚ààQ.
(25f)"
"ROTATION
REFLECTION",0.8,"G.3
PESP"
"ROTATION
REFLECTION",0.8025974025974026,"Periodic event scheduling problem involves determining optimal schedules for a set of events that
occur repeatedly over a fixed period, such as bus or train departures. Consider a set of events E. For
each event i ‚ààE we would like to schedule a time ti ‚àà{1, . . . , T ‚àí1}, where T is the periodic
length. Besides, a set of activities A ‚äÜE √ó E connect events with each other. Each activity a ‚ààA
has a lower bound ‚Ñìa ‚ààN, an upper bound ua ‚ààN, and a weight wa. The goal is to minimize the
weighted sum of the slack ya of all activities, while ensuring all activity slacks are within [0, ua ‚àí‚Ñìa]."
"ROTATION
REFLECTION",0.8051948051948052,"It can be formulated as: min
t X"
"ROTATION
REFLECTION",0.8077922077922078,"a‚ààA
wa(ya + ‚Ñìa)"
"ROTATION
REFLECTION",0.8103896103896104,"s.t. ya = [tj ‚àíti]T
‚àÄa = (i, j) ‚ààA,
(26a)
0 ‚â§ya ‚â§ua ‚àí‚Ñìa
‚àÄa ‚ààA,
(26b)
ti ‚àà{0, . . . , T ‚àí1},
‚àÄi ‚ààE
(26c)"
"ROTATION
REFLECTION",0.812987012987013,"where [¬∑]T denotes the modulo operation, which enforces the periodic nature. It is modeled as
[tj ‚àíti]T ‚âútj ‚àíti + zaT by introducing additional implicit variables {za ‚ààN}. Due to the
existence of modulo operation, we can regard {ti, ‚àÄi ‚ààE} as frames in a clock with intervals
[0, . . . , T ‚àí1]. If all ti rotate the same angles simultaneously, then the activity slacks ya remain
unchanged. In our experimentation, we substitute ti by ti = P"
"ROTATION
REFLECTION",0.8155844155844156,k(k ‚àí1)¬∑xik and P
"ROTATION
REFLECTION",0.8181818181818182,"k xik = 1, where
xik ‚àà{0, 1}, ‚àÄi ‚ààE, k ‚àà{1, T}. Let X ‚àà{0, 1}|E|√óT denotes a feasible solution with its (i, k)-th
entry as the value of variable xik, then this problem has a cyclic group CT w.r.t. the column indices
of X, i.e., any permutation œÄ ‚ààCT acting on the columns of X yields an equivalent feasible solution
[X:œÄ(1), . . . , X:œÄ(T )]."
"ROTATION
REFLECTION",0.8207792207792208,"G.3.1
Data generation by perturbation"
"ROTATION
REFLECTION",0.8233766233766234,"As mentioned above, a PESP instance has a set of events E and a set of activities A ‚äÜE √ó E
connecting events with each other. Each activity has a weight wa. The goal is to assign an appropriate
time ti to each event i ‚ààE to meet some constraints while minimizing the total time slack weighted
by {wa, a ‚ààA}. These weights heavily impact the time assignment. We perturb these weights to
generate new instances by introducing Gaussian noises, i.e., w‚Ä≤
a = wa + na, where na ‚àºN(¬µ =
wa, œÉ = 0.1 ‚àówa)."
"ROTATION
REFLECTION",0.825974025974026,NeurIPS Paper Checklist
CLAIMS,0.8285714285714286,1. Claims
CLAIMS,0.8311688311688312,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?"
CLAIMS,0.8337662337662337,Answer: [Yes]
CLAIMS,0.8363636363636363,Justification: Please refer to Abstract and Section 1.
CLAIMS,0.8389610389610389,Guidelines:
CLAIMS,0.8415584415584415,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8441558441558441,2. Limitations
LIMITATIONS,0.8467532467532467,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8493506493506493,Answer: [Yes]
LIMITATIONS,0.8519480519480519,Justification: Please refer to Section 7.
LIMITATIONS,0.8545454545454545,Guidelines:
LIMITATIONS,0.8571428571428571,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8597402597402597,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8623376623376623,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8649350649350649,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.8675324675324675,"Justification: See Appendix A
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8701298701298701,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.8727272727272727,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Please refer to Section 5 and Appendix E.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8753246753246753,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
THEORY ASSUMPTIONS AND PROOFS,0.8779220779220779,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
THEORY ASSUMPTIONS AND PROOFS,0.8805194805194805,"Answer: [Yes]
Justification: We provide a code link in Section 5.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8831168831168831,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu
blic/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
THEORY ASSUMPTIONS AND PROOFS,0.8857142857142857,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Section 5.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8883116883116883,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
THEORY ASSUMPTIONS AND PROOFS,0.8909090909090909,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report the p-values in Appendix F.2.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.8935064935064935,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
THEORY ASSUMPTIONS AND PROOFS,0.8961038961038961,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8987012987012987,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9012987012987013,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9038961038961039,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9064935064935065,Justification: See Appendix E.2.
EXPERIMENTS COMPUTE RESOURCES,0.9090909090909091,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9116883116883117,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.9142857142857143,9. Code Of Ethics
CODE OF ETHICS,0.9168831168831169,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9194805194805195,Answer: [Yes]
CODE OF ETHICS,0.922077922077922,Justification:The research presented in this paper adheres to the NeurIPS Code of Ethics.
CODE OF ETHICS,0.9246753246753247,Guidelines:
CODE OF ETHICS,0.9272727272727272,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9298701298701298,10. Broader Impacts
BROADER IMPACTS,0.9324675324675324,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.935064935064935,Answer: [NA]
BROADER IMPACTS,0.9376623376623376,"Justification: This paper presents work whose goal is to advance the field of Machine
Learning. There are many potential societal consequences of our work, none of which we
feel must be specifically highlighted here."
BROADER IMPACTS,0.9402597402597402,Guidelines:
BROADER IMPACTS,0.9428571428571428,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.9454545454545454,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.948051948051948,11. Safeguards
SAFEGUARDS,0.9506493506493506,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9532467532467532,Answer: [NA]
SAFEGUARDS,0.9558441558441558,Justification: Our paper does not present any such risks.
SAFEGUARDS,0.9584415584415584,Guidelines:
SAFEGUARDS,0.961038961038961,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9636363636363636,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9662337662337662,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9688311688311688,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9714285714285714,Justification: Please refer to Section 5.
LICENSES FOR EXISTING ASSETS,0.974025974025974,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9766233766233766,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9792207792207792,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9818181818181818,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: See the code link in Section 5.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9844155844155844,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.987012987012987,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]"
LICENSES FOR EXISTING ASSETS,0.9896103896103896,"Justification: Our paper does not involve any crowdsourcing or research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9922077922077922,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]"
LICENSES FOR EXISTING ASSETS,0.9948051948051948,"Justification: Our paper does not involve any crowdsourcing or research with human subjects.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9974025974025974,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
