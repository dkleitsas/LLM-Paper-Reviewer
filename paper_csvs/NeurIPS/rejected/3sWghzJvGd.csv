Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006600660066006601,"World models have recently emerged as a promising approach for reinforcement
1"
ABSTRACT,0.0013201320132013201,"learning (RL), as evidenced by its stimulating successes that world model based
2"
ABSTRACT,0.0019801980198019802,"agents achieve state-of-the-art performance on a wide range of tasks in empirical
3"
ABSTRACT,0.0026402640264026403,"studies. The primary goal of this study is to obtain a deep understanding of the mys-
4"
ABSTRACT,0.0033003300330033004,"terious generalization capability of world models, based on which we devise new
5"
ABSTRACT,0.0039603960396039604,"methods to enhance it further. Thus motivated, we develop a stochastic differential
6"
ABSTRACT,0.0046204620462046205,"equation formulation by treating the world model learning as a stochastic dynamic
7"
ABSTRACT,0.005280528052805281,"system in the latent state space, and characterize the impact of latent representation
8"
ABSTRACT,0.005940594059405941,"errors on generalization, for both cases with zero-drift representation errors and
9"
ABSTRACT,0.006600660066006601,"with non-zero-drift representation errors. Our somewhat surprising findings, based
10"
ABSTRACT,0.007260726072607261,"on both theoretic and experimental studies, reveal that for the case with zero drift,
11"
ABSTRACT,0.007920792079207921,"modest latent representation errors can in fact function as implicit regularization
12"
ABSTRACT,0.008580858085808581,"and hence result in generalization gain. We further propose a Jacobian regulariza-
13"
ABSTRACT,0.009240924092409241,"tion scheme to mitigate the compounding error propagation effects of non-zero
14"
ABSTRACT,0.009900990099009901,"drift, thereby enhancing training stability and generalization. Our experimental
15"
ABSTRACT,0.010561056105610561,"results corroborate that this regularization approach not only stabilizes training but
16"
ABSTRACT,0.011221122112211221,"also accelerates convergence and improves performance on predictive rollouts.
17"
INTRODUCTION,0.011881188118811881,"1
Introduction
18"
INTRODUCTION,0.012541254125412541,"Model-based reinforcement learning (RL) has emerged as a promising learning paradigm to improve
19"
INTRODUCTION,0.013201320132013201,"sample efficiency by enabling agents to exploit a learned model for the physical environment. Notably,
20"
INTRODUCTION,0.013861386138613862,"in recent works [14, 13, 15, 16, 21, 10, 32, 22] on world models, an RL agent learns the latent
21"
INTRODUCTION,0.014521452145214522,"dynamics model of the environment, based on the observations and action signals, and then optimizes
22"
INTRODUCTION,0.015181518151815182,"the policy over the learned dynamics model. Different from conventional approaches, world-model
23"
INTRODUCTION,0.015841584158415842,"based RL takes an end-to-end learning approach, where the building blocks (such as dynamics model,
24"
INTRODUCTION,0.0165016501650165,"perception and action policy) are trained and optimized to achieve a single overarching goal, offering
25"
INTRODUCTION,0.017161716171617162,"significant potential to improve generalization capability. For example, DreamerV2 and DreamerV3
26"
INTRODUCTION,0.01782178217821782,"achieve great progress in mastering diverse tasks involving continuous and discrete actions, image-
27"
INTRODUCTION,0.018481848184818482,"based inputs, and both 2D and 3D environments, thereby facilitating robust learning across unseen
28"
INTRODUCTION,0.01914191419141914,"task domains [14, 13, 15]. Recent empirical studies have also demonstrated the capacity of world
29"
INTRODUCTION,0.019801980198019802,"models to generalize to unseen states in complex environments, such as autonomous driving [19].
30"
INTRODUCTION,0.02046204620462046,"Nevertheless, it remains not well understood when and how world models can generalize well in
31"
INTRODUCTION,0.021122112211221122,"unseen environments.
32"
INTRODUCTION,0.02178217821782178,"In this work, we aim to first obtain a deep understanding of the generalization capability of world
33"
INTRODUCTION,0.022442244224422443,"models by examining the impact of latent representation errors, and then to devise new methods to
34"
INTRODUCTION,0.0231023102310231,"enhance its generalization. While one may expect that optimizing a latent dynamics model (LDM)
35"
INTRODUCTION,0.023762376237623763,"prior to training the task policy would minimize latent representation errors and hence can achieve
36"
INTRODUCTION,0.02442244224422442,"better world model training, our somewhat surprising findings, based on both theoretical and empirical
37"
INTRODUCTION,0.025082508250825083,"batch size
perturbation
α = 10
α = 20
α = 30
β = 25
β = 50
β = 75"
INTRODUCTION,0.02574257425742574,"8
691.62
363.73
153.67
624.67
365.31
216.52
16
830.39
429.62
213.78
842.26
569.42
375.61
32
869.39
436.87
312.99
912.12
776.86
655.26
64
754.47
440.44
80.24
590.41
255.2
119.62
Table 1: Reward values on unseen perturbed states by rotation (α) or mask (β%) with N(0.15, 0.5)."
INTRODUCTION,0.026402640264026403,"studies, reveal that modest latent representation errors in the training phase may in fact be beneficial.
38"
INTRODUCTION,0.02706270627062706,"In particular, the alternating training strategy for world model learning, which simultaneously refines
39"
INTRODUCTION,0.027722772277227723,"both the LDM and the action policy, could actually bring generalization gain, because the modest
40"
INTRODUCTION,0.02838283828382838,"latent representation errors (and the corresponding induced gradient estimation errors) could enable
41"
INTRODUCTION,0.029042904290429043,"the world model to visit unseen states and thus lead to improved generalization capacities. For
42"
INTRODUCTION,0.0297029702970297,"instance, as shown in Table 1, our experimental results suggest that moderate batch sizes (e.g., 16 or
43"
INTRODUCTION,0.030363036303630363,"32) appear to position the induced errors within a regime conferring notable generalization benefits,
44"
INTRODUCTION,0.031023102310231022,"leading to higher generalization improvement, when compared to the cases with very small (e.g., 8)
45"
INTRODUCTION,0.031683168316831684,"or large (e.g., 64) batch sizes.
46"
INTRODUCTION,0.03234323432343234,"In a nutshell, latent representation errors incurred by latent encoders, if designed properly, may
47"
INTRODUCTION,0.033003300330033,"actually facilitate world model training and enhance generalization. This insight aligns with recent
48"
INTRODUCTION,0.033663366336633666,"advances in deep learning, where noise injection schemes have been studied as a form of implicit
49"
INTRODUCTION,0.034323432343234324,"regularization to enhance models’ robustness. For instance, recent study [2] analyzes the effects of
50"
INTRODUCTION,0.03498349834983498,"introducing isotropic Gaussian noise at each layer of neural networks, identifying it as a form of
51"
INTRODUCTION,0.03564356435643564,"implicit regularization. Another recent work [27] explores the addition of zero-drift Brownian motion
52"
INTRODUCTION,0.036303630363036306,"to RNN architectures, demonstrating its regularizing effects in improving network’s stability against
53"
INTRODUCTION,0.036963696369636964,"noise perturbations.
54"
INTRODUCTION,0.03762376237623762,"We caution that latent representation errors in world models differ from the above noise injection
55"
INTRODUCTION,0.03828382838283828,"schemes ([27, 2]), in the following aspects: 1) Unlike the artificially injected noise only added in
56"
INTRODUCTION,0.038943894389438946,"training, these errors are inherent in world models, leading to error propagation in the rollouts; 2)
57"
INTRODUCTION,0.039603960396039604,"Unlike the controlled conditions of isotropic or zero-drift noise examined in prior studies, the errors
58"
INTRODUCTION,0.04026402640264026,"in world models may not exhibit such well-behaved properties in the sense that the drift may be
59"
INTRODUCTION,0.04092409240924092,"non-zero and hence biased; 3) additionally, in the iterative training of world models and agents, the
60"
INTRODUCTION,0.041584158415841586,"error originating from the encoder affects the policy learning and agent exploration. In light of these
61"
INTRODUCTION,0.042244224422442245,"observations, we develop a continuous-time stochastic differential equation (SDE) formulation by
62"
INTRODUCTION,0.0429042904290429,"treating the world model learning as a stochastic dynamic system with stochastic latent states. This
63"
INTRODUCTION,0.04356435643564356,"approach offers an insightful view on model errors as stochastic perturbation, enabling us to obtain
64"
INTRODUCTION,0.04422442244224423,"an explicit characterization to quantify the impacts of the errors on world models’ generalization
65"
INTRODUCTION,0.044884488448844885,"capability. Our main contributions can be summarized as follows.
66"
INTRODUCTION,0.04554455445544554,"• Latent representation errors as implicit regularization: Aiming to understand the generalization
67"
INTRODUCTION,0.0462046204620462,"capability of world models and improve it further, we develop a continuous-time SDE formula-
68"
INTRODUCTION,0.04686468646864687,"tion by treating the world model learning as a stochastic dynamic system in latent state space.
69"
INTRODUCTION,0.047524752475247525,"Leveraging tools in stochastic calculus and differential geometry, we characterize the impact
70"
INTRODUCTION,0.048184818481848184,"of latent representation errors on world models’ generalization. Our findings reveal that under
71"
INTRODUCTION,0.04884488448844884,"some technical conditions, modest latent representation errors can in fact function as implicit
72"
INTRODUCTION,0.04950495049504951,"regularization and hence result in generalization gain.
73"
INTRODUCTION,0.050165016501650166,"• Improving generalization in non-zero drift cases via Jacobian regularization: For the case where
74"
INTRODUCTION,0.050825082508250824,"latent representation errors exhibit non-zero drifts, we show that the additional bias term would
75"
INTRODUCTION,0.05148514851485148,"degrade the implicit regulation and hence may make the learning unstable. We propose to add
76"
INTRODUCTION,0.05214521452145215,"Jacobian regularization to mitigate the effects of non-zero-drift errors in training. Experimental
77"
INTRODUCTION,0.052805280528052806,"studies are carried out to evaluate the efficacy of Jacobian regularization.
78"
INTRODUCTION,0.053465346534653464,"• Reducing error propagation in predictive rollouts: We explicitly characterize the effect of latent
79"
INTRODUCTION,0.05412541254125412,"representation errors on predictive rollouts. Our experimental results corroborate that Jacobian
80"
INTRODUCTION,0.05478547854785479,"regularization can reduce the impact of error propagation on rollouts, leading to enhanced
81"
INTRODUCTION,0.055445544554455446,"prediction performance and accelerated convergence in tasks with longer time horizons.
82"
INTRODUCTION,0.056105610561056105,"• Bounding Latent Representation Error: We establish a novel bound on the latent representation
83"
INTRODUCTION,0.05676567656765676,"error within CNN encoder-decoder architectures. To our knowledge, this is the first quantifiable
84"
INTRODUCTION,0.05742574257425743,"bound applied to a learned latent representation model, and the analysis carries over to other
85"
INTRODUCTION,0.058085808580858087,"architectures (e.g., ReLU) along the same line.
86"
INTRODUCTION,0.058745874587458745,"Notation. We use Einstein summation convention for succinctness, where aibi denotes P"
INTRODUCTION,0.0594059405940594,"i aibi. We
87"
INTRODUCTION,0.06006600660066007,"denote functions in Ck,α as being k-times differentiable with α-Hölder continuity. The Euclidean
88"
INTRODUCTION,0.06072607260726073,"norm of a vector is represented by ∥· ∥, and the Frobenius norm of a matrix by | · |F ; this notation
89"
INTRODUCTION,0.061386138613861385,"may occasionally extend to tensors. The notation xi indicates the ith coordinate of the vector x, and
90"
INTRODUCTION,0.062046204620462043,"Aij the (i, j)-entry of the matrix A. Function composition is denoted by f ◦g, implying f(g). For a
91"
INTRODUCTION,0.0627062706270627,"differentiable function f : Rn →Rm , its Jacobian matrix is denoted by ∂f"
INTRODUCTION,0.06336633663366337,"∂x ∈Rm×n. Its gradient,
92"
INTRODUCTION,0.06402640264026403,"following conventional definitions, is denoted by ∇f. The constant C may represent different values
93"
INTRODUCTION,0.06468646864686468,"in distinct contexts.
94"
RELATED WORK,0.06534653465346535,"2
Related Work
95"
RELATED WORK,0.066006600660066,"World model based RL. World models have demonstrated remarkable efficacy in visual control
96"
RELATED WORK,0.06666666666666667,"tasks across various platforms, including Atari [1] and Minecraft [8], as detailed in the studies by
97"
RELATED WORK,0.06732673267326733,"Hafner et al. [14, 13, 15]. These models typically integrate encoders and memory-augmented neural
98"
RELATED WORK,0.06798679867986798,"networks, such as RNNs [33], to manage the latent dynamics. The use of variational autoencoders
99"
RELATED WORK,0.06864686468646865,"(VAE) [7, 23] to map sensory inputs to a compact latent space was pioneered by Ha et al. [12].
100"
RELATED WORK,0.06930693069306931,"Furthermore, the Dreamer algorithm [13, 16] employs convolutional neural networks (CNNs) [24] to
101"
RELATED WORK,0.06996699669966996,"enhance the processing of both hidden states and image embeddings, yielding models with improved
102"
RELATED WORK,0.07062706270627063,"predictive capabilities in dynamic environments.
103"
RELATED WORK,0.07128712871287128,"Continuous-time RNNs. The continuous-time assumption is standard for theoretical formulations
104"
RELATED WORK,0.07194719471947195,"of RNN models. Li et al. [26] study the optimization dynamics of linear RNNs on memory decay.
105"
RELATED WORK,0.07260726072607261,"Chang et al. [4] propose AntisymmetricRNN, which captures long-term dependencies through the
106"
RELATED WORK,0.07326732673267326,"control of eigenvalues in its underlying ODE. Chen et al. [5] propose the symplectic RNN to model
107"
RELATED WORK,0.07392739273927393,"Hamiltonians. As continuous-time formulations can be discretized with Euler methods [4, 5] (or with
108"
RELATED WORK,0.0745874587458746,"Euler-Maruyama methods if stochastic in [27]) and yield similar insights, this step is often eliminated
109"
RELATED WORK,0.07524752475247524,"for brevity.
110"
RELATED WORK,0.07590759075907591,"Implicit regularization by noise injection in RNN. Studies on noise injection as a form of implicit
111"
RELATED WORK,0.07656765676567656,"regularization have gained traction, with Lim et al. [27] deriving an explicit regularizer under small
112"
RELATED WORK,0.07722772277227723,"noise conditions, demonstrating bias towards models with larger margins and more stable dynamics.
113"
RELATED WORK,0.07788778877887789,"Camuto et al. [2] examine Gaussian noise injections at each layer of neural networks. Similarly, Wei
114"
RELATED WORK,0.07854785478547854,"et al. [31] provide analytic insights into the dual effects of dropout techniques.
115"
RELATED WORK,0.07920792079207921,"3
Demystifying World Model: A Stochastic Differential Equation Approach
116"
RELATED WORK,0.07986798679867987,"As pointed out in [14, 13, 15, 16], critical to the effectiveness of the world model representation is
117"
RELATED WORK,0.08052805280528053,"the stochastic design of its latent dynamics model. The model can be outlined by the following key
118"
RELATED WORK,0.08118811881188119,"components: an encoder that compresses high dimensional observations st into a low-dimensional
119"
RELATED WORK,0.08184818481848184,"latent state zt (Eq.1), a sequence model that captures temporal dependencies in the environment
120"
RELATED WORK,0.08250825082508251,"(Eq.2), a transition predictor that estimates the next latent state (Eq.3), and a latent decoder that
121"
RELATED WORK,0.08316831683168317,"reconstructs observed information from the posterior (Eq.4):
122"
RELATED WORK,0.08382838283828382,"Latent Encoder: zt ∼qenc(zt | ht, st),
(1)
Sequence Model: ht = f(ht−1, zt−1, at−1),
(2)
Transition Predictor: ˜zt ∼p( ˜zt | ht),
(3)
Latent Decoder: ˜st ∼qdec( ˜st | ht, ˜zt)
(4)"
RELATED WORK,0.08448844884488449,"In this work, we consider a popular class of world models, including Dreamer and PlaNet, where {z,
123"
RELATED WORK,0.08514851485148515,"˜z, ˜s} have distributions parameterized by neural networks’ outputs, and are Gaussian when the outputs
124"
RELATED WORK,0.0858085808580858,"are known. It is worth noting that {z, ˜z, ˜s} may not be Gaussian and are non-Gaussian in general.
125"
RELATED WORK,0.08646864686468647,"This is because while z is conditional Gaussian, its mean and variance are random variables which
126"
RELATED WORK,0.08712871287128712,"are learned by the encoder with s and h being the inputs, rendering that z is non-Gaussian due to the
127"
RELATED WORK,0.08778877887788779,"mixture effect. For this setting, we have a continuous-time formulation where the latent dynamics
128"
RELATED WORK,0.08844884488448845,"model can be interpreted as stochastic differential equations (SDEs) with coefficient functions of
129"
RELATED WORK,0.0891089108910891,"known inputs. Due to space limitation, we refer to Proposition B.1 in the Appendix for a more
130"
RELATED WORK,0.08976897689768977,"detailed treatment.
131"
RELATED WORK,0.09042904290429044,"Consider a complete, filtered probability space (Ω, F, {Ft}t∈[0,T ], P ) where independent standard
132"
RELATED WORK,0.09108910891089109,"Brownian motions B enc
t
, B pred
t
, B seq
t
, B dec
t
are defined such that Ft is their augmented filtration, and
133"
RELATED WORK,0.09174917491749175,"T ∈R as the time length of the task environment. We interpret the stochastic dynamics of LDM
134"
RELATED WORK,0.0924092409240924,"with latent representation errors through coupled SDEs representing continuous-time analogs of the
135"
RELATED WORK,0.09306930693069307,"discrete components:
136"
RELATED WORK,0.09372937293729373,"Latent Encoder: d zt = (qenc(ht, st) + ε σ(ht, st)) dt + (¯qenc(ht, st) + ε ¯σ(ht, st)) dB enc
t
,
(5)"
RELATED WORK,0.09438943894389439,"Sequence Model: d ht = f(ht, zt, π(ht, zt)) dt + ¯f(ht, zt, π(ht, zt)) dB seq
t
(6)"
RELATED WORK,0.09504950495049505,"Transition Predictor: d ˜zt = p(ht) dt + ¯p(ht) dB pred
t
,
(7)"
RELATED WORK,0.09570957095709572,"Latent Decoder: d ˜st = qdec(ht, ˜zt) dt + ¯qdec(ht, ˜zt) dB dec
t
,
(8)
where π(h, ˜z) is a policy function as a local maximizer of value function and the stochastic process
137"
RELATED WORK,0.09636963696369637,"st is Ft-adapted. Notice that ¯f is often a zero function indicating that Equation (6) is an ODE,
138"
RELATED WORK,0.09702970297029703,"as the sequence model is generally designed as deterministic. Generally, the coefficient functions
139"
RELATED WORK,0.09768976897689768,"in dt and dBt terms in SDEs are referred to as the drift and diffusion coefficients. Intuitively, the
140"
RELATED WORK,0.09834983498349835,"diffusion coefficients here represent the stochastic model components. In Equation (5), σ(·, ·) and
141"
RELATED WORK,0.09900990099009901,"¯σ(·, ·) denotes the drift and diffusion coefficients of the latent representation errors, respectively.
142"
RELATED WORK,0.09966996699669967,"Both are assumed to be functions of hidden states ht and task states st. In addition, ε indicates the
143"
RELATED WORK,0.10033003300330033,"magnitude of the error.
144"
RELATED WORK,0.100990099009901,"Next, we impose standard assumptions on these SDEs (5) - (8) to guarantee the well-definedness of
145"
RELATED WORK,0.10165016501650165,"the solution to SDEs. For further technical details, we refer readers to fundamental works on SDEs in
146"
RELATED WORK,0.10231023102310231,"the literature (e.g.,[30, 17]).
147"
RELATED WORK,0.10297029702970296,"Assumption 3.1. The drift coefficient functions qenc, f, p and qdec and the diffusion coefficient
148"
RELATED WORK,0.10363036303630363,"functions ¯qenc, ¯p and ¯qdec are bounded and Borel-measurable over the interval [0, T], and of class C3
149"
RELATED WORK,0.1042904290429043,"with bounded Lipschitz continuous partial derivatives. The initial values z0, h0, ˜z0, ˜s0 are square-
150"
RELATED WORK,0.10495049504950495,"integrable random variables.
151"
RELATED WORK,0.10561056105610561,"Assumption 3.2. σ and ¯σ are bounded and Borel-measurable and are of class C3 with bounded
152"
RELATED WORK,0.10627062706270628,"Lipschitz continuous partial derivatives over the interval [0, T].
153"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.10693069306930693,"3.1
Latent Representation Errors in CNN Encoder-Decoder Networks
154"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1075907590759076,"As shown in the empirical studies with different batch sizes (Table 1), the latent representation error
155"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.10825082508250825,"would also enrich generalization when it is within a moderate regime. In this section, we show that
156"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.10891089108910891,"the latent representation error, in the form of approximation error corresponding to widely used CNN
157"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.10957095709570958,"encoder-decoder, could be made sufficiently small by finding appropriate CNN network configuration.
158"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11023102310231023,"In particular, this result provides theoretical justification to interpreting latent representation error as
159"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11089108910891089,"stochastic perturbation in the dynamical system defined in Equations (5 - 8), as the error magnitude ε
160"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11155115511551156,"can be made sufficiently small by CNN network configuration.
161"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11221122112211221,"Consider the state space S ⊂RdS and the latent space Z. Consider a state probability measure Q on
162"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11287128712871287,"the state space S and a probability measure P on the latent space Z. As high-dimensional state space
163"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11353135313531353,"in image-based tasks frequently exhibit intrinsic lower-dimensional geometric structure, we adopt
164"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11419141914191419,"the latent manifold assumption, formally stated as follows:
165"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11485148514851486,"Assumption 3.3. (Latent manifold assumption) For a positive integer k, there exists a dM-
166"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11551155115511551,"dimensional Ck,α submanifold M (with Ck+3,α boundary) with Riemannian metric g and has
167"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11617161716171617,"positive reach and also isometrically embedded in the state space S ⊂RdS and dM << dS, where
168"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11683168316831684,"the state probability measure is supported on. In addition, M is a compact, orientable, connected
169"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11749174917491749,"manifold.
170"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11815181518151815,"Assumption 3.4. (Smoothness of state probability measure) Q is a probability measure supported on
171"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1188118811881188,"M with its Radon-Nikodym derivative q ∈Ck,α(M, R) w.r.t µM.
172"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.11947194719471947,"Let Z be a closed ball in RdM, that is {x ∈RdM : ∥x∥≤1 }. P is a probability measure supported
173"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12013201320132014,"on Z with its Radon-Nikodym derivative p ∈Ck,α(Z, R) w.r.t µZ. In practice, it is usually an easy-
174"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12079207920792079,"to-sample distribution such as uniform distribution which is determined by a specific encoder-decoder
175"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12145214521452145,"architecture choice.
176"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12211221122112212,"Latent Representation Learning. We define the latent representation learning as to find encoder
genc : M →Z and decoder gdec : Z →M as maps that optimize the following objectives:"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12277227722772277,"min
genc∈G W1
 
genc# Q, P

;
min
gdec∈G W1
 
Q, gdec# P

."
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12343234323432344,"Here, genc# Q and gdec# P represent the pushforward measures of Q and P through the encoder
177"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12409240924092409,"map genc and decoder map gdec, respectively. The latent representation error is understood as the
178"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12475247524752475,"“difference"" of pushforward measure by the encoder/decoder and target measure. Here, to understand
179"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1254125412541254,"the ""scale"" of the error ε in Equation (5), we use W1 for the discrepancy between probability
180"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12607260726072608,"measures. In particular, for Dreamer-type loss function that uses KL-divergence, we note that squared
181"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12673267326732673,"W1 distance between two probability measures can be upper bounded by their KL-divergence up to
182"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12739273927392739,"a constant [11], implying that one could reasonably expect the W1 distance to also decrease when
183"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12805280528052806,"KL-divergence is used in the model.
184"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12871287128712872,"CNN configuration. As a popular choice choice in encoder-decoder architecture is CNN, we
185"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.12937293729372937,"consider a general CNN function fCNN : X →R. Let fCNN have L hidden layers, represented
186"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13003300330033005,"as: for x ∈X, fCNN(x) := AL+1 ◦AL ◦· · · ◦A2 ◦A1(x), where Ai’s are either convolutional or
187"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1306930693069307,"downsampling operators. For convolutional layers, Ai(x) = σ(W c
i x + bc
i), where W c
i ∈Rdi×di−1
188"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13135313531353135,"is a structured sparse Toeplitz matrix from the convolutional filter {w(i)
j }s(i)
j=0 with filter length
189"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.132013201320132,"s(i) ∈N+, bc
i ∈Rdi is a bias vector, and σ is the ReLU activation function. For downsampling
190"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13267326732673268,"layers, Ai(x) = Di(x) = (xjmi)⌊di−1/mi⌋
j=1
, where Di : Rdi×di−1 is the downsampling operator
191"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13333333333333333,"with scaling parameter mi ≤di−1 in the i-th layer. We examine the class of functions represented by
192"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13399339933993398,"CNNs, denoted by FCNN, defined as:
193"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13465346534653466,"FCNN = {fCNN as in defined above with any choice of Ai, i = 1, . . . , L + 1}."
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1353135313531353,"For the specific definition of FCNN, we refer to [29]’s (4), (5) and (6).
194"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13597359735973596,"Assumption 3.5. Assume that M and Z are locally diffeomorphic, that is there exists a map
195"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13663366336633664,"F : M →Z such that at every point x on M, det(d F(x)) ̸= 0.
196"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1372937293729373,"Theorem 3.6. (Approximation Error of Latent Representation). Under Assumption 3.3, 3.4 and 3.5,
197"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13795379537953795,"for θ ∈(0, 1), let dθ := O(dMθ−2 log d"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13861386138613863,"θ). For positive integers M and N, there exists an encoder
198"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13927392739273928,"genc and decoder gdec ∈FCNN(L, S, W) s.t.
199"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.13993399339933993,"W1(genc#Q, P) ≤dMC(NM)
−2(k+1)"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1405940594059406,"dθ
,
W1(gdec#P, Q) ≤dMC(NM)
−2(k+1) dθ
."
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.14125412541254126,"Theorem 3.6 indicates that with an appropriate CNN configuration, the W1 approximation error can
200"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1419141914191419,"be made to reside in a small region, as the best candidate within the function class is indeed capable of
201"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.14257425742574256,"approximating the oracle encoder/decoder. In particular, this result indicates that the error magnitude
202"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.14323432343234324,"ε in SDE (5) can be assumed to be small. This allows us to apply the perturbation analysis of the
203"
LATENT REPRESENTATION ERRORS IN CNN ENCODER-DECODER NETWORKS,0.1438943894389439,"dynamical system defined in Equations (5 - 8) in the following sections.
204"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14455445544554454,"3.2
Latent Representation Errors as Implicit Regularization towards Generalization
205"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14521452145214522,"In this section, we investigate the impact of latent representation errors on generalization, for the
206"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14587458745874587,"two cases with zero drift and non-zero drift, respectively. We show that under mild conditions,
207"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14653465346534653,"the zero-drift errors can function as a natural form of implicit regularization, promoting wider
208"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.1471947194719472,"landscapes for improved robustness. Nevertheless, we caution that when latent representation errors
209"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14785478547854786,"have non-zero drift, it could lead to poor regularization with unstable bias and degrade world model’s
210"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.1485148514851485,"generalization, calling for explicit regularization.
211"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.1491749174917492,"To simplify the notation here, we consider the system equations, specifically Equations (5), (6) - (8),
212"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.14983498349834984,"as one stochastic system. Let xt = (zt, ht, ˜zt, ˜st) and Bt = (B enc
t
, B seq
t
, B pred
t
, B dec
t
):
213"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.1504950495049505,"d xt = (g(xt, t) + ε σ(xt, t)) dt +
X"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.15115511551155114,"i
¯gi(xt, t) + ε ¯σi(xt, t) dBi
t,
(9)"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.15181518151815182,"where g, and ¯gi are structured accordingly for the respective components, employing the Einstein
214"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.15247524752475247,"summation convention for concise representation. For abuse of notation, σ = (σ, 0, 0, 0), ¯σ =
215"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.15313531353135312,"(¯σ, 0, 0, 0). For a given error magnitude ε, we denote the solution to SDE (9) as xε
t. Intuitively, xε
t is
216"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.1537953795379538,"the perturbed trajectory of the latent dynamics model. In particular, when ε = 0, indicating that the
217"
LATENT REPRESENTATION ERRORS AS IMPLICIT REGULARIZATION TOWARDS GENERALIZATION,0.15445544554455445,"absence of latent representation error in the model, the solution is denoted as x0
t.
218"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.1551155115511551,"3.2.1
The Case with Zero-drift Representation Errors
219"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15577557755775578,"When the drift coefficient σ = 0, the latent representation errors correspond to a class of well-behaved
220"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15643564356435644,"stochastic processes. The following result translates the induced perturbation on the stochastic latent
221"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.1570957095709571,"dynamics model’s loss function L to a form of explicit regularization. We assume that L ∈C2
222"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15775577557755777,"and depends on zt, ht, ˜zt, ˜st. Loss functions used in practical implementation, e.g. in DreamerV3,
223"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15841584158415842,"reconstruction loss JO, reward loss JR, consistency loss JD, all satisfy this condition.
224"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15907590759075907,"Theorem 3.7. (Explicit Effect Induced by Zero-Drift Representation Error) Under Assumptions
225"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.15973597359735975,"3.1 and 3.2 and considering a loss function L ∈C2, the explicit effects of the zero-drift error can be
226"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.1603960396039604,"marginalized out as follows: as ε →0,
227"
THE CASE WITH ZERO-DRIFT REPRESENTATION ERRORS,0.16105610561056105,"E L (xε
t) = E L(x0
t) + R + O(ε3),
(10)
where the regularization term R is given by R := ε P + ε2  
Q + 1"
S,0.1617161716171617,"2 S

, with
228"
S,0.16237623762376238,"P := E ∇L(x0
t)⊤Φt
X"
S,0.16303630363036303,"k
ξk
t ,
(11)"
S,0.16369636963696368,"S := E
X"
S,0.16435643564356436,"k1,k2
(Φtξk1
t )i∇2L(x0
t, t) (Φtξk2
t )j,
(12)"
S,0.16501650165016502,"Q := E ∇L(x0
t)⊤Φt
Z t"
S,0.16567656765676567,"0
Φ−1
s
Hk(x0
s, s)dBk
t .
(13)"
S,0.16633663366336635,"Square matrix Φt is the stochastic fundamental matrix of the corresponding homogeneous equation:
229"
S,0.166996699669967,dΦt = ∂¯gk
S,0.16765676567656765,"∂x (x0
t, t) Φt dBk
t ,
Φ(0) = I,"
S,0.16831683168316833,"and ξk
t is the shorthand for
R t
0 Φ−1
s ¯σk(x0
s, s)dBk
t . Additionally, Hk(x0
s, s) is represented by for
230
P
k1,k2
∂2¯gk
∂xi∂xj (x0
s, s)
 
ξk1
s
i  
ξk2
s
j.
231"
S,0.16897689768976898,"The proof is relegated to Appendix B in the Supplementary Materials.
232"
S,0.16963696369636963,"When the loss L is convex, then its Hessian, ∇2L, is positive semi-definite, which ensures that the
233"
S,0.1702970297029703,"term S is non-negative. The presence of this Hessian-dependent term S, under latent representation
234"
S,0.17095709570957096,"error, implies a tendency towards wider minima in the loss landscape. Empirical results from [20]
235"
S,0.1716171617161716,"indicates that wider minima correlate with improved robustness of implicit regularization during
236"
S,0.17227722772277226,"training. This observation also aligns with the theoretical insights in [27] that the introduction
237"
S,0.17293729372937294,"of Brownian motion, which is indeed zero-drift by definition, in training RNN models promotes
238"
S,0.1735973597359736,"robustness. We note that in addition, when the error ¯σt(·) is too small, the effect of term S as implicit
239"
S,0.17425742574257425,"regularization would not be as significant as desired. Intuitively, this insight resonates with the
240"
S,0.17491749174917492,"empirical results in Table 1 that model’s robustness gain is not significant when the error induced by
241"
S,0.17557755775577558,"small batch sizes is too small.
242"
S,0.17623762376237623,"We remark that the exact loss form treated here is simplified compared to that in the practical
243"
S,0.1768976897689769,"implementation of world models, which frequently depends on the probability density functions
244"
S,0.17755775577557756,"(PDFs) of zt, ht, ˜zt, ˜st. In principle, the PDE formulation corresponding to the PDFs of the perturbed
245"
S,0.1782178217821782,"xε
t can be derived from the Kolmogorov equation of the SDE (9), and the technicality is more involved
246"
S,0.1788778877887789,"but can offer more direct insight. We will study this in future work.
247"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.17953795379537954,"3.2.2
The Case with Non-Zero-Drift Representation Errors
248"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1801980198019802,"In practice, latent representation errors may not always exhibit zero drift as in idealized noise-injection
249"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18085808580858087,"schemes for deep learning ([27], [2]). When the drift coefficient σ is non-zero or a function of input
250"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18151815181518152,"data ht and st in general, the explicit regularization terms induced by the latent representation error
251"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18217821782178217,"may lead to unstable bias in addition to the regularization term R in Theorem 3.7. With a slight abuse
252"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18283828382838282,"of notation, we denote ¯g0 as g from Equation (9) for convenience.
253"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1834983498349835,"Corollary 3.8. (Additional Bias Induced by Non-Zero Drift Representation Error)
254"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18415841584158416,"Under Assumptions 3.1 and 3.2 and considering a loss function L ∈C2, the explicit effects of the
255"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1848184818481848,"general form error can be marginalized out as follows as ε →0:
256"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18547854785478549,"E L (xε
t) = E L(x0
t) + R + ˜R + O(ε3),
(14)"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18613861386138614,"where the additional bias term ˜R is given by ˜R := ε ˜P + ε2 
˜Q + ˜S

, with
257"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1867986798679868,"˜P := E ∇L(x0
t)⊤Φt ˜ξt,
(15)"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18745874587458747,"˜Q := E ∇L(x0
t)⊤Φt
Z t"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18811881188118812,"0
Φ−1
s
H0(x0
s, s) dt,
(16)"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18877887788778877,"˜S := E
X"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.18943894389438945,"k
(Φt ˜ξt)i∇2L(x0
t, t) (Φtξk
t )j,
(17)"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1900990099009901,"and ˜ξt being the shorthand for
R t
0 Φ−1
s σk(x0
s, s)dt.
258"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19075907590759075,"The presence of the new bias term ˜R implies that regularization effects of latent representation error
259"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19141914191419143,"could be unstable. The presence of ˜ξ in ˜P, ˜Q and ˜S induces a bias to the loss function with its
260"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19207920792079208,"magnitude dependent on the error level ε, since ˜ξ is a non-zero term influenced on the drift term
261"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19273927392739273,"σ. This contrasts with the scenarios described in [27] and [2], where the noise injected for implicit
262"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19339933993399339,"regularization follows a zero-mean Gaussian distribution. To modulate the regularization and bias
263"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19405940594059407,"terms R and ˜R respectively, we note that a common factor, the fundamental matrix Φ, can be bounded
264"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19471947194719472,"by
265"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19537953795379537,"E sup
t
∥Φt∥2
F ≤
X"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19603960396039605,"k
C exp

C E sup
t ∂gk"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.1966996699669967,"∂x (x0
t , t) 2 F"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19735973597359735,"
(18)"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19801980198019803,"which can be shown by using the Burkholder-Davis-Gundy Inequality and Gronwall’s Lemma.
266"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19867986798679868,"Based on this observation, we next propose a regularizer on input-output Jacobian norm ∥∂gk"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.19933993399339933,"∂x ∥F that
267"
THE CASE WITH NON-ZERO-DRIFT REPRESENTATION ERRORS,0.2,"could modulate the new bias term ˜R for stabilized implicit regularization.
268"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20066006600660066,"4
Enhancing Predictive Rollouts via Jacobian Regularization
269"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20132013201320131,"In this section, we study the effects of latent representation errors on predictive rollouts using latent
270"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.201980198019802,"state transitions, which happen in the inference phase in world models. We then propose to use
271"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20264026402640264,"Jacobian regularization to enhance the quality of rollouts. In particular, we first obtain an upper bound
272"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2033003300330033,"of state trajectory divergence in the rollout due to the representation error. We show that the error
273"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20396039603960395,"effects on task policy’s Q function can be controlled through model’s input-output Jacobian norm.
274"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20462046204620463,"In world model learning, the task policy is optimized over the rollouts of dynamics model with the
275"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20528052805280528,"initial latent state z0. Recall that latent representation error is introduced to z0 when latent encoder
276"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20594059405940593,"encodes the initial state s0 from task environment. Intuitively, the latent representation error would
277"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2066006600660066,"propagate under the sequence model and impact the policy learning, which would then affect the
278"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20726072607260726,"generalization capacity through increased exploration.
279"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2079207920792079,"Recall that the sequence model and the transition predictor are given as follows:
280"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2085808580858086,"d ht = f(ht, ˜zt, π(ht, ˜zt)) dt,
d ˜zt = p(ht)dt + ¯p(ht) dBt,
(19)"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.20924092409240924,"with random variables h0, ˜z0 + ε as the initial values, respectively. In particular, ε is a random
281"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2099009900990099,"variable of proper dimension, representing the error from encoder introduced at the initial step. We
282"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21056105610561057,"impose the standard assumption on the error to ensure the well-definedness of the SDEs.
283"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21122112211221122,"Under Assumption 3.1, there exists a unique solution to the SDEs (for Equations 19 with square-
284"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21188118811881188,"integrable ε), denoted as (hε
t, zε
t ). In the case of no error introduced, i.e., ε = 0, we denote the
285"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21254125412541255,"solution of the SDEs as (h0
t, z0
t ) understood as the rollout under the absence of latent representation
286"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2132013201320132,"error. To understand how to modulate impacts of the error in rollouts, our following result gives an
287"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21386138613861386,"upper bound on the expected divergence between the perturbed rollout trajectory (hε
t, zε
t ) and the
288"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2145214521452145,"original (h0
t, z0
t ) over the interval [0, T].
289"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2151815181518152,"Theorem 4.1. (Bounding trajectory divergence) For a square-integrable random variable ε, let
290"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21584158415841584,"δ := E ∥ε∥and dε := E supt∈[0,T ]
hε
t −h0
t
2 +
˜zε
t −˜z0
t
2 . As δ →0,
291"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2165016501650165,"dε ≤δ C (J0 + J1) + δ2 C exp ( H0 (J0 + J1)) + δ2 C exp ( H1 (J0 + J1)) + O(δ3),"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21716171617161717,"where C is a constant dependent on T. J1 and J2 are Jacobian-related terms, and H1 and H2 are Hessian-
292"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21782178217821782,"related terms.
293"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21848184818481847,"The Jacobian-related terms J1 and J2 are defined as J0 := exp (Fh + Fz + Ph) , J1 := exp
  ¯Ph

;
294"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.21914191419141915,"the Hessian-related terms H0 and H1 are defined as H0 := Fhh+Fhz+Fzh+Fzz+Phh, H1 := ¯Phh,
295"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2198019801980198,"where Fh, Fz are the expected sup Frobenius norm of Jacobians of f w.r.t h, z, respectively, and
296"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22046204620462045,"Fhh, Fhz, Fzh, Fzz are the corresponding expected sup Frobenius norm of second-order derivatives.
297"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22112211221122113,"Other terms are similarly defined. A detailed description of all terms, can be found in Appendix C.1.
298"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22178217821782178,"Theorem 4.1 correlates with the empirical findings in [14] regarding the diminished predictive
299"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22244224422442244,"accuracy of latent states ˜zt over the extended horizons. In particular, Theorem 4.1 suggests that the
300"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22310231023102312,"expected divergence from error accumulation hinges on the expected error magnitude, the Jacobian
301"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22376237623762377,"norms within the latent dynamics model and the horizon length T.
302"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22442244224422442,"Our next result reveals how initial latent representation error influences the value function Q during
303"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22508250825082507,"the prediction rollouts, which again verifies that the perturbation is dependent on expected error
304"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22574257425742575,"magnitude, the model’s Jacobian norms and the horizon length T:
305"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2264026402640264,"Corollary 4.2. For a square-integrable ε, let xt := (ht, zt). Then, for any action a ∈A, the
306"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22706270627062705,"following holds for value function Q almost surely:
307"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22772277227722773,"Q(xε
t, a) = Q(x0
t, a) + ∂"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22838283828382838,"∂xQ(x0
t, a)

εi∂i x0
t + 1"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.22904290429042903,"2εi εj ∂2
ij x0
t  + 1"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2297029702970297,"2(εi ∂i x0
t)⊤∂2"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23036303630363036,"∂x2 Q(x0
t, a) (εi ∂i x0
t) + O(δ3),"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23102310231023102,"as δ →0, where stochastic processes ∂i x0
t, ∂2
ij x0
t are the first and second derivatives of x0
t w.r.t ε
308"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2316831683168317,"and are bounded as follows:
309"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23234323432343235,"E sup
t∈[0,T ]"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.233003300330033,"∂i x0
t
 ≤C (J0 + J1) , E sup
t∈[0,T ]"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23366336633663368,"∂2
ij x0
t
 ≤C exp ( H0 (J0 + J1)) + C exp ( H1 (J0 + J1)) ."
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23432343234323433,"This corollary reveals that latent representation errors implicitly encourage exploration of unseen
310"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23498349834983498,"states by inducing a stochastic perturbation in the value function, which again can be regularized
311"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23564356435643563,"through a controlled Jacobian norm.
312"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2363036303630363,"Jacobian Regularization against Non-Zero Drift. The above theoretical results have established
313"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23696369636963696,"a close connection of input-output Jacobian matrices with the stabilized generalization capacity of
314"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2376237623762376,"world models (shown in 18 under non-zero drift form), and perturbation magnitude in predictive
315"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2382838283828383,"rollouts (indicated in the presence of Jacobian terms in Theorem 4.1 and Corollary 4.2.) Based on
316"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.23894389438943894,"this, we propose a regularizer on input-output Jacobian norm ∥∂gk"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2396039603960396,"∂x ∥F that could modulate ˜ξ ( and in
317"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24026402640264027,"addition ξk) for stabilized implicit regularization.
318"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24092409240924093,"The regularized loss function for LDM is defined as follows:
319"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24158415841584158,"¯Ldyn = Ldyn + λ ∥Jθ∥F ,
(20)"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24224422442244226,"where Ldyn is the original loss function for dynamics model, Jθ denotes the data-dependent Jacobian
320"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2429042904290429,"matrix associated with the θ-parameterized dynamics model, and λ is the regularization weight.
321"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24356435643564356,"Our empirical results in 5 with an emphasis on sequential case align with the experimental findings
322"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.24422442244224424,"from [18] that Jacobian regularization can enhance robustness against random and adversarial input
323"
ENHANCING PREDICTIVE ROLLOUTS VIA JACOBIAN REGULARIZATION,0.2448844884488449,"perturbation in machine learning models.
324"
EXPERIMENTAL STUDIES,0.24554455445544554,"5
Experimental Studies
325"
EXPERIMENTAL STUDIES,0.2462046204620462,"In this section, experiments are carried out over a number of tasks in Mujoco environments. Due to
326"
EXPERIMENTAL STUDIES,0.24686468646864687,"space limitation, implementation details and additional results, including the standard deviation of
327"
EXPERIMENTAL STUDIES,0.24752475247524752,"the trials, are relegated to Section D in the Appendix.
328"
EXPERIMENTAL STUDIES,0.24818481848184817,"Enhanced generalization to unseen noisy states. We investigated the effectiveness of Jacobian
329"
EXPERIMENTAL STUDIES,0.24884488448844885,"regularization in model trained against a vanilla model during the inference phase with perturbed
330"
EXPERIMENTAL STUDIES,0.2495049504950495,"state images. We consider three types of perturbations: (1) Gaussian noise across the full image,
331"
EXPERIMENTAL STUDIES,0.2501650165016502,"denoted as N(µ1, σ2
1) ; (2) rotation; and (3) noise applied to a percentage of the image, N(µ2, σ2
2).
332"
EXPERIMENTAL STUDIES,0.2508250825082508,"(In Walker task, µ1 = µ2 = 0.5, σ2
2 = 0.15; in Quadruped task, µ1 = 0, µ2 = 0.05, σ2
2 = 0.2.) In
333"
EXPERIMENTAL STUDIES,0.2514851485148515,"each case of perturbations, we examine a collection of noise levels: (1) variance σ2 from 0.05 to
334"
EXPERIMENTAL STUDIES,0.25214521452145217,"0.55; (2) rotation degree α 20 and 30; and (3) masked image percentage β% from 25 to 75.
335"
EXPERIMENTAL STUDIES,0.2528052805280528,Figure 1: Generalization against increasing degree of perturbation.
EXPERIMENTAL STUDIES,0.25346534653465347,"It can be seen from Table 3 and Figure 1 that thanks to the adoption of Jacobian regularization in
336"
EXPERIMENTAL STUDIES,0.25412541254125415,"training, the rewards (averaged over 5 trials) are higher compared to the baseline, indicating improved
337"
EXPERIMENTAL STUDIES,0.25478547854785477,"generalization to unseen image states in all cases. The experimental results corroborate the findings
338"
EXPERIMENTAL STUDIES,0.25544554455445545,"in Corollary 3.8 that the regularized Jacobian norm could stabilize the induced implicit regularization.
339"
EXPERIMENTAL STUDIES,0.25610561056105613,"full, N(µ1, σ2
1)
rotation, +α◦
mask β%, N(µ2, σ2
2)
clean
σ2
1 = 0.35
σ2
1 = 0.5
α = 20
α = 30
β = 50
β = 75
With Jacobian (Walker)
967.12
742.32
618.98
423.81
226.04
725.81
685.49
Baseline (Walker)
966.53
615.79
333.47
391.65
197.53
583.41
446.74
With Jacobian (Quad)
971.98
269.78
242.15
787.63
610.53
321.55
304.92
Baseline (Quad)
967.91
207.33
194.08
681.03
389.41
222.22
169.58
Table 2: Evaluation on unseen states by various perturbation (Clean means without perturbation).
λ = 0.01."
EXPERIMENTAL STUDIES,0.25676567656765675,"Robustness against encoder errors. Next, we focus on the effects of Jacobian regularization on
340"
EXPERIMENTAL STUDIES,0.25742574257425743,"controlling the error process to the latent states z during training. Since it is very challenging, if
341"
EXPERIMENTAL STUDIES,0.2580858085808581,"not impossible, to characterize the latent representation errors and hence the drift therein explicitly,
342"
EXPERIMENTAL STUDIES,0.25874587458745874,"we consider to evaluate the robustness against two exogenous error signals, namely (1) zero-drift
343"
EXPERIMENTAL STUDIES,0.2594059405940594,"error with µt = 0, σ2
t (σ2
t = 5 in Walker, σ2
t = 0.1 in Quadruped), and (2) non-zero-drift error
344"
EXPERIMENTAL STUDIES,0.2600660066006601,"with µt ∼[0, 5], σ2
t ∼[0, 5] uniformly. Table 3 shows that the model with regularization can
345"
EXPERIMENTAL STUDIES,0.2607260726072607,"consistently learn policies with high returns and also converges faster, compared to the vanilla case.
346"
EXPERIMENTAL STUDIES,0.2613861386138614,"This corroborates our theoretical findings in Corollary 3.8 that the impacts of error to loss L can be
347"
EXPERIMENTAL STUDIES,0.262046204620462,"controlled through the model’s Jacobian norm.
348"
EXPERIMENTAL STUDIES,0.2627062706270627,"Zero drift, Walker
Non-zero drift, Walker
Zero drift, Quad
Non-zero drift, Quad
300k
600k
300k
600k
600k
1.2M
1M
2M
With Jacobian
666.2
966
905.7
912.4
439.8
889
348.3
958.7
Baseline
24.5
43.1
404.6
495
293.6
475.9
48.98
32.87
Table 3: Accumulated rewards under additional encoder errors. λ = 0.01."
EXPERIMENTAL STUDIES,0.2633663366336634,"Faster convergence on tasks with extended horizon. We further evaluate the efficacy of Jacobian
349"
EXPERIMENTAL STUDIES,0.264026402640264,"regularization in tasks with extended horizon, particularly by extending the horizon length in MuJoCo
350"
EXPERIMENTAL STUDIES,0.2646864686468647,"Walker from 50 to 100 steps. Table 4 shows that the model with regularization converges significantly
351"
EXPERIMENTAL STUDIES,0.26534653465346536,"faster (∼100K steps) than the case without Jacobian regularization in training. This corroborates
352"
EXPERIMENTAL STUDIES,0.266006600660066,"results in Theorem 4.1 that regularizing the Jacobian norm can reduce error propagation.
353"
EXPERIMENTAL STUDIES,0.26666666666666666,"Walker 100 len (increased from original 50 len)
Num steps
100k
200k
280k
With Jacobian (λ = 0.05)
639.1
936.3
911.1
With Jacobian (λ = 0.1)
537.5
762.6
927.7
Baseline
582.3
571.2
886.6
Table 4: Accumulated rewards of Walker with extended horizon."
CONCLUSION,0.26732673267326734,"6
Conclusion
354"
CONCLUSION,0.26798679867986797,"In this study, we investigate the impacts of latent representation errors on the generalization capacity
355"
CONCLUSION,0.26864686468646864,"of world models. We utilize a stochastic differential equation formulation to characterize the effects
356"
CONCLUSION,0.2693069306930693,"of latent representation errors as implicit regularization, for both cases with zero-drift errors and
357"
CONCLUSION,0.26996699669966995,"with non-zero drift errors. We develop a Jacobian regularization scheme to address the compounding
358"
CONCLUSION,0.2706270627062706,"effects of non-zero drift, thereby enhancing training stability and generalization. Our empirical
359"
CONCLUSION,0.2712871287128713,"findings validate that Jacobian regularization improves the generalization performance, expanding
360"
CONCLUSION,0.27194719471947193,"the applicability of world models in complex, real-world scenarios. Future research is needed to
361"
CONCLUSION,0.2726072607260726,"investigate how stabilizing latent errors can enhance generalization across more sophisticated tasks
362"
CONCLUSION,0.2732673267326733,"for general non-zero drift cases.
363"
CONCLUSION,0.2739273927392739,"The broader social impact of our work resides in its potential to enhance the robustness and reliability
364"
CONCLUSION,0.2745874587458746,"of RL agents deployed in real-world applications. By improving the generalization capacities of world
365"
CONCLUSION,0.27524752475247527,"models, our work could contribute to the development of RL agents that perform consistently across
366"
CONCLUSION,0.2759075907590759,"diverse and unseen environments. This is particularly relevant in safety-critical domains such as
367"
CONCLUSION,0.2765676567656766,"autonomous driving, where reliable agents can provide intelligent and trustworthy decision-making.
368"
REFERENCES,0.27722772277227725,"References
369"
REFERENCES,0.2778877887788779,"[1] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning
370"
REFERENCES,0.27854785478547855,"environment: An evaluation platform for general agents. Journal of Artificial Intelligence
371"
REFERENCES,0.27920792079207923,"Research, 47:253–279, 2013.
372"
REFERENCES,0.27986798679867986,"[2] Alexander Camuto, Matthew Willetts, Umut ¸Sim¸sekli, Stephen Roberts, and Chris Holmes.
373"
REFERENCES,0.28052805280528054,"Explicit regularisation in gaussian noise injections, 2021.
374"
REFERENCES,0.2811881188118812,"[3] Henri Cartan. Differential calculus on normed spaces. Createspace Independent Publishing
375"
REFERENCES,0.28184818481848184,"Platform, North Charleston, SC, August 2017.
376"
REFERENCES,0.2825082508250825,"[4] Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. Antisymmetricrnn: A dynamical system
377"
REFERENCES,0.28316831683168314,"view on recurrent neural networks, 2019.
378"
REFERENCES,0.2838283828382838,"[5] Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Léon Bottou. Symplectic recurrent neural
379"
REFERENCES,0.2844884488448845,"networks, 2020.
380"
REFERENCES,0.2851485148514851,"[6] Bernard Dacorogna and Jürgen Moser. On a partial differential equation involving the jacobian
381"
REFERENCES,0.2858085808580858,"determinant. Annales de l’I.H.P. Analyse non linéaire, 7(1):1–26, 1990.
382"
REFERENCES,0.2864686468646865,"[7] Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
383"
REFERENCES,0.2871287128712871,"[8] Sean C Duncan. Minecraft, beyond construction and survival. 2011.
384"
REFERENCES,0.2877887788778878,"[9] Lawrence Craig Evans and Ronald F Gariepy. Measure theory and fine properties of functions,
385"
REFERENCES,0.28844884488448846,"revised edition. Textbooks in Mathematics. Apple Academic Press, Oakville, MO, April 2015.
386"
REFERENCES,0.2891089108910891,"[10] C. Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead:
387"
REFERENCES,0.28976897689768977,"World models without forward prediction. Thirty-third Conference on Neural Information
388"
REFERENCES,0.29042904290429045,"Processing Systems (NeurIPS 2019), 2019.
389"
REFERENCES,0.29108910891089107,"[11] Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics.
390"
REFERENCES,0.29174917491749175,"International Statistical Review / Revue Internationale de Statistique, 70(3):419–435, 2002.
391"
REFERENCES,0.29240924092409243,"[12] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
392"
REFERENCES,0.29306930693069305,"[13] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
393"
REFERENCES,0.29372937293729373,"Learning behaviors by latent imagination, 2020.
394"
REFERENCES,0.2943894389438944,"[14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and
395"
REFERENCES,0.29504950495049503,"James Davidson. Learning latent dynamics for planning from pixels. In International conference
396"
REFERENCES,0.2957095709570957,"on machine learning, pages 2555–2565. PMLR, 2019.
397"
REFERENCES,0.2963696369636964,"[15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
398"
REFERENCES,0.297029702970297,"discrete world models, 2022.
399"
REFERENCES,0.2976897689768977,"[16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains
400"
REFERENCES,0.2983498349834984,"through world models. arXiv preprint arXiv:2301.04104, 2023.
401"
REFERENCES,0.299009900990099,"[17] Paul Louis Hennequin, R. M. Dudley, H. Kunita, and F. Ledrappier. Ecole d’ete de Probabilites
402"
REFERENCES,0.2996699669966997,"de Saint-Flour XII-1982. Springer-Verlag, 1984.
403"
REFERENCES,0.30033003300330036,"[18] Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization,
404"
REFERENCES,0.300990099009901,"2019.
405"
REFERENCES,0.30165016501650166,"[19] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie
406"
REFERENCES,0.3023102310231023,"Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving.
407"
REFERENCES,0.30297029702970296,"arXiv preprint arXiv:submit/1234567, Sep 2023. Submitted on 29 Sep 2023.
408"
REFERENCES,0.30363036303630364,"[20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
409"
REFERENCES,0.30429042904290426,"Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima,
410"
REFERENCES,0.30495049504950494,"2017.
411"
REFERENCES,0.3056105610561056,"[21] Samuel Kessler, Mateusz Ostaszewski, Michał Bortkiewicz, Mateusz ˙Zarski, Maciej Wołczyk,
412"
REFERENCES,0.30627062706270625,"Jack Parker-Holder, Stephen J. Roberts, and Piotr Miło´s. The effectiveness of world models for
413"
REFERENCES,0.3069306930693069,"continual reinforcement learning. CoLLAs 2023, 2023.
414"
REFERENCES,0.3075907590759076,"[22] Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, and Daniel Yamins. Active world
415"
REFERENCES,0.30825082508250823,"model learning with progress curiosity. In Proceedings of the 37th International Conference on
416"
REFERENCES,0.3089108910891089,"Machine Learning (ICML), 2020.
417"
REFERENCES,0.3095709570957096,"[23] Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
418"
REFERENCES,0.3102310231023102,"arXiv:1312.6114, 2013.
419"
REFERENCES,0.3108910891089109,"[24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
420"
REFERENCES,0.31155115511551157,"Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
421"
REFERENCES,0.3122112211221122,"Neural computation, 1(4):541–551, 1989.
422"
REFERENCES,0.31287128712871287,"[25] John M. Lee. Introduction to Riemannian Manifolds. Springer International Publishing, 2018.
423"
REFERENCES,0.31353135313531355,"[26] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory
424"
REFERENCES,0.3141914191419142,"for linear continuous-time recurrent neural networks. Journal of Machine Learning Research,
425"
REFERENCES,0.31485148514851485,"23(42):1–85, 2022.
426"
REFERENCES,0.31551155115511553,"[27] Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy
427"
REFERENCES,0.31617161716171616,"recurrent neural networks. Advances in Neural Information Processing Systems, 34:5124–5137,
428"
REFERENCES,0.31683168316831684,"2021.
429"
REFERENCES,0.3174917491749175,"[28] Lynn Harold Loomis and Shlomo Sternberg. Advanced calculus (revised edition). World
430"
REFERENCES,0.31815181518151814,"Scientific Publishing, Singapore, Singapore, March 2014.
431"
REFERENCES,0.3188118811881188,"[29] Guohao Shen, Yuling Jiao, Yuanyuan Lin, and Jian Huang. Approximation with cnns in sobolev
432"
REFERENCES,0.3194719471947195,"space: with applications to classification. In NeurIPS, Oct 2022.
433"
REFERENCES,0.3201320132013201,"[30] J. Michael Steele. Stochastic calculus and Financial Applications. Springer, 2001.
434"
REFERENCES,0.3207920792079208,"[31] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of
435"
REFERENCES,0.3214521452145215,"dropout, 2020.
436"
REFERENCES,0.3221122112211221,"[32] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-
437"
REFERENCES,0.3227722772277228,"dreamer: World models for physical robot learning. In Proceedings of The 6th Conference on
438"
REFERENCES,0.3234323432343234,"Robot Learning, volume 205 of PMLR, pages 2226–2240, 2023.
439"
REFERENCES,0.3240924092409241,"[33] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. A review of recurrent neural
440"
REFERENCES,0.32475247524752476,"networks: Lstm cells and network architectures. Neural computation, 31(7):1235–1270, 2019.
441"
REFERENCES,0.3254125412541254,"Supplementary Materials
442"
REFERENCES,0.32607260726072607,"In this appendix, we provide the supplementary materials supporting the findings of the main paper
443"
REFERENCES,0.32673267326732675,"on the latent representation of latent representations in world models. The organization is as follows:
444"
REFERENCES,0.32739273927392737,"• In Section A, we provide proof on showing the approximation capacity of CNN encoder-
445"
REFERENCES,0.32805280528052805,"decoder architecture in latent representation of world models.
446"
REFERENCES,0.3287128712871287,"• In Section B, we provide proof on implicit regularization of zero-drift errors and additional
447"
REFERENCES,0.32937293729372935,"effects of non-zero-drift errors by showing a proposition on the general form.
448"
REFERENCES,0.33003300330033003,"• In Section C, we provide proof on showing the effects of non-zero-drift errors during
449"
REFERENCES,0.3306930693069307,"predictive rollouts by again showing a result on the general form.
450"
REFERENCES,0.33135313531353133,"• In Section D, we provide additional results and implementation details on our empirical
451"
REFERENCES,0.332013201320132,"studies.
452"
REFERENCES,0.3326732673267327,"A
Approximation Power of Latent Representation with CNN Encoder and
453"
REFERENCES,0.3333333333333333,"Decoder
454"
REFERENCES,0.333993399339934,"To mathematically describe this intrinsic lower-dimensional geometric structure, for an integer k > 0
455"
REFERENCES,0.3346534653465347,"and α ∈(0, 1], we consider the notion of smooth manifold (in the Ck,α sense), formally defined by
456"
REFERENCES,0.3353135313531353,"Definition A.1 (Ck,α manifold). A Ck,α manifold M of dimension n is a topological manifold (i.e.
457"
REFERENCES,0.335973597359736,"a topological space that is locally Euclidean, with countable basis, and Hausdorff) that has a Ck,α
458"
REFERENCES,0.33663366336633666,"structure Ξ that is a collection of coordinate charts {Uα, ψα}α∈A where Uα is an open subset of M,
459"
REFERENCES,0.3372937293729373,"ψα : Uα →Vα ⊆Rn such that
460"
REFERENCES,0.33795379537953796,"• S
α∈A Uα ⊇M, meaning that the the open subsets form an open cover,
461"
REFERENCES,0.33861386138613864,"• Each chart ψα is a diffeomorphism that is a smooth map with smooth inverse (in the Ck,α
462"
REFERENCES,0.33927392739273926,"sense),
463"
REFERENCES,0.33993399339933994,"• Any two charts are Ck,α-compatible with each other, that is for all α1, α2 ∈A, ψα1 ◦ψ−1
α2 :
464"
REFERENCES,0.3405940594059406,"ψα2(Uα1 ∩Uα2) →ψα1(Uα1 ∩Uα2) is Ck,α.
465"
REFERENCES,0.34125412541254124,"Intuitively, a Ck,α manifold is a generalization of Euclidean space by allowing additional spaces with
466"
REFERENCES,0.3419141914191419,"nontrivial global structures through a collection of charts that are diffeomorphisms mapping open
467"
REFERENCES,0.3425742574257426,"subsets from the manifold to open subsets of euclidean space. For technical utility, the defined charts
468"
REFERENCES,0.3432343234323432,"allow to transfer most familiar real analysis tools to the manifold space. For more references, see
469"
REFERENCES,0.3438943894389439,"[25].
470"
REFERENCES,0.3445544554455445,"Definition A.2 (Riemannian volume form). Let X be a smooth, oriented d-dimensional manifold
471"
REFERENCES,0.3452145214521452,"with Riemannian metric g. A volume form dvolM is the canonical volume form on X if for any point
472"
REFERENCES,0.3458745874587459,"x ∈X, for a chosen local coordinate chart (x1, ..., xd), dvolM =
p"
REFERENCES,0.3465346534653465,"det gij dx1 ∧... ∧dxd, where
473"
REFERENCES,0.3471947194719472,gij(x) := g ( ∂
REFERENCES,0.34785478547854787,"∂xi ,
∂
∂xj )(x).
474"
REFERENCES,0.3485148514851485,"Then the induced volume measure by the canonical volume form dvolX is denoted as µX , defined
475"
REFERENCES,0.34917491749174917,"by µX : A 7→
R"
REFERENCES,0.34983498349834985,"A dvolX , for any Borel-measurable subset A on the space X. For more references,
476"
REFERENCES,0.3504950495049505,"see [9].
477"
REFERENCES,0.35115511551155115,"We recall the latent representation problem defined in the main paper.
478"
REFERENCES,0.35181518151815183,"Consider the state space S ⊂RdS and the latent space Z. Consider a state probability measure Q on
479"
REFERENCES,0.35247524752475246,"the state space S and a probability measure P on the latent space Z.
480"
REFERENCES,0.35313531353135313,"Assumption A.3. (Latent manifold assumption) For a positive integer k, there exists a dM-
481"
REFERENCES,0.3537953795379538,"dimensional Ck,α submanifold M (with Ck+3,α boundary) with Riemannian metric g and has
482"
REFERENCES,0.35445544554455444,"positive reach and also isometrically embedded in the state space S ⊂RdS and dM << dS, where
483"
REFERENCES,0.3551155115511551,"the state probability measure is supported on. In addition, M is a compact, orientable, connected
484"
REFERENCES,0.3557755775577558,"manifold.
485"
REFERENCES,0.3564356435643564,"Assumption A.4. (Smoothness of state probability measure) Q is a probability measure supported
486"
REFERENCES,0.3570957095709571,"on M with its Radon-Nikodym derivative q ∈Ck,α(M, R) w.r.t µM.
487"
REFERENCES,0.3577557755775578,"Let Z be a closed ball in RdM, that is {x ∈RdM : ∥x∥≤1 }. P is a probability measure supported
488"
REFERENCES,0.3584158415841584,"on Z with its Radon-Nikodym derivative p ∈Ck,α(Z, R) w.r.t µZ.
489"
REFERENCES,0.3590759075907591,"We consider a general CNN function fCNN : X →R. Let fCNN have L hidden layers, represented as:
490"
REFERENCES,0.35973597359735976,"fCNN(x) = AL+1 ◦AL ◦· · · ◦A2 ◦A1(x),
x ∈X,"
REFERENCES,0.3603960396039604,"where Ai’s are either convolutional or downsampling operators. For convolutional layers,
491"
REFERENCES,0.36105610561056106,"Ai(x) = σ(W c
i x + bc
i),"
REFERENCES,0.36171617161716174,"where W c
i ∈Rdi×di−1 is a structured sparse Toeplitz matrix from the convolutional filter {w(i)
j }s(i)
j=0
492"
REFERENCES,0.36237623762376237,"with filter length s(i) ∈N+, bc
i ∈Rdi is a bias vector, and σ is the ReLU activation function.
493"
REFERENCES,0.36303630363036304,"For downsampling layers,
494"
REFERENCES,0.3636963696369637,"Ai(x) = Di(x) = (xjmi)⌊di−1/mi⌋
j=1
,"
REFERENCES,0.36435643564356435,"Figure 2: Latent Representation Problem: The left and right denote the manifold M with lower dim
dM embedded in a larger Euclidean space, with latent space Z a dM-dimensional ball in middle.
Encoder and decoder as maps respectively pushing forward Q to P and P to Q."
REFERENCES,0.365016501650165,"where Di : Rdi×di−1 is the downsampling operator with scaling parameter mi ≤di−1 in the i-th
495"
REFERENCES,0.36567656765676565,"layer. The convolutional and downsampling operations are elaborated in Appendix [63]. We examine
496"
REFERENCES,0.36633663366336633,"the class of functions represented by CNNs, denoted by FCNN, defined as:
497"
REFERENCES,0.366996699669967,"FCNN = {fCNN as in defined above with any choice of Ai, i = 1, . . . , L + 1}."
REFERENCES,0.36765676567656763,"For more details in the definitions of CNN functions, we refer to [29].
498"
REFERENCES,0.3683168316831683,"Assumption A.5. Assume that M and Z are locally diffeomorphic, that is there exists a map
499"
REFERENCES,0.368976897689769,"F : M →Z such that at every point x on M, det(d F(x)) ̸= 0.
500"
REFERENCES,0.3696369636963696,"Theorem A.6. (Approximation Error of Latent Representation). Under Assumption A.3, A.4 and
501"
REFERENCES,0.3702970297029703,"A.5, for θ ∈(0, 1), let dθ = O(dMθ−2 log d"
REFERENCES,0.37095709570957097,"θ). For positive integers M and N, there exists an
502"
REFERENCES,0.3716171617161716,"encoder genc and decoder gdec ∈FCNN(L, S, W) s.t.
503"
REFERENCES,0.3722772277227723,"W1(genc#Q, P) ≤dMC(NM)−2(k+1) dθ
,"
REFERENCES,0.37293729372937295,"W1(gdec#P, Q) ≤dMC(NM)−2(k+1) dθ
."
REFERENCES,0.3735973597359736,"The primary challenge to show Theorem A.6 is in demonstrating the existence of oracle encoder and
504"
REFERENCES,0.37425742574257426,"decoder maps. These maps, denoted as g∗
enc : M →Z and g∗
dec : Z →M respectively, must satisfy
505"
REFERENCES,0.37491749174917494,"g∗
enc# Q = P,
g∗
dec# P = Q.
(21)"
REFERENCES,0.37557755775577556,"and importantly they have the proper smoothness guarantee, namely g∗
enc ∈Ck+1,α(M, Z) and
506"
REFERENCES,0.37623762376237624,"g∗
dec ∈Ck+1,α(Z, M). Proposition A.7 shows the existence of such oracle map(s).
507"
REFERENCES,0.3768976897689769,"Proposition A.7 (Ck,α, compact). Let M, N be compact, oriented d-dimensional Riemannian
508"
REFERENCES,0.37755775577557754,"manifolds with Ck+3,α boundary with the volume measure µM and µN respectively. Let Q, P be
509"
REFERENCES,0.3782178217821782,"distributions supported on M, N respectively with their Ck,α density functions q, p, that is Q, P are
510"
REFERENCES,0.3788778877887789,"probability measures supported on M, N with their Radon-Nikodym derivatives q ∈Ck,α(M, R)
511"
REFERENCES,0.3795379537953795,"w.r.t µM and p ∈Ck,α(N, R) w.r.t µN . Then, there exists a Ck+1,α map g : N →M such that
512"
REFERENCES,0.3801980198019802,"the pushforward measure g#P = Q, that is for any measurable subset A ∈B(M), Q(A) =
513"
REFERENCES,0.3808580858085809,"P(g−1(A)).
514"
REFERENCES,0.3815181518151815,"Proof. (Proposition A.7) Let ω := p dvolN , then ω is a Ck,α volume form on N, as p ∈Ck,α and for
515"
REFERENCES,0.3821782178217822,"any point x ∈N, we have p(x) > 0. In addition,
R"
REFERENCES,0.38283828382838286,"N ω =
R"
REFERENCES,0.3834983498349835,"N p dvolN =
R"
REFERENCES,0.38415841584158417,"N p dµN = P(N) = 1.
516"
REFERENCES,0.38481848184818485,"Similarly, let η := q dvolM a Ck,α volume form on M and
R"
REFERENCES,0.38547854785478547,"M η = 1.
517 518"
REFERENCES,0.38613861386138615,"Let F : N →M be an orientation-preserving local diffeomorphism, we then have det(dF) > 0
519"
REFERENCES,0.38679867986798677,"everywhere on N.
520"
REFERENCES,0.38745874587458745,"As N is compact and M is connected by assumption, F is a covering map, that is for every point
521"
REFERENCES,0.38811881188118813,"x ∈M, there exists an open neighborhood Ux of x and a discrete set Dx such that F −1(U) =
522"
REFERENCES,0.38877887788778875,"⊔α∈D Vα ⊂N and F|Vα = Vα →U is a diffeomorphism. Furthermore, |Dx| = |Dy| for any points
523"
REFERENCES,0.38943894389438943,"x, y ∈M. In addition, |Dx| is finite from the compactness of N.
524"
REFERENCES,0.3900990099009901,"Let ¯η be the pushforward of ω via F, defined by for any point x ∈M and a neighborhood Ux,
525"
REFERENCES,0.39075907590759074,"¯η(x) :=
1
|Dx| X α∈Dx"
REFERENCES,0.3914191419141914,"
F

Vα"
REFERENCES,0.3920792079207921,"−1∗
ω

Vα.
(22)"
REFERENCES,0.3927392739273927,"¯η is well-defined as it is not dependent on the choice of neighborhoods and the sum and
1
|Dx| are
526"
REFERENCES,0.3933993399339934,"always finite. Furthermore, ¯η is a Ck,α volume form on M, as p ◦

F

Vα"
REFERENCES,0.3940594059405941,"−1
is Ck,α.
527 528"
REFERENCES,0.3947194719471947,"Notice that F

Vα"
REFERENCES,0.3953795379537954,"−1 is orientation-preserving as det d F

Vα"
REFERENCES,0.39603960396039606,"−1 =
1"
REFERENCES,0.3966996699669967,"det d F

Vα
> 0 everywhere on Vα.
529"
REFERENCES,0.39735973597359736,"In addition, F

Vα"
REFERENCES,0.39801980198019804,"−1 is proper: as for any compact subset K of N, K is closed; and as F

Vα"
REFERENCES,0.39867986798679866,"−1
530"
REFERENCES,0.39933993399339934,"is continuous, the preimage of K via F

Vα"
REFERENCES,0.4,"−1 a closed subset of M which is compact, then the
531"
REFERENCES,0.40066006600660065,"preimage of K must also be compact. Hence, F

Vα"
REFERENCES,0.4013201320132013,"−1 is proper. As every F

Vα"
REFERENCES,0.401980198019802,"−1 is proper,
532"
REFERENCES,0.40264026402640263,"orientation-preserving and surjective, then c := deg(F

Vα"
REFERENCES,0.4033003300330033,"−1) = 1.
533"
REFERENCES,0.403960396039604,"Then,
R"
REFERENCES,0.4046204620462046,"M ¯η = c
R"
REFERENCES,0.4052805280528053,"N ω = 1.
534 535"
REFERENCES,0.40594059405940597,"As we have shown that η and ¯η ∈Ck,α and
R"
REFERENCES,0.4066006600660066,"M ¯η =
R"
REFERENCES,0.40726072607260727,"M η, by [6], there exists a diffeomorphism
536"
REFERENCES,0.4079207920792079,"ψ : M →M fixing on the boundary such that ψ∗η = ¯η, where ψ, ψ−1 ∈Ck+1,α.
537"
REFERENCES,0.4085808580858086,"Let g := ψ ◦F, then it holds that g∗η = (ψ ◦F)∗η = F ∗◦ψ∗η = F ∗¯η = ω.
538"
REFERENCES,0.40924092409240925,"Then, for any measurable subset A on the manifold M, we verify that Q(A) =
R"
REFERENCES,0.4099009900990099,"A η =
539
R"
REFERENCES,0.41056105610561056,"g−1(A) g∗η =
R"
REFERENCES,0.41122112211221123,"g−1(A) ω =
R"
REFERENCES,0.41188118811881186,"g−1(A) p dvolN =
R"
REFERENCES,0.41254125412541254,"g−1(A) p dµN = P(g−1(A)).
540 541"
REFERENCES,0.4132013201320132,"Hence, we have shown the existence by an explicit construction. As ψ ∈Ck+1,α, and F ∈C∞, then
542"
REFERENCES,0.41386138613861384,"we have g ∈Ck+1,α.
543"
REFERENCES,0.4145214521452145,"We are now ready to show Theorem A.6 with the existence of oracle map and the low-dimensional
544"
REFERENCES,0.4151815181518152,"approximation results from [29].
545"
REFERENCES,0.4158415841584158,"Proof. (Theorem A.6) For encoder, from Proposition A.7, there exists an Ck+1,α oracle map g :
546"
REFERENCES,0.4165016501650165,"M →Z such that the pushforward measure g#Q = P. Then,
547"
REFERENCES,0.4171617161716172,"W1((genc)#Q , P) =W1((genc)#Q , g#Q)"
REFERENCES,0.4178217821782178,"=
sup
f∈Lip1(Z)  Z"
REFERENCES,0.4184818481848185,"Z
f(y) d((genc)#Q) −
Z"
REFERENCES,0.41914191419141916,"Z
f(y) d(g#Q)"
REFERENCES,0.4198019801980198,"≤
sup
f∈Lip1(Z) Z"
REFERENCES,0.42046204620462047,"M
|f ◦genc(x) −f ◦g(x)| dQ ≤
Z"
REFERENCES,0.42112211221122114,"M
∥genc(x) −g(x)∥dQ"
REFERENCES,0.42178217821782177,"≤dMC(NM)−2(k+1) dθ
,"
REFERENCES,0.42244224422442245,"where the last inequality follows from the special case ρ = 0 of Theorem 2.4 in [29].
548"
REFERENCES,0.4231023102310231,"Similarly, for decoder, from Proposition A.7, there exists an Ck+1,α oracle map ¯g : Z →M such
549"
REFERENCES,0.42376237623762375,"that the pushforward measure ¯g#P = Q.
550"
REFERENCES,0.42442244224422443,"W1((gdec)#P , Q) =W1((gdec)#P , ¯g#P) ≤
Z"
REFERENCES,0.4250825082508251,"Z
∥gdec(y) −¯g(y)∥dP"
REFERENCES,0.42574257425742573,"≤dMC(NM)−2(k+1) dθ
. 551"
REFERENCES,0.4264026402640264,"B
Explicit Regularization of Latent Representation Error in World Model
552"
REFERENCES,0.4270627062706271,"Learning
553"
REFERENCES,0.4277227722772277,"We recall the SDEs for latent dynamics model defined in the main paper. Consider a complete,
554"
REFERENCES,0.4283828382838284,"filtered probability space (Ω, F, {Ft}t∈[0,T ], P ) where independent standard Brownian motions
555"
REFERENCES,0.429042904290429,"B enc
t
, B pred
t
, B seq
t
, B dec
t
are defined such that Ft is their augmented filtration, and T ∈R as the time
556"
REFERENCES,0.4297029702970297,"length of the task environment. We consider the stochastic dynamics of LDM through the following
557"
REFERENCES,0.4303630363036304,"coupled SDEs after error perturbation:
558"
REFERENCES,0.431023102310231,"d zt = (qenc(ht, st) + σ(ht, st)) dt + (¯qenc(ht, st) + ¯σ(ht, st)) dB enc
t
,
(23)"
REFERENCES,0.4316831683168317,"d ht = f(ht, zt, π(ht, zt)) dt + ¯f(ht, zt, π(ht, zt)) dB seq
t
(24)"
REFERENCES,0.43234323432343236,"d ˜zt = p(ht) dt + ¯p(ht) dB pred
t
,
(25)"
REFERENCES,0.433003300330033,"d ˜st = qdec(ht, ˜zt) dt + ¯qdec(ht, ˜zt) dB dec
t
,
(26)"
REFERENCES,0.43366336633663366,"where π(h, ˜z) is a policy function as a local maximizer of value function and the stochastic process
559"
REFERENCES,0.43432343234323434,"st is Ft-adapted.
560"
REFERENCES,0.43498349834983496,"As discussed in the main paper, our analysis applies to a common class of world models that uses
561"
REFERENCES,0.43564356435643564,"Gaussian distributions parameterized by neural networks’ outputs for z, ˜z, ˜s. Their distributions are
562"
REFERENCES,0.4363036303630363,"not non-Gaussian in general.
563"
REFERENCES,0.43696369636963694,"For example, as z is conditional Gaussian and its mean and variance are random variables which are
564"
REFERENCES,0.4376237623762376,"learned by the encoder from r.v.s s and h as inputs, thus rendering z non-Gaussian. However, z is
565"
REFERENCES,0.4382838283828383,"indeed Gaussian when the inputs are known. Under this conditional Gaussian class of world models,
566"
REFERENCES,0.4389438943894389,"to see that the continuous formulation of latent dynamics model can be interrupted as SDEs, one
567"
REFERENCES,0.4396039603960396,"notices that SDEs with coefficient functions of known inputs are indeed Gaussian, matching to this
568"
REFERENCES,0.4402640264026403,"class of world models. Formally, in the context of z without latent representation error:
569"
REFERENCES,0.4409240924092409,"Proposition B.1. (Latent states SDE with known inputs is Gaussian)
570"
REFERENCES,0.4415841584158416,"For the latent state process zt∈[0,T ] without error,
571"
REFERENCES,0.44224422442244227,"d zt = qenc(ht, st) dt + ¯qenc(ht, st))dB enc
t
,
(27)"
REFERENCES,0.4429042904290429,"with zero initial value. Given known ht∈[0,T ] and st∈[0,T ], the process zt is a Gaussian process.
572"
REFERENCES,0.44356435643564357,"Furthermore, for any t ∈[0, T], zt follows a Gaussian distribution with mean µt =
R t
0 qenc(hs, ss)ds
573"
REFERENCES,0.44422442244224425,"and variance σ2
t =
R t
0 ¯qenc(hs, ss)2ds.
574"
REFERENCES,0.4448844884488449,"Proof. Proof follows from Proposition 7.6 in [30].
575"
REFERENCES,0.44554455445544555,"Next, we recall our assumptions from the main text:
576"
REFERENCES,0.44620462046204623,"Assumption B.2. The drift coefficient functions qenc, f, p and qdec and the diffusion coefficient
577"
REFERENCES,0.44686468646864685,"functions ¯qenc, ¯p and ¯qdec are bounded and Borel-measurable over the interval [0, T], and of class C3
578"
REFERENCES,0.44752475247524753,"with bounded Lipschitz continuous partial derivatives. The initial values z0, h0, ˜z0, ˜s0 are square-
579"
REFERENCES,0.44818481848184816,"integrable random variables.
580"
REFERENCES,0.44884488448844884,"Assumption B.3. σ and ¯σ are bounded and Borel-measurable and are of class C3 with bounded
581"
REFERENCES,0.4495049504950495,"Lipschitz continuous partial derivatives over the interval [0, T].
582"
REFERENCES,0.45016501650165014,"One of our main results is the following:
583"
REFERENCES,0.4508250825082508,"Theorem B.4. (Explicit Regularization Induced by Zero-Drift Representation Error)
584"
REFERENCES,0.4514851485148515,"Under Assumption B.2 and B.3 and considering a loss function L ∈C2, the explicit effects of the
585"
REFERENCES,0.4521452145214521,"zero-drift error can be marginalized out as follows:
586"
REFERENCES,0.4528052805280528,"E L (xε
t) = E L(x0
t) + R + O(ε3),
(28)"
REFERENCES,0.4534653465346535,"as ε →0, where the regularization term R is given by R := ε P + ε2  
Q + 1"
S,0.4541254125412541,"2 S

.
587"
S,0.4547854785478548,"Each term of R is as follows:
588"
S,0.45544554455445546,"P := E ∇L(x0
t)⊤Φt
X"
S,0.4561056105610561,"k
ξk
t ,
(29)"
S,0.45676567656765676,"Q := E ∇L(x0
t)⊤Φt Z t"
S,0.45742574257425744,"0
Φ−1
s
Hk(x0
s, s)dBk
t ,
(30)"
S,0.45808580858085807,"S := E
X"
S,0.45874587458745875,"k1,k2
(Φtξk1
t )i∇2L(x0
t, t) (Φtξk2
t )j,
(31)"
S,0.4594059405940594,"where square matrix Φt is the stochastic fundamental matrix of the corresponding homogeneous
589"
S,0.46006600660066005,"equation:
590"
S,0.46072607260726073,dΦt = ∂¯gk
S,0.4613861386138614,"∂x (x0
t, t) Φt dBk
t ,
Φ(0) = I,"
S,0.46204620462046203,"and ξk
t is as the shorthand for
R t
0 Φ−1
s ¯σk(x0
s, s)dBk
t . Additionally, Hk(x0
s, s) is represented by for
591
P"
S,0.4627062706270627,"k1,k2
∂2¯gk
∂xi∂xj (x0
s, s)
 
ξk1
s
i  
ξk2
s
j.
592"
S,0.4633663366336634,"Before proving Theorem B.4, we first show Proposition B.5 on the general case of perturbation to the
593"
S,0.464026402640264,"stochastic system. Consider the following perturbed system given by
594"
S,0.4646864686468647,"d xt = (g0 (xt, t) + ε η0 (xt, t)) dt + m
X"
S,0.46534653465346537,"k=1
(gk (xt, t) + ε ηk (xt, t)) dBk
t
(32)"
S,0.466006600660066,"with initial values x(0) = x0,
595"
S,0.4666666666666667,"Proposition B.5. Suppose that f is a real-valued function that is C2. Then it holds that, with
596"
S,0.46732673267326735,"probability 1, as ε →0, for t ∈[0, T],
597"
S,0.467986798679868,"f (xε
t) = f
 
x0
t

+ε∇f
 
x0
t
⊤∂ε x0
t +ε2 
∇f
 
x0
t
⊤∂2
εx0
t +1"
S,0.46864686468646866,"2∂ε x0
t
⊤∇2f
 
x0
t

∂ε x0
t"
S,0.4693069306930693,"
+O
 
ε3
,"
S,0.46996699669966996,"(33)
where the stochastic process x0
t is the solution to SDE 32 with ε = 0, with its first and second-order
598"
S,0.47062706270627064,"derivatives w.r.t ε denoted as ∂ε x0
t, ∂2
ε x0
t.
599"
S,0.47128712871287126,"Furthermore, it holds that ∂ε x0
t, ∂2
ε x0
t satisfy the following SDEs with probability 1,
600"
S,0.47194719471947194,"d ∂εx0
t =
∂gk"
S,0.4726072607260726,"∂x
 
x0
t, t

∂εx0
t + ηk
 
x0
t, t

dBk
t ,"
S,0.47326732673267324,"d ∂2
εxt =

Ψk
 
∂εx0
t, x0
t, t

+ 2∂ηk"
S,0.4739273927392739,"∂x
 
x0
t, t

∂εx0
t + ∂gk"
S,0.4745874587458746,"∂x
 
x0
t, t

∂2
εx0
t"
S,0.4752475247524752,"
dBk
t ,
(34)"
S,0.4759075907590759,"with initial values ∂ε x(0) = 0, ∂2
ε x(0) = 0, where
601"
S,0.4765676567656766,"Ψk : (∂ε x, x, t) 7→∂ε xi
∂gk
∂xi∂xj (x, t)∂ε xj,"
S,0.4772277227722772,"for k = 0, 1, ..., m.
602"
S,0.4778877887788779,"Proof. We first apply the stochastic version of perturbation theory to SDE 32. For brevity, we will
603"
S,0.47854785478547857,"write t as B0
t and use Einstein summation convention. Hence, SDE 32 is rewritten as
604"
S,0.4792079207920792,"dxt = γε
k (xt, t) dBk
t ,
(35)"
S,0.47986798679867987,"with initial value x(0) = x0.
605"
S,0.48052805280528055,"Step 1: We begin with the corresponding systems to derive the SDEs that characterize ∂ε xε
t and ∂2
ε xε
t.
606"
S,0.48118811881188117,"Our main tool is an important result on smoothness of solutions w.r.t. initial data from Theorem 3.1
607"
S,0.48184818481848185,"from Section 2 in [17].
608"
S,0.48250825082508253,"For ∂ε x, consider the SDEs
609"
S,0.48316831683168315,"d xt = γε
k (xt, t) dBk
t ,
(*)
d εt = 0,"
S,0.48382838283828383,"with initial values x(0) = x0, ε(0) = ε. From an application of Theorem 3.1 from Section 2 in [17]
610"
S,0.4844884488448845,"on *, we have ∂ε x that satisfies the following SDE with probability 1:
611"
S,0.48514851485148514,"d ∂εxt = (αε
k (xt, t) ∂εxt + ηk (xt, t)) dBk
t ,
(36)"
S,0.4858085808580858,"with initial value ∂εx0 = 0 ∈Rn, with probability 1, where xt is the solution to Equation (35) and
612"
S,0.4864686468646865,"the functions αε
k are given by
613"
S,0.4871287128712871,"αε
k : (x, t) 7→∂gk"
S,0.4877887788778878,"∂xj (x, t) + ε∂ηk"
S,0.4884488448844885,"∂xj (x, t) ,"
S,0.4891089108910891,"where k = 0, ..., m.
614"
S,0.4897689768976898,"To characterize ∂2
ε xt, consider the following SDEs
615"
S,0.4904290429042904,"d xt = γε
k (xt, t) dBk
t ,
(**)"
S,0.4910891089108911,"d ∂ε xt = (αε
k (xt, t) ∂ε xt + ηk (xt, t)) dBk
t ,
d εt = 0,"
S,0.49174917491749176,"with initial value x(0) = x0, ∂ε x(0) = 0, ε(0) = ε.
616"
S,0.4924092409240924,"From a similar application of Theorem 3.1 from Section 2 in [17], the second derivative ∂2
ε x satisfies
617"
S,0.49306930693069306,"the following SDE with probability 1:
618"
S,0.49372937293729374,"d ∂2
ε xt =

βε
k (∂εxt, xt, t) + 2∂ηk"
S,0.49438943894389437,"∂x (xt, t) ∂ε xt + αε
k (xt, t) ∂2
εxt"
S,0.49504950495049505,"
dBk
t ,
(37)"
S,0.4957095709570957,"with initial value ∂2
ε x(0) = 0 ∈Rn, where ∂ε xt is the solution to Equation(36), x(t) is the solution
619"
S,0.49636963696369635,"to Equation (35), and the functions
620"
S,0.497029702970297,"βε
k : (∂ε x, x, t) 7→∂ε xj 
∂gi
k
∂xl∂xj (x, t) + ε
∂ηi
k
∂xl∂xj (x, t)

∂ε xl, where k = 0, ..., m.
621"
S,0.4976897689768977,"When ε = 0 in the obtained SDEs (35), (36) and (37), the corresponding solutions of which are
622"
S,0.49834983498349833,"x0
t, ∂ε x0
t, ∂2
ε x0
t, we now have the following:
623"
S,0.499009900990099,"d x0
t = gk
 
x0
t, t

dBk
t ,
(38)"
S,0.4996699669966997,"d ∂ε x0
t =
∂gk"
S,0.5003300330033004,"∂x
 
x0
t, t

∂ε x0 + ηk
 
x0
t, t

dBk
t ,
(39)"
S,0.500990099009901,"d ∂2
ε x0
t =

Ψk
 
∂ε x0
t, x0
t, t

+ 2∂ηk"
S,0.5016501650165016,"∂x
 
x0
t, t

∂ε x0
t + ∂gk"
S,0.5023102310231023,"∂x
 
x0
t, t

∂2
ε x0
t"
S,0.502970297029703,"
dBk
t ,
(40)"
S,0.5036303630363036,"with initial values x(0) = x0, ∂ε x(0) = 0, ∂2
ε x(0) = 0. In particular, Ψk := β0
k is given by
624"
S,0.5042904290429043,"(∂εx, x, t) 7→∂εxi
∂gk
∂xi∂xi (x, t)∂εxj."
S,0.504950495049505,"Step 2: For the next step, we show that the solutions x0
t, ∂s x0
t, ∂2
ε x0
t are indeed bounded by proving
625"
S,0.5056105610561056,"the following lemma B.6:
626"
S,0.5062706270627063,Lemma B.6.
S,0.5069306930693069,"E sup
t∈[0,T ]"
S,0.5075907590759076,"x0
t
2 , E sup
t∈[0,T ]"
S,0.5082508250825083,"∂ε x0
t
2 , and E sup
t∈[0,T ]"
S,0.5089108910891089,"∂2
ε x0
t
2 are bounded."
S,0.5095709570957095,"Proof. To simplify the notations, we take the liberty to write constants as C and notice that C is not
627"
S,0.5102310231023103,"necessarily identical in its each appearance.
628"
S,0.5108910891089109,"(1) We first show that E supt∈[0,T ]
x0
t
2 is bounded.
629"
S,0.5115511551155115,"From Equation (38), we have that"
S,0.5122112211221123,"x0
t = x0 +
Z t"
S,0.5128712871287129,"0
gk (xτ, τ) dBk
τ ."
S,0.5135313531353135,"By Jensen’s inequality. it holds that
630"
S,0.5141914191419142,"E sup
t∈[0,T ]
∥xt∥2 ≤C E ∥x0∥2 + C E sup
t∈[0,T ]  Z t"
GK,0.5148514851485149,"0
gk
 
x0
τ, τ

dBk
τ "
GK,0.5155115511551155,"2
.
(41)"
GK,0.5161716171617162,"For the second term on the right hand side, it is a sum over k from 0 to m by Einstein notation.
631"
GK,0.5168316831683168,"For k = 0, recall that we write t as B0
t :
632"
GK,0.5174917491749175,"E sup
t∈[0,T ]  Z t"
GK,0.5181518151815182,"0
g0
 
x0
τ, τ

dτ"
GK,0.5188118811881188,"2
≤C E sup
t∈[0,T ]
t
Z t 0"
GK,0.5194719471947195,"g0
 
x0
τ, τ
2 dτ,
(i)"
GK,0.5201320132013202,"≤C E sup
t∈[0,T ] Z t"
C,0.5207920792079208,"0
C
 
1 +
x0
τ
2 dτ,
(ii)"
C,0.5214521452145214,"≤C + C
Z T"
E SUP,0.5221122112211221,"0
E sup
s∈[0,τ]"
E SUP,0.5227722772277228,"x0
s
2 dτ,
(iii)"
E SUP,0.5234323432343234,"where we used Jensen’s inequality, the assumption on the linear growth, the inequality property of
633"
E SUP,0.524092409240924,"sup and Fubini’s theorem, respectively.
634"
E SUP,0.5247524752475248,"For k is equal to 1, . . . , m,
635"
E SUP,0.5254125412541254,"E sup
t∈[0,T ]  Z t"
E SUP,0.526072607260726,"0
g1
 
x0
τ,τ, τ

dBτ "
E SUP,0.5267326732673268,"2
≤C E
Z T 0"
E SUP,0.5273927392739274,"g1
 
x0
τ, τ
2 dτ,
(iv)"
E SUP,0.528052805280528,"≤C + C
Z T"
E SUP,0.5287128712871287,"0
E sup
s∈[0,τ]"
E SUP,0.5293729372937294,"x0
s
 dτ,
(v)"
E SUP,0.53003300330033,"where (iv) holds from the Burkholder-Davis-Gundy inequality as
R t
0 gk
 
x0
τ, τ

dBτ is a continuous
636"
E SUP,0.5306930693069307,"local martingale with respect to the filtration Ft; and then one can obtain (v) by following a similar
637"
E SUP,0.5313531353135313,"reasoning of (ii) and (iii).
638"
E SUP,0.532013201320132,"Hence, now from the previous inequality (41),"
E SUP,0.5326732673267327,"E sup
t∈[0,T ]"
E SUP,0.5333333333333333,"x0
t
2 ≤E ∥x0∥2 + C + C
Z T"
E SUP,0.533993399339934,"0
E sup
s∈[0,τ]"
E SUP,0.5346534653465347,"x0
s
 dτ."
E SUP,0.5353135313531353,"By the Gronwall’s lemma, it holds true that"
E SUP,0.5359735973597359,"E sup
t∈[0,T ]"
E SUP,0.5366336633663367,"x0
t
2 ≤

C E ∥x0∥2 + C

exp(C)."
E SUP,0.5372937293729373,"As x0 is square-integrable by assumption, therefore we have shown that E supt∈[0,T ]
x0
t
2 is
639"
E SUP,0.5379537953795379,"bounded.
640"
E SUP,0.5386138613861386,"(2) We then show that E sup
t∈[0,T ]
||∂ε x0
t||2 is also bounded.
641"
E SUP,0.5392739273927393,"From the SDE (39), as we have derived that"
E SUP,0.5399339933993399,"∂ε x0
t =
Z t 0 ∂gk"
E SUP,0.5405940594059406,"∂x
 
x0
τ, τ

∂ε x0
τ + ηk
 
x0
τ, τ

dBk
τ ,"
E SUP,0.5412541254125413,then we have
E SUP,0.5419141914191419,"E sup
t∈[0,τ]"
E SUP,0.5425742574257426,"∂ε x0
t
2 ≤C E sup
t∈[0,τ]  Z t 0 ∂gk"
E SUP,0.5432343234323432,"∂x
 
x0
τ, τ

∂ε x0
τ dBk
τ "
E SUP,0.5438943894389439,"2
+ C E sup
t∈[0,T ]  Z t"
E SUP,0.5445544554455446,"0
ηk
 
x0
τ, τ

dBk
τ  2
."
E SUP,0.5452145214521452,"For k = 0, we have
642"
E SUP,0.5458745874587458,"E sup
t∈[0,T ]  Z t 0 ∂g0"
E SUP,0.5465346534653466,"∂x
 
x0
τ, τ

∂ε x0
τdt"
E SUP,0.5471947194719472,"2
+ E sup
t∈[0,T ]  Z t"
E SUP,0.5478547854785478,"0
η0
 
x0
τ, τ

dτ"
E SUP,0.5485148514851486,"2
,
(vi)"
E SUP,0.5491749174917492,"≤C E sup
t∈[0,T ] Z t 0 ∂g0"
E SUP,0.5498349834983498,"∂x
 
x0
τ, t
"
E SUP,0.5504950495049505,"2 ∂ε x0
τ
2 dτ + CE sup
t∈[0,T ] Z t 0"
E SUP,0.5511551155115512,"η0
 
x0
τ, τ
2 dτ,
(vii)"
E SUP,0.5518151815181518,"≤C E sup
s∈[0,T ] ∂g0"
E SUP,0.5524752475247525,"∂x
 
x0
s, s
"
SUP,0.5531353135313531,"2
sup
t∈[0,T ] Z t 0"
SUP,0.5537953795379538,"∂ε x0
τ
2 dτ + C E sup
t∈[0,T ] Z t"
C,0.5544554455445545,"0
C
 
1 +
x0
τ
2 dτ,"
C,0.5551155115511551,"≤C + C E sup
t∈[0,T ] Z t 0"
C,0.5557755775577558,"∂ε x0
τ
2 dτ + C E sup
t∈[0,T ] Z t 0"
C,0.5564356435643565,"x0
τ
2 dτ,
(viii)"
C,0.5570957095709571,"≤C + C
Z T"
E SUP,0.5577557755775577,"0
E sup
s∈[0,τ]"
E SUP,0.5584158415841585,"∂ε x0
s
2 dτ + C E sup
t∈[0,T ]"
E SUP,0.5590759075907591,"x0
t
2 ,"
E SUP,0.5597359735973597,"where to get to (vi), we used Jensen’s inequality; for (vii), we used the linear growth assumption an
643"
E SUP,0.5603960396039604,"η0, then we obtain (viii) by as derivatives of function g0 are bounded by assumption.
644"
E SUP,0.5610561056105611,"Similarly, for k = 1, ..., m,
645"
E SUP,0.5617161716171617,"C E sup
t∈[0,T ]  Z t 0"
E SUP,0.5623762376237624,"∂g1
∂xi
 
x0
τ, τ

∂ε x0
τdBτ "
E SUP,0.563036303630363,"2
+ C E sup
t∈[0,T ]  Z t"
E SUP,0.5636963696369637,"0
η1
 
x0
τ, τ

dBτ  2
,"
E SUP,0.5643564356435643,"≤C E
Z T 0 ∂g1"
E SUP,0.565016501650165,"∂x
 
x0
τ, τ
"
E SUP,0.5656765676567657,"2 ∂ε x0
τ
2 dτ + C E
Z T 0"
E SUP,0.5663366336633663,"η1
 
x0
τ, τ
2 dτ,
(ix)"
E SUP,0.566996699669967,"≤C + C
Z T"
E SUP,0.5676567656765676,"0
E sup
s∈[0,τ]
||∂ε x0
s||2dτ + C E sup
t∈[0,T ]
||x0
t||2,
(x)"
E SUP,0.5683168316831683,"where we obtain (ix) by the Burkholder-Davis-Gundy inequality and (x) by following similar steps as
646"
E SUP,0.568976897689769,"have shown in (vii) and (viii).
647"
E SUP,0.5696369636963696,"We are now ready to sum up each term to acquire a new inequality:
648"
E SUP,0.5702970297029702,"E sup
t∈[0,T ]"
E SUP,0.570957095709571,"∂ε x0
t
2 ≤C + C E sup
t∈[0,T ]"
E SUP,0.5716171617161716,"x0
t
2 + C
Z T"
E SUP,0.5722772277227722,"0
E sup
s∈[0,τ]"
E SUP,0.572937293729373,"∂ε x0
s
2 dτ."
E SUP,0.5735973597359736,"By Gronwall’s lemma, we have that
649"
E SUP,0.5742574257425742,"E sup
t∈[0,T ]"
E SUP,0.574917491749175,"∂ε x0
t
2 ≤ "
E SUP,0.5755775577557756,"C + C E sup
t∈[0,T ]"
E SUP,0.5762376237623762,"x0
t
2
!"
E SUP,0.5768976897689769,exp(C).
E SUP,0.5775577557755776,"As it is previously shown that E supt∈[0,τ] ∥x◦(t)∥2 is bounded, it is clear that E supt∈[0,T ]
∂ε x0
t
2
650"
E SUP,0.5782178217821782,"is bounded too.
651"
E SUP,0.5788778877887789,"(3) From similar steps, one can also show that E sup
t∈[0,T ]"
E SUP,0.5795379537953795,"∂2
ε x0
t
2 is bounded.
652"
E SUP,0.5801980198019802,"Step 3: Having shown that x0
t, ∂ε x0
t, ∂2
ε x0
t are bounded, we proceed to bound the remainder term by
653"
E SUP,0.5808580858085809,"proving the following lemma.
654"
E SUP,0.5815181518151815,"Lemma B.7. For a given ε ∈R, let"
E SUP,0.5821782178217821,"Rε := (t, ω) 7→1"
E SUP,0.5828382838283829,"ε3
 
xε(t, ω) −x0(t, ω) −ε∂εx0(t, ω) −ε2∂2
ε x0(t, ω)

,"
E SUP,0.5834983498349835,"where the stochastic process xε
t is the solution to Equation (32). Then it holds true that
655"
E SUP,0.5841584158415841,"E sup
t∈[0,T ]
∥Rε(t)∥2 is bounded."
E SUP,0.5848184818481849,"Proof. The main strategy of this proof is to first rewrite ε3Rε as the sum of some simpler terms and
then to bound each term. To simplify the notation, we denote ˜xε
t as x0
t + ε∂ε x0
t + ε2 ∂2
εx0
t.
For k = 0, .., n, we define the following terms:"
E SUP,0.5854785478547855,"θk(t) :=
Z t"
E SUP,0.5861386138613861,"0
gk (xε
τ, τ) −gk (˜xε
τ, τ) dBk
τ ,"
E SUP,0.5867986798679868,"φk(t) :=
Z t"
E SUP,0.5874587458745875,"0
gk (˜xε
τ, τ) −gk
 
x0
τ, τ

−ε∂gk"
E SUP,0.5881188118811881,"∂x
 
x0
τ, τ

∂ε x0
τ −ε2Ψk
 
∂ε x0
τ, x0
τ, τ

−ε2 ∂gk"
E SUP,0.5887788778877888,"∂xi
 
x0
τ, τ

∂2
ε x0
τdBk
τ ,"
E SUP,0.5894389438943894,"σk(t) := −ε
Z t"
E SUP,0.5900990099009901,"0
ηk
 
x0
τ, τ

+ 2ε ∂η"
E SUP,0.5907590759075908,"∂x
 
x0
τ, τ

∂ε x0
τdBk
τ ."
E SUP,0.5914191419141914,"Hence, we have ε3Rε(t) = P1
k=0 θk(t) + φk(t) + σk(t).
656"
E SUP,0.592079207920792,"For θk(t), we have
657"
E SUP,0.5927392739273928,"E sup
t∈[0,T ]
∥θk(t)∥2 ≤C E sup
t∈[0,T ] Z t 0"
E SUP,0.5933993399339934,"gk
 
xε
φ, e

−gk
 
˜xε
φ, τ
2 dτ,
(i)"
E SUP,0.594059405940594,"≤C
Z T"
"E
SUP",0.5947194719471948,"0
E
sup
t∈[0,tau]
∥xε
t −˜xε
t∥2 dτ,
(ii)"
"E
SUP",0.5953795379537954,"≤C
Z T"
E SUP,0.596039603960396,"0
E sup
t∈[0,τ]
∥Rε(t)∥2 dτ, ,
(iii)"
E SUP,0.5966996699669967,"where to obtain (i) we used Jensen’s inequality when k = 0 and by the Burkholder-Davis-Gundy
658"
E SUP,0.5973597359735974,"inequality when k = 1, used the Lipschitz condition of gk to obtain (ii), and for (iii), it is because
659"
E SUP,0.598019801980198,"ε3Rε(t) = ˜xε
t −xε
t.
660"
E SUP,0.5986798679867987,"We note that from Taylor’s theorem, for any s ∈[0, t], k = 0, 1, there exists some εs ∈(0, ε) s.t.
661"
E SUP,0.5993399339933994,"gk (˜xε
s, s) −gk
 
x0
s, s

−ε∂gk"
E SUP,0.6,"∂x
 
x0
s, s

∂εx0
s = ε2 ∂gk"
E SUP,0.6006600660066007,"∂x (˜xεs
s ) ∂2
ε x0
s + ε2Ψ
 
∂ε x0
s, ˜xεs
s , s

.
(42)"
E SUP,0.6013201320132013,"For φk(t), we have
662"
E SUP,0.601980198019802,"E sup
t∈[0,T ]
∥φk(t)∥2"
E SUP,0.6026402640264027,"≤C E sup
t∈[0,T ] Z t"
E SUP,0.6033003300330033,"0
∥∂gk"
E SUP,0.6039603960396039,"∂x (˜xεs
s ) ∂2
ε x0
s + Ψk
 
∂ε x0
s, ˜xεs
s , s

−∂gk"
E SUP,0.6046204620462046,"∂x
 
x0
s

∂2
ε x0
s −Ψk
 
∂ε x0
s, x0
s, s

∥2ds, (iv)"
E SUP,0.6052805280528053,"≤C E sup
t∈[0,T ] Z t 0 ∂gk"
E SUP,0.6059405940594059,"∂x (˜xεs
s ) −∂gk"
E SUP,0.6066006600660065,"∂x
 
x0
s
"
E SUP,0.6072607260726073,"2 ∂2
ε x0
s
2 +
Ψk
 
∂εx0
s, ˜xs, s

−Ψk
 
∂εx0
s, x0
s, s
2 ds, (v)"
E SUP,0.6079207920792079,"≤C E sup
t∈[0,T ] Z t 0"
E SUP,0.6085808580858085,"˜xεs
s −x0
s
2 
C +
∂2
ε x0
s
2
ds,
(vi)"
E SUP,0.6092409240924093,"≤C E sup
t∈[0,T ] Z t 0"
E SUP,0.6099009900990099,"ε∂ε x0
s + ε2∂2
ε x0
s
2 
C +
∂2
ε x0
s
2
ds, ≤C "
E SUP,0.6105610561056105,"E sup
t∈[0,T ]"
E SUP,0.6112211221122112,"∂ε x0
s
2) + E sup
t∈[0,T ]"
E SUP,0.6118811881188119,"∂2
ε x0
s
2) !"
E SUP,0.6125412541254125,"C + E sup
t∈[0,T ]"
E SUP,0.6132013201320132,"∂2
ε x0
s
2
!"
E SUP,0.6138613861386139,",
(vii)"
E SUP,0.6145214521452145,"where for (iv), we used Equation (42) and Jensen’s inequality for k = 0 and the Burkholder-Davis-
663"
E SUP,0.6151815181518152,"Gundy inequality for k = 1; to obtain (v), we applied Jensen’s equality; we then derived (vi) from
664"
E SUP,0.6158415841584158,"the Lipschitz conditions of gk and Ψk; and finally another application of Jensen’s inequality gives
665"
E SUP,0.6165016501650165,"(vii) which is bounded as a result from the Lemma B.6.
666 667"
E SUP,0.6171617161716172,"For σk(t),
668"
E SUP,0.6178217821782178,"sup
t∈[0,T ]
∥σ0(t)∥2 ≤C ε
Z T"
E SUP,0.6184818481848184,"0
E sup
s∈[0,t]"
E SUP,0.6191419141914192,"ηk
 
x0
s, s
2 + CE sup
s∈[0,t] ∂ηk"
E SUP,0.6198019801980198,"∂x
 
x0
s, s
"
E SUP,0.6204620462046204,"2 ∂ε x0
s
2 dt,
(ix)"
E SUP,0.6211221122112212,"≤C
Z T"
C,0.6217821782178218,"0
C "
C,0.6224422442244224,"1 + E sup
s∈[0,t]"
C,0.6231023102310231,"x0
s
2
!"
C,0.6237623762376238,"+ CE sup
t∈[0,T ] ∂ηk"
C,0.6244224422442244,"∂x
 
x0
t, t
"
Z T,0.6250825082508251,2 Z T
E SUP,0.6257425742574257,"0
E sup
s∈[0,t]"
E SUP,0.6264026402640264,"∂εx0
s
2 dt, (x)"
E SUP,0.6270627062706271,"≤c + C E sup
t
∈[0, T]
x0
s
2 + C E sup
t∈[0,T ]"
E SUP,0.6277227722772277,"∂η
∂x
 
x0
t, t
"
E SUP,0.6283828382838283,"2
E sup
t∈[0,T ]"
E SUP,0.6290429042904291,"∂εx0
t
2 , (xi)"
E SUP,0.6297029702970297,"where we obtained (ix) by Jensen’s inequality when k = 0 and by Burkholder-Davis-Gundy inequality
669"
E SUP,0.6303630363036303,"when k = 1, and (x) by the linear growth assumption on ηk; one can see that (xi) is bounded by
670"
E SUP,0.6310231023102311,"recalling the Lemma B.6 and the assumption that ηk has bounded derivatives.
671"
E SUP,0.6316831683168317,"Hence, by Jensen’s inequality and Gronwall’s lemma, we have
672"
E SUP,0.6323432343234323,"E sup
t∈[0,T ]
∥Rε(t)∥2 ≤C K
X"
E SUP,0.633003300330033,"k=0
E sup
t∈[0,T ]
∥θk(t)∥2 + E sup
t∈[0,T ]
∥φk(t)∥2 + E sup
t∈[0,T ]
∥σk(t)∥2 ,"
E SUP,0.6336633663366337,"≤C + C
Z T"
E SUP,0.6343234323432343,"0
E sup
t∈[0,τ]
∥Rε(t)∥2 dτ,"
E SUP,0.634983498349835,≤C exp (C) .
E SUP,0.6356435643564357,"Therefore, E sup ∥Rε(t)∥2 is bounded.
673 674"
E SUP,0.6363036303630363,"Finally, it is now straightforward to show Equation (33) by applying a second-order Taylor expansion
675"
E SUP,0.636963696369637,"on f
 
x0
t + ε∂εx0
t + ε2∂2
εx0
t +ε3Rε(t)

.
676 677"
E SUP,0.6376237623762376,"We are now ready to show Theorem 3.7. One notes that Corollary 3.8 directly follows from the result
678"
E SUP,0.6382838283828383,"too.
679"
E SUP,0.638943894389439,"Proof. (Theorem 3.7) From Proposition B.5, it is noteworthy to point out that the derived SDEs (34)
680"
E SUP,0.6396039603960396,"for ∂ε x0
t and ∂2
ε x0
t are vector-valued general linear SDEs. With some steps of derivations, one can
681"
E SUP,0.6402640264026402,"express the solutions as:
682"
E SUP,0.640924092409241,"∂ε x0
t = Φt Z t"
E SUP,0.6415841584158416,"0
Φ−1
s "
E SUP,0.6422442244224422,"η0(x0
s, s) − m
X k=1 ∂gk"
E SUP,0.642904290429043,"∂x (x0
s, s)ηk(x0
s, s) !"
E SUP,0.6435643564356436,ds + Φt Z t
E SUP,0.6442244224422442,"0
Φ−1
s ηk(x0
s, s)dBk
s
(a)"
E SUP,0.6448844884488449,"∂2
ε x0
t = Φt Z t"
E SUP,0.6455445544554456,"0
Φ−1
s"
E SUP,0.6462046204620462,"
Ψ0(x0
s, ∂ε x0
s, s) + 2 ∂η0"
E SUP,0.6468646864686468,"∂x (x0
s, s)∂ε x0
s − m
X k=1 ∂gk"
E SUP,0.6475247524752475,"∂x (x0
s, s)

Ψk(x0
s, ∂ε x0
s, s) + 2 ∂ηk"
E SUP,0.6481848184818482,"∂x (x0
s, s)∂ε x0
s)

ds, + Φt Z t"
E SUP,0.6488448844884488,"0
Φ−1
s m
X k=1"
E SUP,0.6495049504950495,"
Ψk(x0
s, ∂ε x0
s, s) + 2 ∂ηk"
E SUP,0.6501650165016502,"∂x (x0
s, s)∂ε x0
s"
E SUP,0.6508250825082508,"
dBk
s ,
(b)"
E SUP,0.6514851485148515,"where n × n matrix Φt is the fundamental matrix of the corresponding homogeneous equation:
683"
E SUP,0.6521452145214521,dΦt = ∂gk
E SUP,0.6528052805280528,"∂x (x0
t, t) Φt dBk
t ,
(43)"
E SUP,0.6534653465346535,"with initial value
684"
E SUP,0.6541254125412541,"Φ(0) = I.
(44)"
E SUP,0.6547854785478547,It is worthy to note that the fundamental matrix Φt is non-deterministic and when ∂gi
E SUP,0.6554455445544555,∂x and ∂gj
E SUP,0.6561056105610561,"∂x
685"
E SUP,0.6567656765676567,"commutes, Φt has explicit solution
686"
E SUP,0.6574257425742575,"Φt = exp
Z t 0 ∂gk"
E SUP,0.6580858085808581,"∂x (x0
s, s)dBk
s −1 2 Z t 0 ∂gk"
E SUP,0.6587458745874587,"∂x (x0
s, s)∂gk"
E SUP,0.6594059405940594,"∂x (x0
s, s)⊤ds

.
(45)"
E SUP,0.6600660066006601,"Having obtained the explicit solutions, one can plug in corresponding terms and obtain the results of
687"
E SUP,0.6607260726072607,"Theorem 3.7) after a Taylor expansion of the loss function L.
688"
E SUP,0.6613861386138614,"C
Error Accumulation During the Inference Phase and its Effects to Value
689"
E SUP,0.662046204620462,"Functions
690"
E SUP,0.6627062706270627,"Theorem C.1. (Error accumulation due to initial representation error )
691"
E SUP,0.6633663366336634,"Let δ := E ∥ε∥and dε := E supt∈[0,T ]
hε
t −h0
t
2 +
˜zε
t −˜z0
t
2. It holds that as δ →0,
692"
E SUP,0.664026402640264,"dε ≤δ C (J0 + J1) + δ2 C (exp ( H0 (J0 + J1)) + exp ( H1 (J0 + J1))) + O(δ3),
(46)
where
693"
E SUP,0.6646864686468646,"J0 = exp (Fh + Fz + Ph) , J1 = exp
  ¯Ph

,"
E SUP,0.6653465346534654,"H0 =Fhh + Fhz + Fzh + Fzz + Phh, H1 = ¯Phh
694"
E SUP,0.666006600660066,"Fh =C E sup
t∈[0,T ]"
E SUP,0.6666666666666666,"∂f
∂h + ∂f"
E SUP,0.6673267326732674,∂a ∂hρ 2
E SUP,0.667986798679868,"F
,
Fz = C E sup
t∈[0,T ]"
E SUP,0.6686468646864686,"∂f
∂z + ∂f"
E SUP,0.6693069306930693,"∂a ∂zρ 2 F
,"
E SUP,0.66996699669967,"Ph =C E sup
t∈[0,T ] ∂p
∂h  2"
E SUP,0.6706270627062706,"F
, ¯Ph = C E sup
t∈[0,T ]"
E SUP,0.6712871287128713,"∂¯p
∂h  2 F
,"
E SUP,0.671947194719472,"Fhh =C E sup
t∈[0,T ]"
E SUP,0.6726072607260726,"∂2f
∂h2 + ∂2f"
E SUP,0.6732673267326733,∂h∂a∂hρ + ∂f
E SUP,0.6739273927392739,"∂a ∂2
hhρ 2 F
,"
E SUP,0.6745874587458746,"Fhz =C E sup
t∈[0,T ]"
E SUP,0.6752475247524753,"∂2f
∂h∂z + ∂2f"
E SUP,0.6759075907590759,∂z∂a∂hρ + ∂f
E SUP,0.6765676567656765,"∂a ∂2
zhρ 2 F"
E SUP,0.6772277227722773,"Fzh =C E sup
t∈[0,T ]"
E SUP,0.6778877887788779,"∂2f
∂h∂z + ∂2f"
E SUP,0.6785478547854785,∂h∂a∂zρ + ∂f
E SUP,0.6792079207920793,"∂a ∂2
hzρ 2 F"
E SUP,0.6798679867986799,"Fzz =C E sup
t∈[0,T ]"
E SUP,0.6805280528052805,"∂2f
∂z2 + ∂2f"
E SUP,0.6811881188118812,∂z∂a∂zρ + ∂f
E SUP,0.6818481848184819,"∂a ∂2
zzρ 2 F
,"
E SUP,0.6825082508250825,"Phh =C E sup
t∈[0,T ]"
E SUP,0.6831683168316832,"∂2p
∂h2  2"
E SUP,0.6838283828382838,"F
, ¯Phh = C E sup
t∈[0,T ]"
E SUP,0.6844884488448845,"∂2¯p
∂h2  2 F
,"
E SUP,0.6851485148514852,"where for brevity, when functions always have inputs (˜z0
t , h0
t, t), we adopt the shorthand to write, for
695"
E SUP,0.6858085808580858,"example, f(˜z0
t , h0
t, t) as f.
696"
E SUP,0.6864686468646864,"Before proving the main result C.1, we first show the general case of perturbation in initial values.
697"
E SUP,0.6871287128712872,"Consider the following general system with noise at the initial value:
698"
E SUP,0.6877887788778878,"dxt = g0 (xt, t) dt + gk (xt, t) dBk
t ,
(47)
x(0) = x0 + ε,
(48)"
E SUP,0.6884488448844884,"where the initial perturbation ε ∈Rn × Ω. As gk are C2,α
g
functions, by the classical result on the
699"
E SUP,0.689108910891089,"existence and the uniqueness of solution to SDE, there exists a unique solution to Equation (47),
700"
E SUP,0.6897689768976898,"denoted as xε
t or xε(t).
701"
E SUP,0.6904290429042904,"To simplify the notation, we write ∂i xε
t := ∂xε(t)"
E SUP,0.691089108910891,"∂xi , ∂2
ij xε
t =
∂2xε
t
∂xi∂xj , for i, j = 1, . . . , n that are,
702"
E SUP,0.6917491749174918,"respectively, the first and second-order derivatives of the solution xε(t) w.r.t. the changes in the
703"
E SUP,0.6924092409240924,"corresponding coordinates of the initial value. When ε = 0 ∈Rn, we denote the solutions to
704"
E SUP,0.693069306930693,"Equation (47) as x0
t with its first and second derivatives ∂i x0
t, ∂2
ij x0
t, respectively.
705"
E SUP,0.6937293729372938,"Proposition C.2. Let δ := E ∥ε∥, it holds that
706"
E SUP,0.6943894389438944,"E sup
t∈[0,T ]"
E SUP,0.695049504950495,"xε
t −x0
t
2 ≤
X"
E SUP,0.6957095709570957,"k=0,1
C δ "
E SUP,0.6963696369636964,"C E sup
t∈[0,T ] ∂gk"
E SUP,0.697029702970297,"∂x (x0
t, t) 2 F ! (49)"
E SUP,0.6976897689768977,+ C δ2 exp 
E SUP,0.6983498349834983,"C E sup
t∈[0,T ] ∂2gk"
E SUP,0.699009900990099,"∂x2 (x0
t, t) 2 F X"
E SUP,0.6996699669966997,"¯k=0,1
exp "
E SUP,0.7003300330033003,"C E sup
t∈[0,T ] ∂g¯k"
E SUP,0.700990099009901,"∂x (x0
t, t) 2 F !"
E SUP,0.7016501650165017,"+ O(δ3),"
E SUP,0.7023102310231023,"(50)
as δ →0.
707"
E SUP,0.7029702970297029,"Proof. Similar to the previous section, for notational convenience, we write t as B0
t and employs
708"
E SUP,0.7036303630363037,"Einstein summation notation. Hence, Equation (47) can be shorten as
709"
E SUP,0.7042904290429043,"dxt = gk (xt, t) dBk
t ,
(51)"
E SUP,0.7049504950495049,"with initial values x(0) = x0 + ε.
710"
E SUP,0.7056105610561056,"To begin, we find the SDEs that characterize ∂i xε
t and ∂2
ij xε
t, for i, j = 1, ..., n.
711"
E SUP,0.7062706270627063,"For ∂i xε
t, we apply Theorem 3.1 from Section 2 in [17] on Equation (51) and ∂i xε
t satisfy the
712"
E SUP,0.7069306930693069,"following SDE with probability 1,
713"
E SUP,0.7075907590759076,"d∂i xε
t = ∂gk"
E SUP,0.7082508250825083,"∂x (xε
t, t) ∂i xε
tdBk
t
(52)"
E SUP,0.7089108910891089,"with initial value ∂ixε
0 to be the unit vector ei = (0, 0, . . . , 1, . . . , 0) that is all zeros except one in
714"
E SUP,0.7095709570957096,"the ith coordinate.
715"
E SUP,0.7102310231023102,"For ∂2
ij xε
t, we again apply Theorem 3.1 from Section 2 in [17] on the SDE (52) and obtain that ∂2
ijxε
b
716"
E SUP,0.7108910891089109,"satisfy the following SDE with probability 1,
717"
E SUP,0.7115511551155116,"d∂2
ij xε
t = Ψk (xε
t, ∂i xε
t, t) ∂2
ij xε
tdBk
t ,
(53)"
E SUP,0.7122112211221122,"with the initial value ∂ij xε(0) = ej, where
718"
E SUP,0.7128712871287128,"Ψk : Rd × Rd × [0, T] →Rd×d, (x, ∂i x, t) 7→
 ∂2gl
k
∂xu∂xv (xε
t, t)
"
E SUP,0.7135313531353136,"l,u,v
∂i xv."
E SUP,0.7141914191419142,"For the next step, we show that with probability 1, the following holds
719"
E SUP,0.7148514851485148,"xε
t = x0
t + εi ∂i x0
t + 1"
E SUP,0.7155115511551156,"2 εiεj ∂2
ij x0
t + O
 
ε3
,
(54)"
E SUP,0.7161716171617162,"as ∥ε∥→0.
720"
E SUP,0.7168316831683168,"One can follow the similar steps of proofs for Lemma (B.6) and (B.7) in the previous section to show
721"
E SUP,0.7174917491749175,"that E supt∈[0,T ]
x0
t
2 , E supt∈[0,T ]
∂ix0
t
2 , E supt∈[0,T ]
∂2
ijx0
t
2 and the remainder term are
722"
E SUP,0.7181518151815182,"bounded. Hence, Equation (54) holds with probability 1.
723 724"
E SUP,0.7188118811881188,"Indeed, for E supt∈[0,T ]
∂i x0
t
2, it holds that
725"
E SUP,0.7194719471947195,"E sup
t∈[0,T ]"
E SUP,0.7201320132013201,"∂i x0
t
2 ≤C ∥ei∥2 +
X"
E SUP,0.7207920792079208,"k=0,1
E sup
t∈[0,T ]
C
Z t 0 ∂gk"
E SUP,0.7214521452145215,"∂x (x0
s, s) 2"
E SUP,0.7221122112211221,"F
∥∂i xs∥2 ds
(55) ≤
X"
E SUP,0.7227722772277227,"k=0,1
C exp "
E SUP,0.7234323432343235,"C E sup
t∈[0,T ] ∂gk"
E SUP,0.7240924092409241,"∂x (x0
t, t) 2 F !"
E SUP,0.7247524752475247,".
(56)"
E SUP,0.7254125412541255,"Similarly, for E supt∈[0,T ]
∂2
ij x0
t
2, it holds that
726"
E SUP,0.7260726072607261,"E sup
t∈[0,T ]"
E SUP,0.7267326732673267,"∂2
ij x0
t
2 ≤C ∥ei∥2 +
X"
E SUP,0.7273927392739274,"k=0,1
E sup
t∈[0,T ]
C
Z t 0 ∂2gk"
E SUP,0.7280528052805281,"∂x2 (x0
s, s) 2 F"
E SUP,0.7287128712871287,"∂i x0
s
2 ∂2
ij x0
s
2 ds (57) ≤C"
X,0.7293729372937293,"1
X"
X,0.73003300330033,"k=0
exp "
X,0.7306930693069307,"C E sup
t∈[0,T ] ∂2gk"
X,0.7313531353135313,"∂x2 (x0
t, t) 2 F"
X,0.732013201320132,"∂i x0
t
2
! (58) ≤C
X"
X,0.7326732673267327,"k=0,1
exp "
X,0.7333333333333333,"C E sup
t∈[0,T ] ∂2gk"
X,0.733993399339934,"∂x2 (x0
t, t) 2 F
exp "
X,0.7346534653465346,"C E sup
t∈[0,T ] ∂gk"
X,0.7353135313531353,"∂x (x0
t, t) 2 F !! . (59)"
X,0.735973597359736,"Therefore, we could obtain the proposition by applying Jensen’s inequality to Equation (54) and
727"
X,0.7366336633663366,"plugging with 56 and 57.
728"
X,0.7372937293729372,"Now we are ready to prove Theorem C.1. We note that one could then obtain Corollary 4.2 without
729"
X,0.737953795379538,"much more effort by a standard application of Taylor’s theorem.
730"
X,0.7386138613861386,"Proof. (Proof for Theorem C.1)
731"
X,0.7392739273927392,"At (ht, ˜zt, π(ht, ˜zt)), where the local optimal policy π(ht, ˜zt), denoted as a∗
t , there exists an open
732"
X,0.73993399339934,"neighborhood V ⊆A of a∗
t such that a∗
t is the local maximizer for Q(ht, ˜zt, ·) by definition.
733"
X,0.7405940594059406,"Then, ∂Q"
X,0.7412541254125412,"∂a (ht, ˜zt, a∗
t ) = 0, and ∂2Q"
X,0.7419141914191419,"∂a2 (ht, ˜zt, a) is negative definite. As ∂2Q"
X,0.7425742574257426,"∂a2 is non-degenerate in the
734"
X,0.7432343234323432,"neighborhood V , by the implicit function theorem, there exists a neighborhood U × V of (ht, ˜zt, a∗
t )
735"
X,0.7438943894389439,such that there exists a C2 map ρ : U →V such that ∂Q
X,0.7445544554455445,"∂a (h, ˜z, ρ(h, ˜z)) = 0 and ρ(h, ˜z) is the
736"
X,0.7452145214521452,"local maximizer of Q(h, ˜z, ·) for any h, ˜z ∈U. Furthermore, we have that ∂h ρ = −∂2Q"
X,0.7458745874587459,"∂a2
−1 ∂2Q"
X,0.7465346534653465,"∂a∂h.
737"
X,0.7471947194719472,"Similarly, other first-terms and second-order terms ∂zρ, ∂2
zzρ, ∂2
zhρ, ∂2
hzρ, ∂2
hhρ can be explicitly
738"
X,0.7478547854785479,"expressed without much additional effort (e.g., in [28], [3]).
739"
X,0.7485148514851485,"The rest of the proof is easy to see after plugging in the corresponding terms from Proposition
740"
X,0.7491749174917491,"C.2.
741"
X,0.7498349834983499,"D
Experimental Details
742"
X,0.7504950495049505,"In this section, we provide additional details and results beyond thoese in the main paper.
743"
X,0.7511551155115511,"D.1
Model Implementation and Training
744"
X,0.7518151815181519,"Our baseline is based on the DreamerV2 Tensorflow implementation. Our theoretical and empirical
745"
X,0.7524752475247525,"results should not matter on the choice of specific version; so we chose DreamerV2 as its codebase
746"
X,0.7531353135313531,"implementation is simpler than V3. We incorporated a computationally efficient approximation of
747"
X,0.7537953795379538,"the Jacobian norm for the sequence model, as detailed in [18], using a single projection. During our
748"
X,0.7544554455445545,"experiments, all models were trained using the default hyperparameters (see Table 5) for the MuJoCo
749"
X,0.7551155115511551,"tasks. The training was conducted on an NVIDIA A100 and a GTX 4090, with each session lasting
750"
X,0.7557755775577558,"less than 15 hours.
751"
X,0.7564356435643564,"Hyperparameter
Value
eval_every
1e4
prefill
1000
train_every
5
rssm.hidden
200
rssm.deter
200
model_opt.lr
3e-4
actor_opt.lr
8e-5
replay_capacity
2e6
dataset_batch
16
precision
16
clip_rewards
tanh
expl_behavior
greedy
encoder_cnn_depth
48
decoder_cnn_depth
48
loss_scales_kl
1.0
discount
0.99
jac_lambda
0.01
Table 5: Hyperparameters for DreamerV2 model."
X,0.7570957095709571,"D.2
Additional Results on Generalization on Perturbed States
752"
X,0.7577557755775578,"In this experiment, we investigated the effectiveness of Jacobian regularization in model trained
753"
X,0.7584158415841584,"against a baseline during the inference phase with perturbed state images. We consider three types of
754"
X,0.759075907590759,"perturbations: (1) Gaussian noise across the full image, denoted as N(µ1, σ2
1) ; (2) rotation; and (3)
755"
X,0.7597359735973598,"noise applied to a percentage of the image, N(µ2, σ2
2). (In Walker task, µ1 = µ2 = 0.5, σ2
2 = 0.15;
756"
X,0.7603960396039604,"in Quadruped task, µ1 = 0, µ2 = 0.05, σ2
2 = 0.2.) In each case of perturbations, we examine a
757"
X,0.761056105610561,"collection of noise levels: (1) variance σ2 from 0.05 to 0.55; (2) rotation degree α 20 and 30; and (3)
758"
X,0.7617161716171618,"masked image percentage β% from 25 to 75.
759"
X,0.7623762376237624,"D.3
Walker Task
760"
X,0.763036303630363,"β% mask, N(0.5, 0.15)
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
25%
882.78
28.57199976
929.778
10.13141451
30%
878.732
40.92085898
811.198
7.663919934
35%
856.32
37.56882045
799.98
29.75286097
40%
804.206
47.53578989
688.382
43.21310246
45%
822.97
80.36907477
601.862
42.49662057
50%
725.812
43.87836335
583.418
76.49237076
55%
768.68
50.71423045
562.574
59.88315135
60%
730.864
23.37324967
484.038
90.38940234
65%
696.936
65.26307708
516.936
41.44549462
70%
687.346
70.9078686
411.922
45.85808832
75%
685.492
63.22171723
446.74
40.66898799
Table 6: Walker. Mean and standard deviation of accumulated rewards under masked perturbation of
increasing percentage."
X,0.7636963696369637,"full, N(0.5, σ2)
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
0.05
894.594
39.86907737
929.778
40.91
0.10
922.854
27.28533819
811.198
98.79
0.15
941.512
16.47165049
799.98
106.01
0.20
840.706
66.12470628
688.382
70.78
0.25
811.764
75.06276427
601.862
83.65
0.30
779.504
53.29238107
583.418
173.59
0.35
807.996
34.35949621
562.574
79.30
0.40
751.986
85.20137722
484.038
112.43
0.45
663.578
60.18862658
516.936
90.25
0.50
618.982
61.10094983
411.922
116.94
0.55
578.62
64.25840684
446.74
84.44
Table 7: Walker. Mean and standard deviation of accumulated rewards under Gaussian perturbation
of increasing variance."
X,0.7643564356435644,"rotation, α◦
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
20
423.81
12.90174678
391.65
35.33559636
30
226.04
23.00445979
197.53
15.26706914
Table 8: Walker. Mean and standard deviation of accumulated rewards under rotations."
X,0.765016501650165,"D.4
Quardruped Task
761"
X,0.7656765676567657,"β% mask, N(0.5, 0.15)
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
25%
393.242
41.10002579
361.764
81.41175179
30%
384.11
20.70463958
333.364
101.7413185
35%
354.222
53.14855379
306.972
16.02275164
40%
329.404
39.1193856
266.088
51.20298351
45%
360.662
36.86801622
281.342
47.85950867
50%
321.556
27.66758085
222.222
22.0668251
55%
300.258
31.44931987
203.578
14.38754218
60%
321
18.42956321
217.98
23.81819368
65%
304.62
20.75493676
209.238
47.14895407
70%
301.166
18.2485583
193.514
60.83781004
75%
304.92
18.63214963
169.58
30.83637462
Table 9: Quadruped. Mean and standard deviation of accumulated rewards under masked perturbation
of increasing percentage."
X,0.7663366336633664,"full, N(0, σ2)
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
0.10
416.258
20.87925573
326.74
40.30425536
0.15
308.218
24.26432093
214.718
15.7782198
0.20
314.29
44.73612075
218.756
35.41520832
0.25
293.02
24.29582269
190.78
26.22250465
0.30
269.778
21.83423047
207.336
39.1071161
0.35
282.046
13.55303767
217.048
29.89589972
0.40
273.814
19.81361476
190.208
59.61166975
0.45
267.18
17.5276068
195.606
18.91137964
0.50
268.838
29.45000543
194.082
26.76677642
0.55
252.54
22.516283
150.786
24.53362855
Table 10: Quadruped. Mean and standard deviation of accumulated rewards under Gaussian pertur-
bation of increasing variance."
X,0.766996699669967,"rotation, α◦
mean (with Jac.)
stdev (with Jac.)
mean (baseline)
stdev (baseline)
20
787.634
101.5974723
681.032
133.7507948
30
610.526
97.74499159
389.406
61.5997198
Table 11: Quadruped. Mean and standard deviation of accumulated rewards under rotations."
X,0.7676567656765677,"D.5
Additional Results on Robustness against Encoder Errors
762"
X,0.7683168316831683,"In this experiment, we evaluate the robustness of model trained with Jacobian regularization against
763"
X,0.768976897689769,"two exogenous error signals (1) zero-drift error with µt = 0, σ2
t (σ2
t = 5 in Walker, σ2
t = 0.1 in
764"
X,0.7696369636963697,"Quadruped), and (2) non-zero-drift error with µt ∼[0, 5], σ2
t ∼[0, 5] uniformly. λ weight of Jacobian
765"
X,0.7702970297029703,"regularization is 0.01. In this section, we included plot results of both evaluation and training scores.
766"
X,0.7709570957095709,"D.5.1
Walker Task
767"
X,0.7716171617161716,"Under the Walker task, Figures 3 and 4 show that model with regularization is significantly less
768"
X,0.7722772277227723,"sensitive to perturbations in latent state zt compared to the baseline model without regularization.
769"
X,0.7729372937293729,"This empirical observation supports our theoretical findings in Corollary 3.8, which assert that the
770"
X,0.7735973597359735,"impact of latent representation errors on the loss function L can be effectively controlled by regulating
771"
X,0.7742574257425743,the model’s Jacobian norm.
X,0.7749174917491749,"Figure 3: Walker. Eval (left) and train scores (right) under latent error process µt = 0, σ2
t = 5
. 772"
X,0.7755775577557755,"Figure 4: Walker. Eval (left) and train scores (right) under latent error process µt ∼[0, 5], σ2
t ∼[0, 5]."
X,0.7762376237623763,"D.5.2
Quadruped Task
773"
X,0.7768976897689769,"Under the Quadruped task,we initially examined a smaller latent error process (µt = 0, σ2
t = 0.1) and
774"
X,0.7775577557755775,"observed that the model with Jacobian regularization converged significantly faster, even though the
775"
X,0.7782178217821782,"adversarial effects on the model without regularization were less severe (Figure 5). When considering
776"
X,0.7788778877887789,"the more challenging latent error process (µt ∼[0, 5], σ2
t ∼[0, 5]), we noted that the regularized
777"
X,0.7795379537953795,"model remained significantly less sensitive to perturbations in latent state zt, whereas the baseline
778"
X,0.7801980198019802,"model struggled to learn (Figure 6). These empirical observations reinforce our theoretical findings
779"
X,0.7808580858085808,"in Corollary 3.8, demonstrating that regulating the model’s Jacobian norm effectively controls the
780"
X,0.7815181518151815,impact of latent representation errors.
X,0.7821782178217822,"Figure 5: Quad. Eval (left) and train scores (right) under latent error process µt = 0, σ2
t = 0.1."
X,0.7828382838283828,"Figure 6: Quad. Eval (left) and train scores (right) under latent error process µt ∼[0, 5], σ2
t ∼[0, 5]. 781"
X,0.7834983498349835,"D.6
Additional Results on Faster convergence on tasks with extended horizon.
782"
X,0.7841584158415842,"In this experiment, we evaluate the efficacy of Jacobian regularization in extended horizon tasks,
783"
X,0.7848184818481848,"specifically by increasing the horizon length in MuJoCo Walker from 50 to 100 steps. We tested two
784"
X,0.7854785478547854,"regularization weights λ = 0.1 and λ = 0.05. Figure 7 demonstrates that models with regularization
785"
X,0.7861386138613862,"converge faster, with λ = 0.05 achieving convergence approximately 100,000 steps ahead of the
786"
X,0.7867986798679868,"model without Jacobian regularization. This supports the findings in Theorem 4.1, indicating that
787"
X,0.7874587458745874,"regularizing the Jacobian norm can reduce error propagation, especially over longer time horizons."
X,0.7881188118811882,Figure 7: Extended horizon Walker task. Eval (left) and train scores (right). 788
X,0.7887788778877888,"NeurIPS Paper Checklist
789"
CLAIMS,0.7894389438943894,"1. Claims
790"
CLAIMS,0.7900990099009901,"Question: Do the main claims made in the abstract and introduction accurately reflect the
791"
CLAIMS,0.7907590759075908,"paper’s contributions and scope?
792"
CLAIMS,0.7914191419141914,"Answer: [Yes]
793"
CLAIMS,0.7920792079207921,"Justification: In the abstract and the contribution section 1 in introduction, we provide a
794"
CLAIMS,0.7927392739273927,"clear list of statements outlining the paper’s contributions.
795"
CLAIMS,0.7933993399339934,"Guidelines:
796"
CLAIMS,0.7940594059405941,"• The answer NA means that the abstract and introduction do not include the claims
797"
CLAIMS,0.7947194719471947,"made in the paper.
798"
CLAIMS,0.7953795379537953,"• The abstract and/or introduction should clearly state the claims made, including the
799"
CLAIMS,0.7960396039603961,"contributions made in the paper and important assumptions and limitations. A No or
800"
CLAIMS,0.7966996699669967,"NA answer to this question will not be perceived well by the reviewers.
801"
CLAIMS,0.7973597359735973,"• The claims made should match theoretical and experimental results, and reflect how
802"
CLAIMS,0.7980198019801981,"much the results can be expected to generalize to other settings.
803"
CLAIMS,0.7986798679867987,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
804"
CLAIMS,0.7993399339933993,"are not attained by the paper.
805"
LIMITATIONS,0.8,"2. Limitations
806"
LIMITATIONS,0.8006600660066007,"Question: Does the paper discuss the limitations of the work performed by the authors?
807"
LIMITATIONS,0.8013201320132013,"Answer: [Yes]
808"
LIMITATIONS,0.801980198019802,"Justification: For each of our theoretical results, we state the required assumptions and
809"
LIMITATIONS,0.8026402640264027,"provide relevant discussions that compare our assumptions to the practical implementations
810"
LIMITATIONS,0.8033003300330033,"which involves certain limitations for theoretical simplifications. For empirical results, we
811"
LIMITATIONS,0.803960396039604,"also state the experiment settings and the number of trials run. We also discuss the possible
812"
LIMITATIONS,0.8046204620462046,"future research to extend our work in the conclusion section.
813"
LIMITATIONS,0.8052805280528053,"Guidelines:
814"
LIMITATIONS,0.805940594059406,"• The answer NA means that the paper has no limitation while the answer No means that
815"
LIMITATIONS,0.8066006600660066,"the paper has limitations, but those are not discussed in the paper.
816"
LIMITATIONS,0.8072607260726072,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
817"
LIMITATIONS,0.807920792079208,"• The paper should point out any strong assumptions and how robust the results are to
818"
LIMITATIONS,0.8085808580858086,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
819"
LIMITATIONS,0.8092409240924092,"model well-specification, asymptotic approximations only holding locally). The authors
820"
LIMITATIONS,0.80990099009901,"should reflect on how these assumptions might be violated in practice and what the
821"
LIMITATIONS,0.8105610561056106,"implications would be.
822"
LIMITATIONS,0.8112211221122112,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
823"
LIMITATIONS,0.8118811881188119,"only tested on a few datasets or with a few runs. In general, empirical results often
824"
LIMITATIONS,0.8125412541254126,"depend on implicit assumptions, which should be articulated.
825"
LIMITATIONS,0.8132013201320132,"• The authors should reflect on the factors that influence the performance of the approach.
826"
LIMITATIONS,0.8138613861386138,"For example, a facial recognition algorithm may perform poorly when image resolution
827"
LIMITATIONS,0.8145214521452145,"is low or images are taken in low lighting. Or a speech-to-text system might not be
828"
LIMITATIONS,0.8151815181518152,"used reliably to provide closed captions for online lectures because it fails to handle
829"
LIMITATIONS,0.8158415841584158,"technical jargon.
830"
LIMITATIONS,0.8165016501650165,"• The authors should discuss the computational efficiency of the proposed algorithms
831"
LIMITATIONS,0.8171617161716171,"and how they scale with dataset size.
832"
LIMITATIONS,0.8178217821782178,"• If applicable, the authors should discuss possible limitations of their approach to
833"
LIMITATIONS,0.8184818481848185,"address problems of privacy and fairness.
834"
LIMITATIONS,0.8191419141914191,"• While the authors might fear that complete honesty about limitations might be used by
835"
LIMITATIONS,0.8198019801980198,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
836"
LIMITATIONS,0.8204620462046205,"limitations that aren’t acknowledged in the paper. The authors should use their best
837"
LIMITATIONS,0.8211221122112211,"judgment and recognize that individual actions in favor of transparency play an impor-
838"
LIMITATIONS,0.8217821782178217,"tant role in developing norms that preserve the integrity of the community. Reviewers
839"
LIMITATIONS,0.8224422442244225,"will be specifically instructed to not penalize honesty concerning limitations.
840"
THEORY ASSUMPTIONS AND PROOFS,0.8231023102310231,"3. Theory Assumptions and Proofs
841"
THEORY ASSUMPTIONS AND PROOFS,0.8237623762376237,"Question: For each theoretical result, does the paper provide the full set of assumptions and
842"
THEORY ASSUMPTIONS AND PROOFS,0.8244224422442245,"a complete (and correct) proof?
843"
THEORY ASSUMPTIONS AND PROOFS,0.8250825082508251,"Answer: [Yes]
844"
THEORY ASSUMPTIONS AND PROOFS,0.8257425742574257,"Justification: For each of our theoretical results, we state the assumptions required in both
845"
THEORY ASSUMPTIONS AND PROOFS,0.8264026402640264,"the main text and the provided appendix. We provide the full proofs of all of our theoretical
846"
THEORY ASSUMPTIONS AND PROOFS,0.8270627062706271,"results in Sections A, B and C in Appendix,
847"
THEORY ASSUMPTIONS AND PROOFS,0.8277227722772277,"Guidelines:
848"
THEORY ASSUMPTIONS AND PROOFS,0.8283828382838284,"• The answer NA means that the paper does not include theoretical results.
849"
THEORY ASSUMPTIONS AND PROOFS,0.829042904290429,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
850"
THEORY ASSUMPTIONS AND PROOFS,0.8297029702970297,"referenced.
851"
THEORY ASSUMPTIONS AND PROOFS,0.8303630363036304,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
852"
THEORY ASSUMPTIONS AND PROOFS,0.831023102310231,"• The proofs can either appear in the main paper or the supplemental material, but if
853"
THEORY ASSUMPTIONS AND PROOFS,0.8316831683168316,"they appear in the supplemental material, the authors are encouraged to provide a short
854"
THEORY ASSUMPTIONS AND PROOFS,0.8323432343234324,"proof sketch to provide intuition.
855"
THEORY ASSUMPTIONS AND PROOFS,0.833003300330033,"• Inversely, any informal proof provided in the core of the paper should be complemented
856"
THEORY ASSUMPTIONS AND PROOFS,0.8336633663366336,"by formal proofs provided in appendix or supplemental material.
857"
THEORY ASSUMPTIONS AND PROOFS,0.8343234323432344,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
858"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.834983498349835,"4. Experimental Result Reproducibility
859"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8356435643564356,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
860"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8363036303630363,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
861"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.836963696369637,"of the paper (regardless of whether the code and data are provided or not)?
862"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8376237623762376,"Answer: [Yes]
863"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8382838283828383,"Justification: The full source code required to reproduce the experimental results is included
864"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.838943894389439,"in the submission.
865"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8396039603960396,"Guidelines:
866"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8402640264026403,"• The answer NA means that the paper does not include experiments.
867"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8409240924092409,"• If the paper includes experiments, a No answer to this question will not be perceived
868"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8415841584158416,"well by the reviewers: Making the paper reproducible is important, regardless of
869"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8422442244224423,"whether the code and data are provided or not.
870"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8429042904290429,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
871"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8435643564356435,"to make their results reproducible or verifiable.
872"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8442244224422443,"• Depending on the contribution, reproducibility can be accomplished in various ways.
873"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8448844884488449,"For example, if the contribution is a novel architecture, describing the architecture fully
874"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8455445544554455,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
875"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8462046204620463,"be necessary to either make it possible for others to replicate the model with the same
876"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8468646864686469,"dataset, or provide access to the model. In general. releasing code and data is often
877"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8475247524752475,"one good way to accomplish this, but reproducibility can also be provided via detailed
878"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8481848184818482,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
879"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8488448844884489,"of a large language model), releasing of a model checkpoint, or other means that are
880"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8495049504950495,"appropriate to the research performed.
881"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8501650165016502,"• While NeurIPS does not require releasing code, the conference does require all submis-
882"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8508250825082508,"sions to provide some reasonable avenue for reproducibility, which may depend on the
883"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8514851485148515,"nature of the contribution. For example
884"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8521452145214522,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
885"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8528052805280528,"to reproduce that algorithm.
886"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8534653465346534,"(b) If the contribution is primarily a new model architecture, the paper should describe
887"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8541254125412542,"the architecture clearly and fully.
888"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8547854785478548,"(c) If the contribution is a new model (e.g., a large language model), then there should
889"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8554455445544554,"either be a way to access this model for reproducing the results or a way to reproduce
890"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.856105610561056,"the model (e.g., with an open-source dataset or instructions for how to construct
891"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8567656765676568,"the dataset).
892"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8574257425742574,"(d) We recognize that reproducibility may be tricky in some cases, in which case
893"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.858085808580858,"authors are welcome to describe the particular way they provide for reproducibility.
894"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8587458745874588,"In the case of closed-source models, it may be that access to the model is limited in
895"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8594059405940594,"some way (e.g., to registered users), but it should be possible for other researchers
896"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.86006600660066,"to have some path to reproducing or verifying the results.
897"
OPEN ACCESS TO DATA AND CODE,0.8607260726072608,"5. Open access to data and code
898"
OPEN ACCESS TO DATA AND CODE,0.8613861386138614,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
899"
OPEN ACCESS TO DATA AND CODE,0.862046204620462,"tions to faithfully reproduce the main experimental results, as described in supplemental
900"
OPEN ACCESS TO DATA AND CODE,0.8627062706270627,"material?
901"
OPEN ACCESS TO DATA AND CODE,0.8633663366336634,"Answer: [Yes]
902"
OPEN ACCESS TO DATA AND CODE,0.864026402640264,"Justification: Our task environments Walker and Quardruped are from open source package
903"
OPEN ACCESS TO DATA AND CODE,0.8646864686468647,"MuJoCo. Our baseline implementation is from open source codebase DreamerV2. Our
904"
OPEN ACCESS TO DATA AND CODE,0.8653465346534653,"implementation of Jacobian regularization has a full description in Section D.1.
905"
OPEN ACCESS TO DATA AND CODE,0.866006600660066,"Guidelines:
906"
OPEN ACCESS TO DATA AND CODE,0.8666666666666667,"• The answer NA means that paper does not include experiments requiring code.
907"
OPEN ACCESS TO DATA AND CODE,0.8673267326732673,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
908"
OPEN ACCESS TO DATA AND CODE,0.8679867986798679,"public/guides/CodeSubmissionPolicy) for more details.
909"
OPEN ACCESS TO DATA AND CODE,0.8686468646864687,"• While we encourage the release of code and data, we understand that this might not be
910"
OPEN ACCESS TO DATA AND CODE,0.8693069306930693,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
911"
OPEN ACCESS TO DATA AND CODE,0.8699669966996699,"including code, unless this is central to the contribution (e.g., for a new open-source
912"
OPEN ACCESS TO DATA AND CODE,0.8706270627062707,"benchmark).
913"
OPEN ACCESS TO DATA AND CODE,0.8712871287128713,"• The instructions should contain the exact command and environment needed to run to
914"
OPEN ACCESS TO DATA AND CODE,0.8719471947194719,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
915"
OPEN ACCESS TO DATA AND CODE,0.8726072607260726,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
916"
OPEN ACCESS TO DATA AND CODE,0.8732673267326733,"• The authors should provide instructions on data access and preparation, including how
917"
OPEN ACCESS TO DATA AND CODE,0.8739273927392739,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
918"
OPEN ACCESS TO DATA AND CODE,0.8745874587458746,"• The authors should provide scripts to reproduce all experimental results for the new
919"
OPEN ACCESS TO DATA AND CODE,0.8752475247524752,"proposed method and baselines. If only a subset of experiments are reproducible, they
920"
OPEN ACCESS TO DATA AND CODE,0.8759075907590759,"should state which ones are omitted from the script and why.
921"
OPEN ACCESS TO DATA AND CODE,0.8765676567656766,"• At submission time, to preserve anonymity, the authors should release anonymized
922"
OPEN ACCESS TO DATA AND CODE,0.8772277227722772,"versions (if applicable).
923"
OPEN ACCESS TO DATA AND CODE,0.8778877887788779,"• Providing as much information as possible in supplemental material (appended to the
924"
OPEN ACCESS TO DATA AND CODE,0.8785478547854786,"paper) is recommended, but including URLs to data and code is permitted.
925"
OPEN ACCESS TO DATA AND CODE,0.8792079207920792,"6. Experimental Setting/Details
926"
OPEN ACCESS TO DATA AND CODE,0.8798679867986798,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
927"
OPEN ACCESS TO DATA AND CODE,0.8805280528052806,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
928"
OPEN ACCESS TO DATA AND CODE,0.8811881188118812,"results?
929"
OPEN ACCESS TO DATA AND CODE,0.8818481848184818,"Answer: [Yes]
930"
OPEN ACCESS TO DATA AND CODE,0.8825082508250826,"Justification: We state the hyperparameters used in Table 5. The perturbations we considered
931"
OPEN ACCESS TO DATA AND CODE,0.8831683168316832,"is fully described in the experiment section form the main text.
932"
OPEN ACCESS TO DATA AND CODE,0.8838283828382838,"Guidelines:
933"
OPEN ACCESS TO DATA AND CODE,0.8844884488448845,"• The answer NA means that the paper does not include experiments.
934"
OPEN ACCESS TO DATA AND CODE,0.8851485148514852,"• The experimental setting should be presented in the core of the paper to a level of detail
935"
OPEN ACCESS TO DATA AND CODE,0.8858085808580858,"that is necessary to appreciate the results and make sense of them.
936"
OPEN ACCESS TO DATA AND CODE,0.8864686468646865,"• The full details can be provided either with the code, in appendix, or as supplemental
937"
OPEN ACCESS TO DATA AND CODE,0.8871287128712871,"material.
938"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8877887788778878,"7. Experiment Statistical Significance
939"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8884488448844885,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
940"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8891089108910891,"information about the statistical significance of the experiments?
941"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8897689768976897,"Answer: [Yes]
942"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8904290429042905,"Justification: While our work is predominantly theoretical, we conducted 5 random trials for
943"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8910891089108911,"each perturbation degree and type. For additional results including standard deviation of
944"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8917491749174917,"trials, see Section D.
945"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8924092409240925,"Guidelines:
946"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8930693069306931,"• The answer NA means that the paper does not include experiments.
947"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8937293729372937,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
948"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8943894389438944,"dence intervals, or statistical significance tests, at least for the experiments that support
949"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8950495049504951,"the main claims of the paper.
950"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8957095709570957,"• The factors of variability that the error bars are capturing should be clearly stated (for
951"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8963696369636963,"example, train/test split, initialization, random drawing of some parameter, or overall
952"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.897029702970297,"run with given experimental conditions).
953"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8976897689768977,"• The method for calculating the error bars should be explained (closed form formula,
954"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8983498349834983,"call to a library function, bootstrap, etc.)
955"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.899009900990099,"• The assumptions made should be given (e.g., Normally distributed errors).
956"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8996699669966997,"• It should be clear whether the error bar is the standard deviation or the standard error
957"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9003300330033003,"of the mean.
958"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.900990099009901,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
959"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016501650165016,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
960"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9023102310231023,"of Normality of errors is not verified.
961"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.902970297029703,"• For asymmetric distributions, the authors should be careful not to show in tables or
962"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9036303630363036,"figures symmetric error bars that would yield results that are out of range (e.g. negative
963"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9042904290429042,"error rates).
964"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.904950495049505,"• If error bars are reported in tables or plots, The authors should explain in the text how
965"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9056105610561056,"they were calculated and reference the corresponding figures or tables in the text.
966"
EXPERIMENTS COMPUTE RESOURCES,0.9062706270627062,"8. Experiments Compute Resources
967"
EXPERIMENTS COMPUTE RESOURCES,0.906930693069307,"Question: For each experiment, does the paper provide sufficient information on the com-
968"
EXPERIMENTS COMPUTE RESOURCES,0.9075907590759076,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
969"
EXPERIMENTS COMPUTE RESOURCES,0.9082508250825082,"the experiments?
970"
EXPERIMENTS COMPUTE RESOURCES,0.9089108910891089,"Answer: [Yes]
971"
EXPERIMENTS COMPUTE RESOURCES,0.9095709570957096,"Justification: Relevant computing information is provided in Section D.1.
972"
EXPERIMENTS COMPUTE RESOURCES,0.9102310231023102,"Guidelines:
973"
EXPERIMENTS COMPUTE RESOURCES,0.9108910891089109,"• The answer NA means that the paper does not include experiments.
974"
EXPERIMENTS COMPUTE RESOURCES,0.9115511551155115,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
975"
EXPERIMENTS COMPUTE RESOURCES,0.9122112211221122,"or cloud provider, including relevant memory and storage.
976"
EXPERIMENTS COMPUTE RESOURCES,0.9128712871287129,"• The paper should provide the amount of compute required for each of the individual
977"
EXPERIMENTS COMPUTE RESOURCES,0.9135313531353135,"experimental runs as well as estimate the total compute.
978"
EXPERIMENTS COMPUTE RESOURCES,0.9141914191419142,"• The paper should disclose whether the full research project required more compute
979"
EXPERIMENTS COMPUTE RESOURCES,0.9148514851485149,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
980"
EXPERIMENTS COMPUTE RESOURCES,0.9155115511551155,"didn’t make it into the paper).
981"
CODE OF ETHICS,0.9161716171617161,"9. Code Of Ethics
982"
CODE OF ETHICS,0.9168316831683169,"Question: Does the research conducted in the paper conform, in every respect, with the
983"
CODE OF ETHICS,0.9174917491749175,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
984"
CODE OF ETHICS,0.9181518151815181,"Answer: [Yes]
985"
CODE OF ETHICS,0.9188118811881189,"Justification: This work conforms with the NeurIPS Code of Ethics.
986"
CODE OF ETHICS,0.9194719471947195,"Guidelines:
987"
CODE OF ETHICS,0.9201320132013201,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
988"
CODE OF ETHICS,0.9207920792079208,"• If the authors answer No, they should explain the special circumstances that require a
989"
CODE OF ETHICS,0.9214521452145215,"deviation from the Code of Ethics.
990"
CODE OF ETHICS,0.9221122112211221,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
991"
CODE OF ETHICS,0.9227722772277228,"eration due to laws or regulations in their jurisdiction).
992"
BROADER IMPACTS,0.9234323432343234,"10. Broader Impacts
993"
BROADER IMPACTS,0.9240924092409241,"Question: Does the paper discuss both potential positive societal impacts and negative
994"
BROADER IMPACTS,0.9247524752475248,"societal impacts of the work performed?
995"
BROADER IMPACTS,0.9254125412541254,"Answer: [NA]
996"
BROADER IMPACTS,0.926072607260726,"Justification: The work is of theoretical nature and has no societal impact of the work
997"
BROADER IMPACTS,0.9267326732673268,"performed.
998"
BROADER IMPACTS,0.9273927392739274,"Guidelines:
999"
BROADER IMPACTS,0.928052805280528,"• The answer NA means that there is no societal impact of the work performed.
1000"
BROADER IMPACTS,0.9287128712871288,"• If the authors answer NA or No, they should explain why their work has no societal
1001"
BROADER IMPACTS,0.9293729372937294,"impact or why the paper does not address societal impact.
1002"
BROADER IMPACTS,0.93003300330033,"• Examples of negative societal impacts include potential malicious or unintended uses
1003"
BROADER IMPACTS,0.9306930693069307,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
1004"
BROADER IMPACTS,0.9313531353135314,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
1005"
BROADER IMPACTS,0.932013201320132,"groups), privacy considerations, and security considerations.
1006"
BROADER IMPACTS,0.9326732673267327,"• The conference expects that many papers will be foundational research and not tied
1007"
BROADER IMPACTS,0.9333333333333333,"to particular applications, let alone deployments. However, if there is a direct path to
1008"
BROADER IMPACTS,0.933993399339934,"any negative applications, the authors should point it out. For example, it is legitimate
1009"
BROADER IMPACTS,0.9346534653465347,"to point out that an improvement in the quality of generative models could be used to
1010"
BROADER IMPACTS,0.9353135313531353,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
1011"
BROADER IMPACTS,0.935973597359736,"that a generic algorithm for optimizing neural networks could enable people to train
1012"
BROADER IMPACTS,0.9366336633663367,"models that generate Deepfakes faster.
1013"
BROADER IMPACTS,0.9372937293729373,"• The authors should consider possible harms that could arise when the technology is
1014"
BROADER IMPACTS,0.9379537953795379,"being used as intended and functioning correctly, harms that could arise when the
1015"
BROADER IMPACTS,0.9386138613861386,"technology is being used as intended but gives incorrect results, and harms following
1016"
BROADER IMPACTS,0.9392739273927393,"from (intentional or unintentional) misuse of the technology.
1017"
BROADER IMPACTS,0.9399339933993399,"• If there are negative societal impacts, the authors could also discuss possible mitigation
1018"
BROADER IMPACTS,0.9405940594059405,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
1019"
BROADER IMPACTS,0.9412541254125413,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
1020"
BROADER IMPACTS,0.9419141914191419,"feedback over time, improving the efficiency and accessibility of ML).
1021"
SAFEGUARDS,0.9425742574257425,"11. Safeguards
1022"
SAFEGUARDS,0.9432343234323433,"Question: Does the paper describe safeguards that have been put in place for responsible
1023"
SAFEGUARDS,0.9438943894389439,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
1024"
SAFEGUARDS,0.9445544554455445,"image generators, or scraped datasets)?
1025"
SAFEGUARDS,0.9452145214521452,"Answer: [NA]
1026"
SAFEGUARDS,0.9458745874587459,"Justification: The paper poses no such risks as we do not have any released data or models.
1027"
SAFEGUARDS,0.9465346534653465,"Guidelines:
1028"
SAFEGUARDS,0.9471947194719472,"• The answer NA means that the paper poses no such risks.
1029"
SAFEGUARDS,0.9478547854785478,"• Released models that have a high risk for misuse or dual-use should be released with
1030"
SAFEGUARDS,0.9485148514851485,"necessary safeguards to allow for controlled use of the model, for example by requiring
1031"
SAFEGUARDS,0.9491749174917492,"that users adhere to usage guidelines or restrictions to access the model or implementing
1032"
SAFEGUARDS,0.9498349834983498,"safety filters.
1033"
SAFEGUARDS,0.9504950495049505,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
1034"
SAFEGUARDS,0.9511551155115512,"should describe how they avoided releasing unsafe images.
1035"
SAFEGUARDS,0.9518151815181518,"• We recognize that providing effective safeguards is challenging, and many papers do
1036"
SAFEGUARDS,0.9524752475247524,"not require this, but we encourage authors to take this into account and make a best
1037"
SAFEGUARDS,0.9531353135313532,"faith effort.
1038"
LICENSES FOR EXISTING ASSETS,0.9537953795379538,"12. Licenses for existing assets
1039"
LICENSES FOR EXISTING ASSETS,0.9544554455445544,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1040"
LICENSES FOR EXISTING ASSETS,0.9551155115511551,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1041"
LICENSES FOR EXISTING ASSETS,0.9557755775577558,"properly respected?
1042"
LICENSES FOR EXISTING ASSETS,0.9564356435643564,"Answer: [Yes]
1043"
LICENSES FOR EXISTING ASSETS,0.9570957095709571,"Justification: We cited the baseline implementation in Section D.1.
1044"
LICENSES FOR EXISTING ASSETS,0.9577557755775578,"Guidelines:
1045"
LICENSES FOR EXISTING ASSETS,0.9584158415841584,"• The answer NA means that the paper does not use existing assets.
1046"
LICENSES FOR EXISTING ASSETS,0.9590759075907591,"• The authors should cite the original paper that produced the code package or dataset.
1047"
LICENSES FOR EXISTING ASSETS,0.9597359735973597,"• The authors should state which version of the asset is used and, if possible, include a
1048"
LICENSES FOR EXISTING ASSETS,0.9603960396039604,"URL.
1049"
LICENSES FOR EXISTING ASSETS,0.9610561056105611,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1050"
LICENSES FOR EXISTING ASSETS,0.9617161716171617,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1051"
LICENSES FOR EXISTING ASSETS,0.9623762376237623,"service of that source should be provided.
1052"
LICENSES FOR EXISTING ASSETS,0.9630363036303631,"• If assets are released, the license, copyright information, and terms of use in the
1053"
LICENSES FOR EXISTING ASSETS,0.9636963696369637,"package should be provided. For popular datasets, paperswithcode.com/datasets
1054"
LICENSES FOR EXISTING ASSETS,0.9643564356435643,"has curated licenses for some datasets. Their licensing guide can help determine the
1055"
LICENSES FOR EXISTING ASSETS,0.9650165016501651,"license of a dataset.
1056"
LICENSES FOR EXISTING ASSETS,0.9656765676567657,"• For existing datasets that are re-packaged, both the original license and the license of
1057"
LICENSES FOR EXISTING ASSETS,0.9663366336633663,"the derived asset (if it has changed) should be provided.
1058"
LICENSES FOR EXISTING ASSETS,0.966996699669967,"• If this information is not available online, the authors are encouraged to reach out to
1059"
LICENSES FOR EXISTING ASSETS,0.9676567656765677,"the asset’s creators.
1060"
NEW ASSETS,0.9683168316831683,"13. New Assets
1061"
NEW ASSETS,0.968976897689769,"Question: Are new assets introduced in the paper well documented and is the documentation
1062"
NEW ASSETS,0.9696369636963696,"provided alongside the assets?
1063"
NEW ASSETS,0.9702970297029703,"Answer: [NA]
1064"
NEW ASSETS,0.970957095709571,"Justification: Our work is mostly of theoretical nature and does not release new assets.
1065"
NEW ASSETS,0.9716171617161716,"Guidelines:
1066"
NEW ASSETS,0.9722772277227723,"• The answer NA means that the paper does not release new assets.
1067"
NEW ASSETS,0.972937293729373,"• Researchers should communicate the details of the dataset/code/model as part of their
1068"
NEW ASSETS,0.9735973597359736,"submissions via structured templates. This includes details about training, license,
1069"
NEW ASSETS,0.9742574257425742,"limitations, etc.
1070"
NEW ASSETS,0.974917491749175,"• The paper should discuss whether and how consent was obtained from people whose
1071"
NEW ASSETS,0.9755775577557756,"asset is used.
1072"
NEW ASSETS,0.9762376237623762,"• At submission time, remember to anonymize your assets (if applicable). You can either
1073"
NEW ASSETS,0.976897689768977,"create an anonymized URL or include an anonymized zip file.
1074"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775577557755776,"14. Crowdsourcing and Research with Human Subjects
1075"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782178217821782,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1076"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9788778877887789,"include the full text of instructions given to participants and screenshots, if applicable, as
1077"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795379537953796,"well as details about compensation (if any)?
1078"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801980198019802,"Answer: [NA]
1079"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808580858085808,"Justification: This work does not involve crowdsourcing nor research with human subjects.
1080"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815181518151815,"Guidelines:
1081"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821782178217822,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1082"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828382838283828,"human subjects.
1083"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834983498349835,"• Including this information in the supplemental material is fine, but if the main contribu-
1084"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841584158415841,"tion of the paper involves human subjects, then as much detail as possible should be
1085"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848184818481848,"included in the main paper.
1086"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854785478547855,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1087"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861386138613861,"or other labor should be paid at least the minimum wage in the country of the data
1088"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867986798679867,"collector.
1089"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874587458745875,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1090"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881188118811881,"Subjects
1091"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887788778877887,"Question: Does the paper describe potential risks incurred by study participants, whether
1092"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9894389438943895,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1093"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900990099009901,"approvals (or an equivalent approval/review based on the requirements of your country or
1094"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907590759075907,"institution) were obtained?
1095"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9914191419141914,"Answer: [NA]
1096"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920792079207921,"Justification: This work does not involve crowdsourcing nor research with human subjects.
1097"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927392739273927,"Guidelines:
1098"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933993399339934,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1099"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994059405940594,"human subjects.
1100"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947194719471947,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1101"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953795379537954,"may be required for any human subjects research. If you obtained IRB approval, you
1102"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996039603960396,"should clearly state this in the paper.
1103"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966996699669967,"• We recognize that the procedures for this may vary significantly between institutions
1104"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973597359735974,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1105"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998019801980198,"guidelines for their institution.
1106"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986798679867986,"• For initial submissions, do not include any information that would break anonymity (if
1107"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993399339933994,"applicable), such as the institution conducting the review.
1108"
