Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001567398119122257,"The utility of reinforcement learning is limited by the alignment of reward functions
1"
ABSTRACT,0.003134796238244514,"with the interests of human stakeholders. One promising method for alignment is
2"
ABSTRACT,0.004702194357366771,"to learn the reward function from human-generated preferences between pairs of
3"
ABSTRACT,0.006269592476489028,"trajectory segments. These human preferences are typically assumed to be informed
4"
ABSTRACT,0.007836990595611285,"solely by partial return, the sum of rewards along each segment. We ﬁnd this
5"
ABSTRACT,0.009404388714733543,"assumption to be ﬂawed and propose modeling preferences instead as arising from
6"
ABSTRACT,0.0109717868338558,"a different statistic: each segment’s regret, a measure of a segment’s deviation from
7"
ABSTRACT,0.012539184952978056,"optimal decision-making. Given inﬁnitely many preferences generated according
8"
ABSTRACT,0.014106583072100314,"to regret, we prove that we can identify a reward function equivalent to the reward
9"
ABSTRACT,0.01567398119122257,"function that generated those preferences. We also prove that the previous partial
10"
ABSTRACT,0.017241379310344827,"return model lacks this identiﬁability property without preference noise that reveals
11"
ABSTRACT,0.018808777429467086,"rewards’ relative proportions, and we empirically show that our proposed regret
12"
ABSTRACT,0.02037617554858934,"preference model outperforms it with ﬁnite training data in otherwise the same
13"
ABSTRACT,0.0219435736677116,"setting. Additionally, our proposed regret preference model better predicts real
14"
ABSTRACT,0.023510971786833857,"human preferences and also learns reward functions from these preferences that
15"
ABSTRACT,0.025078369905956112,"lead to policies that are better human-aligned. Overall, this work establishes that
16"
ABSTRACT,0.02664576802507837,"the choice of preference model is impactful, and our proposed regret preference
17"
ABSTRACT,0.02821316614420063,"model provides an improvement upon a core assumption of recent research.
18"
INTRODUCTION,0.029780564263322883,"1
Introduction
19"
INTRODUCTION,0.03134796238244514,"Improvements in reinforcement learning (RL) have led to notable recent achievements [1–6],
20"
INTRODUCTION,0.032915360501567396,"increasing its applicability to real-world problems. Yet, like all optimization algorithms, even perfect
21"
INTRODUCTION,0.034482758620689655,"RL optimization is limited by the objective it optimizes. For RL, this objective is created in large
22"
INTRODUCTION,0.03605015673981191,"part by the reward function. Poor alignment between reward functions and the interests of human
23"
INTRODUCTION,0.03761755485893417,"stakeholders limits the utility of RL and may even pose catastrophic risks [7, 8].
24"
INTRODUCTION,0.03918495297805643,"Inﬂuential recent research has focused on reward learning from preferences over pairs of ﬁxed-length
25"
INTRODUCTION,0.04075235109717868,"trajectory segments. Nearly all of this recent work assumes that human preferences arise probabilis-
26"
INTRODUCTION,0.04231974921630094,"tically from only the sum of rewards over a segment, i.e., the segment’s partial return [9–16]. That is,
27"
INTRODUCTION,0.0438871473354232,"these works assume that people tend to prefer trajectory segments that yield greater rewards during the
28"
INTRODUCTION,0.045454545454545456,"segment. However, this preference model ignores seemingly important information about the segment’s
29"
INTRODUCTION,0.047021943573667714,"desirability, including the state values of the segment’s start and end states. Separately, this partial return
30"
INTRODUCTION,0.048589341692789965,"preference model can prefer suboptimal actions with lucky outcomes, like buying a lottery ticket.
31"
INTRODUCTION,0.050156739811912224,"Thispaperproposesanalternativepreferencemodelbasedontheregretofeachsegment, whichisequiv-
32"
INTRODUCTION,0.05172413793103448,"alent to the negated sum of an optimal policy’s advantage of each transition in the segment (Section 2.2).
33"
INTRODUCTION,0.05329153605015674,"Figure 1 shows an intuitive example of when these two models disagree. Other classes of domains that
34"
INTRODUCTION,0.054858934169279,"the models will differ on are those with constant reward until the end, including competitive games like
35"
INTRODUCTION,0.05642633228840126,"chess, go, and soccer as well as tasks for which the objective is to minimize time until reaching a goal.
36"
INTRODUCTION,0.05799373040752351,"VXP
RIUHZDUG
IRUEXPSLQJ"
INTRODUCTION,0.05956112852664577,WKHZDOO
INTRODUCTION,0.061128526645768025,"VXPRI
UHZDUG"
INTRODUCTION,0.06269592476489028,"HQGVWDWH
FUDVKLPPLQHQW"
INTRODUCTION,0.06426332288401254,"VWDUWVWDWH
VDIH"
INTRODUCTION,0.06583072100313479,HQGVWDWH VDIH
INTRODUCTION,0.06739811912225706,"VWDUWVWDWH
FUDVKOLNHO\"
INTRODUCTION,0.06896551724137931,"Figure 1: Two segments of a car moving at high speed
near a brick wall. Assume the right segment is opti-
mal and the left segment is suboptimal (as deﬁned in
Sec. 2.1). The left segment has a higher sum of reward,
so the partial return preference model tends to prefer
it. The regret preference model instead tends to prefer
the right segment because optimal segments have mini-
mal regret. If we also assume deterministic transitions,
then the regret model includes the difference in values
between the start state and the end state (Eq. 3), and
the right segment would tend to be preferred because
it greatly improves its state values from start to end,
whereas the left segment’s state values greatly worsen.
We suspect our human readers will also tend to prefer
the right segment."
INTRODUCTION,0.07053291536050156,"For these two preference models, we ﬁrst focus the-
37"
INTRODUCTION,0.07210031347962383,"oretically on a normative analysis (Section 3)—i.e.,
38"
INTRODUCTION,0.07366771159874608,"what preference model would we want humans
39"
INTRODUCTION,0.07523510971786834,"to use if we could choose—proving that reward
40"
INTRODUCTION,0.0768025078369906,"learning on inﬁnite, exhaustive preferences with
41"
INTRODUCTION,0.07836990595611286,"our proposed regret preference model identiﬁes a
42"
INTRODUCTION,0.07993730407523511,"reward function with the same set of optimal poli-
43"
INTRODUCTION,0.08150470219435736,"cies as the reward function with which the prefer-
44"
INTRODUCTION,0.08307210031347963,"ences are generated. We also prove that the par-
45"
INTRODUCTION,0.08463949843260188,"tial return preference model is not guaranteed to
46"
INTRODUCTION,0.08620689655172414,"identify such a reward function without preference
47"
INTRODUCTION,0.0877742946708464,"noise. We follow up with a descriptive analysis of
48"
INTRODUCTION,0.08934169278996865,"how well each of these proposed models align with
49"
INTRODUCTION,0.09090909090909091,"actual human preferences by collecting a human-
50"
INTRODUCTION,0.09247648902821316,"labeled dataset of preferences in a rich grid world
51"
INTRODUCTION,0.09404388714733543,"domain (Section 4) and showing that the regret pref-
52"
INTRODUCTION,0.09561128526645768,"erence model better predicts these human prefer-
53"
INTRODUCTION,0.09717868338557993,"ences (Section 5). Finally, we ﬁnd that the policies
54"
INTRODUCTION,0.0987460815047022,"ultimately created through the regret preference
55"
INTRODUCTION,0.10031347962382445,"model tend to outperform those from the partial
56"
INTRODUCTION,0.10188087774294671,"return model learning—both when assessed with
57"
INTRODUCTION,0.10344827586206896,"collected human preferences or when assessed with
58"
INTRODUCTION,0.10501567398119123,"synthetic preferences (Section 6).
59"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.10658307210031348,"2
Preference models for learning reward functions
60"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.10815047021943573,"We assume that the task environment is a Markov decision process (MDP) speciﬁed by the tuple (S, A,
61"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.109717868338558,"T, γ, D0, r). S and A are the sets of possible states and actions, respectively. T is a transition function,
62"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11128526645768025,"T : S ⇥A ! S. γ is the discount factor and D0 is the distribution of start states. Unless otherwise
63"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11285266457680251,"stated, we assume undiscounted tasks (i.e., γ =1) that have terminal states, after which only 0 reward
64"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11442006269592477,"can be received. r is a reward function, r:S⇥A⇥S !R, where the reward rt at time t is a function of
65"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11598746081504702,"st, at, and st+1. An MDP\r is an MDP without a reward function.
66"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11755485893416928,"Throughout this paper, r refers to the ground-truth reward function for some MDP; ˆr refers to a learned
67"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.11912225705329153,"approximation of r; and ˜r refers to any reward function (including r or ˆr). A policy (⇡:S⇥A![0,1])
68"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.1206896551724138,speciﬁes the probability of an action given a state. Q⇤
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.12225705329153605,˜r and V ⇤
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.1238244514106583,"˜r refer respectively to the state-action value
69"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.12539184952978055,"function and state value function for an optimal policy, ⇡⇤, under ˜r. The optimal advantage function is
70"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.12695924764890282,deﬁned as A⇤
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.12852664576802508,"˜r(s,a) , Q⇤"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.13009404388714735,"˜r(s,a)−V ⇤"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.13166144200626959,"˜r (s). Throughout this paper, the ground-truth reward function r
71"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.13322884012539185,"is used to algorithmically generate preferences when they are not human-generated, is hidden during
72"
PREFERENCE MODELS FOR LEARNING REWARD FUNCTIONS,0.13479623824451412,"reward learning, and is used to evaluate the performance of optimal policies under a learned ˆr.
73"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.13636363636363635,"2.1
Reward learning from pairwise preferences
74"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.13793103448275862,"A reward function can be learned by minimizing the cross-entropy loss—i.e., maximizing the
75"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.13949843260188088,"likelihood—of observed human preferences, a common approach in recent literature [9–11, 14, 16].
76"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.14106583072100312,"Segments Let σ denote a segment starting at state sσ,0. Its length |σ| is the number of transitions within
77"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1426332288401254,"the segment. A segment includes |σ|+1 states and |σ| actions: (sσ,0,aσ,0,sσ,1,aσ,1,...,sσ,|σ|). In this
78"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.14420062695924765,"problem setting, segments lack any reward information. As shorthand, we deﬁneσt ,(sσ,t,aσ,t,sσ,t+1).
79"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.14576802507836992,"A segment σ is optimal with respect to ˜r if, for every i 2 {1,...,|σ|-1}, Q⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.14733542319749215,"˜r(sσ,i,aσ,i) = V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.14890282131661442,"˜r (sσ,i). A
80"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15047021943573669,"segment that is not optimal is suboptimal. Given some ˜r and a segment σ, ˜rt , ˜r(sσ,t,aσ,t,sσ,t+1),
81"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15203761755485892,and the partial return of a segment σ is P|σ|−1
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1536050156739812,"t=0 γt˜rt, denoted in shorthand as ⌃σ r.
82"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15517241379310345,"Preference datasets
Each preference over a pair of segments creates a sample (σ1,σ2,µ) in a
83"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15673981191222572,"preference dataset D≻. Vector µ=hµ1,µ2i represents the preference; speciﬁcally, if σ1 is preferred
84"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15830721003134796,"over σ2, denoted σ1 ≻σ2, µ=h1,0i. µ is h0,1i if σ1 ≺σ2 and is h0.5,0.5i for σ1 ⇠σ2 (no preference).
85"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.15987460815047022,"Loss function
To learn a reward function from a preference dataset, D≻, a common assumption
86"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1614420062695925,"is that these preferences were generated by a preference model P that arises from an unobservable
87"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.16300940438871472,"ground-truth reward function r. We approximate r by minimizing cross-entropy loss to learn ˆr:
88"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.164576802507837,"loss(ˆr,D≻)=− X"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.16614420062695925,"(σ1,σ2,µ)2D≻"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1677115987460815,"µ1logP(σ1 ≻σ2|ˆr)+µ2logP(σ1 ≺σ2|ˆr)
(1)"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.16927899686520376,"This loss is under-speciﬁed until P(σ1 ≻σ2|ˆr) is deﬁned, which is the focus of this paper. We show that
89"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.17084639498432602,"the common model of preference probabilities is ﬂawed and introduce an improved preference model.
90"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1724137931034483,"Preference models
A preference model determines the probability of one trajectory segment being
91"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.17398119122257052,"preferred over another, P(σ1 ≻σ2|˜r). Preference models could be applied to model preferences
92"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1755485893416928,"provided by humans or other systems. Preference models can also directly generate preferences, and in
93"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.17711598746081506,"such cases we refer to them as preference generators.
94"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1786833855799373,"2.2
Choice of preference model: partial return and regret
95"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.18025078369905956,"Partial return
Recent work assumes human preferences are generated by a Boltzmann distribution
96"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.18181818181818182,"over the two segments’ partial returns [9–16], expressed here as a logistic function1:
97"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1833855799373041,P⌃r(σ1 ≻σ2|˜r)=logistic ⇣
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.18495297805642633,"⌃σ1˜r−⌃σ2˜r ⌘ .
(2)"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1865203761755486,"Regret
We introduce an alternative preference model based on the regret of each transition in a
98"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.18808777429467086,"segment. We ﬁrst focus on segments with deterministic transitions. For a transition (st,at,st+1) in a
99"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1896551724137931,"deterministic segment, regretd(σt|˜r),V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.19122257053291536,"˜r (sσ,t)−[˜rt+V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.19278996865203762,"˜r (sσ,t+1)]. For a full deterministic segment,
100 101"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.19435736677115986,"regretd(σ|˜r),"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.19592476489028213,"|σ|−1
X t=0"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.1974921630094044,regretd(σt|˜r)=V ⇤
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.19905956112852666,"˜r (sσ,0)−(⌃σ˜r+V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2006269592476489,"˜r (sσ,|σ|)),
(3)"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.20219435736677116,"with the right-hand expression arising from cancelling out intermediate state values. Therefore,
102"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.20376175548589343,deterministic regret measures how much the segment reduces expected return from V ⇤
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.20532915360501566,"˜r (sσ,0). An
103"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.20689655172413793,"optimal segment, σ⇤, always has 0 regret, and a suboptimal segment, σ¬⇤, will always have positive
104"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2084639498432602,"regret, a intuitively appealing property that also plays a role in the identiﬁability proof of Theorem 3.1.
105"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.21003134796238246,"Stochastic transitions, however, can result in regretd(σ⇤|ˆr) > regretd(σ¬⇤|˜r), losing the property
106"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2115987460815047,"above. To retain it, we note that the effect on expected return of transition stochasticity from a
107"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.21316614420062696,"transition (st,at,st+1) is [˜rt+V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.21473354231974923,˜r (st+1)]−Q⇤
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.21630094043887146,"˜r(st,at) and add this expression once per transition to
108"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.21786833855799373,"get regret(σ), removing the subscript d that refers to determinism. The regret for a single transition
109"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.219435736677116,becomes regret(σt|˜r) = [V ⇤
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.22100313479623823,"˜r (sσ,t) −[˜rt + V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2225705329153605,"˜r (sσ,t+1)]] + [[˜rt + V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.22413793103448276,"˜r (sσ,t+1)] −Q⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.22570532915360503,"˜r(sσ,t,aσ,t)] =
110 V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.22727272727272727,"˜r (sσ,t)−Q⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.22884012539184953,"˜r(sσ,t,aσ,t)=−A⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2304075235109718,"˜r(sσ,t,aσ,t). Regret for a full segment is
111"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.23197492163009403,regret(σ|˜r)=
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2335423197492163,"|σ|−1
X t=0"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.23510971786833856,regret(σt|˜r)=
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.23667711598746083,"|σ|−1
X t=0 h V ⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.23824451410658307,"˜r (sσ,t)−Q⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.23981191222570533,"˜r(sσ,t,aσ,t) i ="
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2413793103448276,"|σ|−1
X t=0 −A⇤"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.24294670846394983,"˜r(sσ,t,aσ,t).
(4)"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2445141065830721,"The regret preference model is the Boltzmann distribution over negated regret:
112"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.24608150470219436,"Pregret(σ1 ≻σ2|˜r),logistic ⇣"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2476489028213166,"regret(σ2|˜r)−regret(σ1|˜r) ⌘ .
(5)"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.24921630094043887,"Lastly, we note that if two segments have deterministic transitions, end in terminal states, and have the
113"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2507836990595611,"same starting state, this regret model reduces to the partial return model: Pregret(·|˜r)=P⌃r(·|˜r).
114"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.25235109717868337,"Algorithms in this paper
All algorithms in the body of this paper are deﬁned simply as “minimize
115"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.25391849529780564,"Equation 1”. They differ only in how the preference probabilities are calculated. All reward function
116"
REWARD LEARNING FROM PAIRWISE PREFERENCES,0.2554858934169279,"learning via partial return uses Equation 2. We use two algorithms for reward function learning
117"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.25705329153605017,"1See Appendix B for a derivation of this logistic expression from a Boltzmann distribution with a temperature
of 1. Unless otherwise stated, we ignore the temperature because scaling reward has the same effect."
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.25862068965517243,"via regret. The theory in Section 3 assumes exact measurement of regret, using Equation 5. Our
118"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2601880877742947,"experimental results in Section 6 use Equation 6 to approximate regret. Appendix B introduces other
119"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2617554858934169,"algorithms that use Equation 1, as well as one in Appendix B.2 that generalizes Equation 1.
120"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.26332288401253917,"Regret as a model for human preference
Pregret makes at least three assumptions worth noting.
121"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.26489028213166144,"First, it keeps the assumption that human preferences follow a Boltzmann distribution over some
122"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2664576802507837,"statistic, which is a common model of choice behavior in economics and psychology, where it is
123"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.26802507836990597,"called the Luce-Shepard choice rule [17, 18]. Second, Pregret implicitly assumes humans can identify
124"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.26959247648902823,"optimal and suboptimal segments when they see them, which will less true in domains where the human
125"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2711598746081505,"has less expertise. Lastly, Pregret assumes that in stochastic settings where the best outcome may only
126"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2727272727272727,"result from suboptimal decisions (e.g., buying a lottery ticket), humans instead prefer optimal decisions.
127"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.274294670846395,"We suspect humans are capable of expressing either type of preference—based on decision quality
128"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.27586206896551724,"or desirability of outcomes—and can be inﬂuenced by training or the preference elicitation interface.
129"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2774294670846395,"In practice we determine that the regret model produces improvements over the partial-return model
130"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.27899686520376177,"(Section 6), and its assumptions represent an opportunity for follow-up research.
131"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.28056426332288403,"Alternative methods for learning reward functions Other methods for learning reward functions
132"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.28213166144200624,"include inverse reinforcement learning from demonstrations [19, 20] (discussed in Appendix B.5) and
133"
SEE APPENDIX B FOR A DERIVATION OF THIS LOGISTIC EXPRESSION FROM A BOLTZMANN DISTRIBUTION WITH A TEMPERATURE,0.2836990595611285,"inverse reward design from trial-and-error reward design in multiple instances of a task domain [21].
134"
THEORETICAL COMPARISONS,0.2852664576802508,"3
Theoretical comparisons
135"
THEORETICAL COMPARISONS,0.28683385579937304,"In this section, we consider how different ways of generating preferences affect reward inference, setting
136"
THEORETICAL COMPARISONS,0.2884012539184953,"aside whether humans can be inﬂuenced to give preferences in accordance with a speciﬁc preference
137"
THEORETICAL COMPARISONS,0.28996865203761757,"method. In economic terms, this analysis—and all of our analyses with synthetic preferences—could
138"
THEORETICAL COMPARISONS,0.29153605015673983,"be considered a normative analysis. In artiﬁcial intelligence, this analysis might be cast as a step
139"
THEORETICAL COMPARISONS,0.29310344827586204,"towards deﬁning criteria for a rational preference model.
140"
THEORETICAL COMPARISONS,0.2946708463949843,"Deﬁnition 3.1 (An identiﬁable preference model). For a preference model P, assume an inﬁnite
141"
THEORETICAL COMPARISONS,0.2962382445141066,"dataset D≻of n-length pairs of segments is constructed by repeatedly choosing (σ1,σ2) and sampling
142"
THEORETICAL COMPARISONS,0.29780564263322884,"a label µ⇠P(σ1 ≻σ2|r), using P as a preference generator. Further assume that in this dataset, all
143"
THEORETICAL COMPARISONS,0.2993730407523511,"possible n-length segment pairs appear inﬁnitely many times. For some MDP\r M, let M˜r be M with
144"
THEORETICAL COMPARISONS,0.30094043887147337,the reward function ˜r. Let ⇧⇤
THEORETICAL COMPARISONS,0.30250783699059564,"˜r be the set of optimal policies in M˜r. Let reward-equivalence class R be
145"
THEORETICAL COMPARISONS,0.30407523510971785,"the set of all reward functions such that if r1,r2 2R then ⇧⇤"
THEORETICAL COMPARISONS,0.3056426332288401,r1 =⇧⇤
THEORETICAL COMPARISONS,0.3072100313479624,"r2. Preference model P is identiﬁable
146"
THEORETICAL COMPARISONS,0.30877742946708464,"if, for any choice of n and Mr , any ˆr = argmin˜r,D≻[loss(˜r)]—for the cross-entropy loss (Eqn. 1),
147"
THEORETICAL COMPARISONS,0.3103448275862069,"with P as the preference model—is in the same reward equivalence class as r. I.e., ⇧⇤ r =⇧⇤"
THEORETICAL COMPARISONS,0.31191222570532917,"ˆr.
148"
THEORETICAL COMPARISONS,0.31347962382445144,"Theorem 3.1 (Pregret is identiﬁable). Let Pregret be any function such that if regret(σ1|˜r) <
149"
THEORETICAL COMPARISONS,0.31504702194357365,"regret(σ2|˜r), Pregret(σ1 ≻σ2|˜r) > 0.5, and if regret(σ1|˜r) = regret(σ2|˜r), Pregret(σ1 ≻σ2|˜r) =
150"
THEORETICAL COMPARISONS,0.3166144200626959,"0.5. Pregret is identiﬁable.
151"
THEORETICAL COMPARISONS,0.3181818181818182,"This class of regret preference models includes but is not limited to the Boltzmann distribution of Eqn. 5
152"
THEORETICAL COMPARISONS,0.31974921630094044,"and the narrower class that Theorem 3.1 focuses upon.
153"
THEORETICAL COMPARISONS,0.3213166144200627,"Theorem 3.2 (Noiseless P⌃r is not identiﬁable). Let P⌃r be any function such that if ⌃σ1˜r >⌃σ2˜r,
154"
THEORETICAL COMPARISONS,0.322884012539185,"P⌃r(σ1 ≻σ2|˜r)=1, and if ⌃σ1˜r=⌃σ2˜r, P⌃r(σ1 ≻σ2|˜r)=0.5. There exists an MDP in which P⌃r is
155"
THEORETICAL COMPARISONS,0.32445141065830724,"not identiﬁable.
156"
THEORETICAL COMPARISONS,0.32601880877742945,"Appendix C contains a proof of Theorem 3.1 and two proofs by example for Theorem 3.2, each
157"
THEORETICAL COMPARISONS,0.3275862068965517,"focusing on a different weakness of P⌃r.The ﬁrst proof by example reveals issues when learning
158"
THEORETICAL COMPARISONS,0.329153605015674,"reward functions with stochastic transitions with either P⌃r or deterministic Pregretd. These issues
159"
THEORETICAL COMPARISONS,0.33072100313479624,"directly correspond to the need for preferences over distributions over outcomes (i.e., lotteries) to
160"
THEORETICAL COMPARISONS,0.3322884012539185,"construct a cardinal utility function (see Russell and Norvig [22, Ch. 16]). Note that the noiseless
161"
THEORETICAL COMPARISONS,0.3338557993730408,"version of P⌃r in Theorem 3.2 is achieved in the limit as reward values are scaled higher; equivalently,
162"
THEORETICAL COMPARISONS,0.335423197492163,"one could include a Boltzmann temperature parameter in Equation 2 and scale it towards 0. Intuitively,
163"
THEORETICAL COMPARISONS,0.33699059561128525,"Theorem 3.2 says that P⌃r is not identiﬁable without the distribution over preferences providing
164"
THEORETICAL COMPARISONS,0.3385579937304075,"information about the proportions of rewards with respect to each other. In contrast, to be identiﬁable,
165"
THEORETICAL COMPARISONS,0.3401253918495298,"the regret preference model does not require this preference error (though it can presumably beneﬁt
166"
THEORETICAL COMPARISONS,0.34169278996865204,"from it in certain contexts).
167"
CREATING A HUMAN-LABELED PREFERENCE DATASET,0.3432601880877743,"4
Creating a human-labeled preference dataset
168"
CREATING A HUMAN-LABELED PREFERENCE DATASET,0.3448275862068966,"To empirically investigate the consequences of each preference model when learning reward from
169"
CREATING A HUMAN-LABELED PREFERENCE DATASET,0.3463949843260188,"human preferences, we created a preference dataset labeled by human subjects via Amazon Mechanical
170"
CREATING A HUMAN-LABELED PREFERENCE DATASET,0.34796238244514105,"Turk. This data collection was IRB-approved. Appendix D adds detail to the content below.
171"
THE GENERAL DELIVERY DOMAIN,0.3495297805642633,"4.1
The general delivery domain
172"
THE GENERAL DELIVERY DOMAIN,0.3510971786833856,"The delivery domain consists of a grid of cells, each of a speciﬁc road surface type. The delivery agent’s
173"
THE GENERAL DELIVERY DOMAIN,0.35266457680250785,"state is its location. The agent’s action space is moving one cell in one of the four cardinal directions.
174"
THE GENERAL DELIVERY DOMAIN,0.3542319749216301,"The episode can terminate either at the destination for +50 reward or in failure at a sheep for −50
175"
THE GENERAL DELIVERY DOMAIN,0.3557993730407524,"reward. The reward for a non-terminal transition is the sum of any reward components. Cells with a
176"
THE GENERAL DELIVERY DOMAIN,0.3573667711598746,"white road surface have a −1 reward component, and cells with brick surface have a −2 component.
177"
THE GENERAL DELIVERY DOMAIN,0.35893416927899685,"Additionally, each cell may contain a coin (+1) or a roadblock (−1). Coins do not disappear and at
178"
THE GENERAL DELIVERY DOMAIN,0.3605015673981191,"best cancel out the road surface cost. Actions that would move the agent into a house or beyond the
179"
THE GENERAL DELIVERY DOMAIN,0.3620689655172414,"grid’s perimeter result in no motion and receive reward that includes the current cell’s surface reward
180"
THE GENERAL DELIVERY DOMAIN,0.36363636363636365,"component but not any coin or roadblock components. In this work, the start state distribution, D0, is
181"
THE GENERAL DELIVERY DOMAIN,0.3652037617554859,"always uniformly random over non-terminal states. This domain was designed to permit subjects to
182"
THE GENERAL DELIVERY DOMAIN,0.3667711598746082,"easily identify bad behavior yet also to be difﬁcult for them to determine optimal behavior from most
183"
THE GENERAL DELIVERY DOMAIN,0.3683385579937304,"states, which is representative of many common tasks.
184"
THE DELIVERY TASK,0.36990595611285265,"4.1.1
The delivery task
185"
THE DELIVERY TASK,0.3714733542319749,"Figure 2: The delivery task used to gather
human preferences. The yellow van is the
agent and the red inverted teardrop is the
destination."
THE DELIVERY TASK,0.3730407523510972,"We chose one instantiation of the delivery domain for gath-
186"
THE DELIVERY TASK,0.37460815047021945,"ering our dataset of human preferences. This speciﬁc MDP
187"
THE DELIVERY TASK,0.3761755485893417,"has a 10⇥10 grid. From every state, the highest return pos-
188"
THE DELIVERY TASK,0.3777429467084639,"sible involves reaching the goal, rather than hitting a sheep or
189"
THE DELIVERY TASK,0.3793103448275862,"perpetually avoiding termination. Figure 2 shows this task.
190"
THE USER INTERFACE AND SURVEY,0.38087774294670845,"4.2
The user interface and survey
191"
THE USER INTERFACE AND SURVEY,0.3824451410658307,"This subsection describes the three main stages of the ex-
192"
THE USER INTERFACE AND SURVEY,0.384012539184953,"perimental session. A video showing the full experimental
193"
THE USER INTERFACE AND SURVEY,0.38557993730407525,"protocol can be seen at bit.ly/humanprefs.
194"
THE USER INTERFACE AND SURVEY,0.3871473354231975,"Teaching subjects about the task
Subjects ﬁrst view in-
195"
THE USER INTERFACE AND SURVEY,0.3887147335423197,"structions describing the general domain. To avoid the jargon
196"
THE USER INTERFACE AND SURVEY,0.390282131661442,"of “return” and “reward,” these terms are mapped to equiv-
197"
THE USER INTERFACE AND SURVEY,0.39184952978056425,"alent values in US dollars, and the instructions describe the
198"
THE USER INTERFACE AND SURVEY,0.3934169278996865,"goal of the task as maximizing the delivery vehicle’s ﬁnancial outcome, where the reward components
199"
THE USER INTERFACE AND SURVEY,0.3949843260188088,"are speciﬁc ﬁnancial impacts. This information is shared amongst interspersed interactive episodes,
200"
THE USER INTERFACE AND SURVEY,0.39655172413793105,"in which the subject controls the agent in domain maps that are each designed to teach one or two
201"
THE USER INTERFACE AND SURVEY,0.3981191222570533,"concepts. Our intention during this stage is to inform the later preferences of the subject by teaching
202"
THE USER INTERFACE AND SURVEY,0.3996865203761755,"them about the domain’s dynamics and its reward function, as well as to develop the subject’s sense of
203"
THE USER INTERFACE AND SURVEY,0.4012539184952978,"how desirable various behaviors are. At the end of this stage, the subject controls the agent for two
204"
THE USER INTERFACE AND SURVEY,0.40282131661442006,"episodes in the speciﬁc delivery task shown in Figure 2.
205"
THE USER INTERFACE AND SURVEY,0.4043887147335423,"Preference elicitation
After each subject is trained to understand the task, they indicate their
206"
THE USER INTERFACE AND SURVEY,0.4059561128526646,"preferences between 40–50 randomly-ordered pairs of segments, using the interface shown in Figure 3.
207"
THE USER INTERFACE AND SURVEY,0.40752351097178685,"The users select a preference, no preference (“same""), or “can’t tell”. In this work, we exclude responses
208"
THE USER INTERFACE AND SURVEY,0.4090909090909091,"labeled “can’t tell”, though one might alternatively try to extract information from these responses.
209"
THE USER INTERFACE AND SURVEY,0.4106583072100313,"Users’ task comprehension Subjects then answered questions testing their understanding of the task,
210"
THE USER INTERFACE AND SURVEY,0.4122257053291536,"and we removed their data if they scored poorly. We also removed a subject’s data if they preferred
211"
THE USER INTERFACE AND SURVEY,0.41379310344827586,"colliding the vehicle into a sheep over not doing so, which we interpreted as poor task understanding or
212"
THE USER INTERFACE AND SURVEY,0.4153605015673981,"inattentiveness. This ﬁltered dataset contains 1812 preferences from 50 subjects.
213"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4169278996865204,"4.3
Selection of segment pairs for labeling
214"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.41849529780564265,Figure 3: Interface shown to subjects during preference elicitation.
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4200626959247649,"We collected human prefer-
215"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4216300940438871,"ences in two stages, each
216"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4231974921630094,"with different methods for
217"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.42476489028213166,"selecting which segment
218"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4263322884012539,"pairs to present for label-
219"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4278996865203762,"ing.
The second stage’s
220"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.42946708463949845,"sole purpose was to im-
221"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.43103448275862066,"prove the reward-learning
222"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.43260188087774293,"performance of P⌃r. With-
223"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4341692789968652,"out second-stage data, P⌃r
224"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.43573667711598746,"compared even worse to
225"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.4373040752351097,"Pregret than in the results
226"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.438871473354232,"described in Section 6 (see
227"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.44043887147335425,"Appendix ??). Both stages’
228"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.44200626959247646,"data are combined and used as a single dataset. These methods and their justiﬁcation are described in
229"
SELECTION OF SEGMENT PAIRS FOR LABELING,0.44357366771159873,"Appendix D.3.
230"
DESCRIPTIVE RESULTS,0.445141065830721,"5
Descriptive results
231"
DESCRIPTIVE RESULTS,0.44670846394984326,"Figure 4: Proportions at which subjects preferred each
segment in a pair, plotted by the difference in the seg-
ments’ changes in state values (x-axis) and partial returns
(y-axis). The diagonal line shows points of preference
indifference for Pregret. Points of indifference for P⌃
lie on the x-axis. The shaded gray area indicates where
the two models disagree, each giving a different segment
a preference probability greater than 0.5. Each circle’s
area is proportional to the number of samples it describes."
DESCRIPTIVE RESULTS,0.4482758620689655,"This section considers how well different prefer-
232"
DESCRIPTIVE RESULTS,0.4498432601880878,"ence models explain our dataset of human pref-
233"
DESCRIPTIVE RESULTS,0.45141065830721006,"erences.
234"
CORRELATIONS,0.45297805642633227,"5.1
Correlations
235"
CORRELATIONS,0.45454545454545453,"between preferences and segment statistics
236"
CORRELATIONS,0.4561128526645768,"We hypothesize that the values of segments’ start
237"
CORRELATIONS,0.45768025078369906,"and end states—which are included in Pregret
238"
CORRELATIONS,0.4592476489028213,"but not in P⌃—affect human preferences, inde-
239"
CORRELATIONS,0.4608150470219436,"pendent of partial return. To simplify analysis,
240"
CORRELATIONS,0.46238244514106586,"we combine the two parts of regretd(σ|r) that
241"
CORRELATIONS,0.46394984326018807,"are additional to ⌃σ˜r and introduce the follow-
242"
CORRELATIONS,0.46551724137931033,"ing shorthand: ∆σV˜r , V ⇤"
CORRELATIONS,0.4670846394984326,"˜r (sσ,|σ|)−V ⇤"
CORRELATIONS,0.46865203761755486,"˜r (sσ,0).
243"
CORRELATIONS,0.4702194357366771,"Note that with an algebraic manipulation (see Ap-
244"
CORRELATIONS,0.4717868338557994,"pendix E.1), regretd(σ2|˜r) −regretd(σ1|˜r) =
245"
CORRELATIONS,0.47335423197492166,"(∆σ1V˜r −∆σ2V˜r)+(⌃σ1˜r−⌃σ2˜r). Therefore,
246"
CORRELATIONS,0.47492163009404387,"on the diagonal line in Figure 4, regretd(σ2|r)=
247"
CORRELATIONS,0.47648902821316613,"regretd(σ1|r), making the Pregretd preference model indifferent.
248"
CORRELATIONS,0.4780564263322884,"Preference model
Loss
P(·)=0.5 (uninformed)
0.69
P⌃r (partial return)
0.62
Pregret
0.57"
CORRELATIONS,0.47962382445141066,"Table 1: Mean cross-entropy test loss
over 10-fold cross validation (n=1812)
from predicting human preferences.
Lower is better."
CORRELATIONS,0.48119122257053293,"The dataset of preferences is visualized in Figure 4. This plot
249"
CORRELATIONS,0.4827586206896552,"shows how ∆σVr has inﬂuence independent of partial return
250"
CORRELATIONS,0.4843260188087774,"by focusing only on points at a chosen y-axis value; if the colors
251"
CORRELATIONS,0.48589341692789967,"alongthecorrespondinghorizontallinereddensasthex-axisvalue
252"
CORRELATIONS,0.48746081504702193,"increases, then ∆σVr appears to have independent inﬂuence. To
253"
CORRELATIONS,0.4890282131661442,"statisticallytestforindependentinﬂuenceof∆σVr onpreferences,
254"
CORRELATIONS,0.49059561128526646,"we consider subsets of data where ⌃σ1r−⌃σ2r is constant. For
255"
CORRELATIONS,0.49216300940438873,"⌃σ1r−⌃σ2r =−1 and ⌃σ1r−⌃σ2r =−2, the only values with
256"
CORRELATIONS,0.493730407523511,"more than 30 samples that also include informative samples with both negative and positive values of
257"
CORRELATIONS,0.4952978056426332,"regret(σ1|r)−regret(σ2|r), the Spearman’s rank correlations between ∆σVr and the preferences
258"
CORRELATIONS,0.49686520376175547,"are signiﬁcant (r>=0.3, p<0.0001). This result indicates that ∆σVr inﬂuences human preferences
259"
CORRELATIONS,0.49843260188087773,"independent of partial return, validating our hypothesis that humans form preferences based on
260"
CORRELATIONS,0.5,"information about segments’ start states and end states, not only partial returns.
261"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5015673981191222,"5.2
Likelihood of human preferences under different preference models
262"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5031347962382445,"To examine how well each preference model predicts human preferences, we calculate the cross-
263"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5047021943573667,"entropy loss for each model (Eqn. 1)—i.e., the negative log likelihood—of the preferences in our
264"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5062695924764891,"dataset. Scaling reward by a constant factor does not affect the set of optimal policies. Therefore,
265"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5078369905956113,"throughout this work we ensure that our analyses of preference models are insensitive to reward scaling.
266"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5094043887147336,"To do so for this speciﬁc analysis, we conduct 10-fold cross validation to learn a reward scaling factor
267"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.5109717868338558,"for each of Pregret and P⌃r. Table 1 shows that the loss of Pregret is lower than that of P⌃r, indicating
268"
LIKELIHOOD OF HUMAN PREFERENCES UNDER DIFFERENT PREFERENCE MODELS,0.512539184952978,"that it is more reﬂective of how people actually express preferences.
269"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5141065830721003,"6
Results from learning reward functions
270"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5156739811912225,"Analysis of a preference model’s predictions of human preferences is informative, but such predictions
271"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5172413793103449,"are a means to the ends of learning human-aligned reward functions and policies. We now examine each
272"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5188087774294671,"preference model’s performance on these ends. In all cases, we learn a reward function ˆr according
273"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5203761755485894,to Eqn. 1 and apply value iteration [23] to ﬁnd the approximately optimal Q⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5219435736677116,ˆr function. For this Q⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5235109717868338,"ˆr,
274"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5250783699059561,"we then evaluate the mean return of the maximum-entropy optimal policy—which chooses uniformly
275"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5266457680250783,"randomly among all optimal actions—with respect to the ground-truth reward function r, over D0.
276"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5282131661442007,"To compare performance across different MDPs, the mean return of a policy ⇡, V ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5297805642633229,"r , is normalized
277"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5313479623824452,to (V ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5329153605015674,r −V U
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5344827586206896,r )/V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5360501567398119,"r , where V ⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5376175548589341,r is the optimal expected return and V U
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5391849529780565,"r is the expected return of the
278"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5407523510971787,uniformly random policy (both given D0). Normalized mean return above 0 is better than V U
RESULTS FROM LEARNING REWARD FUNCTIONS,0.542319749216301,"r . Optimal
279"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5438871473354232,"policies have a normalized mean return of 1, and we consider above 0.9 to be near optimal.
280"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5454545454545454,"6.1
An algorithm to learn reward functions with regret(σσ|ˆr)
281"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5470219435736677,"Algorithm 1 is a general algorithm for learning a linear reward function according to Pregret. This
282"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.54858934169279,"regret-speciﬁc algorithm only changes the regret-based algorithm from Section 2.2 by replacing
283"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5501567398119123,"Equation 5 with a tractable approximation of regret, avoiding expensive repeated evaluation of V ⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5517241379310345,"ˆr (·)
284"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5532915360501567,and Q⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.554858934169279,"ˆr(·,·) to compute Pregret(·|ˆr) during reward learning. Speciﬁcally, successor features for a set
285"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5564263322884012,"of policies are used to approximate the optimal state values and state-action values for any reward
286"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5579937304075235,"function.
287"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5595611285266457,"Approximating Pregret with successor features
Following the notation of Barreto et al. [24], assume
288"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5611285266457681,"the ground-truth reward is linear with respect to a feature vector extracted by φ:S⇥A⇥S !Rd and
289"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5626959247648903,"a weight vector wr 2 Rd: r(s,a,s0) = φ(s,a,s0)>wr. During learning, wˆr similarly expresses ˆr as
290"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5642633228840125,"ˆr(s,a,s0)=φ(s,a,s0)>wˆr.
291"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5658307210031348,"Given a policy ⇡, the successor features for (s,a) are the expectation of discounted reward features
292"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.567398119122257,from that state-action pair when following ⇡:  ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5689655172413793,"Q(s,a)=E⇡[P1"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5705329153605015,"i=tγi−tφ(st,at,st+1)|st =s,at =a].
293"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5721003134796239,"Therefore, Q⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5736677115987461,"ˆr (s,a)= ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5752351097178683,"Q(s,a)>wˆr. Additionally, state-based successor features can be calculated
294"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5768025078369906,from the  ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5783699059561128,Q above as  ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5799373040752351,V (s)=P
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5815047021943573,a2A⇡(a|s) ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5830721003134797,"Q(s,a), making V ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5846394984326019,ˆr (s)= ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5862068965517241,"V (s)>wˆr.
295"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5877742946708464,"Given a set  Q of state-action successor feature functions and a set  V of state successor feature func-
296"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5893416927899686,"tions for various policies and given a reward function via wˆr, Q⇡⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5909090909090909,"ˆr (s,a)≥max Q2 Q [ ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5924764890282131,"Q(s,a)>wˆr]
297"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5940438871473355,and V ⇡⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5956112852664577,"ˆr
(s)≥max V 2 V [ ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5971786833855799,"V (s)>wˆr] [24], so we use these two maximizations as approximations of
298 Q⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.5987460815047022,"ˆr(s,a) and V ⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6003134796238244,"ˆr (s), respectively. In practice, to enable gradient-based optimization with current tools,
299"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6018808777429467,"the maximization in this expression is replaced with the softmax-weighted average, making the loss
300"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.603448275862069,function linear. Focusing ﬁrst on the approximation of V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6050156739811913,"ˆr (s), for each  V 2 V , a softmax weight is
301"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6065830721003135,calculated for  ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6081504702194357,V (s): softmax V ( ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.609717868338558,"V (s)>wˆr),[( ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6112852664576802,V (s)>wˆr)1/T ]/[(P 0
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6128526645768025,V 2 V  0⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6144200626959248,"V (s)>wˆr)1/T ],
302"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6159874608150471,where temperature T is a constant hyperparameter. The resulting approximation of V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6175548589341693,"ˆr (s) is there-
303"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6191222570532915,fore deﬁned as ˜V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6206896551724138,"ˆr (s) , P"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.622257053291536,V 2 V softmax V ( ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6238244514106583,V (s)>wˆr)[ ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6253918495297806,"V (s)>wˆr]. Similarly, to approxi-
304"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6269592476489029,mate Q⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6285266457680251,"ˆr(s,a), softmax Q ( ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6300940438871473,"Q(s,a)>wˆr) , [( ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6316614420062696,"Q(s,a)>wˆr)1/T ]/[(P 0"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6332288401253918,Q2  0⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6347962382445141,"Q (s,a)>wˆr)1/T ]
305"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6363636363636364,and ˜Q⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6379310344827587,"ˆr(s,a) , P"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6394984326018809,Q2 Q softmax Q ( ⇡
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6410658307210031,"Q(s,a)>wˆr)[ ⇡"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6426332288401254,"Q(s,a)>wˆr]. Consequently, from Eqns. 4
306"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6442006269592476,"Algorithm 1 Linear reward learning with regret preference model (Pregret), using successor features"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.64576802507837,"1: Input: a set of reward functions and a set of policies (where one set can be ?)
2:   ?
3: for each reward function rSF or policy ⇡SF in the input sets do
4:
if rSF then ⇡SF  estimate of optimal maximum-entropy policy for rSF
5:
estimate  ⇡SF"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6473354231974922,"Q
and  ⇡SF"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6489028213166145,"V
(if not estimated already during step 4)
6:
add  ⇡SF"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6504702194357367,"Q
to  Q
7:
add  ⇡SF"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6520376175548589,"V
to  V
8: end for
9: repeat
10:
optimize wˆr by loss of Eqn. 1, calculating ˜Pregret(σ1 ≻σ2|ˆr) via Eqn. 6, using  Q and  V
11: until stopping criteria are met
12: return wˆr"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6536050156739812,"and 5, the corresponding approximation ˜Pregret of the regret preference model is:
307"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6551724137931034,˜Pregret(σ1 ≻σ2|ˆr)=logistic
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6567398119122257,✓P|σ2|-1 t=0 h ˜V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.658307210031348,"ˆr (sσ2,t)−˜Q⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6598746081504702,"ˆr(sσ2,t,aσ2,t) i"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6614420062695925,−P|σ1|-1 t=0 h ˜V ⇤
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6630094043887147,"ˆr (sσ1,t)−˜Q⇤"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.664576802507837,"ˆr(sσ1,t,aσ1,t) i◆ (6)"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6661442006269592,"The algorithm
In Algorithm 1, lines 9–12 describe the supervised-learning optimization using
308"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6677115987460815,"the approximation ˜Pregret, and the prior lines create  Q and  V . Speciﬁcally, given a set of reward
309"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6692789968652038,"functions, a corresponding set of policies is created (line 4), where each policy is an estimate of the
310"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.670846394984326,"maximum entropy policy for a reward function. Standard policy improvement methods can be used to
311"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6724137931034483,"create each such policy. Alternatively, some or all of the set of policies can be given as input directly,
312"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6739811912225705,"not derived from input reward functions. For each such policy ⇡SF , successor feature functions  ⇡SF Q
313"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6755485893416928,and  ⇡SF
RESULTS FROM LEARNING REWARD FUNCTIONS,0.677115987460815,"V
are estimated (line 5), which by default would be performed by a minor extension of a
314"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6786833855799373,"standard policy evaluation algorithm as detailed by Barreto et al. [24]. Note that the reward function
315"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6802507836990596,"that is ultimately learned is not restricted to be in the input set of reward functions, which is used only
316"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6818181818181818,"to create an approximation of regret.
317"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6833855799373041,"The details of our instantiation of Algorithm 1 for the delivery domain can be found in Appendix F.1,
318"
RESULTS FROM LEARNING REWARD FUNCTIONS,0.6849529780564263,"along with guidance for extending it to reward functions that might be non-linear.
319"
RESULTS FROM SYNTHETIC PREFERENCES,0.6865203761755486,"6.2
Results from synthetic preferences
320"
RESULTS FROM SYNTHETIC PREFERENCES,0.6880877742946708,"Before considering human preferences, we ﬁrst ask how each preference model performs when it is
321"
RESULTS FROM SYNTHETIC PREFERENCES,0.6896551724137931,"correct. In other words, we investigate empirically how well the preference model could perform if
322"
RESULTS FROM SYNTHETIC PREFERENCES,0.6912225705329154,"humans perfectly adhered to it. Recall that the ground-truth reward function, r, is used to create these
323"
RESULTS FROM SYNTHETIC PREFERENCES,0.6927899686520376,"preferences but is inaccessible to the reward-learning algorithms.
324"
RESULTS FROM SYNTHETIC PREFERENCES,0.6943573667711599,"Figure 5: Performance comparison over 100 randomly
generated deterministic MDPs"
RESULTS FROM SYNTHETIC PREFERENCES,0.6959247648902821,"For these evaluations, either a stochastic or
325"
RESULTS FROM SYNTHETIC PREFERENCES,0.6974921630094044,"noiseless preference model acts a preference
326"
RESULTS FROM SYNTHETIC PREFERENCES,0.6990595611285266,"generator to create a preference dataset, and
327"
RESULTS FROM SYNTHETIC PREFERENCES,0.700626959247649,"then the stochastic version of the same model
328"
RESULTS FROM SYNTHETIC PREFERENCES,0.7021943573667712,"is used for reward learning. For the noiseless
329"
RESULTS FROM SYNTHETIC PREFERENCES,0.7037617554858934,"case, the deterministic preference generator com-
330"
RESULTS FROM SYNTHETIC PREFERENCES,0.7053291536050157,"pares a segment pair’s ⌃σr values for P⌃r or
331"
RESULTS FROM SYNTHETIC PREFERENCES,0.7068965517241379,"their regret(σ|r) values for Pregret. Note that
332"
RESULTS FROM SYNTHETIC PREFERENCES,0.7084639498432602,"through reward scaling the preference generators
333"
RESULTS FROM SYNTHETIC PREFERENCES,0.7100313479623824,"approach determinism in the limit, so this noise-
334"
RESULTS FROM SYNTHETIC PREFERENCES,0.7115987460815048,"less analysis examines minimal-entropy versions
335"
RESULTS FROM SYNTHETIC PREFERENCES,0.713166144200627,"of the two preference-generating models. (The opposite extreme, uniformly random preferences,
336"
RESULTS FROM SYNTHETIC PREFERENCES,0.7147335423197492,"would remove all information from preferences and therefore is not examined.) In the stochastic case,
337"
RESULTS FROM SYNTHETIC PREFERENCES,0.7163009404388715,"for each preference model, each segment pair is labeled by sampling from that preference generator’s
338"
RESULTS FROM SYNTHETIC PREFERENCES,0.7178683385579937,"output distribution (Eqs 2 or 5), using the unscaled ground-truth reward function.
339"
RESULTS FROM SYNTHETIC PREFERENCES,0.719435736677116,"We created 100 deterministic MDPs that instantiate variants of our delivery domain (see Section 4.1).
340"
RESULTS FROM SYNTHETIC PREFERENCES,0.7210031347962382,"To create each MDP, we sampled from sets of possible widths, heights, and reward component values,
341"
RESULTS FROM SYNTHETIC PREFERENCES,0.7225705329153606,"and the resultant grid cells were randomly populated with a destination, objects, and road surface types
342"
RESULTS FROM SYNTHETIC PREFERENCES,0.7241379310344828,"(see Appendix F.2 for details). Each segment in the preference datasets for each MDP was generated
343"
RESULTS FROM SYNTHETIC PREFERENCES,0.725705329153605,"by choosing a start state and three actions, all uniformly randomly. For a set number of preferences,
344"
RESULTS FROM SYNTHETIC PREFERENCES,0.7272727272727273,"each method had the same set of segment pairs in its preference dataset. Figure 5 shows the percentage
345"
RESULTS FROM SYNTHETIC PREFERENCES,0.7288401253918495,"of MDPs in which each preference model results in near-optimal performance. The regret preference
346"
RESULTS FROM SYNTHETIC PREFERENCES,0.7304075235109718,"model outperforms the partial return model at every dataset size, both with and without noise. By a
347"
RESULTS FROM SYNTHETIC PREFERENCES,0.731974921630094,"Wilcoxon paired signed-rank test on normalized mean returns, p<0.05 for 86% of these comparisons
348"
RESULTS FROM SYNTHETIC PREFERENCES,0.7335423197492164,"and p<0.01 for 57% of them, as reported in Appendix F.2.
349"
RESULTS FROM SYNTHETIC PREFERENCES,0.7351097178683386,"Further analyses can be found in Appendix F.2, including with stochastic transitions, with different
350"
RESULTS FROM SYNTHETIC PREFERENCES,0.7366771159874608,"segment lengths, and while artiﬁcially lowering the discount factor (as is common in deep RL and
351"
RESULTS FROM SYNTHETIC PREFERENCES,0.7382445141065831,"recent work on deep reward learning from preferences).
352"
RESULTS FROM HUMAN PREFERENCES,0.7398119122257053,"6.3
Results from human preferences
353"
RESULTS FROM HUMAN PREFERENCES,0.7413793103448276,"Figure 6: Performance comparison over various
amounts of human preferences. Each partition has
the number of preferences shown or one less."
RESULTS FROM HUMAN PREFERENCES,0.7429467084639498,"We randomly assign human preferences from our gath-
354"
RESULTS FROM HUMAN PREFERENCES,0.7445141065830722,"ered dataset to different numbers of same-sized parti-
355"
RESULTS FROM HUMAN PREFERENCES,0.7460815047021944,"tions, resulting in different training set sizes, and test
356"
RESULTS FROM HUMAN PREFERENCES,0.7476489028213166,"each preference model on each partition. Figure 6
357"
RESULTS FROM HUMAN PREFERENCES,0.7492163009404389,"shows the results. With smaller training sets (20–100
358"
RESULTS FROM HUMAN PREFERENCES,0.7507836990595611,"partitions), the regret preference model results in near-
359"
RESULTS FROM HUMAN PREFERENCES,0.7523510971786834,"optimal performance more often. With larger training
360"
RESULTS FROM HUMAN PREFERENCES,0.7539184952978056,"sets (1–10 partitions), both preference models always
361"
RESULTS FROM HUMAN PREFERENCES,0.7554858934169278,"reach near-optimal return, but the mean return from
362"
RESULTS FROM HUMAN PREFERENCES,0.7570532915360502,"the regret preference model is higher for all of these
363"
RESULTS FROM HUMAN PREFERENCES,0.7586206896551724,"partitions except for 3 partitions in the 10-partition
364"
RESULTS FROM HUMAN PREFERENCES,0.7601880877742947,"test. Applying a Wilcoxon paired signed-rank test on normalized mean return to each group with 5 or
365"
RESULTS FROM HUMAN PREFERENCES,0.7617554858934169,"more partitions, p<0.05 for all numbers of partitions except 100 and p<0.01 for 20 and 50 partitions.
366"
CONCLUSION,0.7633228840125392,"7
Conclusion
367"
CONCLUSION,0.7648902821316614,"Over numerous evaluations with human preferences, our proposed regret preference model (Pregret)
368"
CONCLUSION,0.7664576802507836,"shows improvements summarized below over the previous partial return preference model (P⌃r).
369"
CONCLUSION,0.768025078369906,"When each preference model generates the preferences for its own inﬁnite and exhaustive training set,
370"
CONCLUSION,0.7695924764890282,"we prove that Pregret identiﬁes the set of optimal policies, whereas P⌃r is not guaranteed to do so
371"
CONCLUSION,0.7711598746081505,"without preference noise that reveals the proportions of rewards with respect to each other. With ﬁnite
372"
CONCLUSION,0.7727272727272727,"training data of synthetic preferences, Pregret also empirically results in learned policies that tend to
373"
CONCLUSION,0.774294670846395,"outperform those resulting from P⌃r. This superior performance of Pregret is also seen with human
374"
CONCLUSION,0.7758620689655172,"preferences. In summary, our analyses suggest that regret preference models are more effective both
375"
CONCLUSION,0.7774294670846394,"descriptively with respect to human preferences and also normatively, as the model we want humans to
376"
CONCLUSION,0.7789968652037618,"follow if we had the choice.
377"
CONCLUSION,0.780564263322884,"Independent of Pregret, this paper also reveals that segments’ changes in state values provide informa-
378"
CONCLUSION,0.7821316614420063,"tion about human preferences that is not fully provided by partial return. More generally, we show that
379"
CONCLUSION,0.7836990595611285,"the choice of preference model impacts the performance of learned reward functions.
380"
CONCLUSION,0.7852664576802508,"This study motivates several new directions for research. Future work could address any of the
381"
CONCLUSION,0.786833855799373,"limitations detailed in Appendix A.1. Speciﬁcally, future work could further test the general superiority
382"
CONCLUSION,0.7884012539184952,"of Pregret or apply it to deep learning settings. Additionally, prescriptive methods could be developed
383"
CONCLUSION,0.7899686520376176,"via the user interface or elsewhere to nudge humans to conform more to Pregret or to other normatively
384"
CONCLUSION,0.7915360501567398,"appealing preference models. Lastly, subsequent efforts could seek preference models that are even
385"
CONCLUSION,0.7931034482758621,"more effective with preferences from actual humans, now that this work has provided conclusive
386"
CONCLUSION,0.7946708463949843,"evidence that the choice of preference model is impactful.
387"
REFERENCES,0.7962382445141066,"References
388"
REFERENCES,0.7978056426332288,"[1] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
389"
REFERENCES,0.799373040752351,"Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
390"
REFERENCES,0.8009404388714734,"with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
391"
REFERENCES,0.8025078369905956,"[2] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin,
392"
REFERENCES,0.8040752351097179,"Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using
393"
REFERENCES,0.8056426332288401,"potentials from deep learning. Nature, 577(7792):706–710, 2020.
394"
REFERENCES,0.8072100313479624,"[3] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
395"
REFERENCES,0.8087774294670846,"Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft
396"
REFERENCES,0.8103448275862069,"II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
397"
REFERENCES,0.8119122257053292,"[4] Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Subhodeep
398"
REFERENCES,0.8134796238244514,"Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using rein-
399"
REFERENCES,0.8150470219435737,"forcement learning. Nature, 588(7836):77–82, 2020.
400"
REFERENCES,0.8166144200626959,"[5] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy Dennison,
401"
REFERENCES,0.8181818181818182,"David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement
402"
REFERENCES,0.8197492163009404,"learning. arXiv preprint arXiv:1912.06680, 2019.
403"
REFERENCES,0.8213166144200627,"[6] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese,
404"
REFERENCES,0.822884012539185,"Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak
405"
REFERENCES,0.8244514106583072,"plasmas through deep reinforcement learning. Nature, 602(7897):414–419, 2022.
406"
REFERENCES,0.8260188087774295,"[7] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete
407"
REFERENCES,0.8275862068965517,"problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
408"
REFERENCES,0.829153605015674,"[8] W Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis)design
409"
REFERENCES,0.8307210031347962,"for autonomous driving. arXiv preprint arXiv:2104.13906, 2021.
410"
REFERENCES,0.8322884012539185,"[9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-
411"
REFERENCES,0.8338557993730408,"ment learning from human preferences. In Advances in Neural Information Processing Systems (NIPS),
412"
REFERENCES,0.835423197492163,"pages 4299–4307, 2017.
413"
REFERENCES,0.8369905956112853,"[10] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning
414"
REFERENCES,0.8385579937304075,"from human preferences and demonstrations in atari. arXiv preprint arXiv:1811.06521, 2018.
415"
REFERENCES,0.8401253918495298,"[11] Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael Laskin. Skill preferences:
416"
REFERENCES,0.841692789968652,"Learning to extract and execute robotic skills from human feedback. In Conference on Robot Learning,
417"
REFERENCES,0.8432601880877743,"pages 1259–1268. PMLR, 2022.
418"
REFERENCES,0.8448275862068966,"[12] Erdem Bıyık, Dylan P Losey, Malayandi Palan, Nicholas C Landolﬁ, Gleb Shevchuk, and Dorsa Sadigh.
419"
REFERENCES,0.8463949843260188,"Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations
420"
REFERENCES,0.8479623824451411,"and preferences. The International Journal of Robotics Research, page 02783649211041652, 2021.
421"
REFERENCES,0.8495297805642633,"[13] Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learning of
422"
REFERENCES,0.8510971786833855,"reward functions. Robotics: Science and Systems, 2017.
423"
REFERENCES,0.8526645768025078,"[14] Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efﬁcient interactive reinforcement learning
424"
REFERENCES,0.85423197492163,"via relabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091, 2021.
425"
REFERENCES,0.8557993730407524,"[15] Kimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-based
426"
REFERENCES,0.8573667711598746,"reinforcement learning. arXiv preprint arXiv:2111.03026, 2021.
427"
REFERENCES,0.8589341692789969,"[16] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,
428"
REFERENCES,0.8605015673981191,"Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
429"
REFERENCES,0.8620689655172413,"human feedback. arXiv preprint arXiv:2203.02155, 2022.
430"
REFERENCES,0.8636363636363636,"[17] R Duncan Luce. Individual choice behavior: A theoretical analysis. John Wiley, 1959.
431"
REFERENCES,0.8652037617554859,"[18] Roger N Shepard. Stimulus and response generalization: A stochastic model relating generalization to
432"
REFERENCES,0.8667711598746082,"distance in psychological space. Psychometrika, 22(4):325–345, 1957.
433"
REFERENCES,0.8683385579937304,"[19] A.Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Seventeenth International
434"
REFERENCES,0.8699059561128527,"Conference on Machine Learning (ICML), 2000.
435"
REFERENCES,0.8714733542319749,"[20] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
436"
REFERENCES,0.8730407523510971,"reinforcement learning. In Twenty-third AAAI Conference on Artiﬁcial Intelligence, volume 8, pages
437"
REFERENCES,0.8746081504702194,"1433–1438, 2008.
438"
REFERENCES,0.8761755485893417,"[21] Dylan Hadﬁeld-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward
439"
REFERENCES,0.877742946708464,"design. In Advances in Neural Information Processing Systems (NIPS), pages 6765–6774, 2017.
440"
REFERENCES,0.8793103448275862,"[22] Stuart Russell and Peter Norvig. Artiﬁcial intelligence: a modern approach. 2020.
441"
REFERENCES,0.8808777429467085,"[23] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
442"
REFERENCES,0.8824451410658307,"[24] André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado Van Hasselt, and David
443"
REFERENCES,0.8840125391849529,"Silver. Successor features for transfer in reinforcement learning. arXiv preprint arXiv:1606.05312, 2016.
444"
REFERENCES,0.8855799373040752,"[25] Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Joint European
445"
REFERENCES,0.8871473354231975,"Conference on Machine Learning and Knowledge Discovery in Databases, pages 12–27. Springer, 2011.
446"
REFERENCES,0.8887147335423198,"[26] Daniel Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum. Safe imitation learning via fast
447"
REFERENCES,0.890282131661442,"bayesian reward inference from preferences. In International Conference on Machine Learning, pages
448"
REFERENCES,0.8918495297805643,"1165–1177. PMLR, 2020.
449"
REFERENCES,0.8934169278996865,"[27] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
450"
REFERENCES,0.8949843260188087,"of the twenty-ﬁrst international conference on Machine learning, page 1, 2004.
451"
REFERENCES,0.896551724137931,"[28] Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon. Reward identiﬁcation in inverse
452"
REFERENCES,0.8981191222570533,"reinforcement learning. In International Conference on Machine Learning, pages 5496–5505. PMLR, 2021.
453"
REFERENCES,0.8996865203761756,"[29] Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and
454"
REFERENCES,0.9012539184952978,"progress. Artiﬁcial Intelligence, 297:103500, 2021.
455"
REFERENCES,0.9028213166144201,"[30] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. Princeton university
456"
REFERENCES,0.9043887147335423,"press, 1944.
457"
REFERENCES,0.9059561128526645,"[31] Yuchen Cui, Qiping Zhang, Alessandro Allievi, Peter Stone, Scott Niekum, and W Bradley Knox. The
458"
REFERENCES,0.9075235109717869,"empathic framework for task learning from implicit human feedback. arXiv preprint arXiv:2009.13649,
459"
REFERENCES,0.9090909090909091,"2020.
460"
REFERENCES,0.9106583072100314,"[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
461"
REFERENCES,0.9122257053291536,"Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
462"
REFERENCES,0.9137931034482759,"DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
463"
REFERENCES,0.9153605015673981,"Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach,
464"
REFERENCES,0.9169278996865203,"H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
465"
REFERENCES,0.9184952978056427,"Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.
466"
REFERENCES,0.9200626959247649,"[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
467"
REFERENCES,0.9216300940438872,"R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
468"
REFERENCES,0.9231974921630094,"Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
469"
REFERENCES,0.9247648902821317,"Checklist
470"
REFERENCES,0.9263322884012539,"1. For all authors...
471"
REFERENCES,0.9278996865203761,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
472"
REFERENCES,0.9294670846394985,"contributions and scope? [Yes]
473"
REFERENCES,0.9310344827586207,"(b) Did you describe the limitations of your work? [Yes] See Appendix A.1.
474"
REFERENCES,0.932601880877743,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
475"
REFERENCES,0.9341692789968652,"Appendix A.2.
476"
REFERENCES,0.9357366771159875,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
477"
REFERENCES,0.9373040752351097,"them? [Yes]
478"
REFERENCES,0.9388714733542319,"2. If you are including theoretical results...
479"
REFERENCES,0.9404388714733543,"(a) Did you state the full set of assumptions of all theoretical results? [Yes] Sections 3 and C
480"
REFERENCES,0.9420062695924765,"include all assumptions.
481"
REFERENCES,0.9435736677115988,"(b) Did you include complete proofs of all theoretical results? [Yes] See Section 3 and
482"
REFERENCES,0.945141065830721,"Appendix C.
483"
REFERENCES,0.9467084639498433,"3. If you ran experiments...
484"
REFERENCES,0.9482758620689655,"(a) Did you include the code, data, and instructions needed to reproduce the main exper-
485"
REFERENCES,0.9498432601880877,"imental results (either in the supplemental material or as a URL)? [No] However, the
486"
REFERENCES,0.95141065830721,"learning code, the code for running experiments, the code and UI elements for gathering
487"
REFERENCES,0.9529780564263323,"human preferences on Mechanical Turk, and the anonymized human preferences data
488"
REFERENCES,0.9545454545454546,"will be opened. We are particularly excited to provide the ﬁrst open dataset of human
489"
REFERENCES,0.9561128526645768,"preferences over pairs of trajectory segments.
490"
REFERENCES,0.957680250783699,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were
491"
REFERENCES,0.9592476489028213,"chosen)? [Yes] Appendix F.1
492"
REFERENCES,0.9608150470219435,"(c) Did you report error bars (e.g., with respect to the random seed after running experiments
493"
REFERENCES,0.9623824451410659,"multiple times)? [Yes] Error bars do not seem applicable to our plots, which do not show
494"
REFERENCES,0.9639498432601881,"the exact data that we do statistical testing on. However, statistical signiﬁcance testing
495"
REFERENCES,0.9655172413793104,"was reported, in Sections 5.1 and 6.2 (with a pointer to the appendix for details).
496"
REFERENCES,0.9670846394984326,"(d) Did you include the total amount of compute and the type of resources used (e.g., type of
497"
REFERENCES,0.9686520376175548,"GPUs, internal cluster, or cloud provider)? [Yes] See Appendix F.1.
498"
REFERENCES,0.9702194357366771,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
499"
REFERENCES,0.9717868338557993,"(a) If your work uses existing assets, did you cite the creators? [Yes] Appendix D does so
500"
REFERENCES,0.9733542319749217,"for visual assets used to visualize the delivery task.
501"
REFERENCES,0.9749216300940439,"(b) Did you mention the license of the assets? [Yes] Appendix D mentions the license for
502"
REFERENCES,0.9764890282131662,"visual assets used to visualize the delivery task.
503"
REFERENCES,0.9780564263322884,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
504"
REFERENCES,0.9796238244514106,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
505"
REFERENCES,0.9811912225705329,"using/curating? [Yes] See Appendix D.
506"
REFERENCES,0.9827586206896551,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
507"
REFERENCES,0.9843260188087775,"information or offensive content? [Yes] See Appendix D
508"
REFERENCES,0.9858934169278997,"5. If you used crowdsourcing or conducted research with human subjects...
509"
REFERENCES,0.987460815047022,"(a) Did you include the full text of instructions given to participants and screenshots, if
510"
REFERENCES,0.9890282131661442,"applicable? [Yes] Section 4.1.1 includes a link to a video of a full experimental session
511"
REFERENCES,0.9905956112852664,"(with an author acting as the subject).
512"
REFERENCES,0.9921630094043887,"(b) Did you describe any potential participant risks, with links to Institutional Review Board
513"
REFERENCES,0.9937304075235109,"(IRB) approvals, if applicable? [Yes] We discuss participant risks from our crowdsourced
514"
REFERENCES,0.9952978056426333,"study and provide a link to the IRB approval in Appendix D.
515"
REFERENCES,0.9968652037617555,"(c) Did you include the estimated hourly wage paid to participants and the total amount
516"
REFERENCES,0.9984326018808778,"spent on participant compensation? [Yes] See Appendix D.
517"
