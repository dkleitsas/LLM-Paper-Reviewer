Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011273957158962795,"A classifier is, in its essence, a function which takes an input and returns the class
1"
ABSTRACT,0.002254791431792559,"of the input and implicitly assumes an underlying distribution. We argue in this
2"
ABSTRACT,0.0033821871476888386,"article that one has to move away from this basic tenet to obtain generalization
3"
ABSTRACT,0.004509582863585118,"across distributions. Specifically, the class of the sample should depend on the
4"
ABSTRACT,0.005636978579481398,"points from its “context distribution” for better generalization across distributions.
5"
ABSTRACT,0.006764374295377677,"How does one achieve this? – The key idea is to “adapt” the outputs of each neuron
6"
ABSTRACT,0.007891770011273957,"of the network to its context distribution. We propose quantile activation,QACT,
7"
ABSTRACT,0.009019165727170236,"which, in simple terms, outputs the relative quantile of the sample in its context
8"
ABSTRACT,0.010146561443066516,"distribution, instead of the actual values in traditional networks.
9"
ABSTRACT,0.011273957158962795,"The scope of this article is to validate the proposed activation across several experi-
10"
ABSTRACT,0.012401352874859075,"mental settings, and compare it with conventional techniques. For this, we use the
11"
ABSTRACT,0.013528748590755355,"datasets developed to test robustness against distortions – CIFAR10C, CIFAR100C,
12"
ABSTRACT,0.014656144306651634,"MNISTC, TinyImagenetC, and show that we achieve a significantly higher gen-
13"
ABSTRACT,0.015783540022547914,"eralization across distortions than the conventional classifiers, across different
14"
ABSTRACT,0.016910935738444193,"architectures. Although this paper is only a proof of concept, we surprisingly find
15"
ABSTRACT,0.018038331454340473,"that this approach outperforms DINOv2(small) at large distortions, even though
16"
ABSTRACT,0.019165727170236752,"DINOv2 is trained with a far bigger network on a considerably larger dataset.
17"
INTRODUCTION,0.020293122886133032,"1
Introduction
18"
INTRODUCTION,0.02142051860202931,"Deep learning approaches have significantly influenced image classification tasks on the machine
19"
INTRODUCTION,0.02254791431792559,"learning pipelines over the past decade. They can easily beat human performance on such tasks by non-
20"
INTRODUCTION,0.02367531003382187,"trivial margins by using innovative ideas such as Batch Normalization [19] and other normalization
21"
INTRODUCTION,0.02480270574971815,"techniques [3, 31], novel rectifiers such as ReLU/PReLU [26, 31, 3] and by using large datasets and
22"
INTRODUCTION,0.02593010146561443,"large models.
23"
INTRODUCTION,0.02705749718151071,"However, these classification systems do not generalize across distributions [2, 34], which leads to
24"
INTRODUCTION,0.02818489289740699,"instability when used in practice. [22] shows that deep networks with ReLU activation degrades in
25"
INTRODUCTION,0.029312288613303268,"performance under distortions. [4] observes that there exists a feature collapse which inhibits the
26"
INTRODUCTION,0.030439684329199548,"networks to be reliable.
27"
INTRODUCTION,0.03156708004509583,"Fundamental Problem of Classification:
We trace the source of the problem to the fact that – Ex-
28"
INTRODUCTION,0.03269447576099211,"isting classification pipelines necessitates single point prediction, i.e., they should allow classification
29"
INTRODUCTION,0.033821871476888386,"of a single sample given in isolation. We argue that, for good generalization, one should move away
30"
INTRODUCTION,0.03494926719278467,"from this basic tenet and instead allow the class to be dependent on the “context distribution” of the
31"
INTRODUCTION,0.036076662908680945,"sample. That is, when performing classification, one needs to know both the sample and the context
32"
INTRODUCTION,0.03720405862457723,"of the sample for classification.
33"
INTRODUCTION,0.038331454340473504,"While this is novel in the context of classification systems, it is widely prevalent in the specific domain
34"
INTRODUCTION,0.03945885005636979,"of Natural Language Processing (NLP) – The meaning of a word is dependent on the context of the
35"
INTRODUCTION,0.040586245772266064,"Severity 0
Severity 1
Severity 2
Severity 3
Severity 4
Severity 5 QACT"
INTRODUCTION,0.04171364148816235,"Accuracy
88.02
82.44
76.74
71.54
68.25
65.34 ReLU"
INTRODUCTION,0.04284103720405862,"Accuracy
90.48
81.06
65.9
48.34
41.54
35.86"
INTRODUCTION,0.043968432919954906,"Figure 1: Comparing TSNE plots of QACT and ReLU activation on CIFAR10C with Gaussian
distortions. Observe that QACT maintains the class structure extremely well across distortions, while
the usual ReLU activations loses the class structure as severity increases."
INTRODUCTION,0.04509582863585118,"word. However, to our knowledge, this has not been considered for general classification systems.
36"
INTRODUCTION,0.046223224351747465,"Even when using dominant NLP architectures such as Transformers for vision [8], the technique has
37"
INTRODUCTION,0.04735062006764374,"been to split the image into patches and then obtain the embedding for an individual image.
38"
INTRODUCTION,0.048478015783540024,"Obtaining a classification framework for incorporating context distribution:
We suspect that
39"
INTRODUCTION,0.0496054114994363,"the main reason why the context distribution is not incorporated into the classification system is
40"
INTRODUCTION,0.05073280721533258,"– The naive approach of considering a lot of samples in the pipeline to classify a single sample
41"
INTRODUCTION,0.05186020293122886,"is computationally expensive. We solve this problem by considering the context distribution of
42"
INTRODUCTION,0.05298759864712514,"each neuron specifically. We introduce Quantile Activation (QACT), which outputs a probability
43"
INTRODUCTION,0.05411499436302142,"depending upon the context of all outputs. This, however, gives rise to new challenges in training,
44"
INTRODUCTION,0.0552423900789177,"which we address in section 2.
45"
INTRODUCTION,0.05636978579481398,"Figure 1 illustrates the differences of the proposed framework with the existing framework. As
46"
INTRODUCTION,0.05749718151071026,"severity increases (w.r.t Gaussian Noise), we observe that ReLU activation loses the class structure.
47"
INTRODUCTION,0.058624577226606536,"This behaviour can be attributed to the fact that, as the input distribution changes, the activations
48"
INTRODUCTION,0.05975197294250282,"either increase/decrease, and due to the multiplicative effect of numerous layers, this leads to very
49"
INTRODUCTION,0.060879368658399095,"different features. On the other hand, the proposed QACT framework does not suffer from this, since
50"
INTRODUCTION,0.06200676437429538,"if all the pre-activations1 change in a systematic way, the quantiles adjust automatically to ensure that
51"
INTRODUCTION,0.06313416009019165,"the inputs for the next layer does not change by much. This is reflected in the fact that class structure
52"
INTRODUCTION,0.06426155580608793,"is preserved with QACT.
53"
INTRODUCTION,0.06538895152198422,"Remark:
Quantile activation is different from existing quantile neural network based approaches,
54"
INTRODUCTION,0.0665163472378805,"such as regression [30], binary quantile classification [36], Anomaly Detection [24, 33]. Our approach
55"
INTRODUCTION,0.06764374295377677,"is achieving best in-class performance by incorporating context distribution in the classification
56"
INTRODUCTION,0.06877113866967305,"paradigm. Our approach is also markedly different from Machine unlearning which is based on
57"
INTRODUCTION,0.06989853438556934,"selective forgetting of certain data points or retraining from scratch [32].
58"
INTRODUCTION,0.07102593010146561,"Contributions:
A decent amount of literature on neuronal activation is available. However, to the
59"
INTRODUCTION,0.07215332581736189,"best of our knowledge, none matches the central idea proposed in this work.
60"
INTRODUCTION,0.07328072153325817,"In [5], the authors propose an approach to calibrate a pre-trained classifier fθ(x) by extending it
61"
INTRODUCTION,0.07440811724915446,"to learn a quantile function, Q(x, θ, τ) (τ denotes the quantile), and then estimate the probabilities
62"
INTRODUCTION,0.07553551296505073,"using
R"
INTRODUCTION,0.07666290868094701,"τ I[Q(x, θ, τ) ≥0.5]dτ 2. They show that this results in probabilities which are robust to
63"
INTRODUCTION,0.0777903043968433,"distortions.
64"
INTRODUCTION,0.07891770011273957,"1. In this article, we extend this approach to the level of a neuron, by suitably deriving the
65"
INTRODUCTION,0.08004509582863585,"forward and backward propagation equations required for learning (section 2).
66"
WE THEN SHOW THAT A SUITABLE INCORPORATION OF OUR EXTENSION PRODUCES CONTEXT DEPENDENT,0.08117249154453213,"2. We then show that a suitable incorporation of our extension produces context dependent
67"
WE THEN SHOW THAT A SUITABLE INCORPORATION OF OUR EXTENSION PRODUCES CONTEXT DEPENDENT,0.08229988726042842,"outputs at the level of each neuron of the neural network.
68"
WE THEN SHOW THAT A SUITABLE INCORPORATION OF OUR EXTENSION PRODUCES CONTEXT DEPENDENT,0.0834272829763247,"1We use the following convention – “Pre-activations” denote the inputs to the activation functions and
“Activations” denote the outputs of the activation function.
2I[] denotes the indicator function"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.08455467869222097,"3. Our approach contributes to achieving better generalization across distributions and is
69"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.08568207440811725,"more robust to distortions, across architectures. We evaluate our method using different
70"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.08680947012401354,"architectures and datasets, and compare with the current state-of-the-art – DINOv2. We
71"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.08793686583990981,"show that QACT proposed here is more robust to distortions than DINOv2, even if we
72"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.08906426155580609,"have considerably less number of parameters (22M for DINOv2 vs 11M for Resnet18).
73"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09019165727170236,"Additionally, DINOv2 is trained on 20 odd datasets, before being applied on CIFAR10C; in
74"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09131905298759865,"contrast, our framework is trained on CIFAR10, and produces more robust outcome (see
75"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09244644870349493,"figures 3,5).
76"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.0935738444193912,"4. The proposed QACT is consistent with all the existing techniques used in DINOv2, and
77"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09470124013528748,"hence can be easily incorporated into any ML framework.
78"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09582863585118377,"5. We also adapt QACT to design a classifier which returns better calibrated probabilities.
79"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09695603156708005,"Related Works on Domain Generalization (DG):
The problem of domain generalizations tries
80"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.09808342728297632,"to answer the question – Can we use a classifier trained on one domain across several other related
81"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.0992108229988726,"domains? The earliest known approach for this is Transfer Learning [28, 37], where a classifier
82"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10033821871476889,"from a single domain is applied to a different domain with/without fine-tuning. Several approaches
83"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10146561443066517,"have been proposed to achieve DG, such as extracting domain-invariant features over single/multiple
84"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10259301014656144,"source domains [11, 1, 9, 29, 16], Meta Learning [17, 9], Invariant Risk Minimization [2]. Self
85"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10372040586245772,"supervised learning is another proposed approach which tries to extract features on large scale datasets
86"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10484780157835401,"in an unsupervised manner, the most recent among them being DINOv2 [27]. Very large foundation
87"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10597519729425028,"models, such as GPT-4V, are also known to perform better with respect to distribution shifts [12].
88"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10710259301014656,"Nevertheless, to the best of our knowledge, none of these models incorporates context distributions
89"
OUR APPROACH CONTRIBUTES TO ACHIEVING BETTER GENERALIZATION ACROSS DISTRIBUTIONS AND IS,0.10822998872604284,"for classification.
90"
QUANTILE ACTIVATION,0.10935738444193913,"2
Quantile Activation
91"
QUANTILE ACTIVATION,0.1104847801578354,"Rethinking Outputs from a Neuron:
To recall – if x denotes the input, a typical neuron does the
92"
QUANTILE ACTIVATION,0.11161217587373168,"following – (i) Applies a linear transformation with parameters w, b, giving wtx + b as the output,
93"
QUANTILE ACTIVATION,0.11273957158962795,"and (ii) applies a rectifier g, returning g(wtx + b). Typically, g is taken to be the ReLU activation -
94"
QUANTILE ACTIVATION,0.11386696730552424,"grelu(x) = max(0, x). Intuitively, we expect that each neuron captures an “abstract” feature, usually
95"
QUANTILE ACTIVATION,0.11499436302142052,"not understood by a human observer.
96"
QUANTILE ACTIVATION,0.1161217587373168,"An alternate way to model a neuron is to consider it as predicting a latent variable y, where y = 1
97"
QUANTILE ACTIVATION,0.11724915445321307,"if the feature is present and y = 0 if the feature is absent. Mathematically, we have the following
98"
QUANTILE ACTIVATION,0.11837655016910936,"model:
99"
QUANTILE ACTIVATION,0.11950394588500564,"z = wtx + b + ϵ
and
y = I[z ≥0]
(1)"
QUANTILE ACTIVATION,0.12063134160090191,"This is very similar to the standard latent variable model for logistic regression, with the main
100"
QUANTILE ACTIVATION,0.12175873731679819,"exception being, the outputs y are not known for each neuron beforehand. If y is known, it is rather
101"
QUANTILE ACTIVATION,0.12288613303269448,"easy to obtain the probabilities – P(z ≥0). Can we still predict the probabilities, even when y itself
102"
QUANTILE ACTIVATION,0.12401352874859076,"is a latent variable?
103"
QUANTILE ACTIVATION,0.12514092446448705,"The authors in [5] propose the following algorithm to estimate the probabilities:
104"
QUANTILE ACTIVATION,0.1262683201803833,"1. Let {xi} denote the set of input samples from the input distribution x and {zi} denote their
105"
QUANTILE ACTIVATION,0.1273957158962796,"corresponding latent outputs, which would be from the distribution z
106"
QUANTILE ACTIVATION,0.12852311161217586,"2. Assign y = 1 whenever z > (1 −τ)th quantile of z, and 0 otherwise. For a specific sample,
107"
QUANTILE ACTIVATION,0.12965050732807215,"we have yi = 1 if zi > (1 −τ)th quantile of {zi}
108"
QUANTILE ACTIVATION,0.13077790304396844,"3. Fit the model Q(x, τ; θ) to the dataset {((xi, τ), yi)}, and estimate the probability as,
109"
QUANTILE ACTIVATION,0.1319052987598647,"P(yi = 1) =
Z 1"
QUANTILE ACTIVATION,0.133032694475761,"τ=0
I[Q(x, τ; θ) ≥0.5]dτ
(2)"
QUANTILE ACTIVATION,0.13416009019165728,"The key idea:
Observe that in step 2., the labelling is done without resorting to actual ground-
110"
QUANTILE ACTIVATION,0.13528748590755355,"truth labels. This allows us to obtain the probabilities on the fly for any set of parameters, only by
111"
QUANTILE ACTIVATION,0.13641488162344984,"considering the quantiles of z.
112"
QUANTILE ACTIVATION,0.1375422773393461,"Defining the Quantile Activation QACT
Let z denote the pre-activation of the neuron, and let
113"
QUANTILE ACTIVATION,0.1386696730552424,"{zi} denote the samples from this distribution. Let Fz denote the cumulative distribution function
114"
QUANTILE ACTIVATION,0.13979706877113868,"(CDF), and let fz denote the density of the distribution. Accordingly, we have that F −1
z
(τ) denotes
115"
QUANTILE ACTIVATION,0.14092446448703494,"Algorithm 1 Forward Propagation for a single neuron
Input: [zi] a vector of pre-activations, 0 < τ1 < τ2 < · · · < τnτ < 1 - a list of quantile indices at
which we compute the quantiles."
QUANTILE ACTIVATION,0.14205186020293123,"Append two large values, c and −c, to the vector [zi].
Count n+ = number of positive values, n−= number of negative values, and assign the weight
w+ = 1/n+ to the positive values, and w−= 1/n−to the negative values.
Compute weighted quantiles {qi} at each of {τi} over the set {zi} ∪{c, −c}
Compute QACT(zi) using the function,"
QUANTILE ACTIVATION,0.14317925591882752,QACT(x) = 1 nτ X
QUANTILE ACTIVATION,0.14430665163472378,"i
I[x ≥qi]
(5)"
QUANTILE ACTIVATION,0.14543404735062007,"Remember [zi], w+, w−, [QACT(zi)] for backward propagation.
return [QACT(zi)]"
QUANTILE ACTIVATION,0.14656144306651633,"Algorithm 2 Backward Propagation for a single neuron
Input: grad_output, 0 < τ1 < τ2 < · · · < τnτ < 1 - a list of quantile indices at which we compute
the quantiles.
Context from Forward Propagation: [zi], w+, w−, [QACT(zi)]"
QUANTILE ACTIVATION,0.14768883878241262,"Obtain a weighted sample from [zi] with weights w+, w−– (say) S.
Obtain a kernel density estimate, using points from S, at each of the points in zi – (say) ˆfz(zi)
Set,
grad_input = grad_output ⊙[ ˆfz(zi)]
(6)"
QUANTILE ACTIVATION,0.1488162344983089,return grad_input
QUANTILE ACTIVATION,0.14994363021420518,"the τ th quantile of z. Using step (2) of the algorithm above, we define,
116"
QUANTILE ACTIVATION,0.15107102593010147,"QACT(z) =
Z 1"
QUANTILE ACTIVATION,0.15219842164599776,"τ=0
I[z > F −1
z
(1 −τ)]dτ
Substitute
=
τ→(1−τ) Z 1"
QUANTILE ACTIVATION,0.15332581736189402,"τ=0
I[z > F −1
z
(τ)]dτ
(3)"
QUANTILE ACTIVATION,0.1544532130777903,"Computing the gradient of QACT:
However, to use QACT in a neural network, we need to
117"
QUANTILE ACTIVATION,0.1555806087936866,"compute the gradient which is required for back-propagation. Let τz denote the quantile at which
118"
QUANTILE ACTIVATION,0.15670800450958286,"F −1
z
(τz) = z. Then we have that QACT(z) = τz since F −1
z
(τ) is an increasing function. So, we
119"
QUANTILE ACTIVATION,0.15783540022547915,"have that QACT(F −1
z
(τ)) = τ. In other words, we have that QACT(z) is Fz(z), which is nothing
120"
QUANTILE ACTIVATION,0.1589627959413754,"but the CDF of z. Hence, we have,
121"
QUANTILE ACTIVATION,0.1600901916572717,∂QACT(z)
QUANTILE ACTIVATION,0.161217587373168,"∂z
= fz(z)
(4)"
QUANTILE ACTIVATION,0.16234498308906425,"where fz(z) denotes the density of the distribution.
122"
QUANTILE ACTIVATION,0.16347237880496054,"Grounding the Neurons:
With the above formulation, observe that since QACT is identical to
123"
QUANTILE ACTIVATION,0.16459977452085683,"CDF, it follows that, QACT(z) is always a uniform distribution between 0 and 1, irrespective of the
124"
QUANTILE ACTIVATION,0.1657271702367531,"distribution z. When training numerous neurons in a layer, this could cause all the neurons to learn
125"
QUANTILE ACTIVATION,0.1668545659526494,"the same behaviour. Specifically, if, half the time, a particular abstract feature is more prevalent
126"
QUANTILE ACTIVATION,0.16798196166854565,"than others, QACT (as presented above) would not be able to learn this feature. To correct this, we
127"
QUANTILE ACTIVATION,0.16910935738444194,"enforce that positive values and negative values have equal weight. Given the input distribution z,
128"
QUANTILE ACTIVATION,0.17023675310033823,"We perform the following transformation before applying QACT. Let
129"
QUANTILE ACTIVATION,0.1713641488162345,"z+ =
z
if z ≥0
0
otherwise
z−=
z
if z < 0
0
otherwise
(7)"
QUANTILE ACTIVATION,0.17249154453213078,"denote the truncated distributions. Then,
130"
QUANTILE ACTIVATION,0.17361894024802707,"z‡ =
z+
with probability 0.5
z−
with probability 0.5
(8)"
QUANTILE ACTIVATION,0.17474633596392333,"From definition of z‡, we get that the median of z‡ is 0. This grounds the input distribution to have
131"
QUANTILE ACTIVATION,0.17587373167981962,"the same positive and negative weight.
132"
QUANTILE ACTIVATION,0.17700112739571588,"(a)
(b) (c) (d)"
QUANTILE ACTIVATION,0.17812852311161217,"Figure 2: Intuition behind quantile activation. (a) shows a simple toy distribution of points (blue), it’s
distortion (orange) and a simple line (red) on which the samples are projected to obtain activations.
(b) shows the distribution of the pre-activations. (c) shows the distributions of the activations with
QACT of the original distribution (blue). (d) shows the distributions of the activations with QACT
under the distorted distribution (orange). Observe that the distributions match perfectly under small
distortions. Note that even if the distribution matches perfectly, the quantile activation is actually a
deterministic function."
QUANTILE ACTIVATION,0.17925591882750846,"Dealing with corner cases:
It is possible that during training, some neurons either only get positive
133"
QUANTILE ACTIVATION,0.18038331454340473,"values or only get negative values. However, for smooth outputs, one should still only give the weight
134"
QUANTILE ACTIVATION,0.18151071025930102,"of 0.5 for positive values. To handle this, we include two values c (large positive) and −c (large
135"
QUANTILE ACTIVATION,0.1826381059751973,"negative) for each neuron. Since, the quantiles are conventionally computed using linear interpolation,
136"
QUANTILE ACTIVATION,0.18376550169109357,"this allows the outputs to vary smoothly. We take c = 100 in this article.
137"
QUANTILE ACTIVATION,0.18489289740698986,"Estimating the Density for Back-Propagation:
Note that the gradient for the back propagation is
138"
QUANTILE ACTIVATION,0.18602029312288612,"given by the density of z‡ (weighted distribution). We use the Kernel Density Estimation (KDE), to
139"
QUANTILE ACTIVATION,0.1871476888387824,"estimate the density. We, (i) First sample N points with weights w+, w−, and (ii) then estimate the
140"
QUANTILE ACTIVATION,0.1882750845546787,"density at all the input points [zi]. This is point-wise multiplied with the backward gradient to get the
141"
QUANTILE ACTIVATION,0.18940248027057496,"gradient for the input. In this article we use N = 1000, which we observe gets reasonable estimates.
142"
QUANTILE ACTIVATION,0.19052987598647125,"Computational Complexity:
Computational Complexity (for a single neuron) is majorly decided
143"
QUANTILE ACTIVATION,0.19165727170236754,"by 2 functions – (i) Computing the quantiles has the complexity for a vector [zi] of size n can be
144"
QUANTILE ACTIVATION,0.1927846674182638,"performed in O(n log(n)). Since this is log-linear in n, it does not increase the complexity drastically
145"
QUANTILE ACTIVATION,0.1939120631341601,"compared to other operations in a deep neural network. (ii) Computational complexity of the KDE
146"
QUANTILE ACTIVATION,0.19503945885005636,"estimates is O(Snτ) where S is the size of sample (weighted sample from [zi]) and nτ is the number
147"
QUANTILE ACTIVATION,0.19616685456595265,"of quantiles, giving a total of O(n + Snτ). In practice, we consider S = 1000 and nτ = 100 which
148"
QUANTILE ACTIVATION,0.19729425028184894,"works well, and hence does not increase with the batch size. This too scales linearly with batch size
149"
QUANTILE ACTIVATION,0.1984216459977452,"n, and hence does not drastically increase the complexity.
150"
QUANTILE ACTIVATION,0.1995490417136415,"Remark:
Algorithms 1, and 2 provide the pseudocode for the quantile activation. For stable
151"
QUANTILE ACTIVATION,0.20067643742953778,"training, in practice, we prepend and append the quantile activation with BatchNorm layers.
152"
QUANTILE ACTIVATION,0.20180383314543404,"Why QACT is robust to distortions?
To understand the idea behind quantile activation, consider a
153"
QUANTILE ACTIVATION,0.20293122886133033,"simple toy example in figure 2. For ease of visualization, assume that the input features (blue) are in 2
154"
QUANTILE ACTIVATION,0.2040586245772266,"dimensions, and also assume that the line of the linear projection is given by the red line in figure 2a.
155"
QUANTILE ACTIVATION,0.20518602029312288,"Now, assume that the blue input features are rotated, leading to a different distribution (indicated here
156"
QUANTILE ACTIVATION,0.20631341600901917,"by orange). Since activations are essentially (unnormalized) signed distances from the line, we plot
157"
QUANTILE ACTIVATION,0.20744081172491544,"the histograms corresponding to the two distributions in figure 2b. As expected, these distributions
158"
QUANTILE ACTIVATION,0.20856820744081173,"are different. However, after performing the quantile activation in equation 3, we have that both are
159"
QUANTILE ACTIVATION,0.20969560315670802,"uniform distribution. This is illustrated in figures 2c and 2d. This behaviour has a normalizing effect
160"
QUANTILE ACTIVATION,0.21082299887260428,"across different distributions, and hence has better distribution generalization than other activations.
161"
TRAINING WITH QACT,0.21195039458850057,"3
Training with QACT
162"
TRAINING WITH QACT,0.21307779030439683,"In the previous section, we described the procedure to adapt a single neuron to its context distribution.
163"
TRAINING WITH QACT,0.21420518602029312,"In this section we discuss how this extends to the Dense/Convolution layers, the loss functions to
164"
TRAINING WITH QACT,0.2153325817361894,"train the network and the inference aspect.
165"
TRAINING WITH QACT,0.21645997745208567,"Extending to standard layers:
The extension of equation 3 to dense outputs is straightforward.
166"
TRAINING WITH QACT,0.21758737316798196,"A typical output of the dense layer would be of the shape (B, Nc) - B denotes the batch size, Nc
167"
TRAINING WITH QACT,0.21871476888387825,"denotes the width of the network. The principle is - The context distribution of a neuron is all the
168"
TRAINING WITH QACT,0.21984216459977451,"values which are obtained using the same parameters. In this case, each of the values across the ‘B’
169"
TRAINING WITH QACT,0.2209695603156708,"dimension are considered to be samples from the context distribution.
170"
TRAINING WITH QACT,0.22209695603156707,"For a convolution layer, the typical outputs are of the form - (B, Nc, H, W) - B denotes the size of
171"
TRAINING WITH QACT,0.22322435174746336,"the batch, Nc denotes the number of channels, H, W denotes the sizes of the images. In this case we
172"
TRAINING WITH QACT,0.22435174746335965,"should consider all values across the 1st,3rd and 4th dimension to be from the context distribution,
173"
TRAINING WITH QACT,0.2254791431792559,"since all these values are obtained using the same parameters. So, the number of samples would be
174"
TRAINING WITH QACT,0.2266065388951522,"B × H × W.
175"
TRAINING WITH QACT,0.2277339346110485,"Loss Functions:
One can use any differentiable loss function to train with quantile activation. We
176"
TRAINING WITH QACT,0.22886133032694475,"specifically experiment with the standard Cross-Entropy Loss, Triplet Loss, and the recently proposed
177"
TRAINING WITH QACT,0.22998872604284104,"Watershed Loss [6] (see section 4). However, if one requires that the boundaries between classes
178"
TRAINING WITH QACT,0.23111612175873733,"adapt to the distribution, then learning similarities instead of boundaries can be beneficial. Both
179"
TRAINING WITH QACT,0.2322435174746336,"Triplet Loss and Watershed Loss fall into this category. We see that learning similarities does have
180"
TRAINING WITH QACT,0.23337091319052988,"slight benefits when considering the embedding quality.
181"
TRAINING WITH QACT,0.23449830890642615,"Inference with QACT:
As stated before, we want to assign a label for classification based on the
182"
TRAINING WITH QACT,0.23562570462232243,"context of the sample. There exist two approaches for this – (1) One way is to keep track of the
183"
TRAINING WITH QACT,0.23675310033821872,"quantiles and the estimated densities for all neurons and use it for inference. This allows inference
184"
TRAINING WITH QACT,0.237880496054115,"for a single sample in the traditional sense. However, this also implies that one would not be able
185"
TRAINING WITH QACT,0.23900789177001128,"to assign classes based on the context at evaluation. (2) Another way is to make sure that, even for
186"
TRAINING WITH QACT,0.24013528748590757,"inference on a single sample, we include several samples from the context distribution, but only use
187"
TRAINING WITH QACT,0.24126268320180383,"the output for a specific sample. This allows one to assign classes based on the context. In this article,
188"
TRAINING WITH QACT,0.24239007891770012,"we follow the latter approach.
189"
TRAINING WITH QACT,0.24351747463359638,"Quantile Classifier:
Observe that the proposed QACT (without normalization) returns the values in
190"
TRAINING WITH QACT,0.24464487034949267,"[0, 1] which can be interpreted as probabilities. Hence, one can also use this for the classification layer.
191"
TRAINING WITH QACT,0.24577226606538896,"Nonetheless, two changes are required – (i) Traditional softmax used in conjunction with negative-
192"
TRAINING WITH QACT,0.24689966178128522,"log-likelihoood loss already considers “relative” activations of the classification in normalization.
193"
TRAINING WITH QACT,0.2480270574971815,"However, QACT does not. Hence, one should use Binary-Cross-Entropy loss with QACT, which
194"
TRAINING WITH QACT,0.2491544532130778,"amounts to one-vs-rest classification. (ii) Also, unlike a neuron in the middle layers, the bias of the
195"
TRAINING WITH QACT,0.2502818489289741,"neuron in the classification layer depends on the class imbalance. For instance, with 10 classes, one
196"
TRAINING WITH QACT,0.25140924464487036,"would have only 1/10 of the samples labelled 1 and 9/10 of the samples labelled 0. To address this,
197"
TRAINING WITH QACT,0.2525366403607666,"we require that the median of the outputs be at 0.9, and hence weight the positive class with 0.9 and
198"
TRAINING WITH QACT,0.25366403607666294,"the negative class with 0.1 respectively. In this article, whenever QACT is used, we use this approach
199"
TRAINING WITH QACT,0.2547914317925592,"for inference.
200"
TRAINING WITH QACT,0.25591882750845546,"We observe that (figures 13 and 14) using quantile classifier on the learned features in general
201"
TRAINING WITH QACT,0.2570462232243517,"improves the consistency of the calibration error and also leads to the reducing the calibration error.
202"
TRAINING WITH QACT,0.25817361894024804,"In this article, for all networks trained with quantile activation, we use quantile classifier to compute
203"
TRAINING WITH QACT,0.2593010146561443,"the accracies/calibration errors.
204"
EVALUATION,0.26042841037204056,"4
Evaluation
205"
EVALUATION,0.2615558060879369,"To summarize, we make the following changes to the existing classification pipeline – (i) Replace the
206"
EVALUATION,0.26268320180383314,"usual ReLU activation with QACT and (ii) Use triplet or watershed loss instead of standard cross
207"
EVALUATION,0.2638105975197294,"entropy loss. We expect this framework to learn context dependent features, and hence be robust
208"
EVALUATION,0.2649379932356257,"to distortions. (iii) Also, use quantile classifier to train the classifier on the embedding for better
209"
EVALUATION,0.266065388951522,"calibrated probabilities.
210"
EVALUATION,0.26719278466741825,"(a) Accuracy
(b) Top-Label Calibration Error"
EVALUATION,0.26832018038331457,"Figure 3: Comparing QACT with ReLU activation and Dinov2 (small) on CIFAR10C. We observe
that, while at low severity of distortions QACT has a similar accuracy as existing pipelines, at
higher levels the drop in accuracy is substantially smaller than existing approaches. With respect to
calibration, we observe that the calibration error remains constant (up to standard deviations) across
distortions."
EVALUATION,0.26944757609921083,"Evaluation Protocol:
To evaluate our approach, we consider the two datasets developed for this
211"
EVALUATION,0.2705749718151071,"purpose – CIFAR10C, CIFAR100C, TinyImagenetC [15], MNISTC[25]. These datasets have a
212"
EVALUATION,0.2717023675310034,"set of 15 distortions at 5 severity levels. To ensure diversity we evaluate our method on 4 ar-
213"
EVALUATION,0.27282976324689967,"chitectures – (overparametrized) LeNet, ResNet18[14] (11M parameters), VGG[35](15M param-
214"
EVALUATION,0.27395715896279593,"eters) and DenseNet [18](1M parameters). The code to reproduce the results can be found at
215"
EVALUATION,0.2750845546786922,"https://anonymous.4open.science/r/QuantAct-2B41.
216"
EVALUATION,0.2762119503945885,"Baselines for Comparison:
To our knowledge, there exists no other framework which proposed
217"
EVALUATION,0.2773393461104848,"classification based on context distribution. So, for comparison, we consider standard ReLU activation
218"
EVALUATION,0.27846674182638104,"[10], pReLU [13], and SELU [20] for all the architectures stated above. Also, we compare our
219"
EVALUATION,0.27959413754227735,"results with DINOv2 (small) [27] (22M parameters) which is current state-of-the-art for domain
220"
EVALUATION,0.2807215332581736,"generalization. Note that for DINOv2, architecture and datasets used for training are substantially
221"
EVALUATION,0.2818489289740699,"different (and substantially larger) from what we consider in this article. Nevertheless, we include the
222"
EVALUATION,0.2829763246899662,"results for understanding where our proposed approach lies on the spectrum. We consider the small
223"
EVALUATION,0.28410372040586246,"version of DINOv2 to match the number of parameters with the compared models.
224"
EVALUATION,0.2852311161217587,"Metrics:
We consider four metrics – Accuracy (ACC), calibration error (ECE) [23] (both marginal
225"
EVALUATION,0.28635851183765504,"and Top-Label) and mean average precision at K (MAP@K) to evaluate the embedding. For the case
226"
EVALUATION,0.2874859075535513,"of ReLU/pReLU/SELU activation with Cross-Entropy, we use the logistic regression trained on the
227"
EVALUATION,0.28861330326944756,"train set embeddings, and for QACT we use the calibrated linear classifier, as proposed above. We do
228"
EVALUATION,0.2897406989853439,"not perform any additional calibration and use the probabilities. We discuss a selected set of results
229"
EVALUATION,0.29086809470124014,"in the main article. Please see appendix C for more comprehensive results.
230"
EVALUATION,0.2919954904171364,"Calibration error measures the reliability of predicted probabilities. In simple words, if one predicts
231"
EVALUATION,0.29312288613303267,"100 samples with (say) probability 0.7, then we expect 70 of the samples to belong to class 1 and the
232"
EVALUATION,0.294250281848929,"rest to class 0. This is measured using either the marginal or top-label calibration error. We refer the
233"
EVALUATION,0.29537767756482525,"reader to [23] for details, which also provides an implementation to estimate the calibration error.
234"
EVALUATION,0.2965050732807215,"Remark:
For all the baselines we use the standard Cross-Entropy loss for training. For inference
235"
EVALUATION,0.2976324689966178,"on corrupted datasets, we retrain the last layer with logistic regression on the train embedding and
236"
EVALUATION,0.2987598647125141,"evaluate it on test/corrupted embedding. For QACT, we as a convention use watershed loss unless
237"
EVALUATION,0.29988726042841035,"otherwise stated, for training. For inference, we train the Quantile Classifier on the train embedding
238"
EVALUATION,0.30101465614430667,"and evaluate it on test/corrupted embedding.
239"
EVALUATION,0.30214205186020293,"The proposed QACT approach is robust to distortions:
In fig. 3 we compare the proposed
240"
EVALUATION,0.3032694475760992,"QACT approach with predominant existing pipeline – ReLU+Cross-Entropy and DINOv2(small)
241"
EVALUATION,0.3043968432919955,"on CIFAR10C. In figure 3a we see that as the severity of the distortion increases, the accuracy of
242"
EVALUATION,0.3055242390078918,"ReLU and DINOv2 drops significantly. On the other hand, while at small distortions the results are
243"
EVALUATION,0.30665163472378804,"comparable, as severity increases QACT performs substantially better than conventional approaches.
244"
EVALUATION,0.30777903043968435,"(a)
(b)
(c)"
EVALUATION,0.3089064261555806,"Figure 4: (a) Dependence on Loss functions. Here we compare watershed with other popular
loss functions – Triplet and Cross-Entropy when used with QACT. We see that watershed per-
forms slightly better with respect to MAP. (b) Comparing QACT with other popular activations –
ReLU/pReLU/SELU with respect to accuracy. (c) Comparing QACT with other popular activations
– ReLU/pReLU/SELU with respect to Calibration Error (Marginal). From both (b) and (c) we can
conclude that QACT is notably more robust across distortions than several of the existing activation.
All the plots use ResNet18 with CIFAR10C dataset."
EVALUATION,0.3100338218714769,"(a)
(b)"
EVALUATION,0.3111612175873732,"Figure 5: Results on CIFAR100C/TinyImagenetC. We compare QACT+watershed to ReLU and
DinoV2 small on CIFAR100C/TinyImagenetC dataset with ResNet18. Note that the observations are
consistent with CIFAR10C. (a) shows how accuracy changes across distortions. Observe that QACT
is similar to DINOv2(s) with respect to embedding quality across all distortions, even if DINOv2
has 22M parameters as compared to Resnet18 11M parameters and is trained on larger datasets. (b)
shows how calibration error (marginal) changes across severities. While other approaches lead to an
increase in calibration error, QACT has similar calibration error across distortions."
EVALUATION,0.31228861330326946,"At severity 5, QACT outperforms DINOv2. On the other hand, we observe that in figure 3b, the
245"
EVALUATION,0.3134160090191657,"calibration error stays consistent across distortions.
246"
EVALUATION,0.314543404735062,"How much does QACT depend on the loss function?
Figure 4a compares the watershed classifier
247"
EVALUATION,0.3156708004509583,"with other popular losses – Triplet and Cross-Entropy. We see that all the loss functions perform com-
248"
EVALUATION,0.31679819616685456,"parably when used in conjunction with QACT. We observe that watershed has a slight improvement
249"
EVALUATION,0.3179255918827508,"when considering MAP and hence, we consider that as the default setting. However, we point out
250"
EVALUATION,0.31905298759864714,"that QACT is compatible with several loss functions as well.
251"
EVALUATION,0.3201803833145434,"QACT vs ReLU/pReLU/SELU activations:
To verify that most existing activations do not share
252"
EVALUATION,0.32130777903043967,"the robustness property of QACT, we compare QACT with other activations in figures 4b and 4c. We
253"
EVALUATION,0.322435174746336,"observe that QACT is greatly more robust with respect to distortions in both accuracy and calibration
254"
EVALUATION,0.32356257046223225,"error than other activation functions.
255"
EVALUATION,0.3246899661781285,"Results on Larger Datasets:
To verify that our observations hold for larger datasets, we use
256"
EVALUATION,0.3258173618940248,"CIFAR100C/TinyImagenetC to compare the proposed QACT+watershed with existing approaches.
257"
EVALUATION,0.3269447576099211,"We observe on figure 5 that QACT performs comparably well as DINOv2, although DINOv2(s)
258"
EVALUATION,0.32807215332581735,"has 22M parameters and is trained on significantly larger datasets. Moreover, we also observe that
259"
EVALUATION,0.32919954904171367,"QACT has approximately constant calibration error across distortions, as opposed to a significantly
260"
EVALUATION,0.33032694475760993,"increasing calibration error for ReLU or DINOv2.
261"
CONCLUSION AND FUTURE WORK,0.3314543404735062,"5
Conclusion And Future Work
262"
CONCLUSION AND FUTURE WORK,0.33258173618940245,"To summarize, traditional classification systems do not consider the “context distributions” when
263"
CONCLUSION AND FUTURE WORK,0.3337091319052988,"assigning labels. In this article, we propose a framework to achieve this by – (i) Making the activation
264"
CONCLUSION AND FUTURE WORK,0.33483652762119503,"adaptive by using quantiles and (ii) Learning a kernel instead of the boundary for the last layer. We
265"
CONCLUSION AND FUTURE WORK,0.3359639233370913,"show that our method is more robust to distortions by considering MNISTC, CIFAR10C, CIFAR100C,
266"
CONCLUSION AND FUTURE WORK,0.3370913190529876,"TinyImagenetC datasets across varying architectures.
267"
CONCLUSION AND FUTURE WORK,0.3382187147688839,"The scope of this article is to provide a proof of concept and a framework for performing inference in
268"
CONCLUSION AND FUTURE WORK,0.33934611048478014,"a context-dependent manner. We outline several potential directions for future research:
269"
CONCLUSION AND FUTURE WORK,0.34047350620067646,"I. The key idea in our proposed approach is that the quantiles capture the distribution of each
270"
CONCLUSION AND FUTURE WORK,0.3416009019165727,"neuron from the batch of samples, providing outputs accordingly. This poses a challenge for
271"
CONCLUSION AND FUTURE WORK,0.342728297632469,"inference, and we have discussed two potential solutions: (i) remember the quantiles and
272"
CONCLUSION AND FUTURE WORK,0.3438556933483653,"density estimates for single sample evaluation, or (ii) ensure that a batch of samples from
273"
CONCLUSION AND FUTURE WORK,0.34498308906426156,"the same distribution is processed together. We adopt the latter method in this article. An
274"
CONCLUSION AND FUTURE WORK,0.3461104847801578,"alternative approach would be to learn the distribution of each neuron using auxiliary loss
275"
CONCLUSION AND FUTURE WORK,0.34723788049605414,"functions, adjusting these distributions to fit the domain at test time. This gives us more
276"
CONCLUSION AND FUTURE WORK,0.3483652762119504,"control over the network at test time compared to current workflows.
277"
CONCLUSION AND FUTURE WORK,0.34949267192784667,"II. Since the aim of the article was to establish a proof-of-concept, we did not focus on scaling,
278"
CONCLUSION AND FUTURE WORK,0.3506200676437429,"and use only a single GPU for all the experiments. To extend it to multi-GPU training,
279"
CONCLUSION AND FUTURE WORK,0.35174746335963925,"one needs to synchronize the quantiles across GPU, in a similar manner as that for Batch-
280"
CONCLUSION AND FUTURE WORK,0.3528748590755355,"Normalization. We expect this to improve the statistics, and to allow considerably larger
281"
CONCLUSION AND FUTURE WORK,0.35400225479143177,"batches of training.
282"
CONCLUSION AND FUTURE WORK,0.3551296505073281,"III. On the theoretical side, there is an interesting analogy between our quantile activation and
283"
CONCLUSION AND FUTURE WORK,0.35625704622322435,"how a biological neuron behaves. It is known that when the inputs to a biological neuron
284"
CONCLUSION AND FUTURE WORK,0.3573844419391206,"change, the neuron adapts to these changes [7]. Quantile activation does something very
285"
CONCLUSION AND FUTURE WORK,0.35851183765501693,"similar, which leads to an open question – can we establish a formal link between the
286"
CONCLUSION AND FUTURE WORK,0.3596392333709132,"adaptability of a biological neuron and the accuracy of classification systems?
287"
CONCLUSION AND FUTURE WORK,0.36076662908680945,"IV. Another theoretical direction to explore involves considering distributions not just at the
288"
CONCLUSION AND FUTURE WORK,0.36189402480270577,"neuron level, but at the layer level, introducing a high-dimensional aspect to the problem.
289"
CONCLUSION AND FUTURE WORK,0.36302142051860203,"The main challenge here is defining and utilizing high dimensional quantiles, which remains
290"
CONCLUSION AND FUTURE WORK,0.3641488162344983,"an open question [21].
291"
CONCLUSION AND FUTURE WORK,0.3652762119503946,"Broad Impact:
In this article, we propose an approach to maintain calibration and generalization
292"
CONCLUSION AND FUTURE WORK,0.3664036076662909,"across small distortions. While, we do not foresee any direct societal consequences of our work, we
293"
CONCLUSION AND FUTURE WORK,0.36753100338218714,"expect the potential future consequences of the technique to reduce the bias in the following ways
294"
CONCLUSION AND FUTURE WORK,0.3686583990980834,"– (i) Since we do not assume normal distribution, our approach is likely to handle long tails better
295"
CONCLUSION AND FUTURE WORK,0.3697857948139797,"than existing methods. This would help in reducing the dataset bias where marginal groups are less
296"
CONCLUSION AND FUTURE WORK,0.370913190529876,"represented. (ii) Note that the output of each QACT layer is a uniform distribution. This can allow us
297"
CONCLUSION AND FUTURE WORK,0.37204058624577224,"to understand the working of each layer in isolation and possibly reduce the black-box nature of the
298"
CONCLUSION AND FUTURE WORK,0.37316798196166856,"current classification systems. (iii) Moreover, by directly modifying the context distribution of each
299"
CONCLUSION AND FUTURE WORK,0.3742953776775648,"neuron, one can easily make the networks more reliable without resorting to expensive re-training the
300"
CONCLUSION AND FUTURE WORK,0.3754227733934611,"entire network.
301"
REFERENCES,0.3765501691093574,"References
302"
REFERENCES,0.37767756482525366,"[1] Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning with
303"
REFERENCES,0.3788049605411499,"accuracy constraint for domain generalization. In European Conf. Mach. Learning, 2019.
304"
REFERENCES,0.37993235625704624,"[2] Martín Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk mini-
305"
REFERENCES,0.3810597519729425,"mization. arXiv:1907.02893, 2019.
306"
REFERENCES,0.38218714768883877,"[3] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization.
307"
REFERENCES,0.3833145434047351,"arXiv:1607.06450, 2016.
308"
REFERENCES,0.38444193912063135,"[4] Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger B. Grosse, and Jörn-Henrik Jacobsen.
309"
REFERENCES,0.3855693348365276,"Understanding and mitigating exploding inverses in invertible neural networks. In Artificial
310"
REFERENCES,0.38669673055242393,"Intelligence and Statistics, 2021.
311"
REFERENCES,0.3878241262683202,"[5] Aditya Challa, Snehanshu Saha, and Soma Dhavala. Quantprob: Generalizing probabilities
312"
REFERENCES,0.38895152198421645,"along with predictions for a pre-trained classifier. arXiv:2304.12766, 2023.
313"
REFERENCES,0.3900789177001127,"[6] Aditya Challa, Sravan Danda, and Laurent Najman. A novel approach to regularising 1nn
314"
REFERENCES,0.39120631341600903,"classifier for improved generalization. arXiv:2402.08405, 2024.
315"
REFERENCES,0.3923337091319053,"[7] Colin WG Clifford, Michael A Webster, Garrett B Stanley, Alan A Stocker, Adam Kohn,
316"
REFERENCES,0.39346110484780156,"Tatyana O Sharpee, and Odelia Schwartz.
Visual adaptation: Neural, psychological and
317"
REFERENCES,0.3945885005636979,"computational aspects. Vision research, 2007.
318"
REFERENCES,0.39571589627959414,"[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
319"
REFERENCES,0.3968432919954904,"Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
320"
REFERENCES,0.3979706877113867,"Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
321"
REFERENCES,0.399098083427283,"recognition at scale. In Int. Conf. on Learning Representations, 2021.
322"
REFERENCES,0.40022547914317924,"[9] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain gener-
323"
REFERENCES,0.40135287485907556,"alization via model-agnostic learning of semantic features. In Neural Inform. Process. Syst.,
324"
REFERENCES,0.4024802705749718,"2019.
325"
REFERENCES,0.4036076662908681,"[10] Kunihiko Fukushima. Correction to ""visual feature extraction by a multilayered network of
326"
REFERENCES,0.4047350620067644,"analog threshold elements"". IEEE Trans. Syst. Sci. Cybern., 1970.
327"
REFERENCES,0.40586245772266066,"[11] Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi.
Domain
328"
REFERENCES,0.4069898534385569,"generalization for object recognition with multi-task autoencoders. In Proc. Int. Conf. Comput.
329"
REFERENCES,0.4081172491544532,"Vision, 2015.
330"
REFERENCES,0.4092446448703495,"[12] Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Tailin Wu, Yilong Yin, Salman H.
331"
REFERENCES,0.41037204058624577,"Khan, Lina Yao, Tongliang Liu, and Kun Zhang.
How well does gpt-4v(ision) adapt to
332"
REFERENCES,0.41149943630214203,"distribution shifts? A preliminary investigation. arXiv:2312.07424, 2023.
333"
REFERENCES,0.41262683201803835,"[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
334"
REFERENCES,0.4137542277339346,"Surpassing human-level performance on imagenet classification. In Proc. Int. Conf. Comput.
335"
REFERENCES,0.41488162344983087,"Vision, 2015.
336"
REFERENCES,0.4160090191657272,"[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
337"
REFERENCES,0.41713641488162345,"recognition. In Proc. Conf. Comput. Vision Pattern Recognition, 2016.
338"
REFERENCES,0.4182638105975197,"[15] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
339"
REFERENCES,0.41939120631341603,"corruptions and perturbations. In Int. Conf. on Learning Representations, 2019.
340"
REFERENCES,0.4205186020293123,"[16] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multido-
341"
REFERENCES,0.42164599774520856,"main discriminant analysis. In Uncertainity in Artificial Intelligence, 2019.
342"
REFERENCES,0.4227733934611049,"[17] Bincheng Huang, Si Chen, Fan Zhou, Cheng Zhang, and Feng Zhang. Episodic training for
343"
REFERENCES,0.42390078917700114,"domain generalization using latent domains. In Int. Conf. on Cogni. Systems and Signal Process.,
344"
REFERENCES,0.4250281848928974,"2020.
345"
REFERENCES,0.42615558060879366,"[18] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
346"
REFERENCES,0.42728297632469,"convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,
347"
REFERENCES,0.42841037204058624,"CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2261–2269. IEEE Computer Society,
348"
REFERENCES,0.4295377677564825,"2017. doi: 10.1109/CVPR.2017.243. URL https://doi.org/10.1109/CVPR.2017.243.
349"
REFERENCES,0.4306651634723788,"[19] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
350"
REFERENCES,0.4317925591882751,"by reducing internal covariate shift. In Int. Conf. Mach. Learning, 2015.
351"
REFERENCES,0.43291995490417134,"[20] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
352"
REFERENCES,0.43404735062006766,"neural networks. In Neural Inform. Process. Syst., 2017.
353"
REFERENCES,0.4351747463359639,"[21] Roger Koenker. Quantile Regression. Econometric Society Monographs. Cambridge University
354"
REFERENCES,0.4363021420518602,"Press, 2005. doi: 10.1017/CBO9780511754098.
355"
REFERENCES,0.4374295377677565,"[22] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes
356"
REFERENCES,0.43855693348365277,"overconfidence in relu networks. In Int. Conf. Mach. Learning, 2020.
357"
REFERENCES,0.43968432919954903,"[23] Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In Neural Inform.
358"
REFERENCES,0.44081172491544535,"Process. Syst., 2019.
359"
REFERENCES,0.4419391206313416,"[24] Zhong Li and Matthijs van Leeuwen. Explainable contextual anomaly detection using quantile
360"
REFERENCES,0.44306651634723787,"regression forests. Data Min. Knowl. Discov., 2023.
361"
REFERENCES,0.44419391206313413,"[25] Norman Mu and Justin Gilmer. MNIST-C: A robustness benchmark for computer vision.
362"
REFERENCES,0.44532130777903045,"arXiv:1906.02337, 2019.
363"
REFERENCES,0.4464487034949267,"[26] Vinod Nair and Geoffrey E. Hinton.
Rectified linear units improve restricted boltzmann
364"
REFERENCES,0.447576099210823,"machines. In Int. Conf. Mach. Learning, 2010.
365"
REFERENCES,0.4487034949267193,"[27] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
366"
REFERENCES,0.44983089064261556,"Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran,
367"
REFERENCES,0.4509582863585118,"Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,
368"
REFERENCES,0.45208568207440814,"Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick
369"
REFERENCES,0.4532130777903044,"Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without
370"
REFERENCES,0.45434047350620066,"supervision. arXiv:2304.07193, 2023.
371"
REFERENCES,0.455467869222097,"[28] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data
372"
REFERENCES,0.45659526493799324,"Eng., 2010.
373"
REFERENCES,0.4577226606538895,"[29] Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi. Efficient domain generalization via
374"
REFERENCES,0.4588500563697858,"common-specific low-rank decomposition. In Int. Conf. Mach. Learning, 2020.
375"
REFERENCES,0.4599774520856821,"[30] Tejas Prashanth, Snehanshu Saha, Sumedh Basarkod, Suraj Aralihalli, Soma S. Dhavala,
376"
REFERENCES,0.46110484780157834,"Sriparna Saha, and Raviprasad Aduri. Lipgene: Lipschitz continuity guided adaptive learning
377"
REFERENCES,0.46223224351747466,"rates for fast convergence on microarray expression data sets. IEEE ACM Trans. Comput. Biol.
378"
REFERENCES,0.4633596392333709,"Bioinform., 2022.
379"
REFERENCES,0.4644870349492672,"[31] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to
380"
REFERENCES,0.46561443066516345,"accelerate training of deep neural networks. In Neural Inform. Process. Syst., 2016.
381"
REFERENCES,0.46674182638105977,"[32] Aditi Seetha, Satyendra Singh Chouhan, Emmanuel S Pilli, Vaskar Raychoudhury, and Snehan-
382"
REFERENCES,0.46786922209695603,"shu Saha. Dievd-sf: Disruptive event detection using continual machine learning with selective
383"
REFERENCES,0.4689966178128523,"forgetting. IEEE Transactions on Computational Social Systems, 2024.
384"
REFERENCES,0.4701240135287486,"[33] Hogeon Seo, Seunghyoung Ryu, Jiyeon Yim, Junghoon Seo, and Yonggyun Yu. Quantile
385"
REFERENCES,0.47125140924464487,"autoencoder for anomaly detection. In AAAI,Workshop on AI for Design and Manufacturing
386"
REFERENCES,0.47237880496054113,"(ADAM), 2022.
387"
REFERENCES,0.47350620067643745,"[34] Zheyan Shen, Peng Cui, Tong Zhang, and Kun Kuang. Stable learning via sample reweighting.
388"
REFERENCES,0.4746335963923337,"In AAAI Conf. on Artificial Intelligence, 2020.
389"
REFERENCES,0.47576099210823,"[35] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
390"
REFERENCES,0.4768883878241263,"image recognition. In Int. Conf. on Learning Representations, 2015.
391"
REFERENCES,0.47801578354002255,"[36] Anuj Tambwekar, Anirudh Maiya, Soma S. Dhavala, and Snehanshu Saha. Estimation and
392"
REFERENCES,0.4791431792559188,"applications of quantiles in deep binary classification. IEEE Trans. Artif. Intell., 2022.
393"
REFERENCES,0.48027057497181513,"[37] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong,
394"
REFERENCES,0.4813979706877114,"and Qing He. A comprehensive survey on transfer learning. Proc. IEEE, 2021.
395"
REFERENCES,0.48252536640360766,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.4836527621195039,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.48478015783540024,Figure 6: Comparing QACT with ReLU activation and Dinov2 (small).
REFERENCES,0.4859075535512965,"A
Experiment details for figure 1
396"
REFERENCES,0.48703494926719276,"We consider the features obtained from ResNet18 with both QACT and RelU activations for the
397"
REFERENCES,0.4881623449830891,"datasets of CIFAR10C with gaussian_noise at all the severity levels. Hence, we have 6 datasets
398"
REFERENCES,0.48928974069898534,"in total. To use TSNE for visualization, we consider 1000 samples from each dataset and obtain
399"
REFERENCES,0.4904171364148816,"the combined TSNE visualizations. Each figure shows a scatter plot of the 2d visualization for the
400"
REFERENCES,0.4915445321307779,"corresponding dataset.
401"
REFERENCES,0.4926719278466742,"B
Compute Resources and Other Experimental Details
402"
REFERENCES,0.49379932356257045,"All experiments were performed on a single NVidia GPU with 32GB memory with Intel Xeon CPU
403"
REFERENCES,0.49492671927846676,"(10 cores). For training, we perform an 80:20 split of the train dataset with seed 42 for reproducibility.
404"
REFERENCES,0.496054114994363,"All networks are initialized using default pytorch initialization technique.
405"
REFERENCES,0.4971815107102593,"We use Adam optimizer with initial learning rate 1e −3. We use ReduceLRonPlateau learning
406"
REFERENCES,0.4983089064261556,"rate scheduler with parameters – factor=0.1, patience=50, cooldown=10, threshold=0.01, thresh-
407"
REFERENCES,0.49943630214205187,"old_mode=abs, min_lr=1e-6. We monitor the validation accuracy for learning rate scheduling. We
408"
REFERENCES,0.5005636978579482,"also use early_stopping when the validation accuracy does not increase by 0.001.
409"
REFERENCES,0.5016910935738444,"C
Extended Results Section
410"
REFERENCES,0.5028184892897407,"Comparing QACT + watershed and ReLU+Cross-Entropy:
Figure 6 shows the corresponding
411"
REFERENCES,0.503945885005637,"results. The first experiment compares QACT + watershed with ReLU + Cross-Entropy on two
412"
REFERENCES,0.5050732807215332,"standard networks – ResNet18 and DenseNet. With respect to accuracy, we observe that while at
413"
REFERENCES,0.5062006764374295,"severity 0, ReLU + Cross-Entropy slightly outperforms QACT + watershed, as severity increases
414"
REFERENCES,0.5073280721533259,"QACT + watershed is far more stable. We even outperform DinoV2(small) (22M parameters) at
415"
REFERENCES,0.5084554678692221,"severity 5. Moreover, with respect to calibration error, we see a consistent trend across distortions.
416"
REFERENCES,0.5095828635851184,"As [5] argues, this helps in building more robust systems compared to one where calibration error
417"
REFERENCES,0.5107102593010147,"increases across distortions.
418"
REFERENCES,0.5118376550169109,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5129650507328072,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5140924464487034,Figure 7: Triplet vs Watershed vs Cross-Entropy
REFERENCES,0.5152198421645998,"Does loss function make a lot of difference?
Figure 7 compares three different loss functions
419"
REFERENCES,0.5163472378804961,"Watershed, Triplet and Cross-Entropy when used in conjunction with QACT. We observe similar
420"
REFERENCES,0.5174746335963923,"trends across all loss functions. However, Watershed performs better with respect to Mean Average
421"
REFERENCES,0.5186020293122886,"Precision (MAP) and hence we use this as a default strategy.
422"
REFERENCES,0.5197294250281849,"Why Mean-Average-Precision? – We argue that the key indicator of distortion invariance should
423"
REFERENCES,0.5208568207440811,"be the quality of embedding. While, accuracy (as measured by a linear classifier) is a good metric,
424"
REFERENCES,0.5219842164599775,"a better one would be to measure the Mean-Average-Precision. With respect to calibration error,
425"
REFERENCES,0.5231116121758738,"due to the scale on the Y-axis, the figures suggest reducing calibration error. However, the standard
426"
REFERENCES,0.52423900789177,"deviations overlap, and hence, these are assumed to be constant across distortions.
427"
REFERENCES,0.5253664036076663,"How well does watershed perform when used with ReLU activation?
Figure 8 shows the
428"
REFERENCES,0.5264937993235626,"corresponding results. We observe that both the watershed loss and cross-entropy have large overlaps
429"
REFERENCES,0.5276211950394588,"in the standard deviations at all severity levels. So, this shows that, when used in conjunction with
430"
REFERENCES,0.5287485907553551,"ReLU watershed and cross-entropy loss are very similar. But in conjunction with QACT, we see that
431"
REFERENCES,0.5298759864712514,"watershed has a slightly higher Mean-Average-Precision.
432"
REFERENCES,0.5310033821871477,"What if we consider an easy classification task?
In figure 9, we perform the comparison of
433"
REFERENCES,0.532130777903044,"QACT+Watershed and ReLU and cross-entropy on MNISTC dataset. Across different architectures,
434"
REFERENCES,0.5332581736189402,"we observe a lot less variation (standard deviation) of QACT+Watershed compared to RelU and
435"
REFERENCES,0.5343855693348365,"cross-entropy. This again suggests robustness against distortions of QACT+Watershed.
436"
REFERENCES,0.5355129650507328,"Comparing with other popular activations:
Figures 10 and 11 shows the comparison of QACT
437"
REFERENCES,0.5366403607666291,"with ReLU, pReLU and SeLU. We observe the same trend across ReLU, pReLU and SeLU, while
438"
REFERENCES,0.5377677564825254,"QACT is far more stable across distortions.
439"
REFERENCES,0.5388951521984217,"Results on CIFAR100/TinyImagenetC:
Figure 12 compares QACT+Watershed and ReLU+Cross-
440"
REFERENCES,0.5400225479143179,"Entropy on CIFAR100C dataset.
We also include the results of QACT+Cross-Entropy vs.
441"
REFERENCES,0.5411499436302142,"ReLU+Cross-Entropy on TinyImagenetC. The results are consistent with what we observe on
442"
REFERENCES,0.5422773393461104,"CIFAR10C, and hence, draw the same conclusions as before.
443"
REFERENCES,0.5434047350620068,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5445321307779031,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5456595264937993,Figure 8: Watershed vs Cross-Entropy when using ReLU activation
REFERENCES,0.5467869222096956,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5479143179255919,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5490417136414881,Figure 9: Results on MNIST
REFERENCES,0.5501691093573844,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5512965050732808,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.552423900789177,Figure 10: QACTvs ReLU vs pReLU vs Selu activations on ResNet18
REFERENCES,0.5535512965050733,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5546786922209695,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5558060879368658,Figure 11: QACTvs ReLU vs pReLU vs Selu activations on Densenet
REFERENCES,0.5569334836527621,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5580608793686584,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5591882750845547,Figure 12: QACTvs ReLU on Resnet18+CIFAR100
REFERENCES,0.560315670800451,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5614430665163472,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5625704622322435,Figure 13: Effect of Quantile Classifier. We use ResNet18 and DinoV2 architectures on CIFAR10.
REFERENCES,0.5636978579481398,"(a) Accuracy
(b) Mean Average Precision (MAP@100)"
REFERENCES,0.5648252536640361,"(c) Marginal Calibration Error
(d) Top-Label Calibration Error"
REFERENCES,0.5659526493799324,Figure 14: Effect of Quantile Classifier. We use ResNet18 and DinoV2 architectures on CIFAR100.
REFERENCES,0.5670800450958287,"Effect of Quantile Classifier:
Figures 13 and 14 shows the effect of quantile classifier on standard
444"
REFERENCES,0.5682074408117249,"ResNet10/DinoV2 outputs with CIFAR10C/CIFAR100C datasets. While the accuracy values are
445"
REFERENCES,0.5693348365276212,"almost equivalent, we observe a “flatter” trend of the calibration errors, sometimes reducing the error
446"
REFERENCES,0.5704622322435174,"as in the case of CIFAR100C.
447"
REFERENCES,0.5715896279594137,"D
Watershed Loss
448"
REFERENCES,0.5727170236753101,"The authors in [6] proposed a novel classifier – watershed classifier, which works by learning
449"
REFERENCES,0.5738444193912063,"similarities instead of the boundaries. Below we give the brief idea of the loss function, and refer the
450"
REFERENCES,0.5749718151071026,"reader to the original paper for further details.
451"
REFERENCES,0.5760992108229989,"1. Let (xi, yi) denote the samples in each batch, and let fθ denote the embedding network.
452"
REFERENCES,0.5772266065388951,"fθ(xi) denotes the corresponding embedding.
453"
REFERENCES,0.5783540022547914,"2. Starting from randomly selected seeds in the batch, propagate the labels to all the samples.
454"
REFERENCES,0.5794813979706878,"Let ˆyi denote the estimated samples. For each fθ(xi) and for each label l, obtain the nearest
455"
REFERENCES,0.580608793686584,"neighbour in the samples in the set,
456"
REFERENCES,0.5817361894024803,"Sl = {fθ(xi) | ˆyi = yi = l}
(9)"
REFERENCES,0.5828635851183765,"that is, all the samples of class l labelled correctly. Denote this nearest neighbour using
457"
REFERENCES,0.5839909808342728,"fθ(xi,l,1nn).
458"
REFERENCES,0.5851183765501691,"3. Then the loss is given by,
459"
REFERENCES,0.5862457722660653,"Watershed Loss =
−1
nsamples"
REFERENCES,0.5873731679819617,"nsamples
X i=1 L
X"
REFERENCES,0.588500563697858,"l=1
I[yi = l] log"
REFERENCES,0.5896279594137542,"exp (−∥fθ(xi) −fθ(xi,l,1nn)∥)
PL
j=1 exp (−∥fθ(xi) −fθ(xi,j,1nn)∥) ! (10)"
REFERENCES,0.5907553551296505,"Why Watershed Loss?:
Observe that the loss in equation 10 implicitly learns representations
460"
REFERENCES,0.5918827508455468,"consistent with the RBF kernel, which is known to be translation invariant. Minimizing this loss
461"
REFERENCES,0.593010146561443,"function, hence, will learn translation invariant kernels. This is important for obtaining networks
462"
REFERENCES,0.5941375422773394,"robust to distortions.
463"
REFERENCES,0.5952649379932357,"If one uses (say) Cross Entropy loss, then the features learned would be such that the classes are
464"
REFERENCES,0.5963923337091319,"linearly separable. Contrast this with watershed, which instead learns a similarity between two points
465"
REFERENCES,0.5975197294250282,"in a translation invariant manner.
466"
REFERENCES,0.5986471251409244,"Remark:
Observe that the watershed loss is very similar to metric learning losses. The authors in
467"
REFERENCES,0.5997745208568207,"[6] claim that this offers better generalization, and show that this is consistent with 1NN classifier.
468"
REFERENCES,0.6009019165727171,"Moreover, they show that this classifier (without considering fθ) has a VC dimension which is equal
469"
REFERENCES,0.6020293122886133,"to the number of classes. While metric learning losses are similar, there is no such guarantee with
470"
REFERENCES,0.6031567080045096,"respect to classification. This motivated our choice of using watershed loss over other metric learning
471"
REFERENCES,0.6042841037204059,"losses.
472"
REFERENCES,0.6054114994363021,"NeurIPS Paper Checklist
473"
REFERENCES,0.6065388951521984,"The checklist is designed to encourage best practices for responsible machine learning research,
474"
REFERENCES,0.6076662908680946,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
475"
REFERENCES,0.608793686583991,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
476"
REFERENCES,0.6099210822998873,"follow the references and precede the (optional) supplemental material. The checklist does NOT
477"
REFERENCES,0.6110484780157835,"count towards the page limit.
478"
REFERENCES,0.6121758737316798,"Please read the checklist guidelines carefully for information on how to answer these questions. For
479"
REFERENCES,0.6133032694475761,"each question in the checklist:
480"
REFERENCES,0.6144306651634723,"• You should answer [Yes] , [No] , or [NA] .
481"
REFERENCES,0.6155580608793687,"• [NA] means either that the question is Not Applicable for that particular paper or the
482"
REFERENCES,0.616685456595265,"relevant information is Not Available.
483"
REFERENCES,0.6178128523111612,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
484"
REFERENCES,0.6189402480270575,"The checklist answers are an integral part of your paper submission. They are visible to the
485"
REFERENCES,0.6200676437429538,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
486"
REFERENCES,0.62119503945885,"(after eventual revisions) with the final version of your paper, and its final version will be published
487"
REFERENCES,0.6223224351747464,"with the paper.
488"
REFERENCES,0.6234498308906427,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
489"
REFERENCES,0.6245772266065389,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
490"
REFERENCES,0.6257046223224352,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
491"
REFERENCES,0.6268320180383314,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
492"
REFERENCES,0.6279594137542277,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
493"
REFERENCES,0.629086809470124,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
494"
REFERENCES,0.6302142051860203,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
495"
REFERENCES,0.6313416009019166,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
496"
REFERENCES,0.6324689966178129,"please point to the section(s) where related material for the question can be found.
497"
REFERENCES,0.6335963923337091,"IMPORTANT, please:
498"
REFERENCES,0.6347237880496054,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
499"
REFERENCES,0.6358511837655016,"• Keep the checklist subsection headings, questions/answers and guidelines below.
500"
REFERENCES,0.636978579481398,"• Do not modify the questions and only use the provided macros for your answers.
501"
CLAIMS,0.6381059751972943,"1. Claims
502"
CLAIMS,0.6392333709131905,"Question: Do the main claims made in the abstract and introduction accurately reflect the
503"
CLAIMS,0.6403607666290868,"paper’s contributions and scope?
504"
CLAIMS,0.6414881623449831,"Answer: [Yes]
505"
CLAIMS,0.6426155580608793,"Justification: Yes. The main contribution is a proof-of-concept that one should move away
506"
CLAIMS,0.6437429537767756,"from single point estimation for better generalization across distortions.
507"
CLAIMS,0.644870349492672,"Guidelines:
508"
CLAIMS,0.6459977452085682,"• The answer NA means that the abstract and introduction do not include the claims
509"
CLAIMS,0.6471251409244645,"made in the paper.
510"
CLAIMS,0.6482525366403608,"• The abstract and/or introduction should clearly state the claims made, including the
511"
CLAIMS,0.649379932356257,"contributions made in the paper and important assumptions and limitations. A No or
512"
CLAIMS,0.6505073280721533,"NA answer to this question will not be perceived well by the reviewers.
513"
CLAIMS,0.6516347237880497,"• The claims made should match theoretical and experimental results, and reflect how
514"
CLAIMS,0.6527621195039459,"much the results can be expected to generalize to other settings.
515"
CLAIMS,0.6538895152198422,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
516"
CLAIMS,0.6550169109357384,"are not attained by the paper.
517"
LIMITATIONS,0.6561443066516347,"2. Limitations
518"
LIMITATIONS,0.657271702367531,"Question: Does the paper discuss the limitations of the work performed by the authors?
519"
LIMITATIONS,0.6583990980834273,"Answer: [Yes]
520"
LIMITATIONS,0.6595264937993236,"Justification: Since, the scope is only a proof-of-concept, we have not considered scaling
521"
LIMITATIONS,0.6606538895152199,"to large datasets/models in this work. Scaling these ideas would require rethinking current
522"
LIMITATIONS,0.6617812852311161,"strategies and does not fit perfectly into existing framework. Moreover, the proposed
523"
LIMITATIONS,0.6629086809470124,"approach slightly more resource intensive than ReLU activation.
524"
LIMITATIONS,0.6640360766629086,"Guidelines:
525"
LIMITATIONS,0.6651634723788049,"• The answer NA means that the paper has no limitation while the answer No means that
526"
LIMITATIONS,0.6662908680947013,"the paper has limitations, but those are not discussed in the paper.
527"
LIMITATIONS,0.6674182638105975,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
528"
LIMITATIONS,0.6685456595264938,"• The paper should point out any strong assumptions and how robust the results are to
529"
LIMITATIONS,0.6696730552423901,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
530"
LIMITATIONS,0.6708004509582863,"model well-specification, asymptotic approximations only holding locally). The authors
531"
LIMITATIONS,0.6719278466741826,"should reflect on how these assumptions might be violated in practice and what the
532"
LIMITATIONS,0.673055242390079,"implications would be.
533"
LIMITATIONS,0.6741826381059752,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
534"
LIMITATIONS,0.6753100338218715,"only tested on a few datasets or with a few runs. In general, empirical results often
535"
LIMITATIONS,0.6764374295377678,"depend on implicit assumptions, which should be articulated.
536"
LIMITATIONS,0.677564825253664,"• The authors should reflect on the factors that influence the performance of the approach.
537"
LIMITATIONS,0.6786922209695603,"For example, a facial recognition algorithm may perform poorly when image resolution
538"
LIMITATIONS,0.6798196166854565,"is low or images are taken in low lighting. Or a speech-to-text system might not be
539"
LIMITATIONS,0.6809470124013529,"used reliably to provide closed captions for online lectures because it fails to handle
540"
LIMITATIONS,0.6820744081172492,"technical jargon.
541"
LIMITATIONS,0.6832018038331454,"• The authors should discuss the computational efficiency of the proposed algorithms
542"
LIMITATIONS,0.6843291995490417,"and how they scale with dataset size.
543"
LIMITATIONS,0.685456595264938,"• If applicable, the authors should discuss possible limitations of their approach to
544"
LIMITATIONS,0.6865839909808342,"address problems of privacy and fairness.
545"
LIMITATIONS,0.6877113866967306,"• While the authors might fear that complete honesty about limitations might be used by
546"
LIMITATIONS,0.6888387824126269,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
547"
LIMITATIONS,0.6899661781285231,"limitations that aren’t acknowledged in the paper. The authors should use their best
548"
LIMITATIONS,0.6910935738444194,"judgment and recognize that individual actions in favor of transparency play an impor-
549"
LIMITATIONS,0.6922209695603156,"tant role in developing norms that preserve the integrity of the community. Reviewers
550"
LIMITATIONS,0.6933483652762119,"will be specifically instructed to not penalize honesty concerning limitations.
551"
THEORY ASSUMPTIONS AND PROOFS,0.6944757609921083,"3. Theory Assumptions and Proofs
552"
THEORY ASSUMPTIONS AND PROOFS,0.6956031567080045,"Question: For each theoretical result, does the paper provide the full set of assumptions and
553"
THEORY ASSUMPTIONS AND PROOFS,0.6967305524239008,"a complete (and correct) proof?
554"
THEORY ASSUMPTIONS AND PROOFS,0.6978579481397971,"Answer: [NA]
555"
THEORY ASSUMPTIONS AND PROOFS,0.6989853438556933,"Justification: We do not consider theoretical aspects in this article. While there are interesting
556"
THEORY ASSUMPTIONS AND PROOFS,0.7001127395715896,"theoretical connections, we leave it for future work.
557"
THEORY ASSUMPTIONS AND PROOFS,0.7012401352874859,"Guidelines:
558"
THEORY ASSUMPTIONS AND PROOFS,0.7023675310033822,"• The answer NA means that the paper does not include theoretical results.
559"
THEORY ASSUMPTIONS AND PROOFS,0.7034949267192785,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
560"
THEORY ASSUMPTIONS AND PROOFS,0.7046223224351748,"referenced.
561"
THEORY ASSUMPTIONS AND PROOFS,0.705749718151071,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
562"
THEORY ASSUMPTIONS AND PROOFS,0.7068771138669673,"• The proofs can either appear in the main paper or the supplemental material, but if
563"
THEORY ASSUMPTIONS AND PROOFS,0.7080045095828635,"they appear in the supplemental material, the authors are encouraged to provide a short
564"
THEORY ASSUMPTIONS AND PROOFS,0.7091319052987599,"proof sketch to provide intuition.
565"
THEORY ASSUMPTIONS AND PROOFS,0.7102593010146562,"• Inversely, any informal proof provided in the core of the paper should be complemented
566"
THEORY ASSUMPTIONS AND PROOFS,0.7113866967305524,"by formal proofs provided in appendix or supplemental material.
567"
THEORY ASSUMPTIONS AND PROOFS,0.7125140924464487,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.713641488162345,"4. Experimental Result Reproducibility
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7147688838782412,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7158962795941376,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7170236753100339,"of the paper (regardless of whether the code and data are provided or not)?
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7181510710259301,"Answer: [Yes]
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7192784667418264,"Justification: We provide an anonymous link to generate all the results provided in the article.
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7204058624577226,"Moreover, we describe all the hyper-parameters used in the appendix as well.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7215332581736189,"Guidelines:
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7226606538895152,"• The answer NA means that the paper does not include experiments.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7237880496054115,"• If the paper includes experiments, a No answer to this question will not be perceived
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7249154453213078,"well by the reviewers: Making the paper reproducible is important, regardless of
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7260428410372041,"whether the code and data are provided or not.
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7271702367531003,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7282976324689966,"to make their results reproducible or verifiable.
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7294250281848929,"• Depending on the contribution, reproducibility can be accomplished in various ways.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7305524239007892,"For example, if the contribution is a novel architecture, describing the architecture fully
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7316798196166855,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7328072153325818,"be necessary to either make it possible for others to replicate the model with the same
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.733934611048478,"dataset, or provide access to the model. In general. releasing code and data is often
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7350620067643743,"one good way to accomplish this, but reproducibility can also be provided via detailed
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7361894024802705,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7373167981961668,"of a large language model), releasing of a model checkpoint, or other means that are
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7384441939120632,"appropriate to the research performed.
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7395715896279594,"• While NeurIPS does not require releasing code, the conference does require all submis-
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7406989853438557,"sions to provide some reasonable avenue for reproducibility, which may depend on the
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.741826381059752,"nature of the contribution. For example
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7429537767756482,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7440811724915445,"to reproduce that algorithm.
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7452085682074409,"(b) If the contribution is primarily a new model architecture, the paper should describe
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7463359639233371,"the architecture clearly and fully.
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7474633596392334,"(c) If the contribution is a new model (e.g., a large language model), then there should
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485907553551296,"either be a way to access this model for reproducing the results or a way to reproduce
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497181510710259,"the model (e.g., with an open-source dataset or instructions for how to construct
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508455467869222,"the dataset).
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7519729425028185,"(d) We recognize that reproducibility may be tricky in some cases, in which case
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7531003382187148,"authors are welcome to describe the particular way they provide for reproducibility.
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7542277339346111,"In the case of closed-source models, it may be that access to the model is limited in
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7553551296505073,"some way (e.g., to registered users), but it should be possible for other researchers
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7564825253664036,"to have some path to reproducing or verifying the results.
607"
OPEN ACCESS TO DATA AND CODE,0.7576099210822999,"5. Open access to data and code
608"
OPEN ACCESS TO DATA AND CODE,0.7587373167981961,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
609"
OPEN ACCESS TO DATA AND CODE,0.7598647125140925,"tions to faithfully reproduce the main experimental results, as described in supplemental
610"
OPEN ACCESS TO DATA AND CODE,0.7609921082299888,"material?
611"
OPEN ACCESS TO DATA AND CODE,0.762119503945885,"Answer: [Yes]
612"
OPEN ACCESS TO DATA AND CODE,0.7632468996617813,"Justification: We provide the code using an anonymous link at https://anonymous.
613"
OPEN ACCESS TO DATA AND CODE,0.7643742953776775,"4open.science/r/QuantAct-2B41. The datasets are all public datasets which can be
614"
OPEN ACCESS TO DATA AND CODE,0.7655016910935738,"downloaded.
615"
OPEN ACCESS TO DATA AND CODE,0.7666290868094702,"Guidelines:
616"
OPEN ACCESS TO DATA AND CODE,0.7677564825253664,"• The answer NA means that paper does not include experiments requiring code.
617"
OPEN ACCESS TO DATA AND CODE,0.7688838782412627,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
618"
OPEN ACCESS TO DATA AND CODE,0.770011273957159,"public/guides/CodeSubmissionPolicy) for more details.
619"
OPEN ACCESS TO DATA AND CODE,0.7711386696730552,"• While we encourage the release of code and data, we understand that this might not be
620"
OPEN ACCESS TO DATA AND CODE,0.7722660653889515,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
621"
OPEN ACCESS TO DATA AND CODE,0.7733934611048479,"including code, unless this is central to the contribution (e.g., for a new open-source
622"
OPEN ACCESS TO DATA AND CODE,0.7745208568207441,"benchmark).
623"
OPEN ACCESS TO DATA AND CODE,0.7756482525366404,"• The instructions should contain the exact command and environment needed to run to
624"
OPEN ACCESS TO DATA AND CODE,0.7767756482525366,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
625"
OPEN ACCESS TO DATA AND CODE,0.7779030439684329,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
626"
OPEN ACCESS TO DATA AND CODE,0.7790304396843292,"• The authors should provide instructions on data access and preparation, including how
627"
OPEN ACCESS TO DATA AND CODE,0.7801578354002254,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
628"
OPEN ACCESS TO DATA AND CODE,0.7812852311161218,"• The authors should provide scripts to reproduce all experimental results for the new
629"
OPEN ACCESS TO DATA AND CODE,0.7824126268320181,"proposed method and baselines. If only a subset of experiments are reproducible, they
630"
OPEN ACCESS TO DATA AND CODE,0.7835400225479143,"should state which ones are omitted from the script and why.
631"
OPEN ACCESS TO DATA AND CODE,0.7846674182638106,"• At submission time, to preserve anonymity, the authors should release anonymized
632"
OPEN ACCESS TO DATA AND CODE,0.7857948139797069,"versions (if applicable).
633"
OPEN ACCESS TO DATA AND CODE,0.7869222096956031,"• Providing as much information as possible in supplemental material (appended to the
634"
OPEN ACCESS TO DATA AND CODE,0.7880496054114995,"paper) is recommended, but including URLs to data and code is permitted.
635"
OPEN ACCESS TO DATA AND CODE,0.7891770011273957,"6. Experimental Setting/Details
636"
OPEN ACCESS TO DATA AND CODE,0.790304396843292,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
637"
OPEN ACCESS TO DATA AND CODE,0.7914317925591883,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
638"
OPEN ACCESS TO DATA AND CODE,0.7925591882750845,"results?
639"
OPEN ACCESS TO DATA AND CODE,0.7936865839909808,"Answer: [Yes]
640"
OPEN ACCESS TO DATA AND CODE,0.7948139797068771,"Justification: We explain the experimental setting in complete detail in the article.
641"
OPEN ACCESS TO DATA AND CODE,0.7959413754227734,"Guidelines:
642"
OPEN ACCESS TO DATA AND CODE,0.7970687711386697,"• The answer NA means that the paper does not include experiments.
643"
OPEN ACCESS TO DATA AND CODE,0.798196166854566,"• The experimental setting should be presented in the core of the paper to a level of detail
644"
OPEN ACCESS TO DATA AND CODE,0.7993235625704622,"that is necessary to appreciate the results and make sense of them.
645"
OPEN ACCESS TO DATA AND CODE,0.8004509582863585,"• The full details can be provided either with the code, in appendix, or as supplemental
646"
OPEN ACCESS TO DATA AND CODE,0.8015783540022547,"material.
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8027057497181511,"7. Experiment Statistical Significance
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8038331454340474,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8049605411499436,"information about the statistical significance of the experiments?
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8060879368658399,"Answer: [Yes]
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8072153325817362,"Justification: The datasets used incorporate 15 kinds of distortions across 5 severity levels.
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8083427282976324,"We report the error bars across the 15 kinds of distortions which should provide a good
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8094701240135288,"picture of the reliability of the results.
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8105975197294251,"Guidelines:
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8117249154453213,"• The answer NA means that the paper does not include experiments.
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8128523111612176,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8139797068771139,"dence intervals, or statistical significance tests, at least for the experiments that support
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8151071025930101,"the main claims of the paper.
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8162344983089064,"• The factors of variability that the error bars are capturing should be clearly stated (for
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8173618940248027,"example, train/test split, initialization, random drawing of some parameter, or overall
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.818489289740699,"run with given experimental conditions).
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8196166854565953,"• The method for calculating the error bars should be explained (closed form formula,
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8207440811724915,"call to a library function, bootstrap, etc.)
664"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8218714768883878,"• The assumptions made should be given (e.g., Normally distributed errors).
665"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8229988726042841,"• It should be clear whether the error bar is the standard deviation or the standard error
666"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8241262683201804,"of the mean.
667"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8252536640360767,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
668"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.826381059751973,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
669"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275084554678692,"of Normality of errors is not verified.
670"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8286358511837655,"• For asymmetric distributions, the authors should be careful not to show in tables or
671"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8297632468996617,"figures symmetric error bars that would yield results that are out of range (e.g. negative
672"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.830890642615558,"error rates).
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8320180383314544,"• If error bars are reported in tables or plots, The authors should explain in the text how
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331454340473506,"they were calculated and reference the corresponding figures or tables in the text.
675"
EXPERIMENTS COMPUTE RESOURCES,0.8342728297632469,"8. Experiments Compute Resources
676"
EXPERIMENTS COMPUTE RESOURCES,0.8354002254791432,"Question: For each experiment, does the paper provide sufficient information on the com-
677"
EXPERIMENTS COMPUTE RESOURCES,0.8365276211950394,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
678"
EXPERIMENTS COMPUTE RESOURCES,0.8376550169109357,"the experiments?
679"
EXPERIMENTS COMPUTE RESOURCES,0.8387824126268321,"Answer:[Yes]
680"
EXPERIMENTS COMPUTE RESOURCES,0.8399098083427283,"Justification: Yes. we include the entire information about the compute resources in the
681"
EXPERIMENTS COMPUTE RESOURCES,0.8410372040586246,"appendix.
682"
EXPERIMENTS COMPUTE RESOURCES,0.8421645997745209,"Guidelines:
683"
EXPERIMENTS COMPUTE RESOURCES,0.8432919954904171,"• The answer NA means that the paper does not include experiments.
684"
EXPERIMENTS COMPUTE RESOURCES,0.8444193912063134,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
685"
EXPERIMENTS COMPUTE RESOURCES,0.8455467869222097,"or cloud provider, including relevant memory and storage.
686"
EXPERIMENTS COMPUTE RESOURCES,0.846674182638106,"• The paper should provide the amount of compute required for each of the individual
687"
EXPERIMENTS COMPUTE RESOURCES,0.8478015783540023,"experimental runs as well as estimate the total compute.
688"
EXPERIMENTS COMPUTE RESOURCES,0.8489289740698985,"• The paper should disclose whether the full research project required more compute
689"
EXPERIMENTS COMPUTE RESOURCES,0.8500563697857948,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
690"
EXPERIMENTS COMPUTE RESOURCES,0.8511837655016911,"didn’t make it into the paper).
691"
CODE OF ETHICS,0.8523111612175873,"9. Code Of Ethics
692"
CODE OF ETHICS,0.8534385569334837,"Question: Does the research conducted in the paper conform, in every respect, with the
693"
CODE OF ETHICS,0.85456595264938,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
694"
CODE OF ETHICS,0.8556933483652762,"Answer: [Yes]
695"
CODE OF ETHICS,0.8568207440811725,"Justification: We have tried to maintain the double-blind policy to the maximum extent
696"
CODE OF ETHICS,0.8579481397970687,"possible.
697"
CODE OF ETHICS,0.859075535512965,"Guidelines:
698"
CODE OF ETHICS,0.8602029312288614,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
699"
CODE OF ETHICS,0.8613303269447576,"• If the authors answer No, they should explain the special circumstances that require a
700"
CODE OF ETHICS,0.8624577226606539,"deviation from the Code of Ethics.
701"
CODE OF ETHICS,0.8635851183765502,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
702"
CODE OF ETHICS,0.8647125140924464,"eration due to laws or regulations in their jurisdiction).
703"
BROADER IMPACTS,0.8658399098083427,"10. Broader Impacts
704"
BROADER IMPACTS,0.8669673055242391,"Question: Does the paper discuss both potential positive societal impacts and negative
705"
BROADER IMPACTS,0.8680947012401353,"societal impacts of the work performed?
706"
BROADER IMPACTS,0.8692220969560316,"Answer: [Yes]
707"
BROADER IMPACTS,0.8703494926719278,"Justification: In this article we propose a novel way to allow robustness against distortions.
708"
BROADER IMPACTS,0.8714768883878241,"We do not expect any negative societal impacts. There could be positive societal impact
709"
BROADER IMPACTS,0.8726042841037204,"since this can potentially stop hallucinations/bias of the ML models.
710"
BROADER IMPACTS,0.8737316798196166,"Guidelines:
711"
BROADER IMPACTS,0.874859075535513,"• The answer NA means that there is no societal impact of the work performed.
712"
BROADER IMPACTS,0.8759864712514093,"• If the authors answer NA or No, they should explain why their work has no societal
713"
BROADER IMPACTS,0.8771138669673055,"impact or why the paper does not address societal impact.
714"
BROADER IMPACTS,0.8782412626832018,"• Examples of negative societal impacts include potential malicious or unintended uses
715"
BROADER IMPACTS,0.8793686583990981,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
716"
BROADER IMPACTS,0.8804960541149943,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
717"
BROADER IMPACTS,0.8816234498308907,"groups), privacy considerations, and security considerations.
718"
BROADER IMPACTS,0.882750845546787,"• The conference expects that many papers will be foundational research and not tied
719"
BROADER IMPACTS,0.8838782412626832,"to particular applications, let alone deployments. However, if there is a direct path to
720"
BROADER IMPACTS,0.8850056369785795,"any negative applications, the authors should point it out. For example, it is legitimate
721"
BROADER IMPACTS,0.8861330326944757,"to point out that an improvement in the quality of generative models could be used to
722"
BROADER IMPACTS,0.887260428410372,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
723"
BROADER IMPACTS,0.8883878241262683,"that a generic algorithm for optimizing neural networks could enable people to train
724"
BROADER IMPACTS,0.8895152198421646,"models that generate Deepfakes faster.
725"
BROADER IMPACTS,0.8906426155580609,"• The authors should consider possible harms that could arise when the technology is
726"
BROADER IMPACTS,0.8917700112739572,"being used as intended and functioning correctly, harms that could arise when the
727"
BROADER IMPACTS,0.8928974069898534,"technology is being used as intended but gives incorrect results, and harms following
728"
BROADER IMPACTS,0.8940248027057497,"from (intentional or unintentional) misuse of the technology.
729"
BROADER IMPACTS,0.895152198421646,"• If there are negative societal impacts, the authors could also discuss possible mitigation
730"
BROADER IMPACTS,0.8962795941375423,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
731"
BROADER IMPACTS,0.8974069898534386,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
732"
BROADER IMPACTS,0.8985343855693348,"feedback over time, improving the efficiency and accessibility of ML).
733"
SAFEGUARDS,0.8996617812852311,"11. Safeguards
734"
SAFEGUARDS,0.9007891770011274,"Question: Does the paper describe safeguards that have been put in place for responsible
735"
SAFEGUARDS,0.9019165727170236,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
736"
SAFEGUARDS,0.90304396843292,"image generators, or scraped datasets)?
737"
SAFEGUARDS,0.9041713641488163,"Answer: [NA]
738"
SAFEGUARDS,0.9052987598647125,"Justification: We do not use any datasets/models that have high risk for misuse.
739"
SAFEGUARDS,0.9064261555806088,"Guidelines:
740"
SAFEGUARDS,0.9075535512965051,"• The answer NA means that the paper poses no such risks.
741"
SAFEGUARDS,0.9086809470124013,"• Released models that have a high risk for misuse or dual-use should be released with
742"
SAFEGUARDS,0.9098083427282976,"necessary safeguards to allow for controlled use of the model, for example by requiring
743"
SAFEGUARDS,0.910935738444194,"that users adhere to usage guidelines or restrictions to access the model or implementing
744"
SAFEGUARDS,0.9120631341600902,"safety filters.
745"
SAFEGUARDS,0.9131905298759865,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
746"
SAFEGUARDS,0.9143179255918827,"should describe how they avoided releasing unsafe images.
747"
SAFEGUARDS,0.915445321307779,"• We recognize that providing effective safeguards is challenging, and many papers do
748"
SAFEGUARDS,0.9165727170236753,"not require this, but we encourage authors to take this into account and make a best
749"
SAFEGUARDS,0.9177001127395716,"faith effort.
750"
LICENSES FOR EXISTING ASSETS,0.9188275084554679,"12. Licenses for existing assets
751"
LICENSES FOR EXISTING ASSETS,0.9199549041713642,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
752"
LICENSES FOR EXISTING ASSETS,0.9210822998872604,"the paper, properly credited and are the license and terms of use explicitly mentioned and
753"
LICENSES FOR EXISTING ASSETS,0.9222096956031567,"properly respected?
754"
LICENSES FOR EXISTING ASSETS,0.923337091319053,"Answer: [Yes]
755"
LICENSES FOR EXISTING ASSETS,0.9244644870349493,"Justification: We have cited the original articles of all the datasets/models we use in the
756"
LICENSES FOR EXISTING ASSETS,0.9255918827508456,"article. We have ensured that these can be used with credit for academic purposes.
757"
LICENSES FOR EXISTING ASSETS,0.9267192784667418,"Guidelines:
758"
LICENSES FOR EXISTING ASSETS,0.9278466741826381,"• The answer NA means that the paper does not use existing assets.
759"
LICENSES FOR EXISTING ASSETS,0.9289740698985344,"• The authors should cite the original paper that produced the code package or dataset.
760"
LICENSES FOR EXISTING ASSETS,0.9301014656144306,"• The authors should state which version of the asset is used and, if possible, include a
761"
LICENSES FOR EXISTING ASSETS,0.9312288613303269,"URL.
762"
LICENSES FOR EXISTING ASSETS,0.9323562570462233,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
763"
LICENSES FOR EXISTING ASSETS,0.9334836527621195,"• For scraped data from a particular source (e.g., website), the copyright and terms of
764"
LICENSES FOR EXISTING ASSETS,0.9346110484780158,"service of that source should be provided.
765"
LICENSES FOR EXISTING ASSETS,0.9357384441939121,"• If assets are released, the license, copyright information, and terms of use in the
766"
LICENSES FOR EXISTING ASSETS,0.9368658399098083,"package should be provided. For popular datasets, paperswithcode.com/datasets
767"
LICENSES FOR EXISTING ASSETS,0.9379932356257046,"has curated licenses for some datasets. Their licensing guide can help determine the
768"
LICENSES FOR EXISTING ASSETS,0.939120631341601,"license of a dataset.
769"
LICENSES FOR EXISTING ASSETS,0.9402480270574972,"• For existing datasets that are re-packaged, both the original license and the license of
770"
LICENSES FOR EXISTING ASSETS,0.9413754227733935,"the derived asset (if it has changed) should be provided.
771"
LICENSES FOR EXISTING ASSETS,0.9425028184892897,"• If this information is not available online, the authors are encouraged to reach out to
772"
LICENSES FOR EXISTING ASSETS,0.943630214205186,"the asset’s creators.
773"
NEW ASSETS,0.9447576099210823,"13. New Assets
774"
NEW ASSETS,0.9458850056369785,"Question: Are new assets introduced in the paper well documented and is the documentation
775"
NEW ASSETS,0.9470124013528749,"provided alongside the assets?
776"
NEW ASSETS,0.9481397970687712,"Answer: [Yes]
777"
NEW ASSETS,0.9492671927846674,"Justification:
We
share
the
code
at
https://anonymous.4open.science/r/
778"
NEW ASSETS,0.9503945885005637,"QuantAct-2B41.
779"
NEW ASSETS,0.95152198421646,"Guidelines:
780"
NEW ASSETS,0.9526493799323562,"• The answer NA means that the paper does not release new assets.
781"
NEW ASSETS,0.9537767756482526,"• Researchers should communicate the details of the dataset/code/model as part of their
782"
NEW ASSETS,0.9549041713641488,"submissions via structured templates. This includes details about training, license,
783"
NEW ASSETS,0.9560315670800451,"limitations, etc.
784"
NEW ASSETS,0.9571589627959414,"• The paper should discuss whether and how consent was obtained from people whose
785"
NEW ASSETS,0.9582863585118376,"asset is used.
786"
NEW ASSETS,0.9594137542277339,"• At submission time, remember to anonymize your assets (if applicable). You can either
787"
NEW ASSETS,0.9605411499436303,"create an anonymized URL or include an anonymized zip file.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9616685456595265,"14. Crowdsourcing and Research with Human Subjects
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9627959413754228,"Question: For crowdsourcing experiments and research with human subjects, does the paper
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.963923337091319,"include the full text of instructions given to participants and screenshots, if applicable, as
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650507328072153,"well as details about compensation (if any)?
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9661781285231116,"Answer: [NA]
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9673055242390078,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684329199549042,"Guidelines:
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695603156708005,"• The answer NA means that the paper does not involve crowdsourcing nor research with
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706877113866967,"human subjects.
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.971815107102593,"• Including this information in the supplemental material is fine, but if the main contribu-
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9729425028184893,"tion of the paper involves human subjects, then as much detail as possible should be
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9740698985343855,"included in the main paper.
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751972942502819,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763246899661782,"or other labor should be paid at least the minimum wage in the country of the data
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9774520856820744,"collector.
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9785794813979707,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979706877113867,"Subjects
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808342728297632,"Question: Does the paper describe potential risks incurred by study participants, whether
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819616685456595,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830890642615558,"approvals (or an equivalent approval/review based on the requirements of your country or
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842164599774521,"institution) were obtained?
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853438556933484,"Answer: [NA]
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864712514092446,"Justification: the paper does not involve crowdsourcing nor research with human subjects.
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875986471251409,"Guidelines:
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887260428410372,"• The answer NA means that the paper does not involve crowdsourcing nor research with
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898534385569335,"human subjects.
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909808342728298,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992108229988726,"may be required for any human subjects research. If you obtained IRB approval, you
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932356257046223,"should clearly state this in the paper.
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9943630214205186,"• We recognize that the procedures for this may vary significantly between institutions
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954904171364148,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966178128523112,"guidelines for their institution.
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977452085682075,"• For initial submissions, do not include any information that would break anonymity (if
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988726042841037,"applicable), such as the institution conducting the review.
822"
