Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001440922190201729,"Ofﬂine reinforcement learning algorithms promise to be applicable in settings
where a ﬁxed dataset is available and no new experience can be acquired. How-
ever, such formulation is inevitably ofﬂine-data-hungry and, in practice, collecting
a large ofﬂine dataset for one speciﬁc task over one speciﬁc environment is also
costly and laborious. In this paper, we thus 1) formulate the ofﬂine dynamics
adaptation by using (source) ofﬂine data collected from another dynamics to relax
the requirement for the extensive (target) ofﬂine data, 2) characterize the dynamics
shift problem in which prior ofﬂine methods do not scale well, and 3) derive a sim-
ple dynamics-aware reward augmentation (DARA) framework from both model-
free and model-based ofﬂine settings. Speciﬁcally, DARA emphasizes learning
from those source transition pairs that are adaptive for the target environment and
mitigates the ofﬂine dynamics shift by characterizing state-action-next-state pairs
instead of the typical state-action distribution sketched by prior ofﬂine RL meth-
ods. The experimental evaluation demonstrates that DARA, by augmenting re-
wards in the source ofﬂine dataset, can acquire an adaptive policy for the target
environment and yet signiﬁcantly reduce the requirement of target ofﬂine data.
With only modest amounts of target ofﬂine data, our performance consistently
outperforms the prior ofﬂine RL methods in both simulated and real-world tasks."
INTRODUCTION,0.002881844380403458,"1
INTRODUCTION"
INTRODUCTION,0.004322766570605188,"100% 50%
20%
10%
5%
Amount of data used for training 0 50 100"
INTRODUCTION,0.005763688760806916,Normalized score
INTRODUCTION,0.007204610951008645,"BEAR
CQL
MOPO"
INTRODUCTION,0.008645533141210375,"Figure 1:
Solid and dashed
lines denote ofﬂine Medium-
Replay and Medium-Expert
data in D4RL (Walker2d)resp."
INTRODUCTION,0.010086455331412104,"Ofﬂine reinforcement learning (RL) (Levine et al., 2020; Lange
et al., 2012), the task of learning from the previously collected
dataset, holds the promise of acquiring policies without any costly
active interaction required in the standard online RL paradigm.
However, we note that although the active trail-and-error (online
exploration) is eliminated, the performance of ofﬂine RL method
heavily relies on the amount of ofﬂine data that is used for training.
As shown in Figure 1, the performance deteriorates dramatically as
the amount of ofﬂine data decreases. A natural question therefore
arises: can we reduce the amount of the (target) ofﬂine data without
signiﬁcantly affecting the ﬁnal performance for the target task?"
INTRODUCTION,0.011527377521613832,"Bringing the idea from the transfer learning (Pan & Yang, 2010), we assume that we have access
to another (source) ofﬂine dataset, hoping that we can leverage this dataset to compensate for the
performance degradation caused by the reduced (target) ofﬂine dataset. In the ofﬂine setting, pre-
vious work (Siegel et al., 2020; Chebotar et al., 2021) has characterized the reward (goal) differ-
ence between the source and target, relying on the ”conﬂicting” or multi-goal ofﬂine dataset (Fu
et al., 2020), while we focus on the relatively unexplored transition dynamics difference between
the source dataset and the target environment. Meanwhile, we believe that this dynamics shift is
not arbitrary in reality: in healthcare treatment, ofﬂine data for a particular patient is often limited,
whereas we can obtain diagnostic data from other patients with the same case (same reward/goal)"
INTRODUCTION,0.012968299711815562,"∗Equal contribution.
†Corresponding author."
INTRODUCTION,0.01440922190201729,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01585014409221902,"and there often exist individual differences between patients (source dataset with different transition
dynamics). Careful treatment with respect to the individual differences is thus a crucial requirement."
INTRODUCTION,0.01729106628242075,"Given source ofﬂine data, the main challenge is to cope with the transition dynamics difference, i.e.,
strictly tracking the state-action supported by the source ofﬂine data can not guarantee that the same
transition (state-action-next-state) can be achieved in the target environment. However, in the ofﬂine
setting, such dynamics shift is not explicitly characterized by the previous ofﬂine RL methods, where
they typically attribute the difﬁculty of learning from ofﬂine data to the state-action distribution
shift (Chen & Jiang, 2019; Liu et al., 2018). The corresponding algorithms (Fujimoto et al., 2019;
Abdolmaleki et al., 2018; Yu et al., 2020) that model the support of state-action distribution induced
by the learned policy, will inevitably suffer from the transfer problem where dynamics shift happens."
INTRODUCTION,0.018731988472622477,"Our approach is motivated by the well established connection between reward modiﬁcation and
dynamics adaptation (Kumar et al., 2020b; Eysenbach & Levine, 2019; Eysenbach et al., 2021),
which indicates that, by modifying rewards, one can train a policy in one environment and make the
learned policy to be suitable for another environment (with different dynamics). Thus, we propose
to exploit the joint distribution of state-action-next-state: besides characterizing the state-action
distribution shift as in prior ofﬂine RL algorithms, we additionally identify the dynamics (i.e., the
conditional distribution of next-state given current state-action pair) shift and penalize the agent with
a dynamics-aware reward modiﬁcation. Intuitively, this reward modiﬁcation aims to discourage
the learning from these ofﬂine transitions that are likely in source but are unlikely in the target
environment. Unlike the concurrent work (Ball et al., 2021; Mitchell et al., 2021) paying attention to
the ofﬂine domain generalization, we explicitly focus on the ofﬂine domain (dynamics) adaptation."
INTRODUCTION,0.020172910662824207,"Our principal contribution in this work is the characterization of the dynamics shift in ofﬂine RL and
the derivation of dynamics-aware reward augmentation (DARA) framework built on prior model-
free and model-based formulations. DARA is simple and general, can accommodate various ofﬂine
RL methods, and can be implemented in just a few lines of code on top of dataloader at training. In
our ofﬂine dynamics adaptation setting, we also release a dataset, including the Gym-MuJoCo tasks
(Walker2d, Hopper and HalfCheetah), with dynamics (mass, joint) shift compared to D4RL, and
a 12-DoF quadruped robot in both simulator and real-world. With only modest amounts of target
ofﬂine data, we show that DARA-based ofﬂine methods can acquire an adaptive policy for the target
tasks and achieve better performance compared to baselines in both simulated and real-world tasks."
RELATED WORK,0.021613832853025938,"2
RELATED WORK"
RELATED WORK,0.023054755043227664,"Ofﬂine RL describes the setting in which a learner has access to only a ﬁxed dataset of experience,
while no interactive data collection is allowed during policy learning (Levine et al., 2020). Prior
work commonly assumes that the ofﬂine experience is collected by some behavior policies on the
same environment that the learned policy be deployed on. Thus, the main difﬁculty of such ofﬂine
setting is the state-action distribution shift (Fujimoto et al., 2019; Liu et al., 2018). Algorithms
address this issue by following the two main directions: the model-free and model-based ofﬂine RL."
RELATED WORK,0.024495677233429394,"Model-free methods for such setting typically fall under three categories: 1) Typical methods mit-
igate this problem by explicitly (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019) or
implicitly (Siegel et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018) constraining the learned
policy away from OOD state-action pairs. 2) Conservative estimation based methods learn pes-
simistic value functions to prevent the overestimation (Kumar et al., 2020a; Xu et al., 2021). 3)
Importance sampling based methods directly estimate the state-marginal importance ratio and ob-
tain an unbiased value estimation (Zhang et al., 2020; Nachum & Dai, 2020; Nachum et al., 2019b)."
RELATED WORK,0.025936599423631124,"Model-based methods typically eliminate the state-action distribution shift by incorporating a reward
penalty, which relies on the uncertainty quantiﬁcation of the learned dynamics (Kidambi et al., 2020;
Yu et al., 2020). To remove this uncertainty estimation, Yu et al. (2021) learns conservative critic
function by penalizing the values of the generated state-action pairs that are not in the ofﬂinedataset."
RELATED WORK,0.027377521613832854,"These methods, however, deﬁne their objective based on the state-action distribution shift, and ig-
nore the potential dynamics shift between the ﬁxed ofﬂine data and the target MDP. In contrast, we
account for dynamics (state-action-next-state) shift and explicitly propose the dynamics aware re-
ward augmentation. A counterpart, close to our work, is off-dynamics RL (Eysenbach et al., 2021),
where they set up dynamics shift in the interactive environment while we focus on the ofﬂine setting."
RELATED WORK,0.02881844380403458,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.03025936599423631,"3
PRELIMINARIES"
PRELIMINARIES,0.03170028818443804,"We study RL in the framework of Markov decision processes (MDPs) speciﬁed by the tuple M :=
(S, A, r, T, ρ0, γ), where S and A denote the state and action spaces, r(s, a) ∈[−Rmax, Rmax] is
the reward function, T(s′|s, a) is the transition dynamics, ρ0(s) is the initial state distribution, and
γ is the discount factor. The goal in RL is to optimize a policy π(a|s) that maximizes the expected
discounted return ηM(π) := Eτ∼pπ
M(τ) [P∞
t=0 γtr(st, at)], where τ := (s0, a0, s1, a1, ...). We
also deﬁne Q-values Q(s, a) := Eτ∼pπ
M(τ) [P∞
t=0 γtr(st, at)|s0 = s, a0 = a], V-values V (s) :=
Ea∼π(a|s) [Q(s, a)], and the (unnormalized) state visitation distribution dπ
M(s) := P∞
t=0 γtP(s|π,
M, t), where P(s|π, M, t) denotes the probability of reaching state s at time t by running π in M."
PRELIMINARIES,0.03314121037463977,"In the ofﬂine RL problem, we are provided with a static dataset D := {(s, a, r, s′)}, which con-
sists of transition tuples from trajectories collected by running one or more behavioral policies,
denoted by πb, on MDP M.
With a slight abuse of notation, we write D = {(s, a, r, s′) ∼
dD(s)πb(a|s)r(s, a)T(s′|s, a)}, where the dD(s) denotes state-marginal distribution in D. In the
ofﬂine setting, the goal is typically to learn the best possible policy using the ﬁxed ofﬂine dataset."
PRELIMINARIES,0.0345821325648415,"Model-free RL algorithms based on dynamic programming typically perform policy iteration to
ﬁnd the optimal policy. Such methods iteratively conduct 1) policy improvement with GMQ :=
arg maxπ Es∼dπ
M(s),a∼π(a|s) [Q(s, a)] and 2) policy evaluation by iterating the Bellman equation
Q(s, a) = Bπ
MQ(s, a) := r(s, a) + γEs′∼T (s′|s,a),a′∼π(a′|s′) [Q(s′, a′)] over dπ
M(s)π(a|s). Given
off-policy D, we resort to 1) improvement with GDQ := arg maxπ Es∼dD(s),a∼π(a|s) [Q(s, a)] and
2) evaluation by iterating Q(s, a) = Bπ
DQ(s, a) := r(s, a) + γEs′∼TD(s′|s,a),a′∼π(a′|s′) [Q(s′, a′)]
over all (s, a) in D. Speciﬁcally, given any initial Q0, it iterates1"
PRELIMINARIES,0.03602305475504323,"Policy improvement: πk+1 = GDQk,
Policy evaluation: Qk+1 = Bπk+1
D
Qk.
(1)"
PRELIMINARIES,0.037463976945244955,"Model-free ofﬂine RL based on the above iteration suffers from the state-action distribution shift,
i.e., policy evaluation Bπk
D Qk−1 may encounter unfamiliar state action regime that is not covered
by the ﬁxed ofﬂine dataset D, causing erroneous estimation of Qk. Policy improvement GDQk
further exaggerates such error, biasing policy πk+1 towards out-of-distribution (OOD) actions with
erroneously high Q-values. To address this distribution shift, prior works 1) explicitly constrain
policy to be close to the behavior policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019;
Ghasemipour et al., 2021), introducing penalty αD(π(a|s), πb(a|s)) into GD or Bπ
D in Equation 1:"
PRELIMINARIES,0.03890489913544669,"GDQ = arg max
π
Es∼dD(s),a∼π(a|s) [Q(s, a) −αD(π(a|s), πb(a|s))] ,"
PRELIMINARIES,0.040345821325648415,"Bπ
DQ(s, a) = r(s, a) + γEs′∼TD(s′|s,a),a′∼π(a′|s′) [Q(s′, a′) −αD(π(a′|s′), πb(a′|s′))] ,
(2)"
PRELIMINARIES,0.04178674351585014,"where D is a divergence function between distributions over actions (e.g., MMD or KL divergence),
or 2) train pessimistic value functions (Kumar et al., 2020a; Yu et al., 2021; Xu et al., 2021), penal-
izing Q-values at states in the ofﬂine dataset D for actions generated by the current policy π:"
PRELIMINARIES,0.043227665706051875,"Q = arg min
Q
Es∼dD(s),a∼π(a|s) [Q(s, a)] ,
s.t. Q = Bπ
DQ.
(3)"
PRELIMINARIES,0.0446685878962536,"Model-based RL algorithms iteratively 1) model the transition dynamics T(s′|s, a), using the data
collected in M: max ˆT Es,a,s′∼dπ
M(s)π(a|s)T (s′|s,a)[log ˆT(s′|s, a)], and 2) infer a policy π from the
modeled ˆ
M = (S, A, r, ˆT, ρ0, γ), where we assume that r and ρ0 are known, maximizing η ˆ
M(π)
with a planner or the Dyna-style algorithms (Sutton, 1990). In this paper, we focus on the latter."
PRELIMINARIES,0.04610951008645533,"Model-based ofﬂine RL algorithms similarly suffer from OOD state-action (Kidambi et al., 2020;
Cang et al., 2021) if we directly apply policy iteration over ˆT := max ˆT Es,a,s′∼D[log ˆT(s′|s, a)].
Like the conservative estimation approach described in Equation 3, recent conservative model-based
ofﬂine RL methods provide the policy with a penalty for visiting states under the estimated ˆT where
ˆT is likely to be incorrect. Taking u(s, a) as the oracle uncertainty (Yu et al., 2020) that provides a
consistent estimate of the accuracy of model ˆT at (s, a), we can modify the reward function to obtain
a conservative MDP: ˆ
Mc = (S, A, r −αu, ˆT, ρ0, γ), then learn a policy π by maximizing η ˆ
Mc(π)."
PRELIMINARIES,0.04755043227665706,"1For parametric Q-function, we often perform Qk+1 ←arg minQ E(s,a)∼D[(Bπk+1
D
Qk(s, a)−Q(s, a))2]."
PRELIMINARIES,0.04899135446685879,Published as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.05043227665706052,"4
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.05187319884726225,"In standard ofﬂine RL problem, the static ofﬂine dataset D consists of samples {(s, a, r, s′) ∼
dD(s)πb(a|s)r(s, a)T(s′|s, a)}. Although ofﬂine RL methods learn policy for the target MDP
M := (S, A, r, T, ρ0, γ) without (costly) online data, as we shown in Figure 1, it requires a fair
amount of (target) ofﬂine data D collected on M. Suppose we have another (source) ofﬂine dataset
D′, consisting of samples {(s, a, r, s′) ∼dD′(s)πb′(a|s)r(s, a)T ′(s′|s, a)} collected by the behav-
ior policy πb′ on MDP M ′ := (S, A, r, T ′, ρ0, γ), then we hope the transfer of knowledge between
ofﬂine dataset {D′ ∪D} can reduce the data requirements on D for learning policy for the target M."
DYNAMICS SHIFT IN OFFLINE RL,0.053314121037463975,"4.1
DYNAMICS SHIFT IN OFFLINE RL"
DYNAMICS SHIFT IN OFFLINE RL,0.05475504322766571,"Although ofﬂine RL methods in Section 3 have incorporated the state-action distribution constrained
backups (policy constraints or conservative estimation), they also fail to learn an adaptive policy for
the target MDP M with the mixed datasets {D′ ∪D}, as we show in Figure 4 (Appendix). We
attribute this failure to the dynamics shift (Deﬁnition 2) between D′ and M in this adaptation setting."
DYNAMICS SHIFT IN OFFLINE RL,0.056195965417867436,"Deﬁnition 1 (Empirical MDP) An empirical MDP estimated from D is ˆ
M := (S, A, r, ˆT, ρ0, γ)
where ˆT = max ˆT Es,a,s′∼D[log ˆT(s′|s, a)] and ˆT(s′|s, a) = 0 for all (s, a, s′) not in dataset D."
DYNAMICS SHIFT IN OFFLINE RL,0.05763688760806916,"Deﬁnition 2 (Dynamics shift) Let ˆ
M := (S, A, r, ˆT, ρ0, γ) be the empirical MDP estimated from
D. To evaluate a policy π for M := (S, A, r, T, ρ0, γ) with ofﬂine dataset D, we say that the
dynamics shift (between D and M) in ofﬂine RL happens if there exists at least one transition pair
(s, a, s′) ∈{(s, a, s′) : dπ
ˆ
M(s)π(a|s) ˆT(s′|s, a) > 0} such that ˆT(s′|s, a) ̸= T(s′|s, a)."
DYNAMICS SHIFT IN OFFLINE RL,0.059077809798270896,"In practice, for a stochastic M and any ﬁnite ofﬂine data D collected in M, there always exists the
dynamics shift. The main concern is that ﬁnite samples are always not sufﬁcient to exactly model
stochastic dynamics. Following Fujimoto et al. (2019), we thus assume both MDPs M and M ′ are
deterministic, which means the empirical ˆ
M and ˆ
M ′ are both also deterministic. More importantly,
such assumption enables us to explicitly characterize the dynamics shift under ﬁnite ofﬂine samples."
DYNAMICS SHIFT IN OFFLINE RL,0.06051873198847262,"Lemma 1 Under deterministic transition dynamics, there is no dynamics shift between D and M."
DYNAMICS SHIFT IN OFFLINE RL,0.06195965417867435,"For ofﬂine RL tasks, prior methods generally apply Bπ
DQ along with the state-action distribution
correction (Equations 2 and 3), which overlooks the potential dynamics shift between the (source)
ofﬂine dataset and the target MDP (e.g., D′ →M). As a result, these methods do not scale well to
the setting in which dynamics shift happens, e.g., learning an adaptive policy for M with (source) D′."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.06340057636887608,"4.2
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.06484149855907781,"From the model-free (policy iteration) view, an exact policy evaluation on M is characterized by
iterating Q(s, a) = Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.06628242074927954,"MQ(s, a) for all (s, a) such that dπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.06772334293948126,"M(s)π(a|s) > 0. Thus, to formalize the pol-
icy evaluation with ofﬂine D or D′ (for an adaptive π on target M), we require that Bellman operator
Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.069164265129683,"DQ(s, a) or Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07060518731988473,"D′Q(s, a) approximates the oracle Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07204610951008646,"MQ(s, a) for all (s, a) in Sπ or S′"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07348703170028818,"π, where Sπ
and S′"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07492795389048991,"π denote the sets {(s, a) : dD(s)π(a|s) > 0} and {(s, a) : dD′(s)π(a|s) > 0} respectively."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07636887608069164,"1) To evaluate a policy π for M with D (i.e., calling the Bellman operator Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.07780979827089338,"D), notable model-
free ofﬂine method BCQ (Fujimoto et al., 2019) translates the requirement of Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.0792507204610951,D = Bπ
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08069164265129683,"M into the
requirement of ˆT(s′|s, a) = T(s′|s, a). Note that under deterministic environments, we have the
property that for all (s, a, s′) in ofﬂine data D, ˆT(s′|s, a) = T(s′|s, a) (Lemma 1). As a result, such
property permits BCQ to evaluate a policy π by calling Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08213256484149856,"D, replacing the oracle Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08357348703170028,"M, meanwhile
constraining Sπ to be a subset of the support of dD(s)πb(a|s). This means a policy π which only
traverses transitions contained in (target) ofﬂine data D, can be evaluated on M without error."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08501440922190202,"2) To evaluate a policy π for M with D′ (i.e., calling the Bellman operator Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08645533141210375,"D′), we have lemma 2:"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.08789625360230548,Lemma 2 Dynamics shift produces that Bπ
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.0893371757925072,"D′Q(s, a) ̸= Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.09077809798270893,"MQ(s, a) for some (s, a) in S′ π."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.09221902017291066,"With the ofﬂine data D′, lemma 2 suggests that the above requirement Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.0936599423631124,D′ = Bπ
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.09510086455331412,"M becomes infea-
sible, which limits the practical applicability of prior ofﬂine RL methods under the dynamics shift."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.09654178674351585,Published as a conference paper at ICLR 2022
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.09798270893371758,"To be speciﬁc, characterizing an adaptive policy for target MDP M with D′ moves beyond the reach
of the off-policy evaluation based on iterating Q = Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.0994236311239193,"D′Q (Equations 2 and 3). Such iteration may
cause the evaluated Q (or learned policy π) overﬁts to ˆT ′ and struggle to adapt to the target T. To
overcome the dynamics shift, we would like to resort an additional compensation ∆ˆT ′,T such that Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10086455331412104,"D′Q(s, a) + ∆ˆT ′,T (s, a) = Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10230547550432277,"MQ(s, a)
(4)"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.1037463976945245,"for all (s, a) in S′"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10518731988472622,"π. Thus, we can apply Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10662824207492795,"D′Q + ∆ˆT ′,T to act as a substitute for the oracle Bπ MQ."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10806916426512968,"From the model-based view, the oracle ηM(π) (calling the Bellman operator Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.10951008645533142,"M on the target M)
and the viable η ˆ
M ′(π) (calling Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11095100864553314,"ˆ
M ′ on the estimated ˆ
M ′ from source D′) have the followinglemma."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11239193083573487,"Lemma 3 Let Bπ
MV (s) = Ea∼π(a|s)

r(s, a) + γEs′∼T (s′|s,a) [V (s′)]

. For any π, we have:"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.1138328530259366,"η ˆ
M ′(π) = ηM(π) + Es∼dπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11527377521613832,"ˆ
M′(s)

Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11671469740634005,"ˆ
M ′VM(s) −Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11815561959654179,"MVM(s)

."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.11959654178674352,"Lemma 3 states that if we maximize η ˆ
M ′(π) subject to |Es∼dπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.12103746397694524,"ˆ
M′(s)[Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.12247838616714697,"ˆ
M ′VM(s) −Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.1239193083573487,"MVM(s)]| ≤ϵ,
ηM(π) will be improved. If F is a set of functions f : S →R that contains VM, then we have
Es∼dπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.12536023054755044,"ˆ
M′(s)

Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.12680115273775217,"ˆ
M ′VM(s) −Bπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.1282420749279539,"MVM(s)
 ≤γEs,a∼dπ"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.12968299711815562,"ˆ
M′(s)π(a|s)
h
dF( ˆT ′(s′|s, a), T(s′|s, a))
i
,
(5)"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.13112391930835735,"where dF( ˆT ′(s′|s, a), T(s′|s, a)) = supf∈F |Es′∼ˆT ′(s′|s,a) [f(s′)]−Es′∼T (s′|s,a) [f(s′)] |, which is
the integral probability metric (IPM). Note that if we directly follow the admissible error assumption
in MOPO (Yu et al., 2020) i.e., assuming dF( ˆT ′(s′|s, a), T(s′|s, a)) ≤u(s, a) for all (s, a), this
would be too restrictive: given that ˆT ′ is estimated from the source ofﬂine samples collected under
T ′, not the target T, thus such error would not decrease as the source data increases. Further, we ﬁnd"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.13256484149855907,"dF( ˆT ′(s′|s, a), T(s′|s, a)) ≤dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) + dF( ˆT(s′|s, a), T(s′|s, a)).
(6)"
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.1340057636887608,"Thus, we can bound the dF( ˆT ′, T) term with the admissible error assumption over dF( ˆT, T), as in
MOPO, and the auxiliary constraints dF( ˆT ′, ˆT). See next section for the detailed implementation."
DYNAMICS SHIFT IN MODEL-FREE AND MODEL-BASED OFFLINE FORMULATIONS,0.13544668587896252,"In summary, we show that both prior ofﬂine model-free and model-based formulations suffer from
the dynamics shift, which also suggests us to learn a modiﬁcation (∆or dF) to eliminate this shift."
DYNAMICS-AWARE REWARD AUGMENTATION,0.13688760806916425,"5
DYNAMICS-AWARE REWARD AUGMENTATION"
DYNAMICS-AWARE REWARD AUGMENTATION,0.138328530259366,"In this section, we propose the dynamics-aware reward augmentation (DARA), a simple data aug-
mentation procedure based on prior (model-free and model-based) ofﬂine RL methods. We ﬁrst
provide an overview of our ofﬂine reward augmentation motivated by the compensation ∆ˆT ′,T in"
DYNAMICS-AWARE REWARD AUGMENTATION,0.13976945244956773,"Equation 4 and the auxiliary constraints dF( ˆT ′, ˆT) in Equation 6, and then describe its theoretical
derivation in both model-free and model-based formulations. With the (reduced) target ofﬂine data
D and the source ofﬂine data D′, we summarize the overall DARA framework in Algorithm 1."
DYNAMICS-AWARE REWARD AUGMENTATION,0.14121037463976946,Algorithm 1 Framework for Dynamics-Aware Reward Augmentation (DARA)
DYNAMICS-AWARE REWARD AUGMENTATION,0.14265129682997119,Require: Target ofﬂine data D (reduced) and source ofﬂine data D′
DYNAMICS-AWARE REWARD AUGMENTATION,0.1440922190201729,"1: Learn classiﬁers (qsas and qsa) that distinguish source data D′ from target data D. (See Appendix A.1.3)
2: Set dynamics-aware ∆r(st, at, st+1) = log
qsas(source|st,at,st+1)"
DYNAMICS-AWARE REWARD AUGMENTATION,0.14553314121037464,"qsas(target|st,at,st+1) −log qsa(source|st,at)"
DYNAMICS-AWARE REWARD AUGMENTATION,0.14697406340057637,"qsa(target|st,at) .
3: Modify rewards for all (st, at, rt, st+1) in D′: rt ←rt −η∆r.
4: Learn policy with {D ∪D′} using prior model-free or model-based ofﬂine RL algorithms."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1484149855907781,"5.1
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.14985590778097982,"Motivated by the well established connection of RL and probabilistic inference (Levine, 2018),
we ﬁrst cast the model-free RL problem as that of inference in a particular probabilistic model.
Speciﬁcally, we introduce the binary random variable O that denotes whether the trajectory τ :="
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15129682997118155,Published as a conference paper at ICLR 2022
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15273775216138327,"(s0, a0, s1, ...) is optimal (O = 1) or not (O = 0). The likelihood of a trajectory can then be modeled
as p(O = 1|τ) = exp (P"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15417867435158503,"t rt/η), where rt := r(st, at) and η > 0 is a temperature parameter."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15561959654178675,"(Reward Augmentation with Explicit Policy/Value Constraints) We now introduce a variational
distribution pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15706051873198848,"ˆ
M ′(τ) = p(s0) Q"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1585014409221902,"t=1 ˆT ′(st+1|st, at)π(at|st) to approximate the posterior distribution
pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.15994236311239193,"M(τ|O = 1), which leads to the evidence lower bound of log pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.16138328530259366,M(O = 1):
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1628242074927954,log pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1642651296829971,M(O = 1) = log Eτ∼pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.16570605187319884,M(τ) [p(O = 1|τ)] ≥Eτ∼pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.16714697406340057,"ˆ
M′(τ) """
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1685878962536023,log p(O = 1|τ) + log pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.17002881844380405,"M(τ)
pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.17146974063400577,"ˆ
M ′(τ) #"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1729106628242075,= Eτ∼pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.17435158501440923,"ˆ
M′(τ) ""X t "
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.17579250720461095,rt/η −log
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.17723342939481268,"ˆT ′(st+1|st, at)
T(st+1|st, at) !# .
(7)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1786743515850144,"Since we are interested in inﬁnite horizon problems, we introduce the discount factor γ and take
the limit of steps in each rollout, i.e., H →∞. Thus, the RL problem on the MDP M, cast as
the inference problem arg maxπ log pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.18011527377521613,"M(O = 1), can be stated as a maximum of the lower bound Eτ∼pπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.18155619596541786,"ˆ
M′(τ)
hP∞
t=0 γt 
rt −η log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1829971181556196,"ˆT ′(st+1|st,at)
T (st+1|st,at)
i
. This is equivalent to an RL problem on ˆ
M ′ with"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1844380403458213,"the augmented reward r ←r(s, a) −η log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.18587896253602307,"ˆT ′(s′|s,a)
T (s′|s,a) . Intuitively, the −η log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1873198847262248,"ˆT ′(s′|s,a)
T (s′|s,a) term discour-
ages transitions (state-action-next-state) in D′ that have low transition probability in the target M.
In the model-free ofﬂine setting, we can add the explicit policy or Q-value constraints (Equations-
2 and 3) to mitigate the OOD state-actions. Thus, such formulation allows the oracle Bπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.18876080691642652,"M to be re-
expressed by Bπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.19020172910662825,D′ and the modiﬁcation log
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.19164265129682997,"ˆT ′
T , which makes the motivation in Equation 4 practical."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1930835734870317,"(Reward Augmentation with Implicit Policy Constraints) If we introduce the variational distri-
bution pπ′"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.19452449567723343,"ˆ
M ′(τ) := p(s0) Q"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.19596541786743515,"t=1 ˆT ′(st+1|st, at)π′(at|st), we can recover the weighted-regression-
style (Wang et al., 2020; Peng et al., 2019; Abdolmaleki et al., 2018; Peters et al., 2010) objective by
maximizing J (π′, π) := Eτ∼pπ′"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.19740634005763688,"ˆ
M′(τ)
hP∞
t=0 γt 
rt −η log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.1988472622478386,"ˆT ′(st+1|st,at)
T (st+1|st,at) −η log π′(at|st)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.20028818443804033,"π(at|st)
i
(lower"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.2017291066282421,bound of log pπ
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.20317002881844382,"M(O = 1)). Following the Expectation Maximization (EM) algorithm, we can maxi-
mize J (π′, π) by iteratively (E-step) improving J (π′, ·) w.r.t. π′ and (M-step) updating π w.r.t. π′."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.20461095100864554,"(E-step) We deﬁne ˜Q(s, a, s′) = Eτ∼pπ′"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.20605187319884727,"ˆ
M′(τ)
hP"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.207492795389049,t γt log
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.20893371757925072,"ˆT ′(s′|s,a)
T (s′|s,a) |s0 = s, a0 = a, s1 = s′i
. Then,"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21037463976945245,"given ofﬂine data D′, we can rewrite J (π′, ·) as a constrained objective (Abdolmaleki et al., 2018):"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21181556195965417,"max
π′ EdD′(s)π′(a|s) ˆT ′(s′|s,a)
h
Q(s, a) −η ˜Q(s, a, s′)
i
,
s.t. Es∼dD′(s) [DKL (π′(a|s)∥π(a|s))] ≤ϵ."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.2132564841498559,"When considering a ﬁxed π, the above optimization over π′ can be solved analytically (Vieillard
et al., 2020; Geist et al., 2019; Peng et al., 2019). The optimal π′
∗is then given by π′
∗(a|s) ∝
π(a|s) exp (Q(s, a)) exp(−η ˜Q(s, a, ˆT ′(s′|s, a))). As the policy evaluation in Equation 1 (Footnote-
2), we estimate Q(s, a) and ˜Q(s, a, s′) by minimizing the Bellman error with ofﬂine samples in D′."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21469740634005763,"(M-step) Then, we can project π′
∗onto the manifold of the parameterized π:"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21613832853025935,"arg min
π
Es∼dD′(s) [DKL (π′
∗(a|s)∥π(a|s))]"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21757925072046108,"= arg max
π
Es,a,s′∼D′"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.21902017291066284,"h
log π(a|s) exp (Q(s, a)) exp

−η ˜Q(s, a, s′)
i
.
(8)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.22046109510086456,"From the regression view, prior work MPO (Abdolmaleki et al., 2018) infers actions with Q-value
weighted regression, progressive approach compared to behavior cloning; however, such paradigm
lacks the ability to capture transition dynamics. We explicitly introduce the exp(−η ˜Q(s, a, s′))
term, which as we show in experiments, is a crucial component for eliminating the dynamics shift."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.2219020172910663,"Implementation: In practice, we adopt ofﬂine samples in D to approximate the true dynamics T of
M, and introduce a pair of binary classiﬁers, qsas(·|s, a, s′) and qsa(·|s, a), to replace log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.22334293948126802,"ˆT ′(s′|s,a)
T (s′|s,a) as"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.22478386167146974,in Eysenbach et al. (2021): log
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.22622478386167147,"ˆT ′(s′|s,a)
T (s′|s,a) = log qsas(source|s,a,s′)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.2276657060518732,"qsas(target|s,a,s′) −log qsa(source|s,a)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.22910662824207492,"qsa(target|s,a) . (See Appendix-
A.1.3 for details). Although the amount of data D sampled from the target M is reduced in our prob-
lem setup, we experimentally ﬁnd that such classiﬁers are sufﬁcient to achieve good performance."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-FREE FORMULATION,0.23054755043227665,Published as a conference paper at ICLR 2022
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.23198847262247838,"5.2
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.2334293948126801,"Following Equation 6, we then characterize the dynamics shift compensation term as in the above
model-free analysis in the model-based ofﬂine formulation. We will ﬁnd that across different deriva-
tions, our reward augmentation ∆r has always maintained the functional consistency and simplicity."
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.23487031700288186,"Following MOPO, we assume F = {f : ∥f∥∞≤1}, then we have dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) =
DTV( ˆT ′(s′|s, a), ˆT(s′|s, a)) ≤(DKL( ˆT ′(s′|s, a), ˆT(s′|s, a))/2)
1
2 , where DTV is the total variance
distance. Then we introduce the admissible error u(s, a) such that dF( ˆT(s′|s, a), T(s′|s, a)) ≤
u(s, a) for all (s, a), and η and δ such that (DKL( ˆT ′, ˆT)/2)
1
2 ≤ηDKL( ˆT ′, ˆT) + δ. Following
Lemma 3, we thus can maximize the following lower bound with the samples in ˆ
M ′ (λ := γRmax"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.23631123919308358,1−γ ):
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.2377521613832853,"ηM(π) ≥Es,a,s′∼dπ"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.23919308357348704,"ˆ
M′(s)π(a|s) ˆT ′(s′|s,a) """
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.24063400576368876,"r(s, a) −ηλ log"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.2420749279538905,"ˆT ′(s′|s, a)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.24351585014409222,"ˆT(s′|s, a)
−λu(s, a) −λδ # .
(9)"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.24495677233429394,"Implementation: We model the dynamics ˆT ′ and ˆT with an ensemble of 2*N parameterized Gaus-
sian distributions: N i"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.24639769452449567,"ˆT ′(µθ′(s, a), Σφ′(s, a)) and N i"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.2478386167146974,"ˆT (µθ(s, a), Σφ(s, a)), where i ∈[1, N]. We
approximate u with the maximum standard deviation of the learned models in the ensemble:
u(s, a) = maxN
i=1 ∥Σφ(s, a)∥F, omit the training-independent δ, and treat λ as a hyperparameter
as in MOPO. For the log ˆT ′"
DYNAMICS-AWARE REWARD AUGMENTATION IN MODEL-BASED FORMULATION,0.24927953890489912,"ˆT term, we resort to the above classiﬁers (qsas and qsa) in model-free set-
ting. (See Appendix-A.3.2 for comparison between using classiﬁers and estimated-dynamics ratio.)"
EXPERIMENTS,0.2507204610951009,"6
EXPERIMENTS"
EXPERIMENTS,0.2521613832853026,"We present empirical demonstrations of our dynamics-aware reward augmentation (DARA) in a
variety of settings. We start with two simple control experiments that illustrate the signiﬁcance of
DARA under the domain (dynamics) adaptation setting. Then we incorporate DARA into state-of-
the-art (model-free and model-based) ofﬂine RL methods and evaluate the performance on the D4RL
tasks. Finally, we compare our framework to several cross-domain-based baselines on simulated
and real-world tasks. Note that for the dynamics adaptation, we also release a (source) dataset as a
complement to D4RL, along with the quadruped robot dataset in simulator (source) and real (target)."
EXPERIMENTS,0.25360230547550433,"6.1
HOW DOES DARA HANDLE THE DYNAMICS SHIFT IN OFFLINE SETTING?"
EXPERIMENTS,0.25504322766570603,"source/target goal ×
×"
EXPERIMENTS,0.2564841498559078,w/o Aug.
EXPERIMENTS,0.2579250720461095,source target DARA
EXPERIMENTS,0.25936599423631124,Q (w/o Aug.) Q (DARA)
EXPERIMENTS,0.260806916426513,"40
20
0"
EXPERIMENTS,0.2622478386167147,"(s1, a1)"
EXPERIMENTS,0.26368876080691644,"(s2, a2)"
EXPERIMENTS,0.26512968299711814,"Figure 2: External dynamics shift: (left) source
and target MDPs (target contains an obstacle rep-
resented with the dashed line); (middle) top plots
(w/o Aug.) depict the trajectories that are gen-
erated by the learned policy with vanilla MPO;
(middle) bottom plots (DARA) depict the tra-
jectories that are generated by the learned pol-
icy with DARA-based MPO; (right) learned Q-
values on the state-action pairs in left subﬁgure."
EXPERIMENTS,0.2665706051873199,"Restricted 
in target
Time step"
EXPERIMENTS,0.2680115273775216,"0.26
0.26
State[11] 0 r"
EXPERIMENTS,0.26945244956772335,"Figure 3: Internal dynamics shift: (left) source
and target MDPs (range of the right-back-leg of
the ant (state[11]) is limited: [−0.52, 0.52] in
source MDP →[−0.26, 0.26] in target MDP);
(right) the solid (orange) line denotes the state
of the right-back-leg over one trajectory col-
lected in source, dashed (blue) line denotes the
learned reward modiﬁcation −∆r over the tra-
jectory, and green and red slices denote transi-
tion pairs where −∆r ≥and −∆r < 0, resp."
EXPERIMENTS,0.27089337175792505,"Here we characterize both external and internal dynamics shifts: In Map tasks (Figure 2 left), the
source dataset D′ is collected in a 2D map and the target D is collected in the same environment but
with an obstacle (the dashed line); In Ant tasks (Figure 3 left), the source dataset D′ is collected using
the Mujoco Ant and the target D is collected with the same Ant but one joint of which is restricted."
EXPERIMENTS,0.2723342939481268,"Using MPO, as an example of ofﬂine RL method, we train a policy on dataset {D′ ∪D} and deploy
the acquired policy in both source and target MDPs. As shown in Figure 2 (middle-top, w/o Aug.),
such training paradigm does not produce an adaptive policy for the target. By modifying rewards in"
EXPERIMENTS,0.2737752161383285,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.27521613832853026,"Table 1: Normalized scores for the (target) D4RL tasks, where our results are averaged over5seeds.
The arrows in each four-tuple indicate whether the current performance has improved (↑) or not (↓)
compared to the previous value. If 1T+10S DARA achieves comparable (less than 10% degradation)
or better performance compared to baseline 10T, we highlight our scores in bold (in each four-tuple)."
EXPERIMENTS,0.276657060518732,"Body Mass Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
EXPERIMENTS,0.2780979827089337,Hopper
EXPERIMENTS,0.27953890489913547,"BEAR
BRAC-p
AWR"
EXPERIMENTS,0.28097982708933716,"Random
11.4
1.0 ↓
4.6 ↑
8.4 ↑
11.0
10.9 ↓
9.6 ↓
11.0 ↑
10.2
10.3 ↑
3.4 ↓
4.5 ↑
Medium
52.1
0.8 ↓
0.9 ↑
1.6 ↑
32.7
29.0 ↓
29.2 ↑
32.9 ↑
35.9
30.9 ↓
20.8 ↓
28.9 ↑
Medium-R
33.7
1.3 ↓
18.2 ↑
34.1 ↑
0.6
5.4 ↑
20.1 ↑
30.8 ↑
28.4
8.8 ↓
4.1 ↓
4.2 ↑
Medium-E
96.3
0.8 ↓
0.6 ↓
1.2 ↑
1.9
34.5 ↑
32.3 ↓
34.7 ↑
27.1
27.0 ↓
26.8 ↓
26.6 ↓"
EXPERIMENTS,0.2824207492795389,Hopper
EXPERIMENTS,0.2838616714697406,"BCQ
CQL
MOPO"
EXPERIMENTS,0.28530259365994237,"Random
10.6
10.6 ↓
8.3 ↓
9.7 ↑
10.8
10.6 ↓
10.2 ↓
10.4 ↑
11.7
4.8 ↓
2.0 ↓
2.1 ↑
Medium
54.5
37.1 ↓
25.7 ↓
38.4 ↑
58.0
43.0 ↓
44.9 ↑
59.3 ↑
28.0
4.1 ↓
5.0 ↑
10.7 ↑
Medium-R
33.1
9.3 ↓
28.7 ↑
32.8 ↑
48.6
9.6 ↓
1.4 ↓
3.7 ↑
67.5
1.0 ↓
5.5 ↑
8.4 ↑
Medium-E
110.9
58 ↓
75.4 ↑
84.2 ↑
98.7
59.7 ↓
53.6 ↓
99.7 ↑
23.7
1.6 ↓
4.8 ↑
5.8 ↑"
EXPERIMENTS,0.28674351585014407,Walker2d
EXPERIMENTS,0.2881844380403458,"BEAR
BRAC-p
AWR"
EXPERIMENTS,0.2896253602305475,"Random
7.3
1.5 ↓
3.1 ↑
3.2 ↑
-0.2
0.0 ↑
1.3 ↑
3.2 ↑
1.5
1.3 ↓
2.0 ↑
2.4 ↑
Medium
59.1
-0.5 ↓
0.6 ↑
0.3 ↓
77.5
6.4 ↓
70.0 ↑
78.0 ↑
17.4
14.8 ↓
17.1 ↑
17.2 ↑
Medium-R
19.2
0.7 ↓
6.5 ↑
7.3 ↑
-0.3
8.5 ↑
9.9 ↑
18.6 ↑
15.5
7.4 ↓
1.6 ↓
1.5 ↓
Medium-E
40.1
-0.1 ↓
1.5 ↑
2.3 ↑
76.9
20.6 ↓
64.1 ↑
77.5 ↑
53.8
35.5 ↓
52.5 ↑
53.3 ↑"
EXPERIMENTS,0.2910662824207493,Walker2d
EXPERIMENTS,0.29250720461095103,"BCQ
CQL
MOPO"
EXPERIMENTS,0.29394812680115273,"Random
4.9
1.8 ↓
4.5 ↑
4.8 ↑
7.0
1.7 ↓
3.2 ↑
3.4 ↑
13.6
-0.2 ↓
-0.1 ↑
-0.1 ↓
Medium
53.1
32.8 ↓
50.9 ↑
52.3 ↑
79.2
42.9 ↓
80.0 ↑
81.7 ↑
17.8
7.0 ↓
5.7 ↓
11.0 ↑
Medium-R
15.0
6.9 ↓
14.9 ↑
15.1 ↑
26.7
4.6 ↓
0.8 ↓
2.0 ↑
39.0
5.1 ↓
3.1 ↓
14.2 ↑
Medium-E
57.5
32.5 ↓
55.2 ↑
57.2 ↑
111.0
49.5 ↓
63.5 ↑
93.3 ↑
44.6
5.3 ↓
5.5 ↑
17.2 ↑"
EXPERIMENTS,0.2953890489913545,"source D′, we show that applying the same training paradigm on the reward augmented data exhibits
a positive transfer ability in Figure 2 (middle-bottom, DARA). In Figure 2 (right), we show that our
DARA produces low Q-values on the obstructive state-action pairs (in left) compared to the vanilla
MPO, which thus prevents the Q-value weighted-regression on these unproductive state-action pairs."
EXPERIMENTS,0.2968299711815562,"More generally, we illustrate how DARA can handle the dynamics adaptation from the reward mod-
iﬁcation view. In Figure 3 (right), the learned reward modiﬁcation −∆r (dashed blue line) clearly
produces a penalty (red slices) on these state-action pairs (in source) that produce infeasible next-
state transitions in the target MDP. If we directly apply prior ofﬂine RL methods, these transitions
that are beyond reach in target and yet are high valued, would yield a negative transfer. Thus, we can
think of DARA as ﬁnding out these transitions that exhibit dynamics shifts and enabling dynamics
adaptation with reward modiﬁcations, e.g., penalizing transitions covered by red slices (−∆r < 0)."
EXPERIMENTS,0.29827089337175794,"6.2
CAN DARA ENABLE AN ADAPTIVE POLICY WITH REDUCED OFFLINE DATA IN TARGET?"
EXPERIMENTS,0.29971181556195964,"To characterize the ofﬂine dynamics shift, we consider the Hopper, Walker2d and Halfcheetah from
the Gym-MuJoCo environment, using ofﬂine samples from D4RL as our target ofﬂine dataset. For
the source dataset, we change the body mass of agents or add joint noise to the motion, and, similar
to D4RL, collect the Random, Medium, Medium-R and Medium-E ofﬂine datasets for the three en-
vironments. Based on various ofﬂine RL algorithms (BEAR, BRAC-p, BCQ, CQL, AWR, MOPO),
we perform the following comparisons: 1) employing the 100% of D4RL data (10T), 2) employing
only 10% of the D4RL data (1T), 3) employing 10% of the D4RL data and 100% of our collected
source ofﬂine data (1T+10S w/o Aug.), and 4) employing 10% of the D4RL data and 100% of our
collected source ofﬂine data along with our reward augmentation (1T+10S DARA). Due to page
limit, here we focus on the dynamics shift concerning the body mass on Walker2d and Hopper. We
refer the reader to appendix for more experimental details, tasks, and more baselines (BC, COMBO)."
EXPERIMENTS,0.3011527377521614,"As shown in Table 1, in most of the tasks, the performance degrades substantially when we decrease
the amount of target ofﬂine data, i.e., 10T →1T. Training with additional ten times source ofﬂine
data (1T+10S w/o Aug.) also does not bring substantial improvement (compensating for the reduced
data in target), which even degrades the performance in some tasks. We believe that such degradation
(compared to 10T) is caused by the lack of target ofﬂine data as well as the dynamics shift (induced
by the source data). Incorporating our reward augmentation, we observe that compared to 1T and
1T+10S w/o Aug. that both use 10% of the target ofﬂine data, our 1T+10S DARA signiﬁcantly
improves the performance across a majority of tasks. Moreover, DARA can achieve comparable or
better performance compared to baseline 10T that training with ten times as much target ofﬂine data."
EXPERIMENTS,0.3025936599423631,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.30403458213256485,"Table 2: Normalized scores in (target) D4RL tasks, where ”Tune” denotes baseline ”ﬁne-tune”. We
observe that with same amount (10%) of target ofﬂine data, DARA greatly outperforms baselines."
EXPERIMENTS,0.30547550432276654,"Body Mass Shift
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
πp ˆT
ˆTπp"
EXPERIMENTS,0.3069164265129683,Hopper
EXPERIMENTS,0.30835734870317005,"BEAR
BRAC-p
BCQ
CQL
MOPO
MABE"
EXPERIMENTS,0.30979827089337175,"Random
0.8
8.4 ↑
6.0
11.0 ↑
8.8
9.7 ↑
31.6
10.4 ↓
0.7
2.1 ↑
10.6
9.0
Medium
0.8
1.6 ↑
22.7
32.9 ↑
31.7
38.4 ↑
44.5
59.3 ↑
0.7
10.7 ↑
48.8
23.1
Medium-R
0.7
34.1 ↑
14.7
30.8 ↑
27.5
32.8 ↑
1.3
3.7 ↑
0.6
8.4 ↑
17.1
20.4
Medium-E
0.9
1.2 ↑
19.2
34.7 ↑
85.9
84.2 ↓
47.6
99.7 ↑
2.2
5.8 ↑
28.1
38.9"
EXPERIMENTS,0.3112391930835735,Walker2d
EXPERIMENTS,0.3126801152737752,"BEAR
BRAC-p
BCQ
CQL
MOPO
MABE"
EXPERIMENTS,0.31412103746397696,"Random
6.6
3.2 ↓
3.9
3.2 ↓
4.7
4.8 ↑
1.1
3.4 ↑
0.1
-0.1 ↓
6.0
-0.2
Medium
0.3
0.3 ↓
76.0
78.0 ↑
28.4
52.3 ↑
72.3
81.7 ↑
-0.2
11.0 ↑
30.1
56.7
Medium-R
1.2
7.3 ↑
10.0
18.6 ↑
10.4
15.1 ↑
1.8
2.0 ↑
0.0
14.2 ↑
13.3
12.5
Medium-E
2.4
2.3 ↓
74.5
77.5 ↑
22.7
57.2 ↑
68.6
93.3 ↑
7.3
17.2 ↑
43.7
82.7"
EXPERIMENTS,0.31556195965417866,"6.3
CAN DARA PERFORM BETTER THAN CROSS-DOMAIN BASELINES?"
EXPERIMENTS,0.3170028818443804,"In Section 6.2, 1T+10S w/o Aug. does not explicitly learn policy for the target dynamics, thus one
proposal (1T+10S ﬁne-tune) for adapting the target dynamics is ﬁne-tuning the model that learned
with source ofﬂine data, using the (reduced) target ofﬂine data. Moreover, we also compare DARA
with the recently proposed MABE (Cang et al., 2021), which is suitable well for our cross-dynamics
setting by introducing behavioral priors πp in the model-based ofﬂine setting. Thus, we implement
two baselines, 1) 1T+10S MABE πp ˆT and 2) 1T+10S MABE ˆTπp, which denote 1) learning πp with
target domain data and ˆT with source domain data, and 2) learning πp with source domain data
and ˆT with target domain data, respectively. We show the results for the Walker (with body mass
shift) in Table 2, and more experiments in Appendix A.3.5. Our results show that DARA achieves
signiﬁcantly better performance than the na¨ıve ﬁne-tune-based approaches in a majority of tasks
(67 ”↑” vs. 13 ”↓”, including results in appendix). On twelve out of the sixteen tasks (including
results in appendix), DARA-based methods outperform the MABE-based methods. We attribute
MABE’s failure to the difﬁculty of the reduced target ofﬂine data, which limits the generalization
of the learned πp or ˆT under such data. However, such reduced data (10% of target) is sufﬁcient to
modify rewards in the source ofﬂine data, which thus encourages better performance for our DARA."
EXPERIMENTS,0.3184438040345821,"Table 3: Average distance cov-
ered in an episode in real robot."
EXPERIMENTS,0.31988472622478387,"(BCQ)
w/o Aug.
DARA"
EXPERIMENTS,0.32132564841498557,"Medium
0.85
1.35 ↑
Medium-E
1.15
1.41 ↑
Medium–R-E
1.27
1.55 ↑"
EXPERIMENTS,0.3227665706051873,"For real-world tasks, we also test DARA in a new ofﬂine dataset
on the quadruped robot (see appendix for details). Note that we
can not access the privileged information (e.g., coordinate) in
real robot, thus the target ofﬂine data (collected in real-world)
does not contain rewards. This means that prior ﬁne-tune-based
and MABE-based methods become unavailable. However, our
reward augmentation frees us from the requisite of rewards in
target domain. We can freely perform ofﬂine training only using the augmented source ofﬂine data as
long as the learned ∆r is sufﬁcient. For comparison, we also employ a baseline (w/o Aug.): directly
deploying the learned policy with source data into the (target) real-world. We present the results
(deployed in real with obstructive stairs) in Table 3 and videos in supplementary material. We can
observe that training with our reward augmentation, the performance can be substantially improved.
Due to page limit, we refer readers to Appendix A.3.6 for more experimental results and discussion."
CONCLUSION,0.3242074927953891,"7
CONCLUSION"
CONCLUSION,0.3256484149855908,"In this paper, we formulate the dynamics shift in ofﬂine RL. Based on prior model-based and model-
free ofﬂine algorithms, we propose the dynamics-award reward augmentation (DARA) framework
that characterizes constraints over state-action-next-state distributions. Empirically we demonstrate
DARA can eliminate the dynamics shift and outperform baselines in simulated and real-world tasks."
CONCLUSION,0.3270893371757925,"In Appendix A.2, we characterize our dynamics-aware reward augmentation from the density reg-
ularization view, which shows that it is straightforward to derive the reward modiﬁcation built on
prior regularized max-return objective e.g., AlgaeDICE (Nachum et al., 2019b). We list some re-
lated works in Table 4, where the majority of the existing work focuses on regularizing state-action
distribution, while dynamics shift receives relatively little attention. Thus, we hope to shift the focus
of the community towards analyzing how dynamics shift affects RL and how to eliminate the effect."
CONCLUSION,0.3285302593659942,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.329971181556196,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3314121037463977,"Our experimental evaluation is conducted with publicly available D4RL (Fu et al., 2020) and Ne-
oRL (Qin et al., 2021). In Appendix A.4 and A.5, we provide the environmental details and training
setup for our real-world sim2real tasks. In supplementary material, we upload our source code and
the collected ofﬂine dataset for the the quadruped robot."
REPRODUCIBILITY STATEMENT,0.33285302593659943,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.33429394812680113,"We thank Zifeng Zhuang, Yachen Kang and Qiangxing Tian for helpful feedback and discussions.
This work is supported by NSFC General Program (62176215)."
REFERENCES,0.3357348703170029,REFERENCES
REFERENCES,0.3371757925072046,"Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, R´emi Munos, Nicolas Heess, and Mar-
tin A. Riedmiller. Maximum a posteriori policy optimisation. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net, 2018."
REFERENCES,0.33861671469740634,"Philip J. Ball, Cong Lu, Jack Parker-Holder, and Stephen J. Roberts. Augmented world models
facilitate zero-shot dynamics generalization from a single ofﬂine environment. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning
Research, pp. 619–629. PMLR, 2021."
REFERENCES,0.3400576368876081,"Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin.
Behavioral priors
and dynamics models: Improving performance and domain transfer in ofﬂine RL.
CoRR,
abs/2106.09119, 2021. URL https://arxiv.org/abs/2106.09119."
REFERENCES,0.3414985590778098,"Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob Varley, Alex
Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, and Sergey Levine. Actionable models:
Unsupervised ofﬂine reinforcement learning of robotic skills. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1518–
1528. PMLR, 2021. URL http://proceedings.mlr.press/v139/chebotar21a.
html."
REFERENCES,0.34293948126801155,"Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pp. 1042–1051. PMLR, 2019."
REFERENCES,0.34438040345821325,"Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action
imitation learning for batch deep reinforcement learning. arXiv preprint arXiv:1910.12179, 2019."
REFERENCES,0.345821325648415,"Erwin Coumans and Yunfei Bai.
Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016–2021."
REFERENCES,0.3472622478386167,"Benjamin Eysenbach and Sergey Levine. If maxent RL is the answer, what is the question? CoRR,
abs/1910.01913, 2019. URL http://arxiv.org/abs/1910.01913."
REFERENCES,0.34870317002881845,"Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa, Sergey Levine, and Ruslan Salakhutdi-
nov. Off-dynamics reinforcement learning: Training for transfer with domain classiﬁers. In 9th
International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May
3-7, 2021. OpenReview.net, 2021."
REFERENCES,0.35014409221902015,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep
data-driven reinforcement learning. CoRR, abs/2004.07219, 2020. URL https://arxiv.
org/abs/2004.07219."
REFERENCES,0.3515850144092219,Published as a conference paper at ICLR 2022
REFERENCES,0.3530259365994236,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2052–2062. PMLR,
2019."
REFERENCES,0.35446685878962536,"Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2160–2169. PMLR,
2019."
REFERENCES,0.3559077809798271,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective ofﬂine and online RL. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,
pp. 3682–3691. PMLR, 2021."
REFERENCES,0.3573487031700288,"Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic al-
gorithms and applications. CoRR, abs/1812.05905, 2018. URL http://arxiv.org/abs/
1812.05905."
REFERENCES,0.35878962536023057,"Behzad Haghgoo, Allan Zhou, Archit Sharma, and Chelsea Finn. Discriminator augmented model-
based reinforcement learning. CoRR, abs/2103.12999, 2021. URL https://arxiv.org/
abs/2103.12999."
REFERENCES,0.36023054755043227,"Atil Iscen, Ken Caluwaerts, Jie Tan, Tingnan Zhang, Erwin Coumans, Vikas Sindhwani, and
Vincent Vanhoucke.
Policies modulating trajectory generators.
In 2nd Annual Conference
on Robot Learning, CoRL 2018, Z¨urich, Switzerland, 29-31 October 2018, Proceedings, vol-
ume 87 of Proceedings of Machine Learning Research, pp. 916–926. PMLR, 2018.
URL
http://proceedings.mlr.press/v87/iscen18a.html."
REFERENCES,0.361671469740634,"Nan Jiang and Jiawei Huang.
Minimax value interval for off-policy evaluation and policy op-
timization.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html."
REFERENCES,0.3631123919308357,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020."
REFERENCES,0.3645533141210375,"Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Ofﬂine reinforcement learning
with ﬁsher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pp. 5774–5783. PMLR, 2021."
REFERENCES,0.3659942363112392,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction.
In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11761–11771,
2019."
REFERENCES,0.36743515850144093,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020a."
REFERENCES,0.3688760806916426,Published as a conference paper at ICLR 2022
REFERENCES,0.3703170028818444,"Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn.
One solution is not
all you need:
Few-shot extrapolation via structured maxent RL.
In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
Advances in Neural Information Processing Systems 33:
Annual Conference on Neu-
ral Information Processing Systems 2020,
NeurIPS 2020,
December 6-12,
2020,
vir-
tual,
2020b.
URL
https://proceedings.neurips.cc/paper/2020/hash/
5d151d1059a6281335a10732fc49620e-Abstract.html."
REFERENCES,0.37175792507204614,"Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45–73. Springer, 2012."
REFERENCES,0.37319884726224783,"Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.
Learn-
ing quadrupedal locomotion over challenging terrain.
Science Robotics, 5(47), 2020.
doi:
10.1126/scirobotics.abc5986. URL https://robotics.sciencemag.org/content/
5/47/eabc5986."
REFERENCES,0.3746397694524496,"Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
CoRR, abs/1805.00909, 2018. URL http://arxiv.org/abs/1805.00909."
REFERENCES,0.3760806916426513,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
Ofﬂine reinforcement learning:
Tutorial, review, and perspectives on open problems.
CoRR, abs/2005.01643, 2020.
URL
https://arxiv.org/abs/2005.01643."
REFERENCES,0.37752161383285304,"Jinxin Liu, Hao Shen, Donglin Wang, Yachen Kang, and Qiangxing Tian. Unsupervised domain
adaptation with dynamics-aware rewards in reinforcement learning. Advances in Neural Informa-
tion Processing Systems, 34, 2021."
REFERENCES,0.37896253602305474,"Qiang
Liu,
Lihong
Li,
Ziyang
Tang,
and
Dengyong
Zhou.
Breaking
the
curse
of
horizon:
Inﬁnite-horizon
off-policy
estimation.
pp.
5361–5371,
2018.
URL
https://proceedings.neurips.cc/paper/2018/hash/
dda04f9d634145a9c68d5dfe53b21272-Abstract.html."
REFERENCES,0.3804034582132565,"Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Ofﬂine meta-
reinforcement learning with advantage weighting. In Marina Meila and Tong Zhang (eds.), Pro-
ceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7780–7791.
PMLR, 2021."
REFERENCES,0.3818443804034582,"Oﬁr Nachum and Bo Dai.
Reinforcement learning via fenchel-rockafellar duality.
CoRR,
abs/2001.01866, 2020. URL http://arxiv.org/abs/2001.01866."
REFERENCES,0.38328530259365995,"Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. arXiv preprint arXiv:1906.04733, 2019a."
REFERENCES,0.38472622478386165,"Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans.
Al-
gaedice: Policy gradient from arbitrary experience.
CoRR, abs/1912.02074, 2019b.
URL
http://arxiv.org/abs/1912.02074."
REFERENCES,0.3861671469740634,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. CoRR, abs/2006.09359, 2020. URL https://arxiv.org/
abs/2006.09359."
REFERENCES,0.38760806916426516,"Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22
(10):1345–1359, 2010. doi: 10.1109/TKDE.2009.191. URL https://doi.org/10.1109/
TKDE.2009.191."
REFERENCES,0.38904899135446686,"Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real trans-
fer of robotic control with dynamics randomization. In 2018 IEEE International Conference on
Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp. 1–8. IEEE,
2018.
doi: 10.1109/ICRA.2018.8460528.
URL https://doi.org/10.1109/ICRA.
2018.8460528."
REFERENCES,0.3904899135446686,Published as a conference paper at ICLR 2022
REFERENCES,0.3919308357348703,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning.
CoRR, abs/1910.00177, 2019.
URL
http://arxiv.org/abs/1910.00177."
REFERENCES,0.39337175792507206,"Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth
AAAI Conference on Artiﬁcial Intelligence, 2010."
REFERENCES,0.39481268011527376,"Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang,
and Yang Yu. Neorl: A near real-world benchmark for ofﬂine reinforcement learning. arXiv
preprint arXiv:2102.00714, 2021."
REFERENCES,0.3962536023054755,"Y. Sakakibara, K. Kan, Y. Hosoda, M. Hattori, and M. Fujie.
Foot trajectory for a quadruped
walking machine. In EEE International Workshop on Intelligent Robots and Systems, Towards a
New Frontier of Applications, pp. 315–322 vol.1, 1990. doi: 10.1109/IROS.1990.262407."
REFERENCES,0.3976945244956772,"Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller.
Keep doing
what worked: Behavioral modelling priors for ofﬂine reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020."
REFERENCES,0.39913544668587897,"Richard S. Sutton.
Integrated architectures for learning, planning, and reacting based on ap-
proximating dynamic programming.
In Bruce W. Porter and Raymond J. Mooney (eds.),
Machine Learning, Proceedings of the Seventh International Conference on Machine Learn-
ing, Austin, Texas, USA, June 21-23, 1990, pp. 216–224. Morgan Kaufmann, 1990.
doi:
10.1016/b978-1-55860-141-3.50030-4."
REFERENCES,0.40057636887608067,"Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. 2017."
REFERENCES,0.4020172910662824,"Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-
policy evaluation. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
Research, pp. 9659–9668. PMLR, 2020. URL http://proceedings.mlr.press/v119/
uehara20a.html."
REFERENCES,0.4034582132564842,"Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R´emi Munos, and Matthieu
Geist. Leverage the average: an analysis of KL regularization in reinforcement learning. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.4048991354466859,"Xingxing Wang. Unitree robotics. https://www.unitree.com/products/a1, 2020."
REFERENCES,0.40634005763688763,"Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020."
REFERENCES,0.40778097982708933,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.
CoRR, abs/1911.11361, 2019. URL http://arxiv.org/abs/1911.11361."
REFERENCES,0.4092219020172911,"Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe ofﬂine
reinforcement learning. CoRR, abs/2107.09003, 2021. URL https://arxiv.org/abs/
2107.09003."
REFERENCES,0.4106628242074928,"Mengjiao Yang, Oﬁr Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation
via the regularized lagrangian. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/488e4104520c6aab692863cc1dba45af-Abstract.html."
REFERENCES,0.41210374639769454,Published as a conference paper at ICLR 2022
REFERENCES,0.41354466858789624,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: model-based ofﬂine policy optimization. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.414985590778098,"Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
COMBO: conservative ofﬂine model-based policy optimization. CoRR, abs/2102.08363, 2021.
URL https://arxiv.org/abs/2102.08363."
REFERENCES,0.4164265129682997,"Hongyin Zhang, Jilong Wang, Zhengqing Wu, Yinuo Wang, and Donglin Wang. Terrain-aware
risk-assessment-network-aided deep reinforcement learning for quadrupedal locomotion in tough
terrain. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 4538–4545. IEEE."
REFERENCES,0.41786743515850144,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans.
Gendice: Generalized ofﬂine esti-
mation of stationary values.
In 8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:
//openreview.net/forum?id=HkxlcnVFwB."
REFERENCES,0.41930835734870314,Published as a conference paper at ICLR 2022
REFERENCES,0.4207492795389049,"A
APPENDIX"
REFERENCES,0.42219020172910665,"A.1
DERIVATION"
REFERENCES,0.42363112391930835,"A.1.1
PROOF OF LEMMA 3"
REFERENCES,0.4250720461095101,"Let Bπ
MV (s) = Ea∼π(a|s)

r(s, a) + γEs′∼T (s′|s,a) [V (s′)]

and r(s) = Ea∼π(a|s) [r(s, a)]. Then,
we have
η ˆ
M ′(π) −ηM(π) = Es0∼ρ0(s)

V ˆ
M ′(s0) −VM(s0)
 = ∞
X"
REFERENCES,0.4265129682997118,"t=0
γtEst∼P (st|π, ˆ
M ′,t)Eat∼π(at|st) [r(st, at)] −Es0∼ρ0(s) [VM(s0)] = ∞
X"
REFERENCES,0.42795389048991356,"t=0
γtEst∼P (st|π, ˆ
M ′,t) [r(st) + VM(st) −VM(st)] −Es0∼ρ0(s) [VM(s0)] = ∞
X"
REFERENCES,0.42939481268011526,"t=0
γtEst∼P (st|π, ˆ
M ′,t)
st+1∼P (st+1|π, ˆ
M ′,t+1)
[r(st) + γVM(st+1) −VM(st)] = ∞
X"
REFERENCES,0.430835734870317,"t=0
γtEst∼P (st|π, ˆ
M ′,t)
st+1∼P (st+1|π, ˆ
M ′,t+1)"
REFERENCES,0.4322766570605187,"
r(st) + γVM(st+1) −
 
r(st) + γEa∼π(a|st),s′∼T (st,a) [VM(s′)]
 = ∞
X"
REFERENCES,0.43371757925072046,"t=0
γtEst∼P (st|π, ˆ
M ′,t)

Bπ"
REFERENCES,0.43515850144092216,"ˆ
M ′VM(st) −Bπ"
REFERENCES,0.4365994236311239,"MVM(st)
"
REFERENCES,0.43804034582132567,= Es∼dπ
REFERENCES,0.43948126801152737,"ˆ
M′(s)

Bπ"
REFERENCES,0.4409221902017291,"ˆ
M ′VM(s) −Bπ"
REFERENCES,0.4423631123919308,"MVM(s)

."
REFERENCES,0.4438040345821326,"A.1.2
MODEL-BASED FORMULATION"
REFERENCES,0.4452449567723343,Here we provide detailed derivation of the lower bound in Equation 9 in the main text.
REFERENCES,0.44668587896253603,Assumption 1 Assume a scale c and a function class F such that VM ∈cF.
REFERENCES,0.44812680115273773,"Following MOPO (Yu et al., 2020), we set F = {f : ∥f∥∞≤1}. In Section Preliminaries, we
have that the reward function is bounded: r(s, a) ∈[−Rmax, Rmax]. Thus, we have ∥VM∥∞≤
P∞
t=0 γtRmax = Rmax"
REFERENCES,0.4495677233429395,1−γ and hence the scale c = Rmax 1−γ .
REFERENCES,0.4510086455331412,"As a direct corollary of Assumption 1 and Equation 5, we have
Es∼dπ"
REFERENCES,0.45244956772334294,"ˆ
M′(s)

Bπ"
REFERENCES,0.4538904899135447,"ˆ
M ′VM(s) −Bπ"
REFERENCES,0.4553314121037464,"MVM(s)
 ≤γc · Es,a∼dπ"
REFERENCES,0.45677233429394815,"ˆ
M′(s)π(a|s)
h
dF( ˆT ′(s′|s, a), T(s′|s, a))
i
. (10)"
REFERENCES,0.45821325648414984,"Further, we ﬁnd
dF( ˆT ′(s′|s, a), T(s′|s, a)) ≤dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) + dF( ˆT(s′|s, a), T(s′|s, a))
(11)"
REFERENCES,0.4596541786743516,"For the ﬁrst term dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) in Equation 12, through Pinsker’s inequality, we have"
REFERENCES,0.4610951008645533,"dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) = DTV( ˆT ′(s′|s, a), ˆT(s′|s, a)) ≤ r"
REFERENCES,0.46253602305475505,"1
2DKL( ˆT ′(s′|s, a), ˆT(s′|s, a))"
REFERENCES,0.46397694524495675,"(12)
To keep consistent with the DARA-based method-free ofﬂine methods, we introduce scale η and
bias δ to eliminate the square root in Equation 12. To be speciﬁc, we assume2 scale η and bias δ"
REFERENCES,0.4654178674351585,"such that
q"
REFERENCES,0.4668587896253602,"1
2DKL( ˆT ′, ˆT) ≤ηDKL( ˆT ′, ˆT) + δ. Thus, we obtain"
REFERENCES,0.46829971181556196,"dF( ˆT ′(s′|s, a), ˆT(s′|s, a)) = DTV( ˆT ′(s′|s, a), ˆT(s′|s, a)) ≤ηDKL( ˆT ′(s′|s, a), ˆT(s′|s, a)) + δ
(13)"
REFERENCES,0.4697406340057637,"2In implementation, we clip the maximum deviation of log"
REFERENCES,0.4711815561959654,"ˆ
T ′(s′|s,a)"
REFERENCES,0.47262247838616717,"ˆ
T (s′|s,a) for each (s, a, s′), which thus makes"
REFERENCES,0.47406340057636887,"DKL( ˆT ′(s′|s, a), ˆT(s′|s, a)) bounded."
REFERENCES,0.4755043227665706,Published as a conference paper at ICLR 2022
REFERENCES,0.4769452449567723,"For the second term dF( ˆT(s′|s, a), T(s′|s, a)) in Equation 11, we assume that we have access to an
oracle uncertainty qualiﬁcation module that provides an upper bound on the error of the estimated
empirical MDP ˆ
M := {S, A, r, ˆT, ρ0, γ}."
REFERENCES,0.4783861671469741,"Assumption 2 Let F be the function class in Assumption 1. We say u : S ×A →R is an admissible
error estimator for ˆT if dF( ˆT(s′|s, a), T(s′|s, a)) ≤u(s, a) for all (s, a)."
REFERENCES,0.47982708933717577,"Thus, we have"
REFERENCES,0.4812680115273775,"Es,a∼dπ"
REFERENCES,0.4827089337175792,"ˆ
M′(s)π(a|s)
h
dF( ˆT(s′|s, a), T(s′|s, a))
i
≤Es,a∼dπ"
REFERENCES,0.484149855907781,"ˆ
M′(s)π(a|s) [u(s, a)]
(14)"
REFERENCES,0.48559077809798273,"Bring Inequations 10, 11, 13, and 14 into Lemma 3, we thus have"
REFERENCES,0.48703170028818443,"ηM(π) ≥Es,a,s′∼dπ"
REFERENCES,0.4884726224783862,"ˆ
M′(s)π(a|s) ˆT ′(s′|s,a) """
REFERENCES,0.4899135446685879,"r(s, a) −ηγc log"
REFERENCES,0.49135446685878964,"ˆT ′(s′|s, a)"
REFERENCES,0.49279538904899134,"ˆT(s′|s, a)
−γcu(s, a) −γcδ #"
REFERENCES,0.4942363112391931,".
(15)"
REFERENCES,0.4956772334293948,"A.1.3
LEARNING CLASSIFIERS"
REFERENCES,0.49711815561959655,"Applying Bayes’ rule, we have"
REFERENCES,0.49855907780979825,"ˆT ′(s′|a, s) := p(s′|s, a, source) = p(source|s, a, s′)p(s, a, s′)"
REFERENCES,0.5,"p(source|s, a)p(s, a)
,"
REFERENCES,0.5014409221902018,"ˆT(s′|a, s) := p(s′|s, a, target) = p(target|s, a, s′)p(s, a, s′)"
REFERENCES,0.5028818443804035,"p(target|s, a)p(s, a)
."
REFERENCES,0.5043227665706052,"Then we parameterize p(·|s, a, s′) and p(·|s, a) with the two classiﬁers qsas and qsa respectively. Us-
ing the standard cross-entropy loss, we learn qsas and qsa with the following optimization objective:"
REFERENCES,0.5057636887608069,"max
E(s,a,s′)∼D′ [log qsas(source|s, a, s′)] + E(s,a,s′)∼D [log qsas(target|s, a, s′)] ,"
REFERENCES,0.5072046109510087,"max
E(s,a)∼D′ [log qsa(source|s, a)] + E(s,a)∼D [log qsa(target|s, a)] ."
REFERENCES,0.5086455331412104,"With the trained qsas and qsa, we have log"
REFERENCES,0.5100864553314121,"ˆT ′(s′|s, a)"
REFERENCES,0.5115273775216138,"ˆT(s′|s, a)
= log qsas(source|s, a, s′)"
REFERENCES,0.5129682997118156,"qsas(target|s, a, s′) −log qsa(source|s, a)"
REFERENCES,0.5144092219020173,"qsa(target|s, a) .
(16)"
REFERENCES,0.515850144092219,"In our implementation, we also clip the above reward modiﬁcation between −10 and 10."
REFERENCES,0.5172910662824207,"A.2
REGULARIZATION VIEW OF DYNAMICS-AWARE REWARD AUGMENTATION"
REFERENCES,0.5187319884726225,"Here we shortly characterize our dynamics-aware reward augmentation from the density regulariza-
tion. Note the standard max-return objective ηM(π) in RL can be written exclusively in terms of the
on-policy distribution dπ"
REFERENCES,0.5201729106628242,"M(s)π(a|s). To introduce an off-policy distribution dD(s)πb(a|s) in the ob-
jective, prior works often incorporate a regularization (penalty): D(dπ"
REFERENCES,0.521613832853026,"M(s)π(a|s)∥dD(s)πb(a|s)), as
in Equations 2 and 3. However, facing dynamics shift, such regularization should take into account
the transition dynamics, which is penalizing D(dπ"
REFERENCES,0.5230547550432276,"M(s)π(a|s)T(s′|s, a)∥dD′(s)πb′(a|s) ˆT ′(s′|s, a)).
From this view, it is also straightforward to derive the reward modiﬁcation built on prior regularized
off-policy max-return objective e.g., the off-policy approach AlgaeDICE (Nachum et al., 2019b)."
REFERENCES,0.5244956772334294,"In Table 4, we provide some related works with respect to the (state-action pair) dD(s)πb(a|s)
regularization and the (state-action-next-state pair) dD′(s)πb′(a|s) ˆT ′(s′|s, a) regularization. We
can ﬁnd that the majority of the existing work focuses on regularizing state-action distribution, while
dynamics shift receives relatively little attention. Thus, we hope to shift the focus of the community
towards analyzing how the dynamics shift affects RL and how to eliminate the effect."
REFERENCES,0.5259365994236311,Published as a conference paper at ICLR 2022
REFERENCES,0.5273775216138329,"Table 4: Some related works with explicit (state-action p(s, a) or state-action-next-state p(s, a, s′))
regularization. More papers with respect to unsupervised RL, inverse RL (imitation learning), meta
RL, multi-agent RL, and hierarchical RL are not included."
REFERENCES,0.5288184438040345,"reg. with dD(s)πb(a|s)
reg. with dD′(s)πb′(a|s) ˆT ′(s′|s, a)
Online:
see summarization in Geist et al.
(2019) and Vieillard et al. (2020)."
REFERENCES,0.5302593659942363,"Eysenbach et al. (2021) (DARC)
Liu et al. (2021) (DARS);
Haghgoo et al. (2021)"
REFERENCES,0.531700288184438,"Ofﬂine (off-policy evaluation):
Fujimoto et al. (2019) (BCQ);
Kumar et al. (2019) (BEAR);
Wu et al. (2019) (BRAC-p);
Abdolmaleki et al. (2018) (MPO);
Peng et al. (2019) (AWR);
Nair et al. (2020) (AWAC);
Wang et al. (2020) (CRR);
Siegel et al. (2020);
Chen et al. (2019); (BAIL)
Kumar et al. (2020a) (CQL);
Xu et al. (2021) (CPQ);
Kostrikov et al. (2021) (Fisher-BRC);
Liu et al. (2018);
Nachum et al. (2019a) (DualDICE);
Nachum et al. (2019b) (AlgaeDICE);
Zhang et al. (2020) (GenDICE);
Yang et al. (2020);
Nachum & Dai (2020);
Jiang & Huang (2020);
Uehara et al. (2020);
Yu et al. (2020) (MOPO);
Kidambi et al. (2020) (MOReL);
Yu et al. (2021) (COMBO);
Cang et al. (2021) (MABE);"
REFERENCES,0.5331412103746398,"A.3
MORE EXPERIMENTS"
REFERENCES,0.5345821325648416,"A.3.1
TRAINING WITH {D′ ∪D}"
REFERENCES,0.5360230547550432,"As we show in Figure 1 in Section Introduction, the performance of prior ofﬂine RL methods dete-
riorates dramatically as the amount of (target) ofﬂine data D decreases. In Figure 4, we show that
directly training with the mixed dataset {D′ ∪D} will not compensate for the deteriorated perfor-
mance caused by the reduced target ofﬂine data, and training with such additional source ofﬂine data
can even lead the performance degradation in some tasks."
REFERENCES,0.537463976945245,"50%
20%
10%
5%
Amount of target data used for training 0 10 20"
REFERENCES,0.5389048991354467,Normalized score
REFERENCES,0.5403458213256485,Medium-Replay
REFERENCES,0.5417867435158501,"CQL with Source
CQL"
REFERENCES,0.5432276657060519,"50%
20%
10%
5%
Amount of target data used for training 0 20 40"
REFERENCES,0.5446685878962536,Normalized score
REFERENCES,0.5461095100864554,Medium-Replay
REFERENCES,0.547550432276657,"MOPO with Source
MOPO"
REFERENCES,0.5489913544668588,"50%
20%
10%
5%
Amount of target data used for training 0 50 100"
REFERENCES,0.5504322766570605,Normalized score
REFERENCES,0.5518731988472623,Medium-Expert
REFERENCES,0.553314121037464,"CQL with Source
CQL"
REFERENCES,0.5547550432276657,"50%
20%
10%
5%
Amount of target data used for training 0 20 40"
REFERENCES,0.5561959654178674,Normalized score
REFERENCES,0.5576368876080692,Medium-Expert
REFERENCES,0.5590778097982709,"MOPO with Source
MOPO"
REFERENCES,0.5605187319884726,"Figure 4: Final performance on the D4RL (Walker2d) task: The orange bars denote the ﬁnal per-
formance with different amount (50%D, 20%D, 10%D, 5%D) of target ofﬂine data; The blue bars
denote the ﬁnal performance of mixing 100% of source ofﬂine data D′ and different amount of target
data x%D (x ∈[50, 20, 10, 5]), i.e., training with {100%D′ ∪x%D}; The red lines denote the ﬁnal
performance of training with 100% of target ofﬂine data D. We can observe that 1) the performance
deteriorates dramatically as the amount of (target) ofﬂine data decreases (100%D (red line) →
50%D (orange bar) →20%D (orange bar) →10%D (orange bar) →5%D (orange bar)), 2) after
training with the additional 100% of source ofﬂine data, {100%D′ ∪x%D}, the ﬁnal performance
is improved in some tasks, but most of the improvement is a pittance compared to the original per-
formance degradation (compared to that training with the 100% of target ofﬂine data, i.e., the red
lines), and 3) what is worse is that adding source ofﬂine data D′ even leads performance degradation
in some tasks, e.g., CQL with 50%D and 20%D in Medium-Random."
REFERENCES,0.5619596541786743,Published as a conference paper at ICLR 2022
REFERENCES,0.5634005763688761,"A.3.2
COMPARISON BETWEEN LEARNING CLASSIFIERS AND LEARNING DYNAMICS (FOR
THE REWARD MODIFICATION)"
REFERENCES,0.5648414985590778,"Table 5: Normalized scores for the Hopper tasks with the body mass (dynamics) shift. Rat. and Cla.
denote estimating the reward modiﬁcation with the estimated-dynamics ratio and learned classiﬁers
(Appendix A.1.3), respectively."
REFERENCES,0.5662824207492796,"Body Mass Shift
BEAR
BRAC-p
AWR
BCQ
CQL
MOPO"
REFERENCES,0.5677233429394812,Hopper
REFERENCES,0.569164265129683,"Rat.
Cla.
Cla.
Cla.
Rat.
Cla.
Rat.
Cla.
Rat.
Cla.
Rat.
Cla."
REFERENCES,0.5706051873198847,"Random
9.9
>
8.4
11.2
>
11.0
3.7
<
4.5
8.5
<
9.7
11.8
>
10.4
1.8
<
2.1
Medium
0.8
<
1.6
31.7
<
32.9
18.0
<
28.9
33.2
<
38.4
45.9
<
59.3
3.1
<
10.7
Medium-R
28.4
<
34.1
36.5
>
30.8
2.5
<
4.2
33.9
>
32.8
2.0
<
3.7
3.8
<
8.4
Medium-E
0.8
<
1.2
50.9
>
34.7
45.8
<
26.6
68.4
<
84.2
107.3
>
99.7
5.7
<
5.8"
REFERENCES,0.5720461095100865,"In Table 5, we show the comparison between learning classiﬁers and learning dynamics (for our
reward modiﬁcation) in the Hopper tasks. We can observe that the two schemes for estimating the
reward modiﬁcation have similar performance. Thus, for simplicity and following Eysenbach et al.
(2021), we adopt the classiﬁers to modify rewards in the source ofﬂine data in our experiments."
REFERENCES,0.5734870317002881,"A.3.3
MORE EXAMPLES WITH RESPECT TO THE REWARD AUGMENTATION"
REFERENCES,0.5749279538904899,Time step
REFERENCES,0.5763688760806917,"0.26
0.26
State 0 r"
REFERENCES,0.5778097982708934,Time step
REFERENCES,0.579250720461095,"0.26 0.26
State[11] 0 r"
REFERENCES,0.5806916426512968,Time step
REFERENCES,0.5821325648414986,"0.26 0.26
State[11] 0 r"
REFERENCES,0.5835734870317003,"Figure 5: We can observe that our reward augmentation 1) encourages (−∆r > 0, i.e., the green
slice parts) these transitions (−0.26 ≤next-state[11] ≤0.26) that have the same dynamics with the
target environment, and 2) discourages (−∆r < 0, i.e., the red slice parts) these transitions that have
different (unreachable) dynamics (next-state[11] ≤−0.26 or next-state[11] ≥0.26) in the target."
REFERENCES,0.5850144092219021,"In Figure 5, we provide more examples with respect to the reward augmentation in the Ant task in
Figure 3 (left)."
REFERENCES,0.5864553314121037,"A.3.4
COMPARISON BETWEEN 10T, 1T, 1T+10S w/o Aug., AND 1T+10S DARA"
REFERENCES,0.5878962536023055,"Based on various ofﬂine RL algorithms (BEAR (Kumar et al., 2019), BRAC-p (Wu et al., 2019),
BCQ (Fujimoto et al., 2019), CQL (Kumar et al., 2020a), AWR (Peng et al., 2019), MOPO (Yu
et al., 2020), BC (behavior cloning), COMBO (Yu et al., 2021)), we provide the additional results in
Tables 6, 7, 8, 9, and 10."
REFERENCES,0.5893371757925072,"Table 6: Normalized scores for the Hopper tasks with the body mass (dynamics) shift. (The com-
parison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)"
REFERENCES,0.590778097982709,"Body Mass Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.5922190201729106,Hopper
REFERENCES,0.5936599423631124,"BC
COMBO"
REFERENCES,0.5951008645533141,"Random
9.8
9.8 ↑
6.9 ↓
10.1 ↑
17.9
0.7 ↓
5.4 ↑
4.6 ↓
Medium
29.0
27.9 ↓
17.6 ↓
25.0 ↑
94.9
1.8 ↓
33.7 ↑
45.7 ↑
Medium-R
11.8
7.8 ↓
7.7 ↓
11.6 ↑
73.1
13.1 ↓
11.0 ↓
27.9 ↑
Medium-E
111.9
21.5 ↓
20.8 ↓
35.7 ↑
111.1
0.8 ↓
14.9 ↑
108.1 ↑"
REFERENCES,0.5965417867435159,Published as a conference paper at ICLR 2022
REFERENCES,0.5979827089337176,Table 7: Normalized scores for the Hopper tasks with the joint noise (dynamics) shift.
REFERENCES,0.5994236311239193,"Joint Noise Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.600864553314121,Hopper
REFERENCES,0.6023054755043228,"BEAR
BRAC-p
AWR"
REFERENCES,0.6037463976945245,"Random
11.4
0.6 ↓
7.4 ↑
4.2 ↓
11.0
10.8 ↓
10.0 ↓
10.8 ↑
10.2
10.1 ↓
3.6 ↓
4.0 ↑
Medium
52.1
0.8 ↓
2.0 ↑
2.0 ↓
32.7
26.6 ↓
27.6 ↑
37.6 ↑
35.9
30.3 ↓
38.8 ↑
41.3 ↑
Medium-R
33.7
2.7 ↓
3.6 ↑
9.9 ↑
0.6
13.4 ↑
89.9 ↑
101.4 ↑
28.4
12.4 ↓
6.7 ↓
7.2 ↑
Medium-E
96.3
0.8 ↓
0.8 ↓
1.4 ↑
1.9
19.8 ↑
57.6 ↑
87.8 ↑
27.1
25.5 ↓
27.0 ↑
27.0 ↓"
REFERENCES,0.6051873198847262,Hopper
REFERENCES,0.6066282420749279,"BCQ
CQL
MOPO"
REFERENCES,0.6080691642651297,"Random
10.6
10.5 ↓
7.0 ↓
9.6 ↑
10.8
10.4 ↓
10.4 ↓
10.8 ↑
11.7
1.5 ↓
1.3 ↓
2.9 ↑
Medium
54.5
45.8 ↓
49.0 ↑
54.4 ↑
58.0
46.2 ↓
58.0 ↑
58.0 ↓
28.0
2.7 ↓
9.2 ↑
17.3 ↑
Medium-R
33.1
13.0 ↓
23.8 ↑
32.0 ↑
48.6
13.6 ↓
2.6 ↓
3.6 ↑
67.5
0.8 ↓
2.3 ↑
6.4 ↑
Medium-E
110.9
44.6 ↓
96 ↑
109 ↑
98.7
50.7 ↓
73.4 ↑
108.9 ↑
23.7
1 ↓
6.1 ↑
7.5 ↑"
REFERENCES,0.6095100864553314,Hopper
REFERENCES,0.6109510086455331,"BC
COMBO"
REFERENCES,0.6123919308357348,"Random
9.8
9.8 ↑
7.5 ↓
9.1 ↑
17.9
0.7 ↓
1.8 ↑
4.9 ↑
Medium
29.0
27.9 ↓
29.0 ↑
29.0 ↑
94.9
1.8 ↓
0.7 ↓
9.6 ↑
Medium-R
11.8
7.8 ↓
8.5 ↑
11.3 ↑
73.1
13.1 ↓
4.0 ↓
9.6 ↑
Medium-E
111.9
21.5 ↓
53.5 ↑
77.9 ↑
111.1
0.8 ↓
34.0 ↑
45.9 ↑"
REFERENCES,0.6138328530259366,"Table 8: Normalized scores for the Walker2d tasks with the body mass (dynamics) shift. (The
comparison results for BEAR, BRAC-p, AWR, CQL, and MOPO are provided in the main text.)"
REFERENCES,0.6152737752161384,"Body Mass Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.6167146974063401,Walker2d
REFERENCES,0.6181556195965417,"BC
COMBO"
REFERENCES,0.6195965417867435,"Random
1.6
0.1 ↓
1.7 ↑
2.7 ↑
7.0
1.8 ↓
2.0 ↑
3.5 ↑
Medium
6.6
5.5 ↓
3.8 ↓
6.6 ↑
75.5
-1.0 ↓
23.9 ↑
36.6 ↑
Medium-R
11.3
6.6 ↓
8.1 ↑
11.0 ↑
56.0
0.1 ↓
11.4 ↑
22.6 ↑
Medium-E
6.4
3.1 ↓
6.0 ↑
6.2 ↑
96.1
-0.9 ↓
-0.1 ↑
-0.1 ↑"
REFERENCES,0.6210374639769453,Table 9: Normalized scores for the Walker2d tasks with the joint noise (dynamics) shift.
REFERENCES,0.622478386167147,"Joint Noise Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.6239193083573487,Walker2d
REFERENCES,0.6253602305475504,"BEAR
BRAC-p
AWR"
REFERENCES,0.6268011527377522,"Random
7.3
2.2 ↓
0.6 ↓
2.6 ↑
-0.2
2.8 ↑
3.3 ↑
8.8 ↑
1.5
0.9 ↓
1.5 ↑
1.5 ↓
Medium
59.1
-0.4 ↓
0.6 ↑
0.1 ↓
77.5
28.8 ↓
55.2 ↑
72.9 ↑
17.4
12.2 ↓
17.2 ↑
17.2 ↓
Medium-R
19.2
0.4 ↓
4 ↑
10.4 ↑
-0.3
6.3 ↑
32.1 ↑
34.8 ↑
15.5
6 ↓
1.4 ↓
2.1 ↑
Medium-E
40.1
-0.2 ↓
0.8 ↑
0.6 ↓
76.9
21.8 ↓
62.3 ↑
74.3 ↑
53.8
40.4 ↓
53 ↑
53.6 ↑"
REFERENCES,0.6282420749279539,Walker2d
REFERENCES,0.6296829971181557,"BCQ
CQL
MOPO"
REFERENCES,0.6311239193083573,"Random
4.9
3.7 ↓
3.4 ↓
5.2 ↑
7
0.5 ↓
2.7 ↑
6.4 ↑
13.6
-0.3 ↓
-0.2 ↑
-0.2 ↓
Medium
53.1
43 ↓
44.9 ↑
52.7 ↑
79.2
43.9 ↓
73.2 ↑
81.2 ↑
17.8
5.8 ↓
7.8 ↑
12.2 ↑
Medium-R
15
5.7 ↓
9.8 ↑
14.6 ↑
26.7
1.8 ↓
1.4 ↓
1.8 ↑
39
0.8 ↓
9.3 ↑
16.4 ↑
Medium-E
57.5
44.5 ↓
40.6 ↓
57.2 ↑
111
46.8 ↓
109.9 ↑
116.5 ↑
44.6
2.9 ↓
15.2 ↑
26.3 ↑"
REFERENCES,0.6325648414985591,Walker2d
REFERENCES,0.6340057636887608,"BC
COMBO"
REFERENCES,0.6354466858789626,"Random
1.6
0.1 ↓
0.9 ↑
1.6 ↑
7.0
1.8 ↓
0.1 ↓
1.5 ↑
Medium
6.6
5.5 ↓
6.4 ↑
6.5 ↑
75.5
-1.0 ↓
0.4 ↑
0.7 ↑
Medium-R
11.3
6.6 ↓
4.6 ↓
10.4 ↑
56.0
0.1 ↓
5.6 ↑
7.4 ↑
Medium-E
6.4
3.1 ↓
6.2 ↑
6.4 ↑
96.1
-0.9 ↓
0.8 ↑
-0.1 ↓"
REFERENCES,0.6368876080691642,Published as a conference paper at ICLR 2022
REFERENCES,0.638328530259366,Table 10: Normalized scores for the Halfcheetah tasks with the joint noise (dynamics) shift.
REFERENCES,0.6397694524495677,"Joint Noise Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.6412103746397695,Halfcheetah
REFERENCES,0.6426512968299711,"BEAR
BRAC-p
AWR"
REFERENCES,0.6440922190201729,"Random
25.1
17.8 ↓
25.0 ↑
25.1 ↑
24.1
10.0 ↓
25.0 ↑
26.7 ↑
2.5
2.7 ↑
3.1 ↑
48.9 ↑
Medium
41.7
-0.2 ↓
0.8 ↑
1.5 ↑
43.8
43.0 ↓
52.4 ↑
53.0 ↑
37.4
38.2 ↑
48.7 ↑
37.4 ↓
Medium-R
38.6
9.3 ↓
-0.6 ↓
-0.5 ↑
45.4
2.5 ↓
-2.3 ↓
45.3 ↑
40.3
2.6 ↓
2.3 ↓
2.3 ↓
Medium-E
53.4
-1.2 ↓
1.0 ↑
-1.4 ↓
44.2
6.9 ↓
0.9 ↓
45.3 ↑
52.7
32.2 ↓
80.6 ↑
79.2 ↓"
REFERENCES,0.6455331412103746,Halfcheetah
REFERENCES,0.6469740634005764,"BCQ
CQL
MOPO"
REFERENCES,0.6484149855907781,"Random
2.2
2.3 ↑
2.2 ↓
2.3 ↑
35.4
-2.3 ↓
-2.4 ↓
10.4 ↑
35.4
2.3 ↓
1.2 ↓
1.1 ↓
Medium
40.7
37.6 ↓
40.0 ↑
48.6 ↑
44.4
35.4 ↓
40.7 ↑
52.6 ↑
42.3
3.2 ↓
3.5 ↓
5.3 ↑
Medium-R
38.2
1.1 ↓
39.4 ↑
41.3 ↑
46.2
0.6 ↓
2.0 ↑
1.9 ↓
53.1
-0.1 ↓
2.6 ↑
4.2 ↑
Medium-E
64.7
37.3 ↓
55.3 ↑
76.9 ↑
62.4
-3.3 ↓
7.7 ↑
1.7 ↓
63.3
4.2 ↓
1.5 ↓
7.2 ↑"
REFERENCES,0.6498559077809798,Halfcheetah
REFERENCES,0.6512968299711815,"BC
COMBO"
REFERENCES,0.6527377521613833,"Random
2.1
2.0 ↓
2.2 ↑
2.2 ↑
38.8
24.0 ↓
18.7 ↓
20.3 ↑
Medium
36.1
36.5 ↑
49.4 ↑
49.8 ↑
54.2
15.7 ↓
14.9 ↓
15.9 ↑
Medium-R
38.4
36.5 ↓
24.6 ↓
15.7 ↓
55.1
-2.6 ↓
-2.4 ↑
4.8 ↑
Medium-E
35.8
36.3 ↑
49.0 ↑
49.3 ↑
90.0
4.4 ↓
6.5 ↑
11.1 ↑"
REFERENCES,0.654178674351585,"A.3.5
COMPARISON WITH THE CROSS-DOMAIN BASED BASELINES"
REFERENCES,0.6556195965417867,"In Tables 11 and 12, we provide the comparison between our DARA-based methods, ﬁne-tune
based methods, and MABE-based methods in Hopper and Walker2d tasks, over the dynamics shift
concerning the joint noise of motion. We can observe that in a majority of tasks, our DARA-
based methods outperforms the ﬁne-tune-based method (67 ”↑” vs. 13 ”↓”, including the results in
the main text). Moreover, our DARA can achieve comparable or better performance compared to
MABE-based baselines on eleven out of sixteen tasks (including the results in the main text)."
REFERENCES,0.6570605187319885,"Table 11: Normalized scores in the (target) D4RL Hopper tasks with the joint noise shift., where
”Tune” denotes baseline ”ﬁne-tune”."
REFERENCES,0.6585014409221902,"Joint Noise Shift
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
πp ˆT
ˆTπp"
REFERENCES,0.659942363112392,Hopper
REFERENCES,0.6613832853025937,"BEAR
BRAC-p
BCQ
CQL
MOPO
MABE"
REFERENCES,0.6628242074927954,"Random
0.8
4.2 ↑
6.4
10.8 ↑
8.1
9.6 ↑
32.2
10.8 ↓
0.6
2.9 ↑
10.8
8.1
Medium
1.9
2.0 ↑
44.9
37.6 ↓
47.7
54.4 ↑
52.5
58.0 ↑
0.8
17.3 ↑
63.5
57.7
Medium-R
0.7
9.9 ↑
32.4
101.4 ↑
29.6
32.0 ↑
1.3
3.6 ↑
1.8
6.4 ↑
21.5
35.4
Medium-E
0.8
1.4 ↑
98.2
87.8 ↓
90.5
109.0 ↑
107.3
108.9 ↑
4.9
7.5 ↑
15.5
104.8"
REFERENCES,0.6642651296829971,"Table 12: Normalized scores in the (target) D4RL Walker2d tasks with the joint noise shift., where
”Tune” denotes baseline ”ﬁne-tune”."
REFERENCES,0.6657060518731989,"Joint Noise Shift
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
Tune
DARA
πp ˆT
ˆTπp"
REFERENCES,0.6671469740634006,Walker2d
REFERENCES,0.6685878962536023,"BEAR
BRAC-p
BCQ
CQL
MOPO
MABE"
REFERENCES,0.670028818443804,"Random
2.7
2.6 ↓
1.4
8.8 ↑
3.0
5.2 ↑
6.7
6.4 ↓
-0.4
-0.2 ↑
5.0
-0.2
Medium
0.5
0.1 ↓
55.8
72.9 ↑
45.1
52.7 ↑
76.6
81.2 ↑
7.0
12.2 ↑
49.4
48.7
Medium-R
3.2
10.4 ↑
12.2
34.8 ↑
13.5
14.6 ↑
-0.4
1.8 ↑
1.9
16.4 ↑
4.5
1.6
Medium-E
-0.4
0.6 ↑
71.7
74.3 ↑
44.8
57.2 ↑
104
116.5 ↑
11.3
26.3 ↑
84.7
82.6"
REFERENCES,0.6714697406340058,"A.3.6
ADDITIONAL RESULTS ON THE QUADRUPED ROBOT"
REFERENCES,0.6729106628242075,"In this ofﬂine sim2real setting, we collect the source ofﬂine data in the simulator (106 or 2 ∗106
steps) and target ofﬂine data in the real world (3 ∗104 steps). See Appendix A.4 for details. For
testing, we directly deploy the learned policy in the real (ﬂat or obstructive) environment and adopt
the average distance covered in an episode (300 steps) as our evaluation metrics."
REFERENCES,0.6743515850144092,Published as a conference paper at ICLR 2022
REFERENCES,0.6757925072046109,"Figure 6: Illustration of the real environment (for testing): (left) the ﬂat and static environment,
(right) the obstructive and dynamic environment."
REFERENCES,0.6772334293948127,Table 13: Average distance (m) covered in an episode (300 steps) in ﬂat and static (real) environment.
REFERENCES,0.6786743515850144,"Sim2real (Flat and Static)
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.6801152737752162,Quadruped Robot
REFERENCES,0.6815561959654178,"BCQ
CQL
MOPO"
REFERENCES,0.6829971181556196,"Medium
1.56
1.64 ↑
1.80
1.82 ↑
0.00
0.00
Medium-R
0.00
0.00
—
—
—
—
Medium-E
2.16
2.47 ↑
2.03
2.02 ↓
0.00
0.00
Medium-R-E
1.69
2.28 ↑
0.00
0.00
—
—"
REFERENCES,0.6844380403458213,"Average performance improvement
13.6%
0.2%
0.0%"
REFERENCES,0.6858789625360231,"(Flat and static environment) We ﬁrst deploy our learned policy in the ﬂat and static environment.
The results (distance covered in an episode) are provided in Table 13."
REFERENCES,0.6873198847262247,"1) BCQ (Figure 7): We ﬁnd that with Medium-R ofﬂine data, w/o Aug. BCQ and DARA BCQ both
could not acquire the locomotion skills, which we think is caused by the lack of high-quality ofﬂine
data. With more ”expert” data (Medium-R →Medium →Medium-E, or Medium-R →Medium-
R-E), w/o-Aug. BCQ allows for progressive performance (0.00 →1.56 →2.16, or 0.00 →1.69 in
BCQ), but with our reward augmentation, such performance can be further improved (with average
improvement 13.6%)."
REFERENCES,0.6887608069164265,"2) CQL (Figure 8): We ﬁnd that with Medium-R or Medium-R-E ofﬂine data, w/o Aug. CQL and
DARA CQL both could not learn the locomotion skills, which we think is caused by the low-quality
”Replay” ofﬂine data. With Medium or Medium-E ofﬂine data, w/o Aug. CQL and DARA CQL
acquire similar performance on this ﬂat and static environment."
REFERENCES,0.6902017291066282,"3) MOPO: We ﬁnd that the model-based MOPO (both w/o Aug. and DARA) could hardly learn the
locomotion skill under the provided ofﬂine data."
REFERENCES,0.69164265129683,"Table 14: Average distance (m) covered in an episode (300 steps) in the obstructive and dynamic
(real) environment."
REFERENCES,0.6930835734870316,"Sim2real (Obstructive and Dynamic)
w/o Aug.
DARA
w/o Aug.
DARA
w/o Aug.
DARA"
REFERENCES,0.6945244956772334,Quadruped Robot
REFERENCES,0.6959654178674352,"BCQ
CQL
MOPO"
REFERENCES,0.6974063400576369,"Medium
0.85
1.35 ↑
0.92
1.40 ↑
—
—
Medium-R
—
—
—
—
—
—
Medium-E
1.15
1.41 ↑
0.77
1.32 ↑
—
—
Medium-R-E
1.27
1.55 ↑
—
—
—
—"
REFERENCES,0.6988472622478387,"Average performance improvement
25.9%
30.9%
—"
REFERENCES,0.7002881844380403,"(Obstructive and dynamic environment) We then deploy our learned policy in the obstructive and
dynamic environment. The results (distance covered in an episode) are provided in Table 14."
REFERENCES,0.7017291066282421,"1) BCQ (Figure 9): In this obstructive environment, we can obtain similar results as in the ﬂat
environment. With more ”expert” data (Medium →Medium-E →Medium-R-E), w/o Aug. BCQ
allows for progressive performance (0.85 →1.15 →1.27), and with our reward augmentation,
such performance can be further improved (with average improvement 25.9%). At the same time,
we can also ﬁnd that due to the presence of environmental obstacles, the performance of both w/o"
REFERENCES,0.7031700288184438,Published as a conference paper at ICLR 2022
REFERENCES,0.7046109510086456,"Aug. BCQ and w/o Aug. DARA is decreased compared to the deployment on the ﬂat environment.
However, we ﬁnd that our DARA exhibits greater average performance improvement under this
obstructive environment (13.6% →25.9%) compared to that in the ﬂat environment. These results
demonstrate that our DARA can learn an adaptive policy for the target environment and thus show
a greater advantage over w/o-Aug. in more complex environments."
REFERENCES,0.7060518731988472,"2) CQL (Figure 10): Similar to BCQ, our DARA CQL exhibits a greater performance improvement
over baseline in the obstructive and dynamic environment (0.2% →30.9%) compared to that in the
ﬂat and static environment."
REFERENCES,0.707492795389049,"In summary, The results in the quadruped robot tasks support our conclusion in the main text
regarding the dynamics shift problem in ofﬂine RL — with only modest amounts of target ofﬂine
data (3 ∗104 steps), DARA-based methods can acquire an adaptive policy for the (both ﬂat and
obstructive) target environment and exhibit better performance compared to baselines under the
dynamics adaptation setting."
REFERENCES,0.7089337175792507,Figure 7: Deployment on the ﬂat and static environment of BCQ.
REFERENCES,0.7103746397694525,Figure 8: Deployment on the ﬂat and static environment of CQL.
REFERENCES,0.7118155619596542,Published as a conference paper at ICLR 2022
REFERENCES,0.7132564841498559,Figure 9: Deployment on the obstructive and dynamic environment of BCQ.
REFERENCES,0.7146974063400576,Figure 10: Deployment on the obstructive and dynamic environment of CQL.
REFERENCES,0.7161383285302594,"A.3.7
ABLATION STUDY WITH RESPECT TO THE AMOUNT OF TARGET OFFLINE DATA"
REFERENCES,0.7175792507204611,"To see whether the amount of target ofﬂine data can be further reduced, we show the results of the
ablation study with respect to the amount of target ofﬂine data in Tables 15 and 16."
REFERENCES,0.7190201729106628,"Table 15: Ablation study with respect to the amount of target Hopper data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target ofﬂine data, respectively, and
additional 100% source ofﬂine data."
REFERENCES,0.7204610951008645,"Body Mass Shift
10%
5%
1%
10%
5%
1%
10%
5%
1%
10%
5%
1%"
REFERENCES,0.7219020172910663,Hopper
REFERENCES,0.723342939481268,"BEAR
BRAC-p
BCQ
CQL"
REFERENCES,0.7247838616714697,"Medium-R
34.1
10.7
6.4
30.8
27.7
20.0
32.8
20.5
16.3
3.7
3.2
2.3
Medium-E
1.2
0.6
0.6
34.7
25.1
20.6
84.2
65.1
55.6
99.7
52.3
38.5"
REFERENCES,0.7262247838616714,"Table 16: Ablation study with respect to the amount of target Walker2d data (body mass shift tasks).
10%, 5% and 1% denote training with 10%, 5% and 1% of target ofﬂine data, respectively, and
additional 100% source ofﬂine data."
REFERENCES,0.7276657060518732,"Body Mass Shift
10%
5%
1%
10%
5%
1%
10%
5%
1%
10%
5%
1%"
REFERENCES,0.729106628242075,Walker2d
REFERENCES,0.7305475504322767,"BEAR
BRAC-p
BCQ
CQL"
REFERENCES,0.7319884726224783,"Medium-R
7.3
5.9
1.3
18.6
21.6
15.8
15.1
12.7
9.7
2.0
1.3
0.5
Medium-E
2.3
-0.2
-0.3
77.5
2.0
-0.2
57.2
29.7
20.6
93.3
0.1
-0.3"
REFERENCES,0.7334293948126801,"A.3.8
ILLUSTRATION OF WHETHER THE LEARNED POLICY IS LIMITED TO THE SOURCE
OFFLINE DATA"
REFERENCES,0.7348703170028819,"If we directly perform DARA with only the source ofﬂine data D′, the learned behaviors will be
restricted to the source ofﬂine data. For example, in the Map task, collecting source dataset with the
obstacle and collecting target dataset without the obstacle. In this case, it can be harder for DARA
(with only the source D′) to capture the change in the transition dynamics, thus harder for the agent"
REFERENCES,0.7363112391930836,Published as a conference paper at ICLR 2022
REFERENCES,0.7377521613832853,"to ﬁgure out the new optimal policy (the shorter path without the obstacle). However, as stated in
Algorithm 1, we perform ofﬂine RL algorithms with both target ofﬂine data and source ofﬂine data
{D′ ∪D}. Thus, to some extent, such limitation can be overcome as long as ofﬂine RL algorithm
captures the information (eg. the short path without the obstacle) contained in the (limited) target
D, see Figure 11 for the illustration."
K,0.739193083573487,0 k DARA
K,0.7406340057636888,target
K,0.7420749279538905,1 k DARA
K,0.7435158501440923,target
K,0.7449567723342939,2 k DARA
K,0.7463976945244957,target
K,0.7478386167146974,5 k DARA
K,0.7492795389048992,target
K,0.7507204610951008,10 k DARA
K,0.7521613832853026,target
K,0.7536023054755043,"Figure 11: We exchange the source environment and the target environment in Figure 2 (in the main
text) so that the source environment has an obstacle and the target environment has no obstacles. In
the source domain, we collect 100k of random transitions. In the target domain, we collect 0k, 1k,
2k, 5k, and 10k random transitions respectively. We set η = 0.1. We can ﬁnd that if we perform
DARA with only source ofﬂine data D′ (i.e., 0k target data), we indeed can not acquire the optimal
trajectory (eg. the short path without the obstacle). However, even there is no transition of passing
through obstacles in the source data, performing DARA with {D′ ∪D} enables us to acquire the
behavior of moving through obstacles. As we increase the number of target ofﬂine data D, training
with {D′ ∪D} can gradually acquire optimal trajectories."
K,0.7550432276657061,"A.3.9
COMPARISON BETWEEN DARA AND IMPORTANCE SAMPLING (IS) BASED DYNAMICS
CORRECTION"
K,0.7564841498559077,"In Table 17, we report the experimental comparison between DARA and importance sampling based
dynamics adaption. We can ﬁnd that in most of the tasks, our DARA performs better than the IS-
based approaches."
K,0.7579250720461095,Table 17: Comparison between DARA and importance sampling (IS) based dynamics correction.
K,0.7593659942363112,"Body Mass Shift
IS
DARA
IS
DARA
IS
DARA"
K,0.760806916426513,Hopper
K,0.7622478386167147,"BEAR
BRAC-p
AWR"
K,0.7636887608069164,"Random
4.6 ± 2.8
8.4 ± 1.2
10.8 ± 0.5
11 ± 0.6
10.2 ± 0.3
4.5 ± 0.9
Medium
1 ± 0.4
1.6 ± 1
17.4 ± 10.6
32.9 ± 7.5
24.8 ± 7.7
28.9 ± 5.5
Medium-R
17.3 ± 4.7
34.1 ± 5.8
21.6 ± 8.3
30.8 ± 4.9
14 ± 2.2
4.2 ± 3.5
Medium-E
0.8 ± 0.2
1.2 ± 0.5
36 ± 13.5
34.7 ± 8.5
29.3 ± 2.6
26.6 ± 2"
K,0.7651296829971181,Hopper
K,0.7665706051873199,"BCQ
CQL
MOPO"
K,0.7680115273775217,"Random
9.2 ± 1.1
9.7 ± 0.2
10.3 ± 0.4
10.4 ± 0.4
2.8 ± 3
2.1 ± 1.7
Medium
28.2 ± 8.8
38.4 ± 1.8
43.3 ± 10
59.3 ± 12.2
7.6 ± 7.2
10.7 ± 5.1
Medium-R
14.2 ± 1.3
32.8 ± 0.9
2.2 ± 0.3
3.7 ± 1.4
4.9 ± 3.8
8.4 ± 3.5
Medium-E
83.4 ± 23.7
84.2 ± 9.8
87.8 ± 16.9
99.7 ± 16.4
4.6 ± 2.9
5.8 ± 2.3"
K,0.7694524495677233,"A.3.10
THE SENSITIVITY OF THE COEFFICIENT OF THE REWARD MODIFICATION"
K,0.770893371757925,"In Table 18, We check the sensitivity of hyper-parameter η, i.e., the coefﬁcient of the reward modi-
ﬁcation in r(s, a) −η∆r(s, a, s′)."
K,0.7723342939481268,Published as a conference paper at ICLR 2022
K,0.7737752161383286,"Table 18: We show the normalized scores for the Hopper tasks with body mass shift, by varying
η ∈{0, 0.05, 0.1, 0.2, 0.5} over BEAR, BRAC-p, AWR, BCQ, CQL, and MOPO."
K,0.7752161383285303,"Body Mass Shift
Hyper-parameter η"
K,0.776657060518732,"0
0.05
0.1
0.2
0.5"
K,0.7780979827089337,Hopper BEAR
K,0.7795389048991355,"Random
4.6 ± 3.4
7.7 ± 0.9
8.4 ± 1.2
7 ± 1.2
4.2 ± 1.1
Medium
0.9 ± 0.3
1.1 ± 0.6
1.6 ± 1
0.9 ± 0.2
0.7 ± 0.1
Medium-R
18.2 ± 5
28.5 ± 5.9
34.1 ± 5.8
29.1 ± 4.4
18.1 ± 4.3
Medium-E
0.6 ± 0
0.8 ± 0.1
1.2 ± 0.5
1.2 ± 0.6
0.7 ± 0.1"
K,0.7809798270893372,Hopper
K,0.7824207492795389,BRAC-p
K,0.7838616714697406,"Random
9.6 ± 3.3
11.2 ± 0.8
11 ± 0.6
10.6 ± 2.4
5.3 ± 1.2
Medium
29.2 ± 2.1
26.5 ± 1.8
32.9 ± 7.5
16.1 ± 0.9
16.7 ± 1.7
Medium-R
20.1 ± 4.8
17.8 ± 3.2
30.8 ± 4.9
13.9 ± 1.7
10.4 ± 2.4
Medium-E
32.3 ± 7.8
40.4 ± 4.4
34.7 ± 8.5
29.4 ± 6.5
25.2 ± 4.1"
K,0.7853025936599424,Hopper AWR
K,0.7867435158501441,"Random
3.4 ± 0.7
4.1 ± 1
4.5 ± 0.9
3.4 ± 0.7
2.5 ± 0.1
Medium
20.8 ± 6.3
31.8 ± 2.9
28.9 ± 5.5
26.6 ± 3.2
17.4 ± 1.5
Medium-R
4.1 ± 1.7
3 ± 0.5
4.2 ± 3.5
2.6 ± 0.6
4.3 ± 1.3
Medium-E
26.8 ± 0.4
27 ± 0
26.6 ± 2
17.8 ± 5.6
24.2 ± 3.9"
K,0.7881844380403458,Hopper BCQ
K,0.7896253602305475,"Random
8.3 ± 0.3
9.6 ± 0.3
9.7 ± 0.2
7.4 ± 0.1
7.6 ± 0.3
Medium
25.7 ± 5.5
24.1 ± 0.8
38.4 ± 1.8
27.1 ± 1.7
26.7 ± 0.8
Medium-R
28.7 ± 1.9
29.5 ± 3
32.8 ± 0.9
25.9 ± 6
21 ± 2.2
Medium-E
75.4 ± 7.8
70.4 ± 5.4
84.2 ± 9.8
67.9 ± 8
61.9 ± 4.3"
K,0.7910662824207493,Hopper CQL
K,0.792507204610951,"Random
10.2 ± 0.3
10 ± 0
10.4 ± 0.4
10 ± 0
10 ± 0
Medium
44.9 ± 2.7
59.8 ± 6
59.3 ± 12.2
44.2 ± 1
37.1 ± 2.6
Medium-R
1.4 ± 0.3
2.1 ± 0.2
3.7 ± 1.4
3.9 ± 1.7
3.4 ± 1
Medium-E
53.6 ± 21.2
65.3 ± 15.4
99.7 ± 16.4
60.5 ± 16
75.9 ± 30"
K,0.7939481268011528,Hopper MOPO
K,0.7953890489913544,"Random
2 ± 2.1
1.8 ± 0
2.1 ± 1.7
1.2 ± 0.4
0.8 ± 0
Medium
5 ± 5.3
6.5 ± 1
10.7 ± 5.1
5.3 ± 1.6
2.8 ± 0.7
Medium-R
5.5 ± 4.6
7.5 ± 0.8
8.4 ± 3.5
5.7 ± 3.5
1.9 ± 0.6
Medium-E
4.8 ± 2.9
8.1 ± 1
5.8 ± 2.3
4.7 ± 0.7
2.1 ± 0.2"
K,0.7968299711815562,Published as a conference paper at ICLR 2022
K,0.7982708933717579,"A.4
ENVIRONMENTS AND DATASET"
K,0.7997118155619597,"Figure 12: Illustration of the suite of tasks considered in this work: (from left to right) Hopper,
Walker2d, Halfcheetah, simulated and real-world quadruped robots. These tasks require the RL
agent to learn locomotion gaits for the illustrated characters."
K,0.8011527377521613,"In this work, the tasks include Hopper, Walker2d, HalfCheetah, simulated (see the dynamics param-
eters in Zhang et al.) and real-world quadruped robot, which are illustrated in Figure 12."
K,0.8025936599423631,"Table 19: Dynamics shift for Hopper, Walker2d, and Halfcheetah tasks. For the body mass shift,
we change the mass of the body in the source MDP M ′. For the joint noise shift, we add a noise
(randomly sampling in [−0.05, +0.05]) to the actions when we collect the source ofﬂine data, i.e.,
D′ := {(s, a, r, s′)} ∼dD′(s)πb′(a|s)r(s, a)T ′(s′|s, a + noise)."
K,0.8040345821325648,"Hopper
Walker2d
HalfCheetah"
K,0.8054755043227666,"Body Mass Shﬁt
Joint Noise Shift
Body Mass Shﬁt
Joint Noise Shift
Body Mass Shﬁt
Joint Noise Shift
Source
mass[-1]=2.5
action[-1]+noise
mass[-1]=1.47
action[-1]+noise
mass[4]=0.5
action[-1]+noise
Target
mass[-1]=5.0
action[-1]+0
mass[-1]=2.94
action[-1]+0
mass[4]=1.0
action[-1]+0"
K,0.8069164265129684,"In the Hopper, Walker2d and HalfCheetah dynamics adaptation setting, we set the D4RL (Fu et al.,
2020) dataset as our target domain. For the source dynamics, we change the body mass (body mass
shift) or add noises to joints (joint noise shift) of the agents (see Table 19 for the details) and then
collect the source ofﬂine dataset in the changed environment. Following Fu et al. (2020), on the
changed source environment, we collect the 1) ”Random” ofﬂine data, generated by unrolling a ran-
domly initialized policy, 2) ”Medium” ofﬂine data, generated by a trained policy with the “medium”
level of performance in the source environment, 3) ”Medium-Replay” (Medium-R) ofﬂine data, con-
sisting of recording all samples in the replay buffer observed during training until the policy reaches
the “medium” level of performance, 4) ”Medium-Expert” (Medium-E) ofﬂine data, mixing equal
amounts of expert demonstrations and ”medium” data in the source environment."
K,0.80835734870317,"Figure 13:
Real-world terrains
(for collecting the target ofﬂine
data)."
K,0.8097982708933718,"In the sim2real setting (for the quadruped robot), we use the
A1 dog from Unitree (Wang, 2020). We collect the target of-
ﬂine data using ﬁve target behavior policies in the real-world
with changing terrains, as shown in Figure 13, and collect
the ”Medium”, ”Medium-Replay” (Medium-R), ”Medium-
Expert” (Medium-E), ”Medium-Replay-Expert” (Medium-R-
E) source ofﬂine data in the simulator, where ”Medium-
Replay-Expert” denotes mixing equal amounts of ”Medium-
Replay” data and expert demonstrations in the simulator. In
Section A.5, we provide the details of how to obtain the tar-
get and source behavior policy, so as to collect our target and
source ofﬂine data."
K,0.8112391930835735,"We list our tasks properties in Table 20 and provide our collected dataset in supplementary material.
In implementation, we set η = 0.1 for all simulated tasks and set η = 0.01 for the sim2real task. In
Table 18, we also report the sensitivity of DARA on the hyper-parameters η."
K,0.8126801152737753,"A.5
TRAINING THE (TARGET AND SOURCE) BEHAVIOR POLICY FOR THE QUADRUPED ROBOT"
K,0.8141210374639769,"To obtain a behavior policy that can be deployed in simulator (for collecting the source ofﬂine data)
or real-world (for collecting the target ofﬂine data), we introduce the prior knowledge (Iscen et al.,
2018) and domain randomization (Tobin et al., 2017; Peng et al., 2018)."
K,0.8155619596541787,Published as a conference paper at ICLR 2022
K,0.8170028818443804,Table 20: Statistics for each task in our adaptation setting.
K,0.8184438040345822,"Environment
Dynamics Shift
Task Name
Target (1T)
Source (10S)"
K,0.8198847262247838,Hopper
K,0.8213256484149856,Body Mass Shﬁt
K,0.8227665706051873,"Random
105 (D4RL)
106"
K,0.8242074927953891,"Medium
105 (D4RL)
106"
K,0.8256484149855908,"Medium-Replay
20092 (D4RL)
106"
K,0.8270893371757925,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8285302593659942,Joint Noise Shift
K,0.829971181556196,"Random
105 (D4RL)
106"
K,0.8314121037463977,"Medium
105 (D4RL)
106"
K,0.8328530259365994,"Medium-Replay
20092 (D4RL)
106"
K,0.8342939481268011,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8357348703170029,Walker2d
K,0.8371757925072046,Body Mass Shﬁt
K,0.8386167146974063,"Random
105 (D4RL)
106"
K,0.840057636887608,"Medium
105 (D4RL)
106"
K,0.8414985590778098,"Medium-Replay
10093 (D4RL)
106"
K,0.8429394812680115,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8443804034582133,Joint Noise Shift
K,0.845821325648415,"Random
105 (D4RL)
106"
K,0.8472622478386167,"Medium
105 (D4RL)
106"
K,0.8487031700288185,"Medium-Replay
10093 (D4RL)
106"
K,0.8501440922190202,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8515850144092219,HalfCheetah
K,0.8530259365994236,Body Mass Shﬁt
K,0.8544668587896254,"Random
105 (D4RL)
106"
K,0.8559077809798271,"Medium
105 (D4RL)
106"
K,0.8573487031700289,"Medium-Replay
10100 (D4RL)
106"
K,0.8587896253602305,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8602305475504323,Joint Noise Shift
K,0.861671469740634,"Random
105 (D4RL)
106"
K,0.8631123919308358,"Medium
105 (D4RL)
106"
K,0.8645533141210374,"Medium-Replay
10100 (D4RL)
106"
K,0.8659942363112392,"Medium-Expert
2 ∗105 (D4RL)
2 ∗106"
K,0.8674351585014409,"A1 robot (Unitree)
Sim2Real"
K,0.8688760806916427,"Medium
3 ∗104 (real-world)
106 (simulator)
Medium-Replay
3 ∗104 (real-world)
106 (simulator)
Medium-Expert
3 ∗104 (real-world)
2 ∗106 (simulator)
Medium-Replay-Expert
3 ∗104 (real-world)
2 ∗106 (simulator)"
K,0.8703170028818443,"Prior Knowledge: To reduce the impact of the foot at the moment of touching the ground during
the robot locomotion, we designed a compound cycloid trajectory (Sakakibara et al., 1990) as prior
knowledge. In our implementation for the foot trajectory, four aspects are mainly considered: 1) The
robot walks stably without obvious shaking; 2) The joint impact of the robot during the locomotion
is small; 3) The joint speed and acceleration of the robot during the locomotion are continuous and
smooth; 4) The feet of the robot will not slide when they are in contact with the ground. Similar
to Lee et al. (2020), we deﬁne a periodic phase variable φi ∈[0.0, 0.6), i = 1, 2, 3, 4 for each leg,
which represents swing phase if φi ∈[0.0, 0.3) and contact phase if φi ∈[0.3, 0.6). At every time
step t, φi = (t ∗f0 + φ0[i] + φoffset[i])(mod 2Tm) where Tm = 0.3, and f0 = 1.1 is the base
frequency, and φ0 = [0, 0.3, 0.3, 0] is the initial phase. φoffset is part of the output of the controller.
The trajectory of the swing leg is:





"
K,0.8717579250720461,"



"
K,0.8731988472622478,"xi = S
h
t
Tm −
1
2π sin

2πt
Tm"
K,0.8746397694524496,"i
+ S0, i = 1, 2"
K,0.8760806916426513,"xi = S
h
t
Tm −
1
2π sin

2πt
Tm"
K,0.877521613832853,"i
−S + S0, i = 3, 4
y = Y0
z = H

sgn
  Tm"
K,0.8789625360230547,"2 −t

(2fE(t) −1) + 1

+ Z0 , where"
K,0.8804034582132565,"fE(t) =
t
Tm
−1"
K,0.8818443804034583,"4π sin
4πt Tm 
, and"
K,0.8832853025936599,"sgn
Tm"
K,0.8847262247838616,"2 −t

=

1
0 ≤t < Tm"
K,0.8861671469740634,"2
−1
Tm"
K,0.8876080691642652,2 ≤t < Tm .
K,0.8890489913544669,Published as a conference paper at ICLR 2022
K,0.8904899135446686,"The trajectory of the standing leg is:




"
K,0.8919308357348703,"


"
K,0.8933717579250721,"xi = S

2Tm−t"
K,0.8948126801152738,"Tm
+
1
2π sin

2πt
Tm"
K,0.8962536023054755,"
+ S0, i = 1, 2"
K,0.8976945244956772,"xi = S

2Tm−t"
K,0.899135446685879,"Tm
+
1
2π sin

2πt
Tm"
K,0.9005763688760807,"
−S + S0, i = 3, 4
y = Y0
z = Z0 ."
K,0.9020172910662824,"where S
=
0.14m, H
=
0.18m are the maximum foot length and height.
S0
=
[0.17, 0.17, −0.2, −0.2], Y0 = [−0.13, 0.13, −0.13, 0.13], Z0 = [−0.32, −0.32, −0.32, −0.32] are
the default target foot position in body frame."
K,0.9034582132564841,"Domain Randomization: To encourage the policy to be robust to variations in the dynamics, we
incorporate the domain randomization. In Table 21, we provide the dynamics parameters and their
respective range of values."
K,0.9048991354466859,Table 21: Dynamic parameters and their respective range of values utilized during training.
K,0.9063400576368876,"Parameter
Range"
K,0.9077809798270894,"Mass
[0.95, 1.1] ×default value
Inertia
[0.80, 1.2] ×default value
Motor Strength
[0.80, 1.2] ×default value
Latency
[0, 0.04] s
Lateral Friction
[0.5, 1.25] Ns/m
Joint Friction
[0, 0.05] Nm"
K,0.909221902017291,"State Space, Action Space and Reward Function: The action is a 16-dimensional vector consist-
ing of leg phase and target foot position residuals in the body frame. The design of state space and
reward function mainly follows the prior work Lee et al. (2020). In Table 22, we provide the state
representation."
K,0.9106628242074928,Table 22: State representation for the behavior policy.
K,0.9121037463976945,"Data
Dimension"
K,0.9135446685878963,"Desired direction
 B
IBˆvd
 xy"
K,0.9149855907780979,"
2
Euler angle(rpy)
3
Base angular velocity
 B
IBω

3
Base linear velocity
 B
IBv

3"
K,0.9164265129682997,"Joint position/velocity

θi, ˙θi

24
FTG phases(sin (φi) , cos (φi))
8
FTG frequencies(fi)
4
Base frequency(fo)
1
Joint position error history
24
Joint velocity history
24
Foot target history

(rf,d)t−1,t−2

24"
K,0.9178674351585014,The reward function is deﬁned as
K,0.9193083573487032,0.1rlv + 0.05ry + 0.05rrp + 0.005rb + 0.02rbc + 0.025rs + 2 · 10−5rτ.
K,0.920749279538905,The individual terms are deﬁned as follows.
K,0.9221902017291066,1) Linear velocity reward rlv :
K,0.9236311239193083,"rlv :=

exp(−30|vpr −0.2|)
vpr < 0.2
1
vpr ≥0.2 ,"
K,0.9250720461095101,Published as a conference paper at ICLR 2022
K,0.9265129682997119,where vpr = vxy · ˆvxy is the base linear velocity projected onto the command direction.
K,0.9279538904899135,"2) Yaw angle reward ry :
ry := exp(−(y −ˆy)2),
(17)"
K,0.9293948126801153,where y and ˆy is the yaw and desired yaw angle.
K,0.930835734870317,3) Roll and pitch reward rrp :
K,0.9322766570605188,"rrp := exp(−1.5
X
(φ −[0, arccos(< Pxz, (0, 0, 1)T >"
K,0.9337175792507204,"∥Pxz ∥
) −π/2])2),
(18)"
K,0.9351585014409222,"where φ are the roll and pitch angle. Pxz = P1 −P4 or Pxz = P2 −P3,Pi, i ∈[1, 4] are the foot
position in world frame. The advantage of designing the target pitch angle in this way is to ensure
that the body of the robot is parallel to the supporting surface of the stand legs, thereby ensuring that
the robot can smoothly over challenge terrain, such as upward stairs."
K,0.9365994236311239,4) Base motion reward rb :
K,0.9380403458213257,"rb := exp(−1.5
X
(vxy −vpr ∗ˆvxy)2) + exp(−1.5
X
(ωxy)2),
(19)"
K,0.9394812680115274,where ωxy are the roll and pitch rates.
K,0.9409221902017291,"5) Body collision reward rbc :
rbc := −|Ibody/Ifoot|,
(20)"
K,0.9423631123919308,"where Ibody and Ifoot are the contact numbers of robot’s body parts and foot with the terrain, re-
spectively."
K,0.9438040345821326,6) Target smooth reward rs :
K,0.9452449567723343,"rs := −||fd,t −2fd,t−1 + fd,t−2||,
(21)"
K,0.946685878962536,"where fd,i(i = t, t −1, t −2) are the target foot positions in the time-step t, t −1 and t −2."
K,0.9481268011527377,"7) Torqure reward rτ:
rτ := −
X"
K,0.9495677233429395,"i
|τi|,
(22)"
K,0.9510086455331412,where τi is the joint torques.
K,0.952449567723343,"Training Details: Both the behavior policy and value networks are Multilayer Perceptron (MLP)
with 3 hidden layers, which have 256, 128 and 64 nodes. The activation function is the Tanh func-
tion, and the optimizer is Adam. With the above prior knowledge, domain randomization and reward
function, we train our behavior policy with SAC (Haarnoja et al., 2018) in PyBullet (Coumans &
Bai, 2016–2021)."
K,0.9538904899135446,"A.6
ADDITIONAL RESULTS"
K,0.9553314121037464,Here we provide additional results regarding the error bars (Tables 23 and 24).
K,0.9567723342939481,Published as a conference paper at ICLR 2022
K,0.9582132564841499,"Table 23: Normalized scores for the D4RL tasks (with body mass shift). We take the baseline results (for 10T) of MOPO from their original papers and that of the
other model-free methods (BEAR, BRAC-p, AWR, BCQ and CQL) from the D4RL paper (Fu et al., 2020)."
K,0.9596541786743515,"Body Mass Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
(DARA)
w/o Aug.
(DARA)
w/o Aug.
(DARA)"
K,0.9610951008645533,Hopper
K,0.962536023054755,"BEAR
BRAC-p
AWR"
K,0.9639769452449568,"Random
11.4
1 ± 0.5
4.6 ± 3.4
8.4 ± 1.2
11
10.9 ± 0.1
9.6 ± 3.3
11 ± 0.6
10.2
10.3 ± 0.3
3.4 ± 0.7
4.5 ± 0.9
Medium
52.1
0.8 ± 0
0.9 ± 0.3
1.6 ± 1
32.7
29 ± 6.2
29.2 ± 2.1
32.9 ± 7.5
35.9
30.9 ± 0.4
20.8 ± 6.3
28.9 ± 5.5
Medium-R
33.7
1.3 ± 1.5
18.2 ± 5
34.1 ± 5.8
0.6
5.4 ± 3.3
20.1 ± 4.8
30.8 ± 4.9
28.4
8.8 ± 4.9
4.1 ± 1.7
4.2 ± 3.5
Medium-E
96.3
0.8 ± 0.1
0.6 ± 0
1.2 ± 0.5
1.9
34.5 ± 14.7
32.3 ± 7.8
34.7 ± 8.5
27.1
27 ± 1.3
26.8 ± 0.4
26.6 ± 2"
K,0.9654178674351584,Hopper
K,0.9668587896253602,"BCQ
CQL
MOPO"
K,0.968299711815562,"Random
10.6
10.6 ± 0.1
8.3 ± 0.3
9.7 ± 0.2
10.8
10.6 ± 0.1
10.2 ± 0.3
10.4 ± 0.4
11.7
4.8 ± 2.4
2 ± 2.1
2.1 ± 1.7
Medium
54.5
37.1 ± 6.3
25.7 ± 5.5
38.4 ± 1.8
58
43 ± 9.2
44.9 ± 2.7
59.3 ± 12.2
28
4.1 ± 2
5 ± 5.3
10.7 ± 5.1
Medium-R
33.1
9.3 ± 4.4
28.7 ± 1.9
32.8 ± 0.9
48.6
9.6 ± 5.2
1.4 ± 0.3
3.7 ± 1.4
67.5
1 ± 0.6
5.5 ± 4.6
8.4 ± 3.5
Medium-E
110.9
58 ± 16.2
75.4 ± 7.8
84.2 ± 9.8
98.7
59.7 ± 34.5
53.6 ± 21.2
99.7 ± 16.4
23.7
1.6 ± 0.6
4.8 ± 2.9
5.8 ± 2.3"
K,0.9697406340057637,Walker2d
K,0.9711815561959655,"BEAR
BRAC-p
AWR"
K,0.9726224783861671,"Random
7.3
1.5 ± 0.9
3.1 ± 0.9
3.2 ± 0.4
-0.2
0 ± 0.2
1.3 ± 0.7
3.2 ± 2.5
1.5
1.3 ± 0.4
2 ± 1
2.4 ± 0.8
Medium
59.1
-0.5 ± 0.3
0.6 ± 0.5
0.3 ± 0.7
77.5
6.4 ± 9.9
70 ± 10.1
78 ± 3.1
17.4
14.8 ± 2.8
17.1 ± 0.2
17.2 ± 0.1
Medium-R
19.2
0.7 ± 0.6
6.5 ± 5.1
7.3 ± 1.3
-0.3
8.5 ± 2.2
9.9 ± 2
18.6 ± 6.5
15.5
7.4 ± 2.1
1.6 ± 0.4
1.5 ± 0.3
Medium-E
40.1
-0.1 ± 0.1
1.5 ± 2.5
2.3 ± 2.2
76.9
20.6 ± 16.8
64.1 ± 10.8
77.5 ± 3.1
53.8
35.5 ± 10.4
52.5 ± 1.2
53.3 ± 0.3"
K,0.9740634005763689,Walker2d
K,0.9755043227665706,"BCQ
CQL
MOPO"
K,0.9769452449567724,"Random
4.9
1.8 ± 0.9
4.5 ± 0.5
4.8 ± 0.3
7
1.7 ± 1.3
3.2 ± 1.4
3.4 ± 1.9
13.6
-0.2 ± 0.2
-0.1 ± 0.1
-0.1 ± 0.2
Medium
53.1
32.8 ± 8.2
50.9 ± 4.3
52.3 ± 1.4
79.2
42.9 ± 24.2
80 ± 1.2
81.7 ± 3.1
17.8
7 ± 3.6
5.7 ± 4.7
11 ± 4.3
Medium-R
15
6.9 ± 0.6
14.9 ± 0.2
15.1 ± 0.2
26.7
4.6 ± 3.9
0.8 ± 0.5
2 ± 1.5
39
5.1 ± 5.7
3.1 ± 2.4
14.2 ± 4.5
Medium-E
57.5
32.5 ± 9.1
55.2 ± 3.8
57.2 ± 0.2
111
49.5 ± 26.7
63.5 ± 22.5
93.3 ± 8.8
44.6
5.3 ± 3.9
5.5 ± 3.5
17.2 ± 8.7"
K,0.978386167146974,Published as a conference paper at ICLR 2022
K,0.9798270893371758,"Table 24: Normalized scores for the D4RL tasks (with joint noise shift). We take the baseline results (for 10T) of MOPO from their original papers and that of the
other model-free methods (BEAR, BRAC-p, AWR, BCQ and CQL) from the D4RL paper (Fu et al., 2020)."
K,0.9812680115273775,"Joint Noise Shift
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
10T
1T
1T+10S
1T+10S
w/o Aug.
(DARA)
w/o Aug.
(DARA)
w/o Aug.
(DARA)"
K,0.9827089337175793,Hopper
K,0.984149855907781,"BEAR
BRAC-p
AWR"
K,0.9855907780979827,"Random
11.4
0.6 ± 0
7.4 ± 0.5
4.2 ± 3.6
11
10.8 ± 0.2
10 ± 0.8
10.8 ± 0
10.2
10.1 ± 0
3.6 ± 0
4 ± 0.4
Medium
52.1
0.8 ± 0
2 ± 1
2 ± 0.1
32.7
26.6 ± 4.8
27.6 ± 2.8
37.6 ± 7
35.9
30.3 ± 0.1
38.8 ± 3.9
41.3 ± 5.4
Medium-R
33.7
2.7 ± 1.6
3.6 ± 0.4
9.9 ± 6.3
0.6
13.4 ± 6.4
89.9 ± 7.8
101.4 ± 0.2
28.4
12.4 ± 6
6.7 ± 4.2
7.2 ± 0.2
Medium-E
96.3
0.8 ± 0.2
0.8 ± 0
1.4 ± 0.6
1.9
19.8 ± 14
57.6 ± 23.4
87.8 ± 13.3
27.1
25.5 ± 1.2
27 ± 0
27 ± 0.1"
K,0.9870317002881844,Hopper
K,0.9884726224783862,"BCQ
CQL
MOPO"
K,0.9899135446685879,"Random
10.6
10.5 ± 0.1
7 ± 0
9.6 ± 0
10.8
10.4 ± 0.1
10.4 ± 0.4
10.8 ± 0
11.7
1.5 ± 0.8
1.3 ± 0.5
2.9 ± 1.5
Medium
54.5
45.8 ± 2.2
49 ± 1.7
54.4 ± 0.1
58
46.2 ± 11.9
58 ± 0
58 ± 0
28
2.7 ± 2.1
9.2 ± 5.4
17.3 ± 3.4
Medium-R
33.1
13 ± 5
23.8 ± 3.2
32 ± 0.9
48.6
13.6 ± 6.4
2.6 ± 0.3
3.6 ± 0.6
67.5
0.8 ± 0.1
2.3 ± 1.7
6.4 ± 0.8
Medium-E
110.9
44.6 ± 18.6
96 ± 0.5
109 ± 0.2
98.7
50.7 ± 26.9
73.4 ± 1.5
108.9 ± 0.7
23.7
1 ± 0.2
6.1 ± 1.4
7.5 ± 0.6"
K,0.9913544668587896,Walker2d
K,0.9927953890489913,"BEAR
BRAC-p
AWR"
K,0.9942363112391931,"Random
7.3
2.2 ± 0.1
0.6 ± 0.1
2.6 ± 0.3
-0.2
2.8 ± 2.8
3.3 ± 2.9
8.8 ± 8
1.5
0.9 ± 0.1
1.5 ± 0.6
1.5 ± 0.2
Medium
59.1
-0.4 ± 0.1
0.6 ± 0
0.1 ± 0.3
77.5
28.8 ± 28.4
55.2 ± 15.8
72.9 ± 9.1
17.4
12.2 ± 0.3
17.2 ± 0.2
17.2 ± 0.2
Medium-R
19.2
0.4 ± 0.2
4 ± 0.2
10.4 ± 2.4
-0.3
6.3 ± 1.2
32.1 ± 11.9
34.8 ± 10.5
15.5
6 ± 1
1.4 ± 0
2.1 ± 0.9
Medium-E
40.1
-0.2 ± 0.2
0.8 ± 0.4
0.6 ± 0.6
76.9
21.8 ± 18.4
62.3 ± 13.1
74.3 ± 1.8
53.8
40.4 ± 12.6
53 ± 0.1
53.6 ± 0"
K,0.9956772334293948,Walker2d
K,0.9971181556195965,"BCQ
CQL
MOPO"
K,0.9985590778097982,"Random
4.9
3.7 ± 1.8
3.4 ± 0.4
5.2 ± 0.3
7
0.5 ± 1
2.7 ± 0.2
6.4 ± 0.6
13.6
-0.3 ± 0.1
-0.2 ± 0
-0.2 ± 0.2
Medium
53.1
43 ± 8.3
44.9 ± 3.3
52.7 ± 0.3
79.2
43.9 ± 21.7
73.2 ± 0.8
81.2 ± 1.1
17.8
5.8 ± 5.9
7.8 ± 6.2
12.2 ± 5
Medium-R
15
5.7 ± 0.5
9.8 ± 5.2
14.6 ± 0.4
26.7
1.8 ± 1.2
1.4 ± 0.4
1.8 ± 0.4
39
0.8 ± 0.7
9.3 ± 5.2
16.4 ± 4.9
Medium-E
57.5
44.5 ± 3.6
40.6 ± 16.4
57.2 ± 0.2
111
46.8 ± 40
109.9 ± 4.5
116.5 ± 9.1
44.6
2.9 ± 3.1
15.2 ± 12.8
26.3 ± 18.4"
