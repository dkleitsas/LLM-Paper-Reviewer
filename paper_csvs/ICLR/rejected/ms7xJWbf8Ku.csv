Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002127659574468085,"We ﬁnd that at sequence length 512 padding tokens represent in excess of 50% of
the Wikipedia dataset used for pretraining BERT (Bidirectional Encoder Repre-
sentations from Transformers). Therefore by removing all padding, we achieve
a 2x speed-up in terms of sequences/sec. To exploit this characteristic of the
dataset, we develop and contrast two packing algorithms. Both algorithms rely
on the assumption that sequences are interchangeable and therefore packing can
be performed on the histogram of sequence lengths, rather than per sample. This
transformation of the problem leads to algorithms which are fast and have linear
complexity in dataset size. The shortest-pack-ﬁrst histogram-packing (SPFHP)
algorithm determines the packing order for the Wikipedia dataset of over 16M
sequences in 0.03 seconds. The non-negative least-squares histogram-packing
(NNLSHP) algorithm converges in 28.4 seconds but produces solutions which are
more depth efﬁcient, managing to get near optimal packing by combining a maxi-
mum of 3 sequences in one sample. Using the dataset with multiple sequences per
sample requires adjusting the model and the hyperparameters to keep the predic-
tive quality of the model. We demonstrate that these changes are straightforward
to implement and have relatively little impact on the achievable performance gain
on modern hardware. Finally, we pretrain BERT-Large using the packed dataset,
demonstrating no loss of convergence and the desired 2x speed-up."
INTRODUCTION,0.00425531914893617,"1
INTRODUCTION"
INTRODUCTION,0.006382978723404255,"Since its introduction in 2019, BERT (Devlin et al., 2019a) has been the backbone driving the most
exciting advances in Natural Language Processing (NLP). Pre-training BERT from scratch requires
substantial computational resources which may be out of reach for researchers and industry pro-
fessionals. To some extent this has been addressed by the public release of pre-trained models of
different sizes and depths (Turc et al., 2019). The introduction of ALBERT (Lan et al., 2019) and
Switch transformers (Fedus et al., 2021) further improved the accessibility of larger models. How-
ever, the dependence on pre-trained models limits the ability of researchers to explore new backbone
architectures. Furthermore, it limits the extent to which practitioners in industry can leverage inter-
nal datasets and adapt the model to their particular needs. Hence, any approach that speeds up the
pre-training process is desirable from an economical as well as environmental perspective."
INTRODUCTION,0.00851063829787234,"In this paper, we present efﬁcient methods to enable researchers to accelerate the pre-training of
BERT by as much as 2x without loss of accuracy. The de-facto pre-training dataset Wikipedia,
as well as many other NLP datasets, have a skewed distribution of sequence lengths. We show
that padding tokens (wasted compute) represent 50% of all tokens of the Wikipedia pre-training
dataset at sequence length 512. Thus, by avoiding processing the padding tokens one can get a
2x speed-up. Overall, the lengths range between 5 tokens up to 512 (see Figure 1). Samples of
length 512 represent only 23.5% of the dataset, a surprising result given that the pre-processing
in BERT attempts to “pack” together sentences so as to ﬁll the sequence length as completely as
possible (Devlin et al., 2019c). While processing the padding tokens wastes compute, it is still
the most standard approach for leveraging modern massively-parallel compute especially on GPUs.
These are most efﬁcient when applying the same operation to each sequence in a batch. By padding
all sequences to the same maximum sequence length, they can easily be batched. We note that this
naive batching is the most widely used and provided in the vanilla BERT implementation as well as
the Hugging Face framework (Wolf et al., 2020) and thus considered as our baseline for comparison."
INTRODUCTION,0.010638297872340425,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.01276595744680851,"0
25
50
75
100
125
sequence length 0.000 0.001 0.002 0.003 0.004 0.005"
INTRODUCTION,0.014893617021276596,probability density 59.9%
INTRODUCTION,0.01702127659574468,"max. sequence length: 128
theoretical max. speed-up: 1.210"
INTRODUCTION,0.019148936170212766,"0
100
200
300
400
sequence length 0.000 0.001 0.002 0.003 0.004 0.005 30.6%"
INTRODUCTION,0.02127659574468085,"max. sequence length: 384
theoretical max. speed-up: 1.742"
INTRODUCTION,0.023404255319148935,"0
100
200
300
400
500
sequence length 0.000 0.001 0.002 0.003 0.004 0.005 23.5%"
INTRODUCTION,0.02553191489361702,"max. sequence length 512
theoretical max. speed-up: 2.001"
INTRODUCTION,0.027659574468085105,"Figure 1: Wikipedia BERT pre-training dataset sequence length histograms (token count excluding
padding) for different maximum sequence lengths. Based on the Wikipedia article dump from Oc-
tober 1st 2020. The theoretical speed-up relates to not using any padding tokens and not having any
overhead from processing the different lengths."
INTRODUCTION,0.029787234042553193,"The most obvious way to reduce the extent of padding in the dataset is to group samples by size be-
fore batching (SORT), i.e., process the shorter samples together and longer samples together. BERT
is pre-trained in two phases, where the ﬁrst phase uses sequence length 128 for 900K steps and
the second phase uses sequence length 512 for 100K steps. However even by splitting the train-
ing in this way, the wasted compute due to padding is approximately 20% (see Figure 1). Other
examples of this “sorted batching” approach can be found in Faster Transformer (NVIDIA, 2021),
lingvo (Shen et al., 2019) fairseq (Ott et al., 2019), and RoBERTa (Liu et al., 2019), which group
samples of similar size together in one batch and ﬁll up with padding only to the maximum length in
this batch. This approach can be highly efﬁcient in cases where the dataset length is multiple orders
of magnitude larger than the batch size and the number of different sequence lengths. Despite its
high computational efﬁciency, this approach has multiple drawbacks. We outline these below and
propose an alternative which maintains the high efﬁciency, while also circumventing the downsides.
Firstly, sorting the data can reduce the overall convergence speed when the batch size is large be-
cause it violates the i.i.d. assumption on the data distribution (Bottou et al., 2018; Meng et al., 2019).
Secondly, processing batches with shorter sequence lengths under-utilizes the compute compared to
running the same batch size with a longer sequence length. For GPUs, a common heuristic to miti-
gate this effect is to adjust the batch size to keep the number of processed tokens near constant (Ott
et al., 2019; Liu et al., 2019). In general however, the relationship between the sequence length
and the optimum batch size is more complex and maximizing compute utilization can require the
model to be sharded differently across multiple accelerators. Avoiding this, often manual process,
is important for ease of use and the portability of methods across different hardware architectures.
Thirdly, modern NLP applications are optimized and compiled for ﬁxed tensor sizes using tools
such as XLA (XLA, 2021; Fedus et al., 2021), which provides a ≈7x acceleration for BERT in
MLPerf™(Mattson et al., 2020) compared to the non-XLA baseline (XLA, 2021). Changing the
sequence length or batch size requires re-optimization of the computational graph and recompila-
tion of the program for the new tensor shapes. For complex models such as BERT, optimization
and recompilation take a non-negligible amount of time. Even if one pre-compiled and cached all
combinations of batch size and sequence length, the kernels would still need to be re-uploaded to
the device every time the shapes change. Depending on how frequently the tensor shapes change,
the overhead from switching kernels adds up. To avoid these issues, it is preferable (and common)
to work with ﬁxed tensor shapes for the entire duration of the training run."
INTRODUCTION,0.031914893617021274,"More advanced approaches for reducing the padding overhead rely on custom computational kernels.
Loosely these are referred to as “un-padding” approaches. In Effective Transformer (ByteDance
Inc., 2021), the input batch is provided as a padded matrix but padding values are dynamically
removed and restored during different calculation stages. While un-padding implementations are
highly sophisticated and are able to completely circumvent the processing of padding tokens, they
introduce a signiﬁcant overhead due to the multiple GPU kernel launches (i.e., one kernel per se-
quence rather than one kernel per batch). Additionally the time to process each batch will ﬂuctuate
depending on the sequence lengths in each batch, i.e., batches with only shorter sequences will
typically be processed faster. When working with more than one accelerator, this variability in
throughput results in all devices in the cluster waiting for the device with the most compute inten-"
INTRODUCTION,0.03404255319148936,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.036170212765957444,"sive batch to ﬁnish processing. As such, un-padding approaches are not appropriate for deployment
on large clusters. The “packing” based approach introduced in this paper offers signiﬁcant advan-
tages over un-padding approaches. Firstly, packing is implemented directly at the framework level
and requires no additional custom kernel implementations. Secondly, the processing time for each
batch is independent of the content of the batch, allowing the packing based approach to maintain
the same speed-up whether running on a single device or thousands."
INTRODUCTION,0.03829787234042553,"While we demonstrate the effectiveness of packing speciﬁcally on the Wikipedia dataset, pack-
ing SQuAD (Rajpurkar et al., 2016) or GLUE datasets (Warstadt et al., 2018; Wang et al., 2018) for
BERT also leads to signiﬁcant speed-ups (some in excess of 9x) (Sections H and I). The effectiveness
of packing is a result of both the length distribution of the documents in the source datasets as well
as the different text preprocessing steps for BERT (Devlin et al., 2019c). The use of bi-directional
self-attention in BERT implies that the input sequences should contain complete sentences. If a
sentence is abruptly cut short, the hidden state on other (preceding) tokens in the sequence will be
affected. Language models with causal attention (only attending to previous tokens in the input) do
not have this issue to the same degree. For such models, if a sequence is cut short at an arbitrary
token, the other tokens (which occur earlier in the sequence) will not be affected. This ability to
cut sequences arbitrarily completely trivializes the packing problem for models based on causal at-
tention. For instance, GPT-3 (Brown et al., 2020) is trained with a maximum sequence length of
2048 where a single sequence may contain multiple segments of sentences separated by a special
end of segment token. The last segment in each sequence is simply cut to meet the sequence length
requirement making the packing problem trivial and avoiding any padding. In the interest of com-
putational efﬁciency GPT-3 does not mask the attention between different segments in a sequence.
In contrast, the packing approach presented in this paper introduces a mask in the attention layer
(see Section 3.2.2) to prevent cross-contamination between examples in a pack. Note, we mask
the interaction between different sequences and not between different sentences or segments in the
same sequence. This ensures that the characteristics of the original dataset and model are matched
as closely as possible. RoBERTa and many other models in production like T5 (Raffel et al., 2019)
use a similar packing approach as GPT-3, combining full sentences/sequences with GREEDY pack-
ing (ﬁrst come ﬁrst concatenate) and also separation tokens or additional padding. The RoBERTa
ablation study shows that mixing of sentences from different documents reduces accuracy, but it is
used nonetheless for load balancing reasons which indicates that sorted batching is not sufﬁcient."
INTRODUCTION,0.04042553191489362,"In summary, the contributions of the paper are as follows. In Section 2, we produce histograms of
the Wikipedia pre-training dataset showing the high percentage of padding tokens. We present two
new deterministic and efﬁcient packing algorithms which efﬁciently pack datasets with millions of
sequences in a matter of seconds (or less) in Section 3.1. We empirically show that the proposed
packing algorithms produce a nearly-optimal packing scheme In Section 3.2 and Section 3.3, we
explain how the BERT model can be adjusted to show the same convergence behavior on packed and
unpacked sequences. In Section 4.2, we demonstrate that the convergence of the BERT large model
on the packed dataset is equivalent to that on the un-packed dataset with 2x throughput increase on
the Wikipedia sequence length 512 pre-training dataset."
WIKIPEDIA BERT PRE-TRAINING DATASET,0.0425531914893617,"2
WIKIPEDIA BERT PRE-TRAINING DATASET"
WIKIPEDIA BERT PRE-TRAINING DATASET,0.04468085106382979,"BERT is pre-trained using masked-language modelling and next-sentence prediction on a large cor-
pus of Wikipedia articles. Each sequence is composed of one <CLS> token followed by the ﬁrst
“segment” of sentences, followed by a <SEP> token, and then ﬁnally the second “segment” of sen-
tences. Because these “segments” are created in sentence-level increments there is no token-level
control of sequence length. Furthermore 10% (default value, (Devlin et al., 2019b)) of sequences are
intentionally cut short. This leads to signiﬁcant levels of padding, especially for longer maximum
sequence lengths (see Figure 1). At sequence length 128 (commonly used in phase 1 of pre-training)
the theoretical speed-up is around 1.2, at sequence length 384 this increases to 1.7, and ﬁnally at
sequence length 512 (commonly used for phase 2 of pre-training) it is 2.0. Despite the widespread
use of the Wikipedia dataset for pre-training BERT such histograms have, to the best of our knowl-
edge, not been published previously. This has perhaps lead to the underestimation of the speed-up
opportunity available. To put things into perspective, the sequence length 512 dataset contains 8.33
billion tokens, of which 4.17 billion are padding tokens."
WIKIPEDIA BERT PRE-TRAINING DATASET,0.04680851063829787,Under review as a conference paper at ICLR 2022
METHODS,0.04893617021276596,"3
METHODS"
METHODS,0.05106382978723404,"Our approach consists of three distinct components. Firstly, we pack the n data samples efﬁciently
during pre-processing to make full use of the maximum sequence length, sm (Sections 3.1.1, 3.1.2,
and E). Secondly, we introduce a series of model changes in Section 3.2 that preserve the equiva-
lence with the original BERT implementation. The changes include a self-attention mask to prevent
the model from attending between different sequences in the same pack (Section 3.2.2), and an ad-
justment of the the positional embeddings (Section 3.2.1) to handle packs of sequences. Other com-
ponents of the model, such as the feed-forward layer (Vaswani et al., 2017), operate on a per-token
basis and do not require modiﬁcation for pre-training. In Section 3.2.3, we also demonstrate how to
compute a per-sequence loss and accuracy for NSP and downstream ﬁne-tuning tasks. Thirdly, we
provide suggestions for hyperparameter adjustment (Section 3.3) that lead to analogous convergence
behavior between the packed and un-packed BERT implementations."
PACKING ALGORITHMS,0.05319148936170213,"3.1
PACKING ALGORITHMS"
PACKING ALGORITHMS,0.05531914893617021,"The problem of optimally concatenating multiple sequences of different length until a maximum
combined length is reached can be directly framed as a bin-packing problem. Since an exact solution
is strongly NP-complete (Korte & Vygen, 2012), we propose two new heuristic algorithms that are
tailored to the NLP setting. A detailed introduction to packing is provided in Section E."
PACKING ALGORITHMS,0.0574468085106383,"3.1.1
SHORTEST-PACK-FIRST HISTOGRAM-PACKING (SPFHP)"
PACKING ALGORITHMS,0.059574468085106386,"Shortest-pack-ﬁrst histogram-packing (SPFHP) works on the bins in the sequence length histogram
(with bin size 1) rather than the individual samples. The histogram is traversed in sorted order from
longest to shortest sequences. Then, to pack the data during the traversal, we apply the worst-ﬁt
algorithm (Johnson, 1973; Yue & Zhang, 1995) such that the histogram bin being processed goes to
the “pack”1 that has the most space remaining (“shortest-pack-ﬁrst”). If the histogram bin does not
ﬁt completely, a new pack is created. We also limit the packing depth, in other words the maximum
number of sequences that are allowed in a pack. Therefore, an existing pack is only extended if it is
not already at maximum packing depth. The detailed code for the algorithm is provided in Listing 3.
The time and space complexity of the algorithm are O(n + s2
m) and O(s2
m) (Section F.2)."
PACKING ALGORITHMS,0.06170212765957447,"3.1.2
NON-NEGATIVE LEAST SQUARES HISTOGRAM-PACKING (NNLSHP)"
PACKING ALGORITHMS,0.06382978723404255,"The proposed NNLSHP algorithm is based on re-stating the packing problem as a (weighted) non-
negative least squares problem (NNLS) (Bro & De Jong, 1997) of the form wAx = wb where x ≥0.
The vector b is the histogram containing the counts of all the sequence lengths in the dataset. Next,
we deﬁne the A matrix (the “packing matrix“) by ﬁrst generating a list of all possible sequence
length combinations (“strategies”) that add up exactly to the maximum sequence length. We focus
speciﬁcally on strategies that consist of at most 3 sequences per pack (independent of b) and encode
each strategy as a column of the sparse matrix A. For example, a strategy consisting of the sequence
length 128, 128, and 256 in represented a column vector that has the value 2 at the 128th row, the
value 1 at the 256th row, and zero at all other rows. The variable x describes the non-negative
repetition count for each strategy. So a 24 in the ith row of x means that the strategy represented by
the ith column of A should repeat 24 times. Moreover, in the un-weighted setting, Ax = b states that
we would like to “mix” the pre-deﬁned strategies (columns of A) such that the number of samples
matches the histogram b, and where each strategy is used x ≥0 times. We use the residual weight
w to control the penalization of the Ax −b residual on different sequence lengths (different rows of
b). Heuristically, we set the weight of 0.09 for all sequences of length 8 or smaller because they are
considered acceptable padding sequences while all other sequence lengths get weight 1. We discuss
this heuristic choice of parameters in Section E.4.5 and E.5. The overall efﬁciency of the packing is
not greatly inﬂuenced by the weighing (less than 1% extra speed-up)."
PACKING ALGORITHMS,0.06595744680851064,"After solving wAx = wb for x ≥0 using an off-the-shelf solver, we obtain a ﬂoating point solution,
which means that the repetition counts are not necessarily integers. Since we cannot use a non-
natural number of strategies, we round the solution ˆx to the nearest integer. The error introduced"
PACKING ALGORITHMS,0.06808510638297872,"1We avoid the ambiguous terms “bin” and “sample/sequence”and use “pack” instead to refer to the multiple
sequences concatenated during packing."
PACKING ALGORITHMS,0.07021276595744681,Under review as a conference paper at ICLR 2022
PACKING ALGORITHMS,0.07234042553191489,"by this rounding is found to be negligible (a few hundred sequences in the worst case) compared to
the size of the dataset (millions of sequences). The time complexity and space complexity of the
algorithm are O(n + s5
m) and O(s3
m). Further details are provided in Section E.4."
PACKING ALGORITHMS,0.07446808510638298,"3.2
PACKEDBERT: MODEL CHANGES"
PACKING ALGORITHMS,0.07659574468085106,"This section describes how any vanilla BERT implementation should be modiﬁed for packed se-
quence processing, such that the behavior of the model is the same as when processing unpacked
sequences. Preserving the mathematical equivalence is necessary to ensure existing BERT pre-
training and ﬁne-tuning practices remain valid, as well as being required by benchmarks such as
MLPerf™(Mattson et al., 2020)."
POSITIONAL EMBEDDINGS FOR PACKED SEQUENCES,0.07872340425531915,"3.2.1
POSITIONAL EMBEDDINGS FOR PACKED SEQUENCES"
POSITIONAL EMBEDDINGS FOR PACKED SEQUENCES,0.08085106382978724,"The BERT model uses three types of embeddings: token, segment, and positional embeddings. The
latter is canonically implemented as a bias add operation, rather than a full embedding look-up. This
is possible because the positional indices are the same for every sequence. However, when using
the packed data format the position index needs to be reset with each new packed sequence. For
instance, when packing two sequences one of length 2 and one of length 3, the positional embedding
indexes that need to be picked up are [0, 1, 0, 1, 2]. To achieve this, the bias add needs to be replaced
by an embedding look-up to extract the correct positional embedding for each token in the pack.
This also requires keeping an extra input which speciﬁes the position of each token in its sequence.
This adjustment has only a minor impact on absolute accuracy/loss but is required to reach the target
accuracy (Section 4.2 and C)."
ATTENTION MASKING FOR PACKED SEQUENCES,0.08297872340425531,"3.2.2
ATTENTION MASKING FOR PACKED SEQUENCES"
ATTENTION MASKING FOR PACKED SEQUENCES,0.0851063829787234,"To maintain an implementation that is consistent with the un-packed version, tokens from different
sequences within a pack should not be able to attend to each other. This is typically achieved in other
implementations by unpacking the sequences using custom attention kernels and then doing the at-
tention per-sequence (ByteDance Inc., 2021). Instead, we propose directly masking the attention
matrix with a block-diagonal mask before the attention softmax. This is straightforward to imple-
ment in modern frameworks (see Figure 2). Naturally, there is a cost to both the mask construction
and applying it to the attention matrix (see Table 1, Section 4.1). However, it is required to keep the
accuracy (Section 4.2 and C)."
ATTENTION MASKING FOR PACKED SEQUENCES,0.08723404255319149,"1
mask = np.array([[1, 1, 1, 2, 2]])
# input
2
zero_one_mask = tf.equal(mask, mask.T)
# 0, 1 mask
3
# for use with softmax:
4
softmax_mask = tf.where(zero_one_mask, 0, -1000) "
ATTENTION MASKING FOR PACKED SEQUENCES,0.08936170212765958,"


"
ATTENTION MASKING FOR PACKED SEQUENCES,0.09148936170212765,"1
1
1
0
0
1
1
1
0
0
1
1
1
0
0
0
0
0
1
1
0
0
0
1
1 "
ATTENTION MASKING FOR PACKED SEQUENCES,0.09361702127659574,"


"
ATTENTION MASKING FOR PACKED SEQUENCES,0.09574468085106383,Figure 2: Attention mask code sample [left] and example zero-one mask [right].
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.09787234042553192,"3.2.3
CALCULATING PER-SEQUENCE LOSS AND ACCURACY"
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.1,"Canonical implementations of BERT compute the cross-entropy loss for the masked language model
on a per-token basis. However other NLP tasks, such as SQuAD, compute the loss and accuracy on
a per-sequence basis. This section discusses how to handle such tasks when training with packed
sequences. Simply feeding packs of sequences to the same implementation of cross-entropy would
result in a per-pack weighted loss. In other words, the overall loss on the micro-batch would sum-
up the losses on the individual packs, rather than individual sequences. As a result, the model
would converge to a different optimum than when running with the un-packed implementation. For
instance, a pack of a single sequence would contribute to the loss with the same weight as a pack of
three sequences."
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.10212765957446808,"To recover the per-sequence averaging behavior of the canonical un-packed BERT implementation,
we effectively “unpack” the incoming logits and labels. Once the sequences have been unpacked,
we can compute the loss on each sequence separately as usual and then add up the losses. How-
ever, rather than looping through the sequences index, we compute on all indexes in parallel (see"
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.10425531914893617,Under review as a conference paper at ICLR 2022
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.10638297872340426,"Figure 3). This minimizes the latency overhead of un-packing the loss calculation. As an example,
we show how per-sequence loss can be implemented for the pre-training task. We use the “masked
lm weight” (Devlin et al., 2019b) input tensor to represent which sequence a given masked token
belongs to (0, 1, 2 and so on). This is consistent with the canonical BERT implementation where
this input takes a value of either 1 (belonging to the sequence) or 0 (belonging to padding). The full
methodology is detailed in Listing 5 and can be applied to other classiﬁcation or pre-training tasks."
CALCULATING PER-SEQUENCE LOSS AND ACCURACY,0.10851063829787234,Figure 3: Vectorized unpacking of the sequence loss. White rectangles correspond to padding.
HYPERPARAMETER ADJUSTMENT,0.11063829787234042,"3.3
HYPERPARAMETER ADJUSTMENT"
HYPERPARAMETER ADJUSTMENT,0.1127659574468085,"In terms of convergence behavior, the primary consequence of packing is an increase in the effective
batch size (with respect to number of sequences and real tokens) with some variation over different
iterations. When the loss is averaged per-sequence, if each pack on average contains two sequences,
the batch size (per optimization step) is effectively doubled on average. Similarly, for per-token
averaged losses depends on the effectiveness of the compression. Reducing the fraction of padding
tokens in the dataset from 50% to 0% results in there being twice as many tokens in the batch i.e.
the dataset is compressed by a 2x factor. While one could subsequently reduce the computational
batch size by the packing factor (average number of sequences per pack) and keep using the same
hyperparameters, this is typically not desirable as it might imply under-utilizing the memory/com-
pute."
HYPERPARAMETER ADJUSTMENT,0.1148936170212766,"Instead, we propose an approximate heuristic for updating the decay parameters of the LAMB op-
timizer (You et al., 2019). For a packed dataset with a packing factor p, we update the decay
parameters as: β1 := βp
1, β2 := βp
2. For p = 2, this corresponds to the exact parameters for calcu-
lating momentum and velocity, when updating with the same gradient twice (Section A). A common
approach is to scale the learning rate with the batch size. However, our experiments in Section 4.2
show that this reduces convergence speed."
HYPERPARAMETER ADJUSTMENT,0.11702127659574468,"Since these adjustments are only heuristics the convergence of the model will be comparable but not
identical. In particular, it is unlikely that simply adjusting the hyperparameters will fully undo the
impact of the increased batch size. However, with these adjustments, researchers should be able to
continue to use existing conﬁgurations."
EXPERIMENTS,0.11914893617021277,"4
EXPERIMENTS"
BIN-PACKING ALGORITHM COMPARISON,0.12127659574468085,"4.1
BIN-PACKING ALGORITHM COMPARISON"
BIN-PACKING ALGORITHM COMPARISON,0.12340425531914893,"We evaluate our algorithms using the following metrics: number of packs, number of all tokens,
number of padding tokens, solution time of the packing algorithm (after histogram and strategy
creation), number of strategies used, packing efﬁciency (the fraction of non-padding tokens in the
packed dataset), the speed-up achieved compared to not packing (depth 1), and the average number
of sequences per sample (packing factor). For SPFHP, we analyse different (maximum) packing
depth, since packing is less efﬁcient with smaller depth and we want to get a general understanding
on how the packing depth inﬂuences the processing time. For NNLSHP, we focus on packing depth
3 because it packs the data sufﬁciently well."
BIN-PACKING ALGORITHM COMPARISON,0.125531914893617,"For the speed-up analysis, we focus on the intelligence processing unit (IPU) (Jia et al., 2019) (IPU-
M2000, 16 accelerator chips). A GPU dynamically loads the code into the accelerator; in contrast,
the IPU works with a static pre-compiled engine that gets loaded onto the chip at the start of the run.
While other approaches result in excessive padding or continuous changes of the code, our approach
can work with the same code for the whole dataset. So in this setting the IPU architecture would"
BIN-PACKING ALGORITHM COMPARISON,0.1276595744680851,Under review as a conference paper at ICLR 2022
BIN-PACKING ALGORITHM COMPARISON,0.12978723404255318,"especially beneﬁt from our approach since it avoids code changes. Nevertheless, it can be applied
to any implementation on GPU or TPU. For determining the speed-up, we take advantage of the
precompiled kernel. Since time measurements are quite noisy, we can proﬁle the kernel and how
many cycles it takes for processing a batch. That way, we can determine the overhead (in cycles)
from processing the additional attention masking and for unpacking the loss. Combining overhead
and packing factor, we get the speed-up estimate. No experiment repetitions are required since the
algorithms and measurements are deterministic."
BIN-PACKING ALGORITHM COMPARISON,0.13191489361702127,"The main results for the performance metric evaluation are displayed in Table 1. The processing time
for SBFHP was around 0.03s and independent from the packing depth. We see that the overhead
slightly increases with packing depth but that the beneﬁts of packing outweigh the cost. The best
speed-up is obtained with NNLSHP at depth 3 which required 28.4s for processing and ran out of
memory for larger depth. With a value of 1.913, it is close to the theoretical upper bound of 2.001.
The results show that efﬁciency, packing factor, and speed-up can be viewed inter-changeably. The
amount of time needed to process a sample (a pack of sequences) is barely changed relative to the
un-packed implementation. The packing factor or the improvement in efﬁciency effectively provide
an accurate estimate of the speed-up."
BIN-PACKING ALGORITHM COMPARISON,0.13404255319148936,Table 1: Key performance results of proposed packing algorithms (SPFHP and NNLSHP).
BIN-PACKING ALGORITHM COMPARISON,0.13617021276595745,"packing
packing
# packs
efﬁciency
packing
overhead
realized
depth
algorithm
[M]
(%)
factor
(%)
speed-up
1
NONE
16.280
49.97
1.000
0.000
1.000
1
SORT
16.280
99.99
2.000
100
1.000
≈10
GREEDY
≈10.397
≈78.24
≈1.566
≈4.48
≈1.5
2
SPFHP
10.102
80.52
1.612
4.283
1.544
3
SPFHP
9.095
89.44
1.790
4.287
1.716
3
NNLSHP
8.155
99.75
1.996
4.287
1.913
4
SPFHP
8.659
93.94
1.880
4.294
1.803
8
SPFHP
8.225
98.90
1.979
4.481
1.895
16/max
SPFHP
8.168
99.60
1.993
4.477
1.905"
BIN-PACKING ALGORITHM COMPARISON,0.13829787234042554,"Packing depth describes the maximum number of packed sequences. NONE is the baseline BERT
implementation, whereas SORT corresponds to sorted batching, and GREEDY concatenates se-
quences as they arrive until they exceed 512 tokens. Setting no limit resulted in a maximum packing
depth of 16. The number of packs describes the length of the new packed dataset. Efﬁciency is
the percentage of real tokens in the packed dataset. The packing factor describes the resulting po-
tential speed-up compared to packing depth 1. With overhead, we denote the percentage decrease
in throughput due to changes to the model to enable packing (such as the masking scheme intro-
duced in Section 3.2.2). The realized speed-up is the combination of the speed-up due to packing
(the packing factor) and the decrease in throughput due to the overhead. It is used to measure the
relative speed-up in throughput and the overhead from masking and loss adjustment."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.14042553191489363,"4.2
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1425531914893617,"For depth 1 (classic BERT) and NNLSHP with depth 3, we additionally evaluate on the MLPerf™
version 0.7 BERT pre-training benchmark (Mattson et al., 2020). Brieﬂy, this involves training from
a standard checkpoint to a masked-language model accuracy of 71.2% using 3 million sequences
with a maximum length of 512 tokens (refer to MLCommons (2020) for details). Following this
standardized benchmark supports reproduction of results even on other systems and makes sure
that the reproduction effort is moderate and setup rules are clearly documented. We compare the
resulting speed-up as well as the respective learning curves by evaluating the data on a held-out
validation dataset. The objective of this additional evaluation is to analyse if convergence behavior
is changed by the packing strategy and if the theoretical speed-up can be achieved in practice."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.14468085106382977,"With packing, we effectively increase the average batch size by the packing factor (≈2). However,
with a different batch size, different hyperparameters are required (see Section 3.3) and there is no
mapping that will generate exact matching of results but only heuristics. In a ﬁrst comparison, we
use the same hyperparameters when comparing packed and unpacked training except for cutting the
accumulation count by half. This way, we make sure that the batch size is constant on average."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.14680851063829786,Under review as a conference paper at ICLR 2022
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.14893617021276595,"In the second comparison, we evaluate our heuristics and how they compensate the difference in
batch size. This setup is more desirable because it is beneﬁcial to use the hardware to its full
potential and cutting the batch size by half usually reduces throughput. In the third comparison, we
compare two optimized setups."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.15106382978723404,"The learning curves are displayed in Figure 4. In the ﬁrst setup, we see the curves almost matching
perfectly when normalizing by the numbers of samples processed. Differences can be explained
by the variation of the number of sequences in the packing batch, and general noise in the training
process. Especially after the initial phase, the curves show a near-identical match. The second setup
shows bigger differences since changing the batch size and hyperparameters changes the training
dynamics. We observe slower convergence early on in training due to the increased batch size. This
is expected. The adjustment of the learning rate actually decreases performance probably because
we correct for the increased number of sequences already in the modiﬁed loss. With the adjustment
of the decay parameter of LAMB, we see matching performance at the later training stages. How-
ever, it is not feasible to completely recover the early convergence behavior of the smaller batch
size by adjusting the hyperparameters. For instance doubling the batch size of unpacked BERT to
3000 and adjusting the LAMB decay parameters leads to more of a slow down in convergence than
when running packed BERT with a batch size of 1500 and a packing factor of 2. n practice, our
implementations exceeds the estimated 1.913 maximum speed-up. This estimate is based on the
reduction in the computational work needed to process the dataset. However, packing the data also
reduces the latency of the transferring the data to the device. Figure 4 shows that the realized total
speed-up from packing exceeds 2x. On Squad 1.1 after full packed pretraining, F1 score is reduced
by 0.003% whereas the EM score is increased by 0.049% (Section D)."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.15319148936170213,"0
1
2
3
samples
1e6 1.5 2.0 2.5 3.0 3.5"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.15531914893617021,training loss
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1574468085106383,"classic, bs: 1500, beta: 0.81
packed, ebs: 768*2, beta: 0.81"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1595744680851064,"0
1
2
3
samples
1e6 2 3 4"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.16170212765957448,training loss
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.16382978723404254,"classic, beta: 0.81
packed, beta: 0.66
packed, beta: 0.66, double lr
packed, beta: 0.81, double lr"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.16595744680851063,"0.0
0.5
1.0
1.5
2.0
relative time 1.5 2.0 2.5 3.0 3.5"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.16808510638297872,training loss
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1702127659574468,"classic, bs: 1500, beta: 0.81
classic, bs: 3000, beta: 0.66
packed, ebs: 1500*2, beta: 0.66"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1723404255319149,"Figure 4: Comparison of learning curves for packed and unpacked processing, where all experiments
converged to the target accuracy within the same number of training samples(3 million). [left] same
effective batch size (ebs is batch size times packing factor), [middle] different heuristic adjustments
of the hyperparameters (batch size 1500 for all runs, such that ebs for packed runs is 1500 ∗2), and
[right] realized speed-up from packing (in excess of desired 2x)."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.17446808510638298,"4.3
SCALING ANALYSIS: IMPACT OF THE NUMBER OF ACCELERATORS"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.17659574468085107,"A further advantage of packing over competing un-padding approaches is the inherent load balanc-
ing provided by packing. So called un-padding approaches rely on dynamically launching custom
kernels that ignore padding. A stated advantage of such implementations is the ability to avoid com-
puting the complete (512 x 512) attention matrix. This provides additional computational savings
compared to packing, where the attention matrix is computed in its entirety and then masked. Be-
cause of these additional savings, un-padding can exceed the theoretical upper bound for speed-up
from packing (2.013 on Wikipedia). As a result of the dynamic nature of the approach, the process-
ing time with un-padding is different for each sequence in the batch, and the amount of time required
to process a batch of sequences will be determined by the processing time of the longest sequence in
the batch (with the sequences being processed in parallel). Furthermore, in the multiple accelerator
setting the processing time on each device will vary depending on the sequences in the batch that it
receives. Devices which ﬁnish early have to wait for the slowest device to ﬁnish before exchanging
gradients. This load-imbalance between the devices (and inside the batch) leads to a considerable
decrease in the speed-up from un-padding as the number of accelerators is increased (see Figure 5)."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.17872340425531916,"In contrast, packing (our approach) is inherently load-balanced. The processing time on each ac-
celerator is independent of the content inside the batch received by the device. Any number of
accelerators can therefore operate in unison without having to wait for the slowest batch to process"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.18085106382978725,Under review as a conference paper at ICLR 2022
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1829787234042553,"(all per-device batches are equally fast). To demonstrate the severity of the load-imbalance issue, we
consider the scaling of an un-padding approach with a per-device batch size of 32 running on eight
devices (NVIDIA, 2020). From there, we readily extrapolate the performance to both larger and
smaller cluster sizes by ﬁtting a Gumbel distribution to the observed processing times (Section B).
On a single device with batch size 32 un-padding outperforms packing and exceeds the theoretical
upper-bound for packing. As the number of devices increases to two or more, the proposed pack-
ing approach outperforms the dynamic un-padding approach. On a cluster with 32 accelerators the
speed-up from un-padding drops to 50% and with 2048 devices the speed-up is only 30%. In con-
trast, the speed-up due to packing is independent of the number of accelerators and stays at 1.913.
Switching to a smaller batch size would reduce the load-imbalance issue to some extent, but would
also result in under-utilization of the available memory and compute."
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.1851063829787234,"1
2
4
8
16
32
64
128
256
512
1024
2048
number of accelerators 1.000 1.100 1.200 1.300 1.400 1.500 1.600 1.700 1.800 1.913 2.013"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.18723404255319148,estimated speed-up
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.18936170212765957,"theoretical upper-bound
packing (our approach)
un-padding
padding"
LEARNING CURVES AND HYPERPARAMETER ADJUSTMENT,0.19148936170212766,"Figure 5: Comparison of the theoretical speed-up achievable as the number of accelerators is in-
creased."
CONCLUSION,0.19361702127659575,"5
CONCLUSION"
CONCLUSION,0.19574468085106383,"We showed that packing can be easily implemented without the need for any custom kernels while
still providing a 2x speed-up without a loss of accuracy. Additionally, we showed that any additional
speed-ups resulting from dynamic un-padding approaches diminish for even moderate batch sizes
or when additional accelerators are added. In contrast, packing is load-balanced and maintains
the 2x throughput when scaling to large numbers of accelerators. Furthermore, the computational
overhead introduced by the positional embedding, the attention mask, and potentially the packed per-
sequence loss are small compared to the achieved acceleration. This overhead remains below 5%
on the IPU for all tested packing depths. The efﬁcient packing algorithms presented in this paper
enable us to efﬁciently pack millions of sequences in a matter of seconds. Compared to both the
pre-processing time for the Wikipedia dataset and the training runtime, this overhead is negligible.
Furthermore, we showed that performing packing as a pre-processing step does not signiﬁcantly
impact the training convergence. Our proposed hyperparameter adjustment scheme additionally
helps practitioners easily modify existing validated optimizer settings for use with packed BERT.
Further exploration of hyperparameter selection is left to future work."
CONCLUSION,0.19787234042553192,"When performing packing as a pre-processing step, the proposed NNLSHP and SPFHP methods
achieve near optimal compression efﬁciency. In this ofﬂine setting, we are able to build a histogram
of the dataset, and thus achieve linear time complexity with respect to the number of samples. This
makes packing modern datasets with millions of sequences possible. In the future, it would be
interesting to extend SPFHP to the online setting where a histogram of the entire dataset cannot
be built. Another interesting direction is the packing of images of different sizes to help accel-
erate computer-vision applications. This is especially relevant given the recent advances in the
use of transformer-based approaches in the computer vision domain, for example the visual trans-
former (Wu et al., 2020). Masking out the self-attention within transformers is easier to implement
than avoiding cross-contamination of convolutions applied to packed images. Future work should
explore improving the performance of other models (RoBERTa, GPT-3, T5) by avoiding contamina-
tion between non-contiguous segments from different documents. Even BERT itself might beneﬁt
from avoiding contamination between the two concatenated segments."
CONCLUSION,0.2,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.20212765957446807,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.20425531914893616,"All code for the packing algorithms is available in the appendix (Section O) and is directly linked to
our GitHub page to simplify the download and usage. We even provide code for different variants
and the histograms of sequence length for different datasets that got tokenized for BERT training of
ﬁne-tuning."
REPRODUCIBILITY STATEMENT,0.20638297872340425,"To generate the learning curves, our public submission to MLPerf™could be used and we are
preparing further code releases in other frameworks. To encourage the use of the adjustments of
models for packed sequences, we additionally provide detailed explanations and code snippets in
TensorFlow."
REPRODUCIBILITY STATEMENT,0.20851063829787234,"Detailed mathematical formulas (Section B and E), a theorem proof (Section A), and complexity
calculations (Section F) are provided in the appendix to support our claims in this paper in full
detail."
REFERENCES,0.21063829787234042,REFERENCES
REFERENCES,0.2127659574468085,"L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization Methods for Large-Scale Machine
Learning. SIAM Review, 60(2):223–311, jan 2018. ISSN 0036-1445. doi: 10.1137/16M1080173."
REFERENCES,0.2148936170212766,"Rasmus Bro and Sijmen De Jong.
A fast non-negativity-constrained least squares algorithm.
Journal of Chemometrics, 11(5):393–401, sep 1997.
ISSN 0886-9383.
doi: 10.1002/(SICI)
1099-128X(199709/10)11:5⟨393::AID-CEM483⟩3.0.CO;2-L."
REFERENCES,0.2170212765957447,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Advances in
Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020), may 2020. URL
http://arxiv.org/abs/2005.14165."
REFERENCES,0.21914893617021278,"ByteDance Inc. Effective Transformer. https://github.com/bytedance/effective_
transformer, 2021."
REFERENCES,0.22127659574468084,"Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. NAACL HLT 2019 - 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference, 1:4171–4186, oct 2019a. URL http://arxiv.
org/abs/1810.04805."
REFERENCES,0.22340425531914893,"Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova.
BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding.
https://github.com/
google-research/bert, 2019b."
REFERENCES,0.225531914893617,"Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. Pre-training data creation
script for BERT. https://github.com/google-research/bert/blob/master/
create_pretraining_data.py#L243, 2019c."
REFERENCES,0.2276595744680851,"William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter
Models with Simple and Efﬁcient Sparsity. arXiv, jan 2021. URL http://arxiv.org/abs/
2101.03961."
REFERENCES,0.2297872340425532,"Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the Graphcore
IPU architecture via microbenchmarking. ArXiv, abs/1912.03413, 2019."
REFERENCES,0.23191489361702128,"David S Johnson. Near-optimal bin packing algorithms. PhD thesis, Massachusetts Institute of
Technology, 1973."
REFERENCES,0.23404255319148937,Under review as a conference paper at ICLR 2022
REFERENCES,0.23617021276595745,"David S. Johnson and Michael R. Garey. A 7160 theorem for bin packing. Journal of Complexity,
1(1):65–106, oct 1985. ISSN 0885064X. doi: 10.1016/0885-064X(85)90022-6. URL https:
//linkinghub.elsevier.com/retrieve/pii/0885064X85900226."
REFERENCES,0.23829787234042554,"Bernhard Korte and Jens Vygen. Combinatorial Optimization, volume 21 of Algorithms and Com-
binatorics.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.
ISBN 978-3-642-24487-
2.
doi: 10.1007/978-3-642-24488-9.
URL http://link.springer.com/10.1007/
978-3-642-24488-9."
REFERENCES,0.2404255319148936,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. ALBERT: A lite BERT for self-supervised learning of language representations. CoRR,
abs/1909.11942, 2019. URL http://arxiv.org/abs/1909.11942."
REFERENCES,0.2425531914893617,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pre-
training Approach. arXiv, jul 2019. URL http://arxiv.org/abs/1907.11692."
REFERENCES,0.24468085106382978,"P. Mattson, V. J. Reddi, C. Cheng, C. Coleman, G. Diamos, D. Kanter, P. Micikevicius, D. Patterson,
G. Schmuelling, H. Tang, G. Wei, and C. Wu. MLPerf: An Industry Standard Benchmark Suite
for Machine Learning Performance. IEEE Micro, 40(2):8–16, 2020. doi: 10.1109/MM.2020.
2974843."
REFERENCES,0.24680851063829787,"Qi Meng, Wei Chen, Yue Wang, Zhi Ming Ma, and Tie Yan Liu. Convergence analysis of dis-
tributed stochastic gradient descent with shufﬂing. Neurocomputing, 337:46–57, apr 2019. ISSN
18728286. doi: 10.1016/j.neucom.2019.01.037."
REFERENCES,0.24893617021276596,"MLCommons.
v0.7 Results.
https://mlcommons.org/en/training-normal-07/,
2020. Result not veriﬁed by MLPerf. Throughput/speedup is not the primary metric of MLPerf.
MLPerf name and logo are trademarks. See www.mlperf.org for more information."
REFERENCES,0.251063829787234,"NVIDIA.
Reference numbers for BERT un-padding results.
https://github.com/
mlcommons/training_results_v0.7/blob/master/NVIDIA/results/
dgxa100_ngc20.06_pytorch/bert/result_0.txt, 2020. Throughput/speedup is not
the primary metric of MLPerf. MLPerf name and logo are trademarks. See www.mlperf.org
for more information."
REFERENCES,0.2531914893617021,"NVIDIA.
Faster
Transformer.
https://github.com/NVIDIA/
DeepLearningExamples/tree/master/FasterTransformer/v1, 2021."
REFERENCES,0.2553191489361702,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019."
REFERENCES,0.2574468085106383,"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015
IEEE International Conference on, pp. 5206–5210. IEEE, 2015."
REFERENCES,0.25957446808510637,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed Text-
to-Text Transformer. Journal of Machine Learning Research, 21, oct 2019. URL http://
arxiv.org/abs/1910.10683."
REFERENCES,0.26170212765957446,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016. Association
for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.
org/anthology/D16-1264."
REFERENCES,0.26382978723404255,"Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, et al. Lingvo: a modular and scalable
framework for sequence-to-sequence modeling, 2019."
REFERENCES,0.26595744680851063,"Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019."
REFERENCES,0.2680851063829787,Under review as a conference paper at ICLR 2022
REFERENCES,0.2702127659574468,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, un-
deﬁnedukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st
International Conference on Neural Information Processing Systems, NIPS’17, pp. 6000–6010,
Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964."
REFERENCES,0.2723404255319149,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceed-
ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/
W18-5446."
REFERENCES,0.274468085106383,"Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.
arXiv preprint arXiv:1805.12471, 2018."
REFERENCES,0.2765957446808511,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6."
REFERENCES,0.27872340425531916,"Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi
Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based
image representation and processing for computer vision, 2020."
REFERENCES,0.28085106382978725,"Tensorﬂow XLA.
XLA: Optimizing Compiler for Machine Learning.
https://www.
tensorflow.org/xla, 2021."
REFERENCES,0.28297872340425534,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large Batch Optimization for Deep
Learning: Training BERT in 76 minutes. arXiv, apr 2019. URL http://arxiv.org/abs/
1904.00962."
REFERENCES,0.2851063829787234,"Minyi Yue and Lei Zhang. A simple proof of the inequality MFFD(L) ≤71/60OPT(L) + 1, L
for the MFFD bin-packing algorithm. Acta Mathematicae Applicatae Sinica, 11(3):318–330, jul
1995. ISSN 01689673. doi: 10.1007/BF02011198."
REFERENCES,0.2872340425531915,Under review as a conference paper at ICLR 2022
REFERENCES,0.28936170212765955,Appendix
REFERENCES,0.29148936170212764,TABLE OF CONTENTS
INTRODUCTION,0.2936170212765957,"1
Introduction
1"
WIKIPEDIA BERT PRE-TRAINING DATASET,0.2957446808510638,"2
Wikipedia BERT pre-training dataset
3"
METHODS,0.2978723404255319,"3
Methods
4"
METHODS,0.3,"3.1
Packing algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4"
METHODS,0.3021276595744681,"3.2
packedBERT: model changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5"
METHODS,0.30425531914893617,"3.3
Hyperparameter adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
EXPERIMENTS,0.30638297872340425,"4
Experiments
6"
BIN-PACKING ALGORITHM COMPARISON,0.30851063829787234,"4.1
Bin-packing algorithm comparison
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6"
BIN-PACKING ALGORITHM COMPARISON,0.31063829787234043,"4.2
Learning curves and hyperparameter adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
BIN-PACKING ALGORITHM COMPARISON,0.3127659574468085,"4.3
Scaling analysis: Impact of the number of accelerators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8"
CONCLUSION,0.3148936170212766,"5
Conclusion
9"
CONCLUSION,0.3170212765957447,"A Theorem on LAMB hyperparameter correction heuristic
14"
CONCLUSION,0.3191489361702128,"B
Un-padding scaling estimate
16"
CONCLUSION,0.32127659574468087,"C Ablation study
17"
CONCLUSION,0.32340425531914896,"D SQuAD 1.1
17"
CONCLUSION,0.32553191489361705,"E
Technical background on packing
18"
CONCLUSION,0.3276595744680851,"E.1
Canonical packing problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
CONCLUSION,0.32978723404255317,"E.2
Approximate bin-packing problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18"
CONCLUSION,0.33191489361702126,"E.3
Deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
CONCLUSION,0.33404255319148934,"E.4
Non-negative least squares histogram-packing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19"
CONCLUSION,0.33617021276595743,"E.5
Discussion of residual weight choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
CONCLUSION,0.3382978723404255,"F
Complexity analysis of the proposed packing approaches
23"
CONCLUSION,0.3404255319148936,"F.1
Complexity Analysis of non-negative least-squares histogram-packing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
CONCLUSION,0.3425531914893617,"F.2
Complexity Analysis of shortest-pack-ﬁrst histogram-packing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
CONCLUSION,0.3446808510638298,"G Performance Comparison to GREEDY Packing in T5
24"
CONCLUSION,0.3468085106382979,"H Packing SQuAD 1.1
25"
CONCLUSION,0.34893617021276596,"I
Packing GLUE
26"
CONCLUSION,0.35106382978723405,"J
Packing Audio Data (LibriSpeech)
27"
CONCLUSION,0.35319148936170214,"K Packing Paper Abstracts (PubMed)
28"
CONCLUSION,0.3553191489361702,"L
Further learning curves
29"
CONCLUSION,0.3574468085106383,"M Fine-tuned longest-pack-ﬁrst histogram-packing
30"
CONCLUSION,0.3595744680851064,"N Extended NNLS with padding token weighting
31"
CONCLUSION,0.3617021276595745,"O Packing source code
32"
CONCLUSION,0.3638297872340426,Under review as a conference paper at ICLR 2022
CONCLUSION,0.3659574468085106,BROADER IMPACT
CONCLUSION,0.3680851063829787,"We showed that when pre-training BERT on Wikipedia, the computational overhead taken to pro-
cess padding tokens is roughly 50%. By eliminating this wasted computational time, the approach
presented in this paper paves a way to halving the carbon footprint of training BERT-based models."
CONCLUSION,0.3702127659574468,"Furthermore, our approach circumvents the need for custom kernels, making the beneﬁts of packing
readily accessible to a broader audience of NLP practitioners. As such, we are hopeful the research
will have a positive impact on the NLP community, and do not see any disadvantage of using this
approach."
CONCLUSION,0.3723404255319149,"The beneﬁt of our algorithm is based on two assumptions: A skewed length distribution in the
training dataset and a hardware setup that trains efﬁciently on a ﬁxed batch size. If efﬁcient train-
ing is possible, with a variable batch size approaches like FasterTransformer and the fairseq sorted
batch approach will result in the same or even larger beneﬁts (due to smaller self-attention matri-
ces). If the dataset is generated differently like in GPT models (Brown et al., 2020) and RoBERTa
(FULL-SENTENCES) (Liu et al., 2019), all sequences will be at full length and sequences cannot
be concatenated and there is indeed no beneﬁt in packing sequences. However, strategies that reach
full sequence length usually combine segments from different unrelated document sources which
can result in reduced performance. Even in the normal BERT model, there might be this contam-
ination between segments from different documents. Our paper introduced an approach to avoid
the contamination between sequences. However, the same approach could also be applied to avoid
contamination between segments and it remains future work to explore its beneﬁts beyond BERT
pretraining."
CONCLUSION,0.37446808510638296,"Future work would need to investigate the applicability of packing on text produced by different cul-
tures and in different languages. We have already shown that the speed-up resulting from using our
methods does not only occur when pre-training BERT on Wikipedia but also on other datasets such
as SQuAD and GLUE. Furthermore, the sentence length distribution of the original English language
text shows similar characteristics. Our research leads us to believe that compressible distributions
arise naturally in language tasks and beyond, for instance in DNA sequence lengths (Hansen et al.,
2017), protein lengths (Guill´en et al., 2013), and speech (Section J). Many such sequence modelling
workloads are based on variations of the BERT/transformer architecture and would therefore easily
beneﬁt from our acceleration."
CONCLUSION,0.37659574468085105,"Failures in NLP can have a big impact on society; many technologies, such as Alexa, Siri, and
Google Home, rely on them. Whilst any errors arising from our approach can be avoided, one po-
tential source of error comes from the implementation. Both the attention mask and the per-sequence
loss need to be modiﬁed to support packing. These changes are signiﬁcantly smaller than those re-
quired by custom kernels, however they may still be time consuming to implement and debug. To
help mitigate the risk of any implementation errors, we share our reference implementations of the
required changes in the appendix."
CONCLUSION,0.37872340425531914,"A
THEOREM ON LAMB HYPERPARAMETER CORRECTION HEURISTIC"
CONCLUSION,0.38085106382978723,"With packing, the effective batch size changes and hence hyperparameters of the LAMB opti-
mizer (You et al., 2019) need to be adjusted. For a packed dataset with a packing factor p, we
update the decay parameters as: β1 := βp
1, β2 := βp
2. For instance if β1 = 0.81 for the un-packed
dataset, then for a packed dataset with an average of 2 sequences per sample one should use a value
of 0.812 ≈0.66 instead. Assuming no or only minor changes in gradients and p being a natural
number, we can prove that this heuristic is the exact solution to make sure that momentum and ve-
locity in LAMB are unaffected by packing. This can be proven by mathematical induction. Note
that p ≥1 by deﬁnition.
Theorem 1. For any p ∈N and assuming that respective gradients on a batch of b random samples
are (approximately) the same, choosing"
CONCLUSION,0.3829787234042553,"β1 := βp
1
(1)"
CONCLUSION,0.3851063829787234,"β2 := βp
2.
(2)
as hyperparameters in the LAMB optimizer ensures that the momentum and velocity after p separate
update steps are the same as with one packed update step with p × b samples."
CONCLUSION,0.3872340425531915,Under review as a conference paper at ICLR 2022
CONCLUSION,0.3893617021276596,Proof.
CONCLUSION,0.39148936170212767,"• Base Case:
For p = 1 the left and right side of the equation are the same which matches exactly the
unpacked case. Hence, the theorem holds for p = 1."
CONCLUSION,0.39361702127659576,"• Inductive hypothesis: Suppose the theorem holds for all values of p up to some k, k ≥1."
CONCLUSION,0.39574468085106385,• Inductive proposition: The theorem holds for p = k + 1.
CONCLUSION,0.39787234042553193,"• Proof of the inductive step: Let l be the loss function, wt the weight vector after t updates,
and xt
1, . . . , xt
b the respective underlying data to calculate the gradient gt. For a single
update step in LAMB with batch size b samples, we compute the gradient"
CONCLUSION,0.4,"gt = 1 b b
X i=1"
CONCLUSION,0.4021276595744681,"∂l
∂w(xt
i, wt).
(3)"
CONCLUSION,0.40425531914893614,"Since g1 ≈g2 ≈. . . ≈gk+1, We have with the inductive hypothesis and the deﬁnitions in
LAMB:"
CONCLUSION,0.40638297872340423,"mk = βk
1m0 + (1 −βk
1)g1
(4)"
CONCLUSION,0.4085106382978723,"vk = βk
2v0 + (1 −βk
2)g2
1
(5)"
CONCLUSION,0.4106382978723404,Now we can calculate (with g1 ≈gk+1)
CONCLUSION,0.4127659574468085,"mk+1 = β1mk + (1 −β1)gk+1
(6)"
CONCLUSION,0.4148936170212766,"≈β1
 
βk
1m0 + (1 −βk
1)g1

+ (1 −β1)g1
(7)"
CONCLUSION,0.41702127659574467,"= βk+1
1
m0 + (1 −βk+1
1
)g1
(8)"
CONCLUSION,0.41914893617021276,"The calculation for vk is the same. As reference for a packed update with p = k + 1 with
β1 and β2, we would get g = 1 pb p
X j=1 b
X i=1"
CONCLUSION,0.42127659574468085,"∂l
∂w(xj
i, w1) = 1 p p
X j=1"
B,0.42340425531914894,"1
b b
X i=1"
B,0.425531914893617,"∂l
∂w(xj
i, w1) ! ≈1 p p
X"
B,0.4276595744680851,"j=1
g1 = g1
(9)"
B,0.4297872340425532,"since we are calculating gradients over b samples which are assumed to be approximately
the same. Consequently, the updates for momentum and velocity would be"
B,0.4319148936170213,"mk = β1m0 + (1 −β1)g1
(10)"
B,0.4340425531914894,"vk = β2v0 + (1 −β2)g2
1.
(11)"
B,0.43617021276595747,"Hence, β1 = βk+1
1
and β2 = βk+1
2
is required to map to the formula with the consecutive
updates (for the same amount of data)."
B,0.43829787234042555,• Conclusion: The theorem holds for any p ∈N.
B,0.44042553191489364,"Since we proved that the formulas β1 := βp
1, β2 := βp
2. hold for all p ∈N, p ≥1, it is safe to
assume that it is an appropriate heuristic for all p ∈R, p ≥1."
B,0.4425531914893617,Under review as a conference paper at ICLR 2022
B,0.44468085106382976,"B
UN-PADDING SCALING ESTIMATE"
B,0.44680851063829785,"Firstly, we retrieve the per-batch processing time for an un-padding implementation running pre-
training on the Wikipedia dataset from (NVIDIA, 2020). These processing times were obtained
using 8 GPUs each with a per-device batch size of 32. We also retrieve the throughput numbers for
the same system running with padding from (NVIDIA, 2021) and use that as the baseline to compare
the un-padded throughput against."
B,0.44893617021276594,"The throughput on the 8 GPU system is effectively limited by the slowest of the eight batches being
processed in parallel. The Gumbel distribution is particularly suited to modelling the maximum or
minimum value of a ﬁxed size collection of i.i.d. samples (in this case batches). We observe that on
8 GPUs the throughput (i.e. speed-up) distribution indeed closely resembles a Gumbel distribution
with α1 = 1.6 and β8 = 0.13 as shown in Figure 6."
B,0.451063829787234,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
speed-up from un-padding 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
B,0.4531914893617021,probability density
B,0.4553191489361702,8 GPUs with (bs=32 each)
B,0.4574468085106383,"fitted Gumbel
data"
B,0.4595744680851064,"0
1
2
3
4
speed-up from un-padding 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
B,0.46170212765957447,1 GPU with bs=32
B,0.46382978723404256,Estimate
B,0.46595744680851064,"Figure 6: Left: Speed-up from un-padding on 8 GPUs closely resembles a Gumbel distribution.
Right: statistical estimate of speed-up distribution on a 1 GPU system running un-padding"
B,0.46808510638297873,"We can extrapolate the performance on the 8 GPU system to larger clusters by recognizing that
the processing time for each cluster is effectively determined by the slowest batch being processed.
Speciﬁcally, we could randomly sample (without replacement) two processing times for the 8 GPU
system, and record the max of the two as the processing time for a system of 16 GPUs. However,
this simple approach is too sensitive to outliers in the data and would result in an under-estimate
of the performance of un-padding on large systems. We mitigate the effect of outliers in the data
by avoiding directly sampling the processing times. Instead, we ﬁt a Gumbel distribution to the
processing times of a single batch of size 32 running on one GPU. To perform the ﬁt, we observe
that the cdf on one GPU (P1) is related to the cdf on 8 GPUs (P8) through (Kotz & Nadarajah,
2000)(section 1.3):"
B,0.4702127659574468,"(1 −P8(s)) = (1 −P1(s))8
(12)"
B,0.4723404255319149,"In other words, if the speed-up on the cluster is larger than s, this implies that the speed-up on
every GPUs in the cluster was at least s. Assuming P1 is Gumbel and given the 8 GPU Gumbel
parameters α8 and β8, we need to ﬁt two parameters, α1 and β1. Consequently for the median
(s = α8 −β8 ln(ln(2)), P8(s) = 0.5), we have:"
B,0.474468085106383,"0.5 = (1 −P1(α8 −β8 ln(ln(2))))8 .
(13)"
B,0.4765957446808511,"And since P8 is Gumbel, we also have an equation for the mode (s = α8, P8(s) = e−1):"
B,0.4787234042553192,"(1 −e−1) = (1 −P1(α8))8 .
(14)"
B,0.4808510638297872,"We solve these two non-linear equations simultaneously using the standard SciPy optimization pack-
age."
B,0.4829787234042553,Under review as a conference paper at ICLR 2022
B,0.4851063829787234,Listing 1: Infer Gumble distribution parameters.
IMPORT NUMPY AS NP,0.48723404255319147,"1
import numpy as np
2
from scipy import stats, optimize
3
alpha_8 = 1.6038
4
beta_8 = 0.1288
5
def g(x):
6
alpha_1, beta_1 = x
7
dist = stats.gumbel_r(loc=alpha_1, scale=beta_1)
8
# Equations for median and mode
9
median = alpha_8 - beta_8*np.log(np.log(2))
10
equation1 = 0.5 - dist.sf(median)**n_gpu
11
mode = alpha_8
12
equation2 = (1-np.exp(-1)) - dist.sf(mode)**n_gpu
13
return (equation1**2 + equation2**2)
14
15
res = optimize.minimize(g, [alpha_8, beta_8], method=""Nelder-Mead"")
16
alpha_1, beta_1 = res.x"
IMPORT NUMPY AS NP,0.48936170212765956,"The resulting estimated speed-up Gumbel distribution for a single device has α = 1.94, β = 0.108
and is shown in Figure 6 [right]. To simulate the performance of a cluster of size n with a batch size
of 32 per device, we take the minimum over n samples from this distribution. Repeating this process
to generate many samples allows us to estimate the expected speed-up for any given cluster size.
Unfortunately, we cannot make any statistical inference about the processing times of individual
sequences since the data is only provided at the granularity of 32 sequences per batch, and it is not
clear how much of the computation is done in parallel and how much in serial."
IMPORT NUMPY AS NP,0.49148936170212765,"C
ABLATION STUDY"
IMPORT NUMPY AS NP,0.49361702127659574,"So far, we have shown that with the introduced adjustments, we can match the accuracy of unpacked
BERT. In the following, we wanted to analyze in how far the masking adjustment is required. In
Figure 7, we can see that without our adjustments, training loss and accuracy worsen drastically and
a longer training time does not lead to a recovery. When not adjusting the positional embedding, the
loss and accuracy almost match. However, the accuracy stalls at 71.8% and does not reach the target
accuracy of 72.1%. So overall, both adjustments are crucial."
IMPORT NUMPY AS NP,0.4957446808510638,"0
500
1000
1500
Iteration count 50 55 60 65 70"
IMPORT NUMPY AS NP,0.4978723404255319,training accuracy (percent)
IMPORT NUMPY AS NP,0.5,"no mask adjustment
packed BERT baseline
no pos. emb. adjustment"
IMPORT NUMPY AS NP,0.502127659574468,"0
500
1000
1500
Iteration count"
IMPORT NUMPY AS NP,0.5042553191489362,2 × 100
IMPORT NUMPY AS NP,0.5063829787234042,3 × 100
IMPORT NUMPY AS NP,0.5085106382978724,4 × 100
IMPORT NUMPY AS NP,0.5106382978723404,training loss
IMPORT NUMPY AS NP,0.5127659574468085,"no mask adjustment
packed BERT baseline
no pos. emb. adjustment"
IMPORT NUMPY AS NP,0.5148936170212766,"Figure 7: Comparison of learning curves with and without mask or positional embedding adjustment
in our packed BERT approach. The grey accuracy baseline to reach is 72.1%."
IMPORT NUMPY AS NP,0.5170212765957447,"D
SQUAD 1.1"
IMPORT NUMPY AS NP,0.5191489361702127,"Packing slightly violates the i.i.d. assumption of data. Thus, it is of interest, if the algorithm matches
downstream performance. This is especially relevant with a full training setup without a starting
checkpoint. We trained Phase 1&2 of BERT base with and without packing. To avoid giving an ad-
vantage to packing by further hyperparameter tuning, we instead reduced the gradient accumulation
count for the packed BERT training for Phase 1 and Phase 2 to match the total number of sequences
that get processed. With this approach, we could use the same hyperparameters and number of
training steps. This gives a slight disadvantage to the packed run. For Phase 2, we used sequence
length 348 since longer range attention is not relevant for SQuAD 1.1. For the ﬁne-tuning training
on SQuAD 1.1, we did not use packing. After 10 repetitions, the results showed that on average, the
F1 score is reduced by 0.003% whereas the EM score is improving by 0.049%."
IMPORT NUMPY AS NP,0.5212765957446809,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.5234042553191489,"E
TECHNICAL BACKGROUND ON PACKING"
IMPORT NUMPY AS NP,0.5255319148936171,"E.1
CANONICAL PACKING PROBLEM"
IMPORT NUMPY AS NP,0.5276595744680851,"The bin-packing problem deals with the assignment of items into bins of a ﬁxed capacity such that
the number of utilized bins is minimized. In the canonical formulation of the packing problem a
vector s(i) of length n is used to represent the items being packed, where s(i) denotes the length of
the i-th sequence/item. The allocation of items into bins is tracked through the assignment matrix B,
where Bij ∈{0, 1} states whether the i-th sequence should be placed into the j-th bin. In the worst
case scenario, every item is assigned to its own bin, thus B ∈Rn×n. Notably, s grows linearly in
the number of sequences/items being packed and B grows with the square. To mask out unused bins
yj ∈{0, 1}, denotes whether the j-th bin is being used. The optimization objective is to minimize the
sum of yj while making sure to assign each si to exactly one bin and not exceeding the maximum
bin capacity sm for each bin. This problem formulation is well known as bin-packing (Korte &
Vygen, 2012)."
IMPORT NUMPY AS NP,0.5297872340425532,"min
y∈{0,1}n,B∈{0,1}n×n n
X"
IMPORT NUMPY AS NP,0.5319148936170213,"j=1
yj
Minimize the number of bins."
IMPORT NUMPY AS NP,0.5340425531914894,"s.t.
X"
IMPORT NUMPY AS NP,0.5361702127659574,"j=1
bij = 1
∀i
Assign each length/sequence to only one bin. n
X"
IMPORT NUMPY AS NP,0.5382978723404256,"i=1
s(i)bij ≤smyj
∀j
Cumulative length cannot exceed capacity. (15)"
IMPORT NUMPY AS NP,0.5404255319148936,"Bin-packing is a strongly NP-complete (Korte & Vygen, 2012) problem. Producing an exact and
optimal solution is possible with a variety of existing algorithms, for example with the branch-and-
cut-and-price algorithm (Belov & Scheithauer, 2006). However, given that we want to apply it for
very large n (16M for the Wikipedia dataset) an approximate approach is required."
IMPORT NUMPY AS NP,0.5425531914893617,"E.2
APPROXIMATE BIN-PACKING PROBLEM"
IMPORT NUMPY AS NP,0.5446808510638298,"Approximate packing approaches are divided into online and ofﬂine algorithms (Johnson, 1973).
Online algorithms process incoming sequences one-by-one in a streaming fashion, whereas ofﬂine
algorithms have a holistic view of all samples to be packed but typically still operate on a per
sample basis. This results in best case time and memory complexities of at least O(n log(n)) and
solutions that can sometimes be far from optimal, especially for the online algorithms which do
not have access to a holistic view of the datasets. The simplest online approach (next-ﬁt) would
be to keep a single open bin at any given time. An incoming sequence is added to this open bin
if it ﬁts, otherwise the bin is closed (can never be appended to again) and a new one is opened to
accommodate the new sequence (Johnson, 1973). In the case of the Wikipedia pre-training dataset
almost 25% of the sequences are of length 512, which makes this approach very inefﬁcient since
bins would frequently be closed because the incoming sequence did not ﬁt. More speciﬁcally, this
approach is not able to efﬁciently combine one long sequence with one shorter sequence, when the
number of long sequences is large. The algorithms that come closest to the approaches proposed
in this paper are the online harmonic-k algorithm (Lee & Lee, 1985), which creates harmonic sized
bins for the assignment decision, and the ofﬂine Modiﬁed First Fit Decreasing method (Johnson &
Garey, 1985; Yue & Zhang, 1995), which sorts the data, groups it into 4 size categories and deﬁnes
a strategy adjusted to these sizes."
IMPORT NUMPY AS NP,0.5468085106382978,"In our approaches, we make three major simpliﬁcations. We make the problem of bin packing less
dependent on n by operating on the histogram of sequence lengths with bin size 1. Hence, we
replace s(i) by its histogram b and the bin assignment y, B by a mixture of strategies x, where the
set of all available packing strategies is modeled as the matrix A (see also Section E.4.2)."
IMPORT NUMPY AS NP,0.548936170212766,"Then, we do not solve the full packing problem but focus on a ﬁxed packing depth (in other words
the well known 3-partition problem). Last but not least, we solve the limited depth packing problem
only approximately either with a non-negativity-constrained linear least squares (Bro & De Jong,"
IMPORT NUMPY AS NP,0.551063829787234,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.5531914893617021,"1997) (NNLS) followed by rounding to nearest integer solution or by applying Worst-Fit (Johnson
& Garey, 1985; Yue & Zhang, 1995) to the histogram, sorted from largest to smallest (in contrast to
using an unsorted dataset). An exact solution would not be appropriate, since the 3-partition problem
is strongly NP-complete (Garey & Johnson, 1990) as well."
IMPORT NUMPY AS NP,0.5553191489361702,"E.3
DEFINITIONS"
IMPORT NUMPY AS NP,0.5574468085106383,"In this section, we standardize the terms used throughout our methods. Firstly, the terms pack and
bin may be used interchangeably. Secondly, the presented packing schemes impose a limit on how
many sequences can be packed into any given bin. This limit is referred to as the maximum packing
depth. For simplicity, we require the different sequence lengths in a pack to always add up exactly
to the bin capacity sm (we can always generate a padding sequence of just the right length to ﬁll-up
the bin). A packing strategy is a sorted list of sequence lengths, for example [5, 7, 500], such that the
total sequence length is no more than sm and the number of sequences in the pack does not exceed
the maximum packing depth. The output of a packing scheme is typically as set of packing strategies
and the corresponding repeat count for each strategy stating how many times each strategy should
be repeated in order to cover the entire dataset. The strategy repeat count is also referred to as the
mixture of strategies. The objective of the packing algorithm is to jointly design a set of packing
strategies and their repeat counts, such that the amount of padding is (approximately) minimized.
The presence of padding in the packs can either be implicit or explicit. For instance for sm = 512 the
strategy [2, 508] has an implicit padding of 2 (needed to ﬁll the pack up to the sm). Alternatively, the
strategy repeat count may over-subscribe a particular sequence length leading to explicit packing.
For instance constructing a pack of [4, 508] may require a new padding sequence of length 4 be
constructed, if there are not enough sequences of that length in the dataset. The packing algorithms,
we present, use both representations."
IMPORT NUMPY AS NP,0.5595744680851064,"E.4
NON-NEGATIVE LEAST SQUARES HISTOGRAM-PACKING"
IMPORT NUMPY AS NP,0.5617021276595745,"The ﬁrst algorithm proposed in this paper is suitable for settings where it is desirable to achieve a
high packing efﬁciency with a limited packing depth. The algorithm is deterministic and has three
major components described in Sections E.4.1, E.4.2 and E.4.3."
IMPORT NUMPY AS NP,0.5638297872340425,"E.4.1
ENUMERATING PACKING STRATEGIES OF FIXED PACKING DEPTH"
IMPORT NUMPY AS NP,0.5659574468085107,"Listing all unique ways of packing up to a maximum packing depth can be achieved through dynamic
programming. We only consider packing at most up to 3 sequences per pack. This is the smallest
packing depth that can eliminate the need for most padding on the Wikipedia dataset. Increasing the
depth to 4, increases the size of the packing problem drastically and yields no throughput beneﬁt 2.
With only two sequences, packing would be not as efﬁcient since the distribution on sequence length
is not symmetric. We use dynamic programming to enumerate all feasible ways/strategies that up
to M sequences of length 1 −512 can be packed into a bin of length 512. For example, a packing
strategy may be [512] or [6, 506] or [95, 184, 233]. To avoid listing the same strategy multiple times,
we enforce the sequence lengths within a pack to occur in sorted order, for example, [95, 184, 233]
is equivalent to [184, 95, 233] and should only be listed once. This reduces the search space as well
as the space of potential solutions by a factor of 6 approximately and thus signiﬁcantly accelerates
the optimization process. If you had the same strategy repeated 6 times instead of having just one
instance of that strategy with weight X, you will have six instances with weight x/6 (for example,
or any other distribution). This would conﬂict with integer rounding of the solutions and with
convergence of optimization algorithms."
IMPORT NUMPY AS NP,0.5680851063829787,"E.4.2
CONSTRUCTING THE PACKING MATRIX"
IMPORT NUMPY AS NP,0.5702127659574469,"The number of rows in the packing matrix is equal to the number of different sequence length
categories. For instance, if we are using a granularity of 1 token to distinguish between different
sequence lengths, then there are “maximum sequence length” rows. Each column of the matrix
corresponds to a valid packing strategy (given the depth of packing). An example packing matrix
for ﬁtting up to 3 sequences into sequence length 8 is given in Table 2. Each column of the matrix"
IMPORT NUMPY AS NP,0.5723404255319149,2For data distributions that are more skewed than Wikipedia this might look different.
IMPORT NUMPY AS NP,0.574468085106383,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.5765957446808511,"represents a packing strategy. For instance, the ﬁrst column represents the strategy [1, 1, 6] of
packing two length-1 sequences and one length-6 sequence together to form a pack of length 8. The
number of strategies (and columns in the matrix) is discussed in Section F. For a packing depth
of 3 and maximum sequence length, we obtain around s2
m+6sm+12"
IMPORT NUMPY AS NP,0.5787234042553191,"12
strategies. For depth 4, around
sm(sm+4)(2sm+1)"
IMPORT NUMPY AS NP,0.5808510638297872,"288
more get added."
IMPORT NUMPY AS NP,0.5829787234042553,"Table 2: Example packing matrix for sequence length 8. Columns represent different kinds of packs.
Rows represent the number of sequences in these packs with a certain length. The last column
represents a pack with only a single sequence of length six."
IMPORT NUMPY AS NP,0.5851063829787234,"2
1
1
1
0
0
0
0
0
0
0
1
0
0
2
1
1
0
0
0
0
0
1
0
0
2
0
1
0
0
0
0
1
0
1
0
0
0
2
0
0
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1"
IMPORT NUMPY AS NP,0.5872340425531914,"E.4.3
SOLUTION OF THE NNLS APPROXIMATE PACKING PROBLEM"
IMPORT NUMPY AS NP,0.5893617021276596,"A solution of the packing problem is the mixture of packing strategies x that minimizes the amount
of padding in the packed dataset. We solve directly for the mixture (positive real numbers) and
recover the padding as the negative portion of the residual (see Section E.4.4).
min
x∈Rm
∥A · x −b∥2"
IMPORT NUMPY AS NP,0.5914893617021276,s.t. x ≥0 (16)
IMPORT NUMPY AS NP,0.5936170212765958,"The solution vector x will represent the mixture of the columns of A, in other words the mixture
of valid packing strategies such that A · x is as close as possible (in the least squares sense) to the
histogram of sequence lengths b. We obtain a solution with a non-negative least squares implemen-
tation (Lawson & Hanson, 1995; Virtanen et al., 2020) Interestingly in the case of sequence length
512 only 634 out of the 22102 available packing strategies of depth up to 3 are used (3%)."
IMPORT NUMPY AS NP,0.5957446808510638,"E.4.4
PADDING AS THE RESIDUALS OF THE PACKING PROBLEM"
IMPORT NUMPY AS NP,0.597872340425532,"We compute the residuals of the least squares solution (after rounding the mixture to integer) as:
r = b −A · round(x)
(17)
The negative portion of the residuals represents sequences that we are “short”. That is, there is a
deﬁcit of those sequences and we are over-subscribing to them. The positive portion of the residuals
represents sequences which have failed to be packed. Typically, there is a deﬁcit of short sequences
and a surplus of long sequences as demonstrated by the following plot."
IMPORT NUMPY AS NP,0.6,"In total, there are n = 16‘279‘552 sequences in the Wikipedia pre-training dataset.
After
the non-negative least squares packing (and rounding to integer solution) there are 56‘799 un-
packed sequences left un-packed (about 0.352%). The residual on sequence lengths 1 to 8 are
[−4620, −4553, −4612, −4614, −3723, −3936, −3628, −3970]. These negative residuals imply
that we need to add this many sequences of their corresponding sequence length to realize the
mixture of packing strategies. In total the ﬁrst iteration introduces 7.94106 tokens of padding. In
contrast large sequence lengths have a positive residual (a surplus of unused sequences). For se-
quence lengths 504 to 512 the values are [3628, 3936, 3724, 4613, 4612, 4553, 4619, 0]. Note that
sequence length 512 has a residual of 0 since they do not need packing. Intermediate sequence
lengths typically have non-zero (but much smaller) residuals."
IMPORT NUMPY AS NP,0.6021276595744681,The detailed code for the algorithm is provided in Listing 2.
IMPORT NUMPY AS NP,0.6042553191489362,"E.4.5
RESIDUAL WEIGHTING"
IMPORT NUMPY AS NP,0.6063829787234043,"A natural extension of the non-negative least squares problem introduced in Section E.4.3 is to
weight the residuals on different sequence length differently."
IMPORT NUMPY AS NP,0.6085106382978723,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6106382978723405,"0
100
200
300
400
500
sequence length 4000 2000 0 2000 4000"
IMPORT NUMPY AS NP,0.6127659574468085,number of residual sequences
IMPORT NUMPY AS NP,0.6148936170212767,Un-weighted nnls packing residual
IMPORT NUMPY AS NP,0.6170212765957447,"under-used sequences
over-used sequences"
IMPORT NUMPY AS NP,0.6191489361702127,Figure 8: Visualization of the residual of the NNLS packing problem
IMPORT NUMPY AS NP,0.6212765957446809,"min
x∈Rm
∥(wA) · x −(wb)∥2"
IMPORT NUMPY AS NP,0.6234042553191489,s.t. x ≥0 (18)
IMPORT NUMPY AS NP,0.625531914893617,"We should not signiﬁcantly penalize a deﬁcit in short sequence lengths (smaller than 8 tokens) as
adding up to 8 tokens of padding is not much overhead. Similarly, a surplus in long sequences is
not worrisome because the amount of padding needed to achieve a sequence length of 512 is small.
Reducing the weight of the residual on the ﬁrst 8 tokens to 0.09 leads to the following residual plot
shown on the right in Figure 9. In this case the residual is almost entirely shifted to the shorter
sequences and the positive residual on the longer sequences has virtual disappeared."
IMPORT NUMPY AS NP,0.6276595744680851,"0
100
200
300
400
500
sequence length 25000 20000 15000 10000 5000 0"
IMPORT NUMPY AS NP,0.6297872340425532,number of residual sequences
IMPORT NUMPY AS NP,0.6319148936170212,Weighted nnls packing residual
IMPORT NUMPY AS NP,0.6340425531914894,"under-used sequences
over-used sequences"
IMPORT NUMPY AS NP,0.6361702127659574,Figure 9: Visualization of the weighted residual of the NNLS packing problem
IMPORT NUMPY AS NP,0.6382978723404256,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6404255319148936,"E.5
DISCUSSION OF RESIDUAL WEIGHT CHOICE"
IMPORT NUMPY AS NP,0.6425531914893617,"This section discusses the choice and effect of the weighting parameters in the NNLSP packing
algorithm. To simplify the problem of selecting reasonable defaults for the residual weights, we
use just two parameters to completely describe the weights: an “offset” parameter and a “weight”
parameter. Originally, all sequence length residuals are given the same weight of 1. This results
in a packing with leftover long sequences, because there are not enough short sequences to pack
them with. To reduce the residual on long sequences, we could either increase the residual weight
on long sequences or reduce the weight on short sequences. We chose to reduce the weight on short
sequences. Speciﬁcally, sequence lengths up to the “offset” length have a reduced “weight”. The
other residual weights stay at 1."
IMPORT NUMPY AS NP,0.6446808510638298,"To start, we chose an offset of 8 tokens, which is the smallest power of 2 for which there are
examples in the Wikipedia dataset. We decrease the weight on sequences shorter than the “offset”
from 1 to 0.9 to 0.09 to see which order of magnitude is the most appropriate. On visual inspection
(looking at the residual plots as in Figure 9), we found that 0.9 still left too many long sequences
unpacked. So, we reduced the weight a further order of magnitude to 0.09. This seemed sufﬁcient
to encourage nearly all long sequences to pack. While visual inspection helps in understanding how
many long/short sequences are leftover, we are also interested in the impact the weights have on the
overall efﬁciency of the packing."
IMPORT NUMPY AS NP,0.6468085106382979,"Without any weighting, we get 99.746359% efﬁciency, whereas the weighted approach results in
99.746274% efﬁciency. Hence, we conclude that the impact of the weights on the packing efﬁ-
ciency is very limited. Additionally, using an “offset” length of 4, resulted in similar numbers, for
the full range of weights from 0 to 1. Using a weight of 0 for an “offset” length of 8 resulted in
insigniﬁcantly higher efﬁciency of 99.7519%, whereas using an “offset” length of 16 reduces per-
formance to 99.38964%. A weight of 0 implies that the residual on those lengths can be safely
ignored, i.e., the packing algorithm can thus add as many short sequences as it chooses without any
penalty. It is very interesting that this does not signiﬁcantly impact the packing efﬁciency, and can
even have a slightly positive impact. However, increasing the “offset” length further signiﬁcantly
decreases the performance with weight 0. Keeping the weight at 0.09 and increasing the length
reduces performance slightly, for example with 99.53% at length 256 and 99.728% at length 16."
IMPORT NUMPY AS NP,0.648936170212766,"For our Squad analysis, weighting improved the efﬁciency slightly from 96.94% to 97.38%. Fine
tuning further with direction grid search, delivered a local optimum of 98.767% efﬁciency with
length 64 and weight 0.002."
IMPORT NUMPY AS NP,0.6510638297872341,"Overall the inﬂuence of different residual weights on the packing efﬁciency (and the acceleration
factor) is less than 1%. This might differ from application to application, but it shows that we are
able to use the residual weights to achieve secondary targets (like not having leftover long sequences)
without signiﬁcantly compromising the packing efﬁciency."
IMPORT NUMPY AS NP,0.6531914893617021,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.6553191489361702,"F
COMPLEXITY ANALYSIS OF THE PROPOSED PACKING APPROACHES"
IMPORT NUMPY AS NP,0.6574468085106383,"Since approximate packing algorithms have a complexity of at least O(n log(n)) and we would
like to be able to tackle datasets with 2K million samples, we will discuss the complexity of our
packing algorithms in this section. The complexity depends on the maximum sequence length sm,
the number of samples n, and the packing depth d."
IMPORT NUMPY AS NP,0.6595744680851063,"To create the histogram, we have to iterate over the data once (O(n)). Our histograms will be binned
by size 1, meaning one bin for each sequence length. Hence, a dictionary can be generated (O(sm))
and used for the sorting (O(1)). The respective histogram vector has dimension sm."
IMPORT NUMPY AS NP,0.6617021276595745,"F.1
COMPLEXITY ANALYSIS OF NON-NEGATIVE LEAST-SQUARES HISTOGRAM-PACKING"
IMPORT NUMPY AS NP,0.6638297872340425,"For a packing depth of one, there is only the strategy [sm]. For a packing depth of two, we add
the strategies [1, sm −1], ..., [sm −⌊sm"
IMPORT NUMPY AS NP,0.6659574468085107,2 ⌋] which results in an additional ⌊sm
IMPORT NUMPY AS NP,0.6680851063829787,"2 ⌋potential strategies.
Following the dynamic programming approach, the number of possible additional strategies of depth
three can be calculated with"
IMPORT NUMPY AS NP,0.6702127659574468,"# potential strategies = ⌊sm 3 ⌋
X j=1 ⌊sm−j 2
⌋
X"
IMPORT NUMPY AS NP,0.6723404255319149,"i=j
1 = ⌊sm 3 ⌋
X j=1"
IMPORT NUMPY AS NP,0.674468085106383,sm −j 2
IMPORT NUMPY AS NP,0.676595744680851,"
−(j −1) ≈ ⌊sm 3 ⌋
X j=1 sm 2 −3"
IMPORT NUMPY AS NP,0.6787234042553192,2j ≈sm
SM,0.6808510638297872,"2
sm 3 −3"
SM,0.6829787234042554,"2
sm/3(sm/3 + 1) 2"
SM,0.6851063829787234,"≈
s2
m
12  (19)"
SM,0.6872340425531915,"Note that for sm = 512 the approximation is exact. This means that our strategy matrix A has the
dimensions sm ×
h
s2
m
12
i
+ ⌊sm"
SM,0.6893617021276596,"2 ⌋+ 1

. Overall, this leaves us with a space complexity of s3
m since
A is larger than w, x, and b. So it contains 11‘316‘224 numbers which is still much smaller than n.
Note that the original data matrix B had n2 entries, which all needed to be optimized together with
the n bin assignments y. We now have only
h
s2
m
12
i
+⌊sm"
SM,0.6914893617021277,"2 ⌋free variables in the strategy vector x. Also
note that A can be precomputed when sm is known and is independent of the number of samples.
Given a problem matrix with dimension i × j, Luo et al. (Luo & Duraiswami, 2011) indicate that
the asymptotic complexity of most solution approaches is O(ij2), whereas they propose an O(ij)
solution. Since we use the standard SciPy implementation (Lawson & Hanson, 1995), our estimated
total time complexity for NNLSHP is O(n + s5
m)."
SM,0.6936170212765957,"For sm = 2048, the estimate would be 350′540 potential strategies which is still far less than the
number of samples. For packing depth 4, we calculate (Wolfram Research Inc.): ⌊sm 4 ⌋
X k=1 ⌊sm−k 3
⌋
X j=k"
SM,0.6957446808510638,"⌊sm−j−k 2
⌋
X i=j
1 ≈ ⌊sm 4 ⌋
X k=1 ⌊sm−k 3
⌋
X j=k"
SM,0.6978723404255319,"sm −k + 2 −3j 2 ≈ ⌊sm 4 ⌋
X k=1"
SM,0.7,"1
12(s + 4 −4k)(s + 3 −4k)"
SM,0.7021276595744681,"≈
1
288s(2s2 + 9s + 4)"
SM,0.7042553191489361,"=
1
288s(s + 4)(2s + 1) (20)"
SM,0.7063829787234043,Under review as a conference paper at ICLR 2022
SM,0.7085106382978723,"So with sm = 512, there would be around 940K strategies. In our implementation, this number of
strategies would be too high to create the problem matrix. One alternatives to simplify would be to
not use the exact length of sequences but to only consider even numbers for the sequence length and
round up. That way arbitrary sequence length could also be handled and the limiting factor would be
the complexity of the attention layer in BERT which does not scale well with the sequence length."
SM,0.7106382978723405,"F.2
COMPLEXITY ANALYSIS OF SHORTEST-PACK-FIRST HISTOGRAM-PACKING"
SM,0.7127659574468085,"The complexity calculation of SPFHP is straightforward. We go over the whole data once for the
histogram sorting. Next, we iterate over each of the sm bins in the histogram. Lastly, we iterate
over all strategies that were encountered so far. It can be proven that, at each iteration, the number
of strategies can be maximally increased by one. In each step, we potentially add a sequence to
existing strategies but a new strategy is opened up only in the ﬁnal step, when we either create a
new strategy or we split one of the existing strategies into two. Hence, the number of strategies is
bounded by sm and the overall time complexity is bounded by O(n + s2
m). The space complexity is
O(s2
m) since we need to store up to sm strategies with maximum sm counts for different sequence
length."
SM,0.7148936170212766,"G
PERFORMANCE COMPARISON TO GREEDY PACKING IN T5"
SM,0.7170212765957447,"T5 (Raffel et al., 2019) is normally trained on the C4 dataset. However, to give an idea of the
difference in packing efﬁciency and acceleration compared to our newly introduced algorithm, we
can analyse the performance of greedy aggregation of samples on our given Wikipedia dataset."
SM,0.7191489361702128,"We take the histogram and cast it back to a list of different sequence lengths since this is all that
matters for analysing packing behaviour. Next, we randomly shufﬂe the dataset and iterate with the
greedy aggregation algorithm multiple times to account for randomness. We iterate sequence by
sequence and combine them provided the maximum sequence length of 512 is not yet reached. If it
is exceeded, the packed sequence is considered ﬁnished and a new sequence is started."
SM,0.7212765957446808,"The greedy packing algorithm itself takes a bit more than 10 seconds, since we are operating on
single sequences and not histogram counts. The efﬁciency of this approach is 78.24% (standard
deviation of 0.005) compared to our 99.75% for NNLSHP. The respective acceleration would be
around 1.566x compared to our 2x. With respective separator tokens, the performance decreases
around 0.13% for one separator token and 0.27% when two separator tokens are required between
two sequences. Following the brief documentation at https://github.com/tensorflow/
tensor2tensor/blob/5623deb79cfcd28f8f8c5463b58b5bd76a81fd0d/
tensor2tensor/data_generators/generator_utils.py#L1086,
two
separa-
tor tokens would be expected in the T5 processing."
SM,0.723404255319149,"In addition to the packing preprocessing, our paper proposes, rather than using separator tokens,
to instead modify the masking of the attention matrix during training. The RoBERTa paper shows
that avoiding contamination of sequences from different documents can consistently improve down-
stream F1 scores by 0.35%."
SM,0.725531914893617,Under review as a conference paper at ICLR 2022
SM,0.7276595744680852,"H
PACKING SQUAD 1.1"
SM,0.7297872340425532,"We tokenized SQuAD (Rajpurkar et al., 2016) for BERT (Devlin et al., 2019a) with maximum
sequence length 384 and visualized the histogram over the sequence length (Figure 10). The distri-
bution looks similar to the Wikipedia dataset but is slightly less skewed. However, the maximum
sequence length only had an occurrence of 1.2% compared to 23.5%. Hence, the theoretical un-
padding speedup is 2.232. In Table 3, we can see that SPFHP does not concatenate more than 3
samples and obtains 97.54% efﬁciency in contrast to a maximally used depth of 16 with 99.60%
efﬁciency on Wikipedia, because of the less skewed distribution. Note that we have less than 90′000
samples. Hence, NNLSHP is less efﬁcient because the rounding in the residuals has a much larger
impact compared to more than 16 million sequences in the Wikipedia dataset."
SM,0.7319148936170212,"50
100
150
200
250
300
350
400
sequence length 0.000 0.002 0.004 0.006 0.008 0.010 0.012"
SM,0.7340425531914894,probability density
SM,0.7361702127659574,"Figure 10: SQuAD 1.1 BERT pre-training dataset sequence length histogram for maximum se-
quence length of 384."
SM,0.7382978723404255,Table 3: Performance results of proposed packing algorithms for SQuAD 1.1 BERT pre-training.
SM,0.7404255319148936,"packing
packing
# strategies
# packs
# tokens
# padding
efﬁciency
packing
depth
algorithm
used
tokens
(%)
factor
1
none
348
88641
34038144
18788665
44.801
1.000
2
SPFHP
348
45335
17408640
2159161
87.597
1.955
3
NNLSHP
398
40808
15670272
420793
97.310
2.172
3/max
SPFHP
344
40711
15633024
383545
97.547
2.177"
SM,0.7425531914893617,Under review as a conference paper at ICLR 2022
SM,0.7446808510638298,"I
PACKING GLUE"
SM,0.7468085106382979,"To explore a variety of datasets and emphasize that skewed distributions are common, we explored
all datasets in the GLUE benchmark (Warstadt et al., 2018; Wang et al., 2018) that came with training
data. We loaded the datasets using the HuggingFace dataset loading API (Wolf et al., 2020). For
preprocessing, we followed the implementation in the HuggingFace transformers repository (Wolf
et al., 2020) 3 and extracted the respective data processing snippets to obtain tokenized data with a
maximum sequence length of 128. The histogram of the sequence length for each of the included
datasets is displayed in Figure 11 and the packing results are given in Table 4. Each dataset beneﬁts
from packing. The lower the mean, the higher the packing factors are that can be reached but with a
higher packing depth."
SM,0.7489361702127659,"0
20
40
60
80
100
120
sequence length 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
SM,0.7510638297872341,probability density
SM,0.7531914893617021,Sequence length distributions of GLUE Datasets
SM,0.7553191489361702,"cola
sst2
mrpc
qqp"
SM,0.7574468085106383,"stsb
mnli
rte
wnli"
SM,0.7595744680851064,Figure 11: GLUE dataset sequence length histograms for maximum sequence length of 128.
SM,0.7617021276595745,"Table 4: Performance results of proposed packing algorithms for the GLUE dataset. Only the base-
line and the SPFHP packing results without limiting the packing depth are displayed."
SM,0.7638297872340426,"data
packing
# strategies
# packs
# tokens
# padding
efﬁciency
packing
name
depth
used
tokens
(%)
factor
cola
1
34
8551
1094528
997669
8.849
1.000
cola
13/max
29
913
116864
20005
82.882
9.366
sst2
1
64
67349
8620672
7723633
10.406
1.000
sst2
15/max
64
7691
984448
87409
91.121
8.757
mrpc
1
77
3668
469504
274214
41.595
1.000
mrpc
4/max
74
1606
205568
10278
95.000
2.284
qqp
1
123
363846
46572288
35448844
23.884
1.000
qqp
5/max
123
97204
12442112
1318668
89.402
3.743
stsb
1
85
5749
735872
575993
21.726
1.000
stsb
6/max
83
1367
174976
15097
91.372
4.206
mnli
1
124
392702
50265856
34636487
31.093
1.000
mnli
8/max
124
123980
15869440
240071
98.487
3.167
rte
1
112
2490
318720
152980
52.002
1.000
rte
4/max
108
1330
170240
4500
97.357
1.872
wnli
1
72
635
81280
57741
28.960
1.000
wnli
6/max
63
192
24576
1037
95.780
3.307"
SM,0.7659574468085106,"3https://github.com/huggingface/transformers/blob/master/examples/
text-classification/run_glue.py"
SM,0.7680851063829788,Under review as a conference paper at ICLR 2022
SM,0.7702127659574468,"J
PACKING AUDIO DATA (LIBRISPEECH)"
SM,0.7723404255319148,"In this section, we show that packing can beneﬁt other domains than NLP like ASR. We use the
LibiSpeech dataset (Panayotov et al., 2015) and preprocess it as described at a reference implemen-
tation 4. The resulting histograms for the subsampled audio sample lengths and respective text labels
are provided in Figure 12."
SM,0.774468085106383,"0
50
100
150
200
250
300
sequence length 0.000 0.002 0.004 0.006 0.008"
SM,0.776595744680851,probability density
SM,0.7787234042553192,"0
20
40
60
80
100
120
sequence length 0.000 0.005 0.010 0.015 0.020 0.025"
SM,0.7808510638297872,probability density
SM,0.7829787234042553,"Figure 12: LibriSpeech sequence length histograms of preprocessed audio data [top] as well as target
text data [bottom]."
SM,0.7851063829787234,"It can be seen that the audio sequence length is dominated by long sequences with 38% of required
padding to meet the max sequence length of 330. Thus the theoretical optimal speed-up of 1.6x
cannot be reached. However, 80% efﬁciency are possible with any of the proposed packing algo-
rithms to achieve 1.3x speed-up. This can be already achieved by combining up to 2 sequences. To
achieve almost perfect packing efﬁciency, a sequence length around 457 and concatenating up to 8
sequences is required. Due to the quadratic increased computational load that usually comes with
longer sequence length, increasing the sequence length is not practical."
SM,0.7872340425531915,"If processing and packing the text data independently of the audio, 99.99% efﬁciency can be
achieved with a speed-up of 2.24x."
SM,0.7893617021276595,"4https://github.com/mlcommons/training/tree/master/rnn_speech_
recognition/pytorch"
SM,0.7914893617021277,Under review as a conference paper at ICLR 2022
SM,0.7936170212765957,"K
PACKING PAPER ABSTRACTS (PUBMED)"
SM,0.7957446808510639,"This section analyses the length of abstracts to give an intuition about how different documents
can be in length. Figure 13 depicts the length of abstracts in characters extracted from PubMed 5.
If these abstracts were directly used as sequences, a character length of 1000 could result in 1.9x
speed-up from packing. The potential speed-ups for length 2000, 3000, 4000 would be 2x, 3x, and
4x, respectively. Note that, document clean-up procedures would usually eliminate documents that
are too short or too long for data sanitizing purposes."
SM,0.7978723404255319,"0
500
1000
1500
2000
2500
3000
3500
4000
number of characters 0 2000 4000 6000 8000 10000 12000 14000 16000"
SM,0.8,count of abstracts
SM,0.8021276595744681,Figure 13: Abstract length distribution in PubMed.
SM,0.8042553191489362,"Note that for the processing in BlueBERT (Peng et al., 2019), paper titles and abstracts get separated
into sequences, tokenized, and then combined with the BERT sequence combination approach for a
maximum sequence length of 128 tokens. Thus, it results in a different distribution."
SM,0.8063829787234043,5https://huggingface.co/datasets/pubmed
SM,0.8085106382978723,Under review as a conference paper at ICLR 2022
SM,0.8106382978723404,"L
FURTHER LEARNING CURVES"
SM,0.8127659574468085,This section provides further learning curves related to Section 4.2.
SM,0.8148936170212766,"0
1
2
3
samples
1e6 0.50 0.55 0.60 0.65 0.70"
SM,0.8170212765957446,training accuracy
SM,0.8191489361702128,"classic, bs: 1500
packed, bs: 768"
SM,0.8212765957446808,"0
1
2
3
samples
1e6 1.5 2.0 2.5 3.0 3.5"
SM,0.823404255319149,training loss
SM,0.825531914893617,"classic, bs: 1500
packed, bs: 768"
SM,0.8276595744680851,"Figure 14: Comparison of learning curves for packed and unpacked processing with reduced batch
size for the packed approach."
SM,0.8297872340425532,"0
1
2
3
samples
1e6 0.4 0.5 0.6 0.7"
SM,0.8319148936170213,training accuracy
SM,0.8340425531914893,"classic, beta: 0.81
packed, beta: 0.66
packed, beta: 0.66, double lr
packed, beta: 0.81, double lr"
SM,0.8361702127659575,"0
1
2
3
samples
1e6 2 3 4"
SM,0.8382978723404255,training loss
SM,0.8404255319148937,"classic, beta: 0.81
packed, beta: 0.66
packed, beta: 0.66, double lr
packed, beta: 0.81, double lr"
SM,0.8425531914893617,"Figure 15: Comparison of learning curves for packed and unpacked processing with heuristics
applied."
SM,0.8446808510638298,"0.0
0.5
1.0
1.5
2.0
relative time 0.50 0.55 0.60 0.65 0.70"
SM,0.8468085106382979,training accuracy
SM,0.8489361702127659,"classic, bs: 1500, beta: 0.81
packed, bs: 1500, beta: 0.66"
SM,0.851063829787234,"0.0
0.5
1.0
1.5
2.0
relative time 1.5 2.0 2.5 3.0 3.5"
SM,0.8531914893617021,training loss
SM,0.8553191489361702,"classic, bs: 1500, beta: 0.81
packed, bs: 1500, beta: 0.66"
SM,0.8574468085106383,"Figure 16: Comparison of learning curves for packed and unpacked processing in the optimized
setup."
SM,0.8595744680851064,Under review as a conference paper at ICLR 2022
SM,0.8617021276595744,"M
FINE-TUNED LONGEST-PACK-FIRST HISTOGRAM-PACKING"
SM,0.8638297872340426,"In the main paper, we focused on SPFHP due its simplicity. In this section, we analyse the effect of
applying the “Best-Fit” algorithm (Johnson, 1973). Here, the longest pack that still ﬁts the sequence
is chosen instead of the shortest one. In contrast to SPFHP, we additionally consider splitting the
histogram count, if it can ﬁt multiple times. A simple example is sequence length 256, where
we divide the respective histogram count by 2 to create the optimal pack with strategy [256, 256]
instead of the strategy [256]. This latter strategy would be complemented by other sequences but
would probably not result in an optimal packing. The implementation of this approach is much
more complex than the SPFHP implementation. The code is provided in Listing 8 and the results in
Table 5."
SM,0.8659574468085106,"pack.
# strat.
# packs
# tokens
# padding
efﬁciency
pack.
depth
used
tokens
(%)
factor
1
508
16279552
8335130624
4170334451
49.967
1.000
2
634
10099081
5170729472
1005933299
80.546
1.612
3
648
9090154
4654158848
489362675
89.485
1.791
4
671
8657119
4432444928
267648755
93.962
1.880
8
670
8207569
4202275328
37479155
99.108
1.983
16
670
8140006
4167683072
2886899
99.931
2.000
29/max
670
8138483
4166903296
2107123
99.949
2.000"
SM,0.8680851063829788,"Table 5: Performance results of longest-pack-ﬁrst histogram-packing for Wikipedia BERT pre-
training with maximum sequence length 512."
SM,0.8702127659574468,"We can see that longest-pack-ﬁrst histogram-packing (LPFHP) uses a much higher packing depth
when no limit is set (29 instead of 16). Splitting the histogram counts results in slightly higher
numbers of used strategies compared to SPFHP where the number of used strategies is limited by
the maximum sequence length. The best efﬁciency of LPFHP is 99.949% with packing factor of 2
which is slightly higher than the 99.75% (1.996 packing factor) for NNLSHP and 99.6% for SPFHP
(1.993 packing factor). All algorithms are very close to the upper limit."
SM,0.8723404255319149,"Note that for NNLSHP, we only ﬁll up the unpacked samples with padding. Applying best-ﬁt on
the remains, similar results can be expected. Although the beneﬁts of the improved algorithm are
negligible, we share the concept and code below in case packing is applied to other data with a
different distribution that would beneﬁt more from it, or for applications where only perfectly packed
sequences without padding are of interest."
SM,0.874468085106383,Under review as a conference paper at ICLR 2022
SM,0.8765957446808511,"N
EXTENDED NNLS WITH PADDING TOKEN WEIGHTING"
SM,0.8787234042553191,"In Section E.4.4, we deﬁned the residual as"
SM,0.8808510638297873,"r = b −A · round(x)
(21)"
SM,0.8829787234042553,"and discovered that a positive residual corresponds to sequences that we did not pack at all and
should be avoided. Negative residuals correspond to padding and should be minimized. Due to
this discrepancy, we decided to set small weights for very short sequences (that don’t occur in the
data). However, it was not possible to directly optimize the amount of padding. A negative residual
component for length i, ri, results in |ri| · i padding tokens, however a positive residual actually
results into (512 −ri) · i padding tokens. This cannot be addressed by our weighting approach in"
SM,0.8851063829787233,"min
x∈Rm
∥(wA) · x −(wb)∥2"
SM,0.8872340425531915,s.t. x ≥0 (22)
SM,0.8893617021276595,"Working within the NNLS approach, we can strictly enforce a non-positive residual r (before round-
ing to integer). To that end, we deﬁne a new auxiliary variable r ≈−(b −Ax) which is the negative
of the residual, r. This will allow us to reformulate the objective r ≤0 to the non-negative con-
straint: r ≥0."
SM,0.8914893617021277,"min
x∈Rm
∥(wA) · x −(wb)∥2 + ∥w · A · x −w · b −w · r∥2"
SM,0.8936170212765957,s.t. x ≥0 r ≥0 (23)
SM,0.8957446808510638,"This will enforce r = Ax −b ≥0 due to the large weight, w := 106, and no upper limits on r.
Now, we can set wi := i to optimize for the padding tokens. Due to the use of the squared error, we
would however optimize the squared sum of padding tokens instead of the preferred sum of padding
tokens. To accomplish the latter, we would have to replace the L2-norm problem by an L1-norm
problem which would be too complex to solve. Note that due to rounding, the unwanted positive
residuals r (r < 0) might still occur. This could be avoided by rounding up x instead of normal
rounding of x. To put the new formulation into a solver, we replace"
SM,0.8978723404255319,"b by

b
b"
SM,0.9,"
, x by

x
r"
SM,0.902127659574468,"
, w by

w
w"
SM,0.9042553191489362,"
, and A by

A
0m
A
−Dm"
SM,0.9063829787234042,"
,
(24)"
SM,0.9085106382978724,"where 0m is an m × m matrix with m being the maximum sequence length, 512, and Dm is a
unit matrix of the same dimensions as 0m. Since, we are already close to optimum especially on the
Wikipedia dataset, the results are only a little bit better. The processing time however increases from
30 to 415 seconds without considering the increased time for constructing the processing matrix.
Since the slightly improved algorithm might be nevertheless relevant for other applications, we
share it in Listing 9."
SM,0.9106382978723404,Under review as a conference paper at ICLR 2022
SM,0.9127659574468086,"O
PACKING SOURCE CODE"
SM,0.9148936170212766,Listing 2: Non-negative least squares histogram-packing
IMPORT TIME,0.9170212765957447,"1
import time
2
import numpy as np
3
from scipy import optimize, stats
4
from functools import lru_cache
5
6
def get_packing_matrix(strategy_set, max_sequence_length):
7
num_strategies = len(strategy_set)
8
A = np.zeros((max_sequence_length, num_strategies), dtype=np.int32)
9
for i, strategy in enumerate(strategy_set):
10
for seq_len in strategy:
11
A[seq_len - 1, i] += 1
12
return A
13
14
@lru_cache(maxsize=None)
15
def get_packing_strategies(start_length, minimum_increment, target_length, depth):
16
gap = target_length - start_length
17
strategies = []
18
# Complete the packing with exactly 1 number
19
if depth == 1:
20
if gap >= minimum_increment:
21
strategies.append([gap])
22
# Complete the sample in ""depth"" steps, recursively
23
else:
24
for new in range(minimum_increment, gap + 1):
25
new_gap = target_length - start_length - new
26
if new_gap == 0:
27
strategies.append([new])
28
else:
29
options = get_packing_strategies(start_length + new, new, target_length, depth - 1)
30
for option in options:
31
if len(option) > 0:
32
strategies.append([new] + option)
33
return strategies
34
35
def pack_using_nnlshp(histogram, max_sequence_length, max_sequences_per_pack):
36
# List all unique ways of packing to the desired maximum sequence length
37
strategy_set = get_packing_strategies(0, 1, max_sequence_length, max_sequences_per_pack)
38
print(f""Packing will involve {len(strategy_set)} unique packing strategies."")
39
# Get the packing matrix corresponding to this list of packing strategies
40
A = get_packing_matrix(strategy_set, max_sequence_length)
41
# Weights that penalize the residual on short sequences less.
42
penalization_cutoff = 8
43
w0 = np.ones([max_sequence_length])
44
w0[:penalization_cutoff] = 0.09
45
# Solve the packing problem
46
print(f""Sequences to pack: "", histogram.sum())
47
start = time.time()
48
strategy_repeat_count, rnorm = optimize.nnls(np.expand_dims(w0, -1) * A, w0 * histogram)
49
print(f""Solving non-negative least squares took {time.time() - start:3.2f} seconds."")
50
# Round the floating point solution to nearest integer
51
strategy_repeat_count = np.rint(strategy_repeat_count).astype(np.int64)
52
# Compute the residuals, shape: [max_sequence_length]
53
residual = histogram - A @ strategy_repeat_count
54
# Handle the left-over sequences i.e. positive part of residual
55
unpacked_seqlen = np.arange(1, max_sequence_length + 1)[residual > 0]
56
for l in unpacked_seqlen:
57
strategy = sorted([l, max_sequence_length - l])
# the depth 1 strategy
58
strategy_index = strategy_set.index(strategy)
59
strategy_repeat_count[strategy_index] += residual[l-1]
60
# Re-compute the residual with the updated strategy_repeat_count
61
# This should now be strictly < 0
62
residual = histogram - A @ strategy_repeat_count
63
# Add padding based on deficit (negative residual portion of residual)
64
padding = np.where(residual < 0, -residual, 0)
65
# Calculate some basic statistics
66
sequence_lengths = np.arange(1, max_sequence_length + 1)
67
old_number_of_samples = histogram.sum()
68
new_number_of_samples = int(strategy_repeat_count.sum())
69
speedup_upper_bound = 1.0/(1 - (histogram*(1 - sequence_lengths / max_sequence_length)).sum()/"
IMPORT TIME,0.9191489361702128,"old_number_of_samples)
70
num_padding_tokens_packed = (sequence_lengths * padding).sum()
71
efficiency = 1 - num_padding_tokens_packed/(new_number_of_samples*max_sequence_length)
72
print(f""Packing efficiency (fraction of real tokens): {efficiency:3.4f}\n"",
73
f""Speed-up theoretical limit: {speedup_upper_bound:3.4f}\n"",
74
f""Achieved speed-up over un-packed dataset: {old_number_of_samples/new_number_of_samples:3.5f}"")
75
return strategy_set, strategy_repeat_count"
IMPORT TIME,0.9212765957446809,Under review as a conference paper at ICLR 2022
IMPORT TIME,0.9234042553191489,Listing 3: Shortest-pack-ﬁrst histogram-packing
FROM COLLECTIONS IMPORT DEFAULTDICT,0.925531914893617,"1
from collections import defaultdict
2
import numpy as np
3
4
def add_pack(pack, count, tmp, final, limit, offset):
5
""""""Filter out packs that reached maximum length or number of sequences.""""""
6
if len(pack) == limit or offset == 0:
7
final[offset].append((count, pack))
8
else:
9
tmp[offset].append((count, pack))
10
11
def pack_using_spfhp(histogram, max_sequence_length, max_sequences_per_pack):
12
""""""Shortest-pack-first histogram-packing algorithm.""""""
13
reversed_histogram = np.flip(histogram)
14
# Initialize main strategy data dictionary.
15
# The key indicates how many tokens are left for full length.
16
# The value is a list of tuples, consisting of counts and respective packs.
17
# A pack is a (sorted) list of sequence length values that get concatenated.
18
tmp_strategies_per_length = defaultdict(list)
19
strategies_per_length = defaultdict(list)
20
# Index i indicates here, how much space is left, due to reversed histogram
21
for i in range(max_sequence_length):
22
n_sequences_to_bin = reversed_histogram[i]
23
length_to_bin = max_sequence_length - i
24
offset = i + 1
# largest possible offset
25
while n_sequences_to_bin > 0:
26
if (length_to_bin + offset) in tmp_strategies_per_length:
27
# extract shortest pack that will get modified
28
n_sequences_to_pack, pack = tmp_strategies_per_length[
29
length_to_bin + offset].pop()
30
new_pack = pack + [length_to_bin]
31
count = min(n_sequences_to_pack, n_sequences_to_bin)
32
if n_sequences_to_pack > n_sequences_to_bin:
33
# old pack gets reduced
34
n_sequences_to_pack -= n_sequences_to_bin
35
tmp_strategies_per_length[length_to_bin + offset].append(
36
(n_sequences_to_pack, pack))
37
n_sequences_to_bin = 0
38
else:
39
n_sequences_to_bin -= n_sequences_to_pack
40
add_pack(new_pack, count,
41
tmp_strategies_per_length, strategies_per_length,
42
max_sequences_per_pack, offset)
43
# clean up to speed up main key search
44
if not tmp_strategies_per_length[length_to_bin + offset]:
45
tmp_strategies_per_length.pop(length_to_bin + offset)
46
else:
47
offset -= 1
48
# Does not fit anywhere. Create new pack.
49
if offset < 0:
50
add_pack([length_to_bin], n_sequences_to_bin,
51
tmp_strategies_per_length, strategies_per_length,
52
max_sequences_per_pack, i)
53
n_sequences_to_bin = 0
54
# merge all strategies
55
for key in tmp_strategies_per_length:
56
strategies_per_length[key].extend(tmp_strategies_per_length[key])
57
# flatten strategies dictionary
58
strategy_set = []
59
strategy_repeat_count = []
60
for key in strategies_per_length:
61
for count, pack in strategies_per_length[key]:
62
pack.reverse()
63
strategy_set.append(pack)
64
strategy_repeat_count.append(count)
65
return strategy_set, np.array(strategy_repeat_count)"
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9276595744680851,Under review as a conference paper at ICLR 2022
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9297872340425531,Listing 4: Evaluation function of shortest-pack-ﬁrst histogram-packing
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9319148936170213,"1
""""""Max depth analysis of shortest-pack-first histogram-packing.""""""
2
from collections import defaultdict
3
import tabulate
4
import time
5
import numpy as np
6
7
def evaluate_spfhp(histogram, max_sequence_length):
8
""""""Evaluate shortest-pack-first histogram-packing algorithm.""""""
9
stats_data = [[""pack. depth"", ""# strat. used"", ""# packs"", ""# tokens"",
10
""# padding tok."", ""efficiency (%)"", ""pack.factor"", ""time""]]
11
for max_sequences_per_pack in [1, 2, 3, 4, 8, 16, ""max""]:
12
start = time.time()
13
strategy_set, strategy_repeat_count = pack_using_spfhp(
14
histogram, max_sequence_length, max_sequences_per_pack)
15
duration = time.time() - start
16
17
# Performance Evaluation of packing approach
18
n_strategies = int(len(strategy_set))
19
packs = int(sum(strategy_repeat_count))
20
sequences = sum([count*len(pack) for count, pack in
21
zip(strategy_repeat_count, strategy_set)])
22
total_tokens = int(max_sequence_length * packs)
23
empty_tokens = int(sum([
24
count*(max_sequence_length-sum(pack)) for count, pack in
25
zip(strategy_repeat_count, strategy_set)]))
26
token_efficiency = 100 - empty_tokens / total_tokens * 100
27
if max_sequences_per_pack == ""max"":
28
m_length = max([len(pack) for pack in strategy_set])
29
max_sequences_per_pack = ""max ({})"".format(m_length)
30
stats_data.append([
31
max_sequences_per_pack, n_strategies, packs, total_tokens,
32
empty_tokens, token_efficiency, sequences / packs, duration])
33
print(tabulate.tabulate(stats_data, headers=""firstrow"", floatfmt="".3f""))"
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9340425531914893,Listing 5: Loss calculation
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9361702127659575,"1
# The number of sequences in each batch may vary
2
sequences_in_batch = tf.reduce_sum(tf.reduce_max(masked_lm_weight, -1))
3
sequences_in_batch = tf.cast(sequences_in_batch, tf.float32)
4
# Create the 0/1 mask that will be used to un-packed sequences
5
masked_lm_weight = tf.reshape(masked_lm_weight, [B, 1, -1])
6
sequence_selection = tf.reshape(tf.range(1, max_sequences_per_pack + 1), [1, -1, 1])
7
sequence_selection = tf.cast(masked_lm_weight == sequence_selection, tf.float32)
8
# Apply the mask to un-pack the loss per sequence
9
nll_per_token = tf.reshape(nll_per_token, [B, 1, -1])
10
nll_per_sequence = sequence_selection * nll_per_token
11
# Normalize the per-sequence loss by the number of mlm-tokens in the sequence (as is standard)
12
attempted = tf.reduce_sum(sequence_selection, -1, keepdims=True)
13
attempted = attempted + tf.cast(attempted == 0, tf.float32)
# prevent NaNs when dividing by attempted
14
nll_per_sequence = nll_per_sequence/attempted
15
# Average per-batch loss (so contributions from different batches are comparable)
16
lm_loss = tf.reduce_sum(nll_per_sequence)/sequences_in_batch"
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9382978723404255,Under review as a conference paper at ICLR 2022
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9404255319148936,Listing 6: Wikipedia and SQuAD 1.1 histograms
IMPORT NUMPY AS NP,0.9425531914893617,"1
import numpy as np
2
wikipedia_histogram = np.array([
3
0, 0, 0, 0, 1821, 1226, 1969, 1315, 1794, 1953, 3082, 3446, 4166, 5062,
4
9554, 16475, 19173, 17589, 17957, 19060, 21555, 23524, 26954, 30661, 33470, 36614, 40134, 43256,
5
46094, 49350, 52153, 55428, 58109, 60624, 63263, 64527, 65421, 66983, 68123, 68830, 70230, 70486,
6
72467, 72954, 73955, 74311, 74836, 74489, 74990, 75377, 74954, 75096, 74784, 74698, 74337, 74638,
7
74370, 73537, 73597, 73153, 72358, 71580, 71082, 70085, 69733, 69445, 67818, 67177, 66641, 65709,
8
64698, 63841, 63218, 62799, 61458, 60848, 60148, 59858, 58809, 58023, 56920, 55999, 55245, 55051,
9
53979, 53689, 52819, 52162, 51752, 51172, 50469, 49907, 49201, 49060, 47948, 47724, 46990, 46544,
10
46011, 45269, 44792, 44332, 43878, 43984, 42968, 42365, 42391, 42219, 41668, 41072, 40616, 40587,
11
39999, 40169, 39340, 38906, 38438, 38142, 37757, 37818, 37535, 37217, 36757, 36589, 36151, 35953,
12
35531, 35496, 35089, 35053, 34567, 34789, 34009, 33952, 33753, 33656, 33227, 32954, 32686, 32880,
13
32709, 31886, 32126, 31657, 31466, 31142, 31106, 30650, 30316, 30494, 30328, 30157, 29611, 29754,
14
29445, 28921, 29271, 29078, 28934, 28764, 28445, 28319, 28141, 28282, 27779, 27522, 27333, 27470,
15
27289, 27102, 27018, 27066, 26925, 26384, 26188, 26385, 26392, 26082, 26062, 25660, 25682, 25547,
16
25425, 25072, 25079, 25346, 24659, 24702, 24862, 24479, 24288, 24127, 24268, 24097, 23798, 23878,
17
23893, 23817, 23398, 23382, 23280, 22993, 23018, 23242, 22987, 22894, 22470, 22612, 22452, 21996,
18
21843, 22094, 21916, 21756, 21955, 21444, 21436, 21484, 21528, 21597, 21301, 21197, 21281, 21066,
19
20933, 21023, 20888, 20575, 20574, 20511, 20419, 20312, 20174, 20023, 20087, 19955, 19946, 19846,
20
19562, 19710, 19556, 19477, 19487, 19387, 19225, 19069, 19360, 18655, 19034, 18763, 18800, 19012,
21
18893, 18714, 18645, 18577, 18317, 18458, 18374, 18152, 17822, 18102, 17735, 17940, 17805, 17711,
22
17690, 17703, 17669, 17410, 17583, 17331, 17313, 16892, 16967, 16870, 16926, 17233, 16845, 16861,
23
16576, 16685, 16455, 16687, 16747, 16524, 16473, 16349, 16273, 16255, 16228, 16219, 16021, 16111,
24
15867, 15751, 16081, 15703, 15751, 15854, 15665, 15469, 15431, 15428, 15464, 15517, 15335, 15461,
25
15237, 15292, 15305, 15351, 15078, 14810, 15119, 14780, 14664, 14869, 14722, 14890, 14672, 14439,
26
14685, 14706, 14840, 14373, 14286, 14596, 14615, 14168, 14299, 13987, 14167, 14107, 14096, 14202,
27
13985, 14118, 14094, 14127, 13896, 13864, 13597, 13572, 13717, 13669, 13782, 13617, 13284, 13333,
28
13425, 13457, 13256, 13404, 13318, 13425, 13317, 13179, 13193, 13257, 13160, 12813, 13149, 13010,
29
12867, 12958, 12818, 12801, 12749, 12810, 12575, 12673, 12514, 12735, 12523, 12677, 12298, 12469,
30
12341, 12445, 12477, 12326, 12110, 12087, 12305, 12156, 12032, 12190, 12150, 11980, 12022, 11825,
31
11969, 11831, 11997, 11924, 11739, 11685, 11702, 11783, 11783, 11659, 11647, 11610, 11526, 11577,
32
11538, 11536, 11497, 11480, 11374, 11234, 11433, 11466, 11475, 11147, 11376, 11217, 11002, 11245,
33
11124, 11000, 11129, 10923, 10966, 11071, 11029, 11036, 10972, 11012, 10800, 10936, 10904, 10750,
34
10669, 10766, 10780, 10675, 10905, 10511, 10598, 10583, 10658, 10471, 10667, 10601, 10430, 10440,
35
10510, 10148, 10468, 10346, 10257, 10286, 10235, 10351, 10182, 10182, 10095, 10192, 9866, 10070,
36
10148, 9956, 10132, 10043, 9741, 10003, 10056, 9920, 10021, 9838, 9854, 9740, 9782, 9799,
37
9798, 9788, 9840, 9747, 9797, 9893, 9593, 9535, 9658, 9554, 9593, 9530, 9523, 9488,
38
9548, 9418, 9418, 9508, 9638, 9521, 9277, 9289, 9255, 9322, 9281, 9351, 9259, 9255,
39
9225, 9098, 9268, 9227, 9224, 9106, 9239, 3815044], dtype=np.int64)
40
41
wikipedia_max_sequence_length = 512
42
43
squad_1_1_histogram = np.array([
44
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
45
0, 0, 3, 2, 0, 9, 10, 16, 22, 24, 36, 35, 46, 42, 48, 57, 86, 83, 86, 87, 86, 97, 90, 99, 85, 94,
46
105, 114, 110, 93, 116, 118, 114, 116, 117, 127, 115, 155, 137, 145, 157, 151, 153, 149, 163, 157,
47
134, 150, 144, 132, 166, 162, 177, 160, 149, 151, 138, 156, 148, 176, 163, 182, 188, 182, 177, 199,
48
182, 203, 201, 264, 250, 244, 289, 346, 327, 298, 377, 386, 444, 431, 503, 553, 532, 570, 611, 677,
49
648, 673, 712, 722, 745, 692, 697, 747, 754, 741, 777, 781, 825, 813, 836, 777, 776, 756, 789, 790,
50
765, 753, 729, 748, 772, 766, 760, 741, 725, 729, 759, 732, 730, 730, 741, 705, 708, 725, 656, 688,
51
688, 677, 662, 628, 635, 618, 586, 527, 562, 619, 562, 578, 538, 558, 582, 541, 575, 526, 556, 498,
52
529, 486, 528, 541, 482, 521, 483, 466, 514, 459, 447, 436, 383, 401, 408, 381, 369, 364, 381, 420,
53
391, 388, 358, 365, 357, 358, 355, 297, 290, 267, 308, 329, 304, 332, 289, 282, 304, 242, 263, 288,
54
238, 257, 271, 288, 277, 264, 253, 239, 217, 260, 214, 247, 237, 212, 205, 193, 200, 208, 195, 193,
55
201, 187, 170, 176, 195, 156, 201, 179, 159, 183, 169, 178, 163, 153, 171, 144, 138, 181, 165, 171,
56
161, 159, 166, 142, 138, 151, 155, 134, 141, 132, 123, 119, 109, 125, 123, 131, 135, 115, 108, 102,
57
117, 105, 99, 84, 100, 85, 85, 85, 95, 122, 105, 114, 113, 100, 80, 96, 86, 79, 80, 87, 92, 73, 73,
58
64, 76, 72, 77, 67, 60, 71, 77, 79, 72, 55, 67, 42, 59, 65, 72, 49, 43, 62, 48, 50, 54, 45, 42, 53,
59
56, 45, 43, 32, 30, 36, 42, 37, 45, 28, 41, 31, 44, 35, 36, 47, 47, 48, 65, 32, 23, 35, 38, 20, 23,
60
22, 21, 27, 20, 26, 18, 18, 22, 17, 17, 14, 26, 15, 20, 22, 19, 24, 17, 15, 20, 20, 22, 22, 17, 20,
61
16, 21, 16, 23, 12, 14, 1054], dtype=np.int64)
62
63
squad_1)1_max_sequence_length = 384"
IMPORT NUMPY AS NP,0.9446808510638298,Under review as a conference paper at ICLR 2022
IMPORT NUMPY AS NP,0.9468085106382979,Listing 7: Histogram creation for GLUE training datasets
FROM TRANSFORMERS IMPORT AUTOTOKENIZER,0.948936170212766,"1
from transformers import AutoTokenizer
2
import datasets
3
import numpy as np
4
5
# constants
6
max_sequence_length = 128
7
task_to_keys = {
8
""cola"": (""sentence"", None),
9
""mnli"": (""premise"", ""hypothesis""),
10
""mrpc"": (""sentence1"", ""sentence2""),
11
""qnli"": (""question"", ""sentence""),
12
""qqp"": (""question1"", ""question2""),
13
""rte"": (""sentence1"", ""sentence2""),
14
""sst2"": (""sentence"", None),
15
""stsb"": (""sentence1"", ""sentence2""),
16
""wnli"": (""sentence1"", ""sentence2""),
17
}
18
glue_keys = [’cola’, ’sst2’, ’mrpc’, ’qqp’, ’stsb’, ’mnli’, ’rte’, ’wnli’]
19
# unused datasets due to missing training data
20
unglue_keys = [’mnli_matched’, ’mnli_mismatched’, ’qnli’, ’ax’]
21
22
# load data
23
dataset_loads = {}
24
for key in glue_keys:
25
dataset_loads[key] = datasets.load_dataset(""glue"", key, split=’train’)
26
27
# tokenize data
28
tokenizer = AutoTokenizer.from_pretrained(’bert-base-uncased’)
29
tokenized_data = {}
30
for key in dataset_loads:
31
sentence1_key, sentence2_key = task_to_keys[key]
32
33
def preprocess_function(examples):
34
""""""Tokenize the texts""""""
35
args = (
36
(examples[sentence1_key],) if sentence2_key is None
37
else (examples[sentence1_key], examples[sentence2_key])
38
)
39
result = tokenizer(*args, padding=False, max_length=max_sequence_length, truncation=True)
40
return result
41
42
tokenized_data[key] = dataset_loads[key].map(preprocess_function, batched=True)
43
44
# extract length information (for histogram plots)
45
histogram_length = {}
46
for key in tokenized_data:
47
histogram_length[key] = []
48
for number, key in enumerate(tokenized_data.keys()):
49
for raw_record in tokenized_data[key][""input_ids""]:
50
histogram_length[key].append(len([x for x in raw_record if x!=0]))
51
52
# create histogram for packing
53
glue_histogram = {}
54
for data_key in histogram_length:
55
glue_histogram[data_key] = np.array([0] * max_sequence_length, dtype=np.int64)
56
for entry in histogram_length[data_key]:
57
glue_histogram[data_key][entry-1] += 1"
FROM TRANSFORMERS IMPORT AUTOTOKENIZER,0.951063829787234,Under review as a conference paper at ICLR 2022
FROM TRANSFORMERS IMPORT AUTOTOKENIZER,0.9531914893617022,Listing 8: Longest-pack-ﬁrst histogram-packing
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9553191489361702,"1
from collections import defaultdict
2
import numpy as np
3
import time
4
5
6
def add_pack(pack, count, tmp, final, limit, offset, max_sequence_length=512):
7
""""""Filter out packs that reached maximum length or number of components.""""""
8
# sanity checks
9
assert(max_sequence_length-sum(pack) == offset), ""Incorrect offset.""
10
assert(offset >= 0), ""Too small offset.""
11
assert(offset < max_sequence_length), ""Too large offset.""
12
if len(pack) == limit or offset == 0:
13
final[offset].append((count, pack))
14
else:
15
tmp[offset].append((count, pack))
16
17
18
def pack_using_lpfhp(histogram, max_sequence_length, max_sequences_per_pack, distribute=True):
19
""""""Longest-pack-first histogram-packing.""""""
20
start = time.time()
21
reversed_histogram = np.flip(histogram)
22
# Initialize main strategy data dictionary.
23
# The key indicates how many tokens are left for full length.
24
# The value is a list of tuples, consisting of counts and respective packs.
25
# A pack is a (sorted) list of sequence length values that get concatenated.
26
tmp_strategies_per_length = defaultdict(list)
27
strategies_per_length = defaultdict(list)
28
if max_sequences_per_pack is ""max"":
29
max_sequences_per_pack = max_sequence_length
30
# Index i indicates here, how much space is left, due to reversed histogram
31
for i in range(max_sequence_length):
32
n_sequences_to_bin = reversed_histogram[i]
33
length_to_bin = max_sequence_length - i
34
offset = 0
# smallest possible offset for perfect fit
35
while n_sequences_to_bin > 0:
36
if (length_to_bin + offset) in tmp_strategies_per_length:
37
# extract worst pack that will get modified
38
n_sequences_to_pack, pack = tmp_strategies_per_length[
39
length_to_bin + offset].pop()
40
# calculate how often the current sequence maximally fits in
41
repeat = min(1 + offset // length_to_bin, max_sequences_per_pack-len(pack))
42
# correct dependent on count
43
while n_sequences_to_bin//repeat == 0:
44
repeat -= 1
45
if not distribute:
46
repeat = 1
47
new_pack = pack + [length_to_bin]*repeat
48
count = min(n_sequences_to_pack, n_sequences_to_bin//repeat)
49
if n_sequences_to_pack > count:
50
# old pack gets reduced
51
n_sequences_to_pack -= count
52
tmp_strategies_per_length[length_to_bin + offset].append(
53
(n_sequences_to_pack, pack))
54
n_sequences_to_bin -= count * repeat
55
else:
56
n_sequences_to_bin -= n_sequences_to_pack * repeat
57
add_pack(new_pack, count,
58
tmp_strategies_per_length, strategies_per_length,
59
max_sequences_per_pack, offset - (repeat - 1) * length_to_bin,
60
max_sequence_length)
61
# clean up to speed up main key search
62
if not tmp_strategies_per_length[length_to_bin + offset]:
63
tmp_strategies_per_length.pop(length_to_bin + offset)
64
# reset offset in case best fit changed
65
offset = 0
66
else:
67
offset += 1
68
# Does not fit anywhere. Create new pack.
69
if offset >= max_sequence_length - length_to_bin + 1:
70
# similar repetition but no dependence on pack.
71
repeat = min(max_sequence_length//length_to_bin, max_sequences_per_pack)
72
while n_sequences_to_bin//repeat == 0:
73
repeat -= 1
74
if not distribute:
75
repeat = 1
76
add_pack([length_to_bin]*repeat, n_sequences_to_bin//repeat,
77
tmp_strategies_per_length, strategies_per_length,
78
max_sequences_per_pack, max_sequence_length-length_to_bin*repeat,
79
max_sequence_length)
80
n_sequences_to_bin -= n_sequences_to_bin//repeat * repeat"
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9574468085106383,Under review as a conference paper at ICLR 2022
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9595744680851064,"1
# merge all strategies
2
for key in tmp_strategies_per_length:
3
strategies_per_length[key].extend(tmp_strategies_per_length[key])
4
# flatten strategies dictionary
5
strategy_set = []
6
strategy_repeat_count = []
7
for key in strategies_per_length:
8
for count, pack in strategies_per_length[key]:
9
pack.reverse()
10
strategy_set.append(pack)
11
strategy_repeat_count.append(count)
12
13
# Summarize efficiency of solution
14
duration = time.time() - start
15
sequence_lengths = np.arange(1, max_sequence_length + 1)
16
strategy_repeat_count = np.array(strategy_repeat_count)
17
n_strategies = len(strategy_set)
18
old_number_of_samples = histogram.sum()
19
new_number_of_samples = strategy_repeat_count.sum()
20
sequences = sum([count*len(pack) for count, pack in
21
zip(strategy_repeat_count, strategy_set)])
22
total_tokens = max_sequence_length * new_number_of_samples
23
empty_tokens = sum([count*(max_sequence_length-sum(pack)) for count, pack
24
in zip(strategy_repeat_count, strategy_set)])
25
efficiency = 100 - empty_tokens / total_tokens * 100
26
speedup_upper_bound = 1.0/(1 - (histogram*(
27
1 - sequence_lengths / max_sequence_length)).sum() / old_number_of_samples)
28
29
print(f""Packing efficiency (fraction of real tokens): {efficiency:3.4f}\n"",
30
f""Speed-up theoretical limit: {speedup_upper_bound:3.4f}\n"",
31
f""Achieved speed-up over un-packed dataset: {old_number_of_samples/new_number_of_samples:3.5f}"",
32
f""Runtime: Packed {old_number_of_samples} sequences in {duration:3.3f} seconds."")
33
34
return strategy_set, strategy_repeat_count"
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9617021276595744,Under review as a conference paper at ICLR 2022
FROM COLLECTIONS IMPORT DEFAULTDICT,0.9638297872340426,Listing 9: Extended non-negative least squares histogram-packing
IMPORT TIME,0.9659574468085106,"1
import time
2
import numpy as np
3
from scipy import optimize, stats
4
from functools import lru_cache
5
6
def get_packing_matrix(strategy_set, max_sequence_length):
7
num_strategies = len(strategy_set)
8
A = np.zeros((max_sequence_length, num_strategies), dtype=np.int32)
9
for i, strategy in enumerate(strategy_set):
10
for seq_len in strategy:
11
A[seq_len - 1, i] += 1
12
return A
13
14
@lru_cache(maxsize=None)
15
def get_packing_strategies(start_length, minimum_increment, target_length, depth):
16
gap = target_length - start_length
17
strategies = []
18
# Complete the packing with exactly 1 number
19
if depth == 1:
20
if gap >= minimum_increment:
21
strategies.append([gap])
22
# Complete the sample in ""depth"" steps, recursively
23
else:
24
for new in range(minimum_increment, gap + 1):
25
new_gap = target_length - start_length - new
26
if new_gap == 0:
27
strategies.append([new])
28
else:
29
options = get_packing_strategies(start_length + new, new, target_length, depth - 1)
30
for option in options:
31
if len(option) > 0:
32
strategies.append([new] + option)
33
return strategies
34
35
def pack_using_ennlshp(histogram, max_sequence_length, max_sequences_per_pack):
36
# List all unique ways of packing to the desired maximum sequence length
37
strategy_set = get_packing_strategies(0, 1, max_sequence_length, max_sequences_per_pack)
38
print(f""Packing will involve {len(strategy_set)} unique packing strategies."")
39
# Get the packing matrix corresponding to this list of packing strategies
40
A = get_packing_matrix(strategy_set, max_sequence_length)
41
# Weights that penalize the residual by the number of resulting padding tokens.
42
w0 = np.array([x+1 for x in range(max_sequence_length)])
43
# construct the packing matrix
44
A_bar = np.zeros((2*max_sequence_length, len(strategy_set) + max_sequence_length), ’d’)
45
# Base weighted matrix
46
A_bar[:max_sequence_length, :len(strategy_set)] = np.expand_dims(w0, -1) * A
47
# Higher weight to avoid positive residual
48
A_bar[max_sequence_length:, :len(strategy_set)] = np.expand_dims(
49
10**6*np.ones([max_sequence_length]), -1) * A
50
# negative diagonal unity matrix for mapping to residual
51
A_bar[max_sequence_length:, len(strategy_set):] = np.expand_dims(
52
10**6*np.ones([max_sequence_length]), -1)*np.ones((max_sequence_length,max_sequence_length))
53
b_bar = np.zeros(2*max_sequence_length)
54
# Apply weighting to histogram vector
55
b_bar[:max_sequence_length] = w0 * histogram
56
b_bar[max_sequence_length:] = 10**6*np.ones([max_sequence_length]) * histogram
57
# Solve the packing problem
58
print(f""Sequences to pack: "", histogram.sum())
59
start = time.time()
60
strategy_residual, rnorm = optimize.nnls(A_bar, b_bar)
61
strategy_repeat_count = strategy_residual[:len(strategy_set)]
62
print(f""Solving non-negative least squares took {time.time() - start:3.2f} seconds."")
63
# Round the floating point solution to nearest integer
64
strategy_repeat_count = np.rint(strategy_repeat_count).astype(np.int64)
65
# Compute the residuals, shape: [max_sequence_length]
66
residual = histogram - A @ strategy_repeat_count
67
# Handle the left-over sequences i.e. positive part of residual
68
unpacked_seqlen = np.arange(1, max_sequence_length + 1)[residual > 0]
69
for l in unpacked_seqlen:
70
strategy = sorted([l, max_sequence_length - l])
# the depth 1 strategy
71
strategy_index = strategy_set.index(strategy)
72
strategy_repeat_count[strategy_index] += residual[l-1]
73
# Re-compute the residual with the updated strategy_repeat_count
74
# This should now be strictly < 0
75
residual = histogram - A @ strategy_repeat_count
76
# Add padding based on deficit (negative residual portion of residual)
77
padding = np.where(residual < 0, -residual, 0)
78
# Calculate some basic statistics
79
sequence_lengths = np.arange(1, max_sequence_length + 1)
80
old_number_of_samples = histogram.sum()
81
new_number_of_samples = int(strategy_repeat_count.sum())
82
speedup_upper_bound = 1.0/(1 - (histogram*(
83
1 - sequence_lengths / max_sequence_length)).sum()/old_number_of_samples)
84
num_padding_tokens_packed = (sequence_lengths * padding).sum()
85
efficiency = 1 - num_padding_tokens_packed/(new_number_of_samples*max_sequence_length)
86
print(f""Packing efficiency (fraction of real tokens): {efficiency:3.4f}\n"",
87
f""Speed-up theoretical limit: {speedup_upper_bound:3.4f}\n"",
88
f""Achieved speed-up over un-packed dataset: {old_number_of_samples/new_number_of_samples:3.5f}"")
89
return strategy_set, strategy_repeat_count"
IMPORT TIME,0.9680851063829787,Under review as a conference paper at ICLR 2022
IMPORT TIME,0.9702127659574468,APPENDIX REFERENCES
IMPORT TIME,0.9723404255319149,"G. Belov and G. Scheithauer.
A branch-and-cut-and-price algorithm for one-dimensional stock
cutting and two-dimensional two-stage cutting. European Journal of Operational Research, 171
(1):85–106, may 2006. ISSN 03772217. doi: 10.1016/j.ejor.2004.08.036. URL https://
linkinghub.elsevier.com/retrieve/pii/S0377221704006150."
IMPORT TIME,0.9744680851063829,"Michael R. Garey and David S. Johnson. Computers and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co., USA, 1990. ISBN 0716710455."
IMPORT TIME,0.9765957446808511,"Gabriel Guill´en, Claudia Diaz-Camino, Carlos Loyola-Torres, Rosaura Aparicio-Fabre, Alejandrina
Hern´andez-L´opez, Mauricio D´ıaz-S´anchez, and Federico Sanchez. Detailed analysis of putative
genes encoding small proteins in legume genomes.
Frontiers in Plant Science, 4:208, 2013.
ISSN 1664-462X. doi: 10.3389/fpls.2013.00208. URL https://www.frontiersin.org/
article/10.3389/fpls.2013.00208."
IMPORT TIME,0.9787234042553191,"Henrik B. Hansen, Peter B. Damgaard, Ashot Margaryan, Jesper Stenderup, Niels Lynnerup, Eske
Willerslev, and Morten E. Allentoft. Comparing ancient dna preservation in petrous bone and
tooth cementum. PLOS ONE, 12(1):1–18, 01 2017. doi: 10.1371/journal.pone.0170940. URL
https://doi.org/10.1371/journal.pone.0170940."
IMPORT TIME,0.9808510638297873,"S. Kotz and S. Nadarajah. Extreme Value Distributions. World Scientiﬁc Publishing Company, 2000.
ISBN 9781783261734."
IMPORT TIME,0.9829787234042553,"Charles L. Lawson and Richard J. Hanson. Solving Least Squares Problems. Society for Industrial
and Applied Mathematics, jan 1995. ISBN 978-0-89871-356-5. doi: 10.1137/1.9781611971217.
URL http://epubs.siam.org/doi/book/10.1137/1.9781611971217."
IMPORT TIME,0.9851063829787234,"C. C. Lee and D. T. Lee. A Simple On-Line Bin-Packing Algorithm. Journal of the ACM (JACM),
32(3):562–572, jul 1985. ISSN 1557735X. doi: 10.1145/3828.3833. URL https://dl.acm.
org/doi/10.1145/3828.3833."
IMPORT TIME,0.9872340425531915,"Y. Luo and Ramani Duraiswami. Efﬁcient parallel non-negative least squares on multi-core archi-
tectures. SIAM Journal on Scientiﬁc Computing, 33:2848 – 2863, 2011."
IMPORT TIME,0.9893617021276596,"NVIDIA.
Performance catalogue for BERT on Pytorch.
https://ngc.nvidia.com/
catalog/resources/nvidia:bert_for_pytorch/performance, 2021."
IMPORT TIME,0.9914893617021276,"Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer Learning in Biomedical Natural Language
Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets. In Proceedings
of the 2019 Workshop on Biomedical Natural Language Processing (BioNLP 2019), pp. 58–65,
2019."
IMPORT TIME,0.9936170212765958,"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing
in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2."
IMPORT TIME,0.9957446808510638,"Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu,
Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Bran-
deis, Sylvain Gugger, Franc¸ois Lagunas, Lysandre Debut, Morgan Funtowicz, Anthony Moi,
Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Muˇstar, Jeff Boudier, and Anna Tordjmann.
Datasets. GitHub. Note: https://github.com/huggingface/datasets, 1, 2020."
IMPORT TIME,0.997872340425532,"Wolfram Research Inc.
Mathematica, Version 12.2.
URL https://www.wolfram.com/
wolfram-alpha-notebook-edition. Champaign, IL, 2020."
