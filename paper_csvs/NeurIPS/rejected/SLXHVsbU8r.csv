Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009293680297397769,"We propose UAD, a method for vision-based end-to-end autonomous driving
1"
ABSTRACT,0.0018587360594795538,"(E2EAD), achieving the best open-loop evaluation performance in nuScenes, mean-
2"
ABSTRACT,0.0027881040892193307,"while showing robust closed-loop driving quality in CARLA. Our motivation stems
3"
ABSTRACT,0.0037174721189591076,"from the observation that current E2EAD models still mimic the modular archi-
4"
ABSTRACT,0.004646840148698885,"tecture in typical driving stacks, with carefully designed supervised perception
5"
ABSTRACT,0.0055762081784386614,"and prediction subtasks to provide environment information for oriented planning.
6"
ABSTRACT,0.006505576208178439,"Although achieving groundbreaking progress, such design has certain drawbacks:
7"
ABSTRACT,0.007434944237918215,"1) preceding subtasks require massive high-quality 3D annotations as supervision,
8"
ABSTRACT,0.008364312267657992,"posing a significant impediment to scaling the training data; 2) each submodule
9"
ABSTRACT,0.00929368029739777,"entails substantial computation overhead in both training and inference. To this end,
10"
ABSTRACT,0.010223048327137546,"we propose UAD, an E2EAD framework with an unsupervised1 proxy to address
11"
ABSTRACT,0.011152416356877323,"all these issues. Firstly, we design a novel Angular Perception Pretext to eliminate
12"
ABSTRACT,0.012081784386617101,"the annotation requirement. The pretext models the driving scene by predicting the
13"
ABSTRACT,0.013011152416356878,"angular-wise spatial objectness and temporal dynamics, without manual annota-
14"
ABSTRACT,0.013940520446096654,"tion. Secondly, a self-supervised training strategy, which learns the consistency of
15"
ABSTRACT,0.01486988847583643,"the predicted trajectories under different augment views, is proposed to enhance
16"
ABSTRACT,0.015799256505576207,"the planning robustness in steering scenarios. Our UAD achieves 38.7% relative
17"
ABSTRACT,0.016728624535315983,"improvements over UniAD on the average collision rate in nuScenes and surpasses
18"
ABSTRACT,0.017657992565055763,"VAD for 6.40 points on the driving score in CARLA’s Town05 Long benchmark.
19"
ABSTRACT,0.01858736059479554,"Moreover, the proposed method only consumes 44.3% training resources of UniAD
20"
ABSTRACT,0.019516728624535316,"and runs 3.4× faster in inference. Our innovative design not only for the first time
21"
ABSTRACT,0.020446096654275093,"demonstrates unarguable performance advantages over supervised counterparts,
22"
ABSTRACT,0.02137546468401487,"but also enjoys unprecedented efficiency in data, training, and inference.
23"
INTRODUCTION,0.022304832713754646,"1
Introduction
24"
INTRODUCTION,0.023234200743494422,"Recent decades have witnessed breakthrough achievements in autonomous driving. The end-to-
25"
INTRODUCTION,0.024163568773234202,"end paradigm, which seeks to integrate perception, prediction, and planning tasks into a unified
26"
INTRODUCTION,0.02509293680297398,"framework, stands as a representative branch [33, 1, 39, 3, 35, 21, 22]. The latest advances in end-to-
27"
INTRODUCTION,0.026022304832713755,"end autonomous driving significantly piqued researchers’ interest [21, 22]. However, handcrafted and
28"
INTRODUCTION,0.02695167286245353,"resource-intensive supervised sub-tasks for perception and prediction, which have previously proved
29"
INTRODUCTION,0.027881040892193308,"their utility in environment modeling [35, 3, 20], continue to be indispensable, as shown in Fig. 1a.
30"
INTRODUCTION,0.028810408921933085,"Then what insights have we gained from the recent advances? It has come to our attention that one of
31"
INTRODUCTION,0.02973977695167286,"the most enlightening innovations lies in the Transformer-based pipeline, in which the queries act
32"
INTRODUCTION,0.03066914498141264,"as a connective thread, seamlessly bridging various tasks. Besides, the capability for environment
33"
INTRODUCTION,0.031598513011152414,"modeling has also seen a significant boost, primarily due to complicated interactions of supervised
34"
INTRODUCTION,0.032527881040892194,"1Following [30, 4], here we consider the methods as “unsupervised” ones as long as no manual annotation is
used and required in the target task or domain."
INTRODUCTION,0.03345724907063197,"(a)
(b)
Figure 1: (a) End-to-end autonomous driving paradigms. 1) The vanilla architecture that directly
predicts control command. 2) The modularized design that combines various preceding tasks. 3)
Our proposed framework with unsupervised pretext task. (b) Comparison of training cost, inference
speed and average L2 error between our method and [21, 22] on 8 NVIDIA Tesla A100 GPUs."
INTRODUCTION,0.03438661710037175,"sub-tasks. However, every coin has two sides. In comparison to the vanilla design [33] (see Fig. 1a),
35"
INTRODUCTION,0.03531598513011153,"modularized methods incur unavoidable computation and annotation overhead. As illustrated in
36"
INTRODUCTION,0.0362453531598513,"Fig. 1b, the training of the recent method UniAD [21] takes 48 GPU days while running at only 2.1
37"
INTRODUCTION,0.03717472118959108,"frames per second (FPS). Moreover, modules in existing perception and prediction design require large
38"
INTRODUCTION,0.03810408921933085,"quantities of high-quality annotated data. The financial overhead for human annotation significantly
39"
INTRODUCTION,0.03903345724907063,"impedes the scalability of such modularized methods with supervised subtasks to leverage massive
40"
INTRODUCTION,0.039962825278810406,"data. As proved by large foundation models [24, 31], scaling up the data volume is the key to bringing
41"
INTRODUCTION,0.040892193308550186,"the model capabilities to the next level. Thus we ask ourselves the question: Is it viable to devise an
42"
INTRODUCTION,0.041821561338289966,"efficient and robust E2EAD framework while alleviating the reliance on 3D annotation?
43"
INTRODUCTION,0.04275092936802974,"In this work, we show the answer is affirmative by proposing an innovative Unsupervised pretext task
44"
INTRODUCTION,0.04368029739776952,"for end-to-end Autonomous Driving (UAD), which seeks to efficiently model the environment. The
45"
INTRODUCTION,0.04460966542750929,"pretext task consists of an angular-wise perception module to learn spatial information by predicting
46"
INTRODUCTION,0.04553903345724907,"the objectness of each sector region in BEV space, and an angular-wise dreaming decoder to absorb
47"
INTRODUCTION,0.046468401486988845,"temporal knowledge by predicting inaccessible future states. The introduced angular queries link
48"
INTRODUCTION,0.047397769516728624,"the two modules as a whole pretext task to perceive the driving scene. Notably, our method shines
49"
INTRODUCTION,0.048327137546468404,"by completely eliminating the annotation requirement for perception and prediction. Such data
50"
INTRODUCTION,0.04925650557620818,"efficiency is not attainable for current methods with complex supervised modularization [21, 22]. The
51"
INTRODUCTION,0.05018587360594796,"supervision for learning spatial objectness is obtained by projecting the 2D region of interests (ROIs)
52"
INTRODUCTION,0.05111524163568773,"from an off-the-shelf open-set detector [28] to BEV space. While utilizing the publicly available open-
53"
INTRODUCTION,0.05204460966542751,"set 2D detector pre-trained with manual annotation from other domains (e.g. COCO [27]), we avoid
54"
INTRODUCTION,0.05297397769516728,"the need for any additional 3D labels within our paradigm and target domains (e.g. nuScenes [2] and
55"
INTRODUCTION,0.05390334572490706,"CARLA [11]), thereby creating a pragmatically unsupervised setting [30]. Furthermore, we introduce
56"
INTRODUCTION,0.05483271375464684,"a self-supervised direction-aware learning strategy to train the planning model. Specifically, the
57"
INTRODUCTION,0.055762081784386616,"visual observations are augmented with different rotation angles, and the consistency loss is applied
58"
INTRODUCTION,0.056691449814126396,"to the predictions for robust planning. Without bells and whistles, the proposed UAD outperforms
59"
INTRODUCTION,0.05762081784386617,"UniAD for 0.13m in nuScenes Avg. L2 error, and surpasses VAD [22] for 9.92 points in CARLA
60"
INTRODUCTION,0.05855018587360595,"route completion score. Such unprecedented performance gain is achieved with a 3.4× inference
61"
INTRODUCTION,0.05947955390334572,"speed, a mere 44.3% training budget of UniAD, and zero annotations, as illustrated in Fig. 1b.
62"
INTRODUCTION,0.0604089219330855,"In summary, our contributions are as follows: 1) We propose an unsupervised pretext task to discard
63"
INTRODUCTION,0.06133828996282528,"the requirement of 3D manual annotation in end-to-end autonomous driving, potentially making
64"
INTRODUCTION,0.062267657992565055,"it more feasible to scale the training data to billions level without any labeling overload; 2) We
65"
INTRODUCTION,0.06319702602230483,"introduce a novel self-supervised direction-aware learning strategy to maximize the consistency of the
66"
INTRODUCTION,0.06412639405204461,"predicted trajectories under different augment views, which enhances planning robustness in steering
67"
INTRODUCTION,0.06505576208178439,"scenarios; 3) Our method shows superiority in both open- and closed-loop evaluation compared with
68"
INTRODUCTION,0.06598513011152417,"other vision-based E2EAD methods, with much lower computation and annotation cost.
69"
RELATED WORK,0.06691449814126393,"2
Related Work
70"
END-TO-END AUTONOMOUS DRIVING,0.06784386617100371,"2.1
End-to-End Autonomous Driving
71"
END-TO-END AUTONOMOUS DRIVING,0.0687732342007435,"End-to-end autonomous driving can be dated back to 1988, when the ALVINN [33] proposed by
72"
END-TO-END AUTONOMOUS DRIVING,0.06970260223048327,"Carnegie Mellon University could successfully navigate a vehicle over 400 meters. After that, to
73"
END-TO-END AUTONOMOUS DRIVING,0.07063197026022305,"BEV
Encoder"
END-TO-END AUTONOMOUS DRIVING,0.07156133828996282,BEV Feature
END-TO-END AUTONOMOUS DRIVING,0.0724907063197026,"OS 2D 
Detector"
D BOXES,0.07342007434944238,2D Boxes
D BOXES,0.07434944237918216,Dreaming
D BOXES,0.07527881040892194,Decoder
D BOXES,0.0762081784386617,BEV Object Mask
D BOXES,0.07713754646840149,"Angular 
Partition"
D BOXES,0.07806691449814127,Angular Objectness Label
D BOXES,0.07899628252788105,Angular BEV Feature
D BOXES,0.07992565055762081,Angular Query
D BOXES,0.08085501858736059,Perception
D BOXES,0.08178438661710037,"Predict
View
Transform"
D BOXES,0.08271375464684015,Objectness Loss
D BOXES,0.08364312267657993,"Cross
Attention"
D BOXES,0.0845724907063197,Ego Query
D BOXES,0.08550185873605948,Direction
D BOXES,0.08643122676579926,Predict
D BOXES,0.08736059479553904,Planning
D BOXES,0.08828996282527882,Predict
D BOXES,0.08921933085501858,Direction-Augmented
D BOXES,0.09014869888475836,GT Ego Trajectory
D BOXES,0.09107806691449814,Multi-view Images
D BOXES,0.09200743494423792,"Direction
Augment"
D BOXES,0.09293680297397769,Direction Loss
D BOXES,0.09386617100371747,Imitation Loss
D BOXES,0.09479553903345725,"Turn 
Right"
D BOXES,0.09572490706319703,"Driving
Command F F F F"
D BOXES,0.09665427509293681,"270
180
90"
D BOXES,0.09758364312267657,"270
180
90"
D BOXES,0.09851301115241635,"Angular Perception Pretext
Direction Aware Planning K K"
D BOXES,0.09944237918215613,"Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue
background, which plans ego trajectory based on the input multi-view images. The training pipeline
consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware
Planning (orange arrows with purple background). “F” in BEV feature indicates the driving direction."
D BOXES,0.10037174721189591,"improve the robustness of E2EAD, a series of modern approaches such as NEAT [6], P3 [35],
74"
D BOXES,0.10130111524163568,"MP3 [3], ST-P3 [20] introduce the design of more dedicated modularization, which integrate auxiliary
75"
D BOXES,0.10223048327137546,"information such as HD maps, and additional tasks like bird’s-eye view (BEV) segmentation. Most re-
76"
D BOXES,0.10315985130111524,"cently, embracing advanced architectures like Transfromer [37] and visual occupancy prediction [29],
77"
D BOXES,0.10408921933085502,"UniAD [21] and VAD [22] demonstrate impressive performance in open-loop evaluation. In this work,
78"
D BOXES,0.1050185873605948,"instead of integrating complex supervised modular sub-tasks, we innovatively propose another path
79"
D BOXES,0.10594795539033457,"proving that an efficient unsupervised pretext task without any human annotation like 3D bounding
80"
D BOXES,0.10687732342007435,"boxes and point cloud categories, can achieve even superior performance than recent state-of-the-arts.
81"
WORLD MODEL,0.10780669144981413,"2.2
World Model
82"
WORLD MODEL,0.1087360594795539,"In pursuit of understanding the dynamic changes in environments, researchers in the fields of gaming
83"
WORLD MODEL,0.10966542750929369,"and robotics have proposed various world models [13, 14, 15, 16]. Recently, the autonomous driving
84"
WORLD MODEL,0.11059479553903345,"community introduces world models for safer maneuvering [32, 18, 12, 38]. MILE [18] considers
85"
WORLD MODEL,0.11152416356877323,"the environment as a high-level embedding and tends to predict its future state with historical
86"
WORLD MODEL,0.11245353159851301,"observations. Drive-WM [38] proposes a framework to integrate world models with existing E2E
87"
WORLD MODEL,0.11338289962825279,"methods to improve planning robustness. In this work, we propose an auto-regressive mechanism,
88"
WORLD MODEL,0.11431226765799256,"tailored to our unsupervised pretext, to capture angular-wise temporal dynamics within each sector.
89"
METHOD,0.11524163568773234,"3
Method
90"
OVERVIEW,0.11617100371747212,"3.1
Overview
91"
OVERVIEW,0.1171003717472119,"As illustrated in Fig. 2, our UAD framework consists of two essential components: 1) the Angular
92"
OVERVIEW,0.11802973977695168,"Perception Pretext, aims to liberate E2EAD from costly modularized tasks in an unsupervised fashion;
93"
OVERVIEW,0.11895910780669144,"2) the Direction-Aware Planning, learns self-supervised consistency of the augmented trajectories.
94"
OVERVIEW,0.11988847583643122,"Specifically, UAD first models the driving environment with the pretext. The spatial knowledge
95"
OVERVIEW,0.120817843866171,"is acquired by estimating the objectness of each sector region within the BEV space. The angular
96"
OVERVIEW,0.12174721189591078,"queries, each responsible for a sector, are introduced to extract features and predict the objectness.
97"
OVERVIEW,0.12267657992565056,"The supervision label is generated by projecting the 2D regions of interests (ROIs) to the BEV space,
98"
OVERVIEW,0.12360594795539033,"which are predicted with an available open-set detector GroundingDINO [28]. This way not only
99"
OVERVIEW,0.12453531598513011,"eliminates the 3D annotation requirement, but also greatly reduces the training budget. Moreover, as
100"
OVERVIEW,0.12546468401486988,"driving is inherently a dynamic and continuous process, we thus propose an angular-wise dreaming
101"
OVERVIEW,0.12639405204460966,"decoder to encode the temporal knowledge. The dreaming decoder can be viewed as an augmented
102"
OVERVIEW,0.12732342007434944,"world model [13] capable of auto-regressively predicting the future states.
103"
OVERVIEW,0.12825278810408922,"Subsequently, direction-aware planning is introduced to train the planning module. The raw BEV
104"
OVERVIEW,0.129182156133829,"feature is augmented with different rotation angles, yielding rotated BEV representations and ego
105"
OVERVIEW,0.13011152416356878,"trajectories. We apply self-supervised consistency loss to the predicted trajectories of each augmented
106"
OVERVIEW,0.13104089219330856,"view, which is expected to improve the robustness for directional change and input noises. The
107"
OVERVIEW,0.13197026022304834,"learning strategy can also be regarded as a novel data augmentation technique customized for end-to-
108"
OVERVIEW,0.13289962825278812,"end autonomous driving, which enhances the diversity of trajectory distribution.
109"
D BOXES,0.13382899628252787,2D Boxes
D BOXES,0.13475836431226765,BEV Object Mask
D BOXES,0.13568773234200743,"Multi-view Images
Multi-view Image Masks"
D BOXES,0.1366171003717472,"Sampling
Detect"
D BOXES,0.137546468401487,Angular Objectness Label K (a)
D BOXES,0.13847583643122677,Angular Query
D BOXES,0.13940520446096655,"Cross
Attention μ σ"
D BOXES,0.14033457249070633,Prior Distribution GRU
D BOXES,0.1412639405204461,Updated Angular Query
D BOXES,0.1421933085501859,Distribution
D BOXES,0.14312267657992564,Sampling
D BOXES,0.14405204460966542,Posterior Distribution
D BOXES,0.1449814126394052,Dreaming Loss
D BOXES,0.14591078066914498,Update
D BOXES,0.14684014869888476,Output
D BOXES,0.14776951672862454,Dreaming Layer
D BOXES,0.14869888475836432,×Planning Steps
D BOXES,0.1496282527881041,Angular BEV Feature K μ σ
D BOXES,0.15055762081784388,"(b)
Figure 3: (a) Label generation for angular perception pretext. (b) Illustration of dreaming decoder."
ANGULAR PERCEPTION PRETEXT,0.15148698884758363,"3.2
Angular Perception Pretext
110"
ANGULAR PERCEPTION PRETEXT,0.1524163568773234,"Spatial Representation Learning. Our model attempts to acquire spatial knowledge of the driving
111"
ANGULAR PERCEPTION PRETEXT,0.1533457249070632,"scene by predicting the objectness of each sector region within the BEV space. Specifically, taking
112"
ANGULAR PERCEPTION PRETEXT,0.15427509293680297,"multi-view images {Ii ∈RHi×Wi×3} as input, the BEV encoder [25] first extracts visual information
113"
ANGULAR PERCEPTION PRETEXT,0.15520446096654275,"into the BEV feature Fb ∈RHb×Wb×C. Then, Fb is partitioned into K sectors with a uniform angle θ
114"
ANGULAR PERCEPTION PRETEXT,0.15613382899628253,"centered around ego car. Each sector contains several feature points in BEV space. Denoting feature
115"
ANGULAR PERCEPTION PRETEXT,0.1570631970260223,"of a sector as f ∈RN×C, where N is the maximum number of feature points in all sectors, we derive
116"
ANGULAR PERCEPTION PRETEXT,0.1579925650557621,"angular BEV feature Fa ∈RK×N×C. Zero-padding is applied on sectors with fewer than N points.
117"
ANGULAR PERCEPTION PRETEXT,0.15892193308550187,"Then why do we partition the rectangular BEV feature to angular-wise formatting? The underlying
118"
ANGULAR PERCEPTION PRETEXT,0.15985130111524162,"reason is that, in the absence of depth information, the region in BEV space corresponding to an ROI
119"
ANGULAR PERCEPTION PRETEXT,0.1607806691449814,"in 2D image is a sector. As illustrated in Fig. 3a, by projecting 3D sampling points to images and
120"
ANGULAR PERCEPTION PRETEXT,0.16171003717472118,"verifying their presence in 2D ROIs, a BEV object mask M∈RHb×Wb×1 is generated, representing
121"
ANGULAR PERCEPTION PRETEXT,0.16263940520446096,"the objectness in BEV space. Specifically, the sampling points falling within 2D ROIs are set to 1,
122"
ANGULAR PERCEPTION PRETEXT,0.16356877323420074,"while the others are 0. It is noticed that the positive sectors are irregularly and sparsely distributed in
123"
ANGULAR PERCEPTION PRETEXT,0.16449814126394052,"BEV space. To make the objectness label more compact, similar to the BEV feature partition, we
124"
ANGULAR PERCEPTION PRETEXT,0.1654275092936803,"uniformly divide M into K equal parts. The segments overlapped with positive sectors are assigned
125"
ANGULAR PERCEPTION PRETEXT,0.16635687732342008,"with 1, constituting the angular objectness label Yobj ∈RK×1. Thanks to the rapid development
126"
ANGULAR PERCEPTION PRETEXT,0.16728624535315986,"of open-set detection, it’s now convenient to obtain 2D ROIs for the input multi-view images by
127"
ANGULAR PERCEPTION PRETEXT,0.16821561338289961,"feeding the pre-defined prompts (e.g., vehicle, pedestrian, and barrier) to a 2D open-set detector like
128"
ANGULAR PERCEPTION PRETEXT,0.1691449814126394,"GroundingDINO [28]. Such design is the key in reducing annotation cost and scaling up the dataset.
129"
ANGULAR PERCEPTION PRETEXT,0.17007434944237917,"To predict the objectness score of each sector, we define angular queries Qa ∈RK×C to summarize
130"
ANGULAR PERCEPTION PRETEXT,0.17100371747211895,"Fa. Each angular query qa ∈R1×C in Qa will interact with corresponding f by cross attention [37],
131"
ANGULAR PERCEPTION PRETEXT,0.17193308550185873,"qa = CrossAttention(qa, f),
(1)"
ANGULAR PERCEPTION PRETEXT,0.17286245353159851,"Finally, we map Qa to the objectness scores Pa ∈RK×1 with a linear layer, which is supervised by
132"
ANGULAR PERCEPTION PRETEXT,0.1737918215613383,"Yobj with binary cross-entropy loss (denoted as Lspat).
133"
ANGULAR PERCEPTION PRETEXT,0.17472118959107807,"Temporal Representation Learning. We propose to capture the temporal information of driving
134"
ANGULAR PERCEPTION PRETEXT,0.17565055762081785,"scenarios with the angular-wise dreaming decoder. As shown in Fig. 3b, the decoder auto-regressively
135"
ANGULAR PERCEPTION PRETEXT,0.17657992565055763,"learns transition dynamics of each sector in a similar way of world model [14]. Assuming the planning
136"
ANGULAR PERCEPTION PRETEXT,0.1775092936802974,"module predicts the trajectories of future T steps, the dreaming decoder accordingly comprises T
137"
ANGULAR PERCEPTION PRETEXT,0.17843866171003717,"layers, where each updates the input angular queries Qa and angular BEV feature Fa based on the
138"
ANGULAR PERCEPTION PRETEXT,0.17936802973977695,"learned temporal dynamics. At step t, the queries Qt−1
a
first grasp environmental dynamics from the
139"
ANGULAR PERCEPTION PRETEXT,0.18029739776951673,"observation feature Ft
a with a gated recurrent unit (GRU) [7], which generates Qt
a (hidden state),
140"
ANGULAR PERCEPTION PRETEXT,0.1812267657992565,"Qt
a = GRU(Qt−1
a
, Ft
a),
(2)"
ANGULAR PERCEPTION PRETEXT,0.1821561338289963,"In previous world models, the hidden state Q is solely used for perceiving observed scenes. The
141"
ANGULAR PERCEPTION PRETEXT,0.18308550185873607,"GRU iteration thus ends at t with the final observation Ft
a. In our framework, Q is also used for
142"
ANGULAR PERCEPTION PRETEXT,0.18401486988847585,"predicting ego trajectories in the future. Yet, the future observation, e.g., Ft+1
a
, is unavailable, as
143"
ANGULAR PERCEPTION PRETEXT,0.18494423791821563,"the world model [14] is designed for forecasting the future with only current observation. To obtain
144"
ANGULAR PERCEPTION PRETEXT,0.18587360594795538,"Qt+1
a
, we first propose to update Ft
a to provide pseudo observations ˆFt+1
a
,
145"
ANGULAR PERCEPTION PRETEXT,0.18680297397769516,"ˆFt+1
a
= CrossAttention(Ft
a, Qt
a).
(3)"
ANGULAR PERCEPTION PRETEXT,0.18773234200743494,"Then Qt+1
a
can be generated with Eq. 2 and inputs of ˆFt+1
a
and Qt
a.
146"
ANGULAR PERCEPTION PRETEXT,0.18866171003717472,"Following the loss design in world models [14, 15, 16], we respectively map Qt−1
a
and Qt
a to distri-
147"
ANGULAR PERCEPTION PRETEXT,0.1895910780669145,"butions of {µt−1
a
, σt−1
a
∈RK×C} and {µt
a, σt
a ∈RK×C}, and then minimize their KL divergence.
148"
ANGULAR PERCEPTION PRETEXT,0.19052044609665428,"For the prior distribution from Qt−1
a
, it’s regarded as a prediction of the future dynamics without
149"
ANGULAR PERCEPTION PRETEXT,0.19144981412639406,"observation. In contrast, the posterior distribution from Qt
a represents the future dynamics with the
150"
ANGULAR PERCEPTION PRETEXT,0.19237918215613384,"observation Ft
a. The KL divergence between the two distributions measures the gap between the
151"
ANGULAR PERCEPTION PRETEXT,0.19330855018587362,"imagined future (prior) and the true future (posterior). We expect to enhance the capability of future
152"
ANGULAR PERCEPTION PRETEXT,0.19423791821561337,"prediction for long-term driving safety, which is realized by optimizing the dreaming loss Ldrm,
153"
ANGULAR PERCEPTION PRETEXT,0.19516728624535315,"Ldrm = KL({µt
a, σt
a}||{µt−1
a
, σt−1
a
}),
(4)"
DIRECTION-AWARE PLANNING,0.19609665427509293,"3.3
Direction-Aware Planning
154"
DIRECTION-AWARE PLANNING,0.1970260223048327,"Planning Head. The outputs of angular perception pretext contain a group of angular queries
155"
DIRECTION-AWARE PLANNING,0.1979553903345725,"{Qt
a (t = 1, ..., T)}. For planning, we correspondingly initialize T ego queries {Qt
ego ∈R1×C (t =
156"
DIRECTION-AWARE PLANNING,0.19888475836431227,"1, ..., T)} to extract planning-relevant information and predict the ego trajectory of each future time
157"
DIRECTION-AWARE PLANNING,0.19981412639405205,"step. The interaction between ego queries and angular queries is performed with cross attention,
158"
DIRECTION-AWARE PLANNING,0.20074349442379183,"Qt
ego = CrossAttention(Qt
ego, Qt
a).
(5)"
DIRECTION-AWARE PLANNING,0.2016728624535316,"The output ego queries {Qt
ego} are then used to predict the ego trajectories of future T steps.
159"
DIRECTION-AWARE PLANNING,0.20260223048327136,"Following previous works [21, 22], a high-level driving signal c (turn left, turn right or go straight) is
160"
DIRECTION-AWARE PLANNING,0.20353159851301114,"provided as prior knowledge. The planning head takes the concatenated ego feature Fego ∈RT ×C
161"
DIRECTION-AWARE PLANNING,0.20446096654275092,"from {Qt
ego} and the driving command c as inputs, and outputs the planning trajectory Ptraj ∈RT ×2,
162"
DIRECTION-AWARE PLANNING,0.2053903345724907,"Ptraj = PlanHead(Fego, c),
(6)
where the PlanHead is the same as UniAD [21]. We apply L1 loss to minimize the distance between
163"
DIRECTION-AWARE PLANNING,0.20631970260223048,"the predicted ego trajectory Ptraj and the ground truth Gtraj, denoted as Limi. Notably, Gtraj is easy
164"
DIRECTION-AWARE PLANNING,0.20724907063197026,"to obtain, and manual annotation is not required in practical scenarios.
165"
DIRECTION-AWARE PLANNING,0.20817843866171004,"Directional Augmentation. Observed that the training data is predominated by the go straight
166"
DIRECTION-AWARE PLANNING,0.20910780669144982,"scenarios, we propose a directional augmentation strategy to balance the distribution. As shown
167"
DIRECTION-AWARE PLANNING,0.2100371747211896,"in Fig. 4, the BEV feature Fb is rotated with different angles r ∈R = {90◦, 180◦, 270◦}, yielding
168"
DIRECTION-AWARE PLANNING,0.21096654275092938,BEV Feature/Mask F
DIRECTION-AWARE PLANNING,0.21189591078066913,"Planner F F
F"
DIRECTION-AWARE PLANNING,0.2128252788104089,Rotate
DIRECTION-AWARE PLANNING,0.2137546468401487,Planner
DIRECTION-AWARE PLANNING,0.21468401486988847,Pred Trajectory
DIRECTION-AWARE PLANNING,0.21561338289962825,Pred Trajectory
DIRECTION-AWARE PLANNING,0.21654275092936803,Consistency Loss
DIRECTION-AWARE PLANNING,0.2174721189591078,GT Trajectory
DIRECTION-AWARE PLANNING,0.2184014869888476,GT Trajectory
DIRECTION-AWARE PLANNING,0.21933085501858737,Rotate
DIRECTION-AWARE PLANNING,0.22026022304832713,"Direction
Augment
Imitation Loss"
DIRECTION-AWARE PLANNING,0.2211895910780669,Imitation Loss
DIRECTION-AWARE PLANNING,0.22211895910780668,Figure 4: Illustration of direction-aware learning strategy.
DIRECTION-AWARE PLANNING,0.22304832713754646,"the rotated representations {Fr
b}. The
169"
DIRECTION-AWARE PLANNING,0.22397769516728624,"augmented features will also be used for
170"
DIRECTION-AWARE PLANNING,0.22490706319702602,"the pretext and planning task, and super-
171"
DIRECTION-AWARE PLANNING,0.2258364312267658,"vised by the aforementioned loss func-
172"
DIRECTION-AWARE PLANNING,0.22676579925650558,"tions (e.g., Lspat). Notably, the BEV ob-
173"
DIRECTION-AWARE PLANNING,0.22769516728624536,"ject mask M and the ground truth ego
174"
DIRECTION-AWARE PLANNING,0.22862453531598512,"trajectory Gtraj are also rotated to pro-
175"
DIRECTION-AWARE PLANNING,0.2295539033457249,"vide corresponding supervision labels.
176"
DIRECTION-AWARE PLANNING,0.23048327137546468,"Furthermore, we propose an auxiliary task to enhance the steering capability. In specific, we predict
177"
DIRECTION-AWARE PLANNING,0.23141263940520446,"the planning direction that the ego car intends to maneuver (i.e., left, straight or right) based on the
178"
DIRECTION-AWARE PLANNING,0.23234200743494424,"ego query Qt
ego, which is mapped to the probabilities of three directions Pt
dir ∈R1×3. The direction
179"
DIRECTION-AWARE PLANNING,0.23327137546468402,"label Yt
dir is generated by comparing the x-axis value of ground truth Gt
traj(x) with the threshold
180"
DIRECTION-AWARE PLANNING,0.2342007434944238,"δ. Specifically, Yt
dir is assigned to straight if −δ < Gt
traj(x) < δ, otherwise Yt
dir = left/right
181"
DIRECTION-AWARE PLANNING,0.23513011152416358,"for Gt
traj(x) ⩽−δ/Gt
traj(x) ⩾δ, respectively. We use the cross-entropy loss to minimize the gap
182"
DIRECTION-AWARE PLANNING,0.23605947955390336,"between the direction prediction Pt
dir and the direction label Yt
dir, denoted as Ldir.
183"
DIRECTION-AWARE PLANNING,0.23698884758364314,"Directional Consistency. Tailored to the introduced directional augmentation, we propose a direc-
184"
DIRECTION-AWARE PLANNING,0.2379182156133829,"tional consistency loss to improve the augmented plan training in a self-supervised manner. It should
185"
DIRECTION-AWARE PLANNING,0.23884758364312267,"be noticed that the augmented trajectory predictions Pt,r
traj incorporate the same scene information as
186"
DIRECTION-AWARE PLANNING,0.23977695167286245,"the original one Pt,r=0
traj , i.e., BEV features with different rotation angles. Therefore, it’s reasonable
187"
DIRECTION-AWARE PLANNING,0.24070631970260223,"to consider the consistency among the predictions and regulate the noises caused by the rotation. The
188"
DIRECTION-AWARE PLANNING,0.241635687732342,"planning head is expected to be more robust to directional change and input distractors. Specifically,
189"
DIRECTION-AWARE PLANNING,0.2425650557620818,"Pt,r
traj are first rotated back to the original scene direction, then L1 loss is applied with Pt,r=0
traj ,
190"
DIRECTION-AWARE PLANNING,0.24349442379182157,"Lcons =
1
T ·|R|
PT
t=1
PR
r ||Rot(Pt,r
traj) −Pt,r=0
traj ||1,
(7)"
DIRECTION-AWARE PLANNING,0.24442379182156135,"where Rot is the inverse rotation.
191"
DIRECTION-AWARE PLANNING,0.24535315985130113,"To summarize, the overall objective for our UAD contains spatial objectness loss, dreaming loss from
192"
DIRECTION-AWARE PLANNING,0.24628252788104088,"the pretext, and imitation learning loss, direction loss, consistency loss from the planning task,
193"
DIRECTION-AWARE PLANNING,0.24721189591078066,"L = ω1Lspat + ω2Ldrm + ω3Limi + ω4Ldir + ω5Lcons,
(8)
where ω1, ω2, ω3, ω4, ω5 are the weight coefficients.
194"
DIRECTION-AWARE PLANNING,0.24814126394052044,"Table 1: Open-loop planning performance in nuScenes [2]. † indicates LiDAR-based method and ‡
denotes TemAvg evaluation protocol used in VAD and ST-P3 (see Eq. 9 for details). ⋄means using
ego status in the planning module and calculating collision rates following BEV-Planner [26]."
DIRECTION-AWARE PLANNING,0.24907063197026022,"Method
Tasks with 3D annotation
L2 (m) ↓
Collision (%) ↓
Intersection (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg.
1s
2s
3s
Avg.
FPS"
DIRECTION-AWARE PLANNING,0.25,"NMP† [39]
Det & Motion
-
-
2.31
-
-
-
1.92
-
-
-
-
-
-
SA-NMP† [39]
Det & Motion
-
-
2.05
-
-
-
1.59
-
-
-
-
-
-
FF† [19]
Occ
0.55
1.20
2.54
1.43
0.06
0.17
1.07
0.43
-
-
-
-
-
EO† [23]
Occ
0.67
1.36
2.78
1.60
0.04
0.09
0.88
0.33
-
-
-
-
-"
DIRECTION-AWARE PLANNING,0.25092936802973975,"ST-P3 [20]
Det & Map
1.72
3.26
4.86
3.28
0.44
1.08
3.01
1.51
2.53
8.17
14.4
8.37
1.8
UniAD [21]
Det&Track&Map&Motion&Occ
0.48
0.96
1.65
1.03
0.05
0.17
0.71
0.31
0.21
1.32
3.63
1.72
2.1
VAD-Tiny [22]
Det & Map & Motion
0.60
1.23
2.06
1.30
0.33
1.33
2.21
1.29
0.94
3.22
7.65
3.94
17.6
VAD-Base [22]
Det & Map & Motion
0.54
1.15
1.98
1.22
0.10
0.24
0.96
0.43
0.60
2.38
5.18
2.72
5.3
OccNet [36]
Det & Map & Occ
1.29
2.13
2.99
2.14
0.21
0.59
1.37
0.72
-
-
-
-
3.3
UAD-Tiny (Ours)
None
0.47
0.99
1.71
1.06
0.08
0.39
0.90
0.46
0.24
1.15
3.12
1.50
18.9
UAD (Ours)
None
0.39
0.81
1.50
0.90
0.01
0.12
0.43
0.19
0.13
0.88
2.66
1.22
7.2"
DIRECTION-AWARE PLANNING,0.25185873605947956,"ST-P3‡ [20]
Det & Map
1.33
2.11
2.90
2.11
0.23
0.62
1.27
0.71
2.53
8.17
14.4
8.37
1.8
UniAD‡ [21]
Det&Track&Map&Motion&Occ
0.44
0.67
0.96
0.69
0.04
0.08
0.23
0.12
0.21
1.32
3.63
1.72
2.1
VAD-Base‡ [22]
Det & Map & Motion
0.41
0.70
1.05
0.72
0.07
0.17
0.41
0.22
0.60
2.38
5.18
2.72
5.3
Drive-WM‡ [38]
Det & Map
0.43
0.77
1.20
0.80
0.10
0.21
0.48
0.26
-
-
-
-
-
UAD‡ (Ours)
None
0.28
0.41
0.65
0.45
0.01
0.03
0.14
0.06
0.13
0.88
2.66
1.22
7.2"
DIRECTION-AWARE PLANNING,0.2527881040892193,"UniAD‡⋄[21]
Det&Track&Map&Motion&Occ
0.20
0.42
0.75
0.46
0.02
0.25
0.84
0.37
0.20
1.33
3.24
1.59
2.1
VAD-Base‡⋄[22]
Det & Map & Motion
0.17
0.34
0.60
0.37
0.04
0.27
0.67
0.33
0.21
2.13
5.06
2.47
5.3
BEV-Planner‡⋄[26]
None
0.16
0.32
0.57
0.35
0.00
0.29
0.73
0.34
0.35
2.62
6.51
3.16
-
UAD‡⋄(Ours)
None
0.13
0.28
0.48
0.30
0.00
0.12
0.55
0.22
0.10
0.80
2.48
1.13
7.2"
EXPERIMENT,0.2537174721189591,"4
Experiment
195"
EXPERIMENTAL SETUP,0.25464684014869887,"4.1
Experimental Setup
196"
EXPERIMENTAL SETUP,0.2555762081784387,"We conduct experiments in nuScenes [2] for open-loop evaluation, that contains 40,157 samples,
197"
EXPERIMENTAL SETUP,0.25650557620817843,"of which 6,019 ones are used for evaluation. Following previous works [20, 21, 22], we adopt the
198"
EXPERIMENTAL SETUP,0.25743494423791824,"metrics of L2 error (in meters) and collision rate (in percentage). Notably, the intersection rate with
199"
EXPERIMENTAL SETUP,0.258364312267658,"road boundary (in percentage), proposed in BEV-Planner [26], is also included for evaluation. For
200"
EXPERIMENTAL SETUP,0.25929368029739774,"the closed-loop setting, we follow previous works [34, 20] to perform evaluation in the Town05 [34]
201"
EXPERIMENTAL SETUP,0.26022304832713755,"benchmark of the CARLA simulator [11]. Route completion (in percentage) and driving score (in
202"
EXPERIMENTAL SETUP,0.2611524163568773,"percentage) are used as the evaluation metrics. We adopt the query-based view transformer [25] to
203"
EXPERIMENTAL SETUP,0.2620817843866171,"learn BEV features from multi-view images. The confidence threshold of the open-set 2D detector
204"
EXPERIMENTAL SETUP,0.26301115241635686,"is set to 0.35 to filter unreliable predictions. The angle θ to partition the BEV space is set to 4◦
205"
EXPERIMENTAL SETUP,0.26394052044609667,"(K=360◦/4◦), and the default threshold δ is 1.2m (see Sec. 3.3). The weight coefficients in Eq. 8 are
206"
EXPERIMENTAL SETUP,0.2648698884758364,"set to 2.0, 0.1, 1.0, 2.0, 1.0. Our model is trained for 24 epochs on 8 NVIDIA Tesla A100 GPUs with
207"
EXPERIMENTAL SETUP,0.26579925650557623,"a batch size of 1 per GPU. Other settings follow UniAD [21] unless otherwise specified.
208"
EXPERIMENTAL SETUP,0.266728624535316,"We observed that ST-P3 [20] and VAD [22] adopt different open-loop evaluation protocols (L2 error
209"
EXPERIMENTAL SETUP,0.26765799256505574,"and collision rate) from UniAD in their official codes. We denote the setting in ST-P3 and VAD as
210"
EXPERIMENTAL SETUP,0.26858736059479554,"TemAvg and the one in UniAD as NoAvg, respectively. In specific, the TemAvg protocol calculates
211"
EXPERIMENTAL SETUP,0.2695167286245353,"metrics by averaging the performances from 0.5s to the corresponding timestamp. Taking the L2
212"
EXPERIMENTAL SETUP,0.2704460966542751,"error at 2s as an example, the calculation in TemAvg is
213"
EXPERIMENTAL SETUP,0.27137546468401486,"L2@2s = Avg(l20.5s, l21.0s, l21.5s, l22.0s),
(9)"
EXPERIMENTAL SETUP,0.27230483271375466,"where Avg is the average operation and 0.5s is the time interval between two consecutive annotated
214"
EXPERIMENTAL SETUP,0.2732342007434944,"frames in nuScenes [2]. For NoAvg protocol, L2@2s = l22.0s.
215"
COMPARISON WITH STATE-OF-THE-ARTS,0.2741635687732342,"4.2
Comparison with State-of-the-arts
216"
COMPARISON WITH STATE-OF-THE-ARTS,0.275092936802974,"Open-loop Evaluation. Tab. 1 presents the performance comparison in terms of L2 error, collision
217"
COMPARISON WITH STATE-OF-THE-ARTS,0.2760223048327137,"rate, intersection rate with road boundary, and FPS. Since ST-P3 and VAD adopt different evaluation
218"
COMPARISON WITH STATE-OF-THE-ARTS,0.27695167286245354,"protocols from UniAD to compute L2 error and collision rate (see Sec. 4.1), we respectively calculate
219"
COMPARISON WITH STATE-OF-THE-ARTS,0.2778810408921933,"the results under different settings, i.e., NoAvg and TemAvg. As shown in Tab. 1, the proposed UAD
220"
COMPARISON WITH STATE-OF-THE-ARTS,0.2788104089219331,"achieves superior planning performance over UniAD and VAD on all metrics, while running faster.
221"
COMPARISON WITH STATE-OF-THE-ARTS,0.27973977695167285,"Notably, our UAD obtains 39.4% and 55.2% relative improvements on Collision@3s compared
222"
COMPARISON WITH STATE-OF-THE-ARTS,0.28066914498141265,"with UniAD and VAD under the NoAvg evaluation protocol (e.g., 39.4%=(0.71%-0.43%)/0.71%),
223"
COMPARISON WITH STATE-OF-THE-ARTS,0.2815985130111524,"demonstrating the longtime robustness of our method. Moreover, UAD runs at 7.2FPS, which is 3.4×
224"
COMPARISON WITH STATE-OF-THE-ARTS,0.2825278810408922,"and 1.4× faster than UniAD and VAD-Base, respectively, verifying the efficiency of our framework.
225"
COMPARISON WITH STATE-OF-THE-ARTS,0.28345724907063197,"Surprisingly, our tiny version, UAD-Tiny, which aligns the settings of backbone, image size, and BEV
226"
COMPARISON WITH STATE-OF-THE-ARTS,0.2843866171003718,"Table 2: Closed-loop evaluation in the
CARLA simulator [11].
† denotes the
LiDAR-based method."
COMPARISON WITH STATE-OF-THE-ARTS,0.2853159851301115,Method
COMPARISON WITH STATE-OF-THE-ARTS,0.2862453531598513,"Town05 Short
Town05 Long
Driving
Score ↑
Route
Completion ↑Driving
Score ↑
Route
Completion ↑"
COMPARISON WITH STATE-OF-THE-ARTS,0.2871747211895911,"CILRS [8]
7.47
13.40
3.68
7.19
LBC [5]
30.97
55.01
7.05
32.09
Transfuser† [34]
54.52
78.41
33.15
56.36
ST-P3 [20]
55.14
86.74
11.45
83.15
VAD-Base [22]
64.29
87.26
30.31
75.20
UAD (Ours)
67.83
91.05
36.71
85.12"
COMPARISON WITH STATE-OF-THE-ARTS,0.28810408921933084,"Table 3: Ablation on the loss functions. We evaluate the
influence of each designed module by applying corre-
sponding loss."
COMPARISON WITH STATE-OF-THE-ARTS,0.28903345724907065,"# Lspat Ldrm Ldir Lcons Limi
L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
COMPARISON WITH STATE-OF-THE-ARTS,0.2899628252788104,"①
-
-
-
-
✓
1.20 3.04 5.30 3.18 0.83 1.33 5.13 2.43
②
✓
-
-
-
✓
0.44 0.93 1.64 1.00 0.30 0.56 1.28 0.71
③
-
✓
-
-
✓
0.51 1.12 1.97 1.20 0.71 1.13 2.71 1.52
④
-
-
✓
-
✓
0.83 1.57 2.40 1.60 0.79 1.29 3.89 1.99
⑤
-
-
-
✓
✓
0.59 1.30 2.34 1.41 0.76 1.25 3.47 1.83
⑥
✓
✓
✓
✓
✓
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19"
COMPARISON WITH STATE-OF-THE-ARTS,0.2908921933085502,Table 4: Ablation on the dreaming decoder.
COMPARISON WITH STATE-OF-THE-ARTS,0.29182156133828996,"# Circular
Update
Dreaming
Loss"
COMPARISON WITH STATE-OF-THE-ARTS,0.29275092936802977,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
COMPARISON WITH STATE-OF-THE-ARTS,0.2936802973977695,"①
-
-
0.98 1.73 2.74 1.82 0.43 0.85 1.71 1.00
②
✓
-
0.50 0.98 1.87 1.12 0.27 0.60 1.37 0.75
③
-
✓
0.44 0.96 1.73 1.04 0.08 0.35 1.13 0.52
④
✓
✓
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19"
COMPARISON WITH STATE-OF-THE-ARTS,0.29460966542750927,Table 5: Ablation on direction-aware learning strategy.
COMPARISON WITH STATE-OF-THE-ARTS,0.2955390334572491,"# Directional
Augment
Directional
Consistency"
COMPARISON WITH STATE-OF-THE-ARTS,0.29646840148698883,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
COMPARISON WITH STATE-OF-THE-ARTS,0.29739776951672864,"①
-
-
0.42 0.88 1.61 0.97 0.05 0.18 0.73 0.32
②
✓
-
0.41 0.83 1.53 0.92 0.05 0.23 0.68 0.32
③
✓
✓
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19"
COMPARISON WITH STATE-OF-THE-ARTS,0.2983271375464684,"resolution in VAD-Tiny, runs at the fastest speed of 18.9FPS while clearly outperforming VAD-Tiny
227"
COMPARISON WITH STATE-OF-THE-ARTS,0.2992565055762082,"and even achieving comparable performance with VAD-Base. This again proves the superiority of
228"
COMPARISON WITH STATE-OF-THE-ARTS,0.30018587360594795,"our design. More detailed runtime comparisons and analyses are presented in the appendix. We adopt
229"
COMPARISON WITH STATE-OF-THE-ARTS,0.30111524163568776,"the NoAvg evaluation protocol in the following ablation experiments unless otherwise specified.
230"
COMPARISON WITH STATE-OF-THE-ARTS,0.3020446096654275,"Recent works discuss the effect of using ego status in the planning module [22, 26]. Following this
231"
COMPARISON WITH STATE-OF-THE-ARTS,0.30297397769516726,"trend, we also fairly compare the ego status equipped version of our model with these works. It shows
232"
COMPARISON WITH STATE-OF-THE-ARTS,0.30390334572490707,"that the superiority of our UAD is still preserved, which also achieves the best performance against
233"
COMPARISON WITH STATE-OF-THE-ARTS,0.3048327137546468,"the compared methods. Moreover, BEV-Planner [26] introduces a new metric named “interaction”
234"
COMPARISON WITH STATE-OF-THE-ARTS,0.30576208178438663,"for better evaluating the performance of E2EAD methods. As shown in Tab. 1, our model obtains
235"
COMPARISON WITH STATE-OF-THE-ARTS,0.3066914498141264,"the average interaction rate of 1.13%, obviously outperforming other methods. This again proves
236"
COMPARISON WITH STATE-OF-THE-ARTS,0.3076208178438662,"the effectiveness of our UAD. On the other hand, this demonstrates the importance of designing a
237"
COMPARISON WITH STATE-OF-THE-ARTS,0.30855018587360594,"suitable pretext for perceiving the environment. Only using ego status is not enough for safe driving.
238"
COMPARISON WITH STATE-OF-THE-ARTS,0.30947955390334575,"Closed-loop Evaluation. The simulation results in CARLA [11] are shown in Tab. 2. Our UAD
239"
COMPARISON WITH STATE-OF-THE-ARTS,0.3104089219330855,"achieves better performance compared with recent E2E planners ST-P3 [20] and VAD [22] in all
240"
COMPARISON WITH STATE-OF-THE-ARTS,0.31133828996282525,"scenarios, proving the effectiveness. Notably, on challenging Town05 Long benchmark, UAD greatly
241"
COMPARISON WITH STATE-OF-THE-ARTS,0.31226765799256506,"outperforms recent E2E method VAD by 6.40 points on the driving score and 9.92 points on route
242"
COMPARISON WITH STATE-OF-THE-ARTS,0.3131970260223048,"completion, respectively. This proves the reliability of our UAD for long-term autonomous driving.
243"
COMPONENT-WISE ABLATION,0.3141263940520446,"4.3
Component-wise Ablation
244"
COMPONENT-WISE ABLATION,0.3150557620817844,"Loss Functions. We first analyze the influence of different loss functions that correspond to the
245"
COMPONENT-WISE ABLATION,0.3159851301115242,"proposed pretext task and self-supervised trajectory learning strategy. The experiments are conducted
246"
COMPONENT-WISE ABLATION,0.31691449814126393,"on the validation split of the nuScenes [2], as shown in Tab. 3. The model with single imitation
247"
COMPONENT-WISE ABLATION,0.31784386617100374,"loss Limi is considered as the baseline (①). With the enhanced perception capability by the spatial
248"
COMPONENT-WISE ABLATION,0.3187732342007435,"objectness loss Lspat, the average L2 error and collision rate are clearly improved to 1.00m and 0.71%
249"
COMPONENT-WISE ABLATION,0.31970260223048325,"from 3.18m and 2.43%, respectively (②v.s. ①). The dreaming loss Ldrm, direction loss Ldir and
250"
COMPONENT-WISE ABLATION,0.32063197026022305,"consistency loss Lcons also respectively bring considerable gains on the average L2 error for 1.98m,
251"
COMPONENT-WISE ABLATION,0.3215613382899628,"1.58m, 1.77m over the baseline model (③,④,⑤v.s. ①). The loss functions are finally combined to
252"
COMPONENT-WISE ABLATION,0.3224907063197026,"construct our UAD (⑥), which obtains the average L2 error of 0.90m and average collision rate of
253"
COMPONENT-WISE ABLATION,0.32342007434944237,"0.19%. The results demonstrate the effectiveness of each proposed component.
254"
COMPONENT-WISE ABLATION,0.3243494423791822,"Temporal Learning with Dreaming Decoder. The temporal learning with the proposed dreaming
255"
COMPONENT-WISE ABLATION,0.3252788104089219,"decoder is realized by Circular Update and Dreaming Loss. The circular update is in charge of both
256"
COMPONENT-WISE ABLATION,0.32620817843866173,"extracting information from observed scenes (Eq. 2) and generating pseudo observations to predict
257"
COMPONENT-WISE ABLATION,0.3271375464684015,"the ego trajectories of future frames (Eq. 3). We study the influence of each module in Tab. 4. Circular
258"
COMPONENT-WISE ABLATION,0.32806691449814124,"Update and Dreaming Loss respectively bring performance gains of 0.70m/0.78m on the average L2
259"
COMPONENT-WISE ABLATION,0.32899628252788105,"error (②,③v.s.①), proving the effectiveness of our designs. Applying both two modules (④) achieves
260"
COMPONENT-WISE ABLATION,0.3299256505576208,"the best performance, showing their complementarity for temporal representation learning.
261"
COMPONENT-WISE ABLATION,0.3308550185873606,"Direction Aware Learning Strategy. Directional Augmentation and Directional Consistency are the
262"
COMPONENT-WISE ABLATION,0.33178438661710036,"two core components of the proposed direction-aware learning strategy. We prove their effectiveness
263"
COMPONENT-WISE ABLATION,0.33271375464684017,"in Tab. 5. It shows that the Directional Augmentation improves the average L2 error for considerable
264"
COMPONENT-WISE ABLATION,0.3336431226765799,Table 7: Performances under different driving scenes. ∗denotes not using direction-aware learning.
COMPONENT-WISE ABLATION,0.3345724907063197,Method
COMPONENT-WISE ABLATION,0.3355018587360595,"Perf. go straight ↓
(5309 samples)"
COMPONENT-WISE ABLATION,0.33643122676579923,"Perf. turn left ↓
(301 samples)"
COMPONENT-WISE ABLATION,0.33736059479553904,"Perf. turn right ↓
(409 samples)"
COMPONENT-WISE ABLATION,0.3382899628252788,"Perf. Overall ↓
(6019 samples)
Avg. L2 (m)
Avg. Col. (%)
Avg. L2 (m)
Avg. Col. (%)
Avg. L2 (m)
Avg. Col. (%)
Avg. L2 (m)
Avg. Col. (%)"
COMPONENT-WISE ABLATION,0.3392193308550186,"UniAD [21]
0.98
0.26
1.48
0.55
1.27
0.73
1.03
0.31
VAD-Base [22]
1.19
0.37
1.47
0.78
1.39
0.81
1.22
0.43
UAD∗(Ours)
0.89
0.28
1.55
0.43
1.51
0.65
0.97
0.32
UAD (Ours)
0.84
0.17
1.39
0.22
1.16
0.33
0.90
0.19"
COMPONENT-WISE ABLATION,0.34014869888475835,"0.05m (②v.s.①). One interesting observation is that applying the augmentation brings more gains
265"
COMPONENT-WISE ABLATION,0.34107806691449816,"for long-term planning than short-term ones, i.e., the L2 error of 1s/3s decreases for 0.01m/0.08m
266"
COMPONENT-WISE ABLATION,0.3420074349442379,"compared with ①, which proves the effectiveness of our augmentation on enhancing longer temporal
267"
COMPONENT-WISE ABLATION,0.3429368029739777,"information. The Directional Consistency further reduces the average collision rate for impressive
268"
COMPONENT-WISE ABLATION,0.34386617100371747,"0.13% (③v.s.②), which enhances the robustness for driving directional change.
269"
COMPONENT-WISE ABLATION,0.3447955390334573,"Angular Design. We further explore the influence of the proposed angular design by removing
270"
COMPONENT-WISE ABLATION,0.34572490706319703,"the angular partition and angular queries. Specifically, the BEV feature is directly fed into the
271"
COMPONENT-WISE ABLATION,0.3466542750929368,"dreaming decoder to predict pixel-wise objectness, which is supervised by the BEV object mask
272"
COMPONENT-WISE ABLATION,0.3475836431226766,"(see Fig. 2) with binary cross-entropy loss. Besides, the ego query directly interacts with the BEV
273"
COMPONENT-WISE ABLATION,0.34851301115241634,"feature by cross-attention to extract environmental information. The results are presented in Tab. 6.
274"
COMPONENT-WISE ABLATION,0.34944237918215615,Table 6: Ablation on the angular design.
COMPONENT-WISE ABLATION,0.3503717472118959,"#
Angular
Design"
COMPONENT-WISE ABLATION,0.3513011152416357,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
COMPONENT-WISE ABLATION,0.35223048327137546,"①
-
0.78 1.31 2.01 1.37 0.61 1.39 2.12 1.37
②
✓
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19"
COMPONENT-WISE ABLATION,0.35315985130111527,"When discarding the angular design, the average L2 er-
275"
COMPONENT-WISE ABLATION,0.354089219330855,"ror degrades for 0.47m, and the average collision rate
276"
COMPONENT-WISE ABLATION,0.3550185873605948,"consistently degrades for 1.18%. This demonstrates the
277"
COMPONENT-WISE ABLATION,0.3559479553903346,"effectiveness of our angular design in perceiving com-
278"
COMPONENT-WISE ABLATION,0.35687732342007433,"plex environments and planning robust driving routes.
279"
FURTHER ANALYSIS,0.35780669144981414,"4.4
Further Analysis
280"
FURTHER ANALYSIS,0.3587360594795539,"Planning Performance in Different Driving Scenes. The direction-aware learning strategy is
281"
FURTHER ANALYSIS,0.3596654275092937,"designed to enhance the planning performance in scenarios of vehicle steering. We demonstrate the
282"
FURTHER ANALYSIS,0.36059479553903345,"superiority of our proposed model by evaluating the metrics of different driving scenes in Tab. 7.
283"
FURTHER ANALYSIS,0.36152416356877326,"According to the given driving command (i.e., go straight, turn left and turn right), we divide the
284"
FURTHER ANALYSIS,0.362453531598513,"6,019 validation samples in nuScenes [2] into three parts, which contain 5,309, 301 and 409 ones,
285"
FURTHER ANALYSIS,0.36338289962825276,"respectively. Not surprisingly, all methods perform better under go straight scenes than the steering
286"
FURTHER ANALYSIS,0.3643122676579926,"scenes, proving the necessity of augmenting the imbalanced training data for robust planning. When
287"
FURTHER ANALYSIS,0.3652416356877323,"applying the proposed direction-aware learning strategy, our UAD achieves considerable gains on
288"
FURTHER ANALYSIS,0.36617100371747213,"the average collision rate of turn left and turn right scenes (UAD v.s. UAD∗). Notably, our model
289"
FURTHER ANALYSIS,0.3671003717472119,"outperforms UniAD and VAD by a large margin in steering scenes, proving its effectiveness.
290"
FURTHER ANALYSIS,0.3680297397769517,"Visualization of Angular Perception and Planning. The angular perception pretext is designed
291"
FURTHER ANALYSIS,0.36895910780669144,"to perceive the objects in each sector region. We show its capability by visualizing the predicted
292"
FURTHER ANALYSIS,0.36988847583643125,"objectness in nuScenes [2] in Fig. 5a. For a better view, we transform the discrete objectness
293"
FURTHER ANALYSIS,0.370817843866171,"scores and ground truth to a pseudo-BEV mask. It shows that our model can successfully capture
294"
FURTHER ANALYSIS,0.37174721189591076,"surrounding objects. Fig. 5a also shows the open-loop planning results of recent SOTA UniAD [21],
295"
FURTHER ANALYSIS,0.37267657992565056,"VAD [22] and our UAD, proving the effectiveness of our method to plan a more reasonable ego
296"
FURTHER ANALYSIS,0.3736059479553903,"trajectory. Fig. 5b compares the closed-loop driving routes between Transfuser [34], ST-P3 [20] and
297"
FURTHER ANALYSIS,0.3745353159851301,"our UAD in CARLA [11]. Our method successfully notices the person and drives in a much safer
298"
FURTHER ANALYSIS,0.3754646840148699,"manner, proving the reliability of our UAD in handling safe-critical issues under complex scenarios.
299"
FURTHER ANALYSIS,0.3763940520446097,"Due to limited space, we present more analyses in the appendix, including 1) the influence of partition
300"
FURTHER ANALYSIS,0.37732342007434944,"angle θ, 2) the influence of direction threshold δ, 3) different backbones and pre-trained weights, 4)
301"
FURTHER ANALYSIS,0.37825278810408924,"replacing 2D ROIs from GroundingDINO with 2D GT boxes, 5) different settings of GroundingDINO
302"
FURTHER ANALYSIS,0.379182156133829,"to generate 2D ROIs, 6) the influence of pre-training to previous method UniAD and our UAD, 7)
303"
FURTHER ANALYSIS,0.38011152416356875,"runtime analysis of each module in our UAD and modularized UniAD, 8) more visualizations, etc.
304"
DISCUSSION,0.38104089219330856,"4.5
Discussion
305"
DISCUSSION,0.3819702602230483,"Ego Status and Open-loop Planning Evaluation. As revealed by [26, 40], it’s not a challenge to
306"
DISCUSSION,0.3828996282527881,"acquire decent performance of L2 error and collision rate (the original metrics in nuScenes [2]) in
307"
DISCUSSION,0.38382899628252787,"the open-loop evaluation of nuScenes by using ego status in the planning module (see Tab. 1). The
308"
DISCUSSION,0.3847583643122677,"question is: is open-loop evaluation meaningless? Our answer is NO. Firstly, the inherent reason for
309"
DISCUSSION,0.3856877323420074,"the observation is that the simple cases of go straight dominate the nuScenes testing dataset. In these
310"
DISCUSSION,0.38661710037174724,"(a)
(b)
Figure 5: (a) Qualitative results in nuScenes. (b) Qualitative results in CARLA."
DISCUSSION,0.387546468401487,"cases, even a linear extrapolation of motion being sufficient for planning is not surprising. However,
311"
DISCUSSION,0.38847583643122674,"as shown in Tab. 7, in more challenging cases like turn right and turn left, the open-loop metrics can
312"
DISCUSSION,0.38940520446096655,"still clearly indicate the difficulty of steering scenarios and the differences in methods, which is also
313"
DISCUSSION,0.3903345724907063,"proved in [26]. Therefore, open-loop evaluation is not meaningless, while the crux is the distribution
314"
DISCUSSION,0.3912639405204461,"of the testing data and the metrics. Secondly, the advantage of open-loop evaluation is its efficiency,
315"
DISCUSSION,0.39219330855018586,"which benefits the fast development of algorithms. This view is also revealed by a recent simulator
316"
DISCUSSION,0.39312267657992567,"design study [9], which tries to transform the closed-loop evaluation into an open-loop fashion.
317"
DISCUSSION,0.3940520446096654,"In our work, we thoroughly compare our model with other methods, which shows consistent improve-
318"
DISCUSSION,0.3949814126394052,"ments against previous works under various driving scenarios (straight or steering), different usage of
319"
DISCUSSION,0.395910780669145,"ego status (w/. or w/o.), diverse evaluation metrics (L2 error, collision rate or intersection rate from
320"
DISCUSSION,0.39684014869888473,"[26]), and different evaluation types (open- or closed-loop). It thus again proves the importance of
321"
DISCUSSION,0.39776951672862454,"designing suitable pretext tasks for end-to-end autonomous driving.
322"
DISCUSSION,0.3986988847583643,"How to Guarantee Safety in Current Auto-Drive System? Safety is the first requirement of
323"
DISCUSSION,0.3996282527881041,"autonomous driving systems in practical products, especially for L4-level auto-vehicles. To guarantee
324"
DISCUSSION,0.40055762081784385,"safety, offline collision check with predicted 3D boxes is an inevitable post-process under current
325"
DISCUSSION,0.40148698884758366,"technological conditions. Then, a question naturally arises: how to safely apply our model to
326"
DISCUSSION,0.4024163568773234,"current auto-driving systems? Before answering this question, we reaffirm our claim that we believe
327"
DISCUSSION,0.4033457249070632,"discarding 3D labels is an efficient, attractive, and potential direction for E2EAD, but it doesn’t mean
328"
DISCUSSION,0.40427509293680297,"we refuse to use any 3D labels if the relatively cheap ones are available in practical product engineering.
329"
DISCUSSION,0.4052044609665427,"For instance, solely annotating bounding boxes without object identity for tracking is much cheaper
330"
DISCUSSION,0.40613382899628253,"than labeling other elements like HD-map, and point-cloud segmentation labels for occupancy.
331"
DISCUSSION,0.4070631970260223,"Therefore, we provide a degraded version of our method by arranging an additional 3D detection head.
332"
DISCUSSION,0.4079925650557621,Table 8: Ablation on the 3D detection head.
DISCUSSION,0.40892193308550184,"#
Detection
Head"
DISCUSSION,0.40985130111524165,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
DISCUSSION,0.4107806691449814,"①
-
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19
②
✓
0.37 0.86 1.57 0.93 0.02 0.17 0.55 0.25"
DISCUSSION,0.4117100371747212,"Then our model can seamlessly integrate into auto-
333"
DISCUSSION,0.41263940520446096,"drive products, and offline collision check is achiev-
334"
DISCUSSION,0.41356877323420077,"able. As shown in Tab. 8, integrating the 3D detection
335"
DISCUSSION,0.4144981412639405,"head doesn’t bring additional improvements, which
336"
DISCUSSION,0.4154275092936803,"again proves the design of our method has sufficiently
337"
DISCUSSION,0.4163568773234201,"encoded 3D information to the planning module.
338"
DISCUSSION,0.41728624535315983,"In a nutshell, 1) our work can easily integrate other 3D tasks if they are inevitable under current
339"
DISCUSSION,0.41821561338289964,"technical conditions; 2) the experiments again prove from the side that our spatial-temporal module
340"
DISCUSSION,0.4191449814126394,"has already encoded important 3D clues for planning; 3) we hope our frontier work can eliminate
341"
DISCUSSION,0.4200743494423792,"some inessential 3D sub-tasks for both research and engineer usage of E2EAD models. An era of
342"
DISCUSSION,0.42100371747211895,"cheap, laboratory-affordable but robust, practical E2EAD design will eventually come!
343"
CONCLUSION,0.42193308550185876,"5
Conclusion
344"
CONCLUSION,0.4228624535315985,"Our work seeks to liberate E2EAD from costly modularization and 3D manual annotation. With this
345"
CONCLUSION,0.42379182156133827,"goal, we propose the unsupervised pretext task to perceive the environment by predicting angular-
346"
CONCLUSION,0.4247211895910781,"wise objectness and future dynamics. To improve the robustness in steering scenarios, we introduce
347"
CONCLUSION,0.4256505576208178,"the direction-aware training strategy for planning. Experiments demonstrate the effectiveness and
348"
CONCLUSION,0.42657992565055763,"efficiency of our method. As discussed, although the ego trajectories are easily obtained, it is almost
349"
CONCLUSION,0.4275092936802974,"impossible to collect billion-level precisely annotated data with perception labels. This impedes the
350"
CONCLUSION,0.4284386617100372,"further development of end-to-end autonomous driving. We believe our work provides a potential
351"
CONCLUSION,0.42936802973977695,"solution to this barrier and may push performance to the next level when massive data are available.
352"
REFERENCES,0.43029739776951675,"References
353"
REFERENCES,0.4312267657992565,"[1] Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence
354"
REFERENCES,0.43215613382899626,"Jackel, and Urs Muller. Explaining how a deep neural network trained with end-to-end learning steers a
355"
REFERENCES,0.43308550185873607,"car. arXiv preprint arXiv:1704.07911, 2017.
356"
REFERENCES,0.4340148698884758,"[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,
357"
REFERENCES,0.4349442379182156,"Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.
358"
REFERENCES,0.4358736059479554,"In CVPR, 2020.
359"
REFERENCES,0.4368029739776952,"[3] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A unified model to map, perceive, predict and plan.
360"
REFERENCES,0.43773234200743494,"In CVPR, 2021.
361"
REFERENCES,0.43866171003717475,"[4] Jun Cen, Peng Yun, Junhao Cai, Michael Yu Wang, and Ming Liu. Open-set 3d object detection. In 2021
362"
REFERENCES,0.4395910780669145,"International Conference on 3D Vision (3DV). IEEE, 2021.
363"
REFERENCES,0.44052044609665425,"[5] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl. Learning by cheating. In Conference
364"
REFERENCES,0.44144981412639406,"on Robot Learning, 2020.
365"
REFERENCES,0.4423791821561338,"[6] Kashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat: Neural attention fields for end-to-end
366"
REFERENCES,0.4433085501858736,"autonomous driving. In ICCV, 2021.
367"
REFERENCES,0.44423791821561337,"[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated
368"
REFERENCES,0.4451672862453532,"recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
369"
REFERENCES,0.44609665427509293,"[8] Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon. Exploring the limitations of
370"
REFERENCES,0.44702602230483274,"behavior cloning for autonomous driving. In ICCV, 2019.
371"
REFERENCES,0.4479553903345725,"[9] NAVSIM Contributors. Navsim: Data-driven non-reactive autonomous vehicle simulation. https:
372"
REFERENCES,0.44888475836431224,"//github.com/autonomousvision/navsim, 2024.
373"
REFERENCES,0.44981412639405205,"[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
374"
REFERENCES,0.4507434944237918,"image database. In CVPR. Ieee, 2009.
375"
REFERENCES,0.4516728624535316,"[11] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open
376"
REFERENCES,0.45260223048327136,"urban driving simulator. In Conference on robot learning, 2017.
377"
REFERENCES,0.45353159851301117,"[12] Zeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren, Jianyu Chen, Shengbo Eben Li, Ping Luo,
378"
REFERENCES,0.4544609665427509,"and Yanfeng Lu. Enhance sample efficiency and robustness of end-to-end urban autonomous driving via
379"
REFERENCES,0.45539033457249073,"semantic masked world model. arXiv preprint arXiv:2210.04017, 2022.
380"
REFERENCES,0.4563197026022305,"[13] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. NeurIPS, 2018.
381"
REFERENCES,0.45724907063197023,"[14] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
382"
REFERENCES,0.45817843866171004,"behaviors by latent imagination. ICLR, 2020.
383"
REFERENCES,0.4591078066914498,"[15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete
384"
REFERENCES,0.4600371747211896,"world models. ICLR, 2021.
385"
REFERENCES,0.46096654275092935,"[16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through
386"
REFERENCES,0.46189591078066916,"world models. arXiv preprint arXiv:2301.04104, 2023.
387"
REFERENCES,0.4628252788104089,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
388"
REFERENCES,0.4637546468401487,"In CVPR, 2016.
389"
REFERENCES,0.4646840148698885,"[18] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez, Corina Gurau, Hudson Yeo, Alex
390"
REFERENCES,0.4656133828996282,"Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. NeurIPS,
391"
REFERENCES,0.46654275092936803,"2022.
392"
REFERENCES,0.4674721189591078,"[19] Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion planning with
393"
REFERENCES,0.4684014869888476,"self-supervised freespace forecasting. In CVPR, 2021.
394"
REFERENCES,0.46933085501858735,"[20] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end
395"
REFERENCES,0.47026022304832715,"vision-based autonomous driving via spatial-temporal feature learning. In ECCV. Springer, 2022.
396"
REFERENCES,0.4711895910780669,"[21] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei
397"
REFERENCES,0.4721189591078067,"Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, 2023.
398"
REFERENCES,0.47304832713754646,"[22] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu,
399"
REFERENCES,0.4739776951672863,"Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving.
400"
REFERENCES,0.474907063197026,"arXiv preprint arXiv:2303.12077, 2023.
401"
REFERENCES,0.4758364312267658,"[23] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable
402"
REFERENCES,0.4767657992565056,"raycasting for self-supervised occupancy forecasting. In ECCV, 2022.
403"
REFERENCES,0.47769516728624534,"[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
404"
REFERENCES,0.47862453531598514,"Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint
405"
REFERENCES,0.4795539033457249,"arXiv:2304.02643, 2023.
406"
REFERENCES,0.4804832713754647,"[25] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai.
407"
REFERENCES,0.48141263940520446,"Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal trans-
408"
REFERENCES,0.48234200743494426,"formers. In ECCV. Springer, 2022.
409"
REFERENCES,0.483271375464684,"[26] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M Alvarez. Is ego status all you
410"
REFERENCES,0.48420074349442377,"need for open-loop end-to-end autonomous driving? In CVPR, 2024.
411"
REFERENCES,0.4851301115241636,"[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
412"
REFERENCES,0.48605947955390333,"and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2014.
413"
REFERENCES,0.48698884758364314,"[28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,
414"
REFERENCES,0.4879182156133829,"Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object
415"
REFERENCES,0.4888475836431227,"detection. arXiv preprint arXiv:2303.05499, 2023.
416"
REFERENCES,0.48977695167286245,"[29] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy
417"
REFERENCES,0.49070631970260226,"networks: Learning 3d reconstruction in function space. In CVPR, 2019.
418"
REFERENCES,0.491635687732342,"[30] Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R Qi, Xinchen Yan, Scott Ettinger, and Dragomir Anguelov.
419"
REFERENCES,0.49256505576208176,"Unsupervised 3d perception with 2d vision-language distillation for autonomous driving. In ICCV, 2023.
420"
REFERENCES,0.49349442379182157,"[31] OpenAI. Chatgpt [large language model]. https://chat.openai.com, 2023.
421"
REFERENCES,0.4944237918215613,"[32] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging
422"
REFERENCES,0.49535315985130113,"noncontrollable visual dynamics in world models. NeurIPS, 2022.
423"
REFERENCES,0.4962825278810409,"[33] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. NeurIPS, 1988.
424"
REFERENCES,0.4972118959107807,"[34] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end
425"
REFERENCES,0.49814126394052044,"autonomous driving. In CVPR, 2021.
426"
REFERENCES,0.49907063197026025,"[35] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive,
427"
REFERENCES,0.5,"predict, and plan: Safe motion planning through interpretable semantic representations. In ECCV. Springer,
428"
REFERENCES,0.5009293680297398,"2020.
429"
REFERENCES,0.5018587360594795,"[36] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping
430"
REFERENCES,0.5027881040892194,"Luo, Dahua Lin, et al. Scene as occupancy. In ICCV, 2023.
431"
REFERENCES,0.5037174721189591,"[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
432"
REFERENCES,0.5046468401486989,"Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
433"
REFERENCES,0.5055762081784386,"[38] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future:
434"
REFERENCES,0.5065055762081785,"Multiview visual forecasting and planning with world model for autonomous driving. CVPR, 2024.
435"
REFERENCES,0.5074349442379182,"[39] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun.
436"
REFERENCES,0.508364312267658,"End-to-end interpretable neural motion planner. In CVPR, 2019.
437"
REFERENCES,0.5092936802973977,"[40] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing
438"
REFERENCES,0.5102230483271375,"Ye, and Jingdong Wang. Rethinking the open-loop evaluation of end-to-end autonomous driving in nuscenes.
439"
REFERENCES,0.5111524163568774,"arXiv preprint arXiv:2305.10430, 2023.
440"
REFERENCES,0.5120817843866171,"A
Appendix
441"
REFERENCES,0.5130111524163569,"The appendix presents additional designing and explaining details of our Unpervised pretext task for
442"
REFERENCES,0.5139405204460966,"end-to-end Autonomous Driving (UAD) in the manuscript.
443"
REFERENCES,0.5148698884758365,"• Different Partition Angles
444"
REFERENCES,0.5157992565055762,"We explore the influence of different partition angles in angular pretext to learn better
445"
REFERENCES,0.516728624535316,"spatio-temporal knowledge.
446"
REFERENCES,0.5176579925650557,"• Different Direction Thresholds
447"
REFERENCES,0.5185873605947955,"We explore the influence of different thresholds in direction prediction to enhance planning
448"
REFERENCES,0.5195167286245354,"robustness in complex driving scenarios.
449"
REFERENCES,0.5204460966542751,"• Different Backbones and Pre-trained Weights
450"
REFERENCES,0.5213754646840149,"We compare the performance of different backbones and pre-trained weights on our method.
451"
REFERENCES,0.5223048327137546,"• Objectness Label Generation with GT Boxes
452"
REFERENCES,0.5232342007434945,"We compare the generated objectness label between using the pseudo ROIs from Ground-
453"
REFERENCES,0.5241635687732342,"ingDINO [28] and ground-truth boxes on different backbones.
454"
REFERENCES,0.525092936802974,"• Settings for ROI Generation
455"
REFERENCES,0.5260223048327137,"We ablate different settings for the open-set 2D detector GroundingDINO, which provides
456"
REFERENCES,0.5269516728624535,"ROIs for the label generation of angular perception pretext.
457"
REFERENCES,0.5278810408921933,"• Different Image Sizes and BEV Resolution
458"
REFERENCES,0.5288104089219331,"We compare the performance with different input sizes of multi-view images and BEV
459"
REFERENCES,0.5297397769516728,"resolutions.
460"
REFERENCES,0.5306691449814126,"• Runtime Analysis
461"
REFERENCES,0.5315985130111525,"We evaluate the runtime of each module of UAD and compare with modularized UniAD [21],
462"
REFERENCES,0.5325278810408922,"which demonstrates the efficiency of our method.
463"
REFERENCES,0.533457249070632,"• Classification of Angular Perception
464"
REFERENCES,0.5343866171003717,"We evaluate the objectness prediction in the angular perception pretext, which demonstrates
465"
REFERENCES,0.5353159851301115,"the enhanced perception capability in complex driving scenarios.
466"
REFERENCES,0.5362453531598513,"• Influence of Pre-training
467"
REFERENCES,0.5371747211895911,"We evaluate the influence of pre-training by detailing the training losses and planning
468"
REFERENCES,0.5381040892193308,"performances with different pre-trained weights.
469"
REFERENCES,0.5390334572490706,"• More Visualizations
470"
REFERENCES,0.5399628252788105,"We provide more visualizations for the predicted angular-wise objectness and planning re-
471"
REFERENCES,0.5408921933085502,"sults in the open-loop evaluation of nuScenes [2] and closed-loop simulation of CARLA [11].
472"
REFERENCES,0.54182156133829,"A.1
Different Partition Angles
473"
REFERENCES,0.5427509293680297,"The proposed angular perception pretext divides the BEV space into multiple sectors. We explore the
474"
REFERENCES,0.5436802973977695,"influence of partition angle θ in Tab 9. Experimental results show that the L2 error and inference
475"
REFERENCES,0.5446096654275093,"speed gradually increase with the partition angle. The model with partition angle of 1◦(①) achieves
476"
REFERENCES,0.5455390334572491,"the best average L2 error of 0.85m. And the partition angle of 4◦contributes to the best average
477"
REFERENCES,0.5464684014869888,"collision rate of 0.19% (③). This reveals that a smaller partition angle helps learn more fine-grained
478"
REFERENCES,0.5473977695167286,"environmental representations, eventually benefiting planning. In contrast, the model with a large
479"
REFERENCES,0.5483271375464684,"partition angle sparsely perceives the scene. Despite reducing the computation cost, it will also
480"
REFERENCES,0.5492565055762082,"degrade the safety of the end-to-end autonomous driving system.
481"
REFERENCES,0.550185873605948,"Table 9: Ablation on different partition angles
in the proposed angular pretext."
REFERENCES,0.5511152416356877,"# Partition
Angle"
REFERENCES,0.5520446096654275,"L2 (m) ↓
Collision (%) ↓
FPS
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.5529739776951673,"①
1◦
0.35 0.78 1.42 0.85 0.01 0.28 0.68 0.32
5.0
②
2◦
0.34 0.77 1.46 0.86 0.01 0.22 0.48 0.24
6.3
③
4◦
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19
7.2
④
8◦
0.38 0.85 1.55 0.93 0.01 0.18 0.55 0.25
7.7
⑤
15◦
0.47 0.94 1.69 1.03 0.03 0.20 0.60 0.28
8.1
⑥
30◦
0.48 1.00 1.75 1.08 0.05 0.28 0.63 0.32
8.4"
REFERENCES,0.5539033457249071,"Table 10: Ablation on different thresholds of direc-
tion prediction in the directional augmentation."
REFERENCES,0.5548327137546468,"# Threshold
(m)"
REFERENCES,0.5557620817843866,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.5566914498141264,"①
0.5
0.35 0.79 1.43 0.86 0.03 0.18 0.71 0.31
②
0.8
0.35 0.77 1.46 0.86 0.01 0.12 0.68 0.27
③
1.2
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19
④
1.5
0.40 0.82 1.52 0.91 0.02 0.15 0.42 0.20
⑤
2.0
0.38 0.85 1.55 0.93 0.01 0.08 0.48 0.19"
REFERENCES,0.5576208178438662,Table 11: Ablation on different backbones and pre-trained weights.
REFERENCES,0.5585501858736059,"# Backbone Pretrained
Weight"
REFERENCES,0.5594795539033457,"L2 (m) ↓
Collision (%) ↓
FPS
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.5604089219330854,"①
Res50
None
0.43 0.94 1.65 1.01 0.03 0.37 0.86 0.42
9.6
②
ImageNet 0.41 0.90 1.66 0.99 0.03 0.32 0.80 0.38 ③"
REFERENCES,0.5613382899628253,Res101
REFERENCES,0.5622676579925651,"None
0.40 0.87 1.59 0.95 0.02 0.23 0.59 0.28"
REFERENCES,0.5631970260223048,"7.2
④
ImageNet 0.37 0.84 1.53 0.91 0.01 0.18 0.50 0.23
⑤
COCO
0.36 0.83 1.51 0.90 0.01 0.16 0.45 0.21
⑥
NuImages 0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19"
REFERENCES,0.5641263940520446,Table 12: Ablation on 2D object boxes in pretext label generation.
REFERENCES,0.5650557620817844,"# Backbone 2D Object
Box"
REFERENCES,0.5659851301115242,"L2 (m) ↓
Collision (%) ↓
FPS
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.5669144981412639,"①
Res50
Pseudo
0.41 0.90 1.66 0.99 0.03 0.32 0.80 0.38
9.6
②
GT
0.41 0.87 1.61 0.96 0.03 0.30 0.71 0.35"
REFERENCES,0.5678438661710037,"③
Res101
Pseudo
0.39 0.81 1.50 0.90 0.01 0.12 0.43 0.19
7.2
④
GT
0.37 0.79 1.45 0.84 0.01 0.13 0.39 0.18"
REFERENCES,0.5687732342007435,"A.2
Different Direction Thresholds
482"
REFERENCES,0.5697026022304833,"The direction prediction that the ego car intends to maneuver (i.e., left, straight and right) is proposed
483"
REFERENCES,0.570631970260223,"to enhance the steering capability for autonomous driving. The label is generated with the threshold δ
484"
REFERENCES,0.5715613382899628,"(see Eq. 7 in the manuscript), which determines the ground-truth direction of each waypoint in the
485"
REFERENCES,0.5724907063197026,"expert trajectory. Here we explore the influence by ablating different thresholds, as shown in Tab. 10.
486"
REFERENCES,0.5734200743494424,"Experimental results show that the L2 error gradually increases with the direction threshold. The
487"
REFERENCES,0.5743494423791822,"model with δ of 0.5m (①) achieves the lowest L2 error of 0.86m. It reveals that a smaller threshold
488"
REFERENCES,0.5752788104089219,"will force the planner to fit the expert navigation, leading to a closer distance between the predicted
489"
REFERENCES,0.5762081784386617,"trajectory and the ground truth. In contrast, the collision rate benefits more from larger thresholds.
490"
REFERENCES,0.5771375464684015,"The model with δ of 2.0m obtains the best collision rate at 2s of 0.08% (⑤), showing the effectiveness
491"
REFERENCES,0.5780669144981413,"for robust planning. Notably, the threshold of 1.2m contributes to a great balance with the average L2
492"
REFERENCES,0.578996282527881,"error of 0.90m and average collision rate of 0.19%.
493"
REFERENCES,0.5799256505576208,"A.3
Different Backbones and Pre-trained Weights
494"
REFERENCES,0.5808550185873605,"As a common sense, pre-training the backbone network with fundamental tasks like image classi-
495"
REFERENCES,0.5817843866171004,"fication on ImageNet [10] will benefit the sub-tasks. The previous method UniAD [21] uses the
496"
REFERENCES,0.5827137546468402,"pre-trained weights of BEVFormer [25]. What surprised us is that when replacing the pre-trained
497"
REFERENCES,0.5836431226765799,"weights with the one learned on ImageNet, the performance of UniAD dramatically degraded (see
498"
REFERENCES,0.5845724907063197,"“Influence of Pre-training” for more details). This inspires us to explore the influence of backbone
499"
REFERENCES,0.5855018587360595,"settings on our framework. As shown in Tab. 11, interestingly, even without any pre-training, our
500"
REFERENCES,0.5864312267657993,"model still outperforms UniAD with pre-trained ResNet101 and VAD with pre-trained ResNet50.
501"
REFERENCES,0.587360594795539,"This verifies the effectiveness of our unsupervised pretext task on modeling the driving scenes. We
502"
REFERENCES,0.5882899628252788,"also use publicly available pre-trained weights on detection datasets like COCO [27] and nuImages [2]
503"
REFERENCES,0.5892193308550185,"to train our model, which shows better performance. These experimental results and observations
504"
REFERENCES,0.5901486988847584,"demonstrate that a potentially promising topic is how to pre-train a model for end-to-end autonomous
505"
REFERENCES,0.5910780669144982,"driving. We leave this to future research.
506"
REFERENCES,0.5920074349442379,"A.4
Objectness Label Generation with GT Boxes
507"
REFERENCES,0.5929368029739777,"As mentioned in the manuscript, the essence of generating the angular objectness label lies in the
508"
REFERENCES,0.5938661710037175,"2D ROIs, which come from the open-set 2D detector GroundingDINO [28]. Here we explore the
509"
REFERENCES,0.5947955390334573,"influence of using the ground-truth 2D boxes as ROIs, which provide more high-quality samples
510"
REFERENCES,0.595724907063197,"for the representation learning in the angular perception pretext. Tab. 12 shows that training with
511"
REFERENCES,0.5966542750929368,"GT boxes achieves consistent performance gains on both ResNet50 [17] and ResNet101 [17] (②,④
512"
REFERENCES,0.5975836431226765,"v.s. ①,③). This reveals that accurate annotation does help to learn better spatio-temporal knowledge
513"
REFERENCES,0.5985130111524164,"and improve ego planning. Considering the cost in real-world deployment, training with accessible
514"
REFERENCES,0.5994423791821561,"Table 13: Ablation on the settings of ROI generation. The Conf. Thresh denotes the confidence
threshold in GroundingDINO [28] to filter unreliable predictions. vehicle,pedestrian,barrier represent
the used prompt words to obtain ROIs of corresponding classes. Rule Filter indicates filtering the
ROIs that are more than half of the length or width of the image."
REFERENCES,0.6003717472118959,"#
Conf.
Thresh
Prompt
Words
Rule
Filter"
REFERENCES,0.6013011152416357,"L2 (m) ↓
Collision (%) ↓
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.6022304832713755,"①
0.35
{vehicle}
-
0.48
0.98
1.75
1.07
0.08
0.38
0.80
0.42
②
0.35
{vehicle,pedestrian}
-
0.47
0.94
1.69
1.03
0.04
0.27
0.71
0.34
③
0.35
{vehicle,pedestrian,barrier}
-
0.43
0.88
1.60
0.97
0.03
0.23
0.60
0.29
④
0.35
{vehicle,pedestrian,barrier}
✓
0.39
0.81
1.50
0.90
0.01
0.12
0.43
0.19
⑤
0.30
{vehicle,pedestrian,barrier}
✓
0.39
0.82
1.45
0.89
0.01
0.21
0.51
0.24
⑥
0.40
{vehicle,pedestrian,barrier}
✓
0.46
0.90
1.57
0.98
0.01
0.13
0.37
0.17"
REFERENCES,0.6031598513011153,"Table 14: Comparison with different backbones, image sizes and BEV resolutions."
REFERENCES,0.604089219330855,"#
Method
Backbone
Image
Size"
REFERENCES,0.6050185873605948,"BEV
Resolution"
REFERENCES,0.6059479553903345,"L2 (m) ↓
Collision (%) ↓
FPS
1s
2s
3s
Avg.
1s
2s
3s
Avg."
REFERENCES,0.6068773234200744,"①
UniAD [21]
R101
1600×900
200×200
0.48
0.96
1.65
1.03
0.05
0.17
0.71
0.31
2.1"
REFERENCES,0.6078066914498141,"②
VAD-Tiny [22]
R50
640×360
100×100
0.60
1.23
2.06
1.30
0.33
1.33
2.21
1.29
17.6
③
VAD-Base [22]
R50
1280×720
200×200
0.54
1.15
1.98
1.22
0.10
0.24
0.96
0.43
5.3"
REFERENCES,0.6087360594795539,"④
UAD (Ours)
R50
640×360
100×100
0.47
0.99
1.71
1.06
0.08
0.39
0.90
0.46
18.9
⑤
UAD (Ours)
R50
1600×900
200×200
0.41
0.90
1.66
0.99
0.03
0.32
0.80
0.38
9.6
⑥
UAD (Ours)
R101
1600×900
200×200
0.39
0.81
1.50
0.90
0.01
0.12
0.43
0.19
7.2"
REFERENCES,0.6096654275092936,"pseudo labels is a more efficient way compared with the manual annotation, which also shows
515"
REFERENCES,0.6105947955390335,"comparable performance in autonomous driving (①v.s. ②and ③v.s. ④).
516"
REFERENCES,0.6115241635687733,"A.5
Settings for ROI Generation.
517"
REFERENCES,0.612453531598513,"The quality of learned spatio-temporal knowledge highly relies on the generated ROIs by the open-set
518"
REFERENCES,0.6133828996282528,"2D detector GroundingDINO [28], which are then projected as the BEV objectness label for training
519"
REFERENCES,0.6143122676579925,"the angular perception pretext. We explore the influence of generated ROIs with different settings,
520"
REFERENCES,0.6152416356877324,"as shown in Tab. 13. We take the setting with the confidence score of 0.35, prompt word of vehicle
521"
REFERENCES,0.6161710037174721,"and without the Rule Filter, as the baseline (①). By appending more prompt words (e.g., pedestrian,
522"
REFERENCES,0.6171003717472119,"barrier), the planning performance gradually improves (③,②v.s.①), showing the enhanced perception
523"
REFERENCES,0.6180297397769516,"capability with more diversified objects. Filtering the ROIs with overlarge size (i.e., Rule Filter)
524"
REFERENCES,0.6189591078066915,"brings considerable gains for the average L2 error of 0.07m and average collision rate of 0.10%
525"
REFERENCES,0.6198884758364313,"(④v.s.③). One interesting observation is that decreasing the confidence threshold would slightly
526"
REFERENCES,0.620817843866171,"improve the L2 error while causing higher collision rate (⑤v.s.④). In contrast, increasing the threshold
527"
REFERENCES,0.6217472118959108,"obtains lower average collision rate of 0.17% and higher average L2 error of 0.98m. This reveals the
528"
REFERENCES,0.6226765799256505,"importance of providing diversified ROIs for angular perception learning as well as ensuring high
529"
REFERENCES,0.6236059479553904,"quality. The model with the confidence score of 0.35, all prompt words and Rule Filter achieves
530"
REFERENCES,0.6245353159851301,"balanced performance with the average L2 error of 0.90m and average collision rate of 0.19%.
531"
REFERENCES,0.6254646840148699,"A.6
Different Image Sizes and BEV Resolution
532"
REFERENCES,0.6263940520446096,"For safe autonomous driving, increasing the input size of the multi-view images and the resolution
533"
REFERENCES,0.6273234200743495,"of the built BEV representation is an effective way, which provide more detailed environmental
534"
REFERENCES,0.6282527881040892,"information. While benefiting perception and planning, it inevitably brings heavy computation cost.
535"
REFERENCES,0.629182156133829,"We then ablate the image size and BEV resolution of our UAD to find a balanced version between
536"
REFERENCES,0.6301115241635687,"performance and efficiency, as shown in Tab. 14. The results show that our UAD with ResNet-
537"
REFERENCES,0.6310408921933085,"101 [17], image size of 1600×900, BEV resolution of 200×200, achieves the best performance
538"
REFERENCES,0.6319702602230484,"compared with previous methods UniAD [21] and VAD-Base [22] while running faster with 7.2FPS
539"
REFERENCES,0.6328996282527881,"(⑥). By replacing the backbone with ResNet-50, our UAD is more efficient with little performance
540"
REFERENCES,0.6338289962825279,"degradation (⑤v.s. ⑥). We further align the settings of VAD-Tiny, which has an inference speed
541"
REFERENCES,0.6347583643122676,"of outstanding 17.6FPS (②), to explore the influence of much smaller input sizes. Tab. 14 shows
542"
REFERENCES,0.6356877323420075,"that our UAD still achieves excellent performance even compared with VAD-Base of high-resolution
543"
REFERENCES,0.6366171003717472,"inputs (④v.s. ③). Notably, our UAD of this version has the fastest inference speed of 18.9FPS. This
544"
REFERENCES,0.637546468401487,"Table 15: Module runtime comparison between
UniAD [21] and our UAD. The inference is
measured on an NVIDIA Tesla A100 GPU."
REFERENCES,0.6384758364312267,"Model
Partition"
REFERENCES,0.6394052044609665,"UniAD
UAD (Ours)"
REFERENCES,0.6403345724907064,"Module
Latency
(ms)
Proportion
(%)
Module
Latency
(ms)
Proportion
(%)"
REFERENCES,0.6412639405204461,"Feature
Extraction"
REFERENCES,0.6421933085501859,"Backbone
38.1 ±0.5
8,2%
Backbone 36.0 ±0.3
26.0%
BEV
Encoder
83.4 ±0.5
17.9%
BEV
Encoder
81.5 ±0.4
58.9%"
REFERENCES,0.6431226765799256,"Det&Track 145.3 ±1.3
31.2%
Map
92.1 ±0.7
19.8%"
REFERENCES,0.6440520446096655,"Angular
Partition
1.1 ±0.1
0.8%"
REFERENCES,0.6449814126394052,"Motion
50.6 ±0.6
10.9%
Sub-
Task"
REFERENCES,0.645910780669145,"Occupancy 45.9 ±0.4
9.9%"
REFERENCES,0.6468401486988847,Dreaming
REFERENCES,0.6477695167286245,"Decoder
18.2 ±0.2
13.2%"
REFERENCES,0.6486988847583643,"Prediction
Planning
Head
9.7 ±0.3
2.1%
Planning
Head
1.5 ±0.1
1.1%"
REFERENCES,0.6496282527881041,"Total
-
465.1 ±4.3
100%
-
138.3 ±1.1
100.0%"
REFERENCES,0.6505576208178439,"Figure 6: Visualization of the PR and ROC curves
for the angular-wise objectness prediction in differ-
ent driving scenes."
REFERENCES,0.6514869888475836,"(a)
(b)
Figure 7: Optimization of UniAD (a) and our UAD (b) with different pre-trained backbone weights."
REFERENCES,0.6524163568773235,"again proves the effectiveness of our method in performing fine-grained perception, as well as the
545"
REFERENCES,0.6533457249070632,"robustness to fit the inputs of different sizes.
546"
REFERENCES,0.654275092936803,"A.7
Runtime Analysis
547"
REFERENCES,0.6552044609665427,"Tab. 15 compares the runtime of each module between the modularized method UniAD [21] and
548"
REFERENCES,0.6561338289962825,"our UAD. As we adopt the Backbone and BEV Encoder from BEVFormer [25] that are the same in
549"
REFERENCES,0.6570631970260223,"UniAD, the latency of feature extraction is similar with little difference due to different pre-processing.
550"
REFERENCES,0.6579925650557621,"The modular sub-tasks in UniAD consume most of the runtime, i.e., significant 71.8% for Det&Track
551"
REFERENCES,0.6589219330855018,"(31.2%), Map (19.8%), Motion (10.9%) and Occupancy (9.9%), respectively. In contrast, our UAD
552"
REFERENCES,0.6598513011152416,"performs simple Angular Partition and Dreaming Decoder, which take only 14.0% (19.3ms) to model
553"
REFERENCES,0.6607806691449815,"the complex environment. This demonstrates our insight that it’s a necessity to liberate end-to-end
554"
REFERENCES,0.6617100371747212,"autonomous driving from costly modularization. The downstream Planning Head takes negligible
555"
REFERENCES,0.662639405204461,"1.5ms to plan the ego trajectory, compared with 9.7ms in UniAD. Finally, our UAD finishes the
556"
REFERENCES,0.6635687732342007,"inference with a total runtime of 138.3ms, 3.4× faster than the 465.1ms of UniAD, showing the
557"
REFERENCES,0.6644981412639405,"efficiency of our design.
558"
REFERENCES,0.6654275092936803,"A.8
Classification of Angular Perception
559"
REFERENCES,0.6663568773234201,"The proposed angular perception pretext learns spatio-temporal knowledge of the driving scene
560"
REFERENCES,0.6672862453531598,"by predicting the objectness of each sector region, which is supervised by the generated binary
561"
REFERENCES,0.6682156133828996,"angular-wise label. We show the perception ability by evaluating the classification metrics based on
562"
REFERENCES,0.6691449814126395,"the validation split of the nuScenes [2] dataset. Fig. 6 draws the Precision-Recall (PR) curve and
563"
REFERENCES,0.6700743494423792,"Receiver-Operating-Characteristic (ROC) curve in different driving scenes (i.e., turn left, go straight
564"
REFERENCES,0.671003717472119,"and turn right). In the PR curve, our UAD achieves balanced precision and recall scores in different
565"
REFERENCES,0.6719330855018587,"driving scenes, showing the effectiveness of our pretext task to perceive the surrounding objects.
566"
REFERENCES,0.6728624535315985,"Notably, the performance of go straight scenes is slightly better than the steering ones under all
567"
REFERENCES,0.6737918215613383,"thresholds. This proves our insight to design tailored direction-aware learning strategy for improving
568"
REFERENCES,0.6747211895910781,"the safety-critical turn left and turn right scenes. The ROC curve shows the robustness of our angular
569"
REFERENCES,0.6756505576208178,"perception pretext to classify the objects from complex environmental observations.
570"
REFERENCES,0.6765799256505576,Figure 8: Visualization of the angular perception.
REFERENCES,0.6775092936802974,"Figure 9: Visualization of the planning results. The first two rows show the success of our method in
safe planning in complex scenarios, while the third row exhibits a failure case of our planner when no
temporal information could be acquired when t=0."
REFERENCES,0.6784386617100372,"A.9
Influence of Pre-training
571"
REFERENCES,0.679368029739777,"Pre-training the backbone network with fundamental tasks is a commonly used metric to benefit
572"
REFERENCES,0.6802973977695167,"representation learning. As mentioned in “Different Backbones and Pre-trained Weights” of Sec. 4.4
573"
REFERENCES,0.6812267657992565,"in the manuscript, the performance of the previous SOTA method UniAD [21] dramatically degrades
574"
REFERENCES,0.6821561338289963,"without the pre-trained weights from BEVFormer [25]. Here we further detail the influence by
575"
REFERENCES,0.6830855018587361,"comparing the training losses and planning performances with different pre-trained weights in Fig. 7.
576"
REFERENCES,0.6840148698884758,"Fig. 7a shows that the training losses increase by about 20 on average when replaced with the
577"
REFERENCES,0.6849442379182156,"pre-trained weights from ImageNet [10]. Correspondingly, the average L2 error is significantly higher
578"
REFERENCES,0.6858736059479554,"than the one with the pre-trained weights from BEVFormer. This reveals that UniAD heavily relies
579"
REFERENCES,0.6868029739776952,"on the perceptive pre-training in BEVFormer to optimize modularized sub-tasks. In contrast, our
580"
REFERENCES,0.6877323420074349,"UAD performs comparably even without any pre-training (see Fig. 7b), proving the effectiveness of
581"
REFERENCES,0.6886617100371747,"our designs for robust optimization.
582"
REFERENCES,0.6895910780669146,Figure 10: Visualization of angular perception and planning in Carla.
REFERENCES,0.6905204460966543,"A.10
More Visualizations
583"
REFERENCES,0.6914498141263941,"Open-loop Planning
We provide more visualizations about the predicted angular-wise objectness
584"
REFERENCES,0.6923791821561338,"and planning results on nuScenes [2]. Fig. 8 compares the discrete objectness scores and ground
585"
REFERENCES,0.6933085501858736,"truth, proving the effectiveness of our angular perception pretext to perceive the objects in each sector
586"
REFERENCES,0.6942379182156134,"region. The planning results of previous SOTA methods (i.e., UniAD [21] and VAD [22]) and our
587"
REFERENCES,0.6951672862453532,"UAD are shown in Fig. 9. With the designed pretext and tailored training strategy, our method could
588"
REFERENCES,0.6960966542750929,"plan a more reasonable ego trajectory under different driving scenarios, proving the effectiveness
589"
REFERENCES,0.6970260223048327,"of our work. The third row shows the failure case of our planner. In this case, the ego car is given
590"
REFERENCES,0.6979553903345725,"the “Turn Right” command when t = 0 (i.e., the first frame of the driving scenario), leading to
591"
REFERENCES,0.6988847583643123,"ineffectiveness of our planner in learning helpful temporal information. A possible solution to deal
592"
REFERENCES,0.699814126394052,"with this is to apply an auxiliary trajectory prior for the first several frames, and we leave this to
593"
REFERENCES,0.7007434944237918,"future work.
594"
REFERENCES,0.7016728624535316,"Closed-loop Simulation
Fig. 10 visualizes the predicted objectness and planning results in the
595"
REFERENCES,0.7026022304832714,"Town05 Long benchmark of CARLA [11]. Following the setting of ST-P3 [20] in closed-loop evalua-
596"
REFERENCES,0.7035315985130112,"tion, we collect visual observations from the cameras of “CAM_FRONT”, “CAM_FRONT_LEFT”,
597"
REFERENCES,0.7044609665427509,"“CAM_FRONT_RIGHT” and “CAM_BACK”. It shows that the sector regions in which the surround-
598"
REFERENCES,0.7053903345724907,"ing objects exist are successfully captured by our UAD, proving the effectiveness and robustness of
599"
REFERENCES,0.7063197026022305,"our design. Notably, the missed objects by GroundingDINO [28], e.g., the black car in the camera of
600"
REFERENCES,0.7072490706319703,"“CAM_FRONT_LEFT” at t = 145, are surprisingly perceived and marked in the corresponding sector.
601"
REFERENCES,0.70817843866171,"This demonstrates our method has the capability of learning perceptive knowledge in a data-driven
602"
REFERENCES,0.7091078066914498,"manner, even with coarse supervision by the generated 2D pseudo boxes from GroundingDINO.
603"
REFERENCES,0.7100371747211895,"NeurIPS Paper Checklist
604"
CLAIMS,0.7109665427509294,"1. Claims
605"
CLAIMS,0.7118959107806692,"Question: Do the main claims made in the abstract and introduction accurately reflect the
606"
CLAIMS,0.7128252788104089,"paper’s contributions and scope?
607"
CLAIMS,0.7137546468401487,"Answer: [Yes]
608"
CLAIMS,0.7146840148698885,"Justification: The main claims made in the abstract and introduction accurately reflect the
609"
CLAIMS,0.7156133828996283,"paper’s contributions and scope, please see Sec. 4.
610"
CLAIMS,0.716542750929368,"Guidelines:
611"
CLAIMS,0.7174721189591078,"• The answer NA means that the abstract and introduction do not include the claims
612"
CLAIMS,0.7184014869888475,"made in the paper.
613"
CLAIMS,0.7193308550185874,"• The abstract and/or introduction should clearly state the claims made, including the
614"
CLAIMS,0.7202602230483272,"contributions made in the paper and important assumptions and limitations. A No or
615"
CLAIMS,0.7211895910780669,"NA answer to this question will not be perceived well by the reviewers.
616"
CLAIMS,0.7221189591078067,"• The claims made should match theoretical and experimental results, and reflect how
617"
CLAIMS,0.7230483271375465,"much the results can be expected to generalize to other settings.
618"
CLAIMS,0.7239776951672863,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
619"
CLAIMS,0.724907063197026,"are not attained by the paper.
620"
LIMITATIONS,0.7258364312267658,"2. Limitations
621"
LIMITATIONS,0.7267657992565055,"Question: Does the paper discuss the limitations of the work performed by the authors?
622"
LIMITATIONS,0.7276951672862454,"Answer: [Yes]
623"
LIMITATIONS,0.7286245353159851,"Justification: The limitations of the work are discussed in this paper, please see Sec. 4.5.
624"
LIMITATIONS,0.7295539033457249,"Guidelines:
625"
LIMITATIONS,0.7304832713754646,"• The answer NA means that the paper has no limitation while the answer No means that
626"
LIMITATIONS,0.7314126394052045,"the paper has limitations, but those are not discussed in the paper.
627"
LIMITATIONS,0.7323420074349443,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
628"
LIMITATIONS,0.733271375464684,"• The paper should point out any strong assumptions and how robust the results are to
629"
LIMITATIONS,0.7342007434944238,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
630"
LIMITATIONS,0.7351301115241635,"model well-specification, asymptotic approximations only holding locally). The authors
631"
LIMITATIONS,0.7360594795539034,"should reflect on how these assumptions might be violated in practice and what the
632"
LIMITATIONS,0.7369888475836431,"implications would be.
633"
LIMITATIONS,0.7379182156133829,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
634"
LIMITATIONS,0.7388475836431226,"only tested on a few datasets or with a few runs. In general, empirical results often
635"
LIMITATIONS,0.7397769516728625,"depend on implicit assumptions, which should be articulated.
636"
LIMITATIONS,0.7407063197026023,"• The authors should reflect on the factors that influence the performance of the approach.
637"
LIMITATIONS,0.741635687732342,"For example, a facial recognition algorithm may perform poorly when image resolution
638"
LIMITATIONS,0.7425650557620818,"is low or images are taken in low lighting. Or a speech-to-text system might not be
639"
LIMITATIONS,0.7434944237918215,"used reliably to provide closed captions for online lectures because it fails to handle
640"
LIMITATIONS,0.7444237918215614,"technical jargon.
641"
LIMITATIONS,0.7453531598513011,"• The authors should discuss the computational efficiency of the proposed algorithms
642"
LIMITATIONS,0.7462825278810409,"and how they scale with dataset size.
643"
LIMITATIONS,0.7472118959107806,"• If applicable, the authors should discuss possible limitations of their approach to
644"
LIMITATIONS,0.7481412639405205,"address problems of privacy and fairness.
645"
LIMITATIONS,0.7490706319702602,"• While the authors might fear that complete honesty about limitations might be used by
646"
LIMITATIONS,0.75,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
647"
LIMITATIONS,0.7509293680297398,"limitations that aren’t acknowledged in the paper. The authors should use their best
648"
LIMITATIONS,0.7518587360594795,"judgment and recognize that individual actions in favor of transparency play an impor-
649"
LIMITATIONS,0.7527881040892194,"tant role in developing norms that preserve the integrity of the community. Reviewers
650"
LIMITATIONS,0.7537174721189591,"will be specifically instructed to not penalize honesty concerning limitations.
651"
THEORY ASSUMPTIONS AND PROOFS,0.7546468401486989,"3. Theory Assumptions and Proofs
652"
THEORY ASSUMPTIONS AND PROOFS,0.7555762081784386,"Question: For each theoretical result, does the paper provide the full set of assumptions and
653"
THEORY ASSUMPTIONS AND PROOFS,0.7565055762081785,"a complete (and correct) proof?
654"
THEORY ASSUMPTIONS AND PROOFS,0.7574349442379182,"Answer: [NA]
655"
THEORY ASSUMPTIONS AND PROOFS,0.758364312267658,"Justification: This paper does not include theoretical results.
656"
THEORY ASSUMPTIONS AND PROOFS,0.7592936802973977,"Guidelines:
657"
THEORY ASSUMPTIONS AND PROOFS,0.7602230483271375,"• The answer NA means that the paper does not include theoretical results.
658"
THEORY ASSUMPTIONS AND PROOFS,0.7611524163568774,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
659"
THEORY ASSUMPTIONS AND PROOFS,0.7620817843866171,"referenced.
660"
THEORY ASSUMPTIONS AND PROOFS,0.7630111524163569,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
661"
THEORY ASSUMPTIONS AND PROOFS,0.7639405204460966,"• The proofs can either appear in the main paper or the supplemental material, but if
662"
THEORY ASSUMPTIONS AND PROOFS,0.7648698884758365,"they appear in the supplemental material, the authors are encouraged to provide a short
663"
THEORY ASSUMPTIONS AND PROOFS,0.7657992565055762,"proof sketch to provide intuition.
664"
THEORY ASSUMPTIONS AND PROOFS,0.766728624535316,"• Inversely, any informal proof provided in the core of the paper should be complemented
665"
THEORY ASSUMPTIONS AND PROOFS,0.7676579925650557,"by formal proofs provided in appendix or supplemental material.
666"
THEORY ASSUMPTIONS AND PROOFS,0.7685873605947955,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695167286245354,"4. Experimental Result Reproducibility
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7704460966542751,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713754646840149,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723048327137546,"of the paper (regardless of whether the code and data are provided or not)?
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7732342007434945,"Answer: [Yes]
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7741635687732342,"Justification: All the information needed to reproduce the main experimental results of the
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.775092936802974,"paper is disclosed, please see Sec. 4.1.
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7760223048327137,"Guidelines:
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7769516728624535,"• The answer NA means that the paper does not include experiments.
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778810408921933,"• If the paper includes experiments, a No answer to this question will not be perceived
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788104089219331,"well by the reviewers: Making the paper reproducible is important, regardless of
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7797397769516728,"whether the code and data are provided or not.
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806691449814126,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7815985130111525,"to make their results reproducible or verifiable.
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7825278810408922,"• Depending on the contribution, reproducibility can be accomplished in various ways.
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783457249070632,"For example, if the contribution is a novel architecture, describing the architecture fully
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7843866171003717,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7853159851301115,"be necessary to either make it possible for others to replicate the model with the same
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7862453531598513,"dataset, or provide access to the model. In general. releasing code and data is often
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871747211895911,"one good way to accomplish this, but reproducibility can also be provided via detailed
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7881040892193308,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7890334572490706,"of a large language model), releasing of a model checkpoint, or other means that are
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7899628252788105,"appropriate to the research performed.
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7908921933085502,"• While NeurIPS does not require releasing code, the conference does require all submis-
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.79182156133829,"sions to provide some reasonable avenue for reproducibility, which may depend on the
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7927509293680297,"nature of the contribution. For example
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7936802973977695,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7946096654275093,"to reproduce that algorithm.
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7955390334572491,"(b) If the contribution is primarily a new model architecture, the paper should describe
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7964684014869888,"the architecture clearly and fully.
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7973977695167286,"(c) If the contribution is a new model (e.g., a large language model), then there should
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7983271375464684,"either be a way to access this model for reproducing the results or a way to reproduce
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7992565055762082,"the model (e.g., with an open-source dataset or instructions for how to construct
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.800185873605948,"the dataset).
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8011152416356877,"(d) We recognize that reproducibility may be tricky in some cases, in which case
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8020446096654275,"authors are welcome to describe the particular way they provide for reproducibility.
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8029739776951673,"In the case of closed-source models, it may be that access to the model is limited in
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8039033457249071,"some way (e.g., to registered users), but it should be possible for other researchers
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8048327137546468,"to have some path to reproducing or verifying the results.
706"
OPEN ACCESS TO DATA AND CODE,0.8057620817843866,"5. Open access to data and code
707"
OPEN ACCESS TO DATA AND CODE,0.8066914498141264,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
708"
OPEN ACCESS TO DATA AND CODE,0.8076208178438662,"tions to faithfully reproduce the main experimental results, as described in supplemental
709"
OPEN ACCESS TO DATA AND CODE,0.8085501858736059,"material?
710"
OPEN ACCESS TO DATA AND CODE,0.8094795539033457,"Answer: [Yes]
711"
OPEN ACCESS TO DATA AND CODE,0.8104089219330854,"Justification: This paper provides open access to the data and code to reproduce the main
712"
OPEN ACCESS TO DATA AND CODE,0.8113382899628253,"experimental results, please see Sec. 4.1.
713"
OPEN ACCESS TO DATA AND CODE,0.8122676579925651,"Guidelines:
714"
OPEN ACCESS TO DATA AND CODE,0.8131970260223048,"• The answer NA means that paper does not include experiments requiring code.
715"
OPEN ACCESS TO DATA AND CODE,0.8141263940520446,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
716"
OPEN ACCESS TO DATA AND CODE,0.8150557620817844,"public/guides/CodeSubmissionPolicy) for more details.
717"
OPEN ACCESS TO DATA AND CODE,0.8159851301115242,"• While we encourage the release of code and data, we understand that this might not be
718"
OPEN ACCESS TO DATA AND CODE,0.8169144981412639,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
719"
OPEN ACCESS TO DATA AND CODE,0.8178438661710037,"including code, unless this is central to the contribution (e.g., for a new open-source
720"
OPEN ACCESS TO DATA AND CODE,0.8187732342007435,"benchmark).
721"
OPEN ACCESS TO DATA AND CODE,0.8197026022304833,"• The instructions should contain the exact command and environment needed to run to
722"
OPEN ACCESS TO DATA AND CODE,0.820631970260223,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
723"
OPEN ACCESS TO DATA AND CODE,0.8215613382899628,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
724"
OPEN ACCESS TO DATA AND CODE,0.8224907063197026,"• The authors should provide instructions on data access and preparation, including how
725"
OPEN ACCESS TO DATA AND CODE,0.8234200743494424,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
726"
OPEN ACCESS TO DATA AND CODE,0.8243494423791822,"• The authors should provide scripts to reproduce all experimental results for the new
727"
OPEN ACCESS TO DATA AND CODE,0.8252788104089219,"proposed method and baselines. If only a subset of experiments are reproducible, they
728"
OPEN ACCESS TO DATA AND CODE,0.8262081784386617,"should state which ones are omitted from the script and why.
729"
OPEN ACCESS TO DATA AND CODE,0.8271375464684015,"• At submission time, to preserve anonymity, the authors should release anonymized
730"
OPEN ACCESS TO DATA AND CODE,0.8280669144981413,"versions (if applicable).
731"
OPEN ACCESS TO DATA AND CODE,0.828996282527881,"• Providing as much information as possible in supplemental material (appended to the
732"
OPEN ACCESS TO DATA AND CODE,0.8299256505576208,"paper) is recommended, but including URLs to data and code is permitted.
733"
OPEN ACCESS TO DATA AND CODE,0.8308550185873605,"6. Experimental Setting/Details
734"
OPEN ACCESS TO DATA AND CODE,0.8317843866171004,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
735"
OPEN ACCESS TO DATA AND CODE,0.8327137546468402,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
736"
OPEN ACCESS TO DATA AND CODE,0.8336431226765799,"results?
737"
OPEN ACCESS TO DATA AND CODE,0.8345724907063197,"Answer: [Yes]
738"
OPEN ACCESS TO DATA AND CODE,0.8355018587360595,"Justification: All the training and test details are specified in this paper, please see Sec. 4.1.
739"
OPEN ACCESS TO DATA AND CODE,0.8364312267657993,"Guidelines:
740"
OPEN ACCESS TO DATA AND CODE,0.837360594795539,"• The answer NA means that the paper does not include experiments.
741"
OPEN ACCESS TO DATA AND CODE,0.8382899628252788,"• The experimental setting should be presented in the core of the paper to a level of detail
742"
OPEN ACCESS TO DATA AND CODE,0.8392193308550185,"that is necessary to appreciate the results and make sense of them.
743"
OPEN ACCESS TO DATA AND CODE,0.8401486988847584,"• The full details can be provided either with the code, in appendix, or as supplemental
744"
OPEN ACCESS TO DATA AND CODE,0.8410780669144982,"material.
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8420074349442379,"7. Experiment Statistical Significance
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8429368029739777,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438661710037175,"information about the statistical significance of the experiments?
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8447955390334573,"Answer: [Yes]
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845724907063197,"Justification: This paper reports information about the statistical significance of experiments,
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8466542750929368,"please see Sec. 4.
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475836431226765,"Guidelines:
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485130111524164,"• The answer NA means that the paper does not include experiments.
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8494423791821561,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8503717472118959,"dence intervals, or statistical significance tests, at least for the experiments that support
755"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513011152416357,"the main claims of the paper.
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8522304832713755,"• The factors of variability that the error bars are capturing should be clearly stated (for
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8531598513011153,"example, train/test split, initialization, random drawing of some parameter, or overall
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.854089219330855,"run with given experimental conditions).
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8550185873605948,"• The method for calculating the error bars should be explained (closed form formula,
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8559479553903345,"call to a library function, bootstrap, etc.)
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8568773234200744,"• The assumptions made should be given (e.g., Normally distributed errors).
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8578066914498141,"• It should be clear whether the error bar is the standard deviation or the standard error
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8587360594795539,"of the mean.
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8596654275092936,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8605947955390335,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8615241635687733,"of Normality of errors is not verified.
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.862453531598513,"• For asymmetric distributions, the authors should be careful not to show in tables or
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8633828996282528,"figures symmetric error bars that would yield results that are out of range (e.g. negative
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8643122676579925,"error rates).
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652416356877324,"• If error bars are reported in tables or plots, The authors should explain in the text how
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8661710037174721,"they were calculated and reference the corresponding figures or tables in the text.
772"
EXPERIMENTS COMPUTE RESOURCES,0.8671003717472119,"8. Experiments Compute Resources
773"
EXPERIMENTS COMPUTE RESOURCES,0.8680297397769516,"Question: For each experiment, does the paper provide sufficient information on the com-
774"
EXPERIMENTS COMPUTE RESOURCES,0.8689591078066915,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
775"
EXPERIMENTS COMPUTE RESOURCES,0.8698884758364313,"the experiments?
776"
EXPERIMENTS COMPUTE RESOURCES,0.870817843866171,"Answer: [Yes]
777"
EXPERIMENTS COMPUTE RESOURCES,0.8717472118959108,"Justification: This paper provides sufficient information on the computer resources, please
778"
EXPERIMENTS COMPUTE RESOURCES,0.8726765799256505,"see Sec. 4.1.
779"
EXPERIMENTS COMPUTE RESOURCES,0.8736059479553904,"Guidelines:
780"
EXPERIMENTS COMPUTE RESOURCES,0.8745353159851301,"• The answer NA means that the paper does not include experiments.
781"
EXPERIMENTS COMPUTE RESOURCES,0.8754646840148699,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
782"
EXPERIMENTS COMPUTE RESOURCES,0.8763940520446096,"or cloud provider, including relevant memory and storage.
783"
EXPERIMENTS COMPUTE RESOURCES,0.8773234200743495,"• The paper should provide the amount of compute required for each of the individual
784"
EXPERIMENTS COMPUTE RESOURCES,0.8782527881040892,"experimental runs as well as estimate the total compute.
785"
EXPERIMENTS COMPUTE RESOURCES,0.879182156133829,"• The paper should disclose whether the full research project required more compute
786"
EXPERIMENTS COMPUTE RESOURCES,0.8801115241635687,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
787"
EXPERIMENTS COMPUTE RESOURCES,0.8810408921933085,"didn’t make it into the paper).
788"
CODE OF ETHICS,0.8819702602230484,"9. Code Of Ethics
789"
CODE OF ETHICS,0.8828996282527881,"Question: Does the research conducted in the paper conform, in every respect, with the
790"
CODE OF ETHICS,0.8838289962825279,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
791"
CODE OF ETHICS,0.8847583643122676,"Answer: [Yes]
792"
CODE OF ETHICS,0.8856877323420075,"Justification: The conducted research conforms with the NeurIPS Code of Ethics.
793"
CODE OF ETHICS,0.8866171003717472,"Guidelines:
794"
CODE OF ETHICS,0.887546468401487,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
795"
CODE OF ETHICS,0.8884758364312267,"• If the authors answer No, they should explain the special circumstances that require a
796"
CODE OF ETHICS,0.8894052044609665,"deviation from the Code of Ethics.
797"
CODE OF ETHICS,0.8903345724907064,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
798"
CODE OF ETHICS,0.8912639405204461,"eration due to laws or regulations in their jurisdiction).
799"
BROADER IMPACTS,0.8921933085501859,"10. Broader Impacts
800"
BROADER IMPACTS,0.8931226765799256,"Question: Does the paper discuss both potential positive societal impacts and negative
801"
BROADER IMPACTS,0.8940520446096655,"societal impacts of the work performed?
802"
BROADER IMPACTS,0.8949814126394052,"Answer: [Yes]
803"
BROADER IMPACTS,0.895910780669145,"Justification: This paper discusses both potential positive societal impacts and negative
804"
BROADER IMPACTS,0.8968401486988847,"societal impacts of the work, please see Sec. 4.5.
805"
BROADER IMPACTS,0.8977695167286245,"Guidelines:
806"
BROADER IMPACTS,0.8986988847583643,"• The answer NA means that there is no societal impact of the work performed.
807"
BROADER IMPACTS,0.8996282527881041,"• If the authors answer NA or No, they should explain why their work has no societal
808"
BROADER IMPACTS,0.9005576208178439,"impact or why the paper does not address societal impact.
809"
BROADER IMPACTS,0.9014869888475836,"• Examples of negative societal impacts include potential malicious or unintended uses
810"
BROADER IMPACTS,0.9024163568773235,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
811"
BROADER IMPACTS,0.9033457249070632,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
812"
BROADER IMPACTS,0.904275092936803,"groups), privacy considerations, and security considerations.
813"
BROADER IMPACTS,0.9052044609665427,"• The conference expects that many papers will be foundational research and not tied
814"
BROADER IMPACTS,0.9061338289962825,"to particular applications, let alone deployments. However, if there is a direct path to
815"
BROADER IMPACTS,0.9070631970260223,"any negative applications, the authors should point it out. For example, it is legitimate
816"
BROADER IMPACTS,0.9079925650557621,"to point out that an improvement in the quality of generative models could be used to
817"
BROADER IMPACTS,0.9089219330855018,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
818"
BROADER IMPACTS,0.9098513011152416,"that a generic algorithm for optimizing neural networks could enable people to train
819"
BROADER IMPACTS,0.9107806691449815,"models that generate Deepfakes faster.
820"
BROADER IMPACTS,0.9117100371747212,"• The authors should consider possible harms that could arise when the technology is
821"
BROADER IMPACTS,0.912639405204461,"being used as intended and functioning correctly, harms that could arise when the
822"
BROADER IMPACTS,0.9135687732342007,"technology is being used as intended but gives incorrect results, and harms following
823"
BROADER IMPACTS,0.9144981412639405,"from (intentional or unintentional) misuse of the technology.
824"
BROADER IMPACTS,0.9154275092936803,"• If there are negative societal impacts, the authors could also discuss possible mitigation
825"
BROADER IMPACTS,0.9163568773234201,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
826"
BROADER IMPACTS,0.9172862453531598,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
827"
BROADER IMPACTS,0.9182156133828996,"feedback over time, improving the efficiency and accessibility of ML).
828"
SAFEGUARDS,0.9191449814126395,"11. Safeguards
829"
SAFEGUARDS,0.9200743494423792,"Question: Does the paper describe safeguards that have been put in place for responsible
830"
SAFEGUARDS,0.921003717472119,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
831"
SAFEGUARDS,0.9219330855018587,"image generators, or scraped datasets)?
832"
SAFEGUARDS,0.9228624535315985,"Answer: [NA]
833"
SAFEGUARDS,0.9237918215613383,"Justification: This paper poses no such risks.
834"
SAFEGUARDS,0.9247211895910781,"Guidelines:
835"
SAFEGUARDS,0.9256505576208178,"• The answer NA means that the paper poses no such risks.
836"
SAFEGUARDS,0.9265799256505576,"• Released models that have a high risk for misuse or dual-use should be released with
837"
SAFEGUARDS,0.9275092936802974,"necessary safeguards to allow for controlled use of the model, for example by requiring
838"
SAFEGUARDS,0.9284386617100372,"that users adhere to usage guidelines or restrictions to access the model or implementing
839"
SAFEGUARDS,0.929368029739777,"safety filters.
840"
SAFEGUARDS,0.9302973977695167,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
841"
SAFEGUARDS,0.9312267657992565,"should describe how they avoided releasing unsafe images.
842"
SAFEGUARDS,0.9321561338289963,"• We recognize that providing effective safeguards is challenging, and many papers do
843"
SAFEGUARDS,0.9330855018587361,"not require this, but we encourage authors to take this into account and make a best
844"
SAFEGUARDS,0.9340148698884758,"faith effort.
845"
LICENSES FOR EXISTING ASSETS,0.9349442379182156,"12. Licenses for existing assets
846"
LICENSES FOR EXISTING ASSETS,0.9358736059479554,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
847"
LICENSES FOR EXISTING ASSETS,0.9368029739776952,"the paper, properly credited and are the license and terms of use explicitly mentioned and
848"
LICENSES FOR EXISTING ASSETS,0.9377323420074349,"properly respected?
849"
LICENSES FOR EXISTING ASSETS,0.9386617100371747,"Answer: [Yes]
850"
LICENSES FOR EXISTING ASSETS,0.9395910780669146,"Justification: The assets used in this paper are credited and the license is respected.
851"
LICENSES FOR EXISTING ASSETS,0.9405204460966543,"Guidelines:
852"
LICENSES FOR EXISTING ASSETS,0.9414498141263941,"• The answer NA means that the paper does not use existing assets.
853"
LICENSES FOR EXISTING ASSETS,0.9423791821561338,"• The authors should cite the original paper that produced the code package or dataset.
854"
LICENSES FOR EXISTING ASSETS,0.9433085501858736,"• The authors should state which version of the asset is used and, if possible, include a
855"
LICENSES FOR EXISTING ASSETS,0.9442379182156134,"URL.
856"
LICENSES FOR EXISTING ASSETS,0.9451672862453532,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
857"
LICENSES FOR EXISTING ASSETS,0.9460966542750929,"• For scraped data from a particular source (e.g., website), the copyright and terms of
858"
LICENSES FOR EXISTING ASSETS,0.9470260223048327,"service of that source should be provided.
859"
LICENSES FOR EXISTING ASSETS,0.9479553903345725,"• If assets are released, the license, copyright information, and terms of use in the
860"
LICENSES FOR EXISTING ASSETS,0.9488847583643123,"package should be provided. For popular datasets, paperswithcode.com/datasets
861"
LICENSES FOR EXISTING ASSETS,0.949814126394052,"has curated licenses for some datasets. Their licensing guide can help determine the
862"
LICENSES FOR EXISTING ASSETS,0.9507434944237918,"license of a dataset.
863"
LICENSES FOR EXISTING ASSETS,0.9516728624535316,"• For existing datasets that are re-packaged, both the original license and the license of
864"
LICENSES FOR EXISTING ASSETS,0.9526022304832714,"the derived asset (if it has changed) should be provided.
865"
LICENSES FOR EXISTING ASSETS,0.9535315985130112,"• If this information is not available online, the authors are encouraged to reach out to
866"
LICENSES FOR EXISTING ASSETS,0.9544609665427509,"the asset’s creators.
867"
NEW ASSETS,0.9553903345724907,"13. New Assets
868"
NEW ASSETS,0.9563197026022305,"Question: Are new assets introduced in the paper well documented and is the documentation
869"
NEW ASSETS,0.9572490706319703,"provided alongside the assets?
870"
NEW ASSETS,0.95817843866171,"Answer: [NA]
871"
NEW ASSETS,0.9591078066914498,"Justification: This paper does not release new assets.
872"
NEW ASSETS,0.9600371747211895,"Guidelines:
873"
NEW ASSETS,0.9609665427509294,"• The answer NA means that the paper does not release new assets.
874"
NEW ASSETS,0.9618959107806692,"• Researchers should communicate the details of the dataset/code/model as part of their
875"
NEW ASSETS,0.9628252788104089,"submissions via structured templates. This includes details about training, license,
876"
NEW ASSETS,0.9637546468401487,"limitations, etc.
877"
NEW ASSETS,0.9646840148698885,"• The paper should discuss whether and how consent was obtained from people whose
878"
NEW ASSETS,0.9656133828996283,"asset is used.
879"
NEW ASSETS,0.966542750929368,"• At submission time, remember to anonymize your assets (if applicable). You can either
880"
NEW ASSETS,0.9674721189591078,"create an anonymized URL or include an anonymized zip file.
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9684014869888475,"14. Crowdsourcing and Research with Human Subjects
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693308550185874,"Question: For crowdsourcing experiments and research with human subjects, does the paper
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702602230483272,"include the full text of instructions given to participants and screenshots, if applicable, as
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711895910780669,"well as details about compensation (if any)?
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721189591078067,"Answer: [NA]
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730483271375465,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739776951672863,"Guidelines:
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974907063197026,"• The answer NA means that the paper does not involve crowdsourcing nor research with
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9758364312267658,"human subjects.
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767657992565055,"• Including this information in the supplemental material is fine, but if the main contribu-
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776951672862454,"tion of the paper involves human subjects, then as much detail as possible should be
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786245353159851,"included in the main paper.
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795539033457249,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804832713754646,"or other labor should be paid at least the minimum wage in the country of the data
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814126394052045,"collector.
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823420074349443,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
897"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983271375464684,"Subjects
898"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842007434944238,"Question: Does the paper describe potential risks incurred by study participants, whether
899"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851301115241635,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
900"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9860594795539034,"approvals (or an equivalent approval/review based on the requirements of your country or
901"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869888475836431,"institution) were obtained?
902"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9879182156133829,"Answer: [NA]
903"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888475836431226,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
904"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897769516728625,"Guidelines:
905"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907063197026023,"• The answer NA means that the paper does not involve crowdsourcing nor research with
906"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991635687732342,"human subjects.
907"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925650557620818,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934944237918215,"may be required for any human subjects research. If you obtained IRB approval, you
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944237918215614,"should clearly state this in the paper.
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9953531598513011,"• We recognize that the procedures for this may vary significantly between institutions
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9962825278810409,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972118959107806,"guidelines for their institution.
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981412639405205,"• For initial submissions, do not include any information that would break anonymity (if
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990706319702602,"applicable), such as the institution conducting the review.
915"
