Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001838235294117647,"Multi-task and multi-domain learning methods seek to learn multiple tasks/domains,
1"
ABSTRACT,0.003676470588235294,"jointly or one after another, using a single unified network. The key challenge
2"
ABSTRACT,0.0055147058823529415,"and opportunity is to exploit shared information across tasks and domains to
3"
ABSTRACT,0.007352941176470588,"improve the efficiency of the unified network. The efficiency can be in terms
4"
ABSTRACT,0.009191176470588236,"of accuracy, storage cost, computation, or sample complexity. In this paper, we
5"
ABSTRACT,0.011029411764705883,"propose a factorized tensor network (FTN) that can achieve accuracy comparable
6"
ABSTRACT,0.012867647058823529,"to independent single-task/domain networks with a small number of additional
7"
ABSTRACT,0.014705882352941176,"parameters. FTN uses a frozen backbone network from a source model and
8"
ABSTRACT,0.016544117647058824,"incrementally adds task/domain-specific low-rank tensor factors to the shared
9"
ABSTRACT,0.01838235294117647,"frozen network. This approach can adapt to a large number of target domains and
10"
ABSTRACT,0.02022058823529412,"tasks without catastrophic forgetting. Furthermore, FTN requires a significantly
11"
ABSTRACT,0.022058823529411766,"smaller number of task-specific parameters compared to existing methods. We
12"
ABSTRACT,0.02389705882352941,"performed experiments on widely used multi-domain and multi-task datasets. We
13"
ABSTRACT,0.025735294117647058,"observed that FTN achieves similar accuracy as single-task/domain methods while
14"
ABSTRACT,0.027573529411764705,"using 2–6% additional parameters per task. We also demonstrate the effectiveness
15"
ABSTRACT,0.029411764705882353,"of FTN with domain adaptation for image generation.
16"
INTRODUCTION,0.03125,"1
Introduction
17"
INTRODUCTION,0.03308823529411765,"The primary objective in multi-task learning (MTL) is to train a single model to learn multiple related
18"
INTRODUCTION,0.034926470588235295,"tasks, either jointly or sequentially. Multi-domain learning (MDL) aims to achieve the same learning
19"
INTRODUCTION,0.03676470588235294,"objective across multiple domains. MTL and MDL techniques seek to improve overall performance
20"
INTRODUCTION,0.03860294117647059,"by leveraging shared information across multiple tasks and domains. On the other hand, single-task or
21"
INTRODUCTION,0.04044117647058824,"single-domain learning do not have that opportunity. Furthermore, the storage and computational cost
22"
INTRODUCTION,0.042279411764705885,"associated with single-task/domain models quickly grows as the number of tasks/domains increases.
23"
INTRODUCTION,0.04411764705882353,"In contrast, MTL and MDL methods can use the same network resources for multiple tasks/domains,
24"
INTRODUCTION,0.04595588235294118,"which keeps the overall computational and storage cost small [1–10].
25"
INTRODUCTION,0.04779411764705882,"In general, MTL and MDL can have different input/output configurations, but we model them as
26"
INTRODUCTION,0.04963235294117647,"task/domain-specific network representation problems. Let us represent a network for MTL or MDL
27"
INTRODUCTION,0.051470588235294115,"as the following general function:
28"
INTRODUCTION,0.05330882352941176,"yt = Ft(x) ≡F(x; Wt, ht),
(1)"
INTRODUCTION,0.05514705882352941,"where Ft represents a function for task/domain t that maps input x to output yt. We further assume
29"
INTRODUCTION,0.05698529411764706,"that F represents a network with a fixed architecture and Wt and ht represent the parameters for
30"
INTRODUCTION,0.058823529411764705,"task/domain-specific feature extraction and classification/inference heads, respectively. The function
31"
INTRODUCTION,0.06066176470588235,"in (1) can represent the network for specific task/domain t using the respective Wt, ht. In the case
32"
INTRODUCTION,0.0625,"of MTL, with T tasks, we can have T outputs y1, . . . , yT for a given input x. In the case of MDL,
33"
INTRODUCTION,0.06433823529411764,"we usually have a single output for a given input, conditioned on the domain t. Our main goal is to
34"
INTRODUCTION,0.0661764705882353,Layer 1
INTRODUCTION,0.06801470588235294,Layer 2
INTRODUCTION,0.06985294117647059,Head 1
INTRODUCTION,0.07169117647058823,Input 1
INTRODUCTION,0.07352941176470588,Output 1
INTRODUCTION,0.07536764705882353,Layer 1
INTRODUCTION,0.07720588235294118,Layer 2
INTRODUCTION,0.07904411764705882,Head 2
INTRODUCTION,0.08088235294117647,Input 2
INTRODUCTION,0.08272058823529412,Output 2
INTRODUCTION,0.08455882352941177,(b) Feature-Extractor
INTRODUCTION,0.08639705882352941,Head 1
INTRODUCTION,0.08823529411764706,Output 1
INTRODUCTION,0.0900735294117647,Head 2
INTRODUCTION,0.09191176470588236,Input 1 or Input 2
INTRODUCTION,0.09375,Output 2
INTRODUCTION,0.09558823529411764,Shared
INTRODUCTION,0.0974264705882353,shared
INTRODUCTION,0.09926470588235294,(c) Factorized Tensor Network (FTN)
INTRODUCTION,0.10110294117647059,"Head 1
Head 2"
INTRODUCTION,0.10294117647058823,Input 1 or Input 2
INTRODUCTION,0.10477941176470588,shared
INTRODUCTION,0.10661764705882353,"shared 1
1 2
2"
INTRODUCTION,0.10845588235294118,"Frozen 
backbone"
INTRODUCTION,0.11029411764705882,weight
INTRODUCTION,0.11213235294117647,"Low-rank tensor
BN Wl Relu Cout Cin k2
k2"
INTRODUCTION,0.11397058823529412,"Cin
Cout"
INTRODUCTION,0.11580882352941177,"rank,1
rank,r Cout Cin k2
k2"
INTRODUCTION,0.11764705882352941,"Cin
Cout"
INTRODUCTION,0.11948529411764706,"rank,1
rank,r Cout Cin k2
k2"
INTRODUCTION,0.1213235294117647,"Cin
Cout"
INTRODUCTION,0.12316176470588236,"rank,1
rank,r"
INTRODUCTION,0.125,"Frozen 
backbone"
INTRODUCTION,0.12683823529411764,weight
INTRODUCTION,0.12867647058823528,"Low-rank tensor Wl+1 Cout Cin k2
k2"
INTRODUCTION,0.13051470588235295,"Cin
Cout"
INTRODUCTION,0.1323529411764706,"rank,1
rank,r Cout Cin k2
k2"
INTRODUCTION,0.13419117647058823,"Cin
Cout"
INTRODUCTION,0.13602941176470587,"rank,1
rank,r Cout Cin k2
k2"
INTRODUCTION,0.13786764705882354,"Cin
Cout"
INTRODUCTION,0.13970588235294118,"rank,1
rank,r Head"
INTRODUCTION,0.14154411764705882,"Output 1
Output 2"
INTRODUCTION,0.14338235294117646,(d) Detailed overview of FTN architecture.
INTRODUCTION,0.14522058823529413,"Low-rank tensors 
for task 1
Low-rank tensors 
for task 2"
INTRODUCTION,0.14705882352941177,(a) Fine-Tuning
INTRODUCTION,0.1488970588235294,"Figure 1: Overview of different MTL/MDL approaches and our proposed method. (a) Fine-Tuning trains
entire network per task/domain. (b) Feature-Extractor trains a backbone network shared by all tasks/domains
with task/domain-specific heads. (c) Our proposed method, Factorized Tensor Network (FTN), adapts to a
new task/domain by adding low-rank factors to shared layers. (d) Detailed overview of FTN. A single network
adapted to three downstream vision tasks (segmentation, depth, and surface normal estimation) by adding
task-specific low-rank tensors (∆Wt). Task/domain-specific blocks are shown in same colors."
INTRODUCTION,0.15073529411764705,"learn the Wt, ht for all t that maximize the performance of MTL/MDL with minimal computation
35"
INTRODUCTION,0.15257352941176472,"and memory overhead compared to single task/domain learning.
36"
INTRODUCTION,0.15441176470588236,"Figure 1(a),(b),(c) illustrate three typical approaches for MTL/MDL. First, we can start with a pre-
37"
INTRODUCTION,0.15625,"trained network and fine-tune all the parameters (Wt) to learn a target task/domain, as shown in
38"
INTRODUCTION,0.15808823529411764,"Figure 1(a). Fine-Tuning approaches can transfer some knowledge from the pretrained network to the
39"
INTRODUCTION,0.15992647058823528,"target task/domain, but they effectively use an independent network for every task/domain [1, 5, 11–
40"
INTRODUCTION,0.16176470588235295,"14]. Second, we can reduce the parameter and computation complexity by using a completely
41"
INTRODUCTION,0.1636029411764706,"shared Feature-Extractor (i.e., Wt = Wshared for all t) and learning task/domain-specific heads as
42"
INTRODUCTION,0.16544117647058823,"last layers, as shown in Figure 1(b). While such approaches reduce the number of parameters, they
43"
INTRODUCTION,0.16727941176470587,"often result in poor overall performance because of limited network capacity and interference among
44"
INTRODUCTION,0.16911764705882354,"features for different tasks/domains [1, 4, 5, 15]. Third, we can divide the network into shared and
45"
INTRODUCTION,0.17095588235294118,"task/domain-specific parameters or pathways, as shown in Figure 1(c). Such an approach can increase
46"
INTRODUCTION,0.17279411764705882,"the network capacity, provide interference-free paths for task/domain-specific feature extraction, and
47"
INTRODUCTION,0.17463235294117646,"enable knowledge sharing across the tasks/domains. In recent years, a number of such methods
48"
INTRODUCTION,0.17647058823529413,"have been proposed for MTL/MDL [1, 4, 5, 9, 16–22]. While existing methods provide performance
49"
INTRODUCTION,0.17830882352941177,"comparable to single-task/domain learning, they require a significantly large number of parameters.
50"
INTRODUCTION,0.1801470588235294,"In this paper, we propose a new method to divide network into shared and task/domain-specific
51"
INTRODUCTION,0.18198529411764705,"parameters using a factorized tensor network (FTN). In particular, our method learns task/domain-
52"
INTRODUCTION,0.18382352941176472,"specific low-rank tensor factors and normalization layers. An illustration of our proposed method is
53"
INTRODUCTION,0.18566176470588236,"shown in Figure 1(d), where we represent network parameters as Wt = Wshared + ∆Wt, where ∆Wt
54"
INTRODUCTION,0.1875,"is a low-rank tensor. Furthermore, we also learn task/domain-specific normalization parameters. We
55"
INTRODUCTION,0.18933823529411764,"demonstrate the effectiveness of our method using different MTL and MDL datasets. Our method
56"
INTRODUCTION,0.19117647058823528,"can achieve accuracy comparable to a single-task/domain network with a small number of additional
57"
INTRODUCTION,0.19301470588235295,"parameters. Existing parameter-efficient MTL/MDL methods [1, 2, 23] introduce small task/domain-
58"
INTRODUCTION,0.1948529411764706,"specific parameters while others [15, 24] add many parameters to boost the performance irrespective
59"
INTRODUCTION,0.19669117647058823,"of the task complexity. In our work, we demonstrate the flexibility of FTNs by selecting the rank
60"
INTRODUCTION,0.19852941176470587,"according to the complexity of the task. Finally, we also present an experiment for multi-domain
61"
INTRODUCTION,0.20036764705882354,"image generation using FTNs.
62"
INTRODUCTION,0.20220588235294118,"Contributions.
The main contributions of this paper can be summarized as follows.
63"
INTRODUCTION,0.20404411764705882,"• We propose a new method for MTL and MDL, called factorized tensor networks (FTN), that adds
64"
INTRODUCTION,0.20588235294117646,"task/domain-specific low-rank tensors to shared weights.
65"
INTRODUCTION,0.20772058823529413,"• We demonstrate that by using as little as 2–6% additional parameters per task/domain, FTNs can
66"
INTRODUCTION,0.20955882352941177,"achieve similar performance as the single-task/domain methods.
67"
INTRODUCTION,0.2113970588235294,"• Our proposed FTNs can be viewed as a plug-in module that can be added to any pretrained network
68"
INTRODUCTION,0.21323529411764705,"and layer.
69"
INTRODUCTION,0.21507352941176472,"• We performed empirical analysis to show that the FTNs enable flexibility by allowing us to vary
70"
INTRODUCTION,0.21691176470588236,"the rank of the task-specific tensors based on the complexity of the problem.
71"
INTRODUCTION,0.21875,"Limitations.
Our proposed method requires a small memory overhead to represent the MTL/MDL
72"
INTRODUCTION,0.22058823529411764,"networks compared to the single task/domain networks. The proposed method does not affect
73"
INTRODUCTION,0.22242647058823528,"the computational cost because we need to compute features for each task/domain using separate
74"
INTRODUCTION,0.22426470588235295,"functional pathways. In our experiments, we used a fixed rank for each layer. In principle, we can
75"
INTRODUCTION,0.2261029411764706,"adaptively select the rank for different layers to further reduce the parameters. MTL/MDL models
76"
INTRODUCTION,0.22794117647058823,"often suffer from task interference or negative transfer learning when multiple conflicting tasks are
77"
INTRODUCTION,0.22977941176470587,"trained jointly. Our method can have similar drawbacks as we did not investigate which tasks/domains
78"
INTRODUCTION,0.23161764705882354,"should be learned jointly.
79"
RELATED WORK,0.23345588235294118,"2
Related Work
80"
RELATED WORK,0.23529411764705882,"Multi-task learning (MTL) methods commonly leverage shared and task-specific layers in a unified
81"
RELATED WORK,0.23713235294117646,"network to solve related tasks [16–18, 25–32]. These methods learn shared and task-specific repre-
82"
RELATED WORK,0.23897058823529413,"sentation through their respective modules. Optimization based methods [33–37] devise a principled
83"
RELATED WORK,0.24080882352941177,"way to evaluate gradients and losses in multi-task settings. MTL networks that incrementally learn
84"
RELATED WORK,0.2426470588235294,"new taks were proposed in [9, 10]. ASTMT [10] proposed a network that emphasizes or suppresses
85"
RELATED WORK,0.24448529411764705,"features depending on the task at hand. RCM [9] reparameterizes the convolutional layer into
86"
RELATED WORK,0.24632352941176472,"non-trainable and task-specific trainable modules. We compare our proposed method with these
87"
RELATED WORK,0.24816176470588236,"incrementally learned networks. Adashare [8] is another related work in MTL that jointly learns
88"
RELATED WORK,0.25,"multiple tasks. It learns task-specific polices and network pathways [38].
89"
RELATED WORK,0.25183823529411764,"Multi-domain learning (MDL) focuses on adapting one network to multiple unseen domains or
90"
RELATED WORK,0.2536764705882353,"tasks. MDL setup trains models on task-specific modules built upon the frozen backbone network.
91"
RELATED WORK,0.2555147058823529,"This setup helps MDL networks to avoid negative transfer learning or catastrophic forgetting, which
92"
RELATED WORK,0.25735294117647056,"is common among multi-task learning methods. The work by [3, 6] introduces the task-specific
93"
RELATED WORK,0.25919117647058826,"parameters called residual adapters. The architecture introduces these adapters as a series or parallel
94"
RELATED WORK,0.2610294117647059,"connection on the backbone for a downstream task. Inspired by pruning techniques, Packnet [2] learns
95"
RELATED WORK,0.26286764705882354,"on multiple domains sequentially on a single task to decrease the overhead storage, which comes at
96"
RELATED WORK,0.2647058823529412,"the cost of performance. Similarly, the Piggyback [1] method uses binary masks as the module for
97"
RELATED WORK,0.2665441176470588,"task-specific parameters. These masks are applied to the weights of the backbone to adapt them to
98"
RELATED WORK,0.26838235294117646,"new domains. To extend this work, WTPB [7] uses the affine transformations of the binary mask on
99"
RELATED WORK,0.2702205882352941,"their backbone to extend the flexibility for better learning. BA2 [4] proposed a budget-constrained
100"
RELATED WORK,0.27205882352941174,"MDL network that selects the feature channels in the convolutional layer. It gives parameter efficient
101"
RELATED WORK,0.27389705882352944,"network by dropping the feature channels based on budget but at the cost of performance. Spot-Tune
102"
RELATED WORK,0.2757352941176471,"[24] learns a policy network, which decides whether to pass each image through Fine-Tuning or
103"
RELATED WORK,0.2775735294117647,"pre-trained networks. It neglects the parameter efficiency factor and emphasises more on performance.
104"
RELATED WORK,0.27941176470588236,"TAPS [5] adaptively learns to change a small number of layers in pre-trained backbone network for
105"
RELATED WORK,0.28125,"the downstream task.
106"
RELATED WORK,0.28308823529411764,"Our proposed method, FTN, achieves performance comparable to or better than other methods by
107"
RELATED WORK,0.2849264705882353,"utilizing a fraction of the parameters. We demonstrated that, unlike other methods, easy domains do
108"
RELATED WORK,0.2867647058823529,"not require the transformation of the backbone by the same complex module, and we can choose the
109"
RELATED WORK,0.28860294117647056,"flexibility of the task-specific parameter for a given domain.
110"
RELATED WORK,0.29044117647058826,"Domain adaptation and transfer learning. The work in this field usually focuses on learning a
111"
RELATED WORK,0.2922794117647059,"network from a given source domain to a closely related target domain. The target domains under
112"
RELATED WORK,0.29411764705882354,"this kind of learning typically have the same category of classes as source domains [11]. Due to
113"
RELATED WORK,0.2959558823529412,"this, it benefits from exploiting the labels of source domains to learn about multiple related target
114"
RELATED WORK,0.2977941176470588,"domains[12, 39]. Some work has a slight domain shift between source and target data like different
115"
RELATED WORK,0.29963235294117646,"camera views [40]. At the same time, recent papers have worked on large domain shifts like converting
116"
RELATED WORK,0.3014705882352941,"targets into sketch or art domains [12, 41]. Transfer learning is related to MDL or domain adaptation,
117"
RELATED WORK,0.30330882352941174,"but focuses on how to generalize better on target tasks [13, 14, 42]. Most of the work in this field
118"
RELATED WORK,0.30514705882352944,"uses the popular Imagenet as a source dataset to learn feature representation and learn to transfer to
119"
RELATED WORK,0.3069852941176471,"target datasets.
120"
TECHNICAL DETAILS,0.3088235294117647,"3
Technical Details
121"
TECHNICAL DETAILS,0.31066176470588236,"In our proposed method, we use task/domain-specific low-rank tensors to adapt every convolutional
122"
TECHNICAL DETAILS,0.3125,"layer of a pretrained backbone network to new tasks and domains. Let us assume the backbone
123"
TECHNICAL DETAILS,0.31433823529411764,"network has L convolution layers that are shared across all task/domains. We represent the shared
124"
TECHNICAL DETAILS,0.3161764705882353,"network weights as Wshared = {W1, . . . , WL} and the low-rank network updates for task/domain
125"
TECHNICAL DETAILS,0.3180147058823529,"t as ∆Wt = {∆W1,t, . . . , ∆WL,t}. To compute features for task/domain t, we update weights at
126"
TECHNICAL DETAILS,0.31985294117647056,"every layer as Wshared + ∆Wt = {W1 + ∆W1,t, . . . , WL + ∆WL,t}.
127"
TECHNICAL DETAILS,0.32169117647058826,"To keep our notations simple, let us only consider lth convolution layer that has k × k filters, Cin
128"
TECHNICAL DETAILS,0.3235294117647059,"channels for input feature tensor, and Cout channels for output feature tensor. We represent the
129"
TECHNICAL DETAILS,0.32536764705882354,"corresponding Wl as a tensor of size k2 × Cin × Cout. We represent the low-rank tensor update as a
130"
TECHNICAL DETAILS,0.3272058823529412,"summation of R rank-1 tensors as
131"
TECHNICAL DETAILS,0.3290441176470588,"∆Wl,t = R
X"
TECHNICAL DETAILS,0.33088235294117646,"r=1
wr
1,t ⊗wr
2,t ⊗wr
3,t,
(2)"
TECHNICAL DETAILS,0.3327205882352941,"where wr
1,t, wr
2,t, wr
3,t represent vectors of length k2, Cin, Cout, respectively, and ⊗represents the
132"
TECHNICAL DETAILS,0.33455882352941174,"Kronecker product.
133"
TECHNICAL DETAILS,0.33639705882352944,"Apart from low-rank tensor update, we also optimize over batchnorm layers (BN) for each task/domain
134"
TECHNICAL DETAILS,0.3382352941176471,"[43, 44]. The BN layer learns two vectors Γ and β, each of length Cout. The BN operation along
135"
TECHNICAL DETAILS,0.3400735294117647,"Cout dimension can be defined as element-wise multiplication and addition:
136"
TECHNICAL DETAILS,0.34191176470588236,"BNΓ,β(u) = Γ"
TECHNICAL DETAILS,0.34375,"u −E[u]
p"
TECHNICAL DETAILS,0.34558823529411764,Var[u] + ϵ !
TECHNICAL DETAILS,0.3474264705882353,"+ β.
(3)"
TECHNICAL DETAILS,0.3492647058823529,"We represent the output of lth convolution layer for task/domain t as
137"
TECHNICAL DETAILS,0.35110294117647056,"Zl,t = BNΓt,βt(conv(Wl + ∆Wl,t, Yl−1,t)),
(4)"
TECHNICAL DETAILS,0.35294117647058826,"where Yl−1,t represents the input tensor and Zl,t represents the output tensor for lth layer. In our
138"
TECHNICAL DETAILS,0.3547794117647059,"proposed FTN, we learn the task/domain-specific factors {wr
1,t, wr
2,t, wr
3,t}R
r=1, and Γt, and βt for
139"
TECHNICAL DETAILS,0.35661764705882354,"every layer in the backbone network.
140"
TECHNICAL DETAILS,0.3584558823529412,"In the FTN method, since we are learning over only ∆W and BN parameters, the rank, R, plays
141"
TECHNICAL DETAILS,0.3602941176470588,"an important role in defining the expressivity of our network. We can define a complex ∆W by
142"
TECHNICAL DETAILS,0.36213235294117646,"increasing the rank R of the low-rank tensor and taking their linear combination. Our experiments
143"
TECHNICAL DETAILS,0.3639705882352941,"showed that this has resulted in a significant performance gain.
144"
TECHNICAL DETAILS,0.36580882352941174,"Initialization. In our approach, the initialization of the low-rank parameter layers and the pre-trained
145"
TECHNICAL DETAILS,0.36764705882352944,"weights of the backbone network plays a crucial role due to their sensitivity towards performance. To
146"
TECHNICAL DETAILS,0.3694852941176471,"establish a favorable starting point, we adopt a strategy that minimizes substantial modifications to
147"
TECHNICAL DETAILS,0.3713235294117647,"the frozen backbone network weights during the initialization of the task-specific parameter layers.
148"
TECHNICAL DETAILS,0.37316176470588236,"To achieve this, we initialize each parameter layer from the xavier uniform distribution [45], thereby
149"
TECHNICAL DETAILS,0.375,"generating ∆W values close to 0 before their addition to the frozen weights. This approach ensures
150"
TECHNICAL DETAILS,0.37683823529411764,"the maintenance of a similar initialization state to the frozen weights at iteration 0.
151"
TECHNICAL DETAILS,0.3786764705882353,"To acquire an effective initialization for our backbone network, we leverage the pre-trained weights
152"
TECHNICAL DETAILS,0.3805147058823529,"obtained from ImageNet. We aim to establish a robust and capable feature extractor for our specific
153"
TECHNICAL DETAILS,0.38235294117647056,"task by incorporating these pre-trained weights into our backbone network.
154"
TECHNICAL DETAILS,0.38419117647058826,"Number of parameters. In a Fine-Tuning setup with T tasks/domains, the total number of required
155"
TECHNICAL DETAILS,0.3860294117647059,"parameters at convolutional layer l can be calculated as T · (k2 × Cin × Cout). Whereas using
156"
TECHNICAL DETAILS,0.38786764705882354,"our proposed FTNs, the total number of frozen backbone (Wl) and low-rank R tensor (∆Wl,t)
157"
TECHNICAL DETAILS,0.3897058823529412,"parameters are given by (Cout × Cin × k2) + T · R · (Cout + Cin + k2). In our results section, we
158"
TECHNICAL DETAILS,0.3915441176470588,"Table 1: Number of parameters and top-1% accuracy for baseline methods, comparative methods, and FTN with
varying ranks on the five domains of the ImageNet-to-Sketch benchmark experiments. Additionally, the mean
top-1% of each method across all domains is shown. The ‘Params’ column gives the number of parameters used
as a multiplier of those for the Feature-Extractor method, along with the absolute number of parameters required
in parentheses."
TECHNICAL DETAILS,0.39338235294117646,"Methods
Params (Abs)
Flowers
Wikiart
Sketch
Cars
CUB
mean"
TECHNICAL DETAILS,0.3952205882352941,"Fine-Tuning
6× (141M)
95.69
78.42
81.02
91.44
83.37
85.98
Feature-Extractor
1× (23.5M)
89.57
57.7
57.07
54.01
67.20
65.11
FC and BN only
1.001× (23.52M)
94.39
70.62
79.15
85.20
78.68
81.60"
TECHNICAL DETAILS,0.39705882352941174,"Piggyback [1]
6× [2.25×] (141M)
94.76
71.33
79.91
89.62
81.59
83.44
Packnet →[2]
[1.60×] (37.6M)
93
69.4
76.20
86.10
80.40
81.02
Packnet ←[2]
[1.60×] (37.6M)
90.60
70.3
78.7
80.0
71.4
78.2
Spot-Tune [24]
7× [7×] (164.5M)
96.34
75.77
80.2
92.4
84.03
85.74
WTPB [7]
6× [2.25×] (141M)
96.50
74.8
80.2
91.5
82.6
85.12
BA2 [4]
3.8× [1.71×] (89.3M)
95.74
72.32
79.28
92.14
81.19
84.13
TAPS [5]
4.12× (96.82M)
96.68
76.94
80.74
89.76
82.65
85.35"
TECHNICAL DETAILS,0.39889705882352944,"FTN, R=1
1.004× (23.95M)
94.79
73.03
78.62
86.85
80.86
82.83
FTN, R=50
1.53× (36.02M)
96.42
78.01
80.6
90.83
82.96
85.76"
TECHNICAL DETAILS,0.4007352941176471,"have shown that the absolute number of parameters required by our method is a fraction of what the
159"
TECHNICAL DETAILS,0.4025735294117647,"Fine-Tuning counterpart needs.
160"
TECHNICAL DETAILS,0.40441176470588236,"Effect of batch normalization. In our experiment section, under the ‘FC and BN only’ setup,
161"
TECHNICAL DETAILS,0.40625,"we have shown that having task-specific batchnorm layers in the backbone network significantly
162"
TECHNICAL DETAILS,0.40808823529411764,"affects the performance of a downstream task/domain. For all the experiments with our proposed
163"
TECHNICAL DETAILS,0.4099264705882353,"approach, we include batch normalization layers as task-specific along with low-rank tensors and
164"
TECHNICAL DETAILS,0.4117647058823529,"classification/decoder layer.
165"
EXPERIMENTS AND RESULTS,0.41360294117647056,"4
Experiments and Results
166"
EXPERIMENTS AND RESULTS,0.41544117647058826,"We evaluated the performance of our proposed FTN on several MTL/MDL datasets for three different
167"
EXPERIMENTS AND RESULTS,0.4172794117647059,"experiments: 1. Multi-domain classification, 2. Multi-task dense prediction, and 3. Multi-domain
168"
EXPERIMENTS AND RESULTS,0.41911764705882354,"image generation. For each set of benchmarks we report the performance of FTN with different
169"
EXPERIMENTS AND RESULTS,0.4209558823529412,"rank increments and compare with results from existing methods. All experiments are run on a single
170"
EXPERIMENTS AND RESULTS,0.4227941176470588,"NVIDIA GeForce RTX 2080 Ti GPU with 12GB memory.
171"
MULTI-DOMAIN CLASSIFICATION,0.42463235294117646,"4.1
Multi-domain classification
172"
MULTI-DOMAIN CLASSIFICATION,0.4264705882352941,"Datasets. We use two MTL/MDL classification-based benchmark datasets. First, ImageNet-to-
173"
MULTI-DOMAIN CLASSIFICATION,0.42830882352941174,"Sketch [1, 2, 5, 7], which contains five different domains: Flowers [46], Cars [47], Sketch [48],
174"
MULTI-DOMAIN CLASSIFICATION,0.43014705882352944,"Caltech-UCSD Birds (CUBs) [49], and WikiArt [50], with different classes. Second, DomainNet
175"
MULTI-DOMAIN CLASSIFICATION,0.4319852941176471,"[51], which contains six domains: Clipart, Sketch, Painting (Paint), Quickdraw (Quick), Inforgraph
176"
MULTI-DOMAIN CLASSIFICATION,0.4338235294117647,"(Info), and Real, with each domain containing an equal 345 classes. The datasets are prepared using
177"
MULTI-DOMAIN CLASSIFICATION,0.43566176470588236,"augmentation techniques as adopted by [5].
178"
MULTI-DOMAIN CLASSIFICATION,0.4375,"Training details. For each benchmark, we report the performance of FTN for various choices for
179"
MULTI-DOMAIN CLASSIFICATION,0.43933823529411764,"ranks, along with several benchmark-specific comparative and baseline methods. The backbone
180"
MULTI-DOMAIN CLASSIFICATION,0.4411764705882353,"weights are pretrained from ImageNet, using ResNet-50 [52] for the ImageNet-to-Sketch benchmarks,
181"
MULTI-DOMAIN CLASSIFICATION,0.4430147058823529,"and ResNet-34 on the DomainNet benchmarks to keep the same setting as [5]. On ImageNet-to-
182"
MULTI-DOMAIN CLASSIFICATION,0.44485294117647056,"Sketch we run FTNs for ranks, R ∈{1, 5, 10, 15, 20, 25, 50} and on DomainNet dataset for ranks,
183"
MULTI-DOMAIN CLASSIFICATION,0.44669117647058826,"R ∈{1, 5, 10, 20, 30, 40}. In the supplementary material, we provide the hyperparameter details to
184"
MULTI-DOMAIN CLASSIFICATION,0.4485294117647059,"train our network.
185"
MULTI-DOMAIN CLASSIFICATION,0.45036764705882354,"Results. We report the top-1% accuracy for each domain and the mean accuracy across all domains
186"
MULTI-DOMAIN CLASSIFICATION,0.4522058823529412,"for each collection of benchmark experiments. We also report the number of frozen and learnable
187"
MULTI-DOMAIN CLASSIFICATION,0.4540441176470588,"parameters in the backbone network. Table 1 compares the FTN method with other methods in terms
188"
MULTI-DOMAIN CLASSIFICATION,0.45588235294117646,"of accuracy and number of parameters (also see Figure 2). FTN outperforms every other method
189"
MULTI-DOMAIN CLASSIFICATION,0.4577205882352941,"while using 36.02 million parameters in the backbone with rank-50 updates for all domains. The
190"
MULTI-DOMAIN CLASSIFICATION,0.45955882352941174,"Table 2: Performance of different methods with resnet-34 backbone on DomainNet dataset. Top-1% accuracy is
shown on different domains with different methods along with the number of parameters."
MULTI-DOMAIN CLASSIFICATION,0.46139705882352944,"Methods
Params (Abs)
Clipart
Sketch
Paint
Quick
Info
Real
mean"
MULTI-DOMAIN CLASSIFICATION,0.4632352941176471,"Fine-Tuning
6× (127.68M)
74.26
67.33
67.11
72.43
40.11
80.36
66.93
Feature-Extractor
1× (21.28M)
60.94
50.03
60.22
54.01
26.19
76.79
54.69
FC and BN only
1.004× (21.35M)
70.24
61.10
64.22
63.09
34.76
78.61
62.00"
MULTI-DOMAIN CLASSIFICATION,0.4650735294117647,"Adashare [8]
5.73× (121.93M)
74.45
64.15
65.74
68.15
34.11
79.39
64.33
TAPS [5]
4.90× (104.27M)
74.85
66.66
67.28
71.79
38.21
80.28
66.51"
MULTI-DOMAIN CLASSIFICATION,0.46691176470588236,"FTN, R=1
1.008× (21.44M)
70.73
62.69
65.08
64.81
35.78
79.12
63.03
FTN, R=40
1.18× (25.22M)
74.2
65.67
67.14
71.00
39.10
80.64
66.29"
MULTI-DOMAIN CLASSIFICATION,0.46875,"mean accuracy performance is better than other methods and is close to Spot-Tune [24], which
191"
MULTI-DOMAIN CLASSIFICATION,0.47058823529411764,"requires nearly 165M parameters. On the Wikiart dataset, we outperform the top-1 accuracy with
192"
MULTI-DOMAIN CLASSIFICATION,0.4724264705882353,"other baseline methods. The performance of baseline methods is taken from TAPS [5] since we are
193"
MULTI-DOMAIN CLASSIFICATION,0.4742647058823529,"running the experiments under the same settings.
194"
MULTI-DOMAIN CLASSIFICATION,0.47610294117647056,"Table 2 shows the results on the DomainNet dataset, which we compare with TAPS [5] and Adashare
195"
MULTI-DOMAIN CLASSIFICATION,0.47794117647058826,"[8]. Again, using FTN, we significantly outperform comparison methods along the required pa-
196"
MULTI-DOMAIN CLASSIFICATION,0.4797794117647059,"rameters (rank-40 needs 25.22 million parameters only). Also, FTN rank-40 attains better top-1%
197"
MULTI-DOMAIN CLASSIFICATION,0.48161764705882354,"accuracy on the Infograph and Real domain, while it attains similar performance on all other domains.
198"
MULTI-DOMAIN CLASSIFICATION,0.4834558823529412,"On DomainNet with resnet-34 and Imagenet-to-Sketch with resnet-50 backbone, the rank-1 low-rank
199"
MULTI-DOMAIN CLASSIFICATION,0.4852941176470588,"tensors require only 16,291 and 49,204 parameters per task, respectively. We have shown additional
200"
MULTI-DOMAIN CLASSIFICATION,0.48713235294117646,"experiments on this dataset under a joint optimization setup in the supplementary material.
201"
MULTI-DOMAIN CLASSIFICATION,0.4889705882352941,"1
2
3
4
5
6
7
Parameter multiplier with baseline 75 80 85 90 95"
MULTI-DOMAIN CLASSIFICATION,0.49080882352941174,Top-1 accuracy [%]
MULTI-DOMAIN CLASSIFICATION,0.49264705882352944,"mean
Flowers
Wikiart
Sketch
Cars
CUBS
Spot-Tune
WTPB
BA2
TAPS
FTN, R=1
FTN, R=25
FTN, R=50"
MULTI-DOMAIN CLASSIFICATION,0.4944852941176471,(a) Imagenet-to-sketch dataset
MULTI-DOMAIN CLASSIFICATION,0.4963235294117647,"1
2
3
4
5
Parameter multiplier with baseline 40 50 60 70 80"
MULTI-DOMAIN CLASSIFICATION,0.49816176470588236,Top-1 accuracy [%]
MULTI-DOMAIN CLASSIFICATION,0.5,"mean
Clipart
Sketch
Painting
Quickdraw
Infograph
Real
Adashare
TAPS
FTN, R=1
FTN, R=40"
MULTI-DOMAIN CLASSIFICATION,0.5018382352941176,(b) DomainNet dataset
MULTI-DOMAIN CLASSIFICATION,0.5036764705882353,"Figure 2: Accuracy vs Parameter multiplier with baseline: We show the top-1% accuracy against the number
of parameter increments through our approach in the backbone network with the baseline backbone. We plot the
performance of our method with other baseline methods, which has shown that our approach attains competitive
performance with an extremely small number of parameters."
MULTI-DOMAIN CLASSIFICATION,0.5055147058823529,"Analysis on rank. We showed the effect of rank on FTNs by performing experiments with multiple
202"
MULTI-DOMAIN CLASSIFICATION,0.5073529411764706,"ranks on both datasets. Figure 3 shows the accuracy vs. ranks plot, where we observe a trend of
203"
MULTI-DOMAIN CLASSIFICATION,0.5091911764705882,"performance improvement as we increase the rank from 1 to 50 on the ImageNet-to-Sketch and from
204"
MULTI-DOMAIN CLASSIFICATION,0.5110294117647058,"1 to 40 on the DomainNet dataset. Also, not all domains need a high rank, as Figure 3 shows that the
205"
MULTI-DOMAIN CLASSIFICATION,0.5128676470588235,"Flowers and Cars domain attains good accuracy at rank 20 and 15, respectively. We can argue that,
206"
MULTI-DOMAIN CLASSIFICATION,0.5147058823529411,"unlike prior works [24, 23], which consume the same task-specific module for easy and complex
207"
MULTI-DOMAIN CLASSIFICATION,0.5165441176470589,"tasks, we can provide different flexibility to each task. Also, in supplementary material, we have a
208"
MULTI-DOMAIN CLASSIFICATION,0.5183823529411765,"heatmap plot showing the adaption of low-rank tensor at every layer upon increasing the rank.
209"
MULTI-TASK DENSE PREDICTION,0.5202205882352942,"4.2
Multi-task dense prediction
210"
MULTI-TASK DENSE PREDICTION,0.5220588235294118,"Dataset. The widely-used NYUD dataset [53] with 795 training and 654 testing images of indoor
211"
MULTI-TASK DENSE PREDICTION,0.5238970588235294,"scenes is used for dense prediction experiments in multi-task learning. The dataset contains four tasks:
212"
MULTI-TASK DENSE PREDICTION,0.5257352941176471,"edge detection (Edge), semantic segmentation (SemSeg), surface normals estimation (Normals), and
213"
MULTI-TASK DENSE PREDICTION,0.5275735294117647,"depth estimation (Depth). We follow the same data-augmentation technique as used by [9].
214"
MULTI-TASK DENSE PREDICTION,0.5294117647058824,"Metrics. On the tasks of the NYUD dataset, we report mean intersection over union for semantic
215"
MULTI-TASK DENSE PREDICTION,0.53125,"segmentation, mean error for surface normal estimation, optimal dataset F-measure [54] for edge
216"
MULTI-TASK DENSE PREDICTION,0.5330882352941176,"R=1
R=5
R=10
R=15
R=20
R=25
R=50"
MULTI-TASK DENSE PREDICTION,0.5349264705882353,"Differnet
ranks 60 65 70 75 80 85 90 95 100"
MULTI-TASK DENSE PREDICTION,0.5367647058823529,"T
op-1 accuracy [%]"
MULTI-TASK DENSE PREDICTION,0.5386029411764706,Flower
MULTI-TASK DENSE PREDICTION,0.5404411764705882,Wikiart
MULTI-TASK DENSE PREDICTION,0.5422794117647058,Sketch Cars CUBS
MULTI-TASK DENSE PREDICTION,0.5441176470588235,"Fine-
T
uning"
MULTI-TASK DENSE PREDICTION,0.5459558823529411,(a) Imagenet-to-sketch dataset
MULTI-TASK DENSE PREDICTION,0.5477941176470589,"R=1
R=5
R=10
R=20
R=30
R=40"
MULTI-TASK DENSE PREDICTION,0.5496323529411765,"Differnet
ranks 10 20 30 40 50 60 70 80"
MULTI-TASK DENSE PREDICTION,0.5514705882352942,"T
op-1 accuracy [%]"
MULTI-TASK DENSE PREDICTION,0.5533088235294118,Clipart
MULTI-TASK DENSE PREDICTION,0.5551470588235294,Sketch
MULTI-TASK DENSE PREDICTION,0.5569852941176471,Painting
MULTI-TASK DENSE PREDICTION,0.5588235294117647,Quickdraw
MULTI-TASK DENSE PREDICTION,0.5606617647058824,Infograph Real
MULTI-TASK DENSE PREDICTION,0.5625,"Fine-
T
uning"
MULTI-TASK DENSE PREDICTION,0.5643382352941176,(b) DomainNet dataset
MULTI-TASK DENSE PREDICTION,0.5661764705882353,"Figure 3: Accuracy vs Low-ranks: We show the top-1% accuracy against the different low-ranks used in our
method for different domains. We start with an ‘only BN’ setup where without any low-rank we keep the BN
(batchnorm) layers as task-specific. Then we show the performance improvement through our approach upon
increasing the rank, R."
MULTI-TASK DENSE PREDICTION,0.5680147058823529,"detection, and root mean squared error for depth estimation. We also report the number of parameters
217"
MULTI-TASK DENSE PREDICTION,0.5698529411764706,"used in the backbone for each method.
218"
MULTI-TASK DENSE PREDICTION,0.5716911764705882,"Training details. ResNet-18 is used as the backbone network, and DeepLabv3+ [55] as the decoder
219"
MULTI-TASK DENSE PREDICTION,0.5735294117647058,"architecture. The Fine-Tuning and Feature-Extractor experiments are implemented in the same
220"
MULTI-TASK DENSE PREDICTION,0.5753676470588235,"way as in the classification-based experiments above. We showed experiments for FTNs with
221"
MULTI-TASK DENSE PREDICTION,0.5772058823529411,"R ∈{1, 10, 20, 30}. Further details are in the supplementary material.
222"
MULTI-TASK DENSE PREDICTION,0.5790441176470589,"Results. Table 3 shows the performance of FTN with various ranks and of other baseline comparison
223"
MULTI-TASK DENSE PREDICTION,0.5808823529411765,"methods for dense prediction tasks on the NYUD dataset. We observe performance improvement by
224"
MULTI-TASK DENSE PREDICTION,0.5827205882352942,"increasing flexibility through higher rank. FTN with rank-30 performs better than all comparison
225"
MULTI-TASK DENSE PREDICTION,0.5845588235294118,"methods and utilizes the least number of parameters. Also, on the Depth and Edge task we can attain
226"
MULTI-TASK DENSE PREDICTION,0.5863970588235294,"good performance by using only rank-20. We take the performance of baseline comparison methods
227"
MULTI-TASK DENSE PREDICTION,0.5882352941176471,from the RCM paper [9] as we run our experiments under the same setting.
MULTI-TASK DENSE PREDICTION,0.5900735294117647,"Table 3: Dense prediction performance on NYUD dataset using ResNet-18 backbone with DeepLabv3+ decoder.
The proposed FTN approach with R = {1, 10, 20, 30} and other methods. The best performing method in bold."
MULTI-TASK DENSE PREDICTION,0.5919117647058824,"Methods
Params (Abs)
Semseg↑
Depth↓
Normals↓
Edge↑"
MULTI-TASK DENSE PREDICTION,0.59375,"Single Task
4× (44.68M)
35.34
0.56
22.20
73.5
Decoder only
1× (11.17M)
24.84
0.71
28.56
71.3
Decoder + BN only
1.002× (11.19M)
29.26
0.61
24.82
71.3"
MULTI-TASK DENSE PREDICTION,0.5955882352941176,"ASTMT (R-18 w/o SE) [10]
1.25× (13.99M)
30.69
0.60
23.94
68.60
ASTMT (R-26 w SE) [10]
2.00× (22.42M)
30.07
0.63
24.32
73.50
Series RA [3]
1.56× (17.51M)
31.87
0.60
23.35
67.56
Parallel RA [6]
1.50× (16.77M)
32.13
0.59
23.20
68.02
RCM [9]
1.56× (17.49M)
34.20
0.57
22.41
68.44"
MULTI-TASK DENSE PREDICTION,0.5974264705882353,"FTN, R=1
1.005× (11.23M)
29.83
0.60
23.56
72.7
FTN, R=10
1.03× (11.54M)
33.66
0.57
22.15
73.5
FTN, R=20
1.06× (11.89M)
34.06
0.55
21.84
73.9
FTN, R=30
1.09× (12.24M)
35.46
0.56
21.78
73.8
228"
MULTI-DOMAIN IMAGE GENERATION,0.5992647058823529,"4.3
Multi-domain image generation
229"
MULTI-DOMAIN IMAGE GENERATION,0.6011029411764706,"A deep generative network G, parameterized by W, can learn to map a low-dimensional latent code
230"
MULTI-DOMAIN IMAGE GENERATION,0.6029411764705882,"z to a high-dimensional natural image x [56–58]. To find a latent representation for a set of images
231"
MULTI-DOMAIN IMAGE GENERATION,0.6047794117647058,"given a pre-trained generative network, we can solve the following optimization problem:
232"
MULTI-DOMAIN IMAGE GENERATION,0.6066176470588235,"min
zi N
X"
MULTI-DOMAIN IMAGE GENERATION,0.6084558823529411,"i=1
∥xi −G(zi; W)∥p
p .
(5)"
MULTI-DOMAIN IMAGE GENERATION,0.6102941176470589,"Summer 
(original)"
MULTI-DOMAIN IMAGE GENERATION,0.6121323529411765,"Summer 
(generated)"
MULTI-DOMAIN IMAGE GENERATION,0.6139705882352942,"Winter 
(generated)"
MULTI-DOMAIN IMAGE GENERATION,0.6158088235294118,"Spring 
(generated)"
MULTI-DOMAIN IMAGE GENERATION,0.6176470588235294,"Autumn 
(generated)"
MULTI-DOMAIN IMAGE GENERATION,0.6194852941176471,Figure 4: Generated images for different seasons using FTN.
MULTI-DOMAIN IMAGE GENERATION,0.6213235294117647,"The work in [58] as well as our experimental results show that this approach is very limited in
233"
MULTI-DOMAIN IMAGE GENERATION,0.6231617647058824,"handling complex and diverse images. If x is an image that belongs to a domain that is different
234"
MULTI-DOMAIN IMAGE GENERATION,0.625,"from the source domain used to train the generator, we are not guaranteed to find a latent vector z∗
235"
MULTI-DOMAIN IMAGE GENERATION,0.6268382352941176,"such that x ≈G(z∗). We showed FTNs can be used to expand the range of G by reparametrizing it
236"
MULTI-DOMAIN IMAGE GENERATION,0.6286764705882353,"as G(zi; W, ∆Wt), where ∆Wt are the domain-specific low-rank factors and Batch Normalization
237"
MULTI-DOMAIN IMAGE GENERATION,0.6305147058823529,"parameters. By optimizing over the latent vectors zi and domain specific parameters, we learned to
238"
MULTI-DOMAIN IMAGE GENERATION,0.6323529411764706,"generate images from new domains.
239"
MULTI-DOMAIN IMAGE GENERATION,0.6341911764705882,"Dataset. We used the multi-domain Transient Attributes [59] dataset that contains outdoor scenes
240"
MULTI-DOMAIN IMAGE GENERATION,0.6360294117647058,"under different weather, lighting, and seasons. We extracted season information, ""summer"", ""winter"",
241"
MULTI-DOMAIN IMAGE GENERATION,0.6378676470588235,"""spring"", and ""autumn"", for each image and categorized them accordingly. Our goal is to learn a
242"
MULTI-DOMAIN IMAGE GENERATION,0.6397058823529411,"single FTN network that can generate images from all seasons.
243"
MULTI-DOMAIN IMAGE GENERATION,0.6415441176470589,"Training details.
Our base network follows the BigGAN architecture [60].
We com-
244"
MULTI-DOMAIN IMAGE GENERATION,0.6433823529411765,"pared our proposed FTN network with models trained under two different setups.
In
245"
MULTI-DOMAIN IMAGE GENERATION,0.6452205882352942,"the first setup, which serves as our baseline, we fine-tuned a pre-trained generator on im-
246"
MULTI-DOMAIN IMAGE GENERATION,0.6470588235294118,"ages from all seasons.
For the second setup, we fine-tuned the same network separately
247"
MULTI-DOMAIN IMAGE GENERATION,0.6488970588235294,"for each season.
Additional training details can be found in the supplementary material.
248"
MULTI-DOMAIN IMAGE GENERATION,0.6507352941176471,"Table 4: Image generation PSNR for different methods
and seasons"
MULTI-DOMAIN IMAGE GENERATION,0.6525735294117647,"Season
Baseline
Single domain
FTN"
MULTI-DOMAIN IMAGE GENERATION,0.6544117647058824,"Summer
10.80
25.30
25.30
Winter
9.24
21.30
22.23
Spring
11.08
22.07
20.50
Autumn
10.92
19.87
20.08 249"
MULTI-DOMAIN IMAGE GENERATION,0.65625,"Results. Table 4 shows the performance, in
250"
MULTI-DOMAIN IMAGE GENERATION,0.6580882352941176,"terms of Peak Signal-to-Noise Ratio (PSNR),
251"
MULTI-DOMAIN IMAGE GENERATION,0.6599264705882353,"for three methods. The baseline model is a sin-
252"
MULTI-DOMAIN IMAGE GENERATION,0.6617647058823529,"gle network trained on all images and shows
253"
MULTI-DOMAIN IMAGE GENERATION,0.6636029411764706,"overall poor performance. Our proposed FTN
254"
MULTI-DOMAIN IMAGE GENERATION,0.6654411764705882,"network achieved a comparable performance to
255"
MULTI-DOMAIN IMAGE GENERATION,0.6672794117647058,"the single-domain networks. Each single do-
256"
MULTI-DOMAIN IMAGE GENERATION,0.6691176470588235,"main network has 71.4M trainable parameters,
257"
MULTI-DOMAIN IMAGE GENERATION,0.6709558823529411,"while the FTN network adds an additional 3.9M
258"
MULTI-DOMAIN IMAGE GENERATION,0.6727941176470589,"parameters per domain over the base network. Figure 4 shows examples of images generated by our
259"
MULTI-DOMAIN IMAGE GENERATION,0.6746323529411765,"proposed FTN network.
260"
CONCLUSION,0.6764705882352942,"5
Conclusion
261"
CONCLUSION,0.6783088235294118,"We have proposed a simple, architecture-agnostic, and easy-to-implement FTN method that adapts
262"
CONCLUSION,0.6801470588235294,"to new unseen domains/tasks by using low-rank task-specific tensors. In our work, we have shown
263"
CONCLUSION,0.6819852941176471,"FTN requires the least number of parameters than other baseline methods in MDL/MTL experiments
264"
CONCLUSION,0.6838235294117647,"and attains better or comparable performance. We can adapt the backbone network with different
265"
CONCLUSION,0.6856617647058824,"flexibility using low-ranks based on the complexity of the domain/task. We conducted experiments
266"
CONCLUSION,0.6875,"with different backbone architectures, and our work can be extended to transformer-based architecture.
267"
CONCLUSION,0.6893382352941176,"Furthermore, we demonstrate experiments with FTN on image generation.
268"
REFERENCES,0.6911764705882353,"References
269"
REFERENCES,0.6930147058823529,"[1] A. Mallya, D. Davis, and S. Lazebnik, “Piggyback: Adapting a single network to multiple tasks
270"
REFERENCES,0.6948529411764706,"by learning to mask weights,” in Proceedings of the European Conference on Computer Vision
271"
REFERENCES,0.6966911764705882,"(ECCV), 2018, pp. 67–82.
272"
REFERENCES,0.6985294117647058,"[2] A. Mallya and S. Lazebnik, “Packnet: Adding multiple tasks to a single network by iterative
273"
REFERENCES,0.7003676470588235,"pruning,” in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
274"
REFERENCES,0.7022058823529411,"2018, pp. 7765–7773.
275"
REFERENCES,0.7040441176470589,"[3] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Learning multiple visual domains with residual
276"
REFERENCES,0.7058823529411765,"adapters,” Advances in neural information processing systems, vol. 30, 2017.
277"
REFERENCES,0.7077205882352942,"[4] R. Berriel, S. Lathuillere, M. Nabi, T. Klein, T. Oliveira-Santos, N. Sebe, and E. Ricci, “Budget-
278"
REFERENCES,0.7095588235294118,"aware adapters for multi-domain learning,” in Proceedings of the IEEE/CVF International
279"
REFERENCES,0.7113970588235294,"Conference on Computer Vision, 2019, pp. 382–391.
280"
REFERENCES,0.7132352941176471,"[5] M. Wallingford, H. Li, A. Achille, A. Ravichandran, C. Fowlkes, R. Bhotika, and S. Soatto,
281"
REFERENCES,0.7150735294117647,"“Task adaptive parameter sharing for multi-task learning,” in Proceedings of the IEEE/CVF
282"
REFERENCES,0.7169117647058824,"Conference on Computer Vision and Pattern Recognition, 2022, pp. 7561–7570.
283"
REFERENCES,0.71875,"[6] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Efficient parametrization of multi-domain deep neural
284"
REFERENCES,0.7205882352941176,"networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
285"
REFERENCES,0.7224264705882353,"2018, pp. 8119–8127.
286"
REFERENCES,0.7242647058823529,"[7] M. Mancini, E. Ricci, B. Caputo, and S. Rota Bulo, “Adding new tasks to a single network with
287"
REFERENCES,0.7261029411764706,"weight transformations using binary masks,” in Proceedings of the European Conference on
288"
REFERENCES,0.7279411764705882,"Computer Vision (ECCV) Workshops, 2018, pp. 0–0.
289"
REFERENCES,0.7297794117647058,"[8] X. Sun, R. Panda, R. Feris, and K. Saenko, “Adashare: Learning what to share for efficient
290"
REFERENCES,0.7316176470588235,"deep multi-task learning,” Advances in Neural Information Processing Systems, vol. 33, pp.
291"
REFERENCES,0.7334558823529411,"8728–8740, 2020.
292"
REFERENCES,0.7352941176470589,"[9] M. Kanakis, D. Bruggemann, S. Saha, S. Georgoulis, A. Obukhov, and L. V. Gool, “Repa-
293"
REFERENCES,0.7371323529411765,"rameterizing convolutions for incremental multi-task learning without task interference,” in
294"
REFERENCES,0.7389705882352942,"European Conference on Computer Vision.
Springer, 2020, pp. 689–707.
295"
REFERENCES,0.7408088235294118,"[10] K.-K. Maninis, I. Radosavovic, and I. Kokkinos, “Attentive single-tasking of multiple tasks,” in
296"
REFERENCES,0.7426470588235294,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019,
297"
REFERENCES,0.7444852941176471,"pp. 1851–1860.
298"
REFERENCES,0.7463235294117647,"[11] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative domain adaptation,”
299"
REFERENCES,0.7481617647058824,"in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp.
300"
REFERENCES,0.75,"7167–7176.
301"
REFERENCES,0.7518382352941176,"[12] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan, “Deep hashing network for
302"
REFERENCES,0.7536764705882353,"unsupervised domain adaptation,” in Proceedings of the IEEE conference on computer vision
303"
REFERENCES,0.7555147058823529,"and pattern recognition, 2017, pp. 5018–5027.
304"
REFERENCES,0.7573529411764706,"[13] B. Mustafa, A. Loh, J. Freyberg, P. MacWilliams, M. Wilson, S. M. McKinney, M. Sieniek,
305"
REFERENCES,0.7591911764705882,"J. Winkens, Y. Liu, P. Bui et al., “Supervised transfer learning at scale for medical imaging,”
306"
REFERENCES,0.7610294117647058,"arXiv preprint arXiv:2101.05913, 2021.
307"
REFERENCES,0.7628676470588235,"[14] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby, “Big transfer
308"
REFERENCES,0.7647058823529411,"(bit): General visual representation learning,” in Computer Vision–ECCV 2020: 16th European
309"
REFERENCES,0.7665441176470589,"Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16.
Springer, 2020, pp.
310"
REFERENCES,0.7683823529411765,"491–507.
311"
REFERENCES,0.7702205882352942,"[15] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, and J. Malik, “Side-tuning: a baseline for network
312"
REFERENCES,0.7720588235294118,"adaptation via additive side networks,” in Computer Vision–ECCV 2020: 16th European
313"
REFERENCES,0.7738970588235294,"Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16.
Springer, 2020, pp.
314"
REFERENCES,0.7757352941176471,"698–714.
315"
REFERENCES,0.7775735294117647,"[16] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, “Cross-stitch networks for multi-task
316"
REFERENCES,0.7794117647058824,"learning,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
317"
REFERENCES,0.78125,"2016, pp. 3994–4003.
318"
REFERENCES,0.7830882352941176,"[17] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard, “Latent multi-task architecture learning,”
319"
REFERENCES,0.7849264705882353,"in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp.
320"
REFERENCES,0.7867647058823529,"4822–4829.
321"
REFERENCES,0.7886029411764706,"[18] Y. Gao, J. Ma, M. Zhao, W. Liu, and A. L. Yuille, “Nddr-cnn: Layerwise feature fusing in multi-
322"
REFERENCES,0.7904411764705882,"task cnns by neural discriminative dimensionality reduction,” in Proceedings of the IEEE/CVF
323"
REFERENCES,0.7922794117647058,"Conference on Computer Vision and Pattern Recognition, 2019, pp. 3205–3214.
324"
REFERENCES,0.7941176470588235,"[19] G. Strezoski, N. v. Noord, and M. Worring, “Many task learning with task routing,” in Proceed-
325"
REFERENCES,0.7959558823529411,"ings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1375–1384.
326"
REFERENCES,0.7977941176470589,"[20] J. Liang, E. Meyerson, and R. Miikkulainen, “Evolutionary architecture search for deep multi-
327"
REFERENCES,0.7996323529411765,"task networks,” in Proceedings of the genetic and evolutionary computation conference, 2018,
328"
REFERENCES,0.8014705882352942,"pp. 466–473.
329"
REFERENCES,0.8033088235294118,"[21] Y. Gao, H. Bai, Z. Jie, J. Ma, K. Jia, and W. Liu, “Mtl-nas: Task-agnostic neural architecture
330"
REFERENCES,0.8051470588235294,"search towards general-purpose multi-task learning,” in IEEE Conference on Computer Vision
331"
REFERENCES,0.8069852941176471,"and Pattern Recognition (CVPR), 2020.
332"
REFERENCES,0.8088235294117647,"[22] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-
333"
REFERENCES,0.8106617647058824,"task learning,” Advances in Neural Information Processing Systems, vol. 33, pp. 5824–5836,
334"
REFERENCES,0.8125,"2020.
335"
REFERENCES,0.8143382352941176,"[23] Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou, “Revisiting batch normalization for practical domain
336"
REFERENCES,0.8161764705882353,"adaptation,” arXiv preprint arXiv:1603.04779, 2016.
337"
REFERENCES,0.8180147058823529,"[24] Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris, “Spottune: transfer learning
338"
REFERENCES,0.8198529411764706,"through adaptive fine-tuning,” in Proceedings of the IEEE/CVF conference on computer vision
339"
REFERENCES,0.8216911764705882,"and pattern recognition, 2019, pp. 4805–4814.
340"
REFERENCES,0.8235294117647058,"[25] S. Liu, E. Johns, and A. J. Davison, “End-to-end multi-task learning with attention,” in Pro-
341"
REFERENCES,0.8253676470588235,"ceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp.
342"
REFERENCES,0.8272058823529411,"1871–1880.
343"
REFERENCES,0.8290441176470589,"[26] S. Vandenhende, S. Georgoulis, B. De Brabandere, and L. Van Gool, “Branched multi-task
344"
REFERENCES,0.8308823529411765,"networks: Deciding what layers to share,” Proceedings BMVC, 20202.
345"
REFERENCES,0.8327205882352942,"[27] D. Bruggemann, M. Kanakis, S. Georgoulis, and L. Van Gool, “Automated search for resource-
346"
REFERENCES,0.8345588235294118,"efficient branched multi-task networks,” Proceedings BMVC, 2020.
347"
REFERENCES,0.8363970588235294,"[28] P. Guo, C.-Y. Lee, and D. Ulbricht, “Learning to branch for multi-task learning,” in International
348"
REFERENCES,0.8382352941176471,"Conference on Machine Learning.
PMLR, 2020, pp. 3854–3863.
349"
REFERENCES,0.8400735294117647,"[29] D. Xu, W. Ouyang, X. Wang, and N. Sebe, “Pad-net: Multi-tasks guided prediction-and-
350"
REFERENCES,0.8419117647058824,"distillation network for simultaneous depth estimation and scene parsing,” in Proceedings of the
351"
REFERENCES,0.84375,"IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 675–684.
352"
REFERENCES,0.8455882352941176,"[30] Z. Zhang, Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang, “Pattern-affinitive propagation across
353"
REFERENCES,0.8474264705882353,"depth, surface normal and semantic segmentation,” in Proceedings of the IEEE/CVF conference
354"
REFERENCES,0.8492647058823529,"on computer vision and pattern recognition, 2019, pp. 4106–4115.
355"
REFERENCES,0.8511029411764706,"[31] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, and J. Yang, “Joint task-recursive learning for semantic
356"
REFERENCES,0.8529411764705882,"segmentation and depth estimation,” in Proceedings of the European Conference on Computer
357"
REFERENCES,0.8547794117647058,"Vision (ECCV), 2018, pp. 235–251.
358"
REFERENCES,0.8566176470588235,"[32] S. Vandenhende, S. Georgoulis, and L. V. Gool, “Mti-net: Multi-scale task interaction networks
359"
REFERENCES,0.8584558823529411,"for multi-task learning,” in European Conference on Computer Vision.
Springer, 2020, pp.
360"
REFERENCES,0.8602941176470589,"527–543.
361"
REFERENCES,0.8621323529411765,"[33] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Gradnorm: Gradient normalization
362"
REFERENCES,0.8639705882352942,"for adaptive loss balancing in deep multitask networks,” in International conference on machine
363"
REFERENCES,0.8658088235294118,"learning.
PMLR, 2018, pp. 794–803.
364"
REFERENCES,0.8676470588235294,"[34] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for
365"
REFERENCES,0.8694852941176471,"scene geometry and semantics,” in Proceedings of the IEEE conference on computer vision and
366"
REFERENCES,0.8713235294117647,"pattern recognition, 2018, pp. 7482–7491.
367"
REFERENCES,0.8731617647058824,"[35] M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-Fei, “Dynamic task prioritization for
368"
REFERENCES,0.875,"multitask learning,” in Proceedings of the European conference on computer vision (ECCV),
369"
REFERENCES,0.8768382352941176,"2018, pp. 270–287.
370"
REFERENCES,0.8786764705882353,"[36] X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong, “Pareto multi-task learning,” Advances
371"
REFERENCES,0.8805147058823529,"in neural information processing systems, vol. 32, 2019.
372"
REFERENCES,0.8823529411764706,"[37] Z. Chen, J. Ngiam, Y. Huang, T. Luong, H. Kretzschmar, Y. Chai, and D. Anguelov, “Just pick
373"
REFERENCES,0.8841911764705882,"a sign: Optimizing deep multitask models with gradient sign dropout,” Advances in Neural
374"
REFERENCES,0.8860294117647058,"Information Processing Systems, vol. 33, pp. 2039–2050, 2020.
375"
REFERENCES,0.8878676470588235,"[38] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with gumbel-softmax,” in
376"
REFERENCES,0.8897058823529411,"International Conference on Learning Representations, 2017.
377"
REFERENCES,0.8915441176470589,"[39] K. Li, C. Liu, H. Zhao, Y. Zhang, and Y. Fu, “Ecacl: A holistic framework for semi-supervised
378"
REFERENCES,0.8933823529411765,"domain adaptation,” in Proceedings of the IEEE/CVF International Conference on Computer
379"
REFERENCES,0.8952205882352942,"Vision, 2021, pp. 8578–8587.
380"
REFERENCES,0.8970588235294118,"[40] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category models to new domains,”
381"
REFERENCES,0.8988970588235294,"in European Conference on Computer Vision (ECCV).
Springer, 2010, pp. 213–226.
382"
REFERENCES,0.9007352941176471,"[41] Y. Zhao, H. Ali, and R. Vidal, “Stretching domain adaptation: How far is too far?” arXiv
383"
REFERENCES,0.9025735294117647,"preprint arXiv:1712.02286, 2017.
384"
REFERENCES,0.9044117647058824,"[42] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
385"
REFERENCES,0.90625,"M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16
386"
REFERENCES,0.9080882352941176,"words: Transformers for image recognition at scale,” International Conference on Learning
387"
REFERENCES,0.9099264705882353,"Representations, 2021.
388"
REFERENCES,0.9117647058823529,"[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing
389"
REFERENCES,0.9136029411764706,"internal covariate shift,” in International conference on machine learning.
pmlr, 2015, pp.
390"
REFERENCES,0.9154411764705882,"448–456.
391"
REFERENCES,0.9172794117647058,"[44] Q. Pham, C. Liu, and H. Steven, “Continual normalization: Rethinking batch normalization for
392"
REFERENCES,0.9191176470588235,"online continual learning,” in International Conference on Learning Representations, 2022.
393"
REFERENCES,0.9209558823529411,"[45] X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural
394"
REFERENCES,0.9227941176470589,"networks,” in Proceedings of the thirteenth international conference on artificial intelligence
395"
REFERENCES,0.9246323529411765,"and statistics.
JMLR Workshop and Conference Proceedings, 2010, pp. 249–256.
396"
REFERENCES,0.9264705882352942,"[46] M.-E. Nilsback and A. Zisserman, “Automated flower classification over a large number of
397"
REFERENCES,0.9283088235294118,"classes,” in 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing.
398"
REFERENCES,0.9301470588235294,"IEEE, 2008, pp. 722–729.
399"
REFERENCES,0.9319852941176471,"[47] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations for fine-grained
400"
REFERENCES,0.9338235294117647,"categorization,” in Proceedings of the IEEE international conference on computer vision
401"
REFERENCES,0.9356617647058824,"workshops, 2013, pp. 554–561.
402"
REFERENCES,0.9375,"[48] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” ACM Transactions on
403"
REFERENCES,0.9393382352941176,"graphics (TOG), vol. 31, no. 4, pp. 1–10, 2012.
404"
REFERENCES,0.9411764705882353,"[49] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd birds-200-2011
405"
REFERENCES,0.9430147058823529,"dataset,” 2011.
406"
REFERENCES,0.9448529411764706,"[50] B. Saleh and A. Elgammal, “Large-scale classification of fine-art paintings: Learning the right
407"
REFERENCES,0.9466911764705882,"metric on the right feature,” International Journal for Digital Art History, no. 2, 2016.
408"
REFERENCES,0.9485294117647058,"[51] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment matching for multi-source
409"
REFERENCES,0.9503676470588235,"domain adaptation,” in Proceedings of the IEEE/CVF international conference on computer
410"
REFERENCES,0.9522058823529411,"vision, 2019, pp. 1406–1415.
411"
REFERENCES,0.9540441176470589,"[52] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in
412"
REFERENCES,0.9558823529411765,"Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.
413"
REFERENCES,0.9577205882352942,"770–778.
414"
REFERENCES,0.9595588235294118,"[53] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation and support inference
415"
REFERENCES,0.9613970588235294,"from rgbd images.” ECCV (5), vol. 7576, pp. 746–760, 2012.
416"
REFERENCES,0.9632352941176471,"[54] D. R. Martin, C. C. Fowlkes, and J. Malik, “Learning to detect natural image boundaries using
417"
REFERENCES,0.9650735294117647,"local brightness, color, and texture cues,” IEEE transactions on pattern analysis and machine
418"
REFERENCES,0.9669117647058824,"intelligence, vol. 26, no. 5, pp. 530–549, 2004.
419"
REFERENCES,0.96875,"[55] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder with atrous
420"
REFERENCES,0.9705882352941176,"separable convolution for semantic image segmentation,” in Proceedings of the European
421"
REFERENCES,0.9724264705882353,"conference on computer vision (ECCV), 2018, pp. 801–818.
422"
REFERENCES,0.9742647058823529,"[56] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,” in Proceedings of the IEEE
423"
REFERENCES,0.9761029411764706,"conference on computer vision and pattern recognition, 2018, pp. 9446–9454.
424"
REFERENCES,0.9779411764705882,"[57] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative visual manipulation
425"
REFERENCES,0.9797794117647058,"on the natural image manifold,” in Computer Vision–ECCV 2016: 14th European Conference,
426"
REFERENCES,0.9816176470588235,"Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14.
Springer, 2016,
427"
REFERENCES,0.9834558823529411,"pp. 597–613.
428"
REFERENCES,0.9852941176470589,"[58] X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, and P. Luo, “Exploiting deep generative prior
429"
REFERENCES,0.9871323529411765,"for versatile image restoration and manipulation,” IEEE Transactions on Pattern Analysis and
430"
REFERENCES,0.9889705882352942,"Machine Intelligence, vol. 44, no. 11, pp. 7474–7489, 2021.
431"
REFERENCES,0.9908088235294118,"[59] P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays, “Transient attributes for high-level un-
432"
REFERENCES,0.9926470588235294,"derstanding and editing of outdoor scenes,” ACM Transactions on Graphics (proceedings of
433"
REFERENCES,0.9944852941176471,"SIGGRAPH), vol. 33, no. 4, 2014.
434"
REFERENCES,0.9963235294117647,"[60] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for high fidelity natural
435"
REFERENCES,0.9981617647058824,"image synthesis,” in International Conference on Learning Representations, 2019.
436"
