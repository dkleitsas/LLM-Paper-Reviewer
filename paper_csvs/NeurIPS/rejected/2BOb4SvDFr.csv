Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0014577259475218659,"We explore two applications of Min-Max-Jump distance (MMJ distance): MMJ-
1"
ABSTRACT,0.0029154518950437317,"based K-means and MMJ-based internal clustering evaluation index. K-means and
2"
ABSTRACT,0.004373177842565598,"its variants are possibly the most popular clustering approach. A key drawback of
3"
ABSTRACT,0.0058309037900874635,"K-means is that it cannot deal with data sets that are not the union of well-separated,
4"
ABSTRACT,0.007288629737609329,"spherical clusters. MMJ-based K-means proposed in this paper overcomes this
5"
ABSTRACT,0.008746355685131196,"demerit of K-means, so that it can handle irregularly shaped clusters. Evaluation (or
6"
ABSTRACT,0.01020408163265306,"""validation"") of clustering results is fundamental to clustering and thus to machine
7"
ABSTRACT,0.011661807580174927,"learning. Popular internal clustering evaluation indices like Silhouette coefficient,
8"
ABSTRACT,0.013119533527696793,"Davies–Bouldin index, and Calinski-Harabasz index performs poorly in evaluating
9"
ABSTRACT,0.014577259475218658,"irregularly shaped clusters. MMJ-based internal clustering evaluation index uses
10"
ABSTRACT,0.016034985422740525,"MMJ distance and Semantic Center of Mass (SCOM) to revise the indices, so that
11"
ABSTRACT,0.01749271137026239,"it can evaluate irregularly shaped data. An experiment shows introducing MMJ
12"
ABSTRACT,0.018950437317784258,"distance to internal clustering evaluation index, can systematically improve the
13"
ABSTRACT,0.02040816326530612,"performance. We also devise two algorithms for calculating MMJ distance.
14"
INTRODUCTION,0.021865889212827987,"1
Introduction
15"
INTRODUCTION,0.023323615160349854,"Distance is a numerical measurement of how far apart objects or points are. It is usually formalized
16"
INTRODUCTION,0.02478134110787172,"in mathematics using the notion of a metric space. A metric space is a set together with a notion of
17"
INTRODUCTION,0.026239067055393587,"distance between its elements, usually called points. The distance is measured by a function called
18"
INTRODUCTION,0.027696793002915453,"a metric or distance function. Metric spaces are the most general setting for studying many of the
19"
INTRODUCTION,0.029154518950437316,"concepts of mathematical analysis and geometry.
20"
INTRODUCTION,0.030612244897959183,"In this paper, we introduce two algorithms for calculating Min-Max-Jump distance (MMJ distance)
21"
INTRODUCTION,0.03206997084548105,"and explore two applications of it. Including MMJ-based K-means (MMJ-K-means) and MMJ-based
22"
INTRODUCTION,0.033527696793002916,"internal clustering evaluation index.
23"
INTRODUCTION,0.03498542274052478,"MMJ-K-means improves K-means, so that it can handle irregularly shaped clusters. We claim MMJ-
24"
INTRODUCTION,0.03644314868804665,"CH is the SOTA (state-of-the-art) internal clustering evaluation index, which achieves an accuracy of
25"
INTRODUCTION,0.037900874635568516,"90/145. MMJ-CH is one of the MMJ-based internal clustering evaluation indices.
26"
RELATED WORK,0.03935860058309038,"2
RELATED WORK
27"
DIFFERENT DISTANCE METRICS,0.04081632653061224,"2.1
Different distance metrics
28"
DIFFERENT DISTANCE METRICS,0.04227405247813411,"Many distance measures have been proposed in literature, such as Euclidean distance or cosine
29"
DIFFERENT DISTANCE METRICS,0.043731778425655975,"similarity. These distance measures often be found in algorithms like k-NN, UMAP, HDBSCAN,
30"
DIFFERENT DISTANCE METRICS,0.04518950437317784,"etc. The most common metric is Euclidean distance. Cosine similarity is often used as a way to
31"
DIFFERENT DISTANCE METRICS,0.04664723032069971,"counteract Euclidean distance’s problem in high dimensionality. The cosine similarity is the cosine
32"
DIFFERENT DISTANCE METRICS,0.048104956268221574,"of the angle between two vectors.
33"
DIFFERENT DISTANCE METRICS,0.04956268221574344,"Hamming distance is the number of values that are different between two vectors. It is typically used
34"
DIFFERENT DISTANCE METRICS,0.05102040816326531,"to compare two binary strings of equal length (1).
35"
DIFFERENT DISTANCE METRICS,0.052478134110787174,"Manhattan distance is a geometry whose usual distance function or metric of Euclidean geometry
36"
DIFFERENT DISTANCE METRICS,0.05393586005830904,"is replaced by a new metric in which the distance between two points is the sum of the absolute
37"
DIFFERENT DISTANCE METRICS,0.05539358600583091,"differences of their Cartesian coordinates (2).
38"
DIFFERENT DISTANCE METRICS,0.056851311953352766,"Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate
39"
DIFFERENT DISTANCE METRICS,0.05830903790087463,"dimension (3).
40"
DIFFERENT DISTANCE METRICS,0.0597667638483965,"Minkowski distance or Minkowski metric is a metric in a normed vector space which can be
41"
DIFFERENT DISTANCE METRICS,0.061224489795918366,"considered as a generalization of both the Euclidean distance and the Manhattan distance (4).
42"
DIFFERENT DISTANCE METRICS,0.06268221574344024,"Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for gauging the
43"
DIFFERENT DISTANCE METRICS,0.0641399416909621,"similarity and diversity of sample sets (5).
44"
DIFFERENT DISTANCE METRICS,0.06559766763848396,"Haversine distance is the distance between two points on a sphere given their longitudes and latitudes.
45"
DIFFERENT DISTANCE METRICS,0.06705539358600583,"It is similar to Euclidean distance in that it calculates the shortest path between two points. The main
46"
DIFFERENT DISTANCE METRICS,0.06851311953352769,"difference is that there is no straight line, since the assumption is that the two points are on a sphere
47"
DIFFERENT DISTANCE METRICS,0.06997084548104957,"(6).
48"
K-MEANS,0.07142857142857142,"2.2
K-means
49"
K-MEANS,0.0728862973760933,"K-means (7) and its variants (8; 9; 10) are possibly the most well-liked clustering approach. K-means
50"
K-MEANS,0.07434402332361516,"divides the data into K groups, where K is a hyper-parameter to be optimized. It aims to reduce the
51"
K-MEANS,0.07580174927113703,"within-cluster dissimilarity. While popular, K-means and its variants perform poorly for data sets
52"
K-MEANS,0.07725947521865889,"that are not the union of well-separated, spherical clusters. MMJ-based K-means (MMJ-K-means)
53"
K-MEANS,0.07871720116618076,"proposed in this paper overcomes this demerit of K-means, so that it can handle irregularly shaped
54"
K-MEANS,0.08017492711370262,"clusters.
55"
INTERNAL CLUSTERING EVALUATION INDEX,0.08163265306122448,"2.3
Internal clustering evaluation index
56"
INTERNAL CLUSTERING EVALUATION INDEX,0.08309037900874636,"Evaluation (or ""validation"") of clustering results is as difficult as the clustering itself (11). Popular
57"
INTERNAL CLUSTERING EVALUATION INDEX,0.08454810495626822,"approaches involve ""internal"" evaluation and ""external"" evaluation. In internal evaluation, a clustering
58"
INTERNAL CLUSTERING EVALUATION INDEX,0.08600583090379009,"result is evaluated based on the data that was clustered itself. Popular internal evaluation indices
59"
INTERNAL CLUSTERING EVALUATION INDEX,0.08746355685131195,"are Davies-Bouldin index (12), Silhouette coefficient (13), Dunn index (14), and Calinski-Harabasz
60"
INTERNAL CLUSTERING EVALUATION INDEX,0.08892128279883382,"index (15) etc. In external evaluation, the clustering result is compared to an existing ""ground truth""
61"
INTERNAL CLUSTERING EVALUATION INDEX,0.09037900874635568,"classification, such as the Rand index (16). However, knowledge of the ground truth classes is almost
62"
INTERNAL CLUSTERING EVALUATION INDEX,0.09183673469387756,"never available in practice.
63"
INTERNAL CLUSTERING EVALUATION INDEX,0.09329446064139942,"In Section 5.2, an experiment shows introducing Min-Max-Jump (MMJ) distance to internal clustering
64"
INTERNAL CLUSTERING EVALUATION INDEX,0.09475218658892129,"evaluation index, can systematically improve the performance.
65"
PATH-BASED DISTANCES,0.09620991253644315,"2.4
Path-based distances
66"
PATH-BASED DISTANCES,0.09766763848396501,"Euclidean distances are frequently used in machine learning and clustering methods to compare
67"
PATH-BASED DISTANCES,0.09912536443148688,"points. However, the distance is data-independent, and not tailored to the geometry of the data. Many
68"
PATH-BASED DISTANCES,0.10058309037900874,"metrics that are data-dependent have been devised, such as diffusion distances (17) and path-based
69"
PATH-BASED DISTANCES,0.10204081632653061,"distances (18; 19). MMJ distance is a path-based distance.
70"
DEFINITION OF MIN-MAX-JUMP,0.10349854227405247,"3
Definition of Min-Max-Jump
71"
DEFINITION OF MIN-MAX-JUMP,0.10495626822157435,"Definition 1. Min-Max-Jump distance (MMJ distance)
72"
DEFINITION OF MIN-MAX-JUMP,0.10641399416909621,"Ωis a set of points (at least one). For any pair of points p, q ∈Ω, the distance between p and q is
73"
DEFINITION OF MIN-MAX-JUMP,0.10787172011661808,"defined by a distance function d(p,q) (such as Euclidean distance). i, j ∈Ω, Ψ(i,j,n,Ω) is a path from
74"
DEFINITION OF MIN-MAX-JUMP,0.10932944606413994,"point i to point j, which has length of n points (see Table 1). Θ(i,j,Ω) is the set of all paths from point
75"
DEFINITION OF MIN-MAX-JUMP,0.11078717201166181,"i to point j. Therefore, Ψ(i,j,n,Ω) ∈Θ(i,j,Ω). max_jump( Ψ(i,j,n,Ω) ) is the maximum jump in path
76"
DEFINITION OF MIN-MAX-JUMP,0.11224489795918367,"Ψ(i,j,n,Ω).
77"
DEFINITION OF MIN-MAX-JUMP,0.11370262390670553,"The Min-Max-Jump distance between a pair of points i, j, which belong to Ω, is defined as:
78"
DEFINITION OF MIN-MAX-JUMP,0.1151603498542274,Table 1: Table of notations
DEFINITION OF MIN-MAX-JUMP,0.11661807580174927,"Ω
A set of N points, with each point indexed from
1 to N;"
DEFINITION OF MIN-MAX-JUMP,0.11807580174927114,"Ω[1,n]
The first n points of Ω, indexed from 1 to n;"
DEFINITION OF MIN-MAX-JUMP,0.119533527696793,"Ωn+1
The (n + 1)th point of Ω;"
DEFINITION OF MIN-MAX-JUMP,0.12099125364431487,"Ci
A cluster of points that is a subset of Ω;"
DEFINITION OF MIN-MAX-JUMP,0.12244897959183673,"ξi
One-SCOM of Ci;"
DEFINITION OF MIN-MAX-JUMP,0.1239067055393586,"Ω+ p
Set Ωplus one new point p. Since p /∈Ω, if Ω
has N points, this new set now has N +1 points;"
DEFINITION OF MIN-MAX-JUMP,0.12536443148688048,"Ψ(i,j,n,Ω)
Ψ(i,j,n,Ω) is a sequence from point i to point j,
which has length of n points. All the points in
the sequence must belong to set Ω. That is to
say, it is a path starts from i, and ends with j.
For convenience, the path is not allowed to have
loops, unless the start and the end is the same
point;"
DEFINITION OF MIN-MAX-JUMP,0.12682215743440234,"d(i, j)
d(i, j) is a distance metric between pair of
points i and j, such as Euclidean distance;"
DEFINITION OF MIN-MAX-JUMP,0.1282798833819242,"max_jump( Ψ(i,j,n,Ω) )
max_jump( Ψ(i,j,n,Ω) ) is the maximum jump
in path Ψ(i,j,n,Ω). A jump is the distance from
two consecutive points p and q in the path;"
DEFINITION OF MIN-MAX-JUMP,0.12973760932944606,"Θ(i,j,Ω)
Θ(i,j,Ω) is the set of all paths from point i to
point j. A path in Θ(i,j,Ω) can have arbitrary
number of points (at least two). All the points
in a path must belong to set Ω;"
DEFINITION OF MIN-MAX-JUMP,0.13119533527696792,"MMJ(i, j | Ω)
MMJ(i, j | Ω) is the MMJ distance between
point i and j, where Ωis the Context of the
MMJ distance;"
DEFINITION OF MIN-MAX-JUMP,0.1326530612244898,"Mk,Ω[1,k]
Mk,Ω[1,k] is the pairwise MMJ distance matrix
of Ω[1,k], which has shape k × k. The MMJ
distances are under the Context of Ω[1,k];"
DEFINITION OF MIN-MAX-JUMP,0.13411078717201166,"MΩ
The pairwise MMJ distance matrix of Ω, MΩ=
MN,Ω[1,N];"
DEFINITION OF MIN-MAX-JUMP,0.13556851311953352,"Π = {max_jump(ϵ) | ϵ ∈Θ(i,j,Ω)}
(1)"
DEFINITION OF MIN-MAX-JUMP,0.13702623906705538,"MMJ(i, j | Ω) = min(Π)
(2)"
DEFINITION OF MIN-MAX-JUMP,0.13848396501457727,"Where ϵ is a path from point i to point j, max_jump(ϵ) is the maximum jump in path ϵ. Π is the set
79"
DEFINITION OF MIN-MAX-JUMP,0.13994169096209913,"of all maximum jumps.
min(Π) is the minimum of Set Π.
80"
DEFINITION OF MIN-MAX-JUMP,0.141399416909621,"Set Ωis called the Context of the Min-Max-Jump distance. It is easy to check MMJ(i, i | Ω) = 0.
81 82"
DEFINITION OF MIN-MAX-JUMP,0.14285714285714285,"In summary, Min-Max-Jump distance is the minimum of maximum jumps of all path between a pair
83"
DEFINITION OF MIN-MAX-JUMP,0.14431486880466474,"of points, under the Context of a set of points.
84"
DEFINITION OF MIN-MAX-JUMP,0.1457725947521866,"Similar distances have actually been studied in many places in the literature, including the maximum
85"
DEFINITION OF MIN-MAX-JUMP,0.14723032069970846,"capacity path problem, the widest path problem, the bottleneck edge query problem, the minimax
86"
DEFINITION OF MIN-MAX-JUMP,0.14868804664723032,"path problem, the bottleneck shortest path problem, and the longest-leg path distance (LLPD)
87"
DEFINITION OF MIN-MAX-JUMP,0.15014577259475217,"(20; 21; 22; 23).
88 a d c b 19
8 17 12
9 28"
DEFINITION OF MIN-MAX-JUMP,0.15160349854227406,Figure 1: An example
DEFINITION OF MIN-MAX-JUMP,0.15306122448979592,"There is a minor difference between Min-Max-Jump distance and other similar distances: Min-Max-
89"
DEFINITION OF MIN-MAX-JUMP,0.15451895043731778,"Jump distance stresses the context of the distance. The context is like the condition in conditional
90"
DEFINITION OF MIN-MAX-JUMP,0.15597667638483964,"probability. The difference becomes non-trivial when we need to calculate the pairwise MMJ distance
91"
DEFINITION OF MIN-MAX-JUMP,0.15743440233236153,"matrix of a set S, under the context of its superset X, such as in Section 6.3 of (24). A set Ωis a
92"
DEFINITION OF MIN-MAX-JUMP,0.1588921282798834,"superset of another set B if all elements of the set B are elements of the set Ω.
93"
AN EXAMPLE,0.16034985422740525,"3.1
An example
94"
AN EXAMPLE,0.1618075801749271,"Suppose Set Ωis composed of the four points in Figure 1. There are five (non-looped) paths from
95"
AN EXAMPLE,0.16326530612244897,"point a to point c in Figure 1:
96"
AN EXAMPLE,0.16472303206997085,"1. a →c, the maximum jump is 28;
97"
AN EXAMPLE,0.1661807580174927,"2. a →b →c, the maximum jump is 19;
98"
AN EXAMPLE,0.16763848396501457,"3. a →d →c, the maximum jump is 17;
99"
AN EXAMPLE,0.16909620991253643,"4. a →b →d →c, the maximum jump is 19;
100"
AN EXAMPLE,0.17055393586005832,"5. a →d →b →c, the maximum jump is 12.
101"
AN EXAMPLE,0.17201166180758018,"According to Definition 1, MMJ(a, c | Ω) = 12.
102"
AN EXAMPLE,0.17346938775510204,"To understand Min-Max-Jump distance, imagine someone is traveling by jumping in Ω. Suppose
103"
AN EXAMPLE,0.1749271137026239,"MMJ(i, j | Ω) = δ. If the person wants to reach j from i, she must have the ability of jumping at
104"
AN EXAMPLE,0.17638483965014579,"least δ. Otherwise, j is unreachable from i for her. Whether the distance to a point is ""far"" or ""near""
105"
AN EXAMPLE,0.17784256559766765,"is measured by how far (or how high) it requires a person to jump. If the requirement is large, then
106"
AN EXAMPLE,0.1793002915451895,"the point is ""far"", otherwise, it is ""near.""
107"
PROPERTIES OF MMJ DISTANCE,0.18075801749271136,"3.2
Properties of MMJ distance
108"
PROPERTIES OF MMJ DISTANCE,0.18221574344023322,"Theorem 1. Suppose i, j, p, q ∈Ω,
109"
PROPERTIES OF MMJ DISTANCE,0.1836734693877551,"MMJ(i, j | Ω) = δ
(3) 110"
PROPERTIES OF MMJ DISTANCE,0.18513119533527697,"d(i, p) < δ
(4)
111"
PROPERTIES OF MMJ DISTANCE,0.18658892128279883,"d(j, q) < δ
(5)"
PROPERTIES OF MMJ DISTANCE,0.1880466472303207,"then,
112"
PROPERTIES OF MMJ DISTANCE,0.18950437317784258,"MMJ(p, q | Ω) = δ
(6)"
PROPERTIES OF MMJ DISTANCE,0.19096209912536444,"where d(x,y) is a distance function (Table 1).
113"
PROPERTIES OF MMJ DISTANCE,0.1924198250728863,"Proof. MMJ(i, j | Ω) = δ is equivalent to ∃P ∈Θ(i,j,Ω), such that M(P) = δ, and ∀T ∈Θ(i,j,Ω),
114"
PROPERTIES OF MMJ DISTANCE,0.19387755102040816,"M(T) ≥δ, where Θ(i,j,Ω) is the set of all paths from point i to point j under context Ω. M(P) is
115"
PROPERTIES OF MMJ DISTANCE,0.19533527696793002,"the maximum jump in path P. We can assume MMJ(p, q | Ω) > δ and MMJ(p, q | Ω) < δ, then
116"
PROPERTIES OF MMJ DISTANCE,0.1967930029154519,"we will arrive to a contradiction in both cases.
117"
PROPERTIES OF MMJ DISTANCE,0.19825072886297376,"Theorem 2. Suppose r ∈{1, 2, . . . , n},
118"
PROPERTIES OF MMJ DISTANCE,0.19970845481049562,"f(t) = max(d(Ωn+1, Ωt), MMJ(Ωt, Ωr | Ω[1,n]))
(7)
119"
PROPERTIES OF MMJ DISTANCE,0.20116618075801748,"X = {f(t) | t ∈{1, 2, . . . , n}}
(8)"
PROPERTIES OF MMJ DISTANCE,0.20262390670553937,"then,
120"
PROPERTIES OF MMJ DISTANCE,0.20408163265306123,"MMJ(Ωn+1, Ωr | Ω[1,n+1]) = min(X)
(9)"
PROPERTIES OF MMJ DISTANCE,0.2055393586005831,"For the meaning of Ωt, Ωr, Ω[1,n], and Ω[1,n+1], see Table 1.
121"
PROPERTIES OF MMJ DISTANCE,0.20699708454810495,"Proof. There are n possibilities of the MMJ path from Ωn+1 to Ωr, under the context of Ω[1,n+1],
122"
PROPERTIES OF MMJ DISTANCE,0.20845481049562684,"set X enumerate them all. Each element of X is the maximum jump of each possibility. Therefore,
123"
PROPERTIES OF MMJ DISTANCE,0.2099125364431487,"according to the definition of MMJ distance, MMJ(Ωn+1, Ωr | Ω[1,n+1]) = min(X).
124"
PROPERTIES OF MMJ DISTANCE,0.21137026239067055,"Corollary 1. Suppose r ∈{1, 2, . . . , N}, p /∈Ω,
125"
PROPERTIES OF MMJ DISTANCE,0.21282798833819241,"f(t) = max(d(p, Ωt), MMJ(Ωt, Ωr | Ω))
(10) 126"
PROPERTIES OF MMJ DISTANCE,0.21428571428571427,"X = {f(t) | t ∈{1, 2, . . . , N}}
(11)"
PROPERTIES OF MMJ DISTANCE,0.21574344023323616,"then,
127"
PROPERTIES OF MMJ DISTANCE,0.21720116618075802,"MMJ(p, Ωr | Ω+ p) = min(X)
(12)"
PROPERTIES OF MMJ DISTANCE,0.21865889212827988,"For the meaning of Ω+ p, see Table 1.
128"
PROPERTIES OF MMJ DISTANCE,0.22011661807580174,"Proof. The proof follows the conclusion of Theorem 2.
129"
PROPERTIES OF MMJ DISTANCE,0.22157434402332363,"Theorem 3. Suppose i, j ∈{1, 2, . . . , n},
130"
PROPERTIES OF MMJ DISTANCE,0.2230320699708455,"x1 = MMJ(Ωi, Ωj | Ω[1,n])
(13) 131"
PROPERTIES OF MMJ DISTANCE,0.22448979591836735,"t1 = MMJ(Ωn+1, Ωi | Ω[1,n+1])
(14)
132"
PROPERTIES OF MMJ DISTANCE,0.2259475218658892,"t2 = MMJ(Ωn+1, Ωj | Ω[1,n+1])
(15)
133"
PROPERTIES OF MMJ DISTANCE,0.22740524781341107,"x2 = max(t1, t2)
(16)"
PROPERTIES OF MMJ DISTANCE,0.22886297376093295,"then,
134"
PROPERTIES OF MMJ DISTANCE,0.2303206997084548,"MMJ(Ωi, Ωj | Ω[1,n+1]) = min(x1, x2)
(17)"
PROPERTIES OF MMJ DISTANCE,0.23177842565597667,"Proof. There are two possibilities of the MMJ path from Ωi to Ωj, under the context of Ω[1,n+1]:
135"
PROPERTIES OF MMJ DISTANCE,0.23323615160349853,"Ωn+1 is in the path or it is not in the path. x2 is the min-max jump of the first possibility; x1 is the
136"
PROPERTIES OF MMJ DISTANCE,0.23469387755102042,"min-max jump of the second possibility. Therefore, according to the definition of MMJ distance,
137"
PROPERTIES OF MMJ DISTANCE,0.23615160349854228,"MMJ(Ωi, Ωj | Ω[1,n+1]) = min(x1, x2).
138"
CALCULATION OF MIN-MAX-JUMP DISTANCE,0.23760932944606414,"4
Calculation of Min-Max-Jump distance
139"
CALCULATION OF MIN-MAX-JUMP DISTANCE,0.239067055393586,"We propose two methods to calculate the pairwise Min-Max-Jump distance matrix of a dataset. There
140"
CALCULATION OF MIN-MAX-JUMP DISTANCE,0.24052478134110788,"are other methods for calculating or estimating it, such as a modified SLINK algorithm (25), or with
141"
CALCULATION OF MIN-MAX-JUMP DISTANCE,0.24198250728862974,"Cartesian trees (26; 27), or from a sequence of nearest neighbor graphs (23), or a modified version of
142"
CALCULATION OF MIN-MAX-JUMP DISTANCE,0.2434402332361516,"the Floyd–Warshall algorithm.
143"
MMJ DISTANCE BY RECURSION,0.24489795918367346,"4.1
MMJ distance by recursion
144"
MMJ DISTANCE BY RECURSION,0.24635568513119532,"The first method calculates MΩby recursion. MΩis the pairwise MMJ distance matrix of Ω(Table
145"
MMJ DISTANCE BY RECURSION,0.2478134110787172,"1). Mk,Ω[1,k] is the MMJ distance matrix of the first k points of Ω(Table 1). Note M2,Ω[1,2] is simple
146"
MMJ DISTANCE BY RECURSION,0.24927113702623907,"to calculate. MΩ= MN,Ω[1,N]. MΩis a N × N symmetric matrix. Rows and columns of MΩare
147"
MMJ DISTANCE BY RECURSION,0.25072886297376096,"indexed from 1 to N.
148"
MMJ DISTANCE BY RECURSION,0.2521865889212828,"Step 7 of Algorithm 1 can be calculated with the conclusion of Theorem 2; Step 12 of Algorithm 1
149"
MMJ DISTANCE BY RECURSION,0.2536443148688047,"can be calculated with the conclusion of Theorem 3.
150"
MMJ DISTANCE BY RECURSION,0.25510204081632654,"Algorithm 1 has complexity of O(n3), where n is the cardinality of Set Ω.
151"
MMJ DISTANCE BY RECURSION,0.2565597667638484,Algorithm 1 MMJ distance by recursion
MMJ DISTANCE BY RECURSION,0.25801749271137026,"Input: Ω
Output: MΩ"
MMJ DISTANCE BY RECURSION,0.2594752186588921,"1: function MMJ_BY_RECURSION(Ω)
2:
N ←length(Ω)
3:
Initialize MΩwith zeros
4:
Calculate M2,Ω[1,2], fill in MΩ[1, 2] and MΩ[2, 1]
5:
for n ←3 to N do
6:
for r ←1 to n −1 do
7:
Calculate MMJ(Ωn, Ωr | Ω[1,n]), fill in MΩ[n, r] and MΩ[r, n]
8:
end for
9:
for i ←1 to n −1 do
10:
for j ←1 to n −1 do
11:
if i < j then
12:
Calculate MMJ(Ωi, Ωj | Ω[1,n]), update MΩ[i, j] and MΩ[j, i]
13:
end if
14:
end for
15:
end for
16:
end for
17:
return MΩ
18: end function"
MMJ DISTANCE BY CALCULATION AND COPY,0.260932944606414,"4.2
MMJ distance by calculation and copy
152"
MMJ DISTANCE BY CALCULATION AND COPY,0.26239067055393583,"According to the conclusion of Theorem 1, there are many duplicated values in MΩ. So in the second
153"
MMJ DISTANCE BY CALCULATION AND COPY,0.26384839650145775,"method we can calculate the MMJ distance value in one position and copy it to other positions in
154"
MMJ DISTANCE BY CALCULATION AND COPY,0.2653061224489796,"MΩ.
155"
MMJ DISTANCE BY CALCULATION AND COPY,0.26676384839650147,"A well-known fact about MMJ distance is: ""the path between any two nodes in a minimum spanning
156"
MMJ DISTANCE BY CALCULATION AND COPY,0.26822157434402333,"tree (MST) is a minimax path."" A minimax path in an undirected graph is a path between two vertices
157"
MMJ DISTANCE BY CALCULATION AND COPY,0.2696793002915452,"v, w that minimizes the maximum weight of the edges on the path. That is to say, it is a MMJ path.
158"
MMJ DISTANCE BY CALCULATION AND COPY,0.27113702623906705,"By utilizing this fact, we propose Algorithm 2.
159"
MMJ DISTANCE BY CALCULATION AND COPY,0.2725947521865889,Algorithm 2 MMJ distance by Calculation and Copy
MMJ DISTANCE BY CALCULATION AND COPY,0.27405247813411077,"Input: Ω
Output: MΩ"
MMJ DISTANCE BY CALCULATION AND COPY,0.2755102040816326,"1: function MMJ_CALCULATION_AND_COPY(Ω)
2:
Initialize MΩwith zeros
3:
Construct a MST of Ω, noted T
4:
Sort edges of T from large to small, generate a list, noted L
5:
for e in L do
6:
Remove e from T. It will result in two connected sub-trees, T1 and T2;
7:
Traverse T1 and T2;
8:
For all pair of nodes (p, q), where p ∈T1, q ∈T2. Fill in MΩ[p, q] and MΩ[q, p] with the
weight of e.
9:
end for
10:
return MΩ
11: end function"
MMJ DISTANCE BY CALCULATION AND COPY,0.27696793002915454,"The complexity of Algorithm 2 is O(n2). Because the construction of a MST of a complete graph is
160"
MMJ DISTANCE BY CALCULATION AND COPY,0.2784256559766764,"O(n2). During the ""for"" part (Step 5 to 9) of the algorithm, it accesses each cell of MΩonly once.
161"
MMJ DISTANCE BY CALCULATION AND COPY,0.27988338192419826,"Unlike Algorithm 1, which accesses each cell of MΩfor O(n) times. The merit of the ""Calculation
162"
MMJ DISTANCE BY CALCULATION AND COPY,0.2813411078717201,"and Copy"" method is that it is easier to understand than using the Cartesian trees (26; 27).
163"
MMJ DISTANCE BY CALCULATION AND COPY,0.282798833819242,"(a) data A
(b) data B
(c) data C"
MMJ DISTANCE BY CALCULATION AND COPY,0.28425655976676384,"(d) data A, Standard K-means
(e) data B, Standard K-means
(f) data C, Standard K-means"
MMJ DISTANCE BY CALCULATION AND COPY,0.2857142857142857,"(g) data A, MMJ-K-means
(h) data B, MMJ-K-means
(i) data C, MMJ-K-means"
MMJ DISTANCE BY CALCULATION AND COPY,0.28717201166180756,Figure 2: Standard K-means vs. MMJ-K-means
APPLICATIONS OF MIN-MAX-JUMP DISTANCE,0.2886297376093295,"5
Applications of Min-Max-Jump distance
164"
APPLICATIONS OF MIN-MAX-JUMP DISTANCE,0.29008746355685133,"We explore two applications of MMJ distance, and test the applications with experiments. All the
165"
APPLICATIONS OF MIN-MAX-JUMP DISTANCE,0.2915451895043732,"MMJ distances in the experiments are calculated with Algorithm 1.
166"
MMJ-BASED K-MEANS,0.29300291545189505,"5.1
MMJ-based K-means
167"
MMJ-BASED K-MEANS,0.2944606413994169,"K-means clustering aims to partition n observations into k clusters in which each observation belongs
168"
MMJ-BASED K-MEANS,0.29591836734693877,"to the cluster with the nearest mean (cluster center or centroid), serving as a prototype of the cluster
169"
MMJ-BASED K-MEANS,0.29737609329446063,"(28). Standard K-means uses Euclidean distance. We can revise K-means to use Min-Max-Jump
170"
MMJ-BASED K-MEANS,0.2988338192419825,"distance, with the cluster centroid replaced by the Semantic Center of Mass (SCOM) (particularly,
171"
MMJ-BASED K-MEANS,0.30029154518950435,"One-SCOM) of each cluster. For the definition of SCOM, see a previous paper (29). One-SCOM is
172"
MMJ-BASED K-MEANS,0.30174927113702626,"like medoid, but has some difference from medoid. Section 6.3 of (29) compares One-SCOM and
173"
MMJ-BASED K-MEANS,0.3032069970845481,"medoid. In simple terms, the One-SCOM of a set of points, is the point which has the smallest sum
174"
MMJ-BASED K-MEANS,0.30466472303207,"of squared distances to all points in the set.
175"
MMJ-BASED K-MEANS,0.30612244897959184,"Standard K-means usually cannot deal with non-spherical shaped data, such as the ones in Figure 2.
176"
MMJ-BASED K-MEANS,0.3075801749271137,"MMJ-based K-means (MMJ-K-means) can cluster such irregularly shaped data. Figure 2 compares
177"
MMJ-BASED K-MEANS,0.30903790087463556,"Standard K-means and MMJ-K-means, on clustering three data which come from the scikit-learn
178"
MMJ-BASED K-MEANS,0.3104956268221574,"project (30). Figure 3 are eight more samples of MMJ-K-means. The data sources corresponding to
179"
MMJ-BASED K-MEANS,0.3119533527696793,"the data IDs can be found at this URL (temporarily hidden for double blind review).
180"
MMJ-BASED K-MEANS,0.31341107871720114,"It can be seen MMJ-K-means can (almost) work properly for clustering the 11 data, which have
181"
MMJ-BASED K-MEANS,0.31486880466472306,"different kinds of shapes. The black circles are Border points (Definition 2), the red stars are the center
182"
MMJ-BASED K-MEANS,0.3163265306122449,"(One-SCOM) of each cluster. During training of MMJ-K-means, the Border points are randomly
183"
MMJ-BASED K-MEANS,0.3177842565597668,"allocated to one of its nearest centers.
184"
MMJ-BASED K-MEANS,0.31924198250728864,"Definition 2. Border point
185"
MMJ-BASED K-MEANS,0.3206997084548105,"A point is defined to be a Border point if its nearest mean (center, centroid, or One-SCOM) is not
186"
MMJ-BASED K-MEANS,0.32215743440233235,"unique.
187"
MMJ-BASED K-MEANS,0.3236151603498542,"Compared with other clustering models that can handle irregularly shaped data, such as Spectral
188"
MMJ-BASED K-MEANS,0.3250728862973761,"clustering or the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), the merit
189"
MMJ-BASED K-MEANS,0.32653061224489793,"of MMJ-K-means is its simplicity; the logic of MMJ-K-means is as simple as K-means. We just
190"
MMJ-BASED K-MEANS,0.32798833819241985,"replace the Euclidean distance with MMJ distance, and the centroid with the Semantic Center of
191"
MMJ-BASED K-MEANS,0.3294460641399417,"Mass (SCOM).
192"
MMJ-BASED K-MEANS,0.33090379008746357,"(a) data 1
(b) data 54
(c) data 62
(d) data 76"
MMJ-BASED K-MEANS,0.3323615160349854,"(e) data 78
(f) data 83
(g) data 89
(h) data 102"
MMJ-BASED K-MEANS,0.3338192419825073,Figure 3: Eight more samples of MMJ-K-means
MMJ-BASED K-MEANS,0.33527696793002915,"CH
SC
DB
CDbw
DBCV
VIASCKDE
New
MMJ-SC
MMJ-CH
MMJ-DB"
MMJ-BASED K-MEANS,0.336734693877551,"Accuracy
27/145
38/145
42/145
8/145
56/145
11/145
74/145
83/145
90/145
69/145"
MMJ-BASED K-MEANS,0.33819241982507287,Table 2: Accuracy of the ten indices
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3396501457725947,"5.2
MMJ-based internal clustering evaluation index
193"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.34110787172011664,"Calinski-Harabasz index, Silhouette coefficient, and Davies-Bouldin index are three of the most
194"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3425655976676385,"popular techniques for internal clustering evaluation. They are used to calculate the goodness of a
195"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.34402332361516036,"clustering technique.
196"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3454810495626822,"The Silhouette coefficient for a single sample is given as:
197"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3469387755102041,"s =
b −a
max(a, b)"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.34839650145772594,"where a is the mean distance between a sample and all other points in the same class. b is the mean
198"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3498542274052478,"distance between a sample and all other points in the next nearest cluster. The Silhouette coefficient
199"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.35131195335276966,"for a set of samples is given as the mean of Silhouette coefficient for each sample.
200"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.35276967930029157,"We can also revise Silhouette coefficient to use Min-Max-Jump distance, forming a new internal
201"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.35422740524781343,"clustering evaluation index called MMJ-based Silhouette coefficient (MMJ-SC). We tested the
202"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3556851311953353,"performance of MMJ-SC with the 145 datasets mentioned in another paper(31). MMJ-SC obtained
203"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.35714285714285715,"a good performance score compared with the other seven internal clustering evaluation indices
204"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.358600583090379,"mentioned in the paper(31). Readers can check Table 2 and compare with Table 5 of Liu’s paper(31).
205"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.36005830903790087,"MMJ-based Calinski-Harabasz index (MMJ-CH) and MMJ-based Davies-Bouldin index (MMJ-DB)
206"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.36151603498542273,"were also tested. In calculation of these two indices, besides using MMJ distance, the center/centroid
207"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3629737609329446,"of a cluster is replaced by the One-SCOM of the cluster again, as in MMJ-K-means. It can be seen
208"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.36443148688046645,"that MMJ distance systematically improves the three internal clustering evaluation indices (Table
209"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.36588921282798836,"2). The best performer is MMJ-CH, which achieves an accuracy of 90/145. The accuracy of an
210"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3673469387755102,"index is computed by evaluating the index’s ability of recognizing the best partition of a dataset from
211"
MMJ-BASED INTERNAL CLUSTERING EVALUATION INDEX,0.3688046647230321,"hundreds of candidate partitions(31).
212"
USING MMJ-SC IN CNNI,0.37026239067055394,"5.2.1
Using MMJ-SC in CNNI
213"
USING MMJ-SC IN CNNI,0.3717201166180758,"The Clustering with Neural Network and Index (CNNI) model uses a Neural Network to cluster
214"
USING MMJ-SC IN CNNI,0.37317784256559766,"data points. Training of the Neural Network mimics supervised learning, with an internal clustering
215"
USING MMJ-SC IN CNNI,0.3746355685131195,"evaluation index acting as the loss function (24). CNNI with standard Silhouette coefficient as the
216"
USING MMJ-SC IN CNNI,0.3760932944606414,"internal clustering evaluation index, cannot deal with non-flat geometry data, such as data B and
217"
USING MMJ-SC IN CNNI,0.37755102040816324,"data C in Figure 2. MMJ-SC gives CNNI model the capability of processing non-flat geometry data.
218"
USING MMJ-SC IN CNNI,0.37900874635568516,"E.g., Figure 4 is the clustering result and decision boundary of data B by CNNI using MMJ-SC.
219"
USING MMJ-SC IN CNNI,0.380466472303207,"It uses Neural Network C of the CNNI paper (24). CNNI equipped with MMJ-SC, achieves the
220"
USING MMJ-SC IN CNNI,0.3819241982507289,Figure 4: Clustering result and decision boundary of data B by CNNI using MMJ-SC
USING MMJ-SC IN CNNI,0.38338192419825073,"first inductive clustering model that can deal with non-flat geometry data (24). For the definition of
221"
USING MMJ-SC IN CNNI,0.3848396501457726,"non-flat geometry data, see this1 Stackexchange question.
222"
DISCUSSION,0.38629737609329445,"6
Discussion
223"
USING PAM,0.3877551020408163,"6.1
Using PAM
224"
USING PAM,0.3892128279883382,"Since One-SCOM is like medoid, in MMJ-K-means, we can also use the Partitioning Around Medoids
225"
USING PAM,0.39067055393586003,"(PAM) algorithm or its variants to find the One-SCOMs (32).
226"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.39212827988338195,"6.2
Multiple One-SCOMs in one cluster
227"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.3935860058309038,"There might be multiple One-SCOM points in a cluster, which have the same smallest sum of squared
228"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.39504373177842567,"distances to all the points in the cluster. Usually they are not far from each other. We can arbitrarily
229"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.3965014577259475,"choose one or keep them all. If we keep them all, then the One-SCOM of a cluster is not a point, but a
230"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.3979591836734694,"set of points. If the One-SCOM is a set, when calculating a point’s MMJ distance to the One-SCOM
231"
MULTIPLE ONE-SCOMS IN ONE CLUSTER,0.39941690962099125,"of a cluster, we can select the minimum of the point’s MMJ distances to all the One-SCOM points.
232"
DIFFERENTIATING BORDER POINTS,0.4008746355685131,"6.3
Differentiating border points
233"
DIFFERENTIATING BORDER POINTS,0.40233236151603496,"Border points defined in Definition 2 can further be differentiated as weak and strong border points.
234"
DIFFERENTIATING BORDER POINTS,0.4037900874635568,"Definition 3. Weak Border Point (WBP)
235"
DIFFERENTIATING BORDER POINTS,0.40524781341107874,"A point is defined to be a WBP if its nearest mean (center or One-SCOM) is not unique but less than
236"
DIFFERENTIATING BORDER POINTS,0.4067055393586006,"K, where K is the number of clusters.
237"
DIFFERENTIATING BORDER POINTS,0.40816326530612246,"Definition 4. Strong Border Point (SBP)
238"
DIFFERENTIATING BORDER POINTS,0.4096209912536443,"A point is defined to be a SBP if its nearest mean (center or One-SCOM) is not unique and equals K,
239"
DIFFERENTIATING BORDER POINTS,0.4110787172011662,"where K is the number of clusters.
240"
DIFFERENTIATING BORDER POINTS,0.41253644314868804,"Then we can process different kinds of border points with different strategies. E.g., deeming the
241"
DIFFERENTIATING BORDER POINTS,0.4139941690962099,"Strong Border Points as outliers and removing them.
242"
CONCLUSION AND FUTURE WORKS,0.41545189504373176,"7
Conclusion and Future Works
243"
CONCLUSION AND FUTURE WORKS,0.41690962099125367,"We proposed two algorithms for calculating Min-Max-Jump distance (MMJ distance), and tested
244"
CONCLUSION AND FUTURE WORKS,0.41836734693877553,"two applications of it: MMJ-based K-means and MMJ-based internal clustering evaluation index.
245"
CONCLUSION AND FUTURE WORKS,0.4198250728862974,"MMJ-K-means overcomes a big drawback of K-means, improving its ability of clustering, so that it
246"
CONCLUSION AND FUTURE WORKS,0.42128279883381925,"can handle irregularly shaped clusters. We claim MMJ-CH is the SOTA (state-of-the-art) internal
247"
CONCLUSION AND FUTURE WORKS,0.4227405247813411,"clustering evaluation index, which achieves an accuracy of 90/145. To thoroughly test the internal
248"
CONCLUSION AND FUTURE WORKS,0.42419825072886297,"clustering evaluation indices, we conducted an experiment on a set of 145 datasets. A normal
249"
CONCLUSION AND FUTURE WORKS,0.42565597667638483,"Machine Learning paper usually uses several or dozens of datasets to test their models or algorithms.
250"
CONCLUSION AND FUTURE WORKS,0.4271137026239067,"In summary, MMJ distance has good capability and potentiality in Machine Learning. Further
251"
CONCLUSION AND FUTURE WORKS,0.42857142857142855,"research may test its applications in other models, such as other clustering evaluation indices.
252"
CONCLUSION AND FUTURE WORKS,0.43002915451895046,"1https://datascience.stackexchange.com/questions/52260/terminology-flat-geometr
y-in-the-context-of-clustering"
REFERENCES,0.4314868804664723,"References
253"
REFERENCES,0.4329446064139942,"[1] S. Z. Li and A. Jain, Eds., Hamming Distance.
Boston, MA: Springer US, 2009, pp. 668–668.
254"
REFERENCES,0.43440233236151604,"[Online]. Available: https://doi.org/10.1007/978-0-387-73003-5_956
255"
REFERENCES,0.4358600583090379,"[2] D. Sinwar and R. Kaushik, “Study of euclidean and manhattan distance metrics using simple
256"
REFERENCES,0.43731778425655976,"k-means clustering,” Int. J. Res. Appl. Sci. Eng. Technol, vol. 2, no. 5, pp. 270–274, 2014.
257"
REFERENCES,0.4387755102040816,"[3] R. Coghetto, “Chebyshev distance,” 2016.
258"
REFERENCES,0.4402332361516035,"[4] P. J. Groenen and K. Jajuga, “Fuzzy clustering with squared minkowski distances,” Fuzzy Sets
259"
REFERENCES,0.44169096209912534,"and Systems, vol. 120, no. 2, pp. 227–237, 2001.
260"
REFERENCES,0.44314868804664725,"[5] S. Fletcher, M. Z. Islam et al., “Comparing sets of patterns with the jaccard index,” Australasian
261"
REFERENCES,0.4446064139941691,"Journal of Information Systems, vol. 22, 2018.
262"
REFERENCES,0.446064139941691,"[6] N. R. Chopde and M. Nichat, “Landmark based shortest path detection by using a* and haversine
263"
REFERENCES,0.44752186588921283,"formula,” International Journal of Innovative Research in Computer and Communication
264"
REFERENCES,0.4489795918367347,"Engineering, vol. 1, no. 2, pp. 298–302, 2013.
265"
REFERENCES,0.45043731778425655,"[7] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The elements of statistical learning:
266"
REFERENCES,0.4518950437317784,"data mining, inference, and prediction.
Springer, 2009, vol. 2.
267"
REFERENCES,0.45335276967930027,"[8] R. Ostrovsky, Y. Rabani, L. J. Schulman, and C. Swamy, “The effectiveness of lloyd-type
268"
REFERENCES,0.45481049562682213,"methods for the k-means problem,” Journal of the ACM (JACM), vol. 59, no. 6, pp. 1–22, 2013.
269"
REFERENCES,0.45626822157434405,"[9] D. Arthur and S. Vassilvitskii, “K-means++ the advantages of careful seeding,” in Proceedings
270"
REFERENCES,0.4577259475218659,"of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 2007, pp. 1027–1035.
271"
REFERENCES,0.45918367346938777,"[10] H.-S. Park and C.-H. Jun, “A simple and fast algorithm for k-medoids clustering,” Expert
272"
REFERENCES,0.4606413994169096,"systems with applications, vol. 36, no. 2, pp. 3336–3341, 2009.
273"
REFERENCES,0.4620991253644315,"[11] D. Pfitzner, R. Leibbrandt, and D. Powers, “Characterization and evaluation of similarity
274"
REFERENCES,0.46355685131195334,"measures for pairs of clusterings,” Knowledge and Information Systems, vol. 19, no. 3, pp.
275"
REFERENCES,0.4650145772594752,"361–394, 2009.
276"
REFERENCES,0.46647230320699706,"[12] S. Petrovi´c, “A comparison between the silhouette index and the davies-bouldin index in
277"
REFERENCES,0.4679300291545189,"labelling ids clusters,” 2006.
278"
REFERENCES,0.46938775510204084,"[13] S. Aranganayagi and K. Thangavel, “Clustering categorical data using silhouette coefficient as a
279"
REFERENCES,0.4708454810495627,"relocating measure,” in International conference on computational intelligence and multimedia
280"
REFERENCES,0.47230320699708456,"applications (ICCIMA 2007), vol. 2.
IEEE, 2007, pp. 13–17.
281"
REFERENCES,0.4737609329446064,"[14] J. C. Bezdek and N. R. Pal, “Cluster validation with generalized dunn’s indices,” in Proceedings
282"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4752186588921283,"1995 second New Zealand international two-stream conference on artificial neural networks
283"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.47667638483965014,"and expert systems.
IEEE Computer Society, 1995, pp. 190–190.
284"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.478134110787172,"[15] U. Maulik and S. Bandyopadhyay, “Performance evaluation of some clustering algorithms and
285"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.47959183673469385,"validity indices,” IEEE Transactions on pattern analysis and machine intelligence, vol. 24,
286"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.48104956268221577,"no. 12, pp. 1650–1654, 2002.
287"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.48250728862973763,"[16] K. Y. Yeung and W. L. Ruzzo, “Details of the adjusted rand index and clustering algorithms,
288"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4839650145772595,"supplement to the paper an empirical study on principal component analysis for clustering gene
289"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.48542274052478135,"expression data,” Bioinformatics, vol. 17, no. 9, pp. 763–774, 2001.
290"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4868804664723032,"[17] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler, F. Warner, and S. W. Zucker,
291"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.48833819241982507,"“Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion
292"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4897959183673469,"maps,” Proceedings of the national academy of sciences, vol. 102, no. 21, pp. 7426–7431, 2005.
293"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4912536443148688,"[18] B. Fischer and J. M. Buhmann, “Path-based clustering for grouping of smooth curves and texture
294"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.49271137026239065,"segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 4,
295"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.49416909620991256,"pp. 513–518, 2003.
296"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4956268221574344,"[19] H. Chang and D.-Y. Yeung, “Robust path-based spectral clustering,” Pattern Recognition,
297"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.4970845481049563,"vol. 41, no. 1, pp. 191–203, 2008.
298"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.49854227405247814,"[20] M. Pollack, “The maximum capacity through a network,” Operations Research, vol. 8, no. 5,
299"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5,"pp. 733–736, 1960.
300"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5014577259475219,"[21] T. Hu, “The maximum capacity route problem,” Operations Research, vol. 9, no. 6, pp. 898–900,
301"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5029154518950437,"1961.
302"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5043731778425656,"[22] P. M. Camerini, “The min-max spanning tree problem and some extensions,” Information
303"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5058309037900874,"Processing Letters, vol. 7, no. 1, pp. 10–14, 1978.
304"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5072886297376094,"[23] A. V. Little, M. Maggioni, and J. M. Murphy, “Path-based spectral clustering: Guarantees,
305"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5087463556851312,"robustness to outliers, and fast algorithms,” J. Mach. Learn. Res., vol. 21, pp. 6:1–6:66, 2020.
306"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5102040816326531,"[Online]. Available: http://jmlr.org/papers/v21/18-085.html
307"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5116618075801749,"[24] G. Liu, “Clustering with neural network and index,” arXiv preprint arXiv:2212.03853, 2022.
308"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5131195335276968,"[25] R. Sibson, “Slink: an optimally efficient algorithm for the single-link cluster method,” The
309"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5145772594752187,"computer journal, vol. 16, no. 1, pp. 30–34, 1973.
310"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5160349854227405,"[26] N. Alon and B. Schieber, Optimal preprocessing for answering on-line product queries.
Cite-
311"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5174927113702624,"seer, 1987.
312"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5189504373177842,"[27] E. D. Demaine, G. M. Landau, and O. Weimann, “On cartesian trees and range minimum
313"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5204081632653061,"queries,” Algorithmica, vol. 68, pp. 610–625, 2014.
314"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.521865889212828,"[28] H.-H. Bock, “Clustering methods: a history of k-means algorithms,” Selected contributions in
315"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5233236151603499,"data analysis and classification, pp. 161–172, 2007.
316"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5247813411078717,"[29] G. Liu, “Topic model supervised by understanding map,” arXiv preprint arXiv:2110.06043,
317"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5262390670553936,"2021.
318"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5276967930029155,"[30] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
319"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5291545189504373,"P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
320"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5306122448979592,"M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
321"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.532069970845481,"Learning Research, vol. 12, pp. 2825–2830, 2011.
322"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5335276967930029,"[31] G. Liu, “A new index for clustering evaluation based on density estimation,” arXiv preprint
323"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5349854227405247,"arXiv:2207.01294, 2022.
324"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5364431486880467,"[32] E. Schubert and P. J. Rousseeuw, “Fast and eager k-medoids clustering: O (k) runtime improve-
325"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5379008746355685,"ment of the pam, clara, and clarans algorithms,” Information Systems, vol. 101, p. 101804,
326"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5393586005830904,"2021.
327"
SECOND NEW ZEALAND INTERNATIONAL TWO-STREAM CONFERENCE ON ARTIFICIAL NEURAL NETWORKS,0.5408163265306123,"NeurIPS Paper Checklist
328"
CLAIMS,0.5422740524781341,"1. Claims
329"
CLAIMS,0.543731778425656,"Question: Do the main claims made in the abstract and introduction accurately reflect the
330"
CLAIMS,0.5451895043731778,"paper’s contributions and scope?
331"
CLAIMS,0.5466472303206997,"Answer: [Yes]
332"
CLAIMS,0.5481049562682215,"Justification: The abstract and introduction clearly state the claims made, including the
333"
CLAIMS,0.5495626822157434,"contributions made in the paper.
334"
CLAIMS,0.5510204081632653,"Guidelines:
335"
CLAIMS,0.5524781341107872,"• The answer NA means that the abstract and introduction do not include the claims
336"
CLAIMS,0.5539358600583091,"made in the paper.
337"
CLAIMS,0.5553935860058309,"• The abstract and/or introduction should clearly state the claims made, including the
338"
CLAIMS,0.5568513119533528,"contributions made in the paper and important assumptions and limitations. A No or
339"
CLAIMS,0.5583090379008746,"NA answer to this question will not be perceived well by the reviewers.
340"
CLAIMS,0.5597667638483965,"• The claims made should match theoretical and experimental results, and reflect how
341"
CLAIMS,0.5612244897959183,"much the results can be expected to generalize to other settings.
342"
CLAIMS,0.5626822157434402,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
343"
CLAIMS,0.5641399416909622,"are not attained by the paper.
344"
LIMITATIONS,0.565597667638484,"2. Limitations
345"
LIMITATIONS,0.5670553935860059,"Question: Does the paper discuss the limitations of the work performed by the authors?
346"
LIMITATIONS,0.5685131195335277,"Answer: [No]
347"
LIMITATIONS,0.5699708454810496,"Justification: We have used 145 datasets to test the models in the paper. Maybe it is not
348"
LIMITATIONS,0.5714285714285714,"enough, we need more datasets to test the models.
349"
LIMITATIONS,0.5728862973760933,"Guidelines:
350"
LIMITATIONS,0.5743440233236151,"• The answer NA means that the paper has no limitation while the answer No means that
351"
LIMITATIONS,0.575801749271137,"the paper has limitations, but those are not discussed in the paper.
352"
LIMITATIONS,0.577259475218659,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
353"
LIMITATIONS,0.5787172011661808,"• The paper should point out any strong assumptions and how robust the results are to
354"
LIMITATIONS,0.5801749271137027,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
355"
LIMITATIONS,0.5816326530612245,"model well-specification, asymptotic approximations only holding locally). The authors
356"
LIMITATIONS,0.5830903790087464,"should reflect on how these assumptions might be violated in practice and what the
357"
LIMITATIONS,0.5845481049562682,"implications would be.
358"
LIMITATIONS,0.5860058309037901,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
359"
LIMITATIONS,0.5874635568513119,"only tested on a few datasets or with a few runs. In general, empirical results often
360"
LIMITATIONS,0.5889212827988338,"depend on implicit assumptions, which should be articulated.
361"
LIMITATIONS,0.5903790087463557,"• The authors should reflect on the factors that influence the performance of the approach.
362"
LIMITATIONS,0.5918367346938775,"For example, a facial recognition algorithm may perform poorly when image resolution
363"
LIMITATIONS,0.5932944606413995,"is low or images are taken in low lighting. Or a speech-to-text system might not be
364"
LIMITATIONS,0.5947521865889213,"used reliably to provide closed captions for online lectures because it fails to handle
365"
LIMITATIONS,0.5962099125364432,"technical jargon.
366"
LIMITATIONS,0.597667638483965,"• The authors should discuss the computational efficiency of the proposed algorithms
367"
LIMITATIONS,0.5991253644314869,"and how they scale with dataset size.
368"
LIMITATIONS,0.6005830903790087,"• If applicable, the authors should discuss possible limitations of their approach to
369"
LIMITATIONS,0.6020408163265306,"address problems of privacy and fairness.
370"
LIMITATIONS,0.6034985422740525,"• While the authors might fear that complete honesty about limitations might be used by
371"
LIMITATIONS,0.6049562682215743,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
372"
LIMITATIONS,0.6064139941690962,"limitations that aren’t acknowledged in the paper. The authors should use their best
373"
LIMITATIONS,0.607871720116618,"judgment and recognize that individual actions in favor of transparency play an impor-
374"
LIMITATIONS,0.60932944606414,"tant role in developing norms that preserve the integrity of the community. Reviewers
375"
LIMITATIONS,0.6107871720116618,"will be specifically instructed to not penalize honesty concerning limitations.
376"
THEORY ASSUMPTIONS AND PROOFS,0.6122448979591837,"3. Theory Assumptions and Proofs
377"
THEORY ASSUMPTIONS AND PROOFS,0.6137026239067055,"Question: For each theoretical result, does the paper provide the full set of assumptions and
378"
THEORY ASSUMPTIONS AND PROOFS,0.6151603498542274,"a complete (and correct) proof?
379"
THEORY ASSUMPTIONS AND PROOFS,0.6166180758017493,"Answer: [Yes]
380"
THEORY ASSUMPTIONS AND PROOFS,0.6180758017492711,"Justification: Proofs of theoretical results have been provided in the paper.
381"
THEORY ASSUMPTIONS AND PROOFS,0.619533527696793,"Guidelines:
382"
THEORY ASSUMPTIONS AND PROOFS,0.6209912536443148,"• The answer NA means that the paper does not include theoretical results.
383"
THEORY ASSUMPTIONS AND PROOFS,0.6224489795918368,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
384"
THEORY ASSUMPTIONS AND PROOFS,0.6239067055393586,"referenced.
385"
THEORY ASSUMPTIONS AND PROOFS,0.6253644314868805,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
386"
THEORY ASSUMPTIONS AND PROOFS,0.6268221574344023,"• The proofs can either appear in the main paper or the supplemental material, but if
387"
THEORY ASSUMPTIONS AND PROOFS,0.6282798833819242,"they appear in the supplemental material, the authors are encouraged to provide a short
388"
THEORY ASSUMPTIONS AND PROOFS,0.6297376093294461,"proof sketch to provide intuition.
389"
THEORY ASSUMPTIONS AND PROOFS,0.6311953352769679,"• Inversely, any informal proof provided in the core of the paper should be complemented
390"
THEORY ASSUMPTIONS AND PROOFS,0.6326530612244898,"by formal proofs provided in appendix or supplemental material.
391"
THEORY ASSUMPTIONS AND PROOFS,0.6341107871720116,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
392"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6355685131195336,"4. Experimental Result Reproducibility
393"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6370262390670554,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
394"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6384839650145773,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
395"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6399416909620991,"of the paper (regardless of whether the code and data are provided or not)?
396"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.641399416909621,"Answer: [Yes]
397"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6428571428571429,"Justification: We have fully disclosed all the information needed to reproduce the main
398"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6443148688046647,"experimental results of the paper.
399"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6457725947521866,"Guidelines:
400"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6472303206997084,"• The answer NA means that the paper does not include experiments.
401"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6486880466472303,"• If the paper includes experiments, a No answer to this question will not be perceived
402"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6501457725947521,"well by the reviewers: Making the paper reproducible is important, regardless of
403"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6516034985422741,"whether the code and data are provided or not.
404"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6530612244897959,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
405"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6545189504373178,"to make their results reproducible or verifiable.
406"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6559766763848397,"• Depending on the contribution, reproducibility can be accomplished in various ways.
407"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6574344023323615,"For example, if the contribution is a novel architecture, describing the architecture fully
408"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6588921282798834,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
409"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6603498542274052,"be necessary to either make it possible for others to replicate the model with the same
410"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6618075801749271,"dataset, or provide access to the model. In general. releasing code and data is often
411"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6632653061224489,"one good way to accomplish this, but reproducibility can also be provided via detailed
412"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6647230320699709,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
413"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6661807580174927,"of a large language model), releasing of a model checkpoint, or other means that are
414"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6676384839650146,"appropriate to the research performed.
415"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6690962099125365,"• While NeurIPS does not require releasing code, the conference does require all submis-
416"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6705539358600583,"sions to provide some reasonable avenue for reproducibility, which may depend on the
417"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6720116618075802,"nature of the contribution. For example
418"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.673469387755102,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
419"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6749271137026239,"to reproduce that algorithm.
420"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6763848396501457,"(b) If the contribution is primarily a new model architecture, the paper should describe
421"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6778425655976676,"the architecture clearly and fully.
422"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6793002915451894,"(c) If the contribution is a new model (e.g., a large language model), then there should
423"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6807580174927114,"either be a way to access this model for reproducing the results or a way to reproduce
424"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6822157434402333,"the model (e.g., with an open-source dataset or instructions for how to construct
425"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6836734693877551,"the dataset).
426"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.685131195335277,"(d) We recognize that reproducibility may be tricky in some cases, in which case
427"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6865889212827988,"authors are welcome to describe the particular way they provide for reproducibility.
428"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6880466472303207,"In the case of closed-source models, it may be that access to the model is limited in
429"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6895043731778425,"some way (e.g., to registered users), but it should be possible for other researchers
430"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6909620991253644,"to have some path to reproducing or verifying the results.
431"
OPEN ACCESS TO DATA AND CODE,0.6924198250728864,"5. Open access to data and code
432"
OPEN ACCESS TO DATA AND CODE,0.6938775510204082,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
433"
OPEN ACCESS TO DATA AND CODE,0.6953352769679301,"tions to faithfully reproduce the main experimental results, as described in supplemental
434"
OPEN ACCESS TO DATA AND CODE,0.6967930029154519,"material?
435"
OPEN ACCESS TO DATA AND CODE,0.6982507288629738,"Answer: [Yes]
436"
OPEN ACCESS TO DATA AND CODE,0.6997084548104956,"Justification: We provide an URL to data and code of the paper, to reproduce the main
437"
OPEN ACCESS TO DATA AND CODE,0.7011661807580175,"experimental results.
438"
OPEN ACCESS TO DATA AND CODE,0.7026239067055393,"Guidelines:
439"
OPEN ACCESS TO DATA AND CODE,0.7040816326530612,"• The answer NA means that paper does not include experiments requiring code.
440"
OPEN ACCESS TO DATA AND CODE,0.7055393586005831,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu
441"
OPEN ACCESS TO DATA AND CODE,0.706997084548105,"blic/guides/CodeSubmissionPolicy) for more details.
442"
OPEN ACCESS TO DATA AND CODE,0.7084548104956269,"• While we encourage the release of code and data, we understand that this might not be
443"
OPEN ACCESS TO DATA AND CODE,0.7099125364431487,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
444"
OPEN ACCESS TO DATA AND CODE,0.7113702623906706,"including code, unless this is central to the contribution (e.g., for a new open-source
445"
OPEN ACCESS TO DATA AND CODE,0.7128279883381924,"benchmark).
446"
OPEN ACCESS TO DATA AND CODE,0.7142857142857143,"• The instructions should contain the exact command and environment needed to run to
447"
OPEN ACCESS TO DATA AND CODE,0.7157434402332361,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
448"
OPEN ACCESS TO DATA AND CODE,0.717201166180758,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
449"
OPEN ACCESS TO DATA AND CODE,0.7186588921282799,"• The authors should provide instructions on data access and preparation, including how
450"
OPEN ACCESS TO DATA AND CODE,0.7201166180758017,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
451"
OPEN ACCESS TO DATA AND CODE,0.7215743440233237,"• The authors should provide scripts to reproduce all experimental results for the new
452"
OPEN ACCESS TO DATA AND CODE,0.7230320699708455,"proposed method and baselines. If only a subset of experiments are reproducible, they
453"
OPEN ACCESS TO DATA AND CODE,0.7244897959183674,"should state which ones are omitted from the script and why.
454"
OPEN ACCESS TO DATA AND CODE,0.7259475218658892,"• At submission time, to preserve anonymity, the authors should release anonymized
455"
OPEN ACCESS TO DATA AND CODE,0.7274052478134111,"versions (if applicable).
456"
OPEN ACCESS TO DATA AND CODE,0.7288629737609329,"• Providing as much information as possible in supplemental material (appended to the
457"
OPEN ACCESS TO DATA AND CODE,0.7303206997084548,"paper) is recommended, but including URLs to data and code is permitted.
458"
OPEN ACCESS TO DATA AND CODE,0.7317784256559767,"6. Experimental Setting/Details
459"
OPEN ACCESS TO DATA AND CODE,0.7332361516034985,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
460"
OPEN ACCESS TO DATA AND CODE,0.7346938775510204,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
461"
OPEN ACCESS TO DATA AND CODE,0.7361516034985423,"results?
462"
OPEN ACCESS TO DATA AND CODE,0.7376093294460642,"Answer: [Yes]
463"
OPEN ACCESS TO DATA AND CODE,0.739067055393586,"Justification: Full details are provided with the code.
464"
OPEN ACCESS TO DATA AND CODE,0.7405247813411079,"Guidelines:
465"
OPEN ACCESS TO DATA AND CODE,0.7419825072886297,"• The answer NA means that the paper does not include experiments.
466"
OPEN ACCESS TO DATA AND CODE,0.7434402332361516,"• The experimental setting should be presented in the core of the paper to a level of detail
467"
OPEN ACCESS TO DATA AND CODE,0.7448979591836735,"that is necessary to appreciate the results and make sense of them.
468"
OPEN ACCESS TO DATA AND CODE,0.7463556851311953,"• The full details can be provided either with the code, in appendix, or as supplemental
469"
OPEN ACCESS TO DATA AND CODE,0.7478134110787172,"material.
470"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.749271137026239,"7. Experiment Statistical Significance
471"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.750728862973761,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
472"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7521865889212828,"information about the statistical significance of the experiments?
473"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7536443148688047,"Answer: [NA]
474"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7551020408163265,"Justification: The paper does not contain statistical experimental results.
475"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7565597667638484,"Guidelines:
476"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7580174927113703,"• The answer NA means that the paper does not include experiments.
477"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7594752186588921,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
478"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.760932944606414,"dence intervals, or statistical significance tests, at least for the experiments that support
479"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7623906705539358,"the main claims of the paper.
480"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7638483965014577,"• The factors of variability that the error bars are capturing should be clearly stated (for
481"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7653061224489796,"example, train/test split, initialization, random drawing of some parameter, or overall
482"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7667638483965015,"run with given experimental conditions).
483"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7682215743440233,"• The method for calculating the error bars should be explained (closed form formula,
484"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7696793002915452,"call to a library function, bootstrap, etc.)
485"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7711370262390671,"• The assumptions made should be given (e.g., Normally distributed errors).
486"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7725947521865889,"• It should be clear whether the error bar is the standard deviation or the standard error
487"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7740524781341108,"of the mean.
488"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7755102040816326,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
489"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7769679300291545,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
490"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7784256559766763,"of Normality of errors is not verified.
491"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7798833819241983,"• For asymmetric distributions, the authors should be careful not to show in tables or
492"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7813411078717201,"figures symmetric error bars that would yield results that are out of range (e.g. negative
493"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.782798833819242,"error rates).
494"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7842565597667639,"• If error bars are reported in tables or plots, The authors should explain in the text how
495"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7857142857142857,"they were calculated and reference the corresponding figures or tables in the text.
496"
EXPERIMENTS COMPUTE RESOURCES,0.7871720116618076,"8. Experiments Compute Resources
497"
EXPERIMENTS COMPUTE RESOURCES,0.7886297376093294,"Question: For each experiment, does the paper provide sufficient information on the com-
498"
EXPERIMENTS COMPUTE RESOURCES,0.7900874635568513,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
499"
EXPERIMENTS COMPUTE RESOURCES,0.7915451895043731,"the experiments?
500"
EXPERIMENTS COMPUTE RESOURCES,0.793002915451895,"Answer: [No]
501"
EXPERIMENTS COMPUTE RESOURCES,0.7944606413994169,"Justification: The paper does not discuss about the efficiency of the models, only effective-
502"
EXPERIMENTS COMPUTE RESOURCES,0.7959183673469388,"ness and time complexity of the algorithms. Because efficiency can be affected by a lot of
503"
EXPERIMENTS COMPUTE RESOURCES,0.7973760932944607,"factors, e.g., using C++ to implement is much faster than using python, and some minor
504"
EXPERIMENTS COMPUTE RESOURCES,0.7988338192419825,"optimization of the codes may drastically improve the speed.
505"
EXPERIMENTS COMPUTE RESOURCES,0.8002915451895044,"Guidelines:
506"
EXPERIMENTS COMPUTE RESOURCES,0.8017492711370262,"• The answer NA means that the paper does not include experiments.
507"
EXPERIMENTS COMPUTE RESOURCES,0.8032069970845481,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
508"
EXPERIMENTS COMPUTE RESOURCES,0.8046647230320699,"or cloud provider, including relevant memory and storage.
509"
EXPERIMENTS COMPUTE RESOURCES,0.8061224489795918,"• The paper should provide the amount of compute required for each of the individual
510"
EXPERIMENTS COMPUTE RESOURCES,0.8075801749271136,"experimental runs as well as estimate the total compute.
511"
EXPERIMENTS COMPUTE RESOURCES,0.8090379008746356,"• The paper should disclose whether the full research project required more compute
512"
EXPERIMENTS COMPUTE RESOURCES,0.8104956268221575,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
513"
EXPERIMENTS COMPUTE RESOURCES,0.8119533527696793,"didn’t make it into the paper).
514"
CODE OF ETHICS,0.8134110787172012,"9. Code Of Ethics
515"
CODE OF ETHICS,0.814868804664723,"Question: Does the research conducted in the paper conform, in every respect, with the
516"
CODE OF ETHICS,0.8163265306122449,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
517"
CODE OF ETHICS,0.8177842565597667,"Answer: [Yes]
518"
CODE OF ETHICS,0.8192419825072886,"Justification: The research conducted in the paper conforms with the NeurIPS Code of
519"
CODE OF ETHICS,0.8206997084548106,"Ethics.
520"
CODE OF ETHICS,0.8221574344023324,"Guidelines:
521"
CODE OF ETHICS,0.8236151603498543,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
522"
CODE OF ETHICS,0.8250728862973761,"• If the authors answer No, they should explain the special circumstances that require a
523"
CODE OF ETHICS,0.826530612244898,"deviation from the Code of Ethics.
524"
CODE OF ETHICS,0.8279883381924198,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
525"
CODE OF ETHICS,0.8294460641399417,"eration due to laws or regulations in their jurisdiction).
526"
BROADER IMPACTS,0.8309037900874635,"10. Broader Impacts
527"
BROADER IMPACTS,0.8323615160349854,"Question: Does the paper discuss both potential positive societal impacts and negative
528"
BROADER IMPACTS,0.8338192419825073,"societal impacts of the work performed?
529"
BROADER IMPACTS,0.8352769679300291,"Answer: [NA]
530"
BROADER IMPACTS,0.8367346938775511,"Justification: There is no societal impact of the work performed.
531"
BROADER IMPACTS,0.8381924198250729,"Guidelines:
532"
BROADER IMPACTS,0.8396501457725948,"• The answer NA means that there is no societal impact of the work performed.
533"
BROADER IMPACTS,0.8411078717201166,"• If the authors answer NA or No, they should explain why their work has no societal
534"
BROADER IMPACTS,0.8425655976676385,"impact or why the paper does not address societal impact.
535"
BROADER IMPACTS,0.8440233236151603,"• Examples of negative societal impacts include potential malicious or unintended uses
536"
BROADER IMPACTS,0.8454810495626822,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
537"
BROADER IMPACTS,0.8469387755102041,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
538"
BROADER IMPACTS,0.8483965014577259,"groups), privacy considerations, and security considerations.
539"
BROADER IMPACTS,0.8498542274052479,"• The conference expects that many papers will be foundational research and not tied
540"
BROADER IMPACTS,0.8513119533527697,"to particular applications, let alone deployments. However, if there is a direct path to
541"
BROADER IMPACTS,0.8527696793002916,"any negative applications, the authors should point it out. For example, it is legitimate
542"
BROADER IMPACTS,0.8542274052478134,"to point out that an improvement in the quality of generative models could be used to
543"
BROADER IMPACTS,0.8556851311953353,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
544"
BROADER IMPACTS,0.8571428571428571,"that a generic algorithm for optimizing neural networks could enable people to train
545"
BROADER IMPACTS,0.858600583090379,"models that generate Deepfakes faster.
546"
BROADER IMPACTS,0.8600583090379009,"• The authors should consider possible harms that could arise when the technology is
547"
BROADER IMPACTS,0.8615160349854227,"being used as intended and functioning correctly, harms that could arise when the
548"
BROADER IMPACTS,0.8629737609329446,"technology is being used as intended but gives incorrect results, and harms following
549"
BROADER IMPACTS,0.8644314868804664,"from (intentional or unintentional) misuse of the technology.
550"
BROADER IMPACTS,0.8658892128279884,"• If there are negative societal impacts, the authors could also discuss possible mitigation
551"
BROADER IMPACTS,0.8673469387755102,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
552"
BROADER IMPACTS,0.8688046647230321,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
553"
BROADER IMPACTS,0.8702623906705539,"feedback over time, improving the efficiency and accessibility of ML).
554"
SAFEGUARDS,0.8717201166180758,"11. Safeguards
555"
SAFEGUARDS,0.8731778425655977,"Question: Does the paper describe safeguards that have been put in place for responsible
556"
SAFEGUARDS,0.8746355685131195,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
557"
SAFEGUARDS,0.8760932944606414,"image generators, or scraped datasets)?
558"
SAFEGUARDS,0.8775510204081632,"Answer: [NA]
559"
SAFEGUARDS,0.8790087463556852,"Justification: The paper poses no such risks.
560"
SAFEGUARDS,0.880466472303207,"Guidelines:
561"
SAFEGUARDS,0.8819241982507289,"• The answer NA means that the paper poses no such risks.
562"
SAFEGUARDS,0.8833819241982507,"• Released models that have a high risk for misuse or dual-use should be released with
563"
SAFEGUARDS,0.8848396501457726,"necessary safeguards to allow for controlled use of the model, for example by requiring
564"
SAFEGUARDS,0.8862973760932945,"that users adhere to usage guidelines or restrictions to access the model or implementing
565"
SAFEGUARDS,0.8877551020408163,"safety filters.
566"
SAFEGUARDS,0.8892128279883382,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
567"
SAFEGUARDS,0.89067055393586,"should describe how they avoided releasing unsafe images.
568"
SAFEGUARDS,0.892128279883382,"• We recognize that providing effective safeguards is challenging, and many papers do
569"
SAFEGUARDS,0.8935860058309038,"not require this, but we encourage authors to take this into account and make a best
570"
SAFEGUARDS,0.8950437317784257,"faith effort.
571"
LICENSES FOR EXISTING ASSETS,0.8965014577259475,"12. Licenses for existing assets
572"
LICENSES FOR EXISTING ASSETS,0.8979591836734694,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
573"
LICENSES FOR EXISTING ASSETS,0.8994169096209913,"the paper, properly credited and are the license and terms of use explicitly mentioned and
574"
LICENSES FOR EXISTING ASSETS,0.9008746355685131,"properly respected?
575"
LICENSES FOR EXISTING ASSETS,0.902332361516035,"Answer: [Yes]
576"
LICENSES FOR EXISTING ASSETS,0.9037900874635568,"Justification: The creators or original owners of assets used in the paper are properly credited.
577"
LICENSES FOR EXISTING ASSETS,0.9052478134110787,"The license and terms of use are explicitly mentioned and properly respected.
578"
LICENSES FOR EXISTING ASSETS,0.9067055393586005,"Guidelines:
579"
LICENSES FOR EXISTING ASSETS,0.9081632653061225,"• The answer NA means that the paper does not use existing assets.
580"
LICENSES FOR EXISTING ASSETS,0.9096209912536443,"• The authors should cite the original paper that produced the code package or dataset.
581"
LICENSES FOR EXISTING ASSETS,0.9110787172011662,"• The authors should state which version of the asset is used and, if possible, include a
582"
LICENSES FOR EXISTING ASSETS,0.9125364431486881,"URL.
583"
LICENSES FOR EXISTING ASSETS,0.9139941690962099,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
584"
LICENSES FOR EXISTING ASSETS,0.9154518950437318,"• For scraped data from a particular source (e.g., website), the copyright and terms of
585"
LICENSES FOR EXISTING ASSETS,0.9169096209912536,"service of that source should be provided.
586"
LICENSES FOR EXISTING ASSETS,0.9183673469387755,"• If assets are released, the license, copyright information, and terms of use in the package
587"
LICENSES FOR EXISTING ASSETS,0.9198250728862973,"should be provided. For popular datasets, paperswithcode.com/datasets has
588"
LICENSES FOR EXISTING ASSETS,0.9212827988338192,"curated licenses for some datasets. Their licensing guide can help determine the license
589"
LICENSES FOR EXISTING ASSETS,0.922740524781341,"of a dataset.
590"
LICENSES FOR EXISTING ASSETS,0.924198250728863,"• For existing datasets that are re-packaged, both the original license and the license of
591"
LICENSES FOR EXISTING ASSETS,0.9256559766763849,"the derived asset (if it has changed) should be provided.
592"
LICENSES FOR EXISTING ASSETS,0.9271137026239067,"• If this information is not available online, the authors are encouraged to reach out to
593"
LICENSES FOR EXISTING ASSETS,0.9285714285714286,"the asset’s creators.
594"
NEW ASSETS,0.9300291545189504,"13. New Assets
595"
NEW ASSETS,0.9314868804664723,"Question: Are new assets introduced in the paper well documented and is the documentation
596"
NEW ASSETS,0.9329446064139941,"provided alongside the assets?
597"
NEW ASSETS,0.934402332361516,"Answer: [NA]
598"
NEW ASSETS,0.9358600583090378,"Justification: The paper does not release new assets.
599"
NEW ASSETS,0.9373177842565598,"Guidelines:
600"
NEW ASSETS,0.9387755102040817,"• The answer NA means that the paper does not release new assets.
601"
NEW ASSETS,0.9402332361516035,"• Researchers should communicate the details of the dataset/code/model as part of their
602"
NEW ASSETS,0.9416909620991254,"submissions via structured templates. This includes details about training, license,
603"
NEW ASSETS,0.9431486880466472,"limitations, etc.
604"
NEW ASSETS,0.9446064139941691,"• The paper should discuss whether and how consent was obtained from people whose
605"
NEW ASSETS,0.9460641399416909,"asset is used.
606"
NEW ASSETS,0.9475218658892128,"• At submission time, remember to anonymize your assets (if applicable). You can either
607"
NEW ASSETS,0.9489795918367347,"create an anonymized URL or include an anonymized zip file.
608"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9504373177842566,"14. Crowdsourcing and Research with Human Subjects
609"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9518950437317785,"Question: For crowdsourcing experiments and research with human subjects, does the paper
610"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9533527696793003,"include the full text of instructions given to participants and screenshots, if applicable, as
611"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9548104956268222,"well as details about compensation (if any)?
612"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.956268221574344,"Answer: [NA]
613"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9577259475218659,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
614"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9591836734693877,"Guidelines:
615"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9606413994169096,"• The answer NA means that the paper does not involve crowdsourcing nor research with
616"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9620991253644315,"human subjects.
617"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9635568513119533,"• Including this information in the supplemental material is fine, but if the main contribu-
618"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650145772594753,"tion of the paper involves human subjects, then as much detail as possible should be
619"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9664723032069971,"included in the main paper.
620"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967930029154519,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
621"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693877551020408,"or other labor should be paid at least the minimum wage in the country of the data
622"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708454810495627,"collector.
623"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9723032069970845,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
624"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737609329446064,"Subjects
625"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752186588921283,"Question: Does the paper describe potential risks incurred by study participants, whether
626"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766763848396501,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
627"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978134110787172,"approvals (or an equivalent approval/review based on the requirements of your country or
628"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795918367346939,"institution) were obtained?
629"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9810495626822158,"Answer: [NA]
630"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825072886297376,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
631"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839650145772595,"Guidelines:
632"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854227405247813,"• The answer NA means that the paper does not involve crowdsourcing nor research with
633"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868804664723032,"human subjects.
634"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883381924198251,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
635"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897959183673469,"may be required for any human subjects research. If you obtained IRB approval, you
636"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912536443148688,"should clearly state this in the paper.
637"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927113702623906,"• We recognize that the procedures for this may vary significantly between institutions
638"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941690962099126,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
639"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9956268221574344,"guidelines for their institution.
640"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970845481049563,"• For initial submissions, do not include any information that would break anonymity (if
641"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9985422740524781,"applicable), such as the institution conducting the review.
642"
