Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002136752136752137,"Adversarial training (AT) with samples generated by Fast Gradient Sign Method
1"
ABSTRACT,0.004273504273504274,"(FGSM), also known as FGSM-AT, is a computationally simple method to train
2"
ABSTRACT,0.00641025641025641,"robust networks. However, during its training procedure, an unstable mode of
3"
ABSTRACT,0.008547008547008548,"“catastrophic overfitting” has been identified in Wong et al. [2020], where the robust
4"
ABSTRACT,0.010683760683760684,"accuracy abruptly drops to zero within a single training step. Existing methods use
5"
ABSTRACT,0.01282051282051282,"gradient regularizers or random initialization tricks to attenuate this issue, whereas
6"
ABSTRACT,0.014957264957264958,"they either take high computational cost or lead to lower robust accuracy. In this
7"
ABSTRACT,0.017094017094017096,"work, we provide the first study which thoroughly examines a collection of tricks
8"
ABSTRACT,0.019230769230769232,"from three perspectives: Data Initialization, Network Structure, and Optimization,
9"
ABSTRACT,0.021367521367521368,"to overcome the catastrophic overfitting in FGSM-AT. Surprisingly, we find that
10"
ABSTRACT,0.023504273504273504,"simple tricks, i.e., masking partial pixels (even without randomness), setting a large
11"
ABSTRACT,0.02564102564102564,"convolution stride and smooth activation functions, or regularizing the weights of
12"
ABSTRACT,0.027777777777777776,"the first convolutional layer can effectively tackle the overfitting issue. Extensive
13"
ABSTRACT,0.029914529914529916,"results on a range of network architectures validate the effectiveness of each
14"
ABSTRACT,0.03205128205128205,"proposed tricks, and the combinations of tricks are also investigated. For example,
15"
ABSTRACT,0.03418803418803419,"trained with PreActResNet-18 on CIFAR-10, our method attains 51.3% accuracy
16"
ABSTRACT,0.03632478632478633,"against PGD-10 attacker and 46.4% accuracy against AutoAttack, demonstrating
17"
ABSTRACT,0.038461538461538464,"that pure FGSM-AT is capable of enabling robust learners. We will release our
18"
ABSTRACT,0.0405982905982906,"code to encourage future exploration on unleashing the potential of FGSM-AT.
19"
INTRODUCTION,0.042735042735042736,"1
Introduction
20"
INTRODUCTION,0.04487179487179487,"Convolution neural networks (CNNs), though achieving compelling performances on various visual
21"
INTRODUCTION,0.04700854700854701,"recognition tasks, are vulnerable to adversarial perturbations Szegedy et al. [2013]. To effectively
22"
INTRODUCTION,0.049145299145299144,"defend against such malicious attacks, adversarial examples are utilized as training data for enhancing
23"
INTRODUCTION,0.05128205128205128,"model robustness, a process known as adversarial training (AT). To generate adversarial examples,
24"
INTRODUCTION,0.053418803418803416,"one of the leading approaches is to perturb the data using the sign of the image gradients, namely the
25"
INTRODUCTION,0.05555555555555555,"Fast Gradient Sign Method (FGSM) Goodfellow et al. [2015].
26"
INTRODUCTION,0.057692307692307696,"The adversarial training with FGSM (FGSM-AT) is computationally efficient, and it lies the founda-
27"
INTRODUCTION,0.05982905982905983,"tion for many followups Kurakin et al. [2016], Madry et al. [2018], Zhang et al. [2019]. Nonetheless,
28"
INTRODUCTION,0.06196581196581197,"interestingly, FGSM-AT is not widely used today because of the catastrophic overfitting: the model
29"
INTRODUCTION,0.0641025641025641,"robustness will collapse after a few training epochs Wong et al. [2020]. To mitigate the catastrophic
30"
INTRODUCTION,0.06623931623931624,"overfitting and stabilize FGSM-AT, several methods have been proposed. For instance, Wong et al.
31"
INTRODUCTION,0.06837606837606838,"[2020] pre-add uniformly random noises to images to generate adversarial examples, i.e., turn
32"
INTRODUCTION,0.07051282051282051,"the FGSM attacker into the PGD-1 attacker.
Andriushchenko and Flammarion [2020] propose
33"
INTRODUCTION,0.07264957264957266,"GradAlign, which regularizes the AT via maximizing the gradient alignment of the perturbations.
34"
INTRODUCTION,0.07478632478632478,"While these approaches successfully alleviate the catastrophic overfitting, some limitations . For
35"
INTRODUCTION,0.07692307692307693,"example, GradAlign requires an extra forward pass compared to the vanilla FGSM-AT, which sig-
36"
INTRODUCTION,0.07905982905982906,"nificantly increases the computational cost; Fast-AT in Wong et al. [2020] shows a relatively lower
37"
INTRODUCTION,0.0811965811965812,"robustness, and may still collapse if training with larger networks.
38"
INTRODUCTION,0.08333333333333333,"In this paper, we aim to develop more effective and computationally efficient solutions for attenuating
39"
INTRODUCTION,0.08547008547008547,"this catastrophic overfitting. Specifically, we revisit FGSM-AT and propose to stabilize its training
40"
INTRODUCTION,0.0876068376068376,"from the following three perspectives:
41"
INTRODUCTION,0.08974358974358974,"• Data Initialization. Following the idea of adding random perturbations Madry et al. [2018], Wong
42"
INTRODUCTION,0.09188034188034189,"et al. [2020], we propose to randomly mask a subset of the input pixels to stabilize FGSM-AT,
43"
INTRODUCTION,0.09401709401709402,"dubbed FGSM-Mask. Surprisingly, additional analysis suggests that the masking process does not
44"
INTRODUCTION,0.09615384615384616,"necessarily need to be set as random during training—we find that applying a pre-defined masking
45"
INTRODUCTION,0.09829059829059829,"pattern to the training set also effectively stabilizes FGSM-AT. This observation also holds for
46"
INTRODUCTION,0.10042735042735043,"adding perturbations as the initialization in Wong et al. [2020], challenging the general belief that
47"
INTRODUCTION,0.10256410256410256,"randomness is the key factor for stabilizing AT.
48"
INTRODUCTION,0.1047008547008547,"• Network Structure. We identify two architectural elements that affect FGSM-AT. Firstly, in
49"
INTRODUCTION,0.10683760683760683,"addition to boosting robustness as shown in Xie et al. [2020], we find a smoother activation function
50"
INTRODUCTION,0.10897435897435898,"can make FGSM-AT more stable. Secondly, we find vanilla FGSM-AT can effectively train ViTs
51"
INTRODUCTION,0.1111111111111111,"without showing catastrophic overfitting. We conjecture this phenomenon may be related to how
52"
INTRODUCTION,0.11324786324786325,"CNNs and ViTs extract features: i.e., CNNs extract features from overlapped image regions (where
53"
INTRODUCTION,0.11538461538461539,"stride size < kernel size), while ViT extract features from non-overlapped image patches (where
54"
INTRODUCTION,0.11752136752136752,"stride size = kernel size). By simply increasing the stride size of the first convolution layer in a
55"
INTRODUCTION,0.11965811965811966,"CNN, we find the resulted model can stably train with FGSM-AT.
56"
INTRODUCTION,0.12179487179487179,"• Optimization. GradAlign Andriushchenko and Flammarion [2020] stabilizes the FGSM-AT by
57"
INTRODUCTION,0.12393162393162394,"setting the norm of the gradients as a regularization term. To further reduce the computational
58"
INTRODUCTION,0.12606837606837606,"cost, we propose ConvNorm, a regularization term that simply constrains the weights of the
59"
INTRODUCTION,0.1282051282051282,"first convolution layer. Different from GradAlign which introduces a significant amount of extra
60"
INTRODUCTION,0.13034188034188035,"computations, our ConvNorm can work as nearly computationally efficient as the vanilla FGSM-AT.
61"
INTRODUCTION,0.13247863247863248,"Our contributions. In summary, we discover a bag of tricks that effectively alleviate the catastrophic
62"
INTRODUCTION,0.1346153846153846,"overfitting in FGSM-AT from different perspectives. We extensively validate the effectiveness of our
63"
INTRODUCTION,0.13675213675213677,"methods with a range of different network structures on the popular CIFAR-10 dataset. Based on our
64"
INTRODUCTION,0.1388888888888889,"results, we can conclude that the pure FGSM-AT is capable of enabling robust learners.
65"
PRELIMINARIES,0.14102564102564102,"2
Preliminaries
66"
PRELIMINARIES,0.14316239316239315,"Given a neural classifier f with parameters θ, we denote x and y as input data and labels from the data
67"
PRELIMINARIES,0.1452991452991453,"generator D, respectively . δ represents the perturbations and L is the cross-entropy loss typically
68"
PRELIMINARIES,0.14743589743589744,"used for image classification tasks.
69"
PRELIMINARIES,0.14957264957264957,"Adversarial Training: We can formulate the adversarial training as an optimization problem Madry
70"
PRELIMINARIES,0.1517094017094017,"et al. [2018] as:
71"
PRELIMINARIES,0.15384615384615385,"min
θ
E(x,y)∼D

max
δ∈∆L(fθ(x + δ), y)

.
(1)"
PRELIMINARIES,0.15598290598290598,"Among different methods for generating adversarial examples, we chose two popular ones to study:
72"
PRELIMINARIES,0.1581196581196581,"• FGSM: Goodfellow et al. [2015] first propose Fast Gradient Sign Method (FGSM) to generate the
73"
PRELIMINARIES,0.16025641025641027,"perturbation δ as follows:
74"
PRELIMINARIES,0.1623931623931624,"δ = ϵ sign(∇xL(fθ(x), y)),
(2)"
PRELIMINARIES,0.16452991452991453,"• PGD: Madry et al. [2018] propose a strong iterative version with a random start based on FGSM,
75"
PRELIMINARIES,0.16666666666666666,"name projected gradient descent (PGD) as:
76"
PRELIMINARIES,0.16880341880341881,"xt+1 = Π∥δ∥∞≤ϵ (xt + αsign(∇xtL(fθ(xt), y))) ,
(3)"
PRELIMINARIES,0.17094017094017094,"where the α denotes the step size of each iteration. PGD provides a better choice for adversarial
77"
PRELIMINARIES,0.17307692307692307,"examples, but it will also cost much more time than FGSM. In the following sections, we call
78"
PRELIMINARIES,0.1752136752136752,"adversarial training with FGSM as FGSM-AT and correspondingly, PGD-AT. where ϵ denotes the
79"
PRELIMINARIES,0.17735042735042736,"maximum size of perturbations.
80"
PRELIMINARIES,0.1794871794871795,"Catastrophic Overfitting:
Wong et al. [2020] believe that non-zero initialization for perturbations
81"
PRELIMINARIES,0.18162393162393162,"is the key to avoiding the overfitting issue and propose to add uniform random noise during each
82"
PRELIMINARIES,0.18376068376068377,"training iteration. The detailed procedure is illustrated in the following equations:
83"
PRELIMINARIES,0.1858974358974359,"δ = Uniform(−ϵ, +ϵ)"
PRELIMINARIES,0.18803418803418803,"δ = δ + α sign(∇xL(fθ(x), y))"
PRELIMINARIES,0.19017094017094016,"δ = max(min(δ, ϵ), −ϵ)
(4)"
PRELIMINARIES,0.19230769230769232,"Andriushchenko and Flammarion [2020] propose a regularization method GradAlign to maximize
84"
PRELIMINARIES,0.19444444444444445,"the gradient alignment between various sets as:
85"
PRELIMINARIES,0.19658119658119658,"E(x,y)∼D

1 −cos(∇xL(fθ(x), y), ∇xL(fθ(x + η), y))

(5)"
PRELIMINARIES,0.1987179487179487,"where η denotes random noise.
86"
BAG OF TRICKS,0.20085470085470086,"3
Bag of Tricks
87"
BAG OF TRICKS,0.202991452991453,"We aim to investigate simple yet effective solutions to overcome the catastrophic overfitting in
88"
BAG OF TRICKS,0.20512820512820512,"FGSM-AT. To stabilize FGSM-AT and make the trained model more robust to adversarial attacks,
89"
BAG OF TRICKS,0.20726495726495728,"we propose strategies from three general perspectives: Data Initialization, Network Structure, and
90"
BAG OF TRICKS,0.2094017094017094,"Optimization. In this section, the experiments are done on CIFAR-10 dataset Krizhevsky [2009]
91"
BAG OF TRICKS,0.21153846153846154,"with PreActResNet-18 He et al. [2016] under the ℓ∞adversarial attack of maximal perturbation
92"
BAG OF TRICKS,0.21367521367521367,"of ϵ = 8/255 without using any additional data. Two kinds of adversarial attacks are designed to
93"
BAG OF TRICKS,0.21581196581196582,"evaluate the robustness of models at the end of training: 10-steps projected gradient descent attack
94"
BAG OF TRICKS,0.21794871794871795,"(PGD-10) Madry et al. [2018] and the standard version of AutoAttack (AA) Croce and Hein [2020b].
95"
BAG OF TRICKS,0.22008547008547008,"Specifically, for the PGD-10 attack, we apply untargeted mode using the ground-truth annotations
96"
BAG OF TRICKS,0.2222222222222222,"with a step size α = 2/255. AutoAttack comprises AutoPGD-CE, AutoPGD-Targeted, FAB Croce
97"
BAG OF TRICKS,0.22435897435897437,"and Hein [2020a], and Square attack Andriushchenko et al. [2020].
98"
BAG OF TRICKS,0.2264957264957265,"Default setting. We set the training framework and hyper-parameters following Pang et al. [2021].
99"
BAG OF TRICKS,0.22863247863247863,"We apply SGD optimizer with a momentum of 0.9, weight decay of 5 × 10−4, and an initial learning
100"
BAG OF TRICKS,0.23076923076923078,"rate of 0.1. ReLU function (without applying label smoothing) is used as the default activation
101"
BAG OF TRICKS,0.2329059829059829,"function. For the CIFAR dataset, we apply random flip and random crop as data augmentation
102"
BAG OF TRICKS,0.23504273504273504,"methods. Following the framework settings in Pang et al. [2021], all models are trained for 110
103"
BAG OF TRICKS,0.23717948717948717,"epochs. The learning rate decays at 105th and 110th epochs. Specially, we report the robustness
104"
BAG OF TRICKS,0.23931623931623933,"results on the last checkpoint. It should be noted that the final result might not be the best during the
105"
BAG OF TRICKS,0.24145299145299146,"training process. Our experiments are conducted with NVIDIA TITAN XP GPUs.
106"
BAG OF TRICKS,0.24358974358974358,"Methods
AT
PreActResNet-18
WideResNet-34-10
Clean
PGD-10
AA
Clean
PGD-10
AA"
BAG OF TRICKS,0.24572649572649571,Baseline
BAG OF TRICKS,0.24786324786324787,"F+FGSM
86.4%
46.7%
41.0%
89.4%
0%
0%
FGSM+GradAlign
81.2%
48.7%
44.0%
81.2%
48.7%
44.0%
PGD-10
82.6%
53.1%
48.3%
86.1%
56.5%
52.2%"
BAG OF TRICKS,0.25,"Data initialization
FGSM-Mask
82.5%
50.0%
44.2%
79.9%
33.7%
29.7%
FGSM-Mask-fixed
80.7%
48.6%
43.1%
72.3%
24.3%
20.9%"
BAG OF TRICKS,0.25213675213675213,"Network Structure
FGSM-Smooth
74.8%
48.5%
43.1%
75.6%
48.6%
44.2%
FGSM-Str2
83.1%
48.7%
44.4%
85.0%
50.4%
46.7%"
BAG OF TRICKS,0.25427350427350426,"Optimization
FGSM+GradNorm
82.4%
47.2%
42.7%
82.8%
50.7%
46.2%
FGSM+WeightNorm
81.7%
48.3%
42.8%
85.7%
48.8%
45.7%"
BAG OF TRICKS,0.2564102564102564,Table 1: Robustness performances of various methods on PreActResNet-18 and WideResNet-34-10. x x
BAG OF TRICKS,0.25854700854700857,x after FGSM-Mask
BAG OF TRICKS,0.2606837606837607,add random noise
BAG OF TRICKS,0.26282051282051283,add mask
BAG OF TRICKS,0.26495726495726496,x after F+FGSM
BAG OF TRICKS,0.2670940170940171,Figure 1: FGSM-Mask V.S. F+FGSM on an input image
DATA INITIALIZATION,0.2692307692307692,"3.1
Data Initialization
107"
DATA INITIALIZATION,0.27136752136752135,"Wong et al. [2020] firstly identify the catastrophic overfitting faced in FGSM-AT and propose to
108"
DATA INITIALIZATION,0.27350427350427353,"resolve this issue by initializing images with uniform noise with size α = ϵ, namely Fast FGSM-AT
109"
DATA INITIALIZATION,0.27564102564102566,"(F+FGSM). As shown in Equation (4), the method is also termed “random initialization” since it
110"
DATA INITIALIZATION,0.2777777777777778,"randomly adds uniform perturbations during different training iterations. This method has been
111"
DATA INITIALIZATION,0.2799145299145299,"shown the capability to prevent general catastrophic overfitting and defend the models from PGD
112"
DATA INITIALIZATION,0.28205128205128205,"attacks.
113"
DATA INITIALIZATION,0.2841880341880342,"FGSM-Mask. Inspired by the core idea of F+FGSM, in this paper, we propose to mask randomly
114"
DATA INITIALIZATION,0.2863247863247863,"a proportion of the input pixels to stabilize the training procedure of FGSM-AT, which we term as
115"
DATA INITIALIZATION,0.28846153846153844,"FGSM-Mask. Fig 1 demonstrates the comparison of FGSM-Mask and F+FGSM when generating
116"
DATA INITIALIZATION,0.2905982905982906,"adversarial examples. In each iteration, FGSM-Mask zeros out some randomly chosen pixels of each
117"
DATA INITIALIZATION,0.29273504273504275,"image x with a mask M according to a given mask ratio. Then the masked image x ⊗M is fed to the
118"
DATA INITIALIZATION,0.2948717948717949,"model to generate adversarial examples via FGSM as:
119"
DATA INITIALIZATION,0.297008547008547,"δ = α sign(∇x⊗ML(fθ(x ⊗M), y)),
(6)"
DATA INITIALIZATION,0.29914529914529914,"Compared with the random initialization method in F+FGSM (Equation (4)), our method exhibits
120"
DATA INITIALIZATION,0.30128205128205127,"a much simpler form—Our FGSM-Mask simply randomizes the mask instead of manipulating the
121"
DATA INITIALIZATION,0.3034188034188034,"original pixel values.
122"
DATA INITIALIZATION,0.3055555555555556,"To demonstrate the effectiveness of our FGSM-Mask, we mask images with different ratios and
123"
DATA INITIALIZATION,0.3076923076923077,"present the robust accuracy in Table 2 and Figure 2 (a). With a mask ratio of 0%, our method is
124"
DATA INITIALIZATION,0.30982905982905984,"reduced to the vanilla FGSM-AT, and therefore it suffers from catastrophic overfitting. As the mask
125"
DATA INITIALIZATION,0.31196581196581197,"ratio increases, the models trained with FGSM-Mask become more stable. A small mask ratio like
126"
DATA INITIALIZATION,0.3141025641025641,"10% or 20% can already attenuate the overfitting issue but the robust accuracy still drops to near-zero
127"
DATA INITIALIZATION,0.3162393162393162,"after decreasing the learning rate. With a mask ratio higher than 30%, the catastrophic overfitting is
128"
DATA INITIALIZATION,0.31837606837606836,"entirely resolved: the robust accuracy stably remains at 50.0%, outperforming F+FGSM (46.7%) by
129"
DATA INITIALIZATION,0.32051282051282054,"more than 3%.
130"
DATA INITIALIZATION,0.32264957264957267,"Randomized
Mask Ratio
Robust
Accuracy"
DATA INITIALIZATION,0.3247863247863248,"Fixed
Mask Ratio
Robust
Accuracy
0~20%
0%
0~20%
0%
30%
50.0%
30%
0%
40%
49.3%
40%
48.6%
50%
49.0%
50%
48.6%
Table 2: Robust accuracy V.S. mask ratio for FGSM-Mask
and FGSM-Mask-Fixed."
DATA INITIALIZATION,0.3269230769230769,"FGSM-Mask-Fixed. Additionally, we
131"
DATA INITIALIZATION,0.32905982905982906,"observe that the randomness of mask-
132"
DATA INITIALIZATION,0.3311965811965812,"ing is not necessary for different train-
133"
DATA INITIALIZATION,0.3333333333333333,"ing iterations.
Instead, simply using
134"
DATA INITIALIZATION,0.33547008547008544,"a fixed masking pattern throughout the
135"
DATA INITIALIZATION,0.33760683760683763,"training process is enough to help stabi-
136"
DATA INITIALIZATION,0.33974358974358976,"lize FGSM-AT. It is worth to be noted
137"
DATA INITIALIZATION,0.3418803418803419,"that the AT with fixed masks is equiva-
138"
DATA INITIALIZATION,0.344017094017094,"lent to preparing a pre-defined masked
139"
DATA INITIALIZATION,0.34615384615384615,"adversarial dataset which will be fixed in the entire training process. The model trained with such a
140"
DATA INITIALIZATION,0.3482905982905983,"(a) FGSM-Mask
(b) FGSM-Mask-fixed"
DATA INITIALIZATION,0.3504273504273504,"Figure 2: Robust accuracy of FGSM with various mask ratios. (a) is with the random mask, and (b)
is with the fixed mask."
DATA INITIALIZATION,0.3525641025641026,"masked dataset achieves remarkably stable and decent robust accuracy without applying any addi-
141"
DATA INITIALIZATION,0.3547008547008547,"tional tricks, as shown in Table 2. We call this method FGSM-Mask-Fixed. Similar to FGSM-Mask,
142"
DATA INITIALIZATION,0.35683760683760685,"with relatively lower mask ratios (≥30%), the catastrophic overfitting cannot be fully resolved by
143"
DATA INITIALIZATION,0.358974358974359,"FGSM-Mask-Fixed, and the trained model result in a final robust accuracy of 0%. As shown in
144"
DATA INITIALIZATION,0.3611111111111111,"Figure 2 (b), when increasing the mask ratio to 50%, the model trained with FGSM-Mask-Fixed
145"
DATA INITIALIZATION,0.36324786324786323,"reaches a robust accuracy of 48.6%, outperforming F+FGSM by about 2%. To show how randomized
146"
DATA INITIALIZATION,0.36538461538461536,"mask ratios and the fixed mask ratios influence the final robustness performance, Table 2 presents the
147"
DATA INITIALIZATION,0.36752136752136755,"robust accuracy with both FGSM-Mask and FGSM-Mask-Fixed under various mask ratios.
148"
DATA INITIALIZATION,0.3696581196581197,"The findings in our Data Initialization section challenge the traditional belief that the randomness
149"
DATA INITIALIZATION,0.3717948717948718,"of initialization in different training iterations plays a crucial role in AT Chen et al. [2020], which
150"
DATA INITIALIZATION,0.37393162393162394,"inspires us to revisit the data initialization strategy in F+FGSM. We further find that it is not necessary
151"
DATA INITIALIZATION,0.37606837606837606,"to pursue the randomness of uniform noise during different training epochs. Instead, fixing the
152"
DATA INITIALIZATION,0.3782051282051282,"uniform noise of F+FGSM can also stabilize the FGSM-AT and finally reach a robust accuracy of
153"
DATA INITIALIZATION,0.3803418803418803,"46.5% under PGD-10 adversarial attack, which is comparable to the vanilla F+FGSM.
154"
NETWORK STRUCTURE,0.38247863247863245,"3.2
Network Structure
155"
NETWORK STRUCTURE,0.38461538461538464,"Existing studies have demonstrated that a well-designed network structure can improve the model
156"
NETWORK STRUCTURE,0.38675213675213677,"robustness. Xie et al. [2020], Singla et al. [2021], Wu et al. [2020]. When trained with FGSM-AT,
157"
NETWORK STRUCTURE,0.3888888888888889,"Vision Transformers (ViTs) Dosovitskiy et al. [2020] have shown better robustness compared with
158"
NETWORK STRUCTURE,0.391025641025641,"CNNs Bai et al. [2021], Paul and Chen [2021], Shao et al. [2021]. Furthermore, Xie et al. [2020],
159"
NETWORK STRUCTURE,0.39316239316239315,"Singla et al. [2021], Gowal et al. [2020] effectively boost the model robustness by replacing the
160"
NETWORK STRUCTURE,0.3952991452991453,"original ReLU activation function with smoother ones. However, these approaches only focus on
161"
NETWORK STRUCTURE,0.3974358974358974,"improving the model robustness in the general training process, but have overlooked the potential
162"
NETWORK STRUCTURE,0.3995726495726496,"value of network structure for addressing the catastrophic overfitting in FGSM-AT. In this section,
163"
NETWORK STRUCTURE,0.4017094017094017,"we investigate the role of network structure in FGSM-AT following the ideas of ViTs and smooth
164"
NETWORK STRUCTURE,0.40384615384615385,"activation functions.
165"
NETWORK STRUCTURE,0.405982905982906,"Larger stride for the first convolution layer. We first examine whether using ViTs can resolve the
166"
NETWORK STRUCTURE,0.4081196581196581,"overfitting issue. We implement vanilla FGSM-AT with the compact Vision Transformer (CVT) Has-
167"
NETWORK STRUCTURE,0.41025641025641024,"sani et al. [2021], a Transformer architecture designed for the dataset with a smaller resolution. We
168"
NETWORK STRUCTURE,0.41239316239316237,"observe that the robust accuracy under PGD attacks does not drop to zero during the whole training
169"
NETWORK STRUCTURE,0.41452991452991456,"process, without applying any other tricks, neither random initialization nor regularization. The fact
170"
NETWORK STRUCTURE,0.4166666666666667,"that ViTs can successfully avoid the overfitting issue motivates us to rethink whether we can achieve
171"
NETWORK STRUCTURE,0.4188034188034188,"the same goal simply by modifying the architecture of CNNs. As one big difference between ViTs
172"
NETWORK STRUCTURE,0.42094017094017094,"and CNNs lies in how they process images at the beginning of the network, we propose to simply
173"
NETWORK STRUCTURE,0.4230769230769231,"modify the first convolution layer of CNNs to approach the similar behaviour of ViTs. ViTs begin
174"
NETWORK STRUCTURE,0.4252136752136752,"with a patchify operation, which splits an image into a sequence of non-overlapping patches. Whereas
175"
NETWORK STRUCTURE,0.42735042735042733,"for CNNs, taking PreActResNet-18 as an example, the first layer is a 3 × 3 convolutional layer with
176"
NETWORK STRUCTURE,0.42948717948717946,"stride 1, which results in overlapping sliding windows when computing the convolution features. To
177"
NETWORK STRUCTURE,0.43162393162393164,"mimic the behaviour of ViTs, we propose to enlarge the stride size of CNNs to reduce the overlapped
178"
NETWORK STRUCTURE,0.4337606837606838,"regions between adjacent sliding windows. By simply increasing the stride size from 1 to 2 or 3, the
179"
NETWORK STRUCTURE,0.4358974358974359,"catastrophic overfitting problem is successfully addressed. As shown in Figure 3, when the stride is
180"
NETWORK STRUCTURE,0.43803418803418803,"set to be 1, the robust accuracy quickly drops to zero. When the stride is set to be 2 or 3, the robust
181"
NETWORK STRUCTURE,0.44017094017094016,"accuracy curve performs much more stable. Among different stride options in our study, we find that
182"
NETWORK STRUCTURE,0.4423076923076923,"FGSM-AT with a stride as 2 achieves the highest robust accuracy. Therefore we adopt this setting in
183"
NETWORK STRUCTURE,0.4444444444444444,"later experiments, namely FGSM-Str2.
184"
NETWORK STRUCTURE,0.4465811965811966,"0
20
40
60
80
100
Epochs 0.0 0.1 0.2 0.3 0.4 0.5"
NETWORK STRUCTURE,0.44871794871794873,Robust Accuracy(%)
NETWORK STRUCTURE,0.45085470085470086,"stride=1
stride=2
stride=3"
NETWORK STRUCTURE,0.452991452991453,"Figure 3: Robust accuracy and clean accu-
racy of Large Stride Size CNN. A larger
stride size builds the robustness success-
fully."
NETWORK STRUCTURE,0.4551282051282051,"0
20
40
60
80
100
Epochs 0 5 10 15 20 25 x 2 grad 0 1 2 3 4 5 6"
NETWORK STRUCTURE,0.45726495726495725,max(weights)
NETWORK STRUCTURE,0.4594017094017094,weight
NETWORK STRUCTURE,0.46153846153846156,"Figure 4: Trend of ∥∇x∥2 and the maximum
value of weights. Both increase dramatically
when the overfitting happens."
NETWORK STRUCTURE,0.4636752136752137,"Smooth activation function. We also investigate the role of the activation function in FGSM-
185"
NETWORK STRUCTURE,0.4658119658119658,"AT. We replace the original ReLU activation function with smoother ones and then explore their
186"
NETWORK STRUCTURE,0.46794871794871795,"effectiveness for addressing the overfitting problem. We select four smooth activation functions:
187"
NETWORK STRUCTURE,0.4700854700854701,"SiLU Ramachandran et al. [2018], ELU Clevert et al. [2016], SoftPlus Nair and Hinton [2010], and
188"
NETWORK STRUCTURE,0.4722222222222222,"GELU Hendrycks and Gimpel [2016]. We display the curves of these activation functions and record
189"
NETWORK STRUCTURE,0.47435897435897434,"their robust accuracy during FGSM-AT in Figure 5(a). It can be observed that smooth activation
190"
NETWORK STRUCTURE,0.47649572649572647,"functions can all mitigate or even fully prevent catastrophic overfitting.
191"
NETWORK STRUCTURE,0.47863247863247865,"We also find that the degree of smoothness affects the robustness. For instance, ELU is smoother
192"
NETWORK STRUCTURE,0.4807692307692308,"than GELU and accordingly the robust accuracy of ELU is stabler than that of GELU. Following Xie
193"
NETWORK STRUCTURE,0.4829059829059829,"et al. [2020], we choose SoftPlus to study the effect of function smoothness because the scaler α in
194"
NETWORK STRUCTURE,0.48504273504273504,"Parametric SoftPlus can control its smoothness as the following:
195"
NETWORK STRUCTURE,0.48717948717948717,"f(α, x) = 1"
NETWORK STRUCTURE,0.4893162393162393,"α log(1 + exp(αx)).
(7)"
NETWORK STRUCTURE,0.49145299145299143,"Figure 5(b) shows the curves of SoftPlus when the α is 2, 5, 10 and the according robust accuracy
196"
NETWORK STRUCTURE,0.4935897435897436,"curves. As α decreases, the activation function becomes smoother, and the robust accuracy becomes
197"
NETWORK STRUCTURE,0.49572649572649574,"stabler. Figure 5(b) validates that the smoothness of activation functions has a positive correlation
198"
NETWORK STRUCTURE,0.49786324786324787,"with the stability of FGSM-AT. Here we choose SoftPlus with α = 2 as our baseline shown in Table 1
199"
NETWORK STRUCTURE,0.5,"as it performs the best among smooth activation functions, and we call this method FGSM-Smooth.
200"
OPTIMIZATION,0.5021367521367521,"3.3
Optimization
201"
OPTIMIZATION,0.5042735042735043,"Adding an extra regularization term has been shown capable to prevent the catastrophic overfit-
202"
OPTIMIZATION,0.5064102564102564,"ting in FGSM-AT but can usually result in extra computation overhead. One typical example is
203"
OPTIMIZATION,0.5085470085470085,"GradAlign Andriushchenko and Flammarion [2020], which adds an additional objective to maximize
204"
OPTIMIZATION,0.5106837606837606,"the gradient alignment inside the perturbation set. GradAlign is quite effective for stabilizing FGSM-
205"
OPTIMIZATION,0.5128205128205128,"AT. However, it comes at the cost of an extra computational burden due to an extra forward and
206"
OPTIMIZATION,0.5149572649572649,"backward propagation to compute the gradient of an adversarial set ∇xL(fθ(x+η), y) (Equation (5)).
207 (a) (b) A(x)"
OPTIMIZATION,0.5170940170940171,SoftPlus(x) x x
OPTIMIZATION,0.5192307692307693,"Figure 5: Curves of activation functions and their corresponding robust accuracy. (a) shows the
comparisons between various activation functions. (b) shows the comparisons between Softplus with
different α."
OPTIMIZATION,0.5213675213675214,"In this paper, to avoid the extra forward propagation in GradAlign, we first introduce a novel
208"
OPTIMIZATION,0.5235042735042735,"regularization method which directly regularizes the L2 norm of gradients on input images, referred
209"
OPTIMIZATION,0.5256410256410257,"to as GradNorm. Then to further reduce the computation cost, we design another simple but efficient
210"
OPTIMIZATION,0.5277777777777778,"method by only regularizing the weights on the first layer, referred to as WeightNorm. Both GradNorm
211"
OPTIMIZATION,0.5299145299145299,"and WeightNorm successfully address the overfitting issue and achieve comparable robust accuracy
212"
OPTIMIZATION,0.532051282051282,"with GradAlign, while WeightNorm significantly reduces the computational cost. For instance,
213"
OPTIMIZATION,0.5341880341880342,"WightNorm and GradAlign take 42 seconds and 56 seconds for each training epoch. WeightNorm is
214"
OPTIMIZATION,0.5363247863247863,"24% faster than GradAlgin. Next we illustrate the technical details of GradNorm and WeightNorm.
215"
OPTIMIZATION,0.5384615384615384,"GradNorm. By taking a closer look at the L2 norm of gradients on input images, we observe that
216"
OPTIMIZATION,0.5405982905982906,"the ∥∇x∥2 becomes 100× larger after the 11th epoch as shown in Figure 4. This observation aligns
217"
OPTIMIZATION,0.5427350427350427,"with the conclusion in Kim et al. [2021], which points out that the increasing gradient norm leads
218"
OPTIMIZATION,0.5448717948717948,"to decision boundary distortion and a highly curved loss surface during adversarial training. This
219"
OPTIMIZATION,0.5470085470085471,"distortion hence makes the adversarially trained model vulnerable to multi-step adversarial attacks
220"
OPTIMIZATION,0.5491452991452992,"(e.g., PGD attacks) and leads to catastrophic overfitting. This phenomenon inspires us to design a
221"
OPTIMIZATION,0.5512820512820513,"new regularizer by directly constraining the gradient norm E[∥∇x∥2]:
222"
OPTIMIZATION,0.5534188034188035,"L = L(fθ(x + δ), y) + β∥∇x∥2
(8)"
OPTIMIZATION,0.5555555555555556,"where the hyper-parameter β controls the weight of the regularizer. As shown in Table 1, GradNorm
223"
OPTIMIZATION,0.5576923076923077,"successfully overcome the overfitting issue and achieves a high robust accuracy of 47.2% against
224"
OPTIMIZATION,0.5598290598290598,"PGD-10 attacks, which is comparable to the result of GradAlign (48.7%).
225"
OPTIMIZATION,0.561965811965812,"WeightNorm. Both GradAlign and GradNorm are highly effective in addressing the overfitting issue.
226"
OPTIMIZATION,0.5641025641025641,"However, as aforementioned, they both suffer from high computational cost due to the additional back-
227"
OPTIMIZATION,0.5662393162393162,"propagation requirement. We hereby aim to design a novel regularization method which addresses
228"
OPTIMIZATION,0.5683760683760684,"the overfitting issue without introducing an extra computational burden. We propose WeightNorm,
229"
OPTIMIZATION,0.5705128205128205,"a regularization term that directly exploits the intermediate features of vanilla FGSM-AT models.
230"
OPTIMIZATION,0.5726495726495726,"Since the goal of adversarial training is to let the predictions of adversarial examples close to that of
231"
OPTIMIZATION,0.5747863247863247,"clean samples as much as possible: fθ(x + δ) →fθ(x), we design to optimize the training process
232"
OPTIMIZATION,0.5769230769230769,"by constraining the prediction difference. For simplicity, we only examine initial features generated
233"
OPTIMIZATION,0.5790598290598291,"by the first convolution layer fω, where the ω denotes the weights of the first convolution layer. The
234"
OPTIMIZATION,0.5811965811965812,"fω(x + δ) −fω(x) can be represented as ω(x + δ) −ωx, which is equal to ωδ. Therefore, therefore,
235"
OPTIMIZATION,0.5833333333333334,"pushing f1(x + δ) →f1(x) is to minimize ωδ. We can either regularize the δ (i.e., gradients of
236"
OPTIMIZATION,0.5854700854700855,"images) or the weights ω.
237"
OPTIMIZATION,0.5876068376068376,"Regularizing ω is cheaper than constraining the image gradient (which is essentially δ) as only a
238"
OPTIMIZATION,0.5897435897435898,"part of model parameters are regularized, which avoids the second-order back propagation. After
239"
OPTIMIZATION,0.5918803418803419,"observing the change of ω, we find that the maximum value of the weights also significantly increases
240"
OPTIMIZATION,0.594017094017094,"when the catastrophic overfitting occurs. As shown in Figure 4, larger weights suggest that the
241"
OPTIMIZATION,0.5961538461538461,"network overfits the training data. Therefore, we design a regularizer aiming at constraining ω. The
242"
OPTIMIZATION,0.5982905982905983,"intuition of this regularizer design is to both avoid large values in weights and also reduce the distance
243"
OPTIMIZATION,0.6004273504273504,"between clean features generated by clean samples and adversarial features generated by adversarial
244"
OPTIMIZATION,0.6025641025641025,"samples. We select L1 norm to define the regularizer as:
245"
OPTIMIZATION,0.6047008547008547,"min
ω λL1(fω(x), fω(x + δ))
(9)"
OPTIMIZATION,0.6068376068376068,"where the λ controls the weight of the regularizer and δ is the adversarial perturbation. The proposed
246"
OPTIMIZATION,0.6089743589743589,"regularizer constrains ω and pushes the first-layer intermediate features of adversarial examples to be
247"
OPTIMIZATION,0.6111111111111112,"closer to that of clean samples. Experiments show that this regularizer could prevent the catastrophic
248"
OPTIMIZATION,0.6132478632478633,"overfitting and it does not require an extra forward pass like GradAlign shown in Equation 5.
249"
COMBINATION OF TRICKS,0.6153846153846154,"3.4
Combination of Tricks
250"
COMBINATION OF TRICKS,0.6175213675213675,"Each approach we propose can mitigate the catastrophic overfitting problem individually. To investi-
251"
COMBINATION OF TRICKS,0.6196581196581197,"gate the aggregated effect, we combine some of them and show results in Table 3. Adding the mask to
252"
COMBINATION OF TRICKS,0.6217948717948718,"images and increasing the stride size at the same time do not improve the performance. WeightNorm
253"
COMBINATION OF TRICKS,0.6239316239316239,"does not benefit other tricks. Smooth activation function can benefit masking image or a large stride
254"
COMBINATION OF TRICKS,0.6260683760683761,"size, showing improvement in the robustness performances. After trying different combinations, we
255"
COMBINATION OF TRICKS,0.6282051282051282,"find that combining a large stride size and smooth activation functions have the best performance.
256"
COMBINATION OF TRICKS,0.6303418803418803,"Methods
Performances
Mask
Large Stride Size
Smooth Activation Function
WeightNorm
Clean
PGD-10
AA
✓
✓
82.5%
49.4%
45.1%
✓
✓
81.1%
51.2%
46.1%
✓
✓
82.2%
51.3%
46.4%
✓
✓
✓
81.3%
51.2%
46.1%
Table 3: Performances of FGSM-AT with combined tricks"
SCALABILITY TO LARGE NETWORKS,0.6324786324786325,"4
Scalability to Large Networks
257"
SCALABILITY TO LARGE NETWORKS,0.6346153846153846,"Compared with small networks, the larger networks are more likely to overfit the training data as
258"
SCALABILITY TO LARGE NETWORKS,0.6367521367521367,"the network parameters increase, and the mentioned tricks might not work. As displayed in Table 1,
259"
SCALABILITY TO LARGE NETWORKS,0.6388888888888888,"when the size of the network increases (from PreActResNet-18 to WideResNet-34-10), F+FGSM
260"
SCALABILITY TO LARGE NETWORKS,0.6410256410256411,"results in 0% of robust accuracy under the adversarial attack. To comprehensively validate the
261"
SCALABILITY TO LARGE NETWORKS,0.6431623931623932,"effectiveness of the methods mentioned above, we conduct experiments on WideResNet-34-10 with
262"
SCALABILITY TO LARGE NETWORKS,0.6452991452991453,"the same training recipe as PreActResNet-18. Table 1 exhibit the robustness performances of different
263"
SCALABILITY TO LARGE NETWORKS,0.6474358974358975,"methods on these two networks, and the displayed results are taken at the final checkpoint. For the
264"
SCALABILITY TO LARGE NETWORKS,0.6495726495726496,"masking methods in Data Initialization, the mask ratio is set larger on WideResNet. Compared with
265"
SCALABILITY TO LARGE NETWORKS,0.6517094017094017,"PreActResNet, the effectiveness of masking methods declines, but they still exhibit higher robust
266"
SCALABILITY TO LARGE NETWORKS,0.6538461538461539,"performances than the vanilla F+FGSM. For the methods in Network Structure, both the FGSM-AT
267"
SCALABILITY TO LARGE NETWORKS,0.655982905982906,"with a larger stride size (FGSM-Str2) and with smooth activation functions (FGSM-Smooth) perform
268"
SCALABILITY TO LARGE NETWORKS,0.6581196581196581,"stably on WideResNet, showing comparable results with PreActRest. On both PreActRestNet
269"
SCALABILITY TO LARGE NETWORKS,0.6602564102564102,"and WideRestNet, the FGSM-Str2 generally outperforms the other three tricks. Furthermore, the
270"
SCALABILITY TO LARGE NETWORKS,0.6623931623931624,"combination of tricks is also validated on WideRestNet. Following the optimal settings from Table 3,
271"
SCALABILITY TO LARGE NETWORKS,0.6645299145299145,"we combine the smooth activation function and large stride size in FGSM-AT. With the combined
272"
SCALABILITY TO LARGE NETWORKS,0.6666666666666666,"tricks, the models respectively achieves a robust accuracy of 51.8% and 47.3% under PGD-10 attack
273"
SCALABILITY TO LARGE NETWORKS,0.6688034188034188,"and AA, outperforming all other FGSM-AT methods.
274"
RELATED WORK,0.6709401709401709,"5
Related Work
275"
RELATED WORK,0.6730769230769231,"Adversarial training. Adversarial training has been regarded as one of the most effective strategies
276"
RELATED WORK,0.6752136752136753,"to defend against the adversarial threats to machine learning systems. The idea of adversarial
277"
RELATED WORK,0.6773504273504274,"training origins in Goodfellow et al. [2015] who proposes to combine clean samples and adversarial
278"
RELATED WORK,0.6794871794871795,"examples to train the model. Madry et al. [2018] first demonstrate the optimization problem in
279"
RELATED WORK,0.6816239316239316,"adversarial training and proposes the PGD adversarial attack. Furthermore, advanced adversarial
280"
RELATED WORK,0.6837606837606838,"training methods are proposed. Zhang et al. [2019] apply a regularization term to achieve the balance
281"
RELATED WORK,0.6858974358974359,"between robustness and clean performance. Shafahi et al. [2019] reduce the high cost of adversarial
282"
RELATED WORK,0.688034188034188,"training by recycling the gradient information. Carmon et al. [2019] first augment CIFAR-10 with
283"
RELATED WORK,0.6901709401709402,"500K unlabeled extra data from 80 Million Tiny Images dataset. Some works also summarise the
284"
RELATED WORK,0.6923076923076923,"tricks of AT and the optimal settings for AT. Pang et al. [2021] list the optimal hyperparameters for
285"
RELATED WORK,0.6944444444444444,"PGD-AT on CIFAR-10 dataset. Gowal et al. [2020] introduce weight average(WA) to adversarial
286"
RELATED WORK,0.6965811965811965,"training and find the optimal ratios of extra data to get the best adversarial robustness.
287"
RELATED WORK,0.6987179487179487,"Catastrophic overfitting. Though as an efficient method, FGSM-AT is not popular now because of
288"
RELATED WORK,0.7008547008547008,"its failure against severe attacks, like PGD adversarial attack. Wong et al. [2020] first find that the
289"
RELATED WORK,0.7029914529914529,"robust accuracy under PGD adversarial attack of FGSM-AT will drop to zero after several epochs,
290"
RELATED WORK,0.7051282051282052,"and this phenomena is named as catastrophic overfitting. Rice et al. [2020] think that catastrophic
291"
RELATED WORK,0.7072649572649573,"overfitting is a special case only existed in FGSM-AT and this overfitting phenomenon is due to a
292"
RELATED WORK,0.7094017094017094,"weaker adversarial attacker. Kim et al. [2021] visualize the decision boundary during adversarial
293"
RELATED WORK,0.7115384615384616,"training and find the decision boundary distortion is closely related to the catastrophic overfitting.
294"
RELATED WORK,0.7136752136752137,"They believe that the fixed distance from adversarial examples to clean images are the key causing
295"
RELATED WORK,0.7158119658119658,"the distortion and propose to apply various step sizes for each image.
296"
RELATED WORK,0.717948717948718,"Data initialization. Data initialization has been a common trick in adversarial training, where
297"
RELATED WORK,0.7200854700854701,"random noise is added to images before AT during each iteration. Madry et al. [2018] first add a
298"
RELATED WORK,0.7222222222222222,"random start for PGD-AT. Tramèr et al. [2018] first propose R+FGSM combining a Gaussian random
299"
RELATED WORK,0.7243589743589743,"initialization in a single-step AT. They add Gaussian random noise to images and do FGSM-AT with
300"
RELATED WORK,0.7264957264957265,"a step size of α = ϵ/2, which is not effective against PGD adversarial attack. Wong et al. [2020]
301"
RELATED WORK,0.7286324786324786,"believe that non-zero initialization for perturbations is the key to avoiding overfitting and propose
302"
RELATED WORK,0.7307692307692307,"adding uniform random noise to prevent overfitting.
303"
RELATED WORK,0.7329059829059829,"Regularization. Wong et al. [2020] point out that early stopping is an effective method to get a
304"
RELATED WORK,0.7350427350427351,"robust model trained by FGSM-AT, but the robustness underperforms as the training epochs are
305"
RELATED WORK,0.7371794871794872,"inadequate. Andriushchenko and Flammarion [2020] demonstrate that the catastrophic overfitting is
306"
RELATED WORK,0.7393162393162394,"irrelevant to the sizes of networks. Instead, the local non-linearity was the true reason. To prevent
307"
RELATED WORK,0.7414529914529915,"overfitting, they propose a regularization method called GradAlign, which maximizes the gradient
308"
RELATED WORK,0.7435897435897436,"alignment between various set to stop the catastrophic overfitting.
309"
CONCLUSION,0.7457264957264957,"6
Conclusion
310"
CONCLUSION,0.7478632478632479,"This study proposes a range of tricks to address the catastrophic overfitting in FGSM-AT and
311"
CONCLUSION,0.75,"comprehensively examine their effectiveness on networks with different scales. Our results show
312"
CONCLUSION,0.7521367521367521,"that the proposed tricks can be simple yet effective solutions to stabilize FGSM-AT at a minimal
313"
CONCLUSION,0.7542735042735043,"computational cost. We hope this study could contribute to the achievement of a fully stabilized
314"
CONCLUSION,0.7564102564102564,"FGSM-AT in the future.
315"
REFERENCES,0.7585470085470085,"References
316"
REFERENCES,0.7606837606837606,"Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial
317"
REFERENCES,0.7628205128205128,"training. ArXiv, abs/2007.02617, 2020.
318"
REFERENCES,0.7649572649572649,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
319"
REFERENCES,0.7670940170940171,"a query-efficient black-box adversarial attack via random search. ArXiv, abs/1912.00049, 2020.
320"
REFERENCES,0.7692307692307693,"Yutong Bai, Jieru Mei, Alan Loddon Yuille, and Cihang Xie. Are transformers more robust than
321"
REFERENCES,0.7713675213675214,"cnns? In NeurIPS, 2021.
322"
REFERENCES,0.7735042735042735,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. Unlabeled data
323"
REFERENCES,0.7756410256410257,"improves adversarial robustness. ArXiv, abs/1905.13736, 2019.
324"
REFERENCES,0.7777777777777778,"J. Chen, Yu Cheng, Zhe Gan, Quanquan Gu, and Jingjing Liu. Efficient robust training via backward
325"
REFERENCES,0.7799145299145299,"smoothing. ArXiv, abs/2010.01278, 2020.
326"
REFERENCES,0.782051282051282,"Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
327"
REFERENCES,0.7841880341880342,"learning by exponential linear units (elus). arXiv: Learning, 2016.
328"
REFERENCES,0.7863247863247863,"Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
329"
REFERENCES,0.7884615384615384,"boundary attack. In ICML, 2020a.
330"
REFERENCES,0.7905982905982906,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
331"
REFERENCES,0.7927350427350427,"of diverse parameter-free attacks. ArXiv, abs/2003.01690, 2020b.
332"
REFERENCES,0.7948717948717948,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
333"
REFERENCES,0.7970085470085471,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
334"
REFERENCES,0.7991452991452992,"image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
335"
REFERENCES,0.8012820512820513,"arXiv:2010.11929, 2020.
336"
REFERENCES,0.8034188034188035,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
337"
REFERENCES,0.8055555555555556,"examples. CoRR, abs/1412.6572, 2015.
338"
REFERENCES,0.8076923076923077,"Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy A. Mann, and Pushmeet Kohli. Uncovering the
339"
REFERENCES,0.8098290598290598,"limits of adversarial training against norm-bounded adversarial examples. ArXiv, abs/2010.03593,
340"
REFERENCES,0.811965811965812,"2020.
341"
REFERENCES,0.8141025641025641,"Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi.
342"
REFERENCES,0.8162393162393162,"Escaping the big data paradigm with compact transformers. ArXiv, abs/2104.05704, 2021.
343"
REFERENCES,0.8183760683760684,"Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.
344"
REFERENCES,0.8205128205128205,"ArXiv, abs/1603.05027, 2016.
345"
REFERENCES,0.8226495726495726,"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.
346"
REFERENCES,0.8247863247863247,"Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-step
347"
REFERENCES,0.8269230769230769,"adversarial training. In AAAI, 2021.
348"
REFERENCES,0.8290598290598291,"Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
349"
REFERENCES,0.8311965811965812,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
350"
REFERENCES,0.8333333333333334,"preprint arXiv:1611.01236, 2016.
351"
REFERENCES,0.8354700854700855,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
352"
REFERENCES,0.8376068376068376,"Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2018.
353"
REFERENCES,0.8397435897435898,"Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In
354"
REFERENCES,0.8418803418803419,"ICML, 2010.
355"
REFERENCES,0.844017094017094,"Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial training.
356"
REFERENCES,0.8461538461538461,"ArXiv, abs/2010.00467, 2021.
357"
REFERENCES,0.8482905982905983,"Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. ArXiv, abs/2105.07581, 2021.
358"
REFERENCES,0.8504273504273504,"Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. ArXiv,
359"
REFERENCES,0.8525641025641025,"abs/1710.05941, 2018.
360"
REFERENCES,0.8547008547008547,"Leslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning. In
361"
REFERENCES,0.8568376068376068,"ICML, 2020.
362"
REFERENCES,0.8589743589743589,"Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S.
363"
REFERENCES,0.8611111111111112,"Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In NeurIPS, 2019.
364"
REFERENCES,0.8632478632478633,"Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness
365"
REFERENCES,0.8653846153846154,"of vision transformers. ArXiv, abs/2103.15670, 2021.
366"
REFERENCES,0.8675213675213675,"Vasu Singla, Sahil Singla, David Jacobs, and Soheil Feizi. Low curvature activations reduce overfitting
367"
REFERENCES,0.8696581196581197,"in adversarial training. ArXiv, abs/2102.07861, 2021.
368"
REFERENCES,0.8717948717948718,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
369"
REFERENCES,0.8739316239316239,"and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
370"
REFERENCES,0.8760683760683761,"Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick Mcdaniel. Ensemble
371"
REFERENCES,0.8782051282051282,"adversarial training: Attacks and defenses. ArXiv, abs/1705.07204, 2018.
372"
REFERENCES,0.8803418803418803,"Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial training.
373"
REFERENCES,0.8824786324786325,"ArXiv, abs/2001.03994, 2020.
374"
REFERENCES,0.8846153846153846,"Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu. Do wider neural networks really
375"
REFERENCES,0.8867521367521367,"help adversarial robustness? 2020.
376"
REFERENCES,0.8888888888888888,"Cihang Xie, Mingxing Tan, Boqing Gong, Alan Loddon Yuille, and Quoc V. Le. Smooth adversarial
377"
REFERENCES,0.8910256410256411,"training. ArXiv, abs/2006.14536, 2020.
378"
REFERENCES,0.8931623931623932,"Hongyang R. Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I.
379"
REFERENCES,0.8952991452991453,"Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
380"
REFERENCES,0.8974358974358975,"Checklist
381"
REFERENCES,0.8995726495726496,"1. For all authors...
382"
REFERENCES,0.9017094017094017,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
383"
REFERENCES,0.9038461538461539,"contributions and scope? [Yes]
384"
REFERENCES,0.905982905982906,"(b) Did you describe the limitations of your work? [Yes] See the paragraph in Sec. 6.
385"
REFERENCES,0.9081196581196581,"(c) Did you discuss any potential negative societal impacts of your work? [Yes] See the
386"
REFERENCES,0.9102564102564102,"first paragraph in Sec. 1 for potential impacts. Negative ones have not been identified
387"
REFERENCES,0.9123931623931624,"since we target at defense.
388"
REFERENCES,0.9145299145299145,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
389"
REFERENCES,0.9166666666666666,"them? [Yes]
390"
REFERENCES,0.9188034188034188,"2. If you are including theoretical results...
391"
REFERENCES,0.9209401709401709,"(a) Did you state the full set of assumptions of all theoretical results? [N/A] Our contribu-
392"
REFERENCES,0.9230769230769231,"tions are mostly empirical.
393"
REFERENCES,0.9252136752136753,"(b) Did you include complete proofs of all theoretical results? [N/A] Our contributions are
394"
REFERENCES,0.9273504273504274,"mostly empirical.
395"
REFERENCES,0.9294871794871795,"3. If you ran experiments...
396"
REFERENCES,0.9316239316239316,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
397"
REFERENCES,0.9337606837606838,"mental results (either in the supplemental material or as a URL)? [Yes] All the code
398"
REFERENCES,0.9358974358974359,"would be released with detailed instructions and annotations. The data are described in
399"
REFERENCES,0.938034188034188,"Sec. 3.
400"
REFERENCES,0.9401709401709402,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
401"
REFERENCES,0.9423076923076923,"were chosen)? [Yes] Details are presented in Sec. 3
402"
REFERENCES,0.9444444444444444,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
403"
REFERENCES,0.9465811965811965,"ments multiple times)? [Yes] Error bars are not reported and computation amount are
404"
REFERENCES,0.9487179487179487,"reported.
405"
REFERENCES,0.9508547008547008,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
406"
REFERENCES,0.9529914529914529,"of GPUs, internal cluster, or cloud provider)? [Yes] Our devices are described in Sec.3
407"
REFERENCES,0.9551282051282052,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
408"
REFERENCES,0.9572649572649573,"(a) If your work uses existing assets, did you cite the creators? [Yes] We state the source
409"
REFERENCES,0.9594017094017094,"of DNNs in experimental setup in Sec. 3.
410"
REFERENCES,0.9615384615384616,"(b) Did you mention the license of the assets? [Yes] All used code assets are under MIT
411"
REFERENCES,0.9636752136752137,"License.
412"
REFERENCES,0.9658119658119658,"(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
413"
REFERENCES,0.967948717948718,"We would release our code as an asset.
414"
REFERENCES,0.9700854700854701,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
415"
REFERENCES,0.9722222222222222,"using/curating? [N/A] Our empirical studies are based on public datasets.
416"
REFERENCES,0.9743589743589743,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
417"
REFERENCES,0.9764957264957265,"information or offensive content? [N/A] Our used public datasets generally do not
418"
REFERENCES,0.9786324786324786,"contain personally identifiable information or offensive content.
419"
REFERENCES,0.9807692307692307,"5. If you used crowdsourcing or conducted research with human subjects...
420"
REFERENCES,0.9829059829059829,"(a) Did you include the full text of instructions given to participants and screenshots, if
421"
REFERENCES,0.9850427350427351,"applicable? [N/A] Our experiments involve no human subjects and crowdsourcing.
422"
REFERENCES,0.9871794871794872,"(b) Did you describe any potential participant risks, with links to Institutional Review
423"
REFERENCES,0.9893162393162394,"Board (IRB) approvals, if applicable? [N/A] Our experiments involve no human
424"
REFERENCES,0.9914529914529915,"subjects or crowdsourcing.
425"
REFERENCES,0.9935897435897436,"(c) Did you include the estimated hourly wage paid to participants and the total amount
426"
REFERENCES,0.9957264957264957,"spent on participant compensation? [N/A] Our experiments involve no human subjects
427"
REFERENCES,0.9978632478632479,"or crowdsourcing.
428"
