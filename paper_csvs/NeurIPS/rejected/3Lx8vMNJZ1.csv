Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001890359168241966,"Dueling bandits are widely used to model preferential feedback prevalent in many
1"
ABSTRACT,0.003780718336483932,"applications such as recommendation systems and ranking. In this paper, we study
2"
ABSTRACT,0.005671077504725898,"the Borda regret minimization problem for dueling bandits, which aims to identify
3"
ABSTRACT,0.007561436672967864,"the item with the highest Borda score while minimizing the cumulative regret. We
4"
ABSTRACT,0.00945179584120983,"propose a rich class of generalized linear dueling bandit models, which cover many
5"
ABSTRACT,0.011342155009451797,"existing models. We first prove a regret lower bound of order Ω(d2/3T 2/3) for the
6"
ABSTRACT,0.013232514177693762,"Borda regret minimization problem, where d is the dimension of contextual vectors
7"
ABSTRACT,0.015122873345935728,"and T is the time horizon. To attain this lower bound, we propose an explore-
8"
ABSTRACT,0.017013232514177693,"then-commit type algorithm for the stochastic setting, which has a nearly matching
9"
ABSTRACT,0.01890359168241966,"regret upper bound eO(d2/3T 2/3). We also propose an EXP3-type algorithm for the
10"
ABSTRACT,0.020793950850661626,"adversarial setting, where the underlying model parameter can change at each round.
11"
ABSTRACT,0.022684310018903593,"Our algorithm achieves an eO(d2/3T 2/3) regret, which is also optimal. Empirical
12"
ABSTRACT,0.024574669187145556,"evaluations on both synthetic data and a simulated real-world environment are
13"
ABSTRACT,0.026465028355387523,"conducted to corroborate our theoretical analysis.
14"
INTRODUCTION,0.02835538752362949,"1
Introduction
15"
INTRODUCTION,0.030245746691871456,"Multi-armed bandits (MAB) (Lattimore and Szepesvári, 2020) is an interactive game where at each
16"
INTRODUCTION,0.03213610586011342,"round, an agent chooses an arm to pull and receives a noisy reward as feedback. In contrast to numer-
17"
INTRODUCTION,0.034026465028355386,"ical feedback considered in classic MAB settings, preferential feedback is more natural in various
18"
INTRODUCTION,0.035916824196597356,"online learning tasks including information retrieval Yue and Joachims (2009), recommendation
19"
INTRODUCTION,0.03780718336483932,"systems Sui and Burdick (2014), ranking Minka et al. (2018), crowdsourcing Chen et al. (2013), etc.
20"
INTRODUCTION,0.03969754253308128,"Moreover, numerical feedback is also more difficult to gauge and prone to errors in many real-world
21"
INTRODUCTION,0.04158790170132325,"applications. For example, when provided with items to shop or movies to watch, it is more natural
22"
INTRODUCTION,0.043478260869565216,"for a customer to pick a preferred one than scoring the options. This motivates Dueling Bandits
23"
INTRODUCTION,0.045368620037807186,"(Yue and Joachims, 2009), where the agent repeatedly pulls two arms at a time and is provided with
24"
INTRODUCTION,0.04725897920604915,"feedback being the binary outcome of “duels” between the two arms.
25"
INTRODUCTION,0.04914933837429111,"In dueling bandits problems, the outcome of duels is commonly modeled as Bernoulli random vari-
26"
INTRODUCTION,0.05103969754253308,"ables due to their binary nature. At each round, suppose the agent chooses to compare arm i and j, then
27"
INTRODUCTION,0.052930056710775046,"the binary feedback is assumed to be sampled independently from a Bernoulli distribution. For a duel-
28"
INTRODUCTION,0.054820415879017016,"ing bandits instance with K arms, the probabilistic model of the instance can be fully characterized by
29"
INTRODUCTION,0.05671077504725898,"a K ×K preference probability matrix with each entry being: pi,j = P (arm i is chosen over arm j) .
30"
INTRODUCTION,0.05860113421550094,"In a broader range of applications such as ranking, “arms” are often referred to as “items”. We will
31"
INTRODUCTION,0.06049149338374291,"use these two terms interchangeably in the rest of this paper. One central goal of dueling bandits
32"
INTRODUCTION,0.062381852551984876,"is to devise a strategy to identify the “optimal” item as quickly as possible, measured by either
33"
INTRODUCTION,0.06427221172022685,"sample complexity or cumulative regret. However, the notion of optimality for dueling bandits is way
34"
INTRODUCTION,0.0661625708884688,"harder to define than for multi-armed bandits. The latter can simply define the arm with the highest
35"
INTRODUCTION,0.06805293005671077,"numerical feedback as the optimal arm, while for dueling bandits there is no obvious definition solely
36"
INTRODUCTION,0.06994328922495274,"dependent on {pi,j|i, j ∈[K]}.
37"
INTRODUCTION,0.07183364839319471,"The first few works on dueling bandits imposed strong assumptions on pi,j. For example, Yue et al.
38"
INTRODUCTION,0.07372400756143667,"(2012) assumed that there exists a true ranking that is coherent among all items, and the preference
39"
INTRODUCTION,0.07561436672967864,"probabilities must satisfy both strong stochastic transitivity (SST) and stochastic triangle inequality
40"
INTRODUCTION,0.07750472589792061,"(STI). While relaxations like weak stochastic transitivity (Falahatgar et al., 2018) or relaxed stochastic
41"
INTRODUCTION,0.07939508506616257,"transitivity (Yue and Joachims, 2011) exist, they typically still assume the true ranking exists and the
42"
INTRODUCTION,0.08128544423440454,"preference probabilities are consistent, i.e., pi,j > 1"
INTRODUCTION,0.0831758034026465,"2 if and only if i is ranked higher than j. In reality,
43"
INTRODUCTION,0.08506616257088846,"the existence of such coherent ranking aligned with item preferences is rarely the case. For example,
44"
INTRODUCTION,0.08695652173913043,"pi,j may be interpreted as the probability of one basketball team i beating another team j, and there
45"
INTRODUCTION,0.0888468809073724,"can be a circle among the match advantage relations.
46"
INTRODUCTION,0.09073724007561437,"In this paper, we do not assume such coherent ranking exists and solely rely on the Borda score
47"
INTRODUCTION,0.09262759924385633,"based on preference probabilities. The Borda score B(i) of an item i is the probability that it is
48"
INTRODUCTION,0.0945179584120983,"preferred when compared with another random item, namely B(i) :=
1
K−1
P"
INTRODUCTION,0.09640831758034027,"j̸=i pi,j. The item with
49"
INTRODUCTION,0.09829867674858223,"the highest Borda score is called the Borda winner. The Borda winner is intuitively appealing and
50"
INTRODUCTION,0.1001890359168242,"always well-defined for any set of preferential probabilities. The Borda score also does not require
51"
INTRODUCTION,0.10207939508506617,"the problem instance to obey any consistency or transitivity, and it is considered one of the most
52"
INTRODUCTION,0.10396975425330812,"general criteria.
53"
INTRODUCTION,0.10586011342155009,"To identify the Borda winner, estimations of the Borda scores are needed. Since estimating the Borda
54"
INTRODUCTION,0.10775047258979206,"score for one item requires comparing it with every other items, the sample complexity is prohibitively
55"
INTRODUCTION,0.10964083175803403,"high when there are numerous items. On the other hand, in many real-world applications, the agent
56"
INTRODUCTION,0.11153119092627599,"has access to side information that can assist the evaluation of pi,j. For instance, an e-commerce item
57"
INTRODUCTION,0.11342155009451796,"carries its category as well as many other attributes, and the user might have a preference for a certain
58"
INTRODUCTION,0.11531190926275993,"category (Wang et al., 2018). For a movie, the genre and the plot as well as the directors and actors
59"
INTRODUCTION,0.11720226843100189,"can also be taken into consideration when making choices (Liu et al., 2017).
60"
INTRODUCTION,0.11909262759924386,"Based on the above motivation, we consider Generalized Linear Dueling Bandits. At each round, the
61"
INTRODUCTION,0.12098298676748583,"agent selects two items from a finite set of items and receives a comparison result of the preferred
62"
INTRODUCTION,0.12287334593572778,"item. The comparisons depend on known intrinsic contexts/features associated with each pair of
63"
INTRODUCTION,0.12476370510396975,"items. The contexts can be obtained from upstream tasks, such as topic modeling (Zhu et al., 2012) or
64"
INTRODUCTION,0.1266540642722117,"embedding (Vasile et al., 2016). Our goal is to adaptively select items and minimize the regret with
65"
INTRODUCTION,0.1285444234404537,"respect to the optimal item (i.e., Borda winner). Our main contributions are summarized as follows:
66"
INTRODUCTION,0.13043478260869565,"• We show a hardness result regarding the Borda regret minimization for the (generalized) linear
67"
INTRODUCTION,0.1323251417769376,"model. We prove a worst-case regret lower bound Ω(d2/3T 2/3) for our dueling bandit model,
68"
INTRODUCTION,0.1342155009451796,"showing that even in the stochastic setting, minimizing the Borda regret is difficult. The construc-
69"
INTRODUCTION,0.13610586011342155,"tion and proof of the lower bound are new and might be of independent interest.
70"
INTRODUCTION,0.13799621928166353,"• We propose an explore-then-commit type algorithm under the stochastic setting, which can achieve
71"
INTRODUCTION,0.13988657844990549,"a nearly matching upper bound eO(d2/3T 2/3). When the number of items K is small, the algorithm
72"
INTRODUCTION,0.14177693761814744,"can also be configured to achieve a smaller regret eO
 
(d log K)1/3T 2/3
.
73"
INTRODUCTION,0.14366729678638943,"• We propose an EXP3 type algorithm for linear dueling bandits under the adversarial setting, which
74"
INTRODUCTION,0.14555765595463138,"can achieve a nearly matching upper bound eO
 
(d log K)1/3T 2/3
.
75"
INTRODUCTION,0.14744801512287334,"• We conduct empirical studies to verify the correctness of our theoretical claims. Under both
76"
INTRODUCTION,0.14933837429111532,"synthetic and real-world data settings, our algorithms can outperform all the baselines in terms of
77"
INTRODUCTION,0.15122873345935728,"cumulative regret.
78"
INTRODUCTION,0.15311909262759923,"Notation
In this paper, we use normal letters to denote scalars, lowercase bold letters to denote
79"
INTRODUCTION,0.15500945179584122,"vectors, and uppercase bold letters to denote matrices. For a vector x, ∥x∥denotes its ℓ2-norm. The
80"
INTRODUCTION,0.15689981096408318,"weighted ℓ2-norm associated with a positive-definite matrix A is defined as ∥x∥A =
√"
INTRODUCTION,0.15879017013232513,"x⊤Ax. The
81"
INTRODUCTION,0.16068052930056712,"minimum eigenvalue of a matrix A is written as λmin(A). We use standard asymptotic notations
82"
INTRODUCTION,0.16257088846880907,"including O(·), Ω(·), Θ(·), and eO(·), eΩ(·), eΘ(·) will hide logarithmic factors. For a positive integer
83"
INTRODUCTION,0.16446124763705103,"N, [N] := {1, 2, . . . , N}.
84"
RELATED WORK,0.166351606805293,"2
Related Work
85"
RELATED WORK,0.16824196597353497,"Multi-armed and Contextual Bandits Multi-armed bandit is a problem of identifying the best
86"
RELATED WORK,0.17013232514177692,"choice in a sequential decision-making system. It has been studied in numerous ways with a wide
87"
RELATED WORK,0.1720226843100189,"range of applications (Even-Dar et al., 2002; Lai et al., 1985; Kuleshov and Precup, 2014). Contextual
88"
RELATED WORK,0.17391304347826086,"linear bandit is a special type of bandit problem where the agent is provided with side information, i.e.,
89"
RELATED WORK,0.17580340264650285,"contexts, and rewards are assumed to have a linear structure. Various algorithms (Rusmevichientong
90"
RELATED WORK,0.1776937618147448,"and Tsitsiklis, 2010; Filippi et al., 2010; Abbasi-Yadkori et al., 2011; Li et al., 2017; Jun et al., 2017)
91"
RELATED WORK,0.17958412098298676,"have been proposed to utilize this contextual information.
92"
RELATED WORK,0.18147448015122875,"Dueling Bandits and Its Performance Metrics Dueling bandits is a variant of MAB with preferential
93"
RELATED WORK,0.1833648393194707,"feedback (Yue et al., 2012; Zoghi et al., 2014a, 2015). A comprehensive survey can be found at Bengs
94"
RELATED WORK,0.18525519848771266,"et al. (2021). As discussed previously, the probabilistic structure of a dueling bandits problem is
95"
RELATED WORK,0.18714555765595464,"governed by the preference probabilities, over which an optimal item needs to be defined. Optimality
96"
RELATED WORK,0.1890359168241966,"under the Borda score criteria has been adopted by several previous works (Jamieson et al., 2015;
97"
RELATED WORK,0.19092627599243855,"Falahatgar et al., 2017a; Heckel et al., 2018; Saha et al., 2021a). The most relevant work to ours is
98"
RELATED WORK,0.19281663516068054,"Saha et al. (2021a), where they studied the problem of regret minimization for adversarial dueling
99"
RELATED WORK,0.1947069943289225,"bandits and proved a T-round Borda regret upper bound eO(K1/3T 2/3). They also provide an
100"
RELATED WORK,0.19659735349716445,"Ω(K1/3T 2/3) lower bound for stationary dueling bandits using Borda regret.
101"
RELATED WORK,0.19848771266540643,"Apart from the Borda score, Copeland score is also a widely used criteria (Urvoy et al., 2013;
102"
RELATED WORK,0.2003780718336484,"Zoghi et al., 2015, 2014b; Wu and Liu, 2016; Komiyama et al., 2016). It is defined as C(i) :=
103"
RELATED WORK,0.20226843100189035,"1
K−1
P"
RELATED WORK,0.20415879017013233,"j̸=i 1{pi,j > 1/2}. A Copeland winner is the item that beats the most number of other items.
104"
RELATED WORK,0.2060491493383743,"It can be viewed as a “thresholded” version of Borda winner. In addition to Borda and Copeland
105"
RELATED WORK,0.20793950850661624,"winners, optimality notions such as a von Neumann winner were also studied in Ramamohan et al.
106"
RELATED WORK,0.20982986767485823,"(2016); Dudík et al. (2015); Balsubramani et al. (2016).
107"
RELATED WORK,0.21172022684310018,"Another line of work focuses on identifying the optimal item or the total ranking, assuming the
108"
RELATED WORK,0.21361058601134217,"preference probabilities are consistent. Common consistency conditions include Strong Stochastic
109"
RELATED WORK,0.21550094517958412,"Transitivity (Yue et al., 2012; Falahatgar et al., 2017a,b), Weak Stochastic Transitivity (Falahatgar
110"
RELATED WORK,0.21739130434782608,"et al., 2018; Ren et al., 2019; Wu et al., 2022; Lou et al., 2022), Relaxed Stochastic Transitivity (Yue
111"
RELATED WORK,0.21928166351606806,"and Joachims, 2011) and Stochastic Triangle Inequality. Sometimes the aforementioned transitivity
112"
RELATED WORK,0.22117202268431002,"can also be implied by some structured models like the Bradley–Terry model. We emphasize that
113"
RELATED WORK,0.22306238185255198,"these consistency conditions are not assumed or implicitly implied in our setting.
114"
RELATED WORK,0.22495274102079396,"Contextual Dueling Bandits In Dudík et al. (2015), contextual information is incorporated in the
115"
RELATED WORK,0.22684310018903592,"dueling bandits framework. Later, Saha (2021) studied a structured contextual dueling bandits setting
116"
RELATED WORK,0.22873345935727787,"where each item i has its own contextual vector xi (sometimes called Linear Stochastic Transitivity).
117"
RELATED WORK,0.23062381852551986,"Each item then has an intrinsic score vi equal to the linear product of an unknown parameter vector
118"
RELATED WORK,0.23251417769376181,"θ∗and its contextual vector xi. The preference probability between two items i and j is assumed to
119"
RELATED WORK,0.23440453686200377,"be µ (vi −vj) where µ(·) is the logistic function. These intrinsic scores of items naturally define a
120"
RELATED WORK,0.23629489603024575,"ranking over items. The regret is also computed as the gap between the scores of pulled items and the
121"
RELATED WORK,0.2381852551984877,"best item. While in this paper, we assume that the contextual vectors are associated with item pairs
122"
RELATED WORK,0.24007561436672967,"and define regret on the Borda score. In Section A.1, we provide a more detailed discussion showing
123"
RELATED WORK,0.24196597353497165,"that the setting considered in Saha (2021) can be viewed as a special case of our model.
124"
BACKGROUNDS AND PRELIMINARIES,0.2438563327032136,"3
Backgrounds and Preliminaries
125"
PROBLEM SETTING,0.24574669187145556,"3.1
Problem Setting
126"
PROBLEM SETTING,0.24763705103969755,"We first consider the stochastic preferential feedback model with K items in the fixed time horizon
127"
PROBLEM SETTING,0.2495274102079395,"setting. We denote the item set by [K] and let T be the total number of rounds. At each round t, the
128"
PROBLEM SETTING,0.2514177693761815,"agent can pick any pair of items (it, jt) to compare and receive stochastic feedback about whether
129"
PROBLEM SETTING,0.2533081285444234,"item it is preferred over item jt, (denoted by it ≻jt). We denote the probability of seeing the event
130"
PROBLEM SETTING,0.2551984877126654,"i ≻j as pi,j ∈[0, 1]. Naturally, we assume pi,j + pj,i = 1, and pi,i = 1/2.
131"
PROBLEM SETTING,0.2570888468809074,"In this paper, we are concerned with the generalized linear model (GLM), where there is assumed
132"
PROBLEM SETTING,0.2589792060491493,"to exist an unknown parameter θ∗∈Rd, and each pair of items (i, j) has its own known contex-
133"
PROBLEM SETTING,0.2608695652173913,"tual/feature vector ϕi,j ∈Rd with ∥ϕi,j∥≤1. There is also a fixed known link function (sometimes
134"
PROBLEM SETTING,0.2627599243856333,"called comparison function) µ(·) that is monotonically increasing and satisfies µ(x) + µ(−x) = 1,
135"
PROBLEM SETTING,0.2646502835538752,"e.g. a linear function or the logistic function µ(x) = 1/(1 + e−x). The preference probability is
136"
PROBLEM SETTING,0.2665406427221172,"defined as pi,j = µ(ϕ⊤
i,jθ∗). At each round, denote rt = 1{it ≻jt}, then we have
137"
PROBLEM SETTING,0.2684310018903592,"E[rt|it, jt] = pit,jt = µ(ϕ⊤
it,jtθ∗)."
PROBLEM SETTING,0.27032136105860116,"Then our model can also be written as
138"
PROBLEM SETTING,0.2722117202268431,"rt = µ(ϕ⊤
it,jtθ∗) + ϵt,"
PROBLEM SETTING,0.2741020793950851,"where the noises {ϵt}t∈[T ] are zero-mean, 1-sub-Gaussian and assumed independent from each other.
139"
PROBLEM SETTING,0.27599243856332706,"Note that, given the constraint pi,j +pj,i = 1, it is implied that ϕi,j = −ϕj,i for any i ∈[K], j ∈[K].
140"
PROBLEM SETTING,0.277882797731569,"The agent’s goal is to maximize the cumulative Borda score. The (slightly modified 1) Borda score of
141"
PROBLEM SETTING,0.27977315689981097,item i is defined as B(i) = 1
PROBLEM SETTING,0.28166351606805295,"K
PK
j=1 pi,j, and the Borda winner is defined as i∗= argmaxi∈[K] B(i).
142"
PROBLEM SETTING,0.2835538752362949,"The problem of merely identifying the Borda winner was deemed trivial (Zoghi et al., 2014a;
143"
PROBLEM SETTING,0.28544423440453687,"Busa-Fekete et al., 2018) because for a fixed item i, uniformly random sampling j and receiving
144"
PROBLEM SETTING,0.28733459357277885,"feedback ri,j = Bernoulli(pi,j) yield a Bernoulli random variable with its expectation being the
145"
PROBLEM SETTING,0.2892249527410208,"Borda score B(i). This so-called Borda reduction trick makes identifying the Borda winner as
146"
PROBLEM SETTING,0.29111531190926276,"easy as the best-arm identification for K-armed bandits. Moreover, if the regret is defined as
147"
PROBLEM SETTING,0.29300567107750475,"Regret(T) = PT
t=1(B(i∗) −B(it)), then any optimal algorithms for multi-arm bandits can achieve
148 eO(
√"
PROBLEM SETTING,0.2948960302457467,"T) regret.
149"
PROBLEM SETTING,0.29678638941398866,"However, the above definition of regret does not respect the fact that a pair of items are selected at
150"
PROBLEM SETTING,0.29867674858223064,"each round. When the agent chooses two items to compare, it is natural to define the regret so that
151"
PROBLEM SETTING,0.3005671077504726,"both items contribute equally. A commonly used regret, e.g., in Saha et al. (2021a), has the following
152"
PROBLEM SETTING,0.30245746691871456,"form:
153"
PROBLEM SETTING,0.30434782608695654,"Regret(T) = T
X t=1"
PROBLEM SETTING,0.30623818525519847," 
2B(i∗) −B(it) −B(jt)

,
(1)"
PROBLEM SETTING,0.30812854442344045,"where the regret is defined as the sum of the sub-optimality of both selected arms. Sub-optimality is
154"
PROBLEM SETTING,0.31001890359168244,"measured by the gap between the Borda scores of the compared items and the Borda winner. This
155"
PROBLEM SETTING,0.31190926275992437,"form of regret deems any classical multi-arm bandit algorithm with Borda reduction vacuous because
156"
PROBLEM SETTING,0.31379962192816635,"taking jt into consideration will invoke Θ(T) regret.
157"
PROBLEM SETTING,0.31568998109640833,"Adversarial Setting
Saha et al. (2021b) considered an adversarial setting for the multi-armed case,
158"
PROBLEM SETTING,0.31758034026465026,"where at each round t, the comparison follows a potentially different probability model, denoted by
159"
PROBLEM SETTING,0.31947069943289225,"{pt
i,j}i,j∈[K]. In this paper, we consider its contextual counterpart. Formally, we assume there is an
160"
PROBLEM SETTING,0.32136105860113423,"underlying parameter θ∗
t , and at round t, the preference probability is defined as pt
i,j = µ(ϕ⊤
i,jθ∗
t ).
161"
PROBLEM SETTING,0.32325141776937616,"The Borda score of item i ∈[K] at round t is defined as Bt(i) =
1
K
PK
j=1 pt
i,j, and the Borda
162"
PROBLEM SETTING,0.32514177693761814,"winner at round T is defined as i∗= argmaxi∈[K]
PT
t=1 Bt(i). The T-round regret is thus defined
163"
PROBLEM SETTING,0.3270321361058601,"as Regret(T) = PT
t=1
 
2Bt(i∗) −Bt(it) −Bt(jt)

.
164"
ASSUMPTIONS,0.32892249527410206,"3.2
Assumptions
165"
ASSUMPTIONS,0.33081285444234404,"In this section, we present the assumptions required for establishing theoretical guarantees. Due to
166"
ASSUMPTIONS,0.332703213610586,"the fact that the analysis technique is largely extracted from Li et al. (2017), we follow them to make
167"
ASSUMPTIONS,0.33459357277882795,"assumptions to enable regret minimization for generalized linear dueling bandits.
168"
ASSUMPTIONS,0.33648393194706994,"We make a regularity assumption about the distribution of the contextual vectors:
169"
ASSUMPTIONS,0.3383742911153119,"Assumption 1. There exists a constant λ0 > 0 such that λmin
  1"
ASSUMPTIONS,0.34026465028355385,"K2
PK
i=1
PK
j=1 ϕi,jϕ⊤
i,j

≥λ0.
170"
ASSUMPTIONS,0.34215500945179583,"This assumption is only utilized to initialize the design matrix Vτ = Pτ
t=1 ϕit,jtϕ⊤
it,jt so that the
171"
ASSUMPTIONS,0.3440453686200378,"minimum eigenvalue is large enough. We follow Li et al. (2017) to deem λ0 as a constant.
172"
ASSUMPTIONS,0.34593572778827975,"We also need the following assumption regarding the link function µ(·):
173"
ASSUMPTIONS,0.34782608695652173,"Assumption 2. Let ˙µ be the first-order derivative of µ. We have κ := inf∥x∥≤1,∥θ−θ∗∥≤1 ˙µ(x⊤θ) >
174"
ASSUMPTIONS,0.3497164461247637,"0.
175"
ASSUMPTIONS,0.3516068052930057,"Assuming κ > 0 is necessary to ensure the maximum log-likelihood estimator can converge to the
176"
ASSUMPTIONS,0.3534971644612476,"true parameter θ∗(Li et al., 2017, Section 3). This type of assumption is commonly made in previous
177"
ASSUMPTIONS,0.3553875236294896,"works for generalized linear models (Filippi et al., 2010; Li et al., 2017; Faury et al., 2020).
178"
ASSUMPTIONS,0.3572778827977316,"Another common assumption is regarding the continuity and smoothness of the link function.
179"
ASSUMPTIONS,0.3591682419659735,"1Previous works define Borda score as B′
i =
1
K−1
P"
ASSUMPTIONS,0.3610586011342155,"j̸=i pi,j, excluding the diagonal term pi,i = 1/2. Our
definition is equivalent since the difference between two items satisfies B(i)−Bj = K−1"
ASSUMPTIONS,0.3629489603024575,"K (B′
i −B′
j). Therefore,
the regret will be in the same order for both definitions."
ASSUMPTIONS,0.3648393194706994,"Assumption 3. µ is twice differentiable. Its first and second-order derivatives are upper-bounded by
180"
ASSUMPTIONS,0.3667296786389414,"constants Lµ and Mµ respectively.
181"
ASSUMPTIONS,0.3686200378071834,"This is a very mild assumption. For example, it is easy to verify that the logistic link function satisfies
182"
ASSUMPTIONS,0.3705103969754253,"Assumption 3 with Lµ = Mµ = 1/4.
183"
THE HARDNESS RESULT,0.3724007561436673,"4
The Hardness Result
184"
THE HARDNESS RESULT,0.3742911153119093,“good” ( “bad” ( 
THE HARDNESS RESULT,0.3761814744801512,
THE HARDNESS RESULT,0.3780718336483932,"1
2 · · · 1"
THE HARDNESS RESULT,0.3799621928166352,"2
... ... ...
1
2 · · · 1 2"
THE HARDNESS RESULT,0.3818525519848771,"3
4+
⟨ϕi,j, θ⟩"
THE HARDNESS RESULT,0.3837429111531191,"1
4+
⟨ϕj,i, θ⟩"
THE HARDNESS RESULT,0.3856332703213611,"1
2 · · · 1"
THE HARDNESS RESULT,0.387523629489603,"2
... ... ...
1
2 · · · 1 2 "
THE HARDNESS RESULT,0.389413988657845,
THE HARDNESS RESULT,0.391304347826087,"3
4 + ⟨bit(0), θ⟩
3
4 + ⟨bit(0), θ⟩
· · · 3"
THE HARDNESS RESULT,0.3931947069943289,"4 + ⟨bit(0), θ⟩
3
4 + ⟨bit(1), θ⟩
3
4 + ⟨bit(1), θ⟩
· · · 3"
THE HARDNESS RESULT,0.3950850661625709,"4 + ⟨bit(1), θ⟩
...
...
... ...
3
4 + ⟨bit(2d −1), θ⟩3"
THE HARDNESS RESULT,0.39697542533081287,"4 + ⟨bit(2d −1), θ⟩· · · 3"
THE HARDNESS RESULT,0.3988657844990548,"4 + ⟨bit(2d −1), θ⟩"
THE HARDNESS RESULT,0.4007561436672968,"Figure 1: Illustration of the hard-to-learn preference probability matrix {pθ
i,j}i∈[K],j∈[K]. There are
K = 2d+1 items in total. The first 2d items are “good” items with higher Borda scores, and the
last 2d items are “bad” items. The upper right block {pi,j}i<2d,j≥2d is defined as shown in the blue
bubble. The lower left block satisfies pi,j = 1 −pj,i. For any θ, there exist one and only best item i
such that bit(i) = sign(θ).
This section presents Theorem 4, a worst-case regret lower bound for the stochastic linear dueling
185"
THE HARDNESS RESULT,0.40264650283553877,"bandits. The proof of Theorem 4 relies on a class of hard instances, as shown in Figure 1. We show
186"
THE HARDNESS RESULT,0.4045368620037807,"that any algorithm will incur a certain amount of regret when applied to this hard instance class. The
187"
THE HARDNESS RESULT,0.4064272211720227,"constructed hard instances follow a stochastic linear model, which is a sub-class of the generalized
188"
THE HARDNESS RESULT,0.40831758034026466,"linear model. Saha et al. (2021b) first proposed a similar construction for finite many arms with no
189"
THE HARDNESS RESULT,0.4102079395085066,"contexts. Our construction is for a contextual setting and the proof of the lower bound takes a rather
190"
THE HARDNESS RESULT,0.4120982986767486,"different route.
191"
THE HARDNESS RESULT,0.41398865784499056,"For any d > 0, we construct the class of hard instances as follows. An instance is specified by a vector
192"
THE HARDNESS RESULT,0.4158790170132325,"θ ∈{−∆, +∆}d. The instance contains 2d+1 items (indexed from 0 to 2d+1 −1). The preference
193"
THE HARDNESS RESULT,0.41776937618147447,"probability for an instance is defined by pθ
i,j as:
194"
THE HARDNESS RESULT,0.41965973534971646,"pθ
i,j = 
 "
THE HARDNESS RESULT,0.4215500945179584,"1
2, if i < 2d, j < 2d or if i ≥2d, j ≥2d"
THE HARDNESS RESULT,0.42344045368620037,"3
4, if i < 2d, j ≥2d"
THE HARDNESS RESULT,0.42533081285444235,"1
4, if i ≥2d, j < 2d
+ ⟨ϕi,j, θ⟩,"
THE HARDNESS RESULT,0.42722117202268434,"and the d-dimensional feature vectors ϕi,j are given by
195"
THE HARDNESS RESULT,0.42911153119092627,"ϕi,j = 
 "
THE HARDNESS RESULT,0.43100189035916825,"0, if i < 2d, j < 2d or if i ≥2d, j ≥2d"
THE HARDNESS RESULT,0.43289224952741023,"bit(i), if i < 2d, j ≥2d"
THE HARDNESS RESULT,0.43478260869565216,"−bit(j), if i ≥2d, j < 2d,"
THE HARDNESS RESULT,0.43667296786389415,"where bit(·) is the (shifted) bit representation of non-negative integers, i.e., suppose x has the binary
196"
THE HARDNESS RESULT,0.43856332703213613,"representation x = b0 × 20 + b1 × 21 + · · · + bd−1 × 2d−1, then
197"
THE HARDNESS RESULT,0.44045368620037806,"bit(x) = (2b0 −1, 2b1 −1, . . . , 2bd−1 −1) = 2b −1."
THE HARDNESS RESULT,0.44234404536862004,"Note that bit(·) ∈{−1, +1}d, and that ϕi,j = −ϕj,i is satisfied. The definition of pθ
i,j can be slightly
198"
THE HARDNESS RESULT,0.444234404536862,"tweaked to fit exactly the model described in Section 3 (see Remark 11 in Appendix).
199"
THE HARDNESS RESULT,0.44612476370510395,"Some calculation shows that the Borda scores of the 2d+1 items are:
200"
THE HARDNESS RESULT,0.44801512287334594,"Bθ(i) =
 5 8 + 1"
THE HARDNESS RESULT,0.4499054820415879,"2⟨bit(i), θ⟩, if i < 2d,
3
8, if i ≥2d."
THE HARDNESS RESULT,0.45179584120982985,"Intuitively, the former half of items (those indexed from 0 to 2d −1) are “good” items (one among
201"
THE HARDNESS RESULT,0.45368620037807184,"them is optimal, others are nearly optimal), while the latter half of items are “bad” items. Under
202"
THE HARDNESS RESULT,0.4555765595463138,"such hard instances, every time one of the two pulled items is a “bad” item, then a one-step regret
203"
THE HARDNESS RESULT,0.45746691871455575,"Bθ(i∗) −Bθ(i) ≥1/4 is incurred. To minimize regret, we should thus try to avoid pulling “bad”
204"
THE HARDNESS RESULT,0.45935727788279773,"items. However, in order to identify the best item among all “good” items, comparisons between
205"
THE HARDNESS RESULT,0.4612476370510397,"“good” and “bad” items are necessary. The reason is simply that comparisons between “good” items
206"
THE HARDNESS RESULT,0.46313799621928164,"give no information about the Borda scores as the comparison probabilities are pθ
i,j = 1"
FOR ALL,0.46502835538752363,"2 for all
207"
FOR ALL,0.4669187145557656,"i, j < 2d. Hence, any algorithm that can decently distinguish among the “good” items has to pull
208"
FOR ALL,0.46880907372400754,"“bad” ones for a fair amount of times, and large regret is thus incurred. A similar observation is also
209"
FOR ALL,0.4706994328922495,"made by Saha et al. (2021a).
210"
FOR ALL,0.4725897920604915,"This specific construction emphasizes the intrinsic hardness of Borda regret minimization: to dif-
211"
FOR ALL,0.47448015122873344,"ferentiate the best item from its close competitors, the algorithm must query the bad items to gain
212"
FOR ALL,0.4763705103969754,"information.
213"
FOR ALL,0.4782608695652174,"Formally, this class of hard instances leads to the following regret lower bound for both stochastic
214"
FOR ALL,0.48015122873345933,"and adversarial settings:
215"
FOR ALL,0.4820415879017013,"Theorem 4. For any algorithm A, there exists a hard instance {pθ
i,j} with T > 4d2, such that A will
216"
FOR ALL,0.4839319470699433,"incur expected regret at least Ω(d2/3T 2/3).
217"
FOR ALL,0.48582230623818523,"The construction of this hard instance for linear dueling bandits is inspired by the worst-case lower
218"
FOR ALL,0.4877126654064272,"bound for the stochastic linear bandit (Dani et al., 2008), which has the order Ω(d
√"
FOR ALL,0.4896030245746692,"T), while ours is
219"
FOR ALL,0.4914933837429111,"Ω(d2/3T 2/3). The difference is that for the linear or multi-armed stochastic bandit, eliminating bad
220"
FOR ALL,0.4933837429111531,"arms can make further exploration less expensive. But in our case, any amount of exploration will
221"
FOR ALL,0.4952741020793951,"not reduce the cost of further exploration. This essentially means that exploration and exploitation
222"
FOR ALL,0.497164461247637,"must be separate, which is also supported by the fact that a simple explore-then-commit algorithm
223"
FOR ALL,0.499054820415879,"shown in Section 5 can be nearly optimal.
224"
STOCHASTIC CONTEXTUAL DUELING BANDIT,0.500945179584121,"5
Stochastic Contextual Dueling Bandit
225"
ALGORITHM DESCRIPTION,0.502835538752363,"5.1
Algorithm Description
226"
ALGORITHM DESCRIPTION,0.504725897920605,Algorithm 1 BETC-GLM
ALGORITHM DESCRIPTION,0.5066162570888468,"1: Input: time horizon T, number of items K, feature dimension d, feature vectors ϕi,j for i ∈[K],
j ∈[K], exploration rounds τ, error tolerance ϵ, failure probability δ.
2: for t = 1, 2, . . . , τ do
3:
sample it ∼Uniform([K]), jt ∼Uniform([K])
4:
query pair (it, jt) and receive feedback rt
5: end for
6: Find the G-optimal design π(i, j) based on ϕi,j for i ∈[K], j ∈[K]"
ALGORITHM DESCRIPTION,0.5085066162570888,"7: Let N(i, j) =
l
dπ(i,j)"
ALGORITHM DESCRIPTION,0.5103969754253308,"ϵ2
m
for any (i, j) ∈supp(π) , denote N = PK
i=1
PK
j=1 N(i, j)"
ALGORITHM DESCRIPTION,0.5122873345935728,"8: for i ∈[K], j ∈[K], s ∈[N(i, j)] do
9:
set t ←t + 1, set (it, jt) = (i, j)
10:
query pair (it, jt) and receive feedback rt
11: end for
12: Calculate the empirical MLE estimator bθτ+N based on all τ + N samples via (2)
13: Estimate the Borda score for each item:"
ALGORITHM DESCRIPTION,0.5141776937618148,"bB(i) = 1 K K
X"
ALGORITHM DESCRIPTION,0.5160680529300568,"j=1
µ(ϕ⊤
i,j bθτ+N),
bi = argmax
i∈[K]
bB(i)"
ALGORITHM DESCRIPTION,0.5179584120982986,"14: Keep querying (bi,bi) for the rest of the time."
ALGORITHM DESCRIPTION,0.5198487712665406,"We propose an algorithm named Borda Explore-Then-Commit for Generalized Linear Models
227"
ALGORITHM DESCRIPTION,0.5217391304347826,"(BETC-GLM), presented in Algorithm 1. Our algorithm is inspired by the algorithm for generalized
228"
ALGORITHM DESCRIPTION,0.5236294896030246,"linear models proposed by Li et al. (2017).
229"
ALGORITHM DESCRIPTION,0.5255198487712666,"At the high level, Algorithm 1 can be divided into two phases: the exploration phase (Line 2-11)
230"
ALGORITHM DESCRIPTION,0.5274102079395085,"and the exploitation phase (Line 12-14). The exploration phase ensures that the MLE estimator bθ
231"
ALGORITHM DESCRIPTION,0.5293005671077504,"is accurate enough so that the estimated Borda score is within eO(ϵ)-range of the true Borda score
232"
ALGORITHM DESCRIPTION,0.5311909262759924,"(ignoring other quantities). Then the exploitation phase simply chooses the empirical Borda winner
233"
ALGORITHM DESCRIPTION,0.5330812854442344,"to incur small regret.
234"
ALGORITHM DESCRIPTION,0.5349716446124764,"During the exploration phase, the algorithm first performs “pure exploration” (Line 2-5), which can
235"
ALGORITHM DESCRIPTION,0.5368620037807184,"be seen as an initialization step for the algorithm. The purpose of this step is to ensure the design
236"
ALGORITHM DESCRIPTION,0.5387523629489603,"matrix Vτ+N = Pτ+N
t=1 ϕit,jtϕ⊤
it,jt is positive definite.
237"
ALGORITHM DESCRIPTION,0.5406427221172023,"After that, the algorithm will perform the “designed exploration”. Line 6 will find the G-optimal
238"
ALGORITHM DESCRIPTION,0.5425330812854442,"design, which minimizes the objective function g(π) = maxi,j ∥ϕi,j∥2
V(π)−1, where V(π) :=
239
P
i,j π(i, j)ϕi,jϕ⊤
i,j. The G-optimal design π∗(·) satisfies ∥ϕi,j∥2
V(π∗)−1 ≤d, and can be efficiently
240"
ALGORITHM DESCRIPTION,0.5444234404536862,"approximated by the Frank-Wolfe algorithm (See Remark 8 for a detailed discussion). Then the
241"
ALGORITHM DESCRIPTION,0.5463137996219282,"algorithm will follow π(·) found at Line 6 to determine how many samples (Line 7) are needed. At
242"
ALGORITHM DESCRIPTION,0.5482041587901701,"Line 8-11, there are in total N = PK
i=1
PK
j=1 N(i, j) samples queried, and the algorithm shall index
243"
ALGORITHM DESCRIPTION,0.5500945179584121,"them by t = τ + 1, τ + 2, . . . , τ + N.
244"
ALGORITHM DESCRIPTION,0.5519848771266541,"At Line 12, the algorithm collects all the τ + N samples and performs the maximum likelihood
245"
ALGORITHM DESCRIPTION,0.553875236294896,"estimation (MLE). For the generalized linear model, the MLE estimator bθτ+N satisfies:
246 τ+N
X"
ALGORITHM DESCRIPTION,0.555765595463138,"t=1
µ(ϕ⊤
it,jt bθτ+N)ϕit,jt = τ+N
X"
ALGORITHM DESCRIPTION,0.55765595463138,"t=1
rtϕit,jt,
(2)"
ALGORITHM DESCRIPTION,0.5595463137996219,"or equivalently, it can be determined by solving a strongly concave optimization problem:
247"
ALGORITHM DESCRIPTION,0.5614366729678639,"bθτ+N ∈argmax
θ τ+N
X t=1"
ALGORITHM DESCRIPTION,0.5633270321361059,"
rtϕ⊤
it,jtθ −m(ϕ⊤
it,jtθ)

,"
ALGORITHM DESCRIPTION,0.5652173913043478,"where ˙m(·) = µ(·). For the logistic link function, m(x) = log(1 + ex). As a special case of
248"
ALGORITHM DESCRIPTION,0.5671077504725898,"our generalized linear model, the linear model has a closed-form solution for (2). For example, if
249"
ALGORITHM DESCRIPTION,0.5689981096408318,µ(x) = 1
ALGORITHM DESCRIPTION,0.5708884688090737,"2 + x, i.e. pi,j = 1"
ALGORITHM DESCRIPTION,0.5727788279773157,"2 + ϕ⊤
i,jθ∗, then (2) becomes:
250"
ALGORITHM DESCRIPTION,0.5746691871455577,"bθτ+N = V−1
τ+N τ+N
X"
ALGORITHM DESCRIPTION,0.5765595463137996,"t=1
(rt −1/2)ϕit,jt,"
ALGORITHM DESCRIPTION,0.5784499054820416,"where Vτ+N = Pτ+N
t=1 ϕit,jtϕ⊤
it,jt.
251"
ALGORITHM DESCRIPTION,0.5803402646502835,"After the MLE estimator is obtained, Line 13 will calculate the estimated Borda score bB(i) for each
252"
ALGORITHM DESCRIPTION,0.5822306238185255,"item based on bθτ+N, and pick the empirically best one.
253"
A MATCHING REGRET UPPER BOUND,0.5841209829867675,"5.2
A Matching Regret Upper Bound
254"
A MATCHING REGRET UPPER BOUND,0.5860113421550095,"Algorithm 1 can be configured to tightly match the worst-case lower bound. The configuration and
255"
A MATCHING REGRET UPPER BOUND,0.5879017013232514,"performance are described as follows:
256"
A MATCHING REGRET UPPER BOUND,0.5897920604914934,"Theorem 5. Suppose Assumption 1-3 hold and T = Ω(d2). For any δ > 0, if we set τ =
257"
A MATCHING REGRET UPPER BOUND,0.5916824196597353,"C4λ−2
0 (d + log(1/δ)) (C4 is a universal constant) and ϵ = d1/6T −1/3, then with probability at least
258"
A MATCHING REGRET UPPER BOUND,0.5935727788279773,"1 −2δ, Algorithm 1 will incur regret bounded by:
259"
A MATCHING REGRET UPPER BOUND,0.5954631379962193,"O

κ−1d2/3T 2/3q"
A MATCHING REGRET UPPER BOUND,0.5973534971644613,"log
 
T/dδ

."
A MATCHING REGRET UPPER BOUND,0.5992438563327032,"By setting δ = T −1, the expected regret is bounded as eO(κ−1d2/3T 2/3).
260"
A MATCHING REGRET UPPER BOUND,0.6011342155009451,"For linear bandit models, such as the hard-to-learn instances in Section 4, κ is a universal constant.
261"
A MATCHING REGRET UPPER BOUND,0.6030245746691871,"Therefore, Theorem 5 tightly matches the lower bound in Theorem 4, up to logarithmic factors.
262"
A MATCHING REGRET UPPER BOUND,0.6049149338374291,"Remark 6 (Regret for Fewer Arms). In typical scenarios, the number of items K is not exponentially
263"
A MATCHING REGRET UPPER BOUND,0.6068052930056711,"large in the dimension d. In this case, we can choose a different parameter set of τ and ϵ such that
264"
A MATCHING REGRET UPPER BOUND,0.6086956521739131,"Algorithm 1 can achieve a smaller regret bound eO
 
κ−1(d log K)1/3T 2/3
with smaller dependence
265"
A MATCHING REGRET UPPER BOUND,0.610586011342155,"on the dimension d. See Theorem 10 in Appendix A.2.
266"
A MATCHING REGRET UPPER BOUND,0.6124763705103969,"Remark 7 (Regret for Infinitely Many Arms). In most practical scenarios of dueling bandits, it is
267"
A MATCHING REGRET UPPER BOUND,0.6143667296786389,"adequate to consider a finite number K of items (e.g., ranking items). Nonetheless, BETC-GLM
268"
A MATCHING REGRET UPPER BOUND,0.6162570888468809,"can be easily adapted to accommodate infinitely many arms in terms of regret. We can construct a
269"
A MATCHING REGRET UPPER BOUND,0.6181474480151229,"covering over all ϕi,j and perform optimal design and exploration on the covering set. The resulting
270"
A MATCHING REGRET UPPER BOUND,0.6200378071833649,"regret will be the same as our upper bound, i.e., eO(d2/3T 2/3) up to some error caused by the epsilon
271"
A MATCHING REGRET UPPER BOUND,0.6219281663516069,"net argument.
272"
A MATCHING REGRET UPPER BOUND,0.6238185255198487,"Remark 8 (Approximate G-optimal Design). Algorithm 1 assumes an exact G-optimal design π is
273"
A MATCHING REGRET UPPER BOUND,0.6257088846880907,"obtained. In the experiments, we use the Frank-Wolfe algorithm to solve the constraint optimization
274"
A MATCHING REGRET UPPER BOUND,0.6275992438563327,"problem (See Algorithm 5, Appendix G.3). To find a policy π such that g(π) ≤(1 + ε)g(π∗),
275"
A MATCHING REGRET UPPER BOUND,0.6294896030245747,"roughly O(d/ε) optimization steps are needed. Such a near-optimal design will introduce a factor of
276"
A MATCHING REGRET UPPER BOUND,0.6313799621928167,"(1 + ε)1/3 into the upper bounds.
277"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6332703213610587,"6
Adversarial Contextual Dueling Bandit
278"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6351606805293005,"This section addresses Borda regret minimization under the adversarial setting. As we introduced in
279"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6370510396975425,"Section 3.1, the unknown parameter θt can vary for each round t, while the contextual vectors ϕi,j
280"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6389413988657845,"are fixed.
281"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6408317580340265,"Our proposed algorithm, BEXP3, is designed for the contextual linear model. Formally, at round t
282"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6427221172022685,"and given pair (i, j), we have pt
i,j = 1"
ADVERSARIAL CONTEXTUAL DUELING BANDIT,0.6446124763705104,"2 + ⟨ϕi,j, θ∗
t ⟩.
283"
ALGORITHM DESCRIPTION,0.6465028355387523,"6.1
Algorithm Description
284"
ALGORITHM DESCRIPTION,0.6483931947069943,Algorithm 2 BEXP3
ALGORITHM DESCRIPTION,0.6502835538752363,"1: Input: time horizon T, number of items K, feature dimension d, feature vectors ϕi,j for i ∈[K],
j ∈[K], learning rate η, exploration parameter γ.
2: Initialize: q1(i) = 1"
ALGORITHM DESCRIPTION,0.6521739130434783,"K .
3: for t = 1, . . . , T do
4:
Sample items it ∼qt, jt ∼qt.
5:
Query pair (it, jt) and receive feedback rt
6:
Calculate Qt = P"
ALGORITHM DESCRIPTION,0.6540642722117203,"i∈[K]
P"
ALGORITHM DESCRIPTION,0.6559546313799622,"j∈[K] qt(i)qt(j)ϕi,jϕ⊤
i,j, bθt = Q−1
t ϕit,jtrt."
ALGORITHM DESCRIPTION,0.6578449905482041,"7:
Calculate the (shifted) Borda score estimates bBt(i) = ⟨1 K
P"
ALGORITHM DESCRIPTION,0.6597353497164461,"j∈[K] ϕi,j, bθt⟩.
8:
Update for all i ∈[K], set"
ALGORITHM DESCRIPTION,0.6616257088846881,"eqt+1(i) =
exp(η Pt
l=1 bBl(i))
P"
ALGORITHM DESCRIPTION,0.6635160680529301,"j∈[K] exp(η Pt
l=1 bBl(j))
;
qt+1(i) = (1 −γ)eqt+1(i) + γ K ."
ALGORITHM DESCRIPTION,0.665406427221172,9: end for
ALGORITHM DESCRIPTION,0.667296786389414,"Algorithm 2 is adapted from the DEXP3 algorithm in Saha et al. (2021b), which deals with the
285"
ALGORITHM DESCRIPTION,0.6691871455576559,"adversarial multi-armed dueling bandit. Algorithm 2 maintains a distribution qt(·) over [K], initialized
286"
ALGORITHM DESCRIPTION,0.6710775047258979,"as uniform distribution (Line 2). At every round t, two items are chosen following qt independently.
287"
ALGORITHM DESCRIPTION,0.6729678638941399,"Then Line 6 calculates the one-sample unbiased estimate bθt of the true underlying parameter θ∗
t .
288"
ALGORITHM DESCRIPTION,0.6748582230623819,"Line 7 further calculates the unbiased estimate of the (shifted) Borda score. Note that the true Borda
289"
ALGORITHM DESCRIPTION,0.6767485822306238,score at round t satisfies Bt(i) = 1
ALGORITHM DESCRIPTION,0.6786389413988658,"2 + ⟨1 K
P"
ALGORITHM DESCRIPTION,0.6805293005671077,"j∈[K] ϕi,j, θ∗
t ⟩. bBt instead only estimates the second
290"
ALGORITHM DESCRIPTION,0.6824196597353497,"term of the Borda score. This is a choice to simplify the proof. The cumulative estimated score
291
Pt
l=1 bBl(i) can be seen as the estimated cumulative reward of item i at round t. In Line 8, qt+1 is
292"
ALGORITHM DESCRIPTION,0.6843100189035917,"defined by the classic exponential weight update, along with a uniform exploration policy controlled
293"
ALGORITHM DESCRIPTION,0.6862003780718336,"by γ.
294"
UPPER BOUNDS,0.6880907372400756,"6.2
Upper Bounds
295"
UPPER BOUNDS,0.6899810964083176,"Algorithm 2 can also be configured to tightly match the worst-case lower bound:
296"
UPPER BOUNDS,0.6918714555765595,"Theorem 9. Suppose Assumption 1 holds.
If we set η = (log K)2/3d−1/3T −2/3 and γ =
p"
UPPER BOUNDS,0.6937618147448015,"ηd/λ0 = (log K)1/3d1/3T −1/3λ−1/2
0
, then the expected regret is upper-bounded by"
UPPER BOUNDS,0.6956521739130435,"O
 
(d log K)1/3T 2/3
."
UPPER BOUNDS,0.6975425330812854,"Note that the lower bound construction is for the linear model and has K = O(2d), thus exactly
297"
UPPER BOUNDS,0.6994328922495274,"matching the upper bound.
298"
EXPERIMENTS,0.7013232514177694,"7
Experiments
299"
EXPERIMENTS,0.7032136105860114,"0
25000
50000
75000 100000
T 0.0 0.5 1.0 1.5 2.0 2.5"
EXPERIMENTS,0.7051039697542533,Regret(T) ×104
EXPERIMENTS,0.7069943289224953,"UCB-Borda
DEXP3
ETC-Borda
BETC-GLM
BETC-GLM-Match
BEXP3"
EXPERIMENTS,0.7088846880907372,(a) Generated Hard Case
EXPERIMENTS,0.7107750472589792,"0
25000
50000
75000 100000
T 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5"
EXPERIMENTS,0.7126654064272212,Regret(T) ×104
EXPERIMENTS,0.7145557655954632,"UCB-Borda
DEXP3
ETC-Borda
BETC-GLM
BETC-GLM-Match
BEXP3"
EXPERIMENTS,0.7164461247637051,(b) EventTime
EXPERIMENTS,0.718336483931947,"Figure 2: The regret of the proposed algorithms
(BETC-GLM, BEXP3) and the baseline algo-
rithms (UCB-BORDA, DEXP3, ETC-BORDA)."
EXPERIMENTS,0.720226843100189,"This section compares the proposed algorithm
300"
EXPERIMENTS,0.722117202268431,"BETC-GLM with existing ones that are capable
301"
EXPERIMENTS,0.724007561436673,"of minimizing Borda regret. We use random re-
302"
EXPERIMENTS,0.725897920604915,"sponses (generated from fixed preferential matri-
303"
EXPERIMENTS,0.7277882797731569,"ces) to interact with all tested algorithms. Each
304"
EXPERIMENTS,0.7296786389413988,"algorithm is run for 50 times over a time hori-
305"
EXPERIMENTS,0.7315689981096408,"zon of T = 106. We report both the mean and
306"
EXPERIMENTS,0.7334593572778828,"the standard deviation of the cumulative Borda
307"
EXPERIMENTS,0.7353497164461248,"regret and supply some analysis. The follow-
308"
EXPERIMENTS,0.7372400756143668,"ing list summarizes all methods we studies in
309"
EXPERIMENTS,0.7391304347826086,"this section, a more complete description of the
310"
EXPERIMENTS,0.7410207939508506,"methods and parameters are available in Ap-
311"
EXPERIMENTS,0.7429111531190926,"pendix E: BETC-GLM(-MATCH): Algorithm 1
312"
EXPERIMENTS,0.7448015122873346,"proposed in this paper with different parameters.
313"
EXPERIMENTS,0.7466918714555766,"UCB-BORDA: The UCB algorithm (Auer et al.,
314"
EXPERIMENTS,0.7485822306238186,"2002) using Borda reduction. DEXP3: Dueling-Exp3 developed by Saha et al. (2021a). ETC-
315"
EXPERIMENTS,0.7504725897920604,"BORDA: A simple explore-then-commit algorithm that does not take any contextual information into
316"
EXPERIMENTS,0.7523629489603024,"account. BEXP3: The proposed method for adversarial Borda bandits displayed in Algorithm 2.
317"
EXPERIMENTS,0.7542533081285444,"Generated Hard Case
We first test the algorithms on the hard instances constructed in Section 4.
318"
EXPERIMENTS,0.7561436672967864,"We generate θ∗randomly from {−∆, +∆}d with ∆=
1
4d so that the comparison probabilities
319"
EXPERIMENTS,0.7580340264650284,"pθ∗
i,j ∈[0, 1] for all i, j ∈[K]. We pick the dimension d = 6 and the number of arms is therefore
320"
EXPERIMENTS,0.7599243856332704,"K = 2d+1 = 128. Note the dual usage of d in our construction and the model setup in Section 3.1.
321"
EXPERIMENTS,0.7618147448015122,"We refer readers to Remark 11 in Appendix B for more details.
322"
EXPERIMENTS,0.7637051039697542,"As depicted in Figure 2a, the proposed algorithms (BETC-GLM, BEXP3) outperform the baseline
323"
EXPERIMENTS,0.7655954631379962,"algorithms in terms of cumulative regret when reaching the end of time horizon T. For UCB-BORDA,
324"
EXPERIMENTS,0.7674858223062382,"since it is not tailored for the dueling regret definition, it suffers from a linear regret as its second
325"
EXPERIMENTS,0.7693761814744802,"arm is always sampled uniformly at random, leading to a constant regret per round. DEXP3 and
326"
EXPERIMENTS,0.7712665406427222,"ETC-BORDA are two algorithms designed for K-armed dueling bandits. Both are unable to utilize
327"
EXPERIMENTS,0.7731568998109641,"contextual information and thus demand more exploration. As expected, their regrets are higher than
328"
EXPERIMENTS,0.775047258979206,"BETC-GLM or BEXP3.
329"
EXPERIMENTS,0.776937618147448,"Real-world Dataset
To showcase the performance of the algorithms in a real-world setting, we use
330"
EXPERIMENTS,0.77882797731569,"EventTime dataset (Zhang et al., 2016). In this dataset, K = 100 historical events are compared in a
331"
EXPERIMENTS,0.780718336483932,"pairwise fashion by crowd-sourced workers. We first calculate the empirical preference probabilities
332"
EXPERIMENTS,0.782608695652174,"epi,j from the collected responses, and construct a generalized linear model based on the empirical
333"
EXPERIMENTS,0.7844990548204159,"preference probabilities. The algorithms are tested under this generalized linear model. Due to space
334"
EXPERIMENTS,0.7863894139886578,"limitations, more details are deferred to Appendix F.
335"
EXPERIMENTS,0.7882797731568998,"As depicted in Figure 2b, the proposed algorithm BETC-GLM outperforms the baseline algorithms
336"
EXPERIMENTS,0.7901701323251418,"in terms of cumulative regret when reaching the end of time horizon T. The other proposed algorithm
337"
EXPERIMENTS,0.7920604914933838,"BEXP3 performs equally well even when misspecified (the algorithm is designed for linear setting,
338"
EXPERIMENTS,0.7939508506616257,"while the comparison probability follows a logistic model).
339"
CONCLUSION AND FUTURE WORK,0.7958412098298677,"8
Conclusion and Future Work
340"
CONCLUSION AND FUTURE WORK,0.7977315689981096,"In this paper, we introduced Borda regret into the generalized linear dueling bandits setting, along
341"
CONCLUSION AND FUTURE WORK,0.7996219281663516,"with an explore-then-commit type algorithm BETC-GLM and an EXP3 type algorithm BEXP3. The
342"
CONCLUSION AND FUTURE WORK,0.8015122873345936,"algorithms can achieve a nearly optimal regret upper bound, which we corroborate with a matching
343"
CONCLUSION AND FUTURE WORK,0.8034026465028355,"lower bound. The theoretical performance of the algorithms is verified empirically. It demonstrates
344"
CONCLUSION AND FUTURE WORK,0.8052930056710775,"superior performance compared to other baseline methods.
345"
CONCLUSION AND FUTURE WORK,0.8071833648393195,"For future works, due to the fact that our exploration scheme guarantees an accurate estimate in all
346"
CONCLUSION AND FUTURE WORK,0.8090737240075614,"directions, our work can be extended to solve the top-k recovery or ranking problem, as long as a
347"
CONCLUSION AND FUTURE WORK,0.8109640831758034,"proper notion of regret can be identified.
348"
REFERENCES,0.8128544423440454,"References
349"
REFERENCES,0.8147448015122873,"ABBASI-YADKORI, Y., PÁL, D. and SZEPESVARI, C. (2011). Improved algorithms for linear
350"
REFERENCES,0.8166351606805293,"stochastic bandits. In NIPS.
351"
REFERENCES,0.8185255198487713,"AUER, P., CESA-BIANCHI, N. and FISCHER, P. (2002). Finite-time analysis of the multiarmed
352"
REFERENCES,0.8204158790170132,"bandit problem. Machine Learning 47 235–256.
353"
REFERENCES,0.8223062381852552,"BALSUBRAMANI, A., KARNIN, Z., SCHAPIRE, R. E. and ZOGHI, M. (2016). Instance-dependent
354"
REFERENCES,0.8241965973534972,"regret bounds for dueling bandits. In Conference on Learning Theory. PMLR.
355"
REFERENCES,0.8260869565217391,"BENGS, V., BUSA-FEKETE, R., EL MESAOUDI-PAUL, A. and HÜLLERMEIER, E. (2021).
356"
REFERENCES,0.8279773156899811,"Preference-based online learning with dueling bandits: A survey. Journal of Machine Learn-
357"
REFERENCES,0.8298676748582231,"ing Research 22 7–1.
358"
REFERENCES,0.831758034026465,"BUSA-FEKETE, R., HÜLLERMEIER, E. and MESAOUDI-PAUL, A. E. (2018). Preference-based
359"
REFERENCES,0.833648393194707,"online learning with dueling bandits: A survey. ArXiv abs/1807.11398.
360"
REFERENCES,0.8355387523629489,"CHEN, X., BENNETT, P. N., COLLINS-THOMPSON, K. and HORVITZ, E. (2013). Pairwise ranking
361"
REFERENCES,0.8374291115311909,"aggregation in a crowdsourced setting. In Proceedings of the sixth ACM international conference
362"
REFERENCES,0.8393194706994329,"on Web search and data mining.
363"
REFERENCES,0.8412098298676749,"DANI, V., HAYES, T. P. and KAKADE, S. M. (2008). Stochastic linear optimization under bandit
364"
REFERENCES,0.8431001890359168,"feedback. In Annual Conference Computational Learning Theory.
365"
REFERENCES,0.8449905482041588,"DUDÍK, M., HOFMANN, K., SCHAPIRE, R. E., SLIVKINS, A. and ZOGHI, M. (2015). Contextual
366"
REFERENCES,0.8468809073724007,"dueling bandits. ArXiv abs/1502.06362.
367"
REFERENCES,0.8487712665406427,"EVEN-DAR, E., MANNOR, S. and MANSOUR, Y. (2002). Pac bounds for multi-armed bandit and
368"
REFERENCES,0.8506616257088847,"markov decision processes. In Annual Conference Computational Learning Theory.
369"
REFERENCES,0.8525519848771267,"FALAHATGAR, M., HAO, Y., ORLITSKY, A., PICHAPATI, V. and RAVINDRAKUMAR, V. (2017a).
370"
REFERENCES,0.8544423440453687,"Maxing and ranking with few assumptions. Advances in Neural Information Processing Systems
371"
REFERENCES,0.8563327032136105,"30.
372"
REFERENCES,0.8582230623818525,"FALAHATGAR, M., JAIN, A., ORLITSKY, A., PICHAPATI, V. and RAVINDRAKUMAR, V. (2018).
373"
REFERENCES,0.8601134215500945,"The limits of maxing, ranking, and preference learning. In International conference on machine
374"
REFERENCES,0.8620037807183365,"learning. PMLR.
375"
REFERENCES,0.8638941398865785,"FALAHATGAR, M., ORLITSKY, A., PICHAPATI, V. and SURESH, A. T. (2017b). Maximum selection
376"
REFERENCES,0.8657844990548205,"and ranking under noisy comparisons. In International Conference on Machine Learning. PMLR.
377"
REFERENCES,0.8676748582230623,"FAURY, L., ABEILLE, M., CALAUZÈNES, C. and FERCOQ, O. (2020). Improved optimistic
378"
REFERENCES,0.8695652173913043,"algorithms for logistic bandits. In International Conference on Machine Learning. PMLR.
379"
REFERENCES,0.8714555765595463,"FILIPPI, S., CAPPE, O., GARIVIER, A. and SZEPESVÁRI, C. (2010). Parametric bandits: The
380"
REFERENCES,0.8733459357277883,"generalized linear case. Advances in Neural Information Processing Systems 23.
381"
REFERENCES,0.8752362948960303,"HECKEL, R., SIMCHOWITZ, M., RAMCHANDRAN, K. and WAINWRIGHT, M. (2018). Approximate
382"
REFERENCES,0.8771266540642723,"ranking from pairwise comparisons. In International Conference on Artificial Intelligence and
383"
REFERENCES,0.8790170132325141,"Statistics. PMLR.
384"
REFERENCES,0.8809073724007561,"JAMIESON, K., KATARIYA, S., DESHPANDE, A. and NOWAK, R. (2015). Sparse dueling bandits.
385"
REFERENCES,0.8827977315689981,"In Artificial Intelligence and Statistics. PMLR.
386"
REFERENCES,0.8846880907372401,"JUN, K.-S., BHARGAVA, A., NOWAK, R. and WILLETT, R. (2017). Scalable generalized linear
387"
REFERENCES,0.8865784499054821,"bandits: Online computation and hashing. Advances in Neural Information Processing Systems 30.
388"
REFERENCES,0.888468809073724,"KOMIYAMA, J., HONDA, J. and NAKAGAWA, H. (2016). Copeland dueling bandit problem:
389"
REFERENCES,0.8903591682419659,"Regret lower bound, optimal algorithm, and computationally efficient algorithm. In International
390"
REFERENCES,0.8922495274102079,"Conference on Machine Learning. PMLR.
391"
REFERENCES,0.8941398865784499,"KULESHOV, V. and PRECUP, D. (2014). Algorithms for multi-armed bandit problems. arXiv preprint
392"
REFERENCES,0.8960302457466919,"arXiv:1402.6028 .
393"
REFERENCES,0.8979206049149339,"LAI, T. L., ROBBINS, H. ET AL. (1985). Asymptotically efficient adaptive allocation rules. Advances
394"
REFERENCES,0.8998109640831758,"in applied mathematics 6 4–22.
395"
REFERENCES,0.9017013232514177,"LATTIMORE, T. and SZEPESVÁRI, C. (2020). Bandit Algorithms. Cambridge University Press.
396"
REFERENCES,0.9035916824196597,"LI, L., LU, Y. and ZHOU, D. (2017). Provably optimal algorithms for generalized linear contextual
397"
REFERENCES,0.9054820415879017,"bandits. In International Conference on Machine Learning. PMLR.
398"
REFERENCES,0.9073724007561437,"LIU, C., JIN, T., HOI, S. C. H., ZHAO, P. and SUN, J. (2017). Collaborative topic regression for
399"
REFERENCES,0.9092627599243857,"online recommender systems: an online and bayesian approach. Machine Learning 106 651–670.
400"
REFERENCES,0.9111531190926276,"LOU, H., JIN, T., WU, Y., XU, P., GU, Q. and FARNOUD, F. (2022). Active ranking without strong
401"
REFERENCES,0.9130434782608695,"stochastic transitivity. Advances in neural information processing systems .
402"
REFERENCES,0.9149338374291115,"MINKA, T. P., CLEVEN, R. and ZAYKOV, Y. (2018). Trueskill 2: An improved bayesian skill rating
403"
REFERENCES,0.9168241965973535,"system.
404"
REFERENCES,0.9187145557655955,"RAMAMOHAN, S., RAJKUMAR, A. and AGARWAL, S. (2016). Dueling bandits: Beyond condorcet
405"
REFERENCES,0.9206049149338374,"winners to general tournament solutions. In NIPS.
406"
REFERENCES,0.9224952741020794,"REN, W., LIU, J. K. and SHROFF, N. (2019). On sample complexity upper and lower bounds for
407"
REFERENCES,0.9243856332703214,"exact ranking from noisy comparisons. Advances in Neural Information Processing Systems 32.
408"
REFERENCES,0.9262759924385633,"RUSMEVICHIENTONG, P. and TSITSIKLIS, J. N. (2010). Linearly parameterized bandits. Mathemat-
409"
REFERENCES,0.9281663516068053,"ics of Operations Research 35 395–411.
410"
REFERENCES,0.9300567107750473,"SAHA, A. (2021). Optimal algorithms for stochastic contextual preference bandits. Advances in
411"
REFERENCES,0.9319470699432892,"Neural Information Processing Systems 34 30050–30062.
412"
REFERENCES,0.9338374291115312,"SAHA, A., KOREN, T. and MANSOUR, Y. (2021a).
Adversarial dueling bandits.
ArXiv
413"
REFERENCES,0.9357277882797732,"abs/2010.14563.
414"
REFERENCES,0.9376181474480151,"SAHA, A., KOREN, T. and MANSOUR, Y. (2021b). Adversarial dueling bandits. In International
415"
REFERENCES,0.9395085066162571,"Conference on Machine Learning. PMLR.
416"
REFERENCES,0.941398865784499,"SUI, Y. and BURDICK, J. (2014). Clinical online recommendation with subgroup rank feedback. In
417"
REFERENCES,0.943289224952741,"Proceedings of the 8th ACM conference on recommender systems.
418"
REFERENCES,0.945179584120983,"URVOY, T., CLÉROT, F., FÉRAUD, R. and NAAMANE, S. (2013). Generic exploration and k-armed
419"
REFERENCES,0.947069943289225,"voting bandits. In ICML.
420"
REFERENCES,0.9489603024574669,"VASILE, F., SMIRNOVA, E. and CONNEAU, A. (2016). Meta-prod2vec: Product embeddings
421"
REFERENCES,0.9508506616257089,"using side-information for recommendation. In Proceedings of the 10th ACM conference on
422"
REFERENCES,0.9527410207939508,"recommender systems.
423"
REFERENCES,0.9546313799621928,"WANG, J., HUANG, P., ZHAO, H., ZHANG, Z., ZHAO, B. and LEE (2018). Billion-scale commodity
424"
REFERENCES,0.9565217391304348,"embedding for e-commerce recommendation in alibaba. Proceedings of the 24th ACM SIGKDD
425"
REFERENCES,0.9584120982986768,"International Conference on Knowledge Discovery & Data Mining .
426"
REFERENCES,0.9603024574669187,"WU, H. and LIU, X. (2016). Double thompson sampling for dueling bandits. ArXiv abs/1604.07101.
427"
REFERENCES,0.9621928166351607,"WU, Y., JIN, T., LOU, H., XU, P., FARNOUD, F. and GU, Q. (2022). Adaptive sampling for
428"
REFERENCES,0.9640831758034026,"heterogeneous rank aggregation from noisy pairwise comparisons. In International Conference on
429"
REFERENCES,0.9659735349716446,"Artificial Intelligence and Statistics. PMLR.
430"
REFERENCES,0.9678638941398866,"YUE, Y., BRODER, J., KLEINBERG, R. D. and JOACHIMS, T. (2012). The k-armed dueling bandits
431"
REFERENCES,0.9697542533081286,"problem. J. Comput. Syst. Sci. 78 1538–1556.
432"
REFERENCES,0.9716446124763705,"YUE, Y. and JOACHIMS, T. (2009). Interactively optimizing information retrieval systems as a
433"
REFERENCES,0.9735349716446124,"dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine
434"
REFERENCES,0.9754253308128544,"Learning.
435"
REFERENCES,0.9773156899810964,"YUE, Y. and JOACHIMS, T. (2011). Beat the mean bandit. In International Conference on Machine
436"
REFERENCES,0.9792060491493384,"Learning.
437"
REFERENCES,0.9810964083175804,"ZHANG, X., LI, G. and FENG, J. (2016). Crowdsourced top-k algorithms: An experimental
438"
REFERENCES,0.9829867674858223,"evaluation. Proc. VLDB Endow. 9 612–623.
439"
REFERENCES,0.9848771266540642,"ZHU, J., AHMED, A. and XING, E. P. (2012). Medlda: maximum margin supervised topic models.
440"
REFERENCES,0.9867674858223062,"J. Mach. Learn. Res. 13 2237–2278.
441"
REFERENCES,0.9886578449905482,"ZOGHI, M., KARNIN, Z. S., WHITESON, S. and DE RIJKE, M. (2015). Copeland dueling bandits.
442"
REFERENCES,0.9905482041587902,"In NIPS.
443"
REFERENCES,0.9924385633270322,"ZOGHI, M., WHITESON, S., MUNOS, R. and DE RIJKE, M. (2014a). Relative upper confidence
444"
REFERENCES,0.994328922495274,"bound for the k-armed dueling bandit problem. ArXiv abs/1312.3393.
445"
REFERENCES,0.996219281663516,"ZOGHI, M., WHITESON, S., MUNOS, R. and RIJKE, M. (2014b). Relative upper confidence bound
446"
REFERENCES,0.998109640831758,"for the k-armed dueling bandit problem. In International conference on machine learning. PMLR.
447"
