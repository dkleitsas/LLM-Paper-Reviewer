Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00558659217877095,"Graph metric learning methods aim to learn the distance metric over graphs such
that similar graphs are closer and dissimilar graphs are farther apart. This is of
critical importance in many graph classiﬁcation applications such as drug discovery
and epidemics categorization. In many real-world applications, the graphs are
typically evolving over time; labeling graph data is usually expensive and also
requires background knowledge. However, state-of-the-art graph metric learning
techniques consider the input graph as static, and largely ignore the intrinsic
dynamics of temporal graphs; Furthermore, most of these techniques require
abundant labeled examples for training in the representation learning process. To
address the two aforementioned problems, we wish to learn a distance metric only
over fewer temporal graphs, which metric could not only help accurately categorize
seen temporal graphs but also be adapted smoothly to unseen temporal graphs. In
this paper, we ﬁrst propose the streaming-snapshot model to describe temporal
graphs on different time scales. Then we propose the METATAG framework: 1) to
learn the metric over a limited number of streaming-snapshot modeled temporal
graphs, 2) and adapt the learned metric to unseen temporal graphs via a few
examples. Finally, we demonstrate the performance of METATAG in comparison
with state-of-the-art algorithms for temporal graph classiﬁcation problems."
INTRODUCTION,0.0111731843575419,"1
INTRODUCTION"
INTRODUCTION,0.01675977653631285,"Metric learning aims to learn a proper distance metric among data items in the input space, which
reﬂects their underlying relationship. With the prevalence of graph data in many real-world appli-
cations, it is of key importance to design a good distance metric function for graph data, such that
the output value of the function is small for similar graphs and large for dissimilar ones. Many
downstream tasks on the graph data can beneﬁt from such a distance metric. For example, it could
lead to signiﬁcantly improved classiﬁcation accuracy for graph classiﬁcation in many domains such
as protein and drug discovery (Schölkopf et al., 2004; Dai et al., 2016), molecular property predic-
tion (Duvenaud et al., 2015; Gilmer et al., 2017), and epidemic infectious pattern analysis (Derr et al.,
2020; Oettershagen et al., 2020); it could also speed up the labeling of graph data in an active learning
framework (Macskassy, 2009)."
INTRODUCTION,0.0223463687150838,"However, current graph metric learning methods (Shaw et al., 2011; Tsitsulin et al., 2018; Bai et al.,
2019; Li et al., 2019; Yoshida et al., 2019) assume the input graph data as static and ignore evolution
patterns of temporal graphs, which may also provide insights for identifying the graph property (Isella
et al., 2011). To best of our knowledge, there is currently no algorithm designed for learning metrics
over temporal graphs to further involve evolution pattern consideration into the learned metric space.
On the other hand, facing limited i.i.d. data, traditional metric learning methods (Goldberger et al.,
2004; Salakhutdinov & Hinton, 2007) have been extended to the few-shot learning by transferring
the learned metric across different tasks (Vinyals et al., 2016; Snell et al., 2017; Oreshkin et al., 2018;
Allen et al., 2019). Label scarcity problem also occurs in the graph research community, because
labeling graph data is typically expensive and requires background knowledge (Hu et al., 2020a;b; Qiu
et al., 2020), especially for domain-speciﬁc applications such as biological graph data labeling (Zitnik
et al., 2018). Inspired by that, graph metric learning via few-shot examples has recently attracted
many nascent researchers’ attention. But, the majority has been devoted to the node-level metric
learning (Yao et al., 2020; Suo et al., 2020; Huang & Zitnik, 2020; Lan et al., 2020; Wang et al.,"
INTRODUCTION,0.027932960893854747,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0335195530726257,"2020; Ding et al., 2020), only a few nascent efforts focus on the graph-level metrics (Ma et al., 2020;
Chauhan et al., 2020), and all of them ignore the graph dynamics but take static graphs as input."
INTRODUCTION,0.03910614525139665,"Figure 1: An example of metric learning on several
temporal graphs. In the right box, each ‘x’ denotes
a temporal graph representation, and each circle
denotes a class representation."
INTRODUCTION,0.0446927374301676,"To wrap up, these discussed-above observa-
tions bring three bottlenecks to present tempo-
ral graph metric learning algorithms: 1) How
to learn a good metric over temporal graphs,
especially on the entire graph level (i.e., accu-
racy of metrics); 2) How to ensure the learning
process only consumes less labelled temporal
graph data; and 3) How to smoothly apply that
learned metric to identify unseen graphs (i.e.,
ﬂexibility of metrics). In this paper, we wish to
learn a distance metric only over fewer tempo-
ral graphs, which metric (as shown in Figure 1)
could not only help accurately classify seen tem-
poral graphs during each metric learning task,
but also be adapted smoothly to new metric
learning tasks and converge fast (i.e., several
training iterations) to classify unseen temporal
graphs by consuming a few labeled examples."
INTRODUCTION,0.05027932960893855,Our main contributions can be summarized as:
INTRODUCTION,0.055865921787709494,"• To describe the evolving graph in a ﬁne-grained manner, we propose the streaming-snapshot
model that contains multiple time scales suitable for complex real-world scenarios and other
merits are discussed in Section 3.
• To learn the metric over a bunch of streaming-snapshot modelled temporal graphs, we
propose the prototypical temporal graph encoder to extract the lifelong evolution repre-
sentation of a temporal graph with the proposed multi-scale time attention mechanism,
such that temporal graphs from the same class share the similar encoded patterns; To make
the extracted metric rapidly adapt to unseen temporal graphs with only a few examples,
we introduce a meta-learner to transfer and tailor knowledge and encapsulate it with the
prototypical temporal graph encoder into an end-to-end model, called METATAG.
• We conduct the temporal graph classiﬁcation experiments on biological network domain
and social network domain, which show the effectiveness of METATAG compared with
state-of-the-art algorithms. Also, we analyze the convergence speed of METATAG during
the meta-testing, the parameter sensitivity, and the ablation study of each part of METATAG."
PRELIMINARIES,0.061452513966480445,"2
PRELIMINARIES"
PRELIMINARIES,0.0670391061452514,"Graph Metric Learning. Learning a distance metric is closely related to the feature extraction
problem (Globerson & Roweis, 2005; Salakhutdinov & Hinton, 2007). To be speciﬁc, given any
distance metric D, we can measure distance D(xi, xj) between two input feature vectors xi ∈Rm
and xj ∈Rm by computing D′(fθ(xi), fθ(xi)), where fθ is a learnable function mapping the input
feature xi ∈Rm into the latent feature hi = fθ(xi) ∈Rf (Salakhutdinov & Hinton, 2007). The
transformation function fθ could be linear or non-linear (Wang & Sun, 2015). When fθ is a linear
function fθ(xi) = Wxi, learning a generalized Mahalanobis metric D can be expressed as follows."
PRELIMINARIES,0.07262569832402235,"D(xi, yj) =
q"
PRELIMINARIES,0.0782122905027933,"(xi −xj)⊤M(xi −xj) =
q"
PRELIMINARIES,0.08379888268156424,"(xi −xj)⊤W⊤W(xi −xj) =
q"
PRELIMINARIES,0.0893854748603352,(Wxi −Wxj)⊤(Wxi −Wxj)
PRELIMINARIES,0.09497206703910614,"= D′(fθ(xi), fθ(xj)) (1)"
PRELIMINARIES,0.1005586592178771,"where M is some arbitrary positive semi-deﬁnite matrix to be determined for the Mahalanobis
metric D, and M can be decomposed as M = W⊤W. Then the Mahalanobis metric D on the
input feature space is equivalent to the Euclidean metric D′ on the hidden feature space, such that"
PRELIMINARIES,0.10614525139664804,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.11173184357541899,"learning a undetermined metric D (e.g., Mahalanobis) on input feature is equivalent to learning
hidden features on a ﬁxed metric D′ (e.g., Euclidean) (Globerson & Roweis, 2005; Salakhutdinov &
Hinton, 2007; Wang & Sun, 2015; Snell et al., 2017). Also, fθ can be a non-linear transformation for
involving more parameters to model higher-order correlations between input data dimensions than
linear transformations (Salakhutdinov & Hinton, 2007; Wang & Sun, 2015; Snell et al., 2017). Based
on the above analysis, we are ready to model our graph metric learning problem: learning a ""good""
distance metric over pairs of graphs is to learn a ""good"" mapping function fθ of graphs in Euclidean
space. The ""goodness"" is controlled by θ and we discuss how we deﬁne it in Section 3."
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.11731843575418995,"3
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP"
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.12290502793296089,"The table of symbols is summarized in Appendix. To specify, we use bold lowercase letters to denote
column vectors (e.g. a), bold capital letters to denote matrices (e.g., A), and A(i, :) to denote the i-th
row of matrix A. Also, we let the parenthesized superscript denote the timestamp like A(t). We use
graph and network interchangeably in this paper."
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.12849162011173185,"Streaming-Snapshot Model. In the streaming-snapshot model, there exists two kinds of timestamps,
te ∈{0, 1, . . . , Te} denotes the edge timestamp and ts ∈{0, 1, . . . , Ts} denotes the snapshot
timestamp. To be speciﬁc, we describe a temporal graph G as a sequence of timestamped snapshots
{S(ts)}Ts
ts=0, and each timestamped snapshot has a set of timestamped edges labeled as (vi, vj, te, ts).
Note that, these two timestamps are different measures, they do not need to have the comparison
relationship. In Figure 2, we provide a temporal graph example whose Te = 4 and Ts = 2."
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.1340782122905028,"The merits of describing the temporal graph within the streaming-snapshot model include: 1) Carrying
multi-scale complex temporal information. Some social networks change rapidly in the microscopic
view (Leskovec et al., 2008), while some graphs like yeast metabolic graph (Tu et al., 2005) and
repeating frames in video analysis (Li et al., 2020) change slowly in the macroscopic view (Leskovec
et al., 2005). If the input temporal graph has these two evolution patterns (i.e., edge timestamps
and snapshot timestamps), our streaming-snapshot model could handle both of them simultaneously
because streaming model could describe the interaction graph in a rapid and continuous manner
and snapshots could compensate for the complement by modeling episodic, slowly-changing, and
periodical patterns (Aggarwal & Subbian, 2014). If not, our streaming-snapshot is also viable by
downgrading into a single streaming or a single snapshot model. 2) Saving computation memory.
When we need to generate the graph-level embedding for a long lifetime temporal graph, we only
need to load each snapshot embedding vector instead of loading every node embedding that appears in
the whole temporal graph. (The detail of how to generate a snapshot embedding through its relevant
node embeddings is discussed in Section 4.1.1, i.e., Multi-Scale Time Attention Mechanism.) Beyond
recent temporal graph representation learning methods (Pareja et al., 2020; Xu et al., 2020; Beladev
et al., 2020) that only focus on one time scale and ignore the whole lifetime evolution representation,
our method can learn the lifelong evolution pattern of a temporal graph on different time scales."
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.13966480446927373,"Figure 2: An example of a temporal graph described by the proposed streaming-snapshot model.
Each edge is labeled by two timestamps, i.e., (vi, vj, te, ts), te ∈{0, 1, 2, 3, 4}, and ts ∈{0, 1, 2}"
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.1452513966480447,"As for the data structure, we store each edge as (vi, vj, te) and each snapshot adjacency matrix as
A(ts) ∈R|V (ts)|×|V (ts)|, i.e., V (ts) ⊆V and |V (ts)| ̸= |V (ts+1)| is allowable. Although our method
is readily designed for evolving input features according to different timestamps, for the notation
clarity, we denote the node feature matrix X ∈Rn×m, such that the input node feature of temporal
graph G is already time-aware, and n = |V | and m denotes the dimension of features."
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.15083798882681565,"Problem Setup. With the streaming-snapshot modelled temporal graphs, our goal is to learn a
parameterized metric that could accurately classify seen temporal graphs and also be smoothly"
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.1564245810055866,Under review as a conference paper at ICLR 2022
STREAMING-SNAPSHOT MODEL AND PROBLEM SETUP,0.16201117318435754,"adapted to unseen temporal graphs. Based on above analysis, this problem can be solved by learning
a ""good"" graph representation learning function fθ in Euclidean metric. To further achieve this
""goodness"" only with less labelled data, we formalize fθ into a bi-level meta-learning paradigm (Finn
et al., 2017). Given the streaming-snapshot modelled temporal graphs and corresponding labels
eG = {(G0, y0), (G1, y1), . . . , (Gn, yn)}, we split eG into eGtrain for meta-training and eGtest for meta-
testing, where the testing set only has unseen graph labels from the training set. We shufﬂe the
training set eGtrain to sample graph metric learning tasks following a distribution Ti ∼P(T ), where
each graph metric learning task Ti is realized by a K-way N-shot temporal graph classiﬁcation task
based on the graph representation fθi(Gn). During each task Ti, we sample a support set eGtrain
support
and a query set eGtrain
query, such that the support set is used to train the graph representation function
fθi to accurately predict the graph labels of the query set. At the meta-testing stage, we transfer the
learned knowledge from each task (i.e., θi) to the meta-learner (i.e., Θ), then we update Θ a few times
by classifying unseen temporal graphs on support set eGtest
support, ﬁnally we report the classiﬁcation
accuracy of ﬁne-tuned Θ on query set eGtest
query. The concrete objective and loss function of each graph
metric learning task Ti, i.e., the ""goodness"", is mathematically expressed in Section 4."
METATAG FRAMEWORK,0.16759776536312848,"4
METATAG FRAMEWORK"
METATAG FRAMEWORK,0.17318435754189945,"We illustrate METATAG with the proposed prototypical temporal graph encoder and meta-learner.
First, prototypical temporal graph encoder captures temporal graph lifelong evolution representations
through the multi-scale time attention mechanism, which serves for learning the parameterized metric
(i.e., θi) in each graph metric learning task (i.e., Ti). Second, meta-learner Θ transfers the knowledge
θi learned from each task Ti for the fast adaption on unseen temporal graphs classiﬁcations."
METATAG FRAMEWORK,0.1787709497206704,"Figure 3: The multi-scale time attention mechanism of the prototypical temporal graph encoder for
encoding temporal graph representation vector z of input temporal graph G."
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.18435754189944134,"4.1
PROTOTYPICAL TEMPORAL GRAPH ENCODER"
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.18994413407821228,"As stated in Eq. 1, the metric D is pair-wise. To save storage and computation complexity, learning D
with labels involves the class representation concept (e.g., chunklet in (Bar-Hillel et al., 2005) and
prototype in (Snell et al., 2017)), such that a sample should be close to its class representation and far
from other class representations in metric D. In this paper, we follow (Snell et al., 2017) to name
class representation as prototype. In each graph metric learning task Ti, prototypical temporal graph
encoder encodes class-distinctive lifelong evolution patterns within two steps. First, multi-scale time
attention mechanism is responsible for learning a single temporal graph representation from different
time domains, i.e., edge timestamps and snapshot timestamps. Second, prototype generator generates
the prototype for the same class temporal graph representations, to ensure same class graphs share
the same prototypical pattern."
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.19553072625698323,Under review as a conference paper at ICLR 2022
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.2011173184357542,"Algorithm 1 Sample Time-Aware Adjacent Node Sequence N (te)
v
for Node v at Edge Timestamp te
Input: node v at te, temporal graph G"
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.20670391061452514,"1: for edge timestamp t < te do
2:
if edge (v′, v, t) exists then
▷connected edges before time te
3:
N (te)
v
appends X(v′, :)∥K(t, te)
▷concatenation of X(v′, :) ∈Rm and K(t, te) ∈Rd"
PROTOTYPICAL TEMPORAL GRAPH ENCODER,0.2122905027932961,"4:
end if
5: end for"
MULTI-SCALE TIME ATTENTION MECHANISM,0.21787709497206703,"4.1.1
MULTI-SCALE TIME ATTENTION MECHANISM"
MULTI-SCALE TIME ATTENTION MECHANISM,0.22346368715083798,"As shown in Figure 3, multi-scale time attention mechanism encodes the streaming-snapshot modeled
temporal graph G into the representation vector z through three components, i.e., node-level time
attention, intra-snapshot time attention, and inter-snapshot time attention."
MULTI-SCALE TIME ATTENTION MECHANISM,0.22905027932960895,"Node-Level Time Attention. Intuitively, this mechanism ﬁrst uses edge timestamp te to learn
streaming pattern of the input graph. We ﬁrst need to sample a time-aware adjacent node sequence
Nv for each node v in the temporal graph G (as shown in Alg. 1), then we apply the node-level time
attention mechanism to learn the time-aware embedding uv of node v from its previous connected
nodes. However, self-attention mechanism has become the key component for representing sequential
data (Vaswani et al., 2017), which itself could not deal with sequential information but rely on
positional encoding function to map discrete position indexes into differentiable functional domain.
Analogically, we need a time encoding function K for our node-level time attention mechanism,
which could map every observed time interval of node connections into a continuous differentiable
functional domain, i.e., K : [te −l, te] →Rd. The intuition of involving K is that, suppose node
v1 connects with node v2 at edge timestamp te −l, when we need to represent the node v2 at
edge timestamp te, we wish the time-aware node representation u(te)
v2
incorporates the temporal
relationship K(te −l, te). This time function K could reﬂect the temporal relationship between
u(te−l)
v1
and u(te)
v2 , and many previous work solve K with the kernel method (Zhou et al., 2013; Du
et al., 2016; Xu et al., 2019; 2020; Zhang et al., 2020). For example, in (Xu et al., 2019; 2020),"
MULTI-SCALE TIME ATTENTION MECHANISM,0.2346368715083799,"K(te −l, te) = Ψ(te −(te −l))
(2) and"
MULTI-SCALE TIME ATTENTION MECHANISM,0.24022346368715083,Ψ(l) = r
MULTI-SCALE TIME ATTENTION MECHANISM,0.24581005586592178,"1
d[cos ω1(l), cos ω2(l), . . . , cos ωd(l)]
(3)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.25139664804469275,"where l = te −(te −l) denotes the input time interval, and {ω1, . . . , ωd} are learnable parameters."
MULTI-SCALE TIME ATTENTION MECHANISM,0.2569832402234637,"After sampling the node time-aware adjacent node sequence N (te)
v
for node v, we next apply the
node-level time attention on N (te)
v
to learn u(te)
v
by setting node v as the query node to query and
aggregate attention weights from previously connected nodes in N (te)
v
. Similar with self-attention
mechanism (Vaswani et al., 2017), we need form queries Q, keys K and values V, then the time-aware
node representation u(te)
v
∈Rr can be computed as follows."
MULTI-SCALE TIME ATTENTION MECHANISM,0.26256983240223464,"u(te)
v
= Attention(Q, K, V) = softmax(QK⊤"
MULTI-SCALE TIME ATTENTION MECHANISM,0.2681564245810056,"√r )V ∈Rr
(4)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.2737430167597765,"where Q = [X(v, :)∥K(te, te)] · WQ, K = N · WK, and V = N · WV . N ∈R|N (te)
v
|×(m+d)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.27932960893854747,"is the matrix whose rows are [X(v′, :)∥K(t, te)] ∈R(m+d) from the sequence N (te)
v
, and
WQ, WK, WV ∈R(m+d)×r are three learnable weight matrices with r denoting the dimension of
the time-aware node presentation vector u(te)
v
."
MULTI-SCALE TIME ATTENTION MECHANISM,0.2849162011173184,"Intra-Snapshot Time Attention. After we learn the time-aware node embedding u(te)
v
that follows
the streaming pattern based at the edge timestamp te, we also want the node in the snapshot S(ts)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.2905027932960894,"follows the snapshot pattern of A(ts) w.r.t snapshot timestamp ts. Therefore, intra-snapshot time
attention is proposed to add constraints on node embeddings u(te)
v
in terms of ts timestamp by
reconstructing A(ts) via a graph autoencoder."
MULTI-SCALE TIME ATTENTION MECHANISM,0.29608938547486036,Under review as a conference paper at ICLR 2022
MULTI-SCALE TIME ATTENTION MECHANISM,0.3016759776536313,"First, we construct the snapshot feature matrix U(ts) ∈R|V (ts)|×r whose rows are time-aware node
embedding vectors. Note that, snapshot S(ts) may not have all nodes of the input temporal graph.
For example, for the timestamped edge (v1, v2, te, ts), U(ts)(v1, :) = u(te)
v1
and U(ts)(v2, :) = u(te)
v2 .
Interestingly, if there is also another edge (v1, v3, t′
e, ts), we will sample the most recent edge
timestamp. For example, if t′
e > te, then U(ts)(v1, :) = u(t′
e)
v1 . The reason we adopt the latest node
embedding is that, according to the sampling strategy shown in Alg.1, the latest node embedding will
encode early node embeddings."
MULTI-SCALE TIME ATTENTION MECHANISM,0.30726256983240224,"Then, with the adjacency matrix A(ts) and snapshot feature matrix U(ts), we add a reconstruction
loss to learn latent intra-snapshot representation matrix H(ts) via the graph autoencoder model (Kipf
& Welling, 2016). The snapshot reconstruction loss ℓof the snapshot S(ts) is deﬁned as follows."
MULTI-SCALE TIME ATTENTION MECHANISM,0.3128491620111732,"ℓ(A(ts), U(ts)) = ∥A(ts) −ˆA(ts)∥F
(5)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.31843575418994413,"where ˆA(ts) = GNNdec(H(ts))GNN⊤
dec(H(ts)) is the reconstructed adjacency matrix computed as
inner product of GNNdec(H(ts)) and its transpose, H(ts) = GNNenc(A(ts), U(ts)) ∈R|V (ts)|×q
denotes the intra-snapshot representation matrix, and ∥· ∥F denotes the Frobenius norm. GNNenc
and GNNdec are realized by GCN (Kipf & Welling, 2017) and Sigmoid function, respectively."
MULTI-SCALE TIME ATTENTION MECHANISM,0.3240223463687151,"Given the extracted intra-snapshot representation matrix H(ts), we apply a Readout function to get
the intra-snapshot representation vector h(ts) at each snapshot timestmap ts as follows."
MULTI-SCALE TIME ATTENTION MECHANISM,0.329608938547486,"h(ts) = Readout(H(ts)(v, :) | v ∈{1, . . . , |V (ts)|} ∈Rq)
(6)"
MULTI-SCALE TIME ATTENTION MECHANISM,0.33519553072625696,"where Readout is a permutation-invariant function and could be instanced by many graph pooling
layer models, such like Zhang et al. (2018); Ying et al. (2018)."
MULTI-SCALE TIME ATTENTION MECHANISM,0.3407821229050279,"Inter-Snapshot Time Attention. After we obtain the intra-snapshot representation vector h(ts) for
each snapshot timestamp ts individually, we are not sure which one or ones should represent the
temporal graph representation vector z to make it class-distinctive. To be speciﬁc, if a certain snapshot
S is shared by different classes of temporal graphs, then that snapshot is less representative and we
should decrease its weight during the snapshots aggregation process, to make different class temporal
graph representations different."
MULTI-SCALE TIME ATTENTION MECHANISM,0.3463687150837989,"Therefore, we design a inter-snapshot time attention mechanism on the extracted intra-snapshot
representation vectors h(ts) to obtain time attention weights for the ﬁnal temporal graph representation
vector z ∈Rf. To be speciﬁc, the inter-snapshot time attention is realized through an attention
pooling layer (Bahdanau et al., 2015) to get the attention weight W(ts) ∈Rf×q for each time ts.
Then, the inter-snapshot time attention is parameterized by the learned weight W(ts) as follows. z = Ts
X"
MULTI-SCALE TIME ATTENTION MECHANISM,0.35195530726256985,"ts=0
(W(ts)h(ts)) ∈Rf
(7)"
PROTOTYPE GENERATOR,0.3575418994413408,"4.1.2
PROTOTYPE GENERATOR"
PROTOTYPE GENERATOR,0.36312849162011174,"Through the proposed prototypical temporal graph encoder, we could embed a temporal graph G into
a representation z as shown in Figure 1. To make same class temporal graphs closer and different
class graphs farther apart in the metric D, we need to make same class graph representations closer to
their own class prototype and farther from other class prototypes in the metric D′ (i.e., Euclidean)."
PROTOTYPE GENERATOR,0.3687150837988827,"To this end, in each graph metric learning task Ti, we set the support set eGtrain
support and the query set
eGtrain
query. The prototype learned on eGtrain
support are used for predicting the class label of graphs in eGtrain
query.
The prototype pk of the class k is expressed as follows."
PROTOTYPE GENERATOR,0.3743016759776536,"pk = 1 Ck Ck
X"
PROTOTYPE GENERATOR,0.37988826815642457,"j
(zj) , Gj ∈eGtrain
support and yj = k
(8)"
PROTOTYPE GENERATOR,0.3854748603351955,"where Gj is the temporal graph with label yj, zj is the embedding of Gj extracted by the prototypical
temporal graph encoder, and Ck denotes the number of k class temporal graphs in eGtrain
support."
PROTOTYPE GENERATOR,0.39106145251396646,Under review as a conference paper at ICLR 2022
PROTOTYPE GENERATOR,0.39664804469273746,Algorithm 2 Meta-Training Process of METATAG
PROTOTYPE GENERATOR,0.4022346368715084,"Input: graph metric learning task distribution P(T ), step size hyperparameters α and β, loss
balancing hyperparameter γ
1: Randomly initialize Θ ▷Θ denotes all parameters in the prototypical temporal graph encoder
2: while not done do
3:
Sample task Ti ∼P(T ) with support set eGtrain
support and query set eGtrain
query
4:
for support set of each task Ti do
5:
Compute zj = fΘ(Gj)
▷Gj ∈eGtrain
support
6:
Construct pk for each class k in eGtrain
support according to Eq. 8.
7:
Evaluate snapshot reconstruction loss ∇ΘℓTi(fΘ) and classiﬁcation loss ∇ΘLTi(fΘ)
8:
Compute parameter θi ←Θ −α(∇ΘℓTi(fΘ) + γ∇ΘLTi(fΘ))
9:
end for
10:
Update Θ ←Θ −β∇Θ
P"
PROTOTYPE GENERATOR,0.40782122905027934,"Ti(ℓTi(fθi) + γLTi(fθi))
▷On query set eGtrain
query
11: end while"
PROTOTYPE GENERATOR,0.4134078212290503,"To help the the class prototype distinctive to each other, we design the temporal graph classiﬁcation
loss L in each graph metric learning task Ti to tune θi. L = − Ck
X"
PROTOTYPE GENERATOR,0.41899441340782123,"j
log
exp(−dist(zj, pk))
P
k′ exp(−dist(zj, pk′)),"
PROTOTYPE GENERATOR,0.4245810055865922,"Gj ∈eGtrain
query and yj = k (9)"
PROTOTYPE GENERATOR,0.4301675977653631,"where pk denotes the k class prototype learned from eGtrain
support, k′ denote the class other than k, dist(·)
denotes Euclidean distance between two vectors, zj is the representation vector of Gj and Ck denotes
the number of k class temporal graphs in the set eGtrain
query."
META-LEARNER,0.43575418994413406,"4.2
META-LEARNER"
META-LEARNER,0.441340782122905,"We have introduced the whole learning procedure and two loss functions (i.e., Eq. 5 and Eq. 9)
for extracting knowledge θi from a single task Ti. Next, we need to break though the knowledge
transfer and adaption cross tasks given only few-shot examples. Here, we introduce a meta-learner to
transfer the learned knowledge θi and tailor the globally shared knowledge Θ, the theory behind is
that transferring shareable knowledge could obtain the fast convergence on unseen tasks (Chauhan
et al., 2020; Ma et al., 2020)."
META-LEARNER,0.44692737430167595,"We formalize the meta-training process of METATAG in a bi-level paradigm (Finn et al., 2017), which
is able to ﬁnd meta-learner Θ that could be fast converged in each graph metric learning task. As
shown in Algorithm 2, we ﬁrst randomly initialize Θ in Step 1. Then, in each graph metric learning
task Ti ∼P(T ), we obtain the temporal graph representation vector in Step 5 and build the class
prototype for each class of the support set in Step 6. In Step 8, we tune Θ to get θi for current task Ti.
In Step 10, we aggregate the loss from each task Ti and ﬁne tune Θ to end the meta-training process.
After that, we can use the ﬁne-tuned Θ as the initialized parameter in meta-testing stage for unseen
graph metric learning tasks, aiming to the fast adaptation via only a few labeled samples."
META-LEARNER,0.45251396648044695,"The meta-testing phase of METATAG is very similar to Algorithm 2. After changing eGtrain
support into
eGtest
support and eGtrain
query into eGtest
query, the only difference is that Step 10 directly reports the accuracy
based on θi instead of getting new Θ."
EXPERIMENTS,0.4581005586592179,"5
EXPERIMENTS"
EXPERIMENTS,0.46368715083798884,"In this section, we test our METATAG in terms of temporal graph classiﬁcations comparing with
state-of-the-art graph kernel and graph metric learning baseline algorithms. More experimental
details about the implementation and the other extensive experimental results like convergence speed,
parameter sensitivity, and ablation study can be found in Appendix."
EXPERIMENTS,0.4692737430167598,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.4748603351955307,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.48044692737430167,"Datasets. Our experiments conclude 12 temporal graph datasets from the biological domain (Fu
& He, 2021), and 6 temporal graph datasets from the social network domain (Morris et al., 2020).
Each biological graph is a dynamic protein-protein interaction network, which describes the proteins
interact of metabolic cycles of different yeast cells, where each node stands for a protein, and
timestamped edge stands for the interact of a pair of proteins. Each social network is a human-contact
relation graph online and ofﬂine, where the edges between individuals stand for the online or ofﬂine
contacts. The statistics of all network data are summarized in Table 1 and Table 2."
EXPERIMENT SETUP,0.4860335195530726,Table 1: Statistics of Biological Temporal Graph Data
EXPERIMENT SETUP,0.49162011173184356,"Graph
#Classes
#Graphs
Total Nodes
Total Edges
Timestamps
Graph
#Classes
#Graphs
Total Nodes
Total Edges
Timestamps
Uetz
1
11
922
2,159
36
Ito
1
11
2,856
8,638
36
Ho
1
11
1,548
42,220
36
Gavin
1
11
2,541
140,040
36
Krogan-LCMS
1
11
2,211
85,133
36
Krogan-MALDI
1
11
2,099
78,297
36
Yu
1
11
1,163
3,602
36
Breitkreutz
1
11
869
39,250
36
Babu
1
11
5,003
111,466
36
Lambert
1
11
697
6,654
36
Tarassov
1
11
1,053
4,826
36
Hazbun
1
11
143
1,959
36"
EXPERIMENT SETUP,0.4972067039106145,Table 2: Statistics of Social Temporal Graph Data
EXPERIMENT SETUP,0.5027932960893855,"Graph (Online)
#Classes
#Graphs
Total Nodes
Total Edges
Timestamps
Graph (Ofﬂine)
#Classes
#Graphs
Total Nodes
Total Edges
Timestamps
Facebook
2
995
95,224
267,673
104
Infectious
2
200
10,000
91,944
48
Tumblr
2
373
19,811
74,520
89
HighSchool
2
180
9,418
98,066
203
DBLP
2
755
39,917
241,674
46
MIT
2
97
1,940
142,508
5,576"
EXPERIMENT SETUP,0.5083798882681564,"Baselines. The selection of baseline algorithms includes three factors, i.e., graph kernel or graph
metric learning, few-shot learning or not few-shot learning, and static or dynamic. Graph kernel
methods include: Vertex histogram kernel (Nikolentzos et al., 2019), Shortest Path kernel (Borgwardt
& Kriegel, 2005), Neighborhood Hash graph kernel (Hido & Kashima, 2009), Weisfeiler-Lehman
Optimal Assignment kernel (Kriege et al., 2016), and Pyramid Match kernel (Nikolentzos et al.,
2017). Graph metric learning or graph representation learning algorithms include: GL2Vec (Chen &
Koga, 2019), NetLSD (Tsitsulin et al., 2018), tdGraphEmbed (Beladev et al., 2020), TGAT (Xu et al.,
2020), and CAW (Wang et al., 2021). GL2Vec and NetLSD are static algorithms, tdGraphEmbed is a
dynamic algorithm that could take a temporal graph as input and output graph embeddings of each
snapshot, and TGAT and CAW are dynamic graph representation learning algorithms but focus on
the node-level. To enable graph metric learning methods the few-shot learning capability, we also
include ProtoNet (Snell et al., 2017) and its special case k-NN method."
EXPERIMENT SETUP,0.5139664804469274,Table 3: Temporal Graph Classiﬁcation Accuracy on Biological Temporal Graphs
EXPERIMENT SETUP,0.5195530726256983,"Methods
3 way - 5 shot
3 way - 3 shot
3 way - 2 shot
3 way - 1 shot"
EXPERIMENT SETUP,0.5251396648044693,"Graph
Kernel"
EXPERIMENT SETUP,0.5307262569832403,"Weisfeiler-Lehman Opt
0.5025 ± 0.3531
0.4625 ± 0.3118
0.4350 ± 0.2420
0.4250 ± 0.2251
Vertex Histogram
0.3150 ± 0.2466
0.2700 ± 0.1881
0.1375 ± 0.1314
0.3125 ± 0.2415
Neighborhood Hash
0.4375 ± 0.4058
0.4400 ± 0.3697
0.2850 ± 0.1815
0.4000 ± 0.3175
Pyramid Match
0.2500 ± 0.1971
0.2525 ± 0.1337
0.2325 ± 0.1569
0.2950 ± 0.2174
Shortest Path
0.2025 ± 0.1477
0.2175 ± 0.1314
0.1875 ± 0.1325
0.1900 ± 0.1329"
EXPERIMENT SETUP,0.5363128491620112,"Graph
Metric
Learning"
EXPERIMENT SETUP,0.5418994413407822,"GL2Vec + KNN
0.1400 ± 0.0616
0.1925 ± 0.0754
0.1175 ± 0.0689
0.1150 ± 0.0591
NetLSD + KNN
0.3600 ± 0.2585
0.3650 ± 0.2747
0.2000 ± 0.0901
0.2625 ± 0.1519
TGAT + KNN
0.2100 ± 0.0817
0.1325 ± 0.2217
0.1650 ± 0.0387
0.0750 ± 0.0208
tdGraphEmbed + KNN
0.3200 ± 0.1272
0.2275 ± 0.1459
0.1750 ± 0.0580
0.1875 ± 0.0150
GL2Vec + ProtoNet
0.6083 ± 0.0099
0.6541 ± 0.0159
0.6542 ± 0.1370
0.5583 ± 0.1578
NetLSD + ProtoNet
0.6916 ± 0.1396
0.7145 ± 0.1396
0.6937 ± 0.1674
0.6667 ± 0.1372
TGAT + ProtoNet
0.2417 ± 0.0500
0.3083 ± 0.0739
0.2917 ± 0.1167
0.2417 ± 0.0319
CAW + ProtoNet
0.1496 ± 0.0104
0.2113 ± 0.0110
0.2404 ± 0.0117
0.2842 ± 0.0044
tdGraphEmbed + ProtoNet
0.6562 ± 0.1882
0.6791 ± 0.1141
0.6271 ± 0.1159
0.4229 ± 0.0463
MetaTag (Ours)
0.7292 ± 0.0682
0.7917 ± 0.1278
0.7062 ± 0.0762
0.6833 ± 0.0589"
TEMPORAL GRAPH CLASSIFICATION,0.547486033519553,"5.2
TEMPORAL GRAPH CLASSIFICATION"
TEMPORAL GRAPH CLASSIFICATION,0.553072625698324,"First, in the biological dataset, given the 12 classes we split 8 classes into the meta-training set
eGtrain and 4 classes into the meta-testing test eGtest. Note that eGtrain and eGtest do not share any
class label. In eGtrain, we sample K-way N-shot graph metric learning tasks Ti ∼P(T ), and
each task has a support set eGtrain
support and a query set eGtrain
query. Then, METATAG is trained on eGtrain"
TEMPORAL GRAPH CLASSIFICATION,0.5586592178770949,Under review as a conference paper at ICLR 2022
TEMPORAL GRAPH CLASSIFICATION,0.5642458100558659,"based on Algorithm 2 and ﬁne tune a few times on eGtrain
support and report the accuracy on eGtrain
query. We
shufﬂe eGtrain and eGtest 4 times for cross-validation and report the average classiﬁcation accuracy
in Table 3, where our METATAG outperforms all the baseline algorithms. For example, in the
3-way 5-shot setting, our METATAG achieve 72.92% temporal graph classiﬁcation accuracy, which
is 5.44% higher than the second place. An intuitive explanation is that different class yeast cells
(i.e., temporal graphs) has class-distinctive metabolic patterns (i.e., temporal patterns), capturing that
pattern comprehensively is helpful in identifying class labels. Also, we observe other interesting
patterns. First, graph kernel methods and KNN-based graph metric learning methods do not perform
well and bear the larger standard deviation. A possible answer is that they do not have few-shot
learning capability and could not transfer knowledge from seen cases to unseen cases. ProtoNet-based
graph metric learning and our method enjoy the data augmentation property from the few-shot
learning manner, thus they have better performance and smaller deviations. Second, our experiments
shows that increasing the number of shots during the meta-training is not always the good choice for
improving the performance of meta-testing 1, because intra-class variances may be ampliﬁed (Cao
et al., 2020). For the page limit, we place the experimental results of temporal graph classiﬁcation on
social network data in Appendix."
RELATED WORK,0.5698324022346368,"6
RELATED WORK"
RELATED WORK,0.5754189944134078,"Graph Metric Learning. Learning a good metric in the input feature can be transferred to learn
proper graph representations in Euclidean space, then graph embedding based graph metric learning
methods are proposed (Shaw et al., 2011; Bai et al., 2019; Li et al., 2019). Facing the label scarcity
problem, many generic metric learning methods consider the few-shot learning or meta-learning
strategy to adapt metrics across different tasks with only a few labeled sample in each task (Snell
et al., 2017; Oreshkin et al., 2018; Allen et al., 2019). Inspired by that, some graph metric learning
methods involve the few-shot learning manner, where the majority of these algorithms focus on
learning the metric over nodes across different graphs (Yao et al., 2020; Suo et al., 2020; Huang
& Zitnik, 2020; Lan et al., 2020; Wang et al., 2020; Ding et al., 2020). Only a few graph metric
learning methods learn the metric over the whole graphs to distinguish distance between graphs (Ma
et al., 2020; Chauhan et al., 2020). Currently, graph metric few-shot learning methods ignore to
consider the dynamics of graphs into the metric learning process. We are the ﬁrst effort to involve
the dynamics and temporal dependencies of input graphs into the learned metric. Graph Kernel.
Given a distance metric D, it should maintain four properties: non-negativity (i.e., D(x, y) ≥0),
coincidence (i.e., D(x, y) = 0 iff x = y), symmetry (i.e., D(x, y) = D(y, x)), and subadditivity
(i.e., D(x, y) + D(y, z) ≥D(x, z) ) (Wang & Sun, 2015). While in graph kernel research, only the
symmetry and non-negativity need to be hold for a kernel function, i.e., the symmetric graph kernel
function K should be a positive semi-deﬁnite function (Vishwanathan et al., 2010). To measure the
similarity among graphs, one category graph kernel methods explicitly deﬁne the kernel function
from the graph topological view, such as Random Walk graph kernel (Vishwanathan et al., 2010)
and Weisfeiler-Lehman graph kernel (Shervashidze et al., 2011). To handle the evolving graph
scenario, some methods map dynamic graphs into constant representations and then apply static
graph kernel functions for dynamic node classiﬁcation (Yao & Holder, 2014) and temporal graph
classiﬁcation (Oettershagen et al., 2020); On the other hand, some graph kernel methods learn the
kernel function instead of hand-crafted designing it (Yanardag & Vishwanathan, 2015; Zhao & Wang,
2019). For example, in (Yanardag & Vishwanathan, 2015), the kernel is determined by learning the
latent representation of substructures of input graphs."
CONCLUSION,0.5810055865921788,"7
CONCLUSION"
CONCLUSION,0.5865921787709497,"In this paper, we ﬁrst propose the streaming-snapshot model to describe a temporal graph, and
then we propose the prototypical temporal graph encoder to capture temporal graph representation
vectors. Last but not the least, we entitle the prototypical temporal graph encoder with a meta-learner
into an end-to-end model, named METATAG, to transfer knowledge among different tasks for the
fast adaption to unseen cases. We execute extensive experiments to show the effectiveness of our
METATAG with different category state-of-the-art baseline algorithms."
CONCLUSION,0.5921787709497207,1The number of shots in the query set during meta-testing is 2.
CONCLUSION,0.5977653631284916,Under review as a conference paper at ICLR 2022
REFERENCES,0.6033519553072626,REFERENCES
REFERENCES,0.6089385474860335,"Charu C. Aggarwal and Karthik Subbian. Evolutionary network analysis: A survey. ACM Comput. Surv., 2014."
REFERENCES,0.6145251396648045,"Kelsey R. Allen, Evan Shelhamer, Hanul Shin, and Joshua B. Tenenbaum. Inﬁnite mixture prototypes for
few-shot learning. In ICML, 2019."
REFERENCES,0.6201117318435754,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. In ICLR, 2015."
REFERENCES,0.6256983240223464,"Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. Simgnn: A neural network
approach to fast graph similarity computation. In WSDM, 2019."
REFERENCES,0.6312849162011173,"Aharon Bar-Hillel, Tomer Hertz, Noam Shental, and Daphna Weinshall. Learning a mahalanobis metric from
equivalence constraints. J. Mach. Learn. Res., 2005."
REFERENCES,0.6368715083798883,"Moran Beladev, Lior Rokach, Gilad Katz, Ido Guy, and Kira Radinsky. tdgraphembed: Temporal dynamic
graph-level embedding. In CIKM, 2020."
REFERENCES,0.6424581005586593,"Karsten M. Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In ICDM, 2005."
REFERENCES,0.6480446927374302,"Tianshi Cao, Marc T. Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot learning.
In ICLR, 2020."
REFERENCES,0.6536312849162011,"Jatin Chauhan, Deepak Nathani, and Manohar Kaul. Few-shot learning on graphs via super-classes based on
graph spectral measures. In ICLR, 2020."
REFERENCES,0.659217877094972,"Hong Chen and Hisashi Koga. Gl2vec: Graph embedding enriched by line graphs with edge features. In ICONIP,
2019."
REFERENCES,0.664804469273743,"Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In
ICML, 2016."
REFERENCES,0.6703910614525139,"Tyler Derr, Yao Ma, Wenqi Fan, Xiaorui Liu, Charu C. Aggarwal, and Jiliang Tang. Epidemic graph convolutional
network. In WSDM, 2020."
REFERENCES,0.6759776536312849,"Kaize Ding, Jianling Wang, Jundong Li, Kai Shu, Chenghao Liu, and Huan Liu. Graph prototypical networks
for few-shot learning on attributed networks. In CIKM, 2020."
REFERENCES,0.6815642458100558,"Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. Recurrent
marked temporal point processes: Embedding event history to vector. In KDD, 2016."
REFERENCES,0.6871508379888268,"David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel,
Alán Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints.
In NeurIPS, 2015."
REFERENCES,0.6927374301675978,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In ICML, 2017."
REFERENCES,0.6983240223463687,"Dongqi Fu and Jingrui He. DPPIN: A biological repository of dynamic protein-protein interaction network data.
CoRR, 2021."
REFERENCES,0.7039106145251397,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In ICML, 2017."
REFERENCES,0.7094972067039106,"Amir Globerson and Sam T. Roweis. Metric learning by collapsing classes. In NeurIPS, 2005."
REFERENCES,0.7150837988826816,"Jacob Goldberger, Sam T. Roweis, Geoffrey E. Hinton, and Ruslan Salakhutdinov. Neighbourhood components
analysis. In NeurIPS, 2004."
REFERENCES,0.7206703910614525,"Shohei Hido and Hisashi Kashima. A linear-time graph kernel. In ICDM, pp. 179–188, 2009."
REFERENCES,0.7262569832402235,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. In ICLR, 2020a."
REFERENCES,0.7318435754189944,"Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. GPT-GNN: generative pre-training
of graph neural networks. In KDD, 2020b."
REFERENCES,0.7374301675977654,"Kexin Huang and Marinka Zitnik. Graph meta learning via local subgraphs. In NeurIPS, 2020."
REFERENCES,0.7430167597765364,Under review as a conference paper at ICLR 2022
REFERENCES,0.7486033519553073,"Lorenzo Isella, Juliette Stehlé, Alain Barrat, Ciro Cattuto, Jean-François Pinton, and Wouter Van den Broeck.
What’s in a crowd? analysis of face-to-face behavioral networks. Journal of theoretical biology, 2011."
REFERENCES,0.7541899441340782,"Thomas N. Kipf and Max Welling. Variational graph auto-encoders. CoRR, 2016."
REFERENCES,0.7597765363128491,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR,
2017."
REFERENCES,0.7653631284916201,"Nils M. Kriege, Pierre-Louis Giscard, and Richard C. Wilson. On valid optimal assignment kernels and
applications to graph classiﬁcation. In NeurIPS, 2016."
REFERENCES,0.770949720670391,"Lin Lan, Pinghui Wang, Xuefeng Du, Kaikai Song, Jing Tao, and Xiaohong Guan. Node classiﬁcation on graphs
with few-shot novel labels via meta transformed network embedding. In NeurIPS, 2020."
REFERENCES,0.776536312849162,"Jure Leskovec, Jon M. Kleinberg, and Christos Faloutsos. Graphs over time: densiﬁcation laws, shrinking
diameters and possible explanations. In KDD, 2005."
REFERENCES,0.7821229050279329,"Jure Leskovec, Lars Backstrom, Ravi Kumar, and Andrew Tomkins. Microscopic evolution of social networks.
In KDD, 2008."
REFERENCES,0.7877094972067039,"Mengtian Li, Yu-Xiong Wang, and Deva Ramanan. Towards streaming perception. In ECCV, 2020."
REFERENCES,0.7932960893854749,"Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks for
learning the similarity of graph structured objects. In ICML, 2019."
REFERENCES,0.7988826815642458,"Ning Ma, Jiajun Bu, Jieyu Yang, Zhen Zhang, Chengwei Yao, Zhi Yu, Sheng Zhou, and Xifeng Yan. Adaptive-
step graph meta-learner for few-shot graph classiﬁcation. In CIKM, 2020."
REFERENCES,0.8044692737430168,"Sofus A. Macskassy. Using graph-based metrics with empirical risk minimization to speed up active learning on
networked data. In KDD, 2009."
REFERENCES,0.8100558659217877,"Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann.
Tudataset: A collection of benchmark datasets for learning with graphs. CoRR, 2020."
REFERENCES,0.8156424581005587,"Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node embeddings for graph
similarity. In AAAI, 2017."
REFERENCES,0.8212290502793296,"Giannis Nikolentzos, Giannis Siglidis, and Michalis Vazirgiannis. Graph kernels: A survey. CoRR, 2019."
REFERENCES,0.8268156424581006,"Lutz Oettershagen, Nils M. Kriege, Christopher Morris, and Petra Mutzel. Temporal graph kernels for classifying
dissemination processes. In SDM, 2020."
REFERENCES,0.8324022346368715,"Boris N. Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. TADAM: task dependent adaptive metric for
improved few-shot learning. In NeurIPS, 2018."
REFERENCES,0.8379888268156425,"Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler,
Tao B. Schardl, and Charles E. Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic
graphs. In AAAI, 2020."
REFERENCES,0.8435754189944135,"Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang.
GCC: graph contrastive coding for graph neural network pre-training. In KDD, 2020."
REFERENCES,0.8491620111731844,"Ruslan Salakhutdinov and Geoffrey E. Hinton. Learning a nonlinear embedding by preserving class neighbour-
hood structure. In AISTATS, 2007."
REFERENCES,0.8547486033519553,"Bernhard Schölkopf, Koji Tsuda, and Jean-Philippe Vert. Kernel Methods in Computational Biology. MIT press,
2004."
REFERENCES,0.8603351955307262,"Blake Shaw, Bert Huang, and Tony Jebara. Learning a distance metric from a network. In NeurIPS, 2011."
REFERENCES,0.8659217877094972,"Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt.
Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 2011."
REFERENCES,0.8715083798882681,"Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In NeurIPS,
2017."
REFERENCES,0.8770949720670391,"Qiuling Suo, Jingyuan Chou, Weida Zhong, and Aidong Zhang. Tadanet: Task-adaptive network for graph-
enriched meta-learning. In KDD, 2020."
REFERENCES,0.88268156424581,"Anton Tsitsulin, Davide Mottin, Panagiotis Karras, Alexander M. Bronstein, and Emmanuel Müller. Netlsd:
Hearing the shape of a graph. In KDD, 2018."
REFERENCES,0.888268156424581,Under review as a conference paper at ICLR 2022
REFERENCES,0.8938547486033519,"Benjamin P. Tu, Andrzej Kudlicki, Maga Rowicka, and Steven L. McKnight. Logic of the yeast metabolic cycle:
Temporal compartmentalization of cellular processes. Science, 2005."
REFERENCES,0.8994413407821229,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017."
REFERENCES,0.9050279329608939,"Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for
one shot learning. In NeurIPS, 2016."
REFERENCES,0.9106145251396648,"S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. Graph kernels. J. Mach.
Learn. Res., 2010."
REFERENCES,0.9162011173184358,"Fei Wang and Jimeng Sun. Survey on distance metric learning and dimensionality reduction in data mining.
Data Min. Knowl. Discov., 2015."
REFERENCES,0.9217877094972067,"Ning Wang, Minnan Luo, Kaize Ding, Lingling Zhang, Jundong Li, and Qinghua Zheng. Graph few-shot
learning with attribute matching. In CIKM, 2020."
REFERENCES,0.9273743016759777,"Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in
temporal networks via causal anonymous walks. In ICLR, 2021."
REFERENCES,0.9329608938547486,"Da Xu, Chuanwei Ruan, Evren Körpeoglu, Sushant Kumar, and Kannan Achan. Self-attention with functional
time representation learning. In NeurIPS, 2019."
REFERENCES,0.9385474860335196,"Da Xu, Chuanwei Ruan, Evren Körpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation
learning on temporal graphs. In ICLR, 2020."
REFERENCES,0.9441340782122905,"Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In KDD, 2015."
REFERENCES,0.9497206703910615,"Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, Junzhou Huang, Nitesh V. Chawla, and
Zhenhui Li. Graph few-shot learning via knowledge transfer. In AAAI, 2020."
REFERENCES,0.9553072625698324,"Yibo Yao and Lawrence B. Holder. Scalable svm-based classiﬁcation in dynamic graphs. In ICDM, 2014."
REFERENCES,0.9608938547486033,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical
graph representation learning with differentiable pooling. In NeurIPS, 2018."
REFERENCES,0.9664804469273743,"Tomoki Yoshida, Ichiro Takeuchi, and Masayuki Karasuyama. Learning interpretable metric between graphs:
Convex formulation and computation with graph mining. In KDD, 2019."
REFERENCES,0.9720670391061452,"Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for
graph classiﬁcation. In AAAI, 2018."
REFERENCES,0.9776536312849162,"Qiang Zhang, Aldo Lipani, Ömer Kirnap, and Emine Yilmaz. Self-attentive hawkes process. In ICML, 2020."
REFERENCES,0.9832402234636871,"Qi Zhao and Yusu Wang.
Learning metrics for persistence-based summaries and applications for graph
classiﬁcation. In NeurIPS, 2019."
REFERENCES,0.9888268156424581,"Ke Zhou, Hongyuan Zha, and Le Song. Learning triggering kernels for multi-dimensional hawkes processes. In
ICML, 2013."
REFERENCES,0.994413407821229,"Marinka Zitnik, Rok Sosiˇc, and Jure Leskovec. Prioritizing network communities. Nature communications,
2018."
