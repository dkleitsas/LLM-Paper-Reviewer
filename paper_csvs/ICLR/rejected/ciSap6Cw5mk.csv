Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025188916876574307,"Federated Learning is a distributed learning paradigm which seeks to preserve the
privacy of each participating node‚Äôs data. However, federated learning is vulner-
able to attacks, speciÔ¨Åcally to our interest, model integrity attacks. In this pa-
per, we propose a novel method for malicious node detection called MANDERA.
By transferring the original message matrix into a ranking matrix whose column
shows the relative rankings of all local nodes along different parameter dimen-
sions, our approach seeks to distinguish the malicious nodes from the benign ones
with high efÔ¨Åciency based on key characteristics of the rank domain. We have
proved, under mild conditions, that MANDERA is guaranteed to detect all mali-
cious nodes under typical Byzantine attacks with no prior knowledge or history
about the participating nodes. The effectiveness of the proposed approach is fur-
ther conÔ¨Årmed by experiments on three classic datasets, CIFAR-10, FASHION-
MNIST and MNIST. Compared to the state-of-art methods in the literature for
defending Byzantine attacks, MANDERA is unique in its way to identify the ma-
licious nodes by ranking and its robustness to effectively defense a wide range of
attacks."
INTRODUCTION,0.005037783375314861,"1
INTRODUCTION"
INTRODUCTION,0.007556675062972292,"Federated learning (FL) has observed a steady rise in use across a plethora of applications. FL
departs from conventional centralized learning by allowing multiple participating nodes to learn
on a local collection of training data, before each respective node‚Äôs updates are sent to a global
coordinator for aggregation. The global model collectively learns from each of these individual
nodes before relaying the updated global update back to the participating nodes. With an aggregation
of multiple nodes, the resulting model observes greater performance than if each node was to learn
on their local subset only. FL presents two key advantages, increased privacy for the contributing
node as local data is not communicated to the global coordinator, and a reduction in computation by
the global node as the computation is ofÔ¨Çoaded to contributing nodes."
INTRODUCTION,0.010075566750629723,"However, the presence of malicious actors in the collaborative process may seek to poison the per-
formance of the global model, to reduce the output performance of the model (Chen et al., 2017;
Fang et al., 2020; Tolpegin et al., 2020b), or to embed hidden back-doors within the model (Bag-
dasaryan et al., 2020). Byzantine attack aims to devastate the performance of the global model by
manipulating the gradient values of malicious nodes in a certain fashion. As these attacks emerged,
researchers seek to defend FL from the negative impacts of these attacks."
INTRODUCTION,0.012594458438287154,"In the literature, there are two typical defense strategies: malicious node detection and robust learn-
ing. Malicious node detection defenses by detecting malicious nodes and removing them from the
aggregation (Blanchard et al., 2017; Guerraoui et al., 2018; Li et al., 2020; So et al., 2021). Robust
learning (Blanchard et al., 2017; Yin et al., 2018; Guerraoui et al., 2018; Fang et al., 2020; Cao et al.,
2020), however, withstands a proportion of malicious nodes and defenses by reducing the negative
impacts of the malicious nodes via various robust learning methods (Wu et al., 2020b; Xie et al.,
2019; 2020; Cao et al., 2021)."
INTRODUCTION,0.015113350125944584,"In this paper, we focus on defensing Byzantine attacks via malicious node detection. In the literature,
there have been a collection of efforts along this research line. Blanchard et al. (2017) propose a
defense referred to as Krum that treats local nodes whose update vector is too far away from the
aggregated barycenter as malicious nodes and precludes them from the downstream aggregation.
Guerraoui et al. (2018) propose Bulyan, a process that performs aggregation on subsets of node"
INTRODUCTION,0.017632241813602016,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020151133501259445,"updates (by iteratively leaving each node out) to Ô¨Ånd a set of nodes with the most aligned updates
given an aggregation rule. Xie et al. (2019) compute a Stochastic Descendant Score (SDS) based
on the estimated descendant of the loss function, and the magnitude of the update submitted to the
global node, and only include a predeÔ¨Åned number of nodes with the highest SDS in the aggregation.
On the other hand, Chen et al. (2021) propose a zero-knowledge approach to detect and remove
malicious nodes by solving a weighted clustering problem. The resulting clusters update the model
individually and accuracy against a validation set are checked. All nodes in a cluster with signiÔ¨Åcant
negative accuracy impact are rejected and removed from the aggregation step."
INTRODUCTION,0.022670025188916875,"Although the aforementioned methods try to detect malicious nodes in different ways, they all share
a common nature: the detection is based on the gradient updates directly. However, it is usually
the case that different dimensions of the gradients remain quite different in the range of values and
follow very different distributions. This phenomena makes it very challenging to precisely detect
malicious nodes directly based on the node updates, as a few dimensions often dominate the Ô¨Ånal
result. Although the weighted clustering method proposed by Chen et al. (2021) could avoid this
problem partially by re-weighting different update dimensions, it is often not trivial to determine the
weights in a principled way."
INTRODUCTION,0.02518891687657431,"In this paper, we propose to resolve this critical problem from a novel perspective. Instead of work-
ing on the node updates directly, we propose to extract information about malicious nodes indirectly
by transforming the node updates from numeric gradient values to the rank domain. Compared to the
original numeric gradient values, whose distribution is difÔ¨Åcult to model, the ranks are much easier
to handle both theoretically and practically. Moreover, as ranks are scale-free, we no longer need to
worry about the scale difference across different dimensions. We proved under mild conditions that
the Ô¨Årst two moments of the transformed rank vectors carry key information to detect the malicious
nodes under a wide range of Byzantine attacks. Based on these theoretical results, a highly efÔ¨Åcient
method called MANDERA is proposed to separate the malicious nodes from the benign ones by
clustering all local nodes into two groups based on the moments of their rank vectors. With the
assumption that malicious nodes are the minority in the node pool, we can simply treat all nodes in
the smaller cluster as malicious nodes and remove them from the aggregation."
INTRODUCTION,0.027707808564231738,"Global 
Model"
INTRODUCTION,0.030226700251889168,MANDERA
INTRODUCTION,0.0327455919395466,"Reject
Malicious
Update"
INTRODUCTION,0.03526448362720403,"0.03, 0.12, 0.06, 0.2, 0.9  
1, 3, 2, 4, 5
Rank Node Parameters ùúΩùüè"
INTRODUCTION,0.037783375314861464,"ùúΩùüê
0.72, 0.90, 0.69, 0.7, 0.1  
4, 5, 2, 3, 1"
INTRODUCTION,0.04030226700251889,"0.48, 0.42, 0.43, 0.5, 0.8  
3, 1, 2, 4, 5  
ùúΩùüë"
INTRODUCTION,0.042821158690176324,"0.18, 0.2, 0.16, 0.3, 0.00  
2, 3, 1, 4, 1
ùúΩùíë
Mean SD"
INTRODUCTION,0.04534005037783375,"Find and reject nodes 
in malicious cluster"
INTRODUCTION,0.04785894206549118,"Aggregate and 
Update Global Model 
with benign cluster"
INTRODUCTION,0.05037783375314862,"Compute 
Rank Mean 
& SD"
INTRODUCTION,0.05289672544080604,"then,
Cluster"
INTRODUCTION,0.055415617128463476,Figure 1: An Overview of MANDERA
INTRODUCTION,0.05793450881612091,"The contributions of this work are as follows. (1) We propose the Ô¨Årst algorithm leveraging the
rank domain of model updates to detect malicious nodes (Figure 1). (2) We provide theoretical
guarantee for the detection of malicious nodes based on the rank domain under Byzantine attacks.
(3) Our method does not assume knowledge on the number of malicious nodes, which is required
in the learning process of prior methods. (4) We experimentally demonstrate the effectiveness and
robustness of our defense on Byzantine attacks, including Gaussian attack, Sign Flipping attack and
Zero Gradient attack, in addition to a more subtle Label Flipping data poisoning attack. (5) An
experimental comparison between MANDERA and a collection of robust aggregation techniques
are provided. The computation times are also compared, demonstrating gains of MANDERA by
operating in the rank domain."
DEFENSE FORMALIZATION,0.060453400503778336,"2
DEFENSE FORMALIZATION"
NOTATIONS,0.06297229219143577,"2.1
NOTATIONS
Suppose there are n local nodes in the federated learning framework, where n1 nodes are benign
nodes whose indices are denoted by Ib and the other n0 = n ‚àín1 nodes are malicious nodes
whose indices are denoted by Im. The training model is denoted by f(Œ∏, D), where Œ∏ ‚ààRp√ó1
is a p-dimensional parameter vector and D is a data matrix. Denote the message matrix received
from all local nodes by the central server as M ‚ààRn√óp, where Mi,: denotes the message received
from node i. For a benign node i, let Di be the data matrix on it with Ni as the sample size, we
have Mi,: = ‚àÇf(Œ∏,Di)"
NOTATIONS,0.0654911838790932,"‚àÇŒ∏
. A malicious node j ‚ààIm, however, tends to attack the learning system by"
NOTATIONS,0.06801007556675064,Under review as a conference paper at ICLR 2022
NOTATIONS,0.07052896725440806,"manipulating Mj,: in some way. Hereinafter, we denote N ‚àó= min({Ni}i‚ààIb) to be the minimal
sample size of the benign nodes."
NOTATIONS,0.07304785894206549,"Given a vector of real numbers a ‚ààRp√ó1, deÔ¨Åne its ranking vector as b = Rank(a) ‚àà
perm{1, ¬∑ ¬∑ ¬∑ , p}, where the ranking operator Rank maps the vector a to its permutation
space perm{1, ¬∑ ¬∑ ¬∑ , p} which is the set of all the permutations of {1, ¬∑ ¬∑ ¬∑ , p}.
For example,
Rank(1.1, ‚àí2, 3.2) = (2, 3, 1). We adopt average ranking, when there are ties. With the Rank
operator, we can transfer the message matrix M to a ranking matrix R by replacing its column
M:,j by the corresponding ranking vector R:,j = Rank(M:,j). Further deÔ¨Åne ei ‚âú1 p p
X"
NOTATIONS,0.07556675062972293,"j=1
Ri,j
and
vi ‚âú1 p p
X"
NOTATIONS,0.07808564231738035,"j=1
(Ri,j ‚àíei)2"
NOTATIONS,0.08060453400503778,"to be the mean and variance of Ri,:, respectively. As it is shown in later subsections, we can judge
whether node i is a malicious node based on (ei, vi) under various attack types. In the following, we
will highlight the behaviour of the benign nodes Ô¨Årst, and then discuss the behaviour of malicious
nodes and their interactions with the benign nodes under various Byzantine attacks respectively."
BEHAVIOUR OF BENIGN NODES,0.08312342569269521,"2.2
BEHAVIOUR OF BENIGN NODES
As the behaviour of benign nodes does not depend on the type of Byzantine attack, we can study the
statistical properties of (ei, vi) for a benign node i ‚ààIb before the speciÔ¨Åcation of a concrete attack
type. For any benign node i, the message generated for jth parameter is"
BEHAVIOUR OF BENIGN NODES,0.08564231738035265,"Mi,j = 1 Ni Ni
X l=1"
BEHAVIOUR OF BENIGN NODES,0.08816120906801007,"‚àÇf(Œ∏, Di,l)"
BEHAVIOUR OF BENIGN NODES,0.0906801007556675,"‚àÇŒ∏j
,
(1)"
BEHAVIOUR OF BENIGN NODES,0.09319899244332494,"where Di,l denotes the lth sample on it. Throughout this paper, we always assume that Di,ls are
independent and identically distributed (IID) samples drawn from a data distribution D. Under the
independent data assumption, since Equation 1 tells us that Mi,j is the sample mean of IID random
variables, i.e., { ‚àÇf(Œ∏,Di,l)"
BEHAVIOUR OF BENIGN NODES,0.09571788413098237,"‚àÇŒ∏j
}Ni
l=1, directly applying the Strong Law of Large Numbers (SLLN) and
Central Limit Theorem (CLT) leads to the lemma below immediately."
BEHAVIOUR OF BENIGN NODES,0.0982367758186398,"Lemma 1. Under the independent data assumption, further denote ¬µj = E( ‚àÇf(Œ∏,Di,l)"
BEHAVIOUR OF BENIGN NODES,0.10075566750629723,"‚àÇŒ∏j
) and œÉ2
j ="
BEHAVIOUR OF BENIGN NODES,0.10327455919395466,"Var( ‚àÇf(Œ∏,Di,l)"
BEHAVIOUR OF BENIGN NODES,0.10579345088161209,"‚àÇŒ∏j
) < ‚àû, with Ni going to inÔ¨Ånity we have for ‚àÄj ‚àà{1, ¬∑ ¬∑ ¬∑ , p}"
BEHAVIOUR OF BENIGN NODES,0.10831234256926953,"Mi,j ‚Üí¬µj a.s.
and
Mi,j ‚Üí
d N
 
¬µj, œÉ2
j /Ni

.
(2)"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.11083123425692695,"2.3
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.11335012594458438,"DeÔ¨Ånition 1 (Gaussian attack). In a Gaussian attack, the attacker manipulates malicious nodes
to send Gaussian random messages to the global coordinator, i.e., {Mi,:}i‚ààIm are independent
random samples from Gaussian distribution MVN(mb,:, Œ£) , where mb,: =
1
n1
P"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.11586901763224182,"i‚ààIb Mi,: and
Œ£ is the covariance matrix determined by the attacker."
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.11838790931989925,"Considering that Mi,j ‚Üí¬µj almost surely (a.s.) with Ni going to inÔ¨Ånity for all i ‚ààIb based on
Lemma 1, it is straightforward to see that limN‚àó‚Üí‚àûmb,j = ¬µj a.s., and the distribution of Mi,j for
each i ‚ààIm converges to the Gaussian distribution centered at ¬µj. Lemma 2 provides the details."
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.12090680100755667,"Lemma 2. Under the same assumption as in Lemma 1, with N ‚àógoing to inÔ¨Ånity, we have for each
malicious node i ‚ààIm under the Gaussian attack that"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.12342569269521411,"Mi,j ‚Üí
d N (¬µj, Œ£j,j) , 1 ‚â§j ‚â§p.
(3)"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.12594458438287154,"Lemma 1 and Lemma 2 tell us that for each parameter dimension j, {Mi,j}n
i=1 are independent
Gaussian random variables with the same mean (i.e, ¬µj) but different variances (i.e., œÉ2
j /Ni or Œ£j,j)
under the Gaussian attack. Due to the symmetry of Gaussian distribution, it is straightforward to see"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.12846347607052896,"E(Ri,j) = n + 1"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.1309823677581864,"2
, 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§p."
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.13350125944584382,Under review as a conference paper at ICLR 2022
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.13602015113350127,"Moreover, the exchangeability of benign nodes and the exchangeability of malicious nodes when
N ‚àóis reasonably large tell us: for each parameter dimension j, there exist two positive constants
s2
b,j and s2
m,j such that"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.1385390428211587,"Var(Ri,j) = s2
b,j, ‚àÄi ‚ààIb,
and
Var(Ri,j) = s2
m,j, ‚àÄi ‚ààIm,"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.14105793450881612,"where both s2
b,j and s2
m,j are complex functions of œÉ2
j , Œ£j,j and {Ni}i‚ààIb. Further assume that
Ri,j‚Äôs are independent of each other, thus ei =
1
p
Pp
j=1 Ri,j is the sum of independent random
variables with a common mean. Thus, according to the Kolmogorov Strong Law of Large Numbers
(KSLLN), we know that ei converges to a constant almost surely, which in turn indicates that vi also
converge some constant almost surely. The Theorem 1 summarizes the results formally, with the
detailed proof provided in Appendix C.
Theorem 1. Assuming {R:,j}1‚â§j‚â§p are independent of each other, under the Gaussian attack, we
have for each local node i that"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.14357682619647355,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûei = n + 1"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.14609571788413098,"2
a.s.,
(4)"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.1486146095717884,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
 
vi ‚àí¬Øs2
b ¬∑ I(i ‚ààIb) ‚àí¬Øs2
m ¬∑ I(i ‚ààIm)

= 0 a.s.,
(5)"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.15113350125944586,"where I(¬∑) stands for the indicator function, ¬Øs2
b ‚âú1"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.15365239294710328,"p
Pp
j=1 s2
b,j and ¬Øs2
m ‚âú1"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.1561712846347607,"p
Pp
j=1 s2
m,j."
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.15869017632241814,"Considering that ¬Øs2
b = ¬Øs2
m if and only if Œ£j,j‚Äôs fall into a lower dimensional manifold whose mea-
surement is zero under the Lebesgue measure, we have P(¬Øs2
b = ¬Øs2
m) = 0 if the attacker speciÔ¨Åes
the Gaussian variance Œ£j,j‚Äôs arbitrarily in the Gaussian attack. Thus, Theorem 1 in fact suggests
that the benign nodes and the malicious nodes are different on the value of vi, and therefore pro-
vides a guideline to detect the malicious nodes. Although the we do need N ‚àóand p to go to inÔ¨Ånity
for getting the theoretical results in Theorem 1, in practice the malicious node detection algorithm
based on the theorem typically works very well when N ‚àóand p are reasonably large and Ni‚Äôs are
not dramatically far away from each other."
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.16120906801007556,"The independent rank assumption in Theorem 1, which assumes that {R:,j}1‚â§j‚â§p are independent
of each other, may look restrictive. However, in fact it is a mild condition that can be easily satis-
Ô¨Åed in practice due to the following reasons. First, for a benign node i ‚ààIb, Mi,j and Mi,k are
often nearly independent, as the correlation between two model parameters Œ∏j and Œ∏k is often very
weak in a larger deep neural network with a huge number of parameters. To verify the statement,
we implemented independence tests for 100,000 column pairs randomly chosen from the message
matrix M generated from the FASHION-MNIST data. Distribution of the p-values of these tests
are demonstrated in Figure 2 via a histogram, which is very close to a uniform distribution, indi-
cating that Mi,j and Mi,k are indeed nearly independent in practice. Second, even some M:,j and
M:,k shows strong correlation, magnitude of the correlation would be reduced greatly during the
transformation from M to R, as the Ô¨Ånal ranking Ri,j also depends on many other factors. Actu-
ally, the independent rank assumption could be relaxed to be uncorrelated rank assumption which
assumes the ranks are uncorrelated with each other. Adopting the weaker assumption will result in a
change of convergence type of our theorems from the ‚Äúalmost surely convergence‚Äù to ‚Äúconvergence
in probability‚Äù, but with no essential inÔ¨Çuence to the our algorithm below. 0 2000 4000 6000"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.163727959697733,"0.00
0.25
0.50
0.75
1.00
p.value count"
BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK,0.16624685138539042,"Figure 2: Independence tests for 100,000 column pairs randomly chosen from message matrix M
generated from FASHION-MNIST data supports the independence assumption made in Theorem 1."
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.16876574307304787,"2.4
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.1712846347607053,"DeÔ¨Ånition 2 (Sign Ô¨Çipping attack). Sign Ô¨Çipping attack aims to generate the gradient values of
malicious nodes by Ô¨Çipping the sign of the average of all the benign nodes‚Äô gradient at each epoch,
i.e., specifying Mi,: = ‚àírmb,: for any i ‚ààIm, where r > 0, mb =
1
n1
P"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.17380352644836272,"k‚ààIb Mk,:."
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.17632241813602015,Under review as a conference paper at ICLR 2022
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.17884130982367757,"Based on the above deÔ¨Ånition, the update message of a malicious node i under the sign Ô¨Çipping
attack is
Mi,: = ‚àírmb,: = ‚àír n1 X"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.181360201511335,"k‚ààIb
Mk,:.
(6)"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.18387909319899245,"For Ô¨Åxed {Mk,:}k‚ààIb, Mi,: is also a Ô¨Åxed vector without randomness, as it is a deterministic function
of {Mk,:}k‚ààIb. On the other hand, however, we can also treat Mi,: as a random vector, since the
randomness of {Mk,:}k‚ààIb can be transferred to Mi,: via the link function in equation 6. In fact, for
any parameter dimension j, considering that Mk,j ‚Üí
d N
 
¬µj, œÉ2
j /Nk

for any k ‚ààIb according to
Lemma 1, it is straightforward to see that Mi,j = ‚àír n1
P"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.18639798488664988,"k‚ààIb Mk,j can also be well approximated
by a Gaussian distribution. The lemma 3 summarizes the result formally.
Lemma 3. Under the sign Ô¨Çipping attack, for each malicious node i ‚ààIm and any parameter
dimension j, we have Mi,j = ‚àír n1
P"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.1889168765743073,"k‚ààIb Mk,j is a deterministic function of {Mk,j}k‚ààIb, whose
limiting distribution when N ‚àógoes to inÔ¨Ånity is"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.19143576826196473,"Mi,j ‚Üí
d N
 
¬µj(r), œÉ2
j (r)

, 1 ‚â§j ‚â§p,
(7)"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.19395465994962216,"where ¬µj(r) = ‚àír¬µj, œÉ2
j (r) =
r2¬∑œÉ2
j
n1¬∑ ¬Ø
Nb , and ¬ØNb =
n1
P"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.1964735516372796,"k‚ààIb
1
Nk
is the harmonic mean of {Nk}k‚ààIb."
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.19899244332493704,"Lemma 1 and Lemma 3 tell us that for each parameter dimension j, the distribution of {Mi,j}n
i=1 is
a mixture of Gaussian components {N
 
¬µj, œÉ2
j /Ni

}i‚ààIb centered at ¬µj plus a point mass located
at ¬µj(r) = ‚àír¬µj. If Ni‚Äôs are reasonably large, variances œÉ2
j /Ni‚Äôs would be very close to zero,
and the probability mass of the mixture distribution would concentrate to two local centers ¬µj and
¬µj(r) = ‚àír¬µj, one for the benign nodes and the other one for the malicious nodes. This intuition
provides us the guidance to identify the malicious nodes in this attack pattern. Transforming to
the rank domain, the above intuition leads to different behavior patterns of the benign nodes and
the malicious nodes in the rank matrix R, which in turn result in different limiting behavior of
(ei, vi) for the benign and malicious nodes. The theorem 2 summarizes the results formally, with
the detailed proof provided in Appendix D.
Theorem 2. With the same independent rank assumption as posed in Theorem 1, under the sign
Ô¨Çipping attack, we have for each local node i that"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.20151133501259447,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûei
=
¬Ø¬µb ¬∑ I(i ‚ààIb) + ¬Ø¬µm ¬∑ I(i ‚ààIm) a.s.,
(8)"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.2040302267002519,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûvi
=
¬Øs2
b ¬∑ I(i ‚ààIb) + ¬Øs2
m ¬∑ I(i ‚ààIm) a.s.,
(9)"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.20654911838790932,where ¬Ø¬µb = n+n0+1
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.20906801007556675,"2
‚àín0œÅ, ¬Ø¬µm = n1œÅ + n0+1"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.21158690176322417,"2
, œÅ = limp‚Üí‚àû"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.2141057934508816,"Pp
j=1 I(¬µj>0)"
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.21662468513853905,"p
, ¬Øs2
m and ¬Øs2
b are both
quadratic functions of œÅ whose concrete form also depends on n0 and n1."
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.21914357682619648,Considering that ¬Ø¬µb = ¬Ø¬µm if and only if œÅ = 1
MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK,0.2216624685138539,"2, and ¬Øs2
b = ¬Øs2
m if and only if œÅ is the solution of a
quadratic function, the probability of (¬Ø¬µb, ¬Øs2
b) = (¬Ø¬µm, ¬Øs2
m) is zero as p ‚Üí‚àû. Such a phenomenon
suggests that we can detect the malicious nodes based on the moments (ei, vi) to defense the sign
Ô¨Çipping attack as well. Noticeably, we note that the limit behaviour of ei and vi does not dependent
on the speciÔ¨Åcation of r, which deÔ¨Ånes the sign Ô¨Çipping attack. Although such a fact looks a bit
abnormal at the Ô¨Årst glance, it is totally understandable once we realize that with the variance of
Mi,j shrinks to zero with Ni goes to inÔ¨Ånity for each benign node i, any different between ¬µj and
¬µj(r) would result in the same rank vector R:,j in the rank domain."
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.22418136020151133,"2.5
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK"
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.22670025188916876,"DeÔ¨Ånition 3 (Zero gradient attack). Zero gradient attack aims to make the aggregated message to
be zero, i.e., Pn
i=1 Mi,: = 0, at each epoch, by specifying Mi,: = ‚àín1"
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.22921914357682618,"n0 mb,: for all i ‚ààIm."
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.23173803526448364,"Apparently, the zero gradient attack deÔ¨Åned above is a special case of sign Ô¨Çipping attack by speci-
fying r = n1"
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.23425692695214106,"n0 . Since the conclusions of Theorem 2 keep unchanged for different speciÔ¨Åcations of r
as we have discussed, we have the following corollary for zero gradient attack.
Corollary 1. Under the zero gradient attack, ei‚Äôs and vi‚Äôs follow exactly the same limiting be-
haviours as described in Theorem 2."
MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK,0.2367758186397985,Under review as a conference paper at ICLR 2022
MANDERA,0.23929471032745592,"2.6
MANDERA
Theorem 1, 2 and Corollary 1 imply that, under these three attacks (Gaussian attack, zero gradient
attack and sign Ô¨Çipping attack), the Ô¨Årst two moments of Ri,:, i.e., (ei, vi), converge to two different
limits for the benign nodes and the malicious nodes, respectively. Thus, for a real dataset where
Ni‚Äôs and p are all Ô¨Ånite but reasonably large numbers, the scatter plot of {(ei, vi)}1‚â§i‚â§n would
demonstrate a clustering structure: one cluster for the benign nodes and the other cluster for the
malicious nodes. Figure 3 illustrates such a scatter plot for the 100 local nodes in a typical epoch
of training the FASHION-MNIST dataset under different FL settings (to keep the two dimensions
of the scatter plot to the same scale, we replaced vi by its square root si = ‚àövi instead). Clearly, a
simple clustering procedure would detect the malicious nodes from the scatter plot. Based on this
intuition, we propose MAlicious Node DEtection via RAnking (MANDERA) to detect the malicious
nodes, whose workÔ¨Çow is detailed in Algorithm 1."
MANDERA,0.24181360201511334,"Algorithm 1 Malicious node detection via ranking (MANDERA)
Input: The message matrix M."
MANDERA,0.24433249370277077,"1: Convert the message matrix M to the ranking matrix R by applying Rank operator.
2: Compute mean and standard deviation of rows in R, i.e., {(ei, si)}1‚â§i‚â§n.
3: Run the clustering algorithm K-means to {(ei, si)}1‚â§i‚â§n with K = 2, and predict the set of
benign nodes with the lager cluster denoted by ÀÜIb.
Output: The predicted benign node set ÀÜIb."
MANDERA,0.24685138539042822,"Remark. MANDERA can be applied to either a single epoch or multiple epochs. For a single-epoch
mode, the input data M is the message matrix received from a single epoch. For multiple-epoch
mode, the data M is the column-concatenation of the message matrices from multiple epochs. By
default, the experiments below all use single epoch to detect the malicious nodes."
MANDERA,0.24937027707808565,"The predicted benign nodes ÀÜIb obtained by MANDERA naturally leads to an aggregated message
ÀÜmb,: =
1
#(ÀÜIb)
P"
MANDERA,0.2518891687657431,"i‚ààÀÜIb Mi,:. The theorem 3 shows that ÀÜIb and ÀÜmb lead to consistent estimations of
Ib and mb respectively, indicating that MANDERA enjoys robustness guarantee (Steinhardt, 2018)
for typical Byzantine attacks.
Theorem 3. Under the three typical Byzantine attacks, i.e., Gaussian attack, sign Ô¨Çipping attack
and zero gradient attack, we have:"
MANDERA,0.25440806045340053,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûP(ÀÜIb = Ib) = 1
and
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûE|| ÀÜmb,: ‚àímb,:||2 = 0.
(10)"
MANDERA,0.25692695214105793,The proof of Theorem 3 can be found in Appendix E.
EXPERIMENTS,0.2594458438287154,"3
EXPERIMENTS"
EXPERIMENTS,0.2619647355163728,"We evaluate the efÔ¨Åcacy in detecting malicious nodes within the federated learning framework with
the use of three Datasets. The Ô¨Årst is the FASHION-MNIST dataset (Xiao et al., 2017), a dataset of
60,000 and 10,000 training and testing samples respectively divided into 10 classes of apparel. The
second is CIFAR-10 (Krizhevsky et al., 2009), a dataset of 60,000 small object images also con-
taining 10 object classes. The third is MNIST (Deng, 2012) dataset which appears in Appendix H.
In these experiments we mainly adopt implementations of Byzantine attacks released by (Wu et al.,
2020b;a) and the label Ô¨Çipping attack (Tolpegin et al., 2020b;a). The label Ô¨Çipping attack is a data
poisoning attack that alters one or more labels of training data to an attacker‚Äôs pre-determined target
label. For example, in CIFAR-10‚Äôs object labels, an attacker may change the labels of their local
cat images to be labelled as dogs. We use the Label Flipping attack as a comparative poisoning
attack that achieves its objective in a more subtle manner. In our experiments, we set Œ£ = 30I for
the Gaussian attack and r = 3 for the sign Ô¨Çipping attack, where I is the identity matrix. For all
experiments we Ô¨Åx n = 100 participating nodes, of which a variable number of nodes are poisoned
|n0| ‚àà{5, 10, 15, 20, 25, 30}. The training process is run until 25 epochs have elapsed. We have
described the structure of these networks in Appendix A."
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.26448362720403024,"3.1
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING
Section 2 speculated that the distribution of parameter ranks differ sufÔ¨Åciently for the detection of
malicious and benign nodes. We validate this hypothesis in Figure 3 by illustrating the difference"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.26700251889168763,Under review as a conference paper at ICLR 2022
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2695214105793451,"5
10
15
20
25
30"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.27204030226700254,"GA
ZG
SF
LF"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.27455919395465994,"48 49 50 51 52 53
48 49 50 51 52 53
48 49 50 51 52 53
48 49 50 51 52 53
48 49 50 51 52 53
48 49 50 51 52 53 20 30 40"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2770780856423174,"25
30
35
40
45 25 30 35 40"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2795969773299748,"27
28
29
30
31
32 Mean SD"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.28211586901763225,"Node.type
Benign
Malicious"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.28463476070528965,"Figure 3: The scatter plots of (ei, si) for the 100 nodes under four types of attack as illustrative ex-
amples demonstrating ranking mean and variance from the 1st epoch of training for the FASHION-
MNIST dataset."
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2871536523929471,"GA
ZG
SF
LF"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.28967254408060455,"Accuracy
Recall
Precision
F1"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.29219143576826195,"5 1015202530
5 1015202530
5 1015202530
5 1015202530 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2947103274559194,Number of malicious nodes Score
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.2972292191435768,"Metric
Accuracy
Recall
Precision
F1"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.29974811083123426,(a) CIFAR-10
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3022670025188917,"GA
ZG
SF
LF"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3047858942065491,"Accuracy
Recall
Precision
F1"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.30730478589420657,"5 1015202530
5 1015202530
5 1015202530
5 1015202530 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.30982367758186397,Number of malicious nodes Score
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3123425692695214,"Metric
Accuracy
Recall
Precision
F1"
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3148614609571788,"(b) FASHION-MNIST
Figure 4: ClassiÔ¨Åcation performance of our proposed approach MANDERA (Algorithm 1) under
four types of attack for CIFAR-10 and FASHION-MNIST data.
Gaussian Attack (GA); Zero-
Gradient (ZG); Sign-Flipping (SF); and Label-Flipping (LF). The boxplot bounds the 25th (Q1)
and 75th (Q3) percentile, with the central line representing the 50th quantile (median). The end
points of the whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively."
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.31738035264483627,"between the benign nodes and malicious nodes in terms of the mean of gradients‚Äô rankings and the
standard deviation of gradients‚Äô ranking."
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3198992443324937,"It can be observed from Figure 3 that, under Gaussian and Label Ô¨Çipping attacks, the average rank-
ings of malicious nodes are of a similar distribution to benign nodes. It is problematic for distin-
guishing between the two types of nodes, if only average ranking information is used. On the other
hand, Figure 3 displays a larger separation of distributions for the standard deviation of ranking. It is
noted that all 4 attacks observe a convergence of the distributions as the number of malicious nodes
increase, increasing the difÔ¨Åculty of defense for both MANDERA and all other defenses. How-
ever, the likelihood of an attacker controlling increasingly large numbers of malicious nodes also
decrease."
ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING,0.3224181360201511,Under review as a conference paper at ICLR 2022
MALICIOUS NODE DETECTION BY MANDERA,0.3249370277078086,"3.2
MALICIOUS NODE DETECTION BY MANDERA
We test the performance of MANDERA on the update gradients of a model under attacks. In this
section, MANDERA acts as an observer without intervening in the learning process to identify
malicious nodes with a set of gradients from a single epoch. Each conÔ¨Åguration of 25 training
epochs, with a given number of malicious nodes was repeated 20 times. Figure 4 demonstrates the
classiÔ¨Åcation performance (Metrics deÔ¨Åned in Appendix B) of MANDERA with different settings
of participating malicious nodes and the four poisoning attacks of Guassian Attack (GA), Zero
Gradient attack (ZG), Sign Flipping attack (SF) and the Label Flipping attack (LF)."
MALICIOUS NODE DETECTION BY MANDERA,0.327455919395466,"While we have formally demonstrated the efÔ¨Åcacy of MANDERA in accurately detecting poten-
tially malicious nodes participating in the federated learning process. In practice, to leverage an
unsupervised K-means clustering algorithm, we must also identify the correct group of nodes as the
malicious group. Our strategy is to identify the group with the most exact gradients, or otherwise
the smaller group (we regard a system with over 50% of their nodes compromised as having larger
issues than just poisoning attacks) 1. We also test other clustering algorithms, such as hierarchical
clustering and Gaussian mixture models (Fraley & Raftery, 2002). It turns out that the performance
of MANDERA is quite robust with different choices of clustering methods. Detailed results can be
found in Appendix F."
MALICIOUS NODE DETECTION BY MANDERA,0.32997481108312343,"From Figure 4, it is immediately evident that the recall of the malicious nodes for the Byzantine at-
tacks is exceptional. However, occasionally benign nodes have also been misclassiÔ¨Åed as malicious
under a SF, and to a lesser extent the ZG attack for both datasets. On all attacks, in the presence of
more malicious nodes, the recall of malicious nodes trends down. As for the data poisoning attack
of LF, it is consistently more difÔ¨Åcult to detect, however we note that the LF attack has a more subtle
inÔ¨Çuence on the model in contrast to the impact of Byzantine attacks."
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.33249370277078083,"3.3
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS
In this section, we encapsulate MANDERA into a module prior to the the aggregation step, MAN-
DERA has the sole objective of identifying malicious nodes, and excluding their updates from the
global aggregation step. Each conÔ¨Åguration of 25 training epochs, a given poisoning attack, defense
method, and a given number of malicious nodes was repeated 10 times. We compare MANDERA
against 5 other robust aggregation defense methods, Krum (Blanchard et al., 2017), Bulyan (Guer-
raoui et al., 2018), Trimmed Mean (Yin et al., 2018), Median (Yin et al., 2018) and FLTrust (Cao
et al., 2020). Of which the Ô¨Årst 2 requires an assumed number of malicious nodes, and the latter 3
only aggregate robustly."
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3350125944584383,"5
10
15
20
25
30"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.33753148614609574,"GA
ZG
SF
LF"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.34005037783375314,0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3425692695214106,"10
20
30
40
50 20 30 40 50"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.345088161209068,"20
30
40
50 30 40 50"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.34760705289672544,Number of Epoch
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3501259445843829,Accuracy
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3526448362720403,Defence Krum
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.35516372795969775,NO‚àíattack
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.35768261964735515,Bulyan
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3602015113350126,Median
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.36272040302267,Trim‚àímean
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.36523929471032746,MANDERA
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3677581863979849,FLTrust
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3702770780856423,(a) CIFAR-10 Dataset
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.37279596977329976,"5
10
15
20
25
30"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.37531486146095716,"GA
ZG
SF
LF"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3778337531486146,0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 25 50 75
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.380352644836272,"40
50
60
70
80
90"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.38287153652392947,"50
60
70
80
90"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3853904282115869,"70
75
80
85"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3879093198992443,Number of Epoch
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3904282115869018,Accuracy
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3929471032745592,Defence Krum
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3954659949622166,NO‚àíattack
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.3979848866498741,Bulyan
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.4005037783375315,Median
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.40302267002518893,Trim‚àímean
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.40554156171284633,MANDERA
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.4080604534005038,FLTrust
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.4105793450881612,"(b) FASHION-MNIST dataset
Figure 5: Model Accuracy at each epoch of training, each line of the curve represents a different
defense against the poisoning attacks."
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.41309823677581864,"From Figure 5, it is observed that MANDERA performs about the same as the best performing
defense mechanisms, close to the performance of a model not under attack. MANDERA‚Äôs accuracy"
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.4156171284634761,"1More informed approaches to selecting the malicious cluster can be tested in future work. E.g. Figure 3
displays less variation of rank variance in malicious cluster compared to benign nodes. This could robust
selection of the malicious group, and enabling selection of malicious groups larger than 50%."
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.4181360201511335,Under review as a conference paper at ICLR 2022
MANDERA FOR DEFENDING AGAINST POISONING ATTACKS,0.42065491183879095,"is observed to vary slightly under the LF attack on fashion data with 30 malicious nodes, this is
consistent with the larger accuracy ranges previously observed in Figure 4b. Interestingly, FLTrust
as a standalone defense is weak in protecting against the most extreme Byzantine attacks. However,
we highlight that FLtrust is a robust aggregation method against targeted attacks that may thwart
defences like Krum, Trimmed mean. We see FLTrust as a complementary defence that relies on a
base method of defence against Byzantine attacks, but expands the protection coverage of the FL
system against adaptive attacks."
"COMPUTATIONAL EFFICIENCY
WE HAVE PREVIOUSLY BEEN ABLE TO OBSERVE THAT MANDERA CAN PERFORM AT PAR WITH THE CURRENT",0.42317380352644834,"3.4
COMPUTATIONAL EFFICIENCY
We have previously been able to observe that MANDERA can perform at par with the current
highest performing poisoning attack defenses. Another beneÔ¨Åt arises with the simpliÔ¨Åcation of
the mitigation strategy with the introduction of ranking at the core of the algorithm. Sorting and
Ranking algorithms are fast. Additionally, we only apply clustering on the two dimensions of rank
mean and standard deviation, in contrast to other works that seek to cluster on the entire node
update (Chen et al., 2021). The times in Table 1 for MANDERA, Krum and Bulyan do not include
the parameter/gradient aggregation step. These times were computed on 1 core of a Dual Xeon
14-core E5-2690, with 8 Gb of system RAM and a single Nvidia Tesla P100. Table 1 demonstrates
that MANDERA is able to achieve a faster speed than that of single Krum 2 (by more than half) and
Bulyan (by an order of magnitude).
Table 1: Mean and standard deviation of computational times for defense function given the same
set of gradients from 100 nodes, of which 30 were malicious. Each function was repeated 100 times."
"COMPUTATIONAL EFFICIENCY
WE HAVE PREVIOUSLY BEEN ABLE TO OBSERVE THAT MANDERA CAN PERFORM AT PAR WITH THE CURRENT",0.4256926952141058,"Defense (Detection)
Mean ¬± SD (ms)
Defense (Aggregation)
Mean ¬± SD (ms)
MANDERA
643
¬± 8.646
Trimmed Mean
3.96 ¬± 0.41
Krum (Single)
1352 ¬± 10.09
Median
9.81 ¬± 3.88
Bulyan
27209 ¬± 233.4
FLTrust
361 ¬± 4.07"
DISCUSSION AND CONCLUSION,0.4282115869017632,"4
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.43073047858942065,"If attackers create more adaptive attacks unlike DeÔ¨Ånition 1, 2 and 3, they may evade MANDERA
and achieve model poisoning. In this work, we have conÔ¨Ågured our Federated Learner to use all 100
nodes in the learning process at every round, we acknowledge FL framework may learn the global
model only using subset of nodes at each round. In these settings MANDERA would still function,
as we would rank and cluster on the parameters of the participating nodes, without assuming any
number of poisoned nodes. In Algorithm 1, performance could be improved by incorporating higher
order moments. MANDERA is unable to function when gradients are securely aggregated in its
current form. However, malicious nodes can be identiÔ¨Åed and excluded from the secure aggregation
step, while still protecting the privacy of participating nodes by performing MANDERA through
secure ranking (Zhang et al., 2013; Lin & Tzeng, 2005) (recall that MANDERA only requires the
ranking matrix to detect poisoned nodes). It remains to be seen the effectiveness of MANDERA on
more advanced poisoning techniques like adversarial poisoning or Evasion attacks."
DISCUSSION AND CONCLUSION,0.4332493702770781,"In conclusion, we have provided theoretical guarantees and experimentally shown efÔ¨Åcacy in the
use of ranking algorithms for the detection of malicious nodes performing poisoning attacks against
federated learning. Our proposed method MANDERA, is able to achieve high detection accuracy
and maintain a model accuracy on par with other seminal, high performing defense mechanisms, but
with three notable advantages. First, provable guarantees for the use of ranking to detect Gaussian,
Zero Gradient and Sign Flipping attacks. Next, faster detection with the use of ranking algorithms.
Finally, the MANDERA defense does not need a prior estimation of the number of poisoned nodes.
In this work we demonstrate how the rank domain can be useful in applications to defend against
malicious actors."
ETHICS STATEMENT,0.4357682619647355,Ethics Statement
ETHICS STATEMENT,0.43828715365239296,"The core objective of our research is to provide an additional means of defense against poisoning
nodes that target Federated Learning. To test our defense we have implemented different attacks
against the Federated Learning framework. Attackers may adopt our defense strategy to design
new poisoning attacks. Fortunately, these poisoning attacks can not be leveraged to leak private
information from Federated learning models, instead only impact its performance."
ETHICS STATEMENT,0.44080604534005036,2The use of multi-krum would have yielded better protection (c.f. Section 3) at the behest of speed.
ETHICS STATEMENT,0.4433249370277078,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.44584382871536526,Reproducibility Statement
REPRODUCIBILITY STATEMENT,0.44836272040302266,"To ensure reproducible research, we have supplemented our proposal for MANDERA, by supplying
both R and Python implementations of MANDERA used in this paper, uploaded with the remainder
of the experiment code. The three datasets featured in this paper is CIFAR-10 (Krizhevsky et al.,
2009), Fasion-MNIST (Xiao et al., 2017), and MNIST (Deng, 2012); we have used each of these
dataset unaltered from their respective sources. We have stated the assumptions in our theorems
and their proofs can be found in the Appendix. But to explain our assumptions in simple terms, (1)
The data samples on each local node are independently drawn from the same distribution. (2) The
gradient value for each parameter is independent to each other."
REFERENCES,0.4508816120906801,REFERENCES
REFERENCES,0.4534005037783375,"Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics,
pp. 2938‚Äì2948. PMLR, 2020."
REFERENCES,0.45591939546599497,"Peva Blanchard,
El Mahdi El Mhamdi,
Rachid Guerraoui,
and Julien Stainer.
Ma-
chine learning with adversaries:
Byzantine tolerant gradient descent.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf."
REFERENCES,0.45843828715365237,"Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust feder-
ated learning via trust bootstrapping. arXiv preprint arXiv:2012.13995, 2020."
REFERENCES,0.4609571788413098,"Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against
malicious clients. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 35,
pp. 6885‚Äì6893, 2021."
REFERENCES,0.4634760705289673,"Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial set-
tings: Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst., 1(2), December 2017.
doi: 10.1145/3154503. URL https://doi.org/10.1145/3154503."
REFERENCES,0.4659949622166247,"Zheyi Chen, Pu Tian, Weixian Liao, and Wei Yu. Zero knowledge clustering based adversarial
mitigation in heterogeneous federated learning. IEEE Transactions on Network Science and En-
gineering, 8(2):1070‚Äì1083, 2021. doi: 10.1109/TNSE.2020.3002796."
REFERENCES,0.46851385390428213,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
Signal Processing Magazine, 29(6):141‚Äì142, 2012."
REFERENCES,0.47103274559193953,"Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong.
Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Se-
curity 20), pp. 1605‚Äì1622, 2020."
REFERENCES,0.473551637279597,"Chris Fraley and Adrian E Raftery.
Model-based clustering, discriminant analysis, and density
estimation. Journal of the American statistical Association, 97(458):611‚Äì631, 2002."
REFERENCES,0.4760705289672544,"Rachid Guerraoui, S¬¥ebastien Rouault, et al.
The hidden vulnerability of distributed learning in
byzantium. In International Conference on Machine Learning, pp. 3521‚Äì3530. PMLR, 2018."
REFERENCES,0.47858942065491183,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.4811083123425693,"Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients
for robust federated learning. arXiv preprint arXiv:2002.00211, 2020."
REFERENCES,0.4836272040302267,"Hsiao-Ying Lin and Wen-Guey Tzeng. An efÔ¨Åcient solution to the millionaires‚Äô problem based on
homomorphic encryption. In International Conference on Applied Cryptography and Network
Security, pp. 456‚Äì466. Springer, 2005."
REFERENCES,0.48614609571788414,Under review as a conference paper at ICLR 2022
REFERENCES,0.48866498740554154,"Jinhyun So, Bas¬∏ak G¬®uler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning.
IEEE Journal on Selected Areas in Communications, 39(7):2168‚Äì2181, 2021. doi: 10.1109/
JSAC.2020.3041404."
REFERENCES,0.491183879093199,"Jacob Steinhardt. Robust learning: Information theory and algorithms. PhD thesis, Stanford Uni-
versity, 2018."
REFERENCES,0.49370277078085645,"Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems - github. https://github.com/git-disl/DataPoisoning FL, 2020a."
REFERENCES,0.49622166246851385,"Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems. In European Symposium on Research in Computer Security, pp. 480‚Äì
501. Springer, 2020b."
REFERENCES,0.4987405541561713,"Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis.
Byrd-saga - github.
https://github.com/MrFive5555/Byrd-SAGA, 2020a."
REFERENCES,0.5012594458438288,"Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced
stochastic gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal
Processing, 68:4583‚Äì4596, 2020b."
REFERENCES,0.5037783375314862,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. 2017."
REFERENCES,0.5062972292191436,"Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In International Conference on Machine Learning, pp. 6893‚Äì
6901. PMLR, 2019."
REFERENCES,0.5088161209068011,"Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In Interna-
tional Conference on Machine Learning, pp. 10495‚Äì10503. PMLR, 2020."
REFERENCES,0.5113350125944585,"Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650‚Äì5659. PMLR, 2018."
REFERENCES,0.5138539042821159,"Lan Zhang, Xiang-Yang Li, Yunhao Liu, and Taeho Jung. VeriÔ¨Åable private multi-party computa-
tion: ranging and ranking. In 2013 Proceedings IEEE INFOCOM, pp. 605‚Äì609. IEEE, 2013."
REFERENCES,0.5163727959697733,"A
NEURAL NETWORK CONFIGURATIONS"
REFERENCES,0.5188916876574308,"We train these models with a batch size of 10, an SGD optimizer operates with a learning rate of
0.01, and 0.5 momentum for 25 epochs. The accuracy of the model is evaluated on a holdout set of
1000 samples."
REFERENCES,0.5214105793450882,"A.1
FASHION-MNIST AND MNIST"
REFERENCES,0.5239294710327456,"‚Ä¢ Layer 1: 1 ‚àó16 ‚àó5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling."
REFERENCES,0.5264483627204031,"‚Ä¢ Layer 2: 16‚àó32‚àó5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling."
REFERENCES,0.5289672544080605,"‚Ä¢ Output: 10 Classes, Linear."
REFERENCES,0.5314861460957179,"A.2
CIFAR-10"
REFERENCES,0.5340050377833753,"‚Ä¢ Layer 1: 1 ‚àó32 ‚àó3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling."
REFERENCES,0.5365239294710328,"‚Ä¢ Layer 2: 32‚àó32‚àó3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling."
REFERENCES,0.5390428211586902,"‚Ä¢ Output: 10 Classes, Linear."
REFERENCES,0.5415617128463476,Under review as a conference paper at ICLR 2022
REFERENCES,0.5440806045340051,"B
METRICS"
REFERENCES,0.5465994962216625,"The metrics observed in Section 3 to evaluate the performance of the defense mechanisms are de-
Ô¨Åned as follows:"
REFERENCES,0.5491183879093199,"Precision =
TP
TP+FP,"
REFERENCES,0.5516372795969773,"Accuracy =
TP+TN
TP+FP+FN+TN,"
REFERENCES,0.5541561712846348,"Recall =
TP
TP+FN,"
REFERENCES,0.5566750629722922,F1 = 2 √ó Precision √ó Recall
REFERENCES,0.5591939546599496,Precision+Recall .
REFERENCES,0.5617128463476071,"C
PROOF OF THEOREM 1"
REFERENCES,0.5642317380352645,"Proof. Because {Ri,j}1‚â§j‚â§p are independent random variables with a Ô¨Ånite upper bound (since n
is Ô¨Åxes) as assumed, direct application of KSLLN leads to"
REFERENCES,0.5667506297229219,"lim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.5692695214105793," 
Ri,j ‚àíE(Ri,j)

= 0 a.s.,
(11)"
REFERENCES,0.5717884130982368,"lim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.5743073047858942,"h 
Ri,j ‚àíE(Ri,j)
2 ‚àíVar(Ri,j)
i
= 0 a.s..
(12)"
REFERENCES,0.5768261964735516,"To prove Theorem 1 based on Equation 11 and 12, we need to derive the concrete form of E(Ri,j)
and Var(Ri,j)."
REFERENCES,0.5793450881612091,"Fortunately, because Mi,j ‚Üí
d N
 
¬µj, Œ£j,j

for ‚àÄi ‚ààIm and Mi,j ‚Üí
d N
 
¬µj, œÉ2
j /Ni

for ‚àÄi ‚ààIb
when N ‚àó‚Üí‚àû, it is straightforward to see due to the symmetry of Gaussian distribution that"
REFERENCES,0.5818639798488665,"lim
N‚àó‚Üí‚àûE(Ri,j) = n + 1"
REFERENCES,0.5843828715365239,"2
, 1 ‚â§i ‚â§n, 1 ‚â§j ‚â§p.
(13)"
REFERENCES,0.5869017632241813,"Moreover, assuming that the sample sizes of different benign nodes approach to each other with N ‚àó
going to inÔ¨Ånity, i.e.,"
REFERENCES,0.5894206549118388,"lim
N ‚àó‚Üí‚àû
1
N ‚àómax
i,k‚ààIb |Ni ‚àíNk| = 0,
(14)"
REFERENCES,0.5919395465994962,"for each parameter dimension j, {Mi,j}i‚ààIb would converge to the same Gaussian distribution
N(¬µj, œÉ2
j /N ‚àó) with the increase of N ‚àó.
Thus, due to the exchangeability of {Mi,j}i‚ààIb and
{Mi,j}i‚ààIm, it is easy to see that there exist two positive constants s2
b and s2
m, such that"
REFERENCES,0.5944584382871536,"lim
N‚àó‚Üí‚àûVar(Ri,j) = s2
b ¬∑ I(i ‚ààIb) + s2
m,j ¬∑ I(i ‚ààIm),
(15)"
REFERENCES,0.5969773299748111,"where s2
b,j and s2
m,j are both complex functions of n0, n1, œÉ2
j , Œ£j,j and N ‚àó, and s2
b,j = s2
m,j if and
only if œÉ2
j /N ‚àó= Œ£j,j."
REFERENCES,0.5994962216624685,"Combining Equation 11 and 13, we have"
REFERENCES,0.6020151133501259,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûei =
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X"
REFERENCES,0.6045340050377834,"j=1
Ri,j = n + 1"
REFERENCES,0.6070528967254408,"2
a.s.,"
REFERENCES,0.6095717884130982,"i.e., Equation 4, which further indicates that ei and E(Ri,j) share the same limit when both p and
N ‚àógo to inÔ¨Ånity. Thus, we have"
REFERENCES,0.6120906801007556,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûvi
=
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.6146095717884131," 
Ri,j ‚àíei
2"
REFERENCES,0.6171284634760705,"=
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.6196473551637279," 
Ri,j ‚àíE(Ri,j)
2 a.s..
(16)"
REFERENCES,0.6221662468513854,Under review as a conference paper at ICLR 2022
REFERENCES,0.6246851385390428,"Combining Equation 12, 15, and 16, we have"
REFERENCES,0.6272040302267002,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
 
vi ‚àí¬Øs2
b ¬∑ I(i ‚ààIb) ‚àí¬Øs2
m ¬∑ I(i ‚ààIm)

= 0 a.s.,"
REFERENCES,0.6297229219143576,"i.e., Equation 5. Thus, the proof is complete."
REFERENCES,0.6322418136020151,"D
PROOF OF THEOREM 2"
REFERENCES,0.6347607052896725,"Proof. It is straightforward to see that equation 11 also holds for sign Ô¨Çipping attack under the
assumptions of Theorem 2. But, we need to re-calculate E(Ri,j) for benign and malicious nodes
under the new setting."
REFERENCES,0.6372795969773299,"Under the sign Ô¨Çipping attack, because Mi,j ‚Üí
d
N
 
¬µj(r), œÉ2
j (r)

for ‚àÄi ‚ààIm and Mi,j ‚Üí
d"
REFERENCES,0.6397984886649875,"N
 
¬µj, œÉ2
j /Ni

for ‚àÄi ‚ààIb when N ‚àó‚Üí‚àû, and"
REFERENCES,0.6423173803526449,"lim
N‚àó‚Üí‚àû(œÉ2
j /Ni) =
lim
N ‚àó‚Üí‚àûœÉ2
j (r) = 0,"
REFERENCES,0.6448362720403022,it is straightforward to see that
REFERENCES,0.6473551637279596,"lim
N‚àó‚Üí‚àûP(Mi,j > Mk,j) = I(¬µj > 0), ‚àÄi ‚ààIb, ‚àÄk ‚ààIm,"
REFERENCES,0.6498740554156172,which further indicates that
REFERENCES,0.6523929471032746,"lim
N‚àó‚Üí‚àûE(Ri,j) = n1 + 1"
REFERENCES,0.654911838790932,"2
¬∑ I(i ‚ààIb) + n + n1 + 1"
REFERENCES,0.6574307304785895,"2
¬∑ I(i ‚ààIm) if ¬µj > 0,"
REFERENCES,0.6599496221662469,"lim
N‚àó‚Üí‚àûE(Ri,j) = n0 + 1"
REFERENCES,0.6624685138539043,"2
¬∑ I(i ‚ààIm) + n + n0 + 1"
REFERENCES,0.6649874055415617,"2
¬∑ I(i ‚ààIb) if ¬µj < 0;
(17)"
REFERENCES,0.6675062972292192,"lim
N‚àó‚Üí‚àûE(R2
i,j) = S2
[1,n1] ¬∑ I(i ‚ààIb) + S2
[n1+1,n] ¬∑ I(i ‚ààIm) if ¬µj > 0,"
REFERENCES,0.6700251889168766,"lim
N‚àó‚Üí‚àûE(R2
i,j) = S2
[1,n0] ¬∑ I(i ‚ààIm) + S2
[n0+1,n] ¬∑ I(i ‚ààIb) if ¬µj < 0,
(18)"
REFERENCES,0.672544080604534,"where S2
[a,b] =
1
b‚àía+1
Pb
k=a k2."
REFERENCES,0.6750629722921915,"Combining Equation 11 and 17, we have"
REFERENCES,0.6775818639798489,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûei ="
REFERENCES,0.6801007556675063,"(
œÅ ¬∑ n+n1+1"
REFERENCES,0.6826196473551638,"2
+ (1 ‚àíœÅ) ¬∑ n0+1"
REFERENCES,0.6851385390428212,"2
= ¬Ø¬µm a.s., if i ‚ààIm,"
REFERENCES,0.6876574307304786,œÅ ¬∑ n1+1
REFERENCES,0.690176322418136,"2
+ (1 ‚àíœÅ) ¬∑ n+n0+1"
REFERENCES,0.6926952141057935,"2
= ¬Ø¬µb a.s., if i ‚ààIb,"
REFERENCES,0.6952141057934509,where œÅ = limp‚Üí‚àû
REFERENCES,0.6977329974811083,"Pp
j=1 I(¬µj>0)"
REFERENCES,0.7002518891687658,"p
, i.e., Equation 8."
REFERENCES,0.7027707808564232,"DeÔ¨Åne ¬Ø¬µi = ¬Ø¬µm ¬∑ I(i ‚ààIm) + ¬Ø¬µb ¬∑ I(i ‚ààIb). Based on KSLLN, we have:"
REFERENCES,0.7052896725440806,"lim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.707808564231738,"h
(Ri,j ‚àí¬Ø¬µi)2 ‚àíE(Ri,j ‚àí¬Ø¬µi)2i
= 0 a.s.."
REFERENCES,0.7103274559193955,As we have proved in Equation 8 that
REFERENCES,0.7128463476070529,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûei = ¬Ø¬µi a.s.,"
REFERENCES,0.7153652392947103,we have
REFERENCES,0.7178841309823678,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.7204030226700252,"h
(Ri,j ‚àíei)2 ‚àíE(Ri,j ‚àí¬Ø¬µi)2i
= 0 a.s.,"
REFERENCES,0.7229219143576826,which implies that
REFERENCES,0.72544080604534,"lim
N ‚àó‚Üí‚àûlim
p‚Üí‚àûvi =
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X"
REFERENCES,0.7279596977329975,"j=1
(Ri,j ‚àíei)2 =
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X"
REFERENCES,0.7304785894206549,"j=1
E(Ri,j ‚àí¬Ø¬µi)2 a.s.."
REFERENCES,0.7329974811083123,Under review as a conference paper at ICLR 2022
REFERENCES,0.7355163727959698,Considering that
REFERENCES,0.7380352644836272,"lim
N‚àó‚Üí‚àûlim
p‚Üí‚àû
1
p p
X"
REFERENCES,0.7405541561712846,"j=1
E(Ri,j ‚àí¬Ø¬µi)2"
REFERENCES,0.743073047858942,"=
lim
p‚Üí‚àû
lim
N ‚àó‚Üí‚àû
1
p p
X j=1"
REFERENCES,0.7455919395465995,"
E(R2
i,j) ‚àí2¬Ø¬µiE(Ri,j) + (¬Ø¬µi)2"
REFERENCES,0.7481108312342569,"=

¬ØœÑm ‚àí(¬Ø¬µm)2
¬∑ I(i ‚ààIm) +

¬ØœÑb ‚àí(¬Ø¬µb)2
¬∑ I(i ‚ààIb),
where
¬ØœÑb = œÅ ¬∑ S2
[1,n1] + (1 ‚àíœÅ) ¬∑ S2
[n0+1,n],"
REFERENCES,0.7506297229219143,"¬ØœÑm = œÅ ¬∑ S2
[n1+1,n] + (1 ‚àíœÅ) ¬∑ S2
[1,n0].
we have
lim
N‚àó‚Üí‚àûlim
p‚Üí‚àûvi =

¬ØœÑm ‚àí(¬Ø¬µm)2
¬∑ I(i ‚ààIm) +

¬ØœÑb ‚àí(¬Ø¬µb)2
¬∑ I(i ‚ààIb)."
REFERENCES,0.7531486146095718,"It completes the proof of Equation 9 by specifying ¬Øs2
b = ¬ØœÑb ‚àí(¬Ø¬µb)2 and ¬Øs2
m = ¬ØœÑm ‚àí(¬Ø¬µm)2."
REFERENCES,0.7556675062972292,"E
PROOF OF THEOREM 3"
REFERENCES,0.7581863979848866,"Proof. According to Theorem 1, 2 and Corollary 1, when both N ‚àóand p are large enough, with
probability 1 there exist (eb, vb), (em, vm) and Œ¥ > 0 such that ||(eb, vb) ‚àí(em, vm)||2 > Œ¥, and"
REFERENCES,0.760705289672544,"||(ei, vi) ‚àí(eb, vb)||2 ‚â§Œ¥"
REFERENCES,0.7632241813602015,"2 for ‚àÄi ‚ààIb
and
||(ei, vi) ‚àí(em, vm)||2 ‚â§Œ¥"
REFERENCES,0.7657430730478589,2 for ‚àÄi ‚ààIm.
REFERENCES,0.7682619647355163,"Therefore, with a reasonable clustering algorithm such as K-mean with K = 2, we would expect
ÀÜIb = Ib with probability 1."
REFERENCES,0.7707808564231738,"Because we can always Ô¨Ånd a ‚àÜ> 0 such that ||Mi,: ‚àíMj,:||2 ‚â§‚àÜfor any node pair (i, j) in a
Ô¨Åxed dataset with a Ô¨Ånite number of nodes, and ÀÜmb,: = mb,: when ÀÜIb = Ib, we have"
REFERENCES,0.7732997481108312,"E|| ÀÜmb,: ‚àímb,:||2 ‚â§‚àÜ¬∑ P(ÀÜIb Ã∏= Ib),
and thus"
REFERENCES,0.7758186397984886,"lim
N ‚àó‚Üí‚àûlim
p‚Üí‚àûE|| ÀÜmb,: ‚àímb,:||2 = 0."
REFERENCES,0.7783375314861462,It completes the proof.
REFERENCES,0.7808564231738035,"F
MANDERA PERFORMANCE WITH DIFFERENT CLUSTERING ALGORITHMS"
REFERENCES,0.783375314861461,"In this section, Figure 6 demonstrate that the discriminating performance of MANDERA when
hierarchical clustering and Gaussian mixture models are used in-place of K-means for FASHION-
MNIST data set remain robust."
REFERENCES,0.7858942065491183,"G
MODEL LOSSES ON CIFAR-10 AND FASHION-MNIST DATA"
REFERENCES,0.7884130982367759,"Figure 7 presents the model loss to accompany the model prediction performance of Figure 5 previ-
ously seen in Section 3."
REFERENCES,0.7909319899244333,"H
MODEL PERFORMANCE ON MNIST DATA"
REFERENCES,0.7934508816120907,"In this section we replicate experiments that were previously performed in Section 3 on the
MNIST (Deng, 2012) dataset. The MNIST dataset is a dataset of 60,000 and 10,000 training and
testing samples respectively divided into 10 classes of handwritten digits from multiple authors.
Figure 8 contains the performance characteristics of MANDERA‚Äôs defense against the four attacks,
whilst Figure 9 contains the comparative accuracy and loss when the different defenses are applied.
Generally speaking, the observations previously observed continue to hold for this dataset."
REFERENCES,0.7959697732997482,Under review as a conference paper at ICLR 2022
REFERENCES,0.7984886649874056,"GA
ZG
SF
LF"
REFERENCES,0.801007556675063,"Accuracy
Recall
Precision
F1"
REFERENCES,0.8035264483627204,"5 1015202530
5 1015202530
5 1015202530
5 1015202530 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.8060453400503779,Number of malicious nodes Score
REFERENCES,0.8085642317380353,"Metric
Accuracy
Recall
Precision
F1"
REFERENCES,0.8110831234256927,(a) Gaussian mixture model.
REFERENCES,0.8136020151133502,"GA
ZG
SF
LF"
REFERENCES,0.8161209068010076,"Accuracy
Recall
Precision
F1"
REFERENCES,0.818639798488665,"5 1015202530
5 1015202530
5 1015202530
5 1015202530 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.8211586901763224,Number of malicious nodes Score
REFERENCES,0.8236775818639799,"Metric
Accuracy
Recall
Precision
F1"
REFERENCES,0.8261964735516373,(b) Hierarchical clustering.
REFERENCES,0.8287153652392947,"Figure 6: ClassiÔ¨Åcation performance of our proposed approach MANDERA (Algorithm 1) with
other clustering algorithms under four types of attack for FASHION-MNIST data. GA: Gaussian
attack; ZG: Zero-gradient attack; SF: Sign-Ô¨Çipping; and LF: Label-Ô¨Çipping. The boxplot bounds the
25th (Q1) and 75th (Q3) percentile, with the central line representing the 50th quantile (median).
The end points of the whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively."
REFERENCES,0.8312342569269522,Under review as a conference paper at ICLR 2022
REFERENCES,0.8337531486146096,"5
10
15
20
25
30"
REFERENCES,0.836272040302267,"GA
ZG
SF
LF"
REFERENCES,0.8387909319899244,0 5 10 15 20 250 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 4 6 8 2.5 3.0 3.5 4.0 4.5 2.6 2.8 3.0 3.2 2.5 2.7 2.9 3.1
REFERENCES,0.8413098236775819,Number of Epoch
REFERENCES,0.8438287153652393,log(Loss)
REFERENCES,0.8463476070528967,Defence Krum
REFERENCES,0.8488664987405542,NO‚àíattack
REFERENCES,0.8513853904282116,Bulyan
REFERENCES,0.853904282115869,Median
REFERENCES,0.8564231738035264,Trim‚àímean
REFERENCES,0.8589420654911839,MANDERA
REFERENCES,0.8614609571788413,FLTrust
REFERENCES,0.8639798488664987,(a) CIFAR-10
REFERENCES,0.8664987405541562,"5
10
15
20
25
30"
REFERENCES,0.8690176322418136,"GA
ZG
SF
LF"
REFERENCES,0.871536523929471,0 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 25 2.5 5.0 7.5
REFERENCES,0.8740554156171285,"1
2
3
4
5
6 1.5 2.0 2.5"
REFERENCES,0.8765743073047859,"1.25
1.50
1.75
2.00
2.25"
REFERENCES,0.8790931989924433,Number of Epoch
REFERENCES,0.8816120906801007,log(Loss)
REFERENCES,0.8841309823677582,Defence Krum
REFERENCES,0.8866498740554156,NO‚àíattack
REFERENCES,0.889168765743073,Bulyan
REFERENCES,0.8916876574307305,Median
REFERENCES,0.8942065491183879,Trim‚àímean
REFERENCES,0.8967254408060453,MANDERA
REFERENCES,0.8992443324937027,FLTrust
REFERENCES,0.9017632241813602,(b) FASHION-MNIST
REFERENCES,0.9042821158690176,"Figure 7: Model Loss at each epoch of training, each line of the curve represents a different defense
against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack; SF: Sign-Ô¨Çipping; and LF:
Label-Ô¨Çipping)."
REFERENCES,0.906801007556675,Under review as a conference paper at ICLR 2022
REFERENCES,0.9093198992443325,"GA
ZG
SF
LF"
REFERENCES,0.9118387909319899,"Accuracy
Recall
Precision
F1"
REFERENCES,0.9143576826196473,"5 1015202530
5 1015202530
5 1015202530
5 1015202530 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.9168765743073047,Number of malicious nodes Score
REFERENCES,0.9193954659949622,"Metric
Accuracy
Recall
Precision
F1"
REFERENCES,0.9219143576826196,"Figure 8: ClassiÔ¨Åcation performance of our proposed approach MANDERA (Algorithm 1) under
four types of attack for MNIST data. GA: Gaussian attack; ZG: Zero-gradient attack; SF: Sign-
Ô¨Çipping; and LF: Label-Ô¨Çipping. The boxplot bounds the 25th (Q1) and 75th (Q3) percentile, with
the central line representing the 50th quantile (median). The end points of the whisker represent the
Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively."
REFERENCES,0.924433249370277,Under review as a conference paper at ICLR 2022
REFERENCES,0.9269521410579346,"5
10
15
20
25
30"
REFERENCES,0.929471032745592,"GA
ZG
SF
LF"
REFERENCES,0.9319899244332494,"0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 25 50 75 100 60 70 80 90 100 60 70 80 90 100 84 88 92 96"
REFERENCES,0.9345088161209067,Number of Epoch
REFERENCES,0.9370277078085643,Accuracy
REFERENCES,0.9395465994962217,Defence Krum
REFERENCES,0.9420654911838791,NO‚àíattack
REFERENCES,0.9445843828715366,Bulyan
REFERENCES,0.947103274559194,Median
REFERENCES,0.9496221662468514,Trim‚àímean
REFERENCES,0.9521410579345088,MANDERA
REFERENCES,0.9546599496221663,FLTrust
REFERENCES,0.9571788413098237,(a) Model prediction accuracy after defense
REFERENCES,0.9596977329974811,"5
10
15
20
25
30"
REFERENCES,0.9622166246851386,"GA
ZG
SF
LF"
REFERENCES,0.964735516372796,"0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0
5 10 15 20 25 0.0 2.5 5.0 7.5"
REFERENCES,0.9672544080604534,"0
1
2
3
4 0 1 2 3 ‚àí0.5"
REFERENCES,0.9697732997481109,"0.0
0.5
1.0
1.5"
REFERENCES,0.9722921914357683,Number of Epoch
REFERENCES,0.9748110831234257,log(Loss)
REFERENCES,0.9773299748110831,Defence Krum
REFERENCES,0.9798488664987406,NO‚àíattack
REFERENCES,0.982367758186398,Bulyan
REFERENCES,0.9848866498740554,Median
REFERENCES,0.9874055415617129,Trim‚àímean
REFERENCES,0.9899244332493703,MANDERA
REFERENCES,0.9924433249370277,FLTrust
REFERENCES,0.9949622166246851,(b) Model prediction loss after defense
REFERENCES,0.9974811083123426,"Figure 9: Model Accuracy and Loss for MNIST data at each epoch of training, each line of the curve
represents a different defense against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack;
SF: Sign-Ô¨Çipping; and LF: Label-Ô¨Çipping)."
