Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024630541871921183,"Vision Language Models (VLMs) excel in zero-shot image classification by pairing
images with textual category names. The expanding variety of Pre-Trained VLMs
enhances the likelihood of identifying a suitable VLM for specific tasks. To
better reuse the VLM resource and fully leverage its potential on different zero-
shot image classification tasks, a promising strategy is selecting appropriate Pre-
Trained VLMs from the VLM Zoo, relying solely on the text data of the target
dataset without access to the dataset’s images. In this paper, we analyze two
inherent challenges in assessing the ability of a VLM in this Language-Only VLM
selection: the “Modality Gap”—the disparity in VLM’s embeddings across two
different modalities, making text a less reliable substitute for images; and the
“Capability Gap”— the discrepancy between the VLM’s overall ranking and its
ranking for target dataset, hindering direct prediction of a model’s dataset-specific
performance from its general performance. We propose VLM Selection With
gAp Bridging (SWAB) to mitigate the negative impact of two gaps. SWAB first
adopts optimal transport to capture the relevance between open-source and target
datasets with a transportation matrix. It then uses this matrix to transfer useful
statistics of VLMs from open-source datasets to the target dataset for bridging two
gaps. By bridging two gaps to obtain better substitutes for test images, SWAB can
accurately predict the performance ranking of different VLMs on the target task
without the need for the dataset’s images. Experiments across various VLMs and
image classification datasets validate SWAB’s effectiveness. Code is available at:
https://github.com/YCaigogogo/SWAB."
INTRODUCTION,0.0049261083743842365,"1
Introduction"
INTRODUCTION,0.007389162561576354,"Vision-Language Models (VLMs) [46, 22, 48, 68] have demonstrated impressive image-text matching
ability. One notable application of VLMs is zero-shot image classification [46, 40, 14, 37], where
VLMs are leveraged to generate image classifiers using only class names directly. This zero-shot
approach has shown considerable success in scenarios with scarce or no training images [35, 18]."
INTRODUCTION,0.009852216748768473,"Despite the success of VLM in image classification, the performance of a VLM may vary substantially
according to the datasets and domains [11], making it challenging to use a single model to handle
all tasks. Fortunately, many open-source VLMs are available [21], and these VLMs form a vast
VLM Zoo. With different architectures, pre-training datasets, or training methods, these VLMs have
different strengths. The diverse pre-trained VLMs increase the likelihood of pinpointing at least one
VLM that excels in a given target dataset in most cases.1 To more effectively reuse the VLM Zoo
across diverse target tasks and unlock its full potential, we need a model selection method to choose
suitable VLMs from the VLM Zoo for the target task. However, in scenarios such as zero-shot image"
INTRODUCTION,0.012315270935960592,"1Throughout this paper, the term “VLM” specifically refers to a pre-trained VLM."
INTRODUCTION,0.014778325123152709,Model Zoo
INTRODUCTION,0.017241379310344827,"Language-Only 
Target Dataset’s Data VLM 1 VLM 2 VLM 3 0.76 0.84 0.51"
INTRODUCTION,0.019704433497536946,Abs / Rel Perf
INTRODUCTION,0.022167487684729065,Selected Model
INTRODUCTION,0.024630541871921183,Model Selection
INTRODUCTION,0.027093596059113302,Algorithm
INTRODUCTION,0.029556650246305417,Open-Source Datasets’ Data T V T V T V “Dog”
INTRODUCTION,0.03201970443349754,“A Dog is running”
INTRODUCTION,0.034482758620689655,“Lion”
INTRODUCTION,0.03694581280788178,“A Lion is sleeping”
INTRODUCTION,0.03940886699507389,Image Data
INTRODUCTION,0.04187192118226601,Text Data
INTRODUCTION,0.04433497536945813,"Step 1: User Gives Task Description
Step 3: Use Model Selection Algorithm to Select Model"
INTRODUCTION,0.046798029556650245,"I want a VLM to help me classify {Cat, Dog} 
classes in {Natural Image} domain."
INTRODUCTION,0.04926108374384237,Model Search Engine
INTRODUCTION,0.05172413793103448,Step 2: Use LLM to Generate Auxiliary Texts
INTRODUCTION,0.054187192118226604,"“Generate 50 captions for {Cat, Dog} separately 
for the {Natural Image} domain.” Input"
INTRODUCTION,0.05665024630541872,“A cat is catching a mouse”
INTRODUCTION,0.059113300492610835,“A dog is barking at a stranger”
INTRODUCTION,0.06157635467980296,Class-Related Auxiliary Texts
INTRODUCTION,0.06403940886699508,"Figure 1: Paradigm of Language-Only VLM Selection (LOVM). Users describe the details of
their target tasks in text form, such as class names and image domains. Then, LOVM utilizes this
information to generate class-related labeled texts through ChatGPT. These texts serve as substitutes
for image samples in subsequent model selection algorithms. The model selection algorithm uses
two types of data, including the open-source datasets (which have image and text data) and the text
data from the target dataset, to predict the VLM’s absolute or relative performance on a target dataset.
It then selects the most appropriate VLM based on the predicted performance."
INTRODUCTION,0.0665024630541872,"classification, many users might not have labeled images for their target tasks, especially those who
are not Machine Learning researchers. They prefer to describe their needs in text and use a Model
Search Engine to find the most suitable model. So one solution is identifying the most suitable VLMs
in the zoo for a target dataset without access to the dataset’s images. This VLM selection is termed
as “Language-Only VLM Selection” (LOVM) [73], and the paradigm is illustrated in Figure 1."
INTRODUCTION,0.06896551724137931,"Two key types of information are available for LOVM. One is the target dataset’s text data, i.e.,
names of the target classes and class-related labeled texts generated by LLMs (Details described
in Section B.1). The other is the open-source datasets, collected in the form of images with their
corresponding class names. Based on these data, the goal is to estimate a VLM’s zero-shot image
classification capability ranking among the VLM zoo on the target dataset. LOVM encounters two
challenges stemming from the inherent heterogeneity in models and datasets. The first challenge is
the Modality Gap across different modal features extracted by a VLM. Since the visual and textual
features extracted by VLMs tend to cluster into two distinct groups and have gap vectors between
them [31], using text data as image proxies to rank VLMs is inaccurate. The second challenge is the
Capability Gap between the VLM’s overall ranking and its ranking in the specific dataset. Owing
to the VLM’s performance variation across different datasets, the VLM’s average performance on
open-source datasets is hard to reflect its performance on a specific target dataset. Thus, selecting a
VLM based solely on its general strength may prove to be a less effective strategy."
INTRODUCTION,0.07142857142857142,"In this paper, we propose VLM Selection With gAp Bridging (SWAB) to address both gaps. The
key idea is to reuse VLMs’ statistics from open-source datasets to estimate their statistics on the
target dataset, which mitigates the negative impact of these two gaps. In particular, SWAB first uses
optimal transport to calculate the transport matrix based on textual similarity between class names of
open-source and target datasets. After applying VLMs on open-source datasets to calculate VLMs’
statistics, i.e., the class-specific modality gap vectors and performance rankings of different VLMs,
SWAB utilizes these statistics to estimate the same type of statistics on the target dataset. After that,
SWAB uses the estimated gap vectors to align the features of texts with the features of images from
the corresponding category, which bridges the modality gap. Meanwhile, SWAB’s estimated VLMs’
ranking also improves the prediction of their rankings on the target task, bridging the capability gap.
The related work is in the appendix C. The main contributions are:"
INTRODUCTION,0.07389162561576355,"• We analyze two key challenges in LOVM — the modality gap across VLM’s modal features and
the capability gap between the VLM’s overall ranking and its ranking on the target dataset.
• We propose SWAB, which utilizes optimal transport to transform useful statistics of VLMs on
open-source datasets to the target dataset to bridge two gaps.
• Experimental results on a LOVM benchmark composed of a wide range of VLMs and image
classification datasets demonstrate the effectiveness of SWAB."
PRELIMINARY,0.07635467980295567,"2
Preliminary"
PRELIMINARY,0.07881773399014778,"We formally introduce the LOVM setting, a baseline method for LOVM, and analyze the two kinds
of gaps in LOVM. We use ∥· ∥to represent the Euclidean norm of a vector unless otherwise defined."
SELECTING VLMS FROM A MODEL ZOO,0.0812807881773399,"2.1
Selecting VLMs from a Model Zoo"
SELECTING VLMS FROM A MODEL ZOO,0.08374384236453201,"Zero-Shot Image Classification of VLM. Assume there is a pre-trained VLM f = (f I, f T )
consisting of an image encoder f I and a text encoder f T . Given an image classification dataset T
with kT class names CT = {cT
1 , · · · , cT
kT }, we input the class names CT (probably with templates
like “A photo of {class}”) into the VLM’s text encoder f T to get the image classifiers {ˆtj}kT
j=1. Then,
given a test image xi, we use the image encoder f I to extract its feature ˆxi. Finally, we predict
the label via the cosine similarity between the image feature ˆxi and image classifiers {ˆtj}kT
j=1. The
class with the highest cosine similarity to the image is selected as the predicted class ˆyi. Given
ˆxi = f I(xi), ˆtj = f T (cT
j ), Equation 1 describes this zero-shot image classification process:"
SELECTING VLMS FROM A MODEL ZOO,0.08620689655172414,"ˆyi = f(xi, CT ) = argmax
cT
j ∈[CT ]"
SELECTING VLMS FROM A MODEL ZOO,0.08866995073891626,"ˆx⊤
i ˆtj
∥ˆxi∥· ∥ˆtj∥.
(1)"
SELECTING VLMS FROM A MODEL ZOO,0.09113300492610837,"VLM Zoo. In recent years, a large number of (pre-trained) VLMs have emerged. Assume a
collection of M VLMs constitute a VLM Zoo M =

fm =
 
f I
m, f T
m
	M
m=1. The capability of fm
is determined by three key factors: the model architecture (e.g., Transformer [57], ConvNeXt [34]),
the pre-trained dataset (e.g., LAION-400M [47], MS-COCO [32]), and the training method (e.g.,
contrastive loss [46], caption loss [67]). Combinations of these factors result in “good and diverse”
VLMs in M. Given a dataset T , it is probable to find a suitable VLM from the VLM zoo with high
zero-shot image classification performance on T ."
SELECTING VLMS FROM A MODEL ZOO,0.09359605911330049,"Language-Only VLM Selection (LOVM). Rather than using images from the target dataset, LOVM
focuses on the zero-shot scenario where only the target dataset’s text data, such as its class names
CT , are available for VLM selection. Besides, we can obtain some open-source image classifi-
cation datasets S. The set of class names in S is CS = {cS
1 , · · · , cS
kS}, and the DI
S denote the
labelled images in these classes. Given a target task T , the VLM selection method h estimates the
zero-shot classification ability of fm based on CT , CS, and DI
S via ˆrT
m = h(fm | CT , CS, DI
S),
where m ∈[1, · · · , M]. ˆrT
m is the predicted ranking of the m-th VLM fm on T . The higher
the ranking, the more probable fm achieves higher zero-shot image classification performance
on T . Assuming we can obtain the test image set DI
T of the target dataset T with |DI
T | im-
ages, then we can calculate the zero-shot image classification accuracy pT
m of fm is calculated by
pT
m =
1
|DI
T |
P"
SELECTING VLMS FROM A MODEL ZOO,0.0960591133004926,"(xi,yi)∈DI
T
I (yi = fm (xi, CT )). fm (xi, CT ) represents the predicted class with the"
SELECTING VLMS FROM A MODEL ZOO,0.09852216748768473,"same manner as Equation 1. I(·) is the indicator function, which outputs 1 if the condition is satisfied,
and 0 otherwise. Based on {pT
m}M
m=1, we obtain the true ranking of M VLMs rT = [rT
1 , . . . , rT
M]
by assigning higher ranking r to models with higher accuracy p. However, in the zero-shot sce-
nario, we can’t obtain the test images set DI
T in advance. Therefore, the goal of LOVM is to make
the predicted ranking ˆrT = [ˆrT
1 , · · · , ˆrT
M] be an accurate estimation of the ground truth ranking
rT = [rT
1 , . . . , rT
M] so that the best VLM can be selected."
SELECTING VLMS FROM A MODEL ZOO,0.10098522167487685,"Evaluation of LOVM Methods. We measure the performance of the LOVM algorithm by comparing
the ranking similarity between rT and ˆrT . Specifically, we calculate the Top-5 Recall R5 (ranges
from 0 to 1) and Kendall’s Rank Correlation τ (ranges from -1 to 1). The larger the better."
POSSIBLE PARADIGMS FOR LOVM,0.10344827586206896,"2.2
Possible Paradigms for LOVM"
POSSIBLE PARADIGMS FOR LOVM,0.10591133004926108,"Non-Learning-based LOVM. There are three main paradigms for LOVM. The first paradigm is to
neglect the visual encoder and select VLM solely on texts. In detail, we can utilize ChatGPT [43] to
generate auxiliary texts ˜DT based on class names CT of T . More details are described in Section
B.1. These class-specific texts act as “image proxies”. Then, whether a VLM fm fits T could be
measured by transferability metrics, e.g., H-Score [2] and LogME [65], between the VLM’s text
encoder f T
m and generated text dataset ˜DT . The second paradigm relies on the general performance"
POSSIBLE PARADIGMS FOR LOVM,0.10837438423645321,"of a certain VLM fm. We use open-source datasets to measure a VLM’s general performance. If fm
achieves high zero-shot classification performance over open-source datasets, then it is expected to be
competitive on T . These methods assume that a VLM’s ranking is relatively consistent across tasks."
POSSIBLE PARADIGMS FOR LOVM,0.11083743842364532,"Learning-based LOVM. The third paradigm is based on the learning process. In detail, the ability of
a VLM could be predicted based on a ranker model fR. The input of fR is a vector sT
m, depicting the
dataset-specific representation of fm on T , while the output of fR is the relative/absolute performance
ˆpT
m ∈R of fm on T . The fR could be learned on open-source datasets S [70, 73]. Due to the
availability of both class names CS and images DI
S in the open-source dataset S such as ImageNet [8],
we can calculate each VLM’s representation {sn
m}M,N
m=1,n=1 and true zero-shot image classification
accuracy {pn
m}M,N
m=1,n=1. Here N refers to the number of datasets in S. After constructing the train
set, the ranker model fR is learned based on the {sn
m, pn
m}M,N
m=1,n=1:"
POSSIBLE PARADIGMS FOR LOVM,0.11330049261083744,"min
fR M
X m=1 N
X"
POSSIBLE PARADIGMS FOR LOVM,0.11576354679802955,"n=1
ℓ(fR(sn
m), pn
m).
(2)"
POSSIBLE PARADIGMS FOR LOVM,0.11822660098522167,"ℓis a loss function that measures the discrepancy between the prediction and the ground truth,
which can be Mean Squared Error Loss and Huber Loss, among others. Given T , the learned fR
is able to predict the performance {ˆpT
m}M
m=1 over {sT
m}M
m=1 via ˆpT
m = fR(sT
m). Finally, we can
get the predicted VLMs’ ranking ˆr based on {ˆpT
m}M
m=1. This approach has similarities with meta-
learning [12, 3]. Meta-learning attempts to use data from multiple datasets to learn a model adaptable
to the target task, while Learning-based LOVM employs data from multiple datasets to learn a ranker
model for selecting the suitable model from a VLM Zoo for the target task. The representation sT
m is
one of the keys in this paradigm, and ModelGPT [73] calculates values sT
m via the capability of a
VLM’s text encoder f T
m."
POSSIBLE PARADIGMS FOR LOVM,0.1206896551724138,"ModelGPT uses generated text data ˜DT as substitutes for images to calculate some metrics, which
measures the zero-shot ability of fm on unseen images by the classification ability of fm on ˜DT :"
POSSIBLE PARADIGMS FOR LOVM,0.12315270935960591,"sT
m,i = Metrici

fm, ˜DT

.
(3)"
POSSIBLE PARADIGMS FOR LOVM,0.12561576354679804,"Here Metrici indicates the i-th metrics function such as Top-1 Accuracy and F1-Score. For example,
the Top-1 Accuracy sT
m,1 could be calculated in a similar manner as Equation 1, with the only
difference being that the test samples were replaced with text samples ti instead of image samples xi:"
POSSIBLE PARADIGMS FOR LOVM,0.12807881773399016,"sT
m,1 =
1
| ˜DT | X"
POSSIBLE PARADIGMS FOR LOVM,0.13054187192118227,"(ti,yi)∈˜
DT
I (yi = fm(ti, CT )) .
(4)"
POSSIBLE PARADIGMS FOR LOVM,0.1330049261083744,"Besides, ModelGPT uses some metrics for assessing the features’ quality extracted by the VLM’s text
encoder f T
m. More details are in the Section B.2. Moreover, the zero-shot classification performance
of fm on ImageNet is also included in sT
m as a general ability measure of fm. ModelGPT implements
fR as a simple linear model."
ANALYSIS OF THE TWO GAPS IN LOVM,0.1354679802955665,"2.3
Analysis of the Two Gaps in LOVM"
ANALYSIS OF THE TWO GAPS IN LOVM,0.13793103448275862,"There are two main challenges that limit the application of the aforementioned paradigms in LOVM.
The first is the modality gap across different modalities’ features in VLM’s feature space, and the
second is the capability gap between VLM’s overall performance and dataset-specific performance.
Modality Gap. As described in Section 2.2, methods like H-Score, LogME, and ModelGPT utilize
the ChatGPT generated auxiliary texts ˜DT as image proxies to calculate metrics that measure the zero-
shot accuracy on the target dataset T . In other words, the zero-shot classification ability across text
and image modalities is estimated by the intra-modality classification ability. The latent assumption
is that the generated texts and their corresponding images are closely aligned in VLM’s feature space.
However, this assumption is difficult to meet [31], and instances’ features are more likely to cluster
according to their modalities. In particular, we define the modality gap vector g between the features
of an image-text pair (xi, ti) as gm,i := f I
m(xi) −f T
m(ti). Values in the gap vector are generally not
close to zero. We name this phenomenon as Modality Gap in LOVM, which makes the scores on ˜DT
hard to reveal the true zero-shot image classification capability of a VLM on a given dataset."
ANALYSIS OF THE TWO GAPS IN LOVM,0.14039408866995073,�= �. ��
ANALYSIS OF THE TWO GAPS IN LOVM,0.14285714285714285,���= �. ��
ANALYSIS OF THE TWO GAPS IN LOVM,0.14532019704433496,"(a) Modality Gap
(b) Capability Gap"
ANALYSIS OF THE TWO GAPS IN LOVM,0.1477832512315271,Model Zoo
ANALYSIS OF THE TWO GAPS IN LOVM,0.15024630541871922,(�= 43) VLM 1 VLM 2
ANALYSIS OF THE TWO GAPS IN LOVM,0.15270935960591134,.  .  . VLM M
ANALYSIS OF THE TWO GAPS IN LOVM,0.15517241379310345,Task Zoo
ANALYSIS OF THE TWO GAPS IN LOVM,0.15763546798029557,(�= 23)
ANALYSIS OF THE TWO GAPS IN LOVM,0.16009852216748768,ImageNet
ANALYSIS OF THE TWO GAPS IN LOVM,0.1625615763546798,.  .  .
ANALYSIS OF THE TWO GAPS IN LOVM,0.16502463054187191,Oxford Pets Test On
ANALYSIS OF THE TWO GAPS IN LOVM,0.16748768472906403,2 5 .. 9
ANALYSIS OF THE TWO GAPS IN LOVM,0.16995073891625614,1 7 .. 4
ANALYSIS OF THE TWO GAPS IN LOVM,0.1724137931034483,.  .  .  .
ANALYSIS OF THE TWO GAPS IN LOVM,0.1748768472906404,4 8 .. 3
ANALYSIS OF THE TWO GAPS IN LOVM,0.17733990147783252,Model Rank
ANALYSIS OF THE TWO GAPS IN LOVM,0.17980295566502463,�1 ∈��
ANALYSIS OF THE TWO GAPS IN LOVM,0.18226600985221675,�2 ∈��
ANALYSIS OF THE TWO GAPS IN LOVM,0.18472906403940886,�M ∈�� �=1
ANALYSIS OF THE TWO GAPS IN LOVM,0.18719211822660098,M std(��)
ANALYSIS OF THE TWO GAPS IN LOVM,0.1896551724137931,"�
= �. �� �=1"
ANALYSIS OF THE TWO GAPS IN LOVM,0.1921182266009852,M (max(��) −min(��))
ANALYSIS OF THE TWO GAPS IN LOVM,0.19458128078817735,"�
= ��. �� T V T V T V VLM"
ANALYSIS OF THE TWO GAPS IN LOVM,0.19704433497536947,"Acc
Acc"
ANALYSIS OF THE TWO GAPS IN LOVM,0.19950738916256158,"Task 1
Task 2 VLM 1 2
3
3 2 1"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2019704433497537,Text Samples
ANALYSIS OF THE TWO GAPS IN LOVM,0.2044334975369458,"Test Image Samples
Calculate"
ANALYSIS OF THE TWO GAPS IN LOVM,0.20689655172413793,Test Acc
ANALYSIS OF THE TWO GAPS IN LOVM,0.20935960591133004,"Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs’
zero-shot image classification accuracy based on generated text data vs. VLM’s true accuracy based
on test images. Each point in the graph represents a model. From the result, we can find that the
predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective
substitutes for image data. (b) We calculate the zero-shot image classification performance rankings
of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of
differences between each VLM’s maximum and minimum ranking. The result shows the performance
of a VLM varies greatly across different datasets."
ANALYSIS OF THE TWO GAPS IN LOVM,0.21182266009852216,"We conduct a validation experiment on ImageNet with 43 VLMs. We first generate 50 auxiliary texts
per class as ˜DT and then calculate the predicted Top-1 accuracy via Equation 4. Next, we use test
images to calculate the VLM’s true Top-1 accuracy. The consistency between the predicted Top-1
accuracy and true zero-shot image classification accuracy pm,T is measured by the Kendall Rank
Correlation (τ, higher is better) and Mean Absolute Error (MAE, lower is better). It can be observed
from the left part of Figure 2 that the predicted accuracy derived from auxiliary texts ˜DT does not
closely match the true accuracy, indicating that these generated auxiliary texts in ˜DT are not effective
proxies for images."
ANALYSIS OF THE TWO GAPS IN LOVM,0.21428571428571427,"To make the auxiliary texts act as better image proxies, one intuitive idea is to estimate the gap
vector g for each image-text pair. Then we can add it to the feature f T
m(ti) of the text ti to eliminate
the modality gap, which may lead to more accurate scores sT
m,i in Equation 3. However, the gap
vector cannot be calculated directly without the target dataset’s images. Furthermore, gap vectors for
different classes are diverse, so using a shared vector across all datasets may not be a good choice."
ANALYSIS OF THE TWO GAPS IN LOVM,0.21674876847290642,"Capability Gap. To select one VLM from the model zoo given a target dataset, one direct approach
is to select the VLM that performs the best on average across multiple datasets. For example, we
may first estimate the VLM’s zero-shot classification ability on open-source datasets and then select
the VLM with the highest performance. The key question is whether a VLM’s average ranking on
the open-source datasets can reveal its true ranking on the target dataset. Our empirical analyses
indicate that there exists a discrepancy between the VLM’s overall ranking and its ranking on a
specific dataset. We name the discrepancy between the VLM’s average ability and its specific ability
as the Capability Gap, which results from the fact that a VLM’s performance fluctuates significantly
across various datasets."
ANALYSIS OF THE TWO GAPS IN LOVM,0.21921182266009853,"To verify the claim, we test 43 VLMs on 23 target datasets provided by [73] and obtain the rankings
of each VLM across these datasets. Based on these ranking results, we calculate the average standard
deviation and the mean value of the difference between each VLM’s maximum and minimum ranking.
The experiment process is illustrated in the right part of Figure 2. We find that the mean difference
between one VLM’s maximum and the minimum ranking is 38.86. Since the total number of VLMs
is 43, such a difference demonstrates that the top-performing VLM in one dataset could likely be
among the worst in another."
ANALYSIS OF THE TWO GAPS IN LOVM,0.22167487684729065,"One solution to bridge such a capability gap is to consider the VLMs’ ranking on a related dataset. In
other words, the ranking of VLMs on datasets from open-source datasets collections that are relevant
to the target task may provide more useful insights than a general performance ranking across all
tasks. The main challenge is to figure out which open-source datasets are similar to the target dataset
and transform the VLM’s ranking on these datasets to the target dataset."
ANALYSIS OF THE TWO GAPS IN LOVM,0.22413793103448276,"Optimal 
Transport
�"
ANALYSIS OF THE TWO GAPS IN LOVM,0.22660098522167488,"Text Encoder
�� ��"
ANALYSIS OF THE TWO GAPS IN LOVM,0.229064039408867,Step 1: Construct the Transport Matrix
ANALYSIS OF THE TWO GAPS IN LOVM,0.2315270935960591,"Open-Source 
Tasks’ Classes"
ANALYSIS OF THE TWO GAPS IN LOVM,0.23399014778325122,"Target
Task’s Classes"
ANALYSIS OF THE TWO GAPS IN LOVM,0.23645320197044334,"�∗
Transport Matrix"
ANALYSIS OF THE TWO GAPS IN LOVM,0.23891625615763548,"Step 2: Bridge the Modality Gap ��
�= ��,1"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2413793103448276,"�
 
⋮
��,��"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2438423645320197,"�
 
(�∗)���
�
��,1 � ��,2 �"
ANALYSIS OF THE TWO GAPS IN LOVM,0.24630541871921183,"Rank
Model 
`"
ANALYSIS OF THE TWO GAPS IN LOVM,0.24876847290640394,"Text Samples
Open-Source
Task’s Gap Vectors"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2512315270935961,Target Task’s
ANALYSIS OF THE TWO GAPS IN LOVM,0.2536945812807882,Gap Vectors
ANALYSIS OF THE TWO GAPS IN LOVM,0.2561576354679803,Modified Text Samples Add Gap
ANALYSIS OF THE TWO GAPS IN LOVM,0.25862068965517243,"Step 3: Bridge the Capability Gap ��
�= ��,1"
ANALYSIS OF THE TWO GAPS IN LOVM,0.26108374384236455,"�
 
⋮
��,��"
ANALYSIS OF THE TWO GAPS IN LOVM,0.26354679802955666,"�
 
��
�= ��,1 �"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2660098522167488,"⋮
��,��"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2684729064039409,"�
 
�� �,(2)"
ANALYSIS OF THE TWO GAPS IN LOVM,0.270935960591133,"Open-Source Tasks’ 
Class-Specific Rank"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2733990147783251,"Target Task’s 
Class-Specific Rank �=1"
ANALYSIS OF THE TWO GAPS IN LOVM,0.27586206896551724,"����,� � ��"
ANALYSIS OF THE TWO GAPS IN LOVM,0.27832512315270935,Step 4: Ensemble Predicted Rankings
ANALYSIS OF THE TWO GAPS IN LOVM,0.28078817733990147,"Weighted
Borda Count
�� �,���"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2832512315270936,"VLM’s Final 
Rank Prediction"
ANALYSIS OF THE TWO GAPS IN LOVM,0.2857142857142857,VLM’s text embedding cluster
ANALYSIS OF THE TWO GAPS IN LOVM,0.2881773399014778,"VLM’s image embedding cluster ��,1 � ��,2 � �� �,(1) �� �,(1) �� �,(2)"
ANALYSIS OF THE TWO GAPS IN LOVM,0.29064039408866993,"(�∗)���
� ..."
ANALYSIS OF THE TWO GAPS IN LOVM,0.29310344827586204,"Figure 3: The workflow of SWAB. SWAB first constructs a transport matrix γ∗∈RkS×kT using
optimal transport, based on textual semantic similarity between classes in the open-source datasets
CS = {cS
1 , · · · , cS
kS} and the target dataset’s classes CT = {cT
1 , · · · , cT
kT }. Using this matrix, SWAB
estimates VLM fm’s class-specific gap vectors {ˆgT
m,1, · · · } on the target dataset T from the gap
vectors GS
m ∈RkS×d in the open-source datasets. These estimated gap vectors help modify text data
to act as more effective substitutes for image data. The modified text data will then be input into the
Ranker Model fR, which predicts VLM’s performance ˆrT ,(1)
m
on the target dataset. Besides, SWAB
also uses the transport matrix γ∗to predict VLM’s performance ranking on the target dataset based
on VLM’s class-specific rankings rS
m ∈RkS on open-source datasets. Finally, SWAB combines these
two ranking predictions ˆrT ,(1)
m
and ˆrT ,(2)
m
to determine the VLM’s final ranking prediction."
ANALYSIS OF THE TWO GAPS IN LOVM,0.2955665024630542,"Summary. We emphasize two kinds of gaps in LOVM, i.e., the modality gap across features of
different modalities generated by a VLM, and the capability gap between a VLM’s overall ranking
and its ranking given a specific target dataset. Both two gaps pose obstacles to previous model
selection methods, such as LogME and ModelGPT, and degrade their abilities in VLM selection.
Moreover, those intuitive approaches to bridge the gaps still face challenges."
VLM SELECTION WITH GAP BRIDGING,0.29802955665024633,"3
VLM Selection with Gap Bridging"
VLM SELECTION WITH GAP BRIDGING,0.30049261083743845,"To mitigate the impact of both gaps on LOVM and integrate non-learning-based and learning-based
LOVM methods, we propose VLM Selection With gAp Bridging (SWAB). The key idea of SWAB is
to bridge modality and capability gaps by utilizing class-level statistics of VLMs from open-source
datasets. By measuring the textual similarity between the target dataset’s class names and those in
open-source datasets, we construct a bridge matrix. Based on it, we estimate the gap vectors between
image and text modalities, which rectifies the text-derived scores in ModelGPT. In addition, we
predict the VLM’s performance ranking for the target dataset based on the bridge matrix and VLM’s
ranking on the open-source dataset. Both estimated statistics will be used to obtain a more accurate
language-only VLM selection. The workflow of SWAB is illustrated in Figure 3."
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.30295566502463056,"3.1
Construct the Bridge Matrix Using Optimal Transport"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3054187192118227,"Benefiting from the open-source datasets, some useful class-level statistics, such as modality gap
vectors and zero-shot classification accuracy of a certain VLM, could be calculated, which can help
the performance ranking estimation of a VLM on the target dataset. To better utilize these class-level
statistics for predicting the corresponding statistics of a VLM on the target task, we introduce semantic
relevance information between open-source datasets’ classes and target dataset’s classes into the
statistics reusing process, which is automatically generated through Optimal Transport [7, 45]."
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3078817733990148,"Recall that the sets of class names of the open-source datasets and the target dataset are CS = {cS
i }kS
i=1
and CT = {cT
i }kT
i=1, respectively. The semantic relevance between two classes could be measured
by the textual similarity between their class names. In detail, we use a pre-trained text encoder
ϕ (e.g., MPNet [49]), which extracts text features for these class names, i.e., {ϕ(cS
1 ), · · · , ϕ(cS
kS)}"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3103448275862069,"and {ϕ(cT
1 ), · · · , ϕ(cT
kT )}. Then, we can calculate the cosine similarity
ϕ(cS
i )⊤ϕ(cT
j )
∥ϕ(cS
i )∥·∥ϕ(cT
j )∥between"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.312807881773399,"the text feature of the i-th class in the open-source datasets ϕ(cS
i ) and that of the j-th class in the
target dataset ϕ(cT
j ). The larger the cosine similarity, the more similar the two classes are. After that,"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.31527093596059114,"we construct the cost matrix in optimal transport via costij = 1 −
ϕ(cS
i )⊤ϕ(cT
j )
∥ϕ(cS
i )∥·∥ϕ(cT
j )∥. In practice, we"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.31773399014778325,"exponentiate each element in cost ∈RkS×kT
≥0
using the base e to amplify its differences. We then
solve the optimal transport problem with the constructed cost matrix to get the transport matrix γ∗.
Since optimal transport aims to obtain a transport matrix that minimizes the transmission cost, the
transport matrix γ∗will reuse more information between semantically similar classes:"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.32019704433497537,γ∗= argmin
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3226600985221675,"γ∈R
kS ×kT
≥0 X"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3251231527093596,"i,j
γi,j costi,j , s.t. γ1 = u; γT 1 = v; γi,j ≥0.
(5)"
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.3275862068965517,"The cost matrix quantifies the expense of moving elements between all class pairs, and γ∗∈RkS×kT
is the transport matrix. OT minimizes the cost indicated by the matrix cost and moves elements
from one distribution u to another v. In SWAB, we define u and v as uniformly distributed vector
u = 1/kS ∈RkS and v = 1/kT ∈RkT . This indicates that we treat all classes as equally important.
We may also incorporate prior knowledge of class importance to define u and v."
CONSTRUCT THE BRIDGE MATRIX USING OPTIMAL TRANSPORT,0.33004926108374383,"The solution γ∗of the OT problem in Equation 5 could be solved efficiently [13], and γ∗acts as a
bridge matrix between open-source datasets’ classes and target dataset’s classes. Usually, the smaller
costi,j is, the larger the corresponding element γ∗
i,j obtained by OT, indicating statistics of the i-th
class of open-source datasets may help more when we estimate the statistics of the j-th target class."
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.33251231527093594,"3.2
Bridge the Modality Gap and Capability Gap"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.33497536945812806,"Bridge the Modality Gap. Given the m-th VLM fm in the model zoo, we want to estimate the
modality gap gT
m,j between the extracted image and text features for the j-th class in the target
dataset T to bridge the modality gap. However, in the zero-shot scenario, we can’t get the target
dataset’s images in advance, so we can’t directly calculate the gap vectors using image-text pairs.
To solve this problem, SWAB estimates the target dataset’s gap vectors based on the open-source
datasets’ gap vectors with γ∗. Given the k-th open-source class cS
k , we can get the set of images
DI
Sk =

(xi, yi) | (xi, yi) ∈DI
S, yi = cS
k
	
from the open-source datasets. |DI
Sk| is the number of
images in DI
Sk. Then, the modality gap vector gS
m,k for class cS
k and model fm can be calculated"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3374384236453202,"through gS
m,k =
1
|DI
Sk |
P"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3399014778325123,"(xi,yi)∈DI
Sk"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.34236453201970446,"
f I
m(xi)
∥f Im(xi)∥−
f T
m(cS
k)
∥f T
m(cS
k)∥"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3448275862068966,"
. gS
m,k is the average difference between"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3472906403940887,"the normalized class text prototype embedding and all normalized image embeddings from the
class cS
k . In a similar manner, the gap vectors of all open-source classes {gS
m,1, · · · , gS
m,kS} can be
obtained given fm. We use a matrix GS
m ∈RkS×dm to represent those kS gap vectors for the m-th
VLM in the VLM Zoo, and dm is the dimensionality of features extracted by fm."
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3497536945812808,"The gap vectors {gT
m,1, · · · , gT
m,kT } for the target dataset could be estimated based on the GS
m and
the transport matrix γ∗. If two classes are semantically similar, then we can reuse the gap vector from
the similar class. We set the predicted gap vector for the j-th target class ˆgT
m,j as a weighted sum of
GS
m, and the weight comes from γ∗, which is ˆgT
m,j = |CT |(γ∗
:,j)⊤GS
m . γ∗
:,j is the j-th column of γ∗.
We use scaling factors |CT | to ensure that for each target class, the sum of γ∗
:,j equals 1. This scale
operation has also been used in previous work [62, 63]. After that, we modify the step of ModelGPT
in Equation 3, where the metrics over the generated auxiliary texts ˜DT are calculated. We add the
gap vector ˆgT
m,j to the embeddings of the auxiliary texts ˜Dj
T from the j-th class in the target dataset:"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3522167487684729,"˜tm,i = f T
m(ti) + ˆgT
m,j , ∀ti ∈˜Dj
T .
(6)"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.35467980295566504,"The modified text embedding ˜tm,i serves as better image proxies. In other words, classification
metrics on f T
m(ti) only reveal the discerning ability of the text encoder of fm, which is far from
the (cross-modal) zero-shot classification ability due to the modality gap. By bridging such a
gap with modified text embedding, classification metrics on ˜tm,i are closer to the classification
metrics on images with textual classifier. Therefore, we use {˜tm,1, · · · } in Equation 6 as better"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.35714285714285715,"inputs to calculate sT
m in Equation 3. The updated score vectors sT
m are then input to the ranker
model fR, which is able to get more accurate VLM’s performance prediction ˆpT
m. Based on the
performance prediction {ˆpT
m}M
m=1, we can obtain the VLMs’ ranking ˆrT ,(1) = [ˆrT ,(1)
1
, · · · , ˆrT ,(1)
M
] =
Ranking
 
ˆpT
1 , · · · , pT
M

. Ranking(·) transforms the accuracies into models’ ranking."
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.35960591133004927,"Bridge the Capability Gap. Whether the m-th VLM fm fits the target task T could also be estimated
by the performance of fm on the open-source datasets related to T . We first calculate the VLM’s
class-level performance ranking over the whole open-source datasets. Given the k-th open-source
class cS
k and the corresponding set of images DI
Sk, we calculate the zero-shot classification accuracy
pS
m,k via Equation 1. We then determine each VLM’s ranking on the k-th open-source class cS
k using
Equation 7. We calculate VLMs’ ranking for other open-source classes in the same way."
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3620689655172414,"rS
k = [rS
1,k, · · · , rS
M,k] = Ranking
 
pS
1,k, · · · , pS
M,k

.
(7)"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3645320197044335,"Next, we estimate the ranking of a certain VLM fm on the target dataset T . By re-organizing the
ranking values in Equation 7, the performance ranking vector of fm on all kS open-source classes
is rS
m = [rS
m,1, · · · , rS
m,kS] ∈ZkS
+ . If fm ranks high on certain open-source classes related to the
classes in the target dataset, the performance ranking of fm on the target dataset may also be high.
Thus, we perform a weighted sum of ranking values in rS
m and assign larger weights to those classes
related to the target dataset. This can be done by using γ∗:"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3669950738916256,"ˆrT
m = rS
m γ∗.
(8)"
BRIDGE THE MODALITY GAP AND CAPABILITY GAP,0.3694581280788177,"Elements in ˆrT
m ∈RkT are the predicted ranking of the m-th VLM fm for kT target classes. Since
we only need the relative order of ranks, there is no additional scale factor in Equation 8. After
that, we average class-specific ranking values in ˆrT
m, and use ˆrT ,(2)
m
= Mean
 ˆrT
m

to denote the
estimated ranking of fm on the target dataset T . In summary, we predict the ranking of models
on each class in the target dataset based on their rankings in classes of the open-source datasets,
guided by the semantic relevance between the classes. We predict the ranking of models on each
category in the target dataset based on their rankings in categories of the open-source dataset, guided
by the semantic relevance between the categories. Benefiting from the models’ consistently aligned
classification performance ranking across similar classes, this approach bridges the capability gap in
VLM selection."
SUMMARY OF SWAB,0.37192118226600984,"3.3
Summary of SWAB"
SUMMARY OF SWAB,0.37438423645320196,"As is described in Section 3.2, given a target dataset T and a VLM fm, we denote the two performance
ranking predictions obtained by bridging the modality gap and capability gap as ˆrT ,(1)
m
and ˆrT ,(2)
m
,
respectively. ˆrT ,(1)
m
is predicted based on ModelGPT with our modified embeddings of the generated
auxiliary texts for T . ˆrT ,(2)
m
is predicted by the weighted sum of VLM’s class-specific ranking values
on the open-source datasets. These two predictions, respectively, originate from learning-based and
non-learning-based LOVM methods. We ensemble two predictions together and achieve a more
accurate model ranking estimation. We utilize the weighted Borda Count to aggregate two rankings:"
SUMMARY OF SWAB,0.3768472906403941,"ˆrT ,ens
m
= α · ˆrT ,(1)
m
+ (1 −α) · ˆrT ,(2)
m
.
(9)"
SUMMARY OF SWAB,0.3793103448275862,"We set α = 0.5. Ultimately, SWAB determines the final predicted ranking of the VLMs in the VLM
Zoo based on ˆrT ,ens
m
. The pseudo-code of SWAB is listed in Algorithm 1."
EXPERIMENTS,0.3817733990147783,"4
Experiments"
EVALUATION ON LOVM BENCHMARK,0.3842364532019704,"4.1
Evaluation on LOVM Benchmark"
EVALUATION ON LOVM BENCHMARK,0.3866995073891626,"Setups. We follow LOVM [73] to use its provided VLM Zoo. In addition, to further enhance
the diversity of the VLM Zoo, we add some representative VLMs such as BLIP [28] and BEiT-
3 [59] to expand the VLM Zoo. The final VLM Zoo contains a total of 43 models, which differ in
aspects such as model architecture, pre-training datasets, and training methods. In the appendix, we
also provide experimental results of different model selection methods on the 35 models originally
offered by LOVM. We evaluate different methods on 23 datasets, i.e. ImageNet [8], Aircraft [36],"
EVALUATION ON LOVM BENCHMARK,0.3891625615763547,"Table 1: Results on LOVM Benchmark. We evaluate our method across 23 datasets and 43 pre-
trained VLMs. The results are averaged over all datasets. Our SWAB achieves the best results across
all metrics. For methods that involve adding random noise to data features, we report the standard
deviation of metrics across 10 experiments to mitigate the impact of randomness on result reliability."
EVALUATION ON LOVM BENCHMARK,0.3916256157635468,"Methods
H-Score
NCE
LEEP
LogME
INB
Avg Rank
ModelGPT
SWAB"
EVALUATION ON LOVM BENCHMARK,0.39408866995073893,"R5(↑)
0.174
0.235
0.161
0.191
0.443
0.443
0.446±0.004
0.504±0.000
τ(↑)
0.000
-0.014
0.014
-0.014
0.267
0.246
0.270±0.009
0.318±0.002"
EVALUATION ON LOVM BENCHMARK,0.39655172413793105,"R5 + τ(↑)
0.174
0.221
0.175
0.177
0.710
0.689
0.716±0.011
0.822±0.002"
EVALUATION ON LOVM BENCHMARK,0.39901477832512317,"CIFAR100 [26] and so on. We obtain VLM’s ground truth ranking based on VLM’s Top-1 Accuracy
calculated on the target dataset’s test image set."
EVALUATION ON LOVM BENCHMARK,0.4014778325123153,"Baseline. We select representative methods for each of the three paradigms mentioned in Section 2.2
as our baselines. For the first paradigm, we use four classic model selection methods: H-Score [2],
NCE [55], LEEP [56] and LogME [65]. For the second paradigm, we use the VLM’s ranking on
ImageNet (INB) and VLM’s average ranking (Avg Rank) on classes of the open-source datasets in
the LOVM Benchmark. For the third paradigm, we compare our method with ModelGPT [73]."
EVALUATION ON LOVM BENCHMARK,0.4039408866995074,"Evaluations. We use Top-5 Recall and Kendall’s Rank Correlation to measure the similarity between
the predicted and the ground truth model rankings to evaluate the LOVM method’s performance. We
also calculate the sum of these two metrics to consider the method’s comprehensive capability."
EVALUATION ON LOVM BENCHMARK,0.4064039408866995,"Implementation Details. For a fair comparison, SWAB follow ModelGPT [73] to sequentially extract
a target dataset from each of the 23 datasets in the LOVM Benchmark and treat the remaining datasets
as open-source datasets. Besides, SWAB adopts ModelGPT’s approach of adding Gaussian noise to
corrupt the target dataset’s generated text embeddings. Since LOVM does not provide the specific
image data for the 23 datasets, we download these datasets ourselves and adopt their standard data
splits. Using the templates provided by LOVM, we construct classifiers and recalculate each VLM’s
zero-shot image classification accuracy on these datasets. Additionally, we utilize the code provided
by LOVM to generate class-related text data using ChatGPT. For H-Score, NCE, LEEP, LogME,
INB, and Avg Rank, we follow the practices of previous work and do not add noise to the model’s
inputs. To ensure reliable results, we conduct ten repeated experiments using random seeds from 1
to 10 and report the mean value and standard deviation of ModelGPT’s performance and SWAB’s
performance in Table 1."
EVALUATION ON LOVM BENCHMARK,0.4088669950738916,"Results Analysis. From Table 1, we can draw the following conclusions: (1) Metric-based non-
learning model selection methods such as LogME show poor performance on the LOVM Benchmark.
This is primarily because such algorithms rely on the target dataset’s images, thus the modality gap
has a greater negative impact on them when using generated text data as a substitute for images.
(2) Using open-source datasets is helpful for LOVM. We find that using open-source datasets in a
non-learning way (e.g. INB, Avg Rank) or a learning way (e.g. ModelGPT) all helps LOVM, since
their performance significantly surpasses that of methods not utilizing open-source datasets (e.g.
LogME). (3) Despite leveraging more open-source datasets, the performance of Average Rank is
worse than INB. This confirms our analysis of the Capability Gap, which suggests a discrepancy
between the average ranking of a VLM and its ranking on a specific dataset. (4) Our SWAB achieves
the best performance across all evaluation metrics. Notably, our final performance of R5 + τ (0.822)
represents a significant improvement of 14.8% over the original SoTA method ModelGPT (0.716)."
ABLATION STUDY,0.41133004926108374,"4.2
Ablation Study"
ABLATION STUDY,0.41379310344827586,"Table 2: Ablation Study of SWAB. SWAB-C,
SWAB-M, and SWAB indicates only bridging
the Capability Gap, only bridging the Modality
Gap, and bridging both gaps in SWAB."
ABLATION STUDY,0.41625615763546797,"Method
R5(↑)
τ(↑)
R5 + τ(↑)"
ABLATION STUDY,0.4187192118226601,"SWAB-C
0.487±0.012
0.296±0.018
0.783±0.019
SWAB-M
0.474±0.006
0.316±0.019
0.790±0.017
SWAB
0.504±0.000
0.318±0.002
0.822±0.002"
ABLATION STUDY,0.4211822660098522,"We conduct an ablation study to demonstrate that
bridging the Modality Gap and Capability Gap are
both essential for SWAB. Table 2 presents our ex-
periment results, from which we can observe that
SWAB achieves the best performance across all met-
rics when both gaps are bridged simultaneously.
The ablation study confirms our analysis."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4236453201970443,"4.3
Influence of Key Components in SWAB"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.42610837438423643,"Will Bridge the Capability Gap Be Beneficial for VLM Selection? We compare the LOVM
performance directly using the VLM’s average ranking on each class of open-source datasets and
weighted-sum ranking based on transport matrix γ∗. The results are shown in Table 3. We can find
that utilizing class relevance to bridge the Capability Gap is beneficial for VLM’s Model Selection."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.42857142857142855,"Will Bridge the Modality Gap Be Beneficial for VLM Selection? To eliminate the interference of
other factors, we solely utilize the learning-based predicted rankings ˆrT ,(1)
m
in SWAB, and the input to
the ranker model fm only consists of metrics calculated on the generated text data ˜DT , which serves
as substitutes for images. In this way, the method’s performance depends solely on the quality of the
generated text data ˜DT . From the Table 4, we can find that the generated text data ˜DT become better
substitutes for image data after bridging the Modality Gap."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.43103448275862066,"Table 3: Results of ˆrT ,(2)
m
on the LOVM before
and after bridging the Capability Gap."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.43349753694581283,"Method
R5(↑)
τ(↑)
R5 + τ(↑)"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.43596059113300495,"Average Rank
0.443
0.246
0.689
OT Weighted Rank
0.513
0.217
0.730"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.43842364532019706,"Table 4: Results of ˆrT ,(2)
m
before and after bridg-
ing the Modality Gap (MG)."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4408866995073892,"Method
R5(↑)
τ(↑)
R5 + τ(↑)"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4433497536945813,"Before Bridging MG
0.216
0.061
0.277
After Bridging MG
0.371
0.080
0.451"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4458128078817734,"Which Kind of Gap Vectors Should We Use? When utilizing the gap vectors from open-source
datasets, we have two options: (1) Use the dataset-level mean gap vector calculated on the whole
dataset’s image-text pairs. (2) Use the class-level mean gap vector calculated on the corresponding
class’s image-text pairs. We hope that the gap vectors are as close to each other as possible so that
their mean vector can substitute well for the whole set. Based on this idea, we calculate the statistics
of the gap vectors within a dataset and within each class. We calculate three metrics which include:
(1) the standard deviation of these gap vectors’ magnitude; (2) the mean cosine similarity between
these gap vectors and their corresponding mean gap vectors; and (3) the standard deviation of these
cosine similarities. These metrics reflect the consistency of the gap vectors. Table 5 shows the results."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4482758620689655,"Table 5: Results of metrics measuring gap vec-
tors’ consistency belonging to the same dataset
or the same class. M: Magnitude, D: Direction."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.45073891625615764,"Gap Vector
ImageNet
M-Std(↓)
D-Mean(↑)
D-Std(↓)"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.45320197044334976,"Dataset Mean
0.04
0.68
0.07
Class Mean
0.03
0.85
0.04"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.45566502463054187,"Table 6: Results of SWAB-M on the LOVM
Benchmark using the dataset-level mean gap
vectors and class-level mean gap vectors."
INFLUENCE OF KEY COMPONENTS IN SWAB,0.458128078817734,"Gap Vector
R5(↑)
τ(↑)
R5 + τ(↑)"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4605911330049261,"Dataset Mean
0.443
0.304
0.747
Class Mean
0.474
0.316
0.790"
INFLUENCE OF KEY COMPONENTS IN SWAB,0.4630541871921182,"From the Table 5, we can find that the class-level gap vectors tend to be more consistent, which
inspires us to use the class-level mean gap vectors. We also compare the results of SWAB-M on the
LOVM Benchmark using the dataset-level mean gap vectors and the class-level mean gap vectors,
respectively. The implementation details are the same as Table 2. Table 6 shows the results, which
verifies that using the class-level mean gap vectors is a better choice."
CONCLUSION,0.46551724137931033,"5
Conclusion"
CONCLUSION,0.46798029556650245,"We analyze and address two key challenges in Language-Only VLM Selection (LOVM), which are
VLM’s modality gap across different modal features and VLM’s Capability gap between its overall
and dataset-specific rankings. Our key insight is that we can reuse the model’s useful statistics on
open-source datasets to help the model selection on the target dataset. SWAB utilizes a transport
matrix between classes of the target dataset and open-source datasets to transfer VLM’s class-specific
modality gap vectors and class-specific rank from open-source datasets to the target dataset, which
mitigates the negative impacts of these two gaps. Experiment results on the LOVM benchmark show
the superiority of our method."
CONCLUSION,0.47044334975369456,Acknowledgements
CONCLUSION,0.4729064039408867,"This work is partially supported by National Key R&D Program of China (2022ZD0114805), NSFC
(62376118, 62250069, 62006112, 61921006), Collaborative Innovation Center of Novel Software
Technology and Industrialization, CCF-Tencent Rhino-Bird Open Research Fund (RAGR20240101)."
REFERENCES,0.4753694581280788,References
REFERENCES,0.47783251231527096,"[1] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C.
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In ICCV, 2019."
REFERENCES,0.4802955665024631,"[2] Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and Leonidas J. Guibas.
An information-theoretic approach to transferability in task transfer learning. In ICIP, 2019."
REFERENCES,0.4827586206896552,"[3] Wei-Lun Chao, Han-Jia Ye, De-Chuan Zhan, Mark E. Campbell, and Kilian Q. Weinberger. Revisiting
meta-learning as supervised learning. CoRR, 2020. URL https://arxiv.org/abs/2002.00573."
REFERENCES,0.4852216748768473,"[4] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and
state of the art. Proc. IEEE, 2017."
REFERENCES,0.4876847290640394,"[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing
textures in the wild. In CVPR, 2014."
REFERENCES,0.49014778325123154,"[6] Adam Coates, Andrew Y. Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In AISTATS, 2011."
REFERENCES,0.49261083743842365,"[7] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013."
REFERENCES,0.49507389162561577,"[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009."
REFERENCES,0.4975369458128079,"[9] Nan Ding, Xi Chen, Tomer Levinboim, Soravit Changpinyo, and Radu Soricut. Pactran: Pac-bayesian
metrics for estimating the transferability of pretrained models to classification tasks. In ECCV, 2022."
REFERENCES,0.5,"[10] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisser-
man.
The pascal visual object classes challenge 2007 (voc2007) results.
http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html, 2007."
REFERENCES,0.5024630541871922,"[11] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig
Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In
ICML, 2022."
REFERENCES,0.5049261083743842,"[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In ICML, 2017."
REFERENCES,0.5073891625615764,"[13] Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas
Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H.
Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien
Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal
transport. JMLR, 2021."
REFERENCES,0.5098522167487685,"[14] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent
Itti, Balaji Lakshminarayanan, and Jiaping Zhao. Improving zero-shot generalization and robustness of
multi-modal models. In CVPR, 2023."
REFERENCES,0.5123152709359606,"[15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti
dataset. IJRR, 2013."
REFERENCES,0.5147783251231527,"[16] Dumitru Ian Goodfellow, Will Cukierski, and Yoshua Bengio. Challenges in representation learning: Facial
expression recognition challenge, 2013."
REFERENCES,0.5172413793103449,"[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016."
REFERENCES,0.5197044334975369,"[18] Xiaoxuan He, Siming Fu, Xinpeng Ding, Yuchen Cao, and Hualiang Wang. Uniformly distributed category
prototype-guided vision-language framework for long-tail recognition. In ACM MM, 2023."
REFERENCES,0.5221674876847291,"[19] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. J-STARS, 2019."
REFERENCES,0.5246305418719212,"[20] Long-Kai Huang, Junzhou Huang, Yu Rong, Qiang Yang, and Ying Wei. Frustratingly easy transferability
estimation. In ICML, 2022."
REFERENCES,0.5270935960591133,"[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal
Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig
Schmidt. Openclip, 2021. URL https://doi.org/10.5281/zenodo.5143773."
REFERENCES,0.5295566502463054,"[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,
Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text
supervision. In ICML, 2021."
REFERENCES,0.5320197044334976,"[23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Fei-Fei Li, C. Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In
CVPR, 2017."
REFERENCES,0.5344827586206896,"[24] Kaggle and EyePacs. Kaggle diabetic retinopathy detection, 2015. URL https://www.kaggle.com/c/
diabetic-retinopathy-detection/data."
REFERENCES,0.5369458128078818,"[25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13),
2013."
REFERENCES,0.5394088669950738,"[26] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical
report, 2009."
REFERENCES,0.541871921182266,"[27] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs, 2010."
REFERENCES,0.5443349753694581,"[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image
pre-training for unified vision-language understanding and generation. In ICML, 2022."
REFERENCES,0.5467980295566502,"[29] Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, and Xu Yang. How to configure good in-context sequence
for visual question answering. In CVPR, 2024."
REFERENCES,0.5492610837438424,"[30] Weihua Li, Wenyang Liu, Yanbu Guo, Bingyi Wang, and Hua Qing. Deep contextual representation
learning for identifying essential proteins via integrating multisource protein features. Chinese Journal of
Electronics, 2023."
REFERENCES,0.5517241379310345,"[31] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou. Mind the gap: Under-
standing the modality gap in multi-modal contrastive representation learning. In NeurIPS, 2022."
REFERENCES,0.5541871921182266,"[32] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014."
REFERENCES,0.5566502463054187,"[33] Jian-Dong Liu, Zhi-Hao Tan, and Zhi-Hua Zhou. Towards making learnware specification and market
evolvable. In AAAI, 2024."
REFERENCES,0.5591133004926109,"[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. In CVPR, 2022."
REFERENCES,0.5615763546798029,"[35] Teli Ma, Shijie Geng, Mengmeng Wang, Jing Shao, Jiasen Lu, Hongsheng Li, Peng Gao, and Yu Qiao. A
simple long-tailed recognition baseline via vision-language model. CoRR, abs/2111.14745, 2021."
REFERENCES,0.5640394088669951,"[36] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
classification of aircraft. CoRR, abs/1306.5151, 2013."
REFERENCES,0.5665024630541872,"[37] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, and Carl
Vondrick. Doubly right object recognition: A why prompt for visual rationales. In CVPR, 2023."
REFERENCES,0.5689655172413793,"[38] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. UMAP: uniform manifold approxi-
mation and projection. J. Open Source Softw., 2018."
REFERENCES,0.5714285714285714,"[39] Deyu Meng and Lina Sun. Some new trends of deep learning research. Chinese Journal of Electronics,
2019."
REFERENCES,0.5738916256157636,"[40] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In
ICLR, 2023."
REFERENCES,0.5763546798029556,"[41] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits
in natural images with unsupervised feature learning. In NeurIPS Workshop, 2011."
REFERENCES,0.5788177339901478,"[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of
classes. In ICVGIP, 2008."
REFERENCES,0.5812807881773399,"[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.
Training language models to follow instructions with human feedback. NeurIPS, 2022."
REFERENCES,0.583743842364532,"[44] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012."
REFERENCES,0.5862068965517241,"[45] Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine
Learning, 2019."
REFERENCES,0.5886699507389163,"[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021."
REFERENCES,0.5911330049261084,"[47] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush
Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400
million image-text pair. CoRR, abs/2111.02114, 2021."
REFERENCES,0.5935960591133005,"[48] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR, 2022."
REFERENCES,0.5960591133004927,"[49] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training
for language understanding. In NeurIPS, pages 16857–16867, 2020."
REFERENCES,0.5985221674876847,"[50] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition
benchmark: a multi-class classification competition. In IJCNN, 2011."
REFERENCES,0.6009852216748769,"[51] Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heteroge-
neous feature spaces without auxiliary data. In IJCAI, 2023."
REFERENCES,0.603448275862069,"[52] Peng Tan, Hai-Tian Liu, Zhi-Hao Tan, and Zhi-Hua Zhou. Handling learnwares from heterogeneous
feature spaces with explicit label exploitation. NeurIPS, 2024."
REFERENCES,0.6059113300492611,"[53] Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heteroge-
neous feature spaces. Mach. Learn., 2024."
REFERENCES,0.6083743842364532,"[54] Zhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng, Hai-Tian Liu, Yi Xie, Xiao-
Chuan Zou, Yang Yu, and Zhi-Hua Zhou. Beimingwu: A learnware dock system. In KDD, 2024."
REFERENCES,0.6108374384236454,"[55] Anh Tuan Tran, Cuong V. Nguyen, and Tal Hassner. Transferability and hardness of supervised classifica-
tion tasks. In ICCV, 2019."
REFERENCES,0.6133004926108374,"[56] Anh Tuan Tran, Cuong V. Nguyen, and Tal Hassner. Leep: A new measure to evaluate transferability of
learned representations. In ICML, 2020."
REFERENCES,0.6157635467980296,"[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017."
REFERENCES,0.6182266009852216,"[58] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant
CNNs for digital pathology, 2018."
REFERENCES,0.6206896551724138,"[59] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language:
BEiT pretraining for vision and vision-language tasks. In CVPR, 2023."
REFERENCES,0.6231527093596059,"[60] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database:
Large-scale scene recognition from abbey to zoo. In CVPR, 2010."
REFERENCES,0.625615763546798,"[61] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context
configurations for image captioning. In NeurIPS, 2023."
REFERENCES,0.6280788177339901,"[62] Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Rectify heterogeneous models with semantic
mapping. In ICML, 2018."
REFERENCES,0.6305418719211823,"[63] Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Heterogeneous few-shot model rectification
with semantic mapping. TPAMI, 2021."
REFERENCES,0.6330049261083743,"[64] Chao Yi, Lu Ren, De-Chuan Zhan, and Han-Jia Ye. Leveraging cross-modal neighbor representation for
improved clip classification. In CVPR, 2024."
REFERENCES,0.6354679802955665,"[65] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of pre-trained
models for transfer learning. In ICML, 2021."
REFERENCES,0.6379310344827587,"[66] Kaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, Michael I. Jordan, and Mingsheng Long. Ranking
and tuning pre-trained models: a new paradigm for exploiting model hubs. JMLR, 2022."
REFERENCES,0.6403940886699507,"[67] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models. TMLR, 2022."
REFERENCES,0.6428571428571429,"[68] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang,
Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang.
Florence: A new foundation model for computer vision. CoRR, abs/2111.11432, 2021."
REFERENCES,0.645320197044335,"[69] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic,
Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem,
Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale
study of representation learning with the visual task adaptation benchmark, 2020."
REFERENCES,0.6477832512315271,"[70] Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye. Model spider: Learning
to rank pre-trained models efficiently. NeurIPS, 2023."
REFERENCES,0.6502463054187192,"[71] Zhi-Hua Zhou. Learnware: on the future of machine learning. Frontiers Comput. Sci., 2016."
REFERENCES,0.6527093596059114,"[72] Zhi-Hua Zhou and Zhi-Hao Tan. Learnware: small models do big. Sci. China Inf. Sci., 2024."
REFERENCES,0.6551724137931034,"[73] Orr Zohar, Shih-Cheng Huang, Kuan-Chieh Wang, and Serena Yeung. Lovm: Language-only vision model
selection. In NeurIPS, 2023."
REFERENCES,0.6576354679802956,"[74] Beiji Zou, Xi Shan, Chengzhang Zhu, Yulan Dai, Kejuan Yue, Yuanqiong Chen, Yalong Xiao, and Jiaer
Huang. Deep learning and its application in diabetic retinopathy screening. Chinese Journal of Electronics,
2020."
REFERENCES,0.6600985221674877,"In the Appendix, we introduce more details about the LOVM Benchmark as well as our extensions to
it. Besides, we introduce ModelGPT’s implementation and our SWAB’s implementation. We also
provide more experimental results of SWAB. The structure of the Appendix is as follows:"
REFERENCES,0.6625615763546798,"• In section A, we introduce the relevant information of the 43 models and 23 datasets used in
our experiments. We also introduce the evaluation metrics used in LOVM Benchmark [73].
• In section B, we introduce the metrics used in Equation 3.
• In section C, we introduce the related work of the paper.
• In section D, we provide some details on SWAB’s implementation.
• In section E, we provide more experimental results of SWAB."
REFERENCES,0.6650246305418719,"A
LOVM Benchmark Details"
REFERENCES,0.6674876847290641,"LOVM Benchmark [73] consists of 35 pre-trained VLMs and 23 datasets, with a total of 35×23 = 805
evaluations. To further enhance the diversity of the VLM Zoo, we add some representative VLMs
such as BLIP [28] and BEiT-3 [59] to expand the VLM Zoo. The final VLM Zoo contains a total
of 43 models, with a total of 43 × 23 = 989 evaluations. For each evaluation, LOVM provides the
VLM’s zero-shot image classification accuracy on the corresponding dataset. Therefore, we can get
the ground truth performance ranking of 43 VLMs on the 23 datasets."
REFERENCES,0.6699507389162561,"A.1
VLMs of LOVM Benchmark"
REFERENCES,0.6724137931034483,"To cover as many types of models as possible, the LOVM Benchmark uses OpenCLIP library [21] to
get diverse VLMs. These VLMs differ from each other in terms of the model architecture (ResNet [17],
Transformer [57], ConvNext [34]), the pre-trained dataset (OpenAI’s Data [46], LAION 2b [47]), the
training method (loss function/hyperparameter/data augmentation) and so on. Table 7 displays the
relevant information of each VLM. We further add BLIP and BEiT-3 to the original VLM Zoo. It
should be noted that in addition to the image-text pair data listed in the Table 7, BEiT-3’s pre-training
data also includes unimodal image dataset (ImageNet-21K) and text datasets (English Wikipedia,
BookCorpus, OpenWebText, CC-News, Stories). The diversity of these VLMs ensures that the
experimental results calculated on them can reflect the performance of the VLM model selection
algorithm in real-world situations."
REFERENCES,0.6748768472906403,"A.2
Datasets of LOVM Benchmark"
REFERENCES,0.6773399014778325,"To cover as wide a distribution of image classification tasks as possible, the LOVM Benchmark
collects 23 diverse datasets. These datasets differ from each other in terms of the number of categories,
category semantics, image domains, and so on. Table 8 displays the relevant information of each
dataset. The diversity of these tasks ensures that the experimental results calculated on them can
reflect the performance of the VLM model selection method in real-world situations."
REFERENCES,0.6798029556650246,"A.3
Evaluation Metrics of LOVM Benchmark"
REFERENCES,0.6822660098522167,"In LOVM, our aim is to maximize the rank similarity between the prediction of VLMs’ ranking
ˆrT = {ˆrT
m}M
m=1 and VLMs’ ground truth ranking rT = {rT
m}M
m=1 on the target dataset, especially
the rank similarity of the top 5 VLMs in ˆrT and rT . This is because we tend to focus only on whether
those appropriate models can be chosen. To ensure fair comparability, we follow the evaluation
metrics used by LOVM [73] to assess different model selection methods and directly utilize the code
provided by LOVM [73] to calculate these metrics:"
REFERENCES,0.6847290640394089,"• Top-5 Recall (R5) – Top-5 Recall R5 measures the model selection algorithm’s accuracy
in identifying the true top five best-performing models within its predicted top five models.
The calculation method is shown in Equation 11. Here IND(ˆr5
T ) and IND(r5
T ) indicates
the model indices sets of the top 5 VLMs in ˆrT and rT , respectively. A Top 5 Recall closer
to 1 signifies greater accuracy in the predicted rankings.
F = IND(ˆr5
T ) ∩IND(r5
T ).
(10)"
REFERENCES,0.687192118226601,R5 = |F|
REFERENCES,0.6896551724137931,"5 .
(11)"
REFERENCES,0.6921182266009852,"Table 7: The detailed information of 43 models used in the LOVM Benchmark. Some of the
information in the table comes from [73]."
REFERENCES,0.6945812807881774,"ID
Model
Name
Dataset
Name"
REFERENCES,0.6970443349753694,"1
RN50
RN50
openai
WIT
2
RN101
RN101
openai
WIT
3
RN50x4
RN50x4
openai
WIT
4
RN50-16
RN50x16
openai
WIT
5
RN50x64
RN50x64
openai
WIT
6
ViT-B-32
ViT-B/32
laion400m_e31
L400m
7
ViT-B-32
ViT-B/32
laion400m_e32
L400m
8
ViT-B-32-quickgelu
ViT-B/32
laion400m_e32
L400m
9
ViT-B-32
ViT-B/32
openai
WIT
10
ViT-B-32
ViT-B/32
laion2b_s34b_b79k
L2b-b
11
ViT-B-32
ViT-B/32
laion2b_e16
L2b-c
12
ViT-B-16
ViT-B/16
laion400m_e32
L400m
13
ViT-B-16
ViT-B/16
openai
WIT
14
ViT-B-16-240
ViT-B/16-240
laion400m_e32
L400m
15
ViT-L-14
ViT-L/14
laion400m_e31
L400m
16
ViT-L-14
ViT-L/14
laion400m_e32
L400m
17
ViT-L-14
ViT-L/14
laion2b_s32b_b82k
L2b-b
18
ViT-L-14
ViT-L/14
openai
WIT
19
ViT-L-14-336
ViT-L/14-336
openai
WIT
20
ViT-G-14
ViT-G/14
laion2b_s12b_b42k
L2b-a
21
ViT-G-14
ViT-G/14
laion2b_s34b_b88k
L2b-a
22
ViT-H-14
ViT-H/14
laion2b_s32b_b79k
L2b-b
23
coca_ViT-B-32
CoCa-ViT-B/32
laion2b_s13b_b90k
L2b-c
24
coca_ViT-B-32
CoCa-ViT-B/32
mscoco_finetuned_laion2b_s13b_b90k
L2b-c + coco
25
coca_ViT-L-14
CoCa-ViT-L/14
laion2b_s13b_b90k
L2b-c
26
coca_ViT-L-14
CoCa-ViT-L/14
mscoco_finetuned_laion2b_s13b_b90k
L2b-c + coco
27
convnext_base
ConvNEXT-B
laion400m_s13b_b51k
L400m-c
28
convnext_base_w
ConvNEXT-BW
laion2b_s13b_b82k
L2b-d
29
convnext_base_w
ConvNEXT-BW
laion2b_s13b_b82k_augreg
L2b-e
30
convnext_base_w
ConvNEXT-BW
laion_aesthetic_s13b_b82k
L2b-f
31
convnext_base_w_320
ConvNEXT-BW-320
laion_aesthetic_s13b_b82k
L2b-f
32
convnext_base_w_320
ConvNEXT-BW-320
laion_aesthetic_s13b_b82k_augreg
L2b-g
33
convnext_large_d
ConvNEXT-LD
laion2b_s26b_b102k_augreg
L2b-h
34
convnext_large_d_320
ConvNEXT-LD-320
laion2b_s29b_b131k_ft
L2b-i
35
convnext_large_d_320
ConvNEXT-LD-320
laion2b_s29b_b131k_ft_soup
L2b-j
36
BLIP_retrieval_base_coco
BLIP_retrieval_base_coco
COCO+VG+CC+CC12M+SBU
BLIP-Dataset
37
BLIP_retrieval_base_f30k
BLIP_retrieval_base_f30k
COCO+VG+CC+CC12M+SBU+Flickr30k
BLIP-Dataset-f
38
BLIP_retrieval_large_coco
BLIP_retrieval_large_coco
COCO+VG+CC+CC12M+SBU
BLIP-Dataset
39
BLIP_retrieval_large_f30k
BLIP_retrieval_large_f30k
COCO+VG+CC+CC12M+SBU+Flickr30k
BLIP-Dataset-f
40
BEiT-3_retrieval_base_coco
BEiT-3_retrieval_base_coco
CC12M+CC3M+SBU+COCO+VG
BEiT-Dataset
41
BEiT-3_retrieval_base_f30k
BEiT-3_retrieval_base_f30k
CC12M+CC3M+SBU+COCO+VG+Flickr30k
BEiT-Dataset-f
42
BEiT-3_retrieval_large_coco
BEiT-3_retrieval_large_coco
CC12M+CC3M+SBU+COCO+VG
BEiT-Dataset
43
BEiT-3_retrieval_large_f30k
BEiT-3_retrieval_large_f30k
CC12M+CC3M+SBU+COCO+VG+Flickr30k
BEiT-Dataset-f"
REFERENCES,0.6995073891625616,Table 8: Detailed information of 23 tasks used in the LOVM Benchmark. This table comes from [73].
REFERENCES,0.7019704433497537,"Dataset
Classes
Task
Domain"
REFERENCES,0.7044334975369458,"Imagenet [8]
1000
classification
natural image
SUN397 [60]
397
scene und.
natural image
Country211 [46]
211
geolocation
natural image
Stanford Cars [25]
196
classification
natural image
Flowers102 [42]
102
classification
natural image
CIFAR100 [26]
100
classification
natural image
DTD [5]
46
classification
textural image
RESISC45 [4]
45
classification
satellite images
GTSRB [50]
43
classification
natural image
Oxford Pets [44]
37
classification
natural image
VOC2007 [10]
20
classification
natural image
STL10 [6]
10
classification
natural image
EuroSAT [19]
10
classification
satellite images
MNIST [27]
10
classification
hand-writing
SVHN [41]
10
OCR
natural image
CLEVR-C [23]
8
object counting
natural image
CLEVR-D [23]
8
distance est.
natural image
FER2013 [16]
7
fac. exp. rec.
natural image
DMLab [69]
6
distance est.
synthetic
Retinopathy [24]
5
classification
retina scan
KITTI [15]
4
distance est.
natural image
PCam [58]
2
classification
histopathology
Rendered SST2 [46]
2
OCR
text image"
REFERENCES,0.7068965517241379,"• Kendall’s Rank Correlation (τ) – Kendall’s Rank Correlation τ measures the ranking
consistency between two ranking lists. We follow LOVM [73] to focus on the VLMs within
the intersection of the top 5 VLMs in ˆrT and rT and use τ to evaluate a model selection
method’s capability."
REFERENCES,0.7093596059113301,"B
ModelGPT Details"
REFERENCES,0.7118226600985221,"ModelGPT is a method proposed by LOVM [73]. In this section, we introduce the metrics that
ModelGPT used in Equation 3."
REFERENCES,0.7142857142857143,"B.1
The Generation Process of Auxiliary Text Samples"
REFERENCES,0.7167487684729064,"ModelGPT [73] utilizes ChatGPT [43] to generate auxiliary text data by designing prompts to query
ChatGPT. This extra text data mainly includes the Captions Dataset and the Synonyms Dataset."
REFERENCES,0.7192118226600985,"Captions Dataset.
ModelGPT uses the following prompt to guide LLM to generate realistic and
confusing text data corresponding to the user-provided classes. The reason for requiring ChatGPT to
generate confusing texts is to increase the classification difficulty of the text data, thereby enhancing
its ability to distinguish the performance of different models."
REFERENCES,0.7216748768472906,"Generate long and confusing image captions for the {domain} domain, which
will be used to evaluate a Vision-Language Model’s {task} performance.
Generate 50 captions for {classname}:"
REFERENCES,0.7241379310344828,"We show some generated auxiliary text examples. For example, in the category of dog, one of the text
samples generated by ChatGPT is ""An adorable dog perfect for cuddles and playtime."" ModelGPT
collects the results from this prompt to form the captions dataset, Dcap."
REFERENCES,0.7266009852216748,"Synonyms Dataset.
ModelGPT uses synonyms to evaluate VLM’s text encoder. For example, we
expect an excellent VLM to extract similar embeddings for the words “chair” and “seat”. The prompt
to guide LLM to generate synonyms is as follows."
REFERENCES,0.729064039408867,"Please list the superclasses/synonyms for {classname}. For example:
chair: [furniture, seat, bench, armchair, sofa]
{classname}:"
REFERENCES,0.7315270935960592,"ModelGPT collects the results from this prompt to form the synonyms dataset, Dsyn."
REFERENCES,0.7339901477832512,"B.2
Text-Derived Scores"
REFERENCES,0.7364532019704434,"ModelGPT uses six metrics for model selection, which can be divided into Text Classification scores
and Dataset Granularity scores. Text Classification scores include the Text top-1 accuracy score
and Text f1-score. While Granularity scores include the Fisher criterion, Silhouette score, Class
Dispersion score and Synonym Consistency score. Here we focus on introducing the various metrics
included in the Granularity scores. We refer to the relevant content in LOVM."
REFERENCES,0.7389162561576355,"Fisher Criterion ϕfisher. The Fisher score measures the closeness of VLM’s text classifier to one
another. Equation 12 shows the calculation process of it where ˆti is the text classifier of the i-th class
derived using the prompt ensemble strategies proposed in [46], θ(·, ·) is a function that calculates the
cosine similarity between two vectors, and |C| is the number of classes."
REFERENCES,0.7413793103448276,"ϕfisher =
1
|C| |C|
X"
REFERENCES,0.7438423645320197,"j=1
maxi,i̸=j

θ(ˆti, ˆtj)

.
(12)"
REFERENCES,0.7463054187192119,"Silhouette Score φsil. The Silhouette Score measures the separation of different-class samples in
the caption dataset Dcap. To calculate it, ModelGPT averages the cosine similarity of captions to the
nearest other class’s classifier by:"
REFERENCES,0.7487684729064039,"φsil =
1
|C| |C|
X"
REFERENCES,0.7512315270935961,"j=1
maxi,i̸=j ""
1
N N
X"
REFERENCES,0.7536945812807881,"k=1
θ(Dcap[j]k, ˆti) #"
REFERENCES,0.7561576354679803,".
(13)"
REFERENCES,0.7586206896551724,"where ˆti is the text classifier of the i-th class derived using the prompt ensemble strategies proposed
in [46], θ(·, ·) is a function that calculates the cosine similarity between two vectors, and |C| is the
number of classes. Dcap[j]k representing sample k of class j in the caption dataset Dcap. There is a
total of N such samples for each class."
REFERENCES,0.7610837438423645,"Class Dispersion Score ρdisp. Class Dispersion Score quantifies the degree of same-class tightness or
data cone radius, which is calculated using the following Equation:"
REFERENCES,0.7635467980295566,"ρdisp =
1
|C|N |C|
X i=1 N
X"
REFERENCES,0.7660098522167488,"k=1
θ(Dcap[i]k, ˆti).
(14)"
REFERENCES,0.7684729064039408,The definitions of all symbols in Equation 14 are consistent with those in Equation 13.
REFERENCES,0.770935960591133,"Synonym Consistency Score γsyn. Synonym consistency allows us to evaluate the degree of content
shift between the VLMs’ pre-training and target dataset. The calculation process is shown as follows:"
REFERENCES,0.7733990147783252,"γsyn =
1
|C|N |C|
X i=1 N
X"
REFERENCES,0.7758620689655172,"k=1
θ(Dsyn[i]k, ˆti).
(15)"
REFERENCES,0.7783251231527094,"The definitions of ˆti, θ(·, ·), |C| and N in Equation 15 are consistent with those in Equation 13.
Dsyn[i]k representing sample k of class i in the synonym dataset Dsyn."
REFERENCES,0.7807881773399015,"C
Related Work"
REFERENCES,0.7832512315270936,"Vision-Language Models. Deep learning has achieved competitive performance [39, 74, 30] and
the vision-language model is a new research hotspot of it. Vision-Language Models represent a
class of multimodal models adept at correlating textual and visual information. These VLMs can
comprehend rich semantics and possess strong generalization capabilities. Hence, they are often used
in data-limited scenarios [61, 29]. Many VLMs are pre-trained or fine-tuned on extensive text-image
pairs using loss functions such as contrastive loss, endowing them with powerful text-image matching
capability. Prominent VLMs include CLIP [46], ALIGN [22], FLAVA [48], Florence [68], and
CoCa [67]. These VLMs possess robust zero-shot image classification capabilities [46], which enables
its widespread application in tasks characterized by long-tail distributions or those where collecting
substantial training data is challenging, such as medical image analysis. Some work [40, 64] has also
shown that by incorporating external knowledge, the zero-shot capabilities of CLIP can be further
enhanced. In recent years, the number of open-source VLMs has been increasing [21]. Previous
work [73] has pointed out that different VLMs possess varying image classification capabilities. This
indicates that the performance of VLMs can vary significantly across different tasks and domains.
These models with diverse capabilities constitute a VLM Zoo rich in knowledge. This VLM Zoo
enables us to utilize different VLMs for various classification tasks, thereby changing the paradigm
of using a single VLM to complete diverse classification tasks. This paper focuses on selecting the
most suitable VLM for the target task from the VLM Zoo."
REFERENCES,0.7857142857142857,"Model Selection. In recent years, how to select the most suitable model for the target task from
the model zoo has received widespread attention. For example, a series of works on learnware [53,
52, 71, 72, 33, 51, 54] have attempted to solve this problem. The essence of the model selection
problem lies in measuring the transferability of the model to the target task. Previous model selection
methods [55, 56, 65, 66, 9, 20] evaluate the PTM’s transferability by performing a forward pass
of the PTM on the target task’s data and calculating the metric of transferability based on the
forward pass’s result. For example, H-Score [2], NCE [55], LEEP [56], LogME [65] estimate
transferred log-likelihood, negative conditional entropy, log expectation, marginalized likelihood to
obtain proxy metric of transferability, respectively. Some new methods, such as Task2Vec [1] and
Model Spider [70], generate representation vectors for both the models and the tasks and measure
the transferability of the model to the task by calculating the similarity between these vectors.
However, VLMs are typically used in zero-shot or few-shot scenarios, where we are unable to obtain
a large amount of data for the target task in advance, making traditional model selection approaches
unsuitable for VLMs. Additionally, previous methods have mainly focused on single-modal models,
overlooking the characteristics of VLMs. Therefore, in this paper, we concentrate on designing
model selection algorithms that are suitable for scenarios with limited data and take into account the
characteristics of VLMs."
REFERENCES,0.7881773399014779,"D
Implementation Details of SWAB"
REFERENCES,0.7906403940886699,"In this section, we provide some details on the implementation of SWAB, which are not mentioned in
the main text due to space constraints."
REFERENCES,0.7931034482758621,"D.1
Filtering the Open-Source Tasks’ Classes"
REFERENCES,0.7955665024630542,"The statistics of classes unrelated to the target class are generally not valuable for reuse. Meanwhile,
when the number of classes |CS| in open-source datasets S is large, solving the optimal transport
problem in Equation 5 can be time-consuming (as current optimal transport toolkits generally compute
via CPU). To reduce the runtime of optimal transport, we can first filter the classes CS. Consider
that only statistics of classes relevant to the target dataset are helpful. Therefore, we can filter out
the classes in CS that are irrelevant to the target dataset T based on the class-level textual semantic
similarity between the open-source datasets’ classes and the target dataset’s classes. This process is
shown in the following Equation:"
REFERENCES,0.7980295566502463,"Sij =
ϕ(cS
i )⊤ϕ(cT
j )
∥ϕ(cS
i )∥· ∥ϕ(cT
j )∥.
(16)"
REFERENCES,0.8004926108374384,"C
′
S = {cS
i | max(Si,:) > λ}, |C
′
s| = k′
S.
(17)"
REFERENCES,0.8029556650246306,"Here Si,: refers to the i-th row of the semantic similarity matrix calculated using Equation 16,
which represents the vector formed by the similarity between the i-th class cS
i in CS and each class
CT = {cT
1 , · · · , cT
kT } of the target task. λ is a threshold and we set λ = 0.5. k′
S refers to the number
of classes in the filtered set C
′
S. Then we use the filter classes C
′
S to calculate the transport matrix
γ∗∈Rk′
S×kT and continue with the following steps."
REFERENCES,0.8054187192118226,"D.2
Using Partial Optimal Transport for Bridging the Capability Gap"
REFERENCES,0.8078817733990148,"Partial optimal transport extends the optimal transport framework, enabling the selective transfer of
elements from a source to a target distribution, rather than moving all elements. Its optimization
problem is defined as in Equation 18. Here mass refers to the total amount of mass actually be
transferred. We set mass = 0.9 in our implementation."
REFERENCES,0.8103448275862069,γ∗= argmin
REFERENCES,0.812807881773399,"γ∈R
k′
S ×kT
+ X"
REFERENCES,0.8152709359605911,"i,j
γi,j costi,j"
REFERENCES,0.8177339901477833,"s.t. γ1 ≤u; γT 1 ≤v; γi,j ≥0;"
REFERENCES,0.8201970443349754,"1T γT 1 = mass ≤min {∥u∥1, ∥v∥1} . (18)"
REFERENCES,0.8226600985221675,"We found that when using Equation 8 to bridge the Capability Gap, the transport matrix γ∗obtained
using partial optimal transport yields better results than the one obtained using the original optimal
transport via solving the Equation 5. Therefore, in our implementation, we use the transport matrix
derived from partial optimal transport to bridge the capability gap. This also indicates that when
estimating VLM’s statistics on the target dataset, different types of statistics have different preferences
for the estimation methods used. This variability is worth further investigation."
REFERENCES,0.8251231527093597,"D.3
Data Normalization in Bridging the Modality Gap."
REFERENCES,0.8275862068965517,"When bridging the Modality Gap as described in Section 3.2, we find that applying z-score normal-
ization to the text and image features used in this process yields better results. Therefore, in our
implementation, we normalize the features of all text and image samples during the modality bridging
process using the following Equation:"
REFERENCES,0.8300492610837439,z = x −µ
REFERENCES,0.8325123152709359,"σ
.
(19)"
REFERENCES,0.8349753694581281,"Here x ∈Rd represents the image sample’s or text sample’s feature, while µ ∈Rd and σ ∈Rd are
calculated using the features of all samples of the same modality within its respective dataset."
REFERENCES,0.8374384236453202,Algorithm 1 SWAB
REFERENCES,0.8399014778325123,"1: Input: Target dataset’s class names CT , open-source datasets’ class names CS, open-source
datasets’ images DI
S.
2: Use ChatGPT to generate auxiliary text data ˜DS and ˜DT based on CS and CT .
3: Calculate VLM’s class-level zero-shot image classification rankings {rS
m,i}m=M,i=kS
m=1,i=1
and class-
level gap vectors {gS
m,i}m=M,i=kS
m=1,i=1
.
4: for kS datasets do
5:
Calculate textual similarity between the current dataset’s class names and other open-source
datasets’ class names to construct a cost matrix.
6:
Solve Optimal Transport Problem based on the cost matrix to get transport matrix γ∗.
7:
Use γ∗and other open-source datasets’ class-level gap vectors to predict the current dataset’s
class-level gap vectors. Add the predicted class-level gap vectors to the corresponding text
data’s feature of ˜DS to get modified text data.
8: end for
9: Calculate textual similarity between open-source datasets’ class names CS and target dataset’s
class names CT to construct cost matrix.
10: Solve Optimal Transport Problem based on the cost matrix to get transport matrix γ∗.
11: Use γ∗and {gS
m,i}m=M,i=kS
m=1,i=1
to predict the class-level vectors {ˆgT
m,i}m=M,i=kT
m=1,i=1
of the target
dataset. Add {ˆgT
m,i}m=M,i=kT
m=1,i=1
to corresponding text data’s feature of ˜DT to get modified text
data.
12: Use open-source datasets’ modified text data and VLMs’ ground truth zero-shot image classifica-
tion performance to train the ranker model fm."
REFERENCES,0.8423645320197044,"13: Use the ranker model fm to predict VLMs’ rankings {ˆrT ,(1)
m
}m=M
m=1 on the target dataset based
on the target dataset’s modified text data.
14: Use γ∗and {rS
m,i}m=M,i=kS
m=1,i=1
to predict the VLMs’ class-level zero-shot image classification
rankings {ˆrT
m,i}m=M,i=kT
m=1,i=1
of the target dataset."
REFERENCES,0.8448275862068966,"15: Calculate the average prediction rankings across all classes in the target dataset {ˆrT ,(2)
m
}m=M
m=1 of
{ˆrT
m,i}m=M,i=kT
m=1,i=1
for each VLM as the VLM’s overall prediction ranking on the target dataset."
REFERENCES,0.8472906403940886,"16: Ensemble {ˆrT ,(1)
m
}m=M
m=1 and {ˆrT ,(2)
m
}m=M
m=1 to get final predicted rankings {ˆrT ,ens
m
}m=M
m=1 ."
REFERENCES,0.8497536945812808,"D.4
Pseudo Code of SWAB"
REFERENCES,0.8522167487684729,Algorithm 1 shows the pseudo-code of SWAB.
REFERENCES,0.854679802955665,"E
More Experiment Results"
REFERENCES,0.8571428571428571,"In this section, we provide more experimental results of SWAB."
REFERENCES,0.8596059113300493,"E.1
Bridging the Modality Gap Leads to Better Image Proxies"
REFERENCES,0.8620689655172413,"In Section 2.3 and Figure 2, we analyze whether generated text data can act as good image proxies.
Our conclusion is that due to the Modality Gap, text samples cannot directly serve as an effective
substitute for images in model evaluation. To demonstrate that our method SWAB can bridge this
Modality Gap and thereby make text samples a better substitute for images, we conduct the following
experiment."
REFERENCES,0.8645320197044335,"From the Figure 4, it is evident that the predicted model accuracy calculated using the modified
text samples is closer to the true model accuracy compared to that calculated with the original text
samples. This suggests that bridging the Modality Gap leads to better image proxies."
REFERENCES,0.8669950738916257,"We use ImageNet as our dataset. First, we employ the method introduced in Section 3.2 to predict
the gap vectors for each class of the target dataset based on gap vectors calculated on open-source
datasets. Then, we add the corresponding class’s predicted gap vectors to the generated text data of
ImageNet to bridge the modality gap. Finally, we calculate the zero-shot classification accuracy of
different models on these modified text data. To measure the consistency between the predicted Top-1"
REFERENCES,0.8694581280788177,"�= �. ��
���= �. ��"
REFERENCES,0.8719211822660099,"Text Samples
Modified Text Samples"
REFERENCES,0.874384236453202,"�= �. ��
���= �. ��"
REFERENCES,0.8768472906403941,"Figure 4: Comparison of the consistency metrics between the accuracy calculated using text data
before and after bridging the gap and the model’s true accuracy. After bridging the modality gap, the
text data act as better substitutes for image data to evaluate the model’s performance."
REFERENCES,0.8793103448275862,"accuracy and the true image classification accuracy, we calculate the Kendall Rank Correlation (τ,
higher is better) and Mean Absolute Error (MAE, lower is better). We compare the consistency
metrics of text data and modified text data. It can be observed that the consistency metrics of modified
text data are better, which proves our method can reduce the gap between the generated text data and
the image data."
REFERENCES,0.8817733990147784,"E.2
Analysis of the Modality Gap in BLIP and BEiT-3"
REFERENCES,0.8842364532019704,"We expand the VLM Zoo provided by LOVM by adding various variants of BLIP and BEiT-3 to
enhance the diversity of the VLM Zoo. We find that the zero-shot image classification performance
of the base models of BLIP and BEiT-3 is poor. Therefore, we use the retrieval version of the
models provided officially. These models are obtained by fine-tuning the base models on COCO
and Flickr30k through contrastive learning, thereby possessing better zero-shot image classification
performance. During the pre-training of BLIP and BEiT-3, they may have used multiple loss functions
in addition to the contrastive loss (BLIP), or perhaps they did not use contrastive loss at all (BEiT-3).
Therefore, the existence of the Modality Gap in the feature spaces of these two models requires
further investigation."
REFERENCES,0.8866995073891626,"We use image samples from the STL-10 dataset and class-related text samples generated by ChatGPT.
For each modality, we randomly extract 200 samples and calculate image-to-image, text-to-text, and
image-to-text similarities, and plot histograms. We also observe the presence of Modality Gaps in
different models through UMAP visualization [38]. Figure 5 and Figure 6 show the experiment result.
Through our experiments, we verify that the feature spaces of the BLIP and BEiT-3 retrieval models
still exhibit the Modality Gap phenomenon."
REFERENCES,0.8891625615763546,"Figure 5: The distribution of image-to-image (i2i) cosine similarity, text-to-text (t2t) cosine similarity,
and image-to-text (i2t) cosine similarity values for different BEiT-3 and BLIP models."
REFERENCES,0.8916256157635468,"Figure 6: UMAP visualization of image sample features and text sample features from different
BEiT-3 and BLIP models."
REFERENCES,0.8940886699507389,"E.3
Experiment Result on LOVM’s original VLM Zoo"
REFERENCES,0.896551724137931,"We have provided experimental results on the VLM Zoo originally provided by LOVM [73]. This
VLM Zoo contains 35 VLMs. Table 9 shows the experimental results, and we can see that SWAB
achieves the best performance across all evaluation metrics."
REFERENCES,0.8990147783251231,"Table 9: Results on LOVM’s original VLM Zoo. We evaluate our method across 23 datasets and 35
pre-trained VLMs. The results are averaged over all datasets."
REFERENCES,0.9014778325123153,"Methods
H-Score
NCE
LEEP
LogME
INB
Avg Rank
ModelGPT
SWAB"
REFERENCES,0.9039408866995073,"R5(↑)
0.183
0.235
0.139
0.200
0.443
0.443
0.446±0.004
0.498±0.005
τ(↑)
0.000
0.000
0.014
-0.014
0.267
0.261
0.272±0.010
0.310±0.012"
REFERENCES,0.9064039408866995,"R5 + τ(↑)
0.183
0.235
0.153
0.186
0.710
0.704
0.718±0.013
0.808±0.011"
REFERENCES,0.9088669950738916,"E.4
Per-Dataset Experiment Result"
REFERENCES,0.9113300492610837,"We present the per-dataset performance comparison between our methods SWAB and ModelGPT on
various datasets of LOVM benchmark in Table 10."
REFERENCES,0.9137931034482759,"Table 10: LOVM Benchmark (top-1 accuracy).We compare the per-dataset experiment result (fixing
the random seed) between ModelGPT and SWAB."
REFERENCES,0.916256157635468,Stanford Cars
REFERENCES,0.9187192118226601,CIFAR100
REFERENCES,0.9211822660098522,CLEVR-DIST.
REFERENCES,0.9236453201970444,CLEVR-COUNT
REFERENCES,0.9261083743842364,Country211
REFERENCES,0.9285714285714286,Retinopathy DMLab DTD
REFERENCES,0.9310344827586207,EuroSAT
REFERENCES,0.9334975369458128,FER2013
REFERENCES,0.9359605911330049,Flowers102 GTSRB
REFERENCES,0.9384236453201971,ImageNet KITTI MNIST PCam
REFERENCES,0.9408866995073891,Oxford Pets
REFERENCES,0.9433497536945813,Rendered SST2
REFERENCES,0.9458128078817734,RESISC45 STL10
REFERENCES,0.9482758620689655,SUN397 SVHN
REFERENCES,0.9507389162561576,VOC2007 Mean R5
REFERENCES,0.9532019704433498,"ModelGPT
0.80 0.80 0.00 0.40 0.60 0.00 0.00 0.80 0.60 0.40 0.80 0.60 0.80 0.00 0.00 0.00 0.80 0.20 0.80 0.20 0.60 0.60 0.60
0.452
SWAB
0.80 0.80 0.00 0.60 0.40 0.20 0.00 0.80 0.60 0.60 0.80 0.60 1.00 0.00 0.40 0.00 0.80 0.20 1.00 0.20 0.60 0.60 0.60
0.504 τ"
REFERENCES,0.9556650246305419,"ModelGPT
0.67 0.67 0.00 0.00 -0.33 0.00 0.00 0.67 1.00 0.00 0.67 1.00 0.67 0.00 0.00 0.00 0.00 0.00 0.67 0.00 -1.00 0.33 1.00
0.261
SWAB
0.67 0.91 0.00 0.33 0.00 0.00 0.00 0.55 1.00 0.33 0.33 1.00 0.89 0.00 0.00 0.00 0.33 0.00 0.67 0.00 -0.82 0.82 0.33
0.320"
REFERENCES,0.958128078817734,NeurIPS Paper Checklist
CLAIMS,0.9605911330049262,1. Claims
CLAIMS,0.9630541871921182,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Please refer to the bullet point summary at the end of the Introduction.
2. Limitations"
CLAIMS,0.9655172413793104,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [No]
Justification: We did not find significant drawbacks in the method.
3. Theory Assumptions and Proofs"
CLAIMS,0.9679802955665024,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justification: Our work does not involve theoretical result.
4. Experimental Result Reproducibility"
CLAIMS,0.9704433497536946,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide our experiment details in Section 4.1 and Appendix D.
5. Open access to data and code"
CLAIMS,0.9729064039408867,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will open-source our code, models, and datasets in the GitHub repository.
6. Experimental Setting/Details"
CLAIMS,0.9753694581280788,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide these details in Section 4.1.
7. Experiment Statistical Significance"
CLAIMS,0.9778325123152709,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]"
CLAIMS,0.9802955665024631,"Justification: To ensure the reliability of our results, we conducted multiple experiments for
results with randomness and reported the mean performance and standard deviation in Table
1.
8. Experiments Compute Resources"
CLAIMS,0.9827586206896551,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: We will place the relevant information in the GitHub repository.
9. Code Of Ethics"
CLAIMS,0.9852216748768473,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We have read and fully comply with the Code of Ethics.
10. Broader Impacts"
CLAIMS,0.9876847290640394,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [No]
Justification: Our work does not involve negative social impacts.
11. Safeguards"
CLAIMS,0.9901477832512315,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper does not involve these situations.
12. Licenses for existing assets"
CLAIMS,0.9926108374384236,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We ensure this requirement is met.
13. New Assets"
CLAIMS,0.9950738916256158,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not involve new assets.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9975369458128078,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing and research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve this."
