Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017921146953405018,"In the context of classification, domain generalization (DG) aims to predict the
1"
ABSTRACT,0.0035842293906810036,"labels of unseen target-domain data using only labeled source-domain data, where
2"
ABSTRACT,0.005376344086021506,"the source and target domains usually share the same label set. However, in
3"
ABSTRACT,0.007168458781362007,"the context of regression, DG is not well studied in the literature, and the main
4"
ABSTRACT,0.008960573476702509,"reason is that the ranges of response variables in two domains are often different,
5"
ABSTRACT,0.010752688172043012,"even disjoint under some extreme conditions. In this paper, we systematically
6"
ABSTRACT,0.012544802867383513,"investigate domain generalization in the regression setting and propose a weighted
7"
ABSTRACT,0.014336917562724014,"meta-learning strategy to obtain optimal initialization across domains to tackle
8"
ABSTRACT,0.016129032258064516,"the challenge. Unlike classification, the labels (responding values) in regression
9"
ABSTRACT,0.017921146953405017,"naturally have ordinal relatedness. The relatedness brings a core challenge in
10"
ABSTRACT,0.01971326164874552,"meta-learning for regression: the hard meta-tasks with less ordinal relatedness
11"
ABSTRACT,0.021505376344086023,"are under-sampled from training domains. To further address the hard meta-tasks,
12"
ABSTRACT,0.023297491039426525,"we adopt the feature discrepancy to calculate the discrepancy between any two
13"
ABSTRACT,0.025089605734767026,"domains and take the discrepancy as the importance of meta-tasks in the meta-
14"
ABSTRACT,0.026881720430107527,"learning framework. Extensive regression experiments on the standard benchmark
15"
ABSTRACT,0.02867383512544803,"DomainBed demonstrate the superiority of the proposed method.
16"
INTRODUCTION,0.03046594982078853,"1
Introduction
17"
INTRODUCTION,0.03225806451612903,"Domain generalization (DG) receives increasing attention due to its challenging setting: learning
18"
INTRODUCTION,0.034050179211469536,"models on source domains and inferring on unseen but related target domains [1, 2]. However,
19"
INTRODUCTION,0.035842293906810034,"most existing approaches focus on semantically invariant representations for classification, limiting
20"
INTRODUCTION,0.03763440860215054,"their practical applications to regression tasks. For example, real-world applications often involve
21"
INTRODUCTION,0.03942652329749104,"predicting the recovery/survival time of patients in clinic or estimating the ages/skeleton joints/gaze
22"
INTRODUCTION,0.04121863799283154,"direction of humans [3, 4, 5]. These tasks can be grouped into cross-domain regression problems.
23"
INTRODUCTION,0.043010752688172046,"In cross-domain regression, the label’s marginal distribution shift can differ significantly compared to
24"
INTRODUCTION,0.044802867383512544,"DG for classification. In DG classification, the shift typically represents variations in class probability
25"
INTRODUCTION,0.04659498207885305,"densities across domains [6]. In regression, the shift can take on a specific form, e.g., when the
26"
INTRODUCTION,0.04838709677419355,"responding (regression) interval of the source domain is [0, 0.7], the shifted responding interval of the
27"
INTRODUCTION,0.05017921146953405,"target domain can be [0.5, 1]. This type of shift often occurs in regression settings such as predicting
28"
INTRODUCTION,0.05197132616487455,"unseen ages, depths and rentals. In some cases, these regression intervals even have no overlap.
29"
INTRODUCTION,0.053763440860215055,"We refer to this particular regression scenario as domain generalization in regression (DGR). Fig. 1
30"
INTRODUCTION,0.05555555555555555,"illustrates the differences between imbalanced domain regression and the DGR. Unlike imbalanced
31"
INTRODUCTION,0.05734767025089606,"regression [7], DGR focuses on exploration or interpolation for regression.
32"
INTRODUCTION,0.05913978494623656,"Comparisons to traditional DG. From the perspective of domain generalization, DGR can be
33"
INTRODUCTION,0.06093189964157706,"viewed as a special generalization case where the target labels are continuous. However, most domain
34"
INTRODUCTION,0.06272401433691756,"generalization methods are suboptimal for addressing the DGR problem due to the ordinal relatedness
35"
INTRODUCTION,0.06451612903225806,"of regression labels. For example, feature alignment [8] might be unnecessary and even harmful in our
36"
INTRODUCTION,0.06630824372759857,"DGR setting. Assuming that a closer feature discrepancy implies closer predictions, feature alignment
37"
INTRODUCTION,0.06810035842293907,"methods may cause the model to exclusively map all predictions into one source interval, which
38"
INTRODUCTION,0.06989247311827956,"(a) Imbalanced Regression
(b) Generalized Regression "
INTRODUCTION,0.07168458781362007,"Y ∈[0, 1] P(Y)"
INTRODUCTION,0.07347670250896057,Source 
INTRODUCTION,0.07526881720430108,Target P(Y)
INTRODUCTION,0.07706093189964158,"Y ∈[0, 1] P(Y)"
INTRODUCTION,0.07885304659498207,Source
INTRODUCTION,0.08064516129032258,"Y ∈[0, 0.7] "
INTRODUCTION,0.08243727598566308,Target P(Y)
INTRODUCTION,0.08422939068100359,"Y ∈[0.5, 1]"
INTRODUCTION,0.08602150537634409,"Figure 1: The label distributions of two different regression settings. (a) In the imbalanced domain
regression, the response values Y ∈[0, 1] exhibit varying probability densities across domains. (b)
The DGR problem focuses on predicting unseen response values in the target domain. The response
values might encompass both overlapping (just like source interval [0, 0.7] and target interval [0.5, 1])
and non-overlapping intervals."
INTRODUCTION,0.08781362007168458,"does not reduce total generalization risks. In addition to feature alignment, feature disentanglement
39"
INTRODUCTION,0.08960573476702509,"usually disentangles semantically related discriminant representation for classification [9], while
40"
INTRODUCTION,0.0913978494623656,"overlooking the ordinal relatedness of the target domain. Furthermore, semantic-related discriminant
41"
INTRODUCTION,0.0931899641577061,"representation might be unnecessary for regression tasks like age estimation. Robust optimization
42"
INTRODUCTION,0.09498207885304659,"methods [10] can perform moderately distributional exploration, but also lack the ability to tackle
43"
INTRODUCTION,0.0967741935483871,"ordinal relatedness in regression.
44"
INTRODUCTION,0.0985663082437276,"Comparisons to open-set DG [1, 11]. Open-set DG primarily focuses on classification applications
45"
INTRODUCTION,0.1003584229390681,"and the ability to detect unknown classes. If open-set DG methods are used to address our problem,
46"
INTRODUCTION,0.10215053763440861,"they can only identify these samples whose response intervals differ from that of the source domain
47"
INTRODUCTION,0.1039426523297491,"but cannot obtain their response values.
48"
INTRODUCTION,0.1057347670250896,"To effectively capture ordinal relations and facilitate modest extrapolation in the DGR problem,
49"
INTRODUCTION,0.10752688172043011,"we propose a robust optimization algorithm via meta-learning. Meta-learning algorithms, e.g.,
50"
INTRODUCTION,0.10931899641577061,"model agnostic meta-learning (MAML, [12]) have been extensively utilized in traditional domain
51"
INTRODUCTION,0.1111111111111111,"generalization [13, 14, 15]. In each meta-task, these methods usually sample a support and a query
52"
INTRODUCTION,0.11290322580645161,"classification task from two distinct domains and optimize the meta-model by a bi-level paradigm.
53"
INTRODUCTION,0.11469534050179211,"However, this paradigm alone falls short for addressing the complexities of DGR. The task sampling
54"
INTRODUCTION,0.11648745519713262,"strategy employed in these methods typically follows an implicit assumption, assuming that all
55"
INTRODUCTION,0.11827956989247312,"training meta-tasks have equal importance [16, 17]. We argue that this implicit assumption no longer
56"
INTRODUCTION,0.12007168458781362,"holds in our regression setting.
57"
INTRODUCTION,0.12186379928315412,"In contrast to classification, regression tasks exhibit ordinal relations between each pair of labels
58"
INTRODUCTION,0.12365591397849462,"[18]. When considering the label discrepancy between the support and query domains, it is observed
59"
INTRODUCTION,0.12544802867383512,"that meta-tasks with a larger regression margin are sampled less frequently compared to those
60"
INTRODUCTION,0.12724014336917563,"with a smaller margin. Additionally, meta-tasks with a larger regression margin tend to be more
61"
INTRODUCTION,0.12903225806451613,"challenging to optimize within the meta-learning framework. These key factors bring a sampling
62"
INTRODUCTION,0.13082437275985664,"bias that harder meta-tasks are less sampled from training data. Consequently, the sampling bias
63"
INTRODUCTION,0.13261648745519714,"makes harder meta-tasks underrepresented in the training data, i.e., the meta-model tends to choose
64"
INTRODUCTION,0.13440860215053763,"the easier meta-tasks, limiting the exploration and interpolation capabilities of the model. To mitigate
65"
INTRODUCTION,0.13620071684587814,"this sampling bias, we propose a simple yet effective strategy: assigning higher weights to harder
66"
INTRODUCTION,0.13799283154121864,"meta-tasks. These weights are computed based on the feature discrepancy between the query and
67"
INTRODUCTION,0.13978494623655913,"support examples of each meta-task.
68"
INTRODUCTION,0.14157706093189965,"In conclusion, we have developed a DGR benchmark that encompasses both overlapping and non-
69"
INTRODUCTION,0.14336917562724014,"overlapping labels between the source and target domains. We conduct experiments on three
70"
INTRODUCTION,0.14516129032258066,"regression tasks, including causality exploration with a toy logic dataset, predicting unseen ages
71"
INTRODUCTION,0.14695340501792115,"according to face images, and forecasting rental prices across different regions. Our proposed method,
72"
INTRODUCTION,0.14874551971326164,"named margin-aware meta regression (MAMR), makes the following main contributions:
73"
INTRODUCTION,0.15053763440860216,"• We investigate generalized regression from the perspective of domain generalization, a
74"
INTRODUCTION,0.15232974910394265,"previously understudied area with significant practical implications.
75"
INTRODUCTION,0.15412186379928317,"• To enhance exploration and interpolation capabilities, we introduce a margin-aware meta-
76"
INTRODUCTION,0.15591397849462366,"learning framework that mitigates sampling bias and encourages the model to recognize
77"
INTRODUCTION,0.15770609318996415,"long-range ordinal relations.
78"
INTRODUCTION,0.15949820788530467,"• Although our solution achieves considerable improvements regarding baselines, our empiri-
79"
INTRODUCTION,0.16129032258064516,"cal analyses demonstrate that generalizing to unseen responses is still challenging.
80"
RELATED WORK,0.16308243727598568,"2
Related Work
81"
RELATED WORK,0.16487455197132617,"In this section, two related research areas are briefly introduced. One is domain adaptation for ordinal
82"
RELATED WORK,0.16666666666666666,"regression and classification, and the other one is generalization for regression.
83"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.16845878136200718,"2.1
Domain Adaptation for Ordinal Regression and Classification
84"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.17025089605734767,"Domain adaptation aims to migrate the knowledge from a source domain to a target domain, where
85"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.17204301075268819,"there may exist a distribution shift between them. Typical domain adaptation methods try to get confi-
86"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.17383512544802868,"dent decision boundaries for classification tasks based on clustering assumption [19]. However, when
87"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.17562724014336917,"it comes to cross-domain regression (also known as ordinal classification [18]), these assumptions
88"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1774193548387097,"are not satisfied, posing challenges for existing domain adaptation methods. Some pioneer works
89"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.17921146953405018,"like [20] try to provide regression discrepancy in reproducing kernel Hilbert space. Most recent
90"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.18100358422939067,"works address cross-domain regression in specific application scenarios, such as estimating object
91"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1827956989247312,"boxes in cross-domain/few-shot object detection [21, 17], regressing human skeleton key-points in
92"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.18458781362007168,"cross-domain gesture estimation [4] and calculating the gaze direction in cross-domain gaze tracing
93"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1863799283154122,"[22]. Furthermore, [3] proposes a general cross-domain regression method via subspace alignment,
94"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1881720430107527,"which reduces domain gap by minimizing representation subspace distance (RSD) with the principal
95"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.18996415770609318,"angles of representation matrices. [23] proposes an adversarial dual regressor to achieve a direct
96"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1917562724014337,"alignment between two domains.
97"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1935483870967742,"However, nearly all cross-domain regression methods inherently assume there only exists covariate
98"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1953405017921147,"shift in input examples, i.e., p(xs) ̸= p(xt), where p(·) is the probability density function and xs, xt
99"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1971326164874552,"denote the source and target examples. This assumption implies that these methods may not be
100"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.1989247311827957,"capable of handling label shift across domains. The label shift in cross-domain regression can arise
101"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2007168458781362,"as interval shift of responding values, e.g., the source interval ys ∈[0.3, 0.5] while the target interval
102"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2025089605734767,"yt ∈[0.6, 0.7]. The responding values in the real world can be gasoline consumption data and
103"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.20430107526881722,"vary significantly across developed and developing countries [24]. [25] also considers the interval
104"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2060931899641577,"shift problem and tries to learn a ranking on the target domain, followed by mapping the ranking
105"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2078853046594982,"to responding values. This method assumes the availability of the responding interval on the target
106"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.20967741935483872,"domain at the adaptation stage, which might be contradictory to the setting of unavailable labels.
107"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2114695340501792,"In contrast, we assume all target domain data are not available at the training stage, which is more
108"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2132616487455197,"practical and challenging in real-world scenarios.
109"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.21505376344086022,"2.2
Generalization/Causality for Regression
110"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2168458781362007,"Domain generalization introduces a more challenging setting where the model can only access the
111"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.21863799283154123,"labeled source data at the training stage [1, 2, 26, 27, 28, 29, 30, 31]. A thorough discussion of
112"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.22043010752688172,"domain generalization might exceed the scope of our paper. We focus on potential methods that
113"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.2222222222222222,"can be applied to regression settings. Among existing generalization methods, some works try to
114"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.22401433691756273,"generalize to continuous outputs by capturing causal relations [32, 33]. Recent works like DDG [9]
115"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.22580645161290322,"concentrate on capturing invariant semantic features, which might overlook the variational features
116"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.22759856630824374,"for continuous predictions. In contrast, the meta-learning paradigm holds potential for regression
117"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.22939068100358423,"settings due to its model-agnostic property and strong generalization ability.
118"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.23118279569892472,"The spearhead work MLDG [13] introduces MAML [12] into the domain generalization framework.
119"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.23297491039426524,"[14] leverages class relationships and local sample clustering to capture the semantic features of
120"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.23476702508960573,"different classes. These two operations are hard to be migrated to regression settings because the
121"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.23655913978494625,"clustering assumption is usually not reasonable for regression. Moreover, in many regression tasks
122"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.23835125448028674,"like age estimation, the semantic features might be unimportant, e.g., distinguishing each face might
123"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.24014336917562723,"be useless for age regression. Instead, the style features, like the texture of the faces might be
124"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.24193548387096775,"important information for age regression. Moreover, [30] proposes an implicit gradient to get stable
125"
DOMAIN ADAPTATION FOR ORDINAL REGRESSION AND CLASSIFICATION,0.24372759856630824,"meta-learning loss, which may provide orthogonal solution compared to our method.
126"
PROBLEM SETTING AND NOTATIONS,0.24551971326164876,"3
Problem Setting and Notations
127"
PROBLEM SETTING AND NOTATIONS,0.24731182795698925,"In this section, we introduce the formal definition of the DGR problem. We denote the input
128"
PROBLEM SETTING AND NOTATIONS,0.24910394265232974,"space and the label space by X and Y, where Y has a continuous range from 0 to 1 and can
129"
PROBLEM SETTING AND NOTATIONS,0.25089605734767023,"include two sub-spaces, e.g., Ysource and Ytarget. Ds = {(x, y) ∈{X × Ysource}} and Dt =
130"
PROBLEM SETTING AND NOTATIONS,0.25268817204301075,"{(x, y) ∈{X × Ytarget}} respectively denote the source and target domain data. The model can
131"
PROBLEM SETTING AND NOTATIONS,0.25448028673835127,"only utilize Ds at the training stage, and then predicts labels in Dt without further adaptation. The
132"
PROBLEM SETTING AND NOTATIONS,0.25627240143369173,"above settings are very similar to the classification tasks of domain generalization. But the label
133"
PROBLEM SETTING AND NOTATIONS,0.25806451612903225,"spaces across domains are different in our regression setting. A prediction ˆy from regression model R
134"
PROBLEM SETTING AND NOTATIONS,0.25985663082437277,"can be denoted with ˆy = R(x) = G(F(x)). We use F : X →Z to denote a feature encoder, where
135"
PROBLEM SETTING AND NOTATIONS,0.2616487455197133,"Z is a feature space. After the encoder, we use a linear regressor with sigmoid activation to map the
136"
PROBLEM SETTING AND NOTATIONS,0.26344086021505375,"range of predictions into [0, 1], i.e., G : Z →Y.
137"
MARGIN-AWARE META REGRESSION,0.26523297491039427,"4
Margin-Aware Meta Regression
138"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2670250896057348,"4.1
Distribution Alignment Produces Regression Margin
139"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.26881720430107525,"Following the typical setting of domain generalization that domain labels are available. We split Ds
140"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.27060931899641577,"into K source domains {D1, D2, · · · , DK} and simulate the generalization setting between Ds and
141"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2724014336917563,"Dt. As we know, feature alignment is the core idea of many typical domain alignment solutions
142"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.27419354838709675,"for domain adaptation [34] as well as domain generalization [8]. For domain generalization, the
143"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.27598566308243727,"alignment is usually performed among multiple source domains to find domain-invariant semantic
144"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2777777777777778,"features. This alignment can be formalized using a general discrepancy measure, i.e., integral
145"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.27956989247311825,"probability metric (IPM, [35]). Let X1, X2 denote two independent random variables from domain
146"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.28136200716845877,"distributions Pi and Pj. The domain discrepancy can be defined with:
147"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2831541218637993,"IPM(Pi, Pj) := sup
f∈H
[E[f(X1)] −E[f(X2)]],
(1)"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2849462365591398,"where E denotes the expectation, f denotes the transformation function in function space H. Applying
148"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2867383512544803,"specific condition on H, IPM can be transformed into many popular measures, such as maximum
149"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2885304659498208,"mean discrepancy (MMD, [36]) and wasserstein distance (WD, [37]).
150"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2903225806451613,"Incorporating the domain discrepancy between Pi and Pj, the objective of the regressor can be
151"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2921146953405018,"formulated as:
152"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2939068100358423,"min
Θ
sup
(x1,y1)∈Di,
(x2,y2)∈Dj"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2956989247311828,"h
LΘ(x1, y1) + LΘ(x2, y2) + d
IPM(x1, x2)
i
,
(2)"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2974910394265233,"where Θ is model parameter, LΘ(x, y) = ||RΘ(x) −y)|| is the empirical risk and can be the squared
153"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.2992831541218638,"loss, d
IPM is the estimator from two batch examples x1 and x2. For example, d
IPM can be the unbiased
154"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3010752688172043,"U-statistic estimator \
MMD
2
u(x1, x2) [36]. In general domain generalization for classification tasks,
155"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.30286738351254483,"all terms in the above objective could be minimized. However, our regression setting is like open
156"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3046594982078853,"domain generalization, which learns a model from the source domain and inferences in unseen target
157"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3064516129032258,"domains with novel classes [11]. To regress unseen target values, one strategy is to simulate the
158"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.30824372759856633,"setting in the training stage. That means the labels in Di and Dj have few or no overlaps. Therefore,
159"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3100358422939068,"when the domain discrepancy d
IPM is minimized, there might be only one term minimized between
160"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3118279569892473,"LΘ(x1, y1) and LΘ(x2, y2). This problem can be formally introduced with the following definition:
161"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.31362007168458783,"Proposition 1 (Regression Margin). Let (X1, Y1) and (X2, Y2) be the random variables correspond-
162"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3154121863799283,"ing to two source domains Di, Dj, the [a, b] and [c, d] be the regression interval of Y1, Y2. When
163"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3172043010752688,"d
IPM is reduced to 0 for a function f, we have
164"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.31899641577060933,"Mi,j = inf |E[f(X1) −Y1] −E[f(X2) −Y2]|
(3)
= inf |(E[f(X1)] −E[f(X2)]) + E[Y2 −Y1]|
(4)
= min(|c −b|, |a −d|).
(5)"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3207885304659498,"The regression margin represents the minimal margin (or difference) between errors in the two
165"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3225806451612903,"domains (i.e., Eq. (3)). Eq. (4) is the rearrangement of Eq. (3). In Eq. (4), because d
IPM is reduced
166"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.32437275985663083,"to 0 for the function f, E[f(X1)] −E[f(X2)] = 0, then obtaining the Eq. (5). The above analysis
167"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.32616487455197135,"suggests that a large domain margin Mi,j can lead to a divergent optimization when simultaneously
168"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3279569892473118,"minimizing the domain discrepancy and the empirical risks. One strategy is to bypass explicit feature
169"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.32974910394265233,"alignment. For example, in the meta-learning paradigm towards domain generalization, one can learn
170"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.33154121863799285,"a meta-model by a bi-level optimization. In the inner optimization, the model learns on a support
171"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3333333333333333,"(source) domain. In the outer optimization, the learned model tries to generalize to a query (target)
172"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.33512544802867383,"domain. This training strategy naturally avoids explicit feature alignment. Moreover, the bi-level
173"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.33691756272401435,"optimization emphasizes the importance of query loss, which might alleviate the above regression
174"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.3387096774193548,"margin because the inner model and outer model can be viewed as different sampling instances in
175"
DISTRIBUTION ALIGNMENT PRODUCES REGRESSION MARGIN,0.34050179211469533,"parameter space.
176"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.34229390681003585,"4.2
Regression Margin Leads to Sampling Bias in Meta-Learning
177"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.34408602150537637,"Existing meta-learning domain generalization methods are sub-optimal for the DGR problem. In
178"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.34587813620071683,"the classification, each meta-task consisting of support tasks and query tasks is assumed to have the
179"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.34767025089605735,"same sampling probability. However, the responding intervals of the support and query have ordinal
180"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.34946236559139787,"relations in regression. When the regression margin between the support and query tasks is larger, the
181"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.35125448028673834,"sampling probability is smaller. The left part of Fig. 2 depicts the relationship between the regression
182"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.35304659498207885,"margin and the sampling strategies of meta-tasks. Intuitively thinking about the extreme case that
183"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3548387096774194,"when the regression margin is close to 1, the corresponding sampling probability of meta-tasks is
184"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.35663082437275984,"close to 0. We formalize this using a simple theorem:
185"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.35842293906810035,"Theorem 1 (Sampling Bias in Meta-Learning). Given a support domain i, let S(j|i) denote the
186"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3602150537634409,"number of available query domain j that can be sampled. Let M 1
i,j, M 2
i,j denote the regression
187"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.36200716845878134,"margin of the meta-task 1 and meta-task 2. if M 1
i,j > M 2
i,j, then S1
(j|i) < S2
(j|i).
188"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.36379928315412186,"The intuitive explanation is: the number of sampling strategies of a larger regression margin meta-task
189"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3655913978494624,"is always less than a small margin meta-task. We will provide a simple and intuitive proof below.
190"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3673835125448029,"Proof. Following the previous description, the source data Ds can be sorted into K disjoint source
191"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.36917562724014336,"domains {D1, D2, · · · , DK} according to their regression interval. The query and support tasks are
192"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3709677419354839,"sampled from Di, Dj with regression interval [a, b] and [c, d] respectively. Let ∆denote the length
193"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3727598566308244,"of single regression interval, n = Mi,j"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.37455197132616486,"∆
denote the number of spanning intervals of regression margin
194"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3763440860215054,"Mi,j. Given a support task on domain index i, the query tasks on j-th domain have S(j|i) choices:
195"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3781362007168459,"S(j|i) = 
 "
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.37992831541218636,"K −(i + n),
if i ≤n
(i −n),
if i > K −n
K −2n + 1,
if i > n and i ≤K −n
(6)"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3817204301075269,"From the above equation, when the regression margin Mi,j increases (i.e., n is increasing), the number
196"
REGRESSION MARGIN LEADS TO SAMPLING BIAS IN META-LEARNING,0.3835125448028674,"of available-to-sample query tasks decreases, leading to a smaller number of eligible meta-tasks.
197"
MARGIN-AWARE META-TRAINING,0.38530465949820786,"4.3
Margin-Aware Meta-Training
198"
MARGIN-AWARE META-TRAINING,0.3870967741935484,"As illustrated by the left part of Fig. 2, a larger regression margin between the support and query
199"
MARGIN-AWARE META-TRAINING,0.3888888888888889,"tasks usually means a harder meta-task. Therefore, without any specialized sampling strategy, the
200"
MARGIN-AWARE META-TRAINING,0.3906810035842294,"meta model is prone to be biased towards the small margin tasks. To alleviate this issue, we want the
201"
MARGIN-AWARE META-TRAINING,0.3924731182795699,"large margin meta-task to have a larger weight in the meta-learning process. One direct strategy is to
202"
MARGIN-AWARE META-TRAINING,0.3942652329749104,"calculate the weight using the domain discrepancy, i.e., a larger regression margin means a larger
203"
MARGIN-AWARE META-TRAINING,0.3960573476702509,"meta-task weight. The learning objective can be redefined with:
204"
MARGIN-AWARE META-TRAINING,0.3978494623655914,"min
Θ
sup
(xq,yq)∈Di,
(xs,ys)∈Dj"
MARGIN-AWARE META-TRAINING,0.3996415770609319,"LΘ′(xq, yq) · d(xs, xq)
s.t. Θ
′ = Θ −β∇Θ [LΘ(xs, ys] ,
(7)"
MARGIN-AWARE META-TRAINING,0.4014336917562724,"where Di, Dj respectively denote the query domain and the support domain, d is discrepancy
205"
MARGIN-AWARE META-TRAINING,0.4032258064516129,"functions like \
MMD
2
u(·, ·) or simple Euclidean metric, and β is the inner loop learning rate on the
206"
MARGIN-AWARE META-TRAINING,0.4050179211469534,"support domain {xs, ys}.
207"
MARGIN-AWARE META-TRAINING,0.4068100358422939,"The graphical training process of one meta-task can be seen in the right part of Fig. 2. Different from
208"
MARGIN-AWARE META-TRAINING,0.40860215053763443,"existing meta-learning models, our MAMR model considers the domain discrepancy by discrepancy
209"
MARGIN-AWARE META-TRAINING,0.4103942652329749,"function d(·), but the data node in d(xs, xq) does not have gradients. The reason is directly minimizing
210"
MARGIN-AWARE META-TRAINING,0.4121863799283154,"this domain discrepancy might harm the generalization ability of our MAMR model. Our task
211"
MARGIN-AWARE META-TRAINING,0.41397849462365593,"weighting method is similar to recent sharpness-aware minimization [38], which simultaneously
212"
MARGIN-AWARE META-TRAINING,0.4157706093189964,"minimizes loss value and loss sharpness. The related topic can also have an extension to penalizing
213"
MARGIN-AWARE META-TRAINING,0.4175627240143369,"gradient norm [39] and independence-driven importance weighting [40]. With Euclidean distance
214"
MARGIN-AWARE META-TRAINING,0.41935483870967744,"d(·), we describe the detailed method in Algorithm 1.
215"
MARGIN-AWARE META-TRAINING,0.4211469534050179,"Query Set
Support Set 
Query Set"
MARGIN-AWARE META-TRAINING,0.4229390681003584,Meta-Task 2
MARGIN-AWARE META-TRAINING,0.42473118279569894,Meta-Task 1 Y
MARGIN-AWARE META-TRAINING,0.4265232974910394,"Inner
Inner Gradients"
MARGIN-AWARE META-TRAINING,0.4283154121863799,Outer Gradients
MARGIN-AWARE META-TRAINING,0.43010752688172044,Meta Model
MARGIN-AWARE META-TRAINING,0.43189964157706096,Task Model
MARGIN-AWARE META-TRAINING,0.4336917562724014,Data Flow
MARGIN-AWARE META-TRAINING,0.43548387096774194,"Figure 2:
Left: The graphical illustration of the regression margin with sampling strategies of
meta-tasks. Right: Our model’s training process. Note that in the training process, meta-models
share identical parameters Θ, and the blue data flow does not involve gradient backpropagation."
MARGIN-AWARE META-TRAINING,0.43727598566308246,"Algorithm 1 Training Algorithm of MAMR
Input: The source domains data Ds, the inner loop learning rate β, the out-loop learning rate α, the
domain number K to split Ds, model parameters Θ.
Output: The learned Θ."
MARGIN-AWARE META-TRAINING,0.4390681003584229,"1: Split the source data Ds into sub-domains {D1, D2, · · · DK}.
2: while not convergence do
3:
Sample T = K(K −1)/2 domain pairs {(Di, Dj)} that i ̸= j.
4:
for index = 0 →T do
5:
Sample a batch of support data (xs, ys) ∈Dj and query data (xq, yq) ∈Di;
6:
Compute task discrepancies: d(xs, xq) = ||F(xs) −F(xq)||2;
7:
Get task-specific model parameters: Θ
′ = Θ −β∇Θ [LΘ(xs, ys];
8:
Compute the weighted regression error: LΘ′(xq, yq) · d(xs, xq);
9:
Update Θ: Θ = Θ −α∇Θ [LΘ′(xq, yq) · d(xs, xq)];
10:
end for
11: end while"
EXPERIMENTS,0.44086021505376344,"5
Experiments
216"
EXPERIMENTS,0.44265232974910396,"In this section, we will empirically explore what MAMR can learn and compare it to related works
217"
EXPERIMENTS,0.4444444444444444,"from the view of performance and methodology, including introductions to baselines and experimental
218"
EXPERIMENTS,0.44623655913978494,"details, results on three datasets, and detailed analyses.
219"
BASELINES,0.44802867383512546,"5.1
Baselines
220"
BASELINES,0.449820788530466,"We use multiple domain generalization and the variants of domain adaptation methods as baselines,
221"
BASELINES,0.45161290322580644,"including: (1) risk minimization methods (ERM [41], IRM [42]); (2) feature alignments and robust
222"
BASELINES,0.45340501792114696,"optimization (MMD [8], CORAL [43], DANN [34], SD [44], Transfer [45]), MODE [10]; (3)
223"
BASELINES,0.4551971326164875,"subspace alignments (RSD [3]); (4) self-supervised and data augmentation methods (SelfReg [46],
224"
BASELINES,0.45698924731182794,"CAD [47], MTL [48]) (5) meta-learning (MLDG [13]) and (6) disentanglement and causality method
225"
BASELINES,0.45878136200716846,"(DDG [9], CausIRL [49]). All the introductions of baselines can be seen in Appendix A.
226"
TRAINING AND EVALUATION,0.460573476702509,"5.2
Training and Evaluation
227"
TRAINING AND EVALUATION,0.46236559139784944,"To ensure fairness and comparability, we put all the baselines into a public evaluation benchmark
228"
TRAINING AND EVALUATION,0.46415770609318996,"DomainBed [50]. For age regression, we uniformly use ResNet12 as the backbone encoder F for all
229"
TRAINING AND EVALUATION,0.4659498207885305,"methods. ResNet12 is a popular encoder in meta-learning for few-shot learning. For rental regression,
230"
TRAINING AND EVALUATION,0.46774193548387094,"we uniformly use a 5-layer MLP as the backbone encoder F. For regressor G, we use a single linear
231"
TRAINING AND EVALUATION,0.46953405017921146,"neural network followed by a sigmoid function. Note that all labels are normalized from 0 to 1.
232"
TRAINING AND EVALUATION,0.471326164874552,"Including toy experiments, all methods are implemented with Pytorch and can be executed on an
233"
TRAINING AND EVALUATION,0.4731182795698925,"NVIDIA RTX 3090 GPU. Appendix B provides detailed settings of the hyper-parameters, such as
234"
TRAINING AND EVALUATION,0.47491039426523296,"the learning rates, the training seeds, etc.
235"
TOY CAUSALITY DATASET AND RESULTS,0.4767025089605735,"5.3
Toy Causality Dataset and Results
236"
TOY CAUSALITY DATASET AND RESULTS,0.478494623655914,"To figure out what the MAMR model can learn in regression problems, we create a toy dataset in
237"
TOY CAUSALITY DATASET AND RESULTS,0.48028673835125446,"which the input examples and their responding values obey some causal mechanism. We assume the
238 X1 0.6 0.7 0.8"
TOY CAUSALITY DATASET AND RESULTS,0.482078853046595,"0.9
1.0 X2"
TOY CAUSALITY DATASET AND RESULTS,0.4838709677419355,"0.6
0.7
0.8
0.9
1.0 Y 0.7 0.8 0.9 1.0"
TOY CAUSALITY DATASET AND RESULTS,0.48566308243727596,(a) ERM (MSE: 0.0219) X1 0.6 0.7 0.8
TOY CAUSALITY DATASET AND RESULTS,0.4874551971326165,"0.9
1.0 X2"
TOY CAUSALITY DATASET AND RESULTS,0.489247311827957,"0.6
0.7
0.8
0.9
1.0 Y 0.7 0.8 0.9 1.0"
TOY CAUSALITY DATASET AND RESULTS,0.4910394265232975,(b) RSD (MSE:0.008) X1 0.6 0.7 0.8
TOY CAUSALITY DATASET AND RESULTS,0.492831541218638,"0.9
1.0 X2"
TOY CAUSALITY DATASET AND RESULTS,0.4946236559139785,"0.6
0.7
0.8
0.9
1.0 Y 0.7 0.8 0.9 1.0"
TOY CAUSALITY DATASET AND RESULTS,0.496415770609319,"(c) Ours (MSE:0.0004)
Figure 3: The toy experiments illustrate the ground truth test landscape (gray color) and prediction
regions (blue color). Each method’s performance is reported with Mean Squared Error (MSE)."
TOY CAUSALITY DATASET AND RESULTS,0.4982078853046595,"1-dimensional random variables X1 and X2 follow a uniform distribution in [0,1], and the responding
239"
TOY CAUSALITY DATASET AND RESULTS,0.5,"values Y are under the control of X1 and X2. The control mechanism can be complex as given in
240"
TOY CAUSALITY DATASET AND RESULTS,0.5017921146953405,"Appendix C. At training stage, regression models can only use X1 ∈[0, 0.6] and X2 ∈[0, 0.6]. At
241"
TOY CAUSALITY DATASET AND RESULTS,0.503584229390681,"the test stage, we record the regression values when given X1 ∈[0.6, 1] and X2 ∈[0.6, 1].
242"
TOY CAUSALITY DATASET AND RESULTS,0.5053763440860215,"The toy experiments sample 15000 and 10000 regression tasks at the training and test stage, respec-
243"
TOY CAUSALITY DATASET AND RESULTS,0.507168458781362,"tively. We use a 4-layers fully connected neural network for ERM, RSD and our MAMR. Fig. 3
244"
TOY CAUSALITY DATASET AND RESULTS,0.5089605734767025,"provides the test time explorations results of the three methods. On 10000 test tasks, the ground-truth
245"
TOY CAUSALITY DATASET AND RESULTS,0.510752688172043,"responding values and the predicted values respectively form a gray region and a blue region. When
246"
TOY CAUSALITY DATASET AND RESULTS,0.5125448028673835,"given unseen values of X1 and X2, ERM fails to use the causal mechanism. The strong baseline
247"
TOY CAUSALITY DATASET AND RESULTS,0.514336917562724,"method RSD captures a part of the causal mechanism. MAMR gets the best exploration performance
248"
TOY CAUSALITY DATASET AND RESULTS,0.5161290322580645,"by maximum causal discovery.
249"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.517921146953405,"5.4
Cross-Domain Age Estimation Datasets
250"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5197132616487455,"Perfect age estimation is based on the assumption that all age data are available, while many real-
251"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.521505376344086,"world datasets are not perfect and have partial ages due to privacy concerns. Hence age estimation
252"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5232974910394266,"has been introduced in cross-domain works [18, 51].
253"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.525089605734767,"CACD1. Cross-Age Celebrity Dataset (CACD) contains 163,446 images from 2,000 celebrities
254"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5268817204301075,"collected from the Internet. The age of celebrities ranges from 16-62 and can be classified into 5
255"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5286738351254481,"disjoint age intervals (domains), i.e., [15−20), [20−30), [30−40), [40−50), [50−60]. The images
256"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5304659498207885,"of each celebrity are sampled by different devices across multiple years. Therefore each domain
257"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.532258064516129,"has different facial characteristics. To consider the overlapped intervals, we further create CACD-O
258"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5340501792114696,"dataset, where each interval has 3 ages of neighbors, e.g., [15 −20) includes 8 different ages from 15
259"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.53584229390681,"to 22 and [20 −30) has 15 ages from 18 to 32.
260"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5376344086021505,"AFAD2. The Asian Face Age Dataset (AFAD) originally is an age estimation dataset containing more
261"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5394265232974911,"than 160K face images and aging labels. We split the dataset into 5 age intervals (domains), i.e.,
262"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5412186379928315,"[15 −20), [20 −25), [25 −30), [30 −35), [35 −40]. Like CACD, each age interval has its own face
263"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.543010752688172,"characteristics and can be viewed as 5 related domains for regression.
264"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.5448028673835126,"In each task, only one domain is viewed as the target domain, and the left is viewed as sources. Please
265"
CROSS-DOMAIN AGE ESTIMATION DATASETS,0.546594982078853,"refer to Appendix E for more details on these age estimation datasets.
266"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5483870967741935,"5.5
Cross-Domain Rental Prediction Dataset
267"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5501792114695341,"The Rental dataset 3 was released by an online competition in 2019 to predict housing rental in Shang
268"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5519713261648745,"Hai, China. The data categories include rental housing, regions, second-hand housing, supporting
269"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.553763440860215,"facilities, new houses, land, population, customers, real rent, etc. We split 15 regions into 4 groups as
270"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5555555555555556,"4 different domains. Each domain has different rentals due to its population and economic conditions.
271"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.557347670250896,"Please refer to Appendix D for more introduction to this dataset.
272"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5591397849462365,"1http://bcsiriuschen.github.io/CARC/
2https://afad-dataset.github.io/
3https://ai.futurelab.tv/contest_detail/3#contest_des"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5609318996415771,"Table 1: Regression results on 4 cross-domain datasets with training-domain validation. The ""Av-
erage"" denotes the average Mean Squared Errors on 4 datasets. The ""-"" denotes not comparable
results due to different architectures. The minimum values are bolded. Note that we set the standard
variances to 0 if they are less than 0.001. More performance details for each dataset can be seen
in Appendix D and Appendix E."
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5627240143369175,"Algorithms/Datasets
CACD
CACD-O
AFAD
Rental
Average"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5645161290322581,"ERM ([41], 1998)
0.0258±0.001
0.0236±0.000
0.0269±0.000
0.0477±0.003
0.0310
IRM ([42], 2019)
0.0368±0.017
0.0256±0.000
0.0285±0.001
0.0496±0.000
0.0351
MLDG ([13], 2018)
0.0260±0.000
0.0235±0.000
0.0268±0.001
0.0465±0.001
0.0307
MMD ([8], 2018)
0.0286±0.000
0.0263±0.000
0.0301±0.000
0.0461±0.000
0.0328
CORAL ([43], 2016)
0.0255±0.000
0.0231±0.000
0.0272±0.003
0.0615±0.019
0.0343
DANN ([34], 2016)
0.0269±0.000
0.0259±0.001
0.0290±0.001
0.0474±0.002
0.0323
SD ([44], 2021)
0.0248±0.000
0.0227±0.000
0.0270±0.001
0.0493±0.000
0.0598
MTL ([48], 2021)
0.1447±0.000
0.1456±0.000
0.2122±0.001
0.0467±0.001
0.1373
SelfReg ([46], 2021)
0.0252±0.000
0.0232±0.000
0.0281±0.000
0.0526±0.010
0.0323
Transfer ([45], 2021)
0.1446±0.000
0.1379±0.000
0.2122±0.000
0.0475±0.001
0.1355
RSD ([3], 2021)
0.0313±0.000
0.0264±0.000
0.0298±0.000
0.0497±0.005
0.0343
CAD ([47], 2022)
0.1447±0.000
0.1849±0.000
0.2122±0.000
0.0555±0.015
0.1493
CausIRL ([49], 2022)
0.0278±0.000
0.0257±0.002
0.0296±0.000
0.0463±0.000
0.0323
DDG ([9], 2022)
0.0490±0.000
0.0268±0.000
0.0302±0.000
−
−
MODE ([10], 2023)
0.0283±0.000
0.0268±0.000
0.0299±0.000
0.0464±0.000
0.0329"
CROSS-DOMAIN RENTAL PREDICTION DATASET,0.5663082437275986,"MAMR
0.0189±0.000
0.0225±0.000
0.0238±0.000
0.0459±0.000
0.0278"
QUANTITATIVE COMPARISONS,0.568100358422939,"5.6
Quantitative Comparisons
273"
QUANTITATIVE COMPARISONS,0.5698924731182796,"Comparison to risk minimization methods. ERM and IRM are typical risk minimization methods.
274"
QUANTITATIVE COMPARISONS,0.5716845878136201,"From Tab. 1, we find that ERM is better than IRM, which might imply that the gradient invariance in
275"
QUANTITATIVE COMPARISONS,0.5734767025089605,"IRM is useless for our problem. Another result is that the naive ERM is surprisingly comparable with
276"
QUANTITATIVE COMPARISONS,0.5752688172043011,"advanced methods, e.g., MMD, DANN and MLDG. Even on AFAD dataset, ERM is a very strong
277"
QUANTITATIVE COMPARISONS,0.5770609318996416,"baseline. Previous works [50, 52] also find a similar phenomenon in classification tasks.
278"
QUANTITATIVE COMPARISONS,0.578853046594982,"Comparison to the methods using feature alignments and robust optimization. As discussed in
279"
QUANTITATIVE COMPARISONS,0.5806451612903226,"Sec. 4, directly using feature alignments, e.g., MMD, DANN and CORAL, may perform poorly due
280"
QUANTITATIVE COMPARISONS,0.5824372759856631,"to the regression margin. Furthermore, DANN and Transfer try to apply adversarial robustness, and
281"
QUANTITATIVE COMPARISONS,0.5842293906810035,"MODE uses style augmentation for distribution robustness. Our results demonstrate the robustness
282"
QUANTITATIVE COMPARISONS,0.5860215053763441,"design in these methods might bring the opposite impact on ordinal predictions.
283"
QUANTITATIVE COMPARISONS,0.5878136200716846,"Comparison to subspace alignments, e.g., RSD. We find that RSD gets comparable performance
284"
QUANTITATIVE COMPARISONS,0.589605734767025,"with respect to feature alignment methods. With principal angle alignment between sub-spaces, the
285"
QUANTITATIVE COMPARISONS,0.5913978494623656,"sub-space alignments effectively slack the traditional feature alignments. This might imply that the
286"
QUANTITATIVE COMPARISONS,0.5931899641577061,"domain adaptation method RSD can also generalize to out-of-distribution data.
287"
QUANTITATIVE COMPARISONS,0.5949820788530465,"Comparison to self-supervised and data augmentation methods, e.g., SelfReg. The self-supervised
288"
QUANTITATIVE COMPARISONS,0.5967741935483871,"methods, especially with contrastive learning, can be strong baselines for our problem. The reason
289"
QUANTITATIVE COMPARISONS,0.5985663082437276,"might be that SelfReg uses strong data augmentation and mixup operation in their models. We find
290"
QUANTITATIVE COMPARISONS,0.600358422939068,"the follow-up work CAD does not surpass SelfReg. The reason might be that the part of marginal
291"
QUANTITATIVE COMPARISONS,0.6021505376344086,"distribution alignment in CAD harms the generalization ability like DANN. MTL augments the
292"
QUANTITATIVE COMPARISONS,0.6039426523297491,"original feature space with the marginal distribution of feature vectors. However, MTL performs
293"
QUANTITATIVE COMPARISONS,0.6057347670250897,"poorly in our regression settings. The reason might be augmenting the original feature space destroys
294"
QUANTITATIVE COMPARISONS,0.6075268817204301,"the ordinal information of features.
295"
QUANTITATIVE COMPARISONS,0.6093189964157706,"Comparison to meta-learning method. MLDG simultaneously optimizes the support risks and query
296"
QUANTITATIVE COMPARISONS,0.6111111111111112,"risks. While in DGR, the support and the query tasks usually change a lot, which makes the MLDG
297"
QUANTITATIVE COMPARISONS,0.6129032258064516,"hard to be optimized. Our method does not simultaneously optimize the two risks and is attentive to
298"
QUANTITATIVE COMPARISONS,0.6146953405017921,"hard tasks. The experiments also demonstrate that our method outperforms MLDG.
299"
QUANTITATIVE COMPARISONS,0.6164874551971327,"Comparison to disentanglement/causality. DDG disentangles the latent representations into semantic
300"
QUANTITATIVE COMPARISONS,0.6182795698924731,"features and variation features. DDG may capture the causal mechanism between the inputs and their
301"
QUANTITATIVE COMPARISONS,0.6200716845878136,"responding values. However, our further experiments with CausIRL method demonstrate that DDG
302"
QUANTITATIVE COMPARISONS,0.6218637992831542,"can collapse with generated variational samples. DDG is originally proposed to minimize the semantic
303"
QUANTITATIVE COMPARISONS,0.6236559139784946,"difference among generated samples from the same class while diversifying the variation across
304"
QUANTITATIVE COMPARISONS,0.6254480286738351,"Table 2: Ablation studies on CACD dataset with training-domain validation. Each regression interval
(domain) denotes the target interval with the others as source intervals."
QUANTITATIVE COMPARISONS,0.6272401433691757,"Methods
[15-20)
[20-30)
[30-40)
[40-50)
[50-60]
Avg"
QUANTITATIVE COMPARISONS,0.6290322580645161,"MAMR-
0.0348±0.01
0.0284±0.01
0.0015±0.00
0.0156±0.01
0.0235±0.01
0.0208
MAMR-G
0.0475±0.00
0.0505±0.03
0.0248±0.02
0.0431±0.02
0.0754±0.04
0.0483
MAMR-P
0.0331±0.01
0.0143±0.00
0.0021±0.00
0.0078±0.00
0.0371±0.01
0.0189"
QUANTITATIVE COMPARISONS,0.6308243727598566,"0.1
0.2
0.3
0.4
Regression Margin 0.015 0.020 0.025 0.030 0.035 0.040"
QUANTITATIVE COMPARISONS,0.6326164874551972,MSE losses
QUANTITATIVE COMPARISONS,0.6344086021505376,"MAMR-
MAMR (a)"
QUANTITATIVE COMPARISONS,0.6362007168458781,"0.1 
  0.05    0.01  0.005    0.001"
QUANTITATIVE COMPARISONS,0.6379928315412187,"1
2
3
4
5
Inner Iterations"
QUANTITATIVE COMPARISONS,0.6397849462365591,0.00741 0.00251 0.00325 0.00668 0.00746
QUANTITATIVE COMPARISONS,0.6415770609318996,0.00741 0.00178 0.00283 0.00665 0.00746
QUANTITATIVE COMPARISONS,0.6433691756272402,0.00741 0.00131 0.00517 0.00736 0.00746
QUANTITATIVE COMPARISONS,0.6451612903225806,0.00741 0.00207 0.00744 0.00746 0.00746
QUANTITATIVE COMPARISONS,0.6469534050179212,0.00741 0.00181 0.00651 0.00745 0.00746
QUANTITATIVE COMPARISONS,0.6487455197132617,0.0000
QUANTITATIVE COMPARISONS,0.6505376344086021,0.0005
QUANTITATIVE COMPARISONS,0.6523297491039427,0.0010
QUANTITATIVE COMPARISONS,0.6541218637992832,0.0015
QUANTITATIVE COMPARISONS,0.6559139784946236,0.0020
QUANTITATIVE COMPARISONS,0.6577060931899642,0.0025
QUANTITATIVE COMPARISONS,0.6594982078853047,0.0030 (b)
QUANTITATIVE COMPARISONS,0.6612903225806451,"0.1 
0.05  0.01  0.005    0.001"
QUANTITATIVE COMPARISONS,0.6630824372759857,"1
2
3
4
5
Inner Iterations"
QUANTITATIVE COMPARISONS,0.6648745519713262,0.00188 0.00157 0.00121 0.00119 0.00139
QUANTITATIVE COMPARISONS,0.6666666666666666,0.00188 0.00492 0.00122 0.00119 0.00140
QUANTITATIVE COMPARISONS,0.6684587813620072,0.00188 0.00453 0.00128 0.00121 0.00142
QUANTITATIVE COMPARISONS,0.6702508960573477,0.00188 0.00146 0.00124 0.00121 0.00141
QUANTITATIVE COMPARISONS,0.6720430107526881,0.00188 0.00284 0.00130 0.00120 0.00139
QUANTITATIVE COMPARISONS,0.6738351254480287,0.0000
QUANTITATIVE COMPARISONS,0.6756272401433692,0.0005
QUANTITATIVE COMPARISONS,0.6774193548387096,0.0010
QUANTITATIVE COMPARISONS,0.6792114695340502,0.0015
QUANTITATIVE COMPARISONS,0.6810035842293907,0.0020
QUANTITATIVE COMPARISONS,0.6827956989247311,0.0025
QUANTITATIVE COMPARISONS,0.6845878136200717,0.0030 (c)
QUANTITATIVE COMPARISONS,0.6863799283154122,"Figure 4: (a) The performances when changing regression margins. (b,c) The MSE heatmaps of
regression tasks [20, 30) and [30, 40) in CACD by Oracle validation."
QUANTITATIVE COMPARISONS,0.6881720430107527,"source domains. This design may let DDG overlook the variation features, which are coincidentally
305"
QUANTITATIVE COMPARISONS,0.6899641577060932,"important in regression setting. Instead, CausIRL captures the style variables and finds sufficient
306"
QUANTITATIVE COMPARISONS,0.6917562724014337,"conditions that do not rely on source domains.
307"
DETAILED ANALYSES,0.6935483870967742,"5.7
Detailed Analyses
308"
DETAILED ANALYSES,0.6953405017921147,"Tab. 2 provides 3 ablation models. MAMR- is our method without the margin-aware weighting
309"
DETAILED ANALYSES,0.6971326164874552,"mechanism. MAMR-G computes a mean weight for query tasks using the MMD with Gaussian
310"
DETAILED ANALYSES,0.6989247311827957,"kernel. MAMR-P computes the pair-wised Euclidean distances among the support and query tasks
311"
DETAILED ANALYSES,0.7007168458781362,"and provides a weight for each query task. We encourage MAMR-P to perform long range exploration
312"
DETAILED ANALYSES,0.7025089605734767,"by our proposed margin-aware weighting, which helps achieve better average regression performance.
313"
DETAILED ANALYSES,0.7043010752688172,"Besides that, the results demonstrate the averaged weight in MAMR-G may be invalid compared to
314"
DETAILED ANALYSES,0.7060931899641577,"pair-wised weights. The pair-wised Euclidean distances can be viewed as a special case of optimal
315"
DETAILED ANALYSES,0.7078853046594982,"transport distances [53] between the query data points and the support data points. Furthermore,
316"
DETAILED ANALYSES,0.7096774193548387,"Fig. 4(a) provides the regression performances of MAMR- and MAMR-P (MAMR). When manually
317"
DETAILED ANALYSES,0.7114695340501792,"enlarging the regression margin on the CACD dataset, MAMR consistently demonstrates better
318"
DETAILED ANALYSES,0.7132616487455197,"performance and smaller variance. Note that we set 0.1 as the start regression margin between the
319"
DETAILED ANALYSES,0.7150537634408602,"domain [20, 30) and [30, 40) in CACD.
320"
DETAILED ANALYSES,0.7168458781362007,"The key hyper-parameters of the MAMR model include the inner loop learning rate β, the outer loop
321"
DETAILED ANALYSES,0.7186379928315412,"learning rate α and the iteration steps of the inner loop. To reduce the search of hyper-parameters, we
322"
DETAILED ANALYSES,0.7204301075268817,"set α = 0.1∗β. We conduct a grid search for β and the iteration steps. Fig. 4(b) and Fig. 4(c) provide
323"
DETAILED ANALYSES,0.7222222222222222,"the MSE heatmaps on the CACD dataset using two generalization tasks. We find that more inner
324"
DETAILED ANALYSES,0.7240143369175627,"iteration steps do not have a significant influence on the generalization results. This phenomenon is
325"
DETAILED ANALYSES,0.7258064516129032,"consistent with our analysis of the method: different from 5 or 10 inner steps in meta-learning for
326"
DETAILED ANALYSES,0.7275985663082437,"few-shot learning, fast adaptation by multi-steps is not necessary for DGR.
327"
CONCLUSION AND LIMITATIONS,0.7293906810035843,"6
Conclusion and Limitations
328"
CONCLUSION AND LIMITATIONS,0.7311827956989247,"We investigate domain generalization for ordinal regression problems. A margin-aware meta-learning
329"
CONCLUSION AND LIMITATIONS,0.7329749103942652,"regression method is proposed to achieve long-range exploration and interpolation. We build a
330"
CONCLUSION AND LIMITATIONS,0.7347670250896058,"regression benchmark to systematically investigate the performance of existing domain generalization
331"
CONCLUSION AND LIMITATIONS,0.7365591397849462,"methods for regression. Limitations: (1) Our empirical analyses demonstrate that domain generaliza-
332"
CONCLUSION AND LIMITATIONS,0.7383512544802867,"tion for regression still has a large exploration space when dealing with high-dimensional data. (2)
333"
CONCLUSION AND LIMITATIONS,0.7401433691756273,"Initial calculation of representation distance in meta-space is not reliable, one strategy is to consider
334"
CONCLUSION AND LIMITATIONS,0.7419354838709677,"a suitable warm-up strategy. (3) Finally, most used datasets have balanced source labels, applying
335"
CONCLUSION AND LIMITATIONS,0.7437275985663082,"MAMR to imbalanced source domains is also a more practical setting.
336"
REFERENCES,0.7455197132616488,"References
337"
REFERENCES,0.7473118279569892,"[1] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization: A survey,” IEEE Transactions
338"
REFERENCES,0.7491039426523297,"on Pattern Analysis and Machine Intelligence, pp. 1–20, 2022. 1, 2, 3
339"
REFERENCES,0.7508960573476703,"[2] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and P. Yu, “Generalizing to unseen
340"
REFERENCES,0.7526881720430108,"domains: A survey on domain generalization,” IEEE Transactions on Knowledge and Data Engineering,
341"
REFERENCES,0.7544802867383512,"pp. 1–1, 2022. 1, 3
342"
REFERENCES,0.7562724014336918,"[3] X. Chen, S. Wang, J. Wang, and M. Long, “Representation subspace distance for domain adaptation
343"
REFERENCES,0.7580645161290323,"regression,” in Proceedings of the 38th International Conference on Machine Learning (M. Meila and
344"
REFERENCES,0.7598566308243727,"T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 1749–1759, PMLR, 18–24 Jul
345"
REFERENCES,0.7616487455197133,"2021. 1, 3, 6, 8
346"
REFERENCES,0.7634408602150538,"[4] J. Jiang, Y. Ji, X. Wang, Y. Liu, J. Wang, and M. Long, “Regressive domain adaptation for unsupervised
347"
REFERENCES,0.7652329749103942,"keypoint detection,” in IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,
348"
REFERENCES,0.7670250896057348,"June 19-25, 2021, pp. 6780–6789, Computer Vision Foundation / IEEE, 2021. 1, 3
349"
REFERENCES,0.7688172043010753,"[5] Y. Wang, Y. Jiang, J. Li, B. Ni, W. Dai, C. Li, H. Xiong, and T. Li, “Contrastive regression for domain
350"
REFERENCES,0.7706093189964157,"adaptation on gaze estimation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
351"
REFERENCES,0.7724014336917563,"Pattern Recognition (CVPR), pp. 19376–19385, June 2022. 1
352"
REFERENCES,0.7741935483870968,"[6] X. Liu, Z. Guo, S. Li, F. Xing, J. You, C.-C. J. Kuo, G. El Fakhri, and J. Woo, “Adversarial unsupervised
353"
REFERENCES,0.7759856630824373,"domain adaptation with conditional and label shift: Infer, align and iterate,” in Proceedings of the IEEE/CVF
354"
REFERENCES,0.7777777777777778,"International Conference on Computer Vision (ICCV), pp. 10367–10376, October 2021. 1
355"
REFERENCES,0.7795698924731183,"[7] Y. Yang, K. Zha, Y.-C. Chen, H. Wang, and D. Katabi, “Delving into deep imbalanced regression,” in
356"
REFERENCES,0.7813620071684588,"International Conference on Machine Learning (ICML), 2021. 1
357"
REFERENCES,0.7831541218637993,"[8] H. Li, S. J. Pan, S. Wang, and A. C. Kot, “Domain generalization with adversarial feature learning,” in
358"
REFERENCES,0.7849462365591398,"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5400–5409, 2018. 1, 4, 6, 8
359"
REFERENCES,0.7867383512544803,"[9] H. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Schölkopf, and E. P. Xing, “Towards principled disentangle-
360"
REFERENCES,0.7885304659498208,"ment for domain generalization,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
361"
REFERENCES,0.7903225806451613,"Pattern Recognition, pp. 8024–8034, 2022. 2, 3, 6, 8
362"
REFERENCES,0.7921146953405018,"[10] R. Dai, Y. Zhang, Z. Fang, B. Han, and X. Tian, “Moderately distributional exploration for domain
363"
REFERENCES,0.7939068100358423,"generalization,” in Proceedings of the 40th International Conference on Machine Learning, 2023. 2, 6, 8
364"
REFERENCES,0.7956989247311828,"[11] Y. Shu, Z. Cao, C. Wang, J. Wang, and M. Long, “Open domain generalization with domain-augmented
365"
REFERENCES,0.7974910394265233,"meta-learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
366"
REFERENCES,0.7992831541218638,"pp. 9624–9633, 2021. 2, 4
367"
REFERENCES,0.8010752688172043,"[12] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,”
368"
REFERENCES,0.8028673835125448,"in Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
369"
REFERENCES,0.8046594982078853,"p. 1126–1135, JMLR.org, 2017. 2, 3
370"
REFERENCES,0.8064516129032258,"[13] D. Li, Y. Yang, Y.-Z. Song, and T. Hospedales, “Learning to generalize: Meta-learning for domain
371"
REFERENCES,0.8082437275985663,"generalization,” in AAAI Conference on Artificial Intelligence, 2018. 2, 3, 6, 8
372"
REFERENCES,0.8100358422939068,"[14] Q. Dou, D. Coelho de Castro, K. Kamnitsas, and B. Glocker, “Domain generalization via model-agnostic
373"
REFERENCES,0.8118279569892473,"learning of semantic features,” in Advances in Neural Information Processing Systems (H. Wallach,
374"
REFERENCES,0.8136200716845878,"H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, eds.), vol. 32, Curran Associates,
375"
REFERENCES,0.8154121863799283,"Inc., 2019. 2, 3
376"
REFERENCES,0.8172043010752689,"[15] Y. Du, X. Zhen, L. Shao, and C. G. M. Snoek, “Metanorm: Learning to normalize few-shot batches across
377"
REFERENCES,0.8189964157706093,"domains,” in International Conference on Learning Representations, 2021. 2
378"
REFERENCES,0.8207885304659498,"[16] H. Yao, Y. Wang, Y. Wei, P. Zhao, M. Mahdavi, D. Lian, and C. Finn, “Meta-learning with an adaptive
379"
REFERENCES,0.8225806451612904,"task scheduler,” in Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer,
380"
REFERENCES,0.8243727598566308,"Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 7497–7509, Curran Associates, Inc., 2021. 2
381"
REFERENCES,0.8261648745519713,"[17] N. Gao, H. Ziesche, N. A. Vien, M. Volpp, and G. Neumann, “What matters for meta-learning vision re-
382"
REFERENCES,0.8279569892473119,"gression tasks?,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
383"
REFERENCES,0.8297491039426523,"(CVPR), pp. 14776–14786, June 2022. 2, 3
384"
REFERENCES,0.8315412186379928,"[18] X. Liu, S. Li, Y. Ge, P. Ye, J. You, and J. Lu, “Recursively conditional gaussian for ordinal unsupervised
385"
REFERENCES,0.8333333333333334,"domain adaptation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),
386"
REFERENCES,0.8351254480286738,"pp. 764–773, October 2021. 2, 3, 7
387"
REFERENCES,0.8369175627240143,"[19] J. Liang, D. Hu, and J. Feng, “Do we really need to access the source data? source hypothesis transfer for
388"
REFERENCES,0.8387096774193549,"unsupervised domain adaptation,” in International Conference on Machine Learning (ICML), pp. 6028–
389"
REFERENCES,0.8405017921146953,"6039, 2020. 3
390"
REFERENCES,0.8422939068100358,"[20] C. Cortes and M. Mohri, “Domain adaptation in regression,” in Algorithmic Learning Theory, (Berlin,
391"
REFERENCES,0.8440860215053764,"Heidelberg), pp. 308–323, Springer Berlin Heidelberg, 2011. 3
392"
REFERENCES,0.8458781362007168,"[21] Y. Zheng, D. Huang, S. Liu, and Y. Wang, “Cross-domain object detection through coarse-to-fine feature
393"
REFERENCES,0.8476702508960573,"adaptation,” in 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3
394"
REFERENCES,0.8494623655913979,"[22] Y. Bao, Y. Liu, H. Wang, and F. Lu, “Generalizing gaze estimation with rotation consistency,” in Proceed-
395"
REFERENCES,0.8512544802867383,"ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4207–4216,
396"
REFERENCES,0.8530465949820788,"June 2022. 3
397"
REFERENCES,0.8548387096774194,"[23] H. Xia, P. Wang, T. Koike-Akino, Y. Wang, P. Orlik, and Z. Ding, “Adversarial bi-regressor network for
398"
REFERENCES,0.8566308243727598,"domain adaptive regression,” in Proceedings of the Thirty-First International Joint Conference on Artificial
399"
REFERENCES,0.8584229390681004,"Intelligence, IJCAI-22 (L. D. Raedt, ed.), pp. 3608–3614, International Joint Conferences on Artificial
400"
REFERENCES,0.8602150537634409,"Intelligence Organization, 7 2022. Main Track. 3
401"
REFERENCES,0.8620071684587813,"[24] T. Teshima, I. Sato, and M. Sugiyama, “Few-shot domain adaptation by causal mechanism transfer,” in
402"
REFERENCES,0.8637992831541219,"Proceedings of the 37th International Conference on Machine Learning, ICML’20, JMLR.org, 2020. 3
403"
REFERENCES,0.8655913978494624,"[25] B. Chidlovskii, A. Sadek, and C. Wolf, “Universal domain adaptation in ordinal regression,” 2021. 3
404"
REFERENCES,0.8673835125448028,"[26] S. Saengkyongam, L. Henckel, N. Pfister, and J. Peters, “Exploiting independent instruments: Identification
405"
REFERENCES,0.8691756272401434,"and distribution generalization,” in Proceedings of the 39th International Conference on Machine Learning
406"
REFERENCES,0.8709677419354839,"(K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of
407"
REFERENCES,0.8727598566308243,"Machine Learning Research, pp. 18935–18958, PMLR, 17–23 Jul 2022. 3
408"
REFERENCES,0.8745519713261649,"[27] A. Rame, C. Dancette, and M. Cord, “Fishr: Invariant gradient variances for out-of-distribution gener-
409"
REFERENCES,0.8763440860215054,"alization,” in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri,
410"
REFERENCES,0.8781362007168458,"S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine
411"
REFERENCES,0.8799283154121864,"Learning Research, pp. 18347–18377, PMLR, 17–23 Jul 2022. 3
412"
REFERENCES,0.8817204301075269,"[28] M. G. Weber, L. Li, B. Wang, Z. Zhao, B. Li, and C. Zhang, “Certifying out-of-domain generalization
413"
REFERENCES,0.8835125448028673,"for blackbox functions,” in Proceedings of the 39th International Conference on Machine Learning
414"
REFERENCES,0.8853046594982079,"(K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of
415"
REFERENCES,0.8870967741935484,"Machine Learning Research, pp. 23527–23548, PMLR, 17–23 Jul 2022. 3
416"
REFERENCES,0.8888888888888888,"[29] H. Wang, H. Si, B. Li, and H. Zhao, “Provable domain generalization via invariant-feature subspace
417"
REFERENCES,0.8906810035842294,"recovery,” in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri,
418"
REFERENCES,0.8924731182795699,"S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine
419"
REFERENCES,0.8942652329749103,"Learning Research, pp. 23018–23033, PMLR, 17–23 Jul 2022. 3
420"
REFERENCES,0.8960573476702509,"[30] B. Gao, H. Gouk, Y. Yang, and T. Hospedales, “Loss function learning for domain generalization by
421"
REFERENCES,0.8978494623655914,"implicit gradient,” in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri,
422"
REFERENCES,0.899641577060932,"S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine
423"
REFERENCES,0.9014336917562724,"Learning Research, pp. 7002–7016, PMLR, 17–23 Jul 2022. 3
424"
REFERENCES,0.9032258064516129,"[31] X. Chu, Y. Jin, W. Zhu, Y. Wang, X. Wang, S. Zhang, and H. Mei, “DNA: Domain generalization with
425"
REFERENCES,0.9050179211469535,"diversified neural averaging,” in Proceedings of the 39th International Conference on Machine Learning
426"
REFERENCES,0.9068100358422939,"(K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of
427"
REFERENCES,0.9086021505376344,"Machine Learning Research, pp. 4010–4034, PMLR, 17–23 Jul 2022. 3
428"
REFERENCES,0.910394265232975,"[32] J. Peters, P. Bühlmann, and N. Meinshausen, “Causal inference using invariant prediction: identification
429"
REFERENCES,0.9121863799283154,"and confidence intervals,” Journal of the Royal Statistical Society, Series B (Statistical Methodology),
430"
REFERENCES,0.9139784946236559,"vol. 78, no. 5, pp. 947–1012, 2016. (with discussion). 3
431"
REFERENCES,0.9157706093189965,"[33] D. Rothenhäusler, N. Meinshausen, P. Bühlmann, and J. Peters, “Anchor regression: Heterogeneous data
432"
REFERENCES,0.9175627240143369,"meet causality,” Journal of the Royal Statistical Society Series B, vol. 83, pp. 215–246, April 2021. 3
433"
REFERENCES,0.9193548387096774,"[34] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky,
434"
REFERENCES,0.921146953405018,"“Domain-adversarial training of neural networks,” Journal of Machine Learning Research, vol. 17, no. 59,
435"
REFERENCES,0.9229390681003584,"pp. 1–35, 2016. 4, 6, 8
436"
REFERENCES,0.9247311827956989,"[35] A. Müller, “Integral probability metrics and their generating classes of functions,” Advances in Applied
437"
REFERENCES,0.9265232974910395,"Probability, vol. 29, no. 2, pp. 429–443, 1997. 4
438"
REFERENCES,0.9283154121863799,"[36] F. Liu, W. Xu, J. Lu, and D. J. Sutherland, “Meta two-sample testing: Learning kernels for testing with
439"
REFERENCES,0.9301075268817204,"limited data,” in NeurIPS, 2021. 4
440"
REFERENCES,0.931899641577061,"[37] J. Shen, Y. Qu, W. Zhang, and Y. Yu, “Wasserstein distance guided representation learning for domain
441"
REFERENCES,0.9336917562724014,"adaptation,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, Apr. 2018. 4
442"
REFERENCES,0.9354838709677419,"[38] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware minimization for efficiently improv-
443"
REFERENCES,0.9372759856630825,"ing generalization,” in International Conference on Learning Representations, 2021. 5
444"
REFERENCES,0.9390681003584229,"[39] Y. Zhao, H. Zhang, and X. Hu, “Penalizing gradient norm for efficiently improving generalization in
445"
REFERENCES,0.9408602150537635,"deep learning,” in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri,
446"
REFERENCES,0.942652329749104,"S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine
447"
REFERENCES,0.9444444444444444,"Learning Research, pp. 26982–26992, PMLR, 17–23 Jul 2022. 5
448"
REFERENCES,0.946236559139785,"[40] R. Xu, X. Zhang, Z. Shen, T. Zhang, and P. Cui, “A theoretical analysis on independence-driven importance
449"
REFERENCES,0.9480286738351255,"weighting for covariate-shift generalization,” in Proceedings of the 39th International Conference on
450"
REFERENCES,0.9498207885304659,"Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162
451"
REFERENCES,0.9516129032258065,"of Proceedings of Machine Learning Research, pp. 24803–24829, PMLR, 17–23 Jul 2022. 5
452"
REFERENCES,0.953405017921147,"[41] V. Vapnik., The nature of statistical learning theory. Springer science business media, 1999. 6, 8
453"
REFERENCES,0.9551971326164874,"[42] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, “Invariant risk minimization,” 2019. 6, 8
454"
REFERENCES,0.956989247311828,"[43] B. Sun and K. Saenko, “Deep coral: Correlation alignment for deep domain adaptation,” in ECCV 2016
455"
REFERENCES,0.9587813620071685,"Workshops, 2016. 6, 8
456"
REFERENCES,0.9605734767025089,"[44] M. Pezeshki, S.-O. Kaba, Y. Bengio, A. Courville, D. Precup, and G. Lajoie, “Gradient starvation: A learn-
457"
REFERENCES,0.9623655913978495,"ing proclivity in neural networks,” in Advances in Neural Information Processing Systems (A. Beygelzimer,
458"
REFERENCES,0.96415770609319,"Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), 2021. 6, 8
459"
REFERENCES,0.9659498207885304,"[45] G. Zhang, H. Zhao, Y. Yu, and P. Poupart, “Quantifying and improving transferability in domain general-
460"
REFERENCES,0.967741935483871,"ization,” Advances in Neural Information Processing Systems, 2021. 6, 8
461"
REFERENCES,0.9695340501792115,"[46] D. Kim, Y. Yoo, S. Park, J. Kim, and J. Lee, “Selfreg: Self-supervised contrastive regularization for
462"
REFERENCES,0.9713261648745519,"domain generalization,” in Proceedings of the IEEE/CVF International Conference on Computer Vision,
463"
REFERENCES,0.9731182795698925,"pp. 9619–9628, 2021. 6, 8
464"
REFERENCES,0.974910394265233,"[47] Y. Ruan, Y. Dubois, and C. J. Maddison, “Optimal representations for covariate shift,” in International
465"
REFERENCES,0.9767025089605734,"Conference on Learning Representations, 2022. 6, 8
466"
REFERENCES,0.978494623655914,"[48] G. Blanchard, A. A. Deshmukh, U. Dogan, G. Lee, and C. Scott, “Domain generalization by marginal
467"
REFERENCES,0.9802867383512545,"transfer learning,” J. Mach. Learn. Res., vol. 22, jan 2021. 6, 8
468"
REFERENCES,0.982078853046595,"[49] M. Chevalley, C. Bunne, A. Krause, and S. Bauer, “Invariant causal mechanisms through distribution
469"
REFERENCES,0.9838709677419355,"matching,” 2022. 6, 8
470"
REFERENCES,0.985663082437276,"[50] I. Gulrajani and D. Lopez-Paz, “In search of lost domain generalization,” in International Conference on
471"
REFERENCES,0.9874551971326165,"Learning Representations, 2021. 6, 8
472"
REFERENCES,0.989247311827957,"[51] X. Liu, S. Li, Y. Ge, P. Ye, J. You, and J. Lu, “Ordinal unsupervised domain adaptation with recursively
473"
REFERENCES,0.9910394265232975,"conditional gaussian imposed variational disentanglement,” IEEE Transactions on Pattern Analysis and
474"
REFERENCES,0.992831541218638,"Machine Intelligence, pp. 1–14, 2022. 7
475"
REFERENCES,0.9946236559139785,"[52] E. Rosenfeld, P. Ravikumar, and A. Risteski, “Domain-adjusted regression or: Erm may already learn
476"
REFERENCES,0.996415770609319,"features sufficient for out-of-distribution generalization,” 2022. 8
477"
REFERENCES,0.9982078853046595,"[53] G. Peyré and M. Cuturi, “Computational optimal transport: With applications to data science,” 2019. 9
478"
