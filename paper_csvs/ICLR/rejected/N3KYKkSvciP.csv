Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0043859649122807015,"Deep learning has achieved many breakthroughs in modern classiﬁcation tasks.
Numerous architectures have been proposed for different data structures but when
it comes to the loss function, the cross-entropy loss is the predominant choice. Re-
cently, several alternative losses have seen revived interests for deep classiﬁers. In
particular, empirical evidence seems to promote square loss but a theoretical justi-
ﬁcation is still lacking. In this work, we contribute to the theoretical understanding
of square loss in classiﬁcation by systematically investigating how it performs for
overparametrized neural networks in the neural tangent kernel (NTK) regime. In-
teresting properties regarding the generalization error, robustness, and calibration
error are revealed. We consider two cases, according to whether classes are sepa-
rable or not. In the general non-separable case, fast convergence rate is established
for both misclassiﬁcation rate and calibration error. When classes are separable,
the misclassiﬁcation rate improves to be exponentially fast. Further, the resulting
margin is proven to be lower bounded away from zero, providing theoretical guar-
antees for robustness. We expect our ﬁndings to hold beyond the NTK regime and
translate to practical settings. To this end, we conduct extensive empirical stud-
ies on practical neural networks, demonstrating the effectiveness of square loss
in both synthetic low-dimensional data and real image data. Comparing to cross-
entropy, square loss has comparable generalization error but noticeable advantages
in robustness and model calibration."
INTRODUCTION,0.008771929824561403,"1
INTRODUCTION"
INTRODUCTION,0.013157894736842105,"The pursuit of better classiﬁers has fueled the progress of machine learning and deep learning re-
search. The abundance of benchmark image datasets, e.g., MNIST, CIFAR, ImageNet, etc., provides
test ﬁelds for all kinds of new classiﬁcation models, especially those based on deep neural networks
(DNN). With the introduction of CNN, ResNets, and transformers, DNN classiﬁers are constantly
improving and catching up to the human-level performance. In contrast to the active innovations
in model architecture, the training objective remains largely stagnant, with cross-entropy loss being
the default choice. Despite its popularity, cross-entropy has been shown to be problematic in some
applications. Among others, Yu et al. (2020) argued that features learned from cross-entropy lack
interpretability and proposed a new loss aiming for maximum coding rate reduction. Pang et al.
(2019) linked the use of cross-entropy to adversarial vulnerability and proposed a new classiﬁcation
loss based on latent space matching. Guo et al. (2017) discovered that the conﬁdence of most DNN
classiﬁers trained with cross-entropy is not well-calibrated."
INTRODUCTION,0.017543859649122806,"Recently, several alternative losses have seen revived interests for deep classiﬁers. In particular,
many existing works have presented empirical evidence promoting the use of square loss over cross-
entropy. Hui & Belkin (2020) conducted large-scale experiments comparing the two and found that
square loss tends to perform better in natural language processing related tasks while cross-entropy
usually yields slightly better accuracy in image classiﬁcation. Similar comparisons are also made
in Demirkaya et al. (2020). Kornblith et al. (2020) compared a variety of loss functions and output
layer regularization strategies on the accuracy and out-of-distribution robustness, and found that
square loss has greater class separation and better out-of-distribution robustness."
INTRODUCTION,0.021929824561403508,"In comparison to the empirical investigation, theoretical understanding of square loss in training
deep learning classiﬁers is still lacking. Through our lens, square loss has its uniqueness among"
INTRODUCTION,0.02631578947368421,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03070175438596491,"classic classiﬁcation losses, and we argue that it has great potentials for modern classiﬁcation tasks.
Below we list our motivations and reasons why."
INTRODUCTION,0.03508771929824561,"Explicit feature modeling
Deep learning’s success can be largely attributed to its superior ability
as feature extractors. For classiﬁcation, the ideal features should be separated between classes and
concentrated within classes. However, when optimizing cross-entropy loss, it’s not clear what the
learned features should look like (Yu et al., 2020). In comparison, square loss uses the label codings
(one-hot, simplex etc.) as features, which can be modeled explicitly to control class separations."
INTRODUCTION,0.039473684210526314,"Model Calibration
An ideal classiﬁer should not only give the correct class prediction, but also
with the correct conﬁdence. Calibration error measures the closeness of the predicted conﬁdence
to the underlying conditional probability ⌘. Using square loss in classiﬁcation can be essentially
viewed as regression where it treats discrete labels as continuous code vectors. It can be shown that
the optimal classiﬁer under square loss is 2⌘−1, linear with the ground truth. This distinguishing
property allows it to easily recover ⌘. In comparison, the optimal classiﬁers under the hinge loss and
cross-entropy are sign(2⌘−1) and log(
⌘
1−⌘), respectively. Therefore, hinge loss doesn’t provide
reliable information on the prediction conﬁdence, and cross-entropy can be problematic when ⌘is
close to 0 or 1 (Zhang, 2004). Hence, in terms of model calibration, square loss is a natural choice."
INTRODUCTION,0.043859649122807015,"Connections to popular approaches
Mixup (Zhang et al., 2017) is a popular data augmentation
technique where augmented data are constructed via convex combinations of inputs and their labels.
Like in square loss, mixup treats labels as continuous and is shown to improve the generalization
of DNN classiﬁers. In knowledge distillation (Hinton et al., 2015), where a student classiﬁer is
trying to learn from a trained teacher, Menon et al. (2021) proved that the “optimal” teacher with
the ground truth conditional probabilities provides the lowest variance in student learning. Since
classiﬁers trained using square loss is a natural consistent estimator of ⌘, one can argue that it is a
better teacher. In supervised contrastive learning (Khosla et al., 2020), the optimal features are the
same as those from square loss with simplex label coding (Graf et al., 2021) (details in Section 4)."
INTRODUCTION,0.04824561403508772,"Despite its lack of popularity in practice, square loss has many advantages that can be easily over-
looked. In this work, we systematically investigate from a statistical estimation perspective, the
properties of deep learning classiﬁers trained using square loss. The neural networks in our anal-
ysis are required to be sufﬁciently overparametrized in the neural tangent kernel (NTK) regime.
Even though this restricts the implication of our results, it is a necessary ﬁrst step towards a deeper
understanding. In summary, our main contributions are:"
INTRODUCTION,0.05263157894736842,"• Generalization error bound: We consider two cases, according to whether classes are separable"
INTRODUCTION,0.05701754385964912,"or not. In the general non-separable case, we adopt the classical binary classiﬁcation setting
with smooth conditional probability. Fast rate of convergence is established for overparametrized
neural network classiﬁers with Tsybakov’s noise condition. If two classes are separable with
positive margins, we show that overparametrized neural network classiﬁers can provably reach
zero misclassiﬁcation error with probability exponentially tending to one. To the best of our
knowledge, this is the ﬁrst such result for separable but not linear separable classes. Furthermore,
we bridge these two cases and offer a uniﬁed view by considering auxiliary random noise injection.
• Robustness (margin property): When two classes are separable, the decision boundary is not"
INTRODUCTION,0.06140350877192982,"unique and large-margin classiﬁers are preferred. In the separable case, we further show that
the decision boundary of overparametrized neural network classiﬁers trained by square loss can-
not be too close to the data support and the resulting margin is lower bounded away from zero,
providing theoretical guarantees for robustness.
• Calibration error: We show that classiﬁers trained using square loss are inherently well-calibrated,"
INTRODUCTION,0.06578947368421052,"i.e., the trained classiﬁer provides consistent estimation of the ground-truth conditional probability
in L1 norm. Such property doesn’t hold for cross-entropy.
• Empirical evaluation: We corroborate our theoretical ﬁndings with empirical experiments in both"
INTRODUCTION,0.07017543859649122,"synthetic low-dimensional data and real image data. Comparing to cross-entropy, square loss has
comparable generalization error but noticeable advantages in robustness and model calibration."
INTRODUCTION,0.07456140350877193,"This work contributes towards the theoretical understanding of deep classiﬁers, from an estimation
point of view, which has been a classic topic in statistics literature. Among others, Mammen &
Tsybakov (1999) established the optimal convergence rate for 0-1 loss excess risk when the decision
boundary is smooth. Zhang (2004); Bartlett et al. (2006) extended the analysis to various surrogate"
INTRODUCTION,0.07894736842105263,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08333333333333333,"losses. Audibert & Tsybakov (2007); Kohler & Krzyzak (2007) studied the convergence rates for
plug-in classiﬁers from local averaging estimators. Steinwart et al. (2007) investigated the conver-
gence rate for support vector machine using Gaussian kernels. We build on and extend classic results
to neural networks in the NTK regime. Comparing to existing works on deep learning classiﬁcation,
e.g., Kim et al. (2018) derived fast convergence rates of ReLU DNN classiﬁers that minimize the
empirical hinge loss, our results incorporate the training algorithm and apply to trained classiﬁers."
INTRODUCTION,0.08771929824561403,"We require the neural network to be overparametrized, which has been extensively studied recently,
under the umbrella term NTK. Most such results are in the regression setting with a handful of ex-
ceptions. Ji & Telgarsky (2019) showed that only polylogarithmic width is sufﬁcient for gradient
descent to overﬁt the training data using logistic loss. Hu et al. (2020) proved generalization error
bound for regularized NTK in classiﬁcation. Cao & Gu (2019; 2020) provided optimization and
generalization guarantees for overparametrized network trained with cross-entropy. In comparison,
our results are sharper in the sense that we take the ground truth data assumptions into consideration.
This allows a faster convergence rate, especially when the classes are separable, where the exponen-
tial convergence rate is attainable. The NTK framework greatly reduces the technical difﬁculty for
our theoretical analysis. However, our results are mainly due to properties of the square loss itself
and we expect them to hold for a wide range of classiﬁers."
INTRODUCTION,0.09210526315789473,"There are other works investigating the use of square loss for training (deep) classiﬁers. Han et al.
(2021) uncovered that the “neural collapse” phenomenon also occurs under square loss where the
last-layer features eventually collapse to their simplex-style class-means. Muthukumar et al. (2020)
compared classiﬁcation and regression tasks in the overparameterized linear model with Gaussian
features, illustrating different roles and properties of loss functions used at the training and test-
ing phases. Poggio & Liao (2019) made interesting observations on effects of popular regulariza-
tion techniques such as batch normalization and weight decay on the gradient ﬂow dynamics under
square loss. These ﬁndings support our theoretical results’ implication, which further strengthens
our beliefs that the essence comes from the square loss and our analysis can go beyond NTK regime."
INTRODUCTION,0.09649122807017543,"The rest of this paper is arranged as follows. Section 2 presents some preliminaries. Main theoretical
results are in Section 3. The simplex label coding is discussed in Section 4 followed by numerical
studies in Section 5 and conclusions in Section 6. Technical proofs and details of the numerical
studies can be found in the Appendix."
PRELIMINARIES,0.10087719298245613,"2
PRELIMINARIES"
PRELIMINARIES,0.10526315789473684,"Notation
For a function f : ⌦! R, let kfk1 = supx2⌦|f(x)| and kfkp = ( R"
PRELIMINARIES,0.10964912280701754,"⌦|f(x)|pdx)1/p.
For a vector x, kxkp denotes its p-norm, for 1 p 1. Lp and lp are used to distinguish function
norms and vector norms. For two positive sequences {an}n2N and {bn}n2N, we write an . bn if
there exists a constant C > 0 such that an Cbn for all sufﬁciently large n. We write an ⇣bn if
an . bn and bn . an. Let [N] = {1, . . . , N} for N 2 N, I be the indicator function, and Id be the
d ⇥d identity matrix. N(µ, ⌃) represents Gaussian distribution with mean µ and covariance ⌃."
PRELIMINARIES,0.11403508771929824,"Classiﬁcation problem settings
Let P be an underlying probability measure on ⌦⇥Y , where
⌦⇢Rd is compact and Y = {1, −1}. Let (X, Y ) be a random variable with respect to P. Suppose
we have observations {(xi, yi)}n"
PRELIMINARIES,0.11842105263157894,"i=1 ⇢(⌦⇥Y )n i.i.d. sampled according to P. The classiﬁcation
task is to predict the unobserved label y given a new input x 2 ⌦. Let ⌘deﬁned on ⌦denote the
conditional probability, i.e., ⌘(x) = P(y = 1|x). Let PX be the marginal distribution of P on X.
The key quantity of interest is the misclassiﬁcation error, i.e., 0-1 loss. In the population level, the
0-1 loss can be written as"
PRELIMINARIES,0.12280701754385964,"L(f) = E(X,Y )⇠P I{sign(f(X)) 6= Y } =EX⇠PX[(1 −⌘(X))I{f(X) ≥0} + ⌘(X)I{f(X) < 0}], (2.1)"
PRELIMINARIES,0.12719298245614036,"where the expectation is taken with respect to the probability measure P. Clearly, an optimal classi-
ﬁer with the minimal 0-1 loss is 2⌘−1."
PRELIMINARIES,0.13157894736842105,"According to whether labels are deterministic, there are two scenarios of interest. If ⌘only takes
values from {0, 1}, i.e., labels are deterministic, we call this case the separable case1. Let ⌦1 ="
PRELIMINARIES,0.13596491228070176,"1In the separable case we consider, the classes are not limited to linearly separable but can be arbitrarily
complicated."
PRELIMINARIES,0.14035087719298245,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.14473684210526316,"{x|⌘(x) = 1}, ⌦2 = {x|⌘(x) = 0} and ⌦= ⌦1 [ ⌦2. If the probability measure of {x|⌘(x) 2
(0, 1)} is non-zero, i.e., the labels contain randomness, we call this case the non-separable case. In
the separable case, we further assume that there exists a positive margin, i.e., dist(⌦1, ⌦2) ≥2γ > 0,
where γ is a constant, and dist(⌦1, ⌦2) = infx2⌦1,x02⌦2 kx −x0k2. In the non-separable case, to
quantify the difﬁculty of classiﬁcation, we adopt the well-established Tsybakov’s noise condition
(Audibert & Tsybakov, 2007), which measures how large the “difﬁcult region” is where ⌘(x) ⇡1/2.
Deﬁnition 2.1 (Tsybakov’s noise condition). Let 2 [0, 1]. We say P has Tsybakov noise expo-
nent if there exists a constant C, T > 0 such that for all 0 < t < T, PX(|2⌘(X)−1| < t) C ·t."
PRELIMINARIES,0.14912280701754385,"A large value of implies the difﬁcult region to be small. It is expected that a larger leads to a faster
convergence rate of a neural network classiﬁer. This intuition is veriﬁed for the overparametrized
neural network classiﬁer trained by square loss and `2 regularization. See Section 3 for more details."
PRELIMINARIES,0.15350877192982457,"Neural network setup
We mainly focus on the one-hidden-layer ReLU neural network family F
with m nodes in the hidden layer, denoted by"
PRELIMINARIES,0.15789473684210525,"fW ,a(x) =
1
pm m
X r=1"
PRELIMINARIES,0.16228070175438597,"arσ(W > r x),"
PRELIMINARIES,0.16666666666666666,"where x 2 ⌦, W = (W1, · · · , Wm) 2 Rd⇥m is the weight matrix in the hidden layer, a =
(a1, · · · , am)> 2 Rm is the weight vector in the output layer, σ(z) = max{0, z} is the rectiﬁed
linear unit (ReLU). The initial values of the weights are independently generated from"
PRELIMINARIES,0.17105263157894737,"Wr(0) ⇠N(0, ⇠2Im), ar ⇠unif{−1, 1}, 8r 2 [m]."
PRELIMINARIES,0.17543859649122806,"Based on the observations {(xi, yi)}n"
PRELIMINARIES,0.17982456140350878,"i=1, the goal of training a neural network is to ﬁnd a solution to min W n
X i=1"
PRELIMINARIES,0.18421052631578946,"l(fW ,a(xi), yi) + µR(W , a),
(2.2)"
PRELIMINARIES,0.18859649122807018,"where l is the loss function, R is the regularization, and µ ≥0 is the regularization parameter.
Note in Equation 2.2 that we only consider training the weights W . This is because a · σ(z) =
sign(a) · σ(|a|z), which allows us to reparametrize the network to have all ai’s to be either 1 or
−1. In this work, we consider square loss associated with `2 regularization, i.e., l(fW ,a(xi), yi) =
(fW ,a(xi) −yi)2 and R(W , a) = kW k2 2."
PRELIMINARIES,0.19298245614035087,"A popular way to train the neural network is via gradient based methods. It has been shown that
the training process of DNNs can be characterized by the neural tangent kernel (NTK) (Jacot et al.,
2018). As is usually assumed in the NTK literature (Arora et al., 2019; Hu et al., 2020; Bietti &
Mairal, 2019; Hu et al., 2021), we consider data on the unit sphere Sd−1, i.e., kxik2 = 1, 8i 2 [n],
and the neural network is highly overparametrized (m ≫n) and trained by gradient descent (GD).
For details about NTK and GD in one-hidden-layer ReLU neural networks, we refer to Appendix
A. In the rest of this work, we use fW (k),a to denote the GD-trained neural network classiﬁer under
square loss associated with `2 regularization, where k is the iteration number satisfying Assumption
D.1 and W (k) is the weight matrix after k-th iteration."
THEORETICAL RESULTS,0.19736842105263158,"3
THEORETICAL RESULTS"
THEORETICAL RESULTS,0.20175438596491227,"In this section, we present our main theoretical results. Throughout the analysis, we assume that the
overparametrized neural network fW ,a and the training process via GD satisfy Assumption D.1 (see
Appendix D), which essentially requires the neural network to be sufﬁciently overparametrized (with
a ﬁnite width), and imposes conditions on the learning rate and iteration number. Our theoretical
results consist of three parts: generalization error, robustness, and calibration error."
GENERALIZATION ERROR BOUND,0.20614035087719298,"3.1
GENERALIZATION ERROR BOUND"
GENERALIZATION ERROR BOUND,0.21052631578947367,"In classiﬁcation, the generalization error is typically referred to as the misclassiﬁcation error, which
can be quantiﬁed by L(f) deﬁned in Equation 2.1. In the non-separable case, the excess risk,
deﬁned by L(f)−L⇤, is used to evaluate the quality of a classiﬁer f, where L⇤= L(2⌘−1), which
minimizes the 0-1 loss. The following theorem states that the overparametrized neural network with
GD and `2 regularization can achieve a small excess risk in the non-separable case."
GENERALIZATION ERROR BOUND,0.2149122807017544,Under review as a conference paper at ICLR 2022
GENERALIZATION ERROR BOUND,0.21929824561403508,"Theorem 3.1 (Excess risk in the non-separable case). Suppose Assumptions D.1, D.2, and D.4
hold. Assume the conditional probability ⌘(x) satisﬁes Tsybakov’s noise condition with component
. Let µ ⇣n"
GENERALIZATION ERROR BOUND,0.2236842105263158,"d−1
2d−1 . Then"
GENERALIZATION ERROR BOUND,0.22807017543859648,"L(fW (k),a) = L⇤+ OP(n−
d(+1)
(2d−1)(+2) ).
(3.1)"
GENERALIZATION ERROR BOUND,0.2324561403508772,"From Theorem 3.1, we can see that as becomes larger, the convergence rate becomes faster, which
is intuitively true. Generalization error bounds in this setting is scarce. To the best of the au-
thors’ knowledge, Hu et al. (2020) is the closest work (the labels are randomly ﬂipped), where the
bound is in the order of OP(1/pn). Our bound is faster, especially with larger . It is known"
GENERALIZATION ERROR BOUND,0.23684210526315788,"that the optimal convergence rate under Assumptions D.2 and D.4 is OP(n−
d(+1)
d+4d−2 ) (Audibert
& Tsybakov, 2007). The differences between Equation 3.1 and the optimal convergence rate is
that there is an extra (d −1)in the denominator of the convergence rate in Equation 3.1 (since"
GENERALIZATION ERROR BOUND,0.2412280701754386,"n−
d(+1)
(2d−1)(+2) = n−
d(+1)
(d−1)+d+4d−2 ). If the conditional probability ⌘has a bounded Lipschitz con-
stant, then Kohler & Krzyzak (2007) showed that the convergence rate based on the plug-in kernel
estimate is OP(n−
+1
+3+d ), which is slower than the rate in Equation 3.1 if d is large."
GENERALIZATION ERROR BOUND,0.24561403508771928,"Now we turn to the separable case. Since ⌘only takes value from {0, 1} in the separable case, ⌘is
bounded away from 1/2. Therefore, one can trivially take ! 1 in Equation 3.1 and obtain the
convergence rate OP(n−d/(2d−1)). However, this rate can be signiﬁcantly improved in the separable
case, as stated in the following theorem.
Theorem 3.2 (Generalization error in the separable case). Suppose Assumptions D.1, D.3, and D.5
hold. Let µ = o(1). There exist positive constants C1, C2 such that the misclassiﬁcation rate is 0%
with probability at least 1 −δ −C1 exp(−C2n), and δ can be arbitrarily small2 by enlarging the
neural network’s width."
GENERALIZATION ERROR BOUND,0.25,"Note that in Theorem 3.2, the regularization parameter can take any rate that converges to zero. In
particular, µ can be zero, and the corresponding classiﬁer overﬁts the training data. Theorem 3.2
states that the convergence rate in the separable case is exponential, if a sufﬁciently wide neural
network is applied. This is because the observed labels are not corrupted by noise, i.e., P(y = 1|x)
is either one or zero. Therefore, it is easier to classify separable data, which is intuitively true."
ROBUSTNESS AND CALIBRATION ERROR,0.2543859649122807,"3.2
ROBUSTNESS AND CALIBRATION ERROR"
ROBUSTNESS AND CALIBRATION ERROR,0.25877192982456143,"If two classes are separable with positive margin, the decision boundary is not unique. Practitioners
often prefer the decision boundary with large margins, which are robust against possible perturbation
on input points (Elsayed et al., 2018; Ding et al., 2018). The following theorem states that the square
loss trained margin can be lower bounded by a positive constant. Recall that in the separable case,
⌦= ⌦1 [ ⌦2, where ⌦1 = {x|⌘(x) = 1} and ⌦2 = {x|⌘(x) = 0}.
Theorem 3.3 (Robustness in the separable case). Suppose the assumptions of Theorem 3.2 are
satisﬁed. Let µ = o(1). Then there exist positive constants C, C1, C2 such that for all n,"
ROBUSTNESS AND CALIBRATION ERROR,0.2631578947368421,"min
x2DT ,x02⌦1[⌦2 kx −x0k2 ≥C,"
ROBUSTNESS AND CALIBRATION ERROR,0.2675438596491228,"and the misclassiﬁcation rate is 0% with probability at least 1 −δ −C1 exp(−C2n), where DT is
the decision boundary, and δ is as in Theorem 3.2."
ROBUSTNESS AND CALIBRATION ERROR,0.2719298245614035,Remark 1. Note that kx −x0k1 ≥ p
ROBUSTNESS AND CALIBRATION ERROR,0.27631578947368424,"dkx −x0k2, thus Theorem 3.3 also indicates l1 robustness."
ROBUSTNESS AND CALIBRATION ERROR,0.2807017543859649,"In the non-separable case, ⌘(x) varies within (0,1) and practitioners may not only want a clas-
siﬁer with a small excess risk, but also want to recover the underlying conditional probability ⌘.
Therefore, square loss is naturally preferred since it treats the classiﬁcation problem as a regression
problem. The following theorem states that, one can recover the conditional probability ⌘by using
an overparametrized neural network with `2 regularization and GD training.
Theorem 3.4 (Calibration error). Suppose the conditions in Theorem 3.1, Assumption D.3 and D.4
are fulﬁlled. Let µ ⇣n"
ROBUSTNESS AND CALIBRATION ERROR,0.2850877192982456,"d−1
2d−1 . Then"
ROBUSTNESS AND CALIBRATION ERROR,0.2894736842105263,"k(fW (k),a + 1)/2 −⌘kL1 = OP(n−
1
4d−2 ).
(3.2)"
ROBUSTNESS AND CALIBRATION ERROR,0.29385964912280704,"2The term δ only depends on the width of the neural network. A smaller δ requires a wider neural network.
If δ = 0, then the number of nodes in the hidden layer is inﬁnity."
ROBUSTNESS AND CALIBRATION ERROR,0.2982456140350877,Under review as a conference paper at ICLR 2022
ROBUSTNESS AND CALIBRATION ERROR,0.3026315789473684,"Theorem 3.4 states that the underlying conditional probability in the non-separable case can be
recovered by (fW (k),a + 1)/2. The form (fW (k),a + 1)/2 is to account for the {−1, 1} label
coding. Under {0,1} coding, the estimator would be fW (k),a itself. The L1 consistency doesn’t
hold for cross-entropy trained neural networks, due to the form of the optimal solution log(
⌘
1−⌘).
With limited capacity, the network’s conﬁdence prediction is bounded away from 0 and 1 (Zhang,
2004). In practice, we want to control the complexity of the neural network thus it is usually the
case that kfW (k),ak1 < C for some constant C. Hence, it cannot accurately estimate ⌘(x) when
⌘(x) >
eC
1+eC or ⌘(x) <
1
1+eC , which makes the calibration error under the cross-entropy loss
always bounded away from zero. However, square loss does not have such a problem."
ROBUSTNESS AND CALIBRATION ERROR,0.30701754385964913,"Notice that the calibration error bound in Theorem 3.4 does not depend on the Tsybakov’s noise
condition, and is slower than the excess risk. This is because, a small calibration error is much
stronger than a small excess risk, since the former requires the conditional probability estimation
to be uniformly accurate, not just matching the sign of ⌘(x) −1/2. To be more speciﬁc, a good
estimated b⌘can always result in a low risk plug-in classiﬁer bf(x) = 2b⌘(x) −1, but not vice versa."
ROBUSTNESS AND CALIBRATION ERROR,0.31140350877192985,"Remark 2 (Technical challenge). Despite the similar forms of regression and classiﬁcation using
square loss, most of the regression analysis techniques cannot be directly applied to the classiﬁ-
cation problem, even if the supports of two classes are non-separable. Moreover, it is clear that
classiﬁcation problems in the separable case are completely different with regression problems."
ROBUSTNESS AND CALIBRATION ERROR,0.3157894736842105,"Remark 3 (Extension on NTK). Although our analysis only concerns overparametrized one-
hidden-layer ReLU neural networks, it can potentially apply to other types of neural networks in
the NTK regime. Recently, it has been shown that overparametrized multi-layer networks corre-
spond to the Laplace kernel (Geifman et al., 2020; Chen & Xu, 2020). As long as the trained neural
networks can approximate the classiﬁer induced by the NTK, our results can be naturally extended."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.3201754385964912,"3.3
TRANSITION FROM SEPARABLE TO NON-SEPARABLE"
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.32456140350877194,"The general non-separable case and the special separable case can be connected via Gaussian noise
injection. In practice, data augmentation is an effective way to improve robustness and the simplest
way is Gaussian noise injection (He et al., 2019). In this section, we only consider it as an auxiliary
tool for theoretical analysis purpose and not for actual robust training. Injecting Gaussian noise
amounts to convoluting a Gaussian distribution N(0, υ2Id) to the marginal distribution PX, which
enlarges both ⌦1 and ⌦2 to Rd and a unique decision boundary Dυ can be induced. Correspondingly,
the “noisy” conditional probability, denoted as e⌘υ, is also smoothed to be continuous on Rd. As
υ ! 0, ke⌘υ −⌘k1 ! 0 on ⌦1 and ⌦2 and the limiting e⌘0 is a piecewise constant function with
discontinuity at the induced decision boundary."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.32894736842105265,"Lemma 3.5 (Tsybakov’s noise condition under Gaussian noises). Let the margin be 2γ > 0, the
noise be N(0, υ2Id). Then there exist some constants T, C > 0 such that for any 0 < t < T,"
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.3333333333333333,"PX(|2e⌘υ(X) −1| < t) Cυ2 γ
exp ✓ −γ2 2υ2 ◆ · t."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.33771929824561403,"Theorem 3.6 (Exponential convergence rate). Suppose the classes are separable with margin 2γ >
0. No matter how complicated ⌦1 [⌦2 are, the excess risk of the over parameterized neural network
classiﬁer satisfying Assumptions D.1 and D.4 has the rate OP(e−nγ/7)."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.34210526315789475,"The proof of Theorem 3.6 involves taking the auxiliary noise to zero, e.g., v = vn ⇣1/pn. The
exponential convergence rate is a direct outcome of Lemma 3.5 and Theorem 3.1. Note that our
exponential convergence rate is much faster than existing ones under the similar separable setting
(Ji & Telgarsky, 2019; Cao & Gu, 2019; 2020), which are all polynomial with n, e.g., OP(1/pn)."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.34649122807017546,"Remark 4. Theorems 3.4 and 3.6 share the same gist that the over parameterized neural network
classiﬁers can have exponential convergence rate when data are separable with positive margin,
while the result of Theorem 3.6 is weaker than that of Theorem 3.4, but with milder conditions.
Nevertheless, Theorem 3.6 bridges the non-separable case and separable case."
TRANSITION FROM SEPARABLE TO NON-SEPARABLE,0.3508771929824561,Under review as a conference paper at ICLR 2022
MULTICLASS CLASSIFICATION,0.35526315789473684,"4
MULTICLASS CLASSIFICATION"
MULTICLASS CLASSIFICATION,0.35964912280701755,"In binary classiﬁcation, the labels are usually encoded as −1 and 1. When there are K > 2 classes,
the default label coding is one-hot. However, it is empirically observed that this vanilla square loss
struggles when the number of classes are large, for which scaling tricks have been proposed (Hui
& Belkin, 2020; Demirkaya et al., 2020). Another popular coding scheme is the simplex coding
(Mroueh et al., 2012), which takes maximally separated K points on the sphere as label features.
When K = 2, this reduces to the typical −1, 1 coding. Many advantages of the simplex coding
have been discussed, including its relationship with cross-entropy loss and supervised contrastive
learning (Papyan et al., 2020; Han et al., 2021; Graf et al., 2021; Fang et al., 2021)."
MULTICLASS CLASSIFICATION,0.36403508771929827,"In this work, we adopt the simplex coding. More discussion and empirical comparison about the
coding choices can be found in Appendix G.2. Given the label coding, one can easily generalize the
theoretical development in Section 3 by employing the following objective function min W K
X j=1 n
X i=1"
MULTICLASS CLASSIFICATION,0.3684210526315789,"(fj,W ,a(xi) −yi,j)2 + µkW k2 2,"
MULTICLASS CLASSIFICATION,0.37280701754385964,"where fW ,a : ⌦7! RK, and yi = (yi,1, ..., yi,K)> is the label of i-th observation."
MULTICLASS CLASSIFICATION,0.37719298245614036,"The following proposition states a relationship between the simplex coding scheme and the condi-
tional probability.
Proposition 4.1 (Conditional probability). Let f ⇤: ⌦! RK minimize the mean square error
EX(f ⇤(X) −vy)2, where vy is the simplex coding vector of label y. Then we have"
MULTICLASS CLASSIFICATION,0.3815789473684211,⌘k(x) := P (y = k|x) = '
MULTICLASS CLASSIFICATION,0.38596491228070173,(K −1)f ⇤(x)>vk + 1 (
MULTICLASS CLASSIFICATION,0.39035087719298245,"/K.
(4.1)"
MULTICLASS CLASSIFICATION,0.39473684210526316,"Unlike the softmax function when using cross entropy, the estimated conditional probability using
square loss is not guaranteed to be within 0 and 1. This will cause issues for adversarial attacks,
which will be discussed in detail in Appendix G.2."
NUMERICAL STUDIES,0.3991228070175439,"5
NUMERICAL STUDIES"
NUMERICAL STUDIES,0.40350877192982454,"Although our theoretical results are for overparametrized neural network in the NTK regime, we
expect our conclusions to generalize to practical network architectures. The focus of this section is
not on improving the state-of-the-art performance for deep classiﬁers, but to illustrate the difference
between cross-entropy and square loss. We provide experiment results on both synthetic and real
data, to support our theoretical ﬁndings and illustrate the practical beneﬁts of square loss in training
overparametrized DNN classiﬁers. Compared with cross-entropy, the square loss has comparable
generalization performance, but with stronger robustness and smaller calibration error."
SYNTHETIC DATA,0.40789473684210525,"5.1
SYNTHETIC DATA"
SYNTHETIC DATA,0.41228070175438597,"We consider the square loss based and cross-entropy based overparametrized neural networks
(ONN) with `2 regularization, denoted as SL-ONN + `2 and CE-ONN + `2, respectively. The
chosen ONNs are two-hidden-layer ReLU neural networks with 500 neurons for each layer, and the
parameter µ is selected via a validation set. More implementation details are in Appendix G.1."
SYNTHETIC DATA,0.4166666666666667,"Separable case
We consider two separated classes with spiral curve like supports. We also present
the performance of the cross-entropy based ONN without `2 regularization (CE-ONN). Figure 1
shows one instance of the test misclassiﬁcation rate and decision boundaries attained by SL-ONN +
`2 (Left), CE-ONN + `2 (Center), and CE-ONN (Right). From this example and other examples in
Appendix G.1, it can be seen that SL-ONN + `2 has a smaller test misclassiﬁcation rate and a much
smoother decision boundary. In particular, in the red region, where the training data are sparse,
SL-ONN + `2 ﬁts the correct data distribution best."
SYNTHETIC DATA,0.42105263157894735,"Non-separable case
We consider the conditional probability ⌘(x) = sin( p"
SYNTHETIC DATA,0.42543859649122806,"2⇡||x||2), x 2
[−1, 1]2, and the calibration performance of SL-ONN + `2 and CE-ONN + `2, where the classiﬁers
are denoted by bfl2 and bfce, respectively. The results are presented in Figure G.8 in the Appendix."
SYNTHETIC DATA,0.4298245614035088,Under review as a conference paper at ICLR 2022 
SYNTHETIC DATA,0.4342105263157895,"Figure 1: Test misclassiﬁcation rates and decision boundaries predicted by: SL-ONN + `2 (Left);
CE-ONN + `2 (Center); CE-ONN (Right) for the separable case."
SYNTHETIC DATA,0.43859649122807015,"The error bar plot of the test calibration error shows that bfl2 has the smaller mean and standard
deviation than bfce. It suggests that square loss generally outperforms cross entropy in calibration.
The histogram and kernel density estimation of the test calibration errors for one case show that the
pointwise calibration errors on the test points of bfl2 are more concentrated around zero than those
of bfce. Moreover, despite a comparable misclassiﬁcation rate with bfce, bfl2 has a smaller calibration
error. Figure G.8 demonstrates that SL-ONN + `2 recovers ⌘much better than CE-ONN + `2."
REAL DATA,0.44298245614035087,"5.2
REAL DATA"
REAL DATA,0.4473684210526316,"To make a fair comparison, we adopt popular architectures, ResNet (He et al., 2016) and Wide
ResNet (Zagoruyko & Komodakis, 2016) and evaluate them on the CIFAR image classiﬁcation
datasets, with only the training loss function changed, from cross-entropy (CE) to square loss with
simplex coding (SL). Further, we don’t employ any large scale hyper-parameter tuning and all the
parameters are kept as default except for the learning rate (lr) and batch size (bs), where we are
choosing from the better of (lr=0.01, bs=32) and (lr=0.1, bs=128). Each experiment setting is repli-
cated 5 times and we report the average performance followed by its standard deviation in the paren-
thesis. (lr=0.01, bs=32) works better for the most cases except for square loss trained WRN-16-10
on CIFAR-100. More experiment details and additional results can be found in Appendix G.2."
REAL DATA,0.4517543859649123,"Generalization
In both CIFAR-10 and CIFAR-100, the performance of cross-entropy and square
loss with simplex coding are quite comparable, as observed in Hui & Belkin (2020). Cross-entropy
tends to perform slightly better for ResNet, especially on CIFAR-100 with an advantage of less than
1%. There is a more signiﬁcant gap with Wide ResNet where square loss outperforms cross-entropy
by more than 1% on both CIFAR-10 and CIFAR-100. The details can be found in Table 1."
REAL DATA,0.45614035087719296,"Table 1: Test accuracy on CIFAR datasets. Average accuracy larger than 0 but less than 0.1 is
denoted as 0⇤without standard deviation."
REAL DATA,0.4605263157894737,"Dataset
Network
Loss
Clean acc %
PGD-100 (l1-strength)
AutoAttack (l1-strength)
2/255
4/255
8/255
2/255
4/255
8/255"
REAL DATA,0.4649122807017544,CIFAR-10
REAL DATA,0.4692982456140351,"ResNet-18
CE
95.15 (0.11)
8.81 (1.61)
0.65 (0.24)
0
2.74 (0.09)
0
0
SL
95.04 (0.07)
30.53 (0.92)
6.64 (0.67)
0.86 (0.24)
4.10 (0.50)
0⇤
0"
REAL DATA,0.47368421052631576,"WRN-16-10
CE
93.94 (0.16)
1.04 (0.10)
0
0
0.33 (0.06)
0
0
SL
95.02 (0.11)
37.47 (0.61)
23.16 (1.28)
7.88 (0.72)
5.37 (0.50)
0⇤
0"
REAL DATA,0.4780701754385965,CIFAR-100
REAL DATA,0.4824561403508772,"ResNet-50
CE
79.82 (0.14)
2.31 (0.07)
0⇤
0
0.99 (0.10)
0⇤
0
SL
78.91 (0.14)
13.76 (1.30)
4.63 (1.20)
1.21 (0.80)
3.67 (0.60)
0.16 (0.05)
0"
REAL DATA,0.4868421052631579,"WRN-16-10
CE
77.89 (0.21)
0.83 (0.07)
0⇤
0
0.42 (0.07)
0
0
SL
79.65 (0.15)
6.48 (0.40)
0.42 (0.04)
0⇤
2.73 (0.20)
0⇤
0"
REAL DATA,0.49122807017543857,"Adversarial robustness
Normally trained deep classiﬁers are found to be adversarially vulnerable
and adversarial attacks provide a powerful tool to evaluate classiﬁcation robustness. For our exper-
iment, we consider the black-box Gaussian noise attack, the classic white-box PGD attack (Madry
et al., 2017) and the state-of-the-art AutoAttack (Croce & Hein, 2020), with attack strength level
2/255, 4/255, 8/255 in l1 norm. AutoAttack contains both white-box and black-box attacks and
offers a more comprehensive evaluation of adversarial robustness. The Gaussian noises results are"
REAL DATA,0.4956140350877193,Under review as a conference paper at ICLR 2022
REAL DATA,0.5,Table 2: Performance on CIFAR-10 dataset for ResNet-18 under standard PGD adversarial training.
REAL DATA,0.5043859649122807,"CIFAR10
Loss
Acc (%)
PGD steps
Strength(l1)
Autoattack"
REAL DATA,0.5087719298245614,"ResNet-18
CE
86.87
3
8/255
37.08
84.50
7
8/255
41.88"
REAL DATA,0.5131578947368421,"ResNet-18
SL
87.31
3
8/255
40.46
84.52
7
8/255
44.76"
REAL DATA,0.5175438596491229,"presented in Table G.3 in the Appendix. At different noise levels, square loss consistently outper-
forms cross-entropy, especially for WRN-16-10, with around 2-4% accuracy improvement. More
details can be found in Appendix G.2. The PGD and AutoAttack results are reported in Table 1.
Even though classiﬁers trained with square loss is far away from adversarially robust, it consistently
gives signiﬁcantly higher adversarial accuracy. The same margin can be carried over to standard
adversarial training as well. Table 2 lists results from standard PGD adversarial training with CE
and SL. By substituting cross-entropy loss to square loss, the robust accuracy increased around 3%
while maintaining higher clean accuracy."
REAL DATA,0.5219298245614035,"One thing to notice is that when constructing white-box attacks, square loss will not work well since
it doesn’t directly reﬂect the classiﬁcation accuracy. More speciﬁcally, for a correctly classiﬁed
image (x, y), maximizing the square loss may result in linear scaling of the classiﬁer f(x), which
doesn’t change the predicted class (see Appendix G.2 for more discussion). To this end, we consider
a special attack for classiﬁers trained by square loss by maximizing the cosine similarity between
f(x) and vy. We call this angle attack and also utilize it for the PGD adversarial training paired
with square loss in Table 2. In our experiments, this special attack rarely outperforms the standard
PGD with cross-entropy and the reported PGD accuracy are from the latter settings. This property
of square loss may be an advantage in defending adversarial attacks."
REAL DATA,0.5263157894736842,"Model calibration
The predicted class probabilities for square loss can be obtained from Equa-
tion 4.1. Expected calibration error (ECE) measures the absolute difference between predicted conﬁ-
dence and the actual accuracy. Deep classiﬁers are usually found to be over-conﬁdent (Vaicenavicius
et al., 2019). Using ResNet as an example, we report the typical reliability diagram in Figure 2. On
CIFAR-10 with ResNet-18, the average ECE for cross-entropy is 0.028 (0.002) while that for square
loss is 0.0097 (0.001). On CIFAR-100 with ResNet-50, the average ECE for cross-entropy is 0.094
(0.005) while that for square loss is 0.068 (0.005). Square loss results are much more calibrated with
signiﬁcantly smaller ECE."
REAL DATA,0.5307017543859649,"Figure 2: Reliability diagrams of ResNet-18 on CIFAR-10 and ResNet-50 on CIFAR-100. Square
loss trained models behave more well-calibrated while cross-entropy trained ones tend to be visibly
more over-conﬁdent."
CONCLUSIONS,0.5350877192982456,"6
CONCLUSIONS"
CONCLUSIONS,0.5394736842105263,"Classiﬁcation problems are ubiquitous in deep learning. As a fundamental problem, any progress
in classiﬁcation can potentially beneﬁt numerous relevant tasks. Despite its lack of popularity in
practice, square loss has many advantages that can be easily overlooked. Through both theoretical
analysis and empirical studies, we identify several ideal properties of using square loss in training
neural network classiﬁers, including provable fast convergence rates, strong robustness, and small
calibration error. We encourage readers to try square loss in your own application scenarios."
CONCLUSIONS,0.543859649122807,Under review as a conference paper at ICLR 2022
CONCLUSIONS,0.5482456140350878,"Ethnics Statement
We acknowledge the ICLR Code of Ethics. This submission is mostly theo-
retical and the authors could not think of any potential violations of them in this submission."
REPRODUCIBILITY STATEMENT,0.5526315789473685,"Reproducibility Statement
For our theoretical results, explanations of assumptions can be found
in Appendix D and a complete proof of the claims can be found in the Appendix E and Appendix F.
Our experiment details can be found in Appendix G, with both data and training descriptions."
REFERENCES,0.5570175438596491,REFERENCES
REFERENCES,0.5614035087719298,"N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68(3):337–404, 1950."
REFERENCES,0.5657894736842105,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of"
REFERENCES,0.5701754385964912,"optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019."
REFERENCES,0.5745614035087719,Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classiﬁers. The
REFERENCES,0.5789473684210527,"Annals of statistics, 35(2):608–633, 2007."
REFERENCES,0.5833333333333334,"Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classiﬁcation, and risk bounds."
REFERENCES,0.5877192982456141,"Journal of the American Statistical Association, 101(473):138–156, 2006."
REFERENCES,0.5921052631578947,"Colin Bennett and Robert C Sharpley. Interpolation of Operators. Academic press, 1988."
REFERENCES,0.5964912280701754,Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. arXiv preprint
REFERENCES,0.6008771929824561,"arXiv:1905.12173, 2019."
REFERENCES,0.6052631578947368,Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
REFERENCES,0.6096491228070176,"deep neural networks. Advances in Neural Information Processing Systems, 32:10836–10846,
2019."
REFERENCES,0.6140350877192983,Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
REFERENCES,0.618421052631579,"parameterized deep relu networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, volume 34, pp. 3349–3356, 2020."
REFERENCES,0.6228070175438597,Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS.
REFERENCES,0.6271929824561403,"arXiv preprint arXiv:2009.10683, 2020."
REFERENCES,0.631578947368421,Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
REFERENCES,0.6359649122807017,"of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206–
2216. PMLR, 2020."
REFERENCES,0.6403508771929824,"Ahmet Demirkaya, Jiasi Chen, and Samet Oymak. Exploring the role of loss functions in multiclass"
REFERENCES,0.6447368421052632,"classiﬁcation. In 2020 54th Annual Conference on Information Sciences and Systems (CISS), pp.
1–5. IEEE, 2020."
REFERENCES,0.6491228070175439,"Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct"
REFERENCES,0.6535087719298246,"input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637,
2018."
REFERENCES,0.6578947368421053,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes"
REFERENCES,0.6622807017543859,"over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018."
REFERENCES,0.6666666666666666,"David Eric Edmunds and Hans Triebel. Function Spaces, Entropy Numbers, Differential Operators,"
REFERENCES,0.6710526315789473,"volume 120. Cambridge University Press, 2008."
REFERENCES,0.6754385964912281,"Gamaleldin F Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large"
REFERENCES,0.6798245614035088,"margin deep networks for classiﬁcation. arXiv preprint arXiv:1803.05598, 2018."
REFERENCES,0.6842105263157895,"Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-"
REFERENCES,0.6885964912280702,"peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy
of Sciences, 118(43), 2021."
REFERENCES,0.6929824561403509,"Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and infer-"
REFERENCES,0.6973684210526315,"ence. Econometrica, 89(1):181–213, 2021."
REFERENCES,0.7017543859649122,Under review as a conference paper at ICLR 2022
REFERENCES,0.706140350877193,"Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On"
REFERENCES,0.7105263157894737,"the similarity between the Laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580,
2020."
REFERENCES,0.7149122807017544,"Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised con-"
REFERENCES,0.7192982456140351,"strastive learning. In International Conference on Machine Learning, pp. 3821–3830. PMLR,
2021."
REFERENCES,0.7236842105263158,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.7280701754385965,"XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity to and"
REFERENCES,0.7324561403508771,"dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021."
REFERENCES,0.7368421052631579,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-"
REFERENCES,0.7412280701754386,"nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016."
REFERENCES,0.7456140350877193,"Zhezhi He, Adnan Siraj Rakin, and Deliang Fan. Parametric noise injection: Trainable randomness"
REFERENCES,0.75,"to improve deep neural network robustness against adversarial attack.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 588–597, 2019."
REFERENCES,0.7543859649122807,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv"
REFERENCES,0.7587719298245614,"preprint arXiv:1503.02531, 2015."
REFERENCES,0.7631578947368421,"Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparamet-"
REFERENCES,0.7675438596491229,"ric perspective on overparametrized neural network. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 829–837. PMLR, 2021."
REFERENCES,0.7719298245614035,"Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on"
REFERENCES,0.7763157894736842,"noisily labeled data with generalization guarantee. In International Conference on Learning Rep-
resentations, 2020."
REFERENCES,0.7807017543859649,Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
REFERENCES,0.7850877192982456,"entropy in classiﬁcation tasks. arXiv preprint arXiv:2006.07322, 2020."
REFERENCES,0.7894736842105263,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-"
REFERENCES,0.793859649122807,"eralization in neural networks. In Advances in Neural Information Processing Systems, pp. 8571–
8580, 2018."
REFERENCES,0.7982456140350878,Ziwei Ji and Matus Telgarsky. Polylogarithmic width sufﬁces for gradient descent to achieve arbi-
REFERENCES,0.8026315789473685,"trarily small test error with shallow ReLU networks. arXiv preprint arXiv:1909.12292, 2019."
REFERENCES,0.8070175438596491,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron"
REFERENCES,0.8114035087719298,"Maschinot, Ce Liu, and Dilip Krishnan.
Supervised contrastive learning.
arXiv preprint
arXiv:2004.11362, 2020."
REFERENCES,0.8157894736842105,"Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for"
REFERENCES,0.8201754385964912,"classiﬁcation. arXiv preprint arXiv:1812.03599, 2018."
REFERENCES,0.8245614035087719,Michael Kohler and Adam Krzyzak. On the rate of convergence of local averaging plug-in classiﬁca-
REFERENCES,0.8289473684210527,"tion rules under a margin condition. IEEE Transactions on Information Theory, 53(5):1735–1742,
2007."
REFERENCES,0.8333333333333334,"Simon Kornblith, Honglak Lee, Ting Chen, and Mohammad Norouzi. Demystifying loss functions"
REFERENCES,0.8377192982456141,for classiﬁcation. 2020.
REFERENCES,0.8421052631578947,Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
REFERENCES,0.8464912280701754,"descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157–
8166, 2018."
REFERENCES,0.8508771929824561,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu."
REFERENCES,0.8552631578947368,"Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017."
REFERENCES,0.8596491228070176,Under review as a conference paper at ICLR 2022
REFERENCES,0.8640350877192983,Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statis-
REFERENCES,0.868421052631579,"tics, 27(6):1808–1829, 1999."
REFERENCES,0.8728070175438597,"Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statisti-"
REFERENCES,0.8771929824561403,"cal perspective on distillation. In International Conference on Machine Learning, pp. 7632–7642.
PMLR, 2021."
REFERENCES,0.881578947368421,"Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jacques Slotine. Multiclass learning"
REFERENCES,0.8859649122807017,"with simplex coding. arXiv preprint arXiv:1209.1360, 2012."
REFERENCES,0.8903508771929824,"Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and"
REFERENCES,0.8947368421052632,"Anant Sahai. Classiﬁcation vs regression in overparameterized regimes: Does the loss function
matter? arXiv preprint arXiv:2005.08054, 2020."
REFERENCES,0.8991228070175439,Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under
REFERENCES,0.9035087719298246,"neural tangent kernel regime. arXiv preprint arXiv:2006.12297, 2020."
REFERENCES,0.9078947368421053,"Atsushi Nitanda,
Geoffrey Chinot,
and Taiji Suzuki.
Gradient descent can learn less
over-parameterized two-layer neural networks on classiﬁcation problems.
arXiv preprint
arXiv:1905.09870, 2019."
REFERENCES,0.9122807017543859,"Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu. Rethinking softmax"
REFERENCES,0.9166666666666666,"cross-entropy loss for adversarial robustness. arXiv preprint arXiv:1905.10626, 2019."
REFERENCES,0.9210526315789473,"Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal"
REFERENCES,0.9254385964912281,"phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652–24663, 2020."
REFERENCES,0.9298245614035088,Tomaso Poggio and Qianli Liao. Generalization in deep network classiﬁers trained with the square
REFERENCES,0.9342105263157895,"loss. Technical report, CBMM Memo No, 2019."
REFERENCES,0.9385964912280702,Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
REFERENCES,0.9429824561403509,"function. The Annals of Statistics, 48(4):1875–1897, 2020."
REFERENCES,0.9473684210526315,"Ingo Steinwart, Clint Scovel, et al. Fast rates for support vector machines using Gaussian kernels."
REFERENCES,0.9517543859649122,"The Annals of Statistics, 35(2):575–607, 2007."
REFERENCES,0.956140350877193,"Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and Thomas"
REFERENCES,0.9605263157894737,"Sch¨on. Evaluating model calibration in classiﬁcation. In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pp. 3459–3467. PMLR, 2019."
REFERENCES,0.9649122807017544,"Holger Wendland. Scattered Data Approximation. Cambridge University Press, 2004."
REFERENCES,0.9692982456140351,"Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and"
REFERENCES,0.9736842105263158,"discriminative representations via the principle of maximal coding rate reduction. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.9780701754385965,"Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016."
REFERENCES,0.9824561403508771,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical"
REFERENCES,0.9868421052631579,"risk minimization. arXiv preprint arXiv:1710.09412, 2017."
REFERENCES,0.9912280701754386,Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk
REFERENCES,0.9956140350877193,"minimization. The Annals of Statistics, pp. 56–85, 2004."
