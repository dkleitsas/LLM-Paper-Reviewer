Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003246753246753247,"Standard deep reinforcement learning (DRL) agents aim to maximize expected
reward, considering collected experiences equally in formulating a policy. This
differs from human decision-making, where gains and losses are valued differently
and outlying outcomes are given increased consideration. It also wastes an oppor-
tunity for the agent to modulate behavior based on distributional context. Several
approaches to distributional DRL have been investigated, with one popular strategy
being to evaluate the projected distribution of returns for possible actions. We
propose a more direct approach, whereby the distribution of full-episode outcomes
is optimized to maximize a chosen function of its cumulative distribution func-
tion (CDF). This technique allows for outcomes to be weighed based on relative
quality, does not require modiﬁcation of the reward function to modulate agent
behavior, and may be used for both continuous and discrete action spaces. We show
how to achieve an asymptotically consistent estimate of the policy gradient for
a broad class of CDF-based objectives via sampling, subsequently incorporating
variance reduction measures to facilitate effective on-policy learning. We use the
resulting algorithm to train agents with different “risk proﬁles” in penalty-based
formulations of six OpenAI Safety Gym environments, observing that moderate
emphasis on improvement in training scenarios where the agent performs poorly
both increases the accumulation of positive rewards and decreases the frequency
of incurred penalties. We found that, in all environments tested, the same risk
proﬁle can be used to produce both stronger overall performance than standard
Proximal Policy Optimization (PPO) and higher levels of positive reward than PPO
constrained by Lagrangians to maintain the same cost levels."
INTRODUCTION,0.006493506493506494,"1
INTRODUCTION"
INTRODUCTION,0.00974025974025974,"While deep reinforcement learning (DRL) has been used to master an impressive array of simulated
tasks in controlled settings, it has not yet been widely adopted for high-stakes, real-world applications.
One reason for this gap is the lack of distributional perspective in standard artiﬁcial agents. Endowing
agents with such perspective could make their decision-making more robust, potentially leading to
increased safety, increased trust from humans, and more widespread real-world adoption."
INTRODUCTION,0.012987012987012988,"In reinforcement learning (RL), risk arises due to uncertainty around the possible outcomes of an
agent’s future actions. It is a result of randomness in the operating environment, mismatch between
training and test conditions, and the inherent randomness of a stochastic policy. Risk-sensitive policies,
or those that consider more than a mean over the distribution of possible outcomes, offer the potential
for added robustness under uncertain and dynamic conditions. There is an evolving landscape of
algorithmic paradigms for handling risk in RL, from constraint-based approaches adapted from
optimal control (Achiam et al., 2017; Chow et al., 2019; Ray et al., 2019; Zhong et al., 2020) to
adversarial approaches emerging from AI Safety (Garc´ıa & Fern´andez, 2015; Amodei et al., 2016).
Within this landscape, learning approaches that optimize distributional measures offer the ability to
express design preferences over the full distribution of potential outcomes, through the speciﬁcation
of a risk-sensitivity criterion."
INTRODUCTION,0.016233766233766232,"Distributional RL has been studied for value-based methods, with a popular strategy being to use the
distributional Bellman equation to estimate the distribution of Q-values for each member of a discrete
set of potential actions (Bellemare et al., 2017; Dabney et al., 2018a;b). However, distributional RL
has not been widely explored for policy gradient methods, which could permit direct optimization of
risk-sensitive measures and naturally accommodate both discrete and continuous action spaces."
INTRODUCTION,0.01948051948051948,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.022727272727272728,"In the following, we introduce a novel framework for risk-sensitive learning using policy gradients.
Our approach allows agents to be trained with different risk proﬁles through design-time speciﬁcation
of both utility and weight functions, with the latter being deﬁned over the estimated distribution of
full-episode rewards. This framework enables agent-based learning that captures aspects of human
decision-making, such as overemphasis of rare occurrences and diminishing marginal utility relative
to a reference outcome (Kahneman & Tversky, 1979). It also allows implementation of another
key strategy of human learning: emphasizing improvement on tasks where one is deﬁcient. We
demonstrate the ability of our algorithm to use this strategy to improve performance relative to both
unconstrained and constrained methods in six OpenAI Safety Gym environments (Ray et al., 2019)."
RELATED WORK,0.025974025974025976,"2
RELATED WORK"
RELATED WORK,0.02922077922077922,"Constrained RL offers a set of approaches to safe exploration (Garc´ıa & Fern´andez (2015)) that aim
to enforce explicit constraints throughout the learning process via methods including Lagrangian
constraints (Ray et al. (2019)) and constraint coefﬁcients (Achiam et al. (2017)). Differing from the
safe exploration scenario, we here consider problems with distinct training and test phases where
agent performance is to be evaluated. Our experiments indicate that risk-sensitive learning can offer
performance improvements over constrained learning in scenarios where safety constraints need not
be enforced during training."
RELATED WORK,0.032467532467532464,"Distributional approaches to risk-sensitive RL have primarily been explored in the value-based
setting. Therein, the value distribution has been explicitly modeled through categorical techniques
(Bellemare et al., 2017) or quantile regression (Dabney et al., 2018a) and used to improve both value
predictions and overall performance. Recent works utilize distributional modeling in the actor-critic
setting to enable application to continuous action spaces, again demonstrating improved performance
over baseline approaches (Ma et al., 2020; Zhang et al., 2021; Duan et al., 2021). In value-based
approaches, risk-sensitivity criteria are applied at run time as a nonlinear warping of the estimated
value distribution."
RELATED WORK,0.03571428571428571,"Policy gradient approaches offer additional promise for risk-sensitive RL, but require direct optimiza-
tion of a parameterized policy with respect to a distributional objective. Some existing methods are
limited to a speciﬁc class of learning objective, such as the set of concave risk measures that allow
a globally-optimal solution (Tamar et al., 2015; Zhong et al., 2020). Others allow a broader class
of measures but are more restrictive in the class of policies that can be represented (Prashanth et al.,
2016). We aim for a risk-sensitive policy gradient approach that both offers signiﬁcant ﬂexibility in
the choice of learning objective and can learn policies parameterized by a deep neural network."
RELATED WORK,0.03896103896103896,"Various measures have been considered in the context of risk-sensitive RL, including exponential
utility (Pratt, 1964), percentile performance criteria (Wu & Lin, 1999), value-at-risk (Leavens, 1945),
conditional value-at-risk (Rockafellar & Uryasev, 2000), and prospect theory (Kahneman & Tversky,
1979). In this work, we consider a class of risk-sensitivity measures motivated by Cumulative
Prospect Theory (CPT) (Tversky & Kahneman, 1992). CPT uniquely models two key aspects of
human decision-making: (1) a utility function u, computed relative to a reference point that induces
more risk-averse behavior in the presence of gains than losses and (2) a weight function w that
prioritizes outlying events. Speciﬁc forms of u and w are given in Tversky & Kahneman (1992) (and
in Appendix A.4), but the general form of CPT admits a wide variety of risk-sensitive objectives."
RELATED WORK,0.04220779220779221,"Here we show how to train agents to optimize this class of objectives through sampling-based
estimation of their policy gradients and requisite variance reduction. The ﬁnal algorithm resembles
well-known on-policy approaches such as Proximal Policy Optimization (Schulman et al., 2017b) and
is similarly widely applicable. Although we do not explore it here, the incorporation of an appropriate
risk-sensitivity criterion could additionally enable risk-aware exploration and adversarial training for
increased robustness (Pinto et al., 2017; Parisi et al., 2019; Zhang et al., 2020)."
RISK-SENSITIVE POLICY OPTIMIZATION,0.045454545454545456,"3
RISK-SENSITIVE POLICY OPTIMIZATION"
RISK-SENSITIVE POLICY OPTIMIZATION,0.048701298701298704,"In this section we formalize the class of distributional objectives to be considered, derive a sampling-
based approximation of its policy gradient, enact variance reduction on this estimate, and use the
result to produce a practical learning algorithm."
RISK-SENSITIVE POLICY OPTIMIZATION,0.05194805194805195,Under review as a conference paper at ICLR 2022
RISK-SENSITIVE POLICY OPTIMIZATION,0.05519480519480519,"3.1
PRELIMINARIES: PROBLEM AND NOTATION"
RISK-SENSITIVE POLICY OPTIMIZATION,0.05844155844155844,"Standard deep reinforcement learning seeks to maximize the expected reward of an agent over
encountered trajectories; that is, it maximizes the objective"
RISK-SENSITIVE POLICY OPTIMIZATION,0.06168831168831169,J(θ) = Eτ∼pθ(τ)  X
RISK-SENSITIVE POLICY OPTIMIZATION,0.06493506493506493,"t
r(st, at)

.
(1)"
RISK-SENSITIVE POLICY OPTIMIZATION,0.06818181818181818,"Here pθ(τ) is the distribution over trajectories τ ≡s1, a1, . . . , sT , aT induced by a policy parameter-
ized by θ; st, at, and r(st, at) denote the state, action, and reward at time t, respectively. To enable
the incorporation of distributional context, we instead consider the objective"
RISK-SENSITIVE POLICY OPTIMIZATION,0.07142857142857142,"J(θ) =
Z +∞"
RISK-SENSITIVE POLICY OPTIMIZATION,0.07467532467532467,"−∞
u(r(τ))
d
dr(τ)"
RISK-SENSITIVE POLICY OPTIMIZATION,0.07792207792207792,"
w(Pθ(r(τ))

dr(τ),
(2)"
RISK-SENSITIVE POLICY OPTIMIZATION,0.08116883116883117,where u(r(τ)) is the utility associated with full-trajectory reward r(τ) ≡P
RISK-SENSITIVE POLICY OPTIMIZATION,0.08441558441558442,"t r(st, at) and w
is a piecewise differentiable weighting function of the CDF of trajectory reward Pθ(r(τ)) =
R r(τ)
−∞pθ(r′)dr′."
RISK-SENSITIVE POLICY OPTIMIZATION,0.08766233766233766,"Equation 2 is inspired by CPT (Tversky & Kahneman, 1992), which includes a pair of integrals of
this form. It was chosen for its generality; by using different utility functions u and weight functions
w one may represent all of the risk measures mentioned in Section 2 and all of the risk measures
evaluated by Dabney et al. (2018a). The form (2) reduces to (1) when u and w are both the identity
mapping. While designed for the episodic setting, the objective (2) may be considered for inﬁnite
horizons through the choice of appropriately long windows."
RISK-SENSITIVE POLICY GRADIENT,0.09090909090909091,"3.2
RISK-SENSITIVE POLICY GRADIENT"
RISK-SENSITIVE POLICY GRADIENT,0.09415584415584416,"To optimize the objective (2), we ﬁrst derive an approximation to its gradient with respect to the policy
parameters θ. Working toward a representation that can be sampled, we assert the independence of
the reward on θ and use the chain rule to write"
RISK-SENSITIVE POLICY GRADIENT,0.09740259740259741,"∇θJ(θ) =
Z ∞"
RISK-SENSITIVE POLICY GRADIENT,0.10064935064935066,"−∞
u(r(τ))
d
dr(τ)"
RISK-SENSITIVE POLICY GRADIENT,0.1038961038961039,"
w′(Pθ(r(τ)))∇θPθ(r(τ))

dr(τ),
(3)"
RISK-SENSITIVE POLICY GRADIENT,0.10714285714285714,"where w′ is the derivative of w with respect to Pθ(r(τ)). The gradient of the CDF may be written as
follows:"
RISK-SENSITIVE POLICY GRADIENT,0.11038961038961038,∇θPθ(r(τ))=∇θ
RISK-SENSITIVE POLICY GRADIENT,0.11363636363636363,Z r(τ)
RISK-SENSITIVE POLICY GRADIENT,0.11688311688311688,"−∞
pθ(r′)dr′ = ∇θ Z"
RISK-SENSITIVE POLICY GRADIENT,0.12012987012987013,"τ ′ H(r(τ) −r(τ ′))pθ(τ ′)dτ ′ =
Z"
RISK-SENSITIVE POLICY GRADIENT,0.12337662337662338,"τ ′H(r(τ)−r(τ ′))∇θpθ(τ ′)dτ ′ =
Z"
RISK-SENSITIVE POLICY GRADIENT,0.1266233766233766,"τ ′H(r(τ)−r(τ ′))pθ(τ ′)∇θ log pθ(τ ′)dτ ′.
(4)"
RISK-SENSITIVE POLICY GRADIENT,0.12987012987012986,"Here the ﬁrst equality is the integral representation of Pθ(r(τ)), the second uses the Heaviside step
function to select all trajectories with total reward ≤r(τ), the third follows from the independence of
reward on θ, and the fourth follows from the expression for the derivative of the natural logarithm. In
the following, we also use the complementary expression"
RISK-SENSITIVE POLICY GRADIENT,0.1331168831168831,∇θPθ(r(τ)) = ∇θ
RISK-SENSITIVE POLICY GRADIENT,0.13636363636363635,"
1 −
Z ∞"
RISK-SENSITIVE POLICY GRADIENT,0.1396103896103896,"r(τ)
pθ(r′)dr′

= −
Z"
RISK-SENSITIVE POLICY GRADIENT,0.14285714285714285,τ ′ H(r(τ ′) −r(τ))pθ(τ ′)∇θ log pθ(τ ′)dτ ′. (5)
RISK-SENSITIVE POLICY GRADIENT,0.1461038961038961,"Either form, or a combination of the two, may be substituted into (3) and the result sampled over N
trajectories by ﬁrst ordering trajectories i = 1 . . . N by increasing reward r(τ). Then"
RISK-SENSITIVE POLICY GRADIENT,0.14935064935064934,"∇θJ(θ) ≈ N
X"
RISK-SENSITIVE POLICY GRADIENT,0.1525974025974026,"i=1
u(r(τi))

w′
 i N"
RISK-SENSITIVE POLICY GRADIENT,0.15584415584415584,"
∇θPθ(r(τi)) −w′
i −1 N"
RISK-SENSITIVE POLICY GRADIENT,0.1590909090909091,"
∇θPθ(r(τi−1))

,
(6)"
RISK-SENSITIVE POLICY GRADIENT,0.16233766233766234,Under review as a conference paper at ICLR 2022
RISK-SENSITIVE POLICY GRADIENT,0.16558441558441558,"where the term w′(0)∇θPθ(r(τ0)) ≡0. This ordering scheme produces an asymptotically consistent
estimate, as shown in the context of CPT value estimation by Prashanth et al. (2016). ∇θPθ(r(τi))
may be sampled in one of two ways, based on either (4) or (5):"
RISK-SENSITIVE POLICY GRADIENT,0.16883116883116883,"∇θPθ(r(τi)) ≈1 N i
X j=1 Tj
X"
RISK-SENSITIVE POLICY GRADIENT,0.17207792207792208,"t=1
∇θ log πθ(aj,t|sj,t) ≈−1 N N
X j=i+1 Tj
X"
RISK-SENSITIVE POLICY GRADIENT,0.17532467532467533,"t=1
∇θ log πθ(aj,t|sj,t).
(7)"
RISK-SENSITIVE POLICY GRADIENT,0.17857142857142858,"The expression (6) may be used to train a policy that optimizes the distributional objective (2) in a
manner similar to REINFORCE (Williams, 1992)."
VARIANCE REDUCTION,0.18181818181818182,"3.3
VARIANCE REDUCTION"
VARIANCE REDUCTION,0.18506493506493507,"Reducing the variance of sample-based gradient estimates enables faster learning. Here we take
several steps to reduce the variance of (6), as has been done with the policy gradient estimate of
REINFORCE (Williams, 1992)."
VARIANCE REDUCTION,0.18831168831168832,"First, note that cross-trajectory terms of the form f(τi, aj,t, sj,t) = u(r(τi))∇θ log πθ(aj,t|sj,t),
while nonzero, do not contribute to the gradient estimate in expectation when i ̸= j. A proof of this
assertion is given in Appendix A.1. Using (4) for the ﬁrst term of (6) and (5) for the second allows us
to write"
VARIANCE REDUCTION,0.19155844155844157,"∇θJ(θ) ≈ N
X"
VARIANCE REDUCTION,0.19480519480519481,"i=1
u(r(τi))

w′
 i N  1 N i
X j=1 Tj
X"
VARIANCE REDUCTION,0.19805194805194806,"t=1
∇θ log πθ(aj,t|sj,t)"
VARIANCE REDUCTION,0.2012987012987013,"+w′
i −1 N  1 N N
X j=i Tj
X"
VARIANCE REDUCTION,0.20454545454545456,"t=1
∇θ log πθ(aj,t|sj,t)

. (8)"
VARIANCE REDUCTION,0.2077922077922078,Removing cross-trajectory terms gives
VARIANCE REDUCTION,0.21103896103896103,"∇θJ(θ) ≈1 N N
X"
VARIANCE REDUCTION,0.21428571428571427,"i=1
u(r(τi))

w′
 i N"
VARIANCE REDUCTION,0.21753246753246752,"
+ w′
i −1 N"
VARIANCE REDUCTION,0.22077922077922077," Ti
X"
VARIANCE REDUCTION,0.22402597402597402,"t=1
∇θ log πθ(ai,t|si,t).
(9)"
VARIANCE REDUCTION,0.22727272727272727,Note that the weight coefﬁcients (w′( i
VARIANCE REDUCTION,0.2305194805194805,N ) + w′( i−1
VARIANCE REDUCTION,0.23376623376623376,"N )) should be normalized over each batch. The
expression (9) is equivalent to (6) in expectation, but with reduced variance (see Appendix A.1 for
justiﬁcation). It has a clear intuition – trajectories are assigned utilities based on their rewards and
their contributions to the gradient are scaled by the derivative of the weight function, just as they are
in Cumulative Prospect Theory (Tversky & Kahneman, 1992)."
VARIANCE REDUCTION,0.237012987012987,"Standard variance reduction techniques may be applied to this simpliﬁed form. Without further
assumption or introduction of additional bias, a static baseline b can be employed:"
VARIANCE REDUCTION,0.24025974025974026,"∇θJ(θ) ≈1 N N
X i=1"
VARIANCE REDUCTION,0.2435064935064935,"
u(r(τi)) −b

w′
 i N"
VARIANCE REDUCTION,0.24675324675324675,"
+ w′
i −1 N"
VARIANCE REDUCTION,0.25," Ti
X"
VARIANCE REDUCTION,0.2532467532467532,"t=1
∇θ log πθ(ai,t|si,t)
(10)"
VARIANCE REDUCTION,0.2564935064935065,"Justiﬁcation for this assertion is given in Appendix A.2. Learning may be further improved if we
additionally assume that utility may be allocated on a per-step basis. In this case, per-step utilities
are computed as the difference between what the full-episode utility would be if the episode were
to end at a given time step and what it would have been had the episode ended at the previous time
step. While not applicable in cases where episode utility is adjusted based on ﬁnal outcome, this
assumption has the signiﬁcant beneﬁt of modeling the temporal allocation of rewards and aligns
with the standard formulation of RL. With it, the variance of (9) may be further reduced through the
incorporation of utility-to-go and a state-dependent baseline Vφ(si,t):"
VARIANCE REDUCTION,0.2597402597402597,"∇θJ(θ)≈1 N N
X i=1"
VARIANCE REDUCTION,0.262987012987013,"
w′
 i N"
VARIANCE REDUCTION,0.2662337662337662,"
+w′
i −1 N"
VARIANCE REDUCTION,0.2694805194805195," Ti
X"
VARIANCE REDUCTION,0.2727272727272727,"t=1
∇θ log πθ(ai,t|si,t)
 Ti
X"
VARIANCE REDUCTION,0.275974025974026,"t′=t
u(si,t′, ai,t′)−Vφ(si,t)

(11)"
VARIANCE REDUCTION,0.2792207792207792,"Here u(si,t′, ai,t′) is the per-step utility. The value function Vφ(si,t) is parameterized by φ and
trained via regression to minimize"
VARIANCE REDUCTION,0.2824675324675325,"L(φ) =
X i,t"
VARIANCE REDUCTION,0.2857142857142857,"
Vφ(si,t) − Ti
X"
VARIANCE REDUCTION,0.288961038961039,"t′=t
u(si,t′, ai,t′)
2
.
(12)"
VARIANCE REDUCTION,0.2922077922077922,Under review as a conference paper at ICLR 2022
VARIANCE REDUCTION,0.29545454545454547,"A standard argument, similar to the approach taken in (Achiam, 2018), can be used to show that
the incorporation of utility-to-go does not change the expected value of (9). The addition of a
state-dependent baseline also does not introduce additional bias, as justiﬁed in Appendix A.2."
VARIANCE REDUCTION,0.2987012987012987,"Finally, discount factors, bootstrapping, and trust regions may be used to provide additional variance
reduction, just as they are in conventional on-policy learning (Appendix A.2). These measures
may introduce additional bias to the policy gradient estimate, but typically lead to more sample-
efﬁcient learning. In our experiments, we evaluate the use of generalized advantage estimation (GAE;
(Schulman et al., 2016)) based on the utility-to-go as well as the clipping-based trust regions of
Proximal Policy Optimization (PPO; (Schulman et al., 2017b)). Incorporating these in the policy
gradient yields"
VARIANCE REDUCTION,0.30194805194805197,"∇θJ(θ) ≈1 N N
X i=1"
VARIANCE REDUCTION,0.3051948051948052,"
w′
 i N"
VARIANCE REDUCTION,0.30844155844155846,"
+w′
i −1 N"
VARIANCE REDUCTION,0.3116883116883117," Ti
X"
VARIANCE REDUCTION,0.31493506493506496,"t=1
∇θLclip"
VARIANCE REDUCTION,0.3181818181818182,"
log πθ(ai,t|si,t), Aπ
u(si,t, ai,t)

, (13)"
VARIANCE REDUCTION,0.32142857142857145,"where Aπ
u(si,t, ai,t) is the standard GAE except with per-step utilities in place of rewards. Trust
regions are implemented similarly to PPO, pessimistically clipping policy updates to be within a
multiplicative factor of 1 ± ϵ of the existing policy:"
VARIANCE REDUCTION,0.3246753246753247,"Lclip = min

log πθ(ai,t|si,t)Aπ
u(si,t, ai,t),"
VARIANCE REDUCTION,0.32792207792207795,"log

clip
 πθ(ai,t|si,t)"
VARIANCE REDUCTION,0.33116883116883117,"πθold(ai,t|si,t), 1 ± ϵ

πθold(ai,t|si,t)

Aπ
u(si,t, ai,t)

.
(14)"
VARIANCE REDUCTION,0.3344155844155844,"They are used to perform multiple policy updates using the same batch of data, providing learning
that is no longer strictly on-policy but that can be signiﬁcantly more sample efﬁcient. When following
this route, we apply the same early stopping mechanism, based on the Kullback-Leibler divergence
(DKL) between old and new policies, as was used by Ray et al. (2019)."
LEARNING ALGORITHM,0.33766233766233766,"3.4
LEARNING ALGORITHM"
LEARNING ALGORITHM,0.3409090909090909,"The above sample-based estimate of the policy gradient may be used to train agents to maximize
distributional objectives of the form (2). The resulting method, Cumulative Prospect Proximal Policy
Optimization (C3PO), is given in Algorithm 1 and mirrors standard on-policy learning."
LEARNING ALGORITHM,0.34415584415584416,Algorithm 1 Cumulative Prospect Proximal Policy Optimization (C3PO)
LEARNING ALGORITHM,0.3474025974025974,"Require: Policy: initial parameters θ0, learning rate αθ, updates per batch Mθ
Require: Value: initial parameters φ0, learning rate αφ, updates per batch Mφ
Require: Early stopping threshold DKL, stop, discount factor γ"
LEARNING ALGORITHM,0.35064935064935066,"for k = 1, 2, . . . do"
LEARNING ALGORITHM,0.3538961038961039,"Collect set of episodes Dk = {τi} by running policy π(θk) in the environment
Compute per-step utilities u(si,t, ai,t)
Fit value function by regression:
for m = 1, . . . Mφ do"
LEARNING ALGORITHM,0.35714285714285715,"φ ←φ + αφ∇φ
1
P"
LEARNING ALGORITHM,0.36038961038961037,"i Ti
P i,t"
LEARNING ALGORITHM,0.36363636363636365,"
Vφ(si,t) −PTi
t′=t γt′−tu(si,t′, ai,t′)
2"
LEARNING ALGORITHM,0.36688311688311687,"end for
Update utility-based advantage estimates Aπ
u(s, a), using new Vφ(s)
Compute weight coefﬁcients based on ordered episode outcomes and normalize
Update policy, using KL-based early stopping:
for m = 1, . . . Mθ do"
LEARNING ALGORITHM,0.37012987012987014,"if DKL(πθ||πθold) < DKL, stop then"
LEARNING ALGORITHM,0.37337662337662336,θ ←θ + αθ 1
LEARNING ALGORITHM,0.37662337662337664,"N
PN
i=1(w′( i"
LEARNING ALGORITHM,0.37987012987012986,N ) + w′( i−1
LEARNING ALGORITHM,0.38311688311688313,"N )) PTi
t=1 ∇θLclip(log πθ(ai,t|si,t), Aπ
u(si,t, ai,t))
else"
LEARNING ALGORITHM,0.38636363636363635,"break
end if
end for
end for"
LEARNING ALGORITHM,0.38961038961038963,Under review as a conference paper at ICLR 2022
LEARNING ALGORITHM,0.39285714285714285,"Figure 1: Example weight functions and their resulting coefﬁcients in the policy gradient estimate
(9). In these plots, outcomes increase in quality from left to right. As in CPT (Tversky & Kahneman,
1992), weight coefﬁcients are proportional to the derivative of the weight function."
LEARNING ALGORITHM,0.3961038961038961,"Beyond the utility and weight components, Algorithm 1 differs from conventional methods in the
requirement to collect full episodes of data in each batch. This requirement can be removed if
outcomes can be deﬁned over partial rather than full episodes, an assumption that is often viable and
matches human decision-making. For instance, while out of scope for this work, our approach could
be applied to the Atari suite (Bellemare et al., 2013) by considering the outcomes of ﬁxed-length
windows and restructuring Algorithm 1 to mimic minibatch PPO (Schulman et al., 2017b)."
EXPERIMENTS,0.39935064935064934,"4
EXPERIMENTS"
EXPERIMENTS,0.4025974025974026,"To evaluate our approach, we sought to both establish that it can effectively optimize different
distributional objectives and explore the impact of using different objectives on agent outcomes. We
found the OpenAI Safety Gym (Ray et al., 2019) to be suitable for these purposes. Safety Gym is a
conﬁgurable suite of continuous, multidimensional control tasks wherein different types of robots
must navigate through obstacles with different dynamics to perform different tasks. By including
both positive and negative events in each training scenario, it allowed us to evaluate how our various
agents handled risk. Safety Gym is also highly stochastic: the locations of the goals and obstacles are
randomized, leading to outcome variability and forcing the agent to learn a generalized navigation
strategy."
EXPERIMENTS,0.40584415584415584,"Safety Gym logs adverse events but does not incorporate them into the reward function. As our
method relies solely on the training signal from the reward, we assigned each logged adverse event
a ﬁxed, negative reward contribution in experiments using it or other unconstrained agents. Our
initial experiments were conducted with a reward contribution of −0.025, which was found to
allow agents to prioritize reaching goals but deter them from collisions with obstacles. To further
emphasize obstacle avoidance, we doubled this contribution to −0.05 in our experiments using
cautious weightings. These choices and the role they play are further discussed in Section 5."
EXPERIMENTS,0.4090909090909091,"To highlight distributional differences, we focused on the publicly available, obstacle-rich level 2
environments.1 Avoiding the longer compute time of the “Doggo” robot, we evaluated the “Point”
and “Car” robots on each task (“Goal”, “Button”, and “Push”). Further details on these environments
and our rationale for choosing them are given in Appendix A.3."
EXPERIMENTS,0.41233766233766234,"In all experiments, we evaluated ﬁve random seeds and matched the hyperparameters used in the
baselines accompanying Safety Gym (Ray et al., 2019) as closely as possible. The neural networks
used to model both policy and value were multilayer perceptrons (MLPs), with two hidden layers of
256 units each and tanh activations. As in Ray et al. (2019), the policy network outputs the mean
values of a multivariate gaussian with diagonal covariance. The control variances are optimized but
independent of state. The full complement of variance reduction measures were used throughout; see
Appendix A.4 for experimental justiﬁcation of this choice."
DIFFERING OBJECTIVES,0.4155844155844156,"4.1
DIFFERING OBJECTIVES"
DIFFERING OBJECTIVES,0.41883116883116883,"Agent performance was explored under four different distributional objectives. In addition to expected
reward and CPT (conﬁgured to match the original form of Tversky & Kahneman (1992) and as given
in Appendix A.4), we optimized for cautious (η = 0.75) and aggressive (η = −0.75) versions of the"
DIFFERING OBJECTIVES,0.42207792207792205,"1In Safety Gym, the default environments have three levels (0, 1, 2); obstacle density increases with level."
DIFFERING OBJECTIVES,0.4253246753246753,Under review as a conference paper at ICLR 2022
DIFFERING OBJECTIVES,0.42857142857142855,"Figure 2: Impact of different distributional objectives in one environment (CarButton2). The shading
in the ﬁrst 2 plots (and subsequent learning curves) reﬂects the standard deviation associated with
running over 5 random seeds. Left: Net reward (positive reward minus penalty) throughout learning.
Middle: Average number of cost events per episode during training (lower is better). Right: Agent
outcome distribution in testing (with sampling turned off). The cautious (Wang (η = 0.75)) weighting
shows higher reward and lower cost once trained. ."
DIFFERING OBJECTIVES,0.4318181818181818,"distortion risk measure proposed in Wang (2000). This measure is deﬁned as w(p) = Φ(Φ−1(p)+η),
where Φ and Φ−1 are the standard normal cumulative distribution function and its inverse. While
we found this form to be convenient, the “Pow” metric in Dabney et al. (2018a) or any other set of
similarly shaped w curves should achieve a similar effect. In experiments using the objective from
Tversky & Kahneman (1992), the reference point was taken to be the mean episode reward of the
current batch, matching the tendency of humans to change their standards over time. The four weight
functions and their resulting coefﬁcients in (9) are shown in Figure 1."
DIFFERING OBJECTIVES,0.43506493506493504,"Plots of the total rewards (including penalties) in training, average cost events per episode in training,
and outcome distributions in testing are shown for one environment in Figure 2 and for two additional
environments in Appendix A.5. The trends were fairly consistent over the three environments
evaluated in this manner. While no explicit effort was made to handle cost (agents were given only
the sum of positive rewards and penalties), the cautious and aggressive weightings consistently
accumulated relatively low and high costs, respectively. The cautious (Wang(η = 0.75)) agent
typically also generated the highest positive and total rewards after 10 millions steps of training."
DIFFERING OBJECTIVES,0.4383116883116883,"To generate the histograms in Figures 2, 5, 8, 9, and 13 as well as the numbers in Table 1, the trained
agents were deployed on a set of 5000 test episodes – 1000 for each of the 5 networks learned using
different random seeds in training. The resulting distributions therefore include contributions from
both aleatoric and epistemic uncertainty. Sampling was turned off, allowing the agents to choose
their perceived optimal action at each time step. In this context the beneﬁt of emphasizing the lower
part of the outcome distribution (i.e., cautious weighting) became more pronounced, in part because
the methods that emphasize poor outcomes tended to maintain higher policy entropy (Appendix A.5)."
CAUTIOUS WEIGHTINGS,0.44155844155844154,"4.2
CAUTIOUS WEIGHTINGS
To further explore the apparent beneﬁts of cautious weightings in Safety Gym, we trained a series of
variably cautious agents by tuning η in the risk-averse weight function proposed by Wang (2000).
Histograms of their episode rewards in testing are given in Appendix A.5 and summarized in Table
1. In these environments, agent performance – both in terms of improving the lower end of the
reward distribution and on average – was seen to generally improve with increasing η until the range
η ∈[0.75, 1.25], subsequently degrading. Additional comparisons were made with PPO (Schulman
et al., 2017b), which unsurprisingly was found to closely track performance of the “Uniform” agent.
We found that naively incorporating cautious weightings into PPO improved its performance (row
PPO + Wang(0.75) in Table 1), though not to the level of the full C3PO method with η = 0.75."
CAUTIOUS WEIGHTINGS,0.4448051948051948,"We then pursued a set of longer runs to compare C3PO with the cautious objective from Wang (2000)
to both unconstrained and constrained benchmarks. Here we did not tune η, keeping it ﬁxed at 0.75
for all experiments. Comparisons with unconstrained methods for three environments are given in
Figures 3, 4, and 5 and for the remaining three environments in Appendix A.6. In addition to PPO,
we compared performance with Trust Region Policy Optimization (TRPO; Schulman et al. (2017a))
as conﬁgured in Ray et al. (2019). Since this TRPO conﬁguration generally outperformed the PPO
conﬁguration in Ray et al. (2019) from which we derived the hyperparameters for C3PO, we would
expect C3PO to be at a disadvantage compared to TRPO. However, we found C3PO had the highest"
CAUTIOUS WEIGHTINGS,0.44805194805194803,Under review as a conference paper at ICLR 2022
CAUTIOUS WEIGHTINGS,0.4512987012987013,"PointButton2
CarGoal2
CarButton2
Mean
Std
Q=0.5
Q=.05
Mean
Std
Q=0.5
Q=.05
Mean
Std
Q=0.5
Q=.05
Uniform
22.5
7.1
22.8
10.7
18.1
6.6
18.0
7.4
12.4
9.1
14.0
-6.1
CPT Value
16.5
7.3
16.3
4.9
13.9
7.7
14.6
-0.5
9.5
10.7
11.2
-11.0
Wang (-0.75)
17.5
6.1
17.6
7.6
13.5
6.9
14.0
0.9
6.4
10.7
8.9
-15.5
Wang (0.5)
23.3
6.3
23.3
13.1
18.5
6.0
18.8
8.2
12.9
9.0
14.4
-5.6
Wang (0.75)
24.2
6.7
24.4
13.7
19.0
6.4
19.3
8.2
14.3
9.5
15.8
-4.4
Wang (1.0)
24.7
6.0
24.9
15.1
20.3
6.9
21.3
7.2
11.4
11.0
13.8
-12.2
Wang (1.25)
25.4
6.1
25.4
15.9
17.3
6.7
17.8
5.6
12.7
10.4
14.8
-8.3
Wang (1.50)
23.6
5.9
23.7
14.1
16.6
7.5
17.2
3.1
10.1
12.0
13.3
-17.9
Wang (1.75)
23.4
6.2
23.5
13.5
12.5
8.0
13.0
-0.6
7.3
13.0
11.1
-22.8
PPO
19.0
6.4
18.9
9.1
15.8
6.0
15.8
5.8
8.3
9.5
9.5
-9.8
PPO + Wang(0.75)
20.4
8.4
21.7
2.6
17.7
6.7
18.0
6.3
12.1
9.5
13.6
-6.8
Table 1: Testing statistics for episode rewards achieved by agents trained over 10 million steps with
different distributional objectives. Q = 0.5 is the median and Q = 0.05 refers to the location of the
0.05 quantile. Blue bold-face represents the best performance for a given environment; in all cases
these occur for moderately cautious weightings (η ∈[0.75, 1.25])."
CAUTIOUS WEIGHTINGS,0.45454545454545453,"Figure 3: Average episode reward (including penalty) over training for different learning approaches
in three different environments. C3PO with Wang (η = 0.75) weighting outperforms others."
CAUTIOUS WEIGHTINGS,0.4577922077922078,"average reward (including penalty) in ﬁve of the six environments and lowest average cost in ﬁve
of the six environments. In addition, agents that used the cautious weightings tended to have more
stable and repeatable training, as evidenced by the tight distribution of their learning curves. This
tightness was found to reﬂect a lack of negative outlier episodes and potentially lower epistemic
uncertainty throughout training. Finally, note that the use of a nonzero penalty for cost events resulted
in signiﬁcantly lower incurred costs than were observed with unconstrained agents trained without a
penalty (Ray et al., 2019). PPO and TRPO were seen to reach similar cost levels without a penalty;
these levels are indicated by red dashed lines in the cost ﬁgures."
CAUTIOUS WEIGHTINGS,0.461038961038961,"Comparisons with versions of PPO and TRPO that use Lagrangian constraints (PPO-Lagrangian
and TRPO-Lagrangian; Ray et al. (2019)) to match the cost level of C3PO are shown in Figure 6
and Appendix A.7 . We see that, given the same level of cost incurred per episode, agents trained
using C3PO consistently achieve higher levels of reward than those trained with PPO-Lagrangian
and TRPO-Lagrangian. As above, training is seen to be more stable and repeatable using our risk-
sensitive method. Additional comparisons were generated with Constrained Policy Optimization
(CPO; Achiam et al. (2017)), but are not shown in Figure 6 because they failed to maintain the cost
levels of the other methods. For completeness, they are given in Appendix A.7."
CAUTIOUS WEIGHTINGS,0.4642857142857143,"Figure 4: Average number of penalty events per episode (lower is better) over training for different
learning approaches in three different environments. The horizontal lines reﬂect the cost levels
reached by both PPO and TRPO training with zero penalty in Ray et al. (2019)."
CAUTIOUS WEIGHTINGS,0.4675324675324675,Under review as a conference paper at ICLR 2022
CAUTIOUS WEIGHTINGS,0.4707792207792208,"Figure 5: Testing reward distributions (including penalty; sampling turned off) for long training runs
of three Safety Gym environments."
CAUTIOUS WEIGHTINGS,0.474025974025974,"Figure 6: Comparison of positive contributions to episode reward during training for our approach
(yellow) and Lagrangian methods conﬁgured to have the same cost level."
DISCUSSION,0.4772727272727273,"5
DISCUSSION"
DISCUSSION,0.4805194805194805,"The analysis above allows for sample-based policy gradient estimates of a broad class of distributional
objectives. Variance reduction measures were shown to enable efﬁcient optimization based on these
estimates (Appendix A.4). However, it was not seen to be the case that a given distributional objective
could be most effectively optimized directly. Instead, the best results were generally obtained through
moderate emphasis on improving negative training outcomes (Table 1)."
DISCUSSION,0.4837662337662338,"To understand this behavior, consider the interplay of optimization and exploration in the training
of cautious and aggressive agents. Cautious weightings continually emphasize the lower part of the
outcome distribution, pushing that part of the distribution upward and adjusting behavior the most
where it is most necessary. Once a part of the state space where the agent is deﬁcient is adequately
addressed, a different part of the state space takes its place. Policy entropy remains high because of
the emphasis on problematic situations, ensuring adequate exploration. This trend continues with
increasing η, until the point where the agent begins to ignore high quality training outcomes too
much. Conversely, aggressive weightings continually emphasize the best outcomes in the distribution.
When an already strong outcome is given increased attention, it is likely to stay at the top. Hence
agents trained with aggressive weightings tend to become myopic, obsessing over a fraction of the
state space while neglecting the rest of it. They tend to explore inadequately and ironically fail to
attain better top-end performance than more cautious weightings."
DISCUSSION,0.487012987012987,"Given the consistent performance gains observed using our method, we propose that it represents
a useful option for improving the performance and stability of on-policy learners. This should be
particularly true in the presence of a meaningful trade-off between positive and negative reward terms
and when there is signiﬁcant stochasticity. While our approach does add an additional hyperparameter
– the shaping constant η – one value for that hyperparameter was seen to provide gains across all
environments tested. While our approach does not provide for a direct choice of cost limit as
constrained methods do, it is simpler to implement and was consistently seen to be more performant
for the cost level it reached. It is also likely possible to use cautious weightings in conjunction with
constrained RL, though this has not yet been investigated."
CONCLUSIONS,0.4902597402597403,"6
CONCLUSIONS"
CONCLUSIONS,0.4935064935064935,"In this work, we proposed a risk-sensitive learning algorithm based on a policy gradient estimate
for a broad class of distributional objectives. When conﬁgured to emphasize improvement in
scenarios where the agent performs poorly, we found our method to compare favorably with existing
unconstrained and constrained on-policy learners."
CONCLUSIONS,0.4967532467532468,Under review as a conference paper at ICLR 2022
REFERENCES,0.5,REFERENCES
REFERENCES,0.5032467532467533,"Josh Achiam. Openai spinning up: Proof for don’t let the past distract you. http://spinningup.
openai.com/en/latest/spinningup/extra_pg_proof1.html, 2018."
REFERENCES,0.5064935064935064,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 22–31, 2017."
REFERENCES,0.5097402597402597,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. arXiv:1606.06565, 2016."
REFERENCES,0.512987012987013,"M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,
jun 2013."
REFERENCES,0.5162337662337663,"Marc G. Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning (ICML),
volume 70, pp. 449–458, 2017."
REFERENCES,0.5194805194805194,"Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar A. Du´e˜nez-
Guzm´an. Lyapunov-based safe policy optimization for continuous control. CoRR, abs/1901.10031,
2019. URL http://arxiv.org/abs/1901.10031."
REFERENCES,0.5227272727272727,"Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning (ICML), volume 80, pp. 1096–1105, 2018a."
REFERENCES,0.525974025974026,"Will Dabney, Mark Rowland, Marc G. Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the 32nd AAAI Conference on Artiﬁcial
Intelligence (AAAI), pp. 2892–2901, 2018b."
REFERENCES,0.5292207792207793,"Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional
soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors. IEEE
Transactions on Neural Networks and Learning Systems, pp. 1–15, 2021."
REFERENCES,0.5324675324675324,"Javier Garc´ıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(42):1437–1480, 2015."
REFERENCES,0.5357142857142857,"Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. Econo-
metrica, 47(2):263–291, 1979."
REFERENCES,0.538961038961039,"Dickson H. Leavens. Diversiﬁcation of investments. Trusts and Estates, 80:469–473, 1945."
REFERENCES,0.5422077922077922,"Xiaoteng Ma, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. DSAC: Distributional soft
actor critic for risk-sensitive reinforcement learning. arXiv:2004.14547, June 2020."
REFERENCES,0.5454545454545454,"German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 113:54–71, May 2019."
REFERENCES,0.5487012987012987,"Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning (ICML),
2017."
REFERENCES,0.551948051948052,"L.A. Prashanth, Cheng Jie, Michael C. Fu, Steven I. Marcus, and Csaba Szepesv´ari. Cumulative
prospect theory meets reinforcement learning: Prediction and control. In Proceedings of the 33nd
International Conference on Machine Learning (ICML), pp. 1406–1415, 2016."
REFERENCES,0.5551948051948052,"John W. Pratt. Risk aversion in the small and in the large. Econometrica, 32:122–136, 1964."
REFERENCES,0.5584415584415584,"Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement
learning. https://cdn.openai.com/safexp-short.pdf, 2019."
REFERENCES,0.5616883116883117,"R. Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of
Risk, 2:21–41, 2000."
REFERENCES,0.564935064935065,Under review as a conference paper at ICLR 2022
REFERENCES,0.5681818181818182,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In Proceedings of the International
Conference on Learning Representations (ICLR), 2016."
REFERENCES,0.5714285714285714,"John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML), 2017a."
REFERENCES,0.5746753246753247,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv:1707.06347, 2017b."
REFERENCES,0.577922077922078,"Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for
coherent risk measures. In Proceedings of the 28th International Conference on Neural Information
Processing Systems (NeurIPS), pp. 1468–1476, 2015."
REFERENCES,0.5811688311688312,"Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of
uncertainty. Journal of Risk and Uncertainty, 5(4):297–323, 1992."
REFERENCES,0.5844155844155844,"Shaun S. Wang. A class of distortion operators for pricing ﬁnancial and insurance risks. The Journal
of Risk and Insurance, 67(1):15, March 2000."
REFERENCES,0.5876623376623377,"Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3):229–256, May 1992."
REFERENCES,0.5909090909090909,"Congbin Wu and Yuanlie Lin. Minimizing risk models in Markov decision processes with policies
depending on target values. Journal of Mathematical Analysis and Applications, 231:47–67, 1999."
REFERENCES,0.5941558441558441,"Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state observations. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Proceedings of the 34th
International Conference on Neural Information Processing Systems (NeurIPS), volume 33, pp.
21024–21037, 2020."
REFERENCES,0.5974025974025974,"Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning
via distributional risk in the dual domain. IEEE Journal on Selected Areas in Information Theory,
2(2):611–626, 2021."
REFERENCES,0.6006493506493507,"Han Zhong, Ethan X. Fang, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive deep RL: Variance-
constrained actor-critic provably ﬁnds globally optimal policy. arXiv:2012.14098, 2020."
REFERENCES,0.6038961038961039,"A
APPENDIX"
REFERENCES,0.6071428571428571,"A.1
EVALUATION OF CROSS-TRAJECTORY TERMS IN POLICY GRADIENT"
REFERENCES,0.6103896103896104,"In this section, we ﬁrst show that the cross-trajectory terms in our policy gradient estimate (6) have
an expectation value of 0. We then argue that their removal leads to a policy gradient estimate with
reduced variance."
REFERENCES,0.6136363636363636,"Lemma 1. Cross-trajectory terms of the form f(τi, aj,t, sj,t) = u(r(τi))∇θ log πθ(aj,t|sj,t), where
i ̸= j, do not contribute to the gradient estimate (6) in expectation."
REFERENCES,0.6168831168831169,"Proof. First, note that"
REFERENCES,0.6201298701298701,"Eτi∼pθ(τ),τj∼pθ(τ)f(τi, aj,t, sj,t) = Eτi∼pθ(τ),τj∼pθ(τ)u(r(τi))∇θ log πθ(aj,t|sj,t)"
REFERENCES,0.6233766233766234,"= Eτi∼pθ(τ) """
REFERENCES,0.6266233766233766,u(r(τi))Eτj∼pθ(τ)
REFERENCES,0.6298701298701299,"
∇θ log πθ(aj,t|sj,t)
τi #"
REFERENCES,0.6331168831168831,Under review as a conference paper at ICLR 2022
REFERENCES,0.6363636363636364,Then consider the innermost expectation:
REFERENCES,0.6396103896103896,Eτj∼pθ(τ)
REFERENCES,0.6428571428571429,"
∇θ log πθ(aj,t|sj,t)
τi 
=
Z"
REFERENCES,0.6461038961038961,"sj,t,aj,t
p(sj,t, aj,t|πθ, τi)∇θ log πθ(aj,t|sj,t)daj,tdsj,t =
Z"
REFERENCES,0.6493506493506493,"sj,t
p(sj,t|πθ, τi)
Z"
REFERENCES,0.6525974025974026,"aj,t
πθ(aj,t|sj,t)∇θ log πθ(aj,t|sj,t)daj,tdsj,t =
Z"
REFERENCES,0.6558441558441559,"sj,t
p(sj,t|πθ, τi)
Z"
REFERENCES,0.6590909090909091,"aj,t
∇θπθ(aj,t|sj,t)daj,tdsj,t =
Z"
REFERENCES,0.6623376623376623,"sj,t
p(sj,t|πθ, τi)∇θ Z"
REFERENCES,0.6655844155844156,"aj,t
πθ(aj,t|sj,t)daj,tdsj,t =
Z"
REFERENCES,0.6688311688311688,"sj,t
p(sj,t|πθ, τi)(∇θ1)dsj,t = 0."
REFERENCES,0.672077922077922,"To see why removal of cross-trajectory terms leads to reduced variance, consider that the
full expression (6) may be written as the sum of terms of the form f(τi, aj,t, sj,t)
=
u(r(τi))∇θ log πθ(aj,t, sj,t). Its variance is the sum of the total variance from terms where i = j,
the total variance from terms where i ̸= j, and a term proportional to the covariance of these two
totals. However, because each term in the covariance contains at least one trajectory that differs from
the rest, the above reasoning may be applied to argue that the covariance is 0. Hence, the removal of
the cross-trajectory terms lowers the variance of the policy gradient estimate by the variance of the
cross-trajectory terms."
REFERENCES,0.6753246753246753,"A.2
INTRODUCTION OF STATIC AND STATE-DEPENDENT BASELINES"
REFERENCES,0.6785714285714286,"Lemma 2. A static baseline of the utility may be added to the policy gradient estimate (9) without
introduction of bias."
REFERENCES,0.6818181818181818,Proof. The additional term is 0 in expectation as
REFERENCES,0.685064935064935,Eτi∼pθ(τ)
REFERENCES,0.6883116883116883,"
b

w′
 i n"
REFERENCES,0.6915584415584416,"
+ w′
i −1 n"
REFERENCES,0.6948051948051948,"
∇θ log πθ(ai,t|si,t)
"
REFERENCES,0.698051948051948,"=b

w′
 i n"
REFERENCES,0.7012987012987013,"
+ w′
i −1 n  Z"
REFERENCES,0.7045454545454546,"si,t,ai,t
p(si,t, ai,t|πθ)

∇θ log πθ(ai,t|si,t)

dai,tdsi,t"
REFERENCES,0.7077922077922078,"=b

w′
 i n"
REFERENCES,0.711038961038961,"
+ w′
i −1 n  Z"
REFERENCES,0.7142857142857143,"si,t
p(si,t|πθ)
Z"
REFERENCES,0.7175324675324676,"ai,t
πθ(ai,t|si,t)∇θ log πθ(ai,t|si,t)dai,tdsi,t"
REFERENCES,0.7207792207792207,"=b

w′
 i n"
REFERENCES,0.724025974025974,"
+ w′
i −1 n  Z"
REFERENCES,0.7272727272727273,"si,t
p(si,t|πθ)∇θ Z"
REFERENCES,0.7305194805194806,"ai,t
πθ(ai,t|si,t)dai,tdsi,t"
REFERENCES,0.7337662337662337,"=b

w′
 i n"
REFERENCES,0.737012987012987,"
+ w′
i −1 n  Z"
REFERENCES,0.7402597402597403,"si,t
p(si,t|πθ)(∇θ1)dsi,t = 0."
REFERENCES,0.7435064935064936,The contribution of the weight terms (w′( i
REFERENCES,0.7467532467532467,n) + w′( i−1
REFERENCES,0.75,"n )) may be pulled out of the integral between
the ﬁrst and second line because of its independence on both state and action. This term is ﬁxed for a
given trajectory by the rank of its reward amongst the rewards accumulated on all trajectories in the
current batch."
REFERENCES,0.7532467532467533,"In our variance reduction experiments (Appendix A.4), the “Base” agent uses b equal to the mean of
full-episode utility in the current batch."
REFERENCES,0.7564935064935064,"As described in Section 3.3, we may further adjust the policy gradient estimate through introduction
of per-step utilities. In this case, we may justify the use of a state-dependent baseline through the
following."
REFERENCES,0.7597402597402597,Under review as a conference paper at ICLR 2022
REFERENCES,0.762987012987013,"Lemma 3. A state-dependent baseline Vφ(si,t) may be added to the policy gradient estimate (9)
without introduction of bias, if per-step utilities are assumed."
REFERENCES,0.7662337662337663,Proof. The additional term is 0 in expectation as
REFERENCES,0.7694805194805194,Eτi∼pθ(τ)
REFERENCES,0.7727272727272727,"
w′
 i n"
REFERENCES,0.775974025974026,"
+ w′
i −1 n"
REFERENCES,0.7792207792207793,"
∇θ log πθ(ai,t|si,t)Vφ(si,t)
"
REFERENCES,0.7824675324675324,"=

w′
 i n"
REFERENCES,0.7857142857142857,"
+ w′
i −1 n  Z"
REFERENCES,0.788961038961039,"si,t,ai,t
p(si,t, ai,t|πθ)Vφ(si,t)∇θ log πθ(ai,t|si,t)dai,tdsi,t"
REFERENCES,0.7922077922077922,"=

w′
 i n"
REFERENCES,0.7954545454545454,"
+ w′
i −1 n  Z"
REFERENCES,0.7987012987012987,"si,t
p(si,t|πθ)Vφ(si,t)
Z"
REFERENCES,0.801948051948052,"ai,t
πθ(ai,t|si,t)∇θ log πθ(ai,t|si,t)dai,tdsi,t"
REFERENCES,0.8051948051948052,"=

w′
 i n"
REFERENCES,0.8084415584415584,"
+ w′
i −1 n  Z"
REFERENCES,0.8116883116883117,"si,t
p(si,t|πθ)Vφ(si,t)∇θ Z"
REFERENCES,0.814935064935065,"ai,t
πθ(ai,t|si,t)dai,tdsi,t"
REFERENCES,0.8181818181818182,"=

w′
 i n"
REFERENCES,0.8214285714285714,"
+ w′
i −1 n  Z"
REFERENCES,0.8246753246753247,"si,t
p(si,t|πθ)Vφ(si,t)(∇θ1)dsi,t = 0."
REFERENCES,0.827922077922078,The rationale for pulling the w′ terms out of the integral is the same as in Lemma 2.
REFERENCES,0.8311688311688312,"Finally, we note that the ability to pull the contribution of the weight terms (w′( i"
REFERENCES,0.8344155844155844,n) + w′( i−1
REFERENCES,0.8376623376623377,"n )) to the
front of Equation 11 allows us to formulate advantage estimates based on per-step utility. Bootstrap
estimates of the value function Vφ(si,t) and Generalized Advantage Estimation as in Schulman et al.
(2016) can be conducted exactly as they are in standard on-policy learning, if rewards are replaced by
per-step utilities."
REFERENCES,0.8409090909090909,"A.3
ADDITIONAL INFORMATION ON SAFETY GYM"
REFERENCES,0.8441558441558441,"As mentioned in Section 4, we chose to evaluate our approach using the OpenAI Safety Gym (Ray
et al., 2019). The choice was governed by our desire to test in conditions with clear cost-beneﬁt
trade-offs, signiﬁcant stochasticity, adequate complexity, and available benchmarks. While our
methods are not limited to particular task types or observation/action spaces, we found Safety Gym
to be suitable for exploring their potential."
REFERENCES,0.8474025974025974,"The six environments chosen were the most obstacle-rich of the publicly available environments that
used the “Point” and “Car” robots. The Point robot is constrained to the 2D plane and has two control
dimensions: one for moving forward/backward and one for turning. The Car robot also has two
control dimensions, corresponding to independently actuated parallel wheels. It has a freely rotating
wheel and, while it is not constrained to the 2D plane, typically remains in it. While we expect our
results to extend to the remaining default robot, “Doggo”, we did not experiment with it because of
the order of magnitude longer training times it exhibited in Ray et al. (2019)."
REFERENCES,0.8506493506493507,"Several types of obstacles and tasks were present in the environments we evaluated. In all cases, the
robot is given a ﬁxed amount of time (1000 steps) to complete the prescribed task as many times as
possible and is motivated by both sparse and dense reward contributions. In the “Goal” environments,
the robot must navigate to a series of randomly-assigned goal positions, with a new target being
assigned as soon as a goal is reached. In the “Button” environments, the robot must reach and press
a sequence of goal buttons while avoiding other buttons. In the “Push” task, the robot must push a
box to a series of goal positions. The set of obstacles are different for each task; among the three
environments there are a total of ﬁve different constraint elements (hazards, vases, incorrect buttons,
pillars, and gremlins), each with different dynamics. See Ray et al. (2019) for further details."
REFERENCES,0.8538961038961039,"A.4
EMPIRICAL PERFORMANCE OF VARIANCE REDUCTION MEASURES"
REFERENCES,0.8571428571428571,"To gauge the impact of the variance reduction techniques outlined in Section 3.3, we evaluated their
performance in maximizing the value function of Cumulative Prospect Theory (Tversky & Kahneman,
1992). As mentioned in Section 3.1, this function has two integrals of the form (2):"
REFERENCES,0.8603896103896104,Under review as a conference paper at ICLR 2022
REFERENCES,0.8636363636363636,"Figure 7: Impact of variance reduction measures on optimization of the CPT value function. Here
“Base” refers to the risk-sensitive policy gradient estimate (10), “UTG” adds utility-to-go and a neural
network baseline (11), “GAE” incorporates generalized advantage estimation, and “TR” implements
trust regions via clipping. Shading represents the variation over ﬁve random seeds."
REFERENCES,0.8668831168831169,"J(θ) = −
Z ∞"
REFERENCES,0.8701298701298701,"−∞
u−(r(τ))
d
dr(τ)"
REFERENCES,0.8733766233766234,"
w−(Pθ(r(τ)))

dr(τ) +
Z ∞"
REFERENCES,0.8766233766233766,"−∞
u+(r(τ))
d
dr(τ)"
REFERENCES,0.8798701298701299,"
−w+(1 −Pθ(r(τ)))

dr(τ)
(15)"
REFERENCES,0.8831168831168831,"In Tversky & Kahneman (1992), the utility functions are computed relative to a reference point and
reﬂect the tendency of humans to be more risk-averse in the presence of gains than in the presence
of losses. The weight functions {w+, w−} model our inclination to emphasize the best and worst
possible outcomes in our decision-making."
REFERENCES,0.8863636363636364,"More speciﬁcally, in these experiments we used the piecewise utility functions u+(r) = H(r −
r0)(r −r0)σ and u−= λH(r0 −r)(r0 −r)σ with static reference r0 = 10, σ = 0.88, and λ = 2.25.
The weight function w(p) =
pη"
REFERENCES,0.8896103896103896,"(pη+(1−p)η)
1
η was used, where η = 0.61 for r < r0 and η = 0.69 for"
REFERENCES,0.8928571428571429,"r ≥r0. Four methods were evaluated, incorporating progressive amounts of variance reduction:"
REFERENCES,0.8961038961038961,• Base: Risk-sensitive policy gradient with a static baseline (10)
REFERENCES,0.8993506493506493,• UTG: Base with utility-to-go and a neural network baseline (11)
REFERENCES,0.9025974025974026,• GAE: UTG with generalized advantage estimation ((13) without clipping)
REFERENCES,0.9058441558441559,• TR: GAE with trust regions ((13) with clipping (14))
REFERENCES,0.9090909090909091,"As shown in Figure 7, the incorporation of these techniques increased the sample efﬁciency of the
CPT value optimization signiﬁcantly. Consequently, we used the full complement (TR) in all other
experiments."
REFERENCES,0.9123376623376623,"A.5
DIFFERING OBJECTIVES: ADDITIONAL RESULTS"
REFERENCES,0.9155844155844156,Below we include results for all environments for the experiments described in Section 4.1.
REFERENCES,0.9188311688311688,Under review as a conference paper at ICLR 2022
REFERENCES,0.922077922077922,"Figure 8: Impact of different distributional objectives in remaining two environments of initial trials.
The shading reﬂects the standard deviation associated with running over 5 random seeds. Left: Net
reward (positive reward minus penalty) throughout learning. Middle: Average number of cost events
per episode during training (lower is better). Right: Agent outcome distribution in testing (i.e., with
sampling turned off)."
REFERENCES,0.9253246753246753,"Figure 9: Agent outcome distributions across trials run over increasingly cautious (η increasing)
objectives. Distributions correspond to results shown in Table 1."
REFERENCES,0.9285714285714286,"In addition, we note the trend of policy entropies with different distributional objectives. In general,
more cautious weightings maintain higher entropy for longer than more aggressive weightings. Note
that these plots represent an upper bound because they do not account for action clipping by the
environment; see Ray et al. (2019) for details."
REFERENCES,0.9318181818181818,"Figure 10: Policy entropy progression during training for three environments. Shading reﬂects the
observed variation over 5 random seeds."
REFERENCES,0.935064935064935,Under review as a conference paper at ICLR 2022
REFERENCES,0.9383116883116883,"A.6
ADDITIONAL COMPARISONS WITH UNCONSTRAINED METHODS"
REFERENCES,0.9415584415584416,"Below are plots of average episode reward and average number of episode cost events throughout
training for the remainder of the environments on which we conducted long runs (Section 4.2). Also
included are histograms of testing performance for those runs."
REFERENCES,0.9448051948051948,"Figure 11: Average episode reward (including penalty) over training for different unconstrained
learning approaches in remaining three environments."
REFERENCES,0.948051948051948,"Figure 12: Average number of cost events per episode (lower is better) over training for different
unconstrained learning approaches in remaining three environments. As above, the “zero-penalty”
line refers to the level reached by PPO and TRPO trained with no penalty in the reward (Ray et al.,
2019)."
REFERENCES,0.9512987012987013,"Figure 13: Testing reward distributions (including penalty; sampling turned off) for long training
runs in the remaining three Safety Gym environments. In ﬁve of the six environments, C3PO with
η = 0.75 provides tangible beneﬁt. A smaller η is likely required to improve performance on
CarPush2."
REFERENCES,0.9545454545454546,"A.7
ADDITIONAL COMPARISONS WITH CONSTRAINED METHODS"
REFERENCES,0.9577922077922078,"As mentioned in Section 4.2, we compared the performance of C3PO with constrained methods by
setting the cost limit of the constrained methods to match the cost level attained by C3PO. Here we
provide"
REFERENCES,0.961038961038961,"• the positive reward plots for the remaining three Safety Gym environments studied,
• the cost plots for each of the six environments, and"
REFERENCES,0.9642857142857143,Under review as a conference paper at ICLR 2022
REFERENCES,0.9675324675324676,• all plots for Constrained Policy Optimization (CPO; Achiam et al. (2017)).
REFERENCES,0.9707792207792207,"The intent of the cost plots of Figure 15 is to show rough consistency between the cost levels of
our approach and Lagrangian methods conﬁgured to have the same cost limit. This is veriﬁed, but
other trends should be noted. First, while the Lagrangian-based methods typically follow the cost
constraint well, they cannot satisfy it in each batch. Second, our approach tends to have comparable
or lower cost rates throughout training. This safe exploration metric, deﬁned in Ray et al. (2019),
refers to the average cost per episode over all of training up to a given point."
REFERENCES,0.974025974025974,"Results related to Constrained Policy Optimization (CPO; Achiam et al. (2017)) are included here but
not in the main text because, consistent with (Ray et al., 2019), we were not able to conﬁgure CPO to
respect the cost levels of the other constrained methods. Here we show the cost levels reached by
CPO compared with C3PO (Figure 16) as well as a comparison of the average episode rewards of
the two (Figure 17). For the latter, we employed the penalty scaling used by C3PO to enable a fair
comparison."
REFERENCES,0.9772727272727273,"Figure 14: Comparison of positive contributions to episode reward during training for our approach
(yellow) and Lagrangian methods conﬁgured to have the same cost level. The plots for the other three
environments are shown in Figure 6."
REFERENCES,0.9805194805194806,"Figure 15: Comparison of cost incurred (lower is better) during training for our approach and
Lagrangian methods conﬁgured to have the same cost level. As intended, cost levels are consistently
matched between the methods. As above, the “zero-penalty” line refers to the level reached by PPO
and TRPO trained with no penalty in the reward (Ray et al., 2019)."
REFERENCES,0.9837662337662337,Under review as a conference paper at ICLR 2022
REFERENCES,0.987012987012987,"Figure 16: Comparison of cost incurred (lower is better) during training for our method and Con-
strained Policy Optimization (Achiam et al., 2017) conﬁgured to have a matching cost limit. Results
are consistent with Ray et al. (2019). As above, the “zero-penalty” line refers to the level reached by
unconstrained PPO and TRPO trained with no penalty in the reward (Ray et al., 2019)."
REFERENCES,0.9902597402597403,Figure 17: Comparison of average episode reward (including penalty) for C3PO and CPO.
REFERENCES,0.9935064935064936,"A.8
SUPPLEMENTARY MATERIALS"
REFERENCES,0.9967532467532467,The code used to produce these results is included in our Supplementary Materials.
