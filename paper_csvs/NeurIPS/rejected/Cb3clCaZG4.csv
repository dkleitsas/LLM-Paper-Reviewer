Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001869158878504673,"Graph pooling is crucial for enlarging receptive field and reducing computational
1"
ABSTRACT,0.003738317757009346,"cost in deep graph representation learning. In this work, we propose a simple
2"
ABSTRACT,0.005607476635514018,"but effective non-deterministic graph pooling method, called graph Bernoulli
3"
ABSTRACT,0.007476635514018692,"pooling (BernPool), to facilitate graph feature learning. In contrast to most graph
4"
ABSTRACT,0.009345794392523364,"pooling methods with deterministic modes, we design a probabilistic Bernoulli
5"
ABSTRACT,0.011214953271028037,"sampling to reach an expected sampling rate through deducing a variational bound
6"
ABSTRACT,0.013084112149532711,"as the constraint. To further mine more useful info, a learnable reference set is
7"
ABSTRACT,0.014953271028037384,"introduced to encode nodes into a latent expressive probability space. Hereby
8"
ABSTRACT,0.016822429906542057,"the resultant Bernoulli sampling would endeavor to capture salient substructures
9"
ABSTRACT,0.018691588785046728,"of the graph while possessing much diversity on sampled nodes due to its non-
10"
ABSTRACT,0.020560747663551402,"deterministic manner. Considering the complementarity of node dropping and
11"
ABSTRACT,0.022429906542056073,"node clustering, further, we propose a hybrid graph pooling paradigm to combine
12"
ABSTRACT,0.024299065420560748,"a compact subgraph (via dropping) and a coarsening graph (via clustering), in
13"
ABSTRACT,0.026168224299065422,"order to retain both representative substructures and input graph info. Extensive
14"
ABSTRACT,0.028037383177570093,"experiments on multiple public graph classification datasets demonstrate that our
15"
ABSTRACT,0.029906542056074768,"BernPool is superior to various graph pooling methods, and achieves state-of-
16"
ABSTRACT,0.03177570093457944,"the-art performance. The code is publicly available in an anonymous format at
17"
ABSTRACT,0.03364485981308411,"https:/github/BernPool.
18"
INTRODUCTION,0.03551401869158879,"1
Introduction
19"
INTRODUCTION,0.037383177570093455,"Graph Neural Networks (GNNs) [13, 36] have been widely used to learn expressive representation
20"
INTRODUCTION,0.03925233644859813,"from ubiquitous graph-structured data such as social networks [31], chemical molecules [15] and
21"
INTRODUCTION,0.041121495327102804,"biological networks [24]. To improve representation ability, multiple GNN variants, e.g., graph
22"
INTRODUCTION,0.04299065420560748,"convolutional networks (GCNs) [16] and graph attention networks (GATs) [25], have been developed
23"
INTRODUCTION,0.044859813084112146,"to facilitate various graph-related tasks including node classification[16], link prediction[19, 35], and
24"
INTRODUCTION,0.04672897196261682,"graph classification[36]. Specifically, for graph-related learning tasks, graph pooling has become
25"
INTRODUCTION,0.048598130841121495,"an essential component in various GNN architectures. Aiming to learn compact representation for
26"
INTRODUCTION,0.05046728971962617,"graphs, graph pooling facilitates graph topology modeling by enlarging receptive fields as well as
27"
INTRODUCTION,0.052336448598130844,"scaling down the graph size which effectively reduces computational costs.
28"
INTRODUCTION,0.05420560747663551,"The existing graph pooling techniques generally fall into two main categories, i.e., the global graph
29"
INTRODUCTION,0.056074766355140186,"pooling [5, 27, 14, 33, 26, 36] and hierarchical graph pooling. The former directly compresses a set
30"
INTRODUCTION,0.05794392523364486,"of nodes into a compact graph-level representation. This operation results in a flat feature as a whole
31"
INTRODUCTION,0.059813084112149535,"graph embedding. In contrast, the hierarchical pooling coarsens graphs gradually and outputs the
32"
INTRODUCTION,0.0616822429906542,"corresponding pooled graphs of smaller sizes. For this purpose, two different types of coarsening,
33"
INTRODUCTION,0.06355140186915888,"named node dropping [17, 10, 20, 37, 18, 11] and node clustering [33, 1, 34], are often employed.
34"
INTRODUCTION,0.06542056074766354,"The node dropping picks up a subset of nodes to construct the coarsened graph, while the node
35"
INTRODUCTION,0.06728971962616823,"clustering learns an assignment matrix to aggregate those nodes in the original graph into new clusters.
36"
INTRODUCTION,0.0691588785046729,"In this work, our proposed BernPool falls in the category of the latter.
37"
INTRODUCTION,0.07102803738317758,"Even though considerable progress has been made, most pooling methods select a part of nodes or
38"
INTRODUCTION,0.07289719626168224,"cluster nodes in a deterministic manner according to the importance scores of nodes, which degrades
39"
INTRODUCTION,0.07476635514018691,"the sampling diversity. For this issue, some previous works [21, 7] propose stochastic node dropping
40"
INTRODUCTION,0.07663551401869159,"independent from the data, i.e., randomly dropping. However, they do not consider the intrinsic
41"
INTRODUCTION,0.07850467289719626,"structural characteristics of data while enriching the sampling diversity. Thus the current bottleneck
42"
INTRODUCTION,0.08037383177570094,"is how to adaptively extract expressive substructures and meanwhile keeping rich sampling diversity
43"
INTRODUCTION,0.08224299065420561,"in graph pooling, with the precondition of high-efficient and effective graph representation learning.
44"
INTRODUCTION,0.08411214953271028,"To address the above problem, in this work, we propose a graph Bernoulli pooling method called
45"
INTRODUCTION,0.08598130841121496,"BernPool to facilitate graph representation learning. Different from the existing graph pooling
46"
INTRODUCTION,0.08785046728971962,"methods, we design a probabilistic Bernoulli sampling by estimating the sampling probabilities of
47"
INTRODUCTION,0.08971962616822429,"graph nodes. To restrict the sampling process, we formulate a variational Bernoulli learning constraint
48"
INTRODUCTION,0.09158878504672897,"by deriving an upper bound between an expected distribution and a learned distribution. To better
49"
INTRODUCTION,0.09345794392523364,"capture expressive info, a learnable reference set is further introduced to encode nodes into a latent
50"
INTRODUCTION,0.09532710280373832,"expressive probability space. Thus the advantage of the resultant Bernoulli sampling is two-fold: i)
51"
INTRODUCTION,0.09719626168224299,"capture representative substructures of graph; and ii) preserve certain diversity (like random dropping)
52"
INTRODUCTION,0.09906542056074766,"due to its non-deterministic manner. Considering the complementary characteristics between node
53"
INTRODUCTION,0.10093457943925234,"dropping and node clustering, we propose a hybrid graph pooling paradigm to fuse a compact
54"
INTRODUCTION,0.102803738317757,"subgraph after node dropping and a coarsening graph after node clustering. The node clustering
55"
INTRODUCTION,0.10467289719626169,"in our framework is also Bernoulli-induced without high-computation cost because it adopts the
56"
INTRODUCTION,0.10654205607476636,"sampled nodes as clustering centers. The hybrid graph pooling can jointly learn representative
57"
INTRODUCTION,0.10841121495327102,"substructures and preserve the input graph topology. We conduct extensive experiments on 8 public
58"
INTRODUCTION,0.1102803738317757,"graph classification datasets to test our BernPool, and the experimental results validate that our
59"
INTRODUCTION,0.11214953271028037,"BernPool achieves better performance than those existing pooling methods and keep high efficiency
60"
INTRODUCTION,0.11401869158878504,"on par with those node-dropping methods.
61"
INTRODUCTION,0.11588785046728972,"The contributions of this work are summarized as: i) propose a probabilistic Bernoulli sampling
62"
INTRODUCTION,0.11775700934579439,"method to not only learn effective sampling but also preserve high efficiency; ii) propose a hybrid
63"
INTRODUCTION,0.11962616822429907,"graph pooling way to retain both those sampled substructures and the remaining info; iii) verify the
64"
INTRODUCTION,0.12149532710280374,"effectiveness and high-efficiency of our BernPool, and report the state-of-the-art performance.
65"
RELATED WORK,0.1233644859813084,"2
Related Work
66"
RELATED WORK,0.1252336448598131,"In this section, we first review the previous methods of Graph Neural Networks (GNNs), then
67"
RELATED WORK,0.12710280373831775,"introduce the related Hierarchical pooling methods.
68"
RELATED WORK,0.12897196261682242,"Graph Neural Network. GNNs were introduced as a form of recurrent neural network by Gori
69"
RELATED WORK,0.1308411214953271,"et.al. [12] and Scarselli et al. [23]. Subsequently, Duvenaud et al.[6] introduced a convolution-
70"
RELATED WORK,0.13271028037383178,"like propagation rule on graphs to extract node representations for graph-level classification. To
71"
RELATED WORK,0.13457943925233645,"enhance the graph representation ability, several convolution operations were proposed (e.g. Graph
72"
RELATED WORK,0.13644859813084112,"Convolution Network(GCN[16]), Graph Attention Network(GAT[25]), GraphSAGE[13], GIN[30])
73"
RELATED WORK,0.1383177570093458,"to extract expressive node representations by aggregating neighbor node features and have achieved
74"
RELATED WORK,0.14018691588785046,"promising performance in various graph-related tasks in recent years. In particular, GCN[16] utilizes
75"
RELATED WORK,0.14205607476635515,"a first-order approximation of spectral convolution via Chebyshev polynomial iteration to improve
76"
RELATED WORK,0.14392523364485982,"efficiency. However, it suffers from the issue of fixed and equal weighting for neighbor nodes during
77"
RELATED WORK,0.14579439252336449,"aggregation, which may not be optimal for all nodes and can lead to information loss. To address this
78"
RELATED WORK,0.14766355140186915,"problem, GAT[25] introduced an attention mechanism to assign different weights to neighbor nodes
79"
RELATED WORK,0.14953271028037382,"during message passing. Furthermore, GrapSAGE[13] learned node embeddings by aggregating
80"
RELATED WORK,0.15140186915887852,"feature information from the neighborhood in an inductive manner. Despite the considerable progress
81"
RELATED WORK,0.15327102803738318,"made by GNNs, they are limited in their ability to generate hierarchical graph representations due to
82"
RELATED WORK,0.15514018691588785,"the lack of pooling operations.
83"
RELATED WORK,0.15700934579439252,"Hierarchical Graph Pooling. Graph pooling is a critical operation to obtain robust representations
84"
RELATED WORK,0.1588785046728972,"and scale down the size of graphs, which can be classified into two categories: global pooling
85"
RELATED WORK,0.16074766355140188,"and hierarchical pooling. The former [5, 27, 14, 33, 26, 36] aggregates node-level features to
86"
RELATED WORK,0.16261682242990655,"generate a graph-level representation. For instance, SortPool[36] ranks and groups the nodes into
87"
RELATED WORK,0.16448598130841122,"clusters according to their features, then aggregates the resulting clusters to generate the graph-
88"
RELATED WORK,0.16635514018691588,"level representation. However, the global pooling suffers from the issue of discarding structure
89"
RELATED WORK,0.16822429906542055,"information in generating graph-level representation. On the other hand, the hierarchical pooling
90"
RELATED WORK,0.17009345794392525,"methods could progressively compress the graph into a smaller one and capture the hierarchical
91"
RELATED WORK,0.17196261682242991,"structure. They can be further divided into node clustering pooling methods [33, 1, 34], node drop
92"
RELATED WORK,0.17383177570093458,"pooling methods [17, 10, 20, 37, 18, 11] and other pooling methods[28, 3]. Among them, the
93"
RELATED WORK,0.17570093457943925,"node drop pooling methods deleted the unimportant nodes based on certain criteria. For instance,
94"
RELATED WORK,0.17757009345794392,"SAGPool [17] computed the node attention scores using graph convolution to preserve the most
95"
RELATED WORK,0.17943925233644858,"important nodes. But the node drop pooling methods may not preserve the original structure well
96"
RELATED WORK,0.18130841121495328,"during the graph compression process. To alleviate this problem, Pang et.al [20] applied contrastive
97"
RELATED WORK,0.18317757009345795,"learning to maximize the mutual information between the input graph and the pooled graphs to
98"
RELATED WORK,0.18504672897196262,"preserve the graph-level dependencies in the pooling layers. Gao et.al [11] proposed a criterion to
99"
RELATED WORK,0.18691588785046728,"assess each node’s information among its neighbors to retain informative features. Our BernPool
100"
RELATED WORK,0.18878504672897195,"introduces probabilistic deduced Bernoulli sampling based on reference set to progressively compress
101"
RELATED WORK,0.19065420560747665,"the original graph by preserving important nodes, rather than selecting nodes in a deterministic
102"
RELATED WORK,0.1925233644859813,"way. This probabilistic manner can lead to more diverse sampling situations and capture the data of
103"
RELATED WORK,0.19439252336448598,"intrinsic characteristics, promoting graph discriminative representation learning. Furthermore, we
104"
RELATED WORK,0.19626168224299065,"propose a hybrid graph pooling module to alleviate the node drop pooling method’s issue of cannot
105"
RELATED WORK,0.19813084112149532,"preserve structure well.
106"
PRELIMINARIES,0.2,"3
Preliminaries
107"
PRELIMINARIES,0.20186915887850468,"Notations For an arbitrary graph G = (V, E, X) with n = |V| nodes and |E| edges. X ∈Rn×d′
108"
PRELIMINARIES,0.20373831775700935,"represents the node feature matrix, where d′ is the dimension of node attributes, and A ∈Rn×n
109"
PRELIMINARIES,0.205607476635514,"denotes the adjacency matrix describing its edge connection information. The graph G has a one-hot
110"
PRELIMINARIES,0.20747663551401868,"vector yi w.r.t its label. A pooled graph of the original graph G is denoted by eG = (eV, eE, eX) with
111"
PRELIMINARIES,0.20934579439252338,"adjacency matrix as eA ∈Ren×en, where en denotes the number of pooled nodes.
112"
PRELIMINARIES,0.21121495327102804,"Graph Convolution In this work, we employ the classic graph convolution network (GCN) as the
113"
PRELIMINARIES,0.2130841121495327,"backbone to extract features, where the l-th convolutional layer is formulated as:
114"
PRELIMINARIES,0.21495327102803738,H(l+1) = σ( ˆD−1
PRELIMINARIES,0.21682242990654205,"2 ˆA ˆD
1
2 H(l)W(l)),
(1)"
PRELIMINARIES,0.21869158878504674,"where σ(·) is a non-linear activation function, H(l) is the hidden-layer feature, ˆA is the added
115"
PRELIMINARIES,0.2205607476635514,"self-loop adjacent matrix, ˆD denotes the degree matrix of ˆA, and W(l) represents a learnable weight
116"
PRELIMINARIES,0.22242990654205608,"matrix at the l-th layer. The initial node features are used at the first convolution, i.e., H(0) = X.
117"
THE PROPOSED BERNPOOL,0.22429906542056074,"4
The Proposed BernPool
118"
OVERVIEW,0.2261682242990654,"4.1
Overview
119 Graph Conv Graph Conv Graph"
OVERVIEW,0.22803738317757008,"Conv
BernPool
BernPool
BernPool
Readout"
OVERVIEW,0.22990654205607478,"Readout
Readout MLP"
OVERVIEW,0.23177570093457944,Reference Set
OVERVIEW,0.2336448598130841,Reparametrization
OVERVIEW,0.23551401869158878,Bernoulli Sampling Factor learning
OVERVIEW,0.23738317757009345,Predicted Label
OVERVIEW,0.23925233644859814,Importance score
OVERVIEW,0.2411214953271028,Pooled graph
OVERVIEW,0.24299065420560748,"Hybrid Graph Pooling
dropping"
OVERVIEW,0.24485981308411214,clustering
OVERVIEW,0.2467289719626168,Figure 1: The architecture of the proposed BernPool framework. Please see Overview in Section 4.1.
OVERVIEW,0.2485981308411215,"The whole framework is illustrated in Fig. 1, where the convolution and our proposed BernPool are
120"
OVERVIEW,0.2504672897196262,"stacked alternately. The proposed BernPool could be seamlessly engaged with any type of graph
121"
OVERVIEW,0.2523364485981308,"convolution to facilitate graph representation. In BernPool, there contains two main modules: graph
122"
OVERVIEW,0.2542056074766355,"Bernoulli sampling (GBS) and hybrid graph pooling (HGP). To boost graph Bernoulli sampling,
123"
OVERVIEW,0.2560747663551402,"we specifically design a reference set S to encode the importance of each graph node, and the
124"
OVERVIEW,0.25794392523364484,"reference set is configured as the optimizable parameters. The reference set is further conditioned on
125"
OVERVIEW,0.25981308411214954,"the orthogonal space so as to reduce redundancy. After transforming via the reference set, we can
126"
OVERVIEW,0.2616822429906542,"estimate the sampling probabilities p of graph nodes. In particular, the probabilities of nodes are
127"
OVERVIEW,0.2635514018691589,"totally restricted with an expected/predefined distribution, which is formulated to maximize the upper
128"
OVERVIEW,0.26542056074766357,"bound of KL-divergence (please see Section 4.2). Just due to the restriction, we can probabilistically
129"
OVERVIEW,0.2672897196261682,"sample a specified proportion of graph nodes. The detail of GBS can be found in Section 4.3. In the
130"
OVERVIEW,0.2691588785046729,"stage of hybrid graph pooling, on the one hand, we prune those unsampled nodes and the associated
131"
OVERVIEW,0.27102803738317754,"edges to generate a compact subgraph; on the other hand, to preserve graph topological structure,
132"
OVERVIEW,0.27289719626168224,"we perform neighbor nodes clustering to form a coarsening graph. Both the compact subgraph
133"
OVERVIEW,0.27476635514018694,"and coarsening graph are fused to form the final pooled graph. The detail of HGP can be found in
134"
OVERVIEW,0.2766355140186916,"Section 4.4. The BernPool attempts to learn the reference set and a few linear transformations. The
135"
OVERVIEW,0.27850467289719627,"whole framework can be optimized in an end-to-end mode through back-propagation.
136"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.2803738317757009,"4.2
Bernoulli Sampling Optimization Objective
137"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.2822429906542056,"To sample a certain proportion of graph nodes in a probabilistic manner, we derive a KL-divergence
138"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.2841121495327103,"constraint, which makes node probabilities tend to be a predefined distribution. To this end, we again
139"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.28598130841121494,"dissect mutual information between the learned subgraph embeddings and their corresponding labels.
140"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.28785046728971964,"Formally, we aim to maximize the mutual information function:
141"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.2897196261682243,"ζMI = MI(y, fψ,ϕ(G, S)),
(2)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.29158878504672897,"where ϕ denotes the parameters of the BernPool, ψ is the parameters of other modules (e.g., convolu-
142"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.29345794392523367,"tion, classifier), and fψ,ϕ(·) represents the graph embedding process.
143"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.2953271028037383,"Suppose the sampling factor z in graph pooling, we resort to the relationship between mutual
144"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.297196261682243,"information and expectation, and rewrite Eqn. (2) as:
145"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.29906542056074764,"Ey|G,S[log
Z
pψ(y|G, S, z)pϕ(z|G, S)dz]"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.30093457943925234,"= Ey|G,S[log
Z
qϕ(z|G, S)pψ(y|G, S, z)pϕ(z|G, S)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.30280373831775703,"qϕ(z|G, S)dz],
(3)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.30467289719626167,"where pψ(y|G, S, z) is the conditional probability of label y, and pϕ(z|G, S) denotes the conditional
146"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.30654205607476637,"probability of the factor z that is usually intractable. After a series of derivation from Eqn. (3), we
147"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.308411214953271,"can deduce a bound with KL-divergence between expected Bernoulli distribution qϕ and learned
148"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3102803738317757,"distribution pϕ:
149"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3121495327102804,"Ey|G,S[log
Z
pψ(y|G, S, z)pϕ(z|G, S)dz]"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.31401869158878504,"≥Ey|G,S[log pψ(y|G, S, z)] −DKL(qϕ(z|G, S)||pϕ(z|G, S))
(4)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.31588785046728973,"= −ζCE −DKL(qϕ(z|G, S)||pϕ(z|G, S)),
(5)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3177570093457944,"where pψ(y|G, S) represents the predicted probability of label based on the input graph and reference
150"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.31962616822429907,"set, ζCE is the cross entropy loss function. Please see the detailed derivation in the supplementary
151"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.32149532710280376,"file. As the expected Bernoulli distribution qϕ(z|G, S) is independent from the input graph and
152"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3233644859813084,"reference set, qϕ(z|G, S) can be denoted as q(z). After adding the soft-orthogonal constraint on the
153"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3252336448598131,"reference set, therefore, the final optimization objective can be converted to minimize:
154"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.32710280373831774,"ζ = ζCE + DKL(q(z)||pϕ(z|G, S)) + β||SS⊺−cI||F ,
(6)"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.32897196261682243,"where the second term forces the sampling factor to follow an expected distribution q, the matrix S
155"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.33084112149532713,"stacks the vectors of reference set S in the third term, I denotes the identity matrix, β is a trade-off
156"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.33271028037383177,"hyper-parameter, and c is a learnable scalar. Thus, the learning of the sampling factor could be
157"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.33457943925233646,"integrated into the objective function as a joint training process. In addition, we can easily extend the
158"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3364485981308411,"above single-layer BernPool into multi-layer networks by deploying independent sampling factors in
159"
BERNOULLI SAMPLING OPTIMIZATION OBJECTIVE,0.3383177570093458,"sequential graph pooling.
160"
BERNOULLI SAMPLING FACTOR LEARNING,0.3401869158878505,"4.3
Bernoulli Sampling Factor Learning
161"
BERNOULLI SAMPLING FACTOR LEARNING,0.34205607476635513,"To extract an expressive sub-graph eG from the original or former-layer graph G, we estimate a
162"
BERNOULLI SAMPLING FACTOR LEARNING,0.34392523364485983,"probabilistic factor z = (z1, · · · , zn)⊺∈{0, 1}n that conforms to Bernoulli distribution, instead
163"
BERNOULLI SAMPLING FACTOR LEARNING,0.34579439252336447,"of a deterministic way such as top-k. In contrast to the deterministic way, our BernPool possesses
164"
BERNOULLI SAMPLING FACTOR LEARNING,0.34766355140186916,"a more diverse sampling in mining substructures and graph topological variation of input data.
165"
BERNOULLI SAMPLING FACTOR LEARNING,0.34953271028037386,"However, as mentioned above in Eqn. (3), it’s rather non-trivial to infer z through Bayes rule:
166"
BERNOULLI SAMPLING FACTOR LEARNING,0.3514018691588785,"p(z|G, S) = p(z)p(G, S|z)/p(G, S). A reason is that the prior p(G, S|z) is intractable. We resort
167"
BERNOULLI SAMPLING FACTOR LEARNING,0.3532710280373832,"to the variational inference to approximate the intractable true posterior p(z|G, S) with q(z) by
168"
BERNOULLI SAMPLING FACTOR LEARNING,0.35514018691588783,"constraining the KL-divergence DKL(qϕ(z)||pϕ(z|G, S)).
169"
BERNOULLI SAMPLING FACTOR LEARNING,0.35700934579439253,"To make the gradient computable, we employ the reparameterization trick to derive p(z|G, S) = B(p),
170"
BERNOULLI SAMPLING FACTOR LEARNING,0.35887850467289717,"where B(·) denotes the Bernoulli distribution based on the probability vector. Concretely, we take
171"
BERNOULLI SAMPLING FACTOR LEARNING,0.36074766355140186,"the graph convolution feature H and the matrix S of reference set as input, to learn the sampling
172"
BERNOULLI SAMPLING FACTOR LEARNING,0.36261682242990656,"probability p. A simple formulation with a fully-connected operation is given as follows:
173"
BERNOULLI SAMPLING FACTOR LEARNING,0.3644859813084112,"p = sigmoid(MLP(cos(H, S))),
(7)"
BERNOULLI SAMPLING FACTOR LEARNING,0.3663551401869159,"where cos(H, S) computes the pairwise similarities (across all graph nodes and all reference points)
174"
BERNOULLI SAMPLING FACTOR LEARNING,0.36822429906542054,"through the cosine measurement, and sigmoid(·) is the sigmoid function.
175"
BERNOULLI HYBRID GRAPH POOLING,0.37009345794392523,"4.4
Bernoulli Hybrid Graph Pooling
176"
BERNOULLI HYBRID GRAPH POOLING,0.3719626168224299,"Based on the above inferred sampling factor z, we propose a hybrid graph pooling to learn expressive
177"
BERNOULLI HYBRID GRAPH POOLING,0.37383177570093457,"substructures while endeavoring to reserve graph information. The hybrid graph pooling contains
178"
BERNOULLI HYBRID GRAPH POOLING,0.37570093457943926,"two components: Bernoulli node dropping and Bernoulli node clustering. The former prunes those
179"
BERNOULLI HYBRID GRAPH POOLING,0.3775700934579439,"unsampled nodes and associated edges to generate a compact graph; the latter clusters neighbor nodes
180"
BERNOULLI HYBRID GRAPH POOLING,0.3794392523364486,"to form a coarsening graph.
181"
BERNOULLI HYBRID GRAPH POOLING,0.3813084112149533,"Bernoulli Node Dropping. Let idx ∈Ren denotes the indices of the preserved nodes according to the
182"
BERNOULLI HYBRID GRAPH POOLING,0.38317757009345793,"sampling factor z, thus a projection matrix P ∈{0, 1}en×n can be defined formally:
183"
BERNOULLI HYBRID GRAPH POOLING,0.3850467289719626,"P = diag(z)[idx, :],
(8)"
BERNOULLI HYBRID GRAPH POOLING,0.38691588785046727,"where diag(·) is the vector diagonalization operation, and X[idx, :] extracts those rows of X w.r.t the
184"
BERNOULLI HYBRID GRAPH POOLING,0.38878504672897196,"indices idx. Accordingly, the compact subgraph eG = ( eH, eA) can then be computed by:
185"
BERNOULLI HYBRID GRAPH POOLING,0.39065420560747666,"eH = PH,
eA = PAP⊺,
(9)"
BERNOULLI HYBRID GRAPH POOLING,0.3925233644859813,"where eH ∈Ren×d is the feature matrix of the compact graph, and eA ∈Ren×en is the subgraph
186"
BERNOULLI HYBRID GRAPH POOLING,0.394392523364486,"adjacency matrix. Intuitively, the dropping directly removes those unselected nodes and connective
187"
BERNOULLI HYBRID GRAPH POOLING,0.39626168224299063,"edges from the input graph.
188"
BERNOULLI HYBRID GRAPH POOLING,0.3981308411214953,"Bernoulli Node Clustering. After Bernoulli sampling, we can obtain those reserved nodes, which
189"
BERNOULLI HYBRID GRAPH POOLING,0.4,"could be used as the clustering centers. Hereby, we only need to transmit those unselected nodes’
190"
BERNOULLI HYBRID GRAPH POOLING,0.40186915887850466,"messages to cluster centers, which would preserve more information of the whole input graph.
191"
BERNOULLI HYBRID GRAPH POOLING,0.40373831775700936,"Compared with previous graph clustering methods, Bernoulli clustering is more efficient because the
192"
BERNOULLI HYBRID GRAPH POOLING,0.405607476635514,"learning of the assignment matrix is bypassed, while increasing the diversity of graph perception. To
193"
BERNOULLI HYBRID GRAPH POOLING,0.4074766355140187,"be specific, our assignment matrix P′ ∈Ren×n is from the original adjacency matrix A, and we can
194"
BERNOULLI HYBRID GRAPH POOLING,0.4093457943925234,"get the coarsening graph as:
195"
BERNOULLI HYBRID GRAPH POOLING,0.411214953271028,"P′ = (diag(z)[idx, :]) × A,
eH′ = P′H,
(10)"
BERNOULLI HYBRID GRAPH POOLING,0.4130841121495327,"where eH′ ∈Ren×d denotes the diffusion features based on input graph, the coarsening graph has the
196"
BERNOULLI HYBRID GRAPH POOLING,0.41495327102803736,"same adjacency matrix eAi as in Eqn. (9).
197"
BERNOULLI HYBRID GRAPH POOLING,0.41682242990654206,"Hybrid Graph. Based on the aforementioned operations, we can obtain the compact subgraph
198"
BERNOULLI HYBRID GRAPH POOLING,0.41869158878504675,"eG = ( eH, eA) and the coarsening graph eG′ = ( eH′, eA), which reflect different aspects of information in
199"
BERNOULLI HYBRID GRAPH POOLING,0.4205607476635514,"the original graph. To fully exploit the extracted informative node representations and expressive sub-
200"
BERNOULLI HYBRID GRAPH POOLING,0.4224299065420561,"structures, the two subgraphs are fused to form the final pooled graph by the following formulation:
201"
BERNOULLI HYBRID GRAPH POOLING,0.42429906542056073,"bH = σ(( eH + eH′)Wh),
(11)"
BERNOULLI HYBRID GRAPH POOLING,0.4261682242990654,"where σ denotes a non-linear activation function and Wh ∈Rd×d is a learnable weight, and
202"
BERNOULLI HYBRID GRAPH POOLING,0.4280373831775701,"bH ∈Ren×d is the aggregated node embeddings of the pooled graph.
203"
READOUT FUNCTION,0.42990654205607476,"4.5
Readout Function
204"
READOUT FUNCTION,0.43177570093457945,"The proposed framework repeats the graph convolution and BernPool operations three times. To
205"
READOUT FUNCTION,0.4336448598130841,"obtain a fixed-size graph-level representation, we apply the concatenation of max-pooling and mean-
206"
READOUT FUNCTION,0.4355140186915888,"pooling in each subgraph following the previous works [37, 29, 22]. Finally, those graph-level
207"
READOUT FUNCTION,0.4373831775700935,"representations can be summarized to form the final embeddings:
208 r =
X"
READOUT FUNCTION,0.4392523364485981,"l=1,···
r(l),
and
r(l) = σ( 1"
READOUT FUNCTION,0.4411214953271028,"n(l)
i"
READOUT FUNCTION,0.44299065420560746,"n(l)
i
X"
READOUT FUNCTION,0.44485981308411215,"i=1
bH(l)
i ||
n(l)
i
max
i=1
bH(l)
i ),
(12)"
READOUT FUNCTION,0.44672897196261685,"where bH(l)
i
denotes the i-th node feature at the l-th pooling, σ is the same non-linear activation
209"
READOUT FUNCTION,0.4485981308411215,"function, and || denotes the feature concatenation operation. The resulting embeddings would finally
210"
READOUT FUNCTION,0.4504672897196262,"be fed into a multi-layer perceptron to predict graph labels.
211"
COMPUTATIONAL COMPLEXITY,0.4523364485981308,"4.6
Computational Complexity
212"
COMPUTATIONAL COMPLEXITY,0.4542056074766355,"The computational complexity of one-layer BernPool can be expressed as O(N ×K ×d+N ′ ×N ×
213"
COMPUTATIONAL COMPLEXITY,0.45607476635514016,"d + N ′ × d × d), where N denotes the number of nodes, K is the number of reference points, d is
214"
COMPUTATIONAL COMPLEXITY,0.45794392523364486,"the dimensionality of nodes features, N ′ represents the number of preserved nodes. Specifically, the
215"
COMPUTATIONAL COMPLEXITY,0.45981308411214955,"complexity of probability score computation is O(N × K × d), and the complexity of the Bernoulli
216"
COMPUTATIONAL COMPLEXITY,0.4616822429906542,"hybrid graph pooling module is O(N ′ × N × d + N ′ × d × d).
217"
EXPERIMENTS,0.4635514018691589,"5
Experiments
218"
EXPERIMENTAL SETUP,0.4654205607476635,"5.1
Experimental Setup
219"
EXPERIMENTAL SETUP,0.4672897196261682,"Datasets. To comprehensively evaluate our proposed model, we conduct extensive experiments
220"
EXPERIMENTAL SETUP,0.4691588785046729,"on eight widely used datasets in the graph classification task, including three social network
221"
EXPERIMENTAL SETUP,0.47102803738317756,"datasets (IMDB-BINARY, IMDB-MULTI [31] and COLLAB[32]) and five Bioinformatics datasets
222"
EXPERIMENTAL SETUP,0.47289719626168225,"(PROTEINS[8], DD[4], NCI1[24], Mutagenicity[15] and ENZYMES[2]). The detailed information
223"
EXPERIMENTAL SETUP,0.4747663551401869,"and statistics of these datasets are summarized in Table. 1.
224"
EXPERIMENTAL SETUP,0.4766355140186916,"Baselines. We compare our proposed method with several state-of-the-art graph pooling methods,
225"
EXPERIMENTAL SETUP,0.4785046728971963,"including three backbones (GCN, GAT, GraphSAGE), eight node drop graph pooling methods
226"
EXPERIMENTAL SETUP,0.4803738317757009,"(TopkPool[10], SAGPool[17], ASAP[22], VIPool[18], iPool[11], CGIPool[20], SEP-G [29] and
227"
EXPERIMENTAL SETUP,0.4822429906542056,"MVPool [37]), three clustering pooling methods (Diffpool[33], MincutPool[1], and StructPool[34]),
228"
EXPERIMENTAL SETUP,0.48411214953271026,"three global pooling methods (Set2Set[26], SortPool[36], DropGIN[21]) and one other pooling
229"
EXPERIMENTAL SETUP,0.48598130841121495,"method (EdgePool[3]).
230"
EXPERIMENTAL SETUP,0.48785046728971965,"Implementation Details. We employ the 10-fold cross-validation protocol following the settings of
231"
EXPERIMENTAL SETUP,0.4897196261682243,"[29, 22] and report the average classification accuracies and standard deviation. For all used datasets,
232"
EXPERIMENTAL SETUP,0.491588785046729,"we set the expected pooling ratio as 0.8, the node embedding dimension d as 128, the number of
233"
EXPERIMENTAL SETUP,0.4934579439252336,"reference points as 32, and the hyper-parameter β in Eqn. 6 as 5. We adopt the Adam optimizer to
234"
EXPERIMENTAL SETUP,0.4953271028037383,"train our model with 1000 epochs, where the learning rate and weight decay are set as 1e-3 and 1e-4,
235"
EXPERIMENTAL SETUP,0.497196261682243,"respectively. Our proposed BernPool is implemented with PyTorch and Pytorch Geometric [9].
236"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.49906542056074765,"5.2
Comparison with the state-of-the-art Methods
237"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5009345794392523,"The graph classification results of BernPool and other state-of-the-art methods are presented in Table 1.
238"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.502803738317757,"In general, most hierarchical pooling approaches including our proposed BernPool can perform better
239"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5046728971962616,"than those global pooling ones in the graph classification task. This may be because global pooling
240"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5065420560747663,"methods ignore the hierarchical graph structures in generating graph-level representation. In particular,
241"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.508411214953271,"our BernPool achieves state-of-the-art performance on all datasets, which demonstrates the robustness
242"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5102803738317757,"of our framework against graph structure data variation. In contrast, previous methods cannot perform
243"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5121495327102804,"well on all eight datasets, while the second highest performances on different datasets are obtained
244"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.514018691588785,"by five different methods. Compared with those methods, Our BernPool outperforms respectively
245"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5158878504672897,"by 1.09%, 3.16%, 13.6%, 2.7%, and 1.86% on the PROTEINS, Mutagenicity, ENZYMES, IMDB-
246"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5177570093457944,"BINARY and COLLAB datasets.
247"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5196261682242991,"Table 1: The statistics of eight datasets and graph classification accuracies comparison of
different methods. We have highlighted the best results in black and marked the second-highest
accuracies with the symbol †."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5214953271028038,"Bioinformatics
Social Network"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5233644859813084,"PROTEINS
DD
NCI1
Mutagenicity
ENZYMES
IMDB-B
IMDB-M
COLLAB"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.525233644859813,"#Graphs(Classes)
1113 (2)
1178 (2)
4110 (2)
4337 (2)
600 (6)
1000 (2)
1500 (3)
5000 (3)"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5271028037383177,"Avg # Nodes
39.1
284.3
29.8
30.3
32.6
19.8
13.0
74.5"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5289719626168224,"Avg # Edges
72.8
715.7
32.3
30.8
62.1
96.5
65.9
2457.8"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5308411214953271,"GCN[16]
74.84±2.82
78.12±4.33
76.3±1.8
79.8±1.6
50.00±5.87
72.67±6.42
50.40±3.02
71.92±3.24"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5327102803738317,"GAT[25]
74.07±4.53
75.56±3.72
74.9±1.7
78.8±1.2
51.00±5.23
74.07±4.53
49.67±4.30
75.80±1.60"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5345794392523364,"GraphSAGE[13]
73.75±2.97
77.27±4.06
74.7±1.3
78.9±2.1
53.33±3.42
72.17±5.29
48.53±5.43
79.70±1.70"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5364485981308411,"Set2Set[26]
73.27±0.85
71.94±0.56
68.55±1.92
71.35±2.1
-
72.90±0.75
50.19±0.39
79.55±0.39"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5383177570093458,"SortPool[36]
73.27±0.85
75.58±0.72
73.82±1.96
70.66±1.51
49.67±4.27
72.12±1.12
48.18±0.83
77.87±0.47"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5401869158878505,"DropGIN[21]
76.3±6.1
-
-
-
-
75.7±4.2
51.4±2.8
-"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5420560747663551,"EdgePool[3]
72.50±3.2
75.85±0.58
-
-
-
72.46±0.74
50.79±0.59
67.10±2.7"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5439252336448598,"DiffPool[33]
73.03±1.00
77.56±0.41
62.32±1.90
77.60±2.70
61.83±5.3
73.14±0.70
51.31±0.72
78.68±0.43"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5457943925233645,"StructPool[34]
80.36
84.19
-
-
63.83
74.70
52.47
74.22"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5476635514018692,"MinCutPool[1]
76.5±2.6
80.8±2.3
74.25±0.86
79.9±2.1
-
72.65±0.75
51.04±0.70
83.4±1.7†"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5495327102803739,"TopKPool[10]
70.48±1.01
73.63±0.55
67.02±2.25
79.14±0.76
50.33±6.3
71.58±0.95
48.59±0.72
77.58±0.85"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5514018691588785,"SAGPool[17]
71.86±0.97
76.45±0.97
67.45±1.11
72.40±2.40
52.67±5.8
72.55±1.28
50.23±0.44
78.03±0.31"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5532710280373832,"ASAP[22]
74.19±0.79
76.87±0.70
71.48±0.42
80.12±0.88
-
72.81±0.50
50.78±0.75
78.64±0.50"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5551401869158878,"VIPool[18]
79.91±4.1
82.68±4.1†
-
80.19±1.02
57.50±6.1
78.60±2.3†
55.20±2.5†
78.82±1.4"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5570093457943925,"iPool[11]
76.46±3.22
78.76±3.45
80.46±1.66†
-
56.00±7.72
72.90±3.08
50.73±3.68
76.86±1.67"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5588785046728972,"SEP-G[29]
76.42±0.39
77.98±0.57
78.35±0.33
-
-
74.12±0.56
51.53±0.65
81.28±0.15"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5607476635514018,"CGIPool[20]
74.10±2.31
-
78.62±1.04
80.65±0.79†
-
72.40±0.87
51.45±0.65
80.30±0.69"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5626168224299065,"MVPool[37]
82.2±1.2†
78.4±1.5
77.5±1.3
80.2±0.8
62.4±2.5†
-
48.6±1.0
-"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5644859813084112,"BernPool(Ours)
83.29 ± 3.69
83.27 ± 2.95
81.44 ± 1.09
83.81 ± 1.43
76.00 ± 3.78
81.30 ± 3.50
55.93 ± 3.8
85.26 ± 1.35"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5663551401869159,"Moreover, the proposed BernPool significantly surpasses GNNs (GCN, GAT, GraphSAGE) without
248"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5682242990654206,"adding the pooling operation, which verifies our BernPool can effectively learn representative
249"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5700934579439252,"substructures and preserve graph topological information. SEP-G [29] gains better performance
250"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5719626168224299,"compared with other node drop pooling methods, which utilizes the structural entropy to assess the
251"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5738317757009346,"importance of each graph node. However, our BernPool exhibits an average 5% relative improvement
252"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5757009345794393,"over SEP-G. This can be attributed to our method of learning sampling factors in a probabilistic
253"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.577570093457944,"manner, which possesses more diversity in mining expressive sub-structures compared with the
254"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5794392523364486,"deterministic way. Compared to DiffPool [33], our BernPool outperforms it on all used datasets,
255"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5813084112149532,"verifying the effectiveness of our proposed hybrid graph pooling module, which jointly leverages the
256"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5831775700934579,"advantages of both node drop and clustering methods. Particularly, DropGIN [21] randomly drops
257"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5850467289719626,"nodes in the training process, which is also a probabilistic manner. Our BernPool outperforms it by
258"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5869158878504673,"6.99%, 5.6%, and 4.53% on the PROTEINS, IMDB-BINARY, and IMDB-MULTI datasets. This is
259"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5887850467289719,"because our BernPool considers the characteristics of data and can adaptively handle the topology
260"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.5906542056074766,"variations.
261"
ABLATION STUDY,0.5925233644859813,"6
Ablation Study
262"
ABLATION STUDY,0.594392523364486,"Effectiveness of the proposed BernPool using GCN variants. To evaluate the performance of
263"
ABLATION STUDY,0.5962616822429907,"our method by employing different convolution layers, we integrate three widely used ones (i.e.,
264"
ABLATION STUDY,0.5981308411214953,"GCN[16], GAT[25] and GraphSAGE[13]) into our BernPool framework. The results on eight datasets
265"
ABLATION STUDY,0.6,"are shown in Table 2. It can be observed that all three variants (BernPool-GCN, BernPool-GAT,
266"
ABLATION STUDY,0.6018691588785047,"BernPool-GraphSAGE) outperform their corresponding backbones, significantly improving the graph
267"
ABLATION STUDY,0.6037383177570094,"classification performances. This observation validates the effectiveness of the proposed BernPool.
268"
ABLATION STUDY,0.6056074766355141,"Effectiveness of Bernoulli hybrid graph pooling. To verify the effectiveness of our proposed
269"
ABLATION STUDY,0.6074766355140186,"Bernoulli hybrid graph pooling, we further conduct experiments on eight datasets. As the hybrid graph
270"
ABLATION STUDY,0.6093457943925233,"pooling consists of node dropping and clustering. We separately remove one of the channels and keep
271"
ABLATION STUDY,0.611214953271028,"the other parts the same. We name the BernPool without dropping and clustering as ""BernPool w/o
272"
ABLATION STUDY,0.6130841121495327,"Dropping"" and ""BernPool w/o Clustering"", respectively. The detailed results are reported in Table 3.
273"
ABLATION STUDY,0.6149532710280374,"Table 2: Graph classification accuracies of BernPool using different backbones. The default
backbone is GCN."
ABLATION STUDY,0.616822429906542,"Variants
Bioinformatics
Social Network"
ABLATION STUDY,0.6186915887850467,"PROTEINS
DD
NCI1
Mutagenicity
ENZYMES
IMDB-B
IMDB-M
COLLAB"
ABLATION STUDY,0.6205607476635514,"#Graphs(Classes)
1113 (2)
1178 (2)
4110 (2)
4337 (2)
600 (6)
1000 (2)
1500 (3)
5000 (3)"
ABLATION STUDY,0.6224299065420561,"GCN
74.84±2.82
78.12±4.33
76.3±1.8
79.8±1.6
50.00±5.87
72.67±6.42
50.40±3.02
71.92±3.24"
ABLATION STUDY,0.6242990654205608,"BernPool-GCN
83.29±3.69 ↑
83.27±2.95 ↑
81.44±1.09 ↑
83.81±1.43 ↑
76.00±3.78 ↑
81.30±3.5 ↑
55.93±3.8 ↑
85.26±1.35 ↑"
ABLATION STUDY,0.6261682242990654,"GAT
74.07±4.53
75.56±3.72
74.9±1.7
78.8±1.2
51.00±5.23
74.07±4.53
49.67±4.30
75.80±1.60"
ABLATION STUDY,0.6280373831775701,"BernPool-GAT
81.49±3.81 ↑
83.44±3.57 ↑
81.29±1.77 ↑
84.34±1.58 ↑
69.50±5.45 ↑
81.20±3.39 ↑
55.00±4.47 ↑
83.86±1.57 ↑"
ABLATION STUDY,0.6299065420560748,"GraphSAGE
73.75±2.97
77.27±4.06
74.7±1.3
78.9±2.1
53.33±3.42
72.17±5.29
48.53±5.43
79.70±1.70"
ABLATION STUDY,0.6317757009345795,"BernPool-GraphSAGE
83.20±4.21 ↑
82.25±3.49 ↑
82.34±1.61 ↑
84.67±1.26 ↑
75.67±3.78 ↑
81.60±3.60 ↑
55.47±3.87 ↑
84.56±1.02 ↑"
ABLATION STUDY,0.6336448598130842,Table 3: Performance Comparison between BernPool and its variants.
ABLATION STUDY,0.6355140186915887,"Variants
Bioinformatics
Social Network"
ABLATION STUDY,0.6373831775700934,"PROTEINS
DD
NCI1
Mutagenicity
ENZYMES
IMDB-B
IMDB-M
COLLAB"
ABLATION STUDY,0.6392523364485981,"#Graphs(Classes)
1113 (2)
1178 (2)
4110 (2)
4337 (2)
600 (6)
1000 (2)
1500 (3)
5000 (3)"
ABLATION STUDY,0.6411214953271028,"BernPool w/o Clustering
81.94±4.27
81.66±2.82
78.22±7.69
82.57±1.44
73.00±3.67
80.60±3.13
55.80±4.11
84.52±1.14"
ABLATION STUDY,0.6429906542056075,"BernPool w/o Dropping
82.40±4.02
82.08±3.98
76.20±10.24
82.75±1.44
72.50±4.25
81.30±3.23
55.60±4.16
85.06±1.18"
ABLATION STUDY,0.6448598130841121,"BernPool
83.29±3.69
83.27 ± 2.95
81.44 ± 1.09
83.81 ± 1.43
76.00 ± 3.78
81.30 ± 3.5
55.93 ± 3.8
85.26 ± 1.35"
ABLATION STUDY,0.6467289719626168,Table 4: Performance comparison between deterministic and probabilistic manner.
ABLATION STUDY,0.6485981308411215,"Variants
Bioinformatics
Social Network"
ABLATION STUDY,0.6504672897196262,"PROTEINS
DD
NCI1
Mutagenicity
ENZYMES
IMDB-B
IMDB-M
COLLAB"
ABLATION STUDY,0.6523364485981309,"#Graphs(Classes)
1113 (2)
1178 (2)
4110 (2)
4337 (2)
600 (6)
1000 (2)
1500 (3)
5000 (3)"
ABLATION STUDY,0.6542056074766355,"BernPool-TopK
81.77±3.53
82.09±3.25
81.09±1.77
83.26±1.08
68.17±3.72
80.20±3.74
55.47±3.51
84.50±1.40"
ABLATION STUDY,0.6560747663551402,"BernPool
83.29±3.69
83.27 ± 2.95
81.44 ± 1.09
83.81 ± 1.43
76.00 ± 3.78
81.30 ± 3.5
55.93 ± 3.8
85.26 ± 1.35"
ABLATION STUDY,0.6579439252336449,"Notably, our BernPool employs just one channel can achieve good results, which demonstrates the
274"
ABLATION STUDY,0.6598130841121496,"effectiveness of BernPool leveraging a probabilistic manner to infer sampling factors. We can observe
275"
ABLATION STUDY,0.6616822429906543,"that the ""BernPool w/o Dropping"" outperforms ""BernPool w/o Clustering"" by 0.46%, 0.42%, 0.70%,
276"
ABLATION STUDY,0.6635514018691588,"and 0.54% on PROTEINS, DD, IMDB-BINARY, and COLLAB datasets, respectively. Furthermore,
277"
ABLATION STUDY,0.6654205607476635,"jointly using both channels can outperform either ""BernPool w/o Dropping"" or ""BernPool w/o
278"
ABLATION STUDY,0.6672897196261682,"Clustering"", which verifies the effectiveness of our proposed hybrid graph pooling.
279"
ABLATION STUDY,0.6691588785046729,"Comparison between the probabilistic and deterministic manner. To make clear the benefit
280"
ABLATION STUDY,0.6710280373831776,"of our proposed probabilistic sampling method, we conduct experiments by replacing Bernoulli
281"
ABLATION STUDY,0.6728971962616822,"sampling with Topk pooling which is in a deterministic manner. In the TopK experiments, we still
282"
ABLATION STUDY,0.6747663551401869,"employ reference set to assess the importance of nodes and the hybrid graph pooling module to jointly
283"
ABLATION STUDY,0.6766355140186916,"learn representative sub-structures and preserve graph topological information. The comparison
284"
ABLATION STUDY,0.6785046728971963,"between ""BernPool-TopK"" and ""BernPool"" is presented in Table 4. It can be observed that BernPool
285"
ABLATION STUDY,0.680373831775701,"outperforms BernPool-TopK on all used eight datasets, especially 7.83% accuracy gains in the
286"
ABLATION STUDY,0.6822429906542056,"ENZYMES dataset, verifying the effectiveness of our designed Bernoulli-deduced sampling strategy.
287"
ABLATION STUDY,0.6841121495327103,"Benefit of the orthogonality for reference set. To evaluate the benefit of orthogonality for the
288"
ABLATION STUDY,0.685981308411215,"reference set, we conduct experiments that remove the orthogonal constraint from BernPool (referred
289"
ABLATION STUDY,0.6878504672897197,"to as ""BernPool w/o orthogonal"" in Fig. 2(a)). The results demonstrate that employing the orthogonal
290"
ABLATION STUDY,0.6897196261682244,"reference set can improve average accuracy by more than 0.6% on three datasets. This verifies the
291"
ABLATION STUDY,0.6915887850467289,"effectiveness of the orthogonal constraint for the reference set.
292"
ABLATION STUDY,0.6934579439252336,"Model parameter quantity comparison. We compare the test accuracy and parameter quantity
293"
ABLATION STUDY,0.6953271028037383,"(only the pooling layer) of BernPool with other pooling methods in the PROTEINS dataset, where
294"
ABLATION STUDY,0.697196261682243,"the hidden layer dimension is set to 128. BernPool achieves superior performance while using
295"
ABLATION STUDY,0.6990654205607477,"fewer parameters. Specifically, BernPool owns 97% fewer parameters than CGIPool and 76% fewer
296"
ABLATION STUDY,0.7009345794392523,"parameters than ASAP. In terms of accuracy, BernPool performs the best, achieving 9.19% higher
297"
ABLATION STUDY,0.702803738317757,"than CGIPool and 9.10% higher accuracy than ASAP.
298"
ABLATION STUDY,0.7046728971962617,"IMDB-B
PROTEINS
COLLAB
60 65 70 75 80 85"
ABLATION STUDY,0.7065420560747664,Accuracy
ABLATION STUDY,0.708411214953271,"81.0
82.22 84.7 81.3 83.29 85.26"
ABLATION STUDY,0.7102803738317757,"BernPool w/o orthogonality
BernPool"
ABLATION STUDY,0.7121495327102804,"(a) Benefit of orthogonality for ref-
erence set"
ABLATION STUDY,0.7140186915887851,"0
100
200
300
400
500
70 72 74 76 78 80 82"
ABLATION STUDY,0.7158878504672898,Accuracy
ABLATION STUDY,0.7177570093457943,"BernPool
TopkPool
ASAP
EdgePool
SAGPool
CGIPool"
ABLATION STUDY,0.719626168224299,(b) Parameter quantity (KB)
ABLATION STUDY,0.7214953271028037,"0.0
0.2
0.4
0.6
0.8
70 72 74 76 78 80 82"
ABLATION STUDY,0.7233644859813084,Accuracy
ABLATION STUDY,0.7252336448598131,"BernPool
TopkPool
ASAP
EdgePool
SAGPool
CGIPool"
ABLATION STUDY,0.7271028037383177,(c) Inference time (seconds)
ABLATION STUDY,0.7289719626168224,"Figure 2: The benefit of orthogonality reference set and comparison of parameter quantity and
inference time."
ABLATION STUDY,0.7308411214953271,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
50 55 60 65 70 75 80 85"
ABLATION STUDY,0.7327102803738318,Accuracy
ABLATION STUDY,0.7345794392523365,"PROTEINS
NCI1"
ABLATION STUDY,0.7364485981308411,(a) Pooling ratio
ABLATION STUDY,0.7383177570093458,"1
2
3
4
50 55 60 65 70 75 80 85"
ABLATION STUDY,0.7401869158878505,Accuracy
ABLATION STUDY,0.7420560747663552,"PROTEINS
NCI1"
ABLATION STUDY,0.7439252336448599,(b) Model layers
ABLATION STUDY,0.7457943925233644,"32
64
128
256
50 55 60 65 70 75 80 85"
ABLATION STUDY,0.7476635514018691,Accuracy
ABLATION STUDY,0.7495327102803738,"PROTEINS
NCI1"
ABLATION STUDY,0.7514018691588785,(c) Hidden layer dimension
ABLATION STUDY,0.7532710280373832,Figure 3: The performance of different hyper-parameters on PROTEINS and NCI1 datasets.
ABLATION STUDY,0.7551401869158878,"Inference time comparison. The computation complexity of our BernPool is described in Section 4.6.
299"
ABLATION STUDY,0.7570093457943925,"Moreover, the inference time also plays a crucial role in evaluating the efficiency of pooling methods.
300"
ABLATION STUDY,0.7588785046728972,"Thus we conduct experiments on the PROTEINS dataset containing about 20,000 nodes and 70,000
301"
ABLATION STUDY,0.7607476635514019,"edges to compare the inference time (single-layer). As shown in Fig. 2(c), BernPool costs less
302"
ABLATION STUDY,0.7626168224299066,"inference time than EdgePool and ASAP while maintaining superior performance. Notably, SAGPool
303"
ABLATION STUDY,0.7644859813084112,"has a similar inference time as our method, but our method outperforms it in terms of classification
304"
ABLATION STUDY,0.7663551401869159,"accuracy. The comparison results verify the high efficiency of our BernPool.
305"
ABLATION STUDY,0.7682242990654206,"Sensitivity of Hyper-parameters. To evaluate the sensitivity of hyper-parameters, including the
306"
ABLATION STUDY,0.7700934579439253,"pooling ratio, layer number, and hidden layer dimension, we additionally conduct experiments on
307"
ABLATION STUDY,0.77196261682243,"PROTEINS and NCI1 datasets. Specifically, we vary the pooling ratio from 0.2 to 0.8 with a step
308"
ABLATION STUDY,0.7738317757009345,"length of 0.2, the number of layers from one to four, and the hidden layer dimensions range from 16
309"
ABLATION STUDY,0.7757009345794392,"to 128. The results are presented in Fig. 3. We can observe that BernPool overall exhibits robustness
310"
ABLATION STUDY,0.7775700934579439,"to variations of parameters. However, performance fluctuations can be observed on the NCI1 dataset
311"
ABLATION STUDY,0.7794392523364486,"when we evaluate the effect of different pooling ratios for BernPool. This may be attributed to the
312"
ABLATION STUDY,0.7813084112149533,"relatively less average number of nodes resulting in less information being retained after performing
313"
ABLATION STUDY,0.7831775700934579,"three pooling layers consecutively. The results shown in Fig. 3(b) indicate that setting the layer
314"
ABLATION STUDY,0.7850467289719626,"number to three achieves the best performance on the PROTEINS and NCI1 datasets. However,
315"
ABLATION STUDY,0.7869158878504673,"increasing the number of layers will require more computation resources and longer training time. As
316"
ABLATION STUDY,0.788785046728972,"shown in Fig. 3(c), the highest accuracy is achieved when the dimension size is set as 128. With the
317"
ABLATION STUDY,0.7906542056074767,"dimension increasing, the accuracy presents a slight increase trend, which suggests that increasing
318"
ABLATION STUDY,0.7925233644859813,"the dimension size can enhance the model’s capacity to capture more complex representations of the
319"
ABLATION STUDY,0.794392523364486,"input graph. However, larger dimension sizes increase computation burdens. Thus we set the pooling
320"
ABLATION STUDY,0.7962616822429907,"ratio as 0.8, the layer number as 3, and the dimension as 128 in our framework.
321"
CONCLUSION,0.7981308411214953,"7
Conclusion
322"
CONCLUSION,0.8,"In this paper, we proposed a simple and effective graph pooling method, called Graph Bernoulli
323"
CONCLUSION,0.8018691588785046,"Pooling (BernPool) to promote the graph classification task. Specifically, a probabilistic Bernoulli
324"
CONCLUSION,0.8037383177570093,"sampling was designed to estimate the sampling probabilities of graph nodes, and to further extract
325"
CONCLUSION,0.805607476635514,"more useful information, we introduced a learnable reference set to encode nodes into a latent
326"
CONCLUSION,0.8074766355140187,"expressive probability space. Compared with the deterministic way, BernPool possessed more
327"
CONCLUSION,0.8093457943925234,"diversity to capture salient substructures. Then, to jointly learn representative substructures and
328"
CONCLUSION,0.811214953271028,"preserve graph topology information, we proposed a hybrid graph pooling paradigm that fuses two
329"
CONCLUSION,0.8130841121495327,"pooling manners. We evaluate BernPool on multiple widely used datasets and dissected the framework
330"
CONCLUSION,0.8149532710280374,"with ablation analysis. The experimental results show that BernPool outperforms state-of-the-art
331"
CONCLUSION,0.8168224299065421,"methods and demonstrates the effectiveness of our proposed modules.
332"
REFERENCES,0.8186915887850468,"References
333"
REFERENCES,0.8205607476635514,"[1] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph
334"
REFERENCES,0.822429906542056,"neural networks for graph pooling. In International conference on machine learning, pages
335"
REFERENCES,0.8242990654205608,"874–883. PMLR, 2020.
336"
REFERENCES,0.8261682242990654,"[2] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J
337"
REFERENCES,0.8280373831775701,"Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics,
338"
REFERENCES,0.8299065420560747,"21(suppl_1):i47–i56, 2005.
339"
REFERENCES,0.8317757009345794,"[3] Frederik Diehl.
Edge contraction pooling for graph neural networks.
arXiv preprint
340"
REFERENCES,0.8336448598130841,"arXiv:1905.10990, 2019.
341"
REFERENCES,0.8355140186915888,"[4] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes
342"
REFERENCES,0.8373831775700935,"without alignments. Journal of molecular biology, 330(4):771–783, 2003.
343"
REFERENCES,0.8392523364485981,"[5] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
344"
REFERENCES,0.8411214953271028,"Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
345"
REFERENCES,0.8429906542056075,"molecular fingerprints. Advances in neural information processing systems, 28, 2015.
346"
REFERENCES,0.8448598130841122,"[6] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
347"
REFERENCES,0.8467289719626169,"Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
348"
REFERENCES,0.8485981308411215,"molecular fingerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,
349"
REFERENCES,0.8504672897196262,"editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,
350"
REFERENCES,0.8523364485981308,"Inc., 2015.
351"
REFERENCES,0.8542056074766355,"[7] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang,
352"
REFERENCES,0.8560747663551402,"Evgeny Kharlamov, and Jie Tang. Graph random neural networks for semi-supervised learning
353"
REFERENCES,0.8579439252336448,"on graphs. Advances in neural information processing systems, 33:22092–22103, 2020.
354"
REFERENCES,0.8598130841121495,"[8] Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, and Karsten Borgwardt.
355"
REFERENCES,0.8616822429906542,"Scalable kernels for graphs with continuous attributes. Advances in neural information process-
356"
REFERENCES,0.8635514018691589,"ing systems, 26, 2013.
357"
REFERENCES,0.8654205607476636,"[9] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
358"
REFERENCES,0.8672897196261682,"arXiv preprint arXiv:1903.02428, 2019.
359"
REFERENCES,0.8691588785046729,"[10] Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,
360"
REFERENCES,0.8710280373831776,"pages 2083–2092. PMLR, 2019.
361"
REFERENCES,0.8728971962616823,"[11] Xing Gao, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Pascal Frossard. ipool—information-
362"
REFERENCES,0.874766355140187,"based pooling in hierarchical graph neural networks. IEEE Transactions on Neural Networks
363"
REFERENCES,0.8766355140186916,"and Learning Systems, 33(9):5032–5044, 2021.
364"
REFERENCES,0.8785046728971962,"[12] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In
365"
REFERENCES,0.8803738317757009,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
366"
REFERENCES,0.8822429906542056,"pages 729–734 vol. 2, 2005.
367"
REFERENCES,0.8841121495327103,"[13] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
368"
REFERENCES,0.8859813084112149,"graphs. Advances in neural information processing systems, 30, 2017.
369"
REFERENCES,0.8878504672897196,"[14] Takeshi D Itoh, Takatomi Kubo, and Kazushi Ikeda. Multi-level attention pooling for graph
370"
REFERENCES,0.8897196261682243,"neural networks: Unifying graph representations with multiple localities. Neural Networks,
371"
REFERENCES,0.891588785046729,"145:356–373, 2022.
372"
REFERENCES,0.8934579439252337,"[15] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for
373"
REFERENCES,0.8953271028037383,"mutagenicity prediction. Journal of medicinal chemistry, 48(1):312–320, 2005.
374"
REFERENCES,0.897196261682243,"[16] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
375"
REFERENCES,0.8990654205607477,"networks. arXiv preprint arXiv:1609.02907, 2016.
376"
REFERENCES,0.9009345794392524,"[17] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International
377"
REFERENCES,0.902803738317757,"conference on machine learning, pages 3734–3743. PMLR, 2019.
378"
REFERENCES,0.9046728971962616,"[18] Maosen Li, Siheng Chen, Ya Zhang, and Ivor Tsang. Graph cross networks with vertex infomax
379"
REFERENCES,0.9065420560747663,"pooling. Advances in Neural Information Processing Systems, 33:14093–14105, 2020.
380"
REFERENCES,0.908411214953271,"[19] David Liben-Nowell and Jon Kleinberg. The link prediction problem for social networks. In
381"
REFERENCES,0.9102803738317757,"Proceedings of the twelfth international conference on Information and knowledge management,
382"
REFERENCES,0.9121495327102803,"pages 556–559, 2003.
383"
REFERENCES,0.914018691588785,"[20] Yunsheng Pang, Yunxiang Zhao, and Dongsheng Li. Graph pooling via coarsened graph
384"
REFERENCES,0.9158878504672897,"infomax. In Proceedings of the 44th International ACM SIGIR Conference on Research and
385"
REFERENCES,0.9177570093457944,"Development in Information Retrieval, pages 2177–2181, 2021.
386"
REFERENCES,0.9196261682242991,"[21] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random
387"
REFERENCES,0.9214953271028037,"dropouts increase the expressiveness of graph neural networks. Advances in Neural Information
388"
REFERENCES,0.9233644859813084,"Processing Systems, 34:21997–22009, 2021.
389"
REFERENCES,0.9252336448598131,"[22] Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. Asap: Adaptive structure aware pooling
390"
REFERENCES,0.9271028037383178,"for learning hierarchical graph representations. In Proceedings of the AAAI Conference on
391"
REFERENCES,0.9289719626168225,"Artificial Intelligence, volume 34, pages 5470–5477, 2020.
392"
REFERENCES,0.930841121495327,"[23] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
393"
REFERENCES,0.9327102803738317,"The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.
394"
REFERENCES,0.9345794392523364,"[24] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
395"
REFERENCES,0.9364485981308411,"Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9),
396"
REFERENCES,0.9383177570093458,"2011.
397"
REFERENCES,0.9401869158878504,"[25] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua
398"
REFERENCES,0.9420560747663551,"Bengio, et al. Graph attention networks. stat, 1050(20):10–48550, 2017.
399"
REFERENCES,0.9439252336448598,"[26] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for
400"
REFERENCES,0.9457943925233645,"sets. arXiv preprint arXiv:1511.06391, 2015.
401"
REFERENCES,0.9476635514018692,"[27] Zhengyang Wang and Shuiwang Ji. Second-order pooling for graph neural networks. IEEE
402"
REFERENCES,0.9495327102803738,"Transactions on Pattern Analysis and Machine Intelligence, 2020.
403"
REFERENCES,0.9514018691588785,"[28] Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for
404"
REFERENCES,0.9532710280373832,"graph classification. In Proceedings of the 30th ACM International Conference on Information
405"
REFERENCES,0.9551401869158879,"& Knowledge Management, pages 2091–2100, 2021.
406"
REFERENCES,0.9570093457943926,"[29] Junran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. Structural entropy guided graph hierarchi-
407"
REFERENCES,0.9588785046728971,"cal pooling. In International Conference on Machine Learning, pages 24017–24030. PMLR,
408"
REFERENCES,0.9607476635514018,"2022.
409"
REFERENCES,0.9626168224299065,"[30] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
410"
REFERENCES,0.9644859813084112,"networks? arXiv preprint arXiv:1810.00826, 2018.
411"
REFERENCES,0.9663551401869159,"[31] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
412"
REFERENCES,0.9682242990654205,"SIGKDD international conference on knowledge discovery and data mining, pages 1365–1374,
413"
REFERENCES,0.9700934579439252,"2015.
414"
REFERENCES,0.9719626168224299,"[32] Pinar Yanardag and SVN Vishwanathan. A structural smoothing framework for robust graph
415"
REFERENCES,0.9738317757009346,"comparison. Advances in neural information processing systems, 28, 2015.
416"
REFERENCES,0.9757009345794393,"[33] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.
417"
REFERENCES,0.9775700934579439,"Hierarchical graph representation learning with differentiable pooling. Advances in neural
418"
REFERENCES,0.9794392523364486,"information processing systems, 31, 2018.
419"
REFERENCES,0.9813084112149533,"[34] Hao Yuan and Shuiwang Ji. Structpool: Structured graph pooling via conditional random fields.
420"
REFERENCES,0.983177570093458,"In Proceedings of the 8th International Conference on Learning Representations, 2020.
421"
REFERENCES,0.9850467289719627,"[35] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in
422"
REFERENCES,0.9869158878504672,"neural information processing systems, 31, 2018.
423"
REFERENCES,0.9887850467289719,"[36] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
424"
REFERENCES,0.9906542056074766,"architecture for graph classification. In Proceedings of the AAAI conference on artificial
425"
REFERENCES,0.9925233644859813,"intelligence, volume 32, 2018.
426"
REFERENCES,0.994392523364486,"[37] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Zhao Li, Chengwei Yao, Huifen Dai,
427"
REFERENCES,0.9962616822429906,"Zhi Yu, and Can Wang. Hierarchical multi-view graph pooling with structure learning. IEEE
428"
REFERENCES,0.9981308411214953,"Transactions on Knowledge and Data Engineering, 35(1):545–559, 2021.
429"
