Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010395010395010396,"Agent behavior simulation empowers robotics, gaming, movies, and VR appli-
1"
ABSTRACT,0.002079002079002079,"cations, but building such simulators often requires laborious effort of manually
2"
ABSTRACT,0.0031185031185031187,"crafting the agent’s decision process and motion patterns. Recent advances in
3"
ABSTRACT,0.004158004158004158,"visual tracking and motion capture have enabled learning agent behavior from
4"
ABSTRACT,0.005197505197505198,"real-world data, but these methods are limited to a few scenarios due to the de-
5"
ABSTRACT,0.006237006237006237,"pendence on specialized sensors (e.g., synchronized multi-camera systems). In a
6"
ABSTRACT,0.007276507276507277,"step towards scalable and realistic behavior simulators, we present Agent-to-Sim
7"
ABSTRACT,0.008316008316008316,"(ATS), a framework for learning simulatable 3D agents in a 3D environment from
8"
ABSTRACT,0.009355509355509356,"casually-captured monocular videos. To deal with partial views, our framework
9"
ABSTRACT,0.010395010395010396,"fuses observations in a canonical space for both the agent and the scene, resulting
10"
ABSTRACT,0.011434511434511435,"in a dense 4D spatiotemporal reconstruction. We then learn an interactive behavior
11"
ABSTRACT,0.012474012474012475,"generator by querying paired data of agents’ perception and actions from the 4D
12"
ABSTRACT,0.013513513513513514,"reconstruction. ATS enables real-to-sim transfer of agents in their familiar envi-
13"
ABSTRACT,0.014553014553014554,"ronments given longitudinal video recordings captured with a smartphone over a
14"
ABSTRACT,0.015592515592515593,"month. We show results on pets (e.g., cat, dog, bunny) and a person, and analyse
15"
ABSTRACT,0.016632016632016633,"how the observer’s motion and 3D scene affect an agent’s behavior.
16"
INTRODUCTION,0.017671517671517672,"1
Introduction
17"
INTRODUCTION,0.018711018711018712,Plausible paths
INTRODUCTION,0.01975051975051975,"Past Tajectory
Consider the scene of the cat in the living room: where will the cat go
18"
INTRODUCTION,0.02079002079002079,"and how will it move? Since we have seen cats interact with the en-
19"
INTRODUCTION,0.02182952182952183,"vironment and other people many times, we know that cats like to go
20"
INTRODUCTION,0.02286902286902287,"to the couch, often move slowly, and follow humans around, but run
21"
INTRODUCTION,0.02390852390852391,"away if people come too close. Such a predictive model of a phys-
22"
INTRODUCTION,0.02494802494802495,"ical agent is what enables plausible behavior simulation, which is
23"
INTRODUCTION,0.02598752598752599,"essential for embodied intelligence, immersive virtual environments
24"
INTRODUCTION,0.02702702702702703,"and robot planning in safety-critical scenarios [9, 31, 41, 45, 54].
25"
INTRODUCTION,0.028066528066528068,"The key challenge with behavior simulation is how to generate plausible and interactive behavior
26"
INTRODUCTION,0.029106029106029108,"(with respect to the scene and other agents). On one hand, prior works [2, 6, 46] utilize trajectory
27"
INTRODUCTION,0.030145530145530147,"computed by path-planning algorithms or hand-designed logic from game simulators [13, 58]. While
28"
INTRODUCTION,0.031185031185031187,"these approaches benefit from high-quality trajectory data paired with perfect object and scene
29"
INTRODUCTION,0.032224532224532226,"geometries, it is laborious to manually craft simulators that suit the needs of each type of application,
30"
INTRODUCTION,0.033264033264033266,"and the data distribution is fundamentally different from the real world, leading to unnatural motion
31"
INTRODUCTION,0.034303534303534305,"and interactions. On the other hand, vision-based motion capture enables learning plausible behavior
32"
INTRODUCTION,0.035343035343035345,"directly from data for certain scenarios, such as autonomous driving [9], human body motion [21, 36],
33"
INTRODUCTION,0.036382536382536385,"and interaction with objects/scenes [14, 24]. However, due to the dependence on specialized sensor
34"
INTRODUCTION,0.037422037422037424,"(synchronized multi-camera systems, IMUs, pre-scanned objects), such systems does not scale well
35"
INTRODUCTION,0.038461538461538464,"to the full spectrum of natural behavior one may care about, such as behavior of animals, casual
36"
INTRODUCTION,0.0395010395010395,"events, and long-term activities.
37"
INTRODUCTION,0.04054054054054054,Observer Agent Scene
INTRODUCTION,0.04158004158004158,Shot 1 Time
INTRODUCTION,0.04261954261954262,A) 4D Spacetime Reconstruction …
INTRODUCTION,0.04365904365904366,Shot M
INTRODUCTION,0.0446985446985447,What happens if  “z”?
INTRODUCTION,0.04573804573804574,B) Interactive Behavior Simulator
INTRODUCTION,0.04677754677754678,"User: “move sideway”
User: “idle”
User: “approach quickly”"
INTRODUCTION,0.04781704781704782,"Figure 1: Learning agent behavior from longitudinal casual video recordings. We answer the
following question: can we simulate the behavior of an agent, by learning from casually-captured
videos of the same agent recorded across a long period of time (e.g., a month)? A) We first reconstruct
videos in 4D (3D & time), which includes the scene, the trajectory of the agent, and the trajectory of
the observer (i.e., camera held by observer). Such individual 4D reconstruction are registered across
time, resulting in a complete 4D reconstructions. B) Then we learn a representation of the agent that
allows for interactive behavior simulation. The behavior model explicitly reasons about goals, paths,
and full body movements conditioned on the agent’s ego-perception and past trajectory. Such agent
representation allows us to simulate novel scenarios through conditioning. For example, conditioned
different observer trajectories, the cat agent choose to walk to the carpet, stays still while quivering
his tail, or hide under the tray stand. Please see videos and results of other agents in the supplement."
INTRODUCTION,0.04885654885654886,"Recent advances in differentiable rendering [10, 12, 23, 38, 42, 52, 59, 65] and monocular MoCap [28,
38"
INTRODUCTION,0.0498960498960499,"43, 69, 70] provide a pathway to obtain high-quality models of scenes and agents from monocular
39"
INTRODUCTION,0.05093555093555094,"videos alone. Despite the potential of covering diverse data of agent behavior that match the real-
40"
INTRODUCTION,0.05197505197505198,"world distributions, none of the existing works brings a solution of reconstructing dense 3D structures
41"
INTRODUCTION,0.05301455301455302,"of both the agent and scene, which is crucial for learning agent behavior grounded in real world
42"
INTRODUCTION,0.05405405405405406,"environments. To address this, we present ATS (Agent-to-Sim), a framework for learning simulatable
43"
INTRODUCTION,0.0550935550935551,"agent from casual videos captured over a long time horizon (e.g. 1 month), as shown in Fig. 1.
44"
INTRODUCTION,0.056133056133056136,"The crucial technical challenge is the presence of partial visibility – in each video captured from
45"
INTRODUCTION,0.057172557172557176,"an observer’s viewpoint, only parts of the agent and the environment are visible. How do we infer
46"
INTRODUCTION,0.058212058212058215,"the states of agent and the environment that are not visible? To build a dense 4D spatiotemporal
47"
INTRODUCTION,0.059251559251559255,"reconstruction, our key insight is to leverage the observations from multiple videos by fusing them
48"
INTRODUCTION,0.060291060291060294,"in a canonical 3D space. We introduce a novel coarse-to-fine registration approach that re-purposes
49"
INTRODUCTION,0.061330561330561334,"“foundational” visual features [40] as a neural localizer, which “registers” the camera with respect
50"
INTRODUCTION,0.062370062370062374,"to a canonical structure. This enables capturing interactive behavior data in a casual setup (e.g.,
51"
INTRODUCTION,0.06340956340956341,"with a smartphone), and provides paired training data of perception and action of an agent that is
52"
INTRODUCTION,0.06444906444906445,"grounded in a natural environment (Fig. 2). To learn an interactive behavior model, we condition the
53"
INTRODUCTION,0.06548856548856549,"action of an agent on their ego-perception, and leverage diffusion models [18, 53] to account for the
54"
INTRODUCTION,0.06652806652806653,"multimodal nature of goals and planned trajectories. The resulting framework, ATS, can simulate
55"
INTRODUCTION,0.06756756756756757,"interactive behaviors like those described at the start: agents like pets that leap onto furniture, dart
56"
INTRODUCTION,0.06860706860706861,"quickly across the room, timidly approach nearby users, and run away if approached too quickly. Our
57"
INTRODUCTION,0.06964656964656965,"contributions are summerized as follows:
58"
INTRODUCTION,0.07068607068607069,"1. Agent-to-Sim (ATS) Framework. We introduce a real-to-sim framework, ATS, to learn
59"
INTRODUCTION,0.07172557172557173,"simulators of interactive agent behavior from casually-captured videos. ATS learns plausible
60"
INTRODUCTION,0.07276507276507277,"agent behavior that matches the real-world, and is scalable to diverse scenarios, such as
61"
INTRODUCTION,0.07380457380457381,"animal behavior and casual events.
62"
INTRODUCTION,0.07484407484407485,"2. Environment-Interactive Behavior Simulation. ATS learns behavior that is interactive
63"
INTRODUCTION,0.07588357588357589,"to the environment, including both the observer and 3D scene. We show the first result
64"
INTRODUCTION,0.07692307692307693,"of generating plausible behavior of animals that are reactive to observer’s motion, and are
65"
INTRODUCTION,0.07796257796257797,"aware of the 3D scene.
66"
INTRODUCTION,0.079002079002079,"Table 1: Related works in behavior data capture. ATS is the only method that builds a complete
4D reconstruction of both the agents and the environment. Different from prior work that focus on
specific domains, ATS can be applied to capture interactive behavior of both animals and humans
from casual RGBD videos (e.g. captured by a smartphone)."
INTRODUCTION,0.08004158004158005,"Method
Agent Model
Scene Model
Capture Setup
Domain"
INTRODUCTION,0.08108108108108109,"UCY [30] & ETH [44]
Point
N.A.
Manual Anno.
Pedestrian
nuScenes [9]
Point
Dense 3D Map
Manual Anno.
Pedestrian, Vehicle
SAMP [14]
Parametric Body
Furniture & Objects
Multi-Camera
Human
AMASS [36]
Parametric Body
N.A.
Multi-Camera
Human
ActionMap [47]
Action Class
Sparse 3D Map
Egocentric Camera
Human
ATS (Ours)
Non-parametric
Dense 3D Map
Casual RGBD
Animal, Human"
INTRODUCTION,0.08212058212058213,"3. Complete 4D Registration & Reconstruction. We present a method to register and
67"
INTRODUCTION,0.08316008316008316,"reconstruct a temporally-evolving 3D scene, whiling accounts for changes in scene layout
68"
INTRODUCTION,0.0841995841995842,"and appearance.
69"
RELATED WORKS,0.08523908523908524,"2
Related Works
70"
RELATED WORKS,0.08627858627858628,"Behavior Prediction and Generation. Behavior prediction has a long history, starting from simple
71"
RELATED WORKS,0.08731808731808732,"physics-based models such as social forces [17] to more sophisticated “planning-based” models that
72"
RELATED WORKS,0.08835758835758836,"cast prediction as reward optimization [26, 76], where the reward is learned via inverse reinforcement
73"
RELATED WORKS,0.0893970893970894,"learning [75]. With the advent of large-scale pedestrian and vehicle motion data collected in the
74"
RELATED WORKS,0.09043659043659044,"navigation and autonomous driving domains [1, 34, 37, 48, 50], generative prediction models such as
75"
RELATED WORKS,0.09147609147609148,"diffusion models have been able to express behavior multi-modality while being easily controlled via
76"
RELATED WORKS,0.09251559251559252,"additional signals such as cost functions [20] or logical formulae [74]. However, to capture plausible
77"
RELATED WORKS,0.09355509355509356,"behavior of agents, these approaches are extremely dependant on high-quality agent trajectory data
78"
RELATED WORKS,0.0945945945945946,"collected “in the wild” with the associated scene context (e.g., 3D map of the scene) [9]. Such data are
79"
RELATED WORKS,0.09563409563409564,"often manually annotated at a bounding box level (Tab. 1), which limits the scale and the level of detail
80"
RELATED WORKS,0.09667359667359668,"they can capture. Beyond autonomous driving setup, existing works for human motion prediction and
81"
RELATED WORKS,0.09771309771309772,"generation [46, 57, 62] have been primarily using simulated data [6] or motion capture data collected
82"
RELATED WORKS,0.09875259875259876,"with multiple synchronized cameras [14, 24, 36]. Such data provide high-quality full body motion
83"
RELATED WORKS,0.0997920997920998,"of human using parametric body models [32], but the interactions with the environment are often
84"
RELATED WORKS,0.10083160083160084,"restricted to a set of pre-defined furnitures and objects [15, 29, 73]. Furthermore, the use of simulated
85"
RELATED WORKS,0.10187110187110188,"data and motion capture data inherently limits the realism of these behavior generators, since real
86"
RELATED WORKS,0.10291060291060292,"agents will behave very differently in their familiar environment. To bridge the gap, we develop
87"
RELATED WORKS,0.10395010395010396,"4D reconstruction method to obtain high-quality trajectories of agents in their natural environment,
88"
RELATED WORKS,0.104989604989605,"with a simple setup that can be achieved with a smartphone. Close to our setup, ActionMap [47]
89"
RELATED WORKS,0.10602910602910603,"associate daily actions performed by a human agent with an reconstructed 3D environment given
90"
RELATED WORKS,0.10706860706860707,"egocentric videos. However, they focus on actions performed by hand and do not reconstruct the full
91"
RELATED WORKS,0.10810810810810811,"body motion of the agent.
92"
RELATED WORKS,0.10914760914760915,"4D Reconstruction from Monocular Videos. Reconstructing agents and the environment from
93"
RELATED WORKS,0.1101871101871102,"monocular videos is challenging due to its under-constrained nature. Given a monocular video,
94"
RELATED WORKS,0.11122661122661123,"there are multiple different interpretations of the underlying 3D geometry, motion, appearance,
95"
RELATED WORKS,0.11226611226611227,"and lighting [56]. As such, reconstructing agents often require category-specific 3D prior (e.g., 3D
96"
RELATED WORKS,0.11330561330561331,"humans) [11, 27, 32]. Along this line of work, researchers reconstruct 3D humans aligned to the world
97"
RELATED WORKS,0.11434511434511435,"coordinate with the help of SLAM and visual odometry [28, 69, 70]. Sitcoms3D [43] reconstructs
98"
RELATED WORKS,0.11538461538461539,"both the scene and human parameters, while relying on shot changes to determine the scale of the
99"
RELATED WORKS,0.11642411642411643,"scene. However, the use of parametric body models limits the degrees of freedom they can capture,
100"
RELATED WORKS,0.11746361746361747,"and makes it difficult to reconstruct agents from arbitrary categories which do not have a pre-built
101"
RELATED WORKS,0.11850311850311851,"body model, for example, animals. Another line of work avoids using category-specific 3D priors and
102"
RELATED WORKS,0.11954261954261955,"optimizes the shape and deformation parameters of the agent given richer visual signals (e.g., optical
103"
RELATED WORKS,0.12058212058212059,"flow and object silhouette) [61, 64, 65], which is shown to work well for a broad range of category
104"
RELATED WORKS,0.12162162162162163,"including human, animals, and vehicles. TotalRecon [52] further incorporates the background scene
105"
RELATED WORKS,0.12266112266112267,"into the model-free reconstruction pipeline, such that the agent’s motion can be decoupled from the
106"
RELATED WORKS,0.12370062370062371,"camera motion and aligned to the scene space. However, none of the existing methods can reconstruct
107"
RELATED WORKS,0.12474012474012475,"both the agent and the scene in high-quality. In practice, individual videos may not contain sufficient
108"
RELATED WORKS,0.1257796257796258,"views, leading to inaccurate and incomplete reconstructions. Our method registers both the agent and
109"
RELATED WORKS,0.12681912681912683,"the environment from multiple videos into a shared space, which leverages large-scale data collection
110"
RELATED WORKS,0.12785862785862787,"to build a high-quality agent and scene model.
111"
APPROACH,0.1288981288981289,"3
Approach
112"
APPROACH,0.12993762993762994,"We describe a method to learn interactive behavior models given longitudinal video recordings of an
113"
APPROACH,0.13097713097713098,"agent in the same environment. We first build a spatiotemporal 4D reconstruction, including the agent,
114"
APPROACH,0.13201663201663202,"the scene, and the observer (Sec. 3.1), which is solved by an optimization involving multi-video
115"
APPROACH,0.13305613305613306,"registration (Sec. 3.2). We then train an interactive behavior model of the agent that is interactive
116"
APPROACH,0.1340956340956341,"with the surrounding environment, including the scene and the motion of the observer (Sec. 3.3).
117"
APPROACH,0.13513513513513514,"3.1
4D Representation: Agent, Scene, and Observer
118"
APPROACH,0.13617463617463618,"Given multiple monocular videos, our goal is to build a dense spatiotemporal 4D reconstruction of
119"
APPROACH,0.13721413721413722,"the underlying world, including a deformable agent, a background scene, and a moving observer.
120"
APPROACH,0.13825363825363826,"The task is ill-posed due to partial visibility – from an observer’s viewpoint, the agent and the
121"
APPROACH,0.1392931392931393,"environment are only partially visible. To deal with this problem, one principle approach is geometric
122"
APPROACH,0.14033264033264034,"registration, where structures not visible from one view can be inferred from the other views they
123"
APPROACH,0.14137214137214138,"appear [51]. We build upon this idea to reconstruct a complete spatiotemporal model of an agent and
124"
APPROACH,0.14241164241164242,"their familiar environment by registering videos captured at different time.
125"
APPROACH,0.14345114345114346,"Problem Setup. Specifically, given images from M videos represented by color and feature descrip-
126"
APPROACH,0.1444906444906445,"tors [40], {Ii, ψi}i={1,...,M}, our goal is to find a 4D spatiotemporal representation that explains the
127"
APPROACH,0.14553014553014554,"video, while pixels with the same semantics can be mapped to consistent canonical 3D locations. Our
128"
APPROACH,0.14656964656964658,"representation factorizes the 4D structure into a static component and a time-varying component.
129"
APPROACH,0.14760914760914762,"Static Representation. T = {σ, c, ψ}. We represent the static component as agent fields and scene
130"
APPROACH,0.14864864864864866,"fields. Both define densities, colors, and semantic features in a canonical space,
131"
APPROACH,0.1496881496881497,"(σs, cs, ψs) = MLPscene(X, βi),
(1) 132"
APPROACH,0.15072765072765074,"(σa, ca, ψa) = MLPagent(X),
(2)"
APPROACH,0.15176715176715178,"where X corresponds to a 3D point. To account for structures that change across videos, we modify
133"
APPROACH,0.15280665280665282,"the scene fields to take a per-video latent code βi as input, which allows fitting video-specific details.
134"
APPROACH,0.15384615384615385,"Time-varying Representation. D = {ξ, G, W}. The time-varying component includes a moving
135"
APPROACH,0.1548856548856549,"observer, represented by the camera pose ξt ∈SE(3), and the motion of an agent, represented by a
136"
APPROACH,0.15592515592515593,"set of rigid bodies, {Gb
t}{b=1,...,25}, referred to as “bones”. Given a time t, the canonical space of
137"
APPROACH,0.15696465696465697,"the agent can be mapped to the camera space by blend-skinning deformation [35, 65],
138"
APPROACH,0.158004158004158,"Xt = GaX = B
X"
APPROACH,0.15904365904365905,"b=1
WbGb
t !"
APPROACH,0.1600831600831601,"X,
(3)"
APPROACH,0.16112266112266113,"which computes the motion of a point by blending the bone transformations (we do so in the dual
139"
APPROACH,0.16216216216216217,"quaternion space [22, 66] to ensure Ga is a valid rigid transformation). The skinning weights W are
140"
APPROACH,0.1632016632016632,"defined as the probability of a point assigned to each bone.
141"
APPROACH,0.16424116424116425,"Rendering. To turn the 4D representation into images, we sample rays in the camera space, map
142"
APPROACH,0.1652806652806653,"them separately to the canonical space of the scene and the agent with D, and query values (e.g.,
143"
APPROACH,0.16632016632016633,"density, color, feature) from corresponding fields of the scene and the agent. The values are then
144"
APPROACH,0.16735966735966737,"combined before ray integration [39, 52]. Consequently, the rendered pixel values are compared
145"
APPROACH,0.1683991683991684,"against the observations to update the world representation {T, D}.
146"
APPROACH,0.16943866943866945,"Decoupling Agent Motion from Observer. {Gb
t}{b=1,...,25} defines the motion of an agent with
147"
APPROACH,0.1704781704781705,"respect to the observer. Given the observer, we compute the motion of the agent in the scene space as,
148"
APPROACH,0.17151767151767153,"Gb→s
t
= ξ−1
t Gb
t,
(4)"
APPROACH,0.17255717255717257,"where the results of extracted trajectories of the agent is shown in Fig. 2
149"
APPROACH,0.1735966735966736,Shot 1
APPROACH,0.17463617463617465,Shot 2
APPROACH,0.17567567567567569,Shot 3
APPROACH,0.17671517671517672,Shot 4
APPROACH,0.17775467775467776,Shot 5
APPROACH,0.1787941787941788,"Registered 4D 
Reconstruction …"
APPROACH,0.17983367983367984,"Figure 2: Results of 4D reconstruction. Top: reference images and renderings of the reconstructions.
The color on the background represents correspondence. The colored blobs on the agent body
represent B = 25 body parts of the agent (e.g., head is represented by the yellow blob). Bottom:
Bird’s eye view of the reconstructed scene and agent trajectories, registered to the same scene
coordinate. Each colored line represents a unique video sequence where boxes and spheres indicate
the starting and the end location. Please see videos and results on other agents in the supplement."
APPROACH,0.18087318087318088,"3.2
Optimization: Multi-Video Registration
150"
APPROACH,0.18191268191268192,"To deal with bad local optima caused by camera poses (Fig. 4), we design a coarse-to-fine registration
151"
APPROACH,0.18295218295218296,"approach that globally aligns the cameras to a shared canonical space with a feed-forward network,
152"
APPROACH,0.183991683991684,"and then jointly optimizes the 3D structures while adjusting the cameras locally.
153"
APPROACH,0.18503118503118504,"Initialization: Neural Localization. Due to the evolving nature of scenes across a long period
154"
APPROACH,0.18607068607068608,"of time [55], there exist both global layout changes (e.g., furniture get rearranged) and appearance
155"
APPROACH,0.18711018711018712,"changes (e.g., table cloth gets replaced), making it challenging to find accurate geometric corre-
156"
APPROACH,0.18814968814968816,"spondences [4, 5, 49]. With the observation that “foundational” visual features have good 3D and
157"
APPROACH,0.1891891891891892,"viewpoint awareness [3], we adapt them for camera localization. We learn a scene-specific neural
158"
APPROACH,0.19022869022869024,"localizer that directly regresses the camera pose of an image with respect to a canonical structure,
159"
APPROACH,0.19126819126819128,"ξ = fθ(ψ),
(5)"
APPROACH,0.19230769230769232,"where fθ is a ResNet-18 [16] and ψ is the DINOv2 [40] feature of the input image. We find it to
160"
APPROACH,0.19334719334719336,"be more robust than geometric correspondence, while being more computationally efficient than
161"
APPROACH,0.1943866943866944,"performing pairwise matches [49]. To learn the neural localizer, we first capture a walk-through video
162"
APPROACH,0.19542619542619544,"and build a dense map of the scene. Then we use it to train the neural localizer by randomly sampling
163"
APPROACH,0.19646569646569648,"camera poses G∗= (R∗, t∗) and rendering images on the fly,
164"
APPROACH,0.19750519750519752,"arg min
θ X j"
APPROACH,0.19854469854469856," 
∥log(RT
0 (θ)R∗)∥+ ∥t0(θ) −t∗∥2
2

,
(6)"
APPROACH,0.1995841995841996,"where we use geodesic distance [19] for camera rotation and L2 error for camera translation. For the
165"
APPROACH,0.20062370062370063,"agent, we follow BANMo [65] to initialize the root pose {Gb}b=0 with a pre-trained pose network.
166"
APPROACH,0.20166320166320167,"Objective: Feature-metric Alignemnt. Given a coarse initialization of the observer (scene camera)
167"
APPROACH,0.20270270270270271,"and the agent’s root pose, we use both photometric and featuremetric losses to optimize {T, D},
168"
APPROACH,0.20374220374220375,"min
T,D X t"
APPROACH,0.2047817047817048," 
∥It −RI(t; T, D)∥2
2 + ∥ψt −Rψ(t; T, D)∥2
2

+ Lreg(T, D),
(7)"
APPROACH,0.20582120582120583,"where R(·) is the rendering function described in Sec 3.1. In contrast to prior works, using feature-
169"
APPROACH,0.20686070686070687,"metric errors makes the optimization robust to change of lighting, appearance, and helps find accurate
170"
APPROACH,0.2079002079002079,"alignment over multiple videos (Fig. 4). The regularization term includes eikonal loss, silhouette loss,
171"
APPROACH,0.20893970893970895,"flow loss and depth loss similar to prior works [52, 65].
172"
APPROACH,0.20997920997921,"Scene Annealing. To encourage the reconstructed scene across videos to share a similar structure, we
173"
APPROACH,0.21101871101871103,"randomly swap the code β of two videos during optimization, and gradually decrease the probability
174"
APPROACH,0.21205821205821207,"of swaps from P = 1.0 →0.05 over the course of optimization. This regularizes the model to
175"
APPROACH,0.2130977130977131,"effectively share information across all videos, and keeps video-specific details (Fig. 4).
176"
INTERACTIVE BEHAVIOR GENERATION,0.21413721413721415,"3.3
Interactive Behavior Generation
177"
INTERACTIVE BEHAVIOR GENERATION,0.2151767151767152,"Now that we build a complete 4D reconstruction from multiple videos, we can extract a scene structure
178"
INTERACTIVE BEHAVIOR GENERATION,0.21621621621621623,"T, and M trajectories of the agent {Gt}t={T1,...,TM} as well as the observer {ξt}t={T1,...,TM}
179"
INTERACTIVE BEHAVIOR GENERATION,0.21725571725571727,"grounded in the environment. We aim to learn an agent that is interactive with the world.
180"
INTERACTIVE BEHAVIOR GENERATION,0.2182952182952183,"Hierarchical Behavior Representation. We model the behavior of an agent by bone transformations
181"
INTERACTIVE BEHAVIOR GENERATION,0.21933471933471935,"in the scene space G ∈R6B×T ∗over a fixed time horizon T ∗= 5.6s, . We design a hierarchical
182"
INTERACTIVE BEHAVIOR GENERATION,0.2203742203742204,"model as shown in Fig. 3. The body motion G is conditioned on path P ∈R3×T ∗, which is further
183"
INTERACTIVE BEHAVIOR GENERATION,0.22141372141372143,"conditioned on goal Z ∈R3. Such decomposition allows agents to react by predicting goals with low
184"
INTERACTIVE BEHAVIOR GENERATION,0.22245322245322247,"latency
185"
INTERACTIVE BEHAVIOR GENERATION,0.2234927234927235,"Goal Generation. We represent a multi-modal distribution of goals Z ∈R3 by its score function
186"
INTERACTIVE BEHAVIOR GENERATION,0.22453222453222454,"s(Z, σ) ∈R3 [18, 53]. The score function is implemented as a coordinate MLP [38],
187"
INTERACTIVE BEHAVIOR GENERATION,0.22557172557172558,"s(Z; σ) = MLPθZ(Z, σ),
(8)"
INTERACTIVE BEHAVIOR GENERATION,0.22661122661122662,"trained by predicting the amount of noise ϵ added to the clean goal, given the corrupted goal Z + ϵ:
188"
INTERACTIVE BEHAVIOR GENERATION,0.22765072765072766,"arg min
θZ
EZEσ∼q(σ)Eϵ∼N(0,σ2I) ∥MLPθZ(Z + ϵ; σ) −ϵ∥2
2 .
(9)"
INTERACTIVE BEHAVIOR GENERATION,0.2286902286902287,"Compared to methods directly learning the multi-modal distribution [8, 25], diffusion models are
189"
INTERACTIVE BEHAVIOR GENERATION,0.22972972972972974,"easy to train and can be used to generate diverse and high-quality samples [18, 53].
190"
INTERACTIVE BEHAVIOR GENERATION,0.23076923076923078,"Path Generation with Control. To guide path generation with goals, we represent its score as
191"
INTERACTIVE BEHAVIOR GENERATION,0.23180873180873182,"s(P; σ) = ControlUNetθP(P, Z, σ),
(10)"
INTERACTIVE BEHAVIOR GENERATION,0.23284823284823286,"where the Control UNet contains two standard UNets with the same architecture [72], one performing
192"
INTERACTIVE BEHAVIOR GENERATION,0.2338877338877339,"unconditional generation taking (P, σ) as input, another injecting goal conditions densely into the
193"
INTERACTIVE BEHAVIOR GENERATION,0.23492723492723494,"neural network blocks of the first one taking (Z, σ) as inputs. Compared to concatenating the goal
194"
INTERACTIVE BEHAVIOR GENERATION,0.23596673596673598,"condition to the noise latent, this encourages close alignment between the goal and the path [62]. We
195"
INTERACTIVE BEHAVIOR GENERATION,0.23700623700623702,"apply the same architecture to control pose generation with paths,
196"
INTERACTIVE BEHAVIOR GENERATION,0.23804573804573806,"s(G; σ) = ControlUNetθG(G, P, σ).
(11)"
INTERACTIVE BEHAVIOR GENERATION,0.2390852390852391,Score map
INTERACTIVE BEHAVIOR GENERATION,0.24012474012474014,Past trajectory
INTERACTIVE BEHAVIOR GENERATION,0.24116424116424118,Observer
INTERACTIVE BEHAVIOR GENERATION,0.24220374220374222,"Sampled goals
Sampled path"
INTERACTIVE BEHAVIOR GENERATION,0.24324324324324326,Past body motion
INTERACTIVE BEHAVIOR GENERATION,0.2442827442827443,Sampled body
INTERACTIVE BEHAVIOR GENERATION,0.24532224532224534,"motion
ωo
ωp
ωs"
INTERACTIVE BEHAVIOR GENERATION,0.24636174636174638,"Encoding: Ego-perception
Decoding: Behavior Generation"
INTERACTIVE BEHAVIOR GENERATION,0.24740124740124741,"Observer
Past
Scene"
INTERACTIVE BEHAVIOR GENERATION,0.24844074844074845,Perception Code ω ∈ℝ192
INTERACTIVE BEHAVIOR GENERATION,0.2494802494802495,World-to-Ego Transform (Eq. 12)
INTERACTIVE BEHAVIOR GENERATION,0.2505197505197505,"(2ms / denoising step)
 (9ms / denoising step)
 (9ms / denoising step)"
INTERACTIVE BEHAVIOR GENERATION,0.2515592515592516,"Goal Z ∈ℝ3
Path P ∈ℝ3×T*
Body motion G ∈ℝ6B×T*
ω"
INTERACTIVE BEHAVIOR GENERATION,0.2525987525987526,"Figure 3: Pipeline for behavior generation. We first encode egocentric information into a perception
code ω and then generate full body motion in a hierarchical fashion. We start by generating goals Z
with low latency, and then generate a path P and body motion G conditioned on the previous node.
Each node is represented by the gradient of its log distribution, trained with the denoising objectives
(Eq. 9). Given G, the dense deformation of an agent can be computed via blend skinning (Eq. 3)."
INTERACTIVE BEHAVIOR GENERATION,0.25363825363825365,"Compared to concatenation, we observe better alignment between the path and the full body pose
197"
INTERACTIVE BEHAVIOR GENERATION,0.25467775467775466,"using the Control Unet.
198 .
199"
INTERACTIVE BEHAVIOR GENERATION,0.25571725571725573,"Ego-Perception Encoding. To generate plausible interactive behaviors, we encode the world
200"
INTERACTIVE BEHAVIOR GENERATION,0.25675675675675674,"egocentrically perceived by the agent, and use it to condition the behavior generation. We use the
201"
INTERACTIVE BEHAVIOR GENERATION,0.2577962577962578,"reconstructed environment T and the observer ξ as a proxy of the world, and transform them to the
202"
INTERACTIVE BEHAVIOR GENERATION,0.2588357588357588,"egocentric coordinate of the agent,
203"
INTERACTIVE BEHAVIOR GENERATION,0.2598752598752599,"ξs→a = G−1
b=0ξ,
Ts→a = G−1
b=0T
(12)"
INTERACTIVE BEHAVIOR GENERATION,0.2609147609147609,"Transforming the world to the egocentric coordinates avoids over-fitting to specific locations of the
204"
INTERACTIVE BEHAVIOR GENERATION,0.26195426195426197,"scene (Tab. 2). To encode ego-perception of the scene, we querying feature values from ψs with a 3D
205"
INTERACTIVE BEHAVIOR GENERATION,0.262993762993763,"grid around the agent and extract a latent scene representation,
206"
INTERACTIVE BEHAVIOR GENERATION,0.26403326403326405,"ωs = ResNet3Dθψ(ψs).
(13)"
INTERACTIVE BEHAVIOR GENERATION,0.26507276507276506,"where ResNet3Dθϕ is a 3D ConvNet with residual connections, and ωs ∈R64 represents the scene
207"
INTERACTIVE BEHAVIOR GENERATION,0.2661122661122661,"perceived by the agent. We encode the observer’s motion in the past T ′ = 0.8s seconds with
208"
INTERACTIVE BEHAVIOR GENERATION,0.26715176715176714,"ωo = MLPθo(ξs→a),
(14)"
INTERACTIVE BEHAVIOR GENERATION,0.2681912681912682,"where ωo ∈R64 represents the observer perceived by the agent. Accounting for the external factors
209"
INTERACTIVE BEHAVIOR GENERATION,0.2692307692307692,"from the “world” enables interactive behavior generation, where the motion of an agent follows the
210"
INTERACTIVE BEHAVIOR GENERATION,0.2702702702702703,"environment constraints and is influenced by the trajectory of the observer (Fig. 5).
211"
INTERACTIVE BEHAVIOR GENERATION,0.2713097713097713,"History Encoding. We additionally encode the past motion of the agent in T ′ seconds,
212"
INTERACTIVE BEHAVIOR GENERATION,0.27234927234927236,"ωp = MLPθp(Gs→a
b=0 ).
(15)"
INTERACTIVE BEHAVIOR GENERATION,0.2733887733887734,"By conditioning on the past motion, we can generate long sequences by chaining individual ones.
213"
EXPERIMENTS,0.27442827442827444,"4
Experiments
214"
EXPERIMENTS,0.27546777546777546,"Dataset. We collect the a dataset that emphasizes the casual interactions of an agent with their
215"
EXPERIMENTS,0.2765072765072765,"familiar environment and the observer. It contains iPhone-captured RGBD video collections of 4
216"
EXPERIMENTS,0.27754677754677753,"types of agents, including 26 videos of a cat, 3 videos of a dog, 2 videos of a bunny, and 2 videos of a
217"
EXPERIMENTS,0.2785862785862786,"human. The time span of the video capture ranges from 1 day to a month, and each video contains 30
218"
EXPERIMENTS,0.2796257796257796,"seconds to 2 minutes of content. The dataset is curated to contain diverse motion of agents, including
219"
EXPERIMENTS,0.2806652806652807,"walking, lying down, eating, as well as diverse interaction patterns with the environment, including
220"
EXPERIMENTS,0.2817047817047817,"following the camera, sitting on a coach, etc. Please refer to the supplement for more details.
221"
EXPERIMENTS,0.28274428274428276,"4.1
4D Reconstruction of Agent & Scene
222"
EXPERIMENTS,0.28378378378378377,"Implementation Details. We extract frames from the videos at 10 FPS, and use off-the-shelf models
223"
EXPERIMENTS,0.28482328482328484,"to produce augmented image measurements, including object segmentation [68], optical flow [63],
224"
EXPERIMENTS,0.28586278586278585,"Our Method
TotalRecon (Multi-video)
W/o NL
W/o FBA
W/o Annealing"
EXPERIMENTS,0.2869022869022869,"Figure 4: Comparison on multi-video scene reconstruction. We show a top-down visualization
of the reconstructed scene using the bunny dataset. Compared to TotalRecon that does not register
multiple videos, ATS produces higher-quality scene reconstruction. Neural localizer and featuremetric
losses are shown important for camera registration. Scene annealing is important for reconstructing
high-quality scenes from limited views in a video."
EXPERIMENTS,0.28794178794178793,"DINOv2 features [40]. We use AdamW to first optimize the environment with featuremetric loss for
225"
EXPERIMENTS,0.288981288981289,"30k iterations, and then jointly optimize the environment and agent for another 30k iterations with a
226"
EXPERIMENTS,0.29002079002079,"combination of optical flow, silouette, and featuremetric losses. Optimization takes roughly 24 hours.
227"
EXPERIMENTS,0.2910602910602911,"8 A100 GPUs used to optimize 26 videos (for the cat data), and 1 A100 GPU is used in a 2-3 video
228"
EXPERIMENTS,0.2920997920997921,"setup (for dog, bunny, and human data).
229"
EXPERIMENTS,0.29313929313929316,"Results. We run 4D reconstruction on all video sequences and report the results qualitatively. A visual
230"
EXPERIMENTS,0.29417879417879417,"comparison on scene registration is shown in Fig. 2. Without the ability to register multiple videos,
231"
EXPERIMENTS,0.29521829521829523,"TotalRecon produces protruded and misaligned structures (as pointed by the red arrow). In contrast,
232"
EXPERIMENTS,0.29625779625779625,"our method reconstructs a single coherent scene. With featuremetric alignment (FBA) alone but
233"
EXPERIMENTS,0.2972972972972973,"without a good camera initialization from neural localization (NL), our method produces inaccurate
234"
EXPERIMENTS,0.2983367983367983,"reconstruction due to global misalignment in cameras poses. Removing FBA while keeping NL,
235"
EXPERIMENTS,0.2993762993762994,"the method fails to accurately localize the cameras and produces noisy scene structures. Finally,
236"
EXPERIMENTS,0.3004158004158004,"removing scene annealing procures lower quality scene structures due to lack of training views. A
237"
EXPERIMENTS,0.30145530145530147,"visual comparison with TotalRecon (Single Video) is shown in Fig. 8, where we show that multiple
238"
EXPERIMENTS,0.3024948024948025,"videos helps reconstructing a higher-quality agent, and a more complete scene.
239"
INTERACTIVE BEHAVIOR PREDICTION,0.30353430353430355,"4.2
Interactive Behavior Prediction
240"
INTERACTIVE BEHAVIOR PREDICTION,0.30457380457380456,"Dataset. We use the cat dataset for quantitative evaluation, where the data are split into a training set
241"
INTERACTIVE BEHAVIOR PREDICTION,0.30561330561330563,"of 22 videos and a validation set of 4 videos. The validation set is representative of three dominant
242"
INTERACTIVE BEHAVIOR PREDICTION,0.30665280665280664,"motion patterns of the agent: (1) trying to engage with the observer, (2) exploring the space and (3)
243"
INTERACTIVE BEHAVIOR PREDICTION,0.3076923076923077,"performing activities while not paying attention to the observer.
244"
INTERACTIVE BEHAVIOR PREDICTION,0.3087318087318087,"Implementation Details. To train the behavior model, we slice the reconstructed trajectory in
245"
INTERACTIVE BEHAVIOR PREDICTION,0.3097713097713098,"the training set into overlapping window of 6.4s, resulting in 12k data samples. We use AdamW
246"
INTERACTIVE BEHAVIOR PREDICTION,0.3108108108108108,"to optimize the parameters of the scores functions {θZ, θP, θG} and the ego-perception encoders
247"
INTERACTIVE BEHAVIOR PREDICTION,0.31185031185031187,"{θψ, θo, θp} for 120k steps with batch size 1024. Training takes 10 hours on a single A100 GPU.
248"
INTERACTIVE BEHAVIOR PREDICTION,0.3128898128898129,"Metrics. The behavior of an agent can be evaluated along multiple axes, and we focus on goal, path,
249"
INTERACTIVE BEHAVIOR PREDICTION,0.31392931392931395,"and body motion prediction. For goal prediction, we use a combination of displacement error (DE)
250"
INTERACTIVE BEHAVIOR PREDICTION,0.31496881496881496,"and minimum displacement error (minDE) [7]. The evaluation asks the model to produce K=64
251"
INTERACTIVE BEHAVIOR PREDICTION,0.316008316008316,"samples. DE computes the avarage distance of the samples to the ground-truth, and minDE finds the
252"
INTERACTIVE BEHAVIOR PREDICTION,0.31704781704781704,"one closest to the ground-truth to compute the distance. For path and body motion prediction, we
253"
INTERACTIVE BEHAVIOR PREDICTION,0.3180873180873181,"use average displacement error (ADE) and minimum average displacement error (minADE), which
254"
INTERACTIVE BEHAVIOR PREDICTION,0.3191268191268191,"are similar to goal prediction, but additionally averages the distance over path and joint locations
255"
INTERACTIVE BEHAVIOR PREDICTION,0.3201663201663202,"before taking the min. When evaluating path prediction and body motion prediction, the output is
256"
INTERACTIVE BEHAVIOR PREDICTION,0.3212058212058212,"conditioned on the ground-truth goal and path respectively.
257"
INTERACTIVE BEHAVIOR PREDICTION,0.32224532224532226,"Comparisons. We re-purpose related methods and adapt them to our new setup of interactive
258"
INTERACTIVE BEHAVIOR PREDICTION,0.3232848232848233,"behavior prediction of animal agents. The quantitative results are shown in Tab. 2. To predict the goal
259"
INTERACTIVE BEHAVIOR PREDICTION,0.32432432432432434,"of an agent, classic methods build statistical models of how likely an agent visits a spatial location of
260"
INTERACTIVE BEHAVIOR PREDICTION,0.32536382536382535,"the scene, referred to as location prior [26, 76]. Given the extracted 3D trajectories of an agent in the
261"
INTERACTIVE BEHAVIOR PREDICTION,0.3264033264033264,"egocentric coordinate, we build a 3D preference map over 3D locations as a histogram, which can
262"
INTERACTIVE BEHAVIOR PREDICTION,0.32744282744282743,"be turned into probabilities and used to sample goals. Since this method does not take into account
263"
INTERACTIVE BEHAVIOR PREDICTION,0.3284823284823285,"Table 2: Evaluation of interactive behavior prediction. We separately evaluate goal, path, and full
body motion prediction. Metrics are displacement errors (DE) in meters and the lower the better.
FaF [33] is re-purposed and re-trained with our data."
INTERACTIVE BEHAVIOR PREDICTION,0.3295218295218295,"Method
Goal: minDE
Goal: DE
Path: minADE
Path: ADE
Body: minADE
Body: ADE"
INTERACTIVE BEHAVIOR PREDICTION,0.3305613305613306,"Location prior [76]
0.575
2.134
N.A.
N.A.
N.A.
N.A.
FaF [33]
N.A.
1.200
N.A.
0.057
N.A.
0.265
ATS (Ours)
0.395
1.299
0.006
0.007
0.226
0.234"
INTERACTIVE BEHAVIOR PREDICTION,0.3316008316008316,"w/o observer ωo
0.525
1.586
0.006
0.007
0.225
0.234
w/o scene ωs
0.702
1.058
0.006
0.007
0.225
0.234
w/o egocentric
0.639
1.424
0.025
0.034
0.212
0.222"
INTERACTIVE BEHAVIOR PREDICTION,0.33264033264033266,"{User, Past, Environment}
{Past, Environment}
{Environment}
Unconditional"
INTERACTIVE BEHAVIOR PREDICTION,0.33367983367983367,Infeasible region
INTERACTIVE BEHAVIOR PREDICTION,0.33471933471933474,"(e.g., gap; 
underground)"
INTERACTIVE BEHAVIOR PREDICTION,0.33575883575883575,User trajectory
INTERACTIVE BEHAVIOR PREDICTION,0.3367983367983368,Past trajectory
INTERACTIVE BEHAVIOR PREDICTION,0.33783783783783783,Sampled goals
INTERACTIVE BEHAVIOR PREDICTION,0.3388773388773389,Frontal view
INTERACTIVE BEHAVIOR PREDICTION,0.3399168399168399,Bird’s eye view
INTERACTIVE BEHAVIOR PREDICTION,0.340956340956341,"Figure 5: Analysis of conditioning signals. We show results of removing one conditioning signal
at a time. Removing observer conditioning and past trajectory conditioning makes the sampled
goals more spread out (e.g., regions both in front of the agent and behind the agent); removing the
environment conditioning introduces infeasible goals that penetrate the ground and the walls."
INTERACTIVE BEHAVIOR PREDICTION,0.341995841995842,"of the scene and the observer, it fails to accurately predict the goal. We then re-purpose FaF [33]
264"
INTERACTIVE BEHAVIOR PREDICTION,0.34303534303534305,"(Fast-and-Furious), a data-driven approach for motion forecasting to our task. FaF takes the same
265"
INTERACTIVE BEHAVIOR PREDICTION,0.34407484407484407,"input as ATS but regresses the goal, path, and body poses. It produces worse results than ATS for
266"
INTERACTIVE BEHAVIOR PREDICTION,0.34511434511434513,"all metrics since directly regressing the target treats the underlying distribution as a unit-variance
267"
INTERACTIVE BEHAVIOR PREDICTION,0.34615384615384615,"Gaussian and fails to account for the multi-modal nature of agent behaviors.
268"
INTERACTIVE BEHAVIOR PREDICTION,0.3471933471933472,"Analysing Interactions. We analyse the agent’s interactions with the environment and the observer
269"
INTERACTIVE BEHAVIOR PREDICTION,0.3482328482328482,"by removing the conditioning signals and study their influence on behavior prediction. In Fig. 5, we
270"
INTERACTIVE BEHAVIOR PREDICTION,0.3492723492723493,"show that by gradually removing conditional signals, the generated goal samples become more spread
271"
INTERACTIVE BEHAVIOR PREDICTION,0.3503118503118503,"out. In Tab. 2, we drop one of the conditioning signals at a time. Dropping the observer conditioning
272"
INTERACTIVE BEHAVIOR PREDICTION,0.35135135135135137,"increases the error in goal prediction, indicating observer’s trajectory is helpful goal prediction.
273"
INTERACTIVE BEHAVIOR PREDICTION,0.3523908523908524,"Dropping the environment conditioning produces worse results on goal prediction (minDE: 0.395 vs
274"
INTERACTIVE BEHAVIOR PREDICTION,0.35343035343035345,"0.702) as well. Surprisingly, it does not affect path prediction. We posit that the scenarios in the test
275"
INTERACTIVE BEHAVIOR PREDICTION,0.35446985446985446,"set are too simple. Conditioned on ground-turth goals, it performs well even without environment
276"
INTERACTIVE BEHAVIOR PREDICTION,0.35550935550935553,"conditioning. Finally learning behavior generation in the world coordinates performs worse for all
277"
INTERACTIVE BEHAVIOR PREDICTION,0.35654885654885654,"metrics since it over-fits to specific locations in the scene.
278"
CONCLUSION,0.3575883575883576,"5
Conclusion
279"
CONCLUSION,0.3586278586278586,"We have presented a framework for learning interactive behavior of agents grounded in natural
280"
CONCLUSION,0.3596673596673597,"environments. To achieve this, we turn multiple casually-captured video recordings into complete 4D
281"
CONCLUSION,0.3607068607068607,"reconstructions including the agent, the environment, and the observer. Such data collected over a
282"
CONCLUSION,0.36174636174636177,"long time period allows us to learn a behavior model of the agent that is reactive to the observer and
283"
CONCLUSION,0.3627858627858628,"respects the environment constraints. We validate our design choices on casual video collections, and
284"
CONCLUSION,0.36382536382536385,"show better results than prior work for 4D reconstruction and interactive behavior prediction.
285"
REFERENCES,0.36486486486486486,"References
286"
REFERENCES,0.3659043659043659,"[1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm:
287"
REFERENCES,0.36694386694386694,"Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on
288"
REFERENCES,0.367983367983368,"computer vision and pattern recognition, pages 961–971, 2016.
289"
REFERENCES,0.369022869022869,"[2] A. Bajcsy, A. Loquercio, A. Kumar, and J. Malik. Learning vision-based pursuit-evasion robot
290"
REFERENCES,0.3700623700623701,"policies. arXiv preprint arXiv:2308.16185, 2023.
291"
REFERENCES,0.3711018711018711,"[3] M. E. Banani, A. Raj, K.-K. Maninis, A. Kar, Y. Li, M. Rubinstein, D. Sun, L. Guibas,
292"
REFERENCES,0.37214137214137216,"J. Johnson, and V. Jampani. Probing the 3d awareness of visual foundation models. arXiv
293"
REFERENCES,0.3731808731808732,"preprint arXiv:2404.08636, 2024.
294"
REFERENCES,0.37422037422037424,"[4] E. Brachmann and C. Rother. Neural- Guided RANSAC: Learning where to sample model
295"
REFERENCES,0.37525987525987525,"hypotheses. In ICCV, 2019.
296"
REFERENCES,0.3762993762993763,"[5] E. Brachmann, T. Cavallari, and V. A. Prisacariu. Accelerated coordinate encoding: Learning to
297"
REFERENCES,0.37733887733887733,"relocalize in minutes using rgb and poses. In CVPR, 2023.
298"
REFERENCES,0.3783783783783784,"[6] Z. Cao, H. Gao, K. Mangalam, Q.-Z. Cai, M. Vo, and J. Malik. Long-term human motion
299"
REFERENCES,0.3794178794178794,"prediction with scene context. In Computer Vision–ECCV 2020: 16th European Conference,
300"
REFERENCES,0.3804573804573805,"Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 387–404. Springer, 2020.
301"
REFERENCES,0.3814968814968815,"[7] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov. Multipath: Multiple probabilistic anchor
302"
REFERENCES,0.38253638253638256,"trajectory hypotheses for behavior prediction. arXiv preprint arXiv:1910.05449, 2019.
303"
REFERENCES,0.38357588357588357,"[8] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation.
304"
REFERENCES,0.38461538461538464,"arXiv preprint arXiv:1410.8516, 2014.
305"
REFERENCES,0.38565488565488565,"[9] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai, B. Sapp, C. R. Qi, Y. Zhou,
306"
REFERENCES,0.3866943866943867,"et al. Large scale interactive motion forecasting for autonomous driving: The waymo open
307"
REFERENCES,0.3877338877338877,"motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
308"
REFERENCES,0.3887733887733888,"pages 9710–9719, 2021.
309"
REFERENCES,0.3898128898128898,"[10] H. Gao, R. Li, S. Tulsiani, B. Russell, and A. Kanazawa. Monocular dynamic view synthesis:
310"
REFERENCES,0.3908523908523909,"A reality check. Advances in Neural Information Processing Systems, 35:33768–33780, 2022.
311"
REFERENCES,0.3918918918918919,"[11] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa*, and J. Malik*. Humans in 4D: Recon-
312"
REFERENCES,0.39293139293139295,"structing and tracking humans with transformers. In ICCV, 2023.
313"
REFERENCES,0.39397089397089397,"[12] C. Guo, T. Jiang, X. Chen, J. Song, and O. Hilliges. Vid2Avatar: 3D Avatar Reconstruction
314"
REFERENCES,0.39501039501039503,"from Videos in the Wild via Self-supervised Scene Decomposition. CVPR, 2023.
315"
REFERENCES,0.39604989604989604,"[13] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of
316"
REFERENCES,0.3970893970893971,"minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100–107,
317"
REFERENCES,0.3981288981288981,"1968.
318"
REFERENCES,0.3991683991683992,"[14] M. Hassan, D. Ceylan, R. Villegas, J. Saito, J. Yang, Y. Zhou, and M. J. Black. Stochastic
319"
REFERENCES,0.4002079002079002,"scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on
320"
REFERENCES,0.40124740124740127,"Computer Vision, pages 11374–11384, 2021.
321"
REFERENCES,0.4022869022869023,"[15] M. Hassan, Y. Guo, T. Wang, M. Black, S. Fidler, and X. B. Peng. Synthesizing physical
322"
REFERENCES,0.40332640332640335,"character-scene interactions. arXiv preprint arXiv:2302.00883, 2023.
323"
REFERENCES,0.40436590436590436,"[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR,
324"
REFERENCES,0.40540540540540543,"pages 770–778, 2016.
325"
REFERENCES,0.40644490644490644,"[17] D. Helbing and P. Molnar. Social force model for pedestrian dynamics. Physical review E, 51
326"
REFERENCES,0.4074844074844075,"(5):4282, 1995.
327"
REFERENCES,0.4085239085239085,"[18] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural
328"
REFERENCES,0.4095634095634096,"information processing systems, 33:6840–6851, 2020.
329"
REFERENCES,0.4106029106029106,"[19] D. Q. Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical
330"
REFERENCES,0.41164241164241167,"Imaging and Vision, 35:155–164, 2009.
331"
REFERENCES,0.4126819126819127,"[20] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov, et al. Motiondiffuser: Con-
332"
REFERENCES,0.41372141372141374,"trollable multi-agent motion prediction using diffusion. In Proceedings of the IEEE/CVF
333"
REFERENCES,0.41476091476091476,"Conference on Computer Vision and Pattern Recognition, pages 9644–9653, 2023.
334"
REFERENCES,0.4158004158004158,"[21] H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T. Godisart, B. Nabbe, I. Matthews,
335"
REFERENCES,0.41683991683991684,"et al. Panoptic studio: A massively multiview system for social interaction capture. TPAMI, 41
336"
REFERENCES,0.4178794178794179,"(1):190–204, 2017.
337"
REFERENCES,0.4189189189189189,"[22] L. Kavan, S. Collins, J. Žára, and C. O’Sullivan. Skinning with dual quaternions. In Proceedings
338"
REFERENCES,0.41995841995842,"of the 2007 symposium on Interactive 3D graphics and games, pages 39–46, 2007.
339"
REFERENCES,0.420997920997921,"[23] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3d gaussian splatting for real-time
340"
REFERENCES,0.42203742203742206,"radiance field rendering. ACM Transactions on Graphics, 42(4):1–14, 2023.
341"
REFERENCES,0.4230769230769231,"[24] J. Kim, J. Kim, J. Na, and H. Joo. Parahome: Parameterizing everyday home activities towards
342"
REFERENCES,0.42411642411642414,"3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232, 2024.
343"
REFERENCES,0.42515592515592515,"[25] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
344"
REFERENCES,0.4261954261954262,"2013.
345"
REFERENCES,0.42723492723492723,"[26] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert. Activity forecasting. In Computer
346"
REFERENCES,0.4282744282744283,"Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October
347"
REFERENCES,0.4293139293139293,"7-13, 2012, Proceedings, Part IV 12, pages 201–214. Springer, 2012.
348"
REFERENCES,0.4303534303534304,"[27] M. Kocabas, N. Athanasiou, and M. J. Black. Vibe: Video inference for human body pose and
349"
REFERENCES,0.4313929313929314,"shape estimation. In CVPR, June 2020.
350"
REFERENCES,0.43243243243243246,"[28] M. Kocabas, Y. Yuan, P. Molchanov, Y. Guo, M. J. Black, O. Hilliges, J. Kautz, and
351"
REFERENCES,0.43347193347193347,"U. Iqbal. Pace: Human and camera motion estimation from in-the-wild videos. arXiv preprint
352"
REFERENCES,0.43451143451143454,"arXiv:2310.13768, 2023.
353"
REFERENCES,0.43555093555093555,"[29] J. Lee and H. Joo. Locomotion-action-manipulation: Synthesizing human-scene interactions
354"
REFERENCES,0.4365904365904366,"in complex 3d environments. In Proceedings of the IEEE/CVF International Conference on
355"
REFERENCES,0.4376299376299376,"Computer Vision (ICCV), 2023.
356"
REFERENCES,0.4386694386694387,"[30] A. Lerner, Y. Chrysanthou, and D. Lischinski. Crowds by example. In Computer graphics
357"
REFERENCES,0.4397089397089397,"forum, volume 26, pages 655–664. Wiley Online Library, 2007.
358"
REFERENCES,0.4407484407484408,"[31] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Martín-Martín, C. Wang, G. Levine,
359"
REFERENCES,0.4417879417879418,"W. Ai, B. Martinez, et al. Behavior-1k: A human-centered, embodied ai benchmark with 1,000
360"
REFERENCES,0.44282744282744285,"everyday activities and realistic simulation. arXiv preprint arXiv:2403.09227, 2024.
361"
REFERENCES,0.44386694386694386,"[32] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: A skinned
362"
REFERENCES,0.44490644490644493,"multi-person linear model. SIGGRAPH Asia, 2015.
363"
REFERENCES,0.44594594594594594,"[33] W. Luo, B. Yang, and R. Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking
364"
REFERENCES,0.446985446985447,"and motion forecasting with a single convolutional net. In Proceedings of the IEEE conference
365"
REFERENCES,0.448024948024948,"on Computer Vision and Pattern Recognition, pages 3569–3577, 2018.
366"
REFERENCES,0.4490644490644491,"[34] W.-C. Ma, D.-A. Huang, N. Lee, and K. M. Kitani. Forecasting interactive dynamics of
367"
REFERENCES,0.4501039501039501,"pedestrians with fictitious play. In Proceedings of the IEEE Conference on Computer Vision
368"
REFERENCES,0.45114345114345117,"and Pattern Recognition, pages 774–782, 2017.
369"
REFERENCES,0.4521829521829522,"[35] T. Magnenat, R. Laperrière, and D. Thalmann. Joint-dependent local deformations for hand
370"
REFERENCES,0.45322245322245325,"animation and object grasping. In Proceedings of Graphics Interface’88, pages 26–33. Canadian
371"
REFERENCES,0.45426195426195426,"Inf. Process. Soc, 1988.
372"
REFERENCES,0.4553014553014553,"[36] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass: Archive of
373"
REFERENCES,0.45634095634095634,"motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on
374"
REFERENCES,0.4573804573804574,"computer vision, pages 5442–5451, 2019.
375"
REFERENCES,0.4584199584199584,"[37] K. Mangalam, Y. An, H. Girase, and J. Malik. From goals, waypoints & paths to long term
376"
REFERENCES,0.4594594594594595,"human trajectory forecasting. In Proceedings of the IEEE/CVF International Conference on
377"
REFERENCES,0.4604989604989605,"Computer Vision, pages 15233–15242, 2021.
378"
REFERENCES,0.46153846153846156,"[38] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:
379"
REFERENCES,0.4625779625779626,"Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.
380"
REFERENCES,0.46361746361746364,"[39] M. Niemeyer and A. Geiger. Giraffe: Representing scenes as compositional generative neural
381"
REFERENCES,0.46465696465696466,"feature fields. In CVPR, pages 11453–11464, 2021.
382"
REFERENCES,0.4656964656964657,"[40] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,
383"
REFERENCES,0.46673596673596673,"D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li,
384"
REFERENCES,0.4677754677754678,"W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal,
385"
REFERENCES,0.4688149688149688,"P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without
386"
REFERENCES,0.4698544698544699,"supervision, 2023.
387"
REFERENCES,0.4708939708939709,"[41] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:
388"
REFERENCES,0.47193347193347196,"Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium
389"
REFERENCES,0.47297297297297297,"on User Interface Software and Technology, pages 1–22, 2023.
390"
REFERENCES,0.47401247401247404,"[42] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla.
391"
REFERENCES,0.47505197505197505,"Nerfies: Deformable neural radiance fields. In ICCV, 2021.
392"
REFERENCES,0.4760914760914761,"[43] G. Pavlakos, E. Weber, M. Tancik, and A. Kanazawa. The one where they reconstructed 3d
393"
REFERENCES,0.47713097713097713,"humans and environments in tv shows. In European Conference on Computer Vision, pages
394"
REFERENCES,0.4781704781704782,"732–749. Springer, 2022.
395"
REFERENCES,0.4792099792099792,"[44] S. Pellegrini, A. Ess, K. Schindler, and L. Van Gool. You’ll never walk alone: Modeling social
396"
REFERENCES,0.4802494802494803,"behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer
397"
REFERENCES,0.4812889812889813,"vision, pages 261–268. IEEE, 2009.
398"
REFERENCES,0.48232848232848236,"[45] X. Puig, E. Undersander, A. Szot, M. D. Cote, T.-Y. Yang, R. Partsey, R. Desai, A. Clegg,
399"
REFERENCES,0.48336798336798337,"M. Hlavac, S. Y. Min, et al. Habitat 3.0: A co-habitat for humans, avatars, and robots. In The
400"
REFERENCES,0.48440748440748443,"Twelfth International Conference on Learning Representations, 2023.
401"
REFERENCES,0.48544698544698545,"[46] D. Rempe, Z. Luo, X. Bin Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler, and O. Litany. Trace
402"
REFERENCES,0.4864864864864865,"and pace: Controllable pedestrian animation via guided trajectory diffusion. In Proceedings of
403"
REFERENCES,0.4875259875259875,"the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13756–13766,
404"
REFERENCES,0.4885654885654886,"2023.
405"
REFERENCES,0.4896049896049896,"[47] N. Rhinehart and K. M. Kitani. Learning action maps of large environments via first-person
406"
REFERENCES,0.49064449064449067,"vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
407"
REFERENCES,0.4916839916839917,"pages 580–588, 2016.
408"
REFERENCES,0.49272349272349275,"[48] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone. Trajectron++: Dynamically-feasible
409"
REFERENCES,0.49376299376299376,"trajectory forecasting with heterogeneous data. In Computer Vision–ECCV 2020: 16th European
410"
REFERENCES,0.49480249480249483,"Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII 16, pages 683–700.
411"
REFERENCES,0.49584199584199584,"Springer, 2020.
412"
REFERENCES,0.4968814968814969,"[49] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk. From coarse to fine: Robust hierarchical
413"
REFERENCES,0.4979209979209979,"localization at large scale. In Proceedings of the IEEE/CVF Conference on Computer Vision
414"
REFERENCES,0.498960498960499,"and Pattern Recognition, pages 12716–12725, 2019.
415"
REFERENCES,0.5,"[50] A. Seff, B. Cera, D. Chen, M. Ng, A. Zhou, N. Nayakanti, K. S. Refaat, R. Al-Rfou, and
416"
REFERENCES,0.501039501039501,"B. Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of
417"
REFERENCES,0.502079002079002,"the IEEE/CVF International Conference on Computer Vision, pages 8579–8590, 2023.
418"
REFERENCES,0.5031185031185031,"[51] N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world from internet photo collections.
419"
REFERENCES,0.5041580041580042,"IJCV, 2008.
420"
REFERENCES,0.5051975051975052,"[52] C. Song, G. Yang, K. Deng, J.-Y. Zhu, and D. Ramanan. Total-recon: Deformable scene
421"
REFERENCES,0.5062370062370062,"reconstruction for embodied view synthesis. In ICCV, 2023.
422"
REFERENCES,0.5072765072765073,"[53] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based
423"
REFERENCES,0.5083160083160083,"generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,
424"
REFERENCES,0.5093555093555093,"2020.
425"
REFERENCES,0.5103950103950103,"[54] S. Srivastava, C. Li, M. Lingelbach, R. Martín-Martín, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen,
426"
REFERENCES,0.5114345114345115,"S. Buch, K. Liu, et al. Behavior: Benchmark for everyday household activities in virtual,
427"
REFERENCES,0.5124740124740125,"interactive, and ecological environments. In Conference on robot learning, pages 477–490.
428"
REFERENCES,0.5135135135135135,"PMLR, 2022.
429"
REFERENCES,0.5145530145530145,"[55] T. Sun, Y. Hao, S. Huang, S. Savarese, K. Schindler, M. Pollefeys, and I. Armeni. Nothing
430"
REFERENCES,0.5155925155925156,"stands still: A spatiotemporal benchmark on 3d point cloud registration under large geometric
431"
REFERENCES,0.5166320166320166,"and temporal change. arXiv preprint arXiv:2311.09346, 2023.
432"
REFERENCES,0.5176715176715176,"[56] R. Szeliski and S. B. Kang. Shape ambiguities in structure from motion. IEEE Transactions on
433"
REFERENCES,0.5187110187110187,"Pattern Analysis and Machine Intelligence, 19(5):506–512, 1997.
434"
REFERENCES,0.5197505197505198,"[57] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-Or, and A. H. Bermano. Human motion
435"
REFERENCES,0.5207900207900208,"diffusion model. arXiv preprint arXiv:2209.14916, 2022.
436"
REFERENCES,0.5218295218295218,"[58] J. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha. Reciprocal n-body collision avoidance.
437"
REFERENCES,0.5228690228690228,"In Robotics Research: The 14th International Symposium ISRR, pages 3–19. Springer, 2011.
438"
REFERENCES,0.5239085239085239,"[59] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman. Hu-
439"
REFERENCES,0.524948024948025,"mannerf: Free-viewpoint rendering of moving people from monocular video. In CVPR, pages
440"
REFERENCES,0.525987525987526,"16210–16220, 2022.
441"
REFERENCES,0.527027027027027,"[60] R. Wu, B. Mildenhall, P. Henzler, K. Park, R. Gao, D. Watson, P. P. Srinivasan, D. Verbin, J. T.
442"
REFERENCES,0.5280665280665281,"Barron, B. Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. arXiv preprint
443"
REFERENCES,0.5291060291060291,"arXiv:2312.02981, 2023.
444"
REFERENCES,0.5301455301455301,"[61] S. Wu, T. Jakab, C. Rupprecht, and A. Vedaldi. Dove: Learning deformable 3d objects by
445"
REFERENCES,0.5311850311850311,"watching videos. arXiv preprint arXiv:2107.10844, 2021.
446"
REFERENCES,0.5322245322245323,"[62] Y. Xie, V. Jampani, L. Zhong, D. Sun, and H. Jiang. Omnicontrol: Control any joint at any time
447"
REFERENCES,0.5332640332640333,"for human motion generation. arXiv preprint arXiv:2310.08580, 2023.
448"
REFERENCES,0.5343035343035343,"[63] G. Yang and D. Ramanan. Volumetric correspondence networks for optical flow. In NeurIPS,
449"
REFERENCES,0.5353430353430353,"2019.
450"
REFERENCES,0.5363825363825364,"[64] G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, H. Chang, D. Ramanan, W. T. Freeman, and
451"
REFERENCES,0.5374220374220374,"C. Liu. LASR: Learning articulated shape reconstruction from a monocular video. In CVPR,
452"
REFERENCES,0.5384615384615384,"2021.
453"
REFERENCES,0.5395010395010394,"[65] G. Yang, M. Vo, N. Natalia, D. Ramanan, A. Vedaldi, and H. Joo. Banmo: Building animatable
454"
REFERENCES,0.5405405405405406,"3d neural models from many casual videos. In CVPR, 2022.
455"
REFERENCES,0.5415800415800416,"[66] G. Yang, C. Wang, N. D. Reddy, and D. Ramanan. Reconstructing Animatable Categories from
456"
REFERENCES,0.5426195426195426,"Videos. CVPR, 2023.
457"
REFERENCES,0.5436590436590436,"[67] G. Yang, S. Yang, J. Z. Zhang, Z. Manchester, and D. Ramanan. Physically plausible recon-
458"
REFERENCES,0.5446985446985447,"struction from monocular videos. In ICCV, 2023.
459"
REFERENCES,0.5457380457380457,"[68] J. Yang, M. Gao, Z. Li, S. Gao, F. Wang, and F. Zheng. Track anything: Segment anything
460"
REFERENCES,0.5467775467775468,"meets videos, 2023.
461"
REFERENCES,0.5478170478170478,"[69] V. Ye, G. Pavlakos, J. Malik, and A. Kanazawa. Decoupling human and camera motion from
462"
REFERENCES,0.5488565488565489,"videos in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and
463"
REFERENCES,0.5498960498960499,"Pattern Recognition, pages 21222–21232, 2023.
464"
REFERENCES,0.5509355509355509,"[70] Y. Yuan, U. Iqbal, P. Molchanov, K. Kitani, and J. Kautz. Glamr: Global occlusion-aware
465"
REFERENCES,0.5519750519750519,"human mesh recovery with dynamic cameras. In Proceedings of the IEEE/CVF conference on
466"
REFERENCES,0.553014553014553,"computer vision and pattern recognition, pages 11038–11049, 2022.
467"
REFERENCES,0.5540540540540541,"[71] Y. Yuan, J. Song, U. Iqbal, A. Vahdat, and J. Kautz. Physdiff: Physics-guided human motion
468"
REFERENCES,0.5550935550935551,"diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer
469"
REFERENCES,0.5561330561330561,"Vision, pages 16010–16021, 2023.
470"
REFERENCES,0.5571725571725572,"[72] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion
471"
REFERENCES,0.5582120582120582,"models, 2023.
472"
REFERENCES,0.5592515592515592,"[73] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang. Synthesizing diverse human motions in 3d
473"
REFERENCES,0.5602910602910602,"indoor scenes. arXiv preprint arXiv:2305.12411, 2023.
474"
REFERENCES,0.5613305613305614,"[74] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone. Guided
475"
REFERENCES,0.5623700623700624,"conditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference
476"
REFERENCES,0.5634095634095634,"on Robotics and Automation (ICRA), pages 3560–3566. IEEE, 2023.
477"
REFERENCES,0.5644490644490644,"[75] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforce-
478"
REFERENCES,0.5654885654885655,"ment learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.
479"
REFERENCES,0.5665280665280665,"[76] B. D. Ziebart, N. Ratliff, G. Gallagher, C. Mertz, K. Peterson, J. A. Bagnell, M. Hebert,
480"
REFERENCES,0.5675675675675675,"A. K. Dey, and S. Srinivasa. Planning-based prediction for pedestrians. In 2009 IEEE/RSJ
481"
REFERENCES,0.5686070686070686,"International Conference on Intelligent Robots and Systems, pages 3931–3936. IEEE, 2009.
482"
REFERENCES,0.5696465696465697,"A
Additional Implementation Details
483"
REFERENCES,0.5706860706860707,"Model Architecture. The score function of the goal is implemented as 6-layer MLP with hidden
484"
REFERENCES,0.5717255717255717,"size 128. The the score functions of the paths and body motions are implemented as 1D UNets
485"
REFERENCES,0.5727650727650727,"taken from MDM [57]. The sampling frequency is set to be 0.1s, resulting a sequence length of 56.
486"
REFERENCES,0.5738045738045738,"The environment encoder is implemented as a 6-layer 3D ConvNet with kernel size 3 and channel
487"
REFERENCES,0.5748440748440748,"dimension 128. The observer encoder and history encoder are implemented as a 3-layer MLP with
488"
REFERENCES,0.5758835758835759,"hidden size 128.
489"
REFERENCES,0.5769230769230769,"We use a linear noise schedule at training time and 50 denoising steps. At test time, each goal
490"
REFERENCES,0.577962577962578,"denoising step takes 2ms and each path/body denoising step takes 9ms on a GeForce RTX 3090 GPU.
491"
REFERENCES,0.579002079002079,"Data Collection. We collect RGBD videos using an iPhone, similar to TotalRecon [52]. To train
492"
REFERENCES,0.58004158004158,"the neural localizer, we use Polycam to take the walkthrough video and extract a textured mesh. For
493"
REFERENCES,0.581081081081081,"behavior capture, we use Record3D App to record videos and extract color images and depth images.
494"
REFERENCES,0.5821205821205822,"B
Additional Results
495"
REFERENCES,0.5831600831600832,"Histogram of Agent / Observer Visitation. We show final camera and agent registration to the
496"
REFERENCES,0.5841995841995842,"canonical scene in Fig. 6. The registered 3D trajectories provides statistics of agent’s and user’s
497"
REFERENCES,0.5852390852390852,"preference over the environment.
498"
REFERENCES,0.5862785862785863,Agent trajectories
REFERENCES,0.5873180873180873,"Agent preference (visitation)
User preference (visitation)"
REFERENCES,0.5883575883575883,User trajectories
REFERENCES,0.5893970893970893,"Low                         High
Low                         High"
REFERENCES,0.5904365904365905,Color: shot id
REFERENCES,0.5914760914760915,"Figure 6: Given the 3D trajectories of the agent and the user accumulated over time (top), one could
compute their preference represented by 3D heatmaps (bottom). Note the high agent preference over
table and sofa."
REFERENCES,0.5925155925155925,"Varying Observer’s Motion. We find that various interactive behaviors can be generated by
499"
REFERENCES,0.5935550935550935,"conditioning the model on different observer motion. The results are shown in Fig. 7.
500"
REFERENCES,0.5945945945945946,"Comparison to TotalRecon. In the main paper, we compare to TotalRecon on scene reconstruction
501"
REFERENCES,0.5956340956340956,"by providing it multiple videos. Here, we include additional comparison in their the original single
502"
REFERENCES,0.5966735966735967,"video setup. We find that TotalRecon fails to build a good agent model, or a complete scene model
503"
REFERENCES,0.5977130977130977,"given limited observations, while our method can leverage multiple videos as inputs to build a better
504"
REFERENCES,0.5987525987525988,"agent and scene model. The results are shown in Fig. 8.
505"
REFERENCES,0.5997920997920998,User trajectory Goals
REFERENCES,0.6008316008316008,Planned Paths
REFERENCES,0.6018711018711018,Past trajectory Goals Goals Goals
REFERENCES,0.6029106029106029,Planned Paths
REFERENCES,0.603950103950104,Planned Paths
REFERENCES,0.604989604989605,Planned Paths
REFERENCES,0.606029106029106,User trajectory
REFERENCES,0.6070686070686071,User trajectory
REFERENCES,0.6081081081081081,User trajectory
REFERENCES,0.6091476091476091,Early                   Late
REFERENCES,0.6101871101871101,"Figure 7: Interactive behavior simulation with user conditioning. By changing the trajectory of the
user, one could influence the behavior of the agent. Given different control inputs, the agent may
follow the user or run away from the user."
REFERENCES,0.6112266112266113,TotalRecon
REFERENCES,0.6122661122661123,Reference image
REFERENCES,0.6133056133056133,Distortion
REFERENCES,0.6143451143451143,"Incomplete
No distortion"
REFERENCES,0.6153846153846154,Complete
REFERENCES,0.6164241164241164,"Complete shape
Good alignment
Missing limbs
Misaligned limbs Ours"
REFERENCES,0.6174636174636174,"Figure 8: Qualitative comparison with TotalRecon [52] on 4D reconstruction. Top: reconstruction
of the agent at at specific frame. Total-recon produces shapes with missing limbs and bone trans-
formations that are misaligned with the shape, while our method produces complete shapes and
good alignment. Bottom: reconstruction of the environment. TotalRecon produces distorted and
incomplete geometry (due to lack of observations from a single video), while our method produces
an accurate and complete environment reconstruction."
REFERENCES,0.6185031185031185,"C
Limitations and Future Works
506"
REFERENCES,0.6195426195426196,"High-level Behavior. The current ATS model is trained with time-horizon of T ∗= 6.4 seconds.
507"
REFERENCES,0.6205821205821206,"We observe that the model only learns mid-level behaviors of an agent (e.g., trying to move to a
508"
REFERENCES,0.6216216216216216,"destination; staying at a location; walking around). We hope incorporating a memory module and
509"
REFERENCES,0.6226611226611226,"training with longer time horizon will enable learning higher-level behaviors of an agent.
510"
REFERENCES,0.6237006237006237,"Scaling-up. As indicated by the experimental results, the goals sampled from ATS may fail to cover
511"
REFERENCES,0.6247401247401247,"the actual goal when evaluated on the (unseen) test data. This raises safety concerns when using
512"
REFERENCES,0.6257796257796258,"ATS for the prediction task (e.g., predicting the behavior of pedestrains in autonomous driving). One
513"
REFERENCES,0.6268191268191268,"potential solution of improving the generalization ability is to collect more diverse behavior data
514"
REFERENCES,0.6278586278586279,"from in the wild videos, or leverage “large” video priors trained on internet-scale videos.
515"
REFERENCES,0.6288981288981289,"Multiple Agents. We show results of learning behavior models of a single agent, but our method for
516"
REFERENCES,0.6299376299376299,"4D reconstruction and interactive goal-driven behavior modeling is not limited to a single agent. We
517"
REFERENCES,0.6309771309771309,"leave learning multi-agent behavior simulation from videos as future work.
518"
REFERENCES,0.632016632016632,"Physical Interactions. Our method reconstructs and generates the kinematics of an agent, which
519"
REFERENCES,0.6330561330561331,"may produce physically-implausible results (e.g., penetration with the ground and foot sliding). One
520"
REFERENCES,0.6340956340956341,"promising way to deal with this problem is to add physics constraints to the reconstruction and motion
521"
REFERENCES,0.6351351351351351,"generation [67, 71].
522"
REFERENCES,0.6361746361746362,"Environment Reconstruction. To build a complete reconstruction of the environment, we register
523"
REFERENCES,0.6372141372141372,"multiple videos to a shared canonical space. However, the transient structures (e.g., cushion that
524"
REFERENCES,0.6382536382536382,"can be moved over time) may not be reconstructed well due to lack of observations. One potential
525"
REFERENCES,0.6392931392931392,"solution of reconstructing these transient structures is to combine generative image priors with the
526"
REFERENCES,0.6403326403326404,"reconstruction pipeline [60].
527"
REFERENCES,0.6413721413721414,"D
Social Impact
528"
REFERENCES,0.6424116424116424,"Our method is able to learn interactive behavior from videos, which could help build simulators for
529"
REFERENCES,0.6434511434511434,"autonomous driving, gaming, and movie applications. It is also capable of building personalized
530"
REFERENCES,0.6444906444906445,"behavior models from casually collected video data, which can benefit users who do not have access
531"
REFERENCES,0.6455301455301455,"to a motion capture studio. On the negative side, the behavior generation model could be used as
532"
REFERENCES,0.6465696465696466,"“deepfake” and poses threats to user’s privacy and social security.
533"
REFERENCES,0.6476091476091476,"NeurIPS Paper Checklist
534"
REFERENCES,0.6486486486486487,"The checklist is designed to encourage best practices for responsible machine learning research,
535"
REFERENCES,0.6496881496881497,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
536"
REFERENCES,0.6507276507276507,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
537"
REFERENCES,0.6517671517671517,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
538"
REFERENCES,0.6528066528066528,"towards the page limit.
539"
REFERENCES,0.6538461538461539,"Please read the checklist guidelines carefully for information on how to answer these questions. For
540"
REFERENCES,0.6548856548856549,"each question in the checklist:
541"
REFERENCES,0.6559251559251559,"• You should answer [Yes] , [No] , or [NA] .
542"
REFERENCES,0.656964656964657,"• [NA] means either that the question is Not Applicable for that particular paper or the
543"
REFERENCES,0.658004158004158,"relevant information is Not Available.
544"
REFERENCES,0.659043659043659,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
545"
REFERENCES,0.66008316008316,"The checklist answers are an integral part of your paper submission. They are visible to the
546"
REFERENCES,0.6611226611226612,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
547"
REFERENCES,0.6621621621621622,"(after eventual revisions) with the final version of your paper, and its final version will be published
548"
REFERENCES,0.6632016632016632,"with the paper.
549"
REFERENCES,0.6642411642411642,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
550"
REFERENCES,0.6652806652806653,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
551"
REFERENCES,0.6663201663201663,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
552"
REFERENCES,0.6673596673596673,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
553"
REFERENCES,0.6683991683991684,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
554"
REFERENCES,0.6694386694386695,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
555"
REFERENCES,0.6704781704781705,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
556"
REFERENCES,0.6715176715176715,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
557"
REFERENCES,0.6725571725571725,"please point to the section(s) where related material for the question can be found.
558"
REFERENCES,0.6735966735966736,"IMPORTANT, please:
559"
REFERENCES,0.6746361746361746,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
560"
REFERENCES,0.6756756756756757,"• Keep the checklist subsection headings, questions/answers and guidelines below.
561"
REFERENCES,0.6767151767151767,"• Do not modify the questions and only use the provided macros for your answers.
562"
CLAIMS,0.6777546777546778,"1. Claims
563"
CLAIMS,0.6787941787941788,"Question: Do the main claims made in the abstract and introduction accurately reflect the
564"
CLAIMS,0.6798336798336798,"paper’s contributions and scope?
565"
CLAIMS,0.6808731808731808,"Answer: [Yes]
566"
CLAIMS,0.681912681912682,"Justification: The main claims made in the abstract and introduction accurately reflect the
567"
CLAIMS,0.682952182952183,"paper’s contributions and scope.
568"
CLAIMS,0.683991683991684,"Guidelines:
569"
CLAIMS,0.685031185031185,"• The answer NA means that the abstract and introduction do not include the claims
570"
CLAIMS,0.6860706860706861,"made in the paper.
571"
CLAIMS,0.6871101871101871,"• The abstract and/or introduction should clearly state the claims made, including the
572"
CLAIMS,0.6881496881496881,"contributions made in the paper and important assumptions and limitations. A No or
573"
CLAIMS,0.6891891891891891,"NA answer to this question will not be perceived well by the reviewers.
574"
CLAIMS,0.6902286902286903,"• The claims made should match theoretical and experimental results, and reflect how
575"
CLAIMS,0.6912681912681913,"much the results can be expected to generalize to other settings.
576"
CLAIMS,0.6923076923076923,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
577"
CLAIMS,0.6933471933471933,"are not attained by the paper.
578"
LIMITATIONS,0.6943866943866944,"2. Limitations
579"
LIMITATIONS,0.6954261954261954,"Question: Does the paper discuss the limitations of the work performed by the authors?
580"
LIMITATIONS,0.6964656964656964,"Answer: [Yes]
581"
LIMITATIONS,0.6975051975051975,"Justification: The paper discusses the limitations of the work performed by the authors.
582"
LIMITATIONS,0.6985446985446986,"Guidelines:
583"
LIMITATIONS,0.6995841995841996,"• The answer NA means that the paper has no limitation while the answer No means that
584"
LIMITATIONS,0.7006237006237006,"the paper has limitations, but those are not discussed in the paper.
585"
LIMITATIONS,0.7016632016632016,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
586"
LIMITATIONS,0.7027027027027027,"• The paper should point out any strong assumptions and how robust the results are to
587"
LIMITATIONS,0.7037422037422038,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
588"
LIMITATIONS,0.7047817047817048,"model well-specification, asymptotic approximations only holding locally). The authors
589"
LIMITATIONS,0.7058212058212058,"should reflect on how these assumptions might be violated in practice and what the
590"
LIMITATIONS,0.7068607068607069,"implications would be.
591"
LIMITATIONS,0.7079002079002079,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
592"
LIMITATIONS,0.7089397089397089,"only tested on a few datasets or with a few runs. In general, empirical results often
593"
LIMITATIONS,0.7099792099792099,"depend on implicit assumptions, which should be articulated.
594"
LIMITATIONS,0.7110187110187111,"• The authors should reflect on the factors that influence the performance of the approach.
595"
LIMITATIONS,0.7120582120582121,"For example, a facial recognition algorithm may perform poorly when image resolution
596"
LIMITATIONS,0.7130977130977131,"is low or images are taken in low lighting. Or a speech-to-text system might not be
597"
LIMITATIONS,0.7141372141372141,"used reliably to provide closed captions for online lectures because it fails to handle
598"
LIMITATIONS,0.7151767151767152,"technical jargon.
599"
LIMITATIONS,0.7162162162162162,"• The authors should discuss the computational efficiency of the proposed algorithms
600"
LIMITATIONS,0.7172557172557172,"and how they scale with dataset size.
601"
LIMITATIONS,0.7182952182952183,"• If applicable, the authors should discuss possible limitations of their approach to
602"
LIMITATIONS,0.7193347193347194,"address problems of privacy and fairness.
603"
LIMITATIONS,0.7203742203742204,"• While the authors might fear that complete honesty about limitations might be used by
604"
LIMITATIONS,0.7214137214137214,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
605"
LIMITATIONS,0.7224532224532224,"limitations that aren’t acknowledged in the paper. The authors should use their best
606"
LIMITATIONS,0.7234927234927235,"judgment and recognize that individual actions in favor of transparency play an impor-
607"
LIMITATIONS,0.7245322245322245,"tant role in developing norms that preserve the integrity of the community. Reviewers
608"
LIMITATIONS,0.7255717255717256,"will be specifically instructed to not penalize honesty concerning limitations.
609"
THEORY ASSUMPTIONS AND PROOFS,0.7266112266112266,"3. Theory Assumptions and Proofs
610"
THEORY ASSUMPTIONS AND PROOFS,0.7276507276507277,"Question: For each theoretical result, does the paper provide the full set of assumptions and
611"
THEORY ASSUMPTIONS AND PROOFS,0.7286902286902287,"a complete (and correct) proof?
612"
THEORY ASSUMPTIONS AND PROOFS,0.7297297297297297,"Answer: [NA]
613"
THEORY ASSUMPTIONS AND PROOFS,0.7307692307692307,"Justification: The paper does not include theoretical results.
614"
THEORY ASSUMPTIONS AND PROOFS,0.7318087318087318,"Guidelines:
615"
THEORY ASSUMPTIONS AND PROOFS,0.7328482328482329,"• The answer NA means that the paper does not include theoretical results.
616"
THEORY ASSUMPTIONS AND PROOFS,0.7338877338877339,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
617"
THEORY ASSUMPTIONS AND PROOFS,0.7349272349272349,"referenced.
618"
THEORY ASSUMPTIONS AND PROOFS,0.735966735966736,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
619"
THEORY ASSUMPTIONS AND PROOFS,0.737006237006237,"• The proofs can either appear in the main paper or the supplemental material, but if
620"
THEORY ASSUMPTIONS AND PROOFS,0.738045738045738,"they appear in the supplemental material, the authors are encouraged to provide a short
621"
THEORY ASSUMPTIONS AND PROOFS,0.739085239085239,"proof sketch to provide intuition.
622"
THEORY ASSUMPTIONS AND PROOFS,0.7401247401247402,"• Inversely, any informal proof provided in the core of the paper should be complemented
623"
THEORY ASSUMPTIONS AND PROOFS,0.7411642411642412,"by formal proofs provided in appendix or supplemental material.
624"
THEORY ASSUMPTIONS AND PROOFS,0.7422037422037422,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432432432432432,"4. Experimental Result Reproducibility
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7442827442827443,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7453222453222453,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7463617463617463,"of the paper (regardless of whether the code and data are provided or not)?
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7474012474012474,"Answer: [Yes]
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7484407484407485,"Justification: The authors tried their best to disclose the information needed to reproduce
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494802494802495,"the experiments.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505197505197505,"Guidelines:
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7515592515592515,"• The answer NA means that the paper does not include experiments.
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7525987525987526,"• If the paper includes experiments, a No answer to this question will not be perceived
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7536382536382537,"well by the reviewers: Making the paper reproducible is important, regardless of
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7546777546777547,"whether the code and data are provided or not.
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557172557172557,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567567567567568,"to make their results reproducible or verifiable.
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577962577962578,"• Depending on the contribution, reproducibility can be accomplished in various ways.
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7588357588357588,"For example, if the contribution is a novel architecture, describing the architecture fully
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7598752598752598,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.760914760914761,"be necessary to either make it possible for others to replicate the model with the same
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761954261954262,"dataset, or provide access to the model. In general. releasing code and data is often
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.762993762993763,"one good way to accomplish this, but reproducibility can also be provided via detailed
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.764033264033264,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7650727650727651,"of a large language model), releasing of a model checkpoint, or other means that are
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7661122661122661,"appropriate to the research performed.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7671517671517671,"• While NeurIPS does not require releasing code, the conference does require all submis-
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7681912681912682,"sions to provide some reasonable avenue for reproducibility, which may depend on the
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7692307692307693,"nature of the contribution. For example
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702702702702703,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713097713097713,"to reproduce that algorithm.
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7723492723492723,"(b) If the contribution is primarily a new model architecture, the paper should describe
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7733887733887734,"the architecture clearly and fully.
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744282744282744,"(c) If the contribution is a new model (e.g., a large language model), then there should
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7754677754677755,"either be a way to access this model for reproducing the results or a way to reproduce
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7765072765072765,"the model (e.g., with an open-source dataset or instructions for how to construct
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775467775467776,"the dataset).
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785862785862786,"(d) We recognize that reproducibility may be tricky in some cases, in which case
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796257796257796,"authors are welcome to describe the particular way they provide for reproducibility.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7806652806652806,"In the case of closed-source models, it may be that access to the model is limited in
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7817047817047817,"some way (e.g., to registered users), but it should be possible for other researchers
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7827442827442828,"to have some path to reproducing or verifying the results.
664"
OPEN ACCESS TO DATA AND CODE,0.7837837837837838,"5. Open access to data and code
665"
OPEN ACCESS TO DATA AND CODE,0.7848232848232848,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
666"
OPEN ACCESS TO DATA AND CODE,0.7858627858627859,"tions to faithfully reproduce the main experimental results, as described in supplemental
667"
OPEN ACCESS TO DATA AND CODE,0.7869022869022869,"material?
668"
OPEN ACCESS TO DATA AND CODE,0.7879417879417879,"Answer: [No]
669"
OPEN ACCESS TO DATA AND CODE,0.7889812889812889,"Justification: The code will be released once we put it in a better shape.
670"
OPEN ACCESS TO DATA AND CODE,0.7900207900207901,"Guidelines:
671"
OPEN ACCESS TO DATA AND CODE,0.7910602910602911,"• The answer NA means that paper does not include experiments requiring code.
672"
OPEN ACCESS TO DATA AND CODE,0.7920997920997921,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
673"
OPEN ACCESS TO DATA AND CODE,0.7931392931392931,"public/guides/CodeSubmissionPolicy) for more details.
674"
OPEN ACCESS TO DATA AND CODE,0.7941787941787942,"• While we encourage the release of code and data, we understand that this might not be
675"
OPEN ACCESS TO DATA AND CODE,0.7952182952182952,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
676"
OPEN ACCESS TO DATA AND CODE,0.7962577962577962,"including code, unless this is central to the contribution (e.g., for a new open-source
677"
OPEN ACCESS TO DATA AND CODE,0.7972972972972973,"benchmark).
678"
OPEN ACCESS TO DATA AND CODE,0.7983367983367984,"• The instructions should contain the exact command and environment needed to run to
679"
OPEN ACCESS TO DATA AND CODE,0.7993762993762994,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
680"
OPEN ACCESS TO DATA AND CODE,0.8004158004158004,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
681"
OPEN ACCESS TO DATA AND CODE,0.8014553014553014,"• The authors should provide instructions on data access and preparation, including how
682"
OPEN ACCESS TO DATA AND CODE,0.8024948024948025,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
683"
OPEN ACCESS TO DATA AND CODE,0.8035343035343036,"• The authors should provide scripts to reproduce all experimental results for the new
684"
OPEN ACCESS TO DATA AND CODE,0.8045738045738046,"proposed method and baselines. If only a subset of experiments are reproducible, they
685"
OPEN ACCESS TO DATA AND CODE,0.8056133056133056,"should state which ones are omitted from the script and why.
686"
OPEN ACCESS TO DATA AND CODE,0.8066528066528067,"• At submission time, to preserve anonymity, the authors should release anonymized
687"
OPEN ACCESS TO DATA AND CODE,0.8076923076923077,"versions (if applicable).
688"
OPEN ACCESS TO DATA AND CODE,0.8087318087318087,"• Providing as much information as possible in supplemental material (appended to the
689"
OPEN ACCESS TO DATA AND CODE,0.8097713097713097,"paper) is recommended, but including URLs to data and code is permitted.
690"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"6. Experimental Setting/Details
691"
OPEN ACCESS TO DATA AND CODE,0.8118503118503119,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
692"
OPEN ACCESS TO DATA AND CODE,0.8128898128898129,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
693"
OPEN ACCESS TO DATA AND CODE,0.8139293139293139,"results?
694"
OPEN ACCESS TO DATA AND CODE,0.814968814968815,"Answer: [Yes]
695"
OPEN ACCESS TO DATA AND CODE,0.816008316008316,"Justification: The authors tried their best to specify all the training and test details.
696"
OPEN ACCESS TO DATA AND CODE,0.817047817047817,"Guidelines:
697"
OPEN ACCESS TO DATA AND CODE,0.818087318087318,"• The answer NA means that the paper does not include experiments.
698"
OPEN ACCESS TO DATA AND CODE,0.8191268191268192,"• The experimental setting should be presented in the core of the paper to a level of detail
699"
OPEN ACCESS TO DATA AND CODE,0.8201663201663202,"that is necessary to appreciate the results and make sense of them.
700"
OPEN ACCESS TO DATA AND CODE,0.8212058212058212,"• The full details can be provided either with the code, in appendix, or as supplemental
701"
OPEN ACCESS TO DATA AND CODE,0.8222453222453222,"material.
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232848232848233,"7. Experiment Statistical Significance
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8243243243243243,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253638253638254,"information about the statistical significance of the experiments?
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264033264033264,"Answer: [No]
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8274428274428275,"Justification: The results currently do not have error bars, but we will try adding them later.
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8284823284823285,"Based on empirical evidence of running the experiments, we think it will not affect the
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295218295218295,"conclusion.
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8305613305613305,"Guidelines:
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316008316008316,"• The answer NA means that the paper does not include experiments.
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326403326403327,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336798336798337,"dence intervals, or statistical significance tests, at least for the experiments that support
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8347193347193347,"the main claims of the paper.
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8357588357588358,"• The factors of variability that the error bars are capturing should be clearly stated (for
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367983367983368,"example, train/test split, initialization, random drawing of some parameter, or overall
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378378378378378,"run with given experimental conditions).
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388773388773388,"• The method for calculating the error bars should be explained (closed form formula,
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.83991683991684,"call to a library function, bootstrap, etc.)
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840956340956341,"• The assumptions made should be given (e.g., Normally distributed errors).
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.841995841995842,"• It should be clear whether the error bar is the standard deviation or the standard error
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.843035343035343,"of the mean.
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8440748440748441,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451143451143451,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8461538461538461,"of Normality of errors is not verified.
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8471933471933472,"• For asymmetric distributions, the authors should be careful not to show in tables or
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8482328482328483,"figures symmetric error bars that would yield results that are out of range (e.g. negative
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8492723492723493,"error rates).
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8503118503118503,"• If error bars are reported in tables or plots, The authors should explain in the text how
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8513513513513513,"they were calculated and reference the corresponding figures or tables in the text.
730"
EXPERIMENTS COMPUTE RESOURCES,0.8523908523908524,"8. Experiments Compute Resources
731"
EXPERIMENTS COMPUTE RESOURCES,0.8534303534303534,"Question: For each experiment, does the paper provide sufficient information on the com-
732"
EXPERIMENTS COMPUTE RESOURCES,0.8544698544698545,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
733"
EXPERIMENTS COMPUTE RESOURCES,0.8555093555093555,"the experiments?
734"
EXPERIMENTS COMPUTE RESOURCES,0.8565488565488566,"Answer: [Yes]
735"
EXPERIMENTS COMPUTE RESOURCES,0.8575883575883576,"Justification: The paper provides information about computer resources.
736"
EXPERIMENTS COMPUTE RESOURCES,0.8586278586278586,"Guidelines:
737"
EXPERIMENTS COMPUTE RESOURCES,0.8596673596673596,"• The answer NA means that the paper does not include experiments.
738"
EXPERIMENTS COMPUTE RESOURCES,0.8607068607068608,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
739"
EXPERIMENTS COMPUTE RESOURCES,0.8617463617463618,"or cloud provider, including relevant memory and storage.
740"
EXPERIMENTS COMPUTE RESOURCES,0.8627858627858628,"• The paper should provide the amount of compute required for each of the individual
741"
EXPERIMENTS COMPUTE RESOURCES,0.8638253638253638,"experimental runs as well as estimate the total compute.
742"
EXPERIMENTS COMPUTE RESOURCES,0.8648648648648649,"• The paper should disclose whether the full research project required more compute
743"
EXPERIMENTS COMPUTE RESOURCES,0.8659043659043659,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
744"
EXPERIMENTS COMPUTE RESOURCES,0.8669438669438669,"didn’t make it into the paper).
745"
CODE OF ETHICS,0.867983367983368,"9. Code Of Ethics
746"
CODE OF ETHICS,0.8690228690228691,"Question: Does the research conducted in the paper conform, in every respect, with the
747"
CODE OF ETHICS,0.8700623700623701,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
748"
CODE OF ETHICS,0.8711018711018711,"Answer: [Yes]
749"
CODE OF ETHICS,0.8721413721413721,"Justification: The authors have reviewed the code of ethics and think the paper follows the
750"
CODE OF ETHICS,0.8731808731808732,"guideline.
751"
CODE OF ETHICS,0.8742203742203742,"Guidelines:
752"
CODE OF ETHICS,0.8752598752598753,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
753"
CODE OF ETHICS,0.8762993762993763,"• If the authors answer No, they should explain the special circumstances that require a
754"
CODE OF ETHICS,0.8773388773388774,"deviation from the Code of Ethics.
755"
CODE OF ETHICS,0.8783783783783784,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
756"
CODE OF ETHICS,0.8794178794178794,"eration due to laws or regulations in their jurisdiction).
757"
BROADER IMPACTS,0.8804573804573804,"10. Broader Impacts
758"
BROADER IMPACTS,0.8814968814968815,"Question: Does the paper discuss both potential positive societal impacts and negative
759"
BROADER IMPACTS,0.8825363825363826,"societal impacts of the work performed?
760"
BROADER IMPACTS,0.8835758835758836,"Answer: [Yes]
761"
BROADER IMPACTS,0.8846153846153846,"Justification: The paper discussed potential positive and negative impact.
762"
BROADER IMPACTS,0.8856548856548857,"Guidelines:
763"
BROADER IMPACTS,0.8866943866943867,"• The answer NA means that there is no societal impact of the work performed.
764"
BROADER IMPACTS,0.8877338877338877,"• If the authors answer NA or No, they should explain why their work has no societal
765"
BROADER IMPACTS,0.8887733887733887,"impact or why the paper does not address societal impact.
766"
BROADER IMPACTS,0.8898128898128899,"• Examples of negative societal impacts include potential malicious or unintended uses
767"
BROADER IMPACTS,0.8908523908523909,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
768"
BROADER IMPACTS,0.8918918918918919,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
769"
BROADER IMPACTS,0.8929313929313929,"groups), privacy considerations, and security considerations.
770"
BROADER IMPACTS,0.893970893970894,"• The conference expects that many papers will be foundational research and not tied
771"
BROADER IMPACTS,0.895010395010395,"to particular applications, let alone deployments. However, if there is a direct path to
772"
BROADER IMPACTS,0.896049896049896,"any negative applications, the authors should point it out. For example, it is legitimate
773"
BROADER IMPACTS,0.8970893970893971,"to point out that an improvement in the quality of generative models could be used to
774"
BROADER IMPACTS,0.8981288981288982,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
775"
BROADER IMPACTS,0.8991683991683992,"that a generic algorithm for optimizing neural networks could enable people to train
776"
BROADER IMPACTS,0.9002079002079002,"models that generate Deepfakes faster.
777"
BROADER IMPACTS,0.9012474012474012,"• The authors should consider possible harms that could arise when the technology is
778"
BROADER IMPACTS,0.9022869022869023,"being used as intended and functioning correctly, harms that could arise when the
779"
BROADER IMPACTS,0.9033264033264033,"technology is being used as intended but gives incorrect results, and harms following
780"
BROADER IMPACTS,0.9043659043659044,"from (intentional or unintentional) misuse of the technology.
781"
BROADER IMPACTS,0.9054054054054054,"• If there are negative societal impacts, the authors could also discuss possible mitigation
782"
BROADER IMPACTS,0.9064449064449065,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
783"
BROADER IMPACTS,0.9074844074844075,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
784"
BROADER IMPACTS,0.9085239085239085,"feedback over time, improving the efficiency and accessibility of ML).
785"
SAFEGUARDS,0.9095634095634095,"11. Safeguards
786"
SAFEGUARDS,0.9106029106029107,"Question: Does the paper describe safeguards that have been put in place for responsible
787"
SAFEGUARDS,0.9116424116424117,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
788"
SAFEGUARDS,0.9126819126819127,"image generators, or scraped datasets)?
789"
SAFEGUARDS,0.9137214137214137,"Answer: [NA]
790"
SAFEGUARDS,0.9147609147609148,"Justification: The paper poses no such risks.
791"
SAFEGUARDS,0.9158004158004158,"Guidelines:
792"
SAFEGUARDS,0.9168399168399168,"• The answer NA means that the paper poses no such risks.
793"
SAFEGUARDS,0.9178794178794178,"• Released models that have a high risk for misuse or dual-use should be released with
794"
SAFEGUARDS,0.918918918918919,"necessary safeguards to allow for controlled use of the model, for example by requiring
795"
SAFEGUARDS,0.91995841995842,"that users adhere to usage guidelines or restrictions to access the model or implementing
796"
SAFEGUARDS,0.920997920997921,"safety filters.
797"
SAFEGUARDS,0.922037422037422,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
798"
SAFEGUARDS,0.9230769230769231,"should describe how they avoided releasing unsafe images.
799"
SAFEGUARDS,0.9241164241164241,"• We recognize that providing effective safeguards is challenging, and many papers do
800"
SAFEGUARDS,0.9251559251559252,"not require this, but we encourage authors to take this into account and make a best
801"
SAFEGUARDS,0.9261954261954262,"faith effort.
802"
LICENSES FOR EXISTING ASSETS,0.9272349272349273,"12. Licenses for existing assets
803"
LICENSES FOR EXISTING ASSETS,0.9282744282744283,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
804"
LICENSES FOR EXISTING ASSETS,0.9293139293139293,"the paper, properly credited and are the license and terms of use explicitly mentioned and
805"
LICENSES FOR EXISTING ASSETS,0.9303534303534303,"properly respected?
806"
LICENSES FOR EXISTING ASSETS,0.9313929313929314,"Answer: [NA]
807"
LICENSES FOR EXISTING ASSETS,0.9324324324324325,"Justification: Thee paper does not use existing assets.
808"
LICENSES FOR EXISTING ASSETS,0.9334719334719335,"Guidelines:
809"
LICENSES FOR EXISTING ASSETS,0.9345114345114345,"• The answer NA means that the paper does not use existing assets.
810"
LICENSES FOR EXISTING ASSETS,0.9355509355509356,"• The authors should cite the original paper that produced the code package or dataset.
811"
LICENSES FOR EXISTING ASSETS,0.9365904365904366,"• The authors should state which version of the asset is used and, if possible, include a
812"
LICENSES FOR EXISTING ASSETS,0.9376299376299376,"URL.
813"
LICENSES FOR EXISTING ASSETS,0.9386694386694386,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
814"
LICENSES FOR EXISTING ASSETS,0.9397089397089398,"• For scraped data from a particular source (e.g., website), the copyright and terms of
815"
LICENSES FOR EXISTING ASSETS,0.9407484407484408,"service of that source should be provided.
816"
LICENSES FOR EXISTING ASSETS,0.9417879417879418,"• If assets are released, the license, copyright information, and terms of use in the
817"
LICENSES FOR EXISTING ASSETS,0.9428274428274428,"package should be provided. For popular datasets, paperswithcode.com/datasets
818"
LICENSES FOR EXISTING ASSETS,0.9438669438669439,"has curated licenses for some datasets. Their licensing guide can help determine the
819"
LICENSES FOR EXISTING ASSETS,0.9449064449064449,"license of a dataset.
820"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"• For existing datasets that are re-packaged, both the original license and the license of
821"
LICENSES FOR EXISTING ASSETS,0.946985446985447,"the derived asset (if it has changed) should be provided.
822"
LICENSES FOR EXISTING ASSETS,0.9480249480249481,"• If this information is not available online, the authors are encouraged to reach out to
823"
LICENSES FOR EXISTING ASSETS,0.9490644490644491,"the asset’s creators.
824"
NEW ASSETS,0.9501039501039501,"13. New Assets
825"
NEW ASSETS,0.9511434511434511,"Question: Are new assets introduced in the paper well documented and is the documentation
826"
NEW ASSETS,0.9521829521829522,"provided alongside the assets?
827"
NEW ASSETS,0.9532224532224532,"Answer: [Yes]
828"
NEW ASSETS,0.9542619542619543,"Justification: The paper discussed the new assets.
829"
NEW ASSETS,0.9553014553014553,"Guidelines:
830"
NEW ASSETS,0.9563409563409564,"• The answer NA means that the paper does not release new assets.
831"
NEW ASSETS,0.9573804573804574,"• Researchers should communicate the details of the dataset/code/model as part of their
832"
NEW ASSETS,0.9584199584199584,"submissions via structured templates. This includes details about training, license,
833"
NEW ASSETS,0.9594594594594594,"limitations, etc.
834"
NEW ASSETS,0.9604989604989606,"• The paper should discuss whether and how consent was obtained from people whose
835"
NEW ASSETS,0.9615384615384616,"asset is used.
836"
NEW ASSETS,0.9625779625779626,"• At submission time, remember to anonymize your assets (if applicable). You can either
837"
NEW ASSETS,0.9636174636174636,"create an anonymized URL or include an anonymized zip file.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9646569646569647,"14. Crowdsourcing and Research with Human Subjects
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656964656964657,"Question: For crowdsourcing experiments and research with human subjects, does the paper
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9667359667359667,"include the full text of instructions given to participants and screenshots, if applicable, as
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9677754677754677,"well as details about compensation (if any)?
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688149688149689,"Answer: [NA]
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9698544698544699,"Justification: The paper does not deal with crowdsourcing or external human subjects.
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708939708939709,"Guidelines:
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719334719334719,"• The answer NA means that the paper does not involve crowdsourcing nor research with
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"human subjects.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974012474012474,"• Including this information in the supplemental material is fine, but if the main contribu-
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975051975051975,"tion of the paper involves human subjects, then as much detail as possible should be
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760914760914761,"included in the main paper.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771309771309772,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781704781704782,"or other labor should be paid at least the minimum wage in the country of the data
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792099792099792,"collector.
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802494802494802,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9812889812889813,"Subjects
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9823284823284824,"Question: Does the paper describe potential risks incurred by study participants, whether
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833679833679834,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844074844074844,"approvals (or an equivalent approval/review based on the requirements of your country or
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854469854469855,"institution) were obtained?
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864864864864865,"Answer: [NA]
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875259875259875,"Justification: The paper does not deal with crowdsourcing or external human subjects.
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9885654885654885,"Guidelines:
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896049896049897,"• The answer NA means that the paper does not involve crowdsourcing nor research with
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9906444906444907,"human subjects.
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916839916839917,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927234927234927,"may be required for any human subjects research. If you obtained IRB approval, you
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937629937629938,"should clearly state this in the paper.
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948024948024948,"• We recognize that the procedures for this may vary significantly between institutions
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958419958419958,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968814968814969,"guidelines for their institution.
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997920997920998,"• For initial submissions, do not include any information that would break anonymity (if
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998960498960499,"applicable), such as the institution conducting the review.
872"
