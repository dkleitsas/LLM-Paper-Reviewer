Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003134796238244514,"Lifelong machine learning (LML) is a well-known paradigm mimicking the hu-
man learning process by utilizing experiences from previous tasks. Nevertheless,
an issue that has been rarely addressed is the lack of labels at the individual task
level. The state-of-the-art of LML largely addresses supervised learning, with a
few semi-supervised continual learning exceptions which require training addi-
tional models, which in turn impose constraints on the LML methods themselves.
Therefore, we propose Mako, a wrapper tool that mounts on top of supervised
LML frameworks, leveraging data programming. Mako imposes no additional
knowledge base overhead and enables continual semi-supervised learning with a
limited amount of labeled data. This tool achieves similar performance, in terms
of per-task accuracy and resistance to catastrophic forgetting, as compared to fully
labeled data. We ran extensive experiments on LML task sequences created from
standard image classiﬁcation data sets including MNIST, CIFAR-10 and CIFAR-
100, and the results show that after utilizing Mako to leverage unlabeled data,
LML tools are able to achieve 97% performance of supervised learning on fully la-
beled data in terms of accuracy and catastrophic forgetting prevention. Moreover,
when compared to baseline semi-supervised LML tools such as CNNL, ORDisCo
and DistillMatch, Mako signiﬁcantly outperforms them, increasing accuracy by
0.25 on certain benchmarks."
INTRODUCTION,0.006269592476489028,"1
INTRODUCTION"
INTRODUCTION,0.009404388714733543,"Since 1995, researchers have been actively seeking machine learning (ML) frameworks that can
better mimic the human learning process by retaining memories from past experiences (Thrun &
Mitchell, 1995; Liu, 2017). Hence, in contrast to traditional isolated ML where knowledge is never
accumulated, the concept of lifelong machine learning (LML) is deﬁned as training ML models for
continual learning over task sequences, with a knowledge base to store information that could be
helpful in the future. Under this deﬁnition, LML can be seen as continual transfer learning (Pan &
Yang, 2015) with memory or sequential multi-task learning (Caruana, 1997)."
INTRODUCTION,0.012539184952978056,"Nevertheless, at the level of individual tasks, the current state-of-the-art of LML still remains largely
supervised. A challenge that has rarely been mentioned is that labels can still be expensive and rarely
available for each task, such that supervised training can hardly provide acceptable performance. For
LML, performance is not only measured at task level, but also for the entire lifetime, with attributes
such as resistance to catastrophic forgetting and space-efﬁciency of knowledge base."
INTRODUCTION,0.01567398119122257,"We take a more generic approach to semi-supervised lifelong learning and develop a technique that
can wrap around any existing continual learning algorithm. Speciﬁcally, we propose to use data
programming (Ratner et al., 2016b) to label new data under the supervision of a set of weak labeling
functions trained from a limited number of labeled data points. Data programming has proven
successful for isolated learning, and so this is the ﬁrst work integrating it with lifelong learning."
INTRODUCTION,0.018808777429467086,"Our approach, called Mako, sits atop an existing lifelong learner to turn it into a semi-supervised
learner. For each task, Mako takes as input a small labeled dataset plus an unlabeled dataset with
no restriction on size, generates labels for the unlabeled data and then updates the lifelong learner
supervised by both sets. This approach builds upon the widely used Snuba algorithm (Varma & R´e,"
INTRODUCTION,0.0219435736677116,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.025078369905956112,"2016), and we show that its theoretical guarantees still hold in the lifelong setting. Mako’s procedure
leverages automatic hyperparameter tuning, data programming and conﬁdence calibration, in order
to adapt automatic labeling to constantly varying LML tasks. We show that Mako produces ade-
quately high performance, in terms of both task accuracy and prevention of catastrophic forgetting,
that approaches that of training on fully labeled data without costing extra knowledge base space
overhead."
INTRODUCTION,0.02821316614420063,"We extensively evaluated Mako on various partially labeled LML task sequences generated from
commonly used datasets including MNIST, CIFAR-10 and CIFAR-100 (LeCun & Cortes, 2010;
Krizhevsky, 2009), while mounting on a diversity of supervised LML frameworks such as Decon-
volutional Factorized CNN (DF-CNN), Dynamic Expandable Network (DEN) and Tensor Factor-
ization (TF) (Lee et al., 2019; Yoon et al., 2018; Bulat et al., 2020). We compared the results to
supervised learning on fully labeled data, as well as the same partially labeled data with existing
semi-supervised LML tools: CNNL, ORDisCo and DistillMatch (Baucum et al., 2017; Wang et al.,
2021; Smith et al., 2021). Empirically, we show that the performance of LML methods improves
as giving more training data even with Mako labels, achieving at least 97% performance relative to
LML methods trained on ground-truth labels and being able to beat existing semi-supervised LML
tools by approximately 0.25 higher accuracy."
INTRODUCTION,0.03134796238244514,Contributions Summary:
INTRODUCTION,0.034482758620689655,"1. Adapting automatic label generation by semi-supervised learning/data programming to LML in
the scenario where labeled training data is expensive to obtain compared to unlabeled for all tasks.
2. Implementing a LML wrapper, namely Mako, for the scenario of expensive labels, such that,
when given partially labeled data of limited size and compared to the current supervised state-of-
the-art with fully labeled data, each LML task sequence (1) requires signiﬁcantly fewer labeled
data to achieve high accuracy on individual tasks, (2) maintains similar resistance to catastrophic
forgetting and (3) does not cost extra knowledge base storage.
3. Extensive experiments on lifelong binary and multiclass image classiﬁcation on 3 commonly
used datasets, including comparison to supervised and semi-supervised LML baselines, accomplish-
ing very close performance to supervised LML on fully labeled data and signiﬁcantly outperforming
baseline semi-supervised LML frameworks."
RELATED WORK,0.03761755485893417,"2
RELATED WORK"
RELATED WORK,0.04075235109717868,"Lifelong machine learning (LML) is a concept of continual learning among a sequence of tasks.
Speciﬁcally, given multiple machine learning tasks arriving continuously, the LML framework ac-
cumulates the knowledge of previous tasks and utilizes it for future ones (Chen & Liu, 2016; Ruvolo
& Eaton, 2013b). The method of retaining and adopting the knowledge base varies depending on
the characteristics of tasks and learning models. For instance, a latent matrix is used to keep proto-
typical weights for linear models (Kumar & Daume, 2012; Ruvolo & Eaton, 2013a), and for deep
neural networks, either weights of layers (Yoon et al., 2018; Lu et al., 2017; Bulat et al., 2020; Lee
et al., 2019) or extracted features of layers (Rusu et al., 2016; Misra et al., 2016; Gao et al., 2019;
Schwarz et al., 2018) are considered as the knowledge base to transfer. Besides research on models
to transfer knowledge across tasks, the LML community has also been working on task order sorting
for higher overall performance (Sun et al., 2018; Ruvolo & Eaton, 2013b). Nevertheless, all these
works assume that each individual task training is supervised with ground-truth labels for the entire
training set. In many real-life ML tasks, obtaining labels has been and continues to be expensive
(Settles, 2009). Only a few works have addressed this issue on LML in very speciﬁc domains, such
as continual sentimental analysis with words partially labeled (Wang et al., 2016) and reinforcement
learning in a sequence of state spaces without explicit rewards (Finn et al., 2017). Different from
previous work, Mako enables high performance in continual learning on generic ML tasks given
partially labeled data, with the cardinality of labels restricted at a small number."
RELATED WORK,0.0438871473354232,"To tackle expensive labels in ML, researchers have purposed various methods such as active learning
(Settles, 2009) or semi-supervised learning (Olivier et al., 2006), which is an ML framework that
takes both labeled and unlabeled data as inputs. The unlabeled data can directly assist the training,
or can serve as a target data set yet to be labeled. In recent years, with the rising enthusiasm and
need in deep learning, various methods have been used to assist semi-supervised learning with DNN
models (Rasmus et al., 2015; Laine & Aila, 2016; Tarvainen & Valpola, 2017). One branch of"
RELATED WORK,0.047021943573667714,Under review as a conference paper at ICLR 2022
RELATED WORK,0.050156739811912224,"semi-supervised learning, namely data programming or weak supervision (Ratner et al., 2016b),
focuses on auto-generation of labels on an unlabeled dataset with the help of a small labeled dataset.
Targeting this problem, researchers leverage weak labeling functions that each can label with an
accuracy slightly higher than a coin ﬂip. Then, ensembling algorithms are able to bag these weak
labelers and produce highly accurate ﬁnal labels. In other words, the weak labels supervise the
training of the ﬁnal ensembled generative model. There exists different implementations of such
generative model training such as Snorkel (Ratner et al., 2016a)."
RELATED WORK,0.05329153605015674,"Semi-supervised learning is an underexplored problem in the LML setting; all the works mentioned
above have addressed problems in isolated ML only. Early work in semi-supervised incremental
learning, in which all tasks are trained simultaneously as batches of unlabeled data are incremen-
tally observed by a classiﬁer, has identiﬁed challenges pertaining to selection of the label generating
model architecture and conﬁdence calibration of the generated labels (Baucum et al., 2017), pro-
cesses which Mako undertakes autonomously through automatic search for weak labelers before en-
sembling, and conﬁdence calibration after labels are produced. Pioneering work in semi-supervised
LML requires training of additional models for out-of-distribution detection (Smith et al., 2021) or
generative replay (Wang et al., 2021). Conversely, by focusing on label generation through data pro-
gramming, our approach achieves similar behavior through a pre-processing workﬂow that does not
impose additional constraints on the LML approach or require additional knowledge base storage."
RELATED WORK,0.05642633228840126,"For data programming tools, it has been a long standing issue to obtain weak labeling functions
as input. One approach is manually designing the weak labelers by domain experts (Ratner et al.,
2016b;a), while other researchers seek automatic labeler generation. For instance, Snuba (Varma
& R´e, 2016) outputs weak supervisors with an accuracy guarantee on the ﬁnal ensembled data
programming model. Thus far, Snuba has been evaluated on various isolated ML tasks, with weak
labelers in form of k-nearest-neighbors, decision stumps and logistic regressors, and has been shown
to ace in natural language processing and image classiﬁcation. It also succeeds in more complex
applications such as classifying bone tumors in medical domain. In our work, we build a weak
labeling function generation module in Mako based on Snuba, aiming to support a larger variety of
labeler models for LML, while maintaining the theoretical accuracy guarantee of generated labels."
PROBLEM FORMULATION,0.05956112852664577,"3
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.06269592476489028,"LML: Given a sequence of ML tasks: T1, T2, . . . , Tm, where m is an unbounded integer, each
task Ti has an underlying data space Xi and label space Yi. Ti has ni,train labeled training data
(Xi,train, Yi,train) ∼Di, such that Xi,train ∈X ni,train
i
and Yi,train ∈Yni,train
i
, following some
task-speciﬁc i.i.d. distribution Di. The task also has ni,test testing data (Xi,test, Yi,test) drawn from
the same distribution."
PROBLEM FORMULATION,0.06583072100313479,"Traditionally, an LML model encounters the tasks in order.
At Ti, it trains on the full set of
(Xi,train, Yi,train) with supervised learning and obtain model fi : Si
j=1 Xi 7→Si
j=1 Yi. Im-
mediately after this training, it predicts the j-th task with ˆYij = fi(Xj,test) for each j ≤i. There
are two objectives:"
PROBLEM FORMULATION,0.06896551724137931,"1. High test accuracy. Formally, denote testing accuracy of task Tj right after the training of task Ti
on (Xi,train, Yi,train) as"
PROBLEM FORMULATION,0.07210031347962383,"aij =
Pnj,test
k=1
1(ˆyk = yk)
nj,test
(1)"
PROBLEM FORMULATION,0.07523510971786834,"with ˆyk and yk the k-th label in ˆYij and Yi,test, respectively. One LML objective in the current
state-of-the-art is to maximize the peak per-task accuracy up to task i deﬁned as"
PROBLEM FORMULATION,0.07836990595611286,"˜ai = 1 i i
X"
PROBLEM FORMULATION,0.08150470219435736,"j=1
ajj
(2)"
PROBLEM FORMULATION,0.08463949843260188,"Peak per task accuracy is deﬁned on all tasks i = 1, . . . , m , and shows the LML performance on
the current training task on the basis of the accumulated knowledge."
PROBLEM FORMULATION,0.0877742946708464,"2. Low catastrophic forgetting. A critical aspect of LML is to prevent catastrophic forgetting of
previous tasks. The resistance to catastrophic forgetting can be quantiﬁed by the average testing
accuracy of all tasks encountered so far. For each task Ti, denote a second objective, ﬁnal accuracy"
PROBLEM FORMULATION,0.09090909090909091,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.09404388714733543,up to task i as
PROBLEM FORMULATION,0.09717868338557993,"¯ai = 1 i i
X"
PROBLEM FORMULATION,0.10031347962382445,"j=1
aij
(3)"
PROBLEM FORMULATION,0.10344827586206896,"The second objective is to maximize ¯ai for all i = 1, . . . , m.The gap between peak per-task accuracy
and ﬁnal accuracy expresses how much knowledge of previous tasks LML method can preserve
through the streams of tasks. Therefore, we deﬁne a third metric to be maximized, catastrophic
forgetting ratio up to task i, as"
PROBLEM FORMULATION,0.10658307210031348,"ci = 1 i i
X j=1"
PROBLEM FORMULATION,0.109717868338558,"aij
ajj
(4)"
PROBLEM FORMULATION,0.11285266457680251,"This metric is less than 1 if the LML model loses its performance on the earlier tasks, and it is greater
than 1 if there is positive knowledge transfer from the later tasks to the earlier ones."
PROBLEM FORMULATION,0.11598746081504702,"Semi-supervised LML with data programming:
The key challenge we want to address is the
cost of labeling data in all LML tasks. That is, the number of available labeled training data is
small. Therefore, we would like to split (Xi,train, Yi,train) in the traditional LML problem into
disjoint (Xi,L, Yi,L, Xi,U), where (Xi,L, Yi,L) is a set of labeled training data with restriction on
size and Xi,U is a set of unlabeled data with no such restriction. In other words, |Xi,L| is small, e.g.
|Xi,L| ≤150 for a 10-way image classiﬁcation problem."
PROBLEM FORMULATION,0.11912225705329153,"Apparently, the LML model is now trained with semi-supervised learning. For task Ti, we would like
to leverage data programming on (Xi,L, Yi,L, Xi,U) to ﬁrst train a generative model πi : Xi 7→Yi.
This model automatically labels Xi,U with Y π
i,U = πi(Xi,U). Then, the LML framework will
be trained on (Xi,L, Yi,L, Xi,U, Y π
i,U). Denote the LML model immediately after this training as
f π
i : Si
j=1 Xi 7→Si
j=1 Yi, and ˆY π
ij = f π
i (Xj,test) for each task j ≤i."
PROBLEM FORMULATION,0.12225705329153605,"In this scenario, we redeﬁne testing accuracy in Equation 1 as"
PROBLEM FORMULATION,0.12539184952978055,"aπ
ij =
Pnj,test
k=1
1(ˆyπ
k = yk)
nj,test
(5)"
PROBLEM FORMULATION,0.12852664576802508,"where the only difference is that ˆyπ
k is the k-th label in ˆY π
ij instead of ˆYij. From Equation 5, we can
then redeﬁne metrics in Equations 2, 3 and 4 as"
PROBLEM FORMULATION,0.13166144200626959,"˜aπ
i = 1 i i
X"
PROBLEM FORMULATION,0.13479623824451412,"j=1
aπ
jj
(6)
¯aπ
i = 1 i i
X"
PROBLEM FORMULATION,0.13793103448275862,"j=1
aπ
ij
(7)
cπ
i = 1 i i
X j=1"
PROBLEM FORMULATION,0.14106583072100312,"aπ
ij
aπ
jj
(8)"
PROBLEM FORMULATION,0.14420062695924765,"Our research problem is to design a semi-supervised LML framework leveraging data programming
that minimizes the performance between using partially labeled data, (Xi,L, Yi,L, Xi,U, Y π
i,U), and
the upper-bound performance using fully labeled data, (Xi,L, Yi,L, Xi,U, Yi,U), in terms of these
three metrics while imposing no additional knowledge base storage overhead in the LML framework."
PROBLEM FORMULATION,0.14733542319749215,"4
MAKO: LIFELONG MACHINE LEARNING FRAMEWORK"
PROBLEM FORMULATION,0.15047021943573669,"We implemented Mako, a semi-supervised LML framework that can be mounted on top of any ex-
isting LML tool, with (Xi,L, Yi,L, Xi,U) as input for each task Ti. The automatic label generation
of Y π
i,U is done by data programming, consisting of generating weak labeling functions and ensem-
bling a generative model. Nevertheless, different from data programming on isolated ML, due to the
variation of tasks throughout LML, we need to actively adapt both the weak and the strong models
to maximize the performance at each task. This is accomplished by automatic search on architec-
ture and training hyperparameter search before weak labeler generation and conﬁdence calibration
after the ensembled model πi is trained. The following subsections explains the Mako workﬂow
step-by-step, as illustrated in Figure 1."
PROBLEM FORMULATION,0.1536050156739812,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.15673981191222572,"Repeat Until Sufficient Labelers 
Satisfying the Theoretical Guarantee"
PROBLEM FORMULATION,0.15987460815047022,"Hyperparameter Search
Data Programming
Confidence Calibration
LML Tools"
PROBLEM FORMULATION,0.16300940438871472,Testing Accuracy
PROBLEM FORMULATION,0.16614420062695925,"Architecture
Temperature Scaling"
PROBLEM FORMULATION,0.16927899686520376,Learning Rates
PROBLEM FORMULATION,0.1724137931034483,"Training 
Parameters"
PROBLEM FORMULATION,0.1755485893416928,"Batch Size
Epochs"
PROBLEM FORMULATION,0.1786833855799373,Bootstrapping
PROBLEM FORMULATION,0.18181818181818182,LF Generation
PROBLEM FORMULATION,0.18495297805642633,"DF-CNN
EWC
TF
DEN
Strong 
Labels"
PROBLEM FORMULATION,0.18808777429467086,Logits
PROBLEM FORMULATION,0.19122257053291536,Calibrated
PROBLEM FORMULATION,0.19435736677115986,"Labels
Labeler Model"
PROBLEM FORMULATION,0.1974921630094044,Ensemble
PROBLEM FORMULATION,0.2006269592476489,Thresholding
PROBLEM FORMULATION,0.20376175548589343,Figure 1: Mako Framework
AUTOMATIC HYPERPARAMETER SEARCH,0.20689655172413793,"4.1
AUTOMATIC HYPERPARAMETER SEARCH"
AUTOMATIC HYPERPARAMETER SEARCH,0.21003134796238246,"Before producing weak labeling functions, two questions must be answered: (1) How do we choose
the model architecture for our weak labeling functions? (2) How do we train these models after
architecture is decided? Formally, these questions lead to the decision of architectural and training
hyperparameters of labelers, which Mako accomplishes automatically."
AUTOMATIC HYPERPARAMETER SEARCH,0.21316614420062696,"The only prior knowledge required is a search space, manually designed, of the architecture and
training hyperparameters. For instance, given that the task is image classiﬁcation, the search space
of labeler architecture will include CNNs with the same input and output dimensions, but different
conﬁgurations of hidden layers. Similarly, the training hyperparameters will be searched on different
learning rates, batch sizes and number of epochs."
AUTOMATIC HYPERPARAMETER SEARCH,0.21630094043887146,"The automatic tuning of weak labelers therefore traverses the search space until a satisfactory con-
ﬁguration is found. The stopping criterion is deﬁned as follows: after training ten labelers with a
given conﬁguration of architecture and training hyperparameters on different bootstrapped subsets
of (Xi,L, Yi,L), 9 out of 10 achieves an accuracy on the entire (Xi,L, Yi,L) above some threshold
a. This criterion shows the generalization capability of sampled labelers and alternative criteria can
be used, such as 8 out of 10 or average accuracy ≥a. We accept such a conﬁguration because we
train the labelers on bootstrapped subsets of data during data programming, which will be further
explained in 4.2. If no conﬁguration in the search space is satisfactory, we decrease a by 0.05 and
repeat the search. Hence, we can start from a sufﬁciently high a such as 0.9. For efﬁciency, Mako
selects the conﬁguration with the smallest number of training epochs among all the acceptable ones."
AUTOMATIC HYPERPARAMETER SEARCH,0.219435736677116,"There exists a efﬁciency-automation trade-off in this hyperparameter search. The less prior knowl-
edge we have for the task, the larger search space we need to input to Mako and thus a larger time
overhead. In contrast, more manual exploration on the task will lead to a smaller search space and
therefore efﬁciency."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2225705329153605,"4.2
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET"
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.22570532915360503,"After deciding the architecture and training hyperparameters of weak labeling functions, Mako kicks
off weak labeler training and ensembling of generative model πi for each task Ti. For the training
process, we are inspired by Snuba (Varma & R´e, 2016) due to the fact that it has a theoretical
accuracy guarantee in the ﬁnal generative model. Nonetheless, in order to increase variance among
the labelers, Snuba trains them on different selected subsets of features. This procedure does little
favor in LML, where we might encounter complex tasks in the sequence that requires the entire
feature space. For instance, in image classiﬁcation, excluding pixels in training may lead to a failure
in capturing important feature-target relationships, especially when we want to take advantage of
local connectivity of CNNs. Therefore, Mako uses bootstrapping on (Xi,L, Yi,L) and trains each
labeler with a sampled subset. The bootstrapping size can be either manually given or searched as a
training hyperparameter."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.22884012539184953,"Besides bootstrapping on data, the rest of weak labeler generation process remains unchanged from
Snuba in order to maintain its theoretical guarantee. Snuba has such a guarantee because it checks
an exit condition on whether the committed labelers have labeled sufﬁcient number of data points in
Xi,L with acceptable conﬁdence. If so, these labelers guarantee that the ﬁnal generative model will
have a learned accuracy on Xi,L close to the labeling accuracy on Xi,U. Formally, this guarantee
can be stated as Proposition 1."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.23197492163009403,"Proposition 1 (Theoretical Guarantee of Snuba): Given h committed weak labelers, denote the
empirical accuracies of all committed weak labelers on Xi,L as a vector ai,L,w. The generative"
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.23510971786833856,Under review as a conference paper at ICLR 2022
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.23824451410658307,"model training outputs learned accuracies of these labelers on Xi,L as ai,L,g before ensembling.
Moreover, the unknown true accuracies of the labelers on Xi,U after generative model training are
denoted as ai,U,g, with ai,L,w, ai,L,g, ai,U,g ∈Rh. We have a measured error ||ai,L,w −ai,L,g||∞≤
ϵ. Then, if each labeler labels a minimum of"
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2413793103448276,"d ≥
1
2(γ −ϵ)2 log
2h2 δ 
(9)"
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2445141065830721,"data points in Xi,L with above some given conﬁdence threshold ν for all iterations, we can guaran-
tee that ||ai,U,g −ai,L,g||∞< γ for all iterations with probability 1 −δ."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2476489028213166,"The proof of Proposition 1 from the original Snuba paper is included in Appendix A. We would like
to show that our bootstrapping technique maintains this guarantee, as in Proposition 2."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2507836990595611,"Proposition 2 (Theoretical Guarantee of Mako Weak Labeler Generation): Despite the modi-
ﬁcations from Snuba, the Mako weak labeler generation process satisﬁes the theoretical guarantees
described in Proposition 1."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.25391849529780564,"Proof of Proposition 2: Although our weak labelers are generated with (1) different model archi-
tectures and (2) sampling by bootstrapping on data instead of features, it still satisﬁes the deﬁnition
of weak heuristics required by Snuba. That is, a function f : Xi 7→Yi ∪{abstain}, where abstain
is a special label assigned to data points with a conﬁdence lower than the threshold ν."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.25705329153605017,"Consequently, by Proposition 2, if Mako checks the same theoretical exit condition at each iteration,
the ﬁnal generative model satisﬁes the same theoretical guarantee as in Snuba. The hyperparameter
search described in 4.1 can be understood as looking for labelers with high ai,L,w. Based on this
guarantee, we will have ||ai,U,g −ai,L,w||∞≤ϵ + γ with probability 1 −δ, and hence we can
probablistically achieve high labeling accuracy ai,U,g."
GUARANTEED DATA PROGRAMMING WITH BOOTSTRAPPING ON TRAINING SET,0.2601880877742947,"The weak labeling functions are then used to label both Xi,L and Xi,U and the labels are input for
the training of generative model πi. We label Xi,L as well for conﬁdence calibration, which will be
explained in the following subsection."
CONFIDENCE CALIBRATION AND THRESHOLDING,0.26332288401253917,"4.3
CONFIDENCE CALIBRATION AND THRESHOLDING"
CONFIDENCE CALIBRATION AND THRESHOLDING,0.2664576802507837,"Considering the variability and complexity of LML tasks, we believe the generative model labels
can be further improved to approach the ground truth. Therefore, Mako takes an extra step to adjust
the trained generative model πi by conﬁdence calibration on the produced logits on Xi,U."
CONFIDENCE CALIBRATION AND THRESHOLDING,0.26959247648902823,"Conﬁdence calibration (Guo et al., 2017) is the process to align the conﬁdence of labeling a data
point to the accuracy, which is an estimation of the probability that the data point is labeled correctly.
The procedure of conﬁdence calibration is adjusting the logits in order to minimize the expected
calibration error (ECE), formalized as
min
pπ
k,i,U
Epπ
k,i,U [| Pr[yπ
k,i,U = yk,i,U | pπ
k,i,U = p] −p|]
(10)"
CONFIDENCE CALIBRATION AND THRESHOLDING,0.2727272727272727,"where yk,i,U is the ground truth label of the k-th data point in Xi,U, yπ
k,i,U is its predicted label by
the data programming generative model and pπ
k,i,U is the associated conﬁdence. However, since the
ground truth labels of Xi,U is unknown, this approach is infeasible. This is the reason for training the
generative model under the supervision of weak labels on both Xi,U and Xi,L. The model is hence
expected to have the similar ECE on labeled and unlabeled data. Mako then performs temperature
scaling (Platt, 1999) to minimize ECE on Xi,L, and then apply the same temperature on Xi,U to
adjust the logits."
CONFIDENCE CALIBRATION AND THRESHOLDING,0.27586206896551724,"After calibration, there could still exist data points labeled with low conﬁdence. We then threshold
Xi,U with a conﬁdence of 1/nclasses + β for some small positive β. For example, in a 10-way
classiﬁcation problem, pick β = 0.01. We only keep the subset of Xi,U with calibrated conﬁdence
≥0.1 + 0.01 = 0.11. Denote this kept portion of unlabeled data and labels as (Xi,U,c, Y π
i,U,c)."
LML TASK TRAINING AND EVALUATION,0.27899686520376177,"4.4
LML TASK TRAINING AND EVALUATION"
LML TASK TRAINING AND EVALUATION,0.28213166144200624,"Finally, Xi,L, Yi,L, Xi,U,c and Y π
i,U,c will be input into the mounted LML tool for the training of
task Ti. The evaluation of the current model will be performed on all the hold-out testing data set
(Xtest,1, Ytest,1), . . . , (Xtest,i, Ytest,i)."
LML TASK TRAINING AND EVALUATION,0.2852664576802508,Under review as a conference paper at ICLR 2022
LML TASK TRAINING AND EVALUATION,0.2884012539184953,"Mako does not alter the internal procedures of the LML tool. Instead, it generates labels for unla-
beled data and then feeds it to the LML tool as a black box. This design enables high modularity
and the LML tool can be easily replaced. In the experiment section, we will demonstrate Mako’s
performance when mounting on various of LML paradigms such as DF-CNN, DEN and TF. This
design also imposes no additional knowledge space storage in LML."
EXPERIMENTS,0.29153605015673983,"5
EXPERIMENTS"
DATASETS AND LML TASK SEQUENCES,0.2946708463949843,"5.1
DATASETS AND LML TASK SEQUENCES"
DATASETS AND LML TASK SEQUENCES,0.29780564263322884,"We evaluate Mako on LML task sequences generated from 3 commonly-used image classiﬁcation
datasets: MNIST (LeCun & Cortes, 2010), CIFAR-10 and CIFAR-100 (Krizhevsky, 2009)."
DATASETS AND LML TASK SEQUENCES,0.30094043887147337,"We mount Mako on existing supervised LML tools: DF-CNN, and TF (Lee et al., 2019; Bulat
et al., 2020) to enable semi-supervised learning, and compare the performance to supervised LML
on labeled data only as well as all data fully labeled. The task sequences are:"
DATASETS AND LML TASK SEQUENCES,0.30407523510971785,"1. Binary MNIST: for each task, pick classes 0 vs 1, 0 vs 2, ..., 8 vs 9, with 45 tasks in total.
2. Binary CIFAR-10: same construction as binary MNIST."
DATASETS AND LML TASK SEQUENCES,0.3072100313479624,"Additionally, we compare Mako-mounted supervised LML tools to existing semi-supervised LML:
CNNL, ORDisCo, and DistillMatch (Baucum et al., 2017; Wang et al., 2021; Smith et al., 2021).
We generate the same task sequences as in their papers:"
DATASETS AND LML TASK SEQUENCES,0.3103448275862069,"3. 10-way CIFAR-10: the entire CIFAR-10 as one task.
4. 5-way CIFAR-100: for each task, pick each superclass in CIFAR-100, with 20 tasks in total.
5. 5-way MNIST: for each task, pick classes 0-4 and 5-9 in MNIST, with 2 tasks in total."
DATASETS AND LML TASK SEQUENCES,0.31347962382445144,"Details of the tasks such as data splits, hyperparameters searched and labeling accuracy are explained
in Appendix B."
COMPARISON TO SUPERVISED LML,0.3166144200626959,"5.2
COMPARISON TO SUPERVISED LML"
COMPARISON TO SUPERVISED LML,0.31974921630094044,"For each task, data is split into a labeled training set, an unlabeled training set and a hold-out test
set. The details of data split and weak labeler models searched are included in Appendix B."
COMPARISON TO SUPERVISED LML,0.322884012539185,"Table 1 and 2 summarize the performance of LML methods according to the amount of training data:
L standing for the labeled training set and U with number denoting the number of instances in the
unlabeled training set used during training of each task. These tables show peak per-task accuracy,
ﬁnal accuracy and catastrophic forgetting ratio up to the last task (Eqn. 6, 7 and 8, respectively).
Additionally, we trained the LML methods on the same data with true labels instead of the Mako-
generated labels to quantify the quality of the generated labels. LML methods achieve better peak"
COMPARISON TO SUPERVISED LML,0.32601880877742945,"LML
Data
Performance
Relative to True Labels
Per-Task Acc.
Final Acc.
Forget. Ratio
Per-Task Acc. Final Acc. TF"
COMPARISON TO SUPERVISED LML,0.329153605015674,"L
95.6 ± 0.4
96.8 ± 0.6
1.02 ± 0.01
-
-
L+U30
95.9 ± 0.4
97.0 ± 0.4
1.01 ± 0.01
0.99
1.00
L+U60
96.1 ± 0.3
96.8 ± 0.5
1.01 ± 0.01
0.99
0.99
L+U120
96.2 ± 0.2
96.9 ± 0.2
1.01 ± 0.00
0.99
0.99
L+U240
96.5 ± 0.2
96.4 ± 0.2
1.00 ± 0.00
0.98
0.98"
COMPARISON TO SUPERVISED LML,0.3322884012539185,DF-CNN
COMPARISON TO SUPERVISED LML,0.335423197492163,"L
93.8 ± 0.4
95.4 ± 0.3
1.02 ± 0.01
-
-
L+U30
94.6 ± 0.3
96.2 ± 0.4
1.02 ± 0.01
0.98
1.00
L+U60
94.9 ± 0.2
96.1 ± 0.7
1.01 ± 0.01
0.98
0.99
L+U120
95.2 ± 0.3
95.8 ± 0.9
1.01 ± 0.01
0.98
0.99
L+U240
95.9 ± 0.2
94.5 ± 1.3
0.99 ± 0.01
0.98
0.97"
COMPARISON TO SUPERVISED LML,0.3385579937304075,"Table 1: Supervised LML experiments on binary MNIST tasks, showing mean ± standard deviation.
LML models are trained on either Labeled data only or Labeled data and instances of Unlabeled data
with Mako-generated labels, and evaluated three metrics up to task 45 (Eqn. 6, 7 and 8) as well as
accuracy metrics relative to LML models trained on the same data with true labels instead."
COMPARISON TO SUPERVISED LML,0.34169278996865204,Under review as a conference paper at ICLR 2022
COMPARISON TO SUPERVISED LML,0.3448275862068966,"LML
Data
Performance
Relative to True Labels
Per-Task Acc.
Final Acc.
Forget. Ratio
Per-Task Acc. Final Acc. TF"
COMPARISON TO SUPERVISED LML,0.34796238244514105,"L
81.5 ± 0.2
76.8 ± 0.7
0.95 ± 0.01
-
-
L+U200
83.4 ± 0.4
76.8 ± 1.6
0.93 ± 0.02
0.99
1.00
L+U400
84.3 ± 0.3
76.7 ± 1.4
0.92 ± 0.02
-
-"
COMPARISON TO SUPERVISED LML,0.3510971786833856,DF-CNN
COMPARISON TO SUPERVISED LML,0.3542319749216301,"L
80.7 ± 0.3
80.4 ± 0.7
1.01 ± 0.01
-
-
L+U200
82.7 ± 0.3
80.0 ± 0.5
0.98 ± 0.01
0.99
0.99
L+U400
84.0 ± 0.3
80.0 ± 0.4
0.96 ± 0.01
-
-"
COMPARISON TO SUPERVISED LML,0.3573667711598746,"Table 2: Supervised LML experiments on binary CIFAR-10 tasks, mean ± standard deviation."
COMPARISON TO SUPERVISED LML,0.3605015673981191,"per-task accuracy as more Mako-labeled data is provided for training. This shows that Mako is able
to generate useful labels for training while the current task, and especially data distribution, keeps
changing in the lifelong learning setting. Training with Mako labels has no signiﬁcant improvement
in ﬁnal accuracy, but, compared to LML models trained on the true data, both peak per-task accuracy
and ﬁnal accuracy are at least 97% of the counterparts. Learning curves for each experiment are
shown in Appendix C."
COMPARISON TO SEMI-SUPERVISED LML,0.36363636363636365,"5.3
COMPARISON TO SEMI-SUPERVISED LML"
COMPARISON TO SEMI-SUPERVISED LML,0.3667711598746082,"As discussed in Section 2, there is little prior work in semi-supervised LML settings. We identify
three approaches for comparison: CNNL, ORDisCo, and DistillMatch (Baucum et al., 2017; Wang
et al., 2021; Smith et al., 2021). Each baseline approach contains multiple modules and networks
which each require their own tuning; to enable fair comparisons, we replicate the experimental con-
ditions (data set, amount of labeled vs. unlabeled data, task deﬁnitions, and network architecture)
and compare our approach to the best results reported in each baseline’s original publication. Specif-
ically, we replicate the instance-incremental learning experiments of CNNL on MNIST and CIFAR-
10, and the class-incremental learning experiments of ORDisCo and DistillMatch on CIFAR-10
and CIFAR-100, respectively. We brieﬂy describe each experiment below; for more details, see
Appendix B for data splits and the original publications for full experimental descriptions."
COMPARISON TO SEMI-SUPERVISED LML,0.36990595611285265,"Instance-Incremental Experiments.
We compare semi-supervised learning using Mako in an
instance-incremental setting (i.e., where all tasks are present at every epoch, but subsequent epochs
contain different batches of unlabeled data), using an identical two-convolutional-layer CNN as de-
scribed in (Baucum et al., 2017). The MNIST experiment uses a labeled data set of 150 images,
with 1000 unlabeled images introduced in each of 30 instance-incremental batches. The CIFAR-
10 experiment uses a labeled data set of 2000 samples, with subsequent batches of 1000 unlabeled
images. Each Mako experiment was run over 10 random seeds. For additional context, we include
the performance of the same network trained in the instance-incremental setting where all data is
labeled with the ground truth, representing an upper bound on semi-supervised performance. As
shown in Table 3, using Mako labels in place of CNNL’s semi-supervised labeler results in compa-
rable classiﬁcation accuracy with better sample efﬁciency (MNIST), or strictly higher classiﬁcation
accuracy (CIFAR-10). Learning curves for each experiment are shown in Appendix D."
COMPARISON TO SEMI-SUPERVISED LML,0.3730407523510972,"MNIST
CIFAR-10
Approach
Final Acc.
Batches to Saturation
Final Acc.
Batches to Saturation
CNNL
90.0
26
45.7
25
Mako Labeled
90.0 ± 0.4
3.3 ± 1.6
54.2 ± 0.5
26.5 ± 1.1
Fully Labeled
99.0 ± 0.1
17.0 ± 4.0
57.6 ± 0.5
27.1 ± 1.7"
COMPARISON TO SEMI-SUPERVISED LML,0.3761755485893417,"Table 3: Instance-incremental semi-supervised LML experiments, showing mean ± standard devi-
ation (where available). Final accuracy (see Eqn. 7) is measured on the data set’s standard held out
test set, batches to saturation is measured as the ﬁrst epoch where a 3-batch sliding window average
meets or exceeds the ﬁnal accuracy"
COMPARISON TO SEMI-SUPERVISED LML,0.3793103448275862,"Class-Incremental Experiments. We compare state-of-the-art LML methods (DF-CNN, DEN,
and TF (Lee et al., 2019; Yoon et al., 2018; Bulat et al., 2020)) using Mako labels to perform semi-
supervised learning in a class-incremental setting (i.e., the model is sequentially presented with tasks
containing new sets of classes). For all experiments below, each Mako-enabled approach was run"
COMPARISON TO SEMI-SUPERVISED LML,0.3824451410658307,Under review as a conference paper at ICLR 2022
COMPARISON TO SEMI-SUPERVISED LML,0.38557993730407525,"over 10 random seeds, and we also include the performance of the same using all ground truth labels.
We ﬁrst compare Mako to ORDisCo over ﬁve binary CIFAR-10 tasks using 400 labeled instances,
with the remaining data unlabeled. We note that we could not directly replicate the ORDisCo clas-
siﬁcation network architecture due to a lack of details in the original publication, but instead show
that using Mako labels achieve a higher total classiﬁcation accuracy with DF-CNN and TF using
a signiﬁcantly smaller network (4 convolutional layers for the Mako approaches, compared to OR-
DisCo’s 9 convolutional layers), shown in Table 4. Additionally, we compare Mako to DistillMatch
over the 5-way CIFAR-100 using 20% labeled data, replicating DistillMatch’s ParentClass task. As
shown in Table 4, all of the LML methods enabled by Mako labeling meet or exceed DistillMatch’s
performance over the learning task. Learning curves for each experiment are shown in Appendix D."
COMPARISON TO SEMI-SUPERVISED LML,0.3887147335423197,"CIFAR-10
CIFAR-100
Approach
Final Acc.
Approach
Final Acc.
Baseline
ORDisCo
74.8
DistillMatch
24.4 ± 0.4
Mako-labeled DF-CNN
86.8 ± 1.3
Mako-labeled DF-CNN
50.0 ± 0.8
Semi-
Mako-labeled DEN
61.4 ± 2.3
Mako-labeled DEN
24.1 ± 0.6
Supervised
Mako-labeled TF
82.1 ± 2.7
Mako-labeled TF
48.9 ± 2.1
Fully-labeled DF-CNN
86.8 ± 1.4
Fully-labeled DF-CNN
52.4 ± 1.4
Supervised
Fully-labeled DEN
61.4 ± 3.7
Fully-labeled DEN
23.7 ± 0.4
Fully-labeled TF
82.5 ± 4.6
Fully-labeled TF
51.0 ± 1.8"
COMPARISON TO SEMI-SUPERVISED LML,0.39184952978056425,"Table 4: Class-incremental semi-supervised LML experiments, showing mean ± standard deviation
(where available). Final accuracy (see Eqn. 7) is measured on the data set’s standard held out test
set, evaluated on all tasks after the ﬁnal task completes training. For a breakdown of individual task
accuracy and catastrophic forgetting ratios, see Appendix D."
CONCLUSION AND FUTURE WORK,0.3949843260188088,"6
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.3981191222570533,"In this paper, we identiﬁed the challenge that collecting task-level labeled data for LML is expensive.
We address this challenge with data programming, where labels are automatically generated, and
implemented the Mako framework that can be mounted on top of existing LML tools. Mako takes
in a limited number of labeled data and an unlimited number of unlabeled data, aiming to minimize
the performance gap to using the same data but fully labeled. We demonstrated Mako can achieve
sufﬁciently high accuracy per task as well as resistance to catastrophic forgetting, while costing no
additional knowledge base storage, over a set of common image classiﬁcation LML task sequences."
CONCLUSION AND FUTURE WORK,0.4012539184952978,"Future work on this topic can consider how to better resolve the issue of expensive labels at individ-
ual LML tasks. This includes how to extend Mako to a larger variety of LML task sequences. For
instance, what alternative methods can be used for labeler hyperparameter tuning other than search-
ing, what prior knowledge of tasks could assist automatic labeling, and how should the efﬁciency-
capability trade-off of different models be balanced. Outside of the Mako framework, alternative
solutions can modify existing LML tools directly, compromising a certain degree of modularity
while possibly improving the overall LML performance."
CONCLUSION AND FUTURE WORK,0.4043887147335423,"Another interesting future direction is to consider the scenario where the labeled data and unlabeled
data input per task are drawn from different distributions. This can be enabled by optimal transport
theory, speciﬁcally the minimization of Sinkhorn distance between two distributions (Cuturi, 2013).
Ideally, this approach could allow unlabeled data from other datasets to assist LML, resolving the
issue of not only expensive labels but also expensive data collection."
CONCLUSION AND FUTURE WORK,0.40752351097178685,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.4106583072100313,"7
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.41379310344827586,"We encourage researchers to replicate our experiments. As discussed in Section 4, we have pro-
vided the step-by-step explanation of Mako workﬂow. We have included more experiment details
in Appendix B for convenience. Additionally, we pushed a sample code to an anonymous GitHub
repository (https://github.com/mako-anon/mako). All readers are welcomed to pull our
code and execute Mako on the example data themselves."
REFERENCES,0.4169278996865204,REFERENCES
REFERENCES,0.4200626959247649,"M. Baucum, D. Belotto, S. Jeannet, E. Savage, P. Mupparaju, and C. W. Morato. Semi-supervised
deep continuous learning. In Proceedings of the International Conference on Deep Learning
Technologies, pp. 11–18, 2017."
REFERENCES,0.4231974921630094,"A. Bulat, J. Kossaiﬁ, G. Tzimiropoulos, and M. Pantic. Incremental multi-domain learning with net-
work latent tensor factorization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence.
AAAI Press, 2020."
REFERENCES,0.4263322884012539,"R. Caruana. Multitask Learning. PhD thesis, Carnegie Mellon University, 1997."
REFERENCES,0.42946708463949845,"Z. Chen and B. Liu. Lifelong Machine Learning. Synthesis Lectures on Artiﬁcial Intelligence and
Machine Learning. Morgan & Claypool Publishers, 2016."
REFERENCES,0.43260188087774293,"M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural
Information Processing Systems (NIPS), 2:2292–2300, 2013."
REFERENCES,0.43573667711598746,"C. Finn, T. Yu, J. Fu, P. Abbeel, and S. Levine. Generalizing skills with semi-supervised reinforce-
ment learning. Proceedings of the International Conference on Learning Representations (ICLR),
2017."
REFERENCES,0.438871473354232,"Y. Gao, Q. She, J. Ma, M. Zhao, W. Liu, and A. L. Yuille. NDDR-CNN: Layer-wise feature fusing
in multi-task cnn by neural discriminative dimensionality reduction. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 3205–3214, 2019."
REFERENCES,0.44200626959247646,"C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. Pro-
ceedings of the International Conference on Machine Learning (ICML), 70:1321–1330, 2017."
REFERENCES,0.445141065830721,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016."
REFERENCES,0.4482758620689655,"A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009."
REFERENCES,0.45141065830721006,"A. Kumar and H. Daume. Learning task grouping and overlap in multi-task learning. In Proceedings
of the 29th International Conference on Machine Learning, pp. 1383–1390. Omnipress, July
2012. ISBN 978-1-4503-1285-1."
REFERENCES,0.45454545454545453,"S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. Proccedings of the Inter-
national Conference on Learning Representations (ICLR), 2016."
REFERENCES,0.45768025078369906,"Y. LeCun and C. Cortes. Mnist handwritten digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/."
REFERENCES,0.4608150470219436,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11), 1998."
REFERENCES,0.46394984326018807,"S. Lee, J Stokes, and E. Eaton. Learning shared knowledge for deep lifelong learning using deconvo-
lutional networks. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence,
pp. 2837–2844, 2019."
REFERENCES,0.4670846394984326,"B. Liu. Lifelong machine learning: a paradigm for continuous learning. Frontier of Computer
Science, 11(3):359–361, 2017."
REFERENCES,0.4702194357366771,Under review as a conference paper at ICLR 2022
REFERENCES,0.47335423197492166,"Y. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris. Fully-adaptive feature sharing in multi-
task networks with applications in person attribute classiﬁcation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, July 2017."
REFERENCES,0.47648902821316613,"I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks for multi-task learning.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3994–
4003, 2016."
REFERENCES,0.47962382445141066,"C. Olivier, S. Bernhard, and Z. Alexander. Semi-supervised learning. IEEE Transactions on Neural
Networks, 20, 2006."
REFERENCES,0.4827586206896552,"S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transaction on Knowledge and Data
Engineering, 22(10):1345–1359, 2015."
REFERENCES,0.48589341692789967,"J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classiﬁers, 10(3):61–74, 1999."
REFERENCES,0.4890282131661442,"A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko. Semi-supervised learning with
ladder networks. Advances in Neural Information Processing Systems (NIPS), 2015."
REFERENCES,0.49216300940438873,"A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. R´e. Snrokel: rapid training data creation
with weak supervision. Proceedings of the VLDB Endowment, 11(3), 2016a."
REFERENCES,0.4952978056426332,"A. Ratner, S. De Sa, C.and Wu, D. Selsam, and C. R´e. Data programming: Creating large training
sets, quickly. Advances in Neural Information Processing Systems (NIPS), pp. 3574–3582, 2016b."
REFERENCES,0.49843260188087773,"A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu,
and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016."
REFERENCES,0.5015673981191222,"P. Ruvolo and E. Eaton. ELLA: An efﬁcient lifelong learning algorithm. Proceedings of the Inter-
national Conference on Machine Learning (PMLR), 28(1), 2013a."
REFERENCES,0.5047021943573667,"P. Ruvolo and E. Eaton. Active task selection for lifelong machine learning. Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, 27(1), 2013b."
REFERENCES,0.5078369905956113,"J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Had-
sell. Progress & compress: A scalable framework for continual learning. In Proceedings of the
International Conference on Machine Learning, pp. 4528–4537, 2018."
REFERENCES,0.5109717868338558,"B. Settles. Active learning literature survey. Technical report, Department of Computer Science,
University of Wisconsin-Madison, 2009."
REFERENCES,0.5141065830721003,"J. Smith, J. Balloch, Y. Hsu, and Z. Kira. Memory-efﬁcient semi-supervised continual learning: The
world is its own replay buffer. In Proceedings of the International Joint Conference on Neural
Networks, 2021."
REFERENCES,0.5172413793103449,"G. Sun, Y. Cong, and X. Xu. Active lifelong learning with ”watchdog”. In Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.5203761755485894,"A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results. Advances in Neural Information Process-
ing Systems (NIPS), 2017."
REFERENCES,0.5235109717868338,"S. Thrun and T. M. Mitchell. Lifelong robot learning. In The Biology and Technology of Intelligent
Autonomous Agents, pp. 165–196. Springer, Berlin, 1995."
REFERENCES,0.5266457680250783,"P. Varma and C. R´e. Snuba: automating weak supervision to label training data. Proceedings of the
VLDB Endowment, 12, 2016."
REFERENCES,0.5297805642633229,"L. Wang, K. Yang, C. Li, L. Hong, Z. Li, and J. Zhu. Ordisco: Effective and efﬁcient usage of incre-
mental unlabeled data for semi-supervised continual learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5383–5392, June 2021."
REFERENCES,0.5329153605015674,"S. Wang, Z. Chen, and B. Liu. Mining aspect-speciﬁc opinion using a holistic lifelong topic model.
Proceedings of the International Conference on World Wide Web (WWW), pp. 167–176, 2016."
REFERENCES,0.5360501567398119,"J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks.
Proccedings of the International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.5391849529780565,Under review as a conference paper at ICLR 2022
REFERENCES,0.542319749216301,"APPENDIX A
SNUBA THEORETICAL GUARANTEE AND PROOF
Proposition 1 in Section 4.2 and its proof are given by the original Snuba paper (Varma & R´e, 2016).
We adapt their notations to our paper to support the propositions in Section 4.2 on an arbitrary LML
task Ti. For convenience, we remove the subscript i in all notations."
REFERENCES,0.5454545454545454,"Proof of Proposition 1:
First, the probability of the l∞error between the labeling accuracies of
the generative model on XU and XL is greater than or equal to γ can be bounded by the following
triangular inequality, with the empirical accuracy of one weak labeler in the middle.
Pr[||aU,g −aL,g||∞≥γ] ≤Pr[||aU,g −aL,w||∞+ ||aL,w −aL,g||∞≥γ]
≤Pr[||aU,g −aL,w||∞+ ϵ ≥γ]
(11)"
REFERENCES,0.54858934169279,"Then, since we have h weak labelers, the following union bound holds for an individual labeler j in
the committed set and its scalar accuracy."
REFERENCES,0.5517241379310345,"Pr[||aU,g −aL,w||∞+ ϵ ≥γ] ≤ h
X"
REFERENCES,0.554858934169279,"j=1
Pr[|aU,g,j −aL,w,j| + ϵ ≥γ]
(12)"
REFERENCES,0.5579937304075235,"In XL, suppose there are dj data points not labeled as abstain by labeler j, the deﬁnition of the
empirical accuracy of weak labeler j is"
REFERENCES,0.5611285266457681,"aL,w,j = 1 dj X"
REFERENCES,0.5642633228840125,"k
1(yk = ˆyj,k)
(13)"
REFERENCES,0.567398119122257,"where yk is the ground truth label of data point k in the non-abstain subset and
ˆ
yj,k is the label
given by heuristic j. By combining Equations 12 and 13, and using Hoeffding’s inequality on the
independent data points, we can get
Pr[||aU,g −aL,w||∞ϵ ≥γ] = Pr[||aU,g −aL,w||∞+ ≥γ −ϵ] ≤ h
X"
REFERENCES,0.5705329153605015,"j=1
Pr[|aU,g,j −aL,w,j| ≥γ −ϵ] = h
X"
REFERENCES,0.5736677115987461,"j=1
Pr
h
|aU,g,j −1 dj X"
REFERENCES,0.5768025078369906,"k
1(yk = ˆyj,k|) ≥γ −ϵ
i ≤ h
X"
REFERENCES,0.5799373040752351,"j=1
2 exp(−2(γ −ϵ)2dj)"
REFERENCES,0.5830721003134797,"≤2h exp(−2(γ −ϵ)2 min(d1, . . . , dh)) (14)"
REFERENCES,0.5862068965517241,"Equation 14 bounds the probability of Snuba’s failure to keep ||aU,g −aL,g||∞< γ in one iteration.
Assume without loss of generality that we commit one more weak labeler per iteration, so for the all
h iterations, we apply union bound again:
Pr[||aU,g −aL,w||∞+ ≥γ −ϵ for any iteration] ≤ h
X"
REFERENCES,0.5893416927899686,"j=1
Pr[||aU,g −aL,w||∞+ ≥γ −ϵ for one iteration]"
REFERENCES,0.5924764890282131,"≤2h2 exp(−2(γ −ϵ)2 min(d1, . . . , dh)) (15)"
REFERENCES,0.5956112852664577,"Therefore, if we denote this probability of failure in any iteration as δ, we have the bound
δ ≤2h2 exp(−2(γ −ϵ)2 min(d1, . . . , dh))
(16)
and thus obtain Equation 9 by letting d = min(d1, . . . , dh)."
REFERENCES,0.5987460815047022,Under review as a conference paper at ICLR 2022
REFERENCES,0.6018808777429467,"APPENDIX B
WEAK LABELER MODEL HYPERPARAMETERS IN
EXPERIMENTS"
REFERENCES,0.6050156739811913,"The LML task conﬁgurations are listed in Table 5 for the convenience of replicating our procedure.
Because even after hyperparameter search (Section 4.1), there are still differences in the architectures
and training hyperparameters throughout a single LML sequence, we have provided shrunken search
spaces in weak labeler architecture and training hyperparameters."
REFERENCES,0.6081504702194357,"LML Task
Sequence
# Tasks
Data Split of
Xi,L, Xi,U, Xi,test"
REFERENCES,0.6112852664576802,"Weak Labeler
Architecture"
REFERENCES,0.6144200626959248,"Training
Hyperparams"
REFERENCES,0.6175548589341693,"Binary MNIST
45
120/11880/2000
Arch A"
REFERENCES,0.6206896551724138,"lr ∈[1e-4, 1e-2]
nbatches ∈[5, 10]
nepochs ∈[30, 100]
bootstrap = 30"
REFERENCES,0.6238244514106583,"Binary CIFAR-10
45
400/9600/2000
Arch B"
REFERENCES,0.6269592476489029,"lr ∈[0.8e-3, 1e-3]
nbatches = 30
nepochs ∈[30, 60]
bootstrap = 350"
REFERENCES,0.6300940438871473,"10-way CIFAR-10
1
2000/30000/10000
Arch B
but ﬁnal out = 10"
REFERENCES,0.6332288401253918,"lr = 1e-2
nbatches = 30
nepochs = 50
bootstrap = 750"
REFERENCES,0.6363636363636364,"5-way CIFAR-100
20
500/2000/500
Arch B
but ﬁnal out = 5"
REFERENCES,0.6394984326018809,"lr ∈[0.8e-3, 1e-2]
nbatches = 20
nepochs ∈[60, 140]
bootstrap = 200"
-WAY MNIST,0.6426332288401254,"5-way MNIST
2
150/29750/5000
Arch A
but ﬁnal out = 5"
-WAY MNIST,0.64576802507837,"lr ∈[0.8e-3, 1.5e-3]
nbatches = 10
nepochs ∈[180, 220]
bootstrap = 50"
-WAY MNIST,0.6489028213166145,"Table 5: LML Task Conﬁgurations in Section 5. Please refer to Figure 2 for weak labeler architec-
tures."
-WAY MNIST,0.6520376175548589,"conv_1: 
out_c=6, k=5,"
-WAY MNIST,0.6551724137931034,"p=2, s=1,"
-WAY MNIST,0.658307210031348,sigmoid
-WAY MNIST,0.6614420062695925,"conv_2: 
out_c=16, k=5,"
-WAY MNIST,0.664576802507837,"p=0, s=1,"
-WAY MNIST,0.6677115987460815,sigmoid
-WAY MNIST,0.670846394984326,dropout_1:
-WAY MNIST,0.6739811912225705,rate=0.2
-WAY MNIST,0.677115987460815,"avgpool_1:
k=2, p=0, s=2"
-WAY MNIST,0.6802507836990596,dropout_2:
-WAY MNIST,0.6833855799373041,rate=0.3
-WAY MNIST,0.6865203761755486,"avgpool_2:
k=2, p=0, s=2,"
-WAY MNIST,0.6896551724137931,flatten
-WAY MNIST,0.6927899686520376,dense_1:
-WAY MNIST,0.6959247648902821,"out=45,
sigmoid"
-WAY MNIST,0.6990595611285266,dense_2:
-WAY MNIST,0.7021943573667712,"out=21,
sigmoid"
-WAY MNIST,0.7053291536050157,dense_3:
-WAY MNIST,0.7084639498432602,"out=2,
softmax"
-WAY MNIST,0.7115987460815048,dropout_3:
-WAY MNIST,0.7147335423197492,rate=0.5
-WAY MNIST,0.7178683385579937,(a) Architecture A: used for MNIST and Fashion-MNIST
-WAY MNIST,0.7210031347962382,"conv_1: 
out_c=16, k=3,"
-WAY MNIST,0.7241379310344828,"p=1, s=1,
batchnorm, relu"
-WAY MNIST,0.7272727272727273,"block_1:
out_c=16"
-WAY MNIST,0.7304075235109718,"block_2:
out_c=32"
-WAY MNIST,0.7335423197492164,"block_3:
out_c=64"
-WAY MNIST,0.7366771159874608,"avgpool_1:
k=2, p=0, s=0,"
-WAY MNIST,0.7398119122257053,flatten
-WAY MNIST,0.7429467084639498,dense_1:
-WAY MNIST,0.7460815047021944,"out=2, 
softmax"
-WAY MNIST,0.7492163009404389,(b) Architecture B: used for CIFAR-based experiments
-WAY MNIST,0.7523510971786834,"conv_1: 
out_c=block_out_c,"
-WAY MNIST,0.7554858934169278,"k=3, p=1, s=3,"
-WAY MNIST,0.7586206896551724,"batchnorm, relu"
-WAY MNIST,0.7617554858934169,"conv_2: 
out_c=block_out_c,"
-WAY MNIST,0.7648902821316614,"k=3, p=1, s=1,"
-WAY MNIST,0.768025078369906,batchnorm
-WAY MNIST,0.7711598746081505,"conv_3: 
out_c=block_out_c,"
-WAY MNIST,0.774294670846395,"k=1, p=0, s=1,"
-WAY MNIST,0.7774294670846394,"batchnorm sum, relu"
-WAY MNIST,0.780564263322884,"conv_4: 
out_c=block_out_c,"
-WAY MNIST,0.7836990595611285,"k=3, p=1, s=2,"
-WAY MNIST,0.786833855799373,"batchnorm, relu"
-WAY MNIST,0.7899686520376176,"conv_5: 
out_c=block_out_c,"
-WAY MNIST,0.7931034482758621,"k=3, p=1, s=1,"
-WAY MNIST,0.7962382445141066,batchnorm
-WAY MNIST,0.799373040752351,"conv_6: 
out_c=block_out_c,"
-WAY MNIST,0.8025078369905956,"k=1, p=0, s=1,"
-WAY MNIST,0.8056426332288401,"batchnorm sum, relu"
-WAY MNIST,0.8087774294670846,"(c) Block structure of Architecture B, where block out c is the out c input into the block"
-WAY MNIST,0.8119122257053292,Figure 2: Searched Architectures of Weak Labeling Functions
-WAY MNIST,0.8150470219435737,"For example, to re-create our binary MNIST LML tasks, one needs to ﬁrst arrange the 45 tasks as
0 vs 1, 0 vs 2, ..., 8 vs 9. Then, split the labeled training data, unlabeled training data and testing"
-WAY MNIST,0.8181818181818182,Under review as a conference paper at ICLR 2022
-WAY MNIST,0.8213166144200627,"data into 120/11880/2000. All the data splits have balanced classes. Finally, perform our procedure
described in Section 4, starting from the hyperparameter search in the shrunken search space."
-WAY MNIST,0.8244514106583072,"Figure 2 illustrates the searched architectures. The search spaces were inspired by various previous
works on CNN designs (Lecun et al., 1998; He et al., 2016). Nonetheless, the ﬁnal architectures are
much smaller since we need fast training of multiple weak labelers. The notations of the ﬁgures are:
out c: output channels/number of ﬁlters, k: size (width and height) of a ﬁlter, p: padding and s:
stride."
-WAY MNIST,0.8275862068965517,Figure 3 shows the labeling accuracies of the Mako-labeled data input into LML.
-WAY MNIST,0.8307210031347962,"Figure 3: Labeling accuracies of the task sequences after conﬁdence calibration and thresholding,
with threshold = 1/num classes + 0.01 for all tasks, i.e. labeling accuracies of (Xi,U,c, Y π
i,U,c)
described in 4.3"
-WAY MNIST,0.8338557993730408,Under review as a conference paper at ICLR 2022
-WAY MNIST,0.8369905956112853,"APPENDIX C
ADDITIONAL SUPERVISED LML EXPERIMENT ANALYSIS"
-WAY MNIST,0.8401253918495298,"We visualize peak per-task accuracy and ﬁnal accuracy of supervised LML experiments (Figure 4)
and learning curve of ﬁnal accuracies in these experiments (Figure 5), summarized in Table 1 and
2. In these plots, we used the same notation L and U of the main text to specify the training data
setting, except TrueU denoting the case trained on the same instances of U but using the true labels
instead. Label L+U30 L+U60"
-WAY MNIST,0.8432601880877743,L+U120
-WAY MNIST,0.8463949843260188,L+U240
-WAY MNIST,0.8495297805642633,L+TrueU30
-WAY MNIST,0.8526645768025078,L+TrueU60
-WAY MNIST,0.8557993730407524,L+TrueU120
-WAY MNIST,0.8589341692789969,L+TrueU240 0.9 0.92 0.94 0.96 0.98 1
-WAY MNIST,0.8620689655172413,Accuracy
-WAY MNIST,0.8652037617554859,"Peak Per-task Accuracy
Final Accuracy"
-WAY MNIST,0.8683385579937304,(a) TF on Binary MNIST Label L+U30 L+U60
-WAY MNIST,0.8714733542319749,L+U120
-WAY MNIST,0.8746081504702194,L+U240 L+U30 L+U60
-WAY MNIST,0.877742946708464,L+U120
-WAY MNIST,0.8808777429467085,L+U240 0.9 0.92 0.94 0.96 0.98 1
-WAY MNIST,0.8840125391849529,Accuracy
-WAY MNIST,0.8871473354231975,"Peak Per-task Accuracy
Final Accuracy"
-WAY MNIST,0.890282131661442,(b) DF-CNN on Binary MNIST
-WAY MNIST,0.8934169278996865,"Label
L+U200
L+U400
L+TrueU200
0.7 0.75 0.8 0.85 0.9"
-WAY MNIST,0.896551724137931,Accuracy
-WAY MNIST,0.8996865203761756,"Peak Per-task Accuracy
Final Accuracy"
-WAY MNIST,0.9028213166144201,(c) TF on Binary CIFAR-10
-WAY MNIST,0.9059561128526645,"Label
L+U200
L+U400
L+TrueU200
0.7 0.75 0.8 0.85 0.9"
-WAY MNIST,0.9090909090909091,Accuracy
-WAY MNIST,0.9122257053291536,"Peak Per-task Accuracy
Final Accuracy"
-WAY MNIST,0.9153605015673981,(d) DF-CNN on Binary CIFAR-10
-WAY MNIST,0.9184952978056427,"Figure 4: Supervised LML experiments, showing mean and 95% conﬁdence interval. Peak per-task
accuracy increases as more mako-labeled data is provided as training, showing the beneﬁt of the
generated labels."
-WAY MNIST,0.9216300940438872,"(a) TF on Binary MNIST
(b) DF-CNN on Binary MNIST"
-WAY MNIST,0.9247648902821317,"(c) TF on Binary CIFAR-10
(d) DF-CNN on Binary CIFAR-10"
-WAY MNIST,0.9278996865203761,Figure 5: Learning Curve Comparisons
-WAY MNIST,0.9310344827586207,Under review as a conference paper at ICLR 2022
-WAY MNIST,0.9341692789968652,"APPENDIX D
ADDITIONAL SEMI-SUPERVISED LML EXPERIMENT
ANALYSIS"
-WAY MNIST,0.9373040752351097,"We include learning curves for each of the instance-incremental semi-supervised LML experiments
in Figure 6 and each of the class-incremental semi-supervised LML experiments in Figure 7. All
training curves show ﬁnal accuracy up to task i as deﬁned in Equation 3, where task i is the current
task being trained. For easier tasks with high labeling accuracy, such as CIFAR-10, the Mako-
enabled semi-supervised LML methods perform similarly to the equivalent supervised fully-labeled
task sequence. Otherwise, the Mako-enabled semi-supervised methods approach fully supervised
performance."
-WAY MNIST,0.9404388714733543,"(a) 5-way MNIST Experiment
(b) 10-way CIFAR-10 Experiment"
-WAY MNIST,0.9435736677115988,Figure 6: Instance-incremental semi-supervised vs fully supervised learning curve comparisons
-WAY MNIST,0.9467084639498433,(a) Binary CIFAR-10 Experiment
-WAY MNIST,0.9498432601880877,(b) 5-way CIFAR-100 Experiment
-WAY MNIST,0.9529780564263323,Figure 7: Class-incremental semi-supervised vs fully supervised learning curve comparisons.
-WAY MNIST,0.9561128526645768,"We additionally analyze the class-incremental semi-supervised LML baseline experiments for catas-
trophic forgetting, using the catastrophic forgetting ratio deﬁned in Equation 4. Catastrophic forget-
ting measures are not available for the baseline methods ORDisCo and DistillMatch as they were
not reported in the original publications. The results are shown in Table 6."
-WAY MNIST,0.9592476489028213,"For a further breakdown of results, we include task-speciﬁc performance measures for all of the
semi-supervised LML experiments in Tables 7, 8, 9, and 10."
-WAY MNIST,0.9623824451410659,Under review as a conference paper at ICLR 2022
-WAY MNIST,0.9655172413793104,"CIFAR-10
CIFAR-100
Approach
Forget. Ratio
Approach
Forget. Ratio
Mako-lab. DF-CNN
0.98 ± 0.04
Mako-lab. DF-CNN
0.85 ± 0.18
Semi-
Mako-lab. DEN
0.72 ± 0.16
Mako-lab. DEN
0.48 ± 0.16
Supervised
Mako-lab. TF
0.92 ± 0.09
Mako-lab. TF
0.83 ± 0.16
Fully-lab. DF-CNN
0.98 ± 0.04
Fully-lab. DF-CNN
0.83 ± 0.19
Supervised
Fully-lab. DEN
0.72 ± 0.17
Fully-lab. DEN
0.48 ± 0.16
Fully-lab. TF
0.93 ± 0.10
Fully-lab. TF
0.82 ± 0.16"
-WAY MNIST,0.9686520376175548,"Table 6: Catastrophic forgetting ratios (see Equation 4) for class-incremental semi-supervised LML
experiments, showing mean ± standard deviation"
-WAY MNIST,0.9717868338557993,"Mako-labeled (Semi-supervised)
Fully-labeled (Supervised)
Approach
Task
Final Acc.
Forget. Ratio
Final Acc.
Forget. Ratio
0
88.1 ± 1.4
0.953 ± 0.015
86.7 ± 5.3
0.944 ± 0.061
1
75.9 ± 4.8
0.942 ± 0.063
76.6 ± 1.7
0.944 ± 0.024
DF-CNN
2
84.4 ± 2.0
0.987 ± 0.020
84.7 ± 1.1
0.989 ± 0.014
3
93.2 ± 0.5
0.999 ± 0.007
93.5 ± 0.7
0.999 ± 0.003
4
92.5 ± 1.1
1.000 ± 0.000
92.6 ± 0.6
1.000 ± 0.000
0
80.5 ± 7.2
0.874 ± 0.078
79.0 ± 11.2
0.853 ± 0.119
1
67.8 ± 6.7
0.845 ± 0.085
71.0 ± 5.0
0.884 ± 0.064
TF
2
78.1 ± 8.8
0.922 ± 0.105
77.4 ± 9.4
0.905 ± 0.111
3
91.3 ± 3.5
0.972 ± 0.035
92.4 ± 1.4
0.987 ± 0.009
4
92.9 ± 0.5
1.000 ± 0.000
92.8 ± 1.0
1.000 ± 0.000
0
50.4 ± 0.7
0.559 ± 0.013
51.7 ± 4.3
0.574 ± 0.045
1
50.2 ± 3.3
0.664 ± 0.049
50.8 ± 9.4
0.678 ± 0.118
DEN
2
58.1 ± 9.7
0.716 ± 0.111
54.5 ± 8.7
0.674 ± 0.106
3
60.7 ± 7.5
0.678 ± 0.086
59.9 ± 5.4
0.662 ± 0.063
4
87.4 ± 3.9
1.000 ± 0.000
90.0 ± 0.6
1.000 ± 0.000"
-WAY MNIST,0.9749216300940439,"Table 7: Class-incremental Binary CIFAR-10 experiment results broken down by task, showing
mean ± standard deviation"
-WAY MNIST,0.9780564263322884,"Mako-labeled (Semi-supervised)
Fully-labeled (Supervised)
Approach
Task
Final Acc.
Forget. Ratio
Final Acc.
Forget. Ratio
0
33.9 ± 5.4
0.663 ± 0.104
31.8 ± 6.0
0.583 ± 0.110
1
35.1 ± 6.5
0.547 ± 0.103
33.8 ± 6.3
0.491 ± 0.093
2
32.8 ± 4.0
0.556 ± 0.067
32.8 ± 5.7
0.509 ± 0.081
3
41.6 ± 6.6
0.640 ± 0.101
38.8 ± 5.9
0.562 ± 0.084
4
37.5 ± 7.5
0.530 ± 0.104
42.0 ± 7.2
0.564 ± 0.092
5
45.8 ± 5.4
0.776 ± 0.080
50.7 ± 7.6
0.788 ± 0.117
6
49.3 ± 5.1
0.831 ± 0.083
51.5 ± 4.5
0.804 ± 0.065
7
46.7 ± 3.5
0.768 ± 0.049
53.6 ± 3.7
0.789 ± 0.066
8
57.2 ± 3.1
0.901 ± 0.048
58.2 ± 5.1
0.873 ± 0.069
DF-CNN
9
64.2 ± 2.7
0.930 ± 0.038
65.9 ± 2.3
0.923 ± 0.035
10
70.1 ± 3.1
0.944 ± 0.047
71.1 ± 3.0
0.930 ± 0.040
11
53.0 ± 3.5
0.917 ± 0.055
57.7 ± 2.5
0.952 ± 0.046
12
61.7 ± 1.7
0.967 ± 0.033
64.2 ± 1.7
0.963 ± 0.033
13
54.1 ± 1.8
0.970 ± 0.034
55.4 ± 1.8
0.922 ± 0.029
14
33.0 ± 1.7
0.987 ± 0.032
36.7 ± 1.9
0.977 ± 0.039
15
50.0 ± 1.3
1.006 ± 0.029
52.7 ± 2.5
0.971 ± 0.035
16
48.1 ± 2.2
1.002 ± 0.024
50.0 ± 2.3
0.967 ± 0.021
17
53.1 ± 1.2
1.000 ± 0.013
56.1 ± 1.6
0.994 ± 0.018
18
62.7 ± 2.1
1.001 ± 0.015
67.7 ± 1.8
0.996 ± 0.009
19
70.5 ± 1.8
1.000 ± 0.000
77.3 ± 1.9
1.000 ± 0.000"
-WAY MNIST,0.9811912225705329,"Table 8: Class-incremental 5-way CIFAR-100 experiment, DF-CNN, results broken down by task,
showing mean ± standard deviation"
-WAY MNIST,0.9843260188087775,Under review as a conference paper at ICLR 2022
-WAY MNIST,0.987460815047022,"Mako-labeled (Semi-supervised)
Fully-labeled (Supervised)
Approach
Task
Final Acc.
Forget. Ratio
Final Acc.
Forget. Ratio
0
36.9 ± 6.7
0.729 ± 0.135
38.8 ± 9.1
0.736 ± 0.174
1
43.4 ± 4.8
0.692 ± 0.082
49.9 ± 9.9
0.750 ± 0.138
2
36.2 ± 7.0
0.639 ± 0.122
37.5 ± 7.0
0.617 ± 0.116
3
56.4 ± 5.2
0.870 ± 0.085
54.0 ± 5.5
0.791 ± 0.072
4
40.6 ± 13.2
0.585 ± 0.193
42.4 ± 10.7
0.600 ± 0.151
5
51.0 ± 7.4
0.839 ± 0.124
50.9 ± 13.6
0.760 ± 0.206
6
46.3 ± 7.1
0.767 ± 0.126
52.6 ± 5.0
0.832 ± 0.085
7
48.8 ± 9.3
0.804 ± 0.152
50.8 ± 7.4
0.804 ± 0.081
8
49.6 ± 8.7
0.773 ± 0.111
47.0 ± 8.5
0.732 ± 0.122
9
60.4 ± 7.1
0.885 ± 0.106
58.5 ± 6.0
0.828 ± 0.095
TF
10
57.5 ± 11.4
0.777 ± 0.154
56.9 ± 14.0
0.732 ± 0.180
11
48.3 ± 8.3
0.842 ± 0.140
46.9 ± 6.0
0.798 ± 0.115
12
53.6 ± 5.2
0.881 ± 0.072
57.6 ± 3.8
0.905 ± 0.064
13
48.4 ± 5.5
0.888 ± 0.097
52.9 ± 8.2
0.847 ± 0.126
14
31.0 ± 2.0
0.924 ± 0.070
31.6 ± 2.9
0.945 ± 0.090
15
41.5 ± 6.0
0.836 ± 0.134
48.1 ± 2.8
0.928 ± 0.046
16
45.1 ± 3.0
0.956 ± 0.073
45.2 ± 5.2
0.936 ± 0.093
17
50.0 ± 4.3
0.939 ± 0.083
54.5 ± 2.4
0.969 ± 0.033
18
62.8 ± 2.7
0.974 ± 0.032
67.8 ± 2.2
0.962 ± 0.023
19
69.6 ± 2.2
1.000 ± 0.000
76.4 ± 2.7
1.000 ± 0.000"
-WAY MNIST,0.9905956112852664,"Table 9: Class-incremental 5-way CIFAR-100 experiment, TF, results broken down by task, showing
mean ± standard deviation"
-WAY MNIST,0.9937304075235109,"Mako-labeled (Semi-supervised)
Fully-labeled (Supervised)
Approach
Task
Final Acc.
Forget. Ratio
Final Acc.
Forget. Ratio
0
21.1 ± 2.9
0.433 ± 0.059
19.6 ± 1.1
0.388 ± 0.033
1
18.6 ± 2.4
0.373 ± 0.061
22.7 ± 2.3
0.455 ± 0.092
2
20.3 ± 2.2
0.424 ± 0.056
21.0 ± 2.4
0.477 ± 0.097
3
20.1 ± 1.6
0.381 ± 0.080
20.5 ± 3.0
0.361 ± 0.058
4
24.5 ± 4.7
0.420 ± 0.106
20.1 ± 1.5
0.373 ± 0.069
5
21.9 ± 1.4
0.444 ± 0.079
21.8 ± 2.5
0.423 ± 0.047
6
19.3 ± 1.0
0.405 ± 0.110
19.6 ± 2.7
0.350 ± 0.062
7
22.4 ± 2.5
0.490 ± 0.171
21.8 ± 2.3
0.452 ± 0.095
8
22.0 ± 1.3
0.451 ± 0.058
21.8 ± 1.3
0.472 ± 0.058
9
24.0 ± 2.6
0.421 ± 0.077
22.0 ± 2.7
0.402 ± 0.062
DEN
10
22.7 ± 1.8
0.350 ± 0.048
23.1 ± 3.1
0.359 ± 0.046
11
22.9 ± 1.9
0.502 ± 0.058
22.8 ± 2.5
0.477 ± 0.050
12
21.9 ± 2.1
0.408 ± 0.073
22.3 ± 1.4
0.432 ± 0.041
13
24.4 ± 3.0
0.506 ± 0.056
24.4 ± 3.3
0.494 ± 0.058
14
20.9 ± 1.5
0.689 ± 0.082
20.2 ± 1.1
0.737 ± 0.083
15
22.8 ± 3.0
0.497 ± 0.068
20.9 ± 1.6
0.479 ± 0.051
16
21.2 ± 0.8
0.516 ± 0.059
21.1 ± 2.1
0.527 ± 0.065
17
22.2 ± 1.6
0.429 ± 0.033
21.6 ± 1.7
0.434 ± 0.038
18
23.1 ± 2.1
0.416 ± 0.065
23.0 ± 2.0
0.446 ± 0.064
19
65.2 ± 3.5
1.000 ± 0.000
64.1 ± 5.2
1.000 ± 0.000"
-WAY MNIST,0.9968652037617555,"Table 10: Class-incremental 5-way CIFAR-100 experiment, DEN, results broken down by task,
showing mean ± standard deviation"
