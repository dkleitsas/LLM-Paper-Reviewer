Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001996007984031936,"Data-driven algorithm design is a promising, learning-based approach for be-
yond worst-case analysis of algorithms with tunable parameters. An important
open problem is the design of computationally efficient data-driven algorithms
for combinatorial algorithm families with multiple parameters. As one fixes the
problem instance and varies the parameters, the “dual” loss function typically has
a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp
transition boundaries. Motivated by prior empirical work, we initiate the study of
techniques to develop efficient ERM learning algorithms for data-driven algorithm
design by enumerating the pieces of the sum dual loss functions for a collection
of problem instances. The running time of our approach scales with the actual
number of pieces that appear as opposed to worst case upper bounds on the number
of pieces. Our approach involves two novel ingredients – an output-sensitive algo-
rithm for enumerating polytopes induced by a set of hyperplanes using tools from
computational geometry, and an execution graph which compactly represents all
the states the algorithm could attain for all possible parameter values. We illustrate
our techniques by giving algorithms for pricing problems, linkage-based clustering
and dynamic-programming based sequence alignment."
INTRODUCTION,0.003992015968063872,"1
Introduction"
INTRODUCTION,0.005988023952095809,"The data-driven algorithm design paradigm captures a widely occuring scenario of solving multiple
related problem instances and allows the design and analysis of algorithms that use machine learning
to learn how to solve the instances which come from the same domain [GR16, Bal20]. Typically
there are large (often infinite) parameterized algorithm families to choose from, and data-driven
algorithm design approaches provide techniques to select algorithm parameters that provably perform
well for instances from the same domain. Data-driven algorithms have been proposed and analyzed
for a variety of combinatorial problems, including clustering, computational biology and mechanism
design [BDL20, BDD+21, BPS20]. But most of the prior work has focused on sample efficiency of
learning good algorithms i.e. the number of problem instances needed to learn algorithm parameters
that perform well on a typical problem from the domain. A major open question for this line of
research is to design computationally efficient learning algorithms [GR20, BDS21]."
INTRODUCTION,0.007984031936127744,"The parameterized family may occur naturally in well-known algorithms used in practice, or one
could potentially design new families interpolating known heuristics. For the problem of aligning
pairs of genomic sequences, one typically obtains the best alignment using a dynamic program
with some costs or weights assigned to edits of different kinds, such as insertions, substitutions,
or reduplications [Wat89]. These costs are the natural parameters for the alignment algorithm.
The quality of the alignment can strongly depend on the parameters, and the best parameters vary
depending on the application (e.g. aligning DNAs or RNAs), the pair of species being compared, or"
INTRODUCTION,0.00998003992015968,"∗Carnegie Mellon University, ninamf@cs.cmu.edu.
†Work done by Christopher Seiler while he was at CMU.
‡Corresponding author: dravy@ttic.edu. Work done by Dravyansh Sharma while he was at CMU."
INTRODUCTION,0.011976047904191617,"Problem
Dim.
TS (prior work)
TS (ours)
TERM (m instances)
Linkage-based
d = 2
O(n18 log n) [BDL20]
O(Rn3)
mTS + ˜O(mn2RΣ)
clustering
any d
O(n8d+2 log n) [BDL20]
˜O(R2n3)
mTS + ˜O(mn2R2
Σ)
DP-based sequ-
d = 2
O(R2 + RTDP) [GBN94]
O(RTDP)
mTS + ˜O(mTDPRΣ)
ence alignment
any d
sO(sd)TDP [BDD+21]
˜O(˜R2L+1TDP)
mTS + ˜O(mTDPR2
Σ)
Two-part tariff
ℓ= 1
O(K3) [BPS20]
˜O(R + K)
O(RΣ + mK log mK)
pricing
any L′
KO(L′) [BPS20]
˜O(R2K)
mTS + ˜O(mKR2
Σ)"
INTRODUCTION,0.013972055888223553,"Table 1: Summary of running times of the proposed algorithms. TERM denotes the running time for
computing the pieces in the sum dual class function in terms of TS, the time for enumerating the
pieces on a single problem instance (Theorem 2.3). RΣ (resp. R) denotes the number of pieces in
the sum dual class function for given m problem instances (resp. dual class function for a single
problem instance). n is the size of the clustering instance. s is the length of sequences to be aligned,
L is the maximum number of subproblems in the sequence alignment DP update, ˜R is the maximum
number of pieces in the dual class function over all subproblems, TDP is the time to solve the DP on
a single instance. K is the number of units of the item sold and ℓis the menu length. The ˜O notation
suppresses logarithmic terms and multiplicative terms that only depend on d or L."
INTRODUCTION,0.015968063872255488,"the purpose of the alignment. Similarly, item prices are natural parameters in automated mechanism
design [BSV18, BPS20]. On the other hand, for linkage-based clustering, one usually chooses from
a set of different available heuristics, such as single or complete linkage. Using an interpolation
of these heuristics to design a parameterized family and tune the parameter, one can often obtain
significant improvements in the quality of clustering [BNVW17, BDL20]."
INTRODUCTION,0.017964071856287425,"A common property satisfied by a large number of interesting parameterized algorithm families is
that the loss4 as a function of the real-valued parameters for any fixed problem instance—called the
“dual class function” [BS21]—is a piecewise structured function, i.e. the parameter space can be
partitioned into “pieces” via sharp transition boundaries such that the loss function is well-behaved
(e.g. constant or linear) within each piece [Bal20]. Prior work on data-driven algorithm design has
largely focused on the sample complexity of the empirical risk minimization (ERM) algorithm which
finds the loss-minimizing value of the parameter over a collection of problem instances drawn from
some fixed unknown distribution. The ERM on a collection of problem instances can be implemented
by enumerating the pieces of the sum dual class loss function. We will design algorithms for
computationally efficient enumeration of these pieces, when the transition boundaries are linear,
and our techniques can be used to learn good domain-specific values of these parameters given
access to multiple problem instances from the problem domain. More precisely, we use techniques
from computational geometry to obtain “output-sensitive” algorithms (formalized in Appendix C),
that scale with the output-size RΣ (i.e. number of pieces in the partition) of the piece enumeration
problem for the sum dual class function on a collection of problem instances. Often, on commonly
occurring problem instances, the number of pieces is much smaller than worst case bounds on it
[BDL20, GS96] and our results imply significant gains in running time whenever RΣ is small."
INTRODUCTION,0.01996007984031936,"Our contributions.
We design novel approaches that use tools from computational geometry
and lead to output-sensitive algorithms for learning good parameters by implementing the ERM
(Empirical Risk Minimization) for several distinct data-driven design problems. The resulting learning
algorithms scale polynomially with the number of sum dual class function pieces RΣ in the worst
case (See Table 1) and are efficient for small constant d."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.021956087824351298,"1. We present a novel output-sensitive algorithm for enumerating the cells induced by a collection
of hyperplanes. Our approach applies to any problem where the loss is piecewise linear, with
convex polytopic piece boundaries. We achieve output-polynomial time by removing redundant
constraints in any polytope using Clarkson’s algorithm (from computational geometry) [Cla94]
and performing an implicit search over the graph of neighboring polytopes (formally Definition 2).
Our results are useful in obtaining output-sensitive running times for several distinct data-driven
algorithm design problems. Theorem 2.3 bounds the running time of the implementation of the"
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.023952095808383235,"4or utitity, basically a function that measures the performance of any algorithm in the family on any problem
instance."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.02594810379241517,"ERM for piecewise-structured duals with linear boundaries (Definition 1) that scales with RΣ
provided we can enumerate the pieces of the dual class function for a single problem instance.
It is therefore sufficient to enumerate the pieces of dual function for a single instance, for which
Theorem 2.2 is useful.
2. We show how to learn multidimensional parameterized families of hierarchical clustering algo-
rithms5. Our approach for enumerating the pieces of the dual class function on a single instance
extends the execution tree based approach introduced for the much simpler single-parameter family
in [BDL20] to multiple dimensions. The key idea is to track the convex polytopic subdivisions
of the parameter space corresponding to a single merge step in the linkage procedure via nodes
of the execution tree and apply our output-sensitive cell-enumeration algorithm at each node.
3. For dynamic programming (DP) based sequence alignment, our approach for enumerating pieces
of the dual class function extends the execution tree to an execution directed acyclic graph (DAG).
For a small fixed d, our algorithm is efficient for small constant L, the maximum number of
subproblems needed to compute any single DP update. Prior work [GBN94] only provides an
algorithm for computing the full partition for d = 2, and the naive approach of comparing all
pairs of alignments takes exponential time."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.027944111776447105,"Our main contribution is to show that when the number of pieces (RΣ in Table 1) is small we can
improve over computational efficiency of prior work. Our techniques involves a novel application
of Clarkson’s algorithm (used to remove redundant inequalities from system of linear equations) to
derive output-sensitive enumeration, and execution graphs which capture problem-specific structure."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.029940119760479042,"Motivation from prior empirical work.
For the clustering problem, we give theoretical bounds
on the output-size R depending on the specific parametric family considered (e.g. Lemmas I.1,
I.2). These bounds imply a strict improvement even for worst-case R, but can be much faster for
typical R. Indeed, prior empirical work indicates R is much smaller than the worst case bounds in
practice [BDL20], where even for d = 1 there is a dramatic speed up of over 1015 times by being
output-sensitive. For the sequence alignment problem, again the upper bounds on R depend on the
nature of cost functions involved. For example, Theorem 2.2 of [GBN94] gives an upper bound on
R for the 2-parameter problem with substitutions and deletions. For the TPT pricing problem we
prove new theoretical bounds on R (Theorem G.3) that show worst-case improvements (cubic to
quadratic) in the running time over prior work. In practice, R is usually much smaller than the worst
case bounds, making our results even stronger. In prior experimental work on computational biology
data (sequence alignment with d = 2, sequence length ~100), [GS96] observe that of a possible more
than 2200 alignments, only R = 120 appear as possible optimal sequence alignments (over R2) for a
pair of immunoglobin sequences (a speed-up of over 58 orders of magnitude)."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.031936127744510975,"Preliminaries.
We follow the notation of [Bal20]. For a given algorithmic problem (say clustering,
or sequence alignment), let Π denote the set of problem instances of interest. We also fix a (potentially
infinite) family of algorithms A, parameterized by a set P ⊆Rd. Here d is the number of real
parameters, and will also be the ‘fixed’ parameter in the FPT (Fixed Parameter Tractability) sense
(Appendix C). Let Aρ denote the algorithm in the family A parameterized by ρ ∈P. The performance
of any algorithm on any problem instance is given by a utility (or loss) function u : Π × P →[0, H],
i.e. u(x, ρ) measures the performance on problem instance x ∈Π of algorithm Aρ ∈A. The utility
of a fixed algorithm Aρ from the family is given by uρ : Π →[0, H], with uρ(x) = u(x, ρ). We
are interested in the structure of the dual class of functions ux : P →[0, H], with ux(ρ) = uρ(x),
which measure the performance of all algorithms of the family for a fixed problem instance x ∈Π.
We will use I{·} to denote the 0-1 valued indicator function. For many parameterized algorithms, the
dual class functions are piecewise structured in the following sense [BDD+21]."
WE PRESENT A NOVEL OUTPUT-SENSITIVE ALGORITHM FOR ENUMERATING THE CELLS INDUCED BY A COLLECTION,0.033932135728542916,"Definition 1 (Piecewise structured with linear boundaries). A function class H ⊆RP that maps a
domain P ⊆Rd to R is (F, t)-piecewise decomposable for a class F ⊆RP of piece functions if the
following holds: for every h ∈H, there are t linear threshold functions g1, . . . , gt : P →{0, 1}, i.e.
gi(x) = I{aT
i x + bi} and a piece function fb ∈F for each bit vector b ∈{0, 1}t such that for all
y ∈P, h(y) = fby(y) where by = (g1(y), . . . , gt(y)) ∈{0, 1}t."
ONE SHOULD CAREFULLY DISTINGUISH THE PARAMETERIZED CLUSTERING ALGORITHM FROM THE ERM-BASED LEARNING,0.03592814371257485,"5One should carefully distinguish the parameterized clustering algorithm from the ERM-based learning
algorithm which learns a parameter from this family given problem instances. Our algorithmic approaches focus
on implementing the ERM efficiently."
ONE SHOULD CAREFULLY DISTINGUISH THE PARAMETERIZED CLUSTERING ALGORITHM FROM THE ERM-BASED LEARNING,0.03792415169660679,"We will refer to connected subsets of the parameter space where the dual class function is a fixed
piece function fc as the (dual) pieces or regions, when the dual class function is clear from the context.
Past work [BSV18] defines a similar definition for mechanism classes called (d, t)-delineable classes
which are a special case of Definition 1, where the piece function class F consists of linear functions,
and focus on sample complexity of learning, i.e. the number of problem instances from the problem
distribution that are sufficient learn near-optimal parameters with high confidence over the draw of
the sample. Our techniques apply for a larger class, where F is a class of convex functions. Past
work [BSV18, BDL20, BDD+21] provides sample complexity guarantees for problems that satisfy
Definition 1, on the other hand we will focus on developing fast algorithms. We develop techniques
that yield efficient algorithms for computing these pieces in the multiparameter setting, i.e. constant
d > 1.The running times of our algorithms are polynomial in the output size (i.e, number of pieces).
For i ∈Z+, we will use [i] to denote the set of positive integers {1, . . . , i}."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.03992015968063872,"2
Output-sensitive cell enumeration and data-driven algorithm design"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.041916167664670656,"Suppose H is a collection of t hyperplanes in Rd. We consider the problem of enumerating the d-faces
(henceforth cells) of the convex polyhedral regions induced by the hyperplanes. The enumerated cells
will be represented as sign-patterns6 of facet-inducing hyperplanes. We will present an approach for
enumerating these cells in OFPT time (Output-sensitive Fixed Parameter Tractable) Definition 3),
which involves two key ingredients: (a) locality-sensitivity, and (b) output-sensitivity. By locality
sensitivity, we mean that our algorithm exploits problem-specific local structure in the neighborhood
of each cell to work with a smaller candidate set of hyperplanes which can potentially constitute
the cell facets. This is abstracted out as a sub-routine COMPUTELOCALLYRELEVANTSEPARATORS
which we will instantiate and analyse for each individual problem. In this section we will focus more
on the output-sensitivity aspect."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.043912175648702596,"To provide an output-sensitive guarantee for this enumeration problem, we compute only the
non-redundant hyperplanes which provide the boundary of each cell c in the partition induced by
the hyperplanes. We denote the closed polytope bounding cell c by Pc. A crucial ingredient for
ensuring good output-sensitive runtime of our algorithm is Clarkson’s algorithm for computing
non-redundant constraints in a system of linear inequalities [Cla94]. A constraint is redundant in
a system if removing it does not change the set of solutions. The key idea is to maintain a set I of
non-redundant constraints detected so far, and solve LPs that detect the redundancy of a remaining
constraint (not in I) when added to I. If the constraint is redundant relative to I, it must also be
redundant in the full system, otherwise we can add a (potentially different) non-redundant constraint
to I (see Appendix D for details). The following runtime guarantee is known for the algorithm.
Theorem 2.1 (Clarkson’s algorithm). Given a list L of k half-space constraints in d dimensions,
Clarkson’s algorithm outputs the set I ⊆L of non-redundant constraints in L in time O(k·LP(d, |I|+
1)), where LP(v, c) is the time for solving an LP with v variables and c constraints."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.04590818363273453,"Algorithm 1 uses AUGMENTEDCLARKSON, which modifies Clarkson’s algorithm with some ad-
ditional bookkeeping (details in Appendix D) to facilitate a search for neighboring regions in our
algorithm, while retaining the same asymptotic runtime complexity. Effectively, our algorithm can be
seen as a breadth-first search over an implicit underlying graph (Definition 2), where the neighbors
(and some auxiliary useful information) are computed dynamically by AUGMENTEDCLARKSON.
Definition 2 (Cell adjacency graph). Define the cell adjacency graph for a set H of hyperplanes in
Rd, written GH = (VH, EH), as:"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.04790419161676647,"• There is a vertex v ∈VH for each cell in the partition ˜C of Rd induced by the hyperplanes;
• For v, v′ ∈VH, add the edge {v, v′} to EH if the corresponding polytopes intersect, i.e. Pv ∩Pv′ ̸=∅."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.0499001996007984,This generalizes to a subdivision (Definition 6) of a polytope in Rd.
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.05189620758483034,"This allows us to state the following guarantee about the runtime of Algorithm 1. In the notation of
Table 1, we have R = |VH| and |EH| = O(R2).
Theorem 2.2. Let H be a set of t hyperplanes in Rd. Suppose that |EH| = E and |VH| = V in the
cell adjacency graph GH = (VH, EH) of H; then if the domain P is bounded by |P| ≤t hyperplanes,"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.05389221556886228,"6For simplicity we will denote these by vectors of the form {0, 1, −1}t where non-zero co-ordinates corre-
spond to signs of facet-inducing hyperplanes. Hash tables would be a practical data structure for implementation."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.05588822355289421,Algorithm 1: OUTPUTSENSITIVEPARTITIONSEARCH
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.05788423153692615,"1: Input: Set H = {ai · x = bi}i∈[t] of t hyperplanes in Rd, convex polytopic domain P ⊆Rd
given by a set of hyperplanes P, procedure
COMPUTELOCALLYRELEVANTSEPARATORS(x, H) with x ∈Rd.
Output: Partition cells ˜C = {˜c(j)} , c(j) ∈{0, 1, −1}t with |˜c(j)
i | = 1 iff ai · x = bi is a
bounding hyperplane for cell j, and sign(˜c(j)
i ) = sign(ai · xj −bi) for interior point xj,
sign(·) ∈{±1}.
2: x1 ←an arbitrary point in Rd (assumed general position w.r.t. H)"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.059880239520958084,"3: Cell c(1) with sign(c(1)
i ) = sign(ai · x1 −bi)
4: q ←empty queue; q.enqueue([c(1), x1])
5: C ←{}; ˜C ←{}
6: while q.non_empty() do
7:
[c, x] ←q.dequeue()
8:
Continue to next iteration if c ∈C
9:
C ←C ∪{c}
10:
˜H ←COMPUTELOCALLYRELEVANTSEPARATORS(x, H) ;
/* subset of
hyperplanes in H that can be facets for cell containing x */
11:
H′ ←{(−sign(ci)ai · x, −sign(ci)bi) | ai · x = bi ∈˜H} ∪P
12:
(˜c, neighbors) ←AUGMENTEDCLARKSON(x, H′, c);
/* Algorithm 2 */
13:
˜C ←˜C ∪{˜c}
14:
q.enqueue([c′, x′]) for each [c′, x′] ∈neighbors
15: return ˜C"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.06187624750499002,Algorithm 1 computes the set VH in time ˜O(dE + V TCLRS + tLRS · P
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.06387225548902195,"c∈VH LP(d, |Ic| + 1)), where
LP(r, s) denotes the time to solve an LP in r variables and s constraints, Ic denotes the number of
facets for cell c ∈VH, TCLRS denotes the running time of COMPUTELOCALLYRELEVANTSEPARA-
TORS and tLRS denotes an upper bound on the number of locally relevant hyperplanes in Line 10."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.0658682634730539,We illustrate the significance of output-sensitivity and locality-sensitivity in our results with examples.
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.06786427145708583,"Example 1. The worst-case size of VH is O(td) and standard (output-insensitive) enumeration
algorithms for computing VH (e.g. [EG86, Xu20]) take O(td) time even when the output size may be
much smaller. For example, if H is a collection of parallel planes in R3, the running time of these
approaches is O(t3). Even a naive implementation of COMPUTELOCALLYRELEVANTSEPARATORS
which always outputs the complete set H gives a better runtime of O(t2).
By employing a
straightforward algorithm which binary searches the closest hyperplanes to x (in a pre-processed
H) as COMPUTELOCALLYRELEVANTSEPARATORS we have tLRS = O(1) and TCLRS = ˜O(1), and
Algorithm 1 attains a running time of ˜O(t). Analogously, if H is a collection of t hyperplanes in Rd"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.06986027944111776,"with 1 ≤k < d distinct unit normal vectors, then output-sensitivity improves the runtime from O(td)
to O(tk+1), and locality-sensitivity can be used to further improve it to ˜O(tk)."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.0718562874251497,"ERM in the statistical learning setting.
We now use Algorithm 1 to compute the sample mimimum
(aka ERM, Empirical Risk Minimization) for the (F, t) piecewise-structured dual losses with linear
boundaries (Definition 1) over a problem sample S ∈Πm, provided piece functions in F can be
efficiently optimized over a polytope (typically the piece functions are constant or linear functions
in our examples). Formally, we define search-ERM for a given parameterized algorithm family A
with parameter space P and the dual class utility function being (F, t)-piecewise decomposable
(Definition 1) as follows: given a set of m problem instances S ∈Πm, compute the pieces, i.e. a
partition of the parameter space into connected subsets such that the utility function is a fixed piece
function in F over each subset for each of the m problem instances. The following result gives a
recipe for efficiently solving the search-ERM problem provided we can efficiently compute the dual
function pieces in individual problem instances, and the the number of pieces in the sum dual class
function over the sample S is not too large. The key idea is to apply Algorithm 1 for each problem
instance, and once again for the search-ERM problem. In the notation of Table 1, we have RΣ = |VS|,"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.07385229540918163,"the number of vertices in the cell adjacency graph corresponding to the polytopic pieces in the sum
utility function Pm
i=1 ui. |ES| = O(RΣ) when d = 2 and O(R2
Σ) for general d."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.07584830339321358,"Theorem 2.3. Let ˜Ci denote the cells partitioning the polytopic parameter space P ⊂Rd corre-
sponding to pieces of the dual class utility function ui on a single problem instance xi ∈Π, from
a collection s = {x1, . . . , xm} of m problem instances. Let (VS, ES) be the cell adjacency graph
corresponding to the polytopic pieces in the sum utility function Pm
i=1 ui. Then there is an algorithm
for computing VS given the cells ˜Ci in time ˜O((d+m)|ES|+mtLRS ·P"
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.07784431137724551,"c∈VS LP(d, |Ic|+1)), where
Ic denotes the number of facets for cell c ∈VS, and tLRS is the number of locally relevant hyperplanes
in a single instance."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.07984031936127745,"An important consequence of the above result is an efficient output-sensitive algorithm for data-driven
algorithm design when the dual class utility function is (F, t)-piecewise decomposable. In the
following sections, we will instantiate the above results for various data-driven parameter selection
problems where the dual class functions are piecewise-structured with linear boundaries (Definition 1).
Prior work [BSV18, BDL20, BDD+21] has shown polynomial bounds on the sample complexity of
learning near-optimal parameters via the ERM algorithm for these problems in the statistical learning
setting, i.e. the problem instances are drawn from a fixed unknown distribution. In other words, ERM
over polynomially large sample size m is sufficient for learning good parameters. In particular, we
will design and analyze running time for problem-specific algorithms for computing locally relevant
hyperplanes. Given Theorem 2.3, it will be sufficient to give an algorithm for computing the pieces
of the dual class function for a single problem instance."
OUTPUT-SENSITIVE CELL ENUMERATION AND DATA-DRIVEN ALGORITHM DESIGN,0.08183632734530938,"Remark 1. Our results in this section directly imply efficient algorithms for pricing problems
in mechanism design which are known to be (F, t)-decomposable where F is the class of linear
functions (summarized in Table 1, see Appendix F for details)."
LINKAGE-BASED CLUSTERING,0.08383233532934131,"3
Linkage-based clustering"
LINKAGE-BASED CLUSTERING,0.08582834331337326,"Clustering data into groups of similar points is a fundamental tool in data analysis and unsupervised
machine learning. A variety of clustering algorithms have been introduced and studied but it is not
clear which algorithms will work best on specific tasks. Also the quality of clustering is heavily
dependent on the distance metric used to compare data points. Interpolating multiple metrics and
clustering heuristics can result in significantly better clustering [BDL20]."
LINKAGE-BASED CLUSTERING,0.08782435129740519,"Problem setup. Let X be the data domain. A clustering instance from the domain consists of a point
set S = {x1, . . . , xn} ⊆X and an (unknown) target clustering C = (C1, . . . , Ck), where the sets
C1, . . . , Ck partition S into k clusters. Linkage-based clustering algorithms output a hierarchical
clustering of the input data, represented by a cluster tree. We measure the agreement of a cluster tree
T with the target clustering C in terms of the Hamming distance between C and the closest pruning
of T that partitions it into k clusters (i.e., k disjoint subtrees that contain all the leaves of T). More
formally, the loss ℓ(T, C) = minP1,...,Pk minσ∈Sn
1
|S|
Pk
i=1 |Ci \ Pσi|, where the first minimum is
over all prunings P1, . . . , Pk of the cluster tree T into k subtrees, and the second minimum is over all
permutations of the k cluster indices."
LINKAGE-BASED CLUSTERING,0.08982035928143713,"A merge function D defines the distance between a pair of clusters Ci, Cj ⊆X in terms of the
pairwise point distances given by a metric d. Cluster pairs with smallest values of the merge
function are merged first. For example, single linkage uses the merge function Dsgl(Ci, Cj; d) =
mina∈Ci,b∈Cj d(a, b) and complete linkage uses Dcmpl(Ci, Cj; d) = maxa∈Ci,b∈Cj d(a, b). Instead
of using extreme points to measure the distance between pairs of clusters, one may also use more cen-
tral points, e.g. we define median linkage as Dmed(Ci, Cj; d) = median({d(a, b) | a ∈Ci, b ∈Cj}),
where median(·) is the usual statistical median of an ordered set S ⊂R7. Appendix H provides
synthetic clustering instances where one of single, complete or median linkage leads to significantly
better clustering than the other two, illustrating the need to learn an interpolated procedure. Single,"
LINKAGE-BASED CLUSTERING,0.09181636726546906,"7median(S) is the smallest element of S such that S has at most half its elements less than
median(S) and at most half its elements more than median(S).
For comparison, the more well-
known average linkage is Davg(Ci, Cj; d) = mean({d(a, b) | a ∈Ci, b ∈Cj}).
We may also use
geometric medians of clusters.
For example, we can define mediod linkage as Dgeomed(Ci, Cj; d) =
d(argmina∈Ci
P"
LINKAGE-BASED CLUSTERING,0.09381237524950099,"a′∈Ci d(a, a′), argminb∈Cj
P"
LINKAGE-BASED CLUSTERING,0.09580838323353294,"b′∈Cj d(b, b′))."
LINKAGE-BASED CLUSTERING,0.09780439121756487,"median and complete linkage are 2-point-based (Definition 4, Appendix I), i.e. the merge function
D(A, B; d) only depends on the distance d(a, b) for two points (a, b) ∈A × B."
LINKAGE-BASED CLUSTERING,0.0998003992015968,"Parameterized algorithm families. Let ∆= {D1, . . . , Dl} denote a finite family of merge func-
tions (measure distances between clusters) and δ = {d1, . . . , dm} be a finite collection of distance
metrics (measure distances between points). We define a parameterized family of linkage-based
clustering algorithms that allows us to learn both the merge function and the distance metric. It
is given by the interpolated merge function D∆
α (A, B; δ) = P
Di∈∆,dj∈δ αi,jDi(A, B; dj), where
α = {αi,j | i ∈[l], j ∈[m], αi,j ≥0}. In order to ensure linear boundary functions for the dual
class function, our interpolated merge function D∆
α (A, B; δ) takes all pairs of distance metrics and
linkage procedures. Due to invariance under constant multiplicative factors, we can set P"
LINKAGE-BASED CLUSTERING,0.10179640718562874,"i,j αi,j = 1
and obtain a set of parameters which allows α to be parameterized by d = lm −1 values8. Define
the parameter space P = ▲d =
n
ρ ∈
 
R≥0d | P"
LINKAGE-BASED CLUSTERING,0.10379241516966067,"i ρi ≤1
o
; for any ρ ∈P we get α(ρ) ∈Rd+1"
LINKAGE-BASED CLUSTERING,0.10578842315369262,"as α(ρ)
i
= ρi for i ∈[d], α(ρ)
d+1 = 1 −P"
LINKAGE-BASED CLUSTERING,0.10778443113772455,"i∈[d] ρi. We focus on learning the optimal ρ ∈P for a single
instance (S, Y). With slight abuse of notation we will sometimes use D∆
ρ (A, B; δ) to denote the
interpolated merge function D∆
α(ρ)(A, B; δ). As a special case we have the family D∆
ρ (A, B; d0)
that interpolates merge functions (from set ∆) for different linkage procedures but the same distance
metric d0. Another interesting family only interpolates the distance metrics, i.e. use a distance metric
dρ(a, b) = P
dj∈δ α(ρ)
j dj(a, b) and use a fixed linkage procedure. We denote this by D1
ρ(A, B; δ).
We will extend the execution tree approach introduced by [BDL20] which computes the pieces
(intervals) of single-parameter linkage-based clustering. A formal treatment of the execution tree,
and how it is extended to the multi-parameter setting, is deferred to Appendix I.1. Informally, for
a single parameter, the execution tree is defined as the partition tree where each node represents an
interval where the first t merges are identical, and edges correspond to the subset relationship between
intervals obtained by refinement from a single merge. The execution, i.e. the sequence of merges,
is unique along any path of this tree. The same properties, i.e. refinement of the partition with each
merge and correspondence to the algorithm’s execution, continue to hold in the multidimensional
case, but with convex polytopes instead of intervals (see Definition 5). Computing the children of
any node of the execution tree corresponds to computing the subdivision of a convex polytope into
polytopic cells where the next merge step is fixed. The children of any node of the execution tree
can be computed using Algorithm 1. We compute the cells by following the neighbors, keeping track
of the cluster pairs merged for the computed cells to avoid recomputation. For any single cell, we
find the bounding polytope along with cluster pairs corresponding to neighboring cells by computing
the tight constraints in a system of linear inequalities. Theorem 2.2 gives the runtime complexity
of the proposed algorithm for computing the children of any node of the execution tree. It only
remains to specify COMPUTELOCALLYRELEVANTSEPARATORS. For a given x = ρ we find the
next merge candidate in time O(dn2) by computing the merge function D∆
ρ (A, B; δ) for all pairs of
candidate (unmerged) clusters A, B. If (A∗, B∗) minimizes the merge function, the locally relevant
hyperplanes are given by D∆
ρ (A∗, B∗; δ) ≤D∆
ρ (A′, B′; δ) for (A′, B′) ̸= (A∗, B∗) i.e. tLRS ≤n2.
Using Theorem 2.2, we give the following bound for the overall runtime of the algorithm (soft-O
notation suppresses logarithmic terms and multiplicative constants in d, proof in Appendix I.2)."
LINKAGE-BASED CLUSTERING,0.10978043912175649,"Theorem 3.1. Let S be a clustering instance with |S| = n, and let Ri = |Pi| and R = Rn. Let
Ht =
{(Q1, Q2) ∈P2
t | Q1 ∩Q2 ̸= ∅}
 denote the total number of adjacencies between any two
pieces of Pi and H = Hn. Then, the leaves of the execution tree on S can be computed in time
˜O
 Pn
i=1 (Hi + RiTM) (n −i + 1)2
, where TM is the time to compute the merge function."
LINKAGE-BASED CLUSTERING,0.11177644710578842,"In the case of single, median, and complete linkage, we may assume TM = O(d) by carefully
maintaining a hashtable containing distances between every pair of clusters. Each merge requires
overhead at most O(n2
t), nt = n −t being the number of unmerged clusters at the node at depth t,
which is absorbed by the cost of solving the LP corresponding to the cell of the merge. We have the
following corollary which states that our algorithm is output-linear for d = 2."
LINKAGE-BASED CLUSTERING,0.11377245508982035,"Corollary 3.2. For d = 2 the leaves of the execution tree of any clustering instance S with |S| = n
can be computed in time O(RTMn3).
Above results yield bounds on TS, the enumeration time for dual function of the pieces in a single
problem instance. Theorem 2.3 further implies bounds on the runtime of ERM (Table 1)."
LINKAGE-BASED CLUSTERING,0.1157684630738523,"8In contrast, the parametric family in [BDL20] has l + m −2 parameters but it does not satisfy Definition 1."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.11776447105788423,"4
Dynamic Programming based sequence alignment"
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.11976047904191617,"Sequence alignment is a fundamental combinatorial problem with applications to computational
biology.
For example, to compare two DNA, RNA or amino acid sequences the standard
approach is to align two sequences to detect similar regions and compute the optimal alignment
[NW70, Wat89, CB00]. However, the optimal alignment depends on the relative costs or weights
used for specific substitutions, insertions/deletions, or gaps (consecutive deletions) in the sequences.
Given a set of weights, the optimal alignment computation is typically a simple dynamic program.
Our goal is to learn the weights, such that the alignment produced by the dynamic program has
application-specific desirable properties."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.1217564870259481,"Problem setup.
Given a pair of sequences s1, s2 over some alphabet Σ of lengths m = |s1|
and n = |s2|, and a ‘space’ character −/∈Σ, a space-extension t of a sequence s over Σ is a
sequence over Σ ∪{−} such that removing all occurrences of −in t gives s. A global alignment
(or simply alignment) of s1, s2 is a pair of sequences t1, t2 such that |t1| = |t2|, t1, t2 are space-
extensions of s1, s2 respectively, and for no 1 ≤i ≤|t1| we have t1[i] = t2[i] = −. Let s[i]
denote the i-th character of a sequence s and s[: i] denote the first i characters of sequence s.
For 1 ≤i ≤|t1|, if t1[i] = t2[i] we call it a match. If t1[i] ̸= t2[i], and one of t1[i] or t2[i]
is the character −we call it a space, else it is a mismatch. A sequence of −characters (in t1
or t2) is called a gap. Matches, mismatches, gaps and spaces are commonly used features of an
alignment, i.e. functions that map sequences and their alignments (s1, s2, t1, t2) to Z≥0 (for example,
the number of spaces). A common measure of cost of an alignment is some linear combination
of features. For example if there are d features given by lk(·), k ∈[d], the cost may be given
by c(s1, s2, t1, t2, ρ) = Pd
k=1 ρklk(s1, s2, t1, t2) where ρ = (ρ1, . . . , ρd) are the parameters that
govern the relative weight of the features [KK06]. Let τ(s, s′, ρ) = argmint1,t2 c(s, s′, t1, t2, ρ) and
C(s, s′, ρ) = mint1,t2 c(s, s′, t1, t2, ρ) denote the optimal alignment and its cost respectively."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.12375249500998003,"A general DP update rule.
For a fixed ρ, suppose the sequence alignment problem can be
solved, i.e. we can find the alignment with the smallest cost, using a dynamic program Aρ with
linear parameter dependence (described below). Our main application will be to the family of
dynamic programs Aρ which compute the optimal alignment τ(s1, s2, ρ) given any pair of sequences
(s1, s2) ∈Σm × Σn = Π for any ρ ∈Rd, but we will proceed to provide a more general abstraction.
See Section J.1 for example DPs using well-known features in computational biology, expressed using
the abstraction below. For any problem (s1, s2) ∈Π, the dynamic program Aρ (ρ ∈P ⊆Rd, the
set of parameters) solves a set π(s1, s2) = {Pi | i ∈[k], Pk = (s1, s2)} of k subproblems (typically,
π(s1, s2) ⊆Πs1,s2 = {(s1[: i′], s2[: j′]) | i′ ∈{0, . . . , m}, j′ ∈{0, . . . , n}} ⊆Π) in some fixed
order P1, . . . , Pk = (s1, s2). Crucially, the subproblems sequence P1, . . . , Pk do not depend on
ρ9. In particular, a problem Pj can be efficiently solved given optimal alignments and their costs
(τi(ρ), Ci(ρ)) for problems Pi for each i ∈[j −1]. Some initial problems in the sequence P1, . . . , Pk
of subproblems are base case subproblems where the optimal alignment and its cost can be directly
computed without referring to a previous subproblem. To solve a (non base case) subproblem Pj,
we consider V alternative cases q : Π →[V ], i.e. Pj belongs to exactly one of the V cases (e.g. if
Pj = (s1[:i′], s2[:j′]), we could have two cases corresponding to s1[i′] = s2[j′] and s1[i′] ̸= s2[j′]).
Typically, V will be a small constant. For any case v = q(Pj) ∈[V ] that Pj may belong to, the
cost of the optimal alignment of Pj is given by a minimum over Lv terms of the form cv,l(ρ, Pj) =
ρ · wv,l + σv,l(ρ, Pj), where l ∈[Lv], wv,l ∈Rd, σv,l(ρ, Pj) = Ct(ρ) ∈{C1(ρ), . . . , Cj−1(ρ)} is
the cost of some previously solved subproblem Pt = (s1[:i′
t], s2[:j′
t]) = (s1[:i′
v,l,j], s2[:j′
v,l,j]) (i.e.
t depends on v, l, j but not on ρ), and cv,l(ρ, Pj) is the cost of alignment τv,l(ρ, Pj) = Tv,l(τt(ρ))
which extends the optimal alignment for subproblem Pt by a ρ-independent transformation Tv,l(·).
That is, the DP update for computing the cost of the optimal alignment takes the form
DP(ρ, Pj) = min
l {ρ · wq(Pj),l + σq(Pj),l(ρ, Pj)},
(1)"
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.12574850299401197,"and the optimal alignment is given by DP ′(ρ, Pj)
=
τq(Pj),l∗(ρ, Pj), where l∗
=
argminl{ρ · wq(Pj),l + σq(Pj),l(ρ, Pj)}. The DP specification is completed by including base cases
{C(s, s′, ρ) = ρ · ws,s′ | (s, s′) ∈B(s1, s2)} (or {τ(s, s′, ρ) = τs,s′ | (s, s′) ∈B(s1, s2)} for the"
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.1277445109780439,"9For the sequence alignment DP in Appendix J.1.1, we have π(s1, s2) = Πs1,s2 and we first solve the base
case subproblems (which have a fixed optimal alignment for all ρ) followed by problems (s1[:i′], s2[:j′]) in a
non-decreasing order of i′ + j′ for any value of ρ."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.12974051896207583,"optimal alignment DP) corresponding to a set of base case subproblems B(s1, s2) ⊆Πs1,s2. Let
L = maxv∈[V ] Lv denote the maximum number of subproblems needed to compute a single DP
update in any of the cases. L is often small, typically 2 or 3 (see examples in Section J.1). Our
main result is to provide an algorithm for computing the polytopic pieces of the dual class functions
efficiently for small constants d and L."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.1317365269461078,"As indicated above, we consider the family of dynamic programs Aρ which compute the optimal
alignment τ(s1, s2, ρ) given any pair of sequences (s1, s2) ∈Σm ×Σn = Π for any ρ ∈Rd. For any
alignment (t1, t2), the algorithm has a fixed real-valued utility (different from the cost function above)
which captures the quality of the alignment, i.e. the utility function u((s1, s2), ρ) only depends on the
alignment τ(s1, s2, ρ). The dual class function is piecewise constant with convex polytopic pieces
(Lemma J.5 in Appendix J.3). For any fixed problem (s1, s2), the space of parameters ρ can be parti-
tioned into R convex polytopic regions where the optimal alignment is fixed. The optimal parameter
can then be found by simply comparing the costs of the alignments in each of these pieces. For the
rest of this section we consider the algorithmic problem of computing these R pieces efficiently."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.13373253493013973,"For the clustering algorithm family, as we have seen in Section 3, we get a refinement of the parameter
space with each new step (merge) performed by the algorithm. This does not hold for the sequence
alignment problem. Instead we obtain the following DAG, from which the desired pieces can be
obtained by looking at nodes with no out-edges (call these terminal nodes). Intuitively, the DAG
is built by iteratively adding nodes corresponding to subproblems P1, . . . , Pk and adding edges
directed towards Pj from all subproblems that appear in the DP update for it. That is, for base case
subproblems, we have singleton nodes with no incoming edges. Using the recurrence relation (1), we
note that the optimal alignment for the pair of sequences (s1[:i], s2[:j]) can be obtained from the
optimal alignments for subproblems {(s1[:iv′,l], s2[:jv′,l])}l∈[Lv′] where v′ = q(s1[:i], s2[:j]). The
DAG for (s1[:i], s2[:j]) is therefore simply obtained by using the DAGs Gv′,l for the subproblems
and adding directed edges from the terminal nodes of Gv′,l to new nodes vp,i,j corresponding to each
piece p of the partition P[i][j] of P given by the set of pieces of u(s1[:i],s2[:j])(ρ). A more compact
representation of the execution graph would have only a single node vi,j for each subproblem
(s1[:i], s2[:j]) (the node stores the corresponding partition P[i][j]) and edges directed towards vi,j
from nodes of subproblems used to solve (s1[:i], s2[:j]). Note that the graph depends on the problem
instance (s1, s2) as the relevant DP cases v′ = q(s1[:i], s2[:j]) depend on the sequences s1, s2. A
naive way to encode the execution graph would be an exponentially large tree corresponding to the
recursion tree of the recurrence relation (1)."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.13572854291417166,"Execution DAG. Formally we define a compact execution graph Ge = (Ve, Ee) as follows. For the
base cases, we have nodes labeled by (s, s′) ∈B(s1, s2) storing the base case solutions (ws,s′, τs,s′)
over the unpartitioned parameter space P = Rd. For i, j > 0, we have a node vi,j labeled by
(s1[:i], s2[:j]) and the corresponding partition P[i][j] of the parameter space, with incoming edges
from nodes of the relevant subproblems {(s1[:iv′,l], s2[:jv′,l])}l∈[Lv′] where v′ = q(s1[:i], s2[:j]).
This graph is a DAG since every directed edge is from some node vi,j to a node vi′,j′ with i′+j′ > i+j
in typical sequence alignment dynamic programs (Appendix J.1). Algorithm 5 (Appendix J.2)) gives a
procedure to compute the partition of the parameter space for any given problem instance (s1, s2) us-
ing the compact execution DAG. We give intuitive overviews of the three main routines in Algorithm 5."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.1377245508982036,"• COMPUTEOVERLAYDP computes an overlay Pi of the input polytopic subdivisions {Ps | s ∈Si}
and uses Clarkson’s algorithm for intersecting polytopes with output-sensitive efficiency. We show
that the overlay can be computed by solving at most ˜RL linear programs (Algorithm 6, Lemma J.1).
• COMPUTESUBDIVISIONDP applies Algorithm 1, in each piece of the overlay we need to find the
polytopic subdivision induced by O(L2) hyperplanes (the set of hyperplanes depends on the piece).
This works because all relevant subproblems have the same solution within any piece of the overlay.
• Finally RESOLVEDEGENERACIESDP merges pieces where the optimal alignment is identical
using a simple search over the resulting subdivision."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.13972055888223553,"For our implementation of the subroutines, we have the following guarantee for Algorithm 5.
Theorem 4.1. Let Ri,j denote the number of pieces in P[i][j], and ˜R = maxi≤m,j≤n Ri,j. If the time
complexity for computing the optimal alignment is O(TDP), then Algorithm 5 can be used to compute
the pieces for the dual class function for any problem instance (s1, s2), in time O(d!L4d ˜R2L+1TDP)."
DYNAMIC PROGRAMMING BASED SEQUENCE ALIGNMENT,0.14171656686626746,"For the special case of d = 2, we show that (Theorem J.6, Appendix J.3) the pieces may be computed
in O(RTDP) time using the ray search technique of [Meg78]."
ACKNOWLEDGMENTS,0.1437125748502994,"5
Acknowledgments"
ACKNOWLEDGMENTS,0.14570858283433133,"We thank Dan DeBlasio for useful discussions on the computational biology literature. We also thank
Avrim Blum and Mikhail Khodak for helpful feedback. This material is based on work supported
by the National Science Foundation under grants CCF-1910321, IIS-1901403, and SES-1919453;
the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003;
a Simons Investigator Award; an AWS Machine Learning Research Award; an Amazon Research
Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship."
REFERENCES,0.14770459081836326,References
REFERENCES,0.1497005988023952,"[AF96] David Avis and Komei Fukuda. Reverse search for enumeration. Discrete applied
mathematics, 65(1-3):21–46, 1996."
REFERENCES,0.15169660678642716,"[Bal20] Maria-Florina Balcan. Data-Driven Algorithm Design. In Tim Roughgarden, editor,
Beyond Worst Case Analysis of Algorithms. Cambridge University Press, 2020."
REFERENCES,0.1536926147704591,"[BB24] Maria-Florina Balcan and Hedyeh Beyhaghi. Learning revenue maximizing menus of
lotteries and two-part tariffs. Transactions on Machine Learning Research (TMLR),
2024."
REFERENCES,0.15568862275449102,"[BBSZ23] Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, and Hongyang Zhang. An
analysis of robustness of non-lipschitz networks. Journal of Machine Learning Research
(JMLR), 24(98):1–43, 2023."
REFERENCES,0.15768463073852296,"[BDD+21] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm,
and Ellen Vitercik. How much data is sufficient to learn high-performing algorithms?
Generalization guarantees for data-driven algorithm design. In Symposium on Theory of
Computing (STOC), pages 919–932, 2021."
REFERENCES,0.1596806387225549,"[BDL20] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. In International
Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.16167664670658682,"[BDS20] Maria-Florina Balcan, Travis Dick, and Dravyansh Sharma. Learning piecewise Lip-
schitz functions in changing environments. In International Conference on Artificial
Intelligence and Statistics, pages 3567–3577. PMLR, 2020."
REFERENCES,0.16367265469061876,"[BDS21] Avrim Blum, Chen Dan, and Saeed Seddighin. Learning complexity of simulated an-
nealing. In International Conference on Artificial Intelligence and Statistics (AISTATS),
pages 1540–1548. PMLR, 2021."
REFERENCES,0.1656686626746507,"[BDSV18] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning
to branch. In International Conference on Machine Learning (ICML), pages 344–353.
PMLR, 2018."
REFERENCES,0.16766467065868262,"[BDV18] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven
algorithm design, online learning, and private optimization. In 2018 IEEE 59th Annual
Symposium on Foundations of Computer Science (FOCS), pages 603–614. IEEE, 2018."
REFERENCES,0.16966067864271456,"[BFM97] David Bremner, Komei Fukuda, and Ambros Marzetta. Primal-dual methods for vertex
and facet enumeration (preliminary version). In Symposium on Computational Geometry
(SoCG), pages 49–56, 1997."
REFERENCES,0.17165668662674652,"[BIW22] Peter Bartlett, Piotr Indyk, and Tal Wagner. Generalization bounds for data-driven
numerical linear algebra. In Conference on Learning Theory (COLT), pages 2013–2040.
PMLR, 2022."
REFERENCES,0.17365269461077845,"[BKST22] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar.
Provably tuning the ElasticNet across instances. Advances in Neural Information
Processing Systems, 35:27769–27782, 2022."
REFERENCES,0.17564870259481039,"[BNVW17] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-
theoretic foundations of algorithm configuration for combinatorial partitioning problems.
In Conference on Learning Theory (COLT), pages 213–274. PMLR, 2017."
REFERENCES,0.17764471057884232,"[BPS20] Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. Efficient algorithms
for learning revenue-maximizing two-part tariffs. In International Joint Conferences on
Artificial Intelligence (IJCAI), pages 332–338, 2020."
REFERENCES,0.17964071856287425,"[BS21] Maria-Florina Balcan and Dravyansh Sharma. Data driven semi-supervised learning.
Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.18163672654690619,"[BSV16] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of
automated mechanism design. Advances in Neural Information Processing Systems, 29,
2016."
REFERENCES,0.18363273453093812,"[BSV18] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of
sample complexity for multi-item profit maximization. In Economics and Computation
(EC), pages 173–174, 2018."
REFERENCES,0.18562874251497005,"[Buc43] Robert Creighton Buck. Partition of space. The American Mathematical Monthly,
50(9):541–544, 1943."
REFERENCES,0.18762475049900199,"[CAK17] Vincent Cohen-Addad and Varun Kanade. Online optimization of smoothed piecewise
constant functions. In Artificial Intelligence and Statistics, pages 412–420. PMLR,
2017."
REFERENCES,0.18962075848303392,"[CB00] Peter Clote and Rolf Backofen. Computational Molecular Biology: An Introduction.
John Wiley Chichester; New York, 2000."
REFERENCES,0.19161676646706588,"[CE92] Bernard Chazelle and Herbert Edelsbrunner. An optimal algorithm for intersecting line
segments in the plane. Journal of the ACM (JACM), 39(1):1–54, 1992."
REFERENCES,0.1936127744510978,"[Cha96] Timothy M Chan. Optimal output-sensitive convex hull algorithms in two and three
dimensions. Discrete & Computational Geometry, 16(4):361–368, 1996."
REFERENCES,0.19560878243512975,"[Cha18] Timothy M Chan. Improved deterministic algorithms for linear programming in low
dimensions. ACM Transactions on Algorithms (TALG), 14(3):1–10, 2018."
REFERENCES,0.19760479041916168,"[Cla94] Kenneth L Clarkson. More output-sensitive geometric algorithms. In Symposium on
Foundations of Computer Science (FOCS), pages 695–702. IEEE, 1994."
REFERENCES,0.1996007984031936,"[DF12] Rodney G Downey and Michael Ralph Fellows. Parameterized complexity. Springer
Science & Business Media, 2012."
REFERENCES,0.20159680638722555,"[EG86] Herbert Edelsbrunner and Leonidas J Guibas. Topologically sweeping an arrangement.
In Symposium on Theory of Computing (STOC), pages 389–403, 1986."
REFERENCES,0.20359281437125748,"[Fer02] Henning Fernau. On parameterized enumeration. In International Computing and
Combinatorics Conference, pages 564–573. Springer, 2002."
REFERENCES,0.2055888223552894,"[FGS+19] Henning Fernau, Petr Golovach, Marie-France Sagot, et al. Algorithmic enumera-
tion: Output-sensitive, input-sensitive, parameterized, approximative (Dagstuhl Seminar
18421). In Dagstuhl Reports, volume 8. Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, 2019."
REFERENCES,0.20758483033932135,"[GBN94] Dan Gusfield, Krishnan Balasubramanian, and Dalit Naor. Parametric optimization of
sequence alignment. Algorithmica, 12(4):312–326, 1994."
REFERENCES,0.20958083832335328,"[GR16] Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm
selection. In Innovations in Theoretical Computer Science Conference (ITCS), 2016."
REFERENCES,0.21157684630738524,"[GR20] Rishi Gupta and Tim Roughgarden. Data-driven algorithm design. Communications of
the ACM, 63(6):87–94, 2020."
REFERENCES,0.21357285429141717,"[GS96] Dan Gusfield and Paul Stelling. Parametric and inverse-parametric sequence alignment
with XPARAL. Methods in Enzymology, 266:481–494, 1996."
REFERENCES,0.2155688622754491,"[KK06] John Kececioglu and Eagu Kim. Simple and fast inverse alignment. In Annual Interna-
tional Conference on Research in Computational Molecular Biology, pages 441–455.
Springer, 2006."
REFERENCES,0.21756487025948104,"[KKW10] John Kececioglu, Eagu Kim, and Travis Wheeler. Aligning protein sequences with
predicted secondary structure. Journal of Computational Biology, 17(3):561–580, 2010."
REFERENCES,0.21956087824351297,"[KPT+21] Krishnan Kumaran, Dimitri J Papageorgiou, Martin Takac, Laurens Lueg, and Nicolas V
Sahinidis. Active metric learning for supervised classification. Computers & Chemical
Engineering, 144:107132, 2021."
REFERENCES,0.2215568862275449,"[Lew41] W Arthur Lewis. The two-part tariff. Economica, 8(31):249–270, 1941."
REFERENCES,0.22355289421157684,"[Meg78] Nimrod Megiddo. Combinatorial optimization with rational objective functions. In
Symposium on Theory of Computing (STOC), pages 1–12, 1978."
REFERENCES,0.22554890219560877,"[MR15] Jamie H Morgenstern and Tim Roughgarden. On the pseudo-dimension of nearly
optimal auctions. Advances in Neural Information Processing Systems, 28, 2015."
REFERENCES,0.2275449101796407,"[Nak04] Shin-ichi Nakano. Efficient generation of triconnected plane triangulations. Computa-
tional Geometry, 27(2):109–122, 2004."
REFERENCES,0.22954091816367264,"[NW70] Saul B Needleman and Christian D Wunsch. A general method applicable to the search
for similarities in the amino acid sequence of two proteins. Journal of Molecular
Biology, 48(3):443–453, 1970."
REFERENCES,0.2315369261477046,"[Oi71] Walter Y Oi. A Disneyland dilemma: Two-part tariffs for a Mickey Mouse monopoly.
The Quarterly Journal of Economics, 85(1):77–96, 1971."
REFERENCES,0.23353293413173654,"[RC18] Miroslav Rada and Michal Cerny. A new algorithm for enumeration of cells of hyper-
plane arrangements and a comparison with Avis and Fukuda’s reverse search. SIAM
Journal on Discrete Mathematics, 32(1):455–473, 2018."
REFERENCES,0.23552894211576847,"[SC11] Shiliang Sun and Qiaona Chen. Hierarchical distance metric learning for large mar-
gin nearest neighbor classification. International Journal of Pattern Recognition and
Artificial Intelligence (IJPRAI), 25:1073–1087, 11 2011."
REFERENCES,0.2375249500998004,"[Sei91] Raimund Seidel. Small-dimensional linear programming and convex hulls made easy.
Discrete & Computational Geometry, 6:423–434, 1991."
REFERENCES,0.23952095808383234,"[SJ23] Dravyansh Sharma and Maxwell Jones.
Efficiently learning the graph for semi-
supervised learning. The Conference on Uncertainty in Artificial Intelligence (UAI),
2023."
REFERENCES,0.24151696606786427,"[Sle99] Nora H Sleumer. Output-sensitive cell enumeration in hyperplane arrangements. Nordic
Journal of Computing, 6(2):137–147, 1999."
REFERENCES,0.2435129740518962,"[Syr17] Vasilis Syrgkanis. A sample complexity measure with applications to learning optimal
auctions. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.24550898203592814,"[Wat89] Michael S Waterman. Mathematical methods for DNA sequences. Boca Raton, FL
(USA); CRC Press Inc., 1989."
REFERENCES,0.24750499001996007,"[WS09] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin
nearest neighbor classification. Journal of Machine Learning Research (JMLR), 10(2),
2009."
REFERENCES,0.249500998003992,"[Xu20] Haifeng Xu. On the tractability of public persuasion with no externalities. In Symposium
on Discrete Algorithms (SODA), pages 2708–2727. SIAM, 2020."
REFERENCES,0.25149700598802394,"A
Additional insights and challenges"
REFERENCES,0.25349301397205587,"We first present an algorithm for enumerating the cells induced by a finite collection of hyperplanes in
Rd in output-sensitive time. Our approach is based on an output-sensitive algorithm for determining
the non-redundant constraints in a linear system due to Clarkson [Cla94]. At any point, we compute
the locally relevant hyperplanes that constitute the candidate hyperplanes which could bound the cell
(or piece) containing that point, using a problem-specific sub-routine, and apply Clarkson’s algorithm
over these to determine the facet-inducing hyperplanes for the cell as well as a collection of points
in neighboring cells. This allows us to compute all the induced cells by traversing an implicit cell
adjacency graph (Definition 2) in a breadth-first order. Our approach gives a recipe for computing
the pieces of piecewise structured dual class functions with linear boundaries, which can be used
to find the best parameters over a single problem instance. We further show how to compute the
pieces of the sum dual class function for a collection of problem instances in output-sensitive time,
by applying our approach to the collection of facet-inducing hyperplanes from each problem instance.
Our approach is useful for several mechanism design problems which are known to be piecewise
structured [BSV18], and we instantiate our results for two-part tariff pricing and item pricing with
anonymous prices. For the single menu two-part tariff case (d = 2), we use additional structure of
the dual class function to give a further improvement in the running time."
REFERENCES,0.2554890219560878,"For linkage-based clustering, we extend the execution tree based approach of [BDL20] for single-
parameter families to d > 1. The key idea is that linkage-based clustering algorithms involve a
sequence of steps (called merges), and the algorithmic decision at any step can only refine the partition
of the parameter space corresponding to a fixed sequence of steps so far. Thus, the pieces can be
viewed as leaves of a tree whose nodes at depth t correspond to a partition of the parameter space
such that the first t steps of the algorithm are identical in each piece of the partition. While in the
single parameter family the pieces are intervals, we approach the significantly harder challenge of
efficiently computing convex polytopic subdivisions (formally Definition 6). We essentially compute
the execution tree top-down for any given problem instance, and use our hyperplane cell enumeration
to compute the children nodes for any node of the tree."
REFERENCES,0.25748502994011974,"We further extend the execution tree approach to a problem where all the algorithmic states corre-
sponding to different parameter values cannot be concisely represented via a tree. For dynamic-
programming based sequence alignment, we define a compact representation of the execution graph
and use primitives from computational geometry, in particular for overlaying two convex subdivisions
in output-polynomial time. A key challenge in this problem is that the execution graph is no longer a
tree but a DAG, and we need to design an efficient representation for this DAG. We cannot directly
apply Clarkson’s algorithm since the number of possible alignments of two strings (and therefore
the number of candidate hyperplanes) is exponential in the size of the strings. Instead, we carefully
combine the polytopes of subproblems in accordance with the dynamic program update for solving
the sequence alignment (for a fixed value of parameters), and the number of candidate hyperplanes is
output-sensitive inside regions where all the relevant subproblems have fixed optimal alignments."
REFERENCES,0.25948103792415167,"B
Additional related work"
REFERENCES,0.26147704590818366,"Data-driven algorithm design. The framework for data-driven design was introduced by [GR16] and
is surveyed in [Bal20]. It allows the design of algorithms with powerful formal guarantees when used
repeatedly on inputs consisting of related problem instances, as opposed to designing and analyzing
for worst-case instances. Data-driven algorithms have been proposed and analyzed for a wide
variety of combinatorial problems, including clustering [BDL20], computational biology [BDD+21],
mechanism design [BSV18], mixed integer linear programming [BDSV18], semi-supervised learning
[BS21, SJ23], linear regression [BKST22], low-rank approximation [BIW22] and adversarially
robust learning [BBSZ23]. Typically these are NP-hard problems with a variety of known heuristics.
Data-driven design provides techniques to select the heuristic (from a collection or parameterized
family) which is best suited for data coming from some domain. Tools for data-driven design
have also been studied in the online learning setting where problems arrive in an online sequence
[CAK17, BDV18, BDS20]. In this work, we focus on the statistical learning setting where the
“related” problems are drawn from a fixed but unknown distribution."
REFERENCES,0.2634730538922156,"Linkage-based clustering. [BNVW17] introduce several single parameter families to interpolate
well-known linkage procedures for linkage-based clustering, and study the sample complexity of"
REFERENCES,0.2654690618762475,"learning the parameter. [BDL20] consider families with multiple parameters, and also consider the
interpolation of different distance metrics. However the proposed execution-tree based algorithm for
computing the constant-performance regions in the parameter space only applies to single-parameter
families, and therefore the results can be used to interpolate linkage procedures or distance metrics,
but not both. Our work removes this limitation and provides algorithms for general multi-parameter
families."
REFERENCES,0.26746506986027946,"Other approaches to learning a distance metric to improve the quality of clustering algorithms have
been explored. For instance, [WS09] and [KPT+21] consider versions of global distance metric
learning, a technique to learn a linear transformation of the input space before applying a known
distance metric. [SC11] also consider a hierarchical distance metric learning, which allows the learned
distance function to depend on already-merged clusters. For such distance learning paradigms, the
underlying objective function is often continuous, admitting gradient descent techniques. However,
in our setting, the objective function is neither convex nor continuous in ρ; instead, it is piecewise
constant for convex pieces. As a result, instead of relying on numerical techniques for finding the
optimum ρ, our multidimensional approach enumerates all the pieces for which the objective function
is constant. Furthermore, this technique determines the exact optimum ρ rather than an approximation;
one consequence of doing so is that our analysis can be extended to apply generalization guarantees
from [BDL20] when learning the optimal ρ over a family of instances."
REFERENCES,0.2694610778443114,"Sequence alignment. For the problem of aligning pairs of genomic sequences, one typically obtains
the best alignment using a dynamic program with some costs or weights assigned to edits of different
kinds, such as insertions, substitutions, or reduplications [Wat89]. Prior work has considered learning
these weights by examining the cost of alignment for the possible weight settings. [BDD+21]
considers the problem for how many training examples (say pairs of DNA sequences with known
ancestral or evolutionary relationship) are needed to learn the best weights for good generalization on
unseen examples. More closely related to our work is the line of work which proposes algorithms
which compute the partition of the space of weights which result in different optimal alignments
[GBN94, GS96]. In this work, we propose a new algorithm which computes this partition more
efficiently."
REFERENCES,0.2714570858283433,"Pricing problems. Data-driven algorithms have been proposed and studied for automated mech-
anism design [MR15, BSV18, BDV18]. Prior work on designing multi-dimensional mechanisms
has focused mainly on the generalization guarantees, and include studying classes of structured
mechanisms for revenue maximization [MR15, BSV16, Syr17, BSV18]. Computationally efficient
algorithms have been proposed for the two-part tariff pricing problem [BPS20]. Our algorithms
have output-sensitive complexity and are more efficient than [BPS20] even for worst-case outputs.
Also, our techniques are more general and apply to mechanism design besides two-part tariff pricing.
[BB24] consider discretization-based techniques which are not output-sensitive and known to not
work in problems beyond mechanism design, for example, data-driven clustering [BNVW17]."
REFERENCES,0.27345309381237526,"On cell enumeration of hyperplane arrangements. A finite set of t hyperplanes in Rd induces a
collection of faces in dimensions ≤d, commonly referred to as an arrangement. A well-known
inductive argument implies that the number of d-faces, or cells, in an arrangement of t hyperplanes in
Rd is at most O(td). Several algorithms, ranging from topological sweep based [EG86] to incremental
construction based [Xu20], can enumerate all the cells (typically represented by collections of facet-
inducing hyperplanes) in optimal worst-case time O(td). The first output-sensitive algorithm proposed
for this problem was the reverse search based approach of [AF96], which runs in O(tdR · LP(d, t))
time, where R denotes the output-size (number of cells in the output) and LP(d, t) is the time for
solving a linear program in d variables and t constraints. This was further improved by a factor
of t via a more refined reverse-search [Sle99], and by additive terms via a different incremental
construction based approach by [RC18]. We propose a novel approach for cell enumeration based on
Clarkson’s algorithm [Cla94] for removing redundant constraints from linear systems of inequalities,
which asymptotically matches the worst case output-sensitive running times of these algorithms
while improving over their runtime in the presence of additional structure possessed by the typical
piecewise structured (Definition 1) loss functions encountered in data-driven algorithm design."
REFERENCES,0.2754491017964072,"C
Output-sensitive Parameterized Complexity"
REFERENCES,0.2774451097804391,"Output-sensitive algorithms have a running time that depends on the size of the output for any input
problem instance. Output-sensitive analysis is frequently employed in computational geometry, for
example Chan’s algorithm [Cha96] computes the convex hull of a set of 2-dimensional points in time
O(n log R), where R is the size of the (output) convex hull. Output-sensitive algorithms are useful if
the output size is variable, and ‘typical’ output instances are much smaller than worst case instances.
Parameterized complexity extends classical complexity theory by taking into account not only the
total input length n, but also other aspects of the problem encoded in a parameter k10. The motivation
is to confine the super-polynomial runtime needed for solving many natural problems strictly to the
parameter. Formally, a parameterized decision problem (or language) is a subset L ⊆Σ∗× N, where
Σ is a fixed alphabet, i.e. an input (x, k) to a parameterized problem consists of two parts, where the
second part k is the parameter. A parameterized problem L is fixed-parameter tractable if there exists
an algorithm which on a given input (x, k) ∈Σ∗× N, decides whether (x, k) ∈L in f(k) · poly(|x|)
time, where f is an arbitrary computable function in k [DF12]. FPT is the class of all parameterized
problems which are fixed-parameter tractable. In contrast, the class XP (aka slicewise-polynomial)
consists of problems for which there is an algorithm with running time |x|f(k). It is known that FPT
⊊XP. We consider an extension of the above to search problems and incorporate output-sensitivity in
the following definition.
Definition 3 (Output-polynomial Fixed Parameter Tractable). A parameterized search problem
P : Σ∗× N →˜Σ∗is said to be output-polynomial fixed-parameter tractable if there exists an
algorithm which on a given input (x, k) ∈Σ∗× N, computes the output P(x, k) ∈˜Σ∗in time
f(k) · poly(|x|, R), where R = |P(x, k)| is the output size and f is an arbitrary computable function
in k."
REFERENCES,0.27944111776447106,"As discussed above, output-sensitvity and fixed-parameter tractability both offer more fine-grained
complexity analysis than traditional (input) polynomial time complexity. Both techniques have been
employed in efficient algorithmic enumeration [Fer02, Nak04] and have gathered recent interest
[FGS+19]. In this work, we consider the optimization problem of selecting tunable parameters over
a continuous domain C ⊂Rd with the fixed-parameter k = d, the number of tunable parameters. We
will design OFPT enumeration algorithms which, roughly speaking, output a finite “search space”
which can be used to easily find the best parameter for the problem instance."
REFERENCES,0.281437125748503,"D
Augmented Clarkson’s algorithm"
REFERENCES,0.2834331337325349,"We describe here the details of the Augmented Clarkson’s algorithm, which modifies the algorithm of
Clarkson [Cla94] with additional bookkeeping needed for tracking the partition cells in Algorithm 1.
The underlying problem solved by Clarkson’s algorithm may be stated as follows."
REFERENCES,0.28542914171656686,"Problem Setup. Given a linear system of inequalities Ax ≤b, an inequality Aix ≤bi is said to be
redundant in the system if the set of solutions is unchanged when the inequality is removed from
the system. Given a system (A ∈Rm×d, b ∈Rm), find an equivalent system with no redundant
inequalities."
REFERENCES,0.2874251497005988,"Note that to test if a single inequality Aix ≤bi is redundant, it is sufficient to solve the following LP
in d variables and m constraints.
maximize
Aix
subject to
Ajx ≤bj,
∀j ∈[m] \ {i}
Aix ≤bi + 1
(2)"
REFERENCES,0.2894211576846307,"Using this directly to solve the redundancy removal problem gives an algorithm with running time
m · LP(d, m), where LP(d, m) denotes the time to solve an LP in d variables and m constraints.
This can be improved using Clarkson’s algorithm if the number of non-redundant constraints s is
much less than the total number of constraints m (Theorem 2.1)."
REFERENCES,0.29141716566866266,"We assume that an interior point z ∈Rd satisfying Ax < b is given. At a high level, one maintains
the set of non-redundant constraints I discovered so far i.e. Aix ≤bi is not redundant for each"
REFERENCES,0.2934131736526946,"10N.B. The term “parameter” is overloaded. It is used to refer to the real-valued parameters in the algorithm
family, as well as the parameter to the optimization problem over the algorithm family."
REFERENCES,0.2954091816367265,"Algorithm 2: AUGMENTEDCLARKSON(z, H = (A, b), c)"
REFERENCES,0.29740518962075846,"Input: A ∈Rm×d, b ∈Rm, z ∈Rd, sign-pattern c ∈{0, 1, −1}m.
Output: list of non-redundant hyperplanes I ⊆[m], points in neighboring cells Z ⊂Rd.
I ←∅, J ←[m], Z ←∅
while J ̸= ∅do"
REFERENCES,0.2994011976047904,"Select k ∈J
Detect if Akx ≤bk is redundant in AI∪{k}x ≤bI∪{k} by solving LP (2).
x∗←optimal solution of the above LP
if redundant then"
REFERENCES,0.3013972055888224,"J ←J \ {k}
j, z∗←RayShoot(A, b, z, x∗);
/* Computes j such that Ajx = bj is a
facet-inducing hyperplane hit by ray from z along x∗−z */
J ←J \ {j}
c′
j ←−cj, c′
i ←ci ∀i ∈[m] \ {j}
I ←I ∪{j}, Z ←Z ∪{[c′, z∗]}
return I, Z"
REFERENCES,0.3033932135728543,"i ∈I. When testing a new index k, the algorithm solves an LP of the form 2 and either detects that
Akx ≤bk is redundant, or finds index j ∈[m] \ I such that Ajx ≤bj is non-redundant. The latter
involves the use of a procedure RayShoot(A, b, z, x) which finds the non-redundant hyperplane hit
by a ray originating from z in the direction x −z (x ∈Rd) in the system A, b. The size of the LP
needed for this test is LP(d, |I| + 1) from which the complexity follows."
REFERENCES,0.30538922155688625,"To implement the RayShoot procedure, we can simply find the intersections of the ray x∗−z with
the hyperplanes Ajx ≤bj and output the one closest to z (defining the cell facet in that direction).
We also output an interior point from the adjacent cell during this computation, which saves us time
relative to [Sle99] where the interior point is computed for each cell (our Clarkson based approach
also circumvents their need for the Raindrop procedure [BFM97]). Finally we state the running time
guarantee of Algorithm 2, which follows from the original result of Clarkson [Cla94]."
REFERENCES,0.3073852295409182,Algorithm 3: RayShoot
REFERENCES,0.3093812375249501,"Input: A ∈Rm×d, b ∈Rm, z ∈Rd, x ∈Rd.
if Ai · (x −z) = 0 for some i then"
REFERENCES,0.31137724550898205,"z ←z + (ϵ, ϵ2, . . . , ϵd) for sufficiently small ϵ ;
/* Ensure general position.
*/"
REFERENCES,0.313373253493014,"ti ←
bi−Ai·z
Ai·(x−z);
/* intersection of z + t(x −z) with Aix = bi */
j = argmini{ti | ti > 0}, t′ = max{mini̸=j{ti | ti > 0}, 0}
return j, z + tj+t′"
REFERENCES,0.3153692614770459,"2
(x −z)"
REFERENCES,0.31736526946107785,"Theorem D.1 (Augmented Clarkson’s algorithm). Given a list L of k half-space constraints in
d dimensions, Algorithm 2 outputs the set I ⊆[L] of non-redundant constraints in L, as well as
auxiliary neighbor information, in time O(k · LP(d, |I| + 1)), where LP(v, c) is the time for solving
an LP with v variables and c constraints."
REFERENCES,0.3193612774451098,"E
Additional details and proofs from Section 2"
REFERENCES,0.3213572854291417,"Proof of Theorem 2.2. Algorithm 1 maintains a set of visited or explored cells in C, and their bound-
ing hyperplanes (corresponding to cell facets) in ˜C. It also maintains a queue of cells, such that each
cell in the queue has been discovered in Line 12 as a neighbor of some visited cell in C, but is yet to
be explored itself. The algorithm detects if a cell has been visited before by using its sign pattern on
the hyperplanes in H. This can be done in O(d log t) time, since |C| = O(td) using a well-known
combinatorial fact (e.g. [Buc43]). For a new cell c, we run AUGMENTEDCLARKSON to compute its
bounding hyperplanes Ic as well as sign patterns for neighbors in time O(tLRS · LP(d, |Ic| + 1)) by
Theorem 2.1. The computed neighbors are added to the queue in Line 14. A cell c gets added to the
queue at this step up to |Ic| times, but is not explored if already visited. Thus we run up to V iterations"
REFERENCES,0.32335329341317365,"of AUGMENTEDCLARKSON and up to 1 + P
c∈VH |Ic| = 2E + 1 queue insertions/removals. Using
efficient union-find data-structures, the set union and membership queries (for C and ˜C) can be done
in ˜O(1) time per iteration of the loop. So total time over the V cell explorations and no more than
2E + 1 iterations of the while loop is ˜O(dE) + O(tLRS · P"
REFERENCES,0.3253493013972056,"c∈VH LP(d, |Ic| + 1)) + O(V TCLRS)."
REFERENCES,0.3273453093812375,"Proof of Theorem 2.3. We will apply Algorithm 1 and compute the locally relevant hyperplanes by
simply taking a union of the facet-inducing hyperplanes at any point x, across the problem instances."
REFERENCES,0.32934131736526945,"Let G(i) = (V (i), E(i)) denote the cell adjacency graph for the cells in ˜Ci. We apply Algorithm 1
implicitly over H = ∪iH
(i) where H
(i) is the collection of facet-inducing hyperplanes in ˜Ci. To
implement COMPUTELOCALLYRELEVANTSEPARATORS(x, H) we simply search the computed
partition cell ˜Ci for the cell containing x in each problem instance xi in time O(P"
REFERENCES,0.3313373253493014,"i |E(i)|) =
O(m|ES|), obtain the set of corresponding facet-inducing hyperplanes H(i)
x , and output ˜Hx =
∪iH(i)
x
in time O(mtLRS). The former step only needs to be computed once, as the partition cells
for subsequent points can be tracked in O(m) time. Theorem 2.2 now gives a running time of
˜O(d|ES| + m|ES| + mtLRS|VS| + mtLRS · P"
REFERENCES,0.3333333333333333,"c∈VS LP(d, |Ic| + 1))."
REFERENCES,0.33532934131736525,"F
Profit maximization in pricing problems"
REFERENCES,0.3373253493013972,"Prior work [MR15, BSV18] on data-driven mechanism design has shown that the profit as a function
of the prices (parameters) is (F, t)-decomposable with F the set of linear functions on Rd for a
large number of mechanism classes (named (d, t)-delineable mechanisms in [BSV18]). We will
instantiate our approach for multi-item pricing problems which are (F, t)-decomposable and analyse
the running times. In contrast, recent work [BB24] employs data-independent discretization for
computationally efficient data-driven algorithm design for mechanism design problems even in the
worst-case. This discretization based approach is not output-sensitive and is known to not work well
for other applications like data-driven clustering."
REFERENCES,0.3393213572854291,"F.1
Two-part tariff pricing"
REFERENCES,0.3413173652694611,"In the two-part tariff problem [Lew41, Oi71], the seller with multiple identical items charges a fixed
price, as well as a price per item purchased. For example, cab meters often charge a base cost for any
trip and an additional cost per mile traveled. Subscription or membership programs often require an
upfront joining fee plus a membership fee per renewal period, or per service usage. Often there is
a menu or tier of prices, i.e. a company may design multiple subscription levels (say basic, silver,
gold, platinum), each more expensive than the previous but providing a cheaper per-unit price. Given
access to market data (i.e. profits for different pricing schemes for typical buyers) we would like to
learn how to set the base and per-item prices to maximize the profit. We define these settings formally
as follows."
REFERENCES,0.34331337325349304,"Two-part tariffs. The seller has K identical units of an item. Suppose the buyers have valuation
functions vi : {1, . . . , K} →R≥0 where i ∈{1, . . . , m} denotes the buyer, and the value is assumed
to be zero if no items are bought. Buyer i will purchase q quantities of the item that maximizes their
utility ui(q) = vi(q) −(p1 + p2q), buying zero units if the utility is negative for each q > 0. The
revenue, which we want to maximize as the seller, is zero if no item is bought, and p1 + p2q if q > 0
items are bought. The algorithmic parameter we want to select is the price ρ = ⟨p1, p2⟩, and the
problem instances are specified by the valuations vi. We also consider a generalization of the above
scheme: instead of just a single two-part tariff (TPT), suppose the seller provides a menu of TPTs
(p1
1, p1
2), . . . , (pℓ
1, pℓ
2) of length ℓ. Buyer i selects a tariff (pj
1, pj
2) from the menu as well as the item
quantity q to maximize their utility uj
i(q) = vi(q) −(pj
1 + pj
2q). This problem has 2ℓparameters,
ρ = (p1
1, p1
2, . . . , pℓ
1, pℓ
2), and the single two-part tariff setting corresponds to ℓ= 1."
REFERENCES,0.34530938123752497,"The dual class functions in this case are known to be piecewise linear with linear boundaries ([BSV18],
restated as Lemma G.2 in Appendix G). We will now implement Algorithm 1 for this problem by
specifying how to compute the locally relevant hyperplanes. For any price vector x = ρ, say the
buyers buy quantities (q1, . . . , qm) ∈{0, . . . , K}m according to tariffs (j1, . . . , jm) ∈[ℓ]m. For a
fixed price vector this can be done in time O(mKℓ) by computing argmaxq,j uj
i(q) for each buyer"
REFERENCES,0.3473053892215569,"at that price for each two-part tariff in the menu. Then for each buyer we have K(ℓ−1) potential
alternative quantities and tariffs given by hyperplanes uji
i (qi) ≥uj′"
REFERENCES,0.34930139720558884,"i (q′), q′ ̸= qi, j′ ̸= ji, for a total
of tLRS = mK(ℓ−1) locally relevant hyperplanes. Thus TCLRS = O(mKℓ) for the above approach,
and Theorem 2.2 implies the following runtime bound."
REFERENCES,0.35129740518962077,"Theorem F.1. There exists an implementation of COMPUTELOCALLYRELEVANTSEPARATORS in
Algorithm 1, which given valuation function v(·) for a single problem instance, computes all the R
pieces of the dual class function uv(·) in time ˜O(R2(2ℓ)ℓ+2K), where the menu length is ℓ, and there
are K units of the good."
REFERENCES,0.3532934131736527,"Theorem F.1 together with Theorem 2.3 implies an implementation of the search-ERM problem over
m buyers (with valuation functions vi(·) for i ∈[m]) in time O(R2
Σ(2ℓ)ℓ+2mK), where RΣ denotes
the number of pieces in the total dual class function U⟨v1,...,vm⟩(·) = P"
REFERENCES,0.35528942115768464,"i uvi(·) (formally, Corollary
G.1 in Appendix G). In contrast, prior work for this problem has only obtained an XP runtime
of (mK)O(ℓ) [BPS20]. For the special case ℓ= 1, we also provide an algorithm (Algorithm 4 in
Appendix G.2) that uses additional structure of the polytopes and employs a computational geometry
algorithm due to [CE92] to compute the pieces in optimal O(mK log(mK) + RΣ) time, improving
over the previously best known runtime of O(m3K3) due to [BPS20] even for worst-case RΣ. The
worst-case improvement follows from a bound of RΣ = O(m2K) on the number of pieces, which
we establish in Theorem G.3. We further show that our running time for ℓ= 1 is asymptotically
optimal under the algebraic decision-tree model of computation, by a linear time reduction from the
element uniqueness problem (Theorem G.6)."
REFERENCES,0.35728542914171657,"F.2
Item-Pricing with anonymous prices"
REFERENCES,0.3592814371257485,"We will consider a market with a single seller, interested in designing a mechanism to sell m distinct
items to n buyers. We represent a bundle of items by a quantity vector q ∈Zm
≥0, such that the number
of units of the ith item in the bundle denoted by q is given by its ith component q[i]. In particular,
the bundle consisting of a single copy of item i is denoted by the standard basis vector ei, where
ei[j] = I{i = j}, where I{·} is the 0-1 valued indicator function. Each buyer j ∈[n] has a valuation
function vj : Zm
≥0 →R≥0 over bundles of items. We denote an allocation as Q = (q1, . . . , qn)
where qj is the bundle of items that buyer j receives under allocation Q. Under anonymous prices,
the seller sets a price pi per item i. There is some fixed but arbitrary ordering on the buyers such
that the first buyer in the ordering arrives first and buys the bundle of items that maximizes their
utility, then the next buyer in the ordering arrives, and so on. For a given buyer j and bundle pair
q1, q2 ∈{0, 1}m, buyer j will prefer bundle q1 over bundle q2 so long as uj(q1) > uj(q2) where
uj(q) = vj(q) −P
i:q[i]=1 pi. Therefore, for a fixed set of buyer values and for each buyer j, their"
REFERENCES,0.36127744510978044,"preference ordering over the bundles is completely determined by the
 2m"
REFERENCES,0.36327345309381237,"2

hyperplanes. The dual
class functions are known to be piecewise linear with linear boundaries [BSV18]."
REFERENCES,0.3652694610778443,"To implement Algorithm 1 for this problem we specify how to compute the locally relevant hy-
perplanes. For any price vector x = (p1, . . . , pm), say the buyers buy bundles (q1, . . . , qn). For a
fixed price vector this can be done in time O(n2m) by computing argmaxq⊆Ij uj(q) for each buyer
at that price vector, where Ij denotes the remaining items after allocations to buyers 1, . . . , j −1
at that price. Then for each buyer we have at most 2m −1 potential alternative bundles given by
hyperplanes uj(q) ≥uj(q′), q′ ̸= qi, for a total of tLRS ≤n(2m −1) locally relevant hyperplanes.
Thus TCLRS = O(n2m) for the above approach, and Theorem 2.2 implies the following runtime
bound."
REFERENCES,0.36726546906187624,"Theorem F.2. There exists an implementation of COMPUTELOCALLYRELEVANTSEPARATORS in
Algorithm 1, which given valuation functions vi(·) for i ∈[n], computes all the R pieces of the dual
class function in time ˜O(R2(2m)mn), where there are m items and n buyers."
REFERENCES,0.36926147704590817,"Proof. In the terminology of Theorem 2.2, we have d = m, E ≤R2, V = R, TCLRS = O(n2m),
tLRS = n(2m −1). By [Cha18], we have P"
REFERENCES,0.3712574850299401,"c∈VH LP(d, |Ic| + 1) ≤O(Edd) ≤O(R2mm). Thus,
Theorem 2.2 implies a runtime bound on Algorithm 1 of ˜O(dE+V TCLRS+tLRS·P"
REFERENCES,0.37325349301397204,"c∈VH LP(d, |Ic|+
1)) = ˜O(R2(2m)mn)."
REFERENCES,0.37524950099800397,"Our approach yields an efficient algorithm when the number of items m and the number of dual class
function pieces R are small. Prior work on item-pricing with anonymous prices has only focussed on
sample complexity of the data-driven design problem [BSV18]."
REFERENCES,0.3772455089820359,"G
Additional details and proofs from Section F"
REFERENCES,0.37924151696606784,"Proof of Theorem F.1. In the terminology of Theorem 2.2, we have d = 2ℓ, E ≤R2, V = R,
TCLRS = O(Kℓ), tLRS ≤Kℓ. By [Cha18], we have P
c∈VH LP(d, |Ic| + 1) ≤O(Edd/2+1) ≤
O(R2(2ℓ)ℓ+1). Thus, Theorem 2.2 implies a runtime bound on Algorithm 1 of ˜O(dE + V TCLRS +
tLRS · P"
REFERENCES,0.3812375249500998,"c∈VH LP(d, |Ic| + 1)) = ˜O(R2(2ℓ)ℓ+2K)."
REFERENCES,0.38323353293413176,"Corollary G.1. There exists an implementation of COMPUTELOCALLYRELEVANTSEPARATORS
in Algorithm 1, which given valuation functions vi(·) for i ∈[m], computes all the RΣ pieces of
the total dual class function U⟨v1,...,vm⟩(·) = P"
REFERENCES,0.3852295409181637,"i uvi(·) in time ˜O(R2
Σ(2ℓ)ℓ+2mK), where the menu
length is ℓ, there are K units of the good and m is the number of buyers."
REFERENCES,0.3872255489021956,"Proof. We first compute the pieces for each of the problem instances (single buyers) and then
the pieces in the sum dual class function using Theorem 2.3.
By Theorem F.1, the former
takes time ˜O(mR2(2ℓ)ℓ+2K) and the latter can be implemented in time O((m + 2ℓ)R2
Σ +
mKℓP"
REFERENCES,0.38922155688622756,"c∈VS LP(d, |Ic|+1)) = ˜O(R2
Σ(2ℓ)ℓ+2mK), which dominates the overall running time."
REFERENCES,0.3912175648702595,"G.1
Piecewise structure of the dual class function"
REFERENCES,0.3932135728542914,"The following lemma restates the result from [BSV18] in terms of Definition 1. Note that uρ in the
following denotes the revenue function (or seller’s utility) and should not be confused with the buyer
utility function ui."
REFERENCES,0.39520958083832336,"Lemma G.2. Let U be the set of functions {uρ : v(·) 7→pj∗
1 + pj∗
2 q∗| q∗, j∗= argmaxq,j v(q) −
ρj · ⟨1, q⟩, ρj = ⟨pj
1, pj
2⟩} that map valuations v(·) to R. The dual class U∗is (F, (Kℓ)2)-piecewise
decomposable, where F = {fc : U →R | c ∈R2ℓ} consists of linear functions fc : uρ 7→ρ · c."
REFERENCES,0.3972055888223553,"We also bound the number of pieces R in the worst-case for ℓ= 1. The following bound implies that
our algorithm is better than prior best algorithm which achieves an O(m3K3) runtime bound, even
for worst case outputs.
Theorem G.3. Let menu length ℓ= 1. The number of pieces RΣ in the total dual class function
U⟨v1,...,vm⟩(·) = P"
REFERENCES,0.3992015968063872,i uvi(·) is at most O(m2K).
REFERENCES,0.40119760479041916,"Proof. By Lemma G.2, the dual class is (F, K2)-piecewise decomposable, where F is the class
of linear functions. That is, the two-dimensional parameter space (p1, p2) can be partitioned into
polygons by at most K2 straight lines such that any dual class function uvi is a linear function inside
any polygon."
REFERENCES,0.4031936127744511,"We first tighten the above result to show that the dual class is in fact (F, 2K + 2)-piecewise decom-
posable, that is the number of bounding lines for the pieces is O(K). This seems counterintuitive
since for any buyer i, we have Θ(K2) lines ui(q) ≥ui(q′) for q < q′ ∈{0, . . . , K}. If q > 0,
ui(q) = ui(q′) are axis-parallel lines with intercepts vi(q′)−vi(q)"
REFERENCES,0.405189620758483,"q′−q
. Since for any pair q, q′ the buyer
has a fixed (but opposite) preference on either side of the axis-parallel line, we have at most K distinct
horizontal ‘slabs’ corresponding to buyer’s preference of quantities, i.e. regions between lines p2 = a
and p2 = b for some a, b > 0. Thus we have at most K non-redundant lines. Together with another
K lines ui(0) = ui(q′) and the axes, we have 2K + 2 bounding lines in all as claimed."
REFERENCES,0.40718562874251496,"We will next use this tighter result to bound the number of points of intersection of non-collinear
non-axial bounding lines, let’s call them crossing points, across all instances ⟨v1, . . . , vm⟩. Consider
a pair of instances given by buyer valuation functions vi, vj. We will establish that the number of
crossing points are at most 4K for the pair of instances. Let li and lj be bounding lines for the pieces
of uvi and uvj respectively. If they are both axis-parallel, then they cannot result in a crossing point.
For pairs of bounding lines li and lj such that li is axis-parallel and lj is not, we can have at most K
crossing points in all. This is because any fixed li can intersect at most one such lj since buyer j’s"
REFERENCES,0.4091816367265469,"preferred quantity qj is fixed along any horizontal line, unless qj changes across li in which case the
crossing points for the consecutive lj’s coincide. Thus, there is at most one such crossing point for
each of at most K axis-parallel li’s. By symmetry, there are at most K crossing points between li, lj
where lj is axis parallel and li is not. Finally, if neither li, lj is axis parallel, we claim there can be
no more than 2K crossing points. Indeed, if we arrange these points in the order of increasing p2,
then the preferred quantity of at least one of the buyers i or j strictly decreases between consecutive
crossing points. Thus, across all instances, there are at most 2m2K crossing points."
REFERENCES,0.4111776447105788,"Finally, observe that the cell adjacency graph GU for the pieces of the total dual class function
U⟨v1,...,vm⟩(·) is planar in this case. The vertices of this graph correspond to crossing points, or
intersections of bounding lines with the axes. The latter is clearly O(mK) since there are O(K)
bounding lines in any problem instance. Using the above bound on crossing points, the number of
vertices in GU is O(m2K). Since GU is a simple, connected, planar graph, the number of faces is no
more than twice the number of vertices and therefore the number of pieces RΣ is also O(m2K)."
REFERENCES,0.41317365269461076,"G.2
Optimal algorithm for Single TPT pricing"
REFERENCES,0.4151696606786427,"Consider the setting with menu-length ℓ= 1. The key insight is to characterize the polytopic structure
of the pieces of the dual class function for a single buyer. We do this in Lemma G.4.
Lemma G.4. Consider a single buyer with valuation function v(·). The buyer buys zero units of the
item except for a set ϱv ⊂R2, where ϱv is a convex polygon with at most K + 2 sides. Moreover,
ϱv can be subdivided into K′ ≤K polygons ϱ(i)
v , each a triangle or a trapezoid with bases parallel
to the ρ1-axis, such that for each i ∈[K′] the buyer buys the same quantity q(i) of the item for all
prices in ϱ(i)
v ."
REFERENCES,0.4171656686626746,"Proof. We proceed by an induction on K, the number of items. For K = 1, it is straightforward to
verify that ϱv is the triangle p1 ≥0, p2 ≥0, p1 + p2 ≤v(1)."
REFERENCES,0.41916167664670656,"Let K > 1. If we consider the restriction of the valuation function v(·) to K −1 items, we have a
convex polygon ϱ′
v satisfying the induction hypothesis. To account for the K-th item we only need to
consider the region p1 ≥0, p2 ≥0, p2 ≤v(K)−v(q)"
REFERENCES,0.42115768463073855,"K−q
for 0 < q < K, and p1 + p2K ≤v(K). If this"
REFERENCES,0.4231536926147705,"region is empty, ϱv = ϱ′
v, and we are done. Otherwise, denoted by ϱ(K′)
v
with q(K′) = K, the region
where the buyer would buy K units of the item is a trapezoid with bases parallel to the ρ1-axis. We
claim that ϱv =

ϱ′
v ∩p2 ≤v(K)−v(q)"
REFERENCES,0.4251497005988024,"K−q

∪ϱ(K′)
v
and it satisfies the properties in the lemma."
REFERENCES,0.42714570858283435,"We have q(K′−1) = argminq
v(K)−v(q)"
REFERENCES,0.4291417165668663,"K−q
such that the buyer’s preference changes from q′ = q(K′−1)"
REFERENCES,0.4311377245508982,to q(K′) = K units across the line p2 = v(K)−v(q′)
REFERENCES,0.43313373253493015,"K−q′
. To prove ϱv is convex, we use the inductive
hypothesis on ϱ′
v and observe that ρ1 = v(K) −p2K coincides with ρ′
1 = v(q′) −p2q′ for p2 =
v(K)−v(q′)"
REFERENCES,0.4351297405189621,"K−q′
. Also the only side of ϱv that is not present in ϱ′
v lies along the line p1 + p2K = v(K),
thus ϱv has at most K +2 sides. The subdivison property is also readily verified given the construction
of ϱv from ϱ′
v and ϱ(K′)
v
."
REFERENCES,0.437125748502994,"Based on this structure, we propose Algorithm 4 which runs in in O(mK log(mK) + RΣ) time.
Theorem G.5. There is an algorithm (Algorithm 4) that, given valuation functions vi(·) for i ∈[m],
computes all the R pieces of the total dual class function U⟨v1,...,vm⟩(·) for K units of the good from
the m samples in O(mK log(mK) + RΣ) time."
REFERENCES,0.43912175648702595,"Proof. Note that if 0 < q < q′ and vi(q) > vi(q′), then for any ρ1 ≥0, ρ2 ≥0 we have that
ui(q) = vi(q) −(p1 + p2q) > vi(q′) −(p1 + p2q), or the buyer will never prefer quantity q′ of the
item over the entire tariff domain. Thus, we will assume the valuations vi(q) are monotonic in q (we
can simply ignore valuations at the larger value for any violation). Algorithm 4 exploits the structure
in Lemma G.4 and computes the O(K) line segments bounding the dual class pieces for a single
buyer i in O(K) time. Across m buyers, we have O(mK) line segments (computed in O(mK)
time). The topological plane-sweep based algorithm of [CE92] now computes all the intersection
points in O(mK(log mK) + RΣ) time. Here we have used that the number of polytopic vertices is
O(RΣ) using standard result for planar graphs."
REFERENCES,0.4411177644710579,"Algorithm 4: ComputeFixedAllocationRegions
Input: vi(·), valuation functions
1. For i = 1 . . . m do
2. Q ←∅(stack), q′ = 1, h = vi(1).
3. For q = 2 . . . K do
3.1
h′ ←(vi(q) −vi(q′))/(q −q′)
3.2
if 0 < h′ < h
h ←h′
Push (q, h′) onto Q
q′ ←q
3.3
else if 0 < h′
while h′ ≥h do
Pop (q′, h) from Q
(q1, h1) ←Top(Q)
h′ ←(vi(q) −vi(q1))/(q −q1)
h ←h1
q′ ←q
Push (q, h′) onto Q
4. For (q, h) ∈Q do
Obtain ϱ(j)
vi for q(j) = q using lines p2 = h and p1 = v(q) −p2q.
5. Compute intersection of segments in ϱ(j)
vi for i ∈[m] using [CE92].
6. Use the intersection points to compute boundaries of all polygons (pieces) formed by the
intersections."
REFERENCES,0.4431137724550898,"We further show that this bound is essentially optimal. A runtime lower bound of Ω(mK + RΣ)
follows simply due to the amount of time needed for reading the complete input and producing
the complete output. We prove a stronger lower bound which matches the above upper bound by
reduction to the element uniqueness problem (given a list of n numbers, are there any duplicates?) for
which an Ω(n log n) lower bound is known in the algebraic decision-tree model of computation.
Theorem G.6. Given a list of n numbers L = ⟨x1, . . . , xn⟩∈Nn, there is a linear time reduction to
a m-buyer, K-item TPT pricing given by vi(·), i ∈[m], with mK = Θ(n), such that the pieces of
the total dual class function U⟨v1,...,vm⟩(·) can be used to solve the element uniqueness problem for
L in O(n) time."
REFERENCES,0.44510978043912175,"Proof. Let mK = n be any factorization of n into two factors. We construct a m-buyer, K + 1 item
single TPT pricing scheme as follows. Define yj = xj + maxk xk + 1 for each xj in the list L.
For every buyer i ∈[m], we set vi(1) = maxk xk + 1 and vi(q + 1) = Pq
j=1 yj+(i−1)K for each
q ∈[K]. Buyer i’s pieces include the segments p2 = (vi(q + 1) −vi(q))/(q + 1 −1) = xq+(i−1)K
for q ∈[K] (Lemma G.4). Thus, across all buyers i ∈[m], we have mK = n segments along the
lines p2 = xj for j ∈[n]. We say a duplicate is present if there are fewer than mK segments parallel
to the p1-axis in the pieces of the total dual class function, otherwise we say ‘No’ (i.e. all elements
are distinct). This completes the linear-time reduction."
REFERENCES,0.4471057884231537,"H
Comparing the quality of single, complete and median linkage procedures
on different data distributions"
REFERENCES,0.4491017964071856,"We will construct clutering instances where each of two-point based linkage procedures, i.e. single,
complete and median linkage, dominates the other two procedures. Let T S
sgl, T S
cmpl and T S
med denote
the cluster tree on clustering instance S using Dsgl, Dcmpl and Dmed as the merge function (defined in
Section 3) respectively for some distance metric d which will be evident from context. We have the
following theorem.
Theorem H.1. For any n ≥10, for i ∈{1, 2, 3}, there exist clustering instances Si with |Si| = n
and target clusterings Ci such that the hamming loss of the optimal pruning of the cluster trees
constructed using single, complete and median linkage procedures (using the same distance metric d)
satisfy"
REFERENCES,0.45109780439121755,"(i) ℓ(T S1
sgl , C1) = O( 1"
REFERENCES,0.4530938123752495,"n), ℓ(T S1
cmpl, C1) = Ω(1) and ℓ(T S1
med, C1) = Ω(1),"
REFERENCES,0.4550898203592814,"(ii) ℓ(T S2
cmpl, C2) = O( 1"
REFERENCES,0.45708582834331335,"n), ℓ(T S2
sgl , C2) = Ω(1) and ℓ(T S2
med, C2) = Ω(1),"
REFERENCES,0.4590818363273453,"(iii) ℓ(T S3
med, C3) = O( 1"
REFERENCES,0.46107784431137727,"n), ℓ(T S3
cmpl, C3) = Ω(1) and ℓ(T S3
sgl , C3) = Ω(1)."
REFERENCES,0.4630738522954092,"Proof. In the following constructions we will have Si ⊂R2 and the distance metric d will be the
Euclidean metric. Also we will have number of target clusters k = 2."
REFERENCES,0.46506986027944114,"Construction of S1, C1. For S1, we will specify the points using their polar coordinates. We place
a single point x at the origin (0, ϕ) and n−1"
REFERENCES,0.46706586826347307,"8
points each along the unit circle at locations y1 =
(1, 0), y2 = (1, π"
REFERENCES,0.469061876247505,"4 −ϵ), y3 = (1, π"
REFERENCES,0.47105788423153694,"2 ), y4 = (1, 3π"
REFERENCES,0.47305389221556887,"4 −ϵ), y5 = (1, π), y6 = (1, 5π"
REFERENCES,0.4750499001996008,"4 −ϵ), y7 = (1, 3π"
REFERENCES,0.47704590818363274,"2 )
and y8 = (1, 7π"
REFERENCES,0.47904191616766467,"4 −ϵ), where ϵ = 0.001. Also set C1 = {{x}, S1 \ {x}}."
REFERENCES,0.4810379241516966,"In each linkage procedure, the first n −9 merges will join coincident points at locations yj, j ∈
[8], let ˜yj denote the corresponding sets of merged points. The next four merges will be zj :=
{˜yj, ˜yj+1} for j ∈{1, 3, 5, 7} for each procedure since d(yj, yj+1) =
p"
REFERENCES,0.48303393213572854,2 −2 cos( π
REFERENCES,0.48502994011976047,"4 −ϵ) <
min{
p"
REFERENCES,0.4870259481037924,2 −2 cos( π
REFERENCES,0.48902195608782434,"4 + ϵ), 1}, again common across all procedures. Now single linkage will continue
to merge clusters on the unit circle since
p"
REFERENCES,0.49101796407185627,2 −2 cos( π
REFERENCES,0.4930139720558882,"4 + ϵ) < 1, however both complete and
median linkage will join each of zj, j ∈{1, 3, 5, 7} to the singleton cluster {x} since the median (and
therefore also the maximum distance between points in zj, zj′, j ̸= j′ is at least
√"
REFERENCES,0.49500998003992014,"2 > 1. Therefore
a 2-pruning11 of T S1
sgl yields C1 i.e ℓ(T S1
sgl , C1) = 0, while a 2-pruning of T S1
cmpl or T S1
med would yield
{zj, S1 \ zj} for some j ∈{1, 3, 5, 7}, corresponding to a hamming loss of 1"
REFERENCES,0.49700598802395207,4 + Ω( 1
REFERENCES,0.499001996007984,n) = Ω(1).
REFERENCES,0.500998003992016,"Construction of S2, C2. For S2, we will specify the points using their Cartesian coordinates. We place
single points x1, x2 at (0, 0) and (3.2, 0.5) and n−2"
REFERENCES,0.5029940119760479,"2
points each at y1 = (1.1, 1.8) and y2 = (1.8, 0.5).
We set C2 = {{(x, y) ∈S2 | y > 1}, {(x, y) ∈S2 | y ≤1}}. The distances between pairs of points
may be ordered as"
REFERENCES,0.5049900199600799,"d(x2, y2) = 1.4 < d(y1, y2) ≈1.5 < d(x1, y2) ≈1.9 < d(x1, y1) ≈2.1 < d(x2, y1) ≈2.5
< d(x1, x2)"
REFERENCES,0.5069860279441117,"All linkage procedures will merge the coincident points at y1 and y2 (respectively) for the first
n −4 merges. Denote the corresponding clusters by ˜y1 and ˜y2 respectively. The next merge
will be z2 := {x2, ˜y2} in all cases. Now single linkage will join z2 with ˜y1. Further, since
n ≥10, n−2"
REFERENCES,0.5089820359281437,"2
≥4 and therefore the median distance between z2 and ˜y1 is also d(y1, y2). However,
since d(x1, y1) < d(x2, y1), the complete linkage procedure will merge {x1, z2}. Finally, the two
remaining clusters are merged in each of the two procedures. Clearly, 2-pruning of T S2
cmpl yields C2 or
ℓ(T S2
cmpl, C2) = 0. However, ℓ(T S2
sgl , C2) = ℓ(T S2
med, C2) = 1"
REFERENCES,0.5109780439121756,2 −O( 1
REFERENCES,0.5129740518962076,n) = Ω(1).
REFERENCES,0.5149700598802395,"Construction of S3, C3. We specify the points in S3 using their Cartesian coordinates. We place n−1"
REFERENCES,0.5169660678642715,"6
points each at x1 = (0, 0), x′
2 = (0, 1 + 2ϵ), n−1"
REFERENCES,0.5189620758483033,"12 points each at x′
1 = (0, ϵ), x2 = (0, 1 + ϵ), n−1"
REFERENCES,0.5209580838323353,"4
points each at y1 = (1+0.9ϵ, ϵ), y2 = (1+ϵ, 1+1.9ϵ), and one point z1 = (0, 2) with ϵ = 0.3. With
some abuse of notation we will use the coordinate variables defined above to also denote the collection
of points at their respective locations. We set C3 = {{(x, y) ∈S2 | x ≤0}, {(x, y) ∈S2 | x > 0}}."
REFERENCES,0.5229540918163673,"After merging the coincident points, all procedures will merge clusters ˜x1 := {x1, x′
1} and ˜x2 :=
{x2, x′
2, z1}. Let’s now consider the single linkage merge function. We have Dsgl(˜x1, ˜x2; d) = 1
and all other cluster pairs are further apart. The next merge is therefore ˜x := {˜x1, ˜x2}. Also,
Dsgl(˜x, y1; d) = 1 + 0.9ϵ < min{Dsgl(˜x, y2; d), Dsgl(y1, y2; d)} leading to the merge {˜x, y1}, and
finally y2 is merged in. A 2-pruning therefore has loss ℓ(T S3
sgl , C3) = Ω(1). On the other hand,
Dmed(˜x1, ˜x2; d) = 1 + ϵ > Dmed(y1, y2; d) =
p"
REFERENCES,0.5249500998003992,"(1 + 0.9ϵ)2 + 0.01ϵ2 and Dmed(˜x1, y1; d) =
p"
REFERENCES,0.5269461077844312,"(1 + 0.9ϵ)2 + ϵ2 > {Dmed(y1, y2; d), Dmed(˜x1, ˜x2; d)}. As a result, median linkage would first"
REFERENCES,0.5289421157684631,"11A k0-pruning for a tree T is a partition of the points contained in T’s root into k0 clusters such that each
cluster is an internal node of T."
REFERENCES,0.530938123752495,"Figure 1: Construction of clustering instances showing the need for interpolating linkage heuristics.
We give concrete instances and target clusterings where each of two-point based linkage procedures,
i.e. single, complete and median linkage, dominates the other two."
REFERENCES,0.5329341317365269,"merge {y1, y2}, followed by {˜x1, ˜x2}, and 2-pruning yields C3. Complete linkage also merges
{y1, y2} first. But Dcmpl(˜x1, ˜x2; d) = 2 > Dcmpl(˜x1, {y1, y2}; d). Thus, ℓ(T S3
cmpl, C3) = Ω(1)."
REFERENCES,0.5349301397205589,"I
Additional details and proofs from Section 3"
REFERENCES,0.5369261477045908,"Definition 4 (2-point-based merge function [BDL20]). A merge function D is 2-point-based if
for any pair of clusters A, B ⊆X and any metric d, there exists a set of points (a, b) ∈A × B
such that D(A, B; d) = d(a, b). Furthermore, the selection of a and b only depend on the relative
ordering of the distances between points in A and B. More formally, for any metrics d and d′
such that d(a, b) ≤d(a′, b′) if and only if d′(a, b) ≤d′(a′, b′), then D(A, B; d) = d(a, b) implies
D(A, B; d′) = d′(a, b)."
REFERENCES,0.5389221556886228,"For instance, single, median and complete linkage are 2-point-based, since the merge function
D(A, B; d) only depends on the distance d(a, b) for some a ∈A, b ∈B. We have the following
observation about our parameterized algorithm families D∆
ρ (A, B; δ) when ∆consists of 2-point-
based merge functions which essentially establishes piecewise structure with linear boundaries (in
the sense of Definition 1).
Lemma I.1. Suppose S ∈Π is a clustering instance, ∆is a set of 2-point-based merge functions with
|∆| = l, and δ is a set of distance metrics with |δ| = m. Consider the family of clustering algorithms
with the parameterized merge function D∆
ρ (A, B; δ). The corresponding dual class function uS(·) is
piecewise constant with O(|S|4lm) linear boundaries."
REFERENCES,0.5409181636726547,"Proof. Let (aij, bij, a′
ij, b′
ij)1≤i≤l,1≤j≤m ⊆S be sequences of lm points each; for each such a, let
ga : P →R denote the function"
REFERENCES,0.5429141716566867,"ga(ρ) =
X"
REFERENCES,0.5449101796407185,"i∈[l],dj∈δ
αi,j(ρ)(dj(aij, bij) −dj(a′
ij, b′
ij))"
REFERENCES,0.5469061876247505,"and let G = {ga | (aij, bij, a′
ij, b′
ij)1≤i≤l,1≤j≤m ⊆S} be the collection of all such linear functions;
notice that |G| = O(|S|4lm). Fix ρ, ρ′ ∈P with g(ρ) and g(ρ′) having the same sign patterns for
all such g. For each A, B, A′, B′ ⊆S, Di ∈∆, and dj ∈δ, we have Di(A, B; dj) = dj(a, b) and
Di(A′, B′; dj) = dj(a′, b′) for some a, b, a′, b′ ∈S (since Di is 2-point-based). Thus we can write
Dρ(A, B; δ) = P"
REFERENCES,0.5489021956087824,"i∈[m],dj∈δ αi,j(ρ)dj(aij, bij) for some aij, bij ∈S; similarly, Dρ(A′, B′; δ) =
P"
REFERENCES,0.5508982035928144,"i∈[m],dj∈δ αi,j(ρ)dj(a′
ij, b′
ij) for some a′
ij, b′
ij ∈S. As a result, Dρ(A, B; δ) ≤Dρ(A′, B′; δ) if
and only if
X"
REFERENCES,0.5528942115768463,"i∈[l],dj∈δ
αi,j(ρ)
 
dj(aij, bij) −dj(a′
ij, b′
ij)

≤0"
REFERENCES,0.5548902195608783,"which is exactly when ga(ρ) ≤0 for some sequence a. Since ga(ρ) and ga(ρ′) have the same sign
pattern, we have Dρ(A, B; δ) ≤Dρ(A′, B′; δ) if and only if Dρ′(A, B; δ) ≤Dρ′(A′, B′; δ). So ρ"
REFERENCES,0.5568862275449101,"Figure 2: The first three levels of an example execution tree of a clustering instance on four points,
with a two-parameter algorithm (P = ▲2). Successive partitions P0, P1, P2 are shown at merge
levels 0, 1, and 2, respectively, and the nested shapes show cluster merges."
REFERENCES,0.5588822355289421,"and ρ′ induce the same sequence of merges, meaning the algorithm’s output is constant on each piece
induced by g, as desired."
REFERENCES,0.5608782435129741,"From Lemma I.1, we obtain a bound on the number of hyperplanes needed to divide P into output-
constant pieces. Let H be a set of hyperplanes which splits P into output-constant pieces; then,
a naive approach to finding a dual-minimizing ρ ∈P is to enumerate all pieces generated by H,
requiring O(|H|d) runtime. However, by constructing regions merge-by-merge and successively
refining the parameter space, we can obtain a better runtime bound which is output-sensitive in the
total number of pieces."
REFERENCES,0.562874251497006,"Proof of Lemma ??. This is a simple corollary of Lemma I.1 for m = 1. In this case, we have
l = d + 1."
REFERENCES,0.564870259481038,"Lemma I.2. Consider the family of clustering algorithms with the parameterized merge function
D1
ρ(A, B; δ). Let T S
ρ denote the cluster tree computed using the parameterized merge function
D∆
ρ (A, B; d0) on sample S. Let U be the set of functions {uρ : S 7→ℓ(T S
ρ , C) | ρ ∈Rd} that
map a clustering instance S to R. The dual class U∗is (F, |S|4)-piecewise decomposable, where
F = {fc : U →R | c ∈R} consists of constant functions fc : uρ 7→c."
REFERENCES,0.5668662674650699,"The key observation for the proof comes from [BDL20] where it is observed that two parameterized
distance metrics dρ1, dρ2 behave identically (yield the same cluster tree) on a given dataset S if the
relative distance for all pairs of two points (a, b), (a′, b′) ∈S2 × S2, dρ(a, b) −dρ(a′, b′), has the
same sign for ρ1, ρ2. This corresponds to a partition of the parameter space with |S|4 hyperplanes,
with all distance metrics behaving identically in each piece of the partition. More formally, we have"
REFERENCES,0.5688622754491018,"Proof of Lemma I.2. Let S be any clustering instance. Fix points a, b, a′, b′ ∈S. Define the linear
function ga,b,a′,b′(ρ) = P"
REFERENCES,0.5708582834331337,"i ρi(di(a, b) −di(a′, b′)). If dρ(·, ·) denotes the interpolated distance
metric, we have that dρ(a, b) ≤dρ(a′, b′) if and only if ga,b,a′,b′(ρ) ≤0. Therefore we have a
set H = {ga,b,a′,b′(ρ) ≤0 | a, b, a′, b′ ∈S} of |S|4 hyperplanes such that in any piece of the
sign-pattern partition of the parameter space by the hyperplanes, the interpolated distance metric
behaves identically, i.e. for any ρ, ρ′ in the same piece dρ(a, b) ≤dρ(a′, b′) iff dρ′(a, b) ≤dρ′(a′, b′).
The resulting clustering is therefore identical in these pieces. This means that for any connected"
REFERENCES,0.5728542914171657,"component R of Rd \ H, there exists a real value cR such that uρ(s1, s2) = cR for all ρ ∈Rd. By
definition of the dual, u∗
s1,s2(uρ) = uρ(s1, s2) = cR. For each hyperplane h ∈H, let g(h) ∈G
denote the corresponding halfspace. Order these k = |S|4 functions arbitrarily as g1, . . . , gk. For a
given connected component R of Rd \ H, let bR ∈{0, 1}k be the corresponding sign pattern. Define
the function f (bR) = fcR and for b not corresponding to any R, f (b) = f0. Thus, for each ρ ∈Rd,"
REFERENCES,0.5748502994011976,"u∗
s1,s2(uρ) =
X"
REFERENCES,0.5768463073852296,"b∈{0,1}k
I{gi(uρ) = bi∀i ∈[k]}f (b)(uρ)."
REFERENCES,0.5788423153692615,"Corollary I.3. For any clustering instance S ∈Π, the dual class function uS(·) for the family in
Lemma I.2 is piecewise constant with O
 
|S|4d
pieces."
REFERENCES,0.5808383233532934,"Lemma I.4. Let S ∈Π be a clustering instance, ∆be a set of merge functions, and δ be a set
of distance metrics. Then, the corresponding dual class function uS(·) is piecewise constant with
O(16|S|) linear boundaries of pieces."
REFERENCES,0.5828343313373253,"Proof. For each subset of points A, B, A′, B′ ⊆S, let gA,B,A′,B′ : P →R denote the function"
REFERENCES,0.5848303393213573,"gA,B,A′,B′(ρ) = Dρ(A, B; δ) −Dρ(A′, B′; δ)"
REFERENCES,0.5868263473053892,"and let G be the collection of all such functions for distinct subsets A, B, A′, B′. Observe that G is
a class of linear functions with |G| ≤
 
2|S|4 = 16|S|. Suppose that for ρ, ρ′ ∈P, g(ρ) and g(ρ′)
have the same sign for all g ∈G; then, the ordering over all cluster pairs A, B of Dρ(A, B; δ) is the
same as that of Dρ′(A, B; δ). At each stage of the algorithm, the cluster pair A, B ⊆S minimizing
Dρ(A, B; δ) is the same as that which minimizes Dρ′(A, B; δ), so the sequences of merges produced
by ρ and ρ′ are the same. Thus the algorithm’s output is constant on the region induced by gA,B,A′,B′,
meaning uS(·) is piecewise constant on the regions induced by G, which have linear boundaries."
REFERENCES,0.5888223552894212,"I.1
Execution Tree"
REFERENCES,0.590818363273453,Formally we define an execution tree (Figure 2) as follows.
REFERENCES,0.592814371257485,"Definition 5 (Execution tree). Let S be a clustering instance with |S| = n, and ∅̸= P ⊆[0, 1]d.
The execution tree on S with respect to P is a depth-n rooted tree T, whose nodes are defined
recursively as follows: r = ([], P) is the root, where [] denotes the empty sequence; then, for any
node v = ([u1, u2, . . . , ut], Q) ∈T with t < n −1, the children of v are defined as"
REFERENCES,0.5948103792415169,children(v) =
REFERENCES,0.5968063872255489,"(
[u1, u2, . . . , ut, (A, B)], QA,B"
REFERENCES,0.5988023952095808,"
:
A, B ⊆S is the (t + 1)st merge by Aρ for
exactly the ρ ∈QA,B ⊆P, with ∅̸= QA,B ⊆Q ) ."
REFERENCES,0.6007984031936128,"For an execution tree T with v ∈T and each i with 0 ≤i ≤n, we let Pi denote the set of Q such
that there exists a depth-i node v ∈T and a sequence of merges M with v = (M, Q). Intuitively, the
execution tree represents all possible execution paths (i.e. sequences for the merges) for the algorithm
family when run on the instance S as we vary the algorithm parameter ρ ∈P. Furthermore, each Pi
is a subdivision of the parameter space into pieces where each piece has the first i merges constant.
We establish the execution tree captures all possible sequences of merges by some algorithm Aρ
in the parameterized family via its nodes, and each node corresponds to a convex polytope if the
parameter space P is a convex polytope (Lemmata I.5 and I.6)."
REFERENCES,0.6027944111776448,"Our cell enumeration algorithm for computing all the pieces of the dual class function now simply
computes the execution tree, using Algorithm 1 to compute the children nodes for any given node,
starting with the root."
REFERENCES,0.6047904191616766,"Lemma I.5. Let S be a clustering instance and T be its execution tree with respect to P. Then, if a
sequence of merges M = [u1, u2, . . . , ut] is attained by Aρ for some ρ ∈P, then there exists some
v ∈T at depth t with v = (M, Q) and with Q ⊆P being the exact set of values of ρ for which
Aρ may attain M. Conversely, for every node v = (M, Q) ∈T, M is a valid sequence of merges
attainable by Aρ for some ρ ∈P."
REFERENCES,0.6067864271457086,"Proof. We proceed by induction on t. For t = 0, the only possible sequence of merges is the empty
sequence, which is obtained for all ρ ∈P. Furthermore, the only node in T at depth 0 is the root
([], P), and the set P is exactly where an empty sequence of merges occurs."
REFERENCES,0.6087824351297405,"Now, suppose the claim holds for some t ≥0. We show both directions in the induction step."
REFERENCES,0.6107784431137725,"For the forward direction, let Mt+1 = [u1, u2, . . . , ut, ut+1], and suppose Mt+1 is attained by Aρ
for some ρ ∈P. This means that Mt = [u1, u2, . . . , ut] is attained by Aρ as well; by the induction
hypothesis, there exists some node vt = (Mt, Qt) ∈T at depth t, where ρ ∈Qt and Qt is exactly
the set of values for which A may attain Mt. Now, ut+1 is a possible next merge by Aρ for some
ρ ∈Qt; by definition of the execution tree, this means vt has some child vt+1 = (Mt+1, Qt+1) in
T such that Qt+1 is the set of values where ut+1 is the next merge in Qt. Moreover, Qt+1 is exactly
the set of values ρ ∈P for which Aρ can attain the merge sequence Mt+1. In other words for any
ρ′ ∈P \ Qt+1, Aρ′ cannot attain the merge sequence Mt+1. Otherwise, either some ρ′ ∈P \ Qt
attains Mt+1, meaning Aρ′ attains Mt (contradicting the induction hypothesis), or Aρ′ attains Mt+1
for some ρ′ ∈Qt+1 \ Qt, contradicting the definition of Qt+1."
REFERENCES,0.6127744510978044,"For the backward direction, let vt+1 = (Mt+1, Qt+1) ∈T at depth t + 1. Since vt+1 is not the
root, vt+1 must be the child of some node vt, which has depth t. By the induction hypothesis,
vt = (Mt, Qt), where Mt = [u1, u2, . . . , ut] is attained by Aρ for some ρ ∈P. Thus by definition
of the execution tree, Mt+1 has the form [u1, u2, . . . , ut, (A, B)], for some merging of cluster pairs
(A, B) which is realizable for ρ ∈Qt. Thus Mt+1 is a valid sequence of merges attainable by Aρ
for some ρ ∈P."
REFERENCES,0.6147704590818364,"Lemma I.6. Let S be a clustering instance and T be its execution tree with respect to P. Suppose P
is a convex polytope; then, for each v = (M, Q) ∈T, Q is a convex polytope."
REFERENCES,0.6167664670658682,"Proof. We proceed by induction on the tree depth t. For t = 0, the only node is ([], P), and P
is a convex polytope. Now, consider a node v ∈T at depth t + 1; by definition of the execution
tree, v = (Mv, Qv) is the child of some node u ∈T, where the depth of u is t. Inductively,
we know that w = (Mw, Qw), for some convex polytope Qw. We also know Mw has the form
Mw = [u1, u2, . . . , ut], and thus Qv is defined to be the set of points ρ ∈Qw where the merge
sequence Mv = [u1, u2, . . . , ut, (A, B)] is attainable for some fixed A, B ⊆S. Notice that the
definition of being attainable by the algorithm Aρ is that Dρ(A, B; δ) is minimized over all choices
of next cluster pairs A′, B′ to merge. That is, Qv is the set of points"
REFERENCES,0.6187624750499002,"Qv = {ρ ∈Qw | Dρ(A, B; δ) ≤Dρ(A′, B′; δ) for all available cluster pairs A′, B′ after Mw}"
REFERENCES,0.6207584830339321,"Since Dρ(A, B; δ) is an affine function of ρ, the constraint Dρ(A, B; δ) ≤Dρ(A′, B′; δ) is a half-
space. In other words, Qv is the intersection of a convex polytope Qw with finitely many half space
constraints, meaning Qv is itself a convex polytope."
REFERENCES,0.6227544910179641,"It follows from Lemma I.6 that Pi forms a convex subdivision of P, where each Pi+1 is a refinement
of Pi; Figure 2 (in the appendix) shows an example execution tree corresponding to a partition of a
2-dimensional parameter space. From Lemma I.5, the sequence of the first i merges stays constant on
each region P ∈Pi. Our algorithm computes a representation of the execution tree of an instance
S with respect to P; to do so, it suffices to provide a procedure to list the children of a node in the
execution tree. Then, a simple breadth-first search from the root will enumerate all the leaves in the
execution tree."
REFERENCES,0.624750499001996,"Now, our goal is to subdivide P into regions in which the (j + 1)st merge is constant. Each region
corresponds to a cluster pair being merged at step j + 1. Since we know these regions are always
convex polytopes (Lemma I.6), we can provide an efficient algorithm for enumerating these regions."
REFERENCES,0.626746506986028,"Our algorithm provides an output-sensitive guarantee by ignoring the cluster pairs which are never
merged. Supposing there are nt unmerged clusters, we start with some point x ∈P and determine
which piece W it is in. Then, we search for more non-empty pieces contained in P by listing the
“neighbors” of W. The neighbors of W are pieces inside P that are adjacent to W; to this end, we
will more formally define a graph GP associated with P where each vertex is a piece and two vertices
have an edge when the pieces are adjacent in space. Then we show that we can enumerate neighbors
of a vertex efficiently and establish that GP is connected. It follows that listing the pieces is simply
a matter of running a graph search algorithm from one vertex of GP , thus only incurring a cost for
each non-empty piece rather than enumerating through all n4
t pairs of pieces."
REFERENCES,0.6287425149700598,"Proof of Corollary 3.2. The key observation is that on any iteration i, the number of adjacencies
Hi = O(Ri). This is because for any region P ∈Pi, P is a polygon divided into convex subpolygons,
and the graph GP has vertices which are faces and edges which cross between faces. Since the
subdivision of P can be embedded in the plane, so can the graph GP . Thus GP is planar, meaning
Hi = O(Ri). Plugging into Theorem 3.1, noting that (n −i + 1)2 ≤n2, Hi ≤H, and Ri ≤R, we
obtain the desired runtime bound of O
 Pn
i=1(R + RTM)n2
= O(RTMn3)."
REFERENCES,0.6307385229540918,"I.2
Auxiliary lemmas and proofs of runtime bounds for our algorithm"
REFERENCES,0.6327345309381237,"Lemma I.7. Fix an affine function f : R →Rd via f(x) = xa + b, for a, b ∈Rd and a ̸= 0d. For a
subset S ⊆R, if f(S) is convex and closed, then S is also convex and closed."
REFERENCES,0.6347305389221557,"Proof. First note that f is injective, since a ̸= 0d. To show convexity of S, take arbitrary x, y ∈S
and λ ∈[0, 1]; we show that λx + (1 −λ)y ∈S. Consider f(λx + (1 −λ)y):"
REFERENCES,0.6367265469061876,"f(λx + (1 −λ)y) = (λx + (1 −λ)y)a + b
= λ(xa + b) + (1 −λ)(ya + b)"
REFERENCES,0.6387225548902196,"By definition, ya + b, xa + b ∈f(S), so it follows that f(λx + (1 −λ)y) ∈f(S) by convexity
of f(S). So there exists some z ∈S with f(z) = f(λx + (1 −λ)y), but since f is injective,
λx + (1 −λ)y = z ∈S. Thus S is convex."
REFERENCES,0.6407185628742516,"To show closedness of S, we show R \ S is open. Let N(x, r) denote the open ball of radius r around
x, in either one-dimensional or d-dimensional space. Let x ∈R \ S; we know f(x) /∈f(S) since f
is injective. Since Rd \ f(S) is open, there exists some r > 0 with N(f(x), r) ⊆Rd \ f(S). Then,
take e =
r
∥a∥2 > 0; for every y ∈N(x, e), we have"
REFERENCES,0.6427145708582834,∥f(x) −f(y)∥2 = ∥xa + b −ya −b∥2 < |x −y|∥a∥2 ≤r
REFERENCES,0.6447105788423154,"and so f(y) ∈N(f(x), r) ⊆Rd \ f(S), meaning y /∈S since f is injective. Thus N(x, e) ⊆R \ S,
meaning S is closed as desired."
REFERENCES,0.6467065868263473,"This allows us to prove the following key lemma. We describe a proof sketch first. For arbitrary
(i, j), (i′, j′) ∈V ∗
P , we show that there exists a path from (i, j) to (i′, j′) in GP . We pick arbitrary
points w ∈Qi,j, x ∈Qi′,j′; we can do this because by definition, V ∗
P only has elements correspond-
ing to non-empty cluster pairs. Then, we draw a straight line segment in P from w to x. When we do
so, we may pass through other sets on the way; each time we pass into a new region, we traverse an
edge in GP , so the sequence of regions we pass through on this line determines a GP -path from w to
x."
REFERENCES,0.6487025948103793,"Lemma I.8. The vertices V ∗
P of the region adjacency graph GP form a connected component; all
other connected components of GP are isolated vertices."
REFERENCES,0.6506986027944112,"Proof. It suffices to show that for arbitrary vertices (i1, j1), (i2, j2) ∈VP , there exists a path from
(i1, j1) to (i2, j2) in GP . For ease of notation, define Q1 = Qi1,j1 and Q2 = Qi2,j2."
REFERENCES,0.6526946107784432,"Fix arbitrary points u ∈Q1 and w ∈Q2. If u = w then we’re done, since the edge from (i1, j1) to
(i2, j2) exists, so suppose u ̸= w. Consider the line segment L defined as"
REFERENCES,0.654690618762475,"L = {λu + (1 −λ)w : λ ∈[0, 1]}"
REFERENCES,0.656686626746507,"Since Q1, Q2 ⊆P, we have u, w ∈P. Furthermore, by convexity of P, it follows that L ⊆P."
REFERENCES,0.6586826347305389,"Define the sets Ri,j as"
REFERENCES,0.6606786427145709,"Ri,j = Qi,j ∩L"
REFERENCES,0.6626746506986028,"Since each Qi,j and L are convex and closed, so is each Ri,j. Furthermore, since S"
REFERENCES,0.6646706586826348,"i,j Qi,j = P, we
must have S"
REFERENCES,0.6666666666666666,"i,j Ri,j = L. Finally, define the sets Si,j as"
REFERENCES,0.6686626746506986,"Si,j = {t ∈[0, 1] : tu + (1 −t)w ∈Ri,j} ⊆[0, 1]"
REFERENCES,0.6706586826347305,"Note that Si,j is convex and closed; the affine map f : Si,j →Ri,j given by f(x) = xu+(1−x)w =
x(u −w) + w has Ri,j as an image. Furthermore, u −w ̸= 0d; by Lemma I.7, the preimage Si,j
must be convex and closed. Furthermore, S"
REFERENCES,0.6726546906187625,"i,j Si,j = [0, 1]."
REFERENCES,0.6746506986027944,"The only convex, closed subsets of [0, 1] are closed intervals. We sort the intervals in increasing order
based on their lower endpoint, giving us intervals I1, I2, . . . , Iℓ. We also assume all intervals are
non-empty (we throw out empty intervals). Let σ(p) denote the corresponding cluster pair associated
with interval Ip; that is, if the interval Ip is formed from the set Si,j, then σ(p) = (i, j)."
REFERENCES,0.6766467065868264,"Define ai, bi to be the lower and upper endpoints, respectively, of Ii. We want to show that for all
1 ≤i ≤ℓ−1, the edge {σ(i), σ(i + 1)} ∈EP ; this would show that σ(1) is connected to σ(ℓ) in
the VP . But σ(1) = (i1, j1) and σ(ℓ) = (i2, j2), so this suffices for our claim."
REFERENCES,0.6786427145708582,"Now consider intervals Ii = [ai, bi] and Ii+1 = [ai+1, bi+1]. It must be the case that bi = ai+1;
otherwise, some smaller interval would fit in the range [bi, ai+1], and it would be placed before Ii+1
in the interval ordering."
REFERENCES,0.6806387225548902,"Since bi ∈Ii∩Ii+1, by definition, ubi+(1−bi)w ∈Rσ(i)∩Rσ(i+1). In particular, ubi+(1−bi)w ∈
Qσ(i) ∩Qσ(i+1); by definition of EP , this means {σ(i), σ(i + 1)} ∈EP , as desired."
REFERENCES,0.6826347305389222,"Theorem 3.1. Let S be a clustering instance with |S| = n, and let Ri = |Pi| and R = Rn. Let
Ht =
{(Q1, Q2) ∈P2
t | Q1 ∩Q2 ̸= ∅}
 denote the total number of adjacencies between any two
pieces of Pi and H = Hn. Then, the leaves of the execution tree on S can be computed in time
˜O
 Pn
i=1 (Hi + RiTM) (n −i + 1)2
, where TM is the time to compute the merge function."
REFERENCES,0.6846307385229541,"Proof. Let T be the execution tree with respect to S, and let Tt denote the vertices of T at depth t.
From Theorem 2.2, for each node v = (M, Q) ∈T with depth t, we can compute the children of v
in time O(n2
t · LP(d, Ev) + Vv · n2
tK), where Vv is the number of children of v, and Ev =  
 "
REFERENCES,0.6866267465069861,"(Q1, Q2) ∈P2
t+1 | Q1 ∩Q2 ̸= ∅and
u1 = (M1, Q1), u2 = (M2, Q2)
for some children u1, u2 of v 
  ."
REFERENCES,0.688622754491018,"Now, observe
X"
REFERENCES,0.6906187624750499,"v∈Tt+1
Ev ≤Ht+1"
REFERENCES,0.6926147704590818,"since Ht+1 counts all adjacent pieces Q1, Q2 in Pt+1; each pair is counted at most once by some
Ev. Similarly, we have P"
REFERENCES,0.6946107784431138,"v∈Tt+1 Vv ≤Rt+1, since Rt+1 counts the total size of Pt+1. Note that
nt+1 = (n−t), since t merges have been executed by time t+1, so ni = n−i+1. Seidel’s algorithm
is a randomized algorithm that may be used for efficiently solving linear programs in low dimensions,
the expected running time for solving an LP in d variables and m constraints is O(d! · Ev) (also
holds with high probability, e.g. Corollary 2.1 of [Sei91]). There are also deterministic algorithms
with the same (in fact slightly better) worst-case runtime bounds [Cha18]. Therefore, we can set
LP(d, Ev) = O(d! · Ev). So that the total cost of computing Pi is O n
X i=1 X"
REFERENCES,0.6966067864271457,"v∈Ti
d! · Ev(n −i + 1)2 + VvK(n −i + 1)2
! = O n
X"
REFERENCES,0.6986027944111777,"i=1
(d! · Hi + RiK) (n −i + 1)2
!"
REFERENCES,0.7005988023952096,as desired.
REFERENCES,0.7025948103792415,"J
Further details and proofs from Section 4"
REFERENCES,0.7045908183632734,"We provide further details for Algorithm 5 in Appendix J.2, and other proofs from Section 4 are
located in Appendix J.3."
REFERENCES,0.7065868263473054,"J.1
Example dynamic programs for sequence alignment"
REFERENCES,0.7085828343313373,"We exhibit how two well-known sequence alignment formulations can be solved using dynamic
programs which fit our model in Section 4. In Section J.1.1 we show a DP with two free parameters
(d = 2), and in Section J.1.2 we show another DP which has three free parameters (d = 3)."
REFERENCES,0.7105788423153693,"J.1.1
Mismatches and spaces"
REFERENCES,0.7125748502994012,"Suppose we only have two features, mismatches and spaces. The alignment that minimizes the cost
c may be obtained using a dynamic program in O(mn) time. The dynamic program is given by
the following recurrence relation for the cost function which holds for any i, j > 0, and for any
ρ = (ρ1, ρ2),"
REFERENCES,0.7145708582834331,"C(s1[:i], s2[:j], ρ) ="
REFERENCES,0.716566866267465,"





"
REFERENCES,0.718562874251497,"




"
REFERENCES,0.720558882235529,"C(s1[:i −1], s2[:j −1], ρ)
if s1[i] = s2[j],"
REFERENCES,0.7225548902195609,"min

ρ1 + C(s1[:i −1], s2[:j −1], ρ),
ρ2 + C(s1[:i −1], s2[:j], ρ),
ρ2 + C(s1[:i], s2[:j −1], ρ)
	
if s1[i] ̸= s2[j]."
REFERENCES,0.7245508982035929,"The base cases are C(ϕ, ϕ, ρ) = 0, C(ϕ, s2[:j], ρ) = jρ2, = C(s1[:i], ϕ, ρ) = iρ2 for i, j ∈[m]×[n].
Here ϕ denotes the empty sequence. One can write down a similar recurrence for computing the
optimal alignment τ(s1, s2, ρ)."
REFERENCES,0.7265469061876247,"We can solve the non base-case subproblems (s1[:i], s2[:j]) in any non-decreasing order of i+j. Note
that the number of cases V = 2, and the maximum number of subproblems needed to compute a single
DP update L = 3 (L1 = 1, L2 = 3). For a non base-case problem (i.e. i, j > 0) the cases are given
by q(s1[:i], s2[:j]) = 1 if s1[i] = s2[j], and q(s1[:i], s2[:j]) = 2 otherwise. The DP update in each
case is a minimum of terms of the form cv,l(ρ, (s1[:i], s2[:j])) = ρ · wv,l + σv,l(ρ, (s1[:i], s2[:j])).
For example if q(s1[: i], s2[: j]) = 2, we have w2,1 = ⟨1, 0⟩and σ2,1(ρ, (s1[: i], s2[: j])) equals
C(s1[:i −1], s2[:j −1], ρ), i.e. the solution of previously solved subproblem (s1[:i −1], s2[:j −1]),
the index of this subproblem depends on l, v and index of (s1[:i], s2[:j]) but not on ρ itself."
REFERENCES,0.7285429141716567,"J.1.2
Mismatches, spaces and gaps"
REFERENCES,0.7305389221556886,"Suppose we have three features, mismatches, spaces and gaps. Typically gaps (consecutive spaces)
are penalized in addition to spaces in this model, i.e. the cost of a sequence of three consecutive gaps
in an alignment (. . . a −−−b . . . , . . . a′ p q r b′ . . . ) would be 3ρ2 + ρ3 where ρ2, ρ3 are costs
for spaces and gaps respectively [KKW10]. The alignment that minimizes the cost c may again be
obtained using a dynamic program in O(mn) time. We will need a slight extension of our DP model
from Section 4 to capture this. We have three subproblems corresponding to any problem in Πs1,s2
(as opposed to exactly one subproblem, which was sufficient for the example in J.1.1). We have a set
of subproblems π(s1, s2) with |π(s1, s2)| ≤3|Πs1,s2| for which our model is applicable. For each
(s1[:i], s2[:j]) we can compute the three costs (for any fixed ρ)"
REFERENCES,0.7325349301397206,"• Cs(s1[:i], s2[:j], ρ) is the cost of optimal alignment that ends with substitution of s1[i] with
s2[j].
• Ci(s1[:i], s2[:j], ρ) is the cost of optimal alignment that ends with insertion of s2[j].
• Cd(s1[:i], s2[:j], ρ) is the cost of optimal alignment that ends with deletion of s1[i]."
REFERENCES,0.7345309381237525,"The cost of the overall optimal alignment is simply C(s1[: i], s2[: j], ρ) = min{Cs(s1[: i], s2[:
j], ρ), Ci(s1[:i], s2[:j], ρ), Cd(s1[:i], s2[:j], ρ)}."
REFERENCES,0.7365269461077845,"The dynamic program is given by the following recurrence relation for the cost function which holds
for any i, j > 0, and for any ρ = (ρ1, ρ2, ρ3),"
REFERENCES,0.7385229540918163,"Cs(s1[:i], s2[:j], ρ) = min 
 "
REFERENCES,0.7405189620758483,"ρ1 + Cs(s1[:i −1], s2[:j −1], ρ),
ρ1 + Ci(s1[:i −1], s2[:j −1], ρ),
ρ1 + Cd(s1[:i −1], s2[:j −1], ρ)"
REFERENCES,0.7425149700598802,"Ci(s1[:i], s2[:j], ρ) = min 
 "
REFERENCES,0.7445109780439122,"ρ2 + ρ3 + Cs(s1[:i], s2[:j −1], ρ),
ρ2 + Ci(s1[:i], s2[:j −1], ρ),
ρ2 + ρ3 + Cd(s1[:i], s2[:j −1], ρ)"
REFERENCES,0.7465069860279441,"Cd(s1[:i], s2[:j], ρ) = min 
 "
REFERENCES,0.7485029940119761,"ρ2 + ρ3 + Cs(s1[:i −1], s2[:j], ρ),
ρ2 + ρ3 + Ci(s1[:i −1], s2[:j], ρ),
ρ2 + Cd(s1[:i −1], s2[:j], ρ)"
REFERENCES,0.7504990019960079,Algorithm 5: COMPUTECOMPACTEXECUTIONDAG
REFERENCES,0.7524950099800399,"1: Input: Execution DAG Ge = (Ve, Ee), problem instance (s1, s2)
P0 ←P
2: v1, . . . , vn ←topological ordering of vertices Ve
3: for i = 1 to n do
4:
Let Si be the set of nodes with incoming edges to vi
5:
For vs ∈Si, let Ps denote the partition corresponding to vs
6:
Pi ←COMPUTEOVERLAYDP({Ps | s ∈Si})
7:
for each p ∈Pi do
8:
p′ ←COMPUTESUBDIVISIONDP(p, (s1, s2))
9:
Pi ←Pi \ {p} ∪p′"
REFERENCES,0.7544910179640718,"10:
Pi ←RESOLVEDEGENERACIESDP(Pi)
11: return Partition Pn"
REFERENCES,0.7564870259481038,"By having three subproblems for each (s1[:i], s2[:j]) and ordering the non base-case problems again
in non-decreasing order of i + j, the DP updates again fit our model (1)."
REFERENCES,0.7584830339321357,"J.2
Details of the Execution-DAG based algorithm"
REFERENCES,0.7604790419161677,We start with some well-known terminology from computational geometry.
REFERENCES,0.7624750499001997,"Definition 6. A (convex) subdivision S of P ⊆Rd is a finite set of disjoint d-dimensional (convex)
sets (called cells) whose union is P. The overlay S of subdivisions S1, . . . , Sn is defined as all
nonempty sets of the form T"
REFERENCES,0.7644710578842315,"i∈[n] si with si ∈Si. With slight abuse of terminology, we will refer to
closures of cells also as cells."
REFERENCES,0.7664670658682635,"The COMPUTEOVERLAY procedure takes a set of partitions, which are convex polytopic subdivisions
of Rd, and computes their overlay. We will represent a convex polytopic subdivision as a list of cells,
each represented as a list of bounding hyperplanes. Now to compute the overlay of subdivisions
P1, . . . , PL, with lists of cells C1, . . . , CL respectively, we define |C1|×· · ·×|CL| sets of hyperplanes
Hj1,...,jL = {S"
REFERENCES,0.7684630738522954,"l∈[L] H(c(l)
jl )}, where c(l)
jl is the jl-th cell of Pl and H(c) denotes the hyperplanes
bounding cell c. We compute the cells of the overlay by applying Clarkson’s algorithm [Cla94] to
each Hj1,...,jL. We have the following guarantee about the running time of Algorithm 6."
REFERENCES,0.7704590818363274,Algorithm 6: COMPUTEOVERLAYDP
REFERENCES,0.7724550898203593,"Input: Convex polytopic subdivisions P1, . . . , PL of Rd, represented as lists Cj of hyperplanes
for each cell in the subdivision
H(c(l)
jl ) ←hyperplanes bounding jl-th cell of Pl for l ∈[L], jl ∈Cl
for each j1, . . . , jL ∈|C1|, . . . , |CL| do"
REFERENCES,0.7744510978043913,"Hj1,...,jL ←{S"
REFERENCES,0.7764471057884231,"l∈[L] H(c(l)
jl )}
H′
j1,...,jL ←CLARKSON(Hj1,...,jL)"
REFERENCES,0.7784431137724551,"C ←non-empty lists of hyperplanes in H′
j1,...,jL for jl ∈Cl
return Partition represented by C"
REFERENCES,0.780439121756487,"Lemma J.1. Let Ri,j denote the number of pieces in P[i][j], and ˜R = maxi≤m,j≤n P[i][j]. There is
an implementation of the COMPUTEOVERLAYDP routine in Algorithm 5 which computes the overlay
of L convex polytopic subdivisions in time O(L ˜RL+1 ·LP(d, ˜RL +1)), which is O(d!L ˜R2L+1) using
algorithms for solving low-dimensional LPs [Cha18]."
REFERENCES,0.782435129740519,"Proof. Consider Algorithm 6. We apply the Clarkson’s algorithm at most ˜RL times, once correspond-
ing to each L-tuple of cells from the L subdivisions. Each iteration corresponding to cell c in the
output overlay O (corresponding to C) has a set of at most L ˜R hyperplanes and yields at most Rc
non-redundant hyperplanes. By Theorem 2.1, each iteration takes time O(L ˜R·LP(d, Rc +1)), where
LP(d, Rc +1) is bounded by O(d!Rc) for the algorithm of [Cha18]. Note that P"
REFERENCES,0.7844311377245509,c Rc corresponds to
REFERENCES,0.7864271457085829,"Algorithm 7: COMPUTESUBDIVISIONDP
Input: Convex Polytope P, problem instance (s1, s2)
v ←the DP case q((s1, s2)) for the problem instance
ρ0 ←an arbitrary point in P
(t0
1, t0
2) ←optimal alignment of (s1, s2) for parameter ρ0, using subproblem
(s1[:iv,l0], s2[:jv,l0]) for some l0 ∈[Lv]
mark ←∅, polys ←new hashtable, poly_queue ←new queue
poly_queue.enqueue(l0)
while poly_queue.non_empty() do"
REFERENCES,0.7884231536926147,"l ←poly_queue.dequeue();
Continue to next iteration if l ∈mark
mark ←mark ∪{l}
L ←P
for all subproblems (s1[:iv,l1], s2[:jv,l1]) for l1 ∈[Lv], l1 ̸= l do"
REFERENCES,0.7904191616766467,"Add the half-space inequality bT ρ ≤c corresponding to
cv,l(ρ, (s1, s2)) ≤cv,l1(ρ, (s1, s2)) to L ; /* Label the constraint (b, c) with
l1 */
I ←CLARKSON(L)
poly_queue.enqueue(l′)
for each l′ such that the constraint labeled by it is in I
polys[l] ←{L[ℓ] : ℓ∈I}
return polys"
REFERENCES,0.7924151696606786,"the total number of edges in the cell adjacency graph of O, which is bounded by ˜R2L. Further note that
Rc ≤˜RL for each c ∈C and |C| ≤˜RL to get a runtime bound of O(L ˜RL+1 · LP(d, ˜RL + 1))."
REFERENCES,0.7944111776447106,"We now consider an implementation for the COMPUTESUBDIVISIONDP subroutine. The algorithm
computes the hyperplanes across which the subproblem used for computing the optimal alignment
changes in the recurrence relation (1) by adapting Algorithm 1. We restate the algorithm in the
context of sequence alignment as Algorithm 7."
REFERENCES,0.7964071856287425,"Lemma J.2. Let Ri,j denote the number of pieces in P[i][j], and ˜R = maxi≤m,j≤n Ri,j. There
is an implementation of COMPUTESUBDIVISIONDP routine in Algorithm 5 with running time at
most O((L2d+2 + L2d ˜RL) · LP(d, L2 + ˜RL)) for each outer loop of Algorithm 5. If the algorithm
of [Cha18] is used to solve the LP, this is at most O(d!L2d+2 ˜R2L)."
REFERENCES,0.7984031936127745,"Proof. Consider Algorithm 7. For any piece p in the overlay, all the required subproblems have a
fixed optimal alignment, and we can find the subdivision of the piece by adapting Algorithm 1 (using
O(L2 + ˜RL) hyperplanes corresponding to subproblems and piece boundaries). The number of
pieces in the subdivision is at most L2d since we have at most L2 hyperplanes intersecting the piece,
so we need O(L2d+2 + L2d ˜RL) time to list all the pieces Cp. The time needed to run Clarkson’s
algorithm is upper bounded by O(P"
REFERENCES,0.8003992015968064,"c∈Cp(L2 + ˜RL) · LP(d, Rc + 1)) = O(P"
REFERENCES,0.8023952095808383,c∈Cp(L2 + ˜RL) ·
REFERENCES,0.8043912175648703,"LP(d, L2 + ˜RL)) = O((L2d+2 + L2d ˜RL) · LP(d, L2 + ˜RL)). Using [Cha18] to solve the LP, this
is at most O(d! ˜R2LL2d+4)."
REFERENCES,0.8063872255489022,"Lemma J.3. Let Ri,j denote the number of pieces in P[i][j], and ˜R = maxi≤m,j≤n Ri,j. There is
an implementation of RESOLVEDEGENERACIESDP routine in Algorithm 5 with running time at
most O( ˜R2LL4d) for each outer loop of Algorithm 5."
REFERENCES,0.8083832335329342,"Proof. The RESOLVEDEGENERACIESDP is computed by a simple BFS over the cell adjacency
graph Gc = (Vc, Ec) (i.e. the graph with polytopic cells as nodes and edges between polytopes
sharing facets). We need to find (maximal) components of a subgraph of the cell adjacency graph
where each node in the same component has the same optimal alignment. This is achieved by a
simple BFS in O(|Vc| + |Ec|) time. Indeed, by labeling each polytope with the corresponding
optimal alignment, we can compute the components of the subgraph of Gc with edges restricted"
REFERENCES,0.810379241516966,"to nodes joining the same optimal alignment. Note that the resulting polytopic subdivision after
the merge is still a convex subdivision using arguments in Lemma J.5, but applied to appropriate
sequence alignment subproblem. As noted in the proof of Lemma J.2, we have |Vc| ≤L2d ˜RL"
REFERENCES,0.812375249500998,"since the number of cells within each piece p is at most L2d and there are at most ˜RL pieces in the
overlay. Since |Ec| ≤|Vc|2, we have an implementation of RESOLVEDEGENERACIESDP in time
O((L2d ˜RL)2) = O( ˜R2LL4d)."
REFERENCES,0.8143712574850299,Finally we can put all the above together to give a proof of Theorem 4.1.
REFERENCES,0.8163672654690619,"Proof of Theorem 4.1. The proof follows by combining Lemma J.1, Lemma J.2 and Lemma J.3. Note
that in the execution DAG, we have |Ve| ≤|Ee| = O(TDP). Further, we invoke COMPUTEOVER-
LAYDP and RESOLVEDEGENERACIESDP |Ve| times across all iterations and COMPUTESUBDIVI-
SIONDP across the |Ve| outer loops."
REFERENCES,0.8183632734530938,"J.3
Additional Proofs"
REFERENCES,0.8203592814371258,"The following results closely follow and extend the corresponding results from [BDD+21]. Specifi-
cally, we generalize to the case of two sequences of unequal length, and provide sharper bounds on
the number of distinct alignments and boundary functions in the piecewise decomposition (even in
the special case of equal lengths). We first have a bound on the total number of distinct alignments.
Lemma J.4. For a fixed pair of sequences s1, s2 ∈Σm × Σn, with m ≤n, there are at most
m(m + n)m distinct alignments."
REFERENCES,0.8223552894211577,"Proof. For any alignment (t1, t2), by definition, we have |t1| = |t2| and for all i ∈[|t1|], if
t1[i] = −, then t2[i] ̸= −and vice versa. This implies that t1 has exactly n −m more gaps
than t2. To prove the upper bound, we count the number of alignments (t1, t2) where t2 has
exactly i gaps for i ∈[m]. There are
 n+i
i

choices for placing the gap in t2. Given a fixed
t2 with i gaps, there are
 
n
n−m+i

choices for placing the gap in t1. Thus, there are at most
 n+i
i
 
n
n−m+i

=
(n+i)!
i!(m−i)!(n−m+i)! ≤(m + n)m possibilities since i ≤m. Summing over all i, we
have at most m(m + n)m alignments of s1, s2."
REFERENCES,0.8243512974051896,"This implies that the dual class functions are piecewise-structured in the following sense.
Lemma J.5. Let U be the set of functions {uρ : (s1, s2) 7→u(s1, s2, ρ) | ρ ∈Rd} that map sequence
pairs s1, s2 ∈Σm × Σn to R by computing the optimal alignment cost C(s1, s2, ρ) for a set of
features (li(·))i∈[d]. The dual class U∗is (F, G, m2(m + n)2m)-piecewise decomposable, where
F = {fc : U →R | c ∈R} consists of constant functions fc : uρ 7→c and G = {gw : U →{0, 1} |
w ∈Rd} consists of halfspace indicator functions gw : uρ 7→I{w · ρ < 0}."
REFERENCES,0.8263473053892215,"Proof. Fix a pair of sequences s1 and s2. Let τ be the set of optimal alignments as we range over
all parameter vectors ρ ∈Rd. By Lemma J.4, we have |τ| ≤m(m + n)m. For any alignment
(t1, t2) ∈τ, the algorithm Aρ will return (t1, t2) if and only if d
X"
REFERENCES,0.8283433133732535,"i=1
ρili(s1, s2, t1, t2) > d
X"
REFERENCES,0.8303393213572854,"i=1
ρili(s1, s2, t′
1, t′
2)"
REFERENCES,0.8323353293413174,"for all (t′
1, t′
2) ∈τ \ {(t1, t2)}. Therefore, there is a set H of at most
 |τ|
2

≤m2(m + n)2m"
REFERENCES,0.8343313373253493,"hyperplanes such that across all parameter vectors ρ in a single connected component of Rd \ H,
the output of the algorithm Aρ on (s1, s2) is fixed. This means that for any connected component
R of Rd \ H, there exists a real value cR such that uρ(s1, s2) = cR for all ρ ∈Rd. By definition
of the dual, u∗
s1,s2(uρ) = uρ(s1, s2) = cR. For each hyperplane h ∈H, let g(h) ∈G denote the
corresponding halfspace. Order these k =
 |τ|
2

functions arbitrarily as g1, . . . , gk. For a given
connected component R of Rd \ H, let bR ∈{0, 1}k be the corresponding sign pattern. Define the
function f (bR) = fcR and for b not corresponding to any R, f (b) = f0. Thus, for each ρ ∈Rd,"
REFERENCES,0.8363273453093812,"u∗
s1,s2(uρ) =
X"
REFERENCES,0.8383233532934131,"b∈{0,1}k
I{gi(uρ) = bi∀i ∈[k]}f (b)(uρ)."
REFERENCES,0.8403193612774451,"For the special case of d = 2, we have an algorithm which runs in time O(RTDP), where R is the
number of pieces in P[m][n] which improves on the prior result O(R2 + RTDP) for two-parameter
sequence alignment problems. The algorithm employs the ray search technique of [Meg78] (also
employed by [GBN94] but for more general sequence alignment problems) and enjoys the following
runtime guarantee.
Theorem J.6. For the global sequence alignment problem with d = 2, for any problem instance
(s1, s2), there is an algorithm to compute the pieces for the dual class function in O(RTDP) time,
where TDP is the time complexity of computing the optimal alignment for a fixed parameter ρ ∈R2,
and R is the number of pieces of u(s1,s2)(·)."
REFERENCES,0.8423153692614771,"Proof. We note that for any alignment (t1, t2), the boundary functions for the piece where (t1, t2) is
an optimal alignment are straight lines through the origin of the form"
REFERENCES,0.844311377245509,"ρ1l1(s1, s2, t1,t2) + ρ2l2(s1, s2, t1, t2) > ρ1l1(s1, s2, t′
1, t′
2) + ρ2l2(s1, s2, t′
1, t′
2)"
REFERENCES,0.846307385229541,"for some alignment (t′
1, t′
2) different from (t1, t2). The intersection of these halfplanes is either
the empty set or the region between two straight lines through the origin. The output subdivision
therefore only consists of the axes and straight lines through the origin in the positive orthant."
REFERENCES,0.8483033932135728,"We will present an algorithm using the ray search technique of [Meg78]. The algorithm computes
the optimal alignment (t1, t2), (t′
1, t′
2) at points ρ = (0, 1) and ρ = (1, 0). If the alignments are
identical, we conclude that (t1, t2) is the optimal alignment everywhere. Otherwise, we find the
optimal alignment (t′′
1, t′′
2) for the intersection of line L joining ρ = (0, 1) and ρ = (1, 0), with the
line L′ given by"
REFERENCES,0.8502994011976048,"ρ1l1(s1, s2, t1,t2) + ρ2l2(s1, s2, t1, t2) > ρ1l1(s1, s2, t′
1, t′
2) + ρ2l2(s1, s2, t′
1, t′
2)"
REFERENCES,0.8522954091816367,"If (t′′
1, t′′
2) = (t1, t2) or (t′′
1, t′′
2) = (t′
1, t′
2), we have exactly 2 optimal alignments and the piece
boundaries are given by L′ and the axes. Otherwise we repeat the above process for alignment pairs
(t′′
1, t′′
2), (t′
1, t′
2) and (t′′
1, t′′
2), (t1, t2). Notice we need to compute at most R + 1 dynamic programs
to compute all the pieces, giving the desired time bound."
REFERENCES,0.8542914171656687,NeurIPS Paper Checklist
REFERENCES,0.8562874251497006,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
REFERENCES,0.8582834331337326,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
REFERENCES,0.8602794411177644,"• You should answer [Yes] , [No] , or [NA] ."
REFERENCES,0.8622754491017964,"• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
REFERENCES,0.8642714570858283,• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
REFERENCES,0.8662674650698603,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
REFERENCES,0.8682634730538922,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
REFERENCES,0.8702594810379242,"IMPORTANT, please:"
REFERENCES,0.872255489021956,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"","
REFERENCES,0.874251497005988,"• Keep the checklist subsection headings, questions/answers and guidelines below."
REFERENCES,0.8762475049900199,• Do not modify the questions and only use the provided macros for your answers.
CLAIMS,0.8782435129740519,1. Claims
CLAIMS,0.8802395209580839,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.8822355289421158,Answer: [Yes]
CLAIMS,0.8842315369261478,"Justification: We clear state the claims and make sure they accurately reflect our contribu-
tions."
CLAIMS,0.8862275449101796,Guidelines:
CLAIMS,0.8882235528942116,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.8902195608782435,2. Limitations
LIMITATIONS,0.8922155688622755,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8942115768463074,Answer: [Yes]
LIMITATIONS,0.8962075848303394,"Justification: We clearly state that our results lead to faster algorithms when the output-size
is small, and worst-case improved runtimes are left for future work.
Guidelines:"
LIMITATIONS,0.8982035928143712,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
LIMITATIONS,0.9001996007984032,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Complete proofs and details are in the Appendix and appropriately referenced
from the main body.
Guidelines:"
LIMITATIONS,0.9021956087824351,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
LIMITATIONS,0.9041916167664671,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification:
Guidelines:"
LIMITATIONS,0.906187624750499,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
LIMITATIONS,0.908183632734531,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification:
Guidelines:"
LIMITATIONS,0.9101796407185628,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable)."
LIMITATIONS,0.9121756487025948,"• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
LIMITATIONS,0.9141716566866267,6. Experimental Setting/Details
LIMITATIONS,0.9161676646706587,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
LIMITATIONS,0.9181636726546906,Answer: [NA]
LIMITATIONS,0.9201596806387226,Justification:
LIMITATIONS,0.9221556886227545,Guidelines:
LIMITATIONS,0.9241516966067864,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9261477045908184,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9281437125748503,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9301397205588823,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9321357285429142,Justification:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9341317365269461,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.936127744510978,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.93812375249501,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9401197604790419,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9421157684630739,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.9441117764471058,Justification:
EXPERIMENTS COMPUTE RESOURCES,0.9461077844311377,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9481037924151696,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
EXPERIMENTS COMPUTE RESOURCES,0.9500998003992016,"• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENTS COMPUTE RESOURCES,0.9520958083832335,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We follow the Ethics Guidelines.
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9540918163672655,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENTS COMPUTE RESOURCES,0.9560878243512974,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: We view our work as theoretical, with techniques leading to faster machine
learning algorithms.
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9580838323353293,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
EXPERIMENTS COMPUTE RESOURCES,0.9600798403193613,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:"
EXPERIMENTS COMPUTE RESOURCES,0.9620758483033932,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9640718562874252,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9660678642714571,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9680638722554891,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9700598802395209,Justification:
LICENSES FOR EXISTING ASSETS,0.9720558882235529,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9740518962075848,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9760479041916168,13. New Assets
NEW ASSETS,0.9780439121756487,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9800399201596807,Answer: [NA]
NEW ASSETS,0.9820359281437125,Justification:
NEW ASSETS,0.9840319361277445,Guidelines:
NEW ASSETS,0.9860279441117764,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880239520958084,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900199600798403,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920159680638723,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940119760479041,"Justification:
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960079840319361,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998003992015968,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
