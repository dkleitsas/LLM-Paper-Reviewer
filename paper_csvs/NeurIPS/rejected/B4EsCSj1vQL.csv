Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024096385542168677,"A major tenet of conventional wisdom dictates that models should not be over-
1"
ABSTRACT,0.004819277108433735,"parameterized: the number of free parameters should not exceed the number of
2"
ABSTRACT,0.007228915662650603,"training data points. This tenet originates from centuries of shallow learning, pri-
3"
ABSTRACT,0.00963855421686747,"marily in the form of linear or logistic regression. It is routinely applied to all kinds
4"
ABSTRACT,0.012048192771084338,"of data analyses and modeling and even to infer properties of the brain. However,
5"
ABSTRACT,0.014457831325301205,"through a variety of precise mathematical examples, we show that this conventional
6"
ABSTRACT,0.016867469879518072,"wisdom is completely wrong as soon as one moves from shallow to deep learning.
7"
ABSTRACT,0.01927710843373494,"In particular, we construct sequences of both linear and non-linear deep learning
8"
ABSTRACT,0.021686746987951807,"models whose number of parameters can grow to arbitrarily large values, and which
9"
ABSTRACT,0.024096385542168676,"remain well deﬁned and trainable using a ﬁxed, ﬁnite size, training set. In deep
10"
ABSTRACT,0.02650602409638554,"models, the parameter space is partitioned into large equivalence classes. Learning
11"
ABSTRACT,0.02891566265060241,"can be viewed as a communication process where information is communicated
12"
ABSTRACT,0.03132530120481928,"from the data to the synaptic weights. The information in the training data only can,
13"
ABSTRACT,0.033734939759036145,"and needs to, specify an equivalence class of the parameters. It cannot, and does
14"
ABSTRACT,0.03614457831325301,"not need to, specify individual parameter values. As such, the number of training
15"
ABSTRACT,0.03855421686746988,"examples can be smaller than the number of free parameters.
16"
INTRODUCTION,0.04096385542168675,"1
Introduction
17"
INTRODUCTION,0.043373493975903614,"A long held form of conventional wisdom is that in order to train a model with n parameters one
18"
INTRODUCTION,0.04578313253012048,"should have at least n training examples, and preferably more. The origin of this statistical “dogma”
19"
INTRODUCTION,0.04819277108433735,"stems from linear regression and other forms of shallow learning1. The soundness of this dogma
20"
INTRODUCTION,0.05060240963855422,"appears to be obvious from our experiences with linear regression: in general n examples are
21"
INTRODUCTION,0.05301204819277108,"necessary and sufﬁcient in order to solve a system of n linear equations in n unknown variables. As a
22"
INTRODUCTION,0.05542168674698795,"result, the dogma is routinely repeated and used in myriads applications of statistics to modeling data
23"
INTRODUCTION,0.05783132530120482,"across all areas of human inquiry, often well beyond shallow learning, and to inspire a fear, if not a
24"
INTRODUCTION,0.060240963855421686,"disgust, for the so-called over-parameterized models. The dogma is also routinely used in a variety
25"
INTRODUCTION,0.06265060240963856,"of “back-of-the-enveloppe”calculations, for instance to infer properties or processing strategies for
26"
INTRODUCTION,0.06506024096385542,"the human brain. Here we show, through a variety of examples, that this central dogma is valid only
27"
INTRODUCTION,0.06746987951807229,"for shallow learning and that it is completely wrong when it comes to deep learning. Hence, in deep
28"
INTRODUCTION,0.06987951807228916,"learning it may not be unwise to get rid of the conventional wisdom entirely.
29"
THE ORIGIN OF THE DOGMA,0.07228915662650602,"1.1
The Origin of the Dogma
30"
THE ORIGIN OF THE DOGMA,0.0746987951807229,"For the past three centuries, since the discovery of least square linear regression by Gauss and
31"
THE ORIGIN OF THE DOGMA,0.07710843373493977,"Legendre in the late 1700s (e.g. [9]), one of the most central dogma of statistics has been that a model
32"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.07951807228915662,"1The mathematically correct distinction between shallow and deep learning is whether there are hidden
units/layers or not"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.0819277108433735,"should not have more parameters than data points. There is little doubt that the origin of this dogma
33"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.08433734939759036,"lies in linear regression, or equivalently in linear systems of equations where in general if there are n
34"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.08674698795180723,"unknown variables one needs n linear equations (or training examples) to uniquely solve the system.
35"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.0891566265060241,"However, this is not a characteristic of linear systems alone. The same holds true immediately for
36"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.09156626506024096,"logistic regression. Since the logistic function is monotone increasing, it has a unique inverse and
37"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.09397590361445783,"by inverting the targets one can reduce logistic regression to a linear system. While this is true for
38"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.0963855421686747,"single linear or logistic neurons, the same result holds for a shallow layer of linear or logistic neurons,
39"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.09879518072289156,"since in this case each neuron operates and learns independently of all the other neurons. Similar
40"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.10120481927710843,"observations can be made for single-variable polynomial regression. Thus, in short, the origin of the
41"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.10361445783132531,"conventional wisdom can easily be traced back to shallow learning and basic results in linear algebra.
42"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.10602409638554217,"Not only the soundness of the dogma seems obvious from basic linear algebra considerations, but
43"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.10843373493975904,"its violation in shallow learning leads to two kinds of problems: (1) an over-parameterized shallow
44"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.1108433734939759,"model is not well deﬁned, in the sense that its parameters are not uniquely determined by the data;
45"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.11325301204819277,"and, as a result, (2) such a model can overﬁt the data by achieving low error on the training data,
46"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.11566265060240964,"while performing poorly on held out data. Finally, the widespread aversion for over-parameterized
47"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.1180722891566265,"models stems also from our sense of elegance and simpliciy, as embodied in the principle of Occam’s
48"
THE MATHEMATICALLY CORRECT DISTINCTION BETWEEN SHALLOW AND DEEP LEARNING IS WHETHER THERE ARE HIDDEN,0.12048192771084337,"razor.
49"
APPLICATIONS OF THE DOGMA,0.12289156626506025,"1.2
Applications of the Dogma
50"
APPLICATIONS OF THE DOGMA,0.12530120481927712,"While the dogma makes sense for shallow learning situations, it is often applied to deep learning
51"
APPLICATIONS OF THE DOGMA,0.12771084337349398,"situations. For instance, many articles have been published in the literature recommending that deep
52"
APPLICATIONS OF THE DOGMA,0.13012048192771083,"learning models ought to have training sets that are 10 times [1] or 50 times [2] bigger than the
53"
APPLICATIONS OF THE DOGMA,0.13253012048192772,"number of free parameters. Obviously these arbitrary, constant, and widely discording prescriptive
54"
APPLICATIONS OF THE DOGMA,0.13493975903614458,"multiplicative factors should be viewed with a grain of suspicion.
55"
APPLICATIONS OF THE DOGMA,0.13734939759036144,"Another standard application of the dogma is to infer properties of complex, non-shallow systems,
56"
APPLICATIONS OF THE DOGMA,0.13975903614457832,"like the brain. For instance, Geoff Hinton and others like to point out that the human brain has on
57"
APPLICATIONS OF THE DOGMA,0.14216867469879518,"the order of say 1015 synapses, while human lives last on the order of 3 × 109 seconds. Assuming
58"
APPLICATIONS OF THE DOGMA,0.14457831325301204,"one training example per second, or even 1000 training examples per second, the brain does not have
59"
APPLICATIONS OF THE DOGMA,0.14698795180722893,"enough training examples to train its army of synapses. From this false premise, one may draw all
60"
APPLICATIONS OF THE DOGMA,0.1493975903614458,"kinds of conclusion from “the brain must be doing something special” to “the majority of synapses
61"
APPLICATIONS OF THE DOGMA,0.15180722891566265,"must be hardwired”. However, as we shall see, all these conclusions are worthless: they may be false
62"
APPLICATIONS OF THE DOGMA,0.15421686746987953,"or true, since they are derived from a false premise. The false premise is obtained by applying a
63"
APPLICATIONS OF THE DOGMA,0.1566265060240964,"statistical principle, correctly observed in shallow learning situations, to deep learning situations.
64"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.15903614457831325,"2
Preliminary Evidence against the Dogma
65"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1614457831325301,"Preliminary evidence that something may be wrong with the dogma comes from at least three
66"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.163855421686747,"directions: Bayesian statistical theory, statistical ensembles, and deep learning practice.
67"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.16626506024096385,"From a purely Bayesian perspective, selecting the complexity of a model based on the amount of
68"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1686746987951807,"training data makes no sense at all, as there is in general no relationship between the two. Using a
69"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1710843373493976,"prior that favors simple models may be convenient, or satisfy tradition, however there is no intrinsic
70"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.17349397590361446,"epistemological reason for selecting such a prior. If anything, a situation with few data points may be
71"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.17590361445783131,"the sign that data are hard or expensive to acquire. In turn, this is possibly the sign of an underlying
72"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1783132530120482,"complex phenomena, which may call for a complex model rather than a simple one. Using a prior
73"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.18072289156626506,"that favors models with few parameters is analogous to the paradigm of searching for one’s car keys
74"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.18313253012048192,"at night under the only lamp present in a dark parking lot: there is no epistemoligical reason for the
75"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1855421686746988,"keys to be under the lamp. But what about Occaam’s razor? As noted in [10, 11], such a prior is
76"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.18795180722891566,"not needed to implement Occam’s razor which naturally emerges from the Bayesian framework. To
77"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.19036144578313252,"see this in a simple way, imagine having an overall class of models comprising two sub-classes of
78"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.1927710843373494,"models: simple models (S) and complex models (C). Imagine that a priori one has no preference
79"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.19518072289156627,"between the two classes S and C, and likewise that within each class one has no preference among
80"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.19759036144578312,"the models in that class. Let s and c denote the value of the constant prior probability shared by all
81"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2,"the models in class S and in class C respectively. Thus the overall prior distribution must satisfy
82"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.20240963855421687,s|S| + c|C| = 1
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.20481927710843373,Input #1
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.20722891566265061,Input #2
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.20963855421686747,Input #3
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.21204819277108433,Input #4
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.21445783132530122,Output
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.21686746987951808,"Hidden
layer
Input
layer"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.21927710843373494,"Output
layer"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2216867469879518,"Figure 1: A A(4, 5, 1) architecture."
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.22409638554216868,"where |S| and |C| represent the volumes of the corresponding classes. Because the complex models
83"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.22650602409638554,"have more parameters, in general |S| << |C|. As a result, we must have: s >> c. In short, simple
84"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2289156626506024,"models will automatically have a much higher prior probability, and this effect will tend to be reﬂected
85"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.23132530120481928,"also in the posterior probabilities.
86"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.23373493975903614,"A second line of evidence against the soundness of the dogma comes from the widespread use,
87"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.236144578313253,"and recognized effectiveness, of statistical ensembles, where many different models are combined
88"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2385542168674699,"together, for instance through a simple weighted averaging operation. This combination alone
89"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.24096385542168675,"generally results in a deep overall model, even if the individual models are shallow. And even if
90"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2433734939759036,"the number of parameters of each individual model satisﬁes the dogma, obviously as the number of
91"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2457831325301205,"models in the ensembles is increased, there is a point where the overall model starts to violate the
92"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.24819277108433735,"dogma. Perhaps surprisingly, the over-parameterization aspect of ensembles does not seem to have
93"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.25060240963855424,"systematically worried statisticians.
94"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.25301204819277107,"Finally, and perhaps most importantly, it has been observed several times that in deep learning practice
95"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.25542168674698795,"that over-parameterized models can work well, with no signiﬁcant sign of overﬁtting. However, this
96"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.25783132530120484,"phenomena has been used either to criticize deep learning, or is regarded as some kind of oddity or a
97"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.26024096385542167,"mystery (e.g. [12, 13, 8]), possibly requiring novel strategies for combating the over-ﬁtting curse.
98"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.26265060240963856,"Here we set out to prove why the conventional wisdom is simply wrong when it comes to deep
99"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.26506024096385544,"learning. In particular we give several examples of large networks with many parameters that can be
100"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2674698795180723,"trained with far fewer examples in both the linear and non-linear cases. We consider primarily the
101"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.26987951807228916,"supervised learning framework, but through the use of autoencoder architectures we show that the
102"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.27228915662650605,"same basic ideas can be applied to the unsupervised, or semi-supervised, learning frameworks. At
103"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2746987951807229,"the linear end of the spectrum of models, we look at deep, fully-connected ,linear networks. At the
104"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.27710843373493976,"other extreme non-linear end of the spectrum, we look at deep, fully-connected, unrestricted Boolean
105"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.27951807228915665,"networks. And in the middle of the spectrum, we look at deep fully-connected networks of linear
106"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.2819277108433735,"threshold gates.
107"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.28433734939759037,"Notation: We use the notation A(n0, n1, . . . , nL) to denote a deep feedforward architectures with
108"
PRELIMINARY EVIDENCE AGAINST THE DOGMA,0.28674698795180725,"ni units in layer i, where the input layer is layer 0 and the output layer is layer L (Figure 1).
109"
THE LINEAR REGIME,0.2891566265060241,"3
The Linear Regime
110"
THE LINEAR REGIME,0.29156626506024097,"Deep feed-forward linear networks have been studied for quite some time (e.g. [4, 7, 5, 6]) in the
111"
THE LINEAR REGIME,0.29397590361445786,"context of least square linear regression. One of the main theoretical results is that, in the fully-
112"
THE LINEAR REGIME,0.2963855421686747,"connected case, the error functions of these networks does not have any spurious local minima. All
113"
THE LINEAR REGIME,0.2987951807228916,"the critical points where the gradient of the error function is zero are either global minima or saddle
114"
THE LINEAR REGIME,0.30120481927710846,"points. As a result, properly applied stochastic gradient descent will tend to converge to a global
115"
THE LINEAR REGIME,0.3036144578313253,"minimum. The structure of the global minima and the saddle points can be understood in terms of
116"
THE LINEAR REGIME,0.3060240963855422,"Principal Component Analysis (CS). Clearly, as the depth of these models is increased the number
117"
THE LINEAR REGIME,0.30843373493975906,"Figure 2: An A(1, . . . , 1) architecture with L single-layer neurons. There is a single synaptic weight wi
connecting neuron i −1 to neuroni. In the linear case, with no biases, the input-output function is given by
y = Px where P is the product of all the synaptic weights. While the number of parameters L can be arbitrarily
large, a single training example is sufﬁcient to constrain the value of the multiplier. Gradient descent rapidly
converges onto an optimal solution where the product of the synaptic weight has the optimal value: P = α/β
where α = E(xt) and β = E(x2) (see text)."
THE LINEAR REGIME,0.3108433734939759,"of parameters can grow to inﬁnity. But what are the requirements on the size of the corresponding
118"
THE LINEAR REGIME,0.3132530120481928,"training sets?
119"
THE SIMPLEST DEEP LINEAR MODEL,0.3156626506024096,"3.1
The Simplest Deep Linear Model
120"
THE SIMPLEST DEEP LINEAR MODEL,0.3180722891566265,"To begin with, we consider an architecture A(1, 1, . . . , 1), with a single linear neuron in each layer
121"
THE SIMPLEST DEEP LINEAR MODEL,0.3204819277108434,"(Figure 2). For simplicity we assume that there are no biases, but the same analysis can easily be
122"
THE SIMPLEST DEEP LINEAR MODEL,0.3228915662650602,"extended to the case with biases. The weights are w1, . . . , wL and the neural network behaves as a
123"
THE SIMPLEST DEEP LINEAR MODEL,0.3253012048192771,"multiplier, in the sense that given an input x the output is simply:
124"
THE SIMPLEST DEEP LINEAR MODEL,0.327710843373494,"y = Px
with
P =
Y i
wi"
THE SIMPLEST DEEP LINEAR MODEL,0.3301204819277108,"This is a deep linear regression architecture with L parameters. The supervised training data consists
125"
THE SIMPLEST DEEP LINEAR MODEL,0.3325301204819277,"of input-target pairs of the form (x, t) that provide information about what the overall multiplier P
126"
THE SIMPLEST DEEP LINEAR MODEL,0.3349397590361446,"should be. Taking expectations over the training data, let E(tx) = α and E(x2) = β. The error
127"
THE SIMPLEST DEEP LINEAR MODEL,0.3373493975903614,"E is the standard least square error. It is easy to check that the error is convex in P and that at the
128"
THE SIMPLEST DEEP LINEAR MODEL,0.3397590361445783,"optimum one must have α −βP = 0 or P = α/β. It can be shown (see [3]) that, except for trivial
129"
THE SIMPLEST DEEP LINEAR MODEL,0.3421686746987952,"cases, given any initial starting point, gradient descent, or even random backpropagation (feedback
130"
THE SIMPLEST DEEP LINEAR MODEL,0.344578313253012,"alignment), will converge to a global minimum satisfying P = α/β.
131"
THE SIMPLEST DEEP LINEAR MODEL,0.3469879518072289,"While the architecture has an arbitrary large number of parameters L, in principle a single training
132"
THE SIMPLEST DEEP LINEAR MODEL,0.3493975903614458,"example is sufﬁcient to determine the value of the correct multiplier. The value of the overall product
133"
THE SIMPLEST DEEP LINEAR MODEL,0.35180722891566263,"P partitions the space of synaptic weights into equivalence classes: all the architectures which
134"
THE SIMPLEST DEEP LINEAR MODEL,0.3542168674698795,"produce the same value P are equivalent. The training data need only to provide enough information
135"
THE SIMPLEST DEEP LINEAR MODEL,0.3566265060240964,"for selecting one equivalence class, but not the value of the individual weights within the equivalence
136"
THE SIMPLEST DEEP LINEAR MODEL,0.35903614457831323,"class. Thus there is a manifold of equivalent solutions satisfying the optimal relationship P = α/β
137"
THE SIMPLEST DEEP LINEAR MODEL,0.3614457831325301,"and the volume of this manifold grows with the number L of parameters. However the training set
138"
THE SIMPLEST DEEP LINEAR MODEL,0.363855421686747,"can remain as small as a single training example, a clear violation of the dogma.
139"
THE SIMPLEST DEEP LINEAR MODEL,0.36626506024096384,"Of course, here and everywhere else in the following examples, one may wonder what could be the
140"
THE SIMPLEST DEEP LINEAR MODEL,0.3686746987951807,"purpose of having L layers, when a single layer could be sufﬁcient to implement the same overall
141"
THE SIMPLEST DEEP LINEAR MODEL,0.3710843373493976,"input-output function. There could be multiple purposes. The most obvious one is that the volume
142"
THE SIMPLEST DEEP LINEAR MODEL,0.37349397590361444,"of the solutions grows with the depth of the architectures and this may facilitate learning. But in
143"
THE SIMPLEST DEEP LINEAR MODEL,0.3759036144578313,"addition, one must also think about the possible constraints that may be associated with physical
144"
THE SIMPLEST DEEP LINEAR MODEL,0.3783132530120482,"neural systems, as opposed to the virtualized simulations of neural systems we routinely carry on
145"
THE SIMPLEST DEEP LINEAR MODEL,0.38072289156626504,"our digital computers using the likes of Keras, PyTorch, and TensorFlow. For example, even in the
146"
THE SIMPLEST DEEP LINEAR MODEL,0.38313253012048193,"simplest linear case described above, imagine that the overall desired multiplier is P = 210 = 1024
147"
THE SIMPLEST DEEP LINEAR MODEL,0.3855421686746988,"but that the individual synaptic weights connecting one neuron to the next are bounded in the [−2, +2]
148"
THE SIMPLEST DEEP LINEAR MODEL,0.38795180722891565,"range. Then no architecture with less than 10 layers is capable of implementing the optimal input-
149"
THE SIMPLEST DEEP LINEAR MODEL,0.39036144578313253,"output function. Deeper architectures are needed to implement the overall optimal function and to
150"
THE SIMPLEST DEEP LINEAR MODEL,0.3927710843373494,"robustly distribute the load across multiple synapses.
151"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.39518072289156625,"3.2
Deep Linear Models with No Bottlenecks
152"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.39759036144578314,"At ﬁrst sight, one may tempted to think that the example above is due to the fact that there is a
153"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.4,"single neuron per layer. However, this is not the case and exactly the same phenomena is observed
154"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.40240963855421685,"for a linear regression architectures of the form A(n, n, . . . , n) where all the layers have size n
155"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.40481927710843374,"and the weights are given by matrices W1, . . . , WL. Again, in vector-matrix form, the input output
156"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.4072289156626506,"relationship is given by:
157"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.40963855421686746,"y = Px
with
P = WLWL−1 . . . W1"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.41204819277108434,"Again it is easy to see that this architecture has Ln2 parameters. The overall input-output function
158"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.41445783132530123,"corresponds to a singlen × n matrix P. But in order to specify such a linear map, we only need
159"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.41686746987951806,"to specify the images of the canonical basis of Rn, in other words, n training examples in general
160"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.41927710843373495,"position are sufﬁcient, again violating the dogma.
161"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.42168674698795183,"Note that this property remains true if the architectures also contains expansive hidden layers of size
162"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.42409638554216866,"greater than n, or if the input and output layers have different sizes and all the hidden layers have size
163"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.42650602409638555,"greater than the input and output layer (i.e. the hidden layers do not affect the rank of the optimal
164"
DEEP LINEAR MODELS WITH NO BOTTLENECKS,0.42891566265060244,"overall input-output function).
165"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.43132530120481927,"3.3
Deep Linear Models with Bottlenecks
166"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.43373493975903615,"In the previous two examples, all the layers have the same size, or are expansive. However it is easy to
167"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.43614457831325304,"relax this assumption and consider compressive architectures. To begin with, consider a purely linear
168"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.43855421686746987,"compressive autoencoder architecture of the form A(n, m, n), with m < n (Figure 3). In this case,
169"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.44096385542168676,"the bottleneck layer imposes a rank restriction on the overall transformation. It is well known [4] that
170"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4433734939759036,"not only the quadratic error function of such an autoencoder has no spurious local minima, but all
171"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4457831325301205,"its critical points correspond, up to changes of coordinates in the hidden layer, to projections onto
172"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.44819277108433736,"subspaces spanned by eigenvectors of the data covariance matrix. The global minima is associated
173"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4506024096385542,"with Principal Component Analysis using projections onto a subspace of dimension m. Obviously
174"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4530120481927711,"one can include additional linear layers of size greater or equal to m between the input layer and
175"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.45542168674698796,"the bottleneck layer, or between the bottleneck layer and the output layer, arbitrarily increasing the
176"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4578313253012048,"total number of parameters, but without affecting the essence of the optimal solution. The minimal
177"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4602409638554217,"training set to specify the optimal solution consists of m vectors of size n to specify the project
178"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.46265060240963857,"hyperplane, providing another egregious violation of the dogma. Again there are large equivalence
179"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4650602409638554,"classes of parameters associated with the same overall performance (e.g. in the linear case with a
180"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4674698795180723,"single bottleneck, we have P = AB = ACC−1B; thus the overall map P is deﬁned up to invertible
181"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.46987951807228917,"transformations applied to the hidden layer). The results in [4, 7] show that the same observations
182"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.472289156626506,"can be made for arbitrary fully connected deep linear architectures (i.e. beyond autoencoders) and
183"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4746987951807229,"not only in the real-valued case, but also in the complex-valued case [6].
184"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4771084337349398,"All the previous examples correspond to linear networks. Thus one may be mislead to think that
185"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4795180722891566,"the analyses apply only to linear networks. Next we show that exactly the same phenomena can be
186"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4819277108433735,"observed in non-linear deep architectures. Among the non-linear model to be discussed, we will
187"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4843373493975904,"examine ﬁrst the most non-linear model of all which is the unrestricted Boolean model, where each
188"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4867469879518072,"neuron implements a Boolean function, with no restrictions on the kinds of Boolean functions. An
189"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4891566265060241,"unrestricted Boolean neuron with n inputs implements a function f with 2n parameters, since one
190"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.491566265060241,"must specify one binary value for each of the 2n possible entries of the truth table of f. Then we will
191"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4939759036144578,"consider also the case of Boolean neurons implemented by linear threshold functions, or perceptrons.
192"
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4963855421686747,"Figure 3: An A(n, m, n) compressive (m < n) autoencoder architecture. In the linear case, the transformations
A and B correspond to matrices and the overall linear transformation P is given by: y = Px = ABx."
DEEP LINEAR MODELS WITH BOTTLENECKS,0.4987951807228916,"4
The Non-Linear Regime: Unrestricted Boolean Model
193"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5012048192771085,"4.1
The Simplest Deep Non-Linear Model
194"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5036144578313253,"We can use the same architecture A(1, . . . , 1) as in the ﬁrst example above. In the Boolean unrestricted
195"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5060240963855421,"model, each Boolean function from one neuron to the next is either the identity or the negation
196"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5084337349397591,"(Boolean NOT function). So there is one binary degree of freedom associated with each layer and
197"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5108433734939759,"again the number of degrees of freedom grows linearly with the depth. The overall input-output
198"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5132530120481927,"function is either the identity, or the negation, and a single training example is sufﬁcient to establish
199"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5156626506024097,"whether the overall function ought to be the identity or the negation of the identity. If the architecture
200"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5180722891566265,"contains an even number of negations the overall input-output function is the identity, and if the
201"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5204819277108433,"architecture contains and odd number of negations, the overall input-output function is the negation.
202"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5228915662650603,"Thus again the dogma is violated.
203"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5253012048192771,"To get a slightly more interesting non-linear example, we can use the same architecture A(1, . . . , 1)
204"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5277108433734939,"as in the ﬁrst example above, with L weights w1, . . . , wL. The difference is that all the neurons have
205"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5301204819277109,"a non-linear activation function g(x) = x2 (more generally we could use for instance g(x) = xk).
206"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5325301204819277,"Thus the overall input-output function is given by:
207"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5349397590361445,"y = (wL.....(w2w1x)2))2......)2 = w2
Lw4
L−1 . . . w2L
1 x2L"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5373493975903615,"or
208"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5397590361445783,"y = Px2L
with
P =
Y
w2L−2i+2
i
Thus in this case the multiplier P realized by the architecture is positive. Again the number of
209"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5421686746987951,"parameters is L and it can be arbitrarily large. As in the linear case, a single training example of
210"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5445783132530121,"the form (x, t) is sufﬁcient to determine the multiplier P, with a manifold of equivalent solutions
211"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5469879518072289,"corresponding to parameters satisfying P = Q w2L−2i+2
i
= α/β, with this time α = E(tx2) and
212"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5493975903614458,"β = E(x4), when α > 0. If α < 0, the optimum is obtained for P = 0 which can be achieved by
213"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5518072289156627,"having at least one of the weights of the architectures equal to zero. In short, in both examples treated
214"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5542168674698795,"in this subsection, the dogma is again violated.
215"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5566265060240964,"4.2
Deep Non-Linear Models with No-Bottlenecks (Unrestricted Boolean)
216"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5590361445783133,"Consider an architecture A(n0, . . . , nL) where each neuron can implements any Boolean function
217"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5614457831325301,"of the neurons in the previous layer. The error function is the Hamming distance between target
218"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.563855421686747,"and output vectors. For simplicity, let us ﬁrst assume that all the layers have the same size n. The
219"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5662650602409639,"overall input-output function is a Boolean map from Hn to Hn, where Hn denotes the n-dimensional
220"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5686746987951807,"hypercube. This architecture has Ln2n parameters, since each unrestricted Boolean neuron with
221"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5710843373493976,"n inputs has 2n free parameters. The overall input-output map can be speciﬁed using only n2n
222"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5734939759036145,"examples. It can easily be implemented with 0 error through a large class of equivalent networks. As
223"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5759036144578313,"the number of layers L goes to inﬁnity the number of parameters goes to inﬁnity, while the number
224"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5783132530120482,"of required training examples remains ﬁxed and is determined entirely by the size of the input and
225"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5807228915662651,"output layers. This can easily be generalized to a Boolean unrestricted architecture of the form
226"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5831325301204819,"A(n0, . . . , nL), as long as there are no bottleneck layers. In such an architecture, the total number of
227"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5855421686746988,"parameters is given by: PL
i=1 ni2ni−1. The number of necessary and sufﬁcient training examples
228"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5879518072289157,"needed to specify the overall input-output function is given by: nL2n0, and thus again the dogma is
229"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5903614457831325,"violated. The case with bottle-neck layers is treated below.
230"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5927710843373494,"4.3
Deep Non-Linear Models with Bottlenecks (Unrestricted Boolean)
231"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5951807228915663,"For simplicity, consider ﬁrst an unrestricted Boolean compressive autoencoder with architecture
232"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.5975903614457831,"A(n, m, n) and m < n. The error function is the Hamming distance between the input vector and
233"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6,"the output vector. The hidden layer can have 2m states. Thus if the number of training examples is
234"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6024096385542169,"at most 2m, it can be realized by the architecture with 0 Hamming distortion, since every input can
235"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6048192771084338,"be mapped to a unique hidden representation and the corresponding representation can be mapped
236"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6072289156626506,"back to the same input using unrestricted Boolean gates. Obviously if additional layers of size at
237"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6096385542168675,"least m are added between the input layer and the hidden layer, or between the hidden layer and the
238"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6120481927710844,"output layer, the number of parameters can be arbitrarily increased, while maintaining the same ﬁxed
239"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6144578313253012,"training set and the ability to implement it exactly with no Hamming distortion. Thus in this regime
240"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6168674698795181,"the dogma is again violated.
241"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.619277108433735,"In the more interesting regime where the number of training examples exceeds 2m, then there must
242"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6216867469879518,"be clusters of training examples that are mapped to the same hidden representation. It is easy to see
243"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6240963855421687,"that for optimality purposes the corresponding representation must be mapped to the binary vector
244"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6265060240963856,"closest to the center of gravity of the cluster, essentially the majority vector, in order to minimize
245"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6289156626506024,"the Hamming distortion. Thus, in short, in this regime the optimal solution corresponds to a form of
246"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6313253012048192,"optimal clustering with respect to the Hamming distance with, in general, 2m clusters. As a back of
247"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6337349397590362,"the enveloppe calculation, assuming the clusters are spherical, these can be described by providing
248"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.636144578313253,"two points corresponding to a diameter. Thus in principle a training set of size 2 × 2m = 2m+1 could
249"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6385542168674698,"sufﬁce. The number of parameters of the architecture is given by: m2n + n2m which far exceeds
250"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6409638554216868,"the number of training examples. And even without the assumption of spherical clusters, it is clear
251"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6433734939759036,"that the number of parameters far exceeds the number of training examples, and that the gap can be
252"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6457831325301204,"made as large as possible, just by adding additional layers of size at least m between the input and
253"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6481927710843374,"the hidden layer, or the hidden layer and the output layer. Thus again the dogma is grossly violated.
254"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6506024096385542,"Finally, we turn to deep non-linear architecture where the neurons are linear or polynomial threshold
255"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.653012048192771,"gates. Linear threshold neurons, or perceptrons, are very similar to sigmoidal (e.g. logistic) neurons.
256"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.655421686746988,"5
The Non-Linear Regime: Linear or Polynomial Threshold Gates
257"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6578313253012048,"Here each neuron in the architecture is a linear or polynomial threshold function of degree d. In
258"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6602409638554216,"the linear threshold case (d = 1), any neuron with n inputs x = (x1, . . . , xn) produces an output
259"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6626506024096386,"equal to sign(P
i wixi)) in the -/+ case; or H((P
i wixi)) in the 0/1 case, where H denotes the
260"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6650602409638554,"Heaviside function. Such a neuron has n synaptic parameters. In the polynomial case of degree d,
261"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6674698795180722,"the output of a neuron has the form sign(p(x)) in the -/+ case; or H(p(x)) in the 0/1 case, where
262"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6698795180722892,"p(x) = p(x1, . . . , xn) is a polynomial of degree d. The number of parameters of a polynomial
263"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.672289156626506,"threshold neuron increases accordingly. As usual a bias can also be added or, equivalently, one of the
264"
THE SIMPLEST DEEP NON-LINEAR MODEL,0.6746987951807228,"input variables is considered to be constant and equal to 1.
265"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6771084337349398,"5.1
The Simplest Deep Non-Linear Model with Linear or Polynomial Threshold Gates
266"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6795180722891566,"We can use the same architecture A(1, . . . , 1) as in the ﬁrst example above. Linear or polynomial
267"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6819277108433734,"threshold neurons can realize the identity and the negation, depending on whether the corresponding
268"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6843373493975904,"incoming weight is positive or negative. So the result here is similar to the Boolean unrestricted
269"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6867469879518072,"case. For instance for linear threshold gates, without the bias, the number of parameters is equal to L.
270"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.689156626506024,"The number of negative weights determines how many negations are present in the chain. A single
271"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.691566265060241,"input-ouput example determines whether the overall chain should be the identity or the negation.
272"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6939759036144578,"Thus again the dogma is violated.
273"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6963855421686747,"5.2
Deep Non-Linear Models with Bottlenecks (Linear or Polynomial Threshold Gates)
274"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.6987951807228916,"We can again start with a compressive autoencoder architecture with shape A(n, m, n) and m < n
275"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7012048192771084,"and linear threshold neurons with the Hamming error function. In the most interesting case where the
276"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7036144578313253,"number of examples exceeds 2m, then the optimal solution corresponds to the optimal approximation
277"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7060240963855422,"to the optimal Hamming clustering that can be achieved using linear threshold gates. The number of
278"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.708433734939759,"parameters of this architecture is 2nm which is not necessarily less than the number 2m+1 of required
279"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7108433734939759,"training examples, under the spherical cluster assumption. However, as in the similar previous
280"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7132530120481928,"examples, the number of parameters can be increased arbitrarily by adding additional layers of size
281"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7156626506024096,"at least m between the input and the hidden layer, or between the hidden layer and the output layer.
282"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7180722891566265,"Thus once again there are large equivalence classes in parameter space (e.g. applying permutations to
283"
THE SIMPLEST DEEP NON-LINEAR MODEL WITH LINEAR OR POLYNOMIAL THRESHOLD GATES,0.7204819277108434,"the neurons in a given layer) and the dogma is grossly violated.
284"
DISCUSSION,0.7228915662650602,"6
Discussion
285"
DISCUSSION,0.7253012048192771,"The conventional dogma that models ought to have less parameters than the number of training
286"
DISCUSSION,0.727710843373494,"examples is a mere product of shallow learning. It arises, and should be applied, only in shallow
287"
DISCUSSION,0.7301204819277108,"learning situations. As soon as one moves to deep learning situations, the dogma becomes non-sense
288"
DISCUSSION,0.7325301204819277,"and all the expectations it creates are simply wrong, even in the linear case. It is simply time to
289"
DISCUSSION,0.7349397590361446,"think about deep models in a different way, without the expectation that over-parameterization must
290"
DISCUSSION,0.7373493975903614,"necessarily lead to over-ﬁtting. This is not to say, of course, that over-parameterized deep learning
291"
DISCUSSION,0.7397590361445783,"models cannot overﬁt, but expecting them to do so just because they are over-parmaterized is unwise
292"
DISCUSSION,0.7421686746987952,"and unnecessary.
293"
DISCUSSION,0.744578313253012,"Over-parameterized models tend to partition the parameter space into large equivalence classes.
294"
DISCUSSION,0.7469879518072289,"All the parameter settings within one class are equivalent in terms of overall performance. Neural
295"
DISCUSSION,0.7493975903614458,"learning can be viewed as a communication process where information is communicated from the
296"
DISCUSSION,0.7518072289156627,"training data to the synaptic weights. The training data needs to contain enough information to select
297"
DISCUSSION,0.7542168674698795,"one of the equivalence classes, but not any particular setting of the weights within that class. Thus
298"
DISCUSSION,0.7566265060240964,"the information needed to specify one equivalence class is much less than the information required
299"
DISCUSSION,0.7590361445783133,"to specify a particular setting of the weights. And this explains why the number of data points can
300"
DISCUSSION,0.7614457831325301,"be much less than the number of parameters. Furthermore, the structure of the deep models and the
301"
DISCUSSION,0.763855421686747,"partitioning into equivalence classes is such that it is not even possible for the training data to be able
302"
DISCUSSION,0.7662650602409639,"to specify each individual weight of the architecture. This is because the system cannot distinguish
303"
DISCUSSION,0.7686746987951807,"between two different settings of the parameters within the same equivalence class. For instance,
304"
DISCUSSION,0.7710843373493976,"once the optimal class is achieved with a particular setting of the weights, the gradient of the error
305"
DISCUSSION,0.7734939759036145,"is zero and there is no way of exploring or distinguishing other optimal architectures in the same
306"
DISCUSSION,0.7759036144578313,"equivalence class.
307"
DISCUSSION,0.7783132530120482,"Shallow learning, in particular linear regression, already contains many of the central themes of
308"
DISCUSSION,0.7807228915662651,"machine learning: from the use of a parameterized family of models, to model ﬁtting by error
309"
DISCUSSION,0.7831325301204819,"minimization, to prediction and so forth. However, when transitioning to deep learning, linear
310"
DISCUSSION,0.7855421686746988,"regression is misleading in three major aspects. First, it has an analytic closed-form solution. Second,
311"
DISCUSSION,0.7879518072289157,"it is interpretable (or visualizable, at least in low dimensions). Third, it requires that the number of
312"
DISCUSSION,0.7903614457831325,"training examples be equal or even exceed the number of parameters in order to completely determine
313"
DISCUSSION,0.7927710843373494,"the solution. The ﬁrst two points are now well established and accepted. We use stochastic gradient
314"
DISCUSSION,0.7951807228915663,"descent for deep learning model ﬁtting and almost no one cares about not having a closed-form
315"
DISCUSSION,0.7975903614457831,"analytic solution. Likewise, no one expects to be able to easily visualize complex non-linear surfaces
316"
DISCUSSION,0.8,"in high-dimensional spaces, although many are still working on various other issues related to
317"
DISCUSSION,0.8024096385542169,"interpretability. However, we are still struggling with the third point. It is time to move on this front
318"
DISCUSSION,0.8048192771084337,"too.
319"
DISCUSSION,0.8072289156626506,"It should be clear from the examples presented that one of the emergent characteristics of over-
320"
DISCUSSION,0.8096385542168675,"parameterized regimes is the existence of large equivalence classes in parameter space, all associated
321"
DISCUSSION,0.8120481927710843,"with roughly the same level of overall performance. The training data needs only to provide enough
322"
DISCUSSION,0.8144578313253013,"information to select one of the equivalence classes (at the relevant quantization level), and not
323"
DISCUSSION,0.8168674698795181,"to specify the value of each one of the parameters. Reﬂecting back on the human brain, most
324"
DISCUSSION,0.8192771084337349,"mature human brains can pass the Turing test and achieve some form of general intelligence using
325"
DISCUSSION,0.8216867469879519,"architectures that are similar, at least at the macroscopic level, and at the level of the basic hardware
326"
DISCUSSION,0.8240963855421687,"components (e.g. pyramidal cells), but presumably with signiﬁcant differences at the level of
327"
DISCUSSION,0.8265060240963855,"individual synapses.
328"
DISCUSSION,0.8289156626506025,"Finally, there is the question of when deep architectures overﬁt the data. The results presented
329"
DISCUSSION,0.8313253012048193,"here provide a clear answer. Consider an architecture with w parameters. At the proper level of
330"
DISCUSSION,0.8337349397590361,"quantization of the weights and the error function, the architecture may partition the space of weights
331"
DISCUSSION,0.8361445783132531,"into e equivalence classes. Thus log2 e bits are needed to specify one of the equivalence classes. If the
332"
DISCUSSION,0.8385542168674699,"training data provides less than log2 e bits of information, then it does not contain enough information
333"
DISCUSSION,0.8409638554216867,"to select a relevant equivalence class and overﬁtting may occur. If the training data provides log2 e
334"
DISCUSSION,0.8433734939759037,"bits of information to select an equivalence class, then there is no overﬁtting and providing more data
335"
DISCUSSION,0.8457831325301205,"is not necessary. In the case of a classiﬁcation architecture with independent binary inputs of length
336"
DISCUSSION,0.8481927710843373,"n, k training examples contain on the order of kn bits of information. Thus the important question is
337"
DISCUSSION,0.8506024096385543,"not whether k ≈w (conventional wisdom) but whether kn ≈log2 e.
338"
REFERENCES,0.8530120481927711,"References
339"
REFERENCES,0.8554216867469879,"[1] Yaser S Abu-Mostafa. Hints. Neural computation, 7(4):639–671, 1995.
340"
REFERENCES,0.8578313253012049,"[2] Ahmad Alwosheel, Sander van Cranenburgh, and Caspar G Chorus. Is your dataset big enough?
341"
REFERENCES,0.8602409638554217,"sample size requirements when using artiﬁcial neural networks for discrete choice analysis.
342"
REFERENCES,0.8626506024096385,"Journal of choice modelling, 28:167–182, 2018.
343"
REFERENCES,0.8650602409638555,"[3] P. Baldi. Deep Learning in Science. Cambridge University Press, Cambridge, UK, 2021.
344"
REFERENCES,0.8674698795180723,"[4] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from
345"
REFERENCES,0.8698795180722891,"examples without local minima. Neural Networks, 2(1):53–58, 1989.
346"
REFERENCES,0.8722891566265061,"[5] P. Baldi and K. Hornik. Learning in linear networks: a survey. IEEE Transactions on Neural
347"
REFERENCES,0.8746987951807229,"Networks, 6(4):837–858, 1994. 1995.
348"
REFERENCES,0.8771084337349397,"[6] P. Baldi and Z. Lu. Complex-valued autoencoders. Neural Networks, 33:136–147, 2012.
349"
REFERENCES,0.8795180722891566,"[7] Pierre Baldi. Linear learning: Landscapes and algorithms. Advances in neural information
350"
REFERENCES,0.8819277108433735,"processing systems, 1, 1988.
351"
REFERENCES,0.8843373493975903,"[8] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model
352"
REFERENCES,0.8867469879518072,"alignment explain generalization in kernel regression and inﬁnitely wide neural networks.
353"
REFERENCES,0.8891566265060241,"Nature communications, 12(1):1–12, 2021.
354"
REFERENCES,0.891566265060241,"[9] Adrien Marie Legendre. Nouvelles méthodes pour la détermination des orbites des cometes. F.
355"
REFERENCES,0.8939759036144578,"Didot, 1805.
356"
REFERENCES,0.8963855421686747,"[10] D. J. C. MacKay. Bayesian interpolation. Neural Computation, 4:415–447, 1992.
357"
REFERENCES,0.8987951807228916,"[11] D. J. C. MacKay. A practical Bayesian framework for backprop networks. Neural Computation,
358"
REFERENCES,0.9012048192771084,"4:448–472, 1992.
359"
REFERENCES,0.9036144578313253,"[12] Terrence J Sejnowski. The unreasonable effectiveness of deep learning in artiﬁcial intelligence.
360"
REFERENCES,0.9060240963855422,"Proceedings of the National Academy of Sciences, 117(48):30033–30038, 2020.
361"
REFERENCES,0.908433734939759,"[13] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
362"
REFERENCES,0.9108433734939759,"deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
363"
REFERENCES,0.9132530120481928,"115, 2021.
364"
REFERENCES,0.9156626506024096,"Checklist
365"
REFERENCES,0.9180722891566265,"1. For all authors...
366"
REFERENCES,0.9204819277108434,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
367"
REFERENCES,0.9228915662650602,"contributions and scope? [Yes]
368"
REFERENCES,0.9253012048192771,"(b) Did you describe the limitations of your work? [Yes]
369"
REFERENCES,0.927710843373494,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
370"
REFERENCES,0.9301204819277108,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
371"
REFERENCES,0.9325301204819277,"them? [Yes]
372"
REFERENCES,0.9349397590361446,"2. If you are including theoretical results...
373"
REFERENCES,0.9373493975903614,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
374"
REFERENCES,0.9397590361445783,"(b) Did you include complete proofs of all theoretical results? [Yes]
375"
REFERENCES,0.9421686746987952,"3. If you ran experiments...
376"
REFERENCES,0.944578313253012,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
377"
REFERENCES,0.946987951807229,"mental results (either in the supplemental material or as a URL)? [N/A]
378"
REFERENCES,0.9493975903614458,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
379"
REFERENCES,0.9518072289156626,"were chosen)? [N/A]
380"
REFERENCES,0.9542168674698795,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
381"
REFERENCES,0.9566265060240964,"ments multiple times)? [N/A]
382"
REFERENCES,0.9590361445783132,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
383"
REFERENCES,0.9614457831325302,"of GPUs, internal cluster, or cloud provider)? [N/A]
384"
REFERENCES,0.963855421686747,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
385"
REFERENCES,0.9662650602409638,"(a) If your work uses existing assets, did you cite the creators? [N/A]
386"
REFERENCES,0.9686746987951808,"(b) Did you mention the license of the assets? [N/A]
387"
REFERENCES,0.9710843373493976,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
388 389"
REFERENCES,0.9734939759036144,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
390"
REFERENCES,0.9759036144578314,"using/curating? [N/A]
391"
REFERENCES,0.9783132530120482,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
392"
REFERENCES,0.980722891566265,"information or offensive content? [N/A]
393"
REFERENCES,0.983132530120482,"5. If you used crowdsourcing or conducted research with human subjects...
394"
REFERENCES,0.9855421686746988,"(a) Did you include the full text of instructions given to participants and screenshots, if
395"
REFERENCES,0.9879518072289156,"applicable? [N/A]
396"
REFERENCES,0.9903614457831326,"(b) Did you describe any potential participant risks, with links to Institutional Review
397"
REFERENCES,0.9927710843373494,"Board (IRB) approvals, if applicable? [N/A]
398"
REFERENCES,0.9951807228915662,"(c) Did you include the estimated hourly wage paid to participants and the total amount
399"
REFERENCES,0.9975903614457832,"spent on participant compensation? [N/A]
400"
