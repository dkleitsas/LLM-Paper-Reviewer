Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0013717421124828531,"Noisy labels (NL) and adversarial examples both undermine trained models, but
interestingly they have hitherto been studied independently. A recent adversarial
training (AT) study showed that the number of projected gradient descent (PGD)
steps to successfully attack a point (i.e., ﬁnd an adversarial example in its proximity)
is an effective measure of the robustness of this point. Given that natural data are
clean, this measure reveals an intrinsic geometric property—how far a point is
from its nearest class boundary. Based on this breakthrough, in this paper, we
ﬁgure out how AT would interact with NL. Firstly, we ﬁnd if a point is too close
to its noisy-class boundary (e.g., one step is enough to attack it), this point is
likely to be mislabeled, which suggests to adopt the number of PGD steps as a
new criterion for sample selection to correct NL. Secondly, we conﬁrm that AT
with strong smoothing effects suffers less from NL (without NL corrections) than
standard training, which suggests that AT itself is an NL correction. Hence, AT
with NL is helpful for improving even the natural accuracy, which again illustrates
the superiority of AT as a general-purpose robust learning criterion."
INTRODUCTION,0.0027434842249657062,"1
INTRODUCTION"
INTRODUCTION,0.00411522633744856,"In practice, the process of data labeling is usually noisy. Thus, it seems inevitable to learn with noisy
labels (Natarajan et al., 2013). To combat noisy labels, researchers have designed robust label-noise
learning methods, such as sample selection (Jiang et al., 2018) and loss/label correction (Patrini et al.,
2017; Nguyen et al., 2019). Meanwhile, safety-critical areas (e.g., medicine and ﬁnance) require deep
neural networks to be robust against adversarial examples (Szegedy et al., 2014; Nguyen et al., 2015).
To combat adversarial examples, adversarial training methods empirically generate adversarial data
on the ﬂy for updating the model (Madry et al., 2018; Zhang et al., 2019a)."
INTRODUCTION,0.0054869684499314125,"An interesting fact is that, the research community is exploring label-noise learning and adversarial
training independently. For example, Ding et al. (2020) and Zhang et al. (2021b) demonstrated
that the non-robust data that are close to the nearest class boundary are easy to be attacked: their
adversarial variants easily cross over the decision boundary. To ﬁne-tune the decision boundaries
for adversarial robustness, Ding et al. (2020) adaptively optimized small margins for non-robust
data, while Zhang et al. (2021b) gave more weights on them. However, both methods in adversarial
training explored the adversarial robustness with an implicit assumption that data have clean labels.
Obviously, it is not realistic in practice. To this end, we ﬁgure out the interaction of adversarial
training with noisy labels."
INTRODUCTION,0.006858710562414266,"We discover that when noisy labels occur in adversarial training (the right panel of Figure 1), incorrect
data (square points) are more likely to be non-robust (i.e., the predicted labels of their adversarial
variants disagree with the given labels). Speciﬁcally, Figure 1 compares the difference between
standard training (ST (Zhang et al., 2017)) and adversarial training (AT (Madry et al., 2018)) with
noisy labels. Commonly, a small number of incorrect data (square points) are surrounded by a large
number of correct data (round points). In ST, deep networks shape two small clusters (the left panel of
Figure 1) around the two incorrect data due to memorization effects (Zhang et al., 2017). In contrast,
AT has strong smoothing effects, i.e., smoothing out the small clusters around incorrect data and
letting incorrect data alone (the right panel of Figure 1)."
INTRODUCTION,0.00823045267489712,"To explain the above phenomenon in AT, we believe that the adversarial counterparts generated by
(majority) correct data can help to smooth the local neighborhoods of correct data, which encourages"
INTRODUCTION,0.009602194787379973,"Under review as a conference paper at ICLR 2022 ST
AT"
INTRODUCTION,0.010973936899862825,"Figure 1: The results of ST and AT on a binary dataset
with noisy labels. Dots denote correct data; squares denote
incorrect data. The color gradient represents the predic-
tion conﬁdence: the deeper color represents the higher
prediction conﬁdence. Left panel: A deep network shapes
two small clusters (red and blue ones in cross-over areas)
around two incorrect data due to memorization effects in
ST. Right panel: These clusters have been smoothed out
in AT. Boxes represent the unit-norm ball of AT."
INTRODUCTION,0.012345679012345678,"20
40
60
80
100
Epochs 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
INTRODUCTION,0.013717421124828532,Average H(Y|X)
INTRODUCTION,0.015089163237311385,Neighborhoods of incorrect data
INTRODUCTION,0.01646090534979424,Noise rate: 0.2 (ST)
INTRODUCTION,0.01783264746227709,Noise rate: 0.2 (AT)
INTRODUCTION,0.019204389574759947,Noise rate: 0.4 (ST)
INTRODUCTION,0.0205761316872428,Noise rate: 0.4 (AT)
INTRODUCTION,0.02194787379972565,"Figure 2: The average entropy of mod-
els trained by ST and AT. This value
is calculated on 100 points in each
neighborhood of incorrect data, us-
ing CIFAR-10 with symmetric-ﬂipping
noise. Both solid and dashed lines rep-
resent ST and AT, respectively. Note
that ST learns incorrect data more de-
terministically than AT."
INTRODUCTION,0.023319615912208505,"deep networks to be locally constant within the neighborhood (Papernot et al., 2016). Therefore, in
AT, it becomes difﬁcult for deep networks to form small but separated clusters around each incorrect
data. Consequently, these incorrect data are non-robust, which echos the parallel ﬁndings that robust
training avoids memorization of label noise (Sanyal et al., 2021)."
INTRODUCTION,0.024691358024691357,"Furthermore, we make quantitative comparisons between ST and AT in the presence of label noise.
Zhang et al. (2017) showed that ST indeed overﬁts noisy labels, which deﬁnitely degrade the
generalization performance of deep networks. From Figure 3, it can be seen that the training accuracy
of deep networks on incorrect data is obviously lower than that on correct data in AT. Nonetheless,
the performance gap totally disappeared in ST. Therefore, compared to ST, AT can always distinguish
correct data and incorrect data. Observing Figure 4, the test accuracy of deep networks ﬁrst increases
then decreases in ST. Nonetheless, such a trend has been largely alleviated or totally eliminated in AT.
Therefore, AT can mitigate negative effects of noisy labels, since the smoothing effects of AT can
prevent memorizing such incorrect data."
INTRODUCTION,0.02606310013717421,"Moreover, under noisy labels, we realize that AT provides a new measure—how difﬁcult it is to attack
data to generate adversarial variants whose predictive labels are different from the given labels—
which can distinguish correct/incorrect data (Figures 7(a) and 7(b)) and typical/rare data (Figure 8)
well. This new measure can be approximately realized by the number of projected gradient descent
(PGD) steps (Madry et al., 2018), i.e., how many PGD iterations we need to generate misclassiﬁed
adversarial variants. Compared with the commonly used measure, i.e., the loss value (Jiang et al.,
2018; Han et al., 2018), we ﬁnd that the number of PGD steps could be an alternative or even better
measure in AT (Figures 7(a) and 7(b)). In addition, we discover that this new measure can easily pick
up rare (atypical) data among typical data (Figure 8), where modern datasets often follow long-tailed
distributions (Feldman & Zhang, 2020)."
INTRODUCTION,0.027434842249657063,"Main contributions.
To sum up, our contributions can be summarized in three aspects as follows."
INTRODUCTION,0.02880658436213992,"1. We explore the in-depth interaction of AT with noisy labels. Namely, we take a closer look
at the smoothing effects of AT under label noise (Section 3). Subsequently, we conduct
quantitative comparisons: compared with ST, AT can always distinguish correct and incorrect
data and mitigate negative effects of label noise (Section 4)."
INTRODUCTION,0.03017832647462277,"2. We realize that AT naturally provides a new measure called the number of PGD steps,
i.e., how many PGD iterations are needed to generate misclassiﬁed adversarial examples.
Such a new measure can clearly differentiate the correct/incorrect data and typical/rare data
(Section 5)."
INTRODUCTION,0.03155006858710562,"3. We provide two simple examples of the applications of our new measure: a) we develop a
robust annotator, which can robustly annotate unlabeled (U) data considering that U data
could be adversarially perturbed (Section 6.1); b) our new measure could be an alternative
to the predictive probability for providing the conﬁdence of annotated labels (Section 6.2)."
INTRODUCTION,0.03292181069958848,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.03429355281207133,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.03566529492455418,"Adversarial training (AT).
As one of the primary defenses against adversarial examples (Good-
fellow et al., 2015; Carlini & Wagner, 2017; Athalye et al., 2018), AT has been widely studied to
improve the adversarial robustness of deep neural networks (DNNs) (Cai et al., 2018; Wang et al.,
2020b;a; Jiang et al., 2020b; Wu et al., 2020; Chen et al., 2020; Bai et al., 2021; Chen et al., 2021;
Tian et al., 2021). The key objective of AT is to minimize the training loss on the adversarial variants
of training data. We review the details of AT (Madry et al., 2018) used in this paper."
BACKGROUND AND RELATED WORK,0.037037037037037035,"Let (X, d∞) denote the input feature space X with the inﬁnity distance metric dinf(x, x′) = ∥x −
x′∥∞, and Bϵ[x] = {x′ ∈X | dinf(x, x′) ≤ϵ} be the closed ball of radius ϵ > 0 centered at x in X.
S = {(xi, yi)}n
i=1 is a dataset and (xi, yi) are i.i.d. from an underlying distribution, where xi ∈X,
yi ∈Y = {0, 1, . . . , C −1}, and C denotes the number of classes. The objective function of AT is"
BACKGROUND AND RELATED WORK,0.038408779149519894,"min
fθ∈F
1
n n
X"
BACKGROUND AND RELATED WORK,0.039780521262002745,"i=1
ℓ(fθ(˜xi), yi),
(1)"
BACKGROUND AND RELATED WORK,0.0411522633744856,"where ˜xi is an adversarial variant of input data xi within the ϵ-ball centered at x and fθ(·) : X →RC
is a score function. ℓ: RC × Y →R is a loss function which is a composition of a base loss
ℓB : ∆C−1 × Y →R (e.g., the cross-entropy loss) and an inverse link function ℓL : RC →∆C−1
(e.g., the soft-max activation), in which ∆C−1 is the corresponding probability simplex—in other
words, ℓ(fθ(·), y) = ℓB(ℓL(fθ(·)), y)."
BACKGROUND AND RELATED WORK,0.04252400548696845,"To generate the adversarial variants ˜x for natural data x, AT employs the PGD method (Madry et al.,
2018). Given a starting point x(0) ∈X and step size α > 0, PGD works as follows:"
BACKGROUND AND RELATED WORK,0.0438957475994513,"x(t+1) = ΠB[x(0)]
 
x(t) + α sign(∇x(t)ℓ(fθ(x(t)), y))

,
(2)"
BACKGROUND AND RELATED WORK,0.04526748971193416,"until a certain stopping criterion is satisﬁed. In the above equation, t ∈N, ℓis the loss function, x(0)
refers to natural data or natural data perturbed by a small Gaussian or uniform random noise, y is the
corresponding label for natural data x, x(t) is an adversarial data point at step t, and ΠBϵ[x0](·) is the
projection function that projects the adversarial data back into the ϵ-ball centered at x(0) if necessary."
BACKGROUND AND RELATED WORK,0.04663923182441701,"It is common to use PGD to generate adversarial variants ˜x in AT methods (Wang et al., 2019; Zhang
et al., 2020). Recently, Zhang et al. (2021b) explored adversarial robustness by giving more weights
on the non-robust data with the assumption that all labels are correct. Speciﬁcally, the non-robust
data are geometrically close to the class boundaries, which can easily go across the class boundaries
by a small perturbation. To approximate the distance between the data and the class boundaries, they
proposed the geometry-aware projected gradient descent (GA-PGD) to calculate the geometry value
κ, which is the least number of iterations that PGD needs to ﬁnd misclassiﬁed adversarial variants
of input data. In this paper, we utilize the geometry value κ to represent our proposed measure (i.e.,
the number of PGD steps); we further explore its applications such as selecting correct/incorrect and
typical/rare data (Section 5), assisting to develop a robust annotator (Section 6.1) and providing the
annotation conﬁdence (Section 6.2)."
BACKGROUND AND RELATED WORK,0.04801097393689986,"Label-noise learning.
We consider a training set with X = (x1, . . . , xN) and its associated
labels Y = (y1, . . . , yN), where yi ∈Y is the one-hot label for the instance xi and (xi, yi) are
drawn i.i.d. from some unknown distribution. In the setting of label noise, we observe noisy labels
eY = (˜y1, . . . , ˜yN) where ˜yi ∈eY might be different from the corresponding ground-truth label
yi ∈Y. In this paper, we mainly focus on typical class-conditional noise: 1) symmetric-ﬂipping
noise (Van Rooyen et al., 2015), where noisy labels are corrupted at random with the uniform
distribution; 2) pair-ﬂipping noise (Han et al., 2018), where noisy labels are corrupted between
adjacent classes that are prone to be mislabeled. Note that pair-ﬂipping noise is an extremely hard
case of asymmetric-ﬂipping noise (Patrini et al., 2017)."
BACKGROUND AND RELATED WORK,0.04938271604938271,"To combat noisy labels, researchers have designed robust label-noise learning methods, such as
sample selection (Malach & Shalev-Shwartz, 2017; Jiang et al., 2020a; Han et al., 2020a), loss
correction (Han et al., 2020b; Liu & Guo, 2020), and label correction (Wang et al., 2018). Among
them, sample selection is emerging due to its simplicity. The key idea of sample selection is to
back-propagate clean samples (regarded as correct data) during training. Since DNNs learn simple"
BACKGROUND AND RELATED WORK,0.05075445816186557,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.05212620027434842,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
BACKGROUND AND RELATED WORK,0.053497942386831275,Standard accuracy (%)
BACKGROUND AND RELATED WORK,0.05486968449931413,Natural training data in ST
BACKGROUND AND RELATED WORK,0.056241426611796985,Noise rate: 0.2 (Correct data)
BACKGROUND AND RELATED WORK,0.05761316872427984,Noise rate: 0.4 (Correct data)
BACKGROUND AND RELATED WORK,0.05898491083676269,Noise rate: 0.2 (Incorrect data)
BACKGROUND AND RELATED WORK,0.06035665294924554,Noise rate: 0.4 (Incorrect data)
BACKGROUND AND RELATED WORK,0.06172839506172839,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
BACKGROUND AND RELATED WORK,0.06310013717421124,Standard accuracy (%)
BACKGROUND AND RELATED WORK,0.0644718792866941,Natural training data in AT
BACKGROUND AND RELATED WORK,0.06584362139917696,(a) CIFAR-10
BACKGROUND AND RELATED WORK,0.06721536351165981,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
BACKGROUND AND RELATED WORK,0.06858710562414266,Standard accuracy (%)
BACKGROUND AND RELATED WORK,0.06995884773662552,Natural training data in ST
BACKGROUND AND RELATED WORK,0.07133058984910837,"60
80
100 97 98 99 100"
BACKGROUND AND RELATED WORK,0.07270233196159122,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
BACKGROUND AND RELATED WORK,0.07407407407407407,Standard accuracy (%)
BACKGROUND AND RELATED WORK,0.07544581618655692,Natural training data in AT
BACKGROUND AND RELATED WORK,0.07681755829903979,"20
40
60
80 100 99.25 99.50 99.75"
BACKGROUND AND RELATED WORK,0.07818930041152264,(b) MNIST
BACKGROUND AND RELATED WORK,0.07956104252400549,"Figure 3: The standard accuracy of ST and AT on correct/incorrect training data using CIFAR-10
and MNIST with symmetric-ﬂipping noise. Solid lines denote the accuracy of correct training data,
while dashed lines correspond to that of incorrect training data. Compared with ST, there is a large
performance gap in the standard accuracy of correct/incorrect training data in AT."
BACKGROUND AND RELATED WORK,0.08093278463648834,"patterns ﬁrst (Zhang et al., 2017; Arpit et al., 2017), the loss value is used as a general criterion for
selecting clean samples (Han et al., 2018; Yao et al., 2020). Speciﬁcally, the data with small-loss
values are considered as clean samples, which are used to update the model. In contrast, the data
with large-loss values are considered as noisy samples, which should be discarded or utilized in
another way (Han et al., 2020a). Note that this paper proposes a new criterion—the number of PGD
steps—for sample selection (Section 5)."
BACKGROUND AND RELATED WORK,0.0823045267489712,"Co-existence of adversarial examples and noisy labels (NL).
Some work has previously studied
adversarial examples and noisy labels (NL) jointly. Alayrac et al. (2019) empirically showed that
NL negatively affects AT’s performance, but AT’s robust accuracy apparently suffers less than AT’s
natural accuracy. Sanyal et al. (2021) empirically found that AT can avoid the memorization of
NL. Damodaran et al. (2019) proposed to use Wasserstein adversarial regularization to combat NL
for beneﬁting ST’s generalization. In contrast, we advocate the AT’s smoothing effect by making
quantitative comparisons between AT and ST under NL. Speciﬁcally, ST has a memorization effect
that gradually memorizes NL and degrades generalization in the end, while AT has a smoothing effect
that avoids the memorization of NL and combats NL for beneﬁting generalization. In addition, we
propose a new measure called the number of PGD steps for sample selection, which can differentiate
the correct/incorrect data and typical/rare data, and we provide two exemplar applications for our
new measure. Furthermore, in Appendix A, we provide extensive comparisons between our study
and the existing literature."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.08367626886145405,"3
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.0850480109739369,"In this section, we take a closer look at the smoothing effects of AT with NL. At a high level, we
conduct experiments on a synthetic dataset with incorrect labels, which explicitly show the smoothing
effects of AT (Figure 1). We then use a real-world dataset, CIFAR-10 (Krizhevsky, 2009), with
incorrect labels, which further validates the smoothing effects of AT (Figure 2). As a key result, we
ﬁnd that AT can smooth out the small clusters around incorrect data (the right panel of Figure 1),
which leads to incorrect data being non-robust in AT, i.e., they can be easily attacked to ﬂip labels.
The setup and more results can be found in Appendix B."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.08641975308641975,"In detail, we empirically conﬁrmed that DNNs can memorize random noise in ST (the left panel of
Figure 1), which has been found in previous works (Zhang et al., 2017; Arpit et al., 2017). However,
a recent study (Sanyal et al., 2021) claimed that AT can avoid the memorization of incorrect data
through analyzing model predictions. Going beyond their analysis, we further investigated AT
with noisy labels and provided an in-depth explanation, namely smoothing effects. Speciﬁcally, AT
prevents incorrect data from forming small clusters, which should be the primary reason for avoiding
the memorization of incorrect data."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.0877914951989026,"To justify our smoothing effect, we performed a series of comparison experiments using ST and AT
on a synthetic dataset with incorrect labels. In Figure 1, the model trained by ST can overﬁt the
incorrect data (yellow and black squares), and thus have incorrect predictions (red and blue clusters
in cross-over areas) around incorrect data. While in AT (with smoothing effects), such clusters have
obviously disappeared. The reason is due to the smoothing effects from the adversarial variants"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.08916323731138547,Under review as a conference paper at ICLR 2022
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09053497942386832,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09190672153635117,Standard accuracy (%)
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09327846364883402,Natural test data in ST
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09465020576131687,"Noise rate: 0.0
Noise rate: 0.2
Noise rate: 0.4"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09602194787379972,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09739368998628258,Standard accuracy (%)
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.09876543209876543,Natural test data in AT
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.10013717421124829,(a) CIFAR-10
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.10150891632373114,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.102880658436214,Standard accuracy (%)
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.10425240054869685,Natural test data in ST
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.1056241426611797,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.10699588477366255,Standard accuracy (%)
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.1083676268861454,Natural test data in AT
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.10973936899862825,"20
40
60
80 100
98.0 98.5 99.0 99.5"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.1111111111111111,(b) MNIST
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11248285322359397,"Figure 4: The standard accuracy of ST and AT on natural test data using CIFAR-10 and MNIST with
symmetric-ﬂipping noise for training. Note that the larger noise rate causes the test accuracy of
ST dropping more seriously due to memorization effects in deep learning, while AT alleviates such
negative effects."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11385459533607682,"generated from correct data. Namely, the number of correct data is larger than that of incorrect data.
Thus, it is difﬁcult for incorrect data to smooth their neighborhood."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11522633744855967,"Further, we calculated the entropy values of the model predictions on the CIFAR-10 dataset, which
aims to validate the smoothing effect in practice. Speciﬁcally, we randomly selected 100 points in
each neighborhood (within a small ϵ-ball) of the incorrect data and calculated their average entropy
values in training (Figure 2). As a measure of uncertainty (Dai & Chen, 2012), the entropy value was
calculated by the following formula:"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11659807956104253,"H(Y|X) = −
X x∈X X"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11796982167352538,"y∈Y
p(x, y) · log p(y|x).
(3)"
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.11934156378600823,"The smaller value represents the higher certainty of model prediction (and vice versa), which indicates
that the model learns the data more deterministically. Thus, the higher certainty leads to the higher
possibility of incorrect data forming small clusters in their neighborhoods."
SMOOTHING EFFECTS OF ADVERSARIAL TRAINING,0.12071330589849108,"We compared the entropy values of ST and AT. During the training process, under the same noise
rate, the entropy value of AT is always higher than that of ST. After epoch 60, the entropy value of ST
drops very fast, while that of AT remains high. It clearly shows that smoothing effects in AT prevent
the model from learning incorrect data with their neighborhoods deterministically, which further
conﬁrms that it is harder for incorrect data to form small clusters. By observing Figures 1 and 2,
we conﬁrmed that it is difﬁcult for incorrect data in AT to form small clusters due to the smoothing
effects from the adversarial variants of correct data."
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING,0.12208504801097393,"4
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING"
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING,0.12345679012345678,"In this section, we explore knock-on effects of AT comprehensively. We show the quantitative
differences between ST and AT with noisy labels. First, in terms of training accuracy, we show
that correct/incorrect data can be always distinguishable in AT (Figure 3). Second, in terms of test
accuracy, we demonstrate that AT alleviates negative effects of incorrect data and then improves the
model generalization (Figure 4). Note that we display the experimental results on the CIFAR-10 and
MNIST datasets (LeCun et al., 1998) with symmetric-ﬂipping noise in this section. More results (e.g.,
pair-ﬂipping noise, the CIFAR-100 dataset, the loss values, and different networks) can be found in
Appendix C."
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING,0.12482853223593965,"4.1
DISTINGUISHABLE CORRECT/INCORRECT DATA"
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING,0.1262002743484225,"In Figure 3, we plotted the standard accuracy of natural training data in ST and AT. In the early stage
of training, there is a clear performance gap between the standard accuracy of ST on correct/incorrect
training data. However, after 60 epochs, the standard accuracy of ST on incorrect training data
increases rapidly, while that of AT on incorrect training data rises relatively slowly. When the training
comes to epoch 100, the standard accuracy of ST on correct/incorrect training data are merged
together. Nonetheless, there is still a large performance gap in AT. Compared to the results on
CIFAR-10, such a gap is more obvious on MNIST."
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING,0.12757201646090535,Under review as a conference paper at ICLR 2022
ALLEVIATION OF MEMORIZATION EFFECTS,0.1289437585733882,"4.2
ALLEVIATION OF MEMORIZATION EFFECTS"
ALLEVIATION OF MEMORIZATION EFFECTS,0.13031550068587106,"In Figure 4, we plotted the standard accuracy of natural test data in ST and AT. It shows that AT can
alleviate the negative effects of label noise and then improve the model generalization. Speciﬁcally,
the larger noise rate causes the test accuracy of ST to drop more seriously, i.e., memorization
effects (Arpit et al., 2017). However, AT reduces such negative effects. By checking the standard
accuracy of natural test data, we ﬁnd that there is no obvious overﬁtting phenomenon in AT. We also
visualized the loss landscape (Li et al., 2018) of models trained by ST and AT to further substantiate
the alleviation, which can be found in Appendix C."
ALLEVIATION OF MEMORIZATION EFFECTS,0.13168724279835392,"It is worthwhile to observe the results on MNIST: simply using AT can make the model obtain a
performance similar to noise-free training. However, on more complex CIFAR-10, incorrect data
still have a certain negative impact on the model trained by AT. To reduce such an impact, a simple
yet effective method is to use sample selection to ﬁlter correct/incorrect data for training (Jiang
et al., 2018; Cheng et al., 2021). Therefore, it is critical to have a measure which can provide the
stratiﬁcation for correct/incorrect data. Normally, the loss value can be a good candidate in ST.
However, in AT, we can ﬁnd a better measure such as the number of PGD steps (i.e., geometry value
κ). Since the smoothing effects in AT can make incorrect data be non-robust, the geometry value
κ—how difﬁcult it is to attack data to let them go across the decision boundary—could be naturally
used as a measure for this task."
ALLEVIATION OF MEMORIZATION EFFECTS,0.13305898491083676,"20
40
60
80
100
Epochs 0 2 4 6 8 10 12 14"
ALLEVIATION OF MEMORIZATION EFFECTS,0.13443072702331962,Loss value ℓ
ALLEVIATION OF MEMORIZATION EFFECTS,0.13580246913580246,Noise rate: 0.2
ALLEVIATION OF MEMORIZATION EFFECTS,0.13717421124828533,"Correct data
Incorrect data"
ALLEVIATION OF MEMORIZATION EFFECTS,0.13854595336076816,"20
40
60
80
100
Epochs 0 2 4 6 8 10 12 14"
ALLEVIATION OF MEMORIZATION EFFECTS,0.13991769547325103,Loss value ℓ
ALLEVIATION OF MEMORIZATION EFFECTS,0.1412894375857339,Noise rate: 0.4
ALLEVIATION OF MEMORIZATION EFFECTS,0.14266117969821673,"Correct data
Incorrect data"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1440329218106996,"20
40
60
80
100
Epochs 0 2 4 6 8 10 12 14"
ALLEVIATION OF MEMORIZATION EFFECTS,0.14540466392318244,Geometry value κ
ALLEVIATION OF MEMORIZATION EFFECTS,0.1467764060356653,Noise rate: 0.2
ALLEVIATION OF MEMORIZATION EFFECTS,0.14814814814814814,"20
40
60
80
100
Epochs 0 2 4 6 8 10 12 14"
ALLEVIATION OF MEMORIZATION EFFECTS,0.149519890260631,Geometry value κ
ALLEVIATION OF MEMORIZATION EFFECTS,0.15089163237311384,Noise rate: 0.4
ALLEVIATION OF MEMORIZATION EFFECTS,0.1522633744855967,"Figure 5: Comparisons of correct/incorrect data in
terms of the loss value (top panel) and the geometry
value κ (bottom panel) on CIFAR-10 with symmetric-
ﬂipping noise in AT. We calculate the mean values in
each epoch. We clearly demonstrate that the value κ
has a similar trend as loss value in AT; both can be
used for differentiating correct/incorrect data in AT."
ALLEVIATION OF MEMORIZATION EFFECTS,0.15363511659807957,"0
2
4
Loss value ℓ 0 5 10"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1550068587105624,Geometry value κ
ALLEVIATION OF MEMORIZATION EFFECTS,0.15637860082304528,"Correct data
Incorrect data"
ALLEVIATION OF MEMORIZATION EFFECTS,0.15775034293552812,"Figure 6: We choose the model trained by
AT using CIFAR-10 with 20% symmetric-
ﬂipping noise. We jointly analyze the ge-
ometry value κ and the loss value, which
shows that the value κ can provide a ﬁne
stratiﬁcation on typical (i.e., larger κ)/rare
(i.e., smaller κ) data."
ALLEVIATION OF MEMORIZATION EFFECTS,0.15912208504801098,"5
NEW MEASURE: GEOMETRY VALUE κ"
ALLEVIATION OF MEMORIZATION EFFECTS,0.16049382716049382,"In this section, we show the geometry value κ could be a new measure for the data stratiﬁcation.
First, the geometry value κ can differentiate correct/incorrect data in AT (Figures 5, 7(a) and 7(b)).
Compared with the loss value, which has been widely used in sample selection (Jiang et al., 2018;
Han et al., 2018; Yu et al., 2019), we show that the geometry value κ can have a better performance to
ﬁlter incorrect data with different noise types. Second, we demonstrate that the geometry value κ can
provide a ﬁner stratiﬁcation on typical/rare data (Figures 6 and 8). We also discussed it with different
conﬁgurations (e.g., PGD step number or the ϵ-ball) and more results can be found in Appendix D."
ALLEVIATION OF MEMORIZATION EFFECTS,0.16186556927297668,"5.1
GEOMETRY VALUE VS. LOSS VALUE"
ALLEVIATION OF MEMORIZATION EFFECTS,0.16323731138545952,"To combat NL, sample selection methods are very effective. As a common measure in sample
selection, the loss value is used to ﬁlter incorrect data. For example, small-loss data can be regarded"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1646090534979424,Under review as a conference paper at ICLR 2022
ALLEVIATION OF MEMORIZATION EFFECTS,0.16598079561042525,"(a) 20% Symmetric-ﬂipping noise
(b) 40% Pair-ﬂipping noise"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1673525377229081,"Figure 7: The density of AT on correct/incorrect data using CIFAR-10 with (a) 20% symmetric-
ﬂipping noise and (b) 40% pair-ﬂipping noise. Top panels: the loss value in AT. Bottom panels: the
geometry value κ in AT. The geometry value κ has a better distinction on correct/incorrect data."
ALLEVIATION OF MEMORIZATION EFFECTS,0.16872427983539096,"as “correct” data. However, there are two limitations in using the loss value as a measure. First, we
need to adjust different thresholds to obtain a better selection effect, when the dataset has different
noise rates and types (Yao et al., 2020). Second, for pair-ﬂipping noise, the loss value cannot
distinguish correct/incorrect data well (the top panel of Figure 7(b))."
ALLEVIATION OF MEMORIZATION EFFECTS,0.1700960219478738,"In Figure 5, we compared the geometry value κ and loss value of correct/incorrect data in the training
process. We found that the value κ can be used to differentiate incorrect data from correct data, since
it has a similar trend to the loss value in AT. To further compare two measures in distinguishing
correct/incorrect data, we plotted the density maps of two measures on the CIFAR-10 dataset with
different noise types in Figures 7(a) and 7(b). To compare the two measures in a meaningful way,
we performed the min-max normalization (Tax & Duin, 2000) on both the loss value and geometric
value κ, which scales the range of values in [0, 1]."
ALLEVIATION OF MEMORIZATION EFFECTS,0.17146776406035666,"For symmetric-ﬂipping noise (Figure 7(a)), although the loss value can distinguish correct data from
incorrect data during the training process, the geometric value κ has a better distinction between
correct and incorrect data. Speciﬁcally, the top panels of Figure 7(a) show that there are a large
number of correct/incorrect data with the same loss value, which requires a carefully designed
threshold to select the correct data from incorrect data. In contrast, correct/incorrect data can be well
divided using the value κ in the bottom panels of Figure 7(a). We can easily select correct/incorrect
data with high purity. More obviously, for pair-ﬂipping noise, the loss value of correct/incorrect data
overlaps in the top panels of Figure 7(b). However, the value κ in the bottom panels of Figure 7(b)
still provides a good discrimination on correct/incorrect data."
ALLEVIATION OF MEMORIZATION EFFECTS,0.1728395061728395,"In addition, we found that the geometry value κ can provide a ﬁne stratiﬁcation on typical/rare data.
First, we jointly analyzed the value κ and the loss value in AT (Figure 6), where we stratiﬁed correct
data via κ. Secondly, by inspecting the semantic information with different κ, we found that the
value κ can represent whether the data is relatively typical or rare (Figures 8(a) and 8(b)). Moreover,
we plotted a bivariate graph of the loss value and the value κ in Figure 6. In this ﬁgure, we mainly
focused on the correctly classiﬁed data (blue scattered dots), since the wrongly classiﬁed data (orange
scattered dots) had been clearly discriminated by big loss values. Note that, for small-loss (correct)
data, the value κ can further subdivide such data into typical and rare types."
ALLEVIATION OF MEMORIZATION EFFECTS,0.17421124828532236,"5.2
DISTINGUISHABLE TYPICAL/RARE DATA"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1755829903978052,"From the macro perspective, the loss value can be regarded as a measure to classify correct and
incorrect data (Jiang et al., 2018). Namely, small-loss data can be regarded as correct data, and
vice versa. However, such stratiﬁcation is a bit rough, which motivates us to seek a micro measure
called the geometry value κ (the number of PGD steps) in AT. To justify our ﬁndings in Figure 6,
we visualized the semantic information of CIFAR-10 (Figure 8(a)) and MNIST (Figure 8(b)) under
different κ. We found that images with large κ (rightmost) are prototypical and easier to recognize
from the viewpoint of human perception, while images with small κ (or κ = 0) seem to be rarer (or
incorrect). These rare images have atypical semantic information, such as some strange shapes (“8”
with κ = 14 in Figure 8(b)) or confusing backgrounds (“deer” with κ = 2 in Figure 8(a)). More
results about the images with different κ can be found in Appendix D."
ALLEVIATION OF MEMORIZATION EFFECTS,0.17695473251028807,Under review as a conference paper at ICLR 2022
ALLEVIATION OF MEMORIZATION EFFECTS,0.17832647462277093,"κ: 0
κ: 2
κ: 3
κ: 10 plane"
ALLEVIATION OF MEMORIZATION EFFECTS,0.17969821673525377,"κ: 0
κ: 2
κ: 3
κ: 10 deer"
ALLEVIATION OF MEMORIZATION EFFECTS,0.18106995884773663,(a) CIFAR-10
ALLEVIATION OF MEMORIZATION EFFECTS,0.18244170096021947,"κ: 0
κ: 14
κ: 29
κ: 40 3"
ALLEVIATION OF MEMORIZATION EFFECTS,0.18381344307270234,"κ: 0
κ: 14
κ: 29
κ: 40 8"
ALLEVIATION OF MEMORIZATION EFFECTS,0.18518518518518517,(b) MNIST
ALLEVIATION OF MEMORIZATION EFFECTS,0.18655692729766804,"Figure 8: The geometry value κ w.r.t. images in CIFAR-10 and MNIST with 20% and 10% symmetric-
ﬂipping noise. The leftmost of each subﬁgure is the given label (i.e., deer and plane or 3 and 8) of
all images on the right. We randomly select four examples with the different κ in each class. As the
geometric value κ increases from left (κ = 0) to right (κ = 10 or 40), the semantic information of
images is more typical and recognizable."
ALLEVIATION OF MEMORIZATION EFFECTS,0.18792866941015088,"Algorithm 1: Robust Annotation Algorithm.
Input
:network fθ, training dataset S = {(xi, yi)}n
i=1, learning rate η, number of epochs T,
batch size m, number of batches M, threshold for geometry value K, threshold for loss
value L.
Output :robust annotator fθ.
for epoch = 1, . . . , T do"
ALLEVIATION OF MEMORIZATION EFFECTS,0.18930041152263374,"for mini-batch = 1, . . . , M do"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1906721536351166,"Sample: a mini-batch {(xi, yi)}m
i=1 from S.
for i = 1,...,m (in parallel) do"
ALLEVIATION OF MEMORIZATION EFFECTS,0.19204389574759945,"Calculate: κi and ℓi of (xi,yi).
if κi < K and ℓi > L then"
ALLEVIATION OF MEMORIZATION EFFECTS,0.1934156378600823,"Update: yi ←arg maxi fθ(x).
end
Generate: adversarial data ˜xi by PGD method.
end
Update: θ ←θ −η∇θ{ℓ(fθ(˜xi), yi)}.
end
end"
ALLEVIATION OF MEMORIZATION EFFECTS,0.19478737997256515,"6
APPLICATIONS OF GEOMETRY VALUE κ"
ALLEVIATION OF MEMORIZATION EFFECTS,0.19615912208504802,"In this section, we provide two applications of our new measure—the geometry value κ (the number
of PGD steps). Since the value κ can differentiate correct/incorrect data in AT (Section 5.1), in the
presence of label noise, we can use it to detect noisy labels and correct labels (Figure 9). Meanwhile,
as it can have a ﬁne stratiﬁcation for typical/rare data (Section 5.2), we can provide the conﬁdence of
annotated labels according to the value κ (Figure 10)."
ALLEVIATION OF MEMORIZATION EFFECTS,0.19753086419753085,"Regardless of ST or AT, high-quality training data are always essential for acquiring a good
model (Deng et al., 2009), but the labeling process of high-quality data requires a lot of human
resources. To deal with such a problem, many methods used ST to facilitate a standard annotator to
annotate large-scale unlabeled (U) data (Carmon et al., 2019; Alayrac et al., 2019). However, this
standard annotator fails when U data are adversarially manipulated."
ALLEVIATION OF MEMORIZATION EFFECTS,0.19890260631001372,"In practice, label-noise issues widely exist in real-world training datasets, and learning with NL seems
inevitable. Meanwhile, the existence of adversarial examples (Szegedy et al., 2014; Goodfellow et al.,
2015) also poses a threat to annotate U data. Therefore, we design a robust annotation algorithm
(Algorithm 1) to assign reliable labels for U data even in the presence of adversarial manipulations
and noisy training labels (Section 6.1). Compared to human beings, the standard annotator cannot
give the information whether the label assignment for U data is reliable. Nonetheless, our new
measure could be an alternative to the predictive probability for providing the conﬁdence of annotated
labels (Section 6.2). The detailed experimental setups can be found in Appendix E."
ALLEVIATION OF MEMORIZATION EFFECTS,0.20027434842249658,Under review as a conference paper at ICLR 2022
ALLEVIATION OF MEMORIZATION EFFECTS,0.20164609053497942,"0.0
0.2
0.4
0.6
0.8
1.0
Ratio of adversarial corrupted U data 0 20 40 60 80 100"
ALLEVIATION OF MEMORIZATION EFFECTS,0.2030178326474623,Accuracy (%)
ALLEVIATION OF MEMORIZATION EFFECTS,0.20438957475994513,The test U data with PGD-20 attack
ALLEVIATION OF MEMORIZATION EFFECTS,0.205761316872428,Standard annotator (Noise rate: 0.0)
ALLEVIATION OF MEMORIZATION EFFECTS,0.20713305898491083,PGD-based annotator (Noise rate: 0.0)
ALLEVIATION OF MEMORIZATION EFFECTS,0.2085048010973937,PGD-based annotator (Noise rate: 0.2)
ALLEVIATION OF MEMORIZATION EFFECTS,0.20987654320987653,Robust annotator (Noise rate: 0.2)
ALLEVIATION OF MEMORIZATION EFFECTS,0.2112482853223594,"0.6
0.7
0.8
0.9
1.0
Ratio of adversarial corrupted U data 51 54 57 60 63 66"
ALLEVIATION OF MEMORIZATION EFFECTS,0.21262002743484226,Accuracy (%)
ALLEVIATION OF MEMORIZATION EFFECTS,0.2139917695473251,Zoom-out results
ALLEVIATION OF MEMORIZATION EFFECTS,0.21536351165980797,"Figure 9: The accuracy of four approaches as-
signing correct labels to adversarial U data from
CIFAR-10. Left panel: the full results. Right
panel: the zoom-out results (without standard
annotator). Our robust annotator has a satisfac-
tory performance on assigning reliable labels."
ALLEVIATION OF MEMORIZATION EFFECTS,0.2167352537722908,"2
4
6
8
10
Geometry value κ 20 40 60 80 100"
ALLEVIATION OF MEMORIZATION EFFECTS,0.21810699588477367,Accuracy (%)
ALLEVIATION OF MEMORIZATION EFFECTS,0.2194787379972565,The test U data
ALLEVIATION OF MEMORIZATION EFFECTS,0.22085048010973937,"2
4
6
8
10
Geometry value κ 0 200 400 600 800 1000 1200 Num"
ALLEVIATION OF MEMORIZATION EFFECTS,0.2222222222222222,The test U data
ALLEVIATION OF MEMORIZATION EFFECTS,0.22359396433470508,"Correctly predicted
Wrongly predicted"
ALLEVIATION OF MEMORIZATION EFFECTS,0.22496570644718794,"Figure 10: The accuracy (left panel) and num-
ber (right panel) of correctly predicted U data
w.r.t. the geometry value κ. We randomly select
2000 test data in CIFAR-10 as unlabeled data.
The larger κ corresponds to the higher prediction
accuracy."
ROBUST ANNOTATOR,0.22633744855967078,"6.1
ROBUST ANNOTATOR"
ROBUST ANNOTATOR,0.22770919067215364,"We can construct a robust annotator to assign labels for U data. Here, we consider a real-world
scenario, namely, existence of label noise in training data and adversarial manipulations in U data.
Our robust annotator has a better labeling performance than the standard annotator, since we use the
value κ and the loss value jointly to select incorrect training data. We re-annotate high-quality pseudo
labels for these incorrect data, and adversarially train on the whole data. Then, our robust annotator
can reliably assign labels."
ROBUST ANNOTATOR,0.22908093278463648,"In Figure 9, we tested the accuracy of assigning correct labels to U data in the presence of adversarial
manipulations. We compared four methods, namely, our robust annotator with 20% symmetric-
ﬂipping noise (red line), the PGD-based annotator with 20% symmetric-ﬂipping noise (orange line),
the PGD-based annotator without noise (oracle, black dashed line), and the standard annotator without
noise (blue line). On normal U data (i.e., zero adversarial ratio), the standard annotator has better
performance of labeling. However, when U data is subject to certain adversarial manipulations (i.e.,
ratio above 0.2), the labeling quality of the standard annotator decreases sharply, but that of our robust
annotator still remains satisfactory. An extreme case is that, when all U data (ratio 1.0) are added to
adversarial manipulations, labels assigned by the standard annotator become completely unreliable,
but our labels assigned by the robust annotator are still better than the PGD-based annotator with
20% symmetric-ﬂipping noise."
CONFIDENCE SCORES,0.23045267489711935,"6.2
CONFIDENCE SCORES"
CONFIDENCE SCORES,0.23182441700960219,"For a given data point, the geometry value κ can provide a conﬁdence score, which represents the
reliability of label annotations. The measure value κ can distinguish between typical data (correctly
labeled with high probability) and rare data (wrongly labeled with high probability) in U data. In
the left panel of Figure 10, we plotted the accuracy of correctly predicted data with the value κ.
The larger κ corresponds to higher prediction accuracy, which shows that the value κ can indeed
represent the reliability of label annotations. In the right panel of Figure 10, we further investigated
the number of correctly predicted data with the value κ. Most of the data have the value κ = 10,
which corresponds to a high prediction accuracy. Meanwhile, a small part of the data have the value
κ ∈[0, 6], which corresponds to a low prediction accuracy. Since the number of data with value
κ ∈[7, 9] is small, the standard deviation of the accuracy is large."
CONCLUSION,0.23319615912208505,"7
CONCLUSION"
CONCLUSION,0.2345679012345679,"In this paper, we explored the interaction of adversarial training (AT) with noisy labels. We took a
closer look at smoothing effects of AT, and further investigated positive knock-on effects of AT. As a
result, AT can distinguish correct/incorrect data and alleviate memorization effects in deep networks.
Since smoothing effects can make incorrect data non-robust, the geometry value κ (i.e., the number
of projected gradient descent steps) could be a new measure to differentiate correct/incorrect and
typical/rare data. Moreover, we gave two applications of our new measure, i.e., the robust annotator
and conﬁdence scores. With the robust annotator, we can assign reliable labels for adversarial U data.
With conﬁdence scores, we can know the reliability of label annotations."
CONCLUSION,0.23593964334705075,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.23731138545953362,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.23868312757201646,"This paper does not raise any ethics concerns. This study does not involve any human subjects,
practices to data set releases, potentially harmful insights, methodologies and applications, potential
conﬂicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security
issues, legal compliance, and research integrity issues."
REPRODUCIBILITY STATEMENT,0.24005486968449932,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.24142661179698216,"To ensure the reproducibility of experimental results, we will provide a link for an anonymous
repository about the source codes of this paper in the discussion forums."
REFERENCES,0.24279835390946503,REFERENCES
REFERENCES,0.24417009602194786,"Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and
Pushmeet Kohli. Are labels required for improving adversarial robustness? In NeurIPS, 2019."
REFERENCES,0.24554183813443073,"Devansh Arpit, Stanisław Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep networks. In ICML, 2017."
REFERENCES,0.24691358024691357,"Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018."
REFERENCES,0.24828532235939643,"Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving
adversarial robustness via channel-wise activation suppressing. In ICLR, 2021."
REFERENCES,0.2496570644718793,"Qi-Zhi Cai, Chang Liu, and Dawn Song. Curriculum adversarial training. In IJCAI, 2018."
REFERENCES,0.25102880658436216,"Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy (SP), 2017."
REFERENCES,0.252400548696845,"Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. Unlabeled data
improves adversarial robustness. In NeurIPS, 2019."
REFERENCES,0.25377229080932784,"Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to ﬁne-tuning. In CVPR, 2020."
REFERENCES,0.2551440329218107,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overﬁtting
may be mitigated by properly learned smoothening. In ICLR, 2021."
REFERENCES,0.25651577503429357,"Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-
dependent label noise: A sample sieve approach. In ICLR, 2021."
REFERENCES,0.2578875171467764,"Wei Dai and Xiaowei Chen. Entropy of function of uncertain variables. Mathematical and Computer
Modelling, 2012."
REFERENCES,0.25925925925925924,"Bharath Bhushan Damodaran, Kilian Fatras, Sylvain Lobry, Rémi Flamary, Devis Tuia, and
Nicolas Courty. Wasserstein adversarial regularization (war) on label noise. arXiv preprint
arXiv:1904.03936, 2019."
REFERENCES,0.2606310013717421,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009."
REFERENCES,0.262002743484225,"Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct
input space margin maximization through adversarial training. In ICLR, 2020."
REFERENCES,0.26337448559670784,"Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long
tail via inﬂuence estimation. In NeurIPS, 2020."
REFERENCES,0.26474622770919065,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015."
REFERENCES,0.2661179698216735,Under review as a conference paper at ICLR 2022
REFERENCES,0.2674897119341564,"Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, 2018."
REFERENCES,0.26886145404663925,"Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor Tsang, and Masashi Sugiyama. Sigua:
Forgetting may make learning with noisy labels more robust. In ICML, 2020a."
REFERENCES,0.27023319615912206,"Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu. Training binary neural
networks through learning with noisy supervision. In ICML, 2020b."
REFERENCES,0.2716049382716049,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018."
REFERENCES,0.2729766803840878,"Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on
controlled noisy labels. In ICML, 2020a."
REFERENCES,0.27434842249657065,"Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial
contrastive learning. In NeurIPS, 2020b."
REFERENCES,0.2757201646090535,"Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada. Label-noise robust generative adversarial
networks. In CVPR, 2019."
REFERENCES,0.27709190672153633,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.2784636488340192,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.27983539094650206,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In NeurIPS, 2018."
REFERENCES,0.2812071330589849,"Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise
rates. In ICML, 2020."
REFERENCES,0.2825788751714678,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018."
REFERENCES,0.2839506172839506,"Eran Malach and Shai Shalev-Shwartz. Decoupling"" when to update"" from"" how to update"". In
NeurIPS, 2017."
REFERENCES,0.28532235939643347,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
ICML, 2010."
REFERENCES,0.28669410150891633,"Amir Najaﬁ, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial
perturbations in learning from incomplete data. In NeurIPS, 2019."
REFERENCES,0.2880658436213992,"Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NeurIPS, 2013."
REFERENCES,0.289437585733882,"Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In CVPR, 2015."
REFERENCES,0.2908093278463649,"Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen,
Laura Beggel, and Thomas Brox. Self: Learning to ﬁlter noisy labels with self-ensembling. In
ICLR, 2019."
REFERENCES,0.29218106995884774,"Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of
security and privacy in machine learning. arXiv:1611.03814, 2016."
REFERENCES,0.2935528120713306,"Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017."
REFERENCES,0.29492455418381347,"Amartya Sanyal, Puneet K Dokania, Varun Kanade, and Philip H. S. Torr. How benign is benign
overﬁtting? In ICLR, 2021."
REFERENCES,0.2962962962962963,Under review as a conference paper at ICLR 2022
REFERENCES,0.29766803840877915,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014."
REFERENCES,0.299039780521262,DM Tax and RP Duin. Feature scaling in support vector data descriptions. 2000.
REFERENCES,0.3004115226337449,"Qi Tian, Kun Kuang, Kelu Jiang, Fei Wu, and Yisen Wang. Analysis and applications of class-wise
robustness in adversarial training. In KDD, 2021."
REFERENCES,0.3017832647462277,"Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label
noise: The importance of being unhinged. In NeurIPS, 2015."
REFERENCES,0.30315500685871055,"Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, and Zhangyang Wang. Once-for-
all adversarial training: In-situ tradeoff between robustness and accuracy for free. 2020a."
REFERENCES,0.3045267489711934,"Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.
Iterative learning with open-set noisy labels. In CVPR, 2018."
REFERENCES,0.3058984910836763,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019."
REFERENCES,0.30727023319615915,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassiﬁed examples. In ICLR, 2020b."
REFERENCES,0.30864197530864196,"Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. NeurIPS, 33, 2020."
REFERENCES,0.3100137174211248,"Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit
memorization effect in learning with noisy labels. In ICML, 2020."
REFERENCES,0.3113854595336077,"Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? In ICML, 2019."
REFERENCES,0.31275720164609055,"Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv:1605.07146, 2016."
REFERENCES,0.31412894375857336,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017."
REFERENCES,0.31550068587105623,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019a."
REFERENCES,0.3168724279835391,"Jingfeng Zhang, Bo Han, Gang Niu, Tongliang Liu, and Masashi Sugiyama. Where is the bottleneck
of adversarial learning with unlabeled data? arXiv preprint arXiv:1911.08696, 2019b."
REFERENCES,0.31824417009602196,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020."
REFERENCES,0.3196159122085048,"Jingfeng Zhang, Xilie Xu, Bo Han, Tongliang Liu, Gang Niu, Lizhen Cui, and Masashi Sugiyama.
Noilin: Does noisy labels always hurt adversarial training? arXiv preprint, 2021a."
REFERENCES,0.32098765432098764,"Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In ICLR, 2021b."
REFERENCES,0.3223593964334705,Under review as a conference paper at ICLR 2022
REFERENCES,0.32373113854595337,"A
DETAILED DISCUSSIONS ON THE DIFFERENCES WITH THE EXISTING
STUDIES"
REFERENCES,0.32510288065843623,"This section discusses the difference between our work and other related studies that focused on
either improving adversarial training (AT) or learning with noisy labels (NL) in ST. Our work focuses
on ﬁguring out the in-depth interaction of (canonical) adversarial training with (generalized) noisy
labels."
REFERENCES,0.32647462277091904,"Improving adversarial training (AT).
Some studies focused on improving the AT’s performance
by leveraging additional unlabelled (U) data (Carmon et al., 2019; Najaﬁet al., 2019; Alayrac et al.,
2019; Zhang et al., 2019b). The main points are leveraging additional U data in AT that can achieve
both higher robust accuracy and higher standard accuracy. The ablation study of (Alayrac et al.,
2019) provided an analysis on the impact of symmetric-ﬂipping noise for model robustness, which
simulates the unreliable annotation of U data. By comparison, our study claims that AT itself an
NL correction. We conduct experiments on the generalized setting of NL, i.e., with different kinds
of label noise (e.g., symmetric-ﬂipping noise and pair-ﬂipping noise) and different noise rates (e.g.,
[0.1,0.4])."
REFERENCES,0.3278463648834019,"For beneﬁting adversarial robustness, Zhang et al. (2021a) proposed to inject NL over the training
process. They assume that the training set is noise-free, and they inject NL on the ﬂy as AT’s
regularization method. By comparison, our settings and motivations are different. We assume the
training set is label-noisy and ﬁnd that AT can naturally mitigate the negative effect of NL in the
training set. We focus on the understanding of AT’s smoothing effects on NL."
REFERENCES,0.3292181069958848,"Learning with noisy labels (NL) in ST.
Damodaran et al. (2019) focused on learning with NL in
ST and designed a Wasserstein Adversarial Regularization (WAR) as a correction method. They
added the WAR into standard training (ST) to combat with the NL. The authors also provided analysis
for their proposed correction method. Speciﬁcally, they explained WAR as a label interpolation
that uses the prediction of the adversarial data to interpolate the original label for the natural data.
By comparison, we identiﬁed and illustrated the inherent reason for AT’s NL correction, i.e., the
smoothing effect of the AT. Due to AT’s smoothing effect, the predictions of the adversarial data are
more trustworthy and can be used to interpolate the noisy labels."
REFERENCES,0.33058984910836764,"Kaneko et al. (2019) focused on robust label noise learning in the generation task. They proposed a
robust GAN to combat with NL. Speciﬁcally, they introduced a noise transition model as an auxiliary
classiﬁer for discriminator, which is similar to the forward correction method in label noise learning.
Their proposal conduct adversarial learning (AT) for generating images. By comparison, we leverage
AT for defending adversarial examples and meanwhile correcting NL."
REFERENCES,0.3319615912208505,"Sanyal et al. (2021) found NL in ST causes the signiﬁcant adversarial vulnerability. Besides, they
found NL widely exists even in some standard datasets (e.g., MNIST and CIFAR-10) and identiﬁed
NL as one of the causes for adversarial vulnerability in ST. In terms of AT, they empirically found
that AT can avoid the memorization of NL by conducting the experiments using symmetric-ﬂipping
noise on AT. By comparison, we ﬁgure out why AT can avoid the memorization of NL, i.e., AT’s
smoothing effect. Furthermore, we conduct extensive experiments across different noise types (e.g.,
asymmetric-ﬂipping noise) on AT and make comprehensive comparisons with ST."
REFERENCES,0.3333333333333333,"To sum up, our main point is understanding the interaction of AT with NL, which is different from the
previous studies. Speciﬁcally, we have shown the AT’s smoothing effects on NL and identiﬁed that
AT itself an NL correction. Therefore, compared with ST, AT can avoid the memorization of NL and
make correct (clean) data and incorrect (noise) data always distinguishable. Besides the discoveries,
we proposed a new measure—PGD steps that can stratify correct/incorrect and typical/rare data in
AT, which may provide a new perspective for sample selections."
REFERENCES,0.3347050754458162,"B
THE SMOOTHING EFFECTS OF ADVERSARIAL TRAINING"
REFERENCES,0.33607681755829905,"In this section, We provide the detailed setup and more results on the synthetic binary dataset and
the real-world dataset (CIFAR-10) with noisy labels, which demonstrate the smoothing effects of
adversarial training (AT)."
REFERENCES,0.3374485596707819,Under review as a conference paper at ICLR 2022
REFERENCES,0.3388203017832647,"ST
AT (PGD-1)
AT (PGD-2)
AT (PGD-3)
AT (PGD-4)"
REFERENCES,0.3401920438957476,"Figure 11: The results of standard training (ST) and adversarial training (AT) on a binary dataset
with noisy labels. Dots denote correct data, while squares denote incorrect data. The color gradient
represents the prediction conﬁdence: the deeper color represents higher prediction conﬁdence. In
the leftmost panel, deep networks shapes two small clusters (red and blue ones in cross-over areas)
around two incorrect data due to memorization effects in ST. As the number of PGD iterations
increases, the smoothing effects in AT gradually strengthens, and two small clusters gradually shrink
until they disappear in the rightmost panel. Namely, these clusters have been smoothed out in AT
(PGD-4). Boxes represent the norm ball of AT."
REFERENCES,0.34156378600823045,"Experimental setup.
To construct synthetic binary dataset, we randomly generate 23 points (i.e.,
(a, b), where a ∈(0, 1) and b ∈(0, 1)) with binary labels (i.e., “0” and “1”) on a two-dimensional
plane. Among all data , we choose two points to assign incorrect labels. For the binary classiﬁcation,
we build a simple network contains 5 linear layers and 4 ReLU (Nair & Hinton, 2010) layers. We
train the simple network in ST and AT using Adam with the initial learning rate=0.001 for 1000
iterations. In AT, we set the perturbation bound ϵ = 0.08 and the PGD step size α = 0.02."
REFERENCES,0.3429355281207133,"Result.
In Figure 11, We plot the classiﬁcation results in the two-dimensional plane for both ST and
AT. We use different PGD iterations to generate adversarial examples, which shows the smoothing
process dynamically. In ST, deep network will shape two small clusters around two incorrect data due
to memorization effects. While in AT, these small clusters will gradually shrink until they disappear,
as the smoothing effects in AT strengthens (i.e., from PGD-1 to PGD-4)."
REFERENCES,0.3443072702331962,"20
40
60
80
100
Epochs 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.345679012345679,Average H(Y|X)
REFERENCES,0.34705075445816186,Neighborhoods of incorrect data
REFERENCES,0.3484224965706447,Noise rate: 0.2 (ST)
REFERENCES,0.3497942386831276,Noise rate: 0.2 (AT)
REFERENCES,0.3511659807956104,Noise rate: 0.4 (ST)
REFERENCES,0.35253772290809327,Noise rate: 0.4 (AT)
REFERENCES,0.35390946502057613,(a) Random direction
REFERENCES,0.355281207133059,"20
40
60
80
100
Epochs 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.35665294924554186,Average H(Y|X)
REFERENCES,0.35802469135802467,Neighborhoods of incorrect data
REFERENCES,0.35939643347050754,(b) Adversarial direction
REFERENCES,0.3607681755829904,"Figure 12: The average entropy of models trained by ST and AT. This value is calculated on 100
points in each neighborhood of incorrect data, using CIFAR-10 with symmetric-ﬂipping noise. Both
solid and dashed lines represent ST and AT, respectively. Note that ST learns incorrect data more
deterministically than AT."
REFERENCES,0.36213991769547327,"Result.
In Figure 12, we plot the entropy values of the model predictions on the CIFAR-10 dataset.
In Figure 12(a), we randomly select 100 points in each neighborhood (within a small ϵ-ball, where
ϵ = 0.031) of the incorrect data and calculate their average entropy values in training using the
models trained by ST and AT. In Figure 12(b), we generate the adversarial variant for each incorrect
data by PGD-1 attack with ϵ = 0.031 and calculate the average entropy values in training. The
detailed training settings can be found in Appendix C.1. On the whole, compared with ST, the
entropy values in AT are always higher. It demonstrates that AT did not learn incorrect data with their
neighborhoods deterministically, which conﬁrms that the smoothing effects in AT prevent incorrect
data from forming small clusters during training."
REFERENCES,0.3635116598079561,Under review as a conference paper at ICLR 2022
REFERENCES,0.36488340192043894,"C
KNOCK-ON EFFECTS OF ADVERSARIAL TRAINING"
REFERENCES,0.3662551440329218,"In this section, we provide more complementary experiments and analysis for the positive knock-on
effects of AT. First, we show the results of standard training and test accuracy on CIFAR-10 and
MNIST datasets with different noise rates and types (Appendix C.1). Second, we show the analysis
of the natural data and adversarial data in AT (Appendix C.2). Third, we use different networks to
investigate positive knock-on effects of AT with noisy labels (Appendix C.3). Finally, we show the
results of loss value with different noise rates and types (Appendix C.4)."
REFERENCES,0.3676268861454047,"C.1
TRAINING ACCURACY AND TEST ACCURACY"
REFERENCES,0.36899862825788754,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.37037037037037035,Standard accuracy (%)
REFERENCES,0.3717421124828532,Natural training data in ST
REFERENCES,0.3731138545953361,"Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.37448559670781895,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.37585733882030176,Standard accuracy (%)
REFERENCES,0.3772290809327846,Natural training data in AT
REFERENCES,0.3786008230452675,(a) CIFAR-10
REFERENCES,0.37997256515775035,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.3813443072702332,Standard accuracy (%)
REFERENCES,0.38271604938271603,Natural training data in ST
REFERENCES,0.3840877914951989,"60
80
100 97 98 99 100"
REFERENCES,0.38545953360768176,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.3868312757201646,Standard accuracy (%)
REFERENCES,0.38820301783264743,Natural training data in AT
REFERENCES,0.3895747599451303,(b) MNIST
REFERENCES,0.39094650205761317,"Figure 13: The standard accuracy of ST and AT on correct/incorrect training data using CIFAR-10
and MNIST with symmetric-ﬂipping noise. Solid lines denote the accuracy of correct training data,
while dashed lines correspond to that of incorrect training data. Compared with ST, there is a large
performance gap in the standard accuracy of correct/incorrect training data in AT."
REFERENCES,0.39231824417009603,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.3936899862825789,Standard accuracy (%)
REFERENCES,0.3950617283950617,Natural training data in ST
REFERENCES,0.39643347050754457,"Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.39780521262002744,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.3991769547325103,Standard accuracy (%)
REFERENCES,0.40054869684499317,Natural training data in AT
REFERENCES,0.401920438957476,(a) CIFAR-10
REFERENCES,0.40329218106995884,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4046639231824417,Standard accuracy (%)
REFERENCES,0.4060356652949246,Natural training data in ST
REFERENCES,0.4074074074074074,"60
80
100
92.5 95.0 97.5 100.0"
REFERENCES,0.40877914951989025,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4101508916323731,Standard accuracy (%)
REFERENCES,0.411522633744856,Natural training data in AT
REFERENCES,0.41289437585733885,(b) MNIST
REFERENCES,0.41426611796982166,"Figure 14: The standard accuracy of ST and AT on correct/incorrect training data using CIFAR-10 and
MNIST with pair-ﬂipping noise. Solid lines denote the accuracy of correct training data, while dashed
lines correspond to that of incorrect training data. Compared with ST, there is a large performance
gap in the standard accuracy of correct/incorrect training data in AT."
REFERENCES,0.4156378600823045,"Experimental setup.
We conduct our experiments on two datasets with different noise rates (e.g.,
[0.1, 0.4]) and different noise types (e.g., symmetric-ﬂipping noise and pair-ﬂipping noise). We
use the method in (Han et al., 2018) to generate noisy training data. For the CIFAR-10 dataset, we
normalize it into [0, 1]: Each pixel is scaled by 1/255. We perform the standard CIFAR-10 data
augmentation: a random 4 pixel crop followed by a random horizontal ﬂip. For the MNIST dataset,
we normalize it into [0, 1]. We train ResNet-18 in ST and AT using SGD with 0.9 momentum for
100 epochs on CIFAR-10 dataset. The initial learning rate is 0.1 divided by 10 at Epoch 30 and
60 respectively. The weight decay=0.0005. For MNIST dataset, we use SmallCNN (Zhang et al.,
2019a), and set the initial learning rate as 0.01. The rest of the settings remain the same as training
on CIFAR-10. In AT, we set the perturbation bound ϵ = 0.031, the PGD step size α = 0.007, and
PGD step numbers K = 10. For standard evaluation, we obtain standard accuracy on natural training
data according to correct/incorrect labels and natural test data with all correct labels. For the robust"
REFERENCES,0.4170096021947874,Under review as a conference paper at ICLR 2022
REFERENCES,0.41838134430727025,"evaluation, we obtain robust accuracy on adversarial training and adversarial test data. The adversarial
test data are generated by PGD-20 attack with the same perturbation bound ϵ = 0.031 and the step
size α = 0.031/4, which keeps the same as Wang et al. (2019). All PGD generation have a random
start, i.e, the uniformly random perturbation of [−ϵ, ϵ] added to the natural data before PGD iterations."
REFERENCES,0.41975308641975306,"Result 1. In Figures 13 and 14, we plot the standard accuracy of correct/incorrect training data with
different noise rates and types. On the whole, compared with ST, correct/incorrect training data can be
always distinguishable in AT regardless of noise rates and types. Compared with symmetric-ﬂipping
noise, AT has a better performance on distinguishing correct/incorrect data with pair-ﬂipping noise.
Note that, under the same noise rates, the standard accuracy on incorrect training data in AT with
pair-ﬂipping noise is lower than that with symmetric-ﬂipping noise."
REFERENCES,0.42112482853223593,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4224965706447188,Standard accuracy (%)
REFERENCES,0.42386831275720166,Natural test data in ST
REFERENCES,0.4252400548696845,"Noise ratio: 0.0
Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.42661179698216734,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4279835390946502,Standard accuracy (%)
REFERENCES,0.42935528120713307,Natural test data in AT
REFERENCES,0.43072702331961593,(a) CIFAR-10
REFERENCES,0.43209876543209874,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
REFERENCES,0.4334705075445816,Standard accuracy (%)
REFERENCES,0.4348422496570645,Natural test data in ST
REFERENCES,0.43621399176954734,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
REFERENCES,0.4375857338820302,Standard accuracy (%)
REFERENCES,0.438957475994513,Natural test data in AT
REFERENCES,0.4403292181069959,"20
40
60
80 100
98.0 98.5 99.0 99.5"
REFERENCES,0.44170096021947874,(b) MNIST
REFERENCES,0.4430727023319616,"Figure 15: The standard accuracy of ST and AT on natural test data, where training data using
CIFAR-10 and MNIST with symmetric-ﬂipping noise. Note that the larger noise rate causes the test
accuracy of ST dropping more seriously due to memorization effects in deep learning, while AT
alleviates such negative effects."
REFERENCES,0.4444444444444444,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4458161865569273,Standard accuracy (%)
REFERENCES,0.44718792866941015,Natural test data in ST
REFERENCES,0.448559670781893,"Noise ratio: 0.0
Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.4499314128943759,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4513031550068587,Standard accuracy (%)
REFERENCES,0.45267489711934156,Natural test data in AT
REFERENCES,0.4540466392318244,(a) CIFAR-10
REFERENCES,0.4554183813443073,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
REFERENCES,0.4567901234567901,Standard accuracy (%)
REFERENCES,0.45816186556927296,Natural test data in ST
REFERENCES,0.45953360768175583,"0
20
40
60
80
100
Epochs 65 70 75 80 85 90 95 100"
REFERENCES,0.4609053497942387,Standard accuracy (%)
REFERENCES,0.46227709190672156,Natural test data in AT
REFERENCES,0.46364883401920437,"40
60
80
100
99.2 99.3 99.4 99.5"
REFERENCES,0.46502057613168724,(b) MNIST
REFERENCES,0.4663923182441701,"Figure 16: The standard accuracy of ST and AT on natural test data, where training data using
CIFAR-10 and MNIST with pair-ﬂipping noise. Note that the larger noise rate causes the test accuracy
of ST dropping more seriously due to memorization effects in deep learning, while AT alleviates such
negative effects."
REFERENCES,0.46776406035665297,"Result 2. In Figures 15 and 16, we plot the standard accuracy of natural test data with different noise
rates and types. On the whole, AT can alleviate the negative effects of label noise due to memorization
effects in deep learning. Under each noise type, as the noise rate increases, standard accuracy of
ST on natural test data drops more seriously in the later stage of training (e.g., after 60 epochs in
CIFAR-10). In AT, we only observe that the larger noise rate causes the lower standard accuracy
on natural test data, while the overﬁtting phenomenon is not obvious. Compared with CIFAR-10,
both symmetirc-ﬂipping and pair-ﬂipping noise have more serious negative effects on MNIST, while
simply using AT can alleviate these effects to a greater extent. In Figure 17, we also visualize the
loss landscape (Li et al., 2018) of models trained by ST and AT on CIFAR-10. Such visualization
can further substantiate that AT mitigates negative effects of label noise via the lens of the model
generalization. Namely, the loss landscape w.r.t. weight space of an adversarially trained model (i.e.,
AT) is smoother and ﬂatter than that of a model using ST."
REFERENCES,0.4691358024691358,Under review as a conference paper at ICLR 2022
REFERENCES,0.47050754458161864,"(a) ST
(b) AT"
REFERENCES,0.4718792866941015,"Figure 17: The loss landscape w.r.t weight space of models trained by ST and AT using CIFAR-10
with 20% symmetric-ﬂipping noise. The red/blue colors denote large/small values, which reﬂect
the relative position in the loss landscape. Note that the loss landscape of a model trained by AT is
smoother and ﬂatter than that by ST, which reﬂects the better model generalization by AT."
REFERENCES,0.4732510288065844,"We also conduct experiments on a harder dataset, i.e., CIFAR-100. The smoothing effect of AT is not
signiﬁcant as that on CIFAR-10 or MNIST but can also be found on this dataset. Speciﬁcally, there is
a larger performance gap in the standard accuracy of correct/incorrect training data in AT (e.g., 24%
-30%) than ST (e.g., 0.15% -0.25%), and the test accuracy of ST dropping more seriously (e.g., 2%
-16%) than AT (e.g., 1% -7%)."
REFERENCES,0.47462277091906724,"C.2
NATURAL DATA AND ADVERSARIAL DATA"
REFERENCES,0.47599451303155005,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4773662551440329,Standard accuracy (%)
REFERENCES,0.4787379972565158,Natural training data in AT
REFERENCES,0.48010973936899864,"Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.48148148148148145,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4828532235939643,Robust accuracy (%)
REFERENCES,0.4842249657064472,PGD-10 training data in AT
REFERENCES,0.48559670781893005,(a) Training accuracy
REFERENCES,0.4869684499314129,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.4883401920438957,Standard accuracy (%)
REFERENCES,0.4897119341563786,Natural test data in AT
REFERENCES,0.49108367626886146,"Noise ratio: 0.0
Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.4924554183813443,"20
30
40
50
60
70
80
90 100
Epochs 32.5 35.0 37.5 40.0 42.5 45.0 47.5 50.0 52.5"
REFERENCES,0.49382716049382713,Robust accuracy (%)
REFERENCES,0.49519890260631,PGD-20 test data in AT
REFERENCES,0.49657064471879286,(b) Test accuracy
REFERENCES,0.49794238683127573,"Figure 18: The standard/robust accuracy of AT on natural training data, adversarial training data
(PGD-10), adversarial test data (PGD-20) using the CIFAR-10 dataset with symmetric-ﬂipping noise."
REFERENCES,0.4993141289437586,"Result.
In Figure 18, we plot the standard and robust accuracy on natural data and adversarial data
(e.g., PGD-10 training data and PGD-20 test data) using CIFAR-10 with symmetirc-ﬂipping noise.
Different from ST, each natural training data will generate a corresponding adversarial data in AT.
We also check the difference in robust accuracy between correct and incorrect adversarial data during
training. We found that AT can also distinguish correct/incorrect adversarial data over the whole
training process. However, we ﬁnd that the difference between correct and incorrect adversarial data
(right panel in Figure 19(a)) is smaller than that between incorrect and correct natural data (left panel
in Figure 19(a))."
REFERENCES,0.5006858710562414,"C.3
DIFFERENT NETWORKS"
REFERENCES,0.5020576131687243,"Result.
In Figure 19, we plot the standard accuracy on natural training/test data using ResNet-
10, ResNet-18, ResNet-26 and ResNet-34 trained by ST and AT. We conduct the experiments
using CIFAR-10 dataset with 20% symmetric-ﬂipping noise. The training settings keep the same
as Appendix C.1. We ﬁnd that, using different networks, AT still has a better performance on
distinguishing correct/incorrect data compared with ST and can alleviate the negative effects of label
noise."
REFERENCES,0.5034293552812071,"Result.
In Figure 20, we plot the standard accuracy on natural test data using a large deep network,
Wide-ResNet (e.g.,WRN-32-10 (Zagoruyko & Komodakis, 2016)), trained by ST and AT. We"
REFERENCES,0.50480109739369,Under review as a conference paper at ICLR 2022
REFERENCES,0.5061728395061729,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.5075445816186557,Standard accuracy (%)
REFERENCES,0.5089163237311386,Natural training data in ST
REFERENCES,0.5102880658436214,ResNet-10 (Correct)
REFERENCES,0.5116598079561042,ResNet-18 (Correct)
REFERENCES,0.5130315500685871,ResNet-26 (Correct)
REFERENCES,0.51440329218107,ResNet-34 (Correct)
REFERENCES,0.5157750342935528,ResNet-10 (Incorrect)
REFERENCES,0.5171467764060357,ResNet-18 (Incorrect)
REFERENCES,0.5185185185185185,ResNet-26 (Incorrect)
REFERENCES,0.5198902606310014,ResNet-34 (Incorrect)
REFERENCES,0.5212620027434842,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.522633744855967,Standard accuracy (%)
REFERENCES,0.52400548696845,Natural training data in AT
REFERENCES,0.5253772290809328,(a) Training accuracy
REFERENCES,0.5267489711934157,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.5281207133058985,Standard accuracy (%)
REFERENCES,0.5294924554183813,Natural test data in ST
REFERENCES,0.5308641975308642,"ResNet-10
ResNet-18
ResNet-26
ResNet-34"
REFERENCES,0.532235939643347,"0
20
40
60
80
100
Epochs 20 40 60 80 100"
REFERENCES,0.53360768175583,Standard accuracy (%)
REFERENCES,0.5349794238683128,Natural test data in AT
REFERENCES,0.5363511659807956,(b) Test accuracy
REFERENCES,0.5377229080932785,"Figure 19: The standard accuracy of AT on natural training/test data using the CIFAR-10 dataset with
20% symmetric-ﬂipping noise. We conduct the experiments using ResNet-10, ResNet-18, ResNet-26
and ResNet-34."
REFERENCES,0.5390946502057613,"0
20
40
60
80
100
120
Epochs 20 40 60 80 100"
REFERENCES,0.5404663923182441,Standard accuracy (%)
REFERENCES,0.541838134430727,Natural test data in ST
REFERENCES,0.5432098765432098,"Noise ratio: 0.0
Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.5445816186556928,"0
20
40
60
80
100
120
Epochs 20 40 60 80 100"
REFERENCES,0.5459533607681756,Standard accuracy (%)
REFERENCES,0.5473251028806584,Natural test data in AT
REFERENCES,0.5486968449931413,(a) symmetirc-ﬂipping noise
REFERENCES,0.5500685871056241,"0
20
40
60
80
100
120
Epochs 20 40 60 80 100"
REFERENCES,0.551440329218107,Standard accuracy (%)
REFERENCES,0.5528120713305898,Natural test data in ST
REFERENCES,0.5541838134430727,"0
20
40
60
80
100
120
Epochs 20 40 60 80 100"
REFERENCES,0.5555555555555556,Standard accuracy (%)
REFERENCES,0.5569272976680384,Natural test data in AT
REFERENCES,0.5582990397805213,(b) pair-ﬂipping noise
REFERENCES,0.5596707818930041,"Figure 20: The standard accuracy of AT on natural test data using the CIFAR-10 dataset with
symmetric-ﬂipping and pair-ﬂipping noise. We conduct the experiments using WRN-32-10."
REFERENCES,0.5610425240054869,"conduct the experiments using the CIFAR-10 dataset with different noise rates and types. We train the
network for 120 epochs and set the weight decay=0.0002, the rest of the settings keep the same as
Appendix C.1. We ﬁnd that AT can still alleviate negative effects of label noise due to memorization
effects of deep networks. Particularly, compared with the symmetric-ﬂipping noise, AT has a better
performance on avoiding memorization of pair-ﬂipping noise, which can be conﬁrmed by Figures 15
and 16."
REFERENCES,0.5624142661179699,"C.4
THE LOSS VALUE"
REFERENCES,0.5637860082304527,"Result.
In Figures 21 and 22, we check the loss value of correct/incorrect training data with different
noise rates and types. On the whole, compared with ST, correct/incorrect training data can also be
more distinguishable in AT using the loss value, regardless of noise rates and types."
REFERENCES,0.5651577503429356,Under review as a conference paper at ICLR 2022
REFERENCES,0.5665294924554184,"0
20
40
60
80
100
Epochs 0 1 2 3 4"
REFERENCES,0.5679012345679012,Loss value ℓ
REFERENCES,0.5692729766803841,Natural training data in ST
REFERENCES,0.5706447187928669,"Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.5720164609053497,"20
40
60
80
100
Epochs 0 1 2 3 4"
REFERENCES,0.5733882030178327,Loss value ℓ
REFERENCES,0.5747599451303155,Natural training data in AT
REFERENCES,0.5761316872427984,(a) CIFAR-10
REFERENCES,0.5775034293552812,"0
20
40
60
80
100
Epochs 0 1 2 3 4"
REFERENCES,0.578875171467764,Loss value ℓ
REFERENCES,0.5802469135802469,Natural training data in ST
REFERENCES,0.5816186556927297,"20
40
60
80
100
Epochs 0 1 2 3 4"
REFERENCES,0.5829903978052127,Loss value ℓ
REFERENCES,0.5843621399176955,Natural training data in AT
REFERENCES,0.5857338820301783,(b) MNIST
REFERENCES,0.5871056241426612,"Figure 21: The loss value of ST and AT on correct/incorrect training data using CIFAR-10 and MNIST
with symmetric-ﬂipping noise. Solid lines denote the loss value of correct training data, while dashed
lines correspond to that of incorrect training data. Compared with ST, there is a large gap in the loss
value of correct/incorrect training data in AT."
REFERENCES,0.588477366255144,"0
20
40
60
80
100
Epochs 0.0 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.5898491083676269,Loss value ℓ
REFERENCES,0.5912208504801097,Natural training data in ST
REFERENCES,0.5925925925925926,"Noise ratio: 0.1
Noise ratio: 0.2
Noise ratio: 0.3
Noise ratio: 0.4"
REFERENCES,0.5939643347050755,"20
40
60
80
100
Epochs 0.0 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.5953360768175583,Loss value ℓ
REFERENCES,0.5967078189300411,Natural training data in AT
REFERENCES,0.598079561042524,(a) CIFAR-10
REFERENCES,0.5994513031550068,"0
20
40
60
80
100
Epochs 0.0 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.6008230452674898,Loss value ℓ
REFERENCES,0.6021947873799726,Natural training data in ST
REFERENCES,0.6035665294924554,"20
40
60
80
100
Epochs 0.0 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.6049382716049383,Loss value ℓ
REFERENCES,0.6063100137174211,Natural training data in AT
REFERENCES,0.607681755829904,(b) MNIST
REFERENCES,0.6090534979423868,"Figure 22: The loss value of ST and AT on correct/incorrect training data using CIFAR-10 and MNIST
with pair-ﬂipping noise. Solid lines denote the loss value of correct training data, while dashed lines
correspond to that of incorrect training data. Compared with ST, there is a large gap in the loss value
of correct/incorrect training data in AT."
REFERENCES,0.6104252400548696,Under review as a conference paper at ICLR 2022
REFERENCES,0.6117969821673526,"D
NEW MEASURE: GEOMETRY VALUE κ"
REFERENCES,0.6131687242798354,"In this section, we provide more experimental results of the geometry value κ vs. that of the loss
value, and provide more visualization about the speciﬁc semantic information corresponds to our new
measure. First, we calculate the loss value and geometry value κ of correct/incorrect data in AT with
different noise rates and types (Appendix D.1). Second, we display more visualization results on
CIFAR-10 and MNIST datasets to show the relationship between the geometry value κ and image data
(Appendix D.2). Moreover, we also discuss the geometry value κ with different PGD conﬁgurations
(Appendix D.3)."
REFERENCES,0.6145404663923183,"D.1
GEOMETRY VALUE VS. LOSS VALUE"
REFERENCES,0.6159122085048011,"Result.
In Figures 23 and 24, we plot the density maps of two measures on CIFAR-10 dataset
with symmetric-ﬂipping and pair-ﬂipping noise. We calculate the loss value of natural data and the
geometry value in AT using 5 checkpoints at different epochs (e.g., Epoch20, Epoch40, Epoch60,
Epoch80, Epoch100), which trained with the same settings in Appendix C.1. We perform the min-max
normalization (Tax & Duin, 2000) on both loss value and geometric value κ, which scales the range
of values in [0, 1]. On the whole, it is clear that the geometry value κ has a stable performance of
distinguishing correct/incorrect data under different noise rates and types. Speciﬁcally, under the
pair-ﬂipping noise with the large noise rate (e.g., Noise rate: 0.4), the loss value cannot differentiate
correct/incorrect data well, while the geometry value κ can still have a satisﬁed distinguishing
performance."
REFERENCES,0.6172839506172839,"D.2
DISTINGUISH RARE AND TYPICAL DATA"
REFERENCES,0.6186556927297668,"Result.
In Figures 25 and 26, we visualize more results about the semantic information of images
corresponding to different κ using CIFAR-10 and MNIST datasets. For obtaining the geometry value
κ, we use the GA-PGD method proposed by (Zhang et al., 2021b), which calculates the least number
of iterations that PGD needs to ﬁnd the mis-classiﬁed adversarial variants of input data. On CIFAR-10,
it is calculated by PGD-10 attack with the perturbation bound ϵ = 0.031 and the step size α = 0.007.
On MNIST, the geometry value κ is calculated by PGD-40 attack with the perturbation bound ϵ = 0.3
and the step size α = 0.01. In general, the geometry value κ can represent whether the data is
relatively typical or rare."
REFERENCES,0.6200274348422496,"About Rare and Incorrect Data.
In AT, the incorrect data are actually far away from the decision
boundary, because AT’s smoothing effect prevents memorizing the incorrect data (see Figure 1).
Therefore, it nearly requires 0 steps. Rare data are near the decision boundary and requires a few
steps (e.g., 1 or 2) to ﬁnd its misclassiﬁed adversarial variants."
REFERENCES,0.6213991769547325,"D.3
IMPACT OF PGD CONFIGURATIONS"
REFERENCES,0.6227709190672154,"Speciﬁcally, we plot more results as Figure 6 with different step size α Figure 27 and the ϵ-ball 28.
To be speciﬁc, we keep the same setting adopted in Madry et al. (2018) (i.e., step size α = 2.5 × ϵ/n,
where n is the step number). With the same ϵ-ball, we can ﬁnd that the smaller step size α (i.e., with
the larger step number n) can provide a nuanced stratiﬁcation compared with the larger one, which
means that the data with similar loss values can have more different κ values. With the same step
size α, a small ball radius ϵ can not well stratify the data since the PGD attack may never be able to
successfully attack some examples."
REFERENCES,0.6241426611796982,"Back to the proposed geometry value κ, the key idea behind it is to approximate the distance from a
speciﬁc sample to the decision boundary. A small step size can provide a nuanced approximation of
the distance. Thus, it may be able to provide a nuanced stratiﬁcation. Since the κ value is captured by
the adversarial attacks, a large radius ϵ-ball is needed to provide the sufﬁcient attacking strength for a
successful adversarial attack to reach the decision boundary. Our previous experimental results have
shown that the parameters of PGD in Madry et al. (2018) (widely considered in related literature)
is an appropriate choice to realize the correct/incorrect or typical/atypical data stratiﬁcation in the
benchmarked MNIST and CIFAR-10 datasets."
REFERENCES,0.6255144032921811,Under review as a conference paper at ICLR 2022
REFERENCES,0.6268861454046639,0.5 0.0 0.5 1.0 1.5
REFERENCES,0.6282578875171467,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.6296296296296297,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.6310013717421125,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6323731138545954,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6337448559670782,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.635116598079561,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6364883401920439,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6378600823045267,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6392318244170097,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6406035665294925,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6419753086419753,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.6433470507544582,Density
"CORRECT DATA
INCORRECT DATA",0.644718792866941,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6460905349794238,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6474622770919067,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6488340192043895,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6502057613168725,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6515775034293553,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6529492455418381,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.654320987654321,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6556927297668038,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6570644718792867,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6584362139917695,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.6598079561042524,Density
"CORRECT DATA
INCORRECT DATA",0.6611796982167353,(a) Noise rate: 0.1
"CORRECT DATA
INCORRECT DATA",0.6625514403292181,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.663923182441701,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.6652949245541838,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.6666666666666666,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6680384087791496,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6694101508916324,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6707818930041153,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6721536351165981,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6735253772290809,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6748971193415638,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6762688614540466,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6776406035665294,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.6790123456790124,Density
"CORRECT DATA
INCORRECT DATA",0.6803840877914952,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6817558299039781,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6831275720164609,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6844993141289437,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6858710562414266,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6872427983539094,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6886145404663924,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6899862825788752,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.691358024691358,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6927297668038409,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.6941015089163237,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.6954732510288066,Density
"CORRECT DATA
INCORRECT DATA",0.6968449931412894,(b) Noise rate: 0.2
"CORRECT DATA
INCORRECT DATA",0.6982167352537723,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.6995884773662552,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.700960219478738,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.7023319615912208,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7037037037037037,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7050754458161865,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7064471879286695,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7078189300411523,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7091906721536351,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.710562414266118,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7119341563786008,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7133058984910837,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.7146776406035665,Density
"CORRECT DATA
INCORRECT DATA",0.7160493827160493,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7174211248285323,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7187928669410151,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.720164609053498,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7215363511659808,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7229080932784636,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7242798353909465,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7256515775034293,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7270233196159122,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7283950617283951,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7297668038408779,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.7311385459533608,Density
"CORRECT DATA
INCORRECT DATA",0.7325102880658436,(c) Noise rate: 0.3
"CORRECT DATA
INCORRECT DATA",0.7338820301783264,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7352537722908093,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.7366255144032922,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.7379972565157751,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7393689986282579,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7407407407407407,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7421124828532236,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7434842249657064,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7448559670781894,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7462277091906722,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.747599451303155,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7489711934156379,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.7503429355281207,Density
"CORRECT DATA
INCORRECT DATA",0.7517146776406035,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7530864197530864,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7544581618655692,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7558299039780522,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.757201646090535,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7585733882030178,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7599451303155007,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7613168724279835,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7626886145404664,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7640603566529492,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7654320987654321,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.766803840877915,Density
"CORRECT DATA
INCORRECT DATA",0.7681755829903978,(d) Noise rate: 0.4
"CORRECT DATA
INCORRECT DATA",0.7695473251028807,"Figure 23: The density of AT on correct/incorrect data using CIFAR-10 with symmetric-ﬂipping noise.
Top panels: the loss value in AT. Bottom panels: the geometry value κ in AT. Note that the geometry
value κ has a better distinction on correct/incorrect data."
"CORRECT DATA
INCORRECT DATA",0.7709190672153635,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.7722908093278463,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7736625514403292,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.7750342935528121,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.7764060356652949,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7777777777777778,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7791495198902606,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7805212620027435,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7818930041152263,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7832647462277091,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7846364883401921,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7860082304526749,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7873799725651578,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.7887517146776406,Density
"CORRECT DATA
INCORRECT DATA",0.7901234567901234,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7914951989026063,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7928669410150891,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7942386831275721,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7956104252400549,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7969821673525377,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.7983539094650206,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.7997256515775034,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8010973936899863,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8024691358024691,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.803840877914952,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.8052126200274349,Density
"CORRECT DATA
INCORRECT DATA",0.8065843621399177,(a) Noise rate: 0.1
"CORRECT DATA
INCORRECT DATA",0.8079561042524005,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8093278463648834,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.8106995884773662,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.8120713305898491,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.813443072702332,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8148148148148148,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8161865569272977,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8175582990397805,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8189300411522634,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8203017832647462,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.821673525377229,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.823045267489712,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.8244170096021948,Density
"CORRECT DATA
INCORRECT DATA",0.8257887517146777,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8271604938271605,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8285322359396433,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8299039780521262,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.831275720164609,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8326474622770919,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8340192043895748,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8353909465020576,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8367626886145405,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8381344307270233,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8395061728395061,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.840877914951989,Density
"CORRECT DATA
INCORRECT DATA",0.8422496570644719,(b) Noise rate: 0.2
"CORRECT DATA
INCORRECT DATA",0.8436213991769548,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8449931412894376,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.8463648834019204,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.8477366255144033,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8491083676268861,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.850480109739369,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8518518518518519,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8532235939643347,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8545953360768176,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8559670781893004,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8573388203017832,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8587105624142661,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.8600823045267489,Density
"CORRECT DATA
INCORRECT DATA",0.8614540466392319,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8628257887517147,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8641975308641975,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8655692729766804,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8669410150891632,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8683127572016461,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.869684499314129,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8710562414266118,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8724279835390947,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8737997256515775,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8751714677640604,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.8765432098765432,Density
"CORRECT DATA
INCORRECT DATA",0.877914951989026,(c) Noise rate: 0.3
"CORRECT DATA
INCORRECT DATA",0.879286694101509,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8806584362139918,Epoch20 0 1 2 3 4 5
"CORRECT DATA
INCORRECT DATA",0.8820301783264746,"6
Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.8834019204389575,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8847736625514403,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8861454046639232,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.887517146776406,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8888888888888888,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8902606310013718,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8916323731138546,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8930041152263375,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8943758573388203,Loss value of natural data in AT
"CORRECT DATA
INCORRECT DATA",0.8957475994513031,Density
"CORRECT DATA
INCORRECT DATA",0.897119341563786,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.8984910836762688,Epoch20 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.8998628257887518,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.9012345679012346,Epoch40 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.9026063100137174,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.9039780521262003,Epoch60 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.9053497942386831,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.906721536351166,Epoch80 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.9080932784636488,0.5 0.0 0.5 1.0 1.5
"CORRECT DATA
INCORRECT DATA",0.9094650205761317,Epoch100 0 1 2 3 4 5 6
"CORRECT DATA
INCORRECT DATA",0.9108367626886146,Geometry value  in AT
"CORRECT DATA
INCORRECT DATA",0.9122085048010974,Density
"CORRECT DATA
INCORRECT DATA",0.9135802469135802,(d) Noise rate: 0.4
"CORRECT DATA
INCORRECT DATA",0.9149519890260631,"Figure 24: The density of AT on correct/incorrect data using CIFAR-10 with pair-ﬂipping noise. Top
panels: the loss value in AT. Bottom panels: the geometry value κ in AT. Note that the geometry
value κ has a better distinction on correct/incorrect data."
"CORRECT DATA
INCORRECT DATA",0.9163237311385459,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.9176954732510288,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 plane"
"CORRECT DATA
INCORRECT DATA",0.9190672153635117,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 car"
"CORRECT DATA
INCORRECT DATA",0.9204389574759945,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 bird"
"CORRECT DATA
INCORRECT DATA",0.9218106995884774,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 cat"
"CORRECT DATA
INCORRECT DATA",0.9231824417009602,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 deer"
"CORRECT DATA
INCORRECT DATA",0.9245541838134431,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 dog"
"CORRECT DATA
INCORRECT DATA",0.9259259259259259,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 frog"
"CORRECT DATA
INCORRECT DATA",0.9272976680384087,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 horse"
"CORRECT DATA
INCORRECT DATA",0.9286694101508917,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 ship"
"CORRECT DATA
INCORRECT DATA",0.9300411522633745,"κ: 0
κ: 0-5
κ: 5-10
κ: 10 truck"
"CORRECT DATA
INCORRECT DATA",0.9314128943758574,"Figure 25: The geometry value κ w.r.t. images in CIFAR-10 with 20% symmetric-ﬂipping noise.
The leftmost is the given label of all images on the right. We randomly selected four examples with
the different κ (κ = 0, κ ∈(0, 5), κ ∈(5, 10), κ = 10) in each class. As the geometric value κ
increases from left (κ = 0) to right (κ = 10), the semantic information of images is more typical and
recognizable."
"CORRECT DATA
INCORRECT DATA",0.9327846364883402,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.934156378600823,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 0"
"CORRECT DATA
INCORRECT DATA",0.9355281207133059,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 1"
"CORRECT DATA
INCORRECT DATA",0.9368998628257887,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 2"
"CORRECT DATA
INCORRECT DATA",0.9382716049382716,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 3"
"CORRECT DATA
INCORRECT DATA",0.9396433470507545,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 4"
"CORRECT DATA
INCORRECT DATA",0.9410150891632373,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 5"
"CORRECT DATA
INCORRECT DATA",0.9423868312757202,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 6"
"CORRECT DATA
INCORRECT DATA",0.943758573388203,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 7"
"CORRECT DATA
INCORRECT DATA",0.9451303155006858,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 8"
"CORRECT DATA
INCORRECT DATA",0.9465020576131687,"κ: 0
κ: 0-20
κ: 20-40
κ: 40 9"
"CORRECT DATA
INCORRECT DATA",0.9478737997256516,"Figure 26: The geometry value κ w.r.t. images in MNIST with 10% symmetric-ﬂipping noise. The
leftmost is the given label of all images on the right. We randomly selected four examples with
the different κ (κ = 0, κ ∈(0, 20), κ ∈(20, 40), κ = 40) in each class. As the geometric value κ
increases from left (κ = 0) to right (κ = 40), the semantic information of images is more typical and
recognizable."
"CORRECT DATA
INCORRECT DATA",0.9492455418381345,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.9506172839506173,"Figure 27: The geometry value κ under the different PGD step numbers n with the ϵ = 8/255 and
the step size α = 2.5 × ϵ/n. Note that the smaller step size α (i.e., with the larger step number n)
can provide a nuanced stratiﬁcation compared with the larger one."
"CORRECT DATA
INCORRECT DATA",0.9519890260631001,"0
1
2
3
4
5
Loss value 0 5 10 15 20"
"CORRECT DATA
INCORRECT DATA",0.953360768175583,Geometry value
"CORRECT DATA
INCORRECT DATA",0.9547325102880658,"Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.9561042524005487,= 8/255
"CORRECT DATA
INCORRECT DATA",0.9574759945130316,"0
1
2
3
4
5
Loss value 0 5 10 15 20"
"CORRECT DATA
INCORRECT DATA",0.9588477366255144,Geometry value
"CORRECT DATA
INCORRECT DATA",0.9602194787379973,"Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.9615912208504801,= 12/255
"CORRECT DATA
INCORRECT DATA",0.9629629629629629,"0
1
2
3
4
5
Loss value 0 5 10 15 20"
"CORRECT DATA
INCORRECT DATA",0.9643347050754458,Geometry value
"CORRECT DATA
INCORRECT DATA",0.9657064471879286,"Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.9670781893004116,= 16/255
"CORRECT DATA
INCORRECT DATA",0.9684499314128944,"0
1
2
3
4
5
Loss value 0 5 10 15 20"
"CORRECT DATA
INCORRECT DATA",0.9698216735253772,Geometry value
"CORRECT DATA
INCORRECT DATA",0.9711934156378601,"Correct data
Incorrect data"
"CORRECT DATA
INCORRECT DATA",0.9725651577503429,= 20/255
"CORRECT DATA
INCORRECT DATA",0.9739368998628258,"Figure 28: The geometry value κ under the different ϵ-ball with the step numbers n = 8 and the
step size α = 2.5 × ϵ/n. Note that a small ball radius ϵ can not well stratify the data since the PGD
attack may never be able to successfully attack some examples (i.e., even n steps it can not ﬁnd a
misclassiﬁed variant)."
"CORRECT DATA
INCORRECT DATA",0.9753086419753086,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.9766803840877915,"E
APPLICATIONS OF GEOMETRY VALUE κ"
"CORRECT DATA
INCORRECT DATA",0.9780521262002744,"In this section, we provide the detailed experimental setups for robust annotator and conﬁdence
scores. First, we provide the detailed version of Algorithm 1 (i.e., Algorithm 2) and the details to
implement the experiment in Figure 9 (Appendix E.1). Second, we provide the details to implement
the experiment in Figure 10 (Appendix E.2)."
"CORRECT DATA
INCORRECT DATA",0.9794238683127572,"Motivation.
Both the two applications are of much signiﬁcance for obtaining high-quality labeled
data. First, since the co-existence of noisy labels and adversarial examples is very realistic, we
need to consider training the annotator from noisy training data, and make it robust to adversarial
manipulation in unlabeled data. Second, since the label annotations are not always correct, we need
the conﬁdence score to show whether the annotation is trustworthy or not."
"CORRECT DATA
INCORRECT DATA",0.9807956104252401,"E.1
ROBUST ANNOTATOR"
"CORRECT DATA
INCORRECT DATA",0.9821673525377229,"In Figure 9, we compare four methods on the CIFAR-10 dataset, namely, our robust annotator with
20% symmetric-ﬂipping noise, the PGD-based annotator with 20% symmetric-ﬂipping noise, the
PGD-based annotator without noise, and the standard annotator without noise."
"CORRECT DATA
INCORRECT DATA",0.9835390946502057,"Algorithm 2: Robust Annotator Algorithm (in detail).
Input
:network fθ, training dataset S = {(xi, yi)}n
i=1, learning rate η, number of epochs T,
batch size m, number of batches M, threshold for geometry value K, threshold for loss
value L.
Output :robust annotator fθ.
for epoch = 1, . . . , T do"
"CORRECT DATA
INCORRECT DATA",0.9849108367626886,"for mini-batch = 1, . . . , M do"
"CORRECT DATA
INCORRECT DATA",0.9862825788751715,"Sample: a mini-batch {(xi, yi)}m
i=1 from S.;
for i = 1,...,m (in parallel) do"
"CORRECT DATA
INCORRECT DATA",0.9876543209876543,"Calculate: κi and ℓi of (xi,yi) by GA-PGD method (Zhang et al., 2021b) and
ℓ(fθ(xi), yi), respectively.;
if κi < K and ℓi > L then"
"CORRECT DATA
INCORRECT DATA",0.9890260631001372,"Update: yi ←arg maxi fθ(x).;
end
Generate: adversarial data ˜xi by PGD method (Madry et al., 2018).;
end
Update: θ ←θ −η∇θ{ℓ(fθ(˜xi), yi)}.
end
end"
"CORRECT DATA
INCORRECT DATA",0.99039780521262,"Choice of Thresholds.
As for the choice of the thresholds in learning with noisy labels, we can
use some existing methods to estimate the noise rate of a dataset and then we can set the dynamic
thresholds based on the two values of the training data (e.g., greater than or equal to the top 20%
largest values)."
"CORRECT DATA
INCORRECT DATA",0.9917695473251029,"Experimental setup.
To generate the noisy training data, we randomly assign the wrong label for
a part of correct training data using the method in (Han et al., 2018). For the CIFAR-10 dataset,
we normalize it into [0, 1]: Each pixel is scaled by 1/255. We perform the standard CIFAR-10 data
augmentation: a random 4 pixel crop followed by a random horizontal ﬂip. For all annotators, we
train WRN-32-10 (Zagoruyko & Komodakis, 2016) for 120 epochs using SGD with 0.9 momentum.
The initial learning rate is 0.1 reduced to 0.01, 0.001 and 0.0005 at epoch 60, 90 and 110. The weight
decay is 0.0002. For standard annotator, we use natural data to update the model. For our robust
annotator and the PGD-based annotator, we generate the adversarial data to update the model, the
perturbation bound ϵtrain = 0.031, the PGD step is ﬁxed to 10, and the step size is ﬁxed to 0.007. All
PGD generation have a random start, i.e, the uniformly random perturbation of [−ϵ, ϵ] added to the
natural data before PGD iterations. For our robust annotator, we use the same generation method with
PGD-based annotator as previous mentioned before Epoch 40. After that, we use our Algorithm 2 to"
"CORRECT DATA
INCORRECT DATA",0.9931412894375857,Under review as a conference paper at ICLR 2022
"CORRECT DATA
INCORRECT DATA",0.9945130315500685,"train our robust annotator. We set the threshold for geometry value K = 2 and the threshold for loss
value L to the loss value of 20% · m largest natural data in each mini-batch, where the m = 128 is
batch size. We use the model predictions of the selected natural data as their new label to generate
the adversarial data. As for the evaluations, we select a part of natural test data on the test set of
CIFAR-10 to add adversarial manipulations by PGD-20 attack. The perturbation bound ϵtest = 0.031,
the step number is 20, and the step size α = ϵtest/4, which keeps the same as Wang et al. (2019). We
use the natural and adversarial test data to check the performance of annotators on assigning correct
labels for the U data. Note that, the experiments are conducted using Tesla V100-SXM2, which takes
about 8 hours for each individual trial."
"CORRECT DATA
INCORRECT DATA",0.9958847736625515,"E.2
CONFIDENCE SCORES"
"CORRECT DATA
INCORRECT DATA",0.9972565157750343,"In Figure 10, we plot the accuracy and number of correctly predicted U data w.r.t the geometry value
κ."
"CORRECT DATA
INCORRECT DATA",0.9986282578875172,"Experimental setup.
We train ResNet-18 model in AT with 20% symmetric-ﬂipping noise on
the CIFAR-10 dataset. The training settings keep the same as Appendix C.1. We use the model
checkpoint at Epoch 35 for assigning labels and we randomly select 2000 test data in CIFAR-10
as unlabeled data. We run the test with 5 repeated times with different random seeds for selecting
different test data. In the left panel of Figure 10, we calculate the mean and standard deviation value
of accuracy. In the right panel of Figure 10, we show the number of correctly/wrongly predicted data
in one of the experiments."
