Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013227513227513227,"While Online Gradient Descent and other no-regret learning procedures are known
to efficiently converge to a coarse correlated equilibrium in games where each
agent‚Äôs utility is concave in their own strategy, this is not the case when utilities
are non-concave ‚Äì a common scenario in machine learning applications involving
strategies parameterized by deep neural networks, or when agents‚Äô utilities are
computed by neural networks, or both. Non-concave games introduce significant
game-theoretic and optimization challenges: (i) Nash equilibria may not exist;
(ii) local Nash equilibria, though they exist, are intractable; and (iii) mixed Nash,
correlated, and coarse correlated equilibria generally have infinite support and
are intractable. To sidestep these challenges, we revisit the classical solution
concept of Œ¶-equilibria introduced by Greenwald and Jafari [GJ03], which is
guaranteed to exist for an arbitrary set of strategy modifications Œ¶ even in non-
concave games [SL07]. However, the tractability of Œ¶-equilibria in such games
remains elusive. In this paper, we initiate the study of tractable Œ¶-equilibria in non-
concave games and examine several natural families of strategy modifications. We
show that when Œ¶ is finite, there exists an efficient uncoupled learning algorithm
that approximates the corresponding Œ¶-equilibria. Additionally, we explore cases
where Œ¶ is infinite but consists of local modifications, showing that Online Gradient
Descent can efficiently approximate Œ¶-equilibria in non-trivial regimes."
INTRODUCTION,0.0026455026455026454,"1
Introduction"
INTRODUCTION,0.003968253968253968,"Von Neumann‚Äôs celebrated minimax theorem establishes the existence of Nash equilibrium in all
two-player zero-sum games where the players‚Äô utilities are continuous as well as concave in their
own strategy [Neu28].1 This assumption that players‚Äô utilities are concave, or quasi-concave, in their
own strategies has been a cornerstone for the development of equilibrium theory in Economics, Game
Theory, and a host of other theoretical and applied fields that make use of equilibrium concepts. In
particular, (quasi-)concavity is key for showing the existence of many types of equilibrium, from
generalizations of min-max equilibrium [Fan53; Sio58] to competitive equilibrium in exchange
economies [AD54; McK54], mixed Nash equilibrium in finite normal-form games [Nas50], and,
more generally, Nash equilibrium in (quasi-)concave games [Deb52; Ros65]."
INTRODUCTION,0.005291005291005291,"1Throughout this paper, we model games using the standard convention in Game Theory that each player
has a utility function that they want to maximize. This is, of course, equivalent to modeling the players as loss
minimizers, a convention more common in learning. When we say that a player‚Äôs utility is concave (respectively
non-concave) in their strategy, this is the same as saying that the player‚Äôs loss is convex (respectively non-convex)
in their strategy."
INTRODUCTION,0.006613756613756613,"Table 1: A comparison between different solution concepts in multi-player non-concave games. We
include definitions of Nash equilibrium, mixed Nash equilibrium, (coarse) correlated equilibrium,
strict local Nash equilibrium, and second-order local Nash equilibrium in Appendix B. We also give
a detailed discussion on the existence and complexity of these solution concepts in Appendix B."
INTRODUCTION,0.007936507936507936,"Solution Concept
Incentive Guarantee
Existence
Complexity
Nash equilibrium
‚úó
Mixed Nash equilibrium
‚úì
(Coarse) Correlated equilibrium
Global stability
‚úì
Strict local Nash equilibrium
Local stability
‚úó
Second-order local Nash equilibrium
Second-order stability
‚úó"
INTRODUCTION,0.009259259259259259,"NP-hard
[MK87; AZ22]"
INTRODUCTION,0.010582010582010581,"Local Nash equilibrium
First-order stability
‚úì
PPAD-hard [DSZ21]"
INTRODUCTION,0.011904761904761904,"Œ¶-equilibrium (finite |Œ¶|)
Stability against
finite deviations
‚úì
Efficient Œµ-approximation
for any Œµ > 0 (Theorem 2)"
INTRODUCTION,0.013227513227513227,"Conv(Œ¶(Œ¥))-equilibrium (finite |Œ¶(Œ¥)|)
‚úì
Efficient Œµ-approximation
for Œµ = ‚Ñ¶(Œ¥2) (Theorem 4)"
INTRODUCTION,0.01455026455026455,"Œ¶proj(Œ¥)-equilibrium
‚úì
Efficient Œµ-approximation
via GD/OG for Œµ = ‚Ñ¶(Œ¥2) (Theorem 3,9)"
INTRODUCTION,0.015873015873015872,Œ¶Int(Œ¥)-equilibrium
INTRODUCTION,0.017195767195767195,"First-order stability
when Œµ = ‚Ñ¶(Œ¥2)
‚úì
Efficient Œµ-approximation
via no-regret learning for Œµ = ‚Ñ¶(Œ¥2) (Theorem 5)"
INTRODUCTION,0.018518518518518517,"Not only are equilibria guaranteed to exist in concave games, but it is also well-established‚Äîthanks
to a long line of work at the interface of game theory, learning and optimization whose origins
can be traced to Dantzig‚Äôs work on linear programming [Geo63], Brown and Robinson‚Äôs work
on fictitious play [Bro51; Rob51], Blackwell‚Äôs approachability theorem [Bla56] and Hannan‚Äôs
consistency theory [Han57]‚Äîthat several solution concepts are efficiently computable both centrally
and via decentralized learning dynamics. For instance, it is well-known that the learning dynamics
produced when the players of a game iteratively update their strategies using no-regret learning
algorithms, such as online gradient descent, is guaranteed to converge to Nash equilibrium in two-
player zero-sum concave games, and to coarse correlated equilibrium in multi-player general-sum
concave games [CL06]. The existence of such simple decentralized dynamics further justifies using
these solution concepts to predict the outcome of real-life multi-agent interactions where agents
deploy strategies, obtain feedback, and use that feedback to update their strategies."
INTRODUCTION,0.01984126984126984,"While (quasi-)concave utilities have been instrumental in the development of equilibrium theory,
as described above, they are also too restrictive an assumption. Several modern applications and
outstanding challenges in Machine Learning, from training Generative Adversarial Networks (GANs)
to Multi-Agent Reinforcement Learning (MARL) as well as generic multi-agent Deep Learning
settings where the agents‚Äô strategies are parameterized by deep neural networks or their utilities
are computed by deep neural networks, or both, give rise to games where the agents‚Äô utilities are
non-concave in their own strategies. We call these games non-concave, following [Das22]."
INTRODUCTION,0.021164021164021163,"Unfortunately, classical equilibrium theory quickly hits a wall in non-concave games. First, Nash
equilibria are no longer guaranteed to exist. Second, while mixed Nash, correlated and coarse
correlated equilibria do exist‚Äîunder convexity and compactness of the strategy sets [Gli52], which
we have been assuming all along in our discussion so far, they have infinite support, in general [Kar14].
Finally, they are computationally intractable; so, a fortiori, they are also intractable to attain via
decentralized learning dynamics."
INTRODUCTION,0.022486772486772486,"In view of the importance of non-concave games in emerging ML applications and the afore-described
state-of-affairs, our investigation is motivated by the following broad and largely open question:"
INTRODUCTION,0.023809523809523808,"Question from [Das22]: Is there a theory of non-concave games? What solution concepts are
meaningful, universal, and tractable?"
CONTRIBUTIONS,0.02513227513227513,"1.1
Contributions"
CONTRIBUTIONS,0.026455026455026454,"We study Daskalakis‚Äô question through the lens of the classical solution concept of Œ¶-equilibria
introduced by Greenwald and Jafari [GJ03]. This concept is guaranteed to exist for virtually any
set of strategy modifications Œ¶, even in non-concave games, as demonstrated by Stoltz and Lugosi
[SL07].2 However, the tractability of Œ¶-equilibria in such games remains elusive. In this paper, we
initiate the study of tractable Œ¶-equilibria in non-concave games and examine several natural families
of strategy modifications."
CONTRIBUTIONS,0.027777777777777776,2Stoltz and Lugosi [SL07] only require the elements of Œ¶ to be measurable functions.
CONTRIBUTIONS,0.0291005291005291,"Figure 1: The relationship between different solution concepts in non-concave games. An arrow
from one solution concept to another means the former is contained in the latter. The dashed arrow
from Conv(Œ¶(Œ¥))-equilibria to Œ¶Finite-equilibria means the former is contained in the latter when
Œ¶(Œ¥) = Œ¶Finite."
CONTRIBUTIONS,0.03042328042328042,"Œ¶!""#$(ùõø)-Eq.
Œ¶%&'(ùõø)-Eq.
CCE"
CONTRIBUTIONS,0.031746031746031744,Local NE NE MNE CE
CONTRIBUTIONS,0.03306878306878307,"Œ¶()&)'* -Eq.
Conv Œ¶ ùõø
-Eq."
CONTRIBUTIONS,0.03439153439153439,"Global Deviation
Local Deviation"
CONTRIBUTIONS,0.03571428571428571,"Œ¶-Equilibrium. The concept of Œ¶-equilibrium generalizes (coarse) correlated equilibrium. A Œ¶-
equilibrium is a joint distribution over Œ†n
i=1Xi, the Cartesian product of all players‚Äô strategy sets,
and is defined in terms of a set, Œ¶Xi, of strategy modifications, for each player i. The set Œ¶Xi
contains functions mapping Xi to itself. A joint distribution over strategy profiles qualifies as a
Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium if no player i can increase their expected utility by using any strategy
modification function, œïi ‚ààŒ¶Xi, on the strategy sampled from the joint distribution. The larger
the set Œ¶, the stronger the incentive guarantee offered by the Œ¶-equilibrium. For example, if Œ¶Xi
contains all constant functions, the corresponding Œ¶-equilibrium coincides with the notion of coarse
correlated equilibrium. Throughout the paper, we also consider Œµ-approximate Œ¶-equilibria, where
no player can gain more than Œµ by deviating using any function from Œ¶Xi. We study several families
of Œ¶ and illustrate their relationships in Figure 1."
CONTRIBUTIONS,0.037037037037037035,"Finite Set of Global Deviations. The first case we consider is when each player i‚Äôs set of strat-
egy modifications, Œ¶Xi, contains a finite number of arbitrary functions mapping Xi to itself. As
shown in [GJ03], if there exists an online learning algorithm where each player i is guaranteed
to have sublinear Œ¶Xi-regret, the empirical distribution of joint strategies played converges to a
Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium. Gordon, Greenwald, and Marks [GGM08] consider Œ¶-regret minimiza-
tion but for concave reward functions, and their results, therefore, do not apply to non-concave games.
Stoltz and Lugosi [SL07] provide an algorithm that achieves no Œ¶Xi-regret in non-concave games;
however, their algorithm requires a fixed-point computation per step, making it computationally
inefficient.3 Our first contribution is to provide an efficient randomized algorithm that achieves no
Œ¶Xi-regret for each player i with high probability."
CONTRIBUTIONS,0.03835978835978836,"Contribution 1: Let X be a strategy set (not necessarily compact or convex), and Œ® an arbitrary
finite set of strategy modification functions for X. We design a randomized online learning
algorithm that achieves O
p"
CONTRIBUTIONS,0.03968253968253968,"T log |Œ®|

Œ®-regret, with high probability, for arbitrary bounded"
CONTRIBUTIONS,0.041005291005291,"reward functions on X (Theorem 2). The algorithm operates in time
‚àö"
CONTRIBUTIONS,0.042328042328042326,"T|Œ®| per iteration. If
every player in a non-concave game adopts this algorithm, the empirical distribution of strategy
profiles played forms an Œµ-approximate Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium, with high probability, for
any Œµ > 0, after poly
  1"
CONTRIBUTIONS,0.04365079365079365,"Œµ, log
 
maxi |Œ¶Xi|

, log n

iterations."
CONTRIBUTIONS,0.04497354497354497,"If players have infinitely many global strategy modifications, we can extend Algorithm 1 by dis-
cretizing the set of strategy modifications under mild assumptions, such as the modifications being
Lipschitz (Corollary 1). The empirical distribution of the strategy profiles still converges to the
corresponding Œ¶-equilibrium, but at a much slower rate of O(T ‚àí
1
d+2 ), where d is the dimension
of the set of strategies. Additionally, the algorithm requires exponential time in the dimension per
iteration, making it inefficient. This inefficiency is unavoidable, as the problem remains intractable
even when Œ¶ contains only constant functions."
CONTRIBUTIONS,0.046296296296296294,"3The existence of the fixed point is guaranteed by the Schauder-Cauty fixed-point theorem [Cau01], a
generalization of the Brouwer fixed-point theorem. Hence, it‚Äôs unlikely such fixed points are tractable."
CONTRIBUTIONS,0.047619047619047616,"To address the limitations associated with infinitely large global strategy modifications, a natural
approach is to focus on local deviations instead. The corresponding Œ¶-equilibrium will guarantee
local stability. The study of local equilibrium concepts in non-concave games has received significant
attention in recent years‚Äîsee e.g., [RBS16; HSZ17; DP18; JNJ20; DSZ21]. However, these
solution concepts either are not guaranteed to exist, are restricted to sequential two-player zero-sum
games [MV21], only establish local convergence guarantees for learning dynamics‚Äîsee e.g., [DP18;
WZB20; FCR20], only establish asymptotic convergence guarantees‚Äîsee e.g., [Das+23], or involve
non-standard solution concepts where local stability is not with respect to a distribution over strategy
profiles [HSZ17]."
CONTRIBUTIONS,0.04894179894179894,"We study the tractability of Œ¶-equilibrium with infinitely large Œ¶ sets that consist solely of local
strategy modifications. These local solution concepts are guaranteed to exist in general multi-player
non-concave games. Specifically, we focus on the following three families of natural deviations."
CONTRIBUTIONS,0.05026455026455026,"- Projection based Local Deviations: Each player i‚Äôs set of strategy modifications, denoted by
Œ¶Xi
Proj(Œ¥), contains all deviations that attempt a small step from their input in a fixed direction and
project if necessary, namely are of the form œïv(x) = Œ†Xi[x ‚àív], where ‚à•v‚à•‚â§Œ¥ and Œ†Xi stands
for the ‚Ñì2-projection onto Xi.
- Convex Combination of Finitely Many Local Deviations: Each player i‚Äôs set of strategy modi-
fications, denoted by Conv(Œ¶Xi(Œ¥)), contains all deviations that can be represented as a convex
combination of a finite set of Œ¥-local strategy modifications, i.e., ‚à•œï(x)‚àíx‚à•‚â§Œ¥ for all œï ‚ààŒ¶Xi(Œ¥).
- Interpolation based Local Deviations: each player i‚Äôs set of local strategy modifications, denoted
by Œ¶Xi
Int(Œ¥), that contains all deviations that interpolate between the input strategy and another
strategy in Xi. Formally, each element œïŒª,x‚Ä≤(x) of Œ¶Xi
Int(Œ¥) can be represented as (1 ‚àíŒª)x + Œªx‚Ä≤
for some x‚Ä≤ ‚ààXi and Œª ‚â§Œ¥/DXi (DXi is the diameter of Xi)."
CONTRIBUTIONS,0.051587301587301584,"For our three families of local strategy modifications, we explore the tractability of Œ¶-equilibrium
within a regime we term the first-order stationary regime, where Œµ = ‚Ñ¶(Œ¥2)4, with Œ¥ representing the
maximum deviation allowed for a player. An Œµ-approximate Œ¶-equilibrium in this regime ensures first-
order stability. This regime is particularly interesting for two reasons: (i) Daskalakis, Skoulakis, and
Zampetakis [DSZ21] have demonstrated that computing an Œµ-approximate Œ¥-local Nash equilibrium
in this regime is intractable.5 This poses an intriguing question: can correlating the players‚Äô strategies,
as in a Œ¶-equilibrium, potentially make the problem tractable? (ii) Extending our algorithm, initially
designed for finite sets of strategy modifications, to these three sets of local deviations results in
inefficiency; specifically, the running time becomes exponential in one of the problem‚Äôs natural
parameters. Designing efficient algorithms for this regime thus presents challenges. Despite these,
we show the following:"
CONTRIBUTIONS,0.05291005291005291,"Contribution 2: For any Œ¥ > 0, for each of the three families of infinite Œ¥-local strategy modifi-
cations mentioned above, there exists an efficient uncoupled learning algorithm that converges
to an Œµ-approximate Œ¶-equilibrium of the non-concave game in the first-order stationary regime,
i.e., Œµ = ‚Ñ¶(Œ¥2)."
CONTRIBUTIONS,0.05423280423280423,"We present our results for the projection-based local deviation in Theorem 3 and Theorem 9. Our
result for the convex combination of local deviations can be found in Theorem 4. Theorem 5 contains
our result for the interpolation-based local deviations. Similar to the finite case, our algorithms
build on the connection between Œ¶-regret minimization and Œ¶-equilibrium. Given that our strategy
modifications are non-standard, it is a priori unclear how to minimize the corresponding Œ¶-regret. For
instance, to our knowledge, no algorithm is known to minimize Œ¶X
Proj(Œ¥)-regret even when the reward
functions are concave, and provably Œ¶X
Proj(Œ¥)-regret is incomparable to external regret (Examples 3
and 4). However, via a novel analysis, we show that Online Gradient Descent (GD) and Optimistic
Gradient (OG) achieve a near-optimal Œ¶X
Proj(Œ¥)-regret guarantee (Theorem 3 and Theorem 8). Our
results provide efficient uncoupled algorithms to compute Œµ-approximate Œ¶(Œ¥)-equilibria in the
first-order stationary regime Œµ = ‚Ñ¶(Œ¥2)."
CONTRIBUTIONS,0.05555555555555555,Further related work is discussed in Appendix A.
CONTRIBUTIONS,0.056878306878306875,"4The regime Œµ = ‚Ñ¶(Œ¥) is trivial when the utility is Lipschitz.
5A strategy profile is considered an Œµ-approximate Œ¥-local Nash equilibrium if no player can gain more than
Œµ by deviating within a Œ¥ distance."
PRELIMINARIES,0.0582010582010582,"2
Preliminaries"
PRELIMINARIES,0.05952380952380952,"A ball of radius r > 0 centered at x ‚ààRd is denoted by Bd(x, r) := {x‚Ä≤ ‚ààRd : ‚à•x ‚àíx‚Ä≤‚à•‚â§r}. We
use ‚à•¬∑ ‚à•for ‚Ñì2 norm throughout. We also write Bd(Œ¥) for a ball centered at the origin with radius Œ¥.
For a ‚ààR, we use [a]+ to denote max{0, a}. We denote DX as the diameter of a set X."
PRELIMINARIES,0.06084656084656084,"Continuous / Smooth Games.
An n-player continuous game has a set of n players [n] :=
{1, 2, . . . , n}. Each player i ‚àà[n] has a nonempty convex and compact strategy set Xi ‚äÜRdi.
For a joint strategy profile x = (xi, x‚àíi) ‚ààQn
j=1 Xj, the reward of player i is determined by a utility
function ui : Qn
j=1 Xj ‚Üí[0, 1]. We denote by d = Pn
i=1 di the dimensionality of the game and
assume maxi‚àà[n]{DXi} ‚â§D. A smooth game is a continuous game whose utility functions further
satisfy the following assumption.
Assumption 1 (Smooth Games). The utility function ui(xi, x‚àíi) for any player i ‚àà[n] is differen-
tiable and satisfies"
PRELIMINARIES,0.062169312169312166,"1. (G-Lipschitzness) ‚à•‚àáxiui(x)‚à•‚â§G for all i and x ‚ààQn
j=1 Xj;"
PRELIMINARIES,0.06349206349206349,"2. (L-smoothness) there exists Li > 0 such that ‚à•‚àáxiui(xi, x‚àíi) ‚àí‚àáxiui(x‚Ä≤
i, x‚àíi)‚à•‚â§Li‚à•xi ‚àíx‚Ä≤
i‚à•
for all xi, x‚Ä≤
i ‚ààXi and x‚àíi ‚ààŒ†jÃ∏=iXj. We denote L = maxi Li as the smoothness of the game."
PRELIMINARIES,0.06481481481481481,"Crucially, we make no assumption on the concavity of ui(xi, x‚àíi)."
PRELIMINARIES,0.06613756613756613,"Œ¶-equilibrium and Œ¶-regret. Below we formally introduce the concept of Œ¶-equilibrium and its
relationship with online learning and Œ¶-regret minimization.
Definition 1 (Œ¶-equilibrium [GJ03; SL07]). In a continuous game, a distribution œÉ over joint
strategy profiles Œ†n
i=1Xi is an Œµ-approximate Œ¶-equilibrium for some Œµ ‚â•0 and a profile of strategy
modification sets Œ¶ = Œ†n
i=1Œ¶i if and only if for all player i ‚àà[n], maxœï‚ààŒ¶i Ex‚àºœÉ[ui(œï(xi), x‚àíi)] ‚â§
Ex‚àºœÉ[ui(x)] + Œµ. When Œµ = 0, we call œÉ a Œ¶-equilibrium."
PRELIMINARIES,0.06746031746031746,"We consider the standard online learning setting: at each day t ‚àà[T], the learner chooses an action xt
from a nonempty convex compact set X ‚äÜRm and the adversary chooses a possibly non-convex loss
function f t : X ‚ÜíR, then the learner suffers a loss f t(xt) and receives feedback. In this paper, we
focus on two feedback models: (1) the player receives an oracle for f t(¬∑); (2) the player receives only
the gradient ‚àáf t(xt). The classic goal of an online learning algorithm is to minimize the external
regret defined as RegT := maxx‚ààX
PT
t=1(f t(xt) ‚àíf t(x)). An algorithm is called no-regret if its
external regret is sublinear in T. The notion of Œ¶-regret generalizes external regret by allowing more
general strategy modifications."
PRELIMINARIES,0.06878306878306878,"Definition 2 (Œ¶-regret). Let Œ¶ be a set of strategy modification functions {œï : X ‚ÜíX}. For T ‚â•1,
the Œ¶-regret of an online learning algorithm is RegT
Œ¶ := maxœï‚ààŒ¶
PT
t=1 (f t(xt) ‚àíf t(œï(xt))). An
algorithm is called no Œ¶-regret if its Œ¶-regret is sublinear in T."
PRELIMINARIES,0.0701058201058201,"Many classic notions of regret can be interpreted as Œ¶-regret. For example, the external regret is
Œ¶ext-regret where Œ¶ext contains all constant strategy modifications œïx‚àó(x) = x‚àófor all x‚àó‚ààX.
The swap regret on simplex ‚àÜm is Œ¶swap-regret where Œ¶swap contains all linear transformations
œï : ‚àÜm ‚Üí‚àÜm. A fundamental result for learning in games is that no-Œ¶-regret learning dynamics in
games converge to an approximate Œ¶-equilibrium [GJ03]."
PRELIMINARIES,0.07142857142857142,"Theorem 1 ([GJ03]). If each player i‚Äôs Œ¶i-regret is upper bounded by RegT
Œ¶i, then their empirical
distribution of strategy profiles played is an (maxi‚àà[n] RegT
Œ¶i/T)-approximate Œ¶-equilibrium."
PRELIMINARIES,0.07275132275132275,"3
Tractable Œ¶-Equilibrium for Finite Œ¶ via Sampling"
PRELIMINARIES,0.07407407407407407,"In this section, we revisit the problem of computing and learning an Œ¶-equilibrium in non-concave
games when each player‚Äôs set of strategy modifications Œ¶Xi is finite."
PRELIMINARIES,0.07539682539682539,"The pioneering work of Stoltz and Lugosi [SL07] gives a no-Œ¶-regret algorithm for this case where
each player chooses a distribution over strategies in each round. This result also implies convergence
to Œ¶-equilibrium. However, the algorithm by Stoltz and Lugosi [SL07] is not computationally
efficient. In each iteration, their algorithm requires computing a distribution that is stationary under a"
PRELIMINARIES,0.07671957671957672,"transformation that can be represented as a mixture of the modifications in Œ¶. The existence of such
a stationary distribution is guaranteed by the Schauder-Cauty fixed-point theorem [Cau01], but the
distribution might require exponential support and be intractable to find."
PRELIMINARIES,0.07804232804232804,"Our main result in this section is an efficient Œ¶-regret minimization algorithm (Algorithm 1) that
circumvents the step of the exact computation of a stationary distribution. Consequently, our algorithm
also ensures efficient convergence to a Œ¶-equilibrium when adopted by all players."
PRELIMINARIES,0.07936507936507936,"Algorithm 1: Œ¶-regret minimization for non-concave reward via sampling
Input: xroot ‚ààX, h ‚â•2, an external regret minimization algorithm RŒ¶ over Œ¶
Output: A Œ¶-regret minimization algorithm for X"
PRELIMINARIES,0.08068783068783068,1 function NEXTSTRATEGY()
PRELIMINARIES,0.082010582010582,"2
pt ‚ÜêRŒ¶.NEXTSTRATEGY(). Note that pt is a distribution over Œ¶."
PRELIMINARIES,0.08333333333333333,"3
return xt ‚ÜêSAMPLESTRATEGY(xroot, h, pt)."
PRELIMINARIES,0.08465608465608465,4 function OBSERVEREWARD(ut(¬∑))
SET UT,0.08597883597883597,"5
Set ut
Œ¶(œï) = ut(œï(xt)) for all œï ‚ààŒ¶."
SET UT,0.0873015873015873,"6
RŒ¶.OBSERVEREWARD(ut
Œ¶(¬∑))."
SET UT,0.08862433862433862,"Algorithm 2: SAMPLESTRATEGY
Input: xroot ‚ààX, h ‚â•2, pt ‚àà‚àÜ(Œ¶)
Output: x ‚ààX"
SET UT,0.08994708994708994,1 x1 ‚Üêxroot.
SET UT,0.09126984126984126,2 for 2 ‚â§k ‚â§h do
SET UT,0.09259259259259259,"3
œï ‚Üêsample form Œ¶ according to pt."
SET UT,0.09391534391534391,"4
xk = œï(xk‚àí1)."
SET UT,0.09523809523809523,"5 return x from {x1, . . . , xh} uniformly at random."
SET UT,0.09656084656084656,"Theorem 2. Let X be a strategy set (not necessarily compact or convex), Œ¶ be an arbitrary finite set of
strategy modifications over X, and u1(¬∑), . . . , uT (¬∑) be an arbitrary sequence of possibly non-concave
reward functions from X to [0, 1]. If we instantiate Algorithm 1 with RŒ¶ being the Hedge algorithm
over Œ¶ and h =
‚àö"
SET UT,0.09788359788359788,"T, the algorithm guarantees that, with probability at least 1 ‚àíŒ≤, it produces a
sequence of strategies x1, . . . , xT with Œ¶-regret at most maxœï‚ààŒ¶
PT
t=1 ut(œï(xt)) ‚àíPT
t=1 ut(xt) ‚â§
8
p"
SET UT,0.0992063492063492,"T(log |Œ¶| + log(1/Œ≤)). Moreover, the algorithm runs in time O(
‚àö"
SET UT,0.10052910052910052,T|Œ¶|) per iteration.
SET UT,0.10185185185185185,"If all players in a non-concave continuous game employ Algorithm 1, then with probability at least
1 ‚àíŒ≤, for any Œµ > 0, the empirical distribution of strategy profiles played forms an Œµ-approximate
Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium, after poly

1
Œµ, log
 
maxi |Œ¶Xi|

, log n"
SET UT,0.10317460317460317,"Œ≤

iterations."
SET UT,0.10449735449735449,"High-level ideas. We adopt the framework in [SL07]. The framework contains two steps in each
iteration t: (1) the learner runs a no-external-regret algorithm over Œ¶ which outputs pt ‚àà‚àÜ(Œ¶) in
each iteration t; (2) the learner chooses a stationary distribution ¬µt = P"
SET UT,0.10582010582010581,"œï‚ààŒ¶ ptœï(¬µt), where we
slightly abuse notation to use œï(¬µt) to denote the image measure of ¬µ by œï. However, how to compute
the stationary distribution ¬µt efficiently is unclear. We essentially provide a computationally efficient
way to carry out step (2) without computing this stationary distribution."
SET UT,0.10714285714285714,"‚Ä¢ We first construct an Œµ-approximate stationary distribution by recursively applying strategy modifi-
cations from Œ¶. The constructed distribution can be viewed as a tree. Our construction is inspired
by the recent work of Zhang, Anagnostides, Farina, and Sandholm [Zha+24] for concave games.
The main difference here is that for non-concave games, the distribution needs to be approximately
stationary with respect to a mixture of strategy modifications rather than a single one as in concave
games. Consequently, this leads to an approximate stationary distribution with prohibitively high
support size (|Œ¶|)
‚àö"
SET UT,0.10846560846560846,"T , as opposed to
‚àö"
SET UT,0.10978835978835978,T in [Zha+24] for concave games.
SET UT,0.1111111111111111,"‚Ä¢ Despite the exponentially large support size of the distribution, we utilize its tree structure to design
a simple and efficient sampling procedure that runs in time
‚àö"
SET UT,0.11243386243386243,T. Equipped with such a sampling
SET UT,0.11375661375661375,"procedure, we provide an efficient randomized algorithm that generates a sequence of strategies so
that, with high probability, the Œ¶-regret for this sequence of strategies is at most O(
p"
SET UT,0.11507936507936507,T log |Œ¶|).
SET UT,0.1164021164021164,"We defer the full proof of Theorem 2 to Section 3.1. An extension of Theorem 2 to infinite Œ¶ holds
when the rewards {ut}t‚àà[T ] are G-Lipschitz and Œ¶ admits an Œ±-cover with size N(Œ±). In particular,
when Œ¶ is the set of all M-Lipschitz functions over [0, 1]d, Œ¶ admits an Œ±-cover with log N(Œ±) of
the order (1/Œ±)d [SL07]. In this case, we have"
SET UT,0.11772486772486772,"Corollary 1. There is a randomized algorithm such that, with probability at least 1 ‚àíŒ≤, the Œ¶-regret
is bounded by c ¬∑ T
d+1
d+2 ¬∑ log(1/Œ≤), where c only depends on G and M. The algorithm runs in time
poly(T, N(T ‚àí1/(d+2)))."
SET UT,0.11904761904761904,"3.1
Proof of Theorem 2"
SET UT,0.12037037037037036,"For a distribution ¬µ ‚àà‚àÜ(X) over strategy space X, we slightly abuse notation and define its expected
utility as ut(¬µ) := Ex‚àº¬µ[ut(x)] ‚àà[0, 1]. We define œï(¬µ) the image of ¬µ under transformation œï. In
each iteration t, the learner chooses their strategy xt ‚ààX according to the distribution ¬µt. For a
sequence of strategies {xt}t‚àà[T ], the Œ¶-regret is RegT
Œ¶ := maxœï‚ààŒ¶
nPT
t=1 (ut(œï(xt)) ‚àíut(xt))
o
.
Algorithm 1 uses an external regret minimization algorithm RŒ¶ over Œ¶ which outputs a distribution
pt ‚àà‚àÜ(Œ¶). We can then decompose the Œ¶-regret into two parts."
SET UT,0.12169312169312169,"RegT
Œ¶ = max
œï‚ààŒ¶ ( T
X"
SET UT,0.12301587301587301,"t=1
ut(œï(xt)) ‚àíEœï‚Ä≤‚àºpt
ut(œï‚Ä≤(xt))

)"
SET UT,0.12433862433862433,"|
{z
}
I: external regret over Œ¶ + T
X"
SET UT,0.12566137566137567,"t=1
Eœï‚Ä≤‚àºpt
ut(œï‚Ä≤(xt))

‚àíut(xt)"
SET UT,0.12698412698412698,"|
{z
}
II: approximation error of stationary distribution ."
SET UT,0.1283068783068783,"I: Bounding the external regret over Œ¶. The external regret over Œ¶ can be bounded directly. This
is equivalent to an online expert problem: in each iteration t, the external regret minimizer RŒ¶
chooses pt ‚àà‚àÜ(Œ¶) and the adversary then determines the utility of each expert œï ‚ààŒ¶ as ut(œï(xt)).
We choose the external regret minimizer RŒ¶ to be the Hedge algorithm [FS99]. Then we have
maxœï‚ààŒ¶
nPT
t=1 ut(œï(xt)) ‚àíEœï‚Ä≤‚àºpt[ut(œï‚Ä≤(xt))]
o
‚â§2
p"
SET UT,0.12962962962962962,"T log |Œ¶| (Theorem 6), where we use the"
SET UT,0.13095238095238096,"fact that the utility function ut is bounded in [0, 1]."
SET UT,0.13227513227513227,"II: Bounding error due to sampling from an approximate stationary distribution. We first
define a distribution ¬µt over X using a complete |Œ¶|-ary tree with depth h. The root of this tree is an
arbitrary strategy xroot ‚ààX. Each internal node x has exactly |Œ¶| children, denoted as {œï(x)}œï‚ààŒ¶.
The distribution ¬µt is supported on the nodes of this tree. Next, we define the probability for each
node under the distribution ¬µt. The root node xroot receives probability 1"
SET UT,0.1335978835978836,"h. The probability of other
nodes is defined in a recursive manner. For every node x = œï(xp) where xp is its parent, x receives
probability Pr¬µt[x] = Pr¬µt[xp] ¬∑ pt(œï). It is then clear that the total probability of the children of
a node xp is exactly Pr¬µt[x is xp‚Äôs child] = P"
SET UT,0.1349206349206349,"œï Pr[xp] ¬∑ pt(œï) = Pr[xp]. Denote the set of nodes
in depth k as Nk. We have Pr¬µt[x ‚ààNk] = 1"
SET UT,0.13624338624338625,h for every depth 1 ‚â§k ‚â§h. Thus the distribution ¬µt
SET UT,0.13756613756613756,supports on |Œ¶|h‚àí1
SET UT,0.1388888888888889,"|Œ¶|‚àí1 points and is well-defined. By the construction above, we know xt output by
Algorithm 2 is a sample from ¬µt."
SET UT,0.1402116402116402,Now we show that the approximation error of ¬µt is bounded by O( 1
SET UT,0.14153439153439154,"h). We can evaluate the approxi-
mation error of ¬µt:"
SET UT,0.14285714285714285,"Eœï‚àºpt
ut(œï(¬µt))

‚àíut(¬µt) = Eœï‚àºpt "" h
X k=1 X"
SET UT,0.14417989417989419,"x‚ààNk
Pr
¬µt [x]ut(œï(x)) # ‚àí "" h
X k=1 X"
SET UT,0.1455026455026455,"x‚ààNk
Pr
¬µt [x]ut(x) # = h
X k=1 X x‚ààNk"
SET UT,0.14682539682539683,"
Eœï‚àºpt

Pr
¬µt [x]ut(œï(x))

‚àíPr
¬µt [x]ut(x)

."
SET UT,0.14814814814814814,"We recall that for a node x = œï(xp) with xp being its parent, we have Pr¬µt[x] = Pr¬µt[xp] ¬∑ pt(œï).
Thus for any 1 ‚â§k ‚â§h ‚àí1, we have X x‚ààNk"
SET UT,0.14947089947089948,"
Eœï‚àºpt

Pr
¬µt [x]ut(œï(x))

‚àíPr
¬µt [x]ut(x)

=
X x‚ààNk Ô£´ Ô£≠X"
SET UT,0.15079365079365079,"œï‚ààŒ¶
pt(œï) Pr
¬µt [x]ut(œï(x)) ‚àíPr
¬µt [x]ut(x) Ô£∂ Ô£∏ =
X"
SET UT,0.15211640211640212,"x‚ààNk+1
Pr
¬µt [x]ut(x) ‚àí
X"
SET UT,0.15343915343915343,"x‚ààNk
Pr
¬µt [x]ut(x)."
SET UT,0.15476190476190477,"Using the above equality, we get"
SET UT,0.15608465608465608,"Eœï‚àºpt
ut(œï(¬µt))

‚àíut(¬µt) = h‚àí1
X k=1 X"
SET UT,0.1574074074074074,"x‚ààNk+1
Pr
¬µt [x]ut(x) +
X x‚ààNh X"
SET UT,0.15873015873015872,"œï‚ààŒ¶
pt(œï) Pr
¬µt [x]ut(œï(x)) ‚àí h
X k=2 X"
SET UT,0.16005291005291006,"x‚ààNk
Pr
¬µt [x]ut(x) ‚àíPr
¬µt [xroot]ut(xroot) =
X x‚ààNh X"
SET UT,0.16137566137566137,"œï‚ààŒ¶
pt(œï) Pr
¬µt [x]ut(œï(x)) ‚àíPr
¬µt [xroot]ut(xroot) ‚â§1 h,"
SET UT,0.1626984126984127,where in the last inequality we use the fact that P
SET UT,0.164021164021164,x‚ààNk Pr¬µt[x] = 1
SET UT,0.16534391534391535,"h for all 1 ‚â§k ‚â§h and the
utility function ut is bounded in [0, 1]. Therefore, for xt ‚àº¬µt, the sequence of random variables
{PœÑ
t=1 (Eœï‚àºpt[ut(œï(xt))] ‚àíut(xt) ‚àí1"
SET UT,0.16666666666666666,"h)}œÑ‚â•1 is a super-martingale. Thanks to the boundedness of
the utility function, we can apply the Hoeffding-Azuma Inequality and get for any Œµ > 0. Pr "" T
X t=1"
SET UT,0.167989417989418,"
Eœï‚àºpt
ut(œï(xt))

‚àíut(xt) ‚àí1 h 
‚â•Œµ #"
SET UT,0.1693121693121693,"‚â§exp

‚àíŒµ2"
T,0.17063492063492064,8T
T,0.17195767195767195,"
.
(1)"
T,0.17328042328042328,"Combining the bounds for I and II with Œµ =
p"
T,0.1746031746031746,"8T log(1/Œ≤) and h =
‚àö"
T,0.17592592592592593,"T, we have, with probability
1 ‚àíŒ≤, that RegT
Œ¶ ‚â§2
p"
T,0.17724867724867724,"T log |Œ¶| + T h +
p"
T,0.17857142857142858,"8T log(1/Œ≤) ‚â§8
p"
T,0.17989417989417988,T(log |Œ¶| + log(1/Œ≤)).
T,0.18121693121693122,"Convergence to Œ¶-equilibrium. If all players in a non-concave continuous game employ Algo-
rithm 1, then we know for each player i, with probability 1 ‚àíŒ≤"
T,0.18253968253968253,"n, its Œ¶Xi-regret is upper bounded
by 8
p"
T,0.18386243386243387,"T(log |Œ¶Xi| + log(n/Œ≤)). By a union bound over all n players, we get with probability
1 ‚àíŒ≤, every player i‚Äôs Œ¶Xi-regret is upper bounded by 8
p"
T,0.18518518518518517,"T(log |Œ¶Xi| + log(n/Œ≤)). Now by The-
orem 1, we know the empirical distribution of strategy profiles played forms an Œµ-approximate
Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium, as long as T ‚â•64(log(maxi |Œ¶Xi|) + log(n/Œ≤))/Œµ2 iterations."
T,0.1865079365079365,"4
Approximate Œ¶-Equilibria under Infinite Local Strategy Modifications"
T,0.18783068783068782,"This section studies Œ¶-equilibrium when |Œ¶| is infinite. It is, in general, computationally hard to
compute a Œ¶-equilibrium even if Œ¶ contains all constant deviations. Instead, we focus on Œ¶ that
consists solely of local strategy modifications. We introduce several natural classes of local strategy
modifications and provide efficient online learning algorithms that converge to Œµ-approximate Œ¶-
equilibrium in the first-order stationary regime where Œµ = ‚Ñ¶(Œ¥2L). These approximate Œ¶-equilibria
guarantee first-order stability.
Definition 3 (Œ¥-local strategy modification). For each agent i, we call a set of strategy modifications
Œ¶Xi Œ¥-local if for all x ‚ààXi and œïi ‚ààŒ¶Xi, ‚à•œïi(x) ‚àíx‚à•‚â§Œ¥. We use notation Œ¶Xi(Œ¥) to denote a
Œ¥-local strategy modification set for agent i. We also use Œ¶(Œ¥) = Œ†n
i=1Œ¶Xi(Œ¥) to denote a profile of
Œ¥-local strategy modification sets."
T,0.18915343915343916,"Below we present a useful reduction from computing an Œµ-approximate Œ¶(Œ¥)-equilibrium in non-
concave smooth games to Œ¶Xi(Œ¥)-regret minimization against convex losses for any Œµ ‚â•Œ¥2L"
T,0.19047619047619047,"2 . The
key observation here is that the L-smoothness of the utility function permits, within a Œ¥-neighborhood,
a Œ¥2L"
T,0.1917989417989418,"2 -approximation using a linear function. We defer the proof to Appendix D.
Lemma 1 (No Œ¶(Œ¥)-Regret for Convex Losses to Approximate Œ¶(Œ¥)-Equilibrium in Non-Concave
Games). For any T ‚â•1 and Œ¥ > 0, let A be an algorithm that guarantees to achieve no more than
RegT
Œ¶Xi(Œ¥) Œ¶Xi(Œ¥)-regret for convex loss functions for each agent i ‚àà[n]. Then"
T,0.1931216931216931,"1. The Œ¶Xi(Œ¥)-regret of A for non-convex and L-smooth losses is at most RegT
Œ¶Xi(Œ¥) + Œ¥2LT"
T,0.19444444444444445,"2
, ‚àÄi."
T,0.19576719576719576,"2. When every agent employs A in a non-concave L-smooth game, their empirical distribution of the
joint strategies played converges to a (maxi‚àà[n]{RegT
Œ¶Xi (Œ¥)}/T + Œ¥2L"
T,0.1970899470899471,2 )-approximate Œ¶(Œ¥)-equilibrium.
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.1984126984126984,"4.1
Projection-Based Local Strategy Modifications"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.19973544973544974,"In this section, we study a set of local strategy modifications based on projection. Specifically, the
set Œ¶X
Proj(Œ¥) encompasses all deviations that essentially add a fixed displacement vector v to the
input strategy and project back to the feasible set: Œ¶X
Proj(Œ¥) := {œïProj,v(x) = Œ†X [x ‚àív] : v ‚àà
Bd(Œ¥)}. It is clear that ‚à•œïv(x) ‚àíx‚à•‚â§‚à•v‚à•‚â§Œ¥. The induced Œ¶X
Proj(Œ¥)-regret is RegT
Proj,Œ¥ :=
maxv‚ààBd(Œ¥)
PT
t=1 (f t(xt) ‚àíf t(Œ†X [xt ‚àív])). We also define Œ¶Proj(Œ¥) = Œ†n
i=1Œ¶Xi
Proj(Œ¥)"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20105820105820105,"By Lemma 1, to achieve convergence to an approximate Œ¶Proj(Œ¥)-equilibrium, it suffices to minimize
Œ¶X
Proj(Œ¥)-regret against convex losses. However, to our knowledge, there does not exist an algorithm
that minimize Œ¶X
Proj(Œ¥)-regret even in the convex case. In fact, external regret and Œ¶X
Proj(Œ¥)-regret are
provably incomparable: a sequence of actions may suffer high RegT but low RegT
Proj,Œ¥ (Example 3)
or vise versa (Example 4). At a high level, the external regret competes against a fixed action, whereas
Œ¶X
Proj(Œ¥)-regret is more akin to the notion of dynamic regret, competing with a sequence of varying
actions. Despite this, surprisingly, we show that classical algorithms like Online Gradient Descent
(GD) and Optimistic Gradient (OG), known for minimizing external regret, also attain near-optimal
Œ¶X
Proj(Œ¥)-regret. We defer the examples and missing proofs to Appendix E."
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20238095238095238,"Œ¶X
Proj(Œ¥)-Regret Minimization in the Adversarial Setting.
We show that (GD) enjoys an
O(G‚àöŒ¥DX T) Œ¶X
Proj(Œ¥)-regret despite the difference between the external regret and Œ¶X
Proj(Œ¥)-
regret. First, let us recall the update rule of GD: given initial point x1 ‚ààX and step size Œ∑ > 0, GD
updates in each iteration t:"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.2037037037037037,"xt+1 = Œ†X [x ‚àíŒ∑‚àáf t(xt)].
(GD)"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20502645502645503,"The key step in our analysis for GD is simple but novel and general (See Appendix E.2). We extend
the analysis to the Optimistic Gradient (OG) algorithm in Appendix E.4."
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20634920634920634,"Theorem 3. Let Œ¥ > 0 and T ‚ààN. For any convex and G-Lipschitz loss functions {f t : X ‚Üí
R}t‚àà[T ], the Œ¶X
Proj(Œ¥)-regret of (GD) with step size Œ∑ > 0 is RegT
Proj,Œ¥ ‚â§Œ¥2"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20767195767195767,2Œ∑ + Œ∑
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.20899470899470898,2G2T + Œ¥DX
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21031746031746032,Œ∑ . We
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21164021164021163,"can choose Œ∑ optimally as
‚àö"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21296296296296297,"Œ¥(Œ¥+DX ) G
‚àö"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21428571428571427,"T
and attain RegT
Proj,Œ¥ ‚â§2G
p"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.2156084656084656,"Œ¥(Œ¥ + DX )T. For any Œ¥ > 0
and any Œµ > 0, when all player employ GD in a smooth game, their empirical distribution of played
strategy profiles converges to an (Œµ + Œ¥2L"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21693121693121692,2 )-approximate Œ¶Proj(Œ¥)-equilibrium in O(1/Œµ2) iterations.
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21825396825396826,"Remark 1. The Œ¶X
Proj(Œ¥)-regret can also be viewed as the dynamic regret [Zin03] with changing"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.21957671957671956,"comparators {pt := Œ†X [x ‚àív]}. However, our analysis does not follow from standard O( (1+PT ) Œ∑
+"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.2208994708994709,"Œ∑T) dynamic regret bound of GD [Zin03] since PT , defined as PT
t=2 ‚à•pt ‚àípt‚àí1‚à•, can be ‚Ñ¶(Œ∑T)."
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.2222222222222222,"Lower bounds for Œ¶X
Proj(Œ¥)-regret. We complement our upper bound with two lower bounds for
Œ¶X
Proj(Œ¥)-regret minimization. The first one is an ‚Ñ¶(Œ¥G
‚àö"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.22354497354497355,"T) lower bound for any online learning
algorithms against linear loss functions (Theorem 7). The second one is an ‚Ñ¶(Œ¥2LT) lower bound
for any algorithm that satisfies the linear span assumption (Proposition 1), which holds for GD and
OG against L-smooth non-convex losses. Combining with Lemma 1, this lower bound suggests that
GD attains nearly optimal Œ¶X
Proj(Œ¥)-regret, even in the non-convex setting, among a natural family of
gradient-based algorithms. We defer the theorem statements and detailed discussion to Appendix E.3."
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.22486772486772486,"Improved Œ¶X
Proj(Œ¥)-Regret in the Game Setting. We complement the ‚Ñ¶(
‚àö"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.2261904761904762,"T) lower bound in the
adversarial setting by considering the game setting where players interact with each other using
the same algorithm setting, which has been extensively studied for concave games [Syr+15; CP20;
DFG21; Ana+22a; Ana+22b; Far+22a]. We prove an improved O(T
1
4 ) individual Œ¶X
Proj(Œ¥)-regret
bound for OG (Theorem 9). We defer the details to Appendix E.4"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.2275132275132275,"4.2
Convex Combination of Finite Local Strategy Modifications"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.22883597883597884,"This section considers Conv(Œ¶) where Œ¶ is a finite set of local strategy modifications. The set of
infinite strategy modifications Conv(Œ¶) is defined as Conv(Œ¶) = {œïp(x) = P"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23015873015873015,"œï‚ààŒ¶ p(œï)œï(x) : p ‚àà
‚àÜ(Œ¶)}. Our main result is an efficient algorithm (Algorithm 3) that guarantees convergence to an
Œµ-approximate Conv(Œ¶)-equilibrium in a smooth game satisfying Assumption 1 for any Œµ > Œ¥2L.
Due to space constraints, we defer Algorithm 3 and the proof to Appendix F.
Theorem 4. Let X be a convex and compact set, Œ¶ be an arbitrary finite set of Œ¥-local strategy
modification functions for X, and u1(¬∑), . . . , uT (¬∑) be a sequence of G-Lipschitz and L-smooth
but possibly non-concave reward functions from X to [0, 1]. If we instantiate Algorithm 3 with
RŒ¶ being the Hedge algorithm over ‚àÜ(Œ¶) and K =
‚àö"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23148148148148148,"T, the algorithm guarantees that, with
probability at least 1 ‚àíŒ≤, it produces a sequence of strategies x1, . . . , xT with Conv(Œ¶)-regret at
most 8
‚àö"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.2328042328042328,"T(GŒ¥
p"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23412698412698413,"log |Œ¶| +
p"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23544973544973544,"log(1/Œ≤)) + Œ¥2LT. The algorithm runs in time
‚àö"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23677248677248677,T|Œ¶| per iteration.
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23809523809523808,"If all players in a non-concave smooth game employ Algorithm 3, then with probability 1 ‚àíŒ≤, for
any Œµ > 0, the empirical distribution of strategy profiles played forms an (Œµ + Œ¥2L)-approximate"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.23941798941798942,"Œ¶ = Œ†n
i=1Œ¶Xi-equilibrium‚Äû after poly

1
Œµ, G, log
 
maxi |Œ¶Xi|

, log n"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.24074074074074073,"Œ≤

iterations."
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.24206349206349206,"4.3
Interpolation-Based Local Strategy Modifications"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.24338624338624337,"We introduce a natural set of local strategy modifications and the corresponding local equilibrium
notion. Given any set of (possibly non-local) strategy modifications Œ® = {œà : X ‚ÜíX}, we define a
set of local strategy modifications as follows: for Œ¥ ‚â§DX and Œª ‚àà[0, 1], each strategy modification
œïŒª,œà interpolates the input strategy x with the modified strategy œà(x): formally,"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.2447089947089947,"Œ¶X
Int,Œ®(Œ¥) := {œïŒª,œà(x) := (1 ‚àíŒª)x + Œªœà(x) : œà ‚ààŒ®, Œª ‚â§Œ¥/DX } ."
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.24603174603174602,"Note that for any œà ‚ààŒ® and Œª ‚â§
Œ¥
DX , we have ‚à•œïŒª,œà(x) ‚àíx‚à•= Œª‚à•x ‚àíœà(x)‚à•‚â§Œ¥, re-
specting the locality constraint. The induced Œ¶X
Int,Œ®(Œ¥)-regret can be written as RegT
Int,Œ®,Œ¥ :=
maxœà‚ààŒ®,Œª‚â§
Œ¥
DX
PT
t=1 (f t(xt) ‚àíf t((1 ‚àíŒª)xt + Œªœà(xt))). To guarantee convergence to the corre-"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.24735449735449735,"sponding Œ¶-equilibrium, it suffices to minimize Œ¶X
Int,Œ®(Œ¥)-regret against convex losses, which we
show further reduces to Œ®-regret minimization against convex losses (Theorem 10 in Appendix G)."
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.24867724867724866,"CCE-like Instantiation. In the special case where Œ® contains only constant strategy modifications
(i.e., œà(x) = x‚àófor all x), we get a coarse correlated equilibrium (CCE)-like instantiation of local
equilibrium, which limits the gain by interpolating with any fixed strategy. We denote the resulting
set of local strategy modification simply as Œ¶X
Int(Œ¥). We can apply any no-external regret algorithm
for efficient Œ¶X
Int(Œ¥)-regret minimization and computation of Œµ-approximate Œ¶Int(Œ¥)-equilibrium in
the first-order stationary regime as summarized in Theorem 5. We also discuss faster convergence
rates in the game setting in Appendix G."
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.25,"Theorem 5. For the Online Gradient Descent algorithm (GD) [Zin03] with step size Œ∑ =
DX
G
‚àö"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.25132275132275134,"T , its"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.2526455026455027,"Œ¶X
Int(Œ¥)-regret is at most 2Œ¥G
‚àö"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.25396825396825395,"T. Furthermore, for any Œ¥ > 0 and any Œµ > Œ¥2L"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.2552910052910053,"2 , when all players
employ the GD algorithm in a smooth game, their empirical distribution of played strategy profiles
converges to an (Œµ + Œ¥2L"
INTERPOLATION-BASED LOCAL STRATEGY MODIFICATIONS,0.2566137566137566,2 )-approximate Œ¶Int(Œ¥)-equilibrium in O(1/Œµ2) iterations.
DISCUSSION AND FUTURE DIRECTIONS,0.25793650793650796,"5
Discussion and Future Directions"
DISCUSSION AND FUTURE DIRECTIONS,0.25925925925925924,"Lower Bound in the Global Regime When Œ¥ equals the diameter of our strategy set, it is NP-hard to
compute an Œµ-approximate Œ¶(Œ¥)-equilibrium (for Œ¶(Œ¥) = Œ¶Proj(Œ¥), Œ¶Int(Œ¥)), even when Œµ = Œò(1)
and G, L = O(poly(d)). Moreover, given black-box access to value and gradient queries, finding
such equilibria requires exponentially many queries in at least one of the parameters d, G, L, 1/Œµ.
These results are presented as Theorem 12 and Theorem 13 in Appendix I."
DISCUSSION AND FUTURE DIRECTIONS,0.2605820105820106,"More Efficient Œ¶-Equilibria We include the discussion of another natural class of local strategy
modifications that is based on beam search, where GD suffers linear regret to Appendix H. This
result shows that even for simple local strategy modification sets Œ¶(Œ¥), the landscape of efficient local
Œ¶(Œ¥)-regret minimization is already quite rich and many questions remain open. A fruitful future
direction is to identify more classes of Œ¶ that admit efficient regret minimization."
DISCUSSION AND FUTURE DIRECTIONS,0.2619047619047619,Acknowledgements
DISCUSSION AND FUTURE DIRECTIONS,0.26322751322751325,"We thank the anonymous reviewers for their constructive comments that improves the paper. Y.C.
acknowledge the support from the NSF Awards CCF-1942583 (CAREER) and CCF-2342642.
W.Z. was supported by the NSF Awards CCF-1942583 (CAREER), CCF-2342642, and a Research
Fellowship from the Center for Algorithms, Data, and Market Design at Yale (CADMY). H.L. was
supported by NSF award IIS-1943607."
REFERENCES,0.26455026455026454,References
REFERENCES,0.26587301587301587,"[AD54]
Kenneth J Arrow and Gerard Debreu. ‚ÄúExistence of an equilibrium for a competitive
economy‚Äù. In: Econometrica (1954), pp. 265‚Äì290.
[AFS23]
Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. ‚ÄúNear-Optimal Phi-
Regret Learning in Extensive-Form Games‚Äù. In: International Conference on Machine
Learning (ICML). 2023.
[AGH19]
Naman Agarwal, Alon Gonen, and Elad Hazan. ‚ÄúLearning in non-convex games with
an optimization oracle‚Äù. In: Conference on Learning Theory. PMLR. 2019, pp. 18‚Äì29.
[AHK12]
Sanjeev Arora, Elad Hazan, and Satyen Kale. ‚ÄúThe multiplicative weights update
method: a meta-algorithm and applications‚Äù. In: Theory of computing 8.1 (2012),
pp. 121‚Äì164.
[ALW21]
Jacob Abernethy, Kevin A Lai, and Andre Wibisono. ‚ÄúLast-iterate convergence rates
for min-max optimization: Convergence of hamiltonian gradient descent and consensus
optimization‚Äù. In: Algorithmic Learning Theory. PMLR. 2021, pp. 3‚Äì47.
[Ana+22a]
Ioannis Anagnostides, Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson,
Noah Golowich, and Tuomas Sandholm. ‚ÄúNear-optimal no-regret learning for correlated
equilibria in multi-player general-sum games‚Äù. In: Proceedings of the 54th Annual
ACM SIGACT Symposium on Theory of Computing (STOC). 2022.
[Ana+22b]
Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng
Luo, and Tuomas Sandholm. ‚ÄúUncoupled Learning Dynamics with O(log T) Swap
Regret in Multiplayer Games‚Äù. In: Advances in Neural Information Processing Systems
(NeurIPS). 2022.
[Aue+02]
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. ‚ÄúThe non-
stochastic multiarmed bandit problem‚Äù. In: SIAM journal on computing 32.1 (2002),
pp. 48‚Äì77.
[AZ22]
Amir Ali Ahmadi and Jeffrey Zhang. ‚ÄúOn the complexity of finding a local minimizer
of a quadratic function over a polytope‚Äù. In: Mathematical Programming 195.1 (2022),
pp. 783‚Äì792.
[AZF19]
Sergul Aydore, Tianhao Zhu, and Dean P Foster. ‚ÄúDynamic local regret for non-convex
online forecasting‚Äù. In: Advances in neural information processing systems 32 (2019).
[Bai+22]
Yu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. ‚ÄúEfficient Phi-Regret
Minimization in Extensive-Form Games via Online Mirror Descent‚Äù. In: Advances in
Neural Information Processing Systems 35 (2022), pp. 22313‚Äì22325.
[Ber+23]
Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Francesco Trov√≤, and
Nicola Gatti. ‚ÄúConstrained Phi-Equilibria‚Äù. In: International Conference on Machine
Learning. 2023.
[Bla56]
David Blackwell. ‚ÄúAn analog of the minimax theorem for vector payoffs.‚Äù In: Pacific
Journal of Mathematics 6.1 (Jan. 1956). Publisher: Pacific Journal of Mathematics, A
Non-profit Corporation, pp. 1‚Äì8.
[Bro51]
George W Brown. ‚ÄúIterative solution of games by fictitious play‚Äù. In: Act. Anal. Prod
Allocation 13.1 (1951), p. 374.
[Bub+15]
S√©bastien Bubeck et al. ‚ÄúConvex optimization: Algorithms and complexity‚Äù. In: Foun-
dations and Trends¬Æ in Machine Learning 8.3-4 (2015), pp. 231‚Äì357.
[Cau01]
Robert Cauty. ‚ÄúSolution du probl√®me de point fixe de Schauder‚Äù. en. In: Fundamenta
Mathematicae 170 (2001). Publisher: Instytut Matematyczny Polskiej Akademii Nauk,
pp. 231‚Äì246.
[CDT09]
Xi Chen, Xiaotie Deng, and Shang-Hua Teng. ‚ÄúSettling the complexity of computing
two-player Nash equilibria‚Äù. In: Journal of the ACM (JACM) 56.3 (2009), pp. 1‚Äì57."
REFERENCES,0.2671957671957672,"[CL06]
Nicolo Cesa-Bianchi and G√°bor Lugosi. Prediction, learning, and games. Cambridge
university press, 2006.
[CP20]
Xi Chen and Binghui Peng. ‚ÄúHedging in games: Faster convergence of external and
swap regrets‚Äù. In: Advances in Neural Information Processing Systems (NeurIPS) 33
(2020), pp. 18990‚Äì18999.
[CZ23]
Yang Cai and Weiqiang Zheng. ‚ÄúAccelerated Single-Call Methods for Constrained
Min-Max Optimization‚Äù. In: International Conference on Learning Representations
(ICLR) (2023).
[Dag+24]
Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. ‚ÄúFrom
External to Swap Regret 2.0: An Efficient Reduction for Large Action Spaces‚Äù. In:
Proceedings of the 56th Annual ACM Symposium on Theory of Computing. 2024,
pp. 1216‚Äì1222.
[Das+23]
Constantinos Daskalakis, Noah Golowich, Stratis Skoulakis, and Emmanouil Zam-
petakis. ‚ÄúSTay-ON-the-Ridge: Guaranteed Convergence to Local Minimax Equilibrium
in Nonconvex-Nonconcave Games‚Äù. In: The Thirty Sixth Annual Conference on Learn-
ing Theory. PMLR. 2023, pp. 5146‚Äì5198.
[Das22]
Constantinos Daskalakis. ‚ÄúNon-Concave Games: A Challenge for Game Theory‚Äôs Next
100 Years‚Äù. In: Cowles Preprints (2022).
[DDJ21]
Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. ‚ÄúEfficient meth-
ods for structured nonconvex-nonconcave min-max optimization‚Äù. In: International
Conference on Artificial Intelligence and Statistics (2021).
[Deb52]
Gerard Debreu. ‚ÄúA social equilibrium existence theorem‚Äù. In: Proceedings of the
National Academy of Sciences 38.10 (1952), pp. 886‚Äì893.
[DFG21]
Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. ‚ÄúNear-optimal no-
regret learning in general games‚Äù. In: Advances in Neural Information Processing
Systems (NeurIPS) (2021).
[DGP09]
Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. ‚ÄúThe com-
plexity of computing a Nash equilibrium‚Äù. In: Communications of the ACM 52.2 (2009),
pp. 89‚Äì97.
[DP18]
Constantinos Daskalakis and Ioannis Panageas. ‚ÄúThe limit points of (optimistic) gra-
dient descent in min-max optimization‚Äù. In: the 32nd Annual Conference on Neural
Information Processing Systems (NeurIPS). 2018.
[DSZ21]
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. ‚ÄúThe complex-
ity of constrained min-max optimization‚Äù. In: Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing (STOC). 2021.
[Fan53]
Ky Fan. ‚ÄúMinimax theorems‚Äù. In: Proceedings of the National Academy of Sciences
39.1 (1953), pp. 42‚Äì47.
[Far+22a]
Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer,
and Tuomas Sandholm. ‚ÄúNear-optimal no-regret learning dynamics for general convex
games‚Äù. In: Advances in Neural Information Processing Systems 35 (2022), pp. 39076‚Äì
39089.
[Far+22b]
Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. ‚ÄúSimple Uncoupled
No-regret Learning Dynamics for Extensive-form Correlated Equilibrium‚Äù. In: Journal
of the ACM 69.6 (2022).
[FCR20]
Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. ‚ÄúImplicit learning dynamics in
stackelberg games: Equilibria characterization, convergence analysis, and empirical
study‚Äù. In: International Conference on Machine Learning. PMLR. 2020, pp. 3133‚Äì
3144.
[FR21]
Tanner Fiez and Lillian J Ratliff. ‚ÄúLocal convergence analysis of gradient descent ascent
with finite timescale separation‚Äù. In: Proceedings of the International Conference on
Learning Representation. 2021.
[FS99]
Yoav Freund and Robert E. Schapire. ‚ÄúAdaptive Game Playing Using Multiplicative
Weights‚Äù. In: Games and Economic Behavior 29 (1999), pp. 79‚Äì103.
[Geo63]
George B. Dantzig. Linear Programming and Extensions. Princeton University Press,
1963."
REFERENCES,0.26851851851851855,"[GGM08]
Geoffrey J Gordon, Amy Greenwald, and Casey Marks. ‚ÄúNo-regret learning in convex
games‚Äù. In: Proceedings of the 25th international conference on Machine learning.
2008, pp. 360‚Äì367.
[GJ03]
Amy Greenwald and Amir Jafari. ‚ÄúA general class of no-regret learning algorithms
and game-theoretic equilibria‚Äù. In: Learning Theory and Kernel Machines: 16th An-
nual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003,
Washington, DC, USA, August 24-27, 2003. Proceedings. Springer. 2003, pp. 2‚Äì12.
[Gli52]
Irving L Glicksberg. ‚ÄúA further generalization of the Kakutani fixed theorem, with
application to Nash equilibrium points‚Äù. In: Proceedings of the American Mathematical
Society 3.1 (1952), pp. 170‚Äì174.
[GLZ18]
Xiand Gao, Xiaobo Li, and Shuzhong Zhang. ‚ÄúOnline learning with non-convex losses
and non-stationary regret‚Äù. In: International Conference on Artificial Intelligence and
Statistics. PMLR. 2018, pp. 235‚Äì243.
[GZL23]
Ziwei Guan, Yi Zhou, and Yingbin Liang. ‚ÄúOnline Nonconvex Optimization with
Limited Instantaneous Oracle Feedback‚Äù. In: The Thirty Sixth Annual Conference on
Learning Theory. PMLR. 2023, pp. 3328‚Äì3355.
[Han57]
James Hannan. ‚ÄúApproximation to Bayes risk in repeated play‚Äù. In: Contributions to
the Theory of Games 3 (1957), pp. 97‚Äì139.
[H√©l+20]
Am√©lie H√©liou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. ‚ÄúOn-
line non-convex optimization with imperfect feedback‚Äù. In: Advances in Neural Infor-
mation Processing Systems 33 (2020), pp. 17224‚Äì17235.
[HMC21a]
Nadav Hallak, Panayotis Mertikopoulos, and Volkan Cevher. ‚ÄúRegret minimization in
stochastic non-convex learning via a proximal-gradient approach‚Äù. In: International
Conference on Machine Learning. PMLR. 2021, pp. 4008‚Äì4017.
[HMC21b]
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. ‚ÄúThe limits of min-max
optimization algorithms: Convergence to spurious non-critical sets‚Äù. In: International
Conference on Machine Learning. PMLR. 2021, pp. 4337‚Äì4348.
[HSZ17]
Elad Hazan, Karan Singh, and Cyril Zhang. ‚ÄúEfficient regret minimization in non-
convex games‚Äù. In: International Conference on Machine Learning. PMLR. 2017,
pp. 1433‚Äì1441.
[JNJ20]
Chi Jin, Praneeth Netrapalli, and Michael Jordan. ‚ÄúWhat is local optimality in
nonconvex-nonconcave minimax optimization?‚Äù In: International conference on ma-
chine learning (ICML). PMLR. 2020, pp. 4880‚Äì4889.
[Kar14]
Samuel Karlin. Mathematical Methods and Theory in Games, Programming, and
Economics: Volume 2: The Theory of Infinite Games. Elsevier, 2014.
[Kar59]
Samuel Karlin. Mathematical methods and theory in games, programming, and eco-
nomics: Volume II: the theory of infinite games. Vol. 2. Addision-Wesley, 1959.
[Kri+15]
Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. ‚ÄúThe
hedge algorithm on a continuum‚Äù. In: International Conference on Machine Learning.
PMLR. 2015, pp. 824‚Äì832.
[McK54]
Lionel McKenzie. ‚ÄúOn equilibrium in Graham‚Äôs model of world trade and other com-
petitive systems‚Äù. In: Econometrica (1954), pp. 147‚Äì161.
[MK87]
Katta G. Murty and Santosh N. Kabadi. ‚ÄúSome NP-complete problems in quadratic
and nonlinear programming‚Äù. en. In: Mathematical Programming 39.2 (June 1987),
pp. 117‚Äì129.
[MM10]
Odalric-Ambrym Maillard and R√©mi Munos. ‚ÄúOnline learning in adversarial lipschitz
environments‚Äù. In: Joint european conference on machine learning and knowledge
discovery in databases. Springer. 2010, pp. 305‚Äì320.
[Mor+21a]
Dustin Morrill, Ryan D‚ÄôOrazio, Reca Sarfati, Marc Lanctot, James R Wright, Amy R
Greenwald, and Michael Bowling. ‚ÄúHindsight and sequential rationality of correlated
play‚Äù. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. 6.
2021, pp. 5584‚Äì5594.
[Mor+21b]
Dustin Morrill, Ryan D‚ÄôOrazio, Marc Lanctot, James R Wright, Michael Bowling, and
Amy R Greenwald. ‚ÄúEfficient deviation types and learning for hindsight rationality in
extensive-form games‚Äù. In: International Conference on Machine Learning. PMLR.
2021, pp. 7818‚Äì7828."
REFERENCES,0.2698412698412698,"[MRS20]
Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. ‚ÄúOn gradient-based learning
in continuous games‚Äù. In: SIAM Journal on Mathematics of Data Science 2.1 (2020),
pp. 103‚Äì131.
[MV21]
Oren Mangoubi and Nisheeth K Vishnoi. ‚ÄúGreedy adversarial equilibrium: an efficient
alternative to nonconvex-nonconcave min-max optimization‚Äù. In: Proceedings of the
53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021, pp. 896‚Äì909.
[MZ19]
Panayotis Mertikopoulos and Zhengyuan Zhou. ‚ÄúLearning in games with continuous
action sets and unknown payoff functions‚Äù. In: Mathematical Programming 173 (2019),
pp. 465‚Äì507.
[Nas50]
John F Nash Jr. ‚ÄúEquilibrium points in n-person games‚Äù. In: Proceedings of the national
academy of sciences 36.1 (1950), pp. 48‚Äì49.
[Neu28]
J v. Neumann. ‚ÄúZur theorie der gesellschaftsspiele‚Äù. In: Mathematische annalen 100.1
(1928), pp. 295‚Äì320.
[Pet+22]
Thomas Pethick, Puya Latafat, Panagiotis Patrinos, Olivier Fercoq, and Volkan Cevher√•.
‚ÄúEscaping limit cycles: Global convergence for constrained nonconvex-nonconcave
minimax problems‚Äù. In: International Conference on Learning Representations (ICLR).
2022.
[Pil+22]
Georgios Piliouras, Mark Rowland, Shayegan Omidshafiei, Romuald Elie, Daniel
Hennes, Jerome Connor, and Karl Tuyls. ‚ÄúEvolutionary dynamics and phi-regret mini-
mization in games‚Äù. In: Journal of Artificial Intelligence Research 74 (2022), pp. 1125‚Äì
1158.
[PR24]
Binghui Peng and Aviad Rubinstein. ‚ÄúFast swap regret minimization and applications to
approximate correlated equilibria‚Äù. In: Proceedings of the 56th Annual ACM Symposium
on Theory of Computing. 2024, pp. 1223‚Äì1234.
[RBS16]
Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. ‚ÄúOn the characterization of
local Nash equilibria in continuous games‚Äù. In: IEEE transactions on automatic control
61.8 (2016), pp. 2301‚Äì2307.
[Rob51]
Julia Robinson. ‚ÄúAn iterative method of solving a game‚Äù. In: Annals of mathematics
(1951), pp. 296‚Äì301.
[Ros65]
J Ben Rosen. ‚ÄúExistence and uniqueness of equilibrium points for concave n-person
games‚Äù. In: Econometrica (1965), pp. 520‚Äì534.
[RS13]
Sasha Rakhlin and Karthik Sridharan. ‚ÄúOptimization, learning, and games with pre-
dictable sequences‚Äù. In: Advances in Neural Information Processing Systems (2013).
[RST11]
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. ‚ÄúOnline learning: Beyond
regret‚Äù. In: Proceedings of the 24th Annual Conference on Learning Theory. JMLR
Workshop and Conference Proceedings. 2011, pp. 559‚Äì594.
[Sha24]
Dravyansh Sharma. ‚ÄúNo Internal Regret with Non-convex Loss Functions‚Äù. In: Proceed-
ings of the AAAI Conference on Artificial Intelligence. Vol. 38. 13. 2024, pp. 14919‚Äì
14927.
[Sio58]
Maurice Sion. ‚ÄúOn general minimax theorems.‚Äù In: Pacific J. Math. 8.4 (1958), pp. 171‚Äì
176.
[SL07]
Gilles Stoltz and G√°bor Lugosi. ‚ÄúLearning correlated equilibria in games with compact
sets of strategies‚Äù. In: Games and Economic Behavior 59.1 (2007), pp. 187‚Äì208.
[SMB22]
Ziang Song, Song Mei, and Yu Bai. ‚ÄúSample-efficient learning of correlated equilibria
in extensive-form games‚Äù. In: Advances in Neural Information Processing Systems 35
(2022), pp. 4099‚Äì4110.
[SN20]
Arun Sai Suggala and Praneeth Netrapalli. ‚ÄúOnline non-convex learning: Following the
perturbed leader is optimal‚Äù. In: Algorithmic Learning Theory. PMLR. 2020, pp. 845‚Äì
861.
[Syr+15]
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. ‚ÄúFast conver-
gence of regularized learning in games‚Äù. In: Advances in Neural Information Processing
Systems (NeurIPS) (2015).
[VF08]
Bernhard Von Stengel and Fran√ßoise Forges. ‚ÄúExtensive-form correlated equilibrium:
Definition and computational complexity‚Äù. In: Mathematics of Operations Research
33.4 (2008), pp. 1002‚Äì1022."
REFERENCES,0.27116402116402116,"[WZB20]
Yuanhao Wang, Guodong Zhang, and Jimmy Ba. ‚ÄúOn Solving Minimax Optimization
Locally: A Follow-the-Ridge Approach‚Äù. In: International Conference on Learning
Representations (ICLR). 2020.
[Zha+24]
Brian Hu Zhang, Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. ‚ÄúEffi-
cient Œ¶-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form
Games‚Äù. In: Advances in Neural Information Processing Systems. 2024.
[Zin03]
Martin Zinkevich. ‚ÄúOnline convex programming and generalized infinitesimal gradient
ascent‚Äù. In: Proceedings of the 20th international conference on machine learning
(ICML). 2003."
REFERENCES,0.2724867724867725,Contents
INTRODUCTION,0.27380952380952384,"1
Introduction
1"
INTRODUCTION,0.2751322751322751,"1.1
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2"
PRELIMINARIES,0.27645502645502645,"2
Preliminaries
5"
PRELIMINARIES,0.2777777777777778,"3
Tractable Œ¶-Equilibrium for Finite Œ¶ via Sampling
5"
PRELIMINARIES,0.2791005291005291,"3.1
Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7"
PRELIMINARIES,0.2804232804232804,"4
Approximate Œ¶-Equilibria under Infinite Local Strategy Modifications
8"
PROJECTION-BASED LOCAL STRATEGY MODIFICATIONS,0.28174603174603174,"4.1
Projection-Based Local Strategy Modifications
. . . . . . . . . . . . . . . . . . .
9"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.2830687830687831,"4.2
Convex Combination of Finite Local Strategy Modifications
. . . . . . . . . . . .
10"
CONVEX COMBINATION OF FINITE LOCAL STRATEGY MODIFICATIONS,0.2843915343915344,"4.3
Interpolation-Based Local Strategy Modifications . . . . . . . . . . . . . . . . . .
10"
DISCUSSION AND FUTURE DIRECTIONS,0.2857142857142857,"5
Discussion and Future Directions
10"
DISCUSSION AND FUTURE DIRECTIONS,0.28703703703703703,"A Related Work
16"
DISCUSSION AND FUTURE DIRECTIONS,0.28835978835978837,"B Additional Preliminaries: Solution Concepts in Non-Concave Games
17"
DISCUSSION AND FUTURE DIRECTIONS,0.2896825396825397,"C Rerget Bound for Hedge
19"
DISCUSSION AND FUTURE DIRECTIONS,0.291005291005291,"D Proof of Lemma 1
19"
DISCUSSION AND FUTURE DIRECTIONS,0.2923280423280423,"E
Missing Details in Section 4.1
19"
DISCUSSION AND FUTURE DIRECTIONS,0.29365079365079366,"E.1
Differences between External Regret and Œ¶X
Proj-regret . . . . . . . . . . . . . . . .
19"
DISCUSSION AND FUTURE DIRECTIONS,0.294973544973545,"E.2
Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
DISCUSSION AND FUTURE DIRECTIONS,0.2962962962962963,"E.3
Lower bounds for Œ¶X
Proj-Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
DISCUSSION AND FUTURE DIRECTIONS,0.2976190476190476,"E.3.1
Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21"
DISCUSSION AND FUTURE DIRECTIONS,0.29894179894179895,"E.3.2
Proof of Proposition 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
DISCUSSION AND FUTURE DIRECTIONS,0.3002645502645503,"E.4
Improved Œ¶X
Proj(Œ¥)-Regret in the Game Setting and Proof of Theorem 9 . . . . . .
22"
DISCUSSION AND FUTURE DIRECTIONS,0.30158730158730157,"E.4.1
Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
DISCUSSION AND FUTURE DIRECTIONS,0.3029100529100529,"E.4.2
Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
DISCUSSION AND FUTURE DIRECTIONS,0.30423280423280424,"F
Missing Details in Section 4.2
24"
DISCUSSION AND FUTURE DIRECTIONS,0.3055555555555556,"F.1
Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
DISCUSSION AND FUTURE DIRECTIONS,0.30687830687830686,"G Missing details in Section 4.3
26"
DISCUSSION AND FUTURE DIRECTIONS,0.3082010582010582,"H Beam-Search Local Strategy Modifications and Local Equilibria
27"
DISCUSSION AND FUTURE DIRECTIONS,0.30952380952380953,"H.1
Proof of Theorem 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28"
DISCUSSION AND FUTURE DIRECTIONS,0.31084656084656087,"I
Hardness in the Global Regime
28"
DISCUSSION AND FUTURE DIRECTIONS,0.31216931216931215,"I.2
Proof of Corollary 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
DISCUSSION AND FUTURE DIRECTIONS,0.3134920634920635,"J
Removing the D dependence for Œ¶X
Proj-regret
30"
DISCUSSION AND FUTURE DIRECTIONS,0.3148148148148148,"J.1
One-Dimensional Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
DISCUSSION AND FUTURE DIRECTIONS,0.31613756613756616,"J.2
d-Dimensional Box Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31"
DISCUSSION AND FUTURE DIRECTIONS,0.31746031746031744,"A
Related Work"
DISCUSSION AND FUTURE DIRECTIONS,0.3187830687830688,"Non-Concave Games.
An important special case of multi-player games are two-player zero-sum
games, which are defined in terms of some function f : X √ó Y ‚ÜíR that one of the two players say
the one choosing x ‚ààX, wants to minimize, while the other player, the one choosing y ‚ààY, wants
to maximize. Finding Nash equilibrium in such games is tractable in the convex-concave setting,
i.e. when f(x, y) is convex with respect to the minimizing player‚Äôs strategy, x, and concave with
respect to the maximizing player‚Äôs strategy, y, but it is computationally intractable in the general
nonconvex-nonconcave setting. Namely, a Nash equilibrium may not exist, and it is NP-hard to
determine if one exists and, if so, find it. Moreover, in this case, stable limit points of gradient-
based dynamics are not necessarily Nash equilibria, not even local Nash equilibria [DP18; MRS20].
Moreover, there are examples including the ‚ÄúPolar Game‚Äù [Pet+22] and the ‚ÄúForsaken Matching
Pennies‚Äù [HMC21b] showing that for GD / OG and many other no-regret learning algorithms in
nonconvex-nonconcave min-max optimization, the last-iterate does not converge and even the average-
iterate fails to be a stationary point. We emphasize that the convergence guarantees we provide
for GD / OG in Section 4.1 and Section 4.3 holds for the empirical distribution of play, not the
average-iterate or the last-iterate."
DISCUSSION AND FUTURE DIRECTIONS,0.3201058201058201,"A line of work focuses on computing Nash equilibrium under additional structure in the game.
This encompasses settings where the game satisfies the (weak) Minty variational inequality [MZ19;
DDJ21; Pet+22; CZ23], or is sufficiently close to being bilinear [ALW21]. However, the study of
universal solution concepts in the nonconvex-nonconcave setting is sparse. Daskalakis, Skoulakis,
and Zampetakis [DSZ21] proved the existence and computational hardness of local Nash equilibrium.
In a more recent work, [Das+23] proposes second-order algorithms with asymptotic convergence to
local Nash equilibrium. Several works study sequential two-player zero-sum games with additional
assumptions about the player who goes second. They propose equilibrium concepts such as local
minimax points [JNJ20], differentiable Stackelberg equilibrium [FCR20], and greedy adversarial
equilibrium [MV21]. Notably, local minimax points are stable limit points of Gradient-Descent-
Ascent (GDA) dynamics [JNJ20; WZB20; FR21] while greedy adversarial equilibrium can be
computed efficiently using second-order algorithms in the unconstrained setting [MV21]. In contrast
to these studies, we focus on the more general case of multi-player non-concave games."
DISCUSSION AND FUTURE DIRECTIONS,0.32142857142857145,"Local Equilibrium.
To address the limitations associated with classical, global equilibrium con-
cepts, a natural approach is to focus on developing equilibrium concepts that guarantee local stability
instead. One definition of interest is the strict local Nash equilibrium, wherein each player‚Äôs strat-
egy corresponds to a local maximizer of their utility function, given the other players‚Äô strategies.
Unfortunately, a strict local Nash equilibrium may not always exist, as demonstrated in Example 1.
Furthermore, a weaker notion‚Äîthe second-order local Nash equilibrium, where each player has no
incentive to deviate based on the second-order Taylor expansion estimate of their utility, is also not
guaranteed to exist as illustrated in Example 1. What‚Äôs more, it is NP-hard to check whether a given
strategy profile is a strict local Nash equilibrium or a second-order local Nash equilibrium, as implied
by the result of Murty and Kabadi [MK87] and Ahmadi and Zhang [AZ22].6 Finally, one can consider
local Nash equilibrium, a first-order stationary solution, which is guaranteed to exist [DSZ21]. Unlike
non-convex optimization, where targeting first-order local optima sidesteps the intractability of global
optima, this first-order local Nash equilibrium has been recently shown to be intractable, even in"
DISCUSSION AND FUTURE DIRECTIONS,0.32275132275132273,"two-player zero-sum non-concave games with joint feasibility constraints [DSZ21].7 See Table 1 for
a summary of solution concepts in non-concave games."
DISCUSSION AND FUTURE DIRECTIONS,0.32407407407407407,"Online Learning with Non-Convex Losses.
A line of work has studied online learning against
non-convex losses. To circumvent the computational intractability of this problem, various approaches
have been pursued: some works assume a restricted set of non-convex loss functions [GLZ18], while
others assume access to a sampling oracle [MM10; Kri+15] or access to an offline optimization
oracle [AGH19; SN20; H√©l+20] or a weaker notion of regret [HSZ17; AZF19; HMC21a; GZL23].
The work most closely related to ours is [HSZ17]. The authors propose a notion of w-smoothed local
regret against non-convex losses, and they also define a local equilibrium concept for non-concave
games. They use the idea of smoothing to average the loss functions in the previous w iterations
and design algorithms with optimal w-smoothed local regret. The concept of regret they introduce
suggests a local equilibrium concept. However, their local equilibrium concept is non-standard in
that its local stability is not with respect to a distribution over strategy profiles sampled by this
equilibrium concept. Moreover, the path to attaining this local equilibrium through decentralized
learning dynamics remains unclear. The algorithms provided in [HSZ17; GZL23] require that every
agent i experiences (over several rounds) the average utility function of the previous w iterates,
denoted as F t
i,w := 1"
DISCUSSION AND FUTURE DIRECTIONS,0.3253968253968254,"w
Pw‚àí1
‚Ñì=0 ut‚àí‚Ñì
i
(¬∑, xt‚àí‚Ñì
‚àíi ). Implementing this imposes a significant coordination
burden on the agents. In contrast, we focus on a natural concept of Œ¶(Œ¥)-equilibrium, which is
incomparable to that of [HSZ17], and we also show that efficient convergence to this concept is
achieved via decentralized gradient-based learning dynamics."
DISCUSSION AND FUTURE DIRECTIONS,0.32671957671957674,"Œ¶-regret and Œ¶-equilibrium.
The concept of Œ¶-regret and the associated Œ¶-equilibrium is in-
troduced by Greenwald and Jafari [GJ03] and has been broadly investigated in the context of
concave games [GJ03; SL07; GGM08; RST11; Pil+22; Ber+23; Dag+24; PR24] and extensive-
form games [VF08; Mor+21a; Mor+21b; Far+22b; Bai+22; SMB22; AFS23; Zha+24]. The work
of [Sha24] studies internal regret minimization against non-convex losses. To our knowledge, no
efficient algorithm exists for the classes of Œ¶-equilibria we consider for non-concave games. Specifi-
cally, all of these algorithms, when applied to compute a (Œµ, Œ¶(Œ¥))-equilibrium for a general Œ¥-local
strategy modification set Œ¶(Œ¥) (using Lemma 1), require running time exponential in either 1/Œµ
or the dimension d. In contrast, we show that for several natural choices of Œ¶(Œ¥), Œµ-approximate
Œ¶(Œ¥)-equilibrium can be computed efficiently, i.e. polynomial in 1/Œµ and d, using simple algorithms."
DISCUSSION AND FUTURE DIRECTIONS,0.328042328042328,"B
Additional Preliminaries: Solution Concepts in Non-Concave Games"
DISCUSSION AND FUTURE DIRECTIONS,0.32936507936507936,"We present definitions of several solution concepts in the literature as well as the existence and
computational complexity of each solution concept.
Definition 4 (Nash Equilibrium). In a continuous game, a strategy profile x ‚ààQn
j=1 Xj is a Nash
equilibrium (NE) if and only if for every player i ‚àà[n],"
DISCUSSION AND FUTURE DIRECTIONS,0.3306878306878307,"ui(x‚Ä≤
i, x‚àíi) ‚â§ui(x), ‚àÄx‚Ä≤
i ‚ààXi"
DISCUSSION AND FUTURE DIRECTIONS,0.33201058201058203,"Definition 5 (Mixed Nash Equilibrium). In a continuous game, a mixed strategy profile p ‚àà
Qn
j=1 ‚àÜ(Xj) (here we denote ‚àÜ(Xi) as the set of probability measures over Xi) is a mixed Nash
equilibrium (MNE) if and only if for every player i ‚àà[n],"
DISCUSSION AND FUTURE DIRECTIONS,0.3333333333333333,"ui(p‚Ä≤
i, p‚àíi) ‚â§ui(p), ‚àÄp‚Ä≤
i ‚àà‚àÜ(Xi)"
DISCUSSION AND FUTURE DIRECTIONS,0.33465608465608465,"Definition 6 ((Coarse) Correlated Equilibrium). In a continuous game, a distribution œÉ over joint
strategy profiles Œ†n
i=1Xi is a correlated equilibrium (CE) if and only if for all player i ‚àà[n],"
DISCUSSION AND FUTURE DIRECTIONS,0.335978835978836,"max
œïi:Xi‚ÜíXi Ex‚àºœÉ[ui(œïi(xi), x‚àíi)] ‚â§Ex‚àºœÉ[ui(x)]."
DISCUSSION AND FUTURE DIRECTIONS,0.3373015873015873,"Similarly, a distribution œÉ over joint strategy profiles Œ†n
i=1Xi is a coarse correlated equilibrium
(CCE) if and only if for all player i ‚àà[n],"
DISCUSSION AND FUTURE DIRECTIONS,0.3386243386243386,"max
x‚Ä≤
i‚ààXi Ex‚àºœÉ[ui(x‚Ä≤
i, x‚àíi)] ‚â§Ex‚àºœÉ[ui(x)]."
DISCUSSION AND FUTURE DIRECTIONS,0.33994708994708994,"7In general sum games, it is not hard to see that the intractability results [DGP09; CDT09] for computing
global Nash equilibria in bimatrix games imply intractability for computing local Nash equilibria."
DISCUSSION AND FUTURE DIRECTIONS,0.3412698412698413,"Definition 7 (Strict Local Nash Equilibrium). In a continuous game, a strategy profile x ‚ààQn
j=1 Xj
is a strict local Nash equilibrium if and only if for every player i ‚àà[n], there exists Œ¥ > 0 such that"
DISCUSSION AND FUTURE DIRECTIONS,0.3425925925925926,"ui(x‚Ä≤
i, x‚àíi) ‚â§ui(x), ‚àÄx‚Ä≤
i ‚ààBdi(xi, Œ¥) ‚à©Xi."
DISCUSSION AND FUTURE DIRECTIONS,0.3439153439153439,"Definition 8 (Second-order Local Nash Equilibrium). Consider a continuous game where each utility
function ui(xi, x‚àíi) is twice-differentiable with respect to xi for any fixed x‚àíi. A strategy profile
x ‚ààQn
j=1 Xj is a second-order local Nash equilibrium if and only if for every player i ‚àà[n], xi
maximizes the second-order Taylor expansion of its utility functions at xi, or formally,"
DISCUSSION AND FUTURE DIRECTIONS,0.34523809523809523,"‚ü®‚àáxiui(x), x‚Ä≤
i ‚àíxi‚ü©+ (x‚Ä≤
i ‚àíxi)‚ä§‚àá2
xiui(x)(x‚Ä≤
i ‚àíxi) ‚â§0, ‚àÄx‚Ä≤
i ‚ààXi."
DISCUSSION AND FUTURE DIRECTIONS,0.34656084656084657,"Existence
Mixed Nash equilibria exist in continuous games, thus smooth games [Deb52; Gli52;
Fan53]. By definition, an MNE is also a CE and a CCE. This also proves the existence of CE and
CCE. In contrast, strict local Nash equilibria, second-order Nash equilibria, or (pure) Nash equilibria
may not exist in a smooth non-concave game, as we show in the following example.
Example 1. Consider a two-player zero-sum non-concave game: the action sets are X1 = X2 =
[‚àí1, 1] and the utility functions are u1(x1, x2) = ‚àíu2(x1, x2) = (x1 ‚àíx2)2. Let x = (x1, x2) ‚àà
X1√óX2 be any strategy profile: if x1 = x2, then player 1 is not at a local maximizer; if x1 Ã∏= x2, then
player 2 is not at a local maximizer. Thus x is not a strict local Nash equilibrium. Since the utility
function is quadratic, we conclude that the game also has no second-order local Nash equilibrium."
DISCUSSION AND FUTURE DIRECTIONS,0.3478835978835979,"Computational Complexity
Consider a single-player smooth non-concave game with a quadratic
utility function f : X ‚ÜíR. The problem of finding a local maximizer of f can be reduced to the
problem of computing a NE, a MNE, a CE, a CCE, a strict local Nash equilibrium, or a second-order
local Nash equilibrium. Since computing a local maximizer or checking if a given point is a local
maximizer is NP-hard [MK87], we know that the computational complexities of NE, MNE, CE, CCE,
strict local Nash equilibria, and second-order local Nash equilibria are all NP-hard."
DISCUSSION AND FUTURE DIRECTIONS,0.3492063492063492,"Representation Complexity
Karlin [Kar59] present a two-player zero-sum non-concave game
whose unique MNE has infinite support. Since in a two-player zero-sum game, the marginal
distribution of a CE or a CCE is an MNE, it also implies that the representation complexity of any CE
or CCE is infinite. We present the example in Karlin [Kar59] here for completeness and also prove
that the game is Lipschitz and smooth.
Example 2 ([Kar59, Chapter 7.1, Example 3]). We consider a two-player zero-sum game with action
sets X1 = X2 = [0, 1]. Let p and q be two distributions over [0, 1]. The only requirement for p and q
is that their cumulative distribution functions are not finite-step functions. For example, we can take
p = q to be the uniform distribution."
DISCUSSION AND FUTURE DIRECTIONS,0.3505291005291005,"Let ¬µn and ŒΩn denote the n-th moments of p and q, respectively. Define the utility function"
DISCUSSION AND FUTURE DIRECTIONS,0.35185185185185186,"u(x, y) = u1(x, y) = ‚àíu2(x, y) = ‚àû
X n=0"
DISCUSSION AND FUTURE DIRECTIONS,0.3531746031746032,"1
2n (xn ‚àí¬µn)(yn ‚àíŒΩn),
0 ‚â§x, y ‚â§1."
DISCUSSION AND FUTURE DIRECTIONS,0.3544973544973545,"Claim 1. The game in Example 2 is 2-Lipschitz and 6-smooth, and (p, q) is its unique (mixed) Nash
equilibrium."
DISCUSSION AND FUTURE DIRECTIONS,0.3558201058201058,"Proof. Fix any y ‚àà[0, 1], since | 1"
DISCUSSION AND FUTURE DIRECTIONS,0.35714285714285715,"2n (yn ‚àíŒΩn)nxn‚àí1| ‚â§
n
2n , the series of ‚àáxu(x, y) is uniformly
convergent. We have |‚àáxu(x, y)| ‚â§P‚àû
n=0
n
2n ‚â§2,
y ‚àà[0, 1]. Similarly, we have |‚àá2
xu(x, y)| ‚â§
P‚àû
n=0
n2
2n ‚â§6 for all y ‚àà[0, 1]. By symmetry, we also have |‚àáy(x, y)| ‚â§2 and |‚àá2
y(x, y)| ‚â§6 for
all x, y ‚àà[0, 1]. Thus, the game is 2-Lispchitz and 6-smooth."
DISCUSSION AND FUTURE DIRECTIONS,0.3584656084656085,Since | 1
DISCUSSION AND FUTURE DIRECTIONS,0.35978835978835977,"2n (xn ‚àí¬µn)(yn ‚àíŒΩn)| ‚â§
1
2n , the series of u(x, y) is absolutely and uniformly convergent.
We have
Z 1"
DISCUSSION AND FUTURE DIRECTIONS,0.3611111111111111,"0
u(x, y)dFp(x) = ‚àû
X n=0"
DISCUSSION AND FUTURE DIRECTIONS,0.36243386243386244,"1
2n (yn ‚àíŒΩn)
Z 1"
DISCUSSION AND FUTURE DIRECTIONS,0.3637566137566138,"0
(xn ‚àí¬µn)dFp(x) ‚â°0, Z 1"
DISCUSSION AND FUTURE DIRECTIONS,0.36507936507936506,"0
u(x, y)Fq(y) = ‚àû
X n=0"
DISCUSSION AND FUTURE DIRECTIONS,0.3664021164021164,"1
2n (xn ‚àí¬µn)
Z 1"
DISCUSSION AND FUTURE DIRECTIONS,0.36772486772486773,"0
(yn ‚àíŒΩn)dFq(y) ‚â°0."
DISCUSSION AND FUTURE DIRECTIONS,0.36904761904761907,"In particular, (p, q) is a mixed Nash equilibrium, and the value of the game is 0. Suppose (p‚Ä≤, q‚Ä≤) is
also a mixed Nash equilibrium. Then (p, q‚Ä≤) is a mixed Nash equilibrium. Note that p supports on
every point in [0, 1]. As a consequence, we have"
DISCUSSION AND FUTURE DIRECTIONS,0.37037037037037035,"0 ‚â°
Z 1"
DISCUSSION AND FUTURE DIRECTIONS,0.3716931216931217,"0
u(x, y)dFq‚Ä≤(y) = ‚àû
X n=0"
DISCUSSION AND FUTURE DIRECTIONS,0.373015873015873,"1
2n (xn ‚àí¬µn)(ŒΩ‚Ä≤
n ‚àíŒΩn)"
DISCUSSION AND FUTURE DIRECTIONS,0.37433862433862436,"for all x ‚àà[0, 1], where ŒΩ‚Ä≤
n is the n-th moment of q‚Ä≤. Since the series vanished identically, the
coefficients of each power of x must vanish. Thus ŒΩ‚Ä≤
n = ŒΩn and q‚Ä≤ = q. Similarly, we have p‚Ä≤ = p,
and the mixed Nash equilibrium is unique."
DISCUSSION AND FUTURE DIRECTIONS,0.37566137566137564,"C
Rerget Bound for Hedge"
DISCUSSION AND FUTURE DIRECTIONS,0.376984126984127,"Theorem 6 ([AHK12]). In an N-expert problem, assume all the rewards are bounded, i.e., ut ‚àà
[‚àíM, M]N, then the Hedge algorithm with step size Œ∑ = min{ 1"
DISCUSSION AND FUTURE DIRECTIONS,0.3783068783068783,"M ,
‚àölog N M
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.37962962962962965,T } has regret
DISCUSSION AND FUTURE DIRECTIONS,0.38095238095238093,"max
p‚àà‚àÜ(N) T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.38227513227513227,"ut, p

‚àí T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.3835978835978836,"ut, pt
‚â§2M
p"
DISCUSSION AND FUTURE DIRECTIONS,0.38492063492063494,T log N.
DISCUSSION AND FUTURE DIRECTIONS,0.3862433862433862,"D
Proof of Lemma 1"
DISCUSSION AND FUTURE DIRECTIONS,0.38756613756613756,"Let {f t}t‚àà[T ] be a sequence of non-convex L-smooth loss functions satisfying Assumption 1. Let
{xt}t‚àà[T ] be the iterates produced by A against {f t}t‚àà[T ]. Then {xt}t‚àà[T ] is also the iterates
produced by A against a sequence of linear loss functions {‚ü®‚àáf t(xt), ¬∑‚ü©}. For the latter, we know"
DISCUSSION AND FUTURE DIRECTIONS,0.3888888888888889,"max
œï‚ààŒ¶X (Œ¥) T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.39021164021164023,"‚àáf t(xt), xt ‚àíœï(xt)

‚â§RegT
Œ¶X (Œ¥)."
DISCUSSION AND FUTURE DIRECTIONS,0.3915343915343915,"Then using L-smoothness of {f t} and the fact that ‚à•œï(x) ‚àíx‚à•‚â§Œ¥ for all œï ‚ààŒ¶(Œ¥), we get"
DISCUSSION AND FUTURE DIRECTIONS,0.39285714285714285,"max
œï‚ààŒ¶X (Œ¥) T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.3941798941798942,"t=1
f t(xt) ‚àíf t(œï(xt)) ‚â§
max
œï‚ààŒ¶X (Œ¥) T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.3955026455026455,"
‚àáf t(xt), xt ‚àíœï(xt)

+ L 2"
DISCUSSION AND FUTURE DIRECTIONS,0.3968253968253968,"xt ‚àíœï(xt)
2
"
DISCUSSION AND FUTURE DIRECTIONS,0.39814814814814814,"‚â§RegT
Œ¶X (Œ¥) + Œ¥2LT 2
."
DISCUSSION AND FUTURE DIRECTIONS,0.3994708994708995,This completes the proof of the first part.
DISCUSSION AND FUTURE DIRECTIONS,0.4007936507936508,"Let each player i ‚àà[n] employ algorithm A in a smooth game independently and produces iterates
{xt}. The averaged joint strategy profile œÉT that chooses xt uniformly at random from t ‚àà[T]
satisfies for any player i ‚àà[n],
max
œï‚ààŒ¶Xi(Œ¥) Ex‚àºœÉ[ui(œï(xi), x‚àíi)] ‚àíEx‚àºœÉ[ui(x)]"
DISCUSSION AND FUTURE DIRECTIONS,0.4021164021164021,"=
max
œï‚ààŒ¶Xi(Œ¥)
1
T T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.40343915343915343," 
ui(œï(xt
i), xt
‚àíi) ‚àíui(xt)
"
DISCUSSION AND FUTURE DIRECTIONS,0.40476190476190477,"‚â§
RegT
Œ¶Xi(Œ¥)
T
+ Œ¥2L 2 ."
DISCUSSION AND FUTURE DIRECTIONS,0.4060846560846561,"Thus œÉT is a (maxi‚àà[n]{RegT
Œ¶Xi(Œ¥)} ¬∑ T ‚àí1 + Œ¥2L"
DISCUSSION AND FUTURE DIRECTIONS,0.4074074074074074,"2 )-approximate Œ¶(Œ¥))-equilibrium. This completes
the proof of the second part."
DISCUSSION AND FUTURE DIRECTIONS,0.4087301587301587,"E
Missing Details in Section 4.1"
DISCUSSION AND FUTURE DIRECTIONS,0.41005291005291006,"E.1
Differences between External Regret and Œ¶X
Proj-regret"
DISCUSSION AND FUTURE DIRECTIONS,0.4113756613756614,"In the following two examples, we show that Œ¶X
Proj(Œ¥)-regret is incomparable with external regret for
convex loss functions . A sequence of actions may suffer high RegT but low RegT
Proj,Œ¥ (Example 3),
and vise versa (Example 4)."
DISCUSSION AND FUTURE DIRECTIONS,0.4126984126984127,"Example 3. Let f 1(x) = f 2(x) = |x| for x ‚ààX = [‚àí1, 1]. Then the Œ¶X
Proj(Œ¥)-regret of the
sequence {x1 = 1"
DISCUSSION AND FUTURE DIRECTIONS,0.414021164021164,"2, x2 = ‚àí1"
DISCUSSION AND FUTURE DIRECTIONS,0.41534391534391535,"2} for any Œ¥ ‚àà(0, 1"
DISCUSSION AND FUTURE DIRECTIONS,0.4166666666666667,"2) is 0. However, the external regret of the same
sequence is 1. By repeating the construction for T"
DISCUSSION AND FUTURE DIRECTIONS,0.41798941798941797,"2 times, we conclude that there exists a sequence of
actions with RegT
Proj,Œ¥ = 0 and RegT = T"
DISCUSSION AND FUTURE DIRECTIONS,0.4193121693121693,2 for all T ‚â•2.
DISCUSSION AND FUTURE DIRECTIONS,0.42063492063492064,"Example 4. Let f 1(x) = ‚àí2x and f 2(x) = x for x ‚ààX = [‚àí1, 1]. Then the Œ¶X
Proj(Œ¥)-regret of
the sequence {x1 = 1"
DISCUSSION AND FUTURE DIRECTIONS,0.421957671957672,"2, x2 = 0} for any Œ¥ ‚àà(0, 1"
DISCUSSION AND FUTURE DIRECTIONS,0.42328042328042326,"2) is Œ¥. However, the external regret of the same
sequence is 0. By repeating the construction for T"
DISCUSSION AND FUTURE DIRECTIONS,0.4246031746031746,"2 times, we conclude that there exists a sequence of
actions with RegT
Proj,Œ¥ = Œ¥T"
DISCUSSION AND FUTURE DIRECTIONS,0.42592592592592593,2 and RegT = 0 for all T ‚â•2.
DISCUSSION AND FUTURE DIRECTIONS,0.42724867724867727,"At a high level, the external regret competes against a fixed action, whereas Œ¶X
Proj(Œ¥)-regret is more
akin to the notion of dynamic regret, competing with a sequence of varying actions. When the
environment is stationary, i.e., f t = f (Example 3), a sequence of actions that are far away from the
global minimum must suffer high regret, but may produce low Œ¶X
Proj(Œ¥)-regret since the change to
the cumulative loss caused by a fixed-direction deviation could be neutralized across different actions
in the sequence. In contrast, in a non-stationary (dynamic) environment (Example 4), every fixed
action performs poorly, and a sequence of actions could suffer low regret against a fixed action but the
Œ¶X
Proj(Œ¥)-regret that competes with a fixed-direction deviation could be large. Nevertheless, despite
these differences between the two notions of regret as shown above, they are compatible for convex
loss functions: our main results in this section provide algorithms that minimize external regret and
Œ¶X
Proj(Œ¥)-regret simultaneously."
DISCUSSION AND FUTURE DIRECTIONS,0.42857142857142855,"E.2
Proof of Theorem 3"
DISCUSSION AND FUTURE DIRECTIONS,0.4298941798941799,"Proof. Let us denote v ‚ààBd(Œ¥) a fixed deviation and define pt = Œ†X [xt ‚àív]. By standard analysis
of GD [Zin03] (see also the proof of [Bub+15, Theorem 3.2] ), we have T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.4312169312169312," 
f t(xt) ‚àíf t(pt)

‚â§ T
X t=1 1
2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.43253968253968256,"xt ‚àípt2 ‚àí
xt+1 ‚àípt2 + Œ∑2‚àáf t(xt)
2 ‚â§"
DISCUSSION AND FUTURE DIRECTIONS,0.43386243386243384,"T ‚àí1
X t=1 1
2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.4351851851851852,"xt+1 ‚àípt+12 ‚àí
xt+1 ‚àípt2
+ Œ¥2"
DISCUSSION AND FUTURE DIRECTIONS,0.4365079365079365,"2Œ∑ + Œ∑ 2G2T,"
DISCUSSION AND FUTURE DIRECTIONS,0.43783068783068785,"where the last step uses ‚à•x1 ‚àíp1‚à•‚â§Œ¥ and ‚à•‚àáf t(xt)‚à•‚â§G. Here the terms ‚à•xt+1 ‚àípt+1‚à•
2 ‚àí
‚à•xt+1 ‚àípt‚à•
2 do not telescope, and we further relax them in the following key step."
DISCUSSION AND FUTURE DIRECTIONS,0.43915343915343913,"Key Step:
We relax the first term as:
xt+1 ‚àípt+12 ‚àí
xt+1 ‚àípt2 =

pt ‚àípt+1, 2xt+1 ‚àípt ‚àípt+1"
DISCUSSION AND FUTURE DIRECTIONS,0.44047619047619047,"=

pt ‚àípt+1, 2xt+1 ‚àí2pt+1
‚àí
pt ‚àípt+12"
DISCUSSION AND FUTURE DIRECTIONS,0.4417989417989418,"= 2

pt ‚àípt+1, v

+ 2

pt ‚àípt+1, xt+1 ‚àív ‚àípt+1
‚àí
pt ‚àípt+12"
DISCUSSION AND FUTURE DIRECTIONS,0.44312169312169314,"‚â§2

pt ‚àípt+1, v

‚àí
pt ‚àípt+12,"
DISCUSSION AND FUTURE DIRECTIONS,0.4444444444444444,where in the last inequality we use the fact that pt+1 is the projection of xt+1 ‚àív onto X and pt
DISCUSSION AND FUTURE DIRECTIONS,0.44576719576719576,"is in X. Now we get a telescoping term 2‚ü®pt ‚àípt+1, u‚ü©and a negative term ‚àí‚à•pt ‚àípt+1‚à•
2. The
negative term is useful for improving the regret analysis in the game setting, but we ignore it for now.
Combining the two inequalities above, we have T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.4470899470899471," 
f t(xt) ‚àíf t(pt)

‚â§Œ¥2"
DISCUSSION AND FUTURE DIRECTIONS,0.44841269841269843,2Œ∑ + Œ∑
DISCUSSION AND FUTURE DIRECTIONS,0.4497354497354497,2G2T + 1 Œ∑
DISCUSSION AND FUTURE DIRECTIONS,0.45105820105820105,"T ‚àí1
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.4523809523809524,"pt ‚àípt+1, v = Œ¥2"
DISCUSSION AND FUTURE DIRECTIONS,0.4537037037037037,2Œ∑ + Œ∑
DISCUSSION AND FUTURE DIRECTIONS,0.455026455026455,2G2T + 1
DISCUSSION AND FUTURE DIRECTIONS,0.45634920634920634,"Œ∑

p1 ‚àípT , v

‚â§Œ¥2"
DISCUSSION AND FUTURE DIRECTIONS,0.4576719576719577,2Œ∑ + Œ∑
DISCUSSION AND FUTURE DIRECTIONS,0.458994708994709,"2G2T + Œ¥DX Œ∑
."
DISCUSSION AND FUTURE DIRECTIONS,0.4603174603174603,"Since the above holds for any v with ‚à•v‚à•‚â§Œ¥, it also upper bounds RegT
Proj,Œ¥."
DISCUSSION AND FUTURE DIRECTIONS,0.46164021164021163,"E.3
Lower bounds for Œ¶X
Proj-Regret"
DISCUSSION AND FUTURE DIRECTIONS,0.46296296296296297,"Theorem 7 (Lower bound for Œ¶X
Proj(Œ¥)-regret against convex losses). For any T ‚â•1, DX > 0,
0 < Œ¥ ‚â§DX , and G ‚â•0, there exists a distribution D on G-Lipschitz linear loss functions
f 1, . . . , f T over X = [‚àíDX , DX ] such that for any online algorithm, its Œ¶X
Proj(Œ¥)-regret on the loss
sequence satisfies ED[RegT
Proj,Œ¥] = ‚Ñ¶(Œ¥G
‚àö T)."
DISCUSSION AND FUTURE DIRECTIONS,0.4642857142857143,"Remark 2. A keen reader may notice that the ‚Ñ¶(GŒ¥
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.4656084656084656,"T) lower bound in Theorem 7 does not match
the O(G‚àöŒ¥DX T) upper bound in Theorem 3, especially when DX ‚â´Œ¥. A natural question is:
which of them is tight? We conjecture that the lower bound is tight. In fact, for the special case
where the feasible set X is a box, we obtain a DX -independent bound O(d
1
4 GŒ¥
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.4669312169312169,"T) using a modified
version of GD, which is tight when d = 1. See Appendix J for a detailed discussion."
DISCUSSION AND FUTURE DIRECTIONS,0.46825396825396826,"This lower bound suggests that GD achieves near-optimal Œ¶X
Proj(Œ¥)-regret for convex losses. For
L-smooth non-convex loss functions, we provide another ‚Ñ¶(Œ¥2LT) lower bound for algorithms that
satisfy the linear span assumption. The linear span assumption states that the algorithm produces
xt+1 ‚àà{Œ†X [P"
DISCUSSION AND FUTURE DIRECTIONS,0.4695767195767196,"i‚àà[t] ai¬∑xi+bi¬∑‚àáf i(xi)] : ai, bi ‚ààR, ‚àÄi ‚àà[t]} as essentially the linear combination
of the previous iterates and their gradients. Many online algorithms such as online gradient descent
and optimistic gradient satisfy the linear span assumption. Combining with Lemma 1, this lower
bound suggests that GD attains nearly optimal Œ¶X
Proj(Œ¥)-regret, even in the non-convex setting, among
a natural family of gradient-based algorithms."
DISCUSSION AND FUTURE DIRECTIONS,0.4708994708994709,"Proposition 1 (Lower bound for Œ¶X
Proj(Œ¥)-regret against non-convex losses). For any T ‚â•1,
Œ¥ ‚àà(0, 1), and L ‚â•0, there exists a sequence of L-Lipschitz and L-smooth non-convex loss functions
f 1, . . . , f T on X = [‚àí1, 1] such that for any algorithm that satisfies the linear span assumption, its
Œ¶X
Proj(Œ¥)-regret on the loss sequence is RegT
Proj,Œ¥ ‚â•Œ¥2LT 2
."
DISCUSSION AND FUTURE DIRECTIONS,0.4722222222222222,"E.3.1
Proof of Theorem 7"
DISCUSSION AND FUTURE DIRECTIONS,0.47354497354497355,"Our proof technique comes from the standard one used in multi-armed bandits [Aue+02, Theorem
5.1]. Suppose that f t(x) = gtx. We construct two possible environments. In the first environment,
gt = G with probability 1+Œµ"
DISCUSSION AND FUTURE DIRECTIONS,0.4748677248677249,"2
and gt = ‚àíG with probability 1‚àíŒµ"
DISCUSSION AND FUTURE DIRECTIONS,0.47619047619047616,"2 ; in the second environment, gt = G
with probability 1‚àíŒµ"
DISCUSSION AND FUTURE DIRECTIONS,0.4775132275132275,"2
and gt = ‚àíG with probability 1+Œµ"
DISCUSSION AND FUTURE DIRECTIONS,0.47883597883597884,"2 . We use Ei and Pi to denote the expectation
and probability measure under environment i, respectively, for i = 1, 2. Suppose that the true
environment is uniformly chosen from one of these two environments. Below, we show that the
expected regret of the learner is at least ‚Ñ¶(Œ¥G
‚àö T)."
DISCUSSION AND FUTURE DIRECTIONS,0.4801587301587302,"Define N+ = PT
t=1 I{xt ‚â•0} be the number of times xt is non-negative, and define f 1:T =
(f 1, . . . , f T ). Then we have"
DISCUSSION AND FUTURE DIRECTIONS,0.48148148148148145,|E1[N+] ‚àíE2[N+]| =  X f 1:T
DISCUSSION AND FUTURE DIRECTIONS,0.4828042328042328,"
P1(f 1:T )E

N+ | f 1:T 
‚àíP2(f 1:T )E

N+ | f 1:T  "
DISCUSSION AND FUTURE DIRECTIONS,0.48412698412698413,"(enumerate all possible sequences of f 1:T ) ‚â§T
X f 1:T"
DISCUSSION AND FUTURE DIRECTIONS,0.48544973544973546,P1(f 1:T ) ‚àíP2(f 1:T )
DISCUSSION AND FUTURE DIRECTIONS,0.48677248677248675,"= T‚à•P1 ‚àíP2‚à•TV ‚â§T
p"
DISCUSSION AND FUTURE DIRECTIONS,0.4880952380952381,"(2 ln 2)KL(P1, P2)
(Pinsker‚Äôs inequality) = T s"
DISCUSSION AND FUTURE DIRECTIONS,0.4894179894179894,"(2 ln 2)T ¬∑ KL

Bernoulli
1 + Œµ 2"
DISCUSSION AND FUTURE DIRECTIONS,0.49074074074074076,"
, Bernoulli
1 ‚àíŒµ 2  = T r"
DISCUSSION AND FUTURE DIRECTIONS,0.49206349206349204,(2 ln 2)TŒµ ln 1 + Œµ
DISCUSSION AND FUTURE DIRECTIONS,0.4933862433862434,"1 ‚àíŒµ ‚â§T
p"
DISCUSSION AND FUTURE DIRECTIONS,0.4947089947089947,"(4 ln 2)TŒµ2.
(2)"
DISCUSSION AND FUTURE DIRECTIONS,0.49603174603174605,"In the first environment, we consider the regret with respect to v = Œ¥. Then we have"
DISCUSSION AND FUTURE DIRECTIONS,0.4973544973544973,"E1

RegT
Proj,Œ¥

‚â•E1 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.49867724867724866,"t=1
f t(xt) ‚àíf t(Œ†X [xt ‚àíŒ¥]) # = E1 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5,"t=1
gt(xt ‚àíŒ†X [xt ‚àíŒ¥]) # = E1 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5013227513227513,"t=1
ŒµG(xt ‚àíŒ†X [xt ‚àíŒ¥]) #"
DISCUSSION AND FUTURE DIRECTIONS,0.5026455026455027,"‚â•ŒµŒ¥GE1 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.503968253968254,"t=1
I{xt ‚â•0} #"
DISCUSSION AND FUTURE DIRECTIONS,0.5052910052910053,"= ŒµŒ¥GE1 [N+] ,"
DISCUSSION AND FUTURE DIRECTIONS,0.5066137566137566,"where in the last inequality we use the fact that if xt ‚â•0 then xt ‚àíŒ†X [xt ‚àíŒ¥] = xt ‚àí(xt ‚àíŒ¥) = Œ¥
because D ‚â•Œ¥. In the second environment, we consider the regret with respect to v = ‚àíŒ¥. Then
similarly, we have"
DISCUSSION AND FUTURE DIRECTIONS,0.5079365079365079,"E2

RegT
Proj,Œ¥

‚â•E2 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5092592592592593,"t=1
f t(xt) ‚àíf t(Œ†X [xt + Œ¥]) # = E2 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5105820105820106,"t=1
gt(xt ‚àíŒ†X [xt + Œ¥]) # = E2 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5119047619047619,"t=1
‚àíŒµG(xt ‚àíŒ†X [xt + Œ¥]) #"
DISCUSSION AND FUTURE DIRECTIONS,0.5132275132275133,"‚â•ŒµŒ¥GE2 "" T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5145502645502645,"t=1
I{xt < 0} #"
DISCUSSION AND FUTURE DIRECTIONS,0.5158730158730159,= ŒµŒ¥G (T ‚àíE2 [N+]) .
DISCUSSION AND FUTURE DIRECTIONS,0.5171957671957672,"Summing up the two inequalities, we get
1
2
 
E1

RegT
Proj,Œ¥

+ E2

RegT
Proj,Œ¥

‚â•1"
DISCUSSION AND FUTURE DIRECTIONS,0.5185185185185185,2 (ŒµŒ¥GT + ŒµŒ¥G(E1[N+] ‚àíE2[N+])) ‚â•1 2
DISCUSSION AND FUTURE DIRECTIONS,0.5198412698412699,"
ŒµŒ¥GT ‚àíŒµŒ¥GTŒµ
p"
DISCUSSION AND FUTURE DIRECTIONS,0.5211640211640212,"(4 ln 2)T

.
(by (2))"
DISCUSSION AND FUTURE DIRECTIONS,0.5224867724867724,"Choosing Œµ =
1
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.5238095238095238,"(16 ln 2)T , we can lower bound the last expression by ‚Ñ¶(Œ¥G
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.5251322751322751,T). The theorem is
DISCUSSION AND FUTURE DIRECTIONS,0.5264550264550265,proven by noticing that 1
DISCUSSION AND FUTURE DIRECTIONS,0.5277777777777778,"2
 
E1

RegT
Proj,Œ¥

+ E2

RegT
Proj,Œ¥

is the expected regret of the learner."
DISCUSSION AND FUTURE DIRECTIONS,0.5291005291005291,"E.3.2
Proof of Proposition 1"
DISCUSSION AND FUTURE DIRECTIONS,0.5304232804232805,"Proof. Consider f : [‚àí1, 1] ‚ÜíR such that f(x) = ‚àíL"
DISCUSSION AND FUTURE DIRECTIONS,0.5317460317460317,"2 x2 and let f t = f for all t ‚àà[T]. Then any
first-order methods that satisfy the linear span assumption with initial point x1 = 0 will produce
xt = 0 for all t ‚àà[T]. The Œ¶X
Proj(Œ¥)-regret is thus PT
t=1(f(0) ‚àíf(Œ¥)) = Œ¥2LT 2
."
DISCUSSION AND FUTURE DIRECTIONS,0.533068783068783,"E.4
Improved Œ¶X
Proj(Œ¥)-Regret in the Game Setting and Proof of Theorem 9"
DISCUSSION AND FUTURE DIRECTIONS,0.5343915343915344,"Any online algorithm suffers an ‚Ñ¶(
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.5357142857142857,"T) Œ¶X
Proj(Œ¥)-regret even against linear loss functions by The-
orem 7. This lower bound, however, holds only in the adversarial setting. In this section, we
show an improved O(T
1
4 ) individual Œ¶X
Proj(Œ¥)-regret bound under a slightly stronger smoothness
assumption (Assumption 2) in the game setting, where players interact with each other using the
same algorithm, previous results show improved external regret [Syr+15; CP20; DFG21; Ana+22a;
Ana+22b; Far+22a]. This assumption is naturally satisfied by finite normal-form games and is also
made for results about concave games [Far+22a].
Assumption 2. For any player i ‚àà[n], the utility ui(x) satisfies ‚à•‚àáxiui(x) ‚àí‚àáxiui(x‚Ä≤)‚à•‚â§
L‚à•x ‚àíx‚Ä≤‚à•for all x, x‚Ä≤ ‚ààX."
DISCUSSION AND FUTURE DIRECTIONS,0.5370370370370371,"We study the Optimistic Gradient (OG) algorithm [RS13], an optimistic variant of GD that has been
shown to have improved individual external regret guarantee in the game setting [Syr+15]. The OG
algorithm initializes w0 ‚ààX arbitrarily and g0 = 0. In each step t ‚â•1, the algorithm plays xt,
receives feedback gt := ‚àáf t(xt), and updates wt, as follows:"
DISCUSSION AND FUTURE DIRECTIONS,0.5383597883597884,"xt = Œ†X

wt‚àí1 ‚àíŒ∑gt‚àí1
,
wt = Œ†X

wt‚àí1 ‚àíŒ∑gt
.
(OG)"
DISCUSSION AND FUTURE DIRECTIONS,0.5396825396825397,"We show that OG has O(
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.541005291005291,"T) Œ¶X
Proj(Œ¥)-regret in the adversarial setting and fast O(T 1/4) Œ¶X
Proj(Œ¥)-
regret and convergence to approximate Œ¶X
Proj(Œ¥)-equilibrium in games.
Theorem 8 (Adversarial Regret Bound for OG). Let Œ¥ > 0 and T ‚ààN. For convex and G-Lipschitz
loss functions {f t : X ‚ÜíR}t‚àà[T ], the Œ¶X
Proj(Œ¥)-regret of (OG) with step size Œ∑ > 0 is"
DISCUSSION AND FUTURE DIRECTIONS,0.5423280423280423,"RegT
Proj,Œ¥ ‚â§Œ¥DX Œ∑
+ Œ∑ T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.5436507936507936,"gt ‚àígt‚àí12."
DISCUSSION AND FUTURE DIRECTIONS,0.544973544973545,"Choosing step size Œ∑ =
‚àöŒ¥DX
2G
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.5462962962962963,"T , we have RegT
Proj,Œ¥ ‚â§4G‚àöŒ¥DX T."
DISCUSSION AND FUTURE DIRECTIONS,0.5476190476190477,"Theorem 9 (Improved Individual Œ¶X
Proj(Œ¥)-Regret of OG in the Game Setting). In a G-Lipschitz
L-smooth (in the sense of Assumption 2) game, when all players employ OG with step size Œ∑ > 0,
then for each player i, Œ¥ > 0, and T ‚â•1, their individual Œ¶Xi
Proj(Œ¥)-regret denoted as RegT,i
Proj,Œ¥
is RegT,i
Proj,Œ¥ ‚â§Œ¥D"
DISCUSSION AND FUTURE DIRECTIONS,0.548941798941799,"Œ∑ + Œ∑G2 + 3nL2G2Œ∑3T. Choosing Œ∑ = min{(Œ¥D/(nL2G2T))
1
4 , (Œ¥D)
1
2 /G}, we"
DISCUSSION AND FUTURE DIRECTIONS,0.5502645502645502,"have RegT,i
Proj,Œ¥ ‚â§4(Œ¥D)
3
4 (nL2G2T)
1
4 + 2
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.5515873015873016,"Œ¥DG. Furthermore, for any Œ¥ > 0 and any Œµ > 0,"
DISCUSSION AND FUTURE DIRECTIONS,0.5529100529100529,their empirical distribution of played strategy profiles converges to an (Œµ + Œ¥2L
DISCUSSION AND FUTURE DIRECTIONS,0.5542328042328042,"2 )-approximate
Œ¶Proj(Œ¥)-equilibrium in O(1/Œµ
4
3 ) iterations."
DISCUSSION AND FUTURE DIRECTIONS,0.5555555555555556,"E.4.1
Proof of Theorem 8"
DISCUSSION AND FUTURE DIRECTIONS,0.5568783068783069,"Proof. Fix any deviation v that is bounded by Œ¥. Let us define p0 = w0 and pt = Œ†X [xt ‚àív].
Following standard analysis of OG [RS13], we have T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5582010582010583,"t=1
f t(xt) ‚àíf t(pt) ‚â§ T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.5595238095238095,"‚àáf t(xt), xt ‚àípt ‚â§ T
X t=1 1
2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.5608465608465608,"wt‚àí1 ‚àípt2 ‚àí
wt ‚àípt2
+ Œ∑
gt ‚àígt‚àí12 ‚àí1 2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.5621693121693122,"xt ‚àíwt2 +
xt ‚àíwt‚àí12 ‚â§ T
X t=1  1 2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.5634920634920635,"wt‚àí1 ‚àípt2 ‚àí1 2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.5648148148148148,"wt‚àí1 ‚àípt‚àí12 + Œ∑
gt ‚àígt‚àí12 ‚àí1 2Œ∑"
DISCUSSION AND FUTURE DIRECTIONS,0.5661375661375662,"xt ‚àíwt‚àí12
(3)"
DISCUSSION AND FUTURE DIRECTIONS,0.5674603174603174,"Now we apply a similar analysis from Theorem 3 to upper bound the term ‚à•wt‚àí1 ‚àípt‚à•
2 ‚àí
‚à•wt‚àí1 ‚àípt‚àí1‚à•
2:
wt‚àí1 ‚àípt2 ‚àí
wt‚àí1 ‚àípt‚àí12"
DISCUSSION AND FUTURE DIRECTIONS,0.5687830687830688,"=

pt‚àí1 ‚àípt, 2wt‚àí1 ‚àípt‚àí1 ‚àípt"
DISCUSSION AND FUTURE DIRECTIONS,0.5701058201058201,"=

pt‚àí1 ‚àípt, 2wt‚àí1 ‚àí2pt
‚àí
pt ‚àípt‚àí12"
DISCUSSION AND FUTURE DIRECTIONS,0.5714285714285714,"= 2

pt‚àí1 ‚àípt, v

+ 2

pt‚àí1 ‚àípt, wt‚àí1 ‚àív ‚àípt
‚àí
pt ‚àípt‚àí12"
DISCUSSION AND FUTURE DIRECTIONS,0.5727513227513228,"= 2

pt‚àí1 ‚àípt, v

+ 2

pt‚àí1 ‚àípt, xt ‚àív ‚àípt
+ 2

pt‚àí1 ‚àípt, wt‚àí1 ‚àíxt
‚àí
pt ‚àípt‚àí12"
DISCUSSION AND FUTURE DIRECTIONS,0.5740740740740741,"‚â§2

pt‚àí1 ‚àípt, v

+
xt ‚àíwt‚àí12,"
DISCUSSION AND FUTURE DIRECTIONS,0.5753968253968254,"where in the last-inequality we use ‚ü®pt‚àí1 ‚àípt, xt ‚àíu ‚àípt‚ü©‚â§0 since pt = Œ†X [xt ‚àív] and X is a
compact convex set; we also use 2‚ü®a, b‚ü©‚àíb2 ‚â§a2. In the analysis above, unlike the analysis of GD
where we drop the negative term ‚àí‚à•pt ‚àípt‚àí1‚à•
2, we use ‚àí‚à•pt ‚àípt‚àí1‚à•
2 to get a term ‚à•xt ‚àíwt‚àí1‚à•
2"
DISCUSSION AND FUTURE DIRECTIONS,0.5767195767195767,which can be canceled by the last term in (3).
DISCUSSION AND FUTURE DIRECTIONS,0.578042328042328,"Now we combine the above two inequalities.
Since the term ‚à•xt ‚àíwt‚àí1‚à•
2 cancels out and
2‚ü®pt‚àí1 ‚àípt, v‚ü©telescopes, we get T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5793650793650794,"t=1
f t(xt) ‚àíf t(pt) ‚â§‚ü®p0 ‚àípT , u‚ü© Œ∑
+ T
X"
DISCUSSION AND FUTURE DIRECTIONS,0.5806878306878307,"t=1
Œ∑
gt ‚àígt‚àí12 ‚â§Œ¥DX Œ∑
+ Œ∑ T
X t=1"
DISCUSSION AND FUTURE DIRECTIONS,0.582010582010582,"gt ‚àígt‚àí12."
DISCUSSION AND FUTURE DIRECTIONS,0.5833333333333334,"E.4.2
Proof of Theorem 9"
DISCUSSION AND FUTURE DIRECTIONS,0.5846560846560847,"In the analysis of Theorem 8 for the adversarial setting, the term ‚à•gt ‚àígt‚àí1‚à•
2 can be as large as 4G2.
In the game setting where every player i employs OG, gt
i ,i.e., ‚àí‚àáxiui(x), depends on other players‚Äô
action xt
‚àíi. Note that the change of the players‚Äô actions ‚à•xt ‚àíxt‚àí1‚à•
2 is only O(Œ∑2). Such stability"
DISCUSSION AND FUTURE DIRECTIONS,0.5859788359788359,"of the updates leads to an improved upper bound on ‚à•gt
i ‚àígt‚àí1
i
‚à•
2 and hence also an improved O(T
1
4 )
Œ¶X
Proj(Œ¥)-regret for the player."
DISCUSSION AND FUTURE DIRECTIONS,0.5873015873015873,"Proof. Let us fix any player i ‚àà[n] in the smooth game. In every step t, player i‚Äôs loss function
f t : Xi ‚ÜíR is ‚ü®‚àí‚àáxiui(xt), ¬∑‚ü©determined by their utility function ui and all players‚Äô actions xt.
Therefore, their gradient feedback is gt = ‚àí‚àáxiui(xt). For all t ‚â•2, we have
gt ‚àígt‚àí12 =
‚àáui(xt) ‚àí‚àáui(xt‚àí1)
2"
DISCUSSION AND FUTURE DIRECTIONS,0.5886243386243386,"‚â§L2xt ‚àíxt‚àí12"
DISCUSSION AND FUTURE DIRECTIONS,0.58994708994709,"= L2
n
X i=1"
DISCUSSION AND FUTURE DIRECTIONS,0.5912698412698413,"xt
i ‚àíxt‚àí1
i
2"
DISCUSSION AND FUTURE DIRECTIONS,0.5925925925925926,"‚â§3L2
n
X i=1"
DISCUSSION AND FUTURE DIRECTIONS,0.593915343915344,"xt
i ‚àíwt
i
2 +
wt
i ‚àíwt‚àí1
i
2 +
wt‚àí1
i
‚àíxt‚àí1
i
2"
DISCUSSION AND FUTURE DIRECTIONS,0.5952380952380952,"‚â§3nL2Œ∑2G2,
where we use L-smoothness of the utility function ui in the first inequality; we use the update rule of
OG and the fact that gradients are bounded by G in the last inequality."
DISCUSSION AND FUTURE DIRECTIONS,0.5965608465608465,"Applying the above inequality to the regret bound obtained in Theorem 8, the individual Œ¶X
Proj(Œ¥)-
regret of player i is upper bounded by"
DISCUSSION AND FUTURE DIRECTIONS,0.5978835978835979,"RegT,i
Proj,Œ¥ ‚â§Œ¥D"
DISCUSSION AND FUTURE DIRECTIONS,0.5992063492063492,Œ∑ + Œ∑G2 + 3nL2G2Œ∑3T.
DISCUSSION AND FUTURE DIRECTIONS,0.6005291005291006,"Choosing Œ∑ = min{(Œ¥D/(nL2G2T))
1
4 , (Œ¥D)
1
2 /G}, we have RegT,i
Proj,Œ¥ ‚â§4(Œ¥D)
3
4 (nL2G2T)
1
4 +
2
‚àö"
DISCUSSION AND FUTURE DIRECTIONS,0.6018518518518519,"Œ¥DG. Using Lemma 1, we have the empirical distribution of played strategy profiles converge to
an (Œµ + Œ¥2L"
DISCUSSION AND FUTURE DIRECTIONS,0.6031746031746031,"2 )-approximate Œ¶Proj(Œ¥))-equilibrium in O(1/Œµ
4
3 ) iterations."
DISCUSSION AND FUTURE DIRECTIONS,0.6044973544973545,"F
Missing Details in Section 4.2"
DISCUSSION AND FUTURE DIRECTIONS,0.6058201058201058,"Algorithm 3: Conv(Œ¶)-regret minimization for Lipschitz smooth non-concave rewards
Input: x1 ‚ààX, K ‚â•2, a no-external-regret algorithm RŒ¶ against linear reward over ‚àÜ(Œ¶)
Output: A Conv(Œ¶)-regret minimization algorithm over X"
DISCUSSION AND FUTURE DIRECTIONS,0.6071428571428571,1 function NEXTSTRATEGY()
DISCUSSION AND FUTURE DIRECTIONS,0.6084656084656085,"2
pt ‚ÜêRŒ¶.NEXTSTRATEGY(). Note that pt is a distribution over Œ¶."
DISCUSSION AND FUTURE DIRECTIONS,0.6097883597883598,"3
xk ‚Üêœïpt(xk‚àí1), for all 2 ‚â§k ‚â§K"
DISCUSSION AND FUTURE DIRECTIONS,0.6111111111111112,"4
return xt ‚Üêuniformly at random from {x1, . . . , xK}."
DISCUSSION AND FUTURE DIRECTIONS,0.6124338624338624,5 function OBSERVEREWARD(‚àáxut(xt))
UT,0.6137566137566137,"6
ut
Œ¶(¬∑) ‚Üêa linear reward over ‚àÜ(Œ¶) with ut
Œ¶(œï) = ‚ü®‚àáxut(xt), œï(xt) ‚àíxt‚ü©for all œï ‚ààŒ¶."
UT,0.6150793650793651,"7
RŒ¶.OBSERVEREWARD(ut
Œ¶(¬∑))."
UT,0.6164021164021164,"F.1
Proof of Theorem 4"
UT,0.6177248677248677,"Proof Sketch. We adopt the framework in [SL07; GGM08] (as described in Section 3) with two
main modifications. First, we utilize the L-smoothness of the utilities to transform the problem of
external regret over ‚àÜ(Œ¶) against non-concave rewards into a linear optimization problem. Second,
we use the technique of ‚Äúfixed point in expectation"" [Zha+24] to circumvent the intractable problem
of finding a fixed point."
UT,0.6190476190476191,"Proof. For a sequence of strategies {xt}t‚àà[T ], its Conv(Œ¶)-regret is"
UT,0.6203703703703703,"RegT
Conv(Œ¶) =
max
œï‚ààConv(Œ¶) ( T
X t=1"
UT,0.6216931216931217," 
ut(œï(xt)) ‚àíut(xt)

)"
UT,0.623015873015873,"= max
p‚àà‚àÜ(Œ¶) ( T
X"
UT,0.6243386243386243,"t=1
ut(œïp(xt)) ‚àíut(œïpt(xt)) )"
UT,0.6256613756613757,"|
{z
}
I: external regret over ‚àÜ(Œ¶) + T
X"
UT,0.626984126984127,"t=1
ut(œïpt(xt)) ‚àíut(xt)"
UT,0.6283068783068783,"|
{z
}
II: approximation error of fixed point ."
UT,0.6296296296296297,"Bounding External Regret over ‚àÜ(Œ¶)
We can define a new reward function f t(p) := ut(œïp(xt))
over p ‚àà‚àÜ(Œ¶). Since ut is non-concave, the reward f t is also non-concave and it is computational
intractable to minimize external regret. We use locality to avoid computational barrier. Here we use
the fact that Œ¶ = Œ¶(Œ¥) contains only Œ¥-local strategy modifications. Then by L-smoothness of ut, we
know for any p ‚àà‚àÜ(Œ¶)
ut(œïp(xt) ‚àíut(xt) ‚àí

‚àáut(xt), œïp(xt) ‚àíxt
)
 ‚â§L 2"
UT,0.6309523809523809,"œïp(xt) ‚àíxt2 ‚â§Œ¥2L 2 ."
UT,0.6322751322751323,"Thus we can approximate the non-concave optimization problem by a linear optimization problem
over ‚àÜ(Œ¶) with only second-order error Œ¥2L"
UT,0.6335978835978836,"2 . Here we use the notation a = b ¬± c to mean b ‚àíc ‚â§
a ‚â§b + c."
UT,0.6349206349206349,"ut(œïp(xt) ‚àíut(xt) =

‚àáut(xt), œïp(xt) ‚àíxt
¬± Œ¥2L 2 = *"
UT,0.6362433862433863,"‚àáut(xt),
X"
UT,0.6375661375661376,"œï‚ààŒ¶
p(œï)œï(xt) ‚àíxt
+ ¬± Œ¥2L 2 =
X"
UT,0.6388888888888888,"œï‚ààŒ¶
p(œï)

‚àáut(xt), œï(xt) ‚àíxt
¬± Œ¥2L 2 ."
UT,0.6402116402116402,"We can then instantiate the external regret RŒ¶ as the Hedge algorithm over reward f t(p) =
P"
UT,0.6415343915343915,"œï‚ààŒ¶ p(œï)‚ü®‚àáut(xt), œï(xt) ‚àíxt‚ü©and get"
UT,0.6428571428571429,"max
p‚àà‚àÜ(Œ¶) ( T
X"
UT,0.6441798941798942,"t=1
ut(œïp(xt)) ‚àíut(œïpt(xt)) )"
UT,0.6455026455026455,"‚â§max
p‚àà‚àÜ(Œ¶) Ô£±
Ô£≤ Ô£≥ T
X t=1 X"
UT,0.6468253968253969,"œï‚ààŒ¶
(p(œï) ‚àípt(œï))

‚àáut(xt), œï(xt) ‚àíxt
) Ô£º
Ô£Ω"
UT,0.6481481481481481,Ô£æ+ Œ¥2LT
UT,0.6494708994708994,"‚â§2GŒ¥
p"
UT,0.6507936507936508,"T log |Œ¶| + Œ¥2LT,"
UT,0.6521164021164021,"where we use the fact that ‚ü®‚àáut(xt), œï(xt) ‚àíxt‚ü©‚â§‚à•‚àáut(xt)‚à•¬∑ ‚à•œï(xt) ‚àíxt‚à•‚â§GŒ¥."
UT,0.6534391534391535,"Bounding error due to sampling from a fixed point in expectation
We choose x1 as an arbitrary
point in X. Then we recursively apply œïpt to get"
UT,0.6547619047619048,"xk = œïpt(xk‚àí1) =
X"
UT,0.656084656084656,"œï‚ààŒ¶
pt(œï)œï(xk‚àí1), ‚àÄ2 ‚â§k ‚â§K."
UT,0.6574074074074074,"We denote ¬µt = Uniform{xk : 1 ‚â§k ‚â§K}. Then the strategy xt ‚àº¬µt is sampled from ¬µt. We
have that ¬µt is an approximate fixed-point in expectation / stationary distribution in the sense that"
UT,0.6587301587301587,"E¬µt
ut(œïpt(xt)) ‚àíut(xt)

= 1 K K
X"
UT,0.66005291005291,"k=1
ut(œïpt(xk) ‚àíut(xk)) = 1"
UT,0.6613756613756614,"K
 
ut(œïpt(xK)) ‚àíut(x1)
 ‚â§1 K ."
UT,0.6626984126984127,"Thanks to the boundedness of ut, we can use Hoeffding-Azuma‚Äôs inequality to conclude that Pr "" T
X t=1"
UT,0.6640211640211641,"
ut(œïpt(xt)) ‚àíut(xt) ‚àí1 K 
‚â•œµ #"
UT,0.6653439153439153,"‚â§exp

‚àíŒµ2"
T,0.6666666666666666,8T
T,0.667989417989418,"
.
(4)"
T,0.6693121693121693,"for any Œµ > 0. Combining the above with Œµ =
p"
T,0.6706349206349206,"8T log(1/Œ≤) and K =
‚àö"
T,0.671957671957672,"T, we get with probability
at least 1 ‚àíŒ≤,"
T,0.6732804232804233,"RegT
Conv(Œ¶) ‚â§2GŒ¥
p"
T,0.6746031746031746,"T log |Œ¶| + Œ¥2LT +
‚àö T +
p"
T,0.6759259259259259,"8T log(1/Œ≤) ‚â§8
‚àö"
T,0.6772486772486772,"T

GŒ¥
p"
T,0.6785714285714286,"log |Œ¶| +
p"
T,0.6798941798941799,"log(1/Œ≤)

+ Œ¥2LT."
T,0.6812169312169312,"Convergence to Œ¶-equilibrium
If all players in a non-concave continuous game employ Algo-
rithm 1, then we know for each player i, with probability 1 ‚àíŒ≤"
T,0.6825396825396826,"n, its Œ¶Xi-regret is upper bounded
by 8
‚àö"
T,0.6838624338624338,"T

GŒ¥
q"
T,0.6851851851851852,"log |Œ¶Xi| +
p"
T,0.6865079365079365,"log(n/Œ≤)

+ Œ¥2LT."
T,0.6878306878306878,"By a union bound over all n players, we get with probability 1 ‚àíŒ≤, every player i‚Äôs Œ¶Xi-regret is
upper bounded by 8
‚àö"
T,0.6891534391534392,"T(GŒ¥
p"
T,0.6904761904761905,"log |Œ¶+Xi| +
p"
T,0.6917989417989417,"log(n/Œ≤)) + Œ¥2LT. Now by Theorem 1, we know the
empirical distribution of strategy profiles played forms an (Œµ + Œ¥2L)-approximate Œ¶ = Œ†n
i=1Œ¶Xi-
equilibrium, as long as T ‚â•128(G2Œ¥2 log |Œ¶Xi|+log(n/Œ≤))"
T,0.6931216931216931,"Œµ2
iterations."
T,0.6944444444444444,"G
Missing details in Section 4.3"
T,0.6957671957671958,"We introduce a natural set of local strategy modifications and the corresponding local equilibrium
notion. Given any set of (possibly non-local) strategy modifications Œ® = {œà : X ‚ÜíX}, we define a
set of local strategy modifications as follows: for Œ¥ ‚â§DX and Œª ‚àà[0, 1], each strategy modification
œïŒª,œà interpolates the input strategy x with the modified strategy œà(x): formally,"
T,0.6970899470899471,"Œ¶X
Int,Œ®(Œ¥) := {œïŒª,œà(x) := (1 ‚àíŒª)x + Œªœà(x) : œà ‚ààŒ®, Œª ‚â§Œ¥/DX } ."
T,0.6984126984126984,"Note that for any œà ‚ààŒ® and Œª ‚â§
Œ¥
DX , we have ‚à•œïŒª,œà(x) ‚àíx‚à•= Œª‚à•x ‚àíœà(x)‚à•‚â§Œ¥, re-
specting the locality constraint. The induced Œ¶X
Int,Œ®(Œ¥)-regret can be written as RegT
Int,Œ®,Œ¥ :=
maxœà‚ààŒ®,Œª‚â§
Œ¥
DX
PT
t=1 (f t(xt) ‚àíf t((1 ‚àíŒª)xt + Œªœà(xt))). We now define the corresponding"
T,0.6997354497354498,"Œ¶Int,Œ®(Œ¥)-equilibrium."
T,0.701058201058201,"Definition 9. Define Œ¶Int,Œ®(Œ¥) = Œ†n
j=1Œ¶Xj
Int,Œ®j(Œ¥). In a continuous game, a distribution œÉ over
strategy profiles is an (Œµ-approximate Œ¶Int,Œ®(Œ¥)-equilibrium if and only if for all player i ‚àà[n],"
T,0.7023809523809523,"max
œà‚ààŒ®i,Œª‚â§Œ¥/DXi
Ex‚àºœÉ[ui((1 ‚àíŒª)xi + Œªœà(xi), x‚àíi)] ‚â§Ex‚àºœÉ[ui(x)] + Œµ."
T,0.7037037037037037,"Intuitively speaking, when a correlation device recommends strategies to players according to an
Œµ-approximate Œ¶Int,Œ®(Œ¥)-equilibrium, no player can increase their utility by more than Œµ through a
local deviation by interpolating with a (possibly global) strategy modification œà ‚ààŒ®. The richness of
Œ® determines the incentive guarantee provided by an Œµ-approximate Œ¶Int,Œ®(Œ¥)-equilibriumas well as
its computational complexity. When we choose Œ® to be the set of all possible strategy modifications,
the corresponding notion of local equilibrium‚Äîlimiting the gain of a player by interpolating with any
strategy‚Äîresembles that of a correlated equilibrium."
T,0.705026455026455,"Computation of Œµ-approximate Œ¶Int,Œ®(Œ¥)-Equilibrium.
By Lemma 1, we know computing an
Œµ-approximate Œ¶Int,Œ®(Œ¥)-equilibrium reduces to minimizing Œ¶X
Int,Œ®(Œ¥)-regret against convex loss
functions. We show that minimizing Œ¶X
Int,Œ®(Œ¥)-regret against convex loss functions further reduces
to Œ®-regret minimization against linear loss functions."
T,0.7063492063492064,"Theorem 10. Let A be an algorithm with Œ®-regret RegT
Œ®(G, DX ) for linear and G-Lipschitz loss
functions over X. Then, for any Œ¥ > 0, the Œ¶X
Int,Œ®(Œ¥)-regret of A for convex and G-Lipschitz loss"
T,0.7076719576719577,"functions over X is at most
Œ¥
DX ¬∑ [RegT
Œ®(G, DX )]
+."
T,0.708994708994709,"Proof. By definition and convexity of f t, we get"
T,0.7103174603174603,"max
œï‚ààŒ¶X
Int,Œ®(Œ¥) T
X"
T,0.7116402116402116,"t=1
f t(xt) ‚àíf t(œï(xt)) =
max
œà‚ààŒ®,Œª‚â§
Œ¥
DX T
X"
T,0.7129629629629629,"t=1
f t(xt) ‚àíf t((1 ‚àíŒª)xt + Œªœà(xt))"
T,0.7142857142857143,"‚â§
Œ¥
DX """
T,0.7156084656084656,"max
œà‚ààŒ® T
X t=1"
T,0.716931216931217,"‚àáf t(xt), xt ‚àíœà(xt)

#+ ."
T,0.7182539682539683,"Figure 2: Illustration of œïProj,v(x) and œïBeam,v(x) ùë•"
T,0.7195767195767195,"ùúô!""#$,&(ùë•)"
T,0.7208994708994709,"ùúô'()*,&(ùë•) ‚àíùë£"
T,0.7222222222222222,"Note that when f t is linear, the reduction is without loss. Thus, any worst-case ‚Ñ¶(r(T))-lower bound
for Œ®-regret implies a ‚Ñ¶(
Œ¥
DX ¬∑ r(T)) lower bound for Œ¶Int,Œ®(Œ¥)-regret. Moreover, for any set Œ® that
admits efficient Œ®-regret minimization algorithms such as swap transformations over the simplex
and more generally any set such that (i) all modifications in the set can be represented as linear
transformations in some finite-dimensional space and (ii) fixed point computation can be carried out
efficiently for any linear transformations [GGM08], we also get an efficient algorithm for computing
an Œµ-approximate Œ¶Int,Œ®(Œ¥)-equilibrium in the first-order stationary regime."
T,0.7235449735449735,"CCE-like Instantiation
In the special case where Œ® contains only constant strategy modifications
(i.e. œà(x) = x‚àófor all x), we get a coarse correlated equilibrium (CCE)-like instantiation of local
equilibrium, which limits the gain by interpolating with any fixed strategy. We denote the resulting
set of local strategy modification simply as Œ¶X
Int. We can apply any no-external regret algorithm
for efficient Œ¶X
Int-regret minimization and computation of Œµ-approximate Œ¶Int(Œ¥)-equilibrium in the
first-order stationary regime as summarized in Theorem 5."
T,0.7248677248677249,"The above Œ¶X
Int(Œ¥)-regret bound of O(
‚àö"
T,0.7261904761904762,"T) is derived for the adversarial setting. In the game setting,
where each player employs the same algorithm, players may have substantially lower external
regret [Syr+15; CP20; DFG21; Ana+22a; Ana+22b; Far+22a] but we need a slightly stronger
smoothness assumption than Assumption 1. This assumption is naturally satisfied by finite normal-
form games and is also made for results about concave games [Far+22a]. Using Assumption 2
and Lemma 1, the no-regret learning dynamics of [Far+22a] that guarantees O(log T) individual
external regret in concave games can be applied to smooth non-concave games so that the individual
Œ¶X
Int(Œ¥)-regret of each player is at most O(log T)+ Œ¥2LT"
T,0.7275132275132276,"2
. This gives an algorithm with faster ÀúO(1/Œµ)
convergence to an (Œµ + Œ¥2L"
T,0.7288359788359788,2 )-approximate Œ¶Int(Œ¥)-equilibrium than GD.
T,0.7301587301587301,"H
Beam-Search Local Strategy Modifications and Local Equilibria"
T,0.7314814814814815,"In Section 4.1 and Section 4.3, we have shown that GD achieves near-optimal performance for both
Œ¶X
Int(Œ¥)-regret and Œ¶X
Proj(Œ¥)-regret. In this section, we introduce another natural set of local strategy
modifications, Œ¶X
Beam(Œ¥), which is similar to Œ¶X
Proj(Œ¥). Specifically, the set Œ¶X
Beam(Œ¥) contains
deviations that try to move as far as possible in a fixed direction (see Figure 2 for an illustration of
the difference between œïBeam,v(x) and œïProj,v(x)):"
T,0.7328042328042328,"Œ¶X
Beam(Œ¥) := {œïBeam,v(x) = x ‚àíŒª‚àóv : v ‚ààBd(Œ¥), Œª‚àó= max{Œª : x ‚àíŒªv ‚ààX, Œª ‚àà[0, 1]}}."
T,0.7341269841269841,"It is clear that ‚à•œïBeam,v(x) ‚àíx‚à•‚â§‚à•v‚à•‚â§Œ¥. We can similarly derive the notion of Œ¶X
Beam-regret
and (Œµ, Œ¶Beam(Œ¥))-equilibrium. Surprisingly, we show that GD suffers linear Œ¶X
Beam(Œ¥)-regret (proof
deferred to Appendix H.1)."
T,0.7354497354497355,"Theorem 11. For any Œ¥, Œ∑ <
1
2 and T ‚â•1, there exists a sequence of linear loss functions
{f t : X ‚äÜ[0, 1]2 ‚ÜíR}t‚àà[T ] such that GD with step size Œ∑ suffers ‚Ñ¶(Œ¥T) Œ¶X
Beam(Œ¥)-regret."
T,0.7367724867724867,"H.1
Proof of Theorem 11"
T,0.7380952380952381,"Let X ‚äÇR2 be a triangle region with vertices A = (0, 0), B = (1, 1), C = (Œ¥, 0). Consider
v = (‚àíŒ¥, 0). The initial point is x1 = (0, 0)."
T,0.7394179894179894,"The adversary will choose ‚Ñìt adaptively so that xt remains on the boundary of X and cycles clockwise
(i.e., A ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíB ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíC ‚Üí¬∑ ¬∑ ¬∑ ‚ÜíA ‚Üí¬∑ ¬∑ ¬∑ ). To achieve this, the adversary will repeat the
following three phases:"
T,0.7407407407407407,"1. Keep choosing ‚Ñìt = u‚àí‚àí‚Üí
BA (u‚àí‚àí‚Üí
BA denotes the unit vector in the direction of ‚àí‚àí‚Üí
BA) until xt+1
reaches B.
2. Keep choosing ‚Ñìt = u‚àí‚àí‚Üí
CB until xt+1 reaches C."
T,0.7420634920634921,"3. Keep choosing ‚Ñìt = u‚àí‚Üí
AC until xt+1 reaches A."
T,0.7433862433862434,"In Phase 1, xt ‚ààAB. By the choice of v = (‚àíŒ¥, 0), we have xt ‚àíœïv(xt) = (‚àíŒ¥(1 ‚àíxt,1), 0), and
the instantaneous regret is Œ¥(1‚àíxt,1)
‚àö 2
‚â•0."
T,0.7447089947089947,"In Phase 2, xt ‚ààBC. By the choice of v = (‚àíŒ¥, 0), we have xt ‚àíœïv(xt) = (0, 0), and the
instantaneous regret is 0."
T,0.746031746031746,"In Phase 3, xt ‚ààCA. By the choice of v = (‚àíŒ¥, 0), we have xt ‚àíœïv(xt) = (‚àíŒ¥ + xt,1, 0), and the
instantaneous regret is ‚àíŒ¥ + xt,1 ‚â§0."
T,0.7473544973544973,"In each cycle, the number of rounds in Phase 1 is of order Œò(
‚àö"
T,0.7486772486772487,"2
Œ∑ ), the number of rounds in Phase 2 is"
T,0.75,between O( 1
T,0.7513227513227513,"Œ∑) and O(
‚àö"
T,0.7526455026455027,"2
Œ∑ ), the number of rounds in Phase 3 is of order Œò( Œ¥ Œ∑)."
T,0.753968253968254,"Therefore, the cumulative regret in each cycle is roughly
‚àö"
T,0.7552910052910053,"2
Œ∑ √ó 0.5Œ¥
‚àö"
T,0.7566137566137566,2 + 0 + Œ¥
T,0.7579365079365079,"Œ∑ (‚àí0.5Œ¥) = 0.5Œ¥ ‚àí0.5Œ¥2 Œ∑
."
T,0.7592592592592593,"On the other hand, the number of cycles is no less than
T
‚àö"
T,0.7605820105820106,"2
Œ∑ +
‚àö"
T,0.7619047619047619,"2
Œ∑ + Œ¥"
T,0.7632275132275133,"Œ∑
= Œò(Œ∑T). Overall, the cumulative"
T,0.7645502645502645,regret is at least 0.5Œ¥‚àí0.5Œ¥2
T,0.7658730158730159,"Œ∑
√ó Œò(Œ∑T) = Œò(Œ¥T) as long as Œ¥ < 0.5."
T,0.7671957671957672,"I
Hardness in the Global Regime"
T,0.7685185185185185,"In the first-order stationary regime Œ¥ ‚â§
p"
T,0.7698412698412699,"2Œµ/L, (Œµ, Œ¥)-local Nash equilibrium is intractable, and we
have shown polynomial-time algorithms for computing the weaker notions of Œµ-approximate Œ¶Int(Œ¥))-
equilibrium and Œµ-approximate Œ¶Proj(Œ¥))-equilibrium. A natural question is whether correlation
enables efficient computation of Œµ-approximate Œ¶(Œ¥))-equilibrium when Œ¥ is in the global regime,
i.e., Œ¥ = ‚Ñ¶(
‚àö"
T,0.7711640211640212,"d). In this section, we prove both computational hardness and a query complexity lower
bound for both notions in the global regime"
T,0.7724867724867724,"To prove the lower bound results, we only require a single-player game. The problem of computing an
Œµ-approximate Œ¶(Œ¥)-equilibrium becomes: given scalars Œµ, Œ¥, G, L > 0 and a polynomial-time Turing
machine Cf evaluating a G-Lipschitz and L-smooth function f : [0, 1]d ‚Üí[0, 1] and its gradient
‚àáf : [0, 1]d ‚ÜíRd, we are asked to output a distribution œÉ that is an Œµ-approximate Œ¶(Œ¥)-equilibrium
or ‚ä•if such equilibrium does not exist."
T,0.7738095238095238,"Hardness of finding Œµ-approximate Œ¶X
Int(Œ¥)-equilibria in the global regime
When Œ¥ =
‚àö"
T,0.7751322751322751,"d,
which equals to the diameter D of [0, 1]d, then the problem of finding an Œµ-approximate Œ¶X
Int(Œ¥)-
equilibrium is equivalent to finding a (Œµ, Œ¥)-local minimum of f: assume œÉ is an Œµ-approximate
Œ¶X
Int(Œ¥)-equilibrium of f, then there exists x ‚àà[0, 1]d in the support of œÉ such that
f(x) ‚àí
min
x‚àó‚àà[0,1]d‚à©Bd(x‚àó,Œ¥) f(x‚àó) ‚â§Œµ."
T,0.7764550264550265,"Then hardness of finding an Œµ-approximate Œ¶X
Int(Œ¥)-equilibrium follows from hardness of finding a
(Œµ, Œ¥)-local minimum of f [DSZ21]. The following Theorem is a corollary of Theorem 10.3 and 10.4
in [DSZ21]."
T,0.7777777777777778,"Theorem 12 (Hardness of finding Œµ-approximate Œ¶X
Int(Œ¥)-equilibria in the global regime). In the
worst case, the following two holds."
T,0.7791005291005291,"‚Ä¢ Computing an Œµ-approximate Œ¶X
Int(Œ¥)-equilibrium for a game on X = [0, 1]d with G =
‚àö"
T,0.7804232804232805,"d,
L = d, Œµ ‚â§
1
24, Œ¥ =
‚àö"
T,0.7817460317460317,d is NP-hard.
T,0.783068783068783,"‚Ä¢ ‚Ñ¶(2d/d) value/gradient queries are needed to determine an Œµ-approximate Œ¶X
Int(Œ¥)-
equilibrium for a game on X = [0, 1]d with G = Œò(d15), L = Œò(d22), Œµ < 1, Œ¥ =
‚àö d."
T,0.7843915343915344,"Hardness of finding Œµ-approximate Œ¶X
Proj(Œ¥)-equilibria in the global regime"
T,0.7857142857142857,"Theorem 13 (Hardness of of finding Œµ-approximate Œ¶X
Proj(Œ¥)-equilibria in the global regime). In the
worst case, the following two holds."
T,0.7870370370370371,"‚Ä¢ Computing an Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium for a game on X = [0, 1]d with G =
Œò(d15), L = Œò(d22), Œµ < 1, Œ¥ =
‚àö"
T,0.7883597883597884,d is NP-hard.
T,0.7896825396825397,"‚Ä¢ ‚Ñ¶(2d/d) value/gradient queries are needed to determine an Œµ-approximate Œ¶X
Proj(Œ¥)-
equilibrium for a game on X = [0, 1]d with G = Œò(d15), L = Œò(d22), Œµ < 1, Œ¥ =
‚àö d."
T,0.791005291005291,"The hardness of computing Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium also implies a lower bound on
Œ¶X
Proj(Œ¥)-regret in the global regime."
T,0.7923280423280423,"Corollary 2 (Lower bound of Œ¶X
Proj(Œ¥)-regret against non-convex functions). In the worst case, the
Œ¶X
Proj(Œ¥)-regret of any online algorithm is at least ‚Ñ¶(2d/d, T) even for loss functions f : [0, 1]d ‚Üí
[0, 1] with G, L = poly(d) and Œ¥ =
‚àö d."
T,0.7936507936507936,The proofs of Theorem 13 and Corollary 2 can be found in the next two sections.
T,0.794973544973545,"I.1
Proof of Theorem 13"
T,0.7962962962962963,"We will reduce the problem of finding an Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium in smooth games to
finding a satisfying assignment of a boolean function, which is NP-complete."
T,0.7976190476190477,"Fact 1. Given only black-box access to a boolean formula œï : {0, 1}d ‚Üí{0, 1}, at least ‚Ñ¶(2d)
queries are needed in order to determine whether œï admits a satisfying assignment x‚àósuch that
œï(x‚àó) = 1. The term black-box access refers to the fact that the clauses of the formula are not given,
and the only way to determine whether a specific boolean assignment is satisfying is by querying the
specific binary string. Moreover, the problem of finding a satisfying assignment of a general boolean
function is NP-hard."
T,0.798941798941799,"We revisit the construction of the hard instance in the proof of [DSZ21, Theorem 10.4] and use its
specific structures. Given black-box access to a boolean formula œï as described in Fact 1, following
[DSZ21], we construct the function fœï(x) : [0, 1]d ‚Üí[0, 1] as follows:"
T,0.8002645502645502,"1. for each corner v ‚ààV = {0, 1}d of the [0, 1]d hypercube, we set fœï(x) = 1 ‚àíœï(x)."
T,0.8015873015873016,"2. for the rest of the points x ‚àà[0, 1]d/V , we set fœï(x) = P"
T,0.8029100529100529,"v‚ààV Pv(x) ¬∑ fœï(v) where Pv(x)
are non-negative coefficients defined in [DSZ21, Definition 8.9]."
T,0.8042328042328042,The function fœï satisfies the following properties:
T,0.8055555555555556,"1. if œï is not satisfiable, then fœï(x) = 1 for all x ‚àà[0, 1]d since fœï(v) = 1 for all v ‚ààV ; if œï
has a satisfying assignment v‚àó, then fœï(v‚àó) = 0."
T,0.8068783068783069,2. fœï is Œò(d12)-Lipschitz and Œò(d25)-smooth.
T,0.8082010582010583,"3. for any point x ‚àà[0, 1]d, the set V (x) := {v ‚ààV : Pv(x) Ã∏= 0} has cardinality at most
d + 1 while P"
T,0.8095238095238095,"v‚ààV Pv(x) = 1; any value / gradient query of fœï can be simulated by d + 1
queries on œï."
T,0.8108465608465608,"In the case there exists a satisfying argument v‚àó, then fœï(v‚àó) = 0. Define the deviation e so that
e[i] = 1 if v‚àó[i] = 0 and e[i] = ‚àí1 if v‚àó[i] = 1. It is clear that ‚à•e‚à•=
‚àö"
T,0.8121693121693122,"d = Œ¥. By properties of
projection on [0, 1]d, for any x ‚àà[0, 1]d, we have Œ†[0,1]n[x ‚àív] = v‚àó. Then any Œµ-approximate
Œ¶X
Proj(Œ¥)-equilibrium œÉ must include some x‚àó‚ààX with fœï(x‚àó) < 1 in the support, since Œµ < 1.
In case there exists an algorithm A that computes an Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium, A must
have queried some x‚àówith fœï(x‚àó) < 1. Since fœï(x‚àó) = P"
T,0.8134920634920635,"v‚ààV (x‚àó) Pv(x‚àó)fœï(v) < 1, there exists
ÀÜv ‚ààV (x‚àó) such that fœï(ÀÜv) = 0. Since |V (x‚àó)| ‚â§d + 1, it takes addition d + 1 queries to find ÀÜv with
fœï(ÀÜv) = 0. By Fact 1 and the fact that we can simulate every value/gradient query of fœï by d + 1
queries on œï, A makes at least ‚Ñ¶(2d/d) value/gradient queries."
T,0.8148148148148148,"Suppose there exists an algorithm B that outputs an Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium œÉ in time
T(B) for Œµ < 1 and Œ¥ =
‚àö"
T,0.8161375661375662,"d. We construct another algorithm C for SAT that terminates in time
T(B) ¬∑ poly(d). C: (1) given a boolean formula œï, construct fœï as described above; (2) run B and get
output œÉ (3) check the support of œÉ to find v ‚àà{0, 1}d such that fœï(v) = 0; (3) if finds v ‚àà{0, 1}d
such that fœï(v) = 0, then œï is satisfiable, otherwise œï is not satisfiable. Since we can evaluate fœï
and ‚àáfœï in poly(d) time and the support of œÉ is smaller than T(B), the algorithm C terminates
in time O(T(B) ¬∑ poly(d)). The above gives a polynomial time reduction from SAT to finding an
Œµ-approximate Œ¶X
Proj(Œ¥)-equilibrium and proves the NP-hardness of the latter problem."
T,0.8174603174603174,"I.2
Proof of Corollary 2"
T,0.8187830687830688,"Let œï : {0, 1}d ‚Üí{0, 1} be a boolean formula and define fœï : [0, 1]d ‚Üí[0, 1] the same as that
in Theorem 13. We know fœï is Œò(poly(d))-Lipschitz and Œò(poly(d))-smooth. Now we let the
adversary pick fœï each time. For any T ‚â§O(2d/d), in case there exists an online learning algorithm
with RegT
Proj,Œ¥ < T"
T,0.8201058201058201,"2 , then œÉ := 1"
T,0.8214285714285714,"T
PT
t=1 1xt is an ( 1"
T,0.8227513227513228,"2, Œ¥)-equilibrium. Applying Theorem 13 and the
fact that in this case, RegT
Proj,Œ¥ is non-decreasing with respect to T concludes the proof."
T,0.8240740740740741,"J
Removing the D dependence for Œ¶X
Proj-regret"
T,0.8253968253968254,"For the regime Œ¥ ‚â§DX which we are more interested in, the lower bound in Theorem 7 is ‚Ñ¶(GŒ¥
‚àö"
T,0.8267195767195767,"T)
while the upper bound in Theorem 3 is O(G‚àöŒ¥DX T). They are not tight especially when DX ‚â´Œ¥.
A natural question is: which of them is the tight bound?
We conjecture that the lower bound is
tight. In fact, for the special case where the feasible set X is a box, we have a way to obtain a
DX -independent bound O(d
1
4 GŒ¥
‚àö"
T,0.828042328042328,"T), which is tight when d = 1. Below, we first describe the
improved strategy in 1-dimension. Then we show how to extend it to the d-dimensional box setting."
T,0.8293650793650794,"J.1
One-Dimensional Case"
T,0.8306878306878307,"In one-dimension, we assume that X = [a, b] for some b ‚àía ‚â•2Œ¥ (if b ‚àía ‚â§2Œ¥, then our original
bound in Theorem 3 is already of order GŒ¥
‚àö"
T,0.832010582010582,"T). We first investigate the case where f t(x) is a linear
function, i.e., f t(x) = gtx for some gt ‚àà[‚àíG, G]. The key idea is that we will only select xt from
the two intervals [a, a + Œ¥] and [b ‚àíŒ¥, b], and never play xt ‚àà(a + Œ¥, b ‚àíŒ¥). To achieve so, we
concatenate these two intervals, and run an algorithm in this region whose diameter is only 2Œ¥. The
key property we would like to show is that the regret is preserved in this modified problem."
T,0.8333333333333334,"More precisely, given the original feasible set X = [a, b], we create a new feasible set Y = [‚àíŒ¥, Œ¥]
and apply our algorithm GD in this new feasible set. The loss function is kept as f t(x) = gtx.
Whenever the algorithm for Y outputs yt ‚àà[‚àíŒ¥, 0], we play xt = yt + a + Œ¥ in X; whenever it
outputs yt ‚àà(0, Œ¥], we play xt = yt + b ‚àíŒ¥. Below we show that the regret is the same in these two"
T,0.8346560846560847,"problems. Notice that when yt ‚â§0, we have for any v ‚àà[‚àíŒ¥, Œ¥],"
T,0.8359788359788359,"xt ‚àíŒ†X [xt ‚àív] = xt ‚àímax
 
min
 
xt ‚àív, b

, a
"
T,0.8373015873015873,"= xt ‚àímax
 
xt ‚àív, a
"
T,0.8386243386243386,(xt ‚àív = yt + a + Œ¥ ‚àív ‚â§a + 2Œ¥ ‚â§b always holds)
T,0.83994708994709,"= yt + a + Œ¥ ‚àímax
 
yt + a + Œ¥ ‚àív, a
"
T,0.8412698412698413,"= yt ‚àímax
 
yt ‚àív, ‚àíŒ¥
"
T,0.8425925925925926,"= yt ‚àímax
 
min
 
yt ‚àív, Œ¥

, ‚àíŒ¥

(yt ‚àív ‚â§Œ¥ always holds)"
T,0.843915343915344,= yt ‚àíŒ†Y[yt ‚àív]
T,0.8452380952380952,"Similarly, when yt > 0, we can follow the same calculation and prove xt ‚àíŒ†X [xt ‚àív] = yt ‚àí
Œ†Y[yt ‚àív]. Thus, the regret in the two problems:"
T,0.8465608465608465,"gt  
xt ‚àíŒ†X [xt ‚àív]

and
gt  
yt ‚àíŒ†Y[yt ‚àív]
"
T,0.8478835978835979,"are exactly the same for any v. Finally, observe that the diameter of Y is only of order O(Œ¥). Thus,
the upper bound in Theorem 3 would give us an upper bound of O(G
‚àö"
T,0.8492063492063492,"Œ¥ ¬∑ Œ¥T) = O(GŒ¥
‚àö T)."
T,0.8505291005291006,"For convex f t, we run the algorithm above with gt = ‚àáf t(xt). Then by convexity we have"
T,0.8518518518518519,"f t(xt) ‚àíf t(Œ†X [xt ‚àív]) ‚â§gt(xt ‚àíŒ†X [xt ‚àív]) = gt(yt ‚àíŒ†Y[yt ‚àív]),"
T,0.8531746031746031,"so the regret in the modified problem (which is O(GŒ¥
‚àö"
T,0.8544973544973545,"T)) still serves as a regret upper bound for
the original problem."
T,0.8558201058201058,"J.2
d-Dimensional Box Case"
T,0.8571428571428571,"A d-dimensional box is of the form X = [a1, b1] √ó [a2, b2] √ó ¬∑ ¬∑ ¬∑ √ó [ad, bd]. The box case is easy
to deal with because we can decompose the regret into individual components in each dimension.
Namely, we have"
T,0.8584656084656085,"f t(xt) ‚àíf t(Œ†X [xt ‚àív]) ‚â§‚àáf t(xt)‚ä§ 
xt ‚àíŒ†X [xt ‚àív]
 = d
X"
T,0.8597883597883598,"i=1
gt
i
 
xt
i ‚àíŒ†Xi[xt
i ‚àívi]
"
T,0.8611111111111112,"where we define Xi = [ai, bi], gt = ‚àáf t(xt), and use subscript i to indicate the i-th component of a
vector. The last equality above is guaranteed by the box structure. This decomposition allows as to
view the problem as d independent 1-dimensional problems."
T,0.8624338624338624,"Now we follow the strategy described in Section J.1 to deal with individual dimensions (if bi‚àíai < 2Œ¥
then we do not modify Xi; otherwise, we shrink Xi to be of length 2Œ¥). Applying the analysis of
Theorem 3 to each dimension, we get d
X"
T,0.8637566137566137,"i=1
gt
i
 
xt
i ‚àíŒ†Xi[xt
i ‚àívi]
 ‚â§ d
X i=1"
T,0.8650793650793651,"v2
i
2Œ∑ + Œ∑ 2 T
X"
T,0.8664021164021164,"t=1
(gt
i)2 + |vi| √ó 2Œ¥ Œ∑ !"
T,0.8677248677248677,(the diameter in each dimension is now bounded by 2Œ¥) ‚â§O
T,0.8690476190476191,"Œ¥ Pd
i=1 |vi|"
T,0.8703703703703703,"Œ∑
+ Œ∑G2T ! ‚â§O Œ¥2‚àö"
T,0.8716931216931217,"d
Œ∑
+ Œ∑G2T !"
T,0.873015873015873,".
(by Cauchy-Schwarz, P"
T,0.8743386243386243,"i |vi| ‚â§
‚àö d
pP"
T,0.8756613756613757,"i |vi|2 ‚â§Œ¥
‚àö d)"
T,0.876984126984127,"Choosing the optimal Œ∑ =
d
1
4 Œ¥
G
‚àö"
T,0.8783068783068783,"T , we get the regret upper bound of order O

d
1
4 GŒ¥
‚àö T

."
T,0.8796296296296297,NeurIPS Paper Checklist
T,0.8809523809523809,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit."
T,0.8822751322751323,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
T,0.8835978835978836,"‚Ä¢ You should answer [Yes] , [No] , or [NA] ."
T,0.8849206349206349,"‚Ä¢ [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available."
T,0.8862433862433863,‚Ä¢ Please provide a short (1‚Äì2 sentence) justification right after your answer (even for NA).
T,0.8875661375661376,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
T,0.8888888888888888,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
T,0.8902116402116402,"IMPORTANT, please:"
T,0.8915343915343915,"‚Ä¢ Delete this instruction block, but keep the section heading ‚ÄúNeurIPS paper checklist"","
T,0.8928571428571429,"‚Ä¢ Keep the checklist subsection headings, questions/answers and guidelines below."
T,0.8941798941798942,‚Ä¢ Do not modify the questions and only use the provided macros for your answers.
CLAIMS,0.8955026455026455,1. Claims
CLAIMS,0.8968253968253969,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?"
CLAIMS,0.8981481481481481,Answer: [Yes]
CLAIMS,0.8994708994708994,"Justification: We state the problem setting and our main contributions in the abstract and
introduction."
CLAIMS,0.9007936507936508,Guidelines:
CLAIMS,0.9021164021164021,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9034391534391535,2. Limitations
LIMITATIONS,0.9047619047619048,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.906084656084656,Answer: [Yes]
LIMITATIONS,0.9074074074074074,"Justification: Our results provide efficient uncoupled algorithms to compute Œµ-approximate
Œ¶-equilibria in the first-order stationary regime. We leave the computational complexity
of finding Œµ-approximate Œ¶-equilibria beyond the first-order stationary regime as an open
question."
LIMITATIONS,0.9087301587301587,Guidelines:
LIMITATIONS,0.91005291005291,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9113756613756614,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9126984126984127,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.9140211640211641,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.9153439153439153,"Justification: We provide assumptions and proofs for our theoretical results in the main body
and the appendix."
THEORY ASSUMPTIONS AND PROOFS,0.9166666666666666,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.917989417989418,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9193121693121693,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9206349206349206,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.921957671957672,Answer: [NA]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9232804232804233,"Justification: This paper does not contain experimental results.
Guidelines:"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9246031746031746,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9259259259259259,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This paper does not contain experimental results.
Guidelines:"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9272486772486772,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9285714285714286,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9298941798941799,6. Experimental Setting/Details
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9312169312169312,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9325396825396826,Answer: [NA]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9338624338624338,Justification: This paper does not contain experimental results.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9351851851851852,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9365079365079365,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9378306878306878,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9391534391534392,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9404761904761905,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9417989417989417,Justification: This paper does not contain experimental results.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9431216931216931,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9444444444444444,"‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9457671957671958,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9470899470899471,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9484126984126984,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.9497354497354498,Justification: This paper does not contain experimental results.
EXPERIMENTS COMPUTE RESOURCES,0.951058201058201,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9523809523809523,‚Ä¢ The answer NA means that the paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.9537037037037037,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper)."
CODE OF ETHICS,0.955026455026455,9. Code Of Ethics
CODE OF ETHICS,0.9563492063492064,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9576719576719577,Answer: [Yes]
CODE OF ETHICS,0.958994708994709,"Justification: We have reviewed the NeurIPS Code of Ethics and the current paper conforms
the NeurIPS Code of Ethics."
CODE OF ETHICS,0.9603174603174603,Guidelines:
CODE OF ETHICS,0.9616402116402116,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9629629629629629,10. Broader Impacts
BROADER IMPACTS,0.9642857142857143,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9656084656084656,Answer: [NA]
BROADER IMPACTS,0.966931216931217,"Justification: This is a purely theoretical paper and we do not see any immediate societal
impact."
BROADER IMPACTS,0.9682539682539683,Guidelines:
BROADER IMPACTS,0.9695767195767195,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9708994708994709,11. Safeguards
SAFEGUARDS,0.9722222222222222,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9735449735449735,Answer: [NA]
SAFEGUARDS,0.9748677248677249,Justification: This is a purely theoretical paper
SAFEGUARDS,0.9761904761904762,Guidelines:
SAFEGUARDS,0.9775132275132276,"‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9788359788359788,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9801587301587301,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9814814814814815,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.9828042328042328,Justification: This is a purely theoretical paper.
LICENSES FOR EXISTING ASSETS,0.9841269841269841,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9854497354497355,"‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators."
NEW ASSETS,0.9867724867724867,13. New Assets
NEW ASSETS,0.9880952380952381,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9894179894179894,Answer: [NA]
NEW ASSETS,0.9907407407407407,Justification: This is a purely theoretical paper.
NEW ASSETS,0.9920634920634921,Guidelines:
NEW ASSETS,0.9933862433862434,"‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947089947089947,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996031746031746,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not contain experiments.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973544973544973,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not contain experiments.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9986772486772487,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
