Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004347826086956522,"Reconstructing medical images from partial measurements is an important inverse
problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI).
Existing solutions based on machine learning typically train a model to directly map
measurements to medical images, leveraging a training dataset of paired images and
measurements. These measurements are typically synthesized from images using a
ﬁxed physical model of the measurement process, which hinders the generalization
capability of models to unknown measurement processes. To address this issue, we
propose a fully unsupervised technique for inverse problem solving, leveraging the
recently introduced score-based generative models. Speciﬁcally, we ﬁrst train a
score-based generative model on medical images to capture their prior distribution.
Given measurements and a physical model of the measurement process at test
time, we introduce a sampling method to reconstruct an image consistent with both
the prior and the observed measurements. Our method does not assume a ﬁxed
measurement process during training, and can thus be ﬂexibly adapted to different
measurement processes at test time. Empirically, we observe comparable or better
performance to supervised learning techniques in several medical imaging tasks in
CT and MRI, while demonstrating signiﬁcantly better generalization to unknown
measurement processes."
INTRODUCTION,0.008695652173913044,"1
INTRODUCTION"
INTRODUCTION,0.013043478260869565,"Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are commonly used imaging
tools for medical diagnosis. Reconstructing CT and MRI images from raw measurements (sinograms
for CT and k-spaces for MRI) are well-known inverse problems. Speciﬁcally, measurements in
CT are given by X-ray projections of an object from various directions, and measurements in MRI
are obtained by inspecting the Fourier spectrum of an object with magnetic ﬁelds. However, since
obtaining the full sinogram for CT causes excessive ionizing radiation for patients, and measuring
the full k-space of MRI is very time-consuming, it has become important to reduce the number
of measurements in CT and MRI. In many cases, only partial measurements, such as sparse-view
sinograms and downsampled k-spaces, are available. Due to this loss of information, the inverse
problems in CT and MRI are often ill-posed, making image reconstruction especially challenging."
INTRODUCTION,0.017391304347826087,"With the rise of machine learning, many methods (Zhu et al., 2018; Mardani et al., 2017; Shen et al.,
2019; Würﬂet al., 2018; Ghani & Karl, 2018; Wei et al., 2020) have been proposed for medical
image reconstruction using a small number of measurements. Most of these methods are supervised
learning techniques. They learn to directly map partial measurements to medical images, by training
on a large dataset comprising pairs of CT/MRI images and measurements. These measurements need
to be synthesized from medical images with a ﬁxed physical model of the measurement process.
However, when the measurement process changes, such as using a different number of CT projections
or different downsampling ratio of MRI k-spaces, we have to re-collect the paired dataset with the
new measurement process and re-train the model. This prevents models from generalizing effectively
to new measurement processes, leading to counter-intuitive instabilities such as more measurements
causing worse performance (Antun et al., 2020)."
INTRODUCTION,0.021739130434782608,˚Joint ﬁrst authors.
INTRODUCTION,0.02608695652173913,Published as a conference paper at ICLR 2022
INTRODUCTION,0.030434782608695653,"In this work, we sidestep this difﬁculty completely by proposing unsupervised methods that do not
require a paired dataset for training, and therefore are not restricted to a ﬁxed measurement process.
Our main idea is to learn the prior distribution of medical images with a generative model in order to
infer the lost information due to partial measurements. Speciﬁcally, we propose to train a score-based
generative model (Song & Ermon, 2019; 2020; Song et al., 2021) on medical images as the data prior,
due to its strong performance in image generation (Ho et al., 2020; Dhariwal & Nichol, 2021). Given
a trained score-based generative model, we provide a family of sampling algorithms to create image
samples that are consistent with the observed measurements and the estimated data prior, leveraging
the physical measurement process. Once our model is trained, it can be used to solve any inverse
problem within the same image domain, as long as the mapping from images to measurements is
linear, which holds for a large number of medical imaging applications."
INTRODUCTION,0.034782608695652174,"We evaluate the performance of our method on several tasks in CT and MRI. Empirically, we observe
comparable or better performance compared to supervised learning counterparts, even when evaluated
with the same measurement process in their training. In addition, we are able to uniformly surpass all
baselines when changing the number of measurements, e.g., using a different number of projections
in sparse-view CT or changing the k-space downsampling ratio in undersampled MRI. Moreover, we
show that by plugging in a different measurement process, we can use a single model to perform both
sparse-view CT reconstruction and metal artifact removal for CT imaging with metallic implants.
To the best of our knowledge, this is the ﬁrst time that generative models are reported successful
on clinical CT data. Collectively, these empirical results indicate that our method is a competitive
alternative to supervised techniques in medical image reconstruction and artifact removal, and has the
potential to be a universal tool for solving many inverse problems within the same image domain."
BACKGROUND,0.0391304347826087,"2
BACKGROUND"
LINEAR INVERSE PROBLEMS,0.043478260869565216,"2.1
LINEAR INVERSE PROBLEMS"
LINEAR INVERSE PROBLEMS,0.04782608695652174,"An inverse problem seeks to recover an unknown signal from a set of observed measurements.
Speciﬁcally, suppose x P Rn is an unknown signal, and y P Rm “ Ax ` ϵ is a noisy observation
given by m linear measurements, where the measurement acquisition process is represented by a
linear operator A P Rmˆn, and ϵ P Rn represents a noise vector. Solving a linear inverse problem
amounts to recovering the signal x from its measurement y. Without further assumptions, the problem
is ill-deﬁned when m ă n, so we additionally assume that x is sampled from a prior distribution ppxq.
In this probabilistic formulation, the measurement and signal are connected through a measurement
distribution ppy | xq “ qϵpy ´ Axq, where qϵ denotes the noise distribution of ϵ. Given ppy | xq
and ppxq, we can solve the inverse problem by sampling from the posterior distribution ppx | yq."
LINEAR INVERSE PROBLEMS,0.05217391304347826,"Examples of linear inverse problems in medical imaging include image reconstruction for CT and
MRI. In both cases, the signal x is a medical image. The measurement y in CT is a sinogram
formed by X-ray projections of the image from various angular directions (Buzug, 2011), while the
measurement y in MRI consists of spatial frequencies in the Fourier space of the image (a.k.a. the
k-space in the MRI community) (Vlaardingerbroek & Boer, 2013)."
SCORE-BASED GENERATIVE MODELS,0.05652173913043478,"2.2
SCORE-BASED GENERATIVE MODELS"
SCORE-BASED GENERATIVE MODELS,0.06086956521739131,"When solving inverse problems in medical imaging, we are given an observation y, the measurement
distribution ppy | xq and aim to sample from the posterior distribution ppx | yq. The prior distribution
ppxq is typically unknown, but we can train generative models on a dataset txp1q, xp2q, ¨ ¨ ¨ , xpNqu „
ppxq to estimate this prior distribution. Given an estimate of ppxq and the measurement distribution
ppy | xq, the posterior distribution ppx | yq can be determined through Bayes’ rule."
SCORE-BASED GENERATIVE MODELS,0.06521739130434782,"We propose to estimate the prior distribution of medical images using the recently introduced score-
based generative models (Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021), whose iterative
sampling procedure makes it especially easy for controllable generation conditioned on an observation
y. Speciﬁcally, we adopt the formulation of score-based generative models in Song et al. (2021),
where we leverage a Markovian diffusion process to progressively perturb data to noise, and then
smoothly convert noise to samples of the data distribution by estimating and simulating its time
reversal. We provide an illustration of this generative modeling framework in Fig. 1."
SCORE-BASED GENERATIVE MODELS,0.06956521739130435,Published as a conference paper at ICLR 2022
SCORE-BASED GENERATIVE MODELS,0.07391304347826087,"Figure 1: We can smoothly
perturb images to noise
by following the trajec-
tory of an SDE. By esti-
mating the score function
∇x log ptpxq with neural
networks (called score mod-
els), it is possible to approx-
imate the reverse SDE and
then solve it to generate im-
age samples from noise."
SCORE-BASED GENERATIVE MODELS,0.0782608695652174,"Perturbation process Suppose the dataset is sampled from an unknown data distribution ppxq. We
perturb datapoints with a stochastic process over a time horizon r0, 1s, governed by a linear stochastic
differential equation (SDE) of the following form"
SCORE-BASED GENERATIVE MODELS,0.08260869565217391,"dxt “ fptqxt dt ` gptq dwt,
t P r0, 1s,
(1)"
SCORE-BASED GENERATIVE MODELS,0.08695652173913043,"where f : r0, 1s Ñ R, g : r0, 1s Ñ R, twt P RnutPr0,1s denotes a standard Wiener process
(a.k.a., Brownian motion), and txt P RnutPr0,1s symbolizes the trajectory of random variables in the
stochastic process. We further denote the marginal probability distribution of xt as ptpxq, and the
transition distribution from x0 to xt as p0tpxt | x0q. By deﬁnition, we clearly have p0pxq ” ppxq.
Moreover, the functions fptq and gptq are speciﬁcally chosen such that for any initial distribution
p0pxq, the distribution at the end of the perturbation process, p1pxq, is close to a pre-deﬁned noise
distribution πpxq. In addition, the transition density p0tpxt | x0q is always a conditional linear
Gaussian distribution, taking the form p0tpxt | x0q “ Npxt | αptqx0, β2ptqIq where α : r0, 1s Ñ R
and β : r0, 1s Ñ R can be derived analytically from fptq and gptq (Särkkä & Solin, 2019). Examples
of such SDEs include Variance Exploding (VE), Variance Preserving (VP), and subVP SDEs proposed
in Song et al. (2021). We found VE SDEs performed the best in our experiments."
SCORE-BASED GENERATIVE MODELS,0.09130434782608696,"Reverse process By reversing the perturbation process in Eq. (1), we can start from a noise sample
x1 „ p1pxq and gradually remove the noise therein to obtain a data sample x0 „ p0pxq ” ppxq.
Crucially, the time reversal of Eq. (1) is given by the following reverse-time SDE (Song et al., 2021)"
SCORE-BASED GENERATIVE MODELS,0.09565217391304348,"dxt “
“
fptqxt ´ gptq2∇xt log ptpxtq
‰
dt ` gptq d ¯wt,
t P r0, 1s,
(2)"
SCORE-BASED GENERATIVE MODELS,0.1,"where t ¯wtutPr0,1s denotes a standard Wiener process in the reverse-time direction, and dt represents
an inﬁnitesimal negative time step, since the above SDE must be solved backwards from t “ 1 to
t “ 0. The quantity ∇xt log ptpxtq is known as the score function of ptpxtq. By the deﬁnition of
time reversal, the trajectory of the reverse stochastic process given by Eq. (2) is txtutPr0,1s, same as
the one from the forward SDE in Eq. (1)."
SCORE-BASED GENERATIVE MODELS,0.10434782608695652,"Sampling Given an initial sample from p1pxq, as well as scores at each intermediate time step,
∇x log ptpxq, we can simulate the reverse-time SDE in Eq. (2) to obtain samples from the data
distribution p0pxq ” ppxq. In practice, the initial sample is approximately drawn from πpxq since
πpxq « p1pxq, and the scores are estimated by training a neural network sθpx, tq (named the score
model) on a dataset txp1q, xp2q, ¨ ¨ ¨ , xpNqu „ ppxq with denoising score matching (Vincent, 2011;
Song et al., 2021), i.e., solving the following objective"
SCORE-BASED GENERATIVE MODELS,0.10869565217391304,"θ˚ “ arg min
θ"
N,0.11304347826086956,"1
N N
ÿ"
N,0.11739130434782609,"i“1
Et„Ur0,1sExpiq
t
„p0tpxpiq
t
|xpiqq"
N,0.12173913043478261,"” sθpxpiq
t , tq ´ ∇xpiq
t log p0tpxpiq
t
| xpiqq

2 2 ı
,"
N,0.12608695652173912,"where Ur0, 1s denotes a uniform distribution over r0, 1s. The theory of denoising score matching
ensures that sθ˚px, tq « ∇x log ptpxq. After training this score model, we plug it into Eq. (2) and
solve the resulting reverse-time SDE"
N,0.13043478260869565,"dxt “
“
fptqxt ´ gptq2sθ˚pxt, tq
‰
dt ` gptq d ¯wt,
t P r0, 1s,
(3)"
N,0.13478260869565217,"for sample generation. One sampling method is to use the Euler-Maruyama discretization for solving
Eq. (3), as given in Algorithm 1. Other sampling methods include annealed Langevin dynamics (ALD,
Song & Ermon, 2019), probability ﬂow ODE solvers (Song et al., 2021), and Predictor-Corrector
samplers (Song et al., 2021)."
N,0.1391304347826087,Published as a conference paper at ICLR 2022
N,0.14347826086956522,Algorithm 1 Unconditional sampling
N,0.14782608695652175,Require: N
N,0.15217391304347827,"1: ˆx1 „ πpxq, ∆t Ð
1
N
2: for i “ N ´ 1 to 0 do
3:
t Ð i`1 N"
N,0.1565217391304348,"4:
ˆxt´∆t Ð ˆxt ´ fptqˆxt∆t
5:
ˆxt´∆t Ð ˆxt´∆t ` gptq2sθ˚pˆxt, tq∆t
6:
z „ Np0, Iq
7:
ˆxt´∆t Ð ˆxt´∆t ` gptq
?"
N,0.1608695652173913,"∆t z
8: return ˆx0"
N,0.16521739130434782,Algorithm 2 Inverse problem solving
N,0.16956521739130434,"Require: N, y, λ"
N,0.17391304347826086,"1: ˆx1 „ πpxq, ∆t Ð
1
N
2: for i “ N ´ 1 to 0 do
3:
t Ð i`1"
N,0.1782608695652174,"N
4:
ˆyt „ p0tpyt | yq
5:
ˆxt Ð T ´1rλΛP´1pΛqˆyt `p1´λqΛT ˆxt `
pI ´ ΛqT ˆxts
6:
ˆxt´∆t Ð ˆxt ´ fptqˆxt∆t
7:
ˆxt´∆t Ð ˆxt´∆t ` gptq2sθ˚pˆxt, tq∆t
8:
z „ Np0, Iq
9:
ˆxt´∆t Ð ˆxt´∆t ` gptq
?"
N,0.1826086956521739,"∆t z
10: return ˆx0"
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.18695652173913044,"3
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS"
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.19130434782608696,"With score-based generative modeling, we can train a score model sθ˚px, tq to generate unconditional
samples from the the prior distribution of medical images ppxq. To solve inverse problems however,
we will need to sample from the posterior ppx | yq. This can be accomplished by conditioning
the original stochastic process txtutPr0,1s on an observation y, yielding a conditional stochastic
process txt | yutPr0,1s. We denote the marginal distribution at t as ptpxt | yq, and our goal is to
sample from p0px0 | yq, the same distribution as ppx | yq by deﬁnition. Much like generating
unconditional samples by solving the reverse-time SDE in Eq. (2), we can reverse the conditional
stochastic process txt | yutPr0,1s to sample from the posterior distribution p0px0 | yq by solving the
following conditional reverse-time SDE (Song et al., 2021):"
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.1956521739130435,"dxt “
“
fptqxt ´ gptq2∇xt log ptpxt | yq
‰
dt ` gptq d ¯wt,
t P r0, 1s.
(4)"
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.2,"The conditional score function ∇xt log ptpxt | yq is a critical part of Eq. (4), yet it is non-trivial to
compute. One solution is to estimate the score function by training a new score model sθ˚pxt, y, tq
that explicitly depends on y (Song et al., 2021; Dhariwal & Nichol, 2021), such that sθ˚pxt, y, tq «
∇xt log ptpxt | yq. However, this requires paired data tpxi, yiquN
i“1 for training and has the same
drawbacks as supervised learning techniques. We do not consider this approach in this work."
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.20434782608695654,"An unsupervised alternative is to approximate the conditional score function with an unconditionally-
trained score model sθ˚pxt, tq « ∇xt log ptpxtq and the measurement distribution ppy | xq. Many
existing works (Song et al., 2021; Kawar et al., 2021; Kadkhodaie & Simoncelli, 2020; Jalal et al.,
2021) have implemented this idea in different ways. However, the methods in Kawar et al. (2021) and
Kadkhodaie & Simoncelli (2020) both require computing the singular value decomposition (SVD)
of A P Rmˆn, which can be difﬁcult for many measurement processes in medical imaging. The
method proposed in Jalal et al. (2021) is only designed for a speciﬁc sampling method called annealed
Langevin dynamics (ALD, Song & Ermon, 2019), which proves to be inferior to more advanced
sampling algorithms such as Predictor-Corrector methods (Song et al., 2021)."
SOLVING INVERSE PROBLEMS WITH SCORE-BASED GENERATIVE MODELS,0.20869565217391303,"In what follows, we propose a new conditional sampling approach for inverse problem solving
with score-based generative models. Our method is computationally efﬁcient for medical image
reconstruction, and is applicable to a large family of iterative sampling methods for score-based
generative models. At a high level, we ﬁrst train an unconditional score model sθ˚px, tq on medical
images without assuming any measurement process. Given an observation y at test time, we form a
stochastic process tytutPr0,1s by adding appropriate noise to y. We then discretize the reverse-time
SDE in Eq. (3) with existing unconditional samplers for sθ˚px, tq, while incorporating the conditional
information from y with a proximal optimization step to generate intermediate samples that are
consistent with tytutPr0,1s."
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.21304347826086956,"3.1
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS"
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.21739130434782608,"Many different measurement processes in medical imaging share same components of computation.
For example, sparse-view CT reconstruction and metal artifact removal for CT both involve computing
the same Radon transform. Similarly, MRI measurement processes require computing the same
spatial Fourier transform regardless of different downsampling ratios. To rigorously characterize this
structure of measurement processes, we propose a special formulation of A that is efﬁcient to obtain"
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.2217391304347826,Published as a conference paper at ICLR 2022
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.22608695652173913,Figure 2: Linear measurement processes for sparse-view CT (left) and undersampled MRI (right).
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.23043478260869565,"in medical imaging applications. Without loss of generality, we assume that the linear operator A has
full rank, i.e., rankpAq “ minpn, mq “ m. The result below gives the alternative formulation of A:
Proposition 1. If rankpAq “ m, then there exist an invertible matrix T P Rnˆn, and a diagonal
matrix Λ P t0, 1unˆn with trpΛq “ m, such that A “ PpΛqT . Here PpΛq P t0, 1umˆn is an
operator that, when multiplied with any vector a P Rn, reduces its dimensionality to m by removing
each i-th element of a for i “ 1, 2, ¨ ¨ ¨ , n if Λii “ 0."
A CONVENIENT FORM OF THE LINEAR MEASUREMENT PROCESS,0.23478260869565218,"We illustrate this decomposition for CT/MRI in Fig. 2. Many measurement processes in medical
imaging share the same T , even if they correspond to different A. For example, T corresponds to
the Radon transform and Fourier transform in sparse-view CT and undersampled MRI respectively,
regardless of the number of measurements, i.e., CT projections and k-space downsampling ratios.
For both sparse-view CT reconstruction and metal artifact removal for CT images, the operator T is
the Radon transform (see Fig. 8). Intuitively, diagpΛq can be viewed as a subsampling mask on the
sinogram/k-space, and PpΛq subsamples the sinogram/k-space into an observation y with a smaller
size according to this subsampling mask. In addition, we note that T ´1 can be efﬁciently implemented
with the inverse Radon transform or the inverse Fourier transform in CT/MRI applications."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2391304347826087,"3.2
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.24347826086956523,"In what follows, we show that the decomposition in Proposition 1 provides an efﬁcient way to generate
approximate samples from the conditional stochastic process txt | yutPr0,1s with an unconditional
score model sθ˚px, tq. The basic idea is to “hijack” the unconditional sampling process of score-
based generative models to incorporate an observed measurement y."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.24782608695652175,"As we have already discussed, it is difﬁcult to directly solve txt | yutPr0,1s for sample generation.
To bypass this difﬁculty, we ﬁrst consider a related stochastic process that is much easier to sample
from. Recall that p0tpxt | x0q “ Npxt | αptqx0, β2ptqIq where αptq and βptq can be derived from
fptq and gptq (Song et al., 2021). Given the unconditional stochastic process txtutPr0,1s, we deﬁne
tytutPr0,1s, where yt “ Axt ` αptqϵ. Unlike txt | yutPr0,1s, the conditional stochastic process
tyt | yutPr0,1s is fully tractable. First, we have y0 “ Ax0 ` αp0qϵ “ Ax0 ` ϵ “ y. Since
p0tpxt | x0q “ Npxt | αptqx0, β2ptqIq, we have xt “ αptqx0 ` βptqz, where z P Rn „ Np0, Iq.
By deﬁnition, yt “ Axt ` αptqϵ, so we have yt “ Apαptqx0 ` βptqzq ` αptqϵ “ αptqpy ´ ϵq `
βptqAz ` αptqϵ “ αptqy ` βptqAz. Therefore, we can easily generate a sample ˆyt „ ptpyt | yq
by ﬁrst drawing z „ Np0, Iq and then computing ˆyt “ αptqy ` βptqAz."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.25217391304347825,"The key of our approach is to modify any existing iterative sampling algorithm designed for the
unconditional stochastic process txtutPr0,1s so that the samples are consistent with tyt | yutPr0,1s. In
general, an iterative sampling process of score-based generative models selects a sequence of time
steps t0 “ t0 ă t1 ă ¨ ¨ ¨ ă tN “ 1u and iterates according to"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2565217391304348,"ˆxti´1 “ hpˆxti, zi, sθ˚pˆxti, tiqq,
i “ N, N ´ 1, ¨ ¨ ¨ , 1,
(5)"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2608695652173913,"where ˆxtN „ πpxq, zi „ Np0, Iq, and θ˚ denotes the parameters in an unconditional score model
sθ˚px, tq. Here the iteration function h takes a noisy sample ˆxti and reduces the noise therein to
generate ˆxti´1, using the unconditional score model sθ˚px, tq. For example, for the Euler-Maruyama
sampler detailed in Algorithm 1, this iteration function is given by"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.26521739130434785,"hpˆxti, zi, sθ˚pˆxti, tiqq “ ˆxti ´ fptiqˆxti{N ` gptiq2sθ˚pˆxti, tiq{N ` gptiqzi{
? N."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.26956521739130435,"Samples obtained by this procedure tˆxtiuN
i“0 constitute an approximation of txtutPr0,1s, where
the last sample ˆxt0 can be viewed as an approximate sample from p0pxq. Most existing sampling"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.27391304347826084,Published as a conference paper at ICLR 2022
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2782608695652174,"Figure 3: (Left) An overview of our method for solving inverse problems with score-based generative
models. (Right) An illustration about how to combine ˆxti and y to form ˆx1
ti."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2826086956521739,"methods for score-based generative models are instances of this iterative sampling paradigm, including
Algorithm 1, ALD (Song & Ermon, 2019), probability ﬂow ODEs (Song et al., 2021) and Predictor-
Corrector samplers (Song et al., 2021)."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.28695652173913044,"To enforce the constraint implied by tyt | yutPr0,1s, we prepend an additional step to the iteration
rule in Eq. (5), leading to"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.29130434782608694,"ˆx1
ti “ kpˆxti, ˆyti, λq
(6)"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.2956521739130435,"ˆxti´1 “ hpˆx1
ti, zi, sθ˚pˆxti, tiqq,
i “ N, N ´ 1, ¨ ¨ ¨ , 1,
(7)"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3,"where ˆxtN „ πpxq, ˆyti „ ptipyti | yq, and 0 ď λ ď 1 is a hyper-parameter. We provide an
illustration of this process in Fig. 3. The iteration function kp¨, ˆyti, λq : Rn Ñ Rn promotes data
consistency by solving a proximal optimization step (Nesterov, 2003; Boyd et al., 2004; Hammernik
et al., 2021) that simultaneously minimizes the distance between ˆx1
ti and ˆxti, and the distance between
ˆx1
ti and the hyperplane tx P Rn | Ax “ ˆytiu, with a hyperparameter 0 ď λ ď 1 balancing between
the two:"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.30434782608695654,"ˆx1
ti “ arg min
zPRn tp1 ´ λq ∥z ´ ˆxti∥2
T ` min
uPRn λ ∥z ´ u∥2
T u
s.t.
Au “ ˆyti.
(8)"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.30869565217391304,"Recall that A “ PpΛqT according to Proposition 1. In the equation above we choose the norm
∥a∥2
T :“ ∥T a∥2
2 to simplify our theoretical analysis. The decomposition in Proposition 1 allows us
to derive a closed-form solution to the optimization problem in Eq. (8), as given below:
Theorem 1. The solution of Eq. (8) can be given by"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3130434782608696,"ˆx1
ti “ T ´1rλΛP´1pΛqˆyti ` p1 ´ λqΛT ˆxti ` pI ´ ΛqT ˆxtis,
(9)"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3173913043478261,where P´1pΛq : Rm Ñ Rn denotes any right inverse of PpΛq.
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3217391304347826,"See Fig. 3 for an illustration of the function ˆx1
ti “ kpˆxti, ˆyti, λq. The right inverse P´1pΛq increases
the dimensionality of a vector a P Rm to n by putting its entries on every index i of an n-dimensional
vector where Λii “ 1. Recall that in sparse-view CT or undersampled MRI, diagpΛq represents a
subsampling mask, and PpΛq subsamples the full sinogram/k-space to generate the observation y. In
this case, P´1pΛq pads the observation y so that it has the same size as the full sinogram/k-space."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.32608695652173914,"When λ “ 0, ˆx1
ti “ kpˆxti, ˆyti, 0q “ ˆxti completely ignores the constraint Aˆx1
ti “ ˆyti, in which
case our sampling method in Eq. (7) performs unconditional generation. On the other hand, when
λ “ 1, ˆx1
ti “ kpˆxti, ˆyti, 1q satisﬁes Aˆx1
ti “ ˆyti exactly. When the measurement is noisy, we
choose 0 ă λ ă 1 to allow slackness in the constraint Aˆx1
ti “ ˆyti. The value of λ is important for
balancing between ˆx1
ti « ˆxti and Aˆx1
ti « ˆyti. In practice, we use Bayesian optimization to tune
this λ automatically on a validation dataset. When the measurement process contains no noise, we
replace ˆxt0 with kpˆxt0, y, 1q at the last sampling step to guarantee Aˆxt0 “ y."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.33043478260869563,"In summary, our method given in Eq. (7) introduces minimal modiﬁcations to an existing iterative
sampling method of score-based generative models. For example, we can convert the sampler in
Algorithm 1 to an inverse problem solver in Algorithm 2 by adding/modifying just three lines of"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3347826086956522,Published as a conference paper at ICLR 2022
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.3391304347826087,"(a) FISTA-TV
(b) cGAN
(c) Neumann
(d) SIN-4c-PRN
(e) Ours
(f) Ground truth"
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.34347826086956523,"Figure 4: Examples of sparse-view CT reconstruction results on LIDC 320 ˆ 320 (Top row) and
LDCT 512 ˆ 512 (Bottom row), all with 23 projections. You may zoom in to view more details."
INCORPORATING A GIVEN OBSERVATION INTO AN UNCONDITIONAL SAMPLING PROCESS,0.34782608695652173,"pseudo-code. Unlike the concurrent work Jalal et al. (2021), our method is not limited to annealed
Langevin dynamics (ALD). As demonstrated in our experiments, we outperform Jalal et al. (2021)
even with the same ALD sampler, and can widen the performance gap further by using more
advanced approaches like the Predictor-Corrector sampler (Song et al., 2021). Unlike Kadkhodaie &
Simoncelli (2020); Kawar et al. (2021), we rely on the efﬁcient alternative representation of A given
in Section 3.1, and do not require expensive SVD computation."
EXPERIMENTS,0.3521739130434783,"4
EXPERIMENTS"
EXPERIMENTS,0.3565217391304348,"We aim to answer the following questions in this section: (1) Can we directly compete with best-in-
class supervised learning techniques for the same measurement process used in their training, even
though our approach is fully unsupervised? (2) Can our method generalize better to new measurement
processes? (3) How do we fare against other unsupervised approaches? To study these questions,
we experiment on several tasks in medical imaging, including sparse-view CT reconstruction, metal
artifact removal (MAR) for CT, and undersampled MRI reconstruction. More experimental details
are provided in Appendix B."
EXPERIMENTS,0.36086956521739133,"Datasets We consider two datasets for CT experiments. The ﬁrst is the Lung Image Database
Consortium (LIDC) image collection dataset (Armato III et al., 2011; Clark et al., 2013) where we
slice the original 3D CT volumes to obtain 130304 2D images of resolution 320 ˆ 320 for training.
The second is the Low Dose CT (LDCT) Image and Projection dataset (Moen et al., 2021) that
contains CT scans of multiple anatomic sites, including head, chest, and abdomen, from which we
generate 47006 2D image slices of resolution 512 ˆ 512 for training. We simulate CT measurements
(sinograms) with a parallel-beam geometry using projection angles equally distributed across 180
degrees. For MAR experiments, we follow Yu et al. (2020) to synthesize metal artifacts. For
undersampled MRI experiments, we use the Brain Tumor Segmentation (BraTS) 2021 dataset (Menze
et al., 2014; Bakas et al., 2017), where we slice 3D MRI volumes to get 297270 images of resolution
240 ˆ 240 as the training dataset. We simulate MRI measurements with Fast Fourier Transform using
a single-coil setup, and follow Zbontar et al. (2018); Knoll et al. (2020) to undersample the k-space
with an equispaced Cartesian mask. The performance is measured on 1000 test images with peak
signal-to-noise ratio (PSNR) and structural similarity (SSIM)."
EXPERIMENTS,0.3652173913043478,"Standard techniques in medical imaging We include two standard learning-free techniques as
baselines for sparse-view CT reconstruction. The ﬁrst is ﬁltered back projection on sparse-view
sinograms, which is denoted by “FBP”. The second is an iterative reconstruction method with total
variation regularization called FISTA-TV (Beck & Teboulle, 2009). For MAR experiments, we
include another learning-free baseline called linear interpolation (LI, Kalender et al., 1987)."
EXPERIMENTS,0.3695652173913043,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.3739130434782609,"Table 1: Results for undersampled MRI reconstruction on BraTS. First two methods are supervised
learning techniques trained with 8ˆ acceleration. The others are unsupervised techniques."
EXPERIMENTS,0.3782608695652174,"Method
24ˆ Acceleration
8ˆ Acceleration
4ˆ Acceleration
PSNRÒ
SSIMÒ
PSNRÒ
SSIMÒ
PSNRÒ
SSIMÒ
Cascade DenseNet
23.39˘2.17
0.765˘0.042
28.35˘2.30
0.845˘0.038
30.97˘2.33
0.902˘0.028
DuDoRNet
18.46˘3.05
0.662˘0.093
37.88˘3.03
0.985˘0.007
30.53˘4.13
0.891˘0.071
Score SDE
27.83˘2.73
0.849˘0.038
35.04˘2.11
0.943˘0.016
37.55˘2.08
0.960˘0.013
Langevin
28.80˘3.21
0.873˘0.039
36.44˘2.28
0.952˘0.016
38.76˘2.32
0.966˘0.012
Ours
29.42˘3.03
0.880˘0.035
37.63˘2.70
0.958˘0.015
39.91˘2.67
0.965˘0.013"
EXPERIMENTS,0.3826086956521739,"Table 2: Results for sparse-view CT reconstruction on LIDC and LDCT. FISTA-TV is a standard
iterative reconstruction method that does not need training. cGAN, Neumann, and SIN-4c-PRN are
supervised learning techniques trained with 23 projection angles."
EXPERIMENTS,0.3869565217391304,"Method
Projections
LIDC 320 ˆ 320
LDCT 512 ˆ 512
PSNRÒ
SSIMÒ
PSNRÒ
SSIMÒ
FBP
23
10.18˘1.38
0.230˘0.072
10.11˘1.19
0.302˘0.078
FISTA-TV
23
20.08˘4.89
0.799˘0.061
21.88˘4.42
0.850˘0.067
cGAN
23
19.83˘3.07
0.479˘0.103
19.90˘2.52
0.545˘0.065
Neumann
23
17.18˘3.79
0.454˘0.128
18.83˘3.29
0.525˘0.073
SIN-4c-PRN
23
30.48˘3.99
0.895˘0.047
34.82˘3.55
0.877˘0.116
10
29.52˘2.63
0.823˘0.061
28.96˘4.41
0.849˘0.086
20
34.40˘2.66
0.895˘0.048
36.80˘4.50
0.936˘0.058
Ours"
EXPERIMENTS,0.391304347826087,"23
35.24˘2.71
0.905˘0.046
37.41˘4.62
0.941˘0.057"
EXPERIMENTS,0.39565217391304347,"Supervised learning baselines
For sparse-view CT on both LIDC and LDCT, we include
cGAN (Ghani & Karl, 2018), Neumann (Gilton et al., 2019), and SIN-4c-PRN (Wei et al., 2020) as
supervised learning baselines. We follow the settings in Wei et al. (2020) and train all methods with
23 projection angles. For MAR, we use cGANMAR (Wang et al., 2018) and SNMAR (Yu et al., 2020)
as the baselines. For undersampled MRI on BraTS, we compare against Cascade DenseNet (Zheng
et al., 2019) and DuDoRNet (Zhou & Zhou, 2020), which are both trained with a 8ˆ acceleration
factor by measuring only 1{8 of the full k-space."
EXPERIMENTS,0.4,"Unsupervised learning baselines For unsupervised techniques, so far only score-based generative
models have witnessed success on clinic data. We compare with several existing methods that apply
score-based generative models to inverse problem solving. Speciﬁcally, we consider the “Langevin”
approach proposed in Jalal et al. (2021), and the “Score SDE” method in Song et al. (2021), where
the former is limited to annealed Langevin dynamics (ALD) sampling, and the latter was based
on a crude approximation to the conditional score function ∇xt log ptpxt | yq in Eq. (4), and was
proposed as a theoretical possibility in Appendix I.4 of Song et al. (2021) without experiments. We
only focus on undersampled MRI for these baselines, since it is the only medical imaging problem
ever tackled with score-based generative models before our work. All methods share the same score
models and only differ in terms of inference. We make sure all sampling algorithms have comparable
number of iteration steps (N in Eqs. (5) and (7))."
EXPERIMENTS,0.4043478260869565,Table 3: MAR results on LIDC.
EXPERIMENTS,0.40869565217391307,"Method
PSNRÒ
SSIMÒ
LI
26.30˘2.62
0.910˘0.028
cGANMAR
27.27˘1.96
0.927˘0.060
SNMAR
27.28˘1.43
0.937˘0.048
Ours
32.16˘2.32
0.939˘0.022"
EXPERIMENTS,0.41304347826086957,"Competing with supervised learning approaches Thanks to
the outstanding sample quality of score-based generative mod-
els, we can achieve comparable or better performance than
best-in-class supervised learning methods even for the same
measurement process used in their training. As shown in Ta-
ble 2, we outperform the top supervised learning technique
SIN-4c-PRN on sparse-view CT reconstruction by a signiﬁcant
margin, on both the LIDC and LDCT datasets. Our results with
20 measurements are even better than supervised learning counterparts with 23 measurements. In
Fig. 4, we provide a visual comparison of the reconstruction quality for various methods, where
it is clear to see that our method can recover more details faithfully. From results in Table 3, we
also outperform the top supervised learning method SNMAR on metal artifact removal. As shown"
EXPERIMENTS,0.41739130434782606,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.4217391304347826,"Figure 5: Performance vs. numbers of measurements. Shaded areas represent standard deviation.
(Left) MRI on BraTS. (Center) CT on LIDC. (Right) Comparing score-based generative models for
undersampled MRI reconstruction on BraTS."
EXPERIMENTS,0.4260869565217391,"in Fig. 7, our method generates images with less artifacts and preserves the structure better. For
undersampled MRI reconstruction results given in Tables 1 and 3, our method is ranked the 2nd for
the case of 8ˆ acceleration, with comparable performance to the top supervised method DuDoRNet."
EXPERIMENTS,0.43043478260869567,"Generalizing to different number of measurements Since our approach is fully unsupervised, we
can naturally apply the same score model to different measurement processes. We ﬁrst consider
changing the number of measurements at the test time, e.g., using different number of projection angles
(resp. different acceleration factors) for sparse-view CT (resp. undersampled MRI) reconstruction.
As shown in Table 1 and Fig. 5 (Left), we achieve the best performance on undersampled MRI for
both 24ˆ and 4ˆ acceleration factors, whereas DuDoRNet fails to generalize when the acceleration
factor changes. The other supervised learning approach Cascade DenseNet demonstrates limited
adaptability by building a model architecture inspired by the physical measurement process of MRI,
but fails to yield top-level performance. For sparse-view CT reconstruction, all supervised learning
methods struggle to generalize to different projection angles, as shown in Fig. 5 (Center)."
EXPERIMENTS,0.43478260869565216,"Generalizing to different measurement processes in CT We can perform both sparse-view CT
reconstruction and metal artifact removal (MAR) with a single score model trained on CT images.
These two tasks are inverse problems in CT imaging with different measurement processes A, but
they share the same T in the decomposition of Proposition 1. We provide a visualization of the
measurement process corresponding to MAR in Fig. 8. As shown in Table 3, we can outperform
supervised learning techniques speciﬁcally designed and trained for MAR, while using the same
score model used in sparse-view CT reconstruction on LIDC."
EXPERIMENTS,0.4391304347826087,"Comparing against existing score-based methods We compare our method against Langevin (Jalal
et al., 2021) and Score SDE (Song et al., 2021) for undersampled MRI reconstruction on BraTS. Two
variants of our approach are considered, which respectively use annealed Langevin dynamics (ALD)
and the Predictor-Corrector (PC) sampler for score-based generative models as the backend. We
denote the former by “ALD + Ours”, and the latter by “PC + Ours” (our default method for all other
experiments). Recall that Langevin uses ALD as the sampler, same as “ALD + Ours”. All results are
provided in Fig. 5 (Right). We observe that “ALD + Ours” uniformly outperform Langevin and Score
SDE across all numbers of measurements in the experiment. Moreover, “PC + Ours” can further
improve “ALD + Ours”, demonstrating the power of switching to more advanced sampling methods
of score-based generative models in our proposed approach."
CONCLUSION,0.4434782608695652,"5
CONCLUSION"
CONCLUSION,0.44782608695652176,"We propose a new method to solve linear inverse problems with score-based generative models. Our
method is fully unsupervised, requires no paired data for training, can ﬂexibly adapt to different
measurement processes at test time, and only requires minimal modiﬁcations to a large number of
existing sampling methods of score-based generative models. Empirical results demonstrate that our
method can match or outperform existing supervised learning counterparts on image reconstruction
for sparse-view CT and undersampled MRI, and has better generalization to new measurement
processes, such as using a different number of projections or downsampling ratios in CT/MRI, and
tackling both sparse-view CT reconstruction and metal artifact removal with a single model."
CONCLUSION,0.45217391304347826,Published as a conference paper at ICLR 2022
AUTHOR CONTRIBUTIONS,0.45652173913043476,AUTHOR CONTRIBUTIONS
AUTHOR CONTRIBUTIONS,0.4608695652173913,"Yang Song designed the project, wrote the paper, and ran all experiments for score-based generative
models. Liyue Shen preprocessed data, ran all baseline experiments, and helped write the paper. Lei
Xing and Stefano Ermon supervised the project, provided valuable feedback, and helped edit the
paper."
AUTHOR CONTRIBUTIONS,0.4652173913043478,ACKNOWLEDGMENTS
AUTHOR CONTRIBUTIONS,0.46956521739130436,"YS is supported by the Apple PhD Fellowship in AI/ML. LS is supported by the Stanford Bio-X
Graduate Student Fellowship. This research was supported by NSF (#1651565, #1522054, #1733686),
ONR (N000141912145), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), Sloan Fellowship,
and Google TPU Research Cloud. This research was also supported by NIH/NCI (1R01 CA256890
and 1R01 CA227713)."
REFERENCES,0.47391304347826085,REFERENCES
REFERENCES,0.4782608695652174,"Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock, and Anders C Hansen. On instabilities
of deep learning in image reconstruction and the potential costs of ai. Proceedings of the National
Academy of Sciences, 117(48):30088–30095, 2020."
REFERENCES,0.4826086956521739,"Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer,
Anthony P Reeves, Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A Hoffman,
et al. The lung image database consortium (lidc) and image database resource initiative (idri): a
completed reference database of lung nodules on ct scans. Medical physics, 38(2):915–931, 2011."
REFERENCES,0.48695652173913045,"Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby,
John B Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas
glioma mri collections with expert segmentation labels and radiomic features. Scientiﬁc data, 4(1):
1–13, 2017."
REFERENCES,0.49130434782608695,"Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183–202, 2009."
REFERENCES,0.4956521739130435,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004."
REFERENCES,0.5,"Thorsten M Buzug. Computed tomography. In Springer handbook of medical technology, pp.
311–342. Springer, 2011."
REFERENCES,0.5043478260869565,"Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen
Moore, Stanley Phillips, David Mafﬁtt, Michael Pringle, et al. The cancer imaging archive (tcia):
maintaining and operating a public information repository. Journal of digital imaging, 26(6):
1045–1057, 2013."
REFERENCES,0.508695652173913,"Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. arXiv preprint
arXiv:2105.05233, 2021."
REFERENCES,0.5130434782608696,"Muhammad Usman Ghani and W Clem Karl. Deep learning-based sinogram completion for low-dose
ct. In 2018 IEEE 13th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP),
pp. 1–5. IEEE, 2018."
REFERENCES,0.5173913043478261,"Davis Gilton, Greg Ongie, and Rebecca Willett. Neumann networks for linear inverse problems in
imaging. IEEE Transactions on Computational Imaging, 6:328–343, 2019."
REFERENCES,0.5217391304347826,"Kerstin Hammernik, Jo Schlemper, Chen Qin, Jinming Duan, Ronald M Summers, and Daniel Rueck-
ert. Systematic evaluation of iterative deep neural networks for fast parallel mri reconstruction
with sensitivity-weighted coil combination. Magnetic Resonance in Medicine, 2021."
REFERENCES,0.5260869565217391,"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5304347826086957,Published as a conference paper at ICLR 2022
REFERENCES,0.5347826086956522,"Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jonathan I Tamir.
Robust compressed sensing mri with deep generative priors. arXiv preprint arXiv:2108.01368,
2021."
REFERENCES,0.5391304347826087,"Zahra Kadkhodaie and Eero P Simoncelli. Solving linear inverse problems using the prior implicit in
a denoiser. arXiv preprint arXiv:2007.13640, 2020."
REFERENCES,0.5434782608695652,"Willi A Kalender, Robert Hebel, and Johannes Ebersberger. Reduction of ct artifacts caused by
metallic implants. Radiology, 164(2):576–577, 1987."
REFERENCES,0.5478260869565217,"Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochas-
tically. arXiv preprint arXiv:2105.14951, 2021."
REFERENCES,0.5521739130434783,"Daniil Kazantsev and Nicola Wadeson. Tomographic model-based reconstruction (tomobar) software
for high resolution synchrotron x-ray tomography. CT Meeting, 2020."
REFERENCES,0.5565217391304348,"Daniil Kazantsev, Edoardo Pasca, Martin J Turner, and Philip J Withers. Ccpi-regularisation toolkit
for computed tomographic image reconstruction with proximal splitting algorithms. SoftwareX, 9:
317–323, 2019."
REFERENCES,0.5608695652173913,"Florian Knoll, Jure Zbontar, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio,
Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastmri: A publicly
available raw k-space and dicom dataset of knee images for accelerated mr image reconstruction
using machine learning. Radiology: Artiﬁcial Intelligence, 2(1):e190007, 2020."
REFERENCES,0.5652173913043478,"Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas Vasanawala, Greg Zaharchuk, Marcus
Alley, Neil Thakur, Song Han, William Dally, John M Pauly, et al. Deep generative adversarial
networks for compressed sensing automates mri. arXiv preprint arXiv:1706.00051, 2017."
REFERENCES,0.5695652173913044,"Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin
Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain
tumor image segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):
1993–2024, 2014."
REFERENCES,0.5739130434782609,"Taylor R Moen, Baiyu Chen, David R Holmes III, Xinhui Duan, Zhicong Yu, Lifeng Yu, Shuai Leng,
Joel G Fletcher, and Cynthia H McCollough. Low-dose ct image and projection dataset. Medical
physics, 48(2):902–911, 2021."
REFERENCES,0.5782608695652174,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.5826086956521739,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019."
REFERENCES,0.5869565217391305,"Matteo Ronchetti. Torchradon: Fast differentiable routines for computed tomography. arXiv preprint
arXiv:2009.14788, 2020."
REFERENCES,0.591304347826087,"Simo Särkkä and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge
University Press, 2019."
REFERENCES,0.5956521739130435,"Liyue Shen, Wei Zhao, and Lei Xing. Patient-speciﬁc reconstruction of volumetric computed
tomography images from a single projection view via deep learning. Nature biomedical engineering,
3(11):880–888, 2019."
REFERENCES,0.6,"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918–11930, 2019."
REFERENCES,0.6043478260869565,Published as a conference paper at ICLR 2022
REFERENCES,0.6086956521739131,"Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.6130434782608696,"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=PxTIG12RRHS."
REFERENCES,0.6173913043478261,"Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
Computation, 23(7):1661–1674, 2011."
REFERENCES,0.6217391304347826,"Marinus T Vlaardingerbroek and Jacques A Boer. Magnetic resonance imaging: theory and practice.
Springer Science & Business Media, 2013."
REFERENCES,0.6260869565217392,"Jianing Wang, Yiyuan Zhao, Jack H Noble, and Benoit M Dawant. Conditional generative adversarial
networks for metal artifact reduction in ct images of the ear. In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pp. 3–11. Springer, 2018."
REFERENCES,0.6304347826086957,"Haoyu Wei, Florian Schiffers, Tobias Würﬂ, Daming Shen, Daniel Kim, Aggelos K Katsaggelos, and
Oliver Cossairt. 2-step sparse-view ct reconstruction with a domain-speciﬁc perceptual network.
arXiv preprint arXiv:2012.04743, 2020."
REFERENCES,0.6347826086956522,"Tobias Würﬂ, Mathis Hoffmann, Vincent Christlein, Katharina Breininger, Yixin Huang, Mathias
Unberath, and Andreas K Maier. Deep learning computed tomography: Learning projection-
domain weights from image domain in limited angle problems. IEEE transactions on medical
imaging, 37(6):1454–1463, 2018."
REFERENCES,0.6391304347826087,"Lequan Yu, Zhicheng Zhang, Xiaomeng Li, and Lei Xing. Deep sinogram completion with image
prior for metal artifact reduction in ct images. IEEE Transactions on Medical Imaging, 40(1):
228–238, 2020."
REFERENCES,0.6434782608695652,"Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J. Muckley,
Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J. Geras,
Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael
Rabbat, Pascal Vincent, Naﬁssa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C. Lawrence
Zitnick, Michael P. Recht, Daniel K. Sodickson, and Yvonne W. Lui. fastMRI: An open dataset
and benchmarks for accelerated MRI. 2018."
REFERENCES,0.6478260869565218,"Hao Zheng, Faming Fang, and Guixu Zhang. Cascaded dilated dense network with two-step data
consistency for mri reconstruction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf."
REFERENCES,0.6521739130434783,"Bo Zhou and S Kevin Zhou. Dudornet: Learning a dual-domain recurrent network for fast mri
reconstruction with deep t1 prior. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 4273–4282, 2020."
REFERENCES,0.6565217391304348,"Bo Zhu, Jeremiah Z Liu, Stephen F Cauley, Bruce R Rosen, and Matthew S Rosen. Image recon-
struction by domain-transform manifold learning. Nature, 555(7697):487–492, 2018."
REFERENCES,0.6608695652173913,Published as a conference paper at ICLR 2022
REFERENCES,0.6652173913043479,"A
PROOFS"
REFERENCES,0.6695652173913044,"Proposition 1. If rankpAq “ m, then there exist an invertible matrix T P Rnˆn, and a diagonal
matrix Λ P t0, 1unˆn with trpΛq “ m, such that A “ PpΛqT . Here PpΛq P t0, 1umˆn is an
operator that, when multiplied with any vector a P Rn, reduces its dimensionality to m by removing
each i-th element of a for i “ 1, 2, ¨ ¨ ¨ , n if Λii “ 0."
REFERENCES,0.6739130434782609,"Proof. Let A
“
paT
1, aT
2, ¨ ¨ ¨ , aT
mq
P
Rmˆn.
Since A has full rank, the row vectors
ta1, a2, ¨ ¨ ¨ , amu are linearly independent. We can therefore extend them to a total of n linearly
independent vectors, i.e., ta1, a2, ¨ ¨ ¨ , am, b1, ¨ ¨ ¨ , bn´mu. Due to the linear independence, we
know T “ paT
1, aT
2, ¨ ¨ ¨ , aT
m, bT
1, ¨ ¨ ¨ , bT
n´mq P Rnˆn has full rank and is invertible. Next, we deﬁne"
REFERENCES,0.6782608695652174,"Λ “ diagp1, 1, ¨ ¨ ¨ , 1
loooomoooon"
REFERENCES,0.6826086956521739,"m
, 0, 0, ¨ ¨ ¨ , 0
loooomoooon"
REFERENCES,0.6869565217391305,"n´m
q,"
REFERENCES,0.691304347826087,"where diag converts a vector to a diagonal matrix. Clearly trpΛq “ m and A “ PpΛqT , which
completes the proof."
REFERENCES,0.6956521739130435,"Lemma 1. Let P´1pΛq : Rm Ñ Rn be any right inverse of PpΛq : Rn Ñ Rm. For any u P Rn
and ˆyt P Rm, we have"
REFERENCES,0.7,PpΛqT u “ ˆyt ðñ ΛT u “ ΛP´1pΛqˆyt
REFERENCES,0.7043478260869566,"Proof. By the deﬁnition of PpΛq, we have PpΛq “ PpΛqΛ, and"
REFERENCES,0.7086956521739131,"@a P Rn, b P Rn :
PpΛqa “ PpΛqb ðñ Λa “ Λb.
(10)"
REFERENCES,0.7130434782608696,"To prove the “if” direction, we note that"
REFERENCES,0.717391304347826,"ΛT u “ ΛP´1pΛqˆyt ùñ PpΛqΛT u “ PpΛqΛP´1pΛqˆyt
ùñ PpΛqT u “ PpΛqP´1pΛqˆyt
ùñ PpΛqT u “ ˆyt."
REFERENCES,0.7217391304347827,"To prove the “only if” direction, we have"
REFERENCES,0.7260869565217392,PpΛqT u “ ˆyt ùñ PpΛqT u “ PpΛqP´1pΛqˆyt
REFERENCES,0.7304347826086957,"piq
ùñ ΛT u “ ΛP´1pΛqˆyt,"
REFERENCES,0.7347826086956522,where (i) is due to the property in Eq. (10). This completes the proof for both directions.
REFERENCES,0.7391304347826086,Theorem 1. The solution of Eq. (8) can be given by
REFERENCES,0.7434782608695653,"ˆx1
ti “ T ´1rλΛP´1pΛqˆyti ` p1 ´ λqΛT ˆxti ` pI ´ ΛqT ˆxtis,
(9)"
REFERENCES,0.7478260869565218,where P´1pΛq : Rm Ñ Rn denotes any right inverse of PpΛq.
REFERENCES,0.7521739130434782,Proof. The optimization objective function in Eq. (8) can be written as
REFERENCES,0.7565217391304347,"p1 ´ λq ∥z ´ ˆxt∥2
T ` λ ∥z ´ u∥2
T
“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` λ ∥T z ´ T u∥2
2
“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` λ ∥ΛT pz ´ uq ` pI ´ ΛqT pz ´ uq∥2
2
“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` λ ∥ΛT pz ´ uq∥2
2 ` λ ∥pI ´ ΛqT pz ´ uq∥2
2
“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` λ
ΛT z ´ ΛP´1pΛqˆyt
2
2 ` λ ∥pI ´ ΛqT pz ´ uq∥2
2"
REFERENCES,0.7608695652173914,Published as a conference paper at ICLR 2022
REFERENCES,0.7652173913043478,"Figure 6: SSIM vs. numbers of measurements. Shaded areas represent standard deviation. (Left) MRI
on BraTS. (Center) CT on LIDC. (Right) Comparing score-based generative models for undersampled
MRI reconstruction on BraTS."
REFERENCES,0.7695652173913043,"(a) Metal artifact
(b) LI
(c) cGANMAR
(d) SNMAR
(e) Ours
(f) Ground truth"
REFERENCES,0.7739130434782608,Figure 7: Examples of metal artifact removal on LIDC. You may zoom in to view more details.
REFERENCES,0.7782608695652173,"Since Au “ ˆyt, we have PpΛqT u “ ˆyt and equivalently ΛT u “ ΛP´1pΛqˆyt due to Lemma 1.
This constraint does not restrict the value of pI ´ ΛqT u. Therefore, when Au “ ˆyt, we have"
REFERENCES,0.782608695652174,"∥z ´ ˆxt∥2
T ` min
u p1 ´ λqλ ∥z ´ u∥2
T"
REFERENCES,0.7869565217391304,"“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` min
u λ
ΛT z ´ ΛP´1pΛqˆyt
2
2 ` λ ∥pI ´ ΛqT pz ´ uq∥2
2"
REFERENCES,0.7913043478260869,"“p1 ´ λq ∥T z ´ T ˆxt∥2
2 ` λ
ΛT z ´ ΛP´1pΛqˆyt
2
2
“p1 ´ λq ∥ΛT z ´ ΛT ˆxt∥2
2 ` λ
ΛT z ´ ΛP´1pΛqˆyt
2
2 ` p1 ´ λq ∥pI ´ ΛqT z ´ pI ´ ΛqT ˆxt∥2
2 ."
REFERENCES,0.7956521739130434,This simpliﬁes the optimization problem in Eq. (8) to
REFERENCES,0.8,"min
z p1 ´ λq ∥ΛT z ´ ΛT ˆxt∥2
2 ` λ
ΛT z ´ ΛP´1pΛqˆyt
2
2 ` p1 ´ λq ∥pI ´ ΛqT z ´ pI ´ ΛqT ˆxt∥2
2 ,"
REFERENCES,0.8043478260869565,which is minimizing a quadratic function of z. The optimal solution z˚ is thus in closed form:
REFERENCES,0.808695652173913,z˚ “ T ´1rpI ´ ΛqT ˆxt ` p1 ´ λqΛT ˆxt ` λΛP´1pΛqˆyts.
REFERENCES,0.8130434782608695,"According to the deﬁnition, ˆx1
t “ z˚, whereby the proof is completed."
REFERENCES,0.8173913043478261,"B
ADDITIONAL EXPERIMENTAL DETAILS"
REFERENCES,0.8217391304347826,"B.1
ADDITIONAL RESULTS"
REFERENCES,0.8260869565217391,"In Fig. 6, we provide SSIM results versus the number of measurements for multiple methods and
tasks. In general, the SSIM curves have very similar trends to the PSNR curves in Fig. 5. We
additionally provide a visualization of metal artifact removal results in Fig. 7."
REFERENCES,0.8304347826086956,"B.2
THE TASK OF METAL ARTIFACT REMOVAL"
REFERENCES,0.8347826086956521,"Metallic implants in an object can cause strong metal artifacts in CT imaging. As shown in Fig. 8, the
source of artifacts come from extremely bright regions in the sinogram, called metal traces. To reduce
or ideally remove metal artifacts from a CT image, we remove metal traces from the sinogram and"
REFERENCES,0.8391304347826087,Published as a conference paper at ICLR 2022
REFERENCES,0.8434782608695652,Figure 8: The linear measurement process of metal artifact removal.
REFERENCES,0.8478260869565217,"leverage the data prior to complete the sinogram. As a result, metal artifact removal can be viewed
as an inverse problem, where the measurement process gives the full sinogram except for the metal
trace region, and our goal is to reconstruct the full CT image using this partially known sinogram,
which will be artifact-free assuming perfect inpainting of the sinogram."
REFERENCES,0.8521739130434782,"B.3
DETAILS OF DATASETS"
REFERENCES,0.8565217391304348,"CT datasets
We conduct experiments of 2D CT image reconstruction on two datasets. First, the
Lung Image Database Consortium image collection (LIDC) (Armato III et al., 2011; Clark et al.,
2013) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans for
lung cancer detection and diagnosis, which contains 1018 cases. Second, the Low Dose CT Image
and Projection dataset (LDCT) (Clark et al., 2013; Moen et al., 2021) involves CT images of multiple
anatomic sites, including 99 head CT scans, 100 chest CT scans, and 100 abdomen CT scans. Note
that for the LDCT dataset, we only use the full-dose CT images in our experiments. In CT image
processing, we convert the Hounsﬁeld units from dicom ﬁles to the attenuation coefﬁcients and set the
background pixels to zero. Then, 2D CT images are sliced from 3D CT volumes. The sinograms are
simulated from 2D CT images based on parallel-beam geometry with different number of projection
angles that are equally distributed across 180 degrees."
REFERENCES,0.8608695652173913,"MRI dataset
The Brain Tumor Segmentation (BraTS) 2021 dataset (Menze et al., 2014; Bakas
et al., 2017) collected for the image segmentation challenge contains 2000 cases (8000 MRI scans),
where each case has four different MR contrasts: native (T1), post-contrast T1-weighted (T1Gd),
T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR). For each 3D MR
volume, we extract 2D slices from 3D volumes and simulate k-space data by Fast Fourier Transform.
To reconstruct MR images, we follow Knoll et al. (2020); Zbontar et al. (2018) to undersample
k-space data with an equispaced Cartesian mask, where the center k-space is fully sampled while the
left k-space is under-sampled by equispaced columns."
REFERENCES,0.8652173913043478,"B.4
DETAILS OF SCORE-BASED GENERATIVE MODELS"
REFERENCES,0.8695652173913043,"We use the NCSN++ model architecture in Song et al. (2021), and perturb the data with the Variance
Exploding (VE) SDE. Our training procedure follows that of Song et al. (2021). Instead of generating
samples according to the numerical SDE solver in Algorithm 1, we use the Predictor-Corrector (PC)
sampler as described in Song et al. (2021) since it generally has better performance for VE SDEs. In
PC samplers, the predictor refers to a numerical solver for the reverse-time SDE while the corrector
can be any Markov chain Monte Carlo (MCMC) method that only depends on the scores. One such
MCMC method considered in this work is Langevin dynamics, whereby we transform any initial
sample xp0q to an approximate sample from ptpxq via the following procedure:"
REFERENCES,0.8739130434782608,"xpi`1q Ð xpiq ` ϵ∇x log ptpxpiqq `
?"
REFERENCES,0.8782608695652174,"2ϵ zpiq,
i “ 0, 1, ¨ ¨ ¨ , N ´ 1.
(11)"
REFERENCES,0.8826086956521739,"Here N P Ną0, ϵ ą 0, and zpiq „ Np0, Iq. The theory of Langevin dynamics guarantees that in the
limit of N Ñ 8 and ϵ Ñ 0, xpNq is a sample from ptpxq under some regularity conditions. Note that
Langevin dynamics only requires the knowledge of ∇x log ptpxq, which can be approximated using
the time-dependent score model sθ˚px, tq. In PC samplers, each predictor step immediately follows
multiple consecutive corrector steps, all using the same sθ˚px, tq evaluated at the same t. This jointly
ensures that our intermediate sample at t is approximately distributed according to ptpxq. As shown"
REFERENCES,0.8869565217391304,Published as a conference paper at ICLR 2022
REFERENCES,0.8913043478260869,"in Song et al. (2021), PC sampling often outperforms numerical solvers for the reverse-time SDE,
especially when the forward SDE in Eq. (1) is a VE SDE. In order to use PC samplers for inverse
problem solving, our modiﬁcation is similar to the change made in Algorithm 2 for Algorithm 1.
Speciﬁcally, we run line 4 & 5 in Algorithm 2 before every corrector or predictor step."
REFERENCES,0.8956521739130435,"When comparing our approach to previous methods with score-based generative models, we use
the same score model to isolate the confounding factors in model training and architecture design.
Moreover, we make sure the total cost of sampling is comparable across different methods. For the
ALD sampler used in Jalal et al. (2021), we use 700 noise scales with 3 steps of Langevin dynamics
per noise scale, resulting in a total of 700ˆ3 “ 2100 steps that require score function evaluation. For
the PC sampler, we use 1000 noise scales and 1 step of Langevin dynamics per noise scale, totalling
1000 ` 1000 “ 2000 steps of score model evaluation."
REFERENCES,0.9,"For PC samplers, the step size ϵ in Langevin dynamics is determined by a signal-to-noise ratio η. For
all methods, we tune η and λ in Eq. (8) with 100 steps of Bayesian optimization on a validation dataset,
and report the results on the test dataset with the optimal parameters. We use the ax-platform
toolkit for Bayesian optimization. The optimal parameters in our experiments are given by"
REFERENCES,0.9043478260869565,"• Sparse-view CT on LIDC 320 ˆ 320: η “ 0.246, λ “ 0.841."
REFERENCES,0.908695652173913,"• Metal artifact removal on LIDC 320 ˆ 320: η “ 0.209, λ “ 0.227."
REFERENCES,0.9130434782608695,"• Sparse-view CT on LDCT 512 ˆ 512: η “ 0.4, λ “ 0.72."
REFERENCES,0.9173913043478261,"• Accelerated MRI on BraTS 240 ˆ 240: η “ 0.577, λ “ 0.982."
REFERENCES,0.9217391304347826,"B.5
TRAINING DETAILS OF BASELINE MODELS"
REFERENCES,0.9260869565217391,"B.5.1
BASELINE MODELS FOR SPARSE-VIEW CT RECONSTRUCTION"
REFERENCES,0.9304347826086956,"FBP
Filtered back projection (FBP) is a standard way for CT image reconstruction, which simply
put the projections (sinogram) back to the image space based on the corresponding projection angles
and geometry to get an approximated estimation of the unknown image. Usually, a high-pass ﬁlter,
ramp ﬁlter is used to eliminate the blurring during this process. In our experiments, we conduct FBP
on sparse-view sinograms using the torch radon toolbox (Ronchetti, 2020)."
REFERENCES,0.9347826086956522,"FISTA-TV
FISTA-TV is a fast iterative shrinkage-thresholding algorithm (FISTA) for solving
linear inverse problems in image processing (Beck & Teboulle, 2009). It adopts a total variation (TV)
term as the regularization in the optimization procedure. Each optimization iteration involves a matrix-
vector multiplication followed by a shrinkage-threshold step. In experiments, FISTA is implemented
using the tomobar toolbox (Kazantsev & Wadeson, 2020) with the regularization using the CCPi
regularisation toolkit (Kazantsev et al., 2019). We run 300 iterations for reconstructing each CT image
with regularization parameter 0.001. Considering the nature of iterative reconstruction in FISTA, it
is quite natural to generalize this method to different number of projections for reconstructing CT
images. In experiments of generalizing to different number of measurements, FISTA method takes
as input the sinogram with different numbers of projections and the corresponding angles for these
input projections for the iterative procedure."
REFERENCES,0.9391304347826087,"cGAN
Conventional iterative CT reconstruction algorithms like FISTA are typically slow due
to their iterative nature. Ghani & Karl (2018) proposed to cast sparse-view CT reconstruction as
a sinogram inpainting problem. Speciﬁcally, it used a conditional generative adversarial network
(cGAN) to ﬁrst complete the sinogram data prior to reconstructing CT images, thereby avoiding the
costly iterative tomographic processing. However, the imperfect sinogram inpainting may further
cause image artifacts. Speciﬁcally, cGAN model takes zero-padded sparse-view sinogram with 23
projections as input and generates the completed full-angle sinogram with 180 projections. The cGAN
model was implemented using PyTorch (Paszke et al., 2019) and trained using a batchsize of 64 and
learning rate of 0.0001 with 50 epochs in total. In experiments of generalizing to different number
of measurements, we deployed the trained cGAN model by zero-padding sparse-view sinogram
with different numbers of projections to full-view sinogram as the input. After obtaining the output
inpainted sinogram, we replace the corresponding projections in the output based on the ground truth
projections in the input. Finally, the images were reconstructed from the overlayed sinogram. Note"
REFERENCES,0.9434782608695652,Published as a conference paper at ICLR 2022
REFERENCES,0.9478260869565217,"that we trained the model using 23 projections and tested it on other projection settings to evaluate
the generalization."
REFERENCES,0.9521739130434783,"SIN-4c-PRN
To further reduce the artifacts in both sinogram and image space, SIN-4c-PRN (Wei
et al., 2020) proposed a two-step sparse-view CT reconstruction model. It involves a sinogram
inpainting network (SIN) to generate super-resolved sinograms with different number of projections,
and then a post-processing reﬁning network (PRN) to further remove image artifacts. Both networks
are connected through a ﬁltered back-projection operation (FBP). Speciﬁcally, SIN model takes 23-
view sinogram as input to ﬁstly upsample to full-view sinogram and then generate sinograms through
network for 23, 45, 90, 180 projections respectively. FBP transforms these generated sinograms to
image space, which was then concatenated and feed into PRN model for reﬁnement. The framework
was implemented using PyTorch (Paszke et al., 2019) while FBP operation was implemented using
. SIN model was trained using a batchsize of 20 and learning rate of 0.0001, while PRN model
was trained using a batchsize of 15 and learning rate of 0.0001. Considering that LIDC dataset
is much larger than LDCT dataset, the SIN-4c-PRN model was trained for 30 epochs on LIDC
dataset and 50 epochs on LDCT dataset. To deploy the trained SIN model to different numbers of
measurements, the sinograms with various number of projections are taken as the input for SIN
model to generate multi-view sinograms, which were also overlayed with corresponding ground truth
projections in inputs. The generated multi-view sinograms are then used for PRN model inference.
Since SIN-4c-PRN model involves the dual-domain learning in both sinogram and image spaces
to remove artifacts, and generates multi-scale sinograms during sinogram inpainting, it shows a
better generalization to different numbers of measurements compared with cGAN model as shown in
Figure 5 and Figure 6."
REFERENCES,0.9565217391304348,"Neumann
Meanwhile, in another parallel direction, researchers proposed to learn the regularizer
used in optimization from training data, outperforming traditional regularizers. Speciﬁcally, Gilton
et al. (2019) presented an end-to-end, data-driven method for learning a nonlinear regularizer for
solving inverse problems inspired by the Neumann series, called Neumann network. Neumann
network was implemented using PyTorch (Paszke et al., 2019). Due to GPU memory constraints, the
model training used the batchsize of 5 on LIDC dataset and the batchsize of 2 on LDCT dataset. The
initial learning rate was 0.00001 with an exponential learning rate decay. The network was trained
with 15 training epochs on both datasets."
REFERENCES,0.9608695652173913,"B.5.2
BASELINE MODELS FOR UNDERSAMPLED MRI RECONSTRUCTION"
REFERENCES,0.9652173913043478,"DuDoRNet
Zhou & Zhou (2020) proposed a dual domain recurrent network (DuDoRNet) to
simultaneously recover k-space data and images for MRI reconstruction, in order to address aliasing
artifacts in both frequency and image domains. The original model in Zhou & Zhou (2020) also
embedded a deep T1 prior to make use of fully-sampled short protocol (T1) as complementary
information. For a fair comparison with other supervised learning approaches, in our experiments,
we do not include this additional information but train the DuDoRNet model without T1 prior. The
DuDoRNet was trained using a batchsize of 6 and a learning rate of 0.0005 with 5 training epochs.
In experiments of generalizing to different number of measurements, we trained the model with an
acceleration factor of 8 and deployed the trained model to other acceleration factors during testing.
Speciﬁcally, for inference, we use different Cartesian masking function corresponding to different
acceleration factors or down-sampling ratios to sub-sample the k-space data for the network input
with the corresponding initial reconstructed image with zero-padding k-space."
REFERENCES,0.9695652173913043,"Cascade DenseNet
To reconstruct de-aliased MR images from under-sampled k-space data, Zheng
et al. (2019) proposed a cascaded dilated dense network (CDDN) for MRI reconstruction, based
on stacked dense blocks with residual connections while using the zero-ﬁlled MR image as inputs.
Speciﬁcally, they used a two-step data consistency layer for k-space correction, and replaced corre-
sponding phase-coding lines of the generated image with the original sampled k-space data after each
block. In experiments, we trained the model using a batchsize of 8 and a learning rate of 0.0001, with
5 epochs on BraTS dataset. In experiments of generalizing to different number of measurements, we
trained the model with an acceleration factor of 8 and deployed the trained model to other acceleration
factors during testing. Similarly, different masking functions corresponding to different acceleration
factors were used to sub-sample k-space data to get network inputs. From results, we observe that"
REFERENCES,0.9739130434782609,Published as a conference paper at ICLR 2022
REFERENCES,0.9782608695652174,"Cascaded DenseNet generalizes better to more measurements than DuDoRNet as shown in Figure 5
and Figure 6."
REFERENCES,0.9826086956521739,"B.5.3
BASELINE MODELS FOR METAL ARTIFACT REMOVAL"
REFERENCES,0.9869565217391304,"LI
One straightforward way for reducing metal artifacts is to complete or inpaint the metal-affected
missing regions in sinogram directly through linear interpolation (Kalender et al., 1987). This
method does not need any network training. However, the imperfect completion of sinogram may
introduce secondary artifacts to the reconstructed image. In our experiments setting, to ﬁt for the
practical applications in real world, we assume the ground truth metal trace and mask information are
unknown, which can only be estimated by a rough thresholding in artifacts-affected images. We use
the estimated metal mask and metal trace for linear interpolation baseline."
REFERENCES,0.991304347826087,"cGANMAR
Wang et al. (2018) proposed a conditional generative adversarial network (cGAN)-
based approach for metal artifacts reduction (MAR) in CT. Speciﬁcally, cGANMAR network learns
the mapping directly from the artifacts-affected CTs to artifacts-free CTs through reﬁnement in image
space. The cGANMAR model was implemented using PyTorch (Paszke et al., 2019) and was trained
with the batchsize of 64 and the learning rate of 0.0001. The network was trained with 400 epochs."
REFERENCES,0.9956521739130435,"SNMAR
Yu et al. (2020) proposed a sinogram completion neural network (SinoNet) to recover the
metal-affected projections. Especially, it leveraged the learning in both sinogram domain and image
domain by using a prior network to generate a good prior image to guide sinogram learning. Note
that in original setting, SNMAR required linear interpolated sinogram and CT as inputs and used
ground truth metal trace and mask information to generated them. But in our method, we assume
the ground truth metal trace and mask information are unknown according to practical scenario and
estimate it by a rough thresholding, which will introduce estimation errors. In SNMAR experiments,
we still follow the original setting to guarantee the best performance of this baseline method for a
strong comparison. We trained the SNMAR using the batchsize of 64 and the learning rate of 0.0001,
with a total of 100 training epochs."
