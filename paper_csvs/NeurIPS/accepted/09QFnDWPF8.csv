Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004901960784313725,"Inverse problems are paramount in Science and Engineering. In this paper, we con-
sider the setup of Statistical Inverse Problem (SIP) and demonstrate how Stochastic
Gradient Descent (SGD) algorithms can be used to solve linear SIP. We provide
consistency and finite sample bounds for the excess risk. We also propose a modi-
fication for the SGD algorithm where we leverage machine learning methods to
smooth the stochastic gradients and improve empirical performance. We exem-
plify the algorithm in a setting of great interest nowadays: the Functional Linear
Regression model. In this case we consider a synthetic data example and a classifi-
cation problem for predicting the main activity of bitcoin addresses based on their
balances."
INTRODUCTION,0.00980392156862745,"1
Introduction"
INTRODUCTION,0.014705882352941176,"Inverse Problems (IP) might be described as the search of an unknown parameter (that could be a
function) that satisfies a given, known equation. Considering the notation:"
INTRODUCTION,0.0196078431372549,"y = A[f ◦] + noise,"
INTRODUCTION,0.024509803921568627,"where f ◦and y are elements of given Hilbert spaces, we would like to compute (or estimate) f ◦
given the data y for some level of noise. Typically, IPs are ill-posed in the sense that the solution
does not depend continuously on the data. There are several very important and impressive examples
of IPs in our daily lives. Medical imaging has been using IPs for decades and it has shaped the area,
as for instance, Computerized Tomography (CT) and Magnetic Resonance Imaging (MRI). For an
introductory text, see Vogel [2002]."
INTRODUCTION,0.029411764705882353,"A vast literature of IPs is devoted to deterministic problems where the noise term is also a element of
a Hilbert space and commonly assumed small in norm, which is not usually verifiable in practice. In
this work, we will take a different avenue, known as Statistical Inverse Problems (SIP). This approach
is a formalization of IPs within a probabilistic setting, where the uncertainty of all measurements are
properly considered. Our focus in this work is to propose a direct and practical method for solving
SIP problems and, at the same time, provide theoretical guarantees for the excess risk performance of
the algorithm we develop. Our algorithm is based on a gradient descent framework, where stochastic
gradients (or base learners that approximate the stochastic gradients) are used to estimate general
functional parameters."
INTRODUCTION,0.03431372549019608,"The paper is organized as follows. We finish this section contextualizing our paper in the broad
literature and stating our main contributions. In Section 2, we formally introduce the learning
problem that we analyze. In Section 3, we provide examples of practical problems that fits within
our formulation. In Section 4 we provide our main results and algorithms. Finally, in Section 5 we
provide numerical examples and a real data application for a Functional Linear Regression problem"
INTRODUCTION,0.0392156862745098,"(FLR). Due to space constraints, some of the figures, proofs and experiments were moved to the
supplementary material."
CONTRIBUTION,0.04411764705882353,"1.1
Contribution"
CONTRIBUTION,0.049019607843137254,"We provide a novel numerical method to estimate functional parameters in SIP problems using
stochastic gradients. More precisely, we extend the properties and flexibility of SGD and boosting
algorithms to a broader class of problems by bridging the gap between the IP and machine learning
communities."
CONTRIBUTION,0.05392156862745098,"Whereas most of the IPs methods focus on regularization strategies to “invert"" the operator A, we
propose a gradient descent type of algorithm to estimate the functional parameter directly. Our
algorithm works in the same spirit as Stochastic Gradient Descent algorithms with sample averaging.
While results of SGD are well understood in the context of regression problems in finite and infinite
dimensions and SGD is well understood in deterministic IPs, SGD have not yet been considered
under the SIP formulation."
CONTRIBUTION,0.058823529411764705,"We show that our procedure also ensures risk consistency in expectation and high probability under
the statistical setting. Furthermore, we propose a modification in our algorithm to substitute the
stochastic gradients by base learners similarly to boosting algorithms, Mason et al. [1999], Friedman
[2001]. This modification improve a common challenge faced by SIP problems: the discretization
procedures of the operator A that arises in SIP."
LITERATURE REVIEW,0.06372549019607843,"1.2
Literature Review"
LITERATURE REVIEW,0.06862745098039216,"Historically, SIP was first introduced in Sudakovand and Khalfin [1964] where IPs from Mathematical
Physics were recast into a statistical framework. For a more structured introduction, we forward the
reader to Kaipio and Somersalo [2004]. Several advances were made in the parametric approach to
SIP, where the unknown function is assumed to be completely described by an unknown parameter
living in a finite dimensional space, see for instance Evans and Stark [2002]. In our paper, however,
we will consider the nonparametric framework as described in Cavalier [2008]. In this setting, we see
the IP as a search of an element of an infinite dimensional space."
LITERATURE REVIEW,0.07352941176470588,"When considering IPs (and SIP, in particular), there are several ways to regularize the problem in order
to deal with its ill-posedness. For instance, one could consider roughness penalty or a functional basis
as in Tenorio [2001]. Additionally, one could examine Tikhonov and spectral cut-off regularizations
as in Bissantz et al. [2007]. For many of those standard approaches, consistency under the SIP setting
and rates of convergences were established. See for instance Bissantz et al. [2004], Bissantz and
Holzmann [2008]. A thoroughly discussion of stochastic gradient algorithms is outside the scope of
this work and we refer the reader to Zinkevich [2003], Nesterov et al. [2018] and references therein."
LITERATURE REVIEW,0.0784313725490196,"There have been numerous applications of Machine Learning (and Deep Learning, in particular) to
solve IPs, in recent years. However, in our opinion, these applications are akin of the capabilities
that these new techniques could bring to this area of research. Some attention have been given to
imaging problems as in Jin et al. [2017] and Ongie et al. [2020]. Under deterministic IP, the paper
Li et al. [2020] studies the regularization and convergence rates of penalized neural networks when
solving regression problems. See also Adler and Öktem [2017] and Bai et al. [2020]. Other important
references regarding SGD for deterministic IP are Jin et al. [2021], Tang et al. [2019], Jin et al. [2020]."
LITERATURE REVIEW,0.08333333333333333,"The main examples we bring in our paper is the class of Functional Linear Regression (FLR). This
problem has drawn the attention of the statistical, econometric and computer science communities in
the past decade, see Cai and Hall [2006], Yao et al. [2005], Hall and Horowitz [2007]. The usual
methodology applied to this problem is the well-known FDA. For example, one could consider a
prespecified functional basis to regularize the regression problem Goldsmith et al. [2011] or one could
use the Functional Principal Component (FPC) basis, Morris [2015]. More recently, methods inspired
in machine learning for standard linear regression problems were also extended to the FLR setting, see
for instance James et al. [2009], Fan et al. [2015] for methods that are suitable for high dimensional
covariates or interpretable in the LASSO sense. In this work we show how our modification to the
SGD algorithm can be seen as an averaging of boosting estimators and can also be used to estimate
FLR models in the high-dimensional setting."
PROBLEM FORMULATION,0.08823529411764706,"2
Problem Formulation"
PROBLEM FORMULATION,0.09313725490196079,"We start by fixing a probability space (Ω, A, P) and a vector space X of inputs. We denote the
random input by X ∈L2(Ω, A, P) taking values in X and consider the space 1L2(X, B(X), µX),
henceforth referred to as L2(X), of functions g : X −→Rd with inner product ⟨g1, g2⟩L2(X) =
E[⟨g1(X), g2(X)⟩] and norm ∥g∥2
L2(X) = E[∥g(X)∥2] < +∞, where ⟨·, ·⟩and ∥· ∥are the inner
product and norm of Rd."
PROBLEM FORMULATION,0.09803921568627451,"We also consider a Hilbert space H with inner product ⟨·, ·⟩H. Finally, we consider an operator
A : H −→L2(X). This operator defines a direct problem and we assume that it is known. Given
f ∈H, we use the notation A[f] ∈L2(X), i.e. A[f] is a square-integrable function A[f] : X −→Rd."
PROBLEM FORMULATION,0.10294117647058823,"We are interested in solving the statistical inverse problem related to A: jointly to observing samples
of X taking values in X, we observe noisy samples of A[f ◦](X), for some fixed, unknown f ◦∈H,
which we denote by Y:"
PROBLEM FORMULATION,0.10784313725490197,"Y = A[f ◦](X) + ϵ,
(1)"
PROBLEM FORMULATION,0.11274509803921569,"where ϵ is a zero-mean random noise. The problem we will pore over in this paper is the estimation
of f ◦based on this given sample."
PROBLEM FORMULATION,0.11764705882352941,"Let ℓ: Rd × Rd →R+ be a point-to-point loss function as, for example, the squared loss ℓ(y, y′) =
1
2∥y −y′∥2, for regression, or the logistic loss function ℓ(y, y′) = log(1 + e−y·y′), for classification.
We define the populational risk as:"
PROBLEM FORMULATION,0.12254901960784313,"RA(f) ≜E[ℓ(Y, A[f](X))],"
PROBLEM FORMULATION,0.12745098039215685,and we would like to solve:
PROBLEM FORMULATION,0.1323529411764706,"inf
f∈F RA(f),
(2)"
PROBLEM FORMULATION,0.13725490196078433,"where F ⊂H with f ◦∈F. We will denote by ∂2 the partial derivative with respect to the second
argument."
PROBLEM FORMULATION,0.14215686274509803,"Given a sample, we will study how to control the excess risk of a functional estimator ˆf of f ◦:"
PROBLEM FORMULATION,0.14705882352941177,"RA( ˆf) −inf
f∈F RA(f).
(3)"
PROBLEM FORMULATION,0.15196078431372548,"Instead of taking the standard route of solving the Empirical Risk Minimization problem and later
establishing results for (3), in Section 4 we show how our algorithms allow us to tackle (3) directly
by constructing stochastic gradients directly for the populational risk."
PROBLEM FORMULATION,0.1568627450980392,"3
Examples: motivation"
PROBLEM FORMULATION,0.16176470588235295,"Before we formalize our results, we first motivate the study of Eq. (1) with a few of applications.
Each of those problems have a myriad of solutions on their own. For more information on those IPs,
see, for instance, Vogel [2002]."
PROBLEM FORMULATION,0.16666666666666666,"Deconvolution. This type of inverse problems relate the values of Y and X through the following
convolution equation: Y =
Z"
PROBLEM FORMULATION,0.1715686274509804,"W
k(X −w)f(w)dµ(w) + ϵ,"
PROBLEM FORMULATION,0.17647058823529413,"where X = W = Rd, H = L2(W, B, µ) and the kernel k : Rd −→R is known. In this case, we
define the operator A as:"
PROBLEM FORMULATION,0.18137254901960784,"A[f](x) =
Z"
PROBLEM FORMULATION,0.18627450980392157,"W
k(x −w)f(w)dµ(w)."
PROBLEM FORMULATION,0.19117647058823528,"Functional Linear Regression. Consider the scalar, multivariate functional linear regression: let
X = D([0, T]) (the space of right-continuous with left limits functions taking values in Rd×k with"
PROBLEM FORMULATION,0.19607843137254902,1We denote by B(X) the Borel sigma algebra in X and by µX the distribution of the random variable X.
PROBLEM FORMULATION,0.20098039215686275,"the sup norm) so that L2(Ω) is the space of stochastic processes with sample paths in D([0, T]) and
norm"
PROBLEM FORMULATION,0.20588235294117646,"∥X∥2 = E """
PROBLEM FORMULATION,0.2107843137254902,"sup
t∈[0,T ]
∥X(t)∥2
# .
(4)"
PROBLEM FORMULATION,0.21568627450980393,"Moreover, H = L2([0, T]) taking values in Rk and Y is given by the following model"
PROBLEM FORMULATION,0.22058823529411764,"Y =
Z T"
PROBLEM FORMULATION,0.22549019607843138,"0
X(s)f(s)ds + ϵ,"
PROBLEM FORMULATION,0.23039215686274508,"where f ∈H. Here we changed the notation from w to s in order to keep the classical notation from
FLR. In this case,"
PROBLEM FORMULATION,0.23529411764705882,"A[f](x) =
Z T"
PROBLEM FORMULATION,0.24019607843137256,"0
x(s)f(s)ds."
PROBLEM FORMULATION,0.24509803921568626,"The model can be easily extended to deal with Y taking label values such as in a classification
problem as we will show in the numerical studies."
PROBLEM FORMULATION,0.25,"Due to space constraints, we provide examples in the FLR setting. In the supplementary material,
we demonstrate how the algorithms presented in Section 4.3 can also be applied in deconvolution
problems."
THEORETICAL RESULTS AND ALGORITHMS,0.2549019607843137,"4
Theoretical Results and Algorithms"
THEORETICAL RESULTS AND ALGORITHMS,0.25980392156862747,"In this paper, we consider the following set of assumptions."
THEORETICAL RESULTS AND ALGORITHMS,0.2647058823529412,Assumption 4.1.
THEORETICAL RESULTS AND ALGORITHMS,0.2696078431372549,"1. A : H −→L2(X) is a linear, bounded operator;"
THEORETICAL RESULTS AND ALGORITHMS,0.27450980392156865,2. ℓis a convex and C2 function in its second argument;
THEORETICAL RESULTS AND ALGORITHMS,0.27941176470588236,"3. There exists θ0 > 0 such that, for all f, g ∈H,"
THEORETICAL RESULTS AND ALGORITHMS,0.28431372549019607,"sup
|θ|≤θ0
E
D
A[g](X), ∂22ℓ
 
Y, A[f](X) + θA[g](X)

A[g](X)
E
< ∞;"
THEORETICAL RESULTS AND ALGORITHMS,0.28921568627450983,4. f ◦∈arg minf∈F RA(f) and RA(f ◦) > −∞;
THEORETICAL RESULTS AND ALGORITHMS,0.29411764705882354,"5. supf,f ′∈F ∥f −f ′∥L2(W) = D < ∞."
THEORETICAL RESULTS AND ALGORITHMS,0.29901960784313725,"Assumption 1 is our strongest one, since it imposes that our operator is linear and bounded. Nev-
ertheless, linear SIPs encompass a wide class of problems of practical and theoretical interest for
engineering, statistics and computer science communities among others, a few of them presented in
Section 3. Moreover, the nonlinear case could be similarly studied with more cumbersome notation
and assumptions. Assumption 2 is standard for gradient based algorithms and is commonly assumed
in many learning problems. Assumption 3 is a mild integrability condition of the loss function com-
monly satisfied in many practical situations. For instance, in the squared loss case, this assumption
becomes E[∥A[g](X)∥2] < ∞, which is automatically satisfied since A[g] ∈L2(X). Assumption 4
is needed so the problem we analyze indeed has a solution. Assumption 5 is stating that the diameter
of the set F is finite."
THEORETICAL RESULTS AND ALGORITHMS,0.30392156862745096,"One should notice that our set of assumptions does include the class of ill-posed (linear) inverse
problems since we do not need to assume that A is bijective. If that were the case, it is known that
then A would have a bounded inverse, and then, the IP would not be ill-posed."
THEORETICAL RESULTS AND ALGORITHMS,0.3088235294117647,"In the next sections we provide our theoretical results. Instead of following the common approach of
minimizing the Empirical Risk Minimization problem, we show how to compute stochastic gradients
in order to control directly for the excess risk (3) both in expectation and in probability."
PRELIMINARIES,0.3137254901960784,"4.1
Preliminaries"
PRELIMINARIES,0.31862745098039214,"Our first result allows us to compute the gradient of the populational risk at a given functional
parameter f. Before we present it, note that, by linearity, A : H →L2(X) is differentiable and, for
every f, g ∈H, we have that the directional derivative of A[f] in the direction g is given by"
PRELIMINARIES,0.3235294117647059,"DA[f](g) = lim
δ→0
1
δ (A[f + δg] −A[f]) = A[g]."
PRELIMINARIES,0.3284313725490196,"Note that the directional derivative does not depend on the point f that we are evaluating the gradient.
Let A∗denote the adjoint operator of A defined as the linear and bounded operator A∗: L2(X) −→H
such that2
⟨A[f], h⟩L2(X) = ⟨f, A∗[h]⟩H, for allf ∈H and h ∈L2(X).
The following lemma holds true:
Lemma 4.2. Under 1, 2 and 3 of Assumption 4.1 we have that"
PRELIMINARIES,0.3333333333333333,"∇RA(f) = A∗[ϕf] ∈H,"
PRELIMINARIES,0.3382352941176471,"where ϕf(x) = E[∂2ℓ(Y, A[f](x)) | X = x]."
PRELIMINARIES,0.3431372549019608,"Proof. Firstly, define, for fixed (X, Y),"
PRELIMINARIES,0.3480392156862745,"ψ(δ) = ℓ(Y, A[f + δg](X)) = ℓ(Y, A[f](X) + δA[g](X))."
PRELIMINARIES,0.35294117647058826,"Then, we get the directional derivative of the risk function in direction g by applying the Taylor
formula for ψ as a function of δ around δ = 0:"
PRELIMINARIES,0.35784313725490197,"DRA(f)(g) = lim
δ→0
1
δ (RA(f + δg) −RA(f))"
PRELIMINARIES,0.3627450980392157,"= lim
δ→0 E
1 δ"
PRELIMINARIES,0.36764705882352944,"
δ
D
∂2ℓ
 
Y, A[f](X)

, A[g](X)
E + 1"
PRELIMINARIES,0.37254901960784315,"2δ2D
A[g](X), ∂22ℓ
 
Y, A[f](X) + θA[g](X)

, A[g](X)
E
,"
PRELIMINARIES,0.37745098039215685,"where θ comes from the Taylor formula and it is between −θ0 and θ0, for some fixed θ0 > 0. Hence,
by Assumption 3, we find"
PRELIMINARIES,0.38235294117647056,"DRA(f)(g) = E
D
∂2ℓ
 
Y, A[f](X)), A[g](X
E
."
PRELIMINARIES,0.3872549019607843,"By the definition of ϕ and by conditioning in X, we find"
PRELIMINARIES,0.39215686274509803,"DRA(f)(g) = E[⟨ϕf(X), A[g](X)⟩] = ⟨ϕf, A[g]⟩L2(X) = ⟨A∗[ϕf], g⟩H."
PRELIMINARIES,0.39705882352941174,"Finally, we get that the descent direction ∇RA(f) is given by A∗[ϕf] ∈H."
UNBIASED ESTIMATOR OF THE GRADIENT,0.4019607843137255,"4.2
Unbiased Estimator of the Gradient"
UNBIASED ESTIMATOR OF THE GRADIENT,0.4068627450980392,"In order to define an unbiased estimator of the gradient ∇RA, we consider the following assumption:
Assumption 4.3."
UNBIASED ESTIMATOR OF THE GRADIENT,0.4117647058823529,1. H is a Hilbert space of functions from W to Rk;
UNBIASED ESTIMATOR OF THE GRADIENT,0.4166666666666667,2. There exists a kernel Φ : X × W −→Rk×d such that
UNBIASED ESTIMATOR OF THE GRADIENT,0.4215686274509804,"A∗[h](w) = E[Φ(X, w)h(X)] ∈H,"
UNBIASED ESTIMATOR OF THE GRADIENT,0.4264705882352941,for all w ∈W and h ∈L2(X).
UNBIASED ESTIMATOR OF THE GRADIENT,0.43137254901960786,"Several examples, including the Functional Linear Regression, as we will verify in Section 5, satisfy
this assumption. Additionally, there are two situations that encompass many important SIPs. The first
one is a restriction of the Hilbert space H without restrictions on the operator A:"
UNBIASED ESTIMATOR OF THE GRADIENT,0.4362745098039216,"2The adjoint of a linear, bounded operator always exists."
UNBIASED ESTIMATOR OF THE GRADIENT,0.4411764705882353,Lemma 4.4. Assumption 4.3 is verified if H is a Reproducing Kernel Hilbert Space (RKHS).
UNBIASED ESTIMATOR OF THE GRADIENT,0.44607843137254904,"Proof. For simplicity of notation, we assume k = 1. Notice that, by the RKHS assumption,
φw : L2(X) −→R defined as φw(h) = A∗[h](w), for h ∈L2(X), is an element of the dual of
L2(X), i.e. there existis Mw < +∞such that"
UNBIASED ESTIMATOR OF THE GRADIENT,0.45098039215686275,|φw(h)| = |A∗[h](w)| ≤Mw∥A∗[h]∥H ≤Mw∥A∗∥∥h∥L2(X).
UNBIASED ESTIMATOR OF THE GRADIENT,0.45588235294117646,"Hence, by the Riesz Representation Theorem, there exists a kernel Φ(·; w) : X −→Rd such that, for
all h ∈L2(X),"
UNBIASED ESTIMATOR OF THE GRADIENT,0.46078431372549017,"A∗[h](w) = φw(h) = ⟨Φ(·; w), h⟩L2(X) = E[⟨Φ(X, w), h(X)⟩],"
UNBIASED ESTIMATOR OF THE GRADIENT,0.46568627450980393,as desired.
UNBIASED ESTIMATOR OF THE GRADIENT,0.47058823529411764,"Remark 4.5. If the RKHS H has kernel K, then Φ is given by Φ(x, w) = A[K(·, w)](x). Indeed,
by the definition of kernel in the RKHS and the definition of A∗, we find"
UNBIASED ESTIMATOR OF THE GRADIENT,0.47549019607843135,"A∗[h](w) = ⟨A∗[h], K(·, w)⟩H = ⟨h, A[K(·, w)]⟩L2(X)."
UNBIASED ESTIMATOR OF THE GRADIENT,0.4803921568627451,"The second situation considers a different (and somewhat less restrictive) assumption for the Hilbert
space H and a particular, yet very general, class of operators A, called integral operators.
Lemma 4.6. If H = L2(W, B, µ) taking values in Rk and A is a integral operator of the form:"
UNBIASED ESTIMATOR OF THE GRADIENT,0.4852941176470588,"A[f](x) =
Z"
UNBIASED ESTIMATOR OF THE GRADIENT,0.49019607843137253,"W
φ(x, w)f(w)dµ(w),"
UNBIASED ESTIMATOR OF THE GRADIENT,0.4950980392156863,"where φ is a kernel taking values in Rd×k such that φ(x, ·)f ∈L1(W, B, µ), then Assumption 4.3 is
verified."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5,"Proof. By the definition of the adjoint operator, we find"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5049019607843137,"⟨f, A∗[h]⟩L2(W) = ⟨A[f], h⟩L2(X) = E[⟨A[f](X), h(X)⟩]"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5098039215686274,"= E
Z"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5147058823529411,"W
φ(X, w)f(w)dµ(w), h(X)
 =
Z"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5196078431372549,"W
E [⟨φ(X, w)f(w), h(X)⟩] dµ(w) = ⟨f, E

φ(X, ·)T h(X)

⟩L2(W)."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5245098039215687,"Hence, we conclude A∗[h](w) = E

φ(X, w)T h(X)

, which implies Assumption 4.3 with kernel
Φ(x, w) = φ(x, w)T ."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5294117647058824,"Remark 4.7. The class of integral operators delivers several of the most important linear IPs. Addi-
tionally, using Green’s function formulation, some PDEs IPs could also be recast as integral equations.
For instance, the recovery the initial condition of a linear PDE with known Green function and
observing the solution of the PDE at some future, fixed time."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5343137254901961,"Under Assumption 4.3, Lemma 4.2 implies the following very useful result that is the cornerstone of
our method.
Corollary 4.8. If Assumption 4.3 is verified, then the gradient of the risk function with respect to f is
given by"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5392156862745098,"∇RA(f)(w) = E[Φ(X, w)ϕf(X)]."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5441176470588235,"Because of the results above, it is possible to construct an unbiased estimator for the gradient for the
risk function for any f. In fact, for a given sample (x, y) and a fixed function f, we define, for any
w ∈W,"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5490196078431373,"uf(w; x, y) = Φ(x, w)∂2ℓ(y, A[f](x)).
(5)"
UNBIASED ESTIMATOR OF THE GRADIENT,0.553921568627451,"Therefore, conditioning in X, we find"
UNBIASED ESTIMATOR OF THE GRADIENT,0.5588235294117647,"E[uf(w; X, Y)] = E[Φ(X, w)∂2ℓ(Y, A[f](X))] = E[E[Φ(X, w)∂2ℓ(Y, A[f](X)) | X]]
= E[Φ(X, w)E[∂2ℓ(Y, A[f](X)) | X])] = E[Φ(X, w)ϕf(X)] = ∇RA(f)(w)."
UNBIASED ESTIMATOR OF THE GRADIENT,0.5637254901960784,"The main benefit is that with a single observation of (X, Y), we are able to compute an unbiased
estimator for the gradient of the risk function under the true distribution."
PROPOSED ALGORITHMS,0.5686274509803921,"4.3
Proposed Algorithms"
PROPOSED ALGORITHMS,0.5735294117647058,"Inspired by Corollary 4.8, we propose the following SGD algorithm for SIP problems that we called
SGD-SIP: given an initial guess f0, for each step i, we compute, following Eq. (5), an unbiased
estimator ui for the gradient of the loss function. Next, we update an accumulated functional
parameter by taking a stochastic gradient step in the direction of ui with step size αi. In the last
step, we average all the accumulated gradient steps in the same spirit as Polyak and Juditsky [1992].
The choice of the step size needs to satisfy two criteria: Pn
i=1 αi sublinear in n, and nαn →∞as
n →+∞. We formally justify those desired properties in Theorem 4.9."
PROPOSED ALGORITHMS,0.5784313725490197,"Algorithm 1: SGD-SIP
input : sample {xi, yi}n
i=1, operator A, initial guess f0
output : ˆfn
ˆg0 = f0;
for 1 ≤i ≤n do"
PROPOSED ALGORITHMS,0.5833333333333334,"Compute ui(w) = Φ(xi, w)∂2ℓ(yi, A[ˆgi−1](xi));
ˆgi = ˆgi−1 −αiui;
end
Set ˆfn = 1"
PROPOSED ALGORITHMS,0.5882352941176471,"n
Pn
i=1 ˆgi;"
PROPOSED ALGORITHMS,0.5931372549019608,"Algorithm 1 uses only one sample at a time in order to estimate the gradient of the true risk function.
In order to preserve this property, we make the number of iterations equal to the sample size; it cannot
be larger. This connects with the stopping rules in iterative algorithms in IP."
PROPOSED ALGORITHMS,0.5980392156862745,"Algorithm 1 has a limitation common to many approaches to Inverse Problems: one cannot hope to
compute ui for every possible wi and some discretization of the operator A is needed, see Kaipio
and Somersalo [2007]. Since the SGD-SIP algorithm only computes the stochastic gradient in the
points of discretization, it risks overfitting the data and provides non-smooth estimators. Next, we
motivate Algorithm 2 in order to overcome the discretization problem by leveraging machine learning
methods."
PROPOSED ALGORITHMS,0.6029411764705882,"Consider that the space W was discretized in a grid of size nw. In order to fully estimate the function
ˆfn(w) for every w ∈W, we consider a hypothesis class H and, in each step, we fit a function
ˆh⋆
i ∈H on the stochastic gradient ui in the discretized grid of W. Note that in this case, F will be
given by the linear span of the class H. Each of these functions h⋆
i can be seen as a base-learner in
the same spirit of Boosting estimators, widely used in standard regression problem in the context of
SIP Mason et al. [1999], Friedman [2001]. Next we present our algorithm ML-SGD."
PROPOSED ALGORITHMS,0.6078431372549019,"Algorithm 2: ML-SGD
input : sample {xi, yi}n
i=1, discretization {wj}nw
j=1 of W, operator A, initial guess f0
output : ˆfn
ˆg0 = f0;
for 1 ≤i ≤n do"
PROPOSED ALGORITHMS,0.6127450980392157,for 1 ≤j ≤nw do
PROPOSED ALGORITHMS,0.6176470588235294,"Compute ui(wj) = Φ(xi, wj)∂2ℓ(yi, A[ˆgi−1](xi));
end
h⋆
i ∈arg minh∈H
Pnz
j=1(ui(wj) −h(wj))2;
ˆgi = ˆgi−1 −αih⋆
i ;
end
Set ˆfn = 1"
PROPOSED ALGORITHMS,0.6225490196078431,"n
Pn
i=1 ˆgi;"
PROPOSED ALGORITHMS,0.6274509803921569,"The goal of ML-SGD is twofold. First, it allows us to interpolate the function h⋆
j to points w
not used in the discretization grid. Second, the ML procedure smooths the noise in each gradient
step calculation leading to smoother approximations that helps avoiding over-fitting. We show in
Section 5 and in the supplementary material the benefits of such an approximation when estimating
the functional parameter f ◦in both simulated and empirical examples."
MAIN RESULT,0.6323529411764706,"4.4
Main Result"
MAIN RESULT,0.6372549019607843,"Our main result is a finite sample bound for the expected excess risk of Algorithm 1. The result also
extends to Algorithm 2 in the case where the base learner are also unbiased estimators.
Theorem 4.9. Under Assumptions 4.1 and 4.3 and if the kernel Φ satisfies C = supx∈X ∥Φ(x, ·)∥2 <
+∞3, we have the following performance guarantee for Algorithm 1:"
MAIN RESULT,0.6421568627450981,"E

RA( ˆfn) −inf
f∈F RA(f)

≤
D2"
MAIN RESULT,0.6470588235294118,"2nαn
+ M(A, F) n n
X"
MAIN RESULT,0.6519607843137255,"i=1
αi,"
MAIN RESULT,0.6568627450980392,"where M(A, F) = C(E[∥Y∥2] + ∥A∥2D2) < ∞."
MAIN RESULT,0.6617647058823529,"The proof of the theorem is provided in the supplementary material. Theorem 4.9 implies that if
we pick the decreasing sequence {αi}n
i=1 so that nαn →∞(αn cannot decrease too fast) but fast
enough so that 1"
MAIN RESULT,0.6666666666666666,"n
Pn
i=1 αi →0, then we get the convergence result. For instance, one could take
αi = η/
√"
MAIN RESULT,0.6715686274509803,"i for some fixed number η normally taken to be in (0, 1). In this case, the excess risk
decreases in expectation with rate O(1/√n)."
MAIN RESULT,0.6764705882352942,"Theorem 4.9 also implies that the excess risk converges to zero in probability. For αi = η/
√"
MAIN RESULT,0.6813725490196079,"i it is
straightforward to check that"
MAIN RESULT,0.6862745098039216,"lim sup
n→+∞P

RA( ˆfn) −inf
f∈F RA(f) > 0

= 0."
MAIN RESULT,0.6911764705882353,"Finite sample bounds with high probability can also be provided under stronger assumptions about
the stochastic gradients. See for instance Nemirovski et al. [2009]."
MAIN RESULT,0.696078431372549,"5
Functional Linear Regression: numerical studies"
MAIN RESULT,0.7009803921568627,"In this section, we provide two applications of the Functional Linear Regression problem. We first
demonstrate the performance of both algorithms in simulated data and next we provide an example
for generalized linear models, applied to an classification problem using bitcoin transaction data. In
Appendix B and Appendix C we also provide an additional numerical study in a different type of
Inverse Problem: The deconvolution problem."
MAIN RESULT,0.7058823529411765,"As we have seen in Section 3, the operator in the FLR case is given by"
MAIN RESULT,0.7107843137254902,"A[f](x) =
Z T"
MAIN RESULT,0.7156862745098039,"0
x(s)f(s)ds.
(6)"
MAIN RESULT,0.7205882352941176,"Remember that in this example we are denoting w by s. Hence, by Lemma 4.6, we find A∗[g](s) =
E

XT (s) g(X)

. One can easily verify that A∗[g] ∈H by the assumption that the norm (4) of X is
finite. Therefore, we have Φ(x, s) = xT (s), and we find, as in Eq. (5),"
MAIN RESULT,0.7254901960784313,"ui(s) = xT (s)∂2ℓ(yi, A[ˆgi−1](xi)).
(7)"
SYNTHETIC DATA,0.7303921568627451,"5.1
Synthetic Data"
SYNTHETIC DATA,0.7352941176470589,"We will consider the simulation study presented in González-Manteiga and Martínez-Calvo [2011].
Specifically, we set W = [0, 1], f ◦(z) = sin(4πz), and X simulated accordingly a Brownian motion
in [0, 1]. We also consider a noise-signal ratio of 0.2. We generate 100 samples of X and Y with the
integral defining the operator A approximated by a finite sum of 1000 points in [0, 1]. We test for
the same specification when f ◦(z) oscillates between 1 and −1 in the points 0.25, 0.5, 0.75, 1. For
the observed data used in the algorithm procedure, we consider a coarser grid where each functional
sample is observed at only 100 equally-spaced times. For the ML-SGD algorithm, we used smoothing
splines and regression trees as base learners. We compared the results with Penalized Functional
Linear Regression (PFLR) with cubic splines and cross-validation to select the number of basis
expansion; we also compare with Landweber iteration method. In order to fit the PFLR model, we"
SYNTHETIC DATA,0.7401960784313726,3This assumption is satisfied for all the examples analyzed in this paper.
SYNTHETIC DATA,0.7450980392156863,"used the package refund Goldsmith et al. [2021] available in R. Detailed numerical results with error
bars are displayed in Appendix C showing that the ML-SGD algorithm is at least as competitive as a
state-of-the-art tailored specifically to FLR problems and superior to Landweber iterations, a general
method for Inverse Problems. In Figure 1, we can see that both SGD-SIP and Landweber iterations
achieve similar fit performance. PFLR with cubic splines and degree of freedom of 20 achieved
a similar performance than ML-SGD with splines with 10 degrees of freedom. Both essentially
recovers the true function c◦perfectly. Despite the smoothness of f ◦, ML-SGD with regression trees
with 30 terminal nodes was also able to approximate f ◦well. Here we focus only on the methods
with best performance. ML-SGD with both regression trees and splines with 20 degrees of freedom
were able to recover the true function as well as PFLR with cubic splines and 20 degrees of freedom.
In the appendix, we provide bar plots with the MSE under both scenarios with error bars for different
simulations. Both Landweber iterations and the SGD algorithm are noisier and seems to overfit the
data. We refer the reader to Appendix C for a detailed comparison among the methods. −1.0 −0.5 0.0 0.5 1.0"
SYNTHETIC DATA,0.75,"0.00
0.25
0.50
0.75
1.00 model"
SYNTHETIC DATA,0.7549019607843137,"Landweber
MLSGD−Spl−k10
MLSGD−Tree−30
PFLR_k20
SGD (a) −1 0 1"
SYNTHETIC DATA,0.7598039215686274,"0.00
0.25
0.50
0.75
1.00 model"
SYNTHETIC DATA,0.7647058823529411,"Landweber
MLSGD−Spl−k20
MLSGD−Tree−30
PFLR_k20
SGD (b)"
SYNTHETIC DATA,0.7696078431372549,Figure 1: Fitted results for synthetic data. True values for f ◦are displayed as black dots.
REAL DATA APPLICATION,0.7745098039215687,"5.2
Real Data Application"
REAL DATA APPLICATION,0.7794117647058824,"Next we consider a classification problem in the FLR setting. The data set contains 3000 bitcoin
addresses spanning from April 2011 and April 2017 and their respective cumulative credit, which is
described as 501 equally spaced observations for the first 3000 hours of each address, normalized
in the interval [0, 1]. For each address, we also have a label describing if the address was used for
criminal activity, commonly called darknet addresses. In Table 1 we present a summary of the
data. We refer the reader to Appendix A for more information about the data set used that we make
available online."
REAL DATA APPLICATION,0.7843137254901961,Table 1: Summary information for the bitcoin wallet observations.
REAL DATA APPLICATION,0.7892156862745098,"category
obs
mean_initial_credit
mean_final_credit
Darknet Marketplace
1494
242
1255
Exchanges
381
807
14278
Gambling
371
95
2289
Pools
366
1086
14071
Services/others
388
184
3883"
REAL DATA APPLICATION,0.7941176470588235,"Here we use the cumulative credit curve at each point in time as the explanatory variables X ∈
X = D([0, 1]) and Y ∈{−1, 1} as the predicted outcome for the indicator variable that the
category is darknet (addresses associated with illegal activities). We propose the following model:
log
P (Y =1|X)
P (Y =−1|X) =
R T
0 f(s)X(s)ds. By using the log-likelihood of the negative binomial, one can
include its gradient with respect to the functional parameter directly in Equation (7) in order to use
our framework. Other type of classification loss functions can also be used in the same spirit."
REAL DATA APPLICATION,0.7990196078431373,"We compare the SGD-SIP and ML-SGD algorithm with PFLR. For the ML-SGD algorithm, we use
two types of base learner, regression trees and cubic splines. The step sizes are taken to be equal of
the form O(1/
√"
REAL DATA APPLICATION,0.803921568627451,"i), where i = 1, · · · , n is the current step of the algorithm and n is the total number
of steps/sample. For the PFLR algorithm, we use cubic splines with different degrees of freedom and
quadratic penalty term. We highlight that those choices of splines and penalty term are widely used in
the literature, see, for instance, Goldsmith et al. [2011]. In Table 2 we provide 3-fold cross validation
for the accuracy and kappa metrics. The SGD-SIP Algorithm achieved the best average performance
in terms of accuracy. The same performance is achieved by the Functional PLR with cubic splines
with number of knots equal to 20 and penalization for the derivative of the estimate. The ML-SGD
algorithm with smooth splines with 20 degrees of freedom also achieved similar performance with a
smoother estimator. Although the benchmark is as good as the ML-SGD algorithm, we highlight
here that PFLR is tailored to Functional Data Analysis problem, while our approach is flexible for
many different types of Linear SIP problems. Moreover, our algorithm can make use of only one
sample at each iteration, which makes it suitable also for online applications. We refer the reader to
the supplementary material for results under different choices of step size, number of knots and base
functions for the PFLR model, other metrics and confusion matrices. In order to fit the PFLR mode,
we used the package refund Goldsmith et al. [2021] available in R."
REAL DATA APPLICATION,0.8088235294117647,Table 2: Results for three fold cross-validation.
REAL DATA APPLICATION,0.8137254901960784,"fold_1
fold_2
fold_3
avg_accuracy
ML-SGD-spline(k = 20)
0.76
0.80
0.79
0.78
ML-SGD-spline(k = 10)
0.72
0.74
0.74
0.73
ML-SGD-tree(depth = 20)
0.78
0.77
0.78
0.78
SGD-SIP
0.79
0.77
0.82
0.79
FPLR(k = 10)
0.76
0.75
0.73
0.75
FPLR(k = 20)
0.77
0.81
0.80
0.79"
CONCLUSION,0.8186274509803921,"6
Conclusion"
CONCLUSION,0.8235294117647058,"In this work, we provided a novel numerical method to solve SIP based on stochastic gradients with
theoretical guarantees for the excess risk. Moreover, we have shown how one can improve algorithmic
performance by estimating base-learners for each stochastic gradient in the same spirit as boosting
algorithms. Our framework can be applied in a variety of settings ranging from deconvolution
problems, Functional Data analysis in both regression and classification problems, integral equations
and other linear IPs related to PDEs. We demonstrate the performance of our method with numerical
studies and also with a real world application data and comparing with widely used techniques in the
FLR setting."
REFERENCES,0.8284313725490197,References
REFERENCES,0.8333333333333334,"Curtis R Vogel. Computational methods for inverse problems. SIAM, 2002."
REFERENCES,0.8382352941176471,"Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient descent in
function space. In Proc. NIPS, volume 12, pages 512–518, 1999."
REFERENCES,0.8431372549019608,"Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages
1189–1232, 2001."
REFERENCES,0.8480392156862745,"V.N. Sudakovand and L.A. Khalfin. Statistical approach to ill-posed problems in mathematical geophysics. Sov.
Math.—Dokl., 157, 1964."
REFERENCES,0.8529411764705882,"J. Kaipio and E. Somersalo. Statistical and Computational Inverse Problems. Springer, 2004."
REFERENCES,0.8578431372549019,"Steven N Evans and Philip B Stark. Inverse problems as statistics. Inverse problems, 18(4):R55, 2002."
REFERENCES,0.8627450980392157,"Laurent Cavalier. Nonparametric statistical inverse problems. Inverse Problems, 24(3):034004, 2008."
REFERENCES,0.8676470588235294,"Luis Tenorio. Statistical regularization of inverse problems. SIAM review, 43(2):347–366, 2001."
REFERENCES,0.8725490196078431,"Nicolai Bissantz, Thorsten Hohage, Axel Munk, and Frits Ruymgaart. Convergence rates of general regularization
methods for statistical inverse problems and applications. SIAM Journal on Numerical Analysis, 45(6):2610–
2636, 2007."
REFERENCES,0.8774509803921569,"Nicolai Bissantz, Thorsten Hohage, and Axel Munk. Consistency and rates of convergence of nonlinear tikhonov
regularization with random noise. Inverse Problems, 20(6):1773, 2004."
REFERENCES,0.8823529411764706,"Nicolai Bissantz and Hajo Holzmann. Statistical inference for inverse problems. Inverse Problems, 24(3):
034009, 2008."
REFERENCES,0.8872549019607843,"Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of
the 20th international conference on machine learning (icml-03), pages 928–936, 2003."
REFERENCES,0.8921568627450981,"Yurii Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018."
REFERENCES,0.8970588235294118,"Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser. Deep convolutional neural
network for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9):4509–4522, 2017."
REFERENCES,0.9019607843137255,"Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca
Willett. Deep learning techniques for inverse problems in imaging. IEEE Journal on Selected Areas in
Information Theory, 1(1):39–56, 2020."
REFERENCES,0.9068627450980392,"Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. NETT: Solving inverse problems with
deep neural networks. Inverse Problems, 36(6):065005, 2020."
REFERENCES,0.9117647058823529,"Jonas Adler and Ozan Öktem. Solving ill-posed inverse problems using iterative deep neural networks. Inverse
Problems, 33(12):124007, 2017."
REFERENCES,0.9166666666666666,"Yanna Bai, Wei Chen, Jie Chen, and Weisi Guo. Deep learning methods for solving linear inverse problems:
Research directions and paradigms. Signal Processing, page 107729, 2020."
REFERENCES,0.9215686274509803,"Bangti Jin, Zehui Zhou, and Jun Zou. On the saturation phenomenon of stochastic gradient descent for linear
inverse problems. SIAM/ASA Journal on Uncertainty Quantification, 9(4):1553–1588, 2021."
REFERENCES,0.9264705882352942,"Junqi Tang, Karen Egiazarian, and Mike Davies. The limitation and practical acceleration of stochastic gradient
algorithms in inverse problems. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 7680–7684. IEEE, 2019."
REFERENCES,0.9313725490196079,"Bangti Jin, Zehui Zhou, and Jun Zou. On the convergence of stochastic gradient descent for nonlinear ill-posed
problems. SIAM Journal on Optimization, 30(2):1421–1450, 2020."
REFERENCES,0.9362745098039216,"T Tony Cai and Peter Hall. Prediction in functional linear regression. The Annals of Statistics, 34(5):2159–2179,
2006."
REFERENCES,0.9411764705882353,"Fang Yao, Hans-Georg Müller, and Jane-Ling Wang. Functional linear regression analysis for longitudinal data.
The Annals of Statistics, pages 2873–2903, 2005."
REFERENCES,0.946078431372549,"Peter Hall and Joel L Horowitz. Methodology and convergence rates for functional linear regression. The Annals
of Statistics, 35(1):70–91, 2007."
REFERENCES,0.9509803921568627,"Jeff Goldsmith, Jennifer Bobb, Ciprian M Crainiceanu, Brian Caffo, and Daniel Reich. Penalized functional
regression. Journal of computational and graphical statistics, 20(4):830–851, 2011."
REFERENCES,0.9558823529411765,"Jeffrey S Morris. Functional regression. Annual Review of Statistics and Its Application, 2:321–359, 2015."
REFERENCES,0.9607843137254902,"Gareth M James, Jing Wang, and Ji Zhu. Functional linear regression that’s interpretable. The Annals of
Statistics, 37(5A):2083–2108, 2009."
REFERENCES,0.9656862745098039,"Yingying Fan, Gareth M James, and Peter Radchenko. Functional additive regression. The Annals of Statistics,
43(5):2296–2325, 2015."
REFERENCES,0.9705882352941176,"Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal
on control and optimization, 30(4):838–855, 1992."
REFERENCES,0.9754901960784313,"Jari Kaipio and Erkki Somersalo. Statistical inverse problems: discretization, model reduction and inverse
crimes. Journal of computational and applied mathematics, 198(2):493–504, 2007."
REFERENCES,0.9803921568627451,"Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609, 2009."
REFERENCES,0.9852941176470589,"Wenceslao González-Manteiga and Adela Martínez-Calvo. Bootstrap in functional linear regression. Journal of
Statistical Planning and Inference, 141(1):453–461, 2011."
REFERENCES,0.9901960784313726,"Jeff Goldsmith, Fabian Scheipl, Lei Huang, Julia Wrobel, Chongzhi Di, Jonathan Gellar, Jaroslaw Harezlak,
Mathew W. McLean, Bruce Swihart, Luo Xiao, Ciprian Crainiceanu, and Philip T. Reiss. refund: Regression
with Functional Data, 2021. URL https://CRAN.R-project.org/package=refund. R package version
0.1-24."
REFERENCES,0.9950980392156863,"Manuel Febrero-Bande, Wenceslao González-Manteiga, Brenda Prallon, and Yuri F Saporito. Functional
classification of bitcoin addresses. arXiv preprint arXiv:2202.12019, 2022."
