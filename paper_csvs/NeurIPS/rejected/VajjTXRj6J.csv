Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008968609865470852,"Language model (LM) post-training (or alignment) involves maximizing a
1"
ABSTRACT,0.0017937219730941704,"reward function that is derived from preference annotations. Direct Preference
2"
ABSTRACT,0.0026905829596412557,"Optimization (DPO) is a popular ofﬂine alignment method that trains a policy
3"
ABSTRACT,0.003587443946188341,"directly on preference data without the need to train a reward model or apply
4"
ABSTRACT,0.004484304932735426,"reinforcement learning. However, typical preference datasets have only a single, or
5"
ABSTRACT,0.0053811659192825115,"at most a few, annotation per preference pair, which causes DPO to overconﬁdently
6"
ABSTRACT,0.006278026905829596,"assign rewards that trend towards inﬁnite magnitude. This frequently leads to
7"
ABSTRACT,0.007174887892376682,"degenerate policies, sometimes causing even the probabilities of the preferred
8"
ABSTRACT,0.008071748878923767,"generations to go to zero. In this work, we analyze this phenomenon and propose
9"
ABSTRACT,0.008968609865470852,"distillation to get a better proxy for the true preference distribution over generation
10"
ABSTRACT,0.009865470852017937,"pairs: we train the LM to produce probabilities that match the distribution induced
11"
ABSTRACT,0.010762331838565023,"by a reward model trained on the preference data. Moreover, to account for
12"
ABSTRACT,0.011659192825112108,"uncertainty in the reward model we are distilling from, we optimize against a
13"
ABSTRACT,0.012556053811659192,"family of reward models that, as a whole, is likely to include at least one reasonable
14"
ABSTRACT,0.013452914798206279,"proxy for the preference distribution. Our results show that distilling from such
15"
ABSTRACT,0.014349775784753363,"a family of reward models leads to improved robustness to distribution shift in
16"
ABSTRACT,0.015246636771300448,"preference annotations, while preserving the simple supervised nature of DPO.
17"
INTRODUCTION,0.016143497757847534,"1
Introduction
18"
INTRODUCTION,0.017040358744394617,"Language model (LM) post-training (or alignment) aims to steer language model policies towards
19"
INTRODUCTION,0.017937219730941704,"responses that agree with human preferences. Early state-of-the-art approaches have focused on
20"
INTRODUCTION,0.01883408071748879,"reward learning from human feedback. In this paradigm, preference annotations are used to train
21"
INTRODUCTION,0.019730941704035873,"reward models, which then guide the optimization of the language model policy through online
22"
INTRODUCTION,0.02062780269058296,"reinforcement learning (an approach broadly referred to as RLHF). Recent research on ofﬂine “Direct
23"
INTRODUCTION,0.021524663677130046,"Preference Optimization” [DPO; 23] and extensions thereof [3; 31], however, has demonstrated that
24"
INTRODUCTION,0.02242152466367713,"it is also possible to directly optimize policies on the preference data, which bypasses the need for a
25"
INTRODUCTION,0.023318385650224215,"separate reward model—and its ofﬂine nature also leads to faster, and simpler, training frameworks.
26"
INTRODUCTION,0.0242152466367713,"While this direct approach to preference optimization is attractive in terms of its simplicity and
27"
INTRODUCTION,0.025112107623318385,"efﬁciency, it also raises important questions about the effectiveness and robustness of the resulting
28"
INTRODUCTION,0.02600896860986547,"policies—as well as the broader utility of using an explicit reward model. In this paper, we argue that
29"
INTRODUCTION,0.026905829596412557,"explicit reward modeling can, in fact, offer substantial practical advantages that are not captured by
30"
INTRODUCTION,0.02780269058295964,"DPO’s formulation. In particular, we theoretically show that relying solely on the preference data
31"
INTRODUCTION,0.028699551569506727,"can be a precarious strategy, with few natural brakes in place to prevent policies trained under the
32"
INTRODUCTION,0.029596412556053813,"DPO objective from careening off towards degenerate policies when the preference data exhibits
33"
INTRODUCTION,0.030493273542600896,"certain idiosyncratic properties. On the other hand, explicit reward models can easily be regularized
34"
INTRODUCTION,0.03139013452914798,"and understood—regardless of whether they are Bradley-Terry models [4], margin-based ranking
35"
INTRODUCTION,0.03228699551569507,"models [40], or simply any other kind of function that correlates well with human preferences [31; 17].
36"
INTRODUCTION,0.033183856502242155,"Taking a step back from pure direct preference optimization, we propose a method that merges the
37"
INTRODUCTION,0.034080717488789235,"best of both worlds: an efﬁcient reward model distillation algorithm that (i) operates effectively in the
38"
INTRODUCTION,0.03497757847533632,"ofﬂine setting, (ii) makes minimal assumptions about the true, optimal reward we aim to maximize,
39"
INTRODUCTION,0.03587443946188341,"and (iii) demonstrates greater robustness to the speciﬁc distribution of prompt/response data used for
40"
INTRODUCTION,0.036771300448430494,"policy alignment. Drawing inspiration from prior knowledge distillation techniques [14; 26; 35; 10],
41"
INTRODUCTION,0.03766816143497758,"we leverage the same change of variables trick employed in DPO to express the language model
42"
INTRODUCTION,0.03856502242152467,"policy in terms of its implicit reward model [23]. We then train the policy to match our desired,
43"
INTRODUCTION,0.039461883408071746,"explicit reward via an L2 loss that directly regresses the pairwise differences in target rewards for
44"
INTRODUCTION,0.04035874439461883,"any two generation pairs (x, y1) and (x, y2). We theoretically establish the equivalence between
45"
INTRODUCTION,0.04125560538116592,"optimizing this distillation loss over a sufﬁciently diverse ofﬂine dataset of unlabeled examples and
46"
INTRODUCTION,0.042152466367713005,"optimizing the traditional online RLHF objective.
47"
INTRODUCTION,0.04304932735426009,"Our reward model distillation approach, however, is not immune to some of the same challenges
48"
INTRODUCTION,0.04394618834080718,"facing DPO-style learning of policies. In particular, reward model distillation requires having a
49"
INTRODUCTION,0.04484304932735426,"reliable reward model—but having a reliable reward requires having a reliable method for extracting
50"
INTRODUCTION,0.045739910313901344,"a reward model from a potentially noisy preference dataset. To address the uncertainty surrounding
51"
INTRODUCTION,0.04663677130044843,"the “right” reward model, we introduce a pessimistic extension to our approach. This extension aims
52"
INTRODUCTION,0.04753363228699552,"to maximize the worst-case improvement of our model across a plausible family of reward models
53"
INTRODUCTION,0.0484304932735426,"(e.g., those sufﬁciently consistent with annotated preference data). This strategy aligns with that of
54"
INTRODUCTION,0.04932735426008968,"existing work in conservative ofﬂine reinforcement learning [5; 16]. Interestingly, we derive that
55"
INTRODUCTION,0.05022421524663677,"this pessimistic objective can be equivalently expressed and optimized by adding a simple additional
56"
INTRODUCTION,0.051121076233183856,"KL-divergence regularization to the original distillation objective.
57"
INTRODUCTION,0.05201793721973094,"Empirically, we ﬁnd that reward model distillation, particularly pessimistic reward model distillation,
58"
INTRODUCTION,0.05291479820627803,"leads to similar performance to prior direct preference optimization methods in settings where the
59"
INTRODUCTION,0.053811659192825115,"preference datasets used are unbiased, but signiﬁcantly better performance in settings where the
60"
INTRODUCTION,0.054708520179372194,"preference datasets are biased, when compared to DPO and the Identity Preference Optimization
61"
INTRODUCTION,0.05560538116591928,"(IPO) framework of [3], which was introduced as a more robust alternative to DPO. To further support
62"
INTRODUCTION,0.05650224215246637,"these empirical observations, we provide an extensive theoretical analysis that both (i) sheds more
63"
INTRODUCTION,0.05739910313901345,"light on the degenerative tendencies of DPO and issues inherent to its objective, and (ii) highlights
64"
INTRODUCTION,0.05829596412556054,"relative advantages of our explicitly regularized approaches.
65"
PRELIMINARIES,0.059192825112107626,"2
Preliminaries
66"
PRELIMINARIES,0.060089686098654706,"We begin with a brief review of Direct Preference Optimization (DPO) [23] and its analysis. Proofs
67"
PRELIMINARIES,0.06098654708520179,"of all theoretical results provided here, and in the rest of the paper, are deferred to Appendix A.
68"
THE PREFERENCE ALIGNMENT PROBLEM,0.06188340807174888,"2.1
The preference alignment problem
69"
THE PREFERENCE ALIGNMENT PROBLEM,0.06278026905829596,"Let x be an input prompt, and let y ∼πθ(· | x) be the language model policy πθ’s response to x.
70"
THE PREFERENCE ALIGNMENT PROBLEM,0.06367713004484304,"Given some reward function r∗(x, y) and another reference policy πref(y | x), the goal of alignment
71"
THE PREFERENCE ALIGNMENT PROBLEM,0.06457399103139014,"is to solve for the “aligned” policy πθ∗(y | x) that maximizes the following RLHF objective, i.e.,
72"
THE PREFERENCE ALIGNMENT PROBLEM,0.06547085201793722,"πθ∗(y | x) = argmax
πθ Eµ(x)

Eπθ(y|x)[r∗(x, y)] −βDKL[πθ(· | x)∥πref(· | x)]

,
(1)"
THE PREFERENCE ALIGNMENT PROBLEM,0.06636771300448431,"where µ(x) is a ﬁxed distribution over prompts, and the KL-divergence term prevents the aligned
73"
THE PREFERENCE ALIGNMENT PROBLEM,0.06726457399103139,"policy from being dramatically different from the anchoring reference policy, πref(y | x). Here,
74"
THE PREFERENCE ALIGNMENT PROBLEM,0.06816143497757847,"the reward function r∗is typically not known in advance, but rather inferred from collected human
75"
THE PREFERENCE ALIGNMENT PROBLEM,0.06905829596412556,"preference data in the form of (x, yw, yℓ), where x is the prompt, yw is the “winning”, or preferred,
76"
THE PREFERENCE ALIGNMENT PROBLEM,0.06995515695067264,"response, and yℓis the “losing”, or dispreferred, response. A common approach is to assume that
77"
THE PREFERENCE ALIGNMENT PROBLEM,0.07085201793721974,"pairs (y1, y2) follow a Bradley-Terry model [4], under which the probability that y1 is preferred to y2
78"
THE PREFERENCE ALIGNMENT PROBLEM,0.07174887892376682,"given the reward function r∗and prompt x is p∗(y1 ≻y2 | x) = σ(r∗(x, y1) −r∗(x, y2)), where
79"
THE PREFERENCE ALIGNMENT PROBLEM,0.0726457399103139,"σ(·) is the sigmoid function and ≻denotes preference. Under this model, we can use the preference
80"
THE PREFERENCE ALIGNMENT PROBLEM,0.07354260089686099,"data (x, yw, yℓ) ∼Dpref to estimate r∗via maximum likelihood estimation, i.e.,
81"
THE PREFERENCE ALIGNMENT PROBLEM,0.07443946188340807,"ˆr ∈argmin
r
E(yw,yℓ,x)∼Dpref

−log σ(r(x, yw) −rφ(x, yℓ))

.
(2)"
THE PREFERENCE ALIGNMENT PROBLEM,0.07533632286995516,"With ˆr in hand, Eq. (1) can be optimized using standard reinforcement learning algorithms [27; 29; 6].
82"
DIRECT PREFERENCE OPTIMIZATION,0.07623318385650224,"2.2
Direct preference optimization
83"
DIRECT PREFERENCE OPTIMIZATION,0.07713004484304933,"DPO is a simple approach for ofﬂine policy optimization that uses preferences to directly align the
84"
DIRECT PREFERENCE OPTIMIZATION,0.07802690582959641,"language model policy, without training an intermediate reward model. Speciﬁcally, DPO leverages
85"
DIRECT PREFERENCE OPTIMIZATION,0.07892376681614349,"the fact that the optimal solution to the KL-constrained objective in (1) takes the form [15]
86"
DIRECT PREFERENCE OPTIMIZATION,0.07982062780269059,"πθ∗(y | x) =
1
Z(x)πref(y | x) exp
 1"
DIRECT PREFERENCE OPTIMIZATION,0.08071748878923767,"β r∗(x, y)

,
(3)"
DIRECT PREFERENCE OPTIMIZATION,0.08161434977578476,where Z(x) = P
DIRECT PREFERENCE OPTIMIZATION,0.08251121076233184,y πref(y | x) exp( 1
DIRECT PREFERENCE OPTIMIZATION,0.08340807174887892,"β r∗(x, y)) is the partition function. DPO reparameterizes the
87"
DIRECT PREFERENCE OPTIMIZATION,0.08430493273542601,"true reward function r∗in terms of the optimal policy πθ∗that it induces, i.e.,
88"
DIRECT PREFERENCE OPTIMIZATION,0.08520179372197309,"r∗(x, y) = β log
 πθ∗(y | x)"
DIRECT PREFERENCE OPTIMIZATION,0.08609865470852018,πref(y | x)
DIRECT PREFERENCE OPTIMIZATION,0.08699551569506726,"
+ β log Z(x).
(4)"
DIRECT PREFERENCE OPTIMIZATION,0.08789237668161436,"Under the Bradley-Terry model, the likelihood that y1 ≻y2 can then be written as
89"
DIRECT PREFERENCE OPTIMIZATION,0.08878923766816144,"p∗(y1 ≻y2 | x) = σ

β log πθ∗(y1)πref(y2)"
DIRECT PREFERENCE OPTIMIZATION,0.08968609865470852,πθ∗(y2)πref(y1)
DIRECT PREFERENCE OPTIMIZATION,0.09058295964125561,"
,
(5)"
DIRECT PREFERENCE OPTIMIZATION,0.09147982062780269,"where now πθ∗can be directly estimated on Dpref following the objective in (2), in place of the
90"
DIRECT PREFERENCE OPTIMIZATION,0.09237668161434978,"intermediate reward model ˆr, i.e., πˆθ(y | x) ∈argminπθ Ldpo(πθ; Dpref) where
91"
DIRECT PREFERENCE OPTIMIZATION,0.09327354260089686,"Ldpo(πθ; Dpref) = E(yw,yℓ,x)∼Dpref"
DIRECT PREFERENCE OPTIMIZATION,0.09417040358744394,"
−log σ

β log πθ∗(yw)πref(yℓ)"
DIRECT PREFERENCE OPTIMIZATION,0.09506726457399103,πθ∗(yℓ)πref(yw)
DIRECT PREFERENCE OPTIMIZATION,0.09596412556053811,"
.
(6)"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.0968609865470852,"2.3
Pitfalls of direct preference optimization
92"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.09775784753363229,"As argued in [3], the Bradley-Terry assumption that DPO strongly relies on for maximum likelihood
93"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.09865470852017937,"estimation is sensitive to the underlying preference data. Speciﬁcally, if we have any two responses y1
94"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.09955156950672646,"and y2 where p∗(y1 ≻y2 | x) = 1, then the Bradley-Terry model dictates that r∗(y1)−r∗(y2) = +∞,
95"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10044843049327354,"and therefore πθ∗(y2 | x) = 0 for any ﬁnite KL-regularization strength β.
96"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10134529147982063,"We can illustrate this phenomenon on a broader level with the following example.
97"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10224215246636771,"Assumption 1. Suppose we are given a preference dataset of (context-free) pairs Dpref
=
98"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.1031390134529148,"{(yw
i , yℓ
i)}n
i=1, the pairs (yw
i , yℓ
i) are mutually disjoint in both the elements. Further suppose
99"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10403587443946188,"that we optimize the DPO objective on Dpref with a single parameter θy for each y.
100"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10493273542600896,"Proposition 1. Under Assumption 1, for any (y, y′) such that y = yw
i and y′ = yℓ
i for some i, we
101"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10582959641255606,have πθ∗(y)πref(y′)
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10672645739910314,"πθ∗(y′)πref(y) →∞, for all global minimizers πθ∗of the DPO objective in (6), for any β > 0.
102"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10762331838565023,"Corollary 1. Under Assumption 1, further assume that 0 < πref(y) < 1 for all y. Then πθ∗is a
103"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10852017937219731,"global minimizer of the DPO objective in (6) iff πθ∗(C(yℓ)c) →1 with πθ∗(yw
i ) > 0 ∀i ∈[n], where
104"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.10941704035874439,"C(yℓ)c is the complement of the set of all responses y that appear as a dispreferred yℓ
i for any i ∈[n].
105"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11031390134529148,"Additional analysis of the training dynamics of DPO is also provided in §5. A signiﬁcant, and non-
106"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11121076233183856,"obvious, implication of Corollary 1 is that the set of global optima of the DPO loss also includes poli-
107"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11210762331838565,"cies that can shift nearly all probability mass to responses that never even appear in the training set—
108"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11300448430493273,"and even assign near zero probability to all of the training data responses that do in fact correspond to
109"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11390134529147983,"winning generations, yw, a phenomenon that has been observed empirically [e.g., 20]. Stated differ-
110"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.1147982062780269,"ently, Corollary 1 implies that any θ∗merely satisfying πθ∗(yℓ
i) = 0 with πθ∗(yw
i ) > 0 ∀i ∈[n] is a
111"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11569506726457399,"global minimizer of the DPO objective in this setting. Though simplistic, the scenario in Assumption 1
112"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11659192825112108,"is closer to reality than might ﬁrst be appreciated: in many practical situations we can almost always
113"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11748878923766816,"expect the ﬁnite-sample preference data to contain one (or at most a few) preference annotations per
114"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11838565022421525,"example (x, y1, y2), while the policies πθ can have billions of parameters (≫n). Of course, this issue
115"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.11928251121076233,"can also be viewed as a classic instance of overﬁtting—with the additional caveat that as opposed to
116"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.12017937219730941,"overpredicting responses within the training set, we might overﬁt to almost never producing anything
117"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.1210762331838565,"like the “good” responses that do appear within the training set. Furthermore, without additional regu-
118"
PITFALLS OF DIRECT PREFERENCE OPTIMIZATION,0.12197309417040358,"larization (beyond β), we can expect this degeneration to easily happen in typical preference datasets.
119"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.12286995515695068,"3
Uncertainty-aware reward model distillation
120"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.12376681614349776,"As discussed in the previous section, a core issue in preference optimization is that the true preference
121"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.12466367713004484,"distribution p∗(y1 ≻y2 | x) is not known. Attempting to infer it from ﬁnite-sample preference data
122"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.12556053811659193,"(that may further be biased or out-of-distribution with respect to the target domain) can then result
123"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.12645739910313902,"in a failure to learn reasonable policies. In this section, we now propose an inherently regularized
124"
UNCERTAINTY-AWARE REWARD MODEL DISTILLATION,0.1273542600896861,"approach to direct preference optimization that uses uncertainty-aware reward model distillation.
125"
REWARD MODEL DISTILLATION,0.12825112107623318,"3.1
Reward model distillation
126"
REWARD MODEL DISTILLATION,0.12914798206278028,"Suppose for the moment that the reward function r∗was in fact known, and did not have to be
127"
REWARD MODEL DISTILLATION,0.13004484304932734,"inferred from sampled preference data. Under this setting, we can then deﬁne an efﬁcient ofﬂine
128"
REWARD MODEL DISTILLATION,0.13094170403587443,"optimization procedure that is similar in spirit to DPO, but no longer relies directly on a preference
129"
REWARD MODEL DISTILLATION,0.13183856502242153,"dataset. Concretely, given unlabeled samples (x, y1, y2) ∼ρ (where the number of samples can be
130"
REWARD MODEL DISTILLATION,0.13273542600896862,"potentially unlimited), we can deﬁne a simple “distillation” loss, Ldistill(r∗, πθ), as follows:
131"
REWARD MODEL DISTILLATION,0.1336322869955157,"Ldistill(r∗, πθ; ρ) = Eρ(x,y1,y2)"
REWARD MODEL DISTILLATION,0.13452914798206278,"""
r∗(x, y1) −r∗(x, y2) −β log πθ(y1 | x)πref(y2 | x)"
REWARD MODEL DISTILLATION,0.13542600896860987,πθ(y2 | x)πref(y1 | x) 2# . (7)
REWARD MODEL DISTILLATION,0.13632286995515694,"Intuitively, the distillation loss seeks to exactly match differences in reward model scores across
132"
REWARD MODEL DISTILLATION,0.13721973094170403,"all generation pairs (x, y1, y2). It is then easy to see that under the Bradley-Terry model, this is
133"
REWARD MODEL DISTILLATION,0.13811659192825113,"equivalent to matching the strength of the preference relationship, y1 ≻y2. Furthermore, by only
134"
REWARD MODEL DISTILLATION,0.13901345291479822,"matching differences, we can still conveniently ignore the log partition term, log Z(x), in the implicit
135"
REWARD MODEL DISTILLATION,0.13991031390134528,"reward formulation for πθ as shown in (4), as it is constant across different y for any given x. Finally,
136"
REWARD MODEL DISTILLATION,0.14080717488789238,"similar to the motivation in DPO, we can show that minimizing Ldistill(r∗, πθ; ρ) indeed results in an
137"
REWARD MODEL DISTILLATION,0.14170403587443947,"optimally aligned policy πθ∗, as long as the data distribution ρ has sufﬁcient support.
138"
REWARD MODEL DISTILLATION,0.14260089686098654,"Theorem 1. Let Y denote the set of all possible responses for any model πθ. Assume that
139"
REWARD MODEL DISTILLATION,0.14349775784753363,"supp(πref(y | x)) = Y, i.e., the reference policy may generate any outcome with non-zero probability.
140"
REWARD MODEL DISTILLATION,0.14439461883408072,"Further, let supp(ρ(x, y1, y2)) = supp(µ(x))×Y ×Y. Let πθ∗(y | x) ∈argminπθ Ldistill(r∗, πθ; ρ)
141"
REWARD MODEL DISTILLATION,0.1452914798206278,"be a minimizer over all possible policies, of the implicit reward distillation loss in (7), for which
142"
REWARD MODEL DISTILLATION,0.14618834080717488,"r∗(x, y) is assumed to be deterministic, and ﬁnite everywhere. Then for any β > 0, πθ∗also
143"
REWARD MODEL DISTILLATION,0.14708520179372198,"maximizes the alignment objective in (1).
144"
REWARD MODEL DISTILLATION,0.14798206278026907,"The above result holds for a broad class of data distributions ρ(x, y1, y2), and makes no assumptions
145"
REWARD MODEL DISTILLATION,0.14887892376681613,"on r∗(e.g., it is no longer necessary for it to be deﬁned using a Bradley-Terry model). In fact, this
146"
REWARD MODEL DISTILLATION,0.14977578475336323,"result can also be seen as strict generalization of the IPO framework of [3] when taking r∗(x, y) ≜
147"
REWARD MODEL DISTILLATION,0.15067264573991032,"1{y = yw}, if labeled pairs (x, yw, yl) are provided instead of the unlabeled pairs (x, y1, y2).
148"
REWARD MODEL DISTILLATION,0.1515695067264574,"Of course, the true reward r∗is usually not known in practice. Still, as in standard RLHF, we can
149"
REWARD MODEL DISTILLATION,0.15246636771300448,"go about constructing good proxies by using the preference data to identify plausible target reward
150"
REWARD MODEL DISTILLATION,0.15336322869955157,"models rtgt—further guided by any amount of regularization and inductive bias that we desire. A
151"
REWARD MODEL DISTILLATION,0.15426008968609867,"natural choice is to ﬁrst learn rtgt on the preference data Dpref using standard methods, and then reuse
152"
REWARD MODEL DISTILLATION,0.15515695067264573,"Dpref to distill πθ, which is similar to classical settings in teacher-based model distillation [14; 26].
153"
REWARD MODEL DISTILLATION,0.15605381165919283,"Furthermore, as rtgt is a real-valued model, at a bare minimum it is guaranteed to induce a regularized
154"
REWARD MODEL DISTILLATION,0.15695067264573992,"Bradley-Terry preference distribution ptgt(y1 ≻y2 | x) > 0, ∀x, y1, y2 ∈X × Y, and thereby avoid
155"
REWARD MODEL DISTILLATION,0.15784753363228698,"some of the degeneracies identiﬁed in §2.3 for the maximum likelihood estimate under DPO.
156"
PESSIMISTIC REWARD MODEL DISTILLATION,0.15874439461883408,"3.2
Pessimistic reward model distillation
157"
PESSIMISTIC REWARD MODEL DISTILLATION,0.15964125560538117,"Choosing a single reward model rtgt for anchoring the LM policy can naturally still lead to degenerate
158"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16053811659192826,"behavior if rtgt is a poor approximation of the true r∗that accurately reﬂects human preferences.
159"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16143497757847533,"However, we can easily extend our framework to handle uncertainty in the right target reward function
160"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16233183856502242,"by deﬁning a conﬁdence set of k ≥1 plausible target reward models, S =

r1
tgt, . . . , rk
tgt
	
, and
161"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16322869955156952,"training πθ∗(y | x) to maximize the following “pessimistic” form of the objective in (1):
162"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16412556053811658,"max
πθ
min
ri
tgt∈SEµ(x)
h
Eπθ(y|x)[ri
tgt(x, y)] −Eπref(y|x)[ri
tgt(x, y)]
|
{z
}
advantage over baseline policy"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16502242152466368,"−βDKL(πθ(· | x)∥πref(· | x))
i
. (8) Pβ(S) •π1"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16591928251121077,"•π2
•
π3 = π∗
θ"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16681614349775784,"•
πSFT"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16771300448430493,"•
πMLE"
PESSIMISTIC REWARD MODEL DISTILLATION,0.16860986547085202,KL(πSFT||π3)
PESSIMISTIC REWARD MODEL DISTILLATION,0.16950672645739912,"πSFT(yw) = 0.3
πSFT(yℓ) = 0.3
πMLE(yw) = 1.0
πMLE(yℓ) = 0.0
π3(yw) = 0.5
π3(yℓ) = 0.05"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17040358744394618,"Figure 1: A toy illustration of Theorem 2, which
states that the optimal πθ∗for (8) is the policy
in Pβ(S) with the lowest forward-KL from πSFT.
The set Pβ(S) contains a (potentially inﬁnite) set
of policies π1, π2, . . . corresponding to target re-
ward models. Here, πSFT assigns equal mass to yw
and yℓ, πMLE is the MLE solution for the DPO ob-
jective, which puts all probability mass on yw, and
π3 is the policy in Pβ(S) with lowest forward-KL."
PESSIMISTIC REWARD MODEL DISTILLATION,0.17130044843049327,"In this pessimistic objective we are no longer op-
163"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17219730941704037,"timizing πθ for a single reward, but optimizing
164"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17309417040358743,"πθ to produce generations that are scored favor-
165"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17399103139013453,"ably on average, even by the worst-case reward
166"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17488789237668162,"model in the set S, relative to the generations of
167"
PESSIMISTIC REWARD MODEL DISTILLATION,0.1757847533632287,"the baseline policy πref.When the set S = {r∗}
168"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17668161434977578,"consists of only the ground-truth reward, the ob-
169"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17757847533632287,"jective (8) is equivalent to standard RLHF (1),
170"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17847533632286997,"up to a constant offset independent of θ. More
171"
PESSIMISTIC REWARD MODEL DISTILLATION,0.17937219730941703,"generally, whenever S includes a good proxy
172"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18026905829596412,"er for r∗, the pessimistic advantage evaluation
173"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18116591928251122,"ensures that the the policy π∗
θ that maximizes
174"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18206278026905828,"eq. (8) still has a large advantage over πref under
175"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18295964125560538,"all r ∈S, including er. This use of pessimism
176"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18385650224215247,"to handle uncertainty in the knowledge of the
177"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18475336322869956,"true reward is related to similar techniques in
178"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18565022421524663,"the ofﬂine RL literature [16; 5].
179"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18654708520179372,"For the objective to be meaningful, the set S has to be chosen carefully. When S is small, it might
180"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18744394618834082,"not include any good proxy for r∗. Conversely, if S is too rich, it forces πθ∗to be nearly identical to
181"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18834080717488788,"πref, since any deviations from πref might be penalized by some reward model in S. Consequently,
182"
PESSIMISTIC REWARD MODEL DISTILLATION,0.18923766816143497,"we want to design S to be the smallest possible set which contains a reasonable approximation to r∗.
183"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19013452914798207,"To optimize (8), it turns out that we can formulate it as an equivalent constrained ofﬂine optimization
184"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19103139013452916,"problem, that we will show to conveniently admit a similar loss form as (7).
185"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19192825112107623,"Theorem 2 (Pessimistic distillation). Deﬁne the constrained minimizer
186"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19282511210762332,"πθ∗(y | x) ∈argmin
πθ∈Pβ(S)
βEµ(x)DKL(πref(· | x)∥πθ(· | x)),
(9)"
PESSIMISTIC REWARD MODEL DISTILLATION,0.1937219730941704,"where Pβ(S) is the set of all possible policies with implicit reward models that are consistent with
187"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19461883408071748,"any target reward model ri
tgt ∈S, i.e., Pβ(S) ≜{πθi}|S|
i=1 where πθi ∝πref(y | x) exp 1"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19551569506726457,"β ri
tgt(x, y).
188"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19641255605381167,"Then for any β > 0, πθ∗also maximizes the pessimistic alignment objective in (8).
189"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19730941704035873,"To unpack this result, Theorem 2 stipulates that the πθ that maximizes the pessimistic objective in (8)
190"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19820627802690582,"is the policy in Pβ(S) that is closest in forward KL-divergence to πref (see Figure 1).1 In addition,
191"
PESSIMISTIC REWARD MODEL DISTILLATION,0.19910313901345292,"this policy also maximizes the expected reward of one of the ri
tgt ∈S (minus the additional weighted
192"
PESSIMISTIC REWARD MODEL DISTILLATION,0.2,"reverse KL-divergence penalty term). Intuitively, the forward KL-divergence term serves the role of
193"
PESSIMISTIC REWARD MODEL DISTILLATION,0.20089686098654708,"biasing the model towards optimizing for reward models that are similar to the implicit reward that
194"
PESSIMISTIC REWARD MODEL DISTILLATION,0.20179372197309417,"πref already maximizes. Otherwise, there might exist a target reward model ri
tgt ∈S for which the
195"
PESSIMISTIC REWARD MODEL DISTILLATION,0.20269058295964126,"advantage of πθ relative to πref will be low, or even negative (a solution that we would like to avoid).
196"
OPTIMIZATION,0.20358744394618833,"3.2.1
Optimization
197"
OPTIMIZATION,0.20448430493273542,"The constraint in (9) can then be relaxed and approximately optimized by introducing an objective
198"
OPTIMIZATION,0.20538116591928252,"with a Lagrangian-style penalty with strength α > 0 on a form of distillation loss as (7), i.e.,
199"
OPTIMIZATION,0.2062780269058296,"min
πθ βEµ(x)DKL(πref(y | x)∥πθ(y | x)) + α min
ri
tgt∈S Ldistill(ri
tgt, πθ; ρ),
(10)"
OPTIMIZATION,0.20717488789237667,"where in practice we divide by α and instead optimize2
200"
OPTIMIZATION,0.20807174887892377,"Lpdistill(S, πθ; ρ) = min
ri
tgt∈S Ldistill(ri
tgt, πθ; ρ) + γEµ(x)DKL(πref(· | x)∥πθ(· | x)),
(11)"
OPTIMIZATION,0.20896860986547086,"where γ = βα−1. In reality, minimizing (11) for γ > 0 is equivalent to solving the constrained
201"
OPTIMIZATION,0.20986547085201793,"optimization problem in (9) with an implicitly larger set of possible reward models Sγ ⊇S indexed
202"
OPTIMIZATION,0.21076233183856502,"by γ. More speciﬁcally, Sγ also contains all reward models ˜r that are approximately consistent with
203"
OPTIMIZATION,0.2116591928251121,"the anchoring reward models ri
tgt contained in S, as the following result states.
204"
OPTIMIZATION,0.21255605381165918,"1Note that the objective in (9) minimizes the forward KL-divergence DKL(πref(· | x)∥πθ(· | x)) even
though the pessimistic objective in (8) is regularized with reverse KL-divergence DKL(πθ(· | x)∥πref(· | x)).
2In practice, we compute and optimize the min over reward models per each mini-batch of examples."
OPTIMIZATION,0.21345291479820627,"Proposition 2 (Soft pessimistic distillation). Assume the same conditions as Theorem 1. Then for
205"
OPTIMIZATION,0.21434977578475337,"any 0 < γ < ∞, there exists a λ ≥0 such that πθ∗(y | x) ∈argminπθ Lpdistill(S, πθ; ρ), where πθ∗
206"
OPTIMIZATION,0.21524663677130046,"is a minimizer over all possible policies, is a solution to (9) for the effective reward model set
207"
OPTIMIZATION,0.21614349775784752,"Sγ =
["
OPTIMIZATION,0.21704035874439462,"ri
tgt∈S"
OPTIMIZATION,0.2179372197309417,"n
˜r: Eρ(x,y1,y2)

(ri
tgt(x, y1) −ri
tgt(x, y2) −˜r(x, y1) + ˜r(x, y2))2
≤λ
o
.
(12)"
OPTIMIZATION,0.21883408071748878,"As a result, optimizing (11) even when using the singleton S = {rtgt} yields an implicitly pessimistic
208"
OPTIMIZATION,0.21973094170403587,"objective, in which the pessimism is over all reward models ˜r that are consistent up to λ with rtgt.
209"
PESSIMISTIC DPO,0.22062780269058296,"3.3
Pessimistic DPO
210"
PESSIMISTIC DPO,0.22152466367713006,"We can also observe that Proposition 2 can be leveraged to obtain an alternative, implicitly pessimistic,
211"
PESSIMISTIC DPO,0.22242152466367712,"objective that uses DPO directly instead of distillation. Consider the following regularized DPO loss:
212"
PESSIMISTIC DPO,0.22331838565022422,"Lpdpo(πθ; Dpref) = Ldpo(πθ; Dpref) + γEµ(x)DKL(πref(y | x)∥πθ(y | x)).
(13)"
PESSIMISTIC DPO,0.2242152466367713,"Following a similar analysis as in Proposition 2, we can derive that this implicitly corresponds to
213"
PESSIMISTIC DPO,0.22511210762331837,"maximizing the pessimistic objective in (8) for the reward model set
214"
PESSIMISTIC DPO,0.22600896860986547,"Sγ =
n
rπθ : Ldpo(πθ; Dpref) ≤min
π′
θ
Ldpo(π′
θ; Dpref) + λ
o
,
(14)"
PESSIMISTIC DPO,0.22690582959641256,"where rπθ(x, y) ≜β log πθ(y | x)/πref(y | x) + β log Z(x) is the implicit reward model deﬁned by
215"
PESSIMISTIC DPO,0.22780269058295965,"πθ. Sγ then corresponds to the set of reward models rπθ that are all approximate minimizers of the
216"
PESSIMISTIC DPO,0.22869955156950672,"DPO loss. This not only includes the MLE, but also all other estimators that obtain nearly the same
217"
PESSIMISTIC DPO,0.2295964125560538,"loss. In principle, this can be expected to help ameliorate some of the issues of §2.3: since driving the
218"
PESSIMISTIC DPO,0.2304932735426009,"reward to ±∞only marginally decreases the Ldpo loss past a certain point, the set S will also include
219"
PESSIMISTIC DPO,0.23139013452914797,"ﬁnite reward functions |rπθ(x, y)| < ∞for any γ > 0. These rewards would then be preferred if they
220"
PESSIMISTIC DPO,0.23228699551569507,"induce a policy with a smaller (forward) KL-divergence to πref than the degenerate, inﬁnite rewards.
221"
EXPERIMENTAL RESULTS,0.23318385650224216,"4
Experimental results
222"
EXPERIMENTAL RESULTS,0.23408071748878923,"The main motivation for reward distillation and pessimism is to increase alignment robustness
223"
EXPERIMENTAL RESULTS,0.23497757847533632,"in challenging settings where it is difﬁcult to learn good policies directly from the preference
224"
EXPERIMENTAL RESULTS,0.2358744394618834,"data. To demonstrate the effectiveness of our approach, we run experiments on the popular TL;DR
225"
EXPERIMENTAL RESULTS,0.2367713004484305,"summarization task [29; 32], in which we simulate a scenario where the preference data has a spurious
226"
EXPERIMENTAL RESULTS,0.23766816143497757,"correlation between the length of a summary and whether or not it is preferred.3
227"
EXPERIMENTAL SETUP,0.23856502242152466,"4.1
Experimental setup
228"
EXPERIMENTAL SETUP,0.23946188340807176,"We ﬁrst train an “oracle” reward model on the TL;DR preference data training set [29] and relabel
229"
EXPERIMENTAL SETUP,0.24035874439461882,"all preference pairs with this oracle. This enables us to use the oracle reward model for evaluation,
230"
EXPERIMENTAL SETUP,0.24125560538116592,"without worrying about the gap to true human preferences. After relabeling, longer responses (where
231"
EXPERIMENTAL SETUP,0.242152466367713,"longer is deﬁned as y1 having at least 10% more tokens than y2) are preferred in 61% of the examples.
232"
EXPERIMENTAL SETUP,0.2430493273542601,"To test the effect of a spurious correlation on preference-based policy optimization, we select as a
233"
EXPERIMENTAL SETUP,0.24394618834080717,"training set 30K examples from the relabeled data such that the longer output is preferred in ρ fraction
234"
EXPERIMENTAL SETUP,0.24484304932735426,"of examples, with ρ ∈{0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. Each such training set is denoted Dρ. At
235"
EXPERIMENTAL SETUP,0.24573991031390136,"each Dρ, we compare our approach to DPO [23] and IPO [3], which are currently the most commonly
236"
EXPERIMENTAL SETUP,0.24663677130044842,"used ofﬂine alignment methods. We test the following variants of distillation and pessimism:
237"
EXPERIMENTAL SETUP,0.24753363228699551,"• Distilled DPO (d-DPO): Trains a reward model rρ on Dρ, and then optimizes Ldistill(rρ, πθ; ρ).
238"
EXPERIMENTAL SETUP,0.2484304932735426,"• Pessimistic DPO (p-DPO): A pessimistic version of DPO as described in §3.3, trained on Dρ.
239"
EXPERIMENTAL SETUP,0.24932735426008967,"• Pessimistic Distilled DPO (pd-DPO): Combines the above two by training a reward model rρ on
240"
EXPERIMENTAL SETUP,0.2502242152466368,"Dρ and optimizing the pessimistic distillation objective (Eq. (11)) with conﬁdence set S = {rtgt}.
241"
EXPERIMENTAL SETUP,0.25112107623318386,"• Pessimistic Ensemble DPO (e-DPO): To create ensembles of reward models, we subsample from
242"
EXPERIMENTAL SETUP,0.2520179372197309,"each Dρ ﬁve preference datasets, Dρ,b, at b ∈B = {0.2, 0.4, 0.5, 0.6, 0.8}, such that the fraction
243"
EXPERIMENTAL SETUP,0.25291479820627805,3Length has been repeatedly shown in the past to correlate with reward [28; 21].
EXPERIMENTAL SETUP,0.2538116591928251,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
length bias ( ) 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
EXPERIMENTAL SETUP,0.2547085201793722,oracle reward
EXPERIMENTAL SETUP,0.2556053811659193,Method
EXPERIMENTAL SETUP,0.25650224215246636,"DPO
IPO
d-DPO
dp-DPO
e-DPO
p-DPO"
EXPERIMENTAL SETUP,0.25739910313901343,"Figure 2: Main results, showing the advantage in oracle reward compared to the initial ﬁnetuned
policy. Errorbars correspond to bootstrap 95% conﬁdence intervals for ﬁnite sample variance.
Ensemble DPO (e-DPO) is signiﬁcantly better than DPO and IPO in the challenging setup where
shorter responses are preferred (ρ ≤0.5), and is generally the best-performing method overall in this
regime. Distilled DPO (d-DPO) performs best when longer responses are preferred (ρ > 0.6)."
EXPERIMENTAL SETUP,0.25829596412556055,"of pairs where the longer response is preferred is b, and train reward models rρ,b on those subsets.
244"
EXPERIMENTAL SETUP,0.2591928251121076,"Consequently, sensitivity to length should vary across ensemble members. We then apply the
245"
EXPERIMENTAL SETUP,0.2600896860986547,"same procedure as pd-DPO above, with a conﬁdence set Sρ = {rρ,b}B
b=1.
246"
EXPERIMENTAL SETUP,0.2609865470852018,"All reward models and policies are initialized from Palm-2-XS [2]. Policies also go through a
247"
EXPERIMENTAL SETUP,0.26188340807174887,"supervised ﬁnetuning step on human-written summaries from the original TL;DR training set [32]
248"
EXPERIMENTAL SETUP,0.262780269058296,"prior to alignment, and we term this policy πSFT. We evaluate performance by sampling summaries
249"
EXPERIMENTAL SETUP,0.26367713004484306,"for test set prompts, evaluating the average reward according to the oracle reward model, and
250"
EXPERIMENTAL SETUP,0.2645739910313901,"computing the advantage in average reward compared to πSFT (before alignment). We train policies
251"
EXPERIMENTAL SETUP,0.26547085201793724,"for 104 steps with batch size 16 and learning rate 10−6, and reward models for 3k steps with
252"
EXPERIMENTAL SETUP,0.2663677130044843,"batch size 64 and learning rate 4 × 10−6. We use the validation set for model selection during
253"
EXPERIMENTAL SETUP,0.2672645739910314,"policy training and to choose the following hyperparameters. For all DPO variants, we sweep over
254"
EXPERIMENTAL SETUP,0.2681614349775785,"β ∈{.01, .1, 1, 3, 10, 30, 100}. For IPO, we sweep over τ ∈{0.01, 0.1, 1, 3, 5, 10, 25}. For all
255"
EXPERIMENTAL SETUP,0.26905829596412556,"pessimistic methods we anneal γ = α/β from 10−4 to 10−2 linearly during the 10k training steps.
256"
RESULTS,0.2699551569506726,"4.2
Results
257"
RESULTS,0.27085201793721975,"We present the results of our experiment in Figure 2. As can be seen in the plot, the more challenging
258"
RESULTS,0.2717488789237668,"setting is when ρ < 0.5, which corresponds to a sample of preference annotations in which shorter
259"
RESULTS,0.2726457399103139,"outputs are generally preferred. This distribution shift is more difﬁcult because as mentioned the oracle
260"
RESULTS,0.273542600896861,"reward model (trained on human annotations) has a bias in favor of longer outputs [28]. Nevertheless
261"
RESULTS,0.27443946188340806,"we get sizable improvements compared to the reference policy πSFT for all length bias values.
262"
RESULTS,0.27533632286995513,"All approaches that invoke distillation (d-DPO, e-DPO, dp-DPO) outperform IPO and DPO (p < .01
263"
RESULTS,0.27623318385650225,"by a Wald test) for ρ ≤0.5, where shorter responses are preferred. Pessimistic ensemble DPO
264"
RESULTS,0.2771300448430493,"(e-DPO) performs particularly well in these settings, generally outperforming all methods that use
265"
RESULTS,0.27802690582959644,"a single reward model. When longer responses are preferred (ρ > 0.6), single reward distillation
266"
RESULTS,0.2789237668161435,"(d-DPO) leads to the highest performance, signiﬁcantly outperforming both DPO and IPO (p < .01
267"
RESULTS,0.27982062780269057,"by a Wald test). Interestingly, p-DPO does not provide empirical beneﬁts relative to the distillation
268"
RESULTS,0.2807174887892377,"based methods, indicating that the distillation loss itself is quite important. For the effect of
269"
RESULTS,0.28161434977578476,"hyper-parameter selection, see Figure D.1. In DPO-based methods, the optimal value of β is inversely
270"
RESULTS,0.2825112107623318,"correlated with the bias; in IPO the same holds for the τ hyperparameter.
271"
RESULTS,0.28340807174887894,"To better understand the utility of reward ensembles in e-DPO, in particular when ρ < 0.5, we
272"
RESULTS,0.284304932735426,"examine the role of each reward model in the ensemble across different biases. Speciﬁcally, given
273"
RESULTS,0.2852017937219731,"the ﬁnal e-DPO policy per length bias, for each example we identify the reward model rρ,b that
274"
RESULTS,0.2860986547085202,"best matches the implicit reward of this policy, i.e., for which reward model is Ldistill minimized on
275"
RESULTS,0.28699551569506726,"that example (see Eq. (7) and (11)). We ﬁnd that when the policy is trained on data where shorter
276"
RESULTS,0.2878923766816143,"preference are preferred (ρ < .5), the reward model that best matches the policy often has the opposite
277"
RESULTS,0.28878923766816145,"bias (b is high), and vice versa. Thus, the success of e-DPO may be explained by its ability to distill
278"
RESULTS,0.2896860986547085,"from reward models that do not suffer from the bias in the policy training data, which is particularly
279"
RESULTS,0.2905829596412556,"helpful when ρ ≤.5 as this bias is also not shared by the oracle RM. We provide the full distribution
280"
RESULTS,0.2914798206278027,"over reward models for all ρ and β in App. C. Overall, these results demonstrate the efﬁcacy of
281"
RESULTS,0.29237668161434976,"training a policy by distilling from a reward model in the presence of distribution shifts, and that a
282"
RESULTS,0.2932735426008969,"careful design of an ensemble to mitigate spurious correlations can lead to further performance gains.4
283"
THEORETICAL ANALYSIS,0.29417040358744395,"5
Theoretical analysis
284"
THEORETICAL ANALYSIS,0.295067264573991,"This section characterizes problems with the DPO objective and solutions offered by pessimistic DPO
285"
THEORETICAL ANALYSIS,0.29596412556053814,"and distillation, focusing on the simpliﬁed scenario in which we optimize with respect to a single
286"
THEORETICAL ANALYSIS,0.2968609865470852,"preference pairs (yw, yℓ). Once again, all proofs are deferred to Appendix A.
287"
THEORETICAL ANALYSIS,0.29775784753363227,"In its Lagrangian formulation, pessimistic DPO adds a forward KL term to the DPO objective (§3.3).
288"
THEORETICAL ANALYSIS,0.2986547085201794,"For the sake of analysis, we assume that the preference annotations are sampled from the reference
289"
THEORETICAL ANALYSIS,0.29955156950672646,"distribution, µ(x) × πref(y | x) × πref(y | x). Then a ﬁnite-sample approximation of the forward
290"
THEORETICAL ANALYSIS,0.3004484304932735,KL term is ˆΩ(Θ) := P
THEORETICAL ANALYSIS,0.30134529147982064,"(yw,yℓ)∈DPref −(log πθ(yℓ) + log πθ(yw)). By applying this ﬁnite-sample
291"
THEORETICAL ANALYSIS,0.3022421524663677,"approximation, p-DPO has a ﬁnite optimum, unlike DPO, as shown in Proposition 1. Note that this
292"
THEORETICAL ANALYSIS,0.3031390134529148,"analysis is limited in two ways: (1) as mentioned, we compute the KL term over the completions
293"
THEORETICAL ANALYSIS,0.3040358744394619,"in the preference data; (2) we directly optimize the probability ratios ψw = πθ(yw)/πref(yw) and
294"
THEORETICAL ANALYSIS,0.30493273542600896,"ψℓ= πθ(yℓ)/πref(yℓ), rather than optimizing them jointly through the parameters. For sufﬁciently ex-
295"
THEORETICAL ANALYSIS,0.305829596412556,"pressive πθ, however, this approximation captures the behavior of the two algorithms reasonably well.
296"
THEORETICAL ANALYSIS,0.30672645739910315,"Proposition 3. Let ˆLpdpo represent a ﬁnite-sample approximation to Lpdpo with the empir-
297"
THEORETICAL ANALYSIS,0.3076233183856502,"ical forward KL term ˆΩ(Θ). For a ﬁxed ˆπθ(yw
i ) and α
>
1, the argminπθ(yℓ) ˆLpdpo is
298"
THEORETICAL ANALYSIS,0.30852017937219733,"min
 
1 −ˆπθ(yw
i ), ˆπθ(yℓ
i)

, with log ˆπθ(yℓ
i) = −1"
THEORETICAL ANALYSIS,0.3094170403587444,"β log (α −1) + log ˆπθ(yw
i ) + log πref(yℓ
i )
πref(yw
i ).
299"
THEORETICAL ANALYSIS,0.31031390134529147,"The optimum in Proposition 3 corresponds to log ψw/ψℓ= β−1 log(α −1). Recall that IPO seeks
300"
THEORETICAL ANALYSIS,0.3112107623318386,to assign a constant value to this ratio by minimizing (log ψw
THEORETICAL ANALYSIS,0.31210762331838565,"ψℓ−τ −1)2; the (unconstrained) optima
301"
THEORETICAL ANALYSIS,0.3130044843049327,"are identical for τ −1 := β−1 log(α −1), but the loss surfaces are different (see Appendix B). DPO
302"
THEORETICAL ANALYSIS,0.31390134529147984,"sets πθ(yℓ
i) →0, as shown in Corollary 1; this is due not only to competition from πθ(yw
i ) but from
303"
THEORETICAL ANALYSIS,0.3147982062780269,"DPO penalizing positive probability on yℓ
i. Analysis of the distilled loss gives a similar result:
304"
THEORETICAL ANALYSIS,0.31569506726457397,"Proposition 4. For any ﬁxed ˆπθ(yw
i ) and β > 0, the argmin of the distilled DPO objective (eq. (7))
305"
THEORETICAL ANALYSIS,0.3165919282511211,"is min(1−ˆπθ(yw
i ), ˆπθ(yℓ
i), with log ˆπθ(yℓ
i) = 1"
THEORETICAL ANALYSIS,0.31748878923766816,"β (rt(x, yℓ
i)−rt(x, yw
i ))+log ˆπθ(yw
i )+log πref(yℓ
i )
πref(yw
i ).
306"
THEORETICAL ANALYSIS,0.3183856502242152,"While the setting is simplistic, the results are comforting: here the additional regularization effects of
307"
THEORETICAL ANALYSIS,0.31928251121076234,"both distillation and pessimism (in the case of p-DPO) clearly help to avoid degenerate optima.
308"
THEORETICAL ANALYSIS,0.3201793721973094,"Why DPO can drive π(yw) to zero.
In §2.3 we pointed out a peculiarity of the DPO global optima:
309"
THEORETICAL ANALYSIS,0.32107623318385653,"in certain cases, it can include policies where π(yw) may be nearly 0 for all yw in the training set. This
310"
THEORETICAL ANALYSIS,0.3219730941704036,"undesirable behavior has also been observed in practice [20; 22; 30]. For intuition on why this may
311"
THEORETICAL ANALYSIS,0.32286995515695066,"happen, consider the simpliﬁed case where the policy is a bag-of-words model, πθ(y) ∝exp (c(y) · θ)
312"
THEORETICAL ANALYSIS,0.3237668161434978,"for c(y) representing a vector of counts in y and θi representing the unnormalized log-probability of
313"
THEORETICAL ANALYSIS,0.32466367713004485,"token i. Then we can formally show that DPO optimization monotonically decreases an upper bound
314"
THEORETICAL ANALYSIS,0.3255605381165919,"on the probability of the preferred completion, ˜πθ(t−1)(yw) ≥˜πθ(t)(yw) ≥πθ(t)(yw).
315"
THEORETICAL ANALYSIS,0.32645739910313903,"Proposition 5. Let yw, yℓ
∈Vn be preferred vs.
dispreferred outputs of length n, with
316"
THEORETICAL ANALYSIS,0.3273542600896861,"πref(yw), πref(yℓ) > 0 and corresponding count vectors c(yw), c(yℓ). Let log πθ(y) = c(y) · θ −
317"
THEORETICAL ANALYSIS,0.32825112107623317,"nZ(θ) for Z(θ) = log PV
i eθi, with upper bound log ˜πθ(y) = c(y)·θ−n maxj θj. Let θ(t) represent
318"
THEORETICAL ANALYSIS,0.3291479820627803,"the parameters of π after t steps of gradient descent on Ldpo({yℓ, yw, x}), with θ(0) = 0. Then
319"
THEORETICAL ANALYSIS,0.33004484304932735,"πθ(t)(yw) ≤˜πθ(t)(yw) ≤˜πθ(t−1)(yw) for all t.
320"
THEORETICAL ANALYSIS,0.3309417040358744,"Where does the probability mass go?
If πθ(t)(yw) decreases in t, what other strings become
321"
THEORETICAL ANALYSIS,0.33183856502242154,"more probable? In the following proposition, we show that under the bag-of-words model, DPO
322"
THEORETICAL ANALYSIS,0.3327354260089686,"optimization moves probability mass away from yw to sequences that contain only the tokens that
323"
THEORETICAL ANALYSIS,0.33363228699551567,"maximize the difference between yw and yℓ. This is a concrete example of the type of undesirable
324"
THEORETICAL ANALYSIS,0.3345291479820628,"optima described in §2.3, now shown here to be realizable.
325"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.33542600896860986,"4We also experimented with an ensemble where members are different checkpoints across training of a
reward model on the preference data and did not observe any empirical gains from this form of ensemble."
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.336322869955157,"Proposition 6. Let yw and yℓbe preferred / dispreferred outputs of length n. Let ∆= c(yw) −c(yℓ)
326"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.33721973094170404,"be the difference in unigram counts. Let ˆy = [i, i, . . . , i], for i ∈arg max ∆, with ||c(ˆy)||1 = n.
327"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.3381165919282511,"Then πθ(t)(yw) −πθ(t)(ˆy) = τ(t)k for some k ≤0 and some non-decreasing τ : Z+ →R+.
328"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.33901345291479823,"We have k = 0 when c(yw) = c(ˆy), and k ≪0 when ||c(yw)||2 ≪||c(ˆy)||2 = n (dense c(yw)) and
329"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.3399103139013453,"||∆||2 = ||∆||∞(sparse ∆). This implies that when yw and yℓare similar, πθ(yw) will degrade more
330"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.34080717488789236,"rapidly. Early stopping will therefore tradeoff between reaching the degenerate solution on such
331"
WE ALSO EXPERIMENTED WITH AN ENSEMBLE WHERE MEMBERS ARE DIFFERENT CHECKPOINTS ACROSS TRAINING OF A,0.3417040358744395,"cases, and underﬁtting other cases in which yw and yℓare more distinct.
332"
RELATED WORK,0.34260089686098655,"6
Related work
333"
RELATED WORK,0.3434977578475336,"Recent work in ofﬂine alignment has focused on DPO [23] as a simpler alternative for aligning
334"
RELATED WORK,0.34439461883408073,"language models from preference data. Subsequent work has identiﬁed issues with DPO, including
335"
RELATED WORK,0.3452914798206278,"weak regularization [3] and a tendency to decrease the probability of winning generations during
336"
RELATED WORK,0.34618834080717487,"training [20]. Other methods have explored various avenues for improvement. These include
337"
RELATED WORK,0.347085201793722,"analyzing the impact of noise on DPO alignment [11], proposing to update the reference policy
338"
RELATED WORK,0.34798206278026905,"during training [12], and suggesting a variant of IPO with a per-context margin [1]. Additional
339"
RELATED WORK,0.3488789237668161,"research has focused on token-level alignment methods [38; 22] and on developing a uniﬁed view of
340"
RELATED WORK,0.34977578475336324,"various ofﬂine alignment methods [31]. This work builds upon several these ﬁndings, and provides
341"
RELATED WORK,0.3506726457399103,"further analysis, as well as a solution based on pessimism and reward distillation.
342"
RELATED WORK,0.3515695067264574,"While ofﬂine alignment methods are popular, recent evidence suggests that online alignment methods
343"
RELATED WORK,0.3524663677130045,"such as RLHF [6; 29], may lead to more favorable outcomes [13; 30; 8; 34]. Notably, Zhu et al. [41]
344"
RELATED WORK,0.35336322869955156,"proposed iterative data smoothing, which uses a trained model to softly label data during RLHF.
345"
RELATED WORK,0.3542600896860987,"Whether online or ofﬂine, however, policies are still succeptible to overﬁtting to certain degenerate
346"
RELATED WORK,0.35515695067264574,"phenomena. To this end, reward ensembles have been widely investigated recently as a mechanism
347"
RELATED WORK,0.3560538116591928,"for tackling reward hacking in RLHF [9; 7; 39; 25], and in the context of multi-objective optimization
348"
RELATED WORK,0.35695067264573993,"[19; 24]. We use an ensemble of rewards to represent the uncertainty with respect to reward models
349"
RELATED WORK,0.357847533632287,"that are suitable given preference data. Moskovitz et al. [19] focus on “composite” rewards, with the
350"
RELATED WORK,0.35874439461883406,"goal of achieving high task reward while ensuring that every individual component is above some
351"
RELATED WORK,0.3596412556053812,"threshold—also by applying a Lagrangian relaxation. In this work, we also consider multiple reward
352"
RELATED WORK,0.36053811659192825,"models, but we only focus on cases where there is no known, obvious reward decomposition.
353"
RELATED WORK,0.3614349775784753,"Finally, the question of using a small amount of ofﬂine data to learn high-quality policies, instead
354"
RELATED WORK,0.36233183856502243,"of online access to reward feedback, has been widely studied in the ofﬂine reinforcement learning
355"
RELATED WORK,0.3632286995515695,"(RL) literature. The predominant approach here is to use pessimism, that is, to learn a policy with
356"
RELATED WORK,0.36412556053811657,"the highest reward under all plausible environment models consistent with the data, with an extensive
357"
RELATED WORK,0.3650224215246637,"theoretical [18; 37; 33] and empirical [16; 5; 36] body of supporting work. The key insight in this
358"
RELATED WORK,0.36591928251121075,"literature is that without pessimism, the RL algorithm learns undesirable behaviors which are not
359"
RELATED WORK,0.3668161434977579,"explicitly ruled out in the training data, and pessimism provides a robust way of preventing such
360"
RELATED WORK,0.36771300448430494,"undesirable extrapolations, while still preserving generalization within the support of the data.
361"
CONCLUSION,0.368609865470852,"7
Conclusion
362"
CONCLUSION,0.3695067264573991,"LM alignment is crucial for deploying safe and helpful assistants, but is difﬁcult due to lack of
363"
CONCLUSION,0.3704035874439462,"access to perfect preference oracles. We presented a thorough theoretical analysis of some of
364"
CONCLUSION,0.37130044843049326,"the degeneracies that DPO is susceptible to when learning from sampled human preference data.
365"
CONCLUSION,0.3721973094170404,"Furthermore, our ﬁndings suggest that explicit reward modeling remains a powerful vehicle for
366"
CONCLUSION,0.37309417040358744,"introducing regularization into post-training. By distilling the reward assigned by a single, explicit
367"
CONCLUSION,0.3739910313901345,"reward model—or a family of explicit reward models—directly into the implicit reward maximized
368"
CONCLUSION,0.37488789237668163,"by our policies using ofﬂine data, we demonstrated that we can achieve improved robustness to
369"
CONCLUSION,0.3757847533632287,"variations in preference dataset quality, while maintaining the simplicity of the DPO framework.
370"
CONCLUSION,0.37668161434977576,"Limitations. The empirical results in the paper are based on one dataset and form of distribution shift.
371"
CONCLUSION,0.3775784753363229,"For deeper understanding of pessimism and ensembling, additional settings should be explored. The
372"
CONCLUSION,0.37847533632286995,"theoretical aspects of the paper are sometimes based on restrictive assumptions and simpliﬁcations.
373"
CONCLUSION,0.379372197309417,"Nonetheless, they provide potential explanations for phenomena observed in real-world settings.
374"
CONCLUSION,0.38026905829596414,"Broader impact. We introduce new ideas to the active ﬁeld of research on preference-based post-
375"
CONCLUSION,0.3811659192825112,"training, which we hope will help facilitate the alignment of large models, and improve understanding
376"
CONCLUSION,0.3820627802690583,"of current approaches—ultimately supporting the development of capable and reliable AI systems.
377"
REFERENCES,0.3829596412556054,"References
378"
REFERENCES,0.38385650224215245,"[1] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset.
379"
REFERENCES,0.3847533632286996,"arXiv preprint arXiv:2402.10571, 2024.
380"
REFERENCES,0.38565022421524664,"[2] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
381"
REFERENCES,0.3865470852017937,"Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
382"
REFERENCES,0.3874439461883408,"Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,
383"
REFERENCES,0.3883408071748879,"Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing
384"
REFERENCES,0.38923766816143496,"Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,
385"
REFERENCES,0.3901345291479821,"James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin
386"
REFERENCES,0.39103139013452914,"Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave,
387"
REFERENCES,0.3919282511210762,"Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg,
388"
REFERENCES,0.39282511210762333,"Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas
389"
REFERENCES,0.3937219730941704,"Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,
390"
REFERENCES,0.39461883408071746,"Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,
391"
REFERENCES,0.3955156950672646,"Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin
392"
REFERENCES,0.39641255605381165,"Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao
393"
REFERENCES,0.39730941704035877,"Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,
394"
REFERENCES,0.39820627802690584,"Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,
395"
REFERENCES,0.3991031390134529,"Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan
396"
REFERENCES,0.4,"Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee
397"
REFERENCES,0.4008968609865471,"Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha
398"
REFERENCES,0.40179372197309415,"Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,
399"
REFERENCES,0.4026905829596413,"John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,
400"
REFERENCES,0.40358744394618834,"Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui
401"
REFERENCES,0.4044843049327354,"Wu. Palm 2 technical report, 2023.
402"
REFERENCES,0.4053811659192825,"[3] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland,
403"
REFERENCES,0.4062780269058296,"Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning
404"
REFERENCES,0.40717488789237666,"from human preferences. In International Conference on Artiﬁcial Intelligence and Statistics,
405"
REFERENCES,0.4080717488789238,"pages 4447–4455. PMLR, 2024.
406"
REFERENCES,0.40896860986547084,"[4] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the
407"
REFERENCES,0.4098654708520179,"method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. ISSN 00063444. URL
408"
REFERENCES,0.41076233183856503,"http://www.jstor.org/stable/2334029.
409"
REFERENCES,0.4116591928251121,"[5] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor
410"
REFERENCES,0.4125560538116592,"critic for ofﬂine reinforcement learning. In International Conference on Machine Learning,
411"
REFERENCES,0.4134529147982063,"pages 3852–3878. PMLR, 2022.
412"
REFERENCES,0.41434977578475335,"[6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
413"
REFERENCES,0.41524663677130047,"reinforcement learning from human preferences. Advances in neural information processing
414"
REFERENCES,0.41614349775784754,"systems, 30, 2017.
415"
REFERENCES,0.4170403587443946,"[7] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help
416"
REFERENCES,0.4179372197309417,"mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.
417"
REFERENCES,0.4188340807174888,"[8] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen
418"
REFERENCES,0.41973094170403585,"Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workﬂow: From reward modeling to online rlhf.
419"
REFERENCES,0.420627802690583,"2024.
420"
REFERENCES,0.42152466367713004,"[9] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvi-
421"
REFERENCES,0.4224215246636771,"jotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping
422"
REFERENCES,0.4233183856502242,"or herding? Reward model ensembles mitigate but do not eliminate reward hacking. arXiv
423"
REFERENCES,0.4242152466367713,"preprint arXiv:2312.09244, 2023.
424"
REFERENCES,0.42511210762331836,"[10] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
425"
REFERENCES,0.4260089686098655,"Born again neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the
426"
REFERENCES,0.42690582959641254,"35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
427"
REFERENCES,0.42780269058295967,"Learning Research, pages 1607–1616. PMLR, 10–15 Jul 2018. URL https://proceedings.
428"
REFERENCES,0.42869955156950673,"mlr.press/v80/furlanello18a.html.
429"
REFERENCES,0.4295964125560538,"[11] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment
430"
REFERENCES,0.4304932735426009,"performance of generative language models. arXiv preprint arXiv:2404.09824, 2024.
431"
REFERENCES,0.431390134529148,"[12] Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Ak-
432"
REFERENCES,0.43228699551569505,"senov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn your reference model for
433"
REFERENCES,0.43318385650224217,"real good alignment. arXiv preprint arXiv:2404.09656, 2024.
434"
REFERENCES,0.43408071748878924,"[13] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexan-
435"
REFERENCES,0.4349775784753363,"dre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from
436"
REFERENCES,0.4358744394618834,"online ai feedback. arXiv preprint arXiv:2402.04792, 2024.
437"
REFERENCES,0.4367713004484305,"[14] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
438"
REFERENCES,0.43766816143497755,"In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.
439"
REFERENCES,0.4385650224215247,"org/abs/1503.02531.
440"
REFERENCES,0.43946188340807174,"[15] Tomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed
441"
REFERENCES,0.44035874439461886,"as bayesian inference. In Findings of the Association for Computational Linguistics: EMNLP
442"
REFERENCES,0.4412556053811659,"2022, pages 1083–1091, 2022.
443"
REFERENCES,0.442152466367713,"[16] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning
444"
REFERENCES,0.4430493273542601,"for ofﬂine reinforcement learning. Advances in Neural Information Processing Systems, 33:
445"
REFERENCES,0.4439461883408072,"1179–1191, 2020.
446"
REFERENCES,0.44484304932735425,"[17] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,
447"
REFERENCES,0.44573991031390137,"Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human
448"
REFERENCES,0.44663677130044843,"feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.
449"
REFERENCES,0.4475336322869955,"[18] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch
450"
REFERENCES,0.4484304932735426,"off-policy reinforcement learning without great exploration. Advances in neural information
451"
REFERENCES,0.4493273542600897,"processing systems, 33:1264–1274, 2020.
452"
REFERENCES,0.45022421524663675,"[19] Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D
453"
REFERENCES,0.45112107623318387,"Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained
454"
REFERENCES,0.45201793721973094,"rlhf. arXiv preprint arXiv:2310.04373, 2023.
455"
REFERENCES,0.452914798206278,"[20] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White.
456"
REFERENCES,0.4538116591928251,"Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint
457"
REFERENCES,0.4547085201793722,"arXiv:2402.13228, 2024.
458"
REFERENCES,0.4556053811659193,"[21] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from
459"
REFERENCES,0.4565022421524664,"quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024.
460"
REFERENCES,0.45739910313901344,"[22] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model
461"
REFERENCES,0.45829596412556056,"is secretly a q-function. 2024.
462"
REFERENCES,0.4591928251121076,"[23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
463"
REFERENCES,0.4600896860986547,"Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
464"
REFERENCES,0.4609865470852018,"Advances in Neural Information Processing Systems, 36, 2024.
465"
REFERENCES,0.4618834080717489,"[24] Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya,
466"
REFERENCES,0.46278026905829595,"Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by
467"
REFERENCES,0.46367713004484307,"interpolating weights ﬁne-tuned on diverse rewards, 2023.
468"
REFERENCES,0.46457399103139013,"[25] Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier
469"
REFERENCES,0.4654708520179372,"Bachem, and Johan Ferret. Warm: On the beneﬁts of weight averaged reward models. 2024.
470"
REFERENCES,0.4663677130044843,"[26] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
471"
REFERENCES,0.4672645739910314,"Yoshua Bengio. Fitnets: Hints for thin deep nets. In In Proceedings of ICLR, 2015.
472"
REFERENCES,0.46816143497757845,"[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
473"
REFERENCES,0.46905829596412557,"policy optimization algorithms. 2017.
474"
REFERENCES,0.46995515695067264,"[28] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating
475"
REFERENCES,0.47085201793721976,"length correlations in rlhf. 2023.
476"
REFERENCES,0.4717488789237668,"[29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
477"
REFERENCES,0.4726457399103139,"Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.
478"
REFERENCES,0.473542600896861,"Advances in Neural Information Processing Systems, 33:3008–3021, 2020.
479"
REFERENCES,0.4744394618834081,"[30] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie,
480"
REFERENCES,0.47533632286995514,"Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference ﬁne-tuning of llms should leverage
481"
REFERENCES,0.47623318385650226,"suboptimal, on-policy data, 2024.
482"
REFERENCES,0.47713004484304933,"[31] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark
483"
REFERENCES,0.4780269058295964,"Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, and Bilal Piot.
484"
REFERENCES,0.4789237668161435,"Generalized preference optimization: A uniﬁed approach to ofﬂine alignment. 2024.
485"
REFERENCES,0.4798206278026906,"[32] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit
486"
REFERENCES,0.48071748878923765,"to learn automatic summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini,
487"
REFERENCES,0.48161434977578477,"and Fei Liu, editors, Proceedings of the Workshop on New Frontiers in Summarization, pages
488"
REFERENCES,0.48251121076233183,"59–63, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
489"
REFERENCES,0.4834080717488789,"doi: 10.18653/v1/W17-4508. URL https://aclanthology.org/W17-4508.
490"
REFERENCES,0.484304932735426,"[33] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-
491"
REFERENCES,0.4852017937219731,"consistent pessimism for ofﬂine reinforcement learning. Advances in neural information
492"
REFERENCES,0.4860986547085202,"processing systems, 34:6683–6694, 2021.
493"
REFERENCES,0.48699551569506727,"[34] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao
494"
REFERENCES,0.48789237668161434,"Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. 2024.
495"
REFERENCES,0.48878923766816146,"[35] Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L. Yuille. Training deep neural networks
496"
REFERENCES,0.4896860986547085,"in generations: a more tolerant teacher educates better students. In Proceedings of the Thirty-
497"
REFERENCES,0.4905829596412556,"Third AAAI Conference on Artiﬁcial Intelligence and Thirty-First Innovative Applications of
498"
REFERENCES,0.4914798206278027,"Artiﬁcial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in
499"
REFERENCES,0.4923766816143498,"Artiﬁcial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-
500"
REFERENCES,0.49327354260089684,"1. doi: 10.1609/aaai.v33i01.33015628. URL https://doi.org/10.1609/aaai.v33i01.
501"
REFERENCES,0.49417040358744396,"33015628.
502"
REFERENCES,0.49506726457399103,"[36] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea
503"
REFERENCES,0.4959641255605381,"Finn. Combo: Conservative ofﬂine model-based policy optimization. Advances in neural
504"
REFERENCES,0.4968609865470852,"information processing systems, 34:28954–28967, 2021.
505"
REFERENCES,0.4977578475336323,"[37] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable beneﬁts of actor-critic
506"
REFERENCES,0.49865470852017935,"methods for ofﬂine reinforcement learning. Advances in neural information processing systems,
507"
REFERENCES,0.49955156950672647,"34:13626–13640, 2021.
508"
REFERENCES,0.5004484304932736,"[38] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-
509"
REFERENCES,0.5013452914798207,"level direct preference optimization. 2024.
510"
REFERENCES,0.5022421524663677,"[39] Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu, Dawei Feng, Bo Ding, and Huaimin
511"
REFERENCES,0.5031390134529148,"Wang. Uncertainty-penalized reinforcement learning from human feedback with diverse reward
512"
REFERENCES,0.5040358744394619,"lora ensembles. 2023.
513"
REFERENCES,0.5049327354260089,"[40] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu.
514"
REFERENCES,0.5058295964125561,"Slic-hf: Sequence likelihood calibration with human feedback. 2023.
515"
REFERENCES,0.5067264573991032,"[41] Banghua Zhu, Michael I. Jordan, and Jiantao Jiao. Iterative data smoothing: Mitigating reward
516"
REFERENCES,0.5076233183856502,"overﬁtting and overoptimization in rlhf. 2024.
517"
REFERENCES,0.5085201793721973,"A
Proofs
518"
REFERENCES,0.5094170403587444,"A.1
Proof of Proposition 1
519"
REFERENCES,0.5103139013452915,"Proof. Since all the preference pairs (y, y′) are mutually disjoint, and θy is speciﬁc to each y, the
520"
REFERENCES,0.5112107623318386,"DPO objective over Dpref is convex in ∆= {∆1, . . . , ∆n}, where
521"
REFERENCES,0.5121076233183857,"∆i = β log πθ(yw
i )πref(yℓ
i)
πθ(yℓ
i)πref(yw
i ).
(15)"
REFERENCES,0.5130044843049327,"Furthermore, the different ∆i are completely independent from each other due to the preference pairs
522"
REFERENCES,0.5139013452914798,"being disjoint, so they can be optimized over separately.
523"
REFERENCES,0.5147982062780269,"In particular, for every i we have that
524"
REFERENCES,0.515695067264574,"lim
∆i→∞−log (σ (∆i)) = 0,
(16)"
REFERENCES,0.5165919282511211,"which implies that ∆∗= {∞}n is the unique global minimizer of the DPO loss over Dpref in the
525"
REFERENCES,0.5174887892376682,"space of ∆’s, and any θ∗that is a global minimizer must therefore satisfy
526"
REFERENCES,0.5183856502242152,"log πθ(yw
i )πref(yℓ
i)
πθ(yℓ
i)πref(yw
i ) = ∞.
(17) 527"
REFERENCES,0.5192825112107623,"A.2
Proof of Corollary 1
528"
REFERENCES,0.5201793721973094,"Proof. Following the same argument of the proof of Proposition 1, we have that all global minimizers
529"
REFERENCES,0.5210762331838565,"θ∗of the DPO satisfy ∆∗
i = ∞, which in turn implies that
530"
REFERENCES,0.5219730941704036,"πθ∗(yw
i )πref(yℓ
i)
πθ∗(yℓ
i)πref(yw
i ) = ∞.
(18)"
REFERENCES,0.5228699551569507,"Since πref(y) is assumed to satisfy 0 < πref(y) < 1 for all y, this implies that all θ∗satisfy
531"
REFERENCES,0.5237668161434977,"πθ∗(yw
i )
πθ∗(yℓ
i) = ∞,
(19)"
REFERENCES,0.5246636771300448,"which further implies that πθ∗(yℓ
i) = 0 and πθ∗(yw
i ) > 0 for all i ∈[n], as πθ∗(yw
i ) ≤1 for any yw
i .
532"
REFERENCES,0.525560538116592,"Aggregating
533"
REFERENCES,0.526457399103139,"C(yℓ) = {y: ∃i ∈[n] s.t yℓ
i = y}
(20)
then gives that
534"
REFERENCES,0.5273542600896861,"πθ∗(C(yℓ)) =
X"
REFERENCES,0.5282511210762332,"y∈C(yℓ)
πθ∗(y) = 0 =⇒πθ∗(C(yℓ)c) = 1.
(21) 535"
REFERENCES,0.5291479820627802,"To prove the converse, let πθ′ be a policy that satisﬁes πθ′(C(yℓ)c) = 1, with πθ′(yw
i ) > 0, ∀i ∈[n],.
536"
REFERENCES,0.5300448430493273,"As πθ′(y) ≥0 for all y, this implies that πθ′(yℓ
i ) = 0 ∀i ∈[n]. Then, we have
537"
REFERENCES,0.5309417040358745,"πθ′(yw
i )
πθ′(yℓ
i) = ∞,
(22)"
REFERENCES,0.5318385650224215,"which by Proposition 1 implies that πθ′ is a global optimum.
538"
REFERENCES,0.5327354260089686,"A.3
Proof of Theorem 1
539"
REFERENCES,0.5336322869955157,"Proof. We know that the optimal policy for the RLHF objective (1) is given by πθ∗(y|x) ∝
540"
REFERENCES,0.5345291479820627,"πref(y|x) exp(r∗(x, y)/β). Plugging this policy into the distillation objective (7), we see that
541"
REFERENCES,0.5354260089686098,"Ldistill(r∗, πθ∗, ρ) = 0 for all ρ. In fact, the loss is equal to 0 pointwise, meaning that πθ∗is
542"
REFERENCES,0.536322869955157,"a global minimizer of the distillation objective (7). Further, let π be some other minimizer of
543"
REFERENCES,0.537219730941704,"Ldistill(r∗, ·, ρ). Then π also has to attain a loss of 0 at all (x, y, y′) in the support of ρ, meaning
544"
REFERENCES,0.5381165919282511,"that log π(y|x) −log π(y′|x) = log πθ∗(y|x) −log πθ∗(y|x) for all (x, y, y′) in the support of ρ.
545"
REFERENCES,0.5390134529147982,"Consequently, the two policies coincide in the support of ρ (due to the normalization constraint, there
546"
REFERENCES,0.5399103139013453,"is no additional offset term allowed as the support of ρ covers all of Y). Finally, noting that the
547"
REFERENCES,0.5408071748878924,"support of the chosen ρ is such that πθ∗puts no mass outside its support due to the KL constraint
548"
REFERENCES,0.5417040358744395,"in (1), we complete the proof.
549"
REFERENCES,0.5426008968609866,"A.4
Proof of Theorem 2
550"
REFERENCES,0.5434977578475336,"Proof. Consider the pessimistic objective:
551"
REFERENCES,0.5443946188340807,"max
πθ
min
rtgt∈S Eµ(x)
h
Eπθ(y|x)[rtgt(x, y)] −Eπref(y|x)[rtgt(x, y)]
i
−βDKL(πθ∥πref).
(23)"
REFERENCES,0.5452914798206278,"As it is linear in rtgt and convex in π, we can switch the order of min and max:
552"
REFERENCES,0.5461883408071749,"min
rtgt∈S"
REFERENCES,0.547085201793722,"
max
π∈Π Eµ(x)
h
Eπ(y|x)[rtgt(x, y)] −Eπref(y|x)[rtgt(x, y)]
i
−βDKL(π∥πref)

.
(24)"
REFERENCES,0.5479820627802691,"Note that every rtgt ∈S can be written in terms of the KL-constrained policy π∗
rtgt it induces, i.e.,
553"
REFERENCES,0.5488789237668161,"rtgt(x, y) = β log
π∗
rtgt(y | x)"
REFERENCES,0.5497757847533632,"πref(y | x) + β log Z(x, rtgt),
(25)"
REFERENCES,0.5506726457399103,"where
554"
REFERENCES,0.5515695067264574,"π∗
rtgt = argmax
πθ Eµ(x)Eπθ(y|x)[rtgt(x, y)] −βDKL(πθ∥πref)
(26)"
REFERENCES,0.5524663677130045,"which has the form
555"
REFERENCES,0.5533632286995516,"π∗
rtgt(y | x) =
1
Z(x, rtgt)πref(y | x) exp
 1"
REFERENCES,0.5542600896860986,"β rtgt(x, y)

(27)"
REFERENCES,0.5551569506726457,"where Z(x, rtgt) is the partition function:
556"
REFERENCES,0.5560538116591929,"Z(x, rtgt) =
X"
REFERENCES,0.5569506726457399,"y∈Y
πref(y | x) exp
 1"
REFERENCES,0.557847533632287,"β rtgt(x, y)

.
(28)"
REFERENCES,0.5587443946188341,"Substituting π∗
rtgt in for maxπ and writing rtgt in terms of π∗
rtgt, we get the simpliﬁed objective
557"
REFERENCES,0.5596412556053811,"min
rtgt∈S"
REFERENCES,0.5605381165919282,"
max
π∈Π Eµ(x)
h
Eπ(y|x)[rtgt(x, y)] −Eπref(y|x)[rtgt(x, y)]
i
−βDKL(π∥πref)
"
REFERENCES,0.5614349775784754,"= min
rtgt∈S"
REFERENCES,0.5623318385650224,"
Eµ(x)"
REFERENCES,0.5632286995515695,"
Eπ∗rtgt(y|x)"
REFERENCES,0.5641255605381166,"
β log
π∗
rtgt(y | x)"
REFERENCES,0.5650224215246636,"πref(y | x) + β log Z(x, rtgt)
"
REFERENCES,0.5659192825112107,−Eπref(y|x)
REFERENCES,0.5668161434977579,"
β log
π∗
rtgt(y | x)"
REFERENCES,0.567713004484305,"πref(y | x) + β log Z(x, rtgt)

(29)"
REFERENCES,0.568609865470852,"−βDKL(π∗
rtgt∥πref | x)
"
REFERENCES,0.5695067264573991,"= min
rtgt∈S β

Eµ(x)"
REFERENCES,0.5704035874439461,"
DKL(π∗
rtgt∥πref | x) + DKL(πref∥π∗
rtgt | x) −DKL(π∗
rtgt∥πref | x)
"
REFERENCES,0.5713004484304933,"= min
rtgt∈S βEµ(x)
h
DKL(πref∥π∗
rtgt | x)
i
. 558"
REFERENCES,0.5721973094170404,"A.5
Proof of Proposition 2
559"
REFERENCES,0.5730941704035875,"Proof. The proof is a standard Lagrangian duality argument, which we reproduce here for complete-
560"
REFERENCES,0.5739910313901345,"ness. For two functions f(z) and g(z), let us deﬁne
561"
REFERENCES,0.5748878923766816,"z∗= argmin
z
f(z) + αg(z).
(30)"
REFERENCES,0.5757847533632287,"Let us also consider the constrained problem
562"
REFERENCES,0.5766816143497758,"z′ = argmin
z
f(z)
s.t. g(z) ≤g(z∗).
(31)"
REFERENCES,0.5775784753363229,"Suppose by contradiction that z∗is not a minimizer of (31). Since z∗is feasible for the constraint by
563"
REFERENCES,0.57847533632287,"construction, we get that f(z′) < f(z∗). Consequently, we further have
564"
REFERENCES,0.579372197309417,"f(z′) + αg(z′) < f(z∗) + αg(z∗),"
REFERENCES,0.5802690582959641,"where the inequality follows from the feasibility of z′ in (31). This contradicts the optimality
565"
REFERENCES,0.5811659192825112,"of z∗in (30), meaning that z∗must be a minimizer of (31). Applying this general result with
566"
REFERENCES,0.5820627802690583,"f = βEµ(x)DKL(πref(y | x)∥πθ(y | x)), g = minri
tgt∈S Ldistill(ri
tgt, πθ; ρ), and z = πθ completes
567"
REFERENCES,0.5829596412556054,"the proof, since we recognize the set Sγ in (12) to be equivalent to S"
REFERENCES,0.5838565022421525,"ri
tgt∈S Ldistill(ri
tgt, πθ; ρ) ≤λ.
568 569"
REFERENCES,0.5847533632286995,"A.6
Proof of Proposition 3
570"
REFERENCES,0.5856502242152466,"Proof. We differentiate Lpdpo with respect to ψℓ= πθ(yℓ)/πref(yℓ) with i implicit, obtaining,
571"
REFERENCES,0.5865470852017938,∂Lpdpo
REFERENCES,0.5874439461883408,"∂ψℓ
=β
ψβ
ℓ
ψβ
w + ψβ
ℓ
ψ−1
ℓ
−β"
REFERENCES,0.5883408071748879,"αψ−1
ℓ
= βψ−1
ℓ"
REFERENCES,0.589237668161435,"ψβ
ℓ
ψβ
w + ψβ
ℓ
−α−1
! (32)"
REFERENCES,0.590134529147982,"which is zero when,
572"
REFERENCES,0.5910313901345291,"αψβ
ℓ=ψβ
w + ψβ
ℓ
(33)"
REFERENCES,0.5919282511210763,"ψℓ=

1
α −1"
REFERENCES,0.5928251121076233,"1/β
ψw
(34)"
REFERENCES,0.5937219730941704,log ψℓ= −1
REFERENCES,0.5946188340807175,"β log(α −1) + log ψw
(35)"
REFERENCES,0.5955156950672645,log πˆθ(yℓ) = log πref(yℓ) −1
REFERENCES,0.5964125560538116,"β log (α −1) + log πθ(yw) −log πref(yw).
(36)"
REFERENCES,0.5973094170403588,"By the second-order condition, the critical point is a minimum. The objective Lpdpo is the sum of two
573"
REFERENCES,0.5982062780269058,"components: the negative log sigmoid term for Li and the negative log probability for ˆΩ. Because
574"
REFERENCES,0.5991031390134529,"each component is a convex function of ψi, so is Lpdpo. As a result, the local minimum log ˆπθ(yℓ) is
575"
REFERENCES,0.6,"also a global minimum.
576"
REFERENCES,0.600896860986547,"A.7
Proof of Proposition 4
577"
REFERENCES,0.6017937219730942,"Proof. This follows directly from differentiating eq. (7) with respect to πθ(y2).
578"
REFERENCES,0.6026905829596413,"A.8
Proof of Proposition 5
579"
REFERENCES,0.6035874439461884,"Proof. Let ∆= [c(yw) −c(yℓ)] and ρ = πref(yw)/πref(yℓ). The theorem assumes |yw| = |yℓ|.
580"
REFERENCES,0.6044843049327354,"Then Ldpo = −log σ (β(∆· θ) + β log ρ) . The derivative with respect to θ is,
581"
REFERENCES,0.6053811659192825,∂Lβ(θ)
REFERENCES,0.6062780269058295,"∂θ
= −(1 −σ(β(∆· θ) + β log ρ))β∆= −Pr(yℓ≻yw; θ)β∆≺0.
(37)"
REFERENCES,0.6071748878923767,"Let δt = β Pr(yℓ≻yw; θ(t)). Then,
582"
REFERENCES,0.6080717488789238,"˜πθ(t) =θ(t) · c(yw) −n max
j
θ(t)
j
(38)"
REFERENCES,0.6089686098654709,"=(θ(t−1) + δt∆) · c(yw) −n max
j (θ(t−1)
j
+ δt∆j)
(39)"
REFERENCES,0.6098654708520179,"=θ(t−1) · c(yw) −n max
j
θ(t−1)
j
+ δt∆· c(yw) −nδt max
j
∆j
(40)"
REFERENCES,0.610762331838565,=˜πθ(t−1) + δt
REFERENCES,0.611659192825112,"
∆· c(yw) −n max
j
∆j"
REFERENCES,0.6125560538116592,"
(41)"
REFERENCES,0.6134529147982063,"=˜πθ(t−1) + δt V
X"
REFERENCES,0.6143497757847534,"j
cj(yw)(∆j −max
j′
∆j′) ≤˜πθ(t−1).
(42)"
REFERENCES,0.6152466367713004,"We obtain maxj

θ(t−1)
j
+ δt∆j

= maxj θ(t−1)
j
+ maxj δt∆j from the fact that θ(0) = 0 and
583"
REFERENCES,0.6161434977578475,"therefore j ∈arg max ∆implies j ∈arg max θ(t′) for all t′ > 0. The second-to-last step uses
584"
REFERENCES,0.6170403587443947,"n = PV
j cj(yw) and the ﬁnal step uses ∆j ≤max′
j ∆j′. Finally, we have πθ(t)(y) ≤˜πθ(t)(yw)
585"
REFERENCES,0.6179372197309417,because Z(θ) = log P
REFERENCES,0.6188340807174888,"j exp θj ≥log maxj exp θj = maxj θj.
586"
REFERENCES,0.6197309417040359,"A.9
Proof of Proposition 6
587"
REFERENCES,0.6206278026905829,"Proof. Applying gradient descent with learning rate η to the gradient from Equation (37), at each
588"
REFERENCES,0.62152466367713,"step t the parameters are,
589"
REFERENCES,0.6224215246636772,"θ(t) =θ(t−1) + ηβ Pr(yℓ≻yw; θ(t−1))∆= t
X"
REFERENCES,0.6233183856502242,"t′=1
ηβ Pr(yℓ≻yw; θ(t′)) !"
REFERENCES,0.6242152466367713,"∆= τ(t)∆.
(43)"
REFERENCES,0.6251121076233184,"Plugging these parameters into the likelihoods,
590"
REFERENCES,0.6260089686098654,"ℓθ(t)(c(yw)) −ℓθ(t)(ˆy) = c(yw) · θ(t) −nZ(θ(t)) −c(ˆy) · θ(t) + nZ(θ(t))
(44)"
REFERENCES,0.6269058295964126,"= (c(yw) −c(ˆy)) · θ(t) = (c(yw) −c(ˆy)) · (τ(t)∆)
(45)
= τ(t)(c(yw) · ∆−n max ∆) = τ(t)k,
(46)"
REFERENCES,0.6278026905829597,"with k ≤0 by c(yw) · ∆≤||c(yw)||1 × ||∆||∞= n max ∆.
591"
REFERENCES,0.6286995515695067,"B
Transitive closure
592"
REFERENCES,0.6295964125560538,"Both p-DPO and IPO target a constant ratio for log ψw/ψl. However, the loss surfaces are different.
593"
REFERENCES,0.6304932735426009,"To see this, we consider a simpliﬁed setting with three possible outputs, y1, y2, y3. We observe either
594"
REFERENCES,0.6313901345291479,"D = {(y1 ≺y2), (y2 ≺y3)} or D = D ∪{(y1 ≺y3)}. If we treat this problem as a multi-arm
595"
REFERENCES,0.6322869955156951,"bandit, the goal is to assign a weight to each arm, which we denote ψi = log πθ(yi|x) + Zx, with Zx
596"
REFERENCES,0.6331838565022422,"an underdetermined log-partition function.
597"
REFERENCES,0.6340807174887892,"Proposition 7. Let D = {(i, i+1) : i ∈1, 2, . . . , n} for n > 2. Let D be the dataset arising from the
598"
REFERENCES,0.6349775784753363,"transitive closure of D. Assume πref is indifferent to all (yi, yj). Let ψ(D)
∞
= maxi ψ(D)
i
−mini ψ(D)
i
.
599"
REFERENCES,0.6358744394618834,"Then ψ(D)
∞
= (n −1)τ −1 > ψ(D)
∞
= 2 n−1"
REFERENCES,0.6367713004484304,"n τ −1.
600"
REFERENCES,0.6376681614349776,"Proof. For D, the IPO objective can be minimized at zero, so that ψ(D)
∞
= (n −1)τ −1. For D,
601"
REFERENCES,0.6385650224215247,"each adjacent pair of completions is separated by γ, and the objective is Pn−1
i=1 (n −i)(iγ −τ −1)2.
602"
REFERENCES,0.6394618834080718,"The minimum is γ =
n(n+1)(n−1)/6
n2(n+1)(n−1)/12τ −1 =
2
nτ −1, so that ψ(D)
∞
= (n −1)γ = 2 n−1"
REFERENCES,0.6403587443946188,"n τ −1 <
603"
REFERENCES,0.6412556053811659,"(n −1)τ −1 = ψ(D)
∞
for n > 2.
604"
REFERENCES,0.6421524663677131,"Intuitively, the observation of (y1 ≺y3) should increase conﬁdence that y3 is superior to y1, but
605"
REFERENCES,0.6430493273542601,"in IPO it has the opposite effect, drawing their scores closer together. While pessimistic DPO also
606"
REFERENCES,0.6439461883408072,"has a target ratio between each preference pair, its loss surface is different: in particular, it does not
607 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6448430493273543,"P-DPO: (y1)
P-DPO: (y2)
P-DPO: (y3)"
REFERENCES,0.6457399103139013,"beta
1.0
3.0
10.0
30.0
transitively_closed
False
True"
REFERENCES,0.6466367713004484,"10
1
100 1 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6475336322869956,IPO: (y1)
REFERENCES,0.6484304932735426,"10
1
100 1"
REFERENCES,0.6493273542600897,IPO: (y2)
REFERENCES,0.6502242152466368,"10
1
100 1"
REFERENCES,0.6511210762331838,IPO: (y3)
REFERENCES,0.6520179372197309,transitively_closed
REFERENCES,0.6529147982062781,"False
True"
REFERENCES,0.6538116591928251,"Figure B.1: Effect of transitive closure on p-DPO and IPO solutions to preference learning in a
multi-arm bandit. Each column shows the learned policy probability for a given arm, based on the
preferences y1 ≺y2 ≺y3. The top row shows that in p-DPO, the probabilities are not materially
affected by the transitive closure y1 ≺y3. The bottom row shows that in IPO, transitive closure
causes the probabilities to be compressed. In each subﬁgure, we sweep a range of effective values of
τ −1, shown on the x-axis."
REFERENCES,0.6547085201793722,"increase quadratically as we move away from the target. We ﬁnd empirically that pessimistic DPO is
608"
REFERENCES,0.6556053811659193,"robust to the transitive closure of preference annotations in the multi-arm bandit setting, as shown in
609"
REFERENCES,0.6565022421524663,"Figure B.1. As discussed above, DPO will set ψ1 →−∞because y1 is never preferred.
610"
REFERENCES,0.6573991031390135,"In our empirical experiments we solve the p-DPO and IPO objectives for both D
=
611"
REFERENCES,0.6582959641255606,"{(y1, y2), (y2, y3)} and D = D ∪{(y1, y3)}, solving with respect to {πθ(yi)}. IPO is solved analyti-
612"
REFERENCES,0.6591928251121076,"cally as a quadratic program; for pessimistic DPO we used projected gradient descent. We consider
613"
REFERENCES,0.6600896860986547,"β ∈(1, 3, 10, 30) and α ∈(5, 10, 20, 50, 100, 1000). As shown in Figure B.1, there are signiﬁcant
614"
REFERENCES,0.6609865470852018,"differences in the IPO solutions with and without transitive closure, while for p-DPO these differences
615"
REFERENCES,0.6618834080717488,"are imperceptible.
616"
REFERENCES,0.662780269058296,"C
Distribution over reward models for e-DPO
617"
REFERENCES,0.6636771300448431,"Figure C.1 investigates the reason for the success of e-DPO, especially when ρ < .5. For every length
618"
REFERENCES,0.6645739910313901,"bias, we show across all training examples the fraction of cases where a certain reward model, rρ,b,
619"
REFERENCES,0.6654708520179372,"best matched the implicit reward of the ﬁnal e-DPO policy. The policy matches different reward
620"
REFERENCES,0.6663677130044843,"models in different examples. Moreover, there is inverse correlation between the data bias for policy
621"
REFERENCES,0.6672645739910313,"training (ρ) and the data bias for training the reward models (b). This suggests that the ensemble
622"
REFERENCES,0.6681614349775785,"in e-DPO helps as the policy is distilling from reward models that do not share the data bias of the
623"
REFERENCES,0.6690582959641256,"policy training set.
624"
REFERENCES,0.6699551569506726,"Figure C.1: We show for every length bias, ρ, the distribution over reward models that best match
the ﬁnal policy trained by e-DPO across all training examples. We observe that the e-DPO policy
matches different reward models across examples. Moreover, when the policy is trained with data
biased towards preferring short responses, the reward model that was trained on longer responses is
often preferred and vice versa."
REFERENCES,0.6708520179372197,"D
Hyperparameters
625"
REFERENCES,0.6717488789237668,"Validation set performance across the range of hyperparameter settings is shown in Figure D.1. In
626"
REFERENCES,0.672645739910314,"pilot studies we found that these results were relatively robust to variation in the random seed, but did
627"
REFERENCES,0.673542600896861,"not conduct extensive investigation of this effect across all methods and hyperparameters due to cost.
628"
REFERENCES,0.6744394618834081,"E
Compute resources
629"
REFERENCES,0.6753363228699552,"We train policies on 32 TPU v3 chips and reward models on 16 TPU v3 chips. We obtain roughly 0.1
630"
REFERENCES,0.6762331838565022,"steps per second when training, for both the policy and reward models.
631 1.0 0.5 0.0 0.5 1.0"
REFERENCES,0.6771300448430493,oracle reward DPO
REFERENCES,0.6780269058295965,dpo_beta
REFERENCES,0.6789237668161435,"0.01
0.1
1.0
3.0
10.0
30.0
100.0 IPO"
REFERENCES,0.6798206278026906,ipo_tau_start
REFERENCES,0.6807174887892377,"0.1
0.3
1.0
3.0
5.0
10.0
25.0 1.0 0.5 0.0 0.5 1.0"
REFERENCES,0.6816143497757847,oracle reward d-DPO
REFERENCES,0.6825112107623318,dpo_beta
REFERENCES,0.683408071748879,"0.01
0.1
1.0
3.0
10.0
30.0
100.0"
REFERENCES,0.684304932735426,dp-DPO
REFERENCES,0.6852017937219731,dpo_beta
REFERENCES,0.6860986547085202,"0.01
0.1
1.0
3.0
10.0
30.0
100.0"
REFERENCES,0.6869955156950672,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
bias ( ) 1.0 0.5 0.0 0.5 1.0"
REFERENCES,0.6878923766816144,oracle reward e-DPO
REFERENCES,0.6887892376681615,dpo_beta
REFERENCES,0.6896860986547085,"0.01
0.1
1.0
3.0
10.0
30.0
100.0"
REFERENCES,0.6905829596412556,"0.2
0.3
0.4
0.5
0.6
0.7
0.8
bias ( ) p-DPO"
REFERENCES,0.6914798206278027,dpo_beta
REFERENCES,0.6923766816143497,"0.01
0.1
1.0
3.0
10.0
30.0
100.0"
REFERENCES,0.6932735426008969,"Figure D.1: Validation set results across hyperparameters for each method. For all methods, different
values of ρ induce different optimal hyperparameters β and τ −1."
REFERENCES,0.694170403587444,"NeurIPS Paper Checklist
632"
REFERENCES,0.695067264573991,"The checklist is designed to encourage best practices for responsible machine learning research,
633"
REFERENCES,0.6959641255605381,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
634"
REFERENCES,0.6968609865470852,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
635"
REFERENCES,0.6977578475336322,"follow the references and precede the (optional) supplemental material. The checklist does NOT
636"
REFERENCES,0.6986547085201794,"count towards the page limit.
637"
REFERENCES,0.6995515695067265,"Please read the checklist guidelines carefully for information on how to answer these questions. For
638"
REFERENCES,0.7004484304932735,"each question in the checklist:
639"
REFERENCES,0.7013452914798206,"• You should answer [Yes] , [No] , or [NA] .
640"
REFERENCES,0.7022421524663677,"• [NA] means either that the question is Not Applicable for that particular paper or the
641"
REFERENCES,0.7031390134529149,"relevant information is Not Available.
642"
REFERENCES,0.7040358744394619,"• Please provide a short (1–2 sentence) justiﬁcation right after your answer (even for NA).
643"
REFERENCES,0.704932735426009,"The checklist answers are an integral part of your paper submission. They are visible to the
644"
REFERENCES,0.705829596412556,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
645"
REFERENCES,0.7067264573991031,"(after eventual revisions) with the ﬁnal version of your paper, and its ﬁnal version will be published
646"
REFERENCES,0.7076233183856502,"with the paper.
647"
REFERENCES,0.7085201793721974,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
648"
REFERENCES,0.7094170403587444,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
649"
REFERENCES,0.7103139013452915,"proper justiﬁcation is given (e.g., ""error bars are not reported because it would be too computationally
650"
REFERENCES,0.7112107623318386,"expensive"" or ""we were unable to ﬁnd the license for the dataset we used""). In general, answering
651"
REFERENCES,0.7121076233183856,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
652"
REFERENCES,0.7130044843049327,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
653"
REFERENCES,0.7139013452914799,"write a justiﬁcation to elaborate. All supporting evidence can appear either in the main paper or the
654"
REFERENCES,0.7147982062780269,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justiﬁcation
655"
REFERENCES,0.715695067264574,"please point to the section(s) where related material for the question can be found.
656"
REFERENCES,0.7165919282511211,"IMPORTANT, please:
657"
REFERENCES,0.7174887892376681,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
658"
REFERENCES,0.7183856502242153,"• Keep the checklist subsection headings, questions/answers and guidelines below.
659"
REFERENCES,0.7192825112107624,"• Do not modify the questions and only use the provided macros for your answers.
660"
CLAIMS,0.7201793721973094,"1. Claims
661"
CLAIMS,0.7210762331838565,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
662"
CLAIMS,0.7219730941704036,"paper’s contributions and scope?
663"
CLAIMS,0.7228699551569506,"Answer: [Yes]
664"
CLAIMS,0.7237668161434978,"Justiﬁcation: In our view, the abstract and introduction accurately summarize the contribu-
665"
CLAIMS,0.7246636771300449,"tions of the paper.
666"
CLAIMS,0.7255605381165919,"Guidelines:
667"
CLAIMS,0.726457399103139,"• The answer NA means that the abstract and introduction do not include the claims
668"
CLAIMS,0.7273542600896861,"made in the paper.
669"
CLAIMS,0.7282511210762331,"• The abstract and/or introduction should clearly state the claims made, including the
670"
CLAIMS,0.7291479820627803,"contributions made in the paper and important assumptions and limitations. A No or
671"
CLAIMS,0.7300448430493274,"NA answer to this question will not be perceived well by the reviewers.
672"
CLAIMS,0.7309417040358744,"• The claims made should match theoretical and experimental results, and reﬂect how
673"
CLAIMS,0.7318385650224215,"much the results can be expected to generalize to other settings.
674"
CLAIMS,0.7327354260089686,"• It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
675"
CLAIMS,0.7336322869955157,"are not attained by the paper.
676"
LIMITATIONS,0.7345291479820628,"2. Limitations
677"
LIMITATIONS,0.7354260089686099,"Question: Does the paper discuss the limitations of the work performed by the authors?
678"
LIMITATIONS,0.736322869955157,"Answer: [Yes]
679"
LIMITATIONS,0.737219730941704,"Justiﬁcation: See Section 7
680"
LIMITATIONS,0.7381165919282511,"Guidelines:
681"
LIMITATIONS,0.7390134529147983,"• The answer NA means that the paper has no limitation while the answer No means that
682"
LIMITATIONS,0.7399103139013453,"the paper has limitations, but those are not discussed in the paper.
683"
LIMITATIONS,0.7408071748878924,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
684"
LIMITATIONS,0.7417040358744394,"• The paper should point out any strong assumptions and how robust the results are to
685"
LIMITATIONS,0.7426008968609865,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
686"
LIMITATIONS,0.7434977578475336,"model well-speciﬁcation, asymptotic approximations only holding locally). The authors
687"
LIMITATIONS,0.7443946188340808,"should reﬂect on how these assumptions might be violated in practice and what the
688"
LIMITATIONS,0.7452914798206278,"implications would be.
689"
LIMITATIONS,0.7461883408071749,"• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
690"
LIMITATIONS,0.747085201793722,"only tested on a few datasets or with a few runs. In general, empirical results often
691"
LIMITATIONS,0.747982062780269,"depend on implicit assumptions, which should be articulated.
692"
LIMITATIONS,0.7488789237668162,"• The authors should reﬂect on the factors that inﬂuence the performance of the approach.
693"
LIMITATIONS,0.7497757847533633,"For example, a facial recognition algorithm may perform poorly when image resolution
694"
LIMITATIONS,0.7506726457399103,"is low or images are taken in low lighting. Or a speech-to-text system might not be
695"
LIMITATIONS,0.7515695067264574,"used reliably to provide closed captions for online lectures because it fails to handle
696"
LIMITATIONS,0.7524663677130045,"technical jargon.
697"
LIMITATIONS,0.7533632286995515,"• The authors should discuss the computational efﬁciency of the proposed algorithms
698"
LIMITATIONS,0.7542600896860987,"and how they scale with dataset size.
699"
LIMITATIONS,0.7551569506726458,"• If applicable, the authors should discuss possible limitations of their approach to
700"
LIMITATIONS,0.7560538116591928,"address problems of privacy and fairness.
701"
LIMITATIONS,0.7569506726457399,"• While the authors might fear that complete honesty about limitations might be used by
702"
LIMITATIONS,0.757847533632287,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
703"
LIMITATIONS,0.758744394618834,"limitations that aren’t acknowledged in the paper. The authors should use their best
704"
LIMITATIONS,0.7596412556053812,"judgment and recognize that individual actions in favor of transparency play an impor-
705"
LIMITATIONS,0.7605381165919283,"tant role in developing norms that preserve the integrity of the community. Reviewers
706"
LIMITATIONS,0.7614349775784753,"will be speciﬁcally instructed to not penalize honesty concerning limitations.
707"
THEORY ASSUMPTIONS AND PROOFS,0.7623318385650224,"3. Theory Assumptions and Proofs
708"
THEORY ASSUMPTIONS AND PROOFS,0.7632286995515695,"Question: For each theoretical result, does the paper provide the full set of assumptions and
709"
THEORY ASSUMPTIONS AND PROOFS,0.7641255605381166,"a complete (and correct) proof?
710"
THEORY ASSUMPTIONS AND PROOFS,0.7650224215246637,"Answer: [Yes]
711"
THEORY ASSUMPTIONS AND PROOFS,0.7659192825112108,"Justiﬁcation: See Appendix A
712"
THEORY ASSUMPTIONS AND PROOFS,0.7668161434977578,"Guidelines:
713"
THEORY ASSUMPTIONS AND PROOFS,0.7677130044843049,"• The answer NA means that the paper does not include theoretical results.
714"
THEORY ASSUMPTIONS AND PROOFS,0.768609865470852,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
715"
THEORY ASSUMPTIONS AND PROOFS,0.7695067264573991,"referenced.
716"
THEORY ASSUMPTIONS AND PROOFS,0.7704035874439462,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
717"
THEORY ASSUMPTIONS AND PROOFS,0.7713004484304933,"• The proofs can either appear in the main paper or the supplemental material, but if
718"
THEORY ASSUMPTIONS AND PROOFS,0.7721973094170403,"they appear in the supplemental material, the authors are encouraged to provide a short
719"
THEORY ASSUMPTIONS AND PROOFS,0.7730941704035874,"proof sketch to provide intuition.
720"
THEORY ASSUMPTIONS AND PROOFS,0.7739910313901345,"• Inversely, any informal proof provided in the core of the paper should be complemented
721"
THEORY ASSUMPTIONS AND PROOFS,0.7748878923766817,"by formal proofs provided in appendix or supplemental material.
722"
THEORY ASSUMPTIONS AND PROOFS,0.7757847533632287,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766816143497758,"4. Experimental Result Reproducibility
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775784753363228,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7784753363228699,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7793721973094171,"of the paper (regardless of whether the code and data are provided or not)?
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7802690582959642,"Answer: [Yes]
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811659192825112,"Justiﬁcation: Details are provided in Section 4.1 and Appendix D.
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7820627802690583,"Guidelines:
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829596412556054,"• The answer NA means that the paper does not include experiments.
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7838565022421524,"• If the paper includes experiments, a No answer to this question will not be perceived
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847533632286996,"well by the reviewers: Making the paper reproducible is important, regardless of
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7856502242152467,"whether the code and data are provided or not.
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7865470852017937,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7874439461883408,"to make their results reproducible or veriﬁable.
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7883408071748879,"• Depending on the contribution, reproducibility can be accomplished in various ways.
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7892376681614349,"For example, if the contribution is a novel architecture, describing the architecture fully
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7901345291479821,"might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7910313901345292,"be necessary to either make it possible for others to replicate the model with the same
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7919282511210762,"dataset, or provide access to the model. In general. releasing code and data is often
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7928251121076233,"one good way to accomplish this, but reproducibility can also be provided via detailed
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7937219730941704,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7946188340807175,"of a large language model), releasing of a model checkpoint, or other means that are
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7955156950672646,"appropriate to the research performed.
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7964125560538117,"• While NeurIPS does not require releasing code, the conference does require all submis-
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7973094170403587,"sions to provide some reasonable avenue for reproducibility, which may depend on the
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7982062780269058,"nature of the contribution. For example
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7991031390134529,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"to reproduce that algorithm.
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8008968609865471,"(b) If the contribution is primarily a new model architecture, the paper should describe
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8017937219730942,"the architecture clearly and fully.
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8026905829596412,"(c) If the contribution is a new model (e.g., a large language model), then there should
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8035874439461883,"either be a way to access this model for reproducing the results or a way to reproduce
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8044843049327354,"the model (e.g., with an open-source dataset or instructions for how to construct
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8053811659192825,"the dataset).
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8062780269058296,"(d) We recognize that reproducibility may be tricky in some cases, in which case
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8071748878923767,"authors are welcome to describe the particular way they provide for reproducibility.
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8080717488789237,"In the case of closed-source models, it may be that access to the model is limited in
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089686098654708,"some way (e.g., to registered users), but it should be possible for other researchers
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.809865470852018,"to have some path to reproducing or verifying the results.
761"
OPEN ACCESS TO DATA AND CODE,0.810762331838565,"5. Open access to data and code
762"
OPEN ACCESS TO DATA AND CODE,0.8116591928251121,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
763"
OPEN ACCESS TO DATA AND CODE,0.8125560538116592,"tions to faithfully reproduce the main experimental results, as described in supplemental
764"
OPEN ACCESS TO DATA AND CODE,0.8134529147982063,"material?
765"
OPEN ACCESS TO DATA AND CODE,0.8143497757847533,"Answer: [No]
766"
OPEN ACCESS TO DATA AND CODE,0.8152466367713005,"Justiﬁcation: Experiments are on publicly-available data, but it is not possible for us to share
767"
OPEN ACCESS TO DATA AND CODE,0.8161434977578476,"code. We believe that the implementation should be relatively straightforward, given the
768"
OPEN ACCESS TO DATA AND CODE,0.8170403587443946,"mathematical descriptions presented here.
769"
OPEN ACCESS TO DATA AND CODE,0.8179372197309417,"Guidelines:
770"
OPEN ACCESS TO DATA AND CODE,0.8188340807174888,"• The answer NA means that paper does not include experiments requiring code.
771"
OPEN ACCESS TO DATA AND CODE,0.8197309417040358,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
772"
OPEN ACCESS TO DATA AND CODE,0.820627802690583,"public/guides/CodeSubmissionPolicy) for more details.
773"
OPEN ACCESS TO DATA AND CODE,0.8215246636771301,"• While we encourage the release of code and data, we understand that this might not be
774"
OPEN ACCESS TO DATA AND CODE,0.8224215246636771,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
775"
OPEN ACCESS TO DATA AND CODE,0.8233183856502242,"including code, unless this is central to the contribution (e.g., for a new open-source
776"
OPEN ACCESS TO DATA AND CODE,0.8242152466367713,"benchmark).
777"
OPEN ACCESS TO DATA AND CODE,0.8251121076233184,"• The instructions should contain the exact command and environment needed to run to
778"
OPEN ACCESS TO DATA AND CODE,0.8260089686098655,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
779"
OPEN ACCESS TO DATA AND CODE,0.8269058295964126,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
780"
OPEN ACCESS TO DATA AND CODE,0.8278026905829596,"• The authors should provide instructions on data access and preparation, including how
781"
OPEN ACCESS TO DATA AND CODE,0.8286995515695067,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
782"
OPEN ACCESS TO DATA AND CODE,0.8295964125560538,"• The authors should provide scripts to reproduce all experimental results for the new
783"
OPEN ACCESS TO DATA AND CODE,0.8304932735426009,"proposed method and baselines. If only a subset of experiments are reproducible, they
784"
OPEN ACCESS TO DATA AND CODE,0.831390134529148,"should state which ones are omitted from the script and why.
785"
OPEN ACCESS TO DATA AND CODE,0.8322869955156951,"• At submission time, to preserve anonymity, the authors should release anonymized
786"
OPEN ACCESS TO DATA AND CODE,0.8331838565022421,"versions (if applicable).
787"
OPEN ACCESS TO DATA AND CODE,0.8340807174887892,"• Providing as much information as possible in supplemental material (appended to the
788"
OPEN ACCESS TO DATA AND CODE,0.8349775784753363,"paper) is recommended, but including URLs to data and code is permitted.
789"
OPEN ACCESS TO DATA AND CODE,0.8358744394618834,"6. Experimental Setting/Details
790"
OPEN ACCESS TO DATA AND CODE,0.8367713004484305,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
791"
OPEN ACCESS TO DATA AND CODE,0.8376681614349776,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
792"
OPEN ACCESS TO DATA AND CODE,0.8385650224215246,"results?
793"
OPEN ACCESS TO DATA AND CODE,0.8394618834080717,"Answer: [Yes]
794"
OPEN ACCESS TO DATA AND CODE,0.8403587443946189,"Justiﬁcation: These details are provided in Section 4.1.
795"
OPEN ACCESS TO DATA AND CODE,0.841255605381166,"Guidelines:
796"
OPEN ACCESS TO DATA AND CODE,0.842152466367713,"• The answer NA means that the paper does not include experiments.
797"
OPEN ACCESS TO DATA AND CODE,0.8430493273542601,"• The experimental setting should be presented in the core of the paper to a level of detail
798"
OPEN ACCESS TO DATA AND CODE,0.8439461883408071,"that is necessary to appreciate the results and make sense of them.
799"
OPEN ACCESS TO DATA AND CODE,0.8448430493273542,"• The full details can be provided either with the code, in appendix, or as supplemental
800"
OPEN ACCESS TO DATA AND CODE,0.8457399103139014,"material.
801"
OPEN ACCESS TO DATA AND CODE,0.8466367713004485,"7. Experiment Statistical Signiﬁcance
802"
OPEN ACCESS TO DATA AND CODE,0.8475336322869955,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
803"
OPEN ACCESS TO DATA AND CODE,0.8484304932735426,"information about the statistical signiﬁcance of the experiments?
804"
OPEN ACCESS TO DATA AND CODE,0.8493273542600897,"Answer: [Yes]
805"
OPEN ACCESS TO DATA AND CODE,0.8502242152466367,"Justiﬁcation: Section 4.2 includes bootstrap 95% conﬁdence intervals on the main ﬁgure
806"
OPEN ACCESS TO DATA AND CODE,0.8511210762331839,"and hypothesis tests for speciﬁc comparisons between methods.
807"
OPEN ACCESS TO DATA AND CODE,0.852017937219731,"Guidelines:
808"
OPEN ACCESS TO DATA AND CODE,0.852914798206278,"• The answer NA means that the paper does not include experiments.
809"
OPEN ACCESS TO DATA AND CODE,0.8538116591928251,"• The authors should answer ""Yes"" if the results are accompanied by error bars, conﬁ-
810"
OPEN ACCESS TO DATA AND CODE,0.8547085201793722,"dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
811"
OPEN ACCESS TO DATA AND CODE,0.8556053811659193,"the main claims of the paper.
812"
OPEN ACCESS TO DATA AND CODE,0.8565022421524664,"• The factors of variability that the error bars are capturing should be clearly stated (for
813"
OPEN ACCESS TO DATA AND CODE,0.8573991031390135,"example, train/test split, initialization, random drawing of some parameter, or overall
814"
OPEN ACCESS TO DATA AND CODE,0.8582959641255605,"run with given experimental conditions).
815"
OPEN ACCESS TO DATA AND CODE,0.8591928251121076,"• The method for calculating the error bars should be explained (closed form formula,
816"
OPEN ACCESS TO DATA AND CODE,0.8600896860986547,"call to a library function, bootstrap, etc.)
817"
OPEN ACCESS TO DATA AND CODE,0.8609865470852018,"• The assumptions made should be given (e.g., Normally distributed errors).
818"
OPEN ACCESS TO DATA AND CODE,0.8618834080717489,"• It should be clear whether the error bar is the standard deviation or the standard error
819"
OPEN ACCESS TO DATA AND CODE,0.862780269058296,"of the mean.
820"
OPEN ACCESS TO DATA AND CODE,0.863677130044843,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
821"
OPEN ACCESS TO DATA AND CODE,0.8645739910313901,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
822"
OPEN ACCESS TO DATA AND CODE,0.8654708520179372,"of Normality of errors is not veriﬁed.
823"
OPEN ACCESS TO DATA AND CODE,0.8663677130044843,"• For asymmetric distributions, the authors should be careful not to show in tables or
824"
OPEN ACCESS TO DATA AND CODE,0.8672645739910314,"ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
825"
OPEN ACCESS TO DATA AND CODE,0.8681614349775785,"error rates).
826"
OPEN ACCESS TO DATA AND CODE,0.8690582959641255,"• If error bars are reported in tables or plots, The authors should explain in the text how
827"
OPEN ACCESS TO DATA AND CODE,0.8699551569506726,"they were calculated and reference the corresponding ﬁgures or tables in the text.
828"
EXPERIMENTS COMPUTE RESOURCES,0.8708520179372198,"8. Experiments Compute Resources
829"
EXPERIMENTS COMPUTE RESOURCES,0.8717488789237668,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
830"
EXPERIMENTS COMPUTE RESOURCES,0.8726457399103139,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
831"
EXPERIMENTS COMPUTE RESOURCES,0.873542600896861,"the experiments?
832"
EXPERIMENTS COMPUTE RESOURCES,0.874439461883408,"Answer: [Yes]
833"
EXPERIMENTS COMPUTE RESOURCES,0.8753363228699551,"Justiﬁcation: Please see Section 4.1.
834"
EXPERIMENTS COMPUTE RESOURCES,0.8762331838565023,"Guidelines:
835"
EXPERIMENTS COMPUTE RESOURCES,0.8771300448430494,"• The answer NA means that the paper does not include experiments.
836"
EXPERIMENTS COMPUTE RESOURCES,0.8780269058295964,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
837"
EXPERIMENTS COMPUTE RESOURCES,0.8789237668161435,"or cloud provider, including relevant memory and storage.
838"
EXPERIMENTS COMPUTE RESOURCES,0.8798206278026905,"• The paper should provide the amount of compute required for each of the individual
839"
EXPERIMENTS COMPUTE RESOURCES,0.8807174887892377,"experimental runs as well as estimate the total compute.
840"
EXPERIMENTS COMPUTE RESOURCES,0.8816143497757848,"• The paper should disclose whether the full research project required more compute
841"
EXPERIMENTS COMPUTE RESOURCES,0.8825112107623319,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
842"
EXPERIMENTS COMPUTE RESOURCES,0.8834080717488789,"didn’t make it into the paper).
843"
CODE OF ETHICS,0.884304932735426,"9. Code Of Ethics
844"
CODE OF ETHICS,0.885201793721973,"Question: Does the research conducted in the paper conform, in every respect, with the
845"
CODE OF ETHICS,0.8860986547085202,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
846"
CODE OF ETHICS,0.8869955156950673,"Answer: [Yes]
847"
CODE OF ETHICS,0.8878923766816144,"Justiﬁcation: The research does not involve human subjects and does not introduce new data.
848"
CODE OF ETHICS,0.8887892376681614,"Its main impact should be to improve effectiveness and understanding of preference-based
849"
CODE OF ETHICS,0.8896860986547085,"post-training.
850"
CODE OF ETHICS,0.8905829596412556,"Guidelines:
851"
CODE OF ETHICS,0.8914798206278027,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
852"
CODE OF ETHICS,0.8923766816143498,"• If the authors answer No, they should explain the special circumstances that require a
853"
CODE OF ETHICS,0.8932735426008969,"deviation from the Code of Ethics.
854"
CODE OF ETHICS,0.8941704035874439,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
855"
CODE OF ETHICS,0.895067264573991,"eration due to laws or regulations in their jurisdiction).
856"
BROADER IMPACTS,0.8959641255605382,"10. Broader Impacts
857"
BROADER IMPACTS,0.8968609865470852,"Question: Does the paper discuss both potential positive societal impacts and negative
858"
BROADER IMPACTS,0.8977578475336323,"societal impacts of the work performed?
859"
BROADER IMPACTS,0.8986547085201794,"Answer: [Yes]
860"
BROADER IMPACTS,0.8995515695067264,"Justiﬁcation: See Section 7
861"
BROADER IMPACTS,0.9004484304932735,"Guidelines:
862"
BROADER IMPACTS,0.9013452914798207,"• The answer NA means that there is no societal impact of the work performed.
863"
BROADER IMPACTS,0.9022421524663677,"• If the authors answer NA or No, they should explain why their work has no societal
864"
BROADER IMPACTS,0.9031390134529148,"impact or why the paper does not address societal impact.
865"
BROADER IMPACTS,0.9040358744394619,"• Examples of negative societal impacts include potential malicious or unintended uses
866"
BROADER IMPACTS,0.9049327354260089,"(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
867"
BROADER IMPACTS,0.905829596412556,"(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
868"
BROADER IMPACTS,0.9067264573991032,"groups), privacy considerations, and security considerations.
869"
BROADER IMPACTS,0.9076233183856502,"• The conference expects that many papers will be foundational research and not tied
870"
BROADER IMPACTS,0.9085201793721973,"to particular applications, let alone deployments. However, if there is a direct path to
871"
BROADER IMPACTS,0.9094170403587444,"any negative applications, the authors should point it out. For example, it is legitimate
872"
BROADER IMPACTS,0.9103139013452914,"to point out that an improvement in the quality of generative models could be used to
873"
BROADER IMPACTS,0.9112107623318386,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
874"
BROADER IMPACTS,0.9121076233183857,"that a generic algorithm for optimizing neural networks could enable people to train
875"
BROADER IMPACTS,0.9130044843049328,"models that generate Deepfakes faster.
876"
BROADER IMPACTS,0.9139013452914798,"• The authors should consider possible harms that could arise when the technology is
877"
BROADER IMPACTS,0.9147982062780269,"being used as intended and functioning correctly, harms that could arise when the
878"
BROADER IMPACTS,0.915695067264574,"technology is being used as intended but gives incorrect results, and harms following
879"
BROADER IMPACTS,0.9165919282511211,"from (intentional or unintentional) misuse of the technology.
880"
BROADER IMPACTS,0.9174887892376682,"• If there are negative societal impacts, the authors could also discuss possible mitigation
881"
BROADER IMPACTS,0.9183856502242153,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
882"
BROADER IMPACTS,0.9192825112107623,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
883"
BROADER IMPACTS,0.9201793721973094,"feedback over time, improving the efﬁciency and accessibility of ML).
884"
SAFEGUARDS,0.9210762331838565,"11. Safeguards
885"
SAFEGUARDS,0.9219730941704036,"Question: Does the paper describe safeguards that have been put in place for responsible
886"
SAFEGUARDS,0.9228699551569507,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
887"
SAFEGUARDS,0.9237668161434978,"image generators, or scraped datasets)?
888"
SAFEGUARDS,0.9246636771300448,"Answer: [NA]
889"
SAFEGUARDS,0.9255605381165919,"Justiﬁcation: No data or models are released.
890"
SAFEGUARDS,0.9264573991031391,"Guidelines:
891"
SAFEGUARDS,0.9273542600896861,"• The answer NA means that the paper poses no such risks.
892"
SAFEGUARDS,0.9282511210762332,"• Released models that have a high risk for misuse or dual-use should be released with
893"
SAFEGUARDS,0.9291479820627803,"necessary safeguards to allow for controlled use of the model, for example by requiring
894"
SAFEGUARDS,0.9300448430493273,"that users adhere to usage guidelines or restrictions to access the model or implementing
895"
SAFEGUARDS,0.9309417040358744,"safety ﬁlters.
896"
SAFEGUARDS,0.9318385650224216,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
897"
SAFEGUARDS,0.9327354260089686,"should describe how they avoided releasing unsafe images.
898"
SAFEGUARDS,0.9336322869955157,"• We recognize that providing effective safeguards is challenging, and many papers do
899"
SAFEGUARDS,0.9345291479820628,"not require this, but we encourage authors to take this into account and make a best
900"
SAFEGUARDS,0.9354260089686098,"faith effort.
901"
LICENSES FOR EXISTING ASSETS,0.9363228699551569,"12. Licenses for existing assets
902"
LICENSES FOR EXISTING ASSETS,0.9372197309417041,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
903"
LICENSES FOR EXISTING ASSETS,0.9381165919282511,"the paper, properly credited and are the license and terms of use explicitly mentioned and
904"
LICENSES FOR EXISTING ASSETS,0.9390134529147982,"properly respected?
905"
LICENSES FOR EXISTING ASSETS,0.9399103139013453,"Answer: [Yes]
906"
LICENSES FOR EXISTING ASSETS,0.9408071748878923,"Justiﬁcation: The main external resource is the TLDR dataset, which we cite. Its license is
907"
LICENSES FOR EXISTING ASSETS,0.9417040358744395,"CC BY 4.0.
908"
LICENSES FOR EXISTING ASSETS,0.9426008968609866,"Guidelines:
909"
LICENSES FOR EXISTING ASSETS,0.9434977578475336,"• The answer NA means that the paper does not use existing assets.
910"
LICENSES FOR EXISTING ASSETS,0.9443946188340807,"• The authors should cite the original paper that produced the code package or dataset.
911"
LICENSES FOR EXISTING ASSETS,0.9452914798206278,"• The authors should state which version of the asset is used and, if possible, include a
912"
LICENSES FOR EXISTING ASSETS,0.9461883408071748,"URL.
913"
LICENSES FOR EXISTING ASSETS,0.947085201793722,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
914"
LICENSES FOR EXISTING ASSETS,0.9479820627802691,"• For scraped data from a particular source (e.g., website), the copyright and terms of
915"
LICENSES FOR EXISTING ASSETS,0.9488789237668162,"service of that source should be provided.
916"
LICENSES FOR EXISTING ASSETS,0.9497757847533632,"• If assets are released, the license, copyright information, and terms of use in the
917"
LICENSES FOR EXISTING ASSETS,0.9506726457399103,"package should be provided. For popular datasets, paperswithcode.com/datasets
918"
LICENSES FOR EXISTING ASSETS,0.9515695067264573,"has curated licenses for some datasets. Their licensing guide can help determine the
919"
LICENSES FOR EXISTING ASSETS,0.9524663677130045,"license of a dataset.
920"
LICENSES FOR EXISTING ASSETS,0.9533632286995516,"• For existing datasets that are re-packaged, both the original license and the license of
921"
LICENSES FOR EXISTING ASSETS,0.9542600896860987,"the derived asset (if it has changed) should be provided.
922"
LICENSES FOR EXISTING ASSETS,0.9551569506726457,"• If this information is not available online, the authors are encouraged to reach out to
923"
LICENSES FOR EXISTING ASSETS,0.9560538116591928,"the asset’s creators.
924"
NEW ASSETS,0.95695067264574,"13. New Assets
925"
NEW ASSETS,0.957847533632287,"Question: Are new assets introduced in the paper well documented and is the documentation
926"
NEW ASSETS,0.9587443946188341,"provided alongside the assets?
927"
NEW ASSETS,0.9596412556053812,"Answer: [NA]
928"
NEW ASSETS,0.9605381165919282,"Justiﬁcation: No new assets are introduced.
929"
NEW ASSETS,0.9614349775784753,"Guidelines:
930"
NEW ASSETS,0.9623318385650225,"• The answer NA means that the paper does not release new assets.
931"
NEW ASSETS,0.9632286995515695,"• Researchers should communicate the details of the dataset/code/model as part of their
932"
NEW ASSETS,0.9641255605381166,"submissions via structured templates. This includes details about training, license,
933"
NEW ASSETS,0.9650224215246637,"limitations, etc.
934"
NEW ASSETS,0.9659192825112107,"• The paper should discuss whether and how consent was obtained from people whose
935"
NEW ASSETS,0.9668161434977578,"asset is used.
936"
NEW ASSETS,0.967713004484305,"• At submission time, remember to anonymize your assets (if applicable). You can either
937"
NEW ASSETS,0.968609865470852,"create an anonymized URL or include an anonymized zip ﬁle.
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695067264573991,"14. Crowdsourcing and Research with Human Subjects
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704035874439462,"Question: For crowdsourcing experiments and research with human subjects, does the paper
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9713004484304932,"include the full text of instructions given to participants and screenshots, if applicable, as
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9721973094170404,"well as details about compensation (if any)?
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730941704035875,"Answer: [NA]
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9739910313901345,"Justiﬁcation: The paper does not involve crowdsourcing or research with human subjects.
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748878923766816,"Guidelines:
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757847533632287,"• The answer NA means that the paper does not involve crowdsourcing nor research with
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766816143497757,"human subjects.
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775784753363229,"• Including this information in the supplemental material is ﬁne, but if the main contribu-
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97847533632287,"tion of the paper involves human subjects, then as much detail as possible should be
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979372197309417,"included in the main paper.
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9802690582959641,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811659192825112,"or other labor should be paid at least the minimum wage in the country of the data
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820627802690582,"collector.
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829596412556054,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838565022421525,"Subjects
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847533632286996,"Question: Does the paper describe potential risks incurred by study participants, whether
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856502242152466,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865470852017937,"approvals (or an equivalent approval/review based on the requirements of your country or
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874439461883409,"institution) were obtained?
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883408071748879,"Answer: [NA]
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989237668161435,"Justiﬁcation: The paper does not involve crowdsourcing or research with human subjects.
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9901345291479821,"Guidelines:
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910313901345291,"• The answer NA means that the paper does not involve crowdsourcing nor research with
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919282511210762,"human subjects.
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9928251121076234,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937219730941704,"may be required for any human subjects research. If you obtained IRB approval, you
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9946188340807175,"should clearly state this in the paper.
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955156950672646,"• We recognize that the procedures for this may vary signiﬁcantly between institutions
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964125560538116,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9973094170403587,"guidelines for their institution.
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9982062780269059,"• For initial submissions, do not include any information that would break anonymity (if
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991031390134529,"applicable), such as the institution conducting the review.
972"
