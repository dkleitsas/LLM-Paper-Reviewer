Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001440922190201729,"The gradient descent-ascent (GDA) algorithm has been widely applied to solve
nonconvex minimax optimization problems. However, the existing GDA-type
algorithms can only ﬁnd ﬁrst-order stationary points of the envelope function of
nonconvex minimax optimization problems, which does not rule out the possibility
to get stuck at suboptimal saddle points. In this paper, we develop Cubic-GDA – the
ﬁrst Newton-type GDA algorithm for escaping strict saddle points in nonconvex-
strongly-concave minimax optimization. Speciﬁcally, the algorithm uses gradient
ascent to estimate the second-order information of the minimax objective function,
and it leverages the cubic regularization technique to efﬁciently escape the strict
saddle points. Under standard smoothness assumptions on the objective function,
we show that Cubic-GDA admits an intrinsic potential function whose value mono-
tonically decreases in the minimax optimization process. Such a property leads
to a desired global convergence of Cubic-GDA to a second-order stationary point
at a sublinear rate. Moreover, we analyze the convergence rate of Cubic-GDA in
the full spectrum of a gradient dominant-type nonconvex geometry. Our result
shows that Cubic-GDA achieves an orderwise faster convergence rate than the
standard GDA for a wide spectrum of gradient dominant geometry. Our study
bridges minimax optimization with second-order optimization and may inspire new
developments along this direction."
INTRODUCTION,0.002881844380403458,"1
INTRODUCTION"
INTRODUCTION,0.004322766570605188,"Nonconvex minimax optimization is a popular optimization framework that has broad applications
in modern machine learning, including game theory (Ferreira et al., 2012), generative adversarial
networks (Goodfellow et al., 2014), adversarial training (Sinha et al., 2017), reinforcement learning
(Qiu et al., 2020; Ho and Ermon, 2016; Song et al., 2018), etc. A standard nonconvex minimax
optimization problem is shown below, where f is a smooth nonconvex function in x."
INTRODUCTION,0.005763688760806916,"min
x∈Rm max
y∈Rn f(x, y).
(P)"
INTRODUCTION,0.007204610951008645,"In the existing literature, many optimization algorithms have been developed to solve different types
of minimax problems. Among them, a simple and popular algorithm is the gradient descent-ascent
(GDA), which alternates between a gradient descent update on x and a gradient ascent update on
y in each iteration. Speciﬁcally, the global convergence of GDA has been established for minimax
problems under various types of global geometries, such as convex-concave-type geometry (f is
convex in x and concave in y) (Nedi´c and Ozdaglar, 2009; Du and Hu, 2019; Mokhtari et al.,
2020; Zhang and Wang, 2021), bi-linear geometry (Neumann, 1928; Robinson, 1951) and Polyak-
Łojasiewicz geometry (Nouiehed et al., 2019; Yang et al., 2020), yet these geometries are not satisﬁed
by general nonconvex minimax problems. Recently, many studies proved the convergence of GDA in
nonconvex minimax optimization for both nonconvex-concave problems (Lin et al., 2020; Nouiehed
et al., 2019; Xu et al., 2020d) and nonconvex-strongly-concave problems (Lin et al., 2020; Xu et al.,
2020d; Chen et al., 2021). In these studies, it has been shown that GDA converges sublinearly to a
stationary point where the gradient of a certain envelope function of the minimax problem vanishes."
INTRODUCTION,0.008645533141210375,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.010086455331412104,"Although GDA can ﬁnd ﬁrst-order stationary points of nonconvex minimax problems, such a type
of convergence guarantee does not rule out the possibility that GDA may get stuck at suboptimal
saddle points of the envelope function, which are well known to be the major challenge for training
high-dimensional machine learning models (Dauphin et al., 2014; Jin et al., 2017; Zhou and Liang,
2018). On the other hand, while numerous algorithms have been developed for escaping saddle points
in conventional nonconvex optimization, e.g., ﬁrst-order algorithms (Ge et al., 2015; Jin et al., 2017;
Carmon and Duchi, 2016; Liu and Yang, 2017) and second-order algorithms (Nesterov and Polyak,
2006; Agarwal et al., 2017; Yue et al., 2019; Zhou et al., 2018), such a type of algorithm has not been
developed for escaping saddle points in nonconvex minimax optimization. Therefore, we want to ask
the following fundamental questions."
INTRODUCTION,0.011527377521613832,"• Q: How to develop a provably convergent Newton-type GDA algorithm that can effectively escape
saddle points in nonconvex minimax optimization? How fast it converges?"
INTRODUCTION,0.012968299711815562,"Developing and analyzing such an algorithm is nontrivial due to the following reasons: 1) we need to
have a good understanding and characterization of both the ﬁrst-order and second-order information
of nonconvex minimax problems; 2) we need to develop a computationally feasible and efﬁcient GDA
algorithm that can leverage the local curvature of the function to escape saddle points; 3) we aim to
develop a uniﬁed analysis framework that can characterize the convergence rate of this algorithm
under different types of nonconvex geometry of the minimax problem."
INTRODUCTION,0.01440922190201729,"In this paper, we provide comprehensive answers to these questions. We develop the ﬁrst Newton-type
GDA algorithm that escapes strict saddle points and converges to second-order stationary points
in nonconvex-strongly-concave minimax optimization. We also characterize the global and local
convergence rates of this algorithm under various types of nonconvex geometry. We summarize our
contributions as follows."
OUR CONTRIBUTIONS,0.01585014409221902,"1.1
OUR CONTRIBUTIONS"
OUR CONTRIBUTIONS,0.01729106628242075,"We consider the minimax optimization problem (P), where f is a twice-differentiable and nonconvex-
strongly-concave function, and its gradient and Jacobian matrices are Lipschitz continuous. Deﬁne an
envelope function Φ(x) := maxy∈Rn f(x, y). The existing GDA algorithms can only ﬁnd ﬁrst-order
stationary points that satisfy ∇Φ(x∗) = 0. In this paper, we develop a Newton-type GDA algorithm
that converges to second-order stationary points of the nonconvex minimax problem (P)."
OUR CONTRIBUTIONS,0.018731988472622477,"Speciﬁcally, we propose Cubic-GDA – a Newton-type GDA algorithm that leverages the classical
cubic regularization technique to escape saddle points. Different from the standard cubic regulariza-
tion algorithm that uses the Hessian information of the function, the Hessian of Φ(x) is not directly
available in nonconvex minimax optimization, and hence we develop a rigorous and computationally
feasible scheme in Cubic-GDA to estimate the Hessian."
OUR CONTRIBUTIONS,0.020172910662824207,"We study the global convergence property of Cubic-GDA in general nonconvex-strongly-concave
optimization. Speciﬁcally, we show that Cubic-GDA admits an intrinsic potential function H(x, x′, y)
(see Proposition 2), which monotonically decreases along the trajectory of Cubic-GDA. Based on the
monotonicity of this potential function, we show that every limit point of the parameter sequence
{xt}t generated by Cubic-GDA is a second-order stationary point of the minimax problem."
OUR CONTRIBUTIONS,0.021613832853025938,"We further analyze the aymptotic convergence rates of Cubic-GDA under a broad spectrum of the
local nonconvex Łojasiewicz gradient geometry. In this case, we show that Cubic-GDA converges to
a unique limit point, which is a second-order stationary point. Moreover, as the geometry parameter
increases (i.e., sharper local geometry), the convergence rate of Cubic-GDA accelerates from sub-
linear convergence up to super-linear convergence, as we summarize in Table 1 below. In particular,
these convergence rates are orderwise faster than those of the standard GDA under the same type of
nonconvex geometry (Chen et al., 2021) 1."
OUR CONTRIBUTIONS,0.023054755043227664,"1Note that the geometry parameter θ in this paper corresponds to
1
1−θ in (Chen et al., 2021)."
OUR CONTRIBUTIONS,0.024495677233429394,Under review as a conference paper at ICLR 2022
OUR CONTRIBUTIONS,0.025936599423631124,"Table 1: Comparison of potential function value gap H(z) −H∗convergence rates of Cubic-GDA
and GDA under different parameterizations of Łojasiewicz gradient geometry."
OUR CONTRIBUTIONS,0.027377521613832854,"Geometry parameter
GDA (Chen et al., 2021)
Cubic-GDA (This paper)"
OUR CONTRIBUTIONS,0.02881844380403458,"θ ∈(2, +∞)
Super-linear convergence
Super-linear convergence"
OUR CONTRIBUTIONS,0.03025936599423631,"θ = 2
Linear convergence
Super-linear convergence"
OUR CONTRIBUTIONS,0.03170028818443804,θ ∈( 3
OUR CONTRIBUTIONS,0.03314121037463977,"2, 2)
Sub-linear convergence
Super-linear convergence θ = 3"
"SUB-LINEAR CONVERGENCE
LINEAR CONVERGENCE",0.0345821325648415,"2
Sub-linear convergence
Linear convergence"
"SUB-LINEAR CONVERGENCE
LINEAR CONVERGENCE",0.03602305475504323,"θ ∈(1, 3"
"SUB-LINEAR CONVERGENCE
LINEAR CONVERGENCE",0.037463976945244955,"2)
Sub-linear convergence
Sub-linear convergence"
RELATED WORK,0.03890489913544669,"1.2
RELATED WORK"
RELATED WORK,0.040345821325648415,"Deterministic GDA algorithms: Many studies characterized the convergence of GDA in nonconvex
minimax optimization. Speciﬁcally, Lin et al. (2020); Nouiehed et al. (2019); Xu et al. (2020d)
studied the convergence of GDA in the nonconvex-concave setting whereas Lin et al. (2020); Xu et al.
(2020d) focused on the nonconvex-strongly-concave setting. In these general nonconvex settings,
it is shown that GDA converges to a certain stationary point at a sublinear rate. Recently, Chen
et al. (2021) proved the parameter convergence of proximal-GDA in regularized nonconvex-strongly-
concave optimization under the Kurdyka-Łojasiewicz geometry. The convergence rates obtained
there are orderwise slower than that of Cubic-GDA. Yang et al. (2020) studied an alternating gradient
descent-ascent (AGDA) algorithm in which the gradient ascent step uses the current variable xt+1
instead of xt. Xu et al. (2020d) studied an alternating gradient projection algorithm which applies
ℓ2 regularizer to the local objective function of GDA followed by projection onto the constraint
sets. Daskalakis and Panageas (2018); Mokhtari et al. (2020); Zhang and Wang (2021) analyzed
optimistic gradient descent-ascent (OGDA) which applies negative momentum to accelerate GDA.
Mokhtari et al. (2020) also studied an extra-gradient algorithm which applies two-step GDA in
each iteration. Nouiehed et al. (2019) studied multi-step GDA where multiple gradient ascent steps
are performed, and they also studied the momentum-accelerated version. Cherukuri et al. (2017);
Daskalakis and Panageas (2018); Jin et al. (2020) studied GDA in continuous time dynamics using
differential equations. Adolphs et al. (2019) analyzed a second-order variant of the GDA algorithm.
In a concurrent work (Luo and Chen, 2021), the authors proposed and studied the same Cubic-GDA
algorithm. They characterize the computation complexity under a special type of inexactness that
approximates the inverse Jacobian using matrix Chebyshev polynomials. As a comparison, this study
focuses on analyzing the global and local convergence properties of Cubic-GDA."
RELATED WORK,0.04178674351585014,"Stochastic GDA algorithms: Lin et al. (2020); Yang et al. (2020) analyzed stochastic GDA and
stochastic AGDA, which are direct extension of GDA and AGDA to the stochastic setting. Variance
reduction techniques have been applied to stochastic minimax optimization, including SVRG-based
(Du and Hu, 2019; Yang et al., 2020), SPIDER-based (Xu et al., 2020c), SREDA (Xu et al., 2020b),
STORM (Qiu et al., 2020) and its gradient free version (Huang et al., 2020). Xie et al. (2020) studied
the complexity lower bound of ﬁrst-order stochastic algorithms for ﬁnite-sum minimax problem."
RELATED WORK,0.043227665706051875,"Cubic regularization (CR): CR algorithm dates back to (Griewank, 1981), where global convergence
of the algorithm is established. In Nesterov and Polyak (2006), the authors analyzed the convergence
rate of CR to second-order stationary points for nonconvex optimization. In (Nesterov, 2008), the
authors established the sub-linear convergence of CR for solving convex smooth problems, and they
further proposed an accelerated version of CR with improved sub-linear convergence. Recently, Yue
et al. (2019) studied the asymptotic convergence properties of CR under the error bound condition,
and established the quadratic convergence of the iterates. Recently, Hallak and Teboulle (2020)
proposed a framework of two directional method for ﬁnding second-order stationary points in general
smooth nonconvex optimization. This main idea of the algorithm is to search for a feasible direction
toward the solution and is not based on cubic regularization. Several other works proposed different
methods to solve the cubic subproblem of CR, e.g., (Agarwal et al., 2017; Carmon and Duchi, 2016;
Cartis et al., 2011b). Another line of work aimed at improving the computation efﬁciency of CR by
solving the cubic subproblem with inexact gradient and Hessian information. In particular, Ghadimi
et al. (2017) proposed an inexact CR for solving convex problem. Also, Cartis et al. (2011a) proposed"
RELATED WORK,0.0446685878962536,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04610951008645533,"an adaptive inexact CR for nonconvex optimization, whereas Jiang et al. (2017) further studied
the accelerated version for convex optimization. Several studies explored subsampling schemes to
implement inexact CR algorithms, e.g., (Kohler and Lucchi, 2017; Xu et al., 2020a; Zhou and Liang,
2018; Wang et al., 2018)."
PROBLEM FORMULATION AND PRELIMINARIES,0.04755043227665706,"2
PROBLEM FORMULATION AND PRELIMINARIES"
PROBLEM FORMULATION AND PRELIMINARIES,0.04899135446685879,"In this section, we introduce the problem formulation and present some preliminary results that will
be used in the analysis."
PROBLEM FORMULATION AND PRELIMINARIES,0.05043227665706052,"Notation: For notation simplicity, we denote ∇1f, ∇2f as the gradients with respect to the ﬁrst and
the second input arguments of f, respectively. We also denote ∇11f, ∇22f as the Jacobian matrices
where the second-order derivatives are taken over the ﬁrst and second arguments of f, respectively.
Moreover, we denote ∇12f as the Jacobian matrix where the second-order derivative is taken over
the ﬁrst argument of f and followed by the second argument, and ∇21f is deﬁned in a similar way."
PROBLEM FORMULATION AND PRELIMINARIES,0.05187319884726225,"We consider the minimax optimization problem (P) that satisﬁes the following standard assumptions.
Assumption 1. The minimax optimization problem (P) satisﬁes:"
PROBLEM FORMULATION AND PRELIMINARIES,0.053314121037463975,"1. Function f(·, ·) is L1-smooth and function f(x, ·) is µ-strongly concave for all ﬁxed x;"
PROBLEM FORMULATION AND PRELIMINARIES,0.05475504322766571,"2. The Jacobian matrices ∇11f, ∇12f, ∇21f, ∇22f are L2-Lipschitz;"
PROBLEM FORMULATION AND PRELIMINARIES,0.056195965417867436,3. Function Φ is bounded below and has compact sub-level sets.
PROBLEM FORMULATION AND PRELIMINARIES,0.05763688760806916,"To elaborate, item 1 considers the class of nonconvex-strongly-concave functions f that has been
widely studied in the minimax optimization literature (Lin et al., 2020; Jin et al., 2020; Xu et al.,
2020d; Lu et al., 2020). Items 2 assumes that the block Jacobian matrices of f are Lipschitz, which
is a standard assumption for analyzing many second-order optimization algorithms (Nesterov and
Polyak, 2006; Agarwal et al., 2017; Yue et al., 2019). Moreover, item 3 guarantees that the minimax
problem has at least one solution. By strong concavity of f(x, ·), it is clear that the maximizer
y∗(x) := arg maxy∈Rn f(x, y) is unique for every x ∈Rm. In particular, if x∗is a second-order
stationary point of Φ(x), then (x∗, y∗(x∗)) is the desired solution of the minimax problem (P)."
PROBLEM FORMULATION AND PRELIMINARIES,0.059077809798270896,"Deﬁne an envelope function Φ(x) := maxy∈Rn f(x, y). Then the minimax problem (P) is equivalent
to the minimization problem. minx∈Rm Φ(x), where Φ(x) = maxy∈Rn f(x, y). As we show in item
2 of Proposition 1 later, this envelope function Φ(x) is smooth and nonconvex. The existing GDA
algorithms can only ﬁnd ﬁrst-order stationary points of the minimax problem that satisfy ∇Φ(x∗) = 0.
In this paper, we aim to develop a provably convergent algorithm that can ﬁnd second-order stationary
points x∗of the function Φ(x) that satisfy the following set of conditions."
PROBLEM FORMULATION AND PRELIMINARIES,0.06051873198847262,"(Second-order stationary):
∇Φ(x∗) = 0,
∇2Φ(x∗) ⪰0."
PROBLEM FORMULATION AND PRELIMINARIES,0.06195965417867435,"In the existing literature, many optimization algorithms have been developed for ﬁnding second-
order stationary points in conventional nonconvex minimization problems. This includes ﬁrst-order
algorithms (Ge et al., 2015; Jin et al., 2017; Carmon and Duchi, 2016; Liu and Yang, 2017) and
second-order algorithms (Nesterov and Polyak, 2006; Agarwal et al., 2017; Yue et al., 2019; Zhou
et al., 2018). However, these algorithms are not directly applicable to solve the problem (P’), as the
function Φ(x) involves a special maximization structure and hence its speciﬁc function form Φ as
well as the gradient ∇Φ and Hessian ∇2Φ are not available in practice. Instead, our algorithm design
can only leverage information of the bi-variate function f."
PROBLEM FORMULATION AND PRELIMINARIES,0.06340057636887608,"Next, we present some important properties regarding the gradient and Jacobian matrices of the
functions f(x, y) and Φ(x). Throughout, we denote κ = L1/µ as the condition number.
Proposition 1. Let Assumption 1 hold. Then, the following statements hold."
PROBLEM FORMULATION AND PRELIMINARIES,0.06484149855907781,1. Mapping y∗(x) is κ-Lipschitz continuous;
PROBLEM FORMULATION AND PRELIMINARIES,0.06628242074927954,"2. Function Φ(x) is L1(1 + κ)-smooth and ∇Φ(x) = ∇1f(x, y∗(x));"
PROBLEM FORMULATION AND PRELIMINARIES,0.06772334293948126,"3. Deﬁne G(x, y) = ∇11f(x, y) −∇12f(x, y)[∇22f(x, y)]−1∇21f(x, y). Then, G is a Lipschitz
mapping with constant LG = L2(1 + κ)2, i.e., ∥G(x′, y′) −G(x, y)∥≤LG∥(x′, y′) −(x, y)∥;"
PROBLEM FORMULATION AND PRELIMINARIES,0.069164265129683,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION AND PRELIMINARIES,0.07060518731988473,"4. The Hessian of Φ satisﬁes ∇2Φ(x) = G(x, y∗(x)), which is Lipschitz continuous with constant
LΦ = LG(1 + κ) = L2(1 + κ)3."
PROBLEM FORMULATION AND PRELIMINARIES,0.07204610951008646,"The ﬁrst two items characterize the gradient of the envelope function Φ in terms of the partial gradient
of the bi-variate objective function f. They are proved in the previous work (Lin et al., 2020) and we
include them for completeness. On the other hand, the last two items further characterize the Hessian
of Φ in terms of the block Jacobian matrices of f. As we present in the next section, the Lipschitz
continuous Hessian ∇2Φ(x) allows us to develop a cubic-regularization-based algorithm for ﬁnding
second-order stationary points. We also note that the proof of items 3 & 4 are not trivial. Speciﬁcally,
we need to ﬁrst develop bounds for the spectrum norm of the block Jacobian matrices in Lemma 1
(see the ﬁrst page of the appendix), which helps prove the Lipschitz continuity of the G mapping
in item 3. Moreover, we leverage the optimality condition of f(x, ·) to derive an expression for the
maximizer mapping y∗(x) (see (15) in the appendix), which is used to further prove item 4."
PROBLEM FORMULATION AND PRELIMINARIES,0.07348703170028818,"3
CUBIC-GDA: CUBIC-REGULARIZED GRADIENT DESCENT-ASCENT"
PROBLEM FORMULATION AND PRELIMINARIES,0.07492795389048991,"In this section, we propose a new Gradient Descent-Ascent (GDA) algorithm that leverages the
cubic regularization technique (Nesterov and Polyak, 2006) to escape strict saddle points and ﬁnd
second-order stationary points of the nonconvex minimax problem (P)."
PROBLEM FORMULATION AND PRELIMINARIES,0.07636887608069164,"Our algorithm design is inspired by the conventional cubic regularization algorithm (Nesterov and
Polyak, 2006). Speciﬁcally, to ﬁnd a second-order stationary point of the envelope function Φ(x), the
conventional cubic regularization algorithm would perform the following iterative update."
PROBLEM FORMULATION AND PRELIMINARIES,0.07780979827089338,"xt+1 ∈arg min
x
∇Φ(xt)⊤(x −xt) + 1"
PROBLEM FORMULATION AND PRELIMINARIES,0.0792507204610951,"2(x −xt)⊤∇2Φ(xt)(x −xt) +
1
6ηx
∥x −xt∥3,
(1)"
PROBLEM FORMULATION AND PRELIMINARIES,0.08069164265129683,"where ηx > 0 is a proper learning rate. However, due to the special maximization structure of
Φ, its gradient and Hessian have complex formulas (see Proposition 1) that involve the mapping
y∗(x), which cannot be computed exactly in practice. Hence, we aim to develop a new algorithm to
efﬁciently compute approximations of the gradient and Hessian of Φ and use them to perform the
cubic regularization update."
PROBLEM FORMULATION AND PRELIMINARIES,0.08213256484149856,"To perform the cubic regularization update in eq. (1), we need to compute ∇Φ(xt) = ∇1f(xt, y∗(xt))
and ∇2Φ(xt) = G(xt, y∗(xt)) (by Proposition 1), both of which depend on the maximizer y∗(xt)
of the function f(xt, ·). Since f(xt, ·) is strongly-concave, we can run Nt iterations of gradient
ascent to obtain an approximated maximizer eyNt ≈y∗(xt), and then approximate ∇Φ(xt), ∇2Φ(xt)
using ∇1f(xt, eyNt) and G(xt, eyNt), respectively. Intuitively, these are good approximations due to
two reasons: (i) eyNt converges to y∗(xt) at a fast linear convergence rate; and (ii) both ∇1f and
G are shown in Proposition 1 to be Lipschitz continuous in their second argument. We refer to
this algorithm as Cubic-Regularized Gradient Descent-Ascent (Cubic-GDA), and summarize its
update rule in Algorithm 1 below."
PROBLEM FORMULATION AND PRELIMINARIES,0.08357348703170028,"Algorithm 1 Cubic-Regularized Gradient Descent-Ascent (Cubic-GDA)
Input: Initialize x0, y0 and learning rates ηx, ηy.
for t = 0, 1, 2, . . . , T −1 do"
PROBLEM FORMULATION AND PRELIMINARIES,0.08501440922190202,"Initialize ey0 = yt.
for k = 0, 1, 2, . . . , Nt −1 do"
PROBLEM FORMULATION AND PRELIMINARIES,0.08645533141210375,"eyk+1 = eyk + ηy∇2f(xt, eyk).
end
Set yt+1 = eyNt and compute G(xt, yt+1) as follows:"
PROBLEM FORMULATION AND PRELIMINARIES,0.08789625360230548,"G(xt, yt+1) = ∇11f(xt, yt+1) −∇12f(xt, yt+1)[∇22f(xt, yt+1)]−1∇21f(xt, yt+1)."
PROBLEM FORMULATION AND PRELIMINARIES,0.0893371757925072,"xt+1 ∈arg minx ∇1f(xt, yt+1)⊤(x −xt) + 1"
PROBLEM FORMULATION AND PRELIMINARIES,0.09077809798270893,"2(x −xt)⊤G(xt, yt+1)(x −xt) +
1
6ηx ∥x −xt∥3.
end
Output: xT , yT ."
PROBLEM FORMULATION AND PRELIMINARIES,0.09221902017291066,"We further comment on the implementation of Cubic-GDA. We note that the Cubic-GDA updates in
Algorithm 1 can be implemented in a computation efﬁcient way. First, note that Cubic-GDA involves"
PROBLEM FORMULATION AND PRELIMINARIES,0.0936599423631124,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION AND PRELIMINARIES,0.09510086455331412,"an inner loop that performs gradient ascent updates. To guarantee the convergence of the algorithm, we
prove in the next section that the number of inner iterations Nt only needs to be kept at logarithm scale.
Therefore, a few number of inner iterations sufﬁce to guarantee convergence in practice. Second, the
cubic regularization sub-problem can be efﬁciently solved by the gradient descent algorithm (Carmon
and Duchi, 2016), and it involves computation of only Jacobian-vector product that can be efﬁciently
computed by the existing machine learning platforms such as TensorFlow (Abadi et al., 2015) and
PyTorch (Paszke et al., 2019). In particular, to compute the Hessian-vector product G(x, y)v for
any vector v, one needs to compute the Jacobian-vector product ∇11f(x, y)v and the matrix-vector
product ∇12f(x, y)[∇22f(x, y)]−1∇21f(x, y)v. We note that this matrix-vector product term can
be computed as follows: ﬁrst compute the Jacobian-vector product b = ∇21f(x, y)v; Then, solve
the invertible linear system ∇22f(x, y)u = b using any standard solver (e.g., conjugate gradient
method), which involves iteratively computing Jacobian-vector products ∇22f(x, y)w for some
vector w; Finally, compute the Jacobian-vector product ∇12f(x, y)u. Hence, one can call multiple
Jacobian-vector product oracles to solve the cubic regularization sub-problem."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.09654178674351585,"4
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.09798270893371758,"In this section, we study the global convergence properties of Cubic-GDA. Importantly, our analysis
is based on characterizing an intrinsic potential function of the Cubic-GDA algorithm in nonconvex
minimax optimization."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.0994236311239193,"Recall that our goal is to ﬁnd a second-order stationary point of the function Φ(x). Our next result
shows that Cubic-GDA admits an intrinsic potential function that monotonically decreases in the
optimization process. The proof of is included in Appendix B.
Proposition 2. Let Assumption 1 hold. Deﬁne the following potential function"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10086455331412104,"H(x, x′, y) := Φ(x) + L2κ3∥x′ −x∥3 + 4L2∥y −y∗(x)∥3,"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10230547550432277,"and denote Ht := H(xt, xt−1, yt+1). Choose Nt ≥max(ln 2, ln[L1∥∇2f(xt,yt)∥/(L2µ)]−2 ln ∥xt−xt−1∥)"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1037463976945245,"ln[κ/(κ−1)]
and learning rates ηx ≤
1
28L2κ3 , ηy ≤
2
L+µ. Then, the sequences {xt, yt}t generated by Cubic-GDA
satisfy, for all t = 0, 1, 2, ..."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10518731988472622,"Ht+1 ≤Ht−L2κ3∥xt+1 −xt∥3−L2∥yt+1 −y∗(xt)∥3.
(2)"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10662824207492795,"Consequently, it holds that"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10806916426512968,"lim
t→∞∥xt+1 −xt∥= 0,
lim
t→∞∥yt+1 −yt∥= 0,
lim
t→∞∥yt −y∗(xt)∥= 0."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.10951008645533142,"Remark 1. We note that the above key result can also be established for an inexact version of Cubic-
GDA, which formulates the cubic subproblem with a general inexact gradient pt ≈∇1f(xt, yt+1)
and inexact Jacobian Pt ≈G(xt, yt+1) that satisfy the conditions"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11095100864553314,"∥pt −∇1f(xt, yt+1)∥≤O(∥xt+1 −xt∥2),
∥Pt −G(xt, yt+1)∥≤O(∥xt+1 −xt∥)."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11239193083573487,"These inexactness conditions are widely studied in the existing literature Cartis et al. (2011b;a).
Under these inexact conditions, our proof of the above proposition remains unchanged, except that
the coefﬁcients of the term ∥xt+1 −xt∥3 would be slightly different."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1138328530259366,"Proposition 2 reveals that Cubic-GDA admits an intrinsic potential function H, which is the objective
function Φ(x) regularized by two cubic terms ∥x′ −x∥3, ∥y −y∗(x)∥3. Such a potential function is
closely connected to the optimization goal. Speciﬁcally, consider a desired case where xt converges
to a certain second-order stationary point x∗and yt converges to y∗(x∗), it is clear that the potential
function Ht would converge to the corresponding function value Φ(x∗). Hence, minimizing the
function Φ is equivalent to minimizing the potential function H. More importantly, Proposition 2
shows that this potential function is monotonically decreasing along the optimization path of Cubic-
GDA, implying that the algorithm continuously makes optimization progress. By leveraging this
property of the potential function, we are able to show that the parameter sequences generated by
Cubic-GDA are asymptotically stable, i.e., xt+1 −xt →0, yt →y∗(xt).
Remark 2. In each outer iteration t, we set the total number of inner gradient ascent iterations
Nt based on ∥∇2f(xt, yt)∥and ∥xt −xt−1∥. Note that both of these two quantities can be easily
computed right after iteration t −1."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11527377521613832,Under review as a conference paper at ICLR 2022
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11671469740634005,"Based on Proposition 2, we are able to prove the convergence of {xt}t to a certain second-order
stationary point, which we formally present in the next theorem. The proof is included in Appendix C.
Theorem 1 (Global convergence). Under the same conditions as those of Proposition 2, the Cubic-
GDA has the following global convergence properties."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11815561959654179,1. The function value sequence {Φ(xt)}t converges to a ﬁnite limit H∗> −∞;
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.11959654178674352,"2. The generated sequences {xt}t, {yt}t are bounded and have a compact sets of limit points."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.12103746397694524,"3. Every limit point x∗of {xt}t is a second-order stationary point of Φ, i.e., ∇Φ(x∗) = 0,
∇2Φ(x∗) ⪰0, and satisﬁes Φ(x∗) = H∗."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.12247838616714697,"The above theorem establishes the global convergence properties of Cubic-GDA. Speciﬁcally, item
1 shows that the function value sequence {Φ(xt)}t converges to a ﬁnite limit H∗, which is also
the limit of the potential function sequence {Ht}t. Moreover, items 2 & 3 further show that all
the limit points of {xt}t are second-order stationary points of the minimax problem, at which the
function Φ achieves the constant value H∗. These results show that Cubic-GDA is guaranteed to ﬁnd
second-order stationary points in nonconvex minimax optimization."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1239193083573487,"By further leveraging the potential function characterized in Proposition 2, we obtain the following
global convergence rate of Cubic-GDA to a second-order stationary point. The proof is included
in Appendix D. Throughout, we adopt the following standard measure of second-order stationary
introduced in (Nesterov and Polyak, 2006)."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.12536023054755044,µ(x) = max (s
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.12680115273775217,"∥∇Φ(x)∥
1/(2ηx) + 5L2κ3 + 4L2
2κ2/L1"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1282420749279539,",
−λmin
 
∇2Φ(x)
"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.12968299711815562,"1/(2ηx) + 4L2
2κ2/L1 ) ."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.13112391930835735,"Intuitively, a smaller µ(x) means that the point x is closer to being second-order stationary.
Theorem 2 (Global convergence rate). Under the same conditions as those of Proposition 2, the
Cubic-GDA converges at the following rate for all T ≥H0−infx∈Rm Φ(x)"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.13256484149855907,"L2κ3/3
."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1340057636887608,"min
0≤t≤T −1 µ(xt) ≤
H0 −infx∈Rm Φ(x)"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.13544668587896252,TL2κ3/3
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.13688760806916425,"1/3
."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.138328530259366,"The above theorem shows that the ﬁrst-order stationary measure ∥∇Φ(xt)∥converges at a sublinear
rate O(T −2"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.13976945244956773,"3 ), and the second-order stationary measure −λmin
 
∇2Φ(x)

converges at a sublinear
rate O(T −1"
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.14121037463976946,"3 ). Both results match the convergence rates of the cubic regularization algorithm for
nonconvex minimization (Nesterov and Polyak, 2006). Therefore, by leveraging the curvature of the
approximated Hessian matrix G(xt, yt+1), Cubic-GDA is able to escape strict saddle points of Φ at a
fast rate."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.14265129682997119,"We note that the proof of the global convergence results in Theorems 1 and 2 are critically based on
the intrinsic potential function H that we characterized in Proposition 2. We elaborate our technical
contribution as follows."
GLOBAL CONVERGENCE PROPERTIES OF CUBIC-GDA,0.1440922190201729,"• First, to identify the potential function, we need to characterize the per-iteration progress induced
by the cubic regularization step. However, the cubic subproblem in Cubic-GDA is constructed by
an inexact gradient ∇1f(xt, yt+1) and Hessian matrix G(xt, yt+1). Therefore, we need to properly
choose the number of inner gradient ascent iterations Nt to control the estimation error of both the
gradient and Hessian approximations at a desired level.
• Due to the inexactness of the gradient and Hessian matrix, the cubic regularization update of
Cubic-GDA does not lead to a monotonically decreasing function value Φ(xt), as opposed to the
original cubic regularization algorithm in nonconvex minimization (which uses exact gradient and
Hessian). Hence, we construct and identify a decreasing potential function H instead."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.14553314121037464,"5
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.14697406340057637,"The (2) of Proposition 2 shows that Cubic-GDA has a special optimization dynamics and therefore
its convergence rate is expected to be different from that of the vanilla GDA in nonconvex minimax"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1484149855907781,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.14985590778097982,"optimization. In this section, we explore the convergence rates of Cubic-GDA under a broad spectrum
of local nonconvex geometries characterized by the Łojasiewicz gradient inequality."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15129682997118155,"We ﬁrst introduce the Łojasiewicz gradient geometry of a function h. Throughout, the point-to-set
distance is denoted as distΩ(x) := infu∈Ω∥x −u∥.
Deﬁnition 1. A differentiable function h is said to satisfy the Łojasiewicz gradient geometry if for
every compact set Ωof critical points on which h takes a constant value hΩ∈R, there exist ε, λ > 0
such that for all x ∈{z ∈Rm : distΩ(z) < ε, hΩ< h(z) < hΩ+ λ}, the following condition holds."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15273775216138327,"h(x) −hΩ≤c∥∇h(x)∥θ,
(3)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15417867435158503,"where c > 0 is a universal constant and θ ∈(1, +∞) is the geometry parameter."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15561959654178675,"Intuitively, the Łojasiewicz gradient geometry is a gradient-dominant-type geometry that characterizes
the local geometry of a nonconvex function around the set of critical points. In particular, it generalizes
the Polyak-Łojasiewicz (PL) geometry that corresponds to the special case θ = 2 Łojasiewicz (1963);
Karimi et al. (2016). In fact, a generalized version of the Łojasiewicz gradient geometry has
been shown to hold for a large class of functions including sub-analytic functions, exponential
functions and semi-algebraic functions, which cover most of the nonconvex functions encountered
in machine learning applications (Zhou et al., 2016; Yue et al., 2019; Zhou and Liang, 2017; Zhou
et al., 2018). For example, consider the class of robust machine learning problems that involve
the minimax problem minθ maxξi
1
n
Pn
i=1 ℓ(hθ(ξi), yi)2 −λ"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15706051873198848,"2 ∥ξi −ai∥2. Here (xi, yi) is the i-th
data sample that includes, e.g., an image xi and its label yi, ξi denotes the adversarial image, hθ
is a classiﬁcation model parameterized by θ, and ℓdenotes the loss function. Such a problem is
nonconvex-stongly-concave when λ is large. In particular, as elaborated in the appendix of (Bolte
et al., 2014), the envelop function Φ(x) := maxξi
1
n
Pn
i=1 ℓ(hθ(ξi), yi)2 −λ"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1585014409221902,"2 ∥ξi −ai∥2 satisﬁes
the local Lojasiewicz gradient geometry if it is semi-algebraic, which holds if every sample loss
f(θ, ξi) := ℓ(hθ(ξi), yi)2 −λ"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.15994236311239193,2 ∥ξi −ai∥2 is semi-algebraic.
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.16138328530259366,"By (2) of Proposition 2 and (30) (proved in Appendix E), we show that the potential function H of
Cubic-GDA satisﬁes the following special optimization dynamics."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1628242074927954,"Ht+1 −Ht ≤−O
 
∥xt+1 −xt∥3
,
(4)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1642651296829971,"∥∇Ht∥≤O
 
∥xt −xt−1∥2 + ∥xt−1 −xt−2∥2
.
(5)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.16570605187319884,"The above dynamics of Cubic-GDA involves higher-order terms than the dynamics of GDA, which
takes the form Ht+1 −Ht ≤−O(∥xt+1 −xt∥2) and ∥∇Ht∥≤O(∥xt+1 −xt∥) (Chen et al., 2021).
Thus, it is expected that Cubic-GDA achieves a faster convergence rate than GDA. On the other hand,
compare with the dynamics of the cubic regularization algorithm (Zhou et al., 2018), the gradient
dynamic of Cubic-GDA in (5) involves an additional term ∥xt−1−xt−2∥2 that depends on the history,
which is due to the inexact gradient and Hessian used in the cubic regularization step."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.16714697406340057,"With the optimization dynamics in (4) and (5) and by leveraging the Łojasiewicz gradient geometry,
we are able to prove the following strengthened convergence result of Cubic-GDA.
Theorem 3. Let Assumption 1 hold and assume that the potential function H satisﬁes the local
Łojasiewicz gradient geometry. Choose the hyperparameters Nt, ηx, ηy in the same way as Proposi-
tion 2. Then, the sequences {(xt, yt)}t generated by Cubic-GDA have a unique limit point, which is
a second-order stationary point of Φ."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1685878962536023,"Recall that Theorem 1 only proves that every limit point of {xt}t is a second-order stationary point
of the minimax problem. Theorem 3 further strengthens Theorem 1 by showing that Cubic-GDA
converges to a unique second-order stationary limit point under the Łojasiewicz gradient geometry."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.17002881844380405,"Next, we further study the asymptotic convergence rates of Cubic-GDA under different parameter
ranges of the Łojasiewicz gradient geometry. We ﬁrst obtain the following function value convergence
rate result, which strengthens the convergence rate result established in Theorem 2. Throughout, t0
denotes a sufﬁciently large positive integer and C0 is a universal positive constant deﬁned as C0 = 3√"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.17146974063400577,"2c1/θL−2/3
2

10L2κ + 24L3
2
L2
1κ + 4L2
2
L1
+
1
2ηxκ2 "
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1729106628242075,The proof is included in Appendix F.
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.17435158501440923,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.17579250720461095,"Theorem 4 (Funtion value convergence rate). Under the same conditions as those of Theorem 3, the
sequence of potential function {Ht}t converges to the limit H∗at the following rates."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.17723342939481268,1. If the geometry parameter θ ∈( 3
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1786743515850144,"2, ∞), then Ht ↓H∗super-linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.18011527377521613,"Ht −H∗≤O

exp

−
2θ 3"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.18155619596541786, t−t0
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1829971181556196,"2 
,
∀t ≥t0;
(6)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1844380403458213,2. If the geometry parameter θ = 3
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.18587896253602307,"2, then Ht ↓H∗linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1873198847262248,"Ht −H∗≤(1 + C3/2
0
)−t−t0"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.18876080691642652,"2 ,
∀t ≥t0;
(7)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.19020172910662825,"3. If the geometry parameter θ ∈(1, 3"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.19164265129682997,"2), then Ht ↓H∗sub-linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1930835734870317,"Ht −H∗≤O

(t −t0)−
2θ
3−2θ

,
∀t ≥t0.
(8)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.19452449567723343,"Remark 3. We note that if the Łojasiewicz gradient geometry holds globally, then the above asymp-
totic convergence rates become global convergence rates."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.19596541786743515,"The above theorem characterizes the convergence rates of the potential function of Cubic-GDA in the
full spectrum of θ of the local Łojasiewicz gradient geometry. Speciﬁcally, it shows that a larger θ
implies that the local geometry is sharper and hence leads to a faster convergence rate. In particular, as
we summarize in Table 1 in the introduction section, the convergence rate of Cubic-GDA is orderwise
faster than that of the vanilla GDA for a wide range of the parameter of the Łojasiewicz gradient
geometry. This demonstrates the advantage of leveraging higher-order information in nonconvex
minimax-optimization."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.19740634005763688,"As a byproduct, we also obtain the following asymptotic convergence rates of the parameter se-
quences generated by Cubic-GDA under the Łojasiewicz gradient geometry. The proof is included in
Appendix G.
Theorem 5 (Parameter convergence rate). Under the same conditions as those of Theorem 3, the
sequences {xt, yt}t generated by Cubic-GDA converge to their limits x∗, y∗(x∗) respectively at the
following rates."
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.1988472622478386,1. If the geometry parameter θ ∈( 3
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.20028818443804033,"2, ∞), then (xt, yt) →(x∗, y∗(x∗)) super-linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.2017291066282421,"max

∥xt −x∗∥, ∥yt −y∗(x∗)∥
	
≤O

exp

−1 3 2θ 3"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.20317002881844382, t−t0
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.20461095100864554,"2
−1
,
∀t ≥t0;
(9)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.20605187319884727,2. If the geometry parameter θ = 3
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.207492795389049,"2, then (xt, yt) →(x∗, y∗(x∗)) linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.20893371757925072,"max

∥xt −x∗∥, ∥yt −y∗(x∗)∥
	
≤O

(1 + C3/2
0
)−t−t0"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.21037463976945245,"2

,
∀t ≥t0;
(10)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.21181556195965417,"3. If the geometry parameter θ ∈(1, 3"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.2132564841498559,"2), then (xt, yt) →(x∗, y∗(x∗)) sub-linearly as"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.21469740634005763,"∥xt −x∗∥≤O

(t −t0)−2(θ−1)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.21613832853025935,"3−2θ

, ∥yt −y∗(xt)∥≤O

(t −t0)−
2θ
3(3−2θ)

,
∀t ≥t0.
(11)"
CONVERGENCE ANALYSIS UNDER LOCAL NONCONVEX GEOMETRY,0.21757925072046108,"It can be seen that, similar to the convergence rate results of the function value sequence, the
convergence rate of the parameter sequence is also affected by the parameterization of the local
geometry."
CONCLUSION,0.21902017291066284,"6
CONCLUSION"
CONCLUSION,0.22046109510086456,"In this paper, we take one step further toward improving the convergence guarantee of GDA-type
algorithms in nonconvex minimax optimization. We develop a Cubic-GDA algorithm that leverages
the second-order information and the cubic regularization technique to effectively escape strict saddle
points in nonconvex minimax optimization. Our key observation is that Cubic-GDA has an intrinsic
potential function that monotonically decreases in the optimization process, and this leads to a
guaranteed global convergence of the algorithm. Moreover, our convergence analysis shows that
Cubic-GDA achieves a faster convergence rate than the standard GDA for a wide spectrum of gradient
dominant-type nonconvex geometries. In the future study, we will develop stochastic variants of
Cubic-GDA to further improve its computation efﬁciency. Another interesting direction is to apply
momentum techniques to further accelerate the convergence of this algorithm."
CONCLUSION,0.2219020172910663,Under review as a conference paper at ICLR 2022
REFERENCES,0.22334293948126802,REFERENCES
REFERENCES,0.22478386167146974,"Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz,
R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C.,
Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015).
TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from
tensorﬂow.org."
REFERENCES,0.22622478386167147,"Adolphs, L., Daneshmand, H., Lucchi, A., and Hofmann, T. (2019). Local saddle point optimization:
A curvature exploitation approach. In Proc. International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 486–495."
REFERENCES,0.2276657060518732,"Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T. (2017). Finding approximate local
minima faster than gradient descent. In Proc. Annual ACM SIGACT Symposium on Theory of
Computing (STOC), pages 1195–1199."
REFERENCES,0.22910662824207492,"Bolte, J., Sabach, S., and Teboulle, M. (2014). Proximal alternating linearized minimization for
nonconvex and nonsmooth problems. Mathematical Programming, 146(1-2):459–494."
REFERENCES,0.23054755043227665,"Carmon, Y. and Duchi, J. C. (2016). Gradient descent efﬁciently ﬁnds the cubic-regularized non-
convex Newton step. ArXiv: 1612.00547."
REFERENCES,0.23198847262247838,"Cartis, C., Gould, N. I. M., and Toint, P. (2011a). Adaptive cubic regularization methods for
unconstrained optimization. part ii: worst-case function- and derivative-evaluation complexity.
Mathematical Programming, 130(2):295–319."
REFERENCES,0.2334293948126801,"Cartis, C., Gould, N. I. M., and Toint, P. L. (2011b). Adaptive cubic regularization methods for
unconstrained optimization. part i : motivation, convergence and numerical results. Mathematical
Programming."
REFERENCES,0.23487031700288186,"Chen, Z., Zhou, Y., Xu, T., and Liang, Y. (2021). Proximal gradient descent-ascent: Variable
convergence under kł geometry. In Proc. International Conference on Learning Representations
(ICLR)."
REFERENCES,0.23631123919308358,"Cherukuri, A., Gharesifard, B., and Cortes, J. (2017). Saddle-point dynamics: conditions for
asymptotic stability of saddle points. SIAM Journal on Control and Optimization, 55(1):486–511."
REFERENCES,0.2377521613832853,"Daskalakis, C. and Panageas, I. (2018). The limit points of (optimistic) gradient descent in min-max
optimization. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages
9236–9246."
REFERENCES,0.23919308357348704,"Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc.
Advances in Neural Information Processing Systems (NeurIPS), page 2933–2941."
REFERENCES,0.24063400576368876,"Du, S. S. and Hu, W. (2019). Linear convergence of the primal-dual gradient method for convex-
concave saddle point problems without strong convexity. In Proc. International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), pages 196–205."
REFERENCES,0.2420749279538905,"Ferreira, M. A. M., Andrade, M., Matos, M. C. P., Filipe, J. A., and Coelho, M. P. (2012). Minimax
theorem and nash equilibrium. International Journal of Latest Trends in Finance & Economic
Sciences."
REFERENCES,0.24351585014409222,"Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015). Escaping from saddle points — online stochastic gra-
dient for tensor decomposition. In Proc. 28th Conference on Learning Theory (COLT), volume 40,
pages 797–842."
REFERENCES,0.24495677233429394,"Ghadimi, S., Liu, H., and Zhang, T. (2017). Second-order methods with cubic regularization under
inexact information. ArXiv: 1710.05782."
REFERENCES,0.24639769452449567,"Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,
and Bengio, Y. (2014). Generative adversarial nets. In Proc. Advances in Neural Information
Processing Systems (NeurIPS), pages 2672–2680."
REFERENCES,0.2478386167146974,Under review as a conference paper at ICLR 2022
REFERENCES,0.24927953890489912,"Griewank, A. (1981). The modiﬁcation of newton’s method for unconstrained optimization by
bounding cubic terms. Technical Report."
REFERENCES,0.2507204610951009,"Hallak, N. and Teboulle, M. (2020). Finding second-order stationary points in constrained min-
imization: A feasible direction approach. Journal of Optimization Theory and Applications,
186(2):480–503."
REFERENCES,0.2521613832853026,"Ho, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 4565–4573."
REFERENCES,0.25360230547550433,"Huang, F., Gao, S., Pei, J., and Huang, H. (2020). Accelerated zeroth-order momentum methods
from mini to minimax optimization. ArXiv:2008.08170."
REFERENCES,0.25504322766570603,"Jiang, B., Lin, T., and Zhang, S. (2017). A uniﬁed scheme to accelerate adaptive cubic regularization
and gradient methods for convex optimization. ArXiv:1710.04788."
REFERENCES,0.2564841498559078,"Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points
efﬁciently. In Proc. International Conference on Machine Learning (ICML), volume 70, pages
1724–1732."
REFERENCES,0.2579250720461095,"Jin, C., Netrapalli, P., and Jordan, M. I. (2020). What is local optimality in nonconvex-nonconcave
minimax optimization? In Proc. International Conference on Machine Learning (ICML)."
REFERENCES,0.25936599423631124,"Karimi, H., Nutini, J., and Schmidt, M. (2016). Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. Machine Learning and Knowledge Discovery in
Databases: European Conference, pages 795–811."
REFERENCES,0.260806916426513,"Kohler, J. M. and Lucchi, A. (2017). Sub-sampled cubic regularization for non-convex optimization.
In Proc. International Conference on Machine Learning (ICML), volume 70, pages 1895–1904."
REFERENCES,0.2622478386167147,"Lin, T., Jin, C., and Jordan, M. I. (2020). On gradient descent ascent for nonconvex-concave minimax
problems. In Proc. International Conference on Machine Learning (ICML)."
REFERENCES,0.26368876080691644,"Liu, M. and Yang, T. (2017). On Noisy Negative Curvature Descent: Competing with Gradient
Descent for Faster Non-convex Optimization. ArXiv:1709.08571v2."
REFERENCES,0.26512968299711814,"Łojasiewicz, S. (1963). A topological property of real analytic subsets. Coll. du CNRS, Les equations
aux derivees partielles, page 87–89."
REFERENCES,0.2665706051873199,"Lu, S., Tsaknakis, I., Hong, M., and Chen, Y. (2020). Hybrid block successive approximation for
one-sided non-convex min-max problems: algorithms and applications. IEEE Transactions on
Signal Processing."
REFERENCES,0.2680115273775216,"Luo, L. and Chen, C. (2021). Finding second-order stationary point for nonconvex-strongly-concave
minimax problem. arXiv:2110.04814."
REFERENCES,0.26945244956772335,"Mokhtari, A., Ozdaglar, A., and Pattathil, S. (2020). A uniﬁed analysis of extra-gradient and optimistic
gradient methods for saddle point problems: Proximal point approach. In Proc. International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 1497–1507."
REFERENCES,0.27089337175792505,"Nedi´c, A. and Ozdaglar, A. (2009). Subgradient methods for saddle-point problems. Journal of
optimization theory and applications, 142(1):205–228."
REFERENCES,0.2723342939481268,"Nesterov, Y. (2008). Accelerating the cubic regularization of newton’s method on convex problems.
Mathematical Programming, 112(1):159–181."
REFERENCES,0.2737752161383285,"Nesterov, Y. and Polyak, B. (2006). Cubic regularization of newton’s method and its global perfor-
mance. Mathematical Programming."
REFERENCES,0.27521613832853026,"Neumann, J. v. (1928). Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295–320."
REFERENCES,0.276657060518732,"Nouiehed, M., Sanjabi, M., Huang, T., Lee, J. D., and Razaviyayn, M. (2019). Solving a class of
non-convex min-max games using iterative ﬁrst order methods. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pages 14934–14942."
REFERENCES,0.2780979827089337,Under review as a conference paper at ICLR 2022
REFERENCES,0.27953890489913547,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,
S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high-
performance deep learning library. In Proc. Advances in Neural Information Processing Systems
(Neurips), pages 8024–8035."
REFERENCES,0.28097982708933716,"Qiu, S., Yang, Z., Wei, X., Ye, J., and Wang, Z. (2020). Single-timescale stochastic nonconvex-
concave optimization for smooth nonlinear td learning. ArXiv:2008.10103."
REFERENCES,0.2824207492795389,"Robinson, J. (1951). An iterative method of solving a game. Annals of mathematics, 54(2):296–301."
REFERENCES,0.2838616714697406,"Sinha, A., Namkoong, H., and Duchi, J. C. (2017). Certifying some distributional robustness with
principled adversarial training. In Proc. International Conference on Learning Representations
(ICLR)."
REFERENCES,0.28530259365994237,"Song, J., Ren, H., Sadigh, D., and Ermon, S. (2018). Multi-agent generative adversarial imitation
learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages 7461–
7472."
REFERENCES,0.28674351585014407,"Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018). Sample Complexity of Stochastic Variance-
Reduced Cubic Regularization for Nonconvex Optimization. ArXiv:1802.07372v1."
REFERENCES,0.2881844380403458,"Xie, G., Luo, L., Lian, Y., and Zhang, Z. (2020). Lower complexity bounds for ﬁnite-sum convex-
concave minimax optimization problems. In Proc. International Conference on Machine Learning
(ICML)."
REFERENCES,0.2896253602305475,"Xu, P., Roosta, F., and Mahoney, M. W. (2020a). Newton-type methods for non-convex optimization
under inexact hessian information. Mathematical Programming, 184(1):35–70."
REFERENCES,0.2910662824207493,"Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020b). Enhanced ﬁrst and zeroth order variance
reduced algorithms for min-max optimization. ArXiv:2006.09361."
REFERENCES,0.29250720461095103,"Xu, T., Wang, Z., Liang, Y., and Poor, H. V. (2020c). Gradient free minimax optimization: Variance
reduction and faster convergence. ArXiv:2006.09361."
REFERENCES,0.29394812680115273,"Xu, Z., Zhang, H., Xu, Y., and Lan, G. (2020d). A uniﬁed single-loop alternating gradient projection
algorithm for nonconvex-concave and convex-nonconcave minimax problems. ArXiv:2006.02032."
REFERENCES,0.2953890489913545,"Yang, J., Kiyavash, N., and He, N. (2020). Global convergence and variance reduction for a class of
nonconvex-nonconcave minimax problems. In Proc. Advances in Neural Information Processing
Systems (NeurIPS)."
REFERENCES,0.2968299711815562,"Yue, M.-C., Zhou, Z., and Man-Cho So, A. (2019). On the quadratic convergence of the cubic
regularization method under a local error bound condition. SIAM Journal on Optimization,
29(1):904–932."
REFERENCES,0.29827089337175794,"Zhang, G. and Wang, Y. (2021). On the suboptimality of negative momentum for minimax optimiza-
tion. In Proc. International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages
2098–2106."
REFERENCES,0.29971181556195964,"Zhou, Y. and Liang, Y. (2017). Characterization of Gradient Dominance and Regularity Conditions
for Neural Networks. In Neurips Optimization for Machine Learning Workshop."
REFERENCES,0.3011527377521614,"Zhou, Y. and Liang, Y. (2018). Critical points of linear neural networks: Analytical forms and
landscape properties. In Proc. International Conference on Learning Representations (ICLR)."
REFERENCES,0.3025936599423631,"Zhou, Y., Wang, Z., and Liang, Y. (2018). Convergence of cubic regularization for nonconvex
optimization under kl property. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pages 3760–3769."
REFERENCES,0.30403458213256485,"Zhou, Y., Zhang, H., and Liang, Y. (2016). Geometrical properties and accelerated gradient solvers
of non-convex phase retrieval. Proc. Annual Allerton Conference on Communication, Control, and
Computing (Allerton), pages 331–335."
REFERENCES,0.30547550432276654,Under review as a conference paper at ICLR 2022
REFERENCES,0.3069164265129683,Appendix
REFERENCES,0.30835734870317005,Table of Contents
REFERENCES,0.30979827089337175,"A Proof of Proposition 1
13"
REFERENCES,0.3112391930835735,"B
Proof of Proposition 2
14"
REFERENCES,0.3126801152737752,"C Proof of Theorem 1
16"
REFERENCES,0.31412103746397696,"D Proof of Theorem 2
17"
REFERENCES,0.31556195965417866,"E
Proof of Theorem 3
18"
REFERENCES,0.3170028818443804,"F
Proof of Theorem 4
20"
REFERENCES,0.3184438040345821,"G Proof of Theorem 5
22"
REFERENCES,0.31988472622478387,"In this supplementary material, we present the proof of all the results claimed in the paper. We ﬁrst
prove the following auxiliary lemma that bounds the spectral norm of the Jacobian matrices.
Lemma 1. Let Assumption 1 hold. Then, for any x ∈Rm and y ∈Rn, the Jacobian matrices of
f(x, y) satisfy the following bounds."
REFERENCES,0.32132564841498557,"∥[∇22f(x, y)]−1∥≤µ−1,
(12)
∥∇12f(x, y)∥= ∥∇21f(x, y)∥≤L1.
(13)"
REFERENCES,0.3227665706051873,"Proof. We ﬁrst prove eq. (12). Consider any x ∈Rm and y ∈Rn. By Assumption 1 we know that
f(x, ·) is µ-strongy concave, which implies that −∇22f(x, y) ⪰µI. Thus, we further conclude that"
REFERENCES,0.3242074927953891,"∥[∇22f(x, y)]−1∥= λmax
 
[−∇22f(x, y)]−1
=

λmin
 
−∇22f(x, y)
−1
≤µ−1."
REFERENCES,0.3256484149855908,"Next, we prove eq. (13). Consider any x, u ∈Rm and y ∈Rn, we have"
REFERENCES,0.3270893371757925,"∥∇21f(x, y)u∥=
 ∂"
REFERENCES,0.3285302593659942,"∂t∇2f(x + tu, y)

t=0 "
REFERENCES,0.329971181556196,"=
 lim
t→0
1"
REFERENCES,0.3314121037463977,"t

∇2f(x + tu, y) −∇2f(x, y)
"
REFERENCES,0.33285302593659943,"= lim
t→0
1
|t|"
REFERENCES,0.33429394812680113,"∇2f(x + tu, y) −∇2f(x, y)"
REFERENCES,0.3357348703170029,"≤lim
t→0
L1 |t|"
REFERENCES,0.3371757925072046,"tu
 = L1∥u∥,
(14)"
REFERENCES,0.33861671469740634,"which implies that ∥∇21f(x, y)∥≤L1. Since f is twice differentiable and has continuous second-
order derivative, we have ∇12f(x, y)⊤= ∇21f(x, y), and hence eq. (13) follows."
REFERENCES,0.3400576368876081,"A
PROOF OF PROPOSITION 1"
REFERENCES,0.3414985590778098,"Proposition 1. Let Assumption 1 hold. Then, the following statements hold."
REFERENCES,0.34293948126801155,1. Mapping y∗(x) is κ-Lipschitz continuous;
REFERENCES,0.34438040345821325,"2. Function Φ(x) is L1(1 + κ)-smooth and ∇Φ(x) = ∇1f(x, y∗(x));"
REFERENCES,0.345821325648415,"3. Deﬁne G(x, y) = ∇11f(x, y) −∇12f(x, y)[∇22f(x, y)]−1∇21f(x, y). Then, G is a Lipschitz
mapping with constant LG = L2(1 + κ)2, i.e., ∥G(x′, y′) −G(x, y)∥≤LG∥(x′, y′) −(x, y)∥;"
REFERENCES,0.3472622478386167,Under review as a conference paper at ICLR 2022
REFERENCES,0.34870317002881845,"4. The Hessian of Φ satisﬁes ∇2Φ(x) = G(x, y∗(x)), which is Lipschitz continuous with constant
LΦ = LG(1 + κ) = L2(1 + κ)3."
REFERENCES,0.35014409221902015,Proof. The items 1 & 2 are proved in Chen et al. (2021); Lin et al. (2020).
REFERENCES,0.3515850144092219,"We ﬁrst prove the item 3. Consider any x, x′ ∈Rm and y, y′ ∈Rn. For convenience we denote
z = (x, y) and z′ = (x′, y′). Then, by Assumption 1 and using the bounds of Lemma 1, we have that"
REFERENCES,0.3530259365994236,"∥G(x′, y′) −G(x, y)∥"
REFERENCES,0.35446685878962536,"≤∥∇11f(x′, y′) −∇11f(x, y)∥+ ∥∇12f(x′, y′) −∇12f(x, y)∥∥[∇22f(x′, y′)]−1∥∥∇21f(x′, y′)∥"
REFERENCES,0.3559077809798271,"+ ∥∇12f(x, y)∥∥[∇22f(x′, y′)]−1 −[∇22f(x, y)]−1∥∥∇21f(x′, y′)∥"
REFERENCES,0.3573487031700288,"+ ∥∇12f(x, y)∥∥[∇22f(x, y)−1]∥∥∇21f(x′, y′) −∇21f(x, y)∥"
REFERENCES,0.35878962536023057,"≤L2∥z′ −z∥+ (L2∥z′ −z∥)µ−1L1
+ L2
1∥[∇22f(x′, y′)]−1∥∥∇22f(x, y) −∇22f(x′, y′)∥∥[∇22f(x, y)]−1∥+ L1µ−1(L2∥z′ −z∥)"
REFERENCES,0.36023054755043227,"≤L2(1 + 2κ)∥z′ −z∥+ L2
1µ−1(L2∥z′ −z∥)µ−1"
REFERENCES,0.361671469740634,≤L2(1 + κ)2∥z′ −z∥.
REFERENCES,0.3631123919308357,"Next, we prove the item 4. Consider any ﬁxed x ∈Rm, we know that f(x, ·) achieves its maximum
at y∗(x), where the gradient vanishes, i.e., ∇2f(x, y∗(x)) = 0. Thus, we further obtain that"
REFERENCES,0.3645533141210375,"0 = ∇x∇2f(x, y∗(x)) = ∇21f(x, y∗(x)) + ∇22f(x, y∗(x))∇y∗(x),"
REFERENCES,0.3659942363112392,which implies that
REFERENCES,0.36743515850144093,"∇y∗(x) = −[∇22f(x, y∗(x))]−1∇21f(x, y∗(x)).
(15)"
REFERENCES,0.3688760806916426,"With the above equation, we take derivative of ∇Φ(x) = ∇1f(x, y∗(x)) and obtain that"
REFERENCES,0.3703170028818444,"∇2Φ(x) =∇11f(x, y∗(x)) + ∇12f(x, y∗(x))∇y∗(x)"
REFERENCES,0.37175792507204614,"=∇11f(x, y∗(x)) −∇12f(x, y∗(x))[∇22f(x, y∗(x))]−1∇21f(x, y∗(x))
(16)
=G(x, y∗(x))."
REFERENCES,0.37319884726224783,"Moreover, we have that"
REFERENCES,0.3746397694524496,"∥∇2Φ(x′) −∇2Φ(x)∥=∥G(x′, y∗(x′)) −G(x, y∗(x))∥"
REFERENCES,0.3760806916426513,"≤LG

∥x′ −x∥+ ∥y∗(x′) −y∗(x)∥
"
REFERENCES,0.37752161383285304,"≤LG(1 + κ)∥x′ −x∥,
(17)"
REFERENCES,0.37896253602305474,where the last step uses the item 1. This proves the item 4.
REFERENCES,0.3804034582132565,"B
PROOF OF PROPOSITION 2"
REFERENCES,0.3818443804034582,Proposition 2. Let Assumption 1 hold. Deﬁne the following potential function
REFERENCES,0.38328530259365995,"H(x, x′, y) := Φ(x) + L2κ3∥x′ −x∥3 + 4L2∥y −y∗(x)∥3,"
REFERENCES,0.38472622478386165,"and denote Ht := H(xt, xt−1, yt+1). Choose Nt ≥max(ln 2, ln[L1∥∇2f(xt,yt)∥/(L2µ)]−2 ln ∥xt−xt−1∥)"
REFERENCES,0.3861671469740634,"ln[κ/(κ−1)]
and learning rates ηx ≤
1
28L2κ3 , ηy ≤
2
L+µ. Then, the sequences {xt, yt}t generated by Cubic-GDA
satisfy, for all t = 0, 1, 2, ..."
REFERENCES,0.38760806916426516,"Ht+1 ≤Ht−L2κ3∥xt+1 −xt∥3−L2∥yt+1 −y∗(xt)∥3.
(2)"
REFERENCES,0.38904899135446686,"Consequently, it holds that"
REFERENCES,0.3904899135446686,"lim
t→∞∥xt+1 −xt∥= 0,
lim
t→∞∥yt+1 −yt∥= 0,
lim
t→∞∥yt −y∗(xt)∥= 0."
REFERENCES,0.3919308357348703,Under review as a conference paper at ICLR 2022
REFERENCES,0.39337175792507206,"Proof. We ﬁrst bound the term ∥yt+1 −y∗(xt)∥, which corresponds to the optimality gap of the
maximization problem. Note that y∗(xt) ∈Rn is the unique maximizer of the strongly concave
function f(xt, y). Note that yt+1 is obtained by applying Nt gradient ascent steps starting from yt.
Hence, By the convergence rate of gradient ascent algorithm under strong concavity, we conclude
that with learning rate ηy =
2
L+µ,"
REFERENCES,0.39481268011527376,"∥yt+1 −y∗(xt)∥≤(1 −κ−1)Nt∥yt −y∗(xt)∥
(18) ≤L2 L1"
REFERENCES,0.3962536023054755,"∥xt −xt−1∥2,
(19)"
REFERENCES,0.3976945244956772,where the second inequality uses the hyperparameter choice
REFERENCES,0.39913544668587897,"Nt ≥ln[L1∥∇2f(xt, yt)∥/(L2µ)] −2 ln ∥xt −xt−1∥"
REFERENCES,0.40057636887608067,"ln[κ/(κ −1)]
≥ln[L1∥yt −y∗(xt)∥/L2] −2 ln ∥xt −xt−1∥"
REFERENCES,0.4020172910662824,"ln[κ/(κ −1)]
,"
REFERENCES,0.4034582132564842,"as ∥∇2f(xt, yt)∥= ∥∇2f(xt, yt) −∇2f(xt, y∗(xt))∥≥µ∥yt −y∗(xt)∥. Moreover, eq. (18) and
the hyperparameter choice that Nt ≥
ln 2
ln[κ/(κ−1)] imply that"
REFERENCES,0.4048991354466859,∥yt+2 −y∗(xt+1)∥3 ≤(1 −κ−1)3Nt∥yt+1 −y∗(xt+1)∥3
REFERENCES,0.40634005763688763,"(i)
≤
1"
REFERENCES,0.40778097982708933,2a + 1
B,0.4092219020172911,"2b

3"
B,0.4106628242074928,"(ii)
≤1"
B,0.41210374639769454,2∥yt+1 −y∗(xt)∥3 + κ3
B,0.41354466858789624,"2 ∥xt+1 −xt∥3.
(20)"
B,0.414985590778098,"where (i) denotes that a := yt+1 −y∗(xt), b := y∗(xt) −y∗(xt+1), (ii) applies Jensen’s inequality
to the convex function ∥· ∥3."
B,0.4164265129682997,"Based on Theorem 10 and Proposition 1 of Nesterov and Polyak (2006), we know that"
B,0.41786743515850144,"λmin

G(xt, yt+1)

≥−1"
B,0.41930835734870314,"2ηx
∥xt+1 −xt∥.
(21)"
B,0.4207492795389049,Since xt+1 minimizes the following cubic sub-problem
B,0.42219020172910665,"gyt+1(x) := ∇1f(xt, yt+1)⊤(x −xt) + 1"
B,0.42363112391930835,"2(x −xt)⊤G(xt, yt+1)(x −xt) +
1
6ηx
∥x −xt∥3,"
B,0.4250720461095101,we obtain the following optimality condition
B,0.4265129682997118,"∇1f(xt, yt+1) + G(xt, yt+1)(xt+1 −xt) +
1
2ηx
∥xt+1 −xt∥(xt+1 −xt) = 0.
(22)"
B,0.42795389048991356,"Next, by the Lipschitz Hessian of Φ, we obtain that"
B,0.42939481268011526,Φ(xt+1) −Φ(xt)
B,0.430835734870317,"≤
 
∇Φ(xt) −∇1f(xt, yt+1)
⊤(xt+1 −xt) + 1"
B,0.4322766570605187,"2(xt+1 −xt)⊤ 
∇2Φ(xt) −G(xt, yt+1)

(xt+1 −xt)"
B,0.43371757925072046,"+ ∇1f(xt, yt+1)⊤(xt+1 −xt) + 1"
B,0.43515850144092216,"2(xt+1 −xt)⊤G(xt, yt+1)(xt+1 −xt) + LΦ"
B,0.4365994236311239,6 ∥xt+1 −xt∥3
B,0.43804034582132567,"(i)
≤L1∥yt+1 −y∗(xt)∥∥xt+1 −xt∥+ LG"
B,0.43948126801152737,2 ∥xt+1 −xt∥2∥yt+1 −y∗(xt)∥ −1
B,0.4409221902017291,"2(xt+1 −xt)⊤G(xt, yt+1)(xt+1 −xt) +
LΦ"
B,0.4423631123919308,"6 −
1
2ηx"
B,0.4438040345821326,"
∥xt+1 −xt∥3"
B,0.4452449567723343,"(ii)
≤
2L3/2
1
3L1/2
2
κ3/2 ∥yt+1 −y∗(xt)∥3/2 + L2κ3"
B,0.44668587896253603,"3
∥xt+1 −xt∥3 + LG 2 h2κ"
B,0.44812680115273773,"3 ∥xt+1 −xt∥3 +
1
3κ2 ∥yt+1 −y∗(xt)∥3i"
B,0.4495677233429395,"+
1
4ηx
∥xt+1 −xt∥3 +
LΦ"
B,0.4510086455331412,"6 −
1
2ηx"
B,0.45244956772334294,"
∥xt+1 −xt∥3
(23)"
B,0.4538904899135447,Under review as a conference paper at ICLR 2022
B,0.4553314121037464,"(iii)
≤
2L2
3κ3/2 ∥xt −xt−1∥3 + 2L2"
B,0.45677233429394815,3 ∥yt+1 −y∗(xt)∥3 +
B,0.45821325648414984,"
3L2κ3 −
1
4ηx "
B,0.4596541786743516,"∥xt+1 −xt∥3,
(24)"
B,0.4610951008645533,"where (i) uses Assumption 1, the item 4 of Proposition 1 and eq. (22), (ii) uses eq. (21) and
the inequality that ab = ((Ca)3/2(Ca)3/2(b/C)3)1/3 ≤2"
B,0.46253602305475505,"3(Ca)3/2 +
b3
3C3 for any a, b ≥0 and
C > 0 (based on AM-GM inequality), (iii) uses eq. (19) and LG = L2(1 + κ)2 ≤4L2κ2,
LΦ = L2(1 + κ)3 ≤8L2κ3."
B,0.46397694524495675,Multiplying eq. (20) with 4L2 and adding it to eq. (24) yield that
B,0.4654178674351585,Φ(xt+1) −Φ(xt) + 4L2∥yt+2 −y∗(xt+1)∥3 ≤2L2
B,0.4668587896253602,3κ3/2 ∥xt −xt−1∥3 + 8L2
B,0.46829971181556196,3 ∥yt+1 −y∗(xt)∥3 +
B,0.4697406340057637,"
5L2κ3 −
1
4ηx "
B,0.4711815561959654,"∥xt+1 −xt∥3,"
B,0.47262247838616717,"(i)
≤L2κ3∥xt −xt−1∥3 + 3L2∥yt+1 −y∗(xt)∥3−2L2κ3∥xt+1 −xt∥3,"
B,0.47406340057636887,"where (i) uses the condition that ηx ≤
1
28L2κ3 . The above inequality implies eq. (2) by deﬁning
Ht := Φ(xt) + L2κ3∥xt −xt−1∥3 + 4L2∥yt+1 −y∗(xt)∥3."
B,0.4755043227665706,"Next, summing eq. (2) over t = 0, 1, ..., T −1, we obtain that for all T ≥1, L2κ3"
B,0.4769452449567723,"T −1
X"
B,0.4783861671469741,"t=0
∥xt+1 −xt∥3 + L2"
B,0.47982708933717577,"T −1
X"
B,0.4812680115273775,"t=0
∥yt+1 −y∗(xt)∥3"
B,0.4827089337175792,"≤H0 −HT ≤H0 −Φ(xT ) ≤H0 −inf
x∈Rm Φ(x) < +∞.
(25)"
B,0.484149855907781,"Letting T →∞yields that P∞
t=0 ∥xt+1 −xt∥3 < +∞and P∞
t=0 ∥yt+1 −y∗(xt)∥3 < +∞, so
limt→∞∥xt+1 −xt∥= limt→∞∥yt+1 −y∗(xt)∥= 0. Hence, ∥yt −y∗(xt)∥≤∥yt −y∗(xt−1)∥+
∥y∗(xt) −y∗(xt−1)∥
t→0 using continuity of y∗, and ∥yt+1 −yt∥≤∥yt+1 −y∗(xt)∥+ ∥yt −
y∗(xt)∥
t→0."
B,0.48559077809798273,"C
PROOF OF THEOREM 1"
B,0.48703170028818443,"Theorem 1 (Global convergence). Under the same conditions as those of Proposition 2, the Cubic-
GDA has the following global convergence properties."
B,0.4884726224783862,1. The function value sequence {Φ(xt)}t converges to a ﬁnite limit H∗> −∞;
B,0.4899135446685879,"2. The generated sequences {xt}t, {yt}t are bounded and have a compact sets of limit points."
B,0.49135446685878964,"3. Every limit point x∗of {xt}t is a second-order stationary point of Φ, i.e., ∇Φ(x∗) = 0,
∇2Φ(x∗) ⪰0, and satisﬁes Φ(x∗) = H∗."
B,0.49279538904899134,"Proof. We ﬁrst prove item 1. We have shown in Proposition 2 that {Ht}t is monotonically decreasing.
Also, Assumption 1 says that Ht ≥Φ(xt) ≥infx∈Rm Φ(x) > −∞. Therefore, we conclude that
{Ht}t converges to a ﬁnite limit H∗> −∞. Since Proposition 2 proves that ∥xt −xt−1∥, ∥yt+1 −
y∗(xt)∥
t→0, we further conclude that Φ(xt)
t→H∗, which proves item 1."
B,0.4942363112391931,"Next, we prove item 2. Note that Φ has compact sub-level sets. Moreover, for all t, we have"
B,0.4956772334293948,"Φ(xt) ≤Ht ≤H0 < +∞.
(26)"
B,0.49711815561959655,"Hence, the sequence {xt}t is bounded and thus has a compact set of limit points. Since y∗is Lipschitz
continuous by Proposition 1, {∥y∗(xt)∥}t is also bounded. Consequently, as ∥yt −y∗(xt)∥→0, we
conclude that {yt}t is also bounded and hence has a compact set of limit points. This proves item 2."
B,0.49855907780979825,"Next, we will prove item 3. Suppose that xtk
k→x∗along a certain sub-sequence {xtk}k. We have
that ytk+1
t→y∗(x∗) based on Proposition 2 and the continuity of y∗. In addition, Proposition 1
implies that Φ, ∇Φ, ∇2Φ and G are continuous. Therefore, for every limit point x∗,"
B,0.5,"Φ(x∗) = lim
tk→∞Φ(xtk) = H∗."
B,0.5014409221902018,Under review as a conference paper at ICLR 2022
B,0.5028818443804035,"∥∇Φ(x∗)∥= lim
tk→∞∥∇Φ(xtk)∥= lim
tk→∞∥∇f(xtk, y∗(xtk))∥= lim
tk→∞∥∇f(xtk, ytk+1)∥"
B,0.5043227665706052,"(i)
= lim
tk→∞"
B,0.5057636887608069,"G(xtk, ytk+1)(xtk+1 −xtk) +
1
2ηx
∥xtk+1 −xtk∥(xtk+1 −xtk)
 = 0"
B,0.5072046109510087,"λmin

∇2Φ(x∗)

= lim
tk→∞λmin

∇2Φ(xtk)

= lim
tk→∞λmin

G(xtk, y∗(xtk))
"
B,0.5086455331412104,"= lim
tk→∞λmin

G(xtk, ytk+1)
 (ii)
≥−1"
B,0.5100864553314121,"2ηx
lim
tk→∞∥xtk+1 −xtk∥= 0"
B,0.5115273775216138,"where (i) uses eq. (22), and (ii) uses eq. (21)."
B,0.5129682997118156,"D
PROOF OF THEOREM 2"
B,0.5144092219020173,"Theorem 2 (Global convergence rate). Under the same conditions as those of Proposition 2, the
Cubic-GDA converges at the following rate for all T ≥H0−infx∈Rm Φ(x)"
B,0.515850144092219,"L2κ3/3
."
B,0.5172910662824207,"min
0≤t≤T −1 µ(xt) ≤
H0 −infx∈Rm Φ(x)"
B,0.5187319884726225,TL2κ3/3
B,0.5201729106628242,"1/3
."
B,0.521613832853026,"Before proving Theorem 2, we ﬁrst prove the following auxiliary lemma.
Lemma 2. For any symmetric matrices A, B ∈Rn×n, we have |λmin(A) −λmin(B)| ≤∥A −B∥."
B,0.5230547550432276,Proof of Lemma 2.
B,0.5244956772334294,"|λmin(A) −λmin(B)| ≤

min
u:∥u∥=1 u⊤Au −
min
u:∥u∥=1 u⊤Bu"
B,0.5259365994236311,"≤
max
u:∥u∥=1"
B,0.5273775216138329,u⊤Au −u⊤Bu
B,0.5288184438040345,"≤
max
u:∥u∥=1"
B,0.5302593659942363," 
∥u∥∥A −B∥∥u∥

= ∥A −B∥."
B,0.531700288184438,Proof of Theorem 2. Note that (25) implies that
B,0.5331412103746398,"min
2≤t≤T −1
 
∥xt+1 −xt∥3 + ∥xt −xt−1∥3∥+ ∥xt−1 −xt−2∥3
≤H0 −infx∈Rm Φ(x)"
B,0.5345821325648416,"TL2κ3/3
,"
B,0.5360230547550432,which further implies that there exists 1 ≤t′ ≤T −1 such that
B,0.537463976945245,"max
 
∥xt′+1 −xt′∥, ∥xt′ −xt′−1∥, ∥xt′−1 −xt′−2∥

≤
H0 −infx∈Rm Φ(x)"
B,0.5389048991354467,TL2κ3/3
B,0.5403458213256485,"1/3
.
(27)"
B,0.5417867435158501,"On the other hand, equation (2.2) of Nesterov and Polyak (2006) implies that,"
B,0.5432276657060519,∥∇Φ(xt′) −∇Φ(xt′−1) −∇2Φ(xt′−1)(xt′ −xt′−1)∥≤LΦ
B,0.5446685878962536,2 ∥xt′ −xt′−1∥2.
B,0.5461095100864554,"Since ∇Φ(x) = ∇1f(x, y∗(x)), ∇2Φ(x) = G(x, y∗(x)), the above inequality implies that"
B,0.547550432276657,∥∇Φ(xt′)∥
B,0.5489913544668588,"≤∥∇1f(xt′−1, y∗(xt′−1)) + G(xt′−1, y∗(xt′−1))(xt′ −xt′−1)∥+ LΦ"
B,0.5504322766570605,2 ∥xt′ −xt′−1∥2
B,0.5518731988472623,"≤∥∇1f(xt′−1, yt′) + G(xt′−1, yt′)(xt′ −xt′−1)∥+ ∥∇1f1(xt′−1, y∗(xt′−1)) −∇1f(xt′−1, yt′)∥"
B,0.553314121037464,"+ ∥
 
G(xt′−1, y∗(xt′−1)) −G(xt′−1, yt′)

(xt′ −xt′−1)∥+ LΦ"
B,0.5547550432276657,2 ∥xt′ −xt′−1∥2
B,0.5561959654178674,"(i)
≤
1
2ηx
∥xt′ −xt′−1∥2 + L1∥yt′ −y∗(xt′−1)∥+ LG∥yt′ −y∗(xt′−1)∥∥xt′ −xt′−1∥ + LΦ"
B,0.5576368876080692,2 ∥xt′ −xt′−1∥2
B,0.5590778097982709,Under review as a conference paper at ICLR 2022
B,0.5605187319884726,"(ii)
≤
 1"
B,0.5619596541786743,"2ηx
+ 4L2κ3
∥xt′ −xt′−1∥2 + L2∥xt′−1 −xt′−2∥2 + 4L2
2κ2 L1"
B,0.5634005763688761,∥xt′ −xt′−1∥2∥xt′+1 −xt′∥
B,0.5648414985590778,"(iii)
≤
 1"
B,0.5662824207492796,"2ηx
+ 5L2κ3H0 −infx∈Rm Φ(x)"
B,0.5677233429394812,TL2κ3/3
B,0.569164265129683,"2/3
+ 4L2
2κ2 L1"
B,0.5706051873198847,H0 −infx∈Rm Φ(x)
B,0.5720461095100865,TL2κ3/3 
B,0.5734870317002881,"(iv)
≤
 1"
B,0.5749279538904899,"2ηx
+ 5L2κ3 + 4L2
2κ2 L1"
B,0.5763688760806917,H0 −infx∈Rm Φ(x)
B,0.5778097982708934,TL2κ3/3
B,0.579250720461095,"2/3
,
(28)"
B,0.5806916426512968,"where (i) uses eq. (22), item 1 of Assumption 1 and item 3 of Proposition 1, (ii) uses eq. (19)
and LG = L2(1 + κ)2 ≤4L2κ2, LΦ = L2(1 + κ)3 ≤8L2κ3, (iii) uses eq. (27), and (iv) uses
T ≥H0−infx∈Rm Φ(x)"
B,0.5821325648414986,"L2κ3/3
. Also, note that"
B,0.5835734870317003,"−λmin
 
∇2Φ(xt′)
"
B,0.5850144092219021,"(i)
≤−λmin
 
G(xt′, yt′+1)

+ ∥G(xt′, y∗(xt′)) −G(xt′, yt′+1)∥"
B,0.5864553314121037,"(ii)
≤
1
2ηx
∥xt′+1 −xt′∥+ LG∥yt′+1 −y∗(xt′)∥"
B,0.5878962536023055,"(iii)
≤
1
2ηx
∥xt′+1 −xt′∥+ LGL2 L1"
B,0.5893371757925072,∥xt′ −xt′−1∥2
B,0.590778097982709,"(iv)
≤
1
2ηx"
B,0.5922190201729106,H0 −infx∈Rm Φ(x)
B,0.5936599423631124,TL2κ3/3
B,0.5951008645533141,"1/3
+ LGL2 L1"
B,0.5965417867435159,H0 −infx∈Rm Φ(x)
B,0.5979827089337176,TL2κ3/3 2/3
B,0.5994236311239193,"(v)
≤
 1"
B,0.600864553314121,"2ηx
+ 4L2
2κ2 L1"
B,0.6023054755043228,H0 −infx∈Rm Φ(x)
B,0.6037463976945245,TL2κ3/3
B,0.6051873198847262,"1/3
,
(29)"
B,0.6066282420749279,"where (i) uses Lemma 2, (ii) uses eq. (21) and item 3 of Proposition 1, (iii) uses eq. (19), (iv) uses
eq. (27), and (v) uses T ≥H0−infx∈Rm Φ(x)"
B,0.6080691642651297,"L2κ3/3
and LG = L2(1 + κ)2 ≤4L2κ2. Equations (28) & (29)
imply Theorem 2."
B,0.6095100864553314,"E
PROOF OF THEOREM 3"
B,0.6109510086455331,"Theorem 3. Let Assumption 1 hold and assume that the potential function H satisﬁes the local
Łojasiewicz gradient geometry. Choose the hyperparameters Nt, ηx, ηy in the same way as Proposi-
tion 2. Then, the sequences {(xt, yt)}t generated by Cubic-GDA have a unique limit point, which is
a second-order stationary point of Φ."
B,0.6123919308357348,"Proof. We ﬁrst derive a bound on ∥∇H(x, x′, y)∥:=
qP3
k=1 ∥∇kH(x, x′, y)∥2 as follows."
B,0.6138328530259366,"∥∇H(xt, xt−1, yt+1)∥
≤∥∇1H(xt, xt−1, yt+1)∥+ ∥∇2H(xt, xt−1, yt+1)∥+ ∥∇3H(xt, xt−1, yt+1)∥"
B,0.6152737752161384,"=
∇Φ(xt) + 3L2κ3∥xt −xt−1∥(xt −xt−1) + 12L2∥yt+1 −y∗(xt)∥∇y∗(xt)⊤
y∗(xt) −yt+1
"
B,0.6167146974063401,"+
3L2κ3∥xt −xt−1∥(xt −xt−1)
 +
12L2∥yt+1 −y∗(xt)∥

yt+1 −y∗(xt)
"
B,0.6181556195965417,"(i)
≤∥∇Φ(xt−1) + ∇2Φ(xt−1)(xt −xt−1)∥+
LΦ"
B,0.6195965417867435,"2 + 6L2κ3
∥xt −xt−1∥2"
B,0.6210374639769453,"+ 12L2∥yt+1 −y∗(xt)∥2
1 + ∥∇22f(x, y∗(x))−1∥∥∇21f(x, y∗(x))∥
"
B,0.622478386167147,"(ii)
≤∥∇Φ(xt−1) + G(xt−1, yt)(xt −xt−1)∥+ ∥G(xt−1, y∗(xt−1)) −G(xt−1, yt)∥∥xt −xt−1∥ +
LΦ"
B,0.6239193083573487,"2 + 6L2κ3
∥xt −xt−1∥2 + 12L2(1 + µ−1L1)∥yt+1 −y∗(xt)∥2"
B,0.6253602305475504,"(iii)
≤
∇1f(xt−1, y∗(xt−1)) −∇1f(xt−1, yt) −
1
2ηx
∥xt −xt−1∥(xt −xt−1)"
B,0.6268011527377522,Under review as a conference paper at ICLR 2022
B,0.6282420749279539,"+ LG∥yt −y∗(xt−1)∥∥xt −xt−1∥+
LΦ"
B,0.6296829971181557,"2 + 6L2κ3
∥xt −xt−1∥2 + 24L2κ∥yt+1 −y∗(xt)∥2"
B,0.6311239193083573,"(iv)
≤L1∥yt −y∗(xt−1)∥+
LΦ"
B,0.6325648414985591,"2 + 6L2κ3 +
1
2ηx"
B,0.6340057636887608,"
∥xt −xt−1∥2"
B,0.6354466858789626,+ LGL2 L1
B,0.6368876080691642,"∥xt−1 −xt−2∥2∥xt −xt−1∥+ 24L3
2κ
L2
1"
B,0.638328530259366,∥xt −xt−1∥4
B,0.6397694524495677,"(v)
≤L2∥xt−1 −xt−2∥2 +
"
B,0.6412103746397695,"10L2κ3 +
1
2ηx"
B,0.6426512968299711,"
∥xt −xt−1∥2"
B,0.6440922190201729,"+ 4L2
2κ2 L1"
B,0.6455331412103746,"∥xt−1 −xt−2∥2∥xt −xt−1∥+ 24L3
2κ
L2
1"
B,0.6469740634005764,"∥xt −xt−1∥4
(30)"
B,0.6484149855907781,"where (i) uses eq. (2.2) of Nesterov and Polyak (2006), the fact that ∇2Φ is LΦ-Lipschitz, and eq.
(15), (ii) uses Lemma 1 and ∇2Φ(x) = G(x, y∗(x)) from item 4 of Proposition 1, (iii) uses eq. (22),
∇Φ(x) = ∇1f(x, y∗(x)) from item 2 of Proposition 1 and the inequality that 1 + κ ≤2κ, (iv) and
(v) use eq. (19), and (v) uses LG = L2(1 + κ)2 ≤4L2κ2, LΦ = L2(1 + κ)3 ≤8L2κ3."
B,0.6498559077809798,"Next, we prove the convergence of the sequence {xt}t under the assumption that H(x, x′, y) satisﬁes
the Łojasiewicz gradient geometry. Recall that we have shown in the proof of Theorem 1 that: 1)
{Ht}t decreases monotonically to the ﬁnite limit H∗; 2) for any limit point x∗, y∗of {xt}t, {yt}t,
Φ(x∗) = H∗. Hence, the Łojasiewicz gradient inequality (see Deﬁnition 1) holds after sufﬁciently
large number of iterations, i.e., there exists t1 ∈N+ such that for all t ≥t1, H(xt, xt−1, yt+1) −
H∗≤c∥∇H(xt, xt−1, yt+1)∥θ. Equivalently,"
B,0.6512968299711815,"ϕ′(Ht −H∗)∥∇H(xt, xt−1, yt+1)∥≥1."
B,0.6527377521613833,"where we deﬁne the concave function that ϕ(s) :=
c1/θ
1−1/θs1−1/θ(θ > 1, s > 0)."
B,0.654178674351585,"In addition, since ∥xt −xt−1∥
t→0 (Proposition 2), there exists t2 ∈N+ such that for all t ≥t2,
∥xt −xt−1∥≤1. Hence, rearranging the above inequality and utilizing eq. (30), we obtain that for
all t ≥t0 := max(t1, t2),"
B,0.6556195965417867,ϕ′(Ht −H∗)
B,0.6570605187319885,"≥∥∇H(xt, xt−1, yt+1)∥−1"
B,0.6585014409221902,"≥
h4L2
2κ2"
B,0.659942363112392,"L1
+ L2"
B,0.6613832853025937,"
∥xt−1 −xt−2∥2 +
"
B,0.6628242074927954,"10L2κ3 + 24L3
2κ
L2
1"
B,0.6642651296829971,"+
1
2ηx"
B,0.6657060518731989,"
∥xt −xt−1∥2i−1
(31)"
B,0.6671469740634006,"By concavity of the function ϕ(s) :=
c1/θ
1−1/θs1−1/θ(θ > 1, s > 0), we know that"
B,0.6685878962536023,ϕ(Ht −H∗) −ϕ(Ht+1 −H∗)
B,0.670028818443804,≥ϕ′(Ht −H∗)(Ht −Ht+1)
B,0.6714697406340058,"(i)
≥
L2κ3∥xt+1 −xt∥3
"
B,0.6729106628242075,"4L2
2κ2"
B,0.6743515850144092,"L1
+ L2"
B,0.6757925072046109,"
∥xt−1 −xt−2∥2 +
"
B,0.6772334293948127,"10L2κ3 + 24L3
2κ
L2
1
+
1
2ηx"
B,0.6786743515850144,"
∥xt −xt−1∥2
(32)"
B,0.6801152737752162,"(ii)
≥
L2κ3∥xt+1 −xt∥3
q"
B,0.6815561959654178,"4L2
2κ2"
B,0.6829971181556196,"L1
+ L2∥xt−1 −xt−2∥+
q"
B,0.6844380403458213,"10L2κ3 + 24L3
2κ
L2
1
+
1
2ηx ∥xt −xt−1∥
2 ,"
B,0.6858789625360231,"where (i) uses Proposition 2 and eq. (31), (ii) uses the inequality that a2 + b2 ≤(a + b)2 for any
a, b ≥0."
B,0.6873198847262247,Rearranging the above inequality yields that
B,0.6887608069164265,L2κ3∥xt+1 −xt∥3
B,0.6902017291066282,"≤[ϕ(Ht −H∗) −ϕ(Ht+1 −H∗)]
 s"
B,0.69164265129683,"4L2
2κ2"
B,0.6930835734870316,"L1
+ L2∥xt−1 −xt−2∥+ s"
B,0.6945244956772334,"10L2κ3 + 24L3
2κ
L2
1"
B,0.6959654178674352,"+
1
2ηx
∥xt −xt−1∥ !2"
B,0.6974063400576369,Under review as a conference paper at ICLR 2022 ≤1 27 
B,0.6988472622478387,C2[ϕ(Ht −H∗) −ϕ(Ht+1 −H∗)] + 2 C s
B,0.7002881844380403,"4L2
2κ2"
B,0.7017291066282421,"L1
+ L2∥xt−1 −xt−2∥ + 2 C s"
B,0.7031700288184438,"10L2κ3 + 24L3
2κ
L2
1"
B,0.7046109510086456,"+
1
2ηx
∥xt −xt−1∥ !3 (33)"
B,0.7060518731988472,"where the ﬁnal step uses the AM-GM inequality that ab2 =

3p"
B,0.707492795389049,"(C2a)(b/C)(b/C)
3 ≤
1
27(C2a +
2b"
B,0.7089337175792507,"C )3 for any a, b ≥0 and C > 0 (the value of C will be assigned later). Taking cubic root of both
sides of the above inequality and telescoping it over t = t0, . . . , T −1, we obtain that κ
3p L2"
B,0.7103746397694525,"T −1
X"
B,0.7118155619596542,"t=t0
∥xt+1 −xt∥ ≤C2"
B,0.7132564841498559,3 [ϕ(Ht0 −H∗) −ϕ(HT −H∗)] + 2
C,0.7146974063400576,3C s
C,0.7161383285302594,"4L2
2κ2"
C,0.7175792507204611,"L1
+ L2"
C,0.7190201729106628,"T −1
X"
C,0.7204610951008645,"t=t0
∥xt−1 −xt−2∥ + 2"
C,0.7219020172910663,3C s
C,0.723342939481268,"10L2κ3 + 24L3
2κ
L2
1"
C,0.7247838616714697,"+
1
2ηx"
C,0.7262247838616714,"T −1
X"
C,0.7276657060518732,"t=t0
∥xt −xt−1∥ ≤C2"
C,0.729106628242075,3 ϕ(Ht0 −H∗) + 2
C,0.7305475504322767,3C s
C,0.7319884726224783,"4L2
2κ2"
C,0.7334293948126801,"L1
+ L2"
C,0.7348703170028819,"T −1
X"
C,0.7363112391930836,"t=t0
∥xt−1 −xt−2∥ + 2"
C,0.7377521613832853,3C s
C,0.739193083573487,"10L2κ3 + 24L3
2κ
L2
1"
C,0.7406340057636888,"+
1
2ηx"
C,0.7420749279538905,"T −1
X"
C,0.7435158501440923,"t=t0
∥xt −xt−1∥"
C,0.7449567723342939,"where the ﬁnal step uses the facts that Ht −H∗≥0 and that ϕ(s) is monotonically in-
creasing.
Since the value of C > 0 is arbitrary, we can select large enough C such that 2
3C q"
C,0.7463976945244957,"4L2
2κ2"
C,0.7478386167146974,"L1
+ L2,
2
3C
q"
C,0.7492795389048992,"10L2κ3 + 24L3
2κ
L2
1
+
1
2ηx < κ 3√L2"
C,0.7507204610951008,"3
. Hence, the inequality above further im-
plies that"
C,0.7521613832853026,κ 3√L2 3
C,0.7536023054755043,"T −1
X"
C,0.7550432276657061,"t=t0
∥xt+1 −xt∥≤C2"
C,0.7564841498559077,3 ϕ(Ht0 −H∗)
C,0.7579250720461095,+ κ 3√L2
C,0.7593659942363112,"3

∥xt0−1 −xt0−2∥+ 2∥xt0 −xt0−1∥

< +∞.
(34)"
C,0.760806916426513,"Letting T →∞concludes that
∞
X"
C,0.7622478386167147,"t=1
∥xt+1 −xt∥< + ∞."
C,0.7636887608069164,"Moreover, this implies that {xt}t is a Cauchy sequence and therefore converges to a certain limit,
i.e., xt
t→x∗. We have shown in Theorem 1 that any such limit point must be a second-order critical
point of Φ. Hence, we conclude that {xt}t converges to a certain second-order critical point x∗of
Φ(x). Also, note that ∥y∗(xt) −yt∥
t→0, xt
t→x∗and y∗is a Lipschitz mapping, so we conclude
that {yt}t converges to y∗(x∗). Finally, the item 3 of Theorem 1 implies that x∗is a second-order
critical point of Φ(x)."
C,0.7651296829971181,"F
PROOF OF THEOREM 4"
C,0.7665706051873199,"Theorem 4 (Funtion value convergence rate). Under the same conditions as those of Theorem 3, the
sequence of potential function {Ht}t converges to the limit H∗at the following rates."
C,0.7680115273775217,1. If the geometry parameter θ ∈( 3
C,0.7694524495677233,"2, ∞), then Ht ↓H∗super-linearly as"
C,0.770893371757925,"Ht −H∗≤O

exp

−
2θ 3"
C,0.7723342939481268, t−t0
C,0.7737752161383286,"2 
,
∀t ≥t0;
(6)"
C,0.7752161383285303,Under review as a conference paper at ICLR 2022
C,0.776657060518732,2. If the geometry parameter θ = 3
C,0.7780979827089337,"2, then Ht ↓H∗linearly as"
C,0.7795389048991355,"Ht −H∗≤(1 + C3/2
0
)−t−t0"
C,0.7809798270893372,"2 ,
∀t ≥t0;
(7)"
C,0.7824207492795389,"3. If the geometry parameter θ ∈(1, 3"
C,0.7838616714697406,"2), then Ht ↓H∗sub-linearly as"
C,0.7853025936599424,"Ht −H∗≤O

(t −t0)−
2θ
3−2θ

,
∀t ≥t0.
(8)"
C,0.7867435158501441,"Proof. Equation (31) implies that there exists t0 ∈N+ such that for any t ≥t0,"
C,0.7881844380403458,ϕ′(Ht −H∗)−1 = c−1/θ(Ht −H∗)1/θ
C,0.7896253602305475,"≤
4L2
2κ2"
C,0.7910662824207493,"L1
+ L2"
C,0.792507204610951,"
∥xt−1 −xt−2∥2 +
"
C,0.7939481268011528,"10L2κ3 + 24L3
2κ
L2
1"
C,0.7953890489913544,"+
1
2ηx"
C,0.7968299711815562,"
∥xt −xt−1∥2
(35) (i)
≤"
C,0.7982708933717579,"4L2
2
L1
+ L2κ−2Ht−2 −Ht−1 L2"
C,0.7997118155619597,"2/3
+
"
C,0.8011527377521613,"10L2κ + 24L3
2
L2
1κ +
1
2ηxκ2"
C,0.8025936599423631,Ht−1 −Ht L2 2/3
C,0.8040345821325648,where (i) uses proposition 2.
C,0.8054755043227666,"Deﬁning
dt
:=
Ht
−
H∗,
C1
:=
c1/θL−2/3
2

4L2
2
L1 + L2κ−2"
C,0.8069164265129684,",
C2
:="
C,0.80835734870317,"c1/θL−2/3
2

10L2κ + 24L3
2
L2
1κ +
1
2ηxκ2 "
C,0.8097982708933718,",
and C0=
3√"
C,0.8112391930835735,"2c1/θL−2/3
2

10L2κ + 24L3
2
L2
1κ + 4L2
2
L1 +
1
2ηxκ2 "
C,0.8126801152737753,",
the above inequality further becomes"
C,0.8141210374639769,"d1/θ
t
≤C1
 
dt−2 −dt−1
2/3 + C2
 
dt−1 −dt
2/3
(36)"
C,0.8155619596541787,"≤2 max(C1, C2)
h1"
C,0.8170028818443804,"2
 
dt−2 −dt−1
2/3 + 1"
C,0.8184438040345822,"2
 
dt−1 −dt
2/3i"
C,0.8198847262247838,"(i)
≤2 max(C1, C2)
h1"
C,0.8213256484149856,"2(dt−2 −dt)
i2/3 (ii)
≤C0(dt−2 −dt)2/3
(37)"
C,0.8227665706051873,"where (i) applies Jensen’s inequality to the concave function ξ(s) = s2/3, and (ii) uses the inequality
that 21/3 max(C1, C2) ≤C0."
C,0.8242074927953891,"Next, we prove the convergence rates case by case."
C,0.8256484149855908,(Case 1) If θ ∈( 3
C,0.8270893371757925,"2, +∞), since dt ≥0, eq. (36) implies that for t ≥t0,"
C,0.8285302593659942,"dt ≤Cθ
0d2θ/3
t−2 ,"
C,0.829971181556196,which is equivalent to that C
C,0.8314121037463977,"3θ
2θ−3
0
dt ≤

C"
C,0.8328530259365994,"3θ
2θ−3
0
dt−2
2θ/3
(38)"
C,0.8342939481268011,"Since dt ↓0, C"
C,0.8357348703170029,"3θ
2θ−3
0
dt0 ≤e−1 for sufﬁciently large t0 ∈N+. Hence, eq. (38) implies that for any
k ∈N+ C"
C,0.8371757925072046,"3θ
2θ−3
0
d2k+t0 ≤

C"
C,0.8386167146974063,"3θ
2θ−3
0
dt0
[(2θ/3)k] ≤exp
h
−
2θ 3 ki
."
C,0.840057636887608,"Hence,"
C,0.8414985590778098,"d2k+t0+1 ≤d2k+t0 ≤C
−
3θ
2θ−3
0
exp
h
−
2θ 3 ki"
C,0.8429394812680115,Note that θ ∈( 3
C,0.8443804034582133,"2, +∞) implies that 2θ"
C,0.845821325648415,"3 > 1, and thus the inequality above implies that Ht ↓H∗at
the super-linear rate given by eq. (6)."
C,0.8472622478386167,(Case 2) If θ = 3
C,0.8487031700288185,"2, eq. (37) implies that for t ≥t0,"
C,0.8501440922190202,"dt ≤(1 + C3/2
0
)−1dt−2.
(39)"
C,0.8515850144092219,Under review as a conference paper at ICLR 2022
C,0.8530259365994236,"Note that dt0 ≤1 for sufﬁciently large t0. Therefore, dt ↓0 (i.e., H(zt) ↓H∗) at the linear rate
given by eq. (7)."
C,0.8544668587896254,"(Case 3) If θ ∈(1, 3"
C,0.8559077809798271,"2), consider the following two subcases."
C,0.8573487031700289,"If dt−2 ≤2dt, denote ψ(s) =
2θ
3−2θs1−3"
C,0.8587896253602305,"2θ , then for any t ≥t0,"
C,0.8602305475504323,"ψ(dt) −ψ(dt−2) =
Z dt−2"
C,0.861671469740634,"dt
−ψ′(s)ds =
Z dt−2"
C,0.8631123919308358,"dt
s−3"
C,0.8645533141210374,"2θ ds
(i)
≥d
−3"
C,0.8659942363112392,"2θ
t−2 (dt−2 −dt)"
C,0.8674351585014409,"(ii)
≥C
−3"
C,0.8688760806916427,"2
0
 dt dt−2  3"
C,0.8703170028818443,"2θ (iii)
≥(2C0)−3"
C,0.8717579250720461,"2
(40)"
C,0.8731988472622478,"where (i) uses dt ≤dt−2, and −3"
C,0.8746397694524496,"2(1 −θ) < 0, (ii) uses the following inequality implied by eq. (37),
and (iii) uses
dt
dt−2 ≥1"
AND,0.8760806916426513,"2 and
3
2θ ∈
 
1, 3 2

."
AND,0.877521613832853,"If dt−2 > 2dt, then for any t ≥t0,"
AND,0.8789625360230547,"ψ(dt) −ψ(dt−2) =
2θ
3 −2θ(d
1−3"
AND,0.8804034582132565,"2θ
t
−d
1−3"
AND,0.8818443804034583,"2θ
t−2 ) ≥
2θ
3 −2θ

d
1−3"
AND,0.8832853025936599,"2θ
t
−(2dt)1−3 2θ "
AND,0.8847262247838616,"≥2θ
 
1 −21−3 2θ "
AND,0.8861671469740634,"3 −2θ
d
1−3"
AND,0.8876080691642652,"2θ
t
≥(2C0)−3"
AND,0.8890489913544669,"2 .
(41)"
AND,0.8904899135446686,"where we use 1 −
3
2θ ∈
 
−1"
AND,0.8919308357348703,"2, 0

,
2θ
3−2θ > 0 and dt ≤dt0 ≤(2C0)
3θ
3−2θ
h 2θ
 
1−21−3 2θ "
AND,0.8933717579250721,"3−2θ
i
2θ
3−2θ for"
AND,0.8948126801152738,sufﬁciently large t0 ∈N+.
AND,0.8962536023054755,"Since at least one of eqs. (40) & (41) holds, we have ψ(dt) −ψ(dt−2) ≥(2C0)−3"
AND,0.8976945244956772,"2 . Hence,"
AND,0.899135446685879,ψ(d2k+t0+1) ≥ψ(d2k+t0) ≥ψ(dt0) + k(2C0)−3
AND,0.9005763688760807,2 ≥k(2C0)−3
AND,0.9020172910662824,"2 ; k ∈N,"
AND,0.9034582132564841,which implies that ψ(dt) ≥t−t0
AND,0.9048991354466859,2 (2C0)−3
AND,0.9063400576368876,"2 By substituing the deﬁnition of ψ, the inequality above
implies that H(zt) ↓H∗in a sub-linear rate given by eq. (8)."
AND,0.9077809798270894,"G
PROOF OF THEOREM 5"
AND,0.909221902017291,"Theorem 5 (Parameter convergence rate). Under the same conditions as those of Theorem 3, the
sequences {xt, yt}t generated by Cubic-GDA converge to their limits x∗, y∗(x∗) respectively at the
following rates."
AND,0.9106628242074928,1. If the geometry parameter θ ∈( 3
AND,0.9121037463976945,"2, ∞), then (xt, yt) →(x∗, y∗(x∗)) super-linearly as"
AND,0.9135446685878963,"max

∥xt −x∗∥, ∥yt −y∗(x∗)∥
	
≤O

exp

−1 3 2θ 3"
AND,0.9149855907780979, t−t0
AND,0.9164265129682997,"2
−1
,
∀t ≥t0;
(9)"
AND,0.9178674351585014,2. If the geometry parameter θ = 3
AND,0.9193083573487032,"2, then (xt, yt) →(x∗, y∗(x∗)) linearly as"
AND,0.920749279538905,"max

∥xt −x∗∥, ∥yt −y∗(x∗)∥
	
≤O

(1 + C3/2
0
)−t−t0"
AND,0.9221902017291066,"2

,
∀t ≥t0;
(10)"
AND,0.9236311239193083,"3. If the geometry parameter θ ∈(1, 3"
AND,0.9250720461095101,"2), then (xt, yt) →(x∗, y∗(x∗)) sub-linearly as"
AND,0.9265129682997119,"∥xt −x∗∥≤O

(t −t0)−2(θ−1)"
AND,0.9279538904899135,"3−2θ

, ∥yt −y∗(xt)∥≤O

(t −t0)−
2θ
3(3−2θ)

,
∀t ≥t0.
(11)"
AND,0.9293948126801153,"Proof. Notice that eq. (34) still holds after increasing t0, i.e., for T ≥t ≥t0 and large enough C"
AND,0.930835734870317,"such that
2
3C q"
AND,0.9322766570605188,"4L2
2κ2"
AND,0.9337175792507204,"L1
+ L2,
2
3C
q"
AND,0.9351585014409222,"10L2κ3 + 24L3
2κ
L2
1
+
1
2ηx < κ 3√L2"
AND,0.9365994236311239,"3
, we have"
AND,0.9380403458213257,κ 3√L2 3
AND,0.9394812680115274,"T −1
X"
AND,0.9409221902017291,"s=t
∥xs+1 −xs∥≤
C2c1/θ"
AND,0.9423631123919308,3(1 −1/θ)(Ht −H∗)1−1/θ
AND,0.9438040345821326,+ κ 3√L2
AND,0.9452449567723343,"3

∥xt−1 −xt−2∥+ 2∥xt −xt−1∥

< +∞.
(42)"
AND,0.946685878962536,Under review as a conference paper at ICLR 2022
AND,0.9481268011527377,Proposition 2 implies that
AND,0.9495677233429395,"∥xt −xt−1∥≤L−1/3
2
κ−1(Ht−1 −Ht)1/3 (i)
≤L−1/3
2
κ−1(Ht−1 −H∗)1/3,
(43)"
AND,0.9510086455331412,where (i) uses Ht−1 ≥Ht ≥H∗.
AND,0.952449567723343,"Therefore, for t ≥t0, eqs. (42) & (43) imply that"
AND,0.9538904899135446,"∥xt −x∗∥≤lim sup
T →∞"
AND,0.9553314121037464,"T −1
X"
AND,0.9567723342939481,"s=t
∥xs+1 −xs∥"
AND,0.9582132564841499,"≤
C2c1/θ"
AND,0.9596541786743515,"κ 3√L2(1 −1/θ)(Ht −H∗)1−1/θ +

(Ht−2 −H∗)1/3 + 2(Ht−1 −H∗)1/3
(44)"
AND,0.9610951008645533,"Next, we discuss case by case."
AND,0.962536023054755,"(Case 1) If θ ∈
  3"
AND,0.9639769452449568,"2, +∞), then"
AND,0.9654178674351584,"∥xt −x∗∥≤O

(Ht−2 −H∗)1/3 + (Ht−1 −H∗)1/3 + (Ht −H∗)1/3
(45)"
AND,0.9668587896253602,"≤O

exp
h
−1 3 2θ 3"
AND,0.968299711815562,(t−t0)/2−1i
AND,0.9697406340057637,"where the two ≤use eqs. (44) & (6) respectively. Hence,"
AND,0.9711815561959655,∥yt −y∗(xt)∥≤∥yt −y∗(xt−1)∥+ ∥y∗(xt−1) −y∗(xt)∥
AND,0.9726224783861671,"(i)
≤L2 L1"
AND,0.9740634005763689,"∥xt−1 −xt−2∥2 + κ∥xt −xt−1∥
(46)"
AND,0.9755043227665706,"(ii)
≤O"
AND,0.9769452449567724,"
(Ht−2 −H∗)2/3 + (Ht−1 −H∗)1/3
(47)"
AND,0.978386167146974,"≤O

exp
h
−1 3 2θ 3"
AND,0.9798270893371758,"(t−t0−2)i
+ exp
h
−1 3 2θ 3"
AND,0.9812680115273775,(t−t0−1)/2i
AND,0.9827089337175793,"≤O

exp
h
−1 3 2θ 3"
AND,0.984149855907781,"(t−t0−1)/2i
,"
AND,0.9855907780979827,"where (i) uses eq. (19) and item 1 of Proposition 1, (ii) uses eq. (43) and (iii) uses eq. (6)."
AND,0.9870317002881844,(Case 2) If θ = 3
AND,0.9884726224783862,"2, then the proof is similar to that of Case 2, and eqs. (45) & (47) still hold. The only
difference is to use the convergence rate of Ht −H∗given by eq. (7) instead of eq. (6)."
AND,0.9899135446685879,"(Case 3) If θ ∈(1, 3"
AND,0.9913544668587896,"2), then similar to the proof of Case 2, we obtain from eq. (44) that"
AND,0.9927953890489913,"∥xt −x∗∥≤O
h
(Ht−2 −H∗)1−1/θ + (Ht−1 −H∗)1−1/θ + (Ht −H∗)1−1/θi"
AND,0.9942363112391931,"≤O

(t −t0)−2(θ−1)"
AND,0.9956772334293948,"3−2θ

,"
AND,0.9971181556195965,"where the two ≤use eqs. (43) & (6) respectively. Then, as eq. (47) still holds, we have"
AND,0.9985590778097982,"∥yt −y∗(xt)∥≤O

(t −t0)−
2θ
3(3−2θ)

."
