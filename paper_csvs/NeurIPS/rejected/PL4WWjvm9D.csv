Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0021551724137931034,"The simultaneous quantile regression (SQR) technique has been used to estimate
1"
ABSTRACT,0.004310344827586207,"uncertainties for deep learning models, but its application is limited by the require-
2"
ABSTRACT,0.00646551724137931,"ment that the solution at the median quantile (τ = 0.5) must minimize the mean
3"
ABSTRACT,0.008620689655172414,"absolute error (MAE). In this article, we address this limitation by demonstrating a
4"
ABSTRACT,0.010775862068965518,"duality between quantiles and estimated probabilities in the case of simultaneous
5"
ABSTRACT,0.01293103448275862,"binary quantile regression (SBQR). This allows us to decouple the construction of
6"
ABSTRACT,0.015086206896551725,"quantile representations from the loss function, enabling us to assign an arbitrary
7"
ABSTRACT,0.017241379310344827,"classifier f(x) at the median quantile and generate the full spectrum of SBQR
8"
ABSTRACT,0.01939655172413793,"quantile representations at different values of τ. We validate our approach through
9"
ABSTRACT,0.021551724137931036,"two applications: (i) detecting out-of-distribution samples, where we show that
10"
ABSTRACT,0.023706896551724137,"quantile representations outperform standard probability outputs, and (ii) calibrat-
11"
ABSTRACT,0.02586206896551724,"ing models, where we demonstrate the robustness of quantile representations to
12"
ABSTRACT,0.028017241379310345,"distortions. We conclude with a discussion of several hypotheses arising from these
13"
ABSTRACT,0.03017241379310345,"findings.
14"
INTRODUCTION,0.032327586206896554,"1
Introduction
15"
INTRODUCTION,0.034482758620689655,"Deep learning models have become ubiquitous across diverse domains, and are increasingly being
16"
INTRODUCTION,0.036637931034482756,"used for several critical applications. Common questions which arise in practice are - (a) Can this
17"
INTRODUCTION,0.03879310344827586,"model be used on the given data input? and (b) If so, how much can one trust the probability
18"
INTRODUCTION,0.040948275862068964,"prediction obtained? The former refers to the problem of Out-of-Distribution (OOD) detection [13, 9]
19"
INTRODUCTION,0.04310344827586207,"and the latter refers to the problem of Calibration [10, 19, 22]. Understanding the applicability of a
20"
INTRODUCTION,0.04525862068965517,"given deep learning model is a topic of current research [30, 6, 25, 14]. In this article we consider the
21"
INTRODUCTION,0.04741379310344827,"quantile regression approach to answer these questions.
22"
INTRODUCTION,0.04956896551724138,"Quantile regression techniques [16, 17] provide much richer information about the model, allowing
23"
INTRODUCTION,0.05172413793103448,"for more comprehensive analysis and understanding relationship between different variables. In [32]
24"
INTRODUCTION,0.05387931034482758,"the authors show how simultaneous quantile regression (SQR) techniques can be used to estimate
25"
INTRODUCTION,0.05603448275862069,"the uncertainties of the deep learning model in the case of regression problems. However, these
26"
INTRODUCTION,0.05818965517241379,"techniques aren’t widely used in modern deep learning based systems since [5] - (a) The loss function
27"
INTRODUCTION,0.0603448275862069,"is restricted to be mean absolute error (MAE) or the pinball loss. This might not compatible with
28"
INTRODUCTION,0.0625,"domain specific losses. (b) Moreover, it is difficult to optimize the loss function in presence of
29"
INTRODUCTION,0.06465517241379311,"non-linearity. (c) Adapting the quantile regression approach for classification is also challenging due
30"
INTRODUCTION,0.0668103448275862,"to piece-wise constant behavior of the loss function, due to discrete labels.
31"
INTRODUCTION,0.06896551724137931,"Decoupling loss function and computing quantile representations:
Consider the problem setting
32"
INTRODUCTION,0.07112068965517242,"where a pre-trained classifier fθ(x) (including the dataset on which it is trained) is given and we
33"
INTRODUCTION,0.07327586206896551,"wish compute the quantile representations for detailed analysis of the pre-trained classifier. Classical
34"
INTRODUCTION,0.07543103448275862,"approach is to retrain the classifier using the quantile loss (equation 2). However, one runs the risk
35"
INTRODUCTION,0.07758620689655173,"of losing important properties while retraining since pinball loss would have different properties
36"
INTRODUCTION,0.07974137931034483,"compared to loss used for pre-training. Moreover, it is not clear how penalties (used for pre-training)
37"
INTRODUCTION,0.08189655172413793,"given to attributes such as gender-bias etc. should change with the quantile1. Further, aspects like
38"
INTRODUCTION,0.08405172413793104,"calibration error of the pre-trained network cannot be established by retraining with a different loss.
39"
INTRODUCTION,0.08620689655172414,"Thus, one requires a more elegant solution than retraining using the pinball loss.
40"
INTRODUCTION,0.08836206896551724,"Main Outcomes:
In this article we propose an approach to decouple the construction of quantile
41"
INTRODUCTION,0.09051724137931035,"representations from the loss function. We achieve this by identifying the Duality property between
42"
INTRODUCTION,0.09267241379310345,"quantiles and probabilities. We leverage the duality to construct the quantile-representations for any
43"
INTRODUCTION,0.09482758620689655,"pre-trained classifier fθ(x) in section 3.1. Such quantile representations are shown to capture the
44"
INTRODUCTION,0.09698275862068965,"training distributions in section 4.2. We show that these representations outperform the baseline for
45"
INTRODUCTION,0.09913793103448276,"OOD detection in section 4.4. And further verify that quantile representations can potentially identify
46"
INTRODUCTION,0.10129310344827586,"OOD samples perfectly. We also show that probabilities arising from quantile-representations are
47"
INTRODUCTION,0.10344827586206896,"invariant to distortions in section 4.3. Moreover, we see that standard post-hoc calibration techniques
48"
INTRODUCTION,0.10560344827586207,"such as Platt-scaling fail to preserve invariance to distortions. Proof-of-concept experiments to
49"
INTRODUCTION,0.10775862068965517,"improve OOD detection and identifying distribution shifts within the data are discussed in the
50"
INTRODUCTION,0.10991379310344827,"appendix (supplementary material).
51"
INTRODUCTION,0.11206896551724138,"Illustrating the Construction of Quantile Representations:
Before diving into the details, we
52"
INTRODUCTION,0.11422413793103449,"illustrate the construction using a simple toy example and considering the problem of OOD detection.
53"
INTRODUCTION,0.11637931034482758,"Figure 1a shows a simple toy example with 3 classes - 0, 1, 2. Class 0 is taken to be out-of-distribution
54"
INTRODUCTION,0.11853448275862069,"(OOD), while classes 1, 2 are taken to in-distribution (ID). To get the quantile representation - (step 1)
55"
INTRODUCTION,0.1206896551724138,"we first construct a simple classifier to differentiate classes 1, 2, (step 2) To get a classifier at quantile
56"
INTRODUCTION,0.12284482758620689,"τ, construct y+
i = I[pi > τ]2, where pi denotes the predicted probability in (step 1). Construct a
57"
INTRODUCTION,0.125,"classifier using the new labels y+
i . Figure 1b illustrates the classifiers obtained at different τ. In
58"
INTRODUCTION,0.1271551724137931,"(step 3) concatenate the outputs (predictions) of all the classifiers at different τ to get the quantile-
59"
INTRODUCTION,0.12931034482758622,"representations. Figures 1c and 1d demonstrate the advantage of having several classifiers as opposed
60"
INTRODUCTION,0.1314655172413793,"to one. By aggregating (detailed later) the outputs from different classifiers, we are able to identify
61"
INTRODUCTION,0.1336206896551724,"OOD vs ID samples (using One-Class-SVM [31]).
62"
INTRODUCTION,0.13577586206896552,"(a)
(b)
(c)
(d)"
INTRODUCTION,0.13793103448275862,"Figure 1: Illustrating the construction of Quantile Representations. (a) Simple toy example. (b) Illus-
trates different classifiers obtained for different τ. (c) OOD detection using Quantile Representations.
(d) OOD detection using the predictions from a single classifier. The region of In-distribution is
highlighted in red. Observe that, in this case, quantile-representations are able to differentiate the
in-distribution (ID) vs out-of-distribution (OOD).
Remark: Note that the construction in (step 2) does not depend on the procedure followed in (step 1),
63"
INTRODUCTION,0.1400862068965517,"but only the output probabilities pi. Thus, one can use any procedure in (step 1) without affecting (step
64"
INTRODUCTION,0.14224137931034483,"2). This property of quantiles is based on the duality between quantiles and probabilities ( section 2).
65"
INTRODUCTION,0.14439655172413793,"Intuitively, quantile representations capture the distribution of the training data. Thus, given a pre-
66"
INTRODUCTION,0.14655172413793102,"trained classifier, quantile representations can be used to analyze the classifier. In particular, as we
67"
INTRODUCTION,0.14870689655172414,"shall shortly illustrate, one can perform calibration and OOD-detection.
68"
INTRODUCTION,0.15086206896551724,"2
Simultaneous Binary Quantile Regression (SBQR)
69"
INTRODUCTION,0.15301724137931033,"In this section, we review some of the theoretical foundations required for constructing quantile
70"
INTRODUCTION,0.15517241379310345,"representations. For more details please refer to [16, 17, 32].
71"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.15732758620689655,"1Each quantile can be thought of as a significance level
2I[.] indicates the indicator function"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.15948275862068967,"Let pdata(X, Y ), denote the distribution from which the data is generated. X denotes the features
72"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.16163793103448276,"and Y denotes the targets (class labels). A classification algorithm predicts the latent variable (a.k.a
73"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.16379310344827586,"logits) Z which are used to make predictions on Y .
74"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.16594827586206898,"Let x ∈Rd denote the d dimensional features and y ∈{0, 1, · · · , k} denote the class labels (targets).
75"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.16810344827586207,"We assume that the training set consists of N i.i.d samples D = {(xi, yi)}. Let zi = fℓ,θ(x; θ)
76"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.17025862068965517,"denote the classification model which predicts the logits zi. In binary case (k = 1), applying the
77"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.1724137931034483,"σ (Sigmoid) function we obtain the probabilities, pi = fθ(xi) = σ(fℓ,θ(xi)). For multi-class
78"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.17456896551724138,"classification we use the softmax(fℓ,θ(xi)) to obtain the probabilities. The final class predictions are
79"
EACH QUANTILE CAN BE THOUGHT OF AS A SIGNIFICANCE LEVEL,0.17672413793103448,"obtained using the arg maxk pi,k, where k denotes the class-index.
80"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1788793103448276,"2.1
Review - Quantile Regression and Binary Quantile Regression
81"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1810344827586207,"Observe that, for binary classification, Z denotes a one dimensional distribution. FZ(z) = P(Z ≤z)
82"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.18318965517241378,"denotes the cumulative distribution of a random variable Z. The function F −1
Z (τ) = inf{z :
83"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1853448275862069,"FZ(z) ≥τ} denotes the quantile distribution of the variable Z, where 0 < τ < 1. The aim of
84"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1875,"quantile regression is to predict the τ th quantile of the latent variable Z from the data. That is, we
85"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1896551724137931,"aim to estimate F −1
Z (τ | X = x). Minimizing pinball-loss or check-loss [16],
86"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.19181034482758622,"pinball loss = n
X"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1939655172413793,"i=1
ρ(fθ(xi), yi)
where,
ρ(ˆy, y; τ) =
τ(y −ˆy)
if (y −ˆy) > 0
(1 −τ)(ˆy −y)
otherwise
(1)"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.1961206896551724,"allows us to learn fθ which estimates the τ th quantile of Y . When τ = 0.5, we obtain the loss
87"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.19827586206896552,"to be equivalent to mean absolute error (MAE). For the multi-class case we follow the one-vs-rest
88"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.20043103448275862,"procedure to learn quantiles for each class.
89"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.2025862068965517,"Simultaneous Quantile Regression (SQR):
Observe that the loss in equation 1 is for a single τ.
90"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.20474137931034483,"[32] argues that - minimizing the expected loss over all τ ∈(0, 1),
91"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.20689655172413793,"min
ψ Eτ∼U[0,1][ρ(ψ(x, τ), y; τ)]
(2)"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.20905172413793102,"is better than optimizing for each τ separately. Using the loss in equation 2 instead of equation 1
92"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.21120689655172414,"enforces the solution to have monotonicity property. If Q(x, τ) denotes the solution to equation 2,
93"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.21336206896551724,"monotonicity requires
94"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.21551724137931033,"Q(x, τi) ≤Q(x, τj) ⇔τi ≤τj
(3)"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.21767241379310345,"Observe that for a given xi, the function Q(xi, τ) can be interpreted as a (continuous) representation
95"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.21982758620689655,"of xi as τ varies over (0, 1). The function Q(x, τ) is referred to as quantile representation. Q(x, τ)
96"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.22198275862068967,"is sometimes written as Q(x, τ; θ), where θ indicates the parameters (such as weights in a neural
97"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.22413793103448276,"neural network). For brevity, we do not include the parameters θ in this article unless explicitly
98"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.22629310344827586,"required.
99"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.22844827586206898,"Remark on Notation: To differentiate between the latent scores (logits) and probabilities - we use
100"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.23060344827586207,"Q(x, τ), fθ(x) to denote the probabilities and Qℓ(x, τ), fℓ,θ(x) to denote the latent scores. Since
101"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.23275862068965517,"we have the relation Q(x, τ) = σ(Qℓ(x, τ)) and fℓ(x) = σ(fℓ,θ(x)) and σ(.) is monotonic, these
102"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.2349137931034483,"quantities are related by a monotonic transformation.
103"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.23706896551724138,"Why Quantile Regression?
Quantile regression techniques are relatively unknown in the machine
104"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.23922413793103448,"learning community, but offers a wide range of advantages over the traditional single point regression.
105"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.2413793103448276,"Quantiles give information about the shape of the distribution, in particular if the distribution is
106"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.2435344827586207,"skewed. They are robust to outliers, can model extreme events, capture uncertainty in predictions.
107"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.24568965517241378,"Quantile regression techniques have been used for pediatric medicine, survival and duration time
108"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.2478448275862069,"studies, discrimination and income inequality. (See supplememtary material for a more thorough
109"
REVIEW - QUANTILE REGRESSION AND BINARY QUANTILE REGRESSION,0.25,"discussion.)
110"
QUANTILE REPRESENTATIONS,0.2521551724137931,"3
Quantile Representations
111"
QUANTILE REPRESENTATIONS,0.2543103448275862,"As discussed earlier, minimizing equation 2 is not recommended due to unaccountable side-effects.
112"
QUANTILE REPRESENTATIONS,0.25646551724137934,"Thus, we require a procedure to construct quantile representations without resorting to minimizing
113"
QUANTILE REPRESENTATIONS,0.25862068965517243,"equation 2. In this section we present duality property of the quantile representations, which allows
114"
QUANTILE REPRESENTATIONS,0.2607758620689655,"us to do this.
115"
QUANTILE REPRESENTATIONS,0.2629310344827586,Algorithm 1 Generating Quantile Representations.
QUANTILE REPRESENTATIONS,0.2650862068965517,"• Let D = {(xi, yi)} denote the training dataset. Assume that a pre-trained binary classifier
fθ(x) is given. The aim is to generate the quantile representations with respect to f(x). We
refer to this fθ(x) as base-classifier.
• Assign Q(x, 0.5) = fθ(x), that is take the median classifier to be the given classifier."
QUANTILE REPRESENTATIONS,0.2672413793103448,"• Define y+
i,τ = I[fθ(xi) > (1 −τ)]. We refer to this as modified labels at quantile τ."
QUANTILE REPRESENTATIONS,0.26939655172413796,"• To obtain Q(x, τ), train the classifier using the dataset D+
τ = {(xi, y+
i,τ)}. Repeating this
for all τ allows us to construct the quantile representation Q(x, τ)."
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.27155172413793105,"3.1
Generating Quantile Representations Using Duality between Quantiles and Probabilities
116"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.27370689655172414,"Observe that, for binary classification, equation 1 can be written as
117"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.27586206896551724,"ρ(ˆy, y; τ) =
τ(1 −ˆy)
if y = 1
(1 −τ)(ˆy)
if y = 0
(4)"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.27801724137931033,"Thus the following property holds :
118"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.2801724137931034,"ρ(ˆy, y; τ) = ρ(1 −τ, y; 1 −ˆy)
(5)"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.2823275862068966,"We refer to the above property as duality between quantiles and probabilities. Let Q(x, τ) denotes a
119"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.28448275862068967,"solution to equation 2. It follows from above that, for a given xi and τ0, if we have Q(xi, τ0) = pi,
120"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.28663793103448276,"then we should also have Q(xi, 1−pi) = 1−τ0. In words, a solution which predicts the τ th quantile
121"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.28879310344827586,"can be interpreted as the quantile at which the probability is 1 −τ. This observation leads to the
122"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.29094827586206895,"algorithm 1 for generating the quantile representations.
123"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.29310344827586204,"Why does algorithm 1 return quantile representations?
Assume for an arbitrary xi, we have
124"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.2952586206896552,"Q(xi, 0.5) = pi. Standard interpretation states - at quantile τ = 0.5, the probability of xi in class 1 is
125"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.2974137931034483,"pi. However, thanks to duality in equation 5, this can also be interpreted as - At quantile τ = (1 −pi),
126"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.2995689655172414,"the probability of xi in class 1 is 0.5.
127"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3017241379310345,"Thanks to monotonocity property in equation 3, we have for all τ < (1 −pi), probability of xi in
128"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.30387931034482757,"class 1 is < 0.5, and hence belongs to class 0. And for all τ > (1 −pi), probability of xi in class 1
129"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.30603448275862066,"is > 0.5, and hence belongs to class 1.
130"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3081896551724138,"This implies that at a given quantile τ ∗, xi will belong to class 1 if τ ∗> (1 −pi) or if pi > (1 −τ ∗)
131"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3103448275862069,"or if fθ(xi) > (1 −τ ∗). Defining, y+
i,τ ∗= I[fθ(xi) > (1 −τ ∗)], we have that the classifier at
132"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3125,"quantile τ ∗fits the data D+
τ = {(xi, y+
i,τ ∗)} and thus can be used to identify Q(x, τ ∗). This gives us
133"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3146551724137931,"the algorithm 1 to get the quantile representations for an arbitrary classifier fθ(x).
134"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3168103448275862,"Remark (Sigmoid vs Indicator function): In theory, we approximate ˆyi = I[ ˆzi > 0] (i.e Indicator
135"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.31896551724137934,"function) with the sigmoid as ˆyi = σ( ˆzi). The algorithm 1 gives a solution up to this approximation.
136"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.32112068965517243,"In particular we have the following theorem
137"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3232758620689655,"Theorem 3.1 Let the base classifier fθ(x) = σ(fℓ,θ(x)) be obtained using the MAE loss, i.e by
138"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3254310344827586,"minimizing
139 min
θ X"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3275862068965517,"i
|yi −fθ(xi)|
(6)"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3297413793103448,"Then the solution Q(x, τ) obtained by algorithm 1 minimizes the cost in equation 2 over the dataset
140"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.33189655172413796,"D, i.e
141"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.33405172413793105,"Q(x, τ) = arg min
ψ
Eτ∈U[0,1] ""
1
N N
X"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.33620689655172414,"i=1
ρ(I[ψ(xi, τ) ≥0.5], yi; τ) # (7)"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.33836206896551724,"The proof for the above theorem is discussed in the supplementary material. In simple words, the
142"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.34051724137931033,"proof follows from the duality and the fact that we are only interested in the labels (for this theorem)
143"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3426724137931034,"obtained by applying the indicator function.
144"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3448275862068966,"Importance of duality:
Algorithm 1 and theorem 3.1 hinges on the duality property. Recall that
145"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.34698275862068967,"pinball loss equation 4 penalizes the positive errors and negative errors differently. In the case of
146"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.34913793103448276,"binary classification, since fθ(x) ∈(0, 1), positive errors occur for class 1 and negative errors occur
147"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.35129310344827586,"for class 0. Hence, the quantile value implicitly controls the probability of class 1, giving the duality
148"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.35344827586206895,"property.
149"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.35560344827586204,"Thus, using quantile value as an input allows us to control the probabilities and hence confidence of
150"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3577586206896552,"our predictions. This is what we have exploited to construct quantile representations without resorting
151"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3599137931034483,"to optimizing equation 2. This ensures that the properties of the pre-trained model are preserved
152"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3620689655172414,"while still being able to compute quantile representations.
153"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3642241379310345,"Remark: The other alternate to computing quantile representations are the Bayesian approaches
154"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.36637931034482757,"[15]. It is known that computing the full predictive distribution - p(y|D, x) =
R
p(y|w, x)p(w|D)dw
155"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.36853448275862066,"is computationally difficult. Quantile representations approximate the inverse of the c.d.f of the
156"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3706896551724138,"predictive distribution for the binary classification.
157"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3728448275862069,"To summarize, thanks to the duality in equation 5, one can compute the quantile representations for
158"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.375,"any arbitrary classifier. This allows for detailed analysis of the classifier and the features learned. In
159"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3771551724137931,"the following section we first discuss the implementation of algorithm 1 in practice and provide both
160"
GENERATING QUANTILE REPRESENTATIONS USING DUALITY BETWEEN QUANTILES AND PROBABILITIES,0.3793103448275862,"qualitative and quantitative analysis for specific models.
161"
EXPERIMENTS AND ANALYSIS,0.38146551724137934,"4
Experiments and Analysis
162"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.38362068965517243,"4.1
Generating Quantile Representations in practice
163"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.3857758620689655,"Let fθ(x) denote a pre-trained classifier. Given a dataset D = {(xi, yi)}i, we construct a quantile
164"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.3879310344827586,"dataset - {(xi, y+
i,τ)}i,τ as described in algorithm 1 with the following modifications:
165"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.3900862068965517,"• Instead of computing y+
i,τ = I[fθ(x) > (1 −τ)], we compute the τ th quantile of logits -
166"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.3922413793103448,"{fℓ,θ(xi)}i. Moreover, as multi-class classification problem gives class imbalance under
167"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.39439655172413796,"one-vs-rest paradigm, we compute weighted-quantiles, where weights are inversely propor-
168"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.39655172413793105,"tional to the size of the class. Observe that since fℓ,θ(x) is a monotonic function of fθ(x),
169"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.39870689655172414,"this does not make a difference in practice. However, this allows us to circumvent precision
170"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.40086206896551724,"issues caused due to the sigmoid function.
171"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.40301724137931033,"• We only consider a fixed finite number of quantiles.
The nτ quantiles are given by
172"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4051724137931034,"{1/nτ +1, 2/nτ +1, · · · , nτ/nτ +1}.
173"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4073275862068966,"For the sake of valid experimentation, we model Q(x, τ) using the same network as fθ(x), except
174"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.40948275862068967,"for the first layer. We concatenate the value of τ to the input, resulting in slightly more number of
175"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.41163793103448276,"parameters in the first layer. For efficient optimization we start the training with the weights of the
176"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.41379310344827586,"pre-trained classifier fθ(x), except for the first layer.
177"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.41594827586206895,"Loss function to train Qℓ(x, τ):
Recall that Qℓ(x, τ) indicates the latent logits.
We use
178"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.41810344827586204,"BinaryCrossEntropy loss to train Qℓ(x, τ) where the targets are given by the modified labels
179"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4202586206896552,"{y+
i,τ}.
180"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4224137931034483,"Inference using Qℓ(x, τ) :
After training, we compute the probabilities as follows
181"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4245689655172414,"pi =
Z 1"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4267241379310345,"τ=0
I[Qℓ(xi, τ) ≥0]dτ ≈1 nτ X"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.42887931034482757,"i
I[Qℓ(xi, τ) ≥0]
(8)"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.43103448275862066,"Remark: For multi-class classification, we follow a one-vs-rest approach. Hence the loss in this case
182"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4331896551724138,"would be sum of losses over all individual classes. The probability, in multi-class case, is taken to be
183"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4353448275862069,"arg maxk pi,k. Note that the probabilities pi,k do not add up to 1 over all classes.
184"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4375,"Remark: Since the aim is to analyze the pre-trained model, we only consider one specific architecture
185"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4396551724137931,"- Resnet34, and two datasets - CIFAR10 and SVHN to illustrate our results. Other related experiments
186"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4418103448275862,"are included in the appendix (supplementary material).
187"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.44396551724137934,"Training Details and Compute:Training quantile representations was done on a DGX server using 4
188"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.44612068965517243,"GPUs. It takes around 10 hours (40 GPU hours in total) to learn the quantile representations for each
189"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4482758620689655,"model. We use stochastic gradient descent with cyclic learning rate for optimization. The base_lr
190"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4504310344827586,"is taken to be 0.02 and max_lr is taken to be 1.0, with exponentially decreasing learning rate using
191"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4525862068965517,"γ = 0.99994. The batch_size is taken to be 1024 for resnet34. The number of steps for the cyclic
192"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.4547413793103448,"learning is taken to be 2(size_dataset/2(batch_size) + 1). The size_dataset describes the size of the
193"
GENERATING QUANTILE REPRESENTATIONS IN PRACTICE,0.45689655172413796,"training data. Complete code has been provided with the supplementary material.
194"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.45905172413793105,"4.2
Quantile Representations captures the distribution of the input data
195"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.46120689655172414,"Firstly, we analyze the learned quantile representations - Qℓ(., .). Broadly, the learned function
196"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.46336206896551724,"Qℓ(., .) captures the 1 dimensional caricature of the input distribution, in the direction where the
197"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.46551724137931033,"label changes. We illustrate this using a simple toy example (figure 2). Consider a 1-dimensional
198"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4676724137931034,"dataset in a 2d-space. The labels are assigned by splitting the dataset at the mean of the values on
199"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4698275862068966,"x-axis. We then learn a simple 1 hidden layer neural network with 100 hidden neurons. Using this
200"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.47198275862068967,"as a base classifier, we then learn the quantile representation function Qℓ(., .) as described above.
201"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.47413793103448276,"Figure 2: Quantile Representations captures the
distribution of the input data distribution."
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.47629310344827586,"Then, we reconstruct data in the original space
202"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.47844827586206895,"as follows - Assign arbitrary labels at each τ
203"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.48060344827586204,"satisfying the monotonicity property equation 3.
204"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4827586206896552,"For each set of labels, keeping the learned func-
205"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4849137931034483,"tion Qℓ(., .) fixed, learn the input which gives
206"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4870689655172414,"these labels. This is illustrated in figure 2. Ob-
207"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4892241379310345,"serve that the shape of the learned inputs (1-d
208"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.49137931034482757,"thread like structure) resembles the shape of
209"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.49353448275862066,"the input dataset. This shows that the function
210"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4956896551724138,"Qℓ(., .) captures how the sample distribution
211"
QUANTILE REPRESENTATIONS CAPTURES THE DISTRIBUTION OF THE INPUT DATA,0.4978448275862069,"changes in the input space.
212"
CALIBRATION OF ML MODELS,0.5,"4.3
Calibration of ML models
213"
CALIBRATION OF ML MODELS,0.5021551724137931,"For several applications the confidence of the
214"
CALIBRATION OF ML MODELS,0.5043103448275862,"predictions is important. This is measured by
215"
CALIBRATION OF ML MODELS,0.5064655172413793,"considering how well the output probabilities
216"
CALIBRATION OF ML MODELS,0.5086206896551724,"from the model reflect it’s predictive uncertainty.
217"
CALIBRATION OF ML MODELS,0.5107758620689655,"This is referred to as Calibration.
218"
CALIBRATION OF ML MODELS,0.5129310344827587,"Several methods [28, 37, 19, 1, 22] are used to improve the calibration of the deep learning models.
219"
CALIBRATION OF ML MODELS,0.5150862068965517,"Most of these methods consider a part of the data (apart from train data) to adjust the probability
220"
CALIBRATION OF ML MODELS,0.5172413793103449,"predictions. However, in [26, 23] it has been shown that most of the calibration approaches fail under
221"
CALIBRATION OF ML MODELS,0.5193965517241379,"distortions. In this section we show that calibration using quantile-representations are invariant to
222"
CALIBRATION OF ML MODELS,0.521551724137931,"distortions.
223"
CALIBRATION OF ML MODELS,0.5237068965517241,"Let pi,k denote the predicted probability that the sample xi belongs to class k. A perfectly calibrated
224"
CALIBRATION OF ML MODELS,0.5258620689655172,"model (binary class) will satisfy [10] P(yi = 1|pi,1 = p∗) = p∗. For multi-class case this is
225"
CALIBRATION OF ML MODELS,0.5280172413793104,"adapted to P(yi = arg maxk(pi,k)| maxk(pi,k) = p∗) = p∗. The degree of miscalibration is usually
226"
CALIBRATION OF ML MODELS,0.5301724137931034,"measured using Expected Calibration Error (ECE)
227"
CALIBRATION OF ML MODELS,0.5323275862068966,"E[|p∗−E[P(y = arg max
k
(pi,k)| max
k (pi,k) = p∗)]|]
(9)"
CALIBRATION OF ML MODELS,0.5344827586206896,"This is computed by binning the predictions into m bins - B1, B2, · · · , Bm and computing ˆ
ECE =
228
Pm
i=1(|Bi|/n)|acc(Bi) −conf(Bi)|. where acc(Bi) = (1/|Bi|) P"
CALIBRATION OF ML MODELS,0.5366379310344828,"j∈Bi I[yj = arg maxk(pj,k)]
229"
CALIBRATION OF ML MODELS,0.5387931034482759,"denotes the accuracy of the predictions lying in Bi, and conf(Bi) = P
j∈Bi maxk(pj,k) indicates
230"
CALIBRATION OF ML MODELS,0.540948275862069,"the average confidence of the predictions lying in Bi.
231"
CALIBRATION OF ML MODELS,0.5431034482758621,"In the ideal scenario, we have that quantile representations predict perfectly calibrated probabilities
232"
CALIBRATION OF ML MODELS,0.5452586206896551,"as illustrated in the following theorem.
233"
CALIBRATION OF ML MODELS,0.5474137931034483,"Theorem 4.1 Let fθ(.) denote the pre-trained model. Assume that the data is generated using the
234"
CALIBRATION OF ML MODELS,0.5495689655172413,"model y = I[fθ(x) + ϵ > 0], where ϵ denotes the error distribution. Let Q(x, τ) denote the quantile
235"
CALIBRATION OF ML MODELS,0.5517241379310345,"representations obtained on this data. The probabilities obtained using equation 8 are perfectly
236"
CALIBRATION OF ML MODELS,0.5538793103448276,"calibrated, that is,
237
Z 1"
CALIBRATION OF ML MODELS,0.5560344827586207,"τ=0
I[Q(x, τ) ≥0]dτ = P(fθ(x) + ϵ > 0)
(10)"
CALIBRATION OF ML MODELS,0.5581896551724138,"(a) ECE (Resnet34)
(b) Accuracy (Resnet34)"
CALIBRATION OF ML MODELS,0.5603448275862069,"Figure 3: Quantile representations can be effective for calibration because they estimate probabilities
using Equation equation 8, which has been shown to be robust to corruptions. As demonstrated using
the CIFAR10C dataset [12], the Expected Calibration Error (ECE) of the probabilities obtained from
quantile representations (QUANT) does not increase with the severity of the corruptions. In contrast,
when using the standard Maximum Softmax Probability (MSP) method, the calibration error increases
as the severity of the corruptions increases."
CALIBRATION OF ML MODELS,0.5625,"The proof for theorem 4.1 is given in the supplementary material. The main idea is the notion that
238"
CALIBRATION OF ML MODELS,0.5646551724137931,"Q(x, τ) captures P(fθ(x) + ϵ < τ).
239"
CALIBRATION OF ML MODELS,0.5668103448275862,"Observe that, we can use theorem 4.1 to predict the calibration error of any pre-trained model,
240"
CALIBRATION OF ML MODELS,0.5689655172413793,"given the quantile representations. (Remark: This is another advantage of computing the quantile
241"
CALIBRATION OF ML MODELS,0.5711206896551724,"representations without retraining the original classifier. If the quantile representations are obtained
242"
CALIBRATION OF ML MODELS,0.5732758620689655,"by minimizing equation 2, then we cannot be sure that calibration error would remain the same.)
243"
CALIBRATION OF ML MODELS,0.5754310344827587,"Experimental Setup
In this study, we employ the CIFAR10 dataset and the Resnet34 model to
244"
CALIBRATION OF ML MODELS,0.5775862068965517,"investigate the robustness of classifiers. To evaluate the classifiers’ robustness, we use the distorted
245"
CALIBRATION OF ML MODELS,0.5797413793103449,"CIFAR10 dataset introduced in [12], which contains 15 types of common corruptions at five severity
246"
CALIBRATION OF ML MODELS,0.5818965517241379,"levels. This dataset is a standard benchmark for testing the robustness of classifiers. We use quantile-
247"
CALIBRATION OF ML MODELS,0.584051724137931,"representations trained on the CIFAR10 training data to assess the generalization performance of the
248"
CALIBRATION OF ML MODELS,0.5862068965517241,"classifiers on the distorted dataset. We compare the performance with Maximum Softmax Probability
249"
CALIBRATION OF ML MODELS,0.5883620689655172,"(MSP) as a baseline and evaluate both accuracy and calibration error. We construct the bins {Bi}
250"
CALIBRATION OF ML MODELS,0.5905172413793104,"using 5 equally spaced quantiles within the predicted probabilities. The probabilities of each class
251"
CALIBRATION OF ML MODELS,0.5926724137931034,"are predicted using equation 8. (Remark: These probabilities do not add upto 1 since we consider a
252"
CALIBRATION OF ML MODELS,0.5948275862068966,"one-vs-rest approach.)
253"
CALIBRATION OF ML MODELS,0.5969827586206896,"Figure 4: Correcting calibration error on the valida-
tion set may not improve performance on corrupted
datasets."
CALIBRATION OF ML MODELS,0.5991379310344828,"We present the results in Figure 3. As we in-
254"
CALIBRATION OF ML MODELS,0.6012931034482759,"crease the severity of the distortions, we ob-
255"
CALIBRATION OF ML MODELS,0.603448275862069,"serve that the accuracy of both the quantile rep-
256"
CALIBRATION OF ML MODELS,0.6056034482758621,"resentations and MSP decreases. However, the
257"
CALIBRATION OF ML MODELS,0.6077586206896551,"probabilities obtained from quantile representa-
258"
CALIBRATION OF ML MODELS,0.6099137931034483,"tions are robust to distortions, as their Expected
259"
CALIBRATION OF ML MODELS,0.6120689655172413,"Calibration Error (ECE) does not increase with
260"
CALIBRATION OF ML MODELS,0.6142241379310345,"severity in the same way as MSP’s does. This
261"
CALIBRATION OF ML MODELS,0.6163793103448276,"indicates that quantile representations can more
262"
CALIBRATION OF ML MODELS,0.6185344827586207,"accurately predict calibration error and are more
263"
CALIBRATION OF ML MODELS,0.6206896551724138,"resistant to distortions.
264"
CALIBRATION OF ML MODELS,0.6228448275862069,"Cannot Correct the Calibration Error
Fig-
265"
CALIBRATION OF ML MODELS,0.625,"ure 3 shows that calibration error from quantile
266"
CALIBRATION OF ML MODELS,0.6271551724137931,"representations is robust to noise. So, an obvious
267"
CALIBRATION OF ML MODELS,0.6293103448275862,"question which follows is - Can we then correct
268"
CALIBRATION OF ML MODELS,0.6314655172413793,"it using validation data, improve the calibration
269"
CALIBRATION OF ML MODELS,0.6336206896551724,"score without compromising invariance to distortions? It turns out that usual methods fail when trying
270"
CALIBRATION OF ML MODELS,0.6357758620689655,"to correct the calibration error of quantile representations. (Remark: A similar result is also obtained
271"
CALIBRATION OF ML MODELS,0.6379310344827587,"in Proposition 1 of [5]).
272"
CALIBRATION OF ML MODELS,0.6400862068965517,"To verify this we perform the same experiment as earlier. Further we use Platt Scaling on validation
273"
CALIBRATION OF ML MODELS,0.6422413793103449,"data and accordingly transform the probability estimates for the corrupted datasets. These results are
274"
CALIBRATION OF ML MODELS,0.6443965517241379,"shown in figure 4. Observe that at severity 0, the calibration error is 0 for the corrected probabilities
275"
CALIBRATION OF ML MODELS,0.646551724137931,"as expected. However, as distortion increases, the calibration error increases as well.
276"
CALIBRATION OF ML MODELS,0.6487068965517241,"Discussion:
Consider the following - Given a specific calibration error C (say), let the set of all
277"
CALIBRATION OF ML MODELS,0.6508620689655172,"probability assignments which give the calibration error C be PC. If the network fθ(x) has to remain
278"
CALIBRATION OF ML MODELS,0.6530172413793104,"invariant to distortions, it should return one of these possible probability distributions PC. Our
279"
CALIBRATION OF ML MODELS,0.6551724137931034,"explanation for the above result is that - The vanilla neural networks do not have this property. The
280"
CALIBRATION OF ML MODELS,0.6573275862068966,"quantile networks, as illustrated, are evidenced to have this property. However, this also implies that
281"
CALIBRATION OF ML MODELS,0.6594827586206896,"calibration error cannot be corrected post-hoc in a sample independent manner.
282"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6616379310344828,"4.4
OOD Detection using Quantile Representations
283"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6637931034482759,"An assumption made across all machine learning models is that - Train and test datasets share the
284"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.665948275862069,"same distributions. However, test data can contain samples which are out-of-distribution (OOD)
285"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6681034482758621,"whose labels have not been seen during the training process [25]. Such samples should be ignored
286"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6702586206896551,"during inference. Hence OOD detection is a key component of reliable ML systems. Several methods
287"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6724137931034483,"[13, 20, 2] have been proposed for OOD detection. Here we check how quantile representations
288"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6745689655172413,"compare to the baseline method in [13] for OOD detection.
289"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6767241379310345,"Quantile representations construct different classifiers at different distances from the base-classifier
290"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6788793103448276,"(illustrated in figure 1b). This allows us to differentiate between OOD samples and ID samples.
291"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6810344827586207,"Intuitively, OOD samples are far from the boundary and result in low softmax probabilities. Thus,
292"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6831896551724138,"one way to assign OOD scores to samples is by considering the maximum softmax probabilities MSP
293"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6853448275862069,"as described in [12]. We compare the OOD detection of quantile representations with this approach.
294"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6875,"Experimental Setup
For this study, we use the CIFAR10[18] and SVHN[24] datasets as in-
295"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6896551724137931,"distribution (ID) datasets and the iSUN[34], LSUN[36], and TinyImagenet[21] datasets as out-of-
296"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6918103448275862,"distribution (OOD) datasets. Two versions of LSUN and TinyImagenet are considered - resized
297"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6939655172413793,"to 32 × 32 and cropped. We evaluate the quantile representations obtained using ResNet34[11]
298"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6961206896551724,"architecture. For evaluation we use (i) AUROC: The area under the receiver operating characteristic
299"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.6982758620689655,"curve of a threshold-based detector. A perfect detector corresponds to an AUROC score of 100%. (ii)
300"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7004310344827587,"TNR at 95% TPR: The probability that an OOD sample is correctly identified (classified as negative)
301"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7025862068965517,"when the true positive rate equals 95%. (iii) Detection accuracy: Measures the maximum possible
302"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7047413793103449,"classification accuracy over all possible thresholds.
303"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7068965517241379,"Methodology and Results
To identify OOD samples with quantile representations, we consider the
304"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.709051724137931,"entire representation - Qℓ(xi, τ) as input features. In our experiments this would be of the dimension
305"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7112068965517241,"nτ × n_classes. To assign an OOD-score we use the One-Class SVM approach. The first rows of
306"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7133620689655172,"Table 1 shows the results. Observe that quantile-representations perform marginally better than than
307"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7155172413793104,"the baseline.
308"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7176724137931034,"The more interesting result, however, is the fact that quantile representations have all the information
309"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7198275862068966,"required to identify OOD samples. To establish this we perform the following experiment - We use
310"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7219827586206896,"a simple linear classifier (logistic regression) to identify if the ID and OOD datasets are linearly
311"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7241379310344828,"separable or not. We measure the training accuracy to quantify linear separability - If the accuracy
312"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7262931034482759,"is close to 100%, then the datasets are considered to be linearly separable. For comparison we
313"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.728448275862069,"perform the same experiment with the pre-trained logits fℓ,θ(x). These results are shown in the
314"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7306034482758621,"bottom rows of Table 1. Note that while the baseline scores vary with the dataset, the quantile scores
315"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7327586206896551,"are consistently close to 100%. This provides additional evidence to the hypothesis that quantile
316"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7349137931034483,"representations capture all the “relevant” information about the train distribution.
317"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7370689655172413,"Table 1: Comparison of Quantile-Representations with baseline for OOD Detection. Observe that
Quantile-Representations outperform the baseline in all the cases. The entries are represented as
BASELINE/QUANTILE-REPRESENTATIONS."
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7392241379310345,"AUROC
TNR-TPR95
Det.Acc
Approach
Model/ID
OOD"
OOD DETECTION USING QUANTILE REPRESENTATIONS,0.7413793103448276,"OneClassSVM
Resnet34/SVHN
iSUN
92.28/96.13
77.43/80.15
89.77/90.65
LSUN(R)
91.50/95.44
74.95/77.05
89.09/89.67
LSUN(C)
92.99/95.76
77.96/80.93
90.10/90.03
Imagenet(R)
93.52/96.21
79.86/79.60
90.58/90.84
Imagenet(C)
94.18/95.98
81.13/79.77
91.23/90.57
ResNet34/CIFAR10
iSUN
90.29/93.53
41.90/61.24
84.28/87.06
LSUN(R)
90.07/93.41
41.24/61.01
84.25/86.77
LSUN(C)
91.74/91.79
45.87/51.96
86.37/85.77
Imagenet(R)
90.33/92.33
42.18/58.95
84.21/85.47
Imagenet(C)
90.96/91.44
43.95/52.08
84.80/84.58
LinearSeparability
Resnet34/SVHN
iSUN
83.00/99.98
60.87/99.90
78.98/99.38
LSUN(R)
81.90/99.98
56.34/99.97
77.76/99.47
LSUN(C)
80.44/99.75
52.80/99.25
75.35/97.76
Imagenet(R)
80.31/99.96
57.61/99.85
77.19/99.19
Imagenet(C)
81.88/99.93
61.28/99.78
78.75/98.91
ResNet34/CIFAR10
iSUN
98.73/99.94
96.06/99.88
95.75/98.92
LSUN(R)
98.80/99.96
96.18/99.84
95.94/99.17
LSUN(C)
92.78/99.55
70.72/98.31
87.47/96.87
Imagenet(R)
95.27/99.74
86.76/98.93
91.02/97.72
Imagenet(C)
94.38/99.67
82.37/98.72
89.15/97.25"
RELATED WORK,0.7435344827586207,"5
Related Work
318"
RELATED WORK,0.7456896551724138,"[16, 27, 29, 3] provides a comprehensive overview of approaches related to quantile regression and
319"
RELATED WORK,0.7478448275862069,"identifying the parameters. [4] extends the quantiles to multi-variate case. [32, 33] use quantile
320"
RELATED WORK,0.75,"regression based approaches for estimating confidence of neural networks based predictions. [1, 8]
321"
RELATED WORK,0.7521551724137931,"uses conformal methods to calibrate probabilities, and is closely related to computing quantiles. [5]
322"
RELATED WORK,0.7543103448275862,"proposes a similar algorithm to overcome the restriction to pinball loss for regression problems. [7]
323"
RELATED WORK,0.7564655172413793,"generates predictive regions using quantile regression techniques.
324"
RELATED WORK,0.7586206896551724,"6
Conclusion, Limitations and Future work
325"
RELATED WORK,0.7607758620689655,"To summarize, in this article we show the duality between quantiles and probabilities in the case
326"
RELATED WORK,0.7629310344827587,"of SBQR. Exploiting the duality, we propose an algorithm to compute quantile representations for
327"
RELATED WORK,0.7650862068965517,"any given base classifier. We verify that the quantile representations model the training distribution
328"
RELATED WORK,0.7672413793103449,"well both qualitatively (by resconstructing the data in the input space) and quantitatively (using
329"
RELATED WORK,0.7693965517241379,"OOD detection baseline). We further show that the probabilities from quantile representations are
330"
RELATED WORK,0.771551724137931,"robust to distortions. Interestingly, we found that traditional approaches cannot be used to correct the
331"
RELATED WORK,0.7737068965517241,"calibration error. Further experiments to validate the observations made in this article are discussed
332"
RELATED WORK,0.7758620689655172,"in the supplementary material.
333"
RELATED WORK,0.7780172413793104,"The main limitation of the approach is the computation required for algorithm 1 for large scale
334"
RELATED WORK,0.7801724137931034,"datasets. Note that algorithm 1 creates nτ copies of the same dataset by assigning different labels.
335"
RELATED WORK,0.7823275862068966,"For large scale datasets and large scale networks this requires a lot more computation than using a
336"
RELATED WORK,0.7844827586206896,"pre-trained classifier. However, we hypothesize that - we only need to retrain the quantile network
337"
RELATED WORK,0.7866379310344828,"only on a small sample size instead of the entire dataset We consider this for future work.
338"
RELATED WORK,0.7887931034482759,"Based on strong convexity and Lipschitzness of loss functions, automatic learning rates can be
339"
RELATED WORK,0.790948275862069,"computed for large networks via the inverse of the Lipschitz constant of the loss function being an
340"
RELATED WORK,0.7931034482758621,"ideal learning rate [35]. We conjecture that exploiting the loss functions which inherit some convexity
341"
RELATED WORK,0.7952586206896551,"and Lipschitz properties from the known, closed form loss representations would achieve higher
342"
RELATED WORK,0.7974137931034483,"learning rates for faster convergence to compute quantile representations. We defer this as future
343"
RELATED WORK,0.7995689655172413,"work.
344"
REFERENCES,0.8017241379310345,"References
345"
REFERENCES,0.8038793103448276,"[1] Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. Candès, Michael I. Jordan, and Lihua
346"
REFERENCES,0.8060344827586207,"Lei. Learn then test: Calibrating predictive algorithms to achieve risk control. arXiv preprint
347"
REFERENCES,0.8081896551724138,"arXiv:2110.01052, 2021.
348"
REFERENCES,0.8103448275862069,"[2] Koby Bibas, Meir Feder, and Tal Hassner. Single layer predictive normalized maximum
349"
REFERENCES,0.8125,"likelihood for out-of-distribution detection. In Neural Inform. Process. Syst., 2021.
350"
REFERENCES,0.8146551724137931,"[3] Probal Chaudhuri. Generalized regression quantiles: Forming a useful toolkit for robust linear
351"
REFERENCES,0.8168103448275862,"regression. L1 Statistical Analysis and Related Methods, Amsterdam: North-Holland, pages
352"
REFERENCES,0.8189655172413793,"169–185, 1992.
353"
REFERENCES,0.8211206896551724,"[4] Probal Chaudhuri. On a geometric notion of quantiles for multivariate data. Journal of the
354"
REFERENCES,0.8232758620689655,"American Statistical Association, 91(434):862–872, 1996.
355"
REFERENCES,0.8254310344827587,"[5] Youngseog Chung, Willie Neiswanger, Ian Char, and Jeff Schneider. Beyond pinball loss:
356"
REFERENCES,0.8275862068965517,"Quantile methods for calibrated uncertainty quantification. In Neural Inform. Process. Syst.,
357"
REFERENCES,0.8297413793103449,"2021.
358"
REFERENCES,0.8318965517241379,"[6] Max H. Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and
359"
REFERENCES,0.834051724137931,"inference. Econometrica, 89(1):181–213, 2021.
360"
REFERENCES,0.8362068965517241,"[7] Shai Feldman, Stephen Bates, and Yaniv Romano. Calibrated multiple-output quantile regression
361"
REFERENCES,0.8383620689655172,"with representation learning. arXiv preprint arXiv:2110.00816, 2021.
362"
REFERENCES,0.8405172413793104,"[8] Shai Feldman, Stephen Bates, and Yaniv Romano. Improving conditional coverage via orthogo-
363"
REFERENCES,0.8426724137931034,"nal quantile regression. In Neural Inform. Process. Syst., 2021.
364"
REFERENCES,0.8448275862068966,"[9] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution
365"
REFERENCES,0.8469827586206896,"detection. In Neural Inform. Process. Syst., 2021.
366"
REFERENCES,0.8491379310344828,"[10] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
367"
REFERENCES,0.8512931034482759,"networks. In Int. Conf. Mach. Learning, 2017.
368"
REFERENCES,0.853448275862069,"[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
369"
REFERENCES,0.8556034482758621,"recognition. In Proc. Conf. Comput. Vision Pattern Recognition, 2016.
370"
REFERENCES,0.8577586206896551,"[12] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common
371"
REFERENCES,0.8599137931034483,"corruptions and perturbations. In Int. Conf. on Learning Representations, 2019.
372"
REFERENCES,0.8620689655172413,"[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
373"
REFERENCES,0.8642241379310345,"examples in neural networks. In Int. Conf. on Learning Representations, 2017.
374"
REFERENCES,0.8663793103448276,"[14] Heinrich Jiang, Been Kim, Melody Y. Guan, and Maya R. Gupta. To trust or not to trust A
375"
REFERENCES,0.8685344827586207,"classifier. In Neural Inform. Process. Syst., 2018.
376"
REFERENCES,0.8706896551724138,"[15] Laurent Valentin Jospin, Hamid Laga, Farid Boussaïd, Wray L. Buntine, and Mohammed
377"
REFERENCES,0.8728448275862069,"Bennamoun. Hands-on bayesian neural networks - A tutorial for deep learning users. IEEE
378"
REFERENCES,0.875,"Comput. Intell. Mag., 17(2):29–48, 2022.
379"
REFERENCES,0.8771551724137931,"[16] Roger Koenker. Quantile Regression. Econometric Society Monographs. Cambridge University
380"
REFERENCES,0.8793103448275862,"Press, 2005.
381"
REFERENCES,0.8814655172413793,"[17] Gregory Kordas. Smoothed binary regression quantiles. Journal of Applied Econometrics,
382"
REFERENCES,0.8836206896551724,"21(3):387–407, 2006.
383"
REFERENCES,0.8857758620689655,"[18] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. online: http://www.
384"
REFERENCES,0.8879310344827587,"cs. toronto. edu/kriz/cifar. html, 2014.
385"
REFERENCES,0.8900862068965517,"[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
386"
REFERENCES,0.8922413793103449,"predictive uncertainty estimation using deep ensembles. In Neural Inform. Process. Syst., 2017.
387"
REFERENCES,0.8943965517241379,"[20] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for
388"
REFERENCES,0.896551724137931,"detecting out-of-distribution samples and adversarial attacks. In Neural Inform. Process. Syst.,
389"
REFERENCES,0.8987068965517241,"2018.
390"
REFERENCES,0.9008620689655172,"[21] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
391"
REFERENCES,0.9030172413793104,"detection in neural networks. In Int. Conf. on Learning Representations, 2018.
392"
REFERENCES,0.9051724137931034,"[22] Jeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
393"
REFERENCES,0.9073275862068966,"narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
394"
REFERENCES,0.9094827586206896,"distance awareness. In Neural Inform. Process. Syst., 2020.
395"
REFERENCES,0.9116379310344828,"[23] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil
396"
REFERENCES,0.9137931034482759,"Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks.
397"
REFERENCES,0.915948275862069,"In Neural Inform. Process. Syst., 2021.
398"
REFERENCES,0.9181034482758621,"[24] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.
399"
REFERENCES,0.9202586206896551,"Reading digits in natural images with unsupervised feature learning. In Neural Inform. Process.
400"
REFERENCES,0.9224137931034483,"Syst. Workshops, 2011.
401"
REFERENCES,0.9245689655172413,"[25] Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled:
402"
REFERENCES,0.9267241379310345,"High confidence predictions for unrecognizable images. In Proc. Conf. Comput. Vision Pattern
403"
REFERENCES,0.9288793103448276,"Recognition, 2015.
404"
REFERENCES,0.9310344827586207,"[26] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin,
405"
REFERENCES,0.9331896551724138,"Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncer-
406"
REFERENCES,0.9353448275862069,"tainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530,
407"
REFERENCES,0.9375,"2019.
408"
REFERENCES,0.9396551724137931,"[27] Emanuel Parzen.
Quantile probability and statistical data modeling.
Statistical Science,
409"
REFERENCES,0.9418103448275862,"19(4):652–662, 2004.
410"
REFERENCES,0.9439655172413793,"[28] J. Platt. Probabilistic outputs for support vector machines and comparison to regularized
411"
REFERENCES,0.9461206896551724,"likelihood methods. In Advances in Large Margin Classifiers, 2000.
412"
REFERENCES,0.9482758620689655,"[29] Stephen Portnoy and Roger Koenker. Adaptive l-estimation for linear models. The Annals of
413"
REFERENCES,0.9504310344827587,"Statistics, 17(1):362–381, 1989.
414"
REFERENCES,0.9525862068965517,"[30] Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. ""why should I trust you?"": Explaining
415"
REFERENCES,0.9547413793103449,"the predictions of any classifier. In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data
416"
REFERENCES,0.9568965517241379,"Minining, 2016.
417"
REFERENCES,0.959051724137931,"[31] Bernhard Schölkopf, John C. Platt, John Shawe-Taylor, Alexander J. Smola, and Robert C.
418"
REFERENCES,0.9612068965517241,"Williamson. Estimating the support of a high-dimensional distribution. Neural Comput.,
419"
REFERENCES,0.9633620689655172,"13(7):1443–1471, 2001.
420"
REFERENCES,0.9655172413793104,"[32] Natasa Tagasovska and David Lopez-Paz. Single-model uncertainties for deep learning. In
421"
REFERENCES,0.9676724137931034,"Neural Inform. Process. Syst., 2019.
422"
REFERENCES,0.9698275862068966,"[33] Anuj Tambwekar, Anirudh Maiya, Soma S. Dhavala, and Snehanshu Saha. Estimation and
423"
REFERENCES,0.9719827586206896,"applications of quantiles in deep binary classification. IEEE Trans. Artif. Intell., 3(2):275–286,
424"
REFERENCES,0.9741379310344828,"2022.
425"
REFERENCES,0.9762931034482759,"[34] Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and
426"
REFERENCES,0.978448275862069,"Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv
427"
REFERENCES,0.9806034482758621,"preprint arXiv:1504.06755, 2015.
428"
REFERENCES,0.9827586206896551,"[35] Rahul Yedida, Snehanshu Saha, and Tejas Prashanth. Lipschitzlr: Using theoretically computed
429"
REFERENCES,0.9849137931034483,"adaptive learning rates for fast convergence. Applied Intelligence, 51:1460–1478, 2021.
430"
REFERENCES,0.9870689655172413,"[36] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction
431"
REFERENCES,0.9892241379310345,"of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
432"
REFERENCES,0.9913793103448276,"arXiv:1506.03365, 2015.
433"
REFERENCES,0.9935344827586207,"[37] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass
434"
REFERENCES,0.9956896551724138,"probability estimates. In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Minining,
435"
REFERENCES,0.9978448275862069,"2002.
436"
