Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018484288354898336,"Text-attributed Graphs (TAGs) are commonly found in the real world, such as
1"
ABSTRACT,0.0036968576709796672,"social networks and citation networks, and consist of nodes represented by textual
2"
ABSTRACT,0.005545286506469501,"descriptions. Currently, mainstream machine learning methods on TAGs involve
3"
ABSTRACT,0.0073937153419593345,"a two-stage modeling approach: (1) unsupervised node feature extraction with
4"
ABSTRACT,0.009242144177449169,"pre-trained language models (PLMs); and (2) supervised learning using Graph
5"
ABSTRACT,0.011090573012939002,"Neural Networks (GNNs). However, we observe that these representations, which
6"
ABSTRACT,0.012939001848428836,"have undergone large-scale pre-training, do not significantly improve performance
7"
ABSTRACT,0.014787430683918669,"with a limited amount of training samples. The main issue is that existing methods
8"
ABSTRACT,0.0166358595194085,"have not effectively integrated information from the graph and downstream tasks
9"
ABSTRACT,0.018484288354898338,"simultaneously. In this paper, we propose a novel framework called G-Prompt,
10"
ABSTRACT,0.02033271719038817,"which combines a graph adapter and task-specific prompts to extract node features.
11"
ABSTRACT,0.022181146025878003,"First, G-Prompt introduces a learnable GNN layer (i.e., adaptor) at the end of PLMs,
12"
ABSTRACT,0.024029574861367836,"which is fine-tuned to better capture the masked tokens considering graph neighbor-
13"
ABSTRACT,0.025878003696857672,"hood information. After the adapter is trained, G-Prompt incorporates task-specific
14"
ABSTRACT,0.027726432532347505,"prompts to obtain interpretable node representations for the downstream task. Our
15"
ABSTRACT,0.029574861367837338,"experiment results demonstrate that our proposed method outperforms current
16"
ABSTRACT,0.031423290203327174,"state-of-the-art (SOTA) methods on few-shot node classification. More importantly,
17"
ABSTRACT,0.033271719038817,"in zero-shot settings, the G-Prompt embeddings can not only provide better task
18"
ABSTRACT,0.03512014787430684,"interpretability than vanilla PLMs but also achieve comparable performance with
19"
ABSTRACT,0.036968576709796676,"fully-supervised baselines.
20"
INTRODUCTION,0.038817005545286505,"1
Introduction
21"
INTRODUCTION,0.04066543438077634,"Text-Attributed Graphs (TAGs) are a type of graph that have textual data as node attributes. These
22"
INTRODUCTION,0.04251386321626617,"types of graphs are prevalent in the real world, such as in citation networks [12] where the node at-
23"
INTRODUCTION,0.04436229205175601,"tribute is the paper’s abstract. TAGs have diverse potential applications, including paper classification
24"
INTRODUCTION,0.04621072088724584,"[3] and user profiling[14]. However, studying TAGs presents a significant challenge: how to model
25"
INTRODUCTION,0.04805914972273567,"the intricate interplay between graph structures and textual features. This issue has been extensively
26"
INTRODUCTION,0.04990757855822551,"explored in several fields, including natural language processing, information extraction, and graph
27"
INTRODUCTION,0.051756007393715345,"representation learning.
28"
INTRODUCTION,0.053604436229205174,"An idealized approach involves combining pre-trained language models (PLMs) [10, 20] with graph
29"
INTRODUCTION,0.05545286506469501,"neural networks and jointly training them [35, 24]. Nevertheless, this method requires fine-tuning the
30"
INTRODUCTION,0.05730129390018484,"PLMs, which demands substantial computational resources. Additionally, trained models are hard to
31"
INTRODUCTION,0.059149722735674676,"be reused in other tasks because finetuning PLM may bring catastrophic forgetting[2].
32"
INTRODUCTION,0.06099815157116451,"Therefore, a more commonly used and efficient approach is the two-stage process [32, 34, 23]:
33"
INTRODUCTION,0.06284658040665435,"(1) utilizing pre-trained language models (PLMs) for unsupervised modeling of the nodes’ textual
34"
INTRODUCTION,0.06469500924214418,"features. (2) supervised learning using Graph Neural Networks (GNNs). Compared to joint training
35"
INTRODUCTION,0.066543438077634,"of PLMs and GNNs, this approach offers several advantages in practical applications. For example,
36"
INTRODUCTION,0.06839186691312385,"it can be combined with numerous GNN frameworks or PLMs, and this approach does not require
37"
INTRODUCTION,0.07024029574861368,"fine-tuning PLMs for every downstream task. However, PLMs are unable to fully leverage the
38"
INTRODUCTION,0.07208872458410351,"wealth of information contained in the graph structure, which represents significant information.
39"
INTRODUCTION,0.07393715341959335,"To overcome these limitations, some works propose self-supervised fine-tuning PLMs using graph
40"
INTRODUCTION,0.07578558225508318,"information to extract graph-aware node features [3]. Such methods have achieved significant success
41"
INTRODUCTION,0.07763401109057301,"across various benchmark datasets[12].
42"
INTRODUCTION,0.07948243992606285,"However, both self-supervised methods and using language models directly to process TAG suffer
43"
INTRODUCTION,0.08133086876155268,"from a fundamental drawback. They cannot incorporate downstream task information, which results in
44"
INTRODUCTION,0.08317929759704251,"identical representations being generated for all downstream tasks. This is evidently counterintuitive
45"
INTRODUCTION,0.08502772643253234,"as the required information may vary for different tasks. For example, height is useful information
46"
INTRODUCTION,0.08687615526802218,"in predicting a user’s weight but fails to accurately predict age. This issue can be resolved by
47"
INTRODUCTION,0.08872458410351201,"utilizing task-specific prompts combined with language models [26] to extract downstream task-
48"
INTRODUCTION,0.09057301293900184,"related representations. For example, suppose we have a paper’s abstract {Abstract} in a citation
49"
INTRODUCTION,0.09242144177449169,"network, and the task is to classify the subject of the paper. We can add some prompts to a node’s
50"
INTRODUCTION,0.09426987060998152,"sentence: {This, is, a, paper, of, [mask], subject, its, abstract, is, :, Abstract}. And then use
51"
INTRODUCTION,0.09611829944547134,"the embedding corresponding to the [mask] token generated by PLMs as the node feature. Yet this
52"
INTRODUCTION,0.09796672828096119,"approach fails to effectively integrate graph information.
53"
INTRODUCTION,0.09981515711645102,"To better integrate task-specific information and knowledge within graph structure, this paper proposes
54"
INTRODUCTION,0.10166358595194085,"a novel framework called G-Prompt. G-Prompt combines a graph adapter and task-specific prompts to
55"
INTRODUCTION,0.10351201478743069,"extract node features. Specifically, G-Prompt contains a graph adapter that helps PLMs become aware
56"
INTRODUCTION,0.10536044362292052,"of graph structures. This graph adapter is self-supervised and trained by fill-mask tasks on specific
57"
INTRODUCTION,0.10720887245841035,"TAGs. G-Prompt then incorporates task-specific prompts to obtain interpretable node representations
58"
INTRODUCTION,0.10905730129390019,"for downstream tasks.
59"
INTRODUCTION,0.11090573012939002,"We conduct extensive experiments on three real-world datasets in the domains of few-shot and zero-
60"
INTRODUCTION,0.11275415896487985,"shot learning, in order to demonstrate the effectiveness of our proposed method. The results of our
61"
INTRODUCTION,0.11460258780036968,"experiments show that G-Prompt achieves state-of-the-art performance in few-shot learning, with an
62"
INTRODUCTION,0.11645101663585952,"average improvement of avg. 4.1% compared to the best baseline. Besides, our G-Prompt embeddings
63"
INTRODUCTION,0.11829944547134935,"are also highly robust in zero-shot settings, outperforming PLMs by avg. 2.7%. Furthermore, we
64"
INTRODUCTION,0.12014787430683918,"conduct an analysis of the representations generated by G-Prompt and found that they have high
65"
INTRODUCTION,0.12199630314232902,"interpretability with respect to task performance.
66"
BACKGROUND,0.12384473197781885,"2
Background
67"
TEXT-ATTRIBUTED GRAPH,0.1256931608133087,"2.1
Text-Attributed Graph
68"
TEXT-ATTRIBUTED GRAPH,0.12754158964879853,"Let G = {V, A} denotes a text-attributed graph (TAG), where V is the node set and A is the
69"
TEXT-ATTRIBUTED GRAPH,0.12939001848428835,"adjacency matrix. Each node i ∈V is associated with a sentence Si = {si,0, si,1, ..., si,|Si|}, which
70"
TEXT-ATTRIBUTED GRAPH,0.13123844731977818,"represents the textual feature of the node. In most cases, the first token in each sentence (i.e., si,0) is
71"
TEXT-ATTRIBUTED GRAPH,0.133086876155268,"[cls], indicating the beginning of the sentence. This paper focuses on how to unsupervised extract
72"
TEXT-ATTRIBUTED GRAPH,0.13493530499075784,"high-quality node features on TAGs for various downstream tasks.
73"
PRETRAINED LANGUAGE MODELS,0.1367837338262477,"2.2
Pretrained Language Models
74"
PRETRAINED LANGUAGE MODELS,0.13863216266173753,"Before we introduce G-Prompt, we require some basic concepts of pre-trained language models.
75"
PRETRAINED LANGUAGE MODELS,0.14048059149722736,"Framework of PLMs. A PLM consists of a multi-layer transformer encoder that takes a sentence Si
76"
PRETRAINED LANGUAGE MODELS,0.1423290203327172,"as input and outputs the hidden states of each token:
77"
PRETRAINED LANGUAGE MODELS,0.14417744916820702,"PLM({si,0, si,1, ..., si,|Si|}) = {hi,0, hi,1, ..., hi,|Si|},
(1)
where hi,k is the dense hidden state of si,k.
78"
PRETRAINED LANGUAGE MODELS,0.14602587800369685,"Pretraining of PLMs. The fill-mask task is commonly used to pre-train PLMs [4, 20, 10]. Given
79"
PRETRAINED LANGUAGE MODELS,0.1478743068391867,"a sentence Si, the mask stage involves randomly selecting some tokens and replacing them with
80"
PRETRAINED LANGUAGE MODELS,0.14972273567467653,"either [mask] or random tokens, resulting in a modified sentence ˆSi = {si,0, si,1, ..., ˆsi,k, ..., si,|Si|},
81"
PRETRAINED LANGUAGE MODELS,0.15157116451016636,"where ˆsi,k represents the masked token. In the filling stage, ˆSi is passed through the transformer
82"
PRETRAINED LANGUAGE MODELS,0.1534195933456562,"encoder, which outputs the hidden states of each token. We denote the hidden state of the masked
83"
PRETRAINED LANGUAGE MODELS,0.15526802218114602,"token ˆsi,k as ˆhi,k, which is used to predict the ID of the masked token:
84"
PRETRAINED LANGUAGE MODELS,0.15711645101663585,"ˆyi,k = fLM(ˆhi,k),
(2)"
PRETRAINED LANGUAGE MODELS,0.1589648798521257,"[This]
[adapter]
[is]
[a]
[good]
[method]
[SEP]
[CLS]"
PRETRAINED LANGUAGE MODELS,0.16081330868761554,"[This]
[adapter]
[is]
[a]
[good]
[method]
[SEP]
[CLS]"
PRETRAINED LANGUAGE MODELS,0.16266173752310537,Pre-trained Language Model
PRETRAINED LANGUAGE MODELS,0.1645101663585952,"(RoBERTa, ALBERT…)"
PRETRAINED LANGUAGE MODELS,0.16635859519408502,Paper A
PRETRAINED LANGUAGE MODELS,0.16820702402957485,Paper B
PRETRAINED LANGUAGE MODELS,0.17005545286506468,Paper C
PRETRAINED LANGUAGE MODELS,0.17190388170055454,Graph Adapter
PRETRAINED LANGUAGE MODELS,0.17375231053604437,Fill-mask Loss
PRETRAINED LANGUAGE MODELS,0.1756007393715342,"[This]
[mothod]"
PRETRAINED LANGUAGE MODELS,0.17744916820702403,Graph Adapter
PRETRAINED LANGUAGE MODELS,0.17929759704251386,Fill-mask Loss
PRETRAINED LANGUAGE MODELS,0.18114602587800369,"[It]
[MASK]
[node]
[feature]
[well]
[SEP]"
PRETRAINED LANGUAGE MODELS,0.18299445471349354,Pre-trained Language Model
PRETRAINED LANGUAGE MODELS,0.18484288354898337,"(RoBERTa, ALBERT…)"
PRETRAINED LANGUAGE MODELS,0.1866913123844732,Paper A
PRETRAINED LANGUAGE MODELS,0.18853974121996303,Paper B
PRETRAINED LANGUAGE MODELS,0.19038817005545286,Paper C
PRETRAINED LANGUAGE MODELS,0.1922365988909427,Graph Adapter
PRETRAINED LANGUAGE MODELS,0.19408502772643252,Filter
PRETRAINED LANGUAGE MODELS,0.19593345656192238,"Tokens
[extract]: 
[clean]: 
[change]:"
PRETRAINED LANGUAGE MODELS,0.1977818853974122,Task-speciﬁc
PRETRAINED LANGUAGE MODELS,0.19963031423290203,"node feature
Scores:
0.7 
0.1
0.2"
PRETRAINED LANGUAGE MODELS,0.20147874306839186,"Romdom mask
Combine with prompts"
PRETRAINED LANGUAGE MODELS,0.2033271719038817,Task: which
PRETRAINED LANGUAGE MODELS,0.20517560073937152,"paper can 
extract node"
PRETRAINED LANGUAGE MODELS,0.20702402957486138,feature?
PRETRAINED LANGUAGE MODELS,0.2088724584103512,"(a) training stage
(b) prompting stage"
PRETRAINED LANGUAGE MODELS,0.21072088724584104,Figure 1: Framework of G-Prompt
PRETRAINED LANGUAGE MODELS,0.21256931608133087,"where fLM is a linear transformation with softmax fuction, ˆyi,k ∈N1×T , and T is the size of the
85"
PRETRAINED LANGUAGE MODELS,0.2144177449168207,"vocabulary. The loss function of the fill-mask task is defined as L = CE(ˆyi,k, yi,k), where yi,k is the
86"
PRETRAINED LANGUAGE MODELS,0.21626617375231053,"ID of the masked token, and CE(·, ·) is the cross-entropy loss.
87"
PRETRAINED LANGUAGE MODELS,0.21811460258780038,"Sentence Embedding. The hidden state of the [cls] token (hi,0) and the mean-pooling of all hidden
88"
PRETRAINED LANGUAGE MODELS,0.2199630314232902,"states are commonly used as sentence embeddings [28, 6].
89"
PRETRAINED LANGUAGE MODELS,0.22181146025878004,"Prompting on PLMs. Sentence embedding and token embedding are simultaneously pre-trained
90"
PRETRAINED LANGUAGE MODELS,0.22365988909426987,"in many PLMs. However, due to the gap between pretraining tasks and downstream tasks, sentence
91"
PRETRAINED LANGUAGE MODELS,0.2255083179297597,"embedding always requires fine-tuning for specific tasks. To address this issue, some studies
92"
PRETRAINED LANGUAGE MODELS,0.22735674676524953,"utilize prompts to extract sentence features [13]. For example, suppose we have a paper’s abstract
93"
PRETRAINED LANGUAGE MODELS,0.22920517560073936,"{Abstract}, and the task is to classify the subject of it. We can add some prompts to the sentence:
94"
PRETRAINED LANGUAGE MODELS,0.23105360443622922,"{This, is, a, paper, of, [mask], subject, its, abstract, is, :, Abstract}
(3)"
PRETRAINED LANGUAGE MODELS,0.23290203327171904,"Then this sentence is encoded by PLMs, and we let hi|p denote the hidden state of the [mask] token
95"
PRETRAINED LANGUAGE MODELS,0.23475046210720887,"in prompts. Extensive experiment shows that using prompts can shorten the gap between PLMs and
96"
PRETRAINED LANGUAGE MODELS,0.2365988909426987,"downstream tasks and maximize the utilization of the knowledge PLMs learned during pretraining.
97"
GRAPH NEURAL NETWORKS,0.23844731977818853,"2.3
Graph Neural Networks
98"
GRAPH NEURAL NETWORKS,0.24029574861367836,"Graph Neural Networks (GNNs) have achieved remarkable success in modeling graph-structured
99"
GRAPH NEURAL NETWORKS,0.24214417744916822,"data[30, 7]. The message-passing framework is a commonly used architecture of GNN. At a high
100"
GRAPH NEURAL NETWORKS,0.24399260628465805,"level, GNNs take a set of node features X0 and an adjacency matrix A as input and iteratively capture
101"
GRAPH NEURAL NETWORKS,0.24584103512014788,"neighbors’ information via message-passing. More specifically, for a given node i ∈V , each layer of
102"
GRAPH NEURAL NETWORKS,0.2476894639556377,"message-passing can be expressed as:
103"
GRAPH NEURAL NETWORKS,0.24953789279112754,"xk
i = Pool{fθ(xk−1
j
)|j ∈Ni}
(4)"
GRAPH NEURAL NETWORKS,0.2513863216266174,"where Pool{·} is an aggregation function that combines the features of neighboring nodes, such as
104"
GRAPH NEURAL NETWORKS,0.2532347504621072,"mean-pooling. And Ni denotes the set of neighbors of node i.
105"
GRAPH NEURAL NETWORKS,0.25508317929759705,"3
Method: G-Prompt
106"
GRAPH NEURAL NETWORKS,0.25693160813308685,"Utilizing the information of downstream tasks and graphs is crucial for generating high-quality
107"
GRAPH NEURAL NETWORKS,0.2587800369685767,"node representations. The term “high quality” is inherently task-specific, as exemplified by the
108"
GRAPH NEURAL NETWORKS,0.26062846580406657,"fact that height is a useful feature in predicting user weight but fails to accurately predict age.
109"
GRAPH NEURAL NETWORKS,0.26247689463955637,"Besides, the valuable topological information of TAGs can significantly enhance the understanding
110"
GRAPH NEURAL NETWORKS,0.2643253234750462,"of textual features in TAGs. However, extracting node features using both task and graph information
111"
GRAPH NEURAL NETWORKS,0.266173752310536,"simultaneously is significantly challenging. Current PLMs used for handling textual features are
112"
GRAPH NEURAL NETWORKS,0.2680221811460259,"graph-free, while current graph-based methods employed to extract node features are primarily
113"
GRAPH NEURAL NETWORKS,0.2698706099815157,"task-free. Therefore, this paper proposes a novel self-supervised method, G-Prompt, capable of
114"
GRAPH NEURAL NETWORKS,0.27171903881700554,"extracting task-specific and graph-aware node representations.
115"
OVERVIEW,0.2735674676524954,"3.1
Overview
116"
OVERVIEW,0.2754158964879852,"While previous works have frequently employed PLMs to process TAGs, these investigations have
117"
OVERVIEW,0.27726432532347506,"been constrained in extracting a broad node representation from the text-based characteristics and
118"
OVERVIEW,0.27911275415896486,"have not incorporated task-specific prior knowledge. Consequently, additional learning supervision
119"
OVERVIEW,0.2809611829944547,"via GNNs is needed to enable the effective adaptation of these node representations to downstream
120"
OVERVIEW,0.2828096118299446,"tasks. To address this limitation, the paper suggests incorporating prompts and PLMs into the process
121"
OVERVIEW,0.2846580406654344,"of extracting task-relevant node features from TAGs. Nevertheless, PLMs only utilize contextual
122"
OVERVIEW,0.28650646950092423,"information to generate the prompts-related output, which may be insufficient for handling TAGs.
123"
OVERVIEW,0.28835489833641403,"Graph structures often contain essential information that can facilitate a better understanding of
124"
OVERVIEW,0.2902033271719039,"textual features. For instance, in a citation network, a masked sentence such as “This paper focuses
125"
OVERVIEW,0.2920517560073937,"on [MASK] learning in AI domain” could have multiple candidate tokens based solely on context.
126"
OVERVIEW,0.29390018484288355,"However, if many papers related to graphs are cited, we can infer with greater confidence that the
127"
OVERVIEW,0.2957486136783734,"masked token is likely “graph”. At present, PLMs operate solely based on context, and their structure
128"
OVERVIEW,0.2975970425138632,"is graph-free. Directly incorporating graph information into PLMs by prompts is not feasible because
129"
OVERVIEW,0.29944547134935307,"limited prompts cannot describe the entire topological structure adequately.
130"
OVERVIEW,0.30129390018484287,"Therefore, the proposed G-Prompt leverages a self-supervised based graph adapter and prompts to
131"
OVERVIEW,0.3031423290203327,"make PLMs aware of the graph information and downstream task. Given a specific TAG, the pipeline
132"
OVERVIEW,0.3049907578558225,"of G-Prompt is as follows: (1) Training an adapter on the given TAG to make PLMs graph-aware.
133"
OVERVIEW,0.3068391866913124,"Specifically, we propose a graph adapter that operates on the prediction layer of PLMs to assist
134"
OVERVIEW,0.30868761552680224,"in capturing graph information, which is fine-tuned by the fill-mask task based on the textual data
135"
OVERVIEW,0.31053604436229204,"contained by the given TAG. (2) Employing task-specific prompts and fine-tuned graph adapters to
136"
OVERVIEW,0.3123844731977819,"generate task-aware and graph-aware node features.
137"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3142329020332717,"3.2
Fine-Tuning PLMs with the Graph Adapter
138"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.31608133086876156,"Using adapters to enable PLMs to perceive graph information is a straightforward idea. However,
139"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3179297597042514,"unlike adapters used for downstream task fine-tuning [11, 18], the graph adapter is used to combine
140"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3197781885397412,"prompts in order to extract task-relevant node representations. This is an unsupervised process, which
141"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.32162661737523107,"means that the graph adapter only receives self-supervised training on given TAGs. Consequently,
142"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3234750462107209,"the most challenging aspect of graph adapters is how to assist PLMs in perceiving graph information
143"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.32532347504621073,"while also maintaining their contextual understanding capability. Additionally, the graph adapter
144"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.32717190388170053,"is only trained on a given TAG, generalizing to prompt tokens can also be quite difficult. Next, we
145"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3290203327171904,"introduce the graph adapter and discuss how it overcomes these challenges in detail.
146"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.33086876155268025,"Context-friendly adapter placement. The fill-mask task involves two modules of PLMs: a
147"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.33271719038817005,"transformer-based module that models context information to obtain representations of masked
148"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3345656192236599,"tokens and a linear transformation that decodes the representation to output the probable IDs of the
149"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3364140480591497,"masked token. To avoid compromising the contextual modeling ability of PLMs, the Graph Adapter
150"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.33826247689463956,"only perform on the last layer of PLMs. More specifically, the graph adapter is a GNN structure
151"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.34011090573012936,"combing with the pre-trained final layer of the PLMs. Given a specific masked token ˆsi,k, The inputs
152"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3419593345656192,"of the Graph Adapter are the masked token ˆhi,k, sentence representations of node i and its neighbors
153"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3438077634011091,"and output is the prediction of the IDs’ of the masked token. This process aligns with intuition —
154"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3456561922365989,"inferring a possible token based on context first and then determining the final token based on graph
155"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.34750462107208874,"information. Formally,
156"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.34935304990757854,"ˆyi,k = GraphAdapter{fLM, ˆhi,k, zi, {zj ∈Ni}, Θ},
(5)
where the zi and zj denote the sentence embedding of node i and j. Note, sentence embedding is
157"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3512014787430684,"task-free and only used to model nodes’ influence on their neighbor. In this paper, we utilize sentence
158"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.35304990757855825,"embedding of nodes’ textual features as their node feature. Θ is all trainable parameters of the Graph
159"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.35489833641404805,"Adapter.
160"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3567467652495379,"Prompting-friendly network structure. The parameters of the adapter are only trained on the
161"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3585951940850277,"fill-mask task based on the textual data contained by the target TAG. But the adapter will be used for
162"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.36044362292051757,"combining prompts to generate task-related node features in various subsequent downstream tasks.
163"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.36229205175600737,"So the generalization ability of the adapter is crucial. On the one hand, the distribution of hidden
164"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.36414048059149723,"states responding to masked tokens in prompts may be different from the hidden states used to train
165"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3659889094269871,"the adapter. On the other hand, the candidate tokens for task-specific prompts may not appear in the
166"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3678373382624769,"tokens of the TAG. Therefore, we carefully design the network structure of the graph adapter and
167"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.36968576709796674,"utilize the pre-trained prediction layer of PLM to improve the generalization ability of it.
168"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.37153419593345655,"When it comes to the graph adapter’s training stage, it’s possible that the hidden states associated with
169"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3733826247689464,"certain prompts may not be present. This means that directly manipulating those hidden states could
170"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3752310536044362,"result in overfitting on the tokens already present in the TAGs. Therefore, the graph adapter models
171"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.37707948243992606,"the influence of each modeled node on the distribution of surrounding neighbor tokens based on node
172"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3789279112754159,"feature, which remains unchanged when prompts are added. Considering that some tokens can be
173"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3807763401109057,"predicted well based solely on their context and that different neighbors may have different influences
174"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3826247689463956,"on the same node, the impact of a neighbor on a token is determined jointly by a gate mechanism and
175"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3844731977818854,"the token’s context. Give a specific node i, it’s neighbor j, and hidden states of a masked token ˆhi,j,
176"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.38632162661737524,"˜hi,k,j = aijˆhi,k + (1 −aij)g(zj, Θg)
(6)"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.38817005545286504,"where aij = sigmoid((ziWq)(zjWk)T ). Here, g(·) represents multi-layer perceptions (MLPs) with
177"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3900184842883549,"parameters Θg that model the influence of node j. It is worth noting that when considering the entire
178"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.39186691312384475,"graph, g(zj, Θg) will be combined with many marked tokens of node j’s neighbors, which can help
179"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.39371534195933455,"to prevent g(zj, Θg) from being overfitted on a few tokens.
180"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3955637707948244,"Subsequently, the graph adapter combines all neighbor influence to infer the final prediction result.
181"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.3974121996303142,"Since the prediction layer of PLM (i.e., fLM(·)) is well-trained on massive tokens, it also contains an
182"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.39926062846580407,"amount of knowledge. Therefore, the graph adapter reuses this layer to predict the final result.
183"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.4011090573012939,"˜yi,k = Pool{fLM(˜hi,k,j)|j ∈Ni},
(7)
In this equation, the Pool(·) used in this paper is mean-pooling. It is worth noting that the position
184"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.4029574861367837,"of fLM(·) can be interchanged with pooling since it is just a linear transformation. All trainable
185"
FINE-TUNING PLMS WITH THE GRAPH ADAPTER,0.4048059149722736,"parameters in the graph adapter are denoted by Θ = {Θg, Wq, Wk}.
186"
MODEL OPTIMIZATION OF G-PROMPT,0.4066543438077634,"3.3
Model optimization of G-Prompt
187"
MODEL OPTIMIZATION OF G-PROMPT,0.40850277264325324,"The graph adapter is optimized by the original fill-mask loss, Li,k = CE(˜yi,k, yi,k), where ˆyi,k is the
188"
MODEL OPTIMIZATION OF G-PROMPT,0.41035120147874304,"predicted probability of the k-th masked token for the node i and yi,k is the true label. We aim to
189"
MODEL OPTIMIZATION OF G-PROMPT,0.4121996303142329,"minimize Li,k with respect to Θ.
190"
MODEL OPTIMIZATION OF G-PROMPT,0.41404805914972276,"However, in actual optimization, the prediction results of ˜yi,k,j = fLM(˜hi,k,j) consist of many small
191"
MODEL OPTIMIZATION OF G-PROMPT,0.41589648798521256,"values because of the large vocabulary size of the language model. Therefore, using mean-pooling
192"
MODEL OPTIMIZATION OF G-PROMPT,0.4177449168207024,"presents a significant problem as it is insensitive to these small values. For example, during some
193"
MODEL OPTIMIZATION OF G-PROMPT,0.4195933456561922,"stages of the optimization process, a node may have mostly 0.9 predictions for the ground truth based
194"
MODEL OPTIMIZATION OF G-PROMPT,0.4214417744916821,"on each edge, with only a few being 0.1. Averaging them together would result in a very smooth loss,
195"
MODEL OPTIMIZATION OF G-PROMPT,0.4232902033271719,"making it difficult to train the influence of neighbors with temporarily predicted values of 0.1. To
196"
MODEL OPTIMIZATION OF G-PROMPT,0.42513863216266173,"address this issue, we use geometric mean instead of mean-pooling in the finetuning stage of the
197"
MODEL OPTIMIZATION OF G-PROMPT,0.4269870609981516,"graph adapter, which is more sensitive to small values. It is easy to prove that the geometric mean of
198"
MODEL OPTIMIZATION OF G-PROMPT,0.4288354898336414,"positive numbers is smaller than the arithmetic means, making it harder to smooth and helping the
199"
MODEL OPTIMIZATION OF G-PROMPT,0.43068391866913125,"model converge faster. formally, in finetuning stage, the loss function is:
200"
MODEL OPTIMIZATION OF G-PROMPT,0.43253234750462105,"Li,k = −yi,k ⊙log{(
Y"
MODEL OPTIMIZATION OF G-PROMPT,0.4343807763401109,"j∈Ni
˜yi,k,j)1/|Ni|} = −
X j∈Ni"
MODEL OPTIMIZATION OF G-PROMPT,0.43622920517560076,"1
|Ni|yi,k ⊙log(˜yi,k,j)
(8)"
MODEL OPTIMIZATION OF G-PROMPT,0.43807763401109057,"On the right-hand side of the equation, we are essentially minimizing ˜yi,k,j through the cross-entropy
201"
MODEL OPTIMIZATION OF G-PROMPT,0.4399260628465804,"loss Li,k,j =
1
|Ni|CE(˜yi,k,j, yi,k). It is worth noting that the graph adapter is only performed on the
202"
MODEL OPTIMIZATION OF G-PROMPT,0.4417744916820702,"last layer of PLMs. As a result, we can sample a set of masked tokens and preserve their hidden states
203"
MODEL OPTIMIZATION OF G-PROMPT,0.4436229205175601,"inferred by the PLMs before training. This implies that training of graph adapters can be achieved
204"
MODEL OPTIMIZATION OF G-PROMPT,0.4454713493530499,"with very few computing resources.
205"
PROMPT-BASED NODE REPRESENTATIONS,0.44731977818853974,"3.4
Prompt-based Node Representations
206"
PROMPT-BASED NODE REPRESENTATIONS,0.4491682070240296,"After training the graph adapter, it can be combined with task-specific prompts to generate task-
207"
PROMPT-BASED NODE REPRESENTATIONS,0.4510166358595194,"specific and graph-aware node representations. Similar to other prompt-based approaches, we simply
208"
PROMPT-BASED NODE REPRESENTATIONS,0.45286506469500926,"add task-specific prompts directly into the textual feature. For example, we might use the prompt
209"
PROMPT-BASED NODE REPRESENTATIONS,0.45471349353049906,"“This is a [MASK] user, consider their profile: [textual feature].” Formally, this process can be
210"
PROMPT-BASED NODE REPRESENTATIONS,0.4565619223659889,"expressed as ˆhi|p = PLM({[P0], [P1]...[MASK], Si}). Where, ˆhi|p represents the hidden state of
211"
PROMPT-BASED NODE REPRESENTATIONS,0.4584103512014787,"the inserted [MASK], while [P0], [P1]... refers to the task-specific prompts. The resulting hidden
212"
PROMPT-BASED NODE REPRESENTATIONS,0.4602587800369686,"state is then fed into the graph encoder to decode the most probable token.
213"
PROMPT-BASED NODE REPRESENTATIONS,0.46210720887245843,"ˆyi|p = Pool{fLM(ai,jˆhi|p + (1 −ai,j)g(zj, Θg))|j ∈Ni}
(9)"
PROMPT-BASED NODE REPRESENTATIONS,0.46395563770794823,Table 1: Statistics of the datasets
PROMPT-BASED NODE REPRESENTATIONS,0.4658040665434381,"Dataset
# Nodes
# Eeges
Avg. Node Degree
Test Ratio (%)
Metric
Arxiv
169,343
1,166,243
13.7
28
ACC
Instagram
11,339
377,812
66.6
60
ROC
Reddit
33,434
198,448
11.9
33
ROC"
PROMPT-BASED NODE REPRESENTATIONS,0.4676524953789279,"ˆyi|p is a |D|-dimensional vector, where |D| is the size of the PLM vocabulary. Therefore, directly
214"
PROMPT-BASED NODE REPRESENTATIONS,0.46950092421441775,"using this prediction result for node representation is not conducive to downstream tasks and storage.
215"
PROMPT-BASED NODE REPRESENTATIONS,0.4713493530499076,"Thus, we use the filtered results as node features, denoted by xi|p = Filter(ˆyi|p). Note, each
216"
PROMPT-BASED NODE REPRESENTATIONS,0.4731977818853974,"dimension represents the probability of a token being inferred by PLMs with the graph adapter based
217"
PROMPT-BASED NODE REPRESENTATIONS,0.47504621072088726,"on node textual features, neighbors’ information, and task-respected prompts. Intuitively, tokens that
218"
PROMPT-BASED NODE REPRESENTATIONS,0.47689463955637706,"are unrelated to downstream tasks are almost the same for all nodes. Therefore, for Yp ∈N|V |×|D|,
219"
PROMPT-BASED NODE REPRESENTATIONS,0.4787430683918669,"which denotes prediction results of all nodes. This paper sorts all columns of Yp in descending order
220"
PROMPT-BASED NODE REPRESENTATIONS,0.4805914972273567,"of standard deviation and keeps the top M columns as the node features. Note, we can also manually
221"
PROMPT-BASED NODE REPRESENTATIONS,0.4824399260628466,"select task-relevant tokens based on prior knowledge of the task and use them as node features.
222"
EXPERIMENT,0.48428835489833644,"4
Experiment
223"
EXPERIMENT SETUP,0.48613678373382624,"4.1
Experiment setup
224"
EXPERIMENT SETUP,0.4879852125693161,"Dataset. We conduct experiments on three public and real-world datasets, which are Ogbn-arxiv[12]
225"
EXPERIMENT SETUP,0.4898336414048059,"(shorted as Arxiv), Instagram[14], and Reddit1, to evaluate the effectiveness of the proposed method
226"
EXPERIMENT SETUP,0.49168207024029575,"G-Prompt. Specifically, Ogbn-arxiv is a citation network where edges represent citation relationships,
227"
EXPERIMENT SETUP,0.49353049907578556,"nodes represent papers and the text attribute is the abstracts of papers. The task is to predict paper
228"
EXPERIMENT SETUP,0.4953789279112754,"subjects. Instagram is a social network where edges represent following relationships, nodes represent
229"
EXPERIMENT SETUP,0.49722735674676527,"users, and the prediction task is to classify commercial users and normal users in this network. The
230"
EXPERIMENT SETUP,0.49907578558225507,"text attribute is the users’ profile. Reddit is also a social network where each node denotes a user, the
231"
EXPERIMENT SETUP,0.5009242144177449,"node features are the content of users’ historically published subreddits, and edges denote whether
232"
EXPERIMENT SETUP,0.5027726432532348,"two users have replied to each other. The prediction task is to classify whether a user is in the top
233"
EXPERIMENT SETUP,0.5046210720887245,"50% popular (average score of all subreddits). Table 1 shows detailed statistics of these datasets.
234"
EXPERIMENT SETUP,0.5064695009242144,"More details about Instagram and Reddit are provided in the Appendix.
235"
EXPERIMENT SETUP,0.5083179297597042,"Compared methods. We compare the proposed G-Prompt with PLM-based and Graph-based node
236"
EXPERIMENT SETUP,0.5101663585951941,"feature-extracting methods. For the PLM-based methods, we consider three options: (1) direct use of
237"
EXPERIMENT SETUP,0.512014787430684,"sentence embedding as node features, and (2) use of the hidden states of masked tokens based on hard
238"
EXPERIMENT SETUP,0.5138632162661737,"prompts as node features. (3) use of the prediction result of masked tokens based on prompts as node
239"
EXPERIMENT SETUP,0.5157116451016636,"feature. For graph-based methods, we compare our proposed method with GAE and GIANT, which
240"
EXPERIMENT SETUP,0.5175600739371534,"first conduct self-supervised learning on graphs to train PLMs or node feature encoders. To ensure a
241"
EXPERIMENT SETUP,0.5194085027726433,"fair comparison, we add prompts into graph-based baselines. Except for GAINT and OGB features,
242"
EXPERIMENT SETUP,0.5212569316081331,"the PLM we use in this paper is RoBERTa-Large[20]. Note that all prompts used in baselines are the
243"
EXPERIMENT SETUP,0.5231053604436229,"same as those in G-Prompt.
244"
EXPERIMENT SETUP,0.5249537892791127,"Implementation details. For G-Prompt, we first train three graph adapters of G-Prompt on Arxiv,
245"
EXPERIMENT SETUP,0.5268022181146026,"Instagram, and Reddit with 50 epochs, 100 epochs, and 100 epochs respectively. All of them are
246"
EXPERIMENT SETUP,0.5286506469500925,"optimized using AdamW[21] with warm-up. For more details on the hyper-parameter settings, please
247"
EXPERIMENT SETUP,0.5304990757855823,"refer to the Appendix. For each node, we replace 10% tokens with [mask] and use these masked
248"
EXPERIMENT SETUP,0.532347504621072,"tokens to train the graph adapter. During the whole training stage, all task-related prompts are
249"
EXPERIMENT SETUP,0.5341959334565619,"invisible. Then we use prompts, finetuned graph adapters, and PLMs to jointly extract node features.
250"
EXPERIMENT SETUP,0.5360443622920518,"For graph-based methods, we train them on each dataset with searched hyper-parameters.
251"
FEW-SHOT LEARNING,0.5378927911275416,"4.2
Few-shot learning
252"
FEW-SHOT LEARNING,0.5397412199630314,"To evaluate the performance of representations generated by different methods in few-shot learning,
253"
FEW-SHOT LEARNING,0.5415896487985212,"we compare the performance of different representations at different shot numbers based on the same
254"
FEW-SHOT LEARNING,0.5434380776340111,"GNN backbone. The GNN backbone used in the performance comparison on different shot numbers is
255"
FEW-SHOT LEARNING,0.5452865064695009,"GraphSAGE[30]. In addition, we also compare the performance of different representations combined
256"
FEW-SHOT LEARNING,0.5471349353049908,"with three different neural network architectures (i.e., MLP, and RevGAT[17]) on downstream tasks
257"
FEW-SHOT LEARNING,0.5489833641404805,1https://convokit.cornell.edu/documentation/subreddit.html
FEW-SHOT LEARNING,0.5508317929759704,Table 2: The performance in different shots on three datasets
FEW-SHOT LEARNING,0.5526802218114603,"Dataset
Arxiv
Instagram
Reddit
# shots per class
10
50
100
10
50
100
10
50
100"
FEW-SHOT LEARNING,0.5545286506469501,"OGB-Feature
0.4576 ±0.0324 0.5495 ±0.0171 0.5875 ±0.0146
-
-
-
-
-
-"
FEW-SHOT LEARNING,0.55637707948244,"PLM+GAE
0.5016 ±0.0510 0.5608 ±0.0101 0.5810 ±0.0125 0.5258 ±0.0635 0.5818 ±0.0101 0.5821 ±0.0058 0.5653 ±0.0256 0.6019 ±0.0174 0.6154 ±0.0128"
FEW-SHOT LEARNING,0.5582255083179297,PLM+GAE+prompt 0.5189 ±0.0333 0.5801 ±0.0102 0.6063 ±0.0109 0.5418 ±0.0298 0.5705 ±0.0233 0.5867 ±0.0100 0.5619 ±0.0425 0.5968 ±0.0237 0.6173 ±0.0160
FEW-SHOT LEARNING,0.5600739371534196,"GIANT
0.5050 ±0.0308 0.5798 ±0.0119 0.6081 ±0.0109 0.5185 ±0.0323 0.5601 ±0.0304 0.5752 ±0.0251 0.5618 ±0.0431 0.5954 ±0.0131 0.6130 ±0.0117"
FEW-SHOT LEARNING,0.5619223659889094,"GIANT + prompt
0.5140 ±0.0320 0.5809 ±0.0223 0.6126 ±0.0159 0.5239 ±0.0309 0.5721 ±0.0361 0.5949 ±0.0089 0.5661 ±0.0459 0.5968 ±0.0096 0.6145 ±0.0105"
FEW-SHOT LEARNING,0.5637707948243993,"PLM-cls
0.4697 ±0.0577 0.5414 ±0.0400 0.5869 ±0.0300 0.5165 ±0.0217 0.5385 ±0.0344 0.5690 ±0.0253 0.4965 ±0.0373 0.5236 ±0.0394 0.5754 ±0.0348"
FEW-SHOT LEARNING,0.5656192236598891,PLM-Prompt-dense 0.5117 ±0.0398 0.5631 ±0.0352 0.5865 ±0.0296 0.5458 ±0.0420 0.5796 ±0.0276 0.6055 ±0.0122 0.5363 ±0.0530 0.5648 ±0.0385 0.5998 ±0.0383
FEW-SHOT LEARNING,0.5674676524953789,PLM-Prompt-sparse 0.5201 ±0.0284 0.5784 ±0.0213 0.6085 ±0.0203 0.5363 ±0.0348 0.5757 ±0.0225 0.5910 ±0.0229 0.5403 ±0.0424 0.5761 ±0.0359 0.6082 ±0.0192
FEW-SHOT LEARNING,0.5693160813308688,"G-Prompt
0.5248±0.0382 0.5927 ±0.0142 0.6167 ±0.0138 0.5576 ±0.0330 0.5917 ±0.0242 0.6090 ±0.0135 0.5728 ±0.0491 0.6167±0.0289 0.6472 ±0.0224"
FEW-SHOT LEARNING,0.5711645101663586,G-Prompt w/o gate 0.5291 ±0.0315 0.5877 ±0.0192 0.6212 ±0.0190 0.5507±0.0336 0.5706 ±0.0262 0.5942 ±0.0178 0.5501 ±0.0604 0.5926±0.0385 0.6361±0.0268
FEW-SHOT LEARNING,0.5730129390018485,G-Prompt w/o graph 0.5226 ±0.0322 0.5880±0.0168 0.6059 ±0.0101 0.5234 ±0.0236 0.5657 ±0.0377 0.5914 ±0.0199 0.5536 ±0.0438 0.5683 ±0.0390 0.6054 ±0.0263
FEW-SHOT LEARNING,0.5748613678373382,G-Prompt w/o SSL 0.5210 ±0.0372 0.5793 ±0.0168 0.6092 ±0.0168 0.5378 ±0.0419 0.5801±0.0269 0.6004±0.0193 0.5494 ±0.0502 0.5885 ±0.0365 0.6149 ±0.0263
FEW-SHOT LEARNING,0.5767097966728281,"with the same number of shots. For Arxiv, we use a publicly available partitioned test set, while for
258"
FEW-SHOT LEARNING,0.5785582255083179,"Instagram and Reddit, we randomly sample 60% and 33% of the data as the test sets, respectively.
259"
FEW-SHOT LEARNING,0.5804066543438078,"To consider the randomness of partitioning and training, each experimental result is based on five
260"
FEW-SHOT LEARNING,0.5822550831792976,"random partitions (the partitions are the same for different baselines), the experiment is repeated five
261"
FEW-SHOT LEARNING,0.5841035120147874,"times for each partition, and the variance of 5×5 results is reported.
262"
FEW-SHOT LEARNING,0.5859519408502772,"The experiment results on different shots-num are shown in Table 2. The experiment shows that: (1)
263"
FEW-SHOT LEARNING,0.5878003696857671,"Graph-aware can improve the performance of node representation. In general, approaches that
264"
FEW-SHOT LEARNING,0.589648798521257,"use sentence representations or those that involve self-supervised training with graph information
265"
FEW-SHOT LEARNING,0.5914972273567468,"tend to outperform non-trained representations. For example, GAE shows an average improvement
266"
FEW-SHOT LEARNING,0.5933456561922366,"of avg. 6.2% compared to RoBERTa’s [cls], and GIANT shows avg. 6.2% improvement over cls
267"
FEW-SHOT LEARNING,0.5951940850277264,"representation. For graph-based self-supervised tasks, fine-tuning language models might be more
268"
FEW-SHOT LEARNING,0.5970425138632163,"suitable for larger datasets. GIANT outperforms GAE by avg. 3.0% on Arxiv, but lags behind
269"
FEW-SHOT LEARNING,0.5988909426987061,"by avg. 1.4% on Instagram and Reddit. (2) Downstream task-related prompts can improve
270"
FEW-SHOT LEARNING,0.600739371534196,"performance for all models. For graph-free language models, prompt-based representations can
271"
FEW-SHOT LEARNING,0.6025878003696857,"improve performance by avg. 5.7%, and the overall performance of prediction values and hidden
272"
FEW-SHOT LEARNING,0.6044362292051756,"states corresponding to prompts is similar. For graph-based methods, prompts in GAE improve
273"
FEW-SHOT LEARNING,0.6062846580406654,"performance by avg. 1.3%, while prompts in GIANT lead to an average improvement of avg. 1.2%.
274"
FEW-SHOT LEARNING,0.6081330868761553,"However, we note that prompts are unstable for graph-based pre-trained models. GAE shows a decline
275"
FEW-SHOT LEARNING,0.609981515711645,"in 4 experiments, while prompts only bring a slight improvement in GIANT (compared to language
276"
FEW-SHOT LEARNING,0.6118299445471349,"models). (3) Our method is capable of utilizing both graph perception and downstream task
277"
FEW-SHOT LEARNING,0.6136783733826248,"prompts simultaneously, achieving state-of-the-art performance. Compared to PLM representations
278"
FEW-SHOT LEARNING,0.6155268022181146,"without prompts, our method improves by avg. 10.6%. Compared to PLM-prompt, it improves by
279"
FEW-SHOT LEARNING,0.6173752310536045,"avg. 4.6%, and compared to GIANT, it improves by avg. 4.1%.
280"
FEW-SHOT LEARNING,0.6192236598890942,"Besides, as Figure 2 shows, the node representation extracted by G-Prompt in different GNN-
281"
FEW-SHOT LEARNING,0.6210720887245841,"backbone also achieves the SOTA performance compared to other baseline methods.
282"
FEW-SHOT LEARNING,0.6229205175600739,"(a) Arxiv
(b) Instagram
(c) Reddit"
FEW-SHOT LEARNING,0.6247689463955638,Figure 2: Comparison with different GNN backbone on 50-shots setting
IN-DEPTH ANALYSIS OF G-PROMPT,0.6266173752310537,"4.3
In-depth analysis of G-Prompt
283"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6284658040665434,"To validate the rationality of G-Prompt, we conduct experiments to compare the performance of
284"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6303142329020333,"G-Prompt and its variants. These variants include removing the gate mechanism in graph-adapter
285"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6321626617375231,"(denoted as “w/o gate”), keeping only self-loops while removing the input graph (denoted as “w/o
286"
IN-DEPTH ANALYSIS OF G-PROMPT,0.634011090573013,"graph”), and not training graph-adapter by self-supervised learning (denoted as “w/o SSL”). The
287"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6358595194085028,"experimental results show that all variants perform worse than G-Prompt. Specifically, removing the
288"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6377079482439926,"Graph-Adapter training process leads to avg. 2.8% decrease in performance, which demonstrates the
289"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6395563770794824,"effectiveness of training graph-adapter through the fill- mask task. After removing the graph input,
290"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6414048059149723,"the performance of G-Prompt decreases by avg. 3.8%, which further confirms that the improvement
291"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6432532347504621,"provided by G-Prompt, compared to using language model prompts directly, stems from the graph
292"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6451016635859519,"adapter’s ability to assist language models in comprehending graph structures. Moreover, removing
293"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6469500924214417,"the gate mechanism results in a avg. 1.8% decrease in performance, indicating that the design of the
294"
IN-DEPTH ANALYSIS OF G-PROMPT,0.6487985212569316,"graph-adapter structure is reasonable.
295"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6506469500924215,"4.4
Zero-shot node classification and interpretability
296"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6524953789279113,"The node features generated through GPrompt represent the probability of each possible word
297"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6543438077634011,"for nodes given task-related prompts, where each dimension corresponds to a specific word. This
298"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6561922365988909,"probability generation incorporates prior knowledge from PLMs, graph information, and node context.
299"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6580406654343808,"Two natural questions arise: How much knowledge is contained within this word probability?
300"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6598890942698706,"Whether the node feature can help us interpret the downstream task? Therefore, we further
301"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6617375231053605,"conduct zero-shot node classification experiments on node representations. Meanwhile, we conduct a
302"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6635859519408502,"case study on Instagram.
303"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6654343807763401,"Zero-shot node classification. We select different sets of candidate words and sum up the probabili-
304"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.66728280961183,"ties of each word in the set to obtain the prediction result for node classification. We employ the ROC
305"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6691312384473198,"as the evaluation metric to assess the performance of node classification. For simplicity, ArXiv dataset
306"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6709796672828097,"only selects two categories, “Artificial Intelligence” and “Linguistics and Language”. The other
307"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6728280961182994,"two datasets remain unchanged. We select completely random, bag-of-words, RoBERTa-base, and
308"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6746765249537893,"RoBERTa-large as baselines, using the same prompts as G-Prompt for PLMs. We provide experiment
309"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6765249537892791,"results of G-Prompt based on RoBERTa-base (Ours-B) and RoBERTa-large (Ours-L).
310"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.678373382624769,"According to the results shown in Table 3. (1) The bag-of-words method has almost no predictive
311"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6802218114602587,"ability. (2)The PLM through Prompts has predictive ability on different tasks (improvement compared
312"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6820702402957486,"to BOW by avg. 13%). But there is a performance difference between base and large even with
313"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6839186691312384,"the same prompt due to the sensitivity of language models to prompts [22]. (3) Compared to a
314"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6857670979667283,"language model, G-Prompt shows significant performance improvement. Specifically, G-Prompt-base
315"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6876155268022182,"improved avg. 2.7% compared to the language model. However, it should be noted that the basic
316"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6894639556377079,"predictive ability of the language model and G-Prompt are correlated. Specifically, the correlation
317"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6913123844731978,"coefficient between the results of GPrompt-L and LM-L is 0.64, while the correlation coefficient with
318"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6931608133086876,"LM-B is 0.84. (4) Moreover, selecting more candidate words through prior knowledge can effectively
319"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6950092421441775,"help G-Prompt improve its zero-shot capability, with an average improvement of avg. 4.8% for the
320"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6968576709796673,"base and avg. 5.3% for the large. However, there is no significant improvement for language models
321"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.6987060998151571,"and bag-of-words. Surprisingly, by adding a small number of candidate words, G-Prompt’s zero-shot
322"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7005545286506469,"performance is already close to or even sometimes surpasses supervised training with 100 shots. This
323"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7024029574861368,"result indicates that combining language models and graphs for zero-shot learning on TAG is feasible.
324"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7042513863216266,"Interpretability. The task on Instagram is to determine whether a node is a commercial user. We use
325"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7060998151571165,"the probability corresponding to each token as the prediction value, calculate its corresponding ROC
326"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7079482439926063,"of prediction performance, and then display the top 7 tokens with the highest scores. For comparison,
327"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7097966728280961,"we also show the scores of tokens corresponding to RoBERTa-Large under the same prompt. Overall,
328"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.711645101663586,"the top 7 tokens given by our model have considerably higher ROC scores than RoBERTa-Large
329"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7134935304990758,"resulting in avg. 7.0% improvement. Additionally, our results are intuitive and can even help explain
330"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7153419593345656,"the task, for example, “premium.” Based on this result, we search and find that there are “premium
331"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7171903881700554,"creator subscriptions” on Instagram, which means “Users can set their own prices and earn extra cash
332"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7190388170055453,"each month,”2 and this information is indeed related to commercial activity. Similarly, “niche” is also
333"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7208872458410351,"a word related to Instagram business behavior.
334"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.722735674676525,2https://www.pcmag.com/news/instagram-introduces-premium-creator-subscriptions
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7245841035120147,Table 3: The performance of different models in zero-shot learning
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7264325323475046,"Dataset
Pos. vocab
Neg. vocab
Rand.
BOW
LM-B
LM-L
Ours-B
Ours-L
100 shot. Arxiv"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7282809611829945,"{intellectual}
{language}
0.5021
±0.0124
0.4994
±0.0000
0.5955
±0.0000
0.6747
±0.0000"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7301293900184843,"0.5840
±0.0000
0.6765∗"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7319778188539742,"±0.0000
0.9040
±0.0253
{intellectual,
decision,
logic, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7338262476894639,"{language,
translation,
speech, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7356746765249538,"0.4988
±0.0139
0.5474
±0.0000
0.6284
±0.0000"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7375231053604436,"0.6075
±0.0000"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7393715341959335,"0.6006
±0.0000
0.7064∗"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7412199630314233,±0.0000
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7430683918669131,Instagram
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.744916820702403,"{commercial}
{normal}
0.5004
±0.0151
0.5001
±0.0007
0.5509∗"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7467652495378928,"±0.0163
0.5365
±0.0054"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7486136783733827,"0.5403
±0.0078"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7504621072088724,"0.5382
±0.0095
0.5690
±0.0253
{commercial,
sponsored,
brand, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7523105360443623,"{normal,
personality,
private, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7541589648798521,"0.5007
±0.0131
0.5022
±0.0008
0.5586
±0.0117
0.5577
±0.0068"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.756007393715342,0.5995∗
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7578558225508318,"±0.0074
0.5957
±0.0081"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7597042513863216,Reddit
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7615526802218114,"{pretty}
{simple}
0.5034
±0.0073
0.5053
±0.0019
0.5608
±0.0050
0.5352
±0.0027"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7634011090573013,"0.5630
±0.0082"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7652495378927912,0.5673∗
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.767097966728281,"±0.0070
0.5754
±0.0348
{pretty,
hilarious,
funny, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7689463955637708,"{simple,
anonymous,
standard, ...}"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7707948243992606,"0.4990
±0.0042
0.5034
±0.0017
0.5604
±0.0081
0.5587
±0.0052"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7726432532347505,"0.5674
±0.0058"
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7744916820702403,0.5742∗
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7763401109057301,±0.0066
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7781885397412199,Table 4: Top 7 Tokens related to predicting commercial users on Instagram
ZERO-SHOT NODE CLASSIFICATION AND INTERPRETABILITY,0.7800369685767098,"RoBERTa-large
G-Prompt
Top 7 tokens
ROC
Top 7 tokens
ROC
critical
0.546
special
0.592
convenient
0.542
convenient
0.579
terrific
0.542
premium
0.579
banner
0.542
unique
0.577
gateway
0.539
great
0.575
compelling
0.539
pioneer
0.575
neat
0.538
niche
0.575"
RELATED WORK,0.7818853974121996,"5
Related work
335"
RELATED WORK,0.7837338262476895,"Modeling TAGs involves numerous works related to the NLP domain and Graph domain. Currently,
336"
RELATED WORK,0.7855822550831792,"pre-trained language models are the primary method for modeling the textual information in text-as-
337"
RELATED WORK,0.7874306839186691,"graphs [25]. Presently, pre-trained language models are mainly based on transformer structures[29],
338"
RELATED WORK,0.789279112754159,"with a variety of pre-training methods, such as fill-mask [4], paragraph prediction[4], adversarial
339"
RELATED WORK,0.7911275415896488,"learning[10], and auto-regressive learning[27]. Based on these tasks, many excellent pre-trained
340"
RELATED WORK,0.7929759704251387,"models have emerged, including BERT[4], RoBERTa[20], and GPT3[1]. PLMs contain an amount
341"
RELATED WORK,0.7948243992606284,"of knowledge acquired through extensive pre-training data[31]. Recently, using prompts has been
342"
RELATED WORK,0.7966728280961183,"proposed to better utilize the performance of pre-trained language models[1]. Based on this finding,
343"
RELATED WORK,0.7985212569316081,"prompt learning[19, 8, 16] has achieved impressive results in few-shot and zero-shot learning and has
344"
RELATED WORK,0.800369685767098,"been widely applied by other domains. Currently, the structural information in modeling TAGs is
345"
RELATED WORK,0.8022181146025879,"primarily modeled through GNNs, such as GraphSAGE[9], GAT[30], APPNP[7, 5] and RevGAT[17],
346"
RELATED WORK,0.8040665434380776,"and there are also many pre-training tasks on graphs such as GAE[15], GraphCL[33] that can be
347"
RELATED WORK,0.8059149722735675,"extended to TAGs. Recently, many methods explore better utilizing the knowledge of PLMs to model
348"
RELATED WORK,0.8077634011090573,"TAGs more effectively, such as pre-training language models through graph-related tasks [3] and
349"
RELATED WORK,0.8096118299445472,"finetuning PLMs together with GNNs via knowledge distillation[24] or variational inference [35].
350"
CONCLUSION,0.8114602587800369,"6
Conclusion
351"
CONCLUSION,0.8133086876155268,"This paper proposes G-Prompt to fuse PLMs and Graphs for extracting task-specific and graph-aware
352"
CONCLUSION,0.8151571164510166,"node representation in TAGs. G-Prompt have two-stage: (1) self-supervised train a graph adapter to
353"
CONCLUSION,0.8170055452865065,"make PLMs graph-aware based TAGs, and (2) employing prompts with the trained graph adapter to
354"
CONCLUSION,0.8188539741219963,"extract node representation from TAGs. Experiments with different shot settings using three datasets
355"
CONCLUSION,0.8207024029574861,"demonstrate that the proposed model can effectively capture both text and graph information, resulting
356"
CONCLUSION,0.822550831792976,"in improved performance for few-shot learning. In zero-shot learning, our model achieves comparable
357"
CONCLUSION,0.8243992606284658,"performance with supervised baselines and has huge potential for future work. Furthermore, our
358"
CONCLUSION,0.8262476894639557,"model provides useful interpretations, which is essential for understanding the tasks and TAGs.
359"
REFERENCES,0.8280961182994455,"References
360"
REFERENCES,0.8299445471349353,"[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
361"
REFERENCES,0.8317929759704251,"Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
362"
REFERENCES,0.833641404805915,"few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
363"
REFERENCES,0.8354898336414048,"[2] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall
364"
REFERENCES,0.8373382624768947,"and learn: Fine-tuning deep pretrained language models with less forgetting. arXiv preprint
365"
REFERENCES,0.8391866913123844,"arXiv:2004.12651, 2020.
366"
REFERENCES,0.8410351201478743,"[3] Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic,
367"
REFERENCES,0.8428835489833642,"and Inderjit S Dhillon. Node feature extraction by self-supervised multi-scale neighborhood
368"
REFERENCES,0.844731977818854,"prediction. arXiv preprint arXiv:2111.00064, 2021.
369"
REFERENCES,0.8465804066543438,"[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
370"
REFERENCES,0.8484288354898336,"deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
371"
REFERENCES,0.8502772643253235,"2018.
372"
REFERENCES,0.8521256931608133,"[5] Hande Dong, Jiawei Chen, Fuli Feng, Xiangnan He, Shuxian Bi, Zhaolin Ding, and Peng
373"
REFERENCES,0.8539741219963032,"Cui. On the equivalence of decoupled graph convolution network and label propagation. In
374"
REFERENCES,0.8558225508317929,"Proceedings of the Web Conference 2021, pages 3651–3662, 2021.
375"
REFERENCES,0.8576709796672828,"[6] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
376"
REFERENCES,0.8595194085027726,"embeddings. arXiv preprint arXiv:2104.08821, 2021.
377"
REFERENCES,0.8613678373382625,"[7] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
378"
REFERENCES,0.8632162661737524,"Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.
379"
REFERENCES,0.8650646950092421,"[8] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for
380"
REFERENCES,0.866913123844732,"few-shot learning. arXiv preprint arXiv:2109.04332, 2021.
381"
REFERENCES,0.8687615526802218,"[9] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
382"
REFERENCES,0.8706099815157117,"graphs. Advances in neural information processing systems, 30, 2017.
383"
REFERENCES,0.8724584103512015,"[10] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced
384"
REFERENCES,0.8743068391866913,"bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.
385"
REFERENCES,0.8761552680221811,"[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
386"
REFERENCES,0.878003696857671,"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
387"
REFERENCES,0.8798521256931608,"preprint arXiv:2106.09685, 2021.
388"
REFERENCES,0.8817005545286506,"[12] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
389"
REFERENCES,0.8835489833641405,"Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
390"
REFERENCES,0.8853974121996303,"Advances in neural information processing systems, 33:22118–22133, 2020.
391"
REFERENCES,0.8872458410351202,"[13] Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei,
392"
REFERENCES,0.88909426987061,"Haizhen Huang, Denvy Deng, and Qi Zhang. Promptbert: Improving bert sentence embeddings
393"
REFERENCES,0.8909426987060998,"with prompts. arXiv preprint arXiv:2201.04337, 2022.
394"
REFERENCES,0.8927911275415896,"[14] Seungbae Kim, Jyun-Yu Jiang, Masaki Nakada, Jinyoung Han, and Wei Wang. Multimodal
395"
REFERENCES,0.8946395563770795,"post attentive profiling for influencer marketing. In Proceedings of The Web Conference 2020,
396"
REFERENCES,0.8964879852125693,"pages 2878–2884, 2020.
397"
REFERENCES,0.8983364140480592,"[15] Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv preprint
398"
REFERENCES,0.9001848428835489,"arXiv:1611.07308, 2016.
399"
REFERENCES,0.9020332717190388,"[16] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient
400"
REFERENCES,0.9038817005545287,"prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
401"
REFERENCES,0.9057301293900185,"[17] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural
402"
REFERENCES,0.9075785582255084,"networks with 1000 layers. In International conference on machine learning, pages 6437–6449.
403"
REFERENCES,0.9094269870609981,"PMLR, 2021.
404"
REFERENCES,0.911275415896488,"[18] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
405"
REFERENCES,0.9131238447319778,"Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
406"
REFERENCES,0.9149722735674677,"learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.
407"
REFERENCES,0.9168207024029574,"[19] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt
408"
REFERENCES,0.9186691312384473,"understands, too. arXiv preprint arXiv:2103.10385, 2021.
409"
REFERENCES,0.9205175600739371,"[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
410"
REFERENCES,0.922365988909427,"Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
411"
REFERENCES,0.9242144177449169,"approach. arXiv preprint arXiv:1907.11692, 2019.
412"
REFERENCES,0.9260628465804066,"[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
413"
REFERENCES,0.9279112754158965,"arXiv:1711.05101, 2017.
414"
REFERENCES,0.9297597042513863,"[22] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically
415"
REFERENCES,0.9316081330868762,"ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv
416"
REFERENCES,0.933456561922366,"preprint arXiv:2104.08786, 2021.
417"
REFERENCES,0.9353049907578558,"[23] Bhaye Malhotra and Dinesh Kumar Vishwakarma. Classification of propagation path and tweets
418"
REFERENCES,0.9371534195933456,"for rumor detection using graphical convolutional networks and transformer based encodings. In
419"
REFERENCES,0.9390018484288355,"2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM), pages 183–190.
420"
REFERENCES,0.9408502772643254,"IEEE, 2020.
421"
REFERENCES,0.9426987060998152,"[24] Costas Mavromatis, Vassilis N Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han
422"
REFERENCES,0.944547134935305,"Zhao, Christos Faloutsos, and George Karypis. Train your own gnn teacher: Graph-aware
423"
REFERENCES,0.9463955637707948,"distillation on textual graphs. arXiv preprint arXiv:2304.10668, 2023.
424"
REFERENCES,0.9482439926062847,"[25] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
425"
REFERENCES,0.9500924214417745,"representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
426"
REFERENCES,0.9519408502772643,"[26] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H
427"
REFERENCES,0.9537892791127541,"Miller, and Sebastian Riedel.
Language models as knowledge bases?
arXiv preprint
428"
REFERENCES,0.955637707948244,"arXiv:1909.01066, 2019.
429"
REFERENCES,0.9574861367837338,"[27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
430"
REFERENCES,0.9593345656192237,"understanding by generative pre-training. 2018.
431"
REFERENCES,0.9611829944547134,"[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
432"
REFERENCES,0.9630314232902033,"networks. arXiv preprint arXiv:1908.10084, 2019.
433"
REFERENCES,0.9648798521256932,"[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
434"
REFERENCES,0.966728280961183,"Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
435"
REFERENCES,0.9685767097966729,"processing systems, 30, 2017.
436"
REFERENCES,0.9704251386321626,"[30] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
437"
REFERENCES,0.9722735674676525,"Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
438"
REFERENCES,0.9741219963031423,"[31] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
439"
REFERENCES,0.9759704251386322,"Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
440"
REFERENCES,0.977818853974122,"language models. arXiv preprint arXiv:2206.07682, 2022.
441"
REFERENCES,0.9796672828096118,"[32] Yiping Yang and Xiaohui Cui. Bert-enhanced text graph neural network for classification.
442"
REFERENCES,0.9815157116451017,"Entropy, 23(11):1536, 2021.
443"
REFERENCES,0.9833641404805915,"[33] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen.
444"
REFERENCES,0.9852125693160814,"Graph contrastive learning with augmentations. Advances in neural information processing
445"
REFERENCES,0.9870609981515711,"systems, 33:5812–5823, 2020.
446"
REFERENCES,0.988909426987061,"[34] Yazhou Zhang, Dan Ma, Prayag Tiwari, Chen Zhang, Mehedi Masud, Mohammad Shorfuz-
447"
REFERENCES,0.9907578558225508,"zaman, and Dawei Song. Stance level sarcasm detection with bert and stance-centered graph
448"
REFERENCES,0.9926062846580407,"attention networks. ACM Transactions on Internet Technology (TOIT), 2022.
449"
REFERENCES,0.9944547134935305,"[35] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian
450"
REFERENCES,0.9963031423290203,"Tang. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint
451"
REFERENCES,0.9981515711645101,"arXiv:2210.14709, 2022.
452"
