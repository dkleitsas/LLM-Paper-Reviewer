Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0019723865877712033,"Model-based RL is an effective approach for reducing sample complexity. How-
ever, when it comes to multi-agent setting where the number of agent is large,
the model estimation can be problematic due to the exponential increased interac-
tions. In this paper, we propose a decentralized model-based reinforcement learn-
ing algorithm for networked multi-agent systems, where agents are cooperative
and communicate locally with their neighbors. We analyze our algorithm theo-
retically and derive an upper bound of performance discrepancy caused by model
usage, and provide a sufﬁcient condition of monotonic policy improvement. In our
experiments, we compare our algorithm against other strong multi-agent baselines
and demonstrate that our algorithm not only matches the asymptotic performance
of model-free methods but also largely increases its sample efﬁciency."
INTRODUCTION,0.0039447731755424065,"1
INTRODUCTION"
INTRODUCTION,0.005917159763313609,"Many real world problems, such as autonomous driving, wireless communications, multi-player
games can be modeled as multi-agent RL problems, where multiple autonomous agents coexist in a
common environment, aiming to maximize its individual or team reward in the long term by inter-
acting with the environment and other agents. Unlike single-agent tasks, multi-agent tasks are more
challenging, due to partial observations and unstable environments when agents update their policies
simultaneously. Therefore, there are hardly any one-ﬁts-all solutions for MARL problems. Exam-
ples include networked systems control (NSC) (Chu et al., 2020), in which agents are connected via
a stationary network. They perform decentralized control based on its local observations and mes-
sages from connected neighbors. Examples of networked systems include connected vehicle control
(Jin & Orosz, 2014), trafﬁc signal control (Chu et al., 2020), etc."
INTRODUCTION,0.007889546351084813,"Despite the success of multi-agent reinforcement (RL) algorithms, their performance relies on a
massive amount of model usage. Typically, a multi-agent RL algorithm needs millions of inter-
action with the environment to converge. On the other hand, model-based reinforcement learning
(MBRL) algorithms, which utilize predictive models of the environment to help data collection, are
empirically more data-efﬁcient than model-free approaches. Although model inaccuracy performs
as a bottleneck of policy quality in model-based algorithms, we can still learn a good policy with an
imperfect model (Luo et al., 2019), especially combined with the trick of branched rollout (Janner
et al., 2019) to limit model usage. Experimentally, MuZero (Schrittwieser et al., 2020), a model-
based RL algorithm, succeeded in matching the performance of AlphaZero on Go, chess and shogi,
and becomes state-of-the-art on Atari games. Model-based MARL is not fully investigated. Exist-
ing MB-MARL algorithms either limit their ﬁeld of research on speciﬁc scenario, e.g. two-player
zero-sum Markov game (Zhang et al., 2020) or pursuit evasion game (Bouzy & M´etivier, 2007),
or use tabular RL method (Bargiacchi et al., 2021). MB-MARL for multi-agent MDPs is still an
open problem to be solved (Zhang et al., 2019), with profound challenges such as scalability issues
caused by large state-action space and incomplete information of other agents’ state or actions."
INTRODUCTION,0.009861932938856016,"In this paper, we develop decentralized model-based algorithms on networked systems, where agents
are cooperative, and able to communicate with each other. We use localized models to predict future
states, and use communication to broadcast their predictions. To address the issue of model error,
we adopt branched rollout (Janner et al., 2019) to limit the rollout length of model trajectories. In
the policy optimization part, we use decentralizd PPO (Schulman et al., 2017) with a extended value
function. At last, we analyze these algorithms theoretically to bound the performance discrepancy"
INTRODUCTION,0.011834319526627219,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013806706114398421,"between our method and its model-free, centralized counterpart. At last, we run these algorithms
in trafﬁc control environments (Chu et al., 2020; Vinitsky et al., 2018) to test the performance of
our algorithm. We show that our algorithm increases sample efﬁciency, and matches the asymptotic
performance of model-free methods."
INTRODUCTION,0.015779092702169626,"In summary, our contributions are three-fold.
Firstly, we propose an algorithmic framework,
which is a fully decentralized model-based reinforcement learning algorithm, which is named as
Decentralized Model-based Policy Optimization (DMPO). Secondly, we analyze the theoretical
performance of our algorithm. Lastly, empirical results on trafﬁc control environments demonstrate
the effectiveness of DMPO in reducing sample complexities and achieving similar asymptotic per-
formance of model-free methods."
RELATED WORK,0.01775147928994083,"2
RELATED WORK"
RELATED WORK,0.01972386587771203,"Model-based methods are known for their data efﬁciency (Kaelbling et al., 1996), especially com-
pared with model-free algorithms. There is a vast literature on the theoretical analysis of model-
based reinforcement learning. In a single-agent scenario, monotonic improvement of policy op-
timization has been achieved (Luo et al., 2019; Sun et al., 2018), and a later work improved the
performance of model-based algorithms by limiting model usage (Janner et al., 2019). But these
analysis is restricted to single-agent scenarios, whereas ours addresses multi-agent problems."
RELATED WORK,0.021696252465483234,"On the other hand, Networked System Control (NSC) (Chu et al., 2020) is a challenging setting for
MARL algorithm to take effect. Some multi-agent algorithms falls into centralized training decen-
tralized execution (CTDE) framework. For example, QMIX (Rashid et al., 2018) and COMA (Foer-
ster et al., 2018) all use a centralized critic. In a large network, however, centralized training might
not scale. In many scenarios, only fully decentralized algorithms can be used. Zhang et al. (2018)
proposed an algorithm of NSC that can be proven to converge under linear approximation. Qu et al.
(2020a) proposed truncated policy gradient, to optimize local policies with limited communication.
Baking in the idea of truncated Q-learning in (Qu et al., 2020a), we generalize their algorithm to
deep RL, rather than tabular RL. Factoring environmental transition into marginal transitions can
be seen as factored MDP. Guestrin et al. (2001) used Dynamic Bayesian Network to predict system
transition. Simao & Spaan (2019) proposed a tabular RL algorithm to ensure policy improvement at
each step. However, our algorithm is a deep RL algorithm, enabling better performance in general
tasks."
RELATED WORK,0.023668639053254437,"There are some works on applying model-based methods in MARL settings. A line of research
focuses on model-based RL for two-player games. For example, Brafman & Tennenholtz (2000)
solved single-controller-stochastic games, which is a certain type of two-player zero-sum game;
Bouzy & M´etivier (2007) performed MB-MARL in the pursuit evasion game; Zhang et al. (2020)
proved that model-based method can be nearly optimally sample efﬁcient in two-player zero-sum
Markov games. Bargiacchi et al. (2021) extended the concept of prioritized sweeping into a MARL
scenario. However, this is a tabular reinforcement algorithm, thus unable to deal with cases where
state and action spaces are relatively large, or even continuous. In contrast to existing works, our
algorithm is not only applicable to more general multi-agent problems, but is also the ﬁrst fully
decentralized model-based reinforcement learning algorithm."
PROBLEM SETUP,0.02564102564102564,"3
PROBLEM SETUP"
PROBLEM SETUP,0.027613412228796843,"In this section, we introduce multi-agent networked MDP and model-based networked system con-
trol."
PROBLEM SETUP,0.029585798816568046,"Networked MDP
We consider environments with a graph structure. Speciﬁcally, n agents coexist
in an underlying undirected and stationary graph G = (V, E). Agents are represented as a node
in the graph, therefore V = {1, ..., n} is the set of agents. E ⊂V × V comprises the edges that
represent the connectivity of agents. Agents are able to communicate along the edges with their
neighbors. Let Ni denote every neighbor of agent i, and ¯
Ni = Ni ∪{i}. Furthermore, let N κ
i denote
the κ-hop neighborhood of i, i.e. the nodes whose graph distance to i is less than or equal to κ. For
the simplicity of notation, we also deﬁne N κ
−i = V \ N κ
i ."
PROBLEM SETUP,0.03155818540433925,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.03353057199211045,"The corresponding networked MDP is deﬁned as (G, {Si, Ai}i∈V, p, r). Each agent i have their
local state si ∈Si, and perform action ai ∈Ai. The global state is the concatenation of all local
states: s = (s1, ..., sn) ∈S := S1 ×...×Sn. Similarly, the global action is a = (a1, ..., an) ∈A :=
A1 × ... × An. For the simplicity of notation, we deﬁne sNi to be the local states of every agent in
Ni, that is, given Ni = {j1, ..., jc}, then sNi = (sj1, ..., sjc). aNi, sN κ
i , aNκ
i are deﬁned similarly.
The transition function is deﬁned as: p(s′|s, a) : S × A →S. Each agent possess a localized policy
πθi
i (ai|s ¯
Ni) that is parameterized by θi ∈Θi, meaning the local policy is dependent only on states
of its neighbors and itself. We use θ = (θ1, ..., θn) to denote the tuple of localized policy parameters,
and πθ(a|s) = Qn
i=1 πθi
i (ai|s ¯
Ni) denote the joint policy. We also assume that reward functions is
only dependent on local state and action: ri(si, ai), and the global reward function is deﬁned to be
the average reward r(s, a) = 1"
PROBLEM SETUP,0.03550295857988166,"n
Pn
i=1 ri(si, ai)."
PROBLEM SETUP,0.03747534516765286,"The goal of reinforcement learning is to maximize the expected sum of discounted rewards, denoted
by η:"
PROBLEM SETUP,0.03944773175542406,"πθ∗= arg max
πθ η[πθ] = arg max
πθ Eπθ
h ∞
X"
PROBLEM SETUP,0.04142011834319527,"t=0
γt · 1 n n
X"
PROBLEM SETUP,0.04339250493096647,"i=1
ri(st, at)
i
,
(1)"
PROBLEM SETUP,0.045364891518737675,"where γ ∈(0, 1) is the temporal discount factor. We deﬁne the stationary distribution under policy
π to be dπ(s)."
PROBLEM SETUP,0.047337278106508875,"Independent Networked System
Networked system may have some extent of locality, meaning
in some cases, local states and actions do not affect the states of distant agents. In such systems,
environmental transitions can be factorized, and agents are able to maintain local models to predict
future local states. We deﬁne Independent Networked System (INS) as follows:
Deﬁnition 1. An environment is an Independent Networked System (INS) if:"
PROBLEM SETUP,0.04930966469428008,"p(s′|s, a) = n
Y"
PROBLEM SETUP,0.05128205128205128,"i=1
pi(s′
i|s ¯
Ni, ai), ∀s′, s ∈S, a ∈A."
PROBLEM SETUP,0.05325443786982249,"INS might be an assumption that is too strong to hold. However, for the dynamics that cannot be
factorized, we can still use an INS to approximate it. Let DT V denote the total variation distance
between distributions, we have the following deﬁnition:
Deﬁnition 2. (ξ-dependent) Assume there exists an Independent Networked System ¯p such that
¯p(s′|s, a) = Qn
i=1 pi(s′
i|s ¯
Ni, ai). An environment is ξ-dependent, if:"
PROBLEM SETUP,0.055226824457593686,"sup
s,a DT V

p(s′|s, a)∥¯p(s′|s, a)

= sup
s,a
1
2 X"
PROBLEM SETUP,0.05719921104536489,"s′∈S
|p(s′|s, a) −¯p(s′|s, a)| ≤ξ."
PROBLEM SETUP,0.05917159763313609,"To explain the intuition behind this deﬁnition, we point out that ξ is actually the lower bound of
model error when we use local models ˆp(s ¯
Ni, ai). Recall that p(s′|s, a) is the real environment
transition, ¯p = Qn
i=1 pi(s′
i|s ¯
Ni, ai) is the product of marginal environment transitions, and ˆp(s, a) =
Qn
i=1 ˆpi(s′
i|s ¯
Ni, ai) is the product of model transitions. Then the universal model error D(p∥ˆp) can
be divided into two parts: dependency bias D(p∥¯p) and model error D(¯p∥ˆp):
D(p∥ˆp) ≤D(p∥¯p) + D(¯p∥ˆp).
Then for a ξ-dependent system, when models become very accurate, meaning D(¯p∥ˆp) ≈0,
sup D(p∥ˆp) ≈sup D(p∥¯p) = ξ. While D can be any appropriate distance metric, we use the
TV-distance hereafter for the ease of presentation. In the following, we develop theory under both
INS and ξ-dependent scenarios."
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.0611439842209073,"4
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION"
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.0631163708086785,"In this section, we formally present Decentralized Model-based Policy Optimization (DMPO),
which is a fully decentralized model-based reinforcement learning algorithm. Compared with inde-
pendent multi-agent PPO, DMPO is augmented in three ways: localized model, policy with one-step
communication, and extended value function. We introduce the detail of localized model in 4.1. Pol-
icy and value functions are introduced in 4.2. The illustration of our algorithm is given in Figure
1. All the components mentioned above are analyzed in Section 5. We argue that under certain
conditions, our algorithm ensures monotonic policy improvement."
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.0650887573964497,Under review as a conference paper at ICLR 2022
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.0670611439842209,"(a) Neighborhood
(b) Value function"
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.06903353057199212,(c) Graph convolutional model
DECENTRALIZED MODEL-BASED POLICY OPTIMIZATION,0.07100591715976332,"Figure 1: (a) presents the concept of neighborhood. If agent i is the node in purple, then purple and
orange is ¯
Ni, and combination of purple, orange and green is N 3
i . (b) explains that extended value
function takes sNκ
i as input, here κ = 3. (c) is the illustration of graph convolutional model."
DECENTRALIZED PREDICTIVE MODEL,0.07297830374753451,"4.1
DECENTRALIZED PREDICTIVE MODEL"
DECENTRALIZED PREDICTIVE MODEL,0.07495069033530571,"To perform decentralized model-based learning, we let each agent maintain a localized model. The
localized model can observe the state of 1-hop neighbor and the action of itself, and the goal of a
localized model is to predict the information of the next timestep, including state, reward and done.
This process is denoted by ˆpi(s′
i, r′
i, d′
i|s ¯
Ni, ai)."
DECENTRALIZED PREDICTIVE MODEL,0.07692307692307693,"We implement a localized model with graph convolutional networks (GCN). Recall that agents are
situated in a graph G = (V, E). In the ﬁrst step, a node-level encoder encodes local state into node
embedding,"
DECENTRALIZED PREDICTIVE MODEL,0.07889546351084813,"h0
i = f encode
i
(si).
(2)
Then we perform one step of graph convolution as follows,"
DECENTRALIZED PREDICTIVE MODEL,0.08086785009861933,"h(i,j) = f edge
(i,j) (h0
i , h0
j),
h1
i = f node
i
(
X"
DECENTRALIZED PREDICTIVE MODEL,0.08284023668639054,"e=(i,j)
h(i,j), ai).
(3)"
DECENTRALIZED PREDICTIVE MODEL,0.08481262327416174,"In this way, h1
i is dependent only on s ¯
Ni and ai. Finally, a node-level decoder generates the predic-
tion of state, reward and done from h1
i as follows:"
DECENTRALIZED PREDICTIVE MODEL,0.08678500986193294,"s′
i = f state
i
(h1
i ) + si,
r′
i = f reward
i
(h1
i ),d′
i = f done
i
(h1
i ).
(4)"
DECENTRALIZED PREDICTIVE MODEL,0.08875739644970414,"Note that we predict the next state with a skip connection, because empirically, it’s more efﬁcient to
predict the change of the state rather than the state itself."
DECENTRALIZED PREDICTIVE MODEL,0.09072978303747535,"In practice, the data are all stored locally by each agent. Data that are collected in the environment
by agent i is denoted as Denv
i
, and those generated by predictive model is denoted as Dmodel
i
."
DECENTRALIZED PREDICTIVE MODEL,0.09270216962524655,"Scaling model-based methods into real tasks can result in decreased performance, even if the model
is relatively accurate. One main reason is the compound modeling error when long model rollouts
are used, and model error compound along the rollout trajectory, making the trajectory ultimately in-
accurate. To reduce the negative effect of model error, we adopt a branched rollout scheme proposed
in (Janner et al., 2019). In branched rollout, model rollout starts not from an initial state, but from a
state that was randomly selected from the most recent environmental trajectory τ. Additionally, the
model rollout length is ﬁxed to be T. This scheme is shown to be effective in reducing the negative
inﬂuence of model error both theoretically and empirically."
DECENTRALIZED PREDICTIVE MODEL,0.09467455621301775,"To deal with the bias of model trajectories, at each model rollout, we allow the algorithm to fall back
to the real trajectory with probability 1 −q0, where q0 is a hyperparameter. We describe the detailed
framework of model usage and experiment storage in Algorithm 1."
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.09664694280078895,"4.2
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.09861932938856016,"To optimize the policies, we need to adopt an algorithm that can exploit network structure, whilst
remaining decentralized. Independent RL algorithms that observes only local state are fully decen-
tralized, but they often fail to learn an optimal policy. Centralized algorithms that utilize centralized"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.10059171597633136,Under review as a conference paper at ICLR 2022
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.10256410256410256,"Algorithm 1: Decentralized Model-based Policy Optimization (DMPO) for MARL
Input: hyperparameters: rollout length T, truncation radius κ"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.10453648915187377,"1: Initialize the model pψi
i , actor πθi
i and critic V φi
i .
2: Initialize replay buffers Denv
i
and Dmodel
i
.
3: for M iterations do
4:
Perform environmental rollout together, and each agent i collect trajectory information τi.
5:
for i in N agents do
6:
Denv
i
= Denv
i
∪{τi}.
7:
Train pψi
i
on Denv
i
.
8:
Dmodel
i
= ∅.
9:
for B inner iterations: do
10:
Generate a random number q ∼U(0, 1).
11:
if q > q0 then
12:
Dmodel
i
= τi. {Fall back to real trajectory with probability 1 −q0.}
13:
else
14:
for R rollouts, s ∈τ do
15:
Perform T-step model rollout starting from s using policy pψ∗, append to Dmodel
∗
.
16:
for G steps, i = 0, ..., n −1 do
17:
Take a step along the gradient to update πθi
i and critic V φi
i
on Dmodel
∗"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.10650887573964497,"critics often achieve better performance than decentralized algorithms, but they might not scale to
large environments where communication costs are expensive."
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.10848126232741617,"We propose Proximal Policy Optimization with extended value function, which is deﬁned as
Vi(sNκ
i ) = EsNκ
−i∼dπ[P∞
t=0 rt
i|s0
N κ
i = sNκ
i ], i ∈V. The intuition behind extended value func-
tion comes from (Qu et al., 2020a), where truncated Q-function Q(sN κ
i , aNκ
i ) is initially proposed.
In 5.3, we prove that Vi(sN κ
i ) is a good approximation of Vi(s), with a difference decreasing expo-
nentially with κ."
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.11045364891518737,"To generate the objective for extended value function, or return Ri, we use reward-to-go technique.
However, because model rollout is short, standard reward-to-go returns would get a biased estima-
tion of Vi. To resolve this issue, we add the value estimation of the last state to the return. In this
way, with a local trajectory τi = {(st
i, at
i, rt
i, (s′)t
i, dt
i, log πt
i), t = 0, 1, ..., T −1}, the objective of
V t
i (sNκ
i ) is"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.11242603550295859,"Rt
i ="
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.11439842209072978,"T −t−1
X"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.11637080867850098,"l=0
γlrt+l
i
+ V φi
i

(s′)T −1
N κ
i

,
(5)"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.11834319526627218,"and the loss of value function is deﬁned as Lvalue
i
= 1 m
P"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1203155818540434,"m∈Dmodel
i

V φi
i (sm
N κ
i ) −Rm
i
2. In policy
training, extended value functions Vi are reduced via communication to their κ-hop neighbors to
generate an estimation of global value function,"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1222879684418146,"˜V t
i = 1 n X"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1242603550295858,"j∈N κ
i"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.126232741617357,"˜V t
j ,
(6)"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1282051282051282,"and advantages ˆAi are computed on ˜Vi with generalized advantage estimation (GAE) (Schulman
et al., 2015) for policy gradient update. The surrogate loss function of a DMPO agent is deﬁned as"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1301775147928994,"Lpolicy
i
= 1 m X"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.13214990138067062,"m∈Dmodel
i min"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.1341222879684418,"πθi
i (at
i|st
¯
Ni)"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.13609467455621302,"π
θk
i
i (at
i|st
¯
Ni)
ˆAi(sV κ
i ), g(ϵ, ˆAi(sV κ
i )) ! ,
(7)"
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.13806706114398423,similar to PPO-Clip loss.
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.14003944773175542,"The communication of κ step might seem costly, yet information of N κ
i is only used in the training
phase. We argue that in the training phase, algorithms are less sensitive with latency than execu-
tion. Furthermore, since model-based learning can effectively increase sample efﬁciency, we might
tolerate more communication."
PROXIMAL POLICY OPTIMIZATION WITH EXTENDED VALUE FUNCTION,0.14201183431952663,Under review as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.14398422090729784,"5
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.14595660749506903,"In this section, we analyze DMPO theoretically. In 5.2, we derive a bound between the true returns
and the returns under a model ˆp in a networked system. In 5.3, we prove that extended value function
Vi(sNκ
i ) is a good approximation of Vi(s), and with extended value function, the true policy gradient
can also be approximated."
THEORETICAL ANALYSIS,0.14792899408284024,"5.1
BACKGROUND: MONOTONIC MODEL-BASED POLICY OPTIMIZATION"
THEORETICAL ANALYSIS,0.14990138067061143,"Let η[π] denote the returns of the policy in the true environment, ˆη[π] denote the returns of the
policy under the approximated model. To analyze the difference between η[π] and ˆη[π], we need to
construct a bound
ηp[π] ≥ˆη ˆp[π] −C(p, ˆp, π, πD),
(8)
where C is a non-negative function, and πD is the data-collecting policy. According to equation 8,
if every policy update ensures an improvement of ˆη[π] by at least C, η[π] will improve monotoni-
cally. This inequality was ﬁrst presented in single agent domain (Janner et al., 2019). In this work,
we extend this to the multi-agent networked system, aiming to achieve monotonic team reward im-
provement."
THEORETICAL ANALYSIS,0.15187376725838264,"In this work, we let π indicate a collective policy π = [π1, ..., πn], and the model ˆp be an INS
ˆp(s′|s, a) = Qn
i=1 ˆpi(s′
i|s ¯
Ni, ai) that approximating the true MDP. In DMPO, each agent learns a
localized model ˆπi, policy πi(|sNk), critic Vi(sNκ
i ), making it never a trivial extension. We give the
detailed analysis in 5.2."
ANALYSIS OF RETURNS BOUND,0.15384615384615385,"5.2
ANALYSIS OF RETURNS BOUND"
ANALYSIS OF RETURNS BOUND,0.15581854043392504,"In model-based learning, different rollout schemes can be chosen. The vanilla rollout assumes
that models are used in an inﬁnite horizon. The branched rollout performs a rollout from a state
sampled by a state distribution of previous policy πD, and runs T steps in ˆπ according to π. Based
on different rollout schemes, we can construct two lower bounds. Under vanilla rollout, real return
and model return can be bounded by model error and policy divergence. Formal results are presented
in Theorem 1. The detailed proof is deferred to Appendix C.
Theorem 1. Consider an independent networked system. Denote local model errors as ϵmi =
maxs ¯
Ni,ai DT V [pi(s′
i|s ¯
Ni, ai)∥ˆpi(s′
i|s ¯
Ni, ai)], and divergences between the data-collecting policy
and evaluated policy as ϵπi = maxs ¯
Ni DT V [πD(ai|s ¯
Ni)∥π(ai|s ¯
Ni)]. Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
η ˆp[π1, ..., πn] denote the returns estimated in the model trajectories, and the states and actions are
collected with πD. Then we have:"
ANALYSIS OF RETURNS BOUND,0.15779092702169625,"|ηp[π1, ..., πn] −η ˆp[π1, ..., πn]| ≤2rmax 1 −γ n
X i=1 ϵπi"
ANALYSIS OF RETURNS BOUND,0.15976331360946747,"n + (ϵmi + 2ϵπi) · ∞
X"
ANALYSIS OF RETURNS BOUND,0.16173570019723865,"k=0
γk+1 | ¯
Ni
k|
n

."
ANALYSIS OF RETURNS BOUND,0.16370808678500987,"Intuitively, the term P∞
k=0 γk+1 | ¯
Ni
k|
n
would be in the same magnitude as
1
1−γ , which might be huge
given the choice of γ, making the bound too loose to be effective. To make tighter the discrepancy
bound in Theorem 1, we adopt the branched rollout scheme. The branched rollout enables a effec-
tive combination of model-based and model-free rollouts. For each rollout, we begin from a state
sample from dπD, and run T steps in each localized ˆπi. When branched rollout is applied in an INS,
Theorem 2 gives the returns bound.
Theorem 2. Consider an independent networked system. Denote local model errors as ϵmi =
maxs ¯
Ni,ai DT V [pi(s′
i|s ¯
Ni, ai)∥ˆpi(s′
i|s ¯
Ni, ai)], and divergences between the data-collecting policy
and evaluated policy as ϵπi = maxs ¯
Ni DT V [πD(ai|s ¯
Ni)∥π(ai|s ¯
Ni)]. Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
ηbranch[π1, ..., πn] denote the returns estimated via T-step branched rollout scheme. Then we have:"
ANALYSIS OF RETURNS BOUND,0.16568047337278108,"|ηp[π1, ..., πn]−ηbranch[π1, ..., πn]| ≤2rmax 1 −γ n
X i=1"
ANALYSIS OF RETURNS BOUND,0.16765285996055226,"h
ϵmi·
  T −1
X"
ANALYSIS OF RETURNS BOUND,0.16962524654832348,"k=0
γk+1 | ¯
Ni
k|
n

+ϵπi·
 
∞
X"
ANALYSIS OF RETURNS BOUND,0.17159763313609466,"k=T
γk+1 | ¯
Ni
k|
n
i"
ANALYSIS OF RETURNS BOUND,0.17357001972386588,Under review as a conference paper at ICLR 2022
ANALYSIS OF RETURNS BOUND,0.1755424063116371,"Comparing the results in Theorem 1 and 2, we can see that branched rollout scheme reduced the co-
efﬁcient before ϵmi from P∞
k=0 γk+1 | ¯
Ni
k|
n
≤
γ
1−γ to PT −1
k=0 γk+1 | ¯
Ni
k|
n
≤PT −1
k=0 γk+1 = γ(1−γT )"
ANALYSIS OF RETURNS BOUND,0.17751479289940827,"1−γ
.
This reduction explains that empirically, branched rollout brings better asymptotic performance.
Also, if we set T = 0, this bound turn into a model-free bound. This indicates that when ϵmi is
lower than ϵπi allowed by our algorithm, a model might increase the performance."
ANALYSIS OF RETURNS BOUND,0.1794871794871795,"In reality, not every system satisﬁes the deﬁnition of INS. Yet we can generalize Theorem 2 into a
ξ-dependent system."
ANALYSIS OF RETURNS BOUND,0.1814595660749507,"Corollary 1. Consider an ξ-dependent networked system. Denote local model errors as ϵmi =
maxs ¯
Ni,ai DT V [pi(s′
i|s ¯
Ni, ai)∥ˆpi(s′
i|s ¯
Ni, ai)], and divergences between the data-collecting policy
and evaluated policy as ϵπi = maxs ¯
Ni DT V [πD(ai|s ¯
Ni)∥π(ai|s ¯
Ni)]. Assume the upper bound of
rewards of all agents is rmax. Let ηp[π1, ..., πn] denote the real returns in the environment. Also, let
ηbranch[π1, ..., πn] denote the returns estimated via T-step branched rollout scheme. Then we have:"
ANALYSIS OF RETURNS BOUND,0.1834319526627219,"|ηp[π1, ..., πn] −ηbranch[π1, ..., πn]|"
ANALYSIS OF RETURNS BOUND,0.1854043392504931,≤2rmaxγ
ANALYSIS OF RETURNS BOUND,0.1873767258382643,"(1 −γ)2 ξ + 2rmax 1 −γ n
X i=1"
ANALYSIS OF RETURNS BOUND,0.1893491124260355,"h
ϵmi ·
  T −1
X"
ANALYSIS OF RETURNS BOUND,0.1913214990138067,"k=0
γk+1 | ¯
Ni
k|
n

+ ϵπi ·
 
∞
X"
ANALYSIS OF RETURNS BOUND,0.1932938856015779,"k=T
γk+1 | ¯
Ni
k|
n
i"
ANALYSIS OF RETURNS BOUND,0.1952662721893491,"The proof can also be found in Appendix C. Compared to Theorem 2, Corollary 1 is more general, as
it is applicable to the multi-agent systems that are not fully independent. Intuitively, if a networked
system seems nearly independent, local models will be effective enough. The bound indicates that
when the policy in optimized in a trust region where D(π, πD) ≤ϵπi, the bound would also be
restricted, making monotonic update more achievable."
EXTENDED VALUE FUNCTION,0.19723865877712032,"5.3
EXTENDED VALUE FUNCTION"
EXTENDED VALUE FUNCTION,0.1992110453648915,"In this section, we analyze the effect of extended value function. The idea of extended value function
Vi(sNκ
i ) comes from truncated Q-function Qi(sNκ
i , aNκ
i ) proposed in (Qu et al., 2020a). We prove
that extended value function is an approximation of the real value function. The detailed proof of
Theorem 3 is deferred to Appendix C."
EXTENDED VALUE FUNCTION,0.20118343195266272,"Theorem 3. Deﬁne Vi(sNκ
i ) = EsNκ
−i∼dπ[P∞
t=0 rt
i|s0
Nκ
i = sNκ
i ], and Vi(s) = E[P∞
t=0 rt
i|s0 = s],
then:"
EXTENDED VALUE FUNCTION,0.20315581854043394,"|Vi(s) −Vi(sNκ
i )| ≤rmax"
EXTENDED VALUE FUNCTION,0.20512820512820512,1 −γ γκ.
EXTENDED VALUE FUNCTION,0.20710059171597633,"From Theorem 3, it is straightforward that the global value function can be approximated with
the average of all extended value functions: |V (s) −1"
EXTENDED VALUE FUNCTION,0.20907297830374755,"n
Pn
i=1 Vi(sNκ
i )| ≤rmax"
EXTENDED VALUE FUNCTION,0.21104536489151873,"1−γ γκ. In PPO, value"
EXTENDED VALUE FUNCTION,0.21301775147928995,"functions are used for calculating advantages ˆA(t) = r(t) + γV (s(t+1)) −V (s(t)), and we have
proven that V (s) can be estimated with the average of extended value functions 1"
EXTENDED VALUE FUNCTION,0.21499013806706113,"n
Pn
i=1 Vi(sNκ
i ).
In practice, an agent might not get the value function of distant agents. However, we can prove
that ˜Vi = 1"
EXTENDED VALUE FUNCTION,0.21696252465483234,"n
P
j∈Nκ
i Vj(sNκ
j ) is already very accurate for calculating the policy gradient for agent
i. Theorem 4 justiﬁes that the policy gradients computed based on the sum of the nearby extended
value functions is a close approximation of true policy gradients."
EXTENDED VALUE FUNCTION,0.21893491124260356,"Theorem 4. Let ˆAt = r(t)+γV (s(t+1))−V (s(t)) be the TD residual, and gi = E[ ˆA∇θi log πi(a|s)]
be the policy gradient. If ˜At and ˜gi are the TD residual and policy gradient when value function
V (s) is replaced by ˜Vi(s) = 1 n
P"
EXTENDED VALUE FUNCTION,0.22090729783037474,"j∈Nκ
i Vj(sNκ
i ), we have:"
EXTENDED VALUE FUNCTION,0.22287968441814596,|gi −˜gi| ≤γκ−1
EXTENDED VALUE FUNCTION,0.22485207100591717,"1 −γ [1 −(1 −γ2)N κ
i
n ]rmaxgmax,"
EXTENDED VALUE FUNCTION,0.22682445759368836,"where rmax and gmax denote the upper bound of the absolute value of reward and gradient, respec-
tively."
EXTENDED VALUE FUNCTION,0.22879684418145957,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.23076923076923078,"6
EXPERIMENTS"
ENVIRONMENTS,0.23274161735700197,"6.1
ENVIRONMENTS"
ENVIRONMENTS,0.23471400394477318,"We test our algorithm in four environments, namely Figure Eight, Ring Attenuation (Wu et al.,
2017a), CACC Catchup, and CACC Slowdown (Chu et al., 2020). Detailed description and visual-
ization of these environments is deferred to Appendix A."
ENVIRONMENTS,0.23668639053254437,"Cooperative Adaptive Cruise Control
The objective of CACC is to adaptively coordinate a pla-
toon of 8 vehicles to minimize the car-following headway and speed perturbations based on real-time
vehicle-to-vehicle communication. CACC consists of two scenarios: Catch-up and Slow-down. In
CACC Catch-up, vehicles need to catch up to the ﬁrst car. In CACC Slow-down, every vehicle is
faster than the optimal speed, and they need to slow down without causing any collision. The agents
receives a negative reward if the headway or the speed is not optimal. Also, whenever a collision
happens, a huge negative reward of -1000 is given."
ENVIRONMENTS,0.23865877712031558,"Flow environments
This task consists of Figure Eight and Ring Attenuation. The objective of
these environments is letting the automated vehicles achieve a target average speed inside the road
network while avoiding collisions. The state of each vehicle is its velocity and position, and the
action is the acceleration of itself. In Ring Attenuation, the objective is to achieve a high speed,
while avoiding stop-and-go loops. Vehicles are rewarded with their speed, but also punished for their
accelerations. In the perspective of a networked system, we assume that the vehicles are connected
with the preceding and succeeding vehicle, thus resulting in a loop-structured graph."
BASELINES,0.2406311637080868,"6.2
BASELINES"
BASELINES,0.24260355029585798,We describe the following algorithms for performance comparison:
BASELINES,0.2445759368836292,"• CPPO: Centralized PPO learns a centralized critic Vi(s). This baseline aims to analyze the
performance when κ is set to be arbitrarily huge, and is used in (Vinitsky et al., 2018) as a
benchmark algorithm for networked system control.
• IC3Net (Singh et al., 2018): A communication-based multi-agent RL algorithm. The agents
maintain their local hidden states with a LSTM kernel, and actively determines the com-
munication target. Compared with DPPO, IC3Net uses hidden state and continuous com-
munication, whereas DPPO agents directly observe the states of their neighbors.
• DPPO: Decentralized PPO learns an independent actor and critic for each agent. We im-
plement it by using neighbor’s state for extended value estimation.
• DMPO (our method): DMPO is a decentralized and model-based algorithm based on
DPPO. On top of it, we use decentralized graph convolutional kernel as predictive model."
RESULTS,0.2465483234714004,"6.3
RESULTS"
RESULTS,0.2485207100591716,"Figure 2 shows the episode reward v.s. number of training samples curves of the algorithms. We
address that in CACC environments, DMPO uses decentralized SAC as base algorithm. Similar with
DPPO, decentralized SAC uses extended Q-function Qi(sNκ
i , aNκ
i ) for its policy gradient. From the
results, we conclude that our algorithm matches the asymptotic performance of model-free methods.
It also learns the policy faster, resulting in increased sample efﬁciency."
RESULTS,0.2504930966469428,"The comparison between DMPO and DPPO can be viewed as an ablation study of model usage. In
ﬁgure eight, DMPO increases sample efﬁciency at the beginning, but as the task becomes difﬁcult,
the sample efﬁciency of our method decreased. In a relatively easy task, ring attenuation, our method
increased sample efﬁciency massively, compared with its model-free counterpart."
RESULTS,0.252465483234714,"The comparison between the asymptotic performance of CPPO and DMPO or DPPO can be viewed
as an ablation study of extended value function. From the result in four environments, we ob-
serve that the asymptotic performance of CPPO does not exceed that of the algorithms that uses
extended value function. In this way, we conclude that by using extended value function, a cen-
tralized algorithm can be decomposed into decentralized algorithm, but the performance would not
drop signiﬁcantly."
RESULTS,0.25443786982248523,Under review as a conference paper at ICLR 2022
RESULTS,0.2564102564102564,"Figure 3 shows the accuracy of our model in predicting the reward and state during training. The er-
ror is deﬁned as the ratio of MSE loss to variance. From the ﬁgures, we conclude that neighborhood
information is accurate enough for a model to predict the next state in these environments. However,
in CACC Slow-down, local models might fail to learn the reward. We observe that the errors may
increase as the agents explore new regions in the state space."
RESULTS,0.2583826429980276,"0
50000 100000 150000 200000 250000 300000 350000 400000
0 50 100 150 200 250 300"
RESULTS,0.2603550295857988,"CPPO
DPPO
IC3Net
DMPO(ours)"
RESULTS,0.26232741617357,(a) Figure Eight
RESULTS,0.26429980276134124,"0
50000 100000 150000 200000 250000 300000 350000 400000
6000 5000 4000 3000 2000 1000 0"
RESULTS,0.26627218934911245,"CPPO
DPPO
IC3Net
DMPO(ours)"
RESULTS,0.2682445759368836,(b) Ring Attenuation
RESULTS,0.2702169625246548,"0
50000
100000
150000
200000
250000
300000
350000
100 0 100 200 300 400 500"
RESULTS,0.27218934911242604,"CPPO
DPPO
IC3Net
DMPO(ours)"
RESULTS,0.27416173570019725,(c) CACC Catch-up
RESULTS,0.27613412228796846,"0
200000
400000
600000
800000
1000000
100 0 100 200 300 400 500 600"
RESULTS,0.2781065088757396,"CPPO
DPPO
IC3Net
DMPO(ours)"
RESULTS,0.28007889546351084,(d) CACC Slow-down
RESULTS,0.28205128205128205,"Figure 2: Training curves on multi-agent environments. Solid curves depict the mean of ﬁve trails,
and shaded region correspond to standard deviation."
RESULTS,0.28402366863905326,"0
20000
40000
60000
80000
100000
10
5 10
4 10
3 10
2 10
1"
RESULTS,0.2859960552268245,"Catchup
slowdown
ring
eight"
RESULTS,0.2879684418145957,(a) State Error
RESULTS,0.28994082840236685,"0
20000
40000
60000
80000
100000
10
6 10
5 10
4 10
3 10
2 10
1"
RESULTS,0.29191321499013806,"Catchup
slowdown
ring
eight"
RESULTS,0.2938856015779093,(b) Reward Error
RESULTS,0.2958579881656805,"Figure 3: Figures of state and reward error. Both state error and reward error < 10% in every
environment."
CONCLUSIONS,0.2978303747534517,"7
CONCLUSIONS"
CONCLUSIONS,0.29980276134122286,"In this paper, we propose algorithm DMPO, a model-based and decentralized multi-agent RL algo-
rithm. We then give a theoretical analysis on the algorithm to analyze its performance discrepancy,
compared with a model-free algorithm. By experiments in several tasks in networked systems,
we show that although our algorithm is decentralized and model-based, it matches the asymptotic
performance of some state-of-art multi-agent algorithms. From the results, we also conclude that
using extended value function instead of centralized value function did not sacriﬁce performance
massively, yet it makes our algorithm scalable."
CONCLUSIONS,0.30177514792899407,Under review as a conference paper at ICLR 2022
REFERENCES,0.3037475345167653,REFERENCES
REFERENCES,0.3057199211045365,"Masako Bando, Katsuya Hasebe, Akihiro Nakayama, Akihiro Shibata, and Yuki Sugiyama. Dy-
namical model of trafﬁc congestion and numerical simulation. Physical review E, 51(2):1035,
1995."
REFERENCES,0.3076923076923077,"Eugenio Bargiacchi, Timothy Verstraeten, and Diederik M. Roijers. Cooperative prioritized sweep-
ing. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021),
pp. 160–168. IFAAMAS, 2021."
REFERENCES,0.3096646942800789,"Bruno Bouzy and Marc M´etivier. Multi-agent model-based reinforcement learning experiments in
the pursuit evasion game. 2007."
REFERENCES,0.3116370808678501,"Ronen I Brafman and Moshe Tennenholtz. A near-optimal polynomial time algorithm for learning
in certain classes of stochastic games. Artiﬁcial Intelligence, 121(1-2):31–47, 2000."
REFERENCES,0.3136094674556213,"Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for net-
worked system control. In International Conference on Learning Representations (ICLR), 2020.
URL https://openreview.net/forum?id=Syx7A3NFvH."
REFERENCES,0.3155818540433925,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018."
REFERENCES,0.3175542406311637,"Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. In
Advances in Neural Information Processing Systems (NeurIPS), volume 1, pp. 1523–1530, 2001."
REFERENCES,0.31952662721893493,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS),
2019."
REFERENCES,0.3214990138067061,"I Ge Jin and G´abor Orosz. Dynamics of connected vehicle systems with delayed acceleration feed-
back. Transportation Research Part C: Emerging Technologies, 46:46–64, 2014."
REFERENCES,0.3234714003944773,"Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A
survey. Journal of artiﬁcial intelligence research, 4:237–285, 1996."
REFERENCES,0.3254437869822485,"Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.32741617357001973,"Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning
for networked systems with average reward. Advances in Neural Information Processing Systems
(NeurIPS), 33, 2020a."
REFERENCES,0.32938856015779094,"Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
multi-agent networked systems. In Learning for Dynamics and Control (L4DC), pp. 256–266.
PMLR, 2020b."
REFERENCES,0.33136094674556216,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.3333333333333333,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020."
REFERENCES,0.33530571992110453,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation.
arXiv preprint
arXiv:1506.02438, 2015."
REFERENCES,0.33727810650887574,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.33925049309664695,Under review as a conference paper at ICLR 2022
REFERENCES,0.34122287968441817,"Thiago D Simao and Matthijs TJ Spaan. Safe policy improvement with baseline bootstrapping in
factored environments. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
volume 33, pp. 4967–4974, 2019."
REFERENCES,0.3431952662721893,"Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale
in multiagent cooperative and competitive tasks. arXiv preprint arXiv:1812.09755, 2018."
REFERENCES,0.34516765285996054,"Wen Sun, Geoffrey J Gordon, Byron Boots, and J Bagnell. Dual policy iteration. Advances in Neural
Information Processing Systems (NeurIPS), 31:7059–7069, 2018."
REFERENCES,0.34714003944773175,"Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu,
Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M Bayen. Benchmarks for reinforce-
ment learning in mixed-autonomy trafﬁc. In Conference on robot learning, pp. 399–409. PMLR,
2018."
REFERENCES,0.34911242603550297,"Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M Bayen. Flow:
Architecture and benchmarking for reinforcement learning in trafﬁc control.
arXiv preprint
arXiv:1710.05465, 10, 2017a."
REFERENCES,0.3510848126232742,"Cathy Wu, Aboudy Kreidieh, Eugene Vinitsky, and Alexandre M Bayen. Emergent behaviors in
mixed-autonomy trafﬁc. In Conference on Robot Learning, pp. 398–407. PMLR, 2017b."
REFERENCES,0.3530571992110454,"Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning (ICML), pp. 5872–5881, 2018."
REFERENCES,0.35502958579881655,"Kaiqing Zhang, Zhuoran Yang, and Tamer Bas¸ar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms, 2019."
REFERENCES,0.35700197238658776,"Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent rl in zero-sum
markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems (NeurIPS), 33, 2020."
REFERENCES,0.358974358974359,Under review as a conference paper at ICLR 2022
REFERENCES,0.3609467455621302,"A
EXPERIMENT DETAILS"
REFERENCES,0.3629191321499014,"The code of our algorithm can be found at https://anonymous.4open.science/r/
RL-algorithms-0E72."
REFERENCES,0.36489151873767256,"A.1
ENVIRONMENT DESCRIPTION"
REFERENCES,0.3668639053254438,"The objective of CACC is to adaptively coordinate a line of vehicles to minimize the car-following
headway and speed perturbations based on real-time vehicle-to-vehicle communication. We conduct
experiments on two scenarios: CACC catch-up, CACC slow-down. The local observation of each
agent consists of headway h, velocity v, acceleration a , and is shared to neighbors within two steps.
The action of each agent is to choose appropriate hyper-parameters (α◦, β◦) for each OVM con-
troller Bando et al. (1995), selected from four levels {(0,0),(0.5,0),(0,0.5),(0.5,0.5)}, where α◦, β◦
denotes headway gain and relative gain for OVM controller respectively. The reward function is
deﬁned as (hi,t −h∗)2 + (vi,t −v∗
t )2 + 0.1a2
i,t to punish the gap between the current state to target
state and speed perturbations, where the target headway and velocity proﬁle are h∗= 20m and v∗
t ,
respectively. Whenever a collision happens (hi,t < 1m), a large penalty of -1000 is assigned to each
agent and the state becomes absorbing. An additional cost 5 (2hst −hi,t)2
+ is provided in training
for potential collisions. In catch-up scenario, initial headway of the ﬁrst vehicle is larger than the
target headway, thus the following agents learn how to catch up with the ﬁrst vehicle, where target
speed v∗
t = 15m/s and initial headway h1,0 > hi,0, ∀i ̸= 1. In slow-down scenario, target speed v∗
t
linearly decreases to 15m/s during the ﬁrst 30s and then stays at constant, thus agents learn how to
slow down speed cooperatively,where initial headway hi,0 = h∗."
REFERENCES,0.368836291913215,"Flow environments consists of Figure Eight and Ring Attenuation. The objective of these environ-
ments is letting the automated vehicles achieve a target average speed inside the road network while
avoiding collisions."
REFERENCES,0.3708086785009862,"The ﬁgure eight network, previously presented in (Wu et al., 2017b), acts as a closed representation
of an intersection. In a ﬁgure eight network containing a total of 14 vehicles, we witness the forma-
tion of queues resulting from vehicles arriving simultaneously at the intersection and slowing down
to obey right-of-way rules. This behavior signiﬁcantly reduces the average speed of vehicles in the
network. The state consists of velocity and position for the vehicle. The action is the acceleration
of the vehicle a ∈R[amin,amax]. The objective of the learning agent is to achieve high speeds while
penalizing collisions. Accordingly, the local reward function is deﬁned as ri = vdes −|vdes −vi|,
where vdes is an arbitrary target velocity."
REFERENCES,0.3727810650887574,"In Ring Attenuation, the objective is to achieve a high speed, while avoiding acceleration-
deceleration loops. To achieve this, vehicles are rewarded with their speed and punished for their
accelerations. The state and action of each vehicle is the same as Figure Eight. In the perspective of
a networked system, we assume that the vehicles are connected with the preceding and succeeding
vehicle, thus resulting in a loop-structured graph."
REFERENCES,0.3747534516765286,"V2V
V2V
V2V
V2V"
REFERENCES,0.3767258382642998,"Speed, acceleration,"
REFERENCES,0.378698224852071,headway
REFERENCES,0.3806706114398422,"Speed, acceleration,"
REFERENCES,0.3826429980276134,headway
REFERENCES,0.38461538461538464,"(a) CACC
(b) Figure Eight
(c) Ring Attenuation"
REFERENCES,0.3865877712031558,"Figure 4: Visualization of CACC and Flow environments. (a) A line of vehicles that need to keep
a stable velocity and desired headway. (b) Vehicles travel in a ﬁgure eight shaped road section to
learn the behavior at an intersection .(c) Vehicles travel in a ring to reduce stop-and-go waves."
REFERENCES,0.388560157790927,Under review as a conference paper at ICLR 2022
REFERENCES,0.3905325443786982,"B
HYPERPARAMETERS"
REFERENCES,0.39250493096646943,We list some of the key hyperparameters for DMPO and DPPO in Table 1 and 2.
REFERENCES,0.39447731755424065,"Hyperparameter
Scenario
Catch Up
Slow Down
Figure Eight
Ring Attenuation"
REFERENCES,0.39644970414201186,"Learning rate of critic
3e-4
3e-4
5e-5
5e-4
Learning rate of π
3e-4
3e-4
5e-5
5e-4
Learning rate of Model
3e-4
3e-4
5e-4
5e-4
Clip
-
-
0.15
0.2
GAE λ
1
1
0.5
0.5
KL target
-
-
7.5e-3
0.01
Model usage probability
1
1
0.5
0.5
Rollout Length
1
1
25
25
κ
1
1
3
3"
REFERENCES,0.398422090729783,Table 1: Hyperparameters for DMPO.
REFERENCES,0.40039447731755423,"Hyperparameter
Scenario
Catch Up
Slow Down
Figure Eight
Ring Attenuation"
REFERENCES,0.40236686390532544,"Learning rate of V
7e-4
5e-4
5e-4
1e-3
Learning rate of π
5e-5
5e-5
5e-5
1e-3
Clip
0.2
0.2
0.2
0.2
GAE λ
0.5
0.5
0.5
0.5
KL target
0.01
0.01
0.01
0.01
κ
2
2
3
3"
REFERENCES,0.40433925049309666,Table 2: Hyperparameters for DPPO.
REFERENCES,0.40631163708086787,"C
PROOF OF THEOREMS"
REFERENCES,0.40828402366863903,"C.1
REMARKS ON PROOFS OF LEMMAS AND THEOREMS"
REFERENCES,0.41025641025641024,"To bound the returns, we use TV distance to assess the distance between si, ai at every timestep,
based on the fact that |Esi,ai∼p1r(si, ai) −Esi,ai∼p2r(si, ai)| ≤rmax · DT V (p1∥p2). To achieve
this, we argue that in an INS, sT
i only depends on sT −1
N1
i , which is the 1-hop neighbors’ states at"
REFERENCES,0.41222879684418146,"previous timestep. Then, inductively, sT
i is only dependent on sT −1
N1
i , sT −2
N2
i , ..., sT −k
Nk
i , and ultimately,"
REFERENCES,0.41420118343195267,"s0
NT
i . Then we consider a chain of states xt = sNN−t
i
, t = 0, 1, ..., N in Lemma 4 to analyze how"
REFERENCES,0.4161735700197239,"st
i depends on model errors and policy divergences in a step-by-step manner. Insipred by Janner
et al., by summing the divergence at each timestep t = 0, 1, ..., we can generate a overall discounted
returns bound of branched rollout in Lemma 5. Lemma 6 is a simple corollary of Lemma 5, which
handles normal rollout scenario."
REFERENCES,0.4181459566074951,"If the returns are bounded, we can show that if every optimization step increase the return by C,
the real return would also almost monotonically increase. Speciﬁcally, if ˆη[πk+1] ≥ˆη[πk] + C,
combined with η[πk+1] ≥ˆη[πk] −C, we have η[πk+1] ≥ˆη[πk]. But note that ˆη[πk] increases
monotonically, and it’s a lower bound of η[πk+1]. This means that ˆη[πk+1] would almost monoton-
ically increase."
REFERENCES,0.42011834319526625,"C.2
PROOF OF THEOREM 1"
REFERENCES,0.42209072978303747,"Proof. ηp[π1, ..., πn] and η ˆp[π1, ..., πn] differs in two ways. First, they are estimated with different
transition dynamics. Second, the latter sample their states and actions with another policy, namely
πD. To deal with these divergences separately, we use an intermediate return η[πD], which is the"
REFERENCES,0.4240631163708087,Under review as a conference paper at ICLR 2022
REFERENCES,0.4260355029585799,return of πD in the environment. Then the difference can be bounded by:
REFERENCES,0.4280078895463511,"|ηp
i [π1, ..., πn] −η ˆp
i [π1, ..., πn]| ≤|ηp
i [π1, ..., πn] −ηp
i [πD]|
|
{z
}
L1"
REFERENCES,0.42998027613412226,"+ |ηp
i [πD] −η ˆp
i [π1, ..., πn]|
|
{z
}
L2 .
(9)"
REFERENCES,0.4319526627218935,"To bound L1, we apply Lemma 6 with ϵmi = 0. Then:"
REFERENCES,0.4339250493096647,L1 ≤2rmax
REFERENCES,0.4358974358974359,"1 −γ

ϵπi + ∞
X"
REFERENCES,0.4378698224852071,"k=0
γk+1 X"
REFERENCES,0.43984220907297833,"j∈Nk
i"
REFERENCES,0.4418145956607495,"ϵπj

.
(10)"
REFERENCES,0.4437869822485207,"In L2, both the dynamics and policies and different, therefore:"
REFERENCES,0.4457593688362919,L2 ≤2rmax
REFERENCES,0.4477317554240631,"1 −γ

ϵπi + ∞
X"
REFERENCES,0.44970414201183434,"k=0
γk+1 X"
REFERENCES,0.4516765285996055,"j∈Nk
i"
REFERENCES,0.4536489151873767,"(ϵmj + ϵπj)

.
(11)"
REFERENCES,0.4556213017751479,Putting Equation 10 and 11 yields:
REFERENCES,0.45759368836291914,"|ηp
i [π1, ..., πn] −η ˆp
i [π1, ..., πn]| ≤2rmax"
REFERENCES,0.45956607495069035,"1 −γ

2ϵπi + ∞
X"
REFERENCES,0.46153846153846156,"k=0
γk+1 X"
REFERENCES,0.4635108481262327,"j∈Nk
i"
REFERENCES,0.46548323471400394,"(ϵmj + 2ϵπj)

.
(12)"
REFERENCES,0.46745562130177515,"And because the binary relation of k-hop neighbor is symmetric, we have:"
N,0.46942800788954636,"1
n n
X i=1 X"
N,0.4714003944773176,"j∈N k
i ϵj = n
X j=1"
N,0.47337278106508873,"|V k
j |
n ϵj.
(13)"
N,0.47534516765285995,"Then, by averaging Equation 12, we have:"
N,0.47731755424063116,"|ηp[π1, ..., πn] −η ˆp[π1, ..., πn]| ≤1 n n
X"
N,0.47928994082840237,"i=1
|ηp
i [π1, ..., πn] −η ˆp
i [π1, ..., πn]|"
N,0.4812623274161736,≤2rmax
N,0.4832347140039448,"1 −γ
 1 n n
X"
N,0.48520710059171596,"i=1
ϵπi + ∞
X"
N,0.48717948717948717,"k=0
γk+1 1 n n
X i=1 X"
N,0.4891518737672584,"j∈Nk
i"
N,0.4911242603550296,"(ϵmj + 2ϵπj)
"
N,0.4930966469428008,= 2rmax
N,0.49506903353057197,"1 −γ
 1 n n
X"
N,0.4970414201183432,"i=1
ϵπi + ∞
X"
N,0.4990138067061144,"k=0
γk+1
n
X i=1"
N,0.5009861932938856,"|N k
i |
n
(ϵmi + 2ϵπi)
"
N,0.5029585798816568,"= 2rmax 1 −γ n
X i=1 ϵπi"
N,0.504930966469428,"n + (ϵmi + 2ϵπi) · ∞
X"
N,0.5069033530571992,"k=0
γk+1 |N k
i |
n
 (14)"
N,0.5088757396449705,"C.3
PROOF OF THEOREM 2"
N,0.5108481262327417,"Proof. To bound |ηp[π1, ..., πn] −ηbranch[π1, ..., πn]|, we need to analyze how do they differ from
each other. ηp denote the real returns of these policies in the environment. ηbranch is the returns esti-
mated in the branched rollout scheme. To explicitly illustrate this point, we describe their difference
in Table 3:"
N,0.5128205128205128,"By Lemma 5, we have:"
N,0.514792899408284,"|ηp
i [π1, ..., πn] −ηbranch
i
[π1, ..., πn]| ≤2rmax"
N,0.5167652859960552,"1 −γ
 T −1
X"
N,0.5187376725838264,"k=0
γk+1 X"
N,0.5207100591715976,"j∈Nk
i ϵmj + ∞
X"
N,0.5226824457593688,"k=T
γk+1 X"
N,0.52465483234714,"j∈Nk
i"
N,0.5266272189349113,"ϵπj

(15)"
N,0.5285996055226825,Under review as a conference paper at ICLR 2022
N,0.5305719921104537,"branch point
before
after
dynamics
policies
dynamics
policies
ηp
p
π
p
π
ηbranch
p
πD
ˆp
π"
N,0.5325443786982249,Table 3: The difference between ηp and ηbranch
N,0.534516765285996,"And because the binary relation of k-hop neighbor is symmetric, we have:"
N,0.5364891518737672,"1
n n
X i=1 X"
N,0.5384615384615384,"j∈N k
i ϵj = n
X j=1"
N,0.5404339250493096,"|V k
j |
n ϵj.
(16)"
N,0.5424063116370809,"Then, by averaging Equation 15, we have:"
N,0.5443786982248521,"|ηp[π1, ..., πn] −ηbranch[π1, ..., πn]| ≤1 n n
X"
N,0.5463510848126233,"i=1
|ηp
i [π1, ..., πn] −ηbranch
i
[π1, ..., πn]|"
N,0.5483234714003945,≤2rmax
N,0.5502958579881657,"1 −γ
 T −1
X"
N,0.5522682445759369,"k=0
γk+1 · 1 n n
X i=1 X"
N,0.5542406311637081,"j∈N k
i ϵmj + ∞
X"
N,0.5562130177514792,"k=T
γk+1 · 1 n n
X i=1 X"
N,0.5581854043392505,"j∈N k
i ϵπj
"
N,0.5601577909270217,= 2rmax
N,0.5621301775147929,"1 −γ
 T −1
X"
N,0.5641025641025641,"k=0
γk+1 · n
X i=1"
N,0.5660749506903353,"|N k
i |
n
ϵmi + ∞
X"
N,0.5680473372781065,"k=T
γk+1 · n
X i=1"
N,0.5700197238658777,"|N k
i |
n
ϵπi
"
N,0.571992110453649,"= 2rmax 1 −γ n
X i=1"
N,0.5739644970414202,"
ϵmi ·
  T −1
X"
N,0.5759368836291914,"k=0
γk+1 |N k
i |
n

+ ϵπi ·
 
∞
X"
N,0.5779092702169625,"k=T
γk+1 |N k
i |
n
 (17)"
N,0.5798816568047337,"C.4
PROOF OF COROLLARY 1"
N,0.5818540433925049,"Proof. Although p(s′|s, a) might not satisfy the criteria of an INS, we can construct another
transition dynamic ˜p = Qn
i=1 pi(si|sNi, ai), which is the product of marginal transitions, thus
being an INS. Recall that by deﬁnition, if p is a transition dynamic of a ξ-dependent system,
sups,a DT V (p(s′|s, a)∥˜p(s′|s, a)) ≤ξ. Then we divide the difference into two parts:"
N,0.5838264299802761,"ηp[π1, ..., πn] −ηbranch[π1, ..., πn]"
N,0.5857988165680473,"= ηp[π1, ..., πn] −η ˜p[π1, ..., πn]
|
{z
}
L1"
N,0.5877712031558185,"+ η ˜p[π1, ..., πn] −ηbranch[π1, ..., πn]
|
{z
}
L2 (18)"
N,0.5897435897435898,"The ﬁrst part is the difference of policy returns, estimated in two environments. Since there are no
policy divergences, and the transition dynamics only differs in a global manner, utilizing Lemma 2
yields that:
DT V
 
pt(s, a)∥˜pt(s, a)

≤tξ.
(19)"
N,0.591715976331361,"Then, we have:"
N,0.5936883629191322,"L1 = ηp[π1, ..., πn] −η ˜p[π1, ..., πn]"
N,0.5956607495069034,"≤2rmax ∞
X"
N,0.5976331360946746,"t=0
γtDT V
 
pt(s, a)∥˜pt(s, a)
"
N,0.5996055226824457,"≤2rmax ∞
X"
N,0.6015779092702169,"t=0
γttξ"
N,0.6035502958579881,= 2rmax
N,0.6055226824457594,"1 −γ ·
γ
1 −γ ξ. (20)"
N,0.6074950690335306,Under review as a conference paper at ICLR 2022
N,0.6094674556213018,"As for L2, because ˜p is an INS, we can directly apply Theorem 2:"
N,0.611439842209073,"L2 ≤2rmax 1 −γ n
X i=1"
N,0.6134122287968442,"
ϵmi ·
  T −1
X"
N,0.6153846153846154,"k=0
γk+1 |N k
i |
n

+ ϵπi ·
 
∞
X"
N,0.6173570019723866,"k=T
γk+1 |N k
i |
n

(21)"
N,0.6193293885601578,Summing Equation 20 and Equation 21 completes the proof.
N,0.621301775147929,"C.5
PROOF OF THEOREM 3"
N,0.6232741617357002,"Proof. In (Qu et al., 2020b), it was proven that if sN κ
i and aN κ
i are ﬁxed, then no matter how other
states and actions changes, Q-function will not change signiﬁcantly:"
N,0.6252465483234714,"|Q(sN κ
i , aNκ
i , sNκ
−i, aNκ
−i) −Q(sNκ
i , aNκ
i , s′
Nκ
−i, a′
Nκ
−i)| ≤rmax"
N,0.6272189349112426,"1 −γ γκ.
(22)"
N,0.6291913214990138,As value function is the expectation of Q-function
N,0.631163708086785,"Vi(s) = Ea∼πQ(s, a)
Vi(sNκ
i ) = Ea∼πQ(sNκ
i , aN κ
i ),
(23)"
N,0.6331360946745562,"we have,
|Vi(s) −Vi(sNκ
i )| = |Ea∼πQ(s, a) −Ea∼πQ(sNκ
i , aNκ
i )|"
N,0.6351084812623274,"≤Ea∼π|Q(s, a) −Q(sNκ
i , aNκ
i )| ≤rmax"
N,0.6370808678500987,"1 −γ γκ,
(24)"
N,0.6390532544378699,which concludes the proof.
N,0.6410256410256411,"C.6
PROOF OF THEOREM 4"
N,0.6429980276134122,Proof. The difference of the gradients can be written as
N,0.6449704142011834,"gi −˜gi =E( ˆA −˜A)∇θi log πi(ai|s ¯
Ni) = 1 nE[
X"
N,0.6469428007889546,"j /∈Nκ
i"
N,0.6489151873767258,"ˆAj]∇θi log πi(ai|s ¯
Ni) + 1 nE[
X"
N,0.650887573964497,"j∈Nκ
i
( ˆAj −˜Aj)]∇θi log πi(ai|s ¯
Ni) = 1 nE[
X"
N,0.6528599605522682,"j /∈Nκ
i
(rj(sj, aj) + γVj(s′) −Vj(s))]∇θi log πi(ai|s ¯
Ni) + 1 nE
X"
N,0.6548323471400395,"j∈Nκ
i
[(rj(sj, aj) + γVj(s′) −Vj(s)) −(rj(sj, aj) + γVj(s′
Nκ
i ) −Vj(sNκ
i ))]∇θi log πi(ai|s ¯
Ni)"
N,0.6568047337278107,"=L1 + L2.
(25)
Because for any function b(s) that depends only on s, E[b(s) log πθi
i (ai|s ¯
Ni)] = 0. Therefore, L2 in
Equation 25 becomes:"
N,0.6587771203155819,"|L2| ≤1 nE
X"
N,0.6607495069033531,"j∈Nκ
i
γ|Vj(s′) −Vj(s′
N κ
i )]||∇θi log πi(ai|s ¯
Ni)|"
N,0.6627218934911243,"≤|N κ
i |
n
γκ+1"
N,0.6646942800788954,1 −γ rmaxgmax. (26)
N,0.6666666666666666,"For L1, note that rj(sj, aj) + γVj(s′) −Vj(s) = −Vj(s) + rj(sj, aj) + Pκ−2
t=1 Eγtrj(st
j, at
j) +
γκ−1Vj(sκ−1). And in an INS, st
j, at
j, t = 0, 1, ..., κ −2 is not affected by policy πi if j /∈N κ
i , we
have that
|L1| ≤1 nE
X"
N,0.6686390532544378,"j /∈Nκ
i
|γκ−1Vj(sκ−1)||∇θi log πi(ai|s ¯
Ni)|"
N,0.6706114398422091,"≤(1 −N κ
i
n ) γκ−1"
N,0.6725838264299803,1 −γ rmaxgmax. (27)
N,0.6745562130177515,Under review as a conference paper at ICLR 2022
N,0.6765285996055227,"Put Equation 26 and 27 together, we have"
N,0.6785009861932939,|gi −˜gi| ≤|L1| + |L2|
N,0.6804733727810651,"≤|N κ
i |
n
γκ+1"
N,0.6824457593688363,"1 −γ rmaxgmax + (1 −N κ
i
n ) γκ−1"
N,0.6844181459566075,1 −γ rmaxgmax
N,0.6863905325443787,= γκ−1
N,0.6883629191321499,"1 −γ [1 −(1 −γ2)N κ
i
n ]rmaxgmax. (28)"
N,0.6903353057199211,"D
USEFUL LEMMAS"
N,0.6923076923076923,"Lemma 1. (TVD of Joint Distribution) Consider two distributions p1(x, y) = p1(x)p1(y|x) and
p2(x, y) = p2(x)p2(y|x). The total variation distance between them can be bounded as:"
N,0.6942800788954635,"DT V
 
p1(x, y)∥p2(x, y)

≤DT V
 
p1(x)∥p2(x)

+ Ex∼p2

DT V
 
p1(y|x)∥p2(y|x)
"
N,0.6962524654832347,"≤DT V
 
p1(x)∥p2(x)

+ max
x
DT V
 
p1(y|x)∥p2(y|x)
"
N,0.6982248520710059,Proof.
N,0.7001972386587771,"DT V
 
p1(x, y)∥p2(x, y)

= 1 2 X"
N,0.7021696252465484,"x,y
|p1(x, y) −p2(x, y)| = 1 2 X"
N,0.7041420118343196,"x,y
|p1(x)p1(y|x) −p2(x)p2(y|x)| = 1 2 X"
N,0.7061143984220908,"x,y
|p1(x)p1(y|x) −p2(x)p1(y|x) + p2(x)p1(y|x) −p2(x)p2(y|x)| ≤1 2 X"
N,0.7080867850098619,"x,y
|p1(x) −p2(x)|p1(y|x) + 1 2 X"
N,0.7100591715976331,"x,y
p2(x)|p1(y|x) −p2(y|x)| = 1 2 X"
N,0.7120315581854043,"x
|p1(x) −p2(x)| +
X"
N,0.7140039447731755,"x
p2(x)DT V (p1(y|x)∥p2(y|x))"
N,0.7159763313609467,"= DT V
 
p1(x)∥p2(x)

+ Ex∼p2

DT V
 
p1(y|x)∥p2(y|x)
"
N,0.717948717948718,"≤DT V
 
p1(x)∥p2(x)

+ max
x
DT V
 
p1(y|x)∥p2(y|x)
"
N,0.7199211045364892,"Lemma 2. Suppose there are two chains of distributions {xt
1, t ≥0}, {xt
2, t ≥0}. At time t, the
states of both chains share an identical state space xt
1, xt
2 ∈X t. Suppose these two chains satisfy
a Markov-like property: pt+1(xt+1|xt, ..., x1, x0) = pt+1(xt+1|xt). Then, the TVD of distributions
of two chains at time t can be decomposed as:"
N,0.7218934911242604,"DT V
 
pT
1 (xT )∥pT
2 (xT )

≤DT V
 
p0
1(x0)∥p0
2(x0)

+ T
X"
N,0.7238658777120316,"t=1
Est−1∼pt−1
2
DT V
 
pt
1(xt|xt−1)∥pt
2(xt|xt−1)

."
N,0.7258382642998028,Proof. We prove this lemma by induction.
N,0.727810650887574,"When T = 0, it’s easy to see that this lemma is true."
N,0.7297830374753451,Under review as a conference paper at ICLR 2022
N,0.7317554240631163,Assume it is true for T = k. We have:
N,0.7337278106508875,"|pk+1
1
(xk+1) −pk+1
2
(xk+1)| =|
X"
N,0.7357001972386588,"xk
[pk+1
1
(xk+1|xk)pk
1(xk) −pk+1
2
(xk+1|xk)pk
2(xk)]| ≤
X"
N,0.73767258382643,"xk
|pk+1
1
(xk+1|xk)pk
1(xk) −pk+1
2
(xk+1|xk)pk
2(xk)| =
X"
N,0.7396449704142012,"xk
|pk+1
1
(xk+1|xk)pk
1(xk) −pk+1
1
(xk+1|xk)pk
2(xk)"
N,0.7416173570019724,"+ pk+1
1
(xk+1|xk)pk
2(xk) −pk+1
2
(xk+1|xk)pk
2(xk)| ≤
X xk"
N,0.7435897435897436,"
pk
2(xk)|pk+1
1
(xk+1|xk) −pk+1
2
(xk+1|xk)| + pk+1
1
(xk+1|xk)|pk
1(xk) −pk
2(xk)|
"
N,0.7455621301775148,"=Exk∼pk
2[|pk+1
1
(xk+1|xk) −pk+1
2
(xk+1|xk)|] +
X"
N,0.747534516765286,"xk
pk+1
1
(xk+1|xk)|pk
1(xk) −pk
2(xk)|"
N,0.7495069033530573,"DT V
 
pk+1
1
(xk+1)∥pk+1
2
(xk+1)
 =1 2 X"
N,0.7514792899408284,"xk+1
|pk+1
1
(xk+1) −pk+1
2
(xk+1)| ≤1 2 X xk+1"
N,0.7534516765285996,"
Exk∼pk
2[|pk+1
1
(xk+1|xk) −pk+1
2
(xk+1|xk)|] +
X"
N,0.7554240631163708,"xk
pk+1
1
(xk+1|xk)|pk
1(xk) −pk
2(xk)|
"
N,0.757396449704142,"=Esk∼pk
2DT V
 
pk+1
1
(xk+1|xk)∥pk+1
2
(xk+1|xk)

+ DT V
 
pk
1(xk)∥pk
2(xk)
"
N,0.7593688362919132,"≤DT V
 
p0
1(x0)∥p0
2(x0)

+ k+1
X"
N,0.7613412228796844,"t=1
Est−1∼pt−1
2
DT V
 
pt
1(xt|xt−1)∥pt
2(xt|xt−1)
"
N,0.7633136094674556,Then this theorem holds for all n ∈N.
N,0.7652859960552268,"Lemma 3. Consider two distributions with pdf/pmf p(x) and q(x), where x = (x1, ..., xn) ∈Rn.
Suppose p and q can be factorized as: p(x) = Qn
i=1 pi(xi), q(x) = Qn
i=1 qi(xi). Then we have:"
N,0.7672583826429981,"DT V [p(x)∥q(x)] ≤ n
X"
N,0.7692307692307693,"i=1
DT V [pi(xi)∥qi(xi)]."
N,0.7712031558185405,"Also, if the distance is measured by KL-divergence, we have:"
N,0.7731755424063116,"DKL[p(x)∥q(x)] = n
X"
N,0.7751479289940828,"i=1
DKL[pi(xi)∥qi(xi)]."
N,0.777120315581854,Under review as a conference paper at ICLR 2022
N,0.7790927021696252,"Proof. We prove this result for discrete distributions, yet by replacing sum with integration, this
result stays true in continuous case."
N,0.7810650887573964,DT V [p(x)∥q(x)] = 1 2 X
N,0.7830374753451677,"x
|p(x) −q(x)| = 1 2 X"
N,0.7850098619329389,"x1,...,xn
| n
Y"
N,0.7869822485207101,"k=1
pk(xk) − n
Y"
N,0.7889546351084813,"k=1
qk(xk)| = 1 2 X"
N,0.7909270216962525,"x1,...,xn  n
X i=1"
N,0.7928994082840237,"h i−1
Y"
N,0.7948717948717948,"k=1
pk(xk) n
Y"
N,0.796844181459566,"k=i+1
qk(xk)(pi(xi) −qi(xi))
i ≤1 2 X"
N,0.7988165680473372,"x1,...,xn n
X i=1"
N,0.8007889546351085,"h i−1
Y"
N,0.8027613412228797,"k=1
pk(xk) n
Y"
N,0.8047337278106509,"k=i+1
qk(xk)|pi(xi) −qi(xi)|
i = 1 2 n
X i=1 X"
N,0.8067061143984221,"xi
|pi(xi) −qi(xi)| = n
X"
N,0.8086785009861933,"i=1
DT V [pi(xi)∥qi(xi)]."
N,0.8106508875739645,"In KL-divergence case, we have:"
N,0.8126232741617357,"DKL[p(x)∥q(x)] =
X"
N,0.814595660749507,"x
p(x) log p(x) q(x) =
X"
N,0.8165680473372781,"x1,...,xn"
N,0.8185404339250493,"h
p(x1, ..., xn) n
X"
N,0.8205128205128205,"i=1
log pi(xi)"
N,0.8224852071005917,"qi(xi) i = n
X i=1 X"
N,0.8244575936883629,"x1,...,xn h
n
Y"
N,0.8264299802761341,"k=1
pk(xk) log pi(xi)"
N,0.8284023668639053,"qi(xi) i = n
X i=1 X"
N,0.8303747534516766,"xi
pi(xi) log pi(xi)"
N,0.8323471400394478,"qi(xi) = n
X"
N,0.834319526627219,"i=1
DKL(pi(xi)∥qi(xi))."
N,0.8362919132149902,"Lemma 4. (N-step distribution distance) Suppose the expected TVD between two dynam-
ics transitions is bounded as ϵmi
= maxsNi,ai DT V

p(s′
i|sNi, ai)∥ˆp(s′
i|sNi, ai)

and ϵπi
=
maxs DT V

πi(ai|sNi)∥ˆπi(ai|sNi)

. If N ≤κ, the N-step distribution distance is bounded as:"
N,0.8382642998027613,"DT V

pN
i (si)∥ˆpN
i (si)

≤ N−1
X t=0 X"
N,0.8402366863905325,"j∈N t
i
(ϵπj + ϵmj). Thus,"
N,0.8422090729783037,"DT V

pN
i (si, ai)∥ˆpN
i (si, ai)

≤ϵπi + N−1
X t=0 X"
N,0.8441814595660749,"j∈Nt
i
(ϵπj + ϵmj)."
N,0.8461538461538461,"Proof. Consider a chain of regional state xt = sN N−t
i
. Two chains of distributions pt(xt), ˆp(xt)
denote the distributions of xt under the environment and our model, respectively. First, because of
the property of an INS, these two chains’ transition dynamics can be decomposed as:"
N,0.8481262327416174,Under review as a conference paper at ICLR 2022
N,0.8500986193293886,"p(xt|xt−1) = p(s′
NN−t
i
|sNN−t+1
i
) =
Y"
N,0.8520710059171598,"j∈N N−t
i"
N,0.854043392504931,"pj(s′
j|sVj)."
N,0.8560157790927022,"And because pj(s′
j|sVj) = P
aj pj(s′
j|sVj, aj)πj(aj|sVj), by the property of TVD, we know that:"
N,0.8579881656804734,"DT V

pj(s′
j|sVj)∥ˆpj(s′
j|sVj)

≤ϵπj + ϵmj."
N,0.8599605522682445,"With Lemma 3, we have:"
N,0.8619329388560157,"DT V

p(xt|xt−1)∥ˆp(xt|xt−1)

≤
X"
N,0.863905325443787,"j∈N N−t
i"
N,0.8658777120315582,(ϵπj + ϵmj).
N,0.8678500986193294,"Then, by Lemma 2, we know that"
N,0.8698224852071006,"DT V

pN
i (si)∥ˆpN
i (si)

= DT V

pN
i (xN)∥ˆpN
i (xN)
 ≤ N
X t=1 X"
N,0.8717948717948718,"j∈NN−t
i"
N,0.873767258382643,"(ϵπj + ϵmj) = N−1
X t=0 X"
N,0.8757396449704142,"j∈N t
i
(ϵπj + ϵmj),"
N,0.8777120315581854,"which completes the proof of the ﬁrst part. And by Lemma 1, the second part holds true."
N,0.8796844181459567,"Lemma 5. (Returns bound measured in branched rollout) Consider two MDPs p(s) and ˆp(s). Sup-
pose they both adopt T-branched rollout scheme. Before the branch, suppose the dynamics dis-
tributions are bounded as maxsNi,ai DT V
 
ppre
i
(s′
i|sNi, ai)∥ˆppre
i
(s′
i|sNi, ai)

= ϵpre
mi , and policy
divergences are bounded as maxsNi DT V
 
πpre
i
(ai|sNi)∥ˆπpre
i
(ai|sNi)

= ϵpre
πi . After the branch,
ϵpost
mi
and ϵpost
πi
are deﬁned similarly. Then the (local) T-step returns are bounded as:"
N,0.8816568047337278,|ηi −ˆηi| ≤2rmax
N,0.883629191321499,"1 −γ

ϵpost
πi
+"
N,0.8856015779092702,"T −1
X"
N,0.8875739644970414,"k=0
γk+1 X"
N,0.8895463510848126,"j∈Nk
i"
N,0.8915187376725838,"(ϵpost
mj + ϵpost
πj ) + ∞
X"
N,0.893491124260355,"k=T
γk+1 X"
N,0.8954635108481263,"j∈Nk
i"
N,0.8974358974358975,"(ϵpre
mj + ϵpre
πj )
"
N,0.8994082840236687,"Proof. We prove this by estimating the state-action distribution divergence at each timestep. For
notation simplicity, we denote ϵpre
Nk
i = ϵpre
Nk
i = P"
N,0.9013806706114399,"j∈Nk
i (ϵpre
mj + ϵpre
πj ), and ϵpost
Nk
i
analogously. If we
divide the chains into two parts: pre-branch and post-branch, and apply Lemma 4 on both parts, we
have:"
N,0.903353057199211,For t ≤T:
N,0.9053254437869822,"DT V

pt
i(si, ai)∥ˆpt
i(si, ai)

≤ϵpost
πi
+ t−1
X"
N,0.9072978303747534,"k=0
ϵpost
N k
i ,"
N,0.9092702169625246,and for t > T:
N,0.9112426035502958,"DT V

pt
i(si, ai)∥ˆpt
i(si, ai)

≤ϵpost
πi
+"
N,0.9132149901380671,"T −1
X"
N,0.9151873767258383,"k=0
ϵpost
N k
i
+ t−1
X"
N,0.9171597633136095,"k=T
ϵpre
Nk
i ."
N,0.9191321499013807,Under review as a conference paper at ICLR 2022
N,0.9211045364891519,"Then, the difference of discounted distribution can be bounded as:"
N,0.9230769230769231,"DT V

pi(si, ai)∥ˆpi(si, ai)

≤(1 −γ) ∞
X"
N,0.9250493096646942,"t=0
DT V

pt
i(si, ai)∥ˆpt
i(si, ai)
"
N,0.9270216962524654,"≤(1 −γ) T
X"
N,0.9289940828402367,"t=0
γt 
ϵpost
πi
+ t−1
X"
N,0.9309664694280079,"k=0
ϵpost
Nk
i

+"
N,0.9329388560157791,"(1 −γ) ∞
X"
N,0.9349112426035503,"t=T +1
γt 
ϵpost
πi
+"
N,0.9368836291913215,"T −1
X"
N,0.9388560157790927,"k=0
ϵpost
Nk
i
+ t−1
X"
N,0.9408284023668639,"k=T
ϵpre
N k
i
"
N,0.9428007889546351,"= ϵpost
πi
+ (1 −γ)"
N,0.9447731755424064,"T −1
X"
N,0.9467455621301775,"k=0
ϵpost
Nk
i (γk+1 + γk+2 + ...)+"
N,0.9487179487179487,"(1 −γ) ∞
X"
N,0.9506903353057199,"k=T
ϵpre
N k
i (γk+1 + γk+2 + ...)"
N,0.9526627218934911,"= ϵpost
πi
+"
N,0.9546351084812623,"T −1
X"
N,0.9566074950690335,"k=0
ϵpost
Nk
i γk+1 + ∞
X"
N,0.9585798816568047,"k=T
ϵpre
Nk
i γk+1."
N,0.960552268244576,We can convert this bound into the returns bound:
N,0.9625246548323472,"|ηi −ˆηi| ≤ ∞
X"
N,0.9644970414201184,"t=0
γt|ri(st
i, ai) −ri(ˆst
i, ˆat
i)|"
N,0.9664694280078896,≤2rmax
N,0.9684418145956607,"1 −γ (1 −γ) ∞
X"
N,0.9704142011834319,"t=0
DT V

pt
i(si, ai)∥ˆpt
i(si, ai)
"
N,0.9723865877712031,= 2rmax
N,0.9743589743589743,"1 −γ DT V

pi(si, ai)∥ˆpi(si, ai)
"
N,0.9763313609467456,= 2rmax
N,0.9783037475345168,"1 −γ

ϵpost
πi
+"
N,0.980276134122288,"T −1
X"
N,0.9822485207100592,"k=0
ϵpost
Nk
i γk+1 + ∞
X"
N,0.9842209072978304,"k=T
ϵpre
Nk
i γk+1"
N,0.9861932938856016,"Lemma 6. (Returns bound measured in full length rollout) Consider two MDPs p(s) and ˆp(s).
Suppose they both run their rollouts until the end of every trajectory, and the dynamic distributions
are bounded as maxsNi,ai DT V
 
pi(s′
i|sNi, ai)∥ˆpi(s′
i|sNi, ai)

= ϵmi, while policy divergences are
bounded as maxsNi DT V
 
πi(ai|sNi)∥ˆπi(ai|sNi)

= ϵπi. Then the (local) returns are bounded as:"
N,0.9881656804733728,|ηi −ˆηi| ≤2rmax
N,0.9901380670611439,"1 −γ

ϵπi + ∞
X"
N,0.9921104536489151,"k=0
γk+1 X"
N,0.9940828402366864,"j∈Nk
i"
N,0.9960552268244576,"(ϵmj + ϵπj)
"
N,0.9980276134122288,"Proof. We can think it as a special case of branched rollout, where ϵpost = ϵpre = ϵ for every
subscript, and T = 0. From this perspective, applying the result of Lemma 5 completes the proof."
