Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0021008403361344537,"Machine learning systems typically assume that the distributions of training and
test sets match closely. However, a critical requirement of such systems in the
real world is their ability to generalize to unseen domains. Here, we propose an
inter-domain gradient matching objective that targets domain generalization by
maximizing the inner product between gradients from different domains. Since
direct optimization of the gradient inner product can be computationally prohibitive
— it requires computation of second-order derivatives —- we derive a simpler
ﬁrst-order algorithm named Fish that approximates its optimization. We perform
experiments on the WILDS benchmark, which captures distribution shift in the
real world, as well as the DOMAINBED benchmark that focuses more on synthetic-
to-real transfer. Our method produces competitive results on both benchmarks,
demonstrating its effectiveness across a wide range of domain generalization tasks.
Code is available at https://github.com/YugeTen/fish."
INTRODUCTION,0.004201680672268907,"1
INTRODUCTION"
INTRODUCTION,0.0063025210084033615,Trajectory of     during IDGM training
INTRODUCTION,0.008403361344537815,inner product small
INTRODUCTION,0.01050420168067227,at initial point
INTRODUCTION,0.012605042016806723,Trajectory of     during ERM training
INTRODUCTION,0.014705882352941176,Gradient of the loss for domain 1
INTRODUCTION,0.01680672268907563,Gradient of the loss for domain 2
INTRODUCTION,0.018907563025210083,Domain 1
INTRODUCTION,0.02100840336134454,"product
large inner"
INTRODUCTION,0.023109243697478993,Domain 2
INTRODUCTION,0.025210084033613446,"Figure 1: Isometric projection of train-
ing with ERM (blue) vs. our objective
(dark blue), using data from Figure 2."
INTRODUCTION,0.0273109243697479,"The goal of domain generalization is to train models that
performs well on unseen, out-of-distribution data, which is
crucial in practice for model deployment in the wild. This
seemingly difﬁcult task is made possible by the presence
of multiple distributions/domains at train time. As we
have seen in past work (Arjovsky et al., 2019; Gulrajani
and Lopez-Paz, 2020; Ganin et al., 2016), a key aspect
of domain generalization is to learn from features that
remain invariant across multiple domains, while ignoring
those that are spuriously correlated to label information
(as deﬁned in Torralba and Efros (2011); Stock and Cisse
(2017)). Consider, for example, a model that is built to distinguish between cows and camels using
photos collected in nature under different climates. Since CNNs are known to have a bias towards
texture (Geirhos et al., 2018; Brendel and Bethge, 2019), if we simply try to minimize the average
loss across different domains, the classiﬁer is prone to spuriously correlate “cow” with grass and
“camels” with desert, and predict the species using only the background. Such a classiﬁer can be
rendered useless when the animals are placed indoors or in a zoo. However, if the model could
recognize that while the landscapes change with climate, the biological characteristics of the animals"
INTRODUCTION,0.029411764705882353,"∗Work done during internship at Facebook AI Research.
†Now at Zoom."
INTRODUCTION,0.031512605042016806,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03361344537815126,"(e.g. humps, neck lengths) remain invariant and use those features to determine the species, we have
a much better chance at generalizing to unseen domains."
INTRODUCTION,0.03571428571428571,"Similar intuitions have already motivated many approaches that consider learning “invariances”
across domains as the main challenge of domain generalization. Typically, a lot of these work focus
on learning invariant representations directly by removing the domain information (Ganin et al.,
2016; Sun and Saenko, 2016; Li et al., 2018). In this work, we propose an inter-domain gradient
matching (IDGM) objective. Instead of learning invariant features by matching the distributions of
representations from different domains, our approach does so by encouraging consistent gradient
directions across domains. Speciﬁcally, our IDGM objective augments the loss with an auxiliary
term that maximizes the gradient inner product between domains, which encourages the alignment
between the domain-speciﬁc gradients. By simultaneously minimizing the loss and matching the
gradients, IDGM encourages the optimization paths to be the same for all domains, favouring invariant
predictions. Figure 1 illustrates a motivating example described in Section 3.2: given 2 domains,
each containing one invariant feature (orange cross) and one spurious feature (yellow and red cross).
While empirical risk minimization (ERM) minimizes the average loss between these domains at the
cost of learning spurious features only, IDGM aligns the gradient directions and is therefore able to
focus on the invariant feature."
INTRODUCTION,0.037815126050420166,"While the IDGM objective achieves the desirable learning dynamic in theory, naive optimization
of the objective by gradient descent is computationally costly due to the second-order derivatives.
Leveraging the theoretical analysis of Reptile, a meta-learning algorithm (Nichol et al., 2018), we
propose to approximate the gradients of IDGM using a simple ﬁrst-order algorithm, which we name
Fish. Fish is simple to implement, computationally effective and as we show in our experiments,
functionally similar to direct optimization of IDGM."
INTRODUCTION,0.03991596638655462,"Our contribution is a simple but effective algorithm for domain generalization, which exhibits state-of-
the-art performance on 13 datasets from recent domain generalization benchmark WILDS (Koh et al.,
2020) and DOMAINBED (Gulrajani and Lopez-Paz, 2020). The strong performance of our method
on a variety of datasets demonstrates that it is broadly applicable in different applications/subgenres
of domain generalization tasks. We also perform detailed analysis in Section 4.4 to explain the
effectiveness of our proposed algorithm."
RELATED WORK,0.04201680672268908,"2
RELATED WORK"
RELATED WORK,0.04411764705882353,"Domain Generalization
In domain generalization, the training data is sampled from one or many
source domains, while the test data is sampled from a new target domain. We will now discuss the
ﬁve main families of approaches to domain generalization:"
RELATED WORK,0.046218487394957986,"1. Distributional Robustness (DRO): DRO approaches minimize the worst-case loss over a set of
data distributions constructed from the training domains. Rojas-Carulla et al. (2015) proposed
DRO to address covariate shift (Gretton et al., 2009a;b), where P(Y |X) remains constant across
domains but P(X) changes. Later work also studied subpopulation shift, where the train and test
distributions are mixtures of the same domains, but the mixture weights change between train and
test (Hu et al., 2018; Sagawa et al., 2019);"
RELATED WORK,0.04831932773109244,"2. Domain-invariant representation learning: This family of approaches to domain generalization
aims at learning high-level features that make domains statistically indistinguishable. Prediction
is then based on these features only. The principle is motivated by a generalization error bound
for unsupervised domain adaptation (Ben-David et al., 2010; Ganin et al., 2016), but the approach
readily applies to domain generalization (Gulrajani and Lopez-Paz, 2020; Koh et al., 2020).
Algorithms include penalising the domain-predictive power of the model (Ganin et al., 2016;
Wang et al., 2019; Huang et al., 2020), aligning domains through contrastive loss (Motiian et al.,
2017), matching mean and variance of feature distributions across domains (Sun and Saenko,
2016), learning useful representations by solving Jigsaw puzzles (Carlucci et al., 2019), using the
maximum mean discrepancy to match the feature distributions (Li et al., 2018b) or introducing
training constraints across domains using mixup formulation (Yan et al., 2020)."
RELATED WORK,0.05042016806722689,"3. Invariant Risk Minimization (IRM): IRM is proposed by Arjovsky et al. (2019), which learns
an intermediate representation such that the optimal classiﬁers (on top of this representation) of all
domains are the same. The motivation is to exploit invariant causal effects between domains while"
RELATED WORK,0.052521008403361345,Published as a conference paper at ICLR 2022
RELATED WORK,0.0546218487394958,"reducing the effect of domain-speciﬁc spurious correlations. From an optimization perspective,
when IRM reaches its optimal, all the gradients (for the linear classiﬁer) has to be zero. This is why
IRM’s solution won’t deviate from ERM when ERM is optimal for every domain, which is not the
case for our proposed IDGM objective due to the gradient inner product term."
RELATED WORK,0.05672268907563025,"4. Data augmentation: More recently, approaches that simulates unseen domains through speciﬁc
types of data augmentation/normalization has been gaining traction. This includes work such as
Zhou et al. (2020); Volpi and Murino (2019); Ilse et al. (2021), as well as Seo et al. (2019) which
utilises ensemble learning."
RELATED WORK,0.058823529411764705,"5. Gradient alignment: Two concurrent work – Koyama and Yamaguchi (2021) and Parascandolo
et al. (2021) – utilise similar gradient-alignment principle for domain generalization. Koyama and
Yamaguchi (2021) proposes IGA, which learns invariant features by minimizing the variance of
inter-domain gradients. The key difference between IGA and our objective is that IGA is completely
identical to ERM when ERM is the optimal solution on every training domain, since the variances
of the gradients will be zero. While they achieve the best performance on the training set, both
IGA and ERM could in some cases, completely fail when generalizing to unseen domains (see
Section 3.2 for such an example). Our method, on the contrary, biases towards non-ERM solutions
as long as the gradients are aligned, and is therefore able to avoid this issue. Parascandolo et al.
(2021) on the other hand, proposes to mask out the gradients that have opposite signs for different
domains. Unlike their work that prunes gradients that are inconsistent, our approach actively
encourage gradients from different domains to be consistent by maximizing the gradient inner
product. Additionally, in Lopez-Paz and Ranzato (2017) we also see the application of gradient-
alignment, however in this case it is applied under the continual learning setting to determine
whether a gradient update will increase the loss of the previous tasks."
RELATED WORK,0.06092436974789916,"Apart from these algorithms that are tailored for domain generalization, a well-studied baseline in
this area is ERM, which simply minimizes the average loss over training domains. Using vanilla
ERM is theoretically unfounded (Hashimoto et al., 2018; Blodgett et al., 2016; Tatman, 2017) since
ERM is guaranteed to work only when train and test distributions match. Nonetheless, recent
benchmarks suggest that ERM obtains strong performance in practice, in many case surpassing
domain generalization algorithms (Gulrajani and Lopez-Paz, 2020; Koh et al., 2020). Our goal is to
ﬁll this gap, using an algorithm signiﬁcantly simpler than previous approaches."
RELATED WORK,0.06302521008403361,"Connections to meta-learning
There are close connections between meta-learning (Thrun and
Pratt, 1998) and (multi-source) domain adaptation. In fact, there are a few works in domain generaliza-
tion that are inspired by the meta-learning principles, such as Li et al. (2018a); Balaji et al. (2018); Li
et al. (2019); Dou et al. (2019). Speciﬁcally, Li et al. (2020) also proposes to adapt Reptile for domain
generalization tasks, however they study their method under the sequential learning setting, whereas
our method can be trained on all domains and therefore learns faster, especially when the number
of domains is large. In Ren et al. (2018), we also see the leveraging of gradient inner product in
meta-learning, where it is used to determine the importance weight of training examples. We discuss
the connection between our proposed algorithm to meta-learning in more details in Appendix A.1."
RELATED WORK,0.06512605042016807,"Note that our proposed algorithm Fish is similar to the Mean Teacher method (Tarvainen and Valpola,
2017), where a teacher model (equivalent to θ in Algorithm 1) is computed using a moving average
of the student model (equivalent to ˜θ in Algorithm 1)."
METHODOLOGY,0.06722689075630252,"3
METHODOLOGY"
GOALS,0.06932773109243698,"3.1
GOALS"
GOALS,0.07142857142857142,"Consider a training dataset Dtr consisting of S domains Dtr = {D1, · · · , DS}, where each domain s
is characterized by a dataset Ds := {(xs
i, ys
i )}ns
i=1 containing data drawn i.i.d. from some probability
distribution. Also consider a test dataset Dte consisting of T domains Dte = {DS+1, · · · , DS+T },
where Dtr ∩Dte = ∅. The goal of domain generalization is to train a model with weights θ that
generalizes well on the test dataset Dte such that:"
GOALS,0.07352941176470588,"arg min
θ
ED∼Dte E(x,y)∼D [l((x, y); θ)] ,
(1)"
GOALS,0.07563025210084033,Published as a conference paper at ICLR 2022
GOALS,0.07773109243697479,"Figure 2: All 3 domains (rows) consist of 3 types of inputs (columns): 1) x1, left: makes up for
50% of each domain, label is always 0, x1 is always [0, 0, 0, 0]; 2) x2, middle: makes up for 40% of
each domain, label is always 1, x2 changes for each domain; 3) x3, right: makes up for 10% of each
domain, labels are randomly assigned with 30% of y = 1 and 70% of y = 0, x3 is always [1, 0, 0, 0]."
GOALS,0.07983193277310924,"where l((x, y); θ) is the loss of model θ evaluated on (x, y)."
GOALS,0.0819327731092437,"A naive approach is to apply ERM, which simply minimizes the average loss on Dtr, ignoring the
discrepancy between train and test domains:"
GOALS,0.08403361344537816,"Lerm(Dtr; θ) = ED∼Dtr E(x,y)∼D [l((x, y); θ)] .
(2)"
GOALS,0.0861344537815126,"The ERM objective does not exploit the invariance across different domains in Dtr and could perform
arbitrarily poorly on test data. We demonstrate this effect with the following simple linear example."
GOALS,0.08823529411764706,"3.2
THE PITFALL OF ERM: A LINEAR EXAMPLE"
GOALS,0.09033613445378151,"Consider a binary classiﬁcation setup where data (x, y) ∈B4 × B, and a data instance is denoted x =
[f1, f2, f3, f4], y. The train domains are {D1, D2}, and test domain is D3. The goal is to learn a
linear model Wx + b = y, W ∈R4, b ∈R on the train data, such that the error on the test domain is
minimized. The setup and dataset of this example is illustrated in Figure 2."
GOALS,0.09243697478991597,"As we can see in Figure 2, f1 is the invariant feature in this dataset, since the correlation between f1
and y is stable across different domains. The relationships between y and f2, f3 and f4 changes for
D1, D2, D3, making them the spurious features. Importantly, if we consider one domain only, the
spurious features f2, f3 and f4 are a more accurate indicator of the label than the invariant feature f1.
For instance, using f2 to predict y can give 97% accuracy on D1, while using f1 only achieves 93%
accuracy."
GOALS,0.09453781512605042,Table 1: Performance comparison on the linear dataset.
GOALS,0.09663865546218488,"Method train acc.
test acc.
W
b"
GOALS,0.09873949579831932,"ERM
97%
57%
[2.8, 3.3, 3.3, 0.0] −2.7
IDGM
93%
93%
[0.4, 0.2, 0.2, 0.0] −0.4
Fish
93%
93%
[0.4, 0.2, 0.2, 0.0] −0.4"
GOALS,0.10084033613445378,"The performance of ERM on this simple example is shown in Table 1 (ﬁrst row). From the trained
parameters W and b, we see that the model places most of its weights on spurious features f2 and f3.
While this achieves the highest train accuracy (97%), the model cannot generalize to unseen domains
and performs poorly on test accuracy (57%)."
GOALS,0.10294117647058823,"3.3
INTER-DOMAIN GRADIENT MATCHING (IDGM)"
GOALS,0.10504201680672269,"To mitigate the problem with ERM, we need an objective that learns from features that are invariant
across domains. Let us consider the case where the train dataset consists of S = 2 domains
Dtr = {D1, D2}. Given model θ and loss function l, the expected gradients for data in the two
domains is expressed as"
GOALS,0.10714285714285714,"G1 = ED1
∂l((x, y); θ)"
GOALS,0.1092436974789916,"∂θ
,
G2 = ED2
∂l((x, y); θ)"
GOALS,0.11134453781512606,"∂θ
.
(3)"
GOALS,0.1134453781512605,"The direction, and by extension, inner product of these gradients are of particular importance to our
goal of learning invariant features. If G1 and G2 point in a similar direction, i.e. G1 · G2 > 0, taking
a gradient step along G1 or G2 improves the model’s performance on both domains, indicating that"
GOALS,0.11554621848739496,Published as a conference paper at ICLR 2022
GOALS,0.11764705882352941,"the features learned by either gradient step are invariant across {D1, D2}. This invariance cannot be
guaranteed if G1 and G2 are pointing in opposite directions, i.e. G1 · G2 ≤0."
GOALS,0.11974789915966387,"To exploit this observation, we propose to maximize the gradient inner product (GIP) to align the
gradient direction across domains. The intended effect is to ﬁnd weights such that the input-output
correspondence is as close as possible across domains. We name our objective inter-domain gradient
matching (IDGM), and it is formed by subtracting the inner product of gradients between domains bG
from the original ERM objective. For the general case where S ≥2, we can write"
GOALS,0.12184873949579832,"Lidgm = Lerm(Dtr; θ) −γ
2
S(S −1)"
GOALS,0.12394957983193278,"i̸=j
X"
GOALS,0.12605042016806722,"i,j∈S
Gi · Gj"
GOALS,0.12815126050420167,"|
{z
}"
GOALS,0.13025210084033614,"GIP, denote as b
G ,
(4)"
GOALS,0.1323529411764706,where γ is the scaling term for bG. Note that GIP can be computed in linear time as bG = || P
GOALS,0.13445378151260504,"i Gi||2 −
P"
GOALS,0.13655462184873948,"i ||Gi||2 (ignoring the constant factor). We can also compute the stochastic estimates of Equation (4)
by replacing out the expectations over the entire dataset by minibatches."
GOALS,0.13865546218487396,"We test this objective on our simple linear dataset, and report results in the second row of Table 1.
Note that to avoid exploding gradient we use the normalized GIP during training. The model has
lower training accuracy compared to ERM (93%), however its accuracy remains the same on the test
set, much higher than ERM. The trained weights W reveal that the model assigns the largest weight
to the invariant feature f1, which is desirable. The visualization in Figure 1 also conﬁrms that by
maximizing the gradient inner product, IDGM is able to focus on the feature that is common between
domains, yielding better generalization performance than ERM."
OPTIMIZING IDGM WITH FISH,0.1407563025210084,"3.4
OPTIMIZING IDGM WITH FISH"
OPTIMIZING IDGM WITH FISH,0.14285714285714285,"The proposed IDGM objective, although effective, requires computing the second-order derivative
of the model’s parameters due to the gradient inner product term, which can be computationally
prohibitive. To mitigate this, we propose a ﬁrst-order algorithm named Fish1 that approximates the
optimization of IDGM with inner-loop updates. In Algorithm 1 we present Fish. As a comparison,
we also present direct optimization of IDGM using SGD in Algorithm 2."
OPTIMIZING IDGM WITH FISH,0.14495798319327732,Algorithm 1 Fish.
OPTIMIZING IDGM WITH FISH,0.14705882352941177,"1: for iterations = 1, 2, · · · do
2:
eθ ←θ
3:
for Di ∈permute({D1, D2, · · · , DS}) do
4:
Sample batch di ∼Di"
OPTIMIZING IDGM WITH FISH,0.14915966386554622,"5:
egi = Edi"
OPTIMIZING IDGM WITH FISH,0.15126050420168066,"""
∂l((x, y); eθ) ∂eθ #"
OPTIMIZING IDGM WITH FISH,0.15336134453781514,//Grad wrt eθ
OPTIMIZING IDGM WITH FISH,0.15546218487394958,"6:
Update eθ ←eθ −αegi"
OPTIMIZING IDGM WITH FISH,0.15756302521008403,"7:
end for"
OPTIMIZING IDGM WITH FISH,0.15966386554621848,"8:
Update θ ←θ + ϵ(eθ −θ)"
OPTIMIZING IDGM WITH FISH,0.16176470588235295,9: end for
OPTIMIZING IDGM WITH FISH,0.1638655462184874,Algorithm 2 Direct optimization of IDGM.
OPTIMIZING IDGM WITH FISH,0.16596638655462184,"1: for iterations = 1, 2, · · · do
2:
eθ ←θ
3:
for Di ∈permute({D1, D2, · · · , DS}) do
4:
Sample batch di ∼Di"
OPTIMIZING IDGM WITH FISH,0.16806722689075632,"5:
gi = Edi"
OPTIMIZING IDGM WITH FISH,0.17016806722689076,"""
∂l((x, y); θ) ∂θ #"
OPTIMIZING IDGM WITH FISH,0.1722689075630252,//Grad wrt θ
OPTIMIZING IDGM WITH FISH,0.17436974789915966,"6:
end for"
OPTIMIZING IDGM WITH FISH,0.17647058823529413,"7:
¯g = 1 S S
X"
OPTIMIZING IDGM WITH FISH,0.17857142857142858,"s=1
gs,
bg ="
OPTIMIZING IDGM WITH FISH,0.18067226890756302,"GIP (batch)
z
}|
{"
OPTIMIZING IDGM WITH FISH,0.18277310924369747,"2
S(S −1)"
OPTIMIZING IDGM WITH FISH,0.18487394957983194,"i̸=j
X"
OPTIMIZING IDGM WITH FISH,0.1869747899159664,"i,j∈S
gi · gj"
OPTIMIZING IDGM WITH FISH,0.18907563025210083,"8:
Update θ ←θ −ϵ (¯g −γ(∂bg/∂θ))
9: end for"
OPTIMIZING IDGM WITH FISH,0.19117647058823528,"Fish performs S inner-loop (l3-l7) update steps with learning rate α on a clone of the original model
eθ, and each update uses a minibatch di from the domain selected in step i. Subsequently, θ is updated
by a weighted difference between the cloned model and the original model ϵ(eθ −θ)."
OPTIMIZING IDGM WITH FISH,0.19327731092436976,"To see why Fish is an approximation to directly optimizing IDGM, we can perform Taylor-series
expansion on its update in l8, Algorithm 1. Doing so reveals two leading terms: 1) ¯g: averaged
gradients over inner-loop’s minibatches (effectively the ERM gradient); 2) ∂bg/∂θ: gradient of the
minibatch version of GIP. Observing l8 of Algorithm 2, we see that ¯g and bg are actually the two
gradient components used in direct optimization of IDGM. Therefore, Fish implicitly optimizes"
OPTIMIZING IDGM WITH FISH,0.1953781512605042,"1Following the convention of naming this style of algorithms after classes of vertebrates (animals with
backbones)."
OPTIMIZING IDGM WITH FISH,0.19747899159663865,Published as a conference paper at ICLR 2022
OPTIMIZING IDGM WITH FISH,0.19957983193277312,"IDGM by construction (up to a constant factor), avoiding the computation of second-order derivative
∂bg/∂θ. We present this more formally for the full gradient G in Theorem 3.1."
OPTIMIZING IDGM WITH FISH,0.20168067226890757,"Theorem 3.1 Given twice-differentiable model with parameters θ and objective l. Let us deﬁne the
following:"
OPTIMIZING IDGM WITH FISH,0.20378151260504201,"Gf = E[(θ −eθ)] −αS · ¯G,
Fish update - αS·ERM grad"
OPTIMIZING IDGM WITH FISH,0.20588235294117646,"Gg = −∂bG/∂θ,
grad of max
θ ( bG)"
OPTIMIZING IDGM WITH FISH,0.20798319327731093,where ¯G = 1
OPTIMIZING IDGM WITH FISH,0.21008403361344538,"S
PS
s=1 Gs and is the full gradient of ERM. Then we have"
OPTIMIZING IDGM WITH FISH,0.21218487394957983,"lim
α→0
Gf · Gg
∥Gf∥· ∥Gg∥= 1."
OPTIMIZING IDGM WITH FISH,0.21428571428571427,"Note that the expectation in Gf is over the sampling of domains and minibatches. Theorem 3.1
indicates that when α is sufﬁciently small, if we remove the scaled ERM gradient component ¯G from
Fish’s update, we are left with a term Gf that is in similar direction to the gradient of maximizing the
GIP term in IDGM, which was originally second-order. Note that this approximation comes at the
cost of losing direct control over the GIP scaling γ — we therefore also derived a smoothed version
of Fish that recovers this scaling term, however we ﬁnd that changing the value of γ does not make
much difference empirically. See Appendix B for more details."
OPTIMIZING IDGM WITH FISH,0.21638655462184875,"The proof to Theorem 3.1 can be found in Appendix A. We follow the analysis from Nichol et al.
(2018), which proposes Reptile for model-agnostic meta-learning (MAML), where the relationship
between inner-loop update and maximization of gradient inner product was ﬁrst highlighted. Nichol
et al. (2018) found the GIP term in their algorithm to be over minibatches from the same domain,
which promoted within-task generalization; in Fish we construct inner-loop using minibatches over
different domains – it therefore instead encourages across-domain generalization. We compare the
two algorithms in further details in Appendix A.1."
OPTIMIZING IDGM WITH FISH,0.2184873949579832,"We also train Fish on our simple linear dataset, with results in Table 1, and see it performs similarly
to IDGM – the model assigns the most weight to the invariant feature f1, and achieves 93% accuracy
on both train and test dataset."
EXPERIMENTS,0.22058823529411764,"4
EXPERIMENTS"
CDSPRITES-N,0.22268907563025211,"4.1
CDSPRITES-N"
CDSPRITES-N,0.22478991596638656,"Dataset We propose a simple shape-color dataset CDSPRITES-N based on the DSPRITES dataset
(Matthey et al., 2017), which contains a collection of white 2D sprites of different shapes, scales,
rotations and positions. CDSPRITES-N contains N domains. The goal is to classify the shape of the
sprites, and there is a shape-color deterministic matching that is speciﬁc per domain. This way we
have shape as the invariant feature and color as the spurious feature. See Figure 3 for an illustration."
CDSPRITES-N,0.226890756302521,"To construct the train split of CDSPRITES-N, we take a subset of DSPRITES that contains only 2
shapes (square and oval). We make N replicas of this subset and assign 2 colors to each, with every
color corresponding to one shape (e.g. yellow block in Figure 3a, pink →squares, purple →oval).
For the test split, we create another replica of the DSPRITES-N subset, and randomly assign one of
the 2N colors in the training set to each shape in the test set."
CDSPRITES-N,0.22899159663865545,"We design this dataset with CNN’s texture bias in mind (Geirhos et al., 2018; Brendel and Bethge,
2019). If the value of N is small enough, the model can simply memorize the N colors that correspond
to each shape, and make predictions solely based on colors, resulting in poor performance on the test
set where color and shape are no longer correlated. Our dataset allows for precise control over the
features that remains stable across domains and the features that change as domains change; we can
also change the number of domains N easily, making it possible to examine the effect N has on the
performance for domain generalization."
CDSPRITES-N,0.23109243697478993,"Results We train the same model using three different objectives including Fish, dicrect optimization
of IDGM and ERM on this dataset with number of domains N ranging from 5 to 50. Again, for direct
optimization of IDGM, we use the normalized gradient inner product to avoid exploding gradient."
CDSPRITES-N,0.23319327731092437,Published as a conference paper at ICLR 2022
CDSPRITES-N,0.23529411764705882,"(a) Train
(b) Test"
CDSPRITES-N,0.23739495798319327,"Figure 3: CDSPRITES-N visual-
ization. Each 3x3 grid (e.g. yel-
low square) is one domain."
CDSPRITES-N,0.23949579831932774,"(a) Train
(b) Test"
CDSPRITES-N,0.2415966386554622,"Figure 4: Performance of Fish, IDGM and ERM on CDSPRITES-
N, with N ∈[5, 50]"
CDSPRITES-N,0.24369747899159663,"We plot the average train, test accuracy for each objective over 5 runs against the number of domains
N in Figure 4. We can see that the train accuracy is always 100% for all methods regardless of N
(Figure 4a), while the test performance varies: Figure 4b shows that direct optimization of IDGM
(red) and Fish (blue) obtain the best performances, with the test accruacy rising to over 90% when
N ≥10 and near 100% when N ≥20. The predictions of ERM (yellow), on the other hand, remain
nearly random on the test set up until N = 20, and reach 95% accuracy only for N ≥40."
CDSPRITES-N,0.24579831932773108,"This experiment conﬁrms the following: 1) the proposed IDGM objective have much stronger domain
generalization capabilities compared to ERM; 2) Fish is an effective approximation of IDGM, with
similar performance to its direct optimization. We also plot the gradient inner product progression of
Fish vs. ERM during training in Figure 9a, showing clearly that Fish does improve the gradient inner
product across domain while ERM does not; 3) we also observe during training that Fish is about 10
times faster than directly optimizing IDGM, demonstrating its computational efﬁciency."
WILDS,0.24789915966386555,"4.2
WILDS"
WILDS,0.25,"Datasets We evaluate our model on the WILDS benchmark (Koh et al., 2020), which contains
multiple datasets that capture real-world distribution shifts across a diverse range of modalities.
We report experimental results on 6 challenging datasets in WILDS, and ﬁnd Fish to outperform
all baselines on most tasks. A summary of the WILDS datasets can be found in Appendix C. For
hyperparameters including learning rate, batch size, choice of optimizer and model architecture, we
follow the exact conﬁguration as reported in the WILDS benchmark. Importantly, we also use the
same model selection strategy used in WILDS to ensure a fair comparison. See details in Appendix D."
WILDS,0.25210084033613445,Table 2: Results on WILDS benchmark.
WILDS,0.2542016806722689,"POVERTYMAP
CAMELYON17
FMOW
CIVILCOMMENTS
IWILDCAM
AMAZON"
WILDS,0.25630252100840334,"Worst-U/R Pearson r
Avg. acc. (%)
Worst acc. (%)
Worst acc. (%)
Macro F1
10-th per. acc. (%)"
WILDS,0.25840336134453784,"Fish
0.30 (±1e-2)
74.7 (±7.1)
34.6 (±0.18)
75.3 (±0.6)
22.0 (±0.0)
53.3 (±0.0)
IRM
0.43 (±7e-2)
64.2 (±8.1)
30.0 (±1.37)
66.3 (±2.1)
15.1 (±4.9)
52.4 (±0.8)
Coral
0.44 (±6e-2)
59.5 (±7.7)
31.7 (±1.24)
65.6 (±1.3)
32.8(±0.1)
52.9 (±0.8)
Reweighted
-
-
-
69.2 (±0.9)
-
52.0 (±0.0)
GroupDRO
0.39 (±6e-2)
68.4 (±7.3)
30.8 (±0.81)
70.0 (±2.0)
23.9 (±2.1)
53.3 (±0.0)
ERM
0.45 (±6e-2)
70.3 (±6.4)
32.3 (±1.25)
56.0 (±3.6)
31.0 (±1.3)
53.8 (±0.8)
ERM (ours)
0.29 (±1e-2)
70.5 (±12.1)
30.9 (±1.53)
58.1 (±1.7)
25.1 (±0.2)
53.3 (±0.8)"
WILDS,0.2605042016806723,"Results See a summary of results in Table 2, where we use the metrics recommended in WILDS for
each dataset. Again, following practices in WILDS, all results are reported over 3 random seed runs,
apart from CAMELYON17 which uses 10 random seeds and CIVILCOMMENTS which uses 5. We
included additional results as well as a in-depth discussion on each dataset in Appendix C, and an
ablation studies on Fish’s hyperparameters in Appendix F and Appendix E. We make the following
observations:"
WILDS,0.26260504201680673,"1. Strong performance across datasets: Considering results on all 6 datasets, Fish is the best
performing algorithm on WILDS. It signiﬁcantly outperforms baselines on 3 datasets and achieves
similar level of performance to the best method on the other 3 (AMAZON and IWILDCAM). Fish’s
strong performance on different types of data and architectures such as RESNET (He et al., 2016),
DENSENET (Huang et al., 2017) and DISTILBERT (Sanh et al., 2019) demonstrated it’s capability
to generalize to a diverse variety of tasks;"
WILDS,0.2647058823529412,Published as a conference paper at ICLR 2022
WILDS,0.2668067226890756,Table 3: Test accuracy (%) on DOMAINBED benchmark.
WILDS,0.2689075630252101,"ERM
IRM
GroupDRO
Mixup
MLDG
Coral
MMD
DANN
CDANN
Fish (ours)"
WILDS,0.2710084033613445,"CMNIST
51.5 (±0.1)
52.0 (±0.1)
52.1 (±0.0)
52.1 (±0.2)
51.5 (±0.1)
51.5 (±0.1)
51.5 (±0.2)
51.5 (±0.3)
51.7 (±0.1)
51.6 (±0.1)
RMNIST
98.0 (±0.0)
97.7 (±0.1)
98.0 (±0.0)
98.0 (±0.1)
97.9 (±0.0)
98.0 (±0.1)
97.9 (±0.0)
97.8 (±0.1)
97.9 (±0.1)
98.0 (±0.0)
VLCS
77.5 (±0.4)
78.5 (±0.5)
76.7 (±0.6)
77.4 (±0.6)
77.2 (±0.4)
78.8 (±0.6)
77.5 (±0.9)
78.6 (±0.4)
77.5 (±0.1)
77.8 (±0.3)
PACS
85.5 (±0.2)
83.5 (±0.8)
84.4 (±0.8)
84.6 (±0.6)
84.9 (±1.0)
86.2 (±0.3)
84.6 (±0.5)
83.6 (±0.4)
82.6 (±0.9)
85.5 (±0.3)
OfﬁceHome
66.5 (±0.3)
64.3 (±2.2)
66.0 (±0.7)
68.1 (±0.3)
66.8 (±0.6)
68.7 (±0.3)
66.3 (±0.1)
65.9 (±0.6)
65.8 (±1.3)
68.6 (±0.4)
TerraInc
46.1 (±1.8)
47.6 (±0.8)
43.2 (±1.1)
47.9 (±0.8)
47.7 (±0.9)
47.6 (±1.0)
42.2 (±1.6)
46.7 (±0.5)
45.8 (±1.6)
45.1 (±1.3)
DomainNet
40.9 (±0.1)
33.9 (±2.8)
33.3 (±0.2)
39.2 (±0.1)
41.2 (±0.1)
41.5 (±0.1)
23.4 (±9.5)
38.3 (±0.1)
38.3 (±0.3)
42.7 (±0.2)"
WILDS,0.27310924369747897,"Average
66.6
65.4
64.8
66.7
66.7
67.5
63.3
66.1
65.6
67.1"
WILDS,0.27521008403361347,"2. Strong performance on different domain generalization tasks: We make special note the
CIVILCOMMENTS dataset captures subpopulation shift problems, where the domains in test are
a subpopulation of the domains in train, while all other WILDS datasets depicts pure domain
generalization problems, where the domains in train and test are disjointed. As a result, the
baseline models for CIVILCOMMENTS selected by the WILDS benchmark are different from the
methods used in all other datasets, and are tailored to avoiding systematic failure on data from
minority subpopulations. We see that Fish works well in this setting too without any changes
or special sampling strategies (used for baselines on CIVILCOMMENTS, see more in Table 10),
demonstrating it’s capability to perform in different domain generalization scenarios;"
WILDS,0.2773109243697479,"3. Failure mode of domain generalization algorithms: We noticed that on IWILDCAM and AMA-
ZON, ERM is the best algorithm, outperforming all domain generalization algorithms except for
Fish on AMAZON. We believe that these domain generalization algorithms failed due to the large
number of domains in these two datasets — 324 for IWILDCAM and 7,676 for AMAZON. This is
a common drawback of current domain generalization literature and is a direction worth exploring."
DOMAINBED,0.27941176470588236,"4.3
DOMAINBED"
DOMAINBED,0.2815126050420168,"Datasets While WILDS is a challenging benchmark capturing realistic distribution shift, to test our
model under the synthetic-to-real transfer setting and provide more comparisons to SOTA methods,
we also performed experiments on the DOMAINBED benchmark (Gulrajani and Lopez-Paz, 2020).
See a summary of DOMAINBED in Appendix H."
DOMAINBED,0.28361344537815125,"Results Following recommendations in DOMAINBED, we report results using training domain as
validation set for model selection. See results in Table 3, reported over 5 random trials. Averaging
the performance over all 7 datasets, Fish ranks second out of 10 domain generalization methods. It
performs only marginally worse than Coral (0.1%), and is one of the three methods that performs
better than ERM. This showcases Fish’s effectiveness on domain generalization datasets with stronger
focus to synthetic-to-real transfer, which again demonstrates its versatility and robustness on different
domain generalization tasks."
ANALYSIS,0.2857142857142857,"4.4
ANALYSIS"
ANALYSIS,0.28781512605042014,"We show extensively through empirical evaluation that Fish is very effective for a variety domain
generalization tasks. In this section, we perform analysis to validate that Fish’s strong performance is
due to inter-domain gradient inner product maximization."
ANALYSIS,0.28991596638655465,"Figure 5: Gradient inner prod-
uct values during the training
for CDSPRITES-N (N=15)."
ANALYSIS,0.2920168067226891,"Does Fish maximize gradient inner product (GIP) empirically?
In Figure 5, we plot the progression of GIP during training using
different objectives. We train both Fish (blue) and ERM (yellow) on
CDSPRITES-N until convergence while tracking the normalized GIP
between minibatches from different domains used in each inner-loop.
To ensure a fair comparison, we use the exact same sequence of data
for Fish and ERM (see Appendix I for more details)."
ANALYSIS,0.29411764705882354,"From Figure 5, it is clear that during training, the normalized GIP
of Fish increases, while that for ERM stays at the same value. The
observations here shows that Fish is indeed effective in increas-
ing/maintaining the level of inter-domain GIP."
ANALYSIS,0.296218487394958,Published as a conference paper at ICLR 2022
ANALYSIS,0.29831932773109243,"Table 4: Test accuracy on four datasets, in three partitions are 1) two baselines Fish and ERM, 2) Fish
with random grouping strategy (Fish, RG) and 3) ERM with domain grouping strategy (ERM, DG, ∗)"
ANALYSIS,0.3004201680672269,"FMOW
VLCS
PACS
OfﬁceHome"
ANALYSIS,0.3025210084033613,"Fish (ours)
34.3 (±0.6)
77.6 (±0.5)
85.5 (±0.3)
68.6 (±0.9)
ERM
31.7 (±1.0)
77.5 (±0.4)
85.5 (±0.2)
66.5 (±0.3)"
ANALYSIS,0.30462184873949577,"Fish, RG
33.4 (±1.7)
77.7 (±0.3)
83.9 (±0.7)
66.5 (±1.0)"
ANALYSIS,0.3067226890756303,"ERM, DG, prop. lr
32.1 (±0.5)
72.7 (±0.4)
83.2 (±0.7)
58.5 (±0.1)
ERM, DG, 0.1× prop. lr
29.9 (±0.7)
73.9 (±0.6)
84.2 (±0.4)
65.1 (±0.1)
ERM, DG, 0.01× prop. lr
29.8 (±0.4)
74.7 (±0.4)
84.0 (±0.6)
63.7 (±0.2)"
ANALYSIS,0.3088235294117647,"We conduct the same GIP tracking experiments for the WILDS datasets we studied as well to shed
some lights on its efﬁciency — see Appendix G for results."
ANALYSIS,0.31092436974789917,"Does maximizing GIP between random batches help domain generalization? In this part of the
analysis, we examine the effectiveness of another element of our algorithm – the construction of
minibatches. We conduct experiments where data are grouped randomly instead of by domain for
the inner-loop update. By doing so, we are still maximizing the inner product between minibatches,
however not strictly between domain. We therefore expect the results to be slightly worse than Fish,
and the bigger the domain gap is, the more advantage Fish has against the random grouping strategy."
ANALYSIS,0.3130252100840336,"We show the results for random grouping (Fish, RG) in Table 4. As expected, the random grouping
strategy performs worse than Fish on all datasets. The experiment demonstrated that our algorithm
does beneﬁt from the domain grouping strategy, and that maximising GIP between random batches of
data, while still achieving strong results, does not achieve the same domain generalization performance
as Fish."
ANALYSIS,0.31512605042016806,"Differences between Fish and ERM We have shown through empirical evaluation that Fish and
ERM are sufﬁciently different. Most notably, they converge to completely distinct minimum in our
linear toy example and CDSPRITES experiment, resulting in large decrepencies in test accuracy."
ANALYSIS,0.3172268907563025,"However, some may notice that Fish as a ﬁrst-order method shares algorithmic similarities to ERM.
In fact, if we ﬁx the meta learning rate of Fish ϵ = 1 and use domain sampling for ERM, the two
algorithms are equivalent. Although the optimal ϵs found through hyperparameter search are much
smaller than 1, one might wonder if we can offset the increased meta learning rate ϵ by reducing
the learning rate α proportionally, and achieve similar performance to Fish using simply ERM with
domain grouping."
ANALYSIS,0.31932773109243695,"In Table 4 (ERM, DG), we verify that this is infeasible empirically. We train an ERM model with
domain grouping strategy (equivalent to Fish with ϵ = 1) using 3 different learning rates: 1) prop. lr,
learning rate α is lowered proportionally to keep αϵ constant; 2) 0.1×prop. lr and 3) 0.001×prop. lr
which further reduces the learning rate. We see that on all datasets, all three conﬁgurations result in
worse performance than ERM and Fish. This shows that one cannot improve the results of ERM to
match that of Fish by simply adjusting the learning rate/adopting domain grouping strategy, and that
Fish’s gain on domain generalization tasks is not due to its proximity to ERM."
ANALYSIS,0.32142857142857145,"In Appendix A.2, we also show that setting ϵ = 1 is equivalent to setting S as the total number of
iterations during training, which causes the effect of maximizing GIP to diminish. This provide
further support to our empirical ﬁnding in this analysis."
ANALYSIS,0.3235294117647059,"Does maximising GIP help domain generalization? In Parascandolo et al. (2021), authors show
that by masking out the gradients that have opposite signs in different domains, they are able to
trade-off some “learning speed” for prioritizing learning the invariances. Our principle for proposing
the IDGM objective is identical, except in this case we actively encourage these gradients to be of
consistent signs. This unfortunately, does result in computing the costly second-order derivative,
which makes it difﬁcult for us to empirically evaluate the IDGM objective on non-toy dataset2. We
believe that the large performance gain of IDGM over ERM on the linear and CDSPRITES datasets,
along with the detailed discussion on the relationship between gradient sign consistency and invariant
representation, is enough to justify for IDGM’s ability to improve domain generalization."
ANALYSIS,0.32563025210084034,"2For context, training an IDGM model on VLCS dataset with batch size 32 and standard RESNET18
backbone does not ﬁt a NVIDIA V100 (32GB) GPU."
ANALYSIS,0.3277310924369748,Published as a conference paper at ICLR 2022
CONCLUSION,0.32983193277310924,"5
CONCLUSION"
CONCLUSION,0.3319327731092437,"In this paper we presented inter-domain gradient matching (IDGM) for domain generalization. To
avoid costly second-order computations, we approximated IDGM with a simple ﬁrst-order algorithm,
Fish. We demonstrated our algorithm’s capability to learn from invariant features (as well as ERM’s
failure to do so) using simple datasets such as CDSPRITES-N and the linear example. We then
evaluated the model’s performance on WILDS and DOMAINBED, demonstrating that Fish performs
well on different subgenres of domain generalization, and surpasses baseline performance on a diverse
range of vision and language tasks using different architectures such as DenseNet, ResNet-50 and
BERT. Our experiments can be replicated with 1500 GPU hours on NVIDIA V100."
CONCLUSION,0.33403361344537813,"Despite its strong performance, similar to previous work on domain generalization, when the number
of domains is large Fish struggles to outperform ERM. We are currently investigating approaches by
which Fish can be made to scale to datasets with orders of magnitude more domains and expect to
report on this improvement in our future work."
CONCLUSION,0.33613445378151263,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3382352941176471,ETHICS STATEMENT
ETHICS STATEMENT,0.3403361344537815,"We believe there are no ethical concerns within this work. None of the datasets used involve human
identities, and our motivation in proposing this work in no way concerns any surveillance/military use.
As with many other domain generalization methods, our algorithm aims at improving the performance
of machine learning systems deployed in the wilds, which can be used in many ways that will beneﬁt
the society."
REFERENCES,0.34243697478991597,REFERENCES
REFERENCES,0.3445378151260504,"M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford,
and N. De Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint
arXiv:1606.04474, 2016."
REFERENCES,0.34663865546218486,"M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint
arXiv:1907.02893, 2019."
REFERENCES,0.3487394957983193,"Y. Balaji, S. Sankaranarayanan, and R. Chellappa. Metareg: towards domain generalization using
meta-regularization. In NIPS’18 Proceedings of the 32nd International Conference on Neural
Information Processing Systems, volume 31, pages 1006–1016, 2018."
REFERENCES,0.35084033613445376,"S. Beery, G. V. Horn, and P. Perona. Recognition in terra incognita. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 472–489, 2018."
REFERENCES,0.35294117647058826,"S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning
from different domains. Machine learning, 79(1):151–175, 2010."
REFERENCES,0.3550420168067227,"S. L. Blodgett, L. Green, and B. T. O’Connor. Demographic dialectal variation in social media:
A case study of african-american english. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages 1119–1130, 2016."
REFERENCES,0.35714285714285715,"W. Brendel and M. Bethge. Approximating cnns with bag-of-local-features models works surprisingly
well on imagenet. In International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.3592436974789916,"F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tommasi. Domain generalization
by solving jigsaw puzzles. In 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2229–2238, 2019."
REFERENCES,0.36134453781512604,"Q. Dou, D. C. de Castro, K. Kamnitsas, and B. Glocker. Domain generalization via model-agnostic
learning of semantic features. In Advances in Neural Information Processing Systems, volume 32,
pages 6450–6461, 2019."
REFERENCES,0.3634453781512605,"C. Fang, Y. Xu, and D. N. Rockmore. Unbiased metric learning: On the utilization of multiple
datasets and web images for softening bias. In 2013 IEEE International Conference on Computer
Vision, pages 1657–1664, 2013."
REFERENCES,0.36554621848739494,"C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In ICML’17 Proceedings of the 34th International Conference on Machine Learning - Volume 70,
pages 1126–1135, 2017."
REFERENCES,0.36764705882352944,"Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning
research, 17(1):2096–2030, 2016."
REFERENCES,0.3697478991596639,"R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel. Imagenet-trained
cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In
International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.37184873949579833,"M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition
with multi-task autoencoders. In 2015 IEEE International Conference on Computer Vision (ICCV),
pages 2551–2559, 2015."
REFERENCES,0.3739495798319328,Published as a conference paper at ICLR 2022
REFERENCES,0.3760504201680672,"A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Schölkopf. Covariate shift
and local learning by distribution matching, pages 131–160. MIT Press, Cambridge, MA, USA,
2009a."
REFERENCES,0.37815126050420167,"A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Schölkopf. Covariate shift by
kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009b."
REFERENCES,0.3802521008403361,"I. Gulrajani and D. Lopez-Paz.
In search of lost domain generalization.
arXiv preprint
arXiv:2007.01434, 2020."
REFERENCES,0.38235294117647056,"T. B. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in
repeated loss minimization. In International Conference on Machine Learning, pages 1929–1938,
2018."
REFERENCES,0.38445378151260506,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016."
REFERENCES,0.3865546218487395,"W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give
robust classiﬁers. In International Conference on Machine Learning, pages 2029–2037, 2018."
REFERENCES,0.38865546218487396,"W. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give
robust classiﬁers? In International Conference on Machine Learning, pages 2029–2037. PMLR,
2018."
REFERENCES,0.3907563025210084,"G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
2261–2269, 2017."
REFERENCES,0.39285714285714285,"Z. Huang, H. Wang, E. P. Xing, and D. Huang. Self-challenging improves cross-domain generalization.
In European Conference on Computer Vision, pages 124–140, 2020."
REFERENCES,0.3949579831932773,"M. Ilse, J. Tomczak, and P. Forré. Selecting data augmentation for simulating interventions. In ICML
2021: 38th International Conference on Machine Learning, pages 4555–4562, 2021."
REFERENCES,0.39705882352941174,"P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga,
R. L. Phillips, S. Beery, J. Leskovec, A. Kundaje, E. Pierson, S. Levine, C. Finn, and P. Liang.
Wilds: A benchmark of in-the-wild distribution shifts. arXiv preprint arXiv:2012.07421, 2020."
REFERENCES,0.39915966386554624,"M. Koyama and S. Yamaguchi. Out-of-distribution generalization with maximal invariant predictor.
In arXiv e-prints, 2021."
REFERENCES,0.4012605042016807,"D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization.
In 2017 IEEE International Conference on Computer Vision (ICCV), pages 5543–5551, 2017."
REFERENCES,0.40336134453781514,"D. Li, Y. Yang, Y.-Z. Song, and T. Hospedales. Learning to generalize: Meta-learning for domain
generalization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32,
2018a."
REFERENCES,0.4054621848739496,"D. Li, J. Zhang, Y. Yang, C. Liu, Y.-Z. Song, and T. Hospedales. Episodic training for domain
generalization. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages
1446–1455, 2019."
REFERENCES,0.40756302521008403,"D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Sequential learning for domain generalization. In
European Conference on Computer Vision, pages 603–619, 2020."
REFERENCES,0.4096638655462185,"H. Li, S. J. Pan, S. Wang, and A. C. Kot. Domain generalization with adversarial feature learning.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
5400–5409, 2018b."
REFERENCES,0.4117647058823529,"Y. Li, X. Tian, M. Gong, Y. Liu, T. Liu, K. Zhang, and D. Tao. Deep domain generalization
via conditional invariant adversarial networks. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 647–663, 2018."
REFERENCES,0.41386554621848737,"D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. In Advances in
Neural Information Processing Systems, volume 30, pages 6467–6476, 2017."
REFERENCES,0.41596638655462187,Published as a conference paper at ICLR 2022
REFERENCES,0.4180672268907563,"L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites
dataset. https://github.com/deepmind/dsprites-dataset/, 2017."
REFERENCES,0.42016806722689076,"S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Uniﬁed deep supervised domain adaptation
and generalization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages
5716–5726, 2017."
REFERENCES,0.4222689075630252,"A. Nichol, J. Achiam, and J. Schulman. On ﬁrst-order meta-learning algorithms. arXiv: Learning,
2018."
REFERENCES,0.42436974789915966,"G. Parascandolo, A. Neitz, A. Orvieto, L. Gresele, and B. Schölkopf. Learning explanations that are
hard to vary. In ICLR 2021: The Ninth International Conference on Learning Representations,
2021."
REFERENCES,0.4264705882352941,"X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source
domain adaptation. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV),
pages 1406–1415, 2019."
REFERENCES,0.42857142857142855,"M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning.
In International Conference on Machine Learning, pages 4334–4343, 2018."
REFERENCES,0.43067226890756305,"M. Rojas-Carulla, B. Schölkopf, R. Turner, and J. Peters. A causal perspective on domain adaptation.
stat, 1050:19, 2015."
REFERENCES,0.4327731092436975,"S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for
group shifts: On the importance of regularization for worst-case generalization. arXiv preprint
arXiv:1911.08731, 2019."
REFERENCES,0.43487394957983194,"S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for
group shifts: On the importance of regularization for worst-case generalization. arXiv preprint
arXiv:1911.08731, 2019."
REFERENCES,0.4369747899159664,"V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,
cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019."
REFERENCES,0.43907563025210083,"S. Seo, Y. Suh, D. Kim, G. Kim, J. Han, and B. Han. Learning to optimize domain speciﬁc
normalization for domain generalization. In European Conference on Computer Vision, pages
68–83, 2019."
REFERENCES,0.4411764705882353,"P. Stock and M. Cisse. Convnets and imagenet beyond accuracy: Explanations, bias detection,
adversarial examples and model criticism. arXiv preprint arXiv:1711.11443, 2017."
REFERENCES,0.4432773109243697,"B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European
conference on computer vision, pages 443–450. Springer, 2016."
REFERENCES,0.44537815126050423,"A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results. In ICLR (Workshop), 2017."
REFERENCES,0.4474789915966387,"R. Tatman. Gender and dialect bias in youtube’s automatic captions. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Processing, pages 53–59, 2017."
REFERENCES,0.4495798319327731,"S. Thrun and L. Pratt, editors. Learning to Learn. Kluwer Academic Publishers, USA, 1998. ISBN
0792380479."
REFERENCES,0.45168067226890757,"A. Torralba and A. A. Efros. Unbiased look at dataset bias. In Proceedings of the 2011 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR ’11, page 1521–1528, USA,
2011. IEEE Computer Society. ISBN 9781457703942. doi: 10.1109/CVPR.2011.5995347. URL
https://doi.org/10.1109/CVPR.2011.5995347."
REFERENCES,0.453781512605042,"L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9(86):2579–2605, 2008."
REFERENCES,0.45588235294117646,"H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. Deep hashing network for
unsupervised domain adaptation. In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5385–5394, 2017."
REFERENCES,0.4579831932773109,Published as a conference paper at ICLR 2022
REFERENCES,0.46008403361344535,"R. Volpi and V. Murino. Addressing model vulnerability to distributional shifts over image trans-
formation sets. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages
7980–7989, 2019."
REFERENCES,0.46218487394957986,"H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing
local predictive power. In Advances in Neural Information Processing Systems, volume 32, pages
10506–10518, 2019."
REFERENCES,0.4642857142857143,"S. Yan, H. Song, N. Li, L. Zou, and L. Ren. Improve unsupervised domain adaptation with mixup
training. arXiv preprint arXiv:2001.00677, 2020."
REFERENCES,0.46638655462184875,"K. Zhou, Y. Yang, T. M. Hospedales, and T. Xiang. Learning to generate novel domains for domain
generalization. In European Conference on Computer Vision, pages 561–578, 2020."
REFERENCES,0.4684873949579832,Published as a conference paper at ICLR 2022
REFERENCES,0.47058823529411764,"A
TAYLOR EXPANSION OF REPTILE AND FISH INNER-LOOP UPDATE"
REFERENCES,0.4726890756302521,"In this section we provide proof to Theorem 3.1. We reproduce and adapt the proof from Nichol et al.
(2018) in the context of Fish, for completeness."
REFERENCES,0.47478991596638653,"We demonstrate that when the inner-loop learning rate α is small, the direction of Gf aligns with that
of Gg, where"
REFERENCES,0.47689075630252103,"Gf = E
h
(θ −˜θ)
i
−αS · ¯G,
(5)"
REFERENCES,0.4789915966386555,"Gg = −∂bG/∂θ,
(6)"
REFERENCES,0.4810924369747899,"Expanding Gg
Gg is the gradient of maximizing the gradient inner product (GIP)."
REFERENCES,0.4831932773109244,"Gg = −
2
S(S −1)"
REFERENCES,0.4852941176470588,"i̸=j
X i,j∈S"
REFERENCES,0.48739495798319327,"∂
∂θGi · Gj
(7)"
REFERENCES,0.4894957983193277,"Expanding Gf
To write out Gf, we need to derive the gradient update of Fish, θ −˜θ. Let us ﬁrst
deﬁne some notations."
REFERENCES,0.49159663865546216,"For each inner-loop with S steps of gradient updates, we assume a loss functions l as well as a
sequence of inputs {di}S
i=1, where di := {xb, yb}B
b=1 denotes a minibatch at step i randomly drawn
from one of the available domains in {D1, · · · , DS}. For reasons that will become clearer later, take
extra note that the subscript i here denotes the index of step, rather than the index of domain. We also
deﬁne the following:"
REFERENCES,0.49369747899159666,egi = Edi
REFERENCES,0.4957983193277311,"∂l((x, y); θi) ∂θi"
REFERENCES,0.49789915966386555,"
(gradient at step i, wrt θi)
(8)"
REFERENCES,0.5,"θi+1 = θi −αegi
(sequence of parameters)
(9)"
REFERENCES,0.5021008403361344,gi = Edi
REFERENCES,0.5042016806722689,"∂l((x, y); θ1) ∂θ1"
REFERENCES,0.5063025210084033,"
(gradient at step i, wrt θ1)
(10)"
REFERENCES,0.5084033613445378,Hi = Edi
REFERENCES,0.5105042016806722,"∂2l((x, y); θ1) ∂θ2
1"
REFERENCES,0.5126050420168067,"
(Hessian at initial point)
(11)"
REFERENCES,0.5147058823529411,"In the following analysis we omit the expectation Edi and input (x, y) to l and instead denote the loss
at step i as li. Performing second-order Taylor approximation to egi yields:
egi = l′
i(θi)
(12)"
REFERENCES,0.5168067226890757,"= l′
i(θ1) + l′′
i (θ1)(θi −θ1) + O(∥θi −θ1∥2)
|
{z
}
=O(α2) (13)"
REFERENCES,0.5189075630252101,"= gi + Hi(θi −θ1) + O(α2)
(14)"
REFERENCES,0.5210084033613446,"= gi −αHi i−1
X"
REFERENCES,0.523109243697479,"j=1
egj + O(α2).
(15)"
REFERENCES,0.5252100840336135,"Applying ﬁrst-order Taylor approximation to egj gives us
egj = gj + O(α),
(16)
plugging this back to Equation (15) yields:"
REFERENCES,0.5273109243697479,"egi = gi −αHi i−1
X"
REFERENCES,0.5294117647058824,"j=1
gj + O(α2).
(17)"
REFERENCES,0.5315126050420168,"For simplicity reason, let us consider performing two steps in inner-loop updates, i.e. S = 2. We can
then write the gradient of Fish θ −˜θ as"
REFERENCES,0.5336134453781513,"θ −˜θ = α(eg1 + eg2)
(18)"
REFERENCES,0.5357142857142857,"= α(g1 + g2
| {z } 1"
REFERENCES,0.5378151260504201,") −α2 H2g1
| {z } 2"
REFERENCES,0.5399159663865546,"+O(α3).
(19)"
REFERENCES,0.542016806722689,Published as a conference paper at ICLR 2022
REFERENCES,0.5441176470588235,"Furthermore, taking the expectation of θ −˜θ under minibatch sampling gives us (assuming indepen-
dence between g1 and g2)"
REFERENCES,0.5462184873949579,"1 = E1,2 [g1 + g2] = G1 + G2"
REFERENCES,0.5483193277310925,"2 = E1,2 [H2g1] = E1,2 [H1g2]
(interchanging indices) = 1"
REFERENCES,0.5504201680672269,"2 E1,2 [H2g1 + H1g2]
(averaging last two eqs) = 1"
REFERENCES,0.5525210084033614,"2 E1,2"
REFERENCES,0.5546218487394958,∂(g1 · g2) ∂θ1  = 1
REFERENCES,0.5567226890756303,"2 · ∂(G1 · G2) ∂θ1
."
REFERENCES,0.5588235294117647,"Note that the only reason we can interchange the indices in 2 is because the subscripts represent
steps in the inner loop rather than index of domains. Plugging 1 , 2 in Equation (19) yields:"
REFERENCES,0.5609243697478992,E[θ −˜θ] = α(G1 + G2) −α2
REFERENCES,0.5630252100840336,2 · ∂(G1 · G2)
REFERENCES,0.5651260504201681,"∂θ1
+ O(α3)
(20)"
REFERENCES,0.5672268907563025,We can also expand this to the general case where S ≥2:
REFERENCES,0.569327731092437,"E[θ −˜θ] = α S
X"
REFERENCES,0.5714285714285714,"s=1
Gs −
α2"
REFERENCES,0.5735294117647058,S(S −1)
REFERENCES,0.5756302521008403,"i̸=j
X i,j∈S"
REFERENCES,0.5777310924369747,∂(Gi · Gj)
REFERENCES,0.5798319327731093,"∂θ1
+ O(α3).
(21)"
REFERENCES,0.5819327731092437,"The second term in Equation (5) is ¯G, which is the full gradient of ERM deﬁned as follow:"
REFERENCES,0.5840336134453782,"¯G = 1 S S
X"
REFERENCES,0.5861344537815126,"s=1
Gs.
(22)"
REFERENCES,0.5882352941176471,Plugging Equation (21) and Equation (22) to Equation (5) yields
REFERENCES,0.5903361344537815,"Gf = E[θ −˜θ] −αS ¯G
(23)"
REFERENCES,0.592436974789916,"= −
α2"
REFERENCES,0.5945378151260504,S(S −1)
REFERENCES,0.5966386554621849,"i̸=j
X i,j∈S"
REFERENCES,0.5987394957983193,"∂
∂θ1
Gi · Gj
(24)"
REFERENCES,0.6008403361344538,"Comparing Equation (7) to Equation (24), we have:"
REFERENCES,0.6029411764705882,"lim
α→0
Gf · Gg
∥Gf∥∥Gg∥= 1."
REFERENCES,0.6050420168067226,"A.1
FISH AND REPTILE: DIFFERENCES AND CONNECTIONS"
REFERENCES,0.6071428571428571,"As we introduced, our algorithm Fish is inspired by Reptile, a model agnostic meta-learning (MAML)
algorithm."
REFERENCES,0.6092436974789915,"Meta-learning aims at reducing the sample complexity of new, unseen tasks. A popular school of
thinking in meta-learning is MAML, ﬁrst proposed in Finn et al. (2017); Andrychowicz et al. (2016).
The key idea is to backpropagate through gradient descent itself to learn representations that can be
easily adapted to unseen tasks. There are close connections between meta-learning (Thrun and Pratt,
1998) and (multi-source) domain adaptation. In fact, there are a few works in domain generalization
that are inspired by the meta-learning principles, such as Li et al. (2018a); Balaji et al. (2018); Li et al.
(2019); Dou et al. (2019). In Ren et al. (2018), we also see the leveraging of gradient inner product in
meta-learning, where it is used to determine the importance weight of training examples."
REFERENCES,0.6113445378151261,"Even though meta learning and domain generalization both study N-way, K-shot problems, there
are some distinct differences that set them apart. The most prominent one is that in meta learning,
some examples in the test dataset will be made available at test time (K > 0), while in domain"
REFERENCES,0.6134453781512605,Published as a conference paper at ICLR 2022
REFERENCES,0.615546218487395,"Algorithm 3 Black fonts denote steps used in both algorithms, colored fonts are steps unique to
Fish or Reptile."
REFERENCES,0.6176470588235294,"1: for i = 1, 2, · · · do
2:
˜θ ←θ
3:
Sample task Dt ∼{D1, · · · , DT }
4:
for s ∈{1, · · · , S} or Dt ∈{D1, · · · , DT } do
5:
Sample batch dt ∼Dt
6:
gt = ∂L(dt; ˜θ)/∂˜θ
7:
Update ˜θ ←˜θ −αgt
8:
end for
9:
Update θ ←θ + ϵ(˜θ −θ)
10: end for"
REFERENCES,0.6197478991596639,"(a) Train
(b) Test"
REFERENCES,0.6218487394957983,"Figure 6: Performance on CDSPRITES-N, with N ∈[5, 50]"
REFERENCES,0.6239495798319328,"generalization no example in the test dataset is seen by the model (K = 0); another important
difference is that while domain generalization aims to train models that perform well on an unseen
distribution of the same task, meta-learning assumes multiple tasks and requires the model to quickly
learn an unseen task using only K examples."
REFERENCES,0.6260504201680672,"Due to these differences, it does not make sense in general to use MAML framework in domain
generalization. As it turns out however, the idea of aligning gradients to improve generalization is
relevant to both methods — The fundamental difference here that MAML algorithms such as Reptile
aligns the gradients between batches from the same task Nichol et al. (2018), while Fish aligns those
between batches from different tasks."
REFERENCES,0.6281512605042017,"To see how this is ahiceved, let us have a look at the algorithmic comparison between Fish (blue)
and Reptile (green) in Algorithm 3. As we can see, the key difference between the algorithm of Fish
and Reptile is that Reptile performs its inner-loop using minibatches from the same task, while Fish
uses minibatches from different tasks (l4-8). Based on the analysis in Nichol et al. (2018) (which we
reproduce in Appendix A), this is why Reptile maximizes the within-task gradient inner products and
Fish maximizes the across-task gradient inner products."
REFERENCES,0.6302521008403361,"A natural question to ask here is – how does this affect their empirical performance? In Figure 6,
we show the train and test performance of Fish (blue) and Reptile (green) on CDSPRITES-N. We
can see that despite the algorithmic similarity between Fish and Reptile, the two methods behave
very differently on this domain generalization task: while Fish’s test accuracy goes to 100% at
N = 10, Reptile’s test performance is always 50% regardless of N. Moreover, we observe a dip in
Reptile’s training performance early on, with the accuracy plateaus at 56% when N > 20. Reptile’s
poor performance on this dataset is to be expected since its inner-loop is designed to encourage
within-domain generalization, which is not helpful for learning what’s invariant across domains."
REFERENCES,0.6323529411764706,"A.2
WHEN ϵ = 1"
REFERENCES,0.634453781512605,"When we set the meta learning rate of Fish ϵ = 1, Fish reduces to ERM with a data-loader that
samples from only one domain in each minibatch. Note that with this change, the inner-loop step S
now represents the total training iterations, and as a result it increases from a constant value (≤10
in our experiments) to orders of magnitude larger (up to 105 for some datasets that we use). We"
REFERENCES,0.6365546218487395,Published as a conference paper at ICLR 2022
REFERENCES,0.6386554621848739,"demonstrate here that for this reason, the effect of maximizing inter-domain gradient inner product is
signiﬁcantly less prominent when ϵ = 1."
REFERENCES,0.6407563025210085,"To see this, let us revisit Equation (21), where we demonstrate that the expectation to Fish’s update
E[θ −˜θ] can be written as the sum of the following three terms via Taylor series expansion,"
REFERENCES,0.6428571428571429,"E[θ −˜θ] = α S
X"
REFERENCES,0.6449579831932774,"s=1
Gs"
REFERENCES,0.6470588235294118,"|
{z
}
(1) −
α2"
REFERENCES,0.6491596638655462,S(S −1)
REFERENCES,0.6512605042016807,"i̸=j
X i,j∈S"
REFERENCES,0.6533613445378151,∂(Gi · Gj)
REFERENCES,0.6554621848739496,"∂θ1
|
{z
}
(2)"
REFERENCES,0.657563025210084,+ O(α3)
REFERENCES,0.6596638655462185,"| {z }
(3)"
REFERENCES,0.6617647058823529,",
(25)"
REFERENCES,0.6638655462184874,"where (1) is the sum of gradients over S steps of SGD updates (equivalent to ERM), (2) the gradient
inner product over S steps of gradients and (3) is the higher order terms of Taylor series expansion."
REFERENCES,0.6659663865546218,"For both Fish and Reptile, the higher order terms are ignored under two conditions: 1) constant/non-
inﬁnite inner-loop step S and 2) small learning rate α. However, when ϵ = 1, inner-loop step S →∞.
As a result, the higher order terms cannot be ignored, and we can no longer conclude that gradient
inner product plays an important rule in the model’s updates."
REFERENCES,0.6680672268907563,"B
SMOOTHFISH: A MORE GENERAL ALGORITHM"
REFERENCES,0.6701680672268907,"B.1
DERIVATION"
REFERENCES,0.6722689075630253,"We conclude in Appendix A that a component of Fish’s update Gf = E[θ −˜θ]−αS · ¯G is in the same
direction as the gradient of GIP, Gg. It is therefore possible to have explicit control over the scaling
of the GIP component in Fish, similar to the original IDGM objective, by writing the following:"
REFERENCES,0.6743697478991597,"Gsm = αS · ¯G + γ

E[θ −˜θ] −αS · ¯G

.
(26)"
REFERENCES,0.6764705882352942,"By introducing the scaling term γ, we have better control on how much the objective focus on inner
product vs average gradient. Note that γ = 1 recovers the original Fish gradient, and when γ = 0 the
gradient Gsm is equivalent to ERM’s gradient with learning rate αS. We name the resulting algorithm
SmoothFish. See Algorithm 4."
REFERENCES,0.6785714285714286,"B.2
RESULTS"
REFERENCES,0.680672268907563,"We run experiments on 4 datasets in WILDS using SmoothFish,
with γ ranging in
[0, 0.1, 0.2, 0.5, 0.8, 1, 2, 10]. Note that when γ = 0, SmoothFish is equivalent to ERM and when
γ = 1 it is equivalent to Fish. See results in Figure 7. The other hyperparameters including α, meta
steps, ϵ used here are the same as the ones used in our main experiments section."
REFERENCES,0.6827731092436975,Published as a conference paper at ICLR 2022
REFERENCES,0.6848739495798319,"Algorithm 4 Smoothed version of Fish, which allows to get approximate gradients for the general
form of Equation (4)."
REFERENCES,0.6869747899159664,"1: for iterations = 1, 2, · · · do
2:
eθ ←θ
3:
for Di ∈permute({D1, D2, · · · , DS}) do
4:
Sample batch di ∼Di"
REFERENCES,0.6890756302521008,"5:
gi = Edi"
REFERENCES,0.6911764705882353,"∂l((x, y); θ) ∂θ"
REFERENCES,0.6932773109243697,"
//Grad wrt θ"
REFERENCES,0.6953781512605042,"6:
egi = Edi"
REFERENCES,0.6974789915966386,"""
∂l((x, y); eθ) ∂eθ #"
REFERENCES,0.6995798319327731,//Grad wrt eθ
REFERENCES,0.7016806722689075,"7:
Update eθ ←eθ −αegi
8:
end for"
REFERENCES,0.7037815126050421,"9:
¯g = 1 S S
X"
REFERENCES,0.7058823529411765,"s=1
gi, gsm = αS¯g + γ

(eθ −θ) −αS¯g
"
REFERENCES,0.707983193277311,"10:
Update θ ←θ + ϵgsm
11: end for"
REFERENCES,0.7100840336134454,"(a) CAMELYON17
(b) CIVILCOMMENTS"
REFERENCES,0.7121848739495799,"(c) FMOW
(d) POVERTY"
REFERENCES,0.7142857142857143,Figure 7: Results on WILDS using SmoothFish with γ ranging from 0 to 10.
REFERENCES,0.7163865546218487,"C
DISCUSSIONS AND RESULTS ON WILDS"
REFERENCES,0.7184873949579832,We provide a more detailed summary of each dataset in Table 5.
REFERENCES,0.7205882352941176,"Some entries in # Domains are omitted because the domains in each split overlap. Note that in
this paper we report the results on WILDS v1 — the benchmark has been updated since with slightly
different dataset splits. We are currently working on updating our results to v2 of WILDS."
REFERENCES,0.7226890756302521,Published as a conference paper at ICLR 2022
REFERENCES,0.7247899159663865,Table 5: Details of the 6 WILDS datasets we experimented on.
REFERENCES,0.726890756302521,"Dataset
Domains (# domains)
Data (x)
Target (y)
# Examples
# Domains"
REFERENCES,0.7289915966386554,"train
val
test
train
val
test"
REFERENCES,0.7310924369747899,"FMOW
Time (16), Regions (5)
Satellite images
Land use (62 classes)
76,863
19,915
22,108
11, -
3, -
2, -
POVERTY
Countries (23), Urban/rural (2)
Satellite images
Asset (real valued)
10,000
4,000
4,000
13, -
5, -
5, -
CAMELYON17
Hospitals (5)
Tissue slides
Tumor (2 classes)
302,436
34,904
85,054
3
1
1
CIVILCOMMENTS
Demographics (8)
Online comments
Toxicity (2 classes)
269,038
45,180
133,782
-
-
-
IWILDCAM2020
Trap locations (324)
Photos
Animal species (186 classes)
142,202
20,784
38,943
245
32
47
AMAZON
Reviewers (7,676)
Product reviews
Star rating (5 classes)
1,000,124
100,050
100,050
5,008
1,334
1,334"
REFERENCES,0.7331932773109243,Table 6: Results on POVERTYMAP-WILDS.
REFERENCES,0.7352941176470589,"Method
Val. Worst-U/R r Test Worst-U/R r"
REFERENCES,0.7373949579831933,"Fish
0.47 (±0.01)
0.30 (±0.01)
IRM
0.53 (±0.05)
0.43 (±0.07)
ERM
0.80 (±0.04)
0.78 (±0.04)
ERM (ours)
0.48 (±0.11)
0.29 (±0.02)
Coral
0.51 (±0.06)
0.45 (±0.06)"
REFERENCES,0.7394957983193278,Table 7: Results on CAMELYON17-WILDS.
REFERENCES,0.7415966386554622,"Method
Val. Accuracy (%)
Test Accuracy (%)"
REFERENCES,0.7436974789915967,"Fish
83.9 (±1.2)
74.7 (±7.1)
ERM
84.9 (±3.1)
70.3 (±6.4)
ERM (ours)
84.1 (±2.4)
70.5 (±12.1)
IRM
86.2 (±1.4)
64.2 (±8.1)
Coral
86.2 (±1.4)
59.5 (±7.7)"
REFERENCES,0.7457983193277311,"C.1
POVERTYMAP-WILDS"
REFERENCES,0.7478991596638656,Task: Asset index prediction (real-valued). Domains: 23 countries
REFERENCES,0.75,"The task is to predict the real-valued asset wealth index of an area, given its satellite imagery. Since
the number of domains considered here is large (23 countries), instead of looping over all S domains
in each inner-loop, we sample N << S domains in each iteration and perform inner-loop updates
using minibatches from these domains only to speed up computation. For this dataset we choose
N = 5 by hyper-parameter search."
REFERENCES,0.7521008403361344,"Evalutaion: Worst-U/R Pearson Correlation (r). Following the practice in WILDS benchmark, we
compare the results by computing the worst region Pearson correlation (r) between the predicted and
ground-truth asset index over 3 random seed runs."
REFERENCES,0.7542016806722689,"Results: We train the model using a ResNet-18 (He et al., 2016) backbone. See Table 6."
REFERENCES,0.7563025210084033,"We see that Fish obtains the highest test performance, with the same validation performance as the
best baseline. The performance is more stable between validation and test, and the standard deviation
is smaller than for the baselines. We also report the results of ERM models trained in our environment
as “ERM (ours)”, which shows similar performance to the canonical results reported in the WILDS
benchmark itself (“ERM”)."
REFERENCES,0.7584033613445378,"C.2
CAMELYON17-WILDS"
REFERENCES,0.7605042016806722,Task: Tumor detection (2 classes). Domains: 5 hospitals
REFERENCES,0.7626050420168067,"The CAMELYON17-WILDS dataset contains 450,000 lymph-node scans from 5 hospitals. Due to
the size of the dataset, instead of training with Fish from scratch, we pre-train the model with ERM
using the recommended hyper-parameters in Koh et al. (2020), and ﬁne-tune with Fish. For this
dataset, we ﬁnd that Fish performs the best when starting from a pretrained model that has not yet
converged, achieving much higher accuracy than the ERM model. we provide an ablation study on
this in Appendix E."
REFERENCES,0.7647058823529411,"Evaluation: Average accuracy. We evaluate the average accuracy of this binary classiﬁcation task.
Following Koh et al. (2020), we show the mean and standard deviation of results over 10 random
seeds runs. The number of random seeds required here is greater than other WILDS datasets due to
the large variance observed in results. Note that these random seeds are not only applied during the
ﬁne-tuning stage, but also to the pretrained models to ensure a fair comparison."
REFERENCES,0.7668067226890757,"Results: Following the practice in WILDS, we adopt DenseNet-121’s (Huang et al., 2017) architecture
for models trained on this dataset. See results in Table 7."
REFERENCES,0.7689075630252101,"The results show that Fish signiﬁcantly outperforms all baselines – its test accuracy surpasses the
best performing baseline by 6%. Also note that for all other baselines, there is a large gap between
validation and test accuracy (11% ∼27%). This is because WILDS chose the hospital that is the most"
REFERENCES,0.7710084033613446,Published as a conference paper at ICLR 2022
REFERENCES,0.773109243697479,"difﬁcult to generalize to as the test split to make the task more challenging. Surprisingly, as we can
observe in Table 7, the discrepancy between test and validation accuracy of Fish is quite small (3%).
The fact that it is able to achieve a similar level of accuracy on the worst-performing domain further
demonstrates that Fish does not rely on domain-speciﬁc information, and instead makes predictions
using the invariant features across domains."
REFERENCES,0.7752100840336135,"To demonstrate that Fish’s strong performance on CAMELYON17’s selected test set is not merely
coincidental, we randomly chose 3 ways of assigning the 5 domains as train/test/val (3/1/1 domains)
splits. See results collected over 10 random seeds in Table 8."
REFERENCES,0.7773109243697479,"Notably, on these different splits of CAMELYON17 Fish still on average outperforms ERM (with only
2 exceptions), demonstrating that Fish can generally achieve good performance on the Camelyon17
dataset."
REFERENCES,0.7794117647058824,Table 8: Results on CAMELYON17-WILDS with shufﬂed splits.
REFERENCES,0.7815126050420168,"Train/Val/Test ID
Val, ERM (%)
Val, Fish (%)
Test, ERM (%)
Test, Fish (%)"
REFERENCES,0.7836134453781513,"012/3/4
71.3 (±6.3)
71.9 (±5.1)
80.9 (±8.6)
88.9 (±6.9)
124/0/3
72.3 (±5.3)
74.1 (±4.3)
63.4 (±6.6)
65.9 (±3.9)
234/0/1
83.9 (±3.8)
84.1 (±2.3)
75.6 (±2.8)
73.9 (±3.5)
034/1/2 (original)
84.3 (±2.1)
82.5 (±1.2)
73.3 (±9.0)
79.5 (±6.0)"
REFERENCES,0.7857142857142857,Table 9: Results on FMOW-WILDS.
REFERENCES,0.7878151260504201,"Method
Val. Accuracy (%)
Test Accuracy (%)"
REFERENCES,0.7899159663865546,"Average
Worst
Average
Worst"
REFERENCES,0.792016806722689,"Fish
57.8 (±0.15)
49.5 (±2.34)
51.8 (±0.32)
34.6 (±0.18)
ERM
59.5 (±0.37)
48.9 (±0.62)
53.0 (±0.55)
32.3 (±1.25)
ERM (ours)
59.9 (±0.22)
47.1 (±1.21)
52.9 (±0.18)
30.9 (±1.53)
IRM
57.4 (±0.37)
47.5 (±1.57)
50.8 (±0.13)
30.0 (±1.37)
Coral
56.9 (±0.25)
47.1 (±0.43)
50.5 (±0.36)
31.7 (±1.24)"
REFERENCES,0.7941176470588235,"Table 10: Results on
CIVILCOMMENTS-WILDS."
REFERENCES,0.7962184873949579,"Method
Val. Accuracy (%)
Test Accuracy (%)"
REFERENCES,0.7983193277310925,"Average
Worst
Average
Worst"
REFERENCES,0.8004201680672269,"Fish
88.8 (±0.6)
70.5 (±1.0)
89.4 (±0.2)
75.3 (±0.6)
Group DRO
89.6 (±0.3)
68.7 (±1.0)
89.4 (±0.3)
70.4 (±2.1)
Reweighted
89.1 (±0.3)
67.9 (±1.2)
88.9 (±0.3)
67.3 (±0.1)
ERM
92.3 (±0.6)
53.6 (±0.7)
92.2 (±0.6)
58.0 (±1.2)
ERM (ours)
92.1 (±0.5)
54.1 (±0.4)
92.5 (±0.3)
58.1 (±1.7)"
REFERENCES,0.8025210084033614,"C.3
FMOW-WILDS"
REFERENCES,0.8046218487394958,Task: Infrastructure classiﬁcation (62 classes). Domains: 80 (16 years x 5 regions)
REFERENCES,0.8067226890756303,"Similar to CAMELYON17-WILDS, since the number of domains is large, we sample N = 5 domains
for each inner-loop. To speed up computation, we also use a pretrained ERM model and ﬁne-tune
with Fish; different from Appendix C.2, we ﬁnd the best-performing models are acquired when using
converged pretrained models (see details in Appendix E)."
REFERENCES,0.8088235294117647,"Evaluation: Average & worst-region accuracies. Following WILDS, the average accuracy evaluates
the model’s ability to generalize over years, and the worst-region accuracy measures the model’s
performance across regions under a time shift. We report results using 3 random seeds."
REFERENCES,0.8109243697478992,"Results: Following Koh et al. (2020), we use a DenseNet-121 pretrained on ImageNet for this dataset.
Results in Table 9 show that Fish has the highest worst-region accuracy on both test and validation
sets. It ranks second in terms of average accuracy, right after ERM. Again, Fish’s performance is
notably stable with the smallest standard deviation across all metrics compared to baselines."
REFERENCES,0.8130252100840336,"C.4
CIVILCOMMENTS-WILDS"
REFERENCES,0.8151260504201681,Task: Toxicity detection in online comments (2 classes). Domains: 8 demographic identities.
REFERENCES,0.8172268907563025,"The CIVILCOMMENTS-WILDS contains 450,000 comments collected from online articles, each
annotated for toxicity and the mentioning of demographic identities. Again, we use ERM pre-trained
model to speed up computation, and sample N = 5 domains for each inner-loop."
REFERENCES,0.819327731092437,"Evaluation: Worst-group accuracy. To study the bias of annotating comments that mentions
demographic groups as toxic, the WILDS benchmark proposes to evaluate the model’s performance"
REFERENCES,0.8214285714285714,Published as a conference paper at ICLR 2022
REFERENCES,0.8235294117647058,"by doing the following: 1) Further separate each of the 8 demographic identities into 2 groups by
toxicity – for example, separate black into black, toxic and black, not toxic; 2) measure the accuracies
of these 8 × 2 = 16 groups and use the lowest accuracy as the ﬁnal evaluation of the model. This
metric is equivalent to computing the sensitivity and speciﬁcity of the classiﬁer on each demographic
identity, and reporting the worse of the two metrics over all domains. Good performance on the
group with the worst accuracy implies that the model does not tend to use demographic identity as an
indicator of toxic comments."
REFERENCES,0.8256302521008403,"Again, following Koh et al. (2020) we report results of 3 random seed runs."
REFERENCES,0.8277310924369747,"Results: We compare results to the baselines used in the WILDS benchmark over 3 random seed runs
in Table 10. All models are trained using DistilBERT (Sanh et al., 2019)."
REFERENCES,0.8298319327731093,"The results show that Fish outperforms the best baseline by 4% and 7% on the test and validation set’s
worst-group accuracy respectively, and is competitive in terms of average accuracy with ERM (within
standard deviation). The strong performance of Fish on worst-group accuracy suggests that the model
relies the least on demographic identity as an indicator of toxic comments compared to other baselines.
ERM, on the other hand, has the highest average accuracy and the lowest worst-group accuracy. This
indicates that it achieves good average performance by leveraging the spurious correlation between
toxic comments and the mention of certain demographic groups."
REFERENCES,0.8319327731092437,"Note that different from all other datasets in WILDS that focus on pure domain generalization (i.e,
no overlap between domains in train and test splits), CIVILCOMMENTS-WILDS is a subpopulation
shift problem, where the domains in test are a subpopulation of the domains in train. As a result,
the baseline models used in WILDS for this dataset are different from the methods used in all other
datasets, and are tailored to avoiding systematic failure on data from minority subpopulations. Fish
works well in this setting too without any changes or special sampling strategies (such as ∗and +
in Table 10). This further demonstrates the good performance of our algorithm on different domain
generalization scenarios."
REFERENCES,0.8340336134453782,"C.5
IWILDCAM-WILDS"
REFERENCES,0.8361344537815126,Task: Animal species (186 classes). Domains: 324 camera locations.
REFERENCES,0.8382352941176471,"The dataset consists of over 200,000 photos of animal in the wild, using stationary cameras across
324 locations. Classifying animal species from these heat or motion-activated photos is especially
challenging: methods can easily rely on the background information of photos from the same camera
setup. Fish models are pretrained with ERM till convergence, and for each inner loop we sample
from N = 10 domains."
REFERENCES,0.8403361344537815,"Evaluation: Macro F1 score. Across the 186 class labels, we report average accuracy and both
weighted and macro F1 scores (F1-w and F1-m, respectively, in Table 11). We run 3 random seeds
for each model."
REFERENCES,0.842436974789916,"Results: All models reported in Table 7 are trained using a ResNet-50. We ﬁnd Fish to outperform
baselines on both test accuracy and weighted F1, with a 1% improvement on both metrics over the
best performing model (ERM). However, this comes at the cost of lower macro F1 score, where Fish
performs 1% worse than ERM models that we trained and 3% than the ERM reported in WILDS.
This suggests that Fish is less good at classifying rarer species, however the overall accuracy on the
test dataset is improved."
REFERENCES,0.8445378151260504,"Although Fish did not outperform the ERM baseline on the primary evaluation metric proposed in
Koh et al. (2020), we found the improvement of Fish in both accuracy and weighted F1 to be robust
across a range of hyperparameters. See more details on this in Appendix D."
REFERENCES,0.8466386554621849,"C.6
AMAZON-WILDS"
REFERENCES,0.8487394957983193,"Task: Sentiment analysis (5 classes). Domains: 7,676 Amazon reviewers."
REFERENCES,0.8508403361344538,"The dataset contains 1.4 million customer reviews on Amazon from 7,676 customers, and the task
is to predict the score (1-5 stars) given the review. Similarly, we pretrained the model with ERM"
REFERENCES,0.8529411764705882,Published as a conference paper at ICLR 2022
REFERENCES,0.8550420168067226,"till convergence, and due to the large number of domains (S = 5008 in train) we sample N = 5
reviewers for each inner loop."
REFERENCES,0.8571428571428571,"Evaluation: 10th percentile accuracy. Reporting the accuracy of the 10th percentile reviewer helps
us assess whether the model performance is consistent across different reviewers. The results in
Table 12 are reported over 3 random seeds."
REFERENCES,0.8592436974789915,"Results: The model is trained using DISTILBERT (Sanh et al., 2019) backbone. While Fish has lower
average accuracy compared to ERM, its 10th percentile accuracy matches that of ERM, outperforming
all other baselines.
Table 11: Results on IWILDCAM-WILDS."
REFERENCES,0.8613445378151261,"Method
Test ID Macro F1 (%)
Test ID Avg Acc (%) Test OOD Macro F1 (%) Test OOD Avg Acc (%)"
REFERENCES,0.8634453781512605,"Fish
40.3 (±0.6)
73.8 (±0.1)
22.0 (±1.8)
64.7 (±2.6)
GroupDRO
37.5 (±1.7)
71.6 (±2.7)
23.9 (±2.1)
72.7 (±2.0)
ERM
47.0 (±1.4)
75.7 (±0.3)
31.0 (±1.3)
71.6 (±2.5)
Coral
43.5 (±3.5)
73.7 (±0.4)
32.8 (±0.1)
73.3 (±4.3)
IRM
22.4 (±7.7)
59.9 (±8.1)
15.1 (±4.9)
59.8 (±3.7)"
REFERENCES,0.865546218487395,Table 12: Results on AMAZON-WILDS.
REFERENCES,0.8676470588235294,"Method
Val. Accuracy (%)
Test Accuracy (%)"
REFERENCES,0.8697478991596639,"Average
10-th per.
Average
10-th per."
REFERENCES,0.8718487394957983,"Fish
72.5 (±0.0)
54.0 (±0.0)
71.7 (±0.0)
53.3 (±0.0)
ERM
72.7 (±0.1)
55.2 (±0.7)
71.9 (±0.1)
53.8 (±0.0)
IRM
71.5 (±0.3)
54.2 (±0.8)
70.5 (±0.3)
52.4 (±0.8)
Reweighted
69.1 (±0.0)
52.1 (±0.2)
68.6 (±0.6) 52.0 (±0.0)"
REFERENCES,0.8739495798319328,"D
HYPERPARAMETERS"
REFERENCES,0.8760504201680672,"In Table 13 we list the hyperparameters we used to train ERM. The same hyperparameters were
used for producing ERM baseline results and as pretrained models for Fish. In val.
metric
we report the metric on validation set that is used for model selection, and in cut-off we specify
when to stop training when using ERM to generate pretrained models."
REFERENCES,0.8781512605042017,"Table 13: Hyperparameters for ERM. We follow the hyperparameters used in WILDS benchmark.
Note that we did not use a pretrained model for POVERTY, therefore its cut-off condition is not
reported."
REFERENCES,0.8802521008403361,"Dataset
Model
Learning rate
Batch size
Weight decay
Optimizer
Val. metric
Cut-off"
REFERENCES,0.8823529411764706,"CAMELYON17
Densenet-121
1e-3
32
0
SGD
acc. avg.
iter 500
CIVILCOMMENTS
DistilBERT
1e-5
16
0.01
Adam
acc. wg.
Best val. metric
FMOW
Densenet-121
1e-4
64
0
Adam
acc. avg.
Best val. metric
IWILDCAM
Resnet-50
1e-4
16
0
Adam
F1-macro (all)
Best val. metric
POVERTY
Resnet-18
1e-3
64
0
Adam
Worst-U/R Pearson (r)
-
AMAZON
DistilBERT
2e-6
8
0.01
Adam
10th percentile acc.
-"
REFERENCES,0.884453781512605,"In Table 14 we list out the hyperparameters we used to train Fish. Note that we train Fish using the
same model, batch size, val metric and optimizer as ERM – these are not listed in Table 14 to avoid
repetitions. Weight decay is always set as 0."
REFERENCES,0.8865546218487395,"E
ABLATION STUDIES ON PRE-TRAINED MODELS"
REFERENCES,0.8886554621848739,"In this section we perform ablation study on the convergence of pretrained ERM models. Note that
the pretraining is done on the same domain generalization dataset that Fish is trained on later, not on
ImageNet. We study the performance of Fish with the following three conﬁgurations of pretrained
ERM models:"
REFERENCES,0.8907563025210085,Published as a conference paper at ICLR 2022
REFERENCES,0.8928571428571429,Table 14: Hyperparameters for Fish.
REFERENCES,0.8949579831932774,"Dataset
Group by
α
ϵ
# domains
Meta steps"
REFERENCES,0.8970588235294118,"CAMELYON17
Hospitals
1e-3 0.01
3
3
CIVILCOMMENTS
Demographics × toxicity
1e-5
0.05
16
5
FMOW
time × regions
1e-4 0.01
80
5
IWILDCAM
Trap locations
1e-4 0.01
324
10
POVERTY
Countries
1e-3
0.1
23
5
AMAZON
Reviewers
2e-6 0.01
7,676
5"
REFERENCES,0.8991596638655462,1. Model is trained on 10% of the data (epoch 1);
REFERENCES,0.9012605042016807,2. Model is trained on 50% of the data (epoch 1);
REFERENCES,0.9033613445378151,3. Model at convergence.
REFERENCES,0.9054621848739496,"By comparing the results between these three settings, we demonstrate how the level of convergence
affects the Fish’s training performance. See results in Table 15. Note that POVERTY is excluded here
because the dataset is small enough that we are able to train Fish from scratch."
REFERENCES,0.907563025210084,Table 15: Ablation study on pretrained ERM models.
REFERENCES,0.9096638655462185,"Model
FMOW
CAMELYON17
IWILDCAM
CIVILCOMMENTS"
REFERENCES,0.9117647058823529,"Test Avg Acc
Test Avg Acc
Test Macro F1
Test Worst Acc"
REFERENCES,0.9138655462184874,"10% data
21.7 (±2.5)
79.1 (±12.3)
13.7 (±0.5)
71.8 (±1.3)
50% data
31.0 (±0.8)
64.6 (±12.3)
19.0 (±0.06)
74.2 (±0.5)
Converged
32.7 (±1.2)
63.5 (±8.2)
23.7 (±0.9)
73.8 (±1.8)"
REFERENCES,0.9159663865546218,"We see that CIVILCOMMENTS sustain good performance using pretrained models at different
convergence levels. FMOW and IWILDCAM on the other hand seems to have strong preference
towards converged model, and the results worsen as the amount of data seen during training goes
down. CAMELYON17 achieves the best performance when only 10% of data is seen, and the test
accuracy decreases while training with models with higher level of convergence."
REFERENCES,0.9180672268907563,"F
ABLATION STUDIES ON HYPERPARAMETERS"
REFERENCES,0.9201680672268907,"α and ϵ
We study the effect of changing Fish’s inner loop learning rate α and outer loop learning
rate ϵ. To make the comparisons more meaningful, we keep α · ϵ constant while changing their
respective values. See results in Figure 8."
REFERENCES,0.9222689075630253,"Meta steps N
For most of the datasets we studied (all apart from CAMELYON17 where T = 3) we
sample a N-sized subset of all T domains available for training (see Table 14 for T of each dataset).
Here we study when N = 5, 10, 20."
REFERENCES,0.9243697478991597,Table 16: Ablation study on meta steps N.
REFERENCES,0.9264705882352942,"N
FMOW
POVERTY
IWILDCAM
CIVILCOMMENTS"
REFERENCES,0.9285714285714286,"Test Avg Acc
Test Pearson r
Test Macro F1
Test Worst Acc"
REFERENCES,0.930672268907563,"5
33.0 (±1.6)
80.3 (±1.7)
23.7 (±0.9)
74.3 (±1.5)
10
32.7 (±1.2)
80.0 (±0.8)
23.7 (±0.5)
73.4 (±1.0)
20
33.3 (±2.1)
77.7 (±2.1)
23.7 (±0.9)
72.6 (±2.3)"
REFERENCES,0.9327731092436975,"In general altering these hyperparameters don’t have a huge impact on the model’s performance,
however it does seem thet when N = 20 the performance on some datasets (POVERTY and CIVIL-
COMMENTS) degrade slightly."
REFERENCES,0.9348739495798319,Published as a conference paper at ICLR 2022
REFERENCES,0.9369747899159664,"(a) CAMELYON17
(b) CIVILCOMMENTS
(c) FMOW"
REFERENCES,0.9390756302521008,"(d) IWILDCAM
(e) POVERTY"
REFERENCES,0.9411764705882353,"Figure 8: Ablation studies on α and ϵ. Note that α × ϵ remains constant in all experiments, and the
midpoint of each plot is the hyperparameter we chose to use to report our experiment results."
REFERENCES,0.9432773109243697,"(a) CDSPRITES-N
(b) CAMELYON17
(c) CIVILCOMMENTS"
REFERENCES,0.9453781512605042,"(d) FMOW
(e) POVERTY
(f) IWILDCAM"
REFERENCES,0.9474789915966386,"Figure 9: Gradient inner product values during the training for CDSPRITES-N (N=15) and 5 different
WILDS datasets."
REFERENCES,0.9495798319327731,"G
TRACKING GRADIENT INNER PRODUCT"
REFERENCES,0.9516806722689075,"In Figure 9, we demonstrate the progression of inter-domain gradient inner products during training
using different objectives. We train both Fish (blue) and ERM (yellow) untill convergence while
recording the normalized gradient inner products (i.e. cosine similarity) between minibatches from
different domains used in each inner-loop. The gradient inner products are computed both before
(dotted) and after (solid) the model’s update. To ensure a fair comparison, we use the exact same
sequence of data for Fish and ERM (see Appendix I for more details)."
REFERENCES,0.9537815126050421,"Inevitably, the gradient inner product trends differently for each dataset since the data, types of
domain splits and the choice of architecture are very different. In fact, the plot for CDSPRITES-N and
POVERTY are signiﬁcantly different from others, with a dip in gradient inner product at the beginning"
REFERENCES,0.9558823529411765,Published as a conference paper at ICLR 2022
REFERENCES,0.957983193277311,"of training – this is because these are the two datasets that we train from scratch. On all other datasets,
the gradient inner products are recorded when ﬁne-tuning with Fish."
REFERENCES,0.9600840336134454,"Despite their differences, there are some important commonalities between these plots: if we compare
the pre-update (dotted) to post-update (solid) curves, we can see that ERM updates often result in
the decrease of gradient inner product, while for Fish it can either increase signiﬁcantly (Figure 9c
and Figure 9e) or at least stay at the same level (Figure 9a, Figure 9b, Figure 9d and Figure 9f). As a
result of this, we can see that the post-update gradient inner product of Fish is always greater than
ERM at convergence."
REFERENCES,0.9621848739495799,"The observations here shows that Fish is effective in increasing/maintaining the level of inter-domain
gradient inner product and sheds some lights on its efﬁciency on the datasets we studied."
REFERENCES,0.9642857142857143,"H
SUMMARY OF DOMAINBED"
REFERENCES,0.9663865546218487,"DOMAINBED is a testbed for domain generalization that implements consistent experimental proto-
cols across SOTA methods to ensure fair comparison. It contains 7 popular domain generalization
datasets, including Colored MNIST (Arjovsky et al., 2019), Rotated MNIST (Ghifary et al., 2015),
VLCS (Fang et al., 2013), PACS (Li et al., 2017), OfﬁceHome (Venkateswara et al., 2017), Terra
Incognita (Beery et al., 2018) and DomainNet (Peng et al., 2019), and offers comparison to a variety
of SOTA domain generalization methods, including IRM (Arjovsky et al., 2019), Group DRO (Hu
et al., 2018; Sagawa et al., 2019), Mixup (Yan et al., 2020), MLDG (Li et al., 2018a), Coral (Sun and
Saenko, 2016), MMD (Li et al., 2018b), DANN (Ganin et al., 2016) and CDANN (Li et al., 2018)."
REFERENCES,0.9684873949579832,"I
ALGORITHM FOR TRACKING GRADIENT INNER PRODUCT"
REFERENCES,0.9705882352941176,"To make sure that the gradients we record for ERM and Fish are comparable, we use the same
sequence of S-minibatches to train both algorithms. See Algorithm 6 for details."
REFERENCES,0.9726890756302521,Algorithm 5 Function GIP
REFERENCES,0.9747899159663865,"1: function GIP({d1, d2, · · · , dN}, θ):
2: for dn ∈{d1, d2, · · · , dN} do
3:
gn = ∂l(dn; θ)/∂θ
4: end for
5: ¯¯g =
1
S(S−1)
Pi̸=j
i,j∈S gi · gj
6: return ¯¯g"
REFERENCES,0.976890756302521,"J
T-SNE VISUALISATION OF LEARNED REPRESENTATIONS"
REFERENCES,0.9789915966386554,"In this section we visualize the representation learned by Fish and ERM with t-SNE (van der Maaten
and Hinton, 2008) for PACS, VLCS and OfﬁceHome datasets. See Figure 10."
REFERENCES,0.9810924369747899,"We can see that Fish and ERM are both capable of forming distinctive label-clusters on PACS (ﬁrst
row) and VLCS (second row), however with ERM we can observe within each label cluster that
sub-clusters of domains are forming. This is particularly the case for the the red and yellow cluster in
Figure 10a and cyan cluster in Figure 10c. The domain clustering phenomena is not observed for
Fish. On the other hand, for the OfﬁceHome dataset where Fish outperforms ERM by more than 1%,
we clearly see that Fish exhibits better label clustering performance than ERM."
REFERENCES,0.9831932773109243,Published as a conference paper at ICLR 2022
REFERENCES,0.9852941176470589,"Algorithm 6 Algorithm of collecting gradient inner product ¯¯g for Fish and ERM both before and
after updates. See GIP in Algorithm 5."
REFERENCES,0.9873949579831933,"1: Initialize Fish θf ←θ, ERM θe ←θ
2: for i = 1, 2, · · · do
3:
//Get all minibatches
4:
for Dn ∈{D1, D2, · · · , DN} do
5:
Sample batch dn ∼Dn
6:
end for
7:
//GradInnerProd before update
8:
¯¯gF b = GIP({d1, d2, · · · , dN}, θf)
9:
¯¯gEb = GIP({d1, d2, · · · , dN}, θe)
10:
//Fish training
11:
˜θ ←θf
12:
for dn ∈{d1, d2, · · · , dN} do
13:
gn = ∂l(dn; ˜θ)/∂˜θ
14:
Update ˜θ ←˜θ −αgn
15:
end for
16:
θf ←θf + ϵ(˜θ −θf)
17:
//Rearrange minibatches
18:
d = shuffle(concat(d1, d2, · · · , dN))
19:
{ ˜d1, ˜d2, · · · , ˜dN} = split(d)
20:
//ERM training
21:
for ˜dn ∈{ ˜d1, ˜d2, · · · , ˜dN} do
22:
gn = ∂l( ˜dn; θe)/∂θe
23:
Update θe ←θe −αgn
24:
end for
25:
//GradInnerProd after update
26:
¯¯gF a = GIP({d1, d2, · · · , dN}, θf)
27:
¯¯gEa = GIP({d1, d2, · · · , dN}, θe)
28: end for
29: Return ¯¯gF b, ¯¯gF a, ¯¯gEb, ¯¯gEa"
REFERENCES,0.9894957983193278,Published as a conference paper at ICLR 2022
REFERENCES,0.9915966386554622,"(a) PACS, ERM
(b) PACS, FISH"
REFERENCES,0.9936974789915967,"(c) VLCS, ERM
(d) VLCS, FISH"
REFERENCES,0.9957983193277311,"(e) ERM, OFFICEHOME
(f) FISH, OFFICEHOME"
REFERENCES,0.9978991596638656,"Figure 10: t-SNE plot for PACS, VLCS and OfﬁceHome. Colors represent labels, markers (shape of
each datapoint) represent domain."
