Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0027247956403269754,"Text-based games (TBG) have emerged as promising environments for driving
research in grounded language understanding and studying problems like
generalization and sample efﬁciency. Several deep reinforcement learning (RL)
methods with varying architectures and learning schemes have been proposed for
TBGs. However, these methods fail to generalize efﬁciently, especially under
distributional shifts.
In a departure from deep RL approaches, in this paper,
we propose a general method inspired by case-based reasoning to train agents
and generalize out of the training distribution. The case-based reasoner collects
instances of positive experiences from the agent’s interaction with the world in the
past and later reuses the collected experiences to act efﬁciently. The method can be
applied in conjunction with any existing on-policy neural agent in the literature for
TBGs. Our experiments show that the proposed approach consistently improves
existing methods, obtains good out-of-distribution generalization, and achieves
new state-of-the-art results on widely used environments."
INTRODUCTION,0.005449591280653951,"1
INTRODUCTION"
INTRODUCTION,0.008174386920980926,"Text-based games (TBGs) have emerged as key benchmarks for studying how reinforcement
learning (RL) agents can tackle the challenges of grounded language understanding, partial
observability, large action spaces, and out-of-distribution generalization (Hausknecht et al., 2020;
Ammanabrolu & Riedl, 2019). While we have indeed made some progress on these fronts in recent
years (Ammanabrolu & Hausknecht, 2020; Adhikari et al., 2020; Murugesan et al., 2021b;a), these
agents are still very inefﬁcient and suffer from insufﬁcient generalization to novel environments.
As an example, state-of-the-art agents require 5 to 10 times more steps than a human to accomplish
even simple household tasks (Murugesan et al., 2021b). As the agents are purely neural architectures
requiring signiﬁcant training experience and computation, they fail to efﬁciently adapt to new
environments and use their past experiences to reason in novel situations. This is in stark contrast to
human learning which is much more robust, efﬁcient and generalizable (Lake et al., 2017)."
INTRODUCTION,0.010899182561307902,"Motivated by this fundamental difference in learning, we propose new agents that rely on case-based
reasoning (CBR) (Aamodt & Plaza, 1994) to efﬁciently act in the world. CBR draws its foundations
in cognitive science (Schank, 1983; Kolodner, 1983) and mimics the process of solving new tasks
based on solutions to previously encountered similar tasks. Concretely, we design a general CBR
framework that enables an agent to collect instances of past situations which led to a positive reward
(known as cases). During decision making, the agent retrieves the case most similar to the current
situation and then applies it after appropriately mapping it to the current context."
INTRODUCTION,0.013623978201634877,"The CBR agent stores past experiences, along with the actions it performed, in a case memory.
In order to efﬁciently use these stored experiences, the agent should be able to represent relevant
contextual information from the state of the game in a compact way, while retaining the property"
INTRODUCTION,0.01634877384196185,Published as a conference paper at ICLR 2022 Knife
INTRODUCTION,0.01907356948228883,Player
INTRODUCTION,0.021798365122615803,"has
Forest
has Troll in
in"
INTRODUCTION,0.02452316076294278,east of
INTRODUCTION,0.027247956403269755,State Graph
INTRODUCTION,0.02997275204359673,"House
Key Knife"
INTRODUCTION,0.0326975476839237,Forest Troll Key
INTRODUCTION,0.035422343324250684,Player
INTRODUCTION,0.03814713896457766,Seeded Graph Attention
INTRODUCTION,0.04087193460490463,"Context
representation"
INTRODUCTION,0.043596730245231606,Discretization House
INTRODUCTION,0.04632152588555858,Symbolic action reuse
INTRODUCTION,0.04904632152588556,Kill troll with knife
INTRODUCTION,0.051771117166212535,Neural agent
INTRODUCTION,0.05449591280653951,Execute or revise action
INTRODUCTION,0.05722070844686648,"Memory writing
if reward > 0"
INTRODUCTION,0.05994550408719346,2-4-1-4
INTRODUCTION,0.06267029972752043,Discretization
INTRODUCTION,0.0653950953678474,"2-4-1-5
Kill orc with sword
Put emerald in case
Memory
1-3-5-2"
INTRODUCTION,0.0681198910081744,Retrieve
INTRODUCTION,0.07084468664850137,"2-4-1-5
Kill orc with sword
Put emerald in case
Memory
1-3-5-2"
INTRODUCTION,0.07356948228882834,"Kill troll with knife
2-4-1-4"
INTRODUCTION,0.07629427792915532,"Reuse
Revise"
INTRODUCTION,0.07901907356948229,Retain
INTRODUCTION,0.08174386920980926,2-4-1-4
INTRODUCTION,0.08446866485013624,"> Take knife.
You take the knife from the
ground."
INTRODUCTION,0.08719346049046321,"> Go east
You find yourself in a forest. 
A nasty-looking troll blocks
your way."
INTRODUCTION,0.08991825613079019,Admissible actions
INTRODUCTION,0.09264305177111716,Observations & Actions
INTRODUCTION,0.09536784741144415,Drop knife
INTRODUCTION,0.09809264305177112,Drop key
INTRODUCTION,0.1008174386920981,"Go west
Kill troll with knife"
INTRODUCTION,0.10354223433242507,"Figure 1: Overview of the approach and architecture of the CBR agent. A memory stores actions that have
been used in previous interactions. The context of the game is learned from the state knowledge graph using
a graph attention mechanism. Actions are retrieved from the memory based on this context representation and
mapped to the current state. If no valid action is obtained using CBR, the algorithm falls back to a neural agent."
INTRODUCTION,0.10626702997275204,"that contexts that require similar actions receive similar representations. We represent the state
of the game as a knowledge graph (Ammanabrolu & Hausknecht, 2020) and we address these
challenges by utilizing (a) a seeded message propagation that focuses only on a subset of relevant
nodes and (b) vector quantization (Ballard, 2000) to efﬁciently map similar contexts to similar
discrete representations. Vector quantization allows the model to signiﬁcantly compress the context
representations while retaining their semantics; thereby, allowing for a scalable implementation of
CBR in RL settings. An illustration of the framework is shown in Figure 1."
INTRODUCTION,0.10899182561307902,"Our experiments show that CBR can be used to consistently boost the performance of various on-
policy RL agents proposed in the literature for TBGs. We obtain a new state-of-the-art on the
TextWorld Commonsense (Murugesan et al., 2021b) dataset and we achieve better or comparable
scores on 24 of the 33 games in the Jericho suite (Hausknecht et al., 2020) compared to previous
work. We also show that CBR agents are resilient to domain shifts and suffer only marginal drops
in performance (6%) on out-of-distribution settings when compared to their counterparts (35%)."
PRELIMINARIES,0.11171662125340599,"2
PRELIMINARIES"
PRELIMINARIES,0.11444141689373297,"Text-based games.
Text-based games (TBGs) provide a challenging environment where an agent
can observe the current state of the game and act in the world using only the modality of text. The
state of the game is hidden, so TBGs can be modeled as a Partially Observable Markov Decision
Process (POMDP) (S, A, O, T , E, r), where S is the set of states of the environment of the game, A
is the natural language action space, O is the set of observations or sequences of words describing
the current state, T are the conditional transition probabilities from one state to another, E are the
conditional observation probabilities, r : S ×A →R is the reward function, which maps a state and
action to a scalar reward that the agent receives."
PRELIMINARIES,0.11716621253405994,"Case-based reasoning.
Case-based reasoning (CBR) is the process of solving new problems based
on the solution of previously seen similar problems. Generally, CBR assumes access to a memory
that stores past problems (known as cases) and their solutions. When a new problem is encountered,
CBR will (i) retrieve a similar problem and its solution from memory; (ii) reuse the solution by
mapping it to the current problem; (iii) revise the solution by testing it and checking whether it is a
viable way to address the new problem; and (iv) retain the solution in memory if the adaptation to
the new problem was successful."
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.11989100817438691,"3
CASE-BASED REASONING IN REINFORCEMENT LEARNING"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1226158038147139,"This section introduces our framework inspired by CBR for improving generalization in TBGs.
Even though we situate our work in TBGs, it serves as a good starting point for applying CBR in
more general RL settings. We consider an on-policy RL agent that, at any given time step t, has
access to a memory Mt, that can be used to retrieve previous experiences. The memory contains"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.12534059945504086,Published as a conference paper at ICLR 2022
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.12806539509536785,"key-value pairs, where the keys are a context representation of a game state and values are actions
that were taken by the agent w.r.t to this context. As mentioned in Section 2, case-based reasoning
can be formalized as a four-step process. We describe our proposed methodology for each step
below. Algorithm 1 provides a detailed formalization of our approach."
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1307901907356948,Algorithm 1: CBR in Text-based RL
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1335149863760218,"• Retrieve
Let Ct = {context(st, at) | at ∈At} be a set
of context selectors for state st at time step t
AM
t
←∅
for ct ∈Ct do"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1362397820163488,"Let (cM
t , aM
t ) =
arg max(cM
t ,aM
t )∈Mt sim(ct, cM
t )
Let δ = sim(ct, cM
t )
if δ > τ then"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.13896457765667575,"AM
t
←AM
t
∪{(aM
t , δ)}
end
end"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.14168937329700274,"• Reuse
Build a set of action candidates:
˜
At = {reuse(aM
t , st, δ) |
(aM
t , δ) ∈AM
t }"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1444141689373297,"• Revise
if At ∩˜
At ̸= ∅then
Let a⋆
t , δ⋆= arg max˜at,δ∈˜
At δ
else"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.14713896457765668,"a⋆
t = arg maxat∈At π(at|st)
end
Let rt = r(st, a⋆
t ) be the reward obtained at
time step t by executing action a⋆
t
• Retain
Let c⋆
t = context(st, a⋆
t ) be the context of
action a⋆
t
T = {(c⋆
t , a⋆
t ), . . . , (c⋆
t−m+1, a⋆
t−m+1)}
if rt > 0 then"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.14986376021798364,"Mt+1 ←Mt ∪retain(T )
end"
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.15258855585831063,"Retrieve.
Given the state of the game st
and the valid actions At, we want to retrieve
from the memory Mt previous experiences
that might be useful in decision-making at the
current state. To this end, for each admissible
action at ∈At, we deﬁne a context selector
ct = context(st, at). The context selector is
an action-speciﬁc representation of the state,
namely the portion of the state that is relevant
to the execution of an action. We will explain
later how the context selector is deﬁned in
our implementation. For each context ct, we
retrieve from the memory the context-action
pair (cM
t , aM
t ), such that cM
t
has maximum
similarity with ct.
We denote as δ
=
sim(ct, cM
t ) ∈[0, 1] the relevance score given
to the retrieved action. Only actions aM
t
with
a relevance score above a retriever threshold τ
are retrieved from Mt. We denote as AM
t
the
ﬁnal set of action-relevance pairs returned by
the retriever, as shown in Algorithm 1."
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.1553133514986376,"Reuse.
The goal of the reuse step is to adapt
the actions retrieved from the memory based on
the current state.
This is accomplished by a
reuse function, that is applied to each retrieved
action to construct a set ˜
At of candidate actions
that should be applicable to the current state,
each paired with a conﬁdence level."
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.15803814713896458,"Revise.
If any of the action candidates ˜
At is
a valid action, then the one with the highest
relevance δ is executed, otherwise a neural
agent π is used to select the best action a⋆
t . We
denote with rt = r(st, a⋆
t ) the obtained reward.
Note that π can be an existing agent for TBGs
(Murugesan et al., 2021c;b; Ammanabrolu &
Hausknecht, 2020)."
CASE-BASED REASONING IN REINFORCEMENT LEARNING,0.16076294277929154,"Retain.
Finally, the retain step stores successful experiences as new cases in the memory, so that
they can be retrieved in the future. In principle, this can be accomplished by storing actions for
which the agent obtained positive rewards. However, we found that storing previous actions as
well can result in improved performance. Therefore, whenever rt > 0, a retain function is used
to select which of the past executed actions and their contexts should be stored in the memory. In
our experiments, the retain function selects the k most recent actions, but other implementations are
possible, as discussed in Appendix D."
A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES,0.16348773841961853,"4
A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES"
A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES,0.16621253405994552,"Designing an agent that can act efﬁciently in TBGs using the described approach poses several
challenges. Above all, efﬁcient memory use is crucial to making the approach practical and scalable.
Since the context selectors are used as keys for accessing values in the memory, their representation"
A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES,0.16893732970027248,Published as a conference paper at ICLR 2022
A CBR POLICY AGENT TO GENERALIZE IN TEXT-BASED GAMES,0.17166212534059946,"needs to be such that contexts where similar actions were taken receive similar representations. At
the same time, as the state space is exponential, context representations need to be focused only on
relevant portions of the state and they need to be compressed and compact."
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.17438692098092642,"4.1
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.1771117166212534,"State space as a knowledge graph.
Following previous works (Ammanabrolu & Riedl, 2019;
Ammanabrolu & Hausknecht, 2020; Murugesan et al., 2021c), we represent the state of the game
as a dynamic knowledge graph Gt = (Vt, Rt, Et), where a node v ∈Vt represents an entity in the
game, r ∈Rt is a relation type, and an edge v
r−→v′ ∈Et represents a relation of type r ∈Rt
between entities v, v′ ∈Vt. In TBGs, the space of valid actions At can be modeled as a template-
based action space, where actions at are instances of a ﬁnite set of templates with a given set of
entities, denoted as Vat ⊆Vt. As an example, the action “kill orc with sword” can be seen as an
instance of the template “kill v1 with v2”, where v1 and v2 are “orc” and “sword” respectively."
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.17983651226158037,"Seeded graph attention.
The state graph Gt and the entities Vat are provided as input to the agent
for each action at ∈At, in order to build an action-speciﬁc contextualized representation of the
state. A pre-trained BERT model (Devlin et al., 2019) is used to get a representation h(0)
v
∈Rd for
each node v ∈Vt. Inspired by Sun et al. (2018), we propose a seeded graph attention mechanism
(GAT), so that the propagation of messages is weighted more for nodes close to the entities Vat. Let
α(l)
vu denote the attention coefﬁcients given by a graph attention network (Velickovic et al., 2018) at
layer l for nodes v, u ∈Vt. Then, for each node v ∈Vt, we introduce a coefﬁcient β(l)
v
that scales
with the amount of messages received by node v at layer l:"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.18256130790190736,"β(1)
v
="
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.18528610354223432,"(
1
|Vat |
if v ∈Vat
0
otherwise ,
β(l+1)
v
= (1 −λ)β(l)
v
+ λ
X"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.1880108991825613,"u∈Nv
α(l)
vuβ(l)
u ,"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.1907356948228883,"where Nv denotes the neighbors of v, considering the graph as undirected. Note that, at layer l = 1,
only the nodes in Vat receive messages, whereas for increasing values of l, β(l)
v
will be non-zero for
their (l −1)-hop neighbors as well. The representation of each v ∈Vt is then updated as:"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.19346049046321526,"h(l)
v
= FFN(l)"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.19618528610354224,"h(l−1)
v
+ β(l)
v
X"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.1989100817438692,"u∈Nv
α(l)
vuW(l)h(l−1)
u ! ,"
REPRESENTING THE CONTEXT THROUGH SEEDED GRAPH ATTENTION,0.2016348773841962,"where FFN(l+1) is a 2-layer feed-forward network with ReLU non-linearity and W(l) ∈Rd×d
are learnable parameters. Finally, we compute a ﬁnal continuous contextualized representation cat
of the state by summing the linear projections of the hidden representations of each v ∈Vat and
passing the result through a further feed-forward network."
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION,0.20435967302452315,"4.2
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION"
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION,0.20708446866485014,"Given a continuous representation cat of the context, we need an efﬁcient way to access the memory
Mt to retrieve or store actions based on such a context selector. Storing and retrieving based on the
continuous representation cat would be impractical for scalability reasons. Additionally, since the
parameters of the agent change over the training time, the same context would result in several
duplicated entries in the memory even with a pre-trained agent over different episodes."
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION,0.2098092643051771,"Discretization of the context.
To address these problems, we propose to use vector quantization
(Ballard, 2000) before reading or writing to memory. Following previous work (Chen et al., 2018;
Sachan, 2020), we learn a discretization function φ : Rd →ZD
K, that maps the continuous
representation cat into a K-way D-dimensional code ct ∈ZD
K, with |ZK| = K (we refer to ct
as a KD code). With reference to Section 3, then we will use ct = context(st, at) = φ(cat) as the
context selector used to access the memory Mt. In order to implement the discretization function,
we deﬁne a set of K key vectors ki ∈Rd, i = 1, . . . , K, and we divide each vector in D partitions
kj
i ∈Rd/D, j = 1, . . . , D. Similarly, we divide cat in D partitions cj
at ∈Rd/D, j = 1, . . . , D.
Then, we compute the j-th code zj of ct by nearest neighbor search, as zj = arg mini ∥cj
at −kj
i∥2
2.
We use the straight-through estimator (Bengio et al., 2013) to address the non differentialbility of
the argmin operator."
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION,0.2125340599455041,Published as a conference paper at ICLR 2022
MEMORY ACCESS THROUGH CONTEXT QUANTIZATION,0.21525885558583105,"Memory access.
The KD codes introduced above are used to provide a memory-efﬁcient
representation of the keys in the memory. Then, given the KD code representing the current context
selector ct, we query the memory by computing a similarity measure sim(ct, cM
t ) between ct and
each cM
t
in Mt. The similarity function is deﬁned as the fraction of codes shared by ct and cM
t . The
context-action pair with the highest similarity is returned as a result of the memory access, together
with a relevance score δ representing the value of the similarity measure."
SYMBOLIC ACTION REUSE AND REVISE POLICY,0.21798365122615804,"4.3
SYMBOLIC ACTION REUSE AND REVISE POLICY"
SYMBOLIC ACTION REUSE AND REVISE POLICY,0.22070844686648503,"We use a simple purely symbolic reuse function to adapt the actions retrieved from the memory to
the current state. Let ct be the context selector computed based on state st and the entities Vat, as
explained in Sections 4.1 and 4.2. Denote with (cM
t , aM
t ) the context-action pair retrieved from Mt
with conﬁdence δ. Then, the reuse function reuse(aM
t , st, δ) constructs the action candidate ˜at as
the action with the same template as aM
t
applied to the entities Vat. If the reuse step cannot generate
a valid action, we revert to the neural policy agent π that outputs a probability distribution over the
current admissible actions At."
TRAINING,0.22343324250681199,"5
TRAINING"
TRAINING,0.22615803814713897,"In Section 3, we have introduced an on-policy RL agent that relies on case-based reasoning to act
in the world efﬁciently. This agent can be trained in principle using any online RL method. This
section discusses the training strategies and learning objectives used in our implementation."
TRAINING,0.22888283378746593,"Objective.
Two main portions of the model need to be trained: (a) the retriever, namely the
neural network that computes the context representation and accesses the memory through its
discretization, and (b) the main neural agent π which is used in the revise step. All agents π
used in our experiments are trained with an Advantage Actor-Critic (A2C) method. For optimizing
the parameters of π, we use the same learning objectives deﬁned by Adolphs & Hofmann (2019),
as described in Appendix A. Whenever the executed action a⋆
t is not chosen by the model π but
it comes from the symbolic reuse step, then we optimize instead an additional objective for the
retriever, namely the following contrastive loss (Hadsell et al., 2006):"
TRAINING,0.23160762942779292,"L(t)
r
= 1"
TRAINING,0.23433242506811988,"2(1 −yt)(1 −sim(ct, cM
t ))2 + 1"
TRAINING,0.23705722070844687,"2yt max{0, µ −1 + sim(ct, cM
t )}2,"
TRAINING,0.23978201634877383,"where ct denotes the context selector of the action executed at time step t, cM
t
is the corresponding
key entry retrieved from Mt, µ is the margin parameter of the contrastive loss, and yt = 1 if rt > 0,
yt = 0 otherwise. This objective encourages the retriever to produce similar representations for two
contexts where reusing an action yielded a positive reward."
TRAINING,0.24250681198910082,"Pretraining.
To make learning more stable and allow the agent to act more efﬁciently, we found
it beneﬁcial to pretrain the retriever. This minimizes large shifts in the context representations over
the training time. We run a baseline agent (Ammanabrolu & Hausknecht, 2020) to collect instances
of the state graph and actions that yielded positive rewards. Then we train the retriever to encode to
similar representations the contexts for which similar actions (i.e., actions with the same template)
were used. This is achieved using the same contrastive loss deﬁned above."
EXPERIMENTS,0.2452316076294278,"6
EXPERIMENTS"
EXPERIMENTS,0.24795640326975477,"This section provides a detailed evaluation of our approach.
We assess quantitatively the
performance of CBR combined with existing RL approaches and we demonstrate its capability
to improve sample efﬁciency and generalize out of the training distribution. Next, we provide
qualitative insights and examples of the behavior of the model and we perform an ablation study
to understand the role played by the different components of the architecture."
EXPERIMENTAL SETUP,0.2506811989100817,"6.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.25340599455040874,"Agents.
We consider several agents obtained by plugging existing RL methods in the revise step.
We ﬁrst deﬁne two simple approaches: CBR-only, where we augment a random policy with the"
EXPERIMENTAL SETUP,0.2561307901907357,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.25885558583106266,"Easy
Medium
Hard
#Steps
Norm. Score
#Steps
Norm. Score
#Steps
Norm. Score"
EXPERIMENTAL SETUP,0.2615803814713896,"Text
23.83 ± 2.16
0.88 ± 0.04
44.08 ± 0.93
0.60 ± 0.02
49.84 ± 0.38
0.30 ± 0.02
TPC
20.59 ± 5.01
0.89 ± 0.06
42.61 ± 0.65
0.62 ± 0.03
48.45 ± 1.13
0.32 ± 0.04
KG-A2C
22.10 ± 2.91
0.86 ± 0.06
41.61 ± 0.37
0.62 ± 0.03
48.00 ± 0.61
0.32 ± 0.00
BiKE
18.27 ± 1.13
0.94 ± 0.02
39.34 ± 0.72
0.64 ± 0.02
47.19 ± 0.64
0.34 ± 0.02"
EXPERIMENTAL SETUP,0.26430517711171664,"CBR-only
22.13 ± 1.98
0.80 ± 0.05
43.76 ± 1.23
0.62 ± 0.03
48.12 ± 1.30
0.33 ± 0.06
Text + CBR
17.53 ± 3.36
0.93 ± 0.04
39.10 ± 1.77
0.66 ± 0.04
47.11 ± 1.21
0.34 ± 0.02
TPC + CBR
16.81 ± 3.12
0.94 ± 0.03
37.05 ± 1.61
0.67 ± 0.03
47.25 ± 1.56
0.37 ± 0.03
KG-A2C + CBR
15.91 ± 2.52
0.95 ± 0.03
36.13 ± 1.65
0.66 ± 0.05
46.11 ± 1.13
0.40 ± 0.04
BiKE + CBR
15.72 ± 1.15
0.95 ± 0.04
35.24 ± 1.22
0.67 ± 0.03
45.21 ± 0.87
0.42 ± 0.04"
EXPERIMENTAL SETUP,0.2670299727520436,Table 1: Test-set performance for TWC in-distribution games
EXPERIMENTAL SETUP,0.26975476839237056,"Easy
Medium
Hard
#Steps
Norm. Score
#Steps
Norm. Score
#Steps
Norm. Score"
EXPERIMENTAL SETUP,0.2724795640326976,"Text
29.90 ± 2.92
0.78 ± 0.02
45.90 ± 0.22
0.55 ± 0.01
50.00 ± 0.00
0.20 ± 0.02
TPC
27.74 ± 4.46
0.78 ± 0.07
44.89 ± 1.52
0.58 ± 0.01
50.00 ± 0.00
0.19 ± 0.03
KG-A2C
28.34 ± 3.63
0.80 ± 0.07
43.05 ± 2.52
0.59 ± 0.01
50.00 ± 0.00
0.21 ± 0.00
BiKE
25.59 ± 1.92
0.83 ± 0.01
41.01 ± 1.61
0.61 ± 0.01
50.00 ± 0.00
0.23 ± 0.02"
EXPERIMENTAL SETUP,0.27520435967302453,"CBR-only
23.43 ± 2.09
0.80 ± 0.04
44.03 ± 1.75
0.63 ± 0.04
48.71 ± 1.15
0.31 ± 0.03
Text + CBR
20.91 ± 1.72
0.89 ± 0.02
40.32 ± 1.27
0.66 ± 0.04
47.89 ± 0.87
0.32 ± 0.06
TPC + CBR
18.90 ± 1.91
0.92 ± 0.01
37.30 ± 1.00
0.66 ± 0.02
47.54 ± 1.67
0.34 ± 0.03
KG-A2C + CBR
18.21 ± 1.32
0.90 ± 0.02
37.02 ± 1.22
0.68 ± 0.03
47.10 ± 1.12
0.38 ± 0.02
BiKE + CBR
17.15 ± 1.45
0.93 ± 0.03
35.45 ± 1.40
0.67 ± 0.03
45.91 ± 1.32
0.40 ± 0.03"
EXPERIMENTAL SETUP,0.2779291553133515,Table 2: Test-set performance for TWC out-of-distribution games
EXPERIMENTAL SETUP,0.28065395095367845,"CBR approach, and Text + CBR, which relies on the CBR method combined with a simple GRU-
based policy network that consumes as input the textual observation from the game. Next, we select
three recently proposed TBG approaches: Text+Commonsense (TPC) (Murugesan et al., 2021b),
KG-A2C (Ammanabrolu & Hausknecht, 2020), and BiKE (Murugesan et al., 2021c), to create the
TPC + CBR, KG-A2C + CBR and BiKE + CBR agents. We treat the original agents as baselines."
EXPERIMENTAL SETUP,0.28337874659400547,"Datasets.
We empirically verify the efﬁcacy of our approach on TextWorld Commonsense
(TWC) (Murugesan et al., 2021b) and Jericho (Hausknecht et al., 2020). Jericho is a well-known and
challenging learning environment including 33 interactive ﬁction games. TWC is an environment
which builds on TextWorld (Côté et al., 2018) and provides a suite of games requiring commonsense
knowledge. TWC allows agents to be tested on two settings: the in-distribution games, where the
objects that the agent encounters in the test set are the same as the objects in the training set, and the
out-of-distribution games which have no entity in common with the training set. For each of these
settings, TWC provides three difﬁculty levels: easy, medium, and hard."
EXPERIMENTAL SETUP,0.28610354223433243,"Evaluation metrics.
Following Murugesan et al. (2021b), we evaluate the agents on TWC based
on the number of steps (#Steps) required to achieve the goal (lower is better) and the normalized
cumulative reward (Norm. Score) obtained by the agent (larger is better). On Jericho, we follow
previous work (Hausknecht et al., 2020; Guo et al., 2020; Ammanabrolu & Hausknecht, 2020) and
we report the average score achieved over the last 100 training episodes."
RESULTS ON TEXTWORLD COMMONSENSE,0.2888283378746594,"6.2
RESULTS ON TEXTWORLD COMMONSENSE"
RESULTS ON TEXTWORLD COMMONSENSE,0.29155313351498635,"Table 1 reports the results on TWC for the in-distribution set of games. Overall, we observe that CBR
consistently improves the performance of all the baselines. The performance boost is large enough
that even a simple method as Text + CBR outperforms all considered baselines except BiKE."
RESULTS ON TEXTWORLD COMMONSENSE,0.29427792915531337,"Out-of-distribution generalization.
CBR’s ability to retrieve similar cases should allow our
method to better generalize to new and unseen problems. We test this hypothesis on the out-of-"
RESULTS ON TEXTWORLD COMMONSENSE,0.2970027247956403,Published as a conference paper at ICLR 2022
RESULTS ON TEXTWORLD COMMONSENSE,0.2997275204359673,"0
20
40
60
80
Episodes 0.3 0.4 0.5 0.6 0.7 0.8"
RESULTS ON TEXTWORLD COMMONSENSE,0.3024523160762943,Normalized Score
RESULTS ON TEXTWORLD COMMONSENSE,0.30517711171662126,"TPC
KG-A2C
BiKE
TPC + CBR
KG-A2C + CBR
BiKE + CBR"
RESULTS ON TEXTWORLD COMMONSENSE,0.3079019073569482,"0
20
40
60
80
Episodes 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70"
RESULTS ON TEXTWORLD COMMONSENSE,0.3106267029972752,Normalized Score
RESULTS ON TEXTWORLD COMMONSENSE,0.3133514986376022,"0
20
40
60
80
Episodes 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50"
RESULTS ON TEXTWORLD COMMONSENSE,0.31607629427792916,Normalized Score
RESULTS ON TEXTWORLD COMMONSENSE,0.3188010899182561,"0
20
40
60
80
Episodes 15 20 25 30 35 40 45"
RESULTS ON TEXTWORLD COMMONSENSE,0.3215258855585831,Number of Steps
RESULTS ON TEXTWORLD COMMONSENSE,0.3242506811989101,"0
20
40
60
80
Episodes 34 36 38 40 42 44 46 48 50"
RESULTS ON TEXTWORLD COMMONSENSE,0.32697547683923706,Number of Steps
RESULTS ON TEXTWORLD COMMONSENSE,0.329700272479564,"0
20
40
60
80
Episodes 44 45 46 47 48 49 50"
RESULTS ON TEXTWORLD COMMONSENSE,0.33242506811989103,Number of Steps
RESULTS ON TEXTWORLD COMMONSENSE,0.335149863760218,"Figure 2: Performance on TWC (showing mean and standard deviation averaged over 5 runs) for the three
difﬁculty levels: easy (left), medium (middle), Hard (right) using normalized score and number of steps."
RESULTS ON TEXTWORLD COMMONSENSE,0.33787465940054495,"distribution games in TWC. The results of this experiment are reported in Table 2. We notice that
all existing approaches fail to generalize out of the training distribution and suffer a substantial drop
in performance in this setting. However, when coupled with CBR, the drop is minor (on average
6% with CBR vs 35% without on the hard level). Interestingly, even the CBR-only agent achieves
competitive results compared to the top-performing baselines."
RESULTS ON TEXTWORLD COMMONSENSE,0.3405994550408719,"Sample efﬁciency.
Another key beneﬁt of our approach comes as better sample efﬁciency. With
its ability to explicitly store prior solutions effectively, CBR allows existing algorithms to learn
faster. Figure 2 shows the learning curves for our best agents and the corresponding baselines. The
plots report the performance of the agent over the training episodes, both in terms of the number of
steps and the normalized score. Overall, we observe that the CBR agents obtain faster convergence
to their counterparts on all difﬁculty levels."
PERFORMANCE ON THE JERICHO GAMES,0.34332425068119893,"6.3
PERFORMANCE ON THE JERICHO GAMES"
PERFORMANCE ON THE JERICHO GAMES,0.3460490463215259,"We evaluate our best performing variant from the experiments on TWC (BiKE + CBR) against
existing approaches on the 33 games in the Jericho environment. We compare our approach against
strong baselines, including TDQN (Hausknecht et al., 2020), DRRN (He et al., 2016), KG-A2C
(Ammanabrolu & Hausknecht, 2020), MPRC-DQN (Guo et al., 2020), and RC-DQN (Guo et al.,
2020). The same experimental setting and handicaps as the baselines are used, as we train for
100 000 steps and we assume access to valid actions. Table 3 summarizes the results of the Jericho
games. We observe that our CBR agent achieves comparable or better performance than any baseline
on 24 (73%) of the games, strictly outperforming all the other agents in 18 games."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.34877384196185285,"6.4
QUALITATIVE ANALYSIS AND ABLATION STUDIES"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.35149863760217986,"Insights on the model.
Figure 3 provides two examples showing the BiKE + CBR agent
interacting with the zork1 game. In the example on top, the agent retrieves an experience that
can be successfully reused and turned into a valid action at the current time step. The heat maps
visualize the value of the context similarity function deﬁned in Section 4 for the top entries in the
memory. In the negative example at the bottom instead, the agent retrieves an action that is not
useful and needs to fall back to the neural policy π. Figure 4 (top) shows the fraction of times that
actions retrieved from the memory are reused successfully in the TWC games. We observe that,
both for in-distribution and out-of-distribution games, the trained agent relies on CBR from 60% to
approximately 70% of the times. Figure 4 (bottom) further shows the fraction of times that the neural
agent would have been able to select a rewarded action as well, when the CBR reuses a successful
action. The plot shows that, for the out-of-distribution games, the neural agent would struggle to
select good actions when the CBR is used."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3542234332425068,Published as a conference paper at ICLR 2022
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3569482288828338,"Game
Human
(max)
Human
(Walkthrough-100)
TDQN
DRRN
KG-A2C
MPRC-DQN
RC-DQN
BiKE + CBR"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.35967302452316074,"905
1
1
0
0
0
0
0
0
acorncourt
30
30
1.6
10
0.3
10
10
12.2
adventureland
100
42
0
20.6
0
24.2
21.7
27.3
afﬂicted
75
75
1.3
2.6
–
8
8
3.2
awaken
50
50
0
0
0
0
0
0
detective
360
350
169
197.8
207.9
317.7
291.3
326.1
dragon
25
25
-5.3
-3.5
0
0.04
4.84
8.3
inhumane
90
70
0.7
0
3
0
0
24.2
library
30
30
6.3
17
14.3
17.7
18.1
22.3
moonlit
1
1
0
0
0
0
0
0
omniquest
50
50
16.8
10
3
10
10
17.2
pentari
70
60
17.4
27.2
50.7
44.4
43.8
52.1
reverb
50
50
0.3
8.2
–
2
2
6.5
snacktime
50
50
9.7
0
0
0
0
22.1
temple
35
20
7.9
7.4
7.6
8
8
7.8
ztuu
100
100
4.9
21.6
9.2
85.4
79.1
87.2
advent
350
113
36
36
36
63.9
36
62.1
balances
51
30
4.8
10
10
10
10
11.9
deephome
300
83
1
1
1
1
1
1
gold
100
30
4.1
0
–
0
0
2.1
jewel
90
24
0
1.6
1.8
4.46
2
6.4
karn
170
40
0.7
2.1
0
10
10
0
ludicorp
150
37
6
13.8
17.8
19.7
17
23.8
yomomma
35
34
0
0.4
–
1
1
1
zenon
20
20
0
0
3.9
0
0
4.1
zork1
350
102
9.9
32.6
34
38.3
38.8
44.3
zork3
7
3
0
0.5
0.1
3.63
2.83
3.2
anchor
100
11
0
0
0
0
0
0
enchanter
400
125
8.6
20
12.1
20
20
36.3
sorcerer
400
150
5
20.8
5.8
38.6
38.3
24.5
spellbrkr
600
160
18.7
37.8
21.3
25
25
41.2
spirit
250
8
0.6
0.8
1.3
3.8
5.2
4.2
tryst205
350
50
0
9.6
–
10
10
13.4"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.36239782016348776,"Best agent
6 (18%)
6 (18%)
5 (15%)
12 (36%)
10 (30%)
24 (73%)"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3651226158038147,"Table 3: Average raw score on the Jericho games. We denote with colors the difﬁculty of the games (green for
possible games, yellow for difﬁcult games and red for extreme games). The last row reports the fraction and the
absolute number of games where an agent achieves the best score. We additionally report human performance
(Human – max) and the 100-step results from a human-written walkthrough (Human – Walkthrough 100).
Results are taken from the original papers or “−” is used if a result was not reported. 0.0 0.2 0.4 0.6 0.8 1.0"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3678474114441417,Take emerald
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.37057220708446864,Take diamond
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.37329700272479566,Put jewels in case
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3760217983651226,Observations & Actions
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3787465940054496,"> Take diamond
[…]
Living Room
Above the trophy case hangs an 
elvish sword of great antiquity.
[…]"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3814713896457766,Memory
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.38419618528610355,Put emerald in case
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3869209809264305,Action reuse:  Put emerald in case          Put diamond in case 0.0 0.2 0.4 0.6 0.8 1.0
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3896457765667575,Take emerald
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3923705722070845,Go north
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.39509536784741145,Take jewels
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.3978201634877384,Observations & Actions
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.40054495912806537,"> Climb tree
[…]
Beside you on the branch is a 
small bird's nest. In the bird's nest 
is a large egg encrusted with 
precious jewels…"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4032697547683924,Memory
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.40599455040871935,Go west
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4087193460490463,"Action reuse:  No valid action, fallback to neural policy"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4114441416893733,"Figure 3: Examples from the zork1 game, showing the
content of the memory and the context similarities, in
a situation where the agent is able to reuse a previous
experience and in a case where the revise step is needed."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4141689373297003,Non-rewarded action IN OUT IN OUT IN OUT
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.41689373297002724,"Easy
Medium
Hard IN OUT IN OUT IN OUT"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4196185286103542,"Easy
Medium
Hard"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4223433242506812,Rewarded Action
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4250681198910082,"Neural agent
Action reuse"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.42779291553133514,"Figure 4: Fraction of times that a retrieved action is
reused successfully on TWC (top). Fraction of times
that the neural agent would have picked a rewarded
action when CBR is used successfully (bottom)."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4305177111716621,"Ablation studies.
In order to understand the role of the main modules of our CBR agent, we
designed some ablation studies. First, instead of using the seeded GAT, we deﬁne the context of a
state-action pair context(st, at) as just one of the entities that at is applied to. This deﬁnition suits
well the TWC games because rewarded actions are always applied to one target object and a location
for that object (see Appendix G for details). Note that, since the set of entities is discrete, no context
quantization is needed. We report the performance of the resulting BiKE + CBR (w/o GAT) agent in"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4332425068119891,Published as a conference paper at ICLR 2022
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4359673024523161,"Easy
Medium
Hard IN"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.43869209809264303,"BiKE + CBR (w/o GAT)
16.32 ± 1.10
36.13 ± 1.40
45.72 ± 0.63
BiKE + CBR (w/o VQ)
22.67 ± 1.23
43.18 ± 2.10
49.21 ± 0.55 OUT"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.44141689373297005,"BiKE + CBR (w/o GAT)
18.15 ± 1.51
37.10 ± 1.41
46.70 ± 0.71
BiKE + CBR (w/o VQ)
27.75 ± 2.11
44.55 ± 1.67
50.00 ± 0.00"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.444141689373297,"Table 4: Results of the ablation study on TWC, evaluated based on the
number of steps (#Steps) to solve the games."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.44686648501362397,"0
20
40
60
80
100
Episodes 10
0 10
1 10
2 10
3 10
4"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.44959128065395093,Memory size OOM
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.45231607629427795,"BiKE + CBR
BiKE + CBR (w/o VQ)"
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.4550408719346049,"Figure 5: Number of entries in
the memory over training."
QUALITATIVE ANALYSIS AND ABLATION STUDIES,0.45776566757493187,"Table 4. The results show that CBR on TWC is effective even with this simple context deﬁnition, but
the lower performance of the agent demonstrates the advantage of incorporating additional context
information. Finally, we investigate the role played by vector quantization, by experimenting with
an agent (BiKE + CBR w/o VQ) that stores the continuous context representations. In general,
this poses scalability challenges, but since TWC has only 5 games per difﬁculty level, each with a
small number of objects, we were able to evaluate the performance of this agent on the three levels
separately. The results, reported in Table 4, show that this agent performs much worse than the other
CBR implementations. This happens because storing continuous representations over the training
results in duplicated entries in the memory and makes it harder to retrieve meaningful experiences.
Figure 5 demonstrates how the size (number of entries) in the memory grows over the training time.
In this experiment, we trained the agent on all difﬁculty levels at the same time, resulting in the
implementation running out of memory (OOM) on the GPU. More ablation studies are reported in
Appendix C, D, E, F, and G."
RELATED WORK,0.4604904632152589,"7
RELATED WORK"
RELATED WORK,0.46321525885558584,"Text-based RL.
TBGs are a rich domain for studying grounded language understanding and how
text information can be utilized in control. Prior work has explored text-based RL to learn strategies
for multi-user dungeon games (Narasimhan et al., 2015) and other environments (Branavan et al.,
2012). Zahavy et al. (2018) proposed the Action-Elimination Deep Q-Network (AE-DQN), which
learns to predict invalid actions in the text-adventure game Zork.
Recently, Côté et al. (2018)
introduced TextWorld, a sandbox learning environment for training and evaluating RL agents
on text-based games. On the same line, Murugesan et al. (2021b) introduced TWC that requires
agents with commonsense knowledge (Murugesan et al., 2020; Basu et al., 2021). The LeDeepChef
system (Adolphs & Hofmann, 2019) achieved good results on the First TextWord Problems (Trischler
et al., 2019) by supervising the model with entities from FreeBase, allowing the agent to
generalize to unseen objects. A recent line of work learns symbolic (typically graph-structured)
representations of the agent’s belief. Notably, Ammanabrolu & Riedl (2019) proposed KG-DQN
and Adhikari et al. (2020) proposed GATA. We also use graphs to model the state of the game."
RELATED WORK,0.4659400544959128,"Case-based reasoning in RL.
In the context of RL, CBR has been used to speed up and improve
transfer learning in heuristic-based RL. Celiberto Jr et al. (2011) and Bianchi et al. (2018) have
shown that cases collected from one domain can be used as heuristics to achieve faster convergence
when learning an RL algorithm on a different domain. In contrast to these works, we present a
scalable way of using CBR alongside deep RL methods in settings with very large state spaces.
More recently, CBR has been successfully applied in the ﬁeld of knowledge-based reasoning. Das
et al. (2020) and Das et al. (2021) show that CBR can effectively learn to generate new logical
reasoning chains from prior cases, to answer questions on knowledge graphs."
CONCLUSION AND FUTURE WORK,0.46866485013623976,"8
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.4713896457765668,"In this work, we proposed new agents for TBGs using case-based reasoning. In contrast to expensive
deep RL approaches, CBR simply builds a collection of its past experiences and uses the ones
relevant to the current situation to decide upon its next action in the game.
Our experiments
showed that CBR when combined with existing RL agents can make them more efﬁcient and aid
generalization in out-of-distribution settings. Even though CBR was quite successful in the TBGs
explored in our work, future work is needed to understand the limitations of CBR in such settings."
CONCLUSION AND FUTURE WORK,0.47411444141689374,Published as a conference paper at ICLR 2022
CONCLUSION AND FUTURE WORK,0.4768392370572207,ACKNOWLEDGEMENTS
CONCLUSION AND FUTURE WORK,0.47956403269754766,"This work was funded in part by an IBM fellowship to SD and in part by a small project grant to MS
from the Hasler Foundation."
REFERENCES,0.4822888283378747,REFERENCES
REFERENCES,0.48501362397820164,"Agnar Aamodt and Enric Plaza.
Case-based reasoning: Foundational issues, methodological
variations, and system approaches. AI communications, 7(1):39–59, 1994."
REFERENCES,0.4877384196185286,"Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau,
Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. Learning
dynamic knowledge graphs to generalize on text-based games. arXiv preprint arXiv:2002.09127,
2020."
REFERENCES,0.4904632152588556,"Leonard Adolphs and Thomas Hofmann.
Ledeepchef: Deep reinforcement learning agent for
families of text-based games. ArXiv, abs/1909.01646, 2019."
REFERENCES,0.49318801089918257,"Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for
natural language action spaces. arXiv preprint arXiv:2001.08837, 2020."
REFERENCES,0.49591280653950953,"Prithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep
reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers), pp. 3557–3565, 2019."
REFERENCES,0.4986376021798365,"Prithviraj Ammanabrolu, Ethan Tien, Matthew J. Hausknecht, and Mark O. Riedl.
How to
avoid being eaten by a grue: Structured exploration strategies for textual worlds.
CoRR,
abs/2006.07409, 2020. URL https://arxiv.org/abs/2006.07409."
REFERENCES,0.5013623978201635,"Mattia Atzeni and Maurizio Atzori.
Translating natural language to code: An unsupervised
ontology-based approach. In First IEEE International Conference on Artiﬁcial Intelligence and
Knowledge Engineering, AIKE 2018, Laguna Hills, CA, USA, September 26-28, 2018, pp. 1–8.
IEEE Computer Society, 2018. doi: 10.1109/AIKE.2018.00009. URL https://doi.org/
10.1109/AIKE.2018.00009."
REFERENCES,0.5040871934604905,"Mattia Atzeni, Jasmina Bogojeska, and Andreas Loukas. SQALER: Scaling question answering
by decoupling multi-hop and logical reasoning. In A. Beygelzimer, Y. Dauphin, P. Liang, and
J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL
https://openreview.net/forum?id=2CQQ_C1i0b."
REFERENCES,0.5068119891008175,"Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and
Aaron C. Courville. Systematic generalization: What is required and can it be learned? In ICLR
2019, 2019."
REFERENCES,0.5095367847411444,"Dana H. Ballard. An introduction to natural computation. Complex adaptive systems. MIT Press,
2000. ISBN 978-0-262-52258-8."
REFERENCES,0.5122615803814714,"Kinjal Basu, Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Tim
Klinger, Murray Campbell, Mrinmaya Sachan, and Gopal Gupta.
A hybrid neuro-symbolic
approach for text-based games using inductive logic programming. In Combining Learning and
Reasoning: Programming Languages, Formalisms, and Representations, 2021."
REFERENCES,0.5149863760217984,"Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation.
CoRR, abs/1308.3432, 2013.
URL
http://arxiv.org/abs/1308.3432."
REFERENCES,0.5177111716621253,"Reinaldo AC Bianchi, Paulo E Santos, Isaac J Da Silva, Luiz A Celiberto, and Ramon Lopez
de Mantaras. Heuristically accelerated reinforcement learning by means of case-based reasoning
and transfer learning. Journal of Intelligent & Robotic Systems, 91(2):301–312, 2018."
REFERENCES,0.5204359673024523,"SRK Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a monte-
carlo framework. Journal of Artiﬁcial Intelligence Research, 43:661–704, 2012."
REFERENCES,0.5231607629427792,Published as a conference paper at ICLR 2022
REFERENCES,0.5258855585831063,"Luiz A Celiberto Jr, Jackson P Matsuura, Ramon Lopez De Mantaras, and Reinaldo AC Bianchi.
Using cases as heuristics in reinforcement learning: a transfer learning application. In Twenty-
Second International Joint Conference on Artiﬁcial Intelligence, 2011."
REFERENCES,0.5286103542234333,"Ting Chen, Martin Renqiang Min, and Yizhou Sun. Learning k-way d-dimensional discrete codes for
compact embedding representations. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research,
pp. 853–862. PMLR, 2018. URL http://proceedings.mlr.press/v80/chen18g.
html."
REFERENCES,0.5313351498637602,"Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James
Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler.
Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018."
REFERENCES,0.5340599455040872,"Rajarshi Das, Ameya Godbole, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. A
simple approach to case-based reasoning in knowledge bases. arXiv preprint arXiv:2006.14198,
2020."
REFERENCES,0.5367847411444142,"Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay-Yoon Lee, Lizhen
Tan, Lazaros Polymenakos, and Andrew McCallum. Case-based reasoning for natural language
queries over knowledge bases. arXiv preprint arXiv:2104.08762, 2021."
REFERENCES,0.5395095367847411,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding.
In Jill Burstein, Christy Doran, and
Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–
4186. Association for Computational Linguistics, 2019. URL https://www.aclweb.org/
anthology/N19-1423/."
REFERENCES,0.5422343324250681,"Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interactive
ﬁction game playing as multi-paragraph reading comprehension with reinforcement learning. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 7755–7765, Online, November 2020. Association for Computational Linguistics.
doi:
10.18653/v1/2020.emnlp-main.624.
URL https://aclanthology.org/2020.
emnlp-main.624."
REFERENCES,0.5449591280653951,"Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping.
In 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR 2006), 17-22 June 2006, New York, NY, USA, pp. 1735–1742. IEEE Computer
Society, 2006. doi: 10.1109/CVPR.2006.100. URL https://doi.org/10.1109/CVPR.
2006.100."
REFERENCES,0.547683923705722,"Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive
ﬁction games: A colossal adventure.
In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pp. 7903–7910, 2020."
REFERENCES,0.5504087193460491,"Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep
reinforcement learning with a natural language action space. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,
Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.
18653/v1/p16-1153. URL https://doi.org/10.18653/v1/p16-1153."
REFERENCES,0.553133514986376,"Pavan Kapanipathi, Veronika Thost, Siva Sankalp Patel, Spencer Whitehead, Ibrahim Abdelaziz,
Avinash Balakrishnan, Maria Chang, Kshitij Fadnis, Chulaka Gunasekara, Bassem Makni,
Nicholas Mattei, Kartik Talamadupula, and Achille Fokoue. Infusing knowledge into the textual
entailment task using graph convolutional networks. AAAI, 2020."
REFERENCES,0.555858310626703,"Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin,
Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, Dmitry Tsarkov, Xiao
Wang, Marc van Zee, and Olivier Bousquet.
Measuring compositional generalization:
A"
REFERENCES,0.55858310626703,Published as a conference paper at ICLR 2022
REFERENCES,0.5613079019073569,"comprehensive method on realistic data.
In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=SygcCnNKwr."
REFERENCES,0.5640326975476839,"Janet L Kolodner. Reconstructive memory: A computer model. Cognitive science, 7(4):281–328,
1983."
REFERENCES,0.5667574931880109,"Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi:
10.1017/S0140525X16001837."
REFERENCES,0.5694822888283378,"K. Murugesan, Subhajit Chaudhury, and Kartik Talamadupula. Eye of the beholder: Improved
relation generalization for text-based reinforcement learning agents.
ArXiv, abs/2106.05387,
2021a."
REFERENCES,0.5722070844686649,"Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, and
Kartik Talamadupula. Enhancing text-based reinforcement learning agents with commonsense
knowledge. CoRR, abs/2005.00811, 2020. URL https://arxiv.org/abs/2005.00811."
REFERENCES,0.5749318801089919,"Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel,
Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based rl
agents with commonsense knowledge: New challenges, environments and baselines. In Thirty
Fifth AAAI Conference on Artiﬁcial Intelligence, 2021b."
REFERENCES,0.5776566757493188,"Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya
Sachan, and Murray Campbell. Efﬁcient text-based reinforcement learning by jointly leveraging
state and commonsense graph representations. In Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 2: Short Papers), pp. 719–725, 2021c."
REFERENCES,0.5803814713896458,"Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based
games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015."
REFERENCES,0.5831062670299727,"Mrinmaya Sachan. Knowledge graph embedding compression. In Dan Jurafsky, Joyce Chai, Natalie
Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2681–2691. Association
for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.238. URL https://
doi.org/10.18653/v1/2020.acl-main.238."
REFERENCES,0.5858310626702997,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://
arxiv.org/abs/1910.01108."
REFERENCES,0.5885558583106267,"Roger C Schank. Dynamic memory: A theory of reminding and learning in computers and people.
cambridge university press, 1983."
REFERENCES,0.5912806539509536,"Ishika Singh, Gargi Singh, and Ashutosh Modi. Pre-trained language models as prior knowledge for
playing text-based games. CoRR, abs/2107.08408, 2021. URL https://arxiv.org/abs/
2107.08408."
REFERENCES,0.5940054495912807,"Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and
William Cohen. Open domain question answering using early fusion of knowledge bases and text.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
pp. 4231–4242, 2018."
REFERENCES,0.5967302452316077,"Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 0262039249."
REFERENCES,0.5994550408719346,"Adam Trischler, Marc-Alexandre Côté, and Pedro Lima. First TextWorld Problems, the competition:
Using text-based games to advance capabilities of AI agents, 2019."
REFERENCES,0.6021798365122616,"Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605, 2008.
URL http://www.jmlr.org/papers/v9/
vandermaaten08a.html."
REFERENCES,0.6049046321525886,Published as a conference paper at ICLR 2022
REFERENCES,0.6076294277929155,"Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ."
REFERENCES,0.6103542234332425,"Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. Deep reinforcement learning
with transformers for text adventure games. In IEEE Conference on Games, CoG 2020, Osaka,
Japan, August 24-27, 2020, pp. 65–72. IEEE, 2020. doi: 10.1109/CoG47356.2020.9231622.
URL https://doi.org/10.1109/CoG47356.2020.9231622."
REFERENCES,0.6130790190735694,"Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not
to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information
Processing Systems, pp. 3562–3573, 2018."
REFERENCES,0.6158038147138964,Published as a conference paper at ICLR 2022
REFERENCES,0.6185286103542235,"A
TRAINING DETAILS"
REFERENCES,0.6212534059945504,"All agents used in our experiments are trained with an Advantage Actor Critic (A2C) method
according to the general scheme deﬁned below. We use the n-step temporal difference method
(Sutton & Barto, 2018) to compute the return R(st, at) of a single time step t in a session of length
T as:"
REFERENCES,0.6239782016348774,"R(st, at) = γT −tV (sT ) +"
REFERENCES,0.6267029972752044,"T −t
X"
REFERENCES,0.6294277929155313,"i=0
γir(st+i, at+i),"
REFERENCES,0.6321525885558583,"where V (sT ) denotes the value of sT computed by the critic network, γ is the discount factor, and
r is the reward function. We then compute the advantage A(st, at) = R(st, at) −V (st). The ﬁnal
objective term consists of four separate objectives. First, we have L(t)
π that denotes the objective of
the policy, which tries to maximize the the advantage A:"
REFERENCES,0.6348773841961853,"L(t)
π = −A(st, a⋆
t ) log π(a⋆
t |st).
We then add the objective of the critic as:"
REFERENCES,0.6376021798365122,"L(t)
v
= 1"
REFERENCES,0.6403269754768393,"2 (R(st, a⋆
t ) −V (st))2 ."
REFERENCES,0.6430517711171662,"L(t)
v
encourages the value of the critic V to better estimate the reward R by reducing the mean
squared error between them. To prevent the policy from assigning a large weight on a single action,
we perform entropy regularization by introducing an additional term:"
REFERENCES,0.6457765667574932,"L(t)
e
= η ·
X"
REFERENCES,0.6485013623978202,"at∈At
π(at|st) · log π(at|st)."
REFERENCES,0.6512261580381471,"The η parameter helps balance the exploration-exploitation trade-off for the policy. The sum of the
above objectives deﬁnes the loss when the neural agent π is used to select the action a⋆
t . In case
actions are reused from the memory, then we use the contrastive loss as discussed in Section 5,
namely we compute the loss as:"
REFERENCES,0.6539509536784741,"L(t)
r
= 1"
REFERENCES,0.6566757493188011,"2(1 −yt)(1 −sim(ct, cM
t ))2 + 1"
REFERENCES,0.659400544959128,"2yt max{0, µ −1 + sim(ct, cM
t )}2."
REFERENCES,0.662125340599455,"B
ENHANCING BASELINE AGENTS WITH CBR ON JERICHO"
REFERENCES,0.6648501362397821,"In this section, we report additional experimental results on a subset of the Jericho games, in order
to show the performance improvement obtained by different baseline agents when enhanced with
case-based reasoning. Table 5 shows the results obtained when coupling the agents described in
Section 6.1 with CBR. Similarly to what we discussed for TWC in Section 6.2, we observe that
CBR consistently improves the performance of all the agents. The best performing agent is BiKE +
CBR, which is the agent that we evaluated on the complete set of games in Section 6.3."
REFERENCES,0.667574931880109,"Game
KG-A2C
KG-A2C + CBR
Text
Text + CBR
TPC
TPC + CBR
BiKE
BiKE + CBR"
REFERENCES,0.670299727520436,"detective
207.9
255.6
205.8
242.3
245.6
315.1
278.2
326.1
inhumane
3
15.6
1.1
14.5
4.5
18.3
9.2
24.2
snacktime
0
15.5
8.1
9.8
15.7
19.3
18.8
22.1
karn
0
0
0
0
0
0
0
0
zork1
34
34.2
31.5
38.2
36
39.2
39.5
44.3
zork3
0.1
1.7
0
1.6
1.7
1.9
2.5
3.2
enchanter
12.1
26.2
10.1
26.4
13.2
24.5
19.7
36.3
spellbrkr
21.3
36.1
20.4
33.2
38.1
40.2
38.8
41.2"
REFERENCES,0.6730245231607629,"Table 5: Additional results on Jericho showing the performance improvement obtained by enhancing several
baseline agents with CBR."
REFERENCES,0.6757493188010899,"C
ABLATION STUDY ON MEMORY ACCESS"
REFERENCES,0.6784741144414169,"In this section, we investigate the effectiveness of our approach based on vector quantization for
efﬁcient memory access. We consider several variants, where VQ is either dropped completely or
replaced with other techniques."
REFERENCES,0.6811989100817438,Published as a conference paper at ICLR 2022
REFERENCES,0.6839237057220708,"C.1
ALTERNATIVES TO VECTOR QUANTIZATION"
REFERENCES,0.6866485013623979,"All the agents we consider are variants of the best-performing agent on Jericho and TWC, namely
the BiKE + CBR agent. We experiment with the following techniques."
REFERENCES,0.6893732970027248,"• BiKE + CBR (w/o VQ) completely removes the vector quantization and stores the continuous
context representations as keys in the case memory. In order to make this feasible, for the
experiments on Jericho, we limit the size of the memory to the 5000 most recent entries.
• BiKE + CBR (RP) relies on random projection (RP) in order to reduce the dimensionality of the
context representations stored by the CBR approach. In this case, each context representation is
projected into a p-dimensional space using a random matrix R ∈Rp×d, with components drawn
from a normal distribution N(0, 1"
REFERENCES,0.6920980926430518,p) with mean 0 and standard deviation 1 p.
REFERENCES,0.6948228882833788,"• BiKE + CBR (SRP) employs sign random projection (SRP) in order to obtain a discrete
representation of the context. In this variant, after projection to a p-dimensional space, the
context representations are discretized by applying an element-wise sign function.
• BiKE + CBR (LSH) replaces vector quantization with locality sensitive hashing (LSH). In this
case, context representations are converted into h-bit hash codes for l different hash tables. The
retrieve step selects the representation with the highest cosine similarity to the query context
encoding, among the vectors falling in the same bucket."
REFERENCES,0.6975476839237057,"C.2
RESULTS AND DISCUSSION"
REFERENCES,0.7002724795640327,"Table 6 shows the scores achieved by each variant of the BiKE + CBR method on a subset of the
Jericho games. We notice that the approaches relying on continuous context representations (w/o
VQ and RP) perform poorly compared to the others and to the plain BiKE agent. This conﬁrms our
hypothesis that CBR needs discrete context representations to make the retrieve step more stable."
REFERENCES,0.7029972752043597,"The BiKE + CBR (LSH) agent, which relies on locality sensitive hashing, achieves competitive
results and consistently outperforms the BiKE agent. This shows that locality sensitive hashing can
be a viable alternative to vector quantization. However, we observe that our BiKE + CBR agent,
based on vector quantization, achieves better results on almost all the games, conﬁrming the beneﬁt
of learning the discrete representation as well. Note also that LSH requires storing the complete
continuous representations to be able to detect false positives, namely contexts with the same hash
code as the query vector, but low cosine similarity. This makes this alternative less memory efﬁcient
compared to our implementation based on VQ."
REFERENCES,0.7057220708446866,"Game
BiKE + CBR (w/o VQ)
BiKE + CBR (RP)
BiKE + CBR (SRP)
BiKE + CBR (LSH)
BiKE + CBR"
REFERENCES,0.7084468664850136,"detective
205.2
203.1
223.2
319.1
326.1
inhumane
1.5
3.2
1.1
20.1
24.2
snacktime
9.1
14.3
13.2
20.3
22.1
karn
0
0
0
0
0
zork1
31.5
36.3
40.5
41.2
44.3
zork3
0
2.7
2.7
3.6
3.2
enchanter
10.2
9.2
10.6
35.3
36.3
spellbrkr
21.3
23.8
30.8
39.3
41.2"
REFERENCES,0.7111716621253406,"Table 6: Ablation study showing the results obtained on a subset of the Jericho games when the vector
quantization is removed (w/o VQ) or replaced with random projection (RP), sign random projection (SRP)
or locality sensitive hashing (LSH)."
REFERENCES,0.7138964577656676,"D
ABLATION STUDY ON THE RETAIN MODULE"
REFERENCES,0.7166212534059946,"In this section, we evaluate different alternatives to select which actions should be retained in the
memory of the agent."
REFERENCES,0.7193460490463215,"D.1
ALTERNATIVES FOR THE RETAIN MODULE"
REFERENCES,0.7220708446866485,"In our main experiments described in Section 6, the retain module has been implemented to store
the last k context-action pairs in the memory, whenever the reward obtained by the agent is positive."
REFERENCES,0.7247956403269755,Published as a conference paper at ICLR 2022
REFERENCES,0.7275204359673024,"For Jericho, we set k = 3, as speciﬁed in Appendix J. However, other design choices are possible.
We consider the following variants of the BiKE + CBR agent."
REFERENCES,0.7302452316076294,"• BiKE + CBR (rewarded action only) retains only the rewarded context-action pair, without
sampling any of the previous actions. This variant is a simple implementation that works well
in practice, but may fail to identify useful actions that were not rewarded."
REFERENCES,0.7329700272479565,"• BiKE + CBR (TD error) samples previous context-action pairs based on the temporal
difference (TD) error. In details, the agent still retains k context-action pairs: whenever the
reward rt at time step t is positive, the rewarded context-action pair (c⋆
t , a⋆
t ) is retained, together
with k −1 additional pairs sampled from the current trajectory, with a probability proportional
to the TD error."
REFERENCES,0.7356948228882834,"D.2
RESULTS AND DISCUSSION"
REFERENCES,0.7384196185286104,"Table 7 shows the scores obtained by the agents on a subset of the Jericho games. Our main
implementation storing the last k = 3 actions achieves the best results, whereas the BiKE +
CBR (rewarded action only) agent performs slightly worse than the other two implementations.
The agent based on the TD error achieves competitive results and a state-of-the-art score on the
snacktime game. We observe that all variants perform well in practice and achieve overall better
results than the baselines in Table 3."
REFERENCES,0.7411444141689373,"Game
BiKE + CBR (rewarded action only)
BiKE + CBR (TD error)
BiKE + CBR"
REFERENCES,0.7438692098092643,"detective
324.1
324.8
326.1
inhumane
20.1
23.5
24.2
snacktime
20.3
23.4
22.1
karn
0
0
0
zork1
40.2
42.4
44.3
zork3
3.2
3.2
3.2
enchanter
35.3
34.1
36.3
spellbrkr
40.3
40.8
41.2"
REFERENCES,0.7465940054495913,"Table 7: Ablation study showing the results obtained on a subset of the Jericho games when only the context-
action pair achieving positive reward is retained (rewarded action only), when context-action pairs are sampled
using the TD error (TD error), and when the last 3 pairs are retained (BiKE + CBR)."
REFERENCES,0.7493188010899182,"E
TRAINING THE AGENT AND THE RETRIEVER JOINTLY"
REFERENCES,0.7520435967302452,"In this experiment, we try to understand if training the neural agent and the CBR retriever jointly
would improve upon our choice of just training the two networks separately."
REFERENCES,0.7547683923705722,"Our implementation works by training the neural agent π and the CBR retriever separately with
different objectives, as described in Appendix A. However, we can make the neural agent aware of
the CBR retriever and train the two networks jointly. In this case, we need to modify the architecture
of the neural agent π in order to take into account the action candidates ˜
At produced by the CBR. In
details, we compute an action selector by attention between the valid actions At and the candidates
˜
At and we concatenate this action selector to the vector used by the neural agent to score admissible
actions. Then, the agent and the retriever can be trained jointly, optimizing an objective given by the
sum of all the losses in Section A."
REFERENCES,0.7574931880108992,"Table 8 and 9 show the results obtained by the baseline agents when they are trained jointly with
the CBR retriever. On TWC, we observe that the joint variants of the agents achieve comparable
results with their counterparts in Table 1 and 2. On Jericho, the agents trained jointly with the
retriever achieve strong results, but they perform slightly worse than our main approach that keeps
the retriever separate from the neural agent. This shows that the joint version is slightly harder to
train. Also, it brings the disadvantage that the architecture of the neural agent has to be changed
to take the CBR into account, whereas our main approach that keeps the retriever separate allows
readily plugging any on-policy agent for TBGs."
REFERENCES,0.7602179836512262,Published as a conference paper at ICLR 2022
REFERENCES,0.7629427792915532,"Easy
Medium
Hard
#Steps
Norm. Score
#Steps
Norm. Score
#Steps
Norm. Score IN"
REFERENCES,0.7656675749318801,"Text + CBR (joint)
17.91 ± 3.80
0.91 ± 0.04
40.22 ± 1.70
0.65 ± 0.05
47.94 ± 1.10
0.33 ± 0.02
TPC + CBR (joint)
16.80 ± 1.97
0.94 ± 0.04
36.12 ± 1.32
0.67 ± 0.03
46.10 ± 0.72
0.41 ± 0.03
KG-A2C + CBR (joint)
16.40 ± 1.89
0.95 ± 0.05
36.50 ± 1.13
0.68 ± 0.03
46.58 ± 0.91
0.40 ± 0.06
BiKE + CBR (joint)
16.01 ± 1.37
0.94 ± 0.03
35.93 ± 1.11
0.67 ± 0.05
46.11 ± 1.14
0.42 ± 0.04 OUT"
REFERENCES,0.7683923705722071,"Text + CBR (joint)
21.47 ± 2.32
0.88 ± 0.07
39.10 ± 1.33
0.67 ± 0.02
48.10 ± 0.92
0.31 ± 0.03
TPC + CBR (joint)
17.89 ± 1.82
0.93 ± 0.01
38.11 ± 1.33
0.65 ± 0.03
47.92 ± 1.55
0.34 ± 0.03
KG-A2C + CBR (joint)
18.19 ± 2.12
0.93 ± 0.02
37.72 ± 2.91
0.66 ± 0.03
47.53 ± 1.11
0.40 ± 0.04
BiKE + CBR (joint)
18.12 ± 1.21
0.94 ± 0.05
35.77 ± 1.05
0.69 ± 0.05
46.16 ± 1.00
0.40 ± 0.04"
REFERENCES,0.771117166212534,"Table 8: Test-set results obtained on TWC in-distribution (IN) and out-of-distribution (OUT) games training
different neural agents and the CBR retriever jointly."
REFERENCES,0.773841961852861,"Game
KG-A2C + CBR (joint)
Text + CBR (joint)
TPC + CBR (joint)
BiKE + CBR (joint)"
REFERENCES,0.776566757493188,"detective
250.1
241.5
316.2
324.2
inhumane
15.1
13.2
16.3
24.1
snacktime
13.2
11.2
20.1
21.8
karn
0
0
0
0
zork1
36.4
36.5
36.3
43.7
zork3
1.5
1.5
1.8
3.6
enchanter
25.1
26.1
21.1
35.6
spellbrkr
32.3
33.3
39.2
41.8"
REFERENCES,0.779291553133515,"Table 9: Results obtained on a subset of the Jericho games training different neural agents and the CBR
retriever jointly. Bold values indicate when the joint variant achieves better scores than the main counterparts
reported in Table 5."
REFERENCES,0.782016348773842,"F
MULTI-PARAGRAPH TEXT-BASED RETRIEVER"
REFERENCES,0.784741144414169,"Knowledge graphs have been used extensively in text-based games and other areas of natural
language understanding (Ammanabrolu & Hausknecht, 2020; Atzeni & Atzori, 2018; Kapanipathi
et al., 2020). This section describes an alternative to our graph-based retriever, which only relies on
the textual observations without modeling the state of the game as a graph."
REFERENCES,0.7874659400544959,"Recent work (Guo et al., 2020) has shown that enriching the current observation with relevant
observations retrieved from the history of interactions with the environment can achieve competitive
results on Jericho. Therefore, in order to assess the effectiveness of our graph-based implementation,
we compare to a multi-paragraph text-based retriever (MTPR) inspired by the work of Guo et al.
(2020).
In this case, we do not model the state as a graph and, subsequently, we remove
the seeded graph attention mechanism from the retriever.
Instead, given the current natural
language observation ot, we compute an action-speciﬁc representation following Guo et al. (2020),
concatenating ot with the n most recent observations that share objects with it or with the given
action.
The encoded observation is then discretized using vector quantization as in our main
architecture."
REFERENCES,0.7901907356948229,"We evaluated the text-based retriever on Jericho, integrating it in the same baseline agents described
in Section 6.1. Table 10 shows the scores obtained by the agents. We observe that, overall, the
graph-based retriever performs better on the vast majority of the games. This result conﬁrms the
ability of our approach based on seeded graph attention to extract relevant information from the
state of the game, compared to a retriever that only relies on text information."
REFERENCES,0.7929155313351499,"G
ADDITIONAL RESULTS USING ENTITIES AS CONTEXT SELECTORS"
REFERENCES,0.7956403269754768,"As mentioned in Section 6.4, we performed an ablation study where the seeded graph attention and
the state graph where not used in the retriever. Instead, we represent the context as just a single focus
entity. This choice suits very well the TWC games, as the goal of each game is to tidy up a house
by putting objects in their commonsensical locations. Hence, each rewarded action in TWC is of the
form “put o on s” or “insert o in c”, where o is an entity of type object, s is a supporter, and c is a"
REFERENCES,0.7983651226158038,Published as a conference paper at ICLR 2022
REFERENCES,0.8010899182561307,"Game
KG-A2C + CBR (MPTR)
Text + CBR (MPTR)
TPC + CBR (MPTR)
BiKE + CBR (MPTR)"
REFERENCES,0.8038147138964578,"detective
245.2
233.7
302.3
321.2
inhumane
13.4
13.2
12.3
20.3
snacktime
12.1
8.2
18.1
19.5
karn
0
0
0
0
zork1
36.2
36.2
37.4
42.2
zork3
0.8
1.2
3.2
3.6
enchanter
19.3
24.3
20.2
32.1
spellbrkr
31.3
30.3
39.3
40.8"
REFERENCES,0.8065395095367848,"Table 10: Results obtained on a subset of the Jericho games using the multi-paragraph text-based retriever
(MPTR) instead of the graph-based one. Bold values indicate when the MPTR variant achieves better scores
than the main counterparts reported in Table 5."
REFERENCES,0.8092643051771117,"container (Murugesan et al., 2021c; Côté et al., 2018). As an example, a rewarded action could be
“insert dirty singlet in washing machine”, where dirty singlet is the object and washing machine is
the container."
REFERENCES,0.8119891008174387,"Representing the context as just single entities of type object, means that the memory of the CBR
agent is storing what action to apply to each object in the game, and therefore the agent is in practice
constructing a registry where each object is paired with its commonsensical location. Let cv, cu be
two context entities. The retriever then computes the similarity between the contexts as:"
REFERENCES,0.8147138964577657,"sim(cv, cu) = cosine(FFN(hv), FFN(hu)),"
REFERENCES,0.8174386920980926,"where cosine denotes the cosine similarity, FFN is a 2-layer feed-forward network and hv, hu
are the BERT encodings of the [CLS] token for objects v and u respectively. The retriever is
therefore encouraged to map objects that should be placed in the same location (either a supporter
or a container) to similar representations."
REFERENCES,0.8201634877384196,"Figure 6 depicts a t-SNE (van der Maaten & Hinton, 2008) visualization of the representations
FFN(hv) learned by the retriever of the BiKE + CBR (w/o GAT) agent. The plot shows that
entities that belong to the same location are mapped to similar representations. This holds both for
objects in the in-distribution games and for objects in the out-of-distribution games."
REFERENCES,0.8228882833787466,"gray cap
derby hat"
REFERENCES,0.8256130790190735,clean red dress
REFERENCES,0.8283378746594006,fleece jacket
REFERENCES,0.8310626702997275,dirty yellow T-shirt
REFERENCES,0.8337874659400545,wet white shirt
REFERENCES,0.8365122615803815,wet orange T-shirt
REFERENCES,0.8392370572207084,clean pan scarf
REFERENCES,0.8419618528610354,dirty gray underpants
REFERENCES,0.8446866485013624,brown cap
REFERENCES,0.8474114441416893,wet azure dress
REFERENCES,0.8501362397820164,white cap
REFERENCES,0.8528610354223434,clean gray blazer
REFERENCES,0.8555858310626703,dirty checkered shirt
REFERENCES,0.8583106267029973,wet hoodie
REFERENCES,0.8610354223433242,dirty brown pullover
REFERENCES,0.8637602179836512,clean white pullover
REFERENCES,0.8664850136239782,dirty magenta skirt
REFERENCES,0.8692098092643051,wet blue jumper
REFERENCES,0.8719346049046321,top hat
REFERENCES,0.8746594005449592,black coat
REFERENCES,0.8773841961852861,dirty singlet
REFERENCES,0.8801089918256131,trench coat
REFERENCES,0.8828337874659401,clean white skirt
REFERENCES,0.885558583106267,clean azure skirt
REFERENCES,0.888283378746594,wet white jumper
REFERENCES,0.8910081743869209,vegetable oil
REFERENCES,0.8937329700272479,"IN
OUT"
REFERENCES,0.896457765667575,"Figure 6: Visualization of the entity representations learned by the retriever. Colors denote the target location
of each object."
REFERENCES,0.8991825613079019,"We evaluated all the CBR agents with the simple retriever deﬁned above. Table 11 reports the
results for the in-distribution and out-of-distribution games. The results conﬁrm that the entity-based
context selection performs well and achieves good out-of-distribution generalization. However,
we remark that the complete retriever described in Section 4 consistently achieves better results,
showing the importance of incorporating additional structured information."
REFERENCES,0.9019073569482289,Published as a conference paper at ICLR 2022
REFERENCES,0.9046321525885559,"Easy
Medium
Hard
#Steps
Norm. Score
#Steps
Norm. Score
#Steps
Norm. Score IN"
REFERENCES,0.9073569482288828,"CBR (w/o GAT)
22.70 ± 2.05
0.81 ± 0.07
44.13 ± 1.15
0.62 ± 0.04
48.05 ± 1.30
0.33 ± 0.05
Text + CBR (w/o GAT)
19.01 ± 3.99
0.89 ± 0.05
40.10 ± 1.52
0.67 ± 0.05
47.80 ± 1.32
0.33 ± 0.02
TPC + CBR (w/o GAT)
17.15 ± 2.91
0.94 ± 0.04
38.32 ± 1.76
0.66 ± 0.03
47.22 ± 1.35
0.36 ± 0.03
KG-A2C + CBR (w/o GAT)
16.67 ± 2.30
0.96 ± 0.03
38.05 ± 1.84
0.66 ± 0.04
46.45 ± 1.02
0.38 ± 0.02
BiKE + CBR (w/o GAT)
16.32 ± 1.10
0.95 ± 0.03
36.13 ± 1.40
0.67 ± 0.04
45.72 ± 0.63
0.41 ± 0.03 OUT"
REFERENCES,0.9100817438692098,"CBR (w/o GAT)
23.90 ± 2.17
0.79 ± 0.05
44.71 ± 1.50
0.61 ± 0.04
48.87 ± 1.89
0.31 ± 0.03
Text + CBR (w/o GAT)
21.64 ± 2.52
0.88 ± 0.02
41.12 ± 1.21
0.66 ± 0.05
48.00 ± 1.10
0.32 ± 0.06
TPC + CBR (w/o GAT)
19.82 ± 2.13
0.92 ± 0.03
39.34 ± 1.01
0.67 ± 0.02
47.33 ± 1.30
0.36 ± 0.04
KG-A2C + CBR (w/o GAT)
19.07 ± 2.50
0.92 ± 0.02
38.41 ± 1.94
0.65 ± 0.04
46.89 ± 2.21
0.37 ± 0.03
BiKE + CBR (w/o GAT)
18.15 ± 1.51
0.92 ± 0.03
37.10 ± 1.41
0.67 ± 0.03
46.70 ± 0.71
0.39 ± 0.03"
REFERENCES,0.9128065395095368,"Table 11: Test-set performance for TWC in-distribution (IN) and out-of-distribution (OUT) games using
entities as context selectors."
REFERENCES,0.9155313351498637,"H
CASE-BASED REASONING AND OUT-OF-DISTRIBUTION GENERALIZATION"
REFERENCES,0.9182561307901907,"Out-of-distribution generalization has recently fueled signiﬁcant research effort and several datasets
and approaches have been proposed in the past few years (Bahdanau et al., 2019; Keysers et al., 2020;
Atzeni et al., 2021). Our experiments on TWC allowed assessing the hypothesis that case-based
reasoning can be used to tackle out-of-distribution (OOD) generalization in text-based games. Table
12 shows the absolute OOD generalization gap of the different agents evaluated in our experiments.
Note that this table does not report any new result, but it simply provides the absolute difference
between the values in Table 1 and 2.
We observe that the agents relying on CBR achieve a
considerably better generalization performance out of the training distribution, almost comparable
to the results obtained on the same distribution as the training data. In some cases, the normalized
score achieved by the CBR agents in the out-of-distribution games equals the score obtained in the
in-distribution games. This happens because case-based reasoning forces the agent to map contexts
including entities that were not seen at training time to the most similar contexts in the CBR memory.
CBR allows the agent to solve completely new problems and generalize by effectively retrieving past
cases and mapping the retrieved actions from the training distribution to the most similar options in
the OOD setting. Note that the only good OOD generalization gap for the agents that are not relying
on CBR (the #Steps of the Text agent on the Hard level) is an artifact of the experiment, as all
agents were limited to a maximum of 50 steps."
REFERENCES,0.9209809264305178,"Figure 6 shows a nice example of the capability of the agent to generalize OOD. In this case,
entity embeddings were used as the context representations, and we observe that entities that are
not included in the training distribution are correctly mapped to the right cluster. This shows that the
CBR approach learns effective and generalizable context representations based on the objective of
the games. These representations are then used by the agent to select relevant experiences and map
the actions used at training time to the most viable alternative in the OOD test set."
REFERENCES,0.9237057220708447,"Easy
Medium
Hard"
REFERENCES,0.9264305177111717,"#Steps
Norm. Score
#Steps
Norm. Score
#Steps
Norm. Score"
REFERENCES,0.9291553133514986,"Text
6.07
0.10
1.82
0.05
0.16
0.10
TPC
7.15
0.11
2.28
0.04
1.55
0.13
KG-A2C
6.24
0.06
1.44
0.03
2.00
0.11
BiKE
7.32
0.11
1.67
0.03
2.81
0.11"
REFERENCES,0.9318801089918256,"CBR-only
1.30
0.00
0.27
0.01
0.59
0.02
Text + CBR
3.38
0.04
1.22
0.00
0.78
0.02
TPC + CBR
2.09
0.02
0.25
0.01
0.29
0.03
KG-A2C + CBR
2.30
0.05
0.89
0.02
0.99
0.02
BiKE + CBR
1.43
0.02
0.21
0.00
0.70
0.02"
REFERENCES,0.9346049046321526,Table 12: Absolute out-of-distribution generalization gap in TWC
REFERENCES,0.9373297002724795,Published as a conference paper at ICLR 2022
REFERENCES,0.9400544959128065,"I
ADDITIONAL BASELINES ON JERICHO"
REFERENCES,0.9427792915531336,"Several methods have been proposed recently for text-based games. In order to keep the results in the
main paper more compact, we only included in Table 3 well-known and top-performing baselines
that were evaluated on the full (or almost) set of Jericho games. For completeness, this section
compares our best agent (BiKE + CBR) with the following additional methods:"
REFERENCES,0.9455040871934605,"• Q*BERT (Ammanabrolu et al., 2020) is a deep reinforcement learning agent that plays text
games by building a knowledge graph of the world and answering questions about it;
• Trans-v-DRRN (Xu et al., 2020) relies on a lightweight transformer encoder to model the state
of the game;
• DBERT-DRRN (Singh et al., 2021) makes use of DistilBERT (Sanh et al., 2019) ﬁne-tuned on
an independent set of human gameplay transcripts."
REFERENCES,0.9482288828337875,"Table 13 shows the scores obtained by these baselines compared to our agent enhanced with CBR.
Overall, we observe that the BiKE + CBR agent outperforms the baselines on the majority of
the games, conﬁrming the effectiveness of case-based reasoning as a viable approach to boost the
performance of text-based RL agents."
REFERENCES,0.9509536784741145,"Game
Q*BERT
Trans-v-DRRN
DBERT-DRRN
BiKE + CBR"
"-
-
-",0.9536784741144414,"905
-
-
-
0
acorncourt
-
10
-
12.2
adventureland
-
25.6
-
27.3
afﬂicted
-
2
-
3.2
awaken
-
-
-
0
detective
246.1
288.8
-
326.1
dragon
-
-
-
8.3
inhumane
-
-
32.8
24.2
library
10.0
17
17
22.3
moonlit
-
-
-
0
omniquest
-
-
4.9
17.2
pentari
48.2
34.5
-
52.1
reverb
-
10.7
6.1
6.5
snacktime
-
-
20
22.1
temple
7.9
7.9
8
7.8
ztuu
5
4.8
-
. 87.2
advent
-
-
-
62.1
balances
9.8
-
-
11.9
deephome
1
-
-
1
gold
-
-
-
2.1
jewel
-
-
6.5
6.4
karn
-
-
-
0
ludicorp
17.6
16
12.5
23.8
yomomma
-
-
0.5
1
zenon
-
-
-
4.1
zork1
33.6
36.4
44.7
44.3
zork3
-
0.19
0.2
3.2
anchor
-
-
-
0
enchanter
-
20.0
-
36.3
sorcerer
-
-
-
24.5
spellbrkr
-
40
38.2
41.2
spirit
-
-
2.1
4.2
tryst205
-
9.6
9.3
13.4"
"-
-
-",0.9564032697547684,"Table 13: Average raw score on the Jericho games. Results are taken from the original papers or “−” is used
if a result was not reported."
"-
-
-",0.9591280653950953,"J
HYPERPARAMETERS AND REPRODUCIBILITY"
"-
-
-",0.9618528610354223,"All CBR agents are trained using the same hyperparameter settings and the same hardware/software
conﬁguration. As mentioned in Section 4, we use a pre-trained BERT model (Devlin et al., 2019)
to represent initial node features in the state graph. BERT is only used to compute the initial
representations of the entities and is not ﬁne-tuned. We use the following hyperparameters for our
experiments."
"-
-
-",0.9645776566757494,"• We set the hidden dimensionality of the model to d = 768 and we use 12 attention heads
for the graph attention network, each applied to 64-dimensional inputs."
"-
-
-",0.9673024523160763,Published as a conference paper at ICLR 2022
"-
-
-",0.9700272479564033,• We use nl = 2 seeded GAT layers for TWC and nl = 3 for Jericho.
"-
-
-",0.9727520435967303,"• On both datasets, we apply a dropout regularization on the seeded GAT with probability of
0.1 at each layer."
"-
-
-",0.9754768392370572,"• Similarly, for the experiments on TWC, we only sample the most recent context-action pair
from T , whereas we sample k = 3 pairs for Jericho. We used k = 2 for the scalability
analysis depicted in Figure 5."
"-
-
-",0.9782016348773842,• The retriever threshold is kept constant to τ = 0.7 across all experiments.
"-
-
-",0.9809264305177112,"• On TWC, we train the agents for 100 episodes and a maximum of 50 steps for each episode.
On Jericho, as mentioned, we follow previous work and we train for 100 000 valid steps,
starting a new episode every 100 steps or whenever the games ends."
"-
-
-",0.9836512261580381,• We set the discount factor γ to 0.9 on all experiments.
"-
-
-",0.9863760217983651,"• For the ablation study on memory access, we set the output dimensionality of the RP and
SRP methods to p = 64. For LSH, we set the number of hash tables to l = 16 and the
length of the hash codes of h = 8 bits. We artiﬁcially limit the size of each bucket to the 4
most recent entries."
"-
-
-",0.989100817438692,"Experiments were parallelized on a cluster where each node was dedicated to a separate run. The
conﬁguration of the execution nodes is as reported in Table 14."
"-
-
-",0.9918256130790191,"Resource
Setting"
"-
-
-",0.9945504087193461,"CPU
Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz
Memory
128GB
GPUs
1 x NVIDIA Tesla k80 12 GB
Disk1
100GB
Disk2
600GB
OS
Ubuntu 18.04-64 Minimal for VSI"
"-
-
-",0.997275204359673,Table 14: Hardware and software conﬁguration used to train the agents
