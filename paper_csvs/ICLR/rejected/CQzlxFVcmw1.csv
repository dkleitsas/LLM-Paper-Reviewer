Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002551020408163265,"Recently, the hyper-relational knowledge graph (HKG) has attracted much attention
due to its widespread existence and potential applications. The pioneer works have
adapted powerful graph neural networks (GNNs) to embed HKGs by proposing
domain-speciﬁc message functions. These message functions for HKG embedding
are utilized to learn relational representations and capture the correlation between
entities and relations of HKGs. However, these works often manually design and
ﬁx structures and operators of message functions, which makes them difﬁcult to
handle complex and diverse relational patterns in various HKGs (i.e., data patterns).
To overcome these shortcomings, we plan to develop a method to dynamically
search suitable message functions that can adapt to patterns of the given HKG.
Unfortunately, it is not trivial to design an expressive search space to enable the
powerful message functions being searched, especially the space cannot be too
large for the sake of search efﬁciency. In this paper, we ﬁrst unify a search space of
message functions to search both structures and operators. The message functions
of existing GNNs and some classic KG/HKG models can be instantiated as special
cases of the proposed search space. Then, we leverage a search algorithm to
search the message function and other GNN components for any given HKGs. We
empirically show that the searched message functions are data-dependent, and can
achieve leading performance in link/relation prediction tasks on benchmark HKGs."
INTRODUCTION,0.00510204081632653,"1
INTRODUCTION"
INTRODUCTION,0.007653061224489796,"Knowledge base (KB) (Auer et al., 2007) is an important tool to organize and explore human
knowledge, thereby promoting a series of applications, e.g., question answering (Lukovnikov et al.,
2017) and recommendation system (Cao et al., 2019). Generally, the KB stores the n-ary fact
r(e1, · · · , en) (n is arity) that represents the relation r ∈R between real-world entities ei ∈E. To
manipulate large scale KBs, KB embedding (Nickel et al., 2015; Wang et al., 2017) proposes to encode
the set of relations R and entities E into a d-dimensional vector space R ∈R|R|×d, E ∈R|E|×d.
In last decades, the research community mainly focuses on embedding knowledge graphs (KGs)
that only contain binary facts {r(e1, e2)}, e.g., isCaptialOf(Beijing,China). Among
kinds of KG embedding models (Rossi et al., 2021), tensor models (Lacroix et al., 2018; Balazevic
et al., 2019) propose to represent a KG into a 3-order tensor and decompose tensors into R and
E, which achieve outstanding empirical performance and theoretical guarantees. Recent studies
start to learn embeddings from n-ary facts (n ≥2) because n-ary facts are widespread in KBs, e.g.,
playCharacterIn(LeonardNimoy,Spock,StarTrek1). For example, more than 30%
of entities in Freebase (Bollacker et al., 2008) involve facts with higher arity (Wen et al., 2016).
Therefore, it is necessary to investigate the more general case of KGs, facts with mixed arities
S = {r(e1, . . . , en) : n ∈{2, . . . , N}} named as hyper-relational KGs (HKGs)."
INTRODUCTION,0.01020408163265306,"Unfortunately, it is hard to extend powerful tensor models from the case of ﬁxed arity (e.g., KG)
to the case of mixed arities (i.e., HKG). That is because a tensor can only model a set of facts
under the same arity. Instead, some pioneer works (Yadati, 2020; Galkin et al., 2020) demonstrate
that the multi-relational hypergraph (MRHG) could be a more natural way to model HKGs (see
Appx. A for more details). Let entities E and relations R be nodes and edge types in the MRHG
G(E, R, S), respectively. The length of MRHG’s edge (e1, . . . , en) (hyperedge) could be variant,
which can represent facts with various arities n. And hyperedges can be labeled by multiple edge
types r ∈R like r(e1, . . . , en) ∈S. Under the MRHG modeling, these works adapt powerful graph"
INTRODUCTION,0.012755102040816327,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.015306122448979591,"neural networks (GNNs) (Kipf & Welling, 2016; Hamilton et al., 2017) to embed HKGs (Yadati,
2020; Galkin et al., 2020). Generally, GNNs learn node embeddings by passing messages (e.g., the
node features) from adjacent nodes to the center node. But in scenarios of HKGs, it is important to
know the type of edge (relation) that connects several entities. Therefore, existing GNNs for HKG
embedding design several domain-speciﬁc message functions to learn relational representations R by
capturing the interaction between entities E and relations R."
INTRODUCTION,0.017857142857142856,"Existing works manually design and ﬁx the structures and operators of message functions. However,
such rigid message function designs are not conducive to pursuing high empirical performance, as
relations usually have distinct patterns in various KGs/HKGs. For example, the message function of
G-MPNN (Yadati, 2020) adopts the inner product way like DistMult (Yang et al., 2015) to compute
the correlation between entities and relations, which has been proven to only cover symmetric
relations (Kazemi & Poole, 2018). Its performance may not be good if there are many non-symmetric
relations existed (see Appx. B.1). It may be a potential solution to design a universal message function
to cover as many relational patterns as possible. But covering a certain pattern does not mean that
the model can reach good performance on it (Meilicke et al., 2018; Rossi et al., 2021). Moreover, a
pioneer work AutoSF (Zhang et al., 2020) shows that designing data-aware models can consistently
achieve high performance on any given KGs. Thus, dynamically searching message functions could
be an effective way to capture the data patterns of the given HKG and pursue high performance."
INTRODUCTION,0.02040816326530612,"Unfortunately, the searching method AutoSF is strictly restricted to bilinear KG models (Yang et al.,
2015; Kazemi & Poole, 2018), which is not applicable to message function design and the HKG
scenario. Besides, although neural architecture search (NAS) (Elsken et al., 2019) has been introduced
to search GNN architectures, current GNN search spaces (Zhang et al., 2021) are not well regularized
to tasks on HKGs. Speciﬁcally, the existing search spaces of message function follow the classic way
to simply aggregate node neighbors to learn node embeddings, while ignoring edge embeddings to
represent relations. Thus, they cannot capture the correlation between entities and relations of HKGs."
INTRODUCTION,0.02295918367346939,"In summary, rigid message function designs for HKGs are not conducive to consistently pursuing
high performance on different data sets, while existing searching methods are not applicable for
HKGs. To bridge this research gap, we propose the Message function SEArch for any given HKGs,
named as MSeaHKG. However, it is non-trivial to design an expressive search space to enable the
powerful message functions being searched, especially the space cannot be too large for the sake of
search efﬁciency. Thus, we identify the necessary computation operators that are domian-speciﬁc
designs for HKGs and propose to search the interaction between these operators for capturing the
relational patterns. Moreover, except for the message function search, we also incorporate other
GNN components (e.g., aggregation function) in the MSeaHKG search space for more performance
improvements. Then, we formulate the discrete HKG models with probabilistic modelings to enable
an efﬁcient NAS algorithm working on our scenario. The main contributions are listed as:"
INTRODUCTION,0.025510204081632654,"• Previous GNN searching methods generally ignore the edge representations, which fails to handle
semantic meaningful relations on HKGs. Besides, their message functions cannot capture complex
interactions between entities with relations. In this paper, we propose a searching method to
dynamically design a suitable GNN that can achieve high performance on the given HKG."
INTRODUCTION,0.02806122448979592,"• Inspired by rigid message function designs, we deﬁne a novel search space of message functions for
HKGs, which enables the message function to be ﬂexibly searched on the given HKG. Especially,
the message function designs of existing GNNs for HKGs and some classic KG/HKG models can
be instantiated as special cases of the proposed space."
INTRODUCTION,0.030612244897959183,"• We conduct experiments on benchmark HKGs for the link prediction and relation prediction
tasks. Experimental results demonstrate that MSeaHKG can consistently achieve state-of-the-art
performance by designing data-aware message functions. Besides, we also transfer MSeaHKG to
other graph-based tasks and further investigate its capability."
RELATED WORK,0.03316326530612245,"2
RELATED WORK"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.03571428571428571,"2.1
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.03826530612244898,"To avoid manual efforts on neural architecture designs, NAS (Hutter et al., 2018; Yao & Wang, 2018)
aims to automatically search suitable neural architectures for the given data and task. Generally, search"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.04081632653061224,Under review as a conference paper at ICLR 2022
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.04336734693877551,"Table 1: Overview of Existing Works. NC, GC, LP, RP denote node classiﬁcation, graph classiﬁcation,
link prediction, relation prediction, respectively. DP(·) is dropout; BN(·) is batch normalization;
¯o(·) outputs the hidden representation after summarizing mixed operations (Zhao et al., 2021); hK
i
denotes the output from i-th operator of O in K-th layer of message function (see Sec. 3.1)."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.04591836734693878,"Type
Model
Scenarios
Message Function
Task
# Type/Length of edge
You et al. (2020) NC/GC/LP"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.04846938775510204,= 1/ = 2
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.05102040816326531,"DP(BN(W ej + b))
NAS
GraphNAS NC"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.05357142857142857,"aijconcat(ei, ej)
for
AGNN
aijW ej
GNNs
SANE
≥1/ = 2
W ¯o({ej}ej∈N(ei))
NAS-GCN
GC
aijMLP(hij)ej"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.05612244897959184,"GNNs for
KGs/HKGs R-GCN LP/RP"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.058673469387755105,"≥1/ = 2
Wrej
CompGCN
Wλ(r)φ(ej, r)
StarE
≥1/ ≥2
Wλ(r)φr(eo, γ(r, hr))
G-MPNN
rP (e) ∗Q"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.061224489795918366,i pei ∗ei
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.06377551020408163,"Ours
MSeaHKG
LP/RP
≥1/ ≥2
MLP&concat({hK
i }|O|
i=1)"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.0663265306122449,"space, search algorithm, and evaluation measurement are three important components in NAS (Elsken
et al., 2019). Search space deﬁnes what network architectures in principle should be searched. The
search algorithm performs an efﬁcient search over the search space and ﬁnds architectures that achieve
good performance. Evaluation measurement decides how to evaluate the searched architectures during
the search. Classical NAS methods are computationally consuming because candidate architectures
are evaluated by the stand-alone way, i.e., evaluating the performance of architecture after training
it to convergence. More recently, one-shot NAS (Pham et al., 2018) proposes the weight sharing
mechanism to share network weights across different candidate architectures and evaluate them on
the shared weights, which can extremely reduce the search cost."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.06887755102040816,"Some pioneer works have explored NAS for GNNs, such as You et al. (2020), GraphNAS (Gao et al.,
2020), AGNN (Zhou et al., 2019), NAS-GCN (Jiang & Balaprakash, 2020). And the one-shot NAS
also has been introduced to search GNN architectures recently, e.g., SANE (Zhao et al., 2021). As
shown in the left part of Fig. 1, most GNN searching methods follow the message passing neural
networks (MPNNs) (Gilmer et al., 2017) to unify two steps of the GNN framework in one layer:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.07142857142857142,"step1 : mi ←agg({mgc(ei, ej)}ej∈N(ei)), step2 : ei ←act(comb(ei, mi)),
(1)"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.07397959183673469,"where ei ∈Rd represents the embedding of node ei, mi is the intermediate embeddings of ei
gathered from its neighborhood N(ei). The search space of operators in Eq. 1 are summarized into:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.07653061224489796,"• Message Function mgc(·): The message function decides the way to gather information from
a neighborhood ej of the center node ei. Zhang et al. (2021) summarizes the typical message
in existing GNN searching methods as mgc(ei, ej) = aijW ej, where aij denotes the attention
scores between nodes ei with ej. Besides, we present more message function designs in Tab. 1."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.07908163265306123,"• Aggregation Function agg(·): It controls the way to aggregate message from nodes’ neigh-
borhood.
Usually agg ∈{sum, mean, max}, where sum(·) = P
ej∈N(ei) mgc(ei, ej),
mean(·) = P
ej∈N(ei) mgc(ei, ej)/|N(v)|, and max(·) denotes channel-wise maximum."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.08163265306122448,"• Combination Function comb(·): It determines the way to merge messages between neighborhood
and node itself. In literature, comb is selected from {concat, add, mlp}, where concat(·) =
[ei, mi], add(·) = ei + mi, and mlp(·) = MLP(ei + mi) (MLP is Multi-layer Perceptron)."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.08418367346938775,"• Activation Function act(·): [identity, sigmoid, tanh, relu, elu] are some of the most commonly
used activation functions (Gao et al., 2020)."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.08673469387755102,"Overall, mgc(·) in Eq. 1 only learns node embeddings, which cannot encode the semantic meaningful
edge types (i.e., relations in HKGs). Note that NAS-GCN (Jiang & Balaprakash, 2020) takes the edge
feature hij between ei and ej as input without learning edge embeddings, and recent AutoGEL (Zhili
et al., 2021) simply extends the message function mgc(·) to learn edge embeddings without studying
the interactions between entities and relations, thereby failing to handle the LP/RP tasks on HKGs."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.08928571428571429,Under review as a conference paper at ICLR 2022 ... ... ...
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.09183673469387756,"CompGCN
G-MPNN
StarE
MPNN"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.09438775510204081,Computing
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.09693877551020408,Operator
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.09948979591836735,Transform
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.10204081632653061,Matrix
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.10459183673469388,"entity
relation role/position"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.10714285714285714,"Figure 1: The framework of several GNN frameworks. The green box refers to the message function.
Note that CompGCN and StarE select sum(·) as agg(·) and omit comb(·), while G-MPNN uses
concat(·) as comb(·)."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.1096938775510204,"2.2
GRAPH NEURAL NETWORKS FOR KG/HKG EMBEDDING"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.11224489795918367,"As introduced in Sec. 1 and Sec. 2.1, message functions in classic GNNs simply aggregate messages
from adjacent nodes (see Tab. 1 and Eq. 1). But in scenarios of KGs and HKGs, it is important to know
the type of edge (relation) that connects several entities. To capture relations, R-GCN (Schlichtkrull
et al., 2018) proposes to model relations R in KGs through relation-speciﬁc weight matrix Wr ∈
Rd×d for r ∈R in message functions, which is instantiated as:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.11479591836734694,"ei = act(sum({Wrej}r(ei,ej)∈I(ei))),
(2)"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.11734693877551021,"where I(ei) = {r(ei, ej) ∈S : r ∈R, ej ∈E} is the set of relational edges incident on ei. But such
relation modeling may lead to the over-parameterization issue because there are many relations in
KGs. Therefore, CompGCN (Vashishth et al., 2020) utilizes the embedding vector r to represent the
relation instead of weigh matrix:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.11989795918367346,"ei = act(sum({Wλ(r)φ(ej, r)}r(ei,ej)∈I(ei))), r = W r
(3)"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.12244897959183673,"where λ(r) records the directional information of edges. The entity-relation composition operator
set {sub, mult, corr} is inspired by classical scoring function design in existing KG embedding
models, such as element-wise subtraction sub(·) = ej −r (Bordes et al., 2013), inner product
mult(·) = ej ∗r (Yang et al., 2015), circular correlation corr(·) = ej ◦r (Nickel et al., 2016)."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.125,"Subsequently, StarE and G-MPNN extend the applications of GNNs from KGs to HKGs.
StarE (Galkin et al., 2020) requires the role information roj of entity ej and assumes a hyper-
relational fact r(e1, . . . , en) is composed by a base triplet r(es, eo) with a set of role-value pairs
{(roj, ej)} such that r(es, eo, {(roj, ej)}). Then StarE takes base relation with the role-value pairs as
a hyper-relation and performs the composition operator φr(·) on the hyper-relation with base entity:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.12755102040816327,"es = act(sum({Wλ(r)φr(eo, γ(r, hr))}r(es,eo,{(roj ,ej)})∈I(es))), hr = W sum({φo(roj, ej)}j),"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.13010204081632654,"where hr is the hidden representation aggregated from the role-value pairs, γ(·) is a concatenate
operator to output the representation of hyper-relation. The update of r in StarE is similar to
CompGCN. Instead of requiring role information, G-MPNN (Yadati, 2020) proposes to model
positional information in GNNs:"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.1326530612244898,"ei = act([ei, agg({rs,P (s) ∗
Y"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.13520408163265307,"j∈{1,...,n}
pej,s ∗ej}r(e1,...,en)∈I(ei))]),
(4)"
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.1377551020408163,"where s represents r(e1, . . . , en) ∈S, P(s) : s →{1, . . . , np} is a positional mapping (np ≤|E|),
and pej,s is the positional embedding vector of ej on fact s. To intuitively check the difference of
message functions, we plot the framework of general MPPNs and GNNs for KGs/HKGs in Fig. 1."
ONE-SHOT NEURAL ARCHITECTURE SEARCH IN GRAPH NEURAL NETWORK,0.14030612244897958,Under review as a conference paper at ICLR 2022
MSEAHKG,0.14285714285714285,"3
MSEAHKG"
MSEAHKG,0.14540816326530612,"In this section, we ﬁrst propose a search space, especially the space of message functions, which
enables the powerful GNNs to be searched for any given HKGs. Then, we formulate our search
problem on the proposed space and solve it by leveraging an efﬁcient search algorithm."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.14795918367346939,"3.1
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.15051020408163265,"As discussed in Sec. 1, designing the proper message function is more conducive to capturing
relational patterns of given HKGs and pursuing high empirical performance. Therefore, the ﬁrst task
is to design an expressive search space so that message functions covering various relational patterns
can be searched. However, the space of existing GNN searching methods (see Eq. 1 and Fig. 1) does
not apply to HKGs and cannot cover the message functions of GNNs for KGs/HKGs (see Sec. 2.2).
Thus, we focus on the search space design of message functions in this paper."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.15306122448979592,"Recently, various message functions have been proposed for KGs/HKGs as presented in Sec. 2.2.
From Fig. 1, we can observe that those message functions are mainly different in these two aspects:
1) the operators (e.g., W , φ, γ) for computing hidden representations, 2) the structure of message
functions that decides how computational operators are connected. For the operator selection, existing
works manually tune them on different data sets, such as φ in CompGCN, φo, φr, γ in StarE, agg(·)
in G-MPNN. Moreover, the structure of message functions for HKGs (StarE and G-MPNN) tends
to be deeper and more complex than those for KGs (e.g., CompGCN). This is because the message
function needs to process more information (e.g., more entities/roles) when facing the facts with
higher arity. These observations motivate us to build space of operators and structures for message
function search. Furthermore, we investigate more about the relationship between operators and
relational patterns. Generally, the relational pattern can be represented as a certain correlation among
r(pemu(e1, . . . , en)) (check more in Appx. B.2), where pemu denotes the permutation. For example,
r is symmetry if r(ej, ei) must be true when r(ei, ej) is true. Therefore, the message function in the
search space must be able to handle such correlation in the form of r(pemu(e1, . . . , en)). In the next,
we ﬁrst introduce the space of operators, and then discuss how they deal with r(pemu(e1, . . . , en))."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1556122448979592,"• Positional Transformation Matrix WP (e): The position of entity in a fact can largely determine
the plausibility of the fact. For example, isCaptialOf(Beijing,China) is true while
isCaptialOf(China,Beijing) is false. G-MPNN utilizes the positional embedding pe,s
and re,P (e) to encode the position of entity e and relation r in different facts, which requires the
model complexity O(d|S|(|E| + N)). However, the training data set is very sparse in KBs. Such
over-parameterization may make the training insufﬁcient. Instead, we adopt the way to transform
one entity e to N possible positions. Let the positional mapping be P(e) : e →{1, . . . , N}, then
the positional matrix is able to transform e to the permutation position in pemu(·) by WP (e)e,
where WP (e) consumes O(Nd2) (|S| ≫d in practice)."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.15816326530612246,"• Concatenate Operator γ(·): It mainly determines the concatenation way between embedding
vectors. In this paper, we set Oγ = {concat, mult, wsum} (Galkin et al., 2020), where wsum
is the weighted sum. In general, γ(·) can concatenate embeddings after the positional transform
matrix WP (e), i.e., encoding pemu(e1, . . . , en)."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.16071428571428573,"• Role Embedding ro: Unlike the positional information of entities encoded by WP (e), the role em-
bedding is utilized to model the semantic information of entities (Liu et al., 2021). For example, the
roles in 2nd position of facts playCharacerIn(actor:ZacharyQuinto,character
:Spock,movie:StarTrek) and actorAwardIn(actor:ZacharyQuinto,award:
BC-BSFC,movie:StarTrek) are different though other entities and roles are same. Thus, the
model should be able to capture the role of candidate entities after using WP (e), e.g., BC-BSFC is
unlike to be the 2nd entity of playCharacerIn since its role is award instead of character."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.16326530612244897,"• Composition Operator φ(·): Following CompGCN, we utilize composition operator φ(·) to
capture message between the node and edge embeddings before aggregation step. Note that
φ actually encodes the interaction between r and pemu(e1, . . . , en). While CompGCN and
StarE empirically selects the most proper φ(·), we include this operator into the space X. Thus,
MSeaHKG is able to automatically search for the most suitable φ(·) in a more efﬁcient way. We
combine the settings of CompGCN and StarE to set Oφ = {sub, mult, corr, rotat} (rotat (Sun"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.16581632653061223,Under review as a conference paper at ICLR 2022
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1683673469387755,MPNN Layer 1
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.17091836734693877,Inputs
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.17346938775510204,MPNN Layer 2
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1760204081632653,layer cnt
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.17857142857142858,MPNN Layer 3
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.18112244897959184,Ouputs
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1836734693877551,layer cnt
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.18622448979591838,layer cnt
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.18877551020408162,"(a) Framework.
(b) One-layer MPNN. ... ..."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1913265306122449,"...
...
...
..."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.19387755102040816,MLP&concat
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.19642857142857142,(c) Message Function Space.
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.1989795918367347,"Figure 2: The framework of MSeaHKG. Fig. (b) represents concrete framework of MPNN layer in
Fig. (a), and Fig. (c) is detailed formulation of mg(·; θ) in Fig. (b). The operator layer cnt enables
the connectivity of different layers (Jiang & Balaprakash, 2020)."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.20153061224489796,"et al., 2019), see others in Sec 2.2). Besides, φ not only occurs between primary relations with
entities, but also captures the correlation between roles with entities.
• Others: (1) The idt(h) = h operation allows inputs to skip one layer in the message function; (2)
Unlike WP (e), the transform matrix W processes the hidden representations."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.20408163265306123,"Among above components, we ﬁx the role embedding and positional transform matrix WP (e) in the
message function (see Fig. 2 (c)) at the initial layer, which are speciﬁc designs to HKG problems.
And we include others into the space of message function O = {W , φ, γ, idt} for searching. As
shown in Fig. 2 (c), we denote the node ok
i as i-th operator of O in k-th layer and hk
i be the hidden
representation outputted by ok
i . Then we have:"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.2066326530612245,"{h0
i }2n+1
i=1
= {WP (e1)e1, . . . , WP (en)en, ro1, . . . , ron, r},
(5)"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.20918367346938777,"hk
i = ok
i
 
{θk
ijhk−1
j
}|O|
j=1

, for k ∈{1, . . . , K} and i ∈{1, . . . , |O|},
(6)"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.21173469387755103,"where θk
ij ∈{0, 1} controls the connection between ok
i with ok−1
j
.
Note that {h0
i }n+1
i=1
=
{WP (e1)e1, . . . , WP (en)en, r} if role information is not available in the given HKG. From Fig. 2 (c),
we can observe that the structure of message functions is controlled by {θk
ij}."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.21428571428571427,"To avoid manual operation selection, we also search for concrete operations of two operators φ and
γ. Given the operator set Oφ and Oγ, let θφk
i , θγk
i
∈{0, 1} records the selection i-th operation
oi ∈Oφ, Oγ at k-layer respectively. Then, φ and γ perform the computation in Eq. 6 could be
φk(h) = P θφk
i oi(h) and γk(h) = P θγk
i oi(h). Note that P"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.21683673469387754,"i θφk
i
= 1 and P"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.2193877551020408,"i θγk
i
= 1. Let
θmg = {θk
ij} ∪{θφk
i } ∪{θγk
i }. The message function parameterized by θmg is deﬁned as:"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.22193877551020408,"mg
 
r(e1, . . . , en); θmg
= MLP&concat({hK
i }|O|
i=1),
(7)"
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.22448979591836735,"where we simplify role embedding ro for simplicity and hK
i is outputted by the last layer of Eq. 6.
Intuitively, existing GNNs for KGs/HKGs are contained in the MSeaHKG space (compare Fig. 1 and
Fig. 2). Moreover, we include more discussions about the instantiations of other KG/HKGs models
in the MSeaHKG space and the capability of handling relational patterns in Appx. B.2."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.22704081632653061,"In addition to message function search, we also search for other operators (e.g., agg, comb, act) like
existing GNN search methods as shown in Fig. 2 (a) and (b). Let Θ = {θmg, θagg, . . . } be parameter
set for all operators selection in our MPNN framework. Then, an architecture can be represented as
XΘ = {mg(·; θmg), agg(·; θagg), · · · } (check other functions in Appx. C.1). And existing GNNs
for KG/HKG embeddings actually can be represented by different instantiations of Θ. Overall, a
GNN model XΘ encodes the given HKG G into embedding space ω = {E, R}, i.e., ω = XΘ(G)."
GNN SEARCH SPACE DESIGN FOR HKG EMBEDDING,0.22959183673469388,Under review as a conference paper at ICLR 2022
SEARCH ALGORITHM DESIGN,0.23214285714285715,"3.2
SEARCH ALGORITHM DESIGN"
SEARCH ALGORITHM DESIGN,0.23469387755102042,"In this subsection, we introduce how to select the GNN XΘ that can achieve high performance on the
given G. First, we evaluate performance of XΘ based on the HKG embedding ω = {E, R} since
XΘ is utilized to encode G(E, R, S) into ω, i.e., ω = XΘ(G). In the KG/HKG embedding, the
scoring function f(s; ω) is to verify the plausibility of fact s = r(e1, . . . , en) by decoding ω (check
more about scoring functions in Appx. C.2). Generally, the good embedding ω can make f(s; ω) to
distinguish true or false for a given fact s. Thus, we build evaluation of XΘ on f(s; ω). Taking link
prediction task (predict the missing entity in a fact, e.g., r(?, e2, . . . , en)) as example, let a scoring
function f(s; ω) decode ω and output a score matrix ps ∈[0, 1]|E|, where ps
e is the probability score
of e ∈E may be the ground truth of the missing entity. Then, we follow Dettmers et al. (2018) to
construct the cross entropy loss ℓ(f(s; ω)) = P"
SEARCH ALGORITHM DESIGN,0.2372448979591837,"e∈E ys
e log ps
e, where ys
e = 1 if e is the ground truth
otherwise 0. Subsequently, we denote L(XΘ, ω; G) = P"
SEARCH ALGORITHM DESIGN,0.23979591836734693,"s∈S ℓ(f(s; ω)) to calculate the overall loss
of a GNN model XΘ with the HKG embedding ω on G(E, R, S). Formally the GNN search problem
for a given HKG G is formulated as:"
SEARCH ALGORITHM DESIGN,0.2423469387755102,"min
Θ,ω L(XΘ, ω; G).
(8)"
SEARCH ALGORITHM DESIGN,0.24489795918367346,"Solving Eq. 8 is a non-trivial task because XΘ (e.g., XΘ = {agg : sum, act : relu, . . . }) is from
a large space. For example, only the structure space size {θk
ij} of message function reaches to
O(2K|O|2+(2N+1)|O|). And XΘ is discrete, indicating the gradient-based optimization cannot be
employed since ∇XΘL(·) does not exist. To enable an efﬁcient search, we ﬁrst relax the parameters of
GNN model Θ from a discrete space into a continuous and probabilistic space ¯Θ. More speciﬁcally,
θk
ij ∈{0, 1} restrictively controls the connectivity between ok
i with ok−1
j
, while ¯θk
ij ∈[0, 1] is the
probability that ok
i is connected with ok−1
j
. Then, let X ∼p ¯Θ(X) represent a GNN model X being
sampled from the distribution p ¯Θ(X). We reformulate the problem in Eq. 8 into:"
SEARCH ALGORITHM DESIGN,0.24744897959183673,"min
¯Θ,ω EX∼p ¯
Θ(X)[L(X, ω; G)],
(9)"
SEARCH ALGORITHM DESIGN,0.25,"where E[·] is the expectation. To compute the gradient w.r.t. ¯Θ, we ﬁrst utilize the reparameterization
trick X = g ¯Θ(U) (Maddison et al., 2016), where U is sampled from a uniform distribution p(U).
Then the gradient w.r.t. ¯Θ and ω is computed as (check full derivation in Appx. C.4):"
SEARCH ALGORITHM DESIGN,0.25255102040816324,"∇¯ΘEX∼p ¯
Θ(X)[L(X, ω; G)] = EU∼p(U)[L′(g ¯Θ(U), ω; D)∇¯Θg ¯Θ(U)],
(10)"
SEARCH ALGORITHM DESIGN,0.25510204081632654,"∇ωEX∼p ¯
Θ(X)[L(X, ω; G)] = EX∼p ¯
Θ(X)[∇ωL(X, ω; G)].
(11)"
SEARCH ALGORITHM DESIGN,0.2576530612244898,"Note that ∇¯Θg ¯Θ(U) can be computed if g ¯Θ(U) is differentiable. Inspired by SNAS (Xie et al.,
2018), we leverage Maddison et al. (2016); Jang et al. (2016) to instantiate g ¯Θ(U) (see Appx. C.4)."
EXPERIMENTS,0.2602040816326531,"4
EXPERIMENTS"
EXPERIMENTAL SETUP,0.2627551020408163,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.2653061224489796,"The experiments are implemented on top of PyTorch (Paszke et al., 2019) and performed on one
single RTX 2080 Ti GPU. Appx. D.1.1 introduces the details of hyper-parameters."
EXPERIMENTAL SETUP,0.26785714285714285,"Data Sets. The details of data sets are summarized into Tab. 7 in Appx. D.1.2. For experiments on
HKGs (facts with mixed arities), we employ 2 benchmark data sets: (1) Wiki-People (Guan et al.,
2019) is extracted from wiki-data, which mainly concerns the entities that belong to the human type.
(2) JF17K (Zhang et al., 2018) is extracted from Freebase (Bollacker et al., 2008). For experiments
on facts with ﬁxed arities, we utilize several data sets with ﬁxed arities and n > 2: WikiPeople-3,
JF17k-3, WikiPeople-4, and JF17k-4. Note that GETD (Liu et al., 2020) ﬁlters out the 3-ary and
4-ary facts from WikiPeople and JF17K to construct WikiPeople-n and JF17k-n, respectively."
EXPERIMENTAL SETUP,0.27040816326530615,"Tasks and Evaluation Metrics. In this paper, we mainly compare HKG embedding models on the
link and relation prediction task in the transductive setting. The link prediction task is to predict
the missing entity in the given fact at n possible positions, e.g., predicting the ﬁrst missing entity
r(?, e2, · · · , en). The relation classiﬁcation task needs to predict the missing relation in a fact when
all entities are known, i.e., ?(e1, · · · , en). We employ Mean Reciprocal Ranking (MRR) (Voorhees,
1999) and Hits@{1, 3, 10}. All reports are in the “ﬁltered” setting (see more in Appx. D.1.3)."
EXPERIMENTAL SETUP,0.2729591836734694,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.2755102040816326,"Table 2: The model comparison of the link prediction task on HKGs. The results of NNs and
multi-linear baselines are copied from Liu et al. (2020), those of Geo and S2S are copied from Di
et al. (2021). And GNN baselines are re-implemented due to the task variance."
EXPERIMENTAL SETUP,0.2780612244897959,"type
model
WikiPeople
JF17K
MRR
Hit@1
Hit@3
Hit@10
MRR
Hit@1
Hit@3
Hit@10
Geo
RAE
0.172
0.102
0.182
0.320
0.310
0.219
0.334
0.504 NNs"
EXPERIMENTAL SETUP,0.28061224489795916,"NaLP
0.338
0.272
0.364
0.466
0.366
0.290
0.391
0.516
HINGE
0.333
0.259
0.361
0.477
0.473
0.397
0.490
0.618
NeuInfer
0.350
0.282
0.381
0.467
0.517
0.436
0.553
0.675
Multi-
HypE
0.292
0.162
0.375
0.502
0.507
0.421
0.550
0.669
linear
RAM
0.380
0.279
0.445
0.539
0.539
0.463
0.573
0.690"
EXPERIMENTAL SETUP,0.28316326530612246,"GNNs
StarE
0.378
0.265
0.452
0.542
0.542
0.454
0.580
0.685
G-MPNN
0.367
0.258
0.439
0.526
0.530
0.459
0.572
0.688"
EXPERIMENTAL SETUP,0.2857142857142857,"Search
S2S
0.372
0.277
0.439
0.533
0.528
0.457
0.570
0.690
MSeaHKG
0.395
0.291
0.470
0.554
0.577
0.481
0.599
0.711"
EXPERIMENTAL SETUP,0.288265306122449,"Table 3: The model comparison of the relation prediction task on HKGs. The results of NNs are
copied from Guan et al. (2020), others are based on our implementations."
EXPERIMENTAL SETUP,0.29081632653061223,"type
model
WikiPeople
JF17K
MRR
Hit@1
Hit@3
Hit@10
MRR
Hit@1
Hit@3
Hit@10"
EXPERIMENTAL SETUP,0.29336734693877553,"NNs
NaLP
0.735
0.595
0.852
0.938
0.825
0.762
0.873
0.927
NeuInfer
0.765
0.686
0.877
0.900
0.861
0.832
0.885
0.910"
EXPERIMENTAL SETUP,0.29591836734693877,"GNNs
StarE
0.800
0.753
0.936
0.951
0.901
0.884
0.929
0.963
G-MPNN
0.777
0.694
0.905
0.912
0.864
0.842
0.883
0.917"
EXPERIMENTAL SETUP,0.29846938775510207,"Search
S2S
0.813
0.744
0.928
0.960
0.912
0.877
0.932
0.951
MSeaHKG
0.831
0.787
0.955
0.972
0.933
0.894
0.950
0.972"
EXPERIMENTAL SETUP,0.3010204081632653,"Baselines. Except for GNNs in Sec. 2.2, we present key functions of most adopted baselines in Tab. 5.
For tasks on HKGs, we mainly compare the proposed method with advanced HKG embedding models:
(1) Geometric model: RAE (Zhang et al., 2018), which is upgrade version of m-TransH (Wen et al.,
2016) that extended from TransH (Wang et al., 2014); (2) GNN-based models: G-MPNN (Yadati,
2020) and StarE (Galkin et al., 2020) (note that we re-implement and tune them because G-MPNN
is under inductive settings and StarE only tests the performance of main triplets in hyper-relational
facts); (3) Other NN-based models: NaLP (Guan et al., 2019), HINGE (Rosso et al., 2020), and
NeuInfer (Guan et al., 2020); (4) Multi-linear models: The ﬁnal score of HypE (Fatemi et al., 2020)
and RAM (Liu et al., 2021) is computed by multi-way inner product which is extended from bilinear
KG embedding models (Yang et al., 2015). (5) Search method: S2S (Di et al., 2021) proposes a search
space for tensor decomposition models and searches for sparse core tensors. It shares embeddings to
jointly learn from facts with mixed arities to alleviate the issue of tensor modeling."
EXPERIMENTAL SETUP,0.30357142857142855,"Except for the above methods on HKGs, we also include the tensor decomposition models (n-CP,
n-TuckER, GETD) that work well on the scenario of facts with ﬁxed arity. n-CP (Lacroix et al., 2018)
leverages CANDECOMP/PARAFAC decomposition (Hitchcock, 1927), while n-TuckER (Balazevic
et al., 2019) is based on Tucker decomposition (Tucker, 1966). GETD (Liu et al., 2020) proposes to
reduce the model complexity by tensor ring decomposition."
MAIN EXPERIMENTAL RESULTS,0.30612244897959184,"4.2
MAIN EXPERIMENTAL RESULTS"
MAIN EXPERIMENTAL RESULTS,0.3086734693877551,"The main results on HKGs (i.e., WikiPeople and JF17K) have been summarized into Tab. 2 and Tab. 3.
Compared with Geometric and classic NN-based methods (e.g., NaLP), GNNs methods (e.g., StarE)
achieve outstanding performance, which demonstrates the power of GNNs on the graph tasks. And
StarE generally is better than G-MPNN in GNNs methods because the inner product way in G-MPNN
cannot handle several relational patterns as mentioned in Sec. 1. Besides, although the multi-linear
method RAM utilizes the simple inner product as its scoring function, it carefully models the role
semantic information and interaction patterns, thus achieving good performance. Another searching
method S2S alleviates the extension issue of tensor decomposition models from the ﬁxed to the mixed
scenario. It is still slightly inferior compared with other state-of-the-art methods. Overall, all existing
methods cannot consistently achieve the leading performance on different tasks and data sets. In this"
MAIN EXPERIMENTAL RESULTS,0.3112244897959184,Under review as a conference paper at ICLR 2022
MAIN EXPERIMENTAL RESULTS,0.3137755102040816,"Table 4: The model comparison of the link prediction task on facts with ﬁxed arity. The results of
tensor models are copied from Liu et al. (2020), others are copied from Di et al. (2021)."
MAIN EXPERIMENTAL RESULTS,0.3163265306122449,"type
model
WikiPeople-3
JF17K-3
WikiPeople-4
JF17K-4
MRR
H@10
MRR
H@10
MRR
H@10
MRR
H@10
Geo
RAE
0.239
0.379
0.505
0.644
0.150
0.273
0.707
0.835 NNs"
MAIN EXPERIMENTAL RESULTS,0.31887755102040816,"NaLP
0.301
0.508
0.515
0.679
0.342
0.540
0.719
0.805
HINGE
0.338
0.508
0.587
0.738
0.352
0.557
0.745
0.842
NeuInfer
0.355
0.521
0.622
0.770
0.361
0.566
0.765
0.871"
MAIN EXPERIMENTAL RESULTS,0.32142857142857145,Tensor
MAIN EXPERIMENTAL RESULTS,0.3239795918367347,"n-CP
0.330
0.496
0.700
0.827
0.265
0.445
0.787
0.890
n-TuckER
0.365
0.548
0.727
0.852
0.362
0.570
0.804
0.902
GETD
0.373
0.558
0.732
0.856
0.386
0.596
0.810
0.913"
MAIN EXPERIMENTAL RESULTS,0.32653061224489793,"Search
S2S
0.386
0.559
0.740
0.860
0.391
0.600
0.822
0.924
MSeaHKG
0.405
0.583
0.757
0.892
0.412
0.628
0.834
0.940"
MAIN EXPERIMENTAL RESULTS,0.32908163265306123,"paper, MSeaHKG pursues the high model performance by dynamically designing the most suitable
message function for the given HKG and task. The searched message functions can capture data-level
properties (see case study in Appx. D.2), thereby showing the leading performance. Especially,
the search space of another search method S2S is based on the tensor modeling. As discussed in
Sec. 1 and Appx. A, the MRHG could be a more natural way to represent HKGs, thereby MSeaHKG
beneﬁts from building a message function search space in GNNs under the MRHG modeling."
MAIN EXPERIMENTAL RESULTS,0.33163265306122447,"We show experiments on facts with ﬁxed arity in Tab. 4. As discussed in Sec. 1, we can indeed
observe that classic tensor decomposition models (n-CP, n-TuckER, GETD) perform better than
Geometric and NN-based methods on the scenario of ﬁxed arity. Then, S2S proposes to dynamically
sparsify the core tensor of tensor decomposition models for the given data and further improve the
performance of tensor models. Moreover, we note that MSeaHKG still performs better than S2S
even in the scenario of ﬁxed arity. That is because S2S simply assumes 3 relationships between
entities and relations in the search space: positive, irrelevant, and negative. But the message function
space in Sec. 3.1 could characterize more complex interactions between entities and relations, thereby
achieving improvements."
MORE INSIGHTS VIA EMPIRICAL STUDY,0.33418367346938777,"4.3
MORE INSIGHTS VIA EMPIRICAL STUDY"
MORE INSIGHTS VIA EMPIRICAL STUDY,0.336734693877551,"Due to the space limitation, we include more experimental results in Appx. D to provide more insights
and verify several claims in this paper. First, we demonstrate case studies in Appx. D.2 that the
searched message functions are data-dependent and can adapt to the given data set. Second, we
conduct ablation studies to analyze the components of the proposed method in Appx. D.3. Speciﬁcally,
we show several variants of the proposed search space to demonstrate: (1) with a simple extension of
message function, the GNN searching method cannot work well on HKGs (e.g., AutoGEL discussed
in Sec. 2.2), (2) automatic operation selection can improve the performance built on the manual
operation tuning, (3) the structure design of message functions is important to improve performance
on HKGs. Then, we implement two more popular NAS algorithms (Liu et al., 2018; Akimoto et al.,
2019) to compare the searching effectiveness and efﬁciency of one-shot NAS search algorithms in
our scenarios. Third, we transfer MSeaHKG to more variety of graph-based tasks in Appx. D.4."
CONCLUSION,0.3392857142857143,"5
CONCLUSION"
CONCLUSION,0.34183673469387754,"In this paper, we propose a new message function searching method for HKGs, named MSeaHKG.
First, we present a novel search space of message functions in MPNNs, which enables both structure
search and operation selection in message functions. With our expressive message function design,
some classic KGs/HKGs models and existing message functions for HKGs could be instantiated
as special cases of the proposed space. Then, we develop an efﬁcient one-shot NAS algorithm to
search the message function and other GNN components for the given HKG. The empirical study
demonstrates that the searched message functions are data-dependent and can adapt to the data
patterns of the given HKGs. Overall, MSeaHKG has shown its effectiveness and efﬁciency on
benchmark data sets."
CONCLUSION,0.34438775510204084,Under review as a conference paper at ICLR 2022
REFERENCES,0.3469387755102041,REFERENCES
REFERENCES,0.3494897959183674,"Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari, Kento Uchida, Shota Saito, and Kouhei
Nishida. Adaptive stochastic natural gradient method for one-shot neural architecture search. In
ICML, 2019."
REFERENCES,0.3520408163265306,"S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
Dbpedia: A nucleus for a web of open data. In The Semantic Web, pp. 722–735. Springer, 2007."
REFERENCES,0.35459183673469385,"I. Balazevic, C. Allen, and T. Hospedales. Tucker: Tensor factorization for knowledge graph
completion. In EMNLP, pp. 5188–5197, 2019."
REFERENCES,0.35714285714285715,"J. Bergstra, D. Yamins, and D. D. Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. 2013."
REFERENCES,0.3596938775510204,"Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabo-
ratively created graph database for structuring human knowledge. In SIGMOD, pp. 1247–1250,
2008."
REFERENCES,0.3622448979591837,"A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings for
modeling multi-relational data. In NIPS, pp. 2787–2795, 2013."
REFERENCES,0.3647959183673469,"Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. Unifying knowledge graph
learning and recommendation: Towards a better understanding of user preferences. In WWW, pp.
151–161, 2019."
REFERENCES,0.3673469387755102,"Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. Fair darts: Eliminating unfair advantages
in differentiable architecture search. In ECCV, pp. 465–480. Springer, 2020."
REFERENCES,0.36989795918367346,"Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
knowledge graph embeddings. In AAAI, 2018."
REFERENCES,0.37244897959183676,"Shimin Di, Quanming Yao, and Lei Chen. Searching to sparsify tensor decomposition for n-ary
relational data. In WebConf, pp. 4043–4054, 2021."
REFERENCES,0.375,"Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey. J."
REFERENCES,0.37755102040816324,"Mach. Learn. Res., 20(55):1–21, 2019."
REFERENCES,0.38010204081632654,"Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimiza-
tion at scale. In ICML, pp. 1437–1446. PMLR, 2018."
REFERENCES,0.3826530612244898,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In WWW, pp. 417–426, 2019."
REFERENCES,0.3852040816326531,"Bahare Fatemi, Perouz Taslakian, David Vazquez, and David Poole. Knowledge hypergraphs:
Prediction beyond binary relations. 2020."
REFERENCES,0.3877551020408163,"Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message
passing for hyper-relational knowledge graphs. In EMNLP, 2020."
REFERENCES,0.3903061224489796,"Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In"
REFERENCES,0.39285714285714285,"IJCAI, volume 20, pp. 1403–1409, 2020."
REFERENCES,0.39540816326530615,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, pp. 1263–1272. PMLR, 2017."
REFERENCES,0.3979591836734694,"Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational
data. In WWW, pp. 583–593, 2019."
REFERENCES,0.4005102040816326,"Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng. Neuinfer: Knowledge
inference on n-ary facts. In ACL, pp. 6141–6151, 2020."
REFERENCES,0.4030612244897959,"William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeurIPS, pp. 1025–1035, 2017."
REFERENCES,0.40561224489795916,Under review as a conference paper at ICLR 2022
REFERENCES,0.40816326530612246,Frank L Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of
REFERENCES,0.4107142857142857,"Mathematics and Physics, 6(1-4):164–189, 1927."
REFERENCES,0.413265306122449,"F. Hutter, L. Kotthoff, and J. Vanschoren.
Automated Machine Learning: Methods, Systems,
Challenges. Springer, 2018."
REFERENCES,0.41581632653061223,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv"
REFERENCES,0.41836734693877553,"preprint arXiv:1611.01144, 2016."
REFERENCES,0.42091836734693877,"Shengli Jiang and Prasanna Balaprakash. Graph neural network architecture search for molecular
property prediction. In IEEE Big Data, pp. 1346–1353. IEEE, 2020."
REFERENCES,0.42346938775510207,"S. Kazemi and D. Poole. Simple embedding for link prediction in knowledge graphs. In NeurIPS, pp.
4284–4295, 2018."
REFERENCES,0.4260204081632653,"D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2014."
REFERENCES,0.42857142857142855,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In ICLR, 2016."
REFERENCES,0.43112244897959184,"T. Lacroix, N. Usunier, and G. Obozinski. Canonical tensor decomposition for knowledge base
completion. In ICML, pp. 2863–2872, 2018."
REFERENCES,0.4336734693877551,"H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable architecture search. In ICLR, 2018."
REFERENCES,0.4362244897959184,"Yu Liu, Quanming Yao, and Yong Li. Generalizing tensor decomposition for n-ary relational
knowledge bases. In WebConf, pp. 1104–1114, 2020."
REFERENCES,0.4387755102040816,"Yu Liu, Quanming Yao, and Yong Li. Role-aware modeling for n-ary relational knowledge bases. In"
REFERENCES,0.4413265306122449,"WebConf, pp. 2660–2671, 2021."
REFERENCES,0.44387755102040816,"Denis Lukovnikov, Asja Fischer, Jens Lehmann, and S¨oren Auer. Neural network-based question
answering over knowledge graphs on word and character level. In WWW, pp. 1211–1220, 2017."
REFERENCES,0.44642857142857145,"Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016."
REFERENCES,0.4489795918367347,"Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Rufﬁnelli, Rainer Gemulla, and Heiner
Stuckenschmidt. Fine-grained evaluation of rule-and embedding-based systems for knowledge
graph completion. In ISWC, pp. 3–20, 2018."
REFERENCES,0.45153061224489793,"Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33, 2015."
REFERENCES,0.45408163265306123,"Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge
graphs. In AAAI, 2016."
REFERENCES,0.45663265306122447,"A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS,
pp. 8024–8035, 2019."
REFERENCES,0.45918367346938777,"H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efﬁcient neural architecture search via parameter
sharing. In ICML, pp. 4092–4101, 2018."
REFERENCES,0.461734693877551,"Andrea Rossi, Denilson Barbosa, Donatella Firmani, Antonio Matinata, and Paolo Merialdo. Knowl-
edge graph embedding for link prediction: A comparative analysis. TKDD, 15(2):1–49, 2021."
REFERENCES,0.4642857142857143,"Paolo Rosso, Dingqi Yang, and Philippe Cudr´e-Mauroux. Beyond triplets: hyper-relational knowledge
graph embedding for link prediction. In WebConf, pp. 1885–1896, 2020."
REFERENCES,0.46683673469387754,"Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In ESWC, pp. 593–607.
Springer, 2018."
REFERENCES,0.46938775510204084,Under review as a conference paper at ICLR 2022
REFERENCES,0.4719387755102041,"DI Shimin, YAO Quanming, Yongqi ZHANG, and CHEN Lei. Efﬁcient relation-aware scoring
function search for knowledge graph embedding. In ICDE, pp. 1104–1115. IEEE, 2021."
REFERENCES,0.4744897959183674,"Z. Sun, Z. Deng, J. Nie, and J. Tang. RotatE: Knowledge graph embedding by relational rotation in
complex space. In ICLR, 2019."
REFERENCES,0.4770408163265306,"Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by
relational rotation in complex space. In ICLR, 2018."
REFERENCES,0.47959183673469385,K. Toutanova and D. Chen. Observed versus latent features for knowledge base and text inference. In
REFERENCES,0.48214285714285715,"Workshop on CVSMC, pp. 57–66, 2015."
REFERENCES,0.4846938775510204,"T. Trouillon, Christopher R., ´E. Gaussier, J. Welbl, S. Riedel, and G. Bouchard. Knowledge graph
completion via complex tensor factorization. JMLR, 18(1):4735–4772, 2017."
REFERENCES,0.4872448979591837,"Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279–311, 1966."
REFERENCES,0.4897959183673469,"Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-
relational graph convolutional networks. In ICLR, 2020."
REFERENCES,0.4923469387755102,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998–6008, 2017."
REFERENCES,0.49489795918367346,"Ellen Voorhees. The trec-8 question answering track report. In TREC, volume 99, pp. 77–82, 1999."
REFERENCES,0.49744897959183676,"Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph embedding: A survey of approaches and
applications. TKDE, 29(12):2724–2743, 2017."
REFERENCES,0.5,"Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In AAAI, 2014."
REFERENCES,0.5025510204081632,"Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He.
Pooling architecture search for
graph classiﬁcation. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management, pp. 2091–2100, 2021."
REFERENCES,0.5051020408163265,"Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and
embedding of knowledge bases beyond binary relations. In IJCAI, 2016."
REFERENCES,0.5076530612244898,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. ML, 8(3-4):229–256, 1992."
REFERENCES,0.5102040816326531,"Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search."
REFERENCES,0.5127551020408163,"arXiv preprint arXiv:1812.09926, 2018."
REFERENCES,0.5153061224489796,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2018."
REFERENCES,0.5178571428571429,Naganand Yadati. Neural message passing for multi-relational ordered and recursive hypergraphs.
REFERENCES,0.5204081632653061,"Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5229591836734694,"B. Yang, W. Yih, X. He, J. Gao, and L. Deng. Embedding entities and relations for learning and
inference in knowledge bases. In ICLR, 2015."
REFERENCES,0.5255102040816326,"Quanming Yao and Mengshuo Wang. Taking human out of learning applications: A survey on
automated machine learning. Technical report, arXiv:1810.13306, 2018."
REFERENCES,0.5280612244897959,"Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances in"
REFERENCES,0.5306122448979592,"Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5331632653061225,"Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Un-
derstanding and robustifying differentiable architecture search. arXiv preprint arXiv:1909.09656,
2019."
REFERENCES,0.5357142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.5382653061224489,"(a) KG with Multi-relational Graph Modeling.
(b) HKG with Multi-relational Hypergraph Modeling."
REFERENCES,0.5408163265306123,"Figure 3: The illustration to KGs and HKGs. The binary facts are simpliﬁed in (b) to make the 3-ary
facts look more prominent."
REFERENCES,0.5433673469387755,"Richong Zhang, Junpeng Li, Jiajie Mei, and Yongyi Mao. Scalable instance reconstruction in
knowledge bases via relatedness afﬁliated embedding. In WWW, pp. 1185–1194, 2018."
REFERENCES,0.5459183673469388,"Yongqi Zhang, Quanming Yao, Wenyuan Dai, and Lei Chen. AutoSF: Searching scoring functions
for knowledge graph embedding. In ICDE, pp. 433–444, 2020."
REFERENCES,0.548469387755102,"Ziwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey. arXiv"
REFERENCES,0.5510204081632653,"preprint arXiv:2103.00742, 2021."
REFERENCES,0.5535714285714286,"Huan Zhao, Quanming Yao, and Weiwei Tu. Search to aggregate neighborhood for graph neural
network. arXiv preprint arXiv:2104.06608, 2021."
REFERENCES,0.5561224489795918,"Wang Zhili, Di Shimin, and Chen Lei. Autogel: An automated graph neural network with explicit
link information. In NeurIPS, 2021."
REFERENCES,0.5586734693877551,"Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of
graph neural networks. arXiv preprint arXiv:1909.03184, 2019."
REFERENCES,0.5612244897959183,"A
DISCUSSION ON THE MODELING OF HKG"
REFERENCES,0.5637755102040817,"Given a set of facts with the ﬁxed arity {r(e1, . . . , en)}, tensor models (e.g., CP (Lacroix et al., 2018)
and TuckER (Balazevic et al., 2019)) use a (n+1)-dimensional tensor Gn+1 ∈{0, 1}|R|×|E|×···×|E|"
REFERENCES,0.5663265306122449,"to represent facts, where Gn+1
r,1,...,n = 1 represents the fact r(e1, . . . , en) existed otherwise Gn+1
r,1,...,n =
0. Then, tensor models leverage the tensor decomposition techniques to decompose Gn+1 into
embeddings R and E with a core tensor Zn+1 ∈Rd×···×d:"
REFERENCES,0.5688775510204082,Gn+1 = Zn+1 ×1 R ×2 E ×3 · · · ×n+1 E.
REFERENCES,0.5714285714285714,"Obviously, a tensor Gn+1 can only model facts with the ﬁxed arity n. Thus, we have to build
{G2, . . . , GN+1} to model the facts with mixed arities S = {r(e1, . . . , en) : n ∈{2, . . . , N}} and
decompose them into N −1 sets of entity and relation embeddings, such as {En, Rn}N
n=2. But KBs
are known to have the data sparse issue, thus learning multiple set of embeddings could lead to severe
problems."
REFERENCES,0.5739795918367347,"As mentioned in Sec. 1, multi-relational hypergraphs (MRHGs) G(E, R, S) could be a more natural
way to represent HKGs. We illustrate examples of KGs and HKGs in Fig. 3 (a) and (b), respectively.
Given a fact with high arity (e.g., playCharacterIn(e1,e3,e5)), MRHG can ﬁrst store the
correspondence between multiple entities by a hyperedge (e1, e3, e5), then label this hyperedge with
the relation playCharacterIn. Moreover, Fig. 3 (b) demonstrates that the facts with mixed
arities (n ∈{2, 3} in this example) can be represented by a MRHG. Thus, MRHG allows us to
directly encode a given HKG into entity and relation embeddings, where every embedding vector is
jointly learned from low and high arities. However, tensor modeling leads the model to learn multiple
sets of embeddings."
REFERENCES,0.576530612244898,Under review as a conference paper at ICLR 2022
REFERENCES,0.5790816326530612,"(a) HolE (Nickel et al.,
2016)."
REFERENCES,0.5816326530612245,"(b) RotatE (Sun et al.,
2019). ..."
REFERENCES,0.5841836734693877,"(c) n-CP (Lacroix et al.,
2018). ..."
REFERENCES,0.5867346938775511,"(d) n-DistMult (Yang
et al., 2015)."
REFERENCES,0.5892857142857143,"Figure 4: Several instantiation cases of MSeaHKG message function space. ◦denotes circular
correlation corr(·) (Nickel et al., 2016), ∗denotes inner product mult(·) (Yang et al., 2015), −
denotes the substraction sub(·) (Bordes et al., 2013), ⋆is from RotatE (Sun et al., 2018)."
REFERENCES,0.5918367346938775,"Besides, HKGs generally contain more complete information compared with KGs. For example, it is
hard to answer “who plays Spock in StarTrek-3?” with the KG example (Fig. 3 (a)). That is because
KGs suffer from the information loss of correspondence among multiple entities. Generally, the
constructing procedure of KG, named Star-to-Clique Conversion, has been veriﬁed to be irreversible
and caused the information loss in the facts with high arities (Wen et al., 2016). Therefore, we believe
that HKG could be a potential solution to solve complex Q&A scenarios."
REFERENCES,0.5943877551020408,"B
DISCUSSION ON DATA-LEVEL PROPERTIES OF KGS/HKGS AND MESSAGE
FUNCTION DESIGN"
REFERENCES,0.5969387755102041,"B.1
DATA-LEVEL PROPERTIES IN KGS: RELATIONAL PATTERNS"
REFERENCES,0.5994897959183674,"In the past decades, the research community has found that the relations of KGs exhibit differ-
ent patterns.
For example, we can infer that neighborOf(location2,location1)
must
be
true
as
long
as
neighborOf(location1,location2)
is
true,
like
neighborOf(ShenZhen,HongKong). But for other relations (e.g., largerThan), we know
largerThan(value2,value1) must be false if largerThan(value1,value2) is true.
Many relational patterns in KGs have been found and discussed (Kazemi & Poole, 2018; Rossi et al.,
2021), such as symmetry, anti-symmetry, asymmetry, inversion, composition, hierarchy, intersection.
However, various KGs usually contain different relational patterns with different proportions of
facts. To handle diverse relational patterns on different KGs (i.e., data-level properties), kinds of
scoring functions have been proposed to cover as many relational patterns as possible, i.e., pursuing
expressiveness. Given the learned embeddings ω = {E, R}, the scoring function f(·) is proposed to
evaluate whether the fact is true or false (check Appx. C.2 and Tab. 5 for more details about scoring
function). For the sake of understanding, we here present the requirements of several relational
patterns on scoring functions:"
REFERENCES,0.6020408163265306,"• Symmetry: r is symmetry if r(ej, ei) must be true when r(ei, ej) is true, i.e., f(r(ei, ej)) =
f(r(ej, ei))"
REFERENCES,0.6045918367346939,"• Anti-symmetry: r is anti-symmetric if r(ej, ei) must be false when r(ei, ej) is true, i.e.,
f(r(ei, ej)) = −f(r(ej, ei))."
REFERENCES,0.6071428571428571,"• Asymmetry: The plausibility of r(ej, ei) is unknown when r(ei, ej) is true, f(r(ei, ej)) ̸=
f(r(ej, ei))."
REFERENCES,0.6096938775510204,"• Inverse: r1 and r2 are inverse relations if r1(ei, ej) is true and r2(ej, ei) is true.
Then,
f(r1(ei, ej)) = f(r2(ej, ei))"
REFERENCES,0.6122448979591837,"As mentioned in Sec. 1, the message function of G-MPNN may not be able to infer the non-symmetric
relations. Suppose that s1 = r(e1, e2) is true and r is one of any non-symmetric relations (e.g., anti-
symmetry s2 = r(e2, e1) must be false). Let hi be the entity feature and hr be the relation feature.
According to the message function of G-MPNN in Eq. 4, we can learn the hidden embeddings of mi:
m1 = m2 = hr ∗h1 ∗pe1,s ∗h2 ∗pe2,s Then ei are updated with mi. With the scoring function"
REFERENCES,0.6147959183673469,Under review as a conference paper at ICLR 2022
REFERENCES,0.6173469387755102,Table 5: Scoring Functions for Facts with High-order Arities.
REFERENCES,0.6198979591836735,"Type
Model
Scoring Function Design f(s; ω)
Notes Geo"
REFERENCES,0.6224489795918368,m-TransH
REFERENCES,0.625,"Pn
i=1 aj(ei −v⊤
r eivr) + r
2
vr is relation dependent."
REFERENCES,0.6275510204081632,"RAE
Pn
i=1 aj(ei −v⊤
r eivr) + r
2 + P"
REFERENCES,0.6301020408163265,"ij FC([ei, ej])
FC(·) is fully connected
layer."
REFERENCES,0.6326530612244898,Tensor
REFERENCES,0.6352040816326531,"n-CP
r ∗e(1)
1
∗e(2)
2
∗· · · ∗e(n)
n"
REFERENCES,0.6377551020408163,"e(j)
i
is for ei in j-th po-
sition."
REFERENCES,0.6403061224489796,"n-TuckER
Z ×1 r ×2 e1 ×3 · · · ×n+1 en
Z ∈Rd×···×d, ×i is ten-
sor product in i-th mode."
REFERENCES,0.6428571428571429,"GETD
TR(W1, . . . , Wk) ×1 e1 ×2 e2 ×3 · · · ×n+1 en"
REFERENCES,0.6454081632653061,"TR(·) is tensor ring de-
composition Wi is a 3-
order tensor."
REFERENCES,0.6479591836734694,"GNNs
G-MPNN
rs,P (s) ∗Q"
REFERENCES,0.6505102040816326,"ej∈Seen(s) pej,s ∗ej
Seen(s)
returns
the
seen entities ej in s"
REFERENCES,0.6530612244897959,"Multi-
linear"
REFERENCES,0.6556122448979592,"HypE
r ∗Conv(e1) ∗Conv(e2) ∗· · · ∗Conv(en)
Conv(·) is the convolu-
tional ﬁlter."
REFERENCES,0.6581632653061225,"RAM
Pn
i=1 ri ∗pr
i (1, :)¯e1 ∗· · · ∗pr
i (n, :)¯en"
REFERENCES,0.6607142857142857,"ri is embedding for i-th
role of r, ¯ej ∈Rm×d is
embedding matrix of e"
REFERENCES,0.6632653061224489,"Search
S2S
P
jr,j1,...,js Zn
i ×1 rjr ×2 ej1
1 ×3 · · · ×n+1 ejn
n"
REFERENCES,0.6658163265306123,"Zn
i
is
the
searched
sparse core tensor,
i
denotes (jr, j1, . . . , js)."
REFERENCES,0.6683673469387755,"of G-MPNN (multiple inner product), we can compute f(r(e1, e2)) = hr ∗pe1,s ∗e1 ∗pe2,s ∗e2.
Because e1, e2 are seen in s1, G-MPNN will assign the positional embeddings e1, e2 in s1 when
computing the score of s2, thereby leading f(r(e1, e2)) = f(r(e2, e1)). Thus, we argue that the
ﬁxed message function is not ﬂexible enough to capture the relational patterns of the given HKGs,
especially its performance may not be good if some uncovered relational patterns exist in the given
data set."
REFERENCES,0.6709183673469388,"B.2
DISCUSSION ON THE CAPABILITY OF MESSAGE FUNCTIONS TO COVER RELATIONAL
PATTERNS ON HKGS"
REFERENCES,0.673469387755102,"As mentioned in Appx. B.1, pursuing expressiveness is one way to cover relational patterns on KGs.
Such potential solution is also discussed in Sec. 1. However, Meilicke et al. (2018); Rossi et al. (2021)
report that being expressive does not mean achieving good performance even the model can cover
those relational patterns. Thus, AutoSF (Zhang et al., 2020) proposes to search bilinear-based scoring
functions for KGs and consistently achieve good performance. Inspired by literature, MSeaHKG
proposes to search proper message functions for the pursuit of high model performance on any given
HKGs."
REFERENCES,0.6760204081632653,"Generally, the relational patterns on HKGs have not been explored before. Inspired by literature on
KGs, we roughly summarize a high level representations of relational patterns in the scenario of high
arity as:
f(r1(pemu(e1, . . . , en)); ω)?f(r2(pemu(e1, . . . , en)); ω),"
REFERENCES,0.6785714285714286,"where r1, r2 may be same relation or different, pemu(·) represents the permutation of n
entities {ei}n
i=1, and ?
∈
{=, >, ̸=, ¬}.
For example, f(r1(pemu(e1, . . . , en)); ω)
=
f(r1(pemu(e1, . . . , en)); ω) indicates r1 is a symmetry. In this paper, since our scoring func-
tion design is a linear transformation (see Appx. C.2), the design of message function in MPNNs (i.e.,
ω = XΘ(G)) will be the crucial component to capture relational patterns. To capture the permutation
information pemu(·), we ﬁrst utilize WP (e) to encode the positional information, which transforms
entity embeddings into corresponding positions. Then, the operator γ(·) concatenates the entity
embeddings after encoding positional information. Lastly, the operator φ(·) computes the interaction
between r with ei. In other words, WP (e) and γ(·) are employed to represent pemu(·), and φ(·) is
utilized to represent the correlation between r and pemu(·). Within such design, most classic scoring
function designs (including Geometric models (Bordes et al., 2013; Nickel et al., 2016; Sun et al.,
2019), Bilinear models (Yang et al., 2015) and Tensor models (Lacroix et al., 2018)) and GNNs"
REFERENCES,0.6811224489795918,Under review as a conference paper at ICLR 2022
REFERENCES,0.6836734693877551,"for KGs/HKGs can be instantiated as special cases in the MSeaHKG space. We have demonstrated
several examples of classic scoring function designs in Fig. 4. Note that the expressiveness of these
models has been fully investigated in literature. Every model can cover one or multiple relational
patterns, which can make our message function space expressive enough to handle most HKGs. In
other words, the search space of MSeaHKG has the potential to return a GNN model that can adapt
to the relational patterns of the given HKG."
REFERENCES,0.6862244897959183,"C
SUPPLEMENTARY OF SEARCH ALGORITHM"
REFERENCES,0.6887755102040817,"C.1
THE PARAMETERIZATION OF OTHER GNN COMPONENTS"
REFERENCES,0.6913265306122449,"Generally, the relaxation of other GNN components (e.g., aggregation and activation) is consistent
with φk(h) = P θφk
i oi(h) and γk(h) = P θγk
i oi(h) as presented in Sec. 3.1. Here we illustrate
more details."
REFERENCES,0.6938775510204082,"• Let Oagg
=
{sum, mean, max} be the set of candidate aggregation functions and
mg(r(e1, . . . , en); θmg) be the output from Eq. 7. Then, the aggregation function agg(·; θagg)
parameterized by θagg can be deﬁned as:"
REFERENCES,0.6964285714285714,"mi = agg({mg(r(e1, . . . , en); θmg)}r(e1,...,en)∈I(ei); θagg) =
X"
REFERENCES,0.6989795918367347,"oj∈Oagg
θagg
j
· oj({mg(r(e1, . . . , en); θmg)}r(e1,...,en)∈I(ei))."
REFERENCES,0.701530612244898,"• Let Oact = {identity, sigmoid, tanh} be the set of candidate activation functions. Then, the
activation function act(·; θact) parameterized by θact can be deﬁned as:"
REFERENCES,0.7040816326530612,"ei = act(comb(ei, mi); θact) =
X"
REFERENCES,0.7066326530612245,"oj∈Oact
θact
j
· oj(comb(ei, mi))"
REFERENCES,0.7091836734693877,"Then, as presented in Sec. 3.2, we can relax θagg and θact into continuous space ¯θagg and ¯θact by
leveraging the Gumbel-Softmax technique."
REFERENCES,0.7117346938775511,"C.2
SCORING FUNCTION FORMULATION"
REFERENCES,0.7142857142857143,"As mentioned in Sec. 3.2, the GNN model X embeds the HKG G(E, R, S) into the low-dimensional
vector space ω = {E, R}, i.e., ω = X(G). Besides, it is also important to design a scoring function
f(s; ω) to interprets the plausibility of the fact s based on the learned ω (Rossi et al., 2021; Zhang
et al., 2020; Shimin et al., 2021). Many promising scoring functions have been proposed in the
past decades, including geometric models (e.g., TransE (Bordes et al., 2013) and RotatE (Sun et al.,
2018)), neural network models (e.g., ConvE (Dettmers et al., 2018)), tensor decomposition models
(e.g., CP (Lacroix et al., 2018), TuckER (Balazevic et al., 2019)). Inspired by the success of literature,
we regard the scoring function design as a component of implementations. In this paper, we focus on
those scoring functions for facts with high arity (n > 2) and summarize the popular ones in Tab. 5.
Note that StarE (Galkin et al., 2020) employs the Transformer (Vaswani et al., 2017) as its scoring
function, thus it is not included in Tab. 5."
REFERENCES,0.7168367346938775,"In principle, MSeaHKG can implement most of existing scoring functions as its decoder. But we
argue that the power of the searched message function could well model the interaction between
entities and relations in the encoding step. Within a powerful encoder, it may be unnecessary to
introduce a complex scoring function like CompGCN (Vashishth et al., 2020) and StarE (Galkin et al.,
2020) to decode embeddings. In this paper, we simply concatenate the embeddings of known entities
and relations in a fact and feed it into the linear transformation with a softmax operator."
REFERENCES,0.7193877551020408,"C.3
LOSS FUNCTION W.R.T. RELATION PREDICTION"
REFERENCES,0.7219387755102041,"Note that the loss function L(X, ω; G) introduced in Sec. 3.2 is under the scenario of link prediction
task. In the relation classiﬁcation task, the probability score is formed to ps ∈[0, 1]|R|, i.e., predicting"
REFERENCES,0.7244897959183674,Under review as a conference paper at ICLR 2022
REFERENCES,0.7270408163265306,"the ground truth r ∈R when the relation is missing in ?(e1, . . . , e2). The loss function is deﬁned as:"
REFERENCES,0.7295918367346939,"L(X, ω; G) =
X s∈S X"
REFERENCES,0.7321428571428571,"r∈R
ys
r log ps
r,"
REFERENCES,0.7346938775510204,"where ys
r = 1 if r is the ground truth relation in s."
REFERENCES,0.7372448979591837,"C.4
FULL DERIVATION INVOLVED IN SEC. 3.2"
REFERENCES,0.7397959183673469,"First, we present the full derivation of Eq. 10 and Eq. 11, and their approximation based on Monte-
Carlo sampling:"
REFERENCES,0.7423469387755102,"∇¯ΘEX∼p ¯
Θ(X)[L(X, ω; G)] = ∇¯ΘEU∼p(U)[L(g ¯Θ(U), ω; G)] = ∇¯Θ"
REFERENCES,0.7448979591836735,"Z
p(U)L(g ¯Θ(U), ω; G)dU"
REFERENCES,0.7474489795918368,"=
Z
p(U)∇¯ΘL(g ¯Θ(U), ω; G)dU = EU∼p(U)[∇¯ΘL(g ¯Θ(U), ω; G)]"
REFERENCES,0.75,"= EU∼p(U)[L′(g ¯Θ(U), ω; G)∇¯Θg ¯Θ(U)],"
REFERENCES,0.7525510204081632,"∇ωEX∼p ¯
Θ(X)[L(X, ω; G)] = ∇ω"
REFERENCES,0.7551020408163265,"Z
p ¯Θ(X)L(X, ω; G)dX =
Z
p ¯Θ(X)∇ωL(X, ω; G)dX"
REFERENCES,0.7576530612244898,"= EX∼p ¯
Θ(X)[∇ωL(X, ω; G)]."
REFERENCES,0.7602040816326531,"Second, we build the reparameterization trick X = g ¯Θ(U) (mentioned in Sec. 3.2) based on Gumbel-
Softmax (Jang et al., 2016) or Concrete distribution (Maddison et al., 2016). For simplicity, we
simplify ¯Θ to the parameter ¯θ for a speciﬁc operator space O:"
REFERENCES,0.7627551020408163,"Xo = g ¯θ(U) =
exp((log ¯θo −log(−log(Uo)))/τ)
P"
REFERENCES,0.7653061224489796,"o′∈O exp((log ¯θo′ −log(−log(Uo′)))/τ),
(12)"
REFERENCES,0.7678571428571429,"where τ is the temperature of softmax, and Uo ∼Uniform(0, 1).
It has been proven that
p(limτ→0 Xo = 1) = ¯θo/ P"
REFERENCES,0.7704081632653061,"o′∈O ¯θo′ making the stochastic differentiable relaxation unbiased
once converged (Xie et al., 2018). And the details of ∇¯Θg ¯Θ(U) can refer to Xie et al. (2018)."
REFERENCES,0.7729591836734694,"D
MORE EXPERIMENTS"
REFERENCES,0.7755102040816326,"D.1
EXPERIMENTAL SETUP"
REFERENCES,0.7780612244897959,"D.1.1
HYPER-PARAMETER SETTINGS"
REFERENCES,0.7806122448979592,"We have summarized the hyper-parameters of this paper in Tab. 6. The hyper-parameter set includes
Adam optimizer (Kingma & Ba, 2014), learning rate ∈{0.1, 0.01, 0.001, 0.0001, 0.00001}, # MPNN
layers ∈{1, 2, 3, 4} (see left part of Fig. 2), # layers in message functions K ∈{1, 2, 3, 4, 5} (see
right part of Fig. 2), batch size ∈{64, 128, 256, 512}, embedding dimension d ∈{64, 128, 256, 512},
dropout ratio ∈{0, 0.05, 0.1, 0.15, . . . , 0.5}, label smoothing ratio τ ∈{0, 0.1, 0.2, . . . , 0.9}. Note
that the label smoothing ratio τ is employed to relax the one-hot label vector y. In practical, we
set yi = 1 −τ for the ground truth entity/relation, while yi = τ/|E|−1 for link prediction and
yi = τ/|R|−1 for relation prediction. That is because there are usually a large candidate space for
relations and entities, while Using one-hot vector is quite restrictive. All hyper-parameters are tuned
with the help of optuna.samplers.TPESampler (Bergstra et al., 2013; Falkner et al., 2018)1."
REFERENCES,0.7831632653061225,"D.1.2
DATA STATISTICS"
REFERENCES,0.7857142857142857,"Here we present the statistic summary of the benchmark data sets in this paper. And the links for
download them."
REFERENCES,0.7882653061224489,"1https://optuna.readthedocs.io/en/stable/reference/generated/optuna.
samplers.TPESampler.html"
REFERENCES,0.7908163265306123,Under review as a conference paper at ICLR 2022
REFERENCES,0.7933673469387755,"Table 6: List of hyper-parameters in main experiments. W and J are abbreviations of WikiPeople and
JF17K, respectively."
REFERENCES,0.7959183673469388,"Hyperparameters
Link Prediction
Relation Prediction
WikiPeople JF17K
W-3
J-3
W-4
J-4
WikiPeople JF17K
Learning rate
0.0001
0.001 0.0001 0.001 0.0001 0.0001
0.0001
0.001
# MPNN layers
2
2
1
1
1
1
2
2
# Layers in mg(·) K
4
4
3
2
2
3
4
4
Batch size
256
128
128
128
256
128
256
128
Embedding dim d
256
256
128
128
256
128
128
128
Dropout ratio
0.15
0.2
0.1
0.1
0.05
0.15
0.2
0.15
Label smoothing ratio τ
0.3
0.8
0.7
0.5
0.8
0.7
0.1
0.1"
REFERENCES,0.798469387755102,Table 7: The statistical summary on data sets.
REFERENCES,0.8010204081632653,"type
data set
# all facts # facts (n > 2) max arity N # ent # rel
train
valid
test"
REFERENCES,0.8035714285714286,"Mixed
JF17K
100,947
46,320
6
28,645 322 76,379
-
24,568
WikiPeople
382,229
44,315
9
47,765 707 305,725 38,223 38,281 Fixed"
REFERENCES,0.8061224489795918,"WN18RR
93,003
0
2
40,943 11
86,835
3,034
3,134
FB15k237
310,116
0
2
14,541 237 272,115 17,535 20,466
JF17K-3
34,544
34,544
3
11,541 104 27,635
3,454
3,455
JF17K-4
9,509
9,509
4
6,536
23
7,607
951
951
WikiPeople-3
25,820
25,820
3
12,270 66
20,656
2,582
2,582
WikiPeople-4
15,188
15,188
4
9,528
50
12,150
1,519
1,519"
REFERENCES,0.8086734693877551,"D.1.3
EVALUATION MEASUREMENT"
REFERENCES,0.8112244897959183,"Let ranki,n denote the rank of ground truth entity at position n of i-th fact s = (r, e1, . . . , eN),
deﬁned as:
ranki,n = |{e′ ∈E \{en} : f(s′; ω) > f(s; ω)}| + 1,
where s′ = (r, e1, . . . , en−1, e′, . . . , eN). As in (Bordes et al., 2013; Wang et al., 2014), we report
performance under the “ﬁltered” setting, i.e., evaluating the rank of test fact after removing all
corrupted facts that exist in the train, valid, and test data set. That is because true facts in data set
should be not considered as faults when evaluating a test fact. Correspondingly, ranki,n under ﬁltered
setting is formed as:"
REFERENCES,0.8137755102040817,"ranki,n = |{e′ ∈E \{en} : f(s′; ω) > f(s; ω) ∩s′ /∈S}| + 1."
REFERENCES,0.8163265306122449,"Then we adopt the classical metrics (Bordes et al., 2013; Wang et al., 2014):"
REFERENCES,0.8188775510204082,"• Mean Reciprocal Ranking (MRR): 1/N|S| P|S|
i=1
PN
n=1 1/ranki,n;"
REFERENCES,0.8214285714285714,"• Hit@1, Hit@3, and Hit@10, where Hit@k is given by 1/|S| P|S|
i=1
PN
n=1 I(ranki,n ≤k) and I(·) is
the indicator function."
REFERENCES,0.8239795918367347,Note that the higher MRR and Hit@k values mean better embedding quality.
REFERENCES,0.826530612244898,"D.2
CASE STUDY"
REFERENCES,0.8290816326530612,"The relational patterns on KGs (see Appx. B) have been fully investigated (Kazemi & Poole, 2018;
Rossi et al., 2021) in existing works. Thus, we ﬁrst show the case studies on two benchmark KGs
WN18RR (Dettmers et al., 2018) and FB15k237 (Toutanova & Chen, 2015) (see Tab. 7 for data
statistics and Tab. 8 for experimental results), which have removed the duplicate and inverse relations
of WN18 and FB15k (Bordes et al., 2013; Dettmers et al., 2018). As presented in Rossi et al. (2021),
the irreﬂexive, anti-symmetric, and symmetric relations account for a major proportion of these two
data sets, especially facts with the symmetric relations reach 37% in WN18RR. We found that the
message function searched on WN18RR (Fig. 5 (a)) is the exact RotatE model (Sun et al., 2019),
which has been proven to cover symmetric relations. As for the message function searched on
FB15k237 (Fig. 5 (b)), it ﬁrst feeds the relation embedding into a transformation matrix W . We infer"
REFERENCES,0.8316326530612245,Under review as a conference paper at ICLR 2022
REFERENCES,0.8341836734693877,"(a) WN18RR.
(b) FB15k237."
REFERENCES,0.8367346938775511,Cat&MLP
REFERENCES,0.8392857142857143,(c) FB15k237.
REFERENCES,0.8418367346938775,Figure 5: Several message functions searched by MSeaHKG on different data sets.
REFERENCES,0.8443877551020408,"Table 8: The model comparison of the link prediction task on KGs. The results of R-GCN and
CompGCN are copied from Vashishth et al. (2020), and others are copied from Rossi et al. (2021)."
REFERENCES,0.8469387755102041,"type
model
FB15k237
WN18RR
MRR
Hit@1
Hit@10
MRR
Hit@1
Hit@10"
REFERENCES,0.8494897959183674,"Geometric
TransE
0.310
0.217
0.497
0.206
0.028
0.495
RotatE
0.336
0.238
0.531
0.475
0.426
0.574
Bilinear
DistMult
0.313
0.224
0.490
0.433
0.397
0.502
Tensor
TuckER
0.352
0.259
0.536
0.459
0.430
0.514"
REFERENCES,0.8520408163265306,"GNNs
R-GCN
0.248
0.151
0.417
-
-
-
CompGCN
0.355
0.264
0.535
0.479
0.443
0.546
Search
MSeaHKG
0.360
0.267
0.545
0.485
0.446
0.554"
REFERENCES,0.8545918367346939,"that is mainly because FB15k237 has 237 relations, which is more than 11 relations on WN18RR. For
the message function searched on facts with high arity, we show an example on JF17K-3 in Fig. 5 (c).
It demonstrates that the message function tends to be deeper and complex in the high arity case. Note
that the full version of message functions searched on WikiPeople (N = 9) and JF17K (N = 6) is
too large to put it on paper. Overall, we can observe that the message functions are data-dependent
from Fig. 5."
REFERENCES,0.8571428571428571,"D.3
ABLATION STUDY"
REFERENCES,0.8596938775510204,"Except for main experimental results, here we report the performance of several variants of MSeaHKG
(see Tab. 9) to investigate some key designs in this paper, including MSeaHKGWr, MSeaHKGop,
MSeaHKGst for the search space, MSeaHKGdarts and MSeaHKGrl for the search algorithm."
REFERENCES,0.8622448979591837,"D.3.1
SEARCH SPACE"
REFERENCES,0.8647959183673469,We ﬁrst present the conﬁguration of variants:
REFERENCES,0.8673469387755102,"• MSeaHKGWr basically enables current GNN searching methods working on HKGs. Inspired by
R-GCN (see Eq. 2), we ﬁrst replace the transform matrix W in mgc(·) (see Eq. 1) to Wr. Then, we
concatenate the entity embeddings as h = concat(e1, . . . , en, 0, . . . , 0). Note that the number of
zero embeddings 0 is equal to nmax −n. We utilize the message function mgc(r(e1, . . . , en)) =
Wrh to replace Eq. 7. Other steps are same with original version."
REFERENCES,0.8698979591836735,"• MSeaHKGop only searches operations of operators φ and γ in mg(·; θ) (i.e., θ = {θφk
i } ∪{θγk
i }),
while keeping the structure of StarE’s message functions. Other steps are same with original
version."
REFERENCES,0.8724489795918368,"• MSeaHKGst only searches structures of the proposed message function mg(·; θ), and sets φ, γ
to corr, wsum respectively (i.e., θ = {θk
ij}). The ﬁxed operations are selected based on better
empirical performance. Other steps are same with original version."
REFERENCES,0.875,Under review as a conference paper at ICLR 2022
REFERENCES,0.8775510204081632,Table 9: The comparison of variants of MSeaHKG in the link prediction task on HKGs.
REFERENCES,0.8801020408163265,"Type
Model
WikiPeople
JF17K
MRR Hit@1 Hit@3 Hit@10 MRR Hit@1 Hit@3 Hit@10"
REFERENCES,0.8826530612244898,"GNNs
StarE
0.378
0.265
0.452
0.542
0.542
0.454
0.580
0.685
G-MPNN
0.367
0.258
0.439
0.526
0.530
0.459
0.572
0.688"
REFERENCES,0.8852040816326531,"Search
S2S
0.372
0.277
0.439
0.533
0.528
0.457
0.570
0.690
MSeaHKG
0.395
0.291
0.470
0.554
0.577
0.481
0.599
0.711"
REFERENCES,0.8877551020408163,"Variants of
space"
REFERENCES,0.8903061224489796,"MSeaHKGWr
0.354
0.233
0.431
0.520
0.512
0.445
0.553
0.671
MSeaHKGop
0.385
0.274
0.460
0.548
0.554
0.468
0.591
0.699
MSeaHKGst
0.391
0.278
0.465
0.552
0.563
0.475
0.602
0.706
Variants of
algorithm"
REFERENCES,0.8928571428571429,"MSeaHKGdarts 0.373
0.275
0.445
0.535
0.554
0.460
0.588
0.697
MSeaHKGrl
0.380
0.281
0.457
0.542
0.563
0.472
0.593
0.701"
REFERENCES,0.8954081632653061,"From Tab. 9, we observe that the simple extension version MSeaHKGWr even cannot achieve as
good performance as existing GNNs (e.g., StarE and G-MPNN). This veriﬁes the claim that the
simple message function in the existing GNN searching method (e.g., AutoGEL (Zhili et al., 2021)
discussed in Sec. 2.2) may not be able to handle the complex correlations between relations and
entities on HKGs. Moreover, MSeaHKGop keeps the same message function structure with StarE
but searches suitable operations. Differ from manually tuning operations in StarE, the automatic
way is more powerful so that MSeaHKGop achieves a minor improvement compared with StarE.
As for MSeaHKGst, it can search for more ﬂexible structures of message functions for the given
HKG and achieve the best performance among several variants. It can illustrate that the message
function design is important to HKG embedding. However, MSeaHKGst is still slightly inferior
compared with the original version of MSeaHKG. This shows that the best structure and operations
are dependent. Simply ﬁxing operations to search the structure may lead to the sub-optimum."
REFERENCES,0.8979591836734694,"D.3.2
SEARCH ALGORITHM"
REFERENCES,0.9005102040816326,"In this paper, we mainly focus on one-shot NAS search algorithms due to their searching efﬁciency
(see more discussion in Sec. 2.1). Existing one-shot NAS algorithms can be roughly categorized
into: stochastic differentiable method (e.g., SNAS (Xie et al., 2018)), deterministic differentiable
method (e.g., DARTS (Liu et al., 2018)), and policy gradient-based method (e.g., ENAS (Pham
et al., 2018), ASNG (Akimoto et al., 2019)). Here, we implement two more variants of MSeaHKG,
MSeaHKGdarts based on DARTS and MSeaHKGrl based on ASNG, to investigate the performance
of other two kinds of NAS search algorithms in our application scenario."
REFERENCES,0.9030612244897959,"• MSeaHKGdarts follows DARTS to directly relax Θ to learnable parameters. Then, the computa-
tion in DAG (see Eq. 6) will be reformed to:"
REFERENCES,0.9056122448979592,"hk
i = |O|
X"
REFERENCES,0.9081632653061225,"j=1
αk
ij · ok
i
 
hk−1
j

, αk
ij =
θk
ij
P|O|
j′=1 θk
ij′
.
(13)"
REFERENCES,0.9107142857142857,"Then, we are able to minimize the loss L(Θ, ω; G) through the gradient ∇ΘL(·)."
REFERENCES,0.9132653061224489,• MSeaHKGrl follows the policy gradient-based NAS search algorithm to derive:
REFERENCES,0.9158163265306123,"∇¯ΘEX∼p ¯
Θ(X)[L(X, ω; G)] = ∇¯Θ"
REFERENCES,0.9183673469387755,"Z
p ¯Θ(X)L(X, ω; G)dX =
Z
L(X, ω; G)∇¯Θp ¯Θ(X)dX"
REFERENCES,0.9209183673469388,"=
Z
L(X, ω; G)∇¯Θ log p ¯Θ(X) · p ¯Θ(X)dX
(14)"
REFERENCES,0.923469387755102,"= ∇¯ΘEX∼p ¯
Θ(X)[L(X, ω; G)∇¯Θ log p ¯Θ(X)],"
REFERENCES,0.9260204081632653,"where ∇¯Θp ¯Θ(X) = log p ¯Θ(X)p ¯Θ(X) is the policy gradient trick (Williams, 1992). In practical,
we utilize ASNG (Akimoto et al., 2019) to instantiate Eq. 14, which implements the ﬁsher
information matrix for fast convergence and adaptive learning rate for robustness."
REFERENCES,0.9285714285714286,We here analyze the model comparison between original MSeaHKG with its variants MSeaHKGrl
REFERENCES,0.9311224489795918,"and MSeaHKGdarts. From Tab. 9 and Tab. 10, we observe that MSeaHKG and MSeaHKGrl have"
REFERENCES,0.9336734693877551,Under review as a conference paper at ICLR 2022
REFERENCES,0.9362244897959183,"Table 10: The training time comparison (in hours) of GNN-based models in the link prediction task
on HKGs."
REFERENCES,0.9387755102040817,"Type
Model
HKGs
Arity=3
Arity=4
WikiPeople
JF17K
W-3
J-3
W-4
J-4"
REFERENCES,0.9413265306122449,"GNNs
G-MPNN
15.4 ± 2.6
4.1 ± 0.3 1.2 ± 0.1 0.6 ± 0.1 0.5 ± 0.1 0.6 ± 0.1
StarE
137.5 ± 6.3 16.7 ± 2.2 2.6 ± 0.4 4.7 ± 0.8 2.3 ± 0.4 1.9 ± 0.2
Search
MSeaHKG
30.9 ± 4.7
7.7 ± 1.8 1.8 ± 0.4 1.4 ± 0.2 0.9 ± 0.1 1.1 ± 0.2
Variants of
algorithm"
REFERENCES,0.9438775510204082,"MSeaHKGdarts 40.2 ± 2.5 11.5 ± 2.2 2.5 ± 0.5 2.1 ± 0.3 1.5 ± 0.2 1.3 ± 0.1
MSeaHKGrl
35.1 ± 3.0 10.3 ± 1.7 2.3 ± 0.3 1.5 ± 0.2 1.4 ± 0.2 1.1 ± 0.1"
REFERENCES,0.9464285714285714,"better performance than MSeaHKGdarts in both effectiveness and efﬁciency comparisons. First,
DARTS aims to train a supernet by mixing all candidate operations during the searching phase, then
it will derive a discrete architecture after ﬁnishing the search. But the weights αk
i in Eq. 13 cannot
cannot converge to a one-hot vector, which lead to performance collapse after removing |O −1|
operations in O (Zela et al., 2019; Chu et al., 2020). Second, it will consume more computational
resources when maintaining all operations during the search. Instead, MSeaHKG and MSeaHKGrl"
REFERENCES,0.9489795918367347,"are to train discrete architectures in searching (the bounded discreteness of MSeaHKG is discussed
in Appx. C.4), which avoids performance collapse and large computational overhead. As for the
comparison between MSeaHKG with MSeaHKGrl, MSeaHKG achieves slight improvements. That
is mainly because MSeaHKG directly calculates the gradient w.r.t. ¯Θ from the loss L(·), while
MSeaHKGrl takes the loss L(·) as a reward to feed it to a RL controller."
REFERENCES,0.951530612244898,"From Tab. 10, we also can observe the efﬁciency comparison between GNN-based models (G-MPNN,
StarE, and MSeaHKG). Like other NAS methods, MSeaHKG requires two training phases, searching
architecture and training the searched architecture from the scratch. Thus, it needs more running time
compared with G-MPNN. Moreover, both G-MPNN and MSeaHKG utilizes the simple decoders
(i.e., scoring function discussed in Appx. C.2), while StarE adopts the complex transformer as its
decoder. Thus, StarE is less efﬁcient."
REFERENCES,0.9540816326530612,"D.4
MSEAHKG CAN BE TRANSFERRED TO OTHER GRAPH-BASED TASKS."
REFERENCES,0.9566326530612245,"Since many conﬁgurations of MSeaHKG are inspired by GNNs and GNN searching methods, we
transfer MSeaHKG to other GNN-related tasks to further investigate its capability."
REFERENCES,0.9591836734693877,"First, we conduct an extensive experiment on social recommendation (Fan et al., 2019), where
the recommendation data sets are formed as multi-relational graphs. In the data set Ciao 2, nodes
represent the users and items, edges have two main types: 1) 5 level of ratings {1, 2, 3, 4, 5} between
users and items, 2) connections among users. The goal of the task is to predict the unknown
ratings of items given by users. After treating the 5 ratings as 5 edge types, the recommendation
task is converted to the relation prediction task, i.e., predict the rating r given ?(user, item). To
compare MSeaHKG with literature more conveniently, we follow Fan et al. (2019) to utilize the
Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as the evaluation metrics, and
60% as training data. We let MSeaHKG compute top-3 scores like a = score(2(user, item)), b =
score(4(user, item)), c = score(5(user, item)), then adopt weighted sum to output the ﬁnal rating
like (a · 2 + b · 4 + c · 5)/(a + b + c). The experimental report in Tab. 11a shows MSeaHKG has a
good generalized ability to the social recommendation task, which is consistent with the performance
on KGs (i.e., multi-relational graphs)."
REFERENCES,0.9617346938775511,"Second, we extend MSeaHKG to graph-level tasks. We incorporate one more essential component at
the ﬁnal layer of MPNNs, i.e., readout funcion rd(·). General MPNNs employ rd(·) to output the
representation of a whole graph G(E, R, S) by aggregating the node embeddings as:"
REFERENCES,0.9642857142857143,hG = rd({ei}ei∈E).
REFERENCES,0.9668367346938775,"Due to the modeling of edge representations, MSeaHKG slightly adjusts the readout function as:"
REFERENCES,0.9693877551020408,"hG = rd({[e1, r, e2]}r(e1,e2)∈S)."
REFERENCES,0.9719387755102041,2https://www.cse.msu.edu/˜tangjili/datasetcode/truststudy.htm
REFERENCES,0.9744897959183674,Under review as a conference paper at ICLR 2022
REFERENCES,0.9770408163265306,Table 11: The model performance of extending MSeaHKG to other tasks.
REFERENCES,0.9795918367346939,"(a) The comparison on the rating pre-
diction task of social recommendation.
The results of baselines are copied from
Fan et al. (2019)."
REFERENCES,0.9821428571428571,"model
Ciao
MAE
RMSE
PMF
0.9520
1.1967
TrustMF
0.7681
1.0543
NeuMF
0.8251
1.0824
GraphRec
0.7540
1.0093
MSeaHKG
0.7511
1.0021"
REFERENCES,0.9846938775510204,"(b) The comparison on the graph classiﬁcation task. The results of
baselines on PROTEINS and IMDB-M are copied from PAS (Wei
et al., 2021), and those on MUTAG are copied from GIN (Xu et al.,
2018)."
REFERENCES,0.9872448979591837,"type
model
PROTEINS IMDB-M MUTAG GNNs"
REFERENCES,0.9897959183673469,"GCN
0.7484
0.5040
0.8560
GraphSAGE
0.7375
0.4853
0.8510
GIN
0.7620
0.5230
0.8940"
REFERENCES,0.9923469387755102,"NAS
for
GNNs"
REFERENCES,0.9948979591836735,"GraphNAS
0.7520
0.4827
-
SNAG
0.7233
0.5000
-
You et al. (2020)
0.7390
0.4780
-
PAS
0.7664
0.5220
-
MSeaHKG
0.7724
0.5317
0.8922"
REFERENCES,0.9974489795918368,"We implement the choices of readout function as {global mean, global max, global sum}. We
report the accuracy in Tab. 11b. Among 3 data sets, the graphs in MUTAG are multi-relational, thus
most of GNN searching methods do not include it in empirical study. We can observe that MSeaHKG
achieves not bad performance on these data sets."
