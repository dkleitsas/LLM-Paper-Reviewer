Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0023752969121140144,"Thanks to the power of representation learning, neural contextual bandit algo-
rithms demonstrate remarkable performance improvement against their classical
counterparts. But because their exploration has to be performed in the entire neu-
ral network parameter space to obtain nearly optimal regret, the resulting com-
putational cost is prohibitively high. We perturb the rewards when updating the
neural network to eliminate the need of explicit exploration and the correspond-
ing computational overhead. We prove that a e
O(ed
√"
ABSTRACT,0.004750593824228029,"T) regret upper bound is still
achievable under standard regularity conditions, where T is the number of rounds
of interactions and ed is the effective dimension of a neural tangent kernel matrix.
Extensive comparisons with several benchmark contextual bandit algorithms, in-
cluding two recent neural contextual bandit models, demonstrate the effectiveness
and computational efﬁciency of our proposed neural bandit algorithm."
INTRODUCTION,0.007125890736342043,"1
INTRODUCTION"
INTRODUCTION,0.009501187648456057,"Contextual bandit is a well-formulated abstraction of many important real-world problems, includ-
ing content recommendation (Li et al., 2010; Wu et al., 2016), online advertising (Schwartz et al.,
2017; Nuara et al., 2018), and mobile health (Lei et al., 2017; Tewari & Murphy, 2017). In such
problems, an agent iteratively interacts with an environment to maximize its accumulated rewards
over time. Its essence is sequential decision-making under uncertainty. Because the reward from
the environment for a chosen action (also referred to as an arm in literature) under each context is
stochastic, a no-regret learning algorithm needs to explore the problem space for improved reward
estimation, i.e., learning the mapping from an arm and its context1 to the expected reward."
INTRODUCTION,0.011876484560570071,"Linear contextual bandit algorithms (Abbasi-Yadkori et al., 2011; Li et al., 2010), which assume
the reward mapping is a linear function of the context vector, dominate the community’s attention
in the study of contextual bandits. Though theoretically sound and practically effective, their lin-
ear reward mapping assumption is incompetent to capture possible complex non-linear relations
between the context vector and reward. This motivated the extended studies in parametric bandits,
such as generalized linear bandits (Filippi et al., 2010; Faury et al., 2020) and kernelized bandits
(Chowdhury & Gopalan, 2017; Krause & Ong, 2011). Recently, to unleash the power of repre-
sentation learning, deep neural networks (DNN) have also been introduced to learn the underlying
reward mapping directly. In (Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), a
deep neural network is applied to provide a feature mapping, and exploration is performed at the
last layer. NeuralUCB (Zhou et al., 2020) and NeuralTS (ZHANG et al., 2020) explore the entire
neural network parameter space to obtain nearly optimal regret using the neural tangent kernel tech-
nique (Jacot et al., 2018). These neural contextual bandit algorithms signiﬁcantly boosted empirical
performance compared to their classical counterparts."
INTRODUCTION,0.014251781472684086,"Nevertheless, a major practical concern of existing neural contextual bandit algorithms is their added
computational cost when performing exploration. Take the recently developed NeuralUCB and Neu-
ralTS for example. Their construction of the high-probability conﬁdence set for model exploration"
INTRODUCTION,0.0166270783847981,"1When no ambiguity is invoked, we refer to the feature vector for an arm and its context as a context vector."
INTRODUCTION,0.019002375296912115,Published as a conference paper at ICLR 2022
INTRODUCTION,0.021377672209026127,"depends on the dimensionality of the network parameters and the learned context vectors’ represen-
tations, which is often very large for DNNs. For instance, a performing neural bandit solution often
has the number of parameters in the order of 100 thousands (if not less). It is prohibitively expen-
sive to compute inverse of the induced covariance matrix on such a huge number of parameters, as
required by their construction of conﬁdence set. As a result, approximations, e.g., only using the
diagonal of the covariance matrix (Zhou et al., 2020; ZHANG et al., 2020), are employed to make
such algorithms operational in practice. But there is no theoretical guarantee for such diagonal ap-
proximations, which directly leads to the gap between the theoretical and empirical performance of
the neural bandit algorithms."
INTRODUCTION,0.023752969121140142,"In this work, to alleviate the computational overhead caused by the expensive exploration, we pro-
pose to eliminate explicit model exploration by learning a neural bandit model with perturbed re-
wards. At each round of model update, we inject pseudo noise generated from a zero-mean Gaussian
distribution to the observed reward history. With the induced randomization, sufﬁcient exploration
is achieved when simply pulling the arm with the highest estimated reward. This brings in consider-
able advantage over existing neural bandit algorithms: no additional computational cost is needed to
obtain no regret. We rigorously prove that with a high probability the algorithm obtains a e
O(ed
√"
INTRODUCTION,0.026128266033254157,"T)
regret, where ed is the effective dimension of a neural tangent kernel matrix and T is the number of
rounds of interactions. This result recovers existing regret bounds for the linear setting where the
effective dimension equals to the input feature dimension. Besides, our extensive empirical evalua-
tions demonstrate the strong advantage in efﬁciency and effectiveness of our solution against a rich
set of state-of-the-art contextual bandit solutions over both synthetic and real-world datasets."
RELATED WORK,0.028503562945368172,"2
RELATED WORK"
RELATED WORK,0.030878859857482184,"Most recently, attempts have been made to incorporate DNNs with contextual bandit algorithms.
Several existing work study neural-linear bandits (Riquelme et al., 2018; Zahavy & Mannor, 2019),
where exploration is performed on the last layer of the DNN. Under the neural tangent kernel
(NTK) framework (Jacot et al., 2018), NeuralUCB (Zhou et al., 2020) constructs conﬁdence sets
with DNN-based random feature mappings to perform upper conﬁdence bound based exploration.
NeuralTS (ZHANG et al., 2020) samples from the posterior distribution constructed with a simi-
lar technique. However, as the exploration is performed in the induced random feature space, the
added computational overhead is prohibitively high, which makes such solutions impractical. The
authors suggested diagonal approximations of the resulting covariance matrix, which however leave
the promised theoretical guarantees of those algorithms up in the air."
RELATED WORK,0.0332541567695962,"Reward perturbation based exploration has been studied in a number of classical bandit models
(Kveton et al., 2019a; 2020; 2019b). In a context-free k-armed bandit setting, Kveton et al. (2019b)
proposed to estimate each arm’s reward over a perturbed history and select the arm with the highest
estimated reward at each round. Such an arm selection strategy is proved to be optimistic with a
sufﬁciently high probability. Later this strategy has been extended to linear and generalized linear
bandits (Kveton et al., 2019a; 2020). In (Kveton et al., 2020), the authors suggested its application
to neural network models; but only some simple empirical evaluations were provided, without any
theoretical justiﬁcations. Our work for the ﬁrst time provides a rigorous regret analysis of neural
contextual bandits with the perturbation-based exploration: a sublinear regret bound is still achiev-
able in terms of the number of interactions between the agent and environment."
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.035629453681710214,"3
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.03800475059382423,"We study the problem of contextual bandit with ﬁnite K arms, where each arm is associated with a
d-dimensional context vector: xi ∈Rd for i ∈[K]. At each round t ∈[T], the agent needs to select
one of the arms, denoted as at, and receives its reward rat,t, which is generated as rat,t = h(xat) +
ηt. In particular, h(x) represents the unknown underlying reward mapping function satisfying 0 ≤
h(x) ≤1 for any x, and ηt is an R-sub-Gaussian random variable that satisﬁes E[exp(µηt)] ≤
exp[µ2R2] for all µ ≥0. The goal is to minimize pseudo regret over T rounds:"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.040380047505938245,"RT = E
XT"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.04275534441805225,"t=1(ra∗−rt,at)

,
(3.1)"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.04513064133016627,Published as a conference paper at ICLR 2022
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.047505938242280284,Algorithm 1 Neural bandit with perturbed reward (NPR)
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.0498812351543943,"1: Input: Number of rounds T, regularization coefﬁcient λ, perturbation parameter ν, network
width m, network depth L.
2: Initialization: θ0 = (vec(W1), . . . , vec(WL)] ∈Rp with Gaussian distribution: for 1 ≤l ≤
L −1, Wl = (W, 0; 0, W) with each entry of W sampled independently from N(0, 4/m);
WL = (w⊤, −w⊤) with each entry of w sampled independently from N(0, 2/m).
3: for t = 1, . . . , T do
4:
if t > K then
5:
Pull arm at and receive reward rt,at, where at = argmaxi∈[K] f(xi, θt−1).
6:
Generate {γt
s}s∈[t] ∼N(0, ν2).
7:
Set θt by the output of gradient descent for solving Eq (3.2).
8:
else
9:
Pull arm ak.
10:
end if
11: end for"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.052256532066508314,where a∗is the optimal arm with the maximum expected reward.
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.05463182897862233,"To deal with the potential non-linearity of h(x) and unleash the representation learning power
of DNNs, we adopt a fully connected neural network f(x; θ) to approximate h(x): f(x; θ) =
√mWLφ

WL−1φ
 
· · · φ(W1x)

, where φ(x) = ReLU(x), θ = [vec(W1), . . . , vec(WL)] ∈"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.057007125890736345,"Rp with p = m + md + m2(L −1), and depth L ≥2. Each hidden layer is assumed to have the
same width (i.e., m) for convenience in later analysis; but this does not affect the conclusion of our
theoretical analysis."
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.05938242280285035,"Existing neural bandit solutions perform explicit exploration in the entire model space (Zhou et al.,
2020; ZHANG et al., 2020; Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), which
introduces prohibitive computational cost. And oftentimes the overhead is so high that approxima-
tion has to be employed (Zhou et al., 2020; ZHANG et al., 2020), which unfortunately breaks the
theoretical promise of these algorithms. In our proposed model, to eliminate such explicit model
exploration in neural bandit, a randomization strategy is introduced in the neural network update.
We name the resulting solution as Neural bandit with Perturbed Rewards, or NPR in short. In NPR,
at round t, the neural model is learned with the t rewards perturbed with designed perturbations:"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.06175771971496437,"min
θ L(θ) =
Xt"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.06413301662707839,"s=1
 
f(xas; θ) −(rs,as + γt
s)
2/2 + mλ∥θ −θ0∥2
2/2
(3.2)"
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.0665083135391924,"where {γt
s}t
s=1 ∼N(0, σ2) are Gaussian random variables that are independently sampled in each
round t, and σ is a hyper-parameter that controls the strength of perturbation (and thus the explo-
ration) in NPR. We use an l2-regularized square loss for model estimation, where the regularization
centers at the randomly initialization θ0 with the trade-off parameter λ."
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.0688836104513064,"The detailed procedure of NPR is given in Algorithm 1. The algorithm starts by pulling all candidate
arms once. This guarantees that for any arm, NPR is sufﬁciently optimistic compared to the true
reward with respect to its approximation error (Lemma 4.4). When all the K arms have been pulled
once, the algorithm pulls the arm with the highest estimated reward, at = argmaxi f(xi; θt−1).
Once received the feedback rat,t, the model perturbs the entire reward history so far via a freshly
sampled noise sequence {γt
s}t
s=1, and updates the neural network by {(xas, ras,s + γt
s)}t
s=1 using
gradient descent. In the regret analysis in Section 4, we prove that the variance from the added
{γt
s}s∈[t] will lead to the necessary optimism for exploration (Abbasi-Yadkori et al., 2011). We
adopt gradient descent for analysis convenience, while stochastic gradient descent can also be used
to solve the optimization problem with a similar theoretical guarantee based on recent works (Allen-
Zhu et al., 2019; Zou et al., 2019)."
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.07125890736342043,"Compared with existing neural contextual bandit algorithms (Zhou et al., 2020; ZHANG et al., 2020;
Zahavy & Mannor, 2019; Riquelme et al., 2018; Xu et al., 2020), NPR does not need any added com-
putation for model exploration, besides the regular neural network update. This greatly alleviates the
overhead for the computation resources (both space and time) and makes NPR sufﬁciently general
to be applied for practical problems. More importantly, our theoretical analysis directly corresponds
to its actual behavior when applied, as no approximation is needed."
NEURAL BANDIT LEARNING WITH PERTURBED REWARDS,0.07363420427553444,Published as a conference paper at ICLR 2022
REGRET ANALYSIS,0.07600950118764846,"4
REGRET ANALYSIS"
REGRET ANALYSIS,0.07838479809976247,"In this section, we provide ﬁnite time regret analysis of NPR, where the time horizon T is set
beforehand. The theoretical analysis is built on the recent studies about the generalization of DNN
models (Cao & Gu, 2020; 2019; Chen et al., 2020; Daniely, 2017; Arora et al., 2019), which illustrate
that with (stochastic) gradient descent, the learned parameters of a DNN locate in a particular regime
with the generalization error being characterized by the best function in the corresponding neural
tangent kernel (NTK) space (Jacot et al., 2018). We leave the background of NTK in the appendix
and focus on the key steps of our proof in this section."
REGRET ANALYSIS,0.08076009501187649,"The analysis starts with the following lemma for the set of context vectors {xi}K
i=1.
Lemma 4.1. There exists a positive constant ¯C such that for any δ ∈(0, 1), with probability at least
1 −δ, when m ≥¯CK4L6 log(K2L/δ)/λ4
0, there exists a θ∗∈Rp, for all i ∈[K],"
REGRET ANALYSIS,0.0831353919239905,"h(xi) = ⟨g(xi; θ0), θ∗−θ0⟩,
√m∥θ∗−θ0∥2 ≤
√"
REGRET ANALYSIS,0.0855106888361045,"2h⊤H−1h,
(4.1)"
REGRET ANALYSIS,0.08788598574821853,"where H is the NTK matrix deﬁned on the context set and h = (h(x1), . . . , h(xK)). Details of H
can be found in the appendix."
REGRET ANALYSIS,0.09026128266033254,"This lemma suggests that with a satisﬁed neural network width m, with high probability, the under-
lying reward mapping function can be approximated by a linear function over g(xi; θ0), parameter-
ized by θ∗−θ0, where g(xi; θ0) = ∇θf(x; θ0) ∈Rd is the gradient of the initial neural network.
We also deﬁne the covariance matrix At at round t as,"
REGRET ANALYSIS,0.09263657957244656,"At =
Xt−1"
REGRET ANALYSIS,0.09501187648456057,"s=1 g(xs,as; θ0)g(xs,as; θ0)⊤/m + λI
(4.2)"
REGRET ANALYSIS,0.09738717339667459,"where λ > 0 is the l2 regularization parameter in Eq (3.2). Discussed before, there exist two kinds
of noises in the training samples: the observation noise and the perturbation noise. To analyze the
effect of each noise in model estimation, two auxiliary least square solutions are introduced, A−1
t
¯bt
and A−1
t bt with"
REGRET ANALYSIS,0.0997624703087886,"¯bt =
Xt−1"
REGRET ANALYSIS,0.1021377672209026,"s=1(rs,as + E[γt−1
s
])g(xs,as; θ0)/√m,
bt =
Xt−1"
REGRET ANALYSIS,0.10451306413301663,"s=1(rs,as + γt−1
s
)g(xs,as; θ0)/√m."
REGRET ANALYSIS,0.10688836104513064,"These two auxiliary solutions isolate the induced deviations: the ﬁrst least square solution only
contains the deviation caused by observation noise {ηs}t−1
s=1; and the second least square solution
has an extra deviation caused by {γt−1
s
}t−1
s=1."
REGRET ANALYSIS,0.10926365795724466,"To analysis the estimation error of the neural network, we ﬁrst deﬁne the following events. At round
t, we have event Et,1 deﬁned as:"
REGRET ANALYSIS,0.11163895486935867,"Et,1 =
n
∀i ∈[K], |⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩−h(xt,i)| ≤αt∥g(xt,i; θ0)∥A−1
t o
."
REGRET ANALYSIS,0.11401425178147269,"According to Lemma 4.1 and the deﬁnition of ¯bt, under event Et,1, the deviation caused by the
observation noise {ηs}t−1
s=1 can be controlled by αt. We also need event Et,2 at round t deﬁned as"
REGRET ANALYSIS,0.1163895486935867,"Et,2 = {∀i ∈[K] : |f(xt,i; θt−1) −⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩| ≤ϵ(m) + βt∥g(xt,i; θ0)∥A−1
t },"
REGRET ANALYSIS,0.1187648456057007,where ϵ(m) is deﬁned as
REGRET ANALYSIS,0.12114014251781473,"ϵ(m) =Cϵ,1m−1/6T 2/3λ−2/3L3p"
REGRET ANALYSIS,0.12351543942992874,"log m + Cϵ,2(1 −ηmλ)Jp TL/λ"
REGRET ANALYSIS,0.12589073634204276,"+ Cϵ,3m−1/6T 5/3λ−5/3L4p"
REGRET ANALYSIS,0.12826603325415678,"log m(1 +
p T/λ),"
REGRET ANALYSIS,0.13064133016627077,"with constants {Cϵ,i}3
i=1. Event Et,2 considers the situation that the deviation caused by the added
noise {γt−1
s
}t−1
s=1 is under control with parameter βt at round t, and the approximation error ϵ(m) is
carefully tracked by properly setting the neural network width m."
REGRET ANALYSIS,0.1330166270783848,"The following lemmas show that given the history Ft−1 = σ{x1, r1, x2, r2, ..., xt−1, rt−1, xt},
which is σ-algebra generated by the previously pulled arms and the observed rewards, with proper
setting, events Et,1 and Et,2 happen with a high probability."
REGRET ANALYSIS,0.13539192399049882,Published as a conference paper at ICLR 2022
REGRET ANALYSIS,0.1377672209026128,"Lemma 4.2. For any t ∈[T], λ > 0 and δ > 0, with αt =
q"
REGRET ANALYSIS,0.14014251781472684,"R2 log
 
det(At)/(δ2 det(λI))

+
√"
REGRET ANALYSIS,0.14251781472684086,"λS, we have P(Et,1|Ft−1) ≥1 −δ."
REGRET ANALYSIS,0.14489311163895488,"In particular, S denotes the upper bound of
√"
REGRET ANALYSIS,0.14726840855106887,"2h⊤H−1h, which is a constant if the reward function
h(·) belongs to the RKHS space induced by the neural tangent kernel.
Lemma 4.3. There exist positive constants {Ci}3
i=1, such that with step size and the neural net-
work satisfy that η = C1(mλ+mLT)−1, m ≥C2
√"
REGRET ANALYSIS,0.1496437054631829,"λL−3/2[log(TKL2/δ)]3/2, and m[log m]−3 ≥
C3 max{TL12λ−1, T 7λ−8L18(λ+LT)6, L21T 7λ−7(1+
p"
REGRET ANALYSIS,0.15201900237529692,"T/λ)6}, given history Ft−1, by choos-
ing βt = σ√4 log t + 2 log K, we have P(Et,2|Ft−1) ≥1 −t−2."
REGRET ANALYSIS,0.1543942992874109,"We leave the detailed proof in the appendix. According to Lemma 4.2 and Lemma 4.3, with more
observations become available, the neural network based reward estimator f(x; θt) will gradually
converge to the true expected reward h(x). In the meantime, the variance of the added perturba-
tion leads to reward overestimation, which will compensate the potential underestimation caused
by the observation noise and the approximation error. The following lemma describes this anti-
concentration behavior of the added noise {γt−1
s
}t−1
s=1."
REGRET ANALYSIS,0.15676959619952494,"Lemma 4.4. For any t ∈[T], with σ = αt(1 −λλ−1
K (AK))−1/2, αt deﬁned in Lemma 4.2,
ϵ(m) deﬁned in Lemma 4.3, given history Ft−1, we have P
 
Et,3) = P(f(xt,i; θt−1) ≥h(xt,i) −
ϵ(m)|Ft−1, Et,1

≥(4e√π)−1, where AK is the covariance matrix constructed after the initial
K-round arm pulling (line 8 to 10 in Algorithm 1) by Eq (4.2), and λK(AK) represents the K-th
largest eigenvalue of matrix AK."
REGRET ANALYSIS,0.15914489311163896,"With αt deﬁned in Lemma 4.2, and ϵ(m) and βt deﬁned in Lemma 4.3, we divide the arms into two
groups. More speciﬁcally, we deﬁne the set of sufﬁciently sampled arms in round t as:"
REGRET ANALYSIS,0.16152019002375298,"Ωt = {∀i ∈[K] : 2ϵ(m) + (αt + βt)∥g(xt,i; θ0)∥A−1
t
≤h(xt,a∗) −h(xt,i)}."
REGRET ANALYSIS,0.16389548693586697,"Accordingly, the set of undersampled arms is deﬁned as ¯Ωt = [K] \ Ωt. Note that by this deﬁnition,
the best arm a∗belongs to ¯Ωt. In the following lemma, we show that at any time step t > K, the
expected one-step regret is upper bounded in terms of the regret due to playing an undersampled
arm at ∈¯Ωt.
Lemma 4.5. With m, η, σ satisfying the conditions in Lemma 4.2, 4.3, 4.4, and ϵ(m) deﬁned in
Lemma 4.3, with probability at least 1 −δ the one step regret of NPR is upper bounded,"
REGRET ANALYSIS,0.166270783847981,"E[h(xt,a∗
t ) −h(xt,at)] ≤P( ¯Et,2) + 4ϵ(m) + (βt + αt)
 
1 + 2/P(at ∈¯Ωt)

∥g(xt,at; θ0)/√m∥A−1
t ."
REGRET ANALYSIS,0.16864608076009502,"Furthermore, P(at ∈¯Ωt) ≥Pt(Et,3) −Pt( ¯Et,2)."
REGRET ANALYSIS,0.171021377672209,"To bound the cumulative regret of NPR, we also need the following lemma."
REGRET ANALYSIS,0.17339667458432304,"Lemma 4.6. With ed as the effective dimension of the NTK matrix H, we have
XT"
REGRET ANALYSIS,0.17577197149643706,"t=1 min

∥g(xt,at; θ0)/√m∥2
A−1
t−1, 1

≤2(ed log(1 + TK/λ) + 1)"
REGRET ANALYSIS,0.17814726840855108,"The effective dimension is ﬁrst proposed in (Valko et al., 2013) to analyze the kernelized contextual
bandit, which roughly measures the number of directions in the kernel space where the data mostly
lies. The detailed deﬁniton of ed can be found in Deﬁnition B.3 in the appendix. Finally, with proper
neural network setup, we provide an m-independent regret upper bound."
REGRET ANALYSIS,0.18052256532066507,"Theorem 4.7. Let ed be the effective dimension of the NTK kernel matrix H. There exist positive
constants C1 and Cr, such that for any δ ∈(0, 1), when network structure and the step size for
model update satisfy m ≥poly(T, L, K, λ−1, S−1, log(1/δ)), η = C1(mTL + mλ)−1 and λ ≥
max{1, S−2}, with probability at least 1 −δ, the cumulative regret of NPR satisﬁes"
REGRET ANALYSIS,0.1828978622327791,"RT ≤Γ

R
q"
REGRET ANALYSIS,0.18527315914489312,"ed log(1 + TK/λ) + 2 −2 log δ +
√"
REGRET ANALYSIS,0.1876484560570071,"λS

·
q"
REGRET ANALYSIS,0.19002375296912113,2edT log(1 + TK/λ) + 2 + 7 + K (4.3)
REGRET ANALYSIS,0.19239904988123516,"where Γ = 44e√π(1 +
q"
REGRET ANALYSIS,0.19477434679334918,"(4 log T + 2 log K)/(1 −λλ−1
K (AK)))."
REGRET ANALYSIS,0.19714964370546317,Published as a conference paper at ICLR 2022
REGRET ANALYSIS,0.1995249406175772,"Proof. By Lemma 4.2, Et,1 holds for all t ∈[T] with probability at least 1 −δ. Therefore, with
probability at least 1 −δ, we have RT = T
X"
REGRET ANALYSIS,0.20190023752969122,"t=1
E[(h(xt,a∗
t ) −h(xt,at))1{Et,1}] ≤ T
X"
REGRET ANALYSIS,0.2042755344418052,"t=K+1
E[(h(xt,a∗
t ) −h(xt,at))1{Et,1}] + K"
REGRET ANALYSIS,0.20665083135391923,"≤4Tϵ(m) + T
X"
REGRET ANALYSIS,0.20902612826603326,"t=1
Pt( ¯Et,2) + (βt + αt)(1 +
2
Pt(Et,3) −Pt( ¯Et,2))∥g(xt,at; θ0)/√m∥A−1
t
+ K."
REGRET ANALYSIS,0.21140142517814728,"The second inequality is due to the analysis of one-step regret in Lemma 4.5. According to Lemma
4.3, by choosing δ = σct, with ct = √4 log t + 2 log K, Pt( ¯Et,2) ≤t−2. Hence, PT
t=1 Pt( ¯Et,2) ≤
π2/6. Based on Lemma 4.4, with σ = αt(1 −λλ−1
K (AK))−1/2, Pt(Et,3) ≥
1
4e√π. Therefore,
1 + 2/(Pt(Et,3) −Pt( ¯Et,2)) ≤44e√π. By chaining all the inequalities, we have the cumulative
regret of NPR upper bounded as,"
REGRET ANALYSIS,0.21377672209026127,RT ≤π2
REGRET ANALYSIS,0.2161520190023753,"3 + 4Tϵ(m) +
XT"
REGRET ANALYSIS,0.21852731591448932,"t=1(βt + αt)(1 +
2
Pt(Et,3) −Pt( ¯Et,2))∥g(xt,at; θ0)/√m∥A−1
t
+ K"
REGRET ANALYSIS,0.2209026128266033,"≤Γ

R
q"
REGRET ANALYSIS,0.22327790973871733,"ed log(1 + TK/λ) + 2 −2 log δ +
√"
REGRET ANALYSIS,0.22565320665083136,"λS

·
q"
REGRET ANALYSIS,0.22802850356294538,2edT log(1 + TK/λ) + 2 + π2 3 + K
REGRET ANALYSIS,0.23040380047505937,"+ Cϵ,1m−1/6T 5/3λ−2/3L3p"
REGRET ANALYSIS,0.2327790973871734,"log m + Cϵ,2(1 −ηmλ)JT
p TL/λ"
REGRET ANALYSIS,0.23515439429928742,"+ Cϵ,3m−1/6T 8/3λ−5/3L4p"
REGRET ANALYSIS,0.2375296912114014,"log m(1 +
p T/λ)"
REGRET ANALYSIS,0.23990498812351543,"where Γ = 44e√π(1 +
q"
REGRET ANALYSIS,0.24228028503562946,"(4 log T + 2 log K)/(1 −λλ−1
K (AK))). The second inequality is due
to Lemma 4.6, and the bounds of events ¯Et,2 and Et,3. By setting η = c(mλ + mLT)−1, and
J = (1+LT/λ)(log(Cϵ,2)+log T
p"
REGRET ANALYSIS,0.24465558194774348,"TL/λ)/c, Cϵ,2(1−ηmλ)JT
p"
REGRET ANALYSIS,0.24703087885985747,"TL/λ ≤1. Then by choosing
the satisﬁed m, Cϵ,1m−1/6T 5/3λ−2/3L3√log m+Cϵ,3m−1/6T 8/3λ−5/3L4√log m(1+
p"
REGRET ANALYSIS,0.2494061757719715,T/λ) ≤
REGRET ANALYSIS,0.2517814726840855,"2/3.
RT can be further bounded by RT ≤Γ

R
q"
REGRET ANALYSIS,0.25415676959619954,"ed log(1 + TK/λ) + 2 −2 log δ +
√"
REGRET ANALYSIS,0.25653206650831356,"λS

·
q"
REGRET ANALYSIS,0.2589073634204275,2edT log(1 + TK/λ) + 2 + 7 + K
REGRET ANALYSIS,0.26128266033254155,This completes the proof.
EXPERIMENTS,0.26365795724465557,"5
EXPERIMENTS"
EXPERIMENTS,0.2660332541567696,"In this section, we empirically evaluate the proposed neural bandit algorithm NPR against several
state-of-the-art baselines, including: linear and neural UCB (LinUCB (Abbasi-Yadkori et al., 2011)
and NeuralUCB (Zhou et al., 2020)), linear and neural Thompson Sampling (LinTS (Agrawal &
Goyal, 2013) and NeuralTS (ZHANG et al., 2020)), perturbed reward for linear bandits (LinFPL)
(Kveton et al., 2019a). To demonstrate the real impact of using diagonal approximation in conﬁ-
dence set construction, we also include NeuralUCB and NeuralTS with diagonal approximation as
suggested in their original papers for comparisons."
EXPERIMENTS,0.2684085510688836,"We evaluated all the algorithms on 1) synthetic dataset via simulation, 2) six K-class classiﬁcation
datasets from UCI machine learning repository (Beygelzimer et al., 2011), and 3) two real-world
datasets extracted from the social bookmarking web service Delicious and music streaming service
LastFM (Wu et al., 2016). We implemented all the algorithms in PyTorch and performed all the
experiments on a server equipped with Intel Xeon Gold 6230 2.10GHz CPU, 128G RAM, four
NVIDIA GeForce RTX 2080Ti graphical cards."
EXPERIMENT ON SYNTHETIC DATASET,0.27078384798099764,"5.1
EXPERIMENT ON SYNTHETIC DATASET"
EXPERIMENT ON SYNTHETIC DATASET,0.27315914489311166,"In our simulation, we ﬁrst generate a size-K arm pool, in which each arm i is associated with a
d-dimensional feature vector xi. The contextual vectors are sampled uniformly at random from a
unit ball. We generate the rewards via the following two nonlinear functions:"
EXPERIMENT ON SYNTHETIC DATASET,0.2755344418052256,"h1(x) = 10−2(x⊤ΣΣ⊤x),
h2(x) = exp(−10(x⊤θ)2) ,"
EXPERIMENT ON SYNTHETIC DATASET,0.27790973871733965,Published as a conference paper at ICLR 2022
EXPERIMENT ON SYNTHETIC DATASET,0.28028503562945367,"0
2000
4000
6000
8000
10000
Rounds 0 500 1000 1500 2000"
EXPERIMENT ON SYNTHETIC DATASET,0.2826603325415677,Regret
EXPERIMENT ON SYNTHETIC DATASET,0.2850356294536817,"LinFPL
LinTS
LinUCB
NPR
NeuralTS
NeuralTS (Diagonal)
NeuralUCB
NeuralUCB (Diagonal)"
EXPERIMENT ON SYNTHETIC DATASET,0.28741092636579574,(a) h1(x) = 10−2(x⊤ΣΣ⊤x)
EXPERIMENT ON SYNTHETIC DATASET,0.28978622327790976,"0
2000
4000
6000
8000
10000
Rounds 0 200 400 600 800 1000 1200 1400"
EXPERIMENT ON SYNTHETIC DATASET,0.2921615201900237,Regret
EXPERIMENT ON SYNTHETIC DATASET,0.29453681710213775,(b) h2(x) = exp(−10(x⊤θ)2)
EXPERIMENT ON SYNTHETIC DATASET,0.29691211401425177,"NeuralUCB
NeuralTS
NeuralUCB 
 (Diagonal)"
EXPERIMENT ON SYNTHETIC DATASET,0.2992874109263658,"NeuralUCB 
 (Diagonal) NPR
0 500 1000 1500 2000"
EXPERIMENT ON SYNTHETIC DATASET,0.3016627078384798,Elapsed Time (sec)
EXPERIMENT ON SYNTHETIC DATASET,0.30403800475059384,"Time for Model Update
Time for Arm Selection"
EXPERIMENT ON SYNTHETIC DATASET,0.30641330166270786,(c) Elapsed time
EXPERIMENT ON SYNTHETIC DATASET,0.3087885985748218,"10
20
50
100
Size of Arm Pool 0 500 1000 1500 2000 2500"
EXPERIMENT ON SYNTHETIC DATASET,0.31116389548693585,Elapsed Time (sec)
EXPERIMENT ON SYNTHETIC DATASET,0.31353919239904987,(d) Effect of pool size
EXPERIMENT ON SYNTHETIC DATASET,0.3159144893111639,"[16 16]
[32 32]
[64 64]
[128 128]
Neural Network Structure 103 104"
EXPERIMENT ON SYNTHETIC DATASET,0.3182897862232779,Elapsed Time (sec)
EXPERIMENT ON SYNTHETIC DATASET,0.32066508313539194,"NeuralUCB
NeuralTS
NeuralUCB (Diagonal)
NeuralTS (Diagonal)
NPR"
EXPERIMENT ON SYNTHETIC DATASET,0.32304038004750596,(e) Effect of network complexity
EXPERIMENT ON SYNTHETIC DATASET,0.3254156769596199,Figure 1: Empirical results of regret and time consumption on synthetic dataset.
EXPERIMENT ON SYNTHETIC DATASET,0.32779097387173395,"where each element of Σ ∈Rd×d is randomly sampled from N(0, 1), and θ is randomly sampled
from a unit ball. At each round t, only a subset of k arms out of the total K arms are sampled without
replacement and disclosed to all algorithms for selection. The ground-truth reward ra is corrupted
by Gaussian noise η = N(0, ξ2) before feeding back to the bandit algorithms. We ﬁxed the feature
dimension d = 50 and the pool size K = 100 with k = 20. ξ was set to 0.1 for both h1 and h2."
EXPERIMENT ON SYNTHETIC DATASET,0.33016627078384797,"Cumulative regret is used to compare the performance of different algorithms. Here the best arm
is deﬁned as the one with the highest expected reward in the presented k candidate arms. All the
algorithms were executed up to 10000 rounds in simulation, and the averaged results over 10 runs
are reported in Figure 1(a) to 1(e). For the neural bandit algorithms, we adopted a 3-layer neural
network with m = 64 units in each hidden layer. We did a grid search on the ﬁrst 1000 rounds
for regularization parameter λ over {10−i}4
i=0, and step size η over {10−i}3
i=0. We also searched
for the concentration parameter δ so that the exploration parameter ν is equivalently searched over
{10−i}4
i=0. The best set of hyper-parameters for each algorithm are applied for the full simulation
over 10000 rounds, i.e., only that trial will be continued for the rest of 9000 rounds."
EXPERIMENT ON SYNTHETIC DATASET,0.332541567695962,"In Figure 1(a) and 1(b), we can observe that linear bandit algorithms clearly failed to learn non-
linear mapping and suffered much higher regret compared to the neural algorithms. NPR achieved
similar performance as NeuralUCB and NeuralTS, which shows the effectiveness of our randomized
exploration strategy in neural bandit learning."
EXPERIMENT ON SYNTHETIC DATASET,0.334916864608076,"Figure 1(c) shows the average running time for all the neural bandit models in this experiment. With
the same network structure, the time spent for model update was similar across different neural
bandit models. But we can clearly observe that NeuralUCB and NeuralTS took much more time in
arm selection. When choosing an arm, NeuralUCB and NeuralTS, and their corresponding versions
with diagonal approximation, need to repeatedly compute: 1) the inverse of the covariance matrix
based on the pulled arms in history; and 2) the conﬁdence set of their reward estimation on each
candidate arm. The size of the covariance matrix depends on the number of parameters in the neural
network (e.g., 7460-by-7460 in this experiment). With diagonal approximation, the computational
cost is signiﬁcantly reduced. But, as mentioned before, there is no theoretical guarantee about the
approximation’s impact on the model’s performance. Intuitively, the impact depends on how the
full covariance matrix concentrates on its diagonal, e.g., whether the random feature mappings of
the network on the pulled arms are correlated among their dimensions. Unfortunately, this cannot
be determined in advance. Figure 1(a) and 1(b) show that the diagonal approximation leads to a
signiﬁcant performance drop. This conﬁrms our concern of such an approximation. In contrast, as
shown in Figure 1(c), there is no extra computation overhead in NPR; in the meantime, it provides
comparable performance to NeuralUCB and NeuralTS with a full covariance matrix."
EXPERIMENT ON SYNTHETIC DATASET,0.33729216152019004,Published as a conference paper at ICLR 2022
EXPERIMENT ON SYNTHETIC DATASET,0.33966745843230406,"0
2000
4000
6000
8000
10000
Rounds 0 500 1000 1500 2000 2500"
EXPERIMENT ON SYNTHETIC DATASET,0.342042755344418,Regret
EXPERIMENT ON SYNTHETIC DATASET,0.34441805225653205,"LinFPL
LinTS
LinUCB
NPR
NeuralTS
NeuralTS (Diagonal)
NeuralUCB
NeuralUCB (Diagonal)"
EXPERIMENT ON SYNTHETIC DATASET,0.34679334916864607,(a) Adult
EXPERIMENT ON SYNTHETIC DATASET,0.3491686460807601,"0
1000
2000
3000
4000
5000
6000
7000
8000
Rounds 0 100 200 300 400"
EXPERIMENT ON SYNTHETIC DATASET,0.3515439429928741,Regret
EXPERIMENT ON SYNTHETIC DATASET,0.35391923990498814,(b) Mushroom
EXPERIMENT ON SYNTHETIC DATASET,0.35629453681710216,"0
2000
4000
6000
8000
10000
Rounds 0 200 400 600 800 1000"
EXPERIMENT ON SYNTHETIC DATASET,0.3586698337292161,Regret
EXPERIMENT ON SYNTHETIC DATASET,0.36104513064133015,(c) Shuttle
EXPERIMENT ON SYNTHETIC DATASET,0.36342042755344417,Figure 2: Comparison of NPR and the baselines on the UCI datasets.
EXPERIMENT ON SYNTHETIC DATASET,0.3657957244655582,"To further investigate the efﬁciency advantage of NPR, we reported the total running time for the
neural bandit algorithms under different sizes of the arm pool and different structures of the neural
network under the same simulation setting. With the same neural network structure, models with
different sizes of candidate arms should share the same running time for model update. Hence,
Figure 1(d) reports how the size of arm pool affects the time spent on arm selection. As we can ﬁnd,
with more candidate arms, NeuralUCB, NeuralTS, and their corresponding diagonal approximations
spent more time on arm selection, or more speciﬁcally on constructing the conﬁdence set of the
reward estimation for each arm. With a ﬁxed size arm pool, the complexity of the neural network
will strongly affect the time for computing the inverse of the covariance matrix. In Figure 1(e), the
x-axis shows the structure for the hidden layers. The running time for NeuralUCB and NeuralTS
signiﬁcantly increased with enlarged neural networks. The running time of NPR stayed stable,
across varying sizes of arm pools and the network complexity. This shows the strong advantage of
NPR in efﬁciency and effectiveness, especially for large-scale problems."
EXPERIMENT ON CLASSIFICATION DATASETS,0.3681710213776722,"5.2
EXPERIMENT ON CLASSIFICATION DATASETS"
EXPERIMENT ON CLASSIFICATION DATASETS,0.37054631828978624,"Following the setting in (Zhou et al., 2020), we evaluated the bandit algorithms on K-class classiﬁ-
cation problem with six public benchmark datasets, including adult, mushroom, and shuttle
from UCI Machine Learning Repository (Dua & Graff, 2017). Due to space limit, we report the
results on these three datasets in this section, and leave the other three, letter, covertype,
and MagicTelescope in the appendix.
We adopted the disjoint model (Li et al., 2010) to
build the context vectors for candidate arms: given an input instance with feature x ∈Rd of
a k-class classiﬁcation problem, we constructed the context features for k candidate arms as:
x1 = (x, 0, . . . , 0), . . . , xk = (0, . . . , 0, x) ∈Rd×k. The model receives reward 1 if it selects
the correct class by pulling the corresponding arm, otherwise 0. The cumulative regret measures the
total mistakes made by the model over T rounds. Similar to the experiment in synthetic datasets,
we adopted a 3-layer neural network with m = 64 units in each hidden layer. We used the same
grid search setting as in our experiments on synthetic datasets. Figure 2 shows the averaged results
across 10 runs for 10000 rounds, except for mushroom which only contains 8124 data instances."
EXPERIMENT ON CLASSIFICATION DATASETS,0.37292161520190026,"There are several key observations in Figure 2. First, the improvement of applying a neural bandit
model depends on the nonlinearity of the dataset. For example, the performance on mushoom and
shuttle was highly boosted by the neural models, while the improvement on adult was limited.
Second, the impact of the diagonal approximation for NeuralUCB and NeuralTS also depends on
the dataset. It did not hurt their performance too much on shuttle, while using the full covariance
matrix led to much better performance on Mushroom. Overall, NPR showed consistently better or
at least comparable performance to the best neural bandit baselines in all six datasets. Such robust
performance and high efﬁciency make it more advantageous in practice."
EXPERIMENT ON LASTFM & DELICIOUS,0.3752969121140142,"5.3
EXPERIMENT ON LASTFM & DELICIOUS"
EXPERIMENT ON LASTFM & DELICIOUS,0.37767220902612825,"The LastFM dataset was extracted from the music streaming service website Last.fm, and the Deli-
cious data set was extracted from the social bookmark sharing service website Delicious (Wu et al.,
2016). In particular, the LastFM dataset contains 1,892 users and 17,632 items (artists). We treat the
“listened artists” in each user as positive feedback. The Delicious dataset contains 1,861 users and
69,226 items (URLs). We treat the bookmarked URLs for each user as positive feedback. Following
the standard setting (Cesa-Bianchi et al., 2013), we pre-processed these two datasets. For the item"
EXPERIMENT ON LASTFM & DELICIOUS,0.38004750593824227,Published as a conference paper at ICLR 2022
EXPERIMENT ON LASTFM & DELICIOUS,0.3824228028503563,"0
2000
4000
6000
8000
10000
Rounds 0 5 10 15 20 25"
EXPERIMENT ON LASTFM & DELICIOUS,0.3847980997624703,Normalized Payoff
EXPERIMENT ON LASTFM & DELICIOUS,0.38717339667458434,"NPR
NeuralTS (Diagonal)
NeuralUCB (Diagonal)
hLinUCB"
EXPERIMENT ON LASTFM & DELICIOUS,0.38954869358669836,(a) LastFM dataset
EXPERIMENT ON LASTFM & DELICIOUS,0.3919239904988123,"0
2000
4000
6000
8000
10000
Rounds 0 5 10 15 20 25 30 35"
EXPERIMENT ON LASTFM & DELICIOUS,0.39429928741092635,Normalized Payoff
EXPERIMENT ON LASTFM & DELICIOUS,0.39667458432304037,"NPR
NeuralTS (Diagonal)
NeuralUCB (Diagonal)
hLinUCB"
EXPERIMENT ON LASTFM & DELICIOUS,0.3990498812351544,(b) Delicious dataset
EXPERIMENT ON LASTFM & DELICIOUS,0.4014251781472684,Figure 3: Comparisons of normalized rewards on LastFM & Delicious datasets.
EXPERIMENT ON LASTFM & DELICIOUS,0.40380047505938244,"features, we used all the tags associated with each item to create its TF-IDF feature vector. Then
PCA was applied to reduce the dimensionality of the features. In both datasets, we used the ﬁrst 25
principle components to construct the item feature vectors. On the user side, we created user fea-
tures by running node2vec (Grover & Leskovec, 2016) on the user relation graph extracted from the
social network provided by the datasets. Each of the users was represented with a 50-dimensional
feature vector. We generated the candidate pool as follows: we ﬁxed the size of candidate arm pool
to k = 25 for each round; for each user, we picked one item from those non-zero payoff items
according to the complete observations in the dataset, and randomly picked the other 24 from those
zero-payoff items. We constructed the context vectors by concatenating the user and item feature
vectors. In our experiment, a 3-layer neural network with m = 128 units in each hidden layer was
applied to learn the bandit model. And the same gird search was performed to ﬁnd the best set of
hyper-parameters. We performed the experiment with 10000 rounds. Because of the high demand
of GPU memory for the matrix inverse computation, we only compared NPR to NeuralUCB and
NeuralTS with the diagonal approximation."
EXPERIMENT ON LASTFM & DELICIOUS,0.40617577197149646,"We compared the cumulative reward from each algorithm in Figure 3. Besides the neural contextual
bandit algorithms, we also include the best reported model on these two datasets, hLinUCB (Wang
et al., 2016) for comparison. To improve visibility, we normalized the cumulative reward by a
random strategy’s cumulative reward (Wu et al., 2016), where the arm was randomly sampled from
the candidate pool. All the neural models showed the power to learn the potential non-linear reward
mapping on these two real-world datasets. NPR showed its advantage over the baselines (especially
on LastFM). As NPR has no additional requirement on the computation resource (e.g., no need for
the expensive matrix operation as in the baselines), we also experienced much shorter running in
NPR than the other two neural bandit models."
CONCLUSION,0.4085510688836104,"6
CONCLUSION"
CONCLUSION,0.41092636579572445,"Existing neural contextual bandit algorithms sacriﬁce computation efﬁciency for effective explo-
ration in the entire neural network parameter space. To eliminate the computational overhead caused
by such an expensive operation, we appealed to randomization-based exploration and obtained the
same level of learning outcome. No extra computational cost or resources are needed besides the
regular neural network update. We proved that the proposed solution obtains eO(ed
√"
CONCLUSION,0.41330166270783847,"T) regret for T
rounds of interactions between the system and user. Extensive empirical results on both synthetic
and real-world datasets support our theoretical analysis, and demonstrate the strong advantage of
our model in efﬁciency and effectiveness, which suggest the potential of deploying powerful neural
bandit model online in practice."
CONCLUSION,0.4156769596199525,"Our empirical efﬁciency analysis shows that the key bottleneck of learning a neural model online is
at the model update step. Our current regret analysis depends on (stochastic) gradient descent over
the entire training set for model update in each round, which is prohibitively expensive. We would
like to investigate the possibility of more efﬁcient model update, e.g., online stochastic gradient de-
scent or continual learning, and the corresponding effect on model convergence and regret analysis.
Currently, our analysis focuses on the K-armed setting, and it is important to extend our solution
and analysis to inﬁnitely many arms in our future work."
CONCLUSION,0.4180522565320665,Published as a conference paper at ICLR 2022
CONCLUSION,0.42042755344418054,ACKNOWLEDGEMENTS
CONCLUSION,0.42280285035629456,"We thank the anonymous reviewers for their helpful comments. YJ and HW are partially supported
by the National Science Foundation under grant IIS-2128019 and IIS-1553568. WZ, DZ and QG are
partially supported by the National Science Foundation CAREER Award 1906169 and IIS-1904183.
The views and conclusions contained in this paper are those of the authors and should not be inter-
preted as representing any funding agencies."
REFERENCES,0.4251781472684085,REFERENCES
REFERENCES,0.42755344418052255,"Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312–2320, 2011."
REFERENCES,0.42992874109263657,"Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
In International Conference on Machine Learning, pp. 127–135, 2013."
REFERENCES,0.4323040380047506,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252, 2019."
REFERENCES,0.4346793349168646,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On ex-
act computation with an inﬁnitely wide neural net. In Advances in Neural Information Processing
Systems, 2019."
REFERENCES,0.43705463182897863,"Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual
bandit algorithms with supervised learning guarantees. In Proceedings of the Fourteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pp. 19–26, 2011."
REFERENCES,0.43942992874109266,"Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.4418052256532066,"Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning over-
parameterized deep relu networks. In the Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence, 2020."
REFERENCES,0.44418052256532065,"Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. A gang of bandits. arXiv preprint
arXiv:1306.0811, 2013."
REFERENCES,0.44655581947743467,"Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suf-
ﬁcient to learn deep relu networks? In International Conference on Learning Representations,
2020."
REFERENCES,0.4489311163895487,"Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pp. 844–853. PMLR, 2017."
REFERENCES,0.4513064133016627,"Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422–2430, 2017."
REFERENCES,0.45368171021377673,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685, 2019."
REFERENCES,0.45605700712589076,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.4584323040380047,"Louis Faury, Marc Abeille, Cl´ement Calauz`enes, and Olivier Fercoq. Improved optimistic algo-
rithms for logistic bandits. In International Conference on Machine Learning, pp. 3052–3060.
PMLR, 2020."
REFERENCES,0.46080760095011875,"Sarah Filippi, Olivier Cappe, Aur´elien Garivier, and Csaba Szepesv´ari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586–594,
2010."
REFERENCES,0.46318289786223277,Published as a conference paper at ICLR 2022
REFERENCES,0.4655581947743468,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855–864, 2016."
REFERENCES,0.4679334916864608,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018."
REFERENCES,0.47030878859857483,"Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances
in neural information processing systems, pp. 2447–2455, 2011."
REFERENCES,0.47268408551068886,"Branislav Kveton, Csaba Szepesv´ari, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbed-
history exploration in stochastic linear bandits. In Proceedings of the 35th Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI), pp. 176, 2019a."
REFERENCES,0.4750593824228028,"Branislav Kveton, Csaba Szepesvari, Sharan Vaswani, Zheng Wen, Tor Lattimore, and Mohammad
Ghavamzadeh. Garbage in, reward out: Bootstrapping exploration in multi-armed bandits. In
International Conference on Machine Learning, pp. 3601–3610. PMLR, 2019b."
REFERENCES,0.47743467933491684,"Branislav Kveton, Manzil Zaheer, Csaba Szepesv´ari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In Proceedings of the
22nd International Conference on Artiﬁcial Intelligence and Statistics, 2020."
REFERENCES,0.47980997624703087,"Huitian Lei, Ambuj Tewari, and Susan A Murphy. An actor-critic contextual bandit algorithm for
personalized mobile health interventions. arXiv preprint arXiv:1706.09090, 2017."
REFERENCES,0.4821852731591449,"Lihong Li, Wei Chu, John Langford, and Robert E Schapire.
A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661–670. ACM, 2010."
REFERENCES,0.4845605700712589,"Alessandro Nuara, Francesco Trovo, Nicola Gatti, and Marcello Restelli. A combinatorial-bandit
algorithm for the online joint bid/budget optimization of pay-per-click advertising campaigns. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018."
REFERENCES,0.48693586698337293,"Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown. In Interna-
tional Conference on Learning Representations, 2018."
REFERENCES,0.48931116389548696,"Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising
using multi-armed bandit experiments. Marketing Science, 36(4):500–522, 2017."
REFERENCES,0.4916864608076009,"Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health.
In Mobile Health, pp. 495–517. Springer, 2017."
REFERENCES,0.49406175771971494,"Michal Valko, Nathaniel Korda, R´emi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time
analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013."
REFERENCES,0.49643705463182897,"Huazheng Wang, Qingyun Wu, and Hongning Wang. Learning hidden features for contextual ban-
dits. In Proceedings of the 25th ACM International on Conference on Information and Knowledge
Management, pp. 1633–1642, 2016."
REFERENCES,0.498812351543943,"Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collabo-
rative environment. In Proceedings of the 39th International ACM SIGIR conference on Research
and Development in Information Retrieval, pp. 529–538, 2016."
REFERENCES,0.501187648456057,"Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu.
Neural contextual bandits with deep
representation and shallow exploration. arXiv preprint arXiv:2012.01780, 2020."
REFERENCES,0.503562945368171,"Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019."
REFERENCES,0.505938242280285,"Weitong ZHANG, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. In
International Conference on Learning Representations, 2020."
REFERENCES,0.5083135391923991,"Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, pp. 11492–11502. PMLR, 2020."
REFERENCES,0.5106888361045131,Published as a conference paper at ICLR 2022
REFERENCES,0.5130641330166271,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. Machine Learning, 2019."
REFERENCES,0.5154394299287411,"A
ADDITIONAL EXPERIMENTS"
REFERENCES,0.517814726840855,"A.1
EXPERIMENTS ON SYNTHETIC DATASETS"
REFERENCES,0.5201900237529691,"A.1.1
REWARD PREDICTION WITH MORE COMPLICATED NEURAL NETWORKS"
REFERENCES,0.5225653206650831,"In the main paper, to compare NPR with the existing neural bandit baselines, we chose a relatively
small neural network, e.g., m = 64, so that NeuralUCB/NeuralTS can be executed with a full co-
variance matrix. To better capture the non-linearity in reward generation, we increased the width of
the neural network to m = 128, and report the results in Figure 4. All other experiment settings
(e.g., underlying reward generation function) are the same as reported in the main paper. And to pro-
vide a more comprehensive picture, we also reported the variance of each algorithm’s performance
in this ﬁgure."
REFERENCES,0.5249406175771971,"With the increased network width and limited by the GPU memory, NeuralUCB and NeuralTS can
only be executed with the diagonal matrix of the covariance matrix when computing their required
“exploration bonus term”. In Figure 4, we can observe 1) all neural bandit algorithms obtained the
expected sublinear regret; and 2) NPR performed better than NeuralUCB and NeuralTS, because of
the diagonal approximation that we have to use on these two baselines. This again strongly suggested
the advantage of NPR: the approximation due to the computational complexity in NeuralUCB and
NeuralTS limits their practical effectiveness, though they have the same theoretical regret bound as
NPR’s. 3) FALCON+ shows better performance in the initial rounds, while with more iterations, its
regret is higher than the neural bandit baselines’ and our NPR model’. Compared to the neural bandit
models, FALCON+ has much slower convergence, e.g, the slope of its regret curve did not decrease
as fast as other models’ over the course of interactions. After a careful investigation, we found
this is caused by its scheduled update: because less and less updates can be performed in the later
rounds, FALCON+ still selected the suboptimal arms quite often. Another important observation is
that FALCON+ has a very high variance in its performance: in some epochs, it could keep sampling
the suboptimal arms. This is again caused by its lazy update: in some epochs of some trials, its
estimation could be biased by large reward noise, which will directly cost it miss the right arm in
almost the entire next epoch."
REFERENCES,0.5273159144893111,"(a) h1(x) = 10−2(x⊤ΣΣ⊤x)
(b) h2(x) = exp(−10(x⊤θ)2)"
REFERENCES,0.5296912114014252,Figure 4: Comparison of NPR and the baselines on the synthetic dataset with m = 128
REFERENCES,0.5320665083135392,"A.1.2
COMPARISONS UNDER DIFFERENT VALUES OF k"
REFERENCES,0.5344418052256532,"For the experiment in the main paper, at each round only k = 20 arms are sampled from the K = 100
arm pool and disclosed to all the algorithms for selection. It is to test a more general setting in
practice, e.g., different recommendation candidates appear across rounds. In the meantime, such
setting also introduces more practical difﬁculties as only a subset of arms are revealed to the agent
to learn from each time. In this experiment, we report the results of the neural bandit models with"
REFERENCES,0.5368171021377672,Published as a conference paper at ICLR 2022
REFERENCES,0.5391923990498813,"(a) h1(x) = 10−2(x⊤ΣΣ⊤x)
(b) h2(x) = exp(−10(x⊤θ)2)"
REFERENCES,0.5415676959619953,Figure 5: Comparison of NPR and the baselines on the synthetic dataset with different k.
REFERENCES,0.5439429928741093,"different value k, e.g., the number of arms disclosed each round. We followed the setting in Section
A.1.1 to set the network width m = 128."
REFERENCES,0.5463182897862233,"Figure 5 demonstrates that for both datasets, revealing all the candidate arms, e.g., k = 100, will
considerably reduce the regret compared to only revealing a subset of the arms. But in both settings,
all bandit algorithms, including NPR, can obtain desired performance (i.e., sublinear regret). And
again, NPR still obtained more satisfactory performance as it is not subject to the added compu-
tational complexity in NeuralUCB and NeuralTS. The FALCON+ baseline performed much better
when k = 20 than k = 100, as it has less chance to select the suboptimal arms in each epoch, which
is expected based on its polynomial dependence on the number of arms K. On the contrary, NPR
only has a logarithmic dependence on K, whose advantage was validated in this experiment."
REFERENCES,0.5486935866983373,"A.2
EXPERIMENTS ON CLASSIFICATION DATASETS"
REFERENCES,0.5510688836104513,"0
2000
4000
6000
8000
10000
Rounds 0 500 1000 1500 2000 2500"
REFERENCES,0.5534441805225653,Regret
REFERENCES,0.5558194774346793,"LinFPL
LinTS
LinUCB
NPR
NeuralTS
NeuralTS (Diagonal)
NeuralUCB
NeuralUCB (Diagonal)"
REFERENCES,0.5581947743467933,(a) MagicTelescope
REFERENCES,0.5605700712589073,"0
2000
4000
6000
8000
10000
Rounds 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.5629453681710214,Regret
REFERENCES,0.5653206650831354,"LinFPL
LinTS
LinUCB
NPR
NeuralTS (Diagonal)
NeuralUCB (Diagonal)"
REFERENCES,0.5676959619952494,(b) Letter
REFERENCES,0.5700712589073634,"0
2000
4000
6000
8000
10000
Rounds 0 500 1000 1500 2000 2500 3000 3500 4000"
REFERENCES,0.5724465558194775,Regret
REFERENCES,0.5748218527315915,"LinFPL
LinTS
LinUCB
NPR
NeuralTS (Diagonal)
NeuralUCB (Diagonal)"
REFERENCES,0.5771971496437055,(c) Covertype
REFERENCES,0.5795724465558195,Figure 6: Comparison of NPR and the baselines on the UCI datasets.
REFERENCES,0.5819477434679335,"Table 1: Dataset statistics
DATASET
ADULT
MUSHROOM
SHUTTLE
MAGIC
LETTER
COVERTYPE
INPUT DIMENSION
2 × 15
2 ×23
7 × 9
2 × 12
16 × 26
54 × 7"
REFERENCES,0.5843230403800475,"We
present
the
experiment
results
of
the
classiﬁcation
problem
on
UCI
datasets,
MagicTelescope, letter and covertype.
The experiment setting and grid search
setting for parameter tuning are the same as described in Section 5. For datasets letter and
covertype, only the results of NeuralUCB and NeuralTS with diagonal approximation are
reported as the required GPU memory for the calculation of matrix inverse, together with the
neural model update, exceeds the supported memory of the NVIDIA GEFORCE RTX 2080
graphical card. Similar observations are obtained for these three datasets. Compared to the linear
contextual models, the neural contextual bandit algorithms signiﬁcantly improve the performance
with much lower regret (mistakes on classiﬁcation). NPR showed consistently better performance
to the best neural bandit baselines.
And due to its perturbation-based exploration strategy, no
extra computational resources are needed, which makes it more practical in real-world problems,
especially large-scale tasks."
REFERENCES,0.5866983372921615,Published as a conference paper at ICLR 2022
REFERENCES,0.5890736342042755,"B
BACKGROUND FOR THEORETICAL ANALYSIS"
REFERENCES,0.5914489311163895,"We denote the set of all the possible K arms by their context vectors as {xi}K
i=1. Our analysis is
built upon the existing work in neural tangent kernel techniques.
Deﬁnition B.1 (Jacot et al. (2018); Cao & Gu (2019)). Let {xi}K
i=1 be a set of contexts. Deﬁne"
REFERENCES,0.5938242280285035,"eH(1)
i,j = Σ(1)
i,j = ⟨xi, xj⟩,
B(l)
i,j ="
REFERENCES,0.5961995249406176,"Σ(l)
i,i
Σ(l)
i,j
Σ(l)
i,j
Σ(l)
j,j ! ,"
REFERENCES,0.5985748218527316,"Σ(l+1)
i,j
= 2E(u,v)∼N(0,B(l)
i,j) [φ(u)φ(v)] ,"
REFERENCES,0.6009501187648456,"eH(l+1)
i,j
= 2 eH(l)
i,jE(u,v)∼N(0,B(l)
i,j) [φ′(u)φ′(v)] + Σ(l+1)
i,j
."
REFERENCES,0.6033254156769596,"Then, H = ( eH(L) + Σ(L))/2 is called the neural tangent kernel (NTK) matrix on the context set."
REFERENCES,0.6057007125890737,"With Deﬁnition B.1, we also have the following assumption on the contexts: {xi}K
i=1.
Assumption B.2. H ⪰λ0I; and moreover, for any 1 ≤i ≤K, ∥xi∥2 = 1 and [xi]j = [xi]j+d/2."
REFERENCES,0.6080760095011877,"By assuming H ⪰λ0I, the neural tangent kernel matrix is non-singular (Du et al., 2019; Arora et al.,
2019; Cao & Gu, 2019). It can be easily satisﬁed when no two context vectors in the context set are
in parallel. The second assumption is just for convenience in analysis and can be easily satisﬁed by:
for any context vector x, ∥x∥2 = 1, we can construct a new context x′ = [x⊤, x⊤]⊤/
√"
IT CAN BE,0.6104513064133017,"2. It can be
veriﬁed that if θ0 is initialized as in Algorithm 1, then f(xi; θ0) = 0 for any i ∈[K]."
IT CAN BE,0.6128266033254157,"The NTK technique builds a connection between the analysis of DNN and kernel methods. With
the following deﬁnition of effective dimension, we are able to analyze the neural network by some
complexity measures previously used for kernel methods."
IT CAN BE,0.6152019002375297,"Deﬁnition B.3. The effective dimension ed of the neural tangent kernel matrix on contexts {xi}K
i=1
is deﬁned as"
IT CAN BE,0.6175771971496437,ed = log det(I + TH/λ)
IT CAN BE,0.6199524940617577,"log(1 + TK/λ)
."
IT CAN BE,0.6223277909738717,"The notion of effective dimension is ﬁrst introduced in Valko et al. (2013) for analyzing kernel
contextual bandits, which describes the actual underlying dimension in the set of observed contexts
{xi}K
i=1."
IT CAN BE,0.6247030878859857,"And we also need the following concentration bound on Gaussian distribution.
Lemma B.4. Consider a normally distributed random variable X ∼N(µ, σ2) and β ≥0. The
probability that X is within a radius of βσ from its mean can be written as,"
IT CAN BE,0.6270783847980997,P(|X −µ| ≤βσ) ≥1 −exp(−β2/2).
IT CAN BE,0.6294536817102138,"C
PROOF OF THEOREMS AND LEMMAS IN SECTION 4"
IT CAN BE,0.6318289786223278,"C.1
PROOF OF LEMMA 4.2"
IT CAN BE,0.6342042755344418,"Proof of Lemma 4.2. By Lemma 4.1, with a satisﬁed m, with probability at least 1 −δ, we have for
any i ∈[K],"
IT CAN BE,0.6365795724465558,"h(xi) = ⟨g(xi; θ0)/√m, √m(θ∗−θ0)⟩, and √m∥θ∗−θ0∥2 ≤
√"
IT CAN BE,0.6389548693586699,2h⊤H−1h ≤S.
IT CAN BE,0.6413301662707839,"Hence, based on the deﬁnition of At and ¯bt, we know that A−1
t
¯bt is the least square solution on top
of the observation noise. Therefore, by Theorem 1 in Abbasi-Yadkori et al. (2011), with probability
at least 1 −δ, for any 1 ≤t ≤T, θ∗satisﬁes that"
IT CAN BE,0.6437054631828979,"∥√m(θ∗−θ0) −A−1
t
¯bt∥At ≤ s"
IT CAN BE,0.6460807600950119,R2 log det(At)
IT CAN BE,0.6484560570071259,"δ2 det(λI) +
√"
IT CAN BE,0.6508313539192399,"λS,
(C.1)"
IT CAN BE,0.6532066508313539,Published as a conference paper at ICLR 2022
IT CAN BE,0.6555819477434679,"where R is the sub-Gaussian variable for the observation noise η. Then we have,"
IT CAN BE,0.6579572446555819,"|⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩−h(xt,i)|"
IT CAN BE,0.6603325415676959,"=|⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩−⟨g(xt,i; θ0), θ∗−θ0⟩|"
IT CAN BE,0.66270783847981,"≤∥√m(θ∗−θ0) −A−1
t
¯bt∥At∥g(xt,i; θ0)/√m∥A−1
t
≤αt∥g(xt,i; θ0)/√m∥A−1
t ."
IT CAN BE,0.665083135391924,"with αt =
q"
IT CAN BE,0.667458432304038,"R2 log
det(At)
δ2 det(λI) +
√"
IT CAN BE,0.669833729216152,λS. This completes the proof.
IT CAN BE,0.672209026128266,"C.2
PROOF OF LEMMA 4.3"
IT CAN BE,0.6745843230403801,"The proof starts with three lemmas that bound the error terms of the function value and gradient of
a neural network.
Lemma C.1 (Lemma B.2, Zhou et al. (2020)). There exist constants { ¯Ci}5
i=1 > 0 such that for
any δ > 0, with J as the number of steps for gradient descent in neural network learning, if for all
t ∈[T], η and m satisfy"
P,0.6769596199524941,"2
p"
P,0.6793349168646081,"t/(mλ) ≥¯C1m−3/2L−3/2[log(KL2/δ)]3/2,"
P,0.6817102137767221,"2
p"
P,0.684085510688836,"t/(mλ) ≤¯C2 min

L−6[log m]−3/2,
 
m(λη)2L−6t−1(log m)−13/8	
,"
P,0.6864608076009501,"η ≤¯C3(mλ + tmL)−1,"
P,0.6888361045130641,"m1/6 ≥¯C4
p"
P,0.6912114014251781,"log mL7/2t7/6λ−7/6(1 +
p t/λ),"
P,0.6935866983372921,"then with probability at least 1 −δ, we have that ∥θt−1 −θ0∥2 ≤2
p"
P,0.6959619952494062,t/(mλ) and
P,0.6983372921615202,"∥θt−1 −θ0 −A−1
t
¯bt/√m∥2 ≤(1 −ηmλ)J/2p"
P,0.7007125890736342,t/(mλ) + ¯C5m−2/3p
P,0.7030878859857482,"log mL7/2t5/3λ−5/3(1 +
p t/λ)."
P,0.7054631828978623,"Lemma C.2 (Lemma B.3 Zhou et al. (2020)). There exist constants { ¯Cu
i }3
i=1 > 0 such that for any
δ > 0, if τ satisﬁes"
P,0.7078384798099763,"¯Cu
1 m−3/2L−3/2[log(KL2/δ)]3/2 ≤τ ≤¯Cu
2 L−6[log m]−3/2,"
P,0.7102137767220903,"then with probability at least 1 −δ, for any eθ and bθ satisfying ∥eθ −θ0∥2 ≤τ, ∥bθ −θ0∥2 ≤τ and
j ∈[K] we have
f(xj; eθ) −f(xj; bθ) −⟨g(xj; bθ), eθ −bθ⟩
 ≤¯Cu
3 τ 4/3L3p"
P,0.7125890736342043,m log m.
P,0.7149643705463183,"Lemma C.3 (Lemma B.4, Zhou et al. (2020)). There exist constants { ¯Cv
i }3
i=1 > 0 such that for any
δ > 0, if τ satisﬁes that"
P,0.7173396674584323,"¯Cv
1m−3/2L−3/2[log(KL2/δ)]3/2 ≤τ ≤¯Cv
2L−6[log m]−3/2,"
P,0.7197149643705463,"then with probability at least 1 −δ, for any ∥θ −θ0∥2 ≤τ and j ∈[K] we have ∥g(xj; θ)∥F ≤
¯Cv
3
√ mL."
P,0.7220902612826603,"With above lemmas, we prove Lemma 4.3 as follows."
P,0.7244655581947743,"Proof of Lemma 4.3. According to Lemma C.1, with satisﬁed neural network and the learning rate
η, we have ∥θt−1 −θ0∥2 ≤2
p"
P,0.7268408551068883,"t/mλ. Further, by the choice of m, Lemma C.2 and C.3 hold."
P,0.7292161520190024,"Therefore, |f(xt,i; θt−1) −⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩| can be ﬁrst upper bounded by:"
P,0.7315914489311164,"|f(xt,i; θt−1) −⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩|"
P,0.7339667458432304,"≤|f(xt,i; θt−1) −f(xt,i; θ0) −⟨g(xt,i; θ0), θt−1 −θ0⟩| + |⟨g(xt,i; θ0), θt−1 −θ0 −A−1
t
¯bt/√m⟩|"
P,0.7363420427553444,"≤Cϵ,1m−1/6T 2/3λ−2/3L3p"
P,0.7387173396674585,"log m + |⟨g(xt,i; θ0), θt−1 −θ0 −A−1
t
¯bt/√m⟩|
(C.2)"
P,0.7410926365795725,"where the ﬁrst inequality holds due to triangle inequality, f(xt,i; θ0) = 0 by the random initial-
ization of θ0, and the second inequality holds due to Lemma C.2 with the fact ∥θt−1 −θ0∥2 ≤
2
p t/mλ."
P,0.7434679334916865,Published as a conference paper at ICLR 2022
P,0.7458432304038005,"Next, we bound the second term of Eq (C.2),"
P,0.7482185273159145,"|⟨g(xt,i; θ0), θt−1 −θ0 −A−1
t
¯bt/√m⟩|"
P,0.7505938242280285,"≤|⟨g(xt,i; θ0), θt−1 −θ0 −A−1
t bt/√m⟩| + |⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩|"
P,0.7529691211401425,"≤∥g(xt,i; θ0)∥2∥θt−1 −θ0 −A−1
t bt/√m∥2 + |⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩|"
P,0.7553444180522565,"≤Cϵ,2(1 −ηmλ)Jp"
P,0.7577197149643705,"TL/λ + Cϵ,3m−1/6T 5/3λ−5/3L4p"
P,0.7600950118764845,"log m(1 +
p T/λ)"
P,0.7624703087885986,"+ |⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩|
(C.3)"
P,0.7648456057007126,"where the ﬁrst inequality holds due to the triangle inequality, the second inequality holds according
to Cauchy–Schwarz inequality, and the third inequality holds due to Lemma C.1, C.3, with the fact
that ∥θt−1 −θ0∥2 ≤2
p"
P,0.7672209026128266,t/mλ and t ≤T.
P,0.7695961995249406,"Now, we provide the upper bound of the last term in Eq (C.3). Based on the deﬁnition of bt and ¯bt,
we have"
P,0.7719714964370546,"|⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩| = |⟨g(xt,i; θ0)/√m, A−1
t
Xt−1"
P,0.7743467933491687,"s=1 γt−1
s
g(xs,as; θ0)⟩|"
P,0.7767220902612827,"According to the deﬁnition of the added random noise γt
s ∼N(0, σ2), and Lemma B.4, with βt ≥0,
we have the following inequality, Pt"
P,0.7790973871733967,"
|⟨g(xt,i; θ0)/√m, A−1
t
Xt−1"
P,0.7814726840855107,"s=1 γt−1
s
g(xs,as; θ0)/√m⟩| ≥βt∥g(xt,i; θ0)/√m∥A−1
t "
P,0.7838479809976246,≤2 exp 
P,0.7862232779097387,"−
β2
t ∥g(xt,i; θ0)/√m∥2
A−1
t
2σ2g(xt,i; θ0)⊤A−1
t (Pt−1
s=1 g(xs,as; θ0)g(xs,as; θ0)⊤/m)A−1
t g(xt,i; θ0)/m !"
P,0.7885985748218527,"≤2 exp

−β2
t
2σ2 
,"
P,0.7909738717339667,"where the last inequality stands as,"
P,0.7933491686460807,"g(xt,i; θ0)⊤A−1
t (
Xt−1"
P,0.7957244655581948,"s=1 g(xs,as; θ0)g(xs,as; θ0)⊤/m)A−1
t g(xt,i; θ0)/m"
P,0.7980997624703088,"≤g(xt,i; θ0)⊤A−1
t (
Xt−1"
P,0.8004750593824228,"s=1 g(xs,as; θ0)g(xs,as; θ0)⊤/m + λI)A−1
t g(xt,i; θ0)/m"
P,0.8028503562945368,"=∥g(xt,i; θ0)/√m∥2
A−1
t ."
P,0.8052256532066508,"Therefore, in round t, for the given arm i,"
P,0.8076009501187649,"Pt

|⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩| ≤βt∥g(xt,i; θ0)/√m∥A−1
t"
P,0.8099762470308789,"
≥1 −exp(−β2
t /(2σ2))."
P,0.8123515439429929,"Taking the union bound over K arms, we have that for any t:"
P,0.8147268408551069,"Pt

∀i ∈[K], |⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩| ≤βt∥g(xt,i; θ0)/√m∥A−1
t"
P,0.8171021377672208,"
≥1 −K exp(−β2
t /(2σ2))."
P,0.8194774346793349,"By choosing βt = σ√4 log t + 2 log K, we have:"
P,0.8218527315914489,"Pt

∀i ∈[K], |⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩| ≤βt∥g(xt,i; θ0)/√m∥A−1
t"
P,0.8242280285035629,"
≥1 −1 t2 ."
P,0.8266033254156769,This completes the proof.
P,0.828978622327791,"C.3
PROOF OF LEMMA 4.4"
P,0.831353919239905,The proof requires the anti-concentration bound of Gaussian distribution.
P,0.833729216152019,"Lemma C.4. For a Gaussian random variable X ∼N(µ, σ2), for any β > 0,"
P,0.836104513064133,"P
X −µ"
P,0.838479809976247,"σ
> β

≥exp(−β2)"
P,0.8408551068883611,"4√πβ
."
P,0.8432304038004751,Published as a conference paper at ICLR 2022
P,0.8456057007125891,"Proof of Lemma 4.4. Under event Et,2, according to Lemma 4.2, (C.2) and (C.3), we have, for all
i ∈[K],"
P,0.8479809976247031,"Pt(f(xt,i; θt) > h(xt,a∗
t ) −ϵ(m))"
P,0.850356294536817,"≥Pt(⟨g(xt,i; θ0), A−1
t bt/√m⟩> ⟨g(xt,i; θ0), A−1
t
¯bt/√m⟩+ αt∥g(xt,i; θ0)/√m∥A−1
t )"
P,0.8527315914489311,"According to the deﬁnition of At, ¯bt and bt, we have,"
P,0.8551068883610451,"⟨g(xt,i; θ0), A−1
t bt/√m −A−1
t
¯bt/√m⟩= t
X"
P,0.8574821852731591,"s=1
γt
s · 1"
P,0.8598574821852731,"mg(xt,i; θ0)⊤A−1
t g(xs,as; θ0) = Ut"
P,0.8622327790973872,which follows a Gaussian distribution with mean E[Ut] = 0 and variance Var[Ut] as:
P,0.8646080760095012,"Var[Ut] =σ2 · (
Xt"
P,0.8669833729216152,s=1( 1
P,0.8693586698337292,"mg(xt,i; θ0)⊤A−1
t g(xs,as; θ0))2)"
P,0.8717339667458432,"=σ2∥g(xt,i; θ0)/√m∥2
A−1
t
−λσ2∥g(xt,i; θ0)/√m∥2
A−2
t
≥σ2(1 −λλ−1
min(At))∥g(xt,i; θ0)/√m∥2
A−1
t
≥σ2(1 −λλ−1
K (AK))∥g(xt,i; θ0)/√m∥2
A−1
t ,"
P,0.8741092636579573,"where the second equality holds according to the deﬁnition of At, and the ﬁrst inequality holds as
for any positive semi-deﬁnite matrix M ∈Rd×d,"
P,0.8764845605700713,"x⊤M2x = λ2
max(M)x⊤(λ−2
max(M)M2)x ≤λmax(M)∥x∥2
M,"
P,0.8788598574821853,"and λmin(At)
≥
λK(AK).
Therefore,
based on Lemma C.4,
by choosing σ
= αt/
q"
P,0.8812351543942993,"1 −λλ−1
K (AK), the target probability could be lower bounded by,"
P,0.8836104513064132,"Pt(f(xt,j; θt) > h(xt,a∗
t ) −ϵ(m)) ≥Pt(Ut > αt∥g(xt,i; θ0)/√m∥A−1
t )"
P,0.8859857482185273,"=Pt(
Ut
Var[Ut] >
αt∥g(xt,i; θ0)/√m∥A−1
t
Var[Ut]
) ≥Pt(
Ut
Var[Ut] > 1) ≥
1
4e√π"
P,0.8883610451306413,This completes the proof.
P,0.8907363420427553,"C.4
PROOF OF LEMMA 4.5"
P,0.8931116389548693,"Proof of Lemma 4.5. With the deﬁned sufﬁciently sampled arms in round t, Ωt, we have the set of
undersampled arms ¯Ωt = [K] \ Ωt. We also have the least uncertain and undersampled arm et in
round t deﬁned as,"
P,0.8954869358669834,"et = argmin
j∈¯Ωt
∥g(xj,t; θ0)/√m∥A−1
t"
P,0.8978622327790974,"In round t, it is easy to verify that E[h(xt,a∗
t ) −h(xt,at)] ≤E[(h(xt,a∗
t ) −h(xt,at))1{Et,2}] +
Pt( ¯Et,2)."
P,0.9002375296912114,"Under event Et,1 and Et,2, we have,"
P,0.9026128266033254,"h(xt,a∗
t ) −h(xt,at)"
P,0.9049881235154394,"=h(xt,a∗
t ) −h(xt,et) + h(xt,et) −h(xt,at)"
P,0.9073634204275535,"≤∆et + f(xt,et, θt) −f(xt,at, θt) + 2ϵ(m) + (βt + αt)(∥g(xt,et; θ0)/√m∥A−1
t
+ ∥g(xt,at; θ0)/√m∥A−1
t )"
P,0.9097387173396675,"≤4ϵ(m) + (βt + αt)(2∥g(xt,et; θ0)/√m∥A−1
t
+ ∥g(xt,at; θ0)/√m∥A−1
t )"
P,0.9121140142517815,"Next, we will bound E[∥g(xt,et; θ0)/√m∥A−1
t ]. It is easy to see that,"
P,0.9144893111638955,"E[∥g(xt,at; θ0)/√m∥A−1
t ]"
P,0.9168646080760094,"=E[∥g(xt,at; θ0)/√m∥A−1
t 1{at ∈¯Ωt}] + E[∥g(xt,at; θ0)/√m∥A−1
t 1{at ∈Ωt}]"
P,0.9192399049881235,"≥∥g(xt,et; θ0)/√m∥A−1
t Pt(at ∈¯Ωt)."
P,0.9216152019002375,Published as a conference paper at ICLR 2022
P,0.9239904988123515,"It can be arranged as ∥g(xt,et; θ0)/√m∥A−1
t
≤E[∥g(xt,at; θ0)/√m∥A−1
t ]/Pt(at ∈¯Ωt). Hence,
the one step regret can be bounded by the following inequality with probability at least 1 −δ,
E[h(xt,a∗
t ) −h(xt,at)]"
P,0.9263657957244655,"≤P( ¯Et,2) + 4ϵ(m) + (βt + αt)
 
1 + 2/P(at ∈¯Ωt)

∥g(xt,at; θ0)/√m∥A−1
t ."
P,0.9287410926365796,"For the probability Pt(at ∈¯Ωt), we have:
Pt(at ∈¯Ωt) ≥Pt(∃ai ∈¯Ωt : f(xt,ai; θt) > max
aj∈Ωt f(xt,aj; θt))"
P,0.9311163895486936,"≥Pt(f(xt,a∗
t ; θt) > max
aj∈Ωt f(xt,aj; θt))"
P,0.9334916864608076,"≥Pt(f(xt,a∗
t ; θt) > max
aj∈Ωt f(xt,aj; θt), Et,2 occurs)"
P,0.9358669833729216,"≥Pt(f(xt,a∗
t ; θt) > h(xt,a∗
t ) −ϵ(m)) −Pt( ¯Et,2)"
P,0.9382422802850356,"≥Pt(Et,3) −Pt( ¯Et,2)
where the fourth inequality holds as for arm aj ∈Ωt, under event Et,1 and Et,2:
f(xt,j; θt) ≤h(xt,j) + ϵ(m) + (βt + αt)∥g(xt,j; θ0)/√m∥A−1
t
≤h(xt,a∗
t ) −ϵ(m)."
P,0.9406175771971497,This completes the proof.
P,0.9429928741092637,"C.5
PROOF OF LEMMA 4.6"
P,0.9453681710213777,"We ﬁrst need the following lemma from Abbasi-Yadkori et al. (2011).
Lemma C.5 (Lemma 11, Abbasi-Yadkori et al. (2011)). We have the following inequality:
XT"
P,0.9477434679334917,"t=1 min

∥g(xt,at; θ0)/√m∥2
A−1
t−1, 1

≤2 log det AT"
P,0.9501187648456056,det λI .
P,0.9524940617577197,"Lemma C.6 (Lemma B.1, Zhou et al. (2020)). Let G = [g(x1; θ0), . . . , g(xK; θ0)]/√m ∈Rp×K.
Let H be the NTK matrix as deﬁned in Deﬁnition B.1. For any δ ∈(0, 1), if"
P,0.9548693586698337,"m = Ω
L6 log(K2L/δ) ϵ4 
,"
P,0.9572446555819477,"then with probability at least 1 −δ, we have
∥G⊤G −H∥F ≤Kϵ."
P,0.9596199524940617,"Proof of Lemma 4.6. Denote G = [g(x1; θ0)/√m, . . . , g(xK; θ0)/√m] ∈Rp×K, then we have"
P,0.9619952494061758,log det AT
P,0.9643705463182898,"det λI = log det

I +
XT"
P,0.9667458432304038,"t=1 g(xt,at; θ0)g(xt,at; θ0)⊤/(mλ)
"
P,0.9691211401425178,"≤log det

I +
XT t=1 XK"
P,0.9714964370546318,"i=1 g(xi; θ0)g(xi; θ0)⊤/(mλ)
"
P,0.9738717339667459,"= log det

I + TGG⊤/λ

= log det

I + TG⊤G/λ

,
(C.4)"
P,0.9762470308788599,"where the inequality holds naively, the third equality holds since for any matrix A ∈Rp×K, we
have det(I + AA⊤) = det(I + A⊤A). We can further bound Eq (C.4) as follows:"
P,0.9786223277909739,"log det

I + TG⊤G/λ

= log det

I + TH/λ + T(G⊤G −H)/λ
"
P,0.9809976247030879,"≤log det

I + TH/λ

+ ⟨(I + TH/λ)−1, T(G⊤G −H)/λ⟩"
P,0.9833729216152018,"≤log det

I + TH/λ

+ ∥(I + TH/λ)−1∥F ∥G⊤G −H∥F · T/λ"
P,0.9857482185273159,"≤log det

I + TH/λ

+ T
√"
P,0.9881235154394299,K∥G⊤G −H∥F
P,0.9904988123515439,"≤log det

I + TH/λ

+ 1 = ed log(1 + TK/λ) + 1,"
P,0.9928741092636579,Published as a conference paper at ICLR 2022
P,0.995249406175772,"where the ﬁrst inequality holds due to the concavity of log det(·), the second inequality holds due
to the fact that ⟨A, B⟩≤∥A∥F ∥B∥F , the third inequality holds due to the facts that I + H/λ ⪰I,
λ ≥1 and ∥A∥F ≤
√"
P,0.997624703087886,"K∥A∥2 for any A ∈RK×K, the fourth inequality holds by Lemma C.6
with the required choice of m, the ﬁfth inequality holds by the deﬁnition of effective dimension in
Deﬁnition B.3. This completes the proof."
