Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001001001001001001,"Given the remarkable capabilities of large language models (LLMs), there has
1"
ABSTRACT,0.002002002002002002,"been a growing interest in evaluating their similarity to the human brain. One
2"
ABSTRACT,0.003003003003003003,"approach towards quantifying this similarity is by measuring how well a model
3"
ABSTRACT,0.004004004004004004,"predicts neural signals, also called ""brain score"". Internal representations from
4"
ABSTRACT,0.005005005005005005,"LLMs achieve state-of-the-art brain scores, leading to speculation that they share
5"
ABSTRACT,0.006006006006006006,"computational principles with human language processing. This inference is only
6"
ABSTRACT,0.007007007007007007,"valid if the subset of neural activity predicted by LLMs reflects core elements
7"
ABSTRACT,0.008008008008008008,"of language processing. Here, we question this assumption by analyzing three
8"
ABSTRACT,0.009009009009009009,"neural datasets used in an impactful study on LLM-to-brain mappings, with a
9"
ABSTRACT,0.01001001001001001,"particular focus on an fMRI dataset where participants read short passages. We
10"
ABSTRACT,0.011011011011011011,"first find that when using shuffled train-test splits, as done in previous studies
11"
ABSTRACT,0.012012012012012012,"with these datasets, a trivial feature that encodes temporal autocorrelation not only
12"
ABSTRACT,0.013013013013013013,"outperforms LLMs but also accounts for the majority of neural variance that LLMs
13"
ABSTRACT,0.014014014014014014,"explain. We therefore caution against shuffled train-test splits, and use contiguous
14"
ABSTRACT,0.015015015015015015,"test splits moving forward. Second, we explain the surprising result that untrained
15"
ABSTRACT,0.016016016016016016,"LLMs have higher-than-expected brain scores by showing they do not account
16"
ABSTRACT,0.01701701701701702,"for additional neural variance beyond two simple features: sentence length and
17"
ABSTRACT,0.018018018018018018,"sentence position. This undermines evidence used to claim that the transformer
18"
ABSTRACT,0.01901901901901902,"architecture biases computations to be more brain-like. Third, we find that brain
19"
ABSTRACT,0.02002002002002002,"scores of trained LLMs on this dataset can largely be explained by sentence
20"
ABSTRACT,0.021021021021021023,"position, sentence length, and static word vectors; a small, additional amount is
21"
ABSTRACT,0.022022022022022022,"explained by sense-specific word embeddings and contextual representations of
22"
ABSTRACT,0.023023023023023025,"sentence structure. We conclude that over-reliance on brain scores can lead to
23"
ABSTRACT,0.024024024024024024,"over-interpretations of similarity between LLMs and brains, and emphasize the
24"
ABSTRACT,0.025025025025025027,"importance of deconstructing what LLMs are mapping to in neural signals.
25"
INTRODUCTION,0.026026026026026026,"1
Introduction
26"
INTRODUCTION,0.02702702702702703,"Recent developments in large language models (LLMs) have led many to wonder whether LLMs
27"
INTRODUCTION,0.028028028028028028,"process language like humans do. Whereas LLMs acquire many abstract linguistic generalizations, it
28"
INTRODUCTION,0.02902902902902903,"remains unclear to what extent their internal machinery bears resemblance to the human brain [1]. A
29"
INTRODUCTION,0.03003003003003003,"number of studies have attempted to answer this question through the framework of neural encoding
30"
INTRODUCTION,0.031031031031031032,"[2–4]. Within this framework, an LLM’s internal representations of some linguistic stimuli are used
31"
INTRODUCTION,0.03203203203203203,"to predict brain activity during comprehension of the same stimuli. Results have been uniformly
32"
INTRODUCTION,0.03303303303303303,"positive, showing that LLM representations are highly effective at predicting neural signals [5, 6].
33"
INTRODUCTION,0.03403403403403404,"In one impactful study, authors evaluated the brain scores of 43 models on three neural datasets [2].
34"
INTRODUCTION,0.035035035035035036,"They found that GPT2-XL [7] achieved the highest brain score and, in one neural dataset, accounted
35"
INTRODUCTION,0.036036036036036036,"for 100% of the ""explainable"" neural variance (i.e., taking into account the noise inherent in the data)
36"
INTRODUCTION,0.037037037037037035,"[8]. This result was interpreted as evidence that the brain may be optimizing for the same objective
37"
INTRODUCTION,0.03803803803803804,"as GPT2, namely, next-word prediction. Surprisingly, the authors further found that untrained (i.e.
38"
INTRODUCTION,0.03903903903903904,"randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer
39"
INTRODUCTION,0.04004004004004004,"architecture biases computations to be more brain-like. The finding that untrained LLMs predict
40"
INTRODUCTION,0.04104104104104104,"neural signals significantly above chance has been replicated in other studies [9, 4, 10].
41"
INTRODUCTION,0.042042042042042045,"More generally, many studies have compared models to brain activity and concluded that high
42"
INTRODUCTION,0.043043043043043044,"prediction performance reveals correspondence between some interesting aspect of the model and
43"
INTRODUCTION,0.044044044044044044,"biological linguistic processing [4, 11–14]. One issue with this approach is that it assumes that the
44"
INTRODUCTION,0.04504504504504504,"subset of neural activity predicted by a model reflects core processes of the human language system
45"
INTRODUCTION,0.04604604604604605,"[15]. However, this assumption is not necessarily true. For example, a recent paper found that, when
46"
INTRODUCTION,0.04704704704704705,"participants listen to stories, the fMRI signal includes an initial ramping, positional artifact [16].
47"
INTRODUCTION,0.04804804804804805,"It is likely that LLMs which contain absolute positional embeddings would be able to predict this
48"
INTRODUCTION,0.04904904904904905,"ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe, [17]) would
49"
INTRODUCTION,0.05005005005005005,"not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical
50"
INTRODUCTION,0.05105105105105105,"interest. This issue relates to a more general trend in machine learning research: a complex algorithm
51"
INTRODUCTION,0.05205205205205205,"solves a task, but it is later discovered that the key innovation was a very simple component of the
52"
INTRODUCTION,0.05305305305305305,"algorithm [18]. Analogous to Weinberger [18], without attempting to rigorously deconstruct the
53"
INTRODUCTION,0.05405405405405406,"mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain’s
54"
INTRODUCTION,0.055055055055055056,"mechanisms for processing language.
55"
INTRODUCTION,0.056056056056056056,"We analyze the same three neural datasets used in [2]. These include the Pereira fMRI dataset, where
56"
INTRODUCTION,0.057057057057057055,"participants read short passages [8]; the Fedorenko electrocorticography (ECoG) dataset, where
57"
INTRODUCTION,0.05805805805805806,"participants read isolated sentences [19]; and the Blank fMRI dataset, where participants listened to
58"
INTRODUCTION,0.05905905905905906,"short stories [20]. As in Schrimpf et al. [2], we focus our analyses on the Pereira dataset. In order to
59"
INTRODUCTION,0.06006006006006006,"deconstruct the mapping between LLMs and the brain, we follow Reddy and Wehbe [21] and de Heer
60"
INTRODUCTION,0.06106106106106106,"et al. [22] by building a set of predictors that describe simple features of the linguistic input, and
61"
INTRODUCTION,0.062062062062062065,"gradually add features that increase in complexity. Our goal is to find the simplest set of features
62"
INTRODUCTION,0.06306306306306306,"which account for the greatest portion of the mapping between LLMs and brains.
63"
METHODS,0.06406406406406406,"2
Methods
64"
EXPERIMENTAL DATA,0.06506506506506507,"2.1
Experimental data
65"
EXPERIMENTAL DATA,0.06606606606606606,"For all three neural datasets, we used the same version as used by [2]. For additional details, refer to
66"
EXPERIMENTAL DATA,0.06706706706706707,"A.1.
67"
EXPERIMENTAL DATA,0.06806806806806807,"Pereira (fMRI): The Pereira dataset is composed of two experiments. Experiment 1 (EXP1) consists
68"
EXPERIMENTAL DATA,0.06906906906906907,"of 96 passages each containing 4 sentences, with n = 9 participants. Experiment 2 (EXP2) consists of
69"
EXPERIMENTAL DATA,0.07007007007007007,"72 passages each consisting of 3 or 4 sentences, with n = 6 participants. Passages in each experiment
70"
EXPERIMENTAL DATA,0.07107107107107107,"were evenly divided into 24 semantic categories which were not related across experiments (4
71"
EXPERIMENTAL DATA,0.07207207207207207,"passages per category in EXP1, and 3 passages per category in EXP2). A single fMRI scan (TR)
72"
EXPERIMENTAL DATA,0.07307307307307308,"was taken after visual presentation of each sentence. Unless otherwise noted, we focus our results
73"
EXPERIMENTAL DATA,0.07407407407407407,"on voxels from within the ""language network"" in the main paper. EXP1 was a 384 × 92450 matrix
74"
EXPERIMENTAL DATA,0.07507507507507508,"(number of sentences × number of voxels) and EXP2 was a 243 × 60100 matrix. All analyses were
75"
EXPERIMENTAL DATA,0.07607607607607608,"conducted separately for each experiment.
76"
EXPERIMENTAL DATA,0.07707707707707707,"Fedorenko (ECoG): Participants (n = 5) read 52 sentences of length 8 words. A total of 97
77"
EXPERIMENTAL DATA,0.07807807807807808,"language-responsive electrodes were used across 5 participants: 47, 8, 9, 15, and 18, for participants
78"
EXPERIMENTAL DATA,0.07907907907907907,"1 through 5, respectively. Neural activity was temporally averaged across the full presentation of each
79"
EXPERIMENTAL DATA,0.08008008008008008,"word after extracting high gamma, and the entire dataset was a 416 × 97 matrix.
80"
EXPERIMENTAL DATA,0.08108108108108109,"Blank (fMRI): The dataset consisted of 5 participants listening to 8 stories from the publicly
81"
EXPERIMENTAL DATA,0.08208208208208208,"available Natural Stories Corpus [23]. An fMRI scan was taken every 2 seconds, resulting in a total
82"
EXPERIMENTAL DATA,0.08308308308308308,"of 1317 TRs across the 8 stories. fMRI BOLD signals were averaged across voxels within each
83"
EXPERIMENTAL DATA,0.08408408408408409,"functional region of interest (fROI). There were 60 fROIs across all 5 participants, resulting in a
84"
EXPERIMENTAL DATA,0.08508508508508508,"1317 × 60 matrix.
85"
LANGUAGE MODELS,0.08608608608608609,"2.2
Language models
86"
LANGUAGE MODELS,0.08708708708708708,"We focus our analyses on GPT2-XL [7], as it was shown to be the best-performing model on the
87"
LANGUAGE MODELS,0.08808808808808809,"Pereira dataset [10, 24, 2]. GPT2 is an auto-regressive transformer model, meaning that it can
88"
LANGUAGE MODELS,0.0890890890890891,"only attend to current and past inputs, trained on next token prediction. The XL variant has ∼1.5B
89"
LANGUAGE MODELS,0.09009009009009009,"parameters and 48 layers. We replicate some of our key findings on Pereira with RoBERTa-Large[25]
90"
LANGUAGE MODELS,0.09109109109109109,"(A.6). RoBERTa is a transformer model with bidirectional attention trained on masked token
91"
LANGUAGE MODELS,0.0920920920920921,"prediction, meaning that it can attend to past and future tokens. The large variant contains 335M
92"
LANGUAGE MODELS,0.09309309309309309,"parameters and 24 layers. Both GPT2 and RoBERTa use learned absolute positional embeddings,
93"
LANGUAGE MODELS,0.0940940940940941,"such that a unique vector corresponding to each token position is added to the input static embeddings.
94"
LLM FEATURE POOLING,0.09509509509509509,"2.3
LLM feature pooling
95"
LLM FEATURE POOLING,0.0960960960960961,"Pereira: Each sentence was fed into an LLM, with previous sentences from the same passage also fed
96"
LLM FEATURE POOLING,0.0970970970970971,"as input. Since each fMRI scan was taken at the end of the sentence, we converted LLM token-level
97"
LLM FEATURE POOLING,0.0980980980980981,"embeddings to sentence-level embeddings by summing across all tokens within a sentence (sum
98"
LLM FEATURE POOLING,0.0990990990990991,"pooling). We used the sum pooling method because it is consistent with other neural encoding studies
99"
LLM FEATURE POOLING,0.1001001001001001,"[26, 27], and it performed better than taking the representation at the last token which was done in
100"
LLM FEATURE POOLING,0.1011011011011011,"[2] A.5.
101"
LLM FEATURE POOLING,0.1021021021021021,"Fedorenko: The current and previous tokens from within the same sentence were fed into the LLM
102"
LLM FEATURE POOLING,0.1031031031031031,"as context. We converted LLM token-level embeddings to word embeddings, since each word has a
103"
LLM FEATURE POOLING,0.1041041041041041,"neural response, by summing across tokens in multi-token words, and leaving single token words
104"
LLM FEATURE POOLING,0.10510510510510511,"unmodified.
105"
LLM FEATURE POOLING,0.1061061061061061,"Blank: For each story, we fed the current and all preceding tokens up to a maximum context size of
106"
LLM FEATURE POOLING,0.10710710710710711,"512 tokens. As in Schrimpf et al. [2], for each TR, we took the representation of the word that was
107"
LLM FEATURE POOLING,0.10810810810810811,"closest to being 4 seconds before the TR. For multi-token words, we took the representation of the
108"
LLM FEATURE POOLING,0.1091091091091091,"last token of that word.
109"
BANDED RIDGE REGRESSION,0.11011011011011011,"2.4
Banded ridge regression
110"
BANDED RIDGE REGRESSION,0.1111111111111111,"We used ridge regression (linear regression with an L2 penalty) to predict activations for each
111"
BANDED RIDGE REGRESSION,0.11211211211211211,"voxel/electrode/fROI independently. We did not use ""vanilla"" ridge regression because it applies a
112"
BANDED RIDGE REGRESSION,0.11311311311311312,"single L2 penalty for all weights, whereas our analyses use multiple sets of distinct features. In such
113"
BANDED RIDGE REGRESSION,0.11411411411411411,"a case, a single penalty causes the regression will be biased against small feature spaces. Moreover,
114"
BANDED RIDGE REGRESSION,0.11511511511511512,"different L2 penalties are likely optimal for each feature space. To remedy this, we employed banded
115"
BANDED RIDGE REGRESSION,0.11611611611611612,"ridge regression which effectively allows a different L2 penalty to be applied to each feature space
116"
BANDED RIDGE REGRESSION,0.11711711711711711,"[28] (for further details, refer to A.2).
117"
BANDED RIDGE REGRESSION,0.11811811811811812,"2.5
Out of sample R2 metric
118"
BANDED RIDGE REGRESSION,0.11911911911911911,"We define the brain score of a model as the out-of-sample R2 metric (R2
oos) [29]. R2
oos quantifies
119"
BANDED RIDGE REGRESSION,0.12012012012012012,"how much better a set of features performs at predicting held-out data compared to a model which
120"
BANDED RIDGE REGRESSION,0.12112112112112113,"simply predicts the mean of the training data (i.e. a regression with only an intercept term). To be
121"
BANDED RIDGE REGRESSION,0.12212212212212212,"precise, given mean squared error (MSE) values from a model using features M and MSE values
122"
BANDED RIDGE REGRESSION,0.12312312312312312,"from an intercept only regression (I), then:
123"
BANDED RIDGE REGRESSION,0.12412412412412413,"R2
oos = 1 −MSEM"
BANDED RIDGE REGRESSION,0.12512512512512514,"MSEI
.
(1)"
BANDED RIDGE REGRESSION,0.12612612612612611,"A positive (negative) value indicates that M was more (less) helpful than predicting the mean of
124"
BANDED RIDGE REGRESSION,0.12712712712712712,"training data. We elected to use R2
oos over the standard R2 because of this clear interpretation
125"
BANDED RIDGE REGRESSION,0.12812812812812813,"and because it is a less biased estimate of test set performance [29]. We use R2
oos over Pearson’s
126"
BANDED RIDGE REGRESSION,0.12912912912912913,"correlation coefficient (r) because R2
oos can be interpreted as the fraction of variance explained,
127"
BANDED RIDGE REGRESSION,0.13013013013013014,"which lends more straightforwardly to estimating how much variance one feature space explains
128"
BANDED RIDGE REGRESSION,0.13113113113113112,"over others. Whenever averaging across voxels, we set R2
oos values to be non-negative to prevent
129"
BANDED RIDGE REGRESSION,0.13213213213213212,"differences in performance on noisy voxels/electrodes/fROIs from significantly impacting the results.
130"
BANDED RIDGE REGRESSION,0.13313313313313313,"We refer to R2
oos as R2 throughout the rest of the paper for brevity, and use the notation R2
M to refer
131"
BANDED RIDGE REGRESSION,0.13413413413413414,"to the performance of features M.
132"
SELECTION OF BEST LAYER,0.13513513513513514,"2.6
Selection of best layer
133"
SELECTION OF BEST LAYER,0.13613613613613615,"We evaluate the R2 for each LLM layer, and select the layer that performs best across vox-
134"
SELECTION OF BEST LAYER,0.13713713713713713,"els/electrodes/fROIs. Due to the stochastic nature of untrained LLMs, we selected the best layer for
135"
SELECTION OF BEST LAYER,0.13813813813813813,"10 random seeds and computed the average R2 across seeds. When reporting the best layer, we refer
136"
SELECTION OF BEST LAYER,0.13913913913913914,"to layer 0 as the input static layer, and layer 1 as the first intermediate layer.
137"
SELECTION OF BEST LAYER,0.14014014014014015,"2.7
Train, validation, and test folds:
138"
SELECTION OF BEST LAYER,0.14114114114114115,"For each dataset, we construct contiguous train-test splits by ensuring neural data from the same
139"
SELECTION OF BEST LAYER,0.14214214214214213,"passage/sentence/story is not included in both train and test data. Due to low sample sizes, we
140"
SELECTION OF BEST LAYER,0.14314314314314314,"employed a nested cross-validation procedure for each dataset (A.3). When computing R2 across
141"
SELECTION OF BEST LAYER,0.14414414414414414,"inner or outer folds, we pooled predictions across folds and computed a single R2 as recommended by
142"
SELECTION OF BEST LAYER,0.14514514514514515,"Hawinkel et al. [29]. The optimal parameters for banded regression were selected based on validation
143"
SELECTION OF BEST LAYER,0.14614614614614616,"data.
144"
SELECTION OF BEST LAYER,0.14714714714714713,"We created shuffled train-test splits, as done in [2], of the same size as the contiguous train-test splits.
145"
SELECTION OF BEST LAYER,0.14814814814814814,"Unless explicitly noted, all results are performed using contiguous train-test splits.
146"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.14914914914914915,"2.8
Correcting for decreases in test-set performance due to addition of feature spaces
147"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15015015015015015,"It is possible for a ""full"" encoding model to perform worse than a ""sub-model"" (which consists
148"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15115115115115116,"of only a subset of the predictors) because we are evaluating performance on a held-out test set
149"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15215215215215216,"[22]. To address this problem, in some analyses we select the best performing sub-model for each
150"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15315315315315314,"voxel/electrode/fROI which includes a given feature of interest. For instance, to examine how much
151"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15415415415415415,"feature space C adds onto features spaces A and B, we select the best sub-model which includes C
152"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15515515515515516,"and denote it as A + B + C*. More precisely, the R2 of A + B + C* is:
153"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15615615615615616,"R2
A+B+C∗= max(R2
C, R2
A+C, R2
B+C, R2
A+B+C).
(2)"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15715715715715717,"2.9
Orthogonal Auto-correlated Sequences Model (OASM)
154"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15815815815815815,"To model temporal auto-correlation in neural activity, we construct a feature matrix for each dataset
155"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.15915915915915915,"by (i) forming an n-dimensional identity matrix, where n is the total number of time points in the
156"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.16016016016016016,"dataset (per voxel / electrode / TR), and (ii) applying a Gaussian filter within ""chunks"" along the
157"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.16116116116116116,"diagonal that correspond to temporally contiguous time points (i.e., within each passage in Pereira,
158"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.16216216216216217,"each sentence in Fedorenko, and each story in Blank). This generates an auto-correlated sequence for
159"
CORRECTING FOR DECREASES IN TEST-SET PERFORMANCE DUE TO ADDITION OF FEATURE SPACES,0.16316316316316315,"each passage/sentence/story that is orthogonal to that of each other passage/sentence/story (A.7).
160"
PEREIRA DATASET,0.16416416416416416,"3
Pereira dataset
161"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.16516516516516516,"3.1
Shuffled train-test splits are severely affected by temporal auto-correlation
162"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.16616616616616617,"Prior LLM encoding studies using this dataset [24, 2, 10, 30, 11] used shuffled train-test splits. Here,
163"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.16716716716716717,"we demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs.
164"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.16816816816816818,"First, we replicated the pattern of neural predictivity across GPT2-XL’s layers reported in [2] and [24]
165"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.16916916916916916,"when using shuffled splits. Using this procedure, early and late layers perform best and intermediate
166"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17017017017017017,"layers perform worst. Strikingly, when using the alternative approach of contiguous train-test splits,
167"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17117117117117117,"the opposite pattern is observed: intermediate layers perform best. Across layers, neural predictivity
168"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17217217217217218,"using the shuffled method is highly anti-correlated with neural predictivity using the contiguous
169"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17317317317317318,"method (r = −.929 in EXP1, r = −.764 in EXP2) (Fig. 1a).
170"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17417417417417416,"Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits
171"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17517517517517517,"could be accounted for by OASM, a model which only represents within passage auto-correlation
172"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17617617617617617,"and between passage orthogonality. OASM out-performed GPT2-XL on both EXP1 and EXP2
173"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17717717717717718,"(Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve
174"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1781781781781782,"absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption
175"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.17917917917917917,"of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a
176"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.18018018018018017,"model’s brain-likeness, .
177 a
b *"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.18118118118118118,"Figure 1: Comparing different approaches for creating train-test splits in the Pereira dataset. Within
each panel, EXP1 results are on the left and EXP2 results are on the right (same formatting in Figure
2,3) (a) R2 values across layers for GPT2-XL on shuffled train-test splits (gray) and contiguous
(unshuffled) splits (blue). (b) Each dot shows the mean R2 value across voxels within a participant,
with bars indicating mean R2 across participants."
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.18218218218218218,"Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small
178"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1831831831831832,"relative to what OASM explains alone. To calculate this, we combine OASM with GPT2-XL and
179"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1841841841841842,"observe how much neural variance they explain together. To prevent OASM from ever weakening
180"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.18518518518518517,"the reported performance of GPT2-XL for any voxel, we correct the R2 value for each voxel with
181"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.18618618618618618,"the OASM+GPT2-XL model to be at least as high as with GPT2-XL alone (denoted OASM+GPT2-
182"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1871871871871872,"XL*) (2.8). Even with these corrections, we find that R2
OASM+GP T 2−XL* was 13.6% higher than
183"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1881881881881882,"R2
OASM in EXP1, and 31.5% higher than R2
OASM in EXP2 (Fig. 1b) (% differences after averaging
184"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1891891891891892,"R2 across participants). To be clear, this means that any linguistically-driven neural variance that
185"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.19019019019019018,"GPT2-XL uniquely explains over OASM is far smaller (13.6% on EXP1 and 31.5% on EXP2) than
186"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.19119119119119118,"what is predicted solely by OASM, a model with no linguistic features that completely lacks the
187"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1921921921921922,"ability to generalize to fully held out passages. Thus, it appears that the largest determinant of
188"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1931931931931932,"model predictivity on this dataset when using shuffled train-test splits is whether a model contains
189"
SHUFFLED TRAIN-TEST SPLITS ARE SEVERELY AFFECTED BY TEMPORAL AUTO-CORRELATION,0.1941941941941942,"autocorrelated sequences within passages that are orthogonal between passages.
190"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.19519519519519518,"3.2
Untrained LLM neural predictivity is fully accounted for by sentence length and position
191"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.1961961961961962,"We next sought to deconstruct what explains the neural predictivity of untrained GPT2-XL (GPT2-
192"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.1971971971971972,"XLU) in the Pereira dataset. We hypothesized that R2
GP T 2−XLU could be explained by two simple
193"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.1981981981981982,"features: sentence length (SL) and sentence position within the passage (SP). Sentence length is
194"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.1991991991991992,"captured by GPT2-XLU because the GELU nonlinearity in the first layer’s MLP transforms normally
195"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2002002002002002,"distributed inputs with zero mean into outputs with a non-zero mean. This introduces a non-zero
196"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2012012012012012,"mean component to each token’s representation in the residual stream. When these representations
197"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2022022022022022,"are sum-pooled, this non-zero mean component accumulates in a way that reflects the sentence length,
198"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2032032032032032,"making the length decodable in the intermediate layers (see A.9 for a formal proof). Sentence position
199"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2042042042042042,"is encoded within GPT2-XLU due to absolute positional embeddings which, although untrained, still
200"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.20520520520520522,"result in sentences at the same position having similar representations when tokens are sum-pooled.
201"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2062062062062062,"We represent sentence position as a 4-dimensional one-hot vector, where each element corresponds
202"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2072072072072072,"to a given position within a passage, and sentence length as the number of words in a passage.
203"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2082082082082082,"To obtain representations from GPT2-XLU, we selected the best-performing layer for each of the 10
204"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2092092092092092,"untrained seeds. For EXP1 the best performing layer was layer 0 for 6 seeds, layer 1 for 3 seeds (first
205"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21021021021021022,"intermediate layer), and layer 19 for one seed. For EXP2 the best layer was layer 1 for 5 seeds, layer
206"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21121121121121122,"2 for 4 seeds, and layer 5 for 1 seed.
207"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2122122122122122,"We fit a regression using all subsets of the following feature spaces, SL, SP, GPT2-XLU, resulting in
208"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2132132132132132,"7 models. For both experiments, R2
SP +SL was descriptively higher than all other models, including
209"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21421421421421422,"the best-performing model with GPT2-XLU (SP+SL+GPT2-XLU) (Fig. 2a). Sentence position was
210"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21521521521521522,"particularly important in EXP1, and sentence length was particularly important in EXP2. This may
211"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21621621621621623,"explain why the static layer often outperformed intermediate layer representations in EXP1 despite
212"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2172172172172172,"encoding sentence length more poorly. Overall, these results suggest that, when averaging across
213"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2182182182182182,"voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding
214"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.21921921921921922,"performance over sentence length and position.
215 102 101 100 102 100 101 0.2 0 L
R 0.3 0 a b
c d"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22022022022022023,"SP+SL*
SP+SL+GPT2-XLU*"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22122122122122123,"SP+SL*
SP+SL+GPT2-XLU*"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2222222222222222,"Figure 2: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a) Brain
score (R2) for different combinations of features. Each dot represents R2 values averaged across
voxels in a single participant, with bars showing mean across participants. (b) 2D histogram of
R2 values for the best model without GPT2-XLU (SP+SL), and the best model with GPT2-XLU
(GPT2-XLU+SP+SL). The dotted lines show y = x, y = 0, and x = 0. Values below y = 0 or
left of x = 0 were clipped when averaging, but are shown here to visualize the full distribution. (c)
Same as (a), but after voxel-wise correction; lines connect data-points from the same participant. (d)
Glass brain plots showing R2 values of SP+SL (left) and GPT2-XLU+SP+SL (right) after voxel-wise
correction. Conventions are the same as Figure 1."
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22322322322322322,"Although GPT2-XLU did not enhance encoding performance when averaging across voxels, there
216"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22422422422422422,"may be a subset of voxels where GPT2-XLU does explain significant additional neural vari-
217"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22522522522522523,"ance. To examine this possibility, we plotted a 2D histogram of voxel-wise R2
SP +SL values vs.
218"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22622622622622623,"R2
SP +SL+GP T 2−XLU values in the language network (Fig. 2b). Values were clustered around the
219"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22722722722722724,"identity line, and there was no cluster of voxels where R2
SP +SL+GP T 2−XLU appeared significantly
220"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22822822822822822,"higher. Next, for each voxel, we performed a one-sided paired t-test between the squared error
221"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.22922922922922923,"values obtained over sentences (EXP1: N = 384 , EXP2: N = 243) between SP+SL+GPT-XLU
222"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23023023023023023,"and SP+SL. Across all functional networks, only 1.26% (EXP1) and 1.42% (EXP2) of voxels were
223"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23123123123123124,"significantly (α = 0.05) better explained by the GPT2-XLU model before false discovery rate
224"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23223223223223224,"(FDR) correction; these numbers dropped to 0.001% (EXP1) and 0.078% (EXP2) after performing
225"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23323323323323322,"FDR correction within each participant and network [31]. None of the significant voxels after FDR
226"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23423423423423423,"correction were inside the language network. Taken together, these results suggest GPT2-XLU does
227"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23523523523523523,"not enhance neural prediction performance over sentence length and position even at the voxel level.
228"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23623623623623624,"To control for voxels where the neural encoding performance of GPT2-XLU is weakened by the
229"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23723723723723725,"addition of SP+SL, we compared SP+SL* and SP+SL+GPT2-XLU*. When averaging across voxels,
230"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23823823823823823,"R2
SP +SL* still exceeded R2
GP T 2−XLU+SP +SL* (Fig. 2c). Furthermore, the values for R2
SP +SL*
231"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.23923923923923923,"and R2
GP T 2−XLU+SP +SL* across brain areas were highly similar in both experiments (Fig. 2d).
232"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24024024024024024,"Only 1.00% (EXP1) and 1.18% (EXP2) of voxels were significantly better explained by the addition
233"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24124124124124124,"of GPT2-XLU before FDR correction; 0% (EXP1) and 0.05% (EXP2) of voxels were better explained
234"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24224224224224225,"Table 1: Mean R2 values (across participants) for each model. For models composed of multiple
features, the best sub-model is used which includes the last feature."
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24324324324324326,"Features
EXP1
EXP2
GPT2-XL
0.032
0.036
SP+SL
0.013
0.031
SP+SL+WORD
0.024
0.039
SP+SL+WORD+SENSE
0.026
0.040
SP+SL+WORD+SENSE+SYNT
0.027
0.043
SP+SL+WORD+SENSE+SYNT+GPT2-XL
0.032
0.045"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24424424424424424,"after FDR correction (once again, no significant voxels were inside the language network ). Thus, our
235"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24524524524524524,"results hold even when controlling for decreases in performance due to the addition of feature spaces.
236"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24624624624624625,"3.3
Sentence length, sentence position, and static word embeddings account for the majority
237"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24724724724724725,"of trained LLM encoding performance
238"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24824824824824826,"We next turned to explaining the neural predictivity of the trained GPT2-XL. In addition to sentence
239"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.24924924924924924,"position and sentence length, we added static word embeddings (WORD). Together, these features
240"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2502502502502503,"defined a baseline model which does not account for any form of linguistic processing of words
241"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25125125125125125,"in context. We next included three more complex features which involved contextual processing.
242"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25225225225225223,"First, we added sense-specific word embeddings from RoBERTa-Large using the LMMS package
243"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25325325325325326,"[32]. Sense embeddings contain distinct representations for different senses of the same word (e.g.,
244"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25425425425425424,"mouse: computer device, and mouse: rodent). LMMS generates sense embeddings by averaging over
245"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2552552552552553,"contextual embeddings corresponding to the same sense of a word (see A.10 for further details).
246"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25625625625625625,"Whereas sense embeddings help disambiguate many content words, they do not disambiguate
247"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25725725725725723,"pronouns, i.e., do not encode the entities that they refer to. Therefore, our sense embeddings were
248"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25825825825825827,"generated for a version of the Pereira text where pronouns were dereferenced (i.e., replaced by
249"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.25925925925925924,"the words that they referred to). To maintain consistency with these sense embeddings, our static
250"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2602602602602603,"word embeddings were created (1) by taking a frequency-weighted average of sense embeddings
251"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26126126126126126,"for the same word, where frequency values were obtained from WordNet [33]; and (2) based on the
252"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26226226226226224,"dereferenced Pereira texts. Importantly, this means the impact of pronoun dereferencing and word
253"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26326326326326327,"and sense embeddings are not decoupled in this study. Finally, we created an abstract representation
254"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26426426426426425,"of the syntax of each sentence (SYNT), using an approach highly similar to that of Caucheteux
255"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2652652652652653,"et al. [34]: we collected sentences that are syntactically equivalent but semantically dissimilar to the
256"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26626626626626626,"original sentence, and averaged their representations from the best layer of GPT2-XL (A.11). We
257"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2672672672672673,"selected the best layer based on averaged R2 across language voxels on test data (EXP1: layer 21,
258"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2682682682682683,"EXP2: layer 16).
259"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.26926926926926925,"We fit a regression to the fMRI data using all subsets of the feature spaces SL+SP, WORD, SENSE,
260"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2702702702702703,"SYNT, GPT2-XL, resulting in 64 models. In this list, features are ranked from least to most complex.
261"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.27127127127127126,"For each feature, we took the model that exhibited the best performance in the language network
262"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2722722722722723,"which included that feature but did not include features more complex than it. For instance, values
263"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2732732732732733,"reported for R2
SL+SP +W ORD+SENSE were taken from the best model which included SENSE,
264"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.27427427427427425,"excluding models which included SYNT and GPT2-XL. By doing so, we were able to examine
265"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2752752752752753,"the impact of adding more complex features in explaining R2GP T 2−XL while still accounting for
266"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.27627627627627627,"decreases in test performance due to adding redundant features. We note that since this procedure is
267"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2772772772772773,"not performed at the voxel-level, we do not add a * to the R2 notation.
268"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2782782782782783,"Table 1 displays the performance of each model, including GPT2-XL on its own (Fig. 2a, 2b). The
269"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.27927927927927926,"baseline SP+SL+WORD model, which does not account for any form of contextual processing,
270"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2802802802802803,"performs 75% as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2. When adding
271"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.28128128128128127,"contextual features, namely SENSE and SYNT, our model performs 84.4% as well as GPT2-XL and
272"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2822822822822823,"the full model in EXP1, and better than GPT2-XL and 95.5% as well as the full model in EXP2,
273"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2832832832832833,"indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond
274"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.28428428428428426,"simple features in this dataset.
275"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2852852852852853,"Similar to previous sections, we perform voxel-wise correction by selecting the best sub-model with
276"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2862862862862863,"GPT2-XL and the best sub-model without GPT2-XL for each voxel. We focus only on sentence
277 102 101 100 102 101 100 0"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2872872872872873,"0.4
0.35 0 L
R *
* *
* a b
c d"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2882882882882883,"Figure 3: For all panels, EXP1 results are on the left and EXP2 results are on the right. (a) For each
model, we display the sub-model which includes the added feature. Dots represent participants and
bars are mean across participants. Grey dashed line is the performance of GPT2-XL alone. (b) 2d
histogram comparing full model and full model with GPT2-XL. (c) Same as (a) but after voxel-wise
correction for SP+SL+WORD and SP+SL+WORD+GPT2-XL. (d) Glass brain plots showing R2
values of SP+SL+WORD (left) and SP+SL+WORD+GPT2-XLU (right) after voxel-wise correction."
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.28928928928928926,"position, sentence length, and static word embeddings because sense and syntax had modest con-
278"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2902902902902903,"tributions beyond these features. R2
SP +SL+W ORD* was 0.028 in EXP1 and 0.048 in EXP2, and
279"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2912912912912913,"R2
SP +SL+W ORD+GP T 2−XL* was 0.036 in EXP1 and 0.056 in EXP2 (mean across participants)
280"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2922922922922923,"(Fig. 3c). This indicates that even after controlling for a reduction in GPT2-XL performance from
281"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.2932932932932933,"the addition of simple features, GPT2-XL only explains an additional 28.57% (EXP1) and 16.7%
282"
UNTRAINED LLM NEURAL PREDICTIVITY IS FULLY ACCOUNTED FOR BY SENTENCE LENGTH AND POSITION,0.29429429429429427,"(EXP2) neural variance over a model composed of features that are all non-contextual.
283"
FEDORENKO DATASET,0.2952952952952953,"4
Fedorenko dataset
284"
FEDORENKO DATASET,0.2962962962962963,"4.1
Shuffled train-test splits also impact ECoG datasets, but less than with fMRI
285"
FEDORENKO DATASET,0.2972972972972973,"We first evaluated the impact of shuffled train-test splits on the Fedorenko dataset. Unlike in Pereira,
286"
FEDORENKO DATASET,0.2982982982982983,"the across-layer performance is well correlated between shuffled and contiguous splits (r = 0.622)
287"
FEDORENKO DATASET,0.2992992992992993,"(Fig. 4a). The OASM model performs 93.1% as well as GPT2-XL when averaging R2 values across
288"
FEDORENKO DATASET,0.3003003003003003,"participants (Fig. 4b). R2
OASM+GP T 2−XL* was 45.3% better than OASM, meaning that the unique
289"
FEDORENKO DATASET,0.3013013013013013,"contribution of GPT2-XL is less than half the total contribution of a simple, auto-correlated model.
290"
FEDORENKO DATASET,0.3023023023023023,"Therefore, shuffled train-test splits also impact results on Fedorenko, albeit less than Pereira. This
291"
FEDORENKO DATASET,0.3033033033033033,"may be due to lower autocorrelation of ECoG compared to fMRI. We use contiguous splits for the
292"
FEDORENKO DATASET,0.30430430430430433,"remainder of the Fedorenko analyses.
293"
FEDORENKO DATASET,0.3053053053053053,"4.2
Word position explains all of untrained, and most of trained, GPT2-XL brain score
294"
FEDORENKO DATASET,0.3063063063063063,"As noted in [35], there was a strong positional signal in the ECoG dataset during comprehension of
295"
FEDORENKO DATASET,0.3073073073073073,"sentences that is likely related to the construction of sentence meaning. We therefore hypothesized
296"
FEDORENKO DATASET,0.3083083083083083,"a
b
c
d *
*
*"
FEDORENKO DATASET,0.30930930930930933,"Figure 4: (a) Across-layer R2, averaged across electrodes in the Fedorenko dataset, for GPT2-XL
with and without shuffled splits. (b) Each dot is a participant, lines connect data-points from the same
participant. Bars display mean across participants. (c) and (d) Same guidelines as (b)."
FEDORENKO DATASET,0.3103103103103103,"that a feature space that accounted for word position (WP) would do well relative to untrained and
297"
FEDORENKO DATASET,0.3113113113113113,"trained GPT2-XL. We generated a simple feature space that encodes word position, such that words
298"
FEDORENKO DATASET,0.3123123123123123,"in nearby positions were given similar representations (A.12). When performing a one-sided paired
299"
FEDORENKO DATASET,0.3133133133133133,"t-test between the squared error predictions of WP+GPT2-XLU* and WP, three electrodes were
300"
FEDORENKO DATASET,0.31431431431431434,"significantly better explained by the addition of GPT2-XLU before FDR correction, and none were
301"
FEDORENKO DATASET,0.3153153153153153,"better explained after FDR correction within each participant. Moreover, WP performs 86.7% as well
302"
FEDORENKO DATASET,0.3163163163163163,"as GPT2-XL, and 82.1% as well as WP+GPT2-XL*. Our results therefore suggest that the mapping
303"
FEDORENKO DATASET,0.3173173173173173,"between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals.
304 305"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3183183183183183,"5
Blank dataset is predicted at near chance levels
306"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.31931931931931934,"Lastly, we address the Blank dataset. We find that OASM achieves an R2 that is 103.6 times
307"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3203203203203203,"larger than that of GPT2-XL when using shuffled splits A.13, demonstrating that such splits are
308"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3213213213213213,"massively contaminated by temporal autocorrelation. We next turn to using contiguous splits, and test
309"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.32232232232232233,"whether GPT2-XL performs better than an intercept only model by applying a one-sided paired t-test
310"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3233233233233233,"between the squared error values obtained from GPT2-XL and the intercept only model (N = 1317
311"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.32432432432432434,"TRs). GPT2-XL predicts 1 fROI significantly better than an intercept only model, and 0 fROIs are
312"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3253253253253253,"significantly better after FDR correction. Our results therefore suggest that GPT2-XL performs at
313"
BLANK DATASET IS PREDICTED AT NEAR CHANCE LEVELS,0.3263263263263263,"near chance levels on the version of the Blank dataset used by [2, 10, 11].
314"
LIMITATIONS AND CONCLUSIONS,0.32732732732732733,"6
Limitations and Conclusions
315"
LIMITATIONS AND CONCLUSIONS,0.3283283283283283,"Our study has three main limitations. First, our method of examining how much neural variance
316"
LIMITATIONS AND CONCLUSIONS,0.32932932932932935,"an LLM predicts over simple features scales poorly when the number of features is large. Second,
317"
LIMITATIONS AND CONCLUSIONS,0.3303303303303303,"although we attempted to correct for cases where adding features decreases test set performance and
318"
LIMITATIONS AND CONCLUSIONS,0.33133133133133136,"employed banded regression, fitting regressions with large feature spaces on noisy neural data with
319"
LIMITATIONS AND CONCLUSIONS,0.33233233233233234,"low sample sizes can lead to poor estimations of the neural variance explained. Finally, we did not
320"
LIMITATIONS AND CONCLUSIONS,0.3333333333333333,"analyze datasets with large amounts of neural data per participant, for instance [36], in which the gap
321"
LIMITATIONS AND CONCLUSIONS,0.33433433433433435,"between the neural predictivity of simple and complex features might be much larger.
322"
LIMITATIONS AND CONCLUSIONS,0.3353353353353353,"In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal
323"
LIMITATIONS AND CONCLUSIONS,0.33633633633633636,"autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and
324"
LIMITATIONS AND CONCLUSIONS,0.33733733733733734,"trained GPT2-XL brain score is largely explained by non-contextual features. We find that the
325"
LIMITATIONS AND CONCLUSIONS,0.3383383383383383,"majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and
326"
LIMITATIONS AND CONCLUSIONS,0.33933933933933935,"on the Blank dataset GPT2-XL predicts neural activity at near chance levels. These results suggest
327"
LIMITATIONS AND CONCLUSIONS,0.34034034034034033,"that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally,
328"
LIMITATIONS AND CONCLUSIONS,0.34134134134134136,"analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding
329"
LIMITATIONS AND CONCLUSIONS,0.34234234234234234,"performance, and an evaluation against simple and theoretically uninteresting features. Only after
330"
LIMITATIONS AND CONCLUSIONS,0.3433433433433433,"such deconstruction can we be somewhat confident that the neural predictivity of LLMs reflects core
331"
LIMITATIONS AND CONCLUSIONS,0.34434434434434436,"aspects of human linguistic processing.
332"
REFERENCES,0.34534534534534533,"References
333"
REFERENCES,0.34634634634634637,"[1] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and
334"
REFERENCES,0.34734734734734735,"Evelina Fedorenko. Dissociating language and thought in large language models. Trends Cogn.
335"
REFERENCES,0.3483483483483483,"Sci., March 2024.
336"
REFERENCES,0.34934934934934936,"[2] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy
337"
REFERENCES,0.35035035035035034,"Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:
338"
REFERENCES,0.35135135135135137,"Integrative modeling converges on predictive processing. Proc. Natl. Acad. Sci. U. S. A., 118
339"
REFERENCES,0.35235235235235235,"(45), November 2021.
340"
REFERENCES,0.3533533533533533,"[3] Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining computational controls with
341"
REFERENCES,0.35435435435435436,"natural text reveals aspects of meaning composition. Nat Comput Sci, 2(11):745–757, November
342"
REFERENCES,0.35535535535535534,"2022.
343"
REFERENCES,0.3563563563563564,"[4] Charlotte Caucheteux and Jean-Rémi King. Brains and algorithms partially converge in natural
344"
REFERENCES,0.35735735735735735,"language processing. Commun Biol, 5(1):134, February 2022.
345"
REFERENCES,0.35835835835835833,"[5] Shailee Jain and Alexander Huth. Incorporating context into language encoding models for
346"
REFERENCES,0.35935935935935936,"fMRI. Adv. Neural Inf. Process. Syst., 31, 2018.
347"
REFERENCES,0.36036036036036034,"[6] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in
348"
REFERENCES,0.3613613613613614,"machines) with natural language-processing (in the brain). Adv. Neural Inf. Process. Syst., pages
349"
REFERENCES,0.36236236236236236,"14928–14938, May 2019.
350"
REFERENCES,0.3633633633633634,"[7] Alec Radford, Jeff Wu, R Child, D Luan, Dario Amodei, and I Sutskever. Language models are
351"
REFERENCES,0.36436436436436437,"unsupervised multitask learners. 2019.
352"
REFERENCES,0.36536536536536535,"[8] Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel J Gershman, Nancy
353"
REFERENCES,0.3663663663663664,"Kanwisher, Matthew Botvinick, and Evelina Fedorenko. Toward a universal decoder of linguistic
354"
REFERENCES,0.36736736736736736,"meaning from brain activation. Nat. Commun., 9(1):963, March 2018.
355"
REFERENCES,0.3683683683683684,"[9] Alexandre Pasquiou, Yair Lakretz, John Hale, Bertrand Thirion, and Christophe Pallier. Neural
356"
REFERENCES,0.36936936936936937,"language models are not born equal to fit brain data, but training helps. July 2022.
357"
REFERENCES,0.37037037037037035,"[10] Eghbal A Hosseini, Martin Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and
358"
REFERENCES,0.3713713713713714,"Evelina Fedorenko. Artificial neural network language models predict human brain responses
359"
REFERENCES,0.37237237237237236,"to language even after a developmentally realistic amount of training. Neurobiol Lang (Camb),
360"
REFERENCES,0.3733733733733734,"5(1):43–63, April 2024.
361"
REFERENCES,0.3743743743743744,"[11] Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.
362"
REFERENCES,0.37537537537537535,"Instruction-tuned LLMs with world knowledge are more aligned to the human brain, 2024.
363"
REFERENCES,0.3763763763763764,"URL https://openreview.net/forum?id=DZ6B5u4vfe.
364"
REFERENCES,0.37737737737737737,"[12] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Rémi King. Evidence of a predictive
365"
REFERENCES,0.3783783783783784,"coding hierarchy in the human brain listening to speech. Nat Hum Behav, 7(3):430–441, March
366"
REFERENCES,0.3793793793793794,"2023.
367"
REFERENCES,0.38038038038038036,"[13] Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush,
368"
REFERENCES,0.3813813813813814,"Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia
369"
REFERENCES,0.38238238238238237,"Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky,
370"
REFERENCES,0.3833833833833834,"Adeen Flinker, Omer Levy, and Uri Hasson. The temporal structure of language processing in
371"
REFERENCES,0.3843843843843844,"the human brain corresponds to the layered hierarchy of deep language models, 2024. URL
372"
REFERENCES,0.38538538538538536,"https://openreview.net/forum?id=95ObXevgHx.
373"
REFERENCES,0.3863863863863864,"[14] Refael Tikochinski, Ariel Goldstein, Yoav Meiri, Uri Hasson, and Roi Reichart. Incremental ac-
374"
REFERENCES,0.38738738738738737,"cumulation of linguistic context in artificial and biological neural networks. bioRxiv, 2024. doi:
375"
REFERENCES,0.3883883883883884,"10.1101/2024.01.15.575798.
URL https://www.biorxiv.org/content/early/2024/
376"
REFERENCES,0.3893893893893894,"01/17/2024.01.15.575798.
377"
REFERENCES,0.39039039039039036,"[15] Jeffrey S Bowers, Gaurav Malhotra, Federico Adolfi, Marin Dujmovi´c, Milton L Montero,
378"
REFERENCES,0.3913913913913914,"Valerio Biscione, Guillermo Puebla, John H Hummel, and Rachel F Heaton. On the importance
379"
REFERENCES,0.3923923923923924,"of severely testing deep learning models of cognition. Cogn. Syst. Res., 82:101158, December
380"
REFERENCES,0.3933933933933934,"2023.
381"
REFERENCES,0.3943943943943944,"[16] Richard Antonello, Aditya R Vaidya, and Alexander G Huth. Scaling laws for language
382"
REFERENCES,0.3953953953953954,"encoding models in fMRI. Adv. Neural Inf. Process. Syst., abs/2305.11863, May 2023.
383"
REFERENCES,0.3963963963963964,"[17] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for
384"
REFERENCES,0.3973973973973974,"word representation. In Proceedings of the 2014 conference on empirical methods in natural
385"
REFERENCES,0.3983983983983984,"language processing (EMNLP), pages 1532–1543, 2014.
386"
REFERENCES,0.3993993993993994,"[18] Kilian Weinberger.
On the importance of deconstruction in machine learning research.
387"
REFERENCES,0.4004004004004004,"ML-Retrospectives @ NeurIPS 2020, 2020. URL https://slideslive.com/38938218/
388"
REFERENCES,0.4014014014014014,"the-importance-of-deconstruction.
389"
REFERENCES,0.4024024024024024,"[19] Evelina Fedorenko, Michael K Behr, and Nancy Kanwisher. Functional specificity for high-level
390"
REFERENCES,0.4034034034034034,"linguistic processing in the human brain. Proc. Natl. Acad. Sci. U. S. A., 108(39):16428–16433,
391"
REFERENCES,0.4044044044044044,"September 2011.
392"
REFERENCES,0.40540540540540543,"[20] Idan Blank, Nancy Kanwisher, and Evelina Fedorenko. A functional dissociation between
393"
REFERENCES,0.4064064064064064,"language and multiple-demand systems revealed in patterns of BOLD signal fluctuations. J.
394"
REFERENCES,0.4074074074074074,"Neurophysiol., 112(5):1105–1118, September 2014.
395"
REFERENCES,0.4084084084084084,"[21] Aniketh Janardhan Reddy and Leila Wehbe. Can fMRI reveal the representation of syntactic
396"
REFERENCES,0.4094094094094094,"structure in the brain? Adv. Neural Inf. Process. Syst., 34:9843–9856, December 2021.
397"
REFERENCES,0.41041041041041043,"[22] Wendy A de Heer, Alexander G Huth, Thomas L Griffiths, Jack L Gallant, and Frédéric E
398"
REFERENCES,0.4114114114114114,"Theunissen. The hierarchical cortical organization of human speech processing. J. Neurosci.,
399"
REFERENCES,0.4124124124124124,"37(27):6539–6557, July 2017.
400"
REFERENCES,0.4134134134134134,"[23] Richard Futrell, Edward Gibson, Harry J Tily, Idan Blank, Anastasia Vishnevetsky, Steven
401"
REFERENCES,0.4144144144144144,"Piantadosi, and Evelina Fedorenko. The natural stories corpus. In Nicoletta Calzolari, Khalid
402"
REFERENCES,0.41541541541541543,"Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Koiti Hasida, Hitoshi Isahara, Bente
403"
REFERENCES,0.4164164164164164,"Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis,
404"
REFERENCES,0.4174174174174174,"and Takenobu Tokunaga, editors, Proceedings of the Eleventh International Conference on
405"
REFERENCES,0.4184184184184184,"Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European
406"
REFERENCES,0.4194194194194194,"Language Resources Association (ELRA).
407"
REFERENCES,0.42042042042042044,"[24] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and Evelina Fedorenko. Lexical-
408"
REFERENCES,0.4214214214214214,"Semantic content, not syntactic structure, is the main contributor to ANN-Brain similarity of
409"
REFERENCES,0.42242242242242245,"fMRI responses in the language network. Neurobiol Lang (Camb), 5(1):7–42, April 2024.
410"
REFERENCES,0.42342342342342343,"[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
411"
REFERENCES,0.4244244244244244,"Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT
412"
REFERENCES,0.42542542542542544,"pretraining approach. July 2019.
413"
REFERENCES,0.4264264264264264,"[26] Alexander G Huth, Wendy A de Heer, Thomas L Griffiths, Frédéric E Theunissen, and Jack L
414"
REFERENCES,0.42742742742742745,"Gallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532
415"
REFERENCES,0.42842842842842843,"(7600):453–458, April 2016.
416"
REFERENCES,0.4294294294294294,"[27] Shailee Jain, Vy A Vo, Shivangi Mahto, Amanda LeBel, Javier Turek, and Alexander G Huth.
417"
REFERENCES,0.43043043043043044,"Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech.
418"
REFERENCES,0.4314314314314314,"Adv. Neural Inf. Process. Syst., 33, October 2020.
419"
REFERENCES,0.43243243243243246,"[28] Tom Dupré la Tour, Michael Eickenberg, Anwar O Nunez-Elizalde, and Jack L Gallant. Feature-
420"
REFERENCES,0.43343343343343343,"space selection with banded ridge regression. Neuroimage, 264:119728, December 2022.
421"
REFERENCES,0.4344344344344344,"[29] Stijn Hawinkel, Willem Waegeman, and Steven Maere. Out-of-Sample r2: Estimation and
422"
REFERENCES,0.43543543543543545,"inference. Am. Stat., pages 1–11.
423"
REFERENCES,0.4364364364364364,"[30] Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, and Bapi
424"
REFERENCES,0.43743743743743746,"Surampudi. Neural language taskonomy: Which NLP tasks are the most predictive of fMRI
425"
REFERENCES,0.43843843843843844,"brain activity? In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz,
426"
REFERENCES,0.4394394394394394,"editors, Proceedings of the 2022 Conference of the North American Chapter of the Association
427"
REFERENCES,0.44044044044044045,"for Computational Linguistics: Human Language Technologies, pages 3220–3237, Seattle,
428"
REFERENCES,0.44144144144144143,"United States, July 2022. Association for Computational Linguistics.
429"
REFERENCES,0.44244244244244246,"[31] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical and
430"
REFERENCES,0.44344344344344344,"powerful approach to multiple testing. J. R. Stat. Soc. Series B Stat. Methodol., 57(1):289–300,
431"
REFERENCES,0.4444444444444444,"1995.
432"
REFERENCES,0.44544544544544545,"[32] Daniel Loureiro, Alípio Mário Jorge, and Jose Camacho-Collados.
LMMS reloaded:
433"
REFERENCES,0.44644644644644643,"Transformer-based sense embeddings for disambiguation and beyond. Artif. Intell., 305:103661,
434"
REFERENCES,0.44744744744744747,"April 2022.
435"
REFERENCES,0.44844844844844844,"[33] George A Miller. WordNet: a lexical database for english. Commun. ACM, 38(11):39–41,
436"
REFERENCES,0.4494494494494494,"November 1995.
437"
REFERENCES,0.45045045045045046,"[34] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi King. Disentangling syntax and
438"
REFERENCES,0.45145145145145144,"semantics in the brain with deep networks. March 2021.
439"
REFERENCES,0.45245245245245247,"[35] Evelina Fedorenko, Terri L Scott, Peter Brunner, William G Coon, Brianna Pritchett, Gerwin
440"
REFERENCES,0.45345345345345345,"Schalk, and Nancy Kanwisher. Neural correlate of the construction of sentence meaning. Proc.
441"
REFERENCES,0.4544544544544545,"Natl. Acad. Sci. U. S. A., 113(41):E6256–E6262, October 2016.
442"
REFERENCES,0.45545545545545546,"[36] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson
443"
REFERENCES,0.45645645645645644,"Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural language fMRI dataset
444"
REFERENCES,0.4574574574574575,"for voxelwise encoding models. Sci Data, 10(1):555, August 2023.
445"
REFERENCES,0.45845845845845845,"[37] Zachary Mineroff, Idan Asher Blank, Kyle Mahowald, and Evelina Fedorenko. A robust
446"
REFERENCES,0.4594594594594595,"dissociation among the language, multiple demand, and default mode networks: Evidence from
447"
REFERENCES,0.46046046046046046,"inter-region correlations in effect size. Neuropsychologia, 119:501–511, October 2018.
448"
REFERENCES,0.46146146146146144,"[38] Jonathan D Power, Alexander L Cohen, Steven M Nelson, Gagan S Wig, Kelly Anne Barnes,
449"
REFERENCES,0.4624624624624625,"Jessica A Church, Alecia C Vogel, Timothy O Laumann, Fran M Miezin, Bradley L Schlaggar,
450"
REFERENCES,0.46346346346346345,"and Steven E Petersen. Functional network organization of the human brain. Neuron, 72(4):
451"
REFERENCES,0.4644644644644645,"665–678, November 2011.
452"
REFERENCES,0.46546546546546547,"[39] Thomas Lumley, Paula Diehr, Scott Emerson, and Lu Chen. The importance of the normality
453"
REFERENCES,0.46646646646646645,"assumption in large public health data sets. Annu. Rev. Public Health, 23:151–169, 2002.
454"
REFERENCES,0.4674674674674675,"[40] Simon Musall, Matthew T Kaufman, Ashley L Juavinett, Steven Gluf, and Anne K Churchland.
455"
REFERENCES,0.46846846846846846,"Single-trial neural dynamics are dominated by richly varied movements. Nat. Neurosci., 22(10):
456"
REFERENCES,0.4694694694694695,"1677–1686, October 2019.
457"
REFERENCES,0.47047047047047047,"[41] Matthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom
458"
REFERENCES,0.47147147147147145,"embeddings, convolutional neural networks and incremental parsing. To appear, 2017.
459"
REFERENCES,0.4724724724724725,"A
Appendix
460"
REFERENCES,0.47347347347347346,"A.1
Experimental data
461"
REFERENCES,0.4744744744744745,"Pereira: For both experiments, each sentence was visually presented for 4 s with 4 s between
462"
REFERENCES,0.4754754754754755,"sentences and an additional 4 s between passages. A single fMRI scan was taken in the interval
463"
REFERENCES,0.47647647647647645,"between each sentence. Because fMRI data is noisy, each experiment was repeated three times and
464"
REFERENCES,0.4774774774774775,"fMRI data was averaged across the repetitions. A single fMRI scanning session consisted of 8 runs,
465"
REFERENCES,0.47847847847847846,"where each run contained 12 passages in EXP1 and 9 passages in EXP2. Participants performed
466"
REFERENCES,0.4794794794794795,"a total of 3 scanning sessions. The division of passages into runs and the order of the runs was
467"
REFERENCES,0.4804804804804805,"randomized for each participant and scanning session.
468"
REFERENCES,0.48148148148148145,"Fedorenko: Participants read sentence on word at a time, and each word was visually displayed for
469"
REFERENCES,0.4824824824824825,"450 or 700 ms. For each electrode, high gamma signal was extracted using gaussian filter banks at
470"
REFERENCES,0.48348348348348347,"center frequencies ranging from 73 −144 Hz, the envelope of the high gamma signal was computed
471"
REFERENCES,0.4844844844844845,"through a hilbert-transform, and the envelope was z-scored within each electrode. For each participant,
472"
REFERENCES,0.4854854854854855,"language-selective electrodes were selected where the z-scored envelope of the gamma activity was
473"
REFERENCES,0.4864864864864865,"significantly higher during the sentences than a condition where participants read nonword lists.
474"
REFERENCES,0.4874874874874875,"Z-scored high gamma activity from these language-selective electrodes were used in subsuquent
475"
REFERENCES,0.48848848848848847,"analyses.
476"
REFERENCES,0.4894894894894895,"Blank: Text was split into 2 s segments corresponding to each TR, with words that were on the
477"
REFERENCES,0.4904904904904905,"boundary being assinged to the later TR. Due to the delay in the hemodynamic response function
478"
REFERENCES,0.4914914914914915,"(HRF), neural activity was predicted using stimuli from 2 TRs (4 s) previous.
479"
REFERENCES,0.4924924924924925,"Functional localization: For Pereira and Blank, the language network was defined by the following
480"
REFERENCES,0.4934934934934935,"procedure [19]. First, voxels were identified in each participant which showed stronger responses
481"
REFERENCES,0.4944944944944945,"to sentences compared to lists of non-words (sentences > non-word lists contrast). These voxels
482"
REFERENCES,0.4954954954954955,"were then constrained by data-driven language activation maps formed by applying the same contrast
483"
REFERENCES,0.4964964964964965,"to many other participants. Finally, the top 10% of the voxels were selected which showed the
484"
REFERENCES,0.4974974974974975,"greatest sentences > non-word lists difference. For Pereira, we perform some analyses using four
485"
REFERENCES,0.4984984984984985,"other networks: multiple demand (MD), default mode network (DMN), auditory, and visual network.
486"
REFERENCES,0.4994994994994995,"The multiple demand (MD) and default mode network (DMN) networks were defined using the same
487"
REFERENCES,0.5005005005005005,"procedure, except that the contrast involved a spatial working memory task, where a hard > easy
488"
REFERENCES,0.5015015015015015,"condition contrast was used for MD and a fixation > hard contrast was used for DMN [37]. Auditory
489"
REFERENCES,0.5025025025025025,"and visual networks were defined using resting state connectivity [38].
490"
REFERENCES,0.5035035035035035,"A.2
Banded ridge regression
491"
REFERENCES,0.5045045045045045,"We used a random search method to optimize the banded regression hyperparameters [28]. Banded
492"
REFERENCES,0.5055055055055055,"regression has two hyperparameters, γ, which is a vector of shape number of feature spaces that
493"
REFERENCES,0.5065065065065065,"determines how much each feature space is scaled, and α, which is the L2 penalty applied across
494"
REFERENCES,0.5075075075075075,"feature spaces. Values for γ are drawn from a Dirichlet distribution and hence sum to 1. Down-scaling
495"
REFERENCES,0.5085085085085085,"a certain feature space relative to others is functionally equivalent to assigning a separate L2 penalty
496"
REFERENCES,0.5095095095095095,"for each feature space. This is because when a feature space is down-scaled, the L2 magnitude of
497"
REFERENCES,0.5105105105105106,"the weights must increase for it to have a meaningful contribution to the predictions, which equates
498"
REFERENCES,0.5115115115115115,"to increasing the L2 penalty for that feature space. The optimal γ and α combination was found
499"
REFERENCES,0.5125125125125125,"for each voxel/electrode/fROI by performing a random search over γ values, storing the α value
500"
REFERENCES,0.5135135135135135,"that performed best for that γ on validation data, and then selecting the best performing γ and α
501"
REFERENCES,0.5145145145145145,"combination.
502"
REFERENCES,0.5155155155155156,"Before starting the random search, we tried all combinations of γ values that removed feature spaces
503"
REFERENCES,0.5165165165165165,"(i.e. down-scaled at least one feature space to 0) to ensure the regression had an opportunity to
504"
REFERENCES,0.5175175175175175,"remove features which hurt performance. In theory, this should obviate the need for the procedure
505"
REFERENCES,0.5185185185185185,"implemented in 2.8. This is because the banded regression procedure can remove feature spaces
506"
REFERENCES,0.5195195195195195,"based on validation data, meaning if a model performs worse than a sub-model the banded procedure
507"
REFERENCES,0.5205205205205206,"has the opportunity to set the γ value corresponding to the additional feature spaces to 0. However,
508"
REFERENCES,0.5215215215215215,"because neural data is noisy and there is often little data per subject, performance on validation data is
509"
REFERENCES,0.5225225225225225,"not always indicative of performance on test-data. Therefore it is possible for the banded regression
510"
REFERENCES,0.5235235235235235,"procedure to include a feature space (since it helps on validation data), and for this feature space to
511"
REFERENCES,0.5245245245245245,"ultimately hurt test set performance, necessitating the correction procedure detailed in 2.8.
512"
REFERENCES,0.5255255255255256,"We ran banded ridge regression for a maximum of 1000 random search iterations with early stopping
513"
REFERENCES,0.5265265265265265,"if the mean R2 did not improve by more than 10−4 after 50 iterations. We treated feature spaces
514"
REFERENCES,0.5275275275275275,"with many dimensions as one features because preliminary results showed this performed better.
515"
REFERENCES,0.5285285285285285,"Specifically, we always treated the following feature spaces as one feature space: static word
516"
REFERENCES,0.5295295295295295,"embeddings, sense-specific word embeddings, syntactic representations, and GPT2-XL and Roberta-
517"
REFERENCES,0.5305305305305306,"Large representations. All other features were treated as their own feature space.
518"
REFERENCES,0.5315315315315315,"We z-score all features across samples before training regressions, as is standard when using ridge
519"
REFERENCES,0.5325325325325325,"regression in neural encoding studies.
520"
REFERENCES,0.5335335335335335,"A.3
Additional details on train, validation, and test folds
521"
REFERENCES,0.5345345345345346,"Pereira: During each outer fold, a single passage from each of the 24 semantic categories from one
522"
REFERENCES,0.5355355355355356,"experiment was selected, and half of these passages were designated as the test set. This equated to 8
523"
REFERENCES,0.5365365365365365,"test folds for experiment 1 (4 passages per semantic category) and 6 test folds for experiment 2 (3
524"
REFERENCES,0.5375375375375375,"passages per semantic category). During each inner fold, we again selected one passage from each
525"
REFERENCES,0.5385385385385385,"semantic category, and half of these passages were designated as validation (leading to 7 inner folds
526"
REFERENCES,0.5395395395395396,"for experiment 1, and 5 inner folds for experiment 2).
527"
REFERENCES,0.5405405405405406,"Fedorenko: For each outer fold, we selected 4 sentences as the test fold, resulting in 13 outer folds.
528"
REFERENCES,0.5415415415415415,"For each inner fold, we once again select 4 sentences as the validation set, resulting in 12 inner folds
529"
REFERENCES,0.5425425425425425,"per outer fold.
530"
REFERENCES,0.5435435435435435,"Blank: For each outer fold, we selected a single story as the test fold, resulting in 8 outer folds. For
531"
REFERENCES,0.5445445445445446,"each inner fold, each of the remaining stories served in turn as the validation set, resulting in 7 inner
532"
REFERENCES,0.5455455455455456,"folds.
533"
REFERENCES,0.5465465465465466,"A.4
Justification of statistical tests
534"
REFERENCES,0.5475475475475475,"We performed a t-test between squared error values from two models to determine if one model
535"
REFERENCES,0.5485485485485485,"performs better than another. While squared error values are not always normally distributed, our
536"
REFERENCES,0.5495495495495496,"sample sizes were large (the minimum sample size was 243) and so we still opted to use a t-test over
537"
REFERENCES,0.5505505505505506,"a non-parametric alternative [39]. One issue with a t-test is that relies on the assumption that samples
538"
REFERENCES,0.5515515515515516,"are not correlated, which is not true for time-series data. However, we note that correlated samples
539"
REFERENCES,0.5525525525525525,"leads one to underestimate the standard error of the mean and exaggerate differences between two
540"
REFERENCES,0.5535535535535535,"models. Since we only perform one-sided t-tests to examine whether adding GPT2-XL representations
541"
REFERENCES,0.5545545545545546,"improves performance, the net impact of this on our results is to overestimate how much GPT2-XL
542"
REFERENCES,0.5555555555555556,"contributes over simple features.
543"
REFERENCES,0.5565565565565566,"A.5
Across layer R2 values in the Pereira dataset
544"
REFERENCES,0.5575575575575575,"Across layer performances in the Pereira dataset for GPT2-XLU and GPT2-XL when using the sum
545"
REFERENCES,0.5585585585585585,"pooling method (Fig. 5a,b) and the last token method (Fig. 5c,d). Performance in language network
546"
REFERENCES,0.5595595595595596,"is higher across the board than performance in DMN, MD, and visual networks. We do not show
547"
REFERENCES,0.5605605605605606,"auditory network results because participants read passages in Pereira and hence auditory brain scores
548"
REFERENCES,0.5615615615615616,"are near 0. Furthermore, performance is lower with the last token method in every case except in
549"
REFERENCES,0.5625625625625625,"EXP1 trained results where the last token method performs slightly better. a c
d b"
REFERENCES,0.5635635635635635,"Figure 5: a) Across layer performances in Pereira dataset for GPT2-XLU for each functional network
when using the sum-pooling method. EXP1 is on the left, and EXP2 is on the right. b) Same as a but
for GPT2-XL, also using the sum-pooling method. c) Same as a but when using the last token method.
Dotted grey line shows performance of best layer of GPT2-XLU in language network when sum
pooling. d) Same as b but when using the last token method. Dotted grey line shows performance of
best layer of GPT2-XL in language network when sum pooling. 550"
REFERENCES,0.5645645645645646,"A.6
RoBERTa-Large shows similar results as GPT2-XL
551"
REFERENCES,0.5655655655655656,"To examine whether our results depending on the choice of LLM, we replicated all of our Pereira
552"
REFERENCES,0.5665665665665666,"trained analyses with RoBERTa-Large (ROB). The overall trend in results was the same as
553"
REFERENCES,0.5675675675675675,"with GPT2-XL (Fig. 6). Namely, SP+SL+WORD performed 76.8% as well as the full model
554"
REFERENCES,0.5685685685685685,"(SP+SL+WORD+SENSE+SYNT+ROB) and 80.0% as well as ROB alone in EXP1, and in EXP2 it
555"
REFERENCES,0.5695695695695696,"performed 88.0% as well as the full model and better than ROB. Furthermore, SENSE and SYNT
556"
REFERENCES,0.5705705705705706,"bridge the gap to the full model by a small amount. In sum, our main conclusion that a large amount
557"
REFERENCES,0.5715715715715716,"of trained LLM brain score in the Pereira dataset is accounted for by non-contextual features also
558"
REFERENCES,0.5725725725725725,"applies to RoBERTa-Large.
559 *
* a b d 102 101 100 c 102 101 100 L
R 0.35 0.0 0.4 0"
REFERENCES,0.5735735735735735,"Figure 6: All panels are the same as Figure 3, except GPT2-XL is replaced with RoBERTa-Large
(ROB)."
REFERENCES,0.5745745745745746,"A.7
Orthogonal autocorrelated sequences model (OASM) hyperparameters
560"
REFERENCES,0.5755755755755756,"The width of the Gaussian filter used for within-block smoothing was σ = 2.2 in Pereira, σ = 1.8 in
561"
REFERENCES,0.5765765765765766,"Fedorenko, and σ = 1.5 in Blank. Gaussian widths were determined by sweeping σ across 50 evenly
562"
REFERENCES,0.5775775775775776,"spaced values between 0.1 and 5.0 and choosing the best-performing σ for each dataset.
563"
REFERENCES,0.5785785785785785,"A.8
Shuffled train test splits confound task-relevant and task-irrelevant neural activity
564"
REFERENCES,0.5795795795795796,"OASM is a model which clearly lacks any linguistic representations that would allow it generalize to
565"
REFERENCES,0.5805805805805806,"fully held-out passages. However, this is is not to say that OASM is not correlated with linguistic
566"
REFERENCES,0.5815815815815816,"features. For instance, sentences in a given passage are more semantically related with each other
567"
REFERENCES,0.5825825825825826,"than with sentences in other passages. Nonetheless, using shuffled train-test splits almost certainly
568"
REFERENCES,0.5835835835835835,"exaggerates the variance explained by a model which, on the basis of semantic similarity, arrives
569"
REFERENCES,0.5845845845845846,"at a similar representational structure as OASM. This is because task-irrelevant neural responses
570"
REFERENCES,0.5855855855855856,"make up a large fraction of neural activity [40], and shuffled train-test splits allow a model with
571"
REFERENCES,0.5865865865865866,"OASM-like representational structure to predict not just the task-relevant neural responses driven
572"
REFERENCES,0.5875875875875876,"by the participant reading the passage, but also any task-irrelevant neural activity that was present
573"
REFERENCES,0.5885885885885885,"throughout the reading of the passage. Hence, we strongly urge researchers to avoid shuffled train
574"
REFERENCES,0.5895895895895896,"test splits when evaluating the neural predictivity of language models, and we surmise that previous
575"
REFERENCES,0.5905905905905906,"studies using shuffled train-test splits to compare neural predictivity between models might have
576"
REFERENCES,0.5915915915915916,"come to erroneous conclusions.
577"
REFERENCES,0.5925925925925926,"A.9
Linear decodability of sentence length
578"
REFERENCES,0.5935935935935935,"Here, we show that the MLP block adds a linearly decodable component with non-zero mean to the
579"
REFERENCES,0.5945945945945946,"residual stream in the GPT2 architecture.
580"
REFERENCES,0.5955955955955956,"Proof :
581"
REFERENCES,0.5965965965965966,"We denote the i’th input to the MLP block in the first layer of GPT2-XL as xi. The output of the
582"
REFERENCES,0.5975975975975976,"MLP block is defined as follows:
583"
REFERENCES,0.5985985985985987,"MLP(xi) = xi + Wd(GELU(Wu(LayerNorm(xi))))
We assume that the elements of xi are normally distributed. For a given xi, it then follows that the
584"
REFERENCES,0.5995995995995996,"distribution of elements in LayerNorm(xi) is normal with µ = 0 and σ = 1 (assuming the standard
585"
REFERENCES,0.6006006006006006,"LayerNorm initialization).
586"
REFERENCES,0.6016016016016016,"Because Wu is initialized from a zero-mean normal distribution, Wu(LayerNorm(xi)) also has
587"
REFERENCES,0.6026026026026026,"zero-mean.
588"
REFERENCES,0.6036036036036037,"Note that GELU is a function for which E[Y ] > 0 for Y normally distributed with mean 0. Hence, the
589"
REFERENCES,0.6046046046046046,"mean value across elements following the GELU is non-zero. Let us denote this mean value across
590"
REFERENCES,0.6056056056056056,"all elements of GELU(Wu(LayerNorm(x))) and across all tokens x as m. Then, for an MLP
591"
REFERENCES,0.6066066066066066,"with up-projected dimension du, we can take the dot product of GELU(Wu(LayerNorm(xi)))
592"
REFERENCES,0.6076076076076076,"and
1
dum × ˆk, where ˆk is a du-dimensional vector of 1s. The resulting value will have mean 1.
593"
REFERENCES,0.6086086086086087,"However, we cannot decode this value directly from the MLP in practice; first, this vector is down-
projected back to the residual stream by Wd. Nonetheless, we can still closely approximate it,
assuming it is approximately orthogonal to xi, by using the pseudo-inverse of Wd. More specifically,
we can extract a scalar with mean 1 as follows:
r"
REFERENCES,0.6096096096096096,"du
dd
×
1
dum × ˆkW †
dMLP(xi)"
REFERENCES,0.6106106106106106,"where dd is the down-projected dimension. Because this extracted scalar value is distributed with
594"
REFERENCES,0.6116116116116116,"mean 1 across token representations xi, assuming independence of token representations within a
595"
REFERENCES,0.6126126126126126,"sentence, the sum of the extracted scalar value across the tokens of a sentence is distributed with
596"
REFERENCES,0.6136136136136137,"mean equaling the number of tokens in the sentence.
597"
REFERENCES,0.6146146146146146,"A.10
LMMS
598"
REFERENCES,0.6156156156156156,"LMMS generates a sense embedding for each word by averaging across contextual embeddings (in
599"
REFERENCES,0.6166166166166166,"our case from RoBERTa-Large) of that sense derived from a sense-annotated corpus. For words in
600"
REFERENCES,0.6176176176176176,"WordNet where labeled senses don’t exist, LMMS sets their sense embeddings equal to the average
601"
REFERENCES,0.6186186186186187,"of sense embeddings with the same sense (or same hypernym/lexname if that approach fails). Finally,
602"
REFERENCES,0.6196196196196196,"the sense embeddings are averaged together with the gloss embeddings for that sense of the word
603"
REFERENCES,0.6206206206206206,"generated using the same LLM. For additional details refer to Loureiro et al. [32].
604"
REFERENCES,0.6216216216216216,"A.11
Contextual syntactic representations
605"
REFERENCES,0.6226226226226226,"Syntactic embeddings are derived by substituting content words (nouns, verbs, adjectives, and
606"
REFERENCES,0.6236236236236237,"adverbs) in the original sentences with words from the Generics KB corpus, matching their part-of-
607"
REFERENCES,0.6246246246246246,"speech and dependency tag via the SpaCy transformer-based tagger [41]. For each sentence in the
608"
REFERENCES,0.6256256256256256,"Pereira dataset, we generate 170 new sentences, ensuring the subtree token indices from each token
609"
REFERENCES,0.6266266266266266,"match those of the original sentence. The top 100 sentences, selected based on summed surprisal
610"
REFERENCES,0.6276276276276276,"with GPT2-XL, are retained. Each sentence’s syntactic embedding is then computed by summing
611"
REFERENCES,0.6286286286286287,"token representations within each sentence and then averaging across the 100 sentences.
612"
REFERENCES,0.6296296296296297,"A.12
Word position feature in Fedorenko dataset
613"
REFERENCES,0.6306306306306306,"The primary finding in the paper which first collected the Fedorenko dataset [35] was a ramping of
614"
REFERENCES,0.6316316316316316,"neural activity across the words of sentences, where each sentence was 8 words long. Hence, we
615"
REFERENCES,0.6326326326326326,"concatenate a linearly ramping 1-dimensional positional signal to an 8-dimensional 1-hot positonal
616"
REFERENCES,0.6336336336336337,"1
2
3
4
5
6
7
8
Word Position
0.0 0.6"
REFERENCES,0.6346346346346347,Figure 7: Word Position feature for a single sentence in the Fedorenko dataset.
REFERENCES,0.6356356356356356,"signal. Because we expect positional signals to be more simlar between adjacent words than more
617"
REFERENCES,0.6366366366366366,"distant words, we apply a Gaussian filter (σ = 1) to the 8-dimensional positional signal. The resulting
618"
REFERENCES,0.6376376376376376,"feature space, which we refer to as ""word position"" in the main text, is shown for a single sentence in
619"
REFERENCES,0.6386386386386387,"the above figure.
620"
REFERENCES,0.6396396396396397,"A.13
OASM and GPT2 Model Comparison on Blank Dataset
621 0.0 0.3 R2"
REFERENCES,0.6406406406406406,"OASM
OASM + GPT2-XL
GPT2-XL"
REFERENCES,0.6416416416416416,"Figure 8: OASM far outperforms GPT2-XL on the Blank dataset, and GPT2-XL does not appear to
explain any variance beyond that explained by OASM."
REFERENCES,0.6426426426426426,"We find that OASM achieves 103.6 times higher neural predictivity than GPT2-XL on the Blank
622"
REFERENCES,0.6436436436436437,"dataset when using shuffled train-test splits. There could be several reasons for this. First, it might
623"
REFERENCES,0.6446446446446447,"be that the method for pooling representations from GPT2-XL used here 2.3 and in [2, 10, 11]
624"
REFERENCES,0.6456456456456456,"did not yield useful enough representations for GPT2-XL to map effectively to the brain data. An
625"
REFERENCES,0.6466466466466466,"additional likely culprit is that, of the three datasets we study here, Blank has the greatest potential for
626"
REFERENCES,0.6476476476476476,"autocorrelation in temporally adjacent samples. This is because, while the Pereira dataset typically
627"
REFERENCES,0.6486486486486487,"has a TR every 8 seconds, the Blank dataset has a TR every 2 seconds. We note that our results here
628"
REFERENCES,0.6496496496496497,"are not completely surprising; given that [2, 10] observed untrained GPT2 models perform far better
629"
REFERENCES,0.6506506506506506,"than trained models on this dataset, it did not seem likely that GPT2-XL would map onto neural
630"
REFERENCES,0.6516516516516516,"representations of linguistic features here.
631"
REFERENCES,0.6526526526526526,"A.14
Computational Resources
632"
REFERENCES,0.6536536536536537,"All analyses were done between 2 machines: One with 2 RTX 3090 GPUs, and another with 1
633"
REFERENCES,0.6546546546546547,"RTX 4090 GPU. The most computationally demanding parts of our analyses were fitting the banded
634"
REFERENCES,0.6556556556556556,"ridge regressions used to generate Figure 3, collecting untrained model results across 10 seeds, and
635"
REFERENCES,0.6566566566566566,"generating syntactic representations, which each took around 3 hours to complete.
636"
REFERENCES,0.6576576576576577,"A.15
Dataset Licenses
637"
REFERENCES,0.6586586586586587,"The Blank dataset was originally released as part of the Natural Stories Corpus, which is provided
638"
REFERENCES,0.6596596596596597,"under the CC BY-NC-SA license [23]. The Pereira dataset is released under the Creative Commons
639"
REFERENCES,0.6606606606606606,"License [8]. The version of the Fedorenko dataset used here is provided under the MIT license. All
640"
REFERENCES,0.6616616616616616,"datasets used are the same versions as in [2] and can be downloaded using the neural-nlp repository:
641"
REFERENCES,0.6626626626626627,"https://github.com/mschrimpf/neural-nlp/tree/master. All datasets were collected with
642"
REFERENCES,0.6636636636636637,"IRB approval at their respective institutions.
643"
REFERENCES,0.6646646646646647,"NeurIPS Paper Checklist
644"
CLAIMS,0.6656656656656657,"1. Claims
645"
CLAIMS,0.6666666666666666,"Question: Do the main claims made in the abstract and introduction accurately reflect the
646"
CLAIMS,0.6676676676676677,"paper’s contributions and scope?
647"
CLAIMS,0.6686686686686687,"Answer: [Yes]
648"
CLAIMS,0.6696696696696697,"Justification: We support each of the three claims made in the abstract regarding shuffled
649"
CLAIMS,0.6706706706706707,"train-test splits, untrained LLM brain scores, and trained LLM brain scores in the Results
650"
CLAIMS,0.6716716716716716,"section. These results support the claim that it is important to deconstruct the mapping
651"
CLAIMS,0.6726726726726727,"between LLMs and the brain.
652"
CLAIMS,0.6736736736736737,"Guidelines:
653"
CLAIMS,0.6746746746746747,"• The answer NA means that the abstract and introduction do not include the claims
654"
CLAIMS,0.6756756756756757,"made in the paper.
655"
CLAIMS,0.6766766766766766,"• The abstract and/or introduction should clearly state the claims made, including the
656"
CLAIMS,0.6776776776776777,"contributions made in the paper and important assumptions and limitations. A No or
657"
CLAIMS,0.6786786786786787,"NA answer to this question will not be perceived well by the reviewers.
658"
CLAIMS,0.6796796796796797,"• The claims made should match theoretical and experimental results, and reflect how
659"
CLAIMS,0.6806806806806807,"much the results can be expected to generalize to other settings.
660"
CLAIMS,0.6816816816816816,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
661"
CLAIMS,0.6826826826826827,"are not attained by the paper.
662"
LIMITATIONS,0.6836836836836837,"2. Limitations
663"
LIMITATIONS,0.6846846846846847,"Question: [Yes]
664"
LIMITATIONS,0.6856856856856857,"Justification:We discuss the three main limitations in the paper in the section titled ""Limita-
665"
LIMITATIONS,0.6866866866866866,"tions and Conclusions"", and additionally include limitations throughout the Appendix (e.g.
666"
LIMITATIONS,0.6876876876876877,"Justification of statistical tests).
667"
LIMITATIONS,0.6886886886886887,"Guidelines:
668"
LIMITATIONS,0.6896896896896897,"• The answer NA means that the paper has no limitation while the answer No means that
669"
LIMITATIONS,0.6906906906906907,"the paper has limitations, but those are not discussed in the paper.
670"
LIMITATIONS,0.6916916916916916,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
671"
LIMITATIONS,0.6926926926926927,"• The paper should point out any strong assumptions and how robust the results are to
672"
LIMITATIONS,0.6936936936936937,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
673"
LIMITATIONS,0.6946946946946947,"model well-specification, asymptotic approximations only holding locally). The authors
674"
LIMITATIONS,0.6956956956956957,"should reflect on how these assumptions might be violated in practice and what the
675"
LIMITATIONS,0.6966966966966966,"implications would be.
676"
LIMITATIONS,0.6976976976976977,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
677"
LIMITATIONS,0.6986986986986987,"only tested on a few datasets or with a few runs. In general, empirical results often
678"
LIMITATIONS,0.6996996996996997,"depend on implicit assumptions, which should be articulated.
679"
LIMITATIONS,0.7007007007007007,"• The authors should reflect on the factors that influence the performance of the approach.
680"
LIMITATIONS,0.7017017017017017,"For example, a facial recognition algorithm may perform poorly when image resolution
681"
LIMITATIONS,0.7027027027027027,"is low or images are taken in low lighting. Or a speech-to-text system might not be
682"
LIMITATIONS,0.7037037037037037,"used reliably to provide closed captions for online lectures because it fails to handle
683"
LIMITATIONS,0.7047047047047047,"technical jargon.
684"
LIMITATIONS,0.7057057057057057,"• The authors should discuss the computational efficiency of the proposed algorithms
685"
LIMITATIONS,0.7067067067067067,"and how they scale with dataset size.
686"
LIMITATIONS,0.7077077077077077,"• If applicable, the authors should discuss possible limitations of their approach to
687"
LIMITATIONS,0.7087087087087087,"address problems of privacy and fairness.
688"
LIMITATIONS,0.7097097097097097,"• While the authors might fear that complete honesty about limitations might be used by
689"
LIMITATIONS,0.7107107107107107,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
690"
LIMITATIONS,0.7117117117117117,"limitations that aren’t acknowledged in the paper. The authors should use their best
691"
LIMITATIONS,0.7127127127127127,"judgment and recognize that individual actions in favor of transparency play an impor-
692"
LIMITATIONS,0.7137137137137137,"tant role in developing norms that preserve the integrity of the community. Reviewers
693"
LIMITATIONS,0.7147147147147147,"will be specifically instructed to not penalize honesty concerning limitations.
694"
THEORY ASSUMPTIONS AND PROOFS,0.7157157157157157,"3. Theory Assumptions and Proofs
695"
THEORY ASSUMPTIONS AND PROOFS,0.7167167167167167,"Question: For each theoretical result, does the paper provide the full set of assumptions and
696"
THEORY ASSUMPTIONS AND PROOFS,0.7177177177177178,"a complete (and correct) proof?
697"
THEORY ASSUMPTIONS AND PROOFS,0.7187187187187187,"Answer: [Yes]
698"
THEORY ASSUMPTIONS AND PROOFS,0.7197197197197197,"Justification: Our only theoretical result is that the MLP layer introduces a non-zero mean
699"
THEORY ASSUMPTIONS AND PROOFS,0.7207207207207207,"component in the residual stream. We provide both a rough sketch in the main paper as well
700"
THEORY ASSUMPTIONS AND PROOFS,0.7217217217217218,"as a formal proof.
701"
THEORY ASSUMPTIONS AND PROOFS,0.7227227227227228,"Guidelines:
702"
THEORY ASSUMPTIONS AND PROOFS,0.7237237237237237,"• The answer NA means that the paper does not include theoretical results.
703"
THEORY ASSUMPTIONS AND PROOFS,0.7247247247247247,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
704"
THEORY ASSUMPTIONS AND PROOFS,0.7257257257257257,"referenced.
705"
THEORY ASSUMPTIONS AND PROOFS,0.7267267267267268,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
706"
THEORY ASSUMPTIONS AND PROOFS,0.7277277277277278,"• The proofs can either appear in the main paper or the supplemental material, but if
707"
THEORY ASSUMPTIONS AND PROOFS,0.7287287287287287,"they appear in the supplemental material, the authors are encouraged to provide a short
708"
THEORY ASSUMPTIONS AND PROOFS,0.7297297297297297,"proof sketch to provide intuition.
709"
THEORY ASSUMPTIONS AND PROOFS,0.7307307307307307,"• Inversely, any informal proof provided in the core of the paper should be complemented
710"
THEORY ASSUMPTIONS AND PROOFS,0.7317317317317318,"by formal proofs provided in appendix or supplemental material.
711"
THEORY ASSUMPTIONS AND PROOFS,0.7327327327327328,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7337337337337337,"4. Experimental Result Reproducibility
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7347347347347347,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
714"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7357357357357357,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
715"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7367367367367368,"of the paper (regardless of whether the code and data are provided or not)?
716"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7377377377377378,"Answer: [Yes]
717"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7387387387387387,"Justification: We include all details regarding the following: banded regression proce-
718"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7397397397397397,"dure, construction of feature spaces, train, validation, and test splits, and selection of
719"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7407407407407407,"voxels/electrodes/fROIs in neural data. These are all the elements needed to reproduce our
720"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417417417417418,"results, with the exception of slight variability due to stochasticity in untrained LLM seeds
721"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7427427427427428,"and the randoms search process in banded regression.
722"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7437437437437437,"Guidelines:
723"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7447447447447447,"• The answer NA means that the paper does not include experiments.
724"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7457457457457457,"• If the paper includes experiments, a No answer to this question will not be perceived
725"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7467467467467468,"well by the reviewers: Making the paper reproducible is important, regardless of
726"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7477477477477478,"whether the code and data are provided or not.
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7487487487487487,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497497497497497,"to make their results reproducible or verifiable.
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507507507507507,"• Depending on the contribution, reproducibility can be accomplished in various ways.
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7517517517517518,"For example, if the contribution is a novel architecture, describing the architecture fully
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7527527527527528,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7537537537537538,"be necessary to either make it possible for others to replicate the model with the same
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7547547547547547,"dataset, or provide access to the model. In general. releasing code and data is often
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7557557557557557,"one good way to accomplish this, but reproducibility can also be provided via detailed
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567567567567568,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7577577577577578,"of a large language model), releasing of a model checkpoint, or other means that are
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7587587587587588,"appropriate to the research performed.
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7597597597597597,"• While NeurIPS does not require releasing code, the conference does require all submis-
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7607607607607607,"sions to provide some reasonable avenue for reproducibility, which may depend on the
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7617617617617618,"nature of the contribution. For example
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7627627627627628,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7637637637637638,"to reproduce that algorithm.
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647647647647647,"(b) If the contribution is primarily a new model architecture, the paper should describe
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657657657657657,"the architecture clearly and fully.
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667667667667668,"(c) If the contribution is a new model (e.g., a large language model), then there should
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677677677677678,"either be a way to access this model for reproducing the results or a way to reproduce
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7687687687687688,"the model (e.g., with an open-source dataset or instructions for how to construct
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7697697697697697,"the dataset).
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7707707707707707,"(d) We recognize that reproducibility may be tricky in some cases, in which case
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7717717717717718,"authors are welcome to describe the particular way they provide for reproducibility.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7727727727727728,"In the case of closed-source models, it may be that access to the model is limited in
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7737737737737738,"some way (e.g., to registered users), but it should be possible for other researchers
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747747747747747,"to have some path to reproducing or verifying the results.
754"
OPEN ACCESS TO DATA AND CODE,0.7757757757757757,"5. Open access to data and code
755"
OPEN ACCESS TO DATA AND CODE,0.7767767767767768,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
756"
OPEN ACCESS TO DATA AND CODE,0.7777777777777778,"tions to faithfully reproduce the main experimental results, as described in supplemental
757"
OPEN ACCESS TO DATA AND CODE,0.7787787787787788,"material?
758"
OPEN ACCESS TO DATA AND CODE,0.7797797797797797,"Answer: [Yes]
759"
OPEN ACCESS TO DATA AND CODE,0.7807807807807807,"Justification: We will release all our code on Github, and all neural datasets are openly
760"
OPEN ACCESS TO DATA AND CODE,0.7817817817817818,"available for use. We also provide anonymized code.
761"
OPEN ACCESS TO DATA AND CODE,0.7827827827827828,"Guidelines:
762"
OPEN ACCESS TO DATA AND CODE,0.7837837837837838,"• The answer NA means that paper does not include experiments requiring code.
763"
OPEN ACCESS TO DATA AND CODE,0.7847847847847848,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
764"
OPEN ACCESS TO DATA AND CODE,0.7857857857857858,"public/guides/CodeSubmissionPolicy) for more details.
765"
OPEN ACCESS TO DATA AND CODE,0.7867867867867868,"• While we encourage the release of code and data, we understand that this might not be
766"
OPEN ACCESS TO DATA AND CODE,0.7877877877877878,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
767"
OPEN ACCESS TO DATA AND CODE,0.7887887887887888,"including code, unless this is central to the contribution (e.g., for a new open-source
768"
OPEN ACCESS TO DATA AND CODE,0.7897897897897898,"benchmark).
769"
OPEN ACCESS TO DATA AND CODE,0.7907907907907908,"• The instructions should contain the exact command and environment needed to run to
770"
OPEN ACCESS TO DATA AND CODE,0.7917917917917918,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
771"
OPEN ACCESS TO DATA AND CODE,0.7927927927927928,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
772"
OPEN ACCESS TO DATA AND CODE,0.7937937937937938,"• The authors should provide instructions on data access and preparation, including how
773"
OPEN ACCESS TO DATA AND CODE,0.7947947947947948,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
774"
OPEN ACCESS TO DATA AND CODE,0.7957957957957958,"• The authors should provide scripts to reproduce all experimental results for the new
775"
OPEN ACCESS TO DATA AND CODE,0.7967967967967968,"proposed method and baselines. If only a subset of experiments are reproducible, they
776"
OPEN ACCESS TO DATA AND CODE,0.7977977977977978,"should state which ones are omitted from the script and why.
777"
OPEN ACCESS TO DATA AND CODE,0.7987987987987988,"• At submission time, to preserve anonymity, the authors should release anonymized
778"
OPEN ACCESS TO DATA AND CODE,0.7997997997997998,"versions (if applicable).
779"
OPEN ACCESS TO DATA AND CODE,0.8008008008008008,"• Providing as much information as possible in supplemental material (appended to the
780"
OPEN ACCESS TO DATA AND CODE,0.8018018018018018,"paper) is recommended, but including URLs to data and code is permitted.
781"
OPEN ACCESS TO DATA AND CODE,0.8028028028028028,"6. Experimental Setting/Details
782"
OPEN ACCESS TO DATA AND CODE,0.8038038038038038,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
783"
OPEN ACCESS TO DATA AND CODE,0.8048048048048048,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
784"
OPEN ACCESS TO DATA AND CODE,0.8058058058058059,"results?
785"
OPEN ACCESS TO DATA AND CODE,0.8068068068068068,"Answer: [Yes]
786"
OPEN ACCESS TO DATA AND CODE,0.8078078078078078,"Justification: We dedicate sections towards explaining the data splits in the main paper, and
787"
OPEN ACCESS TO DATA AND CODE,0.8088088088088088,"the necessary details to run the banded ridge regression in the main paper and Appendix.
788"
OPEN ACCESS TO DATA AND CODE,0.8098098098098098,"Guidelines:
789"
OPEN ACCESS TO DATA AND CODE,0.8108108108108109,"• The answer NA means that the paper does not include experiments.
790"
OPEN ACCESS TO DATA AND CODE,0.8118118118118118,"• The experimental setting should be presented in the core of the paper to a level of detail
791"
OPEN ACCESS TO DATA AND CODE,0.8128128128128128,"that is necessary to appreciate the results and make sense of them.
792"
OPEN ACCESS TO DATA AND CODE,0.8138138138138138,"• The full details can be provided either with the code, in appendix, or as supplemental
793"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"material.
794"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8158158158158159,"7. Experiment Statistical Significance
795"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8168168168168168,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
796"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8178178178178178,"information about the statistical significance of the experiments?
797"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8188188188188188,"Answer: [Yes]
798"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8198198198198198,"Justification: We perform a paired t-test and justify its use in the Appendix. For all plots
799"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8208208208208209,"which show the average across participants we show individual dots for each participant,
800"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8218218218218218,"and for this reason we do not include standard deviation values for the values in Table 1.
801"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8228228228228228,"Guidelines:
802"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8238238238238238,"• The answer NA means that the paper does not include experiments.
803"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8248248248248248,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
804"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8258258258258259,"dence intervals, or statistical significance tests, at least for the experiments that support
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8268268268268268,"the main claims of the paper.
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8278278278278278,"• The factors of variability that the error bars are capturing should be clearly stated (for
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8288288288288288,"example, train/test split, initialization, random drawing of some parameter, or overall
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8298298298298298,"run with given experimental conditions).
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8308308308308309,"• The method for calculating the error bars should be explained (closed form formula,
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318318318318318,"call to a library function, bootstrap, etc.)
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8328328328328328,"• The assumptions made should be given (e.g., Normally distributed errors).
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8338338338338338,"• It should be clear whether the error bar is the standard deviation or the standard error
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8348348348348348,"of the mean.
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8358358358358359,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368368368368369,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378378378378378,"of Normality of errors is not verified.
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8388388388388388,"• For asymmetric distributions, the authors should be careful not to show in tables or
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8398398398398398,"figures symmetric error bars that would yield results that are out of range (e.g. negative
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8408408408408409,"error rates).
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8418418418418419,"• If error bars are reported in tables or plots, The authors should explain in the text how
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8428428428428428,"they were calculated and reference the corresponding figures or tables in the text.
822"
EXPERIMENTS COMPUTE RESOURCES,0.8438438438438438,"8. Experiments Compute Resources
823"
EXPERIMENTS COMPUTE RESOURCES,0.8448448448448449,"Question: For each experiment, does the paper provide sufficient information on the com-
824"
EXPERIMENTS COMPUTE RESOURCES,0.8458458458458459,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
825"
EXPERIMENTS COMPUTE RESOURCES,0.8468468468468469,"the experiments?
826"
EXPERIMENTS COMPUTE RESOURCES,0.8478478478478478,"Answer: [Yes]
827"
EXPERIMENTS COMPUTE RESOURCES,0.8488488488488488,"Justification: We provide a section in the appendix describing the GPUs and CPUs used for
828"
EXPERIMENTS COMPUTE RESOURCES,0.8498498498498499,"our analyses, and we describe how long each experiment took to run.
829"
EXPERIMENTS COMPUTE RESOURCES,0.8508508508508509,"Guidelines:
830"
EXPERIMENTS COMPUTE RESOURCES,0.8518518518518519,"• The answer NA means that the paper does not include experiments.
831"
EXPERIMENTS COMPUTE RESOURCES,0.8528528528528528,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
832"
EXPERIMENTS COMPUTE RESOURCES,0.8538538538538538,"or cloud provider, including relevant memory and storage.
833"
EXPERIMENTS COMPUTE RESOURCES,0.8548548548548549,"• The paper should provide the amount of compute required for each of the individual
834"
EXPERIMENTS COMPUTE RESOURCES,0.8558558558558559,"experimental runs as well as estimate the total compute.
835"
EXPERIMENTS COMPUTE RESOURCES,0.8568568568568569,"• The paper should disclose whether the full research project required more compute
836"
EXPERIMENTS COMPUTE RESOURCES,0.8578578578578578,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
837"
EXPERIMENTS COMPUTE RESOURCES,0.8588588588588588,"didn’t make it into the paper).
838"
CODE OF ETHICS,0.8598598598598599,"9. Code Of Ethics
839"
CODE OF ETHICS,0.8608608608608609,"Question: Does the research conducted in the paper conform, in every respect, with the
840"
CODE OF ETHICS,0.8618618618618619,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
841"
CODE OF ETHICS,0.8628628628628628,"Answer: [Yes]
842"
CODE OF ETHICS,0.8638638638638638,"Justification: We did not conduct any direct interactions with human participants, none of
843"
CODE OF ETHICS,0.8648648648648649,"the data-related concerns apply for us, and we do not see any direct societal impacts from
844"
CODE OF ETHICS,0.8658658658658659,"our work. We make our methods clear to the best of our ability and provide anonymized
845"
CODE OF ETHICS,0.8668668668668669,"code.
846"
CODE OF ETHICS,0.8678678678678678,"Guidelines:
847"
CODE OF ETHICS,0.8688688688688688,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
848"
CODE OF ETHICS,0.8698698698698699,"• If the authors answer No, they should explain the special circumstances that require a
849"
CODE OF ETHICS,0.8708708708708709,"deviation from the Code of Ethics.
850"
CODE OF ETHICS,0.8718718718718719,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
851"
CODE OF ETHICS,0.8728728728728729,"eration due to laws or regulations in their jurisdiction).
852"
BROADER IMPACTS,0.8738738738738738,"10. Broader Impacts
853"
BROADER IMPACTS,0.8748748748748749,"Question: Does the paper discuss both potential positive societal impacts and negative
854"
BROADER IMPACTS,0.8758758758758759,"societal impacts of the work performed?
855"
BROADER IMPACTS,0.8768768768768769,"Answer: [NA]
856"
BROADER IMPACTS,0.8778778778778779,"Justification: We do not develop any novel technology that can be used for good or bad, but
857"
BROADER IMPACTS,0.8788788788788788,"rather show that some high-profile previous results have been over-interpreted. While our
858"
BROADER IMPACTS,0.8798798798798799,"results are relevant for the cognitive neuroscience community, we do not see a direct path to
859"
BROADER IMPACTS,0.8808808808808809,"any larger societal impacts.
860"
BROADER IMPACTS,0.8818818818818819,"Guidelines:
861"
BROADER IMPACTS,0.8828828828828829,"• The answer NA means that there is no societal impact of the work performed.
862"
BROADER IMPACTS,0.8838838838838838,"• If the authors answer NA or No, they should explain why their work has no societal
863"
BROADER IMPACTS,0.8848848848848849,"impact or why the paper does not address societal impact.
864"
BROADER IMPACTS,0.8858858858858859,"• Examples of negative societal impacts include potential malicious or unintended uses
865"
BROADER IMPACTS,0.8868868868868869,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
866"
BROADER IMPACTS,0.8878878878878879,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
867"
BROADER IMPACTS,0.8888888888888888,"groups), privacy considerations, and security considerations.
868"
BROADER IMPACTS,0.8898898898898899,"• The conference expects that many papers will be foundational research and not tied
869"
BROADER IMPACTS,0.8908908908908909,"to particular applications, let alone deployments. However, if there is a direct path to
870"
BROADER IMPACTS,0.8918918918918919,"any negative applications, the authors should point it out. For example, it is legitimate
871"
BROADER IMPACTS,0.8928928928928929,"to point out that an improvement in the quality of generative models could be used to
872"
BROADER IMPACTS,0.8938938938938938,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
873"
BROADER IMPACTS,0.8948948948948949,"that a generic algorithm for optimizing neural networks could enable people to train
874"
BROADER IMPACTS,0.8958958958958959,"models that generate Deepfakes faster.
875"
BROADER IMPACTS,0.8968968968968969,"• The authors should consider possible harms that could arise when the technology is
876"
BROADER IMPACTS,0.8978978978978979,"being used as intended and functioning correctly, harms that could arise when the
877"
BROADER IMPACTS,0.8988988988988988,"technology is being used as intended but gives incorrect results, and harms following
878"
BROADER IMPACTS,0.8998998998998999,"from (intentional or unintentional) misuse of the technology.
879"
BROADER IMPACTS,0.9009009009009009,"• If there are negative societal impacts, the authors could also discuss possible mitigation
880"
BROADER IMPACTS,0.9019019019019019,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
881"
BROADER IMPACTS,0.9029029029029029,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
882"
BROADER IMPACTS,0.9039039039039038,"feedback over time, improving the efficiency and accessibility of ML).
883"
SAFEGUARDS,0.9049049049049049,"11. Safeguards
884"
SAFEGUARDS,0.9059059059059059,"Question: Does the paper describe safeguards that have been put in place for responsible
885"
SAFEGUARDS,0.9069069069069069,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
886"
SAFEGUARDS,0.9079079079079079,"image generators, or scraped datasets)?
887"
SAFEGUARDS,0.908908908908909,"Answer: [NA]
888"
SAFEGUARDS,0.9099099099099099,"Justification: We release no new models or datasets, and do not see any potential for our
889"
SAFEGUARDS,0.9109109109109109,"results being misused in unsafe ways.
890"
SAFEGUARDS,0.9119119119119119,"Guidelines:
891"
SAFEGUARDS,0.9129129129129129,"• The answer NA means that the paper poses no such risks.
892"
SAFEGUARDS,0.913913913913914,"• Released models that have a high risk for misuse or dual-use should be released with
893"
SAFEGUARDS,0.914914914914915,"necessary safeguards to allow for controlled use of the model, for example by requiring
894"
SAFEGUARDS,0.9159159159159159,"that users adhere to usage guidelines or restrictions to access the model or implementing
895"
SAFEGUARDS,0.9169169169169169,"safety filters.
896"
SAFEGUARDS,0.9179179179179179,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
897"
SAFEGUARDS,0.918918918918919,"should describe how they avoided releasing unsafe images.
898"
SAFEGUARDS,0.91991991991992,"• We recognize that providing effective safeguards is challenging, and many papers do
899"
SAFEGUARDS,0.9209209209209209,"not require this, but we encourage authors to take this into account and make a best
900"
SAFEGUARDS,0.9219219219219219,"faith effort.
901"
LICENSES FOR EXISTING ASSETS,0.9229229229229229,"12. Licenses for existing assets
902"
LICENSES FOR EXISTING ASSETS,0.923923923923924,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
903"
LICENSES FOR EXISTING ASSETS,0.924924924924925,"the paper, properly credited and are the license and terms of use explicitly mentioned and
904"
LICENSES FOR EXISTING ASSETS,0.9259259259259259,"properly respected?
905"
LICENSES FOR EXISTING ASSETS,0.9269269269269269,"Answer: [Yes]
906"
LICENSES FOR EXISTING ASSETS,0.9279279279279279,"Justification: We cite the papers in which all datasets used were first published. We provide
907"
LICENSES FOR EXISTING ASSETS,0.928928928928929,"the licenses for the Blank and Pereira datasets in the supplement (we could not find a license
908"
LICENSES FOR EXISTING ASSETS,0.92992992992993,"for the Fedorenko dataset). We also specify the version of the datasets used and provide a
909"
LICENSES FOR EXISTING ASSETS,0.9309309309309309,"link.
910"
LICENSES FOR EXISTING ASSETS,0.9319319319319319,"Guidelines:
911"
LICENSES FOR EXISTING ASSETS,0.9329329329329329,"• The answer NA means that the paper does not use existing assets.
912"
LICENSES FOR EXISTING ASSETS,0.933933933933934,"• The authors should cite the original paper that produced the code package or dataset.
913"
LICENSES FOR EXISTING ASSETS,0.934934934934935,"• The authors should state which version of the asset is used and, if possible, include a
914"
LICENSES FOR EXISTING ASSETS,0.9359359359359359,"URL.
915"
LICENSES FOR EXISTING ASSETS,0.9369369369369369,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
916"
LICENSES FOR EXISTING ASSETS,0.9379379379379379,"• For scraped data from a particular source (e.g., website), the copyright and terms of
917"
LICENSES FOR EXISTING ASSETS,0.938938938938939,"service of that source should be provided.
918"
LICENSES FOR EXISTING ASSETS,0.93993993993994,"• If assets are released, the license, copyright information, and terms of use in the
919"
LICENSES FOR EXISTING ASSETS,0.9409409409409409,"package should be provided. For popular datasets, paperswithcode.com/datasets
920"
LICENSES FOR EXISTING ASSETS,0.9419419419419419,"has curated licenses for some datasets. Their licensing guide can help determine the
921"
LICENSES FOR EXISTING ASSETS,0.9429429429429429,"license of a dataset.
922"
LICENSES FOR EXISTING ASSETS,0.943943943943944,"• For existing datasets that are re-packaged, both the original license and the license of
923"
LICENSES FOR EXISTING ASSETS,0.944944944944945,"the derived asset (if it has changed) should be provided.
924"
LICENSES FOR EXISTING ASSETS,0.9459459459459459,"• If this information is not available online, the authors are encouraged to reach out to
925"
LICENSES FOR EXISTING ASSETS,0.9469469469469469,"the asset’s creators.
926"
NEW ASSETS,0.9479479479479479,"13. New Assets
927"
NEW ASSETS,0.948948948948949,"Question: Are new assets introduced in the paper well documented and is the documentation
928"
NEW ASSETS,0.94994994994995,"provided alongside the assets?
929"
NEW ASSETS,0.950950950950951,"Answer: [NA]
930"
NEW ASSETS,0.9519519519519519,"Justification: We do not release any new assets with this paper.
931"
NEW ASSETS,0.9529529529529529,"Guidelines:
932"
NEW ASSETS,0.953953953953954,"• The answer NA means that the paper does not release new assets.
933"
NEW ASSETS,0.954954954954955,"• Researchers should communicate the details of the dataset/code/model as part of their
934"
NEW ASSETS,0.955955955955956,"submissions via structured templates. This includes details about training, license,
935"
NEW ASSETS,0.9569569569569569,"limitations, etc.
936"
NEW ASSETS,0.9579579579579579,"• The paper should discuss whether and how consent was obtained from people whose
937"
NEW ASSETS,0.958958958958959,"asset is used.
938"
NEW ASSETS,0.95995995995996,"• At submission time, remember to anonymize your assets (if applicable). You can either
939"
NEW ASSETS,0.960960960960961,"create an anonymized URL or include an anonymized zip file.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9619619619619619,"14. Crowdsourcing and Research with Human Subjects
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9629629629629629,"Question: For crowdsourcing experiments and research with human subjects, does the paper
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.963963963963964,"include the full text of instructions given to participants and screenshots, if applicable, as
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964964964964965,"well as details about compensation (if any)?
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.965965965965966,"Answer: [Yes]
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669669669669669,"Justification: We use open source datasets where neural data is obtained from consenting
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9679679679679679,"human adults. Information regarding research protocols is detailed in the references for
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968968968968969,"these datasets.
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.96996996996997,"Guidelines:
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970970970970971,"• The answer NA means that the paper does not involve crowdsourcing nor research with
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719719719719719,"human subjects.
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972972972972973,"• Including this information in the supplemental material is fine, but if the main contribu-
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973973973973974,"tion of the paper involves human subjects, then as much detail as possible should be
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974974974974975,"included in the main paper.
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.975975975975976,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769769769769769,"or other labor should be paid at least the minimum wage in the country of the data
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977977977977978,"collector.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978978978978979,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.97997997997998,"Subjects
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980980980980981,"Question: Does the paper describe potential risks incurred by study participants, whether
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9819819819819819,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.982982982982983,"approvals (or an equivalent approval/review based on the requirements of your country or
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983983983983984,"institution) were obtained?
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984984984984985,"Answer: [Yes]
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.985985985985986,"Justification: All datasets used here were collected with IRB approval at their respective
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986986986986987,"institutions, and this is stated in the appendix. We do not collect any data of our own from
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987987987987988,"human subjects.
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988988988988989,"Guidelines:
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98998998998999,"• The answer NA means that the paper does not involve crowdsourcing nor research with
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.990990990990991,"human subjects.
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991991991991992,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992992992992993,"may be required for any human subjects research. If you obtained IRB approval, you
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993993993993994,"should clearly state this in the paper.
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994994994994995,"• We recognize that the procedures for this may vary significantly between institutions
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995995995995996,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996996996996997,"guidelines for their institution.
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997997997997998,"• For initial submissions, do not include any information that would break anonymity (if
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998998998998999,"applicable), such as the institution conducting the review.
978"
