Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001834862385321101,"Despite Federated Learning (FL)’s trend for learning machine learning models in a
1"
ABSTRACT,0.003669724770642202,"distributed manner, it is susceptible to performance drops when training on hetero-
2"
ABSTRACT,0.005504587155963303,"geneous data. In addition, FL inevitability faces the challenges of synchronization,
3"
ABSTRACT,0.007339449541284404,"efﬁciency, and privacy. Recently, dataset distillation has been explored in order to
4"
ABSTRACT,0.009174311926605505,"improve the efﬁciency and scalability of FL by creating a smaller, synthetic dataset
5"
ABSTRACT,0.011009174311926606,"that retains the performance of a model trained on the local private datasets. We
6"
ABSTRACT,0.012844036697247707,"discover that using distilled local datasets can amplify the heterogeneity issue in
7"
ABSTRACT,0.014678899082568808,"FL. To address this, we propose a new method, called Federated Virtual Learning
8"
ABSTRACT,0.01651376146788991,"on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains
9"
ABSTRACT,0.01834862385321101,"FL using a smaller synthetic dataset (referred as virtual data) created through a
10"
ABSTRACT,0.02018348623853211,"combination of local and global dataset distillation. Speciﬁcally, to handle syn-
11"
ABSTRACT,0.022018348623853212,"chronization and class imbalance, we propose iterative distribution matching to
12"
ABSTRACT,0.023853211009174313,"allow clients to have the same amount of balanced local virtual data; to harmonize
13"
ABSTRACT,0.025688073394495414,"the domain shifts, we use federated gradient matching to distill global virtual data
14"
ABSTRACT,0.027522935779816515,"that are shared with clients without hindering data privacy to rectify heterogeneous
15"
ABSTRACT,0.029357798165137616,"local training via enforcing local-global feature similarity. We experiment on both
16"
ABSTRACT,0.031192660550458717,"benchmark and real-world datasets that contain heterogeneous data from different
17"
ABSTRACT,0.03302752293577982,"sources, and further scale up to an FL scenario that contains large number of
18"
ABSTRACT,0.03486238532110092,"clients with heterogeneous and class imbalance data. Our method outperforms
19"
ABSTRACT,0.03669724770642202,"state-of-the-art heterogeneous FL algorithms under various settings with a very
20"
ABSTRACT,0.03853211009174312,"limited amount of distilled virtual data.
21"
INTRODUCTION,0.04036697247706422,"1
Introduction
22"
INTRODUCTION,0.04220183486238532,"Federated Learning (FL) [29] has become a popular solution for different institutions to collaboratively
23"
INTRODUCTION,0.044036697247706424,"train machine learning models without pooling private data together. Typically, it involves a central
24"
INTRODUCTION,0.045871559633027525,"server and multiple local clients; then the model is trained via aggregation of local network parameter
25"
INTRODUCTION,0.047706422018348627,"updates on the server side iteratively. FL is widely accepted in many areas, such as computer vision,
26"
INTRODUCTION,0.04954128440366973,"natural language processing, and medical image analysis [25, 12, 41].
27"
INTRODUCTION,0.05137614678899083,"On the one hand, clients with different amounts of data cause asynchronization and affect the efﬁciency
28"
INTRODUCTION,0.05321100917431193,"of FL systems. Dataset distillation [39, 5, 46, 44, 45] addresses the issue by only summarizing smaller
29"
INTRODUCTION,0.05504587155963303,"synthetic datasets from the private local datasets to ensure each client owns the same amount of
30"
INTRODUCTION,0.05688073394495413,"data. We refer this underexplored strategy as federated virtual learning, as the models are trained
31"
INTRODUCTION,0.05871559633027523,"from synthetic data [40, 10, 16]. These methods have been found to perform better than model-
32"
INTRODUCTION,0.060550458715596334,"synchronization-based FL approaches while requiring fewer server-client interactions.
33"
INTRODUCTION,0.062385321100917435,"On the other hand, due to different data collection protocols, data from different clients inevitably
34"
INTRODUCTION,0.06422018348623854,"face heterogeneity problems with domain shift, which means data may not be independent and
35"
INTRODUCTION,0.06605504587155964,"identically distributed (iid) among clients. Heterogeneous data distribution among clients becomes a
36"
INTRODUCTION,0.06788990825688074,"key challenge in FL, as aggregating model parameters from non-iid feature distributions suffers from
37"
INTRODUCTION,0.06972477064220184,"client drift [18] and diverges the global model update[26].
38"
INTRODUCTION,0.07155963302752294,"We observe that using locally distilled datasets can amplify the heterogeneity issue. Figure 1 shows
39"
INTRODUCTION,0.07339449541284404,"the tSNE plots of two different datasets, USPS [31] and SynthDigits [9], each considered as a client.
40"
INTRODUCTION,0.07522935779816514,"tSNE takes the original and distilled virtual images as input and embeds them into 2D planes. One
41"
INTRODUCTION,0.07706422018348624,"can observe that the distribution becomes diverse after distillation.
42"
INTRODUCTION,0.07889908256880734,"Figure 1: Distilled local datasets can worsen hetero-
geneity in FL. tSNE plots of (a) original datasets and
(b) distilled virtual datasets of USPS (client 0) and Syn-
thDigits (client 1). The two distributions are marked in
red and blue. We observe fewer overlapped ◦and ⇥in
(b) compared with (a), indicating higher heterogeneity
between two clients after distillation."
INTRODUCTION,0.08073394495412844,"To alleviate the problem of data heterogeneity
43"
INTRODUCTION,0.08256880733944955,"in classical FL settings, two main orthogonal
44"
INTRODUCTION,0.08440366972477065,"approaches can be taken.
Approach 1 aims
45"
INTRODUCTION,0.08623853211009175,"to minimize the difference between the local
46"
INTRODUCTION,0.08807339449541285,"and global model parameters to improve conver-
47"
INTRODUCTION,0.08990825688073395,"gence [25, 18, 38]. Approach 2 enforces con-
48"
INTRODUCTION,0.09174311926605505,"sistency in local embedded features using an-
49"
INTRODUCTION,0.09357798165137615,"chors and regularization loss [37, 47, 42]. The
50"
INTRODUCTION,0.09541284403669725,"ﬁrst approach can be easily applied to distilled
51"
INTRODUCTION,0.09724770642201835,"local datasets, while the second approach has
52"
INTRODUCTION,0.09908256880733946,"limitations when adapting to federated virtual
53"
INTRODUCTION,0.10091743119266056,"learning. Speciﬁcally, VHL [37] samples global
54"
INTRODUCTION,0.10275229357798166,"anchors from untrained StyleGAN [19] suffers
55"
INTRODUCTION,0.10458715596330276,"performance drop when handling ampliﬁed het-
56"
INTRODUCTION,0.10642201834862386,"erogeneity after dataset distillation. Other meth-
57"
INTRODUCTION,0.10825688073394496,"ods, such as those that rely on external global
58"
INTRODUCTION,0.11009174311926606,"data [47], or feature sharing from clients [42],
59"
INTRODUCTION,0.11192660550458716,"are less practical, as they pose greater data privacy risks compared to classical FL settings1. Without
60"
INTRODUCTION,0.11376146788990826,"hindering data privacy, developing strategies following approach 2 for federated virtual learning on
61"
INTRODUCTION,0.11559633027522936,"heterogeneous data remains open questions on 1) how to set up global anchors for locally distilled
62"
INTRODUCTION,0.11743119266055047,"datasets and 2) how to select the proper regularization loss(es).
63"
INTRODUCTION,0.11926605504587157,"To this end, we propose FEDLGD, a federated virtual learning method with local and global dis-
64"
INTRODUCTION,0.12110091743119267,"tillation. We propose iterative distribution matching in local distillation by comparing the feature
65"
INTRODUCTION,0.12293577981651377,"distribution of real and synthetic data using an evolving feature extractor. The local distillation results
66"
INTRODUCTION,0.12477064220183487,"in smaller sets with balanced class distributions, achieving efﬁciency and synchronization while
67"
INTRODUCTION,0.12660550458715597,"avoiding class imbalance. FEDLGD updates the local model on local distilled synthetic datasets
68"
INTRODUCTION,0.12844036697247707,"(named local virtual data). We found that training FL with local virtual data can exacerbate hetero-
69"
INTRODUCTION,0.13027522935779817,"geneity in feature space if clients’ data has domain shift (Figure. 1). Therefore, unlike previously
70"
INTRODUCTION,0.13211009174311927,"proposed federated virtual learning methods that rely solely on local distillation [10, 40, 16], we also
71"
INTRODUCTION,0.13394495412844037,"propose a novel and efﬁcient method, federated gradient matching, that integrated well with FL to
72"
INTRODUCTION,0.13577981651376148,"distill global virtual data as anchors on the server side. This approach aims to alleviate domain shifts
73"
INTRODUCTION,0.13761467889908258,"among clients by promoting similarity between local and global features. Note that we only share
74"
INTRODUCTION,0.13944954128440368,"local model parameters w.r.t. distilled data. Thus, the privacy of local original data is preserved. We
75"
INTRODUCTION,0.14128440366972478,"conclude our contributions as follows:
76"
INTRODUCTION,0.14311926605504588,"• This paper focuses on an important but underexplored FL setting in which local models
77"
INTRODUCTION,0.14495412844036698,"are trained on small distilled datasets, which we refer to as federated virtual learning. We
78"
INTRODUCTION,0.14678899082568808,"design two effective and efﬁcient dataset distillation methods for FL.
79"
INTRODUCTION,0.14862385321100918,"• We are the ﬁrst to reveal that when datasets are distilled from clients’ data with domain shift,
80"
INTRODUCTION,0.15045871559633028,"the heterogeneity problem can be exacerbated in the federated virtual learning setting.
81"
INTRODUCTION,0.15229357798165138,"• We propose to address the heterogeneity problem by mapping clients to similar features
82"
INTRODUCTION,0.15412844036697249,"regularized by gradually updated global virtual data using averaged client gradients.
83"
INTRODUCTION,0.1559633027522936,"• Through comprehensive experiments on benchmark and real-world datasets, we show that
84"
INTRODUCTION,0.1577981651376147,"FEDLGD outperforms existing state-of-the-art FL algorithms.
85"
INTRODUCTION,0.1596330275229358,"1Note that FedFA [47], and FedFM [42] are unpublished works proposed concurrently with our work"
RELATED WORK,0.1614678899082569,"2
Related Work
86"
DATASET DISTILLATION,0.163302752293578,"2.1
Dataset Distillation
87"
DATASET DISTILLATION,0.1651376146788991,"Data distillation aims to improve data efﬁciency by distilling the most essential feature in a large-
88"
DATASET DISTILLATION,0.1669724770642202,"scale dataset (e.g., datasets comprising billions of data points) into a certain terse and high-ﬁdelity
89"
DATASET DISTILLATION,0.1688073394495413,"dataset. For example, Gradient Matching [46] is proposed to make the deep neural network produce
90"
DATASET DISTILLATION,0.1706422018348624,"similar gradients for both the terse synthetic images and the original large-scale dataset. Besides,
91"
DATASET DISTILLATION,0.1724770642201835,"[5] proposes matching the model training trajectory between real and synthetic data to guide the
92"
DATASET DISTILLATION,0.1743119266055046,"update for distillation. Another popular way of conducting data distillation is through Distribution
93"
DATASET DISTILLATION,0.1761467889908257,"Matching [45]. This strategy instead, attempts to match the distribution of the smaller synthetic
94"
DATASET DISTILLATION,0.1779816513761468,"dataset with the original large-scale dataset. It signiﬁcantly improves the distillation efﬁciency.
95"
DATASET DISTILLATION,0.1798165137614679,"Moreover, recent studies have justiﬁed that data distillation also preserves privacy [7, 4], which is
96"
DATASET DISTILLATION,0.181651376146789,"critical in federated learning. In practice, dataset distillation is used in healthcare for medical data
97"
DATASET DISTILLATION,0.1834862385321101,"sharing for privacy protection [22]. Other modern data distillation strategies can be found here [33].
98"
HETEROGENEOUS FEDERATED LEARNING,0.1853211009174312,"2.2
Heterogeneous Federated Learning
99"
HETEROGENEOUS FEDERATED LEARNING,0.1871559633027523,"FL performance downgrading on non-iid data is a critical challenge. A variety of FL algorithms have
100"
HETEROGENEOUS FEDERATED LEARNING,0.1889908256880734,"been proposed ranging from global aggregation to local optimization to handle this heterogeneous
101"
HETEROGENEOUS FEDERATED LEARNING,0.1908256880733945,"issue. Global aggregation improves the global model exchange process for better unitizing the
102"
HETEROGENEOUS FEDERATED LEARNING,0.1926605504587156,"updated client models to create a powerful server model. FedNova [38] notices an imbalance
103"
HETEROGENEOUS FEDERATED LEARNING,0.1944954128440367,"among different local models caused by different levels of training stage (e.g., certain clients train
104"
HETEROGENEOUS FEDERATED LEARNING,0.1963302752293578,"more epochs than others) and tackles such imbalance by normalizing and scaling the local updates
105"
HETEROGENEOUS FEDERATED LEARNING,0.1981651376146789,"accordingly. Meanwhile, FedAvgM [15] applies the momentum to server model aggregation to
106"
HETEROGENEOUS FEDERATED LEARNING,0.2,"stabilize the optimization. Furthermore, there are strategies to reﬁne the server model from learning
107"
HETEROGENEOUS FEDERATED LEARNING,0.2018348623853211,"client models such as FedDF [27] and FedFTG [43]. Local training optimization aims to explore the
108"
HETEROGENEOUS FEDERATED LEARNING,0.2036697247706422,"local objective to tackle the non-iid issue in FL system. FedProx [25] straightly adds L2 norm to
109"
HETEROGENEOUS FEDERATED LEARNING,0.20550458715596331,"regularize the client model and previous server model. Scaffold [18] adds the variance reduction term
110"
HETEROGENEOUS FEDERATED LEARNING,0.20733944954128442,"to mitigate the “clients-drift"". Also, MOON [24] brings mode-level contrastive learning to maximize
111"
HETEROGENEOUS FEDERATED LEARNING,0.20917431192660552,"the similarity between model representations to stable the local training. There is another line of
112"
HETEROGENEOUS FEDERATED LEARNING,0.21100917431192662,"works [42, 37] proposed to use a global anchor to regularize local training. Global anchor can be
113"
HETEROGENEOUS FEDERATED LEARNING,0.21284403669724772,"either a set of virtual global data or global virtual representations in feature space. However, in [37],
114"
HETEROGENEOUS FEDERATED LEARNING,0.21467889908256882,"the empirical global anchor selection may not be suitable for data from every distribution as they
115"
HETEROGENEOUS FEDERATED LEARNING,0.21651376146788992,"don’t update the anchor according to the training datasets.
116"
DATASETS DISTILLATION FOR FL,0.21834862385321102,"2.3
Datasets Distillation for FL
117"
DATASETS DISTILLATION FOR FL,0.22018348623853212,"Dataset distillation for FL is an emerging topic that has attracted attention due to its beneﬁt for
118"
DATASETS DISTILLATION FOR FL,0.22201834862385322,"efﬁcient FL systems. It trains model on distilled synthetic datasets, thus we refer it as federated
119"
DATASETS DISTILLATION FOR FL,0.22385321100917432,"virtual learning. It can help with FL synchronization and improve training efﬁciency by condensing
120"
DATASETS DISTILLATION FOR FL,0.22568807339449543,"every client’s data into a small set. To the best of our knowledge, there are few published works on
121"
DATASETS DISTILLATION FOR FL,0.22752293577981653,"distillation in FL. Concurrently with our work, some studies [10, 40, 16] distill datasets locally and
122"
DATASETS DISTILLATION FOR FL,0.22935779816513763,"share the distilled datasets with other clients/servers. Although privacy is protected against currently
123"
DATASETS DISTILLATION FOR FL,0.23119266055045873,"existing attack models, we consider sharing local distilled data a dangerous move. Furthermore, none
124"
DATASETS DISTILLATION FOR FL,0.23302752293577983,"of the existing work has addressed the heterogeneity issue.
125"
METHOD,0.23486238532110093,"3
Method
126"
METHOD,0.23669724770642203,"In this section, we will describe the problem setup, introduce the key technical contributions and
127"
METHOD,0.23853211009174313,"rationale of the design for FEDLGD, and explain the overall training pipeline.
128"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24036697247706423,"3.1
Setup for Federated Virtual Learning
129"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24220183486238533,"We start with describing the classical FL setting. Suppose there are N parties who own local datasets
130"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24403669724770644,"(D1, . . . , DN), and the goal of a classical FL system, such as FedAvg [29], is to train a global model
131"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24587155963302754,"Serve
r"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24770642201834864,Client N
SETUP FOR FEDERATED VIRTUAL LEARNING,0.24954128440366974,Client 1 …
SETUP FOR FEDERATED VIRTUAL LEARNING,0.25137614678899084,"Local-global distillation
Local-global model update !""! ""!
E """"!
E"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.25321100917431194,"C
Label"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.25504587155963304,"Dist. 
Match"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.25688073394495414,(c) Local virtual data update ###$ $!%& $!%&
SETUP FOR FEDERATED VIRTUAL LEARNING,0.25871559633027524,"If % &' (: !""! '"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26055045871559634,"Global
virtual"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26238532110091745,"E
C
Label"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26422018348623855,"#$()!
Grad. 
Match ∇*"""
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26605504587155965,"#!#+,
!""!"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26788990825688075,"', **"" $"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.26972477064220185,"(a) Global virtual data update #+, !""- ""! E"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.27155963302752295,"C
Label +#+./ #+, + !""! '
E
C"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.27339449541284405,(d) Local model update with virtual data
SETUP FOR FEDERATED VIRTUAL LEARNING,0.27522935779816515,(b) Global model update !!
SETUP FOR FEDERATED VIRTUAL LEARNING,0.27706422018348625,""" ←!!#$ ""
+ 1"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.27889908256880735,"% & ∆!! %! !""! ' **"" $
∆-! ""!"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28073394495412846,"Repeat: ., 0 ∆!!"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28256880733944956,%!= !!
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28440366972477066,%! −!!&$ %!
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28623853211009176,"Local
virtual"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28807339449541286,"Local
private"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.28990825688073396,Local virtual
SETUP FOR FEDERATED VIRTUAL LEARNING,0.29174311926605506,"Global virtual
#+,
If % &' (:"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.29357798165137616,"Repeat: c, 1"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.29541284403669726,"forward
gradient"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.29724770642201837,"update
E
C
feature
extractor"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.29908256880733947,classification head
SETUP FOR FEDERATED VIRTUAL LEARNING,0.30091743119266057,"server 
to client"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.30275229357798167,"client 
to server"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.30458715596330277,random
SETUP FOR FEDERATED VIRTUAL LEARNING,0.30642201834862387,"image
#
loss
function Else:"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.30825688073394497,Repeat: 0 Else:
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3100917431192661,Repeat: 1
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3119266055045872,"Figure 2: Overview pipeline for FEDLGD. We assume T FL rounds will be performed, among which we will
deﬁne the selected distillation rounds as ⌧2 [T] for local-global iteration. For selected rounds (t 2 ⌧), clients
will update local models (d) and reﬁne the local virtual data with the latest network parameters (c), while the
server uses aggregated gradients from cross-entropy loss (LCE) to update global virtual data (a) and update
the global model (b). We term this procedure Iterative Local-global Distillation. For the unselected rounds
(t 2 T\⌧), we perform ordinary FL pipeline on local virtual data with regularization loss (LCon) on global
virtual data."
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3137614678899083,with parameters ✓on the distributed datasets (D ⌘S
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3155963302752294,"i2[N] Di)). The objective function is written as:
132"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3174311926605505,"L(✓) = N
X i=1 |Di|"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3192660550458716,"|D| Li(✓),
(1)"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3211009174311927,"where Li(w) is the empirical loss of client i.
133"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3229357798165138,"In practice, different clients in FL may have variant amounts of training samples, leading to asynchro-
134"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3247706422018349,"nized updates. In this work, we focus on a new type of FL training method – federated virtual learning,
135"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.326605504587156,"that trains on distilled datasets for efﬁciency and synchronization (discussed in Sec.2.3.) Federated
136"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3284403669724771,virtual learning synthesizes local virtual data ˜Di for client i for i 2 [N] and form ˜D ⌘S
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3302752293577982,"i2[N] ˜Di.
137"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3321100917431193,"Typically, | ˜Di| ⌧|Di| and | ˜Di| = | ˜Dj|. A basic setup for federated virtual learning is to replace Di
138"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3339449541284404,"with ˜Di in Eq (1), namely FL model is trained on the virtual datasets. As suggested in FedDM [40],
139"
SETUP FOR FEDERATED VIRTUAL LEARNING,0.3357798165137615,"the clients should not share gradients w.r.t. the original data for privacy concern.
140"
OVERALL PIPELINE,0.3376146788990826,"3.2
Overall Pipeline
141"
OVERALL PIPELINE,0.3394495412844037,"The overall pipeline of our proposed method contains three stages, including 1) initialization, 2)
142"
OVERALL PIPELINE,0.3412844036697248,"iterative local-global distillation, and 3) federated virtual learning. We depict the overview of
143"
OVERALL PIPELINE,0.3431192660550459,"FEDLGD pipeline in Figure 2. However, FL is inevitability affected by several challenges, including
144"
OVERALL PIPELINE,0.344954128440367,"synchronization, efﬁciency, privacy, and heterogeneity. Speciﬁcally, we outline FEDLGD as follows:
145"
OVERALL PIPELINE,0.3467889908256881,"We begin with the initialization of the clients’ local virtual data ˜Dc by performing initial rounds of
146"
OVERALL PIPELINE,0.3486238532110092,"distribution matching (DM) [45]. Meanwhile, the server will initialize global virtual data ˜Dg and
147"
OVERALL PIPELINE,0.3504587155963303,network parameters ✓g
OVERALL PIPELINE,0.3522935779816514,"0. In this stage, we generate the same amount of class-balanced virtual data for
148"
OVERALL PIPELINE,0.3541284403669725,"each client and server.
149"
OVERALL PIPELINE,0.3559633027522936,"Then, we will reﬁne our local and global virtual data using our proposed local-global distillation
150"
OVERALL PIPELINE,0.3577981651376147,"strategies in Sec. 3.3.1 and 3.3.2. This step is performed for a few selected iterations (e.g. ⌧=
151"
OVERALL PIPELINE,0.3596330275229358,"{0, 5, 10}) to update ✓using LCE (Eq 3), ˜Dg using LDist (Eq 5), and ˜Dc using LMMD (Eq 2) in early
152"
OVERALL PIPELINE,0.3614678899082569,"training epochs. For each selected iterations, the server and clients will update their virtual data for a
153"
OVERALL PIPELINE,0.363302752293578,"few distillation steps.
154"
OVERALL PIPELINE,0.3651376146788991,"Finally, after reﬁning local and global virtual data ˜Dg and ˜Dc, we continue federated virtual learning
155"
OVERALL PIPELINE,0.3669724770642202,"in stage 3 on local virtual data ˜Dc using Ltotal (Eq 3), with ˜Dg as regularization anchor to calculate
156"
OVERALL PIPELINE,0.3688073394495413,"LCon (Eq. 4). We provide implementation details, an algorithm box, and an anonymous link to our
157"
OVERALL PIPELINE,0.3706422018348624,"code in the Appendix.
158"
FL WITH LOCAL-GLOBAL DATASET DISTILLATION,0.3724770642201835,"3.3
FL with Local-Global Dataset Distillation
159"
LOCAL DATA DISTILLATION,0.3743119266055046,"3.3.1
Local Data Distillation
160"
LOCAL DATA DISTILLATION,0.3761467889908257,"Our purpose is to decrease the number of local data to achieve efﬁcient training to meet the following
161"
LOCAL DATA DISTILLATION,0.3779816513761468,"goals. First of all, we hope to synthesize virtual data conditional on class labels to achieve class-
162"
LOCAL DATA DISTILLATION,0.3798165137614679,"balanced virtual datasets. Second, we hope to distill local data that is best suited for the classiﬁcation
163"
LOCAL DATA DISTILLATION,0.381651376146789,"task. Last but not least, the process should be efﬁcient due to the limited computational resource
164"
LOCAL DATA DISTILLATION,0.3834862385321101,"locally. To this end, we design Iterative Distribution Matching to fulﬁll our purpose.
165"
LOCAL DATA DISTILLATION,0.3853211009174312,"Iterative distribution matching. We aim to gradually improve distillation quality during FL training.
166"
LOCAL DATA DISTILLATION,0.3871559633027523,"To begin with, we split a model into two parts, feature extractor  (shown as E in ﬁgure 2) and
167"
LOCAL DATA DISTILLATION,0.3889908256880734,"classiﬁcation head h (shown as C in ﬁgure 2). The whole classiﬁcation model is deﬁned as f ✓= h◦ .
168"
LOCAL DATA DISTILLATION,0.3908256880733945,"The high-level idea of distribution matching can be described as follows. Given a feature extractor
169"
LOCAL DATA DISTILLATION,0.3926605504587156,": Rd ! Rd0 , we want to generate ˜D so that P (D) ⇡P ( ˜D) where P is the distribution in
170"
LOCAL DATA DISTILLATION,0.3944954128440367,"feature space. To distill local data during FL efﬁciently that best ﬁts our task, we intend to use
171"
LOCAL DATA DISTILLATION,0.3963302752293578,"the up-to-date server model’s feature extractor as our kernel function to distill better virtual data.
172"
LOCAL DATA DISTILLATION,0.3981651376146789,"Since we can’t obtain ground truth distribution of local data, we utilize empirical maximum mean
173"
LOCAL DATA DISTILLATION,0.4,"discrepancy (MMD) [11] as our loss function for local virtual distillation:
174"
LOCAL DATA DISTILLATION,0.4018348623853211,"LMMD = K
X k"
LOCAL DATA DISTILLATION,0.4036697247706422,"||
1
|Dc k| |Dc k|
X i=1"
LOCAL DATA DISTILLATION,0.4055045871559633,"t(xi)−
1
| ˜Dc,t k |"
LOCAL DATA DISTILLATION,0.4073394495412844,"| ˜
Dc,t k |
X j=1 t(˜xt"
LOCAL DATA DISTILLATION,0.4091743119266055,"j)||2,
(2)"
LOCAL DATA DISTILLATION,0.41100917431192663,"where  t and ˜Dc,t are the server feature extractor and local virtual data from the latest global iteration
175"
LOCAL DATA DISTILLATION,0.41284403669724773,"t. Following [46, 45], we apply the differentiable Siamese augmentation on virtual data ˜Dc. K is the
176"
LOCAL DATA DISTILLATION,0.41467889908256883,"total number of classes, and we sum over MMD loss calculated per class k 2 [K]. In such a way, we
177"
LOCAL DATA DISTILLATION,0.41651376146788993,"can generate balanced local virtual data by optimizing the same number of virtual data per class.
178"
LOCAL DATA DISTILLATION,0.41834862385321103,"Although such an efﬁcient distillation strategy is inspired by DM [45], we highlight the key difference
179"
LOCAL DATA DISTILLATION,0.42018348623853213,"that DM uses randomly initialized deep neural networks to extract features, whereas we use trained
180"
LOCAL DATA DISTILLATION,0.42201834862385323,"FL models with task-speciﬁc supervised loss. We believe iterative updating on the clients’ data using
181"
LOCAL DATA DISTILLATION,0.42385321100917434,"the up-to-date network parameters can generate better task-speciﬁc local virtual data. Our intuition
182"
LOCAL DATA DISTILLATION,0.42568807339449544,"comes from the recent success of the empirical neural tangent kernel for data distribution learning and
183"
LOCAL DATA DISTILLATION,0.42752293577981654,"matching [30, 8]. Especially, the feature extractor of the model trained with FEDLGD could obtain
184"
LOCAL DATA DISTILLATION,0.42935779816513764,"feature information from other clients, which further harmonize the domain shift between clients.
185"
LOCAL DATA DISTILLATION,0.43119266055045874,"We apply DM [45] to the baseline FL methods and demonstrate the effectiveness of our proposed
186"
LOCAL DATA DISTILLATION,0.43302752293577984,"iterative strategy in Sec. 4. Furthermore, note that FEDLGD only requires a few hundreds of local
187"
LOCAL DATA DISTILLATION,0.43486238532110094,"distillations steps using the local model’s feature distribution, which is more computationally efﬁcient
188"
LOCAL DATA DISTILLATION,0.43669724770642204,"than other bi-level dataset distillation methods [46, 5].
189"
LOCAL DATA DISTILLATION,0.43853211009174314,"Harmonizing local heterogeneity with global anchors. Data collected in different sites may have
190"
LOCAL DATA DISTILLATION,0.44036697247706424,"different distributions due to different collecting protocols and populations. Such heterogeneity will
191"
LOCAL DATA DISTILLATION,0.44220183486238535,"degrade the performance of FL. Worse yet, we found increased data heterogeneity among clients
192"
LOCAL DATA DISTILLATION,0.44403669724770645,"when federatively training with distilled local virtual data (see Figure 1). We aim to alleviate the
193"
LOCAL DATA DISTILLATION,0.44587155963302755,"dataset shift by adding a regularization term in feature space to our total loss function for local model
194"
LOCAL DATA DISTILLATION,0.44770642201834865,"updating, which is inspired by [37, 20]:
195"
LOCAL DATA DISTILLATION,0.44954128440366975,"Ltotal = LCE( ˜Dg, ˜Dc; ✓) + λLCon( ˜Dg, ˜Dc),
(3)"
LOCAL DATA DISTILLATION,0.45137614678899085,"and
196"
LOCAL DATA DISTILLATION,0.45321100917431195,LCon = X i2I
LOCAL DATA DISTILLATION,0.45504587155963305,"−1
|P(i)| X"
LOCAL DATA DISTILLATION,0.45688073394495415,p2P (i)
LOCAL DATA DISTILLATION,0.45871559633027525,"log
exp(zg · zp/⌧temp)
P"
LOCAL DATA DISTILLATION,0.46055045871559636,"a2A(i) exp(zg · za/⌧temp),
(4)"
LOCAL DATA DISTILLATION,0.46238532110091746,"where LCE is the cross-entropy measured on the virtual data, and LCon is the supervised contrastive
197"
LOCAL DATA DISTILLATION,0.46422018348623856,"loss where I is the collection of all indices, A(i) indicates all the local and global virtual data indices
198"
LOCAL DATA DISTILLATION,0.46605504587155966,"without i (i.e. A(i) ⌘I\{i}), z =  (x) is the output of feature extractor, P(i) represents the set of
199"
LOCAL DATA DISTILLATION,0.46788990825688076,"images belonging to the same class yi without data i, and ⌧temp is a scalar temperature parameter. In
200"
LOCAL DATA DISTILLATION,0.46972477064220186,"such a way, global virtual data can be served for calibration, where zg is from ˜Dg as an anchor, and
201"
LOCAL DATA DISTILLATION,0.47155963302752296,"zp and za are from ˜Dc. At this point, a critical problem arises: What global virtual data shall we use?
202"
GLOBAL DATA DISTILLATION,0.47339449541284406,"3.3.2
Global Data Distillation
203"
GLOBAL DATA DISTILLATION,0.47522935779816516,"Here, we provide an afﬁrmative solution to the question of generating global virtual data that can
204"
GLOBAL DATA DISTILLATION,0.47706422018348627,"be naturally incorporated into FL pipeline. Although distribution-based matching is efﬁcient, local
205"
GLOBAL DATA DISTILLATION,0.47889908256880737,"clients may not share their features due to privacy concerns. Therefore, we propose to leverage local
206"
GLOBAL DATA DISTILLATION,0.48073394495412847,"clients’ averaged gradients to distill global virtual data and utilize it in Eq. (4). We term our global
207"
GLOBAL DATA DISTILLATION,0.48256880733944957,"data distillation method as Federated Gradient Matching.
208"
GLOBAL DATA DISTILLATION,0.48440366972477067,"Federated gradient matching. The concept of gradient-based dataset distillation is to minimize the
209"
GLOBAL DATA DISTILLATION,0.48623853211009177,"distance between gradients from model parameters trained by original data and distilled data. It is
210"
GLOBAL DATA DISTILLATION,0.48807339449541287,"usually considered as a learning-to-learn problem because the procedure consists of model updates
211"
GLOBAL DATA DISTILLATION,0.48990825688073397,"and distilled data updates. Zhao et al. [46] studies gradient matching in the centralized setting via
212"
GLOBAL DATA DISTILLATION,0.4917431192660551,"bi-level optimization that iteratively optimizes the virtual data and model parameters. However, the
213"
GLOBAL DATA DISTILLATION,0.4935779816513762,"implementation in [46] is not appropriate for our speciﬁc context because there are two fundamental
214"
GLOBAL DATA DISTILLATION,0.4954128440366973,"differences in our settings: 1) for model updating, the gradient-distilled dataset is on the server and
215"
GLOBAL DATA DISTILLATION,0.4972477064220184,"will not directly optimize the targeted task; 2) for virtual data update, the ‘optimal’ model comes
216"
GLOBAL DATA DISTILLATION,0.4990825688073395,"from the optimized local model aggregation. These two steps can naturally be embedded in local
217"
GLOBAL DATA DISTILLATION,0.5009174311926605,"model updating and global virtual data distillation from the aggregated local gradients. First, we
218"
GLOBAL DATA DISTILLATION,0.5027522935779817,"utilize the distance loss LDist [46] for gradient matching:
219"
GLOBAL DATA DISTILLATION,0.5045871559633027,LDist = Dist(5✓L
GLOBAL DATA DISTILLATION,0.5064220183486239,"˜
Dg
CE(✓), 5✓L"
GLOBAL DATA DISTILLATION,0.5082568807339449,"˜
Dc
CE(✓))
(5)"
GLOBAL DATA DISTILLATION,0.5100917431192661,"where ˜Dc and ˜Dg denote local and global virtual data, 5✓L ˜
Dc
CE is the average client gradient. Then,
220"
GLOBAL DATA DISTILLATION,0.5119266055045871,"our proposed federated gradient matching optimize as follows:
221 min"
GLOBAL DATA DISTILLATION,0.5137614678899083,"Dg LDist(✓)
subject to
✓= 1 N ✓ci ⇤,"
GLOBAL DATA DISTILLATION,0.5155963302752293,where ✓ci
GLOBAL DATA DISTILLATION,0.5174311926605505,"⇤= arg min✓Li( ˜Dc) is the optimal local model weights of client i at a certain round t.
222"
GLOBAL DATA DISTILLATION,0.5192660550458715,"Noting that compared with FedAvg [29], there is no additional client information shared for global
223"
GLOBAL DATA DISTILLATION,0.5211009174311927,"distillation. We also note the approach seems similar to the gradient inversion attack [49] but we
224"
GLOBAL DATA DISTILLATION,0.5229357798165137,"consider averaged gradients w.r.t. local virtual data, and the method potentially defenses inference
225"
GLOBAL DATA DISTILLATION,0.5247706422018349,"attack better (Appendix D.8), which is also implied by [40, 7]. Privacy preservation can be further
226"
GLOBAL DATA DISTILLATION,0.5266055045871559,"improved by employing differential privacy [1], but this is not the main focus of our work.
227"
EXPERIMENT,0.5284403669724771,"4
Experiment
228"
EXPERIMENT,0.5302752293577981,"To evaluate FEDLGD, we consider the FL setting in which clients obtain data from different domains
229"
EXPERIMENT,0.5321100917431193,"while performing the same task. Speciﬁcally, we compare with multiple baselines on benchmark
230"
EXPERIMENT,0.5339449541284403,"datasets DIGITS (Sec. 4.2), where each client has data from completely different open-sourced
231"
EXPERIMENT,0.5357798165137615,"datasets. The experiment is designed to show that FEDLGD can effectively mitigate large domain
232"
EXPERIMENT,0.5376146788990825,"shifts. Additionally, we evaluate the performance of FEDLGD on another benchmark dataset,
233"
EXPERIMENT,0.5394495412844037,"CIFAR10C [14], which collects data from different corrupts yielding data distribution shift and
234"
EXPERIMENT,0.5412844036697247,"contains a large number of clients, so that we can investigate varied client sampling in FL. The
235"
EXPERIMENT,0.5431192660550459,"experiment aims to show FEDLGD’s feasibility on large-scale FL environments. We also validate the
236"
EXPERIMENT,0.544954128440367,"performance under medical datasets, RETINA, in Appendix. B.
237"
TRAINING AND EVALUATION SETUP,0.5467889908256881,"4.1
Training and Evaluation Setup
238"
TRAINING AND EVALUATION SETUP,0.5486238532110091,"Model architecture. We conduct the ablation study to explore the effect of different deep neural
239"
TRAINING AND EVALUATION SETUP,0.5504587155963303,"networks’ performance under FEDLGD. Speciﬁcally, we adapt ResNet18 [13] and ConvNet [46]
240"
TRAINING AND EVALUATION SETUP,0.5522935779816514,"in our study. To achieve the optimal performance, we apply the same architecture to perform both
241"
TRAINING AND EVALUATION SETUP,0.5541284403669725,"the local distillation task and the classiﬁcation task, as this combination is justiﬁed to have the best
242"
TRAINING AND EVALUATION SETUP,0.5559633027522936,"output [46, 45]. The detailed model architectures are presented in Appendix D.4.
243"
TRAINING AND EVALUATION SETUP,0.5577981651376147,"Comparison methods. We compare the performance of downsteam classiﬁcation tasks using state-of-
244"
TRAINING AND EVALUATION SETUP,0.5596330275229358,"the-art (SOTA) FL algorithms, FedAvg [29], FedProx [26], FedNova [38], Scaffold [18], MOON [24],
245"
TRAINING AND EVALUATION SETUP,0.5614678899082569,"Table 1: Test accuracy for DIGITS under different images per class (IPC) and model architectures. R and C
stand for ResNet18 and ConvNet, respectively, and we set IPC to 10 and 50. Threre are ﬁve clients (MNIST,
SVHN, USPS, SynthDigits, and MNIST-M) containing data from different domains. ‘Average’ is the unweighted
test accuracy average of all the clients. The best performance under different models is highlighted using bold.
The best results on ConvNet are marked in red and in black for ResNet18."
TRAINING AND EVALUATION SETUP,0.563302752293578,"DIGITS
MNIST
SVHN
USPS
SynthDigits
MNIST-M
Average
IPC
10
50
10
50
10
50
10
50
10
50
10
50"
TRAINING AND EVALUATION SETUP,0.5651376146788991,"FedAvg
R
73.0
92.5
20.5
48.9
83.0
89.7
13.6
28.0
37.8
72.3
45.6
66.3
C
94.0
96.1
65.9
71.7
91.0
92.9
55.5
69.1
73.2
83.3
75.9
82.6"
TRAINING AND EVALUATION SETUP,0.5669724770642202,"FedProx
R
72.6
92.5
19.7
48.4
81.5
90.1
13.2
27.9
37.3
67.9
44.8
65.3
C
93.9
96.1
66.0
71.5
90.9
92.9
55.4
69.0
73.7
83.3
76.0
82.5"
TRAINING AND EVALUATION SETUP,0.5688073394495413,"FedNova
R
75.5
92.3
17.3
50.6
80.3
90.1
11.4
30.5
38.3
67.9
44.6
66.3
C
94.2
96.2
65.5
73.1
90.6
93.0
56.2
69.1
74.6
83.7
76.2
83.0"
TRAINING AND EVALUATION SETUP,0.5706422018348624,"Scaffold
R
75.8
93.4
16.4
53.8
79.3
91.3
11.2
34.2
38.3
70.8
44.2
68.7
C
94.1
96.3
64.9
73.3
90.6
93.4
56.0
70.1
74.6
84.7
76.0
83.6"
TRAINING AND EVALUATION SETUP,0.5724770642201835,"MOON
R
15.5
80.4
15.9
14.2
25.0
82.4
10.0
11.5
11.0
35.4
15.5
44.8
C
85.0
95.5
49.2
70.5
83.4
92.0
31.5
67.2
56.9
82.3
61.2
81.5"
TRAINING AND EVALUATION SETUP,0.5743119266055046,"VHL
R
87.8
95.9
29.5
67.0
88.0
93.5
18.2
60.7
52.2
85.7
55.1
80.5
C
95.0
96.9
68.6
75.2
92.2
94.4
60.7
72.3
76.1
83.7
78.5
84.5
FEDLGD
R
92.9
96.7
46.9
73.3
89.1
93.9
27.9
72.9
70.8
85.2
65.5
84.4
C
95.8
97.1
68.2
77.3
92.4
94.6
67.4
78.5
79.4
86.1
80.6
86.7"
TRAINING AND EVALUATION SETUP,0.5761467889908257,"and VHL [37]2. We directly use local virtual data from our initialization stage for FL methods other
246"
TRAINING AND EVALUATION SETUP,0.5779816513761468,"than ours. We perform classiﬁcation on client’s testing set and report the test accuracies.
247"
TRAINING AND EVALUATION SETUP,0.5798165137614679,"FL training setup. We use the SGD optimizer with a learning rate of 10−2 for DIGITS and CIFAR10C.
248"
TRAINING AND EVALUATION SETUP,0.581651376146789,"If not speciﬁed, our default setting for local model update epochs is 1, total update rounds is 100,
249"
TRAINING AND EVALUATION SETUP,0.5834862385321101,"the batch size for local training is 32, and the number of virtual data update iterations (|⌧|) is 10.
250"
TRAINING AND EVALUATION SETUP,0.5853211009174312,"The numbers of default virtual data distillation steps for clients and server are set to 100 and 500,
251"
TRAINING AND EVALUATION SETUP,0.5871559633027523,"respectively. Since we only have a few clients for DIGITS and RETINA experiments, we will select all
252"
TRAINING AND EVALUATION SETUP,0.5889908256880734,"the clients for each iteration, while the client selection for CIFAR10C experiments will be speciﬁed in
253"
TRAINING AND EVALUATION SETUP,0.5908256880733945,"Sec. 4.3. The experiments are run on NVIDIA GeForce RTX 3090 Graphics cards with PyTorch.
254"
TRAINING AND EVALUATION SETUP,0.5926605504587156,"Proper Initialization for Distillation. We propose to initialize the distilled data using statistics
255"
TRAINING AND EVALUATION SETUP,0.5944954128440367,"from local data to take care of both privacy concerns and model performance. Speciﬁcally, each
256"
TRAINING AND EVALUATION SETUP,0.5963302752293578,"client calculates the statistics of its own data for each class, denoted as µc i, σc"
TRAINING AND EVALUATION SETUP,0.5981651376146789,"i , and then initializes the
257"
TRAINING AND EVALUATION SETUP,0.6,"distillation images per class, x ⇠N(µc i, σc"
TRAINING AND EVALUATION SETUP,0.6018348623853211,"i ), where c and i represent each client and categorical label.
258"
TRAINING AND EVALUATION SETUP,0.6036697247706422,The server only needs to aggregate the statistics and initializes the virtual data as x ⇠N(µg
TRAINING AND EVALUATION SETUP,0.6055045871559633,"i , σg"
TRAINING AND EVALUATION SETUP,0.6073394495412844,"i ). In
259"
TRAINING AND EVALUATION SETUP,0.6091743119266055,"this way, no real data is shared with any participant in the FL system. The comparison results using
260"
TRAINING AND EVALUATION SETUP,0.6110091743119266,"different initialization methods proposed in previous works [46, 45] can be found in Appendix C.
261"
DIGITS EXPERIMENT,0.6128440366972477,"4.2
DIGITS Experiment
262"
DIGITS EXPERIMENT,0.6146788990825688,"Datasets. We use the following datasets for our benchmark experiments: DIGITS = {MNIST [21],
263"
DIGITS EXPERIMENT,0.6165137614678899,"SVHN [31], USPS [17], SynthDigits [9], MNIST-M [9]}. Each dataset in DIGITS contains hand-
264"
DIGITS EXPERIMENT,0.618348623853211,"written, real street and synthetic digit images of 0, 1, · · · , 9. As a result, we have 5 clients in the
265"
DIGITS EXPERIMENT,0.6201834862385321,"experiments, and image size is 28 ⇥28.
266"
DIGITS EXPERIMENT,0.6220183486238532,"Comparison with baselines under various conditions. To validate the effectiveness of FEDLGD,
267"
DIGITS EXPERIMENT,0.6238532110091743,"we ﬁrst compare it with the alternative FL methods varying on two important factors: Image-per-class
268"
DIGITS EXPERIMENT,0.6256880733944954,"(IPC) and different deep neural network architectures (arch). We use IPC 2 {10, 50} and arch 2
269"
DIGITS EXPERIMENT,0.6275229357798165,"{ ResNet18(R), ConvNet(C)} to examine the performance of SOTA models and FEDLGD using
270"
DIGITS EXPERIMENT,0.6293577981651376,"distilled DIGITS. Note that we ﬁx IPC = 10 for global virtual data and vary IPC for local virtual data.
271"
DIGITS EXPERIMENT,0.6311926605504588,"Table 1 shows the test accuracies of DIGITS experiments. In addition to testing with original test sets,
272"
DIGITS EXPERIMENT,0.6330275229357798,"we also show the unweighted averaged test accuracy. One can observe that for each FL algorithm,
273"
DIGITS EXPERIMENT,0.634862385321101,"ConvNet(C) always has the best performance under all IPCs. The observation is consistent with [45]
274"
DIGITS EXPERIMENT,0.636697247706422,"as more complex architectures may cause over-ﬁtting in training virtual data. It is also shown that
275"
DIGITS EXPERIMENT,0.6385321100917432,"using IPC = 50 always outperforms IPC = 10 as expected since more data are available for training.
276"
DIGITS EXPERIMENT,0.6403669724770642,"Overall, FEDLGD outperforms other SOTA methods, where on average accuracy, FEDLGD increases
277"
DIGITS EXPERIMENT,0.6422018348623854,"the best test accuracy results among the baseline methods of 2.1% (IPC =10, arch = C), 10.4% (IPC
278"
DIGITS EXPERIMENT,0.6440366972477064,2The detailed information of the methods can be found in Appendix E.
DIGITS EXPERIMENT,0.6458715596330276,Table 2: Averaged test accuracy for CIFAR10C with ConvNet.
DIGITS EXPERIMENT,0.6477064220183486,"CIFAR10C
FedAvg
FedProx
FedNova
Scaffold
MOON
VHL
FEDLGD
IPC
10
50
10
50
10
50
10
50
10
50
10
50
10
50"
DIGITS EXPERIMENT,0.6495412844036698,Client ratio
DIGITS EXPERIMENT,0.6513761467889908,"0.2
27.0
44.9
27.0
44.9
26.7
34.1
27.0
44.9
20.5
31.3
21.8
45.0
32.9
46.8
0.5
29.8
51.4
29.8
51.4
29.6
45.9
30.6
51.6
23.8
43.2
29.3
51.7
39.5
52.8
1
33.0
54.9
33.0
54.9
30.0
53.2
33.8
54.5
26.4
51.6
34.4
55.2
47.6
57.4"
DIGITS EXPERIMENT,0.653211009174312,"(a) Comparison of Reg. loss
(b) Vary |⌧|
(c) Vary steps
(d) Vary steps"
DIGITS EXPERIMENT,0.655045871559633,"Figure 3: (a) Comparison between different regularization losses and their weighting in total loss
(λ). One can observe that supervised contrastive loss gives us better and more stable performance
with different coefﬁcient choices. (b) The trade-off between |⌧| and computation cost. One can
observe that the model performance improves with the increasing |⌧|, which is a trade-off between
computation cost and model performance. Vary data updating steps for (c) DIGITS and (d) CIFAR10C.
One can observe that FEDLGD yields consistent performance, and the accuracy tends to improve
with an increasing number of local and global steps."
DIGITS EXPERIMENT,0.6568807339449542,"=10, arch = R), 2.2% (IPC = 50, arch = C) and 3.9% (IPC =50, arch = R). VHL [37] is the closest
279"
DIGITS EXPERIMENT,0.6587155963302752,"strategy to FEDLGD and achieves the best performance among the baseline methods, indicating that
280"
DIGITS EXPERIMENT,0.6605504587155964,"the feature alignment solutions are promising for handling heterogeneity in federated virtual learning.
281"
DIGITS EXPERIMENT,0.6623853211009174,"However, VHL is still worse than FEDLGD, and the performance may result from the differences in
282"
DIGITS EXPERIMENT,0.6642201834862386,"synthesizing global virtual data. VHL [37] uses untrained StyleGAN [19] to generate global virtual
283"
DIGITS EXPERIMENT,0.6660550458715596,"data without further updating. On the contrary, we update our global virtual data during FL training.
284"
DIGITS EXPERIMENT,0.6678899082568808,"4.3
CIFAR10C Experiment
285"
DIGITS EXPERIMENT,0.6697247706422018,"Datasets. We conduct real-world FL experiments on CIFAR10C3, where, like previous studies [24],
286"
DIGITS EXPERIMENT,0.671559633027523,"we apply Dirichlet distribution with ↵= 0.5 to generate 3 partitions on each distorted Cifar10-C [14],
287"
DIGITS EXPERIMENT,0.673394495412844,"resulting in 57 clients each with class imbalanced non-IID datasets. In addition, we apply random
288"
DIGITS EXPERIMENT,0.6752293577981652,"client selection with ratio = 0.2, 0.5, and 1 and set image size as 28 ⇥28.
289"
DIGITS EXPERIMENT,0.6770642201834862,"Comparison with baselines under different client sampling ratios. The objective of the experiment
290"
DIGITS EXPERIMENT,0.6788990825688074,"is to test FEDLGD under popular FL questions: class imbalance, large number of clients, different
291"
DIGITS EXPERIMENT,0.6807339449541284,"client sample ratios, and data heterogeneity. One beneﬁt of federated virtual learning is that we can
292"
DIGITS EXPERIMENT,0.6825688073394496,"easily handle class imbalance by distilling the same number (IPC) of virtual data. We will vary IPC
293"
DIGITS EXPERIMENT,0.6844036697247706,"and ﬁx the model architecture to ConvNet since it is validated that ConvNet yields better performance
294"
DIGITS EXPERIMENT,0.6862385321100918,"in virtual training [46, 45]. One can observe from Table 2 that FEDLGD consistently achieves the
295"
DIGITS EXPERIMENT,0.6880733944954128,"best performance under different IPC and client sampling ratios. We would like to point out that
296"
DIGITS EXPERIMENT,0.689908256880734,"when IPC=10, the performance boosts are signiﬁcant, which indicates that FEDLGD is well-suited
297"
DIGITS EXPERIMENT,0.691743119266055,"for FL conditions when there is a large group of clients and each of them has a limited number of
298"
DIGITS EXPERIMENT,0.6935779816513762,"data.
299"
ABLATION STUDIES FOR FEDLGD,0.6954128440366972,"4.4
Ablation studies for FEDLGD
300"
ABLATION STUDIES FOR FEDLGD,0.6972477064220184,"The success of FEDLGD relies on the novel design of local-global data distillation, where the
301"
ABLATION STUDIES FOR FEDLGD,0.6990825688073394,"selection of regularization loss and the number of iterations for data distillation plays a key role. In
302"
ABLATION STUDIES FOR FEDLGD,0.7009174311926606,"this section, we study the choice of regularization loss and its weighting (λ) in the total loss function.
303"
ABLATION STUDIES FOR FEDLGD,0.7027522935779816,"Recall that among the total FL training epochs, we perform local-global distillation on the selected
304"
ABLATION STUDIES FOR FEDLGD,0.7045871559633028,"⌧iterations, and within each selected iteration, the server and clients will perform data updating
305"
ABLATION STUDIES FOR FEDLGD,0.7064220183486238,3Cifar10-C is a collection of augmented Cifar10 that applies 19 different corruptions.
ABLATION STUDIES FOR FEDLGD,0.708256880733945,"Figure 4: tSNE plots on feature space for FedAvg, FEDLGD without regularization, and FEDLGD. One can
observe regularizing training with our global virtual data can rectify feature shift among different clients."
ABLATION STUDIES FOR FEDLGD,0.710091743119266,"for some pre-deﬁned steps. The effect of local-global distillation iterations and data updating steps
306"
ABLATION STUDIES FOR FEDLGD,0.7119266055045872,"will also be discussed. We also perform additional ablation studies such as computation cost and
307"
ABLATION STUDIES FOR FEDLGD,0.7137614678899082,"communication overhead in Appendix C.
308"
ABLATION STUDIES FOR FEDLGD,0.7155963302752294,"Effect of regularization loss. FEDLGD uses supervised contrastive loss LCon as a regularization
309"
ABLATION STUDIES FOR FEDLGD,0.7174311926605504,"term to encourage local and global virtual data embedding into a similar feature space. To demonstrate
310"
ABLATION STUDIES FOR FEDLGD,0.7192660550458716,"the effectiveness of the regularization term in FEDLGD, we perform ablation studies to replace LCon
311"
ABLATION STUDIES FOR FEDLGD,0.7211009174311926,"with an alternative distribution similarity measurement, MMD loss, with different λ’s ranging from
312"
ABLATION STUDIES FOR FEDLGD,0.7229357798165138,"0.1 to 20. Figure 3a shows the average test accuracy. Using supervised contrastive loss gives us better
313"
ABLATION STUDIES FOR FEDLGD,0.7247706422018348,"and more stable performance with different coefﬁcient choices.
314"
ABLATION STUDIES FOR FEDLGD,0.726605504587156,"To explain the effect of our proposed regularization loss on feature representations, we embed the
315"
ABLATION STUDIES FOR FEDLGD,0.728440366972477,"latent features before fully-connected layers to a 2D space using tSNE [28] shown in Figure 4. For
316"
ABLATION STUDIES FOR FEDLGD,0.7302752293577982,"the model trained with FedAvg (Figure 4(a)), features from two clients (⇥and ◦) are closer to their
317"
ABLATION STUDIES FOR FEDLGD,0.7321100917431193,"own distribution regardless of the labels (colors). In Figure 4(b), we perform virtual FL training but
318"
ABLATION STUDIES FOR FEDLGD,0.7339449541284404,"without the regularization term (Eq. 4). Figure 4(c) shows FEDLGD, and one can observe that data
319"
ABLATION STUDIES FOR FEDLGD,0.7357798165137615,"from different clients with the same label are grouped together. This indicates that our regularization
320"
ABLATION STUDIES FOR FEDLGD,0.7376146788990826,"with global virtual data is useful for learning homogeneous feature representations.
321"
ABLATION STUDIES FOR FEDLGD,0.7394495412844037,"Analysis of distillation iterations (|⌧|). Figure 3b shows the improved averaged test accuracy if
322"
ABLATION STUDIES FOR FEDLGD,0.7412844036697248,"we increase the number of distillation iterations with FEDLGD. The base accuracy for DIGITS and
323"
ABLATION STUDIES FOR FEDLGD,0.7431192660550459,"CIFAR10C are 85.8 and 55.2, respectively. We ﬁx local and global update steps to 100 and 500,
324"
ABLATION STUDIES FOR FEDLGD,0.744954128440367,"and the selected iterations (⌧) are deﬁned as arithmetic sequences with d = 5 (i.e., ⌧= {0, 5, ...}).
325"
ABLATION STUDIES FOR FEDLGD,0.7467889908256881,"One can observe that the model performance improves with the increasing |⌧|. This is because we
326"
ABLATION STUDIES FOR FEDLGD,0.7486238532110092,"obtain better virtual data with more local-global distillation iterations, which is a trade-off between
327"
ABLATION STUDIES FOR FEDLGD,0.7504587155963303,"computation cost and model performance. We select |⌧| = 10 for efﬁciency trade-off.
328"
ABLATION STUDIES FOR FEDLGD,0.7522935779816514,"Robustness on virtual data update steps. In Figure 3c and Figure 3d, we ﬁx |⌧| = 10, and vary
329"
ABLATION STUDIES FOR FEDLGD,0.7541284403669725,"(local, global) data updating steps. One can observe that FEDLGD yields stable performance, and the
330"
ABLATION STUDIES FOR FEDLGD,0.7559633027522936,"accuracy slightly improves with an increasing number of local and global steps. Nevertheless, the
331"
ABLATION STUDIES FOR FEDLGD,0.7577981651376147,"results are all the best when comparing with the baselines. It is also worth noting that there is still
332"
ABLATION STUDIES FOR FEDLGD,0.7596330275229358,"trade-off between steps and computation cost (See Appendix).
333"
CONCLUSION,0.7614678899082569,"5
Conclusion
334"
CONCLUSION,0.763302752293578,"In this paper, we introduce a new approach for FL, called FEDLGD. It utilizes virtual data on both
335"
CONCLUSION,0.7651376146788991,"client and server sides to train FL models. We are the ﬁrst to reveal that FL on local virtual data
336"
CONCLUSION,0.7669724770642202,"can increase heterogeneity. Furthermore, we propose iterative distribution matching and federated
337"
CONCLUSION,0.7688073394495413,"gradient matching to iteratively update local and global virtual data, and apply global virtual regu-
338"
CONCLUSION,0.7706422018348624,"larization to effectively harmonize domain shift. Our experiments on benchmark and real medical
339"
CONCLUSION,0.7724770642201835,"datasets show that FEDLGD outperforms current state-of-the-art methods in heterogeneous settings.
340"
CONCLUSION,0.7743119266055046,"Furthermore, FEDLGD can be combined with other heterogenous FL methods such as FedProx [26]
341"
CONCLUSION,0.7761467889908257,"and Scaffold [18] to further improve its performance. The potential limitation lies in the additional
342"
CONCLUSION,0.7779816513761468,"communication and computation cost in data distillation, but we show that the trade-off is acceptable
343"
CONCLUSION,0.7798165137614679,"and can be mitigated by decreasing distillation iterations and steps. Our future direction will be
344"
CONCLUSION,0.781651376146789,"investigating privacy-preserving data generation. We believe that this work sheds light on how to
345"
CONCLUSION,0.7834862385321101,"effectively mitigate data heterogeneity from a dataset distillation perspective and will inspire future
346"
CONCLUSION,0.7853211009174312,"work to enhance FL performance, privacy, and efﬁciency.
347"
REFERENCES,0.7871559633027523,"References
348"
REFERENCES,0.7889908256880734,"[1] Abadi, M., Chu, A., Goodfellow, I., McMahan, H.B., Mironov, I., Talwar, K., Zhang, L.: Deep
349"
REFERENCES,0.7908256880733945,"learning with differential privacy. In: Proceedings of the 2016 ACM SIGSAC conference on
350"
REFERENCES,0.7926605504587156,"computer and communications security. pp. 308–318 (2016)
351"
REFERENCES,0.7944954128440367,"[2] Batista, F.J.F., Diaz-Aleman, T., Sigut, J., Alayon, S., Arnay, R., Angel-Pereira, D.: Rim-one dl:
352"
REFERENCES,0.7963302752293578,"A uniﬁed retinal image database for assessing glaucoma using deep learning. Image Analysis &
353"
REFERENCES,0.7981651376146789,"Stereology 39(3), 161–167 (2020)
354"
REFERENCES,0.8,"[3] Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., Tramer, F.: Membership inference attacks
355"
REFERENCES,0.8018348623853211,"from ﬁrst principles. In: 2022 IEEE Symposium on Security and Privacy (SP). pp. 1897–1914.
356"
REFERENCES,0.8036697247706422,"IEEE (2022)
357"
REFERENCES,0.8055045871559633,"[4] Carlini, N., Feldman, V., Nasr, M.: No free lunch in"" privacy for free: How does dataset
358"
REFERENCES,0.8073394495412844,"condensation help privacy"". arXiv preprint arXiv:2209.14987 (2022)
359"
REFERENCES,0.8091743119266055,"[5] Cazenavette, G., Wang, T., Torralba, A., Efros, A.A., Zhu, J.Y.: Dataset distillation by matching
360"
REFERENCES,0.8110091743119267,"training trajectories. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
361"
REFERENCES,0.8128440366972477,"Pattern Recognition. pp. 4750–4759 (2022)
362"
REFERENCES,0.8146788990825689,"[6] Diaz-Pinto, A., Morales, S., Naranjo, V., Köhler, T., Mossi, J.M., Navea, A.: Cnns for automatic
363"
REFERENCES,0.8165137614678899,"glaucoma assessment using fundus images: an extensive validation. Biomedical engineering
364"
REFERENCES,0.818348623853211,"online 18(1), 1–19 (2019)
365"
REFERENCES,0.8201834862385321,"[7] Dong, T., Zhao, B., Lyu, L.: Privacy for free: How does dataset condensation help privacy?
366"
REFERENCES,0.8220183486238533,"arXiv preprint arXiv:2206.00240 (2022)
367"
REFERENCES,0.8238532110091743,"[8] Franceschi, J.Y., De Bézenac, E., Ayed, I., Chen, M., Lamprier, S., Gallinari, P.: A neural
368"
REFERENCES,0.8256880733944955,"tangent kernel perspective of gans. In: International Conference on Machine Learning. pp.
369"
REFERENCES,0.8275229357798165,"6660–6704. PMLR (2022)
370"
REFERENCES,0.8293577981651377,"[9] Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation. In: International
371"
REFERENCES,0.8311926605504587,"conference on machine learning. pp. 1180–1189. PMLR (2015)
372"
REFERENCES,0.8330275229357799,"[10] Goetz, J., Tewari, A.: Federated learning via synthetic data. arXiv preprint arXiv:2008.04489
373"
REFERENCES,0.8348623853211009,"(2020)
374"
REFERENCES,0.8366972477064221,"[11] Gretton, A., Borgwardt, K.M., Rasch, M.J., Schölkopf, B., Smola, A.: A kernel two-sample test.
375"
REFERENCES,0.8385321100917431,"The Journal of Machine Learning Research 13(1), 723–773 (2012)
376"
REFERENCES,0.8403669724770643,"[12] Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., Eichner, H.,
377"
REFERENCES,0.8422018348623853,"Kiddon, C., Ramage, D.: Federated learning for mobile keyboard prediction. arXiv preprint
378"
REFERENCES,0.8440366972477065,"arXiv:1811.03604 (2018)
379"
REFERENCES,0.8458715596330275,"[13] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-
380"
REFERENCES,0.8477064220183487,"ceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778
381"
REFERENCES,0.8495412844036697,"(2016)
382"
REFERENCES,0.8513761467889909,"[14] Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions
383"
REFERENCES,0.8532110091743119,"and perturbations. arXiv preprint arXiv:1903.12261 (2019)
384"
REFERENCES,0.8550458715596331,"[15] Hsu, T.M.H., Qi, H., Brown, M.: Measuring the effects of non-identical data distribution for
385"
REFERENCES,0.8568807339449541,"federated visual classiﬁcation. arXiv preprint arXiv:1909.06335 (2019)
386"
REFERENCES,0.8587155963302753,"[16] Hu, S., Goetz, J., Malik, K., Zhan, H., Liu, Z., Liu, Y.: Fedsynth: Gradient compression via
387"
REFERENCES,0.8605504587155963,"synthetic data in federated learning. arXiv preprint arXiv:2204.01273 (2022)
388"
REFERENCES,0.8623853211009175,"[17] Hull, J.J.: A database for handwritten text recognition research. IEEE Transactions on pattern
389"
REFERENCES,0.8642201834862385,"analysis and machine intelligence 16(5), 550–554 (1994)
390"
REFERENCES,0.8660550458715597,"[18] Karimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T.: Scaffold: Stochastic
391"
REFERENCES,0.8678899082568807,"controlled averaging for federated learning. In: International Conference on Machine Learning.
392"
REFERENCES,0.8697247706422019,"pp. 5132–5143. PMLR (2020)
393"
REFERENCES,0.8715596330275229,"[19] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversar-
394"
REFERENCES,0.8733944954128441,"ial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern
395"
REFERENCES,0.8752293577981651,"recognition. pp. 4401–4410 (2019)
396"
REFERENCES,0.8770642201834863,"[20] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C.,
397"
REFERENCES,0.8788990825688073,"Krishnan, D.: Supervised contrastive learning. Advances in Neural Information Processing
398"
REFERENCES,0.8807339449541285,"Systems 33, 18661–18673 (2020)
399"
REFERENCES,0.8825688073394495,"[21] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document
400"
REFERENCES,0.8844036697247707,"recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)
401"
REFERENCES,0.8862385321100917,"[22] Li, G., Togo, R., Ogawa, T., Haseyama, M.: Dataset distillation for medical dataset sharing.
402"
REFERENCES,0.8880733944954129,"arXiv preprint arXiv:2209.14603 (2022)
403"
REFERENCES,0.8899082568807339,"[23] Li, Q., Diao, Y., Chen, Q., He, B.: Federated learning on non-iid data silos: An experimental
404"
REFERENCES,0.8917431192660551,"study. arXiv preprint arXiv:2102.02079 (2021)
405"
REFERENCES,0.8935779816513761,"[24] Li, Q., He, B., Song, D.: Model-contrastive federated learning. In: Proceedings of the IEEE/CVF
406"
REFERENCES,0.8954128440366973,"Conference on Computer Vision and Pattern Recognition. pp. 10713–10722 (2021)
407"
REFERENCES,0.8972477064220183,"[25] Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated optimization in
408"
REFERENCES,0.8990825688073395,"heterogeneous networks. Proceedings of Machine Learning and Systems 2, 429–450 (2020)
409"
REFERENCES,0.9009174311926605,"[26] Li, X., Huang, K., Yang, W., Wang, S., Zhang, Z.: On the convergence of fedavg on non-iid
410"
REFERENCES,0.9027522935779817,"data. International Conference on Learning Representations (2020)
411"
REFERENCES,0.9045871559633027,"[27] Lin, T., Kong, L., Stich, S.U., Jaggi, M.: Ensemble distillation for robust model fusion in
412"
REFERENCES,0.9064220183486239,"federated learning. Advances in Neural Information Processing Systems 33, 2351–2363 (2020)
413"
REFERENCES,0.908256880733945,"[28] Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning
414"
REFERENCES,0.9100917431192661,"research 9(11) (2008)
415"
REFERENCES,0.9119266055045872,"[29] McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efﬁcient
416"
REFERENCES,0.9137614678899083,"learning of deep networks from decentralized data. In: Artiﬁcial intelligence and statistics. pp.
417"
REFERENCES,0.9155963302752294,"1273–1282. PMLR (2017)
418"
REFERENCES,0.9174311926605505,"[30] Mohamadi, M.A., Sutherland, D.J.: A fast, well-founded approximation to the empirical neural
419"
REFERENCES,0.9192660550458716,"tangent kernel. arXiv preprint arXiv:2206.12543 (2022)
420"
REFERENCES,0.9211009174311927,"[31] Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits in natural
421"
REFERENCES,0.9229357798165138,"images with unsupervised feature learning (2011)
422"
REFERENCES,0.9247706422018349,"[32] Orlando, J.I., Fu, H., Breda, J.B., van Keer, K., Bathula, D.R., Diaz-Pinto, A., Fang, R., Heng,
423"
REFERENCES,0.926605504587156,"P.A., Kim, J., Lee, J., et al.: Refuge challenge: A uniﬁed framework for evaluating automated
424"
REFERENCES,0.9284403669724771,"methods for glaucoma assessment from fundus photographs. Medical image analysis 59, 101570
425"
REFERENCES,0.9302752293577982,"(2020)
426"
REFERENCES,0.9321100917431193,"[33] Sachdeva, N., McAuley, J.: Data distillation: A survey. arXiv preprint arXiv:2301.04272 (2023)
427"
REFERENCES,0.9339449541284404,"[34] Schuster, A.K., Erb, C., Hoffmann, E.M., Dietlein, T., Pfeiffer, N.: The diagnosis and treatment
428"
REFERENCES,0.9357798165137615,"of glaucoma. Deutsches Ärzteblatt International 117(13), 225 (2020)
429"
REFERENCES,0.9376146788990826,"[35] Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks against
430"
REFERENCES,0.9394495412844037,"machine learning models. In: 2017 IEEE symposium on security and privacy (SP). pp. 3–18.
431"
REFERENCES,0.9412844036697248,"IEEE (2017)
432"
REFERENCES,0.9431192660550459,"[36] Sivaswamy, J., Krishnadas, S., Joshi, G.D., Jain, M., Tabish, A.U.S.: Drishti-gs: Retinal image
433"
REFERENCES,0.944954128440367,"dataset for optic nerve head (onh) segmentation. In: 2014 IEEE 11th international symposium
434"
REFERENCES,0.9467889908256881,"on biomedical imaging (ISBI). pp. 53–56. IEEE (2014)
435"
REFERENCES,0.9486238532110092,"[37] Tang, Z., Zhang, Y., Shi, S., He, X., Han, B., Chu, X.: Virtual homogeneity learning: Defending
436"
REFERENCES,0.9504587155963303,"against data heterogeneity in federated learning. arXiv preprint arXiv:2206.02465 (2022)
437"
REFERENCES,0.9522935779816514,"[38] Wang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V.: Tackling the objective inconsistency problem
438"
REFERENCES,0.9541284403669725,"in heterogeneous federated optimization. Advances in neural information processing systems
439"
REFERENCES,0.9559633027522936,"33, 7611–7623 (2020)
440"
REFERENCES,0.9577981651376147,"[39] Wang, T., Zhu, J.Y., Torralba, A., Efros, A.A.:
Dataset distillation. arXiv preprint
441"
REFERENCES,0.9596330275229358,"arXiv:1811.10959 (2018)
442"
REFERENCES,0.9614678899082569,"[40] Xiong, Y., Wang, R., Cheng, M., Yu, F., Hsieh, C.J.: Feddm: Iterative distribution matching for
443"
REFERENCES,0.963302752293578,"communication-efﬁcient federated learning. arXiv preprint arXiv:2207.09653 (2022)
444"
REFERENCES,0.9651376146788991,"[41] Xu, J., Glicksberg, B.S., Su, C., Walker, P., Bian, J., Wang, F.: Federated learning for healthcare
445"
REFERENCES,0.9669724770642202,"informatics. Journal of Healthcare Informatics Research 5, 1–19 (2021)
446"
REFERENCES,0.9688073394495413,"[42] Ye, R., Ni, Z., Xu, C., Wang, J., Chen, S., Eldar, Y.C.: Fedfm: Anchor-based feature matching
447"
REFERENCES,0.9706422018348624,"for data heterogeneity in federated learning. arXiv preprint arXiv:2210.07615 (2022)
448"
REFERENCES,0.9724770642201835,"[43] Zhang, L., Shen, L., Ding, L., Tao, D., Duan, L.Y.: Fine-tuning global model via data-free knowl-
449"
REFERENCES,0.9743119266055046,"edge distillation for non-iid federated learning. In: Proceedings of the IEEE/CVF Conference
450"
REFERENCES,0.9761467889908257,"on Computer Vision and Pattern Recognition. pp. 10174–10183 (2022)
451"
REFERENCES,0.9779816513761468,"[44] Zhao, B., Bilen, H.: Dataset condensation with differentiable siamese augmentation. In: Inter-
452"
REFERENCES,0.9798165137614679,"national Conference on Machine Learning. pp. 12674–12685. PMLR (2021)
453"
REFERENCES,0.981651376146789,"[45] Zhao, B., Bilen, H.: Dataset condensation with distribution matching. In: Proceedings of the
454"
REFERENCES,0.9834862385321101,"IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 6514–6523 (2023)
455"
REFERENCES,0.9853211009174312,"[46] Zhao, B., Mopuri, K.R., Bilen, H.: Dataset condensation with gradient matching. ICLR 1(2), 3
456"
REFERENCES,0.9871559633027523,"(2021)
457"
REFERENCES,0.9889908256880734,"[47] Zhou, T., Zhang, J., Tsang, D.: Fedfa: Federated learning with feature anchors to align feature
458"
REFERENCES,0.9908256880733946,"and classiﬁer for heterogeneous data. arXiv preprint arXiv:2211.09299 (2022)
459"
REFERENCES,0.9926605504587156,"[48] Zhu, H., Xu, J., Liu, S., Jin, Y.: Federated learning on non-iid data: A survey. Neurocomputing
460"
REFERENCES,0.9944954128440368,"465, 371–390 (2021)
461"
REFERENCES,0.9963302752293578,"[49] Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients. Advances in neural information
462"
REFERENCES,0.998165137614679,"processing systems 32 (2019)
463"
