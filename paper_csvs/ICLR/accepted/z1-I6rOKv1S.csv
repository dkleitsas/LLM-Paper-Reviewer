Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003937007874015748,"Numerous applications of machine learning involve representing probability
distributions over high-dimensional data. We propose autoregressive quantile ﬂows,
a ﬂexible class of normalizing ﬂow models trained using a novel objective based
on proper scoring rules. Our objective does not require calculating computationally
expensive determinants of Jacobians during training and supports new types of
neural architectures, such as neural autoregressive ﬂows from which it is easy to
sample.
We leverage these models in quantile ﬂow regression, an approach that
parameterizes predictive conditional distributions with ﬂows, resulting in improved
probabilistic predictions on tasks such as time series forecasting and object
detection. Our novel objective functions and neural ﬂow parameterizations also
yield improvements on popular generation and density estimation tasks, and
represent a step beyond maximum likelihood learning of ﬂows."
INTRODUCTION,0.007874015748031496,"1
INTRODUCTION"
INTRODUCTION,0.011811023622047244,"Reasoning about uncertainty via the language of probability is important in many application domains
of machine learning, including medicine (Saria, 2018), robotics (Chua et al., 2018; Buckman et al.,
2018), and operations research (Van Roy et al., 1997). Especially important is the estimation of
predictive uncertainties (e.g., conﬁdence intervals around forecasts) in tasks such as clinical diagnosis
(Jiang et al., 2012) or decision support systems (Werling et al., 2015; Kuleshov and Liang, 2015)."
INTRODUCTION,0.015748031496062992,"Normalizing ﬂows (Rezende and Mohamed, 2016; Papamakarios et al., 2019; Kingma et al., 2016)
are a popular framework for deﬁning probabilistic models, and can be used for density estimation
(Papamakarios et al., 2017), out-of-distribution detection (Nalisnick et al., 2019), content generation
(Kingma and Dhariwal, 2018), and more. Flows feature tractable posterior inference and maximum
likelihood estimation; however, maximum likelihood estimation of ﬂows requires carefully designing
a family of bijective functions that are simultaneously expressive and whose Jacobian has a tractable
determinant. In practice, this limits the kinds of neural networks that can be used to parameterize
normalizing ﬂows and makes many types of networks computationally expensive to train."
INTRODUCTION,0.01968503937007874,"This paper takes a step towards addressing this limitation of normalizing ﬂows by proposing new
objectives that contribute towards alleviating the computational cost of calculating determinants of
Jacobians. Speciﬁcally, we argue for training ﬂows using an objective that is different from classical
maximum likelihood and is instead based on proper scoring rules (Gneiting and Raftery, 2007), a
standard tool in the statistics literature for evaluating the quality of probabilistic forecasts. We show
that this objective can be used to train normalizing ﬂows and that it simpliﬁes the computation of
Jacobians in certain types of ﬂows."
INTRODUCTION,0.023622047244094488,"We introduce autoregressive quantile ﬂows (AQFs), a framework that combines the above learning
objective with a set of architectural choices inspired by classical autoregressive ﬂows. Quantile
ﬂows possess characteristics that represent an improvement over existing ﬂow models—including
supporting neural architectures that simultaneously provide fast training and sampling—in addition to
the usual beneﬁts of ﬂows (exact posterior inference and density estimation). Interestingly, quantile
ﬂows can be interpreted as extensions of quantile functions to multiple dimensions."
INTRODUCTION,0.027559055118110236,"We use AQFs as the basis for quantile ﬂow regression (QFR), an approach to predictive uncertainty
estimation in which a probabilistic model directly outputs a normalizing ﬂow as the predictive
distribution. The QFR approach enables neural networks to output highly expressive probabilistic
predictions that make very little assumptions on the form of the predicted variable and that improve
uncertainty estimates in probabilistic and Bayesian models. In the one-dimensional case, our approach
yields quantile function regression and cumulative distribution function regression, two simple,
general, and principled approaches for ﬂexible probabilistic forecasting in regression."
INTRODUCTION,0.031496062992125984,"In addition, we demonstrate the beneﬁts of AQFs on probabilistic modeling tasks that include density
estimation and autoregressive generation. Across our sets of experiments, we observe improved
performance, and we demonstrate properties of quantile ﬂows that traditional ﬂow models do not
possess (e.g., sampling with ﬂexible neural parameterizations)."
INTRODUCTION,0.03543307086614173,"Contributions.
In summary, this work (1) introduces new objectives for ﬂow models that simplify
the computation of determinants of Jacobians, which in turn greatly simpliﬁes the implementation
of ﬂow models and extends the class of models that can be used to parameterize ﬂows. We also (2)
deﬁne autoregressive quantile ﬂows based on this objective, and highlight new architectures supported
by this framework. Finally, (3) we deploy AQFs as part of quantile ﬂow regression, and show that
this approach improves upon existing methods for predictive uncertainty estimation."
BACKGROUND,0.03937007874015748,"2
BACKGROUND"
BACKGROUND,0.04330708661417323,"Notation.
Our goal is to learn a probabilistic model p(x) ∈∆(Rd) in the space ∆(Rd) of
distributions over a high-dimensional x ∈Rd; we use xj ∈R to denote components of x. In
some cases, we have access to features x′ ∈X ′ associated with x and we want to train a forecaster
H : X ′ →∆(Rd) that outputs a predictive probability over x conditioned on x′."
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.047244094488188976,"2.1
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS"
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.051181102362204724,"A normalizing ﬂow deﬁnes a distribution p(x) via an invertible mapping fθ : Rd →Rd with
parameters θ ∈Θ that describes a transformation between x and a random variable z ∈Rd sampled
from a simple prior z ∼p(z) (Rezende and Mohamed, 2016; Papamakarios et al., 2019). We may"
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.05511811023622047,"compute p(x) via the change of variables formula p(x) =
 ∂fθ(z)−1"
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.05905511811023622,"∂z
 p(z), where
 ∂fθ(z)−1"
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.06299212598425197,"∂z
 denotes
the determinant of the inverse Jacobian of fθ. In order to ﬁt ﬂow-based models using maximum
likelihood, we typically choose fθ to be in a family for which the Jacobian is tractable."
NORMALIZING FLOWS AND AUTOREGRESSIVE GENERATIVE MODELS,0.06692913385826772,"A common way to deﬁne ﬂows with a tractable Jacobian is via autoregressive models of the form
xj = τ(zj; hj)
hj = cj(x<j),
where τ(zj; hj) is an invertible transformer, a strictly monotonic function of zj, and cj is the j-th
conditioner, which outputs parameters hj for the transformer. As long as τ is invertible, such
autoregressive models can be used to deﬁne ﬂows (Papamakarios et al., 2019)."
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.07086614173228346,"2.2
EVALUATING FORECASTS WITH PROPER SCORING RULES"
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.07480314960629922,"A common way to represent a probabilistic forecast in the statistics and forecasting literature is
via a cumulative distribution function (CDF) F : Rd →[0, 1]; any probability distribution can be
represented this way, including discrete distributions. Since F is monotonically increasing in each
coordinate, when y is one dimensional, we may deﬁne its inverse Q : [0, 1] →R called the quantile
function (QF), deﬁned as Q(α) = inf{x′ ∈R | F(x′) ≥α}."
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.07874015748031496,"In the statistics literature, the quality of forecasts is often evaluated using proper scoring rules
(or proper scores; Gneiting and Raftery (2007)). For example, when predictions take the form of
CDFs, a popular scoring rule is the continuous ranked probability score (CRPS), deﬁned for two
CDFs F and G as CRPS(F, G) =
R"
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.08267716535433071,"y (F(x) −G(x))2 dx. When we only have samples x1, ..., xm
from G, we can generalize this score as 1"
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.08661417322834646,"m
Pm
i=1
R"
EVALUATING FORECASTS WITH PROPER SCORING RULES,0.09055118110236221,"x (F(x) −I(x −xi))2 dx. Alternatively, we can
evaluate the α-th quantile Q(α) of a QF Q via the check score L : R × R →R+ deﬁned as
Lα(x, f) = α(x −f) if x ≥f and (1 −α)(f −x) otherwise. The check score also provides a
consistent estimator for the conditional quantile of any distribution."
TAKING STEPS BEYOND MAXIMUM LIKELIHOOD LEARNING OF FLOWS,0.09448818897637795,"3
TAKING STEPS BEYOND MAXIMUM LIKELIHOOD LEARNING OF FLOWS"
TAKING STEPS BEYOND MAXIMUM LIKELIHOOD LEARNING OF FLOWS,0.0984251968503937,"Maximum likelihood estimation of ﬂows requires carefully designing a family of bijective functions
that are simultaneously expressive and whose Jacobian has a tractable determinant. In practice, this
makes ﬂows time-consuming to design and computationally expensive to train. In this paper, we
argue for training ﬂows using objectives based on proper scoring rules (Gneiting and Raftery, 2007)."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.10236220472440945,"3.1
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.1062992125984252,"We begin with the one dimensional setting, where a ﬂow fθ : R →R is a bijective mapping that can
be interpreted as a QF. Alternatively, the reverse ﬂow f −1
θ
can be interpreted as a CDF. We will use
Qθ, Fθ to denote fθ and f −1
θ
, respectively; our goal is to ﬁt these models from data."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.11023622047244094,"In order to ﬁt models of the cumulative distribution and the quantile function, we propose objectives
based on proper scoring rules. We propose ﬁtting models Fθ of the CDF using the CRPS:"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.1141732283464567,"L(1)(Fθ, xi) := CRPS(Fθ, xi) =
Z ∞"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.11811023622047244,"−∞
(Fθ(x) −I(xi ≤x))2 dx.
(1)"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.1220472440944882,"When dealing with a model Qθ of the QF, we propose an objective based on the expected check score"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.12598425196850394,"L(2)(Qθ, xi) :=
Z 1"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.12992125984251968,"0
Lα(Qθ(α), xi)dα,
(2)"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.13385826771653545,"where Lα is a check score targeting quantile α. We refer to this objective as the quantile loss. This
objective has been used previously to train value functions in reinforcement learning as well as
conditional distributions in autoregressive models (Dabney et al., 2018a;b). In this paper, we describe
its application to modeling aleatoric predictive uncertainties."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.1377952755905512,"The parametric form of Qθ or Fθ can be any class of strictly monotonic (hence invertible) functions.
Previous works have relied on afﬁne or piecewise linear functions (Wehenkel and Louppe, 2021),
sum-of-squares (Jaini et al., 2019), monotonic neural networks (Huang et al., 2018; Cao et al., 2019),
and other models. Any of these choices suits our framework; we provide more details below."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.14173228346456693,"Equivalence Between the CRPS and Quantile Losses
So far, we have described two methods
for ﬁtting a one-dimensional ﬂow model. Their objectives are actually equivalent.
Proposition 1. For a CDF F : R →[0, 1] and x′ ∈R, the CRPS and quantile losses are equivalent:"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.14566929133858267,"L(1)(F, x′) = a · L(2)(F −1, x′) + b
a, b ∈R, a > 0
(3)"
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.14960629921259844,"This fact appears to be part of statistics folk knowledge, and we have only ever seen it stated brieﬂy in
some works. We provide a complete proof in the appendix. See (Laio and Tamea, 2007) for another
argument."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.15354330708661418,"If the models Fθ, Qθ are analytically invertible (e.g., they are piecewise linear), we are free to choose
ﬁtting the CDF or its inverse. Other representations for F will not lead to analytically invertible
models, which require choosing a training direction, as we discuss below."
LEARNING SIMPLE FLOWS WITH PROPER SCORING RULES,0.15748031496062992,"Practical Implementation.
The quantile and the CRPS losses both involve a potentially intractable
integral. We approximate the integrals using Monte-Carlo; this allows us to obtain gradients using
backpropagation. For the quantile loss, we sample α uniformly at random in [0, 1]; for the CRPS
loss, we choose a reasonable range of y (usually, centered around yi) and sample uniformly in that
range. This approach works well in practice and avoids the complexity of alternative methods such as
quadrature (Durkan et al., 2019)."
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS,0.16141732283464566,"3.2
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS"
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS,0.16535433070866143,"Next, we extend our approach to high-dimensional autoregressive ﬂows by ﬁtting each conditional
distribution with the quantile or the CRPS loss. We start with a general autoregressive ﬂow deﬁned as"
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS,0.16929133858267717,"xj = τ(zj; hj)
hj = cj(x<j),
(4)"
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS,0.1732283464566929,"where τ(zj; hj) is an invertible transformer and cj is the j-th conditioner As in the previous section,
we use Qhj, Fhj to denote τ(zj; hj) and τ −1(xj; hj), respectively."
LEARNING HIGH-DIMENSIONAL AUTOREGRESSIVE FLOWS,0.17716535433070865,We train autoregressive ﬂows using losses that decompose over each dimension as follows:
N,0.18110236220472442,"1
n n
X i=1 d
X"
N,0.18503937007874016,"j=1
L(1)[xij, Fhj]"
N,0.1889763779527559,"|
{z
}
CRPS loss; reverse training"
N,0.19291338582677164,"1
n n
X i=1 d
X"
N,0.1968503937007874,"j=1
L(2)[xij, Qhj]"
N,0.20078740157480315,"|
{z
}
quantile loss; forwards training ,
(5)"
N,0.2047244094488189,"where L(1) and L(2) are, respectively, the CRPS and quantile objectives deﬁned in the previous
section and applied component-wise. We average the losses on a dataset of n points {xi}n
i=1."
N,0.20866141732283464,"Sampling-Based Training of Autoregressive Flows.
Crucially, Equation 5 deﬁnes not only new
objectives but also new training procedures for ﬂows. Speciﬁcally, it deﬁnes a sampling-based
learning process that contrasts with traditional maximum likelihood training."
N,0.2125984251968504,"In order to train with objectives L(1), L(2), we perform Monte Carlo sampling to approximate an
intractable integral. Thus, we sample a source variable (x or z) from a uniform distribution and
transform it into the other variable, which we call the target, after which we compute the loss and its
gradient. This is in contrast to maximum-likelihood learning, where we take the observed variable x,
transform it into z, and evaluate p(z) as well as the determinant of the Jacobian."
N,0.21653543307086615,"The choice of source and target variable depends on the choice of objective function. Each choice
yields a different type of training procedure; we refer to these as forward and reverse training."
N,0.2204724409448819,"Forward and Reverse Training.
In forward training, we sample a z ∼U([0, 1]), pass it through
Qhj, obtaining yj. Thus, at training time, we perform generation and our loss based on the similarity
between samples and the real data x. In contrast, we train standard ﬂows by passing an input y into
the inverse ﬂow to obtain a latent z (and no sampling is involved). Our process share similarities with
the training of GANs, but we don’t rely on a discriminator. The forward training process enables us to
support neural architectures that maximum likelihood training does not support, as we discuss below."
N,0.22440944881889763,"In reverse training, we sample x ∼U([xmin, xmax]) (where the bounds in U are chosen via a
heuristic), and we compute the resulting z via Fθ. The CRPS loss deﬁned on z seeks to make the
distribution uniform on average, which is consistent with the properties of inverse transform sampling.
In this case τ(·; hj) can be interpreted as a CDF function conditioned on x<j via hj."
N,0.2283464566929134,"On Computing Determinants of Jacobians.
Interestingly, since quantile ﬂows are trained using
variants of the CRPS rather than maximum likelihood, they do not rely on the change of variables
formula, and do not require computing the derivative ∂τ/∂z for each transformer τ(·; hj). This fact
has a number of important beneﬁts: (1) it greatly simpliﬁes the implementation of ﬂow-based models,
and (2) it may also support families of transformers for which computing derivatives ∂τ/∂z would
have otherwise been inconvenient."
N,0.23228346456692914,"Note however, that our approach does not fully obviate the need to think about the tractability of
Jacobians: we still rely on autoregressive models for whom the Jacobian is triangular. We will explore
how to further generalize our approach beyond triangular Jacobians in future work."
AUTOREGRESSIVE QUANTILE FLOWS,0.23622047244094488,"4
AUTOREGRESSIVE QUANTILE FLOWS"
AUTOREGRESSIVE QUANTILE FLOWS,0.24015748031496062,"Next, we introduce autoregressive quantile ﬂows (AQFs), a class of models trained with our set of
novel learning objectives. Quantile ﬂows improve over classical ﬂow models by, among other things,
supporting neural architectures that simultaneously provide fast training and sampling."
AUTOREGRESSIVE QUANTILE FLOWS,0.2440944881889764,"An AQF deﬁnes a mapping between z and x that has the form xj = τ(zj; hj) hj = cj(x<j), as in a
classical ﬂow, with τ(zj; hj) being an invertible transformer and cj being the j-th conditioner."
AUTOREGRESSIVE QUANTILE FLOWS,0.24803149606299213,"Architectures for Quantile Flows.
Crucially, we are free to select a parameterization for either
τ(zj; hj) or its inverse τ −1(xj; hj) and the resulting model will be trainable using one of our"
AUTOREGRESSIVE QUANTILE FLOWS,0.25196850393700787,"objectives L(1), L(2). This is in sharp contrast to maximum likelihood, in which we need to learn a
model of τ −1(xj; hj) in order to recover the latent z from the observed data x and compute p(z)."
AUTOREGRESSIVE QUANTILE FLOWS,0.2559055118110236,"This latter limitation has the consequence that in classical ﬂows in which the transformer τ −1 is not
invertible (which is in most non-afﬁne models), it is difﬁcult or not possible to perform sampling
(since sampling requires τ). As a consequence, most non-afﬁne ﬂow models can only be used for
density estimation; ours is the ﬁrst that be used for generation. We give examples below."
AUTOREGRESSIVE QUANTILE FLOWS,0.25984251968503935,"Afﬁne, Piecewise Linear, and Neural Quantile Flows.
Afﬁne transformers have the form
τ(z; a, b) = a·z +b, hence they are analytically invertible. A special case is the Gaussian transformer
with parameters µ, σ2, which yields the masked autoregressive ﬂow (MAF; Papamakarios et al.
(2017)) architecture. The MAF can be trained in our framework using maximum likelihood or
L(1), L(2). Afﬁne ﬂows further extend to piecewise linear transformers, increasing expressivity while
maintaining analytical invertiblity. Training afﬁne and piecewise linear transformers provides both a
forwards and a reverse mapping between x and z in closed form."
AUTOREGRESSIVE QUANTILE FLOWS,0.2637795275590551,"For higher expressivity, we may parameterize transformers with monotonic neural networks, which
can be constructed from a composition of perceptrons with strictly positive weights.
Huang
et al. (2018) proposed an approach where a conditioner c(x<j) directly outputs the weights of
a monotonic neural network which models τ −1(xj, c(x<j)). In our experiments, τ(zj, c(x<j))
is simply parameterized by a network with positive weights that outputs xj as based on zj, x<j,
similarly to (Cao et al., 2019), although AQFs also support the approach of Huang et al. (2018)."
AUTOREGRESSIVE QUANTILE FLOWS,0.2677165354330709,"Comparison to Other Flow Families.
In general, any transformer family that can be used within
autoregressive models can be used within the quantile ﬂows framework, including integration-based
transformers (Wehenkel and Louppe, 2021) spline approximations (Müller et al., 2019; Durkan et al.,
2019; Dolatabadi et al., 2020), piece-wise separable models, and others."
AUTOREGRESSIVE QUANTILE FLOWS,0.27165354330708663,"A key feature of our framework is that it supports both forwards and reverse training while alternatives
do not. For example, when training a neural autoregressive ﬂow (Huang et al., 2018), we need to
compute z from x to compute the log-likelihood. Thus, we parameterize τ −1 with a neural network,
but then τ is not analytically invertible, which precludes us from sampling from it. Our method, on
the other hand, parameterizes τ itself using a ﬂexible neural approximator, making it possible to
easily perform sampling within a ﬂexible neural autoregressive ﬂow for the ﬁrst time."
AUTOREGRESSIVE QUANTILE FLOWS,0.2755905511811024,"Additionally, modeling ﬂexible CDFs enables us to study broader classes of distributions, including
discrete ones. We perform early experiments and leave the full extension to future work."
PREDICTIVE UNCERTAINTY ESTIMATION USING QUANTILE FLOWS,0.2795275590551181,"5
PREDICTIVE UNCERTAINTY ESTIMATION USING QUANTILE FLOWS"
PREDICTIVE UNCERTAINTY ESTIMATION USING QUANTILE FLOWS,0.28346456692913385,"Probabilistic models typically assume a ﬁxed parametric form for their outputs (e.g., a Gaussian),
which limits their expressivity and accuracy (Kuleshov et al., 2018). Here, we use AQFs as the
basis for quantile ﬂow regression (QFR), which models highly expressive aleatoric uncertainties over
high-dimensional variables by directly outputting a normalizing ﬂow."
QUANTILE FLOW REGRESSION,0.2874015748031496,"5.1
QUANTILE FLOW REGRESSION"
QUANTILE FLOW REGRESSION,0.29133858267716534,"The idea of Quantile Flow Regression (QFR) is to train models H : X →(Rd →[0, 1]) that directly
output a normalizing ﬂow. This approach does not impose any further parametric assumptions
on the form of the output distribution (besides that it is parameterized by an expressive function
approximator), enabling the use of ﬂexible probability distributions in regression, and avoiding
restrictive assumptions of the kind that would be made by Gaussian regression."
QUANTILE FLOW REGRESSION,0.2952755905511811,"From an implementation perspective, H outputs quantile ﬂows of the form"
QUANTILE FLOW REGRESSION,0.2992125984251969,"yj = τ(zj; hj)
hj = cj(x<j, g(x)),
(6)"
QUANTILE FLOW REGRESSION,0.3031496062992126,"where each conditioner ci(x<i, g(x′)) in a function of an input x′ processed by a model g(x′). We
provide speciﬁc examples of this parameterization in our experiments section. The entire structure can
be represented in a single computational graph and is trained end-to-end using objectives L(1), L(2)."
QUANTILE FLOW REGRESSION,0.30708661417322836,"Estimating Probabilities from Quantile Flows.
In practice, we may also be interested to estimate
the probability of various events under the joint distribution of x. This may be done by sampling from
the predictive distribution and computing the empirical occurrence of the events of interest. See our
time series experiments for details, as well as the appendix for pseudocode of the sampling algorithm."
QUANTILE FLOW REGRESSION,0.3110236220472441,"Note that while we cannot directly estimate events such as P(x1 ≤x ≤x2) from a multi-variate
quantile ﬂow, we can always compute densities of the form P(x). When ﬁtting ﬂows in the reverse
direction, we can simply differentiate each transformer dτ/dx to obtain conditional densities. When
ﬁtting ﬂows in the forward direction, we can use the formula dτ/dz = 1/P(τ(z))."
QUANTILE AND CUMULATIVE DISTRIBUTION FUNCTION REGRESSION,0.31496062992125984,"5.2
QUANTILE AND CUMULATIVE DISTRIBUTION FUNCTION REGRESSION"
QUANTILE AND CUMULATIVE DISTRIBUTION FUNCTION REGRESSION,0.3188976377952756,"In the one-dimensional case, our approach yields quantile function regression and cumulative
distribution function regression (CDFR), two simple, general, and principled approaches for ﬂexible
probabilistic forecasting in regression. In quantile function regression, a baseline model H outputs
a quantile function Q(α); in practice, we implement this model via a neural network f(x′, α) that
takes as input an extra α ∈[0, 1] and outputs the α-th quantile of the conditional distribution of x
given x′. We train the model f with the quantile loss L(2) while sampling α ∼U([0, 1]) during the
optimization process. We provide several example parameterizations of f in the experiments section.
Cumulative distribution function regression is deﬁned similarly to the above and is trained with the
CRPS loss L(1)."
QUANTILE AND CUMULATIVE DISTRIBUTION FUNCTION REGRESSION,0.3228346456692913,"Quantile vs. Cumulative Distribution Functions.
The quantile function Q makes it easy to derive
α-% conﬁdence intervals around the median by querying Q(1 −α/2) and Q(α/2). Conversely,
the CDF function f makes it easy to query the probability that x falls in a region [x1, x2] via
f(x2) −f(x1). Each operation can also be performed in the alternative model, but would require
bisection or grid search. We suspect users will ﬁt one model or both, depending on their needs."
QUANTILE AND CUMULATIVE DISTRIBUTION FUNCTION REGRESSION,0.32677165354330706,"Extensions to Classiﬁcation.
A CDF can also be used to represent discrete random variables; thus
our method is applicable to classiﬁcation, even though we focus on regression. We may ﬁt a CDF F
over K numerical class labels x1 < ... < xK; the optimal form of F is a step function. When F is not
a step function, we can still extract from it approximate class membership probabilities. For example,
in binary classiﬁcation where classes are encoded as 0 and 1, we may use (F(1 −ϵ) −F(0))/2 as an
estimate of the probability of x = 0. We provide several experiments in this regime."
EXPERIMENTS,0.33070866141732286,"6
EXPERIMENTS"
EXPERIMENTS,0.3346456692913386,"We evaluate quantile ﬂow regression and its extensions against several baselines, including Mixture
Density Networks (MDN; Bishop (1994)), Gaussian regression, Quantile regression (QRO; in which
we ﬁt a separate estimator for 99 quantiles indexed by α = 0.01, ..., 0.99). We report calibration
errors (as in Kuleshov et al. (2018)), check scores (CHK), the CRPS (as in Gneiting and Raftery
(2007)) and L1 loss (MAE); both calibration error (deﬁned in appendix) and check score are indexed
by α = 0.01, ..., 0.99. Additional experiments on synthetic datasets can be found in the appendix."
EXPERIMENTS ON UCI DATASETS,0.33858267716535434,"6.1
EXPERIMENTS ON UCI DATASETS"
EXPERIMENTS ON UCI DATASETS,0.3425196850393701,"Datasets
We used four benchmark UCI regression datasets (Dua and Graff, 2017) varying in
size from 308-1030 instances and 6-10 continuous features. We used two benchmark UCI binary
classiﬁcation datasets with 512 and 722 instances with 22 and 9 features respectively. We randomly
hold out 25% of data for testing."
EXPERIMENTS ON UCI DATASETS,0.3464566929133858,"Models.
QFR and CDFR were trained with learning rates and dropout rates of 3e −3, 3e −4 and
(0.2, 0.1) respectively as described in Section 5.2. All of the models were two-hidden-layer neural
networks with hidden layer size 64, and ReLU activation functions."
EXPERIMENTS ON UCI DATASETS,0.35039370078740156,"Results.
QFR, on average, outperforms the baselines in each of the one-dimensional UCI tasks
(Table 1). Interestingly, our method was also able to learn a CDF for binary classiﬁcation. We"
EXPERIMENTS ON UCI DATASETS,0.3543307086614173,Table 1: Results on the regression benchmark datasets.
EXPERIMENTS ON UCI DATASETS,0.35826771653543305,"Check Score
CRPS
Dataset
Gaussian
QRO
MDN
QFR
CDFR
Gaussian
QRO
MDN
QFR
CDFR
Yacht
0.225
0.202
0.174
0.171
-
0.443
0.381
0.282
0.275
0.415
Boston
1.547
1.191
1.115
0.865
-
2.643
1.441
1.160
0.025
2.373
Concrete
2.243
1.563
2.896
1.191
-
4.157
1.906
5.703
0.697
2.97
Energy
2.254
1.463
2.926
1.183
-
4.171
1.911
5.745
0.578
2.91
MAE
Calibration MAE
Dataset
Gaussian
QRO
MDN
QFR
CDFR
Gaussian
QRO
MDN
QFR
CDFR
Yacht
0.627
0.563
0.494
0.481
0.539
0.097
0.100
0.019
0.028
0.116
Boston
3.909
3.398
2.933
2.215
2.611
0.139
0.140
0.036
0.034
0.038
Concrete
6.329
4.051
9.022
2.853
3.623
0.165
0.114
0.029
0.046
0.066
Energy
6.629
4.151
9.393
2.815
2.829
0.165
0.114
0.032
0.029
0.049"
EXPERIMENTS ON UCI DATASETS,0.36220472440944884,"demonstrated that the CDFR method is able to compete with the neural network baseline and that our
method can be potentially extended to classiﬁcation, testing on the Diabetes and KC2 dataset where
the Neural Network obtained metrics of 0.717 and 0.832 respectively, and our CDFR obtained 0.723
and 0.796 respectively."
OBJECT DETECTION,0.3661417322834646,"6.2
OBJECT DETECTION"
OBJECT DETECTION,0.3700787401574803,"Data.
We also conducted bounding box retrieval experiments against Gaussian baselines on the
data corpus VOC 2007, obtained from Everingham et al. with 20 classes of objects in 9963 images."
OBJECT DETECTION,0.37401574803149606,"Table 2: Obj. Detection
with CDFR"
OBJECT DETECTION,0.3779527559055118,"Method
CRPS"
OBJECT DETECTION,0.38188976377952755,"Gaussian
6.85
CDFR
6.15"
OBJECT DETECTION,0.3858267716535433,"Models.
We modiﬁed the architecture of He et al. (2019) which uses a
KL-loss to ﬁt a Gaussian distribution for each bounding box. The existing
architecture has a MobileNet-v2 backbone (Sandler et al., 2019). Keeping
the backbone unchanged, the quantile regressor method adds an alternate
head with separate layers, which adds uncertainty bounds to the original
output. The original Gaussian method was trained for a total of 60 epochs,
with a learning rate of 2e-4 which is decremented by a factor of 10 at
epochs 30 and 45. The quantile ﬂow layer is trained for an additional 20
epochs, with the same learning rate."
OBJECT DETECTION,0.38976377952755903,Table 3: Results on the object detection task
OBJECT DETECTION,0.3937007874015748,"Method
Cal. MAE
MAP@50
CRPS"
OBJECT DETECTION,0.39763779527559057,"Gaussian
0.133
0.796
8.89
QFR
0.103
0.796
8.59
Gauss. VD
-
0.771
8.73
QFR VD
-
0.772
7.99"
OBJECT DETECTION,0.4015748031496063,"Results.
We implement QFR in conjunction in a
standard model, as well as one that uses variational
dropout (VD; Gal and Ghahramani (2016)). In
each case, the QFR layer improves uncertainty
estimation (as measured by CRPS; Table 3), with
the best results coming from a model trained with
QFR and VD. MAP@50 is deﬁned in the appendix.
This suggests our methods improve both aleatoric
and epistemic uncertainties (the latter being estimated by VD). We illustrate examples of predicted
uncertainties on real images in Figure 1.
We also swapped QFR for CDFR, while keeping
approximately the same architecture, and compared to the Gaussian baseline using the CRPS. CDFR
also outperforms the Gaussian baseline (Table 2)."
TIME SERIES FORECASTING,0.40551181102362205,"6.3
TIME SERIES FORECASTING"
TIME SERIES FORECASTING,0.4094488188976378,"Data.
Our time series experiment uses quantile ﬂows to forecast hourly electricity usage. We
follow the experimental setup of Mashlakov et al. (2021) using their publicly available code. We
use the 2011-2014 Electricity Load dataset, a popular dataset for benchmarking time series models.
It contains 70,299 time series derived from hourly usage by 321 clients across three years. There
exist different ways to pre-process this dataset; we follow the setup of Mashlakov et al. (2021). Our
models predict hourly electricity usage 24 hours into the future for each time series, for a total of
1,687,176 predictions. There are four covariates."
TIME SERIES FORECASTING,0.41338582677165353,"Models.
We benchmark two state-of-the-art time series forecasting models: DeepAR (Salinas
et al., 2019) and MQ-RNN (Wen et al., 2018). Both use an autoregressive LSTM architecture as in"
TIME SERIES FORECASTING,0.41732283464566927,"Figure 1: Examples of bounding box uncertainties on the object detection task. The thicker boundary
represents the original model prediction, and the thinner lines represent ten quantiles (between
0.1 and 0.9) predicted via QFR. High-conﬁdence boundaries (such as those of the car) are closer
together, while low-conﬁdence boundaries (e.g., the bus which is blurred and partially occluded) are
asymmetrically spread out, reﬂecting the advantages of non-Gaussian uncertainty estimation."
TIME SERIES FORECASTING,0.421259842519685,"Mashlakov et al. (2021); DeepAR outputs Gaussians at each time step, while MQ-RNN estimates
ﬁve conditional quantiles, which are followed by linear interpolation."
TIME SERIES FORECASTING,0.4251968503937008,"We train a recurrent model with same LSTM architecture and replace the predictive component with
a quantile ﬂow. Each transformer τ(zi; hi) is a monotone neural network with one hidden layer of 20
neurons, conditioned on the output hi of the LSTM module, acting as the conditioner ci(y<i, x)."
TIME SERIES FORECASTING,0.42913385826771655,Table 4: Time series experiments
TIME SERIES FORECASTING,0.4330708661417323,"Method
ND
RMSE
CHK"
TIME SERIES FORECASTING,0.43700787401574803,"DeepAR
9.9%
0.692
89.15
MQ-RNN
9.9%
0.712
88.91
AQF (ours)
9.8%
0.723
88.02"
TIME SERIES FORECASTING,0.4409448818897638,"Results.
Figures 2a and 2b compare predictions from
DeepAR and AQF (in blue) on illustrative time series
(red). We take samples from both models and visualize the
(0.1, 0.9) quantiles in shaded blue. While both time series
underpredict, the Gaussian is overconﬁdent, while the
samples from AQF contain the true (red) curve. Moreover,
AQF quantiles expand in uncertain regions and contract
with more certainty. We also compare the three methods
quantitatively in Table 4. Both achieve similar levels of
accuracy (ND and RMSE are as deﬁned in Salinas et al. (2019), but we have included them in the
appendix as well), and AQF produces better uncertainties. We also found that for 37.2% of time
series, DeepAR and AQF performed comparably for CRPS (CRPS within 5% of each other); on
30.3% of time series DeepAR fared better, and on 32.1% AQF did best."
GENERATIVE MODELING,0.4448818897637795,"6.4
GENERATIVE MODELING"
GENERATIVE MODELING,0.44881889763779526,"A key feature of AQFs is that they can be used for generative modeling with expressive neural
parameterizations since they can be trained in the forward direction, unlike Huang et al. (2018).
We benchmark three different methods for generative ﬂows, MAF trained via maximum likelihood
(MAF-LL) and with our quantile loss (MAF-QL), and our proposed method, Neural Quantile Flow
(NAQF) with the same number of parameters on various UCI datasets. The models consist of an
LSTM layer followed by a FC layer, with 40 hidden states in each. The MAFs generate the parameters
for a Gaussian distribution from which the results are sampled."
GENERATIVE MODELING,0.452755905511811,"Results.
Figure 3 shows the error curves of the MAF models trained with log-likelihood
and with our loss.
Interestingly, there is little correlation between the two, which is
reminiscent of other model classes trained with different objectives (e.g., LL-Flows and"
GENERATIVE MODELING,0.4566929133858268,"(a) DeepAR Forecaster
(b) Autoregressive Quantile Flows"
GENERATIVE MODELING,0.46062992125984253,Figure 2: Examples of Predictive Uncertainties on the Times Series Forecasting Task
GENERATIVE MODELING,0.4645669291338583,Figure 3: Log likelihood and CRPS error curves for the MAF on three UCI Datasets.
GENERATIVE MODELING,0.468503937007874,"GANs); in some cases, likelihood can even diverge as we train with our loss.
Table 5
evaluates CRPS trained with each model. The best results are achieved with the NAQF, even
though it uses the same number of parameters, indicating the value of neural transformers."
GENERATIVE MODELING,0.47244094488188976,Table 5: CRPS on UCI datasets.
GENERATIVE MODELING,0.4763779527559055,"Dataset
MAF-LL
MAF-QL
NAQF
BSDS 300
.044
.036
.033
Miniboone
.567
.561
.525
Gas
.645
.565
.513
Power
.542
.506
.502
Hepmass
.617
.614
.523"
IMAGE GENERATION,0.48031496062992124,"6.5
IMAGE GENERATION"
IMAGE GENERATION,0.484251968503937,"We demonstrate the ability of our model
to
generate
samples
using
a
neural
parameterization by generating images of
digits from sklearn (Figure 4). We use 5 LSTM
layers with 80 neurons; all models are trained to
convergence with learning rate 1e-3, with 500
epochs for the NAQF and MAF-QL, while the MAF-LL was given an extra 500 epochs to reach
convergence. Additionally, we created a SVM discriminator to differentiate the real data versus the
sampled data from the model (D-Loss in Table 6, deﬁned formally in appendix). The better the
sampled data is, the lower the accuracy achieved by the discriminator."
IMAGE GENERATION,0.4881889763779528,Table 6: Image Generation Experiments
IMAGE GENERATION,0.4921259842519685,"Method
CRPS
LL
Q-Loss
D-Loss"
IMAGE GENERATION,0.49606299212598426,"MAF-LL
2.31
-45
0.625
0.978
MAF-QL
2.16
-6187
0.281
0.779
NAQF
2.14
n/a
0.253
0.723"
IMAGE GENERATION,0.5,"Results.
The NAQF generates signiﬁncantly
better samples that are the most challenging
to discriminate from real samples, and also
echieves the best CRPS values.
The MAF
method trained with LL achieves worst results
and visibly worse samples, despite having the
same number of parameters and being trained
for longer.
Note that despite the MAF-QL
having a considerably worse NLL, the samples generated are still more representative of the data,
according to the discriminative model, and visually the sample quality is much higher."
IMAGE GENERATION,0.5039370078740157,Figure 4: Images of digits sampled from four autoregressive ﬂow methods.
DISCUSSION AND CONCLUSION,0.5078740157480315,"7
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.5118110236220472,"Our work extends the framework of ﬂows Papamakarios et al. (2017; 2019) to use a novel learning
objectives.
Similarly, it generalizes implicit quantile networks Dabney et al. (2018b) into a
normalizing ﬂow framework. Our work is an alternative to variational (Kingma and Welling, 2014),
adversarial (Goodfellow et al., 2014), or hybrid (Larsen et al., 2016; Kuleshov and Ermon, 2017)
generative modeling and extend structured prediciton models (Lafferty et al., 2001; Ren et al., 2018).
Interestingly, quantile ﬂows can be seen as a generalization of quantile functions to higher dimensions.
While several authors previously proposed multi-variate generalizations of quantiles (Serﬂing, 2002;
Liu and Wu, 2009), neither of them possess all the properties of a univariate quantile function, and
ours is an additional proposal."
REFERENCES,0.515748031496063,REFERENCES
REFERENCES,0.5196850393700787,Christopher M Bishop. Mixture density networks. 1994.
REFERENCES,0.5236220472440944,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efﬁcient
reinforcement learning with stochastic ensemble value expansion.
In Advances in Neural
Information Processing Systems, pages 8234–8244, 2018."
REFERENCES,0.5275590551181102,"Nicola De Cao, Ivan Titov, and Wilker Aziz. Block neural autoregressive ﬂow, 2019."
REFERENCES,0.531496062992126,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural
Information Processing Systems 31, pages 4759–4770. Curran Associates, Inc., 2018."
REFERENCES,0.5354330708661418,"Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. CoRR, abs/1806.06923, 2018a. URL http://arxiv.
org/abs/1806.06923."
REFERENCES,0.5393700787401575,"Will Dabney, Mark Rowland, Marc G Bellemare, and Rémi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018b."
REFERENCES,0.5433070866141733,"Hadi M. Dolatabadi, Sarah Erfani, and Christopher Leckie. Invertible generative modeling using
linear rational splines, 2020."
REFERENCES,0.547244094488189,"Dheeru Dua and Casey Graff. Uci machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.5511811023622047,"Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline ﬂows, 2019."
REFERENCES,0.5551181102362205,"M.
Everingham,
L.
Van
Gool,
C.
K.
I.
Williams,
J.
Winn,
and
A.
Zisserman.
The
PASCAL
Visual
Object
Classes
Challenge
2007
(VOC2007)
Results.
http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html."
REFERENCES,0.5590551181102362,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pages 1050–1059.
PMLR, 2016."
REFERENCES,0.562992125984252,"T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of
the American Statistical Association, (10):359–378, 2007."
REFERENCES,0.5669291338582677,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014."
REFERENCES,0.5708661417322834,"Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, and Xiangyu Zhang. Bounding box
regression with uncertainty for accurate object detection. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019."
REFERENCES,0.5748031496062992,"Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
ﬂows. In International Conference on Machine Learning, pages 2078–2087. PMLR, 2018."
REFERENCES,0.5787401574803149,"Priyank Jaini, Kira A. Selby, and Yaoliang Yu. Sum-of-squares polynomial ﬂow, 2019."
REFERENCES,0.5826771653543307,"X. Jiang, M. Osl, J. Kim, and L. Ohno-Machado. Calibrating predictive model estimates to support
personalized medicine. Journal of the American Medical Informatics Association, 19(2):263–274,
2012."
REFERENCES,0.5866141732283464,"Alexander Jordan, Fabian Krüger, and Sebastian Lerch. Evaluating probabilistic forecasts with
scoringrules, 2018."
REFERENCES,0.5905511811023622,"Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions,
2018."
REFERENCES,0.594488188976378,"Diederik P Kingma and Max Welling. Stochastic gradient vb and the variational auto-encoder. In
Second International Conference on Learning Representations, ICLR, 2014."
REFERENCES,0.5984251968503937,"Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. Advances in neural information
processing systems, 29:4743–4751, 2016."
REFERENCES,0.6023622047244095,"Volodymyr Kuleshov and Stefano Ermon. Deep hybrid models: bridging discriminative and generative
approaches. In Uncertainty in Artiﬁcial Intelligence, 2017."
REFERENCES,0.6062992125984252,"Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/
paper/2015/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf."
REFERENCES,0.610236220472441,"Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression, 2018."
REFERENCES,0.6141732283464567,"John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Probabilistic
models for segmenting and labeling sequence data. 2001."
REFERENCES,0.6181102362204725,"F. Laio and S. Tamea. Veriﬁcation tools for probabilistic forecasts of continuous hydrological variables.
Hydrology and Earth System Sciences, 11(4):1267–1277, 2007. doi: 10.5194/hess-11-1267-2007.
URL https://hess.copernicus.org/articles/11/1267/2007/."
REFERENCES,0.6220472440944882,"Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther.
Autoencoding beyond pixels using a learned similarity metric. In International conference on
machine learning, pages 1558–1566. PMLR, 2016."
REFERENCES,0.6259842519685039,"Yufeng Liu and Yichao Wu. Stepwise multiple quantile regression estimation using non-crossing
constraints. Statistics and its Interface, 2(3):299–310, 2009."
REFERENCES,0.6299212598425197,"A. Mashlakov, T. Kuronen, L. Lensu, A. Kaarna, and S. Honkapuro. Assessing the performance
of deep learning models for multivariate probabilistic energy forecasting. Applied Energy, 285:
116405, 2021. doi: https://doi.org/10.1016/j.apenergy.2020.116405."
REFERENCES,0.6338582677165354,"Thomas Müller, Brian McWilliams, Fabrice Rousselle, Markus Gross, and Jan Novák. Neural
importance sampling, 2019."
REFERENCES,0.6377952755905512,"Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Hybrid models with deep and invertible features, 2019."
REFERENCES,0.6417322834645669,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
estimation. arXiv preprint arXiv:1705.07057, 2017."
REFERENCES,0.6456692913385826,"George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019."
REFERENCES,0.6496062992125984,"Hongyu Ren, Russell Stewart, Jiaming Song, Volodymyr Kuleshov, and Stefano Ermon. Learning
with weak supervision from physics and data-driven constraints. AI Magazine, 39(1):27–38, 2018."
REFERENCES,0.6535433070866141,"Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows, 2016."
REFERENCES,0.65748031496063,"David Salinas, Valentin Flunkert, and Jan Gasthaus.
Deepar: Probabilistic forecasting with
autoregressive recurrent networks, 2019."
REFERENCES,0.6614173228346457,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks, 2019."
REFERENCES,0.6653543307086615,"Suchi Saria. Individualized sepsis treatment using reinforcement learning. Nature Medicine, 24(11):
1641–1642, 11 2018. ISSN 1078-8956. doi: 10.1038/s41591-018-0253-x."
REFERENCES,0.6692913385826772,"Robert Serﬂing. Quantile functions for multivariate analysis: approaches and applications. Statistica
Neerlandica, 56(2):214–232, 2002."
REFERENCES,0.6732283464566929,"Benjamin Van Roy, Dimitri P Bertsekas, Yuchun Lee, and John N Tsitsiklis. A neuro-dynamic
programming approach to retailer inventory management.
In Decision and Control, 1997.,
Proceedings of the 36th IEEE Conference on, volume 4, pages 4052–4057. IEEE, 1997."
REFERENCES,0.6771653543307087,"Antoine Wehenkel and Gilles Louppe. Unconstrained monotonic neural networks, 2021."
REFERENCES,0.6811023622047244,"Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A multi-horizon
quantile recurrent forecaster, 2018."
REFERENCES,0.6850393700787402,"Keenon Werling, Arun Tejasvi Chaganty, Percy S Liang, and Christopher D Manning. On-the-job
learning with bayesian decision theory. Neural Information Processing Systems, 2015."
REFERENCES,0.6889763779527559,"A
UCI DATASETS EXPANDED"
REFERENCES,0.6929133858267716,"Yacht. The yacht dataset consists of 308 instances, 6 features and 1 continuous target.
Boston. The boston house prices dataset consists of 506 instances, 14 features and 1 continuous
target.
Concrete. The concrete dataset consists of 1030 instances, 8 features and 1 continuous target.
Energy. The energy efﬁciency dataset consists of 768 instances, 10 features and 1 continuous target.
Classiﬁcation Datasets
KC2. The kc2 dataset consists of 522 instances, 22 features and 1 binary target.
Diabetes. The diabetes dataset consists of 769 instances, 9 features and 1 binary target.
Multivariate Datasets
ENB. The enb dataset consists of 768 instances, 10 features and 2 continous targets."
REFERENCES,0.6968503937007874,"B
PROOF OF PROPOSITION"
REFERENCES,0.7007874015748031,"Proposition 2. For a CDF F : R →[0, 1] and y ∈R, the CRPS and quantile losses are equivalent:"
REFERENCES,0.7047244094488189,"L(1)(F, y′) = a · L(2)(F −1, y′) + b
a, b ∈R, a > 0
(7)"
REFERENCES,0.7086614173228346,Proof. Recall that the pinball loss is deﬁned as
REFERENCES,0.7125984251968503,"Lα(y, f) = α(y −f) if y ≥f and (1 −α)(f −y) otherwise."
REFERENCES,0.7165354330708661,Observe that we can equivalently write
REFERENCES,0.7204724409448819,"2Lα(F −1(α), y′) = |F −1(α) −y′| + 2(α −0.5)(F −1(α) −y′)"
REFERENCES,0.7244094488188977,We take the expected pinball loss and perform the change of variables y = F −1(α):
REFERENCES,0.7283464566929134,"2 ·
Z 1"
REFERENCES,0.7322834645669292,"0
Lα(F −1(α), y′)dα =
Z 1 0"
REFERENCES,0.7362204724409449,"
|F −1(α) −y′| + 2(α −0.5)(F −1(α) −y′)

dα =
Z 1"
REFERENCES,0.7401574803149606,"0
[|y −y′| + (2F(y) −1)(y −y′)] f(y)dy,"
REFERENCES,0.7440944881889764,where f(y) is the probability density function associated with F.
REFERENCES,0.7480314960629921,"Using integration by parts and the fact that F(x′) −I(x′ ≤x) →0 as x →∞and as x →−∞, we
obtain
Z 1"
REFERENCES,0.7519685039370079,"0
[|x −x′| + (1 −2F(x))(x −x′)] f(x)dy =
Z ∞"
REFERENCES,0.7559055118110236,"−∞
(F(x′) −I(x′ ≤x))2 dx."
REFERENCES,0.7598425196850394,"C
EXTRA FIGURES"
REFERENCES,0.7637795275590551,"(a) OpenML KC2 ROC
(b) OpenML Diabetes ROC"
REFERENCES,0.7677165354330708,"D
SYNTHETIC EXPERIMENTS"
REFERENCES,0.7716535433070866,"In addition to the real-world dataset experiments, we also conducted experiments with synthetic
datasets, which provides additional insights into the properties of autoregressive quantile ﬂows."
REFERENCES,0.7755905511811023,Table 8: Calibration Metrics and Accuracy
REFERENCES,0.7795275590551181,"Metric
AQF
Gaussian."
REFERENCES,0.7834645669291339,"RMSE
1.017
1.016
Calibration 0.25
0.236
0.342
Calibration 0.5
0.493
0.510
Calibration 0.75
0.746
0.684"
REFERENCES,0.7874015748031497,Table 9: Results on the 1D synthetic data benchmarks.
REFERENCES,0.7913385826771654,"Check Score
CRPS
Dataset
AQF
Gaussian
MDN
Poisson
True
AQF
Gaussian
MDN
Poisson
True
Gaussian
0.302
0.330
0.308
-
0.294
0.586
0.600
0.610
-
0.581
Beta
0.233
0.251
240
-
0.224
0.460
0.547
0.493
-
0.443
Poisson
0.487
0.517
0.537
0.502
0.452
0.965
1.027
1.059
0.993
0.898"
REFERENCES,0.7952755905511811,"D.1
ONE-DIMENSIONAL EXPERIMENTS"
REFERENCES,0.7992125984251969,"We start with a series of one-dimensional experiments involving the a simple 1D function to which
we add additive Gaussian, Beta, and Poisson noise."
REFERENCES,0.8031496062992126,"The Gaussian and Beta distribution experiments were generated from a base function of f(x′) =
sin(x′/2) + x′/10 such that x = f(x′) + N(0, 1) and x = f(x′) + B(2, 5) respectively. The
Poisson distribution was generated from P(f(x′) + C), where C is a constant term to make all terms
non-negative."
REFERENCES,0.8070866141732284,"We ﬁt these functions using autoregressive quantile ﬂows (AQFs) as well as the following Baselines:
Gaussian regression, Mixture Density Networks (MDNs), and Poisson regression. All models
implement the same neural networks and differ only in how the ﬁnal layer represents aleatoric
uncertainty."
REFERENCES,0.8110236220472441,"We calculated check score and CRPS metrics for each model. In addition, we calculated the metrics
for the true distribution, as a upper bound on the performance of the models. As shown in Table 9,
AQF is much closer to the true distribution compared to the other models with Gaussian assumptions,
especially when performed on non-Gaussian distributions."
REFERENCES,0.8149606299212598,"This shows that ﬁtting an expressive AQF model better captures predictive uncertainty than alternative
models that make parametric assumptions on the output distribution. In other words, using a
parametric distribution (e.g., Poisson or Gaussian) works well if the underlying data actually follows
that distribution. When using an AQF, we don’t need to select a predictive distribution type, it just
works out of the box as if we knew it."
REFERENCES,0.8188976377952756,Table 7: Calibration Error on 1D datasets
REFERENCES,0.8228346456692913,"Dataset
QRF
QRO
Gaussian
Gaussian
1.01
1.03
0.952
Beta
1.39
1.75
4.63
Poisson
2.76
2.57
17.64"
REFERENCES,0.8267716535433071,"We also report calibration error, deﬁned as the absolute
difference between the percentage of points below the
predicted quantile curves and the actual quantile value
(Table 7). For a perfectly Gaussian noise distribution,
the QRF (analogous to AQF in 1-dimension) and QRO
methods (QRO consists of ﬁtting a separate estimator
for 99 quantiles indexed by α
=
0.01, ..., 0.99)
are comparable to a Gaussian regressor in terms of
calibration. When the distributions become non-Gaussian, QRF is adaptable whereas the Gaussian
fails to capture the distribution. Thus, the Gaussian maximum likelihood loss tends to be overconﬁdent,
a fact also demonstrated in Kuleshov et al. (2018); AQF addresses the overconﬁdence."
REFERENCES,0.8307086614173228,"Supervised Accuracy and Calibration at Key Quantiles
We also report calibration values at key
quantiles (25%, 50%, 75%) in Table 8). We also measure predictive accuracy in terms of RMSE and
demonstrate that our method obtains good estimates of uncertainty without sacriﬁcing supervised
performance in terms of regression quality."
REFERENCES,0.8346456692913385,"(a) AQF
(b) Gaussian"
REFERENCES,0.8385826771653543,"Figure 6: 80% Conﬁdence Intervals obtained via AQF (left) and Gaussian regression (right). Note
that conﬁdence intervals obtained from Gaussian regression (right, blue) differ signiﬁcantly from the
true conﬁdence intervals (right, red). Conﬁdence intervals obtained from AQF (left, blue) perfectly
overlap with the true intervals (left, red)."
REFERENCES,0.84251968503937,"In Figure 6a and 6b, we report 80% conﬁdence intervals for AQF and Gaussian regression methods
on the synthetic beta noise data for a visualization of the quantile predictions."
REFERENCES,0.8464566929133859,"D.2
MULTI-DIMENSIONAL EXPERIMENTS"
REFERENCES,0.8503937007874016,"We also conducted multi-dimensional experiments with a d-dimensional target x; we model the
ﬁrst dimension by x0 = 5 sin(x′/3) + x′ + N(0, 1); then, further dimensions are modeled by
yd = 5 sin(xd−1/3) + xd−1 + N(0, 1). We use both the marginal CRPS and the joint CRPS metrics,
as described in Jordan et al. (2018), for both a simple 2d-experiment as well as a more complex
50-dimensional experiment."
REFERENCES,0.8543307086614174,"The methods we evaluate are our proposed AQF, a Gaussian Markov Random Field (a type of
structured prediction algorithm that assumes a multi-variate Gaussian distribution on the output), and
a model in which we train a multi-variate Gaussian distribution using quantile loss instead of log
likelihood (denoted as “Quantile"" in the table). As in the 1D synthetic experiments, we also compare
metrics against samples drawn from the true distribution to obtain an upper bound."
REFERENCES,0.8582677165354331,"Table 10 shows that AQF does a better job at capturing the high-dimensional distribution than models
that assume a more restricted parametric form, such as the Gaussian MRF. Simply replacing the
objective function with quantile loss instead of standard Gaussian noise does not provide as signiﬁcant
of a gain as AQF does. In other words, existing models (e.g., a Gaussian MRF) are not sufﬁciently
expressive to capture the multi-variate distribution that he we have deﬁned. In contrast the AQF
layer is able to achieve close to approach the theoretical upper bound without making any additional
modeling assumptions."
REFERENCES,0.8622047244094488,Table 10: Results on Multidimensional Synthetic Datasets
REFERENCES,0.8661417322834646,"CRPS (Marginal)
CRPS (Joint)
Dataset
AQF
Gaussian
Quantile
True
AQF
Gaussian
Quantile
True
2D
0.624
0.669
0.634
0.585
0.686
0.701
0.695
0.646
50D
1.045
1.133
1.058
0.970
1.238
1.345
1.281
1.163"
REFERENCES,0.8700787401574803,"E
ALGORITHM"
REFERENCES,0.8740157480314961,"Algorithm 1 Training a Neural Network f with the Quantile Loss
Sample α = U(0, 1)
Sample a training instance (x′, x) from the dataset (or a batch of instances)
Compute prediction ypred = f(x′, α)
Compute quantile loss Lα(x, xpred).
Take a gradient step quantile loss Lα(x, xpred)."
REFERENCES,0.8779527559055118,"F
ADDITIONAL DEFINITIONS FOR USED METRICS"
REFERENCES,0.8818897637795275,"F.1
CALIBRATION MAE"
REFERENCES,0.8858267716535433,"If we have a 1-dimensional model which predicts a curve F(q) for quantile q, then we have that our
Calibration MAE on test set X is given by 1
99"
X,0.889763779527559,"99
X"
X,0.8937007874015748,"q=1
|P(X ≤F(q)) −q|"
X,0.8976377952755905,"F.2
MAP@50"
X,0.9015748031496063,"MAP@50 denotes the mean average precision for an IoU value of 0.5. IoU is computed as, given
ground truth boxes G and predicted box B, G∩B"
X,0.905511811023622,"G∪B ≥0.5. MAP is then calculated as the AUC of the
precision-recall curve given the IoU."
X,0.9094488188976378,"F.3
RMSE AND ND (TIME SERIES)"
X,0.9133858267716536,"We will deﬁne data point zi,t to be the t-th time in the i-th context window, and ˆz to be the prediction
for point z."
X,0.9173228346456693,Then RMSE is deﬁned as
X,0.9212598425196851,RMSE = s
X,0.9251968503937008,"1
N(T −t0) X"
X,0.9291338582677166,"i,t
(zi,t −ˆzi,t)2"
X,0.9330708661417323,ND (Normalized Deviation) is calculated as ND = P
X,0.937007874015748,"i,t |zi,t −ˆzi,t|
P"
X,0.9409448818897638,"i,t |zi,t|"
X,0.9448818897637795,"F.4
LL, Q-LOSS, AND D-LOSS (IMAGE GENERATION)"
X,0.9488188976377953,"Log Likelihood and Q-Loss (quantile loss as deﬁned in our paper) with respect to the digits generation
task are optimization objectives and not metrics. We measure these with to depict how one is affected
in the same model trained on the other loss function."
X,0.952755905511811,"D-Loss (D : Rn →{0, 1}) is measured as the accuracy with which a SVM can discern the difference
between generated samples S and true samples Y, which are labeled 0 and 1 respectively."
X,0.9566929133858267,"Given validation set with m data points and labels X ∈{0, 1}, we have that"
X,0.9606299212598425,"D-Loss = 1 m m
X"
X,0.9645669291338582,"i
D(x′(i))x(i) + (1 −D(x′(i)))(1 −x(i))"
X,0.968503937007874,"F.5
MNIST EXPERIMENTS"
X,0.9724409448818898,"For MNIST, we trained autoregressive models with the PixelCNN architecture, Each model was
trained for 300 epochs with a learning rate of 1e-3, and has hidden layer size 784."
X,0.9763779527559056,"(a) PixelMAF-LL
(b) PixelMAF-QL."
X,0.9803149606299213,Figure 7: MNIST samples
X,0.984251968503937,Table 11: MNIST Generation Experiments
X,0.9881889763779528,"Method
U-CRPS
CRPS
NLL"
X,0.9921259842519685,"PixelMAF-LL
.128
.279
-6011
PixelMAF-QL
.099
.215
-5888"
X,0.9960629921259843,"Results.
From the samples in Figure 7, the results generated from the MAF-QL seems to be much
more visually accurate than MAF-LL, which is trained on a LL-based loss, even though they have the
same architecture."
