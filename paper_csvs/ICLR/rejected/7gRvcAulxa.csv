Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017857142857142857,"Adversarial examples pose a unique challenge for deep learning systems. Despite
recent advances in both attacks and defenses, there is still a lack of clarity and
consensus in the community about the true nature and underlying properties of ad-
versarial examples. A deep understanding of these examples can provide new in-
sights towards the development of more effective attacks and defenses. Driven by
the common misconception that adversarial examples are high-frequency noise,
we present a frequency-based understanding of adversarial examples, supported
by theoretical and empirical ﬁndings. Our analysis shows that adversarial ex-
amples are neither in high-frequency nor in low-frequency components, but are
simply dataset dependent. Particularly, we highlight the glaring disparities be-
tween models trained on CIFAR-10 and ImageNet-derived datasets. Utilizing this
framework, we analyze many intriguing properties of training robust models with
frequency constraints, and propose a frequency-based explanation for the com-
monly observed accuracy vs robustness trade-off."
INTRODUCTION AND BACKGROUND,0.0035714285714285713,"1
INTRODUCTION AND BACKGROUND"
INTRODUCTION AND BACKGROUND,0.005357142857142857,"Since the introduction of adversarial examples by Szegedy et al. (2014), there has been a curiosity
in the community around the nature and mechanisms of adversarial vulnerability. There exists an
ever-growing body of work focused on attacking neural networks starting with the simple FGSM
(Goodfellow et al., 2015), followed by the advanced PGD (Madry et al., 2018), a stronger C&W
attack (Carlini & Wagner, 2016), the sparser Deep Fool (Su et al., 2019) and recently even a pa-
rameter free Auto-Attack (Croce & Hein, 2020). These methods and algorithms are consistently
countered by the adversarial defense community, starting with distillation-based methods (Papernot
et al., 2016), logit-based approaches (Kannan et al., 2018), then moving on to the simple, yet pow-
erful PGD training (Madry et al., 2018), ensemble-based methods (Tram`er et al., 2018) and various
other schemes (Zhang et al., 2019; Xie et al., 2019). Despite the immense progress made by the
ﬁeld, there exist many unanswered questions and ambiguities regarding these methods and adver-
sarial examples themselves. Several works (Athalye et al., 2018; Kolter & Wong, 2018; Croce &
Hein, 2020; Carlini & Wagner, 2017) have raised doubts about the efﬁcacy of many methods and
have made appeals to the research community to be more vigilant and skeptical with new defenses."
INTRODUCTION AND BACKGROUND,0.007142857142857143,"Meanwhile, there exists a thriving research corpus dedicated to deeply studying and understanding
adversarial examples themselves. Ilyas et al. (2019) presented a feature-based analysis of adversarial
examples, while Jere et al. (2019) presented preliminary work on PCA-based analysis of adversarial
examples, which was followed up with Jere et al. (2020) offering a nuanced view of the same through
the lens of SVD. Ortiz-Jimenez et al. (2020) derive insights from the margins of classiﬁers."
INTRODUCTION AND BACKGROUND,0.008928571428571428,"Given the intriguing nature of adversarial examples, another way of examining them is through the
signal processing perspective of frequencies. Tsuzuku & Sato (2019) ﬁrst proposed a frequency
framework by studying the sensitivity of CNN’s for different Fourier bases. Yin et al. (2019) then
pursued a related direction where they explored the frequency properties of neural networks with
respect to additive noise. Abello et al. (2021) explore how the frequency properties of the image
itself affect the model’s outputs and robustness. Caro et al. (2021) studied whether convolution op-
erations themselves have an intrinsic frequency bias. Guo et al. (2019) came up with the ﬁrst variant
of adversarial attacks which target the low frequencies and Sharma et al. (2019) strengthened this
line of thought by showing that such attacks had a high success rate against adversarially defended"
INTRODUCTION AND BACKGROUND,0.010714285714285714,Under review as a conference paper at ICLR 2022
INTRODUCTION AND BACKGROUND,0.0125,"models. Deng & Karam (2020) proposed a method of generating adversarial attacks in the frequency
domain itself. Complementary to these, there have been efforts by Lorenz et al. (2021) and Wang
et al. (2020a) in detecting or mitigating adversarial examples by training in the frequency domain."
INTRODUCTION AND BACKGROUND,0.014285714285714285,"These works also analyzed the nature of adversarial examples under the purview of frequencies and
tried to arrive at an explanation for their nature. Wang et al. (2020b) hypothesized how CNNs ex-
ploit high frequency components, leading to less robust models, which is also the primary argument
for a class of pre-processing based defenses, e.g., those based on JPEG. Wang et al. (2020d) also had
arguments in support of this conjecture, based on their analysis on CIFAR-10 (Krizhevsky, 2009).
It is confounding that these results are at odds with the successful low frequency adversarial attacks
by Sharma et al. (2019) and raises the pertinent question: What is the true nature of adversarial
examples in the frequency domain? Our work challenges some pre-existing notions about the nature
of adversarial examples in the frequency domain and arrives at a more nuanced understanding that
is well rooted in theory and backed by extensive empirical observations spanning multiple datasets.
Some of our observations overlap with insights from the concurrent work by Bernhard et al. (2021)
and offers additional evidence in this ongoing debate. Based on these, we arrive at a new frame-
work that explains many properties of adversarial examples, through the lens of frequency analysis.
We also carry out the ﬁrst detailed analysis on the behaviour of frequency-constrained adversarial
training. Our key contributions can be summarized as follows:"
INTRODUCTION AND BACKGROUND,0.01607142857142857,"• We show that adversarial examples are neither high frequency nor low frequency phenom-
ena. It is more nuanced than this dichotomous explanation.
• We propose variations of adversarial training by coupling it with frequency-space analysis,
leading us to some intriguing properties of adversarial examples.
• We propose a new framework of frequency-based robustness analysis that also helps ex-
plain and control the accuracy vs˙robustness trade-off during adversarial training."
INTRODUCTION AND BACKGROUND,0.017857142857142856,"The rest of the paper is organized as follows: we ﬁrst start off with basic notations and preliminaries.
Then we introduce our main ﬁndings about adversarial examples in frequency domain and subse-
quently present a detailed analysis about their properties, complemented by extensive experiments."
PRELIMINARIES,0.019642857142857142,"2
PRELIMINARIES"
PRELIMINARIES,0.02142857142857143,"We denote a neural network with parameter θ by y = h(x; θ), which takes in an input image x ∈
RH×W (omitting the channel dimension for brevity) and outputs y ∈RC where C is the number of
classes. Let D and D−1 represent the forward Type-II DCT (Discrete Cosine Transform) (Ahmed
et al., 1974) and its corresponding inverse. The DCT breaks down the input signal and expresses
it as a linear combination of cosine basis functions. Its inverse recovers the input signal from this
representation. For a 1-D signal, the kth-freq of x ∈RN and its corresponding inverse is given by"
PRELIMINARIES,0.023214285714285715,"D(x)[k] = g[k] = N−1
X"
PRELIMINARIES,0.025,"n=0
xnλk cos (2n + 1)kπ"
N,0.026785714285714284,"2N
,
(1)"
N,0.02857142857142857,"D−1(x) = x[n] = N−1
X"
N,0.030357142857142857,"k=0
g[k]λk cos (2n + 1)kπ"
N,0.03214285714285714,"2N
,
(2)"
N,0.033928571428571426,"where k = {0, 1, . . . , N −1} and λk = 
  q"
N,0.03571428571428571,"1
N for k = 0
q"
N,0.0375,"2
N else.
(3)"
N,0.039285714285714285,"We denote an adversarial attack that is bound by budget ϵ by
max
||δ||p≤ϵ L(h(x + δ; θ), y)
(4)"
N,0.04107142857142857,"where L is the loss associated with the network and δ is the adversarial noise bounded under a
deﬁned Lp norm to be less than perturbation budget ϵ. We perform a standard PGD-style up-
date (Madry et al., 2018) to solve this maximization problem via gradient ascent and for an attack
bounded by an Lp norm and step size α, the adversarial noise is given by"
N,0.04285714285714286,"δ = arg max
||V ||p≤α
V T ∇xL(h(x; θ), y)
(5)"
N,0.044642857142857144,Under review as a conference paper at ICLR 2022
N,0.04642857142857143,"where V is the direction of steepest normalized descent. Now, to generate an adversarial example
that consists of certain frequencies, we restrict its adversarial noise δ to a subspace S deﬁned by
S = Span{f1, f2, . . . , fk}, where fi are orthogonal DCT modes and k ≤N,"
N,0.048214285714285716,"δf = arg max
||V ||p≤α
V T D−1(D(∇xL(h(x; θ), y)) ⊙M)
(6)"
N,0.05,"where Mz(X) =
1 if D(Xz) ∈S
0 if D(Xz) ̸∈S
is the mask to select frequencies.
(7)"
N,0.05178571428571429,"In our work, we consider the L∞and L2 norms, solving for which gives us the update steps:"
N,0.05357142857142857,"δf = α · Sgn(D−1(D(∇xL ⊙M))) for L∞and
(8)"
N,0.055357142857142855,"δf = α · D−1

D

∇xL ⊙M
||∇xL ⊙M||2"
N,0.05714285714285714,"
for L2
(9)"
N,0.05892857142857143,"We refer to this method as DCT-PGD in the rest of the paper. Note that the manual step size selection
of standard PGD is not always accurate, leading to discrepancies in robustness measures as illus-
trated in Lorenz et al. (2021). Hence, we provide our results and observations with a DCT version of
Auto-Attack. Unless mentioned otherwise, we utilize the ResNet-18 architecture for all models in
our experiments. We use the term adversarial training to refer to the method by Madry et al. (2018)
for all models, with the exception for ImageNet models where we use Adversarial training for free
method (Shafahi et al., 2019). We utilize L∞norm with ϵ of 4/255 for TinyImageNet and ImageNet
datasets and ϵ of 8/255 for CIFAR-10 in all our experiments. Exact training details are included in
the Appendix A.2. The terms low frequencies refer to frequency bands 0 to 32 and high frequencies
refer to frequency bands bands 33 to 63."
N,0.060714285714285714,"3
WHY DO WE NEED A FREQUENCY PERSPECTIVE?"
N,0.0625,"The focus of the community has been mostly on generating adversarial examples which are indis-
tinguishable to humans, but can easily fool models. This notion gave rise to the incorrect assump-
tion that since these perturbations are imperceptible to humans and they generally have to be in
higher frequencies. The assumption was solidiﬁed when various pre-processing defense methods
like Gaussian blur and JPEG showed initial success, further adding to conﬁrmation bias. The fal-
lacy is a classic case of Post Hoc Ergo Propter Hoc, i.e., the outcome of events is inﬂuenced by the
mere ordering. Most of these experiments were centered only around CIFAR-10 and one can easily
observe that the efﬁcacy of such methods are questionable when extended to larger datasets like
ImageNet and TinyImageNet (e.g., see (Dziugaite et al., 2016; Das et al., 2017; Xu et al., 2018)).
This incorrect assumption has also led to claims about adversarial training shifting the importance
of frequencies from the higher to the lower end of the spectrum (Wang et al., 2020c;b). As we show
in the subsequent sections, this is not entirely true."
N,0.06428571428571428,"We contend that this entire framework of investigating adversarial examples (e.g., blocking high
frequency components using Gaussian blur pre-processing) is ﬂawed, as one cannot verify the con-
verse setting of blocking low frequency components. This is because low frequency components are
inherently tied with labels (Wang et al., 2020b), conﬂating the two phenomena. Contrary to these,
we argue and show that adversarial examples are neither high frequency nor low frequency and are
dependent on the dataset."
NATURE OF ADVERSARIAL SAMPLES IN FREQUENCY SPACE,0.06607142857142857,"4
NATURE OF ADVERSARIAL SAMPLES IN FREQUENCY SPACE"
NOISE GRADIENTS,0.06785714285714285,"4.1
NOISE GRADIENTS"
NOISE GRADIENTS,0.06964285714285715,"Measuring the change of output with respect to the input is a fundamental aspect of system design.
Whether it is a controls circuit or a mathematical model, the measure dy"
NOISE GRADIENTS,0.07142857142857142,"dx gives us valuable infor-
mation about the working of the model. When the model in question is a black box, like a neural
network, the measure is invaluable as often it is our only insight into the inner mechanisms of the
model. In the case of a classiﬁer, the measure dy"
NOISE GRADIENTS,0.07321428571428572,"dx is a tensor that is the same size as the input, which
tells us about the impact of each pixel in input x on the resulting output y. Drucker & Le Cun (1991)"
NOISE GRADIENTS,0.075,Under review as a conference paper at ICLR 2022
NOISE GRADIENTS,0.07678571428571429,"ﬁrst applied this concept on neural networks and called them input gradients. Over the recent years,
this measure and its variants have found a new home in the model interpretability community (Sel-
varaju et al., 2016; Wang et al., 2019), where it forms the bedrock for various improvements."
NOISE GRADIENTS,0.07857142857142857,"Taking a cue from this, we propose to measure dy"
NOISE GRADIENTS,0.08035714285714286,"dδ or Noise Gradients, which inform us about the
regions of noise, which have maximal impact on the output y. In our work, we are more interested
in the frequency properties of adversarial examples, and hence take this one step further and propose
to measure the DCT of noise gradients, i.e., D

dy
dδ

or D (∇δY ). In a sense, we are measuring the"
NOISE GRADIENTS,0.08214285714285714,"model’s reaction to different frequency components in the adversarial input. This tensor D (∇δY )
(which has same shape as input) will point us towards the speciﬁc frequencies that affect the output
y of the model. To analyze the adversarial frequency properties of a given dataset, we calculate
the average noise gradients with respect to the model, under both normal training and adversarial
training paradigms. Once computed, it will paint a picture about the interplay of adversarial noise
and frequencies."
ANALYSIS OF NOISE GRADIENTS,0.08392857142857142,"4.1.1
ANALYSIS OF NOISE GRADIENTS"
ANALYSIS OF NOISE GRADIENTS,0.08571428571428572,"We deﬁne the quantity D (∇δY )f as the noise gradient at frequency f. Note that this quantity is
useful because it differs from D(δ) by at most a constant multiple, i.e.,"
ANALYSIS OF NOISE GRADIENTS,0.0875,"D(δ) ∝D (∇δY )
(10)"
ANALYSIS OF NOISE GRADIENTS,0.08928571428571429,"Proof. Let ˆx = x + δ where δ is the adversarial noise, then"
ANALYSIS OF NOISE GRADIENTS,0.09107142857142857,"∇δY = ∇xY = ∇ˆxY
and
∇δL = ∇xL = ∇ˆxL
(11)"
ANALYSIS OF NOISE GRADIENTS,0.09285714285714286,"We have (from Appendix A.1),
∇xL ∝∇xY
(12)"
ANALYSIS OF NOISE GRADIENTS,0.09464285714285714,"From the deﬁnition of the PGD update step, we have"
ANALYSIS OF NOISE GRADIENTS,0.09642857142857143,"δ = α · ∇xL = α · ∇δL
(13)"
ANALYSIS OF NOISE GRADIENTS,0.09821428571428571,"for some constant α. Taking DCT of both sides, and by the linearity of the DCT, we have"
ANALYSIS OF NOISE GRADIENTS,0.1,"D(δ) = D(α · ∇δL) = α · D(∇δL), and therefore, D(δ) ∝D(∇δL).
(14)"
ANALYSIS OF NOISE GRADIENTS,0.10178571428571428,Now from equation 11 and equation 12 we get
ANALYSIS OF NOISE GRADIENTS,0.10357142857142858,"D(∇δL) ∝D(∇δY )
(15)"
ANALYSIS OF NOISE GRADIENTS,0.10535714285714286,"Using this in equation 14, we have
D(δ) ∝D(∇δY )
(16)"
ANALYSIS OF NOISE GRADIENTS,0.10714285714285714,We see that the term D(∇δY ) corresponds to the frequencies that are affected by adversarial noise.
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.10892857142857143,"4.1.2
EMPIRICAL OBSERVATION OF NOISE GRADIENTS"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.11071428571428571,"We compute the average DCT of noise gradients over validation sets of TinyImageNet, CIFAR-10,
and ImageNet datasets for models with normal and adversarial training under attack from a PGD-
based L∞adversary. The resulting tensors are visualized in Figure 1a. It shows the path taken by
the PGD attack in the frequency domain under different scenarios for different datasets. We see
that for normally trained CIFAR-10 models, the DCT of noise gradient activations are towards the
higher frequencies and they gradually shift towards lower frequencies once the model is adversari-
ally trained. Whereas for TinyImageNet and ImageNet models, we observe that the activations are
already in lower-mid frequencies and adversarial training further concentrates them. These results
clearly establish the following:"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.1125,"• The DCT content of PGD attacks is highly dataset-dependent and we cannot make general
arguments regarding frequency nature of adversarial samples just based on training."
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.11428571428571428,Under review as a conference paper at ICLR 2022
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.11607142857142858,"0
10
20
30 0 10 20 30"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.11785714285714285,Cifar-10 Normal
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.11964285714285715,"0
20
40
60 0 20 40 60"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.12142857142857143,TinyImageNet Normal
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.12321428571428572,"0
100
200 0 50 100 150 200"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.125,ImageNet Normal
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.12678571428571428,"0
10
20
30 0 10 20 30"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.12857142857142856,Cifar-10 Adversarial
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.13035714285714287,"0
20
40
60 0 20 40 60"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.13214285714285715,TinyImageNet Adversarial
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.13392857142857142,"0
100
200 0 50 100 150 200"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.1357142857142857,ImageNet Adversarial 50 100 150 200 250
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.1375,"(a)
(b)"
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.1392857142857143,"Figure 1: (a) The DCT of Noise Gradients averaged across the validation sets, visualized with
histogram equalization. (b) shows the standard 8×8 DCT block with the all 64 frequencies arranged
in zigzag order."
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.14107142857142857,"• The notion that adversarial training shifts the model focus from higher to lower frequencies
is not entirely true. In many datasets, the model is already biased towards the lower end of
the spectrum even before adversarial training."
EMPIRICAL OBSERVATION OF NOISE GRADIENTS,0.14285714285714285,"• To verify that this phenomenon is attributed to the dataset alone, we also observe similar be-
haviour across other architectures (Appendix A.5), across different image sizes (Appendix
A.6) and for different attacks like L2 (Appendix A.7)."
MEASURING IMPORTANCE OF FREQUENCY COMPONENTS,0.14464285714285716,"5
MEASURING IMPORTANCE OF FREQUENCY COMPONENTS"
MEASURING IMPORTANCE OF FREQUENCY COMPONENTS,0.14642857142857144,"To examine the properties and behaviour of adversarial examples in the frequency domain, we also
craft various empirical metrics that measure the importance of frequency components under various
paradigms."
IMPORTANCE BY VULNERABILITY,0.14821428571428572,"5.1
IMPORTANCE BY VULNERABILITY"
IMPORTANCE BY VULNERABILITY,0.15,"We measure the importance of a frequency component by measuring the attack success rate when an
adversarial attack is constrained to frequency f. Essentially, we are quantifying the importance by
measuring the expected vulnerability of each frequency. This amounts to measuring the accuracy of
h(x+δf), where δf is the adversarial perturbation that is constrained to frequency f, obtained using
the aforementioned DCT-PGD method. A lower accuracy of the model for a particular δf indicates
a more important frequency f. In Figure 2, we visualize the accuracy of models with both normal
training and adversarial training across different datasets under this setting. We see that only in the
case of CIFAR-10, the trends for normal training and adversarial training are reversed, indicating
that attacks constrained to higher frequencies are more successful for normal models, while lower
frequency attacks are more effective on the adversarially trained models. In TinyImageNet and Im-
ageNet datasets, we see that the overall trend remains same across the two training paradigms with
adversarial training improving robustness across the spectrum. To obtain a high level view, we de-
sign another set of experiments where instead of attacking individual frequency components, we
restrict the attack to frequency ranges (or bands, set of 16 equal divisions of the spectrum). The re-
sults of these under DCT-PGD version of Auto-Attack are shown in Figure A.7. In their work, Wang
et al. (2020b) had claimed that low frequency perturbations cause visible changes in the image, thus
defeating the purpose of imperceptibility clause of adversarial examples. However, we ﬁnd that for"
IMPORTANCE BY VULNERABILITY,0.15178571428571427,Under review as a conference paper at ICLR 2022
IMPORTANCE BY VULNERABILITY,0.15357142857142858,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.15535714285714286,Attack Frequency 0.2 0.4 0.6 0.8
IMPORTANCE BY VULNERABILITY,0.15714285714285714,Accuracy
IMPORTANCE BY VULNERABILITY,0.15892857142857142,CIFAR-10 Normal
IMPORTANCE BY VULNERABILITY,0.16071428571428573,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.1625,Attack Frequency 0.0 0.1 0.2 0.3 0.4 0.5
IMPORTANCE BY VULNERABILITY,0.16428571428571428,Accuracy
IMPORTANCE BY VULNERABILITY,0.16607142857142856,TinyImageNet Normal
IMPORTANCE BY VULNERABILITY,0.16785714285714284,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.16964285714285715,Attack Frequency 0.0 0.2 0.4 0.6
IMPORTANCE BY VULNERABILITY,0.17142857142857143,Accuracy
IMPORTANCE BY VULNERABILITY,0.1732142857142857,ImageNet Normal
IMPORTANCE BY VULNERABILITY,0.175,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.1767857142857143,Attack Frequency 0.3 0.4 0.5 0.6 0.7 0.8
IMPORTANCE BY VULNERABILITY,0.17857142857142858,Accuracy
IMPORTANCE BY VULNERABILITY,0.18035714285714285,CIFAR-10 Adversarial
IMPORTANCE BY VULNERABILITY,0.18214285714285713,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.18392857142857144,Attack Frequency 0.0 0.1 0.2 0.3 0.4
IMPORTANCE BY VULNERABILITY,0.18571428571428572,Accuracy
IMPORTANCE BY VULNERABILITY,0.1875,TinyImageNet Adversarial
IMPORTANCE BY VULNERABILITY,0.18928571428571428,"0
10
20
30
40
50
60"
IMPORTANCE BY VULNERABILITY,0.19107142857142856,Attack Frequency 0.0 0.1 0.2 0.3 0.4 0.5
IMPORTANCE BY VULNERABILITY,0.19285714285714287,Accuracy
IMPORTANCE BY VULNERABILITY,0.19464285714285715,ImageNet Adversarial
IMPORTANCE BY VULNERABILITY,0.19642857142857142,"=2/255
=4/255
=8/255
=16/255
=32/255"
IMPORTANCE BY VULNERABILITY,0.1982142857142857,"Figure 2: Vulnerability scores (Accuracy under attack) visualized per frequency across datasets.
Notice that the trends are reversed from normal training to adversarial training in the case of CIFAR-
10. The results for different frequency bands, under Auto-Attack is shown in Figure A.7."
IMPORTANCE BY VULNERABILITY,0.2,"0.2
0.4
0.6
0.8
Drop Rates 93.5 94.0 94.5 95.0 95.5"
IMPORTANCE BY VULNERABILITY,0.2017857142857143,Accuracy
IMPORTANCE BY VULNERABILITY,0.20357142857142857,CIFAR-10
IMPORTANCE BY VULNERABILITY,0.20535714285714285,"0.2
0.4
0.6
0.8
Drop Rates 48 50 52 54 56"
IMPORTANCE BY VULNERABILITY,0.20714285714285716,Accuracy
IMPORTANCE BY VULNERABILITY,0.20892857142857144,TinyImageNet
IMPORTANCE BY VULNERABILITY,0.21071428571428572,"0.2
0.4
0.6
0.8
Drop Rates 20 30 40 50"
IMPORTANCE BY VULNERABILITY,0.2125,Accuracy
IMPORTANCE BY VULNERABILITY,0.21428571428571427,ImageNet
IMPORTANCE BY VULNERABILITY,0.21607142857142858,"Frequency Range Trained: 0-15
Frequency Range Trained: 16-32
Frequency Range Trained: 32-48
Frequency Range Trained: 48-63"
IMPORTANCE BY VULNERABILITY,0.21785714285714286,"Figure 3: Accuracy for models trained with varying drop rates, for different frequency ranges."
IMPORTANCE BY VULNERABILITY,0.21964285714285714,"larger datasets, such perturbations are imperceptible to a human. Example images have been shown
in Appendix (Figure A.20 and A.21)."
IMPORTANCE DURING TRAINING,0.22142857142857142,"5.2
IMPORTANCE DURING TRAINING"
IMPORTANCE DURING TRAINING,0.22321428571428573,"With the objective of understanding the relative importance of frequency components while training,
we formulate an experiment where we train models by masking out (making them zeros) frequency
components of the input in a probabilistic manner and then using the trained model for normal
inference. Example images when certain frequency bands are dropped is shown in Figure A.18. We
train four types of models, where the frequency masking is restricted to four equal frequency bands
and the amount of masking/dropping is controlled by a parameter p. This translates to training"
IMPORTANCE DURING TRAINING,0.225,"arg min
θ
L(h(x ˆ
f; θ), y)
(17)"
IMPORTANCE DURING TRAINING,0.22678571428571428,"where x ˆ
f = D−1(M ⊙D(x))
(18)"
IMPORTANCE DURING TRAINING,0.22857142857142856,"and Mz =
1
z ∼Up ∧z ∈[f1, f2..., fk]
0
else
is the Mask generated using p
(19)"
IMPORTANCE DURING TRAINING,0.23035714285714284,Under review as a conference paper at ICLR 2022
IMPORTANCE DURING TRAINING,0.23214285714285715,"0-15
16-32
32-48
48-63
Attack Frequency 0-15 16-32 32-48 48-63"
IMPORTANCE DURING TRAINING,0.23392857142857143,Train Frequency
IMPORTANCE DURING TRAINING,0.2357142857142857,"0.27
0.05
0.04
0.06"
IMPORTANCE DURING TRAINING,0.2375,"0.01
0.42
0.34
0.29"
IMPORTANCE DURING TRAINING,0.2392857142857143,"0.00
0.10
0.44
0.41"
IMPORTANCE DURING TRAINING,0.24107142857142858,"0.00
0.01
0.05
0.38"
IMPORTANCE DURING TRAINING,0.24285714285714285,"TinyImageNet,  = 4/255"
IMPORTANCE DURING TRAINING,0.24464285714285713,"0
9
18
27
36
45
54
63
Attack Frequency 0 9 18 27 36 45 54 63"
IMPORTANCE DURING TRAINING,0.24642857142857144,Train Frequency
IMPORTANCE DURING TRAINING,0.24821428571428572,"TinyImageNet,  = 4/255"
IMPORTANCE DURING TRAINING,0.25,"0
10
20
30
40
50
60
Train Frequency 0.0 0.2 0.4 0.6 0.8 1.0"
IMPORTANCE DURING TRAINING,0.2517857142857143,Clean Accuracy
IMPORTANCE DURING TRAINING,0.25357142857142856,"Clean Accuracy
0.0 0.2 0.4 0.6 0.8 1.0"
IMPORTANCE DURING TRAINING,0.25535714285714284,"0-15
16-32
32-48
48-63
Attack Frequency 0-15 16-32 32-48 48-63"
IMPORTANCE DURING TRAINING,0.2571428571428571,Train Frequency
IMPORTANCE DURING TRAINING,0.25892857142857145,"0.39
0.00
0.00
0.00"
IMPORTANCE DURING TRAINING,0.26071428571428573,"0.00
0.78
0.31
0.18"
IMPORTANCE DURING TRAINING,0.2625,"0.00
0.31
0.74
0.74"
IMPORTANCE DURING TRAINING,0.2642857142857143,"0.00
0.01
0.36
0.78"
IMPORTANCE DURING TRAINING,0.26607142857142857,"CIFAR-10,  = 8/255"
IMPORTANCE DURING TRAINING,0.26785714285714285,"0
9
18
27
36
45
54
63
Attack Frequency 0 9 18 27 36 45 54 63"
IMPORTANCE DURING TRAINING,0.26964285714285713,Train Frequency
IMPORTANCE DURING TRAINING,0.2714285714285714,"CIFAR-10,  = 8/255"
IMPORTANCE DURING TRAINING,0.2732142857142857,"0
10
20
30
40
50
60
Train Frequency 0.0 0.2 0.4 0.6 0.8 1.0"
IMPORTANCE DURING TRAINING,0.275,Clean Accuracy
IMPORTANCE DURING TRAINING,0.2767857142857143,"Clean Accuracy
0.0 0.2 0.4 0.6 0.8 1.0"
IMPORTANCE DURING TRAINING,0.2785714285714286,"Figure 4: Frequency-based adversarial training across datasets. In the ﬁrst column we show the
results of adversarially training and testing for different frequency ranges. Next, we show results of
the same experiments across individual frequencies. The last column shows clean accuracy for each
frequency."
IMPORTANCE DURING TRAINING,0.28035714285714286,"where x ˆ
f is the input constrained to a particular frequency band within the range [f1, f2, · · · fk].
While training, we select the frequencies to be dropped using a random uniform distribution U, with
the percentage of dropping controlled by parameter p. A value of p = 1 indicates all frequencies
in the speciﬁed band are set to zero. We train a total of 36 models per dataset, encompassing 9
different drop rates (p values) and 4 frequency bands. The experiment is repeated across datasets
and the results are shown in Figure 3. As expected, we observe that a higher drop rate leads to lower
accuracy. We also see that across datasets, high drop rates in low frequency band of 0-15 affects the
model more. This behaviour is expected as lower frequencies have a strong relation with the labels
(Wang et al., 2020b) and their extreme dropping leaves the model with little information to learn
from. But if we observe the degree to which it affects the performance, we see disparities between
the datasets. For example, the model trained on CIFAR-10 experiences a mere ∼2% drop even when
90% of frequencies in the low band (frequencies 0-15) are dropped. Under the same condition,
the model on TinyImageNet experiences ∼10% drop and the model on ImageNet experiences a
whopping ∼35% drop in accuracy, highlighting the relative importance of these frequency bands.
Also, note how very high drop rates in the highest frequency bands (frequencies 48-63) have little
to no effect in non CIFAR-10 models."
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.28214285714285714,"6
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2839285714285714,"Till now, we have analyzed the frequency properties of the model across datasets. In all experi-
ments so far, we merely observed how the model reacts to adversarial perturbations under various
frequency constraints. To further understand the properties of robustness in the frequency domain,
we propose to train models with adversarial perturbations restricted to these frequency subspaces, a
ﬁrst of its kind. The training follows"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2857142857142857,"min
θ
max
||δf ||p≤ϵ L(h(x + δf; θ), y)
(20)"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2875,"where δf is adversarial noise restricted to a frequency subspace deﬁned by f. To obtain a high-level
view of the process, we ﬁrst train models adversarially with frequencies restricted to four equal"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2892857142857143,Under review as a conference paper at ICLR 2022
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2910714285714286,"0
20
40
60
80
100 Steps 10 20 30 40 50"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.29285714285714287,PGD Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.29464285714285715,CIFAR-10
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.29642857142857143,"0
5
10
15
20
25 Steps 5 10 15 20"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.2982142857142857,PGD Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3,TinyImageNet
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.30178571428571427,"0
5
10
15
20 Steps 5 10 15 20 25"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.30357142857142855,PGD Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3053571428571429,ImageNet
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.30714285714285716,"0
20
40
60
80
100 Steps 40 50 60 70 80"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.30892857142857144,Clean Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3107142857142857,"0
5
10
15
20
25 Steps 10 20 30 40"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3125,Clean Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3142857142857143,"0
5
10
15
20 Steps 20 30 40 50 60"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.31607142857142856,Clean Accuracy
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.31785714285714284,"Adv Free Training 
 Higher Weight to Higher Frequencies 
 Higher Weight to Lower Frequencies"
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.3196428571428571,"Figure 5: Illustration of unequal epsilon distribution. Here we see that models where low frequency
perturbations are favoured ends up with higher robustness, but lower clean accuracy."
ADVERSARIAL TRAINING WITH FREQUENCY-BASED PERTURBATIONS,0.32142857142857145,"frequency bands, ranging from low to high. Predictably, the models perform well when adversar-
ial PGD attack is also restricted to the same frequency bands. The resulting robustness heatmap
of attacks across the spectrum is shown in ﬁrst column of Figure 4. For a more ﬁne-grained view
of the same, we adversarially train 64 models for each dataset, by perturbing each individual fre-
quency. Then we adversarially attack these models in every frequency to produce a robustness
heatmap, shown in the second column of Figure 4. In their work, Yin et al. (2019) had claimed
that training with low-frequency perturbations did not help the model to be robust against those
frequencies. Their analysis was not based on adversarial perturbations, but their claim was general-
ized. This effect was not observed in our experiments. We see that the model has good robustness
when trained and tested against low-frequency perturbations, across datasets. The diagonals of the
robustness heatmaps tell us that models perform well against an adversary constrained to the same
frequency used for training. Moreover, we also see that models trained with perturbations restricted
to mid/higher frequencies can withstand attacks from a fairly broad range of frequencies compared
to models trained with lower frequency perturbations. Now that we have established this new train-
ing paradigm, we explore its various nuances and intriguing properties."
THE UNEQUAL EPSILON DISTRIBUTION,0.32321428571428573,"6.1
THE UNEQUAL EPSILON DISTRIBUTION"
THE UNEQUAL EPSILON DISTRIBUTION,0.325,"Do all frequencies have the same impact in adversarial training? To answer this question, we mod-
ify the construction of adversarial perturbation δ by weighing contributions from different frequency
components and manipulating the value of ϵ they receive. It follows δ = K
X"
THE UNEQUAL EPSILON DISTRIBUTION,0.3267857142857143,"i=0
ηi · sgn(∇xL)i for L∞norm
(21)"
THE UNEQUAL EPSILON DISTRIBUTION,0.32857142857142857,"ηi =
ϵ
K −i
(22)"
THE UNEQUAL EPSILON DISTRIBUTION,0.33035714285714285,"where K is the number of equal frequency bands (four in our case) and η is a linear scaling param-
eter. This setting effectively translates to giving more importance to perturbation in one frequency
space over the other. We train 2 models: One as described by equation 22, favoring lower frequency
bands and then its complement, by reversing η and favoring higher frequency bands. For these ex-"
THE UNEQUAL EPSILON DISTRIBUTION,0.33214285714285713,Under review as a conference paper at ICLR 2022
THE UNEQUAL EPSILON DISTRIBUTION,0.3339285714285714,"0.2
0.4
0.6
0.8 0.86 0.88 0.90 0.92"
THE UNEQUAL EPSILON DISTRIBUTION,0.3357142857142857,Clean Accuracy
THE UNEQUAL EPSILON DISTRIBUTION,0.3375,"0.2
0.4
0.6
0.8 0.42 0.44 0.46 0.48"
THE UNEQUAL EPSILON DISTRIBUTION,0.3392857142857143,Clean Accuracy
THE UNEQUAL EPSILON DISTRIBUTION,0.3410714285714286,"0.2
0.4
0.6
0.8 0.54 0.56 0.58 0.60 0.62"
THE UNEQUAL EPSILON DISTRIBUTION,0.34285714285714286,Clean Accuracy 0.1 0.2 0.3 0.4
THE UNEQUAL EPSILON DISTRIBUTION,0.34464285714285714,Robust Accuracy
THE UNEQUAL EPSILON DISTRIBUTION,0.3464285714285714,CIFAR-10 0.050 0.075 0.100 0.125 0.150 0.175 0.200
THE UNEQUAL EPSILON DISTRIBUTION,0.3482142857142857,Robust Accuracy
THE UNEQUAL EPSILON DISTRIBUTION,0.35,TinyImageNet 0.05 0.10 0.15 0.20 0.25
THE UNEQUAL EPSILON DISTRIBUTION,0.3517857142857143,Robust Accuracy
THE UNEQUAL EPSILON DISTRIBUTION,0.3535714285714286,ImageNet
THE UNEQUAL EPSILON DISTRIBUTION,0.35535714285714287,"Clean Accuracy
Adv Free Clean
Robust Accuracy
Adv Free Robust"
THE UNEQUAL EPSILON DISTRIBUTION,0.35714285714285715,"Figure 6: Clean Accuracy vs Robustness across datasets, compared with standard adversarial train-
ing for free method. Note that the Y-axis scales are different. Here λ controls the weight of adver-
sarial perturbation towards lower frequencies."
THE UNEQUAL EPSILON DISTRIBUTION,0.35892857142857143,"periments, we employ Free adversarial training by Shafahi et al. (2019). The plot of PGD and clean
accuracy during training are shown in Figure 5. We see that the model in which lower frequencies
are favoured acts closest to standard PGD-based adversarial training. This shows that for a model
to be robust, it only needs to be adversarially trained in the frequencies that matter most and not the
entire spectrum. But at the same time, we see that the model where high frequency perturbations are
favoured shows superior clean accuracy in all datasets except CIFAR-10. These results tell us that
frequency based perturbations are intricately tied with clean accuracy and robustness of a model.
We explore this in detail in the next section."
THE UNEQUAL EPSILON DISTRIBUTION,0.3607142857142857,"6.2
ACCURACY VS ROBUSTNESS: AN ALTERNATIVE PERSPECTIVE"
THE UNEQUAL EPSILON DISTRIBUTION,0.3625,"Building on top of previous results, we design an experiment to examine the accuracy vs robustness
trade-off that is commonplace while training robust models. We introduce a parameter λ that con-
trols the weight given to frequency components in the perturbation during adversarial training. The
update step for PGD under L∞-norm now looks like:"
THE UNEQUAL EPSILON DISTRIBUTION,0.36428571428571427,"δ = λ ·
h
α · sgn(∇xLLF)
i
+ (1 −λ) ·
h
α · sgn(∇xLHF)
i
(23)"
THE UNEQUAL EPSILON DISTRIBUTION,0.36607142857142855,"where ∇xLLF and ∇xLHF are gradients restricted to low (frequencies 0-31) and high frequencies
(frequencies 32-63) respectively. We adversarially train ten different models by varying the value of
λ and show their clean and robust accuracy in Figure 6. We see that in the case of TinyImageNet
and ImageNet, the clean accuracy decreases when we train with low frequency perturbations, while
increasing robustness. In case of CIFAR-10, we see that there is an initial increase in robustness
followed by a steep fall. This is because higher frequencies have a signiﬁcant role in adversarial
robustness for this dataset, which is not achieved when λ values are high. We also observe a steep
fall in robustness for ImageNet at λ of 0.9. This is because the frequency importance is distributed
in the low-mid range for ImageNet (Figure 1a) and very high λ values tend to ignore the 32-48
frequency bands. These results establish that robustness and clean accuracy of an adversarially
trained model are dependent on the frequencies we perturb. The λ parameter gives us control over
the trade-off, enabling us to be more prudent while designing architectures and training regimes that
demand a mix of clean accuracy and robustness."
CONCLUSION,0.3678571428571429,"7
CONCLUSION"
CONCLUSION,0.36964285714285716,"In this paper, we analyze adversarial robustness through the perspective of spatial frequencies and
show that adversarial examples are not just a high frequency phenomenon. Using both theoretical
and empirical results we show that constituent frequencies of adversarial examples are dependent
on the dataset. Then we propose and study the properties of adversarial training using speciﬁc
frequencies, which can be used to understand the accuracy-robustness trade-off. These results can
be utilized to train robust models more quickly by focusing on the frequencies that matter most. We
hope that our ﬁndings will resolve some misconceptions about the frequency content of adversarial
examples and aid in creating more robust architectures."
CONCLUSION,0.37142857142857144,Under review as a conference paper at ICLR 2022
ETHICS,0.3732142857142857,"8
ETHICS"
ETHICS,0.375,"Adversarial examples pose a unique challenge to real world deep learning systems. We believe that
our analysis will aid in the development of adversarial attacks as well as robust architectures. While
we are aware of the potential for malicious uses of both of these applications we ﬁnd minimal direct
ethical concerns with the work in this paper. It is our hope that our work will only provide a deeper
understanding of what constitutes an adversarial example and the mechanisms behind adversarial
training in order to guide future research in this area."
REFERENCES,0.3767857142857143,REFERENCES
REFERENCES,0.37857142857142856,"Antonio A. Abello, Roberto Hirata, and Zhangyang Wang. Dissecting the high-frequency bias in
convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops, pp. 863–871, June 2021."
REFERENCES,0.38035714285714284,"N. Ahmed, T. Natarajan, and K.R. Rao. Discrete cosine transform. IEEE Transactions on Comput-
ers, C-23(1):90–93, 1974. doi: 10.1109/T-C.1974.223784."
REFERENCES,0.3821428571428571,"Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, 2018."
REFERENCES,0.38392857142857145,"R´emi Bernhard, Pierre-Alain Mo¨ellic, Martial Mermillod, Yannick Bourrier, Romain Cohendet,
Miguel Solinas, and Marina Reyboz. Impact of spatial frequency based constraints on adversarial
robustness. CoRR, abs/2104.12679, 2021. URL https://arxiv.org/abs/2104.12679."
REFERENCES,0.38571428571428573,"Nicholas Carlini and David A. Wagner.
Towards evaluating the robustness of neural networks.
CoRR, abs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644."
REFERENCES,0.3875,"Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security, 2017."
REFERENCES,0.3892857142857143,"Josue Ortega Caro, Yilong Ju, Ryan Pyle, Sourav Dey, Wieland Brendel, Fabio Anselmi, and Ankit
Patel. Local convolutions cause an implicit bias towards high frequency adversarial examples,
2021."
REFERENCES,0.39107142857142857,"Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. ArXiv, abs/2003.01690, 2020."
REFERENCES,0.39285714285714285,"Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, L. Chen, M. Kounavis, and
Duen Horng Chau. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg
compression. ArXiv, abs/1705.02900, 2017."
REFERENCES,0.39464285714285713,"Yingpeng Deng and Lina J. Karam.
Frequency-tuned universal adversarial attacks.
CoRR,
abs/2003.05549, 2020. URL https://arxiv.org/abs/2003.05549."
REFERENCES,0.3964285714285714,"H. Drucker and Y. Le Cun. Double backpropagation increasing generalization performance. In
IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii, pp. 145–150
vol.2, 1991. doi: 10.1109/IJCNN.1991.155328."
REFERENCES,0.3982142857142857,"Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M. Roy. A study of the effect of JPG
compression on adversarial images. CoRR, abs/1608.00853, 2016. URL http://arxiv.
org/abs/1608.00853."
REFERENCES,0.4,"I. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial ex-
amples. CoRR, abs/1412.6572, 2015."
REFERENCES,0.4017857142857143,"Chuan Guo, Jared S. Frank, and Kilian Q. Weinberger. Low frequency adversarial perturbation. In
UAI, 2019."
REFERENCES,0.4035714285714286,"Andrew Ilyas, Shibani Santurkar, D. Tsipras, Logan Engstrom, Brandon Tran, and A. Madry. Ad-
versarial examples are not bugs, they are features. In NeurIPS, 2019."
REFERENCES,0.40535714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.40714285714285714,"Malhar Jere, Sandro Herbig, Christine H. Lind, and F. Koushanfar. Principal component properties
of adversarial samples. ArXiv, abs/1912.03406, 2019."
REFERENCES,0.4089285714285714,"Malhar Jere, Maghav Kumar, and F. Koushanfar. A singular value perspective on model robustness.
ArXiv, abs/2012.03516, 2020."
REFERENCES,0.4107142857142857,"Harini Kannan, A. Kurakin, and I. Goodfellow. Adversarial logit pairing. ArXiv, abs/1803.06373,
2018."
REFERENCES,0.4125,"J. Z. Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In ICML, 2018."
REFERENCES,0.4142857142857143,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.4160714285714286,"P. Lorenz, Paula Harder, Dominik Strassel, Margret Keuper, and Janis Keuper. Detecting autoattack
perturbations in the frequency domain. 2021."
REFERENCES,0.41785714285714287,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb."
REFERENCES,0.41964285714285715,"Guillermo Ortiz-Jimenez, Apostolos Modas,
Seyed-Mohsen Moosavi-Dezfooli,
and Pascal
Frossard. Hold me tight! Inﬂuence of discriminative features on deep network boundaries. In
Advances in Neural Information Processing Systems 34. December 2020."
REFERENCES,0.42142857142857143,"Nicolas Papernot, P. Mcdaniel, Xi Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. 2016 IEEE Symposium on Security and Privacy (SP),
pp. 582–597, 2016."
REFERENCES,0.4232142857142857,"Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-cam: Why did you say that? visual explanations from deep networks via
gradient-based localization. CoRR, abs/1610.02391, 2016. URL http://arxiv.org/abs/
1610.02391."
REFERENCES,0.425,"A. Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, L. Davis,
G. Taylor, and T. Goldstein. Adversarial training for free! In NeurIPS, 2019."
REFERENCES,0.42678571428571427,"Yash Sharma, Gavin Weiguang Ding, and Marcus A. Brubaker. On the effectiveness of low fre-
quency perturbations. ArXiv, abs/1903.00073, 2019."
REFERENCES,0.42857142857142855,"Jiawei Su, Danilo Vasconcellos Vargas, and K. Sakurai. One pixel attack for fooling deep neural
networks. IEEE Transactions on Evolutionary Computation, 23:828–841, 2019."
REFERENCES,0.4303571428571429,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, I. Goodfellow, and
R. Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2014."
REFERENCES,0.43214285714285716,"Florian Tram`er, A. Kurakin, Nicolas Papernot, D. Boneh, and P. Mcdaniel. Ensemble adversarial
training: Attacks and defenses. ArXiv, abs/1705.07204, 2018."
REFERENCES,0.43392857142857144,"Yusuke Tsuzuku and Issei Sato. On the structural sensitivity of deep convolutional networks to
the directions of fourier basis functions. 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 51–60, 2019."
REFERENCES,0.4357142857142857,"H. Wang, Cory Cornelius, Brandon Edwards, and Jason Martin. Toward few-step adversarial training
from a frequency perspective. Proceedings of the 1st ACM Workshop on Security and Privacy on
Artiﬁcial Intelligence, 2020a."
REFERENCES,0.4375,"Haofan Wang, Mengnan Du, Fan Yang, and Zijian Zhang. Score-cam: Improved visual explanations
via score-weighted class activation mapping. CoRR, abs/1910.01279, 2019. URL http://
arxiv.org/abs/1910.01279."
REFERENCES,0.4392857142857143,"Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing. High-frequency component helps explain
the generalization of convolutional neural networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), June 2020b."
REFERENCES,0.44107142857142856,Under review as a conference paper at ICLR 2022
REFERENCES,0.44285714285714284,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.4446428571428571,Accuracy
REFERENCES,0.44642857142857145,Cifar-10 Normal
REFERENCES,0.44821428571428573,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.45,Accuracy
REFERENCES,0.4517857142857143,TinyImageNet Normal
REFERENCES,0.45357142857142857,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.00 0.05 0.10 0.15"
REFERENCES,0.45535714285714285,Accuracy
REFERENCES,0.45714285714285713,ImageNet Normal
REFERENCES,0.4589285714285714,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.4607142857142857,Accuracy
REFERENCES,0.4625,Cifar-10 Adversarial
REFERENCES,0.4642857142857143,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.4660714285714286,Accuracy
REFERENCES,0.46785714285714286,TinyImageNet Adversarial
REFERENCES,0.46964285714285714,"0-15
16-32
33-48
48-63
Attack Frequency Range 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.4714285714285714,Accuracy
REFERENCES,0.4732142857142857,ImageNet Adversarial
REFERENCES,0.475,"=1/255
=3/512"
REFERENCES,0.4767857142857143,"=2/255
=3/255"
REFERENCES,0.4785714285714286,"=4/255
=8/255"
REFERENCES,0.48035714285714287,"=16/255
=32/255"
REFERENCES,0.48214285714285715,"Figure A.7: Extension to experiments shown in Figure 4. DCT-PGD Auto-Attack across different
frequency ranges. Note that for CIFAR-10 Normally trained model, we have shown the results with
slightly lower epsilons."
REFERENCES,0.48392857142857143,"Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, and Zihao Ding. Towards frequency-
based explanation for robust CNN. CoRR, abs/2005.03141, 2020c. URL https://arxiv.
org/abs/2005.03141."
REFERENCES,0.4857142857142857,"Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, and Zihao Ding. Towards frequency-based
explanation for robust cnn. ArXiv, abs/2005.03141, 2020d."
REFERENCES,0.4875,"Cihang Xie, Yuxin Wu, L. V. D. Maaten, A. Yuille, and Kaiming He. Feature denoising for im-
proving adversarial robustness. 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 501–509, 2019."
REFERENCES,0.48928571428571427,"Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. ArXiv, abs/1704.01155, 2018."
REFERENCES,0.49107142857142855,"Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, and Justin Gilmer. A fourier
perspective on model robustness in computer vision. CoRR, abs/1906.08988, 2019. URL http:
//arxiv.org/abs/1906.08988."
REFERENCES,0.4928571428571429,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, E. Xing, L. Ghaoui, and Michael I. Jordan. Theoreti-
cally principled trade-off between robustness and accuracy. In ICML, 2019."
REFERENCES,0.49464285714285716,"A
APPENDIX"
REFERENCES,0.49642857142857144,"A.1
PROOFS"
REFERENCES,0.4982142857142857,Here are the proofs for some results from above. In equation 11 we mentioned
REFERENCES,0.5,"∇δY = ∇xY = ∇ˆxY
(24)"
REFERENCES,0.5017857142857143,Under review as a conference paper at ICLR 2022
REFERENCES,0.5035714285714286,"Consider a neural network y = h(x; θ). Let the adversarial sample be ˆx = x + δ, where δ is the
additive adversarial noise."
REFERENCES,0.5053571428571428,"y = h(ˆx) = h(x + δ)
(25)
dy
dx = h(x + δ)′ · 1 = dy"
REFERENCES,0.5071428571428571,"dδ
(26)"
REFERENCES,0.5089285714285714,"dy
dˆx = h(ˆx)′ = h(x + δ)′ hence
(27)"
REFERENCES,0.5107142857142857,"dy
dˆx = dy"
REFERENCES,0.5125,dδ = dy
REFERENCES,0.5142857142857142,"dx or ∇δY = ∇xY = ∇ˆxY
(28)"
REFERENCES,0.5160714285714286,In the same section’s equation 12 we also mentioned ∇xL ∝∇xY .
REFERENCES,0.5178571428571429,Let L = 1
REFERENCES,0.5196428571428572,"2 (h(x; θ) −ˆy)2 be the loss.
(29) dL"
REFERENCES,0.5214285714285715,"dx = (h(x; θ) −ˆy) · h(x; θ)
′
(30)"
REFERENCES,0.5232142857142857,"here h(x; θ)
′ = dy"
REFERENCES,0.525,"dx and (h(x; θ) −ˆy) is a constant
(31) dL"
REFERENCES,0.5267857142857143,dx = K · dy
REFERENCES,0.5285714285714286,"dx which implies
(32)"
REFERENCES,0.5303571428571429,"∇xL ∝∇xY
(33)"
REFERENCES,0.5321428571428571,"A.2
TRAINING DETAILS"
REFERENCES,0.5339285714285714,"We utilize ResNet-18 in all our experiments (unless stated otherwise). For ImageNet and TinyIma-
geNet datasets, we train for a total of 100 epochs, with an initial learning rate of 0.1 decayed every
30 epochs, momentum of 0.9 and a weight decay of 5e-4. In Madry adversarial training for the
same, we use an ϵ value of 4/255. Under adversarial training for free setting, we train both models
for 25 epochs with learning rate decayed every 8 epochs and the m (repeat step) set to 4."
REFERENCES,0.5357142857142857,"For CIFAR-10, we train the model for total of 350 epochs, starting with a learning rate of 0.1,
decayed at 150 and 250 epochs and use the same setting with an ϵ of 8/255 for Madry training. In
adversarial training for free setting, we train the model for 100 epochs with learning rate decay every
30 epochs and the m value set to 8."
REFERENCES,0.5375,"We utilize the pretrained models provided by PyTorch for ImageNet normal models. All experiments
involving ImageNet-based adversarial training were done using Adversarial training for free method,
with total epochs of 25 and m value set to 4."
REFERENCES,0.5392857142857143,"A.3
FREQUENCY RANGE-BASED PERTURBATIONS"
REFERENCES,0.5410714285714285,"We revisit the results shown in Figure 2 and show the same in a broader sense by attacking different
frequency ranges. The results under DCT-PGD based Auto-Attack are shown in Figure A.7. We can
see that the trends which were observed and discussed in earlier sections remain unchanged."
REFERENCES,0.5428571428571428,"A.4
WHAT DO FREQUENCY ATTACKS TARGET ?"
REFERENCES,0.5446428571428571,"A natural question that might arise with respect to DCT-PGD paradigm is how can we be sure that
there is proportionate distortion in the frequency space as well. ((Rephrase)) We can visualize this
using simple properties of the DCT. Consider the 1-D DCT from above. Since it is a linear transform,
we can rewrite it as :"
REFERENCES,0.5464285714285714,"D(z) = WZ where W is the linear DCT transform on the input tensor Z
(34)
ˆx = x + δ in DCT space becomes
(35)
W · ˆx = W · x + W · δ
(36)
(37)"
REFERENCES,0.5482142857142858,"The elements of W represent different standard DCT basis functions, such that the lower frequencies
are in upper left corner and the higher frequencies are towards the lower right corner. For any"
REFERENCES,0.55,Under review as a conference paper at ICLR 2022
REFERENCES,0.5517857142857143,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.5535714285714286,CIFAR-10
REFERENCES,0.5553571428571429,"0
20
40
60 0 10 20 30 40 50 60"
REFERENCES,0.5571428571428572,TinyImageNet
REFERENCES,0.5589285714285714,"0
50
100
150
200 0 50 100 150 200"
REFERENCES,0.5607142857142857,ImageNet
REFERENCES,0.5625,"Figure A.8: Noise gradients visualized under L2 attack for normally trained ResNet-18 models. We
used attack ϵ = 1 for all models."
REFERENCES,0.5642857142857143,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.5660714285714286,Cifar-10 VGG
REFERENCES,0.5678571428571428,"0
20
40
60 0 10 20 30 40 50 60"
REFERENCES,0.5696428571428571,TinyImageNet VGG
REFERENCES,0.5714285714285714,"0
50
100
150
200 0 50 100 150 200"
REFERENCES,0.5732142857142857,ImageNet VGG
REFERENCES,0.575,"Figure A.9: Average Noise Gradients of VGG-16 models, across datasets"
REFERENCES,0.5767857142857142,Under review as a conference paper at ICLR 2022
REFERENCES,0.5785714285714286,"0
10
20
30 0 10 20 30"
REFERENCES,0.5803571428571429,Cifar-100 Normal
REFERENCES,0.5821428571428572,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.5839285714285715,MNIST Normal
REFERENCES,0.5857142857142857,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.5875,FMNIST Normal
REFERENCES,0.5892857142857143,"0
10
20
30 0 10 20 30"
REFERENCES,0.5910714285714286,Cifar-100 Adversarial
REFERENCES,0.5928571428571429,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.5946428571428571,MNIST Adversarial
REFERENCES,0.5964285714285714,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.5982142857142857,FMNIST Adversarial 0 50 100 150 200 250
REFERENCES,0.6,Figure A.10: DCT of Average Noise Gradients across additional datasets
REFERENCES,0.6017857142857143,"element i that also represents a frequency component, we can say that:"
REFERENCES,0.6035714285714285,"Wi · ˆxi = Wi · xi + Wi · δi
(38)"
REFERENCES,0.6053571428571428,"Essentially, we see that in the frequency space, each component of the resulting adversarial example
ˆx is linearly distorted by the corresponding frequency component of noise δ."
REFERENCES,0.6071428571428571,"A.5
DOES MODEL MATTER?"
REFERENCES,0.6089285714285714,"We run the experiments across VGG-16 to conﬁrm that the above trends are model agnostic and
aren’t just limited to ResNet style architectures. In the results shown in Figure A.9 we see that the
trends remain unchanged across datasets."
REFERENCES,0.6107142857142858,"A.6
DOES IMAGE SIZE MATTER?"
REFERENCES,0.6125,"To conﬁrm that the anomalies of adversarial examples are indeed due the underlying dataset and
not just the size, we repeat the experiment by training models where ImageNet and TinyImagenet
images are resized to smaller sizes using bicubic ﬁlter. The average noise gradients calculated from
these models are shown in Figure A.15."
REFERENCES,0.6142857142857143,"A.7
L2-BASED ADVERSARIAL ATTACKS"
REFERENCES,0.6160714285714286,"We repeat the same experiments to calculate noise gradients under the L2 attack. We do not observe
any divergent behaviour, compared to L∞attack. The results across datasets are shown in Figure
A.8."
REFERENCES,0.6178571428571429,"A.8
EXTENDING TO MORE DATASETS"
REFERENCES,0.6196428571428572,"We also repeat the experiments across datasets, including non-ImageNet derived datasets like
MNIST, Fashion-MNIST and CIFAR-100.The results are shown in Figure A.10."
REFERENCES,0.6214285714285714,Under review as a conference paper at ICLR 2022
REFERENCES,0.6232142857142857,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.625,CIFAR-10 Normal
REFERENCES,0.6267857142857143,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.6285714285714286,CIFAR-100 Normal
REFERENCES,0.6303571428571428,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.6321428571428571,MNIST Normal
REFERENCES,0.6339285714285714,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.6357142857142857,FMNIST Normal
REFERENCES,0.6375,"0
20
40
60 0 10 20 30 40 50 60"
REFERENCES,0.6392857142857142,TinyImageNet Normal
REFERENCES,0.6410714285714286,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.6428571428571429,CIFAR-10 Adversarial
REFERENCES,0.6446428571428572,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.6464285714285715,CIFAR-100 Adversarial
REFERENCES,0.6482142857142857,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.65,MNIST Adversarial
REFERENCES,0.6517857142857143,"0
10
20 0 5 10 15 20 25"
REFERENCES,0.6535714285714286,FMNIST Adversarial
REFERENCES,0.6553571428571429,"0
20
40
60 0 10 20 30 40 50 60"
REFERENCES,0.6571428571428571,TinyImageNet Adversarial 50 100 150 200 250
REFERENCES,0.6589285714285714,Figure A.11: DCT of Average Noise Gradients with Auto-Attack
REFERENCES,0.6607142857142857,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6625,CIFAR-10 Normal - Class: 0
REFERENCES,0.6642857142857143,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6660714285714285,CIFAR-10 Normal - Class: 1
REFERENCES,0.6678571428571428,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6696428571428571,CIFAR-10 Normal - Class: 2
REFERENCES,0.6714285714285714,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6732142857142858,CIFAR-10 Normal - Class: 3
REFERENCES,0.675,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6767857142857143,CIFAR-10 Normal - Class: 4
REFERENCES,0.6785714285714286,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6803571428571429,CIFAR-10 Normal - Class: 5
REFERENCES,0.6821428571428572,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6839285714285714,CIFAR-10 Normal - Class: 6
REFERENCES,0.6857142857142857,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6875,CIFAR-10 Normal - Class: 7
REFERENCES,0.6892857142857143,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6910714285714286,CIFAR-10 Normal - Class: 8
REFERENCES,0.6928571428571428,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6946428571428571,CIFAR-10 Normal - Class: 9
REFERENCES,0.6964285714285714,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.6982142857142857,CIFAR-10 Adv - Class: 0
REFERENCES,0.7,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7017857142857142,CIFAR-10 Adv - Class: 1
REFERENCES,0.7035714285714286,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7053571428571429,CIFAR-10 Adv - Class: 2
REFERENCES,0.7071428571428572,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7089285714285715,CIFAR-10 Adv - Class: 3
REFERENCES,0.7107142857142857,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7125,CIFAR-10 Adv - Class: 4
REFERENCES,0.7142857142857143,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7160714285714286,CIFAR-10 Adv - Class: 5
REFERENCES,0.7178571428571429,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7196428571428571,CIFAR-10 Adv - Class: 6
REFERENCES,0.7214285714285714,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7232142857142857,CIFAR-10 Adv - Class: 7
REFERENCES,0.725,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7267857142857143,CIFAR-10 Adv - Class: 8
REFERENCES,0.7285714285714285,"0
5
10
15
20
25
30 0 5 10 15 20 25 30"
REFERENCES,0.7303571428571428,CIFAR-10 Adv - Class: 9
REFERENCES,0.7321428571428571,Figure A.12: DCT of Average Noise Gradients Classwise for CIFAR-10
REFERENCES,0.7339285714285714,"A.9
EFFECT OF AUTO-ATTACK"
REFERENCES,0.7357142857142858,"We calculate and plot the noise gradients for all models under Auto-attack setting. In general, there
appears to be no signiﬁcant difference when compared to results from PGD attack. The results are
shown in ﬁgure A.11."
REFERENCES,0.7375,Under review as a conference paper at ICLR 2022
REFERENCES,0.7392857142857143,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7410714285714286,MNIST Normal - Class: 0
REFERENCES,0.7428571428571429,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7446428571428572,MNIST Normal - Class: 1
REFERENCES,0.7464285714285714,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7482142857142857,MNIST Normal - Class: 2
REFERENCES,0.75,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7517857142857143,MNIST Normal - Class: 3
REFERENCES,0.7535714285714286,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7553571428571428,MNIST Normal - Class: 4
REFERENCES,0.7571428571428571,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7589285714285714,MNIST Normal - Class: 5
REFERENCES,0.7607142857142857,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7625,MNIST Normal - Class: 6
REFERENCES,0.7642857142857142,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7660714285714286,MNIST Normal - Class: 7
REFERENCES,0.7678571428571429,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7696428571428572,MNIST Normal - Class: 8
REFERENCES,0.7714285714285715,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7732142857142857,MNIST Normal - Class: 9
REFERENCES,0.775,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7767857142857143,MNIST Adv - Class: 0
REFERENCES,0.7785714285714286,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7803571428571429,MNIST Adv - Class: 1
REFERENCES,0.7821428571428571,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7839285714285714,MNIST Adv - Class: 2
REFERENCES,0.7857142857142857,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7875,MNIST Adv - Class: 3
REFERENCES,0.7892857142857143,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7910714285714285,MNIST Adv - Class: 4
REFERENCES,0.7928571428571428,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7946428571428571,MNIST Adv - Class: 5
REFERENCES,0.7964285714285714,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.7982142857142858,MNIST Adv - Class: 6
REFERENCES,0.8,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8017857142857143,MNIST Adv - Class: 7
REFERENCES,0.8035714285714286,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8053571428571429,MNIST Adv - Class: 8
REFERENCES,0.8071428571428572,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8089285714285714,MNIST Adv - Class: 9
REFERENCES,0.8107142857142857,Figure A.13: DCT of Average Noise Gradients Classwise for MNIST
REFERENCES,0.8125,Under review as a conference paper at ICLR 2022
REFERENCES,0.8142857142857143,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8160714285714286,FMNIST Normal - Class: 0
REFERENCES,0.8178571428571428,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8196428571428571,FMNIST Normal - Class: 1
REFERENCES,0.8214285714285714,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8232142857142857,FMNIST Normal - Class: 2
REFERENCES,0.825,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8267857142857142,FMNIST Normal - Class: 3
REFERENCES,0.8285714285714286,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8303571428571429,FMNIST Normal - Class: 4
REFERENCES,0.8321428571428572,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8339285714285715,FMNIST Normal - Class: 5
REFERENCES,0.8357142857142857,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8375,FMNIST Normal - Class: 6
REFERENCES,0.8392857142857143,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8410714285714286,FMNIST Normal - Class: 7
REFERENCES,0.8428571428571429,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8446428571428571,FMNIST Normal - Class: 8
REFERENCES,0.8464285714285714,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8482142857142857,FMNIST Normal - Class: 9
REFERENCES,0.85,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8517857142857143,FMNIST Adv - Class: 0
REFERENCES,0.8535714285714285,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8553571428571428,FMNIST Adv - Class: 1
REFERENCES,0.8571428571428571,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8589285714285714,FMNIST Adv - Class: 2
REFERENCES,0.8607142857142858,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8625,FMNIST Adv - Class: 3
REFERENCES,0.8642857142857143,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8660714285714286,FMNIST Adv - Class: 4
REFERENCES,0.8678571428571429,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8696428571428572,FMNIST Adv - Class: 5
REFERENCES,0.8714285714285714,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8732142857142857,FMNIST Adv - Class: 6
REFERENCES,0.875,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8767857142857143,FMNIST Adv - Class: 7
REFERENCES,0.8785714285714286,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8803571428571428,FMNIST Adv - Class: 8
REFERENCES,0.8821428571428571,"0
5
10
15
20
25 0 5 10 15 20 25"
REFERENCES,0.8839285714285714,FMNIST Adv - Class: 9
REFERENCES,0.8857142857142857,Figure A.14: DCT of Average Noise Gradients Classwise for Fashion-MNIST
REFERENCES,0.8875,Under review as a conference paper at ICLR 2022
REFERENCES,0.8892857142857142,"0
20
40
60 0 10 20 30 40 50 60"
REFERENCES,0.8910714285714286,Size = 64
REFERENCES,0.8928571428571429,"0
10
20
30 0 5 10 15 20 25 30"
REFERENCES,0.8946428571428572,Size = 32
REFERENCES,0.8964285714285715,"0
5
10
15 0 2 4 6 8 10 12 14"
REFERENCES,0.8982142857142857,Size = 16
REFERENCES,0.9,Resized TinyImageNet
REFERENCES,0.9017857142857143,"0
100
200 0 50 100 150 200"
REFERENCES,0.9035714285714286,Size = 224
REFERENCES,0.9053571428571429,"0
20
40
60 0 20 40 60"
REFERENCES,0.9071428571428571,Size = 64
REFERENCES,0.9089285714285714,"0
10
20
30 0 10 20 30"
REFERENCES,0.9107142857142857,Size = 32
REFERENCES,0.9125,"0
5
10
15 0 5 10 15"
REFERENCES,0.9142857142857143,Size = 16
REFERENCES,0.9160714285714285,Resized ImageNet
REFERENCES,0.9178571428571428,Figure A.15: Effect of resizing the images on TinyImageNet and ImageNet.
REFERENCES,0.9196428571428571,"0
20
40
60
Frequencies 0.0 0.2 0.4 0.6"
REFERENCES,0.9214285714285714,Occlusion Score
REFERENCES,0.9232142857142858,CIFAR-10
REFERENCES,0.925,"0
20
40
60
Frequencies 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.9267857142857143,Occlusion Score
REFERENCES,0.9285714285714286,TinyImageNet
REFERENCES,0.9303571428571429,"0
20
40
60
Frequencies 0.0 0.1 0.2 0.3 0.4 0.5"
REFERENCES,0.9321428571428572,Occlusion Score
REFERENCES,0.9339285714285714,ImageNet
REFERENCES,0.9357142857142857,"Normal Training 
 PGD Adversarial Training"
REFERENCES,0.9375,"Figure A.16: Occlusion Scores averaged over validation, across three datasets. Adversarial training
is with L∞norm with ϵ of 8/255 for CIFAR-10 and 4/255 for others."
REFERENCES,0.9392857142857143,"A.10
CLASS WISE RESULTS"
REFERENCES,0.9410714285714286,"We also investigate if there exists different frequency distribution for each class in a dataset. We
show these results for CIFAR-10 (Fig A.12), MNIST (Fig A.13) and Fashion-MNIST (Fig A.14)
datasets, for both normally trained and adversarially trained models. Apart from subtle differences,
we do not see any general shift in the trends and observations."
REFERENCES,0.9428571428571428,"A.11
OCCLUSION SCORE"
REFERENCES,0.9446428571428571,"We borrow a simple metric the “Occlusion Score” from Wang et al. (2020c). Given a network h(x)
and an image x, the occlusion score Of(x) for frequency f on class c is deﬁned as:
Of(x) = |h(x)c −h(x ˆ
f)c|
(39)"
REFERENCES,0.9464285714285714,"where x ˆ
f refers to the input image x with the frequency ˆf removed from the spectrum. A higher
score indicates that there is a drop in model accuracy when that particular frequency is removed,"
REFERENCES,0.9482142857142857,Under review as a conference paper at ICLR 2022
REFERENCES,0.95,"Original Image
Include only Freq: 0 - 15
Include only Freq: 16 - 32
Include only Freq: 32 - 48
Include only Freq: 48 - 63"
REFERENCES,0.9517857142857142,Figure A.17: ImageNet examples where image is reconstructed using only speciﬁed frequency bands
REFERENCES,0.9535714285714286,"Original Image
Dropped Freq: 0 - 15
Dropped Freq: 16 - 32
Dropped Freq: 32 - 48
Dropped Freq: 48 - 63"
REFERENCES,0.9553571428571429,"Figure A.18: ImageNet examples where image is reconstructed after dropping (zeroing) certain
frequency bands"
REFERENCES,0.9571428571428572,"which implies the importance of the frequency. In their paper, Wang et al. (2020c) show the results
of this metric on CIFAR-10 dataset and incorrectly conclude that adversarial training tends to shift
attribution scores from higher frequency regions to lower frequency regions. We show that this is
not the case by simply extending the experiment to include TinyImageNet and ImageNet datasets
From the results shown in A.16, we can clearly see that in non CIFAR datasets, the attribution scores
are already skewed towards lower frequencies and the shift after adversarial training happens across
all frequencies."
REFERENCES,0.9589285714285715,"A.12
EXAMPLES OF FREQUENCY-BASED PERTURBATIONS"
REFERENCES,0.9607142857142857,"We show example images under different perturbation budgets of L∞norm, across datasets in Fig-
ures A.19, A.20 and A.21. We also show examples of images when certain frequency bands are
dropped A.18 and the complementary case of including only speciﬁed frequencies A.17."
REFERENCES,0.9625,Under review as a conference paper at ICLR 2022
REFERENCES,0.9642857142857143,"= 2/255; Freq Range: 0-15
  = 2/255; Freq Range: 16-32
  = 2/255; Freq Range: 32-48
  = 2/255; Freq Range: 48-63"
REFERENCES,0.9660714285714286,"= 4/255; Freq Range: 0-15
  = 4/255; Freq Range: 16-32
  = 4/255; Freq Range: 32-48
  = 4/255; Freq Range: 48-63"
REFERENCES,0.9678571428571429,"= 8/255; Freq Range: 0-15
  = 8/255; Freq Range: 16-32
  = 8/255; Freq Range: 32-48
  = 8/255; Freq Range: 48-63"
REFERENCES,0.9696428571428571,"= 16/255; Freq Range: 0-15
  = 16/255; Freq Range: 16-32
  = 16/255; Freq Range: 32-48
  = 16/255; Freq Range: 48-63"
REFERENCES,0.9714285714285714,"= 32/255; Freq Range: 0-15
  = 32/255; Freq Range: 16-32
  = 32/255; Freq Range: 32-48
  = 32/255; Freq Range: 48-63"
REFERENCES,0.9732142857142857,Figure A.19: CIFAR-10 example images under different attack settings.
REFERENCES,0.975,Under review as a conference paper at ICLR 2022
REFERENCES,0.9767857142857143,"= 2/255; Freq Range: 0-15
  = 2/255; Freq Range: 16-32
  = 2/255; Freq Range: 32-48
  = 2/255; Freq Range: 48-63"
REFERENCES,0.9785714285714285,"= 4/255; Freq Range: 0-15
  = 4/255; Freq Range: 16-32
  = 4/255; Freq Range: 32-48
  = 4/255; Freq Range: 48-63"
REFERENCES,0.9803571428571428,"= 8/255; Freq Range: 0-15
  = 8/255; Freq Range: 16-32
  = 8/255; Freq Range: 32-48
  = 8/255; Freq Range: 48-63"
REFERENCES,0.9821428571428571,"= 16/255; Freq Range: 0-15
  = 16/255; Freq Range: 16-32
  = 16/255; Freq Range: 32-48
  = 16/255; Freq Range: 48-63"
REFERENCES,0.9839285714285714,"= 32/255; Freq Range: 0-15
  = 32/255; Freq Range: 16-32
  = 32/255; Freq Range: 32-48
  = 32/255; Freq Range: 48-63"
REFERENCES,0.9857142857142858,Figure A.20: TinyImageNet example images under different attack settings.
REFERENCES,0.9875,Under review as a conference paper at ICLR 2022
REFERENCES,0.9892857142857143,"= 2/255; Freq Range: 0-15
  = 2/255; Freq Range: 16-32
  = 2/255; Freq Range: 32-48
  = 2/255; Freq Range: 48-63"
REFERENCES,0.9910714285714286,"= 4/255; Freq Range: 0-15
  = 4/255; Freq Range: 16-32
  = 4/255; Freq Range: 32-48
  = 4/255; Freq Range: 48-63"
REFERENCES,0.9928571428571429,"= 8/255; Freq Range: 0-15
  = 8/255; Freq Range: 16-32
  = 8/255; Freq Range: 32-48
  = 8/255; Freq Range: 48-63"
REFERENCES,0.9946428571428572,"= 16/255; Freq Range: 0-15
  = 16/255; Freq Range: 16-32
  = 16/255; Freq Range: 32-48
  = 16/255; Freq Range: 48-63"
REFERENCES,0.9964285714285714,"= 32/255; Freq Range: 0-15
  = 32/255; Freq Range: 16-32
  = 32/255; Freq Range: 32-48
  = 32/255; Freq Range: 48-63"
REFERENCES,0.9982142857142857,Figure A.21: ImageNet example images under different attack settings.
