Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002036659877800407,"We consider minimizing a smooth function subject to a summation constraint
1"
ABSTRACT,0.004073319755600814,"over its variables. By exploiting a connection between the greedy 2-coordinate
2"
ABSTRACT,0.006109979633401222,"update for this problem and equality-constrained steepest descent in the 1-norm, we
3"
ABSTRACT,0.008146639511201629,"give a convergence rate for greedy selection under a proximal Polyak-Łojasiewicz
4"
ABSTRACT,0.010183299389002037,"assumption that is faster than random selection and independent of the problem
5"
ABSTRACT,0.012219959266802444,"dimension n. We then consider minimizing with both a summation constraint and
6"
ABSTRACT,0.014256619144602852,"bound constraints, as arises in the support vector machine dual problem. Existing
7"
ABSTRACT,0.016293279022403257,"greedy rules for this setting either guarantee trivial progress only or require O(n2)
8"
ABSTRACT,0.018329938900203666,"time to compute. We show that bound- and summation-constrained steepest descent
9"
ABSTRACT,0.020366598778004074,"in the L1-norm guarantees more progress per iteration than previous rules and can
10"
ABSTRACT,0.02240325865580448,"be computed in only O(n log n) time.
11"
INTRODUCTION,0.024439918533604887,"1
Introduction
12"
INTRODUCTION,0.026476578411405296,"Coordinate descent (CD) is an iterative optimization algorithm that performs a gradient descent step
13"
INTRODUCTION,0.028513238289205704,"on a single variable at each iteration. CD methods are appealing because they have a convergence
14"
INTRODUCTION,0.03054989816700611,"rate similar to gradient descent, but for some common objective functions the iterations have a much
15"
INTRODUCTION,0.032586558044806514,"lower cost. Thus, there is substantial interest in using CD for training machine learning models.
16"
INTRODUCTION,0.034623217922606926,"Unconstrained coordinate descent: Nesterov [2012] considered CD with random choices of the
17"
INTRODUCTION,0.03665987780040733,"coordinate to update, and proved non-asymptotic linear convergence rates for strongly-convex func-
18"
INTRODUCTION,0.038696537678207736,"tions with Lipschitz-continuous gradients. It was later shown that these linear convergence rates are
19"
INTRODUCTION,0.04073319755600815,"achieved under a generalization of strong convexity called the Polyak-Łojsiewicz condition [Karimi
20"
INTRODUCTION,0.04276985743380855,"et al., 2016]. Moreover, greedy selection of the coordinate to update also leads to faster rates than
21"
INTRODUCTION,0.04480651731160896,"random selection [Nutini et al., 2015]. These faster rates do not depend directly on the dimensionality
22"
INTRODUCTION,0.04684317718940937,"of the problem due to an equivalence between the greedy coordinate update and steepest descent on
23"
INTRODUCTION,0.048879837067209775,"all coordinates in the 1-norm. For a discussion of many other problems where we can implement
24"
INTRODUCTION,0.05091649694501019,"greedy selection rules at similar cost to random rules, see Nutini et al. [2022, Sections 2.4-2.5].
25"
INTRODUCTION,0.05295315682281059,"Bound-constrained coordinate descent: CD is commonly used for optimization with lower and/or
26"
INTRODUCTION,0.054989816700611,"upper bounds on each variable. Nesterov [2012] showed that the unconstrained rates of randomized
27"
INTRODUCTION,0.05702647657841141,"CD can be achieved under these separable constraints using a projected-gradient update of the
28"
INTRODUCTION,0.059063136456211814,"coordinate. Richtárik and Takáˇc [2014] generalize this result to include a non-smooth but separable
29"
INTRODUCTION,0.06109979633401222,"term in the objective function via a proximal-gradient update; this justiﬁes using CD in various
30"
INTRODUCTION,0.06313645621181263,"constrained and non-smooth settings, including least squares regularized by the 1-norm and support
31"
INTRODUCTION,0.06517311608961303,"vector machines with regularized bias. Similar to the unconstrained case, Karimireddy et al. [2019]
32"
INTRODUCTION,0.06720977596741344,"show that several forms of greedy coordinate selection lead to faster convergence rates than random
33"
INTRODUCTION,0.06924643584521385,"selection for problems with bound constraints or separable non-smooth terms.
34"
INTRODUCTION,0.07128309572301425,"Equality-constrained coordinate descent: many problems in machine learning require us to satisfy
35"
INTRODUCTION,0.07331975560081466,"an equality constraint. The most common example is that discrete probabilities must sum to one.
36"
INTRODUCTION,0.07535641547861507,"Another common example is SVMs with an unregularized bias term. The (non-separable) equality
37"
INTRODUCTION,0.07739307535641547,"constraint cannot be maintained by single-coordinate updates, but it can be maintained if we update
38"
INTRODUCTION,0.07942973523421588,"two variables at each iteration. Necoara et al. [2011] analyze random selection of the two coordinates
39"
INTRODUCTION,0.0814663951120163,"to update, while Fang et al. [2018] discuss randomized selection with tighter rates. The LIBSVM
40"
INTRODUCTION,0.0835030549898167,"package [Chang and Lin, 2011] uses a greedy 2-coordinate update for ﬁtting SVMs which has the
41"
INTRODUCTION,0.0855397148676171,"same cost as random selection. But despite LIBSVM being perhaps the most widely-used CD method
42"
INTRODUCTION,0.08757637474541752,"of all time, current analyses of greedy 2-coordinate updates either result in sublinear convergence
43"
INTRODUCTION,0.08961303462321792,"rates or do not lead to faster rates than random selection [Tseng and Yun, 2009, Beck, 2014].
44"
INTRODUCTION,0.09164969450101833,"Our contributions: we ﬁrst give a new analysis for the greedy 2-coordinate update for optimizing
45"
INTRODUCTION,0.09368635437881874,"a smooth function with an equality constraint. The analysis is based on an equivalence between
46"
INTRODUCTION,0.09572301425661914,"the greedy update and equality-constrained steepest descent in the 1-norm. This leads to a simple
47"
INTRODUCTION,0.09775967413441955,"dimension-independent analysis of greedy selection showing that it can converge substantially faster
48"
INTRODUCTION,0.09979633401221996,"than random selection. Next, we consider greedy rules when we have an equality constraint and
49"
INTRODUCTION,0.10183299389002037,"bound constraints. We argue that the rules used by LIBSVM cannot guarantee non-trivial progress
50"
INTRODUCTION,0.10386965376782077,"on each step. We analyze a classic greedy rule based on maximizing progress, but this analysis is
51"
INTRODUCTION,0.10590631364562118,"dimension-dependent and the cost of implementing this rule is O(n2) if we have both lower and upper
52"
INTRODUCTION,0.1079429735234216,"bounds. Finally, we show that steepest descent in the 1-norm with equalities and bounds guarantees
53"
INTRODUCTION,0.109979633401222,"a fast dimension-independent rate and can be implemented in O(n log n). This rule may require
54"
INTRODUCTION,0.1120162932790224,"updating more than 2 variables, in which case the additional variables can only be moved to their
55"
INTRODUCTION,0.11405295315682282,"bounds, but this can only happen for a ﬁnite number of early iterations.
56"
INTRODUCTION,0.11608961303462322,"2
Equality-Constrained Greedy 2-Coordinate Updates
57"
INTRODUCTION,0.11812627291242363,"We ﬁrst consider the problem of minimizing a twice-differentiable function f subject to a simple
58"
INTRODUCTION,0.12016293279022404,"linear equality constraint,
59"
INTRODUCTION,0.12219959266802444,"min
x2Rn f(x),
subject to n
X i=1"
INTRODUCTION,0.12423625254582485,"xi = γ,
(1)"
INTRODUCTION,0.12627291242362526,"where n is the number of variables and γ is a constant. On iteration k the 2-coordinate optimization
60"
INTRODUCTION,0.12830957230142567,"method chooses a coordinate ik and a coordinate jk and updates these two coordinates using
61 xk+1"
INTRODUCTION,0.13034623217922606,"ik
= xk"
INTRODUCTION,0.13238289205702647,"ik + δk,
xk+1"
INTRODUCTION,0.13441955193482688,"jk
= xk"
INTRODUCTION,0.1364562118126273,"jk −δk,
(2)"
INTRODUCTION,0.1384928716904277,"for a scalar δk (the other coordinates are unchanged). We can write this update for all coordinates as
62"
INTRODUCTION,0.14052953156822812,"xk+1 = xk + dk,
where dk"
INTRODUCTION,0.1425661914460285,"ik = δk, dk"
INTRODUCTION,0.1446028513238289,"jk = −δk, and dk"
INTRODUCTION,0.14663951120162932,"m = 0 for m 62 {ik, jj}.
(3)"
INTRODUCTION,0.14867617107942974,"If the iterate xk satisﬁes the constraint then this update maintains the constraint. In the coordinate
63"
INTRODUCTION,0.15071283095723015,gradient descent variant of this update we choose δk = −↵k
INTRODUCTION,0.15274949083503056,"2 (rikf(xk) −rjkf(xk)) for a step size
64"
INTRODUCTION,0.15478615071283094,"↵k. This results in an update to ik and jk of the form
65 xk+1"
INTRODUCTION,0.15682281059063136,"ik
= xk"
INTRODUCTION,0.15885947046843177,ik −↵k
INTRODUCTION,0.16089613034623218,"2 (rikf(xk) −rjkf(xk)), xk+1"
INTRODUCTION,0.1629327902240326,"jk
= xk"
INTRODUCTION,0.164969450101833,jk −↵k
INTRODUCTION,0.1670061099796334,"2 (rjkf(xk) −rikf(xk)).
(4)"
INTRODUCTION,0.1690427698574338,"If f is continuous, this update is guaranteed to decrease f for sufﬁciently small ↵k. The greedy rule
66"
INTRODUCTION,0.1710794297352342,"chooses the coordinates to update by maximizing the difference in their partial derivatives,
67"
INTRODUCTION,0.17311608961303462,ik 2 arg max i
INTRODUCTION,0.17515274949083504,"rif(xk),
jk 2 arg min j"
INTRODUCTION,0.17718940936863545,"rjf(xk).
(5)"
INTRODUCTION,0.17922606924643583,"At the solution of the problem we must have partial derivatives being equal, so intuitively this greedy
68"
INTRODUCTION,0.18126272912423624,"choice updates the coordinates that are furthest above/below the average partial derivative. This choice
69"
INTRODUCTION,0.18329938900203666,"also minimizes the set of 2-coordinate quadratic approximations to the function (see Appendix A.1)
70"
INTRODUCTION,0.18533604887983707,"arg min i,j ⇢"
INTRODUCTION,0.18737270875763748,"min
dij|di+dj=0 f(xk) + rijf(xk)T dij +
1
2↵k kdijk2 # ,
(6)"
INTRODUCTION,0.1894093686354379,"which is a special case of the Gauss-Southwell-q (GS-q) rule of Tseng and Yun [2009].
71"
INTRODUCTION,0.19144602851323828,"We assume that the gradient of f is Lipschitz continuous, and our analysis will depend on a quantity
72"
INTRODUCTION,0.1934826883910387,"we call L2. The quantity L2 bounds the change in the 2-norm of the gradient with respect to any two
73"
INTRODUCTION,0.1955193482688391,"coordinates i and j under a two-coordinate update of any x of the form (3).
74"
INTRODUCTION,0.1975560081466395,"krijf(x + d) −rijf(x)k2 L2kdk2.
(7)"
INTRODUCTION,0.19959266802443992,"Note that L2 is less than or equal to the Lipschitz constant of the gradient of f.
75"
INTRODUCTION,0.20162932790224034,"2.1
Connections between Greedy 2-Coordinate Updates and the 1-Norm
76"
INTRODUCTION,0.20366598778004075,"Our analysis relies on several connections between the greedy update and steepest descent in the
77"
INTRODUCTION,0.20570264765784113,"1-norm, which we outline in this section. First, we note that vectors dk of the form (3) satisfy
78 kdkk2"
INTRODUCTION,0.20773930753564154,1 = 2kdkk2
INTRODUCTION,0.20977596741344195,"2, since
79 kdkk2"
INTRODUCTION,0.21181262729124237,1 = (|δk| + | −δk|)2
INTRODUCTION,0.21384928716904278,= (δk)2 + (δk)2 + 2|δk| · |δk|
INTRODUCTION,0.2158859470468432,= 4(δk)2
INTRODUCTION,0.21792260692464357,= 2((δk)2 + (−δk)2)
INTRODUCTION,0.219959266802444,= 2kdkk2 2.
INTRODUCTION,0.2219959266802444,"Second, if a twice-differentiable function’s gradient satisﬁes the 2-coordinate Lipschitz continuity
80"
INTRODUCTION,0.2240325865580448,"assumption (7) with constant L2, then the full gradient is Lipschitz continuous in the 1-norm with
81"
INTRODUCTION,0.22606924643584522,"constant L1 = L2/2 (see Appendix B). Finally, we note that applying the 2-coordinate update (4)
82"
INTRODUCTION,0.22810590631364563,"is an instance of applying steepest descent over all coordinates in the 1-norm. In particular, in
83"
INTRODUCTION,0.23014256619144602,"Appendix A.2 we show that steepest descent in the 1-norm always admits a greedy 2-coordinate
84"
INTRODUCTION,0.23217922606924643,"update as a solution.
85"
INTRODUCTION,0.23421588594704684,"Lemma 2.1. Let ↵> 0. Then at least one steepest descent direction with respect to the 1-norm has
86"
INTRODUCTION,0.23625254582484725,"exactly two non-zero coordinates. That is,
87"
INTRODUCTION,0.23828920570264767,"min
d2Rn|dT 1=0 rf(x)T d + 1"
INTRODUCTION,0.24032586558044808,2↵||d||2
INTRODUCTION,0.24236252545824846,"1 = min i,j ⇢"
INTRODUCTION,0.24439918533604887,"min
dij2R2|di+dj=0 rijf(x)T dij + 1"
INTRODUCTION,0.24643584521384929,"2↵||dij||2 1 # .
(8)"
INTRODUCTION,0.2484725050916497,"This lemma allows us to equate the progress of greedy 2-coordinate updates to the progress made by
88"
INTRODUCTION,0.2505091649694501,"a full-coordinate steepest descent step descent step in the 1-norm.
89"
INTRODUCTION,0.2525458248472505,"2.2
Proximal-PL Inequality in the 1-Norm
90"
INTRODUCTION,0.2545824847250509,"For lower bounding sub-optimality in terms of the 1-norm, we introduce the proximal-PL inequality
91"
INTRODUCTION,0.25661914460285135,"in the 1-norm. The proximal-PL condition was introduced to allow simpler proofs for various
92"
INTRODUCTION,0.25865580448065173,"constrained and non-smooth optimization problems [Karimi et al., 2016]. The proximal-PL condition
93"
INTRODUCTION,0.2606924643584521,"is normally deﬁned based on the 2-norm, but we deﬁne a variant for the summation-constrained
94"
INTRODUCTION,0.26272912423625255,"problem where distances are measured in the 1-norm.
95"
INTRODUCTION,0.26476578411405294,"Deﬁnition 2.2. A function f, that is L1-Lipschitz with respect to the 1-norm and has a summmation
96"
INTRODUCTION,0.2668024439918534,"constraint on its parameters, satisﬁes the proximal-PL condition in the 1-norm if for a positive
97"
INTRODUCTION,0.26883910386965376,"constants µ1 we have
98"
INTRODUCTION,0.2708757637474542,"1
2D(x, L1) ≥µ1(f(x) −f ⇤),
(9)"
INTRODUCTION,0.2729124236252546,"for all x satisfying the equality constraint. Here, f ⇤is the constrained optimal function value and
99"
INTRODUCTION,0.27494908350305497,"D(x, L) = −2L
min
{y |yT 1=γ} "
INTRODUCTION,0.2769857433808554,"hrf(x), y −xi + L"
INTRODUCTION,0.2790224032586558,2 ||y −x||2 1 %
INTRODUCTION,0.28105906313645623,".
(10)"
INTRODUCTION,0.2830957230142566,"It follows from the equivalence between norms that summation-constrained functions satisfying the
100"
INTRODUCTION,0.285132382892057,"proximal-PL condition in the 2-norm will also satisfy the above proximal-PL condition in the 1-norm.
101"
INTRODUCTION,0.28716904276985744,"In particular, if µ2 is the proximal-PL constant in the 2-norm, then we have µ2"
INTRODUCTION,0.2892057026476578,"n µ1 µ2 (see
102"
INTRODUCTION,0.29124236252545826,"Appendix C). Functions satisfying these conditions include any strongly-convex function f as well as
103"
INTRODUCTION,0.29327902240325865,"relaxations of strong convexity, such as functions of the form f = g(Ax) for a strongly-convex g and
104"
INTRODUCTION,0.2953156822810591,"a matrix A [Karimi et al., 2016]. In the g(Ax) case f is not strongly-convex if A is singular, and we
105"
INTRODUCTION,0.2973523421588595,"note that the SVM dual problem can be written in the form g(Ax).
106"
INTRODUCTION,0.29938900203665986,"2.3
Convergence Rate of Greedy 2-Coordinate Updates under Proximal-PL
107"
INTRODUCTION,0.3014256619144603,"We analyze the greedy 2-coordinate method under the proximal-PL condition based on the connections
108"
INTRODUCTION,0.3034623217922607,"to steepest descent in the 1-norm.
109"
INTRODUCTION,0.3054989816700611,"Theorem 2.3. Let f be a twice-differentiable function whose gradient is 2-coordinate-wise Lips-
110"
INTRODUCTION,0.3075356415478615,"chitz (7) and restricted to the set where xT 1 = γ. If this function satisﬁes the proximal-PL inequality
111"
INTRODUCTION,0.3095723014256619,"in the 1-norm (9) for some positive µ1, then the iterations of the 2-coordinate update (4) with
112"
INTRODUCTION,0.31160896130346233,"↵k = 1/L2 and the greedy rule (5) satisfy:
113"
INTRODUCTION,0.3136456211812627,f(xk) −f(x⇤)  ✓
INTRODUCTION,0.31568228105906315,1 −2µ1 L2 ◆k
INTRODUCTION,0.31771894093686354,"(f(x0) −f ⇤).
(11) 114"
INTRODUCTION,0.319755600814664,"Proof. Starting from the descent lemma restricted to the coordinates ik and jk we have
115"
INTRODUCTION,0.32179226069246436,f(xk+1) f(xk) + rikjkf(xk)T dikjk + L2
INTRODUCTION,0.32382892057026474,2 kdikjkk2
INTRODUCTION,0.3258655804480652,"= f(xk) + min i,j 8
>
< >
:"
INTRODUCTION,0.32790224032586557,"min
dij2R2|
di+dj=0"
INTRODUCTION,0.329938900203666,rijf(xk)T dij + L2
INTRODUCTION,0.3319755600814664,"2 kdijk2 9
>
= >
;"
INTRODUCTION,0.3340122199592668,(GS-q rule)
INTRODUCTION,0.3360488798370672,"= f(xk) + min i,j 8
>
< >
:"
INTRODUCTION,0.3380855397148676,"min
dij2R2|
di+dj=0"
INTRODUCTION,0.34012219959266804,rijf(xk)T dij + L2
INTRODUCTION,0.3421588594704684,"4 kdijk2 1 9
>
= >
; (kdk2"
INTRODUCTION,0.34419551934826886,1 = 2kdk2)
INTRODUCTION,0.34623217922606925,"= f(xk) + min i,j 8
>
< >
:"
INTRODUCTION,0.34826883910386963,"min
dij2R2|
di+dj=0"
INTRODUCTION,0.35030549898167007,rijf(xk)T dij + L1
INTRODUCTION,0.35234215885947046,"2 kdijk2 1 9
>
= >
;"
INTRODUCTION,0.3543788187372709,(L1 = L2/2)
INTRODUCTION,0.3564154786150713,"= f(xk) +
min
d|dT 1=0 ⇢"
INTRODUCTION,0.35845213849287166,rf(xk)T d + L1
INTRODUCTION,0.3604887983706721,2 kdk2 1 #
INTRODUCTION,0.3625254582484725,(Lemma 2.1).
INTRODUCTION,0.3645621181262729,"Now subtracting f ⇤from both sides and using the deﬁnition of D from the proximal-PL assumption,
116"
INTRODUCTION,0.3665987780040733,"f(xk+1) −f(x⇤) f(xk) −f(x⇤) −
1
2L1"
INTRODUCTION,0.36863543788187375,"D(xk, L1)"
INTRODUCTION,0.37067209775967414,= f(xk) −f(x⇤) −µ1 L1
INTRODUCTION,0.3727087576374745,(f(xk) −f ⇤)
INTRODUCTION,0.37474541751527496,= f(xk) −f(x⇤) −2µ1 L2
INTRODUCTION,0.37678207739307534,(f(xk) −f ⇤) = ✓
INTRODUCTION,0.3788187372708758,1 −2µ1 L2 ◆
INTRODUCTION,0.38085539714867617,(f(xk) −f ⇤)
INTRODUCTION,0.38289205702647655,"Applying the inequality recursively completes the proof.
117"
INTRODUCTION,0.384928716904277,"Note that the above rate also holds if we choose ↵k to maximally decrease f, and the same rate holds
118"
INTRODUCTION,0.3869653767820774,"up to a constant if we use a backtracking line search to set ↵k.
119"
COMPARISON TO RANDOMIZED SELECTION,0.3890020366598778,"2.4
Comparison to Randomized Selection
120"
COMPARISON TO RANDOMIZED SELECTION,0.3910386965376782,"If we sample the two coordinates ik and jk from a uniform distribution, then it is known that the
121"
COMPARISON TO RANDOMIZED SELECTION,0.39307535641547864,"2-coordinate descent method satisﬁes [She and Schmidt, 2017]
122"
COMPARISON TO RANDOMIZED SELECTION,0.395112016293279,E[f(xk)] −f(x⇤)  ✓
COMPARISON TO RANDOMIZED SELECTION,0.3971486761710794,"1 −
µ2
n2L2 ◆k"
COMPARISON TO RANDOMIZED SELECTION,0.39918533604887985,"(f(x0) −f ⇤).
(12)"
COMPARISON TO RANDOMIZED SELECTION,0.40122199592668023,"A similar result for a more-general problem class was shown by Necoara and Patrascu [2014]. This is
123"
COMPARISON TO RANDOMIZED SELECTION,0.40325865580448067,"substantially slower than the rate we show for the greedy 2-coordinate descent method. This rate
124"
COMPARISON TO RANDOMIZED SELECTION,0.40529531568228105,"is slower even in the extreme case where µ1 is similar to µ2/n, due to the presence of the n2 term.
125"
COMPARISON TO RANDOMIZED SELECTION,0.4073319755600815,"There also exist analyses for cyclic selection in the equality-constrained case but existing rates for
126"
COMPARISON TO RANDOMIZED SELECTION,0.4093686354378819,"cyclic rules are slower than the random rates Wang and Lin [2014].
127"
COMPARISON TO RANDOMIZED SELECTION,0.41140529531568226,"In the case where f is a dense quadratic function of n variables, which includes SVMs under the most
128"
COMPARISON TO RANDOMIZED SELECTION,0.4134419551934827,"popular kernels, both random selection and greedy selection cost O(n) per iteration to implement. If
129"
COMPARISON TO RANDOMIZED SELECTION,0.4154786150712831,"we consider the time required to reach an accuracy of ✏under random selection using the rate (12)
130"
COMPARISON TO RANDOMIZED SELECTION,0.4175152749490835,"we obtain O(n3log(1/✏)) where = L2/µ2. While for greedy selection under (11) it is between
131"
COMPARISON TO RANDOMIZED SELECTION,0.4195519348268839,"O(n2log(1/✏)) if µ1 is close to µ2/n and O(nlog(1/✏)) if µ1 is close to µ2. Thus, the reduction
132"
COMPARISON TO RANDOMIZED SELECTION,0.4215885947046843,"in total time complexity from using the greedy method is between a factor of O(n) and O(n2). This
133"
COMPARISON TO RANDOMIZED SELECTION,0.42362525458248473,"is a large difference which has not been reﬂected in previous analyses.
134"
COMPARISON TO RANDOMIZED SELECTION,0.4256619144602851,"There exist faster rates than (12) in the literature, but these require additional assumptions such as
135"
COMPARISON TO RANDOMIZED SELECTION,0.42769857433808556,"f being separable or that we know the coordinate-wise Lipschitz constants [Necoara et al., 2011,
136"
COMPARISON TO RANDOMIZED SELECTION,0.42973523421588594,"Necoara and Patrascu, 2014, Necoara et al., 2017, Fang et al., 2018]. However, these assumptions
137"
COMPARISON TO RANDOMIZED SELECTION,0.4317718940936864,"restrict the applicability of the results. Further, unlike convergence rates for random coordinate
138"
COMPARISON TO RANDOMIZED SELECTION,0.43380855397148677,"selection, we note that the new linear convergence rate (11) for greedy 2-coordinate method avoids
139"
COMPARISON TO RANDOMIZED SELECTION,0.43584521384928715,"requiring a direct dependence on the problem dimension. The only previous dimension-independent
140"
COMPARISON TO RANDOMIZED SELECTION,0.4378818737270876,"convergence rate for the greedy 2-coordinate method that we are aware of is due to Beck [2014,
141"
COMPARISON TO RANDOMIZED SELECTION,0.439918533604888,"Theorem 5.2b]. Their work considers functions that are bounded below, which is a weaker assumption
142"
COMPARISON TO RANDOMIZED SELECTION,0.4419551934826884,"than the proximal-PL assumption. However, this only leads to sublinear convergence rates and only
143"
COMPARISON TO RANDOMIZED SELECTION,0.4439918533604888,"on a measure of the violation in the Karush-Kuhn-Tucker conditions. Beck [2014, Theorem 6.2] also
144"
COMPARISON TO RANDOMIZED SELECTION,0.4460285132382892,"gives convergence rates in terms of function values for the special case of convex functions, but these
145"
COMPARISON TO RANDOMIZED SELECTION,0.4480651731160896,"rates are sublinear and dimension dependent.
146"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.45010183299389,"3
Equality- and Bound-Constrained Greedy Coordinate Updates
147"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.45213849287169044,"Equality constraints often appear alongside lower and/or upper bounds on the values of the individual
148"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.45417515274949083,"variables. This results in problems of the form
149"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.45621181262729127,"min
x2Rn f(x),
subject to n
X i=1"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.45824847250509165,"xi = γ, li xi ui.
(13)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.46028513238289204,"This framework includes our motivating problems of optimizing over the probability simplex (li = 0
150"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4623217922606925,"for all i since probabilites are non-negative), and optimizing SVMs with an unregularized bias (where
151"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.46435845213849286,"we have lower and upper bounds). With bound constraints we use a dk of form (3) but where δk is
152"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4663951120162933,"deﬁned so that the step respects the constraints,
153"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4684317718940937,δk = −min n↵k
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.47046843177189407,"2 (rikf(xk) −rjkf(xk)), xk"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4725050916496945,"ik −lik, ujk −xk jk o"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4745417515274949,",
(14)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.47657841140529533,"Unfortunately, analyzing the bound-constrained case is more complicated. There are several possible
154"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4786150712830957,"generalizations of the greedy rule for choosing the coordinates ik and jk to update, depending on
155"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.48065173116089616,"what properties of (5) we want to preserve [see Nutini, 2018, Section 2.7]. In this section we discuss
156"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.48268839103869654,"several possibilities, and how the choice of greedy rule affects the convergence rate and iteration cost.
157"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4847250509164969,"3.1
GS-s: Minimizing Directional Derivative
158"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.48676171079429736,"Up until version 2.7, the greedy rule used in LIBSVM was the Gauss-Southwell-s (GS-s) rule. The
159"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.48879837067209775,"GS-s rule chooses the coordinates resulting in the dk with the most-negative directional derivative.
160"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4908350305498982,"This is a natural generalization of the idea of steepest descent, and the ﬁrst uses of the method that
161"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.49287169042769857,"we aware of are by Keerthi et al. [2001] for SVMs and by Shevade and Keerthi [2003] for 1-norm
162"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.49490835030549896,"regularized optimization. For problem (13) the GS-s rule chooses
163"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4969450101832994,ik 2 arg max
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.4989816700610998,i | xk i >li
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5010183299389002,"rif(xk),
jk 2 arg min"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5030549898167006,j | xk j <ui
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.505091649694501,"rjf(xk).
(15)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5071283095723014,"This is similar to the unbounded greedy rule (5) but excludes variables where the update would
164"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5091649694501018,"immediately violate a bound constraint.
165"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5112016293279023,"Unfortunately, the per-iteration decrease in f obtained by the GS-s rule can be arbitrarily small. In
166"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5132382892057027,"particular, consider the case where the variable i maximizing rif(xk) has a value of xk"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.515274949083503,"i = li + ✏
167"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5173116089613035,"for an arbitrarily small ✏. In this case, we would choose ik and take an arbitrarily small step of
168"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5193482688391039,"δk = ✏. Steps like this that truncate δk are called “bad” steps, and the GS-s rule does not guarantee a
169"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5213849287169042,"non-trivial decrease in f on bad steps. If we only have bound constraints and do not have an equality
170"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5234215885947047,"constraint (so we can update on variable at a time), Karimireddy et al. [2019] show that at most half
171"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5254582484725051,"of the steps are bad steps. Their argument is that after we have taken a bad step on coordinate i,
172"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5274949083503055,"then the next time i is chosen we will not take a bad step. However, with an equality constraint it is
173"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5295315682281059,"possible for a coordinate to be involved in consecutive bad steps. It is possible that a combinatorial
174"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5315682281059063,"argument similar to Lacoste-Julien and Jaggi [2015, Theorem 8] could bound the number of bad
175"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5336048879837068,"steps, but it is not obvious that we do not require an exponential total number of bad steps.
176"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5356415478615071,"3.2
GS-q: Minimum 2-Coordinate Approximation
177"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5376782077393075,"A variant of the Gauss-Southwell-q (GS-q) rule of Tseng and Yun [2009]for problem (13) is
178"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.539714867617108,"arg min i,j"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5417515274949084,"min
dij||di+dj=0 ⇢"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5437881873727087,"f(xk) + rijf(xk)T dij +
1
2↵k kdijk2 : xk + d 2 [l, u] #"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5458248472505092,".
(16)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5478615071283096,"This minimizes a quadratic approximation to the function, restricted to the feasible set. For prob-
179"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5498981670061099,"lem (13), the GS-q rule is equivalent to choosing ik and jk to maximize (14), the distance that we
180"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5519348268839104,"move. We show the following result for the GS-q rule in Appendix D.
181"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5539714867617108,"Theorem 3.1. Let f be a differentiable function whose gradient is 2-coordinate-wise Lipschitz (7)
182"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5560081466395111,"and restricted to the set where xT 1 = γ and li xi ui. If this function satisﬁes the proximal-
183"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5580448065173116,"PL inequality in the 2-norm [Karimi et al., 2016] for some positive µ2, then the iterations of the
184"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.560081466395112,"2-coordinate update (3) with δk given by (14), ↵k = 1/L2, and the greedy GS-q rule (16) satisfy:
185"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5621181262729125,f(xk) −f(x⇤)  ✓
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5641547861507128,"1 −
µ2
L2(n −1) ◆k"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5661914460285132,"(f(x0) −f ⇤).
(17)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5682281059063137,"The proof of this result is more complicated than our previous results, relying on the concept of
186"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.570264765784114,"conformal realizations used by Tseng and Yun [2009]. We prove the result for general block sizes
187"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5723014256619144,"and then specialize to the two-coordinate case. Unlike the GS-s rule, this result shows that the GS-q
188"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5743380855397149,"guarantees non-trivial progress on each iteration. Note that while this result does have a dependence
189"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5763747454175153,"on the dimension n, it does not depend on n2 as the random rate (12) does. Moreover, the dependence
190"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5784114052953157,"on n can be improved by increasing the block size.
191"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5804480651731161,"Unfortunately, the GS-q rule is not always efﬁcient to use. As discussed by Beck [2014], there is
192"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5824847250509165,"no known algorithm faster than O(n2) for computing the GS-q rule (16). One special case where
193"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5845213849287169,"this can be solved in O(n) given the gradient is if we only have lower bounds (or only have upper
194"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5865580448065173,"bounds) [Beck, 2014]. An example with only lower bounds is our motivating problem of optimizing
195"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5885947046843177,"over the probability simplex, which only requires variables to be non-negative and sum to 1. On the
196"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5906313645621182,"other hand, our other motivating problem of SVMs requires lower and upper bounds so computing the
197"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5926680244399185,"GS-q rule would require O(n2). Beginning with version 2.8, LIBSVM began using an approximation
198"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.594704684317719,"to the GS-q rule that can be computed in O(n). In particular, LIBSVM ﬁrst chooses one coordinate
199"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5967413441955194,"using the GS-s rule, and then optimizes the other coordinate according to a variant of the GS-q
200"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.5987780040733197,"rule [Fan et al., 2005].1 While other rules have been proposed, the LIBSVM rule remains among the
201"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6008146639511202,"best-performing methods in practice [Horn et al., 2018]. However, similar to the GS-s rule we cannot
202"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6028513238289206,"guarantee non-trivial progress for the practical variant of the GS-q rule used by LIBSVM.
203"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.604887983706721,"3.3
GS-1: Steepest Descent in the 1-Norm
204"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6069246435845214,"Rather than using the classic GS-s or GS-q selection rules, the Gauss-Southwell-1 (GS-1) rule
205"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6089613034623218,"performs steepest descent in the 1-norm. For problem (13) this gives the update
206"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6109979633401222,"dk 2
arg min
lixi+diui|dT 1=0 ⇢"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6130346232179226,"rf(xk)T d +
1
2↵k ||d||2 1 #"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.615071283095723,".
(18)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6171079429735234,"The GS-1 rule was proposed by Song et al. [2017] for (unconstrained) 1-norm regularized problems.
207"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6191446028513238,"To analyze this method, we modify the deﬁnition of D(x, L) in the proximal-PL assumption to be
208"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6211812627291242,"D(x, L) = −2L
min
{liyiui |yT 1=γ} n"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6232179226069247,"hrf(x), y −xi + L"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6252545824847251,2 ||y −x||2 1 o
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6272912423625254,".
(19)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6293279022403259,"We then have the following dimension-independent convergence rate for the GS-1 rule.
209"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6313645621181263,1The newer LIBSVM rule also uses Lipschitz information about each coordinate; see Section 4 for discussion.
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6334012219959266,Algorithm 1 The GS-1 algorithm (with variables sorted in descending order according to rf(x)).
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6354378818737271,"1: function GS-1(x, rf(x), ↵, l, u)
2:
x0  0; xn+1  0; i  1; j  n; d  0;
3:
while 1 do
4:
δ  ↵"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6374745417515275,4 (rif(x) −rjf(x))
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.639511201629328,"5:
! = i−1
P p=0"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6415478615071283,"xp −lp; = n+1
P q=j+1 u −xq"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6435845213849287,"6:
if δ −! < 0 & δ −< 0 then
7:
if ! < then di = ! −; break;
8:
else dj = ! −; break;
9:
end if
10:
else if δ −! < 0 then dj = ! −; break;
11:
else if δ −< 0 then di = ! −; break;
12:
end if
13:
if xi + ! −δ ≥li & xj −+ δ uj then
14:
di = ! −δ; dj = δ −; break;
15:
end if
16:
if xi + ! −δ < li & xj −+ δ > uj then
17:
if li −(xi + ! −δ) > xj −+ δ −uj then
18:
di = l −xi; i  i + 1
19:
else
20:
dj = u −xj; j  j −1
21:
end if
22:
else if xi + ! −δ < li then di = l −xi; i  i + 1
23:
else dj = u −xj; j  j −1
24:
end if
25:
end while
26:
return d
27: end function"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6456211812627292,"Theorem 3.2. Let f be a differentiable function whose gradient is 2-coordinate-wise Lipschitz (7)
210"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6476578411405295,"and restricted to the set where xT 1 = γ and li xi ui. If this function satisﬁes the proximal-PL
211"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6496945010183299,"inequality in the 1-norm (9) for some positive µ1 with the deﬁnition (19), then the iterations of the
212"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6517311608961304,"update xk+1 = xk + dk with the greedy rule (18) and ↵k = 1/L1 = 2/L2 satisfy:
213"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6537678207739308,f(xk) −f(x⇤)  ✓
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6558044806517311,1 −2µ1 L2 ◆k
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6578411405295316,"(f(x0) −f ⇤).
(20)"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.659877800407332,"Proof. The proof follows the same reasoning as Theorem 2.3, but beginning after the application of
214"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6619144602851323,"Lemma 2.1 since we are directly computing the steepest descent direction.
215"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6639511201629328,"This GS-1 convergence rate is at least as fast as the convergence rate for GS-q, and thus by exploiting
216"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6659877800407332,"a connection to the 1-norm we once again obtain a faster dimension-independent rate. In Algorithm 1
217"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6680244399185336,"we give a method to construct a solution to the GS-1 rule (18) in O(n log n) time (due to sorting the
218"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.670061099796334,"rif(xk) values). Thus, our new GS-1 update guarantees non-trivial progress at each step (unlike the
219"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6720977596741344,"GS-s rule) and is efﬁcient to compute (unlike the GS-q rule). The precise logic of Algorithm 1 is
220"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6741344195519349,"somewhat complicated, but it can intuitively be viewed as a version of GS-s that ﬁxes the bad steps
221"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6761710794297352,"where δk is truncated. Roughly, if the GS-s rule gives a bad step then the GS-1 moves the violating
222"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6782077393075356,"variable to its boundary and then may also update the variable with the next largest/smallest rif(xk).
223"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6802443991853361,"The drawback of the GS-1 update is that it is not strictly a 2-coordinate method. While the GS-1
224"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6822810590631364,"update moves at most 2 variables within the interior of the bound constraints, it may move additional
225"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6843177189409368,"variables to their boundary. The iteration cost of the method will be higher on iterations where more
226"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6863543788187373,"than 2 variables are updated. However, by using an argument similar to Sun et al. [2019], we can show
227"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6883910386965377,"that the GS-1 rule will only update more than 2 variables on a ﬁnite number of early iterations. This
228"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6904276985743381,"is because, after some ﬁnite number of iterations, the variables actively constrained by their bounds
229"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6924643584521385,"will remain at their bounds. At this point, each GS-1 update will only update 2 variables within the
230"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6945010183299389,"interior of the bounds. In the case of SVMs, moving a variable to its lower bound corresponds to
231"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6965376782077393,"removing it as a potential support vector. Thus, this “bug” of GS-1 that it may update more than 2
232"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.6985743380855397,"variables can allow it to quickly remove many support vectors. In our experiments, we found that
233"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.7006109979633401,"GS-1 identiﬁed the support vectors more quickly than other rules and that most GS-1 updates only
234"
EQUALITY- AND BOUND-CONSTRAINED GREEDY COORDINATE UPDATES,0.7026476578411406,"updated 2 or 3 coordinates.
235"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7046843177189409,"4
Greedy Updates using Coordinate-Wise Lipschitz Constants
236"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7067209775967414,"Up until this point, we have measured smoothness based on the maximum blockwise Lipschitz-
237"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7087576374745418,"constant L2. An alternative measure of smoothness is Lipschitz continuity of individual coordinates.
238"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7107942973523421,"In particular, coordinate-wise Lipschitzness of coordinate i requires that for all x and ↵
239"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7128309572301426,"|rif(x + ↵ei) −rif(x)| Li|↵|,"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.714867617107943,"where ei is a vector with a one in position i and zeros in all other positions. For twice-differentiable
240"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7169042769857433,"convex functions, the Lipschitz constant with respect to the block (i, j) is upper bounded by the sum
241"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7189409368635438,"of the coordinate-wise constants Li and Lj [Nesterov, 2012, Lemma 1]. For equality-constrained
242"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7209775967413442,"optimization, Necoara et al. [2011] uses the coordinate-wise Lipschitz constants to design sampling
243"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7230142566191446,"distributions for ik and jk. Their analysis gives rates that can be faster than uniform sampling (12).
244"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.725050916496945,"In Appendix E, we consider greedy rules that depend on the Li values for the equality-constrained
245"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7270875763747454,"case. In particular, we show that the equality-constrained GS-q rule chooses ik and jk by solving
246"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7291242362525459,"arg max i,j"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7311608961303462,⇢(rif(x) −rjf(x))2
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7331975560081466,Li + Lj #
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7352342158859471,",
(21)"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7372708757637475,"which yields the standard greedy rule (5) if all Li values are equal. We show that the coordinate
247"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7393075356415478,"descent update with this selection rule and
248"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7413441955193483,"δk = −(rikf(xk) −rjkf(xk))/(Lik + Ljk),
(22)"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7433808553971487,"can be written as steepest descent in the norm deﬁned by ||d||L , P i"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.745417515274949,"pLi|di|. This yields a
249"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7474541751527495,"convergence rate that can be faster than the greedy rate (11).
250"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7494908350305499,"Unfortunately, it is not obvious how to solve (21) faster than O(n2). Nevertheless a reasonable
251"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7515274949083504,"approximation is to use
252"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7535641547861507,ik 2 arg max i
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7556008146639511,rif(xk)/ p
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7576374745417516,"Li,
jk 2 arg min j"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7596741344195519,rjf(xk)/ p
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7617107942973523,"Lj.
(23)"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7637474541751528,"which we call the ratio approximation. This approximation is (21) after re-parameterizing in terms of
253"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7657841140529531,"variables xi/pLi so that all coordinate-wise Lipschitz constants are 1 in the transformed problem.
254"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7678207739307535,"We can also use this re-parameterization to implement variations of the GS-s/GS-q/GS-1 rules if we
255"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.769857433808554,"also have bound constraints. While the ratio approximation (23) performed nearly as well as the more
256"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7718940936863544,"expensive (21) in our experiments, we found that the gap could be improved slightly if we choose one
257"
GREEDY UPDATES USING COORDINATE-WISE LIPSCHITZ CONSTANTS,0.7739307535641547,"coordinate according to the ratio approximation and then the second coordinate to optimize (21).2
258"
EXPERIMENTS,0.7759674134419552,"5
Experiments
259"
EXPERIMENTS,0.7780040733197556,"Our ﬁrst experiment evaluates the performance of various rules on a synthetic equality-constrained
260"
EXPERIMENTS,0.780040733197556,"least squares problem. Speciﬁcally, the objective is f(x) = 1"
EXPERIMENTS,0.7820773930753564,"2||Ax −b||2 subject to xT 1 = 0. We
261"
EXPERIMENTS,0.7841140529531568,"generate the elements of A 2 R1000⇥1000 from a standard normal and set b = Ax + z where x
262"
EXPERIMENTS,0.7861507128309573,"and z are generated from standard normal distributions. We also consider a variant where each
263"
EXPERIMENTS,0.7881873727087576,"column of A is scaled by a sample from a standard normal to induce very-different Li values. In
264"
EXPERIMENTS,0.790224032586558,"Figure 1 we compare several selection rules: random ik and jk, the greedy rule (5), sampling ik and
265"
EXPERIMENTS,0.7922606924643585,"jk proportional to Li, the exact greedy Li rule (21), the ratio greedy Li rule (23), and a variant where
266"
EXPERIMENTS,0.7942973523421588,"we set one coordinate using (23) and other using (21) (switching between the two). All algorithms use
267"
EXPERIMENTS,0.7963340122199593,"the update (22). In these experiments we see that greedy rules lead to faster convergence than random
268"
EXPERIMENTS,0.7983706720977597,"rules in all cases. We see that knowing the Li values does not signiﬁcantly change the performance
269"
EXPERIMENTS,0.8004073319755601,"of the random method, nor does it change the performance of the greedy methods in the case when
270"
EXPERIMENTS,0.8024439918533605,2This strategy is similar to LIBSVM’s rule beginning in version 2.8 for the special case of quadratic functions.
EXPERIMENTS,0.8044806517311609,"the Li were similar. However, with different Li the (expensive) exact greedy method exploiting Li
271"
EXPERIMENTS,0.8065173116089613,"works much better. We found that the ratio method worked similar to or better than the basic greedy
272"
EXPERIMENTS,0.8085539714867617,"method (depending on the random seed), while the switching method often performed closer to the
273"
EXPERIMENTS,0.8105906313645621,"exact method.
274"
EXPERIMENTS,0.8126272912423625,"Figure 1: Random vs greedy coordinate selection rules, including rules using the coordinate-wise
Lipschitz constants Li. The Li are similar in the left plot, but differ signiﬁcantly on the right."
EXPERIMENTS,0.814663951120163,"Our second experiment considers the same problem but with the additional constraints xi 2 [−1, 1].
275"
EXPERIMENTS,0.8167006109979633,"Figure 2 compares the GS-s, GS-q, and GS-1 rules in this setting. We see that the GS-s rule results in
276"
EXPERIMENTS,0.8187372708757638,"the slowest convergence rate, while the GS-q rule rule takes the longest to identify the active set. The
277"
EXPERIMENTS,0.8207739307535642,"GS-1 rule typically updates 2 or 3 variables, but on early iterations it updates up to 5 variables.
278"
EXPERIMENTS,0.8228105906313645,"Figure 2: Comparison of GS-1, GS-q and GS-s under linear equality constraint and bound constraints.
The left plot shows the function values, the middle plot shows the number of interior variables, and
the right plot shows the number of variables updated by the GS-1 rule."
DISCUSSION,0.824847250509165,"6
Discussion
279"
DISCUSSION,0.8268839103869654,"Despite the popularity of LIBSVM, up until this work we did not have a strong justiﬁcation for using
280"
DISCUSSION,0.8289205702647657,"greedy 2-coordinate methods over simpler random 2-coordinate methods for equality-constrained
281"
DISCUSSION,0.8309572301425662,"optimization methods. This work shows that greedy methods may be faster by a factor ranging from
282"
DISCUSSION,0.8329938900203666,"O(n) up to O(n2). This work is the ﬁrst to identify the equivalence between the greedy 2-coordinate
283"
DISCUSSION,0.835030549898167,"update and steepest descent in the 1-norm. The connection to the 1-norm is key to our simple analyses
284"
DISCUSSION,0.8370672097759674,"and also allows us to analyze greedy rules depending on coordinate-wise Lipschitz constants.
285"
DISCUSSION,0.8391038696537678,"For problems with bound constraints and equality constraints, we analyzed the classic GS-q rule but
286"
DISCUSSION,0.8411405295315683,"also proposed the new GS-1 rule. Unlike the GS-s rule the GS-1 rule guarantees non-trivial progress
287"
DISCUSSION,0.8431771894093686,"on each iteration, and unlike the GS-q rule the GS-1 rule can be implemented in O(n log n). We
288"
DISCUSSION,0.845213849287169,"further expect that the GS-1 rule could be implemented in O(n) by using randomized algorithms,
289"
DISCUSSION,0.8472505091649695,"similar to the techniques used to implement O(n)-time projection onto the 1-norm ball Duchi et al.
290"
DISCUSSION,0.8492871690427699,"[2008], van den Berg et al. [2008]. The disadvantage of the GS-1 rule is that on some iterations it may
291"
DISCUSSION,0.8513238289205702,"update more than 2 coordinates on each step. However, when this happens the additional coordinates
292"
DISCUSSION,0.8533604887983707,"are simply moved to their bound. This can allow us to identify the active set of constraints more
293"
DISCUSSION,0.8553971486761711,"quickly. For SVMs this means identifying the support vectors faster, giving cheaper iterations.
294"
REFERENCES,0.8574338085539714,"References
295"
REFERENCES,0.8594704684317719,"Amir Beck. The 2-coordinate descent method for solving double-sided simplex constrained mini-
296"
REFERENCES,0.8615071283095723,"mization problems. Journal of Optimization Theory and Applications, 162(3):892–919, 2014.
297"
REFERENCES,0.8635437881873728,"Dimitri P Bertsekas. Network optimization: continuous and discrete models. Athena Scientiﬁc
298"
REFERENCES,0.8655804480651731,"Belmont, 1998.
299"
REFERENCES,0.8676171079429735,"Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
300"
REFERENCES,0.869653767820774,"transactions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
301"
REFERENCES,0.8716904276985743,"John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the
302"
REFERENCES,0.8737270875763747,"l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on
303"
REFERENCES,0.8757637474541752,"Machine learning, pages 272–279, 2008.
304"
REFERENCES,0.8778004073319755,"Rong-En Fan, Pai-Hsuen Chen, Chih-Jen Lin, and Thorsten Joachims. Working set selection using
305"
REFERENCES,0.879837067209776,"second order information for training support vector machines. Journal of machine learning
306"
REFERENCES,0.8818737270875764,"research, 6(12), 2005.
307"
REFERENCES,0.8839103869653768,"Qin Fang, Min Xu, and Yiming Ying. Faster convergence of a randomized coordinate descent method
308"
REFERENCES,0.8859470468431772,"for linearly constrained optimization problems. Analysis and Applications, 16(05):741–755, 2018.
309"
REFERENCES,0.8879837067209776,"Daniel Horn, Aydın Demircio˘glu, Bernd Bischl, Tobias Glasmachers, and Claus Weihs. A compar-
310"
REFERENCES,0.890020366598778,"ative study on large scale kernelized support vector machines. Advances in Data Analysis and
311"
REFERENCES,0.8920570264765784,"Classiﬁcation, 12(4):867–883, 2018.
312"
REFERENCES,0.8940936863543788,"Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
313"
REFERENCES,0.8961303462321792,"gradient methods under the polyak-łojasiewicz condition. In Joint European conference on
314"
REFERENCES,0.8981670061099797,"machine learning and knowledge discovery in databases, pages 795–811. Springer, 2016.
315"
REFERENCES,0.90020366598778,"Sai Praneeth Karimireddy, Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Efﬁcient
316"
REFERENCES,0.9022403258655805,"greedy coordinate descent for composite problems. In The 22nd International Conference on
317"
REFERENCES,0.9042769857433809,"Artiﬁcial Intelligence and Statistics, pages 2887–2896. PMLR, 2019.
318"
REFERENCES,0.9063136456211812,"Sai Praneeth Reddy Karimireddy, Sebastian Stich, and Martin Jaggi. Adaptive balancing of gra-
319"
REFERENCES,0.9083503054989817,"dient and update computation times using global geometry and approximate subproblems. In
320"
REFERENCES,0.9103869653767821,"International Conference on Artiﬁcial Intelligence and Statistics, pages 1204–1213. PMLR, 2018.
321"
REFERENCES,0.9124236252545825,"S. Sathiya Keerthi, Shirish Krishnaj Shevade, Chiranjib Bhattacharyya, and Karuturi Radha Krishna
322"
REFERENCES,0.9144602851323829,"Murthy. Improvements to platt’s smo algorithm for svm classiﬁer design. Neural computation, 13
323"
REFERENCES,0.9164969450101833,"(3):637–649, 2001.
324"
REFERENCES,0.9185336048879837,"Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of frank-wolfe optimization
325"
REFERENCES,0.9205702647657841,"variants. Advances in neural information processing systems, 28, 2015.
326"
REFERENCES,0.9226069246435845,"I Necoara, Y Nesterov, and F Glineur. A random coordinate descent method on large optimization
327"
REFERENCES,0.924643584521385,"problems with linear constraints. Technical Report, University Politehnica Bucharest, 2011.
328"
REFERENCES,0.9266802443991853,"Ion Necoara and Andrei Patrascu. A random coordinate descent algorithm for optimization problems
329"
REFERENCES,0.9287169042769857,"with composite objective function and linear coupled constraints. Computational Optimization
330"
REFERENCES,0.9307535641547862,"and Applications, 57(2):307–337, 2014.
331"
REFERENCES,0.9327902240325866,"Ion Necoara, Yurii Nesterov, and François Glineur. Random block coordinate descent methods for
332"
REFERENCES,0.9348268839103869,"linearly constrained optimization over networks. Journal of Optimization Theory and Applications,
333"
REFERENCES,0.9368635437881874,"173(1):227–254, 2017.
334"
REFERENCES,0.9389002036659878,"Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM
335"
REFERENCES,0.9409368635437881,"Journal on Optimization, 22(2):341–362, 2012.
336"
REFERENCES,0.9429735234215886,"Julie Nutini. Greed is good: greedy optimization methods for large-scale structured problems. PhD
337"
REFERENCES,0.945010183299389,"thesis, University of British Columbia, 2018.
338"
REFERENCES,0.9470468431771895,"Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt Koepke. Coordinate
339"
REFERENCES,0.9490835030549898,"descent converges faster with the gauss-southwell rule than random selection. In International
340"
REFERENCES,0.9511201629327902,"Conference on Machine Learning, pages 1632–1641. PMLR, 2015.
341"
REFERENCES,0.9531568228105907,"Julie Nutini, Issam Laradji, and Mark Schmidt. Let’s make block coordinate descent converge faster:
342"
REFERENCES,0.955193482688391,"Faster greedy rules, message-passing, active-set complexity, and superlinear convergence. Journal
343"
REFERENCES,0.9572301425661914,"of Machine Learning Research, 23(131):1–74, 2022.
344"
REFERENCES,0.9592668024439919,"Peter Richtárik and Martin Takáˇc. Iteration complexity of randomized block-coordinate descent
345"
REFERENCES,0.9613034623217923,"methods for minimizing a composite function. Mathematical Programming, 144(1):1–38, 2014.
346"
REFERENCES,0.9633401221995926,"Jennifer She and Mark Schmidt. Linear convergence and support vector identiﬁcation of sequential
347"
REFERENCES,0.9653767820773931,"minimal optimization. In 10th NIPS Workshop on Optimization for Machine Learning, volume 5,
348"
REFERENCES,0.9674134419551935,"page 50, 2017.
349"
REFERENCES,0.9694501018329938,"Shirish Krishnaj Shevade and S Sathiya Keerthi. A simple and efﬁcient algorithm for gene selection
350"
REFERENCES,0.9714867617107943,"using sparse logistic regression. Bioinformatics, 19(17):2246–2253, 2003.
351"
REFERENCES,0.9735234215885947,"Chaobing Song, Shaobo Cui, Yong Jiang, and Shu-Tao Xia. Accelerated stochastic greedy coordinate
352"
REFERENCES,0.9755600814663951,"descent by soft thresholding projection onto simplex. Advances in Neural Information Processing
353"
REFERENCES,0.9775967413441955,"Systems, 30, 2017.
354"
REFERENCES,0.9796334012219959,"Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt. Are we there yet? manifold identiﬁca-
355"
REFERENCES,0.9816700610997964,"tion of gradient-related proximal methods. In The 22nd International Conference on Artiﬁcial
356"
REFERENCES,0.9837067209775967,"Intelligence and Statistics, pages 1110–1119. PMLR, 2019.
357"
REFERENCES,0.9857433808553971,"Paul Tseng and Sangwoon Yun. Block-coordinate gradient descent method for linearly constrained
358"
REFERENCES,0.9877800407331976,"nonsmooth separable optimization. Journal of optimization theory and applications, 140(3):
359"
REFERENCES,0.9898167006109979,"513–535, 2009.
360"
REFERENCES,0.9918533604887984,"E. van den Berg, M. Schmidt, M. Friedlander, and K. Murphy. Group sparsity via linear-time
361"
REFERENCES,0.9938900203665988,"projection. 2008.
362"
REFERENCES,0.9959266802443992,"Po-Wei Wang and Chih-Jen Lin. Iteration complexity of feasible descent methods for convex
363"
REFERENCES,0.9979633401221996,"optimization. The Journal of Machine Learning Research, 15(1):1523–1548, 2014.
364"
