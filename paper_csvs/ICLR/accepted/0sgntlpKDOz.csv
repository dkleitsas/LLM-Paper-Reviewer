Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011160714285714285,"Recent advances at the intersection of dense large graph limits and mean ﬁeld
games have begun to enable the scalable analysis of a broad class of dynamical
sequential games with large numbers of agents. So far, results have been largely
limited to graphon mean ﬁeld systems with continuous-time diffusive or jump
dynamics, typically without control and with little focus on computational meth-
ods. We propose a novel discrete-time formulation for graphon mean ﬁeld games
as the limit of non-linear dense graph Markov games with weak interaction. On
the theoretical side, we give extensive and rigorous existence and approximation
properties of the graphon mean ﬁeld solution in sufﬁciently large systems. On the
practical side, we provide general learning schemes for graphon mean ﬁeld equi-
libria by either introducing agent equivalence classes or reformulating the graphon
mean ﬁeld system as a classical mean ﬁeld system. By repeatedly ﬁnding a reg-
ularized optimal control solution and its generated mean ﬁeld, we successfully
obtain plausible approximate Nash equilibria in otherwise infeasible large dense
graph games with many agents. Empirically, we are able to demonstrate on a
number of examples that the ﬁnite-agent behavior comes increasingly close to the
mean ﬁeld behavior for our computed equilibria as the graph or system size grows,
verifying our theory. More generally, we successfully apply policy gradient rein-
forcement learning in conjunction with sequential Monte Carlo methods."
INTRODUCTION,0.002232142857142857,"1
INTRODUCTION"
INTRODUCTION,0.0033482142857142855,"Today, reinforcement learning (RL) ﬁnds application in various application areas such as robotics
(Kober et al., 2013), autonomous driving (Kiran et al., 2021) or navigation of stratospheric balloons
(Bellemare et al., 2020) as a method to realize effective sequential decision-making in complex
problems. RL remains a very active research area, and there remain many challenges in multi-agent
reinforcement learning (MARL) as a generalization of RL such as learning goals, non-stationarity
and scalability of algorithms (Zhang et al., 2021). Nonetheless, potential applications for MARL
are manifold and include e.g. teams of unmanned aerial vehicles (Tožiˇcka et al., 2018; Pham et al.,
2018) or video games (Berner et al., 2019; Vinyals et al., 2017). While the domain of MARL is
somewhat empirically successful, problems quickly become intractable as the number of agents
grows, and methods in MARL typically miss a theoretical foundation. A recent tractable approach
to handling the scalability problem in MARL are competitive mean ﬁeld games and cooperative
mean ﬁeld control (Gu et al., 2021). Instead of considering generic multi agent Markov games, one
considers many agents under the weak interaction principle, i.e. each agent alone has a negligible
inﬂuence on all other agents. This class of models naturally contains a large number of real world
scenarios and can ﬁnd application e.g. in analysis of power network resilience (Bagagiolo & Bauso,
2014), smart heating (Kizilkale & Malhame, 2014), edge computing (Banez et al., 2019) or ﬂocking
(Perrin et al., 2021b). See also Djehiche et al. (2017) for a review of other engineering applications."
INTRODUCTION,0.004464285714285714,"Mean ﬁeld games.
Mean ﬁeld games (MFGs) were ﬁrst popularized in the independent seminal
works of Huang et al. (2006) and Lasry & Lions (2007) for the setting of differential games with
diffusion-type dynamics given by stochastic differential equations. See also Guéant et al. (2011);
Bensoussan et al. (2013) for a review. Since then, extensions have been manifold and include e.g.
discrete-time (Saldi et al., 2018), partial observability (Saldi et al., 2019), major-minor formulations"
INTRODUCTION,0.005580357142857143,Published as a conference paper at ICLR 2022
INTRODUCTION,0.006696428571428571,"(Nourian & Caines, 2013) and many more. In the learning community, there has been immense
recent interest in ﬁnding and analyzing solution methods for mean ﬁeld equilibria (Cardaliaguet &
Hadikhanloo, 2017; Mguni et al., 2018; Guo et al., 2019; Subramanian & Mahajan, 2019; Elie et al.,
2020; Cui & Koeppl, 2021; Pasztor et al., 2021; Perolat et al., 2021; Perrin et al., 2021a), solving
the inverse reinforcement learning problem (Yang et al., 2018a) or applying related approximations
directly to MARL (Yang et al., 2018b). In contrast to our work, Yang et al. (2018a) consider states
instead of agents on a graph, while Yang et al. (2018b) requires restrictive assumptions and only
considers averages instead of distributions of the neighbors. Recently, focus increased also on the
cooperative case of mean ﬁeld control (Carmona et al., 2019b; Mondal et al., 2021), for which
dynamic programming holds on an enlarged state space, resulting in a high-dimensional Markov
decision process (Pham & Wei, 2018; Motte & Pham, 2019; Gu et al., 2020; Cui et al., 2021)."
INTRODUCTION,0.0078125,"Mean ﬁeld systems on graphs.
For mean ﬁeld systems on dense graphs, prior work mostly con-
siders mean ﬁeld systems without control (Vizuete et al., 2020) or time-dynamics, i.e. the static case
(Parise & Ozdaglar, 2019; Carmona et al., 2019a). To the best of our knowledge, Gao & Caines
(2017) and Caines & Huang (2019) are the ﬁrst to consider general continuous-time diffusion-type
graphon mean ﬁeld systems with control, the latter proposing many clusters of agents as well as
proving an approximate Nash property as the number of clusters and agents grows. There have since
been efforts to control cooperative graphon mean ﬁeld systems with diffusive linear dynamics using
spectral methods (Gao & Caines, 2019a;b). On the other hand, Bayraktar et al. (2020); Bet et al.
(2020) consider large non-clustered systems in a continuous-time diffusion-type setting without con-
trol, while Aurell et al. (2021b) and Aurell et al. (2021a) consider continuous-time linear-quadratic
systems and continuous-time jump processes respectively. To the best of our knowledge, only Vasal
et al. (2021) have considered solving and formulating a graphon mean ﬁeld game in discrete time,
though requiring analytic computation of an inﬁnite-dimensional value function deﬁned over all
mean ﬁelds and thus being inapplicable to arbitrary problems in a black-box, learning manner. In
contrast, we give a general learning scheme and also provide extensive theoretical analysis of our
algorithms and (slightly different) model. Finally, for sparse graphs there exist preliminary results
(Gkogkas & Kuehn, 2020; Lacker & Soret, 2020), though the setting remains to be developed."
INTRODUCTION,0.008928571428571428,"Our contribution.
In this work, we propose a dense graph limit extension of MFGs in discrete
time, combining graphon mean ﬁeld systems with mean ﬁeld games. More speciﬁcally, we consider
limits of many-agent systems with discrete-time graph-based dynamics and weak neighbor interac-
tions. In contrast to prior works, we consider one of the ﬁrst general discrete-time formulations as
well as its controlled case, which is a natural setting for many problems that are inherently discrete
in time or to be controlled digitally at discrete decision times. Our contribution can be summarized
as: (i) formulating one of the ﬁrst general discrete-time graphon MFG frameworks for approximat-
ing otherwise intractable large dense graph games; (ii) providing an extensive theoretical analysis of
existence and approximation properties in such systems; (iii) providing general learning schemes for
ﬁnding graphon mean ﬁeld equilibria, and (iv) empirically evaluating our proposed approach with
veriﬁcation of theoretical results in the ﬁnite N-agent graph system, ﬁnding plausible approximate
Nash equilibria for otherwise infeasible large dense graph games with many agents."
DENSE GRAPH MEAN FIELD GAMES,0.010044642857142858,"2
DENSE GRAPH MEAN FIELD GAMES"
DENSE GRAPH MEAN FIELD GAMES,0.011160714285714286,"In the following, we will give a dense graph N-agent model as well as its corresponding mean ﬁeld
system, where agents are affected only by the overall state distribution of all neighbors, as visualized
in Figure 1. As a result of the law of large numbers, this distribution will become deterministic – the
mean ﬁeld – as N →∞. We begin with graph-theoretical preliminaries, see also Lovász (2012) for
a review. The study of dense large graph limits deals with the limiting representation of adjacency
matrices called graphons. Deﬁne I := [0, 1] and W0 as the space of all bounded, symmetric and
measurable functions (graphons) W ∈W0, W : I × I →R bounded by 0 ≤W ≤1. For any
simple graph G = ({1, . . . , N}, E), we deﬁne its step-graphon a.e. uniquely by"
DENSE GRAPH MEAN FIELD GAMES,0.012276785714285714,"WG(x, y) =
X"
DENSE GRAPH MEAN FIELD GAMES,0.013392857142857142,"i,j∈{1,...,N}
1(i,j)∈E · 1x∈( i−1 N , i"
DENSE GRAPH MEAN FIELD GAMES,0.014508928571428572,"N ] · 1y∈( j−1 N , j"
DENSE GRAPH MEAN FIELD GAMES,0.015625,"N ],
(1)"
DENSE GRAPH MEAN FIELD GAMES,0.016741071428571428,Published as a conference paper at ICLR 2022
DENSE GRAPH MEAN FIELD GAMES,0.017857142857142856,"Figure 1: Graphical model visualization. (a): A graph with 5 nodes; (b): The associated step
graphon of the graph in (a) as a continuous domain version of its adjacency matrix; (c): A visualiza-
tion of the dynamics, i.e. the center agent is affected only by its neighbors (grey)."
DENSE GRAPH MEAN FIELD GAMES,0.018973214285714284,see e.g. Figure 1. We equip W0 with the cut (semi-)norm ∥·∥□and cut (pseudo-)metric δ□
DENSE GRAPH MEAN FIELD GAMES,0.020089285714285716,"∥W∥□:= sup
S,T  Z"
DENSE GRAPH MEAN FIELD GAMES,0.021205357142857144,"S×T
W(x, y) dx dy
 ,
δ□(W, W ′) := inf
ϕ ∥W −W ′
ϕ∥□,
(2)"
DENSE GRAPH MEAN FIELD GAMES,0.022321428571428572,"for graphons W, W ′ ∈W0 and W ′
ϕ(x, y) := W ′(ϕ(x), ϕ(y)), where the supremum is over all
measurable subsets S, T ⊆I and the inﬁmum is over measure-preserving bijections ϕ: I →I."
DENSE GRAPH MEAN FIELD GAMES,0.0234375,"To provide motivation, note that convergence in δ□is equivalent to e.g. convergence of probabil-
ities of locally encountering any ﬁxed subgraph by randomly sampling a subset of nodes. Many
such properties of graph sequences (GN)N∈N converging to some graphon W ∈W0 can then be
described by W, and we point to Lovász (2012) for details. In this work, we will primarily use the
analytical fact that for converging graphon sequences ∥WGN −W∥□→0, we equivalently have"
DENSE GRAPH MEAN FIELD GAMES,0.024553571428571428,"∥WGN −W∥L∞→L1 =
sup
∥g∥∞≤1 Z I  Z"
DENSE GRAPH MEAN FIELD GAMES,0.025669642857142856,"I
(WGN (α, β) −W(α, β))g(β) dβ
 dα →0
(3)"
DENSE GRAPH MEAN FIELD GAMES,0.026785714285714284,"under the operator norm of operators L∞→L1, see e.g. Lovász (2012), Lemma 8.11."
DENSE GRAPH MEAN FIELD GAMES,0.027901785714285716,"By Lovász (2012), Theorem 11.59, the above is equivalent to convergence in the cut metric
δ□(WGN , W) →0 up to relabeling. In the following, we will therefore assume sequences of simple
graphs GN = (VN, EN) with vertices VN = {1, . . . , N}, edge sets EN, edge indicator variables
ξN
i,j := 1(i,j)∈EN for all nodes i, j ∈VN, and associated step graphons WN converging in cut norm."
DENSE GRAPH MEAN FIELD GAMES,0.029017857142857144,"Assumption 1. The sequence of step-graphons (WN)N∈N converges in cut norm ∥·∥□or equiva-
lently in operator norm ∥·∥L∞→L1 as N →∞to some graphon W ∈W0, i.e."
DENSE GRAPH MEAN FIELD GAMES,0.030133928571428572,"∥WN −W∥□→0,
∥WN −W∥L∞→L1 →0 .
(4)"
DENSE GRAPH MEAN FIELD GAMES,0.03125,"Next, we deﬁne W-random graphs to consist of vertices VN
:= {1, . . . , N} with adjacency
matrices ξN generated by sampling graphon indices αi uniformly from I and edges ξN
i,j ∼
Bernoulli(W(αi, αj)) for all vertices i, j ∈VN. For experiments, by Lovász (2012), Lemma 10.16,
we can thereby generate a.s. converging graph sequences by sampling W-random graphs for any
ﬁxed graphon W ∈W0. In principle, one could also consider arbitrary graph generating processes
whenever a valid relabeling function ϕ is known."
DENSE GRAPH MEAN FIELD GAMES,0.03236607142857143,"In our work, the usage of graphons enables us to ﬁnd mean ﬁeld systems on dense graphs and
to extend the expressiveness of classical MFGs. As examples, we will use the limiting graphons
of uniform attachment, ranked attachment and p-Erd˝os–Rényi (ER) random graphs given by
Wunif(x, y) = 1 −max(x, y), Wrank(x, y) = 1 −xy and Wer(x, y) = p respectively (Borgs et al.,
2011; Lovász, 2012), each of which exhibits different node connectivities as shown in Figure 2."
DENSE GRAPH MEAN FIELD GAMES,0.033482142857142856,"Figure 2: Three graphons used in our experiments. (a): Uniform attachment graphon; (b): Ranked
attachment graphon; (c): Erd˝os–Rényi (ER) graphon with edge probability 0.5."
DENSE GRAPH MEAN FIELD GAMES,0.03459821428571429,Published as a conference paper at ICLR 2022
DENSE GRAPH MEAN FIELD GAMES,0.03571428571428571,"Finite agent graph game.
For simplicity of analysis, we consider ﬁnite state and action spaces
X, U as well as times T := {0, 1, . . . , T −1}. On a metric space A, deﬁne the spaces of all Borel
probability measures P(A) and all Borel measures B1(A) bounded by 1, equipped with the L1
norm. For simpliﬁed notation, we denote both a measure ν and its probability mass function by
ν(·). Deﬁne the space of policies Π := P(U)T ×X , i.e. agents apply Markovian feedback policies
πi = (πi
t)t∈T ∈Π that act on local state information. This allows for the deﬁnition of weakly
interacting agent state and action random variables"
DENSE GRAPH MEAN FIELD GAMES,0.036830357142857144,"Xi
0 ∼µ0,
U i
t ∼πi
t(· | Xi
t),
Xi
t+1 ∼P(· | Xi
t, U i
t, Gi
t),
∀t ∈T , ∀i ∈VN
(5)"
DENSE GRAPH MEAN FIELD GAMES,0.03794642857142857,"under some transition kernel P : X ×U ×B1(X) →P(X), where the empirical neighborhood mean
ﬁeld Gi
t of agent i is deﬁned as the B1(X)-valued (unnormalized) neighborhood state distribution"
DENSE GRAPH MEAN FIELD GAMES,0.0390625,"Gi
t := 1 N X"
DENSE GRAPH MEAN FIELD GAMES,0.04017857142857143,"j∈VN
ξN
i,jδXj
t ,
(6)"
DENSE GRAPH MEAN FIELD GAMES,0.041294642857142856,"where δ is the Dirac measure, i.e. each agent affects each other at most negligibly with factor 1/N.
Finally, for each agent i we deﬁne separate, competitive objectives"
DENSE GRAPH MEAN FIELD GAMES,0.04241071428571429,"JN
i (π1, . . . , πN) := E"
DENSE GRAPH MEAN FIELD GAMES,0.04352678571428571,"""T −1
X"
DENSE GRAPH MEAN FIELD GAMES,0.044642857142857144,"t=0
r(Xi
t, U i
t, Gi
t) # (7)"
DENSE GRAPH MEAN FIELD GAMES,0.04575892857142857,"to be maximized over πi, where r : X × U × B1(X) →R is an arbitrary reward function."
DENSE GRAPH MEAN FIELD GAMES,0.046875,"Remark 1. We can also consider inﬁnite horizons, ˜JN
i (π1, . . . , πN) ≡E
P∞
t=0 γtr(Xi
t, U i
t, Gi
t)
"
DENSE GRAPH MEAN FIELD GAMES,0.04799107142857143,"with all results but Theorem 4 holding. One may also extend to state-action distributions, heteroge-
neous starting conditions and time-dependent r, P, though we avoid this for expositional simplicity.
Remark 2. Note that it is straightforward to extend to heterogeneous agents by modelling agent
types as part of the agent state, see also e.g. Mondal et al. (2021). It is only required to model agent
states in a uniﬁed manner, which does not imply that there can be no heterogeneity."
DENSE GRAPH MEAN FIELD GAMES,0.049107142857142856,"With this, we can give a typical notion of Nash equilibria as found e.g. in Saldi et al. (2018). How-
ever, under graph convergence in Assumption 1, it is always possible for a ﬁnite number of nodes
to have an arbitrary neighborhood differing from the graphon as N →∞. Thus, it is impossible
to show approximate optimality for all nodes and only possible to show for an increasingly large
fraction 1 −p →1 of nodes. For this reason, we slightly weaken the notion of Nash equilibria by
restricting to a fraction 1 −p of agents, as e.g. considered in (Carmona, 2004; Elie et al., 2020).
Deﬁnition 1. An (ε, p)-Markov-Nash equilibrium (almost Markov-Nash equilibrium) for ε, p > 0
is deﬁned as a tuple of policies (π1, . . . , πN) ∈ΠN such that for any i ∈WN, we have"
DENSE GRAPH MEAN FIELD GAMES,0.05022321428571429,"JN
i (π1, . . . , πN) ≥sup
π∈Π
JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −ε,
(8)"
DENSE GRAPH MEAN FIELD GAMES,0.05133928571428571,"for some set WN ⊆VN containing at least ⌊(1 −p)N⌋agents, i.e. |WN| ≥⌊(1 −p)N⌋."
DENSE GRAPH MEAN FIELD GAMES,0.052455357142857144,"The minimal such ε > 0 for any ﬁxed policy tuple (and typically p = 0) is also called its exploitabil-
ity. Whilst we ordain ε-optimality only for a fraction 1 −p of agents, if the fraction p is negligible,
it will have negligible impact on other agents as a result of the weak interaction property. Thus, the
solution will remain approximately optimal for almost all agents for sufﬁciently small p regardless
of the behavior of that fraction p of agents. In the following, we will give a limiting system that shall
provide (ε, p)-Markov-Nash equilibria with ε, p →0 as N →∞."
DENSE GRAPH MEAN FIELD GAMES,0.05357142857142857,"Graphon mean ﬁeld game.
The formal N →∞limit of the N-agent game constitutes its
graphon mean ﬁeld game (GMFG), which shall be rigorously justiﬁed in Section 3. We deﬁne the
space of measurable state marginal ensembles Mt := P(X)I and measurable mean ﬁeld ensembles
M := P(X)T ×I, in the sense that α 7→µα
t (x) is measurable for any µ ∈M, t ∈T , x ∈X.
Similarly, we deﬁne the space of measurable policy ensembles Π ⊆ΠI, i.e. with measurable
α 7→πα
t (u | x) for any π ∈Π, t ∈T , x ∈X, u ∈U."
DENSE GRAPH MEAN FIELD GAMES,0.0546875,"In the GMFG, we will consider inﬁnitely many agents α ∈I instead of the ﬁnitely many i ∈VN.
As a result, we will have inﬁnitely many policies πα ∈Π – one for each agent α – through some
measurable policy ensemble π ∈Π. We again deﬁne state and action random variables"
DENSE GRAPH MEAN FIELD GAMES,0.05580357142857143,"Xα
0 ∼µ0,
U α
t ∼πα
t (· | Xα
t ),
Xα
t+1 ∼P(· | Xα
t , U α
t , Gα
t ),
∀(α, t) ∈I × T
(9)"
DENSE GRAPH MEAN FIELD GAMES,0.056919642857142856,Published as a conference paper at ICLR 2022
DENSE GRAPH MEAN FIELD GAMES,0.05803571428571429,where we introduce the (now deterministic) B1(X)-valued neighborhood mean ﬁeld of agents α as
DENSE GRAPH MEAN FIELD GAMES,0.05915178571428571,"Gα
t :=
Z"
DENSE GRAPH MEAN FIELD GAMES,0.060267857142857144,"I
W(α, β)µβ
t dβ
(10)"
DENSE GRAPH MEAN FIELD GAMES,0.06138392857142857,"for some deterministic µ ∈M. Under ﬁxed π ∈Π, µα
t should be understood as the law of Xα
t ,
µα
t ≡L(Xα
t ). Finally, deﬁne the maximization objective of agent α over πα for ﬁxed µ ∈M as"
DENSE GRAPH MEAN FIELD GAMES,0.0625,"Jµ
α (πα) ≡E"
DENSE GRAPH MEAN FIELD GAMES,0.06361607142857142,"""T −1
X"
DENSE GRAPH MEAN FIELD GAMES,0.06473214285714286,"t=0
r(Xα
t , U α
t , Gα
t ) #"
DENSE GRAPH MEAN FIELD GAMES,0.06584821428571429,".
(11)"
DENSE GRAPH MEAN FIELD GAMES,0.06696428571428571,"To formulate the limiting version of Nash equilibria, we deﬁne a map Ψ: Π →M mapping from a
policy ensemble π ∈Π to the corresponding generated mean ﬁeld ensemble µ = Ψ(π) ∈M by"
DENSE GRAPH MEAN FIELD GAMES,0.06808035714285714,"µα
0 ≡µ0,
µα
t+1(x′) ≡
X"
DENSE GRAPH MEAN FIELD GAMES,0.06919642857142858,"x∈X
µα
t (x)
X"
DENSE GRAPH MEAN FIELD GAMES,0.0703125,"u∈U
πα
t (u | x)P(x′ | x, u, Gα
t ),
∀α ∈I
(12)"
DENSE GRAPH MEAN FIELD GAMES,0.07142857142857142,"where integrability in (10) holds by induction, and note how then µα
t = L(Xα
t )."
DENSE GRAPH MEAN FIELD GAMES,0.07254464285714286,"Similarly, let Φ: M →2Π map from a mean ﬁeld ensemble µ to the set of optimal policy ensem-
bles π characterized by πα ∈arg maxπ∈Π Jµ
α (π) for all α ∈I, which is particularly fulﬁlled if
πα
t (u | x) > 0 =⇒u ∈arg maxu′∈U Qµ
α(t, x, u′) for all α ∈I, t ∈T , x ∈X, u ∈U, where Qµ
α
is the optimal action value function under ﬁxed µ ∈M following the Bellman equation"
DENSE GRAPH MEAN FIELD GAMES,0.07366071428571429,"Qµ
α(t, x, u) = r(x, u, Gα
t ) +
X"
DENSE GRAPH MEAN FIELD GAMES,0.07477678571428571,"x′∈X
P(x′ | x, u, Gα
t ) arg max
u′∈U
Qµ
α(t + 1, x′, u′)
(13)"
DENSE GRAPH MEAN FIELD GAMES,0.07589285714285714,"with Qµ
α(T, x, u) ≡0 and generally time-dependent, see Puterman (2014) for a review."
DENSE GRAPH MEAN FIELD GAMES,0.07700892857142858,"We can now deﬁne the GMFG version of Nash equilibria as policy ensembles π generating mean
ﬁeld ensembles µ under which they are optimal, as µα
t = L(Xα
t ) if all agents α ∈I follow πα.
Deﬁnition 2. A Graphon Mean Field Equilibrium (GMFE) is a pair (π, µ) ∈Π × M such that
π ∈Φ(µ) and µ = Ψ(π)."
THEORETICAL ANALYSIS,0.078125,"3
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.07924107142857142,"To obtain meaningful optimality results beyond empirical mean ﬁeld convergence, we will need
a Lipschitz assumption as in the uncontrolled, continuous-time case (cf. Bayraktar et al. (2020),
Condition 2.3) and typical in mean ﬁeld theory (Huang et al., 2006).
Assumption 2. Let r, P, W be Lipschitz continuous with Lipschitz constants Lr, LP , LW > 0."
THEORETICAL ANALYSIS,0.08035714285714286,"Note that all proofs but Theorem 1 also hold for only block-wise Lipschitz continuous W, see
Appendix A.1. Since X × U × B1(X) is compact, r is bounded by the extreme value theorem.
Proposition 1. Under Assumption 2, r will be bounded by |r| ≤Mr for some constant Mr > 0."
THEORETICAL ANALYSIS,0.08147321428571429,"We then obtain existence of a GMFE by reformulating the GMFG as a classical MFG and applying
existing results from Saldi et al. (2018). More precisely, we consider the equivalent MFG with
extended state space X × I, action space U, policy ˜π ∈P(U)T ×X×I, mean ﬁeld ˜µ ∈P(X × I)T ,
reward function ˜r((x, α), u, ˜µ) := r(x, u,
R"
THEORETICAL ANALYSIS,0.08258928571428571,"I W(αt, β)˜µt(·, β) dβ) and transition dynamics such
that the states ( ˜Xt, αt) follow ( ˜X0, α0) ∼˜µ0 := µ0 ⊗Unif([0, 1]) and"
THEORETICAL ANALYSIS,0.08370535714285714,"˜Ut ∼˜πt(· | ˜Xt, αt),
˜Xt+1 ∼P(· | ˜Xt, ˜Ut,
Z"
THEORETICAL ANALYSIS,0.08482142857142858,"I
W(αt, β)˜µt(·, β) dβ),
αt+1 = αt .
(14)"
THEORETICAL ANALYSIS,0.0859375,"Theorem 1. Under Assumption 2, there exists a GMFE (π, µ) ∈Π × M."
THEORETICAL ANALYSIS,0.08705357142857142,"Meanwhile, in ﬁnite games, even showing the existence of Nash equilibria in local feedback policies
is problematic (Saldi et al., 2018). Note however, that while this reformulation will be useful for
learning and existence, it does not allow us to conclude that the ﬁnite graph game is well approx-
imated, as classical MFG approximation theorems e.g. in Saldi et al. (2018) do not consider the
graph structure and directly use the limiting graphon W in the dynamics (14)."
THEORETICAL ANALYSIS,0.08816964285714286,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.08928571428571429,"As our next main result, we shall therefore show rigorously that the GMFE can provide increasingly
good approximations of the N-agent ﬁnite graph game as N →∞. As mentioned, the following
also holds for only block-wise Lipschitz continuous W instead of fully Lipschitz continuous W.
Complete mathematical proofs together with additional theoretical supplements can be found in
Appendix A.1 and A.2. To obtain joint N-agent policies as approximate Nash equilibria from a
GMFE (π, µ), we deﬁne the map ΓN(π) := (π1, π2, . . . , πN) ∈ΠN, where"
THEORETICAL ANALYSIS,0.09040178571428571,"πi
t(u | x) := παi
t (u | x),
∀(α, t, x, u) ∈I × T × X × U
(15)"
THEORETICAL ANALYSIS,0.09151785714285714,"with αi =
i
N , as by Assumption 1 the agents are correctly labeled such that they match up with their
limiting graphon indices αi ∈I. In our experiments, we use the αi generated during the generation
process of the W-random graphs, though for arbitrary ﬁnite systems one would have to ﬁrst identify
the graphon as well as an appropriate assignment of agents to graphon indices αi ∈I, which is a
separate, non-trivial problem requiring at least graphon estimation, e.g. Xu (2018)."
THEORETICAL ANALYSIS,0.09263392857142858,"For theoretical analysis, we propose to lift the empirical distributions and policy tuples to the con-
tinuous domain I, i.e. under an N-agent policy tuple (π1, . . . , πN) ∈ΠN, we deﬁne the step policy
ensemble πN ∈Π and the random empirical step measure ensemble µN ∈M by"
THEORETICAL ANALYSIS,0.09375,"πN,α
t
:=
X"
THEORETICAL ANALYSIS,0.09486607142857142,"i∈VN
1α∈( i−1 N , i"
THEORETICAL ANALYSIS,0.09598214285714286,"N ] · πi
t,
µN,α
t
:=
X"
THEORETICAL ANALYSIS,0.09709821428571429,"i∈VN
1α∈( i−1 N , i"
THEORETICAL ANALYSIS,0.09821428571428571,"N ] · δXj
t ,
∀(α, t) ∈I × T .
(16)"
THEORETICAL ANALYSIS,0.09933035714285714,"In the following, we consider deviations of the i-th agent from (π1, π2, . . . , πN) = ΓN(π) ∈ΠN
to (π1, . . . , πi−1, ˆπ, πi+1, . . . , πN) ∈ΠN, i.e. the i-th agent deviates by instead applying ˆπ ∈Π.
Note that this includes the special case of no agent deviations. For any f : X × I →R and state
marginal ensemble µt ∈Mt, deﬁne µt(f) :=
R I
P"
THEORETICAL ANALYSIS,0.10044642857142858,"x∈X f(x, α)µα
t (x) dα. We are now ready to
state our ﬁrst result of convergence of empirical state distributions to the mean ﬁeld, potentially at
the classical rate O(1/
√"
THEORETICAL ANALYSIS,0.1015625,"N) and consistent with results in uncontrolled, continuous-time diffusive
graphon mean ﬁeld systems (cf. Bayraktar et al. (2020), Theorem 3.2).
Theorem 2. Consider Lipschitz continuous π ∈Π up to a ﬁnite number of discontinuities Dπ,
with associated mean ﬁeld ensemble µ = Ψ(π). Under Assumption 1 and the N-agent policy
(π1, . . . , πi−1, ˆπ, πi+1, . . . , πN) ∈ΠN with (π1, π2, . . . , πN) = ΓN(π) ∈ΠN, ˆπ ∈Π, t ∈T , we
have for all measurable functions f : X × I →R uniformly bounded by some Mf > 0, that"
THEORETICAL ANALYSIS,0.10267857142857142,"E
µN
t (f) −µt(f)

→0
(17)"
THEORETICAL ANALYSIS,0.10379464285714286,"uniformly over all possible deviations ˆπ ∈Π, i ∈VN. Furthermore, if the graphon convergence in
Assumption 1 is at rate O(1/
√"
THEORETICAL ANALYSIS,0.10491071428571429,"N), then this rate of convergence is also O(1/
√ N)."
THEORETICAL ANALYSIS,0.10602678571428571,"In particular, the technical Lipschitz requirement of π typically holds for neural-network-based poli-
cies (Mondal et al., 2021; Pasztor et al., 2021) and includes also the case of ﬁnitely many optimality
regimes over all graphon indices α ∈I, which is sufﬁcient to achieve arbitrarily good approximate
Nash equilibria through our algorithms as shown in Section 4. We would like to remark that the
above result generalizes convergence of state histograms to the mean ﬁeld solution, since the state
marginals of agents are additionally close to each of their graphon mean ﬁeld equivalents. The above
will be necessary to show convergence of the dynamics of a deviating agent to ˆX"
THEORETICAL ANALYSIS,0.10714285714285714,"i
N
0
∼µ0,
ˆU"
THEORETICAL ANALYSIS,0.10825892857142858,"i
N
t
∼ˆπt(· | ˆX"
THEORETICAL ANALYSIS,0.109375,"i
N
t ),
ˆX"
THEORETICAL ANALYSIS,0.11049107142857142,"i
N
t+1 ∼P(· | ˆX"
THEORETICAL ANALYSIS,0.11160714285714286,"i
N
t , ˆU"
THEORETICAL ANALYSIS,0.11272321428571429,"i
N
t , G"
THEORETICAL ANALYSIS,0.11383928571428571,"i
N
t ),
∀t ∈T
(18)"
THEORETICAL ANALYSIS,0.11495535714285714,"for almost all agents i, i.e. the dynamics are approximated by using the limiting deterministic
neighborhood mean ﬁeld G
i
N , see Appendix A.1. This will imply the approximate Nash property:
Theorem 3. Consider a GMFE (π, µ) with Lipschitz continuous π up to a ﬁnite number of discon-
tinuities Dπ. Under Assumptions 1 and 2, for any ε, p > 0 there exists N ′ such that for all N > N ′,
the policy (π1, . . . , πN) = ΓN(π) ∈ΠN is an (ε, p)-Markov Nash equilibrium, i.e."
THEORETICAL ANALYSIS,0.11607142857142858,"JN
i (π1, . . . , πN) ≥max
π∈Π JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −ε
(19)"
THEORETICAL ANALYSIS,0.1171875,for all i ∈WN and some WN ⊆VN : |WN| ≥⌊(1 −p)N⌋.
THEORETICAL ANALYSIS,0.11830357142857142,"In general, Nash equilibria are highly intractable (Daskalakis et al., 2009). Therefore, solving the
GMFG allows obtaining approximate Nash equilibria in the N-agent system for sufﬁciently large N,
since ε, p →0 as N →∞. As a side result, we also obtain ﬁrst results for the uncontrolled discrete-
time case by considering trivial action spaces with |U| = 1, see Corollary A.2 in the Appendix."
THEORETICAL ANALYSIS,0.11941964285714286,Published as a conference paper at ICLR 2022
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12053571428571429,"4
LEARNING GRAPHON MEAN FIELD EQUILIBRIA"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12165178571428571,"By learning GMFE, one may potentially solve otherwise intractable large N-agent games. For learn-
ing, we can apply any existing techniques for classical MFGs (e.g. Mguni et al. (2018); Subramanian
& Mahajan (2019); Guo et al. (2019)), since by (14) we have reformulated the GMFG as a classical
MFG with extended state space. Nonetheless, it may make sense to treat the graphon index α ∈I
separately, e.g. when treating special cases such as block graphons, or by grouping graphically sim-
ilar agents. We repeatedly apply two functions ˆΦ, ˆΨ by beginning with the mean ﬁeld µ0 = ˆΨ(π0)
generated by the uniformly random policy π0, and computing πn+1 = ˆΦ(µn), µn+1 = ˆΨ(πn+1)
for n = 0, 1, . . . until convergence using one of the following two approaches:"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12276785714285714,"• Equivalence classes method. We introduce agent equivalence classes, or discretization,
of I for the otherwise uncountably many agents α ∈I by partitioning I into M subsets.
For example, in the special case of block graphons (block-wise constant W), one can solve
separately for each block equivalence class (type) of agents, since all agents in the class
share the same dynamics. Note that in contrast to multi-class MFGs (Huang et al., 2006),
GMFGs are rigorously connected to ﬁnite graph games and can handle an uncountable
number of classes α. To deal with general graphons, we choose equidistant representatives
αi ∈I, i = 1, . . . , M covering the whole interval I, and approximate each agent α ∈˜Ii
by the nearest αi for the intervals ˜Ii ⊆I of points closest to that αi to obtain M approxi-
mate equivalence classes. Formally, we approximate mean ﬁelds ˆΨ(π) = PM
i=1 1α∈˜Ii ˆµαi
recursively computed over all times for any ﬁxed policy ensemble π, and similarly policies
ˆΦ(µ) = PM
i=1 1α∈˜Iiπαi where παi is the optimal policy of αi for ﬁxed µ. We solve
the optimal control problem for each equivalence class using backwards induction (alter-
natively, one may use reinforcement learning), and solve the evolution equation for the
representatives αi of the equivalence classes recursively. For space reasons, the details are
found in Appendix A.3. Note that this does not mean that we consider the N-agent problem
with N = M, but instead we approximate the limiting problem with the limiting graphon
W, and the solution will be near-optimal for all sufﬁciently large ﬁnite systems at once."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12388392857142858,"• Direct reinforcement learning. We directly apply RL as ˆΦ. The central idea is to consider
the GMFG as a classical MFG with extended state space X × I, i.e. for ﬁxed mean ﬁelds,
we solve the MDP deﬁned by (14). Agents condition their policy not only on their own
state, but also their node index α ∈I and the current time t ∈T , since the mean ﬁelds
are non-stationary in general and require time-dependent policies for optimality. Here, we
assume that we can sample from a simulator of (9) for a given ﬁxed mean ﬁeld as commonly
assumed in MFG learning literature (Guo et al., 2019; Subramanian & Mahajan, 2019). For
application to arbitrary ﬁnite systems, one could apply a model-based RL approach coupled
with graphon estimation, though this remains outside the scope of this work. For solving
the mean ﬁeld evolution equation (12), we can again use any applicable numerical method
and choose a conventional sequential Monte Carlo method for ˆΨ. While it is possible to
exactly solve optimal control problems for each agent equivalence class with ﬁnite state-
action spaces, this is generally not the case for e.g. continuous state-action spaces. Here,
a general reinforcement learning solution can solve otherwise intractable problems in an
elegant manner, since the graphon index α simply becomes part of a continuous state space."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.125,"For convergence, we begin by stating the classical feedback regularity condition (Huang et al., 2006;
Guo et al., 2019) after equipping Π, M e.g. with the supremum metric."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12611607142857142,"Proposition 2. Assume that the maps ˆΨ, ˆΦ are Lipschitz with constants c1, c2 and c1 · c2 < 1. Then
the ﬁxed point iteration µn+1 = ˆΨ(ˆΦ(µn)) converges."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12723214285714285,"Feedback regularity is not assured, and thus there is no general convergence guarantee. Whilst one
could attempt to apply ﬁctitious play (Mguni et al., 2018), additional assumptions will be needed
for convergence. Instead, whenever necessary for convergence, we regularize by introducing Boltz-
mann policies πα
t (u | x) ∝exp( 1"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12834821428571427,"ηQµ
α(t, x, u)) with temperature η, provably converging to an
approximation for sufﬁciently high temperatures (Cui & Koeppl, 2021).
Theorem 4. Under Assumptions 1 and 2, the equivalence classes algorithm with Boltzmann policies
ˆΦ(µ)α
t (u | x) ∝exp( 1"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.12946428571428573,"ηQµ
α(t, x, u)) converges for sufﬁciently high temperatures η > 0."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13058035714285715,Published as a conference paper at ICLR 2022
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13169642857142858,"0
20
40
t 0.0 0.5 1.0"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.1328125,"πα
t (D | S)"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13392857142857142,"(a)
Wunif 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13504464285714285,"0
20
40
t 0.0 0.5 1.0"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13616071428571427,"πα
t (D | S)"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13727678571428573,"(b)
Wrank 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13839285714285715,"0
20
40
t 0.0 0.5 1.0"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.13950892857142858,"πα
t (D | S)"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.140625,"(c)
Wer 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14174107142857142,"0
20
40
t 0.00 0.25 0.50"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14285714285714285,"µα
t (I) 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14397321428571427,"0
20
40
t 0.00 0.25 0.50"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14508928571428573,"µα
t (I) 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14620535714285715,"0
20
40
t 0.00 0.25 0.50"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14732142857142858,"µα
t (I) 0.0 0.5 1.0
α"
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.1484375,"Figure 3: Achieved equilibrium via M = 100 approximate equivalence classes in SIS-Graphon,
plotted for each agent α ∈I. Top: Probability of taking precautions when healthy. Bottom: Proba-
bility of being infected. It can be observed that agents with less connections (higher α) will take less
precautions. (a): Uniform attachment graphon; (b): Ranked attachment graphon; (c): ER graphon."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.14955357142857142,"Importantly, even an exact solution of the GMFG only constitutes an approximate Nash equilibrium
in the ﬁnite-graph system. Furthermore, even the existence of exact ﬁnite-system Nash equilibria
in local feedback policies is not guaranteed, see the discussion in Saldi et al. (2018) and references
therein. Therefore, little is lost by introducing slight additional approximations for the sake of a
tractable solution, if at all needed (e.g. the Investment-Graphon problem in the following converges
without introducing Boltzmann policies), since near optimality holds for small temperatures (Cui &
Koeppl, 2021). Indeed, we ﬁnd that we can show optimality of the equivalence classes approach for
sufﬁciently ﬁne partitions of I, giving us a theoretical foundation for our proposed algorithms."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.15066964285714285,"Theorem 5. Under Assumptions 1 and 2, for a solution (π, µ) ∈Π × M, π ∈ˆΦ(µ), µ = ˆΨ(π)
of the M equivalence classes method and for any ε, p > 0 there exists N ′, M ∈N such that for all
N > N ′, the policy (π1, . . . , πN) = ΓN(π) ∈ΠN is an (ε, p)-Markov Nash equilibrium."
LEARNING GRAPHON MEAN FIELD EQUILIBRIA,0.15178571428571427,"A theoretically rigorous analysis of the elegant direct reinforcement learning approach is beyond our
scope and deferred to future works, though we empirically ﬁnd that both methods agree."
EXPERIMENTS,0.15290178571428573,"5
EXPERIMENTS"
EXPERIMENTS,0.15401785714285715,"In this section, we will give an empirical veriﬁcation of our theoretical results. As we are unaware
of any prior discrete-time GMFGs (except for the example in Vasal et al. (2021), which is similar
to the ﬁrst problem in the following), we propose two problems adapted from existing non-graph-
based works on the three graphons in Figure 2. For space reasons, we defer detailed descriptions of
problems and algorithms, plots as well as further analysis, including exploitability and a veriﬁcation
of stability of our solution with respect to the number of equivalence classes – to Appendix A.3."
EXPERIMENTS,0.15513392857142858,"The SIS-Graphon problem was considered in Cui & Koeppl (2021) as a classical discrete-time
MFG. We impose an epidemics scenario where people (agents) are infected with probability propor-
tional to the number of infected neighbors and recover with ﬁxed probability. People may choose to
take precautions (e.g. social distancing), avoiding potential costly infection periods at a ﬁxed cost."
EXPERIMENTS,0.15625,"In the Investment-Graphon problem – an adaptation of a problem studied by Chen et al. (2021),
where it was in turn adapted from Weintraub et al. (2010) – we consider many ﬁrms maximizing
proﬁts, where proﬁts are proportional to product quality and decrease with total neighborhood prod-
uct quality, i.e. the graph models overlap in e.g. product audience or functionality. Firms can invest
to improve quality, though it becomes more unlikely to improve quality as their quality rises."
EXPERIMENTS,0.15736607142857142,"Learned equilibrium behavior.
For the SIS-Graphon problem, we apply softmax policies for
each approximate equivalence class to achieve convergence, see Appendix A.3 for details on tem-
perature choice and inﬂuence. In Figure 3, the learned behavior can be observed for various α. As
expected, in the ER graphon case, behavior is identical over all α. Otherwise, we ﬁnd that agents
take more precautions with many connections (low α) than with few connections (high α). For the
uniform attachment graphon, we observe no precautions in case of negligible connectivity (α →1),
while for the ranked attachment graphon there is no such α ∈I (cf. Figure 2). Further, the fraction
of infected agents at stationarity rises as α falls. A similar analysis holds for Investment-Graphon
without need for regularization, see Appendix A.3."
EXPERIMENTS,0.15848214285714285,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.15959821428571427,"0
25
50
75
100
N 0 10"
EXPERIMENTS,0.16071428571428573,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.16183035714285715,"(a)
SIS-Graphon, Wunif"
EXPERIMENTS,0.16294642857142858,"0
25
50
75
100
N 0 5"
EXPERIMENTS,0.1640625,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.16517857142857142,"(b)
SIS-Graphon, Wrank"
EXPERIMENTS,0.16629464285714285,"0
25
50
75
100
N 0 10"
EXPERIMENTS,0.16741071428571427,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.16852678571428573,"(c)
SIS-Graphon, Wer"
EXPERIMENTS,0.16964285714285715,"0
25
50
75
100
N 0 10"
EXPERIMENTS,0.17075892857142858,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.171875,"Investment-Graphon, Wunif"
EXPERIMENTS,0.17299107142857142,"0
25
50
75
100
N 0 5"
EXPERIMENTS,0.17410714285714285,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.17522321428571427,"Investment-Graphon, Wrank"
EXPERIMENTS,0.17633928571428573,"0
25
50
75
100
N 0 5"
EXPERIMENTS,0.17745535714285715,"maxi |JN
i −Jαi|"
EXPERIMENTS,0.17857142857142858,"Investment-Graphon, Wer"
EXPERIMENTS,0.1796875,"Figure 4: Decreasing maximum deviation between average N-agent objective and mean ﬁeld objec-
tive over all agents for the GMFE policy and 5 W-random graph sequences. (a): Uniform attachment
graphon; (b): Ranked attachment graphon; (c): ER graphon."
EXPERIMENTS,0.18080357142857142,"Note that the speciﬁc method of solution is not of central importance here, as in general any RL
and ﬁltering method can be substituted to handle 1. otherwise intractable or 2. inherently sample-
based settings. Indeed, we achieve similar results using PPO (Schulman et al., 2017) in Investment-
Graphon, enabling a general RL-based methodology for GMFGs. In Appendix A.3, we ﬁnd that
PPO achieves qualitatively and quantitatively similar behavior to the equivalence classes method,
with slight deviations due to the approximations from PPO and Monte Carlo. In particular, the
PPO exploitability ε ≈2 remains low compared to ε > 30 for the uniform random policy, see
Appendix A.3. In Appendix A.3, we also show how, due to the non-stationarity of the environment,
a naive application of MARL (Yu et al., 2021) fails to converge, while existing mean ﬁeld MARL
techniques (Yang et al., 2018b) remain incomparable as agents must observe the average actions of
all neighbors. On SIS-Graphon, we require softmax policies to achieve convergence, which is not
possible with PPO as no Q-function is learned. In general, one could use entropy regularized poli-
cies, e.g. SAC (Haarnoja et al., 2018), or alternatively use any value-based reinforcement learning
method, though an investigation of the best approach is outside of our scope."
EXPERIMENTS,0.18191964285714285,"Quantitative veriﬁcation of the mean ﬁeld approximation.
To verify the rigorously established
accuracy of our mean ﬁeld system empirically, we will generate W-random graphs. Note that there
are considerable difﬁculties associated with an empirical veriﬁcation of (19), since 1. for any N
one must check the Nash property for (almost) all N agents, 2. ﬁnding optimal ˆπ is intractable,
as no dynamic programming principle holds on the non-Markovian local agent state, while acting
on the full state fails by the curse of dimensionality, and 3. the inaccuracy from estimating all
JN
i , i = 1, . . . , N at once increases with N due to variance, i.e. cost scales fast with N for ﬁxed
variance. Instead, we verify (26) in Appendix A.1 using the GMFE policy on systems of up to
N = 100 agents, i.e. ˆπ = παi for the closest αi and comparing for all agents at once (p = 0).
Shown in Figure 4, for W-random graph sequences, at each N we performed 10000 runs to estimate
maxi |JN
i
−Jαi|. We ﬁnd that the maximum deviation between achieved returns and mean ﬁeld
return decreases as N →∞, verifying that we obtain an increasingly good approximation of the
ﬁnite N-agent graph system. The oscillations in Figure 4 stem from the randomly sampled graphs."
CONCLUSION,0.18303571428571427,"6
CONCLUSION"
CONCLUSION,0.18415178571428573,"In this work, we have formulated a new framework for dense graph-based dynamical games with the
weak interaction property. On the theoretical side, we have given one of the ﬁrst general discrete-
time GMFG formulations with existence conditions and approximate Nash property of the ﬁnite
graph system, thus extending classical MFGs and allowing for a tractable, theoretically well-founded
solution of competitive large-scale graph-based games on large dense graphs. On the practical side,
we have proposed a number of computational methods to tractably compute GMFE and experi-
mentally veriﬁed the plausibility of our methodology on a number of examples. Venues for further
extensions are manifold and include extensions of theory to e.g. continuous spaces, partial ob-
servability or common noise. So far, graphons assume dense graphs and cannot properly describe
sparse graphs (W = 0), which remain an active frontier of research. Finally, real-world applica-
tion scenarios may be of interest, where estimation of agent graphon indices becomes important for
model-based MARL. We hope that our work inspires further applications and research into scalable
MARL using graphical dynamical systems based on graph limit theory and mean ﬁeld theory."
CONCLUSION,0.18526785714285715,Published as a conference paper at ICLR 2022
CONCLUSION,0.18638392857142858,ACKNOWLEDGMENTS
CONCLUSION,0.1875,"This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY
center. The authors also acknowledge support by the German Research Foundation (DFG) via the
Collaborative Research Center (CRC) 1053 – MAKI."
ETHICS STATEMENT,0.18861607142857142,ETHICS STATEMENT
ETHICS STATEMENT,0.18973214285714285,"Existing mean ﬁeld methodologies, including ours, currently require manual modeling and have not
been applied in a model-based reinforcement learning setting for given ﬁnite agent systems. As a
result, we do not foresee any immediate ethical issues stemming from this work."
REPRODUCIBILITY STATEMENT,0.19084821428571427,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.19196428571428573,"For reproducibility, in the supplement we provide all code required to reproduce all results in this
work. This includes but is not limited to our models and problems, algorithms as well as all plotting
scripts for all of the ﬁgures found in this work."
REFERENCES,0.19308035714285715,REFERENCES
REFERENCES,0.19419642857142858,"Alexander Aurell, Rene Carmona, Gokce Dayanikli, and Mathieu Lauriere. Finite state graphon
games with applications to epidemics. arXiv preprint arXiv:2106.07859, 2021a."
REFERENCES,0.1953125,"Alexander Aurell, Rene Carmona, and Mathieu Lauriere. Stochastic graphon games: Ii. the linear-
quadratic case. arXiv preprint arXiv:2105.12320, 2021b."
REFERENCES,0.19642857142857142,"Fabio Bagagiolo and Dario Bauso. Mean-ﬁeld games and dynamic demand management in power
grids. Dynamic Games and Applications, 4(2):155–176, 2014."
REFERENCES,0.19754464285714285,"Reginald A Banez, Lixin Li, Chungang Yang, Lingyang Song, and Zhu Han. A mean-ﬁeld-type
game approach to computation ofﬂoading in mobile edge computing networks. In ICC 2019-
2019 IEEE International Conference on Communications (ICC), pp. 1–6. IEEE, 2019."
REFERENCES,0.19866071428571427,"Erhan Bayraktar, Suman Chakraborty, and Ruoyu Wu. Graphon mean ﬁeld systems. arXiv preprint
arXiv:2003.13180, 2020."
REFERENCES,0.19977678571428573,"Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77–82, 2020."
REFERENCES,0.20089285714285715,"Alain Bensoussan, Jens Frehse, and Phillip Yam. Mean ﬁeld games and mean ﬁeld type control
theory, volume 101. Springer, 2013."
REFERENCES,0.20200892857142858,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019."
REFERENCES,0.203125,"Gianmarco Bet, Fabio Coppini, and Francesca R Nardi. Weakly interacting oscillators on dense
random graphs. arXiv preprint arXiv:2006.07670, 2020."
REFERENCES,0.20424107142857142,"Christian Borgs, Jennifer Chayes, László Lovász, Vera Sós, and Katalin Vesztergombi. Limits of
randomly grown graph sequences. European Journal of Combinatorics, 32(7):985–999, 2011."
REFERENCES,0.20535714285714285,"Peter E Caines and Minyi Huang.
Graphon mean ﬁeld games and the gmfg equations: ε-nash
equilibria. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 286–292. IEEE,
2019."
REFERENCES,0.20647321428571427,"Pierre Cardaliaguet and Saeed Hadikhanloo. Learning in mean ﬁeld games: the ﬁctitious play.
ESAIM: Control, Optimisation and Calculus of Variations, 23(2):569–591, 2017."
REFERENCES,0.20758928571428573,"Guilherme Carmona. Nash equilibria of games with a continuum of players, 2004."
REFERENCES,0.20870535714285715,Published as a conference paper at ICLR 2022
REFERENCES,0.20982142857142858,"René Carmona, Daniel Cooney, Christy Graves, and Mathieu Lauriere. Stochastic graphon games:
I. the static case. arXiv preprint arXiv:1911.10664, 2019a."
REFERENCES,0.2109375,"René Carmona, Mathieu Laurière, and Zongjun Tan. Model-free mean-ﬁeld reinforcement learning:
mean-ﬁeld mdp and mean-ﬁeld q-learning. arXiv preprint arXiv:1910.12802, 2019b."
REFERENCES,0.21205357142857142,"Yang Chen, Jiamou Liu, and Bakhadyr Khoussainov. Agent-level maximum entropy inverse rein-
forcement learning for mean ﬁeld games. arXiv preprint arXiv:2104.14654, 2021."
REFERENCES,0.21316964285714285,"Kai Cui and Heinz Koeppl. Approximately solving mean ﬁeld games via entropy-regularized deep
reinforcement learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp.
1909–1917. PMLR, 2021."
REFERENCES,0.21428571428571427,"Kai Cui, Anam Tahir, Mark Sinzger, and Heinz Koeppl. Discrete-time mean ﬁeld control with
environment states. arXiv preprint arXiv:2104.14900, 2021."
REFERENCES,0.21540178571428573,"Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou.
The complexity of
computing a nash equilibrium. SIAM Journal on Computing, 39(1):195–259, 2009."
REFERENCES,0.21651785714285715,"Boualem Djehiche, Alain Tcheukam, and Hamidou Tembine. Mean-ﬁeld-type games in engineering
[j]. AIMS Electronics and Electrical Engineering, 1(1):18–73, 2017."
REFERENCES,0.21763392857142858,"Romuald Elie, Julien Perolat, Mathieu Laurière, Matthieu Geist, and Olivier Pietquin. On the con-
vergence of model free learning in mean ﬁeld games. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pp. 7143–7150, 2020."
REFERENCES,0.21875,"Shuang Gao and Peter E Caines. The control of arbitrary size networks of linear systems via graphon
limits: An initial investigation. In 2017 IEEE 56th Annual Conference on Decision and Control
(CDC), pp. 1052–1057. IEEE, 2017."
REFERENCES,0.21986607142857142,"Shuang Gao and Peter E Caines. Graphon control of large-scale networks of linear systems. IEEE
Transactions on Automatic Control, 65(10):4090–4105, 2019a."
REFERENCES,0.22098214285714285,"Shuang Gao and Peter E Caines. Spectral representations of graphons in very large network systems
control. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 5068–5075. IEEE,
2019b."
REFERENCES,0.22209821428571427,"Marios-Antonios Gkogkas and Christian Kuehn. Graphop mean-ﬁeld limits for kuramoto-type mod-
els. arXiv preprint arXiv:2007.02868, 2020."
REFERENCES,0.22321428571428573,"Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-ﬁeld controls with q-learning for coop-
erative marl: Convergence and complexity analysis. arXiv preprint arXiv:2002.04131, 2020."
REFERENCES,0.22433035714285715,"Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-ﬁeld multi-agent reinforcement learning:
A decentralized network approach. arXiv preprint arXiv:2108.02731, 2021."
REFERENCES,0.22544642857142858,"Olivier Guéant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean ﬁeld games and applications. In
Paris-Princeton lectures on mathematical ﬁnance 2010, pp. 205–266. Springer, 2011."
REFERENCES,0.2265625,"Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-ﬁeld games. In Advances in
Neural Information Processing Systems, pp. 4966–4976, 2019."
REFERENCES,0.22767857142857142,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.22879464285714285,"Minyi Huang, Roland P Malhamé, and Peter E Caines. Large population stochastic dynamic games:
closed-loop mckean-vlasov systems and the nash certainty equivalence principle. Communica-
tions in Information & Systems, 6(3):221–252, 2006."
REFERENCES,0.22991071428571427,"B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yoga-
mani, and Patrick Pérez. Deep reinforcement learning for autonomous driving: A survey. IEEE
Transactions on Intelligent Transportation Systems, 2021."
REFERENCES,0.23102678571428573,Published as a conference paper at ICLR 2022
REFERENCES,0.23214285714285715,"Arman C Kizilkale and Roland P Malhame. Collective target tracking mean ﬁeld control for electric
space heaters. In 22nd Mediterranean Conference on Control and Automation, pp. 829–834.
IEEE, 2014."
REFERENCES,0.23325892857142858,"Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238–1274, 2013."
REFERENCES,0.234375,"Daniel Lacker and Agathe Soret. A case study on stochastic games on large graphs in mean ﬁeld
and sparse regimes. arXiv preprint arXiv:2005.14102, 2020."
REFERENCES,0.23549107142857142,"Jean-Michel Lasry and Pierre-Louis Lions. Mean ﬁeld games. Japanese journal of mathematics, 2
(1):229–260, 2007."
REFERENCES,0.23660714285714285,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053–3062. PMLR, 2018."
REFERENCES,0.23772321428571427,"László Lovász. Large networks and graph limits, volume 60. American Mathematical Soc., 2012."
REFERENCES,0.23883928571428573,"David Mguni, Joel Jennings, and Enrique Munoz de Cote. Decentralised learning in systems with
many, many strategic agents. Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.23995535714285715,"Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V Ukkusuri. On the approx-
imation of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean ﬁeld
control (mfc). arXiv preprint arXiv:2109.04024, 2021."
REFERENCES,0.24107142857142858,"Médéric Motte and Huyên Pham. Mean-ﬁeld markov decision processes with common noise and
open-loop controls. arXiv preprint arXiv:1912.07883, 2019."
REFERENCES,0.2421875,"Mojtaba Nourian and Peter E Caines. ϵ-nash mean ﬁeld game theory for nonlinear stochastic dy-
namical systems with major and minor agents. SIAM Journal on Control and Optimization, 51
(4):3302–3331, 2013."
REFERENCES,0.24330357142857142,"Francesca Parise and Asuman Ozdaglar. Graphon games. In Proceedings of the 2019 ACM Confer-
ence on Economics and Computation, pp. 457–458, 2019."
REFERENCES,0.24441964285714285,"Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efﬁcient model-based multi-agent mean-ﬁeld
reinforcement learning. arXiv preprint arXiv:2107.04050, 2021."
REFERENCES,0.24553571428571427,"Julien Perolat, Sarah Perrin, Romuald Elie, Mathieu Laurière, Georgios Piliouras, Matthieu Geist,
Karl Tuyls, and Olivier Pietquin. Scaling up mean ﬁeld games with online mirror descent. arXiv
preprint arXiv:2103.00623, 2021."
REFERENCES,0.24665178571428573,"Sarah Perrin, Mathieu Laurière, Julien Pérolat, Romuald Élie, Matthieu Geist, and Olivier Pietquin.
Generalization in mean ﬁeld games by learning master policies. arXiv preprint arXiv:2109.09717,
2021a."
REFERENCES,0.24776785714285715,"Sarah Perrin, Mathieu Laurière, Julien Pérolat, Matthieu Geist, Romuald Élie, and Olivier Pietquin.
Mean ﬁeld games ﬂock! the reinforcement learning way.
arXiv preprint arXiv:2105.07933,
2021b."
REFERENCES,0.24888392857142858,"Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Neﬁan. Cooperative and distributed
reinforcement learning of drones for ﬁeld coverage. arXiv preprint arXiv:1803.07250, 2018."
REFERENCES,0.25,"Huyên Pham and Xiaoli Wei. Bellman equation and viscosity solutions for mean-ﬁeld stochas-
tic control problem. ESAIM: Control, Optimisation and Calculus of Variations, 24(1):437–461,
2018."
REFERENCES,0.25111607142857145,"Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014."
REFERENCES,0.25223214285714285,"Naci Saldi, Tamer Basar, and Maxim Raginsky. Markov–nash equilibria in mean-ﬁeld games with
discounted cost. SIAM Journal on Control and Optimization, 56(6):4256–4287, 2018."
REFERENCES,0.2533482142857143,Published as a conference paper at ICLR 2022
REFERENCES,0.2544642857142857,"Naci Saldi, Tamer Ba¸sar, and Maxim Raginsky. Approximate nash equilibria in partially observed
stochastic games with mean-ﬁeld interactions. Mathematics of Operations Research, 44(3):1006–
1033, 2019."
REFERENCES,0.25558035714285715,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.25669642857142855,"Jayakumar Subramanian and Aditya Mahajan.
Reinforcement learning in stationary mean-ﬁeld
games. In Proceedings of the 18th International Conference on Autonomous Agents and MultiA-
gent Systems, pp. 251–259, 2019."
REFERENCES,0.2578125,"Jan Tožiˇcka, Benedek Szulyovszky, Guillaume de Chambrier, Varun Sarwal, Umar Wani, and Man-
tas Gribulis. Application of deep reinforcement learning to uav ﬂeet control. In Proceedings of
SAI Intelligent Systems Conference, pp. 1169–1177. Springer, 2018."
REFERENCES,0.25892857142857145,"Deepanshu Vasal, Rajesh Mishra, and Sriram Vishwanath. Sequential decomposition of graphon
mean ﬁeld games. In 2021 American Control Conference (ACC), pp. 730–736. IEEE, 2021."
REFERENCES,0.26004464285714285,"Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017."
REFERENCES,0.2611607142857143,"Renato Vizuete, Paolo Frasca, and Federica Garin. Graphon-based sensitivity analysis of sis epi-
demics. IEEE Control Systems Letters, 4(3):542–547, 2020."
REFERENCES,0.2622767857142857,"Gabriel Y Weintraub, C Lanier Benkard, and Benjamin Van Roy. Computational methods for obliv-
ious equilibrium. Operations research, 58(4-part-2):1247–1265, 2010."
REFERENCES,0.26339285714285715,"Jiaming Xu. Rates of convergence of spectral methods for graphon estimation. In International
Conference on Machine Learning, pp. 5433–5442. PMLR, 2018."
REFERENCES,0.26450892857142855,"Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha. Learning deep mean
ﬁeld games for modeling large population behavior. In International Conference on Learning
Representations, 2018a."
REFERENCES,0.265625,"Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean ﬁeld multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571–5580.
PMLR, 2018b."
REFERENCES,0.26674107142857145,"James J Yeh. Real Analysis: Theory Of Measure And Integration. World Scientiﬁc Publishing
Company, 2014."
REFERENCES,0.26785714285714285,"Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games.
arXiv preprint arXiv:2103.01955,
2021."
REFERENCES,0.2689732142857143,"Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp.
321–384, 2021."
REFERENCES,0.2700892857142857,Published as a conference paper at ICLR 2022
REFERENCES,0.27120535714285715,"A
APPENDIX"
REFERENCES,0.27232142857142855,"A.1
THEORETICAL DETAILS"
REFERENCES,0.2734375,"In this section, we will give all intermediate results required to prove the results in the main text, as
well as additional results, e.g. for the uncontrolled case. For convenience, we ﬁrst state all obtained
theoretical result. Proofs for each of the theorems and corollaries can be found in their own sections
further below."
REFERENCES,0.27455357142857145,"Note that except for Theorem 1, as mentioned in the main text we can also slightly weaken As-
sumption 2 to block-wise Lipschitz continuous W, i.e. there exist LW > 0 and disjoint intervals
{I1, . . . , IQ}, ∪iIi = I s.t. ∀i, j ∈{1, . . . , Q},"
REFERENCES,0.27566964285714285,"|W(x, y) −W(˜x, ˜y)| ≤LW (|x −˜x| + |y −˜y|),
∀(x, y), (˜x, ˜y) ∈Ii × Ij
(20)"
REFERENCES,0.2767857142857143,which is fulﬁlled e.g. for block-wise Lipschitz-continuous or block-wise constant graphons.
REFERENCES,0.2779017857142857,"For α ∈I, deﬁne the α-neighborhood maps Gα : Mt →P(X) and empirical α-neighborhood
maps Gα
N : Mt →P(X) as"
REFERENCES,0.27901785714285715,"Gα(µt) :=
Z"
REFERENCES,0.28013392857142855,"I
W(α, β)µβ
t dβ,
Gα
N(µt) :=
Z"
REFERENCES,0.28125,"I
WN(α, β)µβ
t dβ
(21)"
REFERENCES,0.28236607142857145,"and note how we naturally have Gα
t = Gα(µt) in the mean ﬁeld system and Gi
t = G"
REFERENCES,0.28348214285714285,"i
N
N (µN
t ) in
the ﬁnite system. Finally, for ν, ν′ ∈Mt, π ∈Π and graphon W, deﬁne the ensemble transition
kernel operator P π
t,ν,W : Mt →Mt via"
REFERENCES,0.2845982142857143," 
νP π
t,ν′,W
α ≡
X"
REFERENCES,0.2857142857142857,"x∈X
να(x)
X"
REFERENCES,0.28683035714285715,"u∈U
πα
t (u | x)P

· | x, u,
Z"
REFERENCES,0.28794642857142855,"I
W(α, β)ν′β dβ

(22)"
REFERENCES,0.2890625,"and note how we have µt+1 = µtP π
t,µt,W in the mean ﬁeld system."
REFERENCES,0.29017857142857145,"After showing Theorem 2, we continue by showing convergence of the law of deviating agent state
Xi
t to the law of the corresponding auxiliary mean ﬁeld systems given by ˆX"
REFERENCES,0.29129464285714285,"i
N
0
∼µ0,
ˆU"
REFERENCES,0.2924107142857143,"i
N
t
∼ˆπt(· | ˆX"
REFERENCES,0.2935267857142857,"i
N
t ),
ˆX"
REFERENCES,0.29464285714285715,"i
N
t+1 ∼P(· | ˆX"
REFERENCES,0.29575892857142855,"i
N
t , ˆU"
REFERENCES,0.296875,"i
N
t , G"
REFERENCES,0.29799107142857145,"i
N
t ),
∀t ∈T
(23)"
REFERENCES,0.29910714285714285,"for almost all agents i as N →∞.
Lemma A.1. Consider Lipschitz continuous π ∈Π up to a ﬁnite number of discontinuities Dπ,
with associated mean ﬁeld ensemble µ = Ψ(π). Under Assumptions 1 and 2 and the N-agent
policy (π1, . . . , πi−1, ˆπ, πi+1, . . . , πN) ∈ΠN where (π1, π2, . . . , πN) = ΓN(π) ∈ΠN, ˆπ ∈Π
arbitrary, for any uniformly bounded family of functions G from X to R and any ε, p > 0, t ∈T ,
there exists N ′ ∈N such that for all N > N ′ we have"
REFERENCES,0.3002232142857143,"sup
g∈G"
REFERENCES,0.3013392857142857,"E

g(Xi
t)

−E
h
g( ˆX"
REFERENCES,0.30245535714285715,"i
N
t )
i < ε
(24)"
REFERENCES,0.30357142857142855,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋."
REFERENCES,0.3046875,"Similarly, for any uniformly Lipschitz, uniformly bounded family of measurable functions H from
X × B1(X) to R and any ε, p > 0, t ∈T , there exists N ′ ∈N such that for all N > N ′ we have"
REFERENCES,0.30580357142857145,"sup
h∈H"
REFERENCES,0.30691964285714285,"E
h
h(Xi
t, G"
REFERENCES,0.3080357142857143,"i
N
N (µN
t ))
i
−E
h
h( ˆX"
REFERENCES,0.3091517857142857,"i
N
t , G
i
N (µt))
i < ε
(25)"
REFERENCES,0.31026785714285715,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋."
REFERENCES,0.31138392857142855,"As a direct implication of the above results, the objective functions of almost all agents converge
uniformly to the mean ﬁeld objectives.
Corollary A.1. Consider Lipschitz continuous π ∈Π up to a ﬁnite number of discontinuities Dπ,
with associated mean ﬁeld ensemble µ = Ψ(π). Under Assumptions 1 and 2 and the N-agent
policy (π1, . . . , πi−1, ˆπ, πi+1, . . . , πN) ∈ΠN where (π1, π2, . . . , πN) = ΓN(π) ∈ΠN, ˆπ ∈Π
arbitrary, for any ε, p > 0, there exists N ′ ∈N such that for all N > N ′ we have
JN
i (π1, . . . , πi−1, ˆπ, πi+1, . . . , πN) −Jµ
i
N (ˆπ)
 < ε
(26)"
REFERENCES,0.3125,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋."
REFERENCES,0.31361607142857145,Published as a conference paper at ICLR 2022
REFERENCES,0.31473214285714285,"The approximate Nash property (Theorem 3) of a GMFE (π, µ) then follows immediately from the
deﬁnition of a GMFE, since π is by deﬁnition optimal under µ."
REFERENCES,0.3158482142857143,"As a corollary, we also obtain results for the uncontrolled case without actions, which is equivalent
to the case where |U| = 1, i.e. there being only one trivial policy that is always optimal."
REFERENCES,0.3169642857142857,"Corollary A.2. Under Assumption 1 and |U| = 1, we have for all measurable functions f : X ×I →
R uniformly bounded by |f| ≤Mf and all t ∈T that"
REFERENCES,0.31808035714285715,"E
µN
t (f) −µt(f)

→0 .
(27)"
REFERENCES,0.31919642857142855,"Furthermore, if the convergence in Assumption 1 is at rate O(1/
√"
REFERENCES,0.3203125,"N), the rate of convergence is
also at O(1/
√ N)."
REFERENCES,0.32142857142857145,"If further Assumption 2 holds, then for any uniformly bounded family of functions G from X to R
and any ε, p > 0, t ∈T , there exists N ′ ∈N such that for all N > N ′ we have"
REFERENCES,0.32254464285714285,"sup
g∈G"
REFERENCES,0.3236607142857143,"E

g(Xi
t)

−E
h
g( ˆX"
REFERENCES,0.3247767857142857,"i
N
t )
i < ε
(28)"
REFERENCES,0.32589285714285715,"uniformly over i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋, and similarly for any
uniformly Lipschitz, uniformly bounded family of measurable functions H from X × B1(X) to R
and any ε, p > 0, t ∈T , there exists N ′ ∈N such that for all N > N ′ we have"
REFERENCES,0.32700892857142855,"sup
h∈H"
REFERENCES,0.328125,"E
h
h(Xi
t, G"
REFERENCES,0.32924107142857145,"i
N
N (µN
t ))
i
−E
h
h( ˆX"
REFERENCES,0.33035714285714285,"i
N
t , G
i
N (µt))
i < ε
(29)"
REFERENCES,0.3314732142857143,uniformly over i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋.
REFERENCES,0.3325892857142857,"A.2
PROOFS"
REFERENCES,0.33370535714285715,"In this section, we will give full proofs to the statements made in the main text and in Appendix A.1."
REFERENCES,0.33482142857142855,"A.2.1
PROOF OF THEOREM 1"
REFERENCES,0.3359375,"Proof. First, we will verify Saldi et al. (2018), Assumption 1 for the MFG with dynamics given by
(14). For this purpose, as in Saldi et al. (2018) let us metrize the product space with the sup-metric,
and equip the space P(X × I) with the weak topology. Note that the results hold for both the ﬁnite
and inﬁnite horizon setting, see Saldi et al. (2018), Remark 6."
REFERENCES,0.33705357142857145,"(a) The reward function ˜r((x, α), u, ˜µ) := r(x, u,
R"
REFERENCES,0.33816964285714285,"I W(αt, β)˜µt(·, β) dβ) is continuous,
since for ((xn, αn), un, ˜µn) →((x, α), u, ˜µ) we have
Z"
REFERENCES,0.3392857142857143,"I
W(αn, β)˜µn(·, β) dβ →
Z"
REFERENCES,0.3404017857142857,"I
W(α, β)˜µ(·, β) dβ"
REFERENCES,0.34151785714285715,"by Lipschitz continuity of W and weak convergence of ˜µn, and therefore"
REFERENCES,0.34263392857142855,"r(xn, un,
Z"
REFERENCES,0.34375,"I
W(αn, β)˜µn(·, β) dβ) →r(x, u,
Z"
REFERENCES,0.34486607142857145,"I
W(α, β)˜µ(·, β) dβ)"
REFERENCES,0.34598214285714285,by Assumption 2.
REFERENCES,0.3470982142857143,(b) The action space is compact and the state space is locally compact.
REFERENCES,0.3482142857142857,"(c) Consider the moment function w(x, α) ≡2. In this case, we can choose ζ = 1 (we use ζ
instead of α in Saldi et al. (2018))."
REFERENCES,0.34933035714285715,"(d) The stochastic kernel ˜P that fulﬁlls (14) such that ( ˜Xt+1, αt+1) ∼˜P(· | ( ˜Xt, αt), ˜Ut, ˜µt)
is weakly continuous, since for ((xn, αn), un, ˜µn) →((x, α), u, ˜µ) we again have
Z"
REFERENCES,0.35044642857142855,"I
W(αn, β)˜µn(·, β) dβ →
Z"
REFERENCES,0.3515625,"I
W(α, β)˜µ(·, β) dβ"
REFERENCES,0.35267857142857145,Published as a conference paper at ICLR 2022
REFERENCES,0.35379464285714285,"and therefore for any continuous bounded f : X × I →R,
Z"
REFERENCES,0.3549107142857143,"X×I
f d ˜P(· | (xn, αn), un, ˜µn) =
Z I Z"
REFERENCES,0.3560267857142857,"X
f dP α(· | (xn, α), un, ˜µn) δαn(dα) =
Z"
REFERENCES,0.35714285714285715,"X
f dP(· | xn, un,
Z"
REFERENCES,0.35825892857142855,"I
W(αn, β)˜µn(·, β) dβ) →
Z"
REFERENCES,0.359375,"X
f dP(· | x, u,
Z"
REFERENCES,0.36049107142857145,"I
W(α, β)˜µ(·, β) dβ) =
Z"
REFERENCES,0.36160714285714285,"X×I
f d ˜P(· | (x, α), u, ˜µ)"
REFERENCES,0.3627232142857143,by disintegration of ˜P and (14).
REFERENCES,0.3638392857142857,"(e) By boundedness of r, we trivially have v(x) ≡1 ≤∞."
REFERENCES,0.36495535714285715,"(f) By boundedness of r, we can trivially choose β = 1 (we ﬂip the usage of β and γ)."
REFERENCES,0.36607142857142855,"(g) As a result of the above choices, ζγβ = γ < 1 trivially."
REFERENCES,0.3671875,"By Saldi et al. (2018), Theorem 3.3 we have the existence of a mean ﬁeld equilibrium (˜π, ˜µ) with
some Markovian feedback policy ˜π acting on the state ( ˜Xt, αt). By deﬁning the mean ﬁeld and
policy ensembles π, µ via πα
t (u | x) = ˜πt(u | x, α), µα
t = ˜µt(·, α), we obtain existence of the
α-a.e. optimal policy ensemble π, since at any time t ∈T , the joint state-action distribution ˜µt ⊗˜πt
puts mass 1 on optimal state-action pairs (see Saldi et al. (2018), Theorem 3.6), implying that for
a.e. α the policy must be optimal, as otherwise there exists a non-null set ˜I0 ⊆I such that for all
α ∈˜I0, there is some suboptimality ε > 0, which directly contradicts the prequel."
REFERENCES,0.36830357142857145,"For the remaining suboptimal α ∈I0 in the null set I0 ⊆I, we redeﬁne π optimally for those α
(always possible in our case, see e.g. Puterman (2014)). This policy ensemble generates µ = Ψ(π)
α-a.e. uniquely, and we need only consider its α-a.e. unique equivalence class for optimality,
implying π ∈Φ(µ). Furthermore, µ is always measurable by deﬁnition, whereas π is measurable
because ˜πt is by deﬁnition a Markov kernel, and thus ˜πt(u | ·, ·) = ˜πt({u} | ·, ·) for Borel set {u} is
a measurable function, which implies measurability of ˜πt(u | x, ·) (see e.g. Yeh (2014), Appendix
E). Therefore, we have proven existence of the GMFE (π, µ)."
REFERENCES,0.36941964285714285,"A.2.2
PROOF OF THEOREM 2"
REFERENCES,0.3705357142857143,Proof. The proof is by induction as follows.
REFERENCES,0.3716517857142857,"Initial case.
For t = 0, we trivially have for all measurable functions f : X × I →R uniformly
bounded by |f| ≤Mf a law of large numbers result"
REFERENCES,0.37276785714285715,"E
µN
0 (f) −µ0(f)
 = E "" Z I X"
REFERENCES,0.37388392857142855,"x∈X
µN,α
0
(x) f(x, α) −
X"
REFERENCES,0.375,"x∈X
µα
0 (x) f(x, α) dα  # = E"
REFERENCES,0.37611607142857145,"""
1
N X i∈VN Z ( i−1 N , i"
REFERENCES,0.37723214285714285,"N ]
f(Xi
0, α) dα −E ""Z ( i−1 N , i"
REFERENCES,0.3783482142857143,"N ]
f(Xi
0, α) dα #! # ≤  E  "
N,0.3794642857142857,"1
N X i∈VN Z ( i−1 N , i"
N,0.38058035714285715,"N ]
f(Xi
0, α) dα −E ""Z ( i−1 N , i"
N,0.38169642857142855,"N ]
f(Xi
0, α) dα #!!2    1
2 =  1 N 2
X"
N,0.3828125,"i∈VN
E   Z ( i−1 N , i"
N,0.38392857142857145,"N ]
f(Xi
0, α) dα −E ""Z ( i−1 N , i"
N,0.38504464285714285,"N ]
f(Xi
0, α) dα #!2    1
2"
N,0.3861607142857143,"≤2Mf
√ N"
N,0.3872767857142857,"by deﬁnition of µN
t , independence of {Xi
0}i∈VN and Xi
0 ∼µ0 = µα
0 for all i ∈VN, α ∈I, where
the second equality follows from Fubini’s theorem."
N,0.38839285714285715,Published as a conference paper at ICLR 2022
N,0.38950892857142855,"Induction step.
Assume that the induction assumption holds at t. Then by deﬁnition of µN
t , for
all bounded function f : X × I →R with |f| ≤Mf,"
N,0.390625,"E
µN
t+1(f) −µt+1(f)

≤E
hµN
t+1(f) −µN
t P πN
t,µN
t ,WN (f)

i"
N,0.39174107142857145,"+ E
hµN
t P πN
t,µN
t ,WN (f) −µN
t P πN
t,µN
t ,W (f)

i"
N,0.39285714285714285,"+ E
hµN
t P πN
t,µN
t ,W (f) −µN
t P π
t,µN
t ,W (f)

i"
N,0.3939732142857143,"+ E
hµN
t P π
t,µN
t ,W (f) −µN
t P π
t,µt,W (f)

i"
N,0.3950892857142857,"+ E
µN
t P π
t,µt,W (f) −µt+1(f)

."
N,0.39620535714285715,"First term.
We have by deﬁnition of µN
t"
N,0.39732142857142855,"E
hµN
t+1(f) −µN
t P πN
t,µN
t ,WN (f)

i = E "" Z I X"
N,0.3984375,"x∈X
µN,α
t+1 (x) f(x, α) dα −
Z I X"
N,0.39955357142857145,"x∈X
µN,α
t
(x)
X"
N,0.40066964285714285,"u∈U
πN,α
t
(u | x)
X"
N,0.4017857142857143,"x′∈X
P

x′ | x, u,
Z"
N,0.4029017857142857,"I
WN(α, β)µN,β
t
dβ

f(x′, α) dα  # = E"
N,0.40401785714285715,"""
1
N X i∈VN Z ( i−1 N , i"
N,0.40513392857142855,"N ]
f(Xi
t+1, α) dα −E ""Z ( i−1 N , i"
N,0.40625,"N ]
f(Xi
t+1, α) dα Xt #! # ≤  E  "
N,0.40736607142857145,"1
N X i∈VN Z ( i−1 N , i"
N,0.40848214285714285,"N ]
f(Xi
t+1, α) dα −E ""Z ( i−1 N , i"
N,0.4095982142857143,"N ]
f(Xi
t+1, α) dα Xt #!!2    1
2 =  1 N 2
X"
N,0.4107142857142857,"i∈VN
E   Z ( i−1 N , i"
N,0.41183035714285715,"N ]
f(Xi
t+1, α) dα −E ""Z ( i−1 N , i"
N,0.41294642857142855,"N ]
f(Xi
t+1, α) dα Xt #!2    1
2"
N,0.4140625,"≤2Mf
√ N"
N,0.41517857142857145,"where the last equality follows from conditional independence of {Xi
t+1}i∈VN given Xt
≡
{Xi
t}i∈VN and the law of total expectation."
N,0.41629464285714285,"Second term.
We have"
N,0.4174107142857143,"E
hµN
t P πN
t,µN
t ,WN (f) −µN
t P πN
t,µN
t ,W (f)

i = E "" Z I X"
N,0.4185267857142857,"x∈X
µN,α
t
(x)
X"
N,0.41964285714285715,"u∈U
πN,α
t
(u | x)
X"
N,0.42075892857142855,"x′∈X
P

x′ | x, u,
Z"
N,0.421875,"I
WN(α, β)µN,β
t
dβ

f(x′, α) dα −
Z I X"
N,0.42299107142857145,"x∈X
µN,α
t
(x)
X"
N,0.42410714285714285,"u∈U
πN,α
t
(u | x)
X"
N,0.4252232142857143,"x′∈X
P

x′ | x, u,
Z"
N,0.4263392857142857,"I
W(α, β)µN,β
t
dβ

f(x′, α) dα  #"
N,0.42745535714285715,"≤|X|MfLP E
Z I  Z"
N,0.42857142857142855,"I
WN(α, β)µN,β
t
dβ −
Z"
N,0.4296875,"I
W(α, β)µN,β
t
dβ
 dα
"
N,0.43080357142857145,"≤|X|2MfLP sup
x∈X
E
Z I  Z"
N,0.43191964285714285,"I
WN(α, β)µN,β
t
(x) −W(α, β)µN,β
t
(x) dβ
 dα

→0"
N,0.4330357142857143,"by Assumption 1 and µN,β
t
(x) trivially being bounded by 1. If the convergence in Assumption 1 is
at rate O(1/
√"
N,0.4341517857142857,"N), then this convergence is also at rate O(1/
√ N)."
N,0.43526785714285715,Published as a conference paper at ICLR 2022
N,0.43638392857142855,"Third term.
We have"
N,0.4375,"E
hµN
t P πN
t,µN
t ,W (f) −µN
t P π
t,µN
t ,W (f)

i = E "" Z I X"
N,0.43861607142857145,"x∈X
µN,α
t
(x)
X"
N,0.43973214285714285,"u∈U
πN,α
t
(u | x)
X"
N,0.4408482142857143,"x′∈X
P

x′ | x, u,
Z"
N,0.4419642857142857,"I
W(α, β)µN,β
t
dβ

f(x′, α) dα −
Z I X"
N,0.44308035714285715,"x∈X
µN,α
t
(x)
X"
N,0.44419642857142855,"u∈U
πα
t (u | x)
X"
N,0.4453125,"x′∈X
P

x′ | x, u,
Z"
N,0.44642857142857145,"I
W(α, β)µN,β
t
dβ

f(x′, α) dα  #"
N,0.44754464285714285,"≤|X||U|Mf E
Z I"
N,0.4486607142857143,"πN,α
t
(u | x) −πα
t (u | x)
 dα
"
N,0.4497767857142857,"= |X||U|Mf E  
X"
N,0.45089285714285715,"j∈VN\{i} Z ( j−1 N , j N ] π ⌈Nα⌉"
N,0.45200892857142855,"N
t
(u | x) −πα
t (u | x)
 dα  "
N,0.453125,"+ |X||U|Mf E ""Z ( i−1 N , i"
N,0.45424107142857145,"N ]
|ˆπt(u | x) −πα
t (u | x)| dα #"
N,0.45535714285714285,≤|X||U|Mf · Lπ
N,0.4564732142857143,N + |X||U|Mf · 2|Dπ|
N,0.4575892857142857,"N
+ |X||U|Mf · 2"
N,0.45870535714285715,"N
by assumption of Lipschitz continuous π up to a ﬁnite number of discontinuities Dπ as well as the
deviating agent i’s error term, for which the integrands are bounded by 2."
N,0.45982142857142855,"Fourth term.
We have"
N,0.4609375,"E
hµN
t P π
t,µN
t ,W (f) −µN
t P π
t,µt,W (f)

i = E "" Z I X"
N,0.46205357142857145,"x∈X
µN,α
t
(x)
X"
N,0.46316964285714285,"u∈U
πα
t (u | x)
X"
N,0.4642857142857143,"x′∈X
P

x′ | x, u,
Z"
N,0.4654017857142857,"I
W(α, β)µN,β
t
dβ

f(x′, α) dα −
Z I X"
N,0.46651785714285715,"x∈X
µN,α
t
(x)
X"
N,0.46763392857142855,"u∈U
πα
t (u | x)
X"
N,0.46875,"x′∈X
P

x′ | x, u,
Z"
N,0.46986607142857145,"I
W(α, β)µβ
t dβ

f(x′, α) dα  #"
N,0.47098214285714285,"≤Mf|X| E

sup
x,u Z I"
N,0.4720982142857143,"P

x′ | x, u,
Z"
N,0.4732142857142857,"I
W(α, β)µN,β
t
dβ
"
N,0.47433035714285715,"−P

x′ | x, u,
Z"
N,0.47544642857142855,"I
W(α, β)µβ
t dβ
 dα
"
N,0.4765625,"≤Mf|X|LP
X"
N,0.47767857142857145,"x′∈X
E
Z I  Z"
N,0.47879464285714285,"I
W(α, β)µN,β
t
(x′) dβ −
Z"
N,0.4799107142857143,"I
W(α, β)µβ
t (x′) dβ
 dα
"
N,0.4810267857142857,"≤Mf|X|2LP · C′(1)
√ N"
N,0.48214285714285715,"in the case of rate O(1/
√"
N,0.48325892857142855,"N), or uniformly to zero otherwise, from Lipschitz P by deﬁning the
functions f ′
x′,α(x, β) = W(α, β)·1x=x′ for any (x′, α) ∈X ×I and using the induction assumption
on f ′
x′,α to obtain E
Z I  Z"
N,0.484375,"I
W(α, β)µN,β
t
(x′) dβ −
Z"
N,0.48549107142857145,"I
W(α, β)µβ
t (x′) dβ
 dα
 =
Z I
E
 Z"
N,0.48660714285714285,"I
W(α, β)µN,β
t
(x′) dβ −
Z"
N,0.4877232142857143,"I
W(α, β)µβ
t (x′) dβ 
dα =
Z"
N,0.4888392857142857,"I
E
µN
t (f ′
x′,α) −µt(f ′
x′,α)

dα ≤C′(1)
√ N"
N,0.48995535714285715,"for some C′(1) > 0 uniformly over all f ′ bounded by 1 if the convergence in Assumption 1 is at
rate O(1/
√"
N,0.49107142857142855,"N), or uniformly to zero otherwise."
N,0.4921875,Published as a conference paper at ICLR 2022
N,0.49330357142857145,"Fifth term.
We have"
N,0.49441964285714285,"E
µN
t P π
t,µt,W (f) −µtP π
t,µt,W (f)
 = E "" Z I X"
N,0.4955357142857143,"x∈X
µN,α
t
(x)
X"
N,0.4966517857142857,"u∈U
πα
t (u | x)
X"
N,0.49776785714285715,"x′∈X
P

x′ | x, u,
Z"
N,0.49888392857142855,"I
W(α, β)µβ
t dβ

f(x′, α) dα −
Z I X"
N,0.5,"x∈X
µα
t (x)
X"
N,0.5011160714285714,"u∈U
πα
t (u | x)
X"
N,0.5022321428571429,"x′∈X
P

x′ | x, u,
Z"
N,0.5033482142857143,"I
W(α, β)µβ
t dβ

f(x′, α) dα  # = E "" Z I X"
N,0.5044642857142857,"x∈X
µN,α
t
(x)f ′(x, α) dα −
Z I X"
N,0.5055803571428571,"x∈X
µα
t (x)f ′(x, α) dα  #"
N,0.5066964285714286,"= E
µN
t (f ′) −µt(f ′)

≤C′(Mf)
√ N
."
N,0.5078125,"in the case of rate O(1/
√"
N,0.5089285714285714,"N), or uniformly to zero otherwise, again by induction assumption applied
to the function"
N,0.5100446428571429,"f ′(x, α) =
X"
N,0.5111607142857143,"u∈U
πα
t (u | x)
X"
N,0.5122767857142857,"x′∈X
P

x′ | x, u,
Z"
N,0.5133928571428571,"I
W(α, β)µβ
t dβ

f(x′, α)"
N,0.5145089285714286,bounded by Mf. This completes the proof by induction.
N,0.515625,"A.2.3
PROOF OF LEMMA A.1"
N,0.5167410714285714,"Proof. First, we will show that (24) implies (25)."
N,0.5178571428571429,"Proof of (24) =⇒(25).
We consider a uniformly Lipschitz, uniformly bounded family of mea-
surable functions H from X × B1(X) to R. Let Mh be the uniform bound of functions in H and Lh
be the uniform Lipschitz constant. Then, for arbitrary h ∈H we have
E
h
h(Xi
t, G"
N,0.5189732142857143,"i
N
N (µN
t ))
i
−E
h
h( ˆX"
N,0.5200892857142857,"i
N
t , G
i
N (µt))
i"
N,0.5212053571428571,"=
E
h
h(Xi
t, G"
N,0.5223214285714286,"i
N
N (µN
t ))
i
−E
h
h(Xi
t, G"
N,0.5234375,"i
N
N (µt))
i"
N,0.5245535714285714,"+
E
h
h(Xi
t, G"
N,0.5256696428571429,"i
N
N (µt))
i
−E
h
h(Xi
t, G
i
N (µt))
i"
N,0.5267857142857143,"+
E
h
h(Xi
t, G
i
N (µt))
i
−E
h
h( ˆX"
N,0.5279017857142857,"i
N
t , G
i
N (µt))
i"
N,0.5290178571428571,which we will analyze in the following.
N,0.5301339285714286,"First term.
We have
E
h
h(Xi
t, G"
N,0.53125,"i
N
N (µN
t ))
i
−E
h
h(Xi
t, G"
N,0.5323660714285714,"i
N
N (µt))
i"
N,0.5334821428571429,"≤E
h
E
hh(Xi
t, G"
N,0.5345982142857143,"i
N
N (µN
t )) −h(Xi
t, G"
N,0.5357142857142857,"i
N
N (µt))

 Xi
t
ii"
N,0.5368303571428571,"≤Lh E
hG"
N,0.5379464285714286,"i
N
N (µN
t ) −G"
N,0.5390625,"i
N
N (µt)

i"
N,0.5401785714285714,"= Lh
X"
N,0.5412946428571429,"x∈X
E
 Z"
N,0.5424107142857143,"I
WN( i"
N,0.5435267857142857,"N , β)µN,β
t
(x) dβ −
Z"
N,0.5446428571428571,"I
WN( i"
N,0.5457589285714286,"N , β)µβ
t (x) dβ"
N,0.546875,"
≤C(1)
√ N"
N,0.5479910714285714,"by Theorem 2 applied to the functions f ′
N,i,x(x′, β) = WN( i"
N,0.5491071428571429,"N , β) · 1x=x′ uniformly bounded by 1."
N,0.5502232142857143,"Second term.
Similarly, we have
E
h
h(Xi
t, G"
N,0.5513392857142857,"i
N
N (µt))
i
−E
h
h(Xi
t, G
i
N (µt))
i ≤Lh∥G"
N,0.5524553571428571,"i
N
N (µt) −G
i
N (µt)∥1"
N,0.5535714285714286,"Published as a conference paper at ICLR 2022 ≤Lh
X x∈X  Z I"
N,0.5546875,"
WN( i"
N,0.5558035714285714,"N , β) −W( i"
N,0.5569196428571429,"N , β)

µβ
t (x) dβ ≤Lh
X x∈X  Z I  WN( i"
N,0.5580357142857143,"N , β) −N
Z ( i−1 N , i"
N,0.5591517857142857,"N ]
W(α, β) dα !"
N,0.5602678571428571,"µβ
t (x) dβ "
N,0.5613839285714286,"+ Lh
X x∈X  Z I  N
Z ( i−1 N , i"
N,0.5625,"N ]
W(α, β) dα −W( i"
N,0.5636160714285714,"N , β) !"
N,0.5647321428571429,"µβ
t (x) dβ "
N,0.5658482142857143,"where the latter term can be bounded as Lh
X x∈X  Z I  N
Z ( i−1 N , i"
N,0.5669642857142857,"N ]
W(α, β) dα −W( i"
N,0.5680803571428571,"N , β) !"
N,0.5691964285714286,"µβ
t (x) dβ  ≤Lh
X x∈X  Z I
N
Z ( i−1 N , i N ]"
N,0.5703125,"
W(α, β) −W(⌈Nα⌉"
N,0.5714285714285714,"N
, β)

µβ
t (x) dα dβ "
N,0.5725446428571429,≤Lh|X|N · 1
N,0.5736607142857143,N · LW
N,0.5747767857142857,"N
= LW Lh|X|"
N,0.5758928571428571,"N
by Assumption 2."
N,0.5770089285714286,"Alternatively, if we assumed the weaker block-wise Lipschitz condition on W in (20), we can obtain
the same result for almost all i ∈VN, i.e. for any p0 > 0 there exists N ′ ∈N such that for any
N > N ′, there exists a set W0
N, |W0
N| ≥⌊(1 −p0)N⌋such that for all i ∈W0
N the above is true:
Since by (20) there exist only a ﬁnite number Q of intervals and therefore jumps, there can be only
Q many i for which the above fails, while for all other i we again have Z I
N
Z ( i−1 N , i N ]"
N,0.578125,"
W(α, β) −W(⌈Nα⌉"
N,0.5792410714285714,"N
, β)

µβ
t (x) dα dβ  ≤
X"
N,0.5803571428571429,"j∈{1,...,Q}  Z"
N,0.5814732142857143,"Ij
N
Z ( i−1 N , i N ]"
N,0.5825892857142857,"
W(α, β) −W(⌈Nα⌉"
N,0.5837053571428571,"N
, β)

µβ
t (x) dα dβ "
N,0.5848214285714286,≤N · 1
N,0.5859375,N · LW
N,0.5870535714285714,"N
= LW Lh|X| N"
N,0.5881696428571429,"by (20), as ( i−1 N , i"
N,0.5892857142857143,"N ] × Ij ⊆Ik × Ij for some k ∈{1, . . . , Q}."
N,0.5904017857142857,"For the former term we observe that Lh
X x∈X  Z I  WN( i"
N,0.5915178571428571,"N , β) −N
Z ( i−1 N , i"
N,0.5926339285714286,"N ]
W(α, β) dα !"
N,0.59375,"µβ
t (x) dβ  ≤Lh
X"
N,0.5948660714285714,"x∈X
N
Z ( i−1 N , i N ]  Z"
N,0.5959821428571429,"I
(WN(α, β) −W(α, β)) µβ
t (x) dβ
 dα"
N,0.5970982142857143,"and by deﬁning for any x ∈X the terms IN
i (x) via"
N,0.5982142857142857,"IN
i (x) := N
Z ( i−1 N , i N ]  Z"
N,0.5993303571428571,"I
(WN(α, β) −W(α, β)) µβ
t (x) dβ
 dα"
N,0.6004464285714286,and noticing that we have
N,0.6015625,"1
N N
X"
N,0.6026785714285714,"i=1
IN
i (x) =
Z I  Z"
N,0.6037946428571429,"I
(WN(α, β) −W(α, β)) µβ
t (x) dβ
 dα →0"
N,0.6049107142857143,"by Assumption 1, we can conclude that for any ε1, p1 > 0 there exists N ′ ∈N such that for any
N > N ′, there exists a set W1
N, |W1
N| ≥⌊(1 −p1)N⌋such that for all i ∈W1
N we have"
N,0.6060267857142857,"IN
i (x) < ε1,"
N,0.6071428571428571,Published as a conference paper at ICLR 2022
N,0.6082589285714286,"since by the above we can choose N ′ ∈N such that for any N > N ′ we have
1
N
PN
i=1 IN
i (x) <
ε1p1, and from IN
i (x) ≥0 it would otherwise follow that 1"
N,0.609375,"N
PN
i=1 IN
i (x) ≥
1
N · ⌈p1N⌉ε1 ≥ε1p1
which would be a direct contradiction. Therefore, for all i ∈W1
N, we have uniformly Lh
X"
N,0.6104910714285714,"x∈X
N
Z ( i−1 N , i N ]  Z"
N,0.6116071428571429,"I
(WN(α, β) −W(α, β)) µβ
t (x) dβ
 dα = Lh
X"
N,0.6127232142857143,"x∈X
IN
i (x) →0 ."
N,0.6138392857142857,"Third term.
By (24), for any ε2, p2 > 0 there exists a set W2
N, |W2
N| ≥⌊(1 −p2)N⌋such that
for all i ∈W2
N we have
E
h
h(Xi
t, G
i
N (µt))
i
−E
h
h( ˆX"
N,0.6149553571428571,"i
N
t , G
i
N (µt))
i < ε2"
N,0.6160714285714286,independent of ˆπ ∈Π.
N,0.6171875,"The intersection of W0
N, W1
N, W2
N has at least N −⌈p0N⌉−⌈p1N⌉−⌈p2N⌉agents fulﬁlling (25),
which completes the proof of (24) =⇒(25) for almost all agents by choosing ε1, ε2 sufﬁciently
small and p0, p1, p2 < p"
N,0.6183035714285714,"3 such that N −⌈p0N⌉−⌈p1N⌉−⌈p2N⌉≥⌊(1 −p)N⌋, which is equivalent
to 1 −⌈p0N⌉"
N,0.6194196428571429,"N
−⌈p1N⌉"
N,0.6205357142857143,"N
−⌈p2N⌉"
N,0.6216517857142857,"N
≥⌊(1−p)N⌋"
N,0.6227678571428571,"N
and is true for sufﬁciently large N, since in the limit,
1 −⌈p0N⌉"
N,0.6238839285714286,"N
−⌈p1N⌉"
N,0.625,"N
−⌈p2N⌉"
N,0.6261160714285714,"N
→1 −p0 −p1 −p2 and ⌊(1−p)N⌋"
N,0.6272321428571429,"N
→1 −p as N →∞."
N,0.6283482142857143,"Proof of (24).
All that remains is to show (24), which will automatically imply (25) at all times
t ∈T by the prequel. We will show (24) by induction."
N,0.6294642857142857,"Initial case.
At t = 0, L(Xi
t) = µ0 = L( ˆX"
N,0.6305803571428571,"i
N
t ) by deﬁnition. Thus, trivially
E

g(Xi
0)

−E
h
g( ˆX"
N,0.6316964285714286,"i
N
0 )
i = 0 < ε ."
N,0.6328125,"Induction step.
For any uniformly bounded family of functions G from X to R with bound Mg,
we will show that for any ε, p > 0, there exists N ′ ∈N such that for all N > N ′ we have
E

g(Xi
t+1)

−E
h
g( ˆX"
N,0.6339285714285714,"i
N
t+1)
i < ε"
N,0.6350446428571429,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋. Observe that
E

g(Xi
t+1)

−E
h
g( ˆX"
N,0.6361607142857143,"i
N
t+1)
i =
E
h
lN,t(Xi
t, G"
N,0.6372767857142857,"i
N
N (µN
t ))
i
−E
h
lN,t( ˆX"
N,0.6383928571428571,"i
N
t , G
i
N (µt))
i"
N,0.6395089285714286,"where we deﬁned the uniformly bounded, uniformly Lipschitz functions"
N,0.640625,"lN,t(x, ν) ≡
X"
N,0.6417410714285714,"u∈U
ˆπt(u | x)
X"
N,0.6428571428571429,"x′∈X
P(x′ | x, u, ν)g(x′)"
N,0.6439732142857143,"with Lipschitz constant |X|MgLP and uniform bound Mg. By the induction assumption and (24)
=⇒(25) from the prequel, there exists N ′ ∈N such that for all N > N ′ we have
E
h
lN,t(Xi
t, G"
N,0.6450892857142857,"i
N
N (µN
t ))
i
−E
h
lN,t( ˆX"
N,0.6462053571428571,"i
N
t , G
i
N (µt))
i < ε"
N,0.6473214285714286,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋, which completes
the proof by induction."
N,0.6484375,"A.2.4
PROOF OF COROLLARY A.1"
N,0.6495535714285714,"Proof. Deﬁne the uniformly bounded, uniformly Lipschitz functions"
N,0.6506696428571429,"rˆπ(x, ν) ≡
X"
N,0.6517857142857143,"u∈U
r(x, u, ν)ˆπt(u | s)"
N,0.6529017857142857,"with Lipschitz constant |U|Lr and uniform bound Mr such that by Lemma A.1 and Fubini’s theo-
rem, there exists N ′ ∈N such that for all N > N ′ we have
JN
i (π1, . . . , πi−1, ˆπ, πi+1, . . . , ˆπ) −Jµ
i
N (ˆπ)"
N,0.6540178571428571,Published as a conference paper at ICLR 2022 ≤
N,0.6551339285714286,"T −1
X t=0"
N,0.65625,"E
h
rˆπt(Xi
t, G
i
N (µt))
i
−E
h
rˆπt( ˆX"
N,0.6573660714285714,"i
N
t , G
i
N (µt))
i < ε ."
N,0.6584821428571429,"uniformly over ˆπ ∈Π, i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋by choosing the
maximum over all N ′ at each ﬁnite time step from Lemma A.1."
N,0.6595982142857143,"In case of the inﬁnite horizon discounted objective, we instead ﬁrst cut off at a time T >
log ε(1−γ)"
MR,0.6607142857142857,"4Mr
log γ
such that trivially"
MR,0.6618303571428571,"T −1
X"
MR,0.6629464285714286,"t=0
γt E
h
rˆπt(Xi
t, G
i
N (µt))
i
−E
h
rˆπt( ˆX"
MR,0.6640625,"i
N
t , G
i
N (µt))
i"
MR,0.6651785714285714,"+ γT
∞
X"
MR,0.6662946428571429,"t=T
γt−T E
h
rˆπt(Xi
t, G
i
N (µt))
i
−E
h
rˆπt( ˆX"
MR,0.6674107142857143,"i
N
t , G
i
N (µt))
i <"
MR,0.6685267857142857,"T −1
X"
MR,0.6696428571428571,"t=0
γt E
h
rˆπt(Xi
t, G
i
N (µt))
i
−E
h
rˆπt( ˆX"
MR,0.6707589285714286,"i
N
t , G
i
N (µt))
i + ε 2"
MR,0.671875,and then handle the remaining term analogously to the ﬁnite horizon case.
MR,0.6729910714285714,"A.2.5
PROOF OF THEOREM 3"
MR,0.6741071428571429,"Proof. By Corollary A.1, for any ε > 0 there exists N ′ ∈N such that for all N > N ′ we have"
MR,0.6752232142857143,"max
π∈Π
 
JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −JN
i (π1, . . . , πN)
"
MR,0.6763392857142857,"≤max
π∈Π"
MR,0.6774553571428571,"
JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −Jµ
i
N (π)
"
MR,0.6785714285714286,"+ max
π∈Π"
MR,0.6796875,"
Jµ
i
N (π) −Jµ
i
N (π
i
N )
"
MR,0.6808035714285714,"+

Jµ
i
N (π
i
N ) −JN
i (π1, . . . , πN)
 < ε"
MR,0.6819196428571429,2 + 0 + ε 2 = ε
MR,0.6830357142857143,"uniformly over i ∈WN for some WN
⊆VN with |WN| ≥⌊(1 −p)N⌋, since π
i
N
∈
arg maxπ Jµ
i
N (π) by deﬁnition of a GMFE. Reordering completes the proof."
MR,0.6841517857142857,"A.2.6
PROOF OF COROLLARY A.2"
MR,0.6852678571428571,"Proof. The proof follows immediately from Theorem 2 and Lemma A.1 by considering the trivial
policy π that always chooses the only action available together with its generated mean ﬁeld µ =
Ψ(π)."
MR,0.6863839285714286,"A.2.7
PROOF OF PROPOSITION 2"
MR,0.6875,"Proof. The set Π is a complete metric space, since existence of limits follows from completeness
of R, pointwise limits of measurable functions are measurable, and policies will remain normalized.
Banach’s ﬁxed point theorem applied to ˆΦ ◦ˆΨ gives us the desired result."
MR,0.6886160714285714,"A.2.8
PROOF OF THEOREM 4"
MR,0.6897321428571429,"Proof. Formally, we approximate mean ﬁelds by ˆΨ(π) = PM
i=1 1α∈˜Ii ˆµαi for any ﬁxed policy
ensemble π, and similarly policies ˆΦ(µ) = PM
i=1 1α∈˜Iiπαi where παi is the softmax policy of αi
for ﬁxed µ, i.e."
MR,0.6908482142857143,"πα
t (u | x) =
exp

Qµ
α(t,x,u) η
 P"
MR,0.6919642857142857,"u∈U exp

Qµ
α(t,x,u)"
MR,0.6930803571428571,"η
 .
(30)"
MR,0.6941964285714286,Published as a conference paper at ICLR 2022
MR,0.6953125,"By the Bellman equation (13), Qµ
α(t, x, u) is Lipschitz in µ for all (t, x, u) ∈T × X × U un-
der Assumption 2. Since the Lipschitz constants are shared over all α, by Cui & Koeppl (2021),
Lemma B.7.5, (30) is therefore Lipschitz with Lipschitz constant proportional to 1/η, which imme-
diately implies that ˆΦ is also Lipschitz with Lipschitz constant c1/η. By its recursive deﬁnition as
compositions of Lipschitz functions, ˆΨ is Lipschitz as well with some constant c2. Therefore, the
composition of both functions ˆΨ ◦ˆΦ is Lipschitz with constants c1c2/η, which will be less than 1
for sufﬁciently large η. By Proposition 2, the equivalence classes algorithm ˆΨ ◦ˆΦ converges to a
ﬁxed point."
MR,0.6964285714285714,"A.2.9
PROOF OF THEOREM 5"
MR,0.6975446428571429,"Proof. First, note that under the equivalence classes method, the distance between any α and its
representant αi uniformly shrinks to zero as M →∞, i.e. maxi=1,...,M supα∈˜Ii |α −αi| →0."
MR,0.6986607142857143,"We begin by showing that a solution of the M equivalence classes method (π, µ) ∈Π × M,
π ∈ˆΦ(µ), µ = ˆΨ(π) following (31), (32) fulﬁlls approximate optimality, i.e. for any ε > 0 there
exists M ′ s.t. for all M > M ′"
MR,0.6997767857142857,"sup
α∈I
max
π∈Π
 
J ¯µ
α (π) −J ¯µ
α (πα)

< ε,"
MR,0.7008928571428571,"where we introduced the true, exact mean ﬁeld ensemble ¯µ = Ψ(π) following (12) generated by
the block-wise solution policy PM
i=1 1α∈˜Iiπαi of the M equivalence classes method, as well as the
true mean ﬁeld system under ¯µ and any policy π ∈Π
¯Xα
0 ∼µ0,
¯U α
t ∼πt(· | ¯Xα
t ),
¯Xα
t+1 ∼P(· | ¯Xα
t , ¯U α
t , ¯Gα
t ),
∀(α, t) ∈I × T"
MR,0.7020089285714286,"with B1(X)-valued ¯Gα
t :=
R"
MR,0.703125,"I W(α, β)¯µβ
t dβ and J ¯µ
α (π) ≡E
hPT −1
t=0 r( ¯Xα
t , ¯U α
t , ¯Gα
t )
i
, while sys-
tem (9) is to be understood as the system under the approximate mean ﬁeld ensemble µ."
MR,0.7042410714285714,"To see this, we will analyze"
MR,0.7053571428571429,"sup
α∈I
max
π∈Π
 
J ¯µ
α (π) −J ¯µ
α (πα)

≤
max
i=1,...,M sup
α∈˜Ii
max
π∈Π
 
J ¯µ
α (π) −Jµ
α (π)
"
MR,0.7064732142857143,"+
max
i=1,...,M sup
α∈˜Ii
max
π∈Π
 
Jµ
α (π) −Jµ
αi(π)
"
MR,0.7075892857142857,"+
max
i=1,...,M sup
α∈˜Ii
max
π∈Π
 
Jµ
αi(π) −Jµ
αi(παi)
"
MR,0.7087053571428571,"+
max
i=1,...,M sup
α∈˜Ii"
MR,0.7098214285714286," 
Jµ
αi(παi) −Jµ
α (παi)
"
MR,0.7109375,"+
max
i=1,...,M sup
α∈˜Ii"
MR,0.7120535714285714," 
Jµ
α (πα) −J ¯µ
α (πα)

."
MR,0.7131696428571429,"First term.
For any π ∈Π, deﬁne the uniformly bounded, uniformly Lipschitz functions"
MR,0.7142857142857143,"rπ(x, ν) ≡
X"
MR,0.7154017857142857,"u∈U
r(x, u, ν)πt(u | s)"
MR,0.7165178571428571,"with Lipschitz constant |U|Lr and uniform bound Mr such that for the ﬁrst term, we have
 
J ¯µ
α (π) −Jµ
α (π)

≤
J ¯µ
α (π) −Jµ
α (π) ≤"
MR,0.7176339285714286,"T −1
X t=0"
MR,0.71875,"E

rπ( ¯Xα
t , ¯Gα
t )

−E [rπ(Xα
t , Gα
t ] ≤"
MR,0.7198660714285714,"T −1
X t=0"
MR,0.7209821428571429,"E

rπ( ¯Xα
t , ¯Gα
t ) −rπ( ¯Xα
t , Gα
t
 +"
MR,0.7220982142857143,"T −1
X t=0"
MR,0.7232142857142857,"E

rπ( ¯Xα
t , Gα
t

−E [rπ(Xα
t , Gα
t ] ≤"
MR,0.7243303571428571,"T −1
X"
MR,0.7254464285714286,"t=0
|U|Lr  Z"
MR,0.7265625,"I
W(α, β)(¯µβ
t −µβ
t ) dβ)
 +"
MR,0.7276785714285714,"T −1
X t=0"
MR,0.7287946428571429,"E

rπ( ¯Xα
t , Gα
t

−E [rπ(Xα
t , Gα
t ]
 ."
MR,0.7299107142857143,Published as a conference paper at ICLR 2022
MR,0.7310267857142857,"For the former term, note that Z"
MR,0.7321428571428571,"I
W(α, β)(¯µβ
t −µβ
t ) dβ)
 =  X i Z"
MR,0.7332589285714286,"˜Ii
W(α, β)(¯µβ
t −µαi
t ) dβ) "
MR,0.734375,"≤
max
i=1,...,M sup
α∈˜Ii
|X| ∥¯µα
t −µαi
t ∥"
MR,0.7354910714285714,"and we will show by induction over t = 0, 1, . . . , T that supα∈˜Ii ∥¯µα
t −µαi
t ∥→0 over all α
uniformly over all equivalence classes ˜Ii. At t = 0, we have trivially ¯µ0 = µ0. Assume that
supα∈˜Ii ∥¯µα
t −µαi
t ∥→0. Then for t + 1, we have"
MR,0.7366071428571429,"sup
α∈˜Ii"
MR,0.7377232142857143,"¯µα
t+1 −µαi
t+1"
MR,0.7388392857142857,"= sup
α∈˜Ii  X"
MR,0.7399553571428571,"x∈X
¯µα
t (x)
X"
MR,0.7410714285714286,"u∈U
πα
t (u | x)P(· | x, u, ¯Gα
t ) −
X"
MR,0.7421875,"x∈X
µαi
t (x)
X"
MR,0.7433035714285714,"u∈U
παi
t (u | x)P(· | x, u, Gαi
t ) "
MR,0.7444196428571429,"≤sup
α∈˜Ii  X"
MR,0.7455357142857143,"x∈X
¯µα
t (x)
X"
MR,0.7466517857142857,"u∈U
πα
t (u | x)P(· | x, u, ¯Gα
t ) −
X"
MR,0.7477678571428571,"x∈X
¯µαi
t (x)
X"
MR,0.7488839285714286,"u∈U
παi
t (u | x)P(· | x, u, ¯Gαi
t )  +  X"
MR,0.75,"x∈X
¯µαi
t (x)
X"
MR,0.7511160714285714,"u∈U
παi
t (u | x)P(· | x, u, ¯Gαi
t ) −
X"
MR,0.7522321428571429,"x∈X
µαi
t (x)
X"
MR,0.7533482142857143,"u∈U
παi
t (u | x)P(· | x, u, ¯Gαi
t )  +  X"
MR,0.7544642857142857,"x∈X
¯µαi
t (x)
X"
MR,0.7555803571428571,"u∈U
παi
t (u | x)P(· | x, u, ¯Gαi
t ) −
X"
MR,0.7566964285714286,"x∈X
µαi
t (x)
X"
MR,0.7578125,"u∈U
παi
t (u | x)P(· | x, u, Gαi
t ) "
MR,0.7589285714285714,"≤sup
α∈˜Ii  X"
MR,0.7600446428571429,"x∈X
¯µα
t (x)
X"
MR,0.7611607142857143,"u∈U
πα
t (u | x)P(· | x, u, ¯Gα
t ) −
X"
MR,0.7622767857142857,"x∈X
¯µαi
t (x)
X"
MR,0.7633928571428571,"u∈U
παi
t (u | x)P(· | x, u, ¯Gαi
t ) "
MR,0.7645089285714286,"+ |X|2 ∥¯µαi
t
−µαi
t ∥+ |X|2|U|LP ∥¯µαi
t
−µαi
t ∥→0"
MR,0.765625,"as M →∞, since the ﬁrst term is uniformly Lipschitz in α by (12) as a recursive composition, ﬁnite
multiplication and addition of Lipschitz functions, whereas the other terms tend to zero by induction
hypothesis. Since the Lipschitz constants do not depend on ˜Ii, the convergence is uniform."
MR,0.7667410714285714,"To bound the latter term, we ﬁrst note that rπ(·, Gα
t ) is always bounded by Mr regardless of t, α, π,
i.e. it again sufﬁces to show that for any family of functions G from X to R uniformly bounded by
Mr, we have"
MR,0.7678571428571429,"sup
g∈G"
MR,0.7689732142857143,"E

g( ¯Xα
t )

−E [g(Xα
t )]
 →0 ."
MR,0.7700892857142857,"The proof is by induction. At t = 0, we trivially have L( ¯Xα
t ) = µ0 = L(Xα
t ). Assuming that the
induction hypothesis holds at t, then at t + 1 we have"
MR,0.7712053571428571,"sup
g∈G"
MR,0.7723214285714286,"E

g( ¯Xα
t+1)

−E

g(Xα
t+1)
 = sup
g∈G"
MR,0.7734375,"E

lt( ¯Xα
t , Gα
t ))

−E [lt(Xα
t , Gα
t )]
 →0"
MR,0.7745535714285714,"by the induction hypothesis, where we deﬁned the uniformly bounded functions"
MR,0.7756696428571429,"lt(x, ν) ≡
X"
MR,0.7767857142857143,"u∈U
πt(u | x)
X"
MR,0.7779017857142857,"x′∈X
P(x′ | x, u, ν)g(x′)"
MR,0.7790178571428571,"with uniform bound Mr. Therefore, |J ¯µ
α (π) −Jµ
α (π)| →0 uniformly over all α, π."
MR,0.7801339285714286,"Second term.
For the second term, we analogously have
 
Jµ
α (π) −Jµ
αi(π)

≤
Jµ
α (π) −Jµ
αi(π) ≤"
MR,0.78125,"T −1
X"
MR,0.7823660714285714,"t=0
|U|Lr  Z"
MR,0.7834821428571429,"I
(W(α, β) −W(αi, β))µβ
t dβ)
 +"
MR,0.7845982142857143,"T −1
X"
MR,0.7857142857142857,"t=0
|E [rπ(Xα
t , Gαi
t ] −E [rπ(Xαi
t , Gαi
t ]|"
MR,0.7868303571428571,Published as a conference paper at ICLR 2022
MR,0.7879464285714286,"where the former term uniformly tends to zero as M →∞over all α by Lipschitz W from As-
sumption 2 and increasingly ﬁne partition intervals ˜Ii, while for the latter term we again show that
for any family of functions G from X to R uniformly bounded by Mr, we have"
MR,0.7890625,"sup
g∈G
|E [g(Xα
t )] −E [g(Xαi
t )]| →0 ."
MR,0.7901785714285714,"The proof is by induction. At t = 0, we trivially have L(Xα
t ) = µ0 = L(Xαi
t ). Assuming that the
induction hypothesis holds at t, then at t + 1 we have"
MR,0.7912946428571429,"sup
g∈G"
MR,0.7924107142857143,"E

g(Xα
t+1)

−E

g(Xαi
t+1)
 = sup
g∈G
|E [lt(Xα
t , Gαi
t ))] −E [lt(Xαi
t , Gαi
t )]| →0"
MR,0.7935267857142857,"by the induction hypothesis, where we deﬁned the uniformly bounded functions"
MR,0.7946428571428571,"lt(x, ν) ≡
X"
MR,0.7957589285714286,"u∈U
πt(u | x)
X"
MR,0.796875,"x′∈X
P(x′ | x, u, ν)g(x′)"
MR,0.7979910714285714,"with uniform bound Mr. Therefore,
Jµ
α (π) −Jµ
αi(π)
 →0 uniformly over all α, π."
MR,0.7991071428571429,"Third term.
By deﬁnition, we have optimality of π ∈ˆΦ(µ) under the approximate mean ﬁeld µ
at each representative αi. Therefore, the term maxπ∈Π
 
Jµ
αi(π) −Jµ
αi(παi)

is upper bounded by
0, as there is no policy π that improves over παi."
MR,0.8002232142857143,"Fourth and ﬁfth term.
The results follow from the ﬁrst and second term by inserting πα for π."
MR,0.8013392857142857,"Variations on the setting.
The inﬁnite horizon discounted case is handled as in the proof of Corol-
lary A.1, i.e. repeating the above up to some chosen time horizon T and trivially bounding all terms
with t ≥T. The block-wise Lipschitz graphon case (20) is handled by choosing the equivalence
classes ¯Ii ⊆Ij such that they are part of at most one block Ij of the graphon."
MR,0.8024553571428571,"Proof of Theorem 5.
Now ﬁx any ε, p > 0. As a result of the prequel, we have that there exists
M ′ s.t. for all M > M ′"
MR,0.8035714285714286,"sup
α∈I
max
π∈Π |Jµ
α (πα) −Jµ
α (π)| < ε 3."
MR,0.8046875,"Pick any such M > M ′. By Corollary A.1 (for the ﬁrst and third term, since π is constant with at
most M discontinuities) and the prequel (for the second term), there exists N ′ ∈N such that for all
N > N ′ we have"
MR,0.8058035714285714,"max
π∈Π
 
JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −JN
i (π1, . . . , πN)
"
MR,0.8069196428571429,"≤max
π∈Π"
MR,0.8080357142857143,"
JN
i (π1, . . . , πi−1, π, πi+1, . . . , πN) −J ¯µ
i
N (π)
"
MR,0.8091517857142857,"+ max
π∈Π"
MR,0.8102678571428571,"
J ¯µ
i
N (π) −J ¯µ
i
N (π
i
N )
"
MR,0.8113839285714286,"+

J ¯µ
i
N (π
i
N ) −JN
i (π1, . . . , πN)
 < ε 3 + ε 3 + ε 3 = ε"
MR,0.8125,"which holds uniformly over i ∈WN for some WN ⊆VN with |WN| ≥⌊(1 −p)N⌋, since π
i
N ∈
arg maxπ Jµ
i
N (π) by deﬁnition of a GMFE. Reordering completes the proof."
MR,0.8136160714285714,"A.3
EXPERIMENTAL DETAILS"
MR,0.8147321428571429,"In this section, we will give a full description of all the algorithms and hyperparameters we used
during our experiments. For reinforcement learning, we use PPO (Schulman et al., 2017)."
MR,0.8158482142857143,"For the approximate equivalence classes, we shall consider grids (αm ∈[0, 1])m=1,...,M with asso-
ciated policies (παm ∈Π)m=1,...,M and mean ﬁelds (µαm ∈P(X)T )m=1,...,M. For the grid, we"
MR,0.8169642857142857,Published as a conference paper at ICLR 2022
MR,0.8180803571428571,Algorithm 1 Fixed point iteration
MR,0.8191964285714286,"1: Initialize µ0 as the mean ﬁeld induced by the uniformly random policy q.
2: for k = 0, 1, . . . do
3:
Compute πk ∈Π either directly by PPO on (14), or by computing Qµ via Algorithm 2 and
using (38) to obtain a softmax policy.
4:
Compute µk+1 induced by πk using Algorithm 3, or for RL the neighborhood mean ﬁelds
Gα
t directly using Algorithm 4.
5: end for"
MR,0.8203125,Algorithm 2 Backwards induction
MR,0.8214285714285714,"1: Input: Grid (αm ∈[0, 1])m=1,...,M, mean ﬁeld µ ∈M.
2: for m = 1, . . . , M do
3:
Initialize terminal condition Qµ
α(T, x, u) ≡0 for all (x, u) ∈X × U.
4:
for t = T −1, . . . , 0 do
5:
for (x, u) ∈X × U do
6:
Qµ
αm(t, x, u) ←r(x, u, Gαm
t
)+P"
MR,0.8225446428571429,"x′∈X P(x′ | x, u, Gαm
t
) maxu′∈U Qµ
αm(t+1, x′, u′).
7:
end for
8:
end for
9: end for
10: Return (Qµ
αm)m=1,...,M"
MR,0.8236607142857143,"choose the points αm =
m
100 with m = 0, . . . , 100. Here, an agent α shall use the policy παm with
the closest αm."
MR,0.8247767857142857,"To be precise, for the approximate mean ﬁeld µ = ˆΨ(π) we deﬁne µα ≡ˆµαm for the αm closest to
α, i.e. formally, we thus have"
MR,0.8258928571428571,"ˆΨ(π) = M
X"
MR,0.8270089285714286,"m=1
1α∈˜Im ˆµαm
(31)"
MR,0.828125,"for any ﬁxed policy ensemble π, with ˆµ deﬁned through the recursive equation"
MR,0.8292410714285714,"ˆµαm
0
≡µ0,
ˆµαm
t+1(x′) ≡
X"
MR,0.8303571428571429,"x∈X
ˆµαm
t
(x)
X"
MR,0.8314732142857143,"u∈U
παm
t
(u | x)P(x′ | x, u, ˆGαm
t
),
m = 1, . . . , M (32)"
MR,0.8325892857142857,"where under the assumption of equivalence classes ˜Im ≡[am, bm] of size (bm −am), we obtain
neighborhood mean ﬁelds via"
MR,0.8337053571428571,"ˆGα
t = M
X"
MR,0.8348214285714286,"m=1
(bm −am)W(α, αm)ˆµαm
t
.
(33)"
MR,0.8359375,"Note that in our algorithms, we shall assume equisized partitions and use (bm−am) =
1
M . Similarly,
the policy ensemble is approximated by"
MR,0.8370535714285714,"ˆΦ(µ) = M
X"
MR,0.8381696428571429,"i=1
1α∈˜Iiπαi
(34)"
MR,0.8392857142857143,"where παi is the optimal policy of αi for any ﬁxed µ, i.e. the optimal policy and mean ﬁeld of each
α is approximated by the optimal solution and mean ﬁeld of the closest αi, which is an increasingly
good approximation for sufﬁciently ﬁne grids under the standing Lipschitz assumptions. In the case
of block-wise Lipschitz continuous graphons via (20), a similar justiﬁcation holds as long as each
equivalence class remains constrained to one of the blocks of the graphon, see Theorem 5."
MR,0.8404017857142857,"In Algorithm 1, the learning scheme is described on a high level. In our experiments, we either use
approximate equivalence classes via Algorithms 2 and 3, or reinforcement learning in the form of
PPO together with sequential Monte Carlo in Algorithm 4, though in principle one can mix arbitrary
methods."
MR,0.8415178571428571,Published as a conference paper at ICLR 2022
MR,0.8426339285714286,Algorithm 3 Forward simulation
MR,0.84375,"1: Input: Grid (αm ∈[0, 1])m=1,...,M, policy π ∈Π.
2: Initialize starting condition µαm
0
≡µ0 for all m = 1, . . . , M.
3: for t = 0, . . . , T −2 do
4:
for m = 1, . . . , M do
5:
µαm
t+1 ←P"
MR,0.8448660714285714,"x∈X µαm
t
(x) P"
MR,0.8459821428571429,"u∈U παm
t
(u | x)P(· | x, u, 1"
MR,0.8470982142857143,"M
PM
n=1 W(αm, αn)µαn
t ).
6:
end for
7: end for
8: Return (µαm)m=1,...,M"
MR,0.8482142857142857,Algorithm 4 Sequential Monte Carlo
MR,0.8493303571428571,"1: Input: Number of trajectories K = 5, number of particles L = 200, policy π ∈Π.
2: for k = 1, . . . , K do
3:
Initialize particles αm ∼Unif([0, 1]), xm,k
0
∼µ0 for all m = 1, . . . , L.
4:
for t = 1, . . . , T −1 do
5:
for m = 1, . . . , L do
6:
Sample action u ∼παm
t
(· | xm,k
t
).
7:
Sample new particle state xm,k
t+1 ∼P(· | xm,k
t
, u, 1"
MR,0.8504464285714286,"L
PL
n=1 W(αm, αn)δxn,k
t
).
8:
end for
9:
end for
10: end for
11: return neighborhood mean ﬁelds Gα
t ≈1"
MR,0.8515625,"K
PK
k=1
1
L
PL
m=1 W(α, αm)δxm,k
t
."
MR,0.8526785714285714,"We ran each trial of our experiments on a single conventional CPU core, with typical wall-clock
times reaching up to at most a few days. We estimate the required compute to approximately 6500
core hours. We did not use any GPUs or TPUs. More speciﬁcally, the training of our approximate
equivalence class approach took on average approximately 24 hours for 250 iterations in SIS and
50 iterations in Investment. As a result, Figure 5 for the selection of appropriate temperatures took
around 2500 core hours. The PPO experiments took approximately 3 days for each conﬁguration,
resulting in approximately 200 core hours for Figure 7. Finally, for the N-agent evaluations in
Figure 4, each run up to 100 agents takes up to 4 core hours. Adding on top of that around 250 core
hours for the rest of the experiments results in a total of approximately 4000 core hours."
MR,0.8537946428571429,"For PPO, we used the RLlib implementation by Liang et al. (2018) (version 1.2.0, Apache-2.0
license). To allow for time-dependent policies, we append the current time to the network inputs.
Further, discrete-valued observations are one-hot encoded. Any other parameter conﬁgurations are
given in Algorithms 1, 2, 3 and 4, as well as in Table 2."
MR,0.8549107142857143,"As for the speciﬁc conﬁgurations used in the PPO experiments, we give the hyperparameters in
Table 1 and used with a feedforward neural network policy consisting of two hidden layers with 256
nodes and tanh activations, outputting a softmax policy over all actions."
MR,0.8560267857142857,"A.3.1
PROBLEM DEFINITIONS"
MR,0.8571428571428571,"For each possible problem setting, we list the applied temperature setting in Table 2. In the follow-
ing, let G ∈P(X)."
MR,0.8582589285714286,"SIS-Graphon.
In the SIS-Graphon game as described in the main text, we have X = {S, I},
U = {U, D}, µ0(I) = 0.5, r(x, u, G) = −2 · 1{I}(x) −0.5 · 1{D}(u) and T = {0, . . . , 49}.
Similar parameters produce similar results, and we set the transition probabilities as
P(S | I, ·, ·) = 0.2,
P(I | S, U, G) = 0.8 · G(I),
P(I | S, D, ·) = 0 ."
MR,0.859375,"Investment-Graphon.
Similarly, in the Investment-Graphon game we have X = {0, 1, . . . 9},
U = {I, O}, µ0(0) = 1, r(x, u, G) =
0.3x
1+P"
MR,0.8604910714285714,"x′∈X x′G(x′) −2 · 1{I}(u) and T = {0, . . . , 49}. We set"
MR,0.8616071428571429,Published as a conference paper at ICLR 2022
MR,0.8627232142857143,"Table 1: PPO Hyperparameters
Symbol
Function
Value"
MR,0.8638392857142857,"lr
Learning rate
0.00005
γ
Discount rate
1
λ
GAE lambda
0.99
cKL
KL coefﬁcient
0.2
β
KL target
0.006
cent
Entropy coefﬁcient
0.01
ϵ
Clip parameter
0.2
B
Training batch size
4000
Bm
Mini batch size
128
ISGD
SGD iterations per training batch
30"
MR,0.8649553571428571,"Table 2: Temperature conﬁgurations
Experiment
η for approximate equivalence classes"
MR,0.8660714285714286,"SIS-Graphon, Wunif
0.101
SIS-Graphon, Wrank
0.3
SIS-Graphon, Wer
0.101
Investment-Graphon, Wunif
0
Investment-Graphon, Wrank
0
Investment-Graphon, Wer
0.05"
MR,0.8671875,"the transition probabilities for x = 0, 1, . . . , 8 as"
MR,0.8683035714285714,"P(x + 1 | x, I, ·) = 9 −x 10 ,"
MR,0.8694196428571429,"P(x | x, I, ·) = 1 + x 10 ,"
MR,0.8705357142857143,"P(x | x, O, ·) = 1,"
MR,0.8716517857142857,while for x = 9 the next state is always x = 9.
MR,0.8727678571428571,"A.3.2
EXPLOITABILITY AND TEMPERATURE CHOICE"
MR,0.8738839285714286,"In the following, we will explain our choice of temperatures in Table 2 by approximately evaluating
the average exploitability of GMFE candidates (π, µ) – as it is intractable to approximately evaluate
the maximum exploitability over all α ∈I – deﬁned by"
MR,0.875,"∆J(π, µ) =
Z"
MR,0.8761160714285714,"I
sup
π∗∈Π
Jµ
α (π∗) −Jµ
α (πα) dα .
(35)"
MR,0.8772321428571429,"More speciﬁcally, when using approximate equivalence classes, we compute the exploitability of
some policy π by computing the optimal policy π∗obtained via Algorithm 2, under the ﬁxed mean
ﬁeld µ generated by π via Algorithm 3, inserting π∗,α into (35) and then approximating by
Z"
MR,0.8783482142857143,"I
Jµ
α (π) dα ≈1 M X"
MR,0.8794642857142857,"m=1,...,M X"
MR,0.8805803571428571,"x∈X
µ0(x)
X"
MR,0.8816964285714286,"u∈U
π0(u | x)Qµ,π
αm (0, x, u) .
(36)"
MR,0.8828125,"Here, we deﬁned for any policy π ∈Π and α ∈I the policy evaluation functions Qµ,π
α
as usual via"
MR,0.8839285714285714,"Qµ,π
α
(t, x, u) = r(x, u, Gα
t ) +
X"
MR,0.8850446428571429,"x′∈X
P(x′ | x, u, Gα
t )
X"
MR,0.8861607142857143,"u′∈U
π0(u′ | x)Qµ,π
α
(t + 1, x′, u′)
(37)"
MR,0.8872767857142857,"with terminal condition Qµ,π
α
(T, x, u) ≡0, which can be computed as in Algorithm 2, see also
Puterman (2014) for a review."
MR,0.8883928571428571,"To achieve convergence of ﬁxed point iterations to approximate equilibria, for previous µn ∈M
we compute the action value function Qµn
α
via Algorithm 2 using approximate equivalence classes"
MR,0.8895089285714286,Published as a conference paper at ICLR 2022
MR,0.890625,"0.00
0.05
0.10
0.15
0.20
0.25
0.30
η 0 5 10 15 20 ∆J(π)"
MR,0.8917410714285714,"(a)
SIS-Graphon, 250 iterations"
MR,0.8928571428571429,"η-Boltzmann, unif graphon
Uniform policy, unif graphon
η-Boltzmann, rank graphon
Uniform policy, rank graphon
η-Boltzmann, ER graphon
Uniform policy, ER graphon"
MR,0.8939732142857143,"0.00
0.05
0.10
0.15
0.20
0.25
η 0.0 0.2 0.4 0.6 0.8 1.0 1.2 ∆J(π)"
MR,0.8950892857142857,"(b)
Investment-Graphon, 50 iterations"
MR,0.8962053571428571,"η-Boltzmann, unif graphon
η-Boltzmann, rank graphon
η-Boltzmann, ER graphon"
MR,0.8973214285714286,"Figure 5: Final approximate exploitability mean and its minimum / maximum (shaded region) over
the last 10 iterations for various temperatures η. We can see convergence for sufﬁciently high
temperatures and choose the lowest temperature such that we still have convergence with low ex-
ploitability. Furthermore, compared to the uniformly random policy, our approximate exploitability
is signiﬁcantly lower, indicating a good approximate GMFE. For the Investment-Graphon problem,
the approximate exploitability of the uniform policy is not shown, as it is above 30. (a): SIS-
Graphon; (b): Investment-Graphon."
MR,0.8984375,and then deﬁne the next policy πn+1 = ˆΦ(µn) for every α ∈I via the softmax function
MR,0.8995535714285714,"πn+1,αi
t
(u | x) =
exp

Qµn
αi (t,x,u) η  P"
MR,0.9006696428571429,"u∈U exp

Qµn
αi (t,x,u) η"
MR,0.9017857142857143,"
(38)"
MR,0.9029017857142857,for the closest αi with some temperature η > 0 chosen minimally for convergence.
MR,0.9040178571428571,"For choosing the temperature, we evaluate the approximate ﬁnal exploitability at various temper-
atures. The results can be seen in Figure 5, where we plot the average, minimum and maximum
exploitability over the last 10 iterations of the ﬁxed point learning scheme. The reasoning behind
choosing our temperatures as in Table 2 is that we can see no ﬂuctuations (indicating convergence of
our learning scheme) together with a low approximate exploitability at the indicated temperatures."
MR,0.9051339285714286,"A.3.3
ADDITIONAL EXPERIMENTS"
MR,0.90625,"In Figure 6, we plot investment behavior at quality x = 0 as well as expected quality for each α
of the approximate equivalence class solution, and similarly in Figure 7 for the PPO solution with
sequential Monte Carlo. Here, for each α we averaged quality over all particles within a distance of
0.05 to α. We can see that PPO achieves qualitatively and quantitatively similar behavior, deviating
slightly due to the approximate optimality of the PPO algorithm. To be precise, when evaluating
exploitability via either solution, we ﬁnd that the learned policy exploitability remains around ε ≈2,
compared to ε > 30 for the uniform random policy."
MR,0.9073660714285714,"In Figure 8 the equilibrium behavior is shown for the Investment-Graphon problem without softmax
policy regularization (except for the ER graphon case), as we ﬁnd that the problem already converges
to a very good equilibrium with low approximate exploitability, see Figure 5. In this problem, we
ﬁnd that the resulting (deterministic without regularization) policy will let agents invest up to a
certain quality, after which any further investment is avoided. The agents with higher connectivity
will invest up to a lower quality, as they are in competition with more products."
MR,0.9084821428571429,"In Figure 9 and 10, we have performed ablations over the number of equivalence classes for the SIS-
Graphon problem. As can be observed, the solution obtained by approximate equivalence classes
remains stable regardless of the particular number of equivalence classes, showing the stability of
discretization approach and supporting Theorem 5."
MR,0.9095982142857143,"Finally, in Figure 11 we exemplarily show training results of applying state-of-the-art multi-agent
reinforcement learning methods such as multi-agent PPO (MAPPO, Yu et al. (2021)) on the ﬁnite-
agent system with observed, randomized-per-episode graphon indices and W-random graphs. Here,
we use the same hyperparameters as shown in Table 1. As can be seen, due to the non-stationarity
of the other agents, a naive application of MARL techniques fails to converge at all."
MR,0.9107142857142857,Published as a conference paper at ICLR 2022
MR,0.9118303571428571,"0
20
40
t 0 1"
MR,0.9129464285714286,"πα
t (I | x = 0)"
MR,0.9140625,"(a)
Wunif 0.0 0.5 1.0
α"
MR,0.9151785714285714,"0
20
40
t 0 1"
MR,0.9162946428571429,"πα
t (I | x = 0)"
MR,0.9174107142857143,"(b)
Wrank 0.0 0.5 1.0
α"
MR,0.9185267857142857,"0
20
40
t 0 1"
MR,0.9196428571428571,"πα
t (I | x = 0)"
MR,0.9207589285714286,"(c)
Wer 0.0 0.5 1.0
α"
MR,0.921875,"0
20
40
t 0 5"
MR,0.9229910714285714,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9241071428571429,"0
20
40
t 0 5"
MR,0.9252232142857143,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9263392857142857,"0
20
40
t 0 5"
MR,0.9274553571428571,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9285714285714286,"Figure 6: The M = 100 approximate equivalence classes solution of Investment-Graphon. We
plot the probability of investing at state x = 0 (top) together with the evolution of average quality
(bottom). (a): Uniform attachment graphon; (b): Ranked attachment graphon; (c): ER graphon."
MR,0.9296875,"0
20
40
t 0 1"
MR,0.9308035714285714,"πα
t (I | x = 0)"
MR,0.9319196428571429,"(a)
Wunif 0.0 0.5 1.0
α"
MR,0.9330357142857143,"0
20
40
t 0 1"
MR,0.9341517857142857,"πα
t (I | x = 0)"
MR,0.9352678571428571,"(b)
Wrank 0.0 0.5 1.0
α"
MR,0.9363839285714286,"0
20
40
t 0 1"
MR,0.9375,"πα
t (I | x = 0)"
MR,0.9386160714285714,"(c)
Wer 0.0 0.5 1.0
α"
MR,0.9397321428571429,"0
20
40
t 0 5"
MR,0.9408482142857143,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9419642857142857,"0
20
40
t 0 5"
MR,0.9430803571428571,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9441964285714286,"0
20
40
t 0 5"
MR,0.9453125,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9464285714285714,"Figure 7: The probability of investing at state x = 0 (top) together with the evolution of average
quality (bottom) for PPO. The solution is similar to Figure 6, though slightly different due to the
approximations stemming from PPO and sequential Monte Carlo. (a): Uniform attachment graphon;
(b): Ranked attachment graphon; (c): ER graphon."
MR,0.9475446428571429,"0
20
40
t 0.0 0.5 1.0 α"
MR,0.9486607142857143,"(a)
Wunif −1 4 9
ˆx"
MR,0.9497767857142857,"0
20
40
t 0.0 0.5 1.0 α"
MR,0.9508928571428571,"(b)
Wrank −1 4 9
ˆx"
MR,0.9520089285714286,"0
20
40
t 0.0 0.5 1.0 α"
MR,0.953125,"(c)
Wer −1 4 9
ˆx"
MR,0.9542410714285714,"0
20
40
t 0 5"
MR,0.9553571428571429,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9564732142857143,"0
20
40
t 0 5"
MR,0.9575892857142857,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9587053571428571,"0
20
40
t 0 5"
MR,0.9598214285714286,"P
x∈X xµα
t (x) 0.0 0.5 1.0
α"
MR,0.9609375,"Figure 8: Achieved equilibrium via M = 100 approximate equivalence classes in Investment-
Graphon. Top: Maximum quality ˆx up to which agents will invest (πα
t (I | ˆx) > 0.5), shown for
each α ∈I, t ∈T . Bottom: Expected quality versus time of each agent α ∈I. It can be observed
that agents with less connections (higher α) will invest more. (a): Uniform attachment graphon; (b):
Ranked attachment graphon; (c): ER graphon."
MR,0.9620535714285714,Published as a conference paper at ICLR 2022
MR,0.9631696428571429,"0
20
40
t 0.0 0.5 1.0"
MR,0.9642857142857143,"πα
t (D | S)"
MR,0.9654017857142857,"(a)
Wunif 0.0 0.5 1.0
α"
MR,0.9665178571428571,"0
20
40
t 0.0 0.5 1.0"
MR,0.9676339285714286,"πα
t (D | S)"
MR,0.96875,"(b)
Wunif 0.0 0.5 1.0
α"
MR,0.9698660714285714,"0
20
40
t 0.0 0.5 1.0"
MR,0.9709821428571429,"πα
t (D | S)"
MR,0.9720982142857143,"(c)
Wunif 0.0 0.5 1.0
α"
MR,0.9732142857142857,"0
20
40
t 0.00 0.25 0.50"
MR,0.9743303571428571,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9754464285714286,"0
20
40
t 0.00 0.25 0.50"
MR,0.9765625,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9776785714285714,"0
20
40
t 0.00 0.25 0.50"
MR,0.9787946428571429,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9799107142857143,"Figure 9: Achieved equilibrium via approximate equivalence classes in SIS-Graphon for the uniform
attachment graphon, plotted for each representative αi ∈I. Top: Probability of taking precautions
when healthy. Bottom: Probability of being infected. (a): M = 10; (b): M = 30; (c): M = 50."
MR,0.9810267857142857,"0
20
40
t 0.0 0.5 1.0"
MR,0.9821428571428571,"πα
t (D | S)"
MR,0.9832589285714286,"(a)
Wrank 0.0 0.5 1.0
α"
MR,0.984375,"0
20
40
t 0.0 0.5 1.0"
MR,0.9854910714285714,"πα
t (D | S)"
MR,0.9866071428571429,"(b)
Wrank 0.0 0.5 1.0
α"
MR,0.9877232142857143,"0
20
40
t 0.0 0.5 1.0"
MR,0.9888392857142857,"πα
t (D | S)"
MR,0.9899553571428571,"(c)
Wrank 0.0 0.5 1.0
α"
MR,0.9910714285714286,"0
20
40
t 0.00 0.25 0.50"
MR,0.9921875,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9933035714285714,"0
20
40
t 0.00 0.25 0.50"
MR,0.9944196428571429,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9955357142857143,"0
20
40
t 0.00 0.25 0.50"
MR,0.9966517857142857,"µα
t (I) 0.0 0.5 1.0
α"
MR,0.9977678571428571,"Figure 10: Achieved equilibrium via approximate equivalence classes in SIS-Graphon for the ranked
attachment graphon, plotted for each representative αi ∈I. Top: Probability of taking precautions
when healthy. Bottom: Probability of being infected. (a): M = 10; (b): M = 20; (c): M = 30."
MR,0.9988839285714286,"Figure 11: Learning curve and results for an exemplary straightforward application of multi-agent
PPO (MAPPO, Yu et al. (2021)). Left: Sum of expected agent objectives over learning iterations;
Right: Final policy probability of taking precautions when healthy."
