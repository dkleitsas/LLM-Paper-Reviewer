Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002544529262086514,"Differentiable sorting algorithms allow training with sorting and ranking supervi-
sion, where only the ordering or ranking of samples is known. Various methods
have been proposed to address this challenge, ranging from optimal transport-based
differentiable Sinkhorn sorting algorithms to making classic sorting networks dif-
ferentiable. One problem of current differentiable sorting methods is that they are
non-monotonic. To address this issue, we propose a novel relaxation of conditional
swap operations that guarantees monotonicity in differentiable sorting networks.
We introduce a family of sigmoid functions and prove that they produce differen-
tiable sorting networks that are monotonic. Monotonicity ensures that the gradients
always have the correct sign, which is an advantage in gradient-based optimiza-
tion. We demonstrate that monotonic differentiable sorting networks improve upon
previous differentiable sorting methods."
INTRODUCTION,0.005089058524173028,"1
INTRODUCTION"
INTRODUCTION,0.007633587786259542,"Recently, the idea of end-to-end training of neural networks with ordering supervision via continuous
relaxation of the sorting function has been presented by Grover et al. [1]. The idea of ordering
supervision is that the ground truth order of some samples is known while their absolute values
remain unsupervised. This is done by integrating a sorting algorithm in the neural architecture. As
the error needs to be propagated in a meaningful way back to the neural network when training with
a sorting algorithm in the architecture, it is necessary to use a differentiable sorting function. Several
such differentiable sorting functions have been introduced, e.g., by Grover et al. [1], Cuturi et al. [2],
Blondel et al. [3], and Petersen et al. [4]. In this work, we focus on analyzing differentiable sorting
functions [1]–[4] and demonstrate how monotonicity improves differentiable sorting networks [4]."
INTRODUCTION,0.010178117048346057,"Sorting networks are a family of sorting algorithms that consist of two basic components: so called
“wires” (or “lanes”) carrying values, and conditional swap operations that connect pairs of wires [5].
An example of such a sorting network is shown in the center of Figure 1. The conditional swap
operations swap the values carried by these wires if they are not in the desired order. They allow
for fast hardware-implementation, e.g., in ASICs, as well as on highly parallelized general-purpose
hardware like GPUs. Differentiable sorting networks [4] continuously relax the conditional swap
operations by relaxing their step function to a logistic sigmoid function."
INTRODUCTION,0.01272264631043257,"One problem that arises in this context is that using a logistic sigmoid function does not preserve
monotonicity of the relaxed sorting operation, which can cause gradients with the wrong sign. In this
work, we present a family of sigmoid functions that preserve monotonicity of differentiable sorting
networks. These include the cumulative density function (CDF) of the Cauchy distribution, as well
as a function that minimizes the error-bound and thus induces the smallest possible approximation
error. For all sigmoid functions, we prove and visualize the respective properties and validate
their advantages empirically. In fact, by making the sorting function monotonic, it also becomes
quasiconvex, which has been shown to produce favorable convergence rates [6]. In Figure 2, we
demonstrate monotonicity for different choices of sigmoid functions. As can be seen in Figure 4,
existing differentiable sorting operators are either non-monotonic or have an unbounded error."
INTRODUCTION,0.015267175572519083,"Following recent work [1], [2], [4], we benchmark our continuous relaxations by predicting values
displayed on four-digit MNIST images [7] supervised only by their ground truth order. The evaluation
shows that our method outperforms existing relaxations of the sorting function on the four-digit
MNIST ordering task as well as the SVHN ranking task."
INTRODUCTION,0.017811704834605598,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020356234096692113,"a5
CNN"
INTRODUCTION,0.022900763358778626,"a4
CNN"
INTRODUCTION,0.02544529262086514,"a3
CNN"
INTRODUCTION,0.027989821882951654,"a2
CNN"
INTRODUCTION,0.030534351145038167,"a1
CNN"
INTRODUCTION,0.03307888040712468,"a0
CNN b5 b4 b3 b2 b1"
INTRODUCTION,0.035623409669211195,"b0
Q
"
INTRODUCTION,0.03816793893129771,"



"
INTRODUCTION,0.04071246819338423,"1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
0
0
0
0 "
INTRODUCTION,0.043256997455470736,"



 P
"
INTRODUCTION,0.04580152671755725,"



"
INTRODUCTION,0.04834605597964377,".7 .1 .0 .1 .0 .1
.1 .1 .0 .5 .3 .0
.1 .1 .4 .2 .1 .1
.1 .2 .1 .0 .4 .2
.0 .2 .1 .0 .1 .6
.0 .3 .4 .2 .1 .0 "
INTRODUCTION,0.05089058524173028,"



"
INTRODUCTION,0.05343511450381679,"Figure 1: The architecture for training with ordering supervision. Left: input values are fed separately
into a Convolutional Neural Network (CNN) that has the same weights for all instances. The CNN
maps these values to scalar values a0, ..., a5. Center: the odd-even sorting network sorts the scalars
by parallel conditional swap operations such that all inputs can be propagated to their correct ordered
position. Right: It produces a differentiable permutation matrix P. In this experiment, the training
objective is the cross-entropy between P and the ground truth permutation matrix Q. By propagating
the error backward through the sorting network, we can train the CNN."
INTRODUCTION,0.05597964376590331,"Contributions.
In this work, we show that sigmoid functions with speciﬁc characteristics produce
monotonic and error-bounded differentiable sorting networks. We provide theoretical guarantees for
these functions and also give the monotonic function that minimizes the approximation error. We
empirically demonstrate that the proposed functions improve performance."
RELATED WORK,0.058524173027989825,"2
RELATED WORK"
RELATED WORK,0.061068702290076333,"Recently, differentiable approximations of the sorting function for weak supervision were introduced
by Grover et al. [1], Cuturi et al. [2], Blondel et al. [3], and Petersen et al. [4]."
RELATED WORK,0.06361323155216285,"In 2019, Grover et al. [1] proposed NeuralSort, a continuous relaxation of the argsort operator. A
(hard) permutation matrix is a square matrix with entries 0 and 1 such that every row and every column
sums up to 1, which deﬁnes the permutation necessary to sort a sequence. Grover et al. relax hard
permutation matrices by approximating them as unimodal row-stochastic matrices. This relaxation
allows for gradient-based stochastic optimization. On various tasks, including sorting four-digit
MNIST numbers, they benchmark their relaxation against the Sinkhorn and Gumbel-Sinkhorn
approaches proposed by Mena et al. [8]."
RELATED WORK,0.06615776081424936,"Cuturi et al. [2] follow this idea and approach differentiable sorting by smoothed ranking and sorting
operators using optimal transport. As the optimal transport problem alone is costly, they regularize
it and solve it using the Sinkhorn algorithm [9]. By relaxing the permutation matrix, which sorts a
sequence of scalars, they also train a scalar predictor of values displayed by four-digit numbers while
supervising their relative order only."
RELATED WORK,0.06870229007633588,"Blondel et al. [3] cast the problem of sorting and ranking as a linear program over a permutahedron. To
smooth the resulting discontinuous function and provide useful derivatives, they introduce a strongly
convex regularization. They evaluate the proposed approach in the context of top-k classiﬁcation and
label ranking accuracy via a soft Spearman’s rank correlation coefﬁcient."
RELATED WORK,0.07124681933842239,"Recently, Petersen et al. [4] proposed differentiable sorting networks, a differentiable sorting operator
based on sorting networks with differentiably relaxed conditional swap operations. Differentiable
sorting networks achieved a new state-of-the-art on both the four-digit MNIST sorting benchmark and
the SVHN sorting benchmark. Petersen et al. [10] also proposed a general method for continuously
relaxing algorithms via logistic distributions. They apply it, i.a., to the bubble sort algorithm and
benchmark in on the MNIST sorting benchmark."
RELATED WORK,0.0737913486005089,"Applications and Broader Impact. In the domain of recommender systems, Lee et al. [11] propose
differentiable ranking metrics, and Swezey et al. [12] propose PiRank, a learning-to-rank method
using differentiable sorting. Other works explore differentiable sorting-based top-k for applications
such as differentiable image patch selection [13], differentiable k-nearest-neighbor [1], [14], top-k
attention for machine translation [14], and differentiable beam search methods [14], [15]."
RELATED WORK,0.07633587786259542,Published as a conference paper at ICLR 2022
RELATED WORK,0.07888040712468193,"3
BACKGROUND: SORTING NETWORKS"
RELATED WORK,0.08142493638676845,"Sorting networks have a long tradition in computer science since the 1950s [5]. They are highly
parallel data-oblivious sorting algorithms. They are based on so-called conditional pairwise swap
operators that map two inputs to two outputs and ensure that these outputs are in a speciﬁc order. This
is achieved by simply passing through the inputs if they are already in the desired order and swapping
them otherwise. The order of executing conditional swaps is independent of the input values, which
makes them data-oblivious. Conditional swap operators can be implemented using only min and
max. That is, if the inputs are a and b and the outputs a′ and b′, a swap operator ensures a′ ≤b′
and can easily be formalized as a′ = min(a, b) and b′ = max(a, b). Examples of sorting nets are
the odd-even network [16], which alternatingly swaps odd and even wires with their successors, and
the bitonic network [17], which repeatedly merges and sorts bitonic sequences. While the odd-even
network requires n layers, the bitonic network uses the divide-and-conquer principle to sort within
only (log2 n)(1 + log2 n)/2 layers."
RELATED WORK,0.08396946564885496,"Note that, while they are similar in name, sorting networks are not neural networks that sort."
DIFFERENTIABLE SORTING NETWORKS,0.08651399491094147,"3.1
DIFFERENTIABLE SORTING NETWORKS"
DIFFERENTIABLE SORTING NETWORKS,0.089058524173028,"In the following, we recapitulate the core concepts of differentiable sorting networks [4]. An example
of an odd-even sorting network is shown in the center of Figure 1. Here, odd and even neighbors
are conditionally swapped until the entire sequence is sorted. Each conditional swap operation
can be deﬁned in terms of min and max as detailed above. These operators can be relaxed to
differentiable min and max. Note that we denote the differentiable relaxations in italic font and their
hard counterparts in roman font. Note that the differentiable relaxations min and max are different
from the commonly used softmin and softmax, which are relaxations of argmin and argmax [18]."
DIFFERENTIABLE SORTING NETWORKS,0.0916030534351145,One example for such a relaxation of min and max is the logistic relaxation
DIFFERENTIABLE SORTING NETWORKS,0.09414758269720101,"minσ(a, b) = a · σ(b −a) + b · σ(a −b)
and
max σ(a, b) = a · σ(a −b) + b · σ(b −a) (1)"
DIFFERENTIABLE SORTING NETWORKS,0.09669211195928754,where σ is the logistic sigmoid function with inverse temperature β > 0:
DIFFERENTIABLE SORTING NETWORKS,0.09923664122137404,"σ : x 7→
1
1 + e−βx .
(2)"
DIFFERENTIABLE SORTING NETWORKS,0.10178117048346055,"Any layer of a sorting network can also be represented as a relaxed and doubly-stochastic permutation
matrix. Multiplying these (layer-wise) permutation matrices yields a (relaxed) total permutation
matrix P. Multiplying P with an input x yields the differentiably sorted vector ˆx = Px, which
is also the output of the differentiable sorting network. Whether it is necessary to compute P, or
whether ˆx sufﬁces, depends on the speciﬁc application. For example, for a cross-entropy ranking /
sorting loss as used in the experiments in Section 6, P can be used to compute the cross-entropy to a
ground truth permutation matrix Q."
DIFFERENTIABLE SORTING NETWORKS,0.10432569974554708,"In the next section, we build on these concepts to introduce monotonic differentiable sorting networks,
i.e., all differentiably sorted outputs ˆx are non-decreasingly monotonic in all inputs x."
MONOTONIC DIFFERENTIABLE SORTING NETWORKS,0.10687022900763359,"4
MONOTONIC DIFFERENTIABLE SORTING NETWORKS"
MONOTONIC DIFFERENTIABLE SORTING NETWORKS,0.10941475826972011,"In this section, we start by introducing deﬁnitions and building theorems upon them. In Section 4.2,
we use these deﬁnitions and properties to discuss different relaxations of sorting networks."
THEORY,0.11195928753180662,"4.1
THEORY"
THEORY,0.11450381679389313,We start by deﬁning sigmoid functions and will then use them to deﬁne continuous conditional swaps.
THEORY,0.11704834605597965,"Deﬁnition 1 (Sigmoid Function). We deﬁne a (unipolar) sigmoid (i.e., s-shaped) function as a
continuous monotonically non-decreasing odd-symmetric (around 1"
THEORY,0.11959287531806616,2) function f with
THEORY,0.12213740458015267,"f : R →[0, 1]
with
lim
x→−∞f(x) = 0
and
lim
x→∞f(x) = 1."
THEORY,0.12468193384223919,Published as a conference paper at ICLR 2022
THEORY,0.1272264631043257,"Deﬁnition 2 (Continuous Conditional Swaps). Following [4], we deﬁne a continuous conditional
swap in terms of a sigmoid function f : R →[0, 1] as"
THEORY,0.1297709923664122,"minf(a, b) = a · f(b −a) + b · f(a −b),
max f(a, b) = a · f(a −b) + b · f(b −a),
(3)
argminf(a, b) = ( f(b −a),
f(a −b) ) ,
argmax f(a, b) = ( f(a −b),
f(b −a) ) .
(4)"
THEORY,0.13231552162849872,"We require a continuous odd-symmetric sigmoid function to preserve most of the properties of min
and max while also making argmin and argmax continuous as shown in Supplementary Material B.
In the following, we establish doubly-stochasticity and differentiability of P, which are important
properties for differentiable sorting and ranking operators.
Lemma 3 (Doubly-Stochasticity and Differentiability of P). (i) The relaxed permutation matrix P,
produced by a differentiable sorting network, is doubly-stochastic. (ii) P has the same differentiability
as f, e.g., if f is continuously differentiable in the input, P will be continuously differentiable in the
input to the sorting network. If f is differentiable almost everywhere (a.e.), P will be diff. a.e."
THEORY,0.13486005089058525,"Proof. (i) For each conditional swap between two elements i, j, the relaxed permutation matrix is 1
at the diagonal except for rows i and j: at points i, i and j, j the value is v ∈[0, 1], at points i, j and
j, i the value is 1 −v and all other entries are 0. This is doubly-stochastic as all rows and columns
add up to 1 by construction. As the product of doubly-stochastic matrices is doubly-stochastic, the
relaxed permutation matrix P, produced by a differentiable sorting network, is doubly-stochastic."
THEORY,0.13740458015267176,"(ii) The composition of differentiable functions is differentiable and the addition and multiplication
of differentiable functions is also differentiable. Thus, a sorting network is differentiable if the
employed sigmoid function is differentiable. “Differentiable” may be replaced with any other form
of differentiability, such as “differentiable a.e.”"
THEORY,0.13994910941475827,"Now that we have established the ingredients to differentiable sorting networks, we can focus on the
monotonicity of differentiable sorting networks.
Deﬁnition 4 (Monotonic Continuous Conditional Swaps). We say f produces monotonic conditional
swaps if minf(x, 0) is non-decreasingly monotonic in x, i.e., min′
f(x, 0) ≥0 for all x."
THEORY,0.14249363867684478,"It is sufﬁcient to deﬁne it w.l.o.g. in terms of minf(x, 0) due to its commutativity, stability, and
odd-symmetry of the operators (cf. Supplementary Material B).
Theorem 5 (Monotonicity of Continuous Conditional Swaps). A continuous conditional swap (in
terms of a differentiable sigmoid function f) being non-decreasingly monotonic in all arguments and
outputs requires that the derivative of f decays no faster than 1/x2, i.e.,"
THEORY,0.1450381679389313,"f ′(x) ∈Ω
 1 x2"
THEORY,0.1475826972010178,"
.
(5)"
THEORY,0.15012722646310434,"Proof. We show that Equation 5 is a necessary criterion for monotonicity of the conditional swap.
Because f is a continuous sigmoid function with f : R →[0, 1], minf(x, 0) = f(−x) · x > 0 for
some x > 0. Thus, montononicity of minf(x, 0) implies lim supx→∞minf(x, 0) > 0 (otherwise
the value would decrease again from a value > 0.) Thus,"
THEORY,0.15267175572519084,"lim
x→∞minf(x, 0) = lim
x→∞f(−x) · x = lim
x→∞
f(−x) 1/x"
THEORY,0.15521628498727735,"(L’Hôpital’s rule)
=
lim
x→∞
−f ′(−x)"
THEORY,0.15776081424936386,"−1/x2
(6)"
THEORY,0.16030534351145037,"= lim
x→∞
f ′(−x)"
THEORY,0.1628498727735369,"1/x2
= lim
x→∞
f ′(x)"
THEORY,0.16539440203562342,"1/x2 = lim sup
x→∞
f ′(x)"
THEORY,0.16793893129770993,"1/x2 > 0 ⇐⇒f ′(x) ∈Ω
 1 x2"
THEORY,0.17048346055979643,"
.
(7)"
THEORY,0.17302798982188294,"assuming limx→∞
f ′(x)"
THEORY,0.17557251908396945,"1/x2 exists. Otherwise, it can be proven analogously via a proof by contradiction."
THEORY,0.178117048346056,"Corollary 6 (Monotonic Sorting Networks). If the individual conditional swaps of a sorting network
are monotonic, the sorting network is also monotonic."
THEORY,0.1806615776081425,"Proof. If single layers g, h are non-decreasingly monotonic in all arguments and outputs, their
composition h ◦g is also non-decreasingly monotonic in all arguments and outputs. Thus, a network
of arbitrarily many layers is non-decreasingly monotonic."
THEORY,0.183206106870229,Published as a conference paper at ICLR 2022
THEORY,0.18575063613231552,"Above, we formalized the property of monotonicity. Another important aspect is whether the error of
the differentiable sorting network is bounded. It is very desirable to have a bounded error because
without bounded errors the result of the differentiable sorting network diverges from the result of the
hard sorting function. Minimizing this error is desirable."
THEORY,0.18829516539440203,"Deﬁnition 7 (Error-Bounded Continuous Conditional Swaps). A continuous conditional swap has
a bounded error if and only if supx minf(x, 0) = c is ﬁnite. The continuous conditional swap is
therefore said to have an error bounded by c."
THEORY,0.19083969465648856,"It is sufﬁcient to deﬁne it w.l.o.g. in terms of minf(x, 0) due to its commutativity, stability, and
odd-symmetry of the operators (cf. Supplementary Material B). In general, for better comparability
between functions, we assume a Lipschitz continuous function f with Lipschitz constant 1."
THEORY,0.19338422391857507,"Theorem 8 (Error-Bounds of Continuous Conditional Swaps). (i) A differentiable continuous condi-
tional swap has a bounded error if
f ′(x) ∈O
 1 x2"
THEORY,0.19592875318066158,"
.
(8)"
THEORY,0.1984732824427481,"(ii) If it is additionally monotonic, the error-bound can be found as limx→∞minf(x, 0) and addi-
tionally the error is bound only if Equation 8 holds."
THEORY,0.2010178117048346,"Proof. (i) W.l.o.g. we consider x > 0. Let g(z) := f(−1/z), g(0) = 0. Thus, g′(z) = 1/z2 ·
f ′(−1/z) ≤c according to Equation 8. Thus, g(z) = g(0) +
R z
0 g′(t)dt ≤c · z. Therefore,
f(−1/z) ≤c · z =⇒1/z · f(−1/z) ≤c and with x = 1/z =⇒x · f(−x) = minf(x, 0) ≤c."
THEORY,0.2035623409669211,"(ii) Let minf(x, 0) be monotonic and bound by minf(x, 0) ≤c. For x > 0 and h(x) := minf(x, 0),"
THEORY,0.20610687022900764,"h′(x) = −x · f ′(−x) + f(−x) =⇒x2f ′(−x) = −xh′(x)
|
{z
}
≤0"
THEORY,0.20865139949109415,"+x · f(−x) ≤x · f(−x) ≤c .
(9)"
THEORY,0.21119592875318066,"And thus f ′(x) ∈O
  1"
THEORY,0.21374045801526717,"x2

."
THEORY,0.21628498727735368,"Theorem 9 (Error-Bounds of Diff. Sorting Networks). If the error of individual conditional swaps of
a sorting network is bounded by ϵ and the network has ℓlayers, the total error is bounded by ϵ · ℓ."
THEORY,0.21882951653944022,"Proof. For the proof, cf. Supplementary Material D."
THEORY,0.22137404580152673,"Discussion.
Monotonicity is highly desirable as otherwise adverse effects such as an input requiring
to be decreased to increase the output can occur. In gradient-based training, non-mononicity is
problematic as it produces gradients with the opposite sign. In addition, as monotonicity is also given
in hard sorting networks, it is desirable to preserve this property in the relaxation. Further, monotonic
differentiable sorting networks are quasiconvex and quasiconcave as any monotonic function is
both quasiconvex and quasiconcave, which leads to favorable convergence rates [6]. Bounding and
reducing the deviation from its hard counterpart reduces the relaxation error, and thus is desirable."
SIGMOID FUNCTIONS,0.22391857506361323,"4.2
SIGMOID FUNCTIONS"
SIGMOID FUNCTIONS,0.22646310432569974,"Above, we have speciﬁed the space of functions for the differentiable swap operation, as well as
their desirable properties. In the following, we discuss four notable candidates as well as their
properties. The properties of these functions are visualized in Figures 2 and 3 and an overview over
their properties is given in Table 1."
SIGMOID FUNCTIONS,0.22900763358778625,"Logistic distributions.
The ﬁrst candidate is the logistic sigmoid function (the CDF of a logistic
distribution) as proposed in [4]:"
SIGMOID FUNCTIONS,0.23155216284987276,"σ(x) = CDFL
 
βx

=
1
1 + e−βx
(10)"
SIGMOID FUNCTIONS,0.2340966921119593,"This function is the de-facto default sigmoid function in machine learning. It provides a continuous,
error-bounded, and Lipschitz continuous conditional swap. However, for the logistic function,
monotonicity is not given, as displayed in Figure 2."
SIGMOID FUNCTIONS,0.2366412213740458,Published as a conference paper at ICLR 2022
SIGMOID FUNCTIONS,0.23918575063613232,"Table 1: For each function, we display the function, its derivative, and indicate whether the respective
relaxed sorting network is monotonic and has a bounded error."
SIGMOID FUNCTIONS,0.24173027989821882,"Function
f (CDF)
f ′ (PDF)
Eq.
Mono.
Bounded Error"
SIGMOID FUNCTIONS,0.24427480916030533,"σ
(10)

 (≈.0696/α)"
SIGMOID FUNCTIONS,0.24681933842239187,"fR
(11)

 (1/4/α)"
SIGMOID FUNCTIONS,0.24936386768447838,"fC
(12)

 (1/π2/α)"
SIGMOID FUNCTIONS,0.25190839694656486,"fO
(13)

 (1/16/α)"
SIGMOID FUNCTIONS,0.2544529262086514,"Reciprocal Sigmoid Function.
To obtain a function that yields a monotonic as well as error-
bound differentiable sorting network, a necessary criterion is f ′(x) ∈Θ(1/x2) (the intersection of
Equations 5 and 8.) A natural choice is, therefore, f ′
R(x) =
1
(2|x|+1)2 , which produces"
SIGMOID FUNCTIONS,0.25699745547073793,"fR(x) =
Z x −∞"
SIGMOID FUNCTIONS,0.2595419847328244,"1
(2β|t| + 1)2 dt = 1"
SIGMOID FUNCTIONS,0.26208651399491095,"2
2βx
1 + 2β|x| + 1"
SIGMOID FUNCTIONS,0.26463104325699743,"2.
(11)"
SIGMOID FUNCTIONS,0.26717557251908397,"fR fulﬁlls all criteria, i.e., it is an adequate sigmoid function and produces monotonic and error-bound
conditional swaps. It has an ϵ-bounded-error of ϵ = 0.25. It is also an afﬁne transformation of the
elementary bipolar sigmoid function x 7→
x
|x|+1. Properties of this function are visualized in Table 1
and Figures 2 and 3. Proofs for monotonicity can be found in Supplementary Material D."
SIGMOID FUNCTIONS,0.2697201017811705,"Cauchy distributions.
By using the CDF of the Cauchy distribution, we maintain montonicity
while reducing the error-bound to ϵ = 1/π2 ≈0.101. It is deﬁned as"
SIGMOID FUNCTIONS,0.272264631043257,"fC(x) = CDFC
 
βx

= 1 π Z x −∞"
SIGMOID FUNCTIONS,0.2748091603053435,"β
1 + (βt)2 dt = 1"
SIGMOID FUNCTIONS,0.27735368956743,"π arctan
 
βx

+ 1"
SIGMOID FUNCTIONS,0.27989821882951654,"2
(12)"
SIGMOID FUNCTIONS,0.2824427480916031,"In the experimental evaluation, we ﬁnd that tightening the error improves the performance."
SIGMOID FUNCTIONS,0.28498727735368956,"Optimal Monotonic Sigmoid Function.
At this point, we are interested in the monotonic swap
operation that minimizes the error-bound. Here, we set 1-Lipschitz continuity again as a requirement
to make different relaxations of conditional swaps comparable. We show that fO is the best possible
sigmoid function achieving an error-bound of only ϵ = 1/16
Theorem 10 (Optimal Sigmoid Function). The optimal sigmoid function minimizing the error-bound,
while producing a monotonic and 1-Lipschitz continuous (with β = 1) conditional swap operation, is"
SIGMOID FUNCTIONS,0.2875318066157761,"fO(x) = 

 
"
SIGMOID FUNCTIONS,0.2900763358778626,"−
1
16βx
if βx < −1 4,"
SIGMOID FUNCTIONS,0.2926208651399491,"1 −
1
16βx
if βx > + 1 4,"
SIGMOID FUNCTIONS,0.2951653944020356,βx + 1
SIGMOID FUNCTIONS,0.29770992366412213,"2
otherwise. (13)"
SIGMOID FUNCTIONS,0.30025445292620867,"Proof. Given the above conditions, the optimal sigmoid function is uniquely determined and can
easily be derived as follows: Due to stability, it sufﬁces to consider minf(x, 0) = x · f(−x) or
max f(0, x) = −x · f(x). Due to symmetry and inversion, it sufﬁces to consider minf(x, 0) =
x · f(−x) for x > 0."
SIGMOID FUNCTIONS,0.30279898218829515,"Since min(x, 0) = 0 for x > 0, we have to choose f in such a way as to make minf(x, 0) = x·f(−x)
as small as possible, but not negative. For this, f(−x) must be made as small as possible. Since we
know that f(0) = 1"
SIGMOID FUNCTIONS,0.3053435114503817,"2 and we are limited to functions f that are Lipschitz continuous with α = 1,
f(−x) cannot be made smaller than 1"
SIGMOID FUNCTIONS,0.30788804071246817,"2 −x, and hence minf(x, 0) cannot be made smaller than
x·
  1"
SIGMOID FUNCTIONS,0.3104325699745547,"2 −x

. To make minf(x, 0) as small as possible, we have to follow x·
  1"
SIGMOID FUNCTIONS,0.31297709923664124,"2 −x

as far as possible
(i.e., to values x as large as possible). Monotonicity requires that this function can be followed only
up to x = 1"
SIGMOID FUNCTIONS,0.3155216284987277,"4, at which point we have minf( 1"
SIGMOID FUNCTIONS,0.31806615776081426,"4, 0) = 1 4
  1 2 −1"
SIGMOID FUNCTIONS,0.32061068702290074,"4

=
1
16. For larger x, that is, for x > 1"
SIGMOID FUNCTIONS,0.3231552162849873,"4,
the value of x ·
  1"
SIGMOID FUNCTIONS,0.3256997455470738,"2 −x

decreases again and hence the functional form of the sigmoid function f has
to change at x = 1"
SIGMOID FUNCTIONS,0.3282442748091603,4 to remain monotonic.
SIGMOID FUNCTIONS,0.33078880407124683,Published as a conference paper at ICLR 2022
SIGMOID FUNCTIONS,0.3333333333333333,The best that can be achieved for x > 1
SIGMOID FUNCTIONS,0.33587786259541985,"4 is to make it constant, as it must not decrease (due to
monotonicity) and should not increase (to minimize the deviation from the crisp / hard version). That
is, minf(x, 0) =
1
16 for x > 1"
SIGMOID FUNCTIONS,0.3384223918575064,"4. It follows x · f(−x) =
1
16 and hence f(−x) =
1
16x for x > 1"
NOTE,0.34096692111959287,"4. Note
that, if the transition from the linear part to the hyperbolic part were at |x| < 1"
NOTE,0.3435114503816794,"4, the function would
not be Lipschitz continuous with α = 1."
NOTE,0.3460559796437659,"An overview of the selection of sigmoid functions we consider is shown in Table 1. Note how fR, fC
and fO in this order get closer to x + 1"
NOTE,0.3486005089058524,"2 (the gray diagonal line) and hence steeper in their middle part.
This is reﬂected by a widening region of values of the derivatives that are close to or even equal to 1. 0.2 0.1"
NOTE,0.3511450381679389,"1/2
1
3/2
2 x fR fC
fO σ"
NOTE,0.35368956743002544,"Figure 2: minf(x, 0) for different sigmoid
functions f; color coding as in Table 1."
NOTE,0.356234096692112,"Table 1 also indicates whether a sigmoid function
yields a monotonic swap operation or not, which is vi-
sualized in Figure 2: clearly σ-based sorting networks
are not monotonic, while all others are. It also states
whether the error is bounded, which for a monotonic
swap operation means limx→∞minf(x, 0) < ∞,
and gives their bound relative to the Lipschitz con-
stant α."
NOTE,0.35877862595419846,"Figure 3 displays the loss for a sorting network with
n = 3 inputs. We project the hexagon-shaped 3-value
permutahedron onto the x-y-plane, while the z-axis
indicates the loss. Note that, at the rightmost point"
NOTE,0.361323155216285,"Figure 3: Loss for a 3-wire odd-even sorting network, drawn
over a permutahedron projected onto the x-y-plane. For
logistic sigmoid (left) and optimal sigmoid (right)."
NOTE,0.3638676844783715,"(1, 2, 3), the loss is 0 because all ele-
ments are in the correct order, while at
the left front (2, 3, 1) and rear (3, 1, 2)
the loss is at its maximum because all
elements are at the wrong positions.
Along the red center line, the loss rises
logarithmic for the optimal sigmoid
function on the right. Note that the
monotonic sigmoid functions produce
a loss that is larger when more ele-
ments are in the wrong order. For the
logistic function, (3, 2, 1) has the same
loss as (2, 3, 1) even though one of the
ranks is correct at (3, 2, 1), while for
(2, 3, 1) all three ranks are incorrect."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.366412213740458,"5
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS 0.10 0.05"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.36895674300254455,"1/2
1
3/2
2 x"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.37150127226463103,"Sinkhorn
NeuralSort
Relaxed Bubble
Logistic
Logistic w/ART
FastSort"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.37404580152671757,"Figure 4: min(x, 0) for Sinkhorn sort (red),
NeuralSort (red), Relaxed Bubble sort (red),
diffsort with logistic sigmoid (red), diffsort
with activation replacement trick (purple), and
Fast Sort (orange)."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.37659033078880405,"For the special case of n = 2, i.e., for sorting
two elements, NeuralSort [1] and Relaxed Bubble
sort [10] are equivalent to differentiable sorting net-
works with the logistic sigmoid function. Thus, it is
non-monotonic, as displayed in Figure 4."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3791348600508906,"For the Sinkhorn sorting algorithm [2], we can sim-
ply construct an example of non-monotonicity by
keeping one value ﬁxed, e.g., at zero, and varying
the second value (x) as in Figure 4 and displaying
the minimum. Notably, for the case of n = 2, this
function is numerically equal to NeuralSort and differ-
entiable sorting networks with the logistic function."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3816793893129771,"For fast sort [3], we follow the same principle and
ﬁnd that it is indeed monotonic (in this example);
however, the error is unbounded, which is undesirable."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3842239185750636,"For differentiable sorting networks, Petersen et al. [4] proposed to extend the sigmoid function by the"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.38676844783715014,Published as a conference paper at ICLR 2022
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3893129770992366,"Table 2: For each differentiable sorting opera-
tor, whether it is monotonic (M), and whether
it has a bounded error (BE)."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.39185750636132316,"Method
M
BE"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3944020356234097,"NeuralSort

–
Sinkhorn Sort

–
Fast Sort


Relaxed Bubble Sort

–
Diff. Sorting Networks
σ


Diff. Sorting Networks
σ ◦ϕ

"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3969465648854962,"Diff. Sorting Networks
fR


Diff. Sorting Networks
fC


Diff. Sorting Networks
fO

"
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.3994910941475827,"activation replacement trick, which avoids extensive
blurring as well as vanishing gradients. They ap-
ply the activation replacement trick ϕ before feeding
the values into the logistic sigmoid function; thus,
the sigmoid function is effectively σ ◦ϕ.
Here,
ϕ : x 7→
x
|x|λ+ϵ where λ ∈[0, 1] and ϵ ≈10−10.
Here, the asymptotic character of σ ◦ϕ does not ful-
ﬁll the requirement set by Theorem 5, and is thereby
non-monotonic as also displayed in Figure 4 (purple)."
MONOTONICITY OF OTHER DIFFERENTIABLE SORTING OPERATORS,0.4020356234096692,"We summarize monotonicity and error-boundness for
all differentiable sorting functions in Table 2."
EMPIRICAL EVALUATION,0.40458015267175573,"6
EMPIRICAL EVALUATION"
EMPIRICAL EVALUATION,0.4071246819338422,"To evaluate the properties of the proposed function as well as their practical impact in the context of
sorting supervision, we evaluate them with respect to two standard benchmark datasets. The MNIST
sorting dataset [1]–[4] consists of images of numbers from 0000 to 9999 composed of four MNIST
digits [7]. Here, the task is training a network to produce a scalar output value for each image such
that the ordering of the outputs follows the respective ordering of the images. Speciﬁcally, the metrics
here are the proportion of full rankings correctly identiﬁed, and the proportion of individual element
ranks correctly identiﬁed [1]. The same task can also be extended to the more realistic SVHN [19]
dataset with the difference that the images are already multi-digit numbers as shown in [4]."
EMPIRICAL EVALUATION,0.40966921119592875,"Comparison to the State-of-the-Art.
We ﬁrst compare the proposed functions to other state-of-
the-art approaches using the same network architecture and training setup as used in previous works,
as well as among themselves. The respective hyperparameters for each setting can be found in
Supplementary Material A. We report the results in Table 3. The proposed monotonic differentiable
sorting networks outperform current state-of-the-art methods by a considerable margin. Especially
for those cases where more samples needed to be sorted, the gap between monotonic sorting nets
and other techniques grows with larger n. The computational complexity of the proposed method
depends on the employed sorting network architecture leading to a complexity of O(n3) for odd-even
networks and a complexity of O(n2 log2 n) for bitonic networks because all of the employed sigmoid
functions can be computed in closed form. This leads to the same runtime as in [4]."
EMPIRICAL EVALUATION,0.4122137404580153,"Comparing the three proposed functions among themselves, we observe that for odd-even networks
on MNIST, the error-optimal function fO performs best. This is because here the approximation
error is small. However, for the more complex bitonic sorting networks, fC (Cauchy) performs better
than fO. This is because fO does not provide a higher-order smoothness and is only C1 smooth,
while the Cauchy function fC is analytic and C∞smooth."
EMPIRICAL EVALUATION,0.41475826972010177,"Table 3: Results on the four-digit MNIST and SVHN tasks using the same architecture as previous
works [1]–[4]. The metric is the proportion of rankings correctly identiﬁed, and the value in
parentheses is the proportion of individual element ranks correctly identiﬁed. All results are averaged
over 5 runs. SVHN w/ n = 32 is omitted to reduce the carbon impact of the evaluation."
EMPIRICAL EVALUATION,0.4173027989821883,"MNIST
n = 3
n = 5
n = 7
n = 9
n = 15
n = 32
n = 16
n = 32
(bitonic)
(bitonic)"
EMPIRICAL EVALUATION,0.4198473282442748,"NeuralSort
91.9 (94.5)
77.7 (90.1)
61.0 (86.2)
43.4 (82.4)
9.7 (71.6)
0.0 (38.8)
—
—
Sinkhorn Sort
92.8 (95.0)
81.1 (91.7)
65.6 (88.2)
49.7 (84.7)
12.6 (74.2)
0.0 (41.2)
—
—
Fast Sort & Rank
90.6 (93.5)
71.5 (87.2)
49.7 (81.3)
29.0 (75.2)
2.8 (60.9)
—
—
—
Diffsort (Logistic)
92.0 (94.5)
77.2 (89.8)
54.8 (83.6)
37.2 (79.4)
4.7 (62.3)
0.0 (56.3)
10.8 (72.6)
0.3 (63.2)
Diffsort (Log. w/ ART)
94.3 (96.1)
83.4 (92.6)
71.6 (90.0)
56.3 (86.7)
23.5 (79.4)
0.5 (64.9)
19.0 (77.5)
0.8 (63.0)"
EMPIRICAL EVALUATION,0.4223918575063613,"fR : Reciprocal Sigmoid
94.4 (96.1)
85.0 (93.3)
73.4 (90.7)
60.8 (88.1)
30.2 (81.9)
1.0 (66.8)
28.7 (82.1)
1.3 (68.0)
fC : Cauchy CDF
94.2 (96.0)
84.9 (93.2)
73.3 (90.5)
63.8 (89.1)
31.1 (82.2)
0.8 (63.3)
29.0 (82.1)
1.6 (68.1)
fO : Optimal Sigmoid
94.6 (96.3)
85.0 (93.3)
73.6 (90.7)
62.2 (88.5)
31.8 (82.3)
1.4 (67.9)
28.4 (81.9)
1.4 (67.7)"
EMPIRICAL EVALUATION,0.42493638676844786,"SVHN
n = 3
n = 5
n = 7
n = 9
n = 15
—
n = 16
—
(bitonic)"
EMPIRICAL EVALUATION,0.42748091603053434,"Diffsort (Logistic)
76.3 (83.2)
46.0 (72.7)
21.8 (63.9)
13.5 (61.7)
0.3 (45.9)
—
1.2 (50.6)
—
Diffsort (Log. w/ ART)
83.2 (88.1)
64.1 (82.1)
43.8 (76.5)
24.2 (69.6)
2.4 (56.8)
—
3.4 (59.2)
—"
EMPIRICAL EVALUATION,0.4300254452926209,"fR : Reciprocal Sigmoid
85.7 (89.8)
68.8 (84.2)
53.3 (80.0)
40.0 (76.3)
13.2 (66.0)
—
11.5 (64.9)
—
fC : Cauchy CDF
85.5 (89.6)
68.5 (84.1)
52.9 (79.8)
39.9 (75.8)
13.7 (66.0)
—
12.2 (65.6)
—
fO : Optimal Sigmoid
86.0 (90.0)
67.5 (83.5)
53.1 (80.0)
39.1 (76.0)
13.2 (66.3)
—
10.6 (66.8)
—"
EMPIRICAL EVALUATION,0.43256997455470736,Published as a conference paper at ICLR 2022 0.954 0.956 0.958 0.960
EMPIRICAL EVALUATION,0.4351145038167939,"101
102
103 0.93 0.94 0.95"
EMPIRICAL EVALUATION,0.43765903307888043,n = 3 Logistic
EMPIRICAL EVALUATION,0.4402035623409669,n = 3 Logistic w/ ART
EMPIRICAL EVALUATION,0.44274809160305345,n = 3 Reciprocal f
EMPIRICAL EVALUATION,0.44529262086513993,n = 3 Cauchy f
EMPIRICAL EVALUATION,0.44783715012722647,n = 3 Optimal f 0.805 0.810 0.815 0.820 0.825
EMPIRICAL EVALUATION,0.45038167938931295,"101
102
103 0.60 0.65 0.70 0.75 0.80"
EMPIRICAL EVALUATION,0.4529262086513995,n = 15 Logistic
EMPIRICAL EVALUATION,0.455470737913486,n = 15 Logistic w/ ART
EMPIRICAL EVALUATION,0.4580152671755725,n = 15 Reciprocal f
EMPIRICAL EVALUATION,0.46055979643765904,n = 15 Cauchy f
EMPIRICAL EVALUATION,0.4631043256997455,"n = 15 Optimal f
β
β 0.64 0.66 0.68"
EMPIRICAL EVALUATION,0.46564885496183206,"101
102
103
104
0.45 0.50 0.55 0.60"
EMPIRICAL EVALUATION,0.4681933842239186,n = 32 Logistic
EMPIRICAL EVALUATION,0.4707379134860051,n = 32 Logistic w/ ART
EMPIRICAL EVALUATION,0.4732824427480916,n = 32 Reciprocal f
EMPIRICAL EVALUATION,0.4758269720101781,n = 32 Cauchy f
EMPIRICAL EVALUATION,0.47837150127226463,n = 32 Optimal f 0.64 0.66 0.68
EMPIRICAL EVALUATION,0.48091603053435117,"101
102
103
0.45 0.50 0.55 0.60"
EMPIRICAL EVALUATION,0.48346055979643765,n = 32 Logistic
EMPIRICAL EVALUATION,0.4860050890585242,n = 32 Logistic w/ ART
EMPIRICAL EVALUATION,0.48854961832061067,n = 32 Reciprocal f
EMPIRICAL EVALUATION,0.4910941475826972,n = 32 Cauchy f
EMPIRICAL EVALUATION,0.49363867684478374,"n = 32 Optimal f
β
β"
EMPIRICAL EVALUATION,0.4961832061068702,"Figure 5: Evaluating different sigmoid functions on the sorting MNIST task for ranges of different
inverse temperatures β. The metric is the proportion of individual element ranks correctly identiﬁed.
In all settings, the monotonic sorting networks clearly outperform the non-monotonic ones. Top:
Odd-Even sorting networks with n = 3 (left) and n = 15 (right). Bottom: n = 32 with an Odd-Even
(left) and a Bitonic network (right). For small n, such as 3, Cauchy performs best because it has
a low error but is smooth at the same time. For larger n, such as 15 and 32, the optimal sigmoid
function (wrt. error) fO performs better because it, while not being smooth, has the smallest possible
approximation error which is more important for deeper networks. For the bitonic network with its
more complex structure at n = 32 (bottom right), the reciprocal sigmoid fR performs best."
EMPIRICAL EVALUATION,0.49872773536895676,"Evaluation of Inverse Temperature β.
To further understand the behavior of the proposed mono-
tonic functions compared to the logistic sigmoid function, we evaluate all sigmoid functions for
different inverse temperatures β during training. We investigate four settings: odd-even networks for
n ∈{3, 15, 32} and a bitonic sorting network with n = 32 on the MNIST data set. Notably, there are
15 layers in the bitonic sorting networks with n = 32, while the odd-even networks for n = 15 also
has 15 layers. We display the results of this evaluation in Figure 5. In Supplementary Material C,
we show an analogous ﬁgure with additional settings. Note that here, we train for only 50% of the
training steps compared to Table 3 to reduce the computational cost."
EMPIRICAL EVALUATION,0.5012722646310432,"We observe that the optimal inverse temperature depends on the number of layers, rather than the
overall number of samples n. This can be seen when comparing the peak accuracy of each function
for the odd-even sorting network for different n and thus for different numbers of layers. The bitonic
network for n = 32 (bottom right) has the same number of layers as n = 15 in the odd-even network
(top right). Here, the peak performances for each sigmoid function fall within the same range, whereas
the peak performances for the odd-even network for n = 32 (bottom left) are shifted almost an order
of magnitude to the right. For all conﬁgurations, the proposed sigmoid functions for monotonic
sorting networks improve over the standard logistic sigmoid function, as well as the ART."
EMPIRICAL EVALUATION,0.5038167938931297,The source code of this work is publicly available at github.com/Felix-Petersen/diffsort.
CONCLUSION,0.5063613231552163,"7
CONCLUSION"
CONCLUSION,0.5089058524173028,"In this work, we addressed and analyzed monotonicity and error-boundness in differentiable sorting
and ranking operators. Speciﬁcally, we focussed on differentiable sorting networks and presented a
family of sigmoid functions that preserve monotonicity and bound approximation errors in differen-
tiable sorting networks. This makes the sorting functions quasiconvex, and we empirically observe
that the resulting method outperforms the state-of-the-art in differentiable sorting supervision."
CONCLUSION,0.5114503816793893,Published as a conference paper at ICLR 2022
CONCLUSION,0.5139949109414759,ACKNOWLEDGMENTS & FUNDING DISCLOSURE
CONCLUSION,0.5165394402035624,"We warmly thank Robert Denk for helpful discussions. This work was supported by the Goethe
Center for Scientiﬁc Computing (G-CSC) at Goethe University Frankfurt, the IBM-MIT Watson AI
Lab, the DFG in the Cluster of Excellence EXC 2117 “Centre for the Advanced Study of Collective
Behaviour” (Project-ID 390829875), and the Land Salzburg within the WISS 2025 project IDA-Lab
(20102-F1901166-KZP and 20204-WISS/225/197-2019)."
REPRODUCIBILITY STATEMENT,0.5190839694656488,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.5216284987277354,"We made the source code and experiments of this work publicly available at github.com/Felix-
Petersen/diffsort to foster future research in this direction. All data sets are publicly available. We
specify all necessary hyperparameters for each experiment. We use the same model architectures as
in previous works. We demonstrate how the choice of hyperparameter β affects the performance in
Figure 5. Each experiment can be reproduced on a single GPU."
REFERENCES,0.5241730279898219,REFERENCES
REFERENCES,0.5267175572519084,"[1]
A. Grover, E. Wang, A. Zweig, and S. Ermon, “Stochastic Optimization of Sorting Networks via
Continuous Relaxations,” in International Conference on Learning Representations (ICLR), 2019.
[2]
M. Cuturi, O. Teboul, and J.-P. Vert, “Differentiable ranking and sorting using optimal transport,” in
Proc. Neural Information Processing Systems (NeurIPS), 2019.
[3]
M. Blondel, O. Teboul, Q. Berthet, and J. Djolonga, “Fast Differentiable Sorting and Ranking,” in
Proc. Machine Learning Research (PMLR), International Conference on Machine Learning (ICML),
2020.
[4]
F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Differentiable sorting networks for scalable sorting
and ranking supervision,” in Proc. Machine Learning Research (PMLR), International Conference on
Machine Learning (ICML), 2021.
[5]
D. E. Knuth, The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Ed.) Addison
Wesley, 1998.
[6]
K. C. Kiwiel, “Convergence and efﬁciency of subgradient methods for quasiconvex minimization,”
Mathematical Programming, vol. 90, 2001.
[7]
Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,” 2010. [Online]. Available:
http://yann.lecun.com/exdb/mnist.
[8]
G. Mena, D. Belanger, S. Linderman, and J. Snoek, “Learning latent permutations with gumbel-sinkhorn
networks,” in International Conference on Learning Representations (ICLR), 2018.
[9]
M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal transport,” in Proc. Neural Informa-
tion Processing Systems (NeurIPS), 2013.
[10]
F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Learning with algorithmic supervision via continu-
ous relaxations,” in Proc. Neural Information Processing Systems (NeurIPS), 2021.
[11]
H. Lee, S. Cho, Y. Jang, J. Kim, and H. Woo, “Differentiable ranking metric using relaxed sorting for
top-k recommendation,” IEEE Access, 2021.
[12]
R. Swezey, A. Grover, B. Charron, and S. Ermon, “Pirank: Learning to rank via differentiable sorting,”
in Proc. Neural Information Processing Systems (NeurIPS), 2021.
[13]
J.-B. Cordonnier, A. Mahendran, A. Dosovitskiy, D. Weissenborn, J. Uszkoreit, and T. Unterthiner,
“Differentiable patch selection for image recognition,” in Proc. International Conference on Computer
Vision and Pattern Recognition (CVPR), 2021.
[14]
Y. Xie, H. Dai, M. Chen, B. Dai, T. Zhao, H. Zha, W. Wei, and T. Pﬁster, “Differentiable top-k with
optimal transport,” in Proc. Neural Information Processing Systems (NeurIPS), 2020.
[15]
K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous relaxation of beam search for
end-to-end training of neural sequence models,” in AAAI Conference on Artiﬁcial Intelligence, 2018.
[16]
A. N. Habermann, “Parallel neighbor-sort (or the glory of the induction principle),” 1972.
[17]
K. E. Batcher, “Sorting networks and their applications,” in Proc. AFIPS Spring Joint Computing
Conference (Atlantic City, NJ), 1968, pp. 307–314.
[18]
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y.
Bengio, “Generative adversarial networks,” in Proc. Neural Information Processing Systems (NeurIPS),
2014.
[19]
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading digits in natural images with
unsupervised feature learning,” 2011."
REFERENCES,0.5292620865139949,Published as a conference paper at ICLR 2022
REFERENCES,0.5318066157760815,"[20]
D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International Conference on
Learning Representations (ICLR), 2015.
[21]
I. J. Goodfellow, Y. Bulatov, J. Ibarz, S. Arnoud, and V. Shet, “Multi-digit number recognition from
street view imagery using deep convolutional neural networks,” Computing Research Repository (CoRR)
in arXiv, 2013."
REFERENCES,0.5343511450381679,Published as a conference paper at ICLR 2022
REFERENCES,0.5368956743002544,"A
IMPLEMENTATION DETAILS"
REFERENCES,0.539440203562341,"For training, we use the same network architecture as in previous works [1], [2], [4] and also use
the Adam optimizer [20] at a learning rate of 3 · 10−4. For Figure 5, we train for 100 000 steps. For
Table 3, we train for 200 000 steps on MNIST and 1 000 000 steps on SVHN. We preprocess SVHN
as done by Goodfellow et al. [21]."
REFERENCES,0.5419847328244275,"A.1
INVERSE TEMPERATURE β"
REFERENCES,0.544529262086514,"For the inverse temperature β, we use the following values, which correspond to the optima in
Figure 5 and were found via grid search:"
REFERENCES,0.5470737913486005,"Method
n
3
5
7
9
15
32
16
32
odd-even / bitonic
oe
oe
oe
oe
oe
oe
bi
bi"
REFERENCES,0.549618320610687,"σ
Logistic
79
30
33
54
32
128
43
8"
REFERENCES,0.5521628498727735,"σ ◦ϕ
Log. w/ ART
15
20
13
34
16
29
28
26"
REFERENCES,0.55470737913486,"fR
Reciprocal Sigmoid
14
60
69
44
120
1140
124
76"
REFERENCES,0.5572519083969466,"fC
Cauchy CDF
14.5π
51π
71π
15π
40π
169π
12π
48.5π
45.6
160.2
223.1
47.1
125.7
531.0
37.7
152.4"
REFERENCES,0.5597964376590331,"fO
Optimal Sigmoid
6
20
29
32
25
124
17
25"
REFERENCES,0.5623409669211196,"B
PROPERTIES OF min AND max"
REFERENCES,0.5648854961832062,"The core element of differentiable sorting networks is the relaxation of the conditional swap operation,
allowing for a soft transition between passing through and swapping, such that the sorting operator
becomes differentiable. It is natural to try to achieve this by using soft versions of the minimum
(denoted by min) and maximum operators (denoted by max). But before we consider concrete
examples, let us collect some desirable properties that such relaxations should have. Naturally, min
and max should satisfy many properties that their crisp / hard counterparts min and max satisfy, as
well as a few others (for a, b, c ∈R):"
REFERENCES,0.5674300254452926,"Symmetry / Commutativity.
Since min and max are symmetric/commutative, so should be their
soft counterparts: min(a, b) = min(b, a) and max(a, b) = max(b, a)."
REFERENCES,0.5699745547073791,"Ordering.
Certainly a (soft) maximum of two numbers should be at least as large as a (soft)
minimum of the same two numbers: min(a, b) ≤max(b, a)."
REFERENCES,0.5725190839694656,"Continuity in Both Arguments.
Both min and max should be continuous in both arguments."
REFERENCES,0.5750636132315522,"Idempotency.
If the two arguments are equal in value, this value should be the result of min and
max, that is, min(a, a) = max(a, a) = a."
REFERENCES,0.5776081424936387,"Inversion.
As for min and max, the two operators min and max should be connected in such a
way that the result of one operator equals the negated result of the other operator applied to negated
arguments: min(a, b) = −max(−a, −b) and max(a, b) = −min(−a, −b)."
REFERENCES,0.5801526717557252,"Stability / Shift Invariance.
Shifting both arguments by some value c ∈R should shift each
operator’s result by the same value: min(a + c, b + c) = min(a, b) + c and max(a + c, b + c) =
max(a, b) + c. Stability implies that the values of min and max depend effectively only on the
difference of their arguments. Speciﬁcally, choosing c = −a yields min(a, b) = min(0, b −a) + a
and max(a, b) = max(0, b −a) + a, and c = −b yields min(a, b) = min(a −b, 0) + b and
max(a, b) = max(a −b, 0) + b."
REFERENCES,0.5826972010178118,Published as a conference paper at ICLR 2022
REFERENCES,0.5852417302798982,"Sum preservation.
The sum of min and max should equal the sum of min and max: min(a, b) +
max(a, b) = min(a, b) + max(a, b) = a + b. Note that sum preservation follows from stability,
inversion and symmetry: min(a, b) = min(a−b, 0)+b = b−max(0, b−a) = b−(max(a, b)−a) =
a + b −max(a, b)"
REFERENCES,0.5877862595419847,"Bounded by Hard Versions.
Soft operators should not yield values more extreme than their crisp
/ hard counterparts: min(a, b) ≤min(a, b) and max(a, b) ≤max(a, b). Note that together with
ordering this property implies idempotency, vi7.: a = min(a, a) ≤min(a, a) ≤max(a, a) ≤
max(a, a) = a. Otherwise, they cannot be deﬁned via a convex combination of their inputs, making
it impossible to deﬁne proper argmin and argmax, and hence we could not compute differentiable
permutation matrices."
REFERENCES,0.5903307888040712,"Monotonicity in Both Arguments.
For any c > 0, it should be min(a + c, b) ≥min(a, b),
min(a, b + c) ≥min(a, b), max(a + c, b) ≥max(a, b), and max(a, b + c) ≥max(a, b). Note
that the second expression for each operator follows from the ﬁrst with the help of symmetry /
commutativity."
REFERENCES,0.5928753180661578,"Bounded Error / Minimum Deviation from Hard Versions.
Soft versions of minimum and
maximum should differ as little as possible from their crisp / hard counterparts. However, this
condition needs to be made more precise to yield concrete properties (see below for details)."
REFERENCES,0.5954198473282443,"Note that min and max cannot satisfy associativity, as this would force them to be identical to
their hard counterparts. Associativity means that max(a, max(b, c)) = max(max(a, b), c) and
min(a, min(b, c)) = min(min(a, b), c). Now consider a, b ∈R with a < b. Then with associativity
and idempotency max(a, max(a, b)) = max(max(a, a), b) = max(a, b) and hence max(a, b) =
b = max(a, b) (by comparison of the second arguments). Analogously, one can show that if
associativity held, we would have min(a, b) = a = min(a, b). That is, one cannot have both
associativity and idempotency. Note that without idempotency, the soft operators would not be
bounded by their hard versions. As idempotency is thus necessary, associativity has to be given up."
REFERENCES,0.5979643765903307,"If min and max are to be bounded by the crisp / hard version and symmetry, ordering, inversion and
stability (which imply sum preservation) hold, they must be convex combinations of the arguments a
and b with weights that depend only on the difference of a and b. That is,"
REFERENCES,0.6005089058524173,"min(a, b)
=
f(b −a) · a + (1 −f(b −a)) · b
max(a, b)
=
(1 −f(b −a)) · a +
f(b −a) · b,"
REFERENCES,0.6030534351145038,"where f(x) yields a value in [0, 1] (due to boundedness of min and max by their crisp / hard
counterparts). Due to inversion, f must satisfy f(x) = 1−f(−x) and hence f(0) = 1"
MONOTONICITY,0.6055979643765903,"2. Monotonicity
of min and max requires that f is a monotonically increasing function. Continuity requires that f is
a continuous function. In summary, f must be a continuous sigmoid function (in the older meaning
of this term, i.e., an s-shaped function, of which the logistic function is only a special case) satisfying
f(x) = 1 −f(−x)."
MONOTONICITY,0.6081424936386769,"As mentioned, the condition that the soft versions of minimum and maximum should deviate as
little as possible from the crisp / hard versions causes a slight problem: this deviation can always
be made smaller by making the sigmoid function steeper (reaching the crisp / hard versions in
the limit for inﬁnite inverse temperature, when the sigmoid function turns into the Heaviside step
function). Hence, in order to ﬁnd the best shape of the sigmoid function, we have to limit its inverse
temperature. Therefore, w.l.o.g., we require the sigmoid function to be Lipschitz-continuous with
Lipschitz constant α = 1."
MONOTONICITY,0.6106870229007634,"C
ADDITIONAL EXPERIMENTS"
MONOTONICITY,0.6132315521628499,"In Figure 6, we display additional results for more setting analogous to Figure 5."
MONOTONICITY,0.6157760814249363,Published as a conference paper at ICLR 2022 0.954 0.956 0.958 0.960
MONOTONICITY,0.6183206106870229,"101
102
103 0.93 0.94 0.95"
MONOTONICITY,0.6208651399491094,n = 3 Logistic
MONOTONICITY,0.6234096692111959,n = 3 Logistic w/ ART
MONOTONICITY,0.6259541984732825,n = 3 Reciprocal f
MONOTONICITY,0.628498727735369,n = 3 Cauchy f
MONOTONICITY,0.6310432569974554,n = 3 Optimal f 0.922 0.924 0.926 0.928 0.930
MONOTONICITY,0.6335877862595419,"101
102
103
0.84 0.86 0.88 0.90 0.92"
MONOTONICITY,0.6361323155216285,n = 5 Logistic
MONOTONICITY,0.638676844783715,n = 5 Logistic w/ ART
MONOTONICITY,0.6412213740458015,n = 5 Reciprocal f
MONOTONICITY,0.6437659033078881,n = 5 Cauchy f
MONOTONICITY,0.6463104325699746,"n = 5 Optimal f
β
β"
MONOTONICITY,0.648854961832061,0.8950
MONOTONICITY,0.6513994910941476,0.8975
MONOTONICITY,0.6539440203562341,0.9000
MONOTONICITY,0.6564885496183206,0.9025
MONOTONICITY,0.6590330788804071,0.9050
MONOTONICITY,0.6615776081424937,"101
102
103 0.80 0.85"
MONOTONICITY,0.6641221374045801,n = 7 Logistic
MONOTONICITY,0.6666666666666666,n = 7 Logistic w/ ART
MONOTONICITY,0.6692111959287532,n = 7 Reciprocal f
MONOTONICITY,0.6717557251908397,n = 7 Cauchy f
MONOTONICITY,0.6743002544529262,n = 7 Optimal f 0.870 0.875 0.880 0.885
MONOTONICITY,0.6768447837150128,"101
102
103 0.75 0.80 0.85"
MONOTONICITY,0.6793893129770993,n = 9 Logistic
MONOTONICITY,0.6819338422391857,n = 9 Logistic w/ ART
MONOTONICITY,0.6844783715012722,n = 9 Reciprocal f
MONOTONICITY,0.6870229007633588,n = 9 Cauchy f
MONOTONICITY,0.6895674300254453,"n = 9 Optimal f
β
β 0.805 0.810 0.815 0.820 0.825"
MONOTONICITY,0.6921119592875318,"101
102
103 0.60 0.65 0.70 0.75 0.80"
MONOTONICITY,0.6946564885496184,n = 15 Logistic
MONOTONICITY,0.6972010178117048,n = 15 Logistic w/ ART
MONOTONICITY,0.6997455470737913,n = 15 Reciprocal f
MONOTONICITY,0.7022900763358778,n = 15 Cauchy f
MONOTONICITY,0.7048346055979644,n = 15 Optimal f 0.64 0.66 0.68
MONOTONICITY,0.7073791348600509,"101
102
103
104
0.45 0.50 0.55 0.60"
MONOTONICITY,0.7099236641221374,n = 32 Logistic
MONOTONICITY,0.712468193384224,n = 32 Logistic w/ ART
MONOTONICITY,0.7150127226463104,n = 32 Reciprocal f
MONOTONICITY,0.7175572519083969,n = 32 Cauchy f
MONOTONICITY,0.7201017811704835,"n = 32 Optimal f
β
β 0.805 0.810 0.815"
MONOTONICITY,0.72264631043257,"101
102
103
0.60 0.65 0.70 0.75 0.80"
MONOTONICITY,0.7251908396946565,n = 16 Logistic
MONOTONICITY,0.727735368956743,n = 16 Logistic w/ ART
MONOTONICITY,0.7302798982188295,n = 16 Reciprocal f
MONOTONICITY,0.732824427480916,n = 16 Cauchy f
MONOTONICITY,0.7353689567430025,n = 16 Optimal f 0.64 0.66 0.68
MONOTONICITY,0.7379134860050891,"101
102
103
0.45 0.50 0.55 0.60"
MONOTONICITY,0.7404580152671756,n = 32 Logistic
MONOTONICITY,0.7430025445292621,n = 32 Logistic w/ ART
MONOTONICITY,0.7455470737913485,n = 32 Reciprocal f
MONOTONICITY,0.7480916030534351,n = 32 Cauchy f
MONOTONICITY,0.7506361323155216,"n = 32 Optimal f
β
β"
MONOTONICITY,0.7531806615776081,"Figure 6: Additional results analogous to Figure 5. Evaluating different sigmoid functions on the
sorting MNIST task for ranges of different inverse temperatures β. The metric is the proportion
of individual element ranks correctly identiﬁed. In all settings, the monotonic sorting networks
clearly outperform the non-monotonic ones. The ﬁrst three rows use odd-even networks with
n ∈{3, 5, 7, 9, 15, 32}. The last row uses bitonic networks with n ∈{16, 32}."
MONOTONICITY,0.7557251908396947,Published as a conference paper at ICLR 2022
MONOTONICITY,0.7582697201017812,"D
ADDITIONAL PROOFS"
MONOTONICITY,0.7608142493638677,"Theorem 9 (Error-Bounds of Diff. Sorting Networks). If the error of individual conditional swaps of
a sorting network is bounded by ϵ and the network has ℓlayers, the total error is bounded by ϵ · ℓ."
MONOTONICITY,0.7633587786259542,"Proof. Induction over number k of executed layers. Let x(k) be input x differentially sorted for k
layers and x(k) be input x hard sorted for k layers as an anchor. We require this anchor, as it is
possible that x(k)
i
< x(k)
j
but x(k)
i
> x(k)
j
for some i, j, k."
MONOTONICITY,0.7659033078880407,"Begin of induction: k = 0. Input vector x equals the vector x(0) after 0 layers. Thus, the error is
equal to 0 · ϵ."
MONOTONICITY,0.7684478371501272,"Step of induction: Given that after k −1 layers the error is smaller than or equal to (k −1)ϵ, we need
to show that the error after k layers is smaller than or equal to kϵ."
MONOTONICITY,0.7709923664122137,"The layer consists of comparator pairs i, j. W.l.o.g. we assume x(k−1)
i
≤x(k−1)
j
. W.l.o.g. we assume"
MONOTONICITY,0.7735368956743003,"that wire i will be the min and that wire j will be the max, therefore x(k)
i
≤x(k)
j . This implies"
MONOTONICITY,0.7760814249363868,"x(k−1)
i
= x(k)
i
and x(k−1)
j
= x(k)
j . We distinguish two cases:"
MONOTONICITY,0.7786259541984732,"•

x(k−1)
i
≤x(k−1)
j
and x(k−1)
i
≤x(k−1)
j

According to the assumption,
x(k−1)
i
−x(k)
i
 ≤ϵ and
x(k−1)
j
−x(k)
j
 ≤ϵ. Thus,
x(k)
i
−x(k)
i
 ≤
x(k−1)
i
−x(k−1)
i
+
x(k−1)
i
−x(k)
i
 ≤(k−1)ϵ+ϵ = kϵ."
MONOTONICITY,0.7811704834605598,"•

x(k−1)
i
≤x(k−1)
j
but x(k−1)
i
> x(k−1)
j

This case can only occur if
x(k−1)
j
−x(k−1)
i
 ≤"
MONOTONICITY,0.7837150127226463,"(k −1)ϵ and
x(k−1)
i
−x(k−1)
j
 ≤(k −1)ϵ because x(k−1)
i
and x(k−1)
j
have to be so close that within"
MONOTONICITY,0.7862595419847328,"margin of error such a reversed order is possible. According to the assumption,
x(k−1)
j
−x(k)
i
 ≤ϵ and
x(k−1)
i
−x(k)
j
 ≤ϵ. Thus,
x(k)
i
−x(k)
i
 ≤
x(k−1)
j
−x(k−1)
i
+
x(k−1)
j
−x(k)
i
 ≤(k−1)ϵ+ϵ = kϵ."
MONOTONICITY,0.7888040712468194,Theorem 11. minfR and maxfR are monotonic functions with the sigmoid function fR.
MONOTONICITY,0.7913486005089059,"Proof. Wlog., we assume ai = x and aj = 0."
MONOTONICITY,0.7938931297709924,"minfR(x, 0) = x · fR(−x) = x1 2"
MONOTONICITY,0.7964376590330788,"
x
1 + |x| + 1

(14)"
MONOTONICITY,0.7989821882951654,"To show monotonicity, we consider its derivative / slope."
MONOTONICITY,0.8015267175572519,"d
dxminfR(x, 0) = d dx 
x1 2"
MONOTONICITY,0.8040712468193384,"
x
1 + |x| + 1

(15) = 1 2"
MONOTONICITY,0.806615776081425,"
x
1 + |x| + 1

+ x1"
"D
DX",0.8091603053435115,"2
d
dx"
"D
DX",0.811704834605598,"
x
1 + |x| + 1

(16) = 1 2"
"D
DX",0.8142493638676844,"
x
1 + |x| + 1

+ x1"
"D
DX",0.816793893129771,"2
d
dx"
"D
DX",0.8193384223918575,"
x
1 + |x|"
"D
DX",0.821882951653944,"
(17) = 1 2"
"D
DX",0.8244274809160306,"
x
1 + |x| + 1

+ x1 2"
"D
DX",0.8269720101781171,"dx
dx · (1 + |x|) −x · d|x|+1"
"D
DX",0.8295165394402035,"dx
(1 + |x|)2
(18) = 1 2"
"D
DX",0.8320610687022901,"
x
1 + |x| + 1

+ x1"
"D
DX",0.8346055979643766,"2
(1 + |x|) −x sgn(x)"
"D
DX",0.8371501272264631,"(1 + |x|)2
(19) = 1 2"
"D
DX",0.8396946564885496,"
x
1 + |x| + 1

+ x1"
"D
DX",0.8422391857506362,"2
1 + |x| −|x|"
"D
DX",0.8447837150127226,"(1 + |x|)2
(20)"
"D
DX",0.8473282442748091,Published as a conference paper at ICLR 2022 = 1 2
"D
DX",0.8498727735368957,"
x
1 + |x| + 1

+ x1"
"D
DX",0.8524173027989822,"2
1
1 + 2|x| + |x|2
(21) = 1 2"
"D
DX",0.8549618320610687,"
x
1 + |x| + 1 +
x
1 + 2|x| + |x|2"
"D
DX",0.8575063613231552,"
(22) = 1 2"
"D
DX",0.8600508905852418,"
x(1 + |x|)
1 + 2|x| + |x|2 + 1 + 2|x| + |x|2"
"D
DX",0.8625954198473282,"1 + 2|x| + |x|2 +
x
1 + 2|x| + |x|2"
"D
DX",0.8651399491094147,"
(23) = 1 2"
"D
DX",0.8676844783715013,2x + 2|x| + x|x| + |x|2 + 1
"D
DX",0.8702290076335878,1 + 2|x| + |x|2
"D
DX",0.8727735368956743,"
(24) = 1 2"
"D
DX",0.8753180661577609,2(x + |x|) + |x|(x + |x|) + 1
"D
DX",0.8778625954198473,1 + 2|x| + |x|2
"D
DX",0.8804071246819338,"
(25) ≥1 2"
"D
DX",0.8829516539440203,"
1
1 + 2|x| + |x|2"
"D
DX",0.8854961832061069,"
(because x + |x| ≥0)
(26)"
"D
DX",0.8880407124681934,"> 0
(27)
maxfR is analogous."
"D
DX",0.8905852417302799,Theorem 12. minfC and maxfC are monotonic functions with the sigmoid function fC.
"D
DX",0.8931297709923665,"Proof. Wlog., we assume ai = x and aj = 0."
"D
DX",0.8956743002544529,"minfC(x, 0) = x · fC(−x) = x ·
 1"
"D
DX",0.8982188295165394,π arctan(−βx) + 1 2
"D
DX",0.9007633587786259,"
(28)"
"D
DX",0.9033078880407125,"To show monotonicity, we consider its derivative.
∂
∂xminfC(0, x) = ∂"
"D
DX",0.905852417302799,∂x(fC(−x) · x) = x · ∂
"D
DX",0.9083969465648855,∂xfC(−x) + fC(−x) · ∂ ∂xx
"D
DX",0.910941475826972,= x · ∂ ∂x  1
"D
DX",0.9134860050890585,"π arctan(−βx) + 1 2 
+ 1"
"D
DX",0.916030534351145,π arctan(−βx) + 1 2
"D
DX",0.9185750636132316,= x · 1
"D
DX",0.9211195928753181,"π
−β
1 + (βx)2 −1"
"D
DX",0.9236641221374046,π arctan(βx) + 1 2 = 1 2 −1
"D
DX",0.926208651399491,π arctan(βx) −1
"D
DX",0.9287531806615776,"π
βx
1 + (βx)2 = 1 2 −1"
"D
DX",0.9312977099236641,π arctan(z) −1
"D
DX",0.9338422391857506,"π
z
1 + z2
(with z = βx)
(29)"
"D
DX",0.9363867684478372,"To reason about the derivative, we also consider the second derivative:"
"D
DX",0.9389312977099237,"lim
z→∞
1
2 −1"
"D
DX",0.9414758269720102,π arctan(z) −1
"D
DX",0.9440203562340967,"π
z
1 + z2 = 1"
"D
DX",0.9465648854961832,"2 −lim
z→∞
1
π arctan(z) −lim
z→∞
1
π
z
1 + z2 = 1 2 −1 π
π"
"D
DX",0.9491094147582697,"2 −0 = 0
(30)"
"D
DX",0.9516539440203562,"lim
z→−∞
1
2 −1"
"D
DX",0.9541984732824428,π arctan(z) −1
"D
DX",0.9567430025445293,"π
z
1 + z2 = 1"
"D
DX",0.9592875318066157,"2 −
lim
z→−∞
1
π arctan(z) −
lim
z→−∞
1
π
z
1 + z2 = 1 2 −1 π
−π"
"D
DX",0.9618320610687023,"2 −0 = 1 2 + 1 π
π"
"D
DX",0.9643765903307888,"2 −0 = 1
(31)"
"D
DX",0.9669211195928753,"For z ∈(−∞, 0]: The derivative of minfC(0, x) converges to 1 for z →−∞(Eq. 31)."
"D
DX",0.9694656488549618,"For z ∈[0, ∞): The derivative of minfC(0, x) converges to 0 for z →∞(Eq. 30)."
"D
DX",0.9720101781170484,"∂
∂z
1
2 −1"
"D
DX",0.9745547073791349,π arctan(z) −1
"D
DX",0.9770992366412213,"π
z
1 + z2 = −
2
π(1 + z2)2 < 0
(32)"
"D
DX",0.9796437659033079,"The second derivative (Eq. 32) of minfC(0, x) is always negative."
"D
DX",0.9821882951653944,"Therefore, the derivative is always in (0, 1), and therefore always positive. Thus, minfC(0, x) is
strictly monotonic. maxfC is analogous."
"D
DX",0.9847328244274809,Published as a conference paper at ICLR 2022
"D
DX",0.9872773536895675,"E
ADDITIONAL DISCUSSION"
"D
DX",0.989821882951654,"“How would the following baseline perform? Hard rank the predictions and compare it with the ground
truth rank. Then, use their difference as the learning signal (i.e., instead of the gradient).”"
"D
DX",0.9923664122137404,"This kind of supervision does not converge, even for small learning rates and in simpliﬁed settings.
Speciﬁcally, we observed in our experiments that the range of values produced by the CNN
gets compressed heavily by training in this fashion. Also counteracting it by explicitly adding a
term to spread it out again did not help, and training was very unstable. Despite testing various
hyperparameters (learning rate, adaptation factor, both absolute and relative to the range of values in
a batch or in the whole data set, spread factor, etc.) it did not work, even on toy data like single-digit
MNIST with n = 5."
"D
DX",0.9949109414758269,“Could β be jointly trained as a parameter with the model?”
"D
DX",0.9974554707379135,"Yes, it could; however, in our experiments, we found that the entire training performs better if
β is ﬁxed. If β is also a parameter to be trained, its learning rate should be very small as it (i)
should not change too fast and (ii) already accumulates many gradient signals as it is used many times."
