Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004672897196261682,"The Bayes-Adaptive Markov Decision Process (BAMDP) formalism pursues the
Bayes-optimal solution to the exploration-exploitation trade-off in reinforcement
learning. As the computation of exact solutions to Bayesian reinforcement-learning
problems is intractable, much of the literature has focused on developing suitable
approximation algorithms. In this work, before diving into algorithm design, we
first define, under mild structural assumptions, a complexity measure for BAMDP
planning. As efficient exploration in BAMDPs hinges upon the judicious acquisi-
tion of information, our complexity measure highlights the worst-case difficulty
of gathering information and exhausting epistemic uncertainty. To illustrate its
significance, we establish a computationally-intractable, exact planning algorithm
that takes advantage of this measure to show more efficient planning. We then con-
clude by introducing a specific form of state abstraction with the potential to reduce
BAMDP complexity and gives rise to a computationally-tractable, approximate
planning algorithm."
INTRODUCTION,0.009345794392523364,"1
Introduction"
INTRODUCTION,0.014018691588785047,"The Bayes-Adaptive Markov Decision Process (BAMDP) [Duff, 2002] is a classic formalism encap-
sulating the optimal treatment of the exploration-exploitation trade-off by a reinforcement-learning
agent with respect to prior beliefs over an uncertain environment. Unfortunately, the standard formu-
lation suffers from an intractably-large hyperstate space (that is, the joint collection of environment
states coupled with the agent’s current state of knowledge over the unknown environment) and much
of the literature has been dedicated to identifying suitable approximations [Bellman and Kalaba,
1959, Dayan and Sejnowski, 1996, Duff and Barto, 1997, Dearden et al., 1998, Strens, 2000, Duff,"
INTRODUCTION,0.018691588785046728,"2001, 2003b,a, Wang et al., 2005, Poupart et al., 2006, Castro and Precup, 2007, Kolter and Ng,
2009, Asmuth et al., 2009, Dimitrakakis, 2009, Sorg et al., 2010, Araya-López et al., 2012, Guez
et al., 2012, 2013, 2014, Ghavamzadeh et al., 2015, Zintgraf et al., 2019]. In this work, we take steps
toward clarifying the hardness of BAMDPs before outlining an algorithmic concept that may help
mitigate problem difficulty and facilitate near-optimal solutions."
INTRODUCTION,0.02336448598130841,"First, we introduce the notion of information horizon as a complexity measure on BAMDP planning,
characterizing when it is truly difficult to identify the underlying uncertain environment. Naturally, the
agent’s state of knowledge at each timestep (a component of the overall BAMDP hyperstate) reflects
its current epistemic uncertainty and, as the agent accumulates data, this posterior concentrates,
exhausting uncertainty and identifying the true environment; after this point, the Bayes-optimal
policy naturally coincides with the optimal policy of the underlying Markov Decision Process (MDP).
Simply put, the information horizon quantifies the worst-case number of timesteps needed for the
agent to reach this point whereupon there is no more information to be gathered about the uncertain
environment."
INTRODUCTION,0.028037383177570093,"With this complexity measure in hand, we then entertain the idea of epistemic state abstraction
as an effective algorithmic tool for trading off between reduced information horizon (complexity)
and near-Bayes-optimality of the corresponding planning solution. Intuitively, as the total number
of knowledge states an agent may take on drives the intractable size of the hyperstate space, we
operationalize state abstraction [Li et al., 2006, Abel et al., 2016] to perform a lossy compression
of the epistemic state space, inducing a “smaller” and more tractable BAMDP for planning; our
results not only mirror those of analogous work on state aggregation for improved efficiency in
traditional MDP planning [Van Roy, 2006] but also parallel similar findings [Hsu et al., 2007, Zhang
et al., 2012] on the effectiveness of belief state aggregation in partially-observable MDP (POMDP)
planning [Kaelbling et al., 1998]."
INTRODUCTION,0.03271028037383177,"On the whole, our work provides one possible answer to a question that has already been asked
and answered several times in the context of MDPs [Bartlett and Tewari, 2009, Jaksch et al., 2010,
Farahmand, 2011, Maillard et al., 2014, Bellemare et al., 2016, Arumugam et al., 2021, Abel et al.,
2021]: how hard is my BAMDP? While the remainder of the paper goes on to examine how one
particular mechanism for reducing this complexity can translate into a more efficient planning
algorithm, we anticipate that this work can serve as a starting point for building a broader taxonomy
of BAMDPs, paralleling existing structural classes of MDPs [Jiang et al., 2017, Sun et al., 2019,
Agarwal et al., 2020, Jin et al., 2021]."
LIT REVIEW,0.037383177570093455,"2
Problem Formulation"
LIT REVIEW,0.04205607476635514,"In this section, we formally define BAMDPs as studied in this paper. As a point of contrast,
we begin by presenting the standard MDP formalism used throughout the reinforcement-learning
literature [Sutton and Barto, 1998]. We use ∆(X) to denote the set of all probability distributions
with support on an arbitrary set X and denote, for any natural number N ∈N, the index set as
[N] = {1, 2, . . . , N}. For any two arbitrary sets X and Y, we denote the class of all functions
mapping from X to Y as {X →Y} ≜{f | f : X →Y}."
LIT REVIEW,0.04672897196261682,"2.1
Markov Decision Processes"
LIT REVIEW,0.0514018691588785,"We begin with a sequential decision-making problem represented via the traditional finite-horizon
Markov Decision Process (MDP) [Bellman, 1957, Puterman, 1994] ⟨S, A, R, T , β, H⟩where S is a
finite set of states, A is a finite set of actions, R : S×A →[0, 1] is a deterministic reward function, T :
S×A →∆(S) is a transition function prescribing next-state transition distributions for all state-action
pairs, β ∈∆(S) is an initial state distribution, and H ∈N is the horizon denoting the agent’s total
number of steps or interactions with the environment. An agent’s sequential interaction within this
environment proceeds in each timestep h ∈[H], starting with an initial state s1 ∼β(·), by observing
the current state sh ∈S, selecting an action ah ∈A, and then enjoying a reward R(sh, ah) as the
environment transitions to sh+1 ∼T (· | sh, ah). Action selections made by the agent are governed by
its non-stationary policy π: a collection of H stationary, deterministic policies π = (π1, π2, . . . , πH),
where ∀h ∈[H], πh : S →A. We quantify the performance of policy π at timestep h ∈[H] by its
induced value function V π
h : S →R denoting the expected sum of future rewards by deploying policy"
LIT REVIEW,0.056074766355140186,"π from a particular state s ∈S: V π
h (s) = E
 H
P"
LIT REVIEW,0.06074766355140187,"h′=h
R(sh′, ah′) | sh = s

, where the expectation"
LIT REVIEW,0.06542056074766354,"integrates over randomness in the environment transitions. Analogously, we define the action-value
function induced by policy π at timestep h as Qπ
h : S × A →R which denotes the expected
future sum of rewards by being in a particular state s ∈S, executing a particular action a ∈A,"
LIT REVIEW,0.07009345794392523,"and then following policy π thereafter: Qπ
h(s, a) = E
 H
P"
LIT REVIEW,0.07476635514018691,"h′=h
R(sh′, ah′) | sh = s, ah = a

. We are"
LIT REVIEW,0.0794392523364486,"guaranteed the existence of an optimal policy π⋆that achieves supremal value V ⋆
h (s) = sup
π∈ΠH V π
h (s)"
LIT REVIEW,0.08411214953271028,"for all s ∈S, h ∈[H] where the policy class contains all deterministic policies Π = {π | π : S →A}.
Since rewards are bounded in [0, 1], we have that 0 ≤V π
h (s) ≤V ⋆
h (s) ≤H −h + 1 for all
s ∈S, h ∈[H], and π. These value functions obey the Bellman equation and the Bellman optimality
equation, respectively:"
LIT REVIEW,0.08878504672897196,"V π
h (s) = Qπ
h(s, πh(s))
V ⋆
h (s) = max
a∈A Q⋆
h(s, a),
V π
H+1(s) = 0
V ⋆
H+1(s) = 0
∀s ∈S,"
LIT REVIEW,0.09345794392523364,"Qπ
h(s, a) = R(s, a)+Es′∼T (·|s,a)

V π
h+1(s′)

,
Q⋆
h(s, a) = R(s, a)+Es′∼T (·|s,a)

V ⋆
h+1(s′)

."
LIT REVIEW,0.09813084112149532,"2.2
Bayes-Adaptive Markov Decision Processes"
LIT REVIEW,0.102803738317757,"The BAMDP formalism offers a Bayesian treatment of an agent interacting with an uncertain MDP.
More specifically, a decision-making agent is faced with a MDP M = ⟨S, A, R, Tθ, β, H⟩defined
around an unknown transition function Tθ1, for some latent parameter θ ∈Θ. Prior uncertainty in
θ is reflected by the distribution p(θ). In classic work on BAMDPs with finite state-action spaces,
the parameters θ denote visitation counts and p(θ) is a Dirichlet distribution, so as to leverage the
convenience of Dirichlet-multinomial conjugacy for exact posterior updates [Duff, 2002, Poupart
et al., 2006]. For our purposes, we will assume an alternative parameterization whose importance
will be made clear later when defining our complexity measure.
Assumption 1. We assume that Θ is known and |Θ| < ∞such that an agent is only ever reasoning
about its uncertainty over a finite set of |Θ| known MDPs. We further make a realizability assumption
that the true parameters reside in this finite set, θ ∈Θ."
LIT REVIEW,0.10747663551401869,"Under Assumption 1, an agent’s prior uncertainty in Tθ is reflected by the distribution p(θ) ∈∆(Θ)
which, with each step of experience encountered by the agent, may be updated via Bayes’ rule to
recover a corresponding posterior distribution in light of observed data from the environment. For
simplicity, we do not concern ourselves with the computation of the posterior and instead assume
access to a deterministic B : ∆(Θ) × S × A × S →∆(Θ) that performs an exact posterior update to
any input distribution p ∈∆(Θ) based on the experience tuple (s, a, s′) ∈S × A × S in O(1) time."
LIT REVIEW,0.11214953271028037,"The corresponding BAMDP for M is defined around a so-called hyperstate space X = S × ∆(Θ)
such that any hyperstate x = ⟨s, p⟩∈X denotes the agent’s original or physical state s ∈S
within the true MDP while p ∈∆(Θ) denotes the agent’s information state or epistemic state [Lu
et al., 2021] about the uncertain environment; intuitively, the epistemic state represents the agent’s
knowledge of the environment based on all previously observed data. This gives rise to the BAMDP
⟨X, A, R, T , β, H⟩where A is the same action set as the original MDP M, R : X × A →[0, 1] is
the same reward function as in M (that is, R(⟨s, p⟩, a) = R(s, a) ∀⟨s, p⟩∈X, a ∈A), β ∈∆(X)
is defined as β = β × δp(θ) where δp(θ) denotes a Dirac delta centered around the agent’s prior p(θ),
and H is the same horizon as MDP M. Due to the determinism of the posterior updates given by B,
the BAMDP transition function T : X × A →∆(X) is defined as"
LIT REVIEW,0.11682242990654206,"T (x′ | x, a) =
X"
LIT REVIEW,0.12149532710280374,"θ∈Θ
Tθ(s′ | s, a)p(θ)1 (p′ = B(p, s, a, s′)) ,"
LIT REVIEW,0.1261682242990654,"where x′ = ⟨s′, p′⟩∈X. The associated BAMDP policy π = (π1, π2, . . . , πH), πh : X →A, ∀h ∈
[H] selects actions based on the current state of the MDP as well as the agents accumulated knowledge
of the environment thus far. With these components, we may define the associated BAMDP value
functions with x′ = ⟨s′, B(p, s, a, s′)⟩:"
LIT REVIEW,0.1308411214953271,"V π
h (x) = Qπ
h(x, πh(x))
V ⋆
h (x) = max
a∈A Q⋆
h(x, a),
V π
H+1(x) = 0
V ⋆
H+1(x) = 0
∀x ∈X,"
LIT REVIEW,0.13551401869158877,"Qπ
h(x, a) = R(s, a)+
X"
LIT REVIEW,0.14018691588785046,"s′,θ
Tθ(s′ | s, a)p(θ)V π
h+1(x′),
Q⋆
h(x, a) = R(s, a)+
X"
LIT REVIEW,0.14485981308411214,"s′,θ
Tθ(s′ | s, a)p(θ)V ⋆
h+1(x′)."
LIT REVIEW,0.14953271028037382,"Based on these optimality equations, we see that the optimal policy of a BAMDP achieving supremal
value V ⋆
h across all timesteps h ∈[H] is the Bayes-optimal policy which appropriately balances the
exploration-exploitation trade-off in reinforcement learning. An observation is that this Bayes-optimal
policy will tend to achieve lower value than the optimal policy of MDP M as the agent takes more
informative (possibly sub-optimal) actions to identify the true underlying environment."
LIT REVIEW,0.1542056074766355,"3
Related Work"
LIT REVIEW,0.1588785046728972,"Bellman and Kalaba [1959] offer the earliest formulation of Bayesian reinforcement learning, whereby
the individual actions of a decision-making agent not only provide an update to the physical state"
LIT REVIEW,0.16355140186915887,"1For ease of exposition, we focus on uncertainty in transition dynamics although one could model uncertainty
in either the reward function or the full MDP model (rewards & transitions)."
LIT REVIEW,0.16822429906542055,"of the world but also impact the agent’s internal model of how the world operates. Dayan and
Sejnowski [1996] follow this line of thinking to derive implicit exploration bonuses based on how an
agent performs posterior updates. Kolter and Ng [2009] make this more explicit and incorporate a
specific visitation-based bonus that decays with the concentration of the agent’s Dirichlet posterior.
As an alternative, Sorg et al. [2010] incorporate an exploration bonus based on the variance of
the agent’s posterior while Araya-López et al. [2012] achieve optimistic exploration by boosting
transition probabilities. Duff and Barto [1997] identify multi-armed bandits (that is, MDPs with
exactly one state and arbitrarily many actions) as a unique setting where the Bayes-optimal solution
is computationally tractable through the use of Gittins indices [Gittins, 1979]. While the vast space
of more complicated BAMDPs are computationally intractable, a goal of this paper is to add a bit of
nuance and clarify when one might still hope to recover efficient, approximate planning. This is also
distinct from the PAC-BAMDP framework introduced by Kolter and Ng [2009], which serves as a
characterization of algorithmic efficiency, rather than problem hardness."
LIT REVIEW,0.17289719626168223,"Representing uncertainty in the optimal value function rather than environment transition function,
Dearden et al. [1998] derive a practical Bayesian Q-learning algorithm by foregoing representation
of the epistemic state and instead resampling Q⋆-values at each timestep. Strens [2000] finds an
alternate, tractable solution by lazily updating the epistemic state at the frequency of whole episodes,
rather than individual timesteps; a long line of work [Agrawal and Jia, 2017, Osband et al., 2016a,b,
Osband and Van Roy, 2017, O’Donoghue et al., 2018, Osband et al., 2019] analyzes this type of
approximation to the Bayesian reinforcement-learning problem theoretically and also explores how
to scale these solution concepts with deep neural networks."
LIT REVIEW,0.17757009345794392,"Duff [2001] finds tractability in representing policies as finite-state stochastic automata, noting
structural similarities between BAMDPs and partially-observable MDPs (POMDPs) [Kaelbling
et al., 1998]; this type of thinking is further extended by Poupart et al. [2006] who exploit similar
structure between the optimal value functions of BAMDPs and POMDPs. Duff [2003a] examine
improved memory requirements when applying actor-critic algorithms [Konda and Tsitsiklis, 2000]
to BAMDPs while Duff [2003b] consider how to approximately model the stochastic process of
the evolving epistemic state via diffusion models. Wang et al. [2005] introduce a sparse-sampling
approach [Kearns et al., 2002] for balancing computational efficiency against fidelity to Bayes-optimal
action selection. An analogous sparse-sampling approach is also developed by Castro and Precup
[2007], but with a linear-programming methodology for value-function approximation. A line of
work [Guez et al., 2012, 2013, 2014] develops more scalable, sparse-sampling lookahead approaches
on the back of Monte-Carlo tree search [Kocsis and Szepesvári, 2006]; these algorithms are somewhat
similar in spirit to the approach of Asmuth et al. [2009] who merge multiple posterior samples into
a single model while Guez et al. [2014] keep each sample distinct and integrate out the posterior
randomness. For a more complete and detailed survey of Bayesian reinforcement learning, we refer
readers to Ghavamzadeh et al. [2015]. Crucially, the aforementioned approaches largely revolve
around ignoring the epistemic state, lazily updating the epistemic state, or approximating the impact of
the epistemic state via random sampling. In contrast, this work offers a new approach and highlights
how lossy compression of the epistemic state may naturally reduce BAMDP hardness. Perhaps
the most related prior work is by Lee et al. [2018] who introduce a practical approximate-planning
approach by quantizing the epistemic state space; this paper clarifies the theoretical ramifications of
this quantization step."
LIT REVIEW,0.1822429906542056,"Our work is also connected to analyses of approximate value iteration [Bellman, 1957] in the MDP
setting [Tseng, 1990, Littman et al., 1995], where more recent work has managed to recover improved
sample complexity bounds for approximate value iteration [Sidford et al., 2018b,a]. Like Kearns
and Singh [1999], our algorithms utilize exact value iteration almost as a black box and it is an open
question for future work to see if similar ideas and proof techniques for these approximate variants
might be leveraged in the BAMDP setting. Crucially, the variants of value iteration introduced in this
work are merely a backdrop for illustrating the utility of our complexity measure and, more generally,
a regard for underlying information structure in BAMDPs."
LIT REVIEW,0.18691588785046728,"The particular class of epistemic state abstraction introduced and studied in this work revolves around
the covering number of the epistemic state space. Curiously, this deepens an existing connection
between BAMDPs and POMDPs [Duff, 2001], where a line of work establishes the covering number
of the belief state space as a viable complexity measure for the latter both in theory [Hsu et al., 2007]
and in practice [Zhang et al., 2012]. In a less related but similar vein, Kakade et al. [2003a] establish
a provably-efficient reinforcement-learning algorithm when the MDP state space is a metric space;"
LIT REVIEW,0.19158878504672897,"their corresponding sample complexity guarantee depends on the covering number of the state space
under the associated metric. Our planning complexity result for abstract BAMDPs mirrors those
established by these works in its dependence on the covering number of the epistemic state space."
IMPLEMENTATION/METHODS,0.19626168224299065,"4
The Complexity of BAMDP Planning"
IMPLEMENTATION/METHODS,0.20093457943925233,"In this section, we examine the difficulty of solving BAMDPs through the lens of a classic planning
algorithm: value iteration [Bellman, 1957]. Due to space constraints, we relegate pseudocode for
all discussed algorithms to Appendix A. We begin with an quick review of the traditional algorithm
applied to our setting before introducing the information horizon as a complexity measure for
BAMDPs. This quantity gives rise to a more efficient planning algorithm for BAMDPs that waives
excessive dependence on the original problem horizon. In order to facilitate an analysis of planning
complexity in BAMDPs via value iteration, we require a finite hyperstate space X. For now, we will
assume that X is finite, but still considerably large, by virtue of an aggressively-fine quantization of
the (|Θ| −1)-dimensional simplex, also considered in the empirical work of Lee et al. [2018]:"
IMPLEMENTATION/METHODS,0.205607476635514,"Assumption 2. We assume the existence of a suitable, fixed quantization of simplex b∆(Θ) ⊂∆(Θ)
where |b∆(Θ)| < ∞such that the BAMDP hyperstate space X = S × b∆(Θ) is finite, |X| < ∞."
IMPLEMENTATION/METHODS,0.2102803738317757,"4.1
Naive Value Iteration"
IMPLEMENTATION/METHODS,0.21495327102803738,"To help build intuitions, we begin by presenting a typical version of value iteration for finite-horizon
BAMDPs as Algorithm 1. This algorithm iterates backwards through the H timesteps, computing
Q⋆
h across every hyperstate-action pair. With the provision of our posterior update oracle B, we
avoid a square dependence on the hyperstate space (|X|2) and instead only require O(|S||Θ|)
to compute next-state value. Consequently, the resulting planning complexity of Algorithm 1 is
O(|X||A||S||Θ|H). Clearly, this represents an onerous burden for two distinct reasons: (1) we are
forced to contend with a potentially very large horizon H and (2) we must also search through the
entirety of the hyperstate space, X. In the sections that follow, we alleviate the burdens of challenges
(1) and (2) in series, using our new notion of information horizon to mitigate the impact of H and
leveraging epistemic state abstraction to further reduce the role of |X|, where the latter occurs at the
cost of introducing approximation error."
IMPLEMENTATION/METHODS,0.21962616822429906,"4.2
Information Horizon"
IMPLEMENTATION/METHODS,0.22429906542056074,"As noted in the previous section, our planning complexity suffers from its dependence on the BAMDP
horizon H. A key observation, however, is that once an agent has completely resolved its uncertainty
and identified one of the |Θ| environments, all that remains is to deploy the optimal policy for that
particular MDP. As an exaggerated but illustrative example of this, consider a BAMDP where any
action executed at the first timestep completely identifies the true environment θ ∈Θ. With no
residual epistemic uncertainty left, the Bayes-optimal policy would now completely coincide with
the optimal policy and take actions without changing the epistemic state since, at this point, the agent
has acquired all the requisite information about the previously unknown environment. Even if the
problem horizon H is substantially large, a simple BAMDP like the one described should be fairly
easy to solve as epistemic uncertainty is so easily diminished and information is quickly exhausted; it
is this principle that underlies our hardness measure."
IMPLEMENTATION/METHODS,0.22897196261682243,"Let π be an arbitrary non-stationary policy. For any hyperstate x ∈X, we denote by Pπ(xh = x) the
probability that policy π visits hyperstate x at timestep h. With this, we may define the reachable
hyperstate space of policy π at timestep h ∈[H] as X π
h = {x ∈X | Pπ(xh = x) > 0} ⊂X. In
words, the reachable hyperstate space of a policy π at a particular timestep is simply the set of all
possible hyperstates that may be reached by π at that timestep with non-zero probability. Recall
that for any hyperstate x = ⟨s, p⟩∈X, the epistemic state p ∈∆(Θ) is a (discrete) probability
distribution, for which we may denote its corresponding entropy as H(p). Given a BAMDP, we define
the information horizon of a policy π as I(π) = inf{h ∈[H] | ∀xh = ⟨sh, ph⟩∈X π
h , H(ph) = 0}.
The information horizon of a policy, if it exists, identifies the first timestep in [H] where, regardless
of precisely which hyperstate is reached by following π at this timestep, the agent has fully resolved
all of its epistemic uncertainty over the environment θ. At this point, we call attention back to our
structural Assumption 1 for BAMDPs and note that, under the standard parameterization of epistemic"
IMPLEMENTATION/METHODS,0.2336448598130841,"state via count parameters for Dirichlet priors/posteriors, we would only be able to assess residual
epistemic uncertainty through differential entropy which, unlike the traditional (Shannon) entropy
H(·), is potentially negative and has no constant lower bound [Cover and Thomas, 2012].2 Naturally,
to compute the information horizon of the BAMDP, we need only take the supremum across the
non-stationary policy class: I = sup
π∈ΠH I(π), where Π = {X →A}."
IMPLEMENTATION/METHODS,0.2383177570093458,"Clearly, when it exists, we have that 1 ≤I ≤H; the case where I = 1 corresponds to having a
prior p(θ) that is itself a Dirac delta δθ centered around the true environment, in which case, θ is
known completely and the agent may simply compute and deploy the optimal policy for the MDP
⟨S, A, R, Tθ, β, H⟩. At the other end of the spectrum, an information horizon I = H suggests that,
in the worst case, an agent may need all H steps of behavior in order to fully identify the environment.
In the event that there exists any single non-stationary policy π for which the infimum of I(π) does
not exist (that is, I(π) = ∞), then clearly I = ∞; this represents the most difficult, worst-case
scenario wherein an agent may not always capable of fully resolving its epistemic uncertainty within
the specified problem horizon H. For certain scenarios, the supremum taken over the entire non-
stationary policy class may be exceedingly strict and, certainly, creates a computational intractability
should one wish to operationalize the information horizon algorithmically; in these situations, it
may be more natural to consider smaller or regularized policy classes (for instance, the collection of
expressible policies under a chosen neural network architecture) that yield more actionable notions
of BAMDP complexity. We now go on to show how the information horizon can be used to design
a more efficient BAMDP planning algorithm whose planning complexity bears a more favorable
dependence on H."
IMPLEMENTATION/METHODS,0.24299065420560748,"4.3
Informed Value Iteration"
IMPLEMENTATION/METHODS,0.24766355140186916,"The key insight from the previous section is that, when the information horizon exists and once an
agent has acted for I timesteps, the Bayes-optimal policy necessarily falls back to the optimal policy
associated with the true environment. Consequently, if the solutions to all |Θ| possible underlying
MDPs are computed up front, an agent can simply backup their optimal values starting from the Ith
timestep, rather than backing up values beginning at the original horizon H. This high-level idea is
implemented as Algorithm 2 which assumes access to a sub-routine mdp_value_iteration that
consumes a MDP and produces the associated optimal value function for the initial timestep, V ⋆
1 ."
IMPLEMENTATION/METHODS,0.2523364485981308,"Since the underlying unknown MDP is one of |Θ| possible MDPs, Algorithm 2 proceeds by first
computing the optimal value function associated with each of them in sequence using standard value
iteration, incurring a time complexity of O(|Θ||S|2|A|(H −I)). Note that the horizon of each MDP
is reduced to H −I acknowledging that, after identifying the true MDP in I steps, an agent has only
H −I steps of interaction remaining with the environment. With these |Θ| solutions in hand, the
remainder of the algorithm proceeds with standard value iteration for BAMDPs (as in Algorithm 1),
only now bootstrapping value from the I timestep, rather than the original problem horizon H. Note
that in Line 9, we could also compute the corresponding bθ in question by taking the mean of the next
epistemic state p′, however, we use this calculation to make explicit the fact that, by definition of the
information horizon, the agent has no uncertainty in θ at this point. As a result, instead of planning
complexity that scales the hyperstate space size by a potentially large problem horizon, we incur a
complexity of O (|Θ||S||A| (|X|I + |S|(H −I))). Naturally, as the gap between the information
horizon I and problem horizon H increases, the more favorably Algorithm 2 performs relative to the
standard value iteration procedure of Algorithm 1."
IMPLEMENTATION/METHODS,0.2570093457943925,"In this section, we’ve demonstrated how the information horizon of a BAMDP has the potential to
dramatically reduce the computational complexity of planning. Still, however, the corresponding
guarantee bears an unfavorable dependence on the size of the hyperstate space X which, in the reality
that voids Assumption 2, still renders both Algorithms 1 and 2 as computationally intractable. Since
this is likely inescapable for the problem of computing the exact optimal BAMDP value function, the
next section considers one path for reducing this burden at the cost of only being able to realize an
approximately-optimal value function."
IMPLEMENTATION/METHODS,0.2616822429906542,"2Prior work (see, for example, Theorem 1 of Kolter and Ng [2009]) operating with the Dirichlet parameteri-
zation will make an alternative assumption for similar effect where epistemic state updates cease after a certain
number of state-action pair visitations."
IMPLEMENTATION/METHODS,0.26635514018691586,"5
Epistemic State Abstraction"
IMPLEMENTATION/METHODS,0.27102803738317754,"5.1
State Abstraction in MDPs"
IMPLEMENTATION/METHODS,0.2757009345794392,"As numerous sample-efficiency guarantees in reinforcement learning [Kearns and Singh, 2002,
Kakade et al., 2003b, Strehl et al., 2009] bear a dependence on the size of the MDP state space, |S|, a
large body of work has entertained state abstraction as a tool for improving the dependence on state
space size without compromising performance [Whitt, 1978, Bertsekas et al., 1988, Singh et al., 1995,
Gordon, 1995, Tsitsiklis and Van Roy, 1996, Dean and Givan, 1997, Ferns et al., 2004, Jong and
Stone, 2005, Li et al., 2006, Van Roy, 2006, Ferns et al., 2012, Jiang et al., 2015a, Abel et al., 2016,
2018, 2019, Dong et al., 2019, Du et al., 2019, Misra et al., 2020, Abel, 2020]. Broadly speaking,
a state abstraction ϕ : S →Sϕ maps original or ground states of the MDP into abstract states in
Sϕ. Typically, one takes ϕ to be defined with respect to an abstract state space Sϕ with smaller
complexity (in some sense) than S; in the case of state aggregation where all spaces in question are
finite, this desideratum often takes the very simple form of |Sϕ| < |S|. Various works have identified
conditions under which specific classes of state abstractions ϕ yield no approximation error and
perfectly preserve the optimal policy of the original MDP [Li et al., 2006], as well as conditions under
which near-optimal behavior is preserved [Van Roy, 2006, Abel et al., 2016]. As its name suggests,
our proposed notion of epistemic state abstraction aims to lift these kinds of guarantees for MDPs
over to BAMDPs and contend with the intractably large hyperstate space."
IMPLEMENTATION/METHODS,0.2803738317757009,"Before examining BAMDPs, we provide a brief overview of how state abstraction impacts the
traditional MDP, as a point of comparison with the BAMDP setting. Given a MDP ⟨S, A, R, T , β, H⟩,
a state abstraction ϕ : S →Sϕ induces a new abstract MDP Mϕ = ⟨Sϕ, A, Rϕ, Tϕ, H⟩where the
abstract reward function Rϕ : Sϕ × A →[0, 1] and transition function Tϕ : Sϕ × A →∆(Sϕ) are
both defined with respect to a fixed, arbitrary weighting function ω : S →[0, 1] that, intuitively,
measures the contribution of each individual MDP state s ∈S to its allocated abstract state ϕ(s).
More specifically, ω is required to induce a probability distribution on the constituent MDP states
of each abstract state: ∀sϕ ∈Sϕ,
P"
IMPLEMENTATION/METHODS,0.2850467289719626,"s∈ϕ−1(sϕ)
ω(s) = 1. This fact allows for well-defined rewards and"
IMPLEMENTATION/METHODS,0.2897196261682243,transition probabilities as given by
IMPLEMENTATION/METHODS,0.29439252336448596,"Rϕ(sϕ, a) =
X"
IMPLEMENTATION/METHODS,0.29906542056074764,"s∈ϕ−1(sϕ)
R(s, a)ω(s),
Tϕ(s′
ϕ | sϕ, a) =
X"
IMPLEMENTATION/METHODS,0.3037383177570093,s∈ϕ−1(sϕ) X
IMPLEMENTATION/METHODS,0.308411214953271,"s′∈ϕ−1(s′
ϕ)
T (s′ | s, a)ω(s)."
IMPLEMENTATION/METHODS,0.3130841121495327,"As studied by Van Roy [2006], the weighting function ω does bear implications on the efficiency of
learning and planning. Naturally, one may go on to apply various planning or reinforcement-learning
algorithms to Mϕ and induce behavior in the original MDP M by first applying ϕ to the current state
s ∈S and then leveraging the optimal abstract policy or abstract value function of Mϕ. Conditions
under which ϕ will induce a MDP Mϕ that preserves optimal or near-optimal behavior are studied
by Li et al. [2006], Van Roy [2006], Abel et al. [2016]."
IMPLEMENTATION/METHODS,0.3177570093457944,"5.2
Compressing the Epistemic State Space"
IMPLEMENTATION/METHODS,0.32242990654205606,"In this section, we introduce epistemic state abstraction for BAMDPs with the goal of paralleling
the benefits of state abstraction in MDPs. In particular, we leverage the fact that our epistemic state
space ∆(Θ) = ∆|Θ|−1 is the (|Θ| −1)-dimensional probability simplex. Recall that for any set
Z; any threshold parameter δ > 0; and any metric ρ : Z × Z →R+ on Z, a set {z1, z2, . . . , zK}
is a δ-cover of Z if ∀z ∈Z, ∃i ∈[K] such that ρ(z, zi) ≤δ. In this work, we will consider
δ-covers with arbitrary parameter δ > 0 defined on the simplex ∆|Θ|−1 with respect to the total
variation distance metric on probability distributions, denoted || · ||TV. Let ei ∈∆(Θ) be the ith
standard basis vector such that H(ei) = 0, ∀i ∈[|Θ|]. We define an epistemic state abstraction
with parameter δ > 0 as the projection from ∆(Θ) onto the smallest δ-cover of ∆(Θ) with respect
to || · ||TV that contains all standard basis vectors {e1, e2, . . . , e|Θ|}; paralleling notation for the
δ-covering number, we use N(∆(Θ), δ, || · ||TV) to denote the size of this minimal cover and, for
consistency with the state-abstraction literature in MDPs, use ϕ : ∆(Θ) →∆ϕ(Θ) to denote the
epistemic state abstraction. Briefly, we note that while computing exact δ-covers is a NP-hard
problem, approximation algorithms do exist [Hochbaum, 1996, Zhang et al., 2012]; our work here is
exclusively concerned with establishing theoretical guarantees that warrant further investigation of
such approximation techniques to help solve BAMDPs in practice."
IMPLEMENTATION/METHODS,0.32710280373831774,"It is important to note that while there are numerous statistical results expressed in terms of covering
numbers (for instance, Dudley’s Theorem [Dudley, 1967]), our definition of covering number differs
slightly in its inclusion of the standard basis vectors. The simple reason for this constraint is that it
ensures we may still count on the existence of abstract epistemic states for which an agent has fully
exhausted all epistemic uncertainty in the underlying environment. Consequently, we are guaranteed
that the information horizon is still a well-defined quantity under this abstraction3. As δ increases,
larger portions of the epistemic state space where the agent has residual, but still non-zero, epistemic
uncertainty will be immediately mapped to the nearest standard basis vector under ϕ. If such a
lossy compression is done too aggressively, the agent’s beliefs over the uncertain environment may
prematurely and erroneously converge. On the other hand, if done judiciously with a prudent setting
of δ, one has the potential to dramatically reduce the complexity of planning across a much smaller,
finite hyperstate space and recover an approximately-optimal BAMDP value function."
IMPLEMENTATION/METHODS,0.3317757009345794,"To make this intuition more precise, consider an initial BAMDP ⟨X, A, R, T , β, H⟩and, given an
epistemic state abstraction ϕ : ∆(Θ) →∆ϕ(Θ) with fixed parameter δ > 0, we recover an induced
abstract BAMDP ⟨Xϕ, A, Rϕ, T ϕ, βϕ, H⟩where, most importantly, Xϕ = S × ∆ϕ(Θ)4. Just as in
the MDP setting, the model of the abstract BAMDP depends on a fixed, arbitrary weighting function
of the original epistemic states ω : ∆(Θ) →[0, 1] that adheres to the constraint: ∀pϕ ∈∆ϕ(Θ),
R"
IMPLEMENTATION/METHODS,0.3364485981308411,"ϕ−1(pϕ) ω(p)dp = 1, which means abstract rewards and transition probabilities for a current and
next abstract hyperstates, xϕ = ⟨s, pϕ⟩and x′
ϕ = ⟨s′, p′
ϕ⟩, are given by"
IMPLEMENTATION/METHODS,0.3411214953271028,"Rϕ(xϕ, a) =
Z"
IMPLEMENTATION/METHODS,0.34579439252336447,ϕ−1(pϕ)
IMPLEMENTATION/METHODS,0.35046728971962615,"R(x, a)ω(p)dp =
Z"
IMPLEMENTATION/METHODS,0.35514018691588783,"ϕ−1(pϕ)
R(s, a)ω(p)dp = R(s, a)
Z"
IMPLEMENTATION/METHODS,0.3598130841121495,"ϕ−1(pϕ)
ω(p)dp = R(s, a),"
IMPLEMENTATION/METHODS,0.3644859813084112,"T ϕ(x′
ϕ | xϕ, a) =
Z"
IMPLEMENTATION/METHODS,0.3691588785046729,"ϕ−1(pϕ)
ω(p)
X"
IMPLEMENTATION/METHODS,0.37383177570093457,"p′∈ϕ−1(p′
ϕ)"
IMPLEMENTATION/METHODS,0.37850467289719625,"T (x′ | x, a)dp, where x = ⟨s, p⟩and x′ = ⟨s′, p′⟩."
IMPLEMENTATION/METHODS,0.38317757009345793,"The initial abstract hyperstate distribution is defined as βϕ = β × δϕ(p(θ)) where β ∈∆(S) denotes
the initial state distribution of the underlying MDP while δϕ(p(θ)) is a Dirac delta centered around the
agent’s original prior, p(θ), projected by ϕ into the abstract epistemic state space. Observe that the
abstract BAMDP transition function is stochastic with respect to the next abstract epistemic state p′
ϕ,
unlike the original BAMDP transition function whose next epistemic states are deterministic. This is,
perhaps, not a surprising observation as it also occurs in standard state aggregation of deterministic
MDPs as well. Nevertheless, it is important to note the corresponding abstract BAMDP value
functions must now acknowledge this stochasticity for any abstract policy π = (πϕ,1, πϕ,2, . . . , πϕ,H),
πϕ,h : Xϕ →A, ∀h ∈[H]:"
IMPLEMENTATION/METHODS,0.3878504672897196,"V π
ϕ,h(xϕ) = Qπ
ϕ,h(xϕ, πϕ,h(xϕ))
V ⋆
ϕ,h(xϕ) = max
a∈A Q⋆
ϕ,h(xϕ, a),
V π
ϕ,H+1(xϕ) = 0
V ⋆
ϕ,H+1(xϕ) = 0
∀xϕ ∈Xϕ,"
IMPLEMENTATION/METHODS,0.3925233644859813,"Qπ
ϕ,h(xϕ, a) = R(s, a)+
X"
IMPLEMENTATION/METHODS,0.397196261682243,"s′,p′
ϕ"
IMPLEMENTATION/METHODS,0.40186915887850466,"T ϕ(x′
ϕ | xϕ, a)V π
ϕ,h+1(x′
ϕ),
Q⋆
ϕ,h(xϕ, a) = R(s, a)+
X"
IMPLEMENTATION/METHODS,0.40654205607476634,"s′,p′
ϕ"
IMPLEMENTATION/METHODS,0.411214953271028,"T ϕ(x′
ϕ | xϕ, a)V ⋆
ϕ,h+1(x′
ϕ)."
IMPLEMENTATION/METHODS,0.4158878504672897,"Beyond the fact that this abstract BAMDP enjoys a reduced hyperstate space, we further observe
that the information horizon of this new BAMDP, Iϕ, has the potential to be smaller than that of
the original BAMDP. That is, if I steps are needed to fully resolve epistemic uncertainty in the
original BAMDP then, by compressing the epistemic state space via ϕ, we may find epistemic
uncertainty exhausted in fewer than I timesteps within the abstract BAMDP. Furthermore, for a
suitably large setting of the δ parameter, we also have cases where the original BAMDP has I = ∞
while Iϕ < ∞; in words, whereas it may not have been possible to resolve all epistemic uncertainty
within H timesteps, compression of the epistemic state space reduces this difficulty in the abstract
problem as knowledge states near (in the total-variation sense) each vertex of the probability simplex
ei are immediately aggregated. Due to space constraints, we defer further discussion of the abstract
information horizon and its relationship with the original information horizon to Appendix C."
IMPLEMENTATION/METHODS,0.4205607476635514,"3Note that an alternative would be to introduce an additional constant γ ∈R+ and define the information
horizon based on H(p) ≤γ; our construction avoids carrying this cumbersome additional parameter dependence
in the results.
4One could also imagine abstracting over the original MDP state space S which, for clarity, we do not
consider in this work."
IMPLEMENTATION/METHODS,0.4252336448598131,"As a toy illustration of last scenario, consider a ϕ with δ sufficiently large such that any step from
the agent’s prior distribution immediately maps to a next abstract hyperstate with no epistemic
uncertainty. Clearly, regardless of I, we have an abstract BAMDP where Iϕ = 2. Of course, under
such an aggressive abstraction, we should expect to garner an unfavorable degree of approximation
error between the solutions of the abstract and original BAMDPs. The next section makes this error
analysis and performance loss precise alongside an approximate planning algorithm that leverages the
reduced complexity of abstract BAMDPs to recover a near-optimal solution to the original BAMDP
of interest."
IMPLEMENTATION/METHODS,0.42990654205607476,"5.3
Informed Abstract Value Iteration"
IMPLEMENTATION/METHODS,0.43457943925233644,"Observe that if, after inducing the abstract BAMDP according to a given epistemic state abstraction
ϕ, the resulting information horizon is finite Iϕ < ∞, then we are in a position to run Algorithm 2
on the abstract BAMDP. Moreover, we no longer need the crutch of Assumption 2 as, by definition
of ϕ, we are guaranteed a finite abstract hyperstate space of size |Xϕ| = |S| · N(∆(Θ), δ, || · ||TV).
With the solution to the abstract BAMDP in hand, we can supply values to any input hyperstate of the
original BAMDP x = ⟨s, p⟩∈X by simply applying ϕ to the agent’s current epistemic state p and
querying the value of the resulting abstract hyperstate ⟨s, ϕ(p)⟩∈Xϕ. We present this approximate
BAMDP planning procedure as Algorithm 3."
IMPLEMENTATION/METHODS,0.4392523364485981,"By construction, this algorithm inherits the planning complexity guarantee of Algorithm 2, specialized
to the abstract BAMDP input, yielding O

|Θ||S|2|A|

N(∆(Θ), δ, || · ||TV)2Iϕ + (H −Iϕ)

. A
key feature of this result is that we entirely forego a (direct) dependence on the hyperstate space of
the original BAMDP and, instead, take on dependencies with the size of the abstract hyperstate space,
|Xϕ|2 = |S|2N(∆(Θ), δ, || · ||TV)2, and the abstract information horizon Iϕ. While both terms
decrease as δ →1, there is a delicate balance to be maintained between the ease with which one may
solve the abstract BAMDP and the quality of the resulting solution when deployed in the original
BAMDP of interest. We dedicate the remainder of this section to making this balance mathematically
precise. Due to space constraints, all proofs are relegated to Appendix B. A natural first step in our
analysis is to establish an approximation error bound:
Proposition 1. Let V ⋆
h and V ⋆
ϕ,h denote the optimal original and abstract BAMDP value functions,
respectively, for any timestep h ∈[H]. Let ϕ be an epistemic state abstraction as defined above.
Then, max
x∈X |V ⋆
h (x) −V ⋆
ϕ,h(ϕ(x))| ≤2δ(H −h)(H −h + 1)."
IMPLEMENTATION/METHODS,0.4439252336448598,"In order to establish a complimentary performance-loss bound, we require an intermediate result
characterizing performance shortfall of a BAMDP value function induced by a greedy policy with
respect to another near-optimal BAMDP value function. The analogue of this result for MDPs is
proven by Singh and Yee [1994], and the proof for BAMDPs follows similarly.
Proposition 2. Let V = {V1, V2, . . . , VH} be an arbitrary BAMDP value function. We denote by
πh,V the greedy policy with respect to V defined ∀x = ⟨s, p⟩∈X as"
IMPLEMENTATION/METHODS,0.4485981308411215,"πh,V (x) = arg max
a∈A "
IMPLEMENTATION/METHODS,0.4532710280373832,"R(x, a) +
X"
IMPLEMENTATION/METHODS,0.45794392523364486,"θ,s′
Tθ(s′ | s, a)p(θ)Vh+1(x′)  ,"
IMPLEMENTATION/METHODS,0.46261682242990654,"where x′ = ⟨s′, B(p, s, a, s′)⟩∈X. Recall that V ⋆
h+1 denotes the optimal BAMDP value function at
timestep h + 1 and π⋆
h denote the Bayes-optimal policy. If for all h ∈[H], for all s ∈S, and for any
p, q ∈∆(Θ) |V ⋆
h (⟨s, p⟩) −Vh(⟨s, q⟩)| ≤ε, then ||V ⋆
h −V πh,V
h
||∞≤2ε(H −h + 1)."
IMPLEMENTATION/METHODS,0.4672897196261682,"Combining Propositions 1 and 2 immediately yields a corresponding performance-loss bound as
desired, paralleling the analogous result for state aggregation in MDPs (see Theorem 4.1 of Van Roy
[2006]):"
IMPLEMENTATION/METHODS,0.4719626168224299,"Proposition 3. Let π⋆
ϕ,h denote the greedy policy with respect to V ⋆
ϕ,h+1. Then, ||V ⋆
h −V
π⋆
ϕ,h
h
||∞≤
4δ(H −h)(H −h + 1)2."
CONCLUSION/DISCUSSION,0.4766355140186916,"6
Discussion & Conclusion"
CONCLUSION/DISCUSSION,0.48130841121495327,"In this work, we began by characterizing the complexity of a BAMDP via an upper bound on
the total number of interactions needed by an agent to exhaust information and fully resolve its"
CONCLUSION/DISCUSSION,0.48598130841121495,"epistemic uncertainty over the true environment. Under an assumption on the exact form of the agent’s
uncertainty, we showed how this information horizon facilitates more efficient planning when smaller
than the original problem horizon. We recognize that Assumption 1 deviates from the traditional
parameterization of uncertainty in the MDP transition function via the Dirichlet distribution [Poupart
et al., 2006, Kolter and Ng, 2009] (sometimes also known as the flat Dirichlet-Multinomial or FDM
model [Asmuth, 2013]). The driving force behind this choice is to avoid dealing in differential
entropy when engaging with the (residual) uncertainty contained in any epistemic state. Should one
aspire to depart from Assumption 1 altogether in a rigorous way that manifests within the analysis, we
suspect that it may be fruitful to consider a lossy compression of each epistemic state into a discrete,
|Θ|-valued random variable. Under such a formulation, the appropriate tool from information theory
for the analysis would be rate-distortion theory [Cover and Thomas, 2012, Csiszár, 1974]. This
would, in a theoretically-sound way, allow for an arbitrary BAMDP parameterization and, for the
purposes of continuing the use (discrete) Shannon entropy in the definition of the information horizon,
induce a lossy compression of each epistemic state whose approximation error relative to the true
epistemic state could be accounted for via the associated rate-distortion function."
CONCLUSION/DISCUSSION,0.49065420560747663,"Recognizing the persistence of the intractable BAMDP hyperstate space, we then proceeded to outline
epistemic state abstraction as a mechanism that not only induces a finite, tractable hyperstate space
but also has the potential to incur a reduced information horizon within the abstract problem. Through
our analysis of approximation error and performance loss, we observe an immediate consequence of
Proposition 3: if one wishes to compute an ε-optimal BAMDP value function for an original BAMDP
of interest, one need only find the
ε
4(H−h)(H−h+1)2 -cover of the simplex, ∆(Θ), and then apply the
corresponding epistemic state abstraction through Algorithm 3, whose planning complexity bears
no dependence on the hyperstate space of the original BAMDP and has reduced dependence on the
problem horizon. One might observe that the right-hand side of the value-loss bound is maximized at
timestep h = 1, making this first step the limiting factor when determining what value of δ to employ
for computing the epistemic state abstraction. As this cover could become quite large and detract
from the efficiency of utilizing an epistemic state abstraction in subsequent time periods, future work
might benefit from considering abstractions formed by a sequence of exactly H δh-covers, where the
indexing of δh in time h ∈[H] affords better preservation of value (via a straightforward extension
of Proposition 3) across all timesteps simultaneously."
CONCLUSION/DISCUSSION,0.4953271028037383,"One caveat and limitation of our planning algorithms (both exact and approximate) is the provision of
the information horizon as an input. An agent designer may seldom have the prescience of knowing
the underlying BAMDP information structure or, even with suitable regularity assumptions on the
policy class, be able to compute it. An observation is that many sampling-based algorithms for
approximately solving BAMDPs, like BAMCP [Guez et al., 2012], implicitly hypothesize a small
information horizon (typically, a value of 1) through their use of posterior sampling and choice
of rollout policy. Meanwhile, recent work has demonstrated strong performance guarantees for a
reinforcement-learning agent acting in an arbitrary environment [Dong et al., 2022] through the use
of an incrementally increasing discount factor [Jiang et al., 2015b, Arumugam et al., 2018], gradually
expanding the effective range over which the agent is expected to demonstrate competent behavior.
Taking inspiration from this idea, future work might consider designing more-efficient planning
algorithms that, while ignorant of the true information horizon, instead hypothesize a sequence of
increasing information horizons, eventually building up to the complexity of the full BAMDP. Of
course, prior to development of novel algorithms, the notion that existing BAMDP planners may
already make implicit use of the information horizon is in and of itself a task for future work to tease
apart and make mathematically rigorous."
CONCLUSION/DISCUSSION,0.5,"Moreover, similar to how the simulation lemma [Kearns and Singh, 2002] provides a principled
foundation for model-based reinforcement learning, our analysis might also be seen as offering
theoretical underpinnings to the Bayes-optimal exploration strategies learned by meta reinforcement-
learning agents [Ortega et al., 2019, Mikulik et al., 2020] whose practical instantiations already rely
upon approximate representations of epistemic state [Zintgraf et al., 2019, 2021]."
CONCLUSION/DISCUSSION,0.5046728971962616,Acknowledgements
CONCLUSION/DISCUSSION,0.5093457943925234,"The authors gratefully acknowledge the anonymous reviewers for their insightful comments, questions,
and discussions."
REFERENCES,0.514018691588785,References
REFERENCES,0.5186915887850467,"David Abel. A Theory of Abstraction in Reinforcement Learning. PhD thesis, Brown University,
2020. 7"
REFERENCES,0.5233644859813084,"David Abel, David Hershkowitz, and Michael Littman. Near optimal behavior via approximate state
abstraction. In International Conference on Machine Learning, pages 2915–2923. PMLR, 2016. 2,
7"
REFERENCES,0.5280373831775701,"David Abel, Dilip Arumugam, Lucas Lehnert, and Michael L. Littman. Toward good abstractions for
lifelong learning. In NeurIPS Workshop on Hierarchical Reinforcement Learning, 2017. 22"
REFERENCES,0.5327102803738317,"David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman. State abstractions for lifelong
reinforcement learning. In International Conference on Machine Learning, pages 10–19. PMLR,
2018. 7"
REFERENCES,0.5373831775700935,"David Abel, Dilip Arumugam, Kavosh Asadi, Yuu Jinnai, Michael L Littman, and Lawson LS
Wong. State abstraction as compression in apprenticeship learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pages 3134–3142, 2019. 7"
REFERENCES,0.5420560747663551,"David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam, Doina Precup, and Michael
Littman. Value preserving state-action abstractions. In International Conference on Artificial
Intelligence and Statistics, pages 1639–1650. PMLR, 2020. 22"
REFERENCES,0.5467289719626168,"David Abel, Cameron Allen, Dilip Arumugam, D. Ellis Hershkowitz, Michael L. Littman, and
Lawson L.S. Wong. Bad-policy density: A measure of reinforcement learning hardness. In ICML
Workshop on Reinforcement Learning Theory, 2021. 2"
REFERENCES,0.5514018691588785,"Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: Structural com-
plexity and representation learning of low rank MDPs. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 20095–20107, 2020. 2"
REFERENCES,0.5560747663551402,"Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case
regret bounds. In Advances in Neural Information Processing Systems, pages 1184–1194, 2017. 4"
REFERENCES,0.5607476635514018,"Mauricio Araya-López, Vincent Thomas, and Olivier Buffet. Near-optimal BRL using optimistic
local transitions. In Proceedings of the 29th International Conference on Machine Learning, pages
515–522, 2012. 1, 4"
REFERENCES,0.5654205607476636,"Dilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas
Lehnert, and Michael L Littman. Mitigating planner overfitting in model-based reinforcement
learning. arXiv preprint arXiv:1812.01129, 2018. 10"
REFERENCES,0.5700934579439252,"Dilip Arumugam, Peter Henderson, and Pierre-Luc Bacon. An information-theoretic perspective on
credit assignment in reinforcement learning. arXiv preprint arXiv:2103.06224, 2021. 2"
REFERENCES,0.5747663551401869,"John Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian sampling
approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artificial Intelligence, pages 19–26, 2009. 1, 4"
REFERENCES,0.5794392523364486,"John Thomas Asmuth. Model-based Bayesian reinforcement learning with generalized priors.
Rutgers The State University of New Jersey-New Brunswick, 2013. 10"
REFERENCES,0.5841121495327103,"Peter L Bartlett and Ambuj Tewari. REGAL: a regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artificial Intelligence, pages 35–42, 2009. 2"
REFERENCES,0.5887850467289719,"Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and Rémi Munos. Increasing the
action gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 30, 2016. 2"
REFERENCES,0.5934579439252337,"Richard Bellman. A Markovian decision process. Journal of mathematics and mechanics, pages
679–684, 1957. 2, 4, 5"
REFERENCES,0.5981308411214953,"Richard Bellman and Robert Kalaba. On adaptive control processes. IRE Transactions on Automatic
Control, 4(2):1–9, 1959. 1, 3"
REFERENCES,0.602803738317757,"Dimitri P Bertsekas, David A Castanon, et al. Adaptive aggregation methods for infinite horizon
dynamic programming. 1988. 7"
REFERENCES,0.6074766355140186,"Pablo Samuel Castro and Doina Precup. Using linear programming for Bayesian exploration in
Markov decision processes. In IJCAI, volume 24372442, 2007. 1, 4"
REFERENCES,0.6121495327102804,"Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012. 6, 10"
REFERENCES,0.616822429906542,"Imre Csiszár. On an extremum problem of information theory. Studia Scientiarum Mathematicarum
Hungarica, 9, 1974. 10"
REFERENCES,0.6214953271028038,"Peter Dayan and Terrence J Sejnowski. Exploration bonuses and dual control. Machine Learning, 25
(1):5–22, 1996. 1, 4"
REFERENCES,0.6261682242990654,"Thomas Dean and Robert Givan. Model minimization in Markov decision processes. In AAAI/IAAI,
pages 106–111, 1997. 7"
REFERENCES,0.6308411214953271,"Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian Q-learning. In Proceedings of the
Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial
Intelligence, pages 761–768, 1998. 1, 4"
REFERENCES,0.6355140186915887,"Christos Dimitrakakis. Complexity of stochastic branch and bound methods for belief tree search in
Bayesian reinforcement learning. arXiv preprint arXiv:0912.5029, 2009. 1"
REFERENCES,0.6401869158878505,"Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably efficient reinforcement learning with
aggregated states. arXiv preprint arXiv:1912.06366, 2019. 7"
REFERENCES,0.6448598130841121,"Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple Agent, Complex Environment: Efficient
Reinforcement Learning with Agent States. Journal of Machine Learning Research, 23(255):1–54,
2022. URL http://jmlr.org/papers/v23/21-0773.html. 10"
REFERENCES,0.6495327102803738,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, pages 1665–1674. PMLR, 2019. 7"
REFERENCES,0.6542056074766355,"Richard M Dudley. The sizes of compact subsets of Hilbert space and continuity of Gaussian
processes. Journal of Functional Analysis, 1(3):290–330, 1967. 8"
REFERENCES,0.6588785046728972,"Michael O Duff. Monte-Carlo algorithms for the improvement of finite-state stochastic controllers:
Application to Bayes-adaptive Markov decision processes. In International Workshop on Artificial
Intelligence and Statistics, pages 93–97. PMLR, 2001. 1, 4"
REFERENCES,0.6635514018691588,"Michael O Duff. Design for an optimal probe. In Proceedings of the 20th International Conference
on Machine Learning (ICML-03), pages 131–138, 2003a. 1, 4"
REFERENCES,0.6682242990654206,"Michael O Duff. Diffusion approximation for Bayesian Markov chains. In Proceedings of the 20th
International Conference on Machine Learning (ICML-03), pages 139–146, 2003b. 1, 4"
REFERENCES,0.6728971962616822,"Michael O Duff and Andrew G Barto. Local bandit approximation for optimal learning problems. In
Advances in Neural Information Processing Systems, pages 1019–1025, 1997. 1, 4"
REFERENCES,0.677570093457944,"Michael O’Gordon Duff. Optimal Learning: Computational procedures for Bayes-adaptive Markov
decision processes. University of Massachusetts Amherst, 2002. 1, 3"
REFERENCES,0.6822429906542056,"Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. Advances in Neural
Information Processing Systems, 24:172–180, 2011. 2"
REFERENCES,0.6869158878504673,"Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite Markov decision processes.
In UAI, volume 4, pages 162–169, 2004. 7"
REFERENCES,0.6915887850467289,"Norman Ferns, Pablo Samuel Castro, Doina Precup, and Prakash Panangaden. Methods for computing
state similarity in Markov decision processes. arXiv preprint arXiv:1206.6836, 2012. 7"
REFERENCES,0.6962616822429907,"Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. Foundations and Trends® in Machine Learning, 8(5-6):359–483, 2015. 1, 4"
REFERENCES,0.7009345794392523,"John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical
Society: Series B (Methodological), 41(2):148–164, 1979. 4"
REFERENCES,0.705607476635514,"Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning
Proceedings 1995, pages 261–268. Elsevier, 1995. 7"
REFERENCES,0.7102803738317757,"Arthur Guez, David Silver, and Peter Dayan. Efficient Bayes-adaptive reinforcement learning using
sample-based search. In Proceedings of the 25th International Conference on Neural Information
Processing Systems-Volume 1, pages 1025–1033, 2012. 1, 4, 10"
REFERENCES,0.7149532710280374,"Arthur Guez, David Silver, and Peter Dayan. Scalable and efficient Bayes-adaptive reinforcement
learning based on monte-carlo tree search. Journal of Artificial Intelligence Research, 48:841–883,
2013. 1, 4"
REFERENCES,0.719626168224299,"Arthur Guez, Nicolas Heess, David Silver, and Peter Dayan. Bayes-adaptive simulation-based search
with value function approximation. In Advances in Neural Information Processing Systems, pages
451–459, 2014. 1, 4"
REFERENCES,0.7242990654205608,"Dorit S Hochbaum. Approximating covering and packing problems: set cover, vertex cover, indepen-
dent set, and related problems. In Approximation algorithms for NP-hard problems, pages 94–143.
1996. 7"
REFERENCES,0.7289719626168224,"David Hsu, Wee Sun Lee, and Nan Rong. What makes some POMDP problems easy to approximate?
In Proceedings of the 20th International Conference on Neural Information Processing Systems,
pages 689–696, 2007. 2, 4"
REFERENCES,0.7336448598130841,"Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010. 2"
REFERENCES,0.7383177570093458,"Nan Jiang, Alex Kulesza, and Satinder Singh. Abstraction selection in model-based reinforcement
learning. In International Conference on Machine Learning, pages 179–188. PMLR, 2015a. 7"
REFERENCES,0.7429906542056075,"Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous
Agents and Multiagent Systems, pages 1181–1189. Citeseer, 2015b. 10"
REFERENCES,0.7476635514018691,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contex-
tual decision processes with low Bellman rank are PAC-learnable. In International Conference on
Machine Learning, pages 1704–1713. PMLR, 2017. 2"
REFERENCES,0.7523364485981309,"Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman Eluder dimension: New rich classes of RL
problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021. 2"
REFERENCES,0.7570093457943925,"Nicholas K Jong and Peter Stone. State abstraction discovery from irrelevant state variables. In
IJCAI, volume 8, pages 752–757. Citeseer, 2005. 7"
REFERENCES,0.7616822429906542,"Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially
observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998. 2, 4"
REFERENCES,0.7663551401869159,"Sham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 306–
312, 2003a. 4"
REFERENCES,0.7710280373831776,"Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD thesis,
2003b. 7"
REFERENCES,0.7757009345794392,"Michael Kearns and Satinder Singh. Finite-sample convergence rates for Q-learning and indirect
algorithms. Advances in Neural Information Processing Systems, pages 996–1002, 1999. 4"
REFERENCES,0.780373831775701,"Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.
Machine learning, 49(2-3):209–232, 2002. 7, 10"
REFERENCES,0.7850467289719626,"Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-optimal
planning in large Markov decision processes. Machine learning, 49(2):193–208, 2002. 4"
REFERENCES,0.7897196261682243,"Levente Kocsis and Csaba Szepesvári. Bandit based Monte-Carlo planning. In European conference
on machine learning, pages 282–293. Springer, 2006. 4"
REFERENCES,0.794392523364486,"J Zico Kolter and Andrew Y Ng. Near-Bayesian exploration in polynomial time. In Proceedings of
the 26th annual international conference on machine learning, pages 513–520, 2009. 1, 4, 6, 10"
REFERENCES,0.7990654205607477,"Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pages 1008–1014, 2000. 4"
REFERENCES,0.8037383177570093,"Gilwoo Lee, Brian Hou, Aditya Mandalika, Jeongseok Lee, Sanjiban Choudhury, and Siddhartha S
Srinivasa. Bayesian policy optimization for model uncertainty. In International Conference on
Learning Representations, 2018. 4, 5"
REFERENCES,0.8084112149532711,"Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for
MDPs. ISAIM, 4:5, 2006. 2, 7"
REFERENCES,0.8130841121495327,"Michael L Littman, Thomas L Dean, and Leslie Pack Kaelbling. On the complexity of solving
Markov decision problems. In Proceedings of the Eleventh conference on Uncertainty in Artificial
Intelligence, pages 394–402, 1995. 4"
REFERENCES,0.8177570093457944,"Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and Zheng
Wen. Reinforcement learning, bit by bit. arXiv preprint arXiv:2103.04047, 2021. 3"
REFERENCES,0.822429906542056,"Odalric-Ambrym Maillard, Timothy A Mann, and Shie Mannor. How hard is my MDP?"" the
distribution-norm to the rescue"". Advances in Neural Information Processing Systems, 27:1835–
1843, 2014. 2"
REFERENCES,0.8271028037383178,"Vladimir Mikulik, Grégoire Delétang, Tom McGrath, Tim Genewein, Miljan Martic, Shane Legg,
and Pedro A Ortega. Meta-trained agents implement Bayes-optimal agents. In NeurIPS, 2020. 10"
REFERENCES,0.8317757009345794,"Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstrac-
tion and provably efficient rich-observation reinforcement learning. In International conference on
machine learning, pages 6961–6971. PMLR, 2020. 7"
REFERENCES,0.8364485981308412,"Pedro A Ortega, Jane X Wang, Mark Rowland, Tim Genewein, Zeb Kurth-Nelson, Razvan Pascanu,
Nicolas Heess, Joel Veness, Alex Pritzel, Pablo Sprechmann, et al. Meta-learning of sequential
strategies. arXiv preprint arXiv:1905.03030, 2019. 10"
REFERENCES,0.8411214953271028,"Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement
learning? In International Conference on Machine Learning, pages 2701–2710, 2017. 4"
REFERENCES,0.8457943925233645,"Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances in neural information processing systems, pages 4026–4034,
2016a. 4"
REFERENCES,0.8504672897196262,"Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. In International Conference on Machine Learning, pages 2377–2386, 2016b. 4"
REFERENCES,0.8551401869158879,"Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep exploration via randomized
value functions. Journal of Machine Learning Research, 20(124):1–62, 2019. 4"
REFERENCES,0.8598130841121495,"Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty Bellman
equation and exploration. In International Conference on Machine Learning, pages 3836–3845,
2018. 4"
REFERENCES,0.8644859813084113,"Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to discrete
Bayesian reinforcement learning. In Proceedings of the 23rd international conference on Machine
learning, pages 697–704, 2006. 1, 3, 4, 10"
REFERENCES,0.8691588785046729,"Martin L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., New York, NY, 1994. 2"
REFERENCES,0.8738317757009346,"Aaron Sidford, Mengdi Wang, Xian Wu, Lin F Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving Markov decision processes with a generative model. In Proceedings of
the 32nd International Conference on Neural Information Processing Systems, pages 5192–5202,
2018a. 4"
REFERENCES,0.8785046728971962,"Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster
algorithms for solving Markov decision processes. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 770–787. SIAM, 2018b. 4"
REFERENCES,0.883177570093458,"Satinder P Singh and Richard C Yee. An upper bound on the loss from approximate optimal-value
functions. Machine Learning, 16(3):227–233, 1994. 9"
REFERENCES,0.8878504672897196,"Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state
aggregation. Advances in neural information processing systems, pages 361–368, 1995. 7"
REFERENCES,0.8925233644859814,"Jonathan Sorg, Satinder Singh, and Richard L Lewis. Variance-based rewards for approximate
Bayesian reinforcement learning. In Proceedings of the Twenty-Sixth Conference on Uncertainty
in Artificial Intelligence, pages 564–571, 2010. 1, 4"
REFERENCES,0.897196261682243,"Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement learning in finite MDPs: PAC
analysis. Journal of Machine Learning Research, 10(Nov):2413–2444, 2009. 7"
REFERENCES,0.9018691588785047,"Malcolm Strens. A Bayesian framework for reinforcement learning. In ICML, volume 2000, pages
943–950, 2000. 1, 4"
REFERENCES,0.9065420560747663,"Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
RL in contextual decision processes: PAC bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pages 2898–2933. PMLR, 2019. 2"
REFERENCES,0.9112149532710281,Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. 1998. 2
REFERENCES,0.9158878504672897,"Paul Tseng. Solving H-horizon, stationary Markov decision problems in time proportional to log(H).
Operations Research Letters, 9(5):287–297, 1990. 4"
REFERENCES,0.9205607476635514,"John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic program-
ming. Machine Learning, 22(1):59–94, 1996. 7"
REFERENCES,0.9252336448598131,"Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation.
Mathematics of Operations Research, 31(2):234–244, 2006. 2, 7, 9"
REFERENCES,0.9299065420560748,"Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Bayesian sparse sampling for
on-line reward optimization. In Proceedings of the 22nd International Conference on Machine
Learning, pages 956–963, 2005. 1, 4"
REFERENCES,0.9345794392523364,"Ward Whitt. Approximations of dynamic programs, I. Mathematics of Operations Research, 3(3):
231–243, 1978. 7"
REFERENCES,0.9392523364485982,"Zongzhang Zhang, Michael Littman, and Xiaoping Chen. Covering number as a complexity measure
for POMDP planning and learning. In Twenty-Sixth AAAI Conference on Artificial Intelligence,
2012. 2, 4, 7"
REFERENCES,0.9439252336448598,"Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and
Shimon Whiteson. VariBAD: A very good method for Bayes-adaptive deep RL via meta-learning.
In International Conference on Learning Representations, 2019. 1, 10"
REFERENCES,0.9485981308411215,"Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and
Shimon Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning.
In International Conference on Machine Learning, pages 12991–13001. PMLR, 2021. 10"
OTHER,0.9532710280373832,Checklist
OTHER,0.9579439252336449,1. For all authors...
OTHER,0.9626168224299065,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]"
OTHER,0.9672897196261683,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
OTHER,0.9719626168224299,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
OTHER,0.9766355140186916,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [N/A]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [N/A]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
OTHER,0.9813084112149533,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
OTHER,0.985981308411215,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
OTHER,0.9906542056074766,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
OTHER,0.9953271028037384,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
