Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0027247956403269754,"Unsupervised video domain adaptation is a practical yet challenging task. In this
work, for the first time, we tackle it from a disentanglement view. Our key idea
is to handle the spatial and temporal domain divergence separately through disen-
tanglement. Specifically, we consider the generation of cross-domain videos from
two sets of latent factors, one encoding the static information and another encoding
the dynamic information. A Transfer Sequential VAE (TranSVAE) framework
is then developed to model such generation. To better serve for adaptation, we
propose several objectives to constrain the latent factors. With these constraints,
the spatial divergence can be readily removed by disentangling the static domain-
specific information out, and the temporal divergence is further reduced from both
frame- and video-levels through adversarial learning. Extensive experiments on
the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and
superiority of TranSVAE compared with several state-of-the-art approaches.1"
INTRODUCTION,0.005449591280653951,"1
Introduction"
INTRODUCTION,0.008174386920980926,"Over the past decades, unsupervised domain adaptation (UDA) has attracted extensive research
attention [41]. Numerous UDA methods have been proposed and successfully applied to various
real-world applications, e.g., object recognition [38, 42, 47], semantic segmentation [51, 19, 33, 24],
and object detection [3, 15, 45]. However, most of these methods and their applications are limited to
the image domain, while much less attention has been devoted to video-based UDA, where the latter
is undoubtedly more challenging."
INTRODUCTION,0.010899182561307902,"Compared with image-based UDA, the source and target domains also differ temporally in video-
based UDA. Images are spatially well-structured data, while videos are sequences of images with both
spatial and temporal relations. Existing image-based UDA methods can hardly achieve satisfactory
performance on video-based UDA tasks as they fail to consider the temporal dependency of video
frames in handling the domain gaps. For instance, in video-based cross-domain action recognition
tasks, domain gaps are presented by not only the actions of different persons in different scenarios
but also the actions that appear at different timestamps or last at different time lengths."
INTRODUCTION,0.013623978201634877,1Code is publicly available at: https://github.com/ldkong1205/TranSVAE
INTRODUCTION,0.01634877384196185,"Figure 1: Conceptual comparisons between the traditional all-in-one view and the proposed dis-
entanglement view. Prior works often seek to compress implicit domain information to obtain
domain-indistinguishable representations; while in this work, we pursue explicit decouplings of
domain-specific information from other information via generative modeling."
INTRODUCTION,0.01907356948228883,"Recently, few works have been proposed for video-based UDA. The key idea is to achieve domain
alignment by aligning both frame- and video-level features through adversarial learning [7, 26],
contrastive learning [37, 31], attention [10], or combination of these mechanisms, e.g., adversarial
learning with attention [29, 8]. Though they have advanced video-based UDA, there is still room
for improvement. Generally, existing methods follow an all-in-one way, where both spatial and
temporal domain divergence are handled together, for adaptation (Fig. 1, â– â–¡). However, cross-domain
videos are highly complex data containing diverse mixed-up information, e.g., domain, semantic, and
temporal information, which makes the simultaneous elimination of spatial and temporal divergence
insufficient. This motivates us to handle the video-based UDA from a disentanglement perspective
(Fig. 1, â– â–¡) so that the spatial and temporal divergence can be well handled separately."
INTRODUCTION,0.021798365122615803,"Figure 2: Graphical illustrations of
the proposed generative (1st figure)
and inference (2nd figure) models for
video domain disentanglement."
INTRODUCTION,0.02452316076294278,"To achieve this goal, we first consider the generation process
of cross-domain videos, as shown in Fig. 2, where a video is
generated from two sets of latent factors: one set consists of a
sequence of random variables, which are dynamic and incline
to encode the semantic information for downstream tasks, e.g.,
action recognition; another set is static and introduces some
domain-related spatial information to the generated video,
e.g., style or appearance. Specifically, the blue / red nodes
are the observed source / target videos xS / xT , respectively,
over t timestamps. Static latent variables zS
d and zT
d follow a
joint distribution and combining either of them with dynamic
latent variables zt constructs one video data of a domain."
INTRODUCTION,0.027247956403269755,"With the above generative model, we develop a Transfer
Sequential Variational AutoEncoder (TranSVAE) for video-
based UDA. TranSVAE handles the cross-domain divergence
in two levels, where the first level removes the spatial diver-
gence by disentangling zd from zt; while the second level
eliminates the temporal divergence of zt. To achieve this, we
leverage appropriate constraints to ensure that the disentan-
glement indeed serves the adaptation purpose. Firstly, we
enable a good decoupling of the two sets of latent factors by
minimizing their mutual dependence. This encourages these two latent factor sets to be mutually
independent. We then consider constraining each latent factor set. For zD
d with D âˆˆ{S, T }, we
propose a contrastive triplet loss to make them static and domain-specific. This makes us readily
handle spatial divergence by disentangling zD
d out. For zt, we propose to align them across domains
at both frame and video levels through adversarial learning so as to further eliminate the temporal
divergence. Meanwhile, as downstream tasks use zt as input, we also add the task-specific supervision
on zt extracted from source data (w/ ground-truth)."
INTRODUCTION,0.02997275204359673,"To the best of our knowledge, this is the first work that tackles the challenging video-based UDA
from a domain disentanglement view. We conduct extensive experiments on popular benchmarks
(UCF-HMDB, Jester, Epic-Kitchens) and the results show that TranSVAE consistently outperforms
previous state-of-the-art methods by large margins. We also conduct comprehensive ablation studies"
INTRODUCTION,0.0326975476839237,"and disentanglement analyses to verify the effectiveness of the latent factor decoupling. The main
contribution of the paper is summarized as follows:"
INTRODUCTION,0.035422343324250684,"â€¢ We provide a generative perspective on solving video-based UDA problems. We develop a
generative graphical model for the cross-domain video generation process and propose to
utilize the sequential VAE as the base generative model."
INTRODUCTION,0.03814713896457766,"â€¢ Based on the above generative view, we propose a TranSVAE framework for video-based
UDA. By developing four constraints on the latent factors to enable disentanglement to ben-
efit adaptation, the proposed framework is capable of handling the cross-domain divergence
from both spatial and temporal levels."
INTRODUCTION,0.04087193460490463,"â€¢ We conduct extensive experiments on several benchmark datasets to verify the effectiveness
of TranSVAE. A comprehensive ablation study also demonstrates the positive effect of each
loss term on video domain adaptation."
RELATED WORK,0.043596730245231606,"2
Related Work"
RELATED WORK,0.04632152588555858,"Unsupervised Video Domain Adaptation. Despite the great progress in image-based UDA, only
a few methods have recently attempted video-based UDA. In [7], a temporal attentive adversarial
adaptation network (TA3N) is proposed to integrate a temporal relation module for temporal alignment.
Choi et al. [10] proposed a SAVA method using self-supervised clip order prediction and clip attention-
based alignment. Based on a cross-domain co-attention mechanism, the temporal co-attention network
TCoN [29] focused on common key frames across domains for better alignment. Luo et al. [26]
pay more attention to the domain-agnostic classifier by using a network topology of the bipartite
graph to model the cross-domain correlations. Instead of using adversarial learning, Sahoo et al. [31]
developed an end-to-end temporal contrastive learning framework named CoMix with background
mixing and target pseudo-labels. Recently, Chen et al. [8] learned multiple domain discriminators for
multi-level temporal attentive features to achieve better alignment, while Turrisi et al. [37] exploited
two-headed deep architecture to learn a more robust target classifier by the combination of cross-
entropy and contrastive losses. Although these approaches have advanced video-based UDA tasks,
they all adopted to align features with diverse information mixed up from a compression perspective,
which leaves room for further improvements."
RELATED WORK,0.04904632152588556,"Multi-Modal Video Adaptation. Most recently, there are also a few works integrating multiple
modality data for video-based UDA. Although we only use the single modality RGB features, we still
discuss this multi-modal research line for a complete literature review. The very first work exploring
the multi-modal nature of videos for UDA is MM-SADA [28], where the correspondence of multiple
modalities was exploited as a self-supervised alignment in addition to adversarial alignment. A
later work, spatial-temporal contrastive domain adaptation (STCDA) [34], utilized a video-based
contrastive alignment as the multi-modal domain metric to measure the video-level discrepancy across
domains. [18] proposed cross-modal and cross-domain contrastive losses to handle feature spaces
across modalities and domains. [43] leveraged both cross-modal complementary and cross-modal
consensus to learn the most transferable features through a CIA framework. In [12], the authors
proposed to generate noisy pseudo-labels for the target domain data using the source pre-trained
model and select the clean samples in order to increase the quality of the pseudo-labels. Lin et al. [23]
developed a cycle-based approach that alternates between spatial and spatiotemporal learning with
knowledge transfer. Generally, all the above methods utilize the flow as the auxiliary modality input.
Recently, there are also methods exploring other modalities, for instance, A3R [48] with audios and
MixDANN [44] with wild data. It is worth noting that the proposed TranSVAE â€“ although only uses
single modality RGB features â€“ surprisingly achieves better UDA performance compared with most
current state-of-the-art multi-modal methods, which highlights our superiority."
RELATED WORK,0.051771117166212535,"Disentanglement. Feature disentanglement is a wide and hot research topic. We only focus on
works that are closely related to ours. In the image domain, some works consider adaptation from a
generative view. [4] learned a disentangled semantic representation across domains. A similar idea is
then applied to graph domain adaptation [5] and domain generalization [16]. [13] proposed a novel
informative feature disentanglement, equipped with the adversarial network or the metric discrepancy
model. Another disentanglement-related topic is sequential data generation. To generate videos,
existing works [22, 50, 1] extended VAE to a recurrent form with different recursive structures. In ğ±! ğ’®, ğ±#"
RELATED WORK,0.05449591280653951,"ğ’®, â€¦ , ğ±$ ğ’® ğ±! ğ’¯, ğ±#"
RELATED WORK,0.05722070844686648,"ğ’¯, â€¦ , ğ±$ ğ’¯"
RELATED WORK,0.05994550408719346,"Input
Encoder
Bi-LSTM
Latent Space
Sample & Predict ğ’›!"
RELATED WORK,0.06267029972752043,"ğ’®, â€¦ , ğ’›$ ğ’®
ğ’›!"
RELATED WORK,0.0653950953678474,"ğ’¯, â€¦ , ğ’›$ ğ’¯ %ğ±!"
RELATED WORK,0.0681198910081744,"ğ’®, %ğ±#"
RELATED WORK,0.07084468664850137,"ğ’®, â€¦ , %ğ±$ ğ’® %ğ±!"
RELATED WORK,0.07356948228882834,"ğ’¯, %ğ±#"
RELATED WORK,0.07629427792915532,"ğ’¯, â€¦ , %ğ±$ ğ’¯"
RELATED WORK,0.07901907356948229,"Decoder
Reconstruct â„’ â„’ â„’ mean var"
RELATED WORK,0.08174386920980926,Domain-Static Factor ğ‘(ğ’›&
RELATED WORK,0.08446866485013624,ğ’Ÿ|ğ±!:$ ğ’Ÿ) mean var
RELATED WORK,0.08719346049046321,Domain-Invariant
RELATED WORK,0.08991825613079019,"Sequential Factor ğ‘(ğ’›) ğ’Ÿ|ğ±*) ğ’Ÿ) ğ’›& ğ’®
ğ’›& ğ’¯"
RELATED WORK,0.09264305177111716,"â„’
Classify"
RELATED WORK,0.09536784741144415,"/
: Channel Concat"
RELATED WORK,0.09809264305177112,"â„’
Domain Adversarial Eq. (10)
â„’
Task Supervision Eq. (13)
â„’
Contrastive Triplet Eq. (11)
â„’
Mutual Independent Eq. (5)"
RELATED WORK,0.1008174386920981,"Figure 3: TranSVAE overview. The input videos are fed into an encoder to extract the visual features,
followed by an LSTM to explore the temporal information. Two groups of mean and variance
networks are then applied to model the posterior of the latent factors, i.e., q(zD
t |xD
<t) and q(zD
d |xD
1:T ).
The new representations zD
1 , ..., zD
T and zD
d are sampled, and then concatenated and passed to a
decoder for reconstruction. Four constraints are proposed to regulate the latent factors for adaptation."
RELATED WORK,0.10354223433242507,"this paper, we present a VAE-based structure to generate cross-domain videos. We aim at tackling
video-based UDA from a new perspective: sequential domain disentanglement and transfer."
TECHNICAL APPROACH,0.10626702997275204,"3
Technical Approach"
TECHNICAL APPROACH,0.10899182561307902,"Formally, for a typical video-based UDA problem, we have a source domain S and a target domain T .
Domain S contains sufficient labeled data, denoted as {(VS
i , yS
i )}NS
i=1, where VS
i is a video sequence
and yS
i is the class label. Domain T consists of unlabeled data, denoted as {VT
i }NT
i=1. For a sequence
VD
i from domain D âˆˆ{S, T }, it contains T frames {xD
i_1, ..., xD
i_T } in total2. We further denote
N = N S + N T . Domains S and T are of different distributions but share the same label space. The
objective is to utilize both {(VS
i , yS
i )}NS
i=1 and {VT
i }NT
i=1 to train a good classifier for domain T . We
present a table to list the notations in the Appendix."
TECHNICAL APPROACH,0.11171662125340599,"Framework Overview. We adopt a VAE-based structure to model the cross-domain video generation
process and propose to regulate the two sets of latent factors for adaptation purposes. The reconstruc-
tion process is based on the two sets of latent factors, i.e., zD
1 , ..., zD
T and zD
d that are sampled from the
posteriors q(zD
t |xD
<t) and q(zD
d |xD
1:T ), respectively. The overall architecture consists of five segments
including the encoder, LSTM, latent spaces, sampling, and decoder, as shown in Fig. 3. The vanilla
structure only gives an arbitrary disentanglement of latent factors. To make the disentanglement
facilitate adaptation, we carefully constrain the latent factors as follows."
TECHNICAL APPROACH,0.11444141689373297,"Video Sequence Reconstruction. The overall architecture of TranSVAE follows a VAE-based
structure [22, 50, 1] with two sets of latent factors zD
1 , ..., zD
T and zD
d . The generative and inference
graphical models are presented in Fig. 2. Similar to the conventional VAE, we use a standard
Gaussian distribution for static latent factors. For dynamic ones, we use a sequential prior zD
t |zD
<t âˆ¼
N(Âµt, diag(Ïƒ2
t)), that is, the prior distribution of the current dynamic factor is conditioned on the
historical dynamic factors. The distribution parameters can be re-parameterized as a recurrent network,
e.g., LSTM, with all previous dynamic latent factors as the input. Denoting ZD = {zD
1 , ..., zD
T } for
simplification, we then get the prior as follows:"
TECHNICAL APPROACH,0.11716621253405994,"p(zD
d , ZD) = p(zD
d ) T
Y"
TECHNICAL APPROACH,0.11989100817438691,"t=1
p(zD
t |zD
<t).
(1)"
TECHNICAL APPROACH,0.1226158038147139,"2Without confusion, we omit the subscript i for VD and the corresponding notations, e.g., {xD
1 , ..., xD
T }."
TECHNICAL APPROACH,0.12534059945504086,"Following Fig. 2 (the 1st subfigure), xD
t
is generated from zD
d and zD
t , and we thus model
p(xD
t |zD
d , zD
t ) = N(Âµâ€²
t, diag(Ïƒâ€²
t
2)). The distribution parameters are re-parameterized by the
decoder which can be flexible networks like the deconvolutional neural network. Using VD =
{xD
1 , ..., xD
T }, the generation can be formulated as follows:"
TECHNICAL APPROACH,0.12806539509536785,"p(VD) = p(zD
d ) T
Y"
TECHNICAL APPROACH,0.1307901907356948,"t=1
p(xD
t |zD
d , zD
t )p(zD
t |zD
<t).
(2)"
TECHNICAL APPROACH,0.1335149863760218,"Following Fig. 2 (the 2nd subfigure), we model the posterior distributions of the latent factors
as another two Gaussian distributions, i.e., q(zD
d |VD) = N(Âµd, diag(Ïƒ2
d)) and q(zD
t |xD
<t) =
N(Âµâ€²â€²
t , diag(Ïƒâ€²â€²
t
2)). The parameters of these two distributions are re-parameterized by the encoder,
which can be a convolutional or LSTM module. However, the network of the static latent factors uses
the whole sequence as the input while that of the dynamic latent factors only uses previous frames.
Then the inference can be factorized as:"
TECHNICAL APPROACH,0.1362397820163488,"q(zD
d , ZD|VD) = q(zD
d |VD) T
Y"
TECHNICAL APPROACH,0.13896457765667575,"t=1
q(zD
t |xD
<t).
(3)"
TECHNICAL APPROACH,0.14168937329700274,"Combining the above generation and inference, we obtain the VAE-related objective function as:"
TECHNICAL APPROACH,0.1444141689373297,"Lsvae = Eq(zD
d ,ZD|VD)[âˆ’ T
X"
TECHNICAL APPROACH,0.14713896457765668,"t=1
log p(xD
t |zD
d , zD
t )]+"
TECHNICAL APPROACH,0.14986376021798364,"KL(q(zD
d |VD)||p(zD
d )) + T
X"
TECHNICAL APPROACH,0.15258855585831063,"t=1
KL(q(zD
t |xD
<t)||p(zD
t |zD
<t)), (4)"
TECHNICAL APPROACH,0.1553133514986376,"which is a frame-wise negative variational lower bound. Only using the above vanilla VAE-based
loss cannot guarantee that the disentanglement serves for adaptation, and thus we propose additional
constraints on the two sets of latent factors."
TECHNICAL APPROACH,0.15803814713896458,"Mutual Dependence Minimization (Fig. 3, â– â–¡). We first consider explicitly enforcing the two sets of
latent factors to be mutually independent. To do so, we introduce the mutual information [2] loss Lmi
to regulate the two sets of latent factors. Thus, we obtain:"
TECHNICAL APPROACH,0.16076294277929154,"Lmi(zD
d , ZD) = T
X"
TECHNICAL APPROACH,0.16348773841961853,"t=1
KL
 
q(zD
d , zD
t )||q(zD
d )q(zD
t )

= T
X"
TECHNICAL APPROACH,0.16621253405994552,"t=1
[H(zD
d ) + H(zD
t ) âˆ’H(zD
d , zD
t )].
(5)"
TECHNICAL APPROACH,0.16893732970027248,"To calculate Eq. (5), we need to estimate the densities of zD
d , zD
t and (zD
d , zD
t ). Following the
non-parametric way in [9], we use the mini-batch weighted sampling as follows:"
TECHNICAL APPROACH,0.17166212534059946,"H(z) = âˆ’Eq(z)[log q(z)] â‰ˆâˆ’log 1 M M
X"
TECHNICAL APPROACH,0.17438692098092642,"i=1
[log
1
MN M
X"
TECHNICAL APPROACH,0.1771117166212534,"j=1
q (z(xi)|xj)],
(6)"
TECHNICAL APPROACH,0.17983651226158037,"where z is zD
d , zD
t or (zD
d , zD
t ), N denotes the data size and M is the mini-batch size."
TECHNICAL APPROACH,0.18256130790190736,"Domain Specificity & Static Consistency (Fig. 3, â– â–¡). A characteristic of the domain specificity, e.g.,
the video style or the objective appearance, is its static consistency over dynamic frames. With this
observation, we enable the static and domain-specific latent factors so that we can remove the spatial
divergence by disentangling them out. Mathematically, we hope that zD
d does not change a lot when
zD
t varies over time. To achieve this, given a sequence, we randomly shuffle the temporal order of
frames to form a new sequence. The static latent factors disentangled from the original sequence and
the shuffled sequence should be ideally equal or be very close at least. This motivates us to minimize
the distance between these two static factors. Meanwhile, to further enhance the domain specificity,
we enforce the dynamic latent factors from different domains to have a large distance. To this end,
we propose the following contrastive triplet loss:"
TECHNICAL APPROACH,0.18528610354223432,"Lctc = max

D(zD+
d
,ezD+
d
) âˆ’D(zD+
d
, zDâˆ’
d
) + m, 0

,
(7)"
TECHNICAL APPROACH,0.1880108991825613,"where D(Â·, Â·) is Euclidean distance, m is a margin set to 1 in the experiments, zD+
d
, ezD+
d
, and zDâˆ’
d
are
static latent factors of the anchor sequence from domain D+, the shuffled sequence, and a randomly
selected sequence from domain Dâˆ’, respectively. D+ and Dâˆ’represent two different domains."
TECHNICAL APPROACH,0.1907356948228883,"Domain Temporal Alignment (Fig. 3, â– â–¡). We now consider to reduce the temporal divergence
of the dynamic latent factors. There are several ways to achieve this, and in this paper, we take
advantage of the most popular adversarial-based idea [14]. Specifically, we build a domain classifier
to discriminate whether the data is from S or T . When back-propagating the gradients, a gradient
reversal layer (GRL) is adopted to invert the gradients. Like existing video-based UDA methods, we
also conduct both frame-level and video-level alignments. Moreover, as TA3N [7] does, we exploit
the temporal relation network (TRN) [49] to discover the temporal relations among different frames,
and then aggregate all the temporal relation features into the final video-level features. This enables
another level of alignment on the temporal relation features. Thus, we have:"
TECHNICAL APPROACH,0.19346049046321526,"Lf = 1 N N
X i=1"
T,0.19618528610354224,"1
T T
X"
T,0.1989100817438692,"t=1
CE

Gf(zD
i_t), di

,
(8)"
T,0.2016348773841962,"Lr = 1 N N
X i=1"
T,0.20435967302452315,"1
T âˆ’1 T
X"
T,0.20708446866485014,"n=2
CE

Gr
 
TrNn(ZD
i )

, di

,
(9)"
T,0.2098092643051771,"Lv = 1 N N
X"
T,0.2125340599455041,"i=1
CE "" Gv"
T,0.21525885558583105,"1
T âˆ’1 T
X"
T,0.21798365122615804,"n=2
TrNn(ZD
i ) ! , di #"
T,0.22070844686648503,",
(10)"
T,0.22343324250681199,"where di is the domain label, CE denotes the cross-entropy loss function, ZD
i = {zD
i_1, ..., zD
i_T },
TrNi is the n-frame temporal relation network, Gf, Gr, and Gv are the frame feature level, the
temporal relation feature level, and the video feature level domain classifiers, respectively. To this
end, we obtain the domain adversarial loss by summing up Eqs. (8-10):"
T,0.22615803814713897,"Ladv = Lf + Lr + Lv.
(11)"
T,0.22888283378746593,"We assign equal importance to these three levels of losses to reduce the overhead of the hyperparameter
search. To this end, with Lmi, Lctc, and Ladv, the learned dynamic latent factors are expected to be
domain-invariant (the three constraints are interactive and complementary to each other for obtaining
the domain-invariant dynamic latent factor), and then can be used for downstream UDA tasks. In this
paper, we specifically focus on action recognition as the downstream task."
T,0.23160762942779292,"Task Specific Supervision (Fig. 3, â– â–¡). We further encourage the dynamic latent factors to carry
the semantic information. Considering that the source domain has sufficient labels, we accordingly
design the task supervision as the regularization imposed on zS
t . This gives us:"
T,0.23433242506811988,"Lcls =
1
N S NS
X"
T,0.23705722070844687,"i=1
L
 
F(ZS
i ), yS
i

,
(12)"
T,0.23978201634877383,"where F(Â·) is a feature transformer mapping the frame-level features to video-level features, specifi-
cally a TRN in this paper, and L(Â·, Â·) is either cross-entropy or mean squared error loss according to
the targeted task."
T,0.24250681198910082,"Although the dynamic latent factors are constrained to be domain-invariant, we do not completely rely
on source semantics to learn features discriminative for the target domain. We propose to incorporate
target pseudo-labels in task-specific supervision. During the training, we use the prediction network
obtained in the previous epoch to generate the target pseudo-labels of the unlabelled target training
data for the current epoch. However, to increase the reliability of target pseudo-labels, we let the
prediction network be trained only on the source supervision for several epochs and then integrate
the target pseudo-labels in the following training epochs. Meanwhile, a confidence threshold is set
to determine whether to use the target pseudo-labels or not. Thus, we have the final task-specific
supervision as follows:"
T,0.2452316076294278,Lcls = 1 N ï£«
T,0.24795640326975477,"ï£­
N S
X"
T,0.2506811989100817,"i=1
L(F(ZS
i ), yS
i ) + N T
X"
T,0.25340599455040874,"i=1
L(F(ZT
i ), eyT
i ) ï£¶"
T,0.2561307901907357,"ï£¸,
(13)"
T,0.25885558583106266,"where eyT
i is the pseudo-label of ZT
i ."
T,0.2615803814713896,"Summary. To this end, we reach the final objective function of our TranSVAE framework as follows:"
T,0.26430517711171664,"Lâ€² = Lsvae + Î»1Lmi + Î»2Ladv + Î»3Lctc + Î»4Lcls,
(14)"
T,0.2670299727520436,"where Î»i with i = 1, 2, 3, 4 denotes the loss balancing weight."
EXPERIMENTS,0.26975476839237056,"4
Experiments"
EXPERIMENTS,0.2724795640326976,"In this section, we conduct extensive experimental studies on popular video-based UDA benchmarks
to verify the effectiveness of the proposed TranSVAE framework."
DATASETS,0.27520435967302453,"4.1
Datasets"
DATASETS,0.2779291553133515,"UCF-HMDB is constructed by collecting the relevant and overlapping action classes from UCF101
[35] and HMDB51 [20]. It contains 3,209 videos in total with 1,438 training videos and 571 validation
videos from UCF101, and 840 training videos and 360 validation videos from HMDB51. This in turn
establishes two video-based UDA tasks: U â†’H and H â†’U."
DATASETS,0.28065395095367845,"Jester [27] consists of 148,092 videos of humans performing hand gestures. Pan et al. [29] constructed
a large-scale cross-domain benchmark with seven gesture classes, and form a single transfer task
JS â†’JT , where JS and JT contain 51,498 and 51,415 video clips, respectively."
DATASETS,0.28337874659400547,"Epic-Kitchens [11] is a challenging egocentric dataset consisting of videos capturing daily activities
in kitchens. [28] constructs three domains across the eight largest actions. They are D1, D2, and D3
corresponding to P08, P01, and P22 kitchens of the full dataset, resulting in six cross-domain tasks."
DATASETS,0.28610354223433243,"Sprites [22] contains sequences of animated cartoon characters with 15 action categories. The
appearances of characters are fully controlled by four attributes, i.e., body, top wear, bottom wear,
and hair. We construct two domains, P1 and P2. P1 uses the human body with attributes randomly
selected from 3 top wears, 4 bottom wears, and 5 hairs, while P2 uses the alien body with attributes
randomly selected from 4 top wears, 3 bottom wears, and 5 hairs. The attribute pools are non-
overlapping across domains, resulting in completely heterogeneous P1 and P2. Each domain has
900 video sequences, and each sequence is with 8 frames."
IMPLEMENTATION DETAILS,0.2888283378746594,"4.2
Implementation Details
Architecture. Following the latest works [31, 37], we use I3D [6] as the backbone3. However,
different from CoMix which jointly trains the backbone, we simply use the pretrained I3D model
on Kinetics [17], provided by [6], to extract RGB features. For the first three benchmarks, RGB
features are used as the input of TranSVAE. For Sprites, we use the original image as the input,
for the purpose of visualizing the reconstruction and disentanglement results. We use the shared
encoder and decoder structures across the source and target domains. For RGB feature inputs,
the encoder and decoder are fully connected layers. For original image inputs, the encoder and
decoder are the convolution and deconvolution layers (from DCGAN [46]), respectively. For the TRN
model, we directly use the one provided by [7]. Other details on this aspect are placed in Appendix."
IMPLEMENTATION DETAILS,0.29155313351498635,Table 1: UDA performance comparisons on UCF-HMDB.
IMPLEMENTATION DETAILS,0.29427792915531337,"Method & Year
Backbone
U â†’H
H â†’U
Average â†‘"
IMPLEMENTATION DETAILS,0.2970027247956403,"DANN (JMLRâ€™16)
ResNet-101
75.28
76.36
75.82
JAN (ICMLâ€™17)
ResNet-101
74.72
76.69
75.71
AdaBN (PRâ€™18)
ResNet-101
72.22
77.41
74.82
MCD (CVPRâ€™18)
ResNet-101
73.89
79.34
76.62
TA3N (ICCVâ€™19)
ResNet-101
78.33
81.79
80.06
ABG (MMâ€™20)
ResNet-101
79.17
85.11
82.14
TCoN (AAAIâ€™20)
ResNet-101
87.22
89.14
88.18
MA2L-TD (WACVâ€™22)
ResNet-101
85.00
86.59
85.80"
IMPLEMENTATION DETAILS,0.2997275204359673,"Source-only (Sonly)
I3D
80.27
88.79
84.53"
IMPLEMENTATION DETAILS,0.3024523160762943,"DANN (JMLRâ€™16)
I3D
80.83
88.09
84.46
ADDA (CVPRâ€™17)
I3D
79.17
88.44
83.81
TA3N (ICCVâ€™19)
I3D
81.38
90.54
85.96
SAVA (ECCVâ€™20)
I3D
82.22
91.24
86.73
CoMix (NeurIPSâ€™21)
I3D
86.66
93.87
90.22
CO2A (WACVâ€™22)
I3D
87.78
95.79
91.79"
IMPLEMENTATION DETAILS,0.30517711171662126,"TranSVAE (Ours)
I3D
87.78
98.95
93.37"
IMPLEMENTATION DETAILS,0.3079019073569482,"Supervised-target (Tsup)
I3D
95.00
96.85
95.93"
IMPLEMENTATION DETAILS,0.3106267029972752,"Configurations. Our TranSVAE is im-
plemented with PyTorch [30]. We use
Adam with a weight decay of 1eâˆ’4 as
the optimizer. The learning rate is ini-
tially set to be 1eâˆ’3 and follows a com-
monly used decreasing strategy in [14].
The batch size and the learning epoch
are uniformly set to be 128 and 1,000,
respectively, for all the experiments.
We uniformly set 100 epochs of train-
ing under only source supervision and
involved the target pseudo-labels after-
ward. Following the common protocol
in video-based UDA [37], we perform
hyperparameter selection on the valida-
tion set. The specific hyperparameters
used for each task can be found in the
Appendix. NVIDIA A100 GPUs are
used for all experiments. Kindly refer to our Appendix for all other details."
IMPLEMENTATION DETAILS,0.3133514986376022,"3The existing widely adopted backbones for video-based UDA include ResNet-101 and I3D. However, more
recent backbones, e.g. Transformer-based or VideoMAE-based [36, 40], are promising to be used in this task."
IMPLEMENTATION DETAILS,0.31607629427792916,Table 2: UDA performance comparisons to approaches using multi-modality data as the input.
IMPLEMENTATION DETAILS,0.3188010899182561,"Task
Sonly
MM-SADA
STCDA
CMCD
A3R
CleanAdapt
CycDA
MixDANN
CIA
TranSVAE"
IMPLEMENTATION DETAILS,0.3215258855585831,"U â†’H
86.1
84.2
83.1
84.7
/
89.8
88.1
82.2
88.3
87.8 (+1.7)
H â†’U
92.5
91.1
92.1
92.8
/
99.2
98.0
92.8
94.1
99.0 (+6.5)"
IMPLEMENTATION DETAILS,0.3242506811989101,"Average â†‘
89.3
87.7
87.6
88.8
/
94.5
93.1
87.5
91.2
93.4 (+4.1)"
IMPLEMENTATION DETAILS,0.32697547683923706,"D1 â†’D2
43.2
49.5
52.0
50.3
53.2
52.7
/
56.0
52.5
50.5 (+7.3)
D1 â†’D3
42.5
44.1
45.5
46.3
52.1
47.0
/
47.3
47.8
50.3 (+7.8)
D2 â†’D1
43.0
48.2
49.0
49.5
51.9
46.2
/
50.3
49.8
50.3 (+7.3)
D2 â†’D3
48.0
52.7
52.5
52.0
55.5
52.7
/
52.4
53.2
58.6 (+10.6)
D3 â†’D1
43.0
50.9
52.6
48.7
51.5
47.8
/
51.0
52.2
48.0 (+5.0)
D3 â†’D2
55.5
56.1
55.6
56.3
63.2
54.4
/
54.7
57.6
58.0 (+2.5)"
IMPLEMENTATION DETAILS,0.329700272479564,"Average â†‘
45.9
50.3
51.2
51.0
54.1
50.3
/
52.0
52.2
52.6 (+6.7)"
IMPLEMENTATION DETAILS,0.33242506811989103,"Competitors. We compared methods from three lines. We first consider the source-only (Sonly) and
supervised-target (Tsup) which uses only labeled source data and only labeled target data, respectively.
These two baselines serve as the lower and upper bounds for our tasks. Secondly, we consider five
popular image-based UDA methods by simply ignoring temporal information, namely DANN [14],
JAN [25], ADDA [38], AdaBN [21], and MCD [32]. Lastly and most importantly, we compare recent
SoTA video-based UDA methods, including TA3N [7], SAVA [10], TCoN [29], ABG [26], CoMix
[31], CO2A [37], and MA2L-TD [8]. All these methods use single modality features. We directly
quote numbers reported in published papers whenever possible. There exist recent works conducting
video-based UDA using multi-modal data, e.g. RGB + Flow. Although TranSVAE solely uses RGB
features, we still take this set of methods into account. Specifically, we consider MM-SADA [28],
STCDA [34], CMCD [18], A3R [48], CleanAdapt [12], CycDA [23], MixDANN [44] and CIA [43]."
COMPARATIVE STUDY,0.335149863760218,"4.3
Comparative Study"
COMPARATIVE STUDY,0.33787465940054495,"Results on UCF-HMDB. Tab. 1 shows comparisons of TranSVAE with baselines and SoTA methods
on UCF-HMDB. The best result among all the baselines is highlighted using bold. Overall, methods
using the I3D backbone [6] achieve better results than those using ResNet-101. Our TranSVAE
consistently outperforms all previous methods. In particular, TranSVAE achieves 93.37% average
accuracy, improving the best competitor CO2A [37], with the same I3D backbone [6], by 1.38%.
Surprisingly, we observe that TranSVAE even yields better results (by a 2.1% improvement) than the
supervised-target (Tsup) baseline. This is because the H â†’U task already has a good performance
without adaptation, i.e., 88.79% for the source-only (Sonly) baseline, and thus the target pseudo-labels
used in TranSVAE are almost correct. By further aligning domains and equivalently augmenting
training data, TranSVAE outperforms Tsup which is only trained with target data."
COMPARATIVE STUDY,0.3405994550408719,Table 3: Comparison results on Jester and Epic-Kitchens.
COMPARATIVE STUDY,0.34332425068119893,"Task
Sonly
DANN
ADDA
TA3N
CoMix
TranSVAE
Tsup
JS â†’JT
51.5
55.4
52.3
55.5
64.7
66.1 (+14.6)
95.6"
COMPARATIVE STUDY,0.3460490463215259,"D1 â†’D2
32.8
37.7
35.4
34.2
42.9
50.5 (+17.7)
64.0
D1 â†’D3
34.1
36.6
34.9
37.4
40.9
50.3 (+16.2)
63.7
D2 â†’D1
35.4
38.3
36.3
40.9
38.6
50.3 (+14.9)
57.0
D2 â†’D3
39.1
41.9
40.8
42.8
45.2
58.6 (+19.5)
63.7
D3 â†’D1
34.6
38.8
36.1
39.9
42.3
48.0 (+13.4)
57.0
D3 â†’D2
35.8
42.1
41.4
44.2
49.2
58.0 (+22.2)
64.0"
COMPARATIVE STUDY,0.34877384196185285,"Average â†‘
35.3
39.2
37.4
39.9
43.2
52.6 (+17.3)
61.5"
COMPARATIVE STUDY,0.35149863760217986,"Results on Jester & Epic-Kitchens.
Tab. 3 shows the comparison results
on the Jester and Epic-Kitchens bench-
marks. We can see that our TranSVAE
is the clear winner among all the meth-
ods on all the tasks.
Specifically,
TranSVAE achieves a 1.4% improve-
ment and a 9.4% average improvement
over the runner-up baseline CoMix
[31] on Jester and Epic-Kitchens, re-
spectively. This verifies the superiority of TranSVAE over others in handling video-based UDA.
However, we also notice that the accuracy gap between CoMix and Tsup is still significant on Jester.
This is because the large-scale Jester dataset contains highly heterogeneous data across domains, e.g.,
the source domain contains videos of the rolling hand forward, while the target domain only consists
of videos of the rolling hand backward. This leaves much room for improvement in the future."
COMPARATIVE STUDY,0.3542234332425068,"Compare to Multi-Modal Methods. We further compare with four recent video-based UDA methods
that use multi-modalities, e.g. RGB features, and optical flows, although TranSVAE only uses RGB
features. Surprisingly, TranSVAE achieves better average results than seven out of eight multi-modal
methods and is only worse than CleanAdapt [12] on UCF-HMDB and A3R [48] on Epic-Kitchens.
Considering TranSVAE only uses single-modality data, we are confident that there exists great
potential for further improvements of TranSVAE with multi-modal data taken into account."
COMPARATIVE STUDY,0.3569482288828338,Table 5: Loss separation study on different video-based UDA tasks by dropping each loss sequentially.
COMPARATIVE STUDY,0.35967302452316074,"Lsvae
Lcls
Ladv
Lmi
Lctc
PL
U â†’H
H â†’U
JS â†’JT
D1 â†’D2
D1 â†’D3
D2 â†’D1
D2 â†’D3
D3 â†’D1
D3 â†’D2
Avg."
COMPARATIVE STUDY,0.36239782016348776,"âœ“
âœ“
âœ“
âœ“
âœ“
18.61
26.62
22.92
34.00
30.29
33.79
30.49
28.51
34.27
28.83
âœ“
âœ“
âœ“
âœ“
âœ“
83.06
93.52
48.07
40.93
43.33
43.91
51.13
41.84
52.67
55.38
âœ“
âœ“
âœ“
âœ“
âœ“
85.83
93.52
65.12
46.67
48.56
49.43
55.34
45.52
54.53
60.60
âœ“
âœ“
âœ“
âœ“
âœ“
83.89
95.80
64.89
48.53
48.25
48.96
54.21
45.52
55.73
60.64
âœ“
âœ“
âœ“
âœ“
âœ“
87.22
94.40
64.64
49.87
48.25
49.66
56.47
47.59
55.07
61.46"
COMPARATIVE STUDY,0.3651226158038147,"âœ“
âœ“
âœ“
âœ“
âœ“
âœ“
87.78
98.95
66.10
50.53
50.31
50.34
58.62
48.04
58.00
63.19"
COMPARATIVE STUDY,0.3678474114441417,"Figure 4: Domain disentanglement and transfer examples. Left: Video sequence inputs for D = P1
(â€œHuman"", â– â–¡) and D = P2 (â€œAlien"", â– â–¡). Middle: Reconstructed sequences (â– â–¡) with zD
1 , ..., zD
T .
Right: Domain transferred sequences with exchanged zD
d ."
PROPERTY ANALYSIS,0.37057220708446864,"4.4
Property Analysis"
PROPERTY ANALYSIS,0.37329700272479566,"Disentanglement Analysis. We analyze the disentanglement effect of TranSVAE on Sprites [22]
and show results in Fig. 4. The left subfigure shows the original sequences of the two domains. The
â€œHuman"" and â€œAlien"" are completely different appearances and the former is casting spells while the
latter is slashing. The middle subfigure shows the sequences reconstructed only using {zD
1 , ..., zD
T }.
It can be clearly seen that the two sequences keep the same action as the corresponding original ones.
However, if we only focus on the appearance characteristics, it is difficult to distinguish the domain
to which the sequences belong. This indicates that {zD
1 , ..., zD
T } are indeed domain-invariant and
well encode the semantic information. The right subfigure shows the sequences reconstructed by
exchanging zD
d , which results in two sequences with the same actions but exchanged appearance.
This verifies that zD
d is representing the appearance information, which is actually the domain-related
information in this example. This property study sufficiently supports that TranSVAE can successfully
disentangle the domain information from other information, with the former embedded in zD
d and the
latter embedded in {zD
1 , ..., zD
T }."
PROPERTY ANALYSIS,0.3760217983651226,Table 4: Comparison results on model complexity.
PROPERTY ANALYSIS,0.3787465940054496,"Methods
Trainable Params
MACs
FLOPs
FPS"
PROPERTY ANALYSIS,0.3814713896457766,"TA3N
7.6880 M
18.2318 G
36.4636 G
0.0134 s
CoMix
30.3688 M
18.5640 G
37.1280 G
0.0157 s
CO2A
23.6720 M
18.1884 G
36.3768 G
0.0127 s"
PROPERTY ANALYSIS,0.38419618528610355,"TranSVAE
12.7419 M
18.2657 G
36.5314 G
0.0133 s"
PROPERTY ANALYSIS,0.3869209809264305,"Complexity Analysis.
We further con-
duct complexity analysis on our TranSVAE.
Specifically, we compare the number of
trainable parameters, multiply-accumulate
operations (MACs), floating-point opera-
tions (FLOPs), and inference frame-per-
second (FPS) with existing baselines in-
cluding TA3N [7], CO2A [37], and CoMix
[31]. All the comparison results are shown in Tab. 4. From the table, we observe that TranSVAE
requires less trainable parameters than CO2A and CoMix. Although more trainable parameters are
used than TA3N, TranSVAE achieves significant adaptation performance improvement than TA3N
(see Tab. 1 and Tab. 3). Moreover, the MACs, FLOPs, and FPS are competitive among different
methods. This is reasonable since all these approaches adopt the same I3D backbone."
PROPERTY ANALYSIS,0.3896457765667575,"Ablation Study. We now analyze the effectiveness of each loss term in Eq. (14). We compare with
four variants of TranSVAE, each removing one loss term by equivalently setting the weight Î» to 0.
The ablation results on UCF-HMDB, Jester, and Epic-Kitchens are shown in Tab. 5. As can be seen,
removing Lcls significantly reduces the transfer performance in all the tasks. This is reasonable as
Lcls is used to discover the discriminative features. Removing any of Ladv, Lmi, and Lctc leads to
an inferior result than the full TranSVAE setup, and removing Ladv is the most influential. This is
because Ladv is used to explicitly reduce the temporal domain gaps. All these results demonstrate
that every proposed loss matters in our framework."
PROPERTY ANALYSIS,0.3923705722070845,"We further conduct another ablation study by sequentially integrating Lcls, Ladv, Lmi, and Lctc into
our sequential VAE structure using UCF-HMDB. We use this integration order based on the average
positive improvement that a loss brings to TranSVAE as shown in Tab. 5. We also take advantage of
t-SNE [39] to visualize the features learned by these different variants. We plot two sets of t-SNE"
PROPERTY ANALYSIS,0.39509536784741145,ğŸ—ğŸ“. ğŸğŸ
PROPERTY ANALYSIS,0.3978201634877384,ğŸ–ğŸ. ğŸ“ğŸ(+ğŸ. ğŸğŸ‘)
PROPERTY ANALYSIS,0.40054495912806537,ğŸ–ğŸ. ğŸğŸ•
PROPERTY ANALYSIS,0.4032697547683924,ğŸ–ğŸ’. ğŸ’ğŸ’(+ğŸ’. ğŸğŸ•)
PROPERTY ANALYSIS,0.40599455040871935,ğŸ–ğŸ“. ğŸ“ğŸ”(+ğŸ“. ğŸğŸ—)
PROPERTY ANALYSIS,0.4087193460490463,ğŸ–ğŸ•. ğŸğŸ(+ğŸ”. ğŸ—ğŸ“)
PROPERTY ANALYSIS,0.4114441416893733,ğŸ–ğŸ•. ğŸ•ğŸ–(+ğŸ•. ğŸ”ğŸ)
PROPERTY ANALYSIS,0.4141689373297003,"(a)
(b)
(c)
(d)
(e) ğ’®+,&-"
PROPERTY ANALYSIS,0.41689373297002724,"(a)
âœ“
âœ“"
PROPERTY ANALYSIS,0.4196185286103542,"(b)
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.4223433242506812,"(c)
âœ“
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.4250681198910082,"(d)
âœ“
âœ“
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.42779291553133514,"(e)
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“ ğ’¯!./"
PROPERTY ANALYSIS,0.4305177111716621,"#
â„’!""#$ â„’%&! â„’#'"" â„’() â„’%*%
PL
Accuracy (%)"
PROPERTY ANALYSIS,0.4332425068119891,"Figure 5: Loss integration studies on U â†’H. Left: The t-SNE plots for class-wise (top row) and
domain (bottom row, red source & blue target) features. Right: Ablation results (%) by adding each
loss sequentially, i.e., row (a) - row (e)."
PROPERTY ANALYSIS,0.4359673024523161,ğŸ–ğŸ–. ğŸ•ğŸ—
PROPERTY ANALYSIS,0.43869209809264303,9ğŸ. ğŸğŸ—(+ğŸ‘. ğŸ“ğŸ)
PROPERTY ANALYSIS,0.44141689373297005,93. ğŸ“ğŸ(+ğŸ’. ğŸ•ğŸ‘)
PROPERTY ANALYSIS,0.444141689373297,93. ğŸ•ğŸ(+ğŸ’. ğŸ—ğŸ)
PROPERTY ANALYSIS,0.44686648501362397,94. ğŸ’ğŸ(+ğŸ“. ğŸ”ğŸ)
PROPERTY ANALYSIS,0.44959128065395093,98. ğŸ—ğŸ“(+ğŸğŸ. ğŸğŸ”)
PROPERTY ANALYSIS,0.45231607629427795,"(a)
(b)
(c)
(d)
(e)
#
â„’!""#$ â„’%&! â„’#'"" â„’() â„’%*%
PL
Accuracy (%)"
PROPERTY ANALYSIS,0.4550408719346049,"ğŸ—ğŸ”. ğŸ–ğŸ“ ğ’®+,&-"
PROPERTY ANALYSIS,0.45776566757493187,"(a)
âœ“
âœ“"
PROPERTY ANALYSIS,0.4604904632152589,"(b)
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.46321525885558584,"(c)
âœ“
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.4659400544959128,"(d)
âœ“
âœ“
âœ“
âœ“
âœ“"
PROPERTY ANALYSIS,0.46866485013623976,"(e)
âœ“
âœ“
âœ“
âœ“
âœ“
âœ“ ğ’¯!./"
PROPERTY ANALYSIS,0.4713896457765668,"Figure 6: Loss integration studies on H â†’U. Left: The t-SNE plots for class-wise (top row) and
domain (bottom row, red source & blue target) features. Right: Ablation results (%) by adding each
loss sequentially, i.e., row (a) - row (e)."
PROPERTY ANALYSIS,0.47411444141689374,"figures, one using the class-wise label and another using the domain label. Fig. 5 and Fig. 6 show
the visualization and the quantitative results. As can be seen from the t-SNE feature visualizations,
adding a new component improves both the domain and semantic alignments, and the best alignment
is achieved when all the components are considered. The quantitative results further show that the
transfer performance gradually increases with the sequential integration of the four components,
which again verifies the effectiveness of each component in TranSVAE. More ablation study results
can be found in the Appendix."
CONCLUSION AND LIMITATION,0.4768392370572207,"5
Conclusion and Limitation"
CONCLUSION AND LIMITATION,0.47956403269754766,"In this paper, we proposed a TranSVAE framework for video-based UDA tasks. Our key idea
is to explicitly disentangle the domain information from other information during the adaptation.
We developed a novel sequential VAE structure with two sets of latent factors and proposed four
constraints to regulate these factors for adaptation purposes. Note that disentanglement and adaptation
are interactive and complementary. All the constraints serve to achieve a good disentanglement effect
with the two-level domain divergence minimization. Extensive empirical studies clearly verify that
TranSVAE consistently offers performance improvements compared with existing SoTA video-based
UDA methods. We also find that TranSVAE outperforms those multi-modal UDA methods, although
it only uses single-modality data. Comprehensive property analysis further shows that TranSVAE is
an effective and promising method for video-based UDA."
CONCLUSION AND LIMITATION,0.4822888283378747,"We further discuss the limitations of the TranSVAE framework. These are also promising future
directions. Firstly, the empirical evaluation is mainly on the action recognition task. The performance
of TranSVAE on other video-related tasks, e.g. video segmentation, is not tested. Secondly, TranSVAE
is only evaluated on the typical two-domain transfer scenario. The multi-source transfer case is not
considered but is worthy of further study. Thirdly, although TranSVAE exhibits better performance
than multi-modal transfer methods, its current version does not consider multi-modal data functionally
and structurally. An improved TranSVAE with the capacity of using multi-modal data is expected to
further boost the adaptation performance. Fourthly, current empirical evaluations are mainly based
on the I3D backbone, more advanced backbones, e.g., [36, 40], are expected to be explored for
further improvement. Lastly, TranSVAE handles the spatial divergence by disentangling the static
domain-specific latent factors out. However, it may happen that spatial divergence is not completely
captured by the static latent factors due to insufficient disentanglement."
REFERENCES,0.48501362397820164,References
REFERENCES,0.4877384196185286,"[1] Junwen Bai, Weiran Wang, and Carla P Gomes. Contrastively disentangled sequential variational autoen-
coder. In Advances in Neural Information Processing Systems, volume 34, pages 10105â€“10118, 2021. 3,
4"
REFERENCES,0.4904632152588556,"[2] Lejla Batina, Benedikt Gierlichs, Emmanuel Prouff, Matthieu Rivain, FranÃ§ois-Xavier Standaert, and
Nicolas Veyrat-Charvillon. Mutual information analysis: a comprehensive study. Journal of Cryptology,
24(2):269â€“291, 2011. 5"
REFERENCES,0.49318801089918257,"[3] Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, and Ting Yao. Exploring object
relation in mean teacher for cross-domain detection. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 11457â€“11466, 2019. 1"
REFERENCES,0.49591280653950953,"[4] Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentangled
semantic representation for domain adaptation. In International Joint Conferences on Artificial Intelligence,
volume 2019, page 2060, 2019. 3"
REFERENCES,0.4986376021798365,"[5] Ruichu Cai, Fengzhu Wu, Zijian Li, Pengfei Wei, Lingling Yi, and Kun Zhang. Graph domain adaptation:
A generative view. arXiv preprint arXiv:2106.07482, 2021. 3"
REFERENCES,0.5013623978201635,"[6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.
In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6299â€“6308, 2017. 7, 8"
REFERENCES,0.5040871934604905,"[7] Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng. Temporal
attentive alignment for large-scale video domain adaptation. In IEEE/CVF International Conference on
Computer Vision, pages 6321â€“6330, 2019. 2, 3, 6, 7, 8, 9, 17"
REFERENCES,0.5068119891008175,"[8] Peipeng Chen, Yuan Gao, and Andy J Ma. Multi-level attentive adversarial learning with temporal dilation
for unsupervised video domain adaptation. In IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 1259â€“1268, 2022. 2, 3, 8"
REFERENCES,0.5095367847411444,"[9] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement
in variational autoencoders. In Advances in Neural Information Processing Systems, volume 31, pages
1â€“11, 2018. 5"
REFERENCES,0.5122615803814714,"[10] Jinwoo Choi, Gaurav Sharma, Samuel Schulter, and Jia-Bin Huang. Shuffle and attend: Video domain
adaptation. In European Conference on Computer Vision, pages 678â€“695. Springer, 2020. 2, 3, 8, 14"
REFERENCES,0.5149863760217984,"[11] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos
Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision:
The epic-kitchens dataset. In European Conference on Computer Vision, pages 720â€“736. Springer, 2018.
7, 14"
REFERENCES,0.5177111716621253,"[12] Avijit Dasgupta, CV Jawahar, and Karteek Alahari. Overcoming label noise for source-free unsupervised
video domain adaptation. In Proceedings of the Thirteenth Indian Conference on Computer Vision,
Graphics and Image Processing, pages 1â€“9, 2022. 3, 8"
REFERENCES,0.5204359673024523,"[13] Wanxia Deng, Lingjun Zhao, Qing Liao, Deke Guo, Gangyao Kuang, Dewen Hu, Matti PietikÃ¤inen, and
Li Liu. Informative feature disentanglement for unsupervised domain adaptation. IEEE Transactions on
Multimedia, 24:2407â€“2421, 2021. 3"
REFERENCES,0.5231607629427792,"[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of
Machine Learning Research, 17(1):2096â€“2030, 2016. 6, 7, 8"
REFERENCES,0.5258855585831063,"[15] Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, and Yanpeng Cao. Uncertainty-aware unsupervised
domain adaptation in object detection. IEEE Transactions on Multimedia, 24:2502â€“2514, 2021. 1"
REFERENCES,0.5286103542234333,"[16] Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant
variational autoencoders. In Medical Imaging with Deep Learning, pages 322â€“348. PMLR, 2020. 3"
REFERENCES,0.5313351498637602,"[17] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,
Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv
preprint arXiv:1705.06950, 2017. 7"
REFERENCES,0.5340599455040872,"[18] Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, and Manmohan
Chandraker. Learning cross-modal contrastive features for video domain adaptation. In IEEE/CVF
International Conference on Computer Vision, pages 13618â€“13627, 2021. 3, 8"
REFERENCES,0.5367847411444142,"[19] Lingdong Kong, Niamul Quader, and Venice Erin Liong. Conda: Unsupervised domain adaptation for
lidar segmentation via regularized domain concatenation. In IEEE International Conference on Robotics
and Automation, pages 9338â€“9345, 2023. 1"
REFERENCES,0.5395095367847411,"[20] Hildegard Kuehne, Hueihan Jhuang, EstÃ­baliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large
video database for human motion recognition. In IEEE/CVF International Conference on Computer Vision,
pages 2556â€“2563. IEEE, 2011. 7, 14"
REFERENCES,0.5422343324250681,"[21] Yanghao Li, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Jiaying Liu. Adaptive batch normalization for
practical domain adaptation. Pattern Recognition, 80:109â€“117, 2018. 8"
REFERENCES,0.5449591280653951,"[22] Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. In International Conference on
Machine Learning, pages 5670â€“5679. PMLR, 2018. 3, 4, 7, 9, 14, 19"
REFERENCES,0.547683923705722,"[23] Wei Lin, Anna Kukleva, Kunyang Sun, Horst Possegger, Hilde Kuehne, and Horst Bischof. Cycda:
Unsupervised cycle domain adaptation to learn from image to video. In European Conference on Computer
Vision, pages 698â€“715. Springer, 2022. 3, 8"
REFERENCES,0.5504087193460491,"[24] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and
Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. arXiv preprint
arXiv:2306.09347, 2023. 1"
REFERENCES,0.553133514986376,"[25] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International Conference on Machine Learning, pages 2208â€“2217. PMLR, 2017.
8"
REFERENCES,0.555858310626703,"[26] Yadan Luo, Zi Huang, Zijian Wang, Zheng Zhang, and Mahsa Baktashmotlagh. Adversarial bipartite graph
learning for video domain adaptation. In ACM International Conference on Multimedia, pages 19â€“27,
2020. 2, 3, 8"
REFERENCES,0.55858310626703,"[27] Joanna Materzynska, Guillaume Berger, Ingo Bax, and Roland Memisevic. The jester dataset: A large-scale
video dataset of human gestures. In IEEE/CVF International Conference on Computer Vision Workshops,
pages 1â€“12, 2019. 7, 14"
REFERENCES,0.5613079019073569,"[28] Jonathan Munro and Dima Damen. Multi-modal domain adaptation for fine-grained action recognition. In
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 122â€“132, 2020. 3, 7, 8"
REFERENCES,0.5640326975476839,"[29] Boxiao Pan, Zhangjie Cao, Ehsan Adeli, and Juan Carlos Niebles. Adversarial cross-domain action
recognition with co-attention. In AAAI Conference on Artificial Intelligence, pages 11815â€“11822, 2020. 2,
3, 7, 8"
REFERENCES,0.5667574931880109,"[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems, 2019. 7"
REFERENCES,0.5694822888283378,"[31] Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, and Abir Das. Contrast and mix: Tempo-
ral contrastive video domain adaptation with background mixing. In Advances in Neural Information
Processing Systems, volume 34, pages 23386â€“23400, 2021. 2, 3, 7, 8, 9"
REFERENCES,0.5722070844686649,"[32] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrep-
ancy for unsupervised domain adaptation. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3723â€“3732, 2018. 8"
REFERENCES,0.5749318801089919,"[33] Antoine Saporta, Arthur Douillard, Tuan-Hung Vu, Patrick PÃ©rez, and Matthieu Cord. Multi-head distilla-
tion for continual unsupervised domain adaptation in semantic segmentation. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 3751â€“3760, 2022. 1"
REFERENCES,0.5776566757493188,"[34] Xiaolin Song, Sicheng Zhao, Jingyu Yang, Huanjing Yue, Pengfei Xu, Runbo Hu, and Hua Chai. Spatio-
temporal contrastive domain adaptation for action recognition. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9787â€“9795, 2021. 3, 8"
REFERENCES,0.5803814713896458,"[35] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions
classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 7, 14"
REFERENCES,0.5831062670299727,"[36] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient
learners for self-supervised video pre-training. Advances in Neural Information Processing Systems,
35:10078â€“10093, 2022. 7, 10"
REFERENCES,0.5858310626702997,"[37] Victor G Turrisi, Giacomo Zara, Paolo Rota, Thiago Oliveira-Santos, Nicu Sebe, Vittorio Murino, and
Elisa Ricci. Dual-head contrastive domain adaptation for video action recognition. In IEEE/CVF Winter
Conference on Applications of Computer Vision, pages 2234â€“2243, 2022. 2, 3, 7, 8, 9"
REFERENCES,0.5885558583106267,"[38] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation.
In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7167â€“7176, 2017. 1, 8"
REFERENCES,0.5912806539509536,"[39] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9(11), 2008. 9"
REFERENCES,0.5940054495912807,"[40] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.
Videomae v2: Scaling video masked autoencoders with dual masking. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14549â€“14560, 2023. 7, 10"
REFERENCES,0.5967302452316077,"[41] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions
on Intelligent Systems and Technology, 11(5):1â€“46, 2020. 1"
REFERENCES,0.5994550408719346,"[42] Ni Xiao and Lei Zhang. Dynamic weighted learning for unsupervised domain adaptation. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 15242â€“15251, 2021. 1"
REFERENCES,0.6021798365122616,"[43] Lijin Yang, Yifei Huang, Yusuke Sugano, and Yoichi Sato. Interact before align: Leveraging cross-modal
knowledge for domain adaptive action recognition. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 14722â€“14732, 2022. 3, 8"
REFERENCES,0.6049046321525886,"[44] Yuehao Yin, Bin Zhu, Jingjing Chen, Lechao Cheng, and Yu-Gang Jiang. Mix-dann and dynamic-
modal-distillation for video domain adaptation. In ACM International Conference on Multimedia, pages
3224â€“3233, 2022. 3, 8"
REFERENCES,0.6076294277929155,"[45] Fuxun Yu, Di Wang, Yinpeng Chen, Nikolaos Karianakis, Tong Shen, Pei Yu, Dimitrios Lymberopoulos,
Sidi Lu, Weisong Shi, and Xiang Chen. Sc-uda: Style and content gaps aware unsupervised domain
adaptation for object detection. In IEEE/CVF Winter Conference on Applications of Computer Vision,
pages 382â€“391, 2022. 1"
REFERENCES,0.6103542234332425,"[46] Yang Yu, Zhiqiang Gong, Ping Zhong, and Jiaxin Shan. Unsupervised representation learning with deep
convolutional neural network for remote sensing images. In International Conference on Image and
Graphics, pages 97â€“108. Springer, 2017. 7"
REFERENCES,0.6130790190735694,"[47] Jingyi Zhang, Jiaxing Huang, Zichen Tian, and Shijian Lu. Spectral unsupervised domain adaptation
for visual recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
9829â€“9840, 2022. 1"
REFERENCES,0.6158038147138964,"[48] Yunhua Zhang, Hazel Doughty, Ling Shao, and Cees GM Snoek. Audio-adaptive activity recognition
across video domains. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
13791â€“13800, 2022. 3, 8"
REFERENCES,0.6185286103542235,"[49] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos.
In European Conference on Computer Vision, pages 803â€“818. Springer, 2018. 6"
REFERENCES,0.6212534059945504,"[50] Yizhe Zhu, Martin Renqiang Min, Asim Kadav, and Hans Peter Graf. S3vae: Self-supervised sequential
vae for representation disentanglement and data generation. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 6538â€“6547, 2020. 3, 4"
REFERENCES,0.6239782016348774,"[51] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic
segmentation via class-balanced self-training. In European Conference on Computer Vision, pages 289â€“305.
Springer, 2018. 1"
REFERENCES,0.6267029972752044,Appendix
REFERENCES,0.6294277929155313,"In this appendix, we supplement the following materials to support the findings and conclusions
drawn in the main body of this paper:"
REFERENCES,0.6321525885558583,â€¢ Sec. A provides additional implementation details to assist reproductions.
REFERENCES,0.6348773841961853,â€¢ Sec. B contains additional comparative and ablation results of the TranSVAE framework.
REFERENCES,0.6376021798365122,â€¢ Sec. C elaborates on the broader impact of this work.
REFERENCES,0.6403269754768393,â€¢ Sec. D acknowledges the public resources used during the course of this work.
REFERENCES,0.6430517711171662,"A
Additional Implementation Details"
REFERENCES,0.6457765667574932,"In this section, we provide more implementation details including the I3D feature extraction proce-
dure, the concrete model architecture, and the hyperparameter selection in our proposed TranSVAE
framework. We also provide detailed instructions for our live demo."
REFERENCES,0.6485013623978202,"A.1
Notations"
REFERENCES,0.6512261580381471,We present Tab. A to summarize the notations used in this paper.
REFERENCES,0.6539509536784741,"A.2
I3D Feature Extractions"
REFERENCES,0.6566757493188011,"We extract the I3D RGB features following the routine described in SAVA [10]. Given a video
sequence, 16 frames along clips are sampled by sliding a temporal window with a temporal stride
of 1. Specifically, for each frame in the video, the temporal window consists of its previous seven
frames and the following eight frames. Zero padding is used for the beginning and the end of the
video. We then feed the sliding windows to the I3D backbone to extract features, which results in a
1024-dimensional feature vector for each frame of the video."
REFERENCES,0.659400544959128,"A.3
Model Architecture"
REFERENCES,0.662125340599455,"We now provide the detailed model architecture of our TranSVAE. In Fig. A, we show the model
with the image as the input, where the encoder and decoder are more complex convolutional and
deconvolutional layers. For the model with the RGB features as the input, we can simply replace
the encoder and decoder with fully connected linear layers. Note that the dimensionality of all the
modules shown above is uniformly applied in all the experiments."
REFERENCES,0.6648501362397821,"A.4
Hyperparameter Selection"
REFERENCES,0.667574931880109,"There are several hyperparameters used in TranSVAE, including the balancing weights Î»1, Î»2, Î»3, Î»4,
the number of the video frames T, and the confidence threshold Î· for generating target pseudo-labels.
For Î»1 to Î»4, we select from the value set {1eâˆ’3, 1eâˆ’2, 1eâˆ’1, 0.5, 1, 5, 10, 50, 100, 1000}. For T, we
select from {5, 6, 7, 8, 9, 10}. For Î·, we set its value range from 0.9 to 1.0 with a step of 0.01."
REFERENCES,0.670299727520436,"We set a high-value range of Î· to ensure a high confidence score on the correctness of the target
pseudo-labels. Following the common protocol used in video-based UDA, we conduct an extensive
grid search regarding these hyperparameters on the validation set of each transfer task. Tab. B
summarizes the exact used values of these hyperparameters for the UCF-HMDB [35, 20], Jester [27],
and Epic-Kitchens [11] UDA benchmarks. For the Sprites [22] dataset, we do not do a hyperparameter
search as the data is quite simple. We simply set T as 8, which is the original length of the video
sequence. The confidence threshold is set to be 0.99, and Î»1 to Î»4 are all set to be 1."
REFERENCES,0.6730245231607629,"A.5
Demo Instruction"
REFERENCES,0.6757493188010899,"As mentioned in the main body, we include a live demo for our TranSVAE framework. This demo
can be accessed at: https://huggingface.co/spaces/ldkong/TranSVAE. Here we include the
detailed instructions for playing with this demo."
REFERENCES,0.6784741144414169,Table A: Summary of notations used in this paper.
REFERENCES,0.6811989100817438,"Notation
Description"
REFERENCES,0.6839237057220708,"D
Domain"
REFERENCES,0.6866485013623979,"S/T
Source domain / Target domain"
REFERENCES,0.6893732970027248,"VD
A video sequence from domain D"
REFERENCES,0.6920980926430518,"VD
i , yD
i
The i-th video sequence and the corresponding action label of domain D"
REFERENCES,0.6948228882833788,"xD
i , ..., xD
T
T frames of images in the video sequence VD"
REFERENCES,0.6975476839237057,"zD
i , ..., zD
T
The dynamic latent factors of the video sequence from domain D"
REFERENCES,0.7002724795640327,"zD
d
The static latent factors of the video sequence from domain D"
REFERENCES,0.7029972752043597,"xD
<t
The input sequence before timestamp t"
REFERENCES,0.7057220708446866,"xD
1:T
The full input sequence from t = 1 to t = T."
REFERENCES,0.7084468664850136,"Table B: Summary of the best-possible hyperparameter values of the TranSVAE framework for each
UDA task in our experiments after an extensive grid search."
REFERENCES,0.7111716621253406,"Task
Backbone
T
Î·
Î»1
Î»2
Î»3
Î»4"
REFERENCES,0.7138964577656676,"U â†’H
I3D
8
0.93
50
1
1
1"
REFERENCES,0.7166212534059946,"H â†’U
I3D
9
0.96
0.5
0.1
10
1"
REFERENCES,0.7193460490463215,"JS â†’JT
I3D
6
0.95
0.001
10
100
10"
REFERENCES,0.7220708446866485,"D1 â†’D2
I3D
9
0.96
50
10
5
100"
REFERENCES,0.7247956403269755,"D1 â†’D3
I3D
10
1
1
0.5
0.1
100"
REFERENCES,0.7275204359673024,"D2 â†’D1
I3D
8
0.93
100
10
5
100"
REFERENCES,0.7302452316076294,"D2 â†’D3
I3D
10
0.91
0.5
10
50
100"
REFERENCES,0.7329700272479565,"D3 â†’D1
I3D
8
0.91
100
10
50
100"
REFERENCES,0.7356948228882834,"D3 â†’D2
I3D
9
0.91
1000
1
0.1
100"
REFERENCES,0.7384196185286104,"This demo is built upon Hugging Face Spaces4, which provides concise and easy-to-use live demo
interfaces. Our demo consists of one input interface and one output interface as shown in Fig. B.
Specifically, the appearances of the Sprites avatars are fully controlled by four attributes, i.e., body,
hair color, top wear, and bottom wear. We construct two domains, P1 and P2. P1 uses the
â€œHumanâ€ body while P1 uses the â€œAlien"" body. The attribute pools of â€œHuman"" and â€œAlien"" are
non-overlapping across domains, resulting in completely heterogeneous P1 and P2. Each video
sequence contains eight frames in total."
REFERENCES,0.7411444141689373,"For conducting domain disentanglement and transfer with TranSVAE, users are free to choose the
action and the appearance of the avatars on the left-hand side of the interface. Next, simply click the
â€œSubmit"" button and the adaptation results will display on the right-hand side of the interface in a few
seconds. The outputs include:"
REFERENCES,0.7438692098092643,"â€¢ The 1st column: The original input of the â€œHuman"" and â€œAlien"" avatars, i.e., {xP1
1 , ..., xP1
8 }
and {xP2
1 , ..., xP2
8 };"
REFERENCES,0.7465940054495913,"â€¢ The 2nd column: The reconstructed â€œHuman"" and â€œAlien"" avatars {exP1
1 , ..., exP1
8 } and
{exP2
1 , ..., exP2
8 };"
REFERENCES,0.7493188010899182,"â€¢ The 3rd column: The reconstructed â€œHuman"" and â€œAlien"" avatars using only {zD
1 , ..., zD
8 },
D âˆˆ{P1, P2}, which are domain-invariant;"
REFERENCES,0.7520435967302452,4https://huggingface.co
REFERENCES,0.7547683923705722,TranSVAE(
REFERENCES,0.7574931880108992,(encoder): encoder(
REFERENCES,0.7602179836512262,(c1): dcgan_conv(
REFERENCES,0.7629427792915532,(main): Sequential(
REFERENCES,0.7656675749318801,"(0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(c2): dcgan_conv( (main): Sequential("
REFERENCES,0.7683923705722071,"(0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(c3): dcgan_conv( (main): Sequential("
REFERENCES,0.771117166212534,"(0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(c4): dcgan_conv( (main): Sequential("
REFERENCES,0.773841961852861,"(0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(c5): Sequential("
REFERENCES,0.776566757493188,"(0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(1, 1))
(1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
(2): Tanh())
)
(decoder): decoder_woSkip"
REFERENCES,0.779291553133515,(upc1): Sequential(
REFERENCES,0.782016348773842,"(0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(1, 1))
(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) 
(upc2): dcgan_upconv( (main): Sequential("
REFERENCES,0.784741144414169,"(0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(upc3): dcgan_upconv( (main): Sequential("
REFERENCES,0.7874659400544959,"(0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(upc4): dcgan_upconv( (main): Sequential("
REFERENCES,0.7901907356948229,"(0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(2): LeakyReLU(negative_slope=0.2, inplace=True) ) )
(upc5): Sequential("
REFERENCES,0.7929155313351499,"(0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
(1): Sigmoid() ) 
)
(relu): LeakyReLU(negative_slope=0.1) 
(dropout_f): Dropout(p=0.5, inplace=False) 
(dropout_v): Dropout(p=0.5, inplace=False) 
(z_prior_lstm_ly1): LSTMCell(512, 512) 
(z_prior_lstm_ly2): LSTMCell(512, 512) 
(z_prior_mean): Linear(in_features=512, out_features=512, bias=True) 
(z_prior_logvar): Linear(in_features=512, out_features=512, bias=True) 
(z_lstm): LSTM(1024, 512, batch_first=True, bidirectional=True) 
(f_mean): Linear(in_features=1024, out_features=512, bias=True) 
(f_logvar): Linear(in_features=1024, out_features=512, bias=True) 
(z_rnn): RNN(1024, 512, batch_first=True) 
(z_mean): Linear(in_features=512, out_features=512, bias=True) 
(z_logvar): Linear(in_features=512, out_features=512, bias=True) 
(fc_feature_domain_frame): Linear(in_features=512, out_features=512, bias=True) 
(fc_classifier_domain_frame): Linear(in_features=512, out_features=2, bias=True) 
(TRN)
(bn_trn_S): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) 
(bn_trn_T): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(fc_feature_domain_video): Linear(in_features=256, out_features=256, bias=True) 
(fc_classifier_domain_video): Linear(in_features=256, out_features=2, bias=True)
(relation_domain_classifier_all): ModuleList("
REFERENCES,0.7956403269754768,(0): Sequential(
REFERENCES,0.7983651226158038,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) ) 
(1): Sequential("
REFERENCES,0.8010899182561307,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) )
(2): Sequential("
REFERENCES,0.8038147138964578,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) ) 
(3): Sequential("
REFERENCES,0.8065395095367848,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) ) 
(4): Sequential("
REFERENCES,0.8092643051771117,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) )
(5): Sequential("
REFERENCES,0.8119891008174387,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) ) 
(6): Sequential("
REFERENCES,0.8147138964577657,"(0): Linear(in_features=256, out_features=256, bias=True) 
(1): ReLU() 
(2): Linear(in_features=256, out_features=2, bias=True) ) 
)
(pred_classifier_video): Linear(in_features=256, out_features=15, bias=True) 
(fc_feature_domain_latent): Linear(in_features=512, out_features=512, bias=True) 
(fc_classifier_doamin_latent): Linear(in_features=512, out_features=2, bias=True)
)"
REFERENCES,0.8174386920980926,"Figure A: The neural network structure (left) and a Netron graph (right) of the proposed TranSVAE
framework. Zoom-ed in for the details."
REFERENCES,0.8201634877384196,"Figure B: The input (left) and output (right) interfaces of our live demo. Users are free to customize
the actions and appearances of the source and target inputs (i.e., the â€œHuman"" and â€œAlien"" avatars) in
the left-hand side and use them for disentanglement and transfer as shown in the right-hand side."
REFERENCES,0.8228882833787466,Table C: Ablation results without and with disentanglement term.
REFERENCES,0.8256130790190735,"Lsvae
Lcls
Ladv
Lmi
Lctc
U â†’H
H â†’U"
REFERENCES,0.8283378746594006,"âœ“
âœ“
81.67
90.54
âœ“
âœ“
79.44
90.37
âœ“
âœ“
81.11
90.37
âœ“
âœ“
âœ“
âœ“
82.22
91.07"
REFERENCES,0.8310626702997275,"âœ“
âœ“
âœ“
84.44
93.52
âœ“
âœ“
âœ“
83.06
93.70
âœ“
âœ“
âœ“
83.61
91.42
âœ“
âœ“
âœ“
âœ“
âœ“
87.78
98.95"
REFERENCES,0.8337874659400545,"â€¢ The 4th column: The reconstructed â€œHuman"" and â€œAlien"" avatars by exchanging zD
d , which
results in two sequences with the same actions but exchanged appearance, i.e., domain
disentanglement and transfer."
REFERENCES,0.8365122615803815,"B
Additional Experimental Results"
REFERENCES,0.8392370572207084,"In this section, we provide additional quantitative and qualitative results for our TranSVAE framework."
REFERENCES,0.8419618528610354,"B.1
Additional Ablation Studies"
REFERENCES,0.8446866485013624,"Component Integration Study. We further analyze the effect of each loss term without disentan-
glement, i.e., not adding Lsvae, on the UCF-HMDB dataset, as shown in Tab. C. Without Lsvae, the
framework is to learn representations with corresponding constraints as many existing video-based
UDA methods do. Precisely, we have the following remarks for the top-half table:"
REFERENCES,0.8474114441416893,"â€¢ Only with Ladv, the framework is equivalent to TA3N [7], which learns the domain-invariant
temporal representation. We can see the adaptation results are similar to that of [7]."
REFERENCES,0.8501362397820164,"â€¢ Lmi is a term specifically designed for disentanglement purposes. Thus, the effect of using
Lmi alone for adaptation is random as verified in the above table, achieving worse results"
REFERENCES,0.8528610354223434,Table D: Ablation results for the frame number T.
REFERENCES,0.8555858310626703,"Task
5
6
7
8
9
10
12
14
16
20"
REFERENCES,0.8583106267029973,"U â†’H
84.44
86.11
84.17
87.78
84.72
85.28
83.89
84.44
82.78
85.00"
REFERENCES,0.8610354223433242,"H â†’U
96.85
94.22
98.42
94.75
98.95
93.70
93.87
94.40
94.05
94.40"
REFERENCES,0.8637602179836512,Table E: Ablation results for the pseudo-label threshold Î·.
REFERENCES,0.8664850136239782,"Task
w/o
0.90
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99"
REFERENCES,0.8692098092643051,"U â†’H
87.22 (âˆ’0.56)
86.94
87.22
87.50
87.78
86.11
86.67
86.94
86.39
86.11
86.39"
REFERENCES,0.8719346049046321,"H â†’U
94.40 (âˆ’4.55)
98.42
97.37
98.77
98.60
98.25
98.25
98.95
97.90
97.72
95.27"
REFERENCES,0.8746594005449592,"in the U â†’H task and better results in the H â†’U tasks compared with the source-only
baseline."
REFERENCES,0.8773841961852861,"â€¢ Lctc enforces the learned static representation to be domain-specific and brings indirect
benefit to adaptation, thus obtaining slightly better results than the source-only baseline."
REFERENCES,0.8801089918256131,"In sum, without Lsvae, the adaptation performance of each loss term alone is far from optimal.
Moreover, combining all the terms can slightly improve the adaptation performance over the individual
ones, however, is still obviously worse than TranSVAE results. The above ablation study shows the
necessity of the disentanglement in the TranSVAE framework and proves that Lsvae is an indispensable
loss term."
REFERENCES,0.8828337874659401,"Regarding the bottom half table showing the results with Ladv, we have the following remarks:"
REFERENCES,0.885558583106267,"â€¢ Each individual term with Ladv achieves better adaptation results than the source-only
baseline."
REFERENCES,0.888283378746594,"â€¢ Each individual term with Ladv consistently outperforms the corresponding ones without
Ladv."
REFERENCES,0.8910081743869209,"â€¢ The final TranSVAE framework combined all the loss terms yields much better results than
each individual case."
REFERENCES,0.8937329700272479,"This set of ablation studies indicates that 1) each term in TranSVAE brings positive effects to adapta-
tion, and 2) disentanglement and adaptation are interactive and complementary in our framework to
obtain the best possible UDA results."
REFERENCES,0.896457765667575,"Number of Frames T. Tab. D shows the transfer performance with the variation of the number of
frames T on the UCF-HMDB dataset. As can be seen, the two tasks achieve the optimal performance
with different T, specifically T = 8 for U â†’H and T = 9 for H â†’U. Based on this observation,
we apply a grid search on the validation set to obtain the optimal T for each task in our experiments."
REFERENCES,0.8991825613079019,"Target Pseudo-Label Threshold Î·. We show the sensitivity analyses of the transfer performance
with respect to the target pseudo label threshold Î· on the UCF-HMDB dataset in Tab. E. The results
show that different tasks yield the best transfer result with different Î·, specifically Î· = 0.93 for
U â†’H and Î· = 0.96 for H â†’U. Thus, we also apply the grid search on the validation set to
obtain the optimal Î· for each task."
REFERENCES,0.9019073569482289,"B.2
Additional Qualitative Results"
REFERENCES,0.9046321525885559,"For those who cannot access our live demo, we have included more qualitative examples for domain
disentanglement and transfer in Fig. C. We also provide a link5 for GIFs demonstrating various
disentanglement and reconstruction results."
REFERENCES,0.9073569482288828,"In the GIFs link, we show two cases, the first two columns for the first one and the last two columns
for the second one. Each case contains a source and a target cartoon character performing an action.
For each row, we have the following remark:"
REFERENCES,0.9100817438692098,5https://github.com/ldkong1205/TranSVAE#ablation-study
REFERENCES,0.9128065395095368,Input Sequence
REFERENCES,0.9155313351498637,"â€¢ Left: â€œHumanâ€, slash, ğ±ğ!"
REFERENCES,0.9182561307901907,"â€¢ Right: â€œAlienâ€, walk, ğ±ğ"""
REFERENCES,0.9209809264305178,Reconstruction
REFERENCES,0.9237057220708447,"â€¢ Left: â€œHumanâ€, slash, ""ğ±ğ!"
REFERENCES,0.9264305177111717,"â€¢ Right: â€œAlienâ€, walk, ""ğ±ğ"""
REFERENCES,0.9291553133514986,Domain Disentanglement
REFERENCES,0.9318801089918256,"â€¢ Left: â€œNullâ€, slash"
REFERENCES,0.9346049046321526,"â€¢ Right: â€œNullâ€, walk"
REFERENCES,0.9373297002724795,Domain Transfer
REFERENCES,0.9400544959128065,"â€¢ Left: â€œAlienâ€, slash"
REFERENCES,0.9427792915531336,"â€¢ Right: â€œHumanâ€, walk"
REFERENCES,0.9455040871934605,Input Sequence
REFERENCES,0.9482288828337875,"â€¢ Left: â€œHumanâ€, spellcard, ğ±ğ!"
REFERENCES,0.9509536784741145,"â€¢ Right: â€œAlienâ€, slash, ğ±ğ"""
REFERENCES,0.9536784741144414,Reconstruction
REFERENCES,0.9564032697547684,"â€¢ Left: â€œHumanâ€, spellcard, ""ğ±ğ!"
REFERENCES,0.9591280653950953,"â€¢ Right: â€œAlienâ€, slash, ""ğ±ğ"""
REFERENCES,0.9618528610354223,Domain Disentanglement
REFERENCES,0.9645776566757494,"â€¢ Left: â€œNullâ€, spellcard"
REFERENCES,0.9673024523160763,"â€¢ Right: â€œNullâ€, slash"
REFERENCES,0.9700272479564033,Domain Transfer
REFERENCES,0.9727520435967303,"â€¢ Left: â€œAlienâ€, spellcard"
REFERENCES,0.9754768392370572,"â€¢ Right: â€œHumanâ€, slash"
REFERENCES,0.9782016348773842,"Figure C: Additional qualitative results for illustrating the domain disentanglement and transfer
properties in our TranSVAE framework."
REFERENCES,0.9809264305177112,"â€¢ The second row shows the reconstructed results of TranSVAE, i.e. zD
d and zD
t . As can be
seen, TranSVAE reconstructs the image with a high quality.
â€¢ The third row shows the reconstructed results only using the static latent factors, i.e., zD
d
and 0D
t , where we replace zD
t with zero vectors. As can be seen, the reconstructed results
are basically static containing the appearance of the character which is the main domain
difference in the Sprites dataset [22]. Specifically, we find they generally lack arms. This
is reasonable as the target action is slashing or spelling cards with arms moving, and such
dynamic information on the arm is captured by zD
t .
â€¢ The fourth row shows the reconstructed results only using the dynamic latent factors, i.e.,
0D
d and zD
t , where we replace zD
d with zero vectors. As can be seen, the reconstructed
results are performing the right action but the appearance is mixed up. This shows that zD
t
are indeed domain-invariant and contain the semantic information.
â€¢ The last row shows the reconstructed results of exchanging the dynamic latent factors
between domains, i.e., (0S
d , zT
t ) and (0T
d , zS
t ). As can be seen, the reconstructed results are
with the original appearance but perform the transferred action. This indicates the potential
of TranSVAE for some style-transfer tasks."
REFERENCES,0.9836512261580381,"C
Broader Impact"
REFERENCES,0.9863760217983651,"This paper provides a novel transfer method to use cross-domain video data, which effectively helps
reduce the annotation efforts in related video applications. Although the main empirical evaluation is
on the video action recognition task, the model structure proposed in this paper is also applicable
to other video-related tasks, such as action segmentation, video semantic segmentation, etc. More
generally, the idea of disentangling domain information sheds light on other data modality style
transfer tasks, e.g., voice conversion. The negative impacts of this work are difficult to predict.
However, as a deep model, our method shares some common pitfalls of the standard deep learning
models, e.g., demand for powerful computing resources, and vulnerability to adversarial attacks."
REFERENCES,0.989100817438692,"D
Public Resources Used"
REFERENCES,0.9918256130790191,"We acknowledge the use of the following public resources, during the course of this work:"
REFERENCES,0.9945504087193461,"â€¢ UCF1016 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Unknown
â€¢ HMDB517 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY 4.0
â€¢ Jester8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Unknown
â€¢ Epic-Kitchens9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC 4.0
â€¢ Sprites10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .CC-BY-SA-3.0
â€¢ I3D11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0
â€¢ TRN12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . BSD 2-Clause License
â€¢ CoMix13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Apache License 2.0
â€¢ TA3N14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .MIT License
â€¢ CO2A15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown
â€¢ PyTorch16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . PyTorch Custom License
â€¢ Netron17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License
â€¢ flops-counter.pytorch18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License"
REFERENCES,0.997275204359673,"6https://www.crcv.ucf.edu/data/UCF101.php
7https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database
8https://20bn.com/datasets/jester
9https://epic-kitchens.github.io/2021
10https://github.com/YingzhenLi/Sprites
11https://github.com/piergiaj/pytorch-i3d
12https://github.com/zhoubolei/TRN-pytorch
13https://github.com/CVIR/CoMix
14https://github.com/cmhungsteve/TA3N
15https://github.com/vturrisi/CO2A
16https://github.com/pytorch/pytorch
17https://github.com/lutzroeder/netron
18https://github.com/sovrasov/flops-counter.pytorch"
