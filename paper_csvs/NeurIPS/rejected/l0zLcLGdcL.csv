Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020876826722338203,"Universal domain adaptation aims to align the classes and reduce the feature
1"
ABSTRACT,0.0041753653444676405,"gap between the same category of the source and target domains. The target
2"
ABSTRACT,0.006263048016701462,"private category is set as the unknown class during the adaptation process, as it
3"
ABSTRACT,0.008350730688935281,"is not included in the source domain. However, most existing methods overlook
4"
ABSTRACT,0.010438413361169102,"the intra-class structure within a category, especially in cases where there exists
5"
ABSTRACT,0.012526096033402923,"significant concept shift between the samples belonging to the same category. When
6"
ABSTRACT,0.014613778705636743,"samples with large concept shift are forced to be pushed together, it may negatively
7"
ABSTRACT,0.016701461377870562,"affect the adaptation performance. Moreover, from the interpretability aspect, it is
8"
ABSTRACT,0.018789144050104383,"unreasonable to align visual features with significant differences, such as fighter
9"
ABSTRACT,0.020876826722338204,"jets and civil aircraft, into the same category. Unfortunately, due to such semantic
10"
ABSTRACT,0.022964509394572025,"ambiguity and annotation cost, categories are not always classified in detail, making
11"
ABSTRACT,0.025052192066805846,"it difficult for the model to perform precise adaptation. To address these issues,
12"
ABSTRACT,0.027139874739039668,"we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method
13"
ABSTRACT,0.029227557411273485,"that can learn the differences between samples belonging to the same category
14"
ABSTRACT,0.031315240083507306,"and mine sub-classes when there exists significant concept shift between them.
15"
ABSTRACT,0.033402922755741124,"By doing so, our model learns a more reasonable feature space that enhances the
16"
ABSTRACT,0.03549060542797495,"transferability and reflects the inherent differences among samples annotated as
17"
ABSTRACT,0.037578288100208766,"the same category. We evaluate the effectiveness of our MemSPM method over
18"
ABSTRACT,0.03966597077244259,"multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves
19"
ABSTRACT,0.04175365344467641,"state-of-the-art performance on four benchmarks in most cases.
20"
INTRODUCTION,0.04384133611691023,"1
Introduction
21"
INTRODUCTION,0.04592901878914405,"Unsupervised Domain Adaptation (UDA) [15, 22, 41, 44, 9, 19, 21] has become a crucial research
22"
INTRODUCTION,0.04801670146137787,"area of transfer learning, as it allows models trained on a specific dataset to be applied to related but
23"
INTRODUCTION,0.05010438413361169,"distinct domains. However, traditional UDA methods are limited by the assumption that the source
24"
INTRODUCTION,0.05219206680584551,"and target domains have to share the same label space. This assumption is problematic in real-world
25"
INTRODUCTION,0.054279749478079335,"scenarios where the target distribution is complex, open, and diverse. Universal Domain Adaptation
26"
INTRODUCTION,0.05636743215031315,"(UniDA) represents a strategy to address the limitations of traditional unsupervised domain adaptation
27"
INTRODUCTION,0.05845511482254697,"methods. In the UniDA, the target domain have a different label set than the source domain. The
28"
INTRODUCTION,0.060542797494780795,"goal is to correctly classify target domain samples belonging to the shared classes in the source label
29"
INTRODUCTION,0.06263048016701461,"set, while any samples not conforming to the source label set are treated as ""unknown"". The term
30"
INTRODUCTION,0.06471816283924843,"""universal"" characterizes UniDA as not relying on prior knowledge about the label sets of the target
31"
INTRODUCTION,0.06680584551148225,"domain. UniDA relaxes the assumption of a shared class space while aims to learn domain-invariant
32"
INTRODUCTION,0.06889352818371608,"features across a more broad range of domains.
33"
INTRODUCTION,0.0709812108559499,"Despite being widely explored, most existing universal domain adaptation methods [24, 47, 40, 39, 6,
34"
INTRODUCTION,0.07306889352818371,"34, 8, 26] overlook the internal structure intrinsically presented within each image category. These
35"
INTRODUCTION,0.07515657620041753,"methods aim to align the common classes between the source and target domains for adaptation, but
36"
INTRODUCTION,0.07724425887265135,"Figure 1: Illustration of our motivation. (a) Examples of concept shift and intra-class diversity in DA
benchmarks. For the class of alarm clock, we find that digital clock, pointer clock and alarm bell
should be set in different sub-classes. For the class of airplane, we find that images containing more
than one plane, single jetliner, and turboprop aircraft should be differently treated for adaptation.
(b) Previous methods utilize one-hot labels to guide classifying without considering the intra-class
distinction. Consequently, the model forces all samples from the same class to converge towards
a single center, disregarding the diversity in the class. Our method clusters samples with large
intra-class difference into separate sub-class, providing a more accurate representation. (c) During
domain adaptation by our design, the samples in the target domain can also be aligned near the
sub-class centers with similar features rather than just the class centers determined by labels."
INTRODUCTION,0.07933194154488518,"usually train a model to learn the class ""prototype"" representing each annotated category. This is
37"
INTRODUCTION,0.081419624217119,"particularly controversial when significant concept shift exists between samples belonging to the same
38"
INTRODUCTION,0.08350730688935282,"category. These differences can lead to sub-optimal feature learning and adaptation if the intra-class
39"
INTRODUCTION,0.08559498956158663,"structure is neglected during training. Since such kind of semantic ambiguity without fine-grained
40"
INTRODUCTION,0.08768267223382047,"category labels almost happens in all the DA benchmarks, all the methods will encounter this issue.
41"
INTRODUCTION,0.08977035490605428,"In this paper, we aim to propose a method to learn the detailed intra-class distinction and mine ""sub-
42"
INTRODUCTION,0.0918580375782881,"prototypes"" for better alignment and adaptation. This kind of sub-prototype is the further subdivision
43"
INTRODUCTION,0.09394572025052192,"of each category-level prototype, which represents the ""sub-class"" of the annotated categories. The
44"
INTRODUCTION,0.09603340292275574,"main idea of our proposed approach lies in its utilization of a learnable memory structure to learn sub-
45"
INTRODUCTION,0.09812108559498957,"prototypes for their corresponding sub-classes. This can optimize the construction and refinement of
46"
INTRODUCTION,0.10020876826722339,"the feature space, bolstering the classifier’s ability to distinguish class-wise relationships and improve
47"
INTRODUCTION,0.1022964509394572,"the model’s transferability across domains. A comparison between our proposed sub-prototypes
48"
INTRODUCTION,0.10438413361169102,"mining approach and previous methods is illustrated in Figure 1. In previous methods, samples within
49"
INTRODUCTION,0.10647181628392484,"a category were forced to be aligned together in the feature space regardless of whether there exist
50"
INTRODUCTION,0.10855949895615867,"significant differences among them because the labels were one-hot encoded. Contrastively, our
51"
INTRODUCTION,0.11064718162839249,"sub-prototypes’ feature space distinguishes sub-classes with apparent differences within the category,
52"
INTRODUCTION,0.1127348643006263,"thus improving the model’s accuracy of domain adaption and interpretability.
53"
INTRODUCTION,0.11482254697286012,"Our proposed approach, named memory-assisted sub-prototype mining (MemSPM), is inspired by the
54"
INTRODUCTION,0.11691022964509394,"memory mechanism works [17, 10, 45, 36]. In our approach, the memory generates sub-prototypes
55"
INTRODUCTION,0.11899791231732777,"that embody sub-classes learned from the source domain. During testing of the target samples,
56"
INTRODUCTION,0.12108559498956159,"the encoder produces embedding that are compared to source domain sub-prototypes learned in
57"
INTRODUCTION,0.12317327766179541,"the memory. Subsequently, a embedding for the query sample is generated through weighted sub-
58"
INTRODUCTION,0.12526096033402923,"prototype sampling in the memory. This results in reduced domain shifts before the embedding give
59"
INTRODUCTION,0.12734864300626306,"into the classifier. Our proposal of sub-prototypes mining, which are learned from the source domain
60"
INTRODUCTION,0.12943632567849686,"memory, improves the universal domain adaptation performance by promoting more refined visual
61"
INTRODUCTION,0.1315240083507307,"concept alignment.
62"
INTRODUCTION,0.1336116910229645,"MemSPM approach has been evaluated on four benchmark datasets (Office-31 [37], Office-Home [46],
63"
INTRODUCTION,0.13569937369519833,"VisDA [33],and Domain-Net [32]), under various category shift scenarios, including PDA, OSDA,
64"
INTRODUCTION,0.13778705636743216,"and UniDA. Our MemSPM method achieves state-of-the-art performance in most cases. Moreover,
65"
INTRODUCTION,0.13987473903966596,"we design a visualization module for the sub-prototype learned by our memory to demonstrate the
66"
INTRODUCTION,0.1419624217118998,"interpretability of MemSPM. Our contributions can be highlighted as follows:
67"
INTRODUCTION,0.1440501043841336,"• We study the UniDA problem from a new aspect, which focuses on the negative impacts
68"
INTRODUCTION,0.14613778705636743,"caused by overlooking the intra-class structure within a category when simply adopting
69"
INTRODUCTION,0.14822546972860126,"one-hot labels.
70"
INTRODUCTION,0.15031315240083507,"• We propose Memory-Assisted Sub-Prototype Mining(MemSPM), which explores the mem-
71"
INTRODUCTION,0.1524008350730689,"ory mechanism to learn sub-prototypes for improving the model’s adaption performance
72"
INTRODUCTION,0.1544885177453027,"and interpretability. Meanwhile, visualizations reveal the sub-prototypes stored in memory,
73"
INTRODUCTION,0.15657620041753653,"which demonstrate the interpretability of MemSPM approach.
74"
INTRODUCTION,0.15866388308977036,"• Extensive experiments on four benchmarks verify the superior performance of our proposed
75"
INTRODUCTION,0.16075156576200417,"MemSPM compared with previous works.
76"
RELATED WORK,0.162839248434238,"2
Related Work
77"
RELATED WORK,0.1649269311064718,"Closed-Set Domain Adaptation (CSDA). To mitigate the performance degradation caused by
78"
RELATED WORK,0.16701461377870563,"the closed-set domain shift, [16, 29, 48] introduce adversarial learning methods with the domain
79"
RELATED WORK,0.16910229645093947,"discriminator, aiming to minimize the domain gap between source and target domains. Beyond
80"
RELATED WORK,0.17118997912317327,"the use of the additional domain discriminator, some studies [41, 23, 50, 30, 13] have explored the
81"
RELATED WORK,0.1732776617954071,"use of two task-specific classifiers, otherwise referred to as bi-classifier, to implicitly achieve the
82"
RELATED WORK,0.17536534446764093,"adversarial learning. However, the previously mentioned methods for CSDA cannot be directly
83"
RELATED WORK,0.17745302713987474,"applied in scenarios involving the category shift.
84"
RELATED WORK,0.17954070981210857,"Partial Domain Adaptation (PDA). PDA posits that private classes are exclusive to the source
85"
RELATED WORK,0.18162839248434237,"domain. Representative PDA methods, such as those discussed in [3, 49], employ domain discrimi-
86"
RELATED WORK,0.1837160751565762,"nators with weight adjustments or utilize source samples based on their resemblance to the target
87"
RELATED WORK,0.18580375782881003,"domain [5]. Methods incorporating residual correction blocks in PDA have been introduced by Li et
88"
RELATED WORK,0.18789144050104384,"al. and Liang et al. [25, 27]. Other research [7, 11, 38] explores the use of Reinforcement Learning
89"
RELATED WORK,0.18997912317327767,"for source data selection within the context of PDA.
90"
RELATED WORK,0.19206680584551147,"Open-Set Domain Adaptation (OSDA). Saito et al. [42] developed a classifier inclusive of an
91"
RELATED WORK,0.1941544885177453,"additional ’unknown’ class intended to differentiate categories unique to the target domain. Liu et al.
92"
RELATED WORK,0.19624217118997914,"[28] and Shermin et al. [43] propose assigning individual weights to each sample depending on their
93"
RELATED WORK,0.19832985386221294,"importance during domain adaptation. Jang et al. [20] strive to align the source and target-known
94"
RELATED WORK,0.20041753653444677,"distributions, while concurrently distinguishing the target-unknown distribution within the feature
95"
RELATED WORK,0.20250521920668058,"alignment process. The above PDA and OSDA methods are limited to specific category shift.
96"
RELATED WORK,0.2045929018789144,"Universal Domain Adaptation (UniDA) You et al. [47] proposed Universal Adaptation Network
97"
RELATED WORK,0.20668058455114824,"(UAN) to deal with the UniDA setting that the label set of target domain is unknown. Li et al.
98"
RELATED WORK,0.20876826722338204,"[24] proposed Domain Consensus Clustering to differentiate the private classes rather than treat the
99"
RELATED WORK,0.21085594989561587,"unknow classes as one class. Saito et al. [40] suggested that using the minimum inter-class distance in
100"
RELATED WORK,0.21294363256784968,"the source domain as a threshold can be an effective approach for distinguishing between “known” and
101"
RELATED WORK,0.2150313152400835,"“unknown” samples in the target domain. However, most existing methods [24, 47, 40, 39, 6, 34, 8, 26]
102"
RELATED WORK,0.21711899791231734,"overlook the intra-class distinction within one category, especially in cases where there exists
103"
RELATED WORK,0.21920668058455114,"significant concept shift between the samples belonging to the same category.
104"
PROPOSED METHODS,0.22129436325678498,"3
Proposed Methods
105"
PRELIMINARIES,0.22338204592901878,"3.1
Preliminaries
106"
PRELIMINARIES,0.2254697286012526,"In unsupervised domain adaptation, we are provided with labeled source samples Ds = {xs
i, ys
i )}ns
i=1
107"
PRELIMINARIES,0.22755741127348644,"and unlabeled target samples Dt = {(xt
i)}nt
i=1. As the label set for each domain in UniDA setting
108"
PRELIMINARIES,0.22964509394572025,"may not be identical, we use Cs and Ct to represent label sets for the two domains, respectively.
109"
PRELIMINARIES,0.23173277661795408,"Figure 2: Our model first utilizes a fixed pre-trained model as the encoder to extract input-oriented
embedding given an input sample. The extracted input-oriented embedding is then compared with
sub-prototypes learned in memory to find the closest K. These K are then weighted-averaged into a
task-oriented embedding to represent the input, and used for learning downstream tasks. During the
UniDA process, we adopt the cycle-consistent matching method on the task-oriented embedding ˆZ
generated from the memory. Moreover, a decoder is designed to reconstruct the image, allowing for
visualizing of the sub-prototypes in memory and verifying of the effectiveness of sub-class learning."
PRELIMINARIES,0.23382045929018788,"Then, we denote C = Cs ∩Ct as the common label set. ˆCs, ˆCt are denoted as the private label sets
110"
PRELIMINARIES,0.2359081419624217,"of the source domain and target domain, respectively. We aim to train a model on Ds and Dt to
111"
PRELIMINARIES,0.23799582463465555,"classify target samples into |C| + 1 classes, where private samples are treated as unknown class.
112"
PRELIMINARIES,0.24008350730688935,"Our method aims to address the issue of intra-class concept shift that often exists within the labeled
113"
PRELIMINARIES,0.24217118997912318,"categories in most datasets, which is overlooked by previous methods. Our method enables the
114"
PRELIMINARIES,0.24425887265135698,"model to learn an adaptive feature space that better aligns fine-grained sub-class concepts, taking
115"
PRELIMINARIES,0.24634655532359082,"into account the diversity present within each category. Let X denotes the input query, Z denotes the
116"
PRELIMINARIES,0.24843423799582465,"embedding extracted by the encoder, L denotes the data labels, ˆZ denotes the embedding obtained
117"
PRELIMINARIES,0.25052192066805845,"from the memory, ˆX denotes the visualization of the memory, ˆL denotes the prediction of the input
118"
PRELIMINARIES,0.25260960334029225,"query, and the K denotes the top-K relevant sub-prototypes, respectively. The overall pipeline is
119"
PRELIMINARIES,0.2546972860125261,"presented in Figure 2. More details will be described in the following sub-sections.
120"
PRELIMINARIES,0.2567849686847599,"3.2
Input-Oriented Embedding vs. Task-Oriented Embedding
121"
PRELIMINARIES,0.2588726513569937,"Usually, the image feature extracted by a visual encoder is directly used for learning downstream tasks.
122"
PRELIMINARIES,0.2609603340292276,"We call this kind of feature as input-oriented embedding. However, it heavily relys on the original
123"
PRELIMINARIES,0.2630480167014614,"image content. Since different samples of the same category always varies significantly in their visual
124"
PRELIMINARIES,0.2651356993736952,"features, categorization based on the input-oriented embedding sometimes is unattainable. In our
125"
PRELIMINARIES,0.267223382045929,"pipeline, we simply adopt a CLIP-based[35] pre-trained visual encoder to extract the input-oriented
126"
PRELIMINARIES,0.26931106471816285,"embeddings, which is not directly used for learning our downstream task.
127"
PRELIMINARIES,0.27139874739039666,"In our MemSPM, we propose to generate task-oriented embedding, which is obtained by serving
128"
PRELIMINARIES,0.27348643006263046,"input-oriented embedding as a query to retrieve the sub-prototypes from our memory unit. We define
129"
PRELIMINARIES,0.2755741127348643,"f fixed
encode(·) : X →Z to represent the fixed pre-trained encoder and f UniDA
class
(·) : ˆZ →ˆL to represent
130"
PRELIMINARIES,0.2776617954070981,"the UniDA classifier. The input-oriented embedding Z is used to retrieve the relevant sub-prototypes
131"
PRELIMINARIES,0.2797494780793319,"from the memory. The task-oriented embedding ˆZ is obtained using the retrieved sub-prototypes for
132"
PRELIMINARIES,0.2818371607515658,"classification tasks. In conventional ways, ˆZ = Z, which means the ˆZ is obtained directly from Z.
133"
PRELIMINARIES,0.2839248434237996,"Our method obtains the ˆZ by retrieving the sub-prototypes from the memory, which differenciates ˆZ
134"
PRELIMINARIES,0.2860125260960334,"with Z, and eliminates the domain-specific information from the target domain during the testing
135"
PRELIMINARIES,0.2881002087682672,"phase. As a result, it improves the performance of f UniDA
class
(·) when performing UniDA.
136"
MEMORY-ASSISTED SUB-PROTOTYPE MINING,0.29018789144050106,"3.3
Memory-Assisted Sub-Prototype Mining
137"
MEMORY-ASSISTED SUB-PROTOTYPE MINING,0.29227557411273486,"The memory module proposed in MemSPM consists of two key components: a memory unit
138"
MEMORY-ASSISTED SUB-PROTOTYPE MINING,0.29436325678496866,"responsible for learning sub-prototypes, and an attention-based addressing [18] operator to obtain
139"
MEMORY-ASSISTED SUB-PROTOTYPE MINING,0.2964509394572025,"better task-oriented representation ˆZ for the query, which is more domain-invariant.
140"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.2985386221294363,"3.3.1
Memory Structure with Partitioned Sub-Prototype
141"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.30062630480167013,"The memory in MemSPM is represented as a matrix, denoted by M ∈RN×S×D, where N indicates
142"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.302713987473904,"the number of memory items stored, S refers to the number of sub-prototypes partitioned in each
143"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3048016701461378,"memory item, and D represents the dimension of each sub-prototype. For convenience, we assume D
144"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3068893528183716,"is the same to the dimension of Z ∈RC ( RD=RC). Let the vector mi,j, ∀i ∈[N] denote the i-th row
145"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3089770354906054,"of M, where [N] denotes the set of integers from 1 to N, ∀j ∈[S] denote the j-th sub-prototype of
146"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.31106471816283926,"M items, where [S] denotes the set of integers from 1 to S. Each mi denotes a memory item. Given a
147"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.31315240083507306,"embedding Z ∈RD, the memory module obtains ˆZ through a soft addressing vector W ∈R1×1×N
148"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.31524008350730687,"as follows:
149"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3173277661795407,"ˆZ = W · M = ΣN
i=1wi,j=si · mi,j=si,
(1)
150"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.31941544885177453,"wi,j=si = argmax(wi,j, dim = 1),
(2)"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.32150313152400833,"where W is a vector with non-negative entries that indicate the max attention weight of each item’s
151"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3235908141962422,"sub-prototype, si denotes the index of the sub-prototype in the i-th item and wi,j=si denotes the
152"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.325678496868476,"i, j = si-th entry of W. The hyperparameter N determines the maximum capacity for memory items
153"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3277661795407098,"and the hyper-parameter S defines the number of sub-prototypes in each memory item. The effect of
154"
MEMORY STRUCTURE WITH PARTITIONED SUB-PROTOTYPE,0.3298538622129436,"different setting of hyper-parameters is evaluated in Section 4.
155"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.33194154488517746,"3.3.2
Sub-Prototype Addressing and Retrieving
156"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.33402922755741127,"In MemSPM, the memory M is designed to learn the sub-prototypes to represent the input-oriented
157"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.33611691022964507,"embedding Z. We define the memory as a content addressable memory [17, 10, 45, 36] that allows
158"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.33820459290187893,"for direct referencing of the content of the memory being matched. The sub-prototype is retrieved by
159"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.34029227557411273,"attention weights W which are computed based on the similarity between the sub-prototypes in the
160"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.34237995824634654,"memory items and the input-oriented embedding Z. To calculate the weight wi,j, we use a softmax
161"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.3444676409185804,"operation:
162"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.3465553235908142,"wi,j =
exp(d(z, mi,j))
ΣN
n=1ΣS
s=1 exp(d(z, mn,s)),
(3)"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.348643006263048,"where d(·, ·) denotes cosine similarity measurement. As indicated by Eq. 1 and 3, the memory
163"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.35073068893528186,"module retrieves the sub-prototype that is most similar to Z from each memory item in order to
164"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.35281837160751567,"obtain the new representation embedding ˆZ. As a consequence of utilizing the adaptive threshold
165"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.35490605427974947,"addressing technique(Section 3.3.3), only the K can be utilized to obtain a task-oriented embedding
166"
SUB-PROTOTYPE ADDRESSING AND RETRIEVING,0.3569937369519833,"ˆZ, that serves to represent the encoded embedding Z.
167"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.35908141962421714,"3.3.3
Adaptive Threshold Technique for More Efficient Memory
168"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.36116910229645094,"Limiting the amount of sub-prototypes retrieved can enhance memory utilization and avoid negative
169"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.36325678496868474,"impacts on unrelated sub-prototypes during model parameter updates. Despite the natural reduction
170"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3653444676409186,"in the number of selected memory items, the attention-based addressing mechanism may still lead to
171"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3674321503131524,"the combination of small attention weight items into the output embedding ˆZ, which have negative
172"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3695198329853862,"impact on the classifier and sub-prototypes in the memory. Therefore, it is necessary to impose a
173"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.37160751565762007,"mandatory quantity limit on the amount of the relevant sub-prototypes retrieved. To address this
174"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3736951983298539,"issue, we apply a adaptive threshold operation to restrict the amount of sub-prototypes retrieved in a
175"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3757828810020877,"forward process.
176"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3778705636743215,"ˆwi,j=si =
wi,j=si,
wi,j=si > λ
0,
other
(4)"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.37995824634655534,"where ˆwi,j=si denotes the i, j = si-th entry of ˆw, the λ denotes the adaptive threshold:
177"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.38204592901878914,"λ = argmin(topk(w)).
(5)"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.38413361169102295,"Directly implementing the backward for the discontinuous function in Eq. 4 is not a easy task. For
178"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3862212943632568,"simplicity, we use the method [17]that rewrites the operation using the continuous ReLU activation
179"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3883089770354906,"function as:
180"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3903966597077244,"ˆwi,j=si = max(wi,j=si −λ) · wi,j=si"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3924843423799583,"|wi,j=si −λ| + ϵ
,
(6)"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3945720250521921,"where max(·, 0) is commonly referred to as the ReLU activation function, and ϵ is a small positive
181"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3966597077244259,"scalar. The prototype ˆZ will be obtained by ˆZ = ˆW · M. The adaptive threshold addressing
182"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.3987473903966597,"encourages the model to represent embedding Z using fewer but more relevant sub-prototypes,
183"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.40083507306889354,"leading to learning more effective feature in memory and reducing the impact on irrelevant sub-
184"
ADAPTIVE THRESHOLD TECHNIQUE FOR MORE EFFICIENT MEMORY,0.40292275574112735,"prototypes.
185"
VISUALIZATION AND INTERPRETABILITY,0.40501043841336115,"3.4
Visualization and Interpretability
186"
VISUALIZATION AND INTERPRETABILITY,0.407098121085595,"We denote f unfixed
decode (·) : ˆZ →ˆX to represent the decoder. The decoder is trained to visualize
187"
VISUALIZATION AND INTERPRETABILITY,0.4091858037578288,"what has been learned in the memory by taking the retrieved sub-prototype as input. From an
188"
VISUALIZATION AND INTERPRETABILITY,0.4112734864300626,"interpretability perspective, each encoded embedding Z calculates the cosine similarity to find the
189"
VISUALIZATION AND INTERPRETABILITY,0.4133611691022965,"top-K fitting sub-prototype representation for the given input-oriented embedding. Then, these
190"
VISUALIZATION AND INTERPRETABILITY,0.4154488517745303,"sub-prototypes are combined to represent the Z in ˆZ. The sub-prototype in this process can be
191"
VISUALIZATION AND INTERPRETABILITY,0.4175365344467641,"regarded as the visual description for the input embedding Z. In other word, the input image is
192"
VISUALIZATION AND INTERPRETABILITY,0.4196242171189979,"much like the sub-classes represented by these sub-prototypes. In this way, samples with significant
193"
VISUALIZATION AND INTERPRETABILITY,0.42171189979123175,"intra-class differences will be matched to different sub-prototypes, thereby distinguishing different
194"
VISUALIZATION AND INTERPRETABILITY,0.42379958246346555,"sub-classes. The use of a reconstruction auxiliary task can visualize the sub-prototypes in memory
195"
VISUALIZATION AND INTERPRETABILITY,0.42588726513569936,"to confirm whether our approach has learned intra-class differences for the annotated category. The
196"
VISUALIZATION AND INTERPRETABILITY,0.4279749478079332,"results of this visualization are demonstrated in Figure 3.
197"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.430062630480167,"3.5
Cycle-Consistent Alignment and Adaption
198"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4321503131524008,"Once the sub-prototypes are mined through memory learning, the method of cycle-consistent match-
199"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4342379958246347,"ing, inspired by DCC [24], is employed to align the embedding ˆZ. The cycle-consistent matching
200"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4363256784968685,"is preferred due to it can provides a better fit to the memory structure compared to other UniDA
201"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4384133611691023,"methods. The other method, One-vs-All Network (OVANet), proposed by Saito et al. [40], needs
202"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4405010438413361,"to train the memory multiple times, which can lead to a significant computational overhead. In
203"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.44258872651356995,"brief, the Cycle-Consistent Alignment provides a solution by iteratively learning a consensus set of
204"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.44467640918580376,"clusters between the two domains. The consensus clusters are identified based on the similarity of the
205"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.44676409185803756,"prototypes, which is measured using a similarity metric. The similarity metric is calculated on the
206"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4488517745302714,"feature representations of the prototypes. For unknown classes, we set the size N of our memory
207"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4509394572025052,"during the initial phase to be larger than the number of possible sub-classes that may be learned in the
208"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.453027139874739,"source domain. This size is a hyperparameter that is adjusted based on the dataset size. Redundant
209"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4551148225469729,"sub-prototypes are invoked to represent the ˆZ, when encountering unknown classes, allowing for an
210"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4572025052192067,"improved distance separation between unknown and known classes in the feature space.
211"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4592901878914405,"Training Objective. The adaptation loss in our training is similar to that of DCC, as LDA:
212"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4613778705636743,"LDA = Lce + λ1Lcdd + λ2Lreg,
(7)"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.46346555323590816,"where the Lce denotes the cross-entropy loss on source samples, Lcdd denotes the domain alignment
213"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.46555323590814196,"loss and Lreg denotes the regularizer. For the auxiliary reconstruction task, we add a mean-squared-
214"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.46764091858037576,"error (MSE) loss function, denoted as Lrec. Thus, the model is optimized with:
215"
CYCLE-CONSISTENT ALIGNMENT AND ADAPTION,0.4697286012526096,"L = LDA + λ3Lrec = Lce + λ1Lcdd + λ2Lreg + λ3Lrec.
(8)"
EXPERIMENTS,0.4718162839248434,"4
Experiments
216"
DATASETS AND EVALUATION METRICS,0.47390396659707723,"4.1
Datasets and Evaluation Metrics
217"
DATASETS AND EVALUATION METRICS,0.4759916492693111,"We first conduct the experiments in the UniDA setting [47] where private classes exist in both
218"
DATASETS AND EVALUATION METRICS,0.4780793319415449,"domains. Moreover, we also evaluate our approach on two other sub-cases, namely Open-Set Domain
219"
DATASETS AND EVALUATION METRICS,0.4801670146137787,"Adaptation (OSDA) and Partial Domain Adaptation (PDA).
220"
DATASETS AND EVALUATION METRICS,0.4822546972860125,"Datasets.
Our experiments are conducted on four datasets:
Office-31 [37], which con-
221"
DATASETS AND EVALUATION METRICS,0.48434237995824636,"tains 4652 images from three domains (DSLR, Amazon,
and Webcam);
OfficeHome
222"
DATASETS AND EVALUATION METRICS,0.48643006263048016,"Table 2: H-score (%) comparison in UniDA scenario on DomainNet, VisDA and Office-31,some
results are cited from [24, 34]"
DATASETS AND EVALUATION METRICS,0.48851774530271397,"Method
Backbone
DomainNet
VisDA
Office-31
P2R
P2S
R2P
R2S
S2P
S2R
Avg
S2R
A2D
A2W
D2A
D2W
W2A
W2D
Avg
UAN [47]"
DATASETS AND EVALUATION METRICS,0.4906054279749478,ResNet50
DATASETS AND EVALUATION METRICS,0.49269311064718163,"41.9
39.1
43.6
38.7
38.9
43.7
41.0
34.8
59.7
58.6
60.1
70.6
60.3
71.4
63.5
CMU [14]
50.8
45.1
52.2
45.6
44.8
51.0
48.3
32.9
68.1
67.3
71.4
79.3
72.2
80.4
73.1
DCC [24]
56.9
43.7
50.3
43.3
44.9
56.2
49.2
43.0
88.5
78.5
70.2
79.3
75.9
88.6
80.2
OVANet [40]
56.0
47.1
51.7
44.9
47.4
57.2
50.7
53.1
85.8
79.4
80.1
95.4
84.0
94.3
86.5
UMAD [26]
59.0
44.3
50.1
42.1
32.0
55.3
47.1
58.3
79.1
77.4
87.4
90.7
90.4
97.2
87.0
GATE [8]
57.4
48.7
52.8
47.6
49.5
56.3
52.1
56.4
87.7
81.6
84.2
94.8
83.4
94.1
87.6
UniOT [6]
59.3
47.8
51.8
46.8
48.3
58.3
52.0
57.3
83.7
85.3
71.4
91.2
70.9
90.84
82.2
GLC [34]
63.3
50.5
54.9
50.9
49.6
61.3
55.1
73.1
81.5
84.5
89.8
90.4
88.4
92.3
87.8
GLC [34]"
DATASETS AND EVALUATION METRICS,0.49478079331941544,ViT-B/16
DATASETS AND EVALUATION METRICS,0.4968684759916493,"51.2
44.5
55.6
43.1
47.0
39.1
46.8
80.3
80.5
80.4
77.5
95.6
77.7
96.9
84.8
DCC [24]
61.1
38.8
51.8
49.3
49.1
60.3
52.2
61.2
82.2
76.9
83.6
75.2
85.8
88.7
82.1
MemSPM+DCC
62.4
52.8
58.5
53.3
50.4
62.6
56.7
79.3
88.0
84.6
88.7
87.6
87.9
94.3
88.5"
DATASETS AND EVALUATION METRICS,0.4989561586638831,"Table 3: H-score (%) comparison in UniDA scenario on Office-Home, some results are cited from
[24, 34]"
DATASETS AND EVALUATION METRICS,0.5010438413361169,"Method
Backbone
Office-Home
Ar2Cl
Ar2Pr
Ar2Rw
Cl2Ar
Cl2Pr
Cl2Rw
Pr2Ar
Pr2Cl
Pr2Rw
Rw2Ar
Rw2Cl
Rw2Pr
Avg
UAN [47]"
DATASETS AND EVALUATION METRICS,0.5031315240083507,ResNet50
DATASETS AND EVALUATION METRICS,0.5052192066805845,"51.6
51.7
54.3
61.7
57.6
61.9
50.4
47.6
61.5
62.9
52.6
65.2
56.6
CMU [14]
56.0
56.9
59.2
67.0
64.3
67.8
54.7
51.1
66.4
68.2
57.9
69.7
61.6
DCC [24]
58.0
54.1
58.0
74.6
70.6
77.5
64.3
73.6
74.9
81.0
75.1
80.4
70.2
OVANet [40]
62.8
75.6
78.6
70.7
68.8
75.0
71.3
58.6
80.5
76.1
64.1
78.9
71.8
UMAD [26]
61.1
76.3
82.7
70.7
67.7
75.7
64.4
55.7
76.3
73.2
60.4
77.2
70.1
GATE [8]
63.8
75.9
81.4
74.0
72.1
79.8
74.7
70.3
82.7
79.1
71.5
81.7
75.6
UniOT [6]
67.2
80.5
86.0
73.5
77.3
84.3
75.5
63.3
86.0
77.8
65.4
81.9
76.6
GLC [34]
64.3
78.2
89.8
63.1
81.7
89.1
77.6
54.2
88.9
80.7
54.2
85.9
75.6
GLC [34]"
DATASETS AND EVALUATION METRICS,0.5073068893528184,ViT-B/16
DATASETS AND EVALUATION METRICS,0.5093945720250522,"68.5
89.8
91.0
82.4
88.1
89.4
82.1
69.7
88.2
82.4
70.9
88.9
82.6
DCC [24]
62.6
88.7
87.4
63.3
68.5
79.3
67.9
63.8
82.4
70.7
69.8
87.5
74.4
MemSPM+DCC
78.1
90.3
90.7
81.9
90.5
88.3
79.2
77.4
87.8
78.8
76.2
91.6
84.2"
DATASETS AND EVALUATION METRICS,0.511482254697286,"[46], a more difficult dataset consisting of 15500 images across 65 categories and 4
223"
DATASETS AND EVALUATION METRICS,0.5135699373695198,"domains (Artistic images,
Clip-Art images,
Product images,
and Real-World images);
224"
DATASETS AND EVALUATION METRICS,0.5156576200417536,"Table 1: The division on label set,
Common Class (C) / Source-Private
Class ( ˆCs) / Target Private Class ( ˆCt)."
DATASETS AND EVALUATION METRICS,0.5177453027139874,"Dataset
Class Split(C/ ˆCs/ ˆCt)
PDA
OSDA
UniDA
Office-31
10 / 21 / 0
10 / 0 / 11
10 / 10 / 11
OfficeHome
25 / 40 / 0
25 / 0 / 40
10 / 5 / 50
VisDA
6 / 6 / 0
6 / 0 / 6
6 / 3 / 3
DomainNet
——
——
150 / 50 / 145"
DATASETS AND EVALUATION METRICS,0.5198329853862212,"VisDA [33], a large-scale dataset with a synthetic source do-
225"
DATASETS AND EVALUATION METRICS,0.5219206680584552,"main of 15K images and a real-world target domain of 5K
226"
DATASETS AND EVALUATION METRICS,0.524008350730689,"images; and DomainNet [32], the largest domain adaptation
227"
DATASETS AND EVALUATION METRICS,0.5260960334029228,"dataset with approximately 600,000 images. Similar to pre-
228"
DATASETS AND EVALUATION METRICS,0.5281837160751566,"vious studies [14], we evaluate our model on three subsets of
229"
DATASETS AND EVALUATION METRICS,0.5302713987473904,"DomainNet (Painting, Real, and Sketch).
230"
DATASETS AND EVALUATION METRICS,0.5323590814196242,"As in previous work [24, 41, 2, 4, 47], we divide the label set
231"
DATASETS AND EVALUATION METRICS,0.534446764091858,"into three groups: common classes C, source-private classes
232"
DATASETS AND EVALUATION METRICS,0.5365344467640919,"ˆCs, and target-private classes ˆCt. The separation of classes
233"
DATASETS AND EVALUATION METRICS,0.5386221294363257,"for each of the four datasets is shown in Table 1 and is determined according to alphabetical order.
234"
DATASETS AND EVALUATION METRICS,0.5407098121085595,"Evaluation Metrics. We report the averaged results of three runs. For the PDA scenario, we
235"
DATASETS AND EVALUATION METRICS,0.5427974947807933,"calculate the classification accuracy over all target samples. The usual metrics adopted to evaluate
236"
DATASETS AND EVALUATION METRICS,0.5448851774530271,"OSDA are the average class accuracy over the known classes OS∗, and the accuracy of the unknown
237"
DATASETS AND EVALUATION METRICS,0.5469728601252609,"class UNK. In the OSDA and UniDA scenarios, we consider the balance between “known” and
238"
DATASETS AND EVALUATION METRICS,0.5490605427974948,"“unknown” categories and report the H-score [1]:
239"
DATASETS AND EVALUATION METRICS,0.5511482254697286,H-score = 2 × OS∗× UNK
DATASETS AND EVALUATION METRICS,0.5532359081419624,"OS∗+ UNK ,
(9)"
DATASETS AND EVALUATION METRICS,0.5553235908141962,"which is the harmonic mean of the accuracy of “known” and “unknown” samples.
240"
DATASETS AND EVALUATION METRICS,0.55741127348643,"Implementation Details. Our implementation is based on PyTorch [31]. We use ViT-B/16 [12] as
241"
DATASETS AND EVALUATION METRICS,0.5594989561586639,"the backbone pretrained by CLIP [35] for the MemSPM is hard to train with a randomly initialized
242"
DATASETS AND EVALUATION METRICS,0.5615866388308977,"encoder. The classifier consists of two fully-connected layers, which follows the previous design
243"
DATASETS AND EVALUATION METRICS,0.5636743215031316,"[4, 47, 41, 14, 24]. The weights in the L are empirically set as λ1 = 0.1, λ2 = 3 and λ3 = 0.5 fellow
244"
DATASETS AND EVALUATION METRICS,0.5657620041753654,"DCC [24]. For a fair comparison, we also adopt ViT-B/16 as backbone for DCC [24] and state-of-
245"
DATASETS AND EVALUATION METRICS,0.5678496868475992,"art method GLC [34]. We use the official code of DCC [24] (https://github.com/Solacex/
246"
DATASETS AND EVALUATION METRICS,0.569937369519833,"Domain-Consensus-Clustering) and GLC [34] (https://github.com/ispc-lab/GLC).
247"
DATASETS AND EVALUATION METRICS,0.5720250521920668,"Table 4: H-score (%) comparison in OSDA scenario on Office-Home, VisDA and Office-31, some
results are cited from [24, 34]"
DATASETS AND EVALUATION METRICS,0.5741127348643006,"Method
Backbone
Office-Home
Office-31
VisDA
Ar2Cl
Ar2Pr
Ar2Rw
Cl2Ar
Cl2Pr
Cl2Rw
Pr2Ar
Pr2Cl
Pr2Rw
Rw2Ar
Rw2Cl
Rw2Pr
Avg
Avg
Avg
OSBP [41]"
DATASETS AND EVALUATION METRICS,0.5762004175365344,ResNet50
DATASETS AND EVALUATION METRICS,0.5782881002087683,"55.1
65.2
72.9
64.3
64.7
70.6
63.2
53.2
73.9
66.7
54.5
72.3
64.7
83.7
52.3
CMU [14]
55.0
57.0
59.0
59.3
58.2
60.6
59.2
51.3
61.2
61.9
53.5
55.3
57.6
65.2
54.2
DCC [24]
56.1
67.5
66.7
49.6
66.5
64.0
55.8
53.0
70.5
61.6
57.2
71.9
61.7
72.7
59.6
OVANet [40]
58.6
66.3
69.9
62.0
65.2
68.6
59.8
53.4
69.3
68.7
59.6
66.7
64.0
91.7
66.1
UMAD [26]
59.2
71.8
76.6
63.5
69.0
71.9
62.5
54.6
72.8
66.5
57.9
70.7
66.4
89.8
66.8
GATE [8]
63.8
70.5
75.8
66.4
67.9
71.7
67.3
61.5
76.0
70.4
61.8
75.1
69.0
89.5
70.8
ROS [6]
60.1
69.3
76.5
58.9
65.2
68.6
60.6
56.3
74.4
68.8
60.4
75.7
66.2
85.9
66.5
GLC [34]
65.3
74.2
79.0
60.4
71.6
74.7
63.7
63.2
75.8
67.1
64.3
77.8
69.8
89.0
72.5
GLC [34]"
DATASETS AND EVALUATION METRICS,0.5803757828810021,ViT-B/16
DATASETS AND EVALUATION METRICS,0.5824634655532359,"68.4
81.7
84.5
76.0
82.4
83.8
69.9
59.6
84.6
73.3
66.8
83.9
76.2
90.1
81.6
DCC [24]
62.9
73.3
78.4
49.8
69.2
75.0
59.3
61.5
80.9
68.1
62.5
80.0
68.4
81.9
66.2
MemSPM+DCC
69.7
83.2
85.2
72.0
79.2
81.2
72.3
66.7
85.2
72.7
66.0
84.5
76.5
95.6
79.7"
DATASETS AND EVALUATION METRICS,0.5845511482254697,"Table 5: H-score (%) comparison in PDA scenario on Office-Home, VisDA and Office-31, some
results are cited from [24, 34]"
DATASETS AND EVALUATION METRICS,0.5866388308977035,"Method
Backbone
Office-Home
Office-31
VisDA
Ar2Cl
Ar2Pr
Ar2Rw
Cl2Ar
Cl2Pr
Cl2Rw
Pr2Ar
Pr2Cl
Pr2Rw
Rw2Ar
Rw2Cl
Rw2Pr
Avg
Avg
Avg
ETN [5]"
DATASETS AND EVALUATION METRICS,0.5887265135699373,ResNet50
DATASETS AND EVALUATION METRICS,0.5908141962421712,"59.2
77.0
79.5
62.9
65.7
75.0
68.3
55.4
84.4
75.7
57.7
84.5
70.4
96.7
59.8
BA3US [27]
60.6
83.2
88.4
71.8
72.8
83.4
75.5
61.6
86.5
79.3
62.8
86.1
76.0
97.8
54.9
DCC [24]
54.2
47.5
57.5
83.8
71.6
86.2
63.7
65.0
75.2
85.5
78.2
82.6
70.9
93.3
72.4
OVANet [40]
34.1
54.6
72.1
42.4
47.3
55.9
38.2
26.2
61.7
56.7
35.8
68.9
49.5
74.6
34.3
UMAD [26]
51.2
66.5
79.2
63.1
62.9
68.2
63.3
56.4
75.9
74.5
55.9
78.3
66.3
89.5
68.5
GATE [8]
55.8
75.9
85.3
73.6
70.2
83.0
72.1
59.5
84.7
79.6
63.9
83.8
74.0
93.7
75.6
GLC [34]
55.9
79.0
87.5
72.5
71.8
82.7
74.9
41.7
82.4
77.3
60.4
84.3
72.5
94.1
76.2
GLC [34]"
DATASETS AND EVALUATION METRICS,0.592901878914405,ViT-B/16
DATASETS AND EVALUATION METRICS,0.5949895615866388,"63.2
80.7
86.5
76.0
77.9
84.1
74.5
56.8
84.7
79.8
57.4
83.0
75.4
91.5
86.2
DCC [24]
59.4
78.8
83.2
61.95
78.6
79.3
64.2
44.4
82.9
76.5
70.7
84.6
72.1
93.7
79.8
MemSPM+DCC
64.7
81.1
84.5
74.8
74.7
77.5
58.7
60.3
84.2
70.3
77.2
85.8
74.5
94.4
87.9"
COMPARISON WITH STATE-OF-THE-ARTS,0.5970772442588727,"4.2
Comparison with State-of-The-Arts
248"
COMPARISON WITH STATE-OF-THE-ARTS,0.5991649269311065,"We compare our method with previous state-of-the-art algorithms in three sub-cases of unsupervised
249"
COMPARISON WITH STATE-OF-THE-ARTS,0.6012526096033403,"domain adaptation, namely, object-specific domain adaptation (OSDA), partial domain adaptation
250"
COMPARISON WITH STATE-OF-THE-ARTS,0.6033402922755741,"(PDA), and universal domain adaptation (UniDA). In UniDA, we compare our method to previous
251"
COMPARISON WITH STATE-OF-THE-ARTS,0.605427974947808,"universal domain adaptation approaches, which do not take into account the prior that private classes
252"
COMPARISON WITH STATE-OF-THE-ARTS,0.6075156576200418,"exist only in either the source domain (PDA) or the target domain (OSDA). Additionally, we compare
253"
COMPARISON WITH STATE-OF-THE-ARTS,0.6096033402922756,"our method to the OSDA and PDA baselines that consider the prior information unique to each
254"
COMPARISON WITH STATE-OF-THE-ARTS,0.6116910229645094,"sub-case.
255"
COMPARISON WITH STATE-OF-THE-ARTS,0.6137787056367432,"Results on UniDA. In the most challenging setting, i.e. UniDA, our MemSPM approach achieves
256"
COMPARISON WITH STATE-OF-THE-ARTS,0.615866388308977,"the state-of-the-art performance. Table 2 shows the results on DomainNet, VisDA and Office-31,
257"
COMPARISON WITH STATE-OF-THE-ARTS,0.6179540709812108,"and result of Office-Home is summarized in Table 3. We mainly compare with GLC and DCC using
258"
COMPARISON WITH STATE-OF-THE-ARTS,0.6200417536534447,"ViT-B/16 as backbone. On Office-31, the MemSPM+DCC outperform previous state-of-art method
259"
COMPARISON WITH STATE-OF-THE-ARTS,0.6221294363256785,"GLC by 3.7% and surpasses the DCC by 6.4%. On visda, our method surpasses the DCC by a huge
260"
COMPARISON WITH STATE-OF-THE-ARTS,0.6242171189979123,"margin of 16.1%. Our method also surpasses the GLC by 9.9% and the DCC by 4.5% on DomainNet.
261"
COMPARISON WITH STATE-OF-THE-ARTS,0.6263048016701461,"On the Office-Home, we surpasses the DCC by 9.8% and the GLC by 3.7%.
262"
COMPARISON WITH STATE-OF-THE-ARTS,0.6283924843423799,"Results on OSDA and PDA. In table 4 and table 5, we present the results on Office-Home, Office-31
263"
COMPARISON WITH STATE-OF-THE-ARTS,0.6304801670146137,"and VisDA under OSDA and PDA scenarios. In the OSDA scenario, MemSPM+DCC still achieves
264"
COMPARISON WITH STATE-OF-THE-ARTS,0.6325678496868476,"state-of-the-art performance. Specifically, MemSPM+DCC obtains 95.6% H-score on Office-31,
265"
COMPARISON WITH STATE-OF-THE-ARTS,0.6346555323590815,"with an improvement of 5.5% compared to GLC and 13.7% compared to DCC. In the PDA scenario,
266"
COMPARISON WITH STATE-OF-THE-ARTS,0.6367432150313153,"MemSPM still achieves comparable performance compared to methods tailored for PDA. The
267"
COMPARISON WITH STATE-OF-THE-ARTS,0.6388308977035491,"MemSPM+DCC surpasses the DCC by 8.1% on the VisDA.
268"
ABLATION STUDIES,0.6409185803757829,"4.3
Ablation Studies
269"
ABLATION STUDIES,0.6430062630480167,"Visualization with Reconstruction and tSNE We first visualize what the memory learns from Office-
270"
ABLATION STUDIES,0.6450939457202505,"Home by sampling a single sub-prototype and adapting an auxiliary reconstruction task: X →ˆX. We
271"
ABLATION STUDIES,0.6471816283924844,"also provide the tSNE of the ˆZ which retrieving the most related sub-prototypes. The visualization is
272"
ABLATION STUDIES,0.6492693110647182,"shown in Figure 3. The tSNE visualization depicts the distribution of sub-classes within each category,
273"
ABLATION STUDIES,0.651356993736952,"indicative of MemSPM’s successful mining of sub-prototypes. The reconstruction visualization shows
274"
ABLATION STUDIES,0.6534446764091858,"what have been learned by MemSPM, demonstrating its ability to capture intra-class diversity.
275"
ABLATION STUDIES,0.6555323590814196,"Figure 3: (a) The tSNE visualization shows the feature space of the sub-classes belonging to the each
category, which demonstrate the MemSPM mining the sub-prototypes successfully. (b) The results of
different values of S and N. (c) The reconstruction visualization shows what have been learned in
the memory, which demonstrate the intra-class diversity have been learned by MemSPM."
ABLATION STUDIES,0.6576200417536534,"Effect of Memory-Assisted Sub-Prototype Mining. As the results shown in table 2, table 3, table 4
276"
ABLATION STUDIES,0.6597077244258872,"and table 5, the MemSPM+DCC evaluted on four benchmarks has surpassed the DCC on UniDA,
277"
ABLATION STUDIES,0.6617954070981211,"OSDA and PDA scenarios. The MemSPM can significantly improve the performance of the DCC
278"
ABLATION STUDIES,0.6638830897703549,"when using ViT-B/16 as backbone. The reason for utilizing the ViT-B/16 is that the memory module
279"
ABLATION STUDIES,0.6659707724425887,"of the MemSPM with huge latent space is initialized by randomly normal distribution, which make it
280"
ABLATION STUDIES,0.6680584551148225,"hard to retrieve the different sub-prototypes at early stages of training. So, we need ViT as backbone,
281"
ABLATION STUDIES,0.6701461377870563,"which have learned a more global feature space.
282"
ABLATION STUDIES,0.6722338204592901,"Sensitivity to Hyper-parameters. We conducted experiments on the VisDA dataset under the UniDA
283"
ABLATION STUDIES,0.6743215031315241,"setting to demonstrate the impact of hyperparameters S and N on the performance of our method.
284"
ABLATION STUDIES,0.6764091858037579,"The impact of S are shown in Figure 3. When S ≥20, the performance achieve a comparable level.
285"
ABLATION STUDIES,0.6784968684759917,"At the same time, the performance of the model is not sensitive to the value of N, when S = 30.
286"
CONCLUSION,0.6805845511482255,"5
Conclusion
287"
CONCLUSION,0.6826722338204593,"In this paper, we propose the Memory-Assisted Sub-Prototype Mining (MemSPM) method, which can
288"
CONCLUSION,0.6847599164926931,"learn the intra-class diversity by mining the sub-prototypes to represent the sub-classes. Compared
289"
CONCLUSION,0.6868475991649269,"with the previous methods, which overlook the intra-class structure by using one-hot label, our Mem-
290"
CONCLUSION,0.6889352818371608,"SPM can learn the class feature from a more subdivided sub-class perspective to improve adaptation
291"
CONCLUSION,0.6910229645093946,"performance. At the same time, the visualization of the tSNE and reconstruction demonstrates the
292"
CONCLUSION,0.6931106471816284,"sub-prototypes have been well learned as we expected. Our MemSPM method exhibits superior
293"
CONCLUSION,0.6951983298538622,"performance in most cases compared with previous state-of-the-art methods on four benchmarks.
294"
REFERENCES,0.697286012526096,"References
295"
REFERENCES,0.6993736951983298,"[1] Silvia Bucci, Mohammad Reza Loghmani, and Tatiana Tommasi. On the effectiveness of image rotation
296"
REFERENCES,0.7014613778705637,"for open set domain adaptation. In Proceedings of the European Conference on Computer Vision, pages
297"
REFERENCES,0.7035490605427975,"422–438, 2020.
298"
REFERENCES,0.7056367432150313,"[2] Pau Panareda Busto, Ahsan Iqbal, and Juergen Gall. Open set domain adaptation for image and action
299"
REFERENCES,0.7077244258872651,"recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):413–429, 2018.
300"
REFERENCES,0.7098121085594989,"[3] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with
301"
REFERENCES,0.7118997912317327,"selective adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
302"
REFERENCES,0.7139874739039666,"Recognition, pages 2724–2732, 2018.
303"
REFERENCES,0.7160751565762005,"[4] Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In
304"
REFERENCES,0.7181628392484343,"Proceedings of the European Conference on Computer Vision, pages 135–150, 2018.
305"
REFERENCES,0.7202505219206681,"[5] Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer
306"
REFERENCES,0.7223382045929019,"examples for partial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
307"
REFERENCES,0.7244258872651357,"and Pattern Recognition, pages 2985–2994, 2019.
308"
REFERENCES,0.7265135699373695,"[6] Wanxing Chang, Ye Shi, Hoang Tuan, and Jingya Wang. Unified optimal transport framework for universal
309"
REFERENCES,0.7286012526096033,"domain adaptation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
310"
REFERENCES,0.7306889352818372,"Advances in Neural Information Processing Systems, volume 35, pages 29512–29524. Curran Associates,
311"
REFERENCES,0.732776617954071,"Inc., 2022.
312"
REFERENCES,0.7348643006263048,"[7] Jin Chen, Xinxiao Wu, Lixin Duan, and Shenghua Gao. Domain adversarial reinforcement learning for
313"
REFERENCES,0.7369519832985386,"partial domain adaptation. IEEE Transactions on Neural Networks and Learning Systems, 33(2):539–553,
314"
REFERENCES,0.7390396659707724,"2020.
315"
REFERENCES,0.7411273486430062,"[8] Liang Chen, Yihang Lou, Jianzhong He, Tao Bai, and Minghua Deng. Geometric anchor correspondence
316"
REFERENCES,0.7432150313152401,"mining with uncertainty modeling for universal domain adaptation. In Proceedings of the IEEE/CVF
317"
REFERENCES,0.7453027139874739,"Conference on Computer Vision and Pattern Recognition, pages 16134–16143, 2022.
318"
REFERENCES,0.7473903966597077,"[9] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Deep reconstruction-classification networks for unsuper-
319"
REFERENCES,0.7494780793319415,"vised domain adaptation. In Proceedings of the European Conference on Computer Vision, pages 597–613,
320"
REFERENCES,0.7515657620041754,"2016.
321"
REFERENCES,0.7536534446764092,"[10] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Semi-supervised deep learning with memory. In
322"
REFERENCES,0.755741127348643,"Proceedings of the European Conference on Computer Vision, pages 268–283, 2018.
323"
REFERENCES,0.7578288100208769,"[11] Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, and Xinyu Jin. Selective transfer with
324"
REFERENCES,0.7599164926931107,"reinforced transfer network for partial domain adaptation. In Proceedings of the IEEE/CVF Conference on
325"
REFERENCES,0.7620041753653445,"Computer Vision and Pattern Recognition, pages 12706–12714, 2020.
326"
REFERENCES,0.7640918580375783,"[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
327"
REFERENCES,0.7661795407098121,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
328"
REFERENCES,0.7682672233820459,"16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
329"
REFERENCES,0.7703549060542797,"[13] Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient discrepancy minimization
330"
REFERENCES,0.7724425887265136,"for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
331"
REFERENCES,0.7745302713987474,"and Pattern Recognition, pages 3937–3946, 2021.
332"
REFERENCES,0.7766179540709812,"[14] Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal
333"
REFERENCES,0.778705636743215,"domain adaptation. In Proceedings of the European Conference on Computer Vision, pages 567–583, 2020.
334"
REFERENCES,0.7807933194154488,"[15] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis
335"
REFERENCES,0.7828810020876826,"Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning,
336"
REFERENCES,0.7849686847599165,"volume 37 of Proceedings of Machine Learning Research, pages 1180–1189. PMLR, 2015.
337"
REFERENCES,0.7870563674321504,"[16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
338"
REFERENCES,0.7891440501043842,"Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of
339"
REFERENCES,0.791231732776618,"machine learning research, 17(1):2096–2030, 2016.
340"
REFERENCES,0.7933194154488518,"[17] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and
341"
REFERENCES,0.7954070981210856,"Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder
342"
REFERENCES,0.7974947807933194,"for unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on
343"
REFERENCES,0.7995824634655533,"Computer Vision, pages 1705–1714, 2019.
344"
REFERENCES,0.8016701461377871,"[18] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401,
345"
REFERENCES,0.8037578288100209,"2014.
346"
REFERENCES,0.8058455114822547,"[19] Tzu Ming Harry Hsu, Wei Yu Chen, Cheng-An Hou, Yao-Hung Hubert Tsai, Yi-Ren Yeh, and Yu-
347"
REFERENCES,0.8079331941544885,"Chiang Frank Wang. Unsupervised domain adaptation with imbalanced cross-domain data. In Proceedings
348"
REFERENCES,0.8100208768267223,"of the IEEE International Conference on Computer Vision, 2015.
349"
REFERENCES,0.8121085594989561,"[20] JoonHo Jang, Byeonghu Na, Dong Hyeok Shin, Mingi Ji, Kyungwoo Song, and Il-Chul Moon. Unknown-
350"
REFERENCES,0.81419624217119,"aware domain adversarial learning for open-set domain adaptation. Advances in Neural Information
351"
REFERENCES,0.8162839248434238,"Processing Systems, 35:16755–16767, 2022.
352"
REFERENCES,0.8183716075156576,"[21] Tarun Kalluri, Astuti Sharma, and Manmohan Chandraker. Memsac: Memory augmented sample consis-
353"
REFERENCES,0.8204592901878914,"tency for large scale domain adaptation. In Proceedings of the European Conference on Computer Vision,
354"
REFERENCES,0.8225469728601252,"pages 550–568, 2022.
355"
REFERENCES,0.824634655532359,"[22] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G. Hauptmann. Contrastive adaptation network for
356"
REFERENCES,0.826722338204593,"unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
357"
REFERENCES,0.8288100208768268,"Pattern Recognition, June 2019.
358"
REFERENCES,0.8308977035490606,"[23] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy
359"
REFERENCES,0.8329853862212944,"for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision
360"
REFERENCES,0.8350730688935282,"and Pattern Recognition, pages 10285–10295, 2019.
361"
REFERENCES,0.837160751565762,"[24] Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for
362"
REFERENCES,0.8392484342379958,"universal domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
363"
REFERENCES,0.8413361169102297,"Recognition, pages 9757–9766, 2021.
364"
REFERENCES,0.8434237995824635,"[25] Shuang Li, Chi Harold Liu, Qiuxia Lin, Qi Wen, Limin Su, Gao Huang, and Zhengming Ding. Deep
365"
REFERENCES,0.8455114822546973,"residual correction network for partial domain adaptation. IEEE Transactions on Pattern Analysis and
366"
REFERENCES,0.8475991649269311,"Machine Intelligence, 43(7):2329–2344, 2020.
367"
REFERENCES,0.8496868475991649,"[26] Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Umad: Universal model adaptation under domain and
368"
REFERENCES,0.8517745302713987,"category shift. arXiv preprint arXiv:2112.08553, 2021.
369"
REFERENCES,0.8538622129436325,"[27] Jian Liang, Yunbo Wang, Dapeng Hu, Ran He, and Jiashi Feng. A balanced and uncertainty-aware
370"
REFERENCES,0.8559498956158664,"approach for partial domain adaptation. In Proceedings of the European Conference on Computer Vision,
371"
REFERENCES,0.8580375782881002,"pages 123–140, 2020.
372"
REFERENCES,0.860125260960334,"[28] Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. Separate to adapt: Open set
373"
REFERENCES,0.8622129436325678,"domain adaptation via progressive separation. In Proceedings of the IEEE/CVF Conference on Computer
374"
REFERENCES,0.8643006263048016,"Vision and Pattern Recognition, pages 2927–2936, 2019.
375"
REFERENCES,0.8663883089770354,"[29] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain
376"
REFERENCES,0.8684759916492694,"adaptation. Advances in Neural Information Processing Systems, 31, 2018.
377"
REFERENCES,0.8705636743215032,"[30] Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for
378"
REFERENCES,0.872651356993737,"unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
379"
REFERENCES,0.8747390396659708,"Pattern Recognition, pages 9111–9120, 2020.
380"
REFERENCES,0.8768267223382046,"[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
381"
REFERENCES,0.8789144050104384,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
382"
REFERENCES,0.8810020876826722,"Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
383"
REFERENCES,0.8830897703549061,"Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In
384"
REFERENCES,0.8851774530271399,"Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
385"
REFERENCES,0.8872651356993737,"[32] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for
386"
REFERENCES,0.8893528183716075,"multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer
387"
REFERENCES,0.8914405010438413,"Vision, 2019.
388"
REFERENCES,0.8935281837160751,"[33] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The
389"
REFERENCES,0.8956158663883089,"visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.
390"
REFERENCES,0.8977035490605428,"[34] Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, and Changjun Jiang.
391"
REFERENCES,0.8997912317327766,"Upcycling models under domain and category shift. In Proceedings of the IEEE/CVF Conference on
392"
REFERENCES,0.9018789144050104,"Computer Vision and Pattern Recognition, 2023.
393"
REFERENCES,0.9039665970772442,"[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
394"
REFERENCES,0.906054279749478,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
395"
REFERENCES,0.9081419624217119,"transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors,
396"
REFERENCES,0.9102296450939458,"Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of
397"
REFERENCES,0.9123173277661796,"Machine Learning Research, pages 8748–8763. PMLR, 2021.
398"
REFERENCES,0.9144050104384134,"[36] Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex
399"
REFERENCES,0.9164926931106472,"Graves, and Timothy Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes.
400"
REFERENCES,0.918580375782881,"In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information
401"
REFERENCES,0.9206680584551148,"Processing Systems, volume 29. Curran Associates, Inc., 2016.
402"
REFERENCES,0.9227557411273486,"[37] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
403"
REFERENCES,0.9248434237995825,"domains. In Proceedings of the European Conference on Computer Vision, pages 213–226, 2010.
404"
REFERENCES,0.9269311064718163,"[38] Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, and Abir Das. Select, label, and mix:
405"
REFERENCES,0.9290187891440501,"Learning discriminative invariant feature representations for partial domain adaptation. In Proceedings of
406"
REFERENCES,0.9311064718162839,"the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4210–4219, 2023.
407"
REFERENCES,0.9331941544885177,"[39] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through
408"
REFERENCES,0.9352818371607515,"self supervision. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in
409"
REFERENCES,0.9373695198329853,"Neural Information Processing Systems, volume 33, pages 16282–16292. Curran Associates, Inc., 2020.
410"
REFERENCES,0.9394572025052192,"[40] Kuniaki Saito and Kate Saenko.
Ovanet: One-vs-all network for universal domain adaptation.
In
411"
REFERENCES,0.941544885177453,"Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9000–9009, 2021.
412"
REFERENCES,0.9436325678496869,"[41] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy
413"
REFERENCES,0.9457202505219207,"for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
414"
REFERENCES,0.9478079331941545,"Pattern Recognition, 2018.
415"
REFERENCES,0.9498956158663883,"[42] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by
416"
REFERENCES,0.9519832985386222,"backpropagation. In Proceedings of the European Conference on Computer Vision, pages 153–168, 2018.
417"
REFERENCES,0.954070981210856,"[43] Tasfia Shermin, Guojun Lu, Shyh Wei Teng, Manzur Murshed, and Ferdous Sohel. Adversarial network
418"
REFERENCES,0.9561586638830898,"with multiple classifiers for open set domain adaptation. IEEE Transactions on Multimedia, 23:2732–2744,
419"
REFERENCES,0.9582463465553236,"2020.
420"
REFERENCES,0.9603340292275574,"[44] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain
421"
REFERENCES,0.9624217118997912,"adaptation. arXiv preprint arXiv:1802.08735, 2018.
422"
REFERENCES,0.964509394572025,"[45] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In
423"
REFERENCES,0.9665970772442589,"C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
424"
REFERENCES,0.9686847599164927,"Processing Systems, volume 28. Curran Associates, Inc., 2015.
425"
REFERENCES,0.9707724425887265,"[46] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
426"
REFERENCES,0.9728601252609603,"network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision
427"
REFERENCES,0.9749478079331941,"and Pattern Recognition, 2017.
428"
REFERENCES,0.9770354906054279,"[47] Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain
429"
REFERENCES,0.9791231732776617,"adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
430"
REFERENCES,0.9812108559498957,"pages 2720–2729, 2019.
431"
REFERENCES,0.9832985386221295,"[48] Chaohui Yu, Jindong Wang, Yiqiang Chen, and Meiyu Huang. Transfer learning with dynamic adversarial
432"
REFERENCES,0.9853862212943633,"adaptation network. In 2019 IEEE International Conference on Data Mining, pages 778–786. IEEE, 2019.
433"
REFERENCES,0.9874739039665971,"[49] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for
434"
REFERENCES,0.9895615866388309,"partial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
435"
REFERENCES,0.9916492693110647,"Recognition, pages 8156–8164, 2018.
436"
REFERENCES,0.9937369519832986,"[50] Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain
437"
REFERENCES,0.9958246346555324,"adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
438"
REFERENCES,0.9979123173277662,"pages 5031–5040, 2019.
439"
