Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0033783783783783786,"Language models applied to NLP tasks take natural language as the direct model-
1"
ABSTRACT,0.006756756756756757,"ing object. But we believe that natural language is essentially a way of encoding
2"
ABSTRACT,0.010135135135135136,"information, therefore, the object of study for natural language should be the infor-
3"
ABSTRACT,0.013513513513513514,"mation encoded in language, and the organizational and compositional structure of
4"
ABSTRACT,0.016891891891891893,"the information described in language. Based on this understanding, we propose
5"
ABSTRACT,0.02027027027027027,"a database-based natural language processing method that changes the modeling
6"
ABSTRACT,0.02364864864864865,"object from natural language to the information encoded in natural language. On
7"
ABSTRACT,0.02702702702702703,"this basis, the sentence generation task is transformed into read operations imple-
8"
ABSTRACT,0.030405405405405407,"mented on the database and some sentence encoding rules to be followed; The
9"
ABSTRACT,0.033783783783783786,"sentence understanding task is transformed into sentence decoding rules and a se-
10"
ABSTRACT,0.037162162162162164,"ries of Boolean operations implemented on the database. Our method is closer
11"
ABSTRACT,0.04054054054054054,"to the information processing mechanism of the human brain and has excellent
12"
ABSTRACT,0.04391891891891892,"interpretability and scalability.
13"
INTRODUCTION,0.0472972972972973,"1
Introduction
14"
INTRODUCTION,0.05067567567567568,"Enabling machines to understand and use natural language as humans do is the ultimate goal of
15"
INTRODUCTION,0.05405405405405406,"NLP. Many language models have been developed for related NLP tasks. For example: Word2Vec
16"
INTRODUCTION,0.057432432432432436,"[2] and GloVe [3] models the correlations between words by constructing numerical representation
17"
INTRODUCTION,0.060810810810810814,"of words (i.e., word vector) and expect to obtain a word-level understanding by computing the simi-
18"
INTRODUCTION,0.06418918918918919,"larities between the word vectors. Seq2seq [6] and Transformer [7] are used for machine translation
19"
INTRODUCTION,0.06756756756756757,"tasks, they model the mapping relations between words and the mapping relations between sentence
20"
INTRODUCTION,0.07094594594594594,"structures in different languages. ELMo [4], GPT [5] and Bert [1] that pre-train language models on
21"
INTRODUCTION,0.07432432432432433,"a large-scale corpus, are aimed at modeling the sequence features in corpus.
22"
INTRODUCTION,0.0777027027027027,"All these approaches of language models are modeling the surface features of language, while ig-
23"
INTRODUCTION,0.08108108108108109,"noring the fact that natural language is only a way of encoding information. We believe that the
24"
INTRODUCTION,0.08445945945945946,"information described in natural language and the structural relations between these information
25"
INTRODUCTION,0.08783783783783784,"do not change depending on the choice of different encoding methods. Therefore, we propose a
26"
INTRODUCTION,0.09121621621621621,"database-based NLP method, which models the information represented by language and its organi-
27"
INTRODUCTION,0.0945945945945946,"zational and compositional structure described in language, and provides methods for various NLP
28"
INTRODUCTION,0.09797297297297297,"tasks, such as sentence generation and sentence understanding based on this model.
29"
INTRODUCTION,0.10135135135135136,"To summarize the contribution of this work:
30"
INTRODUCTION,0.10472972972972973,"•
Our method changes the modeling object from language to the information represented by lan-
31"
INTRODUCTION,0.10810810810810811,"guage, which makes the model we construct has excellent interpretability and scalability.
32"
INTRODUCTION,0.11148648648648649,"•
We propose a brand new NLP approach that is different from rule-based and statistical model-
33"
INTRODUCTION,0.11486486486486487,"based (i.e., language model) approaches, and it is more closer to the way the human brain processes
34"
INTRODUCTION,0.11824324324324324,"information.
35"
INTRODUCTION,0.12162162162162163,"Figure 1: Differences in modeling objects. Language model-based methods take natural language
as the object to construct the model. Database-based methods take both the information in the real
world and the organizational and compositional structure of the information described in natural
language as the objects to construct the model."
INTRODUCTION,0.125,"The United States is south of Canada.
Duke University is in North Carolina.
The cat is in Tom‘s room.
The table is in front of the fridge.
The cat is on top of the fridge.
The sofa is next to the fridge."
INTRODUCTION,0.12837837837837837,Table 1: Examples of sentences that describe the spatial position of target entities in the real world
INTRODUCTION,0.13175675675675674,"•
Our method directly confronts the challenge of “What is understanding ?” and “How to under-
36"
INTRODUCTION,0.13513513513513514,"stand ?” and provides a convincing solution to the challenge.
37"
BACKGROUND,0.13851351351351351,"2
Background
38"
BACKGROUND,0.14189189189189189,"There are many kinds of information encoded in natural language, which need to be modeled and
39"
BACKGROUND,0.14527027027027026,"processed according to their different nature and characteristics. In this paper, we only take the
40"
BACKGROUND,0.14864864864864866,"spatial position information of entities as the object, and construct a model accordingly by learning
41"
BACKGROUND,0.15202702702702703,"how it is described and encoded in the language. People encode the spatial position information of
42"
BACKGROUND,0.1554054054054054,"entities in the real world into sentences, as shown in Table 1, and communicate them to each other.
43"
BACKGROUND,0.15878378378378377,"Looking at the sentences in Table 1, we see that these sentences have the same structure: (Entity 1)
44"
BACKGROUND,0.16216216216216217,"+ (...) + (Spatial relation) + (Entity 2), where “Entity 1” is the target entity whose spatial position
45"
BACKGROUND,0.16554054054054054,"we want to describe by the sentence, “Entity 2” is a helper entity that helps to locate the target
46"
BACKGROUND,0.16891891891891891,"entity, and “Spatial relation” describes the spatial relation between the target entity and the helper
47"
BACKGROUND,0.17229729729729729,"entity. As shown in Table 2 , there are three types of spatial relations commonly used in languages:1)
48"
BACKGROUND,0.17567567567567569,"spatial range relations, 2) spatial directional relations, and 3) spatial distance relations. the spatial
49"
BACKGROUND,0.17905405405405406,"directional relations can be further divided into 2.1) absolute directional relations and 2.2) relative
50"
BACKGROUND,0.18243243243243243,"directional relations according to the different reference systems.
51"
BACKGROUND,0.1858108108108108,"The above ﬁndings in language reveal how people organize and store the spatial position information
52"
BACKGROUND,0.1891891891891892,"of entities in the real world. We can also see that people are used to using entities with a relatively
53"
BACKGROUND,0.19256756756756757,"stable spatial position as helper entities. We refer to entities with a relatively stable spatial position
54"
BACKGROUND,0.19594594594594594,"as immovable entities and entities whose spatial position is constantly changing as movable entities.
55"
BACKGROUND,0.19932432432432431,"The immovable entities and the spatial relations between them form a stable system that we will use
56"
BACKGROUND,0.20270270270270271,"to construct our model.
57"
MODEL ARCHITECTURE,0.20608108108108109,"3
Model Architecture
58"
MODEL ARCHITECTURE,0.20945945945945946,"We construct a tree-graph hybrid model to describe and store entities and the relative spatial relations
59"
MODEL ARCHITECTURE,0.21283783783783783,"between them. In a tree-graph hybrid model, the immovable entities are abstracted as square nodes
60"
MODEL ARCHITECTURE,0.21621621621621623,"and the movable entities are abstracted as round nodes. The spatial relations between the entities are
61"
ABSTRACT,0.2195945945945946,"abstracted as directed edges E. There are three steps to build our model:
62"
ABSTRACT,0.22297297297297297,"Spatial relations
Lexical representations
Reference system"
RANGE RELATIONS,0.22635135135135134,"1.Range relations
Inside: in, at...
Outside: outside of..."
"DIRECTIONAL
RELATIONS",0.22972972972972974,"2.
Directional
relations"
"ABSOLUTE
DIRECTIONAL
RELATIONS",0.23310810810810811,"2.1
Absolute
directional
relations"
"ABSOLUTE
DIRECTIONAL
RELATIONS",0.23648648648648649,"East: east of...
West: west of...
North: the north side of ...
South: the south side of..."
"RELATIVE
DIRECTIONAL
RELATIONS",0.23986486486486486,"2.2
Relative
directional
relations"
"RELATIVE
DIRECTIONAL
RELATIONS",0.24324324324324326,"Top: on, above, over, on top of...
Bottom: below, under, beneath...
Left: left of...
Right: the right side of...
Front: before, in front of...
Back: behind, back of..."
DISTANCE RELATIONS,0.24662162162162163,"3.Distance relations
by, near, next to, beside..."
DISTANCE RELATIONS,0.25,"Table 2: Classiﬁcation of the relative spatial relations between entities, and lexical representations
of the relative spatial relations."
TREE MODEL,0.2533783783783784,"3.1
Tree Model
63"
TREE MODEL,0.25675675675675674,"First, we use a tree model to describe the spatial range relations between entities. The spatial range
64"
TREE MODEL,0.26013513513513514,"relations Es is consist of two opposite directions, i.e.,Es =
{inside
←−,
outside
−→
}
. For example, we
65"
TREE MODEL,0.2635135135135135,"use the tree in Figure 2 to describe the spatial range relations between entities “North Carolina”,
66"
TREE MODEL,0.2668918918918919,"“Duke University”, “Tom‘s room”, “Classroom 15”, “Table”, “Sofa”, “Fridge”, “Cat”, “Tom” and
67"
TREE MODEL,0.2702702702702703,"“Blackboard”. The tree in Figure 2 can also be written in tabular form as shown in Table 3. In a
68"
TREE MODEL,0.27364864864864863,"tree, the child nodes with the same parent node should be spatially independent of each other, which
69"
TREE MODEL,0.27702702702702703,"means, there is no spatial range inclusion relation between them, if not, the child node must be
70"
TREE MODEL,0.28040540540540543,"moved up or down until all the child nodes are spatially independent of each other.
71"
TREE MODEL,0.28378378378378377,"Figure 2: A tree that describe the spatial range re-
lations between entities “North Carolina”, “Duke
University”, “Tom‘s room”, “Cat”, etc."
TREE MODEL,0.28716216216216217,"Figure 3: A graph that describe the absolute spa-
tial directional relations between some entities in
M1 in Figure 1."
GRAPH MODEL,0.2905405405405405,"3.2
Graph Model
72"
GRAPH MODEL,0.2939189189189189,"Then, We use graph models to describe the spatial directional relations between entities. The spatial
73"
GRAPH MODEL,0.2972972972972973,"directional relations can be future divided into 1) absolute directional relations Ea, which consists
74"
GRAPH MODEL,0.30067567567567566,"of four ﬁxed directions, i.e., Ea =
{east
−→,
west
−→,
North
−→,
South
−→
}
, and 2) relative directional relations
75"
GRAPH MODEL,0.30405405405405406,"Er, which consists of six ﬁxed directions, i.e., Er =
{left
−→,
right
−→,
front
−→,
back
−→,
top
−→,
bottom
−→
}
. Now, we
76"
GRAPH MODEL,0.30743243243243246,"can use the graph in Figure 3 to describe the absolute directional relations between some entities in
77"
GRAPH MODEL,0.3108108108108108,"M1 in Figure 1, and use the graph in Figure 4 to describe the relative directional relations between
78"
GRAPH MODEL,0.3141891891891892,"Es
V
North Carolina
Duke
University
Tom‘s room
Classroom 15
Fridge"
GRAPH MODEL,0.31756756756756754,"inside
−→"
GRAPH MODEL,0.32094594594594594,"Duke
University
Tom‘s room,
Table,
Blackboard,
Coke"
GRAPH MODEL,0.32432432432432434,"Classroom 15
Sofa,
Tom
Cat,
Fridge"
GRAPH MODEL,0.3277027027027027,"outside
←−
∅
North Carolina
Duke
University
Duke
University
Tom‘s room"
GRAPH MODEL,0.3310810810810811,Table 3: The tabular form of the tree in Figure 2
GRAPH MODEL,0.3344594594594595,"Ea
V
Kentucky
Virginia
Tennessee
North Carolina
Alabama"
GRAPH MODEL,0.33783783783783783,"east
−→
Virginia
∅
North Carolina
∅
∅
west
−→
∅
Kentucky
∅
Tennessee
∅
north
−→
∅
∅
Kentucky
Virginia
Tennessee
south
−→
Tennessee
North Carolina
Alabama
∅
∅"
GRAPH MODEL,0.34121621621621623,Table 5: The tabular form of the graph in Figure 3.
GRAPH MODEL,0.34459459459459457,"the entities in M3 in Figure 1. These two graphs can also be written in tabular forms as shown in
79"
GRAPH MODEL,0.34797297297297297,"Table 4 and Table 5.
80"
GRAPH MODEL,0.35135135135135137,"Figure 4: A graph to describe the relative spa-
tial directional relations between entities in M3
in Figure 1"
GRAPH MODEL,0.3547297297297297,"Er
V
Table
Fridge
Sofa"
GRAPH MODEL,0.3581081081081081,"left
−→
∅
∅
Fridge
right
−→
∅
Sofa
∅
front
−→
Fridge
∅
∅
back
−→
∅
Table
∅
top
−→
∅
Cat
bottom
−→
∅
∅
∅"
GRAPH MODEL,0.3614864864864865,"Table 4: The tabular form of the graph in Figure
4"
TREE-GRAPH HYBRID MODEL,0.36486486486486486,"3.3
Tree-graph Hybrid Model
81"
TREE-GRAPH HYBRID MODEL,0.36824324324324326,"At last, Take the nodes common to the tree and the graphs in Figures 2, 3 and 4 as connection points,
82"
TREE-GRAPH HYBRID MODEL,0.3716216216216216,"then we can integrate the tree and graphs into a tree-graph hybrid model as shown in Figure 5. The
83"
TREE-GRAPH HYBRID MODEL,0.375,"tree-graph hybrid model describes spatial range relations between entities on the vertical structure
84"
TREE-GRAPH HYBRID MODEL,0.3783783783783784,"(i.e., the inter-layer structure) and describes the spatial directional relations between entities on the
85"
TREE-GRAPH HYBRID MODEL,0.38175675675675674,"horizontal structure (i.e., the intralayer structure). In a tree-graph hybrid model, the immovable
86"
TREE-GRAPH HYBRID MODEL,0.38513513513513514,"entities and the stable spatial relations between them form a coordinate system, which can be used
87"
TREE-GRAPH HYBRID MODEL,0.3885135135135135,"to locate the entities in the model (or database). A tree-graph hybrid model can be continuously
88"
TREE-GRAPH HYBRID MODEL,0.3918918918918919,"extended upwards and downwards in the vertical structures to add new nodes, and continuously
89"
TREE-GRAPH HYBRID MODEL,0.3952702702702703,"subdivided in the horizontal structure to add new nodes. Therefore, the tree-graph hybrid model
90"
TREE-GRAPH HYBRID MODEL,0.39864864864864863,"could satisfy people’s need to describe and store the spatial position of numerous entities in the real
91"
TREE-GRAPH HYBRID MODEL,0.40202702702702703,"world. If the spatial position of one entity changes, just modiﬁed the related data accordingly in the
92"
TREE-GRAPH HYBRID MODEL,0.40540540540540543,"model. In addition, we can also build datasets to store the spatial position information of movable
93"
TREE-GRAPH HYBRID MODEL,0.40878378378378377,"Figure 5: An example of a tree-graph hybrid model, which describes the spatial range relations
between entities in the vertical structure (inter-layer structure), and describes the spatial directional
relations between entities in the horizontal structure (intralayer structure)"
TREE-GRAPH HYBRID MODEL,0.41216216216216217,"entities to record their footprint. In deed, the tree-graph hybrid model build a information exchange
94"
TREE-GRAPH HYBRID MODEL,0.4155405405405405,"bridge between language and the widely used numerical positioning systems, as shown in Figure 6.
95"
TREE-GRAPH HYBRID MODEL,0.4189189189189189,"Each layer of a tree-graph hybrid model can accommodate multiple subgraphs. Usually, the Ea
96"
TREE-GRAPH HYBRID MODEL,0.4222972972972973,"(absolute directional relations) is used as the reference system of the whole layer, and the Er (relative
97"
TREE-GRAPH HYBRID MODEL,0.42567567567567566,"directional relations) is used as the reference system in each subgraph. As shown in Figure 5, the
98"
TREE-GRAPH HYBRID MODEL,0.42905405405405406,"subgraph G2
0 and G1
0 take Er as their reference system, and the layer L0 is using Ea as its reference
99"
TREE-GRAPH HYBRID MODEL,0.43243243243243246,"system. In practice, when describing the spatial relation between two entities, lots of factors will
100"
TREE-GRAPH HYBRID MODEL,0.4358108108108108,"affect our choice, such as the distance situation between the entities, the scale situation of these
101"
TREE-GRAPH HYBRID MODEL,0.4391891891891892,"entities, etc. All of these factors can be summarized from practice and establish related rules, this
102"
TREE-GRAPH HYBRID MODEL,0.44256756756756754,"part will be discussed in the following application section.
103"
APPLICATION,0.44594594594594594,"4
Application
104"
APPLICATION,0.44932432432432434,"Based on the tree-graph hybrid model, we can now generate a database to describe and store the
105"
APPLICATION,0.4527027027027027,"spatial position of entities in the real world. This database can be used for many purposes. In this
106"
APPLICATION,0.4560810810810811,"paper, we only present its use in NLG and NLU tasks.
107"
NATURAL LANGUAGE GENERATION,0.4594594594594595,"4.1
Natural Language Generation
108"
NATURAL LANGUAGE GENERATION,0.46283783783783783,"The purpose of the sentence generation task is to encode the information that needs to be conveyed
109"
NATURAL LANGUAGE GENERATION,0.46621621621621623,"into sentences. It consists of two subtasks: a) determining the information that needs to be conveyed
110"
NATURAL LANGUAGE GENERATION,0.46959459459459457,"and b) encoding that information into sentences.
111"
READ DATA FROM THE DATABASE,0.47297297297297297,"4.1.1
Read Data From the Database
112"
READ DATA FROM THE DATABASE,0.47635135135135137,"In this paper, the information to be conveyed is the spatial position of the target entity. Following the
113"
READ DATA FROM THE DATABASE,0.4797297297297297,"language expression, we will use a helper entity and the spatial relations between the helper and the
114"
READ DATA FROM THE DATABASE,0.4831081081081081,"target entity to describe the spatial position of the target entity. For example, if we want to describe
115"
READ DATA FROM THE DATABASE,0.4864864864864865,"the spatial position of the entity “Cat”, we ﬁrst need to ﬁnd the corresponding node (target node) of
116"
READ DATA FROM THE DATABASE,0.48986486486486486,"the entity “Cat” in the database in Figure 5, then ﬁnd the helper nodes that have a spatial relation with
117"
READ DATA FROM THE DATABASE,0.49324324324324326,"Figure 6: Three ways to describing (or encoding) the spatial position information of entities in
the real world. And the routes of information exchange between different systems: 1⃝sentences
generation.
2⃝sentences understanding. 3⃝search for neighboring entities.
4⃝get the numerical
position information of target entities."
READ DATA FROM THE DATABASE,0.4966216216216216,"Figure 7: If we take the entity “Cat” as the target, then we can ﬁnd the above 5 data chains in the
database to help locate the entity “Cat”. The nodes marked with ""✓"" are the target nodes."
READ DATA FROM THE DATABASE,0.5,"the target node, such as the nodes “Tom‘s room”, “Fridge”, “Table”, “Duke University”, “Tennessee”
118"
READ DATA FROM THE DATABASE,0.5033783783783784,"and so on, then we can get 5 corresponding data chains as shown in Figure 7, which are composed of
119"
READ DATA FROM THE DATABASE,0.5067567567567568,"the target node, the helper node, and the spatial relations between them. Each of these 5 data chains
120"
READ DATA FROM THE DATABASE,0.5101351351351351,"can describe the spatial position of the entity “Cat”, but their precision is different. If we sort these 5
121"
READ DATA FROM THE DATABASE,0.5135135135135135,"data chains by precision, we can get the following result: L2 > L3 > L1 > L4 > L5. However, the
122"
READ DATA FROM THE DATABASE,0.5168918918918919,"precision is not the only goal we are pursuing. If we want to describe the spatial position of the entity
123"
READ DATA FROM THE DATABASE,0.5202702702702703,"“Cat” to a particular person, we also need to know how much this person knows about the spatial
124"
READ DATA FROM THE DATABASE,0.5236486486486487,"position of the 5 candidate helper nodes mentioned above, and what is the person‘s requirement for
125"
READ DATA FROM THE DATABASE,0.527027027027027,"descriptive precision, so that we can ﬁlter out the appropriate one accordingly. Here, we will skip
126"
READ DATA FROM THE DATABASE,0.5304054054054054,"this part and go straight to the sentence encoding part.
127"
ENCODING RULES FOR DATA CHAIN,0.5337837837837838,"4.1.2
Encoding Rules for Data Chain
128"
ENCODING RULES FOR DATA CHAIN,0.5371621621621622,"Although the rules for encoding a data chain into a sentence vary slightly from language to language,
129"
ENCODING RULES FOR DATA CHAIN,0.5405405405405406,"but the following parts are mandatory: 1) the target and the helper nodes in a data chain, 2) the
130"
ENCODING RULES FOR DATA CHAIN,0.543918918918919,"spatial relations between the target node and helper nodes in the data chain, and 3) the particular
131"
ENCODING RULES FOR DATA CHAIN,0.5472972972972973,"spatial correlation between the target node and helper nodes.
132"
ENCODING RULES FOR DATA CHAIN,0.5506756756756757,"Main parts
Data Chain
L1
L1*
L2
L2*"
"TARGET NODE
THE CAT
THE CAT
THE CAT
THE CAT",0.5540540540540541,"1
Target node
The cat
The cat
The cat
The cat
2
Helper node
Tom‘s room
Tom‘s room
the fridge
the fridge
3
Spatial relations
in
on top of
on top of
in"
PARTICULAR SPATIAL CORRELATION,0.5574324324324325,"4
Particular spatial correlation"
PARTICULAR SPATIAL CORRELATION,0.5608108108108109,"•
True
is
is
•
False
is not
is not"
PARTICULAR SPATIAL CORRELATION,0.5641891891891891,Table 6: Examples of the mandatory parts for encoding a data chain.
PARTICULAR SPATIAL CORRELATION,0.5675675675675675,"Data chain
Target node
Particular spatial
correlation
Spatial relation
Helper node"
PARTICULAR SPATIAL CORRELATION,0.5709459459459459,"L1
The cat
is
in
Tom‘s room
L2
The cat
is
on top of
the fridge
L3
The cat
is
in front of ( next to)
the table
L4
The cat
is
in
Duke University
L5
The cat
is
on the ease side of
Tennessee"
PARTICULAR SPATIAL CORRELATION,0.5743243243243243,"L1*
The cat
is not
on top of
Tom‘s room
L2*
The cat
is not
in
the fridge"
PARTICULAR SPATIAL CORRELATION,0.5777027027027027,"Table 7: Examples of sentence encoding for the data chains in Figure 7. All the above sentences are
100% correct, but some of them might be regarded as the right nonsense, and won’t be adopted in
practice due to their low precision in locating the target entity."
PARTICULAR SPATIAL CORRELATION,0.581081081081081,"Particular spatial correlation: A data chain can describe both “true information” and “false infor-
133"
PARTICULAR SPATIAL CORRELATION,0.5844594594594594,"mation”. Therefore, when encoding a data chain, speakers also need to give their opinion on whether
134"
PARTICULAR SPATIAL CORRELATION,0.5878378378378378,"the information described in the data chain is true or false. The speaker‘s opinion is described by a
135"
PARTICULAR SPATIAL CORRELATION,0.5912162162162162,"particular spatial correlation. For example, the particular spatial correlations that are listed in row 4
136"
PARTICULAR SPATIAL CORRELATION,0.5945945945945946,"of Table 6 are the speaker‘s opinions on the information described in the data chains in Table 6.
137"
PARTICULAR SPATIAL CORRELATION,0.597972972972973,"Operation rules for spatial relations: If there is only one directed edge in a data chain, we can
138"
PARTICULAR SPATIAL CORRELATION,0.6013513513513513,"encode it directly, such as the data chains L1 and L2. If there is more than one directed edge in a
139"
PARTICULAR SPATIAL CORRELATION,0.6047297297297297,"data chain, e.g., the data chains L3, L4 and L5, we should ﬁrst operate the directed edges in the data
140"
PARTICULAR SPATIAL CORRELATION,0.6081081081081081,"chain, then encode the result of the operation. Here, we summarize some operation rules as follows:
141"
PARTICULAR SPATIAL CORRELATION,0.6114864864864865,"•
Elimination operation: e.g.,
inside
−→+
outside
←−= ∅,
left
−→+
right
−→= ∅,
north
−→+
south
−→= ∅...
142"
PARTICULAR SPATIAL CORRELATION,0.6148648648648649,"•
Union operation: e.g.,
inside
−→+
inside
−→=
inside
−→,
east
−→+
east
−→+
north
−→=
northeast
−→
...
143"
PARTICULAR SPATIAL CORRELATION,0.6182432432432432,"•
Hybrid operation: when a data chain contains both spatial range relations and spatial directional
144"
PARTICULAR SPATIAL CORRELATION,0.6216216216216216,"relations, the relations in the upstream of the data chain is dominant, e.g.,
inside
−→+
top
−→=
inside
−→,
145"
PARTICULAR SPATIAL CORRELATION,0.625,"east
−→+
inside
−→=
east
−→...
146"
PARTICULAR SPATIAL CORRELATION,0.6283783783783784,"Applying the operation rules to the spatial relations on data chains L3, L4, and L5 yields the results
147"
PARTICULAR SPATIAL CORRELATION,0.6317567567567568,"below. Based on these operation results, we can encode the data chains L3, L4, and L5 into the
148"
PARTICULAR SPATIAL CORRELATION,0.6351351351351351,"sentences listed in Table 7.
149"
PARTICULAR SPATIAL CORRELATION,0.6385135135135135,"•
L3:
front
−→+
top
−→=
upfront
−→;
L4:
inside
−→+
inside
−→=
inside
−→;
L5:
east
−→+
inside
−→∗3 =
east
−→.
150"
PARTICULAR SPATIAL CORRELATION,0.6418918918918919,"Distance relations: In some cases, e.g.: 1) the spatial distance between the target entity and the
151"
PARTICULAR SPATIAL CORRELATION,0.6452702702702703,"helper entity is very close, or 2) it is not necessary to provide the exact position of the target entity,
152"
PARTICULAR SPATIAL CORRELATION,0.6486486486486487,"then we can use the spatial distance relations as an alternative, just like the sentence L3 in Table 7
153"
PARTICULAR SPATIAL CORRELATION,0.652027027027027,"You may argue that the sentences we generated are too simple. However, at the initial stage of
154"
PARTICULAR SPATIAL CORRELATION,0.6554054054054054,"language appearance, it is just some simple words and short sentences. With the development of
155"
PARTICULAR SPATIAL CORRELATION,0.6587837837837838,"human beings, more and more information is encoded in language, then sophisticated words and
156"
PARTICULAR SPATIAL CORRELATION,0.6621621621621622,"long sentences emerged. Therefore, it is a good start to launch our research with some simple words
157"
PARTICULAR SPATIAL CORRELATION,0.6655405405405406,"and sentences.
158"
PARTICULAR SPATIAL CORRELATION,0.668918918918919,Figure 8: The data chain L6 and its three different cases.
PARTICULAR SPATIAL CORRELATION,0.6722972972972973,"Data chain
Target node
Helper node"
PARTICULAR SPATIAL CORRELATION,0.6756756756756757,"L6
Duke University
is
in
North Carolina
.
L6-1
Is
Duke University
in
North Carolina
?
L6-2
Which state is
Duke University
in
?
L6-3
Which University
is
in
North Carolina
?"
PARTICULAR SPATIAL CORRELATION,0.6790540540540541,Table 8: Comparison of sentence structures that encode different information processing requests.
ENCODING RULES FOR PROCESSING REQUESTS,0.6824324324324325,"4.1.3
Encoding Rules for Processing Requests
159"
ENCODING RULES FOR PROCESSING REQUESTS,0.6858108108108109,"Sentences encode not only the speciﬁc information to be conveyed, but also the processing requests
160"
ENCODING RULES FOR PROCESSING REQUESTS,0.6891891891891891,"for that information. According to the implicit processing requests in the sentences, we divided
161"
ENCODING RULES FOR PROCESSING REQUESTS,0.6925675675675675,"sentences into following three categories: 1) data description sentence (i.e., declarative sentence),
162"
ENCODING RULES FOR PROCESSING REQUESTS,0.6959459459459459,"2) data veriﬁcation sentence (i.e., the yes-no question sentence), 3) data searching sentence (i.e.,
163"
ENCODING RULES FOR PROCESSING REQUESTS,0.6993243243243243,"WH-question sentence).
164"
ENCODING RULES FOR PROCESSING REQUESTS,0.7027027027027027,"Data description sentence: The processing requirement implicit in a data description sentence is
165"
ENCODING RULES FOR PROCESSING REQUESTS,0.706081081081081,"that listeners are expected to store the information in their databases. For example, teachers expect
166"
ENCODING RULES FOR PROCESSING REQUESTS,0.7094594594594594,"the students to remember what was taught in the class, and authors expect the readers to understand
167"
ENCODING RULES FOR PROCESSING REQUESTS,0.7128378378378378,"and remember the ideas shared in the book, and so on.
168"
ENCODING RULES FOR PROCESSING REQUESTS,0.7162162162162162,"Data veriﬁcation sentence: For a data veriﬁcation sentence, listeners are expected to help verify
169"
ENCODING RULES FOR PROCESSING REQUESTS,0.7195945945945946,"whether the particular spatial correlations described in the sentence exist in their database, and then
170"
ENCODING RULES FOR PROCESSING REQUESTS,0.722972972972973,"return the veriﬁcation result as the response. For example, if speakers are not sure whether the spatial
171"
ENCODING RULES FOR PROCESSING REQUESTS,0.7263513513513513,"relation “inside” between the node (Duke University) and the node (North Carolina) exists, as shown
172"
ENCODING RULES FOR PROCESSING REQUESTS,0.7297297297297297,"in data chain L6-1 in Figure 8, they could express the processing request that ask listeners to help
173"
ENCODING RULES FOR PROCESSING REQUESTS,0.7331081081081081,"verity whether the “inside” edge exists by moving the word “Is” to the beginning of the sentence and
174"
ENCODING RULES FOR PROCESSING REQUESTS,0.7364864864864865,"adding a question mark at the end of the sentence, as shown in Table 8.
175"
ENCODING RULES FOR PROCESSING REQUESTS,0.7398648648648649,"Data searching sentence: For a data searching sentence, listeners are expected to search for the
176"
ENCODING RULES FOR PROCESSING REQUESTS,0.7432432432432432,"missing information replaced by WH words in their databases and return the search result as the
177"
ENCODING RULES FOR PROCESSING REQUESTS,0.7466216216216216,"response. Take data chains L6-2 and L6-3 in Figure 8 as examples, speakers can use the word
178"
ENCODING RULES FOR PROCESSING REQUESTS,0.75,"“which” to replace the missing parts and adjust the structure of the sentences, as shown in rows L6-2
179"
ENCODING RULES FOR PROCESSING REQUESTS,0.7533783783783784,"and L6-3 in Table8, to express their expectation that the listener can help to search for the missing
180"
ENCODING RULES FOR PROCESSING REQUESTS,0.7567567567567568,"parts and return the search results.
181"
NATURAL LANGUAGE UNDERSTANDING,0.7601351351351351,"4.2
Natural Language Understanding
182"
NATURAL LANGUAGE UNDERSTANDING,0.7635135135135135,"In this paper, we only need to understand the spatial position information of the entities described
183"
NATURAL LANGUAGE UNDERSTANDING,0.7668918918918919,"in the sentences, the understanding of the other parts of the entities requires other databases, these
184"
NATURAL LANGUAGE UNDERSTANDING,0.7702702702702703,"databases will be published in other papers. The sentence understanding task consists of two parts:
185"
NATURAL LANGUAGE UNDERSTANDING,0.7736486486486487,"a) understanding of the processing requests of the speciﬁc information implicit in a sentence, and b)
186"
NATURAL LANGUAGE UNDERSTANDING,0.777027027027027,"understanding of the speciﬁc information conveyed in the sentence.
187"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.7804054054054054,"4.2.1
Understanding of the Processing Requests
188"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.7837837837837838,"The speciﬁc processing requests are expressed by the speciﬁc sentence structures, speciﬁc feature
189"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.7871621621621622,"words and speciﬁc punctuation. These can be used to classify the sentences and extract the process-
190"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.7905405405405406,"ing requests accordingly.
191"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.793918918918919,Figure 9: (1) General tree structure of sentences. (2) The sentence tree of sentence L1 in Table 7.
UNDERSTANDING OF THE PROCESSING REQUESTS,0.7972972972972973,"4.2.2
Understanding of the Speciﬁc Information
192"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8006756756756757,"First, listeners need to chunk the sentence and extract components of the speciﬁc information. Con-
193"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8040540540540541,"sidering the difference in the number of words and phrases used to represent each class of the com-
194"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8074324324324325,"ponents, the most efﬁcient way is to chunk the sentences according to the order in Figure 9 (1). For
195"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8108108108108109,"example, listeners can chunk the sentence L1 in Table 7 into the sentence tree shown in Figure 9
196"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8141891891891891,"(2). Then, listeners need to verify each parts of the sentence tree in their databases according to
197"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8175675675675675,"the ﬂowchart shown in Figure 10. In the case of the sentence tree in Figure 9, listeners should ﬁrst
198"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8209459459459459,"verity whether the helper entity “Tom‘s room” exists in their database, if the helper entity exists,
199"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8243243243243243,"go ahead; if not, it means that the listeners cannot get the position information of the target entity
200"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8277027027027027,"“The cat” through the helper entity “Tom‘s room”, so the understanding mission fails. If the helper
201"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.831081081081081,"entity “Tom‘s room” exists, the listeners needs to verify whether the target entity “The cat” exists at
202"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8344594594594594,"the other end of the directed edge (i.e., “Inside” edge), if the target entity “The cat” exists, it means
203"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8378378378378378,"that the listeners understand the information conveyed in the sentence tree, although the information
204"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8412162162162162,"conveyed in the sentence tree is known to the listeners; If not, the listeners can create a target node
205"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.8445945945945946,"at the other end of the “Inside” edge, to store the spatial position information of “The cat” in their
206"
UNDERSTANDING OF THE PROCESSING REQUESTS,0.847972972972973,"database.
207"
RESPONDING TO THE PROCESSING REQUESTS,0.8513513513513513,"4.2.3
Responding to the Processing Requests
208"
RESPONDING TO THE PROCESSING REQUESTS,0.8547297297297297,"Strictly speaking, responding to the processing requests implicit in a sentence is not the sentence
209"
RESPONDING TO THE PROCESSING REQUESTS,0.8581081081081081,"understanding task, but the sentence generation task. Here, we brieﬂy introduce the responses to the
210"
RESPONDING TO THE PROCESSING REQUESTS,0.8614864864864865,"different processing requests. In a data description sentence, the response is to store the speciﬁc
211"
RESPONDING TO THE PROCESSING REQUESTS,0.8648648648648649,"information conveyed in the sentence, just as the step marked with a star in Figure 10. For a data
212"
RESPONDING TO THE PROCESSING REQUESTS,0.8682432432432432,"veriﬁcation sentence, the response is to return the veriﬁcation results to the speakers. Take the
213"
RESPONDING TO THE PROCESSING REQUESTS,0.8716216216216216,"sentence L6-1 in Table 8 as an example, in the listeners database, if the directed edge represented
214"
RESPONDING TO THE PROCESSING REQUESTS,0.875,"by the word “in” can be found between the node “Duke University” and node “North Carolina”, the
215"
RESPONDING TO THE PROCESSING REQUESTS,0.8783783783783784,"listener can reply “Yes, it is” as feedback to the speaker. If not, the listener can reply “No, it is not”
216"
RESPONDING TO THE PROCESSING REQUESTS,0.8817567567567568,"as feedback to the speaker. For a data searching sentence, the response is to return the searching
217"
RESPONDING TO THE PROCESSING REQUESTS,0.8851351351351351,"results to the speakers. Take the sentence L6-2 in Table 8 as an example, in the listener‘s database, if
218"
RESPONDING TO THE PROCESSING REQUESTS,0.8885135135135135,"there is a node on the other end of the directed edge (which represented by the word “in”), the search
219"
RESPONDING TO THE PROCESSING REQUESTS,0.8918918918918919,"mission succeeded, and the listener can give the lexical representation of that node to the speaker. If
220"
RESPONDING TO THE PROCESSING REQUESTS,0.8952702702702703,"not, the listener can reply “I don‘t know” or “I don‘t have a clue” to the speaker, to let him or her
221"
RESPONDING TO THE PROCESSING REQUESTS,0.8986486486486487,"know that the search mission failed.
222"
CONCLUSION,0.902027027027027,"5
Conclusion
223"
CONCLUSION,0.9054054054054054,"We demonstrate the feasibility of the database-based method for NLG and NLU tasks, which takes
224"
CONCLUSION,0.9087837837837838,"information encoded in natural language as the object of study. So, what exactly are we study
225"
CONCLUSION,0.9121621621621622,"about natural language? As we have learned in neuroscience, humans receive information through
226"
CONCLUSION,0.9155405405405406,"neural pathways such as eyes, ears, mouth, nose, etc., and then send this received information to
227"
CONCLUSION,0.918918918918919,"the brain for hierarchical processing and storage. Although we cannot explore how this information
228"
CONCLUSION,0.9222972972972973,"is processed and stored in human brains, but a small proportion of this information is encoded as
229"
CONCLUSION,0.9256756756756757,"natural language for external output. Thus, by studying natural language, we can investigate the
230"
CONCLUSION,0.9290540540540541,"mechanisms by which information is stored and processed in the human brain.
231"
CONCLUSION,0.9324324324324325,Figure 10: Understnading ﬂowchart of data description sentences.
REFERENCES,0.9358108108108109,"References
232"
REFERENCES,0.9391891891891891,"[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
233"
REFERENCES,0.9425675675675675,"transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
234"
REFERENCES,0.9459459459459459,"[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in
235"
REFERENCES,0.9493243243243243,"vector space. arXiv preprint arXiv:1301.3781, 2013.
236"
REFERENCES,0.9527027027027027,"[3] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation.
237"
REFERENCES,0.956081081081081,"In Proceedings of the 2014 conference on empirical methods in natural language processing
238"
REFERENCES,0.9594594594594594,"(EMNLP), pages 1532–1543, 2014.
239"
REFERENCES,0.9628378378378378,"[4] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettle-
240"
REFERENCES,0.9662162162162162,"moyer. Deep contextualized word representations. corr abs/1802.05365 (2018). arXiv preprint
241"
REFERENCES,0.9695945945945946,"arXiv:1802.05365, 1802.
242"
REFERENCES,0.972972972972973,"[5] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding
243"
REFERENCES,0.9763513513513513,"with unsupervised learning. 2018.
244"
REFERENCES,0.9797297297297297,"[6] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks.
245"
REFERENCES,0.9831081081081081,"Advances in neural information processing systems, 27, 2014.
246"
REFERENCES,0.9864864864864865,"[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and
247"
REFERENCES,0.9898648648648649,"I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
248"
REFERENCES,0.9932432432432432,"R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
249"
REFERENCES,0.9966216216216216,"Systems, volume 30. Curran Associates, Inc., 2017.
250"
