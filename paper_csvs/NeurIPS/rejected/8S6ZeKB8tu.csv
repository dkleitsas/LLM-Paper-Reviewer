Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002421307506053269,"The evaluation of noisy binary classifiers on unlabeled data is treated as a streaming
1"
ABSTRACT,0.004842615012106538,"task - given a data sketch of the decisions by an ensemble, estimate the true
2"
ABSTRACT,0.007263922518159807,"prevalence of the labels as well as each classifier’s accuracy on them. Two fully
3"
ABSTRACT,0.009685230024213076,"algebraic evaluators are constructed to do this. Both are based on the assumption
4"
ABSTRACT,0.012106537530266344,"that the classifiers make independent errors on the test items. The first is based
5"
ABSTRACT,0.014527845036319613,"on majority voting. The second, the main contribution of the paper, is guaranteed
6"
ABSTRACT,0.01694915254237288,"to be correct for independent classifiers. But how do we know the classifiers are
7"
ABSTRACT,0.01937046004842615,"error independent on any given test? This principal/agent monitoring paradox is
8"
ABSTRACT,0.021791767554479417,"ameliorated by exploiting the failures of the independent evaluator to return sensible
9"
ABSTRACT,0.024213075060532687,"estimates. Some of these failures can be traced to producing algebraic versus real
10"
ABSTRACT,0.026634382566585957,"numbers while evaluating a finite test. A search for nearly error independent trios
11"
ABSTRACT,0.029055690072639227,"is empirically carried out on the adult, mushroom, and two-norm datasets by
12"
ABSTRACT,0.031476997578692496,"using these algebraic failure modes to reject potential evaluation ensembles as too
13"
ABSTRACT,0.03389830508474576,"correlated. At its final steps, the searches are refined by constructing a surface
14"
ABSTRACT,0.03631961259079903,"in evaluation space that must contain the true value point. The surface comes
15"
ABSTRACT,0.0387409200968523,"from considering the algebra of arbitrarily correlated classifiers and selecting a
16"
ABSTRACT,0.04116222760290557,"polynomial subset that is free of any correlation variables. Candidate evaluation
17"
ABSTRACT,0.043583535108958835,"ensembles are then rejected if their data sketches produce independent evaluation
18"
ABSTRACT,0.04600484261501211,"estimates that are too far from the constructed surface. The results produced by the
19"
ABSTRACT,0.048426150121065374,"surviving evaluation ensembles can sometimes be as good as 1%. But handling
20"
ABSTRACT,0.05084745762711865,"even small amounts of correlation remains a challenge. A Taylor expansion of the
21"
ABSTRACT,0.053268765133171914,"estimates produced when error independence is assumed but the classifiers are,
22"
ABSTRACT,0.05569007263922518,"in fact, slightly correlated helps clarify how the proposed independent evaluator
23"
ABSTRACT,0.05811138014527845,"has algebraic ‘blind spots’ of its own. They are points in evaluation space but the
24"
ABSTRACT,0.06053268765133172,"estimate of the independent evaluator has a sensitivity inversely proportional to
25"
ABSTRACT,0.06295399515738499,"the distance of the true point from them. How algebraic stream evaluation can and
26"
ABSTRACT,0.06537530266343826,"cannot help when done for safety or economic reasons is briefly discussed.
27"
INTRODUCTION,0.06779661016949153,"1
Introduction
28"
INTRODUCTION,0.07021791767554479,"Streaming algorithms compute sample statistics of a data stream. A data sketch, selected to fit the
29"
INTRODUCTION,0.07263922518159806,"sample statistic one wants to compute, is updated every time a new item appears in the stream. A
30"
INTRODUCTION,0.07506053268765134,"simple example of such a streaming algorithm is the use of two counters to compute the average
31"
INTRODUCTION,0.0774818401937046,"value of a stream of numbers,
32"
INTRODUCTION,0.07990314769975787,"n, sum = n
X"
INTRODUCTION,0.08232445520581114,"i
xi
(1) (2)"
INTRODUCTION,0.0847457627118644,"Figure 1: Stream of label predictions by three binary classifiers. Eight integer counters are enough to
tally the number of times a particular prediction or voting pattern occurs when looking at per-item
decision events."
INTRODUCTION,0.08716707021791767,"The first integer counter tallies how many numbers have been observed so far. The second keeps
33"
INTRODUCTION,0.08958837772397095,"a running total of the observed values. This data sketch is then used to compute the mean of the
34"
INTRODUCTION,0.09200968523002422,"observed stream,
35"
INTRODUCTION,0.09443099273607748,"sum/n.
(3)"
INTRODUCTION,0.09685230024213075,"Two things are notable about this simple algorithm. It only uses observed variables, and its computa-
36"
INTRODUCTION,0.09927360774818401,"tion is purely algebraic with them. There are no free parameters to tune or know beforehand. This
37"
INTRODUCTION,0.1016949152542373,"paper discusses evaluation algorithms for binary classifiers that act similarly.
38"
INTRODUCTION,0.10411622276029056,"An evaluation on a finite set of labeled data is defined by the sample statistics it computes based on
39"
INTRODUCTION,0.10653753026634383,"knowledge of the true labels. Unlabeled evaluation, the problem considered here, is the estimation of
40"
INTRODUCTION,0.1089588377723971,"the values of these same sample statistics when the true labels are not known. This paper looks at
41"
INTRODUCTION,0.11138014527845036,"how to do so when all we have are statistics of the decisions the members of an ensemble of noisy
42"
INTRODUCTION,0.11380145278450363,"binary classifiers make - it is a ‘black-box’ algorithm. There are 2n possible prediction events when
43"
INTRODUCTION,0.1162227602905569,"we are concerned only about per-item evaluation statistics for n binary classifiers. Figure 1 shows
44"
INTRODUCTION,0.11864406779661017,"how an example of a stream labeled by three classifiers. To keep track of the decisions events at the
45"
INTRODUCTION,0.12106537530266344,"per-item level, only 8 integer counters are needed.
46"
INTRODUCTION,0.1234866828087167,"For three binary classifiers, a basic set of evaluation statistics is defined by the prevalence of one
47"
INTRODUCTION,0.12590799031476999,"of the labels, say label α, ˆPα, and the label accuracies, ˆPi,α and ˆPi,β, for each of the classifiers.
48"
INTRODUCTION,0.12832929782082325,"Note that these are per item statistics. They cannot quantify performance across items in the stream.
49"
INTRODUCTION,0.13075060532687652,"Theorem 1 asserts that these variables are complete to explain the data sketches formed from the
50"
INTRODUCTION,0.13317191283292978,"aligned decisions of independent classifiers. If we knew the value of the value of these evaluation
51"
INTRODUCTION,0.13559322033898305,"statistics, we can predict exactly the value of the per-item data sketch counters. The challenge in
52"
INTRODUCTION,0.13801452784503632,"unlabeled evaluation is to go the other way - to obtain estimates of the basic evaluation statistics
53"
INTRODUCTION,0.14043583535108958,"starting from the data sketch.
54"
INTRODUCTION,0.14285714285714285,"Two evaluators for binary classifiers that are fully algebraic are built. The first is based on majority
55"
INTRODUCTION,0.14527845036319612,"voting (MV). It decides what the correct answer key must be for the test. This makes it nearly
56"
INTRODUCTION,0.14769975786924938,"impossible for it to return a correct answer even when its assumptions are satisfied. Even worse, it
57"
INTRODUCTION,0.15012106537530268,"always provides seemingly correct estimates even when its assumptions are violated. The second
58"
INTRODUCTION,0.15254237288135594,"evaluator is fully inferential. It never decides what the true label is for any items. Theorem 2 proves
59"
INTRODUCTION,0.1549636803874092,"that this approach will correctly trap the true evaluation point to just two point candidates in evaluation
60"
INTRODUCTION,0.15738498789346247,"space. Unlike the MV evaluator, it can return obviously incorrect estimates.
61"
INTRODUCTION,0.15980629539951574,"This paper assumes that these streaming evaluators are being deployed in an environment that has a
62"
INTRODUCTION,0.162227602905569,"principal/agent monitoring paradox. Evaluation ensembles, like decision ensembles, work best when
63"
INTRODUCTION,0.16464891041162227,"they are independent in their error. Just as one would not want to incur the technical debt of having
64"
INTRODUCTION,0.16707021791767554,"an ensemble of classifiers that always agreed, it makes little sense to deploy evaluation ensembles
65"
INTRODUCTION,0.1694915254237288,"that are highly correlated. This raises two challenges when working on unlabeled data - how do we
66"
INTRODUCTION,0.17191283292978207,"find these error independent evaluation trios, and how do we know, on any given evaluation, that they
67"
INTRODUCTION,0.17433414043583534,"are still independent, or nearly so?
68"
INTRODUCTION,0.17675544794188863,"The approach taken here to ameliorate this monitoring paradox is that the failures of the independent
69"
INTRODUCTION,0.1791767554479419,"evaluator can be used to exclude evaluation trios that are too correlated. The independent evaluator,
70"
INTRODUCTION,0.18159806295399517,"by construction, is deterministic and always returns algebraic numbers. But its answers do not always
71"
INTRODUCTION,0.18401937046004843,"make sense. We can detect this because we have prior knowledge about what seemingly correct
72"
INTRODUCTION,0.1864406779661017,"evaluation estimates look like.
73"
PROPERTIES OF THE TRUE EVALUATION POINT,0.18886198547215496,"1.1
Properties of the true evaluation point
74"
PROPERTIES OF THE TRUE EVALUATION POINT,0.19128329297820823,"All the basic evaluation statistics are integer ratios by construction. For example, ˆPα, must be the ratio
75"
PROPERTIES OF THE TRUE EVALUATION POINT,0.1937046004842615,"of two integers. Its numerator is some integer between 0 and the size of the test. The denominator,
76"
PROPERTIES OF THE TRUE EVALUATION POINT,0.19612590799031476,"the size of the test. By construction, their ratio lies inside the unit interval. Similar considerations
77"
PROPERTIES OF THE TRUE EVALUATION POINT,0.19854721549636803,"apply to any of the label accuracies for the classifiers - their true value must be an unknown integer
78"
PROPERTIES OF THE TRUE EVALUATION POINT,0.2009685230024213,"ratio in the unit interval.
79"
PROPERTIES OF THE TRUE EVALUATION POINT,0.2033898305084746,"Seemingly correct estimates are estimated values that seem to be correct because they have this real,
80"
PROPERTIES OF THE TRUE EVALUATION POINT,0.20581113801452786,"integer ratio form. Estimates that do not have this form are obviously incorrect. The naive evaluator
81"
PROPERTIES OF THE TRUE EVALUATION POINT,0.20823244552058112,"built using majority voting by the ensemble always returns seemingly correct answers, never alerting
82"
PROPERTIES OF THE TRUE EVALUATION POINT,0.2106537530266344,"its users that it is wrong and its evaluation assumptions do not apply on a given test.
83"
PROPERTIES OF THE TRUE EVALUATION POINT,0.21307506053268765,"The independent evaluator constructed from Theorems 1 and 2 is not like that. It fails, with varying
84"
PROPERTIES OF THE TRUE EVALUATION POINT,0.21549636803874092,"degrees, to return seemingly correct estimates when the assumptions of a test are violated. The failures
85"
PROPERTIES OF THE TRUE EVALUATION POINT,0.2179176755447942,"vary in their severity. The empirical hypothesis explored here is that their severity are indicative of
86"
PROPERTIES OF THE TRUE EVALUATION POINT,0.22033898305084745,"the magnitude of the unknown decision correlations that are needed to correctly predict the observed
87"
PROPERTIES OF THE TRUE EVALUATION POINT,0.22276029055690072,"data sketch.
88"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.22518159806295399,"1.2
A sample definition of decision error independence
89"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.22760290556900725,"Theorem 2 provides a closed, algebraic solution to the evaluation variety defined by the data sketch
90"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.23002421307506055,"of error independent classifiers. The variety, the geometrical object in variable space that contains all
91"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2324455205811138,"points that satisfy a polynomial ideal, consists of just two points for these classifiers. But as noted
92"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.23486682808716708,"above, independent evaluation ensembles are rare.
93"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.23728813559322035,"Handling correlation correctly requires that we introduce new evaluation variables to quantify it.
94"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2397094430992736,"Theorem 3 provides a a constructive proof of how to connect the data sketch of correlated classifiers
95"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.24213075060532688,"to polynomials using these new correlation statistics plus those in the basic evaluation set. The
96"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.24455205811138014,"evaluation variety, the set of values for the evaluation statistics that solves the polynomials, is not
97"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2469733656174334,"solved here for correlated classifiers. Nonetheless, it can be proven that the variety exists, its exact
98"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.24939467312348668,"shape in evaluation space to be determined in future work. Nonetheless, a partial characterization of
99"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.25181598062953997,"its shape is possible because of Theorem 3. The same process that solved the polynomial system for
100"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2542372881355932,"independent classifiers in Theorem 2 achieves a partial disentanglement of the variables when they
101"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2566585956416465,"are correlated. This defines a subset of the generating set that defines a surface computable without
102"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.25907990314769974,"any knowledge of the correlations between the classifiers. This surface is not the evaluation variety
103"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.26150121065375304,"but is guaranteed to contain it. This surface is used in the experiments to construct nearly independent
104"
A SAMPLE DEFINITION OF DECISION ERROR INDEPENDENCE,0.2639225181598063,"evaluation ensembles.
105"
PREVIOUS WORK AND RELATED TOPICS,0.26634382566585957,"1.3
Previous work and related topics
106"
PREVIOUS WORK AND RELATED TOPICS,0.2687651331719128,"A mathematical treatment of the correctness of the decisions made when correct labels are assigned
107"
PREVIOUS WORK AND RELATED TOPICS,0.2711864406779661,"to majority voting dates back to Condorcet’s analysis of the correctness of human juries during the
108"
PREVIOUS WORK AND RELATED TOPICS,0.2736077481840194,"French Revolution. But it was not until almost two centuries later that a mathematical treatment of
109"
PREVIOUS WORK AND RELATED TOPICS,0.27602905569007263,"using juries, this time human doctors, to evaluate themselves using only their aligned decisions was
110"
PREVIOUS WORK AND RELATED TOPICS,0.2784503631961259,"published by Dawid and Skene [1]. It proposed a probabilistic solution to evaluation by minimizing
111"
PREVIOUS WORK AND RELATED TOPICS,0.28087167070217917,"a likelihood function using the EM algorithm. This work was followed up in the early 2010s by a
112"
PREVIOUS WORK AND RELATED TOPICS,0.28329297820823246,"succession of papers in the NeurIPS conferences that took a Bayesian approach to constructing stream
113"
PREVIOUS WORK AND RELATED TOPICS,0.2857142857142857,"evaluator ([2], [3], [4], [5], [6]). Applications to evaluating workers in crowd-sourcing platforms
114"
PREVIOUS WORK AND RELATED TOPICS,0.288135593220339,"was a big motivator for some of this research. The algebraic methodology proposed here would be
115"
PREVIOUS WORK AND RELATED TOPICS,0.29055690072639223,"economically impractical in such applications since it requires all the classifiers labeling every item
116"
PREVIOUS WORK AND RELATED TOPICS,0.2929782082324455,"in the stream.
117"
PREVIOUS WORK AND RELATED TOPICS,0.29539951573849876,"The algebraic approach proposed here is closest to another probabilistic method, one proposed by
118"
PREVIOUS WORK AND RELATED TOPICS,0.29782082324455206,"Parisi et al. [7]. Rather than minimizing a likelihood function, it considers the spectral properties of
119"
PREVIOUS WORK AND RELATED TOPICS,0.30024213075060535,"matrices created by moments of the observed decisions. By hypothesizing hidden distributions, it then
120"
PREVIOUS WORK AND RELATED TOPICS,0.3026634382566586,"tries to carry out a matrix decomposition that eventually gives evaluation estimates. In contrast, the
121"
PREVIOUS WORK AND RELATED TOPICS,0.3050847457627119,"independent evaluator proposed here is purely algebraic. It invokes no assumptions about distributions.
122"
PREVIOUS WORK AND RELATED TOPICS,0.3075060532687651,"Nonetheless, Theorems 1 and 2 discussed here should be contrasted with Theorem 1 in the paper
123"
PREVIOUS WORK AND RELATED TOPICS,0.3099273607748184,"by Jaffe et al. [8], a solution for distribution-independent classifiers. The Supplement details the
124"
PREVIOUS WORK AND RELATED TOPICS,0.31234866828087166,"mathematical similarities and differences between the two solutions.
125"
PREVIOUS WORK AND RELATED TOPICS,0.31476997578692495,"But research on direct evaluation seems to have waned since the 2010s. The 1st NeurIPS workshop
126"
PREVIOUS WORK AND RELATED TOPICS,0.3171912832929782,"on AI safety occurred last year and its RFP had a section on monitoring and anomaly detection that
127"
PREVIOUS WORK AND RELATED TOPICS,0.3196125907990315,"does not cite any of the above work [9]. Instead, research has focused on other aspects of monitoring
128"
PREVIOUS WORK AND RELATED TOPICS,0.3220338983050847,"that are important for AI safety - for example, carrying out risk minimization computations given
129"
PREVIOUS WORK AND RELATED TOPICS,0.324455205811138,"unknown operating points for an ensemble as was done by Steinhardt et al. in [10].
130"
PREVIOUS WORK AND RELATED TOPICS,0.3268765133171913,"Although stream algorithms do not usually calculate hidden knowledge statistics in a stream, some
131"
PREVIOUS WORK AND RELATED TOPICS,0.32929782082324455,"do. Good-Turing frequency smoothing [11] is one such algorithm. Its core intellectual idea - that
132"
PREVIOUS WORK AND RELATED TOPICS,0.33171912832929784,"you can estimate the probability of seeing hitherto unseen types of items using only the count for
133"
PREVIOUS WORK AND RELATED TOPICS,0.3341404358353511,"observed types - is used by LLMs whenever they want to estimate token sequences that were never
134"
PREVIOUS WORK AND RELATED TOPICS,0.3365617433414044,"observed during training.
135"
PREVIOUS WORK AND RELATED TOPICS,0.3389830508474576,"Finally, most of the mathematical tools used in this paper come from algebraic geometry (AG) [12].
136"
PREVIOUS WORK AND RELATED TOPICS,0.3414043583535109,"The use of algebraic concerns to study statistical problems was pioneered by Pistone et al. [13]. The
137"
PREVIOUS WORK AND RELATED TOPICS,0.34382566585956414,"topic is known as algebraic statistics. Like most of statistics, it is focused mostly on topics related to
138"
PREVIOUS WORK AND RELATED TOPICS,0.34624697336561744,"inferring distributions. This paper uses AG to estimate sample statistics.
139"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3486682808716707,"2
Using majority voting to evaluate noisy classifiers
140"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.35108958837772397,"The correctness of decisions made by an ensemble that majority votes (MV) depends, roughly
141"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.35351089588377727,"speaking, on two things. They must be error independent, and their labeling accuracies must be
142"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3559322033898305,"greater than 50% on each label. Item labels decided by majority voting will be correct more often
143"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3583535108958838,"than not if these conditions are met. This section details how a naive algebraic stream evaluator can
144"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.36077481840193704,"be built on the basis of this decision algorithm. It works by imputing the correct labels or answer key
145"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.36319612590799033,"for the observed items.
146"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.36561743341404357,"The integer counters of the per-item data sketch can be trivially turned into frequency variables. Each
147"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.36803874092009686,"of the counters in a decision sketch tallies how often a decision event has been seen in the stream so
148"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3704600484261501,"far. There are only 8 decision events when considering the per-item decisions of three classifiers. The
149"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3728813559322034,"sum of their fractional frequencies, fℓ1,ℓ2,ℓ3, must sum to one,
150"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.37530266343825663,"fα,α,α + fα,α,β + fα,β,α + fβ,α,α + fβ,β,α + fβ,α,β + fα,β,β + fβ,β,β = 1.
(4)"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.37772397094430993,"The logic of MV evaluation is straightforward. The true label is given by MV, therefore the prevalence
151"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3801452784503632,"of a label is equal to the frequency that label was the majority vote. The estimate for the α label is
152"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.38256658595641646,"thus a simple linear equation of these ensemble decision frequencies,
153"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.38498789346246975,"ˆP (MV)
α
= fα,α,α + fα,α,β + fα,β,α + fβ,α,α.
(5)"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.387409200968523,"Similarly, we can write down algebraic formulas of the decision frequencies for each classifiers label
154"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3898305084745763,"accuracy. For classifier 1, those estimates are,
155"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3922518159806295,"ˆP (MV)
α
= 1 −
fβ,α,α
fα,α,α + fα,α,β + fα,β,α + fα,β,α
(6)"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.3946731234866828,"ˆP (MV)
β
= 1 −
fα,β,β
fβ,β,β + fβ,β,α + fβ,α,β + fα,β,β
.
(7)"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.39709443099273606,"The MV evaluator considers a classifier wrong if it votes against the majority.
156"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.39951573849878935,"These are algebraic functions of the frequencies derived from the data sketch, there are no free
157"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.4019370460048426,"parameters. In addition, the estimates returned by them are always seemingly correct. All the MV
158"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.4043583535108959,"estimates of prevalence and label accuracies are integer ratios inside the unit interval. The MV
159"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.4067796610169492,"evaluator will never be able to alert its user that its assumptions are incorrect even when they are
160"
USING MAJORITY VOTING TO EVALUATE NOISY CLASSIFIERS,0.4092009685230024,"wildly off the mark.
161"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4116222760290557,"2.1
The drawbacks of evaluating by deciding
162"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.41404358353510895,"Decision and inference are traditionally recognized as separate concerns in Machine Learning.
163"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.41646489104116224,"Avoiding decisions and its hard choices until they are absolutely required typically leads to better
164"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4188861985472155,"performance. So it is here. Making a hard choice on the true label is going to be incorrect on some
165"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4213075060532688,"unknown fraction of the events that produced a particular voting pattern. Some of the times the
166"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.423728813559322,"ensemble voted (α, β, α) it could have been a βitem, not an αone. This is expressed by the following
167"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4261501210653753,"equation,
168"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.42857142857142855,"nℓ1,ℓ2,ℓ3 = #(ℓ1, ℓ2, ℓ3 | α) + #(ℓ1, ℓ2, ℓ3 | β).
(8)"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.43099273607748184,"The number of times we saw a voting pattern is equal to the sum of times the items were α plus the
169"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.43341404358353514,"times it was β. For any voting pattern by the ensemble, both labels are possible for any one item, no
170"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4358353510895884,"matter what the majority says. Zeroing out one term in this sum is an approximation. The supplement
171"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.43825665859564167,"works out how this decision step means that the MV evaluator is hardly ever right even though it
172"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4406779661016949,"always seems so. Fixing this naive MV evaluator is easy - include both terms when expressing data
173"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.4430992736077482,"sketch frequencies. Carrying out evaluation with these full equations is much harder but leads to an
174"
THE DRAWBACKS OF EVALUATING BY DECIDING,0.44552058111380144,"evaluator that is guaranteed to be correct when its assumptions are true.
175"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.44794188861985473,"3
Fully inferential evaluation of sample independent binary classifiers
176"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.45036319612590797,"Systems of equations can be wrong. Care must also be taken that they they define objects that exist so
177"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.45278450363196127,"as to avoid making statements about non-existing entities. The two mathematical objects of concern
178"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4552058111380145,"here are systems of polynomial equations and the geometrical objects consisting of the points that
179"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4576271186440678,"solve them. The following theorem does this for error independent classifiers. It establishes that the
180"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4600484261501211,"basic evaluation statistics are sufficient to explain all observed data sketches created by them.
181"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.46246973365617433,"Theorem 1. The per-item data sketch produced by independent classifiers is complete when expressed
182"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4648910411622276,"as polynomials of variables in the basic evaluation set.
183"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.46731234866828086,"fα,α,α = ˆPα ˆP1,α ˆP2,α ˆP3,α + (1 −ˆPα)(1 −ˆP1,β)(1 −ˆP2,β)(1 −ˆP3,β)
(9)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.46973365617433416,"fα,α,β = ˆPα ˆP1,α ˆP2,α(1 −ˆP3,α) + (1 −ˆPα)(1 −ˆP1,β)(1 −ˆP2,β) ˆP3,β
(10)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4721549636803874,"fα,β,α = ˆPα ˆP1,α(1 −ˆP2,α) ˆP3,α + (1 −ˆPα)(1 −ˆP1,β) ˆP2,β(1 −ˆP3,β)
(11)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4745762711864407,"fβ,α,α = ˆPα(1 −ˆP1,α) ˆP2,α ˆP3,α + (1 −ˆPα) ˆP1,β(1 −ˆP2,β)(1 −ˆP3,β)
(12)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.47699757869249393,"fβ,β,α = ˆPα(1 −ˆP1,α)(1 −ˆP2,α) ˆP3,α + (1 −ˆPα) ˆP1,β ˆP2,β(1 −ˆP3,β)
(13)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4794188861985472,"fβ,α,β = ˆPα(1 −ˆP1,α) ˆP2,α (1 −ˆP3,α) + (1 −ˆPα) ˆP1,β(1 −ˆP2,β) ˆP3,β
(14)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.48184019370460046,"fα,β,β = ˆPα ˆP1,α(1 −ˆP2,α)(1 −ˆP3,α) + (1 −ˆPα)(1 −ˆP1,β) ˆP2,β ˆP3,β
(15)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.48426150121065376,"fβ,β,β = ˆPα(1 −ˆP1,α)(1 −ˆP2,α)(1 −ˆP3,α) + (1 −ˆPα) ˆP1,β ˆP2,β ˆP3,β
(16)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.48668280871670705,"These polynomial expressions of the data sketch form a generating set for a non-empty polynomial
184"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4891041162227603,"ideal, the evaluation ideal. The evaluation variety, the set of points that satisfy all the equations in the
185"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4915254237288136,"ideal is also non-empty and contains the true evaluation point.
186"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4939467312348668,"Sketch of the proof. The assumption that true labels exist for the stream items underlies the algebraic
187"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.4963680387409201,"work required for the proof. The correct label of each item can be encoded in indicator functions,
188"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.49878934624697335,"1s(ℓ), that are 1 if its argument is the correct label for item s, and zero otherwise. The existence of a
189"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5012106537530266,"true label for an item s is then equivalent to the equation,
190"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5036319612590799,"1s(α) + 1s(β) = 1.
(17)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5060532687651331,"Consider now the first term in Equation 8, #(ℓ1, ℓ2, ℓ3 | α), as it relates to, say, the decisions event
191"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5084745762711864,"(α, β, α). By using the predicted labels by the classifiers for a given items s, the following expression
192"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5108958837772397,"is exactly equal to one precisely at those decisions events but zero otherwise,
193"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.513317191283293,"1s(ℓ1,s) (1 −1s(ℓ2,s)) 1s(ℓ3,s).
(18)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5157384987893463,"The proof of having a complete representation using the basic evaluation variables then hinges in
194"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5181598062953995,"equating the average of this equation to the variables as follows,
195 1
nα X"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5205811138014528,"1s(α)=1
1s(ℓ1,s) (1 −1s(ℓ2,s)) 1s(ℓ3,s) = ˆPα ˆP1,α (1 −ˆP2,α) ˆP3,α.
(19)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5230024213075061,"This equality is only true for independent classifiers because we have substituted the average of
196"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5254237288135594,"products of the indicator functions by products of their averages. New correlation variables are
197"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5278450363196125,"Table 1: Algebraic evaluation formulas for the prevalence of α, ˆPα, for three classifiers making
independent errors on a test. The ∆i,j and fi,β variables are polynomial functions of the data sketch
frequency counters. Each fi,β is the frequency classifier ‘i’ voted for the β label. The deltas are equal
to fi,j,β −fi,β fj,β, where fi,j,β is the frequency classifiers ‘i’ and ‘j’ voted simultaneously for the β
label."
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5302663438256658,"Evaluator
ˆPα
Majority Voting
fα,α,α + fα,α,β + fα,β,α + fβ,α,α"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5326876513317191,"Fully inferential
1
2 −1"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5351089588377724,"2
(fβ,β,β−(f1,β f2,β f3,β+f1,β ∆2,3+f2,β ∆1,3+f3,β ∆1,2))
√"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5375302663438256,"4 ∆1,2 ∆1,3 ∆2,3+(fβ,β,β−(f1,β f2,β f3,β+f1,β ∆2,3+f2,β ∆1,3+f3,β ∆1,2))2"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5399515738498789,"introduced and then set to zero to define rigorously a sample definition of decision correlations. For
198"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5423728813559322,"example, the definition of the pair correlation variable on a label is given by,
199"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5447941888619855,"Γi,j,ℓ= 1 nℓ X"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5472154963680388,"1s(ℓ)=1
(1s(ℓi,s) −ˆPi,ℓ) (1s(ℓj,s) −ˆPj,ℓ) =  1 nℓ X"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.549636803874092,"1s(ℓ)=1
1s(ℓi,s) 1s(ℓj,s) "
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5520581113801453,"−ˆPi,ℓˆPj,ℓ."
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5544794188861986,"(20)
Setting these pair correlations to zero then guarantees that we can write averages of the product of the
200"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5569007263922519,"indicator functions for two classifiers as the product of their label accuracies. Similar considerations
201"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.559322033898305,"apply to the product of the indicators for three classifiers. The consequence is that any decision
202"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5617433414043583,"event frequency by independent classifiers is complete when written in terms of the basic evaluation
203"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5641646489104116,"statistics. All data sketches produced by independent classifiers are predicted by the basic statistics.
204"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5665859564164649,"Since the proof is constructive and starts from expressions for the true evaluation point, we know
205"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5690072639225182,"that there is at least one point that satisfies all these polynomial equations. We conclude that the
206"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5714285714285714,"evaluation variety for independent classifiers exists and it contains the true evaluation point.
207"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5738498789346247,"Theorem 2 details exactly what the evaluation variety for independent classifiers must be.
208"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.576271186440678,"Theorem 2. The polynomial generating set for independent classifiers has an evaluation variety that
209"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5786924939467313,"has two points, one of which is the true evaluation point.
210"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5811138014527845,"Sketch of the proof. The quartic polynomials of the independent generating set are not trivial to
211"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5835351089588378,"handle. A strategy for solving them is to obtain algebraic consequences of them that isolate the
212"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.585956416464891,"variables. Using the tools of AG, this can be accomplished for independent classifiers. Solving their
213"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5883777239709443,"polynomial system is accomplished by calculating another representation of the evaluation ideal,
214"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5907990314769975,"called the Gröebner basis, that does this. It can be arranged to isolate the ˆPαin a quadratic
215"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5932203389830508,"a(. . .) ˆP 2
α + b(. . .) ˆPα + c(. . .) = 0.
(21)"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5956416464891041,"The coefficients a, b, and c are polynomials of the decision frequencies. Since this is a quadratic,
216"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.5980629539951574,"it follows from the quadratic formula that it can only contain two solutions. This, coupled with
217"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6004842615012107,"the fact that other equations in the evaluation ideal are linear equations relating ˆPαto ˆPi,α or ˆPi,β
218"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6029055690072639,"variables leads one to conclude that only two points exist in the evaluation variety of independent
219"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6053268765133172,"classifiers.
220"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6077481840193705,"Table 1 compares the prevalence estimates of the fully inferential independent evaluator with the
221"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6101694915254238,"naive MV one. By construction, it will be exact when its assumptions apply. But unlike the naive
222"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.612590799031477,"MV evaluator, this formula can return obviously wrong estimates. The next section details how one
223"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6150121065375302,"can carry out experiments on unlabeled data to find evaluation ensembles that are going to be nearly
224"
FULLY INFERENTIAL EVALUATION OF SAMPLE INDEPENDENT BINARY CLASSIFIERS,0.6174334140435835,"independent.
225"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6198547215496368,"4
Experiments with the failure modes of the independent stream evaluator
226"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6222760290556901,"If perfect evaluations are not possible, one should prefer methods that alert us when they fail or their
227"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6246973365617433,"assumptions are incorrect. The experiments discussed here show how we can lever the self-alarming
228"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6271186440677966,"failures of the independent evaluator to reject highly correlated evaluation ensembles. There are four
229"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6295399515738499,"failure modes for the independent evaluator,
230"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6319612590799032,"(a) Percentage of seemingly correct feature parti-
tions by test size."
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6343825665859564,"(b) Percentage of feature partitions that never had
an independent model solution."
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6368038740920097,"Figure 2: Failure rates for candidate evaluation ensembles constructed from disjoint partitions of the
features."
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.639225181598063,"• The evaluation variety corresponding to the independent evaluation ideal is the empty set -
231"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6416464891041163,"no points in evaluation space can zero out the equations in the evaluation ideal.
232"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6440677966101694,"• The two evaluation points contain complex numbers.
233"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6464891041162227,"• The two evaluation points lie outside the real, unit cube.
234"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.648910411622276,"• The estimated values contain unresolved square roots.
235"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6513317191283293,"The fourth failure mode is interesting theoretically but not as practical. The Supplement details
236"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6537530266343826,"how an unresolved square root in the evaluation estimates can be used to prove that the classifiers
237"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6561743341404358,"were not error independent in the evaluation. Its theoretical interest lies in demonstrating that
238"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6585956416464891,"algebraic numbers, unlike real ones, can be used to self-alarm, in an almost perfect fashion, when its
239"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6610169491525424,"assumptions are violated.
240"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6634382566585957,"The first three failure modes are more practical. The first set of experiments will look at how the
241"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6658595641646489,"failures can be used to estimate what test sizes are less likely to have failed evaluations when we use
242"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6682808716707022,"the independent evaluator. The second set of experiments profiles how well another rejection criteria,
243"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6707021791767555,"this time based on the polynomial generating set for correlated classifiers, can help identify nearly
244"
EXPERIMENTS WITH THE FAILURE MODES OF THE INDEPENDENT STREAM EVALUATOR,0.6731234866828087,"independent evaluations.
245"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6755447941888619,"4.1
Rejecting highly correlated evaluation ensembles
246"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6779661016949152,"Given a set of unlabeled data and a smaller set of labeled training data, the goal of the first set
247"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6803874092009685,"of experiments is to construct and identify nearly independent evaluation ensembles on a larger,
248"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6828087167070218,"unlabeled portion of data. This simple experimental set-up is meant to mimic a possible Auto-ML
249"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6852300242130751,"application of stream evaluation. The experiments are meant to answer the question - how big should
250"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6876513317191283,"an evaluation test be? This is done by profiling the algebraic failures as a function of test size.
251"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6900726392251816,"Since the goal is to construct and then test if an evaluation ensemble is near enough independence to
252"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6924939467312349,"give seemingly correct estimates, a generic training protocol was applied to the three datasets studied.
253"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6949152542372882,"A training sample of 600 items was selected and the rest of the dataset was then held-out to carry out
254"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6973365617433414,"unlabeled evaluations. The rate of failures on the held-out data is then used as a guide to select test
255"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.6997578692493946,"sizes that have low failure rates as observed empirically.
256"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7021791767554479,"Independence in the candidate ensembles was maximized by training each member on features
257"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7046004842615012,"disjoint with those used by the others. Disjoint partitions of the small 600 training set, each of size
258"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7070217917675545,"200, were then used to train each candidate ensemble. A single profiling run selected 300 disjoint
259"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7094430992736077,"feature partitions to test as the size of the held out data was changed. Averaging successive profiling
260"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.711864406779661,"runs then gives an empirical measure of the failure rates as a function of test size. Each disjoint
261"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7142857142857143,"feature partition was trained and evaluated ten times.
262"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7167070217917676,"Figure 2 shows profiles of algebraic failures for the adult, mushroom, and twonorm dataset exper-
263"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7191283292978208,"iments. Figure 2a plots the percentage of feature partitions that resulted in evaluation ensembles
264"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7215496368038741,"returning seemingly correct estimates. The twonorm and adult experiments suggest that nearly
265"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7239709443099274,"independent ensembles will be easier for them than in mushroom. Figure 2b plots the percentage of
266"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7263922518159807,"(a) The adult dataset.
(b) The mushroom dataset."
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7288135593220338,"Figure 3: Maximum pair correlation in an evaluation ensemble versus the distance of the independent
evaluator estimate it produces from the containing variety."
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7312348668280871,"feature partitions that never produced data sketches explainable by the independent assumption. This
267"
REJECTING HIGHLY CORRELATED EVALUATION ENSEMBLES,0.7336561743341404,"plot also suggests that mushroom good evaluation ensembles will be harder to find.
268"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7360774818401937,"5
A containing variety for arbitrarily correlated binary classifiers
269"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.738498789346247,"The rejection of evaluation ensembles that have algebraic failures is no guarantee that the remaining
270"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7409200968523002,"ones will return somewhat accurate evaluation estimates. A different criteria has to be found to
271"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7433414043583535,"identify those that are close to independence. We can do this by considering the evaluation ideal of
272"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7457627118644068,"arbitrarily correlated classifiers.
273"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7481840193704601,"Theorem 3 in the Supplement shows how a complete polynomial representation for their data sketch is
274"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7506053268765133,"possible when we include variables for each of the correlation statistics. Its corresponding evaluation
275"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7530266343825666,"variety remains an open problem. But another variety that contains it can be defined. And most
276"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7554479418886199,"importantly, it can be constructed without knowledge of the correlation statistics. Theorem 4 in the
277"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7578692493946732,"Supplement details how the basic evaluation statistics can be disentangled from the correlation ones
278"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7602905569007264,"by finding a suitable Gröbner basis for the generating set. The polynomials of the disentangled set
279"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7627118644067796,"have the forms,
280"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7651331719128329,"ˆPα ( ˆPi,α −fi,α) = (1 −ˆPα) ( ˆPi,β −fi,β)
(22)"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7675544794188862,"( ˆPi,α −fi,α) ( ˆPj,β −fj,β) = ( ˆPi,β −fi,β) ( ˆPj,α −fj,α).
(23)"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7699757869249395,"Since this generating set is a subset of the complete generating set for correlated classifiers, it is
281"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7723970944309927,"guaranteed to contain their evaluation variety, which, in turn, must contain the true evaluation point.
282"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.774818401937046,"By self-consistency, if the evaluation ensemble was truly independent, it would be on this 4-
283"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7772397094430993,"dimensional surface. The second set of experiments looked at the hypothesis that data sketches
284"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7796610169491526,"from correlated classifiers would return independent evaluator estimates whose distance from the
285"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7820823244552058,"containing variety was related to their unknown amount of correlation. Figures 3a and 3b show
286"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.784503631961259,"the observed relation between the distance to the containing variety and an ensemble’s maximum
287"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7869249394673123,"absolute pair correlation for the adult and mushroom experiments. As in the first experiments, the
288"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7893462469733656,"training data was 200 per label items. But the evaluation was carried out on held-out data with 2000
289"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7917675544794189,"per label items. As was expected by the test size profiling runs, the mushroom seemingly correct
290"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7941888619854721,"evaluations were harder to find. The trend in these plots is suggestive but not conclusive.
291"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7966101694915254,"The Supplement contains some of the evaluations from the least-distance evaluation ensembles. The
292"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.7990314769975787,"evaluations on twonorm perform best, perhaps because that dataset is synthetic. But challenges
293"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.801452784503632,"remain when handling correlated ensembles. Perhaps these will be resolved with further work. This
294"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8038740920096852,"may be possible because we have a complete representation for correlated classifiers. Using that
295"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8062953995157385,"representation one can expand the independent evaluator estimates as Taylor series on the unknown
296"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8087167070217918,"correlations. The linear term in Γi,j,ℓhas the inverse,
297"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8111380145278451,"1/( ˆPk,ℓ−fk,ℓ).
(24)"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8135593220338984,"Consequently, independent evaluator estimates becomes worse the closer one is to the “blindspots”
298"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8159806295399515,"in evaluation space at fi,ℓ. A look at the Gröbner basis for correlated classifiers shows how the
299"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8184019370460048,"blindspots shunt off the correlation variables by eliminating them from the basis. An evaluator whose
300"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8208232445520581,"evaluation statistics lie at the blindspots is thus unable to capture correlation effects - its sketch is
301"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8232445520581114,"explainable by an independent ensemble hypothesis that is not correct. This extreme case happens
302"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8256658595641646,"at a finite number of points, so its occurrence would be correspondingly rare. But as the Taylor
303"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8280871670702179,"expansion shows, it can affect the quality of the independent evaluator estimate severely if the true
304"
A CONTAINING VARIETY FOR ARBITRARILY CORRELATED BINARY CLASSIFIERS,0.8305084745762712,"evaluation point lies near them.
305"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8329297820823245,"6
Advantages and disadvantages of algebraic stream evaluation
306"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8353510895883777,"The main advantage of algebraic evaluation is that it bypasses the representation and out-of-
307"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.837772397094431,"distribution problems in ML. Its focus is on estimating sample statistics with no concern for in-
308"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8401937046004843,"ferring models of the phenomena being classified or how the classifiers do it. There are no unknown
309"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8426150121065376,"unknowns in algebraic evaluation.
310"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8450363196125908,"But algebraic evaluation cannot resolve the principal/agent monitoring paradox, only ameliorate
311"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.847457627118644,"it. Its batch approach only estimates average performance on a test. This may not be sufficient to
312"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8498789346246973,"handle anomalies or identify important subsets of the test where the classifiers perform much worse.
313"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8523002421307506,"In addition, sample statistics are not enough to identify the causes of poor performance or predict
314"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8547215496368039,"performance in the future. These are important considerations in settings one bothers to monitor
315"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8571428571428571,"with evaluation ensembles. Algebraic evaluators should be used in conjunction with other evaluation
316"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8595641646489104,"methods, such as the ones discussed in Section 1.3, that do encode more information about the
317"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8619854721549637,"application context.
318"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.864406779661017,"Finally, all evaluation methods on unlabeled data are ambiguous. This is seen here by the two-point
319"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8668280871670703,"variety associated with independent classifiers. Additional assumptions about the evaluation must be
320"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8692493946731235,"made to ‘decode‘ the true evaluation point. For example, in contexts where the prevalences are not
321"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8716707021791767,"expected to vary greatly their known value can be used to select to correct point. Such is the case in
322"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.87409200968523,"the adult dataset where the rare label corresponds to tax record features for people earning more
323"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8765133171912833,"than 50K US dollars annually. Fewer higher income records is a reasonable assumption for future
324"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8789346246973365,"random samples of US tax records. Conversely, if one could have high assurances of the quality
325"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8813559322033898,"of the classifiers and then use them to select the one point that aligns with it. In that case, stream
326"
ADVANTAGES AND DISADVANTAGES OF ALGEBRAIC STREAM EVALUATION,0.8837772397094431,"evaluation is being used to monitor the environment and not the classifiers.
327"
BROADER IMPACTS,0.8861985472154964,"7
Broader Impacts
328"
BROADER IMPACTS,0.8886198547215496,"Evaluation on unlabeled data is a perennial problem in ML. As this conference and others discuss the
329"
BROADER IMPACTS,0.8910411622276029,"impact AI agents have on our safety and society, it becomes necessary to have safeguards that can
330"
BROADER IMPACTS,0.8934624697336562,"protect us from their decisions. The framework proposed here should have a positive impact across
331"
BROADER IMPACTS,0.8958837772397095,"multiple application areas for ML since it is based on generic considerations.
332"
REFERENCES,0.8983050847457628,"References
333"
REFERENCES,0.9007263922518159,"[1] P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
334"
REFERENCES,0.9031476997578692,"em algorithm. Applied Statistics, pages 20–28, 1979.
335"
REFERENCES,0.9055690072639225,"[2] Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin,
336"
REFERENCES,0.9079903147699758,"Luca Bogoni, and Linda Moy. Learning from crowds. Journal of Machine Learning Research,
337"
REFERENCES,0.910411622276029,"11(43):1297–1322, 2010.
338"
REFERENCES,0.9128329297820823,"[3] Fabian L Wauthier and Michael I. Jordan. Bayesian bias mitigation for crowdsourcing. In
339"
REFERENCES,0.9152542372881356,"J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances
340"
REFERENCES,0.9176755447941889,"in Neural Information Processing Systems 24, pages 1800–1808. Curran Associates, Inc., 2011.
341"
REFERENCES,0.9200968523002422,"[4] Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In
342"
REFERENCES,0.9225181598062954,"F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural
343"
REFERENCES,0.9249394673123487,"Information Processing Systems 25, pages 692–700. Curran Associates, Inc., 2012.
344"
REFERENCES,0.927360774818402,"[5] Dengyong Zhou, Sumit Basu, Yi Mao, and John C. Platt.
Learning from the wis-
345"
REFERENCES,0.9297820823244553,"dom of crowds by minimax entropy.
In F. Pereira, C. J. C. Burges, L. Bottou, and
346"
REFERENCES,0.9322033898305084,"K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages
347"
REFERENCES,0.9346246973365617,"2195–2203. Curran Associates, Inc., 2012.
URL http://papers.nips.cc/paper/
348"
REFERENCES,0.937046004842615,"4490-learning-from-the-wisdom-of-crowds-by-minimax-entropy.pdf.
349"
REFERENCES,0.9394673123486683,"[6] Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I Jordan. Spectral methods meet em: A
350"
REFERENCES,0.9418886198547215,"provably optimal algorithm for crowdsourcing. In Z. Ghahramani, M. Welling, C. Cortes, N. D.
351"
REFERENCES,0.9443099273607748,"Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
352"
REFERENCES,0.9467312348668281,"27, pages 1260–1268. Curran Associates, Inc., 2014.
353"
REFERENCES,0.9491525423728814,"[7] Fabio Parisi, Francesco Strino, Boaz Nadler, and Yuval Kluger. Ranking and combining multiple
354"
REFERENCES,0.9515738498789347,"predictors without labeled data. Proceedings of the National Academy of Sciences, 111(4):
355"
REFERENCES,0.9539951573849879,"1253–1258, 2014.
356"
REFERENCES,0.9564164648910412,"[8] Ariel Jaffe, Boaz Nadler, and Yuval Kluger. Estimating the accuracies of multiple classifiers
357"
REFERENCES,0.9588377723970944,"without labeled data. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the
358"
REFERENCES,0.9612590799031477,"Eighteenth International Conference on Artificial Intelligence and Statistics, Proceedings of
359"
REFERENCES,0.9636803874092009,"Machine Learning Research, pages 407–415, San Diego, California, USA, 2015. PMLR.
360"
REFERENCES,0.9661016949152542,"[9] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in
361"
REFERENCES,0.9685230024213075,"ml safety, 2021.
362"
REFERENCES,0.9709443099273608,"[10] Jacob Steinhardt and Percy S Liang. Unsupervised risk estimation using only conditional
363"
REFERENCES,0.9733656174334141,"independence structure. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,
364"
REFERENCES,0.9757869249394673,"Advances in Neural Information Processing Systems, volume 29, pages 3657–3665. Curran
365"
REFERENCES,0.9782082324455206,"Associates, Inc., 2016.
366"
REFERENCES,0.9806295399515739,"[11] Wikipedia.
Good–Turing
frequency
estimation
—
Wikipedia,
the
free
ency-
367"
REFERENCES,0.9830508474576272,"clopedia.
http://en.wikipedia.org/w/index.php?title=Good%E2%80%93Turing%
368"
REFERENCES,0.9854721549636803,"20frequency%20estimation&oldid=1141712803, 2023. [Online; accessed 16-May-2023].
369"
REFERENCES,0.9878934624697336,"[12] D. Cox, J. Little, and D. O’Shea. Ideals, Varieties, and Algorithms: An Introduction to
370"
REFERENCES,0.9903147699757869,"Computational Algebraic Geometry and Commutative Algebra. Springer-Verlag, 4th edition,
371"
REFERENCES,0.9927360774818402,"2015.
372"
REFERENCES,0.9951573849878934,"[13] G. Pistone, E Riccomagno, and H. P. Wynn. Algebraic Statistics: Computational Commutative
373"
REFERENCES,0.9975786924939467,"Algebra in Statistics. Chapman and Hall, 2001.
374"
