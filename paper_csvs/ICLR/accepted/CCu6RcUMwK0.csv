Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0038910505836575876,"Graph neural networks achieve high accuracy in link prediction by jointly
leveraging graph topology and node attributes. Topology, however, is repre-
sented indirectly; state-of-the-art methods based on subgraph classiﬁcation
label nodes with distance to the target link, so that, although topological
information is present, it is tempered by pooling. This makes it challenging
to leverage features like loops and motifs associated with network formation
mechanisms. We propose a link prediction algorithm based on a new pool-
ing scheme called WalkPool. WalkPool combines the expressivity of
topological heuristics with the feature-learning ability of neural networks. It
summarizes a putative link by random walk probabilities of adjacent paths.
Instead of extracting transition probabilities from the original graph, it
computes the transition matrix of a “predictive” latent graph by applying
attention to learned features; this may be interpreted as feature-sensitive
topology ﬁngerprinting. WalkPool can leverage unsupervised node fea-
tures or be combined with GNNs and trained end-to-end. It outperforms
state-of-the-art methods on all common link prediction benchmarks, both
homophilic and heterophilic, with and without node attributes. Applying
WalkPool to a set of unsupervised GNNs signiﬁcantly improves predic-
tion accuracy, suggesting that it may be used as a general-purpose graph
pooling scheme."
INTRODUCTION,0.007782101167315175,"1
Introduction"
INTRODUCTION,0.011673151750972763,"Observed graph
Candidate links"
INTRODUCTION,0.01556420233463035,Subgraph
INTRODUCTION,0.019455252918287938,"Prefer triangles 
with large weights"
INTRODUCTION,0.023346303501945526,"Prefer quadrangles
with large weights"
INTRODUCTION,0.027237354085603113,Completed graph
INTRODUCTION,0.0311284046692607,"Figure 1:
The topological organizing
rules are not universal across graphs."
INTRODUCTION,0.03501945525291829,"Graphs are a natural model for relational data
such as coauthorship networks or the human pro-
tein interactome.
Since real-world graphs are
often only partially observed, a central prob-
lem across all scientiﬁc domains is to predict
missing links (Liben-Nowell & Kleinberg, 2007).
Link prediction ﬁnds applications in predicting
protein interactions (Qi et al., 2006), drug re-
sponses (Stanﬁeld et al., 2017), or completing
the knowledge graph (Nickel et al., 2015).
It
underpins recommender systems in social net-
works (Adamic & Adar, 2003) and online market-
places (L¨u et al., 2012). Successful link prediction
demands an understanding of the principles behind graph formation."
INTRODUCTION,0.038910505836575876,"In this paper we propose a link prediction algorithm which builds on two distinct traditions:
(1) complex networks, from which we borrow ideas about the importance of topology, and
(2) the emerging domain of graph neural networks (GNNs), on which we rely to learn op-
timal features. We are motivated by the fact that the existing GNN-based link prediction
algorithms encode topological1 features only indirectly, while link prediction is a strongly"
INTRODUCTION,0.042801556420233464,"∗These two authors have equal contribution.
†To whom correspondence should be addressed.
1Mathematical topology studies (global) properties of shapes that are preserved under homeo-
morphisms. Our use of “topology” to refer to local patterns is common in the network literature."
INTRODUCTION,0.04669260700389105,Published as a conference paper at ICLR 2022
INTRODUCTION,0.05058365758754864,"topological task (L¨u & Zhou, 2011). Traditional heuristics assume certain topological orga-
nizing rules, such as preference for triangles or quadrangles; see Figure 1 for an illustration.
But organizing rules are not universal across graphs and they need to be learned. The cen-
terpiece of our algorithm is a new random-walk-based pooling mechanism called WalkPool
which may be interpreted as a learnable version of topological heuristics. Our algorithm is
universal in the sense of Chien et al. (2020) in that it models both heterophilic (e.g., 2D-
grid in Figure 1) and homophilic (e.g., Triangle lattice in Figure 1) graphs unlike hardcoded
heuristics or standard GNNs which often work better on homophilic graphs. WalkPool
can learn the topological organizing rules in real graphs (for further discussion of lattices
from Figure 1 see also Section 4.4), and achieves state-of-the-art link prediction results on
all common benchmark datasets, sometimes by a signiﬁcant margin, even on datasets like
USAir (Batagelj & Mrvar, 2006) where the previous state of the art is as high as 97%."
RELATED WORK,0.054474708171206226,"1.1
Related work"
RELATED WORK,0.058365758754863814,"Early studies on link prediction use heuristics from social network analysis (L¨u & Zhou,
2011). The homophily mechanism for example (McPherson et al., 2001) asserts that “simi-
lar” nodes connect. Most heuristics are based on connectivity: the common-neighbor index
(CN) scores a pair of nodes by the number of shared neighbors, yielding predicted graphs
with many triangles; Adami´c–Adar index (AA) assumes that a highly-connected neighbor
contributes less to the score of a focal link (Adamic & Adar, 2003). CN and AA rely on
paths of length two. Others heuristics use longer paths, explicitly accounting for long-range
correlations (L¨u & Zhou, 2011). The Katz index (Katz, 1953) scores a pair of nodes by a
weighted sum of the number of walks of all possible lengths. PageRank scores a link by the
probability that a walker starting at one end of the link reaches the other in the stationary
state under a random walk model with restarts."
RELATED WORK,0.0622568093385214,"Heuristics make strong assumptions such as a particular parametric decay of path weights
with length, often tailored to either homophilic or heterophilic graphs.
Moreover, they
cannot be easily used on graphs with node or edge features. Instead of hard-coded structural
features, link prediction algorithms based on GNNs like the VGAE (Kipf & Welling, 2016b)
or SEAL Zhang & Chen (2018) use learned node-level representations that capture both the
structure and the features, yielding unprecedented accuracy."
RELATED WORK,0.06614785992217899,"Two strategies have proved successful for GNN-based link prediction. The ﬁrst is to devise
a score function which only depends on the two nodes that deﬁne a link.
The second
is to extract a subgraph around the focal link and solve link prediction by (sub)graph
classiﬁcation (Zhang & Chen, 2017).
In both cases the node representations are either
learned in an unsupervised way (Kipf & Welling, 2016b; Mavromatis & Karypis, 2020) or
computed by a GNN trained joinly with the link classiﬁer (Zhang & Chen, 2018; Zhang
et al., 2020)."
RELATED WORK,0.07003891050583658,"Algorithms based on two-body score functions achieved state-of-the-art performance when
they appeared, but real networks brim with many-body correlations. Graphs in biochem-
istry, neurobiology, ecology, and engineering (Milo et al., 2004) exhibit motifs—distinct
patterns occuring much more often than in random graphs. Motifs may correspond to ba-
sic computational elements and relate to function (Shen-Orr et al., 2002). A multilayer
GNN generates node representation by aggregating information from neighbors, capturing
to some extent many-body and long-range correlations. This dependence is however indirect
and complex topological properties such as frequency of motifs are smoothed out."
RELATED WORK,0.07392996108949416,"The current state-of-the-art link prediction algorithm, SEAL (Zhang & Chen, 2018), is
based on subgraph classiﬁcation and thus may account for higher-order correlations. Unlike
in vanilla graph classiﬁcation where a priori all links are equaly important, in graph classi-
ﬁcation for link prediction the focal link plays a special role and the relative placement of
other links with respect to it matters. SEAL takes this into account by labeling each node
of the subgraph by its distance to the focal link. This endows the GNN with ﬁrst-order
topological information and helps distinguish relevant nodes in the pooled representation.
Zhang and Chen show that node labeling is essential for SEAL’s exceptional accuracy. Nev-
ertheless, important structural motifs are represented indirectly (if at all) by such labeling
schemes, even less so after average pooling. We hypothesize that a major bottleneck in link
prediction comes from suboptimal pooling which fails to account for topology."
RELATED WORK,0.07782101167315175,Published as a conference paper at ICLR 2022
OUR CONTRIBUTION,0.08171206225680934,"1.2
Our contribution
Following Zhang & Chen (2018), we approach link prediction from via subgraph classiﬁca-
tion. Instead of encoding relative topological information through node labels, we design
a new pooling mechanism called WalkPool. WalkPool extracts higher-order structural
information by encoding node representations and graph topology into random-walk tran-
sition probabilities on some eﬀective latent graph, and then using those probabilities to
compute features we call walk proﬁles. WalkPool generalizes loop-counting ideas used
to build expressive graph models (Pan et al., 2016). Computing expressive topological de-
scriptors which are simultaneously sensitive to node features is the key diﬀerence between
WalkPool and SEAL. Using normalized probabilities tuned by graph attention mitigates
the well-known issue in graph learning that highly-connected nodes may bias predictions.
Transition probabilities and the derived walk proﬁles have simple topological interpretations."
OUR CONTRIBUTION,0.08560311284046693,"WalkPool can be applied to node representations generated by any unsupervised graph
representation models or combined with GNNs and trained in end-to-end. It achieves state-
of-the-art results on all common link prediction benchmarks. Our code and data are available
online at https://github.com/DaDaCheng/WalkPooling."
LINK PREDICTION ON GRAPHS,0.08949416342412451,"2
Link prediction on graphs"
LINK PREDICTION ON GRAPHS,0.0933852140077821,"We consider an observed graph with N nodes (or vertices), Go = (V, Eo), with V being the
node set and Eo the set of observed links (or edges). The observed link set Eo is a subset
Eo ⊂E∗of the set of all true links E∗. The target of link prediction is to infer missing links
from a candidate set Ec, which contains both true (in E∗) and false (not in E∗) missing links."
LINK PREDICTION ON GRAPHS,0.09727626459143969,"Problem 1. (Link prediction) Design an algorithm LearnLP that takes an observed graph
Go ⊂G and produces a link predictor LearnLP(Go) = Π
:
V × V →{True, False} which
accurately classiﬁes links in Ec."
LINK PREDICTION ON GRAPHS,0.10116731517509728,"Well-performing solutions to Problem 1 exploit structural and functional regularities in the
observed graph to make inferences about unobserved links."
LINK PREDICTION ON GRAPHS,0.10505836575875487,"Path counts and random walks
For simplicity we identify the N vertices with integers
1, . . . , N and represent G by its adjacency matrix, A = (aij)N
i,j=1 ∈{0, 1}N×N with aij = 1 if
{i, j} ∈Eo and aij = 0 otherwise. Nodes may have associated feature vectors (xi ∈RF , i ∈
{1, . . . , N}); we collect all feature vectors in the feature matrix X = [x1, · · · , xN]T ∈RN×F ."
LINK PREDICTION ON GRAPHS,0.10894941634241245,"Integer powers of the adjacency matrix reveal structural information: [Aτ]ij is the number
of paths of length τ connecting nodes i and j. WalkPool relies on random walk transition
matrices. For an adjacency matrix A the transition matrix is deﬁned as P = D−1A where
D = diag(d1, . . . , dN) and di = P"
LINK PREDICTION ON GRAPHS,0.11284046692607004,"j aij = |N(i)| is the degree of the node i. Thus the
probability pij = d−1
i aij that a random walker at node i transitions to node j is inversely
proportional to the number of neighbors of i. An extension to graphs with non-negative edge
weights W = (wij) is straightforward by replacing aij by wij. Powers of P are interpretable:
[Pτ]ij is the probability that a random walker starting at node i will reach node j in τ hops."
LINK PREDICTION ON GRAPHS,0.11673151750972763,"As we show in Section 3.2, transition probabilities in WalkPool are determined as coeﬃ-
cients of an attention mechanism applied to learned node features. The node features are
in turn extracted by a parameterized function fθ, which distills input attributes and to an
extent graph topology into an embedding (a vector) for each node. The feature extractor
fθ : {0, 1}N×N × RN×F →RN×F ′ takes as input the adjacency matrix (which encodes the
graph topology) and the input feature matrix, and outputs a distilled node feature matrix.
It should thus be equivariant to node ordering in A which is satisﬁed by GNNs."
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.12062256809338522,"3
WalkPool for link prediction by subgraph classification"
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.1245136186770428,"We now describe WalkPool which directly leverages higher-order topological correlations
without resorting to link labeling schemes and without making strong a priori assumptions.
WalkPool ﬁrst samples the k-hop subgraph Gk
{i,j} ⊂Go enclosing the target link; and
then computes random-walk proﬁles for sampled subgraphs with and without the target"
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.12840466926070038,Published as a conference paper at ICLR 2022 GNN
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.13229571984435798,"A
B
C
D"
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.13618677042801555,"Figure 2: Illustration of WalkPool. A: The input graph and the focal link e; B: enclosing
subgraphs with and without e; C: attention-processed features ≡random walk transition
probabilities; D: extracted walk proﬁle."
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.14007782101167315,"link.
Random walk proﬁles are then fed into a link classiﬁer.
Computing walk proﬁles
entails"
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.14396887159533073,"1. Feature extraction: Z = fθ(A, X), where fθ is a GNN;
2. Transition matrix computation: P = AttentionCoefficientsθ(Z; G);
3. Walk proﬁles: Extract entries from Pτ for 2 ≤τ ≤τc related to the focal link."
WALKPOOL FOR LINK PREDICTION BY SUBGRAPH CLASSIFICATION,0.14785992217898833,"We emphasize that we do not use attention to compute per-node linear combinations of
features like Veliˇckovi´c et al. (2017) (which is analogous to Vaswani et al. (2017)), but rather
interpret the attention coeﬃcients as random walk transition probabilities. The features Z
may be obtained either by an unsupervised GNN such as VGAE (Kipf & Welling, 2016b),
or they may be computed by a GNN which is trained jointly with WalkPool."
SAMPLING THE ENCLOSING SUBGRAPHS,0.1517509727626459,"3.1
Sampling the enclosing subgraphs 2 3
4 1"
SAMPLING THE ENCLOSING SUBGRAPHS,0.1556420233463035,"1
2
1
1
4
1
1
3
1 1
3
2"
SAMPLING THE ENCLOSING SUBGRAPHS,0.15953307392996108,"Figure 3: Illustration of walk proﬁles for τ = 2. We
assume pi,j = 1/di where di is the degree of node i."
SAMPLING THE ENCLOSING SUBGRAPHS,0.16342412451361868,"Following earlier work we make
the assumption that the presence
of a link only depends on its neigh-
bors within a (small) radius k.
It is known that simple heuris-
tics like AA may already perform
well on many graphs, with longer
walks not bringing about signiﬁ-
cant gains. Indeed, SEAL (Zhang
& Chen, 2018) exploits the fact
that an order k heuristic can be
accurately calculated from a k-hop
subgraph; optimal k is related to
the underlying generative model.
Keeping k small (as small as 2 in
this paper) is pragmatic for rea-
sons beyond diminishing returns: a large k greatly increases memory and compute demands.
The size of a 2-hop neighborhood is graph-dependent, but for the highly-connected E.coli
and PB datasets we already need to enforce a maximum number of nodes per hop to ﬁt the
subgraphs in memory."
SAMPLING THE ENCLOSING SUBGRAPHS,0.16731517509727625,"Let d(x, y) be the shortest-path distance between nodes x and y.
The k-hop enclosing
subgraph Gk
{i,j} for {i, j} is deﬁned as the subgraph induced from Go by the set of nodes"
SAMPLING THE ENCLOSING SUBGRAPHS,0.17120622568093385,"Vk
{i,j} = {x ∈V : d(x, i) ≤k or d(x, j) ≤k}."
SAMPLING THE ENCLOSING SUBGRAPHS,0.17509727626459143,"Then Gk
{i,j} = (Vk
{i,j}, Ek
{i,j}), where {x, y} ∈Ek
i,j when x, y ∈Vk
{i,j} and {x, y} ∈Eo. We
omit the dependence on k and write G{i,j} = (V{i,j}, E{i,j}) for simplicity."
SAMPLING THE ENCLOSING SUBGRAPHS,0.17898832684824903,"We ﬁx an arbitrary order of the nodes in V{i,j} and denote the corresponding adjacency
matrix by A{i,j}. Without loss of generality, we assume that under the chosen order of"
SAMPLING THE ENCLOSING SUBGRAPHS,0.1828793774319066,Published as a conference paper at ICLR 2022
SAMPLING THE ENCLOSING SUBGRAPHS,0.1867704280155642,"nodes, the nodes i and j are labeled as 1 and 2 so that the candidate link {i, j} is mapped
to {1, 2} in its enclosing subgraph. We denote the corresponding node feature matrix Z{i,j},
with values inherited from the full graph (the rows of Z{i,j} are a subset of rows of Z)."
SAMPLING THE ENCLOSING SUBGRAPHS,0.19066147859922178,"For the candidate set of links Ec, we construct a set of enclosing subgraphs Sc = {G{i,j} :
{i, j} ∈Ec}, thus transforming the link prediction problem into classifying these k-hop
enclosing subgraphs. For training, we sample a set of known true and false edges Et and
construct its corresponding enclosing subgraph set St = {Gi,j : (i, j) ∈Et}."
RANDOM-WALK PROFILE,0.19455252918287938,"3.2
Random-walk profile
The next step is to classify the sampled subgraphs from their adjacency relations A{i,j}
and node representations Z{i,j}. Inspired by the walk-based heuristics, we employ random
walks to infer higher-order topological information. Namely, for a subgraph (G = (V, E), Z)
(either in Sc or St) we encode the node representations Z into edge weights and use these
edge weights to compute transition probabilities of a random walk on the underlying graph.
Probabilities of walks of various lengths under this model yield a proﬁle of the focal link."
RANDOM-WALK PROFILE,0.19844357976653695,"We ﬁrst encode two-node correlations as eﬀective edge weights,"
RANDOM-WALK PROFILE,0.20233463035019456,"ωx,y = Qθ(zx)T Kθ(zy)
√"
RANDOM-WALK PROFILE,0.20622568093385213,"F ′′,
(1)"
RANDOM-WALK PROFILE,0.21011673151750973,"for all {x, y} ∈E, where Qθ : RF ′ →RF ′′ and Kθ : RF ′ →RF ′′ are two multilayer
perceptrons (MLPs) and F ′′ is the output dimension of the MLPs. In order to include higher-
order topological information, we compute the random-walk transition matrix P = (px,y)
from the two-body correlations. We set"
RANDOM-WALK PROFILE,0.2140077821011673,"px,y =

softmax
 
(ωx,z)z∈N(x)
"
RANDOM-WALK PROFILE,0.2178988326848249,"y := exp(ωx,y) / P"
RANDOM-WALK PROFILE,0.22178988326848248,"z∈N(x) exp(ωx,z)
(2)"
RANDOM-WALK PROFILE,0.22568093385214008,"for {x, y} ∈E and px,y = 0 otherwise, with N(x) the set of neighbors of x in the enclosing
subgraph. The encoding (2) is analogous to graph attention coeﬃcients (Veliˇckovi´c et al.,
2017; Shi et al., 2020); unlike graph attention, however, we directly use the coeﬃcients
instead of computing linear combinations; this framework also allows multi-head attention."
RANDOM-WALK PROFILE,0.22957198443579765,"Entries of the τ-th power [Pτ]ij are interpreted as probabilities that a random walker goes
from i to j in τ hops. These probabilities thus concentrate the topological and node at-
tributes relevant for the focal link into the form of random-walks:"
RANDOM-WALK PROFILE,0.23346303501945526,"• Topology is included indirectly through the GNN-extracted node features Z, and
directly by the fact that P encodes zero probabilities for non-neighbors and that its
powers thus encode probabilities of paths and loops;
• Input features are included directly through the GNN-extracted node features, and
reﬁned and combined with topology by the key and value functions Qθ, Kθ."
RANDOM-WALK PROFILE,0.23735408560311283,"As a result, WalkPool2 can be interpreted as trainable heuristics."
RANDOM-WALK PROFILE,0.24124513618677043,"From the matrix P and its powers, we now read a list of features to be used in graph
classiﬁcation. We compute node-level, link-level, and graph-level features:"
RANDOM-WALK PROFILE,0.245136186770428,"nodeτ = [Pτ]1,1 + [Pτ]2,2,
linkτ = [Pτ]1,2 + [Pτ]2,1,
graphτ = tr[Pτ].
(3)"
RANDOM-WALK PROFILE,0.2490272373540856,"Computation of all features is illustrated for τ = 2 in Figure 3. Node-level features nodeτ
describe the loop structure around the candidate link (recall that {1, 2} is the focal link in
the subgraph). The summation ensures that the feature is invariant to the ordering of i and
j, consistent with the fact that we study undirected graphs. Link-level features linkτ give
the symmetrical probability that a length-τ random walk goes from node 1 to 2. Finally,
graph-level features graphτ are related to the total probability of length-τ loops. All features
depend on the node representation Z; we omit this dependence for simplicity. The use of P
in WalkPool is diﬀerent from how graph matrices (e.g., A) are used in GNNs. In GNNs,
the powers Aτ serve as shift operators between neighborhoods that are multiplied by ﬁlter
weights and used to weigh node features; WalkPool encodes graph signals into eﬀective
edge weights and directly extracts topological information from the entries of Pτ."
RANDOM-WALK PROFILE,0.2529182879377432,"2The output of pooling (e.g., in a CNN) is a often an object of the same type (e.g., a down-
sampled image). The last layer of a CNN or GNN involves global average pooling which is a graph
summarization mechanism similarly as WalkPool, hence the name."
RANDOM-WALK PROFILE,0.25680933852140075,Published as a conference paper at ICLR 2022
PERTURBATION EXTRACTION,0.2607003891050584,"3.3
Perturbation extraction"
PERTURBATION EXTRACTION,0.26459143968871596,"A true link is by deﬁnition always present in its enclosing subgraph while a negative link
is always absent. This leads to overﬁtting if we directly compute the features (3) on the
enclosing subgraphs since the presence or absence of the link has a strong eﬀect on walk
proﬁles.
For a normalized comparison of true and false links, we adopt a perturbation
approach. Given an enclosing subgraph G = (V, E), we deﬁne its variants G+ = (V, E∪{1, 2})
(resp. G−= (V, E\{1, 2})) with the candidate link forced to be present (resp. absent).
We denote the node-level features (3) computed on G+ and G−by nodeτ,+ and nodeτ,−,
respectively, and analogously for the link- and graph-level features."
PERTURBATION EXTRACTION,0.26848249027237353,"While the node- and link-level features are similar to the heuristics of counting walks,
graphτ,+ and graphτ,−are not directly useful for link prediction as discussed in the intro-
duction: the information related to the presence of {i, j} is obfuscated by the summation
over the entire subgraph (by taking the trace)."
PERTURBATION EXTRACTION,0.2723735408560311,"SEAL attempts to remedy a similar issue for average (as opposed to tr(Pτ)) pooling by
labeling nodes by distance from the link. Here we propose a principled alternative. Since
transition probabilities are normalized and have clear topological meaning, we can suppress
irrelevant information by measuring the perturbation of graph-level features, ∆graphτ =
graphτ,+ −graphτ,−. This “background subtraction” induces a data-driven soft limit on
the longest loop length so that, by design, ∆graphτ concentrates around the focal link.
In Appendix E, we demonstrate locality of WalkPool. Compared to node labeling, the
perturbation approach does not manually assign a relative position (which may wrongly
suggest that nodes at the same distance from the candidate link are of equal importance)."
PERTURBATION EXTRACTION,0.27626459143968873,"In summary, with WalkPool, for all G ∈{G{i,j} : {i, j} ∈Ec}, we read a list of features as"
PERTURBATION EXTRACTION,0.2801556420233463,"WPθ(G, Z) =

ω1,2,
 
nodeτ,+, nodeτ,−, linkτ,+, linkτ,−, ∆graphττc
τ=2"
PERTURBATION EXTRACTION,0.2840466926070039,"
,
(4)"
PERTURBATION EXTRACTION,0.28793774319066145,"where τc is the cutoﬀof the walk length and we treat it as a hyperparameter. The features
are ﬁnally fed into a classiﬁer; we use an MLP Πθ with a sigmoid activation in the output.
The ablation study in Table 5 (Appendix B) shows that all the computed features contribute
relevant predictive information. Walk proﬁle computation is summarized in Figure 2."
TRAINING THE MODEL,0.2918287937743191,"3.4
Training the model"
TRAINING THE MODEL,0.29571984435797666,"The described model contains trainable parameters θ which are ﬁtted on the given observed
set Et of positive and negative training links and their enclosing subgraphs.
We train
WalkPool with MSE loss (see Appendix G for a discussion of the loss),"
TRAINING THE MODEL,0.29961089494163423,"θ∗= arg min
θ"
TRAINING THE MODEL,0.3035019455252918,"1
|Et| X"
TRAINING THE MODEL,0.30739299610894943,"{i,j}∈Et"
TRAINING THE MODEL,0.311284046692607," 
y{i,j} −Πθ
 
WPθ(G{i,j}, Z{i,j}
2"
TRAINING THE MODEL,0.3151750972762646,"where y{i,j} = 1 if {i, j} ∈Eo and 0 otherwise is the label indicating whether the link {i, j}
is true or false. The ﬁtted model is then deployed on the candidate links Ec; the predicted
label for {x, y} ∈Ec is simply by{x,y} = Πθ∗(WPθ∗(G{x,y}), Z{x,y})."
PERFORMANCE OF WALKPOOL ON BENCHMARK DATASETS,0.31906614785992216,"4
Performance of WalkPool on benchmark datasets"
PERFORMANCE OF WALKPOOL ON BENCHMARK DATASETS,0.3229571984435798,"We use area under the curve (AUC) (Bradley, 1997) and average precision (AP) as metrics.
Precision is the fraction of true positives among predictions. Letting TP (FP) be the number
of true (false) positive links, AP = TP/(TP + FP)."
DATASETS,0.32684824902723736,"4.1
Datasets"
DATASETS,0.33073929961089493,"In homophilic (heterophilic) graphs, nodes that are similar (dissimilar) are more likely to
connect.
For node classiﬁcation, a homophily index is deﬁned formally as the average
fraction of neighbors with identical labels (Pei et al., 2020). In the context of link prediction,
if we assume that network formation is driven by the similarity (or dissimilarity) of node
attributes, then a homophilic graph will favor triangles while a heterophilic graph will inhibit"
DATASETS,0.3346303501945525,Published as a conference paper at ICLR 2022
DATASETS,0.33852140077821014,"triangles. Following this intuition, we adopt the average clustering coeﬃcient ACC (Watts &
Strogatz, 1998) as a topological correlate of homophily."
DATASETS,0.3424124513618677,"We experiment with eight datasets without node attributes and seven with attributes. As
graphs without attributes we use: (i) USAir (Batagelj & Mrvar, 2006), (ii) NS (Newman,
2006), (iii) PB (Ackland et al., 2005), (iv) Yeast (Von Mering et al., 2002), (v) C.ele (Watts
& Strogatz, 1998), (vi) Power (Watts & Strogatz, 1998), (vii) Router (Spring et al., 2002),
and (viii) E.coli (Zhang et al., 2018). Properties and statistics of the datasets, including
number of nodes and edges, edge density and ACC can be found in Table 4 of Appendix A."
DATASETS,0.3463035019455253,"In graphs with a very low average clustering coeﬃcient like Power and Router (ACC =
0.080 and 0.012, respectively, see Appendix A), topology-based heuristics usually perform
poorly (L¨u & Zhou, 2011) (cf. Table 2), since heuristics often adopt the homophily assump-
tion and rely on triangles. We show that by learning the topological organizing patterns,
WalkPool performs well even for these graphs."
DATASETS,0.35019455252918286,"For a fair comparison, we use the exact same training and testing sets (including positive
and negative links) as SEAL in Zhang & Chen (2018). 90% of edges are taken as positive
training edges and the remaining 10% are the positive test edges. The same number of
additionally sampled nonexistent links are taken as training and testing negative edges."
DATASETS,0.3540856031128405,"As graphs with node attributes, we use: (i) Cora (McCallum et al., 2000), (ii) Cite-
seer (Giles et al., 1998), (iii) Pubmed (Namata et al., 2012), (iv) Chameleon (Rozember-
czki et al., 2021), (v) Cornell (Craven et al., 1998), (vi) Texas (Craven et al., 1998), and
(vii) Wisconsin (Craven et al., 1998). The properties and statistics of the datasets can be
found in Table 4 of Appendix A; further details are provided in Appendix A"
DATASETS,0.35797665369649806,"Following the experimental protocols in (Kipf & Welling, 2016b; Pan et al., 2018; Mavromatis
& Karypis, 2020), we split the links in three parts: 10% testing, 5% validation, 85% training.
We sample the same number of nonexisting links in each group as negative links."
BASELINES,0.36186770428015563,"4.2
Baselines WP SEAL"
BASELINES,0.3657587548638132,"Router
0.012 85.0 87.5 90.0 92.5 95.0 97.5 100.0"
BASELINES,0.36964980544747084,"Power
0.080
C.ele
0.292"
BASELINES,0.3735408560311284,"Yeast
0.306 PB"
BASELINES,0.377431906614786,E.coli USAir NS 0.320 0.516 0.625 0.638
BASELINES,0.38132295719844356,"Figure 4: Comparison of mean and variance
of AUC between SEAL and WP with 90% ob-
served links. The datasets are sorted by their
clustering coeﬃcients."
BASELINES,0.3852140077821012,"On benchmarks without node attributes,
we compare WalkPool with eight other
methods.
We
consider
three
walk-
based heuristics: AA, Katz and PR; two
subgraph-based heuristic learning meth-
ods:
Weisfeiler–Lehman
graph
kernel
(WLK) (Shervashidze et al., 2011) and
WLNM (Zhang & Chen, 2017); and latent
feature methods: node2vec (N2V) (Grover
&
Leskovec,
2016),
spectral
clustering
(SPC) (Tang & Liu, 2011), matrix fac-
torization (MF) (Koren et al., 2009) and
LINE (Tang et al., 2015). We additionally
consider the GNN-based SEAL, which is the
previous state-of-the-art."
BASELINES,0.38910505836575876,"For datasets with node attributes, we com-
bine WalkPool with three unsupervised GNN-based models: the VGAE, adversarially
regularized variational graph autoencoder (ARGVA) (Pan et al., 2018) and Graph Info-
Clust (GIC) (Mavromatis & Karypis, 2020)."
IMPLEMENTATION DETAILS,0.39299610894941633,"4.3
Implementation details"
IMPLEMENTATION DETAILS,0.3968871595330739,"In the absence of node attributes, we initialize the node representation Z(0) as an N × F (0)
all-ones matrix, with F (0) = 32. We generate node representations by a two-layered Graph
Convolutional Network (GCN) (Kipf & Welling, 2016a): Z(k) = GCN
 
Z(k−1)
, k ∈{1, 2},
where Z(0) = X. We then concatenate the outputs as Z = [Z(0) | Z(1) | Z(2)] to obtain
the ﬁnal node representation used to compute WP(G, Z) and classify. While a node labeling
scheme like the one in SEAL is not needed for the strong performance of WalkPool, we"
IMPLEMENTATION DETAILS,0.40077821011673154,Published as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.4046692607003891,"WP
SEAL
AA
CN
VGAE"
IMPLEMENTATION DETAILS,0.4085603112840467,"Triangle lattice (50 × 50)
99.77±0.12
99.54±0.11
95.16±0.90
95.15±0.87
37.14±1.47
2D-grid (50 × 50)
99.86±0.09
99.51±0.09
49.90±0.09
49.90±0.09
32.43±1.12
Hypercube (210)
100.00±0.00
99.60±0.10
48.01±0.35
48.01±0.35
37.84±1.19
Star (1000)
100.00±0.00
99.57±0.10
14.50±2.40
14.50±2.40
100.00±0.00"
IMPLEMENTATION DETAILS,0.41245136186770426,Table 1: AUC for synthetic graphs over 10 independent trials.
IMPLEMENTATION DETAILS,0.4163424124513619,"ﬁnd that labeling nodes by distance to focal link slightly improves results; we thus evaluate
WalkPool with and without distance labels."
IMPLEMENTATION DETAILS,0.42023346303501946,"For the three datasets with node attributes, we ﬁrst adopt an unsupervised model to generate
an initial node representation Z(0). This is because standard 2-layer GCNs are not expressive
enough to extract useful node features in the presence of node attributes Zhang et al. (2019)."
IMPLEMENTATION DETAILS,0.42412451361867703,"The initial node representation is fed into the same two-layered GCN above, and we take
the concatenated representation Z = [Z(0) | Z(1) | Z(2)] as the input to WalkPool. The
concatenation records local multi-hop information and facilitates training by providing skip
connections. It gives a small improvement over only using the last-layer embeddings, i.e.,
Z = Z(2). The hyperparameters of the model are explained in detail in Appendix C. The
runtime of WalkPool can be found in Appendix H."
RESULTS,0.4280155642023346,"4.4
Results"
RESULTS,0.43190661478599224,"Synthetic datasets
To show that WalkPool successfully learns topology, we use four
synthetic datasets: Triangle lattice (ACC = 0.415), 2D-Grid (ACC = 0), Hypercube
(ACC = 0), and Star (ACC = 0). We randomly remove 10% links from each graph and run
link prediction algorithms. AUC for WalkPool, SEAL, AA, and CN is shown in Table 1.
In these regular graphs the topological organizing rules are known explicitly. It is therefore
clear that a common-neighbor-based heuristic (such as CN or AA) should fail for 2D-grid
since none of the connected nodes have common neighbors. WalkPool successfully learns
the organizing patterns without prior knowledge about the graph formation rules, using the
same hyperparameters as in all other experiments. For Triangle lattice and 2D-grid,
WalkPool achieves a near-100% AUC. Small errors are due to hidden the test edges which
act as noise; for fewer withheld links the error would vanish. In hypercube and Star,
WalkPool achieves an AUC of 100% in all ten trials. The star graph is heterophilic in the
sense that no triangles are present; we indeed observe that AA and CN have AUC below
50% since they (on average) ﬂip true and false edges."
RESULTS,0.4357976653696498,"Datasets without attributes
We perform 10 random splits of the data. The average
AUCs with standard deviations are shown in Table 2; the AP results and statistical signiﬁ-
cance of the results can be found in Appendix D. For WalkPool, we have considered both
the cases with and without the DL node labels as input features."
RESULTS,0.4396887159533074,"From Table 2, WalkPool signiﬁcantly improves the prediction accuracy on all the datasets
we have considered. It also has a smaller standard deviation, indicating that WalkPool
is stable on independent data splits (unlike competing learning methods). Among all meth-
ods, WalkPool with DL performs the best; the second-best method slightly below is
WalkPool without DL. In other words, although DRNL slightly improves WalkPool,
nonetheless, WalkPool achieves SOTA performance already without it."
RESULTS,0.44357976653696496,"WalkPool performs stably on both homophilic (high ACC) and heterophilic (low ACC)
datasets, achieving state-of-the-art performance on all. Remarkably, on Power and Router
where topology-based heuristics such as AA and Katz fail, WalkPool shows strong perfor-
mance (it also outperforms SEAL by about 5% on Power). This conﬁrms that walk proﬁles
are expressive descriptors of the local network organizing patterns, and that WalkPool can
ﬁt their parameters from data without making prior topological assumptions. Experiments
with 50% observed training edges can be found in Appendix F."
RESULTS,0.4474708171206226,"Datasets with node attributes
We apply WalkPool to extract higher-order informa-
tion from node representations generated via unsupervised learning. We again perform 10
random splits of the data and report the average AUC with standard deviations in Table 3;
for AP see Appendix D. In Table 3, we show the results of unsupervised models with and"
RESULTS,0.45136186770428016,Published as a conference paper at ICLR 2022
RESULTS,0.45525291828793774,"Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli"
RESULTS,0.4591439688715953,"AA
95.06±1.03
94.45±0.93
92.36±0.34
89.43±0.62
86.95±1.40
58.79±0.88
56.43±0.51
95.36±0.34
Katz
92.88±1.42
94.85±1.10
92.92±0.35
92.24±0.61
86.34±1.89
65.39±1.59
38.62±1.35
93.50±0.44
PR
94.67±1.08
94.89±1.08
93.54±0.41
92.76±0.55
90.32±1.49
66.00±1.59
38.76±1.39
95.57±0.44
WLK
96.63±0.73
98.57±0.51
93.83±0.59
95.86±0.54
89.72±1.67
82.41±3.43
87.42±2.08
96.94±0.29
WLNM
95.95±1.10
98.61±0.49
93.49±0.47
95.62±0.52
86.18±1.72
84.76±0.98
94.41±0.88
97.21±0.27
N2V
91.44±1.78
91.52±1.28
85.79±0.78
93.67±0.46
84.11±1.27
76.22±0.92
65.46±0.86
90.82±1.49
SPC
74.22±3.11
89.94±2.39
83.96±0.86
93.25±0.40
51.90±2.57
91.78±0.61
68.79±2.42
94.92±0.32
MF
94.08±0.80
74.55±4.34
94.30±0.53
90.28±0.69
85.90±1.74
50.63±1.10
78.03±1.63
93.76±0.56
LINE
81.47±10.71
80.63±1.90
76.95±2.76
87.45±3.33
69.21±3.14
55.63±1.47
67.15±2.10
82.38±2.19
SEAL
97.09±0.70
98.85±0.47
95.01±0.34
97.91±0.52
90.30±1.35
87.61±1.57
96.38±1.45
97.64±0.22"
RESULTS,0.46303501945525294,"WP(ones)
98.52±0.50
98.86±0.42
95.42±0.39
98.16±0.33
92.42±1.22
91.71±0.60
97.18±0.28
98.54±0.20
WP(DL)
98.68±0.48 98.95±0.41 95.60±0.37 98.37±0.25 92.79±1.09 92.56±0.60 97.27±0.28 98.58±0.19"
RESULTS,0.4669260700389105,"Table 2: Prediction accuracy measured by AUC on eight datasets (90% observed links)
without node attributes. Boldface marks the best, underline the second best results."
RESULTS,0.4708171206225681,"VGAE
ARGVA
GIC"
RESULTS,0.47470817120622566,"NO WP
WP
NO WP
WP
NO WP
WP"
RESULTS,0.4785992217898833,"Cora
91.98±0.54
94.64±0.55
92.45±1.11
94.71±0.85
93.68±0.59
95.90±0.50
Citeseer
91.21±1.14
94.32±0.90
91.71±1.38
94.53±1.77
95.03±0.65
95.94±0.53
Pubmed
96.51±0.14
98.49±0.13
96.62±0.12
98.52±0.14
93.00±0.36
98.72±0.10
Chameleon
98.79±0.18
99.51±0.08
98.23±0.24
99.51±0.08
94.13±0.38
99.52±0.08
Cornell
70.59±9.03
78.24±7.51
81.73±4.82
82.39±8.92
63.32±7.47
80.69±7.25
Texas
73.71±9.29
76.02±7.05
68.05±8.29
75.62±5.80
65.43±10.39
74.49±6.85
Wisconsin
75.05±6.88
77.07±6.11
75.69±7.91
79.34±6.32
74.74±6.28
82.27±6.27"
RESULTS,0.48249027237354086,Table 3: Prediction accuracy (AUC) on datasets with node attributes (90% observed links).
RESULTS,0.48638132295719844,"without WalkPool. For all the unsupervised models and datasets, WalkPool improves
the prediction accuracy. On the Pubmed dataset where the relative importance of topology
over features is greater, WalkPool brings about the most signiﬁcant gains."
RESULTS,0.490272373540856,"While traditional heuristics cannot include node attributes, WalkPool encodes the node
attributes into random walk transition probabilities via the attention mechanism, which
allows it to capture structure and node attributes simultaneously. The prediction accuracy
relies on the unsupervised GNN which generates the inital node representation. We ﬁnd that
the combination of GIC and WalkPool yields the highest accuracy for all three datasets.
Importance of initial node representations is plain since WalkPool is not designed to un-
cover structure in node attributes. Nonetheless, WalkPool greatly improves performance
of unsupervised GNN on the downstream link prediction task."
DISCUSSION,0.49416342412451364,"5
Discussion"
DISCUSSION,0.4980544747081712,"The topology of a graph plays a much more important role in link prediction than node
classiﬁcation, even with node attributes. Link prediction and topology are entangled, so
to speak, since topology is deﬁned by the links. Most GNN-based link prediction methods
work with node representations and do not adequately leverage topological information."
DISCUSSION,0.5019455252918288,"Our proposed WalkPool, to the contrary, jointly encodes node representations and graph
topology into learned topological features. The central idea is how to leverage learning:
we apply attention to the node representations and interpret the attention coeﬃcients as
transition probabilities of a graph random walk.
WalkPool is a trainable topological
heuristic, thus explicitly considering long-range correlations, but without making ad hoc
assumptions like the classical heuristics. This intuition is borne out in practice: combining
supervised or unsupervised graph neural networks with WalkPool yields state-of-the-
art performance on a broad range of benchmarks with diverse structural characteristics.
Remarkably, WalkPool achieves this with the same set of hyperparameters on all tasks
regardless of the network type and generating mechanism."
DISCUSSION,0.5058365758754864,Acknowledgments
DISCUSSION,0.5097276264591439,"LP would like to acknowledge support from National Natural Science Foundation of China
under Grand No. 62006122. CS and ID were supported by the European Research Council
(ERC) Starting Grant 852821—SWING."
DISCUSSION,0.5136186770428015,Published as a conference paper at ICLR 2022
REFERENCES,0.5175097276264592,References
REFERENCES,0.5214007782101168,"Robert Ackland et al. Mapping the us political blogosphere: Are conservative bloggers more
prominent? In BlogTalk Downunder 2005 Conference, Sydney, 2005."
REFERENCES,0.5252918287937743,"Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks, 25
(3):211–230, 2003."
REFERENCES,0.5291828793774319,"Vladimir Batagelj and Andrej Mrvar. Pajek datasets website. http://vlado.fmf.uni-lj.
si/pub/networks/data/, 2006."
REFERENCES,0.5330739299610895,"Andrew P Bradley. The use of the area under the roc curve in the evaluation of machine
learning algorithms. Pattern recognition, 30(7):1145–1159, 1997."
REFERENCES,0.5369649805447471,"Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized
pagerank graph neural network. arXiv preprint arXiv:2006.07988, 2020."
REFERENCES,0.5408560311284046,"Mark Craven, Andrew McCallum, Dan PiPasquo, Tom Mitchell, and Dayne Freitag. Learn-
ing to extract symbolic knowledge from the world wide web. Technical report, Carnegie-
mellon univ pittsburgh pa school of computer Science, 1998."
REFERENCES,0.5447470817120622,"C Lee Giles, Kurt D Bollacker, and Steve Lawrence.
Citeseer: An automatic citation
indexing system. In Proceedings of the third ACM conference on Digital libraries, pp.
89–98, 1998."
REFERENCES,0.5486381322957199,"Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
and Data mining, pp. 855–864, 2016."
REFERENCES,0.5525291828793775,"Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):
39–43, 1953."
REFERENCES,0.556420233463035,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016a."
REFERENCES,0.5603112840466926,"Thomas N Kipf and Max Welling.
Variational graph auto-encoders.
arXiv:1611.07308,
2016b."
REFERENCES,0.5642023346303502,"Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, 42(8):30–37, 2009."
REFERENCES,0.5680933852140078,"David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks.
Journal of the American society for information science and technology, 58(7):1019–1031,
2007."
REFERENCES,0.5719844357976653,"Linyuan L¨u and Tao Zhou. Link prediction in complex networks: A survey. Physica A:
statistical mechanics and its applications, 390(6):1150–1170, 2011."
REFERENCES,0.5758754863813229,"Linyuan L¨u, Mat´uˇs Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang, and Tao Zhou.
Recommender systems. Physics Reports, 519(1):1–49, 2012."
REFERENCES,0.5797665369649806,"Costas Mavromatis and George Karypis. Graph infoclust: Leveraging cluster-level node
information for unsupervised graph representation learning. arXiv:2009.06946, 2020."
REFERENCES,0.5836575875486382,"Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automat-
ing the construction of internet portals with machine learning. Information Retrieval, 3
(2):127–163, 2000."
REFERENCES,0.5875486381322957,"Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily
in social networks. Annual review of sociology, 27(1):415–444, 2001."
REFERENCES,0.5914396887159533,"Ron Milo, Shalev Itzkovitz, Nadav Kashtan, Reuven Levitt, Shai Shen-Orr, Inbal Ayzen-
shtat, Michal Sheﬀer, and Uri Alon. Superfamilies of evolved and designed networks.
Science, 303(5663):1538–1542, 2004."
REFERENCES,0.5953307392996109,Published as a conference paper at ICLR 2022
REFERENCES,0.5992217898832685,"Galileo Namata, Ben London, Lise Getoor, and Bert Huang. Query-driven active surveying
for collective classiﬁcation. In 10th International Workshop on Mining and Learning with
Graphs, volume 8, pp. 1, 2012."
REFERENCES,0.603112840466926,"Mark EJ Newman.
Finding community structure in networks using the eigenvectors of
matrices. Physical review E, 74(3):036104, 2006."
REFERENCES,0.6070038910505836,"Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of
relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33,
2015."
REFERENCES,0.6108949416342413,"Liming Pan, Tao Zhou, Linyuan L¨u, and Chin-Kun Hu. Predicting missing links and iden-
tifying spurious links via likelihood analysis. Scientiﬁc Reports, 6(1):1–10, 2016."
REFERENCES,0.6147859922178989,"Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adver-
sarially regularized graph autoencoder for graph embedding. arXiv:1802.04407, 2018."
REFERENCES,0.6186770428015564,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn:
Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020."
REFERENCES,0.622568093385214,"Yanjun Qi, Ziv Bar-Joseph, and Judith Klein-Seetharaman. Evaluation of diﬀerent biological
data and computational classiﬁcation methods for use in protein interaction prediction.
Proteins: Structure, Function, and Bioinformatics, 63(3):490–500, 2006."
REFERENCES,0.6264591439688716,"Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.
Journal of Complex Networks, 9(2):cnab014, 2021."
REFERENCES,0.6303501945525292,"Shai S Shen-Orr, Ron Milo, Shmoolik Mangan, and Uri Alon. Network motifs in the tran-
scriptional regulation network of escherichia coli. Nature genetics, 31(1):64–68, 2002."
REFERENCES,0.6342412451361867,"Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and
Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning
Research, 12(9), 2011."
REFERENCES,0.6381322957198443,"Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun.
Masked label prediction: Uniﬁed message passing model for semi-supervised classiﬁca-
tion. arXiv:2009.03509, 2020."
REFERENCES,0.642023346303502,"Neil Spring, Ratul Mahajan, and David Wetherall. Measuring isp topologies with rocketfuel.
ACM SIGCOMM Computer Communication Review, 32(4):133–145, 2002."
REFERENCES,0.6459143968871596,"Zachary Stanﬁeld, Mustafa Co¸skun, and Mehmet Koyut¨urk. Drug response prediction as a
link prediction problem. Scientiﬁc Reports, 7(1):1–13, 2017."
REFERENCES,0.6498054474708171,"Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In Proceedings of the 24th International Conference
on World Wide web, pp. 1067–1077, 2015."
REFERENCES,0.6536964980544747,"Lei Tang and Huan Liu. Leveraging social media networks for classiﬁcation. Data Mining
and Knowledge Discovery, 23(3):447–478, 2011."
REFERENCES,0.6575875486381323,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pp. 5998–6008, 2017."
REFERENCES,0.6614785992217899,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. arXiv:1710.10903, 2017."
REFERENCES,0.6653696498054474,"Christian Von Mering, Roland Krause, Berend Snel, Michael Cornell, Stephen G Oliver,
Stanley Fields, and Peer Bork. Comparative assessment of large-scale data sets of protein–
protein interactions. Nature, 417(6887):399–403, 2002."
REFERENCES,0.669260700389105,Published as a conference paper at ICLR 2022
REFERENCES,0.6731517509727627,"Duncan J Watts and Steven H Strogatz.
Collective dynamics of ‘small-world’networks.
nature, 393(6684):440–442, 1998."
REFERENCES,0.6770428015564203,"Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pp. 575–583, 2017."
REFERENCES,0.6809338521400778,"Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances
in Neural Information Processing Systems, 31:5165–5175, 2018."
REFERENCES,0.6848249027237354,"Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Pre-
dicting hyperlinks in adjacency space. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence, 2018."
REFERENCES,0.688715953307393,"Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Revisiting graph neural
networks for link prediction. arXiv:2010.16103, 2020."
REFERENCES,0.6926070038910506,"Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks:
a comprehensive review. Computational Social Networks, 6(1):1–23, 2019."
REFERENCES,0.6964980544747081,Published as a conference paper at ICLR 2022
REFERENCES,0.7003891050583657,Appendix A. Benchmark datasets description
REFERENCES,0.7042801556420234,"Brief description of the benchmark datasets is as follows. For the datasets without node
attributes, we have considered:"
REFERENCES,0.708171206225681,"• USAir (Batagelj & Mrvar, 2006): a graph of US airlines with 332 nodes and 2126
edges; ACC = 0.625.
• NS (Newman, 2006): the collaboration relation of network science researchers with
1589 nodes and 2742 edges; ACC = 0.638.
• PB (Ackland et al., 2005): a graph of hyperlinks between weblogs on US politics
with 1222 nodes and 16714 edges; ACC = 0.320.
• Yeast (Von Mering et al., 2002): a protein-protein interaction graph in yeast with
2375 nodes and 11693 edges; ACC = 0.306.
• C.ele (Watts & Strogatz, 1998): the biological neural network of C.elegans with
297 nodes and 2148 edges; ACC = 0.292.
• Power (Watts & Strogatz, 1998): the topology of the Western States Power Grid
of the United States with 4941 nodes and 6594 edges; ACC = 0.080.
• Router (Spring et al., 2002): the router-level Internet with 5022 nodes and 6258
edges; ACC = 0.012.
• E.coli (Zhang et al., 2018): the pairwise reaction relation of metabolites in E.coli
with 1805 nodes and 15660 edges; ACC = 0.516."
REFERENCES,0.7120622568093385,"As graphs with node attributes, we consider three citation graphs with publications described
by binary vectors indicating the absence/presence of the corresponding dictionary word:"
REFERENCES,0.7159533073929961,"• Cora (McCallum et al., 2000): a citation graph with 2708 scientiﬁc publications
and 5278 links. The dictionary consists of 1433 unique words; ACC = 0.241.
• Citeseer (Giles et al., 1998): the dataset consists of 3312 scientiﬁc publications
with 4552 links; the dictionary consists of 3703 unique words; ACC = 0.141.
• Pubmed (Namata et al., 2012): the dataset consists of 19717 scientiﬁc publications
with 44324 links. The dictionary consists of 500 unique words; ACC = 0.060."
REFERENCES,0.7198443579766537,"We also consider a Wikipedia page graph where nodes represent web pages and edges rep-
resent hyperlinks between them. Node features represent several informative nouns in the
pages:"
REFERENCES,0.7237354085603113,"• Chameleon (Rozemberczki et al., 2021): Wikipedia page-page graph under the
topic chameleon. The graph consists of 2277 nodes and 31371 edges where each
node has an attribute vector of dimension 2325; ACC = 0.377."
REFERENCES,0.7276264591439688,"Finally, we consider three webpage graphs which include web pages from computer science
departments of various universities, node features are the bag-of-words representation of
web pages."
REFERENCES,0.7315175097276264,"• Cornell (Craven et al., 1998): a webpage graph with 183 nodes and 277 edges, and
the node attribute has dimension 1703; ACC = 0.167.
• Texas (Craven et al., 1998): a webpage graph with 183 nodes and 279 edges, and
the node attribute has dimension 1703; ACC = 0.198.
• Wisconsin (Craven et al., 1998): a webpage graph with 251 nodes and 450 edges,
and the node attribute has dimension 1703; ACC = 0.208."
REFERENCES,0.7354085603112841,The benchmark dataset properties and statistics are summarized in Table 4.
REFERENCES,0.7392996108949417,Appendix B. Ablation study
REFERENCES,0.7431906614785992,"We conduct ablation studies of WalkPool by excluding or including only each of the node-,
link- and graph-level features in (4). We used short notations {nodeτ} ≡{nodeτ,+, nodeτ,−:"
REFERENCES,0.7470817120622568,Published as a conference paper at ICLR 2022
REFERENCES,0.7509727626459144,"Dataset
USAir
NS
PB
Yeast
C.ele
Power Router E.coli
Cora
Citeseer Pubmed Chameleon Cornell Texas Wisconsin"
REFERENCES,0.754863813229572,"Node
332
1589
1222
2375
297
4941
5022
1805
2708
3312
19717
2277
183
183
251
Edges
2126
2742
16714
11693
2148
6594
6258
15660
5278
4552
44324
31371
227
279
450
ACC
0.625
0.638
0.320
0.306
0.292
0.080
0.012
0.516
0.241
0.141
0.060
0.377
0.167
0.198
0.208
Density
3.86e-2 2.17e-3 2.24e-2 4.15e-3 4.87e-2 5.40e-4
4.96e-4
9.61e-3 1.44e-3
8.30e-4
2.28e-4
1.21e-2
1.35e-2
1.67e-2
1.42e-2
Attributed
No
No
No
No
No
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes"
REFERENCES,0.7587548638132295,Table 4: Benchmark dataset properties and statistics.
REFERENCES,0.7626459143968871,"τ ∈{2, · · · , τc}}, and similarly for {linkτ}, {∆graphτ}. The ablation results for the C.ele
dataset is shown in the In Table. 5."
REFERENCES,0.7665369649805448,"Feature feat
ω12
{nodeτ}
{linkτ}
{∆graphτ}
-"
REFERENCES,0.7704280155642024,"AUC (using only feat)
87.90
91.88
92.44
92.29
-
AUC (using WP\feat)
92.69
92.66
91.08
92.65
92.82"
REFERENCES,0.77431906614786,Table 5: Ablation study in C.ele
REFERENCES,0.7782101167315175,Appendix C. Hyperparamters
REFERENCES,0.7821011673151751,The hyperparamters to reproduce the results are summarized in Table 6.
REFERENCES,0.7859922178988327,"For the split of training, validation and testing edges, we have adopted diﬀerent setups for
datasets with and without node attributes to consist with previous studies. In particular, for
datasets without node attributes, 90% of edges are taken as training positive edges and the
remaining 10% are for testing positive edges. The corresponding same number of negative
edges are sampled randomly for training and testing.
Then among the training edges.
we randomly selected 5% for validation.
The observed graph from which the enclosing
subgraphs are sampled consists of all the training positive edges. For the datasets with
node attributes, all the edges are divided into 85% for training, 10% for testing and 5% for
validation. The observed graph is built only based on the training edges. Some studies of
link prediction choose to keep the observed graph connected when sampling the test edges,
while others do not adopt this option. For the experiment results shown in the paper, we
sample the test edges uniformly random without ensuring the observed graph is connected."
REFERENCES,0.7898832684824902,"For sampling the enclosing subgraphs, we set the number of hops as 2 except 3 for the
Power dataset. The 2-hop subgraphs sampled from E.coli and PB datasets and Pubmed
are relatively numerous in nodes. Thus we set a maximum number nodes per hop for these
two datasets to ﬁt into memory. In particular, for each hop during sampling, we randomly
select a maximum of 100 nodes if the sampled nodes exceeds."
REFERENCES,0.7937743190661478,"For WalkPool, We set τc = 7 for the cutoﬀof walk length in all experiments. When
τc becomes large, the transition probabilities converges to a constant, as the random walk
reaches stationary. Therefore, a larger value of τ introduces little further information. From
experiments, we ﬁnd that introducing a larger τc will not increase the accuracy notably."
REFERENCES,0.7976653696498055,"For the classifer, we use a MLP with layer sizes 72 −1440 −1440 −720 −72 −1. We use
Relu as the activation function for the hidden layers, and sigmoid function for the output
layer."
REFERENCES,0.8015564202334631,Appendix D. Results measured by AP and statistical significance of results
REFERENCES,0.8054474708171206,"The prediction accuracy measured by AP for datasets with and without node attributes are
shown in Table 8 and Table 9, respectively. From the tables, WalkPool still performs best
with the AP metric."
REFERENCES,0.8093385214007782,"To veriﬁed the statistical signiﬁcance of the results, we performed a two-sided hypothesis
test with the null hypothesis that two independent samples (corresponding to the results of
WalkPool and the second best algorithm) have identical average. We compute the corre-
sponding p-value on a per dataset basis for the eight datasets without node attributes. The
results are shown in Table 7. The second best algorithm is SEAL except in C.ele (where"
REFERENCES,0.8132295719844358,Published as a conference paper at ICLR 2022
REFERENCES,0.8171206225680934,"Name
With attributes
No attributes"
REFERENCES,0.8210116731517509,"optimizer
Adam
Adam
learning rate
5e-5
5e-5
weight decay
0
0
test ratio
0.1
0.1
validation ratio
0.05 of training edges
0.05 of all edges
batch size
32
32
epochs
50
50
hops of enclosing subgraph
(∗) 2
2
dimension of initial representation Z(0)
16
32
initial representation Z(0)
ones or DL
unsupervised models
hidden layers of GCN
32
32
output layers of GCN
32
32
hidden layers of attention MLP
32
32
output layers of attention MLP
32
32
walk length cutoﬀτc
7
7
heads
2
2"
REFERENCES,0.8249027237354085,"Table 6: Default hyperparameters for reproducing the reults.
(∗): 3-hop for the Power
dataset."
REFERENCES,0.8287937743190662,"Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli"
REFERENCES,0.8326848249027238,"Second Best 97.09±0.70 98.85±0.47 95.01±0.34 97.91±0.52 90.32±1.49 91.78±0.61 96.38±1.45 97.64±0.22
WP(DL)
98.68±0.48 98.95±0.41 95.60±0.37 98.37±0.25 92.79±1.09 92.56±0.60 97.27±0.28 98.58±0.19
p-value
2.18 · 10−5
6.18 · 10−1
1.60 · 10−3
2.56 · 10−2
6.00 · 10−4
9.90 · 10−3
8.68 · 10−2
7.80 · 10−9"
REFERENCES,0.8365758754863813,"Table 7: p-value by comparing WP and second best algorithm on eight datasets with no
attributes."
REFERENCES,0.8404669260700389,"Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli"
REFERENCES,0.8443579766536965,"AA
95.36±1.00
94.46±0.93
92.36±0.46
89.53±0.63
86.46±1.43
58.76±0.89
56.50±0.51
96.05±0.25
Katz
94.07±1.18
95.05±1.08
93.07±0.46
95.23±0.39
85.93±1.69
79.82±0.91
64.52±0.81
94.83±0.30
PR
95.08±1.16
95.11±1.04
92.97±0.77
95.47±0.43
89.56±1.57
80.56±0.91
64.91±0.85
96.41±0.33
WLK
96.82±0.84
98.79±0.40
93.34±0.89
96.82±0.35
88.96±2.06
83.02±3.19
86.59±2.23
97.25±0.42
WLNM
95.95±1.13
98.81±0.49
92.69±0.64
96.40±0.38
85.08±2.05
87.16±0.77
93.53±1.09
97.50±0.23
N2V
89.71±2.97
94.28±0.91
84.79±1.03
94.90±0.38
83.12±1.90
81.49±0.86
68.66±1.49
90.87±1.48
SPC
78.07±2.92
90.83±2.16
86.57±0.61
94.63±0.56
62.07±2.40
91.00±0.58
73.53±1.47
96.08±0.37
MF
94.36±0.79
78.41±3.85
93.56±0.71
92.01±0.47
83.63±2.09
53.50±1.22
82.59±1.38
95.59±0.31
LINE
79.70±11.76
85.17±1.65
78.82±2.71
90.55±2.39
67.51±2.72
56.66±1.43
71.92±1.53
86.45±1.82
SEAL
97.13±0.80
99.06±0.37
94.55±0.43
98.33±0.37
89.48±1.85
89.55±1.29
96.23±1.71
98.03±0.20"
REFERENCES,0.8482490272373541,"WP(ones)
98.43±0.66
99.04±0.28
95.00±0.46
98.52±0.28
91.14±1.80
92.45±0.72
97.08±0.42
98.74±0.17
WP(DL)
98.66±0.55 99.09±0.29 95.28±0.41 98.64±0.28 91.53±1.33 93.07±0.69 97.20±0.38 98.79±0.21"
REFERENCES,0.8521400778210116,"Table 8: Prediction accuracy measured by AP on eight datasets (90% observed links) with-
out node attributes. Boldface letters are used to mark the best results while underlined
letters indicate the second best results."
REFERENCES,0.8560311284046692,"the second best is PR) and Power (where the second-best is SPC). Recall that a p-value of
0.05 or less is customarily considered statistically signiﬁcant. We see that for all but the NS
and Router datasets the p-value is below 0.05. For most datasets it is orders of magnitude
below. The AUC on the NS dataset is already very close to 100, thus leaving little space for
improvement; for Router it is the large variance of SEAL that gives a p-value a bit above
0.05. Note that even for Router and NS the empirical mean of WalkPool is better. More
trials should easily break the statistical tie even in those cases, but we used the same 10
splits as SEAL for a fair comparisons."
REFERENCES,0.8599221789883269,Published as a conference paper at ICLR 2022
REFERENCES,0.8638132295719845,"VGAE
ARGVA
GIC"
REFERENCES,0.867704280155642,"NO WP
WP
NO WP
WP
NO WP
WP"
REFERENCES,0.8715953307392996,"Cora
92.65±0.59
95.11±0.53
93.11±1.08
95.23±0.84
93.45±0.48
95.97±0.57
Citeseer
92.28±0.92
94.89±0.89
92.69±0.85
95.04±1.46
95.11±0.65
96.04±0.63
Pubmed
96.60±0.13
98.46±0.14
96.52±0.20
98.49±0.14
92.32±0.37
98.65±0.15
Chameleon
98.79±0.22
99.50±0.15
98.19±0.28
99.48±0.15
93.34±0.50
99.46±0.15
Cornell
75.34±8.24
82.74±7.89
84.66±5.48
85.92±7.45
66.94±8.20
84.18±8.39
Texas
78.50±8.18
81.78±5.75
73.10±8.26
81.21±5.00
70.89±9.08
79.60±6.34
Wisconsin
79.56±6.32
82.50±4.96
78.72±6.24
82.25±8.10
78.27±4.78
84.45±4.60"
REFERENCES,0.8754863813229572,"Table 9: Prediction accuracy measured by AP on seven datasets (90% observed links) with
node attributes."
REFERENCES,0.8793774319066148,Appendix E. Locality of graph-level features
REFERENCES,0.8832684824902723,"Let G = (V, E) be an enclosing subgraph, and {1, 2} be the candidate link. We denote the
random walk transition matrices on G+ and G−as P and Q, respectively. For any node
x, y ∈V, let d(x, y) be the shortest path distance on the graph. We deﬁne the distance from
x to the candidate link to be ¯d(x) = min{d(x, 1), d(x, 2)}. We have the following result."
REFERENCES,0.8871595330739299,"Proposition 1. Let Vτ = {x ∈V, ¯d(x) > τ}, then [Pτ]x,y = [Qτ]x,y for all x, y ∈Vτ."
REFERENCES,0.8910505836575876,"Proof. We prove the result via induction. When τ = 1, after the node-wise normalization
from the softmax function in (2), the elements of P and Q are identical among nodes that
not neighbors of the candidate link, i.e., V1 = {x ∈V, ¯d(x) > 1}."
REFERENCES,0.8949416342412452,"Now suppose the claim holds for τ. Let I = {x ∈V, ¯d(x) ≤τ}, let ∂I be the set of nodes
that are adjacency to but not in I, and let Ic be the set of rest nodes in the graph. We can
arrange the order of the nodes that such that P and Pτ are of the following block form P ="
REFERENCES,0.8988326848249028,"PI,I
PI,∂I
0
P∂I,I
P∂I,∂I
P∂I,Ic
0
PIc,∂I
PIc,Ic !"
REFERENCES,0.9027237354085603,",
Pτ = "
REFERENCES,0.9066147859922179,"
Pτ
I,I
Pτ
I,∂I
Pτ
I,Ic
Pτ
∂I,I
Pτ
∂I,∂I
Pτ
∂I,Ic
Pτ
Ic,I
Pτ
Ic,∂I
Pτ
Ic,Ic,  
(5)"
REFERENCES,0.9105058365758755,"where PI,I is the block matrix conﬁned to I and other blocks are deﬁned similarly. By
deﬁnition, PI,Ic = 0 and PIc,I = 0. Multiplication of the block matrices gives"
REFERENCES,0.914396887159533,"Pτ+1
Ic,Ic = PIc,∂IPτ
∂I,Ic + PIc,IcPτ
Ic,Ic.
(6)"
REFERENCES,0.9182879377431906,"We can do a similar computation to obtain Qτ+1
Ic,Ic, with all P replaced by Q in the above
equation. By assumption, we have"
REFERENCES,0.9221789883268483,"PIc∂I = QIc∂I
Pτ
∂I,Ic = Qτ
∂I,Ic
PIc,Ic = QIc,Ic,
Pτ
Ic,Ic = Qτ
Ic,Ic,
(7)"
REFERENCES,0.9260700389105059,"as ∂I, Ic ∈Vτ, therefore we obtain Pτ+1
Ic,Ic = Qτ+1
Ic,Ic. As Ic = Vτ+1, the claim follows by
induction."
REFERENCES,0.9299610894941635,Appendix F. Results with 50% observation
REFERENCES,0.933852140077821,"We further test of the performance of WalkPool under the setup of sparse training set. In
particular, we keep only 50% positive links of the graphs for training and use the rest links
for testing. The same number of negative links are sampled randomly for the traning set and
test set. The results measure by AUC and AP are shown in Table 10 and 11, respectively."
REFERENCES,0.9377431906614786,Appendix G. MSE loss and BCE loss
REFERENCES,0.9416342412451362,"As link prediction is a classiﬁcation problem, we usually adopt a classiﬁcation loss such as
binary cross-entropy (BCE). We opted for MSE based on the following heuristic. In node
classiﬁcation, categories are usually clearly deﬁned. For example, in a citation graph, the
category of a paper is near-deﬁnite. Meanwhile, the topology of real graphs is often fuzzy and
evolving. In a co-authorship graph, some authors who have not published together today"
REFERENCES,0.9455252918287937,Published as a conference paper at ICLR 2022
REFERENCES,0.9494163424124513,"Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli"
REFERENCES,0.953307392996109,"AA
88.61±0.40
77.13±0.75
87.06±0.17
82.63±0.27
73.37±0.80
53.38±0.22
52.94±0.28
87.66±0.56
Katz
88.91±0.51
82.30±0.93
91.25±0.22
88.87±0.28
79.99±0.59
57.34±0.51
54.39±0.38
89.81±0.46
PR
90.57±0.62
82.32±0.94
92.23±0.21
89.35±0.29
84.95±0.58
57.34±0.52
54.44±0.38
92.96±0.43
WLK
91.93±0.71
87.27±1.71
92.54±0.33
91.15±0.35
83.29±0.89
63.44±1.29
71.25±4.37
92.38±0.46
WLNM
91.42±0.95
87.61±1.63
90.93±0.23
92.22±0.32
75.72±1.33
64.09±0.76
86.10±0.52
92.81±0.30
N2V
84.63±1.58
80.29±1.20
79.29±0.67
90.18±0.17
75.53±1.23
55.40±0.84
62.45±0.81
84.73±0.81
SPC
65.42±3.41
79.63±1.34
78.06±1.00
89.73±0.28
47.30±0.91
56.51±0.94
53.87±1.33
92.00±0.50
MF
91.28±0.71
62.95±1.03
93.27±0.16
84.99±0.49
78.49±1.73
50.53±0.60
77.49±0.64
91.75±0.33
LINE
72.51±12.19
65.96±1.60
75.53±1.78
79.44±7.90
59.46±7.08
53.44±1.83
62.43±3.10
74.50±11.10
SEAL
93.36±0.67
90.88±1.18
93.79±0.25
93.90±0.54
82.33±2.31
65.84±1.10
86.64±1.58
94.18±0.41"
REFERENCES,0.9571984435797666,"WP(ones)
95.16±0.70
90.68±1.04
94.50±0.20
94.89±0.22
87.83±0.83
67.03±0.77
88.09±0.52
95.37±0.22
WP(DL)
95.50±0.74 90.97±0.96 94.57±0.16 95.00±0.21
87.62±1.39
67.72±0.86 88.13±0.61
95.33±0.30"
REFERENCES,0.9610894941634242,"Table 10: Prediction accuracy measured by AUC on eight datasets (50% observed links)
without node attributes. Boldface letters are used to mark the best results while underlined
letters indicate the second best results."
REFERENCES,0.9649805447470817,"Data
USAir
NS
PB
Yeast
C.ele
Power
Router
E.coli"
REFERENCES,0.9688715953307393,"AA
89.39±0.39
77.14±0.74
87.24±0.18
82.68±0.27
73.40±0.77
53.37±0.23
52.94±0.27
89.01±0.49
Katz
91.29±0.36
82.69±0.88
91.54±0.16
92.22±0.21
79.94±0.79
57.63±0.52
60.87±0.26
91.93±0.35
PR
91.93±0.50
82.73±0.90
91.92±0.25
92.54±0.23
84.15±0.86
57.61±0.56
61.01±0.30
94.68±0.28
WLK
93.34±0.51
89.97±1.02
92.34±0.34
93.55±0.46
83.20±0.90
63.97±1.81
75.49±3.43
94.51±0.32
WLNM
92.54±0.81
90.10±1.11
91.01±0.20
93.93±0.20
76.12±1.08
66.43±0.85
86.12±0.68
94.47±0.21
N2V
82.51±2.08
86.01±0.87
77.21±0.97
92.45±0.23
72.91±1.74
60.83±0.68
66.77±0.57
85.41±0.94
SPC
70.18±2.16
81.16±1.26
81.30±0.84
92.07±0.27
55.31±0.93
59.10±1.06
59.13±3.22
94.14±0.29
MF
92.33±0.90
66.62±0.89
92.53±0.33
87.28±0.57
77.82±1.59
52.45±0.63
81.25±0.56
94.04±0.36
LINE
71.75±11.85
71.53±0.97
78.72±1.24
83.06±9.70
60.71±6.26
55.11±3.49
64.87±6.76
75.98±14.45
SEAL
94.15±0.54
92.21±0.97
93.42±0.19
95.32±0.38
81.99±2.18
65.28±1.25
87.79±1.71
95.67±0.24"
REFERENCES,0.9727626459143969,"WP(ones)
95.39±0.73
92.15±0.81
94.14±0.27
96.04±0.16
86.49±0.97
69.26±0.64
89.21±0.44
96.35±0.24
WP(DL)
95.87±0.74 92.33±0.76 94.22±0.27 96.15±0.13
86.25±1.42
69.79±0.71
89.17±0.55
96.36±0.34"
REFERENCES,0.9766536964980544,"Table 11: Prediction accuracy measured by AP on eight datasets (50% observed links)
without node attributes. Boldface letters are used to mark the best results while underlined
letters indicate the second best results."
REFERENCES,0.980544747081712,Published as a conference paper at ICLR 2022
REFERENCES,0.9844357976653697,"may have submitted a paper that will come out tomorrow. We therefore expect a less peaky
distribution of link probabilities than node classes, and we choose a loss that minimizes
the miscalibration of the model. In this context where we want to predict probabilities as
accurately as possible, the MSE loss is known as the Brier score (another textbook use of
the Brier score is to calibrate the chance-of-rain forecasts)."
REFERENCES,0.9883268482490273,"We have experimented with both the binary cross-entropy (BCE) loss and the MSE loss, and
we observed no discernible diﬀerence. For example, for the eight datasets without node at-
tributes (the last line of Table 2), accuracies measured by AUC when using MSE/BCE
loss are:
98.68/98.68; 98.95/98.85; 95.60/95.69; 98.37/98.37; 92.79/92.83; 92.56/92.58;
97.27/97.35; 98.58/98.67."
REFERENCES,0.9922178988326849,Appendix H. Runtime of WalkPool
REFERENCES,0.9961089494163424,"The runtime of WalkPool is similar to that of SEAL when using the same number of hops
to construct the subgraphs. On a classical dataset USAir, WalkPool takes 129.62s for
50 epochs with 1-hop subgraph sampling, while SEAL takes 145.94s. The conﬁguration
of WalkPool used throughout our paper has two GCN layers followed by several linear
layers for computing the powers of matrix P, and a four-layer MLP classiﬁer. As such, from
the perspective of runtime, the trainable part of WalkPool architecture is no more complex
than that of a typical GNN. For very large datasets, the most time-consuming step is to
extract the enclosing subgraphs—this is true for WalkPool and any other subgraph-based
link prediction algorithm."
