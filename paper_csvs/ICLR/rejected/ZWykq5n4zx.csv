Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001336898395721925,"High probability generalization bounds of uniformly stable learning algorithms
have recently been actively studied with a series of near-tight results established
by Feldman & Vondrak (2019); Bousquet et al. (2020). However, for randomized
algorithms with on-average uniform stability, such as stochastic gradient descent
(SGD) with time decaying learning rates, it still remains less well understood if
these deviation bounds still hold with high conﬁdence over the internal random-
ness of algorithm. This paper addresses this open question and makes progress to-
wards answering it inside a classic framework of conﬁdence-boosting. To this end,
we ﬁrst establish an in-expectation ﬁrst moment generalization error bound for
randomized learning algorithm with on-average uniform stability, based on which
we then show that a properly designed subbagging process leads to near-tight high
probability generalization bounds over the randomness of data and algorithm. We
further substantialize these generic results to SGD to derive improved high prob-
ability generalization bounds for convex or non-convex optimization with natural
time decaying learning rates, which have not been possible to prove with the ex-
isting uniform stability results. Specially for deterministic uniformly stable algo-
rithms, our conﬁdence-boosting results improve upon the best known generaliza-
tion bounds in terms of a logarithmic factor on sample size, which moves a step
forward towards resolving an open question raised by Bousquet et al. (2020)."
INTRODUCTION,0.00267379679144385,"1
INTRODUCTION"
INTRODUCTION,0.004010695187165776,"In many statistical machine learning problems, the ultimate goal is to design a suitable algorithm
A : ZN 7→W that maps a training data set S = {zi}i∈[N] ∈ZN to a model A(S) in a closed subset
W of an Euclidean space such that the following population risk function evaluated at the model is
as small as possible:
R(A(S)) := EZ∼D[ℓ(A(S); Z)],
where ℓ: W × Z 7→R+ is a non-negative bounded loss function whose value ℓ(w; z) measures
the loss evaluated at z with parameter w, and D represents a distribution over Z. It is generally the
case that the underlying data distribution is unknown, and in this case the training data is usually
assumed to be an i.i.d. set, i.e., S
i.i.d.
∼DN. Then, a natural alternative measurement that mimics the
computationally intractable population risk is the empirical risk deﬁned by"
INTRODUCTION,0.0053475935828877,"RS(A(S)) := EZ∼Unif(S)[ℓ(A(S); Z)] = 1 N N
X"
INTRODUCTION,0.0066844919786096255,"i=1
ℓ(A(S); zi)."
INTRODUCTION,0.008021390374331552,"The bound on the difference between population and empirical risks is of central interest in under-
standing the generalization performance of learning algorithm A. Particularly, we hope to derive
a suitable law of large numbers, i.e., a sample size vanishing rate bN such that the generalization
bound |RS(A(S)) −R(A(S))| ≲bN holds with high probability over the randomness of S and po-
tentially the randomness of A as well. Provided that A(S) is an almost minimizer of the empirical
risk function RS, say RS(A(S)) ≲minw∈W RS(w) + bN, the generalization bound immediately
implies an excess risk bound R(A(S)) −minw∈W R(w) ≲bN +
1
√"
INTRODUCTION,0.009358288770053475,"N due to the standard risk de-
composition and Hoeffding’s inequality. Therefore, generalization bounds also play a crucial role in
understanding the stochastic optimization performance of a learning algorithm."
INTRODUCTION,0.0106951871657754,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.012032085561497326,"The generalization bounds can be naturally implied by uniform bounds on supw∈W |R(w) −
RS(w)| (Bartlett et al., 2006; Bottou & Bousquet, 2008). While broadly applicable (e.g., to non-
convex problems) and leading to tight generalization in some speciﬁc regimes (e.g., margin-based
learning (Kakade et al., 2009)), uniform convergence bounds in general cases might suffer from the
polynomial dependence on dimensionality and thus are not suitable for high-dimensional models
which are ubiquitous in modern machine learning. Alternatively, a powerful proxy for analyzing the
generalization bounds is the stability of learning algorithms to changes in the training dataset. Since
the seminal work of Bousquet & Elisseeff (2002), stability has been extensively demonstrated to
beget dimension-independent generalization bounds for deterministic learning algorithms (Mukher-
jee et al., 2006; Shalev-Shwartz et al., 2010), as well as for randomized learning algorithms (such
as bagging and SGD) (Elisseeff et al., 2005; Hardt et al., 2016). So far, the best known results about
generalization bounds are offered by approaches based on the notion of uniform stability (Feldman
& Vondrak, 2018; 2019; Bousquet et al., 2020). Inspired by these recent breakthrough results, we
seek to derive sharper high-probability generalization bounds for randomized learning algorithms
with on-average uniform stability. A concrete working example of our study is the widely used
stochastic gradient descent (SGD) algorithm that carries out the following recursion for all t ≥1
with learning rate ηt > 0:
wt := ΠW (wt−1 −ηt∇wℓ(wt−1; zξt)) ,
(1)
where ξt ∈[N] is a uniform random index of data under with or without replacement sampling,
and ΠW is the Euclidean projection operator associated with W. Despite its popularity in the study
of stability theory (Hardt et al., 2016; Zhou et al., 2019; Lei & Ying, 2020), the high probability
generalization bounds of SGD are still relatively under explored through the lens of uniform stability."
PRIOR RESULTS,0.013368983957219251,"1.1
PRIOR RESULTS"
PRIOR RESULTS,0.014705882352941176,"Let us now brieﬂy review some state-of-the-art generalization bounds for uniformly stable algo-
rithms and their randomized variants. We denote by S .= S′ if a pair of data sets S and S′ differ
in a single data point. A randomized learning algorithm A is said to have on-average γN-uniform
stability if it satisﬁes the following uniform bound
sup
S .=S′,z∈Z
|EA[ℓ(A(S); z)] −EA[ℓ(A(S′); z)]| ≤γN."
PRIOR RESULTS,0.016042780748663103,"This deﬁnition is equivalent to the concept of uniform stability deﬁned over the on-average loss
EA[ℓ(A(S); z)]. Suppose that the loss function is Lipschitz and bounded in the interval (0, 1]. Then
essentially it has been shown in Feldman & Vondrak (2019) that for any δ ∈(0, 1), with probability
at least 1 −δ over S, the on-average generalization error is upper bounded by"
PRIOR RESULTS,0.017379679144385027,"|EA [R(A(S)) −RS(A(S))]| ≲γN log(N) log
N δ 
+ r"
PRIOR RESULTS,0.01871657754010695,log (1/δ)
PRIOR RESULTS,0.020053475935828877,"N
.
(2)"
PRIOR RESULTS,0.0213903743315508,"Recently, Bousquet et al. (2020) derived a slightly improved uniform stability bound that implies"
PRIOR RESULTS,0.022727272727272728,"|EA [R(A(S)) −RS(A(S))]| ≲γN log(N) log
1 δ 
+ r"
PRIOR RESULTS,0.02406417112299465,log (1/δ)
PRIOR RESULTS,0.02540106951871658,"N
.
(3)"
PRIOR RESULTS,0.026737967914438502,"These generalization bounds are near-tight (up to a logarithmic factor log(N)) in the sense of a lower
bound on sum of random functions provided in that paper. While sharp in the dependence on sample
size, one common limitation of the above uniform stability implied generalization bounds lies in
that these high-probability results only hold in expectation with respect to the internal randomness
of algorithm."
PRIOR RESULTS,0.02807486631016043,"Further suppose that A has γN-uniform stability with probability at least 1 −δ′ for some δ′ ∈(0, 1)
over the randomness of A, i.e., PA ("
PRIOR RESULTS,0.029411764705882353,"sup
S .=S′,z∈Z
|ℓ(A(S); z) −ℓ(A(S′); z)| ≤γN )"
PRIOR RESULTS,0.03074866310160428,"≥1 −δ′.
(4)"
PRIOR RESULTS,0.03208556149732621,"Suppose that the randomness of A is independent of the training set S. Then with probability at
least 1 −δ −δ′ over S and A, the bound of Bousquet et al. (2020) naturally implies"
PRIOR RESULTS,0.03342245989304813,"|R(A(S)) −RS(A(S))| ≲γN log(N) log
1 δ 
+ r"
PRIOR RESULTS,0.034759358288770054,log (1/δ)
PRIOR RESULTS,0.03609625668449198,"N
.
(5)"
PRIOR RESULTS,0.0374331550802139,Under review as a conference paper at ICLR 2022
PRIOR RESULTS,0.03877005347593583,Algorithm 1: Confidence-Boosting for Randomized Learning
PRIOR RESULTS,0.040106951871657755,"Input : A randomized learning algorithm A and a training data set S = {zi}i∈[N]
i.i.d.
∼DN.
Output: Ak∗(Sk∗).
Uniformly divide S into K disjoint subsets such that S = S"
PRIOR RESULTS,0.04144385026737968,k∈[K] Sk and |Sk| = N
PRIOR RESULTS,0.0427807486631016,"K , ∀k ∈[K].
for k = 1, 2, ..., K do"
PRIOR RESULTS,0.04411764705882353,"Estimate Ak(Sk) as an output of the randomized algorithm A over subset Sk.
end
Compute k∗= arg mink∈[K]
RS\Sk(Ak(Sk)) −RSk(Ak(Sk))
."
PRIOR RESULTS,0.045454545454545456,"This is by far the best known generalization bound of randomized stable algorithms that hold with
high probability jointly over data and algorithm. The result, however, relies heavily on the high-
probability uniform stability condition expressed in equation 4. For the SGD (see equation 1) with
ﬁxed learning rate ηt ≡η, it is possible to show that γN ≲η
√"
PRIOR RESULTS,0.04679144385026738,T + ηT
PRIOR RESULTS,0.0481283422459893,N and δ′ = N exp(−N
PRIOR RESULTS,0.04946524064171123,"2 )
in equation 4 (Bassily et al., 2020). For SGD with time decaying learning rate that has been widely
studied in theory Harvey et al. (2019); Rakhlin et al. (2012) and applied in practice for training
popular deep nets such as ResNet and DenseNet Bengio et al. (2017), it is not clear if the condition
in equation 4 is still valid for γN and δ′ of interest. On the other hand, it is possible to show (see
the proofs of Corollary 1 and 2) that SGD with time decaying learning rate has desirable on-average
uniform stability."
PRIOR RESULTS,0.05080213903743316,"More specially for randomized learning methods such as bagging (Breiman, 1996) and SGD equa-
tion 1, the randomness of algorithm can be precisely characterized by a vector of i.i.d. parameters
ξ = {ξ1, ..., ξT } which are independent on data S. In such cases, suppose that A(S; ξ) has uniform
stability with respect to ξ at any given S, i.e., supξ .=ξ′ |ℓ(A(S; ξ)) −ℓ(A(S; ξ′))| ≤ρT . Then the
high probability bound established in Elisseeff et al. (2005) shows that with probability at least 1−δ,"
PRIOR RESULTS,0.05213903743315508,"|R(A(S)) −RS(A(S))| ≲γN +
1 + NγN
√ N
+
√ TρT  s"
PRIOR RESULTS,0.053475935828877004,"log
1 δ"
PRIOR RESULTS,0.05481283422459893,"
.
(6)"
PRIOR RESULTS,0.05614973262032086,"Provided that γN ≲
1
N and ρT ≲
1
T , the above bound shows that the generalization bound scales
as O(
1
√"
PRIOR RESULTS,0.05748663101604278,"N +
1
√"
PRIOR RESULTS,0.058823529411764705,"T ) with high probability. However, the above bound will show no guarantee on
convergence if γN ≳
1
√"
PRIOR RESULTS,0.06016042780748663,"N and/or ρT ≳
1
√"
PRIOR RESULTS,0.06149732620320856,"T . For example, this is actually the case for SGD with"
PRIOR RESULTS,0.06283422459893048,time decaying learning rate ηt = O( 1
PRIOR RESULTS,0.06417112299465241,"t ) on non-convex loss functions in which γN ≲
√"
PRIOR RESULTS,0.06550802139037433,"T
N and ρT
can scale as large as O(1)."
PRIOR RESULTS,0.06684491978609626,"Open problem and motication. Keeping the merits and deﬁciencies of above recalled prior results
in mind, it still remains an open issue if the existing deviation bounds can hold with high conﬁdence
for randomized algorithms with on-average uniform stability (such as SGD with decaying learning
rates). The goal of the present study is to derive sharper high-probability generalization bounds for
randomized algorithms that hold jointly over the randomness of data and algorithm, based on the
relatively weaker notion of on-average uniform stability rather than its high probability counterpart."
OVERVIEW OF OUR RESULTS,0.06818181818181818,"1.2
OVERVIEW OF OUR RESULTS"
OVERVIEW OF OUR RESULTS,0.06951871657754011,"The conﬁdence-boosting technique of Schapire (1990) is a classical meta approach that allows us
to boost the dependence of a learning algorithm on the failure probability δ from 1/δ to log(1/δ),
at a certain cost of computational complexity. The fundamental contribution of our work is to
reveal that the conﬁdence-boosting trick yields near-tight high probability generalization bounds
for uniformly stable randomized learning algorithms. The novelty lies in a reﬁned analysis of the
in-expectation ﬁrst moment generalization error bound for a randomized learning algorithm with on-
average uniform stability, which leads to improved high-probability generalization bounds over the
randomness of data and algorithm via conﬁdence-boosting. More speciﬁcally, given a randomized
learning algorithm A, we consider the following subbagging process over training set:"
OVERVIEW OF OUR RESULTS,0.07085561497326204,"Boosting the Conﬁdence via Subbagging.
We independently run A over K disjoint and u-
niformly divided training subsets {Sk}k∈[K] to obtain solutions {Ak(Sk)}k∈[K].
Then we e-"
OVERVIEW OF OUR RESULTS,0.07219251336898395,Under review as a conference paper at ICLR 2022
OVERVIEW OF OUR RESULTS,0.07352941176470588,"valuate the validation error of each candidate solution over its complementary training subset,
and output Ak∗(Sk∗) that has the smallest gap between training error and validation error, i.e.,
k∗= arg mink∈[K]
RS\Sk(Ak(Sk)) −RSk(Ak(Sk))
. Specially when A is deterministic, this re-
duces to a standard subbagging process, namely a variation of bagging using without-replacement
sampling for subsets generation (see, e.g., Andonova et al., 2002). In general, this is essentially a
subbagging procedure with greedy model selection for randomized algorithms over multiple dis-
joint subsets. Here we have assumed without loss of generality that N is a multiplier of K. The
considered procedure of conﬁdence-boosting for randomized learning is outlined in Algorithm 1."
OVERVIEW OF OUR RESULTS,0.0748663101604278,"Main Results. In what follows, we highlight our main results on the generalization bounds of the
output of Algorithm 1 along with the implications for SGD and deterministic algorithms:"
OVERVIEW OF OUR RESULTS,0.07620320855614973,"• General results. Suppose that the loss is Lipschitz and bounded in the range of (0, 1]. Our main
result in Theorem 1 show that for any δ ∈(0, 1), setting K ≍log( 1"
OVERVIEW OF OUR RESULTS,0.07754010695187166,"δ ) yields the following
generalization bound of the output of Algorithm 1 that holds with probability at least 1 −δ over
S and {Ak}k∈[K]:"
OVERVIEW OF OUR RESULTS,0.07887700534759358,|R(Ak∗(Sk∗)) −RS(Ak∗(Sk∗))| ≲1 K
OVERVIEW OF OUR RESULTS,0.08021390374331551,"pγm2, N"
OVERVIEW OF OUR RESULTS,0.08155080213903744,"K + γm, N K + r K
N ! + r"
OVERVIEW OF OUR RESULTS,0.08288770053475936,"log(K/δ) N
,"
OVERVIEW OF OUR RESULTS,0.08422459893048129,"where γm,N and γm2,N are respectively mean-uniform stability and mean-square-uniform stability
bounds introduced in Deﬁnition 1. In contrast to the bound in equation 6, our bound is not relying
on the uniform stability with respect to the random bits of algorithm.
• Stronger generalization bounds for SGD via conﬁdence-boosting. We then use our general result-
s to study the beneﬁt of conﬁdence-boosting on the generalization bounds of SGD-w (SGD via
with-replacement sampling as outlined in Algorithm 2). The main results are a series of corollar-
ies of Theorem 1 when substantialized to SGD with smooth (Corollary 1) or non-smooth (Corol-
lary 2) convex loss, and smooth non-convex loss functions (Corollary 3). For an instance, our
result in Corollary 1 showcases that when invoked to SGD-w on smooth convex loss with learn-
ing rates ηt = O( 1
√"
OVERVIEW OF OUR RESULTS,0.0855614973262032,"t), the generalization bound of the output of Algorithm 1 with K ≍log( 1"
OVERVIEW OF OUR RESULTS,0.08689839572192513,"δ ) is
upper bounded by"
OVERVIEW OF OUR RESULTS,0.08823529411764706,"|R(ASGD-w,k∗(Sk∗)) −RS(ASGD-w,k∗(Sk∗))| ≲ r"
OVERVIEW OF OUR RESULTS,0.08957219251336898,"log(T) N
+ √ T +
√"
OVERVIEW OF OUR RESULTS,0.09090909090909091,"N log(1/δ) N
."
OVERVIEW OF OUR RESULTS,0.09224598930481283,"Compared with the O(
√"
OVERVIEW OF OUR RESULTS,0.09358288770053476,"T
N ) in-expectation bound of smooth convex SGD (Hardt et al., 2016), our
above bound is competitive in order while it holds with high conﬁdence.
• Sharper bounds for uniformly stable algorithms via conﬁdence-boosting. Specially for a deter-
ministic learning algorithm A that has γN-uniform stability for each data set size N ≥1, we
further show through Corollary 4 that the following bound holds for the output of Algorithm 1
with K = log( 1 δ ):"
OVERVIEW OF OUR RESULTS,0.09491978609625669,|R(A(Sk∗)) −RS(A(Sk∗))| ≲γ N K + r
OVERVIEW OF OUR RESULTS,0.0962566844919786,"log(K/δ) N
."
OVERVIEW OF OUR RESULTS,0.09759358288770054,"In the case of γN ≲
1
√"
OVERVIEW OF OUR RESULTS,0.09893048128342247,"N which holds in some popular learning paradigms such as regularized
ERM, the above bound implies (recall K ≍log( 1 δ ))"
OVERVIEW OF OUR RESULTS,0.10026737967914438,|R(A(Sk∗)) −RS(A(Sk∗))| ≲ r
OVERVIEW OF OUR RESULTS,0.10160427807486631,"log(1/δ) N
,"
OVERVIEW OF OUR RESULTS,0.10294117647058823,"which is sharper than the best known result in equation 5 (under δ′ = 0) in the sense of the
removal of a log(N) factor. This is a side contribution of our work that might be of independent
interest towards answering an open question raised by Bousquet et al. (2020)."
OVERVIEW OF OUR RESULTS,0.10427807486631016,"In addition to the generalization bounds, we have also derived a high probability excess risk bound
for uniformly stable randomized learning with conﬁdence-boosting. More speciﬁcally, with a proper
modiﬁcation of the output of Algorithm 1, we can show that with probability at least 1 −δ over S
and {Ak}k∈[K]:"
OVERVIEW OF OUR RESULTS,0.10561497326203209,"R(Ak∗(Sk∗)) −min
w∈W R(w) ≲γm, N"
OVERVIEW OF OUR RESULTS,0.10695187165775401,K + ∆opt + r
OVERVIEW OF OUR RESULTS,0.10828877005347594,"log(K/δ) N
,"
OVERVIEW OF OUR RESULTS,0.10962566844919786,where ∆opt is the in-expectation empirical risk optimization error as given by equation 7.
OVERVIEW OF OUR RESULTS,0.11096256684491979,Under review as a conference paper at ICLR 2022
GENERALIZATION ANALYSIS WITH CONFIDENCE-BOOSTING,0.11229946524064172,"2
GENERALIZATION ANALYSIS WITH CONFIDENCE-BOOSTING"
GENERALIZATION ANALYSIS WITH CONFIDENCE-BOOSTING,0.11363636363636363,"In this section, we present a set of generic results on the generalization bounds of randomized learn-
ing algorithms with conﬁdence-boosting as described in Algorithm 1."
PRELIMINARIES AND A KEY LEMMA,0.11497326203208556,"2.1
PRELIMINARIES AND A KEY LEMMA"
PRELIMINARIES AND A KEY LEMMA,0.1163101604278075,"We ﬁrst introduce the following concept of mean-uniform stability that serves as a powerful tool for
analyzing the generalization bounds of randomized learning algorithms (Hardt et al., 2016; Elisseeff
et al., 2005; Bassily et al., 2020).
Deﬁnition 1 (Mean and Mean-Square Uniform Stability of Randomized Algorithms). Let A :
ZN 7→W be a randomized learning algorithm that maps a data set S ∈ZN to a model A(S) ∈W.
Then A is said to have γm,N-mean-uniform stability if for every N ≥1,
sup
S .=S′ EA [∥A(S) −A(S′)∥] ≤γm,N."
PRELIMINARIES AND A KEY LEMMA,0.11764705882352941,"Moreover, A is said to have γm2,N-mean-square-uniform stability if for every N ≥1,"
PRELIMINARIES AND A KEY LEMMA,0.11898395721925134,"sup
S .=S′ EA

∥A(S) −A(S′)∥2
≤γm2,N."
PRELIMINARIES AND A KEY LEMMA,0.12032085561497326,"Here the algorithm outputs A(S) and A(S′) share the same random bits associated with the algo-
rithm. The above deﬁned notion of mean-uniform stability is also known as Uniform Argument
Stability (UAS) (Bassily et al., 2020), which was originally introduced by Liu et al. (2017) for non-
parametric hypotheses. The notion of mean-square uniform stability is stronger than mean-uniform
stability in the sense that the former naturally implies the latter such that γm,N ≤√γm2,N. More-
over, we say a function f is G-Lipschitz continuous over W if |f(w) −f(w′)| ≤G∥w −w′∥for all
w, w′ ∈W, and it is L-smooth if ∥∇f(w) −∇f(w′)∥≤L∥w −w′∥for all w, w′ ∈W."
PRELIMINARIES AND A KEY LEMMA,0.12165775401069519,"Inspired by a second moment bound for generalization error of uniformly stable algorithms
from Bousquet et al. (2020, Section 5), we ﬁrst establish the following lemma which states that if
a randomized learning algorithm has γm,N-mean-uniform stability, then its on-average ﬁrst moment
generalization error bound will be as small as O(γm,N + √γm2,N +
1
√"
PRELIMINARIES AND A KEY LEMMA,0.12299465240641712,"N ) when the loss function is
Lipschitz continuous. This result is an adaptation of the second moment bound derived by Bousquet
et al. (2020) to on-average uniform stable randomized algorithms, and that bound is also the source
of γm2,N entering into play. For completeness, we provide a proof for this result in Appendix B.1."
PRELIMINARIES AND A KEY LEMMA,0.12433155080213903,"Lemma 1. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability and γm2,N-mean-square-uniform stability. Assume that the loss function ℓis G-Lipschitz
with respect to its ﬁrst argument and is bounded in the range of [0, M]. Then we have"
PRELIMINARIES AND A KEY LEMMA,0.12566844919786097,"EA,S [|R(A(S)) −RS(A(S))|] ≤G√γm2,N + Gγm,N + M
√ N
."
PRELIMINARIES AND A KEY LEMMA,0.1270053475935829,"Remark 1. In comparison to the on-average bound |EA,S [R(A(S)) −RS(A(S))]| ≤Gγm,N (see,
e.g., Hardt et al., 2016, Theorem 2.2), our on-average ﬁrst moment bound in Lemma 1 is substantial-
ly stronger and it turns out to play a crucial role in deriving high probability bounds for randomized
algorithms. When A is deterministic, the above bound reduces to the explicitly or implicitly known
ﬁrst moment bound for uniformly stable algorithms (see, e.g., Feldman & Vondrak, 2018; Bousquet
et al., 2020) which is tighter in logarithmic factors than the one implied by the p-th moment in-
equality of (Bousquet et al., 2020, Theorem 4). In this case, our bound is stronger than the bound
from Bousquet & Elisseeff (2002, Lemma 9) which essentially scales as
1
√"
PRELIMINARIES AND A KEY LEMMA,0.12834224598930483,"N +√γm,N in our notation."
MAIN RESULTS ON GENERALIZATION BOUND,0.12967914438502673,"2.2
MAIN RESULTS ON GENERALIZATION BOUND"
MAIN RESULTS ON GENERALIZATION BOUND,0.13101604278074866,"Let us recall the subbagging process as described in Algorithm 1: we independently run A over
K even and disjoint training subsets {Sk}k∈[K] to obtain solutions {Ak(Sk)}k∈[K], and then pick
Ak∗(Sk∗) that has the smallest difference between training error and validation error (over the com-
plementary training subset S \ Sk∗). The following theorem is our main result about the high prob-
ability generalization bound of the output Ak∗(Sk∗) evaluated over the entire training set S. See
Appendix B.2 for its proof which builds largely on the ﬁrst moment bound in Lemma 1 and the fact
that at least one of the solutions generated by subbagging generalizes well with high probability."
MAIN RESULTS ON GENERALIZATION BOUND,0.1323529411764706,Under review as a conference paper at ICLR 2022
MAIN RESULTS ON GENERALIZATION BOUND,0.13368983957219252,"Theorem 1. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability and γm2,N-mean-square-uniform stability as well. Assume that the loss function ℓis G-
Lipschitz with respect to its ﬁrst argument and is bounded in the range of [0, M]. Then for any
α, δ ∈(0, 1) and K ≥
1
1−α log( 4"
MAIN RESULTS ON GENERALIZATION BOUND,0.13502673796791445,"δ ), with probability at least 1 −δ over the randomness of S and
{Ak}k∈[K], the output of Algorithm 1 satisﬁes"
MAIN RESULTS ON GENERALIZATION BOUND,0.13636363636363635,"|R(Ak∗(Sk∗)) −RS(Ak∗(Sk∗))| ≲
1
αK "
MAIN RESULTS ON GENERALIZATION BOUND,0.13770053475935828,"Gpγm2, N"
MAIN RESULTS ON GENERALIZATION BOUND,0.13903743315508021,"K + Gγm, N K + M r K
N ! + M r"
MAIN RESULTS ON GENERALIZATION BOUND,0.14037433155080214,"log(K/δ) N
."
MAIN RESULTS ON GENERALIZATION BOUND,0.14171122994652408,"Remark 2. To gain some intuition on the superiority of our bound in Theorem 1, let us consider
K ≍log
  1"
MAIN RESULTS ON GENERALIZATION BOUND,0.14304812834224598,"δ

as allowed in the conditions. If γm,N ≲
1
√"
MAIN RESULTS ON GENERALIZATION BOUND,0.1443850267379679,"N and γm2,N ≲1"
MAIN RESULTS ON GENERALIZATION BOUND,0.14572192513368984,"N , then our high-probability"
MAIN RESULTS ON GENERALIZATION BOUND,0.14705882352941177,"bound in Theorem 1 roughly scales as
q"
MAIN RESULTS ON GENERALIZATION BOUND,0.1483957219251337,log(1/δ)
MAIN RESULTS ON GENERALIZATION BOUND,0.1497326203208556,"N
which is sharper than the on-average bounds
in equation 2 and equation 3 with γN ≲
1
√"
MAIN RESULTS ON GENERALIZATION BOUND,0.15106951871657753,"N . More precise consequences of these general results on
SGD and deterministic uniformly stable estimators such as ERMs and full gradient descent method
will be discussed shortly in the sections to follow."
MAIN RESULTS ON GENERALIZATION BOUND,0.15240641711229946,"Remark 3. In sharp contrast to the bound in equation 5 that requires high probability uniform
stability and the bound in equation 6 that assumes uniform stability over the random bits of algo-
rithm, our bound in Theorem 1 holds under a substantially milder notion of mean(-square)-uniform"
MAIN RESULTS ON GENERALIZATION BOUND,0.1537433155080214,"stability over data. In terms of the tightness of bound, note that the conﬁdence term
q"
MAIN RESULTS ON GENERALIZATION BOUND,0.15508021390374332,log(1/δ)
MAIN RESULTS ON GENERALIZATION BOUND,0.15641711229946523,"N
is
necessary even for an algorithm with ﬁxed output. The uniform stability terms γm, N"
MAIN RESULTS ON GENERALIZATION BOUND,0.15775401069518716,"K and γm2, N"
MAIN RESULTS ON GENERALIZATION BOUND,0.1590909090909091,"K are
also near-tight as the the algorithm output can change arbitrarily with respect to these quantities."
ON EXCESS RISK BOUNDS,0.16042780748663102,"2.3
ON EXCESS RISK BOUNDS"
ON EXCESS RISK BOUNDS,0.16176470588235295,"To understand the optimization performance of a randomized learning algorithm A with conﬁdence-
boosting, we further study here the excess risk bounds of Algorithm 1 which are of special interest
for stochastic convex optimization problems. In the following analysis, the global minimizer of the
population risk and in-expectation empirical risk sub-optimality of the randomized algorithm are
respectively denoted by"
ON EXCESS RISK BOUNDS,0.16310160427807488,"w∗:= arg min
w∈W
R(w) and ∆opt := EA,S"
ON EXCESS RISK BOUNDS,0.16443850267379678,"
RS(A(S)) −min
w∈W RS(w)

.
(7)"
ON EXCESS RISK BOUNDS,0.1657754010695187,"In order to derive the excess risk guarantees, we ﬁrst need to slightly modify the output of Algo-
rithm 1 as Ak∗(Sk∗) where k∗= arg mink∈[K] RS\Sk(Ak(Sk)). The following theorem is our
main result about the high probability excess risk bounds of such a modiﬁed output of conﬁdence-
boosting. See Appendix B.3 for its proof."
ON EXCESS RISK BOUNDS,0.16711229946524064,"Theorem 2. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability. Assume that the loss function ℓis G-Lipschitz with respect to its ﬁrst argument and is
bounded in the range of [0, M]. Then for any α, δ ∈(0, 1) and K ≥
1
1−α log( 4"
ON EXCESS RISK BOUNDS,0.16844919786096257,"δ ), with probability
at least 1 −δ over the randomness of S and {Ak}k∈[K], the modiﬁed output of Algorithm 1 satisﬁes"
ON EXCESS RISK BOUNDS,0.1697860962566845,R(Ak∗(Sk∗)) −R(w∗) ≲1 α
ON EXCESS RISK BOUNDS,0.1711229946524064,"
Gγm, N"
ON EXCESS RISK BOUNDS,0.17245989304812834,"K + ∆opt

+ M r"
ON EXCESS RISK BOUNDS,0.17379679144385027,"log(K/δ) N
."
ON EXCESS RISK BOUNDS,0.1751336898395722,"Remark 4. Unlike the generalization error bounds, the excess risk bounds established in Theorem 2
are not relying on the mean-square uniform stability of the algorithm, but with an additional term of
in-expectation optimization error for minimizing the empirical risk. For deterministic optimization
algorithms such as ERMs with ∆opt = 0, similar excess risk bounds can be implied by the generic
results of Shalev-Shwartz et al. (2010, Theorem 26) developed for the conﬁdence-boosting approach."
ON EXCESS RISK BOUNDS,0.17647058823529413,"Remark 5. Consider K ≍log
  1"
ON EXCESS RISK BOUNDS,0.17780748663101603,"δ

and γm,N ≲
1
√"
ON EXCESS RISK BOUNDS,0.17914438502673796,N . Then the bound in Theorem 2 roughly scales
ON EXCESS RISK BOUNDS,0.1804812834224599,"as O(
q"
ON EXCESS RISK BOUNDS,0.18181818181818182,log(1/δ)
ON EXCESS RISK BOUNDS,0.18315508021390375,"N
+ ∆opt)."
ON EXCESS RISK BOUNDS,0.18449197860962566,"Finally, we comment on the difference between the generalization error and excess risk analysis
inside the considered conﬁdence-boosting framework. Since the excess risk is non-negative and"
ON EXCESS RISK BOUNDS,0.1858288770053476,Under review as a conference paper at ICLR 2022
ON EXCESS RISK BOUNDS,0.18716577540106952,Algorithm 2: SGD via With-Replacement Sampling (ASGD-w)
ON EXCESS RISK BOUNDS,0.18850267379679145,"Input : Data set S = {zi}i∈[N]
i.i.d.
∼DN, step-sizes {ηt}t≥1, #iterations T, initialization w0.
Output: ¯wT = 1 T
P"
ON EXCESS RISK BOUNDS,0.18983957219251338,"t∈[T ] wt.
for t = 1, 2, ..., T do"
ON EXCESS RISK BOUNDS,0.19117647058823528,"Uniformly randomly sample an index ξt ∈[N] with replacement;
Compute wt = ΠW (wt−1 −ηt∇wℓ(wt−1; zξt)).
end"
ON EXCESS RISK BOUNDS,0.1925133689839572,"its in-expectation bound is standardly known for uniformly stable learning algorithms, the high-
conﬁdence bound in Theorem 2 can be easily derived via invoking Markov inequality to the inde-
pendent runs of algorithm over K disjoint subsets. The generalization error analysis, however, is
way more challenging in the sense that establishing tight in-expectation ﬁrst moment generalization
bound (see Lemma 1) for uniformly stable randomized algorithms is by itself highly non-trivial."
IMPLICATIONS FOR STOCHASTIC GRADIENT DESCENT,0.19385026737967914,"3
IMPLICATIONS FOR STOCHASTIC GRADIENT DESCENT"
IMPLICATIONS FOR STOCHASTIC GRADIENT DESCENT,0.19518716577540107,"In this section we demonstrate the applications of the generic bound in Theorem 1 to the widely
used SGD algorithm. We focus on a variant of SGD under with-replacement sampling as outlined
in Algorithm 2, which we call ASGD-w. In what follows, we denote by {ASGD-w,k}k∈K the outputs
of ASGD-w over subsets {Sk}k∈K when applied with Algorithm 1. Our results readily extend to the
without-replacement variant of SGD and the corresponding results can be found in Appendix D."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.196524064171123,"3.1
CONVEX OPTIMIZATION WITH SMOOTH LOSS"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.19786096256684493,"For smooth and convex losses such as logistic loss, we can derive the following result as a direct
consequence of Theorem 1 with α = 1/2 to ASGD-w. See Appendix C.1 for its proof.
Corollary 1. Suppose that the loss function is ℓ(·; ·) is convex, G-Lipschitz and L-smooth with
respect to its ﬁrst argument, and is bounded in the range of [0, M]. Consider Algorithm 1 speciﬁed
to ASGD-w with learning rate ηt ≤2/L for all t ≥1. Then for any δ ∈(0, 1) and K ≍log( 4"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.19919786096256684,"δ ), with
probability at least 1 −δ over the randomness of S and {ASGD-w,k}k∈[K], the generalization error
is upper bounded as |R(ASGD-w,k∗(Sk∗)) −RS(ASGD-w,k∗(Sk∗))| ≲ G2"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20053475935828877,"v
u
u
u
t 1 N  
T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2018716577540107,"t=1
η2
t + 1 N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20320855614973263,"t=1
ηt !2 + G2 N T
X"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20454545454545456,"t=1
ηt + M r"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20588235294117646,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2072192513368984,"Remark 6. For the conventional step-size choice of ηt =
2
L
√"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20855614973262032,"t, the high probability generalization"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.20989304812834225,"bound in Corollary 1 is dominated by O(
q"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.21122994652406418,"log(T ) N
+ √ T +√"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.21256684491978609,N log(1/δ)
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.21390374331550802,"N
), which matches the corre-"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.21524064171122995,"sponding O(
√"
CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.21657754010695188,"T
N ) in-expectation bound for SGD with smooth convex losses (Hardt et al., 2016)."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.2179144385026738,"3.2
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.2192513368983957,"Now we turn to study the case where the loss is convex but not necessarily smooth, such as the
hinge loss and absolute loss. The following result as a direct consequence of Theorem 1 for the
speciﬁcation of Algorithm 1 to ASGD-w with non-smooth convex loss and time varying learning rate
{ηt}t≥1. Its proof is provided in Appendix C.2.
Corollary 2. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its ﬁrst
argument, and is bounded in the range of [0, M]. Consider Algorithm 1 speciﬁed to ASGD-w. Then
for any δ ∈(0, 1) and K ≍log( 4"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22058823529411764,"δ ), with probability at least 1 −δ over the randomness of S and
{ASGD-w,k}k∈[K], the generalization error satisﬁes |R(ASGD-w,k∗(Sk∗)) −RS(ASGD-w,k∗(Sk∗))| ≲ G2"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22192513368983957,"v
u
u
t T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.2232620320855615,"t=1
η2
t + 1 N 2 T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22459893048128343,"t=1
ηt !2 + G2"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22593582887700533,"v
u
u
t T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22727272727272727,"t=1
η2
t + G2 N T
X"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.2286096256684492,"t=1
ηt + M r"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.22994652406417113,"log(1/δ) N
."
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.23128342245989306,Under review as a conference paper at ICLR 2022
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.232620320855615,"Remark 7. For constant rates ηt ≡η, Corollary 2 admits a high probability generalization bound"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.2339572192513369,"of scale O(η
√"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.23529411764705882,"T + η T N +
q"
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.23663101604278075,log(1/δ)
CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS,0.23796791443850268,"N
) which matches the near-optimal rate by Bassily et al. (2020,
Theorem 3.3). More importantly, our deviation bound in Corollary 2 still holds for time varying
learning rates."
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2393048128342246,"3.3
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24064171122994651,"We further study the performance of Algorithm 1 for SGD on smooth but not necessarily convex
loss functions, such as normalized sigmoid loss (Mason et al., 1999). The following result is a direct
application of Theorem 1 to ASGD-w with smooth non-convex loss. See Appendix C.3 for its proof.
Corollary 3. Suppose that the loss function is ℓ(·; ·) is G-Lipschitz and L-smooth with respect to
its ﬁrst argument, and is bounded in the range of [0, M]. Consider Algorithm 1 speciﬁed to ASGD-w
with ηt ≤1"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24197860962566844,"L. Let ut := η2
t + 2ηt
Pt−1
τ=1 exp(L Pt−1
i=τ+1 ηi)ητ for all t ≥1. Then for any δ ∈(0, 1)
and K ≍log( 4"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24331550802139038,"δ ), with probability at least 1−δ over the randomness of S and {ASGD-w,k}k∈[K], the
generalization error is upper bounded as |R(ASGD-w,k∗(Sk∗)) −RS(ASGD-w,k∗(Sk∗))| ≲ G2"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2446524064171123,"v
u
u
t 1 N T
X"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24598930481283424,"t=1
exp  L T
X"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24732620320855614,"τ=t+1
ητ !"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.24866310160427807,"ut + G2 N T
X"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.25,"t=1
exp  L T
X"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.25133689839572193,"τ=t+1
ητ !"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.25267379679144386,ηt + M r
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2540106951871658,"log(1/δ) N
."
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2553475935828877,"Remark 8. For the constant learning rates ηt ≡
1
LT , Corollary 3 admits high probability gener-"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.25668449197860965,"alization bound of scale O(
q"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2580213903743315,log(1/δ)
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.25935828877005346,"N
). For time decaying learning rates ηt =
1
Lνt with arbitrary"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2606951871657754,"ν ≥1, it can be veriﬁed that the corresponding bound is of scale O(
q"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2620320855614973,T 1/ν log(T )
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.26336898395721925,"νN
+
q"
NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS,0.2647058823529412,"log(1/δ) N
)."
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2660427807486631,"4
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.26737967914438504,"This section is devoted to showing that conﬁdence-boosting is also beneﬁcial for deriving stronger
generalization bounds for uniformly stable deterministic learning algorithms. First we note that
when there is no internal randomness in A, the deﬁnition of mean(-square)-uniform stability reduces
to the conventional concept of γN-uniform stability for deterministic algorithms given by
sup
S .=S′ ∥A(S) −A(S′)∥≤γN = γm,N = √γm2,N."
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.26871657754010697,"Let us now consider a speciﬁcation of Algorithm 1 to a uniformly stable deterministic algorithm A.
Since there is no randomness contained in A, we have that Ak(Sk) = A(Sk) for all k ∈[K] in such
a deterministic case. Then the following result is a direct consequence of Theorem 1 when applied
to the considered deterministic learning regime.
Corollary 4. Suppose that a deterministic learning algorithm A : ZN 7→W has γN-uniform
stability. Assume that the loss function ℓis G-Lipschitz with respect to its ﬁrst argument and is
bounded in [0, M]. Then for any α, δ ∈(0, 1) and K ≥log(4/δ)"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2700534759358289,"1−α , with probability at least 1 −δ over
the randomness of S, the output of Algorithm 1 satisﬁes"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2713903743315508,"|R(A(Sk∗)) −RS(A(Sk∗))| ≲
1
αK  Gγ N K + M r K
N ! + M r"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2727272727272727,"log(K/δ) N
."
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.27406417112299464,"To demonstrate the superiority of our bound over prior ones, let us consider α = 0.5 and K ≍log( 1"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.27540106951871657,"δ )
in the above corollary. In the regime γN ≲
1
√"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2767379679144385,"N which is of interest in many popular deterministic
learning paradigms/algorithms such as regularized ERM (Shalev-Shwartz et al., 2009) and full gra-
dient descent (Feldman & Vondrak, 2019), Corollary 4 implies a generalization bound for A(Sk∗)"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.27807486631016043,"over the data set S that scales as |R(A(Sk∗)) −RS(A(Sk∗))| ≲
q"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.27941176470588236,log(1/δ)
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2807486631016043,"N
. In comparison, the best
known bound in equation 5 essentially from Bousquet et al. (2020) gives (keep in mind that δ′ = 0
in the deterministic case) |R(A(S)) −RS(A(S))| ≲log(N)
√"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.2820855614973262,"N
log( 1"
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.28342245989304815,"δ ). As we can see that inside the
carefully designed framework of conﬁdence-bossting via subbagging, our generalization bound gets
rid of the logarithmic factor log(N) from the above best known result, though the generalization is
with respect to the estimation over a speciﬁc part of the sample. We expect this result will fuel future
research towards fully resolving the corresponding open question raised in Bousquet et al. (2020)."
IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS,0.28475935828877,Under review as a conference paper at ICLR 2022
OTHER RELATED WORK,0.28609625668449196,"5
OTHER RELATED WORK"
OTHER RELATED WORK,0.2874331550802139,"The idea of using stability of a learning algorithm, namely the sensitivity of estimated model to the
changes in training data, for generalization performance analysis dates back to the seventies (Vap-
nik & Chervonenkis, 1974; Rogers & Wagner, 1978; Devroye & Wagner, 1979). For deterministic
learning algorithms, algorithmic stability has been extensively studied with a bunch of applications
to establishing strong generalization and excess risk bounds for stable learning models like k-NN
and regularized ERMs (Bousquet & Elisseeff, 2002; Zhang, 2003; Klochkov & Zhivotovskiy, 2021).
The stability theory for randomized learning algorithms was formally introduced and investigated
by Elisseeff et al. (2005). In a recent breakthrough work (Hardt et al., 2016), it was shown in that
the solution obtained via stochastic gradient descent is expected to be stable and generalize well for
smooth convex and non-convex loss functions. For non-smooth convex losses, the stability induced
generalization bounds of SGD have been established in expectation (Lei & Ying, 2020) or devia-
tion (Bassily et al., 2020). In Kuzborskij & Lampert (2018), a set of data-dependent generalization
bounds for SGD were derived based on the stability of algorithm. More broadly, generalization
bounds for stable learning algorithms that converge to global minima were established in Charles
& Papailiopoulos (2018); Lei & Ying (2021). For non-convex sparse learning, algorithmic stabili-
ty theory has been applied to derive the generalization bounds of the popularly used iterative hard
thresholding (IHT) algorithm (Yuan & Li, 2021). The uniform stability bounds on SGD have also
been extensively used for designing differential privacy stochastic optimization algorithms (Bassily
et al., 2019; Feldman et al., 2020)."
OTHER RELATED WORK,0.2887700534759358,"Bagging (or bootstrap aggregating) is one of the earliest and most popular ensemble methods that
has been widely applied to reduce the variance for unstable learning algorithms such as decision
tree and neural networks (Breiman, 1996; Opitz & Maclin, 1999), and sometimes stable algorithms
such as SVMs (Valentini & Dietterich, 2003). As an important variant of bagging, subbagging has
been proposed to reduce the computational cost of bagging via training base models under without-
replacement sampling (B¨uhlmann, 2012). The stability and generalization bounds of bagging have
been analyzed for both uniform (Elisseeff et al., 2005) and non-uniform (Foster et al., 2019) av-
eraging schemes. Unlike these prior results for bagging with averaging aggregation, our bounds
are obtained based on a conﬁdence-boosting greedy aggregation scheme which turns out to yield
sharper dependence on the uniform stability parameter."
OTHER RELATED WORK,0.29010695187165775,"The conﬁdence-boosting technique has long been applied for obtaining sharp high-probability ex-
cess risk bounds from the corresponding strong in-expectation bounds (Shalev-Shwartz et al., 2010;
Mehta, 2017). For generic statistical learning problems, conﬁdence-boosting has been used to con-
vert any low-conﬁdence learning algorithm with linear dependence on 1/δ to a high-conﬁdence
algorithm with logarithmic factor log(1/δ). For learning with exp-concave losses, a relevant ERM
estimator with in-expectation fast rate of convergence was converted to a high-conﬁdence learning
algorithm with an almost identical fast rate of convergence up to a logarithmic factor on 1/δ (Mehta,
2017). While sharing a similar spirit of boosting the conﬁdence, our generalization analysis is sub-
stantially more challenging than those prior excess risk analysis in terms of tightly deriving in-
expectation ﬁrst moment generalization bound for uniformly stable randomized algorithms."
CONCLUSIONS,0.2914438502673797,"6
CONCLUSIONS"
CONCLUSIONS,0.2927807486631016,"In this paper we presented a generic conﬁdence-boosting method for deriving near-optimal high
probability generalization bounds for uniformly stable randomized learning algorithms. At a nut-
shell, our main results in Theorem 1 and Theorem 2 reveal that a carefully designed subbagging
process in Algorithm 1 can yield high-conﬁdence generalization and risk bounds under the notion
of mean(-square)-uniform stability. Our theory has been substantialized to SGD on both convex and
non-convex losses to obtain stronger generalization bounds especially in the case of time decaying
learning rates. When reduced to deterministic algorithms, the proposed method removes a logarith-
mic factor on sample size from the best known bounds. While sharper in the dependence on sample
size, our conﬁdence-boosting results are only applicable to one of the independent runs of algorithm
A over K disjoint training subsets of equal size with K ≍log
  1"
CONCLUSIONS,0.29411764705882354,"δ

. It is so far not clear if these
near-optimal bounds can be further extended to the full-batch setting where the generalization is
with respect to the evaluation of algorithm over the entire sample. We leave the full understanding
of such an open issue raised by Bousquet et al. (2020) for future investigation."
CONCLUSIONS,0.29545454545454547,Under review as a conference paper at ICLR 2022
REFERENCES,0.2967914438502674,REFERENCES
REFERENCES,0.29812834224598933,"Savina Andonova, Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. A simple algo-
rithm for learning stable machines. In ECAI, pp. 513–517, 2002."
REFERENCES,0.2994652406417112,"Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classiﬁcation, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, 2006."
REFERENCES,0.30080213903743314,"Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex
optimization with optimal rates. arXiv preprint arXiv:1908.09970, 2019."
REFERENCES,0.30213903743315507,"Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems, pp.
1–10, 2020."
REFERENCES,0.303475935828877,"Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. MIT press Mas-
sachusetts, USA:, 2017."
REFERENCES,0.3048128342245989,"L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural
Information Processing Systems, pp. 161–168, 2008."
REFERENCES,0.30614973262032086,"Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2(Mar):499–526, 2002."
REFERENCES,0.3074866310160428,"Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610–626, 2020."
REFERENCES,0.3088235294117647,"Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996."
REFERENCES,0.31016042780748665,"Peter B¨uhlmann. Bagging, boosting and ensemble methods. In Handbook of computational statis-
tics, pp. 985–1022. Springer, 2012."
REFERENCES,0.3114973262032086,"Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 744–753,
2018."
REFERENCES,0.31283422459893045,"Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202–207, 1979."
REFERENCES,0.3141711229946524,"Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55–79, 2005."
REFERENCES,0.3155080213903743,"Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad-
vances in Neural Information Processing Systems, pp. 9747–9757, 2018."
REFERENCES,0.31684491978609625,"Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algo-
rithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270–1279, 2019."
REFERENCES,0.3181818181818182,"Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 439–449, 2020."
REFERENCES,0.3195187165775401,"Dylan J Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Srid-
haran. Hypothesis set stability and generalization. In Advances in Neural Information Processing
Systems, 2019."
REFERENCES,0.32085561497326204,"Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234, 2016."
REFERENCES,0.32219251336898397,"Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-
smooth stochastic gradient descent. In Conference on Learning Theory, pp. 1579–1613. PMLR,
2019."
REFERENCES,0.3235294117647059,"Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing
Systems, pp. 793–800, 2009."
REFERENCES,0.32486631016042783,Under review as a conference paper at ICLR 2022
REFERENCES,0.32620320855614976,"Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with conver-
gence rate o(1/n). In Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.32754010695187163,"Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, pp. 2820–2829, 2018."
REFERENCES,0.32887700534759357,"Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for sgd. In
International Conference on Machine Learning, 2020."
REFERENCES,0.3302139037433155,"Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2021."
REFERENCES,0.3315508021390374,"Tongliang Liu, G´abor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, pp. 2159–2167. PMLR, 2017."
REFERENCES,0.33288770053475936,"Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient
descent in function space. In Advances in Neural Information Processing Systems, pp. 512–518,
1999."
REFERENCES,0.3342245989304813,"Nishant Mehta. Fast rates with high probability in exp-concave statistical learning. In Artiﬁcial
Intelligence and Statistics, pp. 1085–1093. PMLR, 2017."
REFERENCES,0.3355614973262032,"Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is
sufﬁcient for generalization and necessary and sufﬁcient for consistency of empirical risk mini-
mization. Advances in Computational Mathematics, 25(1-3):161–193, 2006."
REFERENCES,0.33689839572192515,"David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
Artiﬁcial Intelligence Research, 11:169–198, 1999."
REFERENCES,0.3382352941176471,"Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In International Conference on Machine Learning, pp.
1571–1578, 2012."
REFERENCES,0.339572192513369,"William H Rogers and Terry J Wagner. A ﬁnite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506–514, 1978."
REFERENCES,0.3409090909090909,"Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990."
REFERENCES,0.3422459893048128,"Mark Schmidt, Nicolas L Roux, and Francis R Bach. Convergence rates of inexact proximal-gradient
methods for convex optimization. In Advances in Neural Information Processing Systems, pp.
1458–1466, 2011."
REFERENCES,0.34358288770053474,"Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex opti-
mization. In Conference on Learning Theory, 2009."
REFERENCES,0.3449197860962567,"Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635–2670, 2010."
REFERENCES,0.3462566844919786,"Giorgio Valentini and Thomas G Dietterich. Low bias bagged support vector machines. In Interna-
tional Conference on Machine Learning, pp. 752–759, 2003."
REFERENCES,0.34759358288770054,"V. N. Vapnik and A. Ya. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, 1974."
REFERENCES,0.34893048128342247,"Xiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pp. 1702–1710. PMLR, 2021."
REFERENCES,0.3502673796791444,"Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397–1437,
2003."
REFERENCES,0.3516042780748663,"Yi Zhou, Huishuai Zhang, and Yingbin Liang. Understanding generalization error of sgd in non-
convex optimization. In International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), 2019."
REFERENCES,0.35294117647058826,Under review as a conference paper at ICLR 2022
REFERENCES,0.35427807486631013,"A
AUXILIARY LEMMAS"
REFERENCES,0.35561497326203206,"We need the following lemma from Hardt et al. (2016) which shows that SGD iteration is non-
expansive on convex and smooth loss."
REFERENCES,0.356951871657754,"Lemma 2 (Hardt et al. (2016)). Assume that f is convex and L-smooth. Then for any w, w′ ∈W
and α ≤2/L, we have the following bound holds"
REFERENCES,0.3582887700534759,∥w −α∇f(w) −(w′ −α∇f(w′))∥≤∥w −w′∥.
REFERENCES,0.35962566844919786,"The following lemma, which can be proved by induction (see, e.g., Schmidt et al., 2011), will be
used to prove the main results in Section 3."
REFERENCES,0.3609625668449198,"Lemma 3. Assume that the nonnegative sequence {uτ}τ≥1 satisﬁes the following recursion for all
t ≥1:"
REFERENCES,0.3622994652406417,"u2
t ≤St + t
X"
REFERENCES,0.36363636363636365,"τ=1
ατuτ,"
REFERENCES,0.3649732620320856,"with {Sτ}τ≥1 an increasing sequence, S0 ≥u2
0 and ατ ≥0 for all τ. Then, the following inequality
holds for all t ≥1:"
REFERENCES,0.3663101604278075,"ut ≤
p St + t
X"
REFERENCES,0.36764705882352944,"τ=1
ατ."
REFERENCES,0.3689839572192513,"B
PROOFS FOR THE RESULTS IN SECTION 2"
REFERENCES,0.37032085561497324,"In this section, we present the technical proofs for the main results stated in Section 2."
REFERENCES,0.3716577540106952,"B.1
PROOF OF LEMMA 1"
REFERENCES,0.3729946524064171,"We need the following lemma essentially from Bousquet et al. (2020) that provides a ﬁrst moment
bound for the sum of random functions."
REFERENCES,0.37433155080213903,"Lemma 4. Let S = {Z1, Z2, ..., ZN} be a set of i.i.d. random variables valued in Z. Let g1, ..., gN
be a set of measurable functions gi : ZN 7→R that satisfy ES[g2
i (S)] ≤M 2 and EZi[gi(S)] = 0
for all i ∈[N]. Then we have ES "" N
X"
REFERENCES,0.37566844919786097,"i=1
gi(S)  # ≤
sX"
REFERENCES,0.3770053475935829,"i̸=j
ES,S(j)
h 
gi(S) −gi(S(j))
2i
+ M
√ N,"
REFERENCES,0.3783422459893048,"where S(j) = {Z1, ..., Zj−1, Z′
j, Zj+1, ..., ZN} and S′ = {Z′
1, Z′
2, ..., Z′
N} is another i.i.d. sample
from the same distribution as that of S."
REFERENCES,0.37967914438502676,"Proof. We reproduce the proof in view of the argument in Bousquet et al. (2020, Section 5) showing
that {gi} are weakly correlated. For any i ̸= j, since EZi[gi(S)] = 0 and EZj[gj(S)] = 0, we can
verify that"
REFERENCES,0.3810160427807487,"ES
h
gi(S(j))gj(S)
i
= ES\Zj
h
EZj
h
gi(S(j))gj(S) | S \ Zj
ii
= ES\Zj
h
gi(S(j))EZj [gj(S) | S \ Zj]
i
= 0,"
REFERENCES,0.38235294117647056,"where we have used the independence of the elements in S ∪{Z′
j}. Similarly, we can show that"
REFERENCES,0.3836898395721925,"ES
h
gi(S)gj(S(i))
i
= ES
h
gi(S(j))gj(S(i))
i
= 0."
REFERENCES,0.3850267379679144,"Then it follows that for any i ̸= j,"
REFERENCES,0.38636363636363635,"|ES [gi(S)gj(S)]| =
ES,S(i),S(j) [gi(S)gj(S)]"
REFERENCES,0.3877005347593583,"=
ES,S(i),S(j)
h
(gi(S) −gi(S(j)))(gj(S) −gj(S(i)))
i"
REFERENCES,0.3890374331550802,"≤ES,S(i),S(j)
h(gi(S) −gi(S(j)))(gj(S) −gj(S(i)))

i
."
REFERENCES,0.39037433155080214,Under review as a conference paper at ICLR 2022
REFERENCES,0.3917112299465241,"Based on the above bound and Jensen’s inequality we get ES "" N
X"
REFERENCES,0.393048128342246,"i=1
gi(S)  # ≤"
REFERENCES,0.39438502673796794,"v
u
u
u
tES   N
X"
REFERENCES,0.39572192513368987,"i=1
gi(S) !2 ="
REFERENCES,0.39705882352941174,"v
u
u
tX"
REFERENCES,0.3983957219251337,"i̸=j
ES [gi(S)gj(S)] + N
X"
REFERENCES,0.3997326203208556,"i=1
ES[g2
i (S)] ≤
sX"
REFERENCES,0.40106951871657753,"i̸=j
ES,S(i),S(j)
(gi(S) −gi(S(j)))(gj(S) −gj(S(i)))

+ M
√ N ≤
s 1
2 X"
REFERENCES,0.40240641711229946,"i̸=j
ES,S(i),S(j)
h 
gi(S) −gi(S(j))
2 +
 
gj(S) −gj(S(i))
2i
+ M
√ N =
sX"
REFERENCES,0.4037433155080214,"i̸=j
ES,S(j)
h 
gi(S) −gi(S(j))
2i
+ M
√ N."
REFERENCES,0.4050802139037433,This proves the desired bound.
REFERENCES,0.40641711229946526,Now we are ready to prove the result in Lemma 1.
REFERENCES,0.4077540106951872,"Proof of Lemma 1. Let us consider hi(S) := R(A(S)) −ℓ(A(S); Zi) and gi(S) = hi(S) −
EZi[hi(S)] for i ∈[N]. Then by assumption we have
EZi[gi(S)] = 0,
ES[g2
i (S)] ≤M 2.
For each i ∈[N], let S(i) denote a random data set that is identical to S except that one of the
Zi is replaced by another random sample Z′
i. For any i ̸= j, since the loss is non-negative and
G-Lipschitz, it can be veriﬁed that"
REFERENCES,0.4090909090909091,"|gi(S) −gi(S(j))| ≤max
nhi(S) −hi(S(j))
 ,
EZi[hi(S) −hi(S(j))]

o"
REFERENCES,0.410427807486631,"≤max
n
G∥A(S) −A(S(j))∥, EZi
h
G∥A(S) −A(S(j))∥
io,"
REFERENCES,0.4117647058823529,which readily implies
REFERENCES,0.41310160427807485,"ES,S(j)

gi(S) −gi(S(j))
2
≤ES,S(j)
h
G2∥A(S) −A(S(j))∥2i
."
REFERENCES,0.4144385026737968,"Then invoking Lemma 4 to {gi} yields ES "" N
X"
REFERENCES,0.4157754010695187,"i=1
gi(S)  # ≤
sX"
REFERENCES,0.41711229946524064,"i̸=j
ES,S(j)
h 
gi(S) −gi(S(j))
2i
+ M
√ N ≤G
sX"
REFERENCES,0.4184491978609626,"i̸=j
ES,S(j)

∥A(S) −A(S(j))∥2
+ M
√ N. (A.1)"
REFERENCES,0.4197860962566845,"Further, it can be veriﬁed that ES "" N
X"
REFERENCES,0.42112299465240643,"i=1
EZi[hi(S)]  # =ES "" N
X"
REFERENCES,0.42245989304812837,"i=1
EZi[R(A(S)) −ℓ(A(S); Zi)]  # =ES "" N
X"
REFERENCES,0.42379679144385024,"i=1
EZi[EZ′
i[ℓ(A(S); Z′
i)] −ℓ(A(S); Zi)]  # =ES "" N
X"
REFERENCES,0.42513368983957217,"i=1
EZi
h
EZ′
i[ℓ(A(S); Z′
i)] −EZ′
i[ℓ(A(S(i)); Z′
i)]
i #"
REFERENCES,0.4264705882352941,"≤ES,S(i)
h
G∥A(S) −A(S(i))∥
i
. (A.2)"
REFERENCES,0.42780748663101603,Under review as a conference paper at ICLR 2022
REFERENCES,0.42914438502673796,By combining equation A.1 and equation A.2 we obtain
REFERENCES,0.4304812834224599,"EA,S [|R(A(S)) −RS(A(S))|] = 1"
REFERENCES,0.4318181818181818,"N EA,S "" N
X"
REFERENCES,0.43315508021390375,"i=1
(gi(S) + EZi[hi(S)])  # ≤G N EA  
sX"
REFERENCES,0.4344919786096257,"i̸=j
ES,S(j)

∥A(S) −A(S(j))∥2
 + G"
REFERENCES,0.4358288770053476,"N EA,S,S(i)
h
∥A(S) −A(S(i))∥
i
+ M
√ N ≤G N sX"
REFERENCES,0.43716577540106955,"i̸=j
EA,S,S(j)

∥A(S) −A(S(j))∥2
+ G"
REFERENCES,0.4385026737967914,"N EA,S,S(i)
h
∥A(S) −A(S(i))∥
i
+ M
√ N"
REFERENCES,0.43983957219251335,"≤G√γm2,N + Gγm,N + M
√ N
,"
REFERENCES,0.4411764705882353,where in the last inequality we have used the stability conditions on A. The proof is completed.
REFERENCES,0.4425133689839572,"B.2
PROOF OF THEOREM 1"
REFERENCES,0.44385026737967914,"We ﬁrst establish the following intermediate result that captures the effects of subbagging on ran-
domized algorithms: it basically tells that with K ≍log( 1"
REFERENCES,0.4451871657754011,"δ ), at least one of the solutions generated
by subbagging generalizes well with high probability."
REFERENCES,0.446524064171123,"Lemma 5. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability and γm2,N-mean-square-uniform stability as well. Assume that the loss function ℓis G-
Lipschitz with respect to its ﬁrst argument and is bounded in the range of [0, M]. Then for any α, δ ∈
(0, 1) and K ≥log(2/δ)"
REFERENCES,0.44786096256684493,"1−α , with probability at least 1 −δ over the randomness of {(Ak, Sk)}k∈[K],
the sequence {Ak(Sk)}k∈[K] generated by Algorithm 1 satisﬁes"
REFERENCES,0.44919786096256686,"min
k∈[K] |R(Ak(Sk)) −RSk(Ak(Sk))| ≲1 α "
REFERENCES,0.4505347593582888,"Gpγm2, N"
REFERENCES,0.45187165775401067,"K + Gγm, N K + M r K
N ! ."
REFERENCES,0.4532085561497326,"Proof. From Lemma 1 we have that over randomized algorithm A and data ˜S with | ˜S| = N K ,"
REFERENCES,0.45454545454545453,"EA, ˜S
h
|R(A( ˜S)) −R ˜S(A( ˜S))|
i
≤Gpγm2, N"
REFERENCES,0.45588235294117646,"K + Gγm, N K + M r K
N ."
REFERENCES,0.4572192513368984,"Since {Ak, Sk}k∈[K] are independent to each other, by Markov inequality we know that"
REFERENCES,0.4585561497326203,"P{Ak,Sk} "
REFERENCES,0.45989304812834225,"min
k∈[K] |R(Ak(Sk)) −RSk(Ak(Sk))| ≥1 α "
REFERENCES,0.4612299465240642,"Gpγm2, N"
REFERENCES,0.4625668449197861,"K + Gγm, N K + M r K
N !!"
REFERENCES,0.46390374331550804,"≤αK ≤δ,"
REFERENCES,0.46524064171123,which implies the desired bound.
REFERENCES,0.46657754010695185,Next we proceed to prove the main result in Theorem 1.
REFERENCES,0.4679144385026738,Proof of Theorem 1. Let us consider the following three events: E := (
REFERENCES,0.4692513368983957,"|R(Ak∗(Sk∗)) −RS(Ak∗(Sk∗))| ≲
1
αK "
REFERENCES,0.47058823529411764,"Gpγm2, N"
REFERENCES,0.47192513368983957,"K + Gγm, N K + M r K
N ! + M s"
REFERENCES,0.4732620320855615,K log(K/δ)
REFERENCES,0.47459893048128343,"(K −1)N ) , E1 := ("
REFERENCES,0.47593582887700536,"max
k∈[K] |R(Ak(Sk)) −RS\Sk(Ak(Sk))| ≲M s"
REFERENCES,0.4772727272727273,K log(K/δ)
REFERENCES,0.4786096256684492,"(K −1)N ) , E2 := ("
REFERENCES,0.4799465240641711,"min
k∈[K] |R(Ak(Sk)) −RSk(Ak(Sk))| ≲1 α "
REFERENCES,0.48128342245989303,"Gpγm2, N"
REFERENCES,0.48262032085561496,"K + Gγm, N K + M r K
N !) ."
REFERENCES,0.4839572192513369,Under review as a conference paper at ICLR 2022
REFERENCES,0.4852941176470588,"We can show that E ⊇E1 ∩E2. Indeed, suppose that E1 and E2 simultaneously occur. Consequently
the following inequality is valid:"
REFERENCES,0.48663101604278075,|R(Ak∗(Sk∗)) −RS(Ak∗(Sk∗))|
REFERENCES,0.4879679144385027,"=
R(Ak∗(Sk∗)) −1"
REFERENCES,0.4893048128342246,K RSk∗(Ak∗(Sk∗)) −K −1
REFERENCES,0.49064171122994654,"K
RS\Sk∗(Ak∗(Sk∗)) ≤1"
REFERENCES,0.4919786096256685,K |R(Ak∗(Sk∗)) −RSk∗(Ak∗(Sk∗))| + K −1 K
REFERENCES,0.49331550802139035,R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) ≤1 K
REFERENCES,0.4946524064171123,"RS\Sk∗(Ak∗(Sk∗)) −RSk∗(Ak∗(Sk∗))
 +
R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) ζ1= 1"
REFERENCES,0.4959893048128342,"K min
k∈[K]"
REFERENCES,0.49732620320855614,"RS\Sk(Ak(Sk)) −RSk(Ak(Sk))
 +
R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) = 1"
REFERENCES,0.49866310160427807,"K min
k∈[K]"
REFERENCES,0.5,RS\Sk(Ak(Sk)) −R(Ak(Sk)) + R(Ak(Sk)) −RSk(Ak(Sk))
REFERENCES,0.5013368983957219,"+
R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) ≤1"
REFERENCES,0.5026737967914439,"K min
k∈[K] |R(Ak(Sk)) −RSk(Ak(Sk))| + 1"
REFERENCES,0.5040106951871658,"K max
k∈[K]"
REFERENCES,0.5053475935828877,RS\Sk(Ak(Sk)) −R(Ak(Sk))
REFERENCES,0.5066844919786097,"+
R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) ≤1"
REFERENCES,0.5080213903743316,"K min
k∈[K] |R(Ak(Sk)) −RSk(Ak(Sk))| + K + 1"
REFERENCES,0.5093582887700535,"K
max
k∈[K]"
REFERENCES,0.5106951871657754,"R(Ak(Sk)) −RS\Sk(Ak(Sk)) ζ2
≲1 αK "
REFERENCES,0.5120320855614974,"Gpγm2, N"
REFERENCES,0.5133689839572193,"K + Gγm, N K + M r K
N ! + M s"
REFERENCES,0.5147058823529411,K log(K/δ)
REFERENCES,0.516042780748663,"(K −1)N ,"
REFERENCES,0.517379679144385,"where in “ζ1” we have used the deﬁnition of k∗, and “ζ2” follows from E1, E2. With leading terms
preserved in the above we can see that E occurs."
REFERENCES,0.5187165775401069,"Next we can show that PS,{Ak}(E1) ≤δ"
REFERENCES,0.5200534759358288,"2. Toward this end, let us consider the following events for
all k ∈[K]:"
REFERENCES,0.5213903743315508,"Ek
1 := ("
REFERENCES,0.5227272727272727,|R(Ak(Sk)) −RS\Sk(Ak(Sk))| ≲M s
REFERENCES,0.5240641711229946,K log(K/δ)
REFERENCES,0.5254010695187166,(K −1)N ) .
REFERENCES,0.5267379679144385,"Clearly, it is true that E1 = TK
k=1 Ek
1 . It is sufﬁcient to prove that PS,Ak
"
REFERENCES,0.5280748663101604,"Ek
1

≤
δ
2K holds for each"
REFERENCES,0.5294117647058824,"k ∈[K]. Indeed, consider the random indication function β(S, Ak) := 1Ek
1 associated with the"
REFERENCES,0.5307486631016043,"event Ek
1 . Then we have the following holds for each k ∈[K]:"
REFERENCES,0.5320855614973262,"PS,Ak
"
REFERENCES,0.5334224598930482,"Ek
1
"
REFERENCES,0.5347593582887701,"=ES,Ak [β(S, Ak)]"
REFERENCES,0.536096256684492,"=EAk,Sk

ES\Sk|Ak,Sk [β(S, Ak) | Ak, Sk]
"
REFERENCES,0.5374331550802139,"ζ1=EAk,Sk

ES\Sk [β(S, Ak) | Ak, Sk]
"
REFERENCES,0.5387700534759359,"=EAk,Sk "" PS\Sk "
REFERENCES,0.5401069518716578,|R(Ak(Sk)) −RS\Sk(Ak(Sk))| ≳M s
REFERENCES,0.5414438502673797,K log(K/δ)
REFERENCES,0.5427807486631016,(K −1)N !
REFERENCES,0.5441176470588235,"| Ak, Sk #"
REFERENCES,0.5454545454545454,"ζ2≤EAk,Sk  δ"
REFERENCES,0.5467914438502673,"2K | Ak, Sk"
REFERENCES,0.5481283422459893,"
=
δ
2K ,"
REFERENCES,0.5494652406417112,"where in “ζ1” we have used the independence between {Ak, Sk} and S \ Sk, and “ζ2” is due to
Hoeffding’s inequality conditioned on {Ak, Sk}, keeping in mind that Ak(Sk) is independent on
the data set S \ Sk of size (1 −1/K)N. It follows by union probability that"
REFERENCES,0.5508021390374331,"PS,{Ak}
 "
REFERENCES,0.5521390374331551,"E1

= PS,{Ak} K
[ k=1 Ek
1 ! ≤ K
X"
REFERENCES,0.553475935828877,"k=1
PS,Ak
"
REFERENCES,0.5548128342245989,"Ek
1

≤δ 2."
REFERENCES,0.5561497326203209,Under review as a conference paper at ICLR 2022
REFERENCES,0.5574866310160428,"Further, from the part(b) of Lemma 5 we have PS,{Ak}( ¯E2) ≤δ"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5588235294117647,"2. Combining this and the preceding
bound yields"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5601604278074866,"PS,{Ak} (E) ≥PS,{Ak} (E1 ∩E2) ≥1 −PS,{Ak}
  ¯E1

−PS,{Ak}( ¯E2) ≥1 −δ 2 −δ"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5614973262032086,2 = 1 −δ.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5628342245989305,This implies the desired result in part(b) as K/(K −1) ≤2 for K ≥2.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5641711229946524,"B.3
PROOF OF THEOREM 2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5655080213903744,"We ﬁrst present the following simple lemma about the in-expectation risk bounds of a randomized
algorithm which will be used in our analysis.
Lemma 6. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability. Assume that the loss function ℓis G-Lipschitz with respect to its ﬁrst argument. Then we
have
EA,S [R(A(S)) −R(w∗)] ≤Gγm,N + ∆opt."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5668449197860963,Proof. By risk decomposition we can show that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5681818181818182,"EA,S [R(A(S)) −R(w∗)]
=EA,S [R(A(S)) −RS(A(S)) + RS(A(S)) −RS(w∗) + RS(w∗) −R(w∗)]
≤|EA,S [R(A(S)) −RS(A(S))]| + ∆opt
≤Gγm,N + ∆opt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.56951871657754,"where in the last inequality we have used the on-average generalization bound by Hardt et al. (2016,
Theorem 2.2)."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.570855614973262,"Lemma 7. Suppose that a randomized learning algorithm A : ZN 7→W has γm,N-mean-uniform
stability. Assume that the loss function ℓis G-Lipschitz with respect to its ﬁrst argument. Then
for any α, δ ∈(0, 1) and K ≥
log(2/δ)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5721925133689839,"1−α , with probability at least 1 −δ over the randomness of
{(Ak, Sk)}k∈[K], the sequence {Ak(Sk)}k∈[K] generated by Algorithm 1 with the modiﬁed output
satisﬁes"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5735294117647058,"min
k∈[K] R(Ak(Sk)) −R(w∗) ≲1 α"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5748663101604278,"
Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5762032085561497,"K + ∆opt

."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5775401069518716,"Proof. Recall the modiﬁed output Ak∗(Sk∗) where k∗= arg mink∈[K] RS\Sk(Ak(Sk)). From
Lemma 6 we have that over randomized algorithm A and data ˜S with | ˜S| = N K ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5788770053475936,"EA, ˜S
h
R(A( ˜S)) −R(w∗)
i
≤Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5802139037433155,K + ∆opt.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5815508021390374,"Since {Ak, Sk}k∈[K] are independent to each other, by Markov inequality we know that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5828877005347594,"P{Ak,Sk}"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5842245989304813,"
min
k∈[K] R(Ak(Sk)) −R(w∗) ≥1 α"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5855614973262032,"
Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5868983957219251,"K + ∆opt

≤αK ≤δ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5882352941176471,which implies the desired bound in part(b).
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.589572192513369,Next we proceed to prove the main result in Theorem 2.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5909090909090909,Proof of Theorem 2. Let us consider the following three events: E := (
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5922459893048129,|R(Ak∗(Sk∗)) −RS(Ak∗(Sk∗))| ≲1 α
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5935828877005348,"
Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5949197860962567,"K + ∆opt

+ M s"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5962566844919787,K log(K/δ)
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5975935828877005,"(K −1)N ) , E1 := ("
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.5989304812834224,"max
k∈[K] |R(Ak(Sk)) −RS\Sk(Ak(Sk))| ≲M s"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6002673796791443,K log(K/δ)
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6016042780748663,"(K −1)N ) ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6029411764705882,"E2 :=

min
k∈[K] R(Ak(Sk)) −R(w∗) ≲1 α"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6042780748663101,"
Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6056149732620321,"K + ∆opt

."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.606951871657754,Under review as a conference paper at ICLR 2022
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6082887700534759,"Similarly, we show that E ⊇E1 ∩E2. Indeed, suppose that E1 and E2 simultaneously occur. Conse-
quently the following inequality is valid:"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6096256684491979,"R(Ak∗(Sk∗)) −R(w∗)
=R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) + RS\Sk∗(Ak∗(Sk∗)) −R(w∗)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6109625668449198,"ζ1=R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) + min
k∈[K] RS\Sk(Ak(Sk)) −R(w∗)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6122994652406417,"=R(Ak∗(Sk∗)) −RS\Sk∗(Ak∗(Sk∗)) + min
k∈[K]"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6136363636363636,"
RS\Sk(Ak(Sk)) −R(Ak(Sk)) + R(Ak(Sk)) −R(w∗)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6149732620320856,"≤min
k∈[K](R(Ak(Sk)) −R(w∗)) + 2 max
k∈[K]"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6163101604278075,"RS\Sk(Ak(Sk)) −R(Ak(Sk)) ζ2
≲1 α"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6176470588235294,"
Gγm, N"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6189839572192514,"K + ∆opt

+ M s"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6203208556149733,K log(K/δ)
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6216577540106952,"(K −1)N ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6229946524064172,"where in “ζ1” we have used the deﬁnition of k∗, and “ζ2” follows from E1, E2. With leading terms
preserved in the above we can see that E occurs."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6243315508021391,"Based on the same proof argument as that of the part(b) of Theorem 1 we can show that
PS,{Ak}(E1) ≤
δ
2. Further, from the part(b) of Lemma 7 we have PS,{Ak}( ¯E2) ≤
δ
2. Combin-
ing this and the preceding bound yields"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6256684491978609,"PS,{Ak} (E) ≥PS,{Ak} (E1 ∩E2) ≥1 −PS,{Ak}
  ¯E1

−PS,{Ak}( ¯E2) ≥1 −δ 2 −δ"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6270053475935828,2 = 1 −δ.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6283422459893048,This implies the desired result in part(b) as K/(K −1) ≤2 for K ≥2.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6296791443850267,"C
PROOFS FOR THE RESULTS IN SECTION 3"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6310160427807486,"In this section, we present the technical proofs for the main results stated in Section 3."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6323529411764706,"C.1
PROOF OF COROLLARY 1"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6336898395721925,"We begin with presenting and proving the following lemma that gives the mean(-square)-uniform
stability bounds for ASGD-w on convex and smooth loss functions such as logistic loss."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6350267379679144,"Lemma 8. Suppose that the loss function is ℓ(·; ·) is convex, G-Lipschitz and L-smooth with respect
to its ﬁrst argument. Assume that ηt ≤2/L for all t ≥1. Then ASGD-w has mean-uniform stability
such that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6363636363636364,"sup
S .=S′ EASGD-w [∥ASGD-w(S) −ASGD-w(S′)∥] ≤2G N X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6377005347593583,"t∈[T ]
ηt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6390374331550802,and has mean-square-uniform stability such that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6403743315508021,"sup
S .=S′ EASGD-w

∥ASGD-w(S) −ASGD-w(S′)∥2
≤40G2 N  
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6417112299465241,"t=1
η2
t + 1 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.643048128342246,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6443850267379679,"Proof. The ﬁrst mean-uniform stability bound can be straightforwardly derived based on the argu-
ment of Hardt et al. (2016, Theorem 3.7). We focus on proving the second mean-square-uniform
stability bound. For any pair of S, S′, let us deﬁne the sequences {wt}t∈[T ] and {w′
t}t∈[T ] that are
respectively generated over S and S′ via ASGD-w via sample path ξ = {ξt}t∈[T ]. Note by assumption
that w0 = w′
0. We distinguish the following two complementary cases."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6457219251336899,"Case I: zξt = z′
ξt. In this case, by invoking Lemma 2 we immediately get"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6470588235294118,"∥wt −w′
t∥2 =∥ΠW(wt−1 −ηt∇wℓ(wt−1; zξt)) −ΠW(w′
t−1 −ηt∇wℓ(w′
t−1; z′
ξt))∥2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6483957219251337,"≤∥wt−1 −ηt∇wℓ(wt−1; zξt) −(w′
t−1 −ηt∇wℓ(w′
t−1; z′
ξt))∥2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6497326203208557,"≤∥wt−1 −w′
t−1∥2. (A.3)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6510695187165776,Under review as a conference paper at ICLR 2022
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6524064171122995,"Case II: zξt ̸= z′
ξt. In this case, we have"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6537433155080213,"∥wt −w′
t∥2 =∥ΠW(wt−1 −ηt∇f(w)) −ΠW(w′ −α∇f(w′))∥2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6550802139037433,"≤∥wt−1 −ηt∇wℓ(wt−1; zξt) −(w′
t−1 −ηt∇wℓ(w′
t−1; z′
ξt))∥2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6564171122994652,"≤
 
∥wt−1 −w′
t−1∥+ ηt(∥∇wℓ(wt−1; zξt)∥+ ∥∇wℓ(w′
t−1; z′
ξt)∥)
2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6577540106951871,"≤
 
∥wt−1 −w′
t−1∥+ 2Gηt
2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6590909090909091,"=∥wt−1 −w′
t−1∥2 + 4Gηt∥wt−1 −w′
t−1∥+ 4G2η2
t , (A.4)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.660427807486631,where in the last but inequality we have used ℓ(·; ·) is G-Lipschitz with respect to its ﬁrst argument.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6617647058823529,"Let βt = βt(S, S′, ξ) := 1{zξt̸=z′
ξt} be the random indication function associated with event zξt ̸="
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6631016042780749,"z′
ξt. Based on the recursion forms equation A.3 and equation A.4 and the condition w0 = w′
0 we
can show that for all t ≥1,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6644385026737968,"∥wt −w′
t∥2 ≤ t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6657754010695187,"τ=1
4Gβτητ∥wτ−1 −w′
τ−1∥+ t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6671122994652406,"τ=1
4G2βτη2
τ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6684491978609626,Then applying Lemma 3 with simple algebraic manipulation yields
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6697860962566845,"∥wt −w′
t∥2 ≤8G2  
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6711229946524064,"τ=1
βτη2
τ + 4 t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6724598930481284,"τ=1
βτητ !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6737967914438503,"Since by assumption S and S′ differ only in a single element, under the scheme of uniform sampling
without replacement, we can see that βt(S, S′, ξ) ∼Bernoulli(1/N) and {βt(S, S′, ξ)} is an
i.i.d. sequence of Bernoulli random variables. It follows that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6751336898395722,"Eξ[t]

∥wt −w′
t∥2 ≤8G2  
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6764705882352942,"τ=1
Eξ[t][βτ]η2
τ + 4Eξ[t]   t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6778074866310161,"τ=1
βτητ !2    =8G2  
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.679144385026738,"τ=1
Eξ[t][βτ + 4β2
τ]η2
τ + 4
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.68048128342246,"τ̸=τ ′
1Eξ[t] [βτβτ ′] ητητ ′   =8G2  5 N t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6818181818181818,"τ=1
η2
τ + 4 N 2 t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6831550802139037,"τ=1
ητ !2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6844919786096256,"≤40G2  1 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6858288770053476,"τ=1
η2
τ + 1 N 2 T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6871657754010695,"τ=1
ητ !2 ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6885026737967914,"where we have used Eξt[βt] = Eξt[β2
t ] = 1"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6898395721925134,N . The convexity of squared Euclidean norm leads to
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6911764705882353,"Eξ

∥¯wT −¯w′
T ∥2
≤"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6925133689839572,"PT
t=1 Eξ[t]

∥wt −w′
t∥2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6938502673796791,"T
≤40G2  1 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6951871657754011,"t=1
η2
t + 1 N 2 T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.696524064171123,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6978609625668449,"Note that the above holds for any S .= S′, i.e.,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.6991978609625669,"sup
S .=S′ Eξ

∥¯wT −¯w′
T ∥2
≤40G2 N  
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7005347593582888,"t=1
η2
t + 1 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7018716577540107,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7032085561497327,The proof is concluded.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7045454545454546,"With Lemma 8 in place, we are ready to prove Corollary 1."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7058823529411765,"Proof of Corollary 1. From Lemma 8 we know that ASGD-w has mean-uniform stability with param-
eter
γm,N = 2G N X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7072192513368984,"t∈[T ]
ηt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7085561497326203,Under review as a conference paper at ICLR 2022
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7098930481283422,and mean-square-uniform stability with parameter
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7112299465240641,"γm2,N = 40G2 N  
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7125668449197861,"t=1
η2
t + 1 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.713903743315508,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7152406417112299,The desired results then follow immediately via invoking Theorem 1 with α = 1/2.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7165775401069518,"C.2
PROOF OF COROLLARY 2"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7179144385026738,"We ﬁrst establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the case
of non-smooth convex loss.
Lemma 9. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its ﬁrst
argument. Then ASGD-w has mean-uniform stability such that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7192513368983957,"sup
S .=S′ EASGD-w [∥ASGD-w(S) −ASGD-w(S′)∥] ≤2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7205882352941176,"v
u
u
t T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7219251336898396,"t=1
η2
t + 4G N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7232620320855615,"t=1
ηt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7245989304812834,and has mean-square-uniform stability such that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7259358288770054,"sup
S .=S′ EASGD-w

∥ASGD-w(S) −ASGD-w(S′)∥2
≤40G2
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7272727272727273,"t=1
η2
t + 32G2 N 2 T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7286096256684492,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7299465240641712,"Proof. Let us deﬁne the sequences {wt}t∈[T ] and {w′
t}t∈[T ] that are respectively generated over S
and S′ via ASGD-w via sample path ξ = {ξt}t∈[T ]. Suppose that S .= S′ and consider a hitting time
variable t0 = inf{t : zξt ̸= z′
ξt}. Let βt = βt(S, S′, ξ) := 1{zξt̸=z′
ξt} be the random indication"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7312834224598931,"function associated with event zξt ̸= z′
ξt. Conditioned on t0, it has been shown by Bassily et al.
(2020, Lemma 3.1) that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.732620320855615,"∥wt −w′
t∥≤2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.733957219251337,"v
u
u
t t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7352941176470589,"τ=t0
η2τ + 4G t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7366310160427807,"τ=t0+1
βτητ ≤2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7379679144385026,"v
u
u
t t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7393048128342246,"τ=1
η2τ + 4G t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7406417112299465,"τ=1
βτητ.
(A.5)"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7419786096256684,Then we can show the following for all t ≤T
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7433155080213903,"Eξ[t] [∥wt −w′
t∥] ≤2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7446524064171123,"v
u
u
t t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7459893048128342,"τ=1
η2τ + 4G N t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7473262032085561,"τ=1
ητ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7486631016042781,"where we have used the fact that {βt} is an i.i.d. sequence of Bernoulli(1/N) random variables.
The convexity of Euclidean norm leads to"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.75,"Eξ[T ] [∥¯wT −¯w′
T ∥] ≤"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7513368983957219,"PT
t=1 Eξ[t] [∥wt −w′
t∥]
T
≤2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7526737967914439,"v
u
u
t T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7540106951871658,"t=1
η2
t + 4G N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7553475935828877,"t=1
ηt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7566844919786097,"which is the ﬁrst desired bound. Similarly, based on the square of the bound equation A.5 we can
show that"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7580213903743316,"Eξ[t]

∥wt −w′
t∥2
≤Eξ[t] "
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7593582887700535,"8G2
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7606951871657754,"τ=1
η2
τ + 32G2
 
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7620320855614974,"τ=1
βτητ !2 "
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7633689839572193,"=8G2
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7647058823529411,"τ=1
η2
τ + 32G2Eξ[t]  
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.766042780748663,"τ=1
β2
τη2
τ +
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.767379679144385,"τ̸=τ ′
βτβτ ′ητητ ′  "
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7687165775401069,"=8G2
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7700534759358288,"τ=1
η2
τ + 32G2  1 N t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7713903743315508,"τ=1
η2
τ + 1 N 2
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7727272727272727,"τ̸=τ ′
ητητ ′  "
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7740641711229946,"≤40G2
t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7754010695187166,"τ=1
η2
τ + 32G2 N 2 t
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7767379679144385,"τ=1
ητ !2 ,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7780748663101604,Under review as a conference paper at ICLR 2022
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7794117647058824,"where we have used Eξt[βt] = Eξt[β2
t ] = 1"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7807486631016043,N . It follows directly from the convexity of loss that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7820855614973262,"Eξ[T ]

∥¯wT −¯w′
T ∥2
≤40G2
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7834224598930482,"t=1
η2
t + 32G2 N 2 T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7847593582887701,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.786096256684492,The proof is concluded.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7874331550802139,"Equipped with Lemma 9, we are now in the position to prove Corollary 2."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7887700534759359,"Proof of Corollary 2. From Lemma 9 we know that ASGD-w with non-smooth convex loss has
mean(-square)-uniform stability with parameters"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7901069518716578,"γm,N = 2G"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7914438502673797,"v
u
u
t T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7927807486631016,"t=1
η2
t + 4G N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7941176470588235,"t=1
ηt,
γm2,N = 40G2
T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7954545454545454,"t=1
η2
t + 32G2 N 2 T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7967914438502673,"t=1
ηt !2 ."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7981283422459893,The desired results then follow immediately via invoking Theorem 1 with α = 1/2.
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.7994652406417112,"C.3
PROOF OF COROLLARY 3"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8008021390374331,"We ﬁrst establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the
considered non-convex regime."
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8021390374331551,"Lemma 10. Suppose that the loss function is ℓ(·; ·) is G-Lipschitz and L-smooth with respect to its
ﬁrst argument. Consider ηt ≤1/L. Let"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.803475935828877,"ut := η2
t + 2ηt t−1
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8048128342245989,"τ=1
exp  L t−1
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8061497326203209,"i=τ+1
ηi ! ητ"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8074866310160428,for all t ≥1. Then ASGD-w has mean-uniform stability such that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8088235294117647,"sup
S .=S′ EASGD-w [∥ASGD-w(S) −ASGD-w(S′)∥] ≤2G N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8101604278074866,"t=1
exp  L T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8114973262032086,"τ=t+1
ητ ! ηt,"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8128342245989305,and has mean-square-uniform stability such that
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8141711229946524,"sup
S .=S′ EASGD-w

∥ASGD-w(S) −ASGD-w(S′)∥2
≤4G2 N T
X"
"COMBINING THIS AND THE PRECEDING
BOUND YIELDS",0.8155080213903744,"t=1
exp "
L,0.8168449197860963,"3L T
X"
L,0.8181818181818182,"τ=t+1
ητ ! ut."
L,0.81951871657754,"Proof. Let us deﬁne the sequences {wt}t∈[T ] and {w′
t}t∈[T ] that are respectively generated over
S and S′ via ASGD-w via sample path ξ = {ξt}t∈[T ]. Suppose that S
.= S′. Let us consider
∆t := Eξ[t] [∥wt −w′
t∥]. Then based on the arguments of Hardt et al. (2016, Theorem 3.8) we
know that with probability 1−1"
L,0.820855614973262,"N over ξt, ∥wt −w′
t∥≤(1+ηtL)∥wt−1 −w′
t−1∥, and ∥wt −w′
t∥≤
∥wt−1 −w′
t−1∥+ 2Gηt with probability 1"
L,0.8221925133689839,N . Therefore we have
L,0.8235294117647058,"∆t ≤

1 −1 N"
L,0.8248663101604278,"
(1 + ηtL)∆t−1 + 1"
L,0.8262032085561497,N (∆t−1 + 2Gηt)
L,0.8275401069518716,"=

1 −1 N"
L,0.8288770053475936,"
(1 + ηtL) + 1 N"
L,0.8302139037433155,"
∆t−1 + 2Gηt N"
L,0.8315508021390374,"=

1 +

1 −1 N"
L,0.8328877005347594,"
ηtL

∆t−1 + 2Gηt N"
L,0.8342245989304813,"≤exp

1 −1 N"
L,0.8355614973262032,"
ηtL

∆t−1 + 2Gηt N"
L,0.8368983957219251,"≤exp (ηtL) ∆t−1 + 2Gηt N
,"
L,0.8382352941176471,Under review as a conference paper at ICLR 2022
L,0.839572192513369,"where we have used 1 + x ≤exp(x). Then we can unwind the above recursion form to obtain that
for all t ≥1, ∆t ≤ t
X τ=1 (
tY"
L,0.8409090909090909,"i=τ+1
exp (ηiL)"
L,0.8422459893048129,")
2Gητ"
L,0.8435828877005348,"N
= 2G N t
X"
L,0.8449197860962567,"τ=1
exp  L t
X"
L,0.8462566844919787,"i=τ+1
ηi !"
L,0.8475935828877005,"ητ,
(A.6)"
L,0.8489304812834224,where we have used ∆0 = 0. The convexity of Euclidean norm leads to
L,0.8502673796791443,"Eξ[T ] [∥¯wT −¯w′
T ∥] ≤"
L,0.8516042780748663,"PT
t=1 Eξ[t] [∥wt −w′
t∥]
T
≤2G N T
X"
L,0.8529411764705882,"t=1
exp  L T
X"
L,0.8542780748663101,"τ=t+1
ητ ! ηt,"
L,0.8556149732620321,which immediately implies the ﬁrst desired bound as it holds for all S .= S′.
L,0.856951871657754,"To show the mean-square-uniform stability bound, let us consider ˜∆t := Eξ[t]

∥wt −w′
t∥2
. Then
we can verify that with probability 1 −1"
L,0.8582887700534759,"N over ξt, ∥wt −w′
t∥2 ≤(1 + ηtL)2∥wt−1 −w′
t−1∥2, and
with probability 1 N ,"
L,0.8596256684491979,"∥wt −w′
t∥2 ≤(∥wt−1 −w′
t−1∥+ 2Gηt)2 = ∥wt−1 −w′
t−1∥2 + 4Gηt∥wt−1 −w′
t−1∥+ 4G2η2
t ."
L,0.8609625668449198,Therefore we have
L,0.8622994652406417,"˜∆t ≤

1 −1 N"
L,0.8636363636363636,"
(1 + ηtL)2 ˜∆t−1 + 1 N"
L,0.8649732620320856,"
˜∆t−1 + 4Gηt∆t−1 + 4G2η2
t
"
L,0.8663101604278075,"≤

1 −1 N"
L,0.8676470588235294,"
(1 + ηtL)2 + 1 N"
L,0.8689839572192514,"
˜∆t−1 + 4G2 N "
L,0.8703208556149733,"




η2
t + 2ηt t−1
X"
L,0.8716577540106952,"τ=1
exp  L t−1
X"
L,0.8729946524064172,"i=τ+1
ηi ! ητ"
L,0.8743315508021391,"|
{z
}
ut "
L,0.8756684491978609,"



"
L,0.8770053475935828,"=

1 +

1 −1 N"
L,0.8783422459893048,"
(2ηtL + η2
t L2)

˜∆t−1 + 4G2ut N"
L,0.8796791443850267,"≤exp

1 −1 N"
L,0.8810160427807486,"
(2ηtL + η2
t L2)

˜∆t−1 + 4G2ut N"
L,0.8823529411764706,"≤exp
 
2ηtL + η2
t L2 ˜∆t−1 + 4G2ut N
,"
L,0.8836898395721925,"where in the second inequality we have used the bound equation A.6 on ∆t. Recall that ˜∆0 = 0.
Then we can unwind the above recursion form to obtain"
L,0.8850267379679144,"˜∆t ≤4G2 N t
X τ=1 (
tY"
L,0.8863636363636364,"i=τ+1
exp
 
2ηiL + η2
i L2
)"
L,0.8877005347593583,"uτ ≤4G2 N t
X"
L,0.8890374331550802,"τ=1
exp "
L,0.8903743315508021,"3L t
X"
L,0.8917112299465241,"i=τ+1
ηi ! uτ,"
L,0.893048128342246,where we have used ηt ≤1/L. It follows immediately from the convexity that
L,0.8943850267379679,"Eξ[T ]

∥¯wT −¯w′
T ∥2
≤"
L,0.8957219251336899,"PT
t=1 Eξ[t]

∥wt −w′
t∥2"
L,0.8970588235294118,"T
≤4G2 N T
X"
L,0.8983957219251337,"t=1
exp "
L,0.8997326203208557,"3L T
X"
L,0.9010695187165776,"τ=t+1
ητ ! ut,"
L,0.9024064171122995,which is the second desired bound. The proof is completed.
L,0.9037433155080213,"With Lemma 10 in place, we proceed to prove the main result in Corollary 3."
L,0.9050802139037433,"Proof of Corollary 3. From Lemma 10 we know that ASGD-w with smooth non-convex loss has
mean(-square)-uniform stability with parameters"
L,0.9064171122994652,"γm,N = 2G N T
X"
L,0.9077540106951871,"t=1
exp  L T
X"
L,0.9090909090909091,"τ=t+1
ητ !"
L,0.910427807486631,"ηt,
γm2,N = 4G2 N T
X"
L,0.9117647058823529,"t=1
exp "
L,0.9131016042780749,"3L T
X"
L,0.9144385026737968,"τ=t+1
ητ ! ut."
L,0.9157754010695187,The desired results then follow immediately via invoking Theorem 1 with α = 1/2.
L,0.9171122994652406,Under review as a conference paper at ICLR 2022
L,0.9184491978609626,Algorithm 3: SGD via Without-Replacement Sampling (ASGD-wo)
L,0.9197860962566845,"Input : Data set S = {zi}i∈[N]
i.i.d.
∼DN, step-sizes {ηt}t≥1, #iterations T, initialization w0.
Output: ¯wT = 1 T
P"
L,0.9211229946524064,"t∈[T ] wt.
for t = 1, 2, ..., T do"
L,0.9224598930481284,"Uniformly randomly sample an index ξt ∈[N] with or without replacement;
Compute wt = ΠW (wt−1 −ηt∇wℓ(wt−1; zξt)).
end"
L,0.9237967914438503,"D
AUGMENTED RESULTS FOR SGD UNDER WITHOUT-REPLACEMENT
SAMPLING"
L,0.9251336898395722,"In this section, we further consider applying our main results in Theorem 1 to the variant of SGD
under without-replacement sampling (ASGD-wo), as is outlined in Algorithm 3. For the sake of sim-
plicity and readability, we only consider single-epoch processing with T ≤N, and we focus on the
case where the loss is convex but non-smooth. The extensions of our analysis to multi-epoch pro-
cessing, i.e., T ≤rN for some integer r ≥1, and to convex or non-convex smooth loss functions
are more or less straightforward and thus are omit."
L,0.9264705882352942,"We start by establishing the following lemma on the mean(-square)-uniform stability of ASGD-wo
which can be easily proved based on the result from Bassily et al. (2020, Lemma 3.1).
Lemma 11. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its ﬁrst
argument. Consider T ≤N. Then ASGD-wo has mean-uniform stability such that"
L,0.9278074866310161,"sup
S .=S′ EASGD-wo [∥ASGD-wo(S) −ASGD-wo(S′)∥] ≤2G N T
X t0=1"
L,0.929144385026738,"v
u
u
t T
X"
L,0.93048128342246,"t=t0
η2
t ,"
L,0.9318181818181818,and has mean-square-uniform stability such that
L,0.9331550802139037,"sup
S .=S′ EASGD-wo

∥ASGD-wo(S) −ASGD-wo(S′)∥2
≤4G2 N T
X t0=1 T
X"
L,0.9344919786096256,"t=t0
η2
t ."
L,0.9358288770053476,"Proof. Let ¯wT (S, ξ) and ¯wT (S′, ξ) respectively be the output generated over S = {zi}i∈[N] and
S′ = {z′
i}i∈[N] by ASGD-wo via sample path ξ = {ξt}t∈[T ]. Recall that T ≤N. Let us deﬁne a
hitting time variable t0 = inf{t : zξt ̸= z′
ξt}. Since S .= S′, the uniform randomness of ξt implies
that
P (t0 = j) = 1"
L,0.9371657754010695,"N ,
j ∈[N]."
L,0.9385026737967914,"Given t ∈[T], it follows from Bassily et al. (2020, Lemma 3.1) that"
L,0.9398395721925134,"∥wt −w′
t∥2 ≤4G2
t
X"
L,0.9411764705882353,"τ=t0
η2
τ."
L,0.9425133689839572,Then we have
L,0.9438502673796791,"Eξ[t]

∥wt −w′
t∥2
≤4G2 N t
X t0=1 t
X"
L,0.9451871657754011,"τ=t0
η2
τ ≤4G2 N T
X t0=1 T
X"
L,0.946524064171123,"τ=t0
η2
τ."
L,0.9478609625668449,The convexity of squared Euclidean norm leads to
L,0.9491978609625669,"Eξ[T ]

∥¯wT −¯w′
T ∥2
≤"
L,0.9505347593582888,"PT
t=1 Eξ[t]

∥wt −w′
t∥2"
L,0.9518716577540107,"T
≤4G2 N T
X t0=1 T
X"
L,0.9532085561497327,"t=t0
η2
t ."
L,0.9545454545454546,Similarly we can show
L,0.9558823529411765,"Eξ[T ] [∥¯wT −¯w′
T ∥] ≤2G N T
X t0=1"
L,0.9572192513368984,"v
u
u
t T
X"
L,0.9585561497326203,"t=t0
η2
t ."
L,0.9598930481283422,The proof is completed.
L,0.9612299465240641,Under review as a conference paper at ICLR 2022
L,0.9625668449197861,"The following result is a direct consequence of Theorem 1 when invoking Algorithm 1 to ASGD-wo
with non-smooth convex loss."
L,0.963903743315508,"Corollary 5. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its
ﬁrst argument, and is bounded in the range of [0, M]. Consider Algorithm 1 speciﬁed to ASGD-wo
with T ≤N. Then for any δ ∈(0, 1) and K ≥2 log( 4"
L,0.9652406417112299,"δ ), with probability at least 1 −δ over the
randomness of S and {ASGD-wo,k}k∈[K], the output of ASGD-wo satisﬁes"
L,0.9665775401069518,"|R(ASGD-wo,k∗(Sk∗)) −RS(ASGD-wo,k∗(Sk∗))| ≲G2"
L,0.9679144385026738,"v
u
u
t 1 N T
X t0=1 T
X"
L,0.9692513368983957,"t=t0
η2
t + G2 N T
X t0=1"
L,0.9705882352941176,"v
u
u
t T
X"
L,0.9719251336898396,"t=t0
η2
t + M r"
L,0.9732620320855615,"log(K/δ) N
."
L,0.9745989304812834,Proof. From Lemma 11 we know that ASGD-wo has mean(-square)-uniform stability with parameters
L,0.9759358288770054,"γm,N = 2G N T
X t0=1"
L,0.9772727272727273,"v
u
u
t T
X"
L,0.9786096256684492,"t=t0
η2
t ,
γm2,N = 4G2 N T
X t0=1 T
X"
L,0.9799465240641712,"t=t0
η2
t ."
L,0.9812834224598931,The results then follow immediately via invoking Theorem 1 with α = 1/2.
L,0.982620320855615,"Remark 9. Specially for constant learning rates ηt ≡η, Corollary 5 admits a high probability"
L,0.983957219251337,"generalization bound of scale O

ηT
√"
L,0.9852941176470589,"N + η T
√"
L,0.9866310160427807,"T
N
+
q"
L,0.9879679144385026,log(1/δ) N
L,0.9893048128342246,"
. For general time varying learning"
L,0.9906417112299465,"rates, our bound in Corollary 5 still holds with high probability. For example, when ηt ∝1"
L,0.9919786096256684,"t , the"
L,0.9933155080213903,"generalization bound scales as O
q"
L,0.9946524064171123,"log(T ) N
+
√"
L,0.9959893048128342,"T
N +
q"
L,0.9973262032085561,log(1/δ) N
L,0.9986631016042781,"
with high probability."
