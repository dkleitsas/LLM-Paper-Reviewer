Section,Section Appearance Order,Paragraph
SENSETIME RESEARCH,0.0,"1SenseTime Research
2Shanghai AI Laboratory
3Australian National University
4Northwestern Polytechnical University
5The University of Hong Kong
{lastnamefirstname}@sensetime.com,lpk@cs.hku.hk"
ABSTRACT,0.0036231884057971015,ABSTRACT
ABSTRACT,0.007246376811594203,"Transformer has shown great successes in natural language processing, computer
vision, and audio processing. As one of its core components, the softmax atten-
tion helps to capture long-range dependencies yet prohibits its scale-up due to the
quadratic space and time complexity to the sequence length. Kernel methods are
often adopted to reduce the complexity by approximating the softmax operator.
Nevertheless, due to the approximation errors, their performances vary in differ-
ent tasks/corpus and suffer crucial performance drops when compared with the
vanilla softmax attention. In this paper, we propose a linear transformer called
COSFORMER that can achieve comparable or better accuracy to the vanilla trans-
former in both casual and cross attentions.
COSFORMER is based on two key
properties of softmax attention: i). non-negativeness of the attention matrix; ii).
a non-linear re-weighting scheme that can concentrate the distribution of the at-
tention matrix. As its linear substitute, COSFORMER fulﬁlls these properties with
a linear operator and a cosine-based distance re-weighting mechanism. Extensive
experiments on language modeling and text understanding tasks demonstrate the
effectiveness of our method. We further examine our method on long sequences
and achieve state-of-the-art performance on the Long-Range Arena benchmark.
The source code is available at COSFORMER ."
INTRODUCTION,0.010869565217391304,"1
INTRODUCTION"
INTRODUCTION,0.014492753623188406,"20
30
40
50
60
70
Speed (examples per sec) 46 48 50 52 54 56"
INTRODUCTION,0.018115942028985508,Long-Range Arena Score
INTRODUCTION,0.021739130434782608,BigBird
INTRODUCTION,0.025362318840579712,Linear Transformer
INTRODUCTION,0.028985507246376812,Linformer
INTRODUCTION,0.03260869565217391,Local attention
INTRODUCTION,0.036231884057971016,Longformer
INTRODUCTION,0.03985507246376811,"Performer
Reformer"
INTRODUCTION,0.043478260869565216,cosFormer
INTRODUCTION,0.04710144927536232,Sinkhorn Transformer
INTRODUCTION,0.050724637681159424,Sparse Transformer
INTRODUCTION,0.05434782608695652,Synthesizer
INTRODUCTION,0.057971014492753624,Transformer
INTRODUCTION,0.06159420289855073,"Figure 1: Performance (y axis), speed (x axis), and memory footprint (circle sizes) of efﬁcient transformers
on the Long-Range Arena benchmark. The proposed COSFORMER achieves an all-around supremacy over
competing methods in the top left quadrant."
INTRODUCTION,0.06521739130434782,"With years of development, the transformer model (Vaswani et al., 2017) and its variants (Zaheer
et al., 2020; Wang et al., 2020; Tay et al., 2020a) have been successfully adapted to three most
popular artiﬁcial intelligence (AI) ﬁelds: i.e., natural language processing (Devlin et al., 2019;
Liu et al., 2019), computer vision (Dosovitskiy et al., 2020; Carion et al., 2020; Liu et al., 2021)
and audio processing (Schneider et al., 2019; Baevski et al., 2020). Compared with conventional"
INTRODUCTION,0.06884057971014493,∗Indicates the corresponding author. † Indicates equal contribution.
INTRODUCTION,0.07246376811594203,Published as a conference paper at ICLR 2022
INTRODUCTION,0.07608695652173914,"recurrent (Hochreiter & Schmidhuber, 1997) and convolutional architectures (He et al., 2016),
transformer-based architectures are generally more scalable to data volumes (Brown et al., 2020)
and stronger in capturing global information with less inductive bias, thus excelling on many tasks."
INTRODUCTION,0.07971014492753623,"Dot-product attention with softmax normalization is the cornerstone of the transformer to capture
long-range dependencies. However, its quadratic space and time complexity with regard to the
length of the sequence make its computational overhead prohibitive, especially for long inputs. To
address this issue, numerous methods are proposed recently, such as the sparse attention matrix (Za-
heer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Kitaev et al., 2019; Child et al., 2019),low-
rank representations (Wang et al., 2020) or kernel-based methods (Peng et al., 2020; Choromanski
et al., 2020; Katharopoulos et al., 2020), among many others. These methods achieve reduced
computational complexity with comparable performances when compared with the vanilla attention
architecture on several selected tasks or corpus."
INTRODUCTION,0.08333333333333333,"However, the improved efﬁciency is usually achieved via introducing additional yet often impracti-
cal assumptions on the attention matrix (Wang et al., 2020) or with valid approximation of softmax
operation only within constrained theoretical bounds (Choromanski et al., 2020; Peng et al., 2020)
Therefore, when their assumptions are unsatisﬁed or when approximation errors get accumulated,
these methods may not always be advantageous over the vanilla architecture (Narang et al., 2021).
Consequently, performance deﬁciencies in a broad application spectrum are often observed in these
transformer variants, especially those with linear complexity. For example, the Performer (Choro-
manski et al., 2020), RFA (Peng et al., 2020) and Reformer (Kitaev et al., 2019) show less satis-
factory performance on the GLUE benchmark (Wang et al., 2018) when compared with the vanilla
architecture as suggested in our preliminary experiments (Tab. 2). Furthermore, many of these afore-
mentioned methods are not applicable to casual attentions, which are critical for auto-regressive
training. For example, techniques proposed in Linformer (Wang et al., 2020) and BigBird (Zaheer
et al., 2020) are speciﬁc to cross attentions."
INTRODUCTION,0.08695652173913043,"Since the softmax operator appears to be the main hurdle while efﬁcient yet accurate approximation
to softmax is difﬁcult to achieve, one question naturally arises: “Can we replace the softmax opera-
tor with a linear function instead, while maintaining its key properties?”. By digging into the soft-
max attention, we ﬁnd two key properties that affect its empirical performance: (i) elements in the
attention matrix are non-negative (Tsai et al., 2019; Katharopoulos et al., 2020); (ii) the non-linear
re-weighting scheme acts as a stabilizer for the attention weights (Titsias, 2016; Gao & Pavel, 2017;
Jang et al., 2016). These ﬁndings reveal some new insights of the current approaches. For example,
the linear transformer (Katharopoulos et al., 2020) achieves property (i) using an exponential linear
unit (Clevert et al., 2016) activation function. However, due to lack of the re-weighting scheme, it
underperforms other efﬁcient transformer variants on the Long-Range Arena benchmark as shown
in Figure 1 as well as the language modeling task (Table 2) based on our controlled experiments."
INTRODUCTION,0.09057971014492754,"In this paper, we propose a new variant of linear transformer called COSFORMER that satisﬁes both
of the above properties. Speciﬁcally, we enforce the non-negative property by passing the features
to a ReLU (Agarap, 2018) activation function before computing the similarity scores. In this way,
we encourage the model to avoid aggregating negatively-correlated contextual information. Further,
we adopt a cos re-weighting scheme to stabilize the attention weights. This helps the model to
amplify local correlations, which usually contain more relevant information for natural language
tasks. Thanks to the Ptolemy’s theorem, our attention can be exactly decomposed into a linear form.
We perform extensive experiments on both autoregressive language models and bidirectional models
on ﬁve public benchmarks, including WikiText-103 (Merity et al., 2017), GLUE (Wang et al., 2018),
IMDB (Maas et al., 2011), AMAZON (Ni et al., 2019) and Long-Range Arena benchmark (Tay
et al., 2020b). Our model shows much better inference speed and smaller memory footprint, while
achieving on par performance with the vanilla transformer. It is noteworthy that our method ranks
1st on the Long-Range Arena benchmark, showing favorable performance than other competitors,
which well demonstrates its strong capacity in modeling long sequence inputs."
OUR METHOD,0.09420289855072464,"2
OUR METHOD"
OUR METHOD,0.09782608695652174,"In this section, we provide technique details of our linear transformer called COSFORMER . The key
insight of the COSFORMER is to replace the non-decomposable non-linear softmax operation by a
linear operation with decomposable non-linear re-weighting mechanism. Our model is applicable"
OUR METHOD,0.10144927536231885,Published as a conference paper at ICLR 2022
OUR METHOD,0.10507246376811594,"to both casual and cross attentions with a linear time and space complexity with regard to the input
sequence length, thus exhibiting strong capacity in modeling long-range dependencies."
THE GENERAL FORM OF TRANSFORMER,0.10869565217391304,"2.1
THE GENERAL FORM OF TRANSFORMER
Given an input sequence x with length of N, we ﬁrst represent it in the embedding space x ∈RN×d
with feature dimension of d. A transformer block T : RN×d →RN×d with input x is deﬁned as:"
THE GENERAL FORM OF TRANSFORMER,0.11231884057971014,"T (x) = F(A(x) + x),
(1)"
THE GENERAL FORM OF TRANSFORMER,0.11594202898550725,"where F is a feedforward network that contains a residual connection; A is the self-attention func-
tion that computes the attention matrix A, which has quadratic space and time complexity with
respect to N, thus becoming the computation bottleneck of T on long inputs."
THE GENERAL FORM OF TRANSFORMER,0.11956521739130435,"There are three key components in A, namely, query (Q), key (K), value (V ) computed through
three learnable linear matrices WQ, WK, WV : Q = xWQ, K = xWK, V = xWV . We use Mi to
represent the i-th row of a matrix M, then the output O ∈RN×d of A(x) can be computed as:"
THE GENERAL FORM OF TRANSFORMER,0.12318840579710146,"O = A(x) = [O1, . . . , ON]T ,
Oi =
X j"
THE GENERAL FORM OF TRANSFORMER,0.12681159420289856,"S(Qi, Kj)
P"
THE GENERAL FORM OF TRANSFORMER,0.13043478260869565,"j S(Qi, Kj)Vj,
(2)"
THE GENERAL FORM OF TRANSFORMER,0.13405797101449277,"where S(·) measures the similarity between queries. If S(Q, K) = exp(QKT ), the Eq. 2 becomes
the dot-product attention with softmax normalization. In this case, the space and time complexity
to compute one row of the output Oi is O(N). Therefore, the total space and time complexity for
computing O grows quadratically with respect to the input length."
LINEARIZATION OF SELF-ATTENTION,0.13768115942028986,"2.2
LINEARIZATION OF SELF-ATTENTION
According to Eq. 2, we can select any similarity functions to compute the attention matrix. In order
to maintain a linear computation budget, one solution is to adopt a decomposable similarity function
such that:
S(Qi, Kj) = φ(Qi)φ(Kj)T ,
(3)
where φ is a kernel function that maps the queries and keys to their hidden representations. Then
one can rewrite Eq. 2 in the form of kernel functions as: Oi ="
LINEARIZATION OF SELF-ATTENTION,0.14130434782608695,"PN
j=1(φ(Qi)φ(Kj)T )Vj
PN
j=1(φ(Qi)φ(Kj)T ) .,
(4)"
LINEARIZATION OF SELF-ATTENTION,0.14492753623188406,"After that, attention operation in linear complexity is achieved via the matrix product property:"
LINEARIZATION OF SELF-ATTENTION,0.14855072463768115,"(φ(Q)φ(K)T )V = φ(Q)(φ(K)T V ).
(5)"
LINEARIZATION OF SELF-ATTENTION,0.15217391304347827,"In this form (Eq. 5), instead of explicitly computing the attention matrix A = QKT ∈RN×N, we
calculate the φ(K)T V ∈Rd×d ﬁrst, and then multiplying φ(Q) ∈RN×d. By using this trick, we
only incurs a computation complexity of O(Nd2). Note that in typical natural language tasks, the
feature dimension of one head d is always much smaller than the input sequence length N (d ≪N),
so we can safely omit d and achieve computation complexity of O(N), as illustrated in Figure 2."
LINEARIZATION OF SELF-ATTENTION,0.15579710144927536,"Previous Solutions
As aforementioned, the key to the linear attentions is to ﬁnd a decomposable
similarity function S(·) that generalizes well to different tasks. Most existing linear transformers are
trying to ﬁnd an unbiased estimation of the softmax attention. For example, RFA (Peng et al., 2020)
approximates the softmax operation with random feature maps using theorem of random fourier
features (Rahimi & Recht, 2008) and the Performer (Choromanski et al., 2020) utilizes positive
random features to approximate it. However, we empirically ﬁnd that these methods are sensitive
to the selection of sampling rate and becomes unstable if the sampling rate gets too high. Also, to
accommodate recency bias, gating mechanisms are employed to better exploit more recent context."
LINEARIZATION OF SELF-ATTENTION,0.15942028985507245,"Another group of works attempt to directly replace the softmax with a linear operation. For example,
the linear transformer (Katharopoulos et al., 2020) model replaces the softmax similarity function
with a pure dot product S = QKT , and use a non-linear activation function φ(·) = elu(·) + 1 to
model the pairwise relation between features. However, our controlled experiments show that their
solution does not necessarily generalize well on many downstream tasks (Tab. 2) or the Long-Range
Arena benchmark (Tab. 4). In this paper, we propose a new replacement of softmax that not only
achieves comparable or better performance than the softmax attention in a wide range of tasks, but
also enjoys linear space and time complexity."
LINEARIZATION OF SELF-ATTENTION,0.16304347826086957,Published as a conference paper at ICLR 2022 d x d
LINEARIZATION OF SELF-ATTENTION,0.16666666666666666,"Q’
K’T
V"
LINEARIZATION OF SELF-ATTENTION,0.17028985507246377,O(Nd2)
LINEARIZATION OF SELF-ATTENTION,0.17391304347826086,O(Nd2 + Nd2) ≈O(N)
LINEARIZATION OF SELF-ATTENTION,0.17753623188405798,O(Nd2) N x N
LINEARIZATION OF SELF-ATTENTION,0.18115942028985507,"Q
KT
V"
LINEARIZATION OF SELF-ATTENTION,0.18478260869565216,"d x N
N x d"
LINEARIZATION OF SELF-ATTENTION,0.18840579710144928,O(N2d)
LINEARIZATION OF SELF-ATTENTION,0.19202898550724637,O(N2d + N2d) ≈O(N2)
LINEARIZATION OF SELF-ATTENTION,0.1956521739130435,Softmax
LINEARIZATION OF SELF-ATTENTION,0.19927536231884058,O(N2d)
LINEARIZATION OF SELF-ATTENTION,0.2028985507246377,"Vanilla self attention
Linearized self attention"
LINEARIZATION OF SELF-ATTENTION,0.20652173913043478,"N x d
N x d
N x d
d x N
N x d
N x d"
LINEARIZATION OF SELF-ATTENTION,0.21014492753623187,"Figure 2: Illustration of the computations for vanilla self attention (left) and linearized attention (right). The
input length is N and feature dimension is d, with d ≪N. Tensors in the same box are associated for
computation. The linearized formulation allows O(N) time and space complexity."
ANALYSIS OF SOFTMAX ATTENTION,0.213768115942029,"2.3
ANALYSIS OF SOFTMAX ATTENTION
In the vanilla transformer architecture, when S(Q, K) = exp(QKT ), the softmax operation is ap-
plied to obtain row-wise normalization on the attention matrix A ∈RN×N as shown in the Eq. 2. In
other words, we normalize the relations of each element in the input sequence to all other elements
in order to obtain a weighted aggregation of contextual information. However, apart from the good
empirical performance of softmax attention, what are the crucial and necessary characteristics of it
remain only loosely determined in the original transformer paper and follow-up works."
ANALYSIS OF SOFTMAX ATTENTION,0.21739130434782608,"Table 1: Analysis of the softmax properties. All at-
tention variants are implemented in the RoBERTa (Liu
et al., 2019) architecture and are pre-trained on the
WikiText-103 (Merity et al., 2017) dataset. The Loss
represents the validation loss. We then ﬁne-tune these
variants on each downstream datasets and show the ac-
curacy (the higher the better)."
ANALYSIS OF SOFTMAX ATTENTION,0.2210144927536232,"Loss
QQP
SST-2
MNLI
φI
2.343
84.23
76.26
58.27
φLeakyReLU
2.246
84.46
78.21
74.26"
ANALYSIS OF SOFTMAX ATTENTION,0.2246376811594203,"φReLU
1.993
88.86
89.90
77.86
softmax
1.915
88.41
92.31
79.15"
ANALYSIS OF SOFTMAX ATTENTION,0.22826086956521738,"In this work, we empirically identify two key
properties of the softmax operation that may
play important roles for its performance: 1) it
ensures all values in the attention matrix A to
be non-negative; 2) it provides a non-linear re-
weighting mechanism to concentrates the dis-
tribution of attention connections and stabilizes
the training(Titsias, 2016; Gao & Pavel, 2017;
Jang et al., 2016)."
ANALYSIS OF SOFTMAX ATTENTION,0.2318840579710145,"To validate these assumptions, we design the
following preliminary studies as shown in Ta-
ble 1. First, to validate the importance of non-
negativity, we compare three instantiations of
the function φ in equation 3: an identify mapping φI = I that does not preserve the non-negativity,
and the other variant φReLU(·) = ReLU(·) that retains only positive input values while replacing
negative values to zeros. We also add the φLeakyReLU(·) = LeakyReLU(·) variant as it does not
have the non-negativity as well but have the same non-linearly as the ReLU one. Second, to demon-
strate the effect of non-linear re-weighting, we compare the models using only φReLU(·) without any
re-weighting and those with softmax operations. From Table 1, the superior results of φReLU over
φI and φLeakyReLU demonstrate the beneﬁt of retaining non-negative values. Our conjecture is that
by retaining only positive values in the similarity matrices, the model ignores features with negative
correlations, thus effectively avoiding aggregating irrelevant contextual information. By comparing
the results of φReLU with the softmax, we observe that models with softmax re-weighting converge
faster and generalize better to downstream tasks. This might be explained as softmax normalization
ampliﬁes the correlated pairs, which might be useful to identify useful patterns."
COSFORMER,0.23550724637681159,"2.4
COSFORMER
Based on the observations above, we propose our model COSFORMER , which discards entirely
the softmax normalization while still features the non-negativity and re-weighting mechanism. Our
COSFORMER consists two main components: a linear projection kernel φlinear and a cos-Based Re-
weighting mechanism. Below we describe details of each components:"
COSFORMER,0.2391304347826087,"Linear projection kernel φlinear
Recall the general form of the attention in Eq. 2, let us deﬁne a
linear similarity as:
S(Q, K) = s(φlinear(Q), φlinear(K)) = s(Q
′, K
′)
(6)
where φlinear is the transformation function that map queries Q and keys K to our desired represen-
tations Q
′ and K
′, and s is a function that can be linearly decomposed to measure the similarity
between Q
′ and K
′. Speciﬁcally, in order to ensure a full positive attention matrix A and avoid"
COSFORMER,0.2427536231884058,Published as a conference paper at ICLR 2022
COSFORMER,0.2463768115942029,"(3)cosFormer
(w/o re-weighting)
(2)cosFormer
(1)Vanilla 
Transformer"
COSFORMER,0.25,"(4)cos re-weighting
matrix"
COSFORMER,0.2536231884057971,"Figure 3: (1): Attention matrix of vanilla transformer.(2):Attention matrix of COSFORMER .(3): Attention
matrix of COSFORMER without re-weighting. (4): Visualization of the cos-based distance matrix. After re-
weighting, we can see a smoother attention distribution along the diagonal region of attention matrix, exhibiting
a similar pattern to the vanilla transformer, which assists to stabilize the training."
COSFORMER,0.2572463768115942,"aggregating negatively-correlated information, we adopt ReLU(·) as the transformation functions
and therefore effectively eliminate negative values:"
COSFORMER,0.2608695652173913,"φlinear(x) = ReLU(x)
(7)"
COSFORMER,0.2644927536231884,"As Q
′ and K
′ contain only non-negative values, we directly take their dot-product s(x, y) =
xyT , x, y ∈R1×d followed by a row-wise normalization to compute attention matrices: Oi ="
COSFORMER,0.26811594202898553,"PN
j=1 f(φlinear(Qi),φlinear(Kj))Vj
PN
j=1 f(φlinear(Qi),φlinear(Kj))
="
COSFORMER,0.2717391304347826,"PN
j=1(ReLU(Qi)ReLU(Kj)T )Vj
PN
j=1(ReLU(Qi)ReLU(Kj)T )
(8)"
COSFORMER,0.2753623188405797,"Based on Eq. 4, we rearrange the order of dot-product and obtain the formulation of the proposed
attention in linear complexity as:"
COSFORMER,0.27898550724637683,"Oi =
ReLU(Qi) PN
j=1 ReLU(Kj)T Vj
ReLU(Qi) PN
j=1 ReLU(Kj)T
(9)"
COSFORMER,0.2826086956521739,"cos-Based Re-weighting Mechanism
The non-linear re-weighting mechanism introduced by the
softmax attention can concentrate the distribution of the attention weights and therefore stabilize the
training process (Titsias, 2016; Gao & Pavel, 2017; Jang et al., 2016). We also empirically ﬁnd that
it can punish far-away connections and enforce locality in some cases. In fact, such locality bias,
i.e., a large portion of contextual dependencies are from neighboring tokens, is commonly observed
on downstream NLP tasks (Clark et al., 2019; Kovaleva et al., 2019), as shown in Figure 3 (1)."
COSFORMER,0.286231884057971,"Based on the assumption above, what we need to fulﬁll the second property of softmax may be a de-
composable re-weighting mechanism that can introduce recency bias to the attention matrix. Here,
we propose a cos-based re-weighting mechanism as it perfectly ﬁt our purpose: 1). the Ptolemy’s
theorem ensures the cos weights can be decomposed into two summations; 2). as shown in Fig-
ure 3 (4), the cos will put more weights on the neighbouring tokens and therefore enforces locality.
Also, by comparing the attention matrices in Figure 3 (2) and (3), the COSFORMER enforces more
locality than the one without the re-weighting mechanism."
COSFORMER,0.2898550724637681,"Speciﬁcally, by combining with Eq 6, the model with cosine re-weighting is deﬁned as:"
COSFORMER,0.29347826086956524,"s(Q
′
i, K
′
j) = Q
′
iK
′T
j
cos
  π"
COSFORMER,0.2971014492753623,2 × i−j
COSFORMER,0.3007246376811594,"M

(10)"
COSFORMER,0.30434782608695654,"By leveraging the Ptolemy’s theorem, we decompose this formulation as: Q
′"
COSFORMER,0.3079710144927536,"iK
′T
j
cos
 π"
COSFORMER,0.3115942028985507,2 × i −j M
COSFORMER,0.31521739130434784,"
= Q
′"
COSFORMER,0.3188405797101449,"iK
′T
j"
COSFORMER,0.322463768115942,"
cos
 πi"
M,0.32608695652173914,2M
M,0.32971014492753625,"
cos
 πj"
M,0.3333333333333333,2M
M,0.33695652173913043,"
+ sin
 πi"
M,0.34057971014492755,2M
M,0.3442028985507246,"
sin
 πj"
M,0.34782608695652173,2M 
M,0.35144927536231885,"=

Q
′"
M,0.35507246376811596,"i cos
 πi"
M,0.358695652173913,2M
M,0.36231884057971014," 
K
′"
M,0.36594202898550726,"j cos
 πj"
M,0.3695652173913043,2M
M,0.37318840579710144,"T
+

Q
′"
M,0.37681159420289856,"i sin
 πi"
M,0.3804347826086957,2M
M,0.38405797101449274," 
K
′"
M,0.38768115942028986,"j sin
 πj"
M,0.391304347826087,2M T
M,0.39492753623188404,"where i, j = 1, ..., N, M ≥N, and Q
′ = ReLU(Q), K
′ = ReLU(K). Let Qcos
i
= Q
′
i cos
  πi"
M,0.39855072463768115,"2M

,
Qsin
i
= Q
′
i sin
  πi"
M,0.40217391304347827,"2M

, Kcos
j
= K
′
j cos
  πj"
M,0.4057971014492754,"2M

, Ksin
j
= K
′
j sin
  πj"
M,0.40942028985507245,"2M

, the output of the proposed at-
tention module can be expressed as: Oi ="
M,0.41304347826086957,"PN
j=1 f(Q
′
i,K
′
j)Vj
PN
j=1 f(Q′
i,K′
j)
="
M,0.4166666666666667,"PN
j=1 Qcos
i

(Kcos
j )
T Vj

+PN
j=1 Qsin
i

(Ksin
j )
T Vj
"
M,0.42028985507246375,"PN
j=1 Qcos
i (Kcos
j )
T +PN
j=1 Qsin
i (Ksin
j )
T
,
(11)"
M,0.42391304347826086,Published as a conference paper at ICLR 2022
M,0.427536231884058,"10000
20000
30000
40000
50000
Train Step 3.8 3.2 2.6 2.2"
M,0.4311594202898551,"2.0
1.9"
M,0.43478260869565216,Train Loss
M,0.4384057971014493,"Vanilla Transformer
cosFormer
cosFormer w/o reweight"
M,0.4420289855072464,"10000
20000
30000
40000
50000
Train Step 4.0 3.2 2.6 2.2"
M,0.44565217391304346,"2.0
1.9"
M,0.4492753623188406,Valid Loss
M,0.4528985507246377,"Vanilla Transformer
cosFormer
cosFormer w/o reweight"
M,0.45652173913043476,"Figure 4: Training loss (left) and validation loss (right) of the bidirectional language modeling pre-train. In
both training and validation, the proposed COSFORMER has a faster converge speed than vanilla transformer."
M,0.4601449275362319,"where Oi is the output at the ith position of the sequence from the attention module. Detailed
derivation are included in the Appendix. Without losing the generality, our method achieves a linear
complexity as:"
M,0.463768115942029,"O = S(Q, K)V = (QcosKcos + QsinKsin)V = Qcos(KcosV ) + Qsin(KsinV )
(12)"
M,0.4673913043478261,"Relation to positional encoding.
COSFORMER can be seen as a new way of introducing the rela-
tive positional bias to the efﬁcient transformer. Compared with the Rotary Position Embedding (Su
et al., 2021), they use a more complex position embedding strategy and did not enforce the non-
negativity to the similarity scores as ours. Also, since they only change the position embedding on
the numerator while keeping the denominator unchanged, the summation of their attention scores is
not equal to 1. For Stochastic Positional Encoding (Liutkus et al., 2021), they use a sampling strategy
to approximate the softmax, and introduce relative positional encoding to linear transformers."
EXPERIMENTS,0.47101449275362317,"3
EXPERIMENTS"
EXPERIMENTS,0.4746376811594203,"In this section, we experimentally validate the effectiveness of the proposed method in multiple set-
tings. The purposes of the experiments are three-fold. First, we validate the capacity of COSFORMER
in language modeling through autoregressive (Sec. 3.1) and bidirectional (Sec. 3.2) setups using
WikiText-103 (Merity et al., 2017). In this way, we validate the effectiveness of the proposed linear
attention module in both causal and non-causal cases. Second, we investigate the generalization
ability of COSFORMER on downstream tasks by comparisons with other existing transformer vari-
ants. This is achieved by performing comparative ﬁnetuning experiments on ﬁve datasets, including
GLUE (QQP, SST-2, MNLI) (Wang et al., 2018), IMDB (Maas et al., 2011) and AMAZON (Ni
et al., 2019) (Sec. 3.3). We further compare COSFORMER with other transformer variants on the
long-range-arena benchmark (Tay et al., 2020b) to understand its ability in modeling long-range
dependencies (Sec. 3.4) and show comparative analysis into model efﬁciency (Sec. 3.5). Third, we
conduct ablation studies to understand each component in COSFORMER (Sec. 3.6)."
AUTOREGRESSIVE LANGUAGE MODELING,0.4782608695652174,"3.1
AUTOREGRESSIVE LANGUAGE MODELING"
AUTOREGRESSIVE LANGUAGE MODELING,0.48188405797101447,"In autoregressive or left-to-right language modeling, we estimate the probability distribution of a
token given its previous tokens. We use (Baevski & Auli, 2018) as our baseline model. Speciﬁcally,
we adopt their large model which has 16 cascaded layers with a projected dimensions of 1024, and
replace the self-attention module with our proposed linear attention module. We train our model on
8 Nvidia Tesla A100 GPUs with a sequence length of 512 for 150K updates on the WikiText-103
(Merity et al., 2017) and report perplexity on the validation and test splits in Table 2."
AUTOREGRESSIVE LANGUAGE MODELING,0.4855072463768116,"Table 2: Perplexity (lower is better) results of language
modeling pre-training task on validation set and test set
of the WikiText-103 (Merity et al., 2017) dataset."
AUTOREGRESSIVE LANGUAGE MODELING,0.4891304347826087,"ppl(val) ↓
ppl(test) ↓
Vanilla Transformer
24.5
26.2
Linear Transformer
28.7
30.2
RFA-Gaussian
25.8
27.5
RFA-across
26.4
28.1
RFA-Gate-across
24.8
26.3
RFA-Gate-Gaussian
23.2
25.0
COSFORMER
23.5
23.1"
AUTOREGRESSIVE LANGUAGE MODELING,0.4927536231884058,"We observe that although the baseline model
is a powerful standard transformer which re-
quires quadratic computation complexity, COS-
FORMER outperforms it with a clear margin
in linear computation complexity. Besides, we
achieve comparable perplexity to other meth-
ods on the validation set, and signiﬁcantly out-
perform all competing methods on the test set
by a clear gap, which further demonstrates the
effectiveness of COSFORMER ."
AUTOREGRESSIVE LANGUAGE MODELING,0.4963768115942029,Published as a conference paper at ICLR 2022
AUTOREGRESSIVE LANGUAGE MODELING,0.5,"Table 3: Results on ﬁne-tuned downstream tasks based on pre-trained bidirectional model. Best result is
in boldface and second best is underlined. The proposed COSFORMER achieves superb performances over
competing efﬁcient transformers and is approaching vanilla transformer."
AUTOREGRESSIVE LANGUAGE MODELING,0.5036231884057971,"QQP ↑
SST-2 ↑
MNLI ↑
IMDB ↑
AMAZON ↑
Avg ↑
Vanilla Transformer (Liu et al., 2019)
88.41
92.31
79.15
92.86
75.79
85.70
Performer (Choromanski et al., 2020)
69.92
50.91
35.37
60.36
64.84
56.28
Reformer (Kitaev et al., 2019)
63.18
50.92
35.47
50.01
64.28
52.77
Linear Trans. (Katharopoulos et al., 2020)
74.85
84.63
66.56
91.48
72.50
78.00
Longformer (Beltagy et al., 2020)
85.51
88.65
77.22
91.14
73.34
83.17
RFA (Peng et al., 2020)
75.28
76.49
57.6
78.98
68.15
71.30
COSFORMER
89.26
91.05
76.70
92.95
76.30
85.25"
BIDIRECTIONAL LANGUAGE MODEL,0.5072463768115942,"3.2
BIDIRECTIONAL LANGUAGE MODEL"
BIDIRECTIONAL LANGUAGE MODEL,0.5108695652173914,"For bidirectional language modeling, we adopt RoBERTa (Liu et al., 2019) as the baseline model.
Similarly, we replace the self-attention module in the RoBERTa by the proposed linear attention
module, and keep other structures unchanged. We train this bidirectional task on 2 Nvidia Tesla
A100 GPUs for 50K iterations with a input sequence length 512. As shown in Figure 4, COS-
FORMER converges faster than vanilla transformer on both training and validation sets with a com-
parable or smaller loss values, despite it only consumes linear space and time computation com-
plexity. In addition, the COSFORMER variant with re-weighting mechanism has both notably better
converge speed and ﬁnal results over the counterpart without re-weighting, which further validates
the effectiveness of our cos-based distance matrix and also demonstrates the effectiveness of recency
bias on natural language data."
DOWNSTREAM FINE-TUNING TASKS,0.5144927536231884,"3.3
DOWNSTREAM FINE-TUNING TASKS"
DOWNSTREAM FINE-TUNING TASKS,0.5181159420289855,"In this section, we ﬁne-tune the pre-trained model on downstream tasks to demonstrate the general-
ization ability of COSFORMER on downstream tasks. We use the pre-trained bidirectional model and
ﬁne-tune it on three downstream text classiﬁcation tasks: GLUE (QQP, SST-2, MNLi) (Wang et al.,
2018), IMDB (Maas et al., 2011) and AMAZON (Ni et al., 2019). For fair comparison, we ﬁrst
pre-train all the competing efﬁcient transformer variants for the same 50K iterations on WikiText-
103 (Merity et al., 2017) under the same setting, then we follow the same ﬁne-tuning protocol as
RoBERTa (Liu et al., 2019) to ﬁne-tune these methods on the downstream tasks. From Table 3, we
can see that COSFORMER outperforms baseline (Liu et al., 2019) on three out of ﬁve datasets, and
achieves either best or secondary place on all ﬁve downstream datasets compared to competing efﬁ-
cient transformers. It is worth noting that despite Longformer (Beltagy et al., 2020) achieves better
results on MNLI than COSFORMER , it requires a computation complexity of O(Nw), where w is
window size. As shown in Figure 1, Longformer is slower and requires more memory overhead than
COSFORMER . Other competing methods(Peng et al., 2020; Choromanski et al., 2020; Kitaev et al.,
2019) are all based on kernel functions and have substantial performance gaps compared with our
model. This validates the effectiveness of the proposed COSFORMER model compared with other
efﬁcient transformer variants."
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5217391304347826,"3.4
RESULTS ON LONG-RANGE-ARENA BENCHMARK"
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5253623188405797,"To further evaluate the generalization ability of the proposed method, we train our model from
scratch on Long-range-arena benchmark 2020b. Long-range-arena (Tay et al., 2020b) is a bench-
mark speciﬁcally designed for efﬁcient transformers with long input sequences, thus serving as a
suitable testbed to assess the quality of efﬁcient transformer variants comparatively. To ensure fair
comparison, we ﬁrst implement our method on Jax (Bradbury et al., 2018), then carefully follow
their preprocessing, data split, model structure and training protocol. We evaluate our method on a
variety of tasks including Long sequence ListOps (Nangia & Bowman, 2018), Byte-level text clas-
siﬁcation (Maas et al., 2011), document retrieval using the ACL Anthology Network (Radev et al.,
2013), image classiﬁcation on sequence of pixels on CIFAR-10 (Krizhevsky & Hinton, 2009), and
Pathﬁnder (Linsley et al., 2018). As shown in Table 4, COSFORMER overall achieves competitive
results across all the tasks while achieving best performance on ListOps and Document Retrieval.
For the Pathﬁnder task, since the distance between the two points can be very far from each other,
our introduced locality bias would have negative impact to this task and show a bit lags to other
SOTA methods, despite that the performance gap between our method and the vanilla transformer is
small It is worth mentioning that COSFORMER achieves the best overall scores on Long-range-arena
benchmark, being one of the only two models that surpass vanilla transformer architecture."
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5289855072463768,Published as a conference paper at ICLR 2022
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.532608695652174,"Table 4: Results on Long-range-arena benchmark. Best result is in boldface and second best is underlined.
COSFORMER achieves the best average score across 5 different tasks."
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5362318840579711,"Model
ListOps ↑
Text↑
Retrieval↑
Image↑
Pathﬁnder↑
Avg ↑
Local Attention (Tay et al., 2020b)
15.82
52.98
53.39
41.46
66.63
46.06
Linear Trans. (Katharopoulos et al., 2020)
16.13
65.9
53.09
42.34
75.3
50.55
Reformer (Kitaev et al., 2019)
37.27
56.1
53.4
38.07
68.5
50.67
Sparse Trans.(Child et al., 2019)
17.07
63.58
59.59
44.24
71.71
51.24
Sinkhorn Trans.(Tay et al., 2020a)
33.67
61.2
53.83
41.23
67.45
51.29
Linformer(Wang et al., 2020)
35.7
53.94
52.27
38.56
76.34
51.36
Performer(Choromanski et al., 2020)
18.01
65.4
53.82
42.77
77.05
51.41
Synthesizer (Tay et al., 2021)
36.99
61.68
54.67
41.61
69.45
52.88
Longformer(Beltagy et al., 2020)
35.63
62.85
56.89
42.22
69.71
53.46
Transformer (Vaswani et al., 2017)
36.37
64.27
57.46
42.44
71.4
54.39
BigBird (Zaheer et al., 2020)
36.05
64.02
59.29
40.83
74.87
55.01
COSFORMER
37.9
63.41
61.36
43.17
70.33
55.23"
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5398550724637681,"Table 5: Speed comparison on the long-range-arena benchmark in both training and inference varying se-
quence lengths (1-4k). We mark it with a cross if a method runs out of memory. The higher, the better."
RESULTS ON LONG-RANGE-ARENA BENCHMARK,0.5434782608695652,"Inference Speed(steps per second)↑
Train Speed(steps per second)↑
model
1K
2K
3K
4k
1K
2K
3K
4K
Transformer(Vaswani et al., 2017)
25.37
7.83


6.95
2.23


Local Attention(Tay et al., 2020b)
57.73
33.19
23.36
17.79
13.45
6.71
4.32
3.09
Linformer(Wang et al., 2020)
70.09
39.1
27.05
20.62
14.75
7.09
4.52
3.21
Reformer(Kitaev et al., 2019)
44.21
21.58
12.74
8.37
11.58
4.98
2.94
1.95
Sinkhorn Trans. (Tay et al., 2020a)
43.29
23.58
16.53
12.7
11.09
5.57
3.68
2.68
Synthesizer (Tay et al., 2021)
20.89
6.24


6.36
2.01


BirBird (Zaheer et al., 2020)
20.96
11.5
8.12
6.15
6.46
3.2
2.13
1.53
Linear Trans. (Katharopoulos et al., 2020)
67.85
38.24
26.28
19.98
11.86
5.54
3.53
2.56
Performer (Choromanski et al., 2020)
74.15
42.31
29.5
22.44
14
6.49
4.1
2.94
Longformer (Beltagy et al., 2020)
22.99
6.72


4.4
1.3


Sparse Trans. Child et al. (2019)
24.87
7.5


6.77
2.2


COSFORMER
58.82
33.45
22.77
17.42
12.27
5.72
3.62
2.64"
EFFICIENCY COMPARISON,0.5471014492753623,"3.5
EFFICIENCY COMPARISON"
EFFICIENCY COMPARISON,0.5507246376811594,"In this section, we compare the efﬁciency of COSFORMER with other models, with a focus on
long sequences as inputs. With the proposed linear attention module, we expect that COSFORMER
scales comparably with other linear variants while signiﬁcantly surpassing the vanilla transformer
architecture. For a fair and comprehensive comparison, we implement our method and competing
methods on Jax (Bradbury et al., 2018). We use the byte-level text classiﬁcation benchmark and
report runtime speed during both training and inference under different sequence lengths (1k-4k).
We conduct experiments on one Nvidia A6000 GPU and also report the corresponding inference-
time memory foot prints as shown in Figure 1. As shown in Table 5 and Figure 1, most pattern based
methods (Beltagy et al., 2020; Zaheer et al., 2020; Tay et al., 2020a; 2021) and vanilla transformer
(Vaswani et al., 2017) are much slower and require greater memory than COSFORMER prevents
them from extending to longer sequence. Further, the kernel based methods like (Narang et al.,
2021; Choromanski et al., 2020; Tay et al., 2020a) have comparable speed and memory overheads,
but their performances are less satisfactory compared to COSFORMER across above metrics. In
summary, our model COSFORMER achieves overall better efﬁciency than other linear variants while
maintain superior modeling and generalization ability."
EFFICIENCY COMPARISON,0.5543478260869565,"3.6
ABLATION: cos-BASED RE-WEIGHTING"
EFFICIENCY COMPARISON,0.5579710144927537,"Table 6: Performance comparison of COSFORMER with and
without cos-based re-weighting (φReLU).
We evaluate on
two compositive metrics. Bidirectional ﬁnetuneavg: average
score across 5 datasets reported in Table 3. LRAavg: average
score across 5 tasks reported in Table 4."
EFFICIENCY COMPARISON,0.5615942028985508,"Model
Bidirectional ﬁnetuneavg ↑
LRAavg ↑
φReLU
85.12
54.20
COSFORMER
85.25
55.23"
EFFICIENCY COMPARISON,0.5652173913043478,"By introducing cos-based re-weighting,
we provide a non-linear mechanism to
concentrate the distribution of attention
connections and stabilizes the training. In
this way, we encourage the model to better
take into account the locality inductive bi-
ases commonly observed on many natural
language tasks. In particular, we investi-
gate the effect of the cos-based re-weighting in two aspects. First, as shown in Figure 4, by adding"
EFFICIENCY COMPARISON,0.5688405797101449,Published as a conference paper at ICLR 2022
EFFICIENCY COMPARISON,0.572463768115942,"cos-based re-weighting, we obtain both notably better converge speed and ﬁnal results in autoregres-
sive language modeling. Further, in Table 6, we present a comparison between COSFORMER models
with and without re-weighting mechanism. We use two composite metrics which comprehensively
include 10 different datasets from bidirectional downstream ﬁne-tuning tasks and long-range-arena
(Tay et al., 2020b). COSFORMER achieves overall better results over the counterpart without re-
weighting, improving the average scores on bidirectional ﬁnetuning and long-range-arena by a clear
margin. This veriﬁes that the proposed re-weighting effectively incorporates the locality inductive
biases for natural language tasks."
RELATED WORK,0.5760869565217391,"4
RELATED WORK"
RELATED WORK,0.5797101449275363,"This section will introduce the existing works on improving the efﬁciency of Transformers, they can
be broadly divided into two categories, Pattern based methods and Kernel based methods."
RELATED WORK,0.5833333333333334,"Pattern based method
Pattern based methods sparsify the attention matrix with handcrafted or
learnable patterns. As an early approach, Lee et al. (2019) leverages the inducing points from
the sparse Gaussian process to reduce the quadratic complexities of a transformer. Child et al.
(2019) reduces the complexity by applying combination of strided pattern and local pattern to the
vanilla attention matrix. Longformer (Beltagy et al., 2020) designs ﬁxed diagonal sliding windows
combined with global window, and the sliding window pattern can also be extended with dilation
to enlarge the receptive ﬁeld. Zaheer et al. (2020) presents a more powerful and expressive sparse
attention mechanism, which combines multiple types of attention patterns and gives a thorough
study of sparse attention mechanism. Instead of ﬁxed patterns, Kitaev et al. (2019) and Daras et al.
(2020) group the attention computation process into buckets by local sensitive hashing, while Roy
et al. (2020) uses mini-batch spherical k-means. Nevertheless, Pattern based methods can only cope
with sequences up to a certain length, and the computational complexity still grows rapidly when
the input sequence becomes longer."
RELATED WORK,0.5869565217391305,"Kernel based method
When faced with longer input sequences, it is more efﬁcient to directly
reduce the complexity of the theoretical calculation method. Kernel based methods speed up self-
attention by reducing the computation complexity of self-attention from quadratic to linear. Vyas
et al. (2020) approximate the full attention with a ﬁxed number of cluster attention groups by assum-
ing neighbouring queries in Euclidean space should have similar attention distributions. Peng et al.
(2020) chooses to use the production of Gaussian kernel functions to approximate Softmax, chang-
ing the order of scale dot product calculation, thus reducing the theoretical time to linear complexity
and Choromanski et al. (2020) uses Haar measurement based kernel instead. Wang et al. (2020)
imports the low-rank prior for attention matrix and approximate softmax with SVD decomposition
manner. Xiong et al. (2021) utilizes the Nystr¨om method with segment-means to generate a low-
rank approximation of the Softmax matrix. Katharopoulos et al. (2020) formalizes the transformer
layer as a recurrent neural network. In this paper, we demonstrate that the approximation to Softmax
is unneccessary for Linearization of self-attention module. We instead propose a new method to
replace Softmax with a linear operation with a re-weighting mechanism, which reduces both time
complexity and space complexity to O(N) while maintaining the accuracy."
CONCLUSION,0.5905797101449275,"5
CONCLUSION"
CONCLUSION,0.5942028985507246,"We presented COSFORMER , a new efﬁcient transformer that has linear time and space complexity.
Our COSFORMER is based on two key properties of the original softmax attention: (i) every element
in the attention matrix are non-negative, such that negatively-correlated information are not included
for contextual information aggregation; (ii) the non-linear re-weighting scheme concentrates the dis-
tribution of the attention matrix, in order to better exploit the locality inductive biases on sequence
modeling. To fulﬁll these properties in our COSFORMER , we utilized the RuLU function as our
linear operation to ensure the non-negative property; a new cos-based re-weighting mechanism was
proposed to enforce the locality bias in the original softmax attention. Since our COSFORMER is nat-
urally decomposable, it does not suffer the accumulated approximation error that usually happens in
previous linear transformers. On causal pre-training, bidirectional pre-training, and multiple down-
stream text understanding tasks, COSFORMER achieves comparable or even better performances
than the vanilla transformer. On long sequence benchmark, COSFORMER achieved state-of-the-art
performance over ﬁve different tasks. Further, COSFORMER obtains a signiﬁcant overall advan-
tage in terms of time and memory efﬁciency over all existing efﬁcient transformers, facilitating the
transformers to easily scale to longer input sequence."
CONCLUSION,0.5978260869565217,Published as a conference paper at ICLR 2022
REFERENCES,0.6014492753623188,REFERENCES
REFERENCES,0.605072463768116,"Abien Fred Agarap.
Deep learning using rectiﬁed linear units (relu).
arXiv preprint
arXiv:1803.08375, 2018."
REFERENCES,0.6086956521739131,"Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
International Conference on Learning Representations, 2018."
REFERENCES,0.6123188405797102,"Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-
work for self-supervised learning of speech representations. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.6159420289855072,"Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150, 2020."
REFERENCES,0.6195652173913043,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. Jax: composable transformations of python+ numpy
programs. Version 0.1, 55, 2018."
REFERENCES,0.6231884057971014,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv, 2020."
REFERENCES,0.6268115942028986,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko.
End-to-end object detection with transformers.
In European Conference
on Computer Vision, pp. 213–229. Springer, 2020."
REFERENCES,0.6304347826086957,"Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019."
REFERENCES,0.6340579710144928,"Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020."
REFERENCES,0.6376811594202898,"Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look
at? an analysis of bert’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, 2019."
REFERENCES,0.6413043478260869,"Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). In Yoshua Bengio and Yann LeCun (eds.), 4th Inter-
national Conference on Learning Representations, ICLR, San Juan, Puerto Rico, 2016."
REFERENCES,0.644927536231884,"Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf - efﬁcient atten-
tion using asymmetric clustering. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), NeurIPS, volume 33, pp. 6476–6489. Curran Associates, Inc., 2020."
REFERENCES,0.6485507246376812,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.6521739130434783,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-
age is worth 16x16 words: Transformers for image recognition at scale. In International Confer-
ence on Learning Representations, 2020."
REFERENCES,0.6557971014492754,"Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game
theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017."
REFERENCES,0.6594202898550725,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.6630434782608695,Published as a conference paper at ICLR 2022
REFERENCES,0.6666666666666666,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.6702898550724637,"Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016."
REFERENCES,0.6739130434782609,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma-
chine Learning, pp. 5156–5165. PMLR, 2020."
REFERENCES,0.677536231884058,"Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
Reformer: The efﬁcient transformer.
In
International Conference on Learning Representations, 2019."
REFERENCES,0.6811594202898551,"Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets
of bert. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 4365–4374, 2019."
REFERENCES,0.6847826086956522,"A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009."
REFERENCES,0.6884057971014492,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. In ICML,
pp. 3744–3753, 2019."
REFERENCES,0.6920289855072463,"Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charlie Windolf, and Thomas Serre. Learning
long-range spatial dependencies with horizontal gated recurrent units. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pp. 152–164, 2018."
REFERENCES,0.6956521739130435,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.6992753623188406,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021."
REFERENCES,0.7028985507246377,"Antoine Liutkus, Ondˇrej C´ıfka, Shih-Lun Wu, Umut S¸ims¸ekli, Yi-Hsuan Yang, and Ga¨el Richard.
Relative positional encoding for Transformers with linear complexity. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pp. 7067–7079. PMLR, 18–24 Jul 2021."
REFERENCES,0.7065217391304348,"Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human language technologies, pp. 142–150, 2011."
REFERENCES,0.7101449275362319,"Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. 5th International Conference on Learning Representations, ICLR, Toulon, France, 2017."
REFERENCES,0.7137681159420289,"Nikita Nangia and Samuel Bowman. Listops: A diagnostic dataset for latent tree learning. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Student Research Workshop, pp. 92–99, 2018."
REFERENCES,0.717391304347826,"Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-
ishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations
transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021."
REFERENCES,0.7210144927536232,"Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled
reviews and ﬁne-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pp. 188–197, 2019."
REFERENCES,0.7246376811594203,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.
Random feature attention. In International Conference on Learning Representations, 2020."
REFERENCES,0.7282608695652174,Published as a conference paper at ICLR 2022
REFERENCES,0.7318840579710145,"Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013."
REFERENCES,0.7355072463768116,"Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008."
REFERENCES,0.7391304347826086,"Aurko Roy, Mohammad Taghi Saffar, David Grangier, and Ashish Vaswani. Efﬁcient content-based
sparse attention with routing transformers. In TACL, 2020."
REFERENCES,0.7427536231884058,"Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised
pre-training for speech recognition. In INTERSPEECH, 2019."
REFERENCES,0.7463768115942029,"Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
with rotary position embedding. In arXiv, 2021."
REFERENCES,0.75,"Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
International Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a."
REFERENCES,0.7536231884057971,"Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient
transformers. In International Conference on Learning Representations, 2020b."
REFERENCES,0.7572463768115942,"Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Re-
thinking self-attention for transformer models. In International Conference on Machine Learning,
pp. 10183–10192. PMLR, 2021."
REFERENCES,0.7608695652173914,"Michalis K Titsias. One-vs-each approximation to softmax for scalable estimation of probabilities.
arXiv preprint arXiv:1609.07410, 2016."
REFERENCES,0.7644927536231884,"Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: An uniﬁed understanding for transformer’s attention via
the lens of kernel. In EMNLP, 2019."
REFERENCES,0.7681159420289855,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.7717391304347826,"A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In NeurIPS,
2020."
REFERENCES,0.7753623188405797,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for
NLP, pp. 353–355, 2018."
REFERENCES,0.7789855072463768,"Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768, 2020."
REFERENCES,0.782608695652174,"Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nystr¨omformer: A nystr¨om-based algorithm for approximating self-attention. In
AAAI, 2021."
REFERENCES,0.7862318840579711,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for
longer sequences. In NeurIPS, 2020."
REFERENCES,0.7898550724637681,Published as a conference paper at ICLR 2022
REFERENCES,0.7934782608695652,"A
APPENDIX"
REFERENCES,0.7971014492753623,"A.1
MATHEMATICAL DERIVATION OF cos-BASED RE-WEIGHTING"
REFERENCES,0.8007246376811594,"Following Equation 11, we give a detailed deviation of how to obtain output at position ith position: Oi ="
REFERENCES,0.8043478260869565,"PN
j=1 f(Q
′
i, K
′
j)Vj
PN
j=1 f(Q
′
i, K
′
j) ="
REFERENCES,0.8079710144927537,"PN
j=1"
REFERENCES,0.8115942028985508,"
¯Qcos
i
  ¯Kcos
j
T + ˜Qsin
i

˜Ksin
j
T 
Vj"
REFERENCES,0.8152173913043478,"PN
j=1"
REFERENCES,0.8188405797101449,"
¯Qcos
i
  ¯Kcos
j
T + ˜Qsin
i

˜Ksin
j
T  ="
REFERENCES,0.822463768115942,"PN
j=1 ¯Qcos
i
  ¯Kcos
j
T Vj + PN
j=1 ˜Qsin
i

˜Ksin
j
T
Vj
PN
j=1 ¯Qcos
i
  ¯Kcos
j
T + PN
j=1 ˜Qsin
i

˜Ksin
j
T ="
REFERENCES,0.8260869565217391,"PN
j=1 ¯Qcos
i
  ¯Kcos
j
T Vj

+ PN
j=1 ˜Qsin
i"
REFERENCES,0.8297101449275363,"
˜Ksin
j
T
Vj "
REFERENCES,0.8333333333333334,"PN
j=1 ¯Qcos
i
  ¯Kcos
j
T + PN
j=1 ˜Qsin
i

˜Ksin
j
T (13)"
REFERENCES,0.8369565217391305,"where i, j = 1, ..., N, M ≥N, and Q
′ = ReLU(Q), K
′ = ReLU(K). Let Qcos
i
= Q
′
i cos
  πi"
M,0.8405797101449275,"2M

,
Qcos
i
= Q
′
i cos
  πi"
M,0.8442028985507246,"2M

, Kcos
j
= K
′
j cos
  πj"
M,0.8478260869565217,"2M

, Ksin
j
= K
′
j sin
  πj"
M,0.8514492753623188,"2M

. It presents that the output of the
proposed COSFORMER attention can be obtained in a linear manner."
M,0.855072463768116,"A.2
PSEUDO CODE OF COSFORMER"
M,0.8586956521739131,Algorithm 1 describe the way to compute COSFORMER attention
M,0.8623188405797102,Algorithm 1 COSFORMER attention
M,0.8659420289855072,"Input: Q ∈RN×d1, K ∈RM×d1, V ∈RM×d2;
Output: O ∈RN×d2;
Use Mi to represent the i-th row of matrix M;
Initialize A[i] =
πi
2N , O[i][j] = 0, i = 1, . . . , N, j = 1, . . . , d2;
Initialize Scos[i][j] = 0, Ssin[i][j] = 0, T cos[i] = 0, T sin[i] = 0, i = 1, . . . , d1, j = 1, . . . , d2;
for i in 1, . . . , M do:"
M,0.8695652173913043,"Kcos
i
= Ki cos
  πi"
M,0.8731884057971014,"2M

, Ksin
i
= Ki sin
  πi"
M,0.8768115942028986,"2M

;
Scos += (Kcos
i
)T Vi;
Ssin +=
 
Ksin
i
T Vi;
T cos += Kcos
i
;
T sin += Ksin
i
;
end for
for i in 1, . . . , N do:"
M,0.8804347826086957,"Qcos
i
= Qi cos
  πi"
M,0.8840579710144928,"2M

, Qsin
i
= Qi sin
  πi"
M,0.8876811594202898,"2M

;"
M,0.8913043478260869,"Oi = Qcos
i
Scos+Qsin
i
Ssin"
M,0.894927536231884,"Qcos
i
T cos+Qsin
i
T sin ;
end for"
M,0.8985507246376812,"A.3
ALGORITHM TO VISUALIZE ATTENTION MATRIX"
M,0.9021739130434783,Algorithm 2 describe the way to visualize attention matrix as Figure 3
M,0.9057971014492754,Published as a conference paper at ICLR 2022
M,0.9094202898550725,Algorithm 2 Algorithm to visualize attention matrix
M,0.9130434782608695,"Input: Mk ∈Rd×d, k = 1, . . . , n; threshold ∈[0, 1];
Output: M ∈Rd×d;
Initialize M[i][j] = 0, i ∈1, . . . , d, j ∈1, . . . , d;
for k in 1, . . . , n do:"
M,0.9166666666666666,"for i in 1, . . . , d do:"
M,0.9202898550724637,"index = argsort(Mk[i]) (in descending order)
p = 0
for j in 1, . . . , d do:"
M,0.9239130434782609,"l = index[j]
p += Mk[i][l]
M[i][l] += 1
if p > threshold then: break
end if
end for
end for
end for
M /= n;
Use heatmap to visualize M;"
M,0.927536231884058,"A.4
INTRODUCTION OF DATASET"
M,0.9311594202898551,"We train both models on autoregressive language modeling and bidirectional modeling by Wikitext-
103 dataset, it is split by tokens and its statistics as Table 7.Then we ﬁne-tune the pre-trained bidi-
rectional modeling on several text classiﬁcation tasks."
M,0.9347826086956522,"QQP dataset contain thousands of sentence pair from community question-answering website
Quora.Network need to determine pairs of question are semantically equivalent. SST-2 and IMDB
are collections of movie reviews. The task is to determine whether a review is positive or not. AMA-
ZON dataset contains millions of product reviews from Amazon.The requirement of this task is to
infer the scoring of the product from the review text.MNLI is a crow-source collections of sentence
pairs. The network must distinguish which of the three categories entailment, contradiction and
neutral the given sentences belong to."
M,0.9384057971014492,"The long-range-aren benchmark contains 5 different datasets.ListOps contains some designed clever
mathematical problem to clarify the parsing ability of neural models. IMDB is also used in this
benchmark to examine the text classiﬁcation ability of neural models. CIFAR-10 is a image col-
lection of various of object, this task require models capture 2D spatial relations between ﬂatten
pixels.In pathﬁnder task, models need to determine the connection of two points in the picture, so as
to examine the model’s ability to acquire 2D spatial relationships.AAN dataset is used to evaluate
the ability for models to encode and store compressed representations for retrieving."
M,0.9420289855072463,"Data
Train
Valid
Test
WikiText-103
103M
218K
246K
QQP
364K
-
391K
SST-2
67K
-
1.8K
MNLI
393K
-
20K
IMDB
25K
-
25K
AMAZON
3M
168K
168K
ListOps
90K
-
10K
AAN
147K
18K
17K
CIFAR-10
50K
-
10K
Pathﬁnder
160K
-
20K"
M,0.9456521739130435,"Table 7: Statistics for the datasets.A subset of ”Small” amazon subset on electronics category is
used for experiment"
M,0.9492753623188406,Published as a conference paper at ICLR 2022
M,0.9528985507246377,"A.5
QUALITATIVE RESULTS OF LRA"
M,0.9565217391304348,"We provide our qualitative results of the ListOps and Document Retrieval tasks on Long-Range-
Arena benchmark (Tay et al., 2020b) with a comparison to the vanilla transformer."
M,0.9601449275362319,"ListOps is a ten-way classiﬁcation task which aims to prediction the results of a sequence with a
hierarchical structure and operators MAX, MEAN, MEDIAN and SUM MOD that are enclosed by
delimiters (brackets). The network needs to access all tokens and model the logical structure of the
inputs in order to make a prediction."
M,0.9637681159420289,"Document Retrieval task is to decide whether the two input long documents are similar or not with
a binary label. This task evaluates a model’s ability to encode and store compressed representations
that are useful for matching and retrieval. Since the samples in LRA are too long, We substantially
shorten some selected samples and display them as below:"
M,0.967391304347826,Listops:
M,0.9710144927536232,"1 Input: ( ( ( ( ( ( ( [MED 7 ) 9 ) 3 ) 1
...... 5 ) 6 ) 8 ) ] ) ) 2 )
8 ) 9 ) 5 ) 0 ) ] ) ) 8 ) 5 ) 1 ) 2 ) ] )
Our Output: 0,
Transformer output: 9, Ground-truth: 0
2
3 Input: ( ( ( ( ( ( ( ( ( [SM 5 ) 6 ) 0 ) 7 ) 1 ) ( ( ( ( ( (...... ( ("
M,0.9746376811594203,"( Input: ( ( [MIN 5 ) 8 ) 1 ) 0 ) (( [MED ( ( ( 8 ) 7 ) 2 ) 8 ) 1
) 8 ) ] ) ) 7 ) ] )] ) Our output: 9, Transformer output: 3,
Ground-truth: 9
4
5 Input: ( ( ( ( ( ( ( ( ( [MAX 7 ) 4 ) 8 ) ( ( ( ( ( ( ( ( ( ( ( [MAX 5"
M,0.9782608695652174,") 2 ) ( ( ( ( ( ( [SM 3 ) 6 ) 9 ) ( ( ( ...... ) ) 1 ) 6 ) 4 ) 2
) ] ) ) ] ) Our output: 9, Transformer output: 5, Ground-truth: 9"
M,0.9818840579710145,Listing 1: Examples of LisOps
M,0.9855072463768116,Byte-level document retrieval:
M,0.9891304347826086,1 Text1: b’1 Introduction Recent advances in Statistical Machine
M,0.9927536231884058,"Translation (SMT) are widely centred around two concepts: (a)
hierarchical translation processes, frequently employing
Synchronous Context Free Grammars (SCFGs) and (b) transduction or
synchronous rewrite processes over a linguistic ......
2
3 Text2: b’1 Introduction
Automatic Grammatical Error Correction (GEC)
for non-native English language learners has attracted more and
more attention with the development of natural language processing
, machine
learning and big-data techniques. ?The CoNLL2013 shared
task focuses on the problem of GEC
in five different error types
including determiner,
preposition, noun number......
4
5 Our output: False, Transformer output: True, Ground-truth: False"
M,0.9963768115942029,Listing 2: Examples of Document Retrieval
