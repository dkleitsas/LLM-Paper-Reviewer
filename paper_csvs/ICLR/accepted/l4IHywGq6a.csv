Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029498525073746312,"The problem of molecular generation has received signiﬁcant attention recently.
Existing methods are typically based on deep neural networks and require train-
ing on large datasets with tens of thousands of samples. In practice, however, the
size of class-speciﬁc chemical datasets is usually limited (e.g., dozens of sam-
ples) due to labor-intensive experimentation and data collection. This presents
a considerable challenge for the deep learning generative models to comprehen-
sively describe the molecular design space. Another major challenge is to gen-
erate only physically synthesizable molecules. This is a non-trivial task for neu-
ral network-based generative models since the relevant chemical knowledge can
only be extracted and generalized from the limited training data. In this work,
we propose a data-efﬁcient generative model that can be learned from datasets
with orders of magnitude smaller sizes than common benchmarks. At the heart
of this method is a learnable graph grammar that generates molecules from a
sequence of production rules. Without any human assistance, these production
rules are automatically constructed from training data. Furthermore, additional
chemical knowledge can be incorporated in the model by further grammar op-
timization. Our learned graph grammar yields state-of-the-art results on gener-
ating high-quality molecules for three monomer datasets that contain only ∼20
samples each. Our approach also achieves remarkable performance in a chal-
lenging polymer generation task with only 117 training samples and is com-
petitive against existing methods using 81k data points.
Code is available at
https://github.com/gmh14/data_efficient_grammar."
INTRODUCTION,0.0058997050147492625,"1
INTRODUCTION"
INTRODUCTION,0.008849557522123894,"The rise of computational approaches has started to have a signiﬁcant impact on the discovery of ma-
terials and drugs. Recent advances in machine learning, especially deep learning (DL), have driven
rapid development on generating novel molecular structures (Maziarka et al., 2020; Xu et al., 2019;
Hoffman et al., 2022).Various forms of generative models including generative adversarial networks
(De Cao & Kipf, 2018; Maziarka et al., 2020), variational autoencoders (VAEs) (Jin et al., 2018;
2020; Liu et al., 2018; Sattarov et al., 2019), and reinforcement learning (You et al., 2018) have been
exploited to represent the complicated molecular design space and generate new molecules. They
typically formulate molecular generation as a problem of distribution learning, where the generative
model ﬁrst learns to reproduce the distribution of a training set before generating new molecular
structures. Generative models have also managed to integrate certain chemical constraints (e.g.,
valency restrictions) (Jin et al., 2018; Liu et al., 2018) and shown promising results on the com-
mon benchmarks (Irwin et al., 2012; Ramakrishnan et al., 2014). However, DL-based generative
models face a serious limitation: they require large amounts of training data to achieve reasonable
performance."
INTRODUCTION,0.011799410029498525,"In practice, molecule data is not always abundant (Stanley et al., 2021; Altae-Tran et al., 2017;
Subramanian et al., 2016); for instance, the focus may be on a speciﬁc type of molecules fulﬁlling
certain ingredient requirements. Particularly in the context of polymers, large amounts of training
data are not available, and therefore DL models use manually constructed or generated data (St. John
et al., 2019; Jin et al., 2020; Ma & Luo, 2020). Real data sets, as used in one of the state-of-the-art
papers on polyurethane property prediction, have as little as 20 samples (Menon et al., 2019). In
such scenarios, designing a pure DL-based model is challenging."
INTRODUCTION,0.014749262536873156,Published as a conference paper at ICLR 2022
INTRODUCTION,0.017699115044247787,"N
C
O
N C O"
INTRODUCTION,0.02064896755162242,"O
C
N
N
C
O …
… N
C
O"
INTRODUCTION,0.02359882005899705,"O
C
N
N
C
O N C O"
INTRODUCTION,0.02654867256637168,"O
C
N
N
C
O N C O …"
INTRODUCTION,0.029498525073746312,Bottom-up
INTRODUCTION,0.032448377581120944,Search
INTRODUCTION,0.035398230088495575,"Grammar
Production"
INTRODUCTION,0.038348082595870206,Evaluation
INTRODUCTION,0.04129793510324484,"Training Data
Graph Grammar
Generated Samples"
INTRODUCTION,0.04424778761061947,"Domain-specific Metrics
Update Edge Weight Function"
INTRODUCTION,0.0471976401179941,"Figure 1: Overview. Given molecules and domain-speciﬁc metrics to be optimized, we construct a
graph grammar, which can serve as a generative model. The graph grammar construction process
automatically learns the grammar rules by optimizing the metrics."
INTRODUCTION,0.05014749262536873,"Recent renewed interest in formal grammars (Kajino, 2019; Krenn et al., 2019; Nigam et al., 2021;
Guo et al., 2021) provides an alternative to pure DL methods. In formal language theory, a grammar
is a set of production rules describing how to generate valid strings according to the language’s syn-
tax. A chemical grammar may thus be considered as an interpretable and compact design model that
simultaneously serves as a molecular representation and a generative model; even domain-speciﬁc
constraints can be explicitly incorporated into the rules. Recent examples range from string-based
(Krenn et al., 2019; Nigam et al., 2021) to hypergraph-based (Kajino, 2019) and polymer-speciﬁc
grammars (Guo et al., 2021).
Grammar-based generative models do not rely on large training
datasets and easily extrapolate to generate molecules outside the distribution of training samples.
Yet, they have two major drawbacks. First, current approaches require that chemical grammars are
manually designed by human experts (Krenn et al., 2019; Nigam et al., 2021; Dai et al., 2018). This
is a tedious process that heavily relies on expertise in chemistry. Moreover, the existing grammars
are also very ﬁne-grained (i.e., rules are mostly attaching atoms) in order to cover the syntax of gen-
eral molecules instead of a speciﬁc dataset. Hence, it is not straightforward how to incorporate the
bias of given data (e.g., certain molecular substructures) into such grammars. The second drawback
is that it remains challenging to integrate chemistry knowledge beyond simple constraints (e.g., va-
lency restrictions) into the grammar. Current grammar construction approaches fail to capture more
abstract or complex aspects such as the diversity or synthesizabilty of the generated molecules,
which constrains their practicality. Solutions such as Kajino (2019) resort to costly optimization on
the level of molecules (or their latent representation) after the grammar is built or learned. For a
more detailed delimitation of related work, see Sec. 2."
INTRODUCTION,0.05309734513274336,"In this paper, we propose a generative model combining complex graph grammar construction with a
relatively simple and effective learning technique. In particular, our grammar incorporates substruc-
tures of varying sizes (i.e., above atom level) and the construction process directly optimizes various
chemical metrics (e.g., distribution statistics and synthesizability) while satisfying speciﬁc chemical
constraints (e.g., valency restrictions). Moreover, we have the beneﬁts of symbolic knowledge repre-
sentation: explainability and data efﬁciency. Our evaluation focuses on polymers, particularly their
monomer building blocks. Thus, we curate new, realistic polymer datasets gathered from literature
that represent speciﬁc classes of monomers. Note that our model works for arbitrary molecules."
INTRODUCTION,0.05604719764011799,"Framework. Figure 1 outlines our approach. Given molecules and domain-speciﬁc metrics to be
optimized, we iteratively construct and evaluate a graph grammar as our generative model. We
consider the construction as a minimum spanning forest problem and combine it with optimization
of the metrics, via a learnable function Fθ determining which rules to construct."
INTRODUCTION,0.058997050147492625,"Results. Our model successfully deals with extreme settings – from a DL perspective – learning
meaningful production rules based on only ∼10 samples (e.g., of a speciﬁc class); this is of signif-
icant importance in a practical setting (Stanley et al., 2021; Altae-Tran et al., 2017) but has been
ignored so far. In particular, our model is able to generate members of a speciﬁc monomer class
with a decent success rate. No previous state-of-the-art system achieves similar performance in our
experiments. Generally, our approach works on training data that is orders of magnitude smaller
than the amount needed by DL-based systems to produce meaningful results. Given ∼100 samples,
we achieve performance comparable to state-of-the-art systems trained on 81k samples, across a
wide range of common and new evaluation metrics. Besides, our grammar optimization method
can adjust to any user-deﬁned domain-speciﬁc metrics. The learned grammars can capture domain-
speciﬁc knowledge explicitly, e.g., the characteristic functional groups of a given class of polymers
can be extracted from the production rules."
INTRODUCTION,0.061946902654867256,Published as a conference paper at ICLR 2022
RELATED WORK,0.06489675516224189,"2
RELATED WORK"
RELATED WORK,0.06784660766961652,"Generative Models for Molecules. Existing models can be categorized based on their molecule
representation. It is common to use SMILES strings (Weininger, 1988). Yang et al. (2017), Olive-
crona et al. (2017), Chenthamarakshan et al. (2020), Schiff et al. (2021) and Gómez-Bombarelli
et al. (2018) use recurrent neural networks to generate the strings in a sequential manner. For in-
stance, Gómez-Bombarelli et al. (2018) propose a VAE based on sequence-to-sequence encoders
and decoders. Guimaraes et al. (2017), Sanchez-Lengeling et al. (2017), and Dai et al. (2018) addi-
tionally apply optimization objectives to enforce similarity or semantic correctness to generate valid
SMILES. Alternatively, molecules can be treated as graphs. Simonovsky & Komodakis (2018), Ma
et al. (2018), and De Cao & Kipf (2018) generate the graphs in a single step, directly outputting
adjacency matrices and node labels. GraphNVP (Madhawa et al., 2019) uses two steps to generate
the node labels based on the adjacency matrix; it exploits a model to encode molecules and uses
the same model whose layers are reversed to generate new molecules. You et al. (2018); Li et al.
(2018); Samanta et al. (2020); Liu et al. (2018); Liao et al. (2019); Jin et al. (2018; 2020) build
molecule graphs iteratively based on either atom nodes or substructures. For example, JT-VAE (Jin
et al., 2018) ﬁrst generates a tree and then expands some of its nodes by attaching substructures
based on a vocabulary mined from given molecules; Similarly, HierVAE (Jin et al., 2020) also uses
a vocabulary, but improves upon the combinatorial attachment process by directly applying a multi-
resolution representation, allowing for generating much larger and diverse molecules. All these
methods depend on DL and require large training datasets. Moreover, the resulting models are usu-
ally not interpretable. Our approach alleviates this obstacle and can deal with practically common
setting of only dozens of available data points."
RELATED WORK,0.07079646017699115,"Generative Models using Graph Grammars. Our approach belongs to the class of models generat-
ing new graphs based on the production rules of a graph grammar. These models are non-parametric,
interpretable, and can incorporate meaningful graph properties into the rules. One option is to use
grammars that are manually designed by human experts (Dai et al., 2018; Nigam et al., 2021). For
instance, STONED (Nigam et al., 2021) generates new molecules based on a string-based represen-
tation of given molecules by replacing tokens (representing atoms) in accordance with the SELFIES
grammar (Krenn et al., 2019). While the simplicity of this approach is attractive, the generated
molecules are not necessary reasonable from a chemical perspective (e.g., they might not be synthe-
sizable). This problem can be overcome by mining the grammar from real data. Recently, several
automatic techniques that explicitly construct a graph grammar have been proposed in the context
of large-scale graphs to facilitate understanding and analysis (Sikdar et al., 2019; Aguinaga et al.,
2018; Hibshman et al., 2019). These works are domain-independent and do not allow specialization
of the constructed grammar to reﬂect domain-speciﬁc knowledge. Closest to our method is MHG
(Kajino, 2019), a framework that constructs a molecular hyperedge-replacement grammar based on
Aguinaga et al. (2018). From the given data, MHG learns a ﬁne-grained grammar where the rules
iteratively attach single atoms and therefore incorporates hard chemical constraints (e.g., valency
restrictions). MHG then applies a VAE conditioned on the grammar to the molecules’ tree decom-
positions in order to also learn soft constraints (e.g., stability). Finally, it uses Bayesian optimization
to guide molecule generation. The ﬁne-grained rules in MHG allows for rich diversity. However,
our experiments show that these rules make it hard to capture distribution-speciﬁc properties, e.g.,
characteristic substructures of a speciﬁc molecular class. Such a drawback limits the practical use of
MHG. In contrast, our approach focuses on subgraphs instead of individual atoms only. It directly
incorporates domain-speciﬁc knowledge into grammar construction and therefore avoids compli-
cated post-processing to achieve high generation performance."
PRELIMINARIES,0.07374631268436578,"3
PRELIMINARIES"
PRELIMINARIES,0.07669616519174041,"Molecular Hypergraph. Molecules can naturally be represented as graphs by taking the atoms as
nodes and the bonds as edges. We particularly use a hypergraph representation. Given a molecule
M, the hypergraph HM = (V, EH) consists of a set of nodes V and a set of hyperedges EH; a
hyperedge can join more than two nodes. Given a regular molecular graph, we construct a hyper-
graph by including all nodes and adding hyperedges as follows: a hyperedge is added for each bond
that joins only two nodes, and for each individual ring (including aromatic ones) that joins all nodes
(more than 2) in the ring. Consider Figure 2(a) where we add two hyperedges of the latter kind, one
for each ring."
PRELIMINARIES,0.07964601769911504,"Published as a conference paper at ICLR 2022 N C O N
C
O 2×
2×"
PRELIMINARIES,0.08259587020648967,Hyperedge
PRELIMINARIES,0.0855457227138643,Double Bond
PRELIMINARIES,0.08849557522123894,Single Bond
PRELIMINARIES,0.09144542772861357,Aromatic Bond
PRELIMINARIES,0.0943952802359882,Oxygen Atom Node
PRELIMINARIES,0.09734513274336283,Carbon Atom Node
PRELIMINARIES,0.10029498525073746,Nitrogen Atom Node
PRELIMINARIES,0.10324483775811209,Anchors
PRELIMINARIES,0.10619469026548672,Initial Node
PRELIMINARIES,0.10914454277286136,Non-terminal Node
PRELIMINARIES,0.11209439528023599,"(a) Hypergraph representation of naphthalene diisocyanate
(b) Production rules of a learned graph grammar"
PRELIMINARIES,0.11504424778761062,(c) Generation process of naphthalene diisocyanate using the graph grammar in (b) ’ ’ ’ ’
PRELIMINARIES,0.11799410029498525,"Figure 2: Examples of a molecular hypergraph, one of our possible graph grammars for it, and an
application of the grammar for generating new molecules."
PRELIMINARIES,0.12094395280235988,"Formal Grammar. A grammar G = (N, Σ, P, X) has a ﬁnite set N of non-terminal symbols, an
initial symbol X, and a ﬁnite set of terminal symbols Σ. It describes how to build strings from a
language’s alphabet using a set of production rules P = {pi|i = 1, ..., k} of form pi : LHS →RHS,
where LHS is short for left-hand side and RHS for right-hand side. Based on such a grammar, a
string (of terminal symbols) is generated starting at X, by iteratively selecting rules whose left-
hand side matches a non-terminal symbol inside the current string and replacing that with the rule’s
right-hand side, until the string does not contain any non-terminals."
PRELIMINARIES,0.12389380530973451,"Minimum Spanning Forest (MSF). A minimum spanning tree (MST) is a subset of the edges of
a connected, edge-weighted undirected graph that connects all the vertices, without any cycles and
with the minimum possible total edge weight. Minimum spanning forest is the union of MSTs for a
set of unconnected edge-weighted undirected graphs."
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.12684365781710916,"4
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION"
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.12979351032448377,"Graph Grammar. We focus on a formal grammar over molecule graphs instead of strings, a graph
grammar. As shown in Figure 2(b), both left and right-hand side of each production rule are graphs.
These graphs contain non-terminal nodes (shadowed squares) and terminal nodes (colored circles),
representing atoms. The white nodes are anchor nodes, which do not change from the left to the
right-hand side. Molecule graph generation based on a graph grammar is analogous to the one with
string-based grammars described in Sec. 3 (see also Figure 2(c)). To determine if a production rule
pi is applicable at each step, we use subgraph matching (Gentner, 1983) to test whether the current
graph contains a subgraph that is isomorphic to the rule’s left-hand side. Since the subgraphs are
usually small in scale, the matching process is efﬁcient in practice."
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.13274336283185842,"Overall Pipeline. As shown in Figure 1, the input to our algorithm consists of a set of molecu-
lar structures and a set of evaluation metrics (e.g., diversity and synthesizablity). The goal is to
learn a graph grammar which can be used for molecule generation. To this end, we ﬁrst consider
each molecule as a hypergraph. The grammar construction is a bottom-up procedure, which iter-
atively creates production rules by contracting hyperedges (shown in Figure 3). The hyperedges
to contract are determined by a parameterized function Fθ, implemented as a neural network. We
simultaneously perform multiple randomized searches to obtain multiple grammars which are eval-
uated with respect to the input metrics. This approach learns how to create a grammar that samples
molecules maximizing the input metrics. Hence, domain-speciﬁc knowledge can be incorporated to
the grammar-based generative model. The grammar construction and learning process are described
in detail in the following Sec. 4.1 and 4.2. The ﬁnal molecule generation is detailed in Appendix A."
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.13569321533923304,Published as a conference paper at ICLR 2022
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.13864306784660768,Sample …
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.1415929203539823,Potential Function
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.14454277286135694,Potential Function
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.14749262536873156,Constructed Graph Grammar
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.1504424778761062,Potential Function
GRAPH GRAMMAR LEARNING WITH DOMAIN-SPECIFIC OPTIMIZATION,0.15339233038348082,"Figure 3:
Overview of bottom-up grammar construction. We optimize the iterative, bottom-up
grammar construction by learning how to create a grammar that samples molecules ﬁtting input
metrics. Speciﬁcally we learn which edges to select for contraction in each iteration step using a
neural network Fθ. We perform this construction on all input molecules simultaneously."
BOTTOM-UP GRAMMAR CONSTRUCTION,0.15634218289085547,"4.1
BOTTOM-UP GRAMMAR CONSTRUCTION"
BOTTOM-UP GRAMMAR CONSTRUCTION,0.1592920353982301,"We describe at a high-level our grammar construction approach (shown in Figure 3). A bottom-up
search builds up production rules from the ﬁnest-grained level that comprises individual hyperedges
in the molecular hypergraph. We construct a grammar by iteratively sampling a set of hyperedges
and contracting them into an individual node. The sampling algorithm is described in more detail in
Sec. 4.2. For each contraction step, a production rule is constructed and added to the grammar and
we obtain a new hypergraph with fewer nodes and edges. We simultaneously perform the hyperedge
selection and rule construction for all input molecules until all hyperedges are contracted. Without
the loss of generality, we describe the construction process for a single molecule."
BOTTOM-UP GRAMMAR CONSTRUCTION,0.16224188790560473,"At iteration t, we consider the current graph HM,t = (V, EH) and sample m hyperedges E∗
t =
{e(i)
t
∈EH|i = 1, ..., m}. Let V ∗
t
= {v(i)
t
∈V |i = 1, ..., n} be the nodes joined by these
hyperedges. We then extract all connected components with respect to these hyperedges. Next, we
convert each connected component H(i)
sub,t = (V (i)
sub,t, E(i)
sub,t) into a production rule. The anchor
nodes are those nodes from V that are connected to nodes from H(i)
sub,t in the original graph HM but
that do not occur in H(i)
sub,t themselves. We also provide notation for the relevant edges:"
BOTTOM-UP GRAMMAR CONSTRUCTION,0.16519174041297935,"V (i)
anc,t = {v|(s, v) ∈EH, s ∈V (i)
sub,t, v /∈V (i)
sub,t},
E(i)
anc,t = {(s, v)|s ∈V (i)
anc,t, v ∈V (i)
sub,t}."
BOTTOM-UP GRAMMAR CONSTRUCTION,0.168141592920354,Then we construct the production rule pi : LHS →RHS with non-terminal node R∗on the left:
BOTTOM-UP GRAMMAR CONSTRUCTION,0.1710914454277286,"LHS := H(VL, EL), VL = {R∗} ∪V (i)
anc,t , EL = {(R∗, v)|v ∈V (i)
anc,t} ,"
BOTTOM-UP GRAMMAR CONSTRUCTION,0.17404129793510326,"RHS := H(VR, ER), VR = V (i)
sub,t ∪V (i)
anc,t , ER = E(i)
anc,t ∪E(i)
sub,t.
(1)"
BOTTOM-UP GRAMMAR CONSTRUCTION,0.17699115044247787,"When all connected components have been converted into production rules, we update the original
hypergraph to HM,t+1 by replacing each connected component with the non-terminal node R∗. The
above process continues until the hypergraph only consists of one single non-terminal node. For this
ﬁnally constructed rule, we use the initial node X instead of R∗on the left-hand side."
BOTTOM-UP GRAMMAR CONSTRUCTION,0.17994100294985252,"Our proposal has several properties: (1) As a generative model, the grammar can reproduce all input
molecules. (2) Since the production rules are constructed from subgraphs of real molecules, valency
conditions are naturally obeyed and all generated molecules are thus valid. (3) The generation not
only interpolates the training data but also extrapolates to generate molecular structures outside the
distribution of previously seen examples. (4) The constructed grammar roughly follows Chomsky
normal form (Chomsky, 1959) and hence is easy to parse and can serve for explanation."
BOTTOM-UP GRAMMAR CONSTRUCTION,0.18289085545722714,Published as a conference paper at ICLR 2022
OPTIMIZING GRAMMAR CONSTRUCTION,0.18584070796460178,"4.2
OPTIMIZING GRAMMAR CONSTRUCTION"
OPTIMIZING GRAMMAR CONSTRUCTION,0.1887905604719764,"So far, we have left open a critical question: how to optimize the grammar for the input metrics?
Since the grammar construction is completely determined by the sequence of hyperedge sets se-
lected, we can convert the optimization of the grammar into the optimization of the hyperedge se-
lection sequence; in particular, note that there are no hyperparameters in addition. Thus, the variable
of the optimization problem is the selection sequence with objective to maximize the input metrics."
OPTIMIZING GRAMMAR CONSTRUCTION,0.19174041297935104,"We formulate the search for a sequence of hyperedge sets as an MSF problem. The bottom-up
grammar construction process can be considered as search for a spanning forest for all input graphs.
Note that instead of the structure of MSF itself, we focus on the order of hyperedges added to the
MSF. The order of hyperedges is determined by an edge-weight function F : EH →R mapping each
hyperedge in every considered molecule hypergraphs to a scalar value. Optimizing the hyperedge
selection is equivalent to optimizing the edge weight function. Note that this kind of function is
similar to the concept of potential function in the ﬁeld of graphical models (Barber, 2012). Our goal
here is to learn a potential function F(·) that maximizes the given evaluation metrics."
OPTIMIZING GRAMMAR CONSTRUCTION,0.19469026548672566,"We deﬁne our edge weight function as a parameterized function of hyperedge features as F(e; θ) =
Fθ(f(e)), where f(·) is a feature extractor function for individual hyperedges e, and θ are the
parameters of F(·) to be optimized. There is no speciﬁc restriction on the choice of f(·) so we
use a pretrained neural network in our experiments. Our optimization objective is maxθ
P"
OPTIMIZING GRAMMAR CONSTRUCTION,0.1976401179941003,"i λiMi,
where Mi is the value of the i-th input grammar metric, and λi is the weight of the i-th metric.
Since most grammar metrics can only be obtained by evaluating a set of molecules generated by the
grammar, it is impossible to get the gradient of Mi with respect to the parameters θ via the chain
rule. We address this problem by formulating the process of MSF construction as Monte Carlo
(MC) sampling. To obtain the gradient of non-differentiable evaluation metrics, we draw inspiration
from task-based learning (Donti et al., 2017; Chen et al., 2019) and reinforcement learning, by using
REINFORCE (Williams, 1992) to optimize the potential function. Speciﬁcally, we deﬁne a random
variable X : Ω→{0, 1} on each hyperedge, where X = 1 (X = 0) means the hyperedge is (not)
selected. In each iteration of grammar construction, X follows a Bernoulli distribution, based on the
probability φ(e; θ) to sample e:
X ∼Bernoulli(φ(e; θ)),
φ(e; θ) = P(X = 1) = σ(−Fθ(f(e))),
(2)
where σ(·) is the sigmoid function. For φ(e; θ), the larger the weight of edge e, the lower the proba-
bility of e being sampled in the current iteration, which matches the target of MSF. We sample X for
each hyperedge and construct grammar production rules iteratively as described in Sec. 4.1. Sup-
pose that the constructed grammar is G. Since G is determined by the sampled order of hyperedges,
we have p(G) = p(C(X)) = p(X) = Q
t
Q
j φ(e(j)
t ; θ)X(j)
t (1 −φ(e(j)
t ; θ))1−X(j)
t , where C(·) is"
OPTIMIZING GRAMMAR CONSTRUCTION,0.20058997050147492,"grammar construction process, e(j)
t
is the j-th hyperedge selected in the t-th iteration, and X is the
concatenation of selected hyperedges along all iterations. The optimization objective is:"
OPTIMIZING GRAMMAR CONSTRUCTION,0.20353982300884957,"max
θ
EX
h X"
OPTIMIZING GRAMMAR CONSTRUCTION,0.20648967551622419,"i
λiMi(X)
i
.
(3)"
OPTIMIZING GRAMMAR CONSTRUCTION,0.20943952802359883,"We estimate the expectation with MC sampling and REINFORCE, approximating the gradient with
respect to θ as:"
OPTIMIZING GRAMMAR CONSTRUCTION,0.21238938053097345,"∇θEX
h X"
OPTIMIZING GRAMMAR CONSTRUCTION,0.2153392330383481,"i
λiMi(X)
i
=
Z X X"
OPTIMIZING GRAMMAR CONSTRUCTION,0.2182890855457227,"i
λi∇θp(X)Mi(X)"
OPTIMIZING GRAMMAR CONSTRUCTION,0.22123893805309736,"= EX
X"
OPTIMIZING GRAMMAR CONSTRUCTION,0.22418879056047197,"i
λi∇θ log(p(X))Mi(X)dX ≈1 N N
X n=1 X"
OPTIMIZING GRAMMAR CONSTRUCTION,0.22713864306784662,"i
λi∇θ log(p(X))Mi(X). (4)"
OPTIMIZING GRAMMAR CONSTRUCTION,0.23008849557522124,"We then apply gradient ascent in order to maximize the objective. Note that Mi(X) is normalized
to zero mean for each sampling batch to reduce variance in training. The grammar construction,
evaluation, and optimization process is repeated until θ converges or the iteration number exceeds
a preset limit. Our approach reduces the complexity of the optimization variable space from being
combinatorial (all possible orderings of selected hyperedges) to the dimension of the parameter θ.
Such complexity is also much lower than other deep neural network generative models that are
directly trained on the molecule dataset."
OPTIMIZING GRAMMAR CONSTRUCTION,0.23303834808259588,Published as a conference paper at ICLR 2022
EVALUATION,0.2359882005899705,"5
EVALUATION"
EVALUATION,0.23893805309734514,Our evaluation investigates the following ﬁve questions:
EVALUATION,0.24188790560471976,"• How do SOTA models for molecule generation perform on realistic small monomer datasets?
• Is our approach effective in generating speciﬁc types of monomers that are synthesizable?
• How do the models perform on larger monomer datasets?
• Can our approach learn to weigh and optimize different metrics according to user needs?
• Can our grammar’s explainability support applications, such as functional group extraction?"
EXPERIMENT SETUP,0.2448377581120944,"5.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.24778761061946902,"Data. We use three small datasets, each representing a speciﬁc class of monomers, which we curate
manually from the literature: Acrylates, Chain Extenders, and Isocyanates, containing only 32, 11,
and 11 samples, respectively (printed in Appendix G). For comparison and for pretraining baselines,
we also use a large collection of 81k monomers from St. John et al. (2019) and Jin et al. (2020).1"
EXPERIMENT SETUP,0.25073746312684364,"Evaluation Metrics. We consider commonly used metrics in the literature (Polykovskiy et al.,
2020) and new ones for assessing both individual sample quality and distribution similarity:"
EXPERIMENT SETUP,0.2536873156342183,"• Validity/Uniqueness/Novelty: Percentage of chemically valid/unique/novel molecules.
• Diversity: Average pairwise molecular distance among generated molecules, where the molecular
distance is deﬁned as the Tanimoto distance over Morgan ﬁngerprints (Rogers & Hahn, 2010).
• Chamfer Distance (Fan et al., 2017): Distance between two sets of molecules, wherein the usual
pairwise Euclidean distance is replaced by the Tanimoto distance.
• Retro∗Score (RS): Success rate of the Retro∗model (Chen et al., 2020) which was trained to
ﬁnd a retrosynthesis path to build a molecule from a list of commercially available ones2.
• Membership: Percentage of molecules belonging to the training data’s monomer class."
EXPERIMENT SETUP,0.25663716814159293,"We propose the last three as new metrics. We deﬁne the Chamfer Distance for molecules to account
for “external” diversity between generated data and training data as quantitative indicator of the
generative model’s extrapolation ability. Ideally, generated molecules should not be too close to
any training molecule. Hence, a larger distance is more desired. Note that the commonly used
metrics of existing distances only focus on similarity to the training data. Furthermore, we include
the Retro∗Score as a metric estimating sample synthesizability since the commonly used SA Score
(Ertl & Schuffenhauer, 2009) does not adequately assess more recent molecules (Polykovskiy et al.,
2020). For our small datasets, we check class membership as well. Since a polymer’s class is usually
determined by its monomer types, it is essential that a generative model can generate class members."
EXPERIMENT SETUP,0.25958702064896755,"Baselines.
We compare to various approaches:
GraphNVP, JT-VAE, HierVAE, MHG, and
STONED; for descriptions see Sec. 2.3 We call our method DEG, short for Data-Efﬁcient Graph
Grammar. Appendix B provides the implementation details."
EXPERIMENT SETUP,0.26253687315634217,"5.2
RESULTS ON SMALL, CLASS-SPECIFIC POLYMER DATA"
EXPERIMENT SETUP,0.26548672566371684,"Results. Table 1 shows the results on the Isocyanate data; due to lack of space the other two tables
are in Appendix C.1. Observe that GraphNVP has a rather poor performance. The VAEs and existing
grammar-based systems perform reasonably well on some metrics, but have low scores on the RS
and Membership metrics. In contrast, our method signiﬁcantly outperforms the other methods in
terms of Membership and Retro∗Score on all three datasets. It also achieves the best or comparable
performance on all other metrics."
EXPERIMENT SETUP,0.26843657817109146,"Discussion. Our evaluation shows that learning on monomer datasets, especially when the dataset
size is small, is much more challenging than larger datasets as used in the related work (Irwin et al.,
2012; Ramakrishnan et al., 2014). GraphNVP shows poor performance since it uses molecular
graph adjacency matrix as model input, which is extremely sparse for our relatively large monomer
structures. The vocabulary-based JT-VAE and HierVAE perform reasonably well. However, when"
EXPERIMENT SETUP,0.2713864306784661,"1https://github.com/wengong-jin/hgraph2graph
2https://downloads.emolecules.com/
3We also tested other generative models (De Cao & Kipf, 2018; Dai et al., 2018) but did not get any
meaningful result over our monomer datasets, possibly because these methods are tailored to smaller molecules."
EXPERIMENT SETUP,0.2743362831858407,Published as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.27728613569321536,"Table 1: Results on Isocyanates, best bold, second-best underlined; we omit Novelty since all meth-
ods achieved 100%; the few valid molecules generated by GraphNVP did not allow for reasonable
evaluation on some metrics (–)."
EXPERIMENT SETUP,0.28023598820059,"Method
Valid
Unique
Div.
Chamfer
RS
Memb."
EXPERIMENT SETUP,0.2831858407079646,"Train data
100%
100%
0.61
0.00
100%
100%"
EXPERIMENT SETUP,0.2861356932153392,"GraphNVP
0.16%
–
–
–
0.00%
0.00%"
EXPERIMENT SETUP,0.2890855457227139,"JT-VAE
100%
5.8%
0.72
0.85
5.50%
66.5%"
EXPERIMENT SETUP,0.2920353982300885,"HierVAE
100%
99.6%
0.83
0.76
1.85%
0.05%"
EXPERIMENT SETUP,0.2949852507374631,"MHG
100%
75.9%
0.88
0.83
2.97%
12.1%"
EXPERIMENT SETUP,0.29793510324483774,"STONED
100%
100%
0.85
0.86
5.63%
79.8%"
EXPERIMENT SETUP,0.3008849557522124,"DEG
100%
100%
0.86
0.87
27.2%
96.3%"
EXPERIMENT SETUP,0.30383480825958703,"the dataset is small, JT-VAE is not able to mine a vocabulary that would allow it to generate many
unique molecules. The more diverse vocabulary of HierVAE clearly solves this shortcoming, but
the low Membership score shows that it does not capture monomer class speciﬁcs. In general,
grammar-based methods can better capture class-speciﬁc molecule characteristics than DL-based
methods and have higher Membership scores. However, they perform poorly on RS. Speciﬁcally,
MHG has ﬁne-grained rules that simply attaches atoms. This leads to high diversity but the rules
hardly capture domain-speciﬁc characteristics. The STONED method iteratively replaces atoms
based on the SELFIES grammar and only performs interpolation and exploration based on training
data, making it hard to embed build-in domain-speciﬁc knowledge in the generative model. The
overall results show that: 1) Our learned, substructure-based grammar successfully captures class
speciﬁcs – a critical evaluation criterion which has been ignored so far. 2) Other critical, domain-
speciﬁc metrics such as RS can successfully be optimized during grammar learning. Our score is
≥5× higher than the others. More importantly, the optimization is done in-situ during grammar
construction, and hence it avoids post-processing. 3) Our method is the only one that constantly
achieves stable performance. Altogether, these results clearly differentiate us from the others."
RESULTS ON LARGE POLYMER DATASET,0.30678466076696165,"5.3
RESULTS ON LARGE POLYMER DATASET"
RESULTS ON LARGE POLYMER DATASET,0.30973451327433627,"Our method is developed to model speciﬁc classes of complex molecules (e.g., classes of differ-
ent monomers or polymers) and is expected to deal with most practical scenarios with only a few
dozen samples for training. However, as mentioned above, monomer data itself is different from the
molecules used in related works. Therefore, we also investigate how our method performs on large
monomer datasets comparing with existing methods.4 Since our approach is relatively complex, but
more data-efﬁcient, we apply it to a 0.15% subset. Details are provided in Appendix B."
RESULTS ON LARGE POLYMER DATASET,0.31268436578171094,"Results. Table 3 in the appendix shows the results. In summary, some SOTA systems such as
SMILESVAE and GraphNVP fail to capture any distribution speciﬁcs and mostly generate invalid
molecules. JT-VAE and grammar-based baselines (MHG, STONED) perform poorly with respect to
the former but their sample quality is reasonable. HierVAE performs extremely well on all metrics
except Chamfer distance. Our approach can generally compete with the latter (only trained on 0.15%
data) and achieves better sample quality, especially Chamfer distance is twice as high."
RESULTS ON LARGE POLYMER DATASET,0.31563421828908556,"Discussion.
Monomer data turns out to be much more challenging compared to the common
datasets. Generally, DL-based methods achieve better performance in terms of distribution statistics,
while grammar-based models (including ours) have better sample quality. It is reasonable since DL-
based methods are all based on distribution learning while grammar-based methods are more focused
on modeling chemical rules. Our approach is the only grammar-based system performing well on
distribution statistics, which highlights the importance of grammar construction. Fine-grained gram-
mars that either iteratively attaches single atoms (MHG), or only performs input data interpolation
(STONED) cannot ﬁt training data more speciﬁcally. Our sample quality is among the best. Fur-"
RESULTS ON LARGE POLYMER DATASET,0.3185840707964602,"4The data was also used in Jin et al. (2020), but no grammar-based systems were compared."
RESULTS ON LARGE POLYMER DATASET,0.3215339233038348,Published as a conference paper at ICLR 2022
RESULTS ON LARGE POLYMER DATASET,0.32448377581120946,RS (%)
RESULTS ON LARGE POLYMER DATASET,0.3274336283185841,Diversity 0.50 0.75
RESULTS ON LARGE POLYMER DATASET,0.3303834808259587,"0.76
0.92"
RESULTS ON LARGE POLYMER DATASET,0.3333333333333333,"Diversity
0.78
0.92"
RESULTS ON LARGE POLYMER DATASET,0.336283185840708,RS (%) 0.05 0.35
RESULTS ON LARGE POLYMER DATASET,0.3392330383480826,"O
C
N
N
C
O
O
N
C
O"
RESULTS ON LARGE POLYMER DATASET,0.3421828908554572,"O
C
N
N
C
O O O
O O O O"
RESULTS ON LARGE POLYMER DATASET,0.34513274336283184,(a) Analysis on Isocyanate dataset
RESULTS ON LARGE POLYMER DATASET,0.3480825958702065,(b) Analysis on Acrylate dataset
RESULTS ON LARGE POLYMER DATASET,0.35103244837758113,"O
C
N
N
C
O O O"
RESULTS ON LARGE POLYMER DATASET,0.35398230088495575,"= 0.5,
= 3"
RESULTS ON LARGE POLYMER DATASET,0.35693215339233036,"= 1.5,
= 1"
RESULTS ON LARGE POLYMER DATASET,0.35988200589970504,"= 0,
= 4
= 0.25,
= 3.5"
RESULTS ON LARGE POLYMER DATASET,0.36283185840707965,"= 0.75,
= 2.5
= 1,
= 2
= 1.25,
= 1.5"
RESULTS ON LARGE POLYMER DATASET,0.36578171091445427,"= 1.75,
= 0.5
= 2,
= 0"
RESULTS ON LARGE POLYMER DATASET,0.3687315634218289,"= 0.5,
= 3"
RESULTS ON LARGE POLYMER DATASET,0.37168141592920356,"= 1.5,
= 1"
RESULTS ON LARGE POLYMER DATASET,0.3746312684365782,"= 0,
= 4
= 0.25,
= 3.5"
RESULTS ON LARGE POLYMER DATASET,0.3775811209439528,"= 0.75,
= 2.5
= 1,
= 2
= 1.25,
= 1.5"
RESULTS ON LARGE POLYMER DATASET,0.3805309734513274,"= 1.75,
= 0.5
= 2,
= 0"
RESULTS ON LARGE POLYMER DATASET,0.3834808259587021,"Figure 4: Left: Analysis of balance factor λ. We choose 9 different combinations of λi for two
optimization objectives: Diversity and RS, showing a clear trade-off between the two objectives.
Right: Examples generated by our learned graph grammar. Our graph grammar can generate novel
complex molecular structures that do not exist in the training dataset (e.g., cyclooctane)."
RESULTS ON LARGE POLYMER DATASET,0.3864306784660767,"thermore, we also show that with more training data (0.3% of the whole dataset), our method can
achieve better performance."
ANALYSIS,0.3893805309734513,"5.4
ANALYSIS"
ANALYSIS,0.39233038348082594,"Optimizing for Speciﬁc Metrics, Balance Factor λ. We study the effect of λ weighing the impor-
tance of metrics according to user needs. We choose 9 different combinations for two optimization
objectives: Diversity and RS. λ1 ranges from 0 to 2 with 0.25 as interval, while λ2 ranges from 4 to
0 with 0.5 as interval. Figure 4 depicts results for Isocyanates and Acrylates. We see that λ fulﬁlls
its purpose, as the performance of the two objectives can be well controlled. In our study, we use
λ1 = 1, λ2 = 2 for a balanced trade-off between Diversity and RS."
ANALYSIS,0.3952802359882006,"Explainability Supports Applications, Functional Group Extraction. In Appendix F, we show
production rules of three graph grammars learned from three small polymer datasets. For each
grammar, there is clearly a rule capturing the functional group that characterizes the dataset’s cor-
responding monomer class. For example, p3 is the relevant production rule for Isocyanates. Since
functional groups must be present in all monomers of this type, the relevant rule is easily obtained
by selecting the rule shared by all inputs."
ANALYSIS,0.39823008849557523,"Generated Examples. Figure 4 also shows examples generated using our grammars learned on
Isocyanates and Acrylates. Though we have only 32 and 11 training samples respectively, our graph
grammar can be used to generate novel and complex molecules. For example, cyclooctane is not
contained in the training data but our grammar can generate it by sequentially applying two partial
ring formation rules. For more generated examples on the other datasets, see Appendix C.2."
CONCLUSIONS,0.40117994100294985,"6
CONCLUSIONS"
CONCLUSIONS,0.40412979351032446,"We propose a data-efﬁcient generative model combining graph grammar construction with domain-
speciﬁc optimization. Our grammar incorporates substructures of varying size and the construction
directly optimizes various chemical metrics. Extensive experiments on three small size polymer
datasets and a large polymer dataset demonstrate the effectiveness of our method. Our system is the
only one that is capable of generating monomers in a speciﬁc class with a high success rate. It will
be useful to incorporate property prediction models with our graph grammar for generating superior
molecular candidates for practical use."
CONCLUSIONS,0.40707964601769914,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.41002949852507375,ACKNOWLEDGEMENT
CONCLUSIONS,0.41297935103244837,"This work is supported by the MIT-IBM Watson AI Lab, and its member company, Evonik."
REFERENCES,0.415929203539823,REFERENCES
REFERENCES,0.41887905604719766,"Salvador Aguinaga, David Chiang, and Tim Weninger. Learning hyperedge replacement grammars
for graph generation. IEEE transactions on pattern analysis and machine intelligence, 41(3):
625–638, 2018."
REFERENCES,0.4218289085545723,"Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu, and Vijay Pande. Low data drug discovery
with one-shot learning. ACS central science, 3(4):283–293, 2017."
REFERENCES,0.4247787610619469,"David Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012."
REFERENCES,0.4277286135693215,"G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins.
Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012."
REFERENCES,0.4306784660766962,"Binghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro*: learning retrosynthetic planning
with neural guided a* search. In International Conference on Machine Learning, pp. 1608–1616.
PMLR, 2020."
REFERENCES,0.4336283185840708,"Di Chen, Yada Zhu, Xiaodong Cui, and Carla P Gomes. Task-based learning via task-oriented
prediction network with applications in ﬁnance. arXiv preprint arXiv:1910.09357, 2019."
REFERENCES,0.4365781710914454,"Vijil Chenthamarakshan, Payel Das, Samuel Hoffman, Hendrik Strobelt, Inkit Padhi, Kar Wai Lim,
Benjamin Hoover, Matteo Manica, Jannis Born, Teodoro Laino, et al. Cogmol: target-speciﬁc and
selective drug design for covid-19 using deep generative models. Advances in Neural Information
Processing Systems, 33:4320–4332, 2020."
REFERENCES,0.43952802359882004,"Noam Chomsky. On certain formal properties of grammars. Information and control, 2(2):137–167,
1959."
REFERENCES,0.4424778761061947,"Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoen-
coder for structured data. arXiv preprint arXiv:1802.08786, 2018."
REFERENCES,0.44542772861356933,"Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973, 2018."
REFERENCES,0.44837758112094395,"Priya L Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in stochastic
optimization. arXiv preprint arXiv:1703.04529, 2017."
REFERENCES,0.45132743362831856,"Peter Ertl and Ansgar Schuffenhauer.
Estimation of synthetic accessibility score of drug-like
molecules based on molecular complexity and fragment contributions. Journal of cheminfor-
matics, 1(1):1–11, 2009."
REFERENCES,0.45427728613569324,"Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object recon-
struction from a single image. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 605–613, 2017."
REFERENCES,0.45722713864306785,"Dedre Gentner. Structure-mapping: A theoretical framework for analogy. Cognitive science, 7(2):
155–170, 1983."
REFERENCES,0.46017699115044247,"Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato,
Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven contin-
uous representation of molecules. ACS central science, 4(2):268–276, 2018."
REFERENCES,0.4631268436578171,"Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias,
and Alán Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ) for se-
quence generation models. arXiv preprint arXiv:1705.10843, 2017."
REFERENCES,0.46607669616519176,Published as a conference paper at ICLR 2022
REFERENCES,0.4690265486725664,"Minghao Guo, Wan Shou, Liane Makatura, Timothy Erps, Michael Foshey, and Wojciech Ma-
tusik. Polygrammar: Grammar for digital polymer representation and generation. arXiv preprint
arXiv:2105.05278, 2021."
REFERENCES,0.471976401179941,"Justus Hibshman, Satyaki Sikdar, and Tim Weninger. Towards interpretable graph modeling with
vertex replacement grammars. In 2019 IEEE International Conference on Big Data (Big Data),
pp. 770–779. IEEE, 2019."
REFERENCES,0.4749262536873156,"Samuel C Hoffman, Vijil Chenthamarakshan, Kahini Wadhawan, Pin-Yu Chen, and Payel Das. Opti-
mizing molecules using efﬁcient queries from property evaluations. Nature Machine Intelligence,
4(1):21–31, 2022."
REFERENCES,0.4778761061946903,"Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure
Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265,
2019."
REFERENCES,0.4808259587020649,"John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc:
a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52
(7):1757–1768, 2012."
REFERENCES,0.4837758112094395,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
Junction tree variational autoencoder for
molecular graph generation. In International conference on machine learning, pp. 2323–2332.
PMLR, 2018."
REFERENCES,0.48672566371681414,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs
using structural motifs.
In International Conference on Machine Learning, pp. 4839–4848.
PMLR, 2020."
REFERENCES,0.4896755162241888,"Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In
International Conference on Machine Learning, pp. 3183–3191. PMLR, 2019."
REFERENCES,0.49262536873156343,"Mario Krenn, Florian Häse, A Nigam, Pascal Friederich, and Alán Aspuru-Guzik. Selﬁes: a robust
representation of semantically constrained graphs with an example application in chemistry. arXiv
preprint arXiv:1905.13741, 2019."
REFERENCES,0.49557522123893805,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018."
REFERENCES,0.49852507374631266,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L Hamilton, David Du-
venaud, Raquel Urtasun, and Richard S Zemel. Efﬁcient graph generation with graph recurrent
attention networks. arXiv preprint arXiv:1910.00760, 2019."
REFERENCES,0.5014749262536873,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L Gaunt. Constrained graph vari-
ational autoencoders for molecule design. arXiv preprint arXiv:1805.09076, 2018."
REFERENCES,0.504424778761062,"Ruimin Ma and Tengfei Luo. Pi1m: a benchmark database for polymer informatics. Journal of
Chemical Information and Modeling, 60(10):4684–4690, 2020."
REFERENCES,0.5073746312684366,"Tengfei Ma, Jie Chen, and Cao Xiao.
Constrained generation of semantically valid graphs via
regularizing variational autoencoders. arXiv preprint arXiv:1809.02630, 2018."
REFERENCES,0.5103244837758112,"Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invert-
ible ﬂow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019."
REFERENCES,0.5132743362831859,"Łukasz Maziarka, Agnieszka Pocha, Jan Kaczmarczyk, Krzysztof Rataj, Tomasz Danel, and Michał
Warchoł. Mol-cyclegan: a generative model for molecular optimization. Journal of Cheminfor-
matics, 12(1):1–18, 2020."
REFERENCES,0.5162241887905604,"Aditya Menon, James A Thompson-Colón, and Newell R Washburn. Hierarchical machine learn-
ing model for mechanical property predictions of polyurethane elastomers from small datasets.
Frontiers in Materials, 6:87, 2019."
REFERENCES,0.5191740412979351,"AkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos Passos Gomes, and Alan Aspuru-
Guzik. Beyond generative models: superfast traversal, optimization, novelty, exploration and
discovery (stoned) algorithm for molecules using selﬁes. Chemical science, 2021."
REFERENCES,0.5221238938053098,Published as a conference paper at ICLR 2022
REFERENCES,0.5250737463126843,"Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo
design through deep reinforcement learning. Journal of cheminformatics, 9(1):1–14, 2017."
REFERENCES,0.528023598820059,"Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai
Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark
Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models.
Frontiers in pharmacology, 11:1931, 2020."
REFERENCES,0.5309734513274337,"Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientiﬁc data, 1(1):1–7, 2014."
REFERENCES,0.5339233038348082,"David Rogers and Mathew Hahn. Extended-connectivity ﬁngerprints. Journal of chemical informa-
tion and modeling, 50(5):742–754, 2010."
REFERENCES,0.5368731563421829,"Bidisha Samanta, Abir De, Gourhari Jana, Vicenç Gómez, Pratim Kumar Chattaraj, Niloy Ganguly,
and Manuel Gomez-Rodriguez. Nevae: A deep generative model for molecular graphs. Journal
of machine learning research. 2020 Apr; 21 (114): 1-33, 2020."
REFERENCES,0.5398230088495575,"Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Opti-
mizing distributions over molecular space. an objective-reinforced generative adversarial network
for inverse-design chemistry (organic). 2017."
REFERENCES,0.5427728613569321,"Boris Sattarov, Igor I Baskin, Dragos Horvath, Gilles Marcou, Esben Jannik Bjerrum, and Alexandre
Varnek. De novo molecular design by combining deep autoencoder recurrent neural networks
with generative topographic mapping. Journal of chemical information and modeling, 59(3):
1182–1196, 2019."
REFERENCES,0.5457227138643068,"Yair Schiff, Vijil Chenthamarakshan, Samuel Hoffman, Karthikeyan Natesan Ramamurthy, and
Payel Das. Augmenting molecular deep generative models with topological data analysis rep-
resentations. arXiv preprint arXiv:2106.04464, 2021."
REFERENCES,0.5486725663716814,"Michael D Shultz. Two decades under the inﬂuence of the rule of ﬁve and the changing properties of
approved oral drugs: miniperspective. Journal of medicinal chemistry, 62(4):1701–1714, 2018."
REFERENCES,0.551622418879056,"Satyaki Sikdar, Justus Hibshman, and Tim Weninger. Modeling graphs with vertex replacement
grammars. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 558–567. IEEE,
2019."
REFERENCES,0.5545722713864307,"Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. In International conference on artiﬁcial neural networks, pp. 412–422.
Springer, 2018."
REFERENCES,0.5575221238938053,"Peter C St. John, Caleb Phillips, Travis W Kemper, A Nolan Wilson, Yanfei Guan, Michael F Crow-
ley, Mark R Nimlos, and Ross E Larsen. Message-passing neural networks for high-throughput
polymer screening. The Journal of chemical physics, 150(23):234111, 2019."
REFERENCES,0.56047197640118,"Megan Stanley, John F Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin
Segler, Nadine Schneider, and Marc Brockschmidt.
Fs-mol: A few-shot learning dataset of
molecules. 2021."
REFERENCES,0.5634218289085545,"Govindan Subramanian, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny. Computational
modeling of β-secretase 1 (bace-1) inhibitors using ligand based approaches. Journal of chemical
information and modeling, 56(10):1936–1949, 2016."
REFERENCES,0.5663716814159292,"David Weininger. Smiles, a chemical language and information system. 1. introduction to method-
ology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36,
1988."
REFERENCES,0.5693215339233039,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229–256, 1992."
REFERENCES,0.5722713864306784,"Youjun Xu, Kangjie Lin, Shiwei Wang, Lei Wang, Chenjing Cai, Chen Song, Luhua Lai, and Jian-
feng Pei. Deep learning for molecular generation. Future medicinal chemistry, 11(6):567–597,
2019."
REFERENCES,0.5752212389380531,Published as a conference paper at ICLR 2022
REFERENCES,0.5781710914454278,"Xiufeng Yang, Jinzhe Zhang, Kazuki Yoshizoe, Kei Terayama, and Koji Tsuda. Chemts: an efﬁcient
python library for de novo molecular generation. Science and technology of advanced materials,
18(1):972–976, 2017."
REFERENCES,0.5811209439528023,"Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy
network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018."
REFERENCES,0.584070796460177,Published as a conference paper at ICLR 2022
REFERENCES,0.5870206489675516,"A
DETAILS ON THE MOLECULE GENERATION"
REFERENCES,0.5899705014749262,"Given a learned grammar, we condition the generation strategy based on (non)terminal symbols in
the rules: during molecule generation, we exponentially increase the probability of those production
rules without non-terminal symbol on the right-hand side based on the iteration number. Formally,
the probability to select a certain production rule r at iteration t is p(r) = Z−1 exp(αtxr), where
xr is a binary value indicating whether rule r contains only terminal symbols on the right-hand side,
and Z is a normalization factor. We used α = 0.5 in our experiments, since it reduces the generation
time sufﬁciently while maintaining satisfactory diversity."
REFERENCES,0.5929203539823009,"Note that we initially experimented with uniform random sampling of rules during testing. However,
the possibility of generating arbitrarily large molecules (i.e., by choosing production rules with
a non-terminal symbol on the right-hand side more likely) sometimes resulted in a never-ending
generation process, a problem in practice."
REFERENCES,0.5958702064896755,"B
DETAILS ON THE IMPLEMENTATION"
REFERENCES,0.5988200589970502,"Evaluation Metrics. For the large polymer dataset, we consider 4 additional evaluation metrics,
measuring distribution statistcs of the generated molecules, which are commonly used in DL-based
methods. The metrics are:"
REFERENCES,0.6017699115044248,"• Octanol-Water Partition Coefﬁcient (logP): A ratio of a chemical’s concentration in the octanol
phase to its concentration in the aqueous phase of a two-phase octanol/water system.
• Synthetic Accessibility Score (SA) Estimate of how hard or easy it is to synthesize a given
molecule based on the molecule’s fragments (Ertl & Schuffenhauer, 2009).
• Quantitative Estimation of Drug-likeness (QED): Estimate of how likely a molecule is to be a
viable drug candidate, meant to capture aesthetics in medicinal chemistry (Bickerton et al., 2012).
• Molecular Weight (MW): Sum of atomic weights in a molecule. Differences in the histograms
for the generated and real data may reveal bias towards generating lighter or heavier molecules."
REFERENCES,0.6047197640117994,"Note that, while SA Score and QED are not considered useful sample quality heuristics for more
recent molecules, they are still useful as distribution statistics (Polykovskiy et al., 2020; Shultz,
2018)."
REFERENCES,0.6076696165191741,"Baselines. For STONED, as suggested by the authors, we ﬁrst generate the chemical space for each
sample in the dataset5.Then we take all samples scoring higher than 0.8 and randomly sample the
remaining ones to obtain our 10k samples. For MHG, we learn a hypergraph grammar on the dataset
following the paper’s implementation and generate molecules by randomly sampling and deploying
production rules. For other baselines we mainly use the settings suggested in the original papers. For
the experiments on the small datasets, SMILESVAE, GraphNVP and HierVAE are pretrained on the
large dataset and thereafter ﬁne-tuned on speciﬁc small dataset; the other baseline implementations
do not support pretraining, hence we train them from scratch on the small datasets."
REFERENCES,0.6106194690265486,"Our System. For our approach, we choose a pretrained graph neural network (Hu et al., 2019) as
our feature extractor f(·). Note that we do not ﬁnetune its parameters during training and it can be
replaced by any plug-and-play feature extractors. For the potential function Fθ, we use a two-layer
fully connected network with size 300 and 128. For the optimization objectives, we consider two
metrics: diversity and RS. For hyperparameters, we set MC sampling size as 5. We use the Adam
optimizer to train the two-layer network with learning rate 0.01. We trained for 20 epochs."
REFERENCES,0.6135693215339233,"We train the model and construct grammar on each dataset separately from scratch. For the large
polymer dataset, we only use 117 training samples to optimize the grammar. This is motivated by
the fact that only 436 different motifs exist in the training dataset as stated in Jin et al. (2020). We
can then construct a subset consisting of 436 molecules that can cover these 436 motifs by random
sampling. We then sample from this subset to get the training set for our method. For the basic
setting of our method, we sample 117 training molecules. We also consider the scenario with more
data, where we extend the former set to 239 samples."
REFERENCES,0.616519174041298,"After training, we generated 10k samples per dataset for evaluation. For the large polymer dataset,
besides the naively generated 10k samples, we further construct another set of generated molecules"
REFERENCES,0.6194690265486725,5https://github.com/aspuru-guzik-group/stoned-selfies
REFERENCES,0.6224188790560472,Published as a conference paper at ICLR 2022
REFERENCES,0.6253687315634219,"Table 2: Results on Acrylates and Chain Extenders (best bolded, second-best underlined). The
low validity of molecules generated by GraphNVP did not allow for reasonable evaluation on some
metrics (–)."
REFERENCES,0.6283185840707964,"Dataset
Method
Valid
Unique
Div.
Chamfer
RS
Member."
REFERENCES,0.6312684365781711,Acrylates
REFERENCES,0.6342182890855457,"Train data
100%
100%
0.67
0.00
100%
100%"
REFERENCES,0.6371681415929203,"GraphNVP
0.00%
–
–
–
–
–"
REFERENCES,0.640117994100295,"JT-VAE
100%
0.50%
0.29
0.86
4.9%
48.64%"
REFERENCES,0.6430678466076696,"HierVAE
100%
99.7%
0.83
0.89
3.04%
0.82%"
REFERENCES,0.6460176991150443,"MHG
100%
86.8%
0.89
0.84
36.8%
0.93%"
REFERENCES,0.6489675516224189,"STONED
99.9%
99.8%
0.84
0.88
11.2%
47.9%"
REFERENCES,0.6519174041297935,"DEG
100%
100%
0.86
0.92
43.9%
69.6%"
REFERENCES,0.6548672566371682,"Chain
Extenders"
REFERENCES,0.6578171091445427,"Train data
100%
100%
0.80
0.00
100%
100%"
REFERENCES,0.6607669616519174,"GraphNVP
0.01%
–
–
–
–
–"
REFERENCES,0.6637168141592921,"JT-VAE
100%
2.3%
0.62
0.78
2.20%
79.6%"
REFERENCES,0.6666666666666666,"HierVAE
100%
99.8%
0.83
0.91
2.69%
43.6%"
REFERENCES,0.6696165191740413,"MHG
100%
87.4%
0.90
0.85
50.6%
41.2%"
REFERENCES,0.672566371681416,"STONED
100%
99.8%
0.93
0.87
6.78%
61.0%"
REFERENCES,0.6755162241887905,"DEG
100%
100%
0.93
0.94
67.5%
93.5%"
REFERENCES,0.6784660766961652,"Table 3: Results on the large polymer dataset (best bold, second-best underlined). The low validity
of molecules generated by GraphNVP and SMILESVAE did not allow for reasonable evaluation on
some metrics (–). Our method DEG was trained on 0.15% and 0.3% of the train data. DEG (ﬁtting)
is evaluated using the selected 10k samples via the ﬁtting process described in Appendix B."
REFERENCES,0.6814159292035398,Method
REFERENCES,0.6843657817109144,"Distribution Statistics (↓)
Sample Quality (↑)"
REFERENCES,0.6873156342182891,"logP
SA
QED
MW
Valid
Unique
Div.
Chamfer"
REFERENCES,0.6902654867256637,"Train data
0.12
0.02
0.002
2.98
100%
100%
0.83
0.00"
REFERENCES,0.6932153392330384,"SMILESVAE
9.63
2.99
0.19
751.6
0.01%
–
–
–"
REFERENCES,0.696165191740413,"GraphNVP
2.94
0.65
0.03
435.6
0.23%
–
–
–"
REFERENCES,0.6991150442477876,"JT-VAE
2.93
0.32
0.10
210.1
100%
83.9%
0.88
0.50"
REFERENCES,0.7020648967551623,"HierVAE
0.50
0.08
0.02
42.45
100%
99.9%
0.82
0.32"
REFERENCES,0.7050147492625368,"MHG
9.20
1.91
0.10
380.3
100%
100%
0.91
0.56"
REFERENCES,0.7079646017699115,"STONED
2.43
0.81
0.07
179.9
99.9%
100%
0.83
0.45"
REFERENCES,0.7109144542772862,"DEG (0.15%, ﬁtting)
1.80
0.25
0.02
69.0
100%
100%
0.82
0.60"
REFERENCES,0.7138643067846607,"DEG (0.15%)
5.52
0.51
0.20
334.2
100%
100%
0.86
0.62"
REFERENCES,0.7168141592920354,"DEG (0.3%, ﬁtting)
1.93
0.23
0.02
70.2
100%
100%
0.85
0.62"
REFERENCES,0.7197640117994101,"DEG (0.3%)
5.64
0.41
0.19
311.4
100%
100%
0.88
0.63"
REFERENCES,0.7227138643067846,"to have a fair comparison with DL-based methods on distribution statistics. We perform a ﬁtting
process selecting 10k data points from 100k generated samples using the same learned grammar in
order to get a better performance on distribution statistics. Speciﬁcally, we calculate the four evalu-
ation metrics of distribution statistics for both the training dataset and the 100k generated samples;"
REFERENCES,0.7256637168141593,Published as a conference paper at ICLR 2022
REFERENCES,0.7286135693215339,"each data point can have a corresponding 4-dimensional vector. We perform k-means clustering of
all the vectors of the training dataset with k = 10, 000. Then we partition the 4-dimensional space
into k regions using Voronoi diagram and treat each region ‘equal mass’. For each region, we sam-
ple one data point from our generated samples that lie in the region. Thus, we construct a set of 10k
generated samples and evaluate it the same way as the other methods."
REFERENCES,0.7315634218289085,"C
ADDITIONAL RESULTS"
REFERENCES,0.7345132743362832,"C.1
QUANTITATIVE RESULTS"
REFERENCES,0.7374631268436578,"We report the results on the Acrylate and Chain Extender datasets in Table 2. Since SMILESVAE
cannot achieve reasonable validity for all three small datasets, we omit it in the tables. Our method
signiﬁcantly outperforms the others in terms of Retro∗Score and Membership, while achieving best
or comparable performance across all other metrics."
REFERENCES,0.7404129793510325,"The results for the large polymer dataset are reported in Table 3. As common practice (Jin et al.,
2020; Polykovskiy et al., 2020), we compute Frechet distance between property distributions of
molecules in the generated and test sets for distribution statistics. All but our two new evaluation
metrics are computed using MOSES (Polykovskiy et al., 2020). Since the building blocks of polymer
data in this dataset are from a special database (St. John et al., 2019), which differs signiﬁcantly
from the emolecule database Retro∗is trained on, we do not report RS as it is less meaningful. Our
method achieves remarkable performances across all evaluation metrics. Note that we only use 117
data points while the others are fully trained on the whole dataset."
REFERENCES,0.7433628318584071,"C.2
EXAMPLES OF GENERATED RESULTS O
S
S N O O O O O
S
S O N F"
REFERENCES,0.7463126843657817,"O
H
N
N N O O H
N S O O O O S F
O O O N O S O O O"
REFERENCES,0.7492625368731564,"O
HO
N
H"
REFERENCES,0.7522123893805309,"OH
H2N NH2 O OH NH2 O O
O O H2N O
N"
REFERENCES,0.7551622418879056,"O
HN
HO O N
H OH"
REFERENCES,0.7581120943952803,(a) Examples generated by the graph grammar learned on large polymer dataset
REFERENCES,0.7610619469026548,(b) Examples generated by the graph grammar learned on Chain Extender dataset
REFERENCES,0.7640117994100295,Figure 5: Examples of generated results using our learned graph grammar.
REFERENCES,0.7669616519174042,"D
SWITCHING THE FEATURE EXTRACTOR"
REFERENCES,0.7699115044247787,"In order to demonstrate the plug-and-play capability of our system’s feature extractor, we provide
experimental results where we use most embeddings from the deepchem package6. As it can be seen
in Figure 6, this simple feature extractor yields higher performance than the pretrained GNN at the
start of the optimization, but cannot improve further through the whole learning process. In order to
obtain a reasonable optimized grammar, an ideal feature extractor should capture both the local and"
REFERENCES,0.7728613569321534,"6https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.
html (we just concatenated atom and bond features)"
REFERENCES,0.775811209439528,Published as a conference paper at ICLR 2022
REFERENCES,0.7787610619469026,"the global information of the hyperedges in the hypergraph. Hence, the GNN ﬁts better our purpose
of grammar construction."
REFERENCES,0.7817109144542773,"Epoch
0
20"
REFERENCES,0.7846607669616519,RS Score
REFERENCES,0.7876106194690266,"Epoch
0
20"
REFERENCES,0.7905604719764012,Diversity Score 0.1 0.4 0.82 0.90
REFERENCES,0.7935103244837758,Figure 6: Comparison with simple feature extractor on Isocyanates.
REFERENCES,0.7964601769911505,"E
DEMONSTRATION OF STABILITY OF REINFORCE"
REFERENCES,0.799410029498525,"Since REINFORCE is known to have large variance, we provide results on the stability of our
proposed DEG. We run our algorithm with three random seeds on the Isocyanates data. The ex-
perimental results are shown in Figure 7. The diversity scores are relatively stable across the three
random seeds. The RS scores vary more but all three experiments converge to a similar value around
the end of optimization. We also show convergence curves for the other three datasets in Figure 8."
REFERENCES,0.8023598820058997,"Epoch
0
20"
REFERENCES,0.8053097345132744,RS Score
REFERENCES,0.8082595870206489,"Epoch
0
20"
REFERENCES,0.8112094395280236,Diversity Score 0.05 0.35 0.6 0.9
REFERENCES,0.8141592920353983,Figure 7: Analysis of stability of proposed DEG.
REFERENCES,0.8171091445427728,"Epoch
0
20"
REFERENCES,0.8200589970501475,Optimization Objective
REFERENCES,0.8230088495575221,"Epoch
0
20"
REFERENCES,0.8259587020648967,Optimization Objective
REFERENCES,0.8289085545722714,"Epoch
0
20"
REFERENCES,0.831858407079646,Optimization Objective
REFERENCES,0.8348082595870207,"(a) Acrylates
(b) Chain Extenders
(c) Polymer Dataset (0.15%) 1.5 2.5 1.0 2.0 1.0 2.0"
REFERENCES,0.8377581120943953,Figure 8: Convergence curves on three datasets.
REFERENCES,0.8407079646017699,Published as a conference paper at ICLR 2022
REFERENCES,0.8436578171091446,"F
LEARNED GRAPH GRAMMAR"
REFERENCES,0.8466076696165191,(a) Examples of production rules learned from Isocyanates dataset
REFERENCES,0.8495575221238938,(b) Examples of production rule learned from Acrylates dataset
REFERENCES,0.8525073746312685,(c) Examples of production rule learned from Chain Extenders dataset
REFERENCES,0.855457227138643,Figure 9: Examples of production rules from our learned graph grammar.
REFERENCES,0.8584070796460177,"G
OUR DATASETS"
REFERENCES,0.8613569321533924,"G.1
ISOCYANATES"
REFERENCES,0.8643067846607669,"MDI
O=C=NC1=CC=CC(CC2=CC=C(C=C2N=C=O)CC3=CC=C(C=C3)N=C=O)=C1
MDI
O=C=NC1=CC(CC2=C(C=C(C=C2)CC3=CC=C(C=C3N=C=O)CC4=CC=C
(C=C4)N=C=O)N=C=O)=CC=C1
MDI
O=C=NC1=CC=C(C=C1)CC2=CC=C(C=C2N=C=O)CC3=C(C=C(C=C3)CC4=C
C=C(C=C4N=C=O)CC5=CC=C(C=C5)N=C=O)N=C=O
HDI
O=C=NCCCCCCN=C=O
HDI
O=C=NCCCCCCCCCCCCN=C=O
HDI
O=C=NCCCCCCCCCCCCCCCCCCN=C=O
HDI
O=C=NCCCCCCCCCCCCCCCCCCCCCCCCN=C=O
IPDI
CC1(CC(CC(CN=C=O)(C1)C)N=C=O)C
TDI
CC1=C(C=C(C=C1)CN=C=O)N=C=O
HMDI
O=C=NC1CCC(CC2CCC(CC2)N=C=O)CC1
LDI
CCOC(C(N=C=O)CCCCN=C=O)=O"
REFERENCES,0.8672566371681416,"G.2
ACRYLATES"
REFERENCES,0.8702064896755162,"Benzyl Acrylate
C=CC(=O)OCC1=CC=CC=C1"
REFERENCES,0.8731563421828908,"Phenyl Acrylate
C=CC(=O)OC1=CC=CC=C1"
REFERENCES,0.8761061946902655,"Phenyl Methacrylate
CC(=C)C(=O)OC1=CC=CC=C1"
-PHENYLETHYL ACRYLATE,0.8790560471976401,"2-Phenylethyl Acrylate
C=CC(=O)OCCC1=CC=CC=C1"
-PHENYLETHYL ACRYLATE,0.8820058997050148,"n-Octyl Methacrylate
CCCCCCCCOC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.8849557522123894,"Sec-Butyl Acrylate
CCC(C)OC(=O)C=C"
-PHENYLETHYL ACRYLATE,0.887905604719764,"Benzyl Methacrylate
CC(=C)C(=O)OCC1=CC=CC=C1"
-PHENYLETHYL ACRYLATE,0.8908554572271387,"Pentaﬂuorophenyl acrylate
C=CC(=O)OC1=C(C(=C(C(=C1F)F)F)F)F"
-PHENYLETHYL ACRYLATE,0.8938053097345132,Published as a conference paper at ICLR 2022
-PHENYLETHYL ACRYLATE,0.8967551622418879,"iso-Butyl methacrylate
CC(C)COC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.8997050147492626,"n-Dodecyl methacrylate
CCCCCCCCCCCCOC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.9026548672566371,"sec-Butyl methacrylate
CCC(C)OC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.9056047197640118,"n-Propyl methacrylate
CCCOC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.9085545722713865,"3,3,5-Trimethylcyclohexyl methacrylate
CC1CC(CC(C1)(C)C)OC(=O)C(=C)C"
-PHENYLETHYL ACRYLATE,0.911504424778761,"iso-Decyl acrylate
CC(C)CCCCCCCOC(=O)C=C"
-PHENYLETHYL ACRYLATE,0.9144542772861357,"n-Propyl acrylate
CCCOC(=O)C=C"
-METHOXYETHYL ACRYLATE,0.9174041297935103,"2-Methoxyethyl acrylate
COCCOC(=O)C=C"
-PHENOXYETHYL METHACRYLATE,0.9203539823008849,"2-Phenoxyethyl methacrylate
CC(=C)C(=O)OCCOC1=CC=CC=C1"
-PHENOXYETHYL METHACRYLATE,0.9233038348082596,"n-Hexyl acrylate
CCCCCCOC(=O)C=C"
-N-BUTOXYETHYL METHACRYLATE,0.9262536873156342,"2-n-Butoxyethyl methacrylate
CCCCOCCOC(=O)C(=C)C"
-N-BUTOXYETHYL METHACRYLATE,0.9292035398230089,"Methyl Methacrylate
CC(=C)C(=O)OC"
-N-BUTOXYETHYL METHACRYLATE,0.9321533923303835,"Methyl Acrylate
COC(=O)C=C"
-N-BUTOXYETHYL METHACRYLATE,0.9351032448377581,"Butyl Arylate
CCCCOC(=O)C=C"
-ETHOXYETHYL METHACRYLATE,0.9380530973451328,"2-Ethoxyethyl methacrylate
CCOCCOC(=O)C(=C)C"
-ETHOXYETHYL METHACRYLATE,0.9410029498525073,"Isobornyl methacrylate
CC(=C)C(=O)OC1CC2CCC1(C2(C)C)C"
-ETHYLHEXYL METHACRYLATE,0.943952802359882,"2-Ethylhexyl methacrylate
CCCCC(CC)COC(=O)C(=C)C"
-ETHYLHEXYL METHACRYLATE,0.9469026548672567,"Neopentyl glycol propoxylate diacrylate
CC(C)(COCCCOC(=O)C=C)COCCCOC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9498525073746312,"1,6-Hexanediol diacrylate
C=CC(=O)OCCCCCCOC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9528023598820059,"Pentaerythritol triacrylate
C=CC(=O)OCC(CO)(COC(=O)C=C)COC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9557522123893806,"Trimethylolpropane propoxylate triacrylate
CCC(COCCCOC(=O)C=C)(COCCCOC(=O)C=C)COCCCOC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9587020648967551,"Di(trimethylolpropane) tetraacrylate
CCC(COCC(CC)(COC(=O)C=C)COC(=O)C=C)(COC(=O)C=C)COC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9616519174041298,"Dipentaerythritol pentaacrylate
C=CC(=O)OCC(CO)(COCC(COC(=O)C=C)(COC(=O)C=C)COC(=O)C=C)COC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.9646017699115044,"Dipentaerythritol hexaacrylate
C=CC(=O)OCC(COCC(COC(=O)C=C)(COC(=O)C=C)COC(=O)C=C)
(COC(=O)C=C)COC(=O)C=C"
-ETHYLHEXYL METHACRYLATE,0.967551622418879,"G.3
CHAIN EXTENDERS"
-ETHYLHEXYL METHACRYLATE,0.9705014749262537,"EG
OCCO"
-ETHYLHEXYL METHACRYLATE,0.9734513274336283,"1,3-BD
OC(C)CCO"
-ETHYLHEXYL METHACRYLATE,0.976401179941003,"BD
OCCCCO"
-ETHYLHEXYL METHACRYLATE,0.9793510324483776,"AE-H-AE
OCCNC(=O)NCCCCCCNC(=O)NCCO"
-ETHYLHEXYL METHACRYLATE,0.9823008849557522,"AE-L-AE
OCCN1C(=O)NC(C1(=O))CCCCNC(=O)NCCO"
-ETHYLHEXYL METHACRYLATE,0.9852507374631269,"D-E-D
Oc1ccc(cc1)CCC(=O)OCCOC(=O)CCc1ccc(cc1)O"
-ETHYLHEXYL METHACRYLATE,0.9882005899705014,"LYS
OC(=O)C(N)CCCCN"
-ETHYLHEXYL METHACRYLATE,0.9911504424778761,"L-Orn
OC(=O)C(N)CCCN
Pip
N1CCNCC1"
-ETHYLHEXYL METHACRYLATE,0.9941002949852508,"AFD
Nc1ccc(cc1)SSc2ccc(cc2)N"
-ETHYLHEXYL METHACRYLATE,0.9970501474926253,"MDA
Nc1ccc(cc1)Cc2ccc(cc2)N"
