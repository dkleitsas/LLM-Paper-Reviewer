Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009442870632672333,"Recent advancements in Larger-Scale Transformers have significantly benefited
1"
ABSTRACT,0.0018885741265344666,"from sophisticated attention mechanisms, which are critical for modeling long-
2"
ABSTRACT,0.0028328611898017,"context sequences. However, the computational and memory demands of conven-
3"
ABSTRACT,0.003777148253068933,"tional attention mask computations, typically scaling with an O(ùëÅ2) complexity
4"
ABSTRACT,0.004721435316336166,"where ùëÅis the sequence length, pose significant challenges. This paper intro-
5"
ABSTRACT,0.0056657223796034,"duces FlashMask, a simple yet effective Exact attention algorithm designed to
6"
ABSTRACT,0.0066100094428706326,"substantially reduce both the computational complexity and memory requirements
7"
ABSTRACT,0.007554296506137866,"of attention computations. By adopting a novel column-wise sparse representation
8"
ABSTRACT,0.0084985835694051,"of attention masks, FlashMask achieves a linear memory complexity of O(ùëÅ) and
9"
ABSTRACT,0.009442870632672332,"computational complexity of O(ùëÅ) ‚àºO(ùëÅ2). We assess the performance of Flash-
10"
ABSTRACT,0.010387157695939566,"Mask in a variety of masking scenarios, including causal and customized attention
11"
ABSTRACT,0.0113314447592068,"masks, demonstrating its versatility and robustness across a wide range of attention
12"
ABSTRACT,0.012275731822474031,"patterns and models. Our empirical analysis encompasses a variety of downstream
13"
ABSTRACT,0.013220018885741265,"training modalities, including Supervised Fine-Tuning (SFT), Direct Preference
14"
ABSTRACT,0.014164305949008499,"Optimization (DPO), and Reward Model (RM). We compare FlashMask against
15"
ABSTRACT,0.015108593012275733,"state-of-the-art techniques, including notably FlashAttention [1]. In kernel-level
16"
ABSTRACT,0.016052880075542966,"assessments, FlashMask achieves substantial computational speedups, up to 6.7x
17"
ABSTRACT,0.0169971671388102,"(SFT), 6.9x (DPO), and 8.3x (RM). Furthermore, in end-to-end training, FlashMask
18"
ABSTRACT,0.01794145420207743,"consistently enhances training speed significantly, with accelerations up to 2.4x
19"
ABSTRACT,0.018885741265344664,"(SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios
20"
ABSTRACT,0.019830028328611898,"without sacrificing model accuracy. Additionally, when implemented in the LoRA
21"
ABSTRACT,0.02077431539187913,"scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to
22"
ABSTRACT,0.021718602455146365,"544k, significantly enhancing its capability for long-context input.
23"
INTRODUCTION,0.0226628895184136,"1
Introduction
24"
INTRODUCTION,0.023607176581680833,"Transformers [2] , equipped with self-attention mechanisms, have revolutionized natural language
25"
INTRODUCTION,0.024551463644948063,"processing (NLP) by efficiently modeling data dependencies without the limitations of sequential
26"
INTRODUCTION,0.025495750708215296,"processing. This makes them ideal for handling long sequences. Large Language Models (LLMs),
27"
INTRODUCTION,0.02644003777148253,"which utilize training paradigms such as Supervised Fine-Tuning (SFT) [3, 4] and Reinforcement
28"
INTRODUCTION,0.027384324834749764,"Learning from Human Feedback (RLHF) [5, 6], critically rely on selective attention management
29"
INTRODUCTION,0.028328611898016998,"through masks. Effective mask management is essential to focus selectively on pertinent data
30"
INTRODUCTION,0.02927289896128423,"segments, optimizing both performance and computational efficiency.
31"
INTRODUCTION,0.030217186024551465,"However, the conventional attention mechanism in Transformers entails a quadratic increase in
32"
INTRODUCTION,0.031161473087818695,"computational and memory demands O(ùëÅ2), where ùëÅdenotes the sequence length. This exponential
33"
INTRODUCTION,0.03210576015108593,"growth presents substantial challenges as models scale to sequence lengths ranging from 128K to
34"
INTRODUCTION,0.033050047214353166,"1M in advanced systems like GPT-4 [7], Claude [8], and Gemini [9], necessitating more efficient
35"
INTRODUCTION,0.0339943342776204,"computational approaches. As sequence lengths extend, the memory load for masked attention
36"
INTRODUCTION,0.03493862134088763,"computations also grows quadratically, adversely affecting computational speed and the ability to
37"
INTRODUCTION,0.03588290840415486,"manage various mask configurations across different tasks. Current methodologies often resort to
38"
INTRODUCTION,0.036827195467422094,"approximate sparse attention strategies [10, 11, 12], which unfortunately trade off precision for
39"
INTRODUCTION,0.03777148253068933,"computational efficiency, underscoring an essential gap in achieving high precision with reduced
40"
INTRODUCTION,0.03871576959395656,"computational costs.
41"
INTRODUCTION,0.039660056657223795,"This paper introduces FlashMask, a novel approach utilizing a sparse mask representation to accelerate
42"
INTRODUCTION,0.04060434372049103,"attention computations in transformers, effectively addressing both computational and memory scala-
43"
INTRODUCTION,0.04154863078375826,"bility issues. Unlike previous methods that compromise accuracy for efficiency, FlashMask provides
44"
INTRODUCTION,0.042492917847025496,"precise computations without sacrificing accuracy, ensuring high fidelity in attention mechanisms.
45"
INTRODUCTION,0.04343720491029273,"The contributions of this work include:
46"
INTRODUCTION,0.044381491973559964,"‚Ä¢ Exact Computation. FlashMask uniquely ensures precise attention computations across varying
47"
INTRODUCTION,0.0453257790368272,"sequence lengths and tasks. It employs a unique column-wise sparse mask representation, denoted
48"
INTRODUCTION,0.04627006610009443,"by FlashMaskStart (FMS) and FlashMaskEnd (FME), to precisely mask specific rows within
49"
INTRODUCTION,0.047214353163361665,"columns, ensuring computational efficiency and accuracy.
50"
INTRODUCTION,0.04815864022662889,"‚Ä¢ Long Context Modeling. FlashMask significantly reduces computational and memory demands,
51"
INTRODUCTION,0.049102927289896126,"enabling efficient processing of extended sequences critical for deploying LLMs in resource-limited
52"
INTRODUCTION,0.05004721435316336,"settings.
53"
INTRODUCTION,0.05099150141643059,"‚Ä¢ Efficient Mask Computation. FlashMask leverages strategic sparse masking to increase compu-
54"
INTRODUCTION,0.05193578847969783,"tational throughput, thereby improving processing speeds and broadening the practical utility of
55"
INTRODUCTION,0.05288007554296506,"LLMs in diverse real-world scenarios.
56"
INTRODUCTION,0.053824362606232294,"‚Ä¢ Extensive Empirical Validation. Empirical studies validate FlashMask‚Äôs efficiency in computation
57"
INTRODUCTION,0.05476864966949953,"and storage. Its practical application in real-world scenarios and integration with existing frame-
58"
INTRODUCTION,0.05571293673276676,"works underscore its potential impact. Moreover, a comprehensive comparison with state-of-the-art
59"
INTRODUCTION,0.056657223796033995,"methods like FlashAttention-DenseMask, FlashAttention-Varlen highlights FlashMask‚Äôs efficiency
60"
INTRODUCTION,0.05760151085930123,"and versatility.
61"
BACKGROUND,0.05854579792256846,"2
Background
62"
BACKGROUND,0.059490084985835696,"The attention mechanism has revolutionized data handling in NLP by mimicking human selective
63"
BACKGROUND,0.06043437204910293,"focus, allowing neural networks to prioritize parts of the input data. This addresses limitations
64"
BACKGROUND,0.061378659112370164,"of traditional sequence-to-sequence models, enhancing context awareness in long sequences. The
65"
BACKGROUND,0.06232294617563739,"Transformer model by Vaswani et al. [2] implements this mechanism centrally, using multiple parallel
66"
BACKGROUND,0.06326723323890462,"attention heads instead of recurrent layers, thus improving efficiency and performance.
67"
ATTENTION COMPUTATION,0.06421152030217187,"2.1
Attention Computation
68"
ATTENTION COMPUTATION,0.06515580736543909,"Central to the Transformer architecture is the attention mechanism, which computes relevance
69"
ATTENTION COMPUTATION,0.06610009442870633,"scores between elements in a sequence to focus more on important aspects and less on others. This
70"
ATTENTION COMPUTATION,0.06704438149197356,"mechanism can be expressed as:
71"
ATTENTION COMPUTATION,0.0679886685552408,"Attentionùëöùëéùë†ùëò(ùëÑ, ùêæ,ùëâ) = softmax
ùëÑùêæùëá"
ATTENTION COMPUTATION,0.06893295561850803,"‚àöùëëùëò
+ ùëÄ

ùëâ,
(1)"
ATTENTION COMPUTATION,0.06987724268177525,"where ùëÑ, ùêæ, ùëâ, and ùëÄrepresent the query, key, value, and mask matrices respectively, derived
72"
ATTENTION COMPUTATION,0.0708215297450425,"from the input data, and ùëëùëòis the dimension of keys. The term ùëÄincorporates constraints to
73"
ATTENTION COMPUTATION,0.07176581680830972,"selectively consider certain parts of the input sequence during attention computation, enabling
74"
ATTENTION COMPUTATION,0.07271010387157696,"functionality like masking future tokens in sequence-to-sequence modeling. One inherent challenge
75"
ATTENTION COMPUTATION,0.07365439093484419,"with attention is its computational and memory complexity, both of which scale quadratically with
76"
ATTENTION COMPUTATION,0.07459867799811143,"the length of the input sequence. Processing long sequences presents significant challenges, which
77"
ATTENTION COMPUTATION,0.07554296506137866,"are exacerbated in the downstream pipeline of training large language models (LLMs). Different
78"
ATTENTION COMPUTATION,0.0764872521246459,"training stages, such as Supervised Fine-Tuning (SFT/LoRA [3, 4, 13, 14, 15]), Direct Preference
79"
ATTENTION COMPUTATION,0.07743153918791312,"Optimization (DPO) [16, 17, 18, 19, 20], Reward Model (RM) [5, 21, 22, 23, 24], and Proximal
80"
ATTENTION COMPUTATION,0.07837582625118036,"Policy Optimization (PPO) [25, 6], place diverse demands on the attention mask.
81"
MASKING VARIABLE-LENGTH SEQUENCES,0.07932011331444759,"2.2
Masking Variable-Length Sequences
82"
MASKING VARIABLE-LENGTH SEQUENCES,0.08026440037771483,"The advent of large transformer-based models has marked substantial progression in handling
83"
MASKING VARIABLE-LENGTH SEQUENCES,0.08120868744098206,"increased sequence lengths in natural language processing. Previously, models like BERT [26] and
84"
MASKING VARIABLE-LENGTH SEQUENCES,0.0821529745042493,"GPT-2 [27] were limited to sequences of approximately 512 tokens, whereas more recent adaptations
85"
MASKING VARIABLE-LENGTH SEQUENCES,0.08309726156751653,"such as the LLaMA [28, 29, 30], GPT-4 [7] and Claude series [8] stretched these limits to encompass
86"
MASKING VARIABLE-LENGTH SEQUENCES,0.08404154863078375,"2K to 200K tokens, respectively. Innovations from Google‚Äôs Gemini [9] have further shifted this
87"
MASKING VARIABLE-LENGTH SEQUENCES,0.08498583569405099,"boundary, managing up to 1M tokens. Enhanced sequence management within these models employs
88"
MASKING VARIABLE-LENGTH SEQUENCES,0.08593012275731822,"various masking techniques in the attention matrix, adapting to the length and diversity of input
89"
MASKING VARIABLE-LENGTH SEQUENCES,0.08687440982058546,"sequences. Techniques such as the use of padding operations are illustrated in Figure 1(a), which help
90"
MASKING VARIABLE-LENGTH SEQUENCES,0.08781869688385269,"maintain efficiency by allowing uniform processing of diverse input lengths through padding masks.
91"
MASKING VARIABLE-LENGTH SEQUENCES,0.08876298394711993,"However, conventional padding can lead to inefficiencies due to the diverse sequence lengths typically
92"
MASKING VARIABLE-LENGTH SEQUENCES,0.08970727101038715,"found in training data, often following a long-tail distribution. This issue is adeptly addressed by
93"
MASKING VARIABLE-LENGTH SEQUENCES,0.0906515580736544,"dynamic token allocation technologies like InToken [31, 3, 32, 33, 34], which optimize computational
94"
MASKING VARIABLE-LENGTH SEQUENCES,0.09159584513692162,"resources by adjusting the token count based on actual data needs, significantly improving the training
95"
MASKING VARIABLE-LENGTH SEQUENCES,0.09254013220018886,"efficiency for datasets with various sequence lengths in Figure 1(b)(c).
96"
MASKING VARIABLE-LENGTH SEQUENCES,0.09348441926345609,"(a)
(b)
(c)
(d)
Figure 1: Common patterns of attention masks. (a) Padded masks from single-sequence inputs in
unidirectional (uni-) attention. (b) InToken masks from grouping several masks with different lengths
in uni-attention. (c) InToken masks in bidirectional (bidi-) attention. (d) Question and Answering
Masks in uni-attention."
MASKING VARIABLE-LENGTH SEQUENCES,0.09442870632672333,"Despite having extensive text-handling capabilities, the meticulous design of masking configurations
97"
MASKING VARIABLE-LENGTH SEQUENCES,0.09537299338999056,"remains crucial for specific training scenarios. The illustrated scenarios in Figure 1(d) and Figure 2
98"
MASKING VARIABLE-LENGTH SEQUENCES,0.09631728045325778,"depict various specialized masking mechanisms employed to enhance model training efficiency and
99"
MASKING VARIABLE-LENGTH SEQUENCES,0.09726156751652502,"applicability. Figure 1(d) illustrates a scenario involving DPO/RM with two or more answers, where
100"
MASKING VARIABLE-LENGTH SEQUENCES,0.09820585457979225,"each answer‚Äôs tokens have visibility to the tokens of the question, and tokens from different answers
101"
MASKING VARIABLE-LENGTH SEQUENCES,0.09915014164305949,"are not visible to each other. Multi-shot and in-context learning scenarios facilitated by extended
102"
MASKING VARIABLE-LENGTH SEQUENCES,0.10009442870632672,"attention spans in configurations like Figure 2(a) are becoming prevalent, which allows the final
103"
MASKING VARIABLE-LENGTH SEQUENCES,0.10103871576959396,"question in a series to receive comprehensive attention, enhancing contextual understanding [35,
104"
MASKING VARIABLE-LENGTH SEQUENCES,0.10198300283286119,"36]. Furthermore, hybrid masking forms combining features from different methodologies are
105"
MASKING VARIABLE-LENGTH SEQUENCES,0.10292728989612843,"demonstrated in Figure 2(b). These incorporate sink tokens [37] and a sliding window mask from the
106"
MASKING VARIABLE-LENGTH SEQUENCES,0.10387157695939565,"Big Bird [38], facilitating a localized yet extensive context capture. Figure 2(c) is also derived from
107"
MASKING VARIABLE-LENGTH SEQUENCES,0.1048158640226629,"Big Bird, showing a bi-directional global attention mask, which allows for a comprehensive global
108"
MASKING VARIABLE-LENGTH SEQUENCES,0.10576015108593012,"context capture. Such innovative approaches in masking not only bolster the efficiency of training
109"
MASKING VARIABLE-LENGTH SEQUENCES,0.10670443814919736,"large transformer models but also pave the way for advanced explorations into the capabilities of
110"
MASKING VARIABLE-LENGTH SEQUENCES,0.10764872521246459,"attention mechanisms, such as simulating token eviction during inference as depicted in Figure 2(d).
111"
MASKING VARIABLE-LENGTH SEQUENCES,0.10859301227573183,"These advancements underscore the dynamic and adaptable nature of transformer technology in
112"
MASKING VARIABLE-LENGTH SEQUENCES,0.10953729933899906,"accommodating varying training needs and enhancing the overall performance of LLMs.
113"
ATTENTION OPTIMIZATION TECHNIQUES,0.11048158640226628,"2.3
Attention Optimization Techniques
114"
ATTENTION OPTIMIZATION TECHNIQUES,0.11142587346553352,"As aforementioned in Equation 1, the computational and memory demands of this mechanism,
115"
ATTENTION OPTIMIZATION TECHNIQUES,0.11237016052880075,"particularly the computation of ùëÑùêæùëá, become significant as the sequence length ùëÅincreases. This
116"
ATTENTION OPTIMIZATION TECHNIQUES,0.11331444759206799,"is due to the size of the resultant attention scores matrix, which scales quadratically with the
117"
ATTENTION OPTIMIZATION TECHNIQUES,0.11425873465533522,"sequence length, leading to a complexity of O(ùëÅ2). Several related works has been proposed to
118"
ATTENTION OPTIMIZATION TECHNIQUES,0.11520302171860246,"alleviate the issue. In the realm of model training optimizations, Memory Efficient Attention [39]
119"
ATTENTION OPTIMIZATION TECHNIQUES,0.11614730878186968,"(MEA) and FlashAttention [1] have been pivotal. MEA focuses on reducing the model‚Äôs memory
120"
ATTENTION OPTIMIZATION TECHNIQUES,0.11709159584513693,"demands by altering the self-attention mechanisms. This allows either for the use of larger models
121"
ATTENTION OPTIMIZATION TECHNIQUES,0.11803588290840415,"or for the extension of maximum sequence lengths within existing hardware constraints. On the
122"
ATTENTION OPTIMIZATION TECHNIQUES,0.11898016997167139,"(a)
(b)
(c)
(d)
Figure 2: Extended patterns of attention masks. (a) In-context learning formatted multi-shot masks in
uni-attention. (b) Sink + Slidewindow masks in uni-attention. (c) Global masks in bidi-attention. (d)
Customized masks in uni-attention."
ATTENTION OPTIMIZATION TECHNIQUES,0.11992445703493862,"other hand, FlashAttention enhances the efficiency of attention mechanisms with IO-Awareness to
123"
ATTENTION OPTIMIZATION TECHNIQUES,0.12086874409820586,"better utilize contemporary GPU architectures, resulting in faster computations and reduced energy
124"
ATTENTION OPTIMIZATION TECHNIQUES,0.12181303116147309,"consumption. This method reduces memory overhead to O(ùëÅ) utilizing tiling techniques during
125"
ATTENTION OPTIMIZATION TECHNIQUES,0.12275731822474033,"the computation process, making it particularly effective in scenarios without the need for a custom
126"
ATTENTION OPTIMIZATION TECHNIQUES,0.12370160528800755,"mask. However, for specific training contexts requiring custom masking, the memory overhead
127"
ATTENTION OPTIMIZATION TECHNIQUES,0.12464589235127478,"with FlashAttention remains O(ùëÅ2). Note that, in typical training setups like unidirectional causal
128"
ATTENTION OPTIMIZATION TECHNIQUES,0.125590179414542,"attention or bidirectional full-context attention, the default mode of operation with FlashAttention
129"
ATTENTION OPTIMIZATION TECHNIQUES,0.12653446647780925,"does not involve passing a custom mask.
130"
ATTENTION OPTIMIZATION TECHNIQUES,0.1274787535410765,"During the inference stage, optimizations such as FlashDecoding [40] and FlashDecoding++ [41]
131"
ATTENTION OPTIMIZATION TECHNIQUES,0.12842304060434373,"play crucial roles. FlashDecoding enhances the decoder in transformers to expedite the generation of
132"
ATTENTION OPTIMIZATION TECHNIQUES,0.12936732766761094,"sequences by optimizing state management and employing techniques that minimize computational
133"
ATTENTION OPTIMIZATION TECHNIQUES,0.13031161473087818,"waste. FlashDecoding++ further advances these improvements, incorporating sophisticated dynamic
134"
ATTENTION OPTIMIZATION TECHNIQUES,0.13125590179414542,"batching and more refined state management to significantly boost throughput and reduce latency.
135"
ATTENTION OPTIMIZATION TECHNIQUES,0.13220018885741266,"Concerning long sequence training, RingAttention [42] is notable for its efficiency in distributed
136"
ATTENTION OPTIMIZATION TECHNIQUES,0.13314447592067988,"training contexts, managing communication overhead and memory utilization effectively across
137"
ATTENTION OPTIMIZATION TECHNIQUES,0.13408876298394712,"multiple nodes.
138"
ATTENTION OPTIMIZATION TECHNIQUES,0.13503305004721436,"Another class of study targets on the sparsity/low-rank of attention computation. The Sparse Trans-
139"
ATTENTION OPTIMIZATION TECHNIQUES,0.1359773371104816,"former [10] revolutionizes sequence processing with log-linear complexity. Similarly, Reformer [43]
140"
ATTENTION OPTIMIZATION TECHNIQUES,0.1369216241737488,"optimizes memory via locality-sensitive hashing, while Big Bird [38] introduces a hybrid attention
141"
ATTENTION OPTIMIZATION TECHNIQUES,0.13786591123701605,"method to manage longer sequences efficiently. Linformer [44] reduces complexity using low-rank
142"
ATTENTION OPTIMIZATION TECHNIQUES,0.1388101983002833,"approximations, significantly economizing computation and storage requirements. Both of the pre-
143"
ATTENTION OPTIMIZATION TECHNIQUES,0.1397544853635505,"viously discussed solutions either compromise precision or yield only marginal enhancements in
144"
ATTENTION OPTIMIZATION TECHNIQUES,0.14069877242681775,"efficiency. Conversely, our proposed FlashMask is capable of delivering an exact computations.
145"
ATTENTION OPTIMIZATION TECHNIQUES,0.141643059490085,"3
FlashMask: Algorithm and Analysis
146"
ATTENTION OPTIMIZATION TECHNIQUES,0.14258734655335223,"In this section, we present the critical design of the column-wise sparse mask representation, imple-
147"
ATTENTION OPTIMIZATION TECHNIQUES,0.14353163361661944,"mentation of the mask computation kernel, and a complexity analysis of the proposed FlashMask.
148"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14447592067988668,"3.1
Column-wise Sparse Mask Representation
149"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14542020774315392,"We introduce FlashMask, a column-wise sparse masking technique, represented using FMS, FME ‚àà
150"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14636449480642116,"RùëÅ(the row index of Flash Mask Start and Flash Mask End), where FMSùëê, FMEùëêdenote that
151"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14730878186968838,"elements in the ùëê-th column of the attention score matrix S = QKùëáwithin the interval [FMSùëê, FMEùëê)
152"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14825306893295562,"are masked (set to ‚àí‚àû). As shown in Fig. 2(a), FMS = [4, 4, 4, 4, 10, 10, 10, 10, 10, 10], FME =
153"
COLUMN-WISE SPARSE MASK REPRESENTATION,0.14919735599622286,"[7, 7, 7, 7, 10, 10, 10, 10, 10, 10] indicates that, for the first column, the 4-th to 6-th rows are masked.
154"
INTEGRATION WITH FLASHATTENTION,0.1501416430594901,"3.2
Integration with FlashAttention
155"
INTEGRATION WITH FLASHATTENTION,0.1510859301227573,"Unidirectional (causal) attention, commonly utilized in large language models, incorporates Flash-
156"
INTEGRATION WITH FLASHATTENTION,0.15203021718602455,"Mask within the FlashAttention-2 algorithm, as detailed in Algorithm 1. This paper elaborates the
157"
INTEGRATION WITH FLASHATTENTION,0.1529745042492918,"implementation of FlashMask using the lower triangular section of the mask for illustration, where
158"
INTEGRATION WITH FLASHATTENTION,0.153918791312559,"the blue section represents the computation by the dense mask method (for comparison and not
159"
INTEGRATION WITH FLASHATTENTION,0.15486307837582625,Algorithm 1 Optimized Forward Pass with FlashMask
INTEGRATION WITH FLASHATTENTION,0.1558073654390935,"Require: Matrices Q, K, V ‚ààRùëÅ√óùëëin HBM, block sizes ùêµùëê, ùêµùëü, dense mask D ‚ààRùëÅ√óùëÅ, column-wise sparse
mask starting rows FMS ‚ààRùëÅ, ending rows FME ‚ààRùëÅ.
1: Divide Q into ùëáùëü=
l
ùëÅ
ùêµùëü"
INTEGRATION WITH FLASHATTENTION,0.15675165250236073,"m
blocks Q1, . . . , Qùëáùëüof size ùêµùëü√ó ùëëeach, and divide K, V in to ùëáùëê=
l
ùëÅ
ùêµùëê"
INTEGRATION WITH FLASHATTENTION,0.15769593956562794,"m
blocks
K1, . . . , Kùëáùëêand V1, . . . , Vùëáùëê, of size ùêµùëê√ó ùëëeach.
2: Divide the output O ‚ààRùëÅ√óùëëinto ùëáùëüblocks Oùëñ, . . . , Oùëáùëüof size ùêµùëü√ó ùëëeach, and divide the logsumexp ùêø
into ùëáùëüblocks ùêøùëñ, . . . , ùêøùëáùëüof size ùêµùëüeach.
3: Divide D into ùëáùëü√ó ùëáùëêblocks D1,1, ..., Dùëáùëü,ùëáùëê.
4: Divide FMS into ùëáùëêblocks FMS1, ..., FMSùëáùëê, and divide FME into FME1, ..., FMEùëáùëê.
5: Precompute the max value maxFMS1, ..., maxFMSùëáùëêfor each FMS1, ..., FMSùëáùëê, write to HBM.
6: Precompute the max value maxFME1, ..., maxFMEùëáùëêfor each FME1, ..., FMEùëáùëê, write to HBM.
7: Precompute the min value minFMS1, ..., minFMSùëáùëêfor each FMS1, ..., FMSùëáùëê, write to HBM.
8: Precompute the min value minFME1, ..., minFMEùëáùëêfor each FME1, ..., FMEùëáùëê, write to HBM.
9: for 1 ‚â§ùëñ‚â§ùëáùëüdo
10:
Load Qùëñfrom HBM to on-chip SRAM.
11:
On chip, initialize O(0)
ùëñ
= (0)ùêµùëü√óùëë‚ààRùêµùëü√óùëë, ‚Ñì(0)
ùëñ
= (0)ùêµùëü‚ààRùêµùëü, ùëö(0)
ùëñ
= (‚àí‚àû)ùêµùëü‚ààRùêµùëü.
12:
for 1 ‚â§ùëó‚â§ùëáùëêdo
13:
if ùëñ√ó ùêµùëü‚â•maxFMSùëóand (ùëñ+ 1) √ó ùêµùëü‚â§minFMEùëóthen
14:
Continue
15:
end if
16:
Load Kùëó, V ùëófrom HBM to on-chip SRAM.
17:
Load FMS ùëófrom HBM to on-chip SRAM.
18:
Load FMEùëófrom HBM to on-chip SRAM."
INTEGRATION WITH FLASHATTENTION,0.15864022662889518,"19:
On chip, compute S( ùëó)
ùëñ
= QùëñKùëá
ùëó‚ààRùêµùëü√óùêµùëê."
INTEGRATION WITH FLASHATTENTION,0.15958451369216242,"20:
On chip, set S( ùëó)
ùëñ
= S( ùëó)
ùëñ
+ Dùëñ, ùëó
21:
if (ùëñ+ 1) √ó ùêµùëü‚â•minFMS ùëóand ùëñ√ó ùêµùëü‚â§maxFMEùëóthen"
INTEGRATION WITH FLASHATTENTION,0.16052880075542966,"22:
On chip, set S( ùëó)
ùëñ
[ùë•][ùë¶] = ‚àí‚àû, ‚àÄùë•, ùë¶, such that FMS ùëó[ùë¶] ‚â§ùëñ√ó ùêµùëü+ ùë•‚â§FMEùëó[ùë¶]
23:
end if
24:
On chip, compute ùëö( ùëó)
ùëñ
= max(ùëö( ùëó‚àí1)
ùëñ
, rowmax(S( ùëó)
ùëñ
)) ‚ààRùêµùëü, ÀúP( ùëó)
ùëñ
= exp(S( ùëó)
ùëñ
‚àíùëö( ùëó)
ùëñ
) ‚àà"
INTEGRATION WITH FLASHATTENTION,0.16147308781869688,"Rùêµùëü√óùêµùëê(pointwise), ‚Ñì( ùëó)
ùëñ
= ùëíùëöùëó‚àí1
ùëñ
‚àíùëö( ùëó)
ùëñ‚Ñì( ùëó‚àí1)
ùëñ
+ rowsum( ÀúP( ùëó)
ùëñ
) ‚ààRùêµùëü."
INTEGRATION WITH FLASHATTENTION,0.16241737488196412,"25:
On chip, compute O( ùëó)
ùëñ
= diag(ùëíùëö( ùëó‚àí1)
ùëñ
‚àíùëö( ùëó)
ùëñ)‚àí1O( ùëó‚àí1)
ùëñ
+ ÀúP( ùëó)
ùëñ
V ùëó.
26:
end for
27:
On chip, compute Oùëñ= diag(‚Ñì(ùëáùëê)
ùëñ
)‚àí1O(ùëáùëê)
ùëñ
."
INTEGRATION WITH FLASHATTENTION,0.16336166194523136,"28:
On chip, compute ùêøùëñ= ùëö(ùëáùëê)
ùëñ
+ log(‚Ñì(ùëáùëê)
ùëñ
).
29:
Write Oùëñto HBM as the ùëñ-th block of O.
30:
Write ùêøùëñto HBM as the ùëñ-th block of ùêø.
31: end for
32: Return the output O and the logsumexp ùêø."
INTEGRATION WITH FLASHATTENTION,0.1643059490084986,"present in FlashMask) and the red section indicates the FlashMask computation. FlashAttention
160"
INTEGRATION WITH FLASHATTENTION,0.1652502360717658,"Forward involves two nested loops; the outer loop iterates over each block Qùëñof Q, and the inner
161"
INTEGRATION WITH FLASHATTENTION,0.16619452313503305,"loop iterates over all blocks K ùëóof K and V ùëóof V. In the inner loop, S( ùëó)
ùëñ
= QKùëáis computed on
162"
INTEGRATION WITH FLASHATTENTION,0.1671388101983003,"SRAM. Once S( ùëó)
ùëñ
is generated, the corresponding dense mask is added as a bias (shown in line 20 of
163"
INTEGRATION WITH FLASHATTENTION,0.1680830972615675,"Algorithm 1), whereas FlashMask applies the column-wise sparse mask by setting elements beyond
164"
INTEGRATION WITH FLASHATTENTION,0.16902738432483475,"FMSùëêbut not exceeding FMEùëêto ‚àí‚àû(as shown in lines 21 to 23 of Algorithm 1).
165"
INTEGRATION WITH FLASHATTENTION,0.16997167138810199,"FlashMask further exploits the block computation feature of FlashAttention-2 to reduce computation.
166"
INTEGRATION WITH FLASHATTENTION,0.17091595845136923,"If all elements within a block are masked, the block‚Äôs computation, including matrix multiplication
167"
INTEGRATION WITH FLASHATTENTION,0.17186024551463644,"and softmax operations, can be skipped. A block defined by rows [ùëü0, ùëü1) and columns [ùëê0, ùëê1) is
168"
INTEGRATION WITH FLASHATTENTION,0.17280453257790368,"skipped if ùëü0 ‚â•max(FMSùëê0:ùëê1) and ùëü1 ‚â§min(FMEùëê0:ùëê1). Considering that mask regions often
169"
INTEGRATION WITH FLASHATTENTION,0.17374881964117092,"exhibit continuity, most blocks are either completely masked or not at all, with only boundary blocks
170"
INTEGRATION WITH FLASHATTENTION,0.17469310670443816,"requiring fine-grained masking. A block is completely unmasked if every coordinate (ùëü, ùëê) satisfies
171"
INTEGRATION WITH FLASHATTENTION,0.17563739376770537,"ùëü< FMSùëêor ùëü‚â•FMEùëê, thus skipping fine-grained masking and avoiding extra masking overhead.
172"
INTEGRATION WITH FLASHATTENTION,0.17658168083097261,"To avoid redundant computations in the FlashAttention-2 compute loop, we precompute
173"
INTEGRATION WITH FLASHATTENTION,0.17752596789423986,"max(FMEùëê0:ùëê1) and min(FMEùëê0:ùëê1) for each block before the execution loop using a kernel. This
174"
INTEGRATION WITH FLASHATTENTION,0.17847025495750707,"computation has a complexity of O(ùëÅ) and can be easily distributed over ùëáùëê=
l
ùëÅ
ùêµùëê"
INTEGRATION WITH FLASHATTENTION,0.1794145420207743,"m
thread blocks. A
175"
INTEGRATION WITH FLASHATTENTION,0.18035882908404155,"parallel reduction operation within each thread block then computes the maximum and minimum
176"
INTEGRATION WITH FLASHATTENTION,0.1813031161473088,"values, yielding ùëáùëêvalues. The additional space complexity introduced here is O(ùëáùëê). Similar
177"
INTEGRATION WITH FLASHATTENTION,0.182247403210576,"computations are made for max(FMSùëê0:ùëê1), min(FMSùëê0:ùëê1),.
178"
INTEGRATION WITH FLASHATTENTION,0.18319169027384324,"The backward computation in FlashAttention-2, which is typically column-parallel, benefits more
179"
INTEGRATION WITH FLASHATTENTION,0.18413597733711048,"from the column sparse mask approach. Blocks for which
j max(FMSùëê0:ùëê1 ) ùêµùëü"
INTEGRATION WITH FLASHATTENTION,0.18508026440037773,"k
< ùëñ<
j min(FMEùëê0:ùëê1 ) ùêµùëü"
INTEGRATION WITH FLASHATTENTION,0.18602455146364494,"k
are
180"
INTEGRATION WITH FLASHATTENTION,0.18696883852691218,"fully masked, allowing skipping of these intervals directly. Only blocks satisfying
j min(FMSùëê0:ùëê1 ) ùêµùëü"
INTEGRATION WITH FLASHATTENTION,0.18791312559017942,"k
‚â§
181"
INTEGRATION WITH FLASHATTENTION,0.18885741265344666,"ùëñ‚â§
j max(FMEùëê0:ùëê1 ) ùêµùëü"
INTEGRATION WITH FLASHATTENTION,0.18980169971671387,"k
require fine-grained masking.
182"
INTEGRATION WITH FLASHATTENTION,0.1907459867799811,"It is important to note that unlike various approximate attention algorithms, our method ensures
183"
INTEGRATION WITH FLASHATTENTION,0.19169027384324835,"that each effective element of the attention score matrix is computed identically to FlashAttention-2,
184"
INTEGRATION WITH FLASHATTENTION,0.19263456090651557,"with masked elements explicitly set to ‚àí‚àû, thus maintaining the accuracy of the algorithm‚Äôs results.
185"
INTEGRATION WITH FLASHATTENTION,0.1935788479697828,"Futhermore, FlashMask is easily extendable to bidirectional attention computations.
186"
COMPLEXITY ANALYSIS,0.19452313503305005,"3.3
Complexity Analysis
187"
COMPLEXITY ANALYSIS,0.1954674220963173,"We define sparsity as ùúå=
ùëù
ùëÅ2 , where ùëùis the number of masked elements in the attention score matrix,
188"
COMPLEXITY ANALYSIS,0.1964117091595845,"and ùëÅis the maximum sequence length of Q and K, ùëÅ2 being the total number of elements in the
189"
COMPLEXITY ANALYSIS,0.19735599622285174,"attention score matrix. For a causal mask, ùúå= 2√óùëù"
COMPLEXITY ANALYSIS,0.19830028328611898,"ùëÅ2 since half of the elements in the attention score
190"
COMPLEXITY ANALYSIS,0.19924457034938622,"matrix are already masked by the causal mask. The block sparsity ùõºis defined as ùõº=
ùëé
l
ùëÅ
ùêµùëü"
COMPLEXITY ANALYSIS,0.20018885741265344,"m
√ó
l
ùëÅ
ùêµùëê"
COMPLEXITY ANALYSIS,0.20113314447592068,"m,
191"
COMPLEXITY ANALYSIS,0.20207743153918792,"where ùêµùëü, ùêµùëêare block sizes, and ùëéis the number of completely masked blocks. For a causal mask,
192"
COMPLEXITY ANALYSIS,0.20302171860245516,"ùõº=
2√óùëé
l
ùëÅ
ùêµùëü"
COMPLEXITY ANALYSIS,0.20396600566572237,"m
√ó
l
ùëÅ
ùêµùëê"
COMPLEXITY ANALYSIS,0.2049102927289896,"m.
193"
COMPLEXITY ANALYSIS,0.20585457979225685,"Space complexity. The dense mask is represented as D ‚ààRùëÅ√óùëÅ, with a space complexity of O(ùëÅ2).
194"
COMPLEXITY ANALYSIS,0.20679886685552407,"FlashMask denotes as FMS, FME ‚ààRùëÅ, occupying O(ùëÅ) space, along with four precomputed
195"
COMPLEXITY ANALYSIS,0.2077431539187913,"arrays maxFMS, minFMS, maxFME, minFME ‚ààR"
COMPLEXITY ANALYSIS,0.20868744098205855,"l
ùëÅ
ùêµùëê"
COMPLEXITY ANALYSIS,0.2096317280453258,"m
, also occupying O(ùëÅ) space. Thus, the
196"
COMPLEXITY ANALYSIS,0.210576015108593,"total space complexity for FlashMask is O(ùëÅ), significantly reducing memory usage and supporting
197"
COMPLEXITY ANALYSIS,0.21152030217186024,"training on longer sequences.
198"
COMPLEXITY ANALYSIS,0.21246458923512748,"Memory access complexity. The dense mask accesses the entire D ‚ààRùëÅ√óùëÅmatrix in line 20 of
199"
COMPLEXITY ANALYSIS,0.21340887629839472,"Algorithm 1, totaling ùëÅ2 memory accesses on HBM. FlashMask reads the FMS, FME ‚ààRùëÅvectors
200"
COMPLEXITY ANALYSIS,0.21435316336166194,"from HBM as shown in lines 17 and 18 of Algorithm 1, with each Qùëñreading the entire FMS, FME,
201"
COMPLEXITY ANALYSIS,0.21529745042492918,totaling 2√óùëáùëü√óùëÅmemory accesses. This reduces the memory access to approximately 2√óùëáùëü√óùëÅ
COMPLEXITY ANALYSIS,0.21624173748819642,"ùëÅ2
‚âà
2
ùêµùëü,
202"
COMPLEXITY ANALYSIS,0.21718602455146366,"significantly boosting performance. Due to FlashMask‚Äôs smaller space usage, it is possible to preload
203"
COMPLEXITY ANALYSIS,0.21813031161473087,"FMS, FME into SRAM using only 2 √ó ùêµùëêSRAM, enhancing memory access efficiency. For the
204"
COMPLEXITY ANALYSIS,0.2190745986779981,"backward process, which uses a column-parallel approach, SRAM-stored FMS, FME can be well
205"
COMPLEXITY ANALYSIS,0.22001888574126535,"reused, further reducing the total memory access on HBM to 2 √ó ùëÅ.
206"
COMPLEXITY ANALYSIS,0.22096317280453256,"Computational complexity. The attention computation process normally iterates over the entire
207"
COMPLEXITY ANALYSIS,0.2219074598677998,"attention score matrix, with a computational complexity of O(ùëÅ2). By skipping entirely masked
208"
COMPLEXITY ANALYSIS,0.22285174693106705,"blocks, FlashMask leverages block sparsity to reduce computational complexity to O((1 ‚àíùõº)ùëÅ2).
209"
EXPERIMENTS,0.2237960339943343,"4
Experiments
210"
SETUP,0.2247403210576015,"4.1
Setup
211"
SETUP,0.22568460812086874,"Experiments were conducted using GPU A800-SXM 80G, Intel(R) Xeon(R) Platinum 8350C CPUs,
212"
SETUP,0.22662889518413598,"CUDA 12.0, and driver version 525.125.06. We evaluated FlashMask against various methods
213"
SETUP,0.22757318224740322,"including Vanilla Attention, FlashAttention with dense mask (FA-DenseMask), variable length (FA-
214"
SETUP,0.22851746931067043,"Varlen), and sliding window (FA-Window) across different scenarios and sequence lengths. Both
215"
SETUP,0.22946175637393768,"kernel-level and end-to-end performance demonstrated the effectiveness of our method.
216"
DATA CONSTRUCTION,0.23040604343720492,"4.2
Data Construction
217"
DATA CONSTRUCTION,0.23135033050047216,"As mentioned in the Background section, commercial large models now support sequences up to
218"
DATA CONSTRUCTION,0.23229461756373937,"128K in length. FlashMask, with its lower memory overhead, can facilitate training with even longer
219"
DATA CONSTRUCTION,0.2332389046270066,"2
4
8
16
32
64
128
Sequence Length(K) 100 101 102 103 104"
DATA CONSTRUCTION,0.23418319169027385,Latency(ms) SFT
DATA CONSTRUCTION,0.23512747875354106,"FA-Varlen
FA-DenseMask
VanillaAttention
FlashMask"
DATA CONSTRUCTION,0.2360717658168083,"2
4
8
16
32
64
128
Sequence Length(K) 100 101 102 103 104"
DATA CONSTRUCTION,0.23701605288007555,Latency(ms) DPO
DATA CONSTRUCTION,0.23796033994334279,"FA-DenseMask
VanillaAttention
FlashMask"
DATA CONSTRUCTION,0.23890462700661,"2
4
8
16
32
64
128
Sequence Length(K) 100 101 102 103 104"
DATA CONSTRUCTION,0.23984891406987724,Latency(ms) RM
DATA CONSTRUCTION,0.24079320113314448,"FA-DenseMask
VanillaAttention
FlashMask"
DATA CONSTRUCTION,0.24173748819641172,"Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves
substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM)."
DATA CONSTRUCTION,0.24268177525967893,"contexts. However, currently available public datasets do not contain training data for scenarios
220"
DATA CONSTRUCTION,0.24362606232294617,"exceeding 128K. For comprehensive testing of FlashMask, we constructed synthetic data to simulate
221"
DATA CONSTRUCTION,0.24457034938621341,"long-sequence training.
222"
DATA CONSTRUCTION,0.24551463644948066,"For a given sequence length ùêø, sequences were generated by mimicking InToken method with several
223"
DATA CONSTRUCTION,0.24645892351274787,"sub-sequences. Randomly selecting ùë†‚àà[1, 10] split points uniformly within the range (0, ùêø), the
224"
DATA CONSTRUCTION,0.2474032105760151,"sequence was divided into ùë†sub-sequences. The segment from the last split point to the end of the
225"
DATA CONSTRUCTION,0.24834749763928235,"sequence was considered as Padding. For the RM scenario, shorter sequence lengths used a smaller
226"
DATA CONSTRUCTION,0.24929178470254956,"upper limit on the number of splits: ùë†‚àà[1, 3] for ùêø‚àà(0, 4096] and ùë†‚àà[1, 4] for ùêø‚àà(4096, 8192].
227"
DATA CONSTRUCTION,0.2502360717658168,"By discarding samples not meeting size requirements, we ensure each sub-sequence length was
228"
DATA CONSTRUCTION,0.251180358829084,"at least 128 (SFT, LoRA, DPO) or 512 (RM) and padding not exceeding 128 (SFT, LoRA, DPO)
229"
DATA CONSTRUCTION,0.2521246458923513,"or 512 (RM). Suppose one sub-sequence with length ùêø‚Ä≤ was further divided into a query and ùëò
230"
DATA CONSTRUCTION,0.2530689329556185,"answers based on the scenario. The length of each answer was randomly determined from the
231"
DATA CONSTRUCTION,0.25401322001888577,"range [
0.1ùêø‚Ä≤
1+0.1√óùëò,
0.2ùêø‚Ä≤
1+0.2√óùëò], making the answer lengths approximately [0.1, 0.2] of the query length.
232"
DATA CONSTRUCTION,0.254957507082153,"Therefore, the query length was equal to ùêø‚Ä≤ minus the total answer lengths. A total of 240 valid
233"
DATA CONSTRUCTION,0.2559017941454202,"samples per given sequence length ùêøwere collected and binned into 10 categories by sparsity ùúå, as
234"
DATA CONSTRUCTION,0.25684608120868746,"shown in Appendix A.2.
235"
DATA CONSTRUCTION,0.2577903682719547,"256
512
1024
2048
4096
8192
WindowSize 0 2 4 6 8 10 12"
DATA CONSTRUCTION,0.2587346553352219,Latency(ms)
DATA CONSTRUCTION,0.25967894239848915,Sequence Length 8K
DATA CONSTRUCTION,0.26062322946175637,"FA-Window
FlashMask"
DATA CONSTRUCTION,0.2615675165250236,"256
512
1024
2048
4096
8192
WindowSize 0 5 10 15 20 25 30"
DATA CONSTRUCTION,0.26251180358829085,Latency(ms)
DATA CONSTRUCTION,0.26345609065155806,Sequence Length 16K
DATA CONSTRUCTION,0.26440037771482533,"FA-Window
FlashMask"
DATA CONSTRUCTION,0.26534466477809254,"256
512
1024
2048
4096
8192
WindowSize 0 10 20 30 40 50 60 70"
DATA CONSTRUCTION,0.26628895184135976,Latency(ms)
DATA CONSTRUCTION,0.267233238904627,Sequence Length 32K
DATA CONSTRUCTION,0.26817752596789424,"FA-Window
FlashMask"
DATA CONSTRUCTION,0.26912181303116145,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 120 140 160 180"
DATA CONSTRUCTION,0.2700661000944287,Latency(ms) SFT
DATA CONSTRUCTION,0.27101038715769593,"FA-Varlen
FlashMask"
DATA CONSTRUCTION,0.2719546742209632,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 120 140 160 180"
DATA CONSTRUCTION,0.2728989612842304,Latency(ms) DPO
DATA CONSTRUCTION,0.2738432483474976,FlashMask
DATA CONSTRUCTION,0.2747875354107649,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 120 140 160 180"
DATA CONSTRUCTION,0.2757318224740321,Latency(ms) RM
DATA CONSTRUCTION,0.2766761095372993,FlashMask
DATA CONSTRUCTION,0.2776203966005666,"Figure 4: Top: Comparison of Kernel Latency while Varying Window Size. Bottom: Comparison of
Kernel Latency while Varying Input Sparsity."
KERNEL EXPERIMENTS,0.2785646836638338,"4.3
Kernel Experiments
236"
KERNEL EXPERIMENTS,0.279508970727101,"We conducted tests with batch sizes of 1, 2, and 4 using Vanilla Attention, FA-DenseMask, and
237"
KERNEL EXPERIMENTS,0.2804532577903683,"FlashMask. Each experiment began with 5 warm-up runs followed by 50 measurements, totaling 55
238"
KERNEL EXPERIMENTS,0.2813975448536355,"runs with kernel latency as the performance metric. Additional comparisons were made with FA-
239"
KERNEL EXPERIMENTS,0.28234183191690276,"Varlen in the SFT scenario. Results for batch size 1 are shown in Figure 3 (results for batch sizes 2 and
240"
KERNEL EXPERIMENTS,0.28328611898017,"4
8
16
32
64
128
256
Sequence Length(K) 600 800 1000 1200 1400 1600 1800"
KERNEL EXPERIMENTS,0.2842304060434372,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.28517469310670446,Speed Up 2.46x
KERNEL EXPERIMENTS,0.28611898016997167,LLaMA-7B
KERNEL EXPERIMENTS,0.2870632672332389,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.28800755429650615,"4
8
16
32
64
128
196
Sequence Length(K) 300 400 500 600 700 800 900 1000"
KERNEL EXPERIMENTS,0.28895184135977336,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.2898961284230406,Speed Up 2.35x
KERNEL EXPERIMENTS,0.29084041548630785,LLaMA-13B
KERNEL EXPERIMENTS,0.29178470254957506,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.2927289896128423,"4
8
16
32
64
96
Sequence Length(K) 100 120 140 160 180 200"
KERNEL EXPERIMENTS,0.29367327667610954,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.29461756373937675,Speed Up 1.96x
KERNEL EXPERIMENTS,0.295561850802644,LLaMA-70B
KERNEL EXPERIMENTS,0.29650613786591123,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.29745042492917845,(a) SFT
KERNEL EXPERIMENTS,0.2983947119924457,"4
8
16
32
64
128
256
512
544
Sequence Length(K) 2000 3000 4000 5000 6000 7000 8000"
KERNEL EXPERIMENTS,0.29933899905571293,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3002832861189802,Speed Up 4.16x
KERNEL EXPERIMENTS,0.3012275731822474,LLaMA-7B
KERNEL EXPERIMENTS,0.3021718602455146,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.3031161473087819,"4
8
16
32
64
128
224
Sequence Length(K) 400 600 800 1000 1200"
KERNEL EXPERIMENTS,0.3040604343720491,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3050047214353163,Speed Up 2.59x
KERNEL EXPERIMENTS,0.3059490084985836,LLaMA-13B
KERNEL EXPERIMENTS,0.3068932955618508,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.307837582625118,"4
8
16
32
64
128
Sequence Length(K) 100 120 140 160 180 200 220 240"
KERNEL EXPERIMENTS,0.3087818696883853,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3097261567516525,Speed Up 2.10x
KERNEL EXPERIMENTS,0.31067044381491976,LLaMA-70B
KERNEL EXPERIMENTS,0.311614730878187,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
KERNEL EXPERIMENTS,0.3125590179414542,(b) LoRA
KERNEL EXPERIMENTS,0.31350330500472146,"4
8
16
32
64
96
Sequence Length(K) 600 800 1000 1200 1400"
KERNEL EXPERIMENTS,0.31444759206798867,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3153918791312559,Speed Up 2.49x
KERNEL EXPERIMENTS,0.31633616619452315,LLaMA-7B
KERNEL EXPERIMENTS,0.31728045325779036,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.3182247403210576,"4
8
16
32
64
128
180
Sequence Length(K) 300 400 500 600 700 800"
KERNEL EXPERIMENTS,0.31916902738432484,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.32011331444759206,Speed Up 2.46x
KERNEL EXPERIMENTS,0.3210576015108593,LLaMA-13B
KERNEL EXPERIMENTS,0.32200188857412654,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.32294617563739375,"4
8
16
32
64
80
Sequence Length(K) 80 100 120 140 160"
KERNEL EXPERIMENTS,0.323890462700661,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.32483474976392823,Speed Up 2.02x
KERNEL EXPERIMENTS,0.32577903682719545,LLaMA-70B
KERNEL EXPERIMENTS,0.3267233238904627,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.3276676109537299,(c) DPO
KERNEL EXPERIMENTS,0.3286118980169972,"4
8
16
32
64
128
256
Sequence Length(K) 600 800 1000 1200 1400 1600 1800 2000"
KERNEL EXPERIMENTS,0.3295561850802644,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3305004721435316,Speed Up 2.60x
KERNEL EXPERIMENTS,0.3314447592067989,LLaMA-7B
KERNEL EXPERIMENTS,0.3323890462700661,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.3333333333333333,"4
8
16
32
64
128
196
Sequence Length(K) 300 400 500 600 700 800 900 1000"
KERNEL EXPERIMENTS,0.3342776203966006,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.3352219074598678,Speed Up 2.51x
KERNEL EXPERIMENTS,0.336166194523135,LLaMA-13B
KERNEL EXPERIMENTS,0.3371104815864023,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.3380547686496695,"4
8
16
32
64
96
Sequence Length(K) 100 120 140 160 180 200 220"
KERNEL EXPERIMENTS,0.33899905571293676,Tokens/Sec/GPU
KERNEL EXPERIMENTS,0.33994334277620397,Speed Up 2.04x
KERNEL EXPERIMENTS,0.3408876298394712,LLaMA-70B
KERNEL EXPERIMENTS,0.34183191690273845,"VanillaAttention
FA-DenseMask
FlashMask"
KERNEL EXPERIMENTS,0.34277620396600567,"(d) RM
Figure 5: Comparison of End-to-End Training Throughput on Synthetic Dataset."
KERNEL EXPERIMENTS,0.3437204910292729,"4 can be found in Appendix A.3). FlashMask demonstrated significant latency advantages across all
241"
KERNEL EXPERIMENTS,0.34466477809254015,"lengths, up to 8.3-fold time saving compared to FA-DenseMask. Vanilla Attention was significantly
242"
KERNEL EXPERIMENTS,0.34560906515580736,"more time-consuming and exceeded memory limits at lengths greater than 32K. The closest competitor
243"
KERNEL EXPERIMENTS,0.3465533522190746,"to FlashMask, FA-Varlen, exhibited higher latencies as sequence lengths increased. Similar trends
244"
KERNEL EXPERIMENTS,0.34749763928234184,"were observed in the DPO and RM scenarios, with FlashMask significantly outperforming FA-
245"
KERNEL EXPERIMENTS,0.34844192634560905,"DenseMask and Vanilla Attention, especially in the RM scenario where higher sparsity levels
246"
KERNEL EXPERIMENTS,0.3493862134088763,"further enhanced FlashMask‚Äôs effectiveness. Performance benefits from varying sparsity levels
247"
KERNEL EXPERIMENTS,0.35033050047214354,"were also quantified, with FlashMask showing linear negative correlation with increasing sparsity,
248"
KERNEL EXPERIMENTS,0.35127478753541075,"demonstrating efficient utilization of sample sparsity for acceleration. FlashMask‚Äôs capability to
249"
KERNEL EXPERIMENTS,0.352219074598678,"perform sliding window attention was further tested against FA-Window with window sizes of 256,
250"
KERNEL EXPERIMENTS,0.35316336166194523,"512, 1024, 2048, 4096, and 8192, as shown in Figure 4 Top. FlashMask matched FA-Window in
251"
KERNEL EXPERIMENTS,0.35410764872521244,"latency across sequence lengths of 8K, 16K, and 32K, showing comparable delay performances at
252"
KERNEL EXPERIMENTS,0.3550519357884797,"increasing window sizes.
253"
END-TO-END EXPERIMENTS,0.3559962228517469,"4.4
End-to-End Experiments
254"
END-TO-END EXPERIMENTS,0.35694050991501414,"The end-to-end performance1 of the model was tested using synthetic datasets across three scales of the
255"
END-TO-END EXPERIMENTS,0.3578847969782814,"LLaMA2 model and four downstream scenarios (SFT, LoRA, DPO, RM) at various sequence lengths,
256"
END-TO-END EXPERIMENTS,0.3588290840415486,"measuring throughput in average Tokens/Sec/GPU. Each sequence length of 240 valid samples was
257"
END-TO-END EXPERIMENTS,0.3597733711048159,"trained for one epoch, with results presented in Figure 5. In the SFT scenario, FlashMask showed a
258"
END-TO-END EXPERIMENTS,0.3607176581680831,"clear throughput advantage over FA-DenseMask and Vanilla Attention, performing comparably to FA-
259"
END-TO-END EXPERIMENTS,0.3616619452313503,"Varlen. As sequence lengths increased, the throughput advantage of FlashMask over FA-DenseMask
260"
END-TO-END EXPERIMENTS,0.3626062322946176,"and Vanilla Attention also enhanced, even enabling the completion of longer sequence tasks within the
261"
END-TO-END EXPERIMENTS,0.3635505193578848,"same computational resources. In LoRA, DPO, and RM scenarios, FlashMask consistently showed
262"
END-TO-END EXPERIMENTS,0.364494806421152,"significant advantages. Notably, in the LoRA scenario at the LLaMA2-7B, FlashMask achieved a
263"
END-TO-END EXPERIMENTS,0.3654390934844193,"4.16x throughput improvement over FA-DenseMask, supporting sequence lengths up to 544K. It‚Äôs
264"
END-TO-END EXPERIMENTS,0.3663833805476865,"important to note that FA-Varlen was unable to support the DPO and RM scenarios with the answers
265"
END-TO-END EXPERIMENTS,0.36732766761095376,"sharing one question, whereas FlashMask was capable of handling various scenarios including DPO
266"
END-TO-END EXPERIMENTS,0.36827195467422097,"and RM.
267"
END-TO-END EXPERIMENTS,0.3692162417374882,"Additional experiments were conducted on the open-source dataset LongBench [45], comparing the
268"
END-TO-END EXPERIMENTS,0.37016052880075545,"end-to-end performance of FA-DenseMask, FA-Varlen, and FlashMask at sequence lengths of 16K,
269"
END-TO-END EXPERIMENTS,0.37110481586402266,"32K, and 64K. The performance improvements were consistent with those observed in the synthetic
270"
END-TO-END EXPERIMENTS,0.3720491029272899,"dataset. The detailed results are presented in Appendix A.3. Memory usage during the experiments
271"
END-TO-END EXPERIMENTS,0.37299338999055714,"was also recorded, showing significant reductions for FlashMask compared to FA-DenseMask, with
272"
END-TO-END EXPERIMENTS,0.37393767705382436,"detailed results presented in Appendix A.3.
273"
END-TO-END EXPERIMENTS,0.37488196411709157,"1To simplify the tuning of hyperparameters, we standardize the global batch size to 16, with a batch size of 1
per device. Additional training hyperparameters are detailed in Table 1"
DISCUSSION,0.37582625118035884,"5
Discussion
274"
DISCUSSION,0.37677053824362605,"Several key topics emerge that are crucial for comprehending the full scope and implications of
275"
DISCUSSION,0.3777148253068933,"FlashMask. These include the rationale behind the design choices, adaptations for supporting
276"
DISCUSSION,0.37865911237016053,"bidirectional and other custom masks, and the necessity as well as limits of the current approach.
277"
DISCUSSION,0.37960339943342775,"Necessity and Scope of the Study. The substantial advancement rendered by FlashMask in improving
278"
DISCUSSION,0.380547686496695,"attention mask computation is a significant evolution over the current FlashAttention framework.
279"
DISCUSSION,0.3814919735599622,"Notably, FlashMask addresses and significantly mitigates the limitations observed with FlashAttention
280"
DISCUSSION,0.38243626062322944,"in handling conventional and custom mask computations. This enhancement not only broadens the
281"
DISCUSSION,0.3833805476864967,"applicative reach of FlashAttention but also signifies a key shift in efficiency metrics critical for
282"
DISCUSSION,0.3843248347497639,"Transformer architectures. More importantly, the flexibility of FlashMask extends beyond the
283"
DISCUSSION,0.38526912181303113,"proprietary boundaries of FlashAttention, offering potential benefits to a wider range of Transformer-
284"
DISCUSSION,0.3862134088762984,"based models. By facilitating more efficient computation of the attention mechanism, FlashMask
285"
DISCUSSION,0.3871576959395656,"enables innovations in processing vast datasets and complex models, thereby improving performances
286"
DISCUSSION,0.3881019830028329,"across varied applications in the LLM field. This cross-model adaptability confirms the robustness
287"
DISCUSSION,0.3890462700661001,"and utility of FlashMask as a universally applicable enhancement tool within and potentially outside
288"
DISCUSSION,0.3899905571293673,"the Transformer architecture spectrum, promising substantial gains in computational efficiency and
289"
DISCUSSION,0.3909348441926346,"model scalability.
290"
DISCUSSION,0.3918791312559018,"Bidirectional and Custom Masks. In the exploration of attention mechanisms, the introduction of
291"
DISCUSSION,0.392823418319169,"FlashMask as discussed in this study offers a significant leap in computational efficiency, particularly
292"
DISCUSSION,0.3937677053824363,"for masking processes in unidirectional attention mechanisms. By extending this approach to
293"
DISCUSSION,0.3947119924457035,"bidirectional networks through the simple addition of vectors indicating the start and end indices
294"
DISCUSSION,0.39565627950897075,"of the mask, FlashMask transcends conventional computational bounds, casting itself not just as
295"
DISCUSSION,0.39660056657223797,"a sparse attention methodology, but as a versatile computational paradigm. Its adaptability across
296"
DISCUSSION,0.3975448536355052,"various custom masking tasks and ability to effectively manage diverse types of mask combinations
297"
DISCUSSION,0.39848914069877245,"underscores its potential to greatly enhance the efficiency of attention computations. Moreover, the
298"
DISCUSSION,0.39943342776203966,"inherent sparsity of the attention mask during inference provides a robust justification for employing
299"
DISCUSSION,0.4003777148253069,"FlashMask, indicating its utility and effectiveness in practical applications. This paradigm shift
300"
DISCUSSION,0.40132200188857414,"highlights the importance of developing scalable and efficient computational strategies in the evolving
301"
DISCUSSION,0.40226628895184136,"landscape of transformer architectures, suggesting that future research should continue to leverage
302"
DISCUSSION,0.40321057601510857,"these innovations to tackle increasing computational demands.
303"
DISCUSSION,0.40415486307837584,"Limitations and Future Directions. While FlashMask demonstrates impressive performance in
304"
DISCUSSION,0.40509915014164305,"handling long-context sequences, it is observed that the computational cost of training Transformers
305"
DISCUSSION,0.4060434372049103,"increases more than linearly as the sequence length grows‚Äînot only due to the computation of
306"
DISCUSSION,0.40698772426817753,"masked attention but also because of the extensive use of other operators. This scenario highlights the
307"
DISCUSSION,0.40793201133144474,"inevitable need for leveraging or integrating distributed computing strategies or further algorithmic
308"
DISCUSSION,0.408876298394712,"enhancements to elevate training efficiency. Such advancements could be practical in managing
309"
DISCUSSION,0.4098205854579792,"the computationally intensive tasks involved in processing extended contexts efficiently. As a part
310"
DISCUSSION,0.41076487252124644,"of future research directions, exploring synergistic solutions that combine the strengths of both
311"
DISCUSSION,0.4117091595845137,"algorithmic innovation (like FlashMask) and distributed system designs stands as a promising venture.
312"
DISCUSSION,0.4126534466477809,"This approach is anticipated to address scalability challenges and could set the stage for breakthroughs
313"
DISCUSSION,0.41359773371104813,"in handling unprecedentedly large data sets and complex model architectures.
314"
CONCLUSION,0.4145420207743154,"6
Conclusion
315"
CONCLUSION,0.4154863078375826,"In this paper, we introduced FlashMask, a groundbreaking attention computation paradigm designed
316"
CONCLUSION,0.4164305949008499,"to tackle the high computational and memory demands inherent in conventional attention mechanisms
317"
CONCLUSION,0.4173748819641171,"in large-scale transformers. By implementing a novel column-wise sparse representation of attention
318"
CONCLUSION,0.4183191690273843,"masks, FlashMask substantially reduces the memory and computational complexity from quadratic to
319"
CONCLUSION,0.4192634560906516,"linear with the sequence length, thereby enhancing processing speeds and efficiency. Our algorithm
320"
CONCLUSION,0.4202077431539188,"demonstrates versatility across various masking scenarios and retains robust performance in different
321"
CONCLUSION,0.421152030217186,"training pipelines. Extensive empirical analysis confirms that FlashMask accelerates computational
322"
CONCLUSION,0.42209631728045327,"speed significantly, achieving up to 8.3x speedup in common modalities comparable to state-of-the-art
323"
CONCLUSION,0.4230406043437205,"methods like FlashAttention. This advancement marks a significant leap forward in the design of
324"
CONCLUSION,0.42398489140698775,"attention computation, offering the potential for broader applications and setting a new benchmark in
325"
CONCLUSION,0.42492917847025496,"the efficiency of processing long-context sequences.
326"
REFERENCES,0.4258734655335222,"References
327"
REFERENCES,0.42681775259678945,"[1] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-efficient
328"
REFERENCES,0.42776203966005666,"exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344‚Äì16359,
329"
REFERENCES,0.42870632672332387,"2022.
330"
REFERENCES,0.42965061378659114,"[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
331"
REFERENCES,0.43059490084985835,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
332"
REFERENCES,0.43153918791312557,"30, 2017.
333"
REFERENCES,0.43248347497639283,"[3] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt
334"
REFERENCES,0.43342776203966005,"Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction
335"
REFERENCES,0.4343720491029273,"meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.
336"
REFERENCES,0.43531633616619453,"[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
337"
REFERENCES,0.43626062322946174,"Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.
338"
REFERENCES,0.437204910292729,"Journal of Machine Learning Research, 25(70):1‚Äì53, 2024.
339"
REFERENCES,0.4381491973559962,"[5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
340"
REFERENCES,0.43909348441926344,"Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
341"
REFERENCES,0.4400377714825307,"human feedback. Advances in neural information processing systems, 35:27730‚Äì27744, 2022.
342"
REFERENCES,0.4409820585457979,"[6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and
343"
REFERENCES,0.44192634560906513,"Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv
344"
REFERENCES,0.4428706326723324,"preprint arXiv:2402.03300, 2024.
345"
REFERENCES,0.4438149197355996,"[7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
346"
REFERENCES,0.4447592067988669,"Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
347"
REFERENCES,0.4457034938621341,"preprint arXiv:2303.08774, 2023.
348"
REFERENCES,0.4466477809254013,"[8] Anthropic. Introducing claude. https://www.anthropic.com/news/introducing-claude, 2024.
349"
REFERENCES,0.4475920679886686,"Accessed: May 20, 2024.
350"
REFERENCES,0.4485363550519358,"[9] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
351"
REFERENCES,0.449480642115203,"Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking
352"
REFERENCES,0.45042492917847027,"multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
353"
REFERENCES,0.4513692162417375,"[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
354"
REFERENCES,0.4523135033050047,"transformers. arXiv preprint arXiv:1904.10509, 2019.
355"
REFERENCES,0.45325779036827196,"[11] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R√©. Scatterbrain: Unifying
356"
REFERENCES,0.4542020774315392,"sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:17413‚Äì17426,
357"
REFERENCES,0.45514636449480644,"2021.
358"
REFERENCES,0.45609065155807366,"[12] Zhiqing Sun, Yiming Yang, and Shinjae Yoo. Sparse attention with learning to hash. In International
359"
REFERENCES,0.45703493862134087,"Conference on Learning Representations, 2021.
360"
REFERENCES,0.45797922568460814,"[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
361"
REFERENCES,0.45892351274787535,"Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
362"
REFERENCES,0.45986779981114256,"2021.
363"
REFERENCES,0.46081208687440983,"[14] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-
364"
REFERENCES,0.46175637393767704,"Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint
365"
REFERENCES,0.4627006610009443,"arXiv:2402.09353, 2024.
366"
REFERENCES,0.4636449480642115,"[15] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:
367"
REFERENCES,0.46458923512747874,"Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.
368"
REFERENCES,0.465533522190746,"[16] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
369"
REFERENCES,0.4664778092540132,"Direct preference optimization: Your language model is secretly a reward model. Advances in Neural
370"
REFERENCES,0.46742209631728043,"Information Processing Systems, 36, 2024.
371"
REFERENCES,0.4683663833805477,"[17] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng
372"
REFERENCES,0.4693106704438149,"Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model
373"
REFERENCES,0.4702549575070821,"alignment. arXiv preprint arXiv:2304.06767, 2023.
374"
REFERENCES,0.4711992445703494,"[18] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu.
375"
REFERENCES,0.4721435316336166,"Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.
376"
REFERENCES,0.4730878186968839,"[19] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model
377"
REFERENCES,0.4740321057601511,"alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.
378"
REFERENCES,0.4749763928234183,"[20] Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad
379"
REFERENCES,0.47592067988668557,"Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank.
380"
REFERENCES,0.4768649669499528,"arXiv preprint arXiv:2402.01878, 2024.
381"
REFERENCES,0.47780925401322,"[21] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models.
382"
REFERENCES,0.47875354107648727,"arXiv preprint arXiv:2303.00001, 2023.
383"
REFERENCES,0.4796978281397545,"[22] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John
384"
REFERENCES,0.4806421152030217,"Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050,
385"
REFERENCES,0.48158640226628896,"2023.
386"
REFERENCES,0.4825306893295562,"[23] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu
387"
REFERENCES,0.48347497639282344,"Zhou, Chenyu Shi, et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint
388"
REFERENCES,0.48441926345609065,"arXiv:2401.06080, 2024.
389"
REFERENCES,0.48536355051935787,"[24] Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure
390"
REFERENCES,0.48630783758262514,"Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights
391"
REFERENCES,0.48725212464589235,"fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024.
392"
REFERENCES,0.48819641170915956,"[25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
393"
REFERENCES,0.48914069877242683,"optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
394"
REFERENCES,0.49008498583569404,"[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
395"
REFERENCES,0.4910292728989613,"tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
396"
REFERENCES,0.4919735599622285,"[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
397"
REFERENCES,0.49291784702549574,"models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
398"
REFERENCES,0.493862134088763,"[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
399"
REFERENCES,0.4948064211520302,"Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
400"
REFERENCES,0.49575070821529743,"language models. arXiv preprint arXiv:2302.13971, 2023.
401"
REFERENCES,0.4966949952785647,"[29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
402"
REFERENCES,0.4976392823418319,"Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
403"
REFERENCES,0.4985835694050991,"fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
404"
REFERENCES,0.4995278564683664,"[30] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi,
405"
REFERENCES,0.5004721435316336,"Xianglong Liu, and Michele Magno. How good are low-bit quantized llama3 models? an empirical study.
406"
REFERENCES,0.5014164305949008,"arXiv preprint arXiv:2404.14047, 2024.
407"
REFERENCES,0.502360717658168,"[31] Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgibbon. Efficient sequence packing
408"
REFERENCES,0.5033050047214354,"without cross-contamination: Accelerating large language models without impacting performance. arXiv
409"
REFERENCES,0.5042492917847026,"preprint arXiv:2107.02027, 2021.
410"
REFERENCES,0.5051935788479698,"[32] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,
411"
REFERENCES,0.506137865911237,"Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n‚Äôpack: Navit, a
412"
REFERENCES,0.5070821529745042,"vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems,
413"
REFERENCES,0.5080264400377715,"36, 2024.
414"
REFERENCES,0.5089707271010387,"[33] PaddleNLP Contributors. Paddlenlp: An easy-to-use and high performance nlp library. https://github.
415"
REFERENCES,0.509915014164306,"com/PaddlePaddle/PaddleNLP, 2021.
416"
REFERENCES,0.5108593012275732,"[34] BYTEDANCE INC.
Effective transformer.
https://github.com/bytedance/effective_
417"
REFERENCES,0.5118035882908404,"transformer, 2021.
418"
REFERENCES,0.5127478753541076,"[35] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
419"
REFERENCES,0.5136921624173749,"Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.
420"
REFERENCES,0.5146364494806421,"[36] Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig.
421"
REFERENCES,0.5155807365439093,"In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200,
422"
REFERENCES,0.5165250236071766,"2024.
423"
REFERENCES,0.5174693106704438,"[37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language
424"
REFERENCES,0.5184135977337111,"models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.
425"
REFERENCES,0.5193578847969783,"[38] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
426"
REFERENCES,0.5203021718602455,"Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.
427"
REFERENCES,0.5212464589235127,"Advances in neural information processing systems, 33:17283‚Äì17297, 2020.
428"
REFERENCES,0.52219074598678,"[39] Markus N Rabe and Charles Staats.
Self-attention does not need o (n2) memory.
arXiv preprint
429"
REFERENCES,0.5231350330500472,"arXiv:2112.05682, 2021.
430"
REFERENCES,0.5240793201133145,"[40] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference,
431"
REFERENCES,0.5250236071765817,"2023.
432"
REFERENCES,0.5259678942398489,"[41] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong,
433"
REFERENCES,0.5269121813031161,"and Yu Wang.
Flashdecoding++: Faster large language model inference on gpus.
arXiv preprint
434"
REFERENCES,0.5278564683663833,"arXiv:2311.01282, 2023.
435"
REFERENCES,0.5288007554296507,"[42] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite
436"
REFERENCES,0.5297450424929179,"context. arXiv preprint arXiv:2310.01889, 2023.
437"
REFERENCES,0.5306893295561851,"[43] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint
438"
REFERENCES,0.5316336166194523,"arXiv:2001.04451, 2020.
439"
REFERENCES,0.5325779036827195,"[44] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear
440"
REFERENCES,0.5335221907459868,"complexity. arXiv preprint arXiv:2006.04768, 2020.
441"
REFERENCES,0.534466477809254,"[45] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
442"
REFERENCES,0.5354107648725213,"Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask
443"
REFERENCES,0.5363550519357885,"benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.
444"
REFERENCES,0.5372993389990557,"A
Appendix / supplemental material
445"
REFERENCES,0.5382436260623229,"A.1
Algorithm Details
446"
REFERENCES,0.5391879131255902,"The detail implementation of FlashMask Backward Pass is presented in Algorithm 2. We do
447"
REFERENCES,0.5401322001888574,"precomputations of max and min values of FMS and FME similar to the Forward Pass. Then the
448"
REFERENCES,0.5410764872521246,"FMS ùëóand FME ùëócan be loaded to SRAM outside the inner loop (line 14-15), reducing the HBM
449"
REFERENCES,0.5420207743153919,"accesses to 2 √ó ùëÅ. Then, we do inner loop on ùëÑùëñ(line 16), computing the two valid parts and
450"
REFERENCES,0.5429650613786591,"bypassing the masked part ùëñ‚àà(
j maxFMS ùëó ùêµùëü"
REFERENCES,0.5439093484419264,"k
,
j minFME ùëó ùêµùëü"
REFERENCES,0.5448536355051936,"k
).
451"
REFERENCES,0.5457979225684608,Algorithm 2 Optimized Backward Pass with FlashMask
REFERENCES,0.546742209631728,"Require: Matrices Q, K, V, O, dO ‚ààRùëÅ√óùëëin HBM, vector ùêø‚ààRùëÅin HBM, block sizes ùêµùëê, ùêµùëü, dense bias
mask ùê∑‚ààRùëÅ√óùëÅ, column-wise sparse mask starting rows FMS ‚ààRùëÅ, ending rows FME ‚ààRùëÅ.
1: Divide Q into ùëáùëü=
l
ùëÅ
ùêµùëü"
REFERENCES,0.5476864966949953,"m
blocks Q1, . . . , Qùëáùëüof size ùêµùëü√ó ùëëeach, and divide K, V in to ùëáùëê=
l
ùëÅ
ùêµùëê"
REFERENCES,0.5486307837582625,"m
blocks
K1, . . . , Kùëáùëêand V1, . . . , Vùëáùëê, of size ùêµùëê√ó ùëëeach.
2: Divide O into ùëáùëüblocks Oùëñ, . . . , Oùëáùëüof size ùêµùëü√ó ùëëeach, divide dO into ùëáùëüblocks dOùëñ, . . . , dOùëáùëüof size
ùêµùëü√ó ùëëeach, and divide ùêøinto ùëáùëüblocks ùêøùëñ, . . . , ùêøùëáùëüof size ùêµùëüeach.
3: Initialize dQ = (0)ùëÅ√óùëëin HBM and divide it into ùëáùëüblocks dQ1, . . . , dQùëáùëüof size ùêµùëü√ó ùëëeach. Divide
dK, dV ‚ààRùëÅ√óùëëin to ùëáùëêblocks dK1, . . . , dKùëáùëêand dV1, . . . , dVùëáùëê, of size ùêµùëê√ó ùëëeach.
4: Divide the dense mask D into ùëáùëü√ó ùëáùëêblocks D1,1, ..., Dùëáùëü,ùëáùëê
5: Divide FMS into ùëáùëêblocks FMS1, ..., FMSùëáùëê, and divide FME into FME1, ..., FMEùëáùëê.
6: Precompute the max value maxFMS1, ..., maxFMSùëáùëêfor each FMS1, ..., FMSùëáùëê, write to HBM.
7: Precompute the max value maxFME1, ..., maxFMEùëáùëêfor each FME1, ..., FMEùëáùëê, write to HBM.
8: Precompute the min value minFMS1, ..., minFMSùëáùëêfor each FMS1, ..., FMSùëáùëê, write to HBM.
9: Precompute the min value minFME1, ..., minFMEùëáùëêfor each FME1, ..., FMEùëáùëê, write to HBM.
10: Compute ùê∑= rowsum(dO ‚ó¶O) ‚ààRùëë(pointwise multiply), write ùê∑to HBM and divide it into ùëáùëüblocks
ùê∑1, . . . , ùê∑ùëáùëüof size ùêµùëüeach.
11: for 1 ‚â§ùëó‚â§ùëáùëêdo
12:
Load K ùëó, V ùëófrom HBM to on-chip SRAM.
13:
Initialize dKùëó= (0)ùêµùëê√óùëë, dV ùëó= (0)ùêµùëê√óùëëon SRAM.
14:
Load FMS ùëófrom HBM to on-chip SRAM.
15:
Load FMEùëófrom HBM to on-chip SRAM."
REFERENCES,0.5495750708215298,"16:
for 1 ‚â§ùëñ‚â§
j maxFMS ùëó ùêµùëü"
REFERENCES,0.550519357884797,"k
ùëéùëõùëë
j minFME ùëó ùêµùëü"
REFERENCES,0.5514636449480642,"k
‚â§ùëñ‚â§ùëáùëüdo
17:
Load Qùëñ, Oùëñ, dOùëñ, dQùëñ, ùêøùëñ, ùê∑ùëñfrom HBM to on-chip SRAM.
18:
On chip, compute S( ùëó)
ùëñ
= QùëñKùëá
ùëó‚ààRùêµùëü√óùêµùëê."
REFERENCES,0.5524079320113314,"19:
On chip, set S( ùëó)
ùëñ
= S( ùëó)
ùëñ
+ ùê∑ùëñ, ùëó"
REFERENCES,0.5533522190745986,"20:
if
j maxFME ùëó ùêµùëü"
REFERENCES,0.554296506137866,"k
‚â§ùëñ‚â§
j minFMSùëó ùêµùëü"
REFERENCES,0.5552407932011332,"k
then"
REFERENCES,0.5561850802644004,"21:
On chip, set S( ùëó)
ùëñ
[ùë•][ùë¶] = ‚àí‚àû, for every ùëñ‚àóùêµùëü+ ùë•‚â•ùëÄùëó[ùë¶].
22:
end if
23:
On chip, compute P( ùëó)
ùëñ
= exp(Sùëñùëó‚àíùêøùëñ) ‚ààRùêµùëü√óùêµùëê."
REFERENCES,0.5571293673276676,"24:
On chip, compute dV ùëó‚ÜêdV ùëó+ (P( ùëó)
ùëñ
)‚ä§dOùëñ‚ààRùêµùëê√óùëë."
REFERENCES,0.5580736543909348,"25:
On chip, compute dP( ùëó)
ùëñ
= dOùëñV‚ä§
ùëó‚ààRùêµùëü√óùêµùëê."
REFERENCES,0.559017941454202,"26:
On chip, compute dS( ùëó)
ùëñ
= P( ùëó)
ùëñ
‚ó¶(dP( ùëó)
ùëñ
‚àíùê∑ùëñ) ‚ààRùêµùëü√óùêµùëê."
REFERENCES,0.5599622285174694,"27:
Load dQùëñfrom HBM to SRAM, then on chip, update dQùëñ‚ÜêdQùëñ+ dS( ùëó)
ùëñ
Kùëó‚ààRùêµùëü√óùëë, and write
back to HBM.
28:
On chip, compute dKùëó‚ÜêdKùëó+ dS( ùëó)
ùëñ
‚ä§Qùëñ‚ààRùêµùëê√óùëë.
29:
end for
30:
Write dKùëó, dVùëóto HBM.
31: end for
32: Return dQ, dK, dV."
REFERENCES,0.5609065155807366,"A.2
Supplementary Experimental Details
452"
REFERENCES,0.5618508026440038,"All end-to-end training and testing in this paper were conducted on 4 servers, each equipped with 32
453"
REFERENCES,0.562795089707271,"NVIDIA A800-SXM 80G GPUs. We comprehensively evaluated the performance of the LLaMA2
454"
REFERENCES,0.5637393767705382,"model across three different parameter scales, four downstream task scenarios, and various sequence
455"
REFERENCES,0.5646836638338055,"lengths. Given the diversity of experimental combinations and the specific distributed parallel
456"
REFERENCES,0.5656279508970727,"strategies required by models, in varying parameter scales, the primary goal of the experiments is
457"
REFERENCES,0.56657223796034,"not to achieve optimal end-to-end training performance but to demonstrate the effectiveness of the
458"
REFERENCES,0.5675165250236072,"FlashMask method. Therefore, to ensure consistency, we set the following hyperparameters in Table 1
459"
REFERENCES,0.5684608120868744,"with the same hardware configuration.
460"
REFERENCES,0.5694050991501416,Table 1: Training Hyperparameters for Various Scales of LLaMA2 Models.
REFERENCES,0.5703493862134089,"Model
LLaMA2-7B
LLaMA2-13B
LLaMA2-70B"
REFERENCES,0.5712936732766761,"Global Batch Size
16
16
16
Gradient Accumulation Step
2
4
16"
REFERENCES,0.5722379603399433,"Sharding Stage1 Degree
8
4
1
Tensor Parallel Degree
4
4
8
PipeLine Parallel Degree
1
2
4
Sequence Parallel Degree
‚úì
‚úì
‚úì"
REFERENCES,0.5731822474032106,"To verify the representativeness of our synthetic dataset, sparsity distribution histograms of synthetic
461"
REFERENCES,0.5741265344664778,"dataset are presented in Figure 6. Then we use InToken method with max sequence length of 16K,
462"
REFERENCES,0.5750708215297451,"32K, 64K, and 128K on the open-source dataset LongBench, and compute the distribution histograms,
463"
REFERENCES,0.5760151085930123,"presented in Figure 7. Note that many long sentences are truncated for max sequence length 16K,
464"
REFERENCES,0.5769593956562795,"and 32K. Results indicate that the sparsity distributions of LongBench dataset and synthetic dataset
465"
REFERENCES,0.5779036827195467,"are similar.
466"
REFERENCES,0.5788479697828139,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 Count"
REFERENCES,0.5797922568460812,SFT 2K
REFERENCES,0.5807365439093485,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 Count"
REFERENCES,0.5816808309726157,SFT 4K
REFERENCES,0.5826251180358829,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 Count"
REFERENCES,0.5835694050991501,SFT 8K
REFERENCES,0.5845136921624173,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count"
REFERENCES,0.5854579792256847,SFT 16K
REFERENCES,0.5864022662889519,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 Count"
REFERENCES,0.5873465533522191,SFT 32K
REFERENCES,0.5882908404154863,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 Count"
REFERENCES,0.5892351274787535,SFT 64K
REFERENCES,0.5901794145420207,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 Count"
REFERENCES,0.591123701605288,SFT 128K
REFERENCES,0.5920679886685553,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 Count"
REFERENCES,0.5930122757318225,DPO 2K
REFERENCES,0.5939565627950897,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 Count"
REFERENCES,0.5949008498583569,DPO 4K
REFERENCES,0.5958451369216242,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 80 Count"
REFERENCES,0.5967894239848914,DPO 8K
REFERENCES,0.5977337110481586,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count"
REFERENCES,0.5986779981114259,DPO 16K
REFERENCES,0.5996222851746931,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 80 Count"
REFERENCES,0.6005665722379604,DPO 32K
REFERENCES,0.6015108593012276,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 70 Count"
REFERENCES,0.6024551463644948,DPO 64K
REFERENCES,0.603399433427762,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count"
REFERENCES,0.6043437204910292,DPO 128K
REFERENCES,0.6052880075542965,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count RM 2K"
REFERENCES,0.6062322946175638,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count RM 4K"
REFERENCES,0.607176581680831,"0
20
40
60
80
100
Sparsity(%) 0 10 20 30 40 50 60 Count RM 8K"
REFERENCES,0.6081208687440982,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 Count"
REFERENCES,0.6090651558073654,RM 16K
REFERENCES,0.6100094428706326,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 100 Count"
REFERENCES,0.6109537299339,RM 32K
REFERENCES,0.6118980169971672,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 Count"
REFERENCES,0.6128423040604344,RM 64K
REFERENCES,0.6137865911237016,"0
20
40
60
80
100
Sparsity(%) 0 20 40 60 80 Count"
REFERENCES,0.6147308781869688,RM 128K
REFERENCES,0.615675165250236,Figure 6: Sparsity Distribution of Synthetic Dataset.
REFERENCES,0.6166194523135033,"0
20
40
60
80
100
Sparsity(%) 0 200 400 600 800 1000 1200 1400 1600 Count"
REFERENCES,0.6175637393767706,SFT 16K
REFERENCES,0.6185080264400378,"0
20
40
60
80
100
Sparsity(%) 0 100 200 300 400 500 600 Count"
REFERENCES,0.619452313503305,SFT 32K
REFERENCES,0.6203966005665722,"0
20
40
60
80
100
Sparsity(%) 0 50 100 150 200 250 300 350 400 Count"
REFERENCES,0.6213408876298395,SFT 64K
REFERENCES,0.6222851746931067,"0
20
40
60
80
100
Sparsity(%) 0 50 100 150 200 250 300 350 Count"
REFERENCES,0.623229461756374,SFT 128K
REFERENCES,0.6241737488196412,Figure 7: Sparsity Distribution of LongBench Dataset.
REFERENCES,0.6251180358829084,"A.3
Full Experiment Results
467"
REFERENCES,0.6260623229461756,"Kernel experiments are also conducted on batch sizes 4, and 8. FA-Varlen is excluded by default.
468"
REFERENCES,0.6270066100094429,"Results are presented in Figure 8 and 9. The trends are identical to Figure 3 in Section 4.3, except
469"
REFERENCES,0.6279508970727101,"memory exhaustion occurred with less sequence length, especially for FA-DenseMask and Vanilla
470"
REFERENCES,0.6288951841359773,"Attention which require ùëÇ(ùëÅ2) memory to launch.
471"
REFERENCES,0.6298394711992445,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103 104"
REFERENCES,0.6307837582625118,Latency(ms) SFT
REFERENCES,0.6317280453257791,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.6326723323890463,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103 104"
REFERENCES,0.6336166194523135,Latency(ms) DPO
REFERENCES,0.6345609065155807,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.6355051935788479,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103 104"
REFERENCES,0.6364494806421152,Latency(ms) RM
REFERENCES,0.6373937677053825,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.6383380547686497,Figure 8: Kernel Latency Comparison with Varying the Length of Sequence.(Batch Size = 4)
REFERENCES,0.6392823418319169,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103"
REFERENCES,0.6402266288951841,Latency(ms) SFT
REFERENCES,0.6411709159584513,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.6421152030217187,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103"
REFERENCES,0.6430594900849859,Latency(ms) DPO
REFERENCES,0.6440037771482531,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.6449480642115203,"2
4
8
16
32
64
128
Sequence Length(K) 101 102 103"
REFERENCES,0.6458923512747875,Latency(ms) RM
REFERENCES,0.6468366383380547,"FA-DenseMask
VanillaAttention
FlashMask"
REFERENCES,0.647780925401322,Figure 9: Kernel Latency Comparison with Varying the Length of Sequence. (Batch Size = 8)
REFERENCES,0.6487252124645893,"We evaluate the effectiveness of FlashMask on the open-source dataset LongBench. The throughput of
472"
REFERENCES,0.6496694995278565,"LoRA fine-tuning for LLaMA2-7B are shown in Figure 10. FlashMask performed close to FA-Varlen,
473"
REFERENCES,0.6506137865911237,"showcasing 4.12x faster than FA-DenseMask, proving that FlashMask can deliver significant training
474"
REFERENCES,0.6515580736543909,"accelerations in generalized real-world scenarios.
475"
REFERENCES,0.6525023607176582,"16
32
64
Sequence Length(K) 2000 3000 4000 5000 6000 7000"
REFERENCES,0.6534466477809254,Tokens/Sec/GPU
REFERENCES,0.6543909348441926,Speed Up 4.12x
REFERENCES,0.6553352219074599,LLaMA-7B
REFERENCES,0.6562795089707271,"FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6572237960339944,Figure 10: Comparison of End-to-End Training Throughput on LongBench Dataset.
REFERENCES,0.6581680830972616,"Figure 11 presents the GPU memory consumption in End-to-End training. FlashMask showed linear
476"
REFERENCES,0.6591123701605288,"memory consumption with increasing sequence length, far less than FA-DenseMask. Therefore,
477"
REFERENCES,0.660056657223796,"FlashMask supports training with much longer sequences in memory limits of 80G.
478"
REFERENCES,0.6610009442870632,"4
8
16
32
64
128
256
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6619452313503305,GPU Memory(GB)
REFERENCES,0.6628895184135978,LLaMA-7B
REFERENCES,0.663833805476865,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6647780925401322,"4
8
16
32
64
128
196
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6657223796033994,GPU Memory(GB)
REFERENCES,0.6666666666666666,LLaMA-13B
REFERENCES,0.667610953729934,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6685552407932012,"4
8
16
32
64
96
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6694995278564684,GPU Memory(GB)
REFERENCES,0.6704438149197356,LLaMA-70B
REFERENCES,0.6713881019830028,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.67233238904627,(a) SFT
REFERENCES,0.6732766761095373,"4
8
16
32
64
128
256
512
544
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6742209631728046,GPU Memory(GB)
REFERENCES,0.6751652502360718,LLaMA-7B
REFERENCES,0.676109537299339,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6770538243626062,"4
8
16
32
64
128
224
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6779981114258735,GPU Memory(GB)
REFERENCES,0.6789423984891407,LLaMA-13B
REFERENCES,0.6798866855524079,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6808309726156752,"4
8
16
32
64
128
Sequence Length(K) 0 10 20 30 40 50 60"
REFERENCES,0.6817752596789424,GPU Memory(GB)
REFERENCES,0.6827195467422096,LLaMA-70B
REFERENCES,0.6836638338054769,"VanillaAttention
FA-DenseMask
FA-Varlen
FlashMask"
REFERENCES,0.6846081208687441,(b) LoRA
REFERENCES,0.6855524079320113,"4
8
16
32
64
96
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6864966949952785,GPU Memory(GB)
REFERENCES,0.6874409820585458,LLaMA-7B
REFERENCES,0.6883852691218131,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.6893295561850803,"4
8
16
32
64
128
180
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6902738432483475,GPU Memory(GB)
REFERENCES,0.6912181303116147,LLaMA-13B
REFERENCES,0.6921624173748819,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.6931067044381491,"4
8
16
32
64
80
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6940509915014165,GPU Memory(GB)
REFERENCES,0.6949952785646837,LLaMA-70B
REFERENCES,0.6959395656279509,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.6968838526912181,(c) DPO
REFERENCES,0.6978281397544853,"4
8
16
32
64
128
256
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.6987724268177526,GPU Memory(GB)
REFERENCES,0.6997167138810199,LLaMA-7B
REFERENCES,0.7006610009442871,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.7016052880075543,"4
8
16
32
64
128
196
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.7025495750708215,GPU Memory(GB)
REFERENCES,0.7034938621340887,LLaMA-13B
REFERENCES,0.704438149197356,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.7053824362606232,"4
8
16
32
64
96
Sequence Length(K) 0 10 20 30 40 50 60 70"
REFERENCES,0.7063267233238905,GPU Memory(GB)
REFERENCES,0.7072710103871577,LLaMA-70B
REFERENCES,0.7082152974504249,"VanillaAttention
FA-DenseMask
FlashMask"
REFERENCES,0.7091595845136922,"(d) RM
Figure 11: Comparison of End-to-End Training GPU Memory."
REFERENCES,0.7101038715769594,"NeurIPS Paper Checklist
479"
CLAIMS,0.7110481586402266,"1. Claims
480"
CLAIMS,0.7119924457034938,"Question: Do the main claims made in the abstract and introduction accurately reflect the
481"
CLAIMS,0.7129367327667611,"paper‚Äôs contributions and scope?
482"
CLAIMS,0.7138810198300283,"Answer: [Yes]
483"
CLAIMS,0.7148253068932956,"Justification: This paper‚Äôs contributions and scope are described in Abstract and Introduction.
484"
CLAIMS,0.7157695939565628,"Guidelines:
485"
CLAIMS,0.71671388101983,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
486"
CLAIMS,0.7176581680830972,"made in the paper.
487"
CLAIMS,0.7186024551463644,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
488"
CLAIMS,0.7195467422096318,"contributions made in the paper and important assumptions and limitations. A No or
489"
CLAIMS,0.720491029272899,"NA answer to this question will not be perceived well by the reviewers.
490"
CLAIMS,0.7214353163361662,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
491"
CLAIMS,0.7223796033994334,"much the results can be expected to generalize to other settings.
492"
CLAIMS,0.7233238904627006,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
493"
CLAIMS,0.724268177525968,"are not attained by the paper.
494"
LIMITATIONS,0.7252124645892352,"2. Limitations
495"
LIMITATIONS,0.7261567516525024,"Question: Does the paper discuss the limitations of the work performed by the authors?
496"
LIMITATIONS,0.7271010387157696,"Answer: [Yes]
497"
LIMITATIONS,0.7280453257790368,"Justification: The paper includes a discussion section about limitations.
498"
LIMITATIONS,0.728989612842304,"Guidelines:
499"
LIMITATIONS,0.7299338999055713,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
500"
LIMITATIONS,0.7308781869688386,"the paper has limitations, but those are not discussed in the paper.
501"
LIMITATIONS,0.7318224740321058,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
502"
LIMITATIONS,0.732766761095373,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
503"
LIMITATIONS,0.7337110481586402,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
504"
LIMITATIONS,0.7346553352219075,"model well-specification, asymptotic approximations only holding locally). The authors
505"
LIMITATIONS,0.7355996222851747,"should reflect on how these assumptions might be violated in practice and what the
506"
LIMITATIONS,0.7365439093484419,"implications would be.
507"
LIMITATIONS,0.7374881964117092,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
508"
LIMITATIONS,0.7384324834749764,"only tested on a few datasets or with a few runs. In general, empirical results often
509"
LIMITATIONS,0.7393767705382436,"depend on implicit assumptions, which should be articulated.
510"
LIMITATIONS,0.7403210576015109,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
511"
LIMITATIONS,0.7412653446647781,"For example, a facial recognition algorithm may perform poorly when image resolution
512"
LIMITATIONS,0.7422096317280453,"is low or images are taken in low lighting. Or a speech-to-text system might not be
513"
LIMITATIONS,0.7431539187913125,"used reliably to provide closed captions for online lectures because it fails to handle
514"
LIMITATIONS,0.7440982058545798,"technical jargon.
515"
LIMITATIONS,0.7450424929178471,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
516"
LIMITATIONS,0.7459867799811143,"and how they scale with dataset size.
517"
LIMITATIONS,0.7469310670443815,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
518"
LIMITATIONS,0.7478753541076487,"address problems of privacy and fairness.
519"
LIMITATIONS,0.7488196411709159,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
520"
LIMITATIONS,0.7497639282341831,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
521"
LIMITATIONS,0.7507082152974505,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
522"
LIMITATIONS,0.7516525023607177,"judgment and recognize that individual actions in favor of transparency play an impor-
523"
LIMITATIONS,0.7525967894239849,"tant role in developing norms that preserve the integrity of the community. Reviewers
524"
LIMITATIONS,0.7535410764872521,"will be specifically instructed to not penalize honesty concerning limitations.
525"
THEORY ASSUMPTIONS AND PROOFS,0.7544853635505193,"3. Theory Assumptions and Proofs
526"
THEORY ASSUMPTIONS AND PROOFS,0.7554296506137866,"Question: For each theoretical result, does the paper provide the full set of assumptions and
527"
THEORY ASSUMPTIONS AND PROOFS,0.7563739376770539,"a complete (and correct) proof?
528"
THEORY ASSUMPTIONS AND PROOFS,0.7573182247403211,"Answer: [Yes]
529"
THEORY ASSUMPTIONS AND PROOFS,0.7582625118035883,"Justification: In sec 3.3 Complexity Analysis
530"
THEORY ASSUMPTIONS AND PROOFS,0.7592067988668555,"Guidelines:
531"
THEORY ASSUMPTIONS AND PROOFS,0.7601510859301227,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
532"
THEORY ASSUMPTIONS AND PROOFS,0.76109537299339,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
533"
THEORY ASSUMPTIONS AND PROOFS,0.7620396600566572,"referenced.
534"
THEORY ASSUMPTIONS AND PROOFS,0.7629839471199245,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
535"
THEORY ASSUMPTIONS AND PROOFS,0.7639282341831917,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
536"
THEORY ASSUMPTIONS AND PROOFS,0.7648725212464589,"they appear in the supplemental material, the authors are encouraged to provide a short
537"
THEORY ASSUMPTIONS AND PROOFS,0.7658168083097262,"proof sketch to provide intuition.
538"
THEORY ASSUMPTIONS AND PROOFS,0.7667610953729934,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
539"
THEORY ASSUMPTIONS AND PROOFS,0.7677053824362606,"by formal proofs provided in appendix or supplemental material.
540"
THEORY ASSUMPTIONS AND PROOFS,0.7686496694995278,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695939565627951,"4. Experimental Result Reproducibility
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705382436260623,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7714825306893296,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7724268177525968,"of the paper (regardless of whether the code and data are provided or not)?
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.773371104815864,"Answer: [Yes]
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7743153918791312,"Justification: The source code will be public available and can reproduce the results accord-
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752596789423984,"ing README.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762039660056658,"Guidelines:
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.777148253068933,"‚Ä¢ The answer NA means that the paper does not include experiments.
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7780925401322002,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7790368271954674,"well by the reviewers: Making the paper reproducible is important, regardless of
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7799811142587346,"whether the code and data are provided or not.
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7809254013220018,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7818696883852692,"to make their results reproducible or verifiable.
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7828139754485364,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7837582625118036,"For example, if the contribution is a novel architecture, describing the architecture fully
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847025495750708,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.785646836638338,"be necessary to either make it possible for others to replicate the model with the same
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7865911237016053,"dataset, or provide access to the model. In general. releasing code and data is often
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7875354107648725,"one good way to accomplish this, but reproducibility can also be provided via detailed
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7884796978281398,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.789423984891407,"of a large language model), releasing of a model checkpoint, or other means that are
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7903682719546742,"appropriate to the research performed.
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7913125590179415,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7922568460812087,"sions to provide some reasonable avenue for reproducibility, which may depend on the
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932011331444759,"nature of the contribution. For example
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941454202077431,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7950897072710104,"to reproduce that algorithm.
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7960339943342776,"(b) If the contribution is primarily a new model architecture, the paper should describe
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7969782813975449,"the architecture clearly and fully.
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7979225684608121,"(c) If the contribution is a new model (e.g., a large language model), then there should
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7988668555240793,"either be a way to access this model for reproducing the results or a way to reproduce
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998111425873465,"the model (e.g., with an open-source dataset or instructions for how to construct
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8007554296506137,"the dataset).
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8016997167138811,"(d) We recognize that reproducibility may be tricky in some cases, in which case
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8026440037771483,"authors are welcome to describe the particular way they provide for reproducibility.
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8035882908404155,"In the case of closed-source models, it may be that access to the model is limited in
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8045325779036827,"some way (e.g., to registered users), but it should be possible for other researchers
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8054768649669499,"to have some path to reproducing or verifying the results.
580"
OPEN ACCESS TO DATA AND CODE,0.8064211520302171,"5. Open access to data and code
581"
OPEN ACCESS TO DATA AND CODE,0.8073654390934845,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
582"
OPEN ACCESS TO DATA AND CODE,0.8083097261567517,"tions to faithfully reproduce the main experimental results, as described in supplemental
583"
OPEN ACCESS TO DATA AND CODE,0.8092540132200189,"material?
584"
OPEN ACCESS TO DATA AND CODE,0.8101983002832861,"Answer: [Yes]
585"
OPEN ACCESS TO DATA AND CODE,0.8111425873465533,"Justification: The source code will be public available.
586"
OPEN ACCESS TO DATA AND CODE,0.8120868744098206,"Guidelines:
587"
OPEN ACCESS TO DATA AND CODE,0.8130311614730878,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
588"
OPEN ACCESS TO DATA AND CODE,0.8139754485363551,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
589"
OPEN ACCESS TO DATA AND CODE,0.8149197355996223,"public/guides/CodeSubmissionPolicy) for more details.
590"
OPEN ACCESS TO DATA AND CODE,0.8158640226628895,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
591"
OPEN ACCESS TO DATA AND CODE,0.8168083097261567,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
592"
OPEN ACCESS TO DATA AND CODE,0.817752596789424,"including code, unless this is central to the contribution (e.g., for a new open-source
593"
OPEN ACCESS TO DATA AND CODE,0.8186968838526912,"benchmark).
594"
OPEN ACCESS TO DATA AND CODE,0.8196411709159585,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
595"
OPEN ACCESS TO DATA AND CODE,0.8205854579792257,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
596"
OPEN ACCESS TO DATA AND CODE,0.8215297450424929,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
597"
OPEN ACCESS TO DATA AND CODE,0.8224740321057602,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
598"
OPEN ACCESS TO DATA AND CODE,0.8234183191690274,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
599"
OPEN ACCESS TO DATA AND CODE,0.8243626062322946,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
600"
OPEN ACCESS TO DATA AND CODE,0.8253068932955618,"proposed method and baselines. If only a subset of experiments are reproducible, they
601"
OPEN ACCESS TO DATA AND CODE,0.826251180358829,"should state which ones are omitted from the script and why.
602"
OPEN ACCESS TO DATA AND CODE,0.8271954674220963,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
603"
OPEN ACCESS TO DATA AND CODE,0.8281397544853636,"versions (if applicable).
604"
OPEN ACCESS TO DATA AND CODE,0.8290840415486308,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
605"
OPEN ACCESS TO DATA AND CODE,0.830028328611898,"paper) is recommended, but including URLs to data and code is permitted.
606"
OPEN ACCESS TO DATA AND CODE,0.8309726156751652,"6. Experimental Setting/Details
607"
OPEN ACCESS TO DATA AND CODE,0.8319169027384324,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
608"
OPEN ACCESS TO DATA AND CODE,0.8328611898016998,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
609"
OPEN ACCESS TO DATA AND CODE,0.833805476864967,"results?
610"
OPEN ACCESS TO DATA AND CODE,0.8347497639282342,"Answer: [Yes]
611"
OPEN ACCESS TO DATA AND CODE,0.8356940509915014,"Justification: Refer to Experiments section.
612"
OPEN ACCESS TO DATA AND CODE,0.8366383380547686,"Guidelines:
613"
OPEN ACCESS TO DATA AND CODE,0.8375826251180358,"‚Ä¢ The answer NA means that the paper does not include experiments.
614"
OPEN ACCESS TO DATA AND CODE,0.8385269121813032,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
615"
OPEN ACCESS TO DATA AND CODE,0.8394711992445704,"that is necessary to appreciate the results and make sense of them.
616"
OPEN ACCESS TO DATA AND CODE,0.8404154863078376,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
617"
OPEN ACCESS TO DATA AND CODE,0.8413597733711048,"material.
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.842304060434372,"7. Experiment Statistical Significance
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8432483474976393,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441926345609065,"information about the statistical significance of the experiments?
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451369216241738,"Answer: [Yes]
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.846081208687441,"Justification: All our experimental results are run multiple times and then averaged.
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8470254957507082,"Guidelines:
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8479697828139755,"‚Ä¢ The answer NA means that the paper does not include experiments.
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8489140698772427,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498583569405099,"dence intervals, or statistical significance tests, at least for the experiments that support
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8508026440037771,"the main claims of the paper.
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8517469310670444,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8526912181303116,"example, train/test split, initialization, random drawing of some parameter, or overall
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8536355051935789,"run with given experimental conditions).
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8545797922568461,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8555240793201133,"call to a library function, bootstrap, etc.)
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8564683663833805,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8574126534466477,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8583569405099151,"of the mean.
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8593012275731823,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8602455146364495,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8611898016997167,"of Normality of errors is not verified.
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621340887629839,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8630783758262511,"figures symmetric error bars that would yield results that are out of range (e.g. negative
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8640226628895185,"error rates).
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8649669499527857,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8659112370160529,"they were calculated and reference the corresponding figures or tables in the text.
644"
EXPERIMENTS COMPUTE RESOURCES,0.8668555240793201,"8. Experiments Compute Resources
645"
EXPERIMENTS COMPUTE RESOURCES,0.8677998111425873,"Question: For each experiment, does the paper provide sufficient information on the com-
646"
EXPERIMENTS COMPUTE RESOURCES,0.8687440982058546,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
647"
EXPERIMENTS COMPUTE RESOURCES,0.8696883852691218,"the experiments?
648"
EXPERIMENTS COMPUTE RESOURCES,0.8706326723323891,"Answer: [Yes]
649"
EXPERIMENTS COMPUTE RESOURCES,0.8715769593956563,"Justification: Referring to the Experiments section, we provide a running environment.
650"
EXPERIMENTS COMPUTE RESOURCES,0.8725212464589235,"Guidelines:
651"
EXPERIMENTS COMPUTE RESOURCES,0.8734655335221907,"‚Ä¢ The answer NA means that the paper does not include experiments.
652"
EXPERIMENTS COMPUTE RESOURCES,0.874409820585458,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
653"
EXPERIMENTS COMPUTE RESOURCES,0.8753541076487252,"or cloud provider, including relevant memory and storage.
654"
EXPERIMENTS COMPUTE RESOURCES,0.8762983947119924,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
655"
EXPERIMENTS COMPUTE RESOURCES,0.8772426817752597,"experimental runs as well as estimate the total compute.
656"
EXPERIMENTS COMPUTE RESOURCES,0.8781869688385269,"‚Ä¢ The paper should disclose whether the full research project required more compute
657"
EXPERIMENTS COMPUTE RESOURCES,0.8791312559017942,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
658"
EXPERIMENTS COMPUTE RESOURCES,0.8800755429650614,"didn‚Äôt make it into the paper).
659"
CODE OF ETHICS,0.8810198300283286,"9. Code Of Ethics
660"
CODE OF ETHICS,0.8819641170915958,"Question: Does the research conducted in the paper conform, in every respect, with the
661"
CODE OF ETHICS,0.882908404154863,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
662"
CODE OF ETHICS,0.8838526912181303,"Answer: [Yes]
663"
CODE OF ETHICS,0.8847969782813976,"Justification: We follow the NeurIPS Code of Ethics properly.
664"
CODE OF ETHICS,0.8857412653446648,"Guidelines:
665"
CODE OF ETHICS,0.886685552407932,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
666"
CODE OF ETHICS,0.8876298394711992,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
667"
CODE OF ETHICS,0.8885741265344664,"deviation from the Code of Ethics.
668"
CODE OF ETHICS,0.8895184135977338,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
669"
CODE OF ETHICS,0.890462700661001,"eration due to laws or regulations in their jurisdiction).
670"
BROADER IMPACTS,0.8914069877242682,"10. Broader Impacts
671"
BROADER IMPACTS,0.8923512747875354,"Question: Does the paper discuss both potential positive societal impacts and negative
672"
BROADER IMPACTS,0.8932955618508026,"societal impacts of the work performed?
673"
BROADER IMPACTS,0.8942398489140698,"Answer: [NA]
674"
BROADER IMPACTS,0.8951841359773371,"Justification: There is no societal impact of the work performed.
675"
BROADER IMPACTS,0.8961284230406044,"Guidelines:
676"
BROADER IMPACTS,0.8970727101038716,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
677"
BROADER IMPACTS,0.8980169971671388,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
678"
BROADER IMPACTS,0.898961284230406,"impact or why the paper does not address societal impact.
679"
BROADER IMPACTS,0.8999055712936733,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
680"
BROADER IMPACTS,0.9008498583569405,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
681"
BROADER IMPACTS,0.9017941454202077,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
682"
BROADER IMPACTS,0.902738432483475,"groups), privacy considerations, and security considerations.
683"
BROADER IMPACTS,0.9036827195467422,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
684"
BROADER IMPACTS,0.9046270066100094,"to particular applications, let alone deployments. However, if there is a direct path to
685"
BROADER IMPACTS,0.9055712936732767,"any negative applications, the authors should point it out. For example, it is legitimate
686"
BROADER IMPACTS,0.9065155807365439,"to point out that an improvement in the quality of generative models could be used to
687"
BROADER IMPACTS,0.9074598677998111,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
688"
BROADER IMPACTS,0.9084041548630784,"that a generic algorithm for optimizing neural networks could enable people to train
689"
BROADER IMPACTS,0.9093484419263456,"models that generate Deepfakes faster.
690"
BROADER IMPACTS,0.9102927289896129,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
691"
BROADER IMPACTS,0.9112370160528801,"being used as intended and functioning correctly, harms that could arise when the
692"
BROADER IMPACTS,0.9121813031161473,"technology is being used as intended but gives incorrect results, and harms following
693"
BROADER IMPACTS,0.9131255901794145,"from (intentional or unintentional) misuse of the technology.
694"
BROADER IMPACTS,0.9140698772426817,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
695"
BROADER IMPACTS,0.9150141643059491,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
696"
BROADER IMPACTS,0.9159584513692163,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
697"
BROADER IMPACTS,0.9169027384324835,"feedback over time, improving the efficiency and accessibility of ML).
698"
SAFEGUARDS,0.9178470254957507,"11. Safeguards
699"
SAFEGUARDS,0.9187913125590179,"Question: Does the paper describe safeguards that have been put in place for responsible
700"
SAFEGUARDS,0.9197355996222851,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
701"
SAFEGUARDS,0.9206798866855525,"image generators, or scraped datasets)?
702"
SAFEGUARDS,0.9216241737488197,"Answer: [NA]
703"
SAFEGUARDS,0.9225684608120869,"Justification: The paper poses no such risks.
704"
SAFEGUARDS,0.9235127478753541,"Guidelines:
705"
SAFEGUARDS,0.9244570349386213,"‚Ä¢ The answer NA means that the paper poses no such risks.
706"
SAFEGUARDS,0.9254013220018886,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
707"
SAFEGUARDS,0.9263456090651558,"necessary safeguards to allow for controlled use of the model, for example by requiring
708"
SAFEGUARDS,0.927289896128423,"that users adhere to usage guidelines or restrictions to access the model or implementing
709"
SAFEGUARDS,0.9282341831916903,"safety filters.
710"
SAFEGUARDS,0.9291784702549575,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
711"
SAFEGUARDS,0.9301227573182247,"should describe how they avoided releasing unsafe images.
712"
SAFEGUARDS,0.931067044381492,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
713"
SAFEGUARDS,0.9320113314447592,"not require this, but we encourage authors to take this into account and make a best
714"
SAFEGUARDS,0.9329556185080264,"faith effort.
715"
LICENSES FOR EXISTING ASSETS,0.9338999055712937,"12. Licenses for existing assets
716"
LICENSES FOR EXISTING ASSETS,0.9348441926345609,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
717"
LICENSES FOR EXISTING ASSETS,0.9357884796978282,"the paper, properly credited and are the license and terms of use explicitly mentioned and
718"
LICENSES FOR EXISTING ASSETS,0.9367327667610954,"properly respected?
719"
LICENSES FOR EXISTING ASSETS,0.9376770538243626,"Answer: [Yes]
720"
LICENSES FOR EXISTING ASSETS,0.9386213408876298,"Justification: CC BY-NC-ND 4.0
721"
LICENSES FOR EXISTING ASSETS,0.939565627950897,"Guidelines:
722"
LICENSES FOR EXISTING ASSETS,0.9405099150141643,"‚Ä¢ The answer NA means that the paper does not use existing assets.
723"
LICENSES FOR EXISTING ASSETS,0.9414542020774316,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
724"
LICENSES FOR EXISTING ASSETS,0.9423984891406988,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
725"
LICENSES FOR EXISTING ASSETS,0.943342776203966,"URL.
726"
LICENSES FOR EXISTING ASSETS,0.9442870632672332,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
727"
LICENSES FOR EXISTING ASSETS,0.9452313503305004,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
728"
LICENSES FOR EXISTING ASSETS,0.9461756373937678,"service of that source should be provided.
729"
LICENSES FOR EXISTING ASSETS,0.947119924457035,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
730"
LICENSES FOR EXISTING ASSETS,0.9480642115203022,"package should be provided. For popular datasets, paperswithcode.com/datasets
731"
LICENSES FOR EXISTING ASSETS,0.9490084985835694,"has curated licenses for some datasets. Their licensing guide can help determine the
732"
LICENSES FOR EXISTING ASSETS,0.9499527856468366,"license of a dataset.
733"
LICENSES FOR EXISTING ASSETS,0.9508970727101038,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
734"
LICENSES FOR EXISTING ASSETS,0.9518413597733711,"the derived asset (if it has changed) should be provided.
735"
LICENSES FOR EXISTING ASSETS,0.9527856468366384,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
736"
LICENSES FOR EXISTING ASSETS,0.9537299338999056,"the asset‚Äôs creators.
737"
NEW ASSETS,0.9546742209631728,"13. New Assets
738"
NEW ASSETS,0.95561850802644,"Question: Are new assets introduced in the paper well documented and is the documentation
739"
NEW ASSETS,0.9565627950897073,"provided alongside the assets?
740"
NEW ASSETS,0.9575070821529745,"Answer: [NA]
741"
NEW ASSETS,0.9584513692162417,"Justification: The paper does not release new assets.
742"
NEW ASSETS,0.959395656279509,"Guidelines:
743"
NEW ASSETS,0.9603399433427762,"‚Ä¢ The answer NA means that the paper does not release new assets.
744"
NEW ASSETS,0.9612842304060434,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
745"
NEW ASSETS,0.9622285174693107,"submissions via structured templates. This includes details about training, license,
746"
NEW ASSETS,0.9631728045325779,"limitations, etc.
747"
NEW ASSETS,0.9641170915958451,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
748"
NEW ASSETS,0.9650613786591123,"asset is used.
749"
NEW ASSETS,0.9660056657223796,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
750"
NEW ASSETS,0.9669499527856469,"create an anonymized URL or include an anonymized zip file.
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9678942398489141,"14. Crowdsourcing and Research with Human Subjects
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9688385269121813,"Question: For crowdsourcing experiments and research with human subjects, does the paper
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697828139754485,"include the full text of instructions given to participants and screenshots, if applicable, as
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707271010387157,"well as details about compensation (if any)?
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716713881019831,"Answer: [NA]
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726156751652503,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735599622285175,"Guidelines:
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9745042492917847,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9754485363550519,"human subjects.
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763928234183191,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773371104815864,"tion of the paper involves human subjects, then as much detail as possible should be
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782813975448537,"included in the main paper.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792256846081209,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801699716713881,"or other labor should be paid at least the minimum wage in the country of the data
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811142587346553,"collector.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820585457979226,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9830028328611898,"Subjects
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983947119924457,"Question: Does the paper describe potential risks incurred by study participants, whether
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848914069877243,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858356940509915,"approvals (or an equivalent approval/review based on the requirements of your country or
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867799811142587,"institution) were obtained?
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987724268177526,"Answer: [NA]
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886685552407932,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896128423040604,"Guidelines:
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905571293673276,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9915014164305949,"human subjects.
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9924457034938622,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933899905571294,"may be required for any human subjects research. If you obtained IRB approval, you
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9943342776203966,"should clearly state this in the paper.
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952785646836638,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996222851746931,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9971671388101983,"guidelines for their institution.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981114258734656,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990557129367328,"applicable), such as the institution conducting the review.
785"
