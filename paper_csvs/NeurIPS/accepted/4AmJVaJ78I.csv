Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013850415512465374,"Coordinate descent methods are popular in machine learning and optimization for
their simple sparse updates and excellent practical performance. In the context
of large-scale sequential game solving, these same properties would be attractive,
but until now no such methods were known, because the strategy spaces do not
satisfy the typical separable block structure exploited by such methods. We present
the first cyclic coordinate-descent-like method for the polytope of sequence-form
strategies, which form the strategy spaces for the players in an extensive-form
game (EFG). Our method exploits the recursive structure of the proximal update
induced by what are known as dilated regularizers, in order to allow for a pseudo
block-wise update. We show that our method enjoys a O(1/T) convergence rate to
a two-player zero-sum Nash equilibrium, while avoiding the worst-case polynomial
scaling with the number of blocks common to cyclic methods. We empirically show
that our algorithm usually performs better than other state-of-the-art first-order
methods (i.e., mirror prox), and occasionally can even beat CFR+, a state-of-
the-art algorithm for numerical equilibrium computation in zero-sum EFGs. We
then introduce a restarting heuristic for EFG solving. We show empirically that
restarting can lead to speedups, sometimes huge, both for our cyclic method, as
well as for existing methods such as mirror prox and predictive CFR+."
INTRODUCTION,0.002770083102493075,"1
Introduction"
INTRODUCTION,0.004155124653739612,"Extensive-form games (EFGs) are a broad class of game-theoretic models which are played on a
tree. They can compactly model both simultaneous and sequential moves, private and/or imperfect
information, and stochasticity. Equilibrium computation for a two-player zero-sum EFG can be
formulated as the following bilinear saddle-point problem (BSPP)"
INTRODUCTION,0.00554016620498615,"min
x∈X max
y∈Y ⟨Mx, y⟩.
(PD)"
INTRODUCTION,0.006925207756232687,"Here, the set of strategies X, Y for the x and y players are convex polytopes known as sequence-form
polytopes [44]. The (PD) formulation lends itself to first-order methods (FOMs) [14, 25], linear"
INTRODUCTION,0.008310249307479225,∗Authors are ordered alphabetically.
INTRODUCTION,0.009695290858725761,"programming [44], and online learning-based approaches [6, 8, 15, 17, 42, 49], since the feasible sets
are convex and compact polytopes, and the objective is bilinear."
INTRODUCTION,0.0110803324099723,"A common approach for solving BSPPs is by using first-order methods, where local gradient informa-
tion is used to iteratively improve the solution in order to converge to an equilibrium asymptotically.
In the game-solving context, such methods rely on two oracles: a first-order oracle that returns a
(sub)gradient at the current pair of strategies, and a pair of prox oracles for the strategy spaces X, Y,
which allow one to perform a generalized form of projected gradient descent steps on X, Y. These
prox oracles are usually constructed through the choice of an appropriate regularizer. For EFGs, it
is standard to focus on regularizers for which the prox oracle can be computed in linear time with
respect to the size of the polytope, which is only known to be achievable through what is known
as dilated regularizers [23]. Most first-order methods for EFGs require full-tree traversals for the
first-order oracle, and full traversals of the decision sets for the prox computation, before making
a strategy update for each player. For large EFGs these full traversals, especially for the first-order
oracle, can be very expensive, and it may be desirable to make strategy updates before a full traversal
has been performed, in order to more rapidly incorporate partial first-order information."
INTRODUCTION,0.012465373961218837,"In other settings, one commonly used approach for solving large-scale problems is through coordinate
methods (CMs) [34, 46]. These methods involve computing the gradient for a restricted set of
coordinates at each iteration of the algorithm, and using these partial gradients to construct descent
directions. The convergence rate of these methods typically is able to match the rate of full gradient
methods. However, in some cases they may exhibit worse runtime due to constants introduced by the
method. In spite of this, they often serve practical benefits of being more time and space efficient,
and enabling distributed computation [2, 3, 11, 19, 22, 28, 30, 32, 34, 47, 48]."
INTRODUCTION,0.013850415512465374,"Generally, coordinate descent methods assume that the problem is separable, i.e., there exists a
partition of the coordinates into blocks so that the feasible set can be decomposed as a Cartesian
product of feasible sets, one for each block. This assumption is crucial, as it allows the methods to
perform block-wise updates without worrying about feasibility, and it simplifies the convergence
analysis. Extending CDMs to EFGs is non-trivial because the constraints of the sequence-form
polytope do not possess this separable structure; instead the strategy space is such that the decision
at a given decision point affects all variables that occur after that decision. We are only aware of a
couple examples in the literature where separability is not assumed [1, 10], but those methods require
strong assumptions which are not applicable in EFG settings."
INTRODUCTION,0.015235457063711912,"Contributions. We propose the Extrapolated Cyclic Primal-Dual Algorithm (ECyclicPDA). Our
algorithm is the first cyclic coordinate method for the polytope of sequence-form strategies. It
achieves a O(1/T) convergence rate to a two-player zero-sum Nash equilibrium, with no dependence
on the number of blocks; this, is in contrast with the worst-case polynomial dependence on the
number of blocks that commonly appears in convergence rate guarantees for cyclic methods. Our
method crucially leverages the recursive structure of the prox updates induced by dilated regularizers.
In contrast to true cyclic (block) coordinate descent methods, the intermediate iterates generated
during one iteration of ECyclicPDA are not feasible because of the non-separable nature of the
constraints of sequence-form polytopes. Due to this infeasibility we refer to our updates as being
pseudo-block updates. The only information that is fully determined after one pseudo-block update, is
the behavioral strategy for all sequences at decision points in the block that was just considered. The
behavioral strategy is converted back to sequence-form at the end of a full iteration of our algorithm."
INTRODUCTION,0.01662049861495845,"At a very high level, our algorithm is inspired by the CODER algorithm due to Song and Diakonikolas
[39]. However, there are several important differences due to the specific structure of the bilinear
problem (PD) that we solve. First of all, the CODER algorithm is not directly applicable to our setting,
as the feasible set (treeplex) that appears in our problem formulation is not separable. Additionally,
CODER only considers Euclidean setups with quadratic regularizers, whereas our work considers
more general normed settings; in particular, the ℓ1 setup is of primary interest for our problem setup,
since it yields a much better dependence on the game size."
INTRODUCTION,0.018005540166204988,"These two issues regarding the non-separability of the feasible set and the more general normed
spaces and regularizers are handled in our work by (i) considering dilated regularizers, which allow
for blockwise (up to scaling) updates in a bottom-up fashion, respecting the treeplex ordering;
and (ii) introducing different extrapolation steps (see Lines 10 and 13 in Algorithm 1) that are
unique to our work and specific to the bilinear EFG problem formulation. Additionally, our special
problem structure and the choice of the extrapolation sequences exk and eyk allows us to remove"
INTRODUCTION,0.019390581717451522,"any nonstandard Lipschitz assumptions used in Song and Diakonikolas [39]. Notably, unlike Song
and Diakonikolas [39] and essentially all the work on cyclic methods we are aware of, which pay
polynomially for the number of blocks in the convergence bound, our convergence bound in the ℓ1
setting is never worse than the optimal bound of full vector-update methods such as mirror prox [33]
and dual extrapolation [35], which we consider a major contribution of our work."
INTRODUCTION,0.02077562326869806,"Numerically, we demonstrate that our algorithm performs better than mirror prox (MP), and can be
competitive with CFR+ and its variants on certain domains. We also propose the use of adaptive
restarting as a general heuristic tool for EFG solving: whenever an EFG solver constructs a solution
with duality gap at most a constant fraction of its initial value since the last restart, we restart it and
initialize the new run with the output solution at restart. Restarting is theoretically supported by the
fact that BSPPs possess the sharpness property [5, 18, 21, 43], and restarting combined with certain
Euclidean-based FOMs leads to a linear convergence rate under sharpness [5, 21]. We show that with
restarting, it is possible for our ECyclicPDA methods to outperform CFR+ on some games; this is
the first time that a FOM has been observed to outperform CFR+ on non-trivial EFGs. Somewhat
surprisingly, we then show that for some games, restarting can drastically speed up CFR+ as well. In
particular, we find that on one game, CFR+ with restarting exhibits a linear convergence rate, and so
does a recent predictive variant of CFR+ [15], on the same game and on an additional one."
INTRODUCTION,0.0221606648199446,"Related Work. CMs have been widely studied in the past decade and a half [1–3, 7, 10, 11, 19, 22,
28, 30, 32, 34, 37–39, 46, 48]. CMs can be grouped into three broad classes [38]: greedy methods,
which greedily select coordinates that will lead to the largest progress; randomized methods, which
select (blocks of) coordinates according to a probability distribution over the blocks; and cyclic
methods, which make updates in cyclic orders. Because greedy methods typically require full
gradient evaluation (to make the greedy selection), the focus in the literature has primarily been
on randomized (RCMs) and cyclic (CCMs) variants. RCMs require separability of the problem’s
constraints so we focus on CCMs. However, establishing convergence arguments for CCMs through
connections with convergence arguments for full gradient methods is difficult. Some guarantees
have been provided in the literature, either making restrictive assumptions [37] or by treating the
cyclical coordinate gradient as an approximation of a full gradient [7], and thus incurring a linear
dependence on the number of blocks in the convergence guarantee. Song and Diakonikolas [39]
were the first make an improvement on reducing the dependence on the number of blocks by using a
novel extrapolation strategy and introducing new block Lipschitz assumptions. That paper was the
main inspiration for our work, but inapplicable to our setting, thus necessitating new technical ideas,
as already discussed. While primal-dual coordinate methods for bilinear saddle-point problems have
been explored in Carmon et al. [9], their techniques are not clearly extendable to our problem. The
ℓ1 −ℓ1 setup they consider is the one which is relevant to the two-player zero-sum game setting we
study, but their assumption in that case is that the feasible set for each of the players is a probability
simplex, which is a much simpler feasible set than the treeplex considered in our work. It is unclear
how to generalize their result to our setting, as their results depend on the simplex structure."
INTRODUCTION,0.023545706371191136,"There has also been significant work on FOMs for two-player zero-sum EFG solving. Because this
is a BSPP, off-the-shelf FOMs for BSPPs can be applied, with the caveat that proximal oracles are
required. The most popular proximal oracles have been based on dilated regularizers [23], which
lead to a proximal update that can be performed with a single pass over the decision space, and
strong theoretical dependence on game constants [13, 14, 23, 25]. A second popular approach is
the counterfactual regret minimization (CFR) framework, which decomposes regret minimization
on the EFG decision sets into local simplex-based regret minimization [49]. In theory, CFR-based
results have mostly led to an inferior T −1/2 rate of convergence, but in practice the CFR framework
instantiated with regret matching+ (RM+) [41] or predictive RM+ (PRM+) [15] is the fastest
approach for essentially every EFG setting. The most competitive FOM-based approaches for
practical performance are based on dilated regularizers [14, 24], but these have not been able to
beat CFR+ on EFG settings; we show for the first time that it is possible to beat CFR+ through
a combination of block-coordinate updates and restarting, at least on some games. An extended
discussion of FOM and CFR approaches to EFG solving is given in Appendix A."
NOTATION AND PRELIMINARIES,0.024930747922437674,"2
Notation and Preliminaries"
NOTATION AND PRELIMINARIES,0.02631578947368421,"In this section, we provide the necessary background and notation subsequently used to describe and
analyze our algorithm. As discussed in the introduction, our focus is on bilinear problems (PD)."
NOTATION AND OPTIMIZATION BACKGROUND,0.027700831024930747,"2.1
Notation and Optimization Background"
NOTATION AND OPTIMIZATION BACKGROUND,0.029085872576177285,"We use bold lowercase letters to denote vectors and bold uppercase letters to denote matrices. We
use ∥· ∥to denote an arbitrary ℓp norm for p ≥1 applied to a vector in either Rm or Rn, depending
on the context. The norm dual to ∥· ∥is denoted by ∥· ∥∗and defined in the standard way as
∥z∥∗= supx̸=0
⟨z,x⟩"
NOTATION AND OPTIMIZATION BACKGROUND,0.030470914127423823,"∥x∥, where ⟨z, x⟩denotes the standard inner product. In particular, for ∥·∥= ∥·∥p,
where p ≥1, we have ∥· ∥∗= ∥· ∥p∗, where 1"
NOTATION AND OPTIMIZATION BACKGROUND,0.03185595567867036,"p +
1
p∗= 1. We further use ∥· ∥∗to denote the"
NOTATION AND OPTIMIZATION BACKGROUND,0.0332409972299169,"induced matrix norm defined by ∥M∥∗= supx̸=0
∥Mx∥∗"
NOTATION AND OPTIMIZATION BACKGROUND,0.03462603878116344,"∥x∥. In particular, for the Euclidean norm
∥· ∥= ∥· ∥2, the dual norm ∥· ∥∗= ∥· ∥2 is also the Euclidean norm, and ∥M∥∗= ∥M∥2
is the matrix operator norm. For the ℓ1 norm ∥· ∥= ∥· ∥1, the dual norm is the ℓ∞-norm,
∥· ∥∗= ∥· ∥∞, while the matrix norm is ∥M∥∗= ∥M∥∞→1 = supx̸=0
∥Mx∥∞"
NOTATION AND OPTIMIZATION BACKGROUND,0.036011080332409975,"∥x∥1
= maxi,j |Mij|.
We use ∆n = {x ∈Rn : x ≥0, ⟨1, x⟩= 1} to denote the probability simplex in n dimensions."
NOTATION AND OPTIMIZATION BACKGROUND,0.037396121883656507,"Primal-dual Gap. Given x ∈Rd, the primal value of the problem (PD) is maxv∈X ⟨Mx, v⟩.
Similarly, the dual value of (PD) is defined by minu∈Y ⟨Mu, y⟩. Given a primal-dual pair (x, y) ∈
X × Y, the primal-dual gap (or saddle-point gap) is defined by"
NOTATION AND OPTIMIZATION BACKGROUND,0.038781163434903045,"Gap(x, y) = max
v∈X ⟨Mx, v⟩−min
u∈Y ⟨Mu, y⟩=
max
(u,v)∈X×Y Gapu,v(x, y),"
NOTATION AND OPTIMIZATION BACKGROUND,0.04016620498614958,"where we define Gapu,v(x, y) = ⟨Mx, v⟩−⟨Mu, y⟩. For our analysis, it is useful to work with
the relaxed gap Gapu,v(x, y) for some arbitrary but fixed u ∈X, v ∈Y, and then draw conclusions
about a candidate solution by making concrete choices of u, v."
NOTATION AND OPTIMIZATION BACKGROUND,0.04155124653739612,"Definitions and Facts from Convex Analysis. In this paper, we primarily work with convex
functions f : Rn →R ∪{±∞} that are differentiable on the interior of their domain. We say that f
is cf-strongly convex w.r.t. a norm ∥· ∥if ∀y ∈Rn, ∀x ∈int domf,"
NOTATION AND OPTIMIZATION BACKGROUND,0.04293628808864266,"f(y) ≥f(x) + ⟨∇f(x), y −x⟩+ cf"
NOTATION AND OPTIMIZATION BACKGROUND,0.0443213296398892,2 ∥y −x∥2.
NOTATION AND OPTIMIZATION BACKGROUND,0.045706371191135735,"We will also need convex conjugates and Bregman divergences. Given an extended real valued
function f : Rn →R∪{±∞}, its convex conjugate is defined by f ∗(z) = supz∈Rn{⟨z, x⟩−f(x)}.
Let f : Rn →R ∪{±∞} be a function that is differentiable on the interior of its domain. Given
y ∈Rn and x ∈int domf, the Bregman divergence Df(y, x) is defined by Df(y, x) = f(y) −
f(x) −⟨∇f(x), y −x⟩. If the function f is cf-strongly convex, then Df(y, x) ≥cf"
NOTATION AND OPTIMIZATION BACKGROUND,0.04709141274238227,2 ∥y −x∥2.
NOTATION AND OPTIMIZATION BACKGROUND,0.04847645429362881,"2.2
Extensive-Form Games: Background and Additional Notation"
NOTATION AND OPTIMIZATION BACKGROUND,0.04986149584487535,"Extensive form games are represented by game trees. Each node v in the game tree belongs to exactly
one player i ∈{1, . . . , n} ∪{c} whose turn it is to move. Player c is a special player called the
chance player; it is used to denote random events that happen in the game, such as drawing a card
from a deck or tossing a coin. At terminal nodes of the game, players are assigned payoffs. We
focus on two-player zero-sum games, where n = 2 and payoffs sum to zero. Private information is
modeled using information sets (infosets): a player cannot distinguish between nodes in the same
infoset, so the set of actions available to them must be the same at each node in the infoset."
NOTATION AND OPTIMIZATION BACKGROUND,0.05124653739612189,"Treeplexes. The decision problem for a player in a perfect recall EFG can be described as follows.
There exists a set of decision points J , and at each decision point j the player has a set of actions Aj
with |Aj| = nj actions in total. These decision points coincide with infosets in the EFG. Without loss
of generality, we let there be a single root decision point, representing the first decision the player
makes in the game. The choice to play an action a ∈Aj for a decision point j ∈J is represented
using a sequence (j, a), and after playing this sequence, the set of possible next decision points is
denoted by Cj,a (which may be empty in case the game terminates). The set of decisions form a tree,
meaning that Cj,a ∩Cj′,a′ = ∅unless j = j′ and a = a′; this is known as perfect recall. The last
sequence (necessarily unique) encountered on the path from the root to decision point j is denoted by
pj. We define ↓j as the set consisting of all decision points that can be reached from j. An example
of the use of this notation for a player in Kuhn poker [26] can be found in Appendix B."
NOTATION AND OPTIMIZATION BACKGROUND,0.05263157894736842,"The set of strategies for a player can be characterized using the sequence-form, where the value of
the decision variable assigned to playing the sequence (j, a) is the product of the decision variable
assigned to playing the parent sequence pj and the probability of playing action a when at j [44]."
NOTATION AND OPTIMIZATION BACKGROUND,0.054016620498614956,"The set of all sequence-form strategies of a player form a polytope known as the sequence-form
polytope. Sequence-form polytopes fall into a class of polytopes known as treeplexes [23], which can
be characterized inductively using convex hull and Cartesian product operations:"
NOTATION AND OPTIMIZATION BACKGROUND,0.055401662049861494,"Definition 2.1 (Treeplex). A treeplex X for a player can be characterized recursively as follows,
where r is the the root decision point for a player."
NOTATION AND OPTIMIZATION BACKGROUND,0.05678670360110803,"Xj,a =
Y"
NOTATION AND OPTIMIZATION BACKGROUND,0.05817174515235457,"j′∈Cj,a
X↓j′,"
NOTATION AND OPTIMIZATION BACKGROUND,0.05955678670360111,"X↓j = {(λ1, . . . , λ|Aj|, λ1x1, . . . , λ|Aj|x|Aj| : (λ1, . . . , λ|Aj|) ∈∆|Aj|, xa ∈Xj,a},"
NOTATION AND OPTIMIZATION BACKGROUND,0.060941828254847646,X = {1} × X↓r.
NOTATION AND OPTIMIZATION BACKGROUND,0.062326869806094184,"This formulation allows the expected loss of a player to be formulated as a bilinear function ⟨Mx, y⟩
of players’ strategies x, y. This gives rise to the BSPP in Equation (PD), and the set of saddle points
of that BSPP are exactly the set of Nash equilibria of the EFG. The payoff matrix M is a sparse
matrix, whose nonzeroes correspond to the set of leaf nodes of the game tree."
NOTATION AND OPTIMIZATION BACKGROUND,0.06371191135734072,"Indexing Notation. A sequence-form strategy of a player can be written as a vector v, with an entry
for each sequence (j, a). We use vj to denote the subset of size |Aj| of entries of v that correspond
to sequences (j, a) formed by taking actions a ∈Aj and let v↓j denote the subset of entries of v that
are indexed by sequences that occur in the subtreeplex rooted at j. Additionally, we use vpj to denote
the (scalar) value of the parent sequence of decision point j. By convention, for the root decision
point j, we let vpj = 1. Observe that for any j ∈J , vj/vpj is in the probability simplex."
NOTATION AND OPTIMIZATION BACKGROUND,0.06509695290858726,"Given a treeplex Z we denote by JZ the set of infosets for this treeplex. We say that a partition of
JZ into k ≤|JZ| sets JZ
(1), . . . , JZ
(k) respects the treeplex ordering if for any two sets JZ
(i),
JZ
(i′) with i < i′ and any two infosets j ∈JZ
(i), j′ ∈JZ
(i′), j does not intersect the path from
j′ to the root decision point. The set of infosets for the player x is denoted by JX , while the set of
infosets for player y is denoted by JY. We assume that JX and JY are partitioned into s nonempty
sets JX
(1), JX
(2), . . . , JX
(s) and JY
(1), JY
(2), . . . , JY
(s), where s ≤min{|JX |, |JY|} and the
ordering of the sets in the two partitions respect the treeplex ordering of X, Y, respectively."
NOTATION AND OPTIMIZATION BACKGROUND,0.0664819944598338,"Given a pair (t, t′), we use Mt,t′ to denote the full-dimensional (m × n) matrix obtained from the
matrix M by keeping all entries indexed by JX
(t) and JY
(t′), and zeroing out the rest. When in
place of t or t′ we use “:”, it corresponds to keeping as non-zeros all rows (for the first index) or all
columns (for the second index). In particular, Mt,: is the matrix that keeps all rows of M indexed
by JX
(t) intact and zeros out the rest. Further, notation Mt′,t:s is used to indicate that we select
rows indexed by JX
(t′) and all columns of M indexed by JY
(t), JY
(t+1), . . . , JY
(s), while we zero
out the rest; similarly for Mt:s,t′. Notation Mt′,1:t is used to indicate that we select rows indexed
by JX
(t′) and all columns of M indexed by JY
(1), JY
(2), . . . , JY
(t), while we zero out the rest;
similarly for M1:t,t′. Given a vector x ∈X, x(t) denotes the entries of x indexed by the elements of
JX
(t); similarly, for y ∈Y, y(t) denotes the entries of y indexed by the elements of JY
(t)."
NOTATION AND OPTIMIZATION BACKGROUND,0.06786703601108034,"Additionally, we use M(t,t′) to denote the submatrix of M obtained by selecting rows indexed by
JX
(t) and columns indexed by JY
(t′). M(t,t′) is (p × q)-dimensional, for p = P"
NOTATION AND OPTIMIZATION BACKGROUND,0.06925207756232687,"j∈JX (t) |Aj| and
q = P"
NOTATION AND OPTIMIZATION BACKGROUND,0.07063711911357341,j∈JY (t′) |Aj|. Notation “:” has the same meaning as in the previous paragraph.
NOTATION AND OPTIMIZATION BACKGROUND,0.07202216066481995,"Dilated Regularizers. We assume access to strongly convex functions ϕ : X →R and ψ : Y →R
with known strong convexity parameters cϕ > 0 and cψ > 0, and that are continuously differentiable
on the interiors of their respective domains. We further assume that these functions are nice as defined
by Farina et al. [14]: their gradients and the gradients of their convex conjugates can be computed in
time linear (or nearly linear) in the dimension of the treeplex."
NOTATION AND OPTIMIZATION BACKGROUND,0.07340720221606649,"A dilated regularizer is a framework for constructing nice regularizing functions for treeplexes. It
makes use of the inductive characterization of a treeplex via Cartesian product and convex hull
operations to generalize from the local simplex structure of the sequence-form polytope at a decision
point to the entire sequence-form polytope. In particular, given a local “nice” regularizer ϕj for each
decision point j, a dilated regularizer for the treeplex can be defined as ϕ(x) = P"
NOTATION AND OPTIMIZATION BACKGROUND,0.07479224376731301,j∈JX xpjϕj( xj
NOTATION AND OPTIMIZATION BACKGROUND,0.07617728531855955,xpj ).
NOTATION AND OPTIMIZATION BACKGROUND,0.07756232686980609,"The key property of these dilated regularizing functions is that the prox computations of the form
xk = argminx∈X {⟨h, x⟩+ Dϕ(xk, xk−1)} decompose into bottom-up updates, where, up to a
scaling factor, each set of coordinates from set JX
(t) can be computed solely based on the coordinates
of xk from sets JX
(1), . . . , JX
(t−1) and coordinates of g from sets JX
(1), . . . , JX
(t). Concretely,
the recursive structure of the prox update is as follows (this was originally shown by [23], here we
show a variation from Farina et al. [13]):"
NOTATION AND OPTIMIZATION BACKGROUND,0.07894736842105263,"Proposition 2.2 (Farina et al. [13]). A prox update to compute xk, with gradient h and center xk−1
on a treeplex X using a Bregman divergence constructed from a dilated DGF ϕ can be decomposed
into local prox updates at each decision point j ∈JX as follows:"
NOTATION AND OPTIMIZATION BACKGROUND,0.08033240997229917,"xj
k = xpj
k · argmin
bj∈∆nj"
NOTATION AND OPTIMIZATION BACKGROUND,0.0817174515235457,"
hj + ˆhj, bj
+ Dϕj

bj, xj
k−1
xpj
k−1 
,"
NOTATION AND OPTIMIZATION BACKGROUND,0.08310249307479224,"ˆh(j,a) =
X"
NOTATION AND OPTIMIZATION BACKGROUND,0.08448753462603878,"j′∈Cj,a"
NOTATION AND OPTIMIZATION BACKGROUND,0.08587257617728532,"
ϕ↓j′∗ 
−h↓j + ∇ϕ↓j′ 
x↓j′"
NOTATION AND OPTIMIZATION BACKGROUND,0.08725761772853186,"k−1

−ϕj′ xj′"
NOTATION AND OPTIMIZATION BACKGROUND,0.0886426592797784,"k−1
x(j,a)
k−1"
NOTATION AND OPTIMIZATION BACKGROUND,0.09002770083102493,"
+

∇ϕj′ xj′"
NOTATION AND OPTIMIZATION BACKGROUND,0.09141274238227147,"k−1
x(j,a)
k−1"
NOTATION AND OPTIMIZATION BACKGROUND,0.09279778393351801,"
, xj′"
NOTATION AND OPTIMIZATION BACKGROUND,0.09418282548476455,"k−1
x(j,a)
k−1 
."
EXTRAPOLATED CYCLIC ALGORITHM,0.09556786703601108,"3
Extrapolated Cyclic Algorithm"
EXTRAPOLATED CYCLIC ALGORITHM,0.09695290858725762,"Our extrapolated cyclic primal-dual algorithm is summarized in Algorithm 1. As discussed in
Section 2, under the block partition and ordering that respects the treeplex ordering, the updates for
x(t)
k
in Line 9 (respectively, y(t)
k
in Line 12), up to scaling by the value of their respective parent"
EXTRAPOLATED CYCLIC ALGORITHM,0.09833795013850416,"sequences, can be carried out using only the information about xj
k
x
pj
k and hj
k (respectively, yj
k
y
pj
k
and gj
k)"
EXTRAPOLATED CYCLIC ALGORITHM,0.0997229916897507,"for infosets j that are “lower” on the treeplex. The specific choices of the extrapolation sequences exk
and eyk that only utilize the information from prior cycles and the scaled values of xj
k
x
pj
k and yj
k
y
pj
k
for"
EXTRAPOLATED CYCLIC ALGORITHM,0.10110803324099724,"infosets j updated up to the block t updates for xk and yk are what crucially enables us to decompose
the updates for xk and yk into local block updates carried out in the bottom-up manner. At the end"
EXTRAPOLATED CYCLIC ALGORITHM,0.10249307479224377,"of the cycle, once xj
k
x
pj
k and yj
k
y
pj
k
has been updated for all infosets, we can carry out a top-to-bottom"
EXTRAPOLATED CYCLIC ALGORITHM,0.1038781163434903,"update to fully determine vectors xk and yk, as summarized in the last two for loops in Algorithm 1.
We present an implementation-specific version of the algorithm in Appendix D, which explicitly
demonstrates that our algorithm’s runtime does not have a dependence on the number of blocks used.
In our analysis of the implementation-specific version of the algorithm, we argue that the per-iteration
complexity of our algorithm matches that of MP. To support this analysis, we compare the empirical
runtimes of our algorithm with MP and CFR+ variants in Section 4."
EXTRAPOLATED CYCLIC ALGORITHM,0.10526315789473684,"Our convergence argument is built on the decomposition of the relaxed gap Gapu,v(xk, vk) for
arbitrary but fixed (u, v) ∈X × Y into telescoping and non-positive terms, which is common in
first-order methods. The first idea that enables leveraging cyclic updates lies in replacing vectors
Mxk and M⊤yk by “extrapolated” vectors gk and hk that can be partially updated in a blockwise
fashion as a cycle of the algorithm progresses, as stated in Proposition 3.1. To our knowledge, this
basic idea originates in Song and Diakonikolas [39]. Unique to our work are the specific choices
of gk and hk, which leverage all the partial information known to the algorithm up to the current
iteration and block update. Crucially, we leverage the treeplex structure to show that our chosen
updates are sufficient to bound the error sequence Ek and obtain the claimed convergence bound in
Theorem 3.2. Due to space constraints, the proof is deferred to Appendix C."
EXTRAPOLATED CYCLIC ALGORITHM,0.10664819944598337,"To simplify the exposition, we introduce the following notation: Mx := s−1
X"
EXTRAPOLATED CYCLIC ALGORITHM,0.10803324099722991,"t=1
Mt,t+1:s, My := M −Mx = s
X"
EXTRAPOLATED CYCLIC ALGORITHM,0.10941828254847645,"t=1
Mt:s,t;"
EXTRAPOLATED CYCLIC ALGORITHM,0.11080332409972299,"µx := ∥Mx∥∗+ ∥My∥∗, µy := ∥M⊤
x ∥∗+ ∥M⊤
y ∥∗. (3.1)"
EXTRAPOLATED CYCLIC ALGORITHM,0.11218836565096953,"When the norm of the space is ∥· ∥= ∥· ∥1, both µx and µy are bounded above by 2 maxi,j |Mij|."
EXTRAPOLATED CYCLIC ALGORITHM,0.11357340720221606,"The next proposition decomposes the relaxed gap into an error term and telescoping terms. The
proposition is independent of the specific choices of extrapolated vectors gk, hk."
EXTRAPOLATED CYCLIC ALGORITHM,0.1149584487534626,Algorithm 1 Extrapolated Cyclic Primal-Dual EFG Solver (ECyclicPDA)
EXTRAPOLATED CYCLIC ALGORITHM,0.11634349030470914,"1: Initialization: x0 ∈X, y0 ∈Y, η0 = H0 = 0, η =
√cϕcψ
µx+µy , ¯x0 = x0, ¯y0 = y0, g0 = 0, h0 = 0
2: for k = 1 : K do
3:
Choose ηk ≤η, Hk = Hk−1 + ηk
4:
gk = gk−1, hk = hk−1
5:
exk = xk−1 + ηk−1"
EXTRAPOLATED CYCLIC ALGORITHM,0.11772853185595568,"ηk (xk−1 −xk−2), eyk = yk−1 + ηk−1"
EXTRAPOLATED CYCLIC ALGORITHM,0.11911357340720222,"ηk (yk−1 −yk−2)
6:
for t = 1 : s do
7:
h(t)
k
= (M(:,t))⊤eyk"
EXTRAPOLATED CYCLIC ALGORITHM,0.12049861495844875,"8:
x(t)
k
=
h
argminx∈X
n
ηk ⟨x, hk⟩+ Dϕ(x, xk−1)
oi(t)"
EXTRAPOLATED CYCLIC ALGORITHM,0.12188365650969529,"9:
ex(t)
k
=
h xj
k
x
pj
k xpj
k−1 + ηk−1 ηk"
EXTRAPOLATED CYCLIC ALGORITHM,0.12326869806094183,"
xj
k−1 −
xj
k−1
x
pj
k−1 xpj
k−2
i"
EXTRAPOLATED CYCLIC ALGORITHM,0.12465373961218837,j∈JX (t)
EXTRAPOLATED CYCLIC ALGORITHM,0.1260387811634349,"10:
g(t)
k
= M(t,:)exk"
EXTRAPOLATED CYCLIC ALGORITHM,0.12742382271468145,"11:
y(t)
k
=
h
argmaxv∈Y
n
ηk ⟨gk, v⟩−Dψ(v, yk−1)
oi(t)"
EXTRAPOLATED CYCLIC ALGORITHM,0.12880886426592797,"12:
ey(t)
k
=
h yj
k
y
pj
k ypj
k−1 + ηk−1 ηk"
EXTRAPOLATED CYCLIC ALGORITHM,0.13019390581717452,"
yj
k−1 −
yj
k−1
y
pj
k−1 ypj
k−2
i"
EXTRAPOLATED CYCLIC ALGORITHM,0.13157894736842105,j∈JY (t)
EXTRAPOLATED CYCLIC ALGORITHM,0.1329639889196676,"13:
for j ∈JX do"
EXTRAPOLATED CYCLIC ALGORITHM,0.13434903047091412,"14:
xj
k = xpj
k ·
  xj
k
x
pj
k "
EXTRAPOLATED CYCLIC ALGORITHM,0.13573407202216067,"15:
for j ∈JY do"
EXTRAPOLATED CYCLIC ALGORITHM,0.1371191135734072,"16:
yj
k = ypj
k ·
  yj
k
y
pj
k "
EXTRAPOLATED CYCLIC ALGORITHM,0.13850415512465375,"17:
¯xk = Hk−ηk"
EXTRAPOLATED CYCLIC ALGORITHM,0.13988919667590027,"Hk
¯xk−1 + ηk"
EXTRAPOLATED CYCLIC ALGORITHM,0.14127423822714683,"Hk xk, ¯yk = Hk−ηk"
EXTRAPOLATED CYCLIC ALGORITHM,0.14265927977839335,"Hk
¯yk−1 + ηk Hk yk"
EXTRAPOLATED CYCLIC ALGORITHM,0.1440443213296399,"18: Return: ¯xK, ¯yK"
EXTRAPOLATED CYCLIC ALGORITHM,0.14542936288088643,"Proposition 3.1. Let xk, yk be the iterates of Algorithm 1 for k ≥1. Then, for all k ≥1, xk ∈X,
yk ∈Y, we have"
EXTRAPOLATED CYCLIC ALGORITHM,0.14681440443213298,"ηkGapu,v(xk, yk) ≤Ek −Dϕ(u, xk) + Dϕ(u, xk−1) −Dψ(v, yk) + Dψ(v, yk−1),"
EXTRAPOLATED CYCLIC ALGORITHM,0.1481994459833795,where the error sequence Ek is defined by
EXTRAPOLATED CYCLIC ALGORITHM,0.14958448753462603,"Ek := ηk ⟨Mxk −gk, v −yk⟩−ηk

u −xk, M⊤yk −hk

−Dψ(yk, yk−1) −Dϕ(xk, xk−1)."
EXTRAPOLATED CYCLIC ALGORITHM,0.15096952908587258,"To obtain our main result, we leverage the blockwise structure of the problem, the bilinear structure
of the objective, and the treeplex structure of the feasible sets to control the error sequence Ek. A
key property that enables this result is that normalized entries xj
k/xpj
k−1 from the same information
set belong to a probability simplex. This property is crucially used in controlling the error of the
extrapolation vectors. The main result is summarized in the following theorem.
Theorem 3.2. Consider the iterates xk, yk for k ≥1 in Algorithm 1 and the output primal-dual pair
¯xK, ¯yK. Then, ∀k ≥1,"
EXTRAPOLATED CYCLIC ALGORITHM,0.1523545706371191,"µxDϕ(x∗, xK) + µyDψ(y∗, yK)"
EXTRAPOLATED CYCLIC ALGORITHM,0.15373961218836565,"µx + µy
≤Dϕ(x∗, x0) + Dψ(y∗, y0), and, further,"
EXTRAPOLATED CYCLIC ALGORITHM,0.15512465373961218,"Gap(¯xK, ¯yK) =
sup
u∈X,v∈Y
{⟨M¯xK, v⟩−⟨Mu, ¯yK⟩} ≤supu∈X,v∈Y{Dϕ(u, x0) + Dψ(v, y0)} HK
."
EXTRAPOLATED CYCLIC ALGORITHM,0.15650969529085873,"In the above bound, if ∀k ≥1, ηk = η =
√cϕcψ
µx+µy , then HK = Kη. As a consequence, for any ϵ > 0,"
EXTRAPOLATED CYCLIC ALGORITHM,0.15789473684210525,"Gap(¯xK, ¯yK) ≤ϵ after at most
 (µx+µy)(supu∈X,v∈Y{Dϕ(u,x0)+Dψ(v,y0)})
√cϕcψϵ

iterations."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1592797783933518,"4
Experimental Evaluation and Discussion"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16066481994459833,"We evaluate the performance of ECyclicPDA instantiated with three different dilated regularizers:
dilated entropy [25], dilatable global entropy [14], and dilated ℓ2 [13]. In the case of the dilated ℓ2"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16204986149584488,"regularizer, we use dual averaging of the “extrapolated” vectors gk and hk in our algorithm, since
otherwise we have no guarantee that the iterates would remain in the relative interior of the domain
of the dilated DGF, and the Bregman divergence may become undefined. We compare our method
to MP, which is state-of-the-art among first-order methods for EFG solving. We test ECyclicPDA
and MP with three different averaging schemes: uniform, linear, and quadratic averaging since
Gao et al. [20] suggest that these different averaging schemes can lead to faster convergence in
practice. We also compare against empirical state-of-the-art CFR+ variants: CFR+ [41], and the
predictive CFR+ variant (PCFR+) [15]. We emphasize that our method achieves the same O( 1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1634349030470914,"T )
average-iterate convergence rate as MP, and that all the CFR+ variants have the same suboptimal
O( 1
√"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16481994459833796,"T ) average-iterate convergence rate. We experiment on four standard benchmark games for
EFG solving: Goofspiel (4 ranks), Liar’s Dice, Leduc (13 ranks), and Battleship. In all experiments,
we run for 10, 000 full (or equivalent) gradient computations. This corresponds to 5,000 iterations
of ECyclicPDA, CFR+, and PCFR+, and 2,500 iterations of MP.2 A description of all games is
provided in Appendix E. Additional experimental details are provided in Appendix G."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16620498614958448,"For each instantiation of ECyclicPDA considered on a given game (choice of regularizer, averaging,
and block construction strategy) the stepsize is tuned by taking power of 2 multiples of η (2l · η for
l ∈N), where η is the theoretical stepsize stated in Theorem 3.2, and then choosing the stepsize η∗
among these multiples of η that has the best performance. Within the algorithm, we use a constant
stepsize, letting ηk = η0 for all k. We apply the same tuning scheme for MP stepsizes (for a given
choice of regularizer and averaging). Note that this stepsize tuning is coarse, and so it is possible that
better results can be achieved for ECyclicPDA and MP using finer stepsize tuning."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16759002770083103,"We test our algorithm with four different block construction strategies. The single block construction
strategy puts every decision point in a single block, and thus it corresponds to the non-block-based
version of ECyclicPDA. The children construction strategy iterates through the decision points of the
treeplex bottom-up (by definition, this will respect the treeplex ordering), and placing each set of
decision points that have parent sequences starting from the same decision point in its own block. The
postorder construction strategy iterates through the decision points bottom-up (again, by definition,
this will respect the treeplex ordering). The order is given by a postorder traversal of the treeplex,
treating all decision points that have the same parent sequence as a single node (and when the node is
processed, all decision points are sequentially added to the block). It greedily makes blocks as large
as possible, while only creating a new block if it causes a parent decision point and child decision
point to end up in the same block. The infosets construction strategy places each decision point in its
own block. We provide further description of the block construction strategies in Appendix F."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.16897506925207756,"We show the results of different block construction strategies in Figure 1. For each block construction
strategy, ECyclicPDA is instantiated with the choice of regularizer and averaging that yields the
fastest convergence among all choices of parameters. We can see that the different block construction
strategies do not make a significant difference in Goofspiel (4 ranks) or in Leduc (13 ranks). However,
we see benefits of using blocks in Liar’s Dice and Battleship. In Liar’s Dice, children and postorder
have a clear advantage, and children outperforms the other block construction strategies in Battleship."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1703601108033241,"We show the results of comparing our algorithm against MP, CFR+, and PCFR+ in Figure 2.
ECyclicPDA is instantiated with the choice of regularizer, averaging, and block construction strategy
that yields the fastest convergence among all choices for ECyclicPDA, and MP is instantiated with
the choice of regularizer and averaging that yields the fastest convergence among all choices for MP.
We see that ECyclicPDA performs better than MP in all games besides Goofspiel (4 ranks), where
they perform about the same. In Liar’s Dice and Battleship, the games where ECyclicPDA benefits
from having multiple blocks, we see competitiveness with CFR+ and PCFR+. In particular, in Liar’s
Dice, ECyclicPDA is overtaking CFR+ at 10, 000 gradient computations. On Battleship, we see that
both ECyclicPDA and MP outperform CFR+, and that ECyclicPDA is competitive with PCFR+."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.17174515235457063,"Restarting. We now introduce restarting as a heuristic tool for speeding up EFG solving. While
restarting is only known to lead to a linear convergence rate in the case of using the ℓ2 regularizer
in certain FOMs [5, 21], we apply restarting as a heuristic across our methods based on dilated
regularizers and to CFR-based methods. To the best of our knowledge, restarting schemes have not
been empirically evaluated on EFG algorithms such as MP, CFR+, or (obviously), our new method.
We implement restarting by resetting the averaging process when the duality gap has halved since
the last time the averaging process was reset. After resetting, the initial iterate is set equal to the last"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1731301939058172,2Here we count one gradient evaluation for x and one for y as two gradient evaluations total.
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1745152354570637,"102
104
Gradient computations 10−2 100"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.17590027700831026,Duality gap
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1772853185595568,Goofspiel (4 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1786703601108033,"102
104
Gradient computations 10−4 10−1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18005540166204986,Liar’s Dice
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1814404432132964,"102
104
Gradient computations 10−3 10−1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18282548476454294,Leduc (13 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18421052631578946,"102
104
Gradient computations 10−5 10−2"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18559556786703602,Battleship
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18698060941828254,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1883656509695291,"Figure 1: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.18975069252077562,"102
104
Gradient computations 10−4 10−1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.19113573407202217,Duality gap
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1925207756232687,Goofspiel (4 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.19390581717451524,"102
104
Gradient computations 10−6 10−2"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.19529085872576177,Liar’s Dice
EXPERIMENTAL EVALUATION AND DISCUSSION,0.19667590027700832,"102
104
Gradient computations 10−2"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.19806094182825484,"101
Leduc (13 ranks)"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.1994459833795014,"102
104
Gradient computations 10−5 10−1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.20083102493074792,Battleship
EXPERIMENTAL EVALUATION AND DISCUSSION,0.20221606648199447,"ECyclicPDA
MP
CFR+
PCFR+"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.203601108033241,"Figure 2: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA, MP, CFR+, PCFR+."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.20498614958448755,"iterate we saw before restarting. Since the restarting heuristic is one we introduce, we distinguish
restarted variants in the plots by prepending the name of the algorithm with “r”. For example, when
restarting is applied to CFR+, the label used is rCFR+."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.20637119113573407,"We show the results of different block construction strategies when restarting is used on ECyclicPDA
in Figure 3. As before, we take the combination of regularizer and averaging scheme that works
best. Again, we can see that the different block construction strategies do not make a significant
difference in Goofspiel (4 ranks) or in Leduc (13 ranks), while making a difference for Liar’s Dice
and Battleship. However, with restarting, the benefit of the children and postorder for Liar’s Dice
and Battleship is even more pronounced relative to the other block construction strategies; the gap is
several orders of magnitude after 104 gradient computations. Note that while children performed
worse than single block for Battleship previously, with restarting, children performs much better."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2077562326869806,"Finally, we compare the performance of the restarted version of our algorithm, with restarted versions
of MP, CFR+, and PCFR+ in Figure 4. As before, we take the combination of regularizer, averaging
scheme, and block construction strategy that works best for ECyclicPDA, and the combination of
regularizer and averaging scheme that works best for MP. Firstly, we note that the scale of the y-axis
is different from Figure 2 for all games besides Leduc (13 ranks), because restarting tends to hit
much higher levels of precision. We see that restarting provides significant benefits for PCFR+
in Goofspiel (4 ranks) allowing it to converge to numerical precision, while the other algorithms
do not benefit much. In Liar’s Dice, restarted CFR+ and PCFR+ converge to numerical precision
within 200 gradient computations, and restarted ECyclicPDA converges to numerical precision at 104
gradient computations. Additionally, restarted MP achieves a much lower duality gap. For Battleship,
ECyclicPDA, MP, and PCFR+ all benefit from restarting, and restarted ECyclicPDA is competitive
with restarted PCFR+. Similar to the magnification in benefit of using blocks versus not using blocks
when restarting in Liar’s Dice and Battleship, we see that restarted ECyclicPDA achieves significantly
better duality gap than MP in these games."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.20914127423822715,"Wall-clock time experiments. In Table 1, we show the average wall-clock time per iteration of
our algorithm instantiated with different block construction strategies as well as MP, CFR+, and
PCFR+. It is clear from this table that the runtimes are pretty similar to each other, and this is without
extensive optimization of our particular implementation. Clearly, our algorithm is at least as fast
as MP per “full” gradient computation. Since the computational bottleneck of gradient and prox
computations becomes apparent in bigger games, Battleship demonstrates the speed of our algorithm
relative to MP best."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.21052631578947367,"102
104
Gradient computations 10−2 100"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.21191135734072022,Duality gap
EXPERIMENTAL EVALUATION AND DISCUSSION,0.21329639889196675,Goofspiel (4 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2146814404432133,"102
104
Gradient computations 10−10 10−4"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.21606648199445982,Liar’s Dice
EXPERIMENTAL EVALUATION AND DISCUSSION,0.21745152354570638,"102
104
Gradient computations 10−2 100"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2188365650969529,Leduc (13 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22022160664819945,"102
104
Gradient computations 10−10 10−4"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22160664819944598,Battleship
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22299168975069253,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22437673130193905,"Figure 3: Duality gap as a function of the number of full (or equivalent) gradient computations for
when restarting is applied to ECyclicPDA with different block construction strategies. We take the
best duality gap seen so far so that the plot is monotonic."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2257617728531856,"102
104
Gradient computations 10−10 10−4"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22714681440443213,Duality gap
EXPERIMENTAL EVALUATION AND DISCUSSION,0.22853185595567868,Goofspiel (4 ranks)
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2299168975069252,"102
104
Gradient computations 10−10 10−4"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23130193905817176,Liar’s Dice
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23268698060941828,"102
104
Gradient computations 10−2"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23407202216066483,"101
Leduc (13 ranks)"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23545706371191136,"102
104
Gradient computations 10−10 10−4"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23684210526315788,Battleship
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23822714681440443,"rECyclicPDA
rMP
rCFR+
rPCFR+"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.23961218836565096,"Figure 4: Duality gap as a function of the number of full (or equivalent) gradient computations for
when restarting is applied to ECyclicPDA, MP, CFR+, PCFR+. We take the best duality gap seen so
far so that the plot is monotonic."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2409972299168975,"Table 1: The average wall clock time per iteration in milliseconds of ECyclicPDA instantiated with
different block construction strategies, MP, CFR+, and PCFR+. The duality gap is computed every
100 iterations."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.24238227146814403,"Name
Goofspiel (4 ranks)
Liar’s Dice
Leduc (13 ranks)
Battleship
ECyclicPDA single block
2.470
11.370
6.770
46.020"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.24376731301939059,"ECyclicPDA children
4.270
15.020
7.450
47.230"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2451523545706371,"ECyclicPDA infosets
8.850
16.240
7.370
53.490"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.24653739612188366,"ECyclicPDA postorder
4.340
14.350
7.430
49.020"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.24792243767313019,"MP
3.720
19.530
10.360
83.450"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.24930747922437674,"CFR+
1.330
1.360
0.370
7.980"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.25069252077562326,"Predictive CFR+
1.750
1.920
0.430
9.410"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2520775623268698,"Discussion. We develop the first cyclic block-coordinate-like method for two-player zero-sum
EFGs. Our algorithm relies on the recursive nature of the prox updates for dilated regularizers,
cycling through blocks that respect the partial order induced on decision points by the treeplex, and
extrapolation to conduct pseudo-block updates, produce feasible iterates, and achieve O( 1"
EXPERIMENTAL EVALUATION AND DISCUSSION,0.25346260387811637,"T ) ergodic
convergence. Furthermore, the runtime of our algorithm has no dependence on the number of blocks.
We present empirical evidence that our algorithm generally outperforms MP, and is the first FOM to
compete with CFR+ and PCFR+ on non-trivial EFGs. Finally, we introduce a restarting heuristic for
EFG solving, and demonstrate often huge gains in convergence rate. An open question raised by our
work is understanding what makes restarting work for methods used with regularizers besides the
ℓ2 regularizer (the only setting for which there exist linear convergence guarantees). This may be
challenging because existing proofs require upper bounding the corresponding Bregman divergence
(for a given non-ℓ2 regularizer) between iterates by the distance to optimality. This is difficult for
entropy or any dilated regularizer because the initial iterate used by the algorithm after restarting
may have entries arbitrarily close to zero even if they are guaranteed to not exactly be zero (as is the
case for entropy). Relatedly, both our block-coordinate method and restarting have a much bigger
advantage in some numerical instances (Battleship, Liar’s Dice) than others (Leduc and Goofspiel); a
crucial question is to understand what type of game structure drives this behavior."
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2548476454293629,Acknowledgements
EXPERIMENTAL EVALUATION AND DISCUSSION,0.2562326869806094,"Darshan Chakrabarti was supported by National Science Foundation Graduate Research Fellowship
Program under award number DGE-2036197. Jelena Diakonikolas was supported by the Office
of Naval Research under award number N00014-22-1-2348. Christian Kroer was supported by
the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374, and the National
Science Foundation awards IIS-2147361 and IIS-2238960."
REFERENCES,0.25761772853185594,References
REFERENCES,0.2590027700831025,"[1] Aviad Aberdam and Amir Beck. An accelerated coordinate gradient descent algorithm for
non-separable composite optimization. Journal of Optimization Theory and Applications, 193
(1-3):219–246, 2021."
REFERENCES,0.26038781163434904,"[2] Ahmet Alacaoglu, Quoc Tran Dinh, Olivier Fercoq, and Volkan Cevher. Smooth primal-dual
coordinate descent algorithms for nonsmooth convex optimization. In Advances in Neural
Information Processing Systems, 2017."
REFERENCES,0.26177285318559557,"[3] Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik, and Yang Yuan.
Even faster accelerated
coordinate descent using non-uniform sampling. In Proceedings of International Conference on
Machine Learning, 2016."
REFERENCES,0.2631578947368421,"[4] Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. Near-optimal ϕ-regret learning
in extensive-form games. arXiv preprint arXiv:2208.09747, 2022."
REFERENCES,0.26454293628808867,"[5] David Applegate, Oliver Hinder, Haihao Lu, and Miles Lubin. Faster first-order primal-dual
methods for linear programming using restarts and sharpness. arXiv preprint arXiv:2105.12715,
2022."
REFERENCES,0.2659279778393352,"[6] Yu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. Efficient phi-regret minimization in
extensive-form games via online mirror descent. In Advances in Neural Information Processing
Systems, 2022."
REFERENCES,0.2673130193905817,"[7] Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type methods.
SIAM Journal on Optimization, 23(4):2037–2060, 2013."
REFERENCES,0.26869806094182824,"[8] Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted
regret minimization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019."
REFERENCES,0.27008310249307477,"[9] Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Coordinate methods for matrix games.
arXiv preprint arXiv:2009.08447, 2020."
REFERENCES,0.27146814404432135,"[10] Flavia Chorobura and Ion Necoara. Random coordinate descent methods for nonseparable
composite optimization. arxiv preprint arXiv:2203.14368, 2022."
REFERENCES,0.27285318559556787,"[11] Jelena Diakonikolas and Lorenzo Orecchia. Alternating randomized block coordinate descent.
In Proceedings of International Conference on Machine Learning, 2018."
REFERENCES,0.2742382271468144,"[12] Gabriele Farina, Christian Kroer, Noam Brown, and Tuomas Sandholm. Stable-predictive
optimistic counterfactual regret minimization. In Proceedings of the International Conference
on Machine Learning, 2019."
REFERENCES,0.2756232686980609,"[13] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Optimistic regret minimization
for extensive-form games via dilated distance-generating functions. In Advances in Neural
Information Processing Systems, 2019."
REFERENCES,0.2770083102493075,"[14] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Better regularization for sequential
decision spaces: Fast convergence rates for Nash, correlated, and team equilibria. In Proceedings
of the ACM Conference on Economics and Computation, 2021."
REFERENCES,0.278393351800554,"[15] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive
blackwell approachability: Connecting regret matching and mirror descent. In Proceedings of
the AAAI Conference on Artificial Intelligence, 2021."
REFERENCES,0.27977839335180055,"[16] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer, and
Tuomas Sandholm. Near-optimal no-regret learning dynamics for general convex games.
Advances in Neural Information Processing Systems, 2022."
REFERENCES,0.28116343490304707,"[17] Gabriele Farina, Chung-Wei Lee, Haipeng Luo, and Christian Kroer. Kernelized multiplicative
weights for 0/1-polyhedral games: Bridging the gap between learning in extensive-form and
normal-form games. In Proceedings of the International Conference on Machine Learning,
2022."
REFERENCES,0.28254847645429365,"[18] Olivier Fercoq. Quadratic error bound of the smoothed gap and the restarted averaged primal-
dual hybrid gradient. arXiv preprint arXiv:2206.03041, 2023."
REFERENCES,0.2839335180055402,"[19] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010."
REFERENCES,0.2853185595567867,"[20] Yuan Gao, Christian Kroer, and Donald Goldfarb. Increasing iterate averaging for solving
saddle-point problems. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021."
REFERENCES,0.2867036011080332,"[21] Andrew Gilpin, Javier Pena, and Tuomas Sandholm. First-order algorithm with O(ln(1/ϵ))
convergence for ϵ-equilibrium in two-person zero-sum games. Mathematical programming, 133
(1):279–298, 2012."
REFERENCES,0.2880886426592798,"[22] Mert Gürbüzbalaban, Asuman Ozdaglar, Pablo A Parrilo, and N Denizcan Vanli. When
cyclic coordinate descent outperforms randomized coordinate descent. In Advances in Neural
Information Processing Systems, 2017."
REFERENCES,0.2894736842105263,"[23] Samid Hoda, Andrew Gilpin, Javier Pena, and Tuomas Sandholm. Smoothing techniques for
computing Nash equilibria of sequential games. Mathematics of Operations Research, 35(2):
494–512, 2010."
REFERENCES,0.29085872576177285,"[24] Christian Kroer, Gabriele Farina, and Tuomas Sandholm. Solving large sequential games with
the excessive gap technique. In Advances in Neural Information Processing Systems, 2018."
REFERENCES,0.2922437673130194,"[25] Christian Kroer, Kevin Waugh, Fatma Kılınç-Karzan, and Tuomas Sandholm. Faster algo-
rithms for extensive-form game solving via improved smoothing functions. Mathematical
Programming, pages 1–33, 2020."
REFERENCES,0.29362880886426596,"[26] Harold William Kuhn and Albert William Tucker, editors. 11. Extensive Games and the Problem
of Information, pages 193–216. Princeton University Press, 2016."
REFERENCES,0.2950138504155125,"[27] Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form
games. In Advances in Neural Information Processing Systems, 2021."
REFERENCES,0.296398891966759,"[28] Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated randomized proximal coordinate
gradient method and its application to regularized empirical risk minimization. SIAM Journal
on Optimization, 25(4):2244–2273, 2015."
REFERENCES,0.29778393351800553,"[29] Viliam Lisý, Marc Lanctot, and Michael Bowling. Online monte carlo counterfactual re-
gret minimization for search in imperfect information games. In Proceedings of the 2015
International Conference on Autonomous Agents and Multiagent Systems, AAMAS ’15, page
27–36. International Foundation for Autonomous Agents and Multiagent Systems, 2015. ISBN
9781450334136."
REFERENCES,0.29916897506925205,"[30] Ji Liu, Stephen J. Wright, Christopher Ré, Victor Bittorf, and Srikrishna Sridhar. An asyn-
chronous parallel stochastic coordinate descent algorithm. arXiv preprint arxiv:1311.1873,
2014."
REFERENCES,0.30055401662049863,"[31] Mingyang Liu, Asuman Ozdaglar, Tiancheng Yu, and Kaiqing Zhang. The power of regular-
ization in solving extensive-form games. In Proceedings of the International Conference on
Learning Representations, 2023."
REFERENCES,0.30193905817174516,"[32] Rahul Mazumder, Jerome H Friedman, and Trevor Hastie. Sparsenet: Coordinate descent with
nonconvex penalties. Journal of the American Statistical Association, 106(495):1125–1138,
2011."
REFERENCES,0.3033240997229917,"[33] Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization, 15(1):229–251, 2004."
REFERENCES,0.3047091412742382,"[34] Yu. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAM Journal on Optimization, 22(2):341–362, 2012."
REFERENCES,0.3060941828254848,"[35] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and
related problems. Mathematical Programming, 109(2-3):319–344, 2007."
REFERENCES,0.3074792243767313,"[36] Sheldon M. Ross. Goofspiel — the game of pure strategy. Journal of Applied Probability, 8(3):
621–625, 1971."
REFERENCES,0.30886426592797783,"[37] Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate descent
methods. SIAM Journal on Optimization, 23(1):576–601, 2013."
REFERENCES,0.31024930747922436,"[38] Hao-Jun Michael Shi, Shenyinying Tu, Yangyang Xu, and Wotao Yin. A primer on coordinate
descent algorithms. arXiv preprint arXiv:1610.00040, 2016."
REFERENCES,0.31163434903047094,"[39] Chaobing Song and Jelena Diakonikolas. Fast cyclic coordinate dual averaging with extrapola-
tion for generalized variational inequalities. arXiv preprint arXiv:2102.13244, 2021."
REFERENCES,0.31301939058171746,"[40] Finnegan Southey, Michael P. Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse
Billings, and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. arXiv preprint
arXiv:1207.1411, 2012."
REFERENCES,0.314404432132964,"[41] Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint
arXiv:1407.5042, 2014."
REFERENCES,0.3157894736842105,"[42] Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up
limit Texas hold’em. In Twenty-Fourth International Joint Conference on Artificial Intelligence,
2015."
REFERENCES,0.3171745152354571,"[43] Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.
Journal of Computational and Applied Mathematics, 60(1-2):237–252, 1995."
REFERENCES,0.3185595567867036,"[44] Bernhard von Stengel. Efficient computation of behavior strategies. Games and Economic
Behavior, 14(2):220–246, 1996."
REFERENCES,0.31994459833795014,"[45] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate conver-
gence in constrained saddle-point optimization. In Proceedings of International Conference on
Learning Representations, 2021."
REFERENCES,0.32132963988919666,"[46] Stephen J. Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3–34,
2015."
REFERENCES,0.32271468144044324,"[47] Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression.
The Annals of Applied Statistics, 2(1):224–244, 2008."
REFERENCES,0.32409972299168976,"[48] Yuchen Zhang and Xiao Lin. Stochastic primal-dual coordinate method for regularized empirical
risk minimization. In Proceedings of International Conference on Machine Learning, 2015."
REFERENCES,0.3254847645429363,"[49] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret mini-
mization in games with incomplete information. In Advances in Neural Information Processing
Systems, 2007."
REFERENCES,0.3268698060941828,"A
Additional Related Work"
REFERENCES,0.32825484764542934,"There has been significant work on FOMs for two-player zero-sum EFG solving. Because this
is a BSPP, off-the-shelf FOMs for BSPPs can be applied, with the caveat that proximal oracles
are required. The standard Euclidean distance has been used in some cases [21], but it requires
solving a projection problem that takes O(n log2 n) time, where n is the dimension of a player’s
decision space [16, 21]. While this is “nearly” linear time, such projections have not been used
much in practice. Proximal oracles have instead been based on dilated regularizers [23], which
lead to a proximal update that can be performed with a single pass over the decision space. With
the dilated entropy regularizer, this can be performed in linear time, and this regularizer leads to
the strongest bounds on game constants that impact the convergence rate of proximal-oracle-based
FOMs [14, 25]. More recently, it has been shown that a specialized kernelization can be used to
achieve linear-time proximal updates and stronger convergence rates specifically for the dilated
entropy with optimistic online mirror descent through a correspondence with optimistic multiplicative
weights on the exponentially-many vertices of the decision polytope [6, 17]. Yet this approach was
shown to have somewhat disappointing numerical performance in Farina et al. [17], and thus is less
important practically despite its theoretical significance."
REFERENCES,0.3296398891966759,"A completely different approach for first-order-based updates is the CFR framework, which decom-
poses regret minimization on the EFG decision sets into local simplex-based regret minimization [49].
In theory, CFR-based results have mostly led to an inferior T −1/2 rate of convergence, but in practice
the CFR framework instantiated with regret matching+ (RM+) [41] or predictive RM+ (PRM+) [15]
is the fastest approach for essentially every EFG setting. RM+ is often fastest for “poker-like”
EFGs, while PRM+ is often fastest for other classes of games [15]. Improved rates on the order of
T −3/4 [12] and log T/T [4] have been achieved within the CFR framework, but only while using
regret minimizers that lead to significantly worse practical performance (in particular, numerically
these perform worse than the best 1/T FOMs such as mirror prox with appropriate stepsize tuning)."
REFERENCES,0.33102493074792244,"In the last few years there has been a growing literature on last-iterate convergence in EFGs. There,
the goal is to show that one can converge to an equilibrium without averaging the iterates generated
by a FOM or CFR-based method. It has long been known that with the Euclidean regularizer,
it is possible to converge at a linear rate in last iterate with e.g., the extragradient method (a.k.a.
mirror prox with the Euclidean regularizer) on BSPPs with polyhedral decision sets, as they are in
EFGs [21, 43, 45]. More recently, it has been shown that a linear rate can be achieved with certain
dilated regularizers [27], with the kernelization approach of Farina et al. [17], and in a regularized
CFR setup [31]. At this stage, however, these last-iterate results are of greater theoretical significance
than practical significance, because the linear rate often does not occur until after quite many iterations,
and typically the methods do not match the performance of ergodic methods at reasonable time scales.
For this reason, we do not compare to last-iterate algorithms in our experiments."
REFERENCES,0.33240997229916897,"B
Treeplex Example"
REFERENCES,0.3337950138504155,"As an example, consider the treeplex of Kuhn poker [26] adapted from [13] shown in Figure 5. Kuhn
poker is a game played with a three card deck: jack, queen, and king. In this case, for example, we
have JX = {0, 1, 2, 3, 4, 5, 6}, p0 = ∅, p1 = p2 = p3 = (0, start), p4 = (1, check), p5 = (2, check),
p6 = (3, check), A0 = {start}, A1 = A2 = A3 = {check, raise}, A4 = A5 = A6 = {fold, call},
C(0,start) = {1, 2, 3}, C(1,raise) = C(1,raise) = C(2,raise) = C(3,raise) = C(4,fold) = C(5,fold) = C(6,fold) =
C(4,call) = C(5,call) = C(6,call) = ∅, ↓0 = JX , ↓1 = {1, 4}, ↓2 = {2, 5}, ↓3 = {3, 6}, ↓4 = {4},
↓5 = {5}, ↓6 = {6}. In this case, ∅represents the empty sequence."
REFERENCES,0.33518005540166207,"C
Proofs"
REFERENCES,0.3365650969529086,"C.1
Proof of Proposition 3.1"
REFERENCES,0.3379501385041551,"Proof. The claim that xk ∈X, yk ∈Y is immediate from the algorithm description, as both are
solutions to constrained optimization problems with these same constraints."
REFERENCES,0.33933518005540164,"Figure 5: The sequential decision problem for the first player in Kuhn poker. ⬣represents the end of
the decision process and ⊗represents an observation (which may lead to multiple decision points).
Adapted from [13]."
REFERENCES,0.3407202216066482,"For the remaining claim, observe first that
ηk ⟨Mxk, v⟩= ηk ⟨gk, v⟩−Dψ(v, yk−1) + Dψ(v, yk−1) + ηk ⟨Mxk −gk, v⟩."
REFERENCES,0.34210526315789475,Recall from Algorithm 1 that
REFERENCES,0.34349030470914127,"yk = argmax
v∈Y"
REFERENCES,0.3448753462603878,"n
ηk ⟨gk, v⟩−Dψ(v, yk−1)
o
."
REFERENCES,0.3462603878116344,"Define the function under the max defining yk by Ψk. Then as Ψk(·) is the sum of −ψ(·) and linear
terms, we have DΨk(·, y) = −Dψ(·, y), for any y. Further, as Ψk is maximized by yk, we have
Ψk(v) ≤Ψk(yk) −Dψ(v, yk). Thus, it follows that"
REFERENCES,0.3476454293628809,"ηk ⟨Mxk, v⟩≤ηk ⟨gk, yk⟩−Dψ(yk, yk−1) + ⟨Mxk −gk, v⟩
−Dψ(v, yk) + Dψ(v, yk−1).
(C.1)"
REFERENCES,0.3490304709141274,"Using the same ideas for the primal side, we have"
REFERENCES,0.35041551246537395,"ηk ⟨Mu, yk⟩≥ηk ⟨xk, hk⟩+ Dϕ(xk, xk−1) + ηk

u, M⊤yk −hk"
REFERENCES,0.3518005540166205,"+ Dϕ(u, xk) −Dϕ(u, xk−1)
(C.2)"
REFERENCES,0.35318559556786705,"Combining (C.1) and (C.2),"
REFERENCES,0.3545706371191136,"ηkGapu,v(xk, yk) ≤ηk ⟨Mxk −gk, v −yk⟩−ηk

u −xk, M⊤yk −hk"
REFERENCES,0.3559556786703601,"−Dψ(yk, yk−1) −Dϕ(xk, xk−1)
−Dϕ(u, xk) + Dϕ(u, xk−1) −Dψ(v, yk) + Dψ(v, yk−1).
To complete the proof, it remains to combine the definition of Ek from the proposition statement with
the last inequality."
REFERENCES,0.3573407202216066,"C.2
Proof of Theorem 3.2"
REFERENCES,0.3587257617728532,"For notational convenience, in this proof we define vectors ˆxk and ˆyk by ˆxj
k =
xj
k
x
pj
k xpj
k−1 for"
REFERENCES,0.3601108033240997,"j ∈JX and ˆyj
k =
yj
k
y
pj
k ypj
k−1 for j ∈JY, so that exk = xk −ˆxk −ηk−1"
REFERENCES,0.36149584487534625,ηk (xk−1 −ˆxk−1) and
REFERENCES,0.3628808864265928,eyk = yk −ˆyk −ηk−1
REFERENCES,0.36426592797783935,ηk (yk−1 −ˆyk−1).
REFERENCES,0.3656509695290859,"To prove the theorem, we first prove the following auxiliary lemma which bounds the inner product
terms appearing in the error terms Ek.
Lemma C.1. In all iterations k of Algorithm 1, for any (u, v) ∈X × Y and any α, β > 0,"
REFERENCES,0.3670360110803324,"ηk ⟨Mxk −gk, v −yk⟩≤ηk ⟨My(xk −ˆxk), v −yk⟩−ηk−1 ⟨My(xk−1 −ˆxk−1), v −yk−1⟩
+ ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk−1⟩"
REFERENCES,0.3684210526315789,"+ ηk−1
∥Mx∥∗+ ∥My∥∗ 2"
REFERENCES,0.3698060941828255,"
α∥xk−1 −xk−2∥2 + 1"
REFERENCES,0.37119113573407203,"α∥yk −yk−1∥2
. and"
REFERENCES,0.37257617728531855,"−ηk

u −xk, M⊤yk −hk

≤−ηk

M⊤
x (yk −ˆyk), u −xk

+ ηk−1

M⊤
x (yk−1 −ˆyk−1), u −xk−1"
REFERENCES,0.3739612188365651,"−ηk

M⊤
y (yk −yk−1), u −xk

+ ηk−1

M⊤
y (yk−1 −yk−2), u −xk−1"
REFERENCES,0.37534626038781166,"+ ηk−1
∥M⊤
x + M⊤
y ∥∗
2"
REFERENCES,0.3767313019390582,"
β∥xk−1 −xk∥2 + 1"
REFERENCES,0.3781163434903047,"β ∥yk−1 −yk−2∥2
."
REFERENCES,0.37950138504155123,"Proof. Observe first that, by Algorithm 1,"
REFERENCES,0.3808864265927978,"M(t,:)xk −g(t)
k
= M(t,1:t)
xk −ˆxk −ηk−1"
REFERENCES,0.38227146814404434,"ηk
(xk−1 −ˆxk−1)
(1:t)"
REFERENCES,0.38365650969529086,"+ M(t,t+1:s)
xk −xk−1 −ηk−1"
REFERENCES,0.3850415512465374,"ηk
(xk−1 −xk−2)
(t+1:s)
.
(C.3)"
REFERENCES,0.3864265927977839,"Additionally, by definition (see Eq. (3.1)), Ps
t=1 Mt,1:t = M −Mx = My. Hence, ηk s
X t=1"
REFERENCES,0.3878116343490305,"M(t,1:t)
xk −ˆxk −ηk−1"
REFERENCES,0.389196675900277,"ηk
(xk−1 −ˆxk−1)
(1:t)
, v(t) −y(t)
k  = ηk s
X t=1"
REFERENCES,0.39058171745152354,"Mt,1:t

xk −ˆxk −ηk−1"
REFERENCES,0.39196675900277006,"ηk
(xk−1 −ˆxk−1)

, v −yk  = ηk"
REFERENCES,0.39335180055401664,"My

xk −ˆxk −ηk−1"
REFERENCES,0.39473684210526316,"ηk
(xk−1 −ˆxk−1)

, v −yk "
REFERENCES,0.3961218836565097,"= ηk ⟨My(xk −ˆxk), v −yk⟩−ηk−1 ⟨My(xk−1 −ˆxk−1), v −yk−1⟩
−ηk−1 ⟨My(xk−1 −ˆxk−1), yk−1 −yk⟩.
(C.4)"
REFERENCES,0.3975069252077562,"The
first
two
terms
in
(C.4)
telescope,
so
we
focus
on
bounding
−ηk−1 ⟨My(xk−1 −ˆxk−1), yk−1 −yk⟩. By definition of ˆxk, for all j ∈JX ,"
REFERENCES,0.3988919667590028,"xj
k−1 −ˆxj
k−1 = xj
k−1
xpj
k−1"
REFERENCES,0.4002770083102493," 
xpj
k−1 −xpj
k−2

."
REFERENCES,0.40166204986149584,"By the definition of a treeplex, each vector
xj
k−1
x
pj
k−1 belongs to a probability simplex of the appropriate"
REFERENCES,0.40304709141274236,size. This further implies that
REFERENCES,0.40443213296398894,∥xk−1 −ˆxk−1∥=
REFERENCES,0.40581717451523547,"xj
k−1
xpj
k−1"
REFERENCES,0.407202216066482," 
xpj
k−1 −xpj
k−2
 j∈JX "
REFERENCES,0.4085872576177285,"≤∥[xpj
k−1 −xpj
k−2]j∈JX ∥"
REFERENCES,0.4099722991689751,"≤∥xk−1 −xk−2∥,
(C.5)"
REFERENCES,0.4113573407202216,"where the notation [aj]j∈JX is used to denote the vector with entries aj, for j ∈JX . The first"
REFERENCES,0.41274238227146814,"inequality in (C.5) holds for any ℓp norm (p ≥1), by its definition and
D
1, xj
k−1
E
= 1, ∀j. Thus,
applying the definitions of the norms from the preliminaries,"
REFERENCES,0.41412742382271467,"−⟨My(xk−1 −ˆxk−1), yk−1 −yk⟩≤∥My(xk−1 −ˆxk−1)∥∗∥yk−1 −yk∥
≤∥My∥∗∥xk−1 −xk−2∥∥yk−1 −yk∥"
REFERENCES,0.4155124653739612,≤∥My∥∗ 2
REFERENCES,0.4168975069252078,"
α∥xk−1 −xk−2∥2 + 1"
REFERENCES,0.4182825484764543,"α∥yk −yk−1∥2
, (C.6)"
REFERENCES,0.4196675900277008,where the last line is by Young’s inequality and holds for any α > 0.
REFERENCES,0.42105263157894735,"On the other hand, recalling that Mx = Ps−1
t=1 Mt,t+1:s, we also have ηk s
X t=1"
REFERENCES,0.4224376731301939,"M(t,t+1:s)
xk −xk−1 −ηk−1"
REFERENCES,0.42382271468144045,"ηk
(xk−1 −xk−2)
(t+1:s)
, v(t) −y(t)
k  = ηk s
X t=1"
REFERENCES,0.425207756232687,"Mt,t+1:s

xk −xk−1 −ηk−1"
REFERENCES,0.4265927977839335,"ηk
(xk−1 −xk−2)

, v −yk  = ηk"
REFERENCES,0.4279778393351801,"Mx

xk −xk−1 −ηk−1"
REFERENCES,0.4293628808864266,"ηk
(xk−1 −xk−2)

, v −yk "
REFERENCES,0.4307479224376731,"= ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk⟩
= ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk−1⟩
+ ηk−1 ⟨Mx(xk−1 −xk−2), yk −yk−1⟩.
(C.7)
Observe that in (C.7) the first two terms telescope and thus we only need to focus on bounding the last
term. Applying the definitions of dual and matrix norms from Section 2 and using Young’s inequality,
we have that for any α > 0,
⟨Mx(xk−1 −xk−2), yk −yk−1⟩≤∥Mx(xk−1 −xk−2)∥∗∥yk −yk−1∥
≤∥Mx∥∗∥xk−1 −xk−2∥∥yk −yk−1"
REFERENCES,0.43213296398891965,≤∥Mx∥∗ 2
REFERENCES,0.43351800554016623,"
α∥xk−1 −xk−2∥2 + 1"
REFERENCES,0.43490304709141275,"α∥yk −yk−1∥2
.
(C.8)"
REFERENCES,0.4362880886426593,"Hence, combining (C.3)–(C.8), we can conclude that
ηk ⟨Mxk −gk, v −yk⟩≤ηk ⟨My(xk −ˆxk), v −yk⟩−ηk−1 ⟨My(xk−1 −ˆxk−1), v −yk−1⟩
+ ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk−1⟩"
REFERENCES,0.4376731301939058,"+ ηk−1
∥Mx∥∗+ ∥My∥∗ 2"
REFERENCES,0.4390581717451524,"
α∥xk−1 −xk−2∥2 + 1"
REFERENCES,0.4404432132963989,"α∥yk −yk−1∥2
,"
REFERENCES,0.44182825484764543,completing the proof of the first claim.
REFERENCES,0.44321329639889195,"Similarly, we observe from Algorithm 1 that
 
M(:,t)⊤yk −h(t)
k
=
 
M(1:t−1,t)⊤
yk −ˆyk + ηk−1"
REFERENCES,0.4445983379501385,"ηk
(yk−1 −ˆyk−1)
(1:t−1)"
REFERENCES,0.44598337950138506,"+
 
M(t:s,t)⊤
yk −yk−1 + ηk−1"
REFERENCES,0.4473684210526316,"ηk
(yk−1 −yk−2)
(t:s)
."
REFERENCES,0.4487534626038781,"Observing that Ps
t=1 M1:t−1,t = Mx and Ps
t=1 Mt:s,t = My, using the same sequence of argu-
ments as for bounding (C.3), we can conclude that for any β > 0,
−ηk

u −xk, M⊤yk −hk"
REFERENCES,0.45013850415512463,"≤−ηk

M⊤
x (yk −ˆyk), u −xk

+ ηk−1

M⊤
x (yk−1 −ˆyk−1), u −xk−1"
REFERENCES,0.4515235457063712,"−ηk

M⊤
y (yk −yk−1), u −xk

+ ηk−1

M⊤
y (yk−1 −yk−2), u −xk−1"
REFERENCES,0.45290858725761773,"+ ηk−1
∥M⊤
x + M⊤
y ∥∗
2"
REFERENCES,0.45429362880886426,"
β∥xk−1 −xk∥2 + 1"
REFERENCES,0.4556786703601108,"β ∥yk−1 −yk−2∥2
,"
REFERENCES,0.45706371191135736,completing the proof.
REFERENCES,0.4584487534626039,"Proof Theorem 3.2. Recalling the definition of Ek, by Lemma C.1,
Ek ≤ηk ⟨My(xk −ˆxk), v −yk⟩−ηk−1 ⟨My(xk−1 −ˆxk−), v −yk−1⟩
+ ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk−1⟩"
REFERENCES,0.4598337950138504,"+ ηk−1
∥Mx∥∗+ ∥My∥∗ 2"
REFERENCES,0.46121883656509693,"
α∥xk−1 −xk−2∥2 + 1"
REFERENCES,0.4626038781163435,α∥yk −yk−1∥2
REFERENCES,0.46398891966759004,"−ηk

M⊤
x (yk −ˆyk), u −xk

+ ηk−1

M⊤
x (yk−1 −ˆyk−1), u −xk−1"
REFERENCES,0.46537396121883656,"−ηk

M⊤
y (yk −yk−1), u −xk

+ ηk−1

M⊤
y (yk−1 −yk−2), u −xk−1"
REFERENCES,0.4667590027700831,"+ ηk−1
∥M⊤
x ∥∗+ ∥M⊤
y ∥∗
2"
REFERENCES,0.46814404432132967,"
β∥xk−1 −xk∥2 + 1"
REFERENCES,0.4695290858725762,β ∥yk−1 −yk−2∥2
REFERENCES,0.4709141274238227,"−Dψ(yk, yk−1) −Dϕ(xk, xk−1). (C.9)"
REFERENCES,0.47229916897506924,"Recalling that ψ is cψ-strongly convex, ϕ is cϕ-strongly convex, setting α = β =
q cϕ"
REFERENCES,0.47368421052631576,"cψ , and using"
REFERENCES,0.47506925207756234,"that ηk−1 ≤
√cϕcψ
∥Mx∥∗+∥My∥∗+∥M⊤
x ∥∗+∥M⊤
y ∥∗=
√cϕcψ
µx+µy , (C.9) simplifies to"
REFERENCES,0.47645429362880887,"Ek ≤ηk ⟨My(xk −ˆxk), v −yk⟩−ηk−1 ⟨My(xk−1 −ˆxk−), v −yk−1⟩
+ ηk ⟨Mx(xk −xk−1), v −yk⟩−ηk−1 ⟨Mx(xk−1 −xk−2), v −yk−1⟩"
REFERENCES,0.4778393351800554,"−ηk

M⊤
x (yk −ˆyk), u −xk

+ ηk−1

M⊤
x (yk−1 −ˆyk−1), u −xk−1"
REFERENCES,0.4792243767313019,"−ηk

M⊤
y (yk −yk−1), u −xk

+ ηk−1

M⊤
y (yk−1 −yk−2), u −xk−1"
REFERENCES,0.4806094182825485,"+
cψµy
2(µx + µy)
 
∥yk−1 −yk−2∥2 −∥yk −yk−1∥2"
REFERENCES,0.481994459833795,"+
cϕµx
2(µx + µy)
 
∥xk−1 −xk−2∥2 −∥xk −xk−1∥2
."
REFERENCES,0.48337950138504154,(C.10)
REFERENCES,0.48476454293628807,"Telescoping (C.10) and recalling that η0 = 0, we now have K
X"
REFERENCES,0.48614958448753465,"k=1
Ek ≤ηK ⟨Mx(xK −xK−1), v −yK⟩−ηK

M⊤
y (yK −yK−1), u −xK"
REFERENCES,0.48753462603878117,"+ ηK ⟨My(xK −ˆxK), v −yK⟩−ηK

M⊤
x (yK −ˆyK), u −xK"
REFERENCES,0.4889196675900277,"−
cψµy
2(µx + µy)∥yK −yK−1∥2"
REFERENCES,0.4903047091412742,"−
cϕµx
2(µx + µy)∥xK −xK−1∥2."
REFERENCES,0.4916897506925208,(C.11)
REFERENCES,0.4930747922437673,"Observe that Gapu,v(·, ·) is linear in both its arguments.
Hence, Gapu,v(¯xK, ¯yK)
=
1
HK
PK
k=1 ηkGapu,v(xk, yk). Applying Proposition 3.1 and combining with (C.11), we now have"
REFERENCES,0.49445983379501385,"HKGapu,v(¯xk, ¯yk) ≤Dϕ(u, x0) + Dψ(v, y0)"
REFERENCES,0.49584487534626037,"+ ηK ⟨Mx(xK −xK−1), v −yK⟩−ηK

M⊤
y (yK −yK−1), u −xK"
REFERENCES,0.49722991689750695,"+ ηK ⟨My(xK −ˆxK), v −yK⟩−ηK

M⊤
x (yK −ˆyK), u −xK"
REFERENCES,0.4986149584487535,"−
cψµy
2(µx + µy)∥yK −yK−1∥2 −
cϕµx
2(µx + µy)∥xK −xK−1∥2"
REFERENCES,0.5,"−Dϕ(u, xK) −Dψ(v, yK).
(C.12)
To complete bounding the gap, it remains to argue that the right-hand side of (C.12) is bounded
by Dϕ(u, x0) + Dψ(v, y0) −
µx
µx+µy Dϕ(u, xK) −
µy
µx+µy Dψ(v, yK). This is done using the same
sequence of arguments as in bounding Ek and is omitted."
REFERENCES,0.5013850415512465,"Let (x∗, y∗) ∈X × Y be any primal-dual solution to (PD). Then Gap(x∗,y∗)(¯xK, ¯yK) ≥0 and we
can conclude that
µx
µx + µy
Dϕ(x∗, xK) +
µy
µx + µy
Dψ(y∗, yK) ≤Dϕ(x∗, x0) + Dψ(y∗, y0)."
REFERENCES,0.502770083102493,"Further, using that Dϕ(·, ·) ≥0, Dψ(·, ·) ≥0, we can also conclude that"
REFERENCES,0.5041551246537396,"sup
u∈X,v∈Y
Gapu,v(¯xK, ¯yK) =
sup
u∈X,v∈Y
{⟨M¯xK, v⟩−⟨Mu, ¯yK⟩}"
REFERENCES,0.5055401662049861,"≤supu∈X,v∈Y{Dϕ(u, x0) + Dψ(v, y0)} HK
."
REFERENCES,0.5069252077562327,"Finally, setting ηk =
√cϕcψ
µx+µy for all k ≥1 immediately leads to the conclusion that HK = K
√cϕcψ
µx+µy ,"
REFERENCES,0.5083102493074793,"as HK = PK
k=1 ηk, by definition. The last bound is by setting
supu∈X,v∈Y{Dϕ(u,x0)+Dψ(v,y0)}"
REFERENCES,0.5096952908587258,"HK
≤ϵ,
and solving for K."
REFERENCES,0.5110803324099723,"D
Algorithm Implementation Details"
REFERENCES,0.5124653739612188,"In Algorithm 2, we present an implementation-specific version of ECyclicPDA, in order to make
it clear that our algorithm can be implemented without any extra computation compared to the"
REFERENCES,0.5138504155124654,"computation needed for gradient and prox updates in MP. Note that MP performs two gradient
computations and two prox computations per player, due to how it achieves “extrapolation”; we want
to argue that we perform an equivalent number operations as needed for a single gradient computation
and prox computation per player. Note that the overall complexity of first-order methods when
applied to EFGs is dominated by the gradient and prox update computations; this is why we compare
our algorithm to MP on this basis. The key differences from Algorithm 1 are that we explicitly use
ˆxk and ˆyk to represent the behavioral strategy that is computed via the partial prox updates (which
are then scaled at the end of a full iteration of our method to xk and yk), and that we use ˆhj
k and ˆgj
k
to accumulate gradient contributions from decision points that occur underneath j, to make the partial
prox update explicit."
REFERENCES,0.5152354570637119,"In Lines 8 and 13, we are only dealing with the columns and rows, respectively, of the payoff matrix
that correspond to the current block number t, which means that as t ranges from 1 to s, for the
computation of the gradient, we will only consider each column and row, respectively, once, as would
have to be done in a full gradient computation for MP."
REFERENCES,0.5166204986149584,"The more difficult aspect of the implementation is ensuring that we do the same number of operations
for the prox computation in ECyclicPDA as an analogous single prox computation in MP. We achieve
this by applying the updates in Proposition 2.2 only for the decision points in the current block, in
Lines 9 to 12 for x and 14 to 17 for y."
REFERENCES,0.518005540166205,"We focus on the updates for x; the argument is analogous for y. When applying this local prox
update for decision point j ∈JX
(t), we have already correctly computed ˆhj, the contributions to
the gradient for the local prox update that originate from the children of j, again because the blocks
represent the treeplex ordering; in particular, whenever we have encountered a child decision point
of j in the past, we accumulate its contribution to the gradient for its parent at ˆhpj. Since the prox
update decomposition from Proposition 2.2 has to be applied for every single decision point in JX in
a full prox update (as done in MP), we again do not incur any dependence on the number of blocks."
REFERENCES,0.5193905817174516,"E
Description of EFG Benchmarks"
REFERENCES,0.5207756232686981,"We provide game descriptions of the games we run our experiments on below. Our game descriptions
are adapted from Farina et al. [15]. In Table 2, we provide the number of sequences for player x (n),
the number of sequences for player y (m), and the number of leaves in the game (NNZ(M))."
REFERENCES,0.5221606648199446,"Table 2: Number of sequences for both players and number of leaves for each game. These correspond
to the dimensions n and m of M, and the number of nonzero entries of M, respectively."
REFERENCES,0.5235457063711911,"Game
Num. of x sequences
Num. of y sequences
Num. of leaves"
REFERENCES,0.5249307479224377,"Goofspiel (4 ranks)
21329
21329
13824"
REFERENCES,0.5263157894736842,"Liar’s Dice
24571
24571
147420"
REFERENCES,0.5277008310249307,"Leduc (13 ranks)
6007
6007
98956"
REFERENCES,0.5290858725761773,"Battleship
73130
253940
552132"
REFERENCES,0.5304709141274239,"E.1
Goofspiel (4 ranks)"
REFERENCES,0.5318559556786704,"Goofspiel is a card-based game that is a standard benchmark in the EFG-solving community [36].
In the version that we test on, there are 4 unique cards (ranks), and there are 3 copies of each rank,
divided into 3 separate decks. Each player gets a deck, and the third deck is known as the prize deck.
Cards are randomly drawn from the prize deck, and each player submits a bid for the drawn card by
submitting a card from one of their respective decks, the value of which represents their bid. Whoever
submits the higher bid wins the card from the prize deck. Once all the cards from the prize deck have
been drawn, bid on, and won by one of the players, the game terminates, and the payoffs for players
are given by the sum of the prize cards they won."
REFERENCES,0.5332409972299169,Algorithm 2 Extrapolated Cyclic Primal-Dual EFG Solver (Implementation Version)
REFERENCES,0.5346260387811634,"1: Input: M, m, n
2: Initialization: x0 ∈X, y0 ∈Y, η0 = H0 = 0, η =
√cϕcψ
µx+µy , ¯x0 = x0, ¯y0 = y0, g0 = 0, h0 = 0
3: for k = 1 : K do
4:
Choose ηk ≤η, Hk = Hk−1 + ηk
5:
gk = 0, hk = 0, ˆgk = 0, ˆhk = 0
6:
exk = xk−1 + ηk−1"
REFERENCES,0.53601108033241,"ηk (xk−1 −xk−2), eyk = yk−1 + ηk−1"
REFERENCES,0.5373961218836565,"ηk (yk−1 −yk−2)
7:
for t = 1 : s do
8:
h(t)
k
= (M(:,t))⊤eyk
9:
for j ∈JX
(t) do"
REFERENCES,0.538781163434903,"10:
ˆxj
k = argminbj∈∆nj
nD
ˆhj
k + hj
k, bjE
+ Dϕj

bj, ˆxj
k−1
o"
REFERENCES,0.5401662049861495,"11:
(j′, a) = pj"
REFERENCES,0.5415512465373962,"12:
ˆhpj
k +=
h
ϕ↓j∗
−h↓j
k + ∇ϕ↓j 
x↓j
k−1

−ϕj 
ˆxj
k−1

+
D
∇ϕj 
ˆxj
k−1

, ˆxj
k−1
Ei"
REFERENCES,0.5429362880886427,"13:
g(t)
k
= M(t,:)exk
14:
for j ∈JY
(t) do"
REFERENCES,0.5443213296398892,"15:
ˆyj
k = argminbj∈∆nj
nD
ˆgj
k + gj
k, bjE
+ Dψj

bj, ˆyj
k−1
o"
REFERENCES,0.5457063711911357,"16:
(j′, a) = pj"
REFERENCES,0.5470914127423823,"17:
ˆhpj
k +=
h
ψ↓j∗
−g↓j
k + ∇ψ↓j 
y↓j
k−1

−ψj 
ˆyj
k−1

+
D
∇ψj 
ˆyj
k−1

, ˆyj
k−1
Ei"
REFERENCES,0.5484764542936288,"18:
for j ∈JX
(t) do"
REFERENCES,0.5498614958448753,"19:
exj
k =
h
ˆxj
kxpj
k−1 + ηk−1 ηk"
REFERENCES,0.5512465373961218,"
xj
k−1 −ˆxj
k−1xpj
k−2
i"
REFERENCES,0.5526315789473685,"20:
for j ∈JY
(t) do"
REFERENCES,0.554016620498615,"21:
eyj
k =
h
ˆyj
kypj
k−1 + ηk−1 ηk"
REFERENCES,0.5554016620498615,"
yj
k−1 −ˆyj
k−1ypj
k−2
i"
REFERENCES,0.556786703601108,"22:
for j ∈JX do
23:
xj
k = xpj
k · ˆxj
k
24:
for j ∈JY do
25:
yj
k = ypj
k · ˆyj
k
26:
¯xk = Hk−ηk"
REFERENCES,0.5581717451523546,"Hk
¯xk−1 + ηk"
REFERENCES,0.5595567867036011,"Hk xk, ¯yk = Hk−ηk"
REFERENCES,0.5609418282548476,"Hk
¯yk−1 + ηk Hk yk"
REFERENCES,0.5623268698060941,"27: Return: ¯xK, ¯yK"
REFERENCES,0.5637119113573407,"E.2
Liar’s Dice"
REFERENCES,0.5650969529085873,"Liar’s Dice is another standard benchmark in the EFG-solving community [29]. In the version that
we test on, each player rolls an unbiased six-sided die, and they take turns either calling higher bids
or challenging the other player. A bid consists of a combination of a value v between one and six,
and a number of dice between one and two, n, representing the number of dice between the two
players that has v pips showing. A higher bid involves either increasing n holding v fixed, increasing
v holding n fixed, or both. When a player is challenged (or the highest possible bid of “two dice
each showing six pips” is called), the dice are revealed, and whoever is correct wins 1 (either the
challenger if the bid is not true, or the player who last called a bid, if the bid is true), and the other
player receives a utility of -1."
REFERENCES,0.5664819944598338,"E.3
Leduc (13 ranks)"
REFERENCES,0.5678670360110804,"Leduc is yet another standard benchmark in the EFG-solving community [40] and is a simplified
version of Texas Hold’Em. In the version we test on, there are 13 unique cards (ranks), and there are
2 copies of each rank (half the size of a standard 52 card deck). There are two rounds of betting that
take place, and before the first round each player places an ante of 1 into the pot, and is dealt a single
pocket (private) card. In addition, two cards are placed face down, and these are community cards
that will be used to form hands. The two hands that can be formed with the community cards are pair,
and highest card."
REFERENCES,0.5692520775623269,"During the first round of betting, player 1 acts first. There is a max of two raises allowed in each round
of betting. Each player can either check, raise, or fold. If a player folds, the other player immediately
wins the pots and the game terminates. If a player checks, the other player has an opportunity to
raise if they have not already previously checked or raised, and if they previously checked, the game
moves on to the next round. If a player raises, the other player has an opportunity to raise if they
have not already previously raised. The game then moves on the second round, during which one
of the community cards is placed face up, and then similar betting dynamics as the first round take
place. After the second round terminates, there is a showdown, and whoever can form the better hand
(higher ranked pair, or highest card) with the community cards takes the pot."
REFERENCES,0.5706371191135734,"E.4
Battleship"
REFERENCES,0.5720221606648199,"This is an instantiation of the classic board game, Battleship, in which players take turns shooting
at the opposing player’s ships. Before the game begins, the players place two ships of length 2 and
value 4, on a grid of size 2 by 3. The ships need to be placed in a way so that the ships take up exactly
four spaces within the grid (they do not overlap with each other, and are contained entirely in the
grid). Each player gets three shots, and players take turns firing at cells of their opponent’s grid. A
ship is sunk when the two cells it has been placed on have been shot at. At the end of the game, the
utility for a player is the difference between the cumulative value of the opponent’s sunk ships and
the cumulative value of the player’s sunk ships."
REFERENCES,0.5734072022160664,"F
Block Construction Strategies"
REFERENCES,0.574792243767313,"As discussed in the main paper, the postorder block construction strategy can be viewed as traversing
the decision points of the treeplex in postorder, treating decision points with the same parent sequence
as the same node, and then greedily putting decision points in the same block until we reach a decision
point that has a child decision point in the current block (at which point we start a new block). We
make this postorder traversal and greedy block construction explicit in Algorithm 3."
REFERENCES,0.5761772853185596,"In Algorithm 4 we provide pseudocode for constructing blocks using the children block construction
strategy. As discussed in the main paper, the children block construction strategy corresponds to
placing decision points with the same parent decision point (same decision point at which their parent
sequences start at) in the same block. In our implementation, instead of doing a bottom-up traversal,
we do a top down implementation, and at the end, reverse the order of the blocks (this allows us to
respect the treeplex ordering)."
REFERENCES,0.5775623268698061,"In both Algorithm 3 and Algorithm 4, ∅represents the empty sequence."
REFERENCES,0.5789473684210527,Algorithm 3 Postorder Block Construction
REFERENCES,0.5803324099722992,"1: procedure POSTORDERHELPER(j, a)
2:
accumulator = []
3:
for j′ ∈Cj,a do
4:
for a′ ∈Aj′ do
5:
accumulator.insert(postorder(j′, a′))
6:
for j′ ∈Cj,a do
7:
accumulator.insert(j′)
8:
Return: accumulator
9: procedure POSTORDERBLOCKS(J )
10:
ordered = POSTORDERHELPER(∅)
11:
blocks = []
12:
current_block = []
13:
for j ∈ordered do
14:
if ∃j′ ∈current_block s.t. j′ is a child decision point of j then
15:
blocks.insert(current_block)
16:
current_block = [j]
17:
else
18:
current_block.insert(j)
19:
return: blocks"
REFERENCES,0.5817174515235457,Algorithm 4 Children Block Construction
REFERENCES,0.5831024930747922,"1: procedure CHILDRENBLOCKS(J )
2:
blocks = []
3:
explore = C∅
4:
for j ∈explore do
5:
current_block = []
6:
for a ∈Aj do
7:
for j′ ∈Cj,a do
8:
current_block.insert(j’)
9:
explore.insert(j’)
blocks.insert(current_block)
10:
return: blocks.reverse()"
REFERENCES,0.5844875346260388,"We can now illustrate each of the block construction strategies on the treeplex for player 1 in Kuhn that
was presented in Appendix B. If we use single block, then we have JX
(1) = JX = {0, 1, 2, 3, 4, 5, 6}.
If we use infosets, then we have JX
(i) = {7 −i} for i ∈{1, 2, 3, 4, 5, 6, 7} (we have to subtract in
order to label the infosets in a manner that respects the treeplex ordering). If we use children, then
we have JX
(1) = {4}, JX
(2) = {5}, JX
(3) = {6}, JX
(4) = {1, 2, 3}, and JX
(5) = {0}. If we use
postorder, then we have JX
(1) = {4, 5, 6}, JX
(2) = {1, 2, 3}, and JX
(3) = {0}."
REFERENCES,0.5858725761772853,"Note that in the implementation of our algorithm, it is not actually important that the number of
blocks for both players are the same; if one player has more blocks than the other, for iterations of
our algorithm that correspond to block numbers that do not exist for the other player, we just do not
do anything for the other player. Nevertheless, the output of the algorithm does not change if we
combine all the blocks for the player with more blocks after the minimum number of blocks between
the two players is exceeded, into one block. For example, if player 1 has s1 blocks, and player 2
has s2 blocks, with s1 < s2, we can actually combine blocks s1 + 1, . . . , s2 all into the same block
for player 2, and this would not change the execution of the algorithm. This is what we do in our
implementation."
REFERENCES,0.5872576177285319,"Additionally, given a choice of a partition of decision points into blocks, there may exist many
permutations of decision points within the blocks which satisfy the treeplex ordering of the decision
points. Unless the game that is being tested upon possesses some structure which leads to a single
canonical ordering of the decision points (which respects the treeplex ordering), an arbitrary decision
needs to be made regarding what order is used."
REFERENCES,0.5886426592797784,"G
Experiments"
REFERENCES,0.590027700831025,"G.1
Additional Experimental Details"
REFERENCES,0.5914127423822715,"Block Construction Strategy Comparison. In this section, we provide additional plots (Figures 6
to 14) comparing different block construction strategies for our algorithm, for specific choices of
regularizer and averaging scheme. Note that for the games for which there is a benefit to using
blocks (Liar’s Dice and Battleship), the benefit is generally apparent across different regularizers and
averaging schemes. Furthermore, when there is not a benefit for a particular regularizer and averaging
scheme, there is no significant cost either (using blocks does not lead to worse performance)."
REFERENCES,0.592797783933518,"Block Construction Strategy Comparison with Restarts. We repeat a similar analysis as above
(comparing the block construction strategies holding a regularizer and averaging scheme fixed) but
this time with the adaptive restarting heuristic applied to our algorithm: the plots can be seen in
(Figures 15 to 23). As mentioned in Section 4, we prepend the algorithm name with “r” to denote the
restarted variant of the algorithm in the plots."
REFERENCES,0.5941828254847645,"As discussed in the main body, the trend of the benefit of using blocks being more pronounced
with restarting (for games for which blocks are beneficial) holds generally even when holding the
regularizer and averaging scheme fixed. This can be seen by comparing each of the restarted block
construction strategy comparison plots with the corresponding non-restarted block construction
strategy comparison plot."
REFERENCES,0.5955678670360111,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.5969529085872576,Duality gap
REFERENCES,0.5983379501385041,Goofspiel (4 ranks)
REFERENCES,0.5997229916897507,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6011080332409973,Liar’s Dice
REFERENCES,0.6024930747922438,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6038781163434903,Leduc (13 ranks)
REFERENCES,0.6052631578947368,"102
104
Gradient computations 10−1 100"
REFERENCES,0.6066481994459834,Battleship
REFERENCES,0.6080332409972299,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.6094182825484764,"Figure 6: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and uniform averaging."
REFERENCES,0.610803324099723,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6121883656509696,Duality gap
REFERENCES,0.6135734072022161,Goofspiel (4 ranks)
REFERENCES,0.6149584487534626,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6163434903047091,Liar’s Dice
REFERENCES,0.6177285318559557,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6191135734072022,Leduc (13 ranks)
REFERENCES,0.6204986149584487,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6218836565096952,Battleship
REFERENCES,0.6232686980609419,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.6246537396121884,"Figure 7: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and uniform averaging."
REFERENCES,0.6260387811634349,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6274238227146814,Duality gap
REFERENCES,0.628808864265928,Goofspiel (4 ranks)
REFERENCES,0.6301939058171745,"102
104
Gradient computations 10−2 100"
REFERENCES,0.631578947368421,Liar’s Dice
REFERENCES,0.6329639889196675,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6343490304709142,Leduc (13 ranks)
REFERENCES,0.6357340720221607,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6371191135734072,Battleship
REFERENCES,0.6385041551246537,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.6398891966759003,"Figure 8: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer and
uniform averaging."
REFERENCES,0.6412742382271468,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6426592797783933,Duality gap
REFERENCES,0.6440443213296398,Goofspiel (4 ranks)
REFERENCES,0.6454293628808865,"102
104
Gradient computations 10−3 100"
REFERENCES,0.646814404432133,Liar’s Dice
REFERENCES,0.6481994459833795,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.649584487534626,Leduc (13 ranks)
REFERENCES,0.6509695290858726,"102
104
Gradient computations 10−1 100"
REFERENCES,0.6523545706371191,Battleship
REFERENCES,0.6537396121883656,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.6551246537396122,"Figure 9: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and linear averaging."
REFERENCES,0.6565096952908587,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6578947368421053,Duality gap
REFERENCES,0.6592797783933518,Goofspiel (4 ranks)
REFERENCES,0.6606648199445984,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6620498614958449,Liar’s Dice
REFERENCES,0.6634349030470914,"102
104
Gradient computations 10−3 10−1"
REFERENCES,0.6648199445983379,Leduc (13 ranks)
REFERENCES,0.6662049861495845,"102
104
Gradient computations 10−3 100"
REFERENCES,0.667590027700831,Battleship
REFERENCES,0.6689750692520776,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.6703601108033241,"Figure 10: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and linear averaging."
REFERENCES,0.6717451523545707,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6731301939058172,Duality gap
REFERENCES,0.6745152354570637,Goofspiel (4 ranks)
REFERENCES,0.6759002770083102,"102
104
Gradient computations 10−2 100"
REFERENCES,0.6772853185595568,Liar’s Dice
REFERENCES,0.6786703601108033,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6800554016620498,Leduc (13 ranks)
REFERENCES,0.6814404432132964,"102
104
Gradient computations 10−3 100"
REFERENCES,0.682825484764543,Battleship
REFERENCES,0.6842105263157895,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.685595567867036,"Figure 11: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer and
linear averaging."
REFERENCES,0.6869806094182825,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6883656509695291,Duality gap
REFERENCES,0.6897506925207756,Goofspiel (4 ranks)
REFERENCES,0.6911357340720221,"102
104
Gradient computations 10−4 10−1"
REFERENCES,0.6925207756232687,Liar’s Dice
REFERENCES,0.6939058171745153,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.6952908587257618,Leduc (13 ranks)
REFERENCES,0.6966759002770083,"102
104
Gradient computations 10−1 100"
REFERENCES,0.6980609418282548,Battleship
REFERENCES,0.6994459833795014,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.7008310249307479,"Figure 12: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and quadratic averaging."
REFERENCES,0.7022160664819944,"102
104
Gradient computations 10−2 100"
REFERENCES,0.703601108033241,Duality gap
REFERENCES,0.7049861495844876,Goofspiel (4 ranks)
REFERENCES,0.7063711911357341,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7077562326869806,Liar’s Dice
REFERENCES,0.7091412742382271,"102
104
Gradient computations 10−3 10−1"
REFERENCES,0.7105263157894737,Leduc (13 ranks)
REFERENCES,0.7119113573407202,"102
104
Gradient computations 10−5 10−2"
REFERENCES,0.7132963988919667,Battleship
REFERENCES,0.7146814404432132,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.7160664819944599,"Figure 13: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and quadratic averaging."
REFERENCES,0.7174515235457064,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7188365650969529,Duality gap
REFERENCES,0.7202216066481995,Goofspiel (4 ranks)
REFERENCES,0.721606648199446,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7229916897506925,Liar’s Dice
REFERENCES,0.724376731301939,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.7257617728531855,Leduc (13 ranks)
REFERENCES,0.7271468144044322,"102
104
Gradient computations 10−4 10−1"
REFERENCES,0.7285318559556787,Battleship
REFERENCES,0.7299168975069252,"ECyclicPDA single block
ECyclicPDA postorder
ECyclicPDA children
ECyclicPDA infosets"
REFERENCES,0.7313019390581718,"Figure 14: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer and
quadratic averaging."
REFERENCES,0.7326869806094183,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.7340720221606648,Duality gap
REFERENCES,0.7354570637119113,Goofspiel (4 ranks)
REFERENCES,0.7368421052631579,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.7382271468144044,Liar’s Dice
REFERENCES,0.739612188365651,"102
104
Gradient computations 10−1 100"
REFERENCES,0.7409972299168975,Leduc (13 ranks)
REFERENCES,0.7423822714681441,"102
104
Gradient computations 10−1 100"
REFERENCES,0.7437673130193906,Battleship
REFERENCES,0.7451523545706371,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.7465373961218836,"Figure 15: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and uniform averaging as well as restarting. We take the best duality gap seen so far so that the plot
is monotonic."
REFERENCES,0.7479224376731302,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7493074792243767,Duality gap
REFERENCES,0.7506925207756233,Goofspiel (4 ranks)
REFERENCES,0.7520775623268698,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7534626038781164,Liar’s Dice
REFERENCES,0.7548476454293629,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7562326869806094,Leduc (13 ranks)
REFERENCES,0.7576177285318559,"102
104
Gradient computations 10−9 10−3"
REFERENCES,0.7590027700831025,Battleship
REFERENCES,0.760387811634349,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.7617728531855956,"Figure 16: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and uniform averaging as well as restarting. We take the best duality gap seen so far so
that the plot is monotonic."
REFERENCES,0.7631578947368421,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.7645429362880887,Duality gap
REFERENCES,0.7659279778393352,Goofspiel (4 ranks)
REFERENCES,0.7673130193905817,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7686980609418282,Liar’s Dice
REFERENCES,0.7700831024930748,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.7714681440443213,Leduc (13 ranks)
REFERENCES,0.7728531855955678,"102
104
Gradient computations 10−3 100"
REFERENCES,0.7742382271468145,Battleship
REFERENCES,0.775623268698061,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.7770083102493075,"Figure 17: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer and
uniform averaging as well as restarting. We take the best duality gap seen so far so that the plot is
monotonic."
REFERENCES,0.778393351800554,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.7797783933518005,Duality gap
REFERENCES,0.7811634349030471,Goofspiel (4 ranks)
REFERENCES,0.7825484764542936,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.7839335180055401,Liar’s Dice
REFERENCES,0.7853185595567868,"102
104
Gradient computations 10−1 100"
REFERENCES,0.7867036011080333,Leduc (13 ranks)
REFERENCES,0.7880886426592798,"102
104
Gradient computations 10−1 100"
REFERENCES,0.7894736842105263,Battleship
REFERENCES,0.7908587257617729,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.7922437673130194,"Figure 18: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and linear averaging as well as restarting. We take the best duality gap seen so far so that the plot is
monotonic."
REFERENCES,0.7936288088642659,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7950138504155124,Duality gap
REFERENCES,0.796398891966759,Goofspiel (4 ranks)
REFERENCES,0.7977839335180056,"102
104
Gradient computations 10−2 100"
REFERENCES,0.7991689750692521,Liar’s Dice
REFERENCES,0.8005540166204986,"102
104
Gradient computations 10−2 100"
REFERENCES,0.8019390581717452,Leduc (13 ranks)
REFERENCES,0.8033240997229917,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.8047091412742382,Battleship
REFERENCES,0.8060941828254847,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.8074792243767313,"Figure 19: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and linear averaging as well as restarting. We take the best duality gap seen so far so that
the plot is monotonic."
REFERENCES,0.8088642659279779,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.8102493074792244,Duality gap
REFERENCES,0.8116343490304709,Goofspiel (4 ranks)
REFERENCES,0.8130193905817175,"102
104
Gradient computations 10−2 100"
REFERENCES,0.814404432132964,Liar’s Dice
REFERENCES,0.8157894736842105,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.817174515235457,Leduc (13 ranks)
REFERENCES,0.8185595567867036,"102
104
Gradient computations 10−3 100"
REFERENCES,0.8199445983379502,Battleship
REFERENCES,0.8213296398891967,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.8227146814404432,"Figure 20: Duality gap as a function of the number of full (or equivalent) gradient computations
for ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer
and linear averaging as well as restarting. We take the best duality gap seen so far so that the plot is
monotonic."
REFERENCES,0.8240997229916898,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.8254847645429363,Duality gap
REFERENCES,0.8268698060941828,Goofspiel (4 ranks)
REFERENCES,0.8282548476454293,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.8296398891966759,Liar’s Dice
REFERENCES,0.8310249307479224,"102
104
Gradient computations 10−1 100"
REFERENCES,0.832409972299169,Leduc (13 ranks)
REFERENCES,0.8337950138504155,"102
104
Gradient computations 10−1 100"
REFERENCES,0.8351800554016621,Battleship
REFERENCES,0.8365650969529086,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.8379501385041551,"Figure 21: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated entropy regularizer
and quadratic averaging as well as restarting. We take the best duality gap seen so far so that the plot
is monotonic."
REFERENCES,0.8393351800554016,"102
104
Gradient computations 10−2 100"
REFERENCES,0.8407202216066482,Duality gap
REFERENCES,0.8421052631578947,Goofspiel (4 ranks)
REFERENCES,0.8434903047091413,"102
104
Gradient computations 10−3 10−1"
REFERENCES,0.8448753462603878,Liar’s Dice
REFERENCES,0.8462603878116344,"102
104
Gradient computations 10−2 100"
REFERENCES,0.8476454293628809,Leduc (13 ranks)
REFERENCES,0.8490304709141274,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.850415512465374,Battleship
REFERENCES,0.8518005540166205,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.853185595567867,"Figure 22: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilatable global entropy
regularizer and quadratic averaging as well as restarting. We take the best duality gap seen so far so
that the plot is monotonic."
REFERENCES,0.8545706371191135,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.8559556786703602,Duality gap
REFERENCES,0.8573407202216067,Goofspiel (4 ranks)
REFERENCES,0.8587257617728532,"102
104
Gradient computations 10−2 100"
REFERENCES,0.8601108033240997,Liar’s Dice
REFERENCES,0.8614958448753463,"102
104
Gradient computations 10−2 10−1 100"
REFERENCES,0.8628808864265928,Leduc (13 ranks)
REFERENCES,0.8642659279778393,"102
104
Gradient computations 10−3 100"
REFERENCES,0.8656509695290858,Battleship
REFERENCES,0.8670360110803325,"rECyclicPDA single block
rECyclicPDA postorder
rECyclicPDA children
rECyclicPDA infosets"
REFERENCES,0.868421052631579,"Figure 23: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA with different block construction strategies when using the dilated ℓ2 regularizer and
quadratic averaging as well as restarting. We take the best duality gap seen so far so that the plot is
monotonic."
REFERENCES,0.8698060941828255,"Regularizer Comparison. In this section (Figures 24 to 26) we compare the performance of
ECyclicPDA and MP instantiated with different regularizers for each averaging scheme, against the
performance of CFR+ and PCFR+."
REFERENCES,0.871191135734072,"It is apparent from these plots, that our algorithm generally outperforms MP, holding the averaging
scheme and regularizer fixed. This can be seen by examining the corresponding figure for a choice of
averaging scheme, and noting that for any given regularizer, the corresponding MP line is generally
above the corresponding ECyclicPDA line."
REFERENCES,0.8725761772853186,"Regularizer Comparisons with Restarts. We repeat a similar analysis in this section (Figures 27
to 29), instead now comparing the performance of ECyclicPDA and MP instantiated with different
regularizers for each averaging scheme, against the performance of CFR+ and PCFR+, when all
methods are restarted. The trend noted above of our method generally beating MP, even holding the
regularizer and averaging scheme fixed, still holds even when restarting."
REFERENCES,0.8739612188365651,"102
104
Gradient computations 10−4 10−1"
REFERENCES,0.8753462603878116,Duality gap
REFERENCES,0.8767313019390581,Goofspiel (4 ranks)
REFERENCES,0.8781163434903048,"102
104
Gradient computations 10−6 10−2"
REFERENCES,0.8795013850415513,Liar’s Dice
REFERENCES,0.8808864265927978,"102
104
Gradient computations 10−2"
REFERENCES,0.8822714681440443,"101
Leduc (13 ranks)"
REFERENCES,0.8836565096952909,"102
104
Gradient computations 10−5 10−1"
REFERENCES,0.8850415512465374,Battleship
REFERENCES,0.8864265927977839,"ECyclicPDA dilated entropy
ECyclicPDA dilatable global entropy"
REFERENCES,0.8878116343490304,"ECyclicPDA dilated ℓ2
MP dilated entropy"
REFERENCES,0.889196675900277,"MP dilatable global entropy
MP dilated ℓ2 CFR+ PCFR+"
REFERENCES,0.8905817174515236,"Figure 24: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA, MP, CFR+, PCFR+, using a uniform averaging scheme for ECyclicPDA and MP."
REFERENCES,0.8919667590027701,"102
104
Gradient computations 10−4 10−1"
REFERENCES,0.8933518005540166,Duality gap
REFERENCES,0.8947368421052632,Goofspiel (4 ranks)
REFERENCES,0.8961218836565097,"102
104
Gradient computations 10−6 10−2"
REFERENCES,0.8975069252077562,Liar’s Dice
REFERENCES,0.8988919667590027,"102
104
Gradient computations 10−2"
REFERENCES,0.9002770083102493,"101
Leduc (13 ranks)"
REFERENCES,0.9016620498614959,"102
104
Gradient computations 10−5 10−1"
REFERENCES,0.9030470914127424,Battleship
REFERENCES,0.9044321329639889,"ECyclicPDA dilated entropy
ECyclicPDA dilatable global entropy"
REFERENCES,0.9058171745152355,"ECyclicPDA dilated ℓ2
MP dilated entropy"
REFERENCES,0.907202216066482,"MP dilatable global entropy
MP dilated ℓ2 CFR+ PCFR+"
REFERENCES,0.9085872576177285,"Figure 25: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA, MP, CFR+, PCFR+, using a linear averaging scheme for ECyclicPDA and MP."
REFERENCES,0.909972299168975,"102
104
Gradient computations 10−4 10−1"
REFERENCES,0.9113573407202216,Duality gap
REFERENCES,0.9127423822714681,Goofspiel (4 ranks)
REFERENCES,0.9141274238227147,"102
104
Gradient computations 10−6 10−2"
REFERENCES,0.9155124653739612,Liar’s Dice
REFERENCES,0.9168975069252078,"102
104
Gradient computations 10−2"
REFERENCES,0.9182825484764543,"101
Leduc (13 ranks)"
REFERENCES,0.9196675900277008,"102
104
Gradient computations 10−5 10−1"
REFERENCES,0.9210526315789473,Battleship
REFERENCES,0.9224376731301939,"ECyclicPDA dilated entropy
ECyclicPDA dilatable global entropy"
REFERENCES,0.9238227146814404,"ECyclicPDA dilated ℓ2
MP dilated entropy"
REFERENCES,0.925207756232687,"MP dilatable global entropy
MP dilated ℓ2 CFR+ PCFR+"
REFERENCES,0.9265927977839336,"Figure 26: Duality gap as a function of the number of full (or equivalent) gradient computations for
ECyclicPDA, MP, CFR+, PCFR+, using a quadratic averaging scheme for ECyclicPDA and MP."
REFERENCES,0.9279778393351801,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9293628808864266,Duality gap
REFERENCES,0.9307479224376731,Goofspiel (4 ranks)
REFERENCES,0.9321329639889196,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9335180055401662,Liar’s Dice
REFERENCES,0.9349030470914127,"102
104
Gradient computations 10−2"
REFERENCES,0.9362880886426593,"101
Leduc (13 ranks)"
REFERENCES,0.9376731301939059,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9390581717451524,Battleship
REFERENCES,0.9404432132963989,"rECyclicPDA dilated entropy
rECyclicPDA dilatable global entropy"
REFERENCES,0.9418282548476454,"rECyclicPDA dilated ℓ2
rMP dilated entropy"
REFERENCES,0.943213296398892,"rMP dilatable global entropy
rMP dilated ℓ2 rCFR+"
REFERENCES,0.9445983379501385,rPCFR+
REFERENCES,0.945983379501385,"Figure 27: Duality gap as a function of the number of full (or equivalent) gradient computations for
when restarting is applied to ECyclicPDA, MP, CFR+, PCFR+, using a uniform averaging scheme
for ECyclicPDA and MP. We take the best duality gap seen so far so that the plot is monotonic."
REFERENCES,0.9473684210526315,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9487534626038782,Duality gap
REFERENCES,0.9501385041551247,Goofspiel (4 ranks)
REFERENCES,0.9515235457063712,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9529085872576177,Liar’s Dice
REFERENCES,0.9542936288088643,"102
104
Gradient computations 10−2"
REFERENCES,0.9556786703601108,"101
Leduc (13 ranks)"
REFERENCES,0.9570637119113573,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9584487534626038,Battleship
REFERENCES,0.9598337950138505,"rECyclicPDA dilated entropy
rECyclicPDA dilatable global entropy"
REFERENCES,0.961218836565097,"rECyclicPDA dilated ℓ2
rMP dilated entropy"
REFERENCES,0.9626038781163435,"rMP dilatable global entropy
rMP dilated ℓ2 rCFR+"
REFERENCES,0.96398891966759,rPCFR+
REFERENCES,0.9653739612188366,"Figure 28: Duality gap as a function of the number of full (or equivalent) gradient computations for
when restarting is applied to ECyclicPDA, MP, CFR+, PCFR+, using a linear averaging scheme for
ECyclicPDA and MP. We take the best duality gap seen so far so that the plot is monotonic."
REFERENCES,0.9667590027700831,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9681440443213296,Duality gap
REFERENCES,0.9695290858725761,Goofspiel (4 ranks)
REFERENCES,0.9709141274238227,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9722991689750693,Liar’s Dice
REFERENCES,0.9736842105263158,"102
104
Gradient computations 10−2"
REFERENCES,0.9750692520775623,"101
Leduc (13 ranks)"
REFERENCES,0.9764542936288089,"102
104
Gradient computations 10−10 10−4"
REFERENCES,0.9778393351800554,Battleship
REFERENCES,0.9792243767313019,"rECyclicPDA dilated entropy
rECyclicPDA dilatable global entropy"
REFERENCES,0.9806094182825484,"rECyclicPDA dilated ℓ2
rMP dilated entropy"
REFERENCES,0.981994459833795,"rMP dilatable global entropy
rMP dilated ℓ2 rCFR+"
REFERENCES,0.9833795013850416,rPCFR+
REFERENCES,0.9847645429362881,"Figure 29: Duality gap as a function of the number of full (or equivalent) gradient computations for
when restarting is applied to ECyclicPDA, MP, CFR+, PCFR+, using a quadratic averaging scheme
for ECyclicPDA and MP. We take the best duality gap seen so far so that the plot is monotonic."
REFERENCES,0.9861495844875346,"Additional wall-clock time experiments In Table 3, we show the wall-clock time required to reach
a duality gap of 10−4. It is clear that our algorithm is competitive with MP: MP and its restarted
variant time out on Leduc, and MP times out on Battleship (while our algorithm does not even take
close to 30 seconds). Furthermore, we are outperforming CFR+ and its restarted variant in Battleship."
REFERENCES,0.9875346260387812,"Table 3: The wall clock time in seconds required for ECyclicPDA, MP, CFR+, and PCFR+, and their
restarted variants, denoted using an “r” at the front of the algorithm name, to reach a duality gap of
10−4. We let each algorithm run for at most 30 seconds; a value of 30.000 means the algorithm could
not reach the target gap in 30 seconds. The duality gap is computed every 100 iterations."
REFERENCES,0.9889196675900277,"Name
Goofspiel (4 ranks)
Liar’s Dice
Leduc (13 ranks)
Battleship
ECyclicPDA
8.797
6.148
9.343
16.818"
REFERENCES,0.9903047091412742,"rECyclicPDA
6.688
3.519
10.724
4.914"
REFERENCES,0.9916897506925207,"MP
3.183
3.930
30.000
30.000"
REFERENCES,0.9930747922437673,"rMP
2.691
2.360
30.000
8.374"
REFERENCES,0.9944598337950139,"CFR+
5.869
0.111
0.236
6.001"
REFERENCES,0.9958448753462604,"rCFR+
5.866
0.057
0.161
6.042"
REFERENCES,0.997229916897507,"PCFR+
0.244
0.067
0.225
0.736"
REFERENCES,0.9986149584487535,"rPCFR+
0.208
0.077
0.197
0.736"
