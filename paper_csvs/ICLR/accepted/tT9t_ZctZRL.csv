Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016260162601626016,"Graph convolutional networks (GCNs) and their variants have achieved great suc-
cess in dealing with graph-structured data. Nevertheless, it is well known that
deep GCNs suffer from the over-smoothing problem, where node representations
tend to be indistinguishable as more layers are stacked up. The theoretical re-
search to date on deep GCNs has focused primarily on expressive power rather
than trainability, an optimization perspective. Compared to expressivity, train-
ability attempts to address a more fundamental question: Given a sufﬁciently ex-
pressive space of models, can we successfully ﬁnd a good solution via gradient
descent-based optimizers? This work ﬁlls this gap by exploiting the Graph Neural
Tangent Kernel (GNTK), which governs the optimization trajectory under gradi-
ent descent for wide GCNs. We formulate the asymptotic behaviors of GNTK
in the large depth, which enables us to reveal the dropping trainability of wide
and deep GCNs at an exponential rate in the optimization process. Additionally,
we extend our theoretical framework to analyze residual connection-based tech-
niques, which are found to be merely able to mitigate the exponential decay of
trainability mildly. Inspired by our theoretical insights on trainability, we propose
Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, to
alleviate the exponential decay problem more fundamentally. Experimental eval-
uation consistently conﬁrms using our proposed method can achieve better results
compared to relevant counterparts with both inﬁnite-width and ﬁnite-width."
INTRODUCTION,0.0032520325203252032,"1
INTRODUCTION"
INTRODUCTION,0.004878048780487805,"Recently, Graph Neural Networks (GNNs) have shown incredible abilities to learn node or graph
representations and achieved superior performance on various downstream tasks, such as node clas-
siﬁcation (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Hamilton et al., 2017), graph classiﬁca-
tion (Xu et al., 2019; Lee et al., 2019b; Yuan & Ji, 2020), and link prediction (Kipf & Welling,
2016), etc. However, most GNNs (e.g., GCNs) achieve their best only with a shallow depth, e.g., 2
or 3 layers, and their performance on those tasks would promptly degrade as the number of layers
grows. Towards this phenomenon, research attempts have been made to deepen understanding of
current GNN architectures and their expressive power. Li et al. (2018) showed that GCN is a special
form of Laplacian smoothing, which mixes node representations with nearby neighbors. This mech-
anism potentially poses the risk of over-smoothing as more layers are stacked together, where node
representations tend to be indistinguishable from each other. Oono & Suzuki (2020) investigated"
INTRODUCTION,0.0065040650406504065,"∗Equal Contribution.
†Work partially performed while at The University of Sydney"
INTRODUCTION,0.008130081300813009,Published as a conference paper at ICLR 2022
INTRODUCTION,0.00975609756097561,"the expressive power of GNNs using the asymptotic behaviors as the layer goes to inﬁnity. They
proved that under certain conditions, the expressive power of GCN is determined by the topological
information of the underlying graph inherent in the graph spectra."
INTRODUCTION,0.011382113821138212,"Nevertheless, it remains elusive how to theoretically understand why deep GCNs fail to optimize.
Existing theoretical investigation (Oono & Suzuki, 2020; Xu et al., 2019) on GNNs focus mainly
on expressivity, which measures the complexity of functions that can be represented by a neural
network. Exploring expressivity is theoretically convenient, but the corresponding conditions may
be violated during the gradient training process, thereby leading to inconsistencies between theo-
retical conclusions and empirical results of trained networks (G¨uhring et al., 2020). Compared to
expressivity, trainability addresses a more difﬁcult but fundamental perspective of neural networks:
How effectively a neural network can be optimized via gradient descent. The advantage of investi-
gating trainability is that we can directly determine whether GNNs can be successfully trained under
certain conditions, and to what extent. We are therefore inspired to raise two important questions:"
INTRODUCTION,0.013008130081300813,"• Can we theoretically characterize the trainability of graph neural networks with respect to depth,
thus understanding why deep GCNs fail to generalize?"
INTRODUCTION,0.014634146341463415,"• Can we further design an algorithm to facilitate deeper GCNs, beneﬁting from our theoretical
investigation?"
INTRODUCTION,0.016260162601626018,"Our answers are yes to both questions. We resort to the inﬁnitely-wide multi-layer GCN to de-
rive our solution. The research on inﬁnitely-wide networks can be traced back to the seminal work
of Neal (1996), which showed that single hidden layer networks with random weights at initializa-
tion (without training) are Gaussian Processes (GPs) in the inﬁnite width limit. Later, the connection
between GPs and multi-layer inﬁnitely-wide networks with Gaussian initialization (Lee et al., 2018;
de G. Matthews et al., 2018) and orthogonal weights (Huang et al., 2021) was reported. Recent
trends in Neural Tangent Kernel (NTK) have led to a proliferation of studies on the optimization and
generalization of inﬁnitely (ultra)-wide networks. In particular, Jacot et al. (2018) made a ground-
breaking discovery that gradient descent training in the inﬁnite width limit can be captured by an
NTK. Du et al. (2019b) formulated Graph Neural Tangent Kernel (GNTK) for inﬁnitely-wide GNNs
and shed light on theoretical guarantees for GNNs. Prior to the discovery of GNTK, there was lit-
tle understanding of the non-convexity of GNNs, which is analytically intractable. In the learning
regime of GCN governed by GNTK, the optimization becomes an almost convex problem, making
GNTK a promising perspective to study the trainability of deep GCNs."
INTRODUCTION,0.01788617886178862,"In this work, we leverage the GNTK techniques of inﬁnitely-wide networks to investigate whether
ultra-wide GCNs are trainable in the large depth. In particular, we formulate the large-depth asymp-
totic behavior of the GNTK, illuminated by innovative works on deep networks (Hayou et al., 2019b;
Xiao et al., 2020), through which we can analyze the optimization properties of deep GCNs. Specif-
ically, we make the following contributions:"
INTRODUCTION,0.01951219512195122,"• To our best knowledge, we are the ﬁrst to investigate the trainability of deep GCNs through GNTK.
We prove that all entries of a GNTK matrix regarding a pair of graphs converge exponentially to
the same value, making the GNTK matrix singular in the large depth. We thus establish a corollary
that the trainability of ultra-wide GCNs exponentially collapses on node classiﬁcation tasks."
INTRODUCTION,0.02113821138211382,"• We apply our theoretical analysis to the residual connection-based techniques for GCNs. Our
theory shows that residual connection can, to some extent, slow down the exponential decay rate
of trainability, but lack the ability to fundamentally solve the problem. This result enables to better
understand why and to what extent recent residual connection-based methods work."
INTRODUCTION,0.022764227642276424,"• Our theoretical framework provides insights to guide the development of deep GCNs. We further
propose an edge-based sampling method, named Critical DropEdge, to effectively mitigate the
exponential decay of trainability. This graph-adaptive and connectivity-aware method is easy
to implement in both ﬁnitely-wide and inﬁnitely-wide GNNs. Our experiments show using the
proposed method can outperform competitors in the large depth."
BACKGROUND AND PRELIMINARIES,0.024390243902439025,"2
BACKGROUND AND PRELIMINARIES"
BACKGROUND AND PRELIMINARIES,0.026016260162601626,"We ﬁrst review the results of inﬁnitely-wide neural networks at initialization. We then review NTK,
making a connection to trainability. Finally, we introduce GCNs along with our setup and notation."
BACKGROUND AND PRELIMINARIES,0.027642276422764227,Published as a conference paper at ICLR 2022
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.02926829268292683,"2.1
INFINITELY-WIDE NETWORKS AT INITIALIZATION"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.030894308943089432,"We begin by considering a fully-connected network of depth L with width ml in each layer. The
weight and bias in the l-th layer are denoted by W (l) ∈Rml×ml−1 and b(l) ∈Rml. Letting the
pre-activations be given by h(l)
i , information propagation in this network is governed by,"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.032520325203252036,"h(l)
i
= σw
√ml ml
X"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.03414634146341464,"j=1
φ
 W (l)
ij h(l−1)
j
 + σbb(l)
i
(1)"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.03577235772357724,"where φ : R →R is the activation function, σw and σb deﬁne the variance scale of the weights
and biases, respectively. Given the parameterization that weights and biases are randomly generated
by i.i.d. normal distribution, i.e., W (l)
ij , b(l)
i
∼N(0, 1), the pre-activations are Gaussian distributed
in the inﬁnite width limit as m1, m2, . . . , ml−1 →∞. This results from the central limit theorem
(CLT). Consider a dataset X ∈Rn×d of size n = |X|, the covariance matrix of Gaussian process
kernel (GPK) regarding inﬁnitely-wide network is deﬁned by Σ(l)(x, x′) = E[h(l)
i (x)h(l)
i (x′)]. Ac-
cording to the signal propagation (1), the covariance matrix or GPK with respect to layer can be
described by a recursion relation, Σ(l)(x, x′) = σ2
wEh∼N(0,Σ(l−1))[φ(h(x))φ(h(x′))] + σ2
b."
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.03739837398373984,"The mean-ﬁeld theory is a paradigm that studies the limiting behavior of GPK, which is a mea-
sure of expressivity for networks (Poole et al., 2016; Schoenholz et al., 2017). In particular, ex-
pressivity describes to what extent two different inputs can be distinguished.
The property of
evolution for expressivity Σ(l)(x, x′) is determined by how fast it converges to its ﬁxed point
Σ∗(x, x′) ≡liml→∞Σ(l)(x, x′). It is shown that in almost the entire parameter space spanned
by hyper-parameters σw and σb, the evolution exhibits a dramatic convergence rate formulated by
an exponential function except for a critical line known as the edge of chaos (Poole et al., 2016;
Schoenholz et al., 2017). Consequently, an inﬁnitely-wide network loses its expressivity exponen-
tially in most cases while retaining the expressivity at the edge of chaos. Given this reason, we focus
on the edge of chaos in this work. In particular, we set the value of hyper-parameters to satisfy,
σ2
w
R
Dz[φ′(√q∗z)]2 = 1, where q∗is the ﬁxed point of diagonal entries in the covariance matrix,
and
R
Dz =
1
√"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.03902439024390244,"2π
R
dze−1"
INFINITELY-WIDE NETWORKS AT INITIALIZATION,0.04065040650406504,"2 z2 is the measure for a normal distribution. For the ReLU activation, edge
of chaos requires σ2
w = 2 and σ2
b = 0."
NEURAL TANGENT KERNEL AND TRAINABILITY,0.04227642276422764,"2.2
NEURAL TANGENT KERNEL AND TRAINABILITY"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.04390243902439024,"Most studies on inﬁnitely-wide networks through mean-ﬁeld theory (Poole et al., 2016; Schoen-
holz et al., 2017) have focused solely on initialization without training. Jacot et al. (2018) took
a step further by considering inﬁnitely-wide networks trained with gradient descent.
Let η be
the learning rate, and L be the loss function.
The dynamics of gradient ﬂow for parameters
θ = vec({W (l)
ij , b(l)
i }) ∈R(P"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.04552845528455285,"l ml(ml−1+1))×1, the vector of all parameters, is given by, ∂θ"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.04715447154471545,"∂t = −η∇θL = −η∇θft(X)T ∇ft(X)L
(2)"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.04878048780487805,"Then, the dynamics of output functions f(X) = vec(f(x)x∈X) ∈RnmL×1 follow,"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.05040650406504065,∂ft(X)
NEURAL TANGENT KERNEL AND TRAINABILITY,0.05203252032520325,"∂t
= ∇θft(X)∂θ"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.05365853658536585,"∂t = −ηΘt(X, X)∇ft(X)L
(3)"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.055284552845528454,"where the NTK at time t is deﬁned as,"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.056910569105691054,"Θt(X, X) ≡∇θft(X)∇θft(X)T ∈RnmL×nmL
(4)"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.05853658536585366,"In a general case, the NTK varies with the training time, thus providing no substantial insights into
the convergence property of neural networks. Interestingly, as shown by Jacot et al. (2018), the
NTK converges to an explicit limiting kernel and does not change during training in the inﬁnite-
width limit. This leads to a simple but profound result in the case of mean squared error (MSE) loss,
L = 1"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.06016260162601626,"2∥ft(X) −Y ∥2
2, where Y is the label associated with the input X,"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.061788617886178863,"ft(X) = (I −e−ηΘ∞(X,X)t)Y + e−ηΘ∞(X,X)tf0(X)
(5)"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.06341463414634146,"where Θ∞is the limiting kernel. This is the solution to an ordinary differential equation. As the
training time t tends to inﬁnity, the output function ﬁts the label very well, i.e., f∞(X) = Y . As"
NEURAL TANGENT KERNEL AND TRAINABILITY,0.06504065040650407,Published as a conference paper at ICLR 2022
NEURAL TANGENT KERNEL AND TRAINABILITY,0.06666666666666667,"proved by Lemma 1 in Hayou et al. (2019b), the network is trainable only if Θ∞(X, X) is non-
singular. Quantitatively, the condition number κ ≡λmax/λmin can be a measure of trainability as
conﬁrmed by Xiao et al. (2020)."
GRAPH CONVOLUTIONAL NETWORKS,0.06829268292682927,"2.3
GRAPH CONVOLUTIONAL NETWORKS"
GRAPH CONVOLUTIONAL NETWORKS,0.06991869918699187,"We deﬁne an undirected graph as G = (V, E), where V is a set of nodes and E is a set of edges. We
denote the number of nodes in graph G by n = |V|. The nodes are associated with a node feature
matrix X ∈Rn×d, and the corresponding labels are Y ∈Rn×k, with d and k being the dimension
of node features and number of classes, respectively. In this work, we develop our theory towards
understanding the trainability of GCNs on node classiﬁcation tasks."
GRAPH CONVOLUTIONAL NETWORKS,0.07154471544715447,"GCNs iteratively update node features through aggregating and transforming the representations of
their neighbors. Figure 3 in Appendix A illustrates an overview of the information propagation
in a general GCN. We deﬁne a propagation unit to be the combination of a R-layer multi-layer
perceptron (MLP) and one aggregation operation. We use subscript (r) to denote the layer index
of MLP in each propagation unit and superscript (l) to indicate the index of aggregation operation,
which is also the index of the propagation unit. L is the total number of propagation units. To be
speciﬁc, the node representation propagation in GCNs through an MLP follows the expression,"
GRAPH CONVOLUTIONAL NETWORKS,0.07317073170731707,"h(l)
(0)(u) =
1
|N(u)| + 1 X"
GRAPH CONVOLUTIONAL NETWORKS,0.07479674796747968,"v∈N(u)∪u
h(l−1)
(R) (v)
(6)"
GRAPH CONVOLUTIONAL NETWORKS,0.07642276422764227,"h(l)
(r)(u) = σw
√mφ
 W (l)
(r)h(l)
(r−1)(u)
 + σbb(l)
(r)
(7)"
GRAPH CONVOLUTIONAL NETWORKS,0.07804878048780488,"where h(0)
(0) = X, W (l)
(r) ∈Rml×ml−1, and b(l)
(r) ∈Rml are the learnable weights and biases, respec-
tively, φ is the activation function, N(u) is the neighborhood of node u, and N(u) ∪u is the union
of node u and its neighbors. Equation (6) reveals the node feature aggregation operation among its
neighborhood according to a GCN variant (Hamilton et al., 2017). Equation (7) is a standard non-
linear transformation with NTK-parameterization (Jacot et al., 2018), where m is the width, i.e.,
number of neurons in each layer, σw and σb deﬁne the variance scale of the weights and biases. For
the activation function, we focus on both ReLU and Tanh, which are denoted as φ(x) = max{0, x}
and φ(x) = tanh(x), respectively. Without loss of generality, our theoretical framework can handle
other common activation functions, whereas the GNTK work (Du et al., 2019b) only adopted ReLU."
AGGREGATION PROVABLY LEADS TO EXPONENTIAL TRAINABILITY LOSS,0.07967479674796749,"3
AGGREGATION PROVABLY LEADS TO EXPONENTIAL TRAINABILITY LOSS"
GNTK FORMULATION,0.08130081300813008,"3.1
GNTK FORMULATION"
GNTK FORMULATION,0.08292682926829269,"Based on the deﬁnition of NTK (4), we recursively formulate the propagation of GNTK in the
inﬁnite-width limit. As information propagation in a GCN is built on two operations: aggregation (6)
and non-linear transformation (7), the corresponding formulas of GNTK are expressed as follows,"
GNTK FORMULATION,0.08455284552845528,"Θ(l)
(0)(u, u′) =
1
|N(u)| + 1
1
|N(u′)| + 1 X"
GNTK FORMULATION,0.08617886178861789,v∈N(u)∪u X
GNTK FORMULATION,0.08780487804878048,"v′∈N(u′)∪u′
Θ(l−1)
(R) (v, v′)
(8)"
GNTK FORMULATION,0.08943089430894309,"Θ(l)
(r)(u, u′) = Θ(l)
(r−1)(u, u′) ˙Σ(l)
(r)(u, u′) + Σ(l)
(r)(u, u′)
(9)"
GNTK FORMULATION,0.0910569105691057,"The two equations above correspond to the aggregation operation and MLP transformation, respec-
tively. To compute the GNTK with respect to the depth, the key step is to obtain the covariance
matrix Σ(l)
(r)(u, u′) ≡E[h(l)
(r)(u)h(l)
(r)(u′)]. According to the CLT, node representation h(l)
(r)(u) is a
Gaussian distribution in the inﬁnite-width limit. Applying this result to equations (6) and (7), the
resultant covariance matrix is composed of two parts,"
GNTK FORMULATION,0.09268292682926829,"Σ(l)
(0)(u, u′) =
1
|N(u)| + 1
1
|N(u′)| + 1 X"
GNTK FORMULATION,0.0943089430894309,v∈N(u)∪u X
GNTK FORMULATION,0.0959349593495935,"v′∈N(u′)∪u′
Σ(l−1)
(R) (v, v′)
(10)"
GNTK FORMULATION,0.0975609756097561,"Σ(l)
(r)(u, u′) = σ2
wEz1,z2∼N
 
0,˜Σ(l)
(r−1)

φ(z1)φ(z2)

+ σ2
b"
GNTK FORMULATION,0.0991869918699187,"˙Σ(l)
(r)(u, u′) = σ2
wEz1,z2∼N
 
0,˜Σ(l)
(r−1)
 ˙φ(z1) ˙φ(z2)

+ σ2
b
(11)"
GNTK FORMULATION,0.1008130081300813,Published as a conference paper at ICLR 2022
GNTK FORMULATION,0.1024390243902439,"The ﬁrst equation (10) results from the aggregation operation. Meanwhile, the second equation
(11) corresponds to the R-times non-linear transformations, where Ez1,z2 takes the expectation with
respect to a centered Gaussian process of covariance ˜Σ(l)
(r−1) ∈R2×2 for previous MLP layer across"
GNTK FORMULATION,0.1040650406504065,"u, u′, and ˙φ denotes the derivative of φ."
TRAINABILITY IN THE LARGE DEPTH,0.10569105691056911,"3.2
TRAINABILITY IN THE LARGE DEPTH"
TRAINABILITY IN THE LARGE DEPTH,0.1073170731707317,"We aim to characterize the behavior of GNTK matrix Θ(l)
(r)(G) ∈Rn×n, as the depth tends to inﬁnity.
From the GNTK formulation, both aggregation (8) and transformation (9) contribute simultaneously
to the ﬁnal limiting result. We derive our theorem on the asymptotic behavior of inﬁnitely-wide GCN
in the large depth limit, which is given as follows:"
TRAINABILITY IN THE LARGE DEPTH,0.10894308943089431,"Theorem 1 (Convergence rate of GNTK). If transition matrix A(G) ∈Rn2×n2 is irreducible and
aperiodic, with a stationary distribution vector ⃗π(G) ∈Rn2×1, where ⃗Θ(l)
(0)(G) = A(G)⃗Θ(l−1)
(R) (G)"
TRAINABILITY IN THE LARGE DEPTH,0.11056910569105691,"and ⃗Θ(l)
(r)(G) ∈Rn2×1 is the result of being vectorized. Then, there exist constants 0 < α < 1 and"
TRAINABILITY IN THE LARGE DEPTH,0.11219512195121951,"C > 0, and constant vectors ⃗v,⃗v′ ∈Rn2×1 depending on the number of MLP iterations R, such
that
Θ(l)
(r)(u, u′) −⃗π(G)T  
Rl⃗v + ⃗v′ ≤Cαl."
TRAINABILITY IN THE LARGE DEPTH,0.11382113821138211,"The proof sketch follows a divide-and-conquer manner. In particular, we ﬁrst analyze the network
with only aggregation and prove that A(G) is a Markov transition matrix. Then, we formulate the
behavior of MLP in the large depth based on Hayou et al. (2019b). Finally, we derive the ﬁnal result
by considering the two operations. We leave the complete proof in Appendix B."
TRAINABILITY IN THE LARGE DEPTH,0.11544715447154472,"In Theorem 1, we rigorously characterize the convergence properties of GNTK in the large depth
limit. As the depth goes to inﬁnity, all entries in the GNTK converge to a unique quantity at an
exponential rate. We thus have Θ(l)
(r)(G) ≈⃗π(G)T (Rl⃗v)1n×n as l →∞, where 1n×n is an (n×n)-"
TRAINABILITY IN THE LARGE DEPTH,0.11707317073170732,"dimensional matrix whose entries are one. The exponential convergence rate of Θ(l)
(r)(G) implies
that the trainability of inﬁnitely-wide GCNs degenerates dramatically, as stated below."
TRAINABILITY IN THE LARGE DEPTH,0.11869918699186992,"Corollary 1 (Trainability of ultra-wide GCNs). Consider a GCN of the form (6) and (7), with
depth l, number of non-linear transformations r, an MSE loss, and a Lipchitz activation, trained
with gradient descent on a node classiﬁcation task. Then, the output function follows, ft(X) ="
TRAINABILITY IN THE LARGE DEPTH,0.12032520325203253,"e−ηΘ(l)
(r)(G)tf0(X) + (I −e−ηΘ(l)
(r)(G)t)Y . Then, Θ(l)
(r)(G) is singular when l →∞. Moreover, there
exists a constant C > 0 such that for all t > 0, ∥ft(X) −Y ∥> C."
TRAINABILITY IN THE LARGE DEPTH,0.12195121951219512,"We leave proof in Appendix C. According to the above corollary, as l →∞, the GNTK matrix
would become a singular matrix. This would lead to a discrepancy between output ft(X) and label
Y , which means GNTK loses the ability to ﬁt the label. Therefore, an ultra-wide GCN with a large
depth cannot be trained successfully on node classiﬁcation tasks."
TOWARDS DEEPENING GRAPH NEURAL NETWORKS,0.12357723577235773,"4
TOWARDS DEEPENING GRAPH NEURAL NETWORKS"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.12520325203252033,"4.1
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.12682926829268293,"We have so far characterized the trainability of vanilla GCNs through the GNTK and showed that
the trainability of ultra-wide GCNs drops at an exponential rate. Recently, considerable efforts have
been made to deepen GCNs, among which residual connection-based techniques are widely applied
to resolve the over-smoothing problem (Li et al., 2019). We now apply our theoretical framework to
analyze to what extent residual connection techniques could alleviate the trainability loss problem."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.12845528455284552,"We ﬁrst consider residual connection in aggregation, in which the propagation of the GNTK can be
formulated as,
⃗Θ(l)(G) = (1 −δ)A(G)⃗Θ(l−1)(G) + δ⃗Θ(l−1)(G),
(12)"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13008130081300814,"where 0 < δ < 1.
Taking equation (12) as a new aggregation process, then ⃗Θ(l)(G) =
˜
A(G)⃗Θ(l−1)(G), where ˜
A(G) = (1 −δ)A(G) + δI. We prove that ˜
A(G) is also a transition
matrix with a greater second largest eigenvalue compared to the original matrix A(G)."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13170731707317074,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13333333333333333,"Theorem 2 (Convergence rate of residual connection in aggregation). Consider a GNTK of non-
linear transformation (9) and residual connection (12). Then with a stationary vector ˜⃗π(G) for
˜
A(G), there exist constants 0 < ˜α < 1 and C > 0, and constant vectors ⃗v and ⃗v′ depending on
R, such that
Θ(l)
(r)(u, u′) −˜⃗π(G)T  
Rl⃗v + ⃗v′ ≤C ˜αl. Furthermore, we denote the second largest"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13495934959349593,"eigenvalue of A(G) and ˜
A(G) as λ2 and ˜λ2, respectively. Then, ˜λ2 > λ2."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13658536585365855,"The detailed proof of Theorem 2 and the relationship between convergence rate and the second
largest eigvenvalue can be found in Appendix D.1. This theorem implies that a residual connection
in aggregation can slow down the convergence rate, which is consistent with empirical observations
that residual connection can help deepen GCNs."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13821138211382114,"Then, we consider residual connection only applied on non-linear transformation (MLP). In this
case, the recursive equation for the corresponding GNTK can be expressed as,"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.13983739837398373,"Θ(l)
(r)(u, u′) =Θ(l)
(r−1)(u, u′)
  ˙Σ(l)
(r)(u, u′) + 1

+ Σ(l)
(r)(u, u′)
(13)"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14146341463414633,"This formula is similar to the vanilla GNTK in the inﬁnite-width limit. Only an additional residual
term appears according to residual connection. It turns out that this term may not help slow down
the convergence rate for non-linear transformation.
Theorem 3 (Convergence rate of GNTK with residual connection between transformations). Con-
sider a GNTK of the form (8) and (13). If A(G) is irreducible and aperiodic, with a stationary
distribution ⃗π(G), then there exist constants 0 < α < 1 and C > 0, and constant vectors ⃗v and ⃗v′"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14308943089430895,"depending on R, such that,
Θ(l)
(r)(u, u′) −⃗π(G)T  
Rl(1 + σ2
w
2 )Rl⃗v + ⃗v′ ≤Cαl."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14471544715447154,"Note that α in Theorem 3 is the same as in Theorem 1. The proof of Theorem 3 can be found
in Appendix D.2. Theorem 3 demonstrates that adding residual connection in MLP can not even
reduce the convergence rate of trainability. Finally, we consider residual connection applied to both
aggregation and non-linear transformation simultaneously:
Corollary 2 (Convergence rate of GNTK with residual connection in aggregation and transforma-
tion). Consider a GNTK of the form (12) and (13). If ˜
A(G) is irreducible and aperiodic, with a
stationary distribution ˜⃗π(G), there exist constants 0 < ˜α < 1 and C > 0, and constant vectors
⃗v,⃗v′ ∈Rn2×1 depending on R, such that
Θ(l)
(r)(u, u′) −˜⃗π(G)T  
Rl(1 + σ2
w
2 )Rl⃗v + ⃗v′ ≤C ˜αl."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14634146341463414,"4.2
A NEW SAMPLING METHOD: CRITICAL DROPEDGE"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14796747967479676,"Residual connection is designed from a layer-wise perspective, but it has limited abilities to miti-
gate the exponential decay of trainability. To better resolve this problem, we need to look deeper
into the root cause of the problem – the transition matrix corresponding to the aggregation opera-
tion. A necessary condition for matrix A(G) to be a probability transition matrix is that graph G
is connected. Thus, breaking the connectivity condition is a promising way of better solving the
exponential decay problem. One such method is to perform edge sampling guided by the critical
percolation theory (Huang et al., 2018; Erd˝os & R´enyi, 1961) in random graphs."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.14959349593495935,"On a ﬁnite complete graph of n nodes, there exist Et = n(n −1)/2 edges between all pairs of
nodes. A random graph ˆG is achieved by randomly and uniformly preserving some edges from the
complete graph with an edge probability as p = |E|"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15121951219512195,"Et , where |E| is the number of edges preserved
in the random graph. In this way, the critical percolation can be realized in the random graph with
a critical edge probability pc = 1/(n −1) (Erd˝os & R´enyi, 1961). In the thermodynamic limit of
n →∞, the critical random graph exhibits critical connectivity: the probability that there exists a
path from a ﬁxed point to another point within a certain distance decreases polynomially.
Proposition 1 (Critical connectivity in random graph (Erd˝os & R´enyi, 1961)). Suppose a random
graph ˆG has n nodes with a constant edge probability p. (1) If p < pc, then almost every random
graph is such that its largest component1 is of size O(log n); (2) If p > pc, the random graph has a
giant component of size (1 −αp + o(1))n, where αp < 1; (3) If p = pc, then the maximal size of a
component of almost every graph has order n2/3."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15284552845528454,"1In graph theory, a component of an undirected graph is an induced subgraph in which any two nodes are
connected to each other by paths."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15447154471544716,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15609756097560976,"0
250
500
750
0.00 0.25 0.50 0.75 1.00"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15772357723577235,Normalized GNTK
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.15934959349593497,(a) Vanilla GNTK Collapse
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16097560975609757,"0
250
500
750
0.00 0.25 0.50 0.75 1.00"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16260162601626016,(b) Residual Aggregation GNTK Collapse
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16422764227642275,"0
250
500
750
0.0 0.2 0.4 0.6 0.8"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16585365853658537,(c) Residual MLP GNTK Collapse
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16747967479674797,"0
250
500
750
Depth 10−6 10−4 10−2 100"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.16910569105691056,Element-Wise Distance of GNTK
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.17073170731707318,(d) Vanilla GNTK Convergence Rate
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.17235772357723578,y=exp(-0.15x)
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.17398373983739837,"0
250
500
750
Depth 10−4 10−2 100"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.17560975609756097,(e) Residual Aggregation Convergence Rate
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.1772357723577236,y=exp(-0.15x)
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.17886178861788618,y=exp(-0.11x)
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.18048780487804877,"0
250
500
750
Depth 10−7 10−5 10−3 10−1"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.1821138211382114,"101
(f) Residual MLP Convergence Rate"
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.183739837398374,y=exp(-0.15x)
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.18536585365853658,"Figure 1: Convergence rate of GNKT. (a)-(c) Entries of the normalized (residual connection) GNTK
as a function of the depth, deﬁned as Rl + r. All entries tend to have the same value as the
depth grows. (d)-(f) The element (entry)-wise distance of the normalized (residual connection)
GNTK as a function of the depth. The convergence rate can be bounded by an exponential function
y = exp(−0.15x) for vanilla and residual MLP GNTK, whereas the convergence rate of residual
aggregation is bounded by y = exp(−0.11x)."
THEORETICAL ANALYSIS ON RESIDUAL CONNECTION,0.18699186991869918,"The proposition above implies that the information transforms in the critical random graph at a poly-
nomial rate rather than an exponential rate. This inspires us to solve the problem of exponentially
dropping trainability through designing a graph-dependent and connectivity-aware sampling algo-
rithm called Critical DropEdge. In particular, given a graph G, we randomly drop some edges and
preserve the number of edges as Er = Et · pc = n/2, to approximate the critical graph. Unlike
DropEdge (Rong et al., 2019) that randomly removes a certain number of edges from the input
graph, our method ﬁxes the edge preserving percentage as ρ = Er/|E| =
|V |
2|E|. It is worth noting
that DropEdge may choose the edge probability as p < pc where information can only be passed
to a distance of O(log n) in the graph, or p > pc where the exponential decay of trainability may
occur. A further discussion of the relationship between Proposition 1 and the trainability of the
corresponding GNTK can be found in Appendix E."
EXPERIMENTS,0.1886178861788618,"5
EXPERIMENTS"
EXPERIMENTS,0.1902439024390244,"In this section, we empirically verify our theoretical results and validate the proposed Critical
DropEdge method on node classiﬁcation tasks. Details of four real-world graph datasets used for
node classiﬁcation are summarized in Table 3 in Appendix F.1."
CONVERGENCE RESULTS OF GNTKS,0.191869918699187,"5.1
CONVERGENCE RESULTS OF GNTKS"
CONVERGENCE RESULTS OF GNTKS,0.19349593495934958,"Theorems 1-3 provide theoretical convergence rates for (residual) GNTK. We show the correspond-
ing numerical veriﬁcation in Figure 1. We select a graph randomly from a bioinformatics dataset
(i.e., MUTAG), which consists of 18 nodes and 21 edges. We generate a GNTK of the graph using
the implementation of Du et al. (2019b), with ReLU activation, R = 3, and L = 300. Figure 1(a)-
(c) show that all entries of normalized GNTKs converge to an identical value as the depth goes
larger. Figure 1(d)-(f) further indicates that the convergence rate of GNTK is exponential, as re-
ﬂected by our theorems. By comparing convergence rates, we conclude that the residual connection
in aggregation can slow down the convergence rate, which is consistent with Theorem 2."
TRAINABILITY OF WIDE GCNS,0.1951219512195122,"5.2
TRAINABILITY OF WIDE GCNS"
TRAINABILITY OF WIDE GCNS,0.1967479674796748,"We further examine whether ultra-wide GCNs can be trained successfully for node classiﬁcation.
We conduct experiments on a GCN (Kipf & Welling, 2017), where we apply a width of 1, 000 at"
TRAINABILITY OF WIDE GCNS,0.1983739837398374,Published as a conference paper at ICLR 2022
TRAINABILITY OF WIDE GCNS,0.2,"each hidden layer and the depth ranging from 2 to 29. Figure 2 shows the training and test accuracy
on Cora, Citesser and Pubmed after 300 training epochs. These results show a dramatic drop in
both training and test accuracy as the depth grows, conﬁrming that wide GCNs lose trainability
signiﬁcantly in the large depth on node classiﬁcation, as revealed by Corollary 1."
PERFORMANCE OF CRITICAL DROPEDGE,0.2016260162601626,"5.3
PERFORMANCE OF CRITICAL DROPEDGE"
PERFORMANCE OF CRITICAL DROPEDGE,0.2032520325203252,"0
10
20
30
Depth 0.0 0.2 0.4 0.6 0.8 1.0"
PERFORMANCE OF CRITICAL DROPEDGE,0.2048780487804878,Accuracy
PERFORMANCE OF CRITICAL DROPEDGE,0.20650406504065041,Node Classiﬁcation
PERFORMANCE OF CRITICAL DROPEDGE,0.208130081300813,"Citeseer
Cora
Pubmed"
PERFORMANCE OF CRITICAL DROPEDGE,0.2097560975609756,"Figure 2:
Training and test accuracy w.r.t.
model depth. Solid and dashed lines are train
and test accuracy respectively."
PERFORMANCE OF CRITICAL DROPEDGE,0.21138211382113822,"We apply Critical DropEdge (referred to as C-
DropEdge) to ﬁnitely-wide and inﬁnitely-wide
GNNs on semi-supervised node classiﬁcation. The
implementation details are given in Appendix F.2."
PERFORMANCE OF CRITICAL DROPEDGE,0.21300813008130082,"For ﬁnitely-wide GNNs, we consider three back-
bones:
GCN, JKNet,
and IncepGCN (Rong
et al., 2019). For DropEdge, we use the hyper-
parameters reported in Rong et al. (2019) to obtain
the results. For C-DropEdge, we perform a random
hyper-parameter search and ﬁx the edge preserving
rate as ρ(G) =
|V |
2|E|. In addition, we compare with
DGN (Zhou et al., 2020), a normalization-based
baseline, which uses GCN (Kipf & Welling, 2017)
and GAT (Veliˇckovi´c et al., 2018) as backbones.
Table 1 summarizes node classiﬁcation performance of ﬁnitely-wide GNNs with 4/8/16/32 layers
on three citation networks (Cora, Citesser, Pubmed) and one co-author network (Physics). In par-
ticular, we report the best performance across different backbones for DropEdge, C-DropEdge and
DGN, and leave separate results with different backbones in Appendix G.2. The reported results are
the mean and standard deviation over 10 times. As can be seen, C-DropEdge consistently outper-
forms GCN, DGN, and DropEdge, especially when the model is deep. Besides, C-DropEdge can
achieve smaller error variances, demonstrating stronger robustness than DropEdge."
PERFORMANCE OF CRITICAL DROPEDGE,0.2146341463414634,"For inﬁnitely-wide GCNs, we consider two backbones: GCN (Kipf & Welling, 2017) and JKNet
(Xu et al., 2018). The corresponding results can be found in Appendix G.1."
PERFORMANCE OF CRITICAL DROPEDGE,0.216260162601626,"Comparison to DropEdge.
DropEdge and C-DropEdge differ largely in their hyper-parameter
search space. To ensure good performance, DropEdge needs to exhaustively search for the most
appropriate edge preserving percentage – one of the most signiﬁcant hyper-parameters – that has a
crucial inﬂuence on the ﬁnal performance. In contrast, C-DropEdge has a ﬁxed and graph-dependent
edge preserving rate, which implies its hyper-parameter space is much smaller than that of DropE-
dge. To verify if C-DropEdge can achieve the results close to optimal, we conduct experiments with
various edge preserving rates, and present the results in Table 2. We conclude that, from both theo-
retical and empirical perspectives, the edge preserving percentage set by C-DropEdge is reasonable
and effective, achieving the results close to optimal."
RELATED WORK,0.21788617886178863,"6
RELATED WORK"
RELATED WORK,0.21951219512195122,"Neural Tangent Kernel.
NTKs are used to describe the dynamics of inﬁnitely-wide networks
during gradient descent training. In the inﬁnite-width limit, NTK converges to an explicit limit-
ing kernel; besides, it stays constant during training, providing a convergence guarantee for over-
parameterized networks (Jacot et al., 2018; Lee et al., 2019a; Allen-Zhu et al., 2019; Du et al.,
2019a; Zou et al., 2018). Besides, NTK has been applied to various architectures and brought a
wealth of results, such as orthogonal initialization (Huang et al., 2021), convolutions (Arora et al.,
2019), SVM (Chen et al., 2021), attention (Hron et al., 2020). As for graph networks, GNTK helps
us understand how GNNs learn a class of smooth functions on graphs (Du et al., 2019b) and how
they extrapolate differently from multi-layer perceptron (Xu et al., 2020)."
RELATED WORK,0.22113821138211381,"Deep Graph Neural Networks.
Since deep GNNs suffer from the over-smoothing problem, a
large and growing body of literature has made efforts in deepening GNNs. There is a line of methods
that resort to residual connection to retain their feature expressivity in deep layers. Xu et al. (2018)
use skip connection along with node representations from different neighborhood ranges to preserve
the locality of node representations. Klicpera et al. (2019) derive a personalized propagation of"
RELATED WORK,0.22276422764227644,Published as a conference paper at ICLR 2022
RELATED WORK,0.22439024390243903,"Table 1: Comparison results of test accuracy (%) between C-DropEdge, GCN, DropEdge, and DGN."
RELATED WORK,0.22601626016260162,"Datasets
Methods
4-layer
8-layer
16-layer
32-layer Cora"
RELATED WORK,0.22764227642276422,"GCN
79.8 ± 1.1
73.2 ± 2.7
36.3 ± 13.8
20.1 ± 2.4
DropEdge
82.2 ± 0.7
82.0 ± 0.9
82.2 ± 0.7
82.1 ± 0.5
DGN
82.0 ± 0.9
80.2 ± 0.8
77.7 ± 1.0
73.0 ± 0.8
C-DropEdge
82.5 ± 0.7
82.3 ± 0.6
82.4 ± 0.8
82.6 ± 0.9"
RELATED WORK,0.22926829268292684,Citeseer
RELATED WORK,0.23089430894308943,"GCN
61.2 ± 3.0
50.2 ± 5.7
30.8 ± 2.2
21.7 ± 3.0
DropEdge
70.2 ± 1.0
70.8 ± 1.1
70.7 ± 1.0
70.2 ± 0.8
DGN
69.0 ± 0.9
66.5 ± 1.1
62.9 ± 1.2
63.2 ± 0.9
C-DropEdge
70.8 ± 0.6
70.9 ± 0.9
71.0 ± 1.0
70.7 ± 0.9"
RELATED WORK,0.23252032520325203,Pubmed
RELATED WORK,0.23414634146341465,"GCN
77.4 ± 0.7
57.2 ± 8.4
39.5 ± 10.3
36.3 ± 8.4
Dropedge
77.6 ± 1.4
77.3 ± 1.3
76.7 ± 1.3
77.2 ± 1.3
DGN
78.2 ± 1.0
77.8 ± 1.2
77.2 ± 1.3
77.0 ± 1.1
C-DropEdge
78.0 ± 0.4
77.9 ± 1.0
77.2 ± 1.0
77.8 ± 1.0"
RELATED WORK,0.23577235772357724,Physics
RELATED WORK,0.23739837398373984,"GCN
90.2 ± 0.9
83.5 ± 2.2
41.6 ± 6.2
28.8 ± 9.4
Dropedge
91.6 ± 0.8
91.5 ± 0.7
91.2 ± 0.5
91.3 ± 0.8
DGN
92.2 ± 1.0
86.4 ± 0.7
83.4 ± 0.6
83.2 ± 0.8
C-DropEdge
91.9 ± 0.7
91.7 ± 0.6
92.0 ± 0.4
91.6 ± 0.6"
RELATED WORK,0.23902439024390243,"Table 2: Comparison results of test accuracy at various edge preserving rates. Critical preserving
percentage for each dataset is marked in bold. The three best results per model are shaded in blue ."
RELATED WORK,0.24065040650406505,"Cora
Citeseer
Pubmed"
RELATED WORK,0.24227642276422764,"Percentage
GCN-8
JKNet-4
Percentage
GCN-4
IncepGCN-4
Percentage
JKNet-16
IncepGCN-32"
RELATED WORK,0.24390243902439024,"0.05
58.2 ± 19.6
82.0 ± 0.6
0.15
68.8 ± 1.2
70.1 ± 0.7
0.01
76.1 ± 1.8
75.5 ± 1.9
0.10
69.6 ± 14.4
82.1 ± 0.6
0.20
68.8 ± 1.1
70.6 ± 0.9
0.05
76.2 ± 1.5
76.1 ± 1.3
0.15
69.7 ± 12.5
82.2 ± 0.6
0.25
68.7 ± 0.8
70.5 ± 0.9
0.10
76.0 ± 1.4
76.8 ± 1.3
0.20
75.4 ± 4.0
82.5 ± 0.7
0.30
68.9 ± 0.8
70.5 ± 0.9
0.15
76.9 ± 0.9
77.0 ± 1.4"
RELATED WORK,0.24552845528455283,"0.25
77.3 ± 2.5
82.5 ± 0.7
0.35
69.0 ± 0.8
70.8 ± 0.6
0.22
76.9 ± 0.9
77.8 ± 0.9"
RELATED WORK,0.24715447154471545,"0.30
77.2 ± 2.7
82.2 ± 1.1
0.40
68.9 ± 0.9
70.5 ± 0.4
0.25
76.9 ± 1.1
75.8 ± 2.5
0.35
77.2 ± 2.8
81.7 ± 0.8
0.45
68.4 ± 1.7
70.1 ± 0.7
0.30
76.8 ± 1.1
77.1 ± 1.2
0.40
74.9 ± 5.7
81.4 ± 0.6
0.50
68.1 ± 0.9
70.2 ± 0.8
0.35
76.4 ± 1.2
77.6 ± 1.3
0.45
75.5 ± 6.4
81.7 ± 0.7
0.55
68.0 ± 1.0
70.3 ± 0.7
0.40
76.3 ± 1.3
77.6 ± 1.1"
RELATED WORK,0.24878048780487805,"neural predictions (PPNP) based on personalized Pagerank. Other similar works using residual
connection can also be seen in (Li et al., 2019; Gong et al., 2020; Chen et al., 2020; Liu et al.,
2020). Another line of approaches tackle the issue via regularization mechanisms, such as node/edge
dropping (Hou et al., 2019; Rong et al., 2019), batch normalization (Dwivedi et al., 2020), pair
normalization (Zhao & Akoglu, 2020), and group normalization (Zhou et al., 2020). Recently, a
series of latest works (Loukas, 2020; Zeng et al., 2020; Li et al., 2020; Cong et al., 2021; Huang et al.,
2020) explore the underlying reasons for performance degradation towards mitigation solutions."
CONCLUSION AND DISCUSSION,0.25040650406504067,"7
CONCLUSION AND DISCUSSION"
CONCLUSION AND DISCUSSION,0.25203252032520324,"In this work, we have characterized the asymptotic behavior of GNTK to measure the trainability
of wide GCNs in the large depth. We prove that the trainability drops at an exponential rate due
to the aggregation operation. Furthermore, we apply our theoretical framework to investigate to
what extent residual connection-based techniques help deepen GCNs. We demonstrate that these
techniques can merely slow down the decay rate, but are unable to solve the exponential decay
problem in essence. To overcome the trainability loss problem, we further propose Critical DropE-
dge illuminated by our theoretical framework. The experimental results conﬁrm that our method
can mitigate the trainability problem of deep GCNs. Future research directions include designing a
critical node-centric method so as to make better use of node information."
CONCLUSION AND DISCUSSION,0.25365853658536586,Published as a conference paper at ICLR 2022
CONCLUSION AND DISCUSSION,0.2552845528455285,ACKNOWLEDGMENTS
CONCLUSION AND DISCUSSION,0.25691056910569104,"This work was partially supported by a Collaborative Research Project grant between The University
of Sydney and Data61, Australia. We thank the anonymous reviewers for useful suggestions to
improve the paper. We also thank Ye Su for helpful discussions. This work was also in collaboration
with Digital Research Centre Denmark – DIREC, supported by Innovation Fund Denmark."
REFERENCES,0.25853658536585367,REFERENCES
REFERENCES,0.2601626016260163,"Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019."
REFERENCES,0.26178861788617885,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141–8150, 2019."
REFERENCES,0.2634146341463415,"Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph con-
volutional networks. In International Conference on Machine Learning, pp. 1725–1735. PMLR,
2020."
REFERENCES,0.26504065040650404,"Yilan Chen, Wei Huang, Lam Nguyen, and Tsui-Wei Weng. On the equivalence between neural
network and support vector machine. Advances in Neural Information Processing Systems, 34,
2021."
REFERENCES,0.26666666666666666,"Fan Chung and Mary Radcliffe. On the spectra of general random graphs. The electronic journal of
combinatorics, pp. P215–P215, 2011."
REFERENCES,0.2682926829268293,"Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable beneﬁts of depth in training
graph convolutional networks. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.26991869918699185,"Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.27154471544715447,"Xue Ding and Tiefeng Jiang. Spectral distributions of adjacency and laplacian matrices of random
graphs. The annals of applied probability, pp. 2086–2117, 2010."
REFERENCES,0.2731707317073171,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global
minima of deep neural networks. In International conference on machine learning, pp. 1675–
1685. PMLR, 2019a."
REFERENCES,0.27479674796747966,"Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems, pp. 5723–5733, 2019b."
REFERENCES,0.2764227642276423,"Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020."
REFERENCES,0.2780487804878049,"Paul Erd˝os and Alfr´ed R´enyi. On the strength of connectedness of a random graph. Acta Mathemat-
ica Hungarica, 12(1):261–267, 1961."
REFERENCES,0.27967479674796747,"Paul Erd¨os and Alfr´ed R´enyi. On the evolution of random graphs. In The structure and dynamics of
networks, pp. 38–82. Princeton University Press, 2011."
REFERENCES,0.2813008130081301,"Ari Freedman. Convergence theorem for ﬁnite markov chains. Proc. REU, 2017."
REFERENCES,0.28292682926829266,"Shunwang Gong, Mehdi Bahri, Michael M Bronstein, and Stefanos Zafeiriou. Geometrically prin-
cipled connections in graph neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 11415–11424, 2020."
REFERENCES,0.2845528455284553,"Ingo G¨uhring, Mones Raslan, and Gitta Kutyniok. Expressivity of deep neural networks. arXiv
preprint arXiv:2007.04759, 2020."
REFERENCES,0.2861788617886179,Published as a conference paper at ICLR 2022
REFERENCES,0.28780487804878047,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024–1034, 2017."
REFERENCES,0.2894308943089431,"Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on
deep neural networks training. In International conference on machine learning, pp. 2672–2680.
PMLR, 2019a."
REFERENCES,0.2910569105691057,"Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-ﬁeld behaviour of neural tangent
kernel for deep neural networks. arXiv preprint arXiv:1905.13654, 2019b."
REFERENCES,0.2926829268292683,"Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang
Yang.
Measuring and improving the use of graph information in graph neural networks.
In
International Conference on Learning Representations, 2019."
REFERENCES,0.2943089430894309,"Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Inﬁnite attention: Nngp and
ntk for deep attention networks. In International Conference on Machine Learning, pp. 4376–
4386. PMLR, 2020."
REFERENCES,0.2959349593495935,"Wei Huang, Pengcheng Hou, Junfeng Wang, Robert M Ziff, and Youjin Deng. Critical percolation
clusters in seven dimensions and on a complete graph. Physical Review E, 97(2):022107, 2018."
REFERENCES,0.2975609756097561,"Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks
with orthogonal initialization. In Proceedings of the Thirtieth International Joint Conference on
Artiﬁcial Intelligence, IJCAI-21, pp. 2577–2583, 2021."
REFERENCES,0.2991869918699187,"Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang.
Tackling over-
smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864, 2020."
REFERENCES,0.3008130081300813,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018."
REFERENCES,0.3024390243902439,"Dongkwan Kim and Alice Oh. How to ﬁnd your friendly neighborhood: Graph attention design
with self-supervision. In International Conference on Learning Representations, 2020."
REFERENCES,0.3040650406504065,"Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning, 2016."
REFERENCES,0.3056910569105691,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.3073170731707317,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann.
Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019."
REFERENCES,0.3089430894308943,"Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
Learning Representations, 2018."
REFERENCES,0.3105691056910569,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, 2019a."
REFERENCES,0.3121951219512195,"Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International confer-
ence on machine learning, pp. 3734–3743. PMLR, 2019b."
REFERENCES,0.31382113821138213,"Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can GCNs go as deep
as CNNs? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
9267–9276, 2019."
REFERENCES,0.3154471544715447,"Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020."
REFERENCES,0.3170731707317073,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI conference on artiﬁcial intelligence, 2018."
REFERENCES,0.31869918699186994,Published as a conference paper at ICLR 2022
REFERENCES,0.3203252032520325,"Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
338–348, 2020."
REFERENCES,0.32195121951219513,"Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Con-
ference on Learning Representations, 2020."
REFERENCES,0.3235772357723577,"Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks, pp. 29–53.
Springer, 1996."
REFERENCES,0.3252032520325203,"Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classiﬁcation. In International Conference on Learning Representations, 2020."
REFERENCES,0.32682926829268294,"Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural informa-
tion processing systems, pp. 3360–3368, 2016."
REFERENCES,0.3284552845528455,"Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classiﬁcation. In International Conference on Learning Repre-
sentations, 2019."
REFERENCES,0.3300813008130081,"Jeffrey S Rosenthal. Convergence rates for markov chains. Siam Review, 37(3):387–405, 1995."
REFERENCES,0.33170731707317075,"Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
Graph neural
networks exponentially lose expressive power for node classiﬁcation. In International Conference
on Learning Representations, 2017."
REFERENCES,0.3333333333333333,"Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G¨unnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018."
REFERENCES,0.33495934959349594,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018."
REFERENCES,0.33658536585365856,"Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and general-
ization in deep neural networks. In International Conference on Machine Learning, pp. 10462–
10472. PMLR, 2020."
REFERENCES,0.3382113821138211,"Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453–5462. PMLR, 2018."
REFERENCES,0.33983739837398375,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks? In International Conference on Learning Representations, 2019."
REFERENCES,0.34146341463414637,"Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint
arXiv:2009.11848, 2020."
REFERENCES,0.34308943089430893,"Hao Yuan and S. Ji. Structpool: Structured graph pooling via conditional random ﬁelds. In ICLR,
2020."
REFERENCES,0.34471544715447155,"Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Rajgopal Kannan, Viktor Prasanna,
Long Jin, Andrey Malevich, and Ren Chen. Deep graph neural networks with shallow subgraph
samplers. ICLR, 2020."
REFERENCES,0.3463414634146341,"Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations, 2020."
REFERENCES,0.34796747967479674,"Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper
graph neural networks with differentiable group normalization. In Advances in neural information
processing systems, 2020."
REFERENCES,0.34959349593495936,"Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arxiv e-prints, art.
arXiv preprint arXiv:1811.08888,
2018."
REFERENCES,0.35121951219512193,Published as a conference paper at ICLR 2022
REFERENCES,0.35284552845528455,"A
APPENDIX: A GENERAL GCN ARCHITECTURE"
REFERENCES,0.3544715447154472,"GCNs iteratively update node features through aggregating and transforming the representation of
their neighbors. Figure 3 illustrates an overview of information propagation in a general GCN
considered in this work. …"
REFERENCES,0.35609756097560974,"MLP
Aggregation 𝑟= 𝑅 𝑙= 0 𝑟= 0 𝑙= L"
REFERENCES,0.35772357723577236,Propagation Unit
REFERENCES,0.359349593495935,"MLP
Aggregation 𝑟= 0"
REFERENCES,0.36097560975609755,Propagation Unit 𝑟= 𝑅
REFERENCES,0.36260162601626017,Figure 3: Overview of the information propagation in a general GCN.
REFERENCES,0.3642276422764228,"B
APPENDIX: PROOFS FOR THEOREM 1"
REFERENCES,0.36585365853658536,"B.1
CONVERGENCE OF AGGREGATION GNTK"
REFERENCES,0.367479674796748,"In Theorem 1 we demonstrate that matrix A(G) is a transition matrix, we ﬁrst prove this proposition.
To this end, we block the non-linear transformation by setting R = 0. Then, the propagation of
GNTK can be expressed as,"
REFERENCES,0.36910569105691055,"Θ(l)(u, u′) =
1
|N(u)| + 1
1
|N(u′)| + 1 X"
REFERENCES,0.37073170731707317,v∈N(u)∪u X
REFERENCES,0.3723577235772358,"v′∈N(u′)∪u′
Θ(l−1)(v, v′)
(14)"
REFERENCES,0.37398373983739835,"In order to facilitate the calculation, we rewrite Equation (14) as the following format,"
REFERENCES,0.375609756097561,"⃗Θ(l)(G) = A(G)⃗Θ(l−1)(G)
(15)"
REFERENCES,0.3772357723577236,"where ⃗Θ(l)(G) ∈Rn2×1 is the result of being vectorized, and matrix A(G) ∈Rn2×n2 is a square
matrix. We show the key result that A(G) is a probability transition matrix, and the limiting behavior
of Θ(l)(G) in the following lemma,
Lemma 1 (Convergence of aggregation). Assume R = 0, then"
REFERENCES,0.37886178861788616,"lim
l→∞Θ(l)(u, u′) = ⃗π(G)T ⃗Θ(0)(G)"
REFERENCES,0.3804878048780488,"where π(G) ∈Rn2×1, satisfying A(G)⃗π(G) = ⃗π(G)."
REFERENCES,0.3821138211382114,"Proof. When R = 0, Equation (14) reduces to,"
REFERENCES,0.383739837398374,"Θ(l)(u, u′) =
1
|N(u)| + 1
1
|N(u′)| + 1 X"
REFERENCES,0.3853658536585366,v∈N(u)∪u X
REFERENCES,0.38699186991869916,"v′∈N(u′)∪u′
Θ(l−1)(v, v′)"
REFERENCES,0.3886178861788618,"In order to facilitate calculation, we rewrite the above equation in the format of matrix,"
REFERENCES,0.3902439024390244,⃗Θ(l)(G) = A(l)(G)⃗Θ(l−1)(G)
REFERENCES,0.39186991869918697,"where ⃗Θ(l)(G) ∈Rn2×1, is the result of being vectorized. Thus, the matrix operation A(l)(G) ∈
Rn2×n2. It is obvious that,"
REFERENCES,0.3934959349593496,A(L)(G) = A(L−1)(G) = · · · = A(l)(G) = · · · = A(1)(G)
REFERENCES,0.3951219512195122,"This implies that the aggregation operation is the same for each layer. The next step is to prove
A(G) is a stochastic matrix (transition matrix):
X"
REFERENCES,0.3967479674796748,"j
A(G)ij = 1."
REFERENCES,0.3983739837398374,Published as a conference paper at ICLR 2022
REFERENCES,0.4,"According to Equation (14), A(G) can be expressed as a Kronecker product of two matrices,"
REFERENCES,0.4016260162601626,"A(G) =

B(G)C(G)

⊗

B(G)C(G)
"
REFERENCES,0.4032520325203252,"where B(G), C(G) ∈Rn×n. We then analyse the two matrices separately:"
REFERENCES,0.40487804878048783,"(1) B(G) is a diagonal matrix, which corresponds to the factor
1
N(u)+1 ."
REFERENCES,0.4065040650406504,"B(G) =  
"
REFERENCES,0.408130081300813,"1
N(u1)+1
...
1
N(un)+1  
"
REFERENCES,0.4097560975609756,"(2) The element of matrix C(G) is determined by whether there exists an edge between two vertices,"
REFERENCES,0.4113821138211382,C(G)ij = ˜δij
REFERENCES,0.41300813008130083,"where ˜δij = 1 if i == j or there is an edge between vertex i and j, otherwise ˜δij = 0."
REFERENCES,0.4146341463414634,"We then use the property of matrix B and C before Kronecker product, X j"
REFERENCES,0.416260162601626,"
B(G)C(G)
"
REFERENCES,0.41788617886178864,"ij =
1
N(ui) + 1 X"
REFERENCES,0.4195121951219512,"j
˜δij =
1
N(ui) + 1(N(ui) + 1) = 1"
REFERENCES,0.4211382113821138,"According to the deﬁnition of Kronecker product, we have,
X"
REFERENCES,0.42276422764227645,"j
A(l)(G)ij =
X b X"
REFERENCES,0.424390243902439,"d
[B(G)C(G)]ab[B(G)C(G)]cd = 1"
REFERENCES,0.42601626016260163,"where i = a + (c −1)n, and j = b + (d −1)n."
REFERENCES,0.4276422764227642,"So far, we have proved that A(G) is a stochastic matrix. According to the Perron-Frobenius Theory,
a stationary probability vector ⃗π(G) ∈Rn2×1 is deﬁned as a distribution, which does not change
under application of the transition matrix; that is, it is deﬁned as a probability distribution, which is
also an eigenvector of the probability matrix, associated with eigenvalue 1:"
REFERENCES,0.4292682926829268,A(G)⃗π(G) = ⃗π(G)
REFERENCES,0.43089430894308944,"Note that the spectral radius of every stochastic matrix is at most 1 by Gershgorin circle theorem.
Thus, the convergence rate is governed by the second largest eigenvalue."
REFERENCES,0.432520325203252,"Since liml→∞A(l)
ij (G) = ⃗πj(G), we have,"
REFERENCES,0.43414634146341463,"lim
l→∞
⃗Θ(l)(G) = lim
l→∞Al(G)⃗Θ(0)(G) = Π(G)⃗Θ(0)(G)"
REFERENCES,0.43577235772357725,where Π(G) = 
REFERENCES,0.4373983739837398,"


"
REFERENCES,0.43902439024390244,⃗π(G)T
REFERENCES,0.44065040650406506,"⃗π(G)T
...
⃗π(G)T "
REFERENCES,0.44227642276422763,"


is the n2 × n2 matrix all of whose rows are the stationary distribu-"
REFERENCES,0.44390243902439025,"tion. Then, we can see that every element in ⃗Θ(l)(G) converges exponentially to an identical value,
depending on the stationary distribution and initial state,"
REFERENCES,0.44552845528455287,"lim
l→∞Θ(l)(u, u′) = ⃗π(G)T ⃗Θ(0)(G)"
REFERENCES,0.44715447154471544,"Remark 1. This lemma can be extended to the multi-graph setting, where matrix A(G, G′) ∈
Rnn′×nn′ with respect to a pair of graphs G, G′ is a transition matrix as well, and n′ is the number
of vertices in graph G′."
REFERENCES,0.44878048780487806,Published as a conference paper at ICLR 2022
REFERENCES,0.4504065040650406,"B.2
CONVERGENCE OF MLP GPK"
REFERENCES,0.45203252032520325,"Before we prove Theorem 1, we introduce the result for the Gaussian Process kernel Σ(l)
(r)(G) of
a pure MLP. By doing so, we consider a network with only non-linear transformation, known as a
pure MLP. This leads to Σ(l)
(r)(u, u′) = Σ(r)(u, u′), where we use subscript (r) to denote the layer
index. And we rewrite the propagation function for GPK as follows,"
REFERENCES,0.45365853658536587,"Σ(r)(u, u′) = σ2
wEz1,z2∼N
 
0,˜Σ(r−1)

φ(z1)φ(z2)

+ σ2
b
(16)"
REFERENCES,0.45528455284552843,"and the variance ˜Σ(r) ∈R2×2 is,"
REFERENCES,0.45691056910569106,"˜Σ(r) =

Σ(r)(u, u)
Σ(r)(u, u′)
Σ(r)(u′, u)
Σ(r)(u′, u′)"
REFERENCES,0.4585365853658537,"
(17)"
REFERENCES,0.46016260162601624,"The large depth behavior has been well studied in Hayou et al. (2019a), and we introduce the result
when the edge of chaos is realized. In particular, we set the value of hyper-parameters to satisfy, σ2
w"
REFERENCES,0.46178861788617886,"Z
Dz[φ′(√q∗z)]2 = 1
(18)"
REFERENCES,0.4634146341463415,"where q∗is the ﬁxed point of diagonal elements in the covariance matrix, and
R
Dz
=
1
√"
REFERENCES,0.46504065040650405,"2π
R
dze−1"
REFERENCES,0.4666666666666667,"2 z2 is the measure for a normal distribution. For the ReLU activation, equation (18)
requires σ2
w = 2 and σ2
b = 0."
REFERENCES,0.4682926829268293,"The key idea is to study the asymptotic behavior of the normalized correlation deﬁned as,"
REFERENCES,0.46991869918699186,"Cr(u, u′) ≡
Σ(r)(u, u′)
p"
REFERENCES,0.4715447154471545,"Σ(r)(u, u) Σ(r)(u′, u′).
(19)"
REFERENCES,0.47317073170731705,"Lemma 2 (Proposition 1 and 3 in Hayou et al. (2019a)). Assume a network without aggregation,
i.e., L = 0, with a Lipschitz nonlinearity φ, then,"
REFERENCES,0.47479674796747967,"• φ(x) = (x)+, 1 −Cr(u, u′) ∼9π2"
REFERENCES,0.4764227642276423,2r2 as r →∞
REFERENCES,0.47804878048780486,"• φ(x) = tanh(x), 1 −Cr(u, u′) ∼β"
REFERENCES,0.4796747967479675,"r as r →∞where β =
2
R"
REFERENCES,0.4813008130081301,"Dz [φ′(√q∗z)2] q
R"
REFERENCES,0.48292682926829267,Dz [φ′′(√q∗z)2].
REFERENCES,0.4845528455284553,"Proof. We ﬁrst decompose the integral calculation in the Equation (16) into two parts, diagonal
elements and non-diagonal elements:"
REFERENCES,0.4861788617886179,"Σ(r)(u, u) = σ2
w Z"
REFERENCES,0.4878048780487805,"Dz
φ2(
q"
REFERENCES,0.4894308943089431,"Σ(r−1)(u, u)z) + σ2
b"
REFERENCES,0.49105691056910566,"Σ(r)(u, u′) = σ2
w Z"
REFERENCES,0.4926829268292683,"Dz1Dz2
φ(u1)φ(u2) + σ2
b"
REFERENCES,0.4943089430894309,"where
u1
=
p"
REFERENCES,0.4959349593495935,"Σ(r−1)(u, u)z1,
and
u2
=
p"
REFERENCES,0.4975609756097561,"Σ(r−1)(u′, u′)

Cr−1(u, u′)z1
+
q"
REFERENCES,0.4991869918699187,"1 −C2
r−1(u, u′)z2"
REFERENCES,0.5008130081300813,"
, with Cr−1(u, u′) = Σ(r−1)(u, u′)/
p"
REFERENCES,0.5024390243902439,"Σ(r−1)(u, u)Σ(r−1)(u′, u′)."
REFERENCES,0.5040650406504065,"For simplicity, we deﬁne qr(u) = Σ(r)(u, u), qr(u′) = Σ(r)(u′, u′). We then proceed with the proof
by dividing the activation φ(x) into two classes, namely, ReLU and Tanh."
REFERENCES,0.5056910569105691,"ReLU activation, φ(x) = max{0, x}.
The recursive equation (16) for qr(u) reduces to,"
REFERENCES,0.5073170731707317,"qr(u) = σ2
w
2 qr−1(u) + σ2
b"
REFERENCES,0.5089430894308943,"The edge of chaos condition σ2
w
R
Dz[φ′(√q∗z)]2 = 1 requires σ2
w = 2 and σ2
b = 0 for ReLU
activation, which leads to,
lim
r→∞qr(u) = q0(u) ≡q(u)"
REFERENCES,0.510569105691057,Published as a conference paper at ICLR 2022
REFERENCES,0.5121951219512195,"Then the second integration for Cr(u, u′) becomes,"
REFERENCES,0.5138211382113821,"Cr(u, u′) =
σ2
w
R"
REFERENCES,0.5154471544715448,"Dz1Dz2 φ
 p"
REFERENCES,0.5170731707317073,"qr−1(u)z1

φ
 p"
REFERENCES,0.5186991869918699,"qr−1(u)(Cr−1(u, u′)z1 +
p"
REFERENCES,0.5203252032520326,"1 −Cr−1(u, u′)2z2)

+ σ2
b
qr−1(u)"
REFERENCES,0.5219512195121951,"To investigate the propagation of Cr(u, u′), we set qr(u) = qr(u′) = q, and deﬁne,"
REFERENCES,0.5235772357723577,"f(x) =
σ2
w
R"
REFERENCES,0.5252032520325203,"Dz1Dz2 φ
 √qz1

φ
 √q(xz1 +
√"
REFERENCES,0.526829268292683,"1 −x2z2)

+ σ2
b
q"
REFERENCES,0.5284552845528455,"Let x ∈[0, 1], the derivative of f(x) satisﬁes,"
REFERENCES,0.5300813008130081,"f ′(x) = 2
Z"
REFERENCES,0.5317073170731708,"Dz1Dz2
1z1>01xz1+
√"
REFERENCES,0.5333333333333333,1−x2z2>0
REFERENCES,0.5349593495934959,"This can be seen from a simple derivation,"
REFERENCES,0.5365853658536586,"f ′(x) =
Z Dz1 Z"
REFERENCES,0.5382113821138211,"Dz2
φ(z1)φ′(xz1 +
p"
REFERENCES,0.5398373983739837,"1 −x2z2)(z1 −
x
√"
REFERENCES,0.5414634146341464,"1 −x2 z2) =
Z Dz1 Z"
REFERENCES,0.5430894308943089,"Dz2
φ(z1)φ′(xz1 +
p"
REFERENCES,0.5447154471544715,"1 −x2z2)(z1) −
Z Dz1 Z"
REFERENCES,0.5463414634146342,"Dz2
φ(z1)φ′(xz1 +
p"
REFERENCES,0.5479674796747968,"1 −x2z2)(
x
√"
REFERENCES,0.5495934959349593,1 −x2 z2)
REFERENCES,0.551219512195122,"Using an identity for Gaussian integration
R"
REFERENCES,0.5528455284552846,"Dz zg(z) =
R"
REFERENCES,0.5544715447154471,"Dz g′(z) yields,"
REFERENCES,0.5560975609756098,"f ′(x) =
Z Dz1 Z Dz2"
REFERENCES,0.5577235772357724,"
φ′(z1)φ′(xz1 +
p"
REFERENCES,0.5593495934959349,"1 −x2z2) + φ(z1)φ′′(xz1 +
p"
REFERENCES,0.5609756097560976,"1 −x2z2) −φ(z1)φ′′(xz1 +
p"
REFERENCES,0.5626016260162602,"1 −x2z2)
 =
Z Dz1 Z"
REFERENCES,0.5642276422764227,"Dz2
φ′(z1)φ′(xz1 +
p"
REFERENCES,0.5658536585365853,1 −x2z2)
REFERENCES,0.567479674796748,"Then the second derivative of f(x) becomes, f ′′(x) =
1
π
√"
REFERENCES,0.5691056910569106,"1−x2 . So using the equation above and
the condition f ′(0) = 1"
REFERENCES,0.5707317073170731,"2, we can obtain another form of the derivative of f(x),"
REFERENCES,0.5723577235772358,f ′(x) = 1
REFERENCES,0.5739837398373984,π arcsin(x) + 1 2
REFERENCES,0.5756097560975609,"Because
R
arcsin = x arcsin +
√"
REFERENCES,0.5772357723577236,"1 −x2 and f(1) = 1, then for x ∈[0, 1],"
REFERENCES,0.5788617886178862,"f(x) = 2x arcsin(x) + 2
√"
REFERENCES,0.5804878048780487,"1 −x2 + xπ
2π"
REFERENCES,0.5821138211382114,"Substituting f(x) = Cr(u, u′) and x = Cr−1(u, u′), into expression above here, we have,"
REFERENCES,0.583739837398374,"Cr(u, u′) =
2Cr−1(u, u′) arcsin(Cr−1(u, u′)) + 2
q"
REFERENCES,0.5853658536585366,"1 −C2
r−1(u, u′) + Cr−1(u, u′)π 2π"
REFERENCES,0.5869918699186992,"Now we study the behavior of Cr(u, u′) as r tends to inﬁnity. Using Taylor expansion, we have,"
REFERENCES,0.5886178861788618,"f(x)|x→1−= x + 2
√"
REFERENCES,0.5902439024390244,"2
3π (1 −x)3/2 + O((1 −x)5/2)"
REFERENCES,0.591869918699187,"By induction analysis, the sequence Cr(u, u′) increases to the ﬁxed point 1. Besides, we can replace
x with Cr(u, u′),"
REFERENCES,0.5934959349593496,"Cr+1(u, u′) = Cr(u, u′) + 2
√"
REFERENCES,0.5951219512195122,"2
3π (1 −Cr(u, u′))3/2 + O((1 −Cr(u, u′))5/2)"
REFERENCES,0.5967479674796748,"Let γr = 1 −Cr(u, u′), then we have,"
REFERENCES,0.5983739837398374,"γr+1 = γr −2
√"
REFERENCES,0.6,"2
3π γ3/2
r
+ O(γ5/2
r
)"
REFERENCES,0.6016260162601627,Published as a conference paper at ICLR 2022
REFERENCES,0.6032520325203252,"so that,"
REFERENCES,0.6048780487804878,"γ−1/2
r+1
= γ−1/2
r
(1 −2
√"
REFERENCES,0.6065040650406504,"2
3π γ1/2
r
+ O(γ3/2
r
))−1/2 = γ−1/2
r
+ √"
REFERENCES,0.608130081300813,"2
3π + O(γr)"
REFERENCES,0.6097560975609756,"As r tends to inﬁnity, we have,"
REFERENCES,0.6113821138211382,"γ−1/2
r+1 −γ−1/2
r
∼ √"
REFERENCES,0.6130081300813008,"2
3π
It means,"
REFERENCES,0.6146341463414634,"γ−1/2
r
∼ √"
REFERENCES,0.616260162601626,"2
3π r"
REFERENCES,0.6178861788617886,"Therefore, we have,"
REFERENCES,0.6195121951219512,"1 −Cr(u, u′) ∼9π2 2r2"
REFERENCES,0.6211382113821138,"Tanh activation, φ(x) = tanh(x).
For the argument of ﬁxed point, we ask readers to refer to
Hayou et al. (2019a). Here, we only discuss the convergence rate of GPK, which means we directly
assume that f(x) tends to 1 as the depth tends to inﬁnity limr→∞Cr(u, u′) = 1. For the function
f(x), a Taylor expansion near 1 yields,"
REFERENCES,0.6227642276422765,f(x) = 1 + (x −1)f ′(1) + (x −1)2
REFERENCES,0.624390243902439,"2
f ′′(1) + O((x −1)5/2)"
REFERENCES,0.6260162601626016,"where f ′(1) = σ2
w
R"
REFERENCES,0.6276422764227643,"Dz[φ′(√qz)2], and f ′′(1) = σ2
wq
R"
REFERENCES,0.6292682926829268,"Dz[φ′′(√qz)2]. Denote γr = 1 −Cr(u, u′),
then we have,"
REFERENCES,0.6308943089430894,"γr+1 = γr −γ2
r
β + O(γ5/2
l
)"
REFERENCES,0.6325203252032521,"where β =
2
R"
REFERENCES,0.6341463414634146,"Dz [φ′(√q∗z)2] q
R"
REFERENCES,0.6357723577235772,"Dz [φ′′(√q∗z)2]. Therefore,"
REFERENCES,0.6373983739837399,"γ−1
r+1 = γ−1
r (1 −γr"
REFERENCES,0.6390243902439025,"β + O(γ3/2
r
)) = γ−1
r
+ 1"
REFERENCES,0.640650406504065,"β + O(γ1/2
r
)."
REFERENCES,0.6422764227642277,"Thus, we have,"
REFERENCES,0.6439024390243903,"1 −C(r)(u, u′) ∼β"
REFERENCES,0.6455284552845528,r as r →∞
REFERENCES,0.6471544715447154,"Lemma 2 shows that the covariance matrix converges to a constant matrix at a polynomial rate of
1/r2 for ReLU and of 1/r for tanh activation on the edge of chaos. This implies that a network
without aggregation could retain its expressivity at a large depth. However, for general cases, due
to aggregation, the rate of convergence would change from polynomial to exponential. This would
cause the trainability of deep GNNs to be problematic, as shown in the following section."
REFERENCES,0.6487804878048781,"B.3
CONVERGENCE OF GPK FOR GCNS"
REFERENCES,0.6504065040650406,"Then we formally characterize the convergence of Gaussian Process kernel Σ(l)
(r)(u, u′) of a GCN in
the inﬁnite-width limit:"
REFERENCES,0.6520325203252032,"Lemma 3. If A(G) is ireducible and aperiodic, with a stationary distribution vector ⃗π(G), then
there exist constants 0 < α < 1 and C > 0, and constant vector ⃗v ∈Rn2×1 depending on the
number of MLP iterations R, such that"
REFERENCES,0.6536585365853659,"|Σ(l)
(r)(u, u′) −⃗π(G)T⃗v| ≤Cαl"
REFERENCES,0.6552845528455284,"Proof. We prove the result via an induction method.
For l = 0, according to the Cauchy-
Buniakowsky-Schwarz Inequality"
REFERENCES,0.656910569105691,"Σ(0)
(0)(u, u′) = h(0)
u h(0)
u′ ≤∥h(0)
u ∥2∥h(0)
u′ ∥2 = 1"
REFERENCES,0.6585365853658537,Published as a conference paper at ICLR 2022
REFERENCES,0.6601626016260163,"Thus, there is a constant C, depending on G(V, E), and the number of MLP operations R, over
feature initialization, such that,
Σ(0)
(0)(u, u′) −⃗π(G)T⃗v
 < C"
REFERENCES,0.6617886178861788,"Assume the result is valid for Σ(l)
(r)(u, u′), then we have,"
REFERENCES,0.6634146341463415,"Σ(l)
(r)(u, u′) −⃗π(G)T⃗v
 ≤C0αl"
REFERENCES,0.6650406504065041,"where C0 is a constant satisfying 0 < C0 < C. Now we consider the distance between Σ(l+1)
(r)
(u, u′)
and Cαl+1. To compute this, we need to divide the propagation from l layer to l + 1 layer into three
steps:"
REFERENCES,0.6666666666666666,"1. Σ(l)
(r) →Σ(l)
(r+1) →· · · →Σ(l)
(R)"
REFERENCES,0.6682926829268293,"2. Σ(l)
(R) →Σ(l+1)
(0)"
REFERENCES,0.6699186991869919,"3. Σ(l+1)
(0)
→Σ(l+1)
(1)
→· · · →Σ(l+1)
(r)
."
REFERENCES,0.6715447154471544,"It is not hard to ﬁnd that steps 1 and 3 correspond to non-linear transformation while step 2 corre-
sponds to aggregation operation, we then characterize them one by one,"
REFERENCES,0.6731707317073171,"MLP Propagation
The assumption
Σ(l)
(r)(u, u′) −⃗π(G)T⃗v
 ≤C0αl implicitly implies that"
REFERENCES,0.6747967479674797,"Cr(u, u′) is close to one. Because Cr(u, u′) =
Σ(l)
(r)(u,u′)
q"
REFERENCES,0.6764227642276422,"Σ(l)
(r)(u,u) Σ(l)
(r)(u′,u′), then,"
REFERENCES,0.6780487804878049,⃗π(G)T⃗v −C0αl
REFERENCES,0.6796747967479675,"⃗π(G)T⃗v + C0αl ≤Cr(u, u′) ≤⃗π(G)T⃗v + C0αl"
REFERENCES,0.6813008130081301,⃗π(G)T⃗v −C0αl
REFERENCES,0.6829268292682927,"Because C0αl ≪⃗π(G)T⃗v, we have Cr(u, u′) = 1+O(αl). Recall the property of MLP propagation
function f(x) for x = Cr(u, u′), when Cr(u, u′) is close to 1:"
REFERENCES,0.6845528455284553,"f(x)|x→1−= x + 2
√"
REFERENCES,0.6861788617886179,"2
3π (1 −x)3/2 + O((1 −x)5/2)"
REFERENCES,0.6878048780487804,"This implies,"
REFERENCES,0.6894308943089431,"|C(r+1)(u, u′) −C(r)(u, u′)| = 2
√"
REFERENCES,0.6910569105691057,"2
3π (1 −C(r)(u, u′))
3
2 + O(1 −C(r)(u, u′))
5
2 = O(α
3
2 l)"
REFERENCES,0.6926829268292682,"With this result, we can further obtain,"
REFERENCES,0.6943089430894309,"|Σ(l)
(r+1)(u, u′) −⃗π(G)T⃗v| = |Σ(l)
(r+1)(u, u′) −Σ(l)
(r)(u, u′) + Σ(l)
(r)(u, u′) −⃗π(G)T⃗v|"
REFERENCES,0.6959349593495935,"≤|Σ(l)
(r+1)(u, u′) −Σ(l)
(r)(u, u′)| + |Σ(l)
(r)(u, u′) −⃗π(G)T⃗v|"
REFERENCES,0.697560975609756,"= |C(l)
(r+1)(u, u′) −C(l)
(r)(u, u′)|
q"
REFERENCES,0.6991869918699187,"Σ(l)
(r)(u, u)Σ(l)
(r)(u′, u′) + |Σ(l)
(r)(u, u′) −⃗π(G)T⃗v|"
REFERENCES,0.7008130081300813,"= C1α
3
2 l + C0αl ≤(C0 + C1)αl."
REFERENCES,0.7024390243902439,"where C1 is a positive constant. Repeat the proof process, we have a relation for Σ(l)
(R)(u, u′) at the
last step of 1,"
REFERENCES,0.7040650406504065,"|Σ(l)
(R)(u, u′) −⃗π(G)T⃗v| ≤(C0 + (R −r)C1)αl.
(20)"
REFERENCES,0.7056910569105691,Published as a conference paper at ICLR 2022
REFERENCES,0.7073170731707317,"Aggregation Propagation
We go through an aggregation operation A(G). In this case, we use a
matrix form, and take equation (20) into aggregation function, yielding the result as follows,"
REFERENCES,0.7089430894308943,"⃗Σ(l+1)
(0)
(G) = A(G)⃗Σ(l)
(R)(G) = A(G)(Π(G)⃗v + ⃗O(αl))"
REFERENCES,0.7105691056910569,where ⃗O(αl) ∈Rn2×1 denotes a vector in which every element is bounded by αl.
REFERENCES,0.7121951219512195,"According to Theorem 4.9 in Freedman (2017), we have AlΠ(G)⃗v = Π(G)⃗v as l →∞. When l is
ﬁnite, the error is of exponential decay. Here, we use 0 < α < 1 to denote the corresponding base
number. Therefore,"
REFERENCES,0.7138211382113822,"|Σ(l+1)
(0)
(u, u′) −⃗π(G)T⃗v| ≤(C0 + (R −r)C1)αl+1."
REFERENCES,0.7154471544715447,"Finally, by repeating the result in step 1, we have,"
REFERENCES,0.7170731707317073,"|Σ(l+1)
(r)
(u, u′) −⃗π(G)T⃗v| ≤(C0 + RC1)αl+1 = Cαl+1."
REFERENCES,0.71869918699187,"Remark 2. In the proof, there is a RC1 term in each propagation of l →l + 1, which may lead
to explosion when l tends to inﬁnity. Basically, this problem can be solved by a careful analysis,
because the constant C1 is associated with O(α
3
2 l), which has a smaller order compared to O(αl)."
REFERENCES,0.7203252032520325,"B.4
CONVERGENCE OF GNTK"
REFERENCES,0.7219512195121951,"Finally, we arrive at our main theorem:"
REFERENCES,0.7235772357723578,"Theorem 1 (Convergence rate of GNTK). If transition matrix A(G) ∈Rn2×n2 is irreducible and
aperiodic, with a stationary distribution vector ⃗π(G) ∈Rn2×1, where ⃗Θ(l)
(0)(G) = A(G)⃗Θ(l−1)
(R) (G)"
REFERENCES,0.7252032520325203,"and ⃗Θ(l)
(r)(G) ∈Rn2×1 is the result of being vectorized. Then there exist constants 0 < α < 1 and"
REFERENCES,0.7268292682926829,"C > 0, and constant vectors ⃗v,⃗v′ ∈Rn2×1 depending on the number of MLP iterations R, such
that
Θ(l)
(r)(u, u′) −⃗π(G)T  
Rl⃗v + ⃗v′ ≤Cαl."
REFERENCES,0.7284552845528456,"Proof. This proof has the same strategy to that of Lemma 3. The ﬁrst step is to understand Equation
(9) in the large-depth limit."
REFERENCES,0.7300813008130081,"Θ(l)
(r)(u, u′) = Θ(l)
(r−1)(u, u′) ˙Σ(l)
(r)(u, u′) + Σ(l)
(r)(u, u′)"
REFERENCES,0.7317073170731707,"According to the result of Lemma 3, we have already known,"
REFERENCES,0.7333333333333333,"Σ(l)
(r)(u, u′) = ⃗π(G)T⃗v + O(αl)"
REFERENCES,0.734959349593496,"To proceed the proof, we need to work out the behavior of ˙Σ(l)
(r)(u, u′) in the large depth."
REFERENCES,0.7365853658536585,"ReLU activation, φ(x) = max{0, x}.
Recall that we deﬁne Cr+1 = f(Cr), and we have,"
REFERENCES,0.7382113821138211,f ′(x) = 1
REFERENCES,0.7398373983739838,π arcsin(x) + 1 2
REFERENCES,0.7414634146341463,"Then, at the edge of chaos,"
REFERENCES,0.7430894308943089,"˙Σ(r)(u, u′) = f ′(Cr(u, u′)) = 1"
REFERENCES,0.7447154471544716,"π arcsin(Cr(u, u′)) + 1 2"
REFERENCES,0.7463414634146341,= 1 −2
REFERENCES,0.7479674796747967,"π (1 −Cr(u, u′))1/2 + O(1 −Cr(u, u′))3/2 = 1 + O(αl/2)"
REFERENCES,0.7495934959349594,Published as a conference paper at ICLR 2022
REFERENCES,0.751219512195122,"Tanh activation, φ(x) = tanh(x).
We have"
REFERENCES,0.7528455284552845,f ′(x) = 1 −(x −1)f ′′(1) + O((x −1)2)
REFERENCES,0.7544715447154472,"At the edge of chaos,
˙Σ(r)(u, u′) = 1 + O(αl)"
REFERENCES,0.7560975609756098,"Now we prove the result via an induction method. For l = 0, we directly have,"
REFERENCES,0.7577235772357723,"Θ(0)
(0)(u, u′) = Σ(0)
(0)(u, u′) ≤∥h(0)
u ∥2∥h(0)
u′ ∥2 = 1"
REFERENCES,0.759349593495935,"Thus there is a constant C, depending on G(V, E), and the number of MLP operations R, over
feature initialization,
|Θ(0)
(0)(u, u′) −⃗π(G)T⃗v′| < C"
REFERENCES,0.7609756097560976,"Assume the result is valid for Θ(l)
(r)(u, u′), then we have,"
REFERENCES,0.7626016260162601,"|Θ(l)
(r)(u, u′) −⃗π(G)T (Rl⃗v + ⃗v′)| ≤Cαl"
REFERENCES,0.7642276422764228,"Now we consider the distance between Θ(l+1)
(r)
(u, u′) and ⃗π(G)T (R(l +1)⃗v +⃗v′). To prove this, we
need to divide the propagation from l layer to l + 1 layer into three steps:"
REFERENCES,0.7658536585365854,"1. Θ(l)
(r) →Θ(l)
(r+1) →· · · →Θ(l)
(R)"
REFERENCES,0.767479674796748,"2. Θ(l)
(R) →Θ(l+1)
(0)"
REFERENCES,0.7691056910569106,"3. Θ(l+1)
(0)
→Θ(l+1)
(1)
→· · · →Θ(l+1)
(r)
."
REFERENCES,0.7707317073170732,"For step 1, we have,"
REFERENCES,0.7723577235772358,"|Θ(l)
(r+1)(u, u′) −⃗π(G)T ((lR + 1)⃗v + ⃗v′)|"
REFERENCES,0.7739837398373983,"= |Σ(l)
(r+1)(u, u′) + ˙Σ(l)
(r+1)(u, u′)Θ(l)
(r)(u, u′) −⃗π(G)T ((lR + 1)⃗v + ⃗v′)|"
REFERENCES,0.775609756097561,"= |⃗π(G)T⃗v(1 + O(αl)) + (1 + O(αl/2))Θ(l)
(r)(u, u′) −⃗π(G)T ((lR + 1)⃗v + ⃗v′)| ≤Cαl"
REFERENCES,0.7772357723577236,"Repeat the process, we have a relation for Θ(l)
(R)(u, u′) at the last transformation in step 1,"
REFERENCES,0.7788617886178861,"|Θ(l)
(R)(u, u′) −⃗π(G)T ((lR + R −r)⃗v + ⃗v′)| ≤Cαl."
REFERENCES,0.7804878048780488,"Secondly, we go through an aggregation operation. Because it is a Markov chain step, we have,"
REFERENCES,0.7821138211382114,"|Θ(l+1)
(0)
(u, u′) −⃗π(G)T ((Rl + R −r)⃗v + ⃗v′)| ≤Cαl+1."
REFERENCES,0.7837398373983739,"Repeat the result in step 1, we ﬁnally have,"
REFERENCES,0.7853658536585366,"|Θ(l+1)
(r)
(u, u′) −⃗π(G)T ((Rl + R)⃗v + ⃗v′)|"
REFERENCES,0.7869918699186992,"=|Θ(l+1)
(r)
(u, u′) −⃗π(G)T (R(l + 1)⃗v + ⃗v′| ≤Cαl+1."
REFERENCES,0.7886178861788617,"C
TRAINABILITY OF ULTRA-WIDE GCNS"
REFERENCES,0.7902439024390244,"Corollary 1 (Trainability of ultra-wide GCNs). Consider a GCN of the form (6) and (7), with
depth l, number of non-linear transformations r, an MSE loss, and a Lipchitz activation, trained
with gradient descent on a node classiﬁcation task. Then, the output function follows, ft(X) ="
REFERENCES,0.791869918699187,"e−ηΘ(l)
(r)(G)tf0(X)+(I −e−ηΘ(l)
(r)(G)tY ), where X and Y are node features and labels from training
set. Then, Θ(l)
(r)(G) is singular when l →∞. Moreover, there exists a constant C > 0 such that for
all t > 0, ∥ft(X) −Y ∥> C."
REFERENCES,0.7934959349593496,Published as a conference paper at ICLR 2022
REFERENCES,0.7951219512195122,"Proof. According to the result from Jacot et al. (2018), GNTK Θ(l)
(r)(G) converges to a deterministic
kernel and remains constant during gradient descent in the inﬁnite-width limit. We omit the proof
procedure for this result, since it is now a standard conclusion in the NTK study."
REFERENCES,0.7967479674796748,"Based on the conclusion above, Lee et al. (2019a) proved that the inﬁnitely-wide neural network is
equivalent to its linearized mode,"
REFERENCES,0.7983739837398374,"f lin
t (X) = f0(X) + ∇θf0(X)|θ=θ0ωt"
REFERENCES,0.8,"where ωt = θt −θ0. We call it a linearized model because only zero and ﬁrst order term of Taylor
expansion are kept. Since we know the dynamics of gradient ﬂow using this linearized function are
governed by,
˙ωt = −η∇θf0(X)T ∇f lin
t
(X)L"
REFERENCES,0.8016260162601626,"˙f lin
t (X) = −ηΘ(l)
(r)(G)∇f lin
t
(X)L"
REFERENCES,0.8032520325203252,"where L is an MSE loss, then the above equations have closed form solutions"
REFERENCES,0.8048780487804879,"f lin
t (X) = e−ηΘ(l)
(r)(G)tf0(X) + (I −e−ηΘ(l)
(r)(G)t)Y"
REFERENCES,0.8065040650406504,"Since Lee et al. Lee et al. (2019a) showed that f lin
t (X) = ft(X) in the inﬁnite width limit, thus we
have,
ft(X) = e−ηΘ(l)
(r)(G)tf0(X) + (I −e−ηΘ(l)
(r)(G)t)Y
(21)"
REFERENCES,0.808130081300813,"In Theorem 1, we have shown Θ(l)
(r)(G) converges to a constant matrix at an exponential rate, thus
being singular in the large depth limit. According to Equation (21), we know that,"
REFERENCES,0.8097560975609757,"∥ft(X) −Y ∥= ∥e−ηΘ(l)
(r)(G)t(f0(X) −Y ∥"
REFERENCES,0.8113821138211382,"Let Θ(∞)
(r) be the GNTK in the large depth limit, Θ(∞)
(r) (G) = QT DQ be the decomposition of the"
REFERENCES,0.8130081300813008,"GNTK, where Q is an orthogonal matrix and D is a diagonal matrix. Because Θ(∞)
(r) (G) is singular,
D has at least one zero value dj = 0, then"
REFERENCES,0.8146341463414634,∥ft(X) −Y ∥= ∥QT (ft(X) −Y )Q∥≥∥[QT (f0(X) −Y )Q]j∥
REFERENCES,0.816260162601626,"Remark 3. In the proof we assume the loss is MSE. Nevertheless, the conclusion regarding train-
ability can be applied to other common losses such as cross-entropy. For cross-entropy loss, even
though we cannot derive an analytical expression for the solution, we can prove that the GNTK
still governs the trainability and the law of GNTK is not affected by the loss. Thus, the results for
trainability still hold in the cross-entropy case."
REFERENCES,0.8178861788617886,"D
CONVERGENCE OF RESIDUAL GNTK"
REFERENCES,0.8195121951219512,"D.1
RESIDUAL CONNECTION IN AGGREGATION"
REFERENCES,0.8211382113821138,"Theorem 2 (Convergence rate of residual connection in aggregation). Consider a GNTK of non-
linear transformation (9) and residual connection (12). Then with a stationary vector ˜⃗π(G) for
˜
A(G), there exist constants 0 < ˜α < 1 and C > 0, and constant vectors ⃗v and ⃗v′ depending on
R, such that
Θ(l)
(r)(u, u′) −˜⃗π(G)T  
Rl⃗v + ⃗v′ ≤C ˜αl. Furthermore, we denote the second largest"
REFERENCES,0.8227642276422764,"eigenvalue of A(G) and ˜
A(G) as λ2 and ˜λ2, respectively. Then, ˜λ2 > λ2."
REFERENCES,0.824390243902439,"Proof. According to the aggregation function for covariance matrix, we have"
REFERENCES,0.8260162601626017,⃗Θ(l)(G) = (1 −δ)A(G)⃗Θ(l−1)(G) + δ⃗Θ(l−1)(G)
REFERENCES,0.8276422764227642,"= ((1 −δ)A + δI)⃗Θ(l−1)(G) = ˜
A(G)⃗Θ(l−1)(G)"
REFERENCES,0.8292682926829268,Published as a conference paper at ICLR 2022
REFERENCES,0.8308943089430895,"The new aggregation matrix ˜
A(G) is a stochastic transition matrix as well, which can be seen from,
X"
REFERENCES,0.832520325203252,"j
˜
A(G)ij = (1 −δ)
X"
REFERENCES,0.8341463414634146,"j
A(G)ij + δ
X"
REFERENCES,0.8357723577235773,"j
Iij = 1"
REFERENCES,0.8373983739837398,"Then we compare the second largest eigenvalue between two transition matrices. Suppose the eigen-
values of the original matrix A(G) are {λ1 > λ2 > · · · > λn2}. We already know that the maximum
eigenvalue is λ1 = 1, and the converge rate is governed by the second largest eigenvalue λ2. Now
we consider the second largest eigenvalue ˜λ2 of matrix ˜
A:"
REFERENCES,0.8390243902439024,˜λ2 = (1 −δ)λ2 + δ = λ2 + δ(1 −λ2) > λ2
REFERENCES,0.8406504065040651,"Remark 4. The theorem above gives us a certain conclusion about the relationship between λ2 and
˜λ2. Here, we discuss more about the relationship between α and ˜α. According to (Rosenthal, 1995),
the relationship between convergence rate α and the second largest eigenvalue λ2 of transition
matrix A(G) can be expressed as,"
REFERENCES,0.8422764227642277,"Θ(l)
(r)(u, u′) −⃗π(G)T  
Rl⃗v + ⃗v′ ≤ClJ−1λl−J
2
≤Cαl"
REFERENCES,0.8439024390243902,"where α > λ2, and J > 1 is the size of the largest Jordan block of A(G). From the above inequality,
we can conclude that the second largest eigenvalue almost governs the convergence rate. However,
to maintain rigor, we cannot directly obtain α < ˜α. Instead, from the result that ˜λ2 > λ2, we say
that ˜α is roughly larger than α."
REFERENCES,0.8455284552845529,"D.2
RESIDUAL CONNECTION IN MLP"
REFERENCES,0.8471544715447155,"Theorem 3 (Convergence rate of GNTK with residual connection between transformations). Con-
sider a GNTK of the form (8) and (12). If A(G) is irreducible and aperiodic, with a stationary
distribution π(G), then there exist constants 0 < α < 1 and C > 0, and constant vectors v and v′"
REFERENCES,0.848780487804878,"depending on R, such that, |Θ(l)
(r)(u, u′) −⃗π(G)T ·
 
Rl(1 + σ2
w
2 )Rl⃗v + ⃗v′
| ≤Cαl."
REFERENCES,0.8504065040650407,"Proof. For residual connection in MLP, we have,"
REFERENCES,0.8520325203252033,"qr(u) = qr−1(u) + σ2
w
2 qr−1(u) = (1 + σ2
w
2 )qr−1(u)"
REFERENCES,0.8536585365853658,"Since σ2
w > 0, qr(u) grows at an exponential rate."
REFERENCES,0.8552845528455284,"Now, we turn to compute the correlation term Cr(u, u′). For convenience, we suppose qr(u) =
qr(u′). Then,"
REFERENCES,0.8569105691056911,"Cr+1(u, u′) = Σr+1(u, u′)"
REFERENCES,0.8585365853658536,"qr+1(u)
=
1"
REFERENCES,0.8601626016260162,"1 + σ2
w
2"
REFERENCES,0.8617886178861789,"Σr(u, u′)"
REFERENCES,0.8634146341463415,"qr(u)
+
1"
REFERENCES,0.865040650406504,"1 + σ2
w
2"
REFERENCES,0.8666666666666667,"σ2
w
2 f(Cr(u, u′)) =
1"
REFERENCES,0.8682926829268293,1 + σ2w
REFERENCES,0.8699186991869918,"2
Cr(u, u′) +"
REFERENCES,0.8715447154471545,"σ2
w
2
1 + σ2w"
REFERENCES,0.8731707317073171,"2
f(Cr(u, u′))"
REFERENCES,0.8747967479674796,"Using Taylor expansion of f near 1, as have been done in the proof of Lemma 2,"
REFERENCES,0.8764227642276423,"f(x)|x→1−= x + 2
√"
REFERENCES,0.8780487804878049,"2
3π (1 −x)3/2 + O((1 −x)5/2)"
REFERENCES,0.8796747967479674,"we have,"
REFERENCES,0.8813008130081301,"Cr+1(u, u′) = Cr(u, u′) + 2
√ 2
3π"
REFERENCES,0.8829268292682927,"σ2
w
2
1 + σ2w 2"
REFERENCES,0.8845528455284553,"
(1 −Cr(u, u′)3/2 + O((1 −Cr(u, u′)5/2))
"
REFERENCES,0.8861788617886179,Published as a conference paper at ICLR 2022
REFERENCES,0.8878048780487805,Note that it is similar to the case of MLP without residual connection:
REFERENCES,0.8894308943089431,"Cr+1(u, u′) = Cr(u, u′) + 2
√"
REFERENCES,0.8910569105691057,"2
3π

(1 −Cr(u, u′)3/2 + O((1 −Cr(u, u′)5/2))
"
REFERENCES,0.8926829268292683,"Following the proof diagram in Lemma 3 and Theorem 1, we can obtain the behavior of Σ(l)
(r)(u, u′)"
REFERENCES,0.8943089430894309,"and Θ(l)
(r)(u, u′) in the large depth limit,"
REFERENCES,0.8959349593495934,"Σ(l)
(r)(u, u′) −⃗π(G)T  
(1 + σ2
w
2 )Rl⃗v
 ≤Cαl"
REFERENCES,0.8975609756097561,"Θ(l)
(r)(u, u′) −⃗π(G)T  
Rl(1 + σ2
w
2 )Rl⃗v + ⃗v′ ≤Cαl"
REFERENCES,0.8991869918699187,"E
APPENDIX: DISCUSSION ON THE TRAINABILITY OF GNTK WITH
CRTICAL DROPEDGE"
REFERENCES,0.9008130081300812,"According to the critical percolation theory, there exists a large connected component of order O(n)
(Erd˝os & R´enyi, 1961; Erd¨os & R´enyi, 2011), where n is the number of nodes in a graph. This
implies that there exists a (connected) graph of size O(n) when we use the critical dropping rate.
To preserve information, edges would be resampled at every iteration. This implies that at each
iteration during training, the GNN takes a random and large graph. For the whole training process,
we can think that we have used all the information conveyed by the graph."
REFERENCES,0.9024390243902439,"It would be interesting to consider deriving some theoretical results for the limiting behavior of the
GNTK in the large depth with Critical DropEdge. Since Critical DropEdge is originated from the
random graph theory, a promising approach is to analyze spectral distributions of adjacency matrix
and Laplacian matrix with a random graph model, like existing studies (Ding & Jiang, 2010; Chung
& Radcliffe, 2011)."
REFERENCES,0.9040650406504065,"F
APPENDIX: DATASETS AND IMPLEMENTATION DETAILS"
REFERENCES,0.9056910569105691,"F.1
DATASETS"
REFERENCES,0.9073170731707317,"For node classiﬁcation tasks, Cora, Citeseer and Pubmed (Kipf & Welling, 2017)) are three com-
monly used citation networks, and Physics (Shchur et al., 2018) is a co-author network. Detailed
information of the four benchmark datasets is listed as follows and summarized in Table 3."
REFERENCES,0.9089430894308943,"• The Cora dataset consists of 2,708 scientiﬁc publications classiﬁed into one of seven
classes, and 5,429 links. Each publication is described by a 0/1-valued word vector indi-
cating the absence/presence of the corresponding word from the dictionary. The dictionary
consists of 1,433 unique words."
REFERENCES,0.9105691056910569,"• The Citeseer dataset consists of 3,312 scientiﬁc publications classiﬁed into one of six
classes, and 4,732 links. Each publication is described by a 0/1-valued word vector indi-
cating the absence/presence of the corresponding word from the dictionary. The dictionary
consists of 3,703 unique words."
REFERENCES,0.9121951219512195,"• The Pubmed Diabetes dataset consists of 19,717 scientiﬁc publications from PubMed
database pertaining to diabetes classiﬁed into one of three classes. The citation network
consists of 44,338 links. Each publication is described by a TF-IDF weighted word vector
from a dictionary comprised of 500 unique words."
REFERENCES,0.9138211382113821,"• The Physics dataset consists of 34,493 authors as nodes, and edges indicate whether two
authors have co-authored a paper. Node features are paper keywords from the author’s
papers. Following Kim & Oh (2020), we reduce the original dimension (6,805 and 8,415)
to 500 using PCA. The split is the 20-per-class/30-per-class/rest. The goal of this task is to
classify each author’s respective ﬁeld of study."
REFERENCES,0.9154471544715447,Published as a conference paper at ICLR 2022
REFERENCES,0.9170731707317074,"Table 3: Details of Datasets
Dataset
Nodes
Edges
Classes
Features
Critical Edge Sampling
Train/Val/Test"
REFERENCES,0.9186991869918699,"Cora
2,708
5,429
7
1,433
24.94%
0.05/0.18/0.37
Citeseer
3,327
4,732
6
3,703
35.15%
0.04/0.15/0.30
Pubmed
19,717
44,338
3
500
22.23%
0.003/0.03/0.05
Physics
34,493
247,962
5
500
6.96 %
0.003/0.004/0.99"
REFERENCES,0.9203252032520325,Table 4: Comparison results of test accuracy (%) with different inﬁnite-wide backbones.
REFERENCES,0.9219512195121952,"Dataset Backbone
4-layer
8-layer
32-layer
64-layer
Orignal C-DropEdge Original C-DropEdge Original C-DropEdge Original C-DropEdge"
REFERENCES,0.9235772357723577,"Cora
GCN
79.98
79.98
79.48
80.61
73.75
75.86
72.59
74.03
JKNet
78.27
77.29
80.26
80.02
78.00
79.01
75.90
76.75"
REFERENCES,0.9252032520325203,"Citeseer
GCN
66.38
66.97
54.42
64.88
50.72
51.22
43.77
46.49
JKNet
67.04
67.67
65.54
66.45
54.57
56.39
46.71
47.96"
REFERENCES,0.926829268292683,"Pubmed
GCN
74.59
74.77
73.58
76.67
71.63
74.77
66.28
71.72
JKNet
75.30
74.35
76.42
77.17
75.71
76.68
74.81
75.74"
REFERENCES,0.9284552845528455,"F.2
EXPERIMENTAL IMPLEMENTATION"
REFERENCES,0.9300813008130081,We use the PyTorch implementation to simulate both inﬁnitely-wide and ﬁnitely-wide GCNs:
REFERENCES,0.9317073170731708,"• The inﬁnitely-wide GCNs use part of code from Du et al. (2019b), which is originally
adopted for graph classiﬁcation. We redesign the calculation method of GNTK (Du et al.,
2019b)2 according to the formula in Section 3.1 and use it to process node classiﬁcation
tasks."
REFERENCES,0.9333333333333333,"• For ﬁnitely-wide GNNs with DropEdge (Rong et al., 2019)3, we use the code from (Rong
et al., 2019), we perform random hyper-parameter search for each model, and report the
case giving the best accuracy on validation set of each benchmark, following the same
strategy as Rong et al. (2019). The difference is that, in each experiment, we ﬁx the edge
sampling percentage as ρ =
|V |
2|E|, which listed in the last column of Table 3."
REFERENCES,0.9349593495934959,"All codes mentioned above use the MIT license. All experiments are conducted on two Nvidia
Quadro RTX 6000 GPUs."
REFERENCES,0.9365853658536586,"G
APPENDIX: ADDITIONAL EXPERIMENTS AND DISCUSSION"
REFERENCES,0.9382113821138212,"G.1
INFINITELY-WIDE BACKBONES"
REFERENCES,0.9398373983739837,"In the main paper, we reported the performance of GCNs with ﬁnite-width, here we show more
results with respect to inﬁnitely-wide GCNs, as shown in Table 4. We apply Critical DropEdge
to inﬁnitely-wide backbones on semi-supervised node classiﬁcation. We consider two backbones,
which are GCN (Kipf & Welling, 2017) and JKNet (Xu et al., 2018), and the edge preserving per-
centage ρ(G) is shown in Table 3. First, we calculate a GNTK with respect to the graph data and
GNN. Then, we apply kernel regression with calculated GNTK to complete node classiﬁcation task."
REFERENCES,0.9414634146341463,"Table 4 summaries the performance on three citation networks. In this table, we report the perfor-
mance of GCNs with 4/8/32/64 layers. It is shown that, in most cases, using Critical DropEdge
(C-DropEdge) can achieve better results than original GNNs in the inﬁnitely-wide case, especially
when the model is deep."
REFERENCES,0.943089430894309,"2We use the implementation of GNTK available at https://github.com/KangchengHou/gntk.
3We use the DropEdgeimplementation available at https://github.com/DropEdge/DropEdge."
REFERENCES,0.9447154471544715,Published as a conference paper at ICLR 2022
REFERENCES,0.9463414634146341,Table 5: Comparison results of test accuracy (%) between C-DropEdge and DropEdge.
REFERENCES,0.9479674796747968,"Dataset
Backbone
4-layer
8-layer
Finite
Original
DropEdge
C-DropEdge
Original
DropEdge
C-DropEdge"
REFERENCES,0.9495934959349593,"Cora
GCN
79.8 ± 1.1
80.4 ± 1.4
82.0 ± 1.2
73.2 ± 2.7 75.1 ± 2.4
77.3 ± 2.5
JKNet
81.1 ± 1.0
82.2 ± 0.7
82.5 ± 0.7
80.9 ± 1.2 82.0 ± 0.9
82.1 ± 0.5
IncepGCN
80.0 ± 0.9
80.6 ± 1.2
82.4 ± 0.5
78.6 ± 1.7 81.2 ± 1.3
82.3 ± 0.6"
REFERENCES,0.9512195121951219,"Citeseer
GCN
61.2 ± 3.0
63.7 ± 2.5
69.0 ± 0.8
50.2 ± 5.7 52.8 ± 5.1
54.1 ± 5.9
JKNet
69.6 ± 1.2
70.2 ± 1.0
70.3 ± 0.7
70.7 ± 1.0 70.5 ± 1.1
70.8 ± 1.2
IncepGCN
69.4 ± 1.5
70.0 ± 1.0
70.8 ± 0.6
69.0 ± 1.2 70.8 ± 1.1
70.9 ± 0.5"
REFERENCES,0.9528455284552846,"Pubmed
GCN
77.4 ± 0.7
77.6 ± 1.4
78.0 ± 0.4
57.2 ± 8.4 57.5 ± 6.1
58.9 ± 6.9
JKNet
76.5 ± 0.9
77.1 ± 1.1
77.4 ± 1.0
76.1 ± 1.4 76.6 ± 1.0
76.6 ± 0.9
IncepGCN
76.7 ± 1.7
77.4 ± 1.5
77.6 ± 1.1
77.5 ± 1.3 77.3 ± 1.2
77.9 ± 1.0"
REFERENCES,0.9544715447154472,"Physics
GCN
90.2 ± 0.9
91.6 ± 0.8
91.9 ± 0.7
83.5 ± 2.2 86.3 ± 1.8
85.2 ± 2.3
JKNet
90.6 ± 1.7
91.0 ± 0.9
91.5 ± 0.4
90.4 ± 1.5 91.5 ± 0.7
91.7 ± 0.6
IncepGCN
91.0 ± 1.1
91.4 ± 0.5
91.5 ± 0.3
90.5 ± 0.8 91.4 ± 0.8
91.5 ± 0.6"
REFERENCES,0.9560975609756097,"Dataset
Backbone
16-layer
32-layer
Finite
Original
DropEdge
C-DropEdge
Original
DropEdge
C-DropEdge"
REFERENCES,0.9577235772357724,"Cora
GCN
36.3 ± 13.8 55.1 ± 5.2
58.5 ± 3.9
20.1 ± 2.4 22.1 ± 2.0
24.7 ± 1.8
JKNet
79.9 ± 1.6
82.2 ± 0.7
82.4 ± 0.8
80.4 ± 1.4 82.1 ± 0.5
82.6 ± 0.9
IncepGCN
78.7 ± 1.0
80.2 ± 1.3
82.0 ± 0.8
78.5 ± 1.8 80.9 ± 0.8
81.6 ± 0.9"
REFERENCES,0.959349593495935,"Citeseer
GCN
30.8 ± 2.2
35.7 ± 1.9
36.6 ± 2.0
21.7 ± 3.0 23.1 ± 1.0
25.2 ± 0.9
JKNet
69.2 ± 1.2
68.8 ± 1.6
69.5 ± 1.0
68.1 ± 2.3 69.9 ± 1.4
70.1 ± 0.6
IncepGCN
68.4 ± 1.2
70.7 ± 1.0
71.0 ± 1.0
68.6 ± 1.9 70.2 ± 0.8
70.7 ± 0.9"
REFERENCES,0.9609756097560975,"Pubmed
GCN
39.5 ± 10.3 37.0 ± 9.6
42.6 ± 6.4
36.3 ± 8.4 37.4 ± 8.8
38.5 ± 6.1
JKNet
76.6 ± 0.9
76.1 ± 0.8
76.9 ± 0.9
77.1 ± 0.8 77.0 ± 0.9
77.2 ± 1.0
IncepGCN
76.5 ± 1.5
76.7 ± 1.3
77.2 ± 1.0
77.0 ± 1.4 77.2 ± 1.3
77.8 ± 1.0"
REFERENCES,0.9626016260162602,"Physics
GCN
41.6 ± 6.2
45.8 ± 6.3
46.2 ± 5.7
28.8 ± 9.4 31.1 ± 8.8
36.2 ± 8.4
JKNet
90.3 ± 1.0
91.2 ± 0.5
91.5 ± 0.4
90.6 ± 1.0 91.3 ± 0.8
91.6 ± 0.6
IncepGCN
91.4 ± 0.4
92.0 ± 0.5
92.0 ± 0.4
OOM
OOM
OOM"
REFERENCES,0.9642276422764228,OOM means Out-Of-Memory.
REFERENCES,0.9658536585365853,"G.2
PERFORMANCE OF USING DIFFERENT BACKBONES"
REFERENCES,0.967479674796748,"The results presented in Table 1 are the best across different backbones. Table 5 further compares
the results of vanilla GNN, DropEdge and C-DropEdge for three different backbones (GCN, JKNet
and IncepGCN). From the table we can conclude that Critical DropEdge consistently outperforms
DropEdge and vanilla GNNs."
REFERENCES,0.9691056910569106,"G.3
TRAINABILITY OF GNTK WITH CRITICAL DROPEDGE"
REFERENCES,0.9707317073170731,"We implement deep GCNs with/without DropEdge and C-DropEdge to compare the training per-
formance. The results are shown in Figure 4. The training loss of GCN and GCN with DropEdge
has a slower convergence rate and converges to a larger error compared to GCN with C-DropEdge.
This indicates that C-DropEdge can achieve better trainability compared to GCN and GCN with
DropEdge. Besides, the convergence results of GNTKs with C-DropEdge are shown in Figure 5.
Compared to residual connection, we ﬁnd that the normalized GNTK elements would not converge
to a single value, and the convergence rate curve does not depend on the depth. This implies that
residual connection mitigates the trainability loss by slowing down the exponential convergence rate,
while C-DropEdge directly changes the convergence results."
REFERENCES,0.9723577235772358,Published as a conference paper at ICLR 2022
REFERENCES,0.9739837398373984,"0
100
200
300
400
Epochs 0.0 0.5 1.0 1.5 2.0"
REFERENCES,0.975609756097561,Training Loss
REFERENCES,0.9772357723577236,(a) Cora GCN-8
REFERENCES,0.9788617886178862,"NoDrop
DropEdge
C-DropEdge"
REFERENCES,0.9804878048780488,"0
100
200
300
400
Epochs 0.0 0.5 1.0 1.5"
REFERENCES,0.9821138211382113,"2.0
(b) Citeseer GCN-12"
REFERENCES,0.983739837398374,"NoDrop
DropEdge
C-DropEdge"
REFERENCES,0.9853658536585366,"Figure 4: Training loss as a function of epochs. (a) We implement 8-layer GCN, GCN with DropE-
dge, GCN with C-DropEdge on Cora. (b) 12-layer GCN, GCN with DropEdge, GCN with C-
DropEdge on Citeseer. The training loss of GCN and GCN with DropEdge has a slower convergence
rate and converges to a larger error compared to GCN with C-DropEdge."
REFERENCES,0.9869918699186991,"0
200
400
600
800
Depth 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9886178861788618,Normalized GNTK
REFERENCES,0.9902439024390244,(a) C-DropEdge GNTK Collapse
REFERENCES,0.991869918699187,"0
200
400
600
800
Depth 10−4 10−2 100"
REFERENCES,0.9934959349593496,Element-Wise Distance of GNTK
REFERENCES,0.9951219512195122,(b) C-DropEdge Convergence Rate
REFERENCES,0.9967479674796748,y=exp(-0.15x)
REFERENCES,0.9983739837398374,"Figure 5: Convergence rate of GNTK with C-DropEdge. (a) Elements of the normalized (residual
connection) GNTK as a function of the depth, deﬁned as Rl + r. Different elements tend to have
different value as depth grows (b) The element-wide distance of the normalized GNTK as a function
of the depth. The converge rate is no longer bounded by an exponential function. Instead, it remains
parallel to the horizontal depth axis."
