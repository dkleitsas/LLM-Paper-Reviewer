Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0022935779816513763,"Federated learning (FL) has been widely studied as a new paradigm to achieve
multi-party collaborative modelling on decentralized data with privacy protection.
Unfortunately, traditional horizontal FL suffers from Non-IID data distribution,
where clients’ private models after FL are even inferior to models trained stan-
dalone. To tackle this challenge, most existing approaches focus on personalized
federated learning (PFL) to improve personalized private models but present lim-
ited accuracy improvements. To this end, we design pFedKT, a novel personal-
ized federated learning framework with private and global knowledge transfer,
towards boosting the performances of personalized private models on Non-IID
data. It involves two types of knowledge transfer: a) transferring historical pri-
vate knowledge to new private models by local hypernetworks; b) transferring
the global model’s knowledge to private models through contrastive learning. Af-
ter absorbing the historical private knowledge and the latest global knowledge,
the personalization and generalization of private models are both enhanced. Be-
sides, we derive pFedKT’s generalization and prove its convergence theoretically.
Extensive experiments verify that pFedKT presents 1.38% −1.62% accuracy im-
provements of private models compared with the state-of-the-art baseline."
INTRODUCTION,0.0045871559633027525,"1
INTRODUCTION
With frequent privacy leakage, directly collecting data and modelling it would violate privacy pro-
tection regulations such as GDPR (Kairouz & et al., 2021). To implement collaborative modelling
while protecting data privacy, horizontal federated learning (FL) came into being (McMahan & et al,
2017). As shown in Fig. 1 (a), FL consists of a central server and multiple clients. In each communi-
cation round, the server broadcasts the global model (abbr. GM) to selected clients; then clients train
it locally on their local datasets and upload trained private models (abbr. PMs) to the server; finally,
the server aggregates received private models to update the global model. The whole procedure is
repeated until the global model converges. In short, FL fulfils collaborative modelling by allowing
clients to only communicate model updates with the server, while data is always stored locally."
INTRODUCTION,0.006880733944954129,"However, FL still faces several challenges such as communication efficiency, robustness to attacks,
and model accuracy which we focus on in this work. The motivation for clients to participate in
FL is to improve their local models’ quality. However, the decentralized data held by clients are
often not independent and identically distributed (Non-IID) (Kairouz & et al., 2021), and the global
model aggregated through a typical FL algorithm FedAvg (McMahan & et al, 2017) based on Non-
IID data may perform worse than clients’ solely trained models. Zhao & et al (2018) have verified
this fact experimentally and argued that the global model aggregated by skewed local models trained
on Non-IID data deviates from the optima (model trained on all local data). To alleviate the accuracy
degradation caused by Non-IID data, personalized FL (PFL) methods (Shamsian & et al, 2021) have
been widely studied to improve clients’ personalized model quality."
INTRODUCTION,0.009174311926605505,"Existing researches implement PFL by fine-tuning Mansour & et al (2020); Wang & et al (2019),
model mixup Arivazhagan & et al (2019); Collins & et al (2021) and etc. But they suffer from
limited improvements in the accuracy of private models."
INTRODUCTION,0.011467889908256881,"To further improve personalized private models on Non-IID data, we propose a novel personalized
FL framework named pFedKT with two types of transferred knowledge: 1) private knowledge: we
deploy a local hypernetwork for each client to transfer historical PMs’ knowledge to new PMs; 2)
global knowledge: we exploit contrastive learning to enable PMs to absorb the GM’s knowledge.
We analyzed pFedKT’s generalization and proved its convergence theoretically. We also conducted
extensive experiments to verify that pFedKT fulfils the state-of-the-art PM’s accuracy."
INTRODUCTION,0.013761467889908258,"Contributions. Our main contributions are summarized as follows: a) We devised two types of
knowledge transfer to simultaneously enhance the generalization and personalization of private mod-
els. b) We analyzed pFedKT’s generalization and convergence in theory. c) Extensive experiments
verified the superiority of pFedKT on the accuracy of personalized private models."
INTRODUCTION,0.016055045871559634,Under review as a conference paper at ICLR 2023
INTRODUCTION,0.01834862385321101,"GM𝑡𝑡
𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡"
INTRODUCTION,0.020642201834862386,"𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡
GM𝑡𝑡
𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴"
INTRODUCTION,0.022935779816513763,𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡
INTRODUCTION,0.02522935779816514,𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺
INTRODUCTION,0.027522935779816515,"𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑘𝑘
𝐷𝐷𝑘𝑘"
INTRODUCTION,0.02981651376146789,(a) FedAvg 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200
INTRODUCTION,0.03211009174311927,Rounds
INTRODUCTION,0.034403669724770644,"GM
PM-0
PM-1
PM-2
PM-3
PM-4
PM-5
PM-6
PM-7
PM-8
PM-9
PM-10
PM-11
PM-12
PM-13
PM-14
PM-15
PM-16
PM-17
PM-18
PM-19"
INTRODUCTION,0.03669724770642202,Models
INTRODUCTION,0.0389908256880734,Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0
INTRODUCTION,0.04128440366972477,"(b) GM v.s. PM
Figure 1: (a): Workflow of FedAvg in the t-th round. (b): The test accuracy of the GM and 20 PMs
are recorded per 10 rounds. Since the server has no data, we evaluate the test accuracy of the GM
on clients’ test datasets and calculate mean test accuracy as the GM’s accuracy. We evaluate the test
accuracy of a local model PM after local training on its local test data as the PM’s accuracy."
LIT REVIEW,0.04357798165137615,"2
RELATED WORK"
LIT REVIEW,0.045871559633027525,"Recent personalized federated learning (PFL) approaches include: a) Fine-tuning, in FL’s last
round, clients fine-tune the received GM on local data to get PMs (Wang & et al, 2019; Man-
sour & et al, 2020). b) Federated meta-learning, some methods apply meta-learning in FL, such
as MAML-based distributed variants (Li & et al, 2017; Fallah & et al, 2020b;a). c) Federated
multi-task learning, it treats each client as a learning task, e.g., MOCHA (Smith & et al, 2017),
FedU (Dinh & et al, 2021). d) Model mixup, the PM’s parameters are split into two parts, only
one part is shared through the server and another is trained locally, as in FedPer (Arivazhagan &
et al, 2019), FedFu (Yao et al., 2019), FLDA (Peterson et al., 2019), LG-FEDAVG (Liang & et al,
2020), MAPPER (Mansour et al., 2020), FedRep (Collins & et al, 2021), pFedGP (Achituve & et al,
2021), (Sun & et al, 2021). e) Aggregation Delay, RADFed Xue et al. (2021) proposed redistri-
bution rounds that delay aggregation to alleviate the negative impacts on model performance due to
Non-IID data. f) Federated clustering, the server clusters PMs with similar parameter distributions
and performs aggregation within clusters, e.g., HYPCLUSTER (Mansour & et al, 2020), ClusterFL
(Ouyang & et al, 2021), CFL (Agrawal & et al, 2021). g) Local aggregation, instead of aggrega-
tion within the server’s clustered groups, FedFOMO (Zhang & et al, 2021) makes each client pull
other clients’ PMs and selects more beneficial ones for local aggregation to update its own PMs.
h) Knowledge distillation-based, FedPHP (Li & et al., 2021) linearly accumulates historical PMs
and new trained PMs to teach the received GM through knowledge distillation in each round of
FL. FML (Shen et al., 2020) makes each client’s PM interact with the GM through mutual learn-
ing. KT-pFL (Zhang et al., 2021) allocates a public dataset to each client, and only logits computed
on the public dataset are shared through the server. i) Contrastive learning-based, MOON (Li
et al., 2021) utilizes contrastive learning to make PMs close to the GM, towards obtaining a better
GM. j) Hypernetwork-based, pFedHN Shamsian & et al (2021) deploys a global hypernetwork
on the server to learn PMs’ parameter distributions and generate personalized parameters for PMs.
The latest work Fed-RoD (Chen & Chao, 2022) trains private personalized headers with parameters
generated by local hypernetworks. It improves both the GM and PMs, but extra communication cost
incurs by communicating hypernetworks."
LIT REVIEW,0.0481651376146789,"3
PRELIMINARIES AND MOTIVATION"
LIT REVIEW,0.05045871559633028,"3.1
UTILITY OF PRIVATE MODELS"
LIT REVIEW,0.052752293577981654,"As the workflow of FedAvg shown in Fig.1 (a), we abbreviate the private model as PM and the global
model as GM, and the detailed definition of FL is introduced in Appendix A. It’s worth noting that:
in FedAvg, 1) clients no longer store PMs after uploading them to the server; 2) in the next round,
clients regard the received GM as PM and then train PM on local datasets, i.e., the trained PMs only
play as “temporary models” for aggregation and their utilities are not sufficiently developed."
LIT REVIEW,0.05504587155963303,"To explore the utilities of PMs, we train a CNN model on a natural Non-IID FEMINIST dataset in an
FL system with 20 clients. From Fig.1 (b), we observe that there are always some PMs performing
better than GM in each round (some PMs show lighter pixels than GM), so we can further develop
PMs’ self-utility during FL to boost the accuracy of personalized private models."
LIT REVIEW,0.05733944954128441,Under review as a conference paper at ICLR 2023
LIT REVIEW,0.05963302752293578,"32×32×3
14×14×16 14×14×32
5×5×32 1920 80"
LIT REVIEW,0.06192660550458716,"10
28×28×16
32×32×3
14×14×16 14×14×32
5×5×32 1920 80"
LIT REVIEW,0.06422018348623854,"10
28×28×16"
LIT REVIEW,0.06651376146788991,"Target 
Model"
LIT REVIEW,0.06880733944954129,Hypernetwork
LIT REVIEW,0.07110091743119266,100 100 100 400
LIT REVIEW,0.07339449541284404,embedding
LIT REVIEW,0.07568807339449542,"conv
conv
pool
pool
fc
fc
fc (a)"
LIT REVIEW,0.0779816513761468,"0
100
200
300
Rounds 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
LIT REVIEW,0.08027522935779817,Train Loss
LIT REVIEW,0.08256880733944955,CIFAR-10
LIT REVIEW,0.08486238532110092,"NN without HN
NN with HN (b)"
LIT REVIEW,0.0871559633027523,"0
100
200
300
Rounds 0.0 0.2 0.4 0.6 0.8 1.0"
LIT REVIEW,0.08944954128440367,Test Accuracy
LIT REVIEW,0.09174311926605505,CIFAR-10
LIT REVIEW,0.09403669724770643,"NN without HN
NN with HN"
LIT REVIEW,0.0963302752293578,"(c)
Figure 2: (a): A hypernetwork generates parameters for a target model. (b): the test accuracy of two
target models (NN with/out HN) vary as communication rounds on CIFAR-10 dataset."
LIT REVIEW,0.09862385321100918,"3.2
MOTIVATION
3.2.1
PRIVATE KNOWLEDGE ENHANCEMENT"
LIT REVIEW,0.10091743119266056,"To fully utilize PMs’ self-utility, FedPHP linearly accumulates historical PMs and new trained PMs
to teach the received GM through knowledge distillation in each round of FL. Since historical private
models with obsolete parameters compromise convergence, linearly stacking them with manually
selected weights may suffer from degraded accuracy, which has been verified in our subsequent
experiments (Sec. 6.2). A hypernetwork (abbr. HN) (Ha & et al, 2017) is generally a small model
that generates parameters for large target models, belonging to the category of generative networks
in unsupervised learning. pFedHN and Fed-RoD exploit hypernetworks to personalize PMs, but
they have a few weaknesses. In pFedHN, the footprint of the server’s hypernetwork is designed to
be larger than that of PMs, especially, the hypernetwork’s output layer’s parameter capacity is equal
to that of a complete PM, which burdens the storage cost and computation overhead of the server.
Fed-RoD requires to communicate local hypernetworks (which generate parameters for personalized
headers) between clients and the server, introducing extra communication cost which is the main
bottleneck of FL (Kairouz & et al., 2021)."
LIT REVIEW,0.10321100917431193,"To enhance personalization while avoiding the above problems, our pFedKT attempts to allocate a
local hypernetwork for each client to learn its PM’s historical private knowledge (parameter dis-
tributions) and then generate parameters for its complete private model. In other words, we use
the local hypernetwork to accumulate historical PM’s knowledge to develop PMs’ self-utility. The
clients’ local hypernetworks used in our pFedKT are simple fully connected networks with lower
parameter capacity than PMs, so it has lower computation complexity than the server’s large hyper-
network in pFedHN. Besides, the server and clients still communicate PMs in pFedKT, which has a
communication cost the same as FedAvg and lower than Fed-RoD."
LIT REVIEW,0.10550458715596331,"To verify the feasibility of the above insight, we conduct preliminary experiments on a single client.
Specifically, we train a randomly initialized hypernetwork (abbr. HN) and a target model (abbr. NN,
i.e., PM) with parameters generated by the hypernetwork in an end-to-end form on the CIFAR-10
dataset. How to use hypernetworks to generate parameters for target models and how to update
hypernetwork are detailed in Appendix B and C. Fig. 2 displays the structures of the target model
and hypernetwork, as well as the experimental results. We observe that the final test accuracies of
the solely trained target model (NN without HN) and the target model trained with the hypernetwork
(NN with HN) are 91.84% and 93.84%, respectively, i.e., the latter performs better, indicating that
regarding HN as a meta-model to continually learn PM’s historical knowledge boosts PM’s accuracy.
Therefore, we can safely utilize the local hypernetwork to accumulate PM’s private knowledge."
LIT REVIEW,0.10779816513761468,"3.2.2
GLOBAL KNOWLEDGE ENHANCEMENT
To train a better GM on Non-IID data, MOON Li et al. (2021) utilizes contrastive learning to keep
the current round’s PM (anchor) close to the received GM (positive) and away from the last round’s
PM (negative). Since it initializes the current round’s PM (anchor) with GM’s parameters, the PM’s
personalization is impaired. Nevertheless, it still Inspires us to transfer global knowledge to PMs."
LIT REVIEW,0.11009174311926606,"Unlike MOON, our pFedKT regards the PM generated by a hypernetwork which has absorbed
historical private knowledge as the anchor in contrastive learning. And it also keeps the generated
PM close to GM for acquiring global knowledge. This discrepancy facilitates the effective fusion of
the latest global knowledge and historical private knowledge, promoting PM’s personalization and
generalization in available classes of local datasets."
LIT REVIEW,0.11238532110091744,Under review as a conference paper at ICLR 2023 s GM𝑡𝑡
LIT REVIEW,0.11467889908256881,"෪
𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡"
LIT REVIEW,0.11697247706422019,"𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡−1"
LIT REVIEW,0.11926605504587157,"𝐻𝐻𝐻𝐻𝑘𝑘
𝑡𝑡"
LIT REVIEW,0.12155963302752294,"𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡"
LIT REVIEW,0.12385321100917432,"𝑑𝑑𝐾𝐾𝐾𝐾
+"
LIT REVIEW,0.12614678899082568,"𝑑𝑑𝐾𝐾𝐾𝐾
−"
LIT REVIEW,0.12844036697247707,𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝐶𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙
LIT REVIEW,0.13073394495412843,"−
𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑆𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙 +"
LIT REVIEW,0.13302752293577982,"𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡
GM𝑡𝑡+1
𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴"
LIT REVIEW,0.1353211009174312,𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡
LIT REVIEW,0.13761467889908258,𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺𝑺
LIT REVIEW,0.13990825688073394,𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑪𝑘𝑘 GM𝑡𝑡
LIT REVIEW,0.14220183486238533,"෪
𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡"
LIT REVIEW,0.1444954128440367,"𝑑𝑑𝐾𝐾𝐾𝐾
+"
LIT REVIEW,0.14678899082568808,𝐥𝐥𝐥𝐥𝐥𝐥𝐥𝐥𝐥𝐥𝐥𝐥output
LIT REVIEW,0.14908256880733944,"𝑑𝑑𝐾𝐾𝐾𝐾(ℛGM𝑡𝑡(𝐷𝐷𝑘𝑘) ∥ℛ෪
𝑃𝑃𝑃𝑃𝑘𝑘
𝑡𝑡(𝐷𝐷𝑘𝑘)) ⑥ ① ② ③ ④ ⑤"
LIT REVIEW,0.15137614678899083,e𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚
LIT REVIEW,0.1536697247706422,ℎ𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙
LIT REVIEW,0.1559633027522936,ℎ𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑖𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙𝑙
LIT REVIEW,0.15825688073394495,𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜𝑜
LIT REVIEW,0.16055045871559634,"𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐
𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐
𝑓𝑓𝑓𝑓
𝑓𝑓𝑓𝑓"
LIT REVIEW,0.1628440366972477,"𝑓𝑓𝑓𝑓
𝑓𝑓𝑓𝑓
𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐
𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐"
LIT REVIEW,0.1651376146788991,𝑯𝑯𝑯𝑯𝒌𝒌
LIT REVIEW,0.16743119266055045,"𝒕𝒕: Hypernetwork of k-th client in (t)-th round
෪
𝑷𝑷𝑷𝑷𝒌𝒌"
LIT REVIEW,0.16972477064220184,"𝒕𝒕: Private Model generated by HN in (t)-th round
𝑮𝑮𝑮𝑮𝒕𝒕: Global model aggregated in (t-1)-th round
𝑷𝑷𝑷𝑷𝒌𝒌"
LIT REVIEW,0.1720183486238532,"𝒕𝒕−𝟏𝟏: Private Model after training in (t-1)-th round
𝑷𝑷𝑷𝑷𝒌𝒌"
LIT REVIEW,0.1743119266055046,𝒕𝒕: Private Model after training in (t)-th round 𝜇𝜇∗
LIT REVIEW,0.17660550458715596,(1 −𝜇𝜇) ∗𝑃𝑃𝑡
LIT REVIEW,0.17889908256880735,"𝐻𝐻𝐻𝐻𝑘𝑘
𝑡𝑡 ⑦ ⑧"
LIT REVIEW,0.1811926605504587,"𝐷𝐷𝑘𝑘
𝐷𝐷𝑘𝑘"
IMPLEMENTATION/METHODS,0.1834862385321101,"Figure 3: Left: workflow of pFedKT. Right: KL divergence between two models is calculated
on logits Rω·(Dk); and the local hypernetwork used in pFedKT is a simple fully connected (FC)
network with much smaller footprint than that of PM.
4
METHODOLOGY
In this section, we first outline pFedKT’s workflow, and then detail the two types of knowledge
transfer: a) private knowledge transfer by local hypernetworks and b) global knowledge transfer by
contrastive learning. Finally, we analyze pFedKT’s computational budgets and storage costs.
4.1
OVERVIEW"
IMPLEMENTATION/METHODS,0.18577981651376146,"Principle. Building on the above motivations, we devise a novel personalized FL framework named
pFedKT, which involves two types of knowledge transfer: 1) Transferring knowledge from old
PM to new PM. We configure a local hypernetwork (with much smaller footprint than that of PM)
for each client to learn the old PM’s knowledge and transfer it to the new PM. 2) Transferring
knowledge from GM to PM. We exploit contrastive learning to keep the new PM (have carried
old private knowledge) close to GM. In this way, during each round of local training, the private
knowledge from the old PM, the global knowledge from the latest GM, and absolutely the knowl-
edge from local data are simultaneously incorporated into the trained new PM, which facilitates
personalization and generalization in available classes."
IMPLEMENTATION/METHODS,0.18807339449541285,"Workflow. Specifically, as displayed in Fig. 3, the complete workflow of pFedKT framework in-
cludes the following steps: in the t-th round, a) the server first broadcasts the global model GM t"
IMPLEMENTATION/METHODS,0.19036697247706422,"to selected clients. 2) The k-th client uses its local hypernetwork HN t
k (with the embedding of
our pre-divided chunk id as input) to generate parameters for the target private model PM t0
k (to be
trained) in a stacked manner, where we detailed the generation procedure in Appendix B. 3) Then,
we regard the generated private model PM t0
k as the anchor, the received global model GM t as a
positive item, the last round’s trained private model PM t−1
k
as a negative item. The k-th client
computes the distances d+
KL and d−
KL measured by KL (Kullback-Leibler) divergence Kullback &
Leibler (1951) from PM t0
k to GM t and PM t−1
k
, respectively. After that, the k-th client computes
the contrastive loss with the two distances. 4) The k-th client trains PM t0
k with contrastive loss and
supervised loss from labels on local dataset Dk. After training, the trained private model is marked
as PM t
k. 5) The k-th client updates the old private model PM t−1
k
with the latest trained private
model PM t
k. 6) The k-th client updates the hypernetwork HN t
k to HN t+1
k
with the parameter vari-
ations between generated PM t0
k and trained PM t
k, where we detailed the hypernetwork’s updating
procedure in Appendix C. 7) The k-th client uploads the trained private model PM t
k to the server.
8) The server aggregates the received private models [PM t
k, · · · ] through the weighted aggregation
rule of FedAvg, and updates the global model to be GM t+1. The above steps are executed iteratively
until PMs converge. In the end, we acquire personalized PMs. A detailed algorithm description of
pFedKT is given in Appendix D, Alg. 1."
IMPLEMENTATION/METHODS,0.1926605504587156,"4.2
PRIVATE KNOWLEDGE TRANSFER VIA LOCAL HYPERNETWORKS"
IMPLEMENTATION/METHODS,0.19495412844036697,"Motivated by the availability of hypernetworks validated in Sec. 3.2.1, we view hypernetworks as
“information carriers” to continuously transfer old private knowledge of previous PMs to new PMs
generated by hypernetworks. In particular, we deploy a local hypernetwork for each client. Utilizing
hypernetworks to achieve private knowledge transfer involves two directions: a) knowledge release
(forward) and b) knowledge absorption (backward)."
IMPLEMENTATION/METHODS,0.19724770642201836,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.19954128440366972,"Knowledge release (forward). As step ➁in Fig. 3, we first use the hypernetwork φt
k to generate
parameters for the new private model eθt
k, in which hypernetworks release old knowledge of the last
round’s trained private models ωt−1
k
to the new generated model eθt
k."
IMPLEMENTATION/METHODS,0.2018348623853211,"Then, the generated private model eθt
k will be trained on local dataset Dk, and the trained private
model is denoted as ωt
k which will be uploaded to the server for aggregation."
IMPLEMENTATION/METHODS,0.20412844036697247,"Knowledge absorption (backward). Instead of abandoning trained private models after uploading
them to the server as in FedAvg, we use local hypernetworks to absorb the knowledge (parameter
distribution) of trained private models. This procedure is implemented by updating hypernetworks’
parameters with parameter variations between generated new PM and trained new PM, as step 6⃝in
Fig. 3. Specifically, according to the rule of updating hypernetworks in Eq. (10) of Appendix C, we
utilize the difference between the generated new PM eθt
k and trained new PM ωt
k to update the local
hypernetwork φt
k, i.e.,
φt+1
k
←φt
k −ηHN(∇φt
kθt0
k )T ∆(θt0
k , ωt
k).
(1)"
IMPLEMENTATION/METHODS,0.20642201834862386,"where ηHN is the hypernetwork’s learning rate. Then the updated hypernetwork φt+1
k
absorbs the
knowledge of the latest trained PM ωt
k."
IMPLEMENTATION/METHODS,0.20871559633027523,"Since the two-way knowledge transfer is executed in each round, hypernetworks continuously learn
historical private knowledge and transfer it to the new generated PMs during the whole FL, which
promotes the personalization of PMs."
IMPLEMENTATION/METHODS,0.21100917431192662,"4.3
GLOBAL KNOWLEDGE TRANSFER VIA CONTRASTIVE LEARNING"
IMPLEMENTATION/METHODS,0.21330275229357798,"Once the new PM eθt
k is generated by the local hypernetwork, old private knowledge has been trans-
ferred into the new model. To make eθt
k further obtain the latest GM’s knowledge, we exploit con-
trastive learning to bridge GM ωt and PM eθt
k. Specifically, we view eθt
k as the anchor, and keep it
close to GM ωt (positive) since we hope eθt
k to learn knowledge from other clients via GM; while
keeping eθt
k away from the last round’s PM ωt−1
k
(negative), so as to avoid being trapped in a local
optimum and slowing down convergence due to excessively skewing previous PM’s obsolete pa-
rameters. We use triplet loss (Schroff & et al., 2015) in typical contrastive learning as pFedKT’s
contrastive loss ℓcon. Then, we calculate contrastive loss by:"
IMPLEMENTATION/METHODS,0.21559633027522937,"Lωt ←Rωt(Dk), Lωt
k ←Rωt
k(Dk), Lωt−1
k
←Rωt−1
k
(Dk);"
IMPLEMENTATION/METHODS,0.21788990825688073,"d+
KL = dKL(Lωt∥Lωt
k), d−
KL = dKL(Lωt−1
k
∥Lωt
k);"
IMPLEMENTATION/METHODS,0.22018348623853212,"ℓcon = max{d+
KL −d−
KL + α, 0}. (2)"
IMPLEMENTATION/METHODS,0.22247706422018348,"The distance d+
KL of eθt
k (anchor) and ωt (positive), and the distance d−
KL of eθt
k (anchor) and ωt−1
k
(negative) are measured by the KL divergence of their logits L(ω·) (i.e., extracted representation
Rω·(Dk)). α ≥0, is the maximum margin between the anchor-positive distance and anchor-
negative distance. If α = 0, the generated PM eθt
k is as far from GM ωt as from the last round’s
PM ωt−1
k
, i.e., “neutral” status. If α > 0, then d+
KL + α ≤d−
KL, i.e., generated PM eθt
k is close to
the GM ωt and away the last round’s PM ωt−1
k
, and vice versa."
IMPLEMENTATION/METHODS,0.22477064220183487,"After computing contrastive loss ℓcon, we further calculate the supervised loss ℓsup (e.g., Cross
Entropy loss) with the training model’s predictions and labels. Finally, we linearly weight the two
types of loss to build the complete loss function f, i.e.,"
IMPLEMENTATION/METHODS,0.22706422018348624,"f = µ ∗ℓcon + (1 −µ) ∗ℓsup,
(3)"
IMPLEMENTATION/METHODS,0.22935779816513763,"where µ ∈(0, 1) is the weight of contrastive loss. Next, we train the generated PM eθt
k on local data
Dk through gradient descent with the complete loss function f, and obtain trained PM ωt
k, i.e.,"
IMPLEMENTATION/METHODS,0.231651376146789,"ωt
k ←θt0
k −ηNN∇f(θt0
k ; Dk),
(4)"
IMPLEMENTATION/METHODS,0.23394495412844038,where ηNN is the private model’s learning rate.
IMPLEMENTATION/METHODS,0.23623853211009174,"With contrastive learning, the trained PM θt
k absorbs the GM’s global knowledge (i.e., private knowl-
edge from other clients), enhancing the generalization of PMs in available classes of local data."
IMPLEMENTATION/METHODS,0.23853211009174313,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.2408256880733945,"4.4
COMPUTATIONAL BUDGET AND STORAGE COST"
IMPLEMENTATION/METHODS,0.24311926605504589,"In this section, we analyze the computational complexity and storage overhead of pFedKT compared
with state-of-the-art pFedHN. Limited to pages, the detailed analysis and comparisons with more
baselines are presented in Appendix G. After careful comparisons, we summarized as follows:"
IMPLEMENTATION/METHODS,0.24541284403669725,"Computational Complexity. pFedKT consumes a comparable computational cost to pFedHN. In
cross-silo FL scenarios, multiple enterprises with sufficient computational power can tolerate exe-
cuting pFedKT. Besides, pFedKT inherently offloads the serial learning tasks of the hypernetwork
on the server in pFedHN to parallel clients’ sub-tasks, which reduces computation delay and tackles
the blocking issue that possibly occurred on the server in pFedHN."
IMPLEMENTATION/METHODS,0.24770642201834864,"Storage Overhead. Since the local hypernetwork in pFedKT has a smaller footprint than the hy-
pernetwork deployed on the server in pFedHN. Hence, pFedKT shows a lower storage cost than
pFedHN from the perspective of one device (client in pFedKT or server in pFedHN)."
IMPLEMENTATION/METHODS,0.25,"5
THEORETICAL ANALYSIS AND PROOF"
IMPLEMENTATION/METHODS,0.25229357798165136,"In this section, we analyze pFedKT’s generalization and prove its convergence in theory."
IMPLEMENTATION/METHODS,0.2545871559633027,"5.1
ANALYSIS FOR GENERALIZATION"
IMPLEMENTATION/METHODS,0.25688073394495414,"We refer to the theoretical analysis in Shamsian & et al (2021) and derive similar conclusions in
Theorem 5.1, the detailed assumptions and derivations are illustrated in Appendix E."
IMPLEMENTATION/METHODS,0.2591743119266055,Theorem 5.1 If one client has at least M = O( 1
IMPLEMENTATION/METHODS,0.26146788990825687,ϵ2 (embdim + HN size)log( rLωLφ
IMPLEMENTATION/METHODS,0.26376146788990823,"ϵ
) + 1"
IMPLEMENTATION/METHODS,0.26605504587155965,ϵ2 log( 1
IMPLEMENTATION/METHODS,0.268348623853211,"δ ))
samples, for hypernetwork φ, there is at least 1 −δ probability that satisfies: |f(φ) −ˆfDk(φ)| ≤ϵ."
IMPLEMENTATION/METHODS,0.2706422018348624,"where f is loss function, embdim and HN size are the input embedding dimension and size (param-
eter capacity) of hypernetworks (the hypernetwork is a fully connected model with chunk id as input
embedding), Lω, Lφ are assumed Lipschitz constants, r, ϵ are constants defined in derivation. This
Theorem reflects that pFedKT’s generalization is impacted by both the hypernetwork’s input em-
bedding dimension and size, Lipschitz constants Lω and Lφ which have been verified to marginally
affect the hypernetwork’s utility in Shamsian & et al (2021). So, we experimentally verify how the
hypernetwork’s input embedding dimension and size affect pFedKT’s generalization in Sec. 6.3.1."
IMPLEMENTATION/METHODS,0.27293577981651373,"5.2
PROOF FOR CONVERGENCE"
IMPLEMENTATION/METHODS,0.27522935779816515,"Insight. Shamsian & et al (2021) explains that the mapping of hypernetworks to generated target
models is essentially similar to the principle of PCA dimension reduction. That is, the hypernetwork
can be viewed as the main component (core information) of the target model after reducing dimen-
sion. Therefore, target models generated by hypernetworks would have a similar convergence rate
to pure target models, as shown in preliminaries (Fig. 2 (b) in Sec. 3)."
IMPLEMENTATION/METHODS,0.2775229357798165,"Proof. We refer to the convergence proof in Li et al. (2020), and derive the following Theorem
(detailed proof is presented in Appendix E):"
IMPLEMENTATION/METHODS,0.2798165137614679,"Theorem 5.2 Assuming E[f(ωT )] is the average loss in the T-th round, f ∗is the minimum loss of
ω during T rounds; κ, γ, B, C, µ, L are defined constants in Li et al. (2020); ω0 is the initial model,
ω∗is the optimal model with minimum loss. Then we can get: E[f(ωT )] −f ∗≤
2κ
γ+T ( B+C"
IMPLEMENTATION/METHODS,0.28211009174311924,"µ
+ 2L ·
Lφσ42) ∼O(1/T), where Lφ and σ4 are constants defined in our extra assumptions."
IMPLEMENTATION/METHODS,0.28440366972477066,"From Theorem 5.2, we conclude that pFedKT has the same convergence rate O(1/T) with FedAvg."
RESULTS/EXPERIMENTS,0.286697247706422,"6
EXPERIMENTS"
RESULTS/EXPERIMENTS,0.2889908256880734,"We implement pFedKT and all baselines with PyTorch and simulate their FL processes on NVIDIA
GeForce RTX 3090 GPUs with 24G memory. We evaluate pFedKT on two image classification
datasets: CIFAR-10/100 1 (Krizhevsky & et al, 2009) with manually Non-IID division and a large
real-world Non-IID dataset: Stack Overflow 2. The Codes will be made public after acceptance."
RESULTS/EXPERIMENTS,0.29128440366972475,"1https://www.cs.toronto.edu/ kriz/cifar.html
2https://www.tensorflow.org/federated/api docs/python/tff/simulation/datasets/stackoverflow/load data"
RESULTS/EXPERIMENTS,0.29357798165137616,Under review as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.2958715596330275,"6.1
SETTINGS
Datasets and Models. Referring to the Non-IID data divisions in Shamsian & et al (2021); Charles
& et al. (2021), we manually divide the three datasets into Non-IID distributions. Specifically, for
CIFAR-10, we assign only 2 classes of data to each client, 50 clients totally; for CIFAR-100, we
assign only 10 classes of data to each client, 50 clients totally; for Stack Overflow, we assign only
posts from one author to each client, 100 clients totally. In CIFAR-10/100, one class allocated to
different clients has different numbers and features. Specifically, if one class requires to be allo-
cated into 10 clients, then we use random.uniform(low, high) function (low, high ∈(0, 1)) to
produce 10 ratios of data counts and then generate this class’s data subnets with different numbers
and features to different clients. In Stack Overflow, each user’s posts show naturally diverse in class
and features. After dividing Non-IID data into each client, each client’s local data is further divided
into training set, evaluating set, and testing set with the ratio of 8:1:1, i.e., the testing set is stored
locally in each client and shows a consistent distribution with local training set. We train a small
CNN model and a large CNN model on the CIFAR-10 and 100 datasets in two image classification
tasks, respectively, and train an LSTM model with the same structure as McMahan & et al (2017)
on Stack Overflow dataset in a next-word prediction task. We use the same hypernetworks in three
tasks. The structures of two CNN models and hypernetworks are shown in Appendix F.1, Tab. 2."
RESULTS/EXPERIMENTS,0.2981651376146789,"Baselines. We compare pFedKT with the following algorithms: 1) Local Training, in which each
client trains its model locally. 2) FedAvg, a typical FL algorithm. 3) Cluster-based PFL meth-
ods: HYPERCLUSTER. 4) PFL methods with model mixup: FedRep, FedPer, LG-FEDAVG,
MAPPER, PartialFed. 5) PFL methods with local aggregation: FedFOMO. 6) PFL methods with
knowledge distillation: FML, FedPHP. 7) PFL methods related to our pFedKT: MOON with con-
trastive learning which regards GM as the anchor, pFedHN with the server’s hypernetwork, and
Fed-RoD with personalized headers generated by hypernetworks."
RESULTS/EXPERIMENTS,0.30045871559633025,Metrics. We measure the trained private models’ mean accuracy and denote it as PM@Acc (%).
RESULTS/EXPERIMENTS,0.30275229357798167,"Training Strategy. We set grid-searched optimal FL hyperparameters for all algorithms: the client
sampling rate C is 0.1; the learning rate of the local target model (ηNN) is 1e −2, using the SGD
optimizer with the momentum of 0.9, weight decay of 5e −5, and batch size of 64, local epochs of
{10, 50, 100}, and the hypernetwork’s learning rate (ηHN) is 5e−3; the total communication rounds
are at most 500. Our pFedKT’s unique hyperparameters are reported in Appendix F.1, Tab. 3."
RESULTS/EXPERIMENTS,0.30504587155963303,"6.2
COMPARISONS WITH BASELINES
Limited to pages, here we only report the comparison results on the CIFAR-10/100 dataset, the
results on the Stack Overflow dataset are recorded in Tab. 4 of Appendix F.2.1. Tab. 1 records PMs’
mean accuracy of our pFedKT and all baselines, and Fig. 4 displays how the PMs’ mean accuracy
varies with rounds and 50 clients’ individual PMs’ accuracy after convergence."
RESULTS/EXPERIMENTS,0.3073394495412844,"Results. As shown in Tab. 1, our pFedKT’s mean PM accuracy outperforms all baselines. On the
CIFAR-10 dataset, pFedKT’s mean PM’s accuracy is 98.12%, which is 1.11% improved than the
second-highest PM@Acc 97.01% achieved by pFedHN. On the CIFAR-100 dataset, pFedKT’s mean
PM’s accuracy is 69.38%, increased 1.62% than the second-highest PM@Acc 67.76% achieved by
FedPHP. The 100-classification task is a bit more complex than the 10-classification task, so it is in-
spiring that pFedKT fulfils a larger improvement of PM’s accuracy on the CIFAR-100 dataset. Since
pFedHN’s global hypernetwork may not fully learn private models’ knowledge in a more complex
task while our pFedKT’s local hypernetwork can well learn its own generated private model’s knowl-
edge and then still maintains the highest accuracy. Besides, in Fig. 4, it’s obvious to see that: (1)
our pFedKT converges to the highest PM’s accuracy. (2) Overall, the 50 PMs’ individual accuracies
after convergence in pFedKT are better (lighter color) than the baselines, which demonstrates the
highest personalization degree of pFedKT."
RESULTS/EXPERIMENTS,0.30963302752293576,"Analysis. The PM@Acc of pFedHN and Fed-RoD are second only to our pFedKT, which benefits
from that: pFedHN uses the server’s large hypernetwork to learn clients’ private knowledge and
transfer it to each client, promoting PM’s feature extraction ability; Fed-RoD utilizes local hyper-
networks to learn personalized header’s knowledge, improving PM’s prediction ability. FedPHP
marginally improves PM due to linear cumulative knowledge; MOON fails to improve PM may be
due to choosing GM as the initial anchor. Profiting from transferring historical private knowledge
via local hypernetworks into PMs and transferring global knowledge via contrastive learning into
PMs, PM’s personalization and generalization in available classes are both enhanced in pFedKT,
so it achieves the state-of-the-art personalized PM’s accuracy."
RESULTS/EXPERIMENTS,0.3119266055045872,Under review as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.31422018348623854,"Table 1: The PMs’ mean accuracy of pFedKT and compared baselines on CIFAR-10 (Non-IID: 2/10)
and CIFAR-100 (Non-IID: 10/100) datasets. For fair comparisons, we record the best PM@Acc (%)
during 500 rounds for all algorithms. Bold: highest, italic: second-highest."
RESULTS/EXPERIMENTS,0.3165137614678899,"Dataset
CIFAR-10
CIFAR-100
Local Training
95.11
59.78
FedAvg (McMahan & et al, 2017)
96.42
65.73
HYPCLUSTER (Mansour & et al, 2020)
95.75
63.30
FedRep (Collins & et al, 2021)
96.74
66.64
FedPer (Arivazhagan & et al, 2019)
96.61
66.28
LG-FEDAVG (Liang & et al, 2020)
92.60
58.51
MAPPER (Mansour et al., 2020)
95.68
61.91
PartialFed (Sun & et al, 2021)
95.48
61.62
FedFOMO (Zhang & et al, 2021)
94.19
53.4
FML (Shen et al., 2020)
83.18
47.82
FedPHP (Li & et al., 2021)
96.60
67.76
MOON (Li et al., 2021)
89.21
48.64
pFedHN (Shamsian & et al, 2021)
97.01
67.41
Fed-RoD (Chen & Chao, 2022)
96.61
67.50
pFedKT (Ours)
98.12
69.38"
RESULTS/EXPERIMENTS,0.31880733944954126,"0
100
200
300
400
500
Communication Rounds 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
RESULTS/EXPERIMENTS,0.3211009174311927,Test Accuracy
RESULTS/EXPERIMENTS,0.32339449541284404,CIFAR-10 (Non-IID: 2/10)
RESULTS/EXPERIMENTS,0.3256880733944954,"MOON
pFedHN
Fed-RoD
pFedKT"
RESULTS/EXPERIMENTS,0.32798165137614677,"0
100
200
300
400
500
Communication Rounds 0.2 0.3 0.4 0.5 0.6"
RESULTS/EXPERIMENTS,0.3302752293577982,Test Accuracy
RESULTS/EXPERIMENTS,0.33256880733944955,CIFAR-100 (Non-IID: 10/100)
RESULTS/EXPERIMENTS,0.3348623853211009,"MOON
pFedHN
Fed-RoD
pFedKT c0 c2 c4 c6 c8 c10 c12 c14 c16 c18 c20 c22 c24 c26 c28 c30 c32 c34 c36 c38 c40 c42 c44 c46 c48"
RESULTS/EXPERIMENTS,0.33715596330275227,Client ID MOON
RESULTS/EXPERIMENTS,0.3394495412844037,pFedHN
RESULTS/EXPERIMENTS,0.34174311926605505,FedROD
RESULTS/EXPERIMENTS,0.3440366972477064,pFedKT
RESULTS/EXPERIMENTS,0.3463302752293578,Algorithm
RESULTS/EXPERIMENTS,0.3486238532110092,Test Accuracy of PMs (CIFAR-10:Non-IID) 0.6 0.8 1.0 c0 c2 c4 c6 c8 c10 c12 c14 c16 c18 c20 c22 c24 c26 c28 c30 c32 c34 c36 c38 c40 c42 c44 c46 c48
RESULTS/EXPERIMENTS,0.35091743119266056,Client ID MOON
RESULTS/EXPERIMENTS,0.3532110091743119,pFedHN
RESULTS/EXPERIMENTS,0.3555045871559633,FedROD
RESULTS/EXPERIMENTS,0.3577981651376147,pFedKT
RESULTS/EXPERIMENTS,0.36009174311926606,Algorithm
RESULTS/EXPERIMENTS,0.3623853211009174,Test Accuracy of PMs (CIFAR-100:Non-IID) 0.2 0.4 0.6
RESULTS/EXPERIMENTS,0.3646788990825688,"Figure 4: On CIFAR-10/100 datasets, left-two: the smoothed PMs’ mean accuracy varies with
communication rounds; right-two: 50 clients’ individual PM’s accuracy after convergence."
RESULTS/EXPERIMENTS,0.3669724770642202,"Non-IID and Client Participating Rate. In addition, we also verified that pFedKT presents su-
periority in highly Non-IID degrees and shows robustness to diverse client participation rates. The
detailed experimental settings, results and analysis are reported in Appendix F.2.2."
RESULTS/EXPERIMENTS,0.36926605504587157,"6.3
CASE STUDY"
RESULTS/EXPERIMENTS,0.37155963302752293,"In this section, we study the affects of several cases on pFedKT, which include: the hypernetwork’s
input embedding dimension and size, the weight of contrastive loss, the margin of triplet loss, and
diverse loss functions. All case studies are executed within 500 rounds on CIFAR-10 (Non-IID:
2/10) and CIFAR-100 (Non-IID: 10/100) datasets. For stable comparisons, we record the average of
PM@Acc (%) within the last 30 rounds."
RESULTS/EXPERIMENTS,0.3738532110091743,"6.3.1
INPUT EMBEDDING DIMENSION AND SIZE OF HYPERNETWORK"
RESULTS/EXPERIMENTS,0.3761467889908257,"A. Input embedding dimension. Sec. 5.1 analyzed that the hypernetwork’s input embedding di-
mension may affect pFedKT’s generalization, so we experimentally explore its actual influence.
Like Shamsian & et al (2021) computing the embedding dimension by ⌊1 + N/β⌋(N is the to-
tal number of clients, N = 50), we select β ∈{1, 2, 3, 4, 10}, i.e., embdim ∈{51, 26, 17, 13, 6}.
Fig. 7 of Appendix F.3 displays that the accuracy of PM varies with different embedding dimen-
sions, and the detailed values are recorded in Tab. 6 of Appendix F.3. From Fig. 7, we can see that:
on the CIFAR-10 dataset, the embedding dimension has random effects on pFedKT; on the CIFAR-
100 dataset, the PM’s accuracy rises as the embedding dimension increases. However, the larger
input embedding dimension has higher computation complexity, which slows down local training.
In Appendix F.3, Tab. 6, when embedding dimensions are 13 and 51 on CIFAR-10/100 datasets
respectively, pFedKT obtains the best PM."
RESULTS/EXPERIMENTS,0.37844036697247707,"B. Size. Sec. 5.1 also mentioned that pFedKT’s generalization may be affected by the hypernet-
work’s size, so we vary the number of hypernetworks’ hidden layers ∈{1, 2, 3, 4, 5}, and results
are reported in Fig. 7 and Tab. 7 of Appendix F.3. From Fig. 7, we find that the accuracy of PM
drops as hypernetwork’s size increases, which is probably due to that larger hypernetworks require
more parameters to be trained, degrading their generalization. In Tab. 7, pFedKT performs the best
PM when the number of the hypernetwork’s hidden layers is 1, 2 on CIFAR-10, 100 datasets."
RESULTS/EXPERIMENTS,0.38073394495412843,Under review as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.3830275229357798,"6.3.2
HYPERPARAMETERS IN CONTRASTIVE LEARNING
Next, we explore how the following key parameters of contrastive learning affect pFedKT: (1) µ,
which controls the weight of contrastive loss; (2) α, the margin of triplet loss; and (3) different
combinations of loss functions and distance measurements."
RESULTS/EXPERIMENTS,0.3853211009174312,"A. Weight of contrastive loss. We vary µ ∈{0, 0.0001, 0.001, 0.01, 0.1, .., 0.9}, and the results are
reported in Fig. 7 and Tab. 8 of Appendix F.3. Tab. 8 shows that pFedKT with µ = 0.001, 0.0001
obtain the best PM on CIFAR-10/100. Fig. 7 presents that when µ > 0.1, PM’s accuracy degrades
obviously as µ rises, which may because that: a larger weight of contrastive loss and a smaller weight
of supervised loss lead to PM’s insufficient training, since PM accesses less supervised information
from labels. Hence, the weight of contrastive loss should be set smaller than that of supervised loss."
RESULTS/EXPERIMENTS,0.3876146788990826,"B. Margin of triplet loss. We vary α ∈{0, 0.1, 1, 5, 10, 20, 30, 40, 50}, and the results are reported
in Fig. 7 and Tab. 9 of Appendix F.3. Tab. 9 shows that pFedKT with α = 0.1, 5 achieve optimal
PM on CIFAR-10, 100 datasets, respectively. Fig. 7 displays that: when α ≥30, PM’s accuracy
drops obviously since larger α leads initialized PM to be overly biased to the immature GM, directly
compromising PM’s personalization. Therefore, setting an appropriate α is necessary to balance
the private knowledge and global knowledge transferred to PM."
RESULTS/EXPERIMENTS,0.38990825688073394,"C. Loss functions. We also explore how diverse combinations of distance measurements and loss
functions affect pFedKT, and the detailed experimental settings, results and analysis are given in
Appendix F.3. From the results in Appendix F.3, Tab. 10, we conclude that the combination of
triplet loss and KL divergence we designed in pFedKT performs better model accuracy than others.
6.4
ABLATION STUDY
pFedKT involves two important parts: (1) transferring private knowledge via hypernetwork (HN)
and (2) transferring global knowledge via contrastive learning. To verify the effectiveness of each
part, we conduct ablation experiments. We explore the following four cases: (A) Without part-
(1,2), then pFedKT degenerates into FedAvg. (B) Only executing part-(1), i,e., only repeating the
local training steps: generating PM by HN →training PM →updating HN’s parameters. C) Only
executing part-(2), viewing the PM initialized with GM as the anchor, GM as the positive item, and
the last round’s PM as the negative item, then this case degrades to MOON, but we still use triplet
loss with KL divergence to compute contrastive loss. D) Executing both the two parts, i.e., pFedKT."
RESULTS/EXPERIMENTS,0.3922018348623853,"As shown in Appendix F.3, Tab. 11, case-(A) (i.e., FedAvg) has the lowest PM accuracy. Com-
pared case-(B) with case-(A), the PM’s accuracy is obviously improved, indicating that using hy-
pernetworks to transfer private knowledge is reasonable. Compared case-(C) with case-(A), there
are marginal improvements in PM’s accuracy, reflecting that MOON takes limited improvements
on model accuracy. Compared case-(D) with case-(B), pFedKT with contrastive loss takes slight
accuracy improvement than without it. Fig. 8 in Appendix showed that pFedKT with contrastive
loss has more stable convergence than w/o contrastive loss on CIFAR-10, and it converges faster
to the highest accuracy within 300 rounds while pFedKT w/o contrastive loss requires 500 rounds
for convergence on CIFAR-100. The above results indicate that the global knowledge transferred
by contrastive loss enhances PM’s generalization in available classes with lower computation cost.
Case-(D) (i.e., pFedKT) achieved the highest PM’s accuracy, demonstrating that the two-type knowl-
edge transfers are necessary to enhance PM’s personalization."
RESULTS/EXPERIMENTS,0.3944954128440367,"Summary. Overall, pFedKT fulfils the state-of-the-art personalized PM’s performances. Besides,
both the hypernetwork’s input embedding dimension and size influence pFedKT’s generalization.
And it’s necessary to select the proper weight of contrastive loss, the margin of triplet loss, and the
combination of the contrastive loss function and distance measurement. Finally, ablation experi-
ments verified the feasibility and effectiveness of both two types of knowledge transfer in pFedKT.
7
CONCLUDING REMARKS
In this paper, we proposed a novel personalized FL framework named pFedKT to boost personal-
ized PMs. It consists of two types of knowledge transfer: a) transferring historical private knowledge
to PMs by local hypernetworks, and b) transferring global knowledge to PMs through contrastive
learning. The two-type knowledge transfer enables PMs to acquire both historical private knowledge
and the latest global knowledge, which promotes PM’s personalization and generalization in avail-
able classes simultaneously. Besides, we theoretically and experimentally verified that pFedKT’s
generalization is related to hypernetworks’ input embedding dimension and size and also proved its
convergence. Extensive experiments demonstrated that pFedKT achieves the state-of-the-art per-
sonalized PM’s accuracy. In practice, pFedKT can be broadly applied to cross-silo FL scenarios."
REFERENCES,0.3967889908256881,Under review as a conference paper at ICLR 2023
REFERENCES,0.39908256880733944,REFERENCES
REFERENCES,0.4013761467889908,"Idan Achituve and et al.
Personalized federated learning with gaussian processes.
CoRR,
abs/2106.15482, 2021."
REFERENCES,0.4036697247706422,"Shaashwat Agrawal and et al. Genetic CFL: optimization of hyper-parameters in clustered federated
learning. CoRR, abs/2107.07233, 2021."
REFERENCES,0.4059633027522936,"Manoj Ghuhan Arivazhagan and et al.
Federated learning with personalization layers.
CoRR,
abs/1912.00818, 2019."
REFERENCES,0.40825688073394495,"Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res., 12:149–198, 2000."
REFERENCES,0.4105504587155963,"Zachary Charles and et al. On large-cohort training for federated learning. In Proc. NeurIPS, 2021."
REFERENCES,0.41284403669724773,"Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning. In
Proc. ICLR. OpenReview.net, 2022."
REFERENCES,0.4151376146788991,"Shuo Chen, Gang Niu, Chen Gong, Jun Li, Jian Yang, and Masashi Sugiyama. Large-margin con-
trastive learning with distance polarization regularizer. In Proc. ICML, volume 139, pp. 1673–
1683. PMLR, 2021."
REFERENCES,0.41743119266055045,"Liam Collins and et al. Exploiting shared representations for personalized federated learning. In
Proc. ICML, Virtual Event, volume 139, pp. 2089–2099. PMLR, 2021."
REFERENCES,0.4197247706422018,"Canh T. Dinh and et al. Fedu: A unified framework for federated multi-task learning with laplacian
regularization. CoRR, abs/2102.07148, 2021."
REFERENCES,0.42201834862385323,"A. Fallah and et al. Personalized federated learning with theoretical guarantees: A model-agnostic
meta-learning approach. In Proc. NeurIPS, virtual, 2020a."
REFERENCES,0.4243119266055046,"A. Fallah and et al. On the convergence theory of gradient-based model-agnostic meta-learning
algorithms. In Proc. AISTATS, Online. PMLR, 2020b."
REFERENCES,0.42660550458715596,"David Ha and et al. Hypernetworks. In Proc. ICLR 2017, Toulon, France. OpenReview.net, 2017."
REFERENCES,0.4288990825688073,"Peter Kairouz and et al. Advances and open problems in federated learning. Found. Trends Mach.
Learn., 14(1-2):1–210, 2021."
REFERENCES,0.43119266055045874,Alex Krizhevsky and et al. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.4334862385321101,"Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79–86, 1951."
REFERENCES,0.43577981651376146,"Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proc. CVPR,
pp. 10713–10722, 2021."
REFERENCES,0.4380733944954128,"Tian Li and et al. Federated optimization in heterogeneous networks. In Proc. MLSys, Austin, TX,
USA. mlsys.org, 2020."
REFERENCES,0.44036697247706424,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In Proc. ICLR, 2020. URL https://openreview.net/forum?
id=HJxNAnVtDS."
REFERENCES,0.4426605504587156,"Xin-Chun Li and et al. Fedphp: Federated personalization with inherited private models. In Proc.
ECML-PKDD, volume 12975, pp. 587–602. Springer, 2021."
REFERENCES,0.44495412844036697,"Zhenguo Li and et al.
Meta-sgd: Learning to learn quickly for few shot learning.
CoRR,
abs/1707.09835, 2017."
REFERENCES,0.44724770642201833,"Paul Pu Liang and et al.
Think locally, act globally: Federated learning with local and global
representations. CoRR, abs/2001.01523, 2020."
REFERENCES,0.44954128440366975,"Yishay Mansour and et al. Three approaches for personalization with applications to federated
learning. CoRR, abs/2002.10619, 2020."
REFERENCES,0.4518348623853211,Under review as a conference paper at ICLR 2023
REFERENCES,0.4541284403669725,"Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh.
Three approaches for
personalization with applications to federated learning. CoRR, abs/2002.10619, 2020."
REFERENCES,0.45642201834862384,"Brendan McMahan and et al. Communication-efficient learning of deep networks from decentralized
data. In Proc. AISTATS, Fort Lauderdale, FL, USA, volume 54, pp. 1273–1282. PMLR, 2017."
REFERENCES,0.45871559633027525,"X. Ouyang and et al. Clusterfl: a similarity-aware federated learning system for human activity
recognition. In Proc. MobiSys, USA, pp. 54–66. ACM, 2021."
REFERENCES,0.4610091743119266,"Yijiang Pang, Boyang Liu, and Jiayu Zhou. RUSH: robust contrastive learning via randomized
smoothing. CoRR, abs/2207.05127, 2022."
REFERENCES,0.463302752293578,"Daniel W. Peterson, Pallika Kanani, and Virendra J. Marathe. Private federated learning with domain
adaptation. CoRR, abs/1912.06733, 2019."
REFERENCES,0.46559633027522934,"Qi Qian and et al. Softtriple loss: Deep metric learning without triplet sampling. In Proc. ICCV, pp.
6449–6457. IEEE, 2019."
REFERENCES,0.46788990825688076,"Florian Schroff and et al. Facenet: A unified embedding for face recognition and clustering. In Proc.
CVPR, pp. 815–823, 2015."
REFERENCES,0.4701834862385321,"Aviv Shamsian and et al. Personalized federated learning using hypernetworks. In Proc. ICML,
Virtual Event, volume 139, pp. 9489–9502. PMLR, 2021."
REFERENCES,0.4724770642201835,"Tao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Fei Wu, and Chao Wu.
Federated mutual learning. CoRR, abs/2006.16765, 2020."
REFERENCES,0.47477064220183485,"Virginia Smith and et al. Federated multi-task learning. In Proc. NeurIPS, Long Beach, CA, USA,
pp. 4424–4434, 2017."
REFERENCES,0.47706422018348627,"Benyuan Sun and et al. Partialfed: Cross-domain personalized federated learning via partial initial-
ization. In Proc. NeurIPS, 2021."
REFERENCES,0.4793577981651376,"Kangkang Wang and et al.
Federated evaluation of on-device personalization.
CoRR,
abs/1910.10252, 2019."
REFERENCES,0.481651376146789,"Ye Xue, Diego Klabjan, and Yuan Luo.
Aggregation delayed federated learning.
CoRR,
abs/2108.07433, 2021."
REFERENCES,0.48394495412844035,"Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, and Lifeng Sun. Towards faster and better
federated learning: A feature fusion approach. In Proc. ICIP, pp. 175–179. IEEE, 2019."
REFERENCES,0.48623853211009177,"Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized
knowledge transfer for personalized federated learning. In Proc. NeurIPS, pp. 10092–10104,
2021."
REFERENCES,0.48853211009174313,"Michael Zhang and et al. Personalized federated learning with first order model optimization. In
Proc. ICLR, 2021."
REFERENCES,0.4908256880733945,"Yue Zhao and et al. Federated learning with non-iid data. CoRR, abs/1806.00582, 2018."
REFERENCES,0.49311926605504586,Under review as a conference paper at ICLR 2023
APPENDIX,0.4954128440366973,"A
DEFINITION OF FL"
APPENDIX,0.49770642201834864,"FedAvg (McMahan & et al, 2017) is a typical federated learning algorithm. Assuming that there are
N clients in total, as shown in Fig. 1, the server samples fraction C of clients St (|St| = N · C =
K) to participate in FL and broadcasts the global model to them. The k-th client initializes its
local model ωk with the received global model and then trains it on local datasets Dk, the training
objective is:"
APPENDIX,0.5,min Fk(ωk) = 1 nk X
APPENDIX,0.5022935779816514,"i∈Dk
fi(ωk),
(5)"
APPENDIX,0.5045871559633027,"where nk = |Dk|; fi(ωk) = ℓ(xi, yi; ωk), i.e., the loss of i-th instance (xi, yi) on the local model
ωk. The local epoch is E, batch size is B, so local training executes E nk"
APPENDIX,0.5068807339449541,"B iterations. Then, clients
upload trained local models to the server, and the server aggregates received local models to update
the global model: ω = K−1
X k=0 nk"
APPENDIX,0.5091743119266054,"n ωk,
(6)"
APPENDIX,0.5114678899082569,"where n is the total number of instances owned by all clients. All the steps iteratively execute until
the global model converges or the maximum round reaches."
APPENDIX,0.5137614678899083,"B
HOW TO USE HYPERNETWORKS TO GENERATE PARAMETERS FOR TARGET
MODELS?"
APPENDIX,0.5160550458715596,"Taking the hypernetwork and CNN (target model) for CIFAR-10 in Tab. 2 as examples, here we
introduce how we use the hypernetwork to generate parameters for target models. Since the volume
(2.e + 09) of CNN’s parameters are more than the parameters amounts (400) of the hypernetwork’s
output layer, so we first divide CNN’s parameters into multiple chunks with ordered ids, each chunk
has no more than 400 ( hypernetwork’s output dimension) parameters. Then we sequentially input
the embedding of chunk id into the hypernetwork to get the corresponding chunk’s target parameters.
Finally, we concatenate the parameters from all chunks and reshape them with the CNN’s parameter
shape. To sum up, we call the hyperparameters multiple times to generate parameters for the target
model in a stacked form. Since each client’s local hypernetwork is unrelated to others, we applied
the above parameter generation way for all clients in our pFedKT."
APPENDIX,0.518348623853211,"C
HOW TO UPDATE HYPERNETWORK?"
APPENDIX,0.5206422018348624,"Assuming that the hypernetwork is φ, and it generates parameters for the target model ω = h(v; φ)
(v is the hypernetwork’s input embedding). The hypernetwork φ and the generated target model
ω are trained in an end-to-end manner. Specifically, the generated target model ω first executes
gradient descent, then the hypernetwork φ is updated also through gradient descent. We assume that
the loss function of the generated target model ω is ℓ(ω) = ℓ(h(v; φ)), so we refer to Shamsian &
et al (2021) and also utilize chain rule to derive the following equation:"
APPENDIX,0.5229357798165137,"∇φℓ(ω) = ∇ωℓ(ω) · ∇φω = (∇φω)T · ∇ωℓ(ω).
(7)"
APPENDIX,0.5252293577981652,"For ∇ωℓ(ω), we can use its first-order gradient approximation (Shamsian & et al, 2021; Zhang &
et al, 2021) to represent as:
∇ωℓ(ω) := ∆ω = bω −ω,
(8)"
APPENDIX,0.5275229357798165,"where bω and ω are the target models after/before training. So we replace ∇ωℓ(ω) of Eq. 7 with
Eq. 8, and get:
∇φℓ(ω) = (∇φω)T · ∆ω
(9)"
APPENDIX,0.5298165137614679,"After computing the gradients of the hypernetwork φ, its parameters are updated through gradient
descent, i.e.,
φ ←φ −ηHN(∇φω)T · ∆ω,
(10)"
APPENDIX,0.5321100917431193,where ηHN is the learning rate of the hypernetwork φ.
APPENDIX,0.5344036697247706,Under review as a conference paper at ICLR 2023
APPENDIX,0.536697247706422,"Algorithm 1: pFedKT
Input: N, number of clients; K, number of selected clients; R, total number of rounds; ηNN, learning
rate of private target networks; ηHN, learning rate of private hypernetworks; E, local epochs, B,
batch size; µ, weight of contrastive loss, α, margin in triplet loss
Randomly initialize the global model ω0, private model ω0
k = ω
0, private hypernetwork φ0
k
for each round t=0,1,..,R-1 do"
APPENDIX,0.5389908256880734,"St ←randomly select K clients from N clients
Clients execute:
for each client k ∈St do"
APPENDIX,0.5412844036697247,"Receive the latest global model ωt form the server
Utilize hypernetwork φt
k to generate initial private model eθt
k
Train eθt
k on local dataset Dk to get trained private model ωt
k by contrastive learning:
B ←split local dataset Dk into batches of size B
ωt
k ←eθt
k
for each local epoch e from 1 to E do"
APPENDIX,0.5435779816513762,for each batch b ∈B do
APPENDIX,0.5458715596330275,"Compute logits: Lωt ←Rωt(b), Lωt
k ←Rωt
k(b), Lωt−1
k
←Rωt−1
k
(b)"
APPENDIX,0.5481651376146789,"Compute distances: d+
KL = dKL(Lωt ∥Lωt
k), d−
KL = dKL(Lωt−1
k
∥Lωt
k)"
APPENDIX,0.5504587155963303,"Compute contrastive loss: ℓcon = max{d+
KL −d−
KL + α, 0} // triplet loss
Compute supervised loss: ℓsup = CrossEntropy(output of ωt
k(b), label)
Complete complete loss function: f = µ ∗ℓcon + (1 −µ) ∗ℓsup
Gradient descent: ωt
k ←ωt
k −ηNN∇f
end
end
Store trained model ωt
k locally
Use trained private model ωt
k and initial private model eθt
k to update hypernetwork:
φt+1
k
←φt
k −ηHN(∇φt
kθt0
k )T ∆(θt0
k , ωt
k) // according to Eq. 1
Upload private trained model ωt
k to the server
end
Server executes:
Receive private models [ωt
k, ...] from clients and aggregate them by:
ωt+1 = PK
k=1
nk"
APPENDIX,0.5527522935779816,"n ωt
k // nk, number of k-th client’s samples; n, total number of all clients’ samples
Send the updated global model ωt+1 to clients selected in the next round
end
Return personalized private models [ωR−1
0
, ωR−1
1
, ..., ωR−1
N−1]"
APPENDIX,0.555045871559633,"D
PFEDKT ALGORITHM"
APPENDIX,0.5573394495412844,"Here, we illustrate the detailed algorithm of pFedKT in Alg. 1."
APPENDIX,0.5596330275229358,"E
DETAILED THEORETICAL ANALYSIS AND PROOF"
APPENDIX,0.5619266055045872,"In this section, we detailed the assumptions and derivations for Theorem 5.1 and Theorem 5.2,
respectively."
APPENDIX,0.5642201834862385,"E.1
DETAILED DERIVATIONS FOR THEOREM 5.1"
APPENDIX,0.5665137614678899,"We assume that the k-th client’s local dataset Dk = {(x(k)
i
, y(k)
i
)}
|Dk|"
APPENDIX,0.5688073394495413,"i=1 , Dk ∼Pk. Since we assume
that the private model ωk is nonlinear, the following derivations are adaptive to non-convex situa-
tions. Then the empirical loss and expected loss of the k-th client’s private model ωk can be denoted
as:"
APPENDIX,0.5711009174311926,"Empirical loss : ˆfDk(ωk) =
1
|Dk|"
APPENDIX,0.573394495412844,"|Dk|
X"
APPENDIX,0.5756880733944955,"i=1
ℓ(x(k)
i
, y(k)
i
; ωk),
(11)"
APPENDIX,0.5779816513761468,"Expected loss : f(ωk) = EPk[ℓ(x(k), y(k); ωk)].
(12)"
APPENDIX,0.5802752293577982,Under review as a conference paper at ICLR 2023
APPENDIX,0.5825688073394495,"Since we utilize local hypernetworks to generate parameters for private models, ωk = h(v; φk). So
we can replace ωk with h(v; φk) in the above two losses, i.e.,"
APPENDIX,0.5848623853211009,"Empirical loss : ˆfDk(φk) =
1
|Dk|"
APPENDIX,0.5871559633027523,"|Dk|
X"
APPENDIX,0.5894495412844036,"i=1
ℓ(x(k)
i
, y(k)
i
; h(v; φk)),
(13)"
APPENDIX,0.591743119266055,"Expected loss : f(φk) = EPk[ℓ(x(k), y(k); h(v; φk))].
(14)"
APPENDIX,0.5940366972477065,"Since each client holds its own hypernetwork which is unrelated to other’s hypernetworks, i.e., the
hypernetwork’s input embedding v is independent of clients, so the variant is only φk"
APPENDIX,0.5963302752293578,"We refer to the assumptions about the parameters φ of hypernetworks in Shamsian & et al (2021),
in which the hypernetwork’s parameters are bounded in a spherical space with radius r. Besides, we
also assume the following two Lipschitz conditions:"
APPENDIX,0.5986238532110092,"Assumption E.1 The supervised loss ℓsup of the private model ω is Lipschitz smooth, i.e., ℓsup
satisfies:
∥ℓsup(x, y; ω1) −ℓsup(x, y; ω2)∥≤Lω∥ω1 −ω2∥.
(15)"
APPENDIX,0.6009174311926605,"As analyzed in Qian & et al. (2019); Chen et al. (2021); Pang et al. (2022), we can also assume that
the triplet loss ℓcon of pFedKT is Lipschitz smooth. So we can update the above assumption as:"
APPENDIX,0.6032110091743119,Assumption E.2 The complete loss ℓof the private model ω satisfies:
APPENDIX,0.6055045871559633,"∥ℓ(x, y; ω1) −ℓ(x, y; ω2)∥≤Lω∥ω1 −ω2∥.
(16)"
APPENDIX,0.6077981651376146,"Assumption E.3 The mapping from the hypernetwork φ to the target private model ω is Lipschitz
smooth, i.e.,
∥h(v; φ1) −h(v; φ2)∥≤Lφ∥φ1 −φ2∥.
(17)"
APPENDIX,0.6100917431192661,"Among the above two Lipschitz conditions, Lω, Lφ are Lipschitz constants."
APPENDIX,0.6123853211009175,"Based on the above assumptions and the derived threshold of local data volume M
=
O( 1"
APPENDIX,0.6146788990825688,"ϵ2 log( C(ϵ,Hl)"
APPENDIX,0.6169724770642202,"δ
) in Shamsian & et al (2021); Baxter (2000), C(ϵ, Hl) is the covering number of
Hl, Hl is parameterized by the hypernetwork φ. And the distance between two hypernetworks
φ1, φ2 on dataset with distribution P can be computed by:"
APPENDIX,0.6192660550458715,"d(φ1, φ2) = E(xi,yi)∼P [|ℓ(h(v; φ1)(xi), yi) −ℓ(h(v; φ2)(xi), yi)|]"
APPENDIX,0.6215596330275229,"≤Lω∥ℓ(h(v; φ1)) −ℓ(h(v; φ2))∥
≤LωLφ∥φ1 −φ2∥.
(18)"
APPENDIX,0.6238532110091743,"Then we choose a ϵ-covering parameter space, in which we can always find at least one neighbor φ2
with
ϵ
LωLφ distance to φ1, so we can get:"
APPENDIX,0.6261467889908257,"log(C(ϵ, Hl)) = O((embdim + HNsize) log(rLωLφ"
APPENDIX,0.6284403669724771,"ϵ
)),
(19)"
APPENDIX,0.6307339449541285,where embdim and HN size are the input embedding and parameter capacity of the hypernetwork.
APPENDIX,0.6330275229357798,"Similar to Theorem 1 in Shamsian & et al (2021), we can conclude that our pFedKT’s generalization
is affected by the hypernetwork’s input embedding and size, and also the above Lipschitz constants,
as illustrated in Theorem 5.1."
APPENDIX,0.6353211009174312,"E.2
DETAILED DERIVATIONS FOR THEOREM 5.2"
APPENDIX,0.6376146788990825,"Based on above Lipschitz conditions, we further make the following assumptions:"
APPENDIX,0.6399082568807339,"Assumption E.4 The gradients and parameters of models are bounded, i.e.,"
APPENDIX,0.6422018348623854,"E[∥g(ω)∥2] ≤σ1
2, E[∥ω∥2] ≤σ2
2,"
APPENDIX,0.6444954128440367,"E[∥g(φ)∥2] ≤σ3
2, E[∥φ∥2] ≤σ4
2,
(20)"
APPENDIX,0.6467889908256881,Under review as a conference paper at ICLR 2023
APPENDIX,0.6490825688073395,"where φ is the hypernetwork and ω is the target model with parameters generated by the hypernet-
work φ, σ1,2,3,4 are constants."
APPENDIX,0.6513761467889908,"Li et al. (2020) have proved that FedAvg can converge to O(1/T) on Non-IID dataset when partial
clients participate in FL, i.e.,"
APPENDIX,0.6536697247706422,"E[f(ωT )] −f ∗≤
2κ
γ + T (B + C"
APPENDIX,0.6559633027522935,"µ
+ 2L∥ω0 −ω∗∥2),
(21)"
APPENDIX,0.658256880733945,"where E[f(ωT )] is the average loss in the T-th round; f ∗is the minimum loss of ω during T
rounds’ optimization; κ, γ, B, C, µ, L are constants assumed or derived in Li et al. (2020); ω0 is the
initialized model, ω∗is the optima with f ∗. Since the above conclusion has been proved in Li et al.
(2020), here we no further detail it."
APPENDIX,0.6605504587155964,"In our pFedKT, each client’s private model ω is generated by its private hypernetwork φ, so we have:"
APPENDIX,0.6628440366972477,"ω0 = h(v; φ0), ω∗= h(v; φ∗).
(22)"
APPENDIX,0.6651376146788991,"Hence, Eq. (21) can be replaced as:"
APPENDIX,0.6674311926605505,"E[f(ωT )] −f ∗≤
2κ
γ + T (B + C"
APPENDIX,0.6697247706422018,"µ
+ 2L∥h(v; φ0) −h(v; φ∗)∥2).
(23)"
APPENDIX,0.6720183486238532,"From Assumption E.3, the above equation can be further derived as:"
APPENDIX,0.6743119266055045,"E[f(ωT )] −f ∗≤
2κ
γ + T (B + C"
APPENDIX,0.676605504587156,"µ
+ 2L · Lφ∥φ0 −φ∗∥2).
(24)"
APPENDIX,0.6788990825688074,"According to the Lipschitz condition in Assumption E.4, we can further get:"
APPENDIX,0.6811926605504587,"E[f(ωT )] −f ∗≤
2κ
γ + T (B + C"
APPENDIX,0.6834862385321101,"µ
+ 2L · Lφσ2
4) ∼O(1/T),
(25)"
APPENDIX,0.6857798165137615,"where Lφ and σ4 are the constants defined in Assumption E.3 and Assumption E.4. Therefore, our
pFedKT has the same convergence rate O(1/T) with FedAvg."
APPENDIX,0.6880733944954128,"F
EXPERIMENTAL DETAILS"
APPENDIX,0.6903669724770642,"F.1
MODEL STRUCTURES AND HYPERPARAMETERS"
APPENDIX,0.6926605504587156,"We describe the structures of CNN models used on CIFAR-10/100 datasets and hypernetworks used
for all tasks in Tab. 2. And we also report the detailed pFedKT’s hyperparameters used in three tasks
in Tab. 3."
APPENDIX,0.694954128440367,"Table 2: Structures of CNN models on CIFAR-10/100 datasets and hypernetworks used for CIFAR-
10 and Stack Overflow datasets. Note: the hypernetwork used on CIFAR-100 dataset has 2 hidden
layers, i.e., its structure: fc1 (emb dim, 100) →fc2 (100, 100) →fc3 (100, 100), →fc4 (100, 400)."
APPENDIX,0.6972477064220184,"CNN (CIFAR-10)
CNN (CIFAR-100)
Hypernetwork
layer name
input size
output size
filter
input size
output size
filter
input size
output size
filter
conv1
3×32×32
16×28×28
5×5×16
3×32×32
16×28×28
5×5×16
-
-
-
max pool1
16×28×28
16×14×14
2×2
16×28×28
16×14×14
2×2
-
-
-
conv2
16×14×14
32×10×10
5×5×32
16×14×14
32×10×10
5×5×32
-
-
-
max pool2
32×10×10
32×5×5
2×2
32×10×10
32×5×5
2×2
-
-
-
fc1
800
1920
-
800
1920
-
Emb dim
100
-
fc2
1920
80
-
1920
80
-
100
100
-
fc3
80
10
-
80
100
-
100
400
-"
APPENDIX,0.6995412844036697,"F.2
MORE EXPERIMENTAL RESULTS OF COMPARISONS WITH BASELINES"
APPENDIX,0.7018348623853211,"Here, we also detailedly report the experimental results on Stack Overflow dataset and the com-
parison results of our pFedKT and four related advanced baselines on CIFAR-10/100 dataset with
different Non-IID degrees and diverse client participation rates."
APPENDIX,0.7041284403669725,Under review as a conference paper at ICLR 2023
APPENDIX,0.7064220183486238,"Table 3: The settings of pFedKT’s hyperparameters in three tasks. emb dim is the hypernetwork’s
input embedding dimension; n hidden is the hypernetwork’s number of hidden layers; µ is the
weight of contrastive loss; α is the margin in triplet loss. Note that all the hyperparameters are
approximate optimal due to our coarse-grained searching."
APPENDIX,0.7087155963302753,"Dataset
emb dim
n hidden
µ
α
contrastive loss
distance
CIFAR-10 (Non-IID) 13 1 0.001 0.1"
APPENDIX,0.7110091743119266,"triplet loss
KL divergence
CIFAR-100 (Non-IID)
2
5
Stack Overflow
1
0.1"
APPENDIX,0.713302752293578,"Table 4: The experimental results of our pFedKT and baselines on Stack Overflow dataset (a large
natural real-world Non-IID dataset)."
APPENDIX,0.7155963302752294,"Model@Acc
Local Training
FedAvg
FedPHP
MOON
pFedHN
Fed-RoD
pFedKT (ours)
PM@Acc
15.71±2.30
23.52±0.05
23.64±0.03
23.79±1.41
25.19±0.01
24.98±0.15
25.33±0.01"
APPENDIX,0.7178899082568807,"F.2.1
EXPERIMENTAL RESULTS ON STACK OVERFLOW DATASET"
APPENDIX,0.7201834862385321,"We report the experimental results on a large natural real-world Non-IID dataset Stack Overflow in
Tab. 4. We can observe that our pFedKT presents the best PM’s accuracy, again verifying its utility."
APPENDIX,0.7224770642201835,"F.2.2
COMPARISONS WITH BASELINES ON DATASETS WITH DIFFERENT NON-IID DEGREES"
APPENDIX,0.7247706422018348,"To explore how the Non-IID degree affects the performances of our pFedKT and four related ad-
vanced baselines, we allocate {1, 2, ..., 10} classes of data into one client for the CIFAR-10 dataset
and divide {10, 20, ..., 100} classes of data into one client for CIFAR-100 dataset, and the results
are reported in Fig. 5 and Tab. 5."
APPENDIX,0.7270642201834863,"Results. From Fig. 5, we observe that PM’s accuracy drops as the IID degree rises (the number of
classes owned by one client rises). Since clients hold more classes of data, their PMs have lower
preferences to each class (i.e., compromised personalization), so PMs’ accuracy degrades. This
result is also consistent with the argument in Shen et al. (2020): GM targets to improve its gener-
alization but PM aims to improve its personalization, so Non-IID is beneficial to PM but harmful
to GM. Our pFedKT presents superiority on severely Non-IID data, but performs marginally worse
than baselines in high IID data, such as CIFAR-10 with class = {8, 9, 10} and CIFAR-100 with class
= {70, 80, 90, 100}. Nevertheless, decentralized data held by devices participating in FL are often
highly Non-IID (Kairouz & et al., 2021), hence our pFedKT’s utility is still practical."
APPENDIX,0.7293577981651376,"Analysis. pFedHN uses a server’s hypernetwork to learn local models’ parameter distributions
instead of simple weighted aggregation in FedAvg, when local datasets are more IID, the server’s
hypernetwork learn local knowledge more evenly and hence showing higher accuracy as IID degrees
increase. FedPHP allows each client’s linearly accumulated old local model to teach the received
global model in a knowledge distillation paradigm, it actually trains the global model, so it adapts
more to IID data. Our pFedKT performs worse than baselines in IID data because it emphasizes the
local training of personalized private models and increasing IID degrees may compromise personal-
ization."
APPENDIX,0.731651376146789,"2
4
6
8
10
class 60 80 100"
APPENDIX,0.7339449541284404,PM@Acc (%)
APPENDIX,0.7362385321100917,CIFAR-10 (Non-IID) PM@Acc
APPENDIX,0.7385321100917431,"FedPHP
MOON
pFedHN
Fed-ROD
pFedKT"
APPENDIX,0.7408256880733946,"20
40
60
80
100
class 20 40 60"
APPENDIX,0.7431192660550459,PM@Acc (%)
APPENDIX,0.7454128440366973,CIFAR-100 (Non-IID) PM@Acc
APPENDIX,0.7477064220183486,"FedPHP
MOON
pFedHN
Fed-ROD
pFedKT"
APPENDIX,0.75,Figure 5: Test accuracy of PM on CIFAR-10/100 datasets varies with different Non-IID degrees.
APPENDIX,0.7522935779816514,Under review as a conference paper at ICLR 2023
APPENDIX,0.7545871559633027,Table 5: Numerical test accuracy of PM on CIFAR-10/100 datasets with different Non-IID degrees.
APPENDIX,0.7568807339449541,"PM@Acc, CIFAR-10 (Non-IID)
class
1
2
3
4
5
6
7
8
9
10
FedPHP
100.00
52.46
75.02
72.99
69.09
68.01
66.40
66.99
65.30
63.60
MOON
100.00
51.78
71.22
72.46
65.06
63.29
65.18
64.36
64.84
63.27
pFedHN
100.00
90.03
81.93
76.82
72.17
68.20
67.23
65.10
64.65
60.02
Fed-RoD
100.00
88.41
81.63
74.71
69.45
66.12
64.95
64.61
63.78
61.84
pFedKT
100.00
90.48
84.44
75.94
71.51
66.64
67.21
64.76
63.29
60.47
PM@Acc, CIFAR-100 (Non-IID)
class
10
20
30
40
50
60
70
80
90
100
FedPHP
6.47
32.43
28.60
26.81
24.90
23.86
23.13
23.31
22.74
21.68
MOON
5.78
28.07
26.90
26.42
23.57
21.67
22.71
22.17
20.57
19.43
pFedHN
58.20
41.40
34.22
30.81
26.36
25.08
24.42
21.68
22.79
19.70
Fed-RoD
58.25
45.37
37.01
31.78
27.50
25.31
24.26
22.96
23.37
20.41
pFedKT
61.24
44.79
33.00
32.02
25.35
24.57
20.74
19.80
18.16
15.51"
APPENDIX,0.7591743119266054,"F.2.3
COMPARISONS WITH BASELINES UNDER DIFFERENT CLIENT PARTICIPATION RATES"
APPENDIX,0.7614678899082569,"To evaluate the effects of our pFedKT and baselines under different client participation rates (i.e.,
fraction C), we conduct experiments on CIFAR-10/100 datasets with 50 clients. We vary fraction
C ∈{0.1, 0.2, ..., 1} and report the results in Fig. 6. We can observe that: a) our pFedKT presents the
highest PM accuracy in any fraction setting; b) pFedKT’s PM accuracy is less affected by fraction.
In addition, we also test our pFedKT and state-of-the-art Fed-RoD under the FL settings with 500
clients and lower frac = 0.01 client participation rate. The PM accuracy of Fed-RoD and our
pFedKT are 79.79%, 79.98% on CIFAR-10 dataset and 30.85%, 31.63% on CIFAR-100 dataset.
All the above results verify that pFedKT is robust to client participation rates."
APPENDIX,0.7637614678899083,"0.2
0.4
0.6
0.8
1.0
frac 60 80"
APPENDIX,0.7660550458715596,PM@Acc (%)
APPENDIX,0.768348623853211,CIFAR-10 (Non-IID) PM@Acc
APPENDIX,0.7706422018348624,"FedAvg
FedROD
pFedKT"
APPENDIX,0.7729357798165137,"0.2
0.4
0.6
0.8
1.0
frac 20 40 60"
APPENDIX,0.7752293577981652,PM@Acc (%)
APPENDIX,0.7775229357798165,CIFAR-100 (Non-IID) PM@Acc
APPENDIX,0.7798165137614679,"FedAvg
FedROD
pFedKT"
APPENDIX,0.7821100917431193,"Figure 6: Test accuracy of PM varies with different client participation rates (fraction C) on CIFAR-
10/100 datasets."
APPENDIX,0.7844036697247706,"F.3
DETAILED EXPERIMENTAL RESULTS IN CASE STUDY"
APPENDIX,0.786697247706422,"Here we report the detailed experimental results of five cases on CIFAR-10 (Non-IID: 2/10) and
CIFAR-100 (Non-IID: 10/100) datasets in Fig. 7 and Tab. 6-9."
APPENDIX,0.7889908256880734,"20
40
emb_dim 89.5 90.0"
APPENDIX,0.7912844036697247,PM@Acc (%)
APPENDIX,0.7935779816513762,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.7958715596330275,"10
20
30
40
50
emb_dim 60 62"
APPENDIX,0.7981651376146789,PM@Acc (%)
APPENDIX,0.8004587155963303,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.8027522935779816,"1
2
3
4
5
n_hidden 60 70 80"
APPENDIX,0.805045871559633,PM@Acc (%)
APPENDIX,0.8073394495412844,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.8096330275229358,"1
2
3
4
5
n_hidden 20 40"
APPENDIX,0.8119266055045872,PM@Acc (%)
APPENDIX,0.8142201834862385,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.8165137614678899,"0.0
0.2
0.4
0.6
0.8 20 40 60 80"
APPENDIX,0.8188073394495413,PM@Acc (%)
APPENDIX,0.8211009174311926,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.823394495412844,"0.00
0.02
82.5 85.0"
APPENDIX,0.8256880733944955,"0.0
0.2
0.4
0.6
0.8
0 20 40"
APPENDIX,0.8279816513761468,PM@Acc (%)
APPENDIX,0.8302752293577982,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.8325688073394495,"0.00
0.02"
APPENDIX,0.8348623853211009,"50.0
52.5"
APPENDIX,0.8371559633027523,"0
20
40
alpha 85 86 87"
APPENDIX,0.8394495412844036,PM@Acc (%)
APPENDIX,0.841743119266055,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.8440366972477065,"0
20
40
alpha 48 50 52 54"
APPENDIX,0.8463302752293578,PM@Acc (%)
APPENDIX,0.8486238532110092,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.8509174311926605,"Figure 7: PMs’ mean test accuracy varies with the hypernetwork’s input embedding dimension and
number of hidden layers (size), the weight µ of contrastive loss, and the margin α in triplet loss."
APPENDIX,0.8532110091743119,Under review as a conference paper at ICLR 2023
APPENDIX,0.8555045871559633,"Table 6: The test accuracy of PM varies with the
hypernetwork’s input embedding dimension."
APPENDIX,0.8577981651376146,"Dataset
CIFAR-10 (Non-IID)
CIFAR-100 (Non-IID)
emb dim
PM@Acc
PM@Acc
6
89.30
58.46
13
90.34
59.99
17
90.24
58.88
26
89.90
62.07
51
90.29
62.83"
APPENDIX,0.8600917431192661,"Table 7: The test accuracy of PM varies with the
hypernetwork’s number of hidden layers (size)."
APPENDIX,0.8623853211009175,"Dataset
CIFAR-10 (Non-IID)
CIFAR-100 (Non-IID)
n hidden
PM@Acc
PM@Acc
1
86.16
52.21
2
85.78
53.76
3
83.13
48.77
4
75.56
17.26
5
57.79
12.15"
APPENDIX,0.8646788990825688,"Table 8: The test accuracy of PM varies with the
weight µ of contrastive loss."
APPENDIX,0.8669724770642202,"Dataset
CIFAR-10 (Non-IID)
CIFAR-100 (Non-IID)
µ
PM@Acc
PM@Acc
0
82.39
48.92
0.0001
83.47
52.74
0.001
86.16
52.21
0.01
84.43
50.50
0.1
53.30
11.41
0.3
47.32
8.67
0.5
35.23
5.35
0.7
24.49
4.73
0.9
19.07
1.24"
APPENDIX,0.8692660550458715,"Table 9: The test accuracy of PM varies with the
margin α in triplet loss."
APPENDIX,0.8715596330275229,"Dataset
CIFAR-10 (Non-IID)
CIFAR-100 (Non-IID)
alpha
PM@Acc
PM@Acc
0
86.16
52.21
0.1
87.54
51.38
1
85.63
53.20
5
87.27
53.73
10
87.02
52.19
20
86.74
53.65
30
86.74
53.65
40
85.04
52.06
50
84.56
47.91"
APPENDIX,0.8738532110091743,"C. Loss functions in contrastive learning
There are various ways to measure the distances between two models’ logits vectors, such as KL
divergence, 1−(cosine similarity), L2 norm, and MSE (the square of L2 norm). There are also
diverse contrastive loss functions, such as triplet loss (Schroff & et al., 2015), loss in MOON (Li
et al., 2021), and etc. In fact, we pursue PM to be close to GM via contrastive learning, so a naive
way is to add the euclidean distance (L2 norm) between the GM and PM as the regularization to the
supervised loss, like FedProx (Li & et al, 2020). We evaluated several combinations of the above
distance measurements and loss functions, and the results are reported in Tab. 10."
APPENDIX,0.8761467889908257,"We can observe that the combination of triplet loss and KL divergence that we designed in pFedKT
achieves the best PM on the CIFAR-10 dataset. pFedKT with loss used in MOON (Li et al., 2021)
gets the highest PM’s accuracy on the CIFAR-100 dataset but lower PM’s accuracy on CIFAR-10
dataset than pFedKT with triplet loss and KL divergence. Other combinations also show worse
model performances than our designed loss for pFedKT. It’s worth noting that pFedKT with con-
trastive loss has higher accuracy than with L2 regularization, which verifies using contrastive loss to
keep new PM (generated by HN) away from old PM is necessary and does prevent being into local
optima."
APPENDIX,0.8784403669724771,Table 10: PM’s test accuracy varies with combinations of distance measurements and loss functions.
APPENDIX,0.8807339449541285,"Dataset
CIFAR-10
CIFAR-100
Loss Function
Distance Measurement
PM@Acc
PM@Acc"
APPENDIX,0.8830275229357798,Triplet Loss
APPENDIX,0.8853211009174312,"KL divergence
86.16
52.21
1−(cosine similarity)
84.38
53.48
L2 norm
85.94
52.41
MSE
59.18
12.96
Moon loss
cosine similarity
84.66
53.81"
APPENDIX,0.8876146788990825,L2 Regularization
APPENDIX,0.8899082568807339,"KL divergence
84.84
52.96
cosine similarity
84.39
53.22
L2 norm
85.99
51.34
MSE
83.55
49.17"
APPENDIX,0.8922018348623854,"Table 11: Results of ablation experiments. “HN→PM” denotes private knowledge transfer by hy-
pernetworks, “CL” represents global knowledge transfer through contrastive learning."
APPENDIX,0.8944954128440367,"Dataset
CIFAR-10
CIFAR-100
Case
HN→PM
CL
PM@Acc
PM@Acc
A
✗
✗
51.64±0.02
4.59±0.39
B
✓
✗
90.10±0.51
61.23±0.67
C
✗
✓
51.78±0.07
16.40±0.03
D
✓
✓
90.34±0.12
61.66±0.08"
APPENDIX,0.8967889908256881,Under review as a conference paper at ICLR 2023
APPENDIX,0.8990825688073395,"0
100
200
300
400
500
Communication Rounds 0.65 0.70 0.75 0.80 0.85 0.90"
APPENDIX,0.9013761467889908,Test Accuracy
APPENDIX,0.9036697247706422,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.9059633027522935,"Case B
Case D"
APPENDIX,0.908256880733945,"0
100
200
300
400
500
Communication Rounds 0.1 0.2 0.3 0.4 0.5 0.6"
APPENDIX,0.9105504587155964,Test Accuracy
APPENDIX,0.9128440366972477,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.9151376146788991,"Case B
Case D"
APPENDIX,0.9174311926605505,"Figure 8: PMs’ mean test accuracy of case B (HN→PM w/o CL) and case D (HN→PM w/ CL) vary
with communication rounds on CIFAR-10/100 datasets."
APPENDIX,0.9197247706422018,"G
PFEDKT’S COMPUTATIONAL COMPLEXITY, STORAGE COST AND MODEL
PERFORMANCES"
APPENDIX,0.9220183486238532,"In this section, we compare pFedKT, state-of-the-art pFedHN and other baselines in computational
complexity, storage overhead, and model performances."
APPENDIX,0.9243119266055045,"G.1
COMPUTATIONAL COMPLEXITY"
APPENDIX,0.926605504587156,"Taking the CNN (CIFAR-10) and the hypernetwork in Tab. 2 as examples, we denote CNN (CIFAR-
10) as NN (i.e., target private model) and the hypernetwork as HN(small). In pFedHN (Shamsian
& et al, 2021), one large hypernetwork is deployed on the server, which we denotes as HN(large).
For fair comparisons, we set the same number of hidden layers of HN(small) and HN(large), i.e.,
the structure of HN(large) is: fc1(emb dim=13, 100) →fc2(100,100) →fc3(100, NN Param). We
compute the tree models’ parameter capacity and computational overhead for one-time forward
operation, and the results are recorded in Tab. 12"
APPENDIX,0.9288990825688074,"Table 12: The three models’ parameter capacity and computational overhead (FLOPs) for one-time
forward operation."
APPENDIX,0.9311926605504587,"Metric
Parameter Capacity
Computational Overhead (FLOPs)
Layers
NN
HN (small)
HN (large)
NN
HN (small)
HN (large)
conv1
(5*5*3+1)*16
-
-
((3*5*5)+(3*5*5-1)+1)*16*28*28
-
-
conv2
(5*5*16+1)*32
-
-
((16*5*5)+(16*5*5-1)+1)*32*10*10
-
-
fc1
(800+1)*1920
(13+1)*100
(13+1)*100
2*800*1920
2*13*100
2*13*100
fc2
(1920+1)*80
(100+1)*100
(100+1)*100
2*1920*80
2*100*100
2*100*100
fc3
(80+1)*10
(100+1)*400
(100+1)* 1706458
2*80*10
2*100*400
2*100*1706458
Total (number)
1706458
51900
172363758
7822400
102600
341314200
Total (MB/GB)
6.5096 MB
0.1980 MB
657.5156 MB
0.0291 GB
0.0004 GB
1.2715 GB"
APPENDIX,0.9334862385321101,"500
1000
1500
2000
HN_output 1.2 1.4 1.6 1.8 2.0"
APPENDIX,0.9357798165137615,HN(small)_FLOPs/HN(large)_FLOPs
APPENDIX,0.9380733944954128,Computational Complexity (a)
APPENDIX,0.9403669724770642,"0
100
200
300
400
500
Round 0.4 0.5 0.6 0.7 0.8 0.9"
APPENDIX,0.9426605504587156,PM Test Accuracy
APPENDIX,0.944954128440367,CIFAR-10 (Non-IID: 2/10)
APPENDIX,0.9472477064220184,"pFedKT(large HN)
pFedKT(small HN) (b)"
APPENDIX,0.9495412844036697,"0
100
200
300
400
500
Round 0.0 0.2 0.4 0.6"
APPENDIX,0.9518348623853211,PM Test Accuracy
APPENDIX,0.9541284403669725,CIFAR-100 (Non-IID: 10/100)
APPENDIX,0.9564220183486238,"pFedKT(large HN)
pFedKT(small HN) (c)"
APPENDIX,0.9587155963302753,"Figure 9: (a): the ratio of computational complexity (FLOPs) between HN (small) and HN (large)
varies with the output dimension of HN (small); (b-c): PM’s accuracy of pFedKT with HN (small)
and HN (large) on CIFAR-10/100 datasets varies with rounds."
APPENDIX,0.9610091743119266,"From Tab. 12, it requires to call HN (small) (NN Paras/output of HN (small)) times to generate
the whole parameters for one NN in a stacking form, so generating NN by HN (small) requires
(NN Paras/output of HN(small))*HN (small) FLOPs, i.e., (1706458/400) ∗102600 = 1.6306 GB"
APPENDIX,0.963302752293578,Under review as a conference paper at ICLR 2023
APPENDIX,0.9655963302752294,"FLOPs. Using HN (large) to generate one NN once consumes 1.2715 GB FLOPs, as shown in
Tab. 12. Hence the ratio of the former and the latter is about 1.28×, and it tends to be 1× as the
output dimension of HN (small) increases, as shown in Fig. 9 (a). In short, our pFedKT consumes
comparable computational cost to pFedHN."
APPENDIX,0.9678899082568807,"Besides, pFedHN updates the server’s HN (large) once if it receives one private model. Using one
private model to update HN (large) and then using the updated HN (large) to generate parameters
for the private model consume 1.2715*2 GB FLOPs. When multiple private models reach the server
simultaneously, computational blocking may occur due to the high computational complexity of HN
(large). Whereas, our pFedKT deploys one HN (small) on each client. From the perspective of
computational complexity, our pFedKT inherently offloads the training tasks with the server’s large
hypernetwork in pFedHN to clients’ sub-tasks, which tackles the above blocking issue."
APPENDIX,0.9701834862385321,"G.2
STORAGE OVERHEAD"
APPENDIX,0.9724770642201835,"pFedHN’s server requires about 657 MB to store the HN (large), while our pFedKT’s N clients
consume N ∗0.1980 MB storage cost. When the number N of clients participating in FL is about
3321, our pFedKT has a comparable storage cost to pFedHN. But in the cross-silo FL scenario, there
are often a few companies or institutions joining in FL (Kairouz & et al., 2021), so our pFedKT has
obvious strength than pFedHN in terms of storage cost."
APPENDIX,0.9747706422018348,"G.3
MODEL PERFORMANCE"
APPENDIX,0.9770642201834863,"We have compared the model performances of pFedHN and our pFedKT in Sec. 6.2, here we do not
repeat it. As illustrated above, we require to call HN (large) once or HN (small) multiple times to
generate parameters for one NN. Here, we also test our pFedKT with private HN (small) and private
HN (large) on CIFAR-10/100 datasets, and the results are shown in Tab. 13 and Fig. 9 (b)-(d). It
can be seen that pFedKT with HN (large) shows similar model performances with FedAvg, which
is consistent with our conclusion of the case study on HN’s size in Sec. ??: larger HN is harder to
train, hence showing worse model accuracy. This evaluation also verifies the strength of our calling
HN (small) multiple times on model performances."
APPENDIX,0.9793577981651376,Table 13: The results of pFedKT with HN (large) and HN (small) on CIFAR-10/100 datasets.
APPENDIX,0.981651376146789,"Dataset
CIFAR-10
CIFAR-100
Method
PM@Acc
PM@Acc
FedAvg
51.64
4.59
pFedHN
90.03
58.2
pFedKT (small HN)
90.34
61.66
pFedKT (large HN)
51.67
5.04"
APPENDIX,0.9839449541284404,"G.4
COMPARED WITH MORE BASELINES"
APPENDIX,0.9862385321100917,"We also compare our pFedKT with the typical FedAvg and related MOON, Fed-ROD in computa-
tional overhead and storage efficiency."
APPENDIX,0.9885321100917431,"pFedKT v.s. FedAvg. Compared with FedAvg, pFedKT introduces extra local computation of a
local hypernetwork and contrastive loss, and additionally stores a local hypernetwork and a previous
local model. But, as show in Tab. 1 pFedKT’s accuracy is improved 38.7% in CIFAR-10 and 57.07%
in CIFAR-100. Therefore, in cross-silo FL scenario, increased cost compared with obvious accuracy
improvement is acceptable for participating enterprises and institutions."
APPENDIX,0.9908256880733946,"pFedKT v.s. MOON. Compared with MOON, pFedKT introduces the computation of a local hyper-
network and additionally store a local hypernetwork for each client. But pFedKT improves 38.56%
and 56.41% accuracy than MOON in CIFAR-10 and CIFAR-100, respectively."
APPENDIX,0.9931192660550459,"pFedKT v.s. Fed-ROD. Compared with FedAvg, Fed-ROD’s extra computations involve that using
a local hypernetwork to generate parameters for a personalized header and locally train personalized
branch, while it also requires to store a local hypernetwork additionally. Compared with Fed-ROD,
our pFedKT introduces the following computations: using a local hypernetwork to generate pa-
rameters for a complete local target model and calculate contrastive loss. Since pFedKT’s local"
APPENDIX,0.9954128440366973,Under review as a conference paper at ICLR 2023
APPENDIX,0.9977064220183486,"hypernetwork generates parameters for a whole local model and Fed-ROD’s local hypernetwork
generates parameters for the personalized header of a local model, pFedKT incurs relatively higher
computation cost. In addition, pFedKT requires to store a local hypernetwork and a previous local
model, but Fed-ROD only stores a local hypernetwork, so pFedKT has a slightly higher storage
overhead. However, similar to the above analysis, pFedKT improves 0.31% and 3.46% accuracy
improvements than Fed-ROD in CIFAR-10/100 datasets, which is acceptable."
