Section,Section Appearance Order,Paragraph
MCGILL UNIVERSITY,0.0,"1McGill University
2Mila – Quebec Artiﬁcial Intelligence Institute 3Microsoft Research
4Canada CIFAR AI Chair, Mila"
ABSTRACT,0.004219409282700422,ABSTRACT
ABSTRACT,0.008438818565400843,"A fundamental characteristic of natural language is the high rate at which speak-
ers produce novel expressions. Because of this novelty, a heavy-tail of rare events
accounts for a signiﬁcant amount of the total probability mass of distributions
in language (Baayen, 2001). Standard language modeling metrics such as per-
plexity quantify the performance of language models (LM) in aggregate. As a
result, we have relatively little understanding of whether neural LMs accurately
estimate the probability of sequences in this heavy-tail of rare events. To address
this gap, we develop a controlled evaluation scheme which uses generative models
trained on natural data as artiﬁcial languages from which we can exactly compute
sequence probabilities. Training LMs on generations from these artiﬁcial lan-
guages, we compare the sequence-level probability estimates given by LMs to the
true probabilities in the target language. Our experiments reveal that LSTM and
Transformer language models (i) systematically underestimate the probability of
sequences drawn from the target language, and (ii) do so more severely for less-
probable sequences. Investigating where this probability mass went, (iii) we ﬁnd
that LMs tend to overestimate the probability of ill-formed (perturbed) sequences.
In addition, we ﬁnd that this underestimation behaviour (iv) is weakened, but not
eliminated by greater amounts of training data, and (v) is exacerbated for target
distributions with lower entropy."
INTRODUCTION,0.012658227848101266,"1
INTRODUCTION"
INTRODUCTION,0.016877637130801686,"Figure 1: GPT2 sequence probability es-
timates plotted against the true sequence
probabilities. Neural LMs underestimate the
probability of sequences drawn from the lan-
guage they are trained to model."
INTRODUCTION,0.02109704641350211,"Natural language is fundamentally creative—speakers
and listeners frequently produce and comprehend sen-
tences which have never been produced before (Fodor,
1975; Fodor & Pylyshyn, 1988; Chomsky, 1975, 1955).
As a side-effect of this property, distributions in natural
language are characterized by a heavy-tail of individu-
ally improbable events which collectively account for a
signiﬁcant amount of the total probability mass of the
distribution (Khmaladze, 1988; Baayen, 2001). Precisely
approximating this large number of rare events is one
of the foundational challenges for models of natural lan-
guage (Good, 1953; Jelinek, 1980; Katz, 1987; Kneser &
Ney, 1995; Wood et al., 2011; Goldwater et al., 2011).
Autoregressive neural language models (Bengio et al.,
2003; Mikolov et al., 2013; Radford et al., 2019) attempt
to do so by decomposing the probability of an event (a
sequence) into a series of conditional distributions, each
parameterized by a shared neural network."
INTRODUCTION,0.02531645569620253,"Recently, a growing body work has sought to understand
how these language models (LM) ﬁt the distribution of a language beyond standard measures such"
INTRODUCTION,0.029535864978902954,"†Corresponding author: benjamin.lebrun@mail.mcgill.ca
*Co-senior autorship"
INTRODUCTION,0.03375527426160337,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0379746835443038,"as perplexity. Meister & Cotterell (2021), for example, investigated the statistical tendencies of the
distribution deﬁned by neural LMs, whereas Kulikov et al. (2021) explored whether they adequately
capture the modes of the distribution they attempt to model. At the same time, increased focus has
been given to performance on rare or novel events in the data distribution, both for models of natural
language (McCoy et al., 2021; Lent et al., 2021; Dudy & Bedrick, 2020; Oren et al., 2019) and neural
models more generally (see, for example Sagawa et al., 2020; D’souza et al., 2021; Chen et al., 2021;
Blevins & Zettlemoyer, 2020; Czarnowska et al., 2019; Horn & Perona, 2017; Ouyang et al., 2016;
Bengio, 2015; Zhu et al., 2014). Neither of these branches of work, however, has explored instance-
level LM performance on rare sequences in the distribution. As a result, we have relatively little
understanding of how neural LMs approximate sequences in the heavy-tail characteristic of natural
language."
INTRODUCTION,0.04219409282700422,"In this work, we introduce a controlled methodology to explore how LMs estimate the probability
of sequences in the heavy-tail of the distribution. Our instance-level evaluation scheme explicitly
compares the target probability distribution of the language to the distribution deﬁned by the LM.
Since the true distribution of any natural language is in practice unknown, we use a Transformer
LM trained on natural data as a generative model to deﬁne target artiﬁcial languages for which we
can exactly compute sequence probabilities. Training LSTM and Transformer LMs on sequences
sampled from these target artiﬁcial languages, we compare the sequence-level probability estimates
given by neural LMs to the target probabilities in the language. By controlling the entropy of the
generative model’s conditional distributions, we create a set of artiﬁcial languages with varying
distributional properties, and analyze how LM estimation behaviour is modulated by the properties
of the target distribution."
INTRODUCTION,0.046413502109704644,"Our experiments uncover the extent to which neural LMs provide a distorted ﬁt of the language
they are trained to model. We ﬁnd that LSTM and Transformer LMs (i) systematically underesti-
mate the probability of sequences drawn from the target language and (ii) do so more when such
sequences are rare. Where did this underestimated probability mass go? We do not ﬁnd that the
underestimation is accompanied by overestimation in the head of distribution. Rather, we ﬁnd that
LMs tend to (iii) overestimate the probability of rare perturbed (ill-formed) sequences. Interpreted
together, these ﬁndings indicate that on the one hand, neural LMs under-represent well-formed se-
quences in the tail of the language they attempt to model, and on the other hand, over-represent
ill-formed sequences far away from high probability zones in sequence-space. In addition, we ﬁnd
that (iv) greater amounts of training data lessen underestimation but do not eliminate it and that (v)
underestimation is exacerbated for target distributions with lower entropy."
BACKGROUND,0.05063291139240506,"2
BACKGROUND"
BACKGROUND,0.05485232067510549,"We begin by brieﬂy characterizing why distributions with a large number of rare events (LNRE)
emerge in natural language, and why these events pose challenges for LMs. Furthermore, we moti-
vate the need for instance-level evaluation when dealing with a large number of rare events."
BACKGROUND,0.05907172995780591,"Productivity
In the context of language production, a language user has the ability to produce,
at any given point in their linguistic lifespan, an utterance which they have never produced before.
This creativity is the result of the generative property of productivity, which states that on the basis
of ﬁnite linguistic experience, a language user can produce and comprehend an unbounded number
of grammatically acceptable utterances (Chomsky, 1975, 1955). Productive processes induce a dis-
tribution which places non-zero probability on unseen events at all practical sample sizes. Because
of this property, many of the distributions in natural language—particularly the distribution over the
sequences of a language—are characterized by a heavy-tail of rare events."
BACKGROUND,0.06329113924050633,"LNRE Zone
To make explicit the connection between productivity and a heavy-tail of rare events,
let PN denote the probability of sampling a novel (previously unseen) event from some distribution
after having sampled N events. Then productivity as described above states that PN > 0 for all
sample sizes N that occur in practice. The range of sample sizes N for which it is the case that
PN > 0 is known as the LNRE zone (Khmaladze, 1988; Baayen, 2001). The LNRE zone for
natural language appears to be very large, and it seems likely that PN will remain greater than 0 for
samples of natural language many orders of magnitude larger than all the data currently available"
BACKGROUND,0.06751054852320675,Published as a conference paper at ICLR 2022
BACKGROUND,0.07172995780590717,"for training LMs.1 In the LNRE zone, it is difﬁcult to obtain accurate estimates of the probability of
events using straightforward maximum likelihood estimation (MLE). Accounting for this enormous
amount of novelty is thus a central challenge in language modeling."
BACKGROUND,0.0759493670886076,"Language modeling
A model M of the language L attempts to deﬁne a distribution pM which
closely resembles the true distribution of the language pL. In a locally-normalized autoregressive
model, this distribution is deﬁned by assigning probabilities to variable length sequences x via a
chain-rule decomposition:"
BACKGROUND,0.08016877637130802,"p(x) = |x|
Y"
BACKGROUND,0.08438818565400844,"i=1
p(xi | x1:i−1) = |x|
Y i=1"
BACKGROUND,0.08860759493670886,"exp ρ(x1:i−1, xi)
P"
BACKGROUND,0.09282700421940929,"x∈Σ exp ρ(x1:i−1, x)
(1)"
BACKGROUND,0.0970464135021097,"where Σ is the vocabulary, and ρ(x1:i−1, xi; θ) is the non-negative score of token xi given the
sequence preﬁx x1:i−1, which is computed by a neural network with parameters θ."
BACKGROUND,0.10126582278481013,"For pM to perfectly approximate pL, we expect pM(x) = pL(x) for all x ∈Σ∗, where Σ∗is the
set of all strings of ﬁnite length (the Kleene closure of Σ). In the LNRE zone, pM is deﬁned over a
support containing a very large set of sequences which have never occurred in a training corpus (or
equivalently, have all occurred with frequency 0), and which take on a very wide array of differing
probabilities. For example, while the sequences x1, x2 ∈Σ∗have likely never occurred in any
sample of English, most would agree that x1 is far more probable than x2:"
BACKGROUND,0.10548523206751055,x1: The East pond in Parc Lafontaine was ﬁlled to the brim with Diet Coke.
BACKGROUND,0.10970464135021098,x2: Certain nak indicate liberationing among theorter codity voters vandalized.
BACKGROUND,0.11392405063291139,"LM Evaluation
For a perfect LM of English, we would expect the estimated probabilities of
the sequences x1 and x2 to match their probabilities under the true distribution pEnglish. However,
since pEnglish and it’s underlying generative process are unknown, it is not possible to explicitly
evaluate how closely instance-level probability estimates align. As a proxy, the mean perplexity of
the model on a holdout set of sequences D is typically used, which measures whether the model,
on average, assigns high likelihood to unseen instances. This measure does not, however, tell us
whether instance-level estimates align with their true counterparts, nor is it necessarily indicative of
performance on rare, idiosyncratic events in D. In this way, the lack of access to the ground-truth
distribution severely complicates LM evaluation on the heavy-tail of rare sequences in language.
The following section introduces a methodology to overcome these limitations."
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.11814345991561181,"3
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE"
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.12236286919831224,"Component
Notation
Description"
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.12658227848101267,"Generative model
L
A LM trained on natural instance-level data.
Artiﬁcial language
pL
The distribution over sequences induced by a sampling scheme from L.
Language model
pM
The distribution of a LM trained on sequences sampled from pL.
Target probabilities
pL(x)
The probability assigned by pL to the sequence x.
Model probabilities
pM(x)
The probability assigned by pM to the sequence x."
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.1308016877637131,"Table 1: Components of our instance-level evaluation scheme. Training pM on samples from pL,
we compare pM(x) to pL(x) for x ∈Σ∗."
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.1350210970464135,"We propose evaluating language model performance on the heavy-tail of rare events via a known
probability distribution over sequences. Speciﬁcally, we train a Transformer LM on sequences sam-
pled from a corpus of natural language to deﬁne a generative model L. The distribution over se-
quences induced by a sampling scheme from L, denoted pL, is then our artiﬁcial language. We ex-
pect a model M of this artiﬁcial language to assign probabilities pM(x) to sequences x which match"
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.13924050632911392,"1For an empirical validation of this claim on a sample of practical size from the OpenWebText corpus, see
the Appendix."
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.14345991561181434,Published as a conference paper at ICLR 2022
LANGUAGE MODEL EVALUATION IN THE LNRE ZONE,0.14767932489451477,"the target probabilities pL(x) of x under pL. To characterize neural LM behaviour on rare events,
we train Transformer and LSTM LMs on data sampled from pL, and compare the instance-level
probability estimates given by pM to target probabilities under pL. We summarize the components
of this methodology in Table 1, and overview it in greater detail in the following section."
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.1518987341772152,"3.1
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS"
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.15611814345991562,"x
log pL(x)"
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.16033755274261605,"“we’re very excited to have the opportunity to help them,” he says.
−29.3811
so what’s going to happen?
−17.4128
to me, the ﬁsheries are in the midst of a global ﬁnancial crisis.
−41.4835"
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.16455696202531644,Table 2: Sample of sequences drawn from our artiﬁcial language.
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.16877637130801687,"To deﬁne a generative model L, we train a randomly-initialized GPT2-medium on 1.5M sentences
sampled from the OpenWebText corpus (Gokaslan & Cohen, 2019).2 We set the maximum sequence
length to be 128 tokens. We additionally train a byte-pair-encoding (BPE) tokenizer on this data set
with a standard GPT2 vocabulary size of 57,256 tokens. For simplicity, this tokenizer is used for all
models."
ARTIFICAL LANGUAGES AS TARGET DISTRIBUTIONS,0.1729957805907173,"Using this generative model L, we deﬁne the target distribution over sequences—the artiﬁcial
language—as the distribution induced by an ancestral sampling scheme from L. Thus, we draw
instances x = (x1, . . . , x|x|) from our language pL by recursively sampling from the conditional
distribution over tokens at each time step: xt ∼pL(· | x<t) where x1 = BOS and xt = EOS. All
experiments up until Section 4.5 are conducted on the distribution induced by ancestrally sampling
from L with softmax temperature T = 0.85.3 In Section 4.5, we explore the effects of different
values of T when sampling from pL. Table 2 shows three sequences sampled from this distribution."
SEQUENCE PROBABILITY ESTIMATION IN THE LNRE ZONE,0.17721518987341772,"3.2
SEQUENCE PROBABILITY ESTIMATION IN THE LNRE ZONE"
SEQUENCE PROBABILITY ESTIMATION IN THE LNRE ZONE,0.18143459915611815,"Given an artiﬁcial language pL, the task of M—the language model—is to deﬁne a distribution
pM whose sequence-level probability estimates closely align with the sequence-level probabilities
given by pL. We refer to any deviation from this desiderata as model estimation error. To quantify
the model estimation error for a sequence x, we take the difference between the sequence’s log
probability under M and its true log probability under L:"
SEQUENCE PROBABILITY ESTIMATION IN THE LNRE ZONE,0.18565400843881857,"error(x) = log pM(x) −log pL(x)
(2)"
SEQUENCE PROBABILITY ESTIMATION IN THE LNRE ZONE,0.189873417721519,"This quantity is the log probability ratio, which measures, in log-space, the number of times more
or less likely the sequence x is under the language model M. Note that error(x) < 0 indicates that
M underestimates the probability of x, whereas error(x) > 0 indicates that M overestimates the
probability of x. In practice, we train M on a set of sequences Dtrain sampled from pL, and compute
model estimation error on a separate set of sequences Dtest sampled from pL. In all cases, we
compute the probability of a sequence x as its chain rule decomposition: p(x) = Q|x|
i=1 p(xi | x<i)
where x0 = BOS and x|x| = EOS. When computing the ground-truth sequence probabilities for
pL, we take into account any softmax tempering."
NEURAL LANGUAGE MODELS,0.1940928270042194,"3.3
NEURAL LANGUAGE MODELS"
NEURAL LANGUAGE MODELS,0.19831223628691982,"We study the estimation performance of two neural LM architectures: the Transformer (Vaswani
et al., 2017) and the LSTM (Melis et al., 2020). When training either architecture, we halve the
learning rate if validation loss increases at the end of an epoch. For all model sizes, we use a batch
size of 128 sequences. Models with the lowest cross-entropy loss on a withheld validation set are
used in experiments unless otherwise mentioned."
NEURAL LANGUAGE MODELS,0.20253164556962025,"2All Transformer implementations were obtained from Huggingface, and training was done on two or four
RTX-8000 GPUs (depending on model size) with mixed ﬂoating point precision.
3We temper in an effort to deﬁne a ground-truth distribution whose entropy more closely resembles that of
a natural language. Note that our ﬁndings hold for untempered (T = 1.00) ground-truth distributions as well
(see A.1.1 and A.1.2)."
NEURAL LANGUAGE MODELS,0.20675105485232068,Published as a conference paper at ICLR 2022
NEURAL LANGUAGE MODELS,0.2109704641350211,"Figure 2: Test sequence probability estimates given by neural LMs. Three left-most ﬁgures: The
joint histograms of sequence probability estimates. The dotted line denotes the cases in which the
model’s estimates perfectly align with the target probability; shading to the right of this line denotes
underestimation. Right-most ﬁgure: Mean sequence estimation error by target sequence probability."
NEURAL LANGUAGE MODELS,0.21518987341772153,"We use the Huggingface (Wolf et al., 2020) implementations of GPT2-small, GPT2-medium and
GPT2-large (Radford et al., 2019) as representative Transformer LMs. We use Adam Optimization
with ϵ = 1e−8 and learning rates α = 5e−5, α = 4e−5 and α = 3e−5 for GPT2-small, -medium and
-large respectively. Since these models are in the same model class as our artiﬁcial target language
pL, we expect the task of recovering the ground-truth distribution to be relatively easy compared to
the true problem faced in modeling natural language, where both the distribution and the underlying
generative process are unknown. For the LSTM (Hochreiter & Schmidhuber, 1997), we follow the
implementation of the baseline LM described in (Melis et al., 2020). We use 2 layers and adjust
the hidden state and embedding dimension (2048 and 1024, respectively) to be such that the total
number of parameters is approximately equal to GPT2-small (110M)."
RESULTS,0.21940928270042195,"4
RESULTS"
ESTIMATION ERROR WITH FIXED DATA,0.22362869198312235,"4.1
ESTIMATION ERROR WITH FIXED DATA"
ESTIMATION ERROR WITH FIXED DATA,0.22784810126582278,"We begin by exploring model estimation error on a ﬁxed training set Dtrain of 1M sequences sampled
from pL. We ﬁrst train LSTM and GPT models on Dtrain, early-stopping as described above. Fol-
lowing training, we sample a test set Dtest of 500,000 sequences from pL, and score each sequence
under both the model distribution pM and the true language distribution pL. From this, we obtain
a set of probability estimates: Stest = {⟨pL(x), pM(x)⟩| x ∈Dtest}. If the model M perfectly
models the language L, then for each ⟨pL(x), pM(x)⟩∈Stest we would expect pL(x) = pM(x).
Figures 2(A) and 2(B) visualize this relationship with the x- and y-axes denoting the true and model
estimated sequence probabilities respectively, and a dashed line representing equality. To compare
probability estimates, we represent the set Stest in the form of a joint histogram over this coordinate
space. Histogram bins are shaded based on the number of tuples which lie in the coordinate range
they deﬁne. Importantly, any deviation of this histogram from the identity line indicates that the
model distorts the shape of the distribution of the language."
ESTIMATION ERROR WITH FIXED DATA,0.2320675105485232,"Figures 2(A) and 2(B) provide evidence for distributional distortion in the form of underestimation.
The majority of probability tuples in Stest lie to the right of the identity line, indicating that LSTM
and GPT2 models consistently underestimate the probability of sequences sampled from pL. Fur-
thermore, the distance between the identity line and the probability tuples grows non-linearly as
function of the true sequence probability, indicating that underestimation error is more severe for
rarer sequences in the language. We validate these observations in the right-most plot of Figure
2, which shows mean estimation error decreasing non-linearly as a function of the target sequences
probability.4 In addition, comparing underestimation behaviour across model size, we ﬁnd that while
GPT2-medium performs slightly better than GPT2-small, these improvements are typically within
the range deﬁned by the bootstrapped 95% conﬁdence intervals. See A.1.3 for evidence indicating
that this underestimation behaviour also occurs in pre-trained models ﬁne-tuned on Dtrain."
ESTIMATION ERROR WITH FIXED DATA,0.23628691983122363,"4To compute this curve, we split the target sequence probability pL(x) range into N equally sized bins (by
probability range). We report the mean estimation error for each bin with > 10 sequences. We additionally
compute 95% conﬁdence intervals with 10, 000 bootstraps for each mean, resampling n equal to the number of
sequences in the given bin."
ESTIMATION ERROR WITH FIXED DATA,0.24050632911392406,Published as a conference paper at ICLR 2022
ESTIMATION ERROR WITH FIXED DATA,0.24472573839662448,"Figure 3: Mean model estimation error by training epoch (GPT2-medium). Each line denotes the
mean estimation error for a 50th of the sequences; darker lines represent less probable sequences.
The shaded area denotes the area in which validation cross-entropy reaches a minimum."
ESTIMATION ERROR ACROSS TRAINING TIME,0.2489451476793249,"4.2
ESTIMATION ERROR ACROSS TRAINING TIME"
ESTIMATION ERROR ACROSS TRAINING TIME,0.25316455696202533,"To understand the training dynamics underlying the previously reported underestimation, we com-
pute model probability estimates on subsets of Dtrain and Dtest at the end of each training iteration i.
Once computed, we sort each set of probability tuples S(i)
train and S(i)
test by their target sequence proba-
bilities pL(x), and split the probability tuples into 50 equally-sized bins. We plot estimation curves
in Figure 3: Each curve represents a 50th of the sequences, with darker curves denoting estimation
error for sequences with lower target probabilities (rarer sequences). At any given point, then, the
distance between estimation curves represents the degree to which estimation error is dependent on
the target probability of the sequence."
ESTIMATION ERROR ACROSS TRAINING TIME,0.25738396624472576,"Figure 3 left visualizes underestimation error for sequences seen in training. Around the ﬁfth epoch,
estimation error for train sentences converges to zero, that is (pL(x) ≈pM(x)), indicating that
GPT2-medium is able to almost perfectly recover the target probabilities of training sequences no
matter their target probability. At the same time, this convergence happens almost simultaneously
for all sequences, indicating that a complete reduction in error during training occurs throughout the
entire range of target sequence probabilities."
ESTIMATION ERROR ACROSS TRAINING TIME,0.2616033755274262,"Figure 3 right visualizes GPT2-medium model’s performance on a separate set of test sequences.
First, unlike for Dtrain, estimation error for Dtest does not converge to zero, meaning that even when
the model has perfectly recovered the target probability of train sequences, the target probabilities
for test sequences remain underestimated. Second, in the case of Dtrain, the difference between esti-
mation curves of different shades converges to zero, indicating that estimation performance becomes
uniform across the distribution of train sequences. We do not see such behaviour in Dtest. Instead,
the error curves remain at a relatively consistent distance from one another, indicating that the dis-
crepancy in estimation error at different parts of the distribution is unchanging for sequences not
seen during training."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.26582278481012656,"4.3
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA"
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.270042194092827,"Figure 4: GPT2-medium, GPT2-large and LSTM trained on 30M sequences sampled from pL.
Main plots: Mean estimation error on test sequences as a function of the number of sequences seen
in training. Inset plots: Relative change in mean estimation error of test sequences as a function of
the number of sequences seen in training. In both cases, each line denotes estimation behaviour for
a 50th of the test sequences; darker lines represent less probable sequences."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.2742616033755274,"Our previous experiment trained languages models on a set of 1M sequences. A plausible expla-
nation for the model’s underestimation behaviour on unseen test sequences is therefore that the
language model has not seen enough samples from the target distribution. Here we explore how"
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.27848101265822783,Published as a conference paper at ICLR 2022
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.28270042194092826,"Figure 5: GPT2-medium estimation behaviour for 15M sequences in Σ∗across two dimensions:
sequence rarity (x-axis) and degree of perturbation (y-axis). The heat map is shaded based on
estimation error severity; blue areas indicate overestimation, whereas brown areas indicate underes-
timation. We also include example sequences from two zones in this sequence space."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.2869198312236287,"estimation error varies as a function of the amount of training data. We train GPT2-medium, GPT2-
large and an LSTM model in the online “Ideal World” setting (Nakkiran et al., 2020) by sampling,
at the beginning of each training iteration, a fresh set of 500,000 sequences from pL, and training
M on this sample. Doing so for 60 iterations, we obtain LMs which have been trained on 30 million
sequences. We compute model estimation error on Dtest at the end of each iteration i. Figure 4 vi-
sualizes underestimation error throughout training for these LMs. We again split test sequences by
their true probability, with darker lines denoting estimation trends for less probable target sequences."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.2911392405063291,"The estimation curves in Figure 4 suggest that while increasing the amount of data in training ini-
tially leads to lower estimation error, this reduction eventually asymptotes. In the insets of Figure 4,
we visualize the relative change in mean estimation between epochs i −1 and i. Relative change in
estimation error eventually ﬂuctuates around 0 (minimal change) for the majority of the distribution.
Comparing architectures, we ﬁnd that the Transformer is signiﬁcantly more efﬁcient at reducing
mean estimation error throughout the distribution."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.29535864978902954,"4.4
WHERE DID THE PROBABILITY MASS GO?"
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.29957805907172996,"In the previous section, we saw that even when increasing the amount of training data, pM consis-
tently underestimates the probabilities of sequences sampled from the tail of pL. At the same time,
we did not ﬁnd that pM overestimated sequences in the head of pL. Under the assumption that pM is
a proper probability distribution,5 that is, P"
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.3037974683544304,"x∈Σ∗pM(x) = 1, these ﬁndings suggest that there ex-
ists sequences in Σ∗whose probability is overestimated by the model. In this section, we investigate
where this probability mass went."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.3080168776371308,"Swap two tokens in x.
Delete a token from x.
Insert a token from Σ
at a position in x.
Substitute a token in x
with a token from Σ."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.31223628691983124,"Table 3: PERTURB(x) randomly ap-
plies one of these perturbations to x."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.31645569620253167,"To do so, we compute model estimation error on perturbed
sequences from pL—sequences in Σ∗which are increasingly
far away from the high-probability zones in pL.
We build
a corpus of perturbed sequences by recursively applying 30
random perturbations to each sequence x ∈Dtest. Formally,
the set of sequences at perturbation step i can be expressed
as: D(i)
perturbed = {PERTURB(x) | x ∈D(i−1)
perturbed} where"
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.3206751054852321,"PERTURB(x) is a function which returns a novel perturbed
version of x, and D(0)
perturbed = Dtest. Sequence perturbation
operations are shown in Table 3. While it is possible that these
operations produce other well-formed strings, we expect this to be a relatively rare outcome. We
score each of these 15M sequences under both the target generative model pL and the LM pM. Note
that we use as LM the GPT2-medium model from the previous section (trained on 30M sequences)."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.32489451476793246,5See Welleck et al. (2020a) for discussion on consistency in the context of neural language model decoding.
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.3291139240506329,Published as a conference paper at ICLR 2022
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.3333333333333333,"Figure 6: Model estimation error on test sequences as a function of target sequence probability for
three different artiﬁcial languages. Each line visualizes estimation error for a GPT2-medium model
trained to model a language with a speciﬁc softmax temperature parameter T."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.33755274261603374,"Figure 5 visualizes GPT2-medium’s mean estimation error for these 15M sequences across two
dimensions. On the x-axis, we plot the target probability of the sequence under pL, and on the y-
axis, the number of perturbations performed on the sequence. For example, the bottom-left corner
(1) of Figure 5 visualizes estimation behaviour for rare sequences sampled directly from pL, whereas
the top-left corner (2) visualizes estimation error sequences which are equally rare, but which have
been perturbed up to 30 times."
ESTIMATION ERROR BY AMOUNT OF TRAINING DATA,0.34177215189873417,"Figure 5 offers a nuanced characterization of underestimation behaviour. The brown area on the
bottom of the ﬁgure re-states the underestimation ﬁndings of the previous section. When increasing
the number of perturbations performed, however, we begin entering into a space of sequences which
are at ﬁrst well-estimated by pM (the white areas) but then are quickly overestimated by pM (the
dark blue areas), conﬁrming that there are indeed sequences in Σ∗which are overestimated by the
language model. Furthermore, these ﬁndings suggest that the tail of rare events deﬁned by the
language model does not match the tail of the artiﬁcial language—the rare events typical in pL are
under-represented in pM in favour of other sequences in Σ∗. See Section A.1.4 for experiments
ﬁnding that random sequences from Σ∗are also overestimated by pM."
MODULATING THE SHAPE OF THE TARGET DISTRIBUTION,0.3459915611814346,"4.5
MODULATING THE SHAPE OF THE TARGET DISTRIBUTION"
MODULATING THE SHAPE OF THE TARGET DISTRIBUTION,0.350210970464135,"Up to this point, the target artiﬁcial language pL was given as the distribution induced by an ancestral
sampling scheme with softmax T = 0.85 from the generative model L. In the previous section, we
saw that pM placed excess probability mass on areas in Σ∗with low-probability under pL. Here we
modulate the shape of the sequence space deﬁned by pL to investigate how estimation error varies
with respect to systematic interventions in the target distribution. To adjust the way that pL allocates
probability mass over Σ∗, we control the entropy of the conditional distributions at each generation
step t by dividing the pre-softmax logits by a temperature value T.6"
MODULATING THE SHAPE OF THE TARGET DISTRIBUTION,0.35443037974683544,"We visualize the effects of T on the shape of the distribution in the left of Figure 6. By increasing
the value of T, we increase the entropy of the distributions over next tokens, which in turn, spreads
probability mass across a larger number of sequences in Σ∗. We deﬁne four artiﬁcial languages
with varying T and train GPT2-medium on an ancestral sample of 1M sequences from each of these
artiﬁcial languages. Figure 6 visualizes model estimation error by true sequence probability for
each model. We ﬁnd that models trained on languages with increased entropy perform compara-
tively better than models trained on low entropy languages. Estimation error for models trained on
languages with greater T is less severe, and this holds throughout nearly all target sequence prob-
abilities. These results indicate that neural LMs provide a more accurate approximation of target
distributions which spread mass more uniformly across Σ∗."
RELATED WORK,0.35864978902953587,"5
RELATED WORK"
RELATED WORK,0.3628691983122363,"This paper contributes to recent work investigating the properties of the distributions deﬁned by
LMs. Prior studies have focused on exploring (Takahashi & Tanaka-Ishii, 2019; 2017) and develop-"
RELATED WORK,0.3670886075949367,"6Formally, for the ith component of the length K pre-softmax logits x, this operation is given as:"
RELATED WORK,0.37130801687763715,"SOFTMAX(x)i =
exp( xi"
RELATED WORK,0.3755274261603376,"T )
PK
j=1 exp (
xj T )"
RELATED WORK,0.379746835443038,Published as a conference paper at ICLR 2022
RELATED WORK,0.38396624472573837,"ing frameworks (Meister & Cotterell, 2021) to better understand whether the large-scale statistical
tendencies of natural language, such as Zipf’s law (Zipf, 1949), are captured by LMs. We take a
more ﬁne-grained approach, proposing a methodology which draws off of instance-level evaluation
schemes (Zhong et al., 2021) and the experimental control afforded by artiﬁcial corpora (White &
Cotterell, 2021; Papadimitriou & Jurafsky, 2020). Indeed, closely related to our work is Kulikov
et al. (2021)’s, in which artiﬁcial corpora produced by generative models were used to explore mode
recovery in neural language modeling. Our analysis exploring the overestimation of ill-formed
sequences extends previous ﬁndings on locally normalized conditional models assigning arbitrary
probability mass to unlikely sequences (Andor et al., 2016; Goyal et al., 2019; Lafferty et al., 2001),
neural LMs assigning high likelihood to sequences with repetitions (Welleck et al., 2020b), the con-
sistency of decoding algorithms (Welleck et al., 2020a), and on machine translation models placing
signiﬁcant probability mass on the empty sequence (Stahlberg & Byrne, 2019)."
RELATED WORK,0.3881856540084388,"We additionally contribute to the body work seeking to characterize and adapt neural model per-
formance on rare or novel examples and classes (Horn & Perona, 2017; Bengio, 2015). In the
context of language modeling, Lent et al. (2021) explored performance on under-resourced lan-
guages, whereas Oren et al. (2019) did so on under-represented domains in training corpora. Mc-
Coy et al. (2021) introduced analyses to assess sequential and syntactic novelty in LMs. Focusing
on the word frequency distribution, Dudy & Bedrick (2020) found that LMs under-perform when
less frequent examples are encountered at test time. In the classiﬁcation setting, various approaches
have been proposed to help alleviate class imbalance in the data distribution, such as data aug-
mentation (Sagawa et al., 2020) or the transfer of knowledge from high-frequency classes to infre-
quent ones (Ouyang et al., 2016; Zhu et al., 2014; Chen et al., 2021). Prior to the current neural
paradigm (Bengio et al., 2003), multiple approaches have been proposed to deal with the heavy-tail,
such as smoothing and back-off approaches in statistical n-grams (Chen & Goodman, 1999) and
two-stage Bayesian approaches (Goldwater et al., 2006)."
CONCLUSION,0.3924050632911392,"6
CONCLUSION"
CONCLUSION,0.39662447257383965,"Emerging as a result of a language user’s ability to produce and comprehend novel expressions, the
heavy-tail of rare events is one of the fundamental features of distributions in natural language. In
this work, we introduce a controlled methodology to evaluate instance-level LM performance on
this set of individually rare but collectively frequent events. We use generative models trained on
natural language corpora to deﬁne a set of artiﬁcial languages for which we can exactly compute the
probability of sequences. Training LSTM and Transformer LMs on sequences sampled from these
artiﬁcial languages, our analysis compares the probability estimates given to sequences by the LMs
to the target probabilities of sequences under the artiﬁcial language."
CONCLUSION,0.4008438818565401,"Our results indicate that neural LMs systematically under-represent sequences in the tail of the target
distribution, even when increasing the amount of the training data. Investigating where this proba-
bility mass went, our perturbation experiments reveal that neural LMs do not tend to overestimate
the head of the distribution, but rather overestimate the probability of sequences outside those typ-
ical in the target distribution. Comparing model performance on target distributions with varying
properties, we ﬁnd that neural LMs tend to provide more accurate approximations of distributions
with greater entropy. Interpreted together, these results indicate that autoregressive neural language
models have a tendency to spread probability mass too uniformly across the space of possible se-
quences."
CONCLUSION,0.4050632911392405,"Finally, we would like to acknowledge that we do not know the degree of structural difference
between our Transformer-generated ground-truth distributions and the distributions of actual natural
languages. It is likely that the distribution deﬁned by our ground truth models is less structured
than the distribution of a natural language. Therefore, it is possible that some systematic difference
between natural language distributions and our ground-truth distributions may affect our results to
a certain degree. That being said, our experiments in Section 4.5 suggest that it may actually be
easier for neural LMs to learn less structured distributions, and we expect the task of recovering
a ground-truth distribution to be made easier when the target distribution and LM are in the same
model class. Nevertheless, future work should seek to conduct similar experiments using ground-
truth distributions with more explicit structure."
CONCLUSION,0.4092827004219409,Published as a conference paper at ICLR 2022
REFERENCES,0.41350210970464135,REFERENCES
REFERENCES,0.4177215189873418,"Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev,
Slav Petrov, and Michael Collins. Globally normalized transition-based neural networks, 2016."
REFERENCES,0.4219409282700422,R. Baayen. Word frequency distributions. 2001.
REFERENCES,0.42616033755274263,"R. Harald Baayen. Productivity in language production. Language and Cognitive Processes, 9(3):
447–469, 1994. doi: 10.1080/01690969408402127. URL https://doi.org/10.1080/
01690969408402127."
REFERENCES,0.43037974683544306,"R Harald Baayen. 41. corpus linguistics in morphology: Morphological productivity. In Corpus
linguistics, pp. 899–919. De Gruyter Mouton, 2009."
REFERENCES,0.4345991561181435,"Samy Bengio. The battle against the long tail. In Talk on Workshop on Big Data and Statistical
Machine Learning, volume 1, 2015."
REFERENCES,0.4388185654008439,"Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. The journal of machine learning research, 3:1137–1155, 2003."
REFERENCES,0.4430379746835443,"Terra Blevins and Luke Zettlemoyer. Moving down the long tail of word sense disambiguation with
gloss-informed biencoders. CoRR, abs/2005.02590, 2020. URL https://arxiv.org/abs/
2005.02590."
REFERENCES,0.4472573839662447,"Howard Chen, Mengzhou Xia, and Danqi Chen. Non-parametric few-shot learning for word sense
disambiguation. arXiv preprint arXiv:2104.12677, 2021."
REFERENCES,0.45147679324894513,"Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13(4):359–394, 1999."
REFERENCES,0.45569620253164556,"N. Chomsky.
The Logical Structure of Linguistic Theory.
Springer US, 1975, 1955.
ISBN
9780306307607. URL https://books.google.ca/books?id=1D66ktXOITAC."
REFERENCES,0.459915611814346,"Paula Czarnowska, Sebastian Ruder, Edouard Grave, Ryan Cotterell, and Ann Copestake. Don’t for-
get the long tail! a comprehensive analysis of morphological generalization in bilingual lexicon
induction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 974–983, Hong Kong, China, November 2019. Association for Computational Lin-
guistics. doi: 10.18653/v1/D19-1090. URL https://aclanthology.org/D19-1090."
REFERENCES,0.4641350210970464,"Daniel D’souza, Zach Nussbaum, Chirag Agarwal, and Sara Hooker. A tale of two long tails. arXiv
preprint arXiv:2107.13098, 2021."
REFERENCES,0.46835443037974683,"Shiran Dudy and Steven Bedrick. Are some words worth more than others?, 2020."
REFERENCES,0.47257383966244726,"Jerry A Fodor. The language of thought, volume 5. Harvard university press, 1975."
REFERENCES,0.4767932489451477,"Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analy-
sis. Cognition, 28(1-2):3–71, 1988."
REFERENCES,0.4810126582278481,"Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus, 2019."
REFERENCES,0.48523206751054854,"Sharon Goldwater,
Mark Johnson,
and Thomas Grifﬁths.
Interpolating between types
and tokens by estimating power-law generators.
In Y. Weiss,
B. Sch¨olkopf,
and
J. Platt (eds.), Advances in Neural Information Processing Systems, volume 18. MIT
Press,
2006.
URL
https://proceedings.neurips.cc/paper/2005/file/
4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
REFERENCES,0.48945147679324896,"Sharon Goldwater, Thomas L. Grifﬁths, and Mark Johnson.
Producing power-law distribu-
tions and damping word frequencies with two-stage language models.
Journal of Machine
Learning Research, 12(68):2335–2382, 2011.
URL http://jmlr.org/papers/v12/
goldwater11a.html."
REFERENCES,0.4936708860759494,Published as a conference paper at ICLR 2022
REFERENCES,0.4978902953586498,"I. J. Good. The population frequencies of species and the estimation of population parameters.
Biometrika, 40(3/4):237–264, 1953.
ISSN 00063444.
URL http://www.jstor.org/
stable/2333344."
REFERENCES,0.5021097046413502,"Kartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick. An empirical investigation of global and
local normalization for recurrent neural sequence models using a continuous relaxation to beam
search, 2019."
REFERENCES,0.5063291139240507,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.510548523206751,"Grant Van Horn and Pietro Perona. The devil is in the tails: Fine-grained classiﬁcation in the wild.
CoRR, abs/1709.01450, 2017. URL http://arxiv.org/abs/1709.01450."
REFERENCES,0.5147679324894515,F. Jelinek. Interpolated estimation of markov source parameters from sparse data. 1980.
REFERENCES,0.5189873417721519,"S. Katz. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401,
1987. doi: 10.1109/TASSP.1987.1165125."
REFERENCES,0.5232067510548524,E. Khmaladze. The statistical analysis of a large number of rare events. 1988.
REFERENCES,0.5274261603375527,"R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In 1995 International
Conference on Acoustics, Speech, and Signal Processing, volume 1, pp. 181–184 vol.1, 1995.
doi: 10.1109/ICASSP.1995.479394."
REFERENCES,0.5316455696202531,"Ilia Kulikov, Sean Welleck, and Kyunghyun Cho. Mode recovery in neural autoregressive sequence
modeling. CoRR, abs/2106.05459, 2021. URL https://arxiv.org/abs/2106.05459."
REFERENCES,0.5358649789029536,"John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira.
Conditional random ﬁelds:
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning, ICML ’01, pp. 282–289, San Francisco, CA,
USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558607781."
REFERENCES,0.540084388185654,"Heather Lent, Emanuele Bugliarello, Miryam de Lhoneux, Chen Qiu, and Anders Søgaard. On
language models for creoles, 2021."
REFERENCES,0.5443037974683544,"R Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much
do language models copy from their training data? evaluating linguistic novelty in text generation
using raven. arXiv preprint arXiv:2111.09509, 2021."
REFERENCES,0.5485232067510548,"Clara Meister and Ryan Cotterell. Language model evaluation beyond perplexity, 2021."
REFERENCES,0.5527426160337553,"G´abor Melis, Tom´aˇs Koˇcisk´y, and Phil Blunsom.
Mogriﬁer lstm.
In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
SJe5P6EYvS."
REFERENCES,0.5569620253164557,"Tom´aˇs Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 conference of the north american chapter of the
association for computational linguistics: Human language technologies, pp. 746–751, 2013."
REFERENCES,0.5611814345991561,"Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap: Good online learn-
ers are good ofﬂine generalizers. CoRR, abs/2010.08127, 2020. URL https://arxiv.org/
abs/2010.08127."
REFERENCES,0.5654008438818565,"Yonatan Oren, Shiori Sagawa, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Process-
ing (EMNLP-IJCNLP), pp. 4227–4237, Hong Kong, China, November 2019. Association for
Computational Linguistics. doi: 10.18653/v1/D19-1432. URL https://aclanthology.
org/D19-1432."
REFERENCES,0.569620253164557,"Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang Yang.
Factors in ﬁnetuning deep
model for object detection with long-tail distribution. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016."
REFERENCES,0.5738396624472574,Published as a conference paper at ICLR 2022
REFERENCES,0.5780590717299579,"Isabel Papadimitriou and Dan Jurafsky. Pretraining on non-linguistic structure as a tool for analyzing
learning bias in language models. CoRR, abs/2004.14601, 2020. URL https://arxiv.org/
abs/2004.14601."
REFERENCES,0.5822784810126582,"Alec Radford, Jeff Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models
are unsupervised multitask learners. 2019."
REFERENCES,0.5864978902953587,"Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang.
An investigation of why
overparameterization exacerbates spurious correlations, 2020."
REFERENCES,0.5907172995780591,"Felix Stahlberg and Bill Byrne. On NMT search errors and model errors: Cat got your tongue?
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pp. 3356–3362, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1331. URL https://aclanthology.org/D19-1331."
REFERENCES,0.5949367088607594,"Shuntaro Takahashi and Kumiko Tanaka-Ishii. Do neural nets learn statistical laws behind natural
language? PloS one, 12(12):e0189326, 2017."
REFERENCES,0.5991561181434599,"Shuntaro Takahashi and Kumiko Tanaka-Ishii. Evaluating Computational Language Models with
Scaling Properties of Natural Language. Computational Linguistics, 45(3):481–513, 09 2019.
ISSN 0891-2017. doi: 10.1162/coli a 00355. URL https://doi.org/10.1162/coli_
a_00355."
REFERENCES,0.6033755274261603,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.6075949367088608,"Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency
of a recurrent language model with respect to incomplete decoding, 2020a."
REFERENCES,0.6118143459915611,"Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.
Neural text generation with unlikelihood training. In International Conference on Learning Rep-
resentations, 2020b. URL https://openreview.net/forum?id=SJeYe0NtvH."
REFERENCES,0.6160337552742616,"Jennifer C. White and Ryan Cotterell. Examining the inductive bias of neural language models with
artiﬁcial languages. CoRR, abs/2106.01044, 2021. URL https://arxiv.org/abs/2106.
01044."
REFERENCES,0.620253164556962,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Associ-
ation for Computational Linguistics.
doi: 10.18653/v1/2020.emnlp-demos.6.
URL https:
//aclanthology.org/2020.emnlp-demos.6."
REFERENCES,0.6244725738396625,"Frank Wood, Jan Gasthaus, C´edric Archambeau, Lancelot James, and Yee Whye Teh. The sequence
memoizer. Communications of the ACM, 54(2):91–98, 2011."
REFERENCES,0.6286919831223629,"Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob Steinhardt. Are larger pretrained language
models uniformly better? comparing performance at the instance level, 2021."
REFERENCES,0.6329113924050633,"Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan. Capturing long-tail distributions of object
subcategories. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2014."
REFERENCES,0.6371308016877637,George Kingsley Zipf. Human behavior and the principle of least effort. 1949.
REFERENCES,0.6413502109704642,Published as a conference paper at ICLR 2022
REFERENCES,0.6455696202531646,"A
APPENDIX"
REFERENCES,0.6497890295358649,"A.1
ADDITIONAL EXPERIMENTS"
REFERENCES,0.6540084388185654,"A.1.1
PRE-TRAINED GROUND-TRUTH MODEL"
REFERENCES,0.6582278481012658,"In our previous experiments, our ground-truth model was given as a Transformer language model
trainined on 1.5M sequences from the OpenWebText corpus. Here we explore underestimation
behaviour when the ground-truth distribution is given by a pretrained GPT2-medium model ﬁne-
tuned on 1.5M sequences from the OpenWebText corpus. We sample from pL with softmax T =
1.00."
REFERENCES,0.6624472573839663,"Analogously to the experiment in Section 4.1, we train a randomly-initialized GPT2-medium model
on 1M sequences sampled from the ﬁne-tuned model. In the center of Figure 7, we visualize mean
model estimation error for 50, 000 test sequences as a function of true sequence probability. Sim-
ilarly to our experiments in Section 4.4, we ﬁnd that the LM underestimates the probability of the
majority of sequences, and does so more severely for less probable sequences. Note that this LM
obtains a test perplexity of 67.97."
REFERENCES,0.6666666666666666,"Figure 7: Left: Joint histogram of sequence probability estimates for test sequences. Center: Mean
model estimation error by true sequence probability for test sequences. Right: Mean model estima-
tion error by true sequence probability for sequences randomly sampled from Σ∗."
REFERENCES,0.6708860759493671,"Finally, to ensure that our ﬁndings regarding the overestimation of ill-formed sequences hold, we
compute model estimation error on random sequences sampled from Σ∗(see A.1.4 for details on
how these sequences are constructed). Figure 7 center visualizes mean model estimation error as a
function of target sequence probability. We ﬁnd that pM overestimates the majority of ill-formed
sequences, indicating that these ﬁndings hold when the ground-truth distribution is deﬁned using a
pretrained model."
REFERENCES,0.6751054852320675,"A.1.2
UNTEMPERED (T = 1.00) GROUND-TRUTH DISTRIBUTION"
REFERENCES,0.679324894514768,"In Sections 4.1 to 4.4, we conducted all experiments on an artiﬁcial language deﬁned by ancestral
sampling scheme with T = 0.85. In Section 4.5, we saw that the underestimation ﬁndings held
regardless of T. To provide further evidence that these results hold for other values of T, we conduct
similar experiments as in Section 4.3 with an artiﬁcial language pL deﬁned by an ancestral sampling
scheme with T = 1.00. Speciﬁcally, we train GPT2-medium and an GPT2-large on a total of
30M sequences sampled from pL, and we compute model estimation error on a set of withheld test
sequences at each training iteration."
REFERENCES,0.6835443037974683,"A.1.3
PRE-TRAINED MODEL ESTIMATION ERROR"
REFERENCES,0.6877637130801688,"In Section 4.3, we explored how estimation error varies as a function of the amount of training data,
ﬁnding that while increased data weakens estimation error, the underestimation behaviour persists.
As an alternative way to explore how underestimation varies with increasing data, we ﬁne-tune Hug-
gingface’s pre-trained GPT2-small, -medium and -large models on the set of 1M sequences used in
Section 4.1. Computing estimation error on a set of unseen test sequences, we ﬁnd, analogously to
our experiments on models trained from scratch, that pre-trained models underestimate the proba-
bility of the majority of sequences sampled from the target distribution, and do so more severely for
rarer sequences (we visualize this in Figure 9). Furthermore, in Figure 10, we increase the amount"
REFERENCES,0.6919831223628692,Published as a conference paper at ICLR 2022
REFERENCES,0.6962025316455697,"Figure 8: GPT2-medium and GPT2-large trained on 30M sequences sampled from pL with T =
1.00. Main plots: Mean estimation error on test sequences as a function of the number of sequences
seen in training. Inset plots: Relative change in mean estimation error of test sequences as a function
of the number of sequences seen in training. In both cases, each line denotes estimation behaviour
for a 50th of the test sequences; darker lines represent less probable sequences."
REFERENCES,0.70042194092827,"of ﬁne-tuning data substantially, and we plot test sequence estimation behaviour at the end of each
epoch for a pre-trained GPT2-medium model. Once again, increased training data lessens but does
eliminate the underestimation behaviour."
REFERENCES,0.7046413502109705,"Figure 9: Test sequence probability estimates given by pretrained neural LMs ﬁne-tuned on 1M
sequences sampled from pL. Three left-most ﬁgures: The joint histograms of sequence probability
estimates. The dotted line denotes the cases in which the model’s estimates perfectly align with the
target probability; shading to the right of this line denotes underestimation. Right-most ﬁgure: Mean
sequence estimation error by target sequence probability."
REFERENCES,0.7088607594936709,"Figure 10: Pre-trained GPT2-medium ﬁne-tuned on 30M sequences sampled from pL. Main plots:
Mean estimation error on test sequences as a function of the number of sequences seen in ﬁne-
tuning. Inset plots: Relative change in mean estimation error of test sequences as a function of the
number of sequences seen in ﬁne-tuning. In both cases, each line denotes estimation behaviour for
a 50th of the test sequences; darker lines represent less probable sequences."
REFERENCES,0.7130801687763713,"A.1.4
ALTERNATIVE PERTURBATIONS"
REFERENCES,0.7172995780590717,"In Section 4.4, we study where the language model’s underestimated probability mass went by
computing model estimation error on perturbed sequences. We obtain a set of perturbed sequences
by (i) sampling a sequence from pL and then (ii) recursively perturbing this sequence according to
the perturbations provided in Table 3."
REFERENCES,0.7215189873417721,Published as a conference paper at ICLR 2022
REFERENCES,0.7257383966244726,"This method provides us with sequences which are increasingly far away from high-probability
zones under pL. However, it does so with initial sequences sampled directly from pL, and as a
result, produces strings which are edit-adjacent to the high-probability zones (under pL) in Σ∗."
REFERENCES,0.729957805907173,"Figure 11: Mean model estimation error by
true sequence probability for sequences ran-
domly sampled from Σ∗."
REFERENCES,0.7341772151898734,"To ensure that our results hold in other low-
probability subspaces, we conduct an analogous ex-
periment on sequences randomly sampled from Σ∗.
We sample a sequence from Σ∗by ﬁrst sampling a
sequence length l from a Poisson distribution with
λ = 10. Given this length, we sample l tokens from
Σ∗and concatenate all tokens to form a sequence.
We then score the sequence under both pL and pM,
and compute model estimation error. We use as ar-
tiﬁcial language pL GPT2-medium with T = 0.85
and we use as language model pM a GPT2-medium
model trained on 30M sequences ancestrally sam-
pled from pL."
REFERENCES,0.7383966244725738,"Figure 11 visualizes mean estimation error as a
function of the ground-truth probability of the se-
quence. Similarly to all other perturbation exper-
iments, we do indeed ﬁnd that pM overestimates
these sequences, regardless of their true sequence probability."
REFERENCES,0.7426160337552743,"A.1.5
ESTIMATION ERROR BY SEQUENCE LENGTH"
REFERENCES,0.7468354430379747,"Autoregressive neural language models decompose the joint distribution p(x) over sequences into
a series of conditional distributions p(xi | x<i). Generating a sequence of length n, then, involves
estimating n conditional distributions. Since the probability of a sequence is inversely correlated
with its length, our ﬁndings that estimation error is worse for rarer sentences may be explained by
compounding errors."
REFERENCES,0.7510548523206751,"Figure 12: Expected and observed estimation error by sentence length for GPT2-small and GPT2-
medium."
REFERENCES,0.7552742616033755,"To test this claim, we ask whether the observed estimation error is worse than would be expected
if it was due to error compounding. Speciﬁcally, in Figure 12, we plot the expected model esti-
mation (black) and the mean observed error (blue) by sequence length. Expected estimation error
for sequence length n is computed by multiplying the average token level estimation error by the
sequence length, i.e., n¯ϵ, where"
REFERENCES,0.759493670886076,"¯ϵ =
1
|D| X x∈D 1
|x| |x|
X"
REFERENCES,0.7637130801687764,"i=1
log pM(xi | x<i) −log pL(xi | x<i)"
REFERENCES,0.7679324894514767,"Observed estimation error is computed as the mean estimation error for test sentences of length
n. Note that the shaded areas around this curve denote the 95% bootstrapped conﬁdence intervals
for this mean. As shown in Figure 12, observed estimation error for both GPT2-small and GPT2-
medium is more severe than expected estimation error as we increase sentence length. This suggests
that estimation error for longer (and typically rarer) sequences is not solely due to error compound-
ing."
REFERENCES,0.7721518987341772,Published as a conference paper at ICLR 2022
REFERENCES,0.7763713080168776,"A.2
EMPIRICALLY MEASURING THE LNRE ZONE"
REFERENCES,0.7805907172995781,"In section 2, we formally deﬁned the LNRE zone as the range of values of N for which there is
non-zero probability of sampling a novel event at the N + 1th draw. Here we introduce a frequentist
estimator for this probability. This in turn allows us to empirically verify if a sample exists in the
LNRE zone."
REFERENCES,0.7848101265822784,"A.2.1
ESTIMATING THE POTENTIAL PRODUCTIVITY"
REFERENCES,0.7890295358649789,"Suppose we have a set of N events D = {x1, . . . , xN} drawn from some generative process φ.
Given D, we aim to obtain an empirical estimate for the potential productivity ˆ
PN: the amount of
probability allocated to unseen events as a function of N. We can do so using the Good-Turing
estimate for the probability of an event given its frequency (Good, 1953)."
REFERENCES,0.7932489451476793,"Speciﬁcally, let f(x, D) be a function which returns the frequency of the event x in D. Let Nm
denote the number of types (unique events) in D for which f(x, D) = m. Good-Turing says that
for large N, the probability of the event x given that it has occurred with frequency m in our sample
D of size N is equal to
ˆP(x | f(x, D) = m) = (m + 1)"
REFERENCES,0.7974683544303798,"N
Nm+1"
REFERENCES,0.8016877637130801,"Nm
(3)"
REFERENCES,0.8059071729957806,"To obtain an estimate for PN, we set m = 0:"
REFERENCES,0.810126582278481,"ˆ
PN = N0 ˆP(x | f(x, D) = 0) = N1 N
(4)"
REFERENCES,0.8143459915611815,"which states that the total amount of probability mass allocated to unseen events is equal to the
proportion of events which occurred only once (hapax legomena) in D. This quantity is known as
the potential productivity of a linguistic process (Baayen, 2009; 2001; 1994)."
REFERENCES,0.8185654008438819,"A.2.2
THE LNRE ZONE IN OPENWEBTEXT"
REFERENCES,0.8227848101265823,"We apply this method to a subset of OpenWebText, a popular language modeling corpus. In Figure
13, we plot the the empirical estimate of PN as function of the sample size N for n-grams sampled
from a subset of OpenWebText. Particularly for n-grams with n ≥3, we ﬁnd that there is signiﬁcant
probability of sampling a previously unseen event, even for N > 10, 000, 000."
REFERENCES,0.8270042194092827,"Figure 13: The probability of sampling a novel item (potential productivity P) as a function of
sample size N. Many distributions in natural language are characterized by a potentially unbounded
amount of novelty."
REFERENCES,0.8312236286919831,"A.3
MODEL PERPLEXITY VALUES"
REFERENCES,0.8354430379746836,"In this section we report relevant perplexity values for all models used. For each model, we report
mean perplexity across sentences drawn from (i) the validation set generated by the artiﬁcial lan-
guage they attempt to model and (ii) the OpenWebText corpus. While we include the perplexity
values for our models on sentences of English, this is not to claim that our ground-truth models are
meant to deﬁne a distribution which closely resembles the distribution of English."
REFERENCES,0.8396624472573839,Published as a conference paper at ICLR 2022
REFERENCES,0.8438818565400844,"Model
mean PP (val)
mean PP (eng)"
REFERENCES,0.8481012658227848,"LSTM
36.53
158.31
GPT2-small
26.66
129.84
GPT2-medium
25.42
132.56
GPT2-small (pretrained)
21.02
53.79
GPT2-medium (pretrained)
20.87
48.42
GPT2-large (pretrained)
20.90
47.30
LSTM (increased data)
35.01
149.02
GPT2-medium (increased data)
21.60
94.03
GPT2-large (increased data)
21.32
91.28"
REFERENCES,0.8523206751054853,"GPT2-medium (ground-truth model)
-
73.79"
REFERENCES,0.8565400843881856,"Table 4: PP on validation set generated by the artiﬁcial language the LM attempts to model (val),
and on real sentences sampled from OpenWebText (eng)."
REFERENCES,0.8607594936708861,"Softmax T
mean PP (val)
mean PP (eng)"
REFERENCES,0.8649789029535865,"0.70
11.61
223.60
0.85
25.42
132.56
1.00
75.30
106.21
1.15
290.69
106.29"
REFERENCES,0.869198312236287,"Table 5: GPT2-medium PP on validation set generated by the artiﬁcial language the LM attempts
to model (val), and on real sentences sampled from OpenWebText (eng) for models used in Section
4.5"
REFERENCES,0.8734177215189873,"A.4
LANGUAGE SAMPLES"
REFERENCES,0.8776371308016878,"x
log pL(x)"
REFERENCES,0.8818565400843882,"he was a complete east end player .
−26.399"
REFERENCES,0.8860759493670886,"a former harvard university graduate
told cnn that in recent weeks , u.s. intelligence ofﬁcials have
begun to gather evidence that trump ’s campaign
colluded with russia to inﬂuence the election .
−61.846"
REFERENCES,0.890295358649789,"in the current study , we examined whether
participants in the study performed more or less
“ active ” in weight loss -lrb- p = 0.05 -rrb- .
−51.6566"
REFERENCES,0.8945147679324894,"you , the one that is the republican candidate ,
who is taking over the senate and government
as a democrat and who is a bipartisan democrat ,
and you ’ve got to be able to get that done .
−92.703"
REFERENCES,0.8987341772151899,"since the 1970s , the city has been in the midst of a landmark urban pride .
−44.9523"
REFERENCES,0.9029535864978903,"Table 6: Sequences ancestrally sampled from the artiﬁcial language generated by GPT2-medium
with softmax T = 0.70."
REFERENCES,0.9071729957805907,Published as a conference paper at ICLR 2022
REFERENCES,0.9113924050632911,"x
log pL(x)"
REFERENCES,0.9156118143459916,"” i have no doubt that ’s a big part of any kind of campaign machine , ” he told al jazeera .
−54.5951"
REFERENCES,0.919831223628692,"and it ’s not a very good idea to work with claims in comics and ﬁlm novels .
−59.7626"
REFERENCES,0.9240506329113924,"according to the new york times , susan macmahon , 54 ,
and her husband , elizabeth bailey , were in the vehicle with their son ,
aged between 12 and 19 .
−86.5217"
REFERENCES,0.9282700421940928,"anticipating experience
−21.7095"
REFERENCES,0.9324894514767933,"this is the reason i ’d like to take advantage of these ideas
in an attempt to transform my life .
−53.1399"
REFERENCES,0.9367088607594937,"Table 7: Sequences ancestrally sampled from the artiﬁcial language generated by GPT2-medium
with softmax T = 0.85."
REFERENCES,0.9409282700421941,"x
log pL(x)"
REFERENCES,0.9451476793248945,"beautiful bacon cookies
−19.8038"
REFERENCES,0.9493670886075949,"” this accident , despite most of its life ,
is dependent on one of the most precious bodies on this planet ,
which is comparable to that on mostrughenai mountains .
−142.4362"
REFERENCES,0.9535864978902954,"it ’s not fair to say that we ’re not in a position to permit same -
sex marriages , and all the same issues that are presented on regular basis . ”
−88.9437"
REFERENCES,0.9578059071729957,"from video -lrb- without editing -rrb- you can see original images produced at
the ofﬁcial website , competitors , and event prices .
−100.3791"
REFERENCES,0.9620253164556962,"consumer electronics have become a perfect ﬁt for bm ’s ultra - low - cost turn .
−73.0217"
REFERENCES,0.9662447257383966,"Table 8: Sequences ancestrally sampled from the artiﬁcial language generated by GPT2-medium
with softmax T = 1.00."
REFERENCES,0.9704641350210971,"x
log pL(x)"
REFERENCES,0.9746835443037974,"zy brace gym ﬁngerprint – bom jerozo dell ’
−94.0307"
REFERENCES,0.9789029535864979,"shortly thereafter , will be a buoyant as the dynamo bust series starts one .
−90.5994"
REFERENCES,0.9831223628691983,"disjitor doom efeco became a gothicrevolution manager ofby
city library for marvel studios .
−143.6075"
REFERENCES,0.9873417721518988,"earlier this week chicago ’s writer andrew o’t text ’
american catholics to interested readers the holiday kingsburyobee -lrb- expiration -rrb-
at red for now atmotionweism race and bachelor whitney university ,
register as 1852 jim banner of airbus beer
and of course smiled at us in formula 1 where he tells friends by name , ” ﬁnnish catholics :
the encryptings on robot - type mothers .
−493.9313"
REFERENCES,0.9915611814345991,"middleware in germany
−27.3411"
REFERENCES,0.9957805907172996,"Table 9: Sequences ancestrally sampled from the artiﬁcial language generated by GPT2-medium
with softmax T = 1.15."
