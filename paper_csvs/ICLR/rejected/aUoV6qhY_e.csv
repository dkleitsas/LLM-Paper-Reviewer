Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006211180124223602,"Transformers have greatly advanced the state-of-the-art in Natural Language Pro-
cessing (NLP) in recent years, but are especially demanding in terms of their com-
putation and storage requirements. Transformers are ﬁrst pre-trained on a large
dataset, and subsequently ﬁne-tuned for different downstream tasks. We observe
that this design process leads to models that are not only over-parameterized for
downstream tasks, but also contain elements that adversely impact accuracy of the
downstream tasks. We propose a Specialization framework to create optimized
transformer models for a given downstream task. Our framework systematically
uses accuracy-driven pruning, i.e., it identiﬁes and prunes parts of the ﬁne-tuned
Transformer that hinder performance on the downstream task. We also replace
the dense soft-attention in selected layers with sparse hard-attention to help the
model focus on the relevant parts of the input. In effect, our framework leads to
models that are not only faster and smaller, but also more accurate. The large
number of parameters contained in Transformers presents a challenge in the form
of a large pruning design space. Further, the traditional iterative prune-retrain ap-
proach is not applicable to Transformers, since the ﬁne-tuning data is often very
small and re-training quickly leads to overﬁtting. To address these challenges,
we propose a hierarchical, re-training-free pruning method with model- and task-
speciﬁc heuristics. Our experiments on GLUE and SQUAD show that Specialized
models are consistently more accurate (by up to 4.5%), while also being up to
2.5× faster and up to 3.2× smaller than the conventional ﬁne-tuned models. In
addition, we demonstrate that Specialization can be combined with previous ef-
forts such as distillation or quantization to achieve further beneﬁts. For example,
Specialized Q8BERT and DistilBERT models exceed the performance of BERT-
Base, while being up to 3.7× faster and up to 12.1× smaller."
INTRODUCTION,0.012422360248447204,"1
INTRODUCTION"
INTRODUCTION,0.018633540372670808,"Transformers models such as BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and GPT-
3 (Brown et al., 2020) have revolutionized the ﬁeld of Natural Language Processing (NLP), greatly
advancing the state-of-the-art in many NLP tasks. Models that achieve good performance on these
tasks are of high practical signiﬁcance, ﬁnding their place in commercial applications such as social
media monitoring (sentiment analysis), AI chat assistants (question answering), automated summa-
rization tools (analyzing sentence similarity), etc. Therefore, there is a strong interest in creating
more accurate and efﬁcient Transformer models for these tasks."
INTRODUCTION,0.024844720496894408,"Transformers are ﬁrst pre-trained on very large datasets, and subsequently ﬁne-tuned for different
downstream tasks. However, the current method of pre-training and ﬁne-tuning Transformers has
two major drawbacks. First, ﬁne-tuning the pre-trained Transformers leads to models that are highly
over-parameterized models for the downstream tasks, especially since many of these tasks have very
limited training data. This could lead to unstable models (Dodge et al., 2020) with sub-optimal
generalization ability (Michel et al., 2019). Second, these large ﬁne-tuned models present high
computation and storage requirements for inference. This problem is further exacerbated by the
trend towards larger and more accurate models over time. For instance, increasing the number
of parameters from 1.5B to 175B enabled a reduction in perplexity for Language Modelling (on the
Penn Treebank dataset) from 35.8 in GPT-2 (2019) to 20.5 in GPT-3 (2020). In this work, we address
both these challenges, taking advantage of the over-parameterized nature of pre-trained models to"
INTRODUCTION,0.031055900621118012,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.037267080745341616,"create individually Specialized models for the different downstream tasks that are smaller, faster and
more accurate than the conventional ﬁne-tuned models."
INTRODUCTION,0.043478260869565216,"Prior research efforts have explored approximation techniques, such as quantization, pruning and
distillation, for improving the inference efﬁciency of various classes of neural networks. However,
these techniques invariably involve a trade-off between accuracy and efﬁciency. In addition, the vast
majority of these techniques require re-training or additional ﬁne-tuning. This becomes especially
problematic for Transformers, since these large models require signiﬁcant time and energy to train
and ﬁne-tune. Fine-tuning is usually performed on a very small data set, and as a result, conventional
iterative prune-and-retrain methods quickly end up overﬁtting when applied to Transformer models.
In contrast, our Specialization framework utilizes the unique characteristics of Transformer training
and deployment to enable substantial gain in both accuracy as well as efﬁciency. Our framework
also does not require any additional re-training or ﬁne-tuning, and can be applied in a plug-and-play
manner to any Transformer model that is ﬁne-tuned for any downstream task."
INTRODUCTION,0.049689440993788817,"During pre-training, Transformers capture rich linguistic knowledge and gain a deep understanding
of the structure of the target language. Fine-tuning reﬁnes this knowledge for a speciﬁc downstream
task by training a task-speciﬁc ﬁnal layer. We observe that, due to the nature of their design process,
Transformer models are not only over-parameterized but also contain parts that are, in fact, harmful
to performance of downstream tasks. In order to exploit this observation in a principled manner,
we introduce a framework to identify and prune the harmful elements of a Transformer (parameters,
grouped at different levels of granularity i.e., self-attention blocks, feed-forward neural network
blocks, attention heads and neurons), with the goal of maximizing accuracy on the downstream task.
In contrast with prior pruning methods that prune elements with little-or-no impact on the network
output, the proposed method prunes elements that have a considerable impact on the output, leading
to the highest positive impact on accuracy."
INTRODUCTION,0.055900621118012424,"In order to reduce the large pruning space, we analyze the different elements of the ﬁne-tuned Trans-
former in a hierarchical manner, starting with entire self-attention or feed-forward neural network
blocks, followed by attention heads, and neurons, and prune the harmful elements. The core of the
Transformer is self-attention, where each token in the input builds its representation based on the
extent of attention it places on all the other tokens. However, we observe that in some cases, restrict-
ing the attention span of each token to only focus on the relevant tokens (in certain layers) leads
to better information ﬂow inside the model. Hence, our framework is also equipped with the abil-
ity to identify the appropriate layers and replace the ”soft” self-attention with ”hard” self-attention
in these layers. We also introduce Transformer-speciﬁc heuristics to minimize the run-time of our
framework, thereby enabling it to scale to large Transformer models."
INTRODUCTION,0.062111801242236024,We summarize our main contributions as follows:
INTRODUCTION,0.06832298136645963,"• We introduce a Specialization framework that optimizes Transformer models for speciﬁc
downstream tasks through the use of accuracy-driven pruning and selective hard attention.
• We incorporate multiple heuristics in the framework, such as hierarchical processing,
model-driven insights, and run-time based ordering of elements, in order to minimize the
overheads.
• We propose a signiﬁcance analysis technique to identify the importance of each element of
the ﬁne-tuned Transformer for a given downstream task. We use this technique to prune
elements that are harmful to performance on the downstream task.
• We propose the selective replacement of the ”soft” self-attention with hard attention in the
appropriate layers, helping the model focus only on the relevant parts on the input to build
better representations.
• Across a suite of different Transformer networks, we demonstrate that Specialized models
are consistently more accurate and stable, while also being signiﬁcantly faster and smaller
than their conventional ﬁne-tuned non-Specialized counterparts."
RELATED WORK,0.07453416149068323,"2
RELATED WORK"
RELATED WORK,0.08074534161490683,"Task-agnostic optimizations. Given the effectiveness and popularity of Transformer models, sev-
eral techniques have been proposed to overcome their computational and memory challenges, and"
RELATED WORK,0.08695652173913043,Under review as a conference paper at ICLR 2022
RELATED WORK,0.09316770186335403,"to accelerate inference using these models. A vast majority of these works introduce task-agnostic
optimizations, using popular approximation techniques such as knowledge distillation (Sanh et al.,
2019; Sun et al., 2020; Wang et al., 2020), early exit/ depth modulation (Elbayad et al., 2020; Xin
et al., 2020; Zhou et al., 2020), quantization (Zafrir et al., 2019), attention head pruning (Zhang et al.,
2020) and parameter sharing (Lan et al., 2020). In addition, Fan et al. (2020) randomly drop layers
during pre-training, thereby enabling their dropping during inference; Khetan & Karnin (2020) learn
the optimal sizes of the BERT elements during pre-training, and Wu et al. (2020) use Long-Short
Range Attention to speed up the self-attention operation. Using DistilBERT and Q8BERT as ex-
amples, we demonstrate that our techniques are complementary to these works, and can be applied
while ﬁne-tuning for a speciﬁc task to create accurate ultra-efﬁcient models for speciﬁc downstream
tasks."
RELATED WORK,0.09937888198757763,"Task-speciﬁc optimizations. Hou et al. (2020), Shen et al. (2020), Jiao et al. (2019), Wang et al.
(2020), Lagunas et al. (2021) and Sajjad et al. (2020) introduce task-speciﬁc optimizations, but the
gain in efﬁciency comes at the cost of degradation in accuracy on the downstream task. In contrast,
our framework improves both accuracy as well as efﬁciency, and hence appeals to a wider range
of users. We also demonstrate that task-speciﬁc optimizations are more effective when applied to
Specialized models compared to conventional ﬁne-tuned models."
RELATED WORK,0.10559006211180125,"Pruning techniques. Finally, Structured Pruning has been applied to various classes of neural
networks (Anwar et al., 2017; Molchanov et al., 2017), and greedy pruning strategies have also been
explored to identify weights and parameters that the output is least sensitive to (Zhuang et al., 2018;
Ye et al., 2020). In contrast, our method is designed to identify and prune parameters that have most
detrimental effect on the output."
METHODOLOGY TO SPECIALIZE TRANSFORMERS,0.11180124223602485,"3
METHODOLOGY TO SPECIALIZE TRANSFORMERS"
METHODOLOGY TO SPECIALIZE TRANSFORMERS,0.11801242236024845,"We propose a framework for producing Specialized Transformer models that are optimized for a
speciﬁc downstream task, illustrated in Algorithm 1. Our framework performs two main optimiza-
tions: (1) It identiﬁes and prunes elements that hinder performance on the downstream task at hand.
(2) It selectively replaces soft self-attention with hard self-attention to help the model focus only on
the relevant parts of the input."
ACCURACY-DRIVEN PRUNING,0.12422360248447205,"3.1
ACCURACY-DRIVEN PRUNING"
ACCURACY-DRIVEN PRUNING,0.13043478260869565,"The problem of identifying an optimal set of elements to prune is challenging, and this is especially
true for Transformers. In order to optimize a given model, we would ideally want to characterize
the signiﬁcance of each and every parameter in the model, rank them in order of importance, and
ﬁnally prune only the least signiﬁcant parameters. However, Transformers have billions of param-
eters, making this process computationally infeasible. In addition, previously proposed techniques
that can efﬁciently estimate the importance of each parameter, such as using Taylor expansion, are
not applicable. This is because the {approximate, ﬁne-tune, approximate} cycle does not work for
Transformers during ﬁne-tuning, since they very quickly overﬁt the limited training data for the
downstream tasks (usually within 5 epochs). We address both these issues through the use of a hier-
archical greedy algorithm that does not require any additional training or ﬁne-tuning. To determine
the signiﬁcance of each Transformer element, we ﬁrst ﬁne-tune the original Transformer model for
the given downstream task to obtain the baseline loss. Then, for the element under consideration in
each iteration of the framework, we compute the loss of the current Transformer model with the el-
ement removed. We prune the element under consideration if the validation loss when it is removed
is less than the minimum loss seen thus far during the optimization process, since the goal is to ﬁnd
a model with minimum loss. Also, in order to prevent overﬁtting to the validation set, we introduce
a generalization constraint in addition to the aforementioned loss condition. This constraint ensures
that an element is pruned only if it decreases (or retains) the loss of at least a certain number (N)
of samples in the validation set over the current best solution (computed by num samples helped
function in Alg. 1), where N is the number of elements in the validation dataset whose loss is less
than the average loss of the misclassiﬁed samples (we consider samples whose loss is greater than
the average loss of the misclassiﬁed samples to be outliers). Therefore, elements are pruned only if
a vast majority of the samples in the validation set beneﬁt from their removal, resulting in improved
generalization performance. If the loss with the element removed is greater than the minimum loss"
ACCURACY-DRIVEN PRUNING,0.13664596273291926,Under review as a conference paper at ICLR 2022
ACCURACY-DRIVEN PRUNING,0.14285714285714285,"Algorithm 1: Transformer Specialization
Input : Fine-tuned (for the given downstream task) Transformer T, Validation set D
Output: Specialized Transformer for the given downstream task T
Function analyze element(element E):"
ACCURACY-DRIVEN PRUNING,0.14906832298136646,"Tpruned = T −E
New Loss = Evaluate(Tpruned, D)
if New Loss < Min Loss and num samples helped > N then"
ACCURACY-DRIVEN PRUNING,0.15527950310559005,"Min Loss = New Loss
T = Tpruned"
ACCURACY-DRIVEN PRUNING,0.16149068322981366,"Function num samples helped: computes the number of samples in the validation set that
beneﬁt from the pruning of an element; must be > N for an element to be pruned
Baseline Loss = Evaluate (T,D)
Min Loss = Baseline Loss
Q = Order elements for inspection(T, D)
for each layer L in T do"
ACCURACY-DRIVEN PRUNING,0.16770186335403728,"Replace soft self-attention in L with hard self-attention
New Loss = Evaluate(T, D)
if New Loss < Min Loss and num samples helped > N then"
ACCURACY-DRIVEN PRUNING,0.17391304347826086,"Min Loss = New Loss
else"
ACCURACY-DRIVEN PRUNING,0.18012422360248448,Restore soft self-attention in L
ACCURACY-DRIVEN PRUNING,0.18633540372670807,while Q is not empty do
ACCURACY-DRIVEN PRUNING,0.19254658385093168,"TrialElement = Q.pop()
analyze element(TrialElement)
if TrialElement has not been pruned from T and New Loss < Baseline Loss then"
ACCURACY-DRIVEN PRUNING,0.19875776397515527,if TrialElement is an attention block then
ACCURACY-DRIVEN PRUNING,0.20496894409937888,for each attention head h in TrialElement do
ACCURACY-DRIVEN PRUNING,0.2111801242236025,analyze element(h)
ACCURACY-DRIVEN PRUNING,0.21739130434782608,else if TrialElement is a feed-forward block then
ACCURACY-DRIVEN PRUNING,0.2236024844720497,for each neuron w in TrialElement do
ACCURACY-DRIVEN PRUNING,0.22981366459627328,analyze element(w)
ACCURACY-DRIVEN PRUNING,0.2360248447204969,return T
ACCURACY-DRIVEN PRUNING,0.2422360248447205,"seen so far but less than the baseline loss, we inspect the element at a ﬁner granularity, and prune
only parts of the element that hinder performance (rather than pruning the entire element)."
ACCURACY-DRIVEN PRUNING,0.2484472049689441,"Hierarchical processing of elements. It is computationally prohibitive to analyze every single
parameter in large Transformers using the method described in Alg 1. Since the framework iterates
through the entries of the queue sequentially, its efﬁcacy is dependent on both the total number of
elements under consideration, and the time required to analyze each element. We take advantage
of the inherently hierarchical structure of Transformers and consider the elements in a hierarchical
manner, ordered by increasing granularity. Speciﬁcally, we analyze entire feed-forward and self-
attention blocks ﬁrst, and inspect them at ﬁner granularity (attention heads and neurons) only when
required. Through this ordering, we are able to quickly eliminate large numbers of parameters
from further consideration. In addition, due to the over-parameterized nature of Transformers, it is
likely that time-consuming blocks are pruned from the Transformer earlier in the process, thereby
speeding up future iterations of the framework. For example, eliminating a single feed-forward
block in the BERT-Base model removes 5.6% of all parameters under consideration, and speeds up
future iterations by 1.15×. To further reduce the number of elements under consideration, we also
dynamically remove elements if they are encompassed by a high-importance block. For example,
if a given self-attention block is determined to be of high importance (the validation loss with the
block removed is greater than the baseline loss), we remove all heads within that block from further
consideration."
ACCURACY-DRIVEN PRUNING,0.2546583850931677,"Creating an ordered queue of elements for inspection. Since our framework performs greedy
pruning of highly over-parameterized models, it is essential to know where the harmful elements"
ACCURACY-DRIVEN PRUNING,0.2608695652173913,Under review as a conference paper at ICLR 2022
ACCURACY-DRIVEN PRUNING,0.2670807453416149,"are likely to be. Some elements may appear to be harmful for the downstream task (especially in
early iterations of the framework), but this often ends up being an artifact of over-parameterization.
As a result, when elements are not carefully ordered for inspection, our framework lands in local
minima of the validation loss function, leading to inefﬁcient models with sub-optimal generalization
ability. Our solution to this problem utilizes the unique linguistic properties captured by the different
Transformer layers (Jawahar et al., 2019) to guide the ordering of elements for inspection. For
example, it was found that BERT captures phrase-level information in the lower layers, mapping
related tokens together. The lower layers also capture surface features, while the middle layers
capture syntactic features and higher layers capture semantic features. It was also observed that
BERT requires deeper layers only when long-range dependency information is required. Different
tasks require different types of linguistic knowledge. For example, sentiment analysis requires only
local context, and long-range information often ends up confusing the model, since sentiments often
change rapidly; it is also unlikely that syntactic and semantic information are needed. Hence, we
place the ﬁnal layer at the front of the queue, and work our way backwards towards the ﬁrst layer,
since blocks in the ﬁnal layers are more likely to hinder performance on sentiment analysis. This
ordering of elements ensures that elements that are pruned early in our framework (when the model
is most over-parameterized) do not lead the system into bad local minima."
SELECTIVE USE OF HARD SELF-ATTENTION,0.2732919254658385,"3.2
SELECTIVE USE OF HARD SELF-ATTENTION"
SELECTIVE USE OF HARD SELF-ATTENTION,0.2795031055900621,"In traditional Transformer architectures, the self-attention operation computes the attention scores
of each token in the input sequence with all the other tokens. These attention scores, after passing
through the softmax operation, are used to build the new representation for the token based on the
extent of attention it places on the other tokens. The use of this ”soft” attention enables end-to-end
training of the Transformer. However, we observe that replacing the softmax operation on all at-
tention scores with a softmax only on the attention scores above a certain threshold Ta in selected
layers after training and ﬁne-tuning is completed helps the Transformer focus only on the relevant
parts of the input sequence (Fig. 1). This leads to better information ﬂow inside the model, improv-
ing performance on many downstream tasks (especially those involving sequence classiﬁcation and
question answering). This also introduces a large amount of activation sparsity that can be exploited
to analyze the information faster, and helps alleviate the critical memory bottleneck in Transform-
ers. Ta is set to a fraction of the maximum attention score of each token, and tokens on which the
selected token places signiﬁcantly less attention than it places on its most relevant token are consid-
ered irrelevant. The exact value is tuned for the different downstream tasks (based on maximizing
the validation loss), since we observe that different tasks require different attention spans for opti-
mal performance. We note that replacing soft attention with hard attention in all layers (especially
the deeper layers) leads to loss of important information in the model and hence, loss of accuracy,
necessitating its selective use. In order to identify the layers that beneﬁt from hard attention, we
replace the soft attention with hard attention in each layer (one by one), and inspect which type of
attention leads to smaller validation loss."
SELECTIVE USE OF HARD SELF-ATTENTION,0.2857142857142857,"Figure 1: Illustrations of (a) Traditional ”soft” self-attention and (b) ”Hard” self-attention for
the word ”he”. In hard self-attention, ”he” concentrates all of its focus on ”John”, while in soft
self-attention, ”he” places a small amount of attention on the other irrelevant words also. Therefore,
hard self-attention helps build a better representation for ”he”."
SELECTIVE USE OF HARD SELF-ATTENTION,0.2919254658385093,Under review as a conference paper at ICLR 2022
EXPERIMENTS AND RESULTS,0.2981366459627329,"4
EXPERIMENTS AND RESULTS"
EXPERIMENTS AND RESULTS,0.30434782608695654,"We implement our techniques within Huggingface’s Transformers library in PyTorch (Wolf et al.,
2019). We use Intel AI’s NLP Architect for experiments on Q8BERT. The experiments were per-
formed on a GeForce RTX 2080 Ti GPU with 11GB memory. All results are reported on the test set,
averaged across 10 runs with random seeds after 3 epochs of ﬁne-tuning, unless otherwise speciﬁed.
We randomly sample 15% of the training set with class balance, and use it as the validation set
to guide the Specialization process. We present results on the GLUE dev set in Appendix A."
EXPERIMENTS AND RESULTS,0.3105590062111801,"Specialization leads to more accurate models that are also faster and smaller. We present results
on GLUE (Wang et al., 2019), a set of Language Understanding tasks, and SQUADv1.1 (Rajpurkar
et al., 2016), a Question Answering task, in Table 1. For GLUE, we present results on the test set
using the GLUE evaluation server to obtain the scores (Xu et al., 2020), and for SQUAD, we present
results on the dev set. Specialized models are up to 4.5% more accurate, while also being up to
2.5x faster and up to 3.2x smaller than their baseline counterparts. In addition, Specialization is
able to obtain substantial improvements over Q8BERT-base (already 4x smaller than BERT-base
due to the use of 8-bit integer quantization) and DistilBERT-base (60% faster and smaller than
BERT-base). Specialized Q8BERT-base and DistilBERT-base models exceed the accuracy of non-
Specialized BERT-base models, while being up to 3.7x faster and 12.1x smaller than BERT-base.
Therefore, Specialization can be used in conjunction with other approaches that improve the efﬁ-
ciency of Transformers to create highly efﬁcient accurate models for different downstream tasks."
EXPERIMENTS AND RESULTS,0.3167701863354037,"Table 1: Results on GLUE and SQUAD v1.1. We report Matthews correlation for CoLA, Pearson
Correlation for STS-B and accuracy for all other tasks. We report only “matched” accuracy for
MNLI and the Exact Match score for SQUAD. Speedup and Compression are reported over the
non-Specialized baselines for DistilBERT and Q8BERT, and not over BERT-base."
EXPERIMENTS AND RESULTS,0.32298136645962733,"Specialization reduces sensitivity to random seed initialization. Previous research (Dodge et al.,
2020) has shown that the random seed (which determines the initialization of the task-speciﬁc layer
and the order of training data for the downstream task) has a signiﬁcant impact on the quality of
the ﬁne-tuned models. When the amount of training data is small (which is the case with most
downstream NLP tasks), this effect becomes more pronounced, evidenced by the fact that WNLI,
which has the least amount of training samples (634 training samples), exhibits highest variance
across runs (Table 2). Therefore, in order to ﬁnd a model with high accuracy on a downstream
task, multiple models need to be created (using different random seeds), and evaluated. Here, we
demonstrate that Specialized models exhibit signiﬁcantly less sensitivity to random seeds than their
non-Specialized counterparts (Table 2), thereby greatly increasing the odds of ﬁnding “good” models
in fewer iterations. In particular, we ﬁnd that even if two models have vastly different accuracies"
EXPERIMENTS AND RESULTS,0.32919254658385094,Under review as a conference paper at ICLR 2022
EXPERIMENTS AND RESULTS,0.33540372670807456,"on the validation set before Specialization, they converge to similar accuracies after Specialization.
This, in turn, reduces variance on the test set."
EXPERIMENTS AND RESULTS,0.3416149068322981,"Table 2: Sensitivity to random seeds. Results reported are averaged across the GLUE tasks and
SQUAD on the Base models of each Transformer."
EXPERIMENTS AND RESULTS,0.34782608695652173,"Transformer
Average variance
across tasks"
EXPERIMENTS AND RESULTS,0.35403726708074534,"Maximum
variance for a
single task"
EXPERIMENTS AND RESULTS,0.36024844720496896,"Average of
maximum task
scores"
EXPERIMENTS AND RESULTS,0.36645962732919257,"Average of
minimum task
scores
BERT
1.48
4.86 (WNLI)
80.05
76.53
Specialized BERT
0.86
1.95 (WNLI)
82.02
79.93
Q8BERT
1.61
4.62 (WNLI)
79.01
75.29
Specialized
Q8BERT
1.1
2.02 (WNLI)
81.14
79.58"
EXPERIMENTS AND RESULTS,0.37267080745341613,"DistilBERT
1.29
3.85 (WNLI)
78.78
74.89
Specialized
DistilBERT
0.83
1.78 (WNLI)
80.96
79.59"
EXPERIMENTS AND RESULTS,0.37888198757763975,"Larger models Specialize better. Current state-of-the-art Transformer networks, such as T5 (Raffel
et al., 2019) and GPT-3 (Brown et al., 2020), have hundreds of billions of parameters. Model sizes
are also expected to grow further in the future as increasing the number of parameters has been
shown to improve performance. This makes it computationally challenging to train Transformers as
well as perform inference using them. Recent research (Li et al., 2020) has shown that larger models
converge in a signiﬁcantly smaller number of training iterations than smaller models, and hence they
train faster in spite of requiring more time per iteration. However, larger models are signiﬁcantly
slower than smaller models at inference time. Here, we demonstrate that Specialized larger models
achieve much higher accuracy while being comparable in speed to Specialized smaller models (Table
3). In addition to having a higher accuracy ceiling, we also demonstrate that Specialized BERT-
Large (pruned beyond the max accuracy point, by relaxing the constraints to trade-off accuracy for
efﬁciency), is 1.5x faster than Specialized BERT-base at iso-accuracy. This indicates that larger
models capture greater linguistic knowledge, but also contain more redundant parameters for the
different downstream tasks, further motivating the development of larger and better language models
that can be Specialized for fast and accurate inference."
EXPERIMENTS AND RESULTS,0.38509316770186336,"Table 3: Specialization of BERT-base and BERT-large. Results reported are averaged across the
GLUE tasks and SQUAD."
EXPERIMENTS AND RESULTS,0.391304347826087,"Transformer
Average accuracy
Average samples/second
BERT-Base
78.88
0.21
Specialized BERT-Base
80.58
0.12
BERT-Large
80.98
0.45
Specialized BERT-Large
82.89
0.15"
EXPERIMENTS AND RESULTS,0.39751552795031053,"Figure 2: The winning tickets of con-
ventional ﬁne-tuned and Specialized
BERT-Base on MRPC."
EXPERIMENTS AND RESULTS,0.40372670807453415,"Previously proposed techniques to improve inference
efﬁciency are more effective when applied to Special-
ized models. Specialization identiﬁes and prunes harm-
ful elements of the Transformer for the downstream task
at hand. Techniques that prune elements that have mini-
mal impact on the output are complementary to our tech-
niques, and we demonstrate this using the popular Lottery
Ticket Hypothesis (Frankle & Carbin, 2019), which ﬁnds
sparse sub-networks that can be trained to match the per-
formance of the large network. Lottery Ticket Hypoth-
esis has been successfully applied to Transformers also
(Prasanna et al., 2020; Liang et al., 2021; Chen et al.,
2020), and using these techniques, we demonstrate that
winning tickets of the Specialized model are consistently
more accurate than the iso-efﬁcient winning tickets of the
conventional ﬁne-tuned models (Figure 2)."
EXPERIMENTS AND RESULTS,0.40993788819875776,Under review as a conference paper at ICLR 2022
EXPERIMENTS AND RESULTS,0.4161490683229814,"Specialization provides insights into the working of Transformers. We analyze which elements
of the Transformer are pruned for different downstream tasks using different Transformer models
(Fig. 3). We ﬁnd that the differences in importance of elements are more pronounced across different
tasks than across different models. For example, for sentiment analysis, long-range dependency
information is not required, and often ends up confusing the model. Hence, for all models ﬁne-
tuned for sentiment analysis, we observe that components in later layers (closer to the output) are
more likely to be pruned. This is not the case with Language Modelling tasks (predicting masked
words in text), where longer attention spans are required. Across models, we only observe subtle
differences. For example, we ﬁnd that XLNet (auto-regressive) is able to learn important task-
speciﬁc information earlier than BERT (non auto-regressive), similar to the observation made in
(Sajjad et al., 2020). Hence, we are able to drop more components (in earlier layers) in XLNet than
in BERT, leading to more efﬁcient models for inference. In DistilBERT (a distilled model), we ﬁnd
that there is a clear demarcation in linguistic knowledge across layers due to the reduced capacity of
the model. This is evidenced by the fact that elements in the top four layers are never pruned across
all Language Understanding tasks, while the boundaries are more soft in the original models. We
also observe that Hard Attention is most useful in the lower layers, where phrase-level information
is captured, for all the models and tasks. As future work, our framework can be combined with
previously proposed techniques (such as probing classiﬁers) to gain deeper understanding of the
working of Transformers, especially at ﬁner levels of granularity."
EXPERIMENTS AND RESULTS,0.422360248447205,"Figure 3: Elements pruned for (a) downstream tasks and (b) Transformer architectures across
GLUE and SQUAD in our most accurate models for each task. Here, ATTN-Approx means only
certain attention heads inside the attention block are pruned, and FFN-Approx means only certain
neurons inside the feed-forward block are pruned."
EXPERIMENTS AND RESULTS,0.42857142857142855,"Specialization using previously proposed importance estimation techniques leads to less accu-
rate models.
L1-norm and magnitude based pruning techniques are designed to prune elements
that have minimal impact on the output, and hence, they are not effective at identifying and prun-
ing elements that have most detrimental impact on output. Gradient-based methods such as Taylor
expansion are not reliable for increasing accuracy on the downstream tasks, since Transformers can-
not be repeatedly ﬁne-tuned to recover the accuracy losses from approximating the model (they very
quickly overﬁt the limited training data for the downstream tasks). In addition, while knowledge dis-
tillation is complementary to our method (as evidenced by our results on DistilBERT), we show that
the accuracy gains from Specialization are signiﬁcantly higher than those from regularized knowl-
edge distillation proposed in (Hou et al., 2020). Super-tickets (Liang et al., 2021) demonstrates that
accuracy can be improved by ﬁnding winning lottery tickets at low levels of pruning. However, we
ﬁnd that alleviating the over-parameterization issue by pruning elements that hinder performance
(and the selective use of hard attention) leads to more accurate models than by pruning elements that
have least impact on output in the original model (Table 4)."
EXPERIMENTS AND RESULTS,0.43478260869565216,"Specialization incurs minimal overhead. Our Specialization framework does not require any ad-
ditional training or ﬁne-tuning iterations. It only requires multiple passes through a small validation"
EXPERIMENTS AND RESULTS,0.4409937888198758,Under review as a conference paper at ICLR 2022
EXPERIMENTS AND RESULTS,0.4472049689440994,"set. Our hierarchical processing ensures that the number of elements to inspect is greatly reduced.
In addition, each pass is expected to become progressively faster, since the framework potentially
prunes an element in each iteration, and our ordering of elements ensures that large, time-consuming
blocks are pruned early in the process, making future iterations faster. Therefore, the overheads of
Specialization are negligible compared to the time required for re-training or additional ﬁne-tuning.
We ﬁnd that the average (for GLUE and SQUAD) wall-clock time for Specialization is only 7.6
minutes for BERT-base and 18.3 minutes for BERT-large on a single GPU."
EXPERIMENTS AND RESULTS,0.453416149068323,"We also evaluate the heuristics used in our framework (Table 4). When elements are not carefully
ordered for inspection, we ﬁnd that elements that are pruned early in the Specialization process are
pruned due to over-parameterization, and not because the linguistic knowledge contained in these
elements is harmful/not useful for the task at hand. This leads the system into a bad local minima,
where the validation loss cannot be further reduced by pruning other elements. We observe that
starting at the layer granularity leads to worse models than starting at the block granularity. This
is because the attention and feed-forward blocks in each layer have vastly differently functionality,
and hence, the effect of removing an attention block of relatively high signiﬁcance is often masked
by removing the feed-forward block that greatly hinders performance in the same layer (and vice-
versa), causing the entire layer to be pruned. This destabilizes the Specialization process, and also
leads the system into bad local minima, thereby creating inferior models. To drive home the fact
that our greedy approach combined with a global error bound does not lead to inferior models, we
also experiment with an adaptive loss threshold. In particular, we use a very tight constraint when
analyzing elements at coarse granularities, and relax the constraint as we move towards ﬁner granu-
larities. We again ﬁnd that there is negligible change in the quality of the ﬁnal model produced (the
accuracy is slightly lower, possibly due to over-ﬁtting to the validation set), but the Specialization
process is signiﬁcantly slower. We hypothesize that a single global error bound is sufﬁcient because
we order the elements in such a way that for the given task at hand, we intuitively expect that the
elements at the head of the queue are likely to be removed using the linguistic knowledge in different
layers."
EXPERIMENTS AND RESULTS,0.45962732919254656,"Table 4: [Left] Results of Specialization with previously proposed methods (MRPC with BERT-
Base). For L1-norm, magnitude and Taylor (absolute gradient of loss), we prune 5% of the least
important weights at a time and record the test accuracy. We report the highest test accuracy seen in
this process. [Right] Results of Specialization with different heuristics. All heuristics use hier-
archical processing of ordered elements with Selective Hard Attention, unless otherwise speciﬁed."
EXPERIMENTS AND RESULTS,0.4658385093167702,"Pruning Method
Accuracy
Baseline (No
Pruning)
83.11"
EXPERIMENTS AND RESULTS,0.4720496894409938,"L1-Norm
83.16
Magnitude
83.18
Taylor Expansion
(with additional
ﬁne-tuning) 83.19"
EXPERIMENTS AND RESULTS,0.4782608695652174,"Taylor Expansion (no
additional
ﬁne-tuning) 83.32"
EXPERIMENTS AND RESULTS,0.484472049689441,"Regularized
Knowledge
Distillation
(DynaBERT) 83.46"
EXPERIMENTS AND RESULTS,0.4906832298136646,"Super-ticket
84.62
Ours
85.88"
EXPERIMENTS AND RESULTS,0.4968944099378882,Heuristic
EXPERIMENTS AND RESULTS,0.5031055900621118,"Specialized
Model
Accuracy"
EXPERIMENTS AND RESULTS,0.5093167701863354,"Wall-clock
Specialization
Time
(minutes)
Randomly
ordered
elements"
EXPERIMENTS AND RESULTS,0.515527950310559,"83.42
7.2"
EXPERIMENTS AND RESULTS,0.5217391304347826,"Start by
inspecting
layers"
EXPERIMENTS AND RESULTS,0.5279503105590062,"83.44
5.8"
EXPERIMENTS AND RESULTS,0.5341614906832298,"No Selective
Hard Attention
84.94
6.3"
EXPERIMENTS AND RESULTS,0.5403726708074534,"Adaptive
Threshold
85.82
29.3"
EXPERIMENTS AND RESULTS,0.546583850931677,"Ours
85.88
6.6"
CONCLUSION,0.5527950310559007,"5
CONCLUSION"
CONCLUSION,0.5590062111801242,"We proposed a Specialization framework to optimize ﬁne-tuned Transformers for the different down-
stream tasks. The framework identiﬁes elements that are hinder performance on the downstream task
at hand, and prunes these elements. We also demonstrated the advantage of selectively using hard
self-attention in selected layers to improve information ﬂow. Using this framework, we produced
models that were guaranteeably more accurate, while also being up to faster and smaller."
CONCLUSION,0.5652173913043478,Under review as a conference paper at ICLR 2022
REFERENCES,0.5714285714285714,REFERENCES
REFERENCES,0.577639751552795,"Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung.
Structured pruning of deep convolutional
neural networks.
ACM J. Emerg. Technol. Comput. Syst., 13(3):32:1–32:18, 2017.
doi:
10.1145/3005348. URL https://doi.org/10.1145/3005348."
REFERENCES,0.5838509316770186,"Tom B. Brown, Benjamin Mann, and Nick Ryder et al. Language models are few-shot learners.
CoRR, abs/2005.14165, 2020."
REFERENCES,0.5900621118012422,"Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang,
and Michael Carbin.
The lottery ticket hypothesis for pre-trained BERT networks.
In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
b6af2c9703f203a2794be03d443af2e3-Abstract.html."
REFERENCES,0.5962732919254659,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding.
In Jill Burstein, Christy Doran, and
Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–
4186. Association for Computational Linguistics, 2019.
doi: 10.18653/v1/n19-1423.
URL
https://doi.org/10.18653/v1/n19-1423."
REFERENCES,0.6024844720496895,"Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.
Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.
CoRR, abs/2002.06305, 2020. URL https://arxiv.org/abs/2002.06305."
REFERENCES,0.6086956521739131,"Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
Depth-adaptive transformer.
In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
SJg7KhVKPH."
REFERENCES,0.6149068322981367,"Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with struc-
tured dropout. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.
net/forum?id=SylO2yStDr."
REFERENCES,0.6211180124223602,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.
net/forum?id=rJl-b3RcF7."
REFERENCES,0.6273291925465838,"Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic
BERT with adaptive width and depth. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-
sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/6f5216f8d89b086c18298e043bfe48ed-Abstract.html."
REFERENCES,0.6335403726708074,"Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of
language?
In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez (eds.), Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy,
July 28- August 2, 2019, Volume 1: Long Papers, pp. 3651–3657. Association for Computational
Linguistics, 2019. doi: 10.18653/v1/p19-1356. URL https://doi.org/10.18653/v1/
p19-1356."
REFERENCES,0.639751552795031,"Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling BERT for natural language understanding. CoRR, abs/1909.10351, 2019.
URL http://arxiv.org/abs/1909.10351."
REFERENCES,0.6459627329192547,Under review as a conference paper at ICLR 2022
REFERENCES,0.6521739130434783,"Ashish Khetan and Zohar S. Karnin. schubert: Optimizing elements of BERT. In Dan Jurafsky,
Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2807–
2818. Association for Computational Linguistics, 2020. URL https://www.aclweb.org/
anthology/2020.acl-main.250/."
REFERENCES,0.6583850931677019,"Franc¸ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. Block pruning for faster
transformers.
In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau
Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 Novem-
ber, 2021, pp. 10619–10629. Association for Computational Linguistics, 2021. URL https:
//aclanthology.org/2021.emnlp-main.829."
REFERENCES,0.6645962732919255,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
H1eA7AEtvS."
REFERENCES,0.6708074534161491,"Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.
Train big, then compress: Rethinking model size for efﬁcient training and inference of transform-
ers. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5958–
5968. PMLR, 2020. URL http://proceedings.mlr.press/v119/li20m.html."
REFERENCES,0.6770186335403726,"Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo
Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compres-
sion to improving generalization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
(eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 6524–6538. Associa-
tion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.510. URL https:
//doi.org/10.18653/v1/2021.acl-long.510."
REFERENCES,0.6832298136645962,"Paul Michel, Omer Levy, and Graham Neubig.
Are sixteen heads really better than one?
In
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December
2019, Vancouver, BC, Canada, pp. 14014–14024, 2019. URL http://papers.nips.cc/
paper/9551-are-sixteen-heads-really-better-than-one."
REFERENCES,0.6894409937888198,"Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efﬁcient inference. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=SJGCiw5gl."
REFERENCES,0.6956521739130435,"Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT plays the lottery, all tickets are
winning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, On-
line, November 16-20, 2020, pp. 3208–3229. Association for Computational Linguistics, 2020.
doi: 10.18653/v1/2020.emnlp-main.259. URL https://doi.org/10.18653/v1/2020.
emnlp-main.259."
REFERENCES,0.7018633540372671,"Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019."
REFERENCES,0.7080745341614907,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683."
REFERENCES,0.7142857142857143,"Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for
machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), Proceedings"
REFERENCES,0.7204968944099379,Under review as a conference paper at ICLR 2022
REFERENCES,0.7267080745341615,"of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pp. 2383–2392. The Association for Computational
Linguistics, 2016. doi: 10.18653/v1/d16-1264. URL https://doi.org/10.18653/v1/
d16-1264."
REFERENCES,0.7329192546583851,"Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. Poor man’s bert: Smaller and faster
transformer models, 2020."
REFERENCES,0.7391304347826086,"Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://
arxiv.org/abs/1910.01108."
REFERENCES,0.7453416149068323,"Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney,
and Kurt Keutzer. Q-BERT: hessian based ultra low precision quantization of BERT. In The
Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innova-
tive Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, pp. 8815–8821. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/
AAAI/article/view/6409."
REFERENCES,0.7515527950310559,"Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert:
a compact task-agnostic BERT for resource-limited devices. In Dan Jurafsky, Joyce Chai, Natalie
Schluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2158–2170. Associa-
tion for Computational Linguistics, 2020. URL https://www.aclweb.org/anthology/
2020.acl-main.195/."
REFERENCES,0.7577639751552795,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net, 2019.
URL https://openreview.net/forum?id=
rJ4km2R5t7."
REFERENCES,0.7639751552795031,"Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
Minilm:
Deep self-attention distillation for task-agnostic compression of pre-trained transformers.
In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
REFERENCES,0.7701863354037267,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew.
Huggingface’s
transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL
http://arxiv.org/abs/1910.03771."
REFERENCES,0.7763975155279503,"Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short
range attention. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.
net/forum?id=ByeMPlHKPH."
REFERENCES,0.782608695652174,"Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for
accelerating BERT inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault
(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
ACL 2020, Online, July 5-10, 2020, pp. 2246–2251. Association for Computational Linguistics,
2020. URL https://www.aclweb.org/anthology/2020.acl-main.204/."
REFERENCES,0.7888198757763976,"Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing
BERT by progressive module replacing. CoRR, abs/2002.02925, 2020. URL https://arxiv.
org/abs/2002.02925."
REFERENCES,0.7950310559006211,Under review as a conference paper at ICLR 2022
REFERENCES,0.8012422360248447,"Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good sub-
networks provably exist: Pruning via greedy forward selection. In Proceedings of the 37th In-
ternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-
ume 119 of Proceedings of Machine Learning Research, pp. 10820–10830. PMLR, 2020. URL
http://proceedings.mlr.press/v119/ye20b.html."
REFERENCES,0.8074534161490683,"Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT: quantized 8bit BERT.
CoRR, abs/1910.06188, 2019. URL http://arxiv.org/abs/1910.06188."
REFERENCES,0.8136645962732919,"Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu, and Maosong Sun. Know what you don’t
need: Single-shot meta-pruning for attention heads. CoRR, abs/2011.03770, 2020. URL https:
//arxiv.org/abs/2011.03770."
REFERENCES,0.8198757763975155,"Wangchunshu
Zhou,
Canwen
Xu,
Tao
Ge,
Julian
J.
McAuley,
Ke
Xu,
and
Furu
Wei.
BERT loses patience:
Fast and robust inference with early exit.
In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33:
Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual,
2020.
URL https://proceedings.neurips.cc/paper/2020/hash/
d4dd111a4fd973394238aca5c05bebe3-Abstract.html."
REFERENCES,0.8260869565217391,"Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou
Huang, and Jin-Hui Zhu. Discrimination-aware channel pruning for deep neural networks. In
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and
Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montr´eal, Canada, pp. 883–894, 2018.
URL https://proceedings.neurips.cc/
paper/2018/hash/55a7cf9c71f1c9c495413f934dd1a158-Abstract.html."
REFERENCES,0.8322981366459627,Under review as a conference paper at ICLR 2022
REFERENCES,0.8385093167701864,"A
ADDITIONAL EXPERIMENTS AND STUDIES"
REFERENCES,0.84472049689441,"A.1
RESULTS ON THE GLUE DEVELOPMENT SET."
REFERENCES,0.8509316770186336,"We present additional results on the GLUE dev set using the same models used to present results on
the test set in Table 5. We use 15% of the training set as the validation set for Specialization. Similar
to results on the test set, Specialized models are consistently more accurate, faster and smaller than
their conventional ﬁne-tuned counterparts. In addition, Specialization is able to obtain substantial
improvements over already optimized models (Q8BERT and DistilBERT)."
REFERENCES,0.8571428571428571,"Table 5:
Results on GLUE. We report Matthews correlation for CoLA, Pearson Correlation for
STS-B and accuracy for all other tasks. We report only “matched” accuracy for MNLI. Speedup and
Compression are reported over the non-Specialized baselines for DistilBERT and Q8BERT, and not
over BERT-base."
REFERENCES,0.8633540372670807,"A.2
SIGNIFICANCE OF ORDERING ELEMENTS FOR INSPECTION"
REFERENCES,0.8695652173913043,"We ﬁnd that the order in which elements of the ﬁne-tuned model are inspected during accuracy-
driven pruning has a large impact on the generalization performance of the Specialized model. Prior
research (Jawahar et al., 2019) has shown that the linguistic knowledge contained in different Trans-
former layers can be demarcated into three main functional regions: phrase-level information in
bottom layers (the ﬁrst one-third of transformer layers closest to the input), semantic and syntactic
information in the middle layers, and long-range dependency information in the top layers. When
elements are not carefully ordered for inspection, we ﬁnd that elements that are pruned early in
the Specialization process are pruned due to over-parameterization, and not because the linguistic
knowledge contained in these elements is harmful/not useful for the task at hand. This leads the
system into a bad local minima, where the validation loss cannot be further reduced by pruning
other elements (Table 6). The quality of the Specialized model is worst when the blocks containing
the most important linguistic knowledge is inspected ﬁrst (bottom layers for sentiment analysis, see
Appendix A.3), since these blocks may get pruned simply to alleviate the over-parameterization. In
addition, we ﬁnd that the ordering of elements within the same functional block (for example, blocks
in layer 10 before layer 11 or vice-versa), has negligible impact on the quality of the Specialized
model, further demonstrating the existence of regions with different kinds of linguistic knowledge."
REFERENCES,0.8757763975155279,Under review as a conference paper at ICLR 2022
REFERENCES,0.8819875776397516,"Table 6: Specialized accuracy from inspecting elements in different orders on SST-2 using
BERT-Base. The baseline validation and dev accuracy of the ﬁne-tuned model are 93.98 and 93.23,
respectively."
REFERENCES,0.8881987577639752,"Order in which elements are
inspected"
REFERENCES,0.8944099378881988,"Validation accuracy of
Specialized model"
REFERENCES,0.9006211180124224,"Dev accuracy of Specialized
model
Bottom, Middle, Top
94.06
93.28
Bottom, Top, Middle
94.38
93.41
Middle, Bottom, Top
94.44
93.52
Middle, Top, Bottom
95.22
94.28
Top, Bottom, Middle
95.18
93.88
Top, Middle, Bottom
95.24
94.56"
REFERENCES,0.906832298136646,"A.3
ANALYZING THE LINGUISTIC KNOWLEDGE REQUIRED FOR DIFFERENT DOWNSTREAM
TASKS"
REFERENCES,0.9130434782608695,"Different downstream tasks require different types of linguistic knowledge. We ﬁnd that the most
important functional block of the Transformer model for each task (corresponding to the most im-
portant kind of linguistic knowledge required to effectively solve the task) agrees with our intuition
about the task. For example, we expect sentiment analysis to require only local context, since long-
range information often ends up confusing the model (sentiments often change rapidly); it is also
unlikely that syntactic and semantic information are needed. This is experimentally validated by the
fact that inspecting the blocks in top layers, followed by the middle and ﬁnally the bottom layers
leads to most accurate models on SST-2 (Table 7). Similarly, we expect knowledge about language
syntax and semantics to be most important for a task that tests the linguistic acceptability of a given
sentence (CoLA). We also ﬁnd that this holds across all Transformer architectures that we study in
this work (BERT, DistilBERT, Q8BERT and XLNet). We leverage this intuition about the different
NLP tasks in our framework to reduce the overheads of Specialization without compromising on
the quality of the Specialized model. If there is no prior knowledge of the task, we need to inspect
elements in multiple orders to obtain the best model."
REFERENCES,0.9192546583850931,"Table 7: The order of inspected elements that provides maximum accuracy for different down-
stream tasks using BERT-Base, DistilBERT-Base, Q8BERT-Base and XLNet-Base. We ﬁnd
that the same ordering provides best performance on all the studied Transformer architectures."
REFERENCES,0.9254658385093167,"Dataset
Task
Best ordering"
REFERENCES,0.9316770186335404,"Most important type
of linguistic knowledge
for solving this task
CoLA
Linguistic acceptability
Top, Bottom, Middle
Semantic/ Syntactic
MNLI
Entailment
Top, Middle, Bottom
Phrase-level
MRPC
Semantic equivalence
Top, Middle, Bottom
Phrase-level
QNLI
Question answering
Top, Middle, Bottom
Phrase-level
QQP
Semantic equivalence
Top, Middle, Bottom
Phrase-level
RTE
Entailment
Top, Middle, Bottom
Phrase-level
SST-2
Sentiment Analysis
Top, Middle, Bottom
Phrase-level
STS-B
Sentence similarity
Top, Middle, Bottom
Phrase-level
WNLI
Reading comprehension
Top, Middle, Bottom
Phrase-level
SQUAD
Question answering
Top, Middle, Bottom
Phrase-level"
REFERENCES,0.937888198757764,Under review as a conference paper at ICLR 2022
REFERENCES,0.9440993788819876,"A.4
ANALYZING THE BENEFITS OF SPECIALIZATION"
REFERENCES,0.9503105590062112,"Transformers are pre-trained on a large text corpus on a difﬁcult NLP task, such as predicting the
next word in text given all the preceding words. When pre-trained models are ﬁne-tuned for different
downstream tasks, they contain lots of task-irrelevant information that add noise and confuse the
model. In addition, since the pre-trained models are highly overparameterized, they severely overﬁt
the small ﬁne-tuning datasets. Intuitively, Specialization helps the model focus on the task at hand
by eliminating the irrelevant information in the ﬁne-tuned models."
REFERENCES,0.9565217391304348,"A.4.1
IMPROVED GENERALIZATION"
REFERENCES,0.9627329192546584,"Accuracy-driven pruning can be seen as a form of training, since the objective of accuracy-driven
pruning – minimizing loss on the (validation) dataset – is exactly the same as the objective of train-
ing. When large, over-parameterized models are trained on very limited data, the constraints of
accuracy-driven pruning – (1) loss can be reduced only be setting weights to 0 (equivalent to prun-
ing elements), and no other changes to weights are allowed, and (2) a majority of samples in the
validation set must beneﬁt from each weight update; reduction in loss is a necessary but insufﬁcient
condition for weights to get updated – help it learn more effectively than SGD. As a result, we ﬁnd
that training with SGD on a subset of the training data, followed by accuracy-driven pruning on the
remaining unseen data, produces models with better generalization performance than training with
SGD on the entire dataset (Figure 4). As an added beneﬁt, accuracy-driven pruning also leads to
smaller and faster models at inference time."
REFERENCES,0.968944099378882,"Figure 4: Change in class boundaries during Specialization on MRPC with BERT-Base. We
use T-distributed Stochastic Neighbor Embedding (TSNE) on word embeddings right before the
ﬁnal classiﬁer, using the 3 main components of each embedding. Here, 0 and 1 refer to sentence
pairs that are semantically non-equivalent and equivalent, respectively. Specialized models show
better class separability, and hence, are more accurate."
REFERENCES,0.9751552795031055,"A.4.2
REDUCED VARIANCE"
REFERENCES,0.9813664596273292,"Accuracy-driven pruning and selective Hard Attention ﬁlter out noise in the model by removing
irrelevant information that ends up confusing the model. Accuracy-driven pruning reduces noise by
removing task-irrelevant information. Selective Hard Attention further reduces noise by ensuring
that the model focuses only on the relevant parts of the input. We ﬁnd that this leads to reduced
variance on the dev/test sets, where Specialized models from different seeds acquire similar word
representations, even if the initial ﬁne-tuned models have vastly different word representations (and
hence accuracies). (Figure 5). However, the reduction in variance is limited by the effects of random
initialization of the task-speciﬁc ﬁnal layer."
REFERENCES,0.9875776397515528,Under review as a conference paper at ICLR 2022
REFERENCES,0.9937888198757764,"Figure 5: Attention patterns of Specialized models on WNLI using BERT-Base, shown at the
ﬁnal ATTN block in the model (layer 12 in the ﬁne-tuned model, layer 9 in the Specialized
model). WNLI has the smallest training set among all of our studied tasks, and hence, exhibits
highest sensitivity to random seeds."
