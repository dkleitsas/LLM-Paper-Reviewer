Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009149130832570906,"With the ability to learn from static datasets, Offline Reinforcement Learning (RL)
1"
ABSTRACT,0.0018298261665141812,"emerges as a compelling avenue for real-world applications. However, state-of-the-
2"
ABSTRACT,0.0027447392497712718,"art offline RL algorithms perform sub-optimally when confronted with limited data
3"
ABSTRACT,0.0036596523330283625,"confined to specific regions within the state space. The performance degradation
4"
ABSTRACT,0.004574565416285453,"is attributed to the inability of offline RL algorithms to learn appropriate actions
5"
ABSTRACT,0.0054894784995425435,"for rare or unseen observations. This paper proposes a novel domain knowledge-
6"
ABSTRACT,0.006404391582799634,"based regularization technique and adaptively refines the initial domain knowledge
7"
ABSTRACT,0.007319304666056725,"to considerably boost performance in limited data with partially omitted states.
8"
ABSTRACT,0.008234217749313814,"The key insight is that the regularization term mitigates erroneous actions for
9"
ABSTRACT,0.009149130832570906,"sparse samples and unobserved states covered by domain knowledge. Empirical
10"
ABSTRACT,0.010064043915827997,"evaluations on standard discrete environment datasets demonstrate a substantial
11"
ABSTRACT,0.010978956999085087,"average performance increase compared to ensemble of domain knowledge and
12"
ABSTRACT,0.011893870082342177,"existing offline RL algorithms operating on limited data.
13"
INTRODUCTION,0.012808783165599268,"1
Introduction
14"
INTRODUCTION,0.013723696248856358,"Offline RL [9, 1], also referred to as batch RL, is a learning approach that focuses on extracting
15"
INTRODUCTION,0.01463860933211345,"knowledge solely from static datasets. This class of algorithms has a wider range of applications being
16"
INTRODUCTION,0.01555352241537054,"particularly appealing to real-world data sets from business [46], healthcare [25], and robotics [35].
17"
INTRODUCTION,0.01646843549862763,"However, offline RL poses unique challenges, including over-fitting and the need for generalization
18"
INTRODUCTION,0.01738334858188472,"to data not present in the dataset. To surpass the behavior policy, offline RL algorithms need to
19"
INTRODUCTION,0.018298261665141813,"query Q values of actions not in the dataset, causing extrapolation errors [21]. Most offline RL
20"
INTRODUCTION,0.0192131747483989,"algorithms address this problem by enforcing constraints that ensure that the learned policy does not
21"
INTRODUCTION,0.020128087831655993,"deviate too far away from the data set’s state action distribution [13, 11] or is conservative towards
22"
INTRODUCTION,0.021043000914913082,"Out-of-Distribution (OOD) actions [21, 20]. However, such approaches are designed on coherent
23"
INTRODUCTION,0.021957913998170174,"batches [13], which do not account for OOD states.
24"
INTRODUCTION,0.022872827081427266,"In many domains, such as business and healthcare, available data is scarce and often confined to expert
25"
INTRODUCTION,0.023787740164684355,"behaviors within a limited state space. For example, a sales recommendation system, where historic
26"
INTRODUCTION,0.024702653247941447,"data may not contain details about many active users and operator gives coupon of higher value to
27"
INTRODUCTION,0.025617566331198535,"attract sales. Learning on such limited data sets can curtail the generalization capabilities of state-of-
28"
INTRODUCTION,0.026532479414455627,"the-art (SOTA) offline RL algorithms, resulting in sub-optimal performance [23]. We illustrate this
29"
INTRODUCTION,0.027447392497712716,"limitation via Fig 1. In Fig 1a) the state action space of a simple Mountain Car environment [27] is
30"
INTRODUCTION,0.028362305580969808,"plotted for an expert dataset [32] and a partial dataset with first 10% samples from the entire dataset.
31"
INTRODUCTION,0.0292772186642269,"Fig 1b) shows the average reward obtained over these data sets and the average difference between
32"
INTRODUCTION,0.03019213174748399,"the Q value of action taken by the under-performing Conservative Q Learning (CQL) [21] agent and
33"
INTRODUCTION,0.03110704483074108,"the action in the full expert dataset for unseen states. It can be observed that the performance of the
34"
INTRODUCTION,0.03202195791399817,"offline RL agent considerably drops. This is attributed to the critic overestimating the Q value of
35"
INTRODUCTION,0.03293687099725526,"non-optimal actions for states that do not occur in the dataset while training.
36"
INTRODUCTION,0.03385178408051235,"Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing
distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy
for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset
for unseen states."
INTRODUCTION,0.03476669716376944,"In numerous real-world applications, expert insights regarding the general behavior of a policy are
37"
INTRODUCTION,0.035681610247026534,"often accessible [33]. For example, sales operators often distribute lower discount coupons to active
38"
INTRODUCTION,0.036596523330283626,"users to maximize profit. While these insights may not be optimal, they serve as valuable guidelines
39"
INTRODUCTION,0.03751143641354071,"for understanding the overall behavior of the policy. A rich literature in knowledge distillation [18]
40"
INTRODUCTION,0.0384263494967978,"has shown that teacher networks trained on domain knowledge can transfer knowledge to another
41"
INTRODUCTION,0.039341262580054895,"network unaware of it. This work aims to leverage a teacher network mimicking simple decision
42"
INTRODUCTION,0.04025617566331199,"tree-based domain knowledge to help offline RL generalize in limited data settings.
43"
INTRODUCTION,0.04117108874656908,"The paper makes the following novel contributions:
44"
INTRODUCTION,0.042086001829826164,"• We introduce an algorithm dubbed ExID, leveraging intuitive human obtainable expert
45"
INTRODUCTION,0.043000914913083256,"insights. The domain expertise is incorporated into a teacher policy, which improves offline
46"
INTRODUCTION,0.04391582799634035,"RL in limited-data settings through regularization.
47"
INTRODUCTION,0.04483074107959744,"• The teacher based on expected performance improvement of the offline policy during
48"
INTRODUCTION,0.04574565416285453,"training, improving the teacher network beyond initial heuristics.
49"
INTRODUCTION,0.04666056724611162,"• We demonstrate the effectiveness of our methodology on real sales promotion dataset,
50"
INTRODUCTION,0.04757548032936871,"several discrete OpenAI gym and Minigrid environments with standard offline RL data sets
51"
INTRODUCTION,0.0484903934126258,"and show that ExID significantly exceeds the performance when faced with limited data.
52"
RELATED WORK,0.04940530649588289,"2
Related Work
53"
RELATED WORK,0.05032021957913998,"This work improves offline RL learning on batches sampled from static datasets using domain
54"
RELATED WORK,0.05123513266239707,"expertise. One of the major concerns in offline RL is the erroneous extrapolation of OOD actions
55"
RELATED WORK,0.05215004574565416,"[13]. Two techniques have been studied in the literature to prevent such errors. 1) Constraining the
56"
RELATED WORK,0.053064958828911254,"policy to be close to the behavior policy 2) Penalizing overly optimistic Q values [24]. We discuss a
57"
RELATED WORK,0.053979871912168347,"few relevant algorithms following these principles. In Batch-Constrained deep Q-learning (BCQ)
58"
RELATED WORK,0.05489478499542543,"[13] candidate actions sampled from an adversarial generative model are considered, aiming to
59"
RELATED WORK,0.055809698078682524,"balance proximity to the batch while enhancing action diversity. Algorithms like Random Ensemble
60"
RELATED WORK,0.056724611161939616,"Mixture Model (REM) [2], Ensemble-Diversified Actor-Critic (EDAC) [3] and Uncertainty Weighted
61"
RELATED WORK,0.05763952424519671,"Actor-Critic (UWAC) [42] penalize the Q value according to uncertainty by either using Q ensemble
62"
RELATED WORK,0.0585544373284538,"networks or directly weighting the loss with uncertainty. CQL [21] enforces regularization on Q-
63"
RELATED WORK,0.059469350411710885,"functions by incorporating a term that reduces Q-values for OOD actions while increasing Q-values
64"
RELATED WORK,0.06038426349496798,"for actions within the expected distribution. However, these algorithms do not handle OOD actions
65"
RELATED WORK,0.06129917657822507,"for states not in the static dataset and can have errors induced by changes in transition probability.
66"
RELATED WORK,0.06221408966148216,"Integration of domain knowledge in offline RL, though an important avenue, has not yet been
67"
RELATED WORK,0.06312900274473925,"extensively explored. Domain knowledge incorporation has improved online RL with tight regret
68"
RELATED WORK,0.06404391582799634,"bounds [33, 4]. In offline RL, bootstrapping via blending heuristics computed using Monte-Carlo
69"
RELATED WORK,0.06495882891125343,"returns with rewards has shown to outperform SOTA algorithms by 9% [15]. Recent works improve
70"
RELATED WORK,0.06587374199451052,"offline RL by incorporating a safety expert [40] and preference query [44], contrary to our work
71"
RELATED WORK,0.06678865507776761,"which improves imperfect domain knowledge. The closest to our work is Domain Knowledge guided
72"
RELATED WORK,0.0677035681610247,"Q learning (DKQ) [46] where domain knowledge is represented in terms of action importance and
73"
RELATED WORK,0.0686184812442818,"the Q value is weighted according to importance. However, obtaining action importance in practical
74"
RELATED WORK,0.06953339432753888,"scenarios is nontrivial.
75"
PRELIMINARIES,0.07044830741079597,"3
Preliminaries
76"
PRELIMINARIES,0.07136322049405307,"A DRL setting is represented by a Markov Decision Process (MDP) formalized as (S, A, T, r, ρ0, γ).
77"
PRELIMINARIES,0.07227813357731015,"Here, S denotes the state space, A signifies the action space, T(s′|s, a) represents the transition prob-
78"
PRELIMINARIES,0.07319304666056725,"ability distribution, r : S × A →R is the reward function, ρ0 represents the initial state distribution,
79"
PRELIMINARIES,0.07410795974382434,"and γ ∈(0, 1] is the discount factor. The primary objective of any DRL algorithm is to identify an
80"
PRELIMINARIES,0.07502287282708142,"optimal policy π(a|s) that maximizes Est,at[P∞
t=0 γtr(st, at)] where, s0 ∼d0(.), at ∼π(.|st), and
81"
PRELIMINARIES,0.07593778591033852,"s′ ∼T(.|st, at). Deep Q networks (DQNs) [26] learn this objective by minimizing the Bellman resid-
82"
PRELIMINARIES,0.0768526989935956,"ual (Qθ(s, a) −BπθQθ(s, a))2 where BπθQθ(s, a) = Es′∼T [r(s, a) + γEa′∼πθ(.|s′)[Qθ′(s′, a′)]].
83"
PRELIMINARIES,0.0777676120768527,"The policy πθ chooses actions that maximize the Q value maxa′∈AQθ(s′, a′). However, in offline
84"
PRELIMINARIES,0.07868252516010979,"RL where transitions are sampled from a pre-collected dataset B, the chosen action a′ may exhibit a
85"
PRELIMINARIES,0.07959743824336687,"bias towards OOD actions with inaccurately high Q-values. To handle the erroneous propagation
86"
PRELIMINARIES,0.08051235132662397,"from OOD actions, CQL [22] learns conservative Q values by penalizing OOD actions. The CQL
87"
PRELIMINARIES,0.08142726440988106,"loss for discrete action space is given by
88"
PRELIMINARIES,0.08234217749313816,"Lcql(θ) = min
Q α Es∼B[log
X"
PRELIMINARIES,0.08325709057639524,"a
exp(Qθ(s, a))−"
PRELIMINARIES,0.08417200365965233,"Ea∼B|s[Qθ(s, a)]] + 1"
PRELIMINARIES,0.08508691674290943,"2Es,a,s′∼B[Qθ −Qθ′]2
(1)"
PRELIMINARIES,0.08600182982616651,"Eq. 1 encourages the policy to be close to the actions seen in the dataset. However, CQL works on the
89"
PRELIMINARIES,0.08691674290942361,"assumption of coherent batches, i.e., if (s, a, s′) ∈B, then s′ ∈B. There is no provision for handling
90"
PRELIMINARIES,0.0878316559926807,"OOD actions for s /∈B, which can lead to policy failure when data is limited. In the next sections, we
91"
PRELIMINARIES,0.08874656907593778,"present ExID, a domain knowledge-based approach to improve performance in data-scarce scenarios.
92"
PROBLEM SETTING AND METHODOLOGY,0.08966148215919488,"4
Problem Setting and Methodology
93"
PROBLEM SETTING AND METHODOLOGY,0.09057639524245197,"In our problem setting, the RL agent learns the policy on a limited dataset with rare and unseen
94"
PROBLEM SETTING AND METHODOLOGY,0.09149130832570906,"demonstrations. We define the characteristics of this dataset as follows:
95"
PROBLEM SETTING AND METHODOLOGY,0.09240622140896615,"Definition 4.1. A reduced buffer Br is a proper subset of the full dataset B i.e., Br ⊂B satisfying
96"
PROBLEM SETTING AND METHODOLOGY,0.09332113449222323,"the following conditions:
97"
PROBLEM SETTING AND METHODOLOGY,0.09423604757548033,"• Some states in B are not present in Br, i.e., ∃s′ ∈B ∧∀(s, a, s′) : (s, a, s′) /∈Br
98"
PROBLEM SETTING AND METHODOLOGY,0.09515096065873742,"• The number of samples N(s, a, s′) for some transitions in B are less in Br i.e, ∃(s, a, s′) ∈
99"
PROBLEM SETTING AND METHODOLOGY,0.09606587374199452,"B : N(s, a, s′)Br < N(s, a, s′)B
100"
PROBLEM SETTING AND METHODOLOGY,0.0969807868252516,"We observe, performing Q −Learning by sampling from a limited buffer Br may not converge
101"
PROBLEM SETTING AND METHODOLOGY,0.09789569990850869,"to an optimal policy for the MDP MB representing the full buffer. This can be shown as a special
102"
PROBLEM SETTING AND METHODOLOGY,0.09881061299176579,"case of (Theorem 1,[13]) as pB(s′|s, a) ̸= pBr(s′|s, a) and no Q updates for (s, a) /∈Br leading to
103"
PROBLEM SETTING AND METHODOLOGY,0.09972552607502287,"sub-optimal policy. Please refer to the App. B for analysis and example.
104"
PROBLEM SETTING AND METHODOLOGY,0.10064043915827996,"We also assume a set of common sense rules in the form of domain knowledge is available. Domain
105"
PROBLEM SETTING AND METHODOLOGY,0.10155535224153706,"knowledge D is defined as hierarchical decision nodes capturing S →A as represented by Eq. 2.
106"
PROBLEM SETTING AND METHODOLOGY,0.10247026532479414,"Each decision node Tηi is represented by a constraint ϕηi and Boolean indicator µηi function selects
107"
PROBLEM SETTING AND METHODOLOGY,0.10338517840805124,"the branch to be traversed based on ϕηi.
108"
PROBLEM SETTING AND METHODOLOGY,0.10430009149130832,"Action =

aηi
if leaf
µηiTηi↙(s) + (1 −µηi)Tηi↘(s)
o/w"
PROBLEM SETTING AND METHODOLOGY,0.10521500457456541,"µηi(s) =

1
if s |= ϕηi
0
o/w
(2)"
PROBLEM SETTING AND METHODOLOGY,0.10612991765782251,"Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain
knowledge and synthetic data (b) Updating the offline RL critic network with teacher network"
PROBLEM SETTING AND METHODOLOGY,0.1070448307410796,"We assume that D gives heuristically reasonable actions for s |= D and SD ∩SBr ̸= ∅where SD, SBr
109"
PROBLEM SETTING AND METHODOLOGY,0.10795974382433669,"are the state coverage of D and Br.
110"
PROBLEM SETTING AND METHODOLOGY,0.10887465690759378,"Training Teacher: An overview of our methodology is depicted in Fig 2. We first construct a
111"
PROBLEM SETTING AND METHODOLOGY,0.10978956999085086,"trainable actor network πω
t parameterized by ω from D, Fig 2 step 1. For training πω
t synthetic
112"
PROBLEM SETTING AND METHODOLOGY,0.11070448307410796,"data ˆS is generated by sampling states from a uniform random distribution over state boundaries
113"
PROBLEM SETTING AND METHODOLOGY,0.11161939615736505,"B(s), ˆS = U(B(S)). Note that this does not represent the true state distribution and may have state
114"
PROBLEM SETTING AND METHODOLOGY,0.11253430924062215,"combinations that will never occur. We train πω
t using behavior cloning where state ˆs ∼ˆS is checked
115"
PROBLEM SETTING AND METHODOLOGY,0.11344922232387923,"with root decision node in Eq. 2. A random action is chosen if ˆs does not satisfy decision node Tη0
116"
PROBLEM SETTING AND METHODOLOGY,0.11436413540713632,"or leaf action is absent. If ˆs satisfies a Tηi, Tηi is traversed and action aηi is returned from the leaf
117"
PROBLEM SETTING AND METHODOLOGY,0.11527904849039342,"node. This is illustrated in Fig 2 (a). We term the pre-trained actor network πω
t as the teacher policy.
118"
PROBLEM SETTING AND METHODOLOGY,0.1161939615736505,"Regularizing Critic: We now introduce Algo 1 (App C) to train an offline RL agent on Br. Algo 1
119"
PROBLEM SETTING AND METHODOLOGY,0.1171088746569076,"takes Br and pretrained πω
t as input. The algorithm uses two hyper-parameters, warm start parameter
120"
PROBLEM SETTING AND METHODOLOGY,0.11802378774016468,"k and mixing parameter λ. A critic network Qθ
s with Monte-Carlo (MC) dropout and target network
121"
PROBLEM SETTING AND METHODOLOGY,0.11893870082342177,"Qθ′
s are initialized. ExID is divided into two phases. In the first phase, we aim to warm start the critic
122"
PROBLEM SETTING AND METHODOLOGY,0.11985361390667887,"network Qθ
s with actions from πω
t as shown in Fig 2b( i). However, this must be done selectively
123"
PROBLEM SETTING AND METHODOLOGY,0.12076852698993595,"as the teacher’s policy is random around the states that do not satisfy domain knowledge. In each
124"
PROBLEM SETTING AND METHODOLOGY,0.12168344007319305,"iteration, we first check the states sampled from a mini-batch of Br with D. For the states which
125"
PROBLEM SETTING AND METHODOLOGY,0.12259835315645014,"satisfy D we compute the teacher action πω
t (s) and critic’s action argmaxa(Qθ
s(s, a)) and collect it
126"
PROBLEM SETTING AND METHODOLOGY,0.12351326623970722,"in lists at, as, Algo 1 lines 4-10. Our main objective is to keep actions chosen by the critic network
127"
PROBLEM SETTING AND METHODOLOGY,0.12442817932296432,"for s |= D close to the teacher’s policy. To achieve this, we introduce a regularization term:
128"
PROBLEM SETTING AND METHODOLOGY,0.1253430924062214,"Lr(θ) =
Es∼Br∧s|=D
|
{z
}
states matching domain rule"
PROBLEM SETTING AND METHODOLOGY,0.1262580054894785,"[Qθ
s(s, as) −Qθ
s(s, at)]2
|
{z
}
Q regularizer (3)"
PROBLEM SETTING AND METHODOLOGY,0.12717291857273558,"Eq 3 incentivizes the critic to increase Q values for actions from πω
t and decreases Q values for other
129"
PROBLEM SETTING AND METHODOLOGY,0.1280878316559927,"actions when argmaxa(Qθ
s(s, a)) ̸= πω
t (s) for states that satisfy domain knowledge. Note that Eq 3
130"
PROBLEM SETTING AND METHODOLOGY,0.12900274473924978,"will only be 0 when argmaxa(Qθ
s(s, a)) = πω
t (s) for s |= D. It is also set to 0 for s ̸|= D. However,
131"
PROBLEM SETTING AND METHODOLOGY,0.12991765782250686,"since πω
t mimicking heuristic rules is sub-optimal, it is also important to incorporate learning from
132"
PROBLEM SETTING AND METHODOLOGY,0.13083257090576395,"the data. The final loss is a combination of Eq. 1 and Eq. 3 with a mixing parameter λ ∈[0, 1]:
133"
PROBLEM SETTING AND METHODOLOGY,0.13174748398902103,"L(θ) = Lcql(θ) + λEs∼Br∧s|=D[Qθ
s(s, as) −Qθ
s(s, at)]2
(4)"
PROBLEM SETTING AND METHODOLOGY,0.13266239707227814,"The choice of λ and the warm start parameter k depends on the quality of D. In the case of perfect
134"
PROBLEM SETTING AND METHODOLOGY,0.13357731015553523,"domain knowledge, λ would be set to 1, and setting λ to 0 would lead to the vanilla CQL loss. Mixing
135"
PROBLEM SETTING AND METHODOLOGY,0.1344922232387923,"both the losses allows the critic to learn both from the data in Br and knowledge in D.
136"
PROBLEM SETTING AND METHODOLOGY,0.1354071363220494,"Updating Teacher: Given a reasonable warm start, the critic is expected to give higher Q values
137"
PROBLEM SETTING AND METHODOLOGY,0.13632204940530648,"for optimal actions for s ∈D ∩Br as it learns from data. We aim to leverage this knowledge
138"
PROBLEM SETTING AND METHODOLOGY,0.1372369624885636,"to enhance the initial teacher policy πω
t trained on heuristic domain knowledge. For s ∼B and
139"
PROBLEM SETTING AND METHODOLOGY,0.13815187557182068,"s |= D, we calculate the average Q values over critic actions and teacher actions and check which
140"
PROBLEM SETTING AND METHODOLOGY,0.13906678865507777,"one is higher in Algo 1 line 11 which refers to Cond. 6. For brevity Es∼Br∧s|=D is written as E.
141"
PROBLEM SETTING AND METHODOLOGY,0.13998170173833485,"If E(Qθ
s(s, as)) > E(Qθ
s(s, at)) it denotes the critic expects a better return on an average over its
142"
PROBLEM SETTING AND METHODOLOGY,0.14089661482159194,"own policy than the teacher’s policy. Hence, we can use the critic’s policy to update πω
t , making
143"
PROBLEM SETTING AND METHODOLOGY,0.14181152790484905,"it better over D. However, only checking the critic’s value can be erroneous as the critic can have
144"
PROBLEM SETTING AND METHODOLOGY,0.14272644098810613,"high values for OOD actions. We check the average uncertainty of the predicted Q values to prevent
145"
PROBLEM SETTING AND METHODOLOGY,0.14364135407136322,"the teacher from getting updated by OOD actions. Uncertainty has been shown to be a good metric
146"
PROBLEM SETTING AND METHODOLOGY,0.1445562671546203,"for OOD action detection by [42, 3]. A well-established methodology to capture uncertainty is
147"
PROBLEM SETTING AND METHODOLOGY,0.1454711802378774,"predictive variance, which takes inspiration from Bayesian formulation for the critic function and
148"
PROBLEM SETTING AND METHODOLOGY,0.1463860933211345,"aims to maximize p(θ|X, Y ) = p(Y |X, θ)p(θ)/p(Y |X) where X = (s, a) and Y represents the true
149"
PROBLEM SETTING AND METHODOLOGY,0.1473010064043916,"Q value of the states. However, p(Y |X) is generally intractable and is approximated using Monte
150"
PROBLEM SETTING AND METHODOLOGY,0.14821591948764867,"Carlo (MC) dropout, which involves including dropout before every layer of the critic network and
151"
PROBLEM SETTING AND METHODOLOGY,0.14913083257090576,"using it during inference [14]. Following [42], we measure the uncertainty of prediction using Eq 5.
152"
PROBLEM SETTING AND METHODOLOGY,0.15004574565416284,"V arT [Q(s, a)] ≈1 T T
X"
PROBLEM SETTING AND METHODOLOGY,0.15096065873741996,"t=1
[Q(s, a) −¯Q(s, a)]2
(5)"
PROBLEM SETTING AND METHODOLOGY,0.15187557182067704,"Eq 5 estimates the variance of Q value Q(s, a) for an action a using T forward passes on the Qθ
s(s, a)
153"
PROBLEM SETTING AND METHODOLOGY,0.15279048490393413,"with dropout where ¯Q(s, a) represents the predictive mean. We check the average uncertainty of
154"
PROBLEM SETTING AND METHODOLOGY,0.1537053979871912,"the Q value for action chosen by the critic and teacher policy over the states that match domain
155"
PROBLEM SETTING AND METHODOLOGY,0.1546203110704483,"knowledge in a batch. The teacher network is updated using the critic’s action only when the policy
156"
PROBLEM SETTING AND METHODOLOGY,0.1555352241537054,"expects a higher average Q return on its action and the average uncertainty of taking this action is
157"
PROBLEM SETTING AND METHODOLOGY,0.1564501372369625,"lower than the teacher action. E(V arT Qθ
s(sr, as)) < E(V arT Qθ
s(sr, at)) indicates the actions were
158"
PROBLEM SETTING AND METHODOLOGY,0.15736505032021958,"learned from the expert data in the buffer and are not OOD samples. The condition is summarized in
159"
PROBLEM SETTING AND METHODOLOGY,0.15827996340347666,"cond. 6:
160"
PROBLEM SETTING AND METHODOLOGY,0.15919487648673375,"E(Qθ
s(sr, as)) > E(Qθ
s(sr, at))∧"
PROBLEM SETTING AND METHODOLOGY,0.16010978956999086,"E(V arT Qθ
s(sr, as)) < E(V arT Qθ
s(sr, at))
(6)
We update the teacher with cross-entropy described in Eq 7:
161"
PROBLEM SETTING AND METHODOLOGY,0.16102470265324795,"L(ω) = −
X"
PROBLEM SETTING AND METHODOLOGY,0.16193961573650503,"s|=D
(πω
t (s)log(πs(s)))
(7)"
PROBLEM SETTING AND METHODOLOGY,0.16285452881976212,"where, πs(s, a) =
eQ(s,a)
P"
PROBLEM SETTING AND METHODOLOGY,0.1637694419030192,"a′ Q(s,a′). When the critic’s policy is better than the teacher’s policy, Lr(θ) is
162"
PROBLEM SETTING AND METHODOLOGY,0.16468435498627632,"set to 0 Algo 1 Lines 11 to 13. Finally, the critic network is updated using calculated loss L(θ) Algo
163"
PROBLEM SETTING AND METHODOLOGY,0.1655992680695334,"1 Lines 17-18. We theoretically analyse the implications of using ExID in propositions 4.2 and 4.3.
164"
PROBLEM SETTING AND METHODOLOGY,0.16651418115279049,"Proposition 4.2. Denote ˆπ as the policy learned by ExID, πu as any offline RL policy learned on Br
165"
PROBLEM SETTING AND METHODOLOGY,0.16742909423604757,"and optimal Q function as Q∗and V function as V ∗. Then it holds that
166"
PROBLEM SETTING AND METHODOLOGY,0.16834400731930466,"η(ˆπ) −η(πu) ≥Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −¯ρˆπα"
PROBLEM SETTING AND METHODOLOGY,0.16925892040256177,"Where α = Es∼O[V ∗(s) −Q∗(s, ˆπ(s))], ¯ρπ(s) = [
1
|Sˆπ|(1−γ),
1
1−γ ] (| Sˆπ | is the number of different
167"
PROBLEM SETTING AND METHODOLOGY,0.17017383348581885,"states observed by ˆπ) and O /∈Br. Here α denotes the quality of regularized action for s /∈Br. Hence,
168"
PROBLEM SETTING AND METHODOLOGY,0.17108874656907594,"updating πω
t is important as high divergence of action from the optimal can lead to performance
169"
PROBLEM SETTING AND METHODOLOGY,0.17200365965233302,"degradation. In offline RL, the extrapolation error for non optimal action is usually high for states not
170"
PROBLEM SETTING AND METHODOLOGY,0.1729185727355901,"observed in dataset (as illustrated in 1b), regularization can lead performance improvement when πω
t
171"
PROBLEM SETTING AND METHODOLOGY,0.17383348581884722,"is reasonable. Furthermore, in ExID coarse actions from πω
t are updated driving them closer to the
172"
PROBLEM SETTING AND METHODOLOGY,0.1747483989021043,"optimal actions, improving the performance lower bound. Additionally πω
t increases | Sˆπ | making
173"
PROBLEM SETTING AND METHODOLOGY,0.1756633119853614,"¯ρπ ≪1 in practice further improving the performance lower bound. Proof is deferred to App. A.
174"
PROBLEM SETTING AND METHODOLOGY,0.17657822506861848,"Proposition 4.3. ExID reduces generalization error if Q∗(s, πω
t (s)) > Q∗(s, πu(s)) for s ∈D ∩Br.
175"
PROBLEM SETTING AND METHODOLOGY,0.17749313815187556,"Proof is deferred to App. A. In the next section, we discuss our empirical evaluations.
176"
EMPIRICAL EVALUATIONS,0.17840805123513268,"5
Empirical Evaluations
177"
EMPIRICAL EVALUATIONS,0.17932296431838976,"We investigate the following through our empirical evaluations: 1. Does ExID perform better than
178"
EMPIRICAL EVALUATIONS,0.18023787740164685,"combining D and offline RL algos on different environments with datasets exhibiting rare and OOD
179"
EMPIRICAL EVALUATIONS,0.18115279048490393,"states Sec 5.2? 2. Does ExID generalize to OOD states covered by D Sec 5.4? 3. What is the effect of
180"
EMPIRICAL EVALUATIONS,0.18206770356816102,"varying k, λ and updating πω
t Sec 5.5? 4. How does performance vary with the quality of D Sec 5.6?
181"
EXPERIMENTAL SETTING,0.18298261665141813,"5.1
Experimental Setting
182"
EXPERIMENTAL SETTING,0.1838975297346752,"We evaluate our methodology on open-AI gym [5], MiniGrid [6] and real sales promotion (SP) [30]
183"
EXPERIMENTAL SETTING,0.1848124428179323,"offline data sets. All our data sets are generated using standard methodologies defined in [32, 31]
184"
EXPERIMENTAL SETTING,0.18572735590118938,"except SP which is generated by human operators. All experiments have been conducted on a
185"
EXPERIMENTAL SETTING,0.18664226898444647,"Ubuntu 22.04.2 LTS system with 1 NVIDIA K80 GPU, 4 CPUs, and 61GiB RAM. App. F notes the
186"
EXPERIMENTAL SETTING,0.18755718206770358,"hyperparameter values and network architectures.
187"
EXPERIMENTAL SETTING,0.18847209515096067,"Dataset: We experiment on three types of data sets. Expert Data-set [10, 16, 22] generated using
188"
EXPERIMENTAL SETTING,0.18938700823421775,"an optimal policy without any exploration with high trajectory quality but low state action coverage.
189"
EXPERIMENTAL SETTING,0.19030192131747484,"Replay Data-set [2, 13] generated from a policy while training it online, exhibiting a mixture of
190"
EXPERIMENTAL SETTING,0.19121683440073192,"multiple behavioral policies with high trajectory quality and state action coverage. Noisy Data-set
191"
EXPERIMENTAL SETTING,0.19213174748398903,"[12, 13, 22, 16] generated using an optimal policy that also selects random actions with ϵ greedy
192"
EXPERIMENTAL SETTING,0.19304666056724612,"strategy where ϵ = 0.2 having low trajectory quality and high state action coverage. Additionally we
193"
EXPERIMENTAL SETTING,0.1939615736505032,"also experiment on human generated dataset for sales promotion task.
194"
EXPERIMENTAL SETTING,0.1948764867337603,"Baselines: We do comparative studies on 10 baselines for OpenAI gym datasets. The first baseline
195"
EXPERIMENTAL SETTING,0.19579139981701738,"simply checks the conditions of D and applies corresponding actions in execution. The performance
196"
EXPERIMENTAL SETTING,0.19670631290027446,"of this baseline shows that D is imperfect and does not achieve the optimal reward. CQL SE is
197"
EXPERIMENTAL SETTING,0.19762122598353157,"from [40] where the expert is replaced by D. The other baselines are an ensemble of D and eight
198"
EXPERIMENTAL SETTING,0.19853613906678866,"algorithms popular in the Offline RL literature for discrete environments. These algorithms include
199"
EXPERIMENTAL SETTING,0.19945105215004574,"Behavior Cloning (BC) [29], Behaviour Value Estimation (BVE) [16], Quantile Regression DQN
200"
EXPERIMENTAL SETTING,0.20036596523330283,"(QRDQN) [7], REM, MCE, BCQ, CQL and Critic Regularized Regression Q-Learning (CRR) [41].
201"
EXPERIMENTAL SETTING,0.2012808783165599,"For a fair comparison, we use actions from domain knowledge for states not in the buffer and actions
202"
EXPERIMENTAL SETTING,0.20219579139981703,"from the trained policy for other states to obtain the final reward. Hence, each algorithm is renamed
203"
EXPERIMENTAL SETTING,0.2031107044830741,"with the suffix D in Table 5.1.
204"
EXPERIMENTAL SETTING,0.2040256175663312,"Limiting Data: To create limited-data settings for benchmark datasets, we first extract a small
205"
EXPERIMENTAL SETTING,0.20494053064958828,"percentage of samples from the full dataset and remove some of the samples based on state conditions.
206"
EXPERIMENTAL SETTING,0.20585544373284537,"This is done to ensure the reduced buffer satisfies the conditions defined in Def 4.1. We describe
207"
EXPERIMENTAL SETTING,0.20677035681610248,"the specific conditions of removal in the next section. Further insights and the state visualizations
208"
EXPERIMENTAL SETTING,0.20768526989935956,"for selected reduced datasets are in App H. Note : no data reduction has been performed on SP
209"
EXPERIMENTAL SETTING,0.20860018298261665,"dataset to demonstrate a real dataset exhibits characteristics of reduced buffer.
210"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.20951509606587373,"5.2
Performance across Different Datasets
211"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21043000914913082,"Our results for OpenAI gym environments are summarised in Table 5.1 and Minigrid in Table 3 (App
212"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21134492223238793,"D). We observe the performance of offline RL algorithms degrades substantially when part of the data
213"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21225983531564502,"is not seen and trajectory ratios change. For these cases with only 10% partial data, ExID surpasses
214"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2131747483989021,"the performance by at least 27% in the presence of reasonable domain knowledge. The proposed
215"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2140896614821592,"method performs strongest on the replay dataset where the contribution of Lr(θ) is significant due
216"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21500457456541627,"to state coverage, and the teacher learns from high-quality trajectories. Environment details are
217"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21591948764867339,"described in the App. D. All domain knowledge trees are shown in the App. D Fig 10. We describe
218"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21683440073193047,"limiting data conditions and domain knowledge specific to the environment as follows:
219"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21774931381518756,"Mountain Car Environment: [27] We use simple, intuitive domain knowledge in this environment
220"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21866422689844464,"shown in the App. D Fig 10 (c), which represents taking a left action when the car is at the bottom of
221"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.21957913998170173,"the valley with low velocity to gain momentum; otherwise, taking the right action to drive the car up.
222"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22049405306495884,"Fig 6 (c) shows the state action pairs this rule generates on states sampled from a uniform random
223"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22140896614821592,"distribution over the state boundaries. It can be observed that the states of D cover part of the missing
224"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.222323879231473,Table 1: Average reward [↑] obtained during online evaluation over 3 seeds on openAI gym envs
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2232387923147301,"ENV
DATA
DATA
TYPE
D
QRDQN
D
REM
D
BVE
D
CRR
D
MCE
D
BC
D
BCQ
D
CQL
D
CQL
SE
CQL
(FULL)
EXID
(OURS)"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22415370539798718,"MOUNTAIN
CAR"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2250686184812443,EXPERT
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22598353156450138,"-159.9
±
52.28"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22689844464775846,"-168.2
±
33.71"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22781335773101555,"-147.7
±
21.54"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22872827081427263,"-175.36
±
25.16"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.22964318389752975,"-157.2
±
39.09"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23055809698078683,"-152
±
37.41"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23147301006404392,"-181.38
±
28.60"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.232387923147301,"-172.9
±
27.5"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23330283623055809,"-167.49
±
12.3"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2342177493138152,"-161.33
±
18.57"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23513266239707228,"-128.63
±
10.94"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23604757548032937,"-125.5
±
2.60"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23696248856358645,"REPLAY
-137.14
±
39.27"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23787740164684354,"-136.26
±
40.15"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23879231473010065,"-152.0
±
35.06"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.23970722781335774,"-137.23
±
42.79"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24062214089661482,"-139.91
±
40.01"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2415370539798719,"-137.26
±
43.04"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.242451967063129,"-136.29
±
36.15"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2433668801463861,"-140.38
±
33.58"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2442817932296432,"-150.67
±
16.68"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24519670631290028,"-135.4
±
3.74"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24611161939615736,"-105.79
±
11.38"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24702653247941445,"NOISY
-141.61
±
33.04"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24794144556267156,"-134.99
±
32.60"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24885635864592864,"-173.95
±
39.60"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.24977127172918573,"-178.99
±
23.58"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2506861848124428,"-168.69
±
38.78"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2516010978956999,"-140.0
±
28.5"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.252516010978957,"-144.52
±
43.04"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.25343092406221407,"-179.8
±
29.99"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.25434583714547115,"-126.96
±
17.84"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2552607502287283,"-107.06
±
12.73"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2561756633119854,"-109.9
±
13.45"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.25709057639524246,"CART
POLE"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.25800548947849955,EXPERT
PERFORMANCE ACROSS DIFFERENT DATASETS,0.25892040256175664,"57.0
±
5.35"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2598353156450137,"33.23
±
3.17"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2607502287282708,"41.31
±
8.76"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2616651418115279,"16.16
±
9.41"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.262580054894785,"15.24
±
5.62"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.26349496797804206,"16.1
±
4.4"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2644098810612992,"225.76
±
74.39"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2653247941445563,"165.36
±
15.01"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.26623970722781337,"121.8
±
14.0"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.26715462031107046,"155.78
±
26.47"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.26806953339432754,"364.1
±
22.15"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2689844464775846,"307.18
±
137.72"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2698993595608417,"REPLAY
149.09
±
14.05"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2708142726440988,"180.70
±
62.79"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2717291857273559,"11.1
±
2.13"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.27264409881061297,"11.24
±
2.71"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2735590118938701,"9.16
±
0.25"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2744739249771272,"144.43
±
2.41"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2753888380603843,"144.76
±
6.04"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.27630375114364136,"131.97
±
23.23"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.27721866422689845,"113.37
±
5.88"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.27813357731015553,"250.02
±
55.02"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2790484903934126,"340.26
±
30.58"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2799634034766697,"NOISY
161
±
6.40"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2808783165599268,"15.33
±
0.58"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2817932296431839,"11.53
±
3.77"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.282708142726441,"13.68
±
7.49"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2836230558096981,"10.66
±
2.04"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2845379688929552,"68.4
±
14.67"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.28545288197621227,"63.53
±
14.08"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.28636779505946935,"92.6
±
22.05"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.28728270814272644,"92.6
±
22.05"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2881976212259835,"93.72
±
37.79"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2891125343092406,"228.61
±
38.64"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2900274473924977,"LUNAR
LANDER"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2909423604757548,EXPERT
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2918572735590119,"52.48
±
26.51"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.292772186642269,"5.14
±
25.10"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2936870997255261,"-184.84
±
26.45"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2946020128087832,"-681.67
±
34.86"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.29551692589204026,"8.79
±
25.38"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.29643183897529735,"19.71
±
10.52"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.29734675205855443,"38.40
±
23.21"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2982616651418115,"-45.99
±
30.47"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.2991765782250686,"65.43
±
71.37"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3000914913083257,"53.22
±
78.85"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3010064043915828,"167.74
±
29.4"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3019213174748399,"161.34
±
17.10"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.302836230558097,"REPLAY
-444.20
±
12.20"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3037511436413541,"-556.81
±
21.39"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.30466605672461117,"-572
±
27.93"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.30558096980786825,"-131.21
±
31.97"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.30649588289112534,"-115.23
±
18.16"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3074107959743824,"136.63
±
12.40"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3083257090576395,"111.47
±
54.67"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3092406221408966,"61.83
±
45.57"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3101555352241537,"87.70
±
18.20"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3110704483074108,"187.72
±
25.62"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3119853613906679,"156.03
±
56.67"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.312900274473925,"NOISY
-4.81
±
97.28"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3138151875571821,"21.41
±
14.71"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.31473010064043916,"28.65
±
12.26"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.31564501372369624,"-158.27
±
7.71"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.31655992680695333,"-50.47
±
15.78"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3174748398902104,"98.62
±
28.01"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3183897529734675,"101.59
±
30.83"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3193046660567246,"5.01
±
128.63"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3202195791399817,"40.35
±
65.72"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3211344922232388,"111
±
52.32"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3220494053064959,"163.57
±
49.24"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.322964318389753,"Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online
evaluation (c) Evaluation curves for the sales promotion dataset"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.32387923147301007,"data in Fig 1 (a). For limiting datasets, we remove states with position > -0.8. The performance of
225"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.32479414455626715,"CQLD and ExID are shown in Fig 3 (a),(b) where ExID surpasses CQLD for all three datasets.
226"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.32570905763952424,"Cart-pole Environment: For this environment, we use domain knowledge from [33], which aims to
227"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3266239707227813,"move in the direction opposite to the lean of the pole, keeping the cart close enough to the center. If
228"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3275388838060384,"the cart is close to an edge, the domain knowledge attempts to account for the cart’s velocity and
229"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3284537968892955,"recenter the cart. The full tree is given in the App. D Fig 10 (a). We remove states with cart velocity
230"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.32936870997255263,"> -1.5 to create the reduced buffer.
231"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3302836230558097,"Lunar-Lander Environment: We borrow the decision nodes from [34] and get actions from a
232"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3311985361390668,"sub-optimal policy trained online with an average reward of 52.48. The full set of decision nodes is
233"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3321134492223239,"shown in the App. D Fig 10 (b). D focuses on keeping the lander balanced when the lander is above
234"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.33302836230558097,"ground. When the lander is near the surface, D focuses on keeping the y velocity lower. To create the
235"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.33394327538883806,"reduced datasets, we remove data of lander angle < -0.04.
236"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.33485818847209514,"Mini-Grid Environments: For our experiments, we choose two environments: Random Dynamic
237"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3357731015553522,"Obstacles 6X6 and LavaGapS 7X7. We use intuitive domain knowledge which avoids crashing into
238"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3366880146386093,"obstacles in front, left, or right of agent ref. App. D Fig 10 (d), (e). We remove states with obstacles
239"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3376029277218664,"on the right for creating limited data settings. Due to limitation of space we report the results of the
240"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.33851784080512354,"best-performing algorithms on the replay dataset in Table 3 (App D).
241"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3394327538883806,"5.3
Case study on real human generated Sales Promotion (SP) dataset
242"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3403476669716377,"SP dataset and environment [30] simulates a real-world sales promotion platform. The number of
243"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3412625800548948,"coupons and the discount the user received will affect his behavior. A higher discount will promote
244"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3421774931381519,"the sales, but the cost will also increase. The goal for the platform operator is to maximize the
245"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.34309240622140896,"total profit. The horizon of the dataset is 50 days for the training and 30 days for the test. Domain
246"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.34400731930466605,"knowledge ([30], App A] : Active users can be given more coupons with lower discount to maximize
247"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.34492223238792313,"profit. We model this as ordernumber > 60∧Avgfee > 0.8 =⇒[5, 0.95] where action 1 is number
248"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3458371454711802,"of coupons range [0,5] and action 2 is coupon value (discount value = (1-coupon value)) range
249"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3467520585544373,"[0.6,0.95]. The dataset exhibits the properties in Def 4.1 as first 50 days of sales does not contain
250"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.34766697163769444,"many active users as reported in the coverage column of Tab 2 depicting scarcity. The domain rule is
251"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.34858188472095153,"imperfect as coupon value and number depend on multiple factors such as user purchase history and
252"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3494967978042086,"behavior. As illustrated in the table 2 and Fig 3 (c) the intuitive domain rule enhances performance
253"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3504117108874657,"by 10.49% in the real dataset.
254"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3513266239707228,Table 2: Results on human generated Sales Promotion dataset
PERFORMANCE ACROSS DIFFERENT DATASETS,0.35224153705397987,"Dataset
D
coverage D
CQL + D
CQLSE
EXID
Performance
gain"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.35315645013723695,"Sales
Promotion
654.68
± 20.06
20.32%
722.06 ± 71.40
727.03 ±
49.56
802.91
± 41.69
10.49%"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.35407136322049404,"5.4
Generalization to OOD states and contribution of Lr(θ)
255"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3549862763037511,"Figure 4: Q value difference between CQL and EXID for expert and policy action on states not
present in the buffer for a) expert b) noisy in log scale c) contribution of Lr(θ)"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3559011893870082,"In Fig 4 (a), (b), we plot Qθ
s(s, aexpert)−Qθ
s(s, aθ) for CQL and EXID policies for different datasets
256"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.35681610247026535,"of Mountain-Car environments. Action aexpert is obtained from the full expert dataset where position
257"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.35773101555352244,"> −0.8. We observe that the Q value for actions of CQL policy diverges from the expert policy
258"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3586459286367795,"actions with high values for the states not in the reduced buffer, whereas ExID stays close to the
259"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3595608417200366,"expert actions for the unseen states. This empirically shows generalization to OOD states not in the
260"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3604757548032937,"dataset but covered by domain knowledge. In Fig 4 (d), we plot the contribution by Lr(θ) during the
261"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3613906678865508,"training and observe the contribution is higher for replay data sets with more state coverage.
262"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.36230558096980786,"5.5
Performance on varying λ, k, and ablation of πω
t
263"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.36322049405306495,"We study the effect of varying λ on the algorithm for the given domain knowledge. We empirically
264"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.36413540713632203,"observe setting a high or a low λ can yield sub-optimal performance, and λ = 0.5 generally gives
265"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3650503202195791,"good performance. In Fig 5 (a), we show this effect for LunarLander. Plots for other environments
266"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.36596523330283626,"are in the App. G Fig 11. For k we observe setting the warm start parameter to 0 yields a sub-optimal
267"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.36688014638609334,"policy, as the critic may update πω
t without completely learning from it. The starting performance
268"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3677950594693504,"increases with an increase in k as shown in Fig 5 (b) for LunarLander. k = 30 works best according
269"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3687099725526075,"to empirical evaluations. Plots for other environments are in the App. G Fig 12. We show two
270"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3696248856358646,"ablations for Cart-pole in Fig 5 (c) with no teacher update after the warm start and no inclusion of
271"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.3705397987191217,"Lr(θ) after the warm start. The warm start in this environment is set to 30 episodes. Fig 5 c) shows
272"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.37145471180237877,"without teacher updated, the sub-optimal teacher drags down the performance of the policy beyond
273"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.37236962488563585,"the warm start, exhibiting the necessity of πω
t update. Also, the student converges to a sub-optimal
274"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.37328453796889294,"policy if no Lr(θ) is included beyond the warm start.
275"
PERFORMANCE ACROSS DIFFERENT DATASETS,0.37419945105215,"Figure 5: (a) Effect of different λ on the performance of ExID on Lunar Lander (b) Effect of different
k on the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no
teacher update, and just warm start on Cart-pole."
PERFORMANCE ACROSS DIFFERENT DATASETS,0.37511436413540716,"Figure 6: (a) D with different average rewards (b) Performance effect on Lunar-lander (c) State
distribution generated for training the teacher network for mountain-car"
EFFECT OF VARYING D QUALITY,0.37602927721866425,"5.6
Effect of varying D quality
276"
EFFECT OF VARYING D QUALITY,0.37694419030192133,"We show the effect of choosing policies as D with different average rewards for Lunar-Lander expert
277"
EFFECT OF VARYING D QUALITY,0.3778591033851784,"data in Fig 6 (a) and (b). Rule 1 is optimal and has almost the same effect as Rule 3, which is the D
278"
EFFECT OF VARYING D QUALITY,0.3787740164684355,"used in our experiments exhibiting that updating a sub-optimal D can lead to equivalent performance
279"
EFFECT OF VARYING D QUALITY,0.3796889295516926,"as optimal D. Using a rule with high uncertainty, as Rule 2, induces high uncertainty in the learned
280"
EFFECT OF VARYING D QUALITY,0.3806038426349497,"policy but performs slightly better than the baseline. Rule 4, which has a lower average reward, also
281"
EFFECT OF VARYING D QUALITY,0.38151875571820676,"causes gains on average performance with slower convergence. Finally, Rule 5, with very bad actions,
282"
EFFECT OF VARYING D QUALITY,0.38243366880146384,"affects policy performance adversely and leads to a performance lower than baseline CQL.
283"
CONCLUSION AND LIMITATION,0.38334858188472093,"6
Conclusion and Limitation
284"
CONCLUSION AND LIMITATION,0.38426349496797807,"In this paper, we study the effect of limited and partial data on offline RL and observe that the
285"
CONCLUSION AND LIMITATION,0.38517840805123515,"performance of SOTA offline RL algorithms is sub-optimal in such settings. The paper proposes a
286"
CONCLUSION AND LIMITATION,0.38609332113449224,"methodology to handle offline RL’s performance degradation using domain insights. We incorporate
287"
CONCLUSION AND LIMITATION,0.3870082342177493,"a regularization loss in the CQL training using a teacher policy and refine the initial teacher policy
288"
CONCLUSION AND LIMITATION,0.3879231473010064,"while training. We show that incorporating reasonable domain knowledge in offline RL enhances
289"
CONCLUSION AND LIMITATION,0.3888380603842635,"performance, achieving a performance close to full data. However, this method is limited by the
290"
CONCLUSION AND LIMITATION,0.3897529734675206,"quality of the domain knowledge and the overlap between domain knowledge states and reduced
291"
CONCLUSION AND LIMITATION,0.39066788655077767,"buffer data. The study is also limited to discrete domains. In the future, the authors would like to
292"
CONCLUSION AND LIMITATION,0.39158279963403475,"improve on capturing domain knowledge into the policy network without dependence on data and
293"
CONCLUSION AND LIMITATION,0.39249771271729184,"extending the methodology to algorithms that handle continuous action space.
294"
BROADER IMPACT,0.3934126258005489,"7
Broader Impact
295"
BROADER IMPACT,0.39432753888380606,"During the trial-and-error training phase, RL agents may exhibit irrational behavior, which can be
296"
BROADER IMPACT,0.39524245196706315,"risky and costly in real-world scenarios. As a more practical alternative to online RL, offline RL
297"
BROADER IMPACT,0.39615736505032023,"utilizes pre-existing collected data to eliminate the need for real-time interactions during training.
298"
BROADER IMPACT,0.3970722781335773,"However, a drawback of offline RL is its dependence on the quality and quantity of historical data,
299"
BROADER IMPACT,0.3979871912168344,"which, when sub-optimal, could adversely affect overall performance. Therefore, through this work,
300"
BROADER IMPACT,0.3989021043000915,"we use domain knowledge to suppress erroneous actions when available data is limited. However, this
301"
BROADER IMPACT,0.39981701738334857,"inclusion may facilitate harmful behavior in the presence of biased domain knowledge. Therefore,
302"
BROADER IMPACT,0.40073193046660566,"we advocate the use of well-regulated domain knowledge obtained from experts. Beyond this, we do
303"
BROADER IMPACT,0.40164684354986274,"not foresee any ethical impact on our work.
304"
REFERENCES,0.4025617566331198,"References
305"
REFERENCES,0.40347666971637697,"[1] A survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on
306"
REFERENCES,0.40439158279963405,"Neural Networks and Learning Systems, 2023. ISSN 21622388. doi: 10.1109/TNNLS.2023.3250269.
307"
REFERENCES,0.40530649588289114,"[2] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
308"
REFERENCES,0.4062214089661482,"reinforcement learning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020.
309"
REFERENCES,0.4071363220494053,"[3] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement
310"
REFERENCES,0.4080512351326624,"learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436–7447,
311"
REFERENCES,0.4089661482159195,"2021.
312"
REFERENCES,0.40988106129917656,"[4] Peter L. Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement learning in
313"
REFERENCES,0.41079597438243365,"weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
314"
REFERENCES,0.41171088746569073,"Intelligence, UAI ’09, page 35–42, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958.
315"
REFERENCES,0.4126258005489479,"[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
316"
REFERENCES,0.41354071363220496,"Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
317"
REFERENCES,0.41445562671546204,"[6] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou,
318"
REFERENCES,0.41537053979871913,"Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable
319"
REFERENCES,0.4162854528819762,"reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.
320"
REFERENCES,0.4172003659652333,"[7] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforcement learning
321"
REFERENCES,0.4181152790484904,"with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,
322"
REFERENCES,0.41903019213174747,"2018.
323"
REFERENCES,0.41994510521500455,"[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
324"
REFERENCES,0.42086001829826164,"tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
325"
REFERENCES,0.4217749313815188,"[9] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal
326"
REFERENCES,0.42268984446477587,"of Machine Learning Research, 6, 2005.
327"
REFERENCES,0.42360475754803295,"[10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
328"
REFERENCES,0.42451967063129004,"data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
329"
REFERENCES,0.4254345837145471,"[11] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances
330"
REFERENCES,0.4263494967978042,"in neural information processing systems, 34:20132–20145, 2021.
331"
REFERENCES,0.4272644098810613,"[12] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep
332"
REFERENCES,0.4281793229643184,"reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.
333"
REFERENCES,0.42909423604757546,"[13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo-
334"
REFERENCES,0.43000914913083255,"ration. In International conference on machine learning, pages 2052–2062. PMLR, 2019.
335"
REFERENCES,0.4309240622140897,"[14] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural
336"
REFERENCES,0.43183897529734677,"networks. Advances in neural information processing systems, 29, 2016.
337"
REFERENCES,0.43275388838060386,"[15] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline rl by blending
338"
REFERENCES,0.43366880146386094,"heuristics. arXiv preprint arXiv:2306.00321, 2023.
339"
REFERENCES,0.434583714547118,"[16] Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad
340"
REFERENCES,0.4354986276303751,"Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior
341"
REFERENCES,0.4364135407136322,"value estimation. arXiv preprint arXiv:2103.09575, 2021.
342"
REFERENCES,0.4373284537968893,"[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
343"
REFERENCES,0.43824336688014637,"preprint arXiv:1503.02531, 2015.
344"
REFERENCES,0.43915827996340345,"[18] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks
345"
REFERENCES,0.4400731930466606,"with logic rules. arXiv preprint arXiv:1603.06318, 2016.
346"
REFERENCES,0.4409881061299177,"[19] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
347"
REFERENCES,0.44190301921317476,"Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–274, 2002.
348"
REFERENCES,0.44281793229643185,"[20] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with
349"
REFERENCES,0.44373284537968893,"fisher divergence critic regularization. In International Conference on Machine Learning, pages 5774–5783.
350"
REFERENCES,0.444647758462946,"PMLR, 2021.
351"
REFERENCES,0.4455626715462031,"[21] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning
352"
REFERENCES,0.4464775846294602,"via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.
353"
REFERENCES,0.4473924977127173,"[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
354"
REFERENCES,0.44830741079597436,"reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.
355"
REFERENCES,0.4492223238792315,"[23] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu.
Offline reinforcement learning: Tutorial,
356"
REFERENCES,0.4501372369624886,"review, and perspectives on open problems.
ArXiv, abs/2005.01643, 2020.
URL https://api.
357"
REFERENCES,0.45105215004574567,"semanticscholar.org/CorpusID:218486979.
358"
REFERENCES,0.45196706312900276,"[24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
359"
REFERENCES,0.45288197621225984,"review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
360"
REFERENCES,0.4537968892955169,"[25] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng.
361"
REFERENCES,0.454711802378774,"Reinforcement learning for clinical decision support in critical care: comprehensive review. Journal of
362"
REFERENCES,0.4556267154620311,"medical Internet research, 22(7):e18477, 2020.
363"
REFERENCES,0.4565416285452882,"[26] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
364"
REFERENCES,0.45745654162854527,"Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
365"
REFERENCES,0.4583714547118024,"deep reinforcement learning. nature, 518(7540):529–533, 2015.
366"
REFERENCES,0.4592863677950595,"[27] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University
367"
REFERENCES,0.4602012808783166,"of Cambridge, Computer Laboratory, 1990.
368"
REFERENCES,0.46111619396157366,"[28] Susan A Murphy. A generalization error for q-learning. 2005.
369"
REFERENCES,0.46203110704483075,"[29] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural
370"
REFERENCES,0.46294602012808783,"computation, 3(1):88–97, 1991.
371"
REFERENCES,0.4638609332113449,"[30] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu.
372"
REFERENCES,0.464775846294602,"Neorl: A near real-world benchmark for offline reinforcement learning. Advances in Neural Information
373"
REFERENCES,0.4656907593778591,"Processing Systems, 35:24753–24765, 2022.
374"
REFERENCES,0.46660567246111617,"[31] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling,
375"
REFERENCES,0.46752058554437326,"Vihang Prakash Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on offline
376"
REFERENCES,0.4684354986276304,"reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021. URL https://openreview.net/
377"
REFERENCES,0.4693504117108875,"forum?id=A4EWtf-TO3Y.
378"
REFERENCES,0.47026532479414457,"[32] Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vihang Prakash
379"
REFERENCES,0.47118023787740165,"Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A dataset perspective on offline
380"
REFERENCES,0.47209515096065874,"reinforcement learning. In Conference on Lifelong Learning Agents, pages 470–517. PMLR, 2022.
381"
REFERENCES,0.4730100640439158,"[33] Andrew Silva and Matthew Gombolay. Encoding human domain knowledge to warm start reinforcement
382"
REFERENCES,0.4739249771271729,"learning. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 5042–5050,
383"
REFERENCES,0.47483989021043,"2021.
384"
REFERENCES,0.4757548032936871,"[34] Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization
385"
REFERENCES,0.47666971637694416,"methods for interpretable differentiable decision trees applied to reinforcement learning. In International
386"
REFERENCES,0.4775846294602013,"conference on artificial intelligence and statistics, pages 1855–1865. PMLR, 2020.
387"
REFERENCES,0.4784995425434584,"[35] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for offline
388"
REFERENCES,0.4794144556267155,"reinforcement learning in robotics. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors,
389"
REFERENCES,0.48032936870997256,"Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning
390"
REFERENCES,0.48124428179322964,"Research, pages 907–917. PMLR, 08–11 Nov 2022.
391"
REFERENCES,0.48215919487648673,"[36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple
392"
REFERENCES,0.4830741079597438,"semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.
393"
REFERENCES,0.4839890210430009,"[37] Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance
394"
REFERENCES,0.484903934126258,"for recommender system. In Proceedings of the 24th ACM SIGKDD international conference on knowledge
395"
REFERENCES,0.48581884720951507,"discovery & data mining, pages 2289–2298, 2018.
396"
REFERENCES,0.4867337602927722,"[38] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific
397"
REFERENCES,0.4876486733760293,"knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019.
398"
REFERENCES,0.4885635864592864,"[39] Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent
399"
REFERENCES,0.48947849954254347,"reinforcement learning with knowledge distillation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
400"
REFERENCES,0.49039341262580055,"K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages
401"
REFERENCES,0.49130832570905764,"226–237. Curran Associates, Inc., 2022.
402"
REFERENCES,0.4922232387923147,"[40] Richa Verma, Durgesh Kalwar, Harshad Khadilkar, and Balaraman Ravindran. Guiding offline reinforce-
403"
REFERENCES,0.4931381518755718,"ment learning using a safety expert. In Proceedings of the 7th Joint International Conference on Data
404"
REFERENCES,0.4940530649588289,"Science & Management of Data (11th ACM IKDD CODS and 29th COMAD), pages 82–90, 2024.
405"
REFERENCES,0.494967978042086,"[41] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed,
406"
REFERENCES,0.4958828911253431,"Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression.
407"
REFERENCES,0.4967978042086002,"Advances in Neural Information Processing Systems, 33:7768–7778, 2020.
408"
REFERENCES,0.4977127172918573,"[42] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and
409"
REFERENCES,0.4986276303751144,"Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In International Confer-
410"
REFERENCES,0.49954254345837146,"ence on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:234763307.
411"
REFERENCES,0.5004574565416285,"[43] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves
412"
REFERENCES,0.5013723696248856,"imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern
413"
REFERENCES,0.5022872827081427,"recognition, pages 10687–10698, 2020.
414"
REFERENCES,0.5032021957913998,"[44] Qisen Yang, Shenzhi Wang, Matthieu Gaetan Lin, Shiji Song, and Gao Huang. Boosting offline reinforce-
415"
REFERENCES,0.5041171088746569,"ment learning with action preference query. In International Conference on Machine Learning, pages
416"
REFERENCES,0.505032021957914,"39509–39523. PMLR, 2023.
417"
REFERENCES,0.505946935041171,"[45] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label
418"
REFERENCES,0.5068618481244281,"smoothing regularization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition
419"
REFERENCES,0.5077767612076852,"(CVPR), pages 3902–3910, 2020. doi: 10.1109/CVPR42600.2020.00396.
420"
REFERENCES,0.5086916742909423,"[46] Xiaoxuan Zhang and S Zhang Y Yu. Domain knowledge guided offline q learning. In Second Offline
421"
REFERENCES,0.5096065873741995,"Reinforcement Learning Workshop at Neurips, volume 2021, 2021.
422"
REFERENCES,0.5105215004574566,"[47] Ying Zheng, Haoyu Chen, Qingyang Duan, Lixiang Lin, Yiyang Shao, Wei Wang, Xin Wang, and
423"
REFERENCES,0.5114364135407137,"Yuedong Xu. Leveraging domain knowledge for robust deep reinforcement learning in networking.
424"
REFERENCES,0.5123513266239708,"In IEEE INFOCOM 2021 - IEEE Conference on Computer Communications, pages 1–10, 2021. doi:
425"
REFERENCES,0.5132662397072278,"10.1109/INFOCOM42981.2021.9488863.
426"
REFERENCES,0.5141811527904849,"A
Theoretical Analysis
427"
REFERENCES,0.515096065873742,"Notations
428"
REFERENCES,0.5160109789569991,"For any deterministic policy π the performance return is formulated as η(π) = Eτ∼π[P∞
t=0 γtr(st, at)]
429"
REFERENCES,0.5169258920402562,"For any policy π, ρπ is the (unormalized) discounted visitation frequency given by ρπ(s) = P∞
t=0 γtP(st = s)
430"
REFERENCES,0.5178408051235133,"where s0 ∼ρ0(s0) and the trajectory (s0, s1, . . . ) is sampled from the policy π and ρπ(s) ∈[0,
1
1−γ ].
431"
REFERENCES,0.5187557182067704,"¯ρπ(s) = sup{ρπ(s), s ∈S} ∈[
1
|Sπ|(1−γ),
1
(1−γ)]
432"
REFERENCES,0.5196706312900274,"We denote the regularized policy learned by ExID on Br as ˆπ and the unregularized policy as πu.
433"
REFERENCES,0.5205855443732845,"Lemmas
434"
REFERENCES,0.5215004574565416,"We introduce the following Lemma required for our theoretical analysis.
435"
REFERENCES,0.5224153705397987,"Lemma A.1. ([44]) Given two policies π1 and π2
436"
REFERENCES,0.5233302836230558,"η(π1) −η(π2) =
Z"
REFERENCES,0.5242451967063129,"s∈S
ρπ1(s)(Q∗(s, π1(s) −V ∗(s))ds −
Z"
REFERENCES,0.52516010978957,"s∈S
ρπ2(s)(Q∗(s, π2(s) −V ∗(s))ds"
REFERENCES,0.526075022872827,"Proof. Please refer to Lemma A.1 Eq 17 in [44]
437"
REFERENCES,0.5269899359560841,"Proposition A.2. (4.2) Denote ˆπ as the policy learned by ExID, πu as any offline RL policy learned on Br and
438"
REFERENCES,0.5279048490393413,"optimal Q function as Q∗and V function as V ∗. Then it holds that
439"
REFERENCES,0.5288197621225984,"η(ˆπ) −η(πu) ≥Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −¯ρˆπα"
REFERENCES,0.5297346752058555,"Proof. According to [19] performance improvement between two policies if given by
440"
REFERENCES,0.5306495882891126,"η(π1) = η(π2) + Eτ∼π1 "" ∞
X"
REFERENCES,0.5315645013723697,"t=0
γtQπ2(st, at) −Vπ2(st) # (8)"
REFERENCES,0.5324794144556267,"Replacing π1 by ˆπ and π2 by πu and by following Lemma A.1
441"
REFERENCES,0.5333943275388838,"η(ˆπ) −η(πu) =
Z"
REFERENCES,0.5343092406221409,"s∈S
ρˆπ(s)(Q∗(s, ˆπ(s)) −V ∗(s))ds −
Z"
REFERENCES,0.535224153705398,"s∈S
ρπu(s)(Q∗(s, πu(s)) −V ∗(s))ds
(9) =
Z"
REFERENCES,0.5361390667886551,"s∈S
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5370539798719122,"s∈S
ρˆπ(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
(10)"
REFERENCES,0.5379688929551693,"Dividing the state space into in dataset domain states (I) and OOD states (O). The
442"
REFERENCES,0.5388838060384263,"(11)
Z"
REFERENCES,0.5397987191216834,"s∈I
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5407136322049405,"s∈I
ρˆπ(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
"
REFERENCES,0.5416285452881976,"|
{z
}
a + Z"
REFERENCES,0.5425434583714547,"s∈O
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5434583714547118,"s∈O
ρˆπ(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
"
REFERENCES,0.5443732845379688,"|
{z
}
b (12)"
REFERENCES,0.5452881976212259,"Since the regularization loss facilitates visitation to OOD states via knowledge distillation we assume
ρˆπ = ρπu −∆i for s ∈i and ρˆπ = ρπu + ∆o for s ∈o where ∆i ∈[0, ρπu(s)] and ∆o ∈[0,
1
1−γ −ρπu(s)]
443 a =
Z"
REFERENCES,0.546203110704483,"s∈I
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5471180237877402,"s∈I
(ρπu −∆i)(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
(13) =
Z"
REFERENCES,0.5480329368709973,"s∈I
ρπu(s)(Q∗(s, ˆπ(s)) −Q∗(s, πu(s)))ds +
Z"
REFERENCES,0.5489478499542544,"s∈I
∆i(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
(14)"
REFERENCES,0.5498627630375115,"Under assumption in distribution action can be learned from the dataset due to conservatism of offline RL
(Q∗(s, ˆπ(s)) −Q∗(s, πu(s))) ≈0, a ≥0
444 b =
Z"
REFERENCES,0.5507776761207686,"s∈O
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5516925892040256,"s∈O
(ρπu + ∆o)(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
(15) ≥
Z"
REFERENCES,0.5526075022872827,"s∈O
ρπu(s)(V ∗(s) −Q∗(s, πu(s)))ds −
Z"
REFERENCES,0.5535224153705398,"s∈O
ρˆπ(s)(V ∗(s) −Q∗(s, ˆπ(s)))ds
(16)"
REFERENCES,0.5544373284537969,"≥Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −Es∼O|ˆπ[V ∗(s) −Q∗(s, ˆπ(s))]
(17)"
REFERENCES,0.555352241537054,"Further loosening the lower bound
445"
REFERENCES,0.5562671546203111,"= Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −¯ρˆπ
Z s∈O ρˆπ"
REFERENCES,0.5571820677035682,"¯ρˆπ (V ∗(s) −Q∗(s, ˆπ(s)))ds
(18)"
REFERENCES,0.5580969807868252,"≥Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −¯ρˆπ
Z"
REFERENCES,0.5590118938700823,"s∈O
(V ∗(s) −Q∗(s, ˆπ(s)))ds
(19)"
REFERENCES,0.5599268069533394,"Combining Eq 14, 17 and 19, and denoting α = Es∼O[V ∗(s) −Q∗(s, ˆπ(s))]
446"
REFERENCES,0.5608417200365965,"η(ˆπ) −η(πu) ≥Es∼O|πu[V ∗(s) −Q∗(s, πu(s))] −¯ρˆπα
(20)"
REFERENCES,0.5617566331198536,"Hence, Proposition 4.2 follows Q.E.D
447 448"
REFERENCES,0.5626715462031107,"Proposition A.3. (4.3) Algo 1 reduces generalization error if Q∗(s, πω
t (s)) > Q∗(s, π(s)) for s ∈D ∩Br,
449"
REFERENCES,0.5635864592863677,"where π is vanilla offline RL policy learnt on Br.
450"
REFERENCES,0.5645013723696248,"Proof. Generalization error for any policy π as defined by [28] can be written as:
451"
REFERENCES,0.565416285452882,"Gπ = V ∗(s0) −Vπ(s0) = −Eτ∼π[ T
X"
REFERENCES,0.5663311985361391,"t=0
γtQ∗(st, π(st)) −V ∗(st)]
(21)"
REFERENCES,0.5672461116193962,"Here, Eτ∼π represents sampling trajectories with policy π. Since the state space is continuous, we can represent
the expectation as an integral over the state space
452 = − T
X"
REFERENCES,0.5681610247026533,"t=0
γt
Z"
REFERENCES,0.5690759377859104,"s∈S
P(st = s|π)(Q∗(st, π(st)) −V ∗(st))ds
(22) = −
Z s∈S T
X"
REFERENCES,0.5699908508691675,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st))ds
(23)"
REFERENCES,0.5709057639524245,"Analysing with respect to s ∈D ∩Br we can break the integration into two parts
453 = − ""Z s∈S/D T
X"
REFERENCES,0.5718206770356816,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st))ds +
Z s∈D T
X"
REFERENCES,0.5727355901189387,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st)) # (24) = − """
REFERENCES,0.5736505032021958,"f(s|π) +
Z s∈D T
X"
REFERENCES,0.5745654162854529,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st)) # (25)"
REFERENCES,0.57548032936871,"For a policy ˆπ learnt in Algo 1 the action for st = s ∈D is regularized to be close to πω
t which either follows
domain knowledge or expert demonstrations. Hence, it is reasonable to assume Q∗(st, πω
t (st)) > Q∗(st, π(st)).
It follows
454 Z s∈D T
X"
REFERENCES,0.576395242451967,"t=0
γtP(st = s|ˆπ)(Q∗(st, ˆπ(st)) −V ∗(st)) <
Z s∈D T
X"
REFERENCES,0.5773101555352241,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st)) (26)"
REFERENCES,0.5782250686184812,"Note for s /∈D, f(s|ˆπ) ≈f(s|π). This is because the regularization term assigns max Q value to a different
action for s ∈D but maxa(Q(s, a)) remains same
455 ∴− """
REFERENCES,0.5791399817017383,"f(s|ˆπ) +
Z s∈D T
X"
REFERENCES,0.5800548947849954,"t=0
γtP(st = s|ˆπ)(Q∗(st, ˆπ(st)) −V ∗(st)) # < − """
REFERENCES,0.5809698078682525,"f(s|π) +
Z s∈D T
X"
REFERENCES,0.5818847209515096,"t=0
γtP(st = s|π)(Q∗(st, π(st)) −V ∗(st)) # (27)"
REFERENCES,0.5827996340347666,"Hence, Gˆπ < Gπ Proposition 2 follows Q.E.D
456 457"
REFERENCES,0.5837145471180238,"B
Missing Examples
458"
REFERENCES,0.5846294602012809,"Performing Q −Learning by sampling from a reduced batch Br may not converge to an optimal policy for the
459"
REFERENCES,0.585544373284538,"MDP MB representing the full buffer.
460"
REFERENCES,0.5864592863677951,"Example (Theorem 1,[13]) defines MDP MB of B from same state action space of the original MDP M with
461"
REFERENCES,0.5873741994510522,"transition probabilities pB(s′|s, a) =
N(s,a,s′)
P
˜s N(s,a,˜s) where N(s, a, s′) is the number of times (s, a, s′) occurs in B
462"
REFERENCES,0.5882891125343093,"and an terminal state sinit. It states pB(sinit|s, a) = 1 when P"
REFERENCES,0.5892040256175664,"˜s N(s, a, ˜s) = 0. This happens when transitions
463"
REFERENCES,0.5901189387008234,"of some s′ of (s, a, s′) are missing from the buffer, which may occur in Br when Br ⊂B. r(sinit, s, a) is
464"
REFERENCES,0.5910338517840805,"initialized to Q(s, a). We assume that a policy learned on reduced dataset Br converges to optimal value function
465"
REFERENCES,0.5919487648673376,"and disprove it using the following counterexample:
466"
REFERENCES,0.5928636779505947,"Figure 7: Example MDP, sampled buffer MDP and reduced buffer with Q tables"
REFERENCES,0.5937785910338518,"Figure 8: We hypothesize the suboptimal perfor-
mance of offline RL for limited data can be ad-
dressed via domain knowledge via action regular-
ization and knowledge distillation."
REFERENCES,0.5946935041171089,"We take a simple MDP illustrated in Fig 7 with 3
467"
REFERENCES,0.595608417200366,"states and 2 actions (0,1). The reward of each ac-
468"
REFERENCES,0.596523330283623,"tion is marked along the transition. The sampled
469"
REFERENCES,0.5974382433668801,"MDP is constructed the following samples (1,0,2)-
470"
REFERENCES,0.5983531564501372,"2,(1,1,2)-3, (2,0,3)-3, and (2,1,3)-2 and the reduced
471"
REFERENCES,0.5992680695333943,"buffer MDP with samples (1,0,2)-2 and (1,1,2)-1.
472"
REFERENCES,0.6001829826166514,"The probabilities are marked along the transition.
473"
REFERENCES,0.6010978956999085,"It is easy to see that the policy learned under the
474"
REFERENCES,0.6020128087831657,"reduced MDP converges to a nonoptimal policy af-
475"
REFERENCES,0.6029277218664227,"ter one step of the Q table update with Q(s, a) =
476"
REFERENCES,0.6038426349496798,"r(s, a) + p(s′|s, a) ∗maxa′(Q(s′, a′)). This hap-
477"
REFERENCES,0.6047575480329369,"pens because of transition probability shift on reduc-
478"
REFERENCES,0.605672461116194,"ing samples pB(s′|s, a) ̸= pBr(s′|s, a) and no Q
479"
REFERENCES,0.6065873741994511,"updates for (s, a) /∈Br.
480"
REFERENCES,0.6075022872827082,"Our methodology addresses these issues as follows:
481"
REFERENCES,0.6084172003659652,"• For s ∈D ∩Br better actions are enforced
482"
REFERENCES,0.6093321134492223,"through regularization using πω
t even when
483"
REFERENCES,0.6102470265324794,"the transition probabilities are low for op-
484"
REFERENCES,0.6111619396157365,"timal transitions.
485"
REFERENCES,0.6120768526989936,"• Incorporating regularization distills the
486"
REFERENCES,0.6129917657822507,"teacher’s knowledge in the critic-enhancing
487"
REFERENCES,0.6139066788655078,"generalization.
488"
REFERENCES,0.6148215919487648,"A visualization is shown in Fig 8.
489"
REFERENCES,0.6157365050320219,"C
Algorithm
490"
REFERENCES,0.616651418115279,"The pseudo code of the algorithm is described in Algo 1.
491"
REFERENCES,0.6175663311985361,Algorithm 1 Pseudo code for EXID
REFERENCES,0.6184812442817932,"1: Input: Reduced buffer Br, Initial teacher network πω
t , Training steps N, Warm-up steps k, Soft
update τ, hyperparameters: λ, α
2: Initialize Critic with MC dropout and Target Critic Qθ
s, Qθ′
s
3: for n ←1 to N do
4:
Sample mini-batch b of transitions (s, a, r, s′) ∼Br at = [], as = [], sr = []
5:
for s ∈b do
6:
if s |= D and πω
t (s) ̸= argmaxa(Qθ
s(s, a)) then
7:
at.append(πω
t (s))
8:
as.append(argmaxa(Qθ
s(s, a)))
9:
sr.append(s)
10:
end if
11:
end for
12:
if n > k∧Cond. 6 then
13:
Update πω
t (s) using Eq 7
14:
Lr(θ) = 0
15:
else
16:
Calculate Lr(θ) using Eq 3
17:
end if
18:
Calculate L(θ) using Eq 4
19:
Update Qθ
s with L(θ) and softy update Qθ′
s and τ
20: end for"
REFERENCES,0.6193961573650503,"D
Environments and Domain Knowledge Trees
492"
REFERENCES,0.6203110704483074,"Figure 9: Graphical visualizations of environments used in the experiments. These environments are
a) MountainCar-v0 b) CartPole-v1 c) LunarLander-v2 d) MiniGrid-LavaGapS7-v0 e) MiniGrid-
Dynamic-Obstacles-Random-6x6-v0"
REFERENCES,0.6212259835315646,"The graphical visualization of each environment is depicted in Fig 9. The choice of environment in this paper
493"
REFERENCES,0.6221408966148216,"depended on two factors: a) Pre-existing standard methods of generating offline RL datasets. b) Possibility of
494"
REFERENCES,0.6230558096980787,"creating intuitive decision tree-based domain knowledge. All datasets have been created via [31]. We explain the
495"
REFERENCES,0.6239707227813358,"environments in detail as follows:
496"
REFERENCES,0.6248856358645929,"Mountain-car Environment: This environment Fig 9 a) has two state variables, position and velocity, and three
497"
REFERENCES,0.62580054894785,"discrete actions: left push, right push, and no action [27]. The goal is to drive a car up a valley to reach the flag.
498"
REFERENCES,0.6267154620311071,"This environment is challenging for offline RL because of sparse rewards, which are only obtained on reaching
499"
REFERENCES,0.6276303751143641,"the flag.
500"
REFERENCES,0.6285452881976212,"Cart-pole Environment The environment Fig 9 b) has 4 states and 2 actions representing left force and right
501"
REFERENCES,0.6294602012808783,"force. The objective is to balance a pole on a moving cart.
502"
REFERENCES,0.6303751143641354,"Lunar-Lander Environment: The task is to land a lunar rover between two flags Fig 9 c) by observing 8 states
503"
REFERENCES,0.6312900274473925,"and applying one of 4 actions.
504"
REFERENCES,0.6322049405306496,"Minigrid Environments: Mini-grid [6] is an environment suite containing 2D grid-worlds with goal oriented
505"
REFERENCES,0.6331198536139067,"tasks. As explained in the main text, we experiment using MiniGrid-LavaGapS7-v0 and MiniGrid-Dynamic-
506"
REFERENCES,0.6340347666971637,"Obstacles-Random-6x6-v0 from this environment suite is shown in Fig 9 d) and e). In MiniGrid-LavaGapS7-v0,
507"
REFERENCES,0.6349496797804208,"the agent has to avoid Lava and pass through the gap to reach the goal. Dynamic obstacles are similar; however,
508"
REFERENCES,0.6358645928636779,"the agent can start at a random position and has to avoid dynamically moving balls to reach the goal. The
509"
REFERENCES,0.636779505946935,"environment has image observation with 3 channels (OBJECT_ID, COLOR_ID, STATE). Following [31]
510"
REFERENCES,0.6376944190301921,"experiments, we flatten the image to an array of 98 observations and restrict action space to three actions: Turn
511"
REFERENCES,0.6386093321134492,"left, Turn Right, and Move forward. The results of minigrid environment are reported in Table 3. Since this
512"
REFERENCES,0.6395242451967064,"environment uses a semantic map from image observation, we collect states from a fixed policy with random
513"
REFERENCES,0.6404391582799634,"actions to generate the teacher’s state distribution. CQL on the full dataset achieves the average reward of
514"
REFERENCES,0.6413540713632205,"0.92 ± 0.1 for DynamicObstacles and 0.53 ± 0.01 for LavaGapS.
515"
REFERENCES,0.6422689844464776,"The domain knowledge trees for all the environments are shown in Fig 10. The cart pole domain knowledge
516"
REFERENCES,0.6431838975297347,"tree Fig 10 a) is taken from [33] (Fig 7). The Lunar Lander decision nodes Fig 10 b) have been taken from [34]
517"
REFERENCES,0.6440988106129918,"(Fig4). For the mini-grid environments, we construct intuitive decision trees shown in Fig 10 d) and Fig 10 e).
518"
REFERENCES,0.6450137236962489,"Positions 52, 40, and 68 represent positions front, right, and left of the agent. Value 0.2 represents a wall, 0.9
519"
REFERENCES,0.645928636779506,"represents Lava, and 0.6 represents a ball. We check positions 52, 40, and 68 for these obstacles and choose the
520"
REFERENCES,0.646843549862763,"recommended actions as domain knowledge.
521"
REFERENCES,0.6477584629460201,"Figure 10: Domain knowledge trees for a) CartPole-v1 b) LunarLander-v2 c) MountainCar-v0 d)
MiniGrid-LavaGapS7-v0 e) MiniGrid-Dynamic-Obstacles-Random-6x6-v0 environments"
REFERENCES,0.6486733760292772,"E
Related Work: Knowledge Distillation
522"
REFERENCES,0.6495882891125343,"Knowledge distillation is a well-embraced technique of incorporating additional information in neural networks
523"
REFERENCES,0.6505032021957914,"and has been applied to various fields like computer vision [43, 36], natural language processing [8, 38], and
524"
REFERENCES,0.6514181152790485,"recommendation systems [37]. [17] introduced the concept of distilling knowledge from a complex, pre-trained
525"
REFERENCES,0.6523330283623056,"model (teacher) into a smaller model (student). In recent years, researchers have explored the integration
526"
REFERENCES,0.6532479414455626,"of rule-based regularization techniques within the context of knowledge distillation. Rule regularization
527"
REFERENCES,0.6541628545288197,"introduces additional constraints based on predefined rules, guiding the learning process of the student model
528"
REFERENCES,0.6550777676120768,Table 3: Average reward [↑] obtained during online evaluation over 3 seeds on Minigrid environments
REFERENCES,0.6559926806953339,"ENVIRONMENT
D
BC
D
BCQ
D
CQL
D
EXID"
REFERENCES,0.656907593778591,"MINIGRID
DYNAMIC
RANDOM6X6"
REFERENCES,0.6578225068618482,"0.50
±
0.08"
REFERENCES,0.6587374199451053,"0.59
±
0.07"
REFERENCES,0.6596523330283623,"0.24
±
0.22"
REFERENCES,0.6605672461116194,"0.14
±
0.1"
REFERENCES,0.6614821591948765,"0.79
±
0.07"
REFERENCES,0.6623970722781336,"MINIGRID
LAVAGAPS
7X7"
REFERENCES,0.6633119853613907,"0.27
±
0.09"
REFERENCES,0.6642268984446478,"0.29
±
0.11"
REFERENCES,0.6651418115279049,"0.26
±
0.1"
REFERENCES,0.6660567246111619,"0.28
±
0.12"
REFERENCES,0.666971637694419,"0.46
±
0.13"
REFERENCES,0.6678865507776761,"[18, 45]. These techniques have shown to reduce overfitting and enhance generalization [38]. Knowledge
529"
REFERENCES,0.6688014638609332,"distillation is also prevalent in the field of RL [47] and offline RL [39]. Contrary to prevalent teacher-student
530"
REFERENCES,0.6697163769441903,"knowledge distillation techniques, our work does not enforce parameter sharing among the networks. Through
531"
REFERENCES,0.6706312900274474,"experiments, we demonstrate that a simple regularization loss and expected performance-based updates can
532"
REFERENCES,0.6715462031107045,"improve generalization to unobserved states covered by domain knowledge. There are also no constraints on
533"
REFERENCES,0.6724611161939615,"keeping the same network structure for the teacher, paving ways for capturing the domain knowledge into more
534"
REFERENCES,0.6733760292772186,"structured networks such as Differentiable Decision Trees (DDTs).
535"
REFERENCES,0.6742909423604757,"F
Network Architecture and Hyper-parameters
536"
REFERENCES,0.6752058554437328,"We follow the network architecture and hyper-parameters proposed by [31] for all our networks, including the
537"
REFERENCES,0.67612076852699,"baseline networks. The teacher BC network πt
ω and Critic network Qθ
s(s, a) consists of 3 linear layers, each
538"
REFERENCES,0.6770356816102471,"having a hidden size of 256 neurons. The number of input and output neurons depends on the environment’s state
539"
REFERENCES,0.6779505946935042,"and action size. All layers except the last are SELU activation functions; the final layer uses linear activation.
540"
REFERENCES,0.6788655077767612,"πt
ω uses a softmax activation function in the last layer for producing action probabilities. A learning rate of
541"
REFERENCES,0.6797804208600183,"0.0001 with batch size 32 and α = 0.1 is used for all environments. MC dropout probability of 0.5 and
542"
REFERENCES,0.6806953339432754,"number of stochastic passes T=10 have been used for the critic network. The uncertainty check is performed
543"
REFERENCES,0.6816102470265325,"every 15 episodes after the warm start to avoid computational overhead. The hyper-parameters specific to our
544"
REFERENCES,0.6825251601097896,"algorithm for OpenAI gym are reported in Table F. The hyper-parameters specific to our algorithm for Minigrid
545"
REFERENCES,0.6834400731930467,"environments are reported in Table 5.
546"
REFERENCES,0.6843549862763038,Table 4: Hyperparameters for openAI gym environments
REFERENCES,0.6852698993595608,"HYPERPARAM
MOUNTAINCAR
CARTPOLE
LUNAR-
LANDER"
REFERENCES,0.6861848124428179,"DATA TYPE
EXPERT REPLAY
NOISY
EXPERT REPLAY NOISY
EXPERT REPLAY
NOISY"
REFERENCES,0.687099725526075,"λ
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5"
REFERENCES,0.6880146386093321,"k
30
30
30
30
30
30
30
30
30"
REFERENCES,0.6889295516925892,"πt
ω LR
1e5
1e5
1e5
1e2
1e2
1e2
1e4
1e4
1e4"
REFERENCES,0.6898444647758463,"TRAINING
STEPS
42000
36000
36000
30000
17000
17000
18000
18000
18000"
REFERENCES,0.6907593778591034,Table 5: Hyper-parameters for Mini-grid environments for replay dataset
REFERENCES,0.6916742909423604,"Environment
DynamicObstRandom6x6-
v0
LavaGapS7v0"
REFERENCES,0.6925892040256175,"λ
0.1
0.1
k
30
30"
REFERENCES,0.6935041171088746,"πt
ω lr
1e4
1e4"
REFERENCES,0.6944190301921317,"training steps
5000
10000"
REFERENCES,0.6953339432753889,Figure 11: Effect of λ on the performance of ExID for different environments expert datasets.
REFERENCES,0.696248856358646,"G
Effect of k and λ and Evaluation Plots
547"
REFERENCES,0.6971637694419031,"We empirically evaluate the effect of λ In Fig 11 and k in Fig 12. We believe these parameters depend on the
548"
REFERENCES,0.6980786825251601,"quality of D. For the given D in the environments we empirically observe, λ = 0.5 generally performs well,
549"
REFERENCES,0.6989935956084172,"except for Minigrid environments where λ = 0.1 works better. Increasing the warm start parameter k generally
550"
REFERENCES,0.6999085086916743,"increases the initial performance of the policy, allowing it to learn from the teacher. Meanwhile, no warm start
551"
REFERENCES,0.7008234217749314,"adversely affects policy performance as the critic may erroneously update the teacher. From empirical evaluation,
552"
REFERENCES,0.7017383348581885,"we observe that k = 30 gives a reasonable start to the policy. All the evaluation plots are shown in Fig 13, where
553"
REFERENCES,0.7026532479414456,"it can be observed that ExID performs better than baseline CQL.
554"
REFERENCES,0.7035681610247027,Figure 12: Effect of k on the performance of ExID for different environments expert datasets.
REFERENCES,0.7044830741079597,"H
Data reduction design and data distribution visualization of reduced
555"
REFERENCES,0.7053979871912168,"dataset
556"
REFERENCES,0.7063129002744739,"In this section, we discuss the intuition behind our data-limiting choices. We also visually represent selected
557"
REFERENCES,0.707227813357731,"reduced datasets for the OpenAI gym environments.
558"
REFERENCES,0.7081427264409881,"Figure 13: Evaluation plots of CQL and EXID algorithms for Cartpole, Lunar-Lander, and Minigrid
environments using different data types and seeds reported in the main paper Table 5.1."
REFERENCES,0.7090576395242452,"Figure 14: (a) The effect of data reduction and removal on baseline CQL visualized on Mountain Car
Environment (b) Performance of ExID on removing different parts of the data based on nodes of Fig
10 (c) from Mountain Car expert dataset"
REFERENCES,0.7099725526075022,"Reducing transitions from the dataset: For all datasets, 10% of the data samples were extracted from the full
559"
REFERENCES,0.7108874656907593,"dataset. This experimental design choice is based on the observation shown in Fig 14 (a). Performance degrades
560"
REFERENCES,0.7118023787740164,"on reducing samples to 0.1% of the dataset and reduces further on reducing samples to 0.05% of the dataset.
561"
REFERENCES,0.7127172918572735,"However, this drop is not substantial. The performance also reduces on removing part of the dataset from the
562"
REFERENCES,0.7136322049405307,"full dataset with states > −0.8. However, the worst performance is observed when both samples are reduced
563"
REFERENCES,0.7145471180237878,"and data is omitted, attributing to accumulated errors from probability ratio shift contributing to an increase in
564"
REFERENCES,0.7154620311070449,"generalization error. Our methodology aims to address this gap in performance.
565"
REFERENCES,0.716376944190302,"Removing part of the state space: Due to the simplicity of the Mountain-Car environment, we analyze the
566"
REFERENCES,0.717291857273559,"Mountain-Car expert dataset to show the effect of removing data matching state conditions of the different nodes
567"
REFERENCES,0.7182067703568161,"in the decision tree in Fig 10 (c). The performance for each condition is summarised in Table 6. The most
568"
REFERENCES,0.7191216834400732,"informative node in the tree is position > −0.5; removing states matching this condition causes a performance
569"
REFERENCES,0.7200365965233303,"drop in the algorithm as the domain knowledge regularization does not contribute significant information to the
570"
REFERENCES,0.7209515096065874,"policy. Similarly, removing data with velocity < 0.01 causes a performance drop. However, both performances
571"
REFERENCES,0.7218664226898445,"are higher than the baseline CQL trained on reduced data. Based on this observation, we choose state removal
572"
REFERENCES,0.7227813357731016,"conditions that preserve states matching part of the information in the tree such that the regularization term
573"
REFERENCES,0.7236962488563586,"contributes substantially to the policy. Fig 15 shows the data distribution plot of 10% samples extracted from
574"
REFERENCES,0.7246111619396157,"mountain car replay and noisy data with states > −0.8 removed. Fig 16 shows visualizations for 10% samples
575"
REFERENCES,0.7255260750228728,"extracted from expert data with velocity > −1.5 removed. Fig 17 shows visualizations for 10% samples
576"
REFERENCES,0.7264409881061299,"extracted from expert data with lander angle < −0.04 removed.
577"
REFERENCES,0.727355901189387,"Table 6: Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c)
from Mountain Car expert dataset"
REFERENCES,0.7282708142726441,"Position>-0.5
Position<-0.5
Velocity>0.01
Velocity<0.01"
REFERENCES,0.7291857273559011,"-121.89 ± 7.69
-151 ± 13.6
-128.48 ± 11.84
-147.80 ± 5.01"
REFERENCES,0.7301006404391582,"Figure 15: Data distribution of reduced dataset compared to the full dataset for mountain replay and
noisy data"
REFERENCES,0.7310155535224153,Figure 16: Data distribution of reduced cart pole expert dataset compared to the full dataset
REFERENCES,0.7319304666056725,Figure 17: Data distribution of reduced LunarLander expert dataset compared to the full dataset
REFERENCES,0.7328453796889296,"NeurIPS Paper Checklist
578"
CLAIMS,0.7337602927721867,"1. Claims
579"
CLAIMS,0.7346752058554438,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
580"
CLAIMS,0.7355901189387009,"contributions and scope?
581"
CLAIMS,0.7365050320219579,"Answer: [Yes]
582"
CLAIMS,0.737419945105215,"Justification: The claims made in the paper have been experimented on different settings for validity
583"
CLAIMS,0.7383348581884721,"and generalization. Please refer to sec 5.2.
584"
CLAIMS,0.7392497712717292,"Guidelines:
585"
CLAIMS,0.7401646843549863,"• The answer NA means that the abstract and introduction do not include the claims made in the
586"
CLAIMS,0.7410795974382434,"paper.
587"
CLAIMS,0.7419945105215004,"• The abstract and/or introduction should clearly state the claims made, including the contributions
588"
CLAIMS,0.7429094236047575,"made in the paper and important assumptions and limitations. A No or NA answer to this
589"
CLAIMS,0.7438243366880146,"question will not be perceived well by the reviewers.
590"
CLAIMS,0.7447392497712717,"• The claims made should match theoretical and experimental results, and reflect how much the
591"
CLAIMS,0.7456541628545288,"results can be expected to generalize to other settings.
592"
CLAIMS,0.7465690759377859,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
593"
CLAIMS,0.747483989021043,"attained by the paper.
594"
LIMITATIONS,0.7483989021043,"2. Limitations
595"
LIMITATIONS,0.7493138151875571,"Question: Does the paper discuss the limitations of the work performed by the authors?
596"
LIMITATIONS,0.7502287282708143,"Answer: [Yes]
597"
LIMITATIONS,0.7511436413540714,"Justification: The paper acknowledges the dependency on reasonable domain knowledge and coverage
598"
LIMITATIONS,0.7520585544373285,"please refer to sec 6
599"
LIMITATIONS,0.7529734675205856,"Guidelines:
600"
LIMITATIONS,0.7538883806038427,"• The answer NA means that the paper has no limitation while the answer No means that the paper
601"
LIMITATIONS,0.7548032936870998,"has limitations, but those are not discussed in the paper.
602"
LIMITATIONS,0.7557182067703568,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
603"
LIMITATIONS,0.7566331198536139,"• The paper should point out any strong assumptions and how robust the results are to violations of
604"
LIMITATIONS,0.757548032936871,"these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
605"
LIMITATIONS,0.7584629460201281,"asymptotic approximations only holding locally). The authors should reflect on how these
606"
LIMITATIONS,0.7593778591033852,"assumptions might be violated in practice and what the implications would be.
607"
LIMITATIONS,0.7602927721866423,"• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
608"
LIMITATIONS,0.7612076852698993,"on a few datasets or with a few runs. In general, empirical results often depend on implicit
609"
LIMITATIONS,0.7621225983531564,"assumptions, which should be articulated.
610"
LIMITATIONS,0.7630375114364135,"• The authors should reflect on the factors that influence the performance of the approach. For
611"
LIMITATIONS,0.7639524245196706,"example, a facial recognition algorithm may perform poorly when image resolution is low or
612"
LIMITATIONS,0.7648673376029277,"images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
613"
LIMITATIONS,0.7657822506861848,"closed captions for online lectures because it fails to handle technical jargon.
614"
LIMITATIONS,0.7666971637694419,"• The authors should discuss the computational efficiency of the proposed algorithms and how
615"
LIMITATIONS,0.7676120768526989,"they scale with dataset size.
616"
LIMITATIONS,0.7685269899359561,"• If applicable, the authors should discuss possible limitations of their approach to address problems
617"
LIMITATIONS,0.7694419030192132,"of privacy and fairness.
618"
LIMITATIONS,0.7703568161024703,"• While the authors might fear that complete honesty about limitations might be used by reviewers
619"
LIMITATIONS,0.7712717291857274,"as grounds for rejection, a worse outcome might be that reviewers discover limitations that
620"
LIMITATIONS,0.7721866422689845,"aren’t acknowledged in the paper. The authors should use their best judgment and recognize
621"
LIMITATIONS,0.7731015553522416,"that individual actions in favor of transparency play an important role in developing norms that
622"
LIMITATIONS,0.7740164684354987,"preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
623"
LIMITATIONS,0.7749313815187557,"honesty concerning limitations.
624"
THEORY ASSUMPTIONS AND PROOFS,0.7758462946020128,"3. Theory Assumptions and Proofs
625"
THEORY ASSUMPTIONS AND PROOFS,0.7767612076852699,"Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
626"
THEORY ASSUMPTIONS AND PROOFS,0.777676120768527,"(and correct) proof?
627"
THEORY ASSUMPTIONS AND PROOFS,0.7785910338517841,"Answer: [Yes]
628"
THEORY ASSUMPTIONS AND PROOFS,0.7795059469350412,"Justification: Please refer to App A in the supplement material for theoretical analysis and proofs.
629"
THEORY ASSUMPTIONS AND PROOFS,0.7804208600182982,"Guidelines:
630"
THEORY ASSUMPTIONS AND PROOFS,0.7813357731015553,"• The answer NA means that the paper does not include theoretical results.
631"
THEORY ASSUMPTIONS AND PROOFS,0.7822506861848124,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7831655992680695,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
633"
THEORY ASSUMPTIONS AND PROOFS,0.7840805123513266,"• The proofs can either appear in the main paper or the supplemental material, but if they appear in
634"
THEORY ASSUMPTIONS AND PROOFS,0.7849954254345837,"the supplemental material, the authors are encouraged to provide a short proof sketch to provide
635"
THEORY ASSUMPTIONS AND PROOFS,0.7859103385178408,"intuition.
636"
THEORY ASSUMPTIONS AND PROOFS,0.7868252516010978,"• Inversely, any informal proof provided in the core of the paper should be complemented by
637"
THEORY ASSUMPTIONS AND PROOFS,0.787740164684355,"formal proofs provided in appendix or supplemental material.
638"
THEORY ASSUMPTIONS AND PROOFS,0.7886550777676121,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7895699908508692,"4. Experimental Result Reproducibility
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7904849039341263,"Question: Does the paper fully disclose all the information needed to reproduce the main experimental
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7913998170173834,"results of the paper to the extent that it affects the main claims and/or conclusions of the paper
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923147301006405,"(regardless of whether the code and data are provided or not)?
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932296431838975,"Answer: [Yes]
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941445562671546,"Justification: Yes all hyper-parameters and experimental setting have been clearly listed in the paper.
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7950594693504117,"Please refer to App F and sec 5.1.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7959743824336688,"Guidelines:
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7968892955169259,"• The answer NA means that the paper does not include experiments.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.797804208600183,"• If the paper includes experiments, a No answer to this question will not be perceived well by the
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7987191216834401,"reviewers: Making the paper reproducible is important, regardless of whether the code and data
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7996340347666971,"are provided or not.
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8005489478499542,"• If the contribution is a dataset and/or model, the authors should describe the steps taken to make
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014638609332113,"their results reproducible or verifiable.
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8023787740164684,"• Depending on the contribution, reproducibility can be accomplished in various ways. For
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8032936870997255,"example, if the contribution is a novel architecture, describing the architecture fully might suffice,
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8042086001829826,"or if the contribution is a specific model and empirical evaluation, it may be necessary to either
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8051235132662397,"make it possible for others to replicate the model with the same dataset, or provide access to
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8060384263494969,"the model. In general. releasing code and data is often one good way to accomplish this, but
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8069533394327539,"reproducibility can also be provided via detailed instructions for how to replicate the results,
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.807868252516011,"access to a hosted model (e.g., in the case of a large language model), releasing of a model
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8087831655992681,"checkpoint, or other means that are appropriate to the research performed.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8096980786825252,"• While NeurIPS does not require releasing code, the conference does require all submissions
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8106129917657823,"to provide some reasonable avenue for reproducibility, which may depend on the nature of the
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8115279048490394,"contribution. For example
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8124428179322964,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how to
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8133577310155535,"reproduce that algorithm.
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8142726440988106,"(b) If the contribution is primarily a new model architecture, the paper should describe the
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8151875571820677,"architecture clearly and fully.
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8161024702653248,"(c) If the contribution is a new model (e.g., a large language model), then there should either be
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8170173833485819,"a way to access this model for reproducing the results or a way to reproduce the model (e.g.,
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.817932296431839,"with an open-source dataset or instructions for how to construct the dataset).
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.818847209515096,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors are
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8197621225983531,"welcome to describe the particular way they provide for reproducibility. In the case of
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8206770356816102,"closed-source models, it may be that access to the model is limited in some way (e.g.,
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8215919487648673,"to registered users), but it should be possible for other researchers to have some path to
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8225068618481244,"reproducing or verifying the results.
676"
OPEN ACCESS TO DATA AND CODE,0.8234217749313815,"5. Open access to data and code
677"
OPEN ACCESS TO DATA AND CODE,0.8243366880146387,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
678"
OPEN ACCESS TO DATA AND CODE,0.8252516010978957,"faithfully reproduce the main experimental results, as described in supplemental material?
679"
OPEN ACCESS TO DATA AND CODE,0.8261665141811528,"Answer: [Yes]
680"
OPEN ACCESS TO DATA AND CODE,0.8270814272644099,"Justification: The code is provided with the submission in a zip file with Readme for instructions.
681"
OPEN ACCESS TO DATA AND CODE,0.827996340347667,"Guidelines:
682"
OPEN ACCESS TO DATA AND CODE,0.8289112534309241,"• The answer NA means that paper does not include experiments requiring code.
683"
OPEN ACCESS TO DATA AND CODE,0.8298261665141812,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
684"
OPEN ACCESS TO DATA AND CODE,0.8307410795974383,"guides/CodeSubmissionPolicy) for more details.
685"
OPEN ACCESS TO DATA AND CODE,0.8316559926806953,"• While we encourage the release of code and data, we understand that this might not be possible,
686"
OPEN ACCESS TO DATA AND CODE,0.8325709057639524,"so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
687"
OPEN ACCESS TO DATA AND CODE,0.8334858188472095,"this is central to the contribution (e.g., for a new open-source benchmark).
688"
OPEN ACCESS TO DATA AND CODE,0.8344007319304666,"• The instructions should contain the exact command and environment needed to run to reproduce
689"
OPEN ACCESS TO DATA AND CODE,0.8353156450137237,"the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
690"
OPEN ACCESS TO DATA AND CODE,0.8362305580969808,"guides/CodeSubmissionPolicy) for more details.
691"
OPEN ACCESS TO DATA AND CODE,0.8371454711802379,"• The authors should provide instructions on data access and preparation, including how to access
692"
OPEN ACCESS TO DATA AND CODE,0.8380603842634949,"the raw data, preprocessed data, intermediate data, and generated data, etc.
693"
OPEN ACCESS TO DATA AND CODE,0.838975297346752,"• The authors should provide scripts to reproduce all experimental results for the new proposed
694"
OPEN ACCESS TO DATA AND CODE,0.8398902104300091,"method and baselines. If only a subset of experiments are reproducible, they should state which
695"
OPEN ACCESS TO DATA AND CODE,0.8408051235132662,"ones are omitted from the script and why.
696"
OPEN ACCESS TO DATA AND CODE,0.8417200365965233,"• At submission time, to preserve anonymity, the authors should release anonymized versions (if
697"
OPEN ACCESS TO DATA AND CODE,0.8426349496797805,"applicable).
698"
OPEN ACCESS TO DATA AND CODE,0.8435498627630376,"• Providing as much information as possible in supplemental material (appended to the paper) is
699"
OPEN ACCESS TO DATA AND CODE,0.8444647758462946,"recommended, but including URLs to data and code is permitted.
700"
OPEN ACCESS TO DATA AND CODE,0.8453796889295517,"6. Experimental Setting/Details
701"
OPEN ACCESS TO DATA AND CODE,0.8462946020128088,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
702"
OPEN ACCESS TO DATA AND CODE,0.8472095150960659,"how they were chosen, type of optimizer, etc.) necessary to understand the results?
703"
OPEN ACCESS TO DATA AND CODE,0.848124428179323,"Answer: [Yes]
704"
OPEN ACCESS TO DATA AND CODE,0.8490393412625801,"Justification: The paper uses open source code to create the dataset and lists the modifications in
705"
OPEN ACCESS TO DATA AND CODE,0.8499542543458372,"details in the main paper and supplement material. Please refer to App F and sec 5.1.
706"
OPEN ACCESS TO DATA AND CODE,0.8508691674290942,"Guidelines:
707"
OPEN ACCESS TO DATA AND CODE,0.8517840805123513,"• The answer NA means that the paper does not include experiments.
708"
OPEN ACCESS TO DATA AND CODE,0.8526989935956084,"• The experimental setting should be presented in the core of the paper to a level of detail that is
709"
OPEN ACCESS TO DATA AND CODE,0.8536139066788655,"necessary to appreciate the results and make sense of them.
710"
OPEN ACCESS TO DATA AND CODE,0.8545288197621226,"• The full details can be provided either with the code, in appendix, or as supplemental material.
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554437328453797,"7. Experiment Statistical Significance
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8563586459286368,"Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8572735590118938,"tion about the statistical significance of the experiments?
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8581884720951509,"Answer: [Yes]
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.859103385178408,"Justification: All experiments have been run on 3 random seeds and the error bounds have been
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8600182982616651,"reported in Table 5.1, Table 2 and Table 3.
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8609332113449222,"Guidelines:
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618481244281794,"• The answer NA means that the paper does not include experiments.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8627630375114365,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8636779505946935,"intervals, or statistical significance tests, at least for the experiments that support the main claims
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8645928636779506,"of the paper.
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8655077767612077,"• The factors of variability that the error bars are capturing should be clearly stated (for example,
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8664226898444648,"train/test split, initialization, random drawing of some parameter, or overall run with given
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8673376029277219,"experimental conditions).
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.868252516010979,"• The method for calculating the error bars should be explained (closed form formula, call to a
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.869167429094236,"library function, bootstrap, etc.)
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8700823421774931,"• The assumptions made should be given (e.g., Normally distributed errors).
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8709972552607502,"• It should be clear whether the error bar is the standard deviation or the standard error of the
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8719121683440073,"mean.
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8728270814272644,"• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8737419945105215,"a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8746569075937786,"not verified.
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8755718206770357,"• For asymmetric distributions, the authors should be careful not to show in tables or figures
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8764867337602927,"symmetric error bars that would yield results that are out of range (e.g. negative error rates).
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8774016468435498,"• If error bars are reported in tables or plots, The authors should explain in the text how they were
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8783165599268069,"calculated and reference the corresponding figures or tables in the text.
737"
EXPERIMENTS COMPUTE RESOURCES,0.879231473010064,"8. Experiments Compute Resources
738"
EXPERIMENTS COMPUTE RESOURCES,0.8801463860933212,"Question: For each experiment, does the paper provide sufficient information on the computer
739"
EXPERIMENTS COMPUTE RESOURCES,0.8810612991765783,"resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
740"
EXPERIMENTS COMPUTE RESOURCES,0.8819762122598354,"Answer: [Yes]
741"
EXPERIMENTS COMPUTE RESOURCES,0.8828911253430924,"Justification: Please refer to the Experimental setup section in the main paper sec 5.1.
742"
EXPERIMENTS COMPUTE RESOURCES,0.8838060384263495,"Guidelines:
743"
EXPERIMENTS COMPUTE RESOURCES,0.8847209515096066,"• The answer NA means that the paper does not include experiments.
744"
EXPERIMENTS COMPUTE RESOURCES,0.8856358645928637,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
745"
EXPERIMENTS COMPUTE RESOURCES,0.8865507776761208,"provider, including relevant memory and storage.
746"
EXPERIMENTS COMPUTE RESOURCES,0.8874656907593779,"• The paper should provide the amount of compute required for each of the individual experimental
747"
EXPERIMENTS COMPUTE RESOURCES,0.888380603842635,"runs as well as estimate the total compute.
748"
EXPERIMENTS COMPUTE RESOURCES,0.889295516925892,"• The paper should disclose whether the full research project required more compute than the
749"
EXPERIMENTS COMPUTE RESOURCES,0.8902104300091491,"experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into
750"
EXPERIMENTS COMPUTE RESOURCES,0.8911253430924062,"the paper).
751"
CODE OF ETHICS,0.8920402561756633,"9. Code Of Ethics
752"
CODE OF ETHICS,0.8929551692589204,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
753"
CODE OF ETHICS,0.8938700823421775,"of Ethics https://neurips.cc/public/EthicsGuidelines?
754"
CODE OF ETHICS,0.8947849954254345,"Answer: [Yes]
755"
CODE OF ETHICS,0.8956999085086916,"Justification: The authors have reviewed the code of ethics and the paper adheres to it.
756"
CODE OF ETHICS,0.8966148215919487,"Guidelines:
757"
CODE OF ETHICS,0.8975297346752058,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
758"
CODE OF ETHICS,0.898444647758463,"• If the authors answer No, they should explain the special circumstances that require a deviation
759"
CODE OF ETHICS,0.8993595608417201,"from the Code of Ethics.
760"
CODE OF ETHICS,0.9002744739249772,"• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
761"
CODE OF ETHICS,0.9011893870082343,"to laws or regulations in their jurisdiction).
762"
BROADER IMPACTS,0.9021043000914913,"10. Broader Impacts
763"
BROADER IMPACTS,0.9030192131747484,"Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
764"
BROADER IMPACTS,0.9039341262580055,"of the work performed?
765"
BROADER IMPACTS,0.9048490393412626,"Answer: [Yes]
766"
BROADER IMPACTS,0.9057639524245197,"Justification: Please refer to the section broader impacts 7.
767"
BROADER IMPACTS,0.9066788655077768,"Guidelines:
768"
BROADER IMPACTS,0.9075937785910339,"• The answer NA means that there is no societal impact of the work performed.
769"
BROADER IMPACTS,0.9085086916742909,"• If the authors answer NA or No, they should explain why their work has no societal impact or
770"
BROADER IMPACTS,0.909423604757548,"why the paper does not address societal impact.
771"
BROADER IMPACTS,0.9103385178408051,"• Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
772"
BROADER IMPACTS,0.9112534309240622,"disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
773"
BROADER IMPACTS,0.9121683440073193,"ment of technologies that could make decisions that unfairly impact specific groups), privacy
774"
BROADER IMPACTS,0.9130832570905764,"considerations, and security considerations.
775"
BROADER IMPACTS,0.9139981701738334,"• The conference expects that many papers will be foundational research and not tied to particular
776"
BROADER IMPACTS,0.9149130832570905,"applications, let alone deployments. However, if there is a direct path to any negative applications,
777"
BROADER IMPACTS,0.9158279963403476,"the authors should point it out. For example, it is legitimate to point out that an improvement in
778"
BROADER IMPACTS,0.9167429094236048,"the quality of generative models could be used to generate deepfakes for disinformation. On the
779"
BROADER IMPACTS,0.9176578225068619,"other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
780"
BROADER IMPACTS,0.918572735590119,"could enable people to train models that generate Deepfakes faster.
781"
BROADER IMPACTS,0.9194876486733761,"• The authors should consider possible harms that could arise when the technology is being used
782"
BROADER IMPACTS,0.9204025617566332,"as intended and functioning correctly, harms that could arise when the technology is being used
783"
BROADER IMPACTS,0.9213174748398902,"as intended but gives incorrect results, and harms following from (intentional or unintentional)
784"
BROADER IMPACTS,0.9222323879231473,"misuse of the technology.
785"
BROADER IMPACTS,0.9231473010064044,"• If there are negative societal impacts, the authors could also discuss possible mitigation strategies
786"
BROADER IMPACTS,0.9240622140896615,"(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-
787"
BROADER IMPACTS,0.9249771271729186,"ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the
788"
BROADER IMPACTS,0.9258920402561757,"efficiency and accessibility of ML).
789"
SAFEGUARDS,0.9268069533394327,"11. Safeguards
790"
SAFEGUARDS,0.9277218664226898,"Question: Does the paper describe safeguards that have been put in place for responsible release of
791"
SAFEGUARDS,0.9286367795059469,"data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
792"
SAFEGUARDS,0.929551692589204,"scraped datasets)?
793"
SAFEGUARDS,0.9304666056724611,"Answer: [NA]
794"
SAFEGUARDS,0.9313815187557182,"Justification: The algorithm proposed in this paper does not not pose any such risk of misuse.
795"
SAFEGUARDS,0.9322964318389753,"Guidelines:
796"
SAFEGUARDS,0.9332113449222323,"• The answer NA means that the paper poses no such risks.
797"
SAFEGUARDS,0.9341262580054894,"• Released models that have a high risk for misuse or dual-use should be released with necessary
798"
SAFEGUARDS,0.9350411710887465,"safeguards to allow for controlled use of the model, for example by requiring that users adhere to
799"
SAFEGUARDS,0.9359560841720037,"usage guidelines or restrictions to access the model or implementing safety filters.
800"
SAFEGUARDS,0.9368709972552608,"• Datasets that have been scraped from the Internet could pose safety risks. The authors should
801"
SAFEGUARDS,0.9377859103385179,"describe how they avoided releasing unsafe images.
802"
SAFEGUARDS,0.938700823421775,"• We recognize that providing effective safeguards is challenging, and many papers do not require
803"
SAFEGUARDS,0.939615736505032,"this, but we encourage authors to take this into account and make a best faith effort.
804"
LICENSES FOR EXISTING ASSETS,0.9405306495882891,"12. Licenses for existing assets
805"
LICENSES FOR EXISTING ASSETS,0.9414455626715462,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
806"
LICENSES FOR EXISTING ASSETS,0.9423604757548033,"properly credited and are the license and terms of use explicitly mentioned and properly respected?
807"
LICENSES FOR EXISTING ASSETS,0.9432753888380604,"Answer: [Yes]
808"
LICENSES FOR EXISTING ASSETS,0.9441903019213175,"Justification: All codes and datasets used in this paper are under MIT licence and the original owners
809"
LICENSES FOR EXISTING ASSETS,0.9451052150045746,"have been cited.
810"
LICENSES FOR EXISTING ASSETS,0.9460201280878316,"Guidelines:
811"
LICENSES FOR EXISTING ASSETS,0.9469350411710887,"• The answer NA means that the paper does not use existing assets.
812"
LICENSES FOR EXISTING ASSETS,0.9478499542543458,"• The authors should cite the original paper that produced the code package or dataset.
813"
LICENSES FOR EXISTING ASSETS,0.9487648673376029,"• The authors should state which version of the asset is used and, if possible, include a URL.
814"
LICENSES FOR EXISTING ASSETS,0.94967978042086,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
815"
LICENSES FOR EXISTING ASSETS,0.9505946935041171,"• For scraped data from a particular source (e.g., website), the copyright and terms of service of
816"
LICENSES FOR EXISTING ASSETS,0.9515096065873742,"that source should be provided.
817"
LICENSES FOR EXISTING ASSETS,0.9524245196706312,"• If assets are released, the license, copyright information, and terms of use in the package should
818"
LICENSES FOR EXISTING ASSETS,0.9533394327538883,"be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
819"
LICENSES FOR EXISTING ASSETS,0.9542543458371455,"some datasets. Their licensing guide can help determine the license of a dataset.
820"
LICENSES FOR EXISTING ASSETS,0.9551692589204026,"• For existing datasets that are re-packaged, both the original license and the license of the derived
821"
LICENSES FOR EXISTING ASSETS,0.9560841720036597,"asset (if it has changed) should be provided.
822"
LICENSES FOR EXISTING ASSETS,0.9569990850869168,"• If this information is not available online, the authors are encouraged to reach out to the asset’s
823"
LICENSES FOR EXISTING ASSETS,0.9579139981701739,"creators.
824"
NEW ASSETS,0.958828911253431,"13. New Assets
825"
NEW ASSETS,0.959743824336688,"Question: Are new assets introduced in the paper well documented and is the documentation provided
826"
NEW ASSETS,0.9606587374199451,"alongside the assets?
827"
NEW ASSETS,0.9615736505032022,"Answer: [NA]
828"
NEW ASSETS,0.9624885635864593,"Justification: No new assets have been introduced in this paper.
829"
NEW ASSETS,0.9634034766697164,"Guidelines:
830"
NEW ASSETS,0.9643183897529735,"• The answer NA means that the paper does not release new assets.
831"
NEW ASSETS,0.9652333028362305,"• Researchers should communicate the details of the dataset/code/model as part of their sub-
832"
NEW ASSETS,0.9661482159194876,"missions via structured templates. This includes details about training, license, limitations,
833"
NEW ASSETS,0.9670631290027447,"etc.
834"
NEW ASSETS,0.9679780420860018,"• The paper should discuss whether and how consent was obtained from people whose asset is
835"
NEW ASSETS,0.9688929551692589,"used.
836"
NEW ASSETS,0.969807868252516,"• At submission time, remember to anonymize your assets (if applicable). You can either create an
837"
NEW ASSETS,0.970722781335773,"anonymized URL or include an anonymized zip file.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716376944190301,"14. Crowdsourcing and Research with Human Subjects
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725526075022873,"Question: For crowdsourcing experiments and research with human subjects, does the paper include
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9734675205855444,"the full text of instructions given to participants and screenshots, if applicable, as well as details about
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743824336688015,"compensation (if any)?
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752973467520586,"Answer: [NA]
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762122598353157,"Justification: The paper did not require any crowdsourcing or human subject for experimentation.
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9771271729185728,"Guidelines:
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780420860018298,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9789569990850869,"subjects.
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979871912168344,"• Including this information in the supplemental material is fine, but if the main contribution of the
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807868252516011,"paper involves human subjects, then as much detail as possible should be included in the main
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817017383348582,"paper.
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826166514181153,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835315645013724,"labor should be paid at least the minimum wage in the country of the data collector.
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844464775846294,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853613906678865,"Question: Does the paper describe potential risks incurred by study participants, whether such
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9862763037511436,"risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871912168344007,"equivalent approval/review based on the requirements of your country or institution) were obtained?
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881061299176578,"Answer: [NA]
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890210430009149,"Justification: The paper did not require any crowdsourcing or human subject for experimentation.
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989935956084172,"Guidelines:
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908508691674291,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917657822506862,"subjects.
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926806953339433,"• Depending on the country in which research is conducted, IRB approval (or equivalent) may be
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9935956084172004,"required for any human subjects research. If you obtained IRB approval, you should clearly state
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9945105215004575,"this in the paper.
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954254345837146,"• We recognize that the procedures for this may vary significantly between institutions and
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9963403476669717,"locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9972552607502287,"their institution.
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9981701738334858,"• For initial submissions, do not include any information that would break anonymity (if applica-
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990850869167429,"ble), such as the institution conducting the review.
869"
