Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017730496453900709,"Kernel-based optimal transport (OT) estimation is an alternative to the standard
1"
ABSTRACT,0.0035460992907801418,"plug-in OT estimation. Recent works suggested that kernel-based OT estimators are
2"
ABSTRACT,0.005319148936170213,"more statistically efﬁcient than plug-in OT estimators when comparing probability
3"
ABSTRACT,0.0070921985815602835,"measures in high-dimensions [59]. However, the computation of these estimators
4"
ABSTRACT,0.008865248226950355,"relies on the short-step interior-point method for which the required number of
5"
ABSTRACT,0.010638297872340425,"iterations is known to be large in practice. In this paper, we propose a nonsmooth
6"
ABSTRACT,0.012411347517730497,"equation model for kernel-based OT estimation and show that it can be efﬁciently
7"
ABSTRACT,0.014184397163120567,"solved via a specialized semismooth Newton (SSN) method. Indeed, by exploring
8"
ABSTRACT,0.015957446808510637,"the special problem structure, the per-iteration cost of performing one SSN step can
9"
ABSTRACT,0.01773049645390071,"be signiﬁcantly reduced in practice. We also prove that our algorithm can achieve a
10"
ABSTRACT,0.01950354609929078,"global convergence rate of O(1/
√"
ABSTRACT,0.02127659574468085,"k) and a local quadratic convergence rate under
11"
ABSTRACT,0.02304964539007092,"some standard regularity conditions. Finally, we demonstrate the effectiveness of
12"
ABSTRACT,0.024822695035460994,"our algorithm by conducing the experiments on both synthetic and real datasets.
13"
INTRODUCTION,0.026595744680851064,"1
Introduction
14"
INTRODUCTION,0.028368794326241134,"Optimal transport (OT) theory [60] has provided a principled framework for comparing probability
15"
INTRODUCTION,0.030141843971631204,"distributions. It has been extensively adopted in machine learning and related ﬁelds, with examples
16"
INTRODUCTION,0.031914893617021274,"including generative modeling [2, 21, 51, 57], classiﬁcation and clustering [20, 55, 25], and domain
17"
INTRODUCTION,0.03368794326241135,"adaptation [9, 10, 49], see also the monograph [43]. It has also had an impact in applied areas such as
18"
INTRODUCTION,0.03546099290780142,"neuroimaging [27] and cell trajectory prediction [53, 66].
19"
INTRODUCTION,0.03723404255319149,"Curse of Dimensionality. In many real application problems, the OT cost is computed for squared
20"
INTRODUCTION,0.03900709219858156,"Euclidean distance on the sampled distributions with n observations (leading to the 2-Wasserstein
21"
INTRODUCTION,0.040780141843971635,"distance). It is known that OT estimation suffers from the curse of dimensionality [16, 19, 62]:
22"
INTRODUCTION,0.0425531914893617,"the standard plug-in estimator, which consists in computing the OT distance between the sampled
23"
INTRODUCTION,0.044326241134751775,"distributions with n observations, converges to the OT distance between true distributions at a rate of
24"
INTRODUCTION,0.04609929078014184,"O(n−1/d), which degrades exponentially in the dimension d. This rate can be improved to O(n−1/2d)
25"
INTRODUCTION,0.047872340425531915,"when true distributions are different [7] but it is still problematic in a high-dimensional regime. This
26"
INTRODUCTION,0.04964539007092199,"issue can be a barrier to its adoption in machine learning since various application problems arising
27"
INTRODUCTION,0.051418439716312055,"from image processing and bioengineering are high-dimensional. Practitioners have long been aware
28"
INTRODUCTION,0.05319148936170213,"of such limitations and proposed efﬁcient computational schemes that not only improve computational
29"
INTRODUCTION,0.0549645390070922,"complexity but also carry out statistical regularization.
30"
INTRODUCTION,0.05673758865248227,"Regularization. In this context, two threads have been investigated to regularize the OT distance:
31"
INTRODUCTION,0.05851063829787234,"entropic regularization [11, 12, 22, 36] or low-dimensional projection [48, 4, 41, 29, 39, 31, 32, 40].
32"
INTRODUCTION,0.06028368794326241,"For the former approach, the sample complexity of entropic OT is bounded by O(η−d/2n−1/2) for a
33"
INTRODUCTION,0.06205673758865248,"regularization parameter η > 0. For the latter approach, the sample complexity of projection OT is
34"
INTRODUCTION,0.06382978723404255,"bounded by O(n−1/k) for an integer-valued projection dimension k ≤d. Even though these bounds
35"
INTRODUCTION,0.06560283687943262,"attain the dimension-free dependence on n, they deteriorate when η is small or k is large, either of
36"
INTRODUCTION,0.0673758865248227,"which is needed to study the sample complexity of OT [7], and which plays a role in real applications.
37"
INTRODUCTION,0.06914893617021277,"Leveraging Smoothness. A recent line of works have focused on the wavelet-based OT estimators
38"
INTRODUCTION,0.07092198581560284,"under a strong smoothness condition [63, 26, 15, 34]. Although these estimators are minimax optimal
39"
INTRODUCTION,0.0726950354609929,"from a statistical viewpoint, they are algorithmically intractable [59]. In contrast, a speciﬁc entropic
40"
INTRODUCTION,0.07446808510638298,"regularized OT estimator is computationally tractable but still suffers from the curse of dimensionality
41"
INTRODUCTION,0.07624113475177305,"when the dimension is sufﬁciently large [44]. Recently, Vacher et al. [59] has closed this statistical-
42"
INTRODUCTION,0.07801418439716312,"computational gap by designing a kernel-based estimator relying on kernel sums-of-squares (SoS)
43"
INTRODUCTION,0.0797872340425532,"and showed that it can be computed by a short-step interior-point method with polynomial-time
44"
INTRODUCTION,0.08156028368794327,"complexity guarantee. However, the short-step interior-point method is well known to be ineffective
45"
INTRODUCTION,0.08333333333333333,"for large number of iterations required as the sample size increases, diminishing their value from
46"
INTRODUCTION,0.0851063829787234,"both statistical and practical viewpoints1. In this context, Muzellec et al. [38] proposed to use the
47"
INTRODUCTION,0.08687943262411348,"relaxation model and solve it using gradient-based methods. However, the relaxation model may not
48"
INTRODUCTION,0.08865248226950355,"be a good approximation for kernel-based OT estimator, thereby lacking any statistical guarantee.
49"
INTRODUCTION,0.09042553191489362,"Goal: While there is an ongoing debate in the OT literature on the merits of computing the plug-in
50"
INTRODUCTION,0.09219858156028368,"OT estimators v.s. kernel-based OT estimators, we adopt the perspective that Vacher et al. [59]
51"
INTRODUCTION,0.09397163120567376,"does introduce a fairly novel approach and we believe that it is worth studying if the kernel-based
52"
INTRODUCTION,0.09574468085106383,"OT estimation can provide leads for practical use. The goal of this paper is therefore to facilitate
53"
INTRODUCTION,0.0975177304964539,"the computational aspect by designing new algorithms, and to ﬁgure out whether that estimator’s
54"
INTRODUCTION,0.09929078014184398,"theoretical claims is also supported by practical relevance. The statistical analysis of kernel-based OT
55"
INTRODUCTION,0.10106382978723404,"estimation itself, e.g., the proper choice of penalty parameters, is beyond the scope of this paper.
56"
INTRODUCTION,0.10283687943262411,"Contribution: In this paper, we propose a nonsmooth equation model for computing kernel-based
57"
INTRODUCTION,0.10460992907801418,"OT estimators and show that it has a special problem structure, allowing it to be solved in an efﬁcient
58"
INTRODUCTION,0.10638297872340426,"manner using semismooth Newton method [37, 47, 46, 58].
59"
INTRODUCTION,0.10815602836879433,"We ﬁrst propose a nonsmooth equation model for computing the kernel-based OT estimator and
60"
INTRODUCTION,0.1099290780141844,"deﬁne an approximate OT value, which allows us to carry out a ﬁnite-time analysis of the algorithm.
61"
INTRODUCTION,0.11170212765957446,"Then, we propose a specialized semismooth Newton method for computing the kernel-based OT
62"
INTRODUCTION,0.11347517730496454,"estimator and prove a global convergence rate of O(1/
√"
INTRODUCTION,0.11524822695035461,"k) (Theorem 3.3) and a local quadratic
63"
INTRODUCTION,0.11702127659574468,"convergence rate under standard regularity conditions (Theorem 3.4). Notably, we signiﬁcantly
64"
INTRODUCTION,0.11879432624113476,"reduce the per-iteration computational cost by exploiting the special problem structure. Finally, we
65"
INTRODUCTION,0.12056737588652482,"conduct the experiments to evaluate our algorithm on both synthetic and real datasets. Experimental
66"
INTRODUCTION,0.12234042553191489,"results demonstrate its efﬁciency for solving the kernel-based OT estimation.
67"
INTRODUCTION,0.12411347517730496,"Organization. The remainder of the paper is organized as follows. In Section 2, we present the
68"
INTRODUCTION,0.12588652482269502,"nonsmooth equation model for computing the kernel-based OT estimators and deﬁne the optimality
69"
INTRODUCTION,0.1276595744680851,"notion based on the residual map. In Section 3, we propose and analyze the specialized semismooth
70"
INTRODUCTION,0.12943262411347517,"Newton (SSN) algorithm for computing the kernel-based OT estimators and prove that our algorithm
71"
INTRODUCTION,0.13120567375886524,"achieves the convergence rate guarantee in both global and local sense. In Section 4, we conduct the
72"
INTRODUCTION,0.13297872340425532,"experiments on both synthetic and real datasets, demonstrating that our algorithm can effectively
73"
INTRODUCTION,0.1347517730496454,"compute the kernel-based OT estimators and is more efﬁcient than short-step interior-point methods.
74"
INTRODUCTION,0.13652482269503546,"In Section 5, we conclude this paper. In the supplementary material, we provide further background
75"
INTRODUCTION,0.13829787234042554,"materials on SSN methods, additional experimental results, and missing proofs for key results.
76"
PRELIMINARIES AND TECHNICAL BACKGROUND,0.1400709219858156,"2
Preliminaries and Technical Background
77"
PRELIMINARIES AND TECHNICAL BACKGROUND,0.14184397163120568,"In this section, we present the basic setup for the kernel-based optimal transport (OT) estimation and
78"
PRELIMINARIES AND TECHNICAL BACKGROUND,0.14361702127659576,"propose a nonsmooth equation model for its computation.
79"
KERNEL-BASED OT ESTIMATION,0.1453900709219858,"2.1
Kernel-based OT estimation
80"
KERNEL-BASED OT ESTIMATION,0.14716312056737588,"We formally deﬁne the OT distance and review the kernel-based OT estimation [59]. Indeed, the OT
81"
KERNEL-BASED OT ESTIMATION,0.14893617021276595,"distance with strong smooth distributions can be estimated at a dimension-free statistical rate with
82"
KERNEL-BASED OT ESTIMATION,0.15070921985815602,"high probability by solving a suitably deﬁned optimization model.
83"
KERNEL-BASED OT ESTIMATION,0.1524822695035461,"1The short-step interior-point method proposed by Vacher et al. [59] is in fact a Newton barrier method and
does not exploit the special structure of kernel-based OT estimation. The required number of iterations is large
as shown by our experiments in the subsequent of this paper."
KERNEL-BASED OT ESTIMATION,0.15425531914893617,"Let X and Y be two bounded domains in Rd and let P(X) and P(Y ) be the set of Borel probability
84"
KERNEL-BASED OT ESTIMATION,0.15602836879432624,"measures in X and Y . Suppose that µ ∈P(X), ν ∈P(Y ) and Π(µ, ν) is the set of couplings
85"
KERNEL-BASED OT ESTIMATION,0.15780141843971632,"between µ and ν, the OT distance [60] is given by
86"
KERNEL-BASED OT ESTIMATION,0.1595744680851064,"OT(µ, ν) := 1 2"
KERNEL-BASED OT ESTIMATION,0.16134751773049646,"
inf
π∈Π(µ,ν) Z"
KERNEL-BASED OT ESTIMATION,0.16312056737588654,"X×Y
∥x −y∥2 dπ(x, y)

."
KERNEL-BASED OT ESTIMATION,0.16489361702127658,"Its dual formulation is stated as follows,
87"
KERNEL-BASED OT ESTIMATION,0.16666666666666666,"sup
u,v∈C(Rd) Z"
KERNEL-BASED OT ESTIMATION,0.16843971631205673,"X
u(x)dµ(x) +
Z"
KERNEL-BASED OT ESTIMATION,0.1702127659574468,"Y
v(y)dν(y),
s.t. 1"
KERNEL-BASED OT ESTIMATION,0.17198581560283688,"2∥x −y∥2 ≥u(x) + v(y), ∀(x, y) ∈X × Y,"
KERNEL-BASED OT ESTIMATION,0.17375886524822695,"where C(Rd) is the space of continuous functions on Rd. Note that the supremum can be attained and
88"
KERNEL-BASED OT ESTIMATION,0.17553191489361702,"the corresponding optimal dual functions u⋆and v⋆are referred to as the Kantorovich potentials [52].
89"
KERNEL-BASED OT ESTIMATION,0.1773049645390071,This problem is delicate to solve since 1
KERNEL-BASED OT ESTIMATION,0.17907801418439717,"2∥x−y∥2 ≥u(x)+v(y) needs to be satisﬁed on a continuous
90"
KERNEL-BASED OT ESTIMATION,0.18085106382978725,"set X × Y . A natural approach is to take n points {(˜x1, ˜y1), . . . , (˜xn, ˜yn)} ⊆X × Y and consider
91"
KERNEL-BASED OT ESTIMATION,0.18262411347517732,the constraints 1
KERNEL-BASED OT ESTIMATION,0.18439716312056736,"2∥˜xi −˜yi∥2 ≥u(˜xi) + v(˜yi) for all 1 ≤i ≤n. However, it can not leverage the
92"
KERNEL-BASED OT ESTIMATION,0.18617021276595744,"smoothness of potentials [3], yielding an error of Ω(n−1/d). Vacher et al. [59] has overcome this
93"
KERNEL-BASED OT ESTIMATION,0.1879432624113475,"difﬁculty by replacing the inequality constraints with equality constraints that are equivalent and
94"
KERNEL-BASED OT ESTIMATION,0.18971631205673758,"considering the equality constraints over n points. Following their works, we impose the following
95"
KERNEL-BASED OT ESTIMATION,0.19148936170212766,"assumption on the support sets X, Y and the densities of µ and ν.
96"
KERNEL-BASED OT ESTIMATION,0.19326241134751773,"Assumption 2.1 Let d ≥1 be the dimension and let m > 2d + 2 be the order of smoothness. Then,
97"
KERNEL-BASED OT ESTIMATION,0.1950354609929078,"we assume that (i) the support sets X, Y are convex, bounded, and open with Lipschitz boundaries;
98"
KERNEL-BASED OT ESTIMATION,0.19680851063829788,"(ii) the densities of µ, ν are ﬁnite, bounded away from zero and m-times differentiable.
99"
KERNEL-BASED OT ESTIMATION,0.19858156028368795,"Assumption 2.1 guarantees that the potentials u⋆and v⋆have a similar order of differentiability [14],
100"
KERNEL-BASED OT ESTIMATION,0.20035460992907803,"leading to an effective way to represent u and v via a reproducing Kernel Hilbert space (RKHS) [42].
101"
KERNEL-BASED OT ESTIMATION,0.20212765957446807,"In particular, we deﬁne Hs(Z) := {f ∈L2(Z) | ∥f∥Hs(Z) := P"
KERNEL-BASED OT ESTIMATION,0.20390070921985815,"|α|≤s ∥Dαf∥L2(Z) < +∞} and
102"
KERNEL-BASED OT ESTIMATION,0.20567375886524822,remark that Hs(Z) ⊆Ck(Z) for any s > d
KERNEL-BASED OT ESTIMATION,0.2074468085106383,"2 + k, where k ≥0 is integer-valued. This implies that
103"
KERNEL-BASED OT ESTIMATION,0.20921985815602837,"Hm+1(X), Hm+1(Y ) and Hm(X × Y ) are RKHS under Assumption 2.1 and they are associated
104"
KERNEL-BASED OT ESTIMATION,0.21099290780141844,"with three bounded continuous feature maps φX : X 7→Hm+1(X), φY : Y 7→Hm+1(Y ) and
105"
KERNEL-BASED OT ESTIMATION,0.2127659574468085,"φXY : X × Y 7→Hm(X × Y ). For simplicity, we let HX = Hm+1(X), HY = Hm+1(Y ) and
106"
KERNEL-BASED OT ESTIMATION,0.21453900709219859,"HXY = Hm(X × Y ). Vacher et al. [59, Corollary 7] shows that (i) u⋆∈HX and v⋆∈HY with
107 Z"
KERNEL-BASED OT ESTIMATION,0.21631205673758866,"X
u(x)dµ(x) = ⟨u, wµ⟩HX,
Z"
KERNEL-BASED OT ESTIMATION,0.21808510638297873,"X
v(y)dν(y) = ⟨v, wν⟩HY ,"
KERNEL-BASED OT ESTIMATION,0.2198581560283688,"where wµ =
R"
KERNEL-BASED OT ESTIMATION,0.22163120567375885,"X φX(x)dµ(x) and wν =
R"
KERNEL-BASED OT ESTIMATION,0.22340425531914893,"Y φY (y)dν(y) are kernel mean embeddings; (ii) A⋆∈
108"
KERNEL-BASED OT ESTIMATION,0.225177304964539,"S+(HXY )2 exists and satisﬁes the equality constraint as follows:
109"
KERNEL-BASED OT ESTIMATION,0.22695035460992907,"1
2∥x −y∥2 −u⋆(x) −v⋆(y) = ⟨φXY (x, y), A⋆φXY (x, y)⟩HXY ."
KERNEL-BASED OT ESTIMATION,0.22872340425531915,"Putting these pieces yields a representation theorem for estimating the OT distance. Indeed, under
110"
KERNEL-BASED OT ESTIMATION,0.23049645390070922,"Assumption 2.1, the dual OT problem is equivalent to the RKHS-based problem given by
111"
KERNEL-BASED OT ESTIMATION,0.2322695035460993,"max
u,v,A
⟨u, wµ⟩HX + ⟨v, wν⟩HY ,"
KERNEL-BASED OT ESTIMATION,0.23404255319148937,"s.t.
1
2∥x −y∥2 −u(x) −v(y) = ⟨φXY (x, y), AφXY (x, y)⟩HXY .
(2.1)"
KERNEL-BASED OT ESTIMATION,0.23581560283687944,"The above equation offers two advantages: (i) The equality constraint can be well approximated
112"
KERNEL-BASED OT ESTIMATION,0.2375886524822695,"under Assumption 2.1; (ii) RKHSs allow the kernel trick: computing parameters are expressed in
113"
KERNEL-BASED OT ESTIMATION,0.2393617021276596,"terms of kernel functions that correspond to
114"
KERNEL-BASED OT ESTIMATION,0.24113475177304963,"kX(x, x′) = ⟨φX(x), φX(x′)⟩HX,
kY (y, y′) = ⟨φY (y), φY (y′)⟩HY ,"
KERNEL-BASED OT ESTIMATION,0.2429078014184397,"and
115"
KERNEL-BASED OT ESTIMATION,0.24468085106382978,"kXY ((x, y), (x′, y′)) = ⟨φXY (x, y), φXY (x′, y′)⟩HXY ,
where the kernel functions are explicit and can be computed in O(d) given the samples. The ﬁnal
116"
KERNEL-BASED OT ESTIMATION,0.24645390070921985,"step is to approximate Eq. (2.1) using the data x1, . . . , xnsample ∼µ and y1, . . . , ynsample ∼ν, and
117"
KERNEL-BASED OT ESTIMATION,0.24822695035460993,"the ﬁlling points {(˜x1, ˜y1), . . . , (˜xn, ˜yn)} ⊆X × Y . Indeed, we deﬁne ˆµ =
1
nsample
Pnsample
i=1
δxi and
118"
KERNEL-BASED OT ESTIMATION,0.25,"2We refer to S+(HXY ) as the set of linear, positive and self-adjoint operators on HXY ."
KERNEL-BASED OT ESTIMATION,0.25177304964539005,"ˆν =
1
nsample
Pnsample
i=1
δyi, and use ⟨u, wˆµ⟩HX + ⟨v, wˆν⟩HY instead of ⟨u, wµ⟩HX + ⟨v, wν⟩HY where
119"
KERNEL-BASED OT ESTIMATION,0.25354609929078015,"wˆµ =
1
nsample
Pnsample
i=1
φX(xi) and wˆν =
1
nsample
Pnsample
i=1
φY (yi). We also impose the penalization terms
120"
KERNEL-BASED OT ESTIMATION,0.2553191489361702,"for u, v, and A to alleviate the error induced by sampling the corresponding equality constraints.
121"
KERNEL-BASED OT ESTIMATION,0.2570921985815603,"Then, the resulting problem with regularization parameters λ1, λ2 > 0 is summarized as follows:
122"
KERNEL-BASED OT ESTIMATION,0.25886524822695034,"max
u,v,A
⟨u, wˆµ⟩HX + ⟨v, wˆν⟩HY −λ1Tr(A) −λ2(∥u∥2
HX + ∥v∥2
HY ),"
KERNEL-BASED OT ESTIMATION,0.26063829787234044,"s.t.
1
2∥˜xi −˜yi∥2 −u(˜xi) −v(˜yi) = ⟨φXY (˜xi, ˜yi), AφXY (˜xi, ˜yi)⟩HXY .
(2.2)"
KERNEL-BASED OT ESTIMATION,0.2624113475177305,"Focusing on the case nsample = Θ(n), we let ˆu⋆and ˆv⋆be the unique maximizers of Eq. (2.2). Then,
123"
KERNEL-BASED OT ESTIMATION,0.2641843971631206,"the estimator for OT(µ, ν) we consider corresponds to
124"
KERNEL-BASED OT ESTIMATION,0.26595744680851063,"c
OT
n = ⟨ˆu⋆, wˆµ⟩HX + ⟨ˆv⋆, wˆν⟩HY .
(2.3) 125"
KERNEL-BASED OT ESTIMATION,0.26773049645390073,"Remark 2.2 It follows from Vacher et al. [59, Corollary 3] that the norm of empirical potentials can
126"
KERNEL-BASED OT ESTIMATION,0.2695035460992908,"be controlled using λ1 = ˜Θ(n−1/2) and λ2 = ˜Θ(n−1/2) in high probability sense, leading to the
127"
KERNEL-BASED OT ESTIMATION,0.2712765957446808,"sample complexity bound: |c
OT
n −OT(µ, ν)| = ˜O(n−1/2). In comparison with plug-in estimators,
128"
KERNEL-BASED OT ESTIMATION,0.2730496453900709,"the kernel-based OT estimators are better when the sample size is small and the dimension is high.
129"
KERNEL-BASED OT ESTIMATION,0.274822695035461,"Note that Eq. (2.2) is an inﬁnite-dimensional optimization problem and is thus difﬁcult to be solved.
130"
KERNEL-BASED OT ESTIMATION,0.2765957446808511,"Thanks to Vacher et al. [59, Theorem 15], we have that the dual problem of Eq. (2.2) can be presented
131"
KERNEL-BASED OT ESTIMATION,0.2783687943262411,"in a ﬁnite-dimensional space and the strong duality holds true. Indeed, we deﬁne Q ∈Rn×n with
132"
KERNEL-BASED OT ESTIMATION,0.2801418439716312,"Qij = kX(˜xi, ˜xj) + kY (˜yi, ˜yj), and z ∈Rn with zi = wˆµ(˜xi) + wˆν(˜yi) −λ2∥˜xi −˜yi∥2, and
133"
KERNEL-BASED OT ESTIMATION,0.28191489361702127,"q2 = ∥wˆµ∥2
HX + ∥wˆν∥HY , where we have
134"
KERNEL-BASED OT ESTIMATION,0.28368794326241137,"wˆµ(˜xi) =
1
nsample"
KERNEL-BASED OT ESTIMATION,0.2854609929078014,"nsample
X"
KERNEL-BASED OT ESTIMATION,0.2872340425531915,"j=1
kX(xj, ˜xi),
wˆν(˜yi) =
1
nsample"
KERNEL-BASED OT ESTIMATION,0.28900709219858156,"nsample
X"
KERNEL-BASED OT ESTIMATION,0.2907801418439716,"j=1
kY (yj, ˜yi),"
KERNEL-BASED OT ESTIMATION,0.2925531914893617,"and
135"
KERNEL-BASED OT ESTIMATION,0.29432624113475175,"∥wˆµ∥2
HX =
1
n2
sample X"
KERNEL-BASED OT ESTIMATION,0.29609929078014185,"1≤i,j≤nsample
kX(xi, xj),
∥wˆν∥2
HY =
1
n2
sample X"
KERNEL-BASED OT ESTIMATION,0.2978723404255319,"1≤i,j≤nsample
kY (yi, yj)."
KERNEL-BASED OT ESTIMATION,0.299645390070922,"We deﬁne K ∈Rn×n with Kij = kXY ((˜xi, ˜yi), (˜xj, ˜yj)) and R as an upper triangular matrix for
136"
KERNEL-BASED OT ESTIMATION,0.30141843971631205,"the Cholesky decomposition of K. We let Φi be the ith column of R. Then, the dual problem of
137"
KERNEL-BASED OT ESTIMATION,0.30319148936170215,"Eq. (2.2) reads:
138"
KERNEL-BASED OT ESTIMATION,0.3049645390070922,"min
γ∈Rn
1
4λ2 γ⊤Qγ −
1
2λ2 γ⊤z +
q2"
KERNEL-BASED OT ESTIMATION,0.3067375886524823,"4λ2 ,
s.t. n
X"
KERNEL-BASED OT ESTIMATION,0.30851063829787234,"i=1
γiΦiΦ⊤
i + λ1I ⪰0.
(2.4)"
KERNEL-BASED OT ESTIMATION,0.3102836879432624,"Suppose that ˆγ is one minimizer, we have
139"
KERNEL-BASED OT ESTIMATION,0.3120567375886525,"c
W n =
q2"
KERNEL-BASED OT ESTIMATION,0.31382978723404253,"2λ2 −
1
2λ2 n
X"
KERNEL-BASED OT ESTIMATION,0.31560283687943264,"i=1
ˆγi(wˆµ(˜xi) + wˆν(˜yi))."
KERNEL-BASED OT ESTIMATION,0.3173758865248227,"To our knowledge, the existing method proposed for solving Eq. (2.4) is a short-step interior-point
140"
KERNEL-BASED OT ESTIMATION,0.3191489361702128,"method for which the required number of iterations is known to be large when n is large, which
141"
KERNEL-BASED OT ESTIMATION,0.32092198581560283,"is necessary to guarantee small statistical error. To avoid this issue, Muzellec et al. [38] proposed
142"
KERNEL-BASED OT ESTIMATION,0.32269503546099293,"solving an unconstrained relaxation model which allows for the application of gradient-based methods.
143"
KERNEL-BASED OT ESTIMATION,0.324468085106383,"However, the estimators obtained from solving such relaxation model lack any statistical guarantee.
144"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3262411347517731,"2.2
Nonsmooth equation model and optimality condition
145"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3280141843971631,"For simplicity, we deﬁne the operator Φ : Rn×n 7→Rn and its adjoint Φ⋆: Rn 7→Rn×n by
146"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.32978723404255317,"Φ(X) =  
"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.33156028368794327,"⟨X, Φ1Φ⊤
1 ⟩
...
⟨X, ΦnΦ⊤
n ⟩ "
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3333333333333333,"
,
Φ⋆(γ) = n
X"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3351063829787234,"i=1
γiΦiΦ⊤
i ."
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.33687943262411346,"We present the optimality notion for Eq. (2.4) as follows:
147"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.33865248226950356,"Deﬁnition 2.1 A point ˆγ ∈Rn is an optimal solution of Eq. (2.4) if we have Φ⋆(ˆγ) + λ1I ⪰0 and
148"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3404255319148936,"1
4λ2 ˆγ⊤Qˆγ −
1
2λ2 ˆγ⊤z + q2"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3421985815602837,"4λ2 ≤
1
4λ2 γ⊤Qγ −
1
2λ2 γ⊤z + q2"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.34397163120567376,"4λ2 for all γ satisfying that Φ⋆(γ)+λ1I ⪰0.
149"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.34574468085106386,"Clearly, Eq. (2.4) can be reformulated as the following optimization problem given by
150"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3475177304964539,"min
γ∈Rn max
X⪰0
1
4λ2 γ⊤Qγ −
1
2λ2 γ⊤z +
q2"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.34929078014184395,"4λ2 −⟨X, Φ⋆(γ) + λ1I⟩.
(2.5)"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.35106382978723405,"We denote w = (γ, X) as a vector-matrix pair and let R : Rn × Rn×n →Rn × Rn×n be given by
151"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3528368794326241,"R(w) =

1
2λ2 Qγ −
1
2λ2 z −Φ(X)
X −projSn
+(X −(Φ⋆(γ) + λ1I))"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3546099290780142,"
.
(2.6)"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.35638297872340424,"where Sn
+ = {X ∈Rn×n : X ⪰0}. Then, we can measure the optimality of w via appeal to the
152"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.35815602836879434,"quantity ∥R(w)∥and shows that the notion is the same as used in Deﬁnition 2.1.
153"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3599290780141844,"Proposition 2.3 A point ˆγ is an optimal solution of Eq. (2.4) if and only if ˆw = (ˆγ, ˆX) satisﬁes
154"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3617021276595745,"R( ˆw) = 0 for some ˆX ⪰0.
155"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.36347517730496454,"Proposition 2.3 shows that we can compute the kernel-based OT estimators by solving the nonsmooth
156"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.36524822695035464,"equation model R(w) = 0. The optimality criterion based on the residual map R(·) allows for a
157"
NONSMOOTH EQUATION MODEL AND OPTIMALITY CONDITION,0.3670212765957447,"global convergence rate analysis for our specialized semismooth Newton method.
158"
ALGORITHM AND CONVERGENCE ANALYSIS,0.36879432624113473,"3
Algorithm and Convergence Analysis
159"
ALGORITHM AND CONVERGENCE ANALYSIS,0.37056737588652483,"In this section, we derive our algorithm and provide a convergence rate analysis. The key idea here is
160"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3723404255319149,"to apply the regularized semismooth Newton (SSN) method for solving R(w) = 0 and improve the
161"
ALGORITHM AND CONVERGENCE ANALYSIS,0.374113475177305,"computation of each SSN step by exploring the special structure of generalized Jacobian. We also
162"
ALGORITHM AND CONVERGENCE ANALYSIS,0.375886524822695,"safeguard the regularized SSN method by min-max method to achieve a global rate.
163"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3776595744680851,"Generalized Jacobian. We ﬁrst examine the special structure of the generalized Jacobian of R(w).
164"
ALGORITHM AND CONVERGENCE ANALYSIS,0.37943262411347517,"Indeed, by using the deﬁnition of Sn
+, we have projSn
+(Z) = PαΣαP ⊤
α where
165"
ALGORITHM AND CONVERGENCE ANALYSIS,0.38120567375886527,"Z = PΣP ⊤= (Pα
P¯α)

Σα
0
0
Σ¯α"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3829787234042553," 
P ⊤
α
P ⊤
¯α"
ALGORITHM AND CONVERGENCE ANALYSIS,0.38475177304964536,"
,
(3.1)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.38652482269503546,"with Σ = diag(σ1, . . . , σn) and the sets of the indices of positive and nonpositive eigenvalues of Z
166"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3882978723404255,"(we denote these sets by α = {i | σi > 0} and ¯α = {1, 2, . . . , n} \ α). Moreover, we notice that R
167"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3900709219858156,"is Lipschitz continuous. Then, Rademacher’s theorem can guarantee that R is almost everywhere
168"
ALGORITHM AND CONVERGENCE ANALYSIS,0.39184397163120566,"differentiable. We introduce the concepts of generalized Jacobian [8].
169"
ALGORITHM AND CONVERGENCE ANALYSIS,0.39361702127659576,"Deﬁnition 3.1 Suppose that R is Lipschitz continuous and DR is the set of differentiable points of R.
170"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3953900709219858,"The B-subdifferential of R at w is given by ∂BR(w) := {limk→+∞∇F(wk) | wk ∈DR, wk →w}.
171"
ALGORITHM AND CONVERGENCE ANALYSIS,0.3971631205673759,"The set ∂R(w) = conv(∂BR(w)) is called generalized Jacobian where conv denotes the convex hull.
172"
ALGORITHM AND CONVERGENCE ANALYSIS,0.39893617021276595,"We deﬁne a generalized operator M(Z) ∈∂projSn
+(Z) using its application to an n × n matrix S:
173"
ALGORITHM AND CONVERGENCE ANALYSIS,0.40070921985815605,"M(Z)[S] = P(Ω◦(P ⊤SP))P ⊤for all S ⪰0,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4024822695035461,"where the ◦symbol denotes a Hadamard product and Ω=
Eαα
ηα¯α
η⊤
α¯α
0"
ALGORITHM AND CONVERGENCE ANALYSIS,0.40425531914893614,"
with Eαα being a matrix
174"
ALGORITHM AND CONVERGENCE ANALYSIS,0.40602836879432624,"of ones and ηij =
σi
σi−σj for all (i, j) ∈α × ¯α. Note that all entries of Ωlie in the interval (0, 1]. In
175"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4078014184397163,"general, it is nontrivial to characterize the generalized Jacobian ∂R(w) exactly but we can compute
176"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4095744680851064,"an element J (w) ∈∂R(w) using M(·) as deﬁned before.
177"
ALGORITHM AND CONVERGENCE ANALYSIS,0.41134751773049644,"We next introduce the deﬁnition of the (strong) semismoothness of an operator.
178"
ALGORITHM AND CONVERGENCE ANALYSIS,0.41312056737588654,"Deﬁnition 3.2 Suppose that R is Lipschitz continuous. Then, R is (strongly) semismooth at w if (i)
179"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4148936170212766,"R is directionally differentiable at w; and (ii) for any ∆w and J ∈∂R(w + ∆w), we have
180"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4166666666666667,"(semismooth)
∥R(w+∆w)−R(w)−J [∆w]∥"
ALGORITHM AND CONVERGENCE ANALYSIS,0.41843971631205673,"∥∆w∥
→0,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.42021276595744683,"(strongly semismooth)
∥R(w+∆w)−R(w)−J [∆w]∥"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4219858156028369,"∥∆w∥2
≤C. ,
as ∆w →0."
ALGORITHM AND CONVERGENCE ANALYSIS,0.4237588652482269,"Algorithm 1 Solving Eq. (3.2) where rk = (r1
k, r2
k) ∈Rn × Rn×n)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.425531914893617,"1: Compute a1 = −r1
k −
1
µk+1(Φ(r2
k + Tk[r2
k])) and a2 = −r2
k."
ALGORITHM AND CONVERGENCE ANALYSIS,0.42730496453900707,2: Use the CG or symmetric QMS method to solve ( 1
ALGORITHM AND CONVERGENCE ANALYSIS,0.42907801418439717,"2λ2 Q + µkI + ΦTkΦ⋆)−1˜a1 = a1 inexactly
and compute ˜a2 =
1
µk+1(a2 + Tk[a2]), where Tk[·] is computed using the trick [68]."
ALGORITHM AND CONVERGENCE ANALYSIS,0.4308510638297872,"3: Compute the direction ∆wk = (∆w1
k, ∆w2
k) by ∆w1
k = ˜a1 and ∆w2
k = ˜a2 −Tk[Φ⋆(˜a1)]."
ALGORITHM AND CONVERGENCE ANALYSIS,0.4326241134751773,"The following proposition characterizes the residual map given in Eq. (2.6) and its generalized
181"
ALGORITHM AND CONVERGENCE ANALYSIS,0.43439716312056736,"Jacobian matrix. It also guarantees that the SSN method is suitable to solve R(w) = 0.
182"
ALGORITHM AND CONVERGENCE ANALYSIS,0.43617021276595747,"Proposition 3.1 The residual map R given in Eq. (2.6) is strongly semismooth.
183"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4379432624113475,"Regularized SSN step. We then discuss how to compute the Newton direction efﬁciently. In
184"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4397163120567376,"particular, at a given iterate wk, we compute a Newton direction ∆wk by solving the equation
185"
ALGORITHM AND CONVERGENCE ANALYSIS,0.44148936170212766,"(Jk + µkI)[∆wk] = −rk,
(3.2)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4432624113475177,"where Jk ∈∂R(wk), rk = R(wk) and I is an identity operator. The regularization parameter
186"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4450354609929078,"is chosen as µk = θk∥rk∥for stabilizing the semismooth Newton method in practice. From a
187"
ALGORITHM AND CONVERGENCE ANALYSIS,0.44680851063829785,"computational point of view, it is not practical to solve the linear system in Eq. (3.2) exactly. Thus,
188"
ALGORITHM AND CONVERGENCE ANALYSIS,0.44858156028368795,"we seek an approximation step ∆wk by solving Eq. (3.2) approximately such that
189"
ALGORITHM AND CONVERGENCE ANALYSIS,0.450354609929078,"∥(Jk + µkI)[∆wk] + rk∥≤τ min{1, κ∥rk∥∥∆wk∥},
(3.3)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4521276595744681,"where 0 < τ, κ < 1 are some positive constants and ∥·∥is deﬁned for a vector-matrix pair w = (γ, X)
190"
ALGORITHM AND CONVERGENCE ANALYSIS,0.45390070921985815,"(i.e., ∥w∥= ∥γ∥2 + ∥X∥F where ∥· ∥2 is Euclidean norm and ∥· ∥F is Frobenius norm).
191"
ALGORITHM AND CONVERGENCE ANALYSIS,0.45567375886524825,"Since Jk in Eq. (3.2) is nonsymmetric and its dimension is large, we consider applying the Schur
192"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4574468085106383,"complement trick to transform Eq. (3.2) into a smaller symmetric system. If we vectorize the
193"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4592198581560284,"vector-matrix pair ∆w3, the operators M(Z) and Φ can be expressed as matrices:
194"
ALGORITHM AND CONVERGENCE ANALYSIS,0.46099290780141844,"M(Z) = ˜PΓ ˜P ⊤∈Rn2×n2,
A =  
"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4627659574468085,"Φ⊤
1 ⊗Φ⊤
1
...
Φ⊤
n ⊗Φ⊤
n "
ALGORITHM AND CONVERGENCE ANALYSIS,0.4645390070921986,"
∈Rn×n2,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.46631205673758863,"where ˜P = P ⊗P and Γ = diag(vec(Ω)).
195"
ALGORITHM AND CONVERGENCE ANALYSIS,0.46808510638297873,"We next provide a key lemma on the matrix form of Jk + µkI at a given iterate wk = (γk, Xk).
196"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4698581560283688,"Lemma 3.2 Given an iterate wk = (γk, Xk), we compute Zk = Xk −(Φ⋆(γk) + λ1I) and use
197"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4716312056737589,"Eq. (3.1) to obtain Pk, Σk, αk and ¯αk. We then obtain Ωk, ˜Pk = Pk ⊗Pk and Γk = diag(vec(Ωk)).
198"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4734042553191489,"Then, the matrix form of Jk + µkI is given by
199"
ALGORITHM AND CONVERGENCE ANALYSIS,0.475177304964539,"(Jk + µkI)−1 = C1BC2,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4769503546099291,"where
200"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4787234042553192,"C1 =

I
0
−TkA⊤
I"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4804964539007092,"
,
C2 =
I
1
µk+1(A + ATk)
0
I 
,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.48226950354609927,"and
201"
ALGORITHM AND CONVERGENCE ANALYSIS,0.48404255319148937,"B =
( 1"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4858156028368794,"2λ2 Q + µkI + ATkA⊤)−1
0
0
1
µk+1(I + Tk) 
,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4875886524822695,"with Tk = ˜PkLk ˜P ⊤
k where Lk is a diagonal matrix with (Lk)ii =
(Γk)ii
µk+1−(Γk)ii and (Γk)ii ∈(0, 1]
202"
ALGORITHM AND CONVERGENCE ANALYSIS,0.48936170212765956,"is then denoted as the ith diagonal entry of Γk.
203"
ALGORITHM AND CONVERGENCE ANALYSIS,0.49113475177304966,"As a consequence of Lemma 3.2, the solution of Eq. (3.2) can be obtained by solving one certain
204"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4929078014184397,"symmetric linear system with the matrix
1
2λ2 Q + µkI + ATkA⊤. We remark that this system is
205"
ALGORITHM AND CONVERGENCE ANALYSIS,0.4946808510638298,"well-deﬁned since both Q and ATkA⊤are positive semideﬁnite and the coefﬁcient µk is chosen such
206"
ALGORITHM AND CONVERGENCE ANALYSIS,0.49645390070921985,"that
1
2λ2 Q + µkI + ATkA⊤is invertible. This also shows that Eq. (3.2) is well-deﬁned.
207"
ALGORITHM AND CONVERGENCE ANALYSIS,0.49822695035460995,"3If w = (γ, X) is a vector-matrix pair, we deﬁne vec(w) = (γ; vec(X)) as its vectorization."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5,Algorithm 2 A specialized SSN method with safeguarding
ALGORITHM AND CONVERGENCE ANALYSIS,0.50177304964539,"1: Input: τ, κ, α2 ≥α1 > 0, β0 < 1, β1, β2 > 1 and θ, θ > 0.
2: Initialization: v0 = w0 ∈Rn × Sn
+ and θ0 > 0. Set k = 0.
3: for k = 0, 1, 2, . . . do
4:
Update vk+1 from vk using one-step EG.
5:
Select Jk ∈∂R(wk).
6:
Solve the linear system in Eq. (3.2) approximately such that ∆wk satisﬁes Eq. (3.3).
7:
Compute ˜wk+1 = wk + ∆wk.
8:
Update θk+1 using Eq. (3.4) accordingly.
9:
Set wk+1 = ˜wk+1 if ∥R( ˜wk+1)∥≤∥R(vk+1)∥is satisﬁed. Otherwise, set wk+1 = vk+1."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5035460992907801,"We deﬁne Tk and Q as the operator form of Tk = ˜PkLk ˜P ⊤
k and Q and write rk = (r1
k, r2
k) explicitly
208"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5053191489361702,"where r1
k ∈Rn and r2
k ∈Rn×n. Then, we have
209"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5070921985815603,"vec(a) = −
I
1
µk+1(A + AT)
0
I"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5088652482269503,"
vec(rk) =⇒
 a1 = −r1
k −
1
µk+1(Φ(r2
k + Tk[r2
k])),
a2 = −r2
k."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5106382978723404,"The next step consists in solving a new symmetric linear system and is given by
210"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5124113475177305,"vec(˜a) =
( 1"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5141843971631206,"2λ2 Q + µkI + ATkA⊤)−1
0
0
1
µk+1(I + Tk)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5159574468085106,"
vec(a),"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5177304964539007,"which leads to
211
 ˜a1 = ( 1"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5195035460992907,"2λ2 Q + µkI + ΦTkΦ⋆)−1a1,
˜a2 =
1
µk+1(a2 + Tk[a2])."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5212765957446809,"Compared to Eq. (3.2) whose matrix form has size (n2 +n)×(n2 +n), we remark that the one in the
212"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5230496453900709,"step above is smaller with the size of n × n and can be efﬁciently solved by conjugate gradient (CG)
213"
ALGORITHM AND CONVERGENCE ANALYSIS,0.524822695035461,"method or symmetric quasi-minimal residual (QMR) method [28, 50]. The ﬁnal step is to compute
214"
ALGORITHM AND CONVERGENCE ANALYSIS,0.526595744680851,"the Newton direction ∆wk = (∆w1
k, ∆w2
k) as follows,
215"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5283687943262412,"vec(∆wk) =

I
0
−TA⊤
I"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5301418439716312,"
vec(˜a) =⇒

∆w1
k = ˜a1,
∆w2
k = ˜a2 −Tk[Φ⋆(˜a1)]."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5319148936170213,"It remains to provide an efﬁcient manner to compute Tk[·]. Since Tk is deﬁned as the operator form
216"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5336879432624113,"of T = ˜PkLk ˜P ⊤
k , we have
217"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5354609929078015,"Tk[S] = Pk(Ψk ◦(P ⊤
k SPk))P ⊤
k ,
where Ψk is determined by µk and Ωk. Indeed, we have
218"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5372340425531915,"Ωk =
Eαkαk
ηαk ¯αk
η⊤
αk ¯αk
0"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5390070921985816,"
=⇒Ψk =
 1"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5407801418439716,"µk Eαkαk
ξαk ¯αk
ξ⊤
αk ¯αk
0 
,"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5425531914893617,"where ξij =
ηij
µk+1−ηij for all (i, j) ∈αk × ¯αk. Following Zhao et al. [68], we use the decomposition
219"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5443262411347518,"Tk[S] = G + G⊤where U = Pk(:, αk)⊤S and
220"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5460992907801419,"G = Pk(:, αk)(
1
2µk (UPk(:, αk))Pk(:, αk)⊤+ ξαk ¯αk ◦(UPk(:, ¯αk))Pk(:, ¯αk)⊤)."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5478723404255319,"The number of ﬂops required to compute Tk[S] is 8|αk|n2. For the case of |αk| > ¯αk, we compute
221"
ALGORITHM AND CONVERGENCE ANALYSIS,0.549645390070922,"Tk[S] via Tk[S] =
1
µk S −Pk(( 1"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5514184397163121,"µk E −Ψk) ◦(P ⊤
k SPk))P ⊤
k using 8|¯αk|n2 ﬂops. This demonstrates
222"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5531914893617021,"that we can obtain an approximate solution of Eq. (3.2) efﬁciently whenever |αk| or |¯αk| is small.
223"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5549645390070922,"We present the scheme for computing an approximate Newton direction in Algorithm 1.
224"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5567375886524822,"Adaptive strategy. We propose a rule for updating θk where µk = θk∥rk∥is deﬁned in Eq. (3.2).
225"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5585106382978723,"Indeed, we compute ρk = −⟨R(wk), ∆wk⟩and use it to update θk+1. The update rule is summarized
226"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5602836879432624,"as follows:
227"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5620567375886525,"θk+1 = 
 "
ALGORITHM AND CONVERGENCE ANALYSIS,0.5638297872340425,"max{θ, β0θk},
if ρk ≥α2∥∆wk∥2,
β1θk,
if α1∥∆wk∥2 ≤ρk < α2∥∆wk∥2,
min{θ, β2θk},
otherwise.
(3.4)"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5656028368794326,"where β0 < 1, β1, β2 > 1 and θ, θ > 0.
228"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5673758865248227,"Figure 1: Visualization of the OT map with nsample = n ∈{50, 100, 200}."
ALGORITHM AND CONVERGENCE ANALYSIS,0.5691489361702128,"Main scheme. We summarize the complete scheme of our new algorithm in Algorithm 2. Indeed,
229"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5709219858156028,"we generate a sequence of iterates by alternating between extragradient (EG) method [17, 6] and the
230"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5726950354609929,"aforementioned regularized SSN method.
231"
ALGORITHM AND CONVERGENCE ANALYSIS,0.574468085106383,"Note that we maintain one auxiliary sequence of iterates {vk}k≥0. This sequence is directly generated
232"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5762411347517731,"by the EG method for solving the min-max optimization problem in Eq. (2.5) and is used to safeguard
233"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5780141843971631,"the regularized SSN method to achieve a global convergence rate. More speciﬁcally, we start with
234"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5797872340425532,"v0 = w0 ∈Rn × Sn
+ and perform the kth iteration as follows,
235"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5815602836879432,"1. Update vk+1 from vk using one-step EG.
236"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5833333333333334,"2. Update ˜wk+1 from wk using one-step regularized SSN.
237"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5851063829787234,"3. Set wk+1 = ˜wk+1 if ∥R( ˜wk+1)∥≤∥R(vk+1)∥and wk+1 = vk+1 otherwise.
238"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5868794326241135,"In our experiment, we ﬁnd that the main iterates are mostly generated by regularized SSN steps and
239"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5886524822695035,"the whole algorithm converges at a superlinear rate. This phenomenon is quite intuitive: if the initial
240"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5904255319148937,"point is sufﬁciently close to one nondegenerate optimal solution, the regularized SSN method can
241"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5921985815602837,"achieve the similar quadratic convergence rate (cf. Theorem 3.4) as shared by other SSN methods in
242"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5939716312056738,"the existing literature [35, 18, 1]. The detailed analysis will be provided in the appendix.
243"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5957446808510638,"Main results. We establish the convergence guarantee of Algorithm 2 in the following theorems.
244"
ALGORITHM AND CONVERGENCE ANALYSIS,0.5975177304964538,"Theorem 3.3 Suppose that {wk}k≥0 is a sequence of iterates generated by Algorithm 2. Then, the
245"
ALGORITHM AND CONVERGENCE ANALYSIS,0.599290780141844,"residuals of {wk}k≥0 converge to 0 at a rate of 1/
√"
ALGORITHM AND CONVERGENCE ANALYSIS,0.601063829787234,"k, i.e., ∥R(wk)∥= O(1/
√"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6028368794326241,"k).
246"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6046099290780141,"Theorem 3.4 Suppose that {wk}k≥0 is a sequence of iterates generated by Algorithm 2. Then, the
247"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6063829787234043,"residuals of {wk}k≥0 converge to 0 at a quadratic rate if the initial point w0 is sufﬁciently close to
248"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6081560283687943,"w⋆with R(w⋆) = 0 and every element of ∂R(w⋆) is invertible.
249"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6099290780141844,"Remark 3.5 In the context of constrained convex-concave min-max optimization problem, Cai et al.
250"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6117021276595744,"[6] proved the O(1/
√"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6134751773049646,"k) last-iterate convergence rate of the EG, matching the lower bounds [24, 23].
251"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6152482269503546,"Since the kernel-based OT estimation can be solved as a min-max problem, the global convergence
252"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6170212765957447,"rate in Theorem 3.3 demonstrates the efﬁciency of Algorithm 2. It remains unclear whether or not we
253"
ALGORITHM AND CONVERGENCE ANALYSIS,0.6187943262411347,"can improve the convergence result by exploring special structure of Eq. (2.5).
254"
EXPERIMENTS,0.6205673758865248,"4
Experiments
255"
EXPERIMENTS,0.6223404255319149,"We present the results of experiments that evaluate the kernel-based OT estimation with our algorithm.
256"
EXPERIMENTS,0.624113475177305,"The baseline approach is the short-step interior-point method [59]; we exclude the gradient-based
257"
EXPERIMENTS,0.625886524822695,"method [38] from our experiment since it only solves the relaxation model. All the experiments were
258"
EXPERIMENTS,0.6276595744680851,"conducted on a MacBook Pro with an Intel Core i9 2.4GHz and 16GB memory.
259"
EXPERIMENTS,0.6294326241134752,"Following the setup in Vacher et al. [59], we draw nsample samples from µ and nsample samples from ν,
260"
EXPERIMENTS,0.6312056737588653,"where µ is a mixture of 3 d-dimensional Gaussian distributions and ν is a mixture of 5 d-dimensional
261"
EXPERIMENTS,0.6329787234042553,"Gaussian distributions. Then, we sample n ﬁlling samples from a 2d Sobol sequence. We also set the
262"
EXPERIMENTS,0.6347517730496454,bandwidth σ2 = 0.01 and parameters λ1 = 1
EXPERIMENTS,0.6365248226950354,"n and λ2 =
1
√nsample . Focusing on the case of d = 1 (i.e.,
263"
EXPERIMENTS,0.6382978723404256,"1-dimensional setting), we report the visualization results in Figure 1 and 2 and ﬁnd that the inferred
264"
EXPERIMENTS,0.6400709219858156,"OT map will be closer the the true OT map as the number of ﬁlling points and data samples increase.
265"
EXPERIMENTS,0.6418439716312057,"Figure 2: Visualization of the constraint with nsample = n ∈{50, 100}. The right one is ground truth."
EXPERIMENTS,0.6436170212765957,"0
100
200
300
400
500
Number of filling samples 0 5 10 15 20"
EXPERIMENTS,0.6453900709219859,Time (seconds)
EXPERIMENTS,0.6471631205673759,The dimension is 2
EXPERIMENTS,0.648936170212766,"Short-step IPM
Specialized SSN"
EXPERIMENTS,0.650709219858156,"0
100
200
300
400
500
Number of filling samples 0 5 10 15 20"
EXPERIMENTS,0.6524822695035462,Time (seconds)
EXPERIMENTS,0.6542553191489362,The dimension is 5
EXPERIMENTS,0.6560283687943262,"Short-step IPM
Specialized SSN"
EXPERIMENTS,0.6578014184397163,"0
100
200
300
400
500
Number of filling samples 0 5 10 15 20 25"
EXPERIMENTS,0.6595744680851063,Time (seconds)
EXPERIMENTS,0.6613475177304965,The dimension is 10
EXPERIMENTS,0.6631205673758865,"Short-step IPM
Specialized SSN"
EXPERIMENTS,0.6648936170212766,Figure 3: Comparisons of mean computation time of IPM and our algorithm on CPU time.
EXPERIMENTS,0.6666666666666666,"By varying the dimension d ∈{2, 5, 10}, we also report the computation efﬁciency results in Figure 3.
266"
EXPERIMENTS,0.6684397163120568,"It indicates that the our new algorithm is more efﬁcient than the IPM as the number of ﬁlling points
267"
EXPERIMENTS,0.6702127659574468,"increases, with smaller variance in computation time (seconds).
268"
EXPERIMENTS,0.6719858156028369,"The experiments comparing kernel-based OT estimators with plug-in OT estimators on synthetic
269"
EXPERIMENTS,0.6737588652482269,"datasets have been conducted before [59, 38] and the results demonstrate that the kernel-based OT
270"
EXPERIMENTS,0.675531914893617,"estimators behave better when the number of samples is small. Here, we repeat such experiment but
271"
EXPERIMENTS,0.6773049645390071,"using the real-world 4i datasets from Bunne et al. [5], which contains single-cell perturbed responses,
272"
EXPERIMENTS,0.6790780141843972,"and which include the unperturbed cells and cells subject to drug perturbations. Our experiments are
273"
EXPERIMENTS,0.6808510638297872,"conducted on 15 datasets with different drug perturbations.
274"
EXPERIMENTS,0.6826241134751773,"Due to space limit, we defer the results to Appendix G (see Figure 4). We can see that the kernel-based
275"
EXPERIMENTS,0.6843971631205674,"OT estimators computed by our algorithm achieve satisfactory performance and behave better in most
276"
EXPERIMENTS,0.6861702127659575,"cases when the number of training samples is small; in particular, they better on 6 datasets, comparable
277"
EXPERIMENTS,0.6879432624113475,"on 5 datasets and worse on 4 datasets. Note that OTT computes the entropic regularized plug-in OT
278"
EXPERIMENTS,0.6897163120567376,"estimators and is heavily optimized to effectively handle noisy data. Therefore, it would be no surprise
279"
EXPERIMENTS,0.6914893617021277,"that OTT outperforms our algorithm when the number of training samples is sufﬁcient. However, the
280"
EXPERIMENTS,0.6932624113475178,"kernel-based OT estimation still provides a fairly effective alternative when the number of training
281"
EXPERIMENTS,0.6950354609929078,"samples is small, which is consistent with the previous observations on synthetic data [59, 38]. Our
282"
EXPERIMENTS,0.6968085106382979,"results also validate the effectiveness of our algorithm for computing kernel-based OT estimators.
283"
CONCLUDING REMARKS,0.6985815602836879,"5
Concluding Remarks
284"
CONCLUDING REMARKS,0.700354609929078,"In this paper, we propose a nonsmooth equation model for computing kernel-based OT estimators
285"
CONCLUDING REMARKS,0.7021276595744681,"and show that it has a special problem structure, allowing it to be solved in an efﬁcient manner using
286"
CONCLUDING REMARKS,0.7039007092198581,"semismooth Newton method. In particular, we propose a specialized semismooth Newton method that
287"
CONCLUDING REMARKS,0.7056737588652482,"achieves low per-iteration computational cost by exploiting the special problem structure, and prove
288"
CONCLUDING REMARKS,0.7074468085106383,"a global sublinear convergence rate and a local quadratic convergence rate under standard regularity
289"
CONCLUDING REMARKS,0.7092198581560284,"conditions. Preliminary experimental results on synthetic datasets show that our algorithm is more
290"
CONCLUDING REMARKS,0.7109929078014184,"efﬁcient than the short-step interior-point method [59], and the results on real data demonstrate the
291"
CONCLUDING REMARKS,0.7127659574468085,"effectiveness of our algorithm. Future work includes the applications of kernel-based OT estimators
292"
CONCLUDING REMARKS,0.7145390070921985,"to deep generative models and other real-world problems.
293"
REFERENCES,0.7163120567375887,"References
294"
REFERENCES,0.7180851063829787,"[1] A. Ali, E. Wong, and J. Z. Kolter. A semismooth Newton method for fast, generic convex
295"
REFERENCES,0.7198581560283688,"programming. In ICML, pages 70–79. PMLR, 2017. (Cited on page 8.)
296"
REFERENCES,0.7216312056737588,"[2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In ICML,
297"
REFERENCES,0.723404255319149,"pages 214–223, 2017. (Cited on page 1.)
298"
REFERENCES,0.725177304964539,"[3] P-C. Aubin-Frankowski and Z. Szabó. Hard shape-constrained kernel machines. In NeurIPS,
299"
REFERENCES,0.7269503546099291,"pages 384–395, 2020. (Cited on page 3.)
300"
REFERENCES,0.7287234042553191,"[4] N. Bonneel, J. Rabin, G. Peyré, and H. Pﬁster. Sliced and radon Wasserstein barycenters of
301"
REFERENCES,0.7304964539007093,"measures. Journal of Mathematical Imaging and Vision, 51(1):22–45, 2015. (Cited on page 1.)
302"
REFERENCES,0.7322695035460993,"[5] C. Bunne, S. G. Stark, G. Gut, J. S. del Castillo, K-V. Lehmann, L. Pelkmans, A. Krause, and
303"
REFERENCES,0.7340425531914894,"G. Ratsch. Learning single-cell perturbation responses using neural optimal transport. BioRxiv,
304"
REFERENCES,0.7358156028368794,"2021. (Cited on pages 9 and 17.)
305"
REFERENCES,0.7375886524822695,"[6] Y. Cai, A. Oikonomou, and W. Zheng. Finite-time last-iterate convergence for learning in
306"
REFERENCES,0.7393617021276596,"multi-player games. In NeurIPS, pages 33904–33919, 2022. (Cited on pages 8 and 16.)
307"
REFERENCES,0.7411347517730497,"[7] L. Chizat, P. Roussillon, F. Léger, F-X. Vialard, and G. Peyré. Faster Wasserstein distance
308"
REFERENCES,0.7429078014184397,"estimation with the Sinkhorn divergence. In NeurIPS, pages 2257–2269, 2020. (Cited on pages 1
309"
REFERENCES,0.7446808510638298,"and 2.)
310"
REFERENCES,0.7464539007092199,"[8] F. H. Clarke. Optimization and Nonsmooth Analysis. SIAM, 1990. (Cited on page 5.)
311"
REFERENCES,0.74822695035461,"[9] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy.
Optimal transport for domain
312"
REFERENCES,0.75,"adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1853–1865,
313"
REFERENCES,0.75177304964539,"2016. (Cited on page 1.)
314"
REFERENCES,0.7535460992907801,"[10] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal trans-
315"
REFERENCES,0.7553191489361702,"portation for domain adaptation. In NIPS, pages 3733–3742, 2017. (Cited on page 1.)
316"
REFERENCES,0.7570921985815603,"[11] M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS, pages
317"
REFERENCES,0.7588652482269503,"2292–2300, 2013. (Cited on page 1.)
318"
REFERENCES,0.7606382978723404,"[12] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In ICML, pages
319"
REFERENCES,0.7624113475177305,"685–693. PMLR, 2014. (Cited on page 1.)
320"
REFERENCES,0.7641843971631206,"[13] M. Cuturi, L. Meng-Papaxanthos, Y. Tian, C. Bunne, G. Davis, and O. Teboul. Optimal transport
321"
REFERENCES,0.7659574468085106,"tools (OTT): A jax toolbox for all things wasserstein. ArXiv Preprint: 2201.12324, 2022. (Cited
322"
REFERENCES,0.7677304964539007,"on page 17.)
323"
REFERENCES,0.7695035460992907,"[14] G. De Philippis and A. Figalli. The Monge-Ampère equation and its link to optimal trans-
324"
REFERENCES,0.7712765957446809,"portation. Bulletin of the American Mathematical Society, 51(4):527–580, 2014. (Cited on
325"
REFERENCES,0.7730496453900709,"page 3.)
326"
REFERENCES,0.774822695035461,"[15] N. Deb, P. Ghosal, and B. Sen. Rates of estimation of optimal transport maps using plug-in
327"
REFERENCES,0.776595744680851,"estimators via barycentric projections. In NeurIPS, pages 29736–29753, 2021. (Cited on page 2.)
328"
REFERENCES,0.7783687943262412,"[16] R. M. Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical
329"
REFERENCES,0.7801418439716312,"Statistics, 40(1):40–50, 1969. (Cited on page 1.)
330"
REFERENCES,0.7819148936170213,"[17] F. Facchinei and J-S. Pang. Finite-Dimensional Variational Inequalities and Complementarity
331"
REFERENCES,0.7836879432624113,"Problems. Springer Science & Business Media, 2007. (Cited on page 8.)
332"
REFERENCES,0.7854609929078015,"[18] F. Facchinei, A. Fischer, and C. Kanzow. Inexact Newton methods for semismooth equations
333"
REFERENCES,0.7872340425531915,"with applications to variational inequality problems. In Nonlinear Optimization and Applications,
334"
REFERENCES,0.7890070921985816,"pages 125–139. Springer, 1996. (Cited on page 8.)
335"
REFERENCES,0.7907801418439716,"[19] N. Fournier and A. Guillin. On the rate of convergence in wasserstein distance of the empirical
336"
REFERENCES,0.7925531914893617,"measure. Probability Theory and Related Fields, 162(3):707–738, 2015. (Cited on page 1.)
337"
REFERENCES,0.7943262411347518,"[20] C. Frogner, C. Zhang, H. Mobahi, M. Araya-Polo, and T. Poggio. Learning with a Wasserstein
338"
REFERENCES,0.7960992907801419,"loss. In NIPS, pages 2053–2061, 2015. (Cited on page 1.)
339"
REFERENCES,0.7978723404255319,"[21] A. Genevay, G. Peyré, and M. Cuturi. Learning generative models with Sinkhorn divergences.
340"
REFERENCES,0.799645390070922,"In AISTATS, pages 1608–1617, 2018. (Cited on page 1.)
341"
REFERENCES,0.8014184397163121,"[22] A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyré. Sample complexity of Sinkhorn
342"
REFERENCES,0.8031914893617021,"divergences. In AISTATS, pages 1574–1583. PMLR, 2019. (Cited on page 1.)
343"
REFERENCES,0.8049645390070922,"[23] N. Golowich, S. Pattathil, and C. Daskalakis. Tight last-iterate convergence rates for no-regret
344"
REFERENCES,0.8067375886524822,"learning in multi-player games. In NeurIPS, pages 20766–20778, 2020. (Cited on page 8.)
345"
REFERENCES,0.8085106382978723,"[24] N. Golowich, S. Pattathil, C. Daskalakis, and A. Ozdaglar. Last iterate is slower than averaged
346"
REFERENCES,0.8102836879432624,"iterate in smooth convex-concave saddle point problems. In COLT, pages 1758–1784. PMLR,
347"
REFERENCES,0.8120567375886525,"2020. (Cited on page 8.)
348"
REFERENCES,0.8138297872340425,"[25] N. Ho, X. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel clustering
349"
REFERENCES,0.8156028368794326,"via Wasserstein means. In ICML, pages 1501–1509. PMLR, 2017. (Cited on page 1.)
350"
REFERENCES,0.8173758865248227,"[26] J-C. Hütter and P. Rigollet. Minimax estimation of smooth optimal transport maps. The Annals
351"
REFERENCES,0.8191489361702128,"of Statistics, 49(2):1166–1194, 2021. (Cited on page 2.)
352"
REFERENCES,0.8209219858156028,"[27] H. Janati, T. Bazeille, B. Thirion, M. Cuturi, and A. Gramfort. Multi-subject MEG/EEG source
353"
REFERENCES,0.8226950354609929,"imaging with sparse multi-task regression. NeuroImage, 220:116847, 2020. (Cited on page 1.)
354"
REFERENCES,0.824468085106383,"[28] C. T. Kelley. Iterative Methods for Linear and Nonlinear Equations. SIAM, 1995. (Cited on
355"
REFERENCES,0.8262411347517731,"page 7.)
356"
REFERENCES,0.8280141843971631,"[29] S. Kolouri, K. Nadjahi, U. ¸Sim¸sekli, R. Badeau, and G. K. Rohde. Generalized sliced Wasser-
357"
REFERENCES,0.8297872340425532,"stein distances. In NIPS, pages 261–272, 2019. (Cited on page 1.)
358"
REFERENCES,0.8315602836879432,"[30] X. Li, D. Sun, and K-C. Toh. A highly efﬁcient semismooth Newton augmented Lagrangian
359"
REFERENCES,0.8333333333333334,"method for solving Lasso problems. SIAM Journal on Optimization, 28(1):433–458, 2018.
360"
REFERENCES,0.8351063829787234,"(Cited on page 14.)
361"
REFERENCES,0.8368794326241135,"[31] T. Lin, C. Fan, N. Ho, M. Cuturi, and M. I. Jordan. Projection robust Wasserstein distance and
362"
REFERENCES,0.8386524822695035,"Riemannian optimization. In NeurIPS, pages 9383–9397, 2020. (Cited on page 1.)
363"
REFERENCES,0.8404255319148937,"[32] T. Lin, Z. Zheng, E. Chen, M. Cuturi, and M. I. Jordan. On projection robust optimal transport:
364"
REFERENCES,0.8421985815602837,"Sample complexity and model misspeciﬁcation. In AISTATS, pages 262–270. PMLR, 2021.
365"
REFERENCES,0.8439716312056738,"(Cited on page 1.)
366"
REFERENCES,0.8457446808510638,"[33] Y. Liu, Z. Wen, and W. Yin. A multiscale semismooth Newton method for optimal transport.
367"
REFERENCES,0.8475177304964538,"Journal of Scientiﬁc Computing, 91(2):1–29, 2022. (Cited on page 14.)
368"
REFERENCES,0.849290780141844,"[34] T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman. Plugin estimation of smooth
369"
REFERENCES,0.851063829787234,"optimal transport maps. ArXiv Preprint: 2107.12364, 2021. (Cited on page 2.)
370"
REFERENCES,0.8528368794326241,"[35] J. Martínez and L. Qi. Inexact Newton methods for solving nonsmooth equations. Journal of
371"
REFERENCES,0.8546099290780141,"Computational and Applied Mathematics, 60(1-2):127–145, 1995. (Cited on page 8.)
372"
REFERENCES,0.8563829787234043,"[36] G. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: Sample complex-
373"
REFERENCES,0.8581560283687943,"ity and the central limit theorem. In NIPS, pages 4541–4551, 2019. (Cited on page 1.)
374"
REFERENCES,0.8599290780141844,"[37] R. Mifﬂin. Semismooth and semiconvex functions in constrained optimization. SIAM Journal
375"
REFERENCES,0.8617021276595744,"on Control and Optimization, 15(6):959–972, 1977. (Cited on pages 2 and 14.)
376"
REFERENCES,0.8634751773049646,"[38] B. Muzellec, A. Vacher, F. Bach, F-X. Vialard, and A. Rudi. Near-optimal estimation of smooth
377"
REFERENCES,0.8652482269503546,"transport maps with kernel sums-of-squares. ArXiv Preprint: 2112.01907, 2021. (Cited on
378"
REFERENCES,0.8670212765957447,"pages 2, 4, 8, and 9.)
379"
REFERENCES,0.8687943262411347,"[39] K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahrampour, and U. ¸Sim¸sekli. Statistical and
380"
REFERENCES,0.8705673758865248,"topological properties of sliced probability divergences. In NeurIPS, pages 20802–20812, 2020.
381"
REFERENCES,0.8723404255319149,"(Cited on page 1.)
382"
REFERENCES,0.874113475177305,"[40] J. Niles-Weed and P. Rigollet. Estimation of Wasserstein distances in the spiked transport model.
383"
REFERENCES,0.875886524822695,"Bernoulli, 28(4):2663–2688, 2022. (Cited on page 1.)
384"
REFERENCES,0.8776595744680851,"[41] F-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In ICML, pages 5072–5081.
385"
REFERENCES,0.8794326241134752,"PMLR, 2019. (Cited on page 1.)
386"
REFERENCES,0.8812056737588653,"[42] V. I. Paulsen and M. Raghupathi. An Introduction to The Theory of Reproducing Kernel Hilbert
387"
REFERENCES,0.8829787234042553,"Spaces, volume 152. Cambridge University Press, 2016. (Cited on page 3.)
388"
REFERENCES,0.8847517730496454,"[43] G. Peyré and M. Cuturi. Computational optimal transport: With applications to data science.
389"
REFERENCES,0.8865248226950354,"Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019. (Cited on page 1.)
390"
REFERENCES,0.8882978723404256,"[44] A-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. ArXiv
391"
REFERENCES,0.8900709219858156,"Preprint: 2109.12004, 2021. (Cited on page 2.)
392"
REFERENCES,0.8918439716312057,"[45] H. Qi and D. Sun. An augmented Lagrangian dual approach for the H-weighted nearest
393"
REFERENCES,0.8936170212765957,"correlation matrix problem. IMA Journal of Numerical Analysis, 31(2):491–511, 2011. (Cited
394"
REFERENCES,0.8953900709219859,"on page 14.)
395"
REFERENCES,0.8971631205673759,"[46] L. Qi and D. Sun. A survey of some nonsmooth equations and smoothing Newton methods. In
396"
REFERENCES,0.898936170212766,"Progress in Optimization, pages 121–146. Springer, 1999. (Cited on page 2.)
397"
REFERENCES,0.900709219858156,"[47] L. Qi and J. Sun. A nonsmooth version of Newton’s method. Mathematical Programming, 58
398"
REFERENCES,0.9024822695035462,"(1):353–367, 1993. (Cited on pages 2 and 14.)
399"
REFERENCES,0.9042553191489362,"[48] J. Rabin, G. Peyré, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture
400"
REFERENCES,0.9060283687943262,"mixing. In International Conference on Scale Space and Variational Methods in Computer
401"
REFERENCES,0.9078014184397163,"Vision, pages 435–446. Springer, 2011. (Cited on page 1.)
402"
REFERENCES,0.9095744680851063,"[49] I. Redko, N. Courty, R. Flamary, and D. Tuia. Optimal transport for multi-source domain
403"
REFERENCES,0.9113475177304965,"adaptation under target shift. In AISTATS, pages 849–858. PMLR, 2019. (Cited on page 1.)
404"
REFERENCES,0.9131205673758865,"[50] Y. Saad. Iterative Methods for Sparse Linear Systems. SIAM, 2003. (Cited on page 7.)
405"
REFERENCES,0.9148936170212766,"[51] T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport.
406"
REFERENCES,0.9166666666666666,"In ICLR, 2018. URL https://openreview.net/forum?id=rkQkBnJAb. (Cited on page 1.)
407"
REFERENCES,0.9184397163120568,"[52] F. Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations, PDEs,
408"
REFERENCES,0.9202127659574468,"and Modeling, volume 87. Birkhäuser, 2015. (Cited on page 3.)
409"
REFERENCES,0.9219858156028369,"[53] G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu,
410"
REFERENCES,0.9237588652482269,"S. Lin, and P. Berube. Optimal-transport analysis of single-cell gene expression identiﬁes
411"
REFERENCES,0.925531914893617,"developmental trajectories in reprogramming. Cell, 176(4):928–943, 2019. (Cited on page 1.)
412"
REFERENCES,0.9273049645390071,"[54] M. V. Solodov and B. F. Svaiter. A globally convergent inexact Newton method for systems
413"
REFERENCES,0.9290780141843972,"of monotone equations. Reformulation: Nonsmooth, Piecewise Smooth, Semismooth and
414"
REFERENCES,0.9308510638297872,"Smoothing Methods, pages 355–369, 1999. (Cited on page 14.)
415"
REFERENCES,0.9326241134751773,"[55] S. Srivastava, V. Cevher, Q. Dinh, and D. Dunson. WASP: Scalable Bayes via barycenters of
416"
REFERENCES,0.9343971631205674,"subset posteriors. In AISTATS, pages 912–920. PMLR, 2015. (Cited on page 1.)
417"
REFERENCES,0.9361702127659575,"[56] D. Sun and J. Sun. Semismooth matrix-valued functions. Mathematics of Operations Research,
418"
REFERENCES,0.9379432624113475,"27(1):150–169, 2002. (Cited on page 15.)
419"
REFERENCES,0.9397163120567376,"[57] I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein auto-encoders. In ICLR,
420"
REFERENCES,0.9414893617021277,"2018. (Cited on page 1.)
421"
REFERENCES,0.9432624113475178,"[58] M. Ulbrich. Semismooth Newton Methods for Variational Inequalities and Constrained Opti-
422"
REFERENCES,0.9450354609929078,"mization Problems in Function Spaces. SIAM, 2011. (Cited on pages 2 and 14.)
423"
REFERENCES,0.9468085106382979,"[59] A. Vacher, B. Muzellec, A. Rudi, F. Bach, and F-X. Vialard. A dimension-free computational
424"
REFERENCES,0.9485815602836879,"upper-bound for smooth optimal transport estimation. In COLT, pages 4143–4173. PMLR,
425"
REFERENCES,0.950354609929078,"2021. (Cited on pages 1, 2, 3, 4, 8, and 9.)
426"
REFERENCES,0.9521276595744681,"[60] C. Villani. Optimal Transport: Old and New, volume 338. Springer, 2009. (Cited on pages 1
427"
REFERENCES,0.9539007092198581,"and 3.)
428"
REFERENCES,0.9556737588652482,"[61] C. Wang, D. Sun, and K-C. Toh. Solving log-determinant optimization problems by a Newton-
429"
REFERENCES,0.9574468085106383,"CG primal proximal point algorithm. SIAM Journal on Optimization, 20(6):2994–3013, 2010.
430"
REFERENCES,0.9592198581560284,"(Cited on page 14.)
431"
REFERENCES,0.9609929078014184,"[62] J. Weed and F. Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
432"
REFERENCES,0.9627659574468085,"measures in Wasserstein distance. Bernoulli, 25(4A):2620–2648, 2019. (Cited on page 1.)
433"
REFERENCES,0.9645390070921985,"[63] J. Weed and Q. Berthet. Estimation of smooth densities in Wasserstein distance. In COLT,
434"
REFERENCES,0.9663120567375887,"pages 3118–3119. PMLR, 2019. (Cited on page 2.)
435"
REFERENCES,0.9680851063829787,"[64] X. Xiao, Y. Li, Z. Wen, and L. Zhang. A regularized semismooth Newton method with projection
436"
REFERENCES,0.9698581560283688,"steps for composite convex programs. Journal of Scientiﬁc Computing, 76(1):364–389, 2018.
437"
REFERENCES,0.9716312056737588,"(Cited on page 14.)
438"
REFERENCES,0.973404255319149,"[65] J. Yang, D. Sun, and K-C. Toh. A proximal point algorithm for log-determinant optimization
439"
REFERENCES,0.975177304964539,"with group Lasso regularization. SIAM Journal on Optimization, 23(2):857–893, 2013. (Cited
440"
REFERENCES,0.9769503546099291,"on page 14.)
441"
REFERENCES,0.9787234042553191,"[66] K. D. Yang, K. Damodaran, S. Venkatachalapathy, A. C. Soylemezoglu, G. V. Shivashankar,
442"
REFERENCES,0.9804964539007093,"and C. Uhler. Predicting cell lineages using autoencoders and optimal transport. PLoS Compu-
443"
REFERENCES,0.9822695035460993,"tational Biology, 16(4):e1007828, 2020. (Cited on page 1.)
444"
REFERENCES,0.9840425531914894,"[67] L. Yang, D. Sun, and K-C. Toh. SDPNAL++: a majorized semismooth Newton-CG augmented
445"
REFERENCES,0.9858156028368794,"Lagrangian method for semideﬁnite programming with nonnegative constraints. Mathematical
446"
REFERENCES,0.9875886524822695,"Programming Computation, 7(3):331–366, 2015. (Cited on page 14.)
447"
REFERENCES,0.9893617021276596,"[68] X-Y. Zhao, D. Sun, and K-C. Toh. A Newton-CG augmented Lagrangian method for semideﬁnite
448"
REFERENCES,0.9911347517730497,"programming. SIAM Journal on Optimization, 20(4):1737–1765, 2010. (Cited on pages 6, 7,
449"
REFERENCES,0.9929078014184397,"and 14.)
450"
REFERENCES,0.9946808510638298,"[69] G. Zhou and K-C. Toh. Superlinear convergence of a Newton-type algorithm for monotone
451"
REFERENCES,0.9964539007092199,"equations. Journal of Optimization Theory and Applications, 125(1):205–221, 2005. (Cited on
452"
REFERENCES,0.99822695035461,"page 14.)
453"
