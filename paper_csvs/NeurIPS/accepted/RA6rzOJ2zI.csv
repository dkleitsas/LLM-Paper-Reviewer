Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002551020408163265,"Inrecentyears, DynamicSparseTraining(DST)hasemergedasanalternativetopost-
training pruning for generating efficient models. In principle, DST allows for a more
memory efficient training process, as it maintains sparsity throughout the entire train-
ing run. However, current DST implementations fail to capitalize on this in practice.
Becausesparsematrixmultiplicationismuchlessefficientthandensematrixmultipli-
cation on GPUs, most implementations simulate sparsity by masking weights. In this
paper, we leverage recent advances in semi-structured sparse training to apply DST
in the domain of classification with large output spaces, where memory-efficiency is
paramount. With a label space of possibly millions of candidates, the classification
layer alone will consume several gigabytes of memory. Switching from a dense to
a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however,
severely hampers training convergence, especially at the largest label spaces. We
find that poor gradient flow from the sparse classifier to the dense text encoder make
it difficult to learn good input representations. By employing an intermediate layer or
adding an auxiliary training objective, we recover most of the generalisation perfor-
mance of the dense model. Overall, we demonstrate the applicability and practical
benefits of DST in a challenging domain — characterized by a highly skewed label
distribution that differs substantially from typical DST benchmark datasets — which
enables end-to-end training with millions of labels on commodity hardware."
INTRODUCTION,0.00510204081632653,"1
Introduction"
INTRODUCTION,0.007653061224489796,"Recent research [1, 2, 3, 4] has demonstrated that densely-connected neural networks contain sparse
subnetworks — often dubbed “winning lottery tickets” — that can deliver performance comparable to
the full networks but with substantially reduced compute and memory demands. Unlike conventional
techniques that start with a trained dense model and employ iterative pruning or one-shot pruning,
Dynamic Sparse Training (DST) [5, 6, 7] initializes a sparse architecture and dynamically explores
subnetwork configurations through periodic pruning and regrowth, typically informed with heuristic
saliency criteria such as weight and gradient magnitudes. This approach is particularly advantageous
in scenarios constrained by a fixed memory budget during the training phase, making DST viable
across various domains [4, 8, 9]. For instance, in reinforcement learning [10, 11], DST has been
shown to significantly outperform traditional dense models. Additionally, models trained using DST
often exhibit enhanced robustness [12, 13, 14, 15, 16]. However, the application of DST comes with
challenges, notably prolonged training times; for example, RigL [6] and ITOP [7] require up to five
and two times as many optimization steps during training, respectively, to match the generalisation"
INTRODUCTION,0.01020408163265306,encoder W S
INTRODUCTION,0.012755102040816327,+Intermediate
INTRODUCTION,0.015306122448979591,encoder S DST
INTRODUCTION,0.017857142857142856,encoder S Aux
INTRODUCTION,0.02040816326530612,+Auxiliary
INTRODUCTION,0.02295918367346939,label space size
K,0.025510204081632654,"31k
4k
670k"
K,0.02806122448979592,"0.70
0.75
0.80
0.85
0.90
0.95
Sparsity 0.1 0.2 0.3 0.4 0.5"
K,0.030612244897959183,Precision at 1
K,0.03316326530612245,Amazon670K
K,0.03571428571428571,"Dense
Static Sparsity
DST+Auxiliary
DST"
K,0.03826530612244898,"Figure 1: Model configurations and performance comparisons at various sparsity levels. The left
panel illustrates our model configurations: ‘S’ represents a semi-structured fixed fan-in sparse layer,
‘W’ denotes an intermediate layer, and ‘Aux’ refers to an auxiliary head of meta-classifiers. These
configurations help maintain performance as the label space size increases from 31K to 670K and
beyond. The right panel demonstrates the comparative precision at 1 for our model against other
methods across increasing levels of sparsity on the Amazon670K dataset."
K,0.04081632653061224,"performance of dense networks at high sparsity levels (≥80%). The prolonged training time in these
works is often linked to the need for in-time overparameterization [7] and poor gradient flow in sparse
networks. Recent advances [17, 18, 19, 20] aimed at improving gradient flow have been introduced
to mitigate these extended training durations, enhancing the practicality of DST methodologies."
K,0.04336734693877551,"In this paper, we investigate the integration of DST into extreme multi-label classification (XMC)
[21, 22, 23]. XMC problems are characterized by a very large label space, with hundreds of thousands
to millions of labels, often in the same order of magnitude as the number of training examples. The
large label space in such problems makes calculating logits for every label a very costly operation.
Consequently, contemporary XMC methodologies [24, 25, 26, 27, 28, 29, 30, 31] utilize modular and
sampling-based techniques to achieve sublinear compute costs. However, these strategies do not help
in addressing the immense memory requirement associated with the classification layer, which can
be enormous: for an embedding dimension of 768, one million labels lead to a memory consumption of
about 12 GB taking into account weights, gradients, and optimizer state.1 Memory efficiency in XMC
has been pursued in the context of sparse linear models [22, 32, 33] or by using label-hashing [34],
but such methods do not yield predictive performance competitive with modern transformer-based
deep networks. Schultheis and Babbar [35] demonstrated that applying a DST method to the extreme
classification layer can lead to substantial memory savings at marginal accuracy drops; however, that
work presupposed the existence of fixed, well-trained document embeddings which output the hidden
representations used by the classifier, whereas in a realistic setting these need to be trained jointly."
K,0.04591836734693878,"Recently, Jain et al. [36] demonstrated that full end-to-end training of XMC models can be very
successful, given sufficient computational resources. To make this accessible to consumer-grade
hardware, we propose to switch the dense classification layer to a DST-trained sparse layer. Not
only does this result in a training procedure that allows XMC models to be trained in a GPU-memory
constrained setting, but it also provides an evaluation of DST algorithms outside typical, well-behaved
benchmarks. This is particularly important since recent works [37, 38] have found that sparse training
algorithms that appear promising on standard benchmark datasets may fail to produce adequate results
on actual real-world tasks. As such, we introduce XMC problems — with their long-tailed label
distribution [39, 40, 41], missing labels [39, 42, 43, 44], and general training data scarcity issues [32]
— as a new setting to challenge current sparsity approaches."
K,0.04846938775510204,"In fact, direct application of existing DST methods yields unsatisfactory results on XMC tasks due to
typically noisy data and poor gradient signal propagation through the sparse classifier, slowing training
convergence to an extent that it is not practically useful. Consequently, we follow Schultheis and
Babbar [35] and adapt the model architecture by integrating an intermediate layer that is larger than the"
K,0.05102040816326531,"1Using mixed precision with torch.amp has little benefit here, because optimizer states and model parameters
are still maintained in 32 bit, and only down-converted to speed-up matrix multiplications."
K,0.05357142857142857,"embedding from the encoder but still significantly smaller than the final output layer. While this was
found to be sufficient to achieve good results with fixed encodings, it fails if the encoder is a trainable
transformer [45, 46] for label spaces with more than one hundred thousand elements, particularly at
high levels of sparsity. The primary challenge arises from the noisy gradients prevalent at the onset
of training, which are inadequate for guiding the fine-tuning of the encoder effectively. To mitigate
this issue, we introduce an auxiliary loss. This loss uses a more coarse-grained objective, assigning
instances to clusters of labels, where scores for each cluster are calculated using a dense classification
layer. This auxiliary component stabilizes the gradient flow and enhances the encoder’s adaptability
during the critical early phases of training and is turned off during later epochs to not interfere with
the main task. Figure 1 illustrates the architectural changes that ensure good training performance
at different label space sizes and sparsity levels."
K,0.05612244897959184,"To materialize actual memory savings, we propose SPARTEX, which uses semi-structured spar-
sity [47, 35] with a fixed fan-in constraint, together with magnitude-based pruning and random
regrowth (SET [5]), which does not require any additional memory buffers. In our experiments, we
show that SPARTEX achieves a 3.4-fold reduction of GPU memory requirements from 46.3 to 13.5 GiB
for training on the Amazon-3M [21] dataset, with only an approximately 3% reduction in predictive
performance. In comparison, a naïve parameter reduction using a bottleneck layer at the same memory
budget decreases precision by about 6%."
K,0.058673469387755105,Our primary contributions are as follows:
K,0.061224489795918366,"• Enhancements in training efficiency: We propose novel modifications to the conventional
DST framework that significantly curtail training durations while delivering competitive
performance metrics when benchmarked against dense model baselines and other specialized
XMC methodologies. These enhancements are pivotal in demonstrating DST’s scalability
and efficiency to large label spaces.
• Optimized hardware utilization: We provide PyTorch bindings for custom CUDA kernels2"
K,0.06377551020408163,"which enable a streamlined integration of memory-efficient sparse training into an existing
XMC pipeline. This implementation enables the deployment of our training methodologies on
conventional, commercially available hardware, thus democratizing access to state-of-the-art
XMC model training.
• Robustness to Label distribution challenges: Our empirical results demonstrate that the
DST framework, as adapted and optimized by our modifications, can effectively manage
datasets characterized by label imbalances and the presence of missing labels, with minimal
performance degradation for tail labels."
DYNAMIC SPARSE TRAINING FOR EXTREME MULTI-LABEL CLASSIFICATION,0.0663265306122449,"2
Dynamic Sparse Training for Extreme Multi-label Classification"
BACKGROUND,0.06887755102040816,"2.1
Background"
BACKGROUND,0.07142857142857142,"Problem setup
Given a multi-label training dataset with N samples, D = {(xi,Pi)N
i=1}, where
L represents the total number of labels, and Pi ⊂[L] denotes a subset of relevant labels associated
with the data point xi ∈χ. Typically, the instances are text based, such as the contents of a Wikipedia
article [21] or the title of a product on Amazon [48] with labels corresponding to Wikipedia categories
and frequently bought together products, respectively, for example. Traditional XMC methods used
to handle labels the same way as is typically done in other fields, as featureless integers."
BACKGROUND,0.07397959183673469,"However, the labels themselves usually carry some information, e.g., a textual representation, as the
following examples, taken from (i) LF-AmazonTitles-131K (recommend related products given a
product name) and (ii) LF-WikiTitles-500K (predict relevant categories, given the title of a Wikipedia
page) illustrate:"
BACKGROUND,0.07653061224489796,"Example 1: For “Nintendo Land” on Amazon, we have available: Mario Tennis Ultra Smash(Nintendo
Wii U) | Star Fox Zero (Nintendo Wii U), as the recommended products."
BACKGROUND,0.07908163265306123,"Example 2: For the “2024 United States presidential election” Wikipedia page, we have the available
categories: Joe Biden | Joe Biden 2024 presidential campaign | Donald Trump | Donald Trump 2024
presidential campaign | Kamala Harris | November 2024 events in the United States."
BACKGROUND,0.08163265306122448,2Code is available at https://github.com/xmc-aalto/NeurIPS24-dst
BACKGROUND,0.08418367346938775,"Consequently, more recent XMC approaches have started to take these label features into account
to alleviate the data scarcity problems [27, 49]."
BACKGROUND,0.08673469387755102,"XMC and DST
XMC models are typically comprised of two main components: (i) an encoder Eθ :
χ→Rd, which embeds data points into a d-dimensional real space, primarily utilizing a transformer ar-
chitecture [50] and (ii) A One-vs-All classifier W ={wl}l∈[L], where wl denotes the classifier for label
l, integrated as the last layer of the neural network in end-to-end training settings. In a typical DST sce-
nario, one would sparsify the language model used as the encoder, potentially even leaving the classifier
fully dense [51]. However, in XMC, most of the networks weights are in the classifier layer, so in order
to achieve a reduction in memory consumption, its weight matrix Ws ={ws
l }l∈[L] must be sparsified."
BACKGROUND,0.08928571428571429,"This sparse layer Ws is then periodically updated in a prune-regrow-train loop, that is, every ∆T steps, a
fraction of active weights is pruned and the same number of inactive weights are regrown. The updated
sparse topology is then trained with regular gradient descent for the next ∆T steps. There are many
possible choices for pruning and regrowth criteria [52]; to keep memory consumption low, however, we
need to choose a method that does not require auxiliary buffers proportional to the size of the dense layer.
Thisexcludesmethodssuchasrequiringsecond-orderinformation[53], ortrackingofdensegradientsor
otherper-weightinformation[54,55,56]. Evcietal.[6]arguethatRigLonlyneedsdensegradientsinan
ephemeral capacity — they can be discarded as soon as the regrowth step for the current layer is done, but
beforetheregrowstepofthenextlayerisstarted—butintheXMCsetup, theprohibitivelylargememory
consumption arises already from a single layer. Therefore, we select magnitude-based pruning and
random regrowth [5]. Magnitude-based pruning has been shown to be a remarkably strong baseline [57]."
BACKGROUND,0.09183673469387756,"However, to actually achieve efficient training with these algorithms in the XMC setting, several
challenges need to be overcome as discussed below."
BACKGROUND,0.09438775510204081,"2.2
Memory-Efficient Training: Fixed Fan-In Sparse Layer"
BACKGROUND,0.09693877551020408,"Unstructured sparsity is notoriously difficult to speed-up on GPUs [58], and consequently most DST
studies simulate sparsity by means of a binary mask [19, 59]. On the other hand, highly structured
sparsity, such as 2:4 sparsity [60], enjoys hardware acceleration and memory reduction [61], but
may result in deteriorated model accuracy compared to unstructured sparsity [62]. As a compromise,
semi-structured sparsity [63, 47, 35] imposes a fixed fan-in to each neuron. This eliminates work
imbalances between different neurons, leading to an efficient and simple storage format for sparse
weights, where each sparse weight needs only a single integer index, resulting in ELLPACK format [64]
without any padding. For 32-bit floating point weights with 16-bit indices (i.e., at most 65k features
in the embedding layer), this leads to a 50% storage overhead for sparse weights; however, for training,
gradient and two momentum terms are needed, which share the same indexing structure, reducing
the effective overhead to just 12.5%."
BACKGROUND,0.09948979591836735,"While fixed fan-in provides substantial speed-ups for the forward pass, due to the transposed matrix
multiplication required for gradients, it does not give any direct benefits for the backwards pass.
Fortunately, when used for the classification matrix, the backpropagated gradient is the loss derivative,
which will exhibit high activation sparsity if the loss function is hinge-like [35]. In the enormous label
space of XMC, for each instance only a small subset of labels will be hard negatives. The rest will
be easily classified as true negatives, and not contribute to the backward pass."
BACKGROUND,0.10204081632653061,"As additional measures to keep the memory consumption low, we enable torch.amp automatic
mixed-precision training [65] and activation checkpointing [66] for the BERT encoder."
BACKGROUND,0.10459183673469388,"2.3
Improved Gradient Flow: Auxiliary Objective"
BACKGROUND,0.10714285714285714,"We find that, despite using a fully dense network, training the encoder using gradients backpropagated
from a sparse classification layer requires more optimization steps to converge compared with to a
dense classification layer. This compounds with the already-increased number of epochs required
for DST [6, 7], further increasing the training duration of end-to-end XMC training, which requires
longer training than comparable modularized or shortlisting-based methods [36]. Furthermore, the
intermediate activations in the transformer-based encoder also take up a considerable amount of
GPU memory, so to meet memory budgets, we may need to switch to smaller batch sizes or employ
activation checkpointing, increasing the per-step time."
BACKGROUND,0.1096938775510204,"Therefore, we need to improve gradient flow through the sparse layer. Schultheis and Babbar [35]
inserted a large intermediate layer preceding the actual classifier to achieve significant improvements
in performance. While this method is sufficient to achieve good results with fixed encodings, we
observe that it fails to perform well if the encoder is a trainable transformer [45, 46] for label spaces
with more than one hundred thousand elements, particularly for high sparsity levels. Therefore, we
instead propose to use the label shortlisting task that is typical for XMC pipelines as an auxiliary
objective to generate informative gradients for the encoder."
BACKGROUND,0.11224489795918367,"Rethinking the role of clusters and meta-classifier in XMC
Many prevailing XMC methods,
apart from learning the per-label classifiers W = {wl}l∈[L] for the extreme task, also employ a
meta-classifier. The meta-classifier learns to classify over clusters of similar labels that are created by
recursively partitioning the label set into equal parts using, for example, balanced k-means clustering
[24, 26, 29, 25, 67]. These meta-classifiers are primarily used for label shortlisting or retrieval prior
to the final classification or re-ranking at the extreme scale. We investigated the impact on the final
performance of the extreme task when the labels are randomly assigned to the clusters (instead of
the following the k-means objective). We observed that such reassignments do not negatively affect
the extreme task’s performance (detailed of this observation are shown in Appendix E). This leads
us to hypothesize that beyond merely shortlisting labels, meta-classifier branch of the XMC training
pipelines provides useful gradient signals during encoder training, which is particularly crucial for
larger datasets with O(106) labels such as Amazon-670K (Figure 1) and Amazon-3M."
BACKGROUND,0.11479591836734694,"0.0
0.2
0.4
0.6
0.8
1.0
Training Step 1e6 10
3 10
2 10
1 100"
BACKGROUND,0.11734693877551021,Gradient Norm
BACKGROUND,0.11989795918367346,Amazon-670K (sparsity=96%)
BACKGROUND,0.12244897959183673,"with auxiliary
without auxiliary"
BACKGROUND,0.125,"Figure 2: Gradient Flow of the encoder during train-
ing with and without Auxiliary Objective."
BACKGROUND,0.12755102040816327,"Auxiliary objective and DST convergence for
XMC
Towards addressing the challenge of
gradient instability, we augment our training
pipeline with an additional meta-classifier branch
which aids gradient information during back-
propagation. This is especially useful during
the initial training phase where the fixed-fan-in
sparse layer tends to encounter difficulties. Im-
portantly, in our model the output layer operates
independently of the meta-classifier’s outputs,
enabling a seamless end-to-end training process."
BACKGROUND,0.13010204081632654,"Although a meta-classifier assists during the ini-
tial stages of training, maintaining it throughout
the entire training process can deteriorate the en-
coder’s representation quality. This degradation
occurs because the task associated with the meta classifiers differs from the final task, yet both share the
same encoder. Similar observations have been noted in related studies [26]. To address this issue, we
implement a decaying scaling factor on the auxiliary loss, gradually reducing its influence as training
progresses."
BACKGROUND,0.1326530612244898,"The impact of the auxiliary branch on the norm of the gradient in the encoder is demonstrated in
Figure 2 for the Amazon-670K dataset. The larger gradient signal speeds up initial learning, but it is
misaligned with the true objective, so is gradually turned off at around 200k steps. Furthermore, the
improvement in prediction performance, as reflected in Figure 1 (right panel), reinforces the quality of
gradient as compared to the training pipeline without the auxiliary objective."
EXPERIMENTS AND DISCUSSION,0.13520408163265307,"3
Experiments and discussion"
DATASETS,0.1377551020408163,"3.1
Datasets"
DATASETS,0.14030612244897958,"In this study, we evaluate our proposed modifications of DST under the extreme classification
setting across a diverse set of large-scale datasets, including Wiki10-31K [68], Wiki-500K [21],
Amazon-670K [48], and Amazon-3M [69]. The datasets are publicly available at the Extreme
Classification Repository3. These were selected due to their inherent complexity and the challenges
posed by their long-tailed label distributions, which are emblematic of real-world data scenarios and
test the robustness of DST methodologies. Further validation of our approach is conducted using the"
DATASETS,0.14285714285714285,3http://manikvarma.org/downloads/XC/XMLRepository.html
DATASETS,0.14540816326530612,"Table 1: Statistics of XMC Datasets with and without Label Features. This table presents a comparison
across various datasets, detailing the total number of training instances (N), unique labels (L), number
of test instances (N ′), average label count per instance (L), and average data points per label (ˆL)."
DATASETS,0.14795918367346939,"Dataset
N
L
N ′
L
ˆL"
DATASETS,0.15051020408163265,Datasets without Label Features
DATASETS,0.15306122448979592,"Wiki10-31K
14,146
30,938
6616
18.64
48.52
Wiki-500K
1,779,881
501,070
769,421
4.75
16.86
Amazon-670K
490,449
670,091
153,025
5.45
3.99
Amazon-3M
1,717,899 2,812,281
742,507
36.17
31.64"
DATASETS,0.1556122448979592,Datasets with Label Features
DATASETS,0.15816326530612246,"LF-AmazonTitles-131K
294,805
131,073
134,835
5.15
2.29
LF-WikiSeeAlso-320K
693,082
312,330
177,515
4.67
2.11"
DATASETS,0.16071428571428573,"datasets LF-AmazonTitles-131K and LF-WikiSeeAlso-320K. These datasets are particularly relevant
due to their augmentation with rich label metadata and their concise text formats, traits that have
gained considerable traction in the XMC research community of late. The datasets’ detailed statistical
profiles are delineated in Table 1."
BASELINES AND EVALUATION METRICS,0.16326530612244897,"3.2
Baselines and evaluation metrics"
BASELINES AND EVALUATION METRICS,0.16581632653061223,"To ensure a comprehensive and fair evaluation of our proposed DST methodologies applied to XMC
problems, we compare our proposed framework, SPARTEX, across three principal categories of baseline
methods:"
BASELINES AND EVALUATION METRICS,0.1683673469387755,"1. Dense Models: Consistent with traditional DST evaluations, we compare the performance of
our sparse models against their dense counterparts."
BASELINES AND EVALUATION METRICS,0.17091836734693877,"2. Dense Models with bottleneck layer: This category (referred to as Dense BN in Table 2)
includes dense models with the same number of parameters as our proposed DST method
by having a bottleneck layer with the same dimensionality as the FFI size. This ensures that
comparisons focus on the impact of sparsity rather than differences in model size or capacity."
BASELINES AND EVALUATION METRICS,0.17346938775510204,"3. XMC Methods: For datasets devoid of label features, we benchmark against the lat-
est transformer-based models such as CASCADEXML [26], LIGHTXML [24], and XR-
TRANSFORMER. For datasets that incorporate label features, our comparison includes leading
Siamese methods like SIAMESEXML [27] and NGAME[28], as well as other relevant
transformer-based approaches."
BASELINES AND EVALUATION METRICS,0.1760204081632653,"Notably, RENEE [36] qualifies as both a dense model and a state-of-the-art XMC method. However, in
some instances, RENEE employs larger encoders (e.g., Roberta-Large [70]). To maintain consistency
and fairness in our evaluations, we exclude configurations employing larger encoders from this analysis.
For conceptual validation, we used RIGL[6] on datasets with label spaces up to 670K."
BASELINES AND EVALUATION METRICS,0.17857142857142858,"As is standard in XMC literature, we compare the methods on metrics which only consider prediction
at top-k slots. This includes : Precision@k and its propensity-scored variant (which is more sensitive to
performance on tail-labels). The details of these metrics are given in Appendix A."
EMPIRICAL PERFORMANCE,0.18112244897959184,"3.3
Empirical performance"
EMPIRICAL PERFORMANCE,0.1836734693877551,"Table 2 presents our primary results on the datasets, compared with the aforementioned baselines.
The performance metrics for XMC baselines are reported from their original papers. However, for
peak memory consumption, we re-ran these baselines in half precision with the same batch size, as
all baselines are also evaluated in half precision. Following DST protocols, we extended the training
duration for RIGL, Dense Bottleneck, and our method to twice the number of steps used for dense
models. Certain baselines (referred to as OOM) could not scale to the Amazon-3M dataset. Our
results demonstrate that our method significantly reduces memory usage while maintaining competitive
performance. On the Amazon-3M dataset, our approach delivers comparable performance to dense"
EMPIRICAL PERFORMANCE,0.18622448979591838,"Table 2: Comparison with different methods. Comparing our sparse model against its dense counterpart
and with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3M
datasets. Mtr(GiB) indicates peak GPU memory consumption during training."
EMPIRICAL PERFORMANCE,0.18877551020408162,"Method
Sparsity
(%)
P@1
P@3
P@5
Mtr
(GiB)"
EMPIRICAL PERFORMANCE,0.1913265306122449,"Sparsity
(%)
P@1
P@3
P@5
Mtr
(GiB)"
EMPIRICAL PERFORMANCE,0.19387755102040816,"Wiki10-31K
Wiki-500K"
EMPIRICAL PERFORMANCE,0.19642857142857142,"ATTENTIONXML
-
87.1
77.8
68.8
8.2
-
75.1
56.5
44.4
13.1
LIGHTXML
-
87.8
77.3
68.0
16.5
-
76.2
57.2
44.1
14.6
CASCADEXML
-
88.4
78.3
68.9
8.2
-
77.0
58.3
45.1
18.8"
EMPIRICAL PERFORMANCE,0.1989795918367347,"DENSE
-
87.8
77.2
68.1
2.5
-
78.5
59.2
45.6
9"
EMPIRICAL PERFORMANCE,0.20153061224489796,"DENSE BN
-
86.7
76.3
66.0
2.1
-
73.8
55.1
42.0
4.3
RIGL
92
87.7
77.3
67.7
2.6
83
74.5
54.7
41.8
9.7
SPARTEX
92
88.6
77.7
67.4
2.1
83
76.7
57.8
44.5
4.1"
EMPIRICAL PERFORMANCE,0.20408163265306123,"Amazon-670K
Amazon-3M"
EMPIRICAL PERFORMANCE,0.2066326530612245,"ATTENTIONXML
-
45.7
40.7
36.9
10.7
-
49.1
46.0
43.9
71.2
LIGHTXML
-
47.1
42.0
38.2
11.2
-
-
-
OOM
CASCADEXML
-
48.5
43.7
40.0
18.3
-
51.3
49.0
46.9
87.0"
EMPIRICAL PERFORMANCE,0.20918367346938777,"DENSE
-
49.8
44.2
40.1
11.5
-
53.4
50.6
48.5
46.3"
EMPIRICAL PERFORMANCE,0.21173469387755103,"DENSE BN
-
44.5
39.7
36.1
4.0
-
47.0
44.6
42.7
13.1
RIGL
83
45.2
38.7
36.0
12.4
83
-
-
-
OOM
SPARTEX
83
47.1
41.8
38.0
3.7
83
50.2
47.1
44.8
13.5"
EMPIRICAL PERFORMANCE,0.21428571428571427,"models while achieving a 3.4-fold reduction in memory usage and a 5-fold reduction compared to
the XMC baseline. Furthermore, within the memory-efficient model regime, our method consistently
outperforms the Dense Bottleneck model. To further validate the robustness of our approach, we
evaluated it on the label features datasets, as shown in Table 3. Notably, as the label space size increases,
we need to adjust to a comparatively lower sparsity to maintain performance, discussed in detail in
subsequent sections."
EMPIRICAL PERFORMANCE,0.21683673469387754,"3.4
Adapting to increased sparsity and label size: the role of auxiliary objective"
EMPIRICAL PERFORMANCE,0.2193877551020408,"The DST approach is widely recognized to be problematic when dealing with high sparsity levels
(≥90%). This is also apparent in our experiment and can be observed in Figure 3 (right) when the label
space size is constant. Our findings indicate that incorporating an auxiliary objective significantly aids"
EMPIRICAL PERFORMANCE,0.22193877551020408,"Table 3: Comparison on label feature datasets LF-AmazonTitles-131K and LF-WikiSeeAlso-320K.
Mtr(GiB) indicates peak GPU memory usage during training."
EMPIRICAL PERFORMANCE,0.22448979591836735,"Method
Sparsity
(%)
P@1
P@3
P@5
Mtr
(GiB)"
EMPIRICAL PERFORMANCE,0.22704081632653061,"Sparsity
(%)
P@1
P@3
P@5
Mtr
(GiB)"
EMPIRICAL PERFORMANCE,0.22959183673469388,"LF-AmazonTitles-131K
LF-WikiSeeAlso-320K"
EMPIRICAL PERFORMANCE,0.23214285714285715,"LIGHTXML
-
35.6
24.2
17.5
11.6
-
34.5
22.3
16.8
13.5
ECLARE
-
40.7
27.5
19.9
8.8
-
40.6
26.9
20.1
10.2
SIAMESEXML
-
41.4
27.9
21.2
7.1
-
42.2
28.1
21.4
8.9
NGAME
-
46
30.3
21.5
9.0
-
47.7
31.6
23.6
19.3
DEXML
-
42.5
-
20.6
30.2
-
46.1
29.9
22.3
56.1"
EMPIRICAL PERFORMANCE,0.23469387755102042,"RENEE (Dense)
-
46.1
30.8
22
3.0
-
47.9
31.9
24.1
9.1"
EMPIRICAL PERFORMANCE,0.2372448979591837,"DENSE BN
-
39.2
25.7
18.2
2.2
-
44.5
28.4
21.5
6.0
RIGL
83
43.0
28.6
20.4
3.2
67
44.9
29.0
21.7
9.2
SPARTEX
83
44.5
29.8
21.3
2.2
67
46.0
29.9
22.1
6.1 31000"
EMPIRICAL PERFORMANCE,0.23979591836734693,131073
EMPIRICAL PERFORMANCE,0.2423469387755102,501070
EMPIRICAL PERFORMANCE,0.24489795918367346,670091
EMPIRICAL PERFORMANCE,0.24744897959183673,2812281
EMPIRICAL PERFORMANCE,0.25,Label Space Size 14 12 10 8 6 4 2 0
EMPIRICAL PERFORMANCE,0.25255102040816324,P@1 Performance Drop (%) -0.9% 3.5% 2.3% 5.2% 6.0% 1.3% 14.1% 6.0% 10.5% 12.0% 0.2%
EMPIRICAL PERFORMANCE,0.25510204081632654,"3.5%
3.4% 8.7% 11.4%"
EMPIRICAL PERFORMANCE,0.2576530612244898,"FFI+Aux
Small Dense
FFI"
EMPIRICAL PERFORMANCE,0.2602040816326531,"0
20
40
60
80
100
Epoch 0.0 0.1 0.2 0.3 0.4 P@1"
EMPIRICAL PERFORMANCE,0.2627551020408163,FFI + Aux FFI
EMPIRICAL PERFORMANCE,0.2653061224489796,FFI+Aux
EMPIRICAL PERFORMANCE,0.26785714285714285,FFI+Aux Gap
EMPIRICAL PERFORMANCE,0.27040816326530615,Label Space Size = 670K
EMPIRICAL PERFORMANCE,0.2729591836734694,"sparsity=0.96
sparsity=0.96
sparsity=0.83
sparsity=0.92"
EMPIRICAL PERFORMANCE,0.2755102040816326,Sparsity=0.83
EMPIRICAL PERFORMANCE,0.2780612244897959,"Figure 3: left: Comparison of performance declines as the size of the label space increases, given a
fixed sparsity. right: Performance of our model at different epochs, across various sparsity ratios."
EMPIRICAL PERFORMANCE,0.28061224489795916,"in maintaining performance, particularly in the high sparsity regime. Conversely, at lower sparsity
levels (≤67%), the benefit of the auxiliary objective diminishes. In the context of XMC problems, the
performance of DST degrades as the label space size increases. Figure 3 (left) depicts the performance
degradation of our approach relative to a dense baseline across datasets with increasing label space sizes:
31K, 131K, 500K, 670K, and 3M (detailed in the Table 1), all evaluated at 83% sparsity. Interestingly,
for the wiki31K dataset, we observe a performance improvement, potentially due to the lower number
of training samples relative to the label space size. Compared to other methods with equivalent memory
requirements, our approach demonstrates superior performance retention at larger label space sizes."
EFFECT OF REWIRING INTERVAL,0.28316326530612246,"3.5
Effect of Rewiring Interval"
EFFECT OF REWIRING INTERVAL,0.2857142857142857,"The rewiring interval is crucial for balancing the trade-off between under-exploration and unreliable
exploration. In XMC problems, tail label performance is particularly significant due to its application
domain. The rewiring interval directly influences how frequently each parameter topology encounters
tail label examples before updates. In this section, we focus on assessing the performance impact
of various rewiring intervals, including their effect on tail labels. We conducted experiments on
the LF-AmazonTitles-131K dataset using rewiring intervals ∆T ∈[100,500,700,1000,2000]. The
corresponding results for P@1 and PSP@1 metrics are illustrated in Figures 4 left and right, respectively,
with a fixed rewiring fraction of 0.15. Our findings reveal that both P@1 and PSP@1 improve as the
interval increases up to a certain point. Interestingly, while P@1 shows a decline beyond this threshold,
PSP@1 continues to rise. This divergence suggests that larger rewiring intervals, despite potentially
limiting the diversity of topology exploration, provide each topology sufficient exposure to more tail
labels, thereby improving model performance in handling rare categories."
PERFORMANCE ON TAIL LABELS,0.288265306122449,"3.6
Performance on Tail Labels"
PERFORMANCE ON TAIL LABELS,0.29081632653061223,"Table 4 presents a comparison of Propensity-Scored Precision (PSP) for various Extreme Multi-
label Classification (XMC) models, including ATTENTIONXML [25], CASCADEXML [26], Dense,
Dense Bottleneck, and our proposed method, across four benchmark datasets: Wiki10-31K, Wiki-
500K, Amazon-670K, and Amazon-3M. For the Wiki10-31K dataset, our model achieves PSP@1 of
13.2, PSP@3 of 15.1, and PSP@5 of 16.4, surpassing the Dense model. On the Wiki-500K dataset,
our method records a PSP@1 of 31.9, outperforming other XMC models and closely trailing the
top-performing approaches. These findings underscore our model’s consistent performance across
varied datasets, frequently exceeding or closely competing with XMC and Dense benchmarks. It’s
noteworthy that the performance of ATTENTIONXML on Wiki10-31K is attributed to its utilization of
an LSTM encoder, which is particularly advantageous given the dataset’s smaller number of training
samples relative to its label space. This configuration also explains our model’s superior performance
compared to the Dense model, which incorporates a form of regularization. In comparisons involving"
PERFORMANCE ON TAIL LABELS,0.29336734693877553,"Table 4: Propensity-Scored Precision (PSP) comparison of our sparse model with its dense counterpart
and state-of-the-art XMC methods on the Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3M
datasets. The same sparsity levels as mentioned in previous tables are used."
PERFORMANCE ON TAIL LABELS,0.29591836734693877,"Method
PSP@1
PSP@3
PSP@5
PSP@1
PSP@3
PSP@5"
PERFORMANCE ON TAIL LABELS,0.29846938775510207,"Wiki10-31K
Wiki-500K"
PERFORMANCE ON TAIL LABELS,0.3010204081632653,"ATTENTIONXML
16.2
17.1
17.9
30.1
37.3
41.7
CASCADEXML
13.2
14.7
16.1
31.3
39.4
43.3
DENSE
10.6
12.6
13.9
32.5
41.0
44.9
DENSE BN
12.4
14.5
16.0
28.5
36.5
40.1
SPARTEX
13.2
15.1
16.4
31.9
39.8
43.4"
PERFORMANCE ON TAIL LABELS,0.30357142857142855,"Amazon-670K
Amazon-3M"
PERFORMANCE ON TAIL LABELS,0.30612244897959184,"ATTENTIONXML
29.3
32.4
35.1
15.5
18.5
20.6
CASCADEXML
30.2
34.9
38.8
-
-
-
DENSE
33.3
36.9
39.9
15.6
19.0
21.5
DENSE BN
26.9
30.9
34.5
13.5
16.4
18.6
SPARTEX
29.9
33.3
36.4
14.3
17.2
19.4"
PERFORMANCE ON TAIL LABELS,0.3086734693877551,"memory efficiency, our approach significantly surpasses the same-capacity Dense Bottleneck model,
demonstrating its suitability in resource-constrained settings where tail-label performance is critical."
IMPACT OF VARYING SPARSITY LEVELS,0.3112244897959184,"3.7
Impact of Varying Sparsity Levels"
IMPACT OF VARYING SPARSITY LEVELS,0.3137755102040816,"Table 5 illustrates the impact of varying sparsity levels (ranging from 50% to 96%) in conjunction
with the use of auxiliary loss for Amazon-670K dataset. As sparsity levels increase, there are benefits
in memory usage, training time, and inference time; however, performance metrics simultaneously
decline. Additionally, the importance of auxiliary loss becomes particularly significant at higher
sparsity levels."
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.3163265306122449,"3.8
Sensitivity to Auxiliary Loss cut-off epochs"
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.31887755102040816,"We employ auxiliary loss with an initial scalar weight that decays until a specified cut-off epoch.
Table 6 illustrates the model’s final performance at various cut-off epochs for two sparsity levels. A
value of 0 (No aux) indicates the absence of auxiliary loss, while ’No cut-off’ signifies its application
throughout training. Our analysis reveals that prolonging the auxiliary loss beyond optimal cut-off
epochs adversely affects performance for both sparsity levels. Notably, maintaining the auxiliary loss
throughout training leads to performance deterioration, resulting in scores lower than those achieved
without its use."
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.32142857142857145,"250
500
750
1000
1250
1500
1750
2000
Rewiring Interval ( T) 0.420 0.425 0.430 0.435 0.440 0.445"
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.3239795918367347,Precision at 1 (P@1)
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.32653061224489793,rewire fraction=0.15
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.32908163265306123,"FFI+Aux
RigL"
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.33163265306122447,"250
500
750
1000
1250
1500
1750
2000
Rewiring Interval ( T) 0.345 0.350 0.355 0.360 0.365 0.370 0.375 0.380 0.385"
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.33418367346938777,Propensity scored Precision at 1 (PSP@1)
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.336734693877551,rewire fraction=0.15
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.3392857142857143,"FFI+Aux
RigL"
SENSITIVITY TO AUXILIARY LOSS CUT-OFF EPOCHS,0.34183673469387754,"Figure 4: Effect of rewiring interval on final performance for Precision@1 (left) and propensity-scored
Precision@1 (right) in the LF-AmazonTitles-131K dataset."
DST WITH FIXED EMBEDDING VS END-TO-END TRAINING,0.34438775510204084,"3.9
DST with Fixed Embedding vs End-to-End Training"
DST WITH FIXED EMBEDDING VS END-TO-END TRAINING,0.3469387755102041,"In Table 7, we compare the performance of models using fixed embeddings [35] with trained end-to-end
using DST on the Wiki-500K and Amazon-670K datasets. End-to-end training yields consistent
improvements over fixed embeddings across all metrics, with significant gains in P@1 (an increase of
3.1% on Wiki-500K and 4.5% on Amazon-670K). These highlight the need of enabling the model to
adapt its representations while training for the best possible performance."
CONCLUSION AND FUTURE WORK,0.3494897959183674,"4
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.3520408163265306,"In this paper, we demonstrated the feasibility of DST for end-to-end training of classifiers with hundreds
of thousands of labels. When appropriately adapted for XMC problems with fixed fan-in sparsity
and an auxiliary objective, DST offers a significant reduction in peak memory usage while delivering
superior performance compared to bottleneck-based weight reduction. It is anticipated that the Python
bindings of the CUDA kernels will be useful for the research community in making their existing and
forthcoming deep XMC pipelines more memory efficient. We hope that our work will enable further
research towards developing techniques which could be (i) combined with explicit negative mining
strategies, and (ii) used in conjunction with larger transformer encoders such as Roberta-large."
CONCLUSION AND FUTURE WORK,0.35459183673469385,"Table 5: Comparison of Fan-in (sparsity) effects on model performance and memory usage for Amazon-
670K dataset."
CONCLUSION AND FUTURE WORK,0.35714285714285715,"Fan in (sparsity)
Auxiliary Loss
P@1
P@3
P@5
Mtr (GiB)
Epoch Time
(mins)
Inference Time
(ms)"
CONCLUSION AND FUTURE WORK,0.3596938775510204,"384 (50%)
No
49.0
43.5
39.5
7.03
18:13
10.2
384 (50%)
Yes
49.2
43.7
39.6
7.13
18:19
10.2
256 (67%)
No
47.0
41.6
37.7
5.27
15:45
9.18
256 (67%)
Yes
47.6
42.3
38.4
5.36
15:50
9.18
128 (83%)
No
45.6
40.2
36.3
3.60
13:20
8.54
128 (83%)
Yes
47.1
41.8
38.0
3.70
13:23
8.54
64 (92%)
No
30.7
26.5
23.6
2.88
12:12
8.14
64 (92%)
Yes
42.3
37.2
33.3
2.97
12:13
8.14
32 (96%)
No
5.5
5.0
4.6
2.52
11:36
7.94
32 (96%)
Yes
38.4
33.8
30.4
2.61
11:38
7.94"
CONCLUSION AND FUTURE WORK,0.3622448979591837,"Table 6: Comparison of Auxiliary loss cut-off epoch effects on model performance for different Fan-in
(sparsity) levels."
CONCLUSION AND FUTURE WORK,0.3647959183673469,"Fan in: 128 (83% Sparsity)
Fan in: 64 (92% Sparsity)"
CONCLUSION AND FUTURE WORK,0.3673469387755102,"Auxiliary cut-off epoch
P@1
P@3
P@5
Auxiliary cut-off epoch
P@1
P@3
P@5"
CONCLUSION AND FUTURE WORK,0.36989795918367346,"0 (No aux)
45.6
40.2
36.3
0 (No aux)
30.7
26.5
23.6
15
47.1
41.8
38.0
15
42.3
37.2
33.3
30
46.6
41.1
37.1
30
32.8
28.3
25.2
60
45.9
40.6
36.6
60
32.5
28.0
24.9
90
44.6
39.7
35.9
90
31.3
27.0
23.9
No cut-off (full training)
42.1
37.4
33.7
No cut-off (full training)
22.7
17.7
14.4"
CONCLUSION AND FUTURE WORK,0.37244897959183676,"Table 7: Performance comparison between fixed CascadeXML [26] embeddings and end-to-end
training with DST on Wiki-500K and Amazon-670K datasets."
CONCLUSION AND FUTURE WORK,0.375,"Wiki-500K
Amazon-670K"
CONCLUSION AND FUTURE WORK,0.37755102040816324,"Method
P@1
P@3
P@5
P@1
P@3
P@5"
CONCLUSION AND FUTURE WORK,0.38010204081632654,"Fixed
73.6
54.8
42.1
42.6
37.1
33.1
End-to-End
76.7
57.8
44.5
47.1
41.8
38.0"
LIMITATIONS AND SOCIETAL IMPACT,0.3826530612244898,"5
Limitations and societal impact"
LIMITATIONS AND SOCIETAL IMPACT,0.3852040816326531,"For XMC tasks, this paper attempts to explore the landscape of sparse neural networks, which can be
trained on affordable and easily accessible commodity GPUs. While the proposed scheme is able to
achieve comparable results to state-of-the-art methods at a fraction of GPU memory consumption, it
is unable to surpass the baseline with dense last layer on all occasions. The exact relative decline in
prediction performance, when our proposed adaptations of Fixed Fan-In and auxiliary objective are
employed, is shown in Figure 3."
LIMITATIONS AND SOCIETAL IMPACT,0.3877551020408163,"While we do not anticipate any negative societal impact of our work, it is expected that it will further
enable the exploration of novel training methodologies for deep networks which are more affordable
and easily accessible to a broader research community outside the big technology companies."
ACKNOWLEDGEMENTS,0.3903061224489796,"6
Acknowledgements"
ACKNOWLEDGEMENTS,0.39285714285714285,"We thank Niki Loppi of NVIDIA AI Technology Center Finland for useful discussions on the sparse
CUDA kernel implementations. YI acknowledges the support of Alberta Innovates (ALLRP-577350-
22, ALLRP-222301502), the Natural Sciences and Engineering Research Council of Canada (RGPIN-
2022-03120, DGECR-2022-00358), and Defence Research and Development Canada (DGDND-2022-
03120). This research was enabled in part by support provided by the Digital Research Alliance of
Canada (alliancecan.ca). RB acknowledges the support of Academy of Finland (Research Council of
Finland) via grants 347707 and 348215. NU acknowledges the support of computational resources
provided by the Aalto Science-IT project, and CSC IT Center for Science, Finland."
REFERENCES,0.39540816326530615,References
REFERENCES,0.3979591836734694,"[1] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.4005102040816326,"[2] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity
and the lottery ticket hypothesis. In International Conference on Machine Learning, pages 3259–3269.
PMLR, 2020."
REFERENCES,0.4030612244897959,"[3] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:
Pruning is all you need. In International Conference on Machine Learning, pages 6682–6691. PMLR, 2020."
REFERENCES,0.40561224489795916,"[4] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and Zhangyang
Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision
models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages
16306–16316, 2021."
REFERENCES,0.40816326530612246,"[5] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and
Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by
network science. Nature communications, 9(1):2383, 2018."
REFERENCES,0.4107142857142857,"[6] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making
all tickets winners. In International conference on machine learning, pages 2943–2952. PMLR, 2020."
REFERENCES,0.413265306122449,"[7] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need dense
over-parameterization? in-time over-parameterization in sparse training. In International Conference on
Machine Learning, pages 6989–7000. PMLR, 2021."
REFERENCES,0.41581632653061223,"[8] Anastasia Dietrich, Frithjof Gressmann, Douglas Orr, Ivan Chelombiev, Daniel Justus, and Carlo Luschi.
Towards structured dynamic sparse pre-training of bert. arXiv preprint arXiv:2108.06277, 2021."
REFERENCES,0.41836734693877553,"[9] Shiwei Liu, Iftitahu Ni’mah, Vlado Menkovski, Decebal Constantin Mocanu, and Mykola Pechenizkiy.
Efficient and effective training of sparse recurrent neural networks. Neural Computing and Applications, 33:
9625–9636, 2021."
REFERENCES,0.42091836734693877,"[10] Yiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, and Longbo Huang. Rlx2: Training a sparse deep reinforcement
learning model from scratch. In The Eleventh International Conference on Learning Representations, 2022."
REFERENCES,0.42346938775510207,"[11] Laura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training in deep
reinforcement learning. In International Conference on Machine Learning, pages 7766–7792. PMLR, 2022."
REFERENCES,0.4260204081632653,"[12] Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. Sparse dnns with improved adversarial
robustness. Advances in neural information processing systems, 31, 2018."
REFERENCES,0.42857142857142855,"[13] Ozan Özdenizci and Robert Legenstein. Training adversarially robust sparse networks via bayesian connec-
tivity sampling. In International Conference on Machine Learning, pages 8314–8324. PMLR, 2021."
REFERENCES,0.43112244897959184,"[14] Tianlong Chen, Zhenyu Zhang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang, et al.
Sparsitywinningtwice: Betterrobustgeneralizationfrommoreefficienttraining. InInternationalConference
on Learning Representations, 2021."
REFERENCES,0.4336734693877551,"[15] James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand:
Compressing deep networks can improve out-of-distribution robustness. Advances in neural information
processing systems, 34:664–676, 2021."
REFERENCES,0.4362244897959184,"[16] Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola
Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead for
either training or testing: The all-round blessings of dynamic sparsity. In 10th International Conference on
Learning Representations, ICLR 2022. OpenReview, 2022."
REFERENCES,0.4387755102040816,"[17] Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin. Gradient flow in sparse neural networks and
how lottery tickets win. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages
6577–6586, 2022."
REFERENCES,0.4413265306122449,"[18] Ilan Price and Jared Tanner. Dense for the price of sparse: Improved performance of sparsely initialized
networks via a subspace offset. In International Conference on Machine Learning, pages 8620–8629. PMLR,
2021."
REFERENCES,0.44387755102040816,"[19] Selima Curci, Decebal Constantin Mocanu, and Mykola Pechenizkiyi. Truly sparse neural networks at scale.
arXiv preprint arXiv:2102.01732, 2021."
REFERENCES,0.44642857142857145,"[20] Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao,
Shivani Agrawal, Utku Evci, and Tushar Krishna. Progressive gradient flow for robust n: M sparsity training
in transformers. arXiv e-prints, pages arXiv–2402, 2024."
REFERENCES,0.4489795918367347,"[21] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classifica-
tion repository: Multi-label datasets and code, 2016. URL http://manikvarma.org/downloads/XC/
XMLRepository.html."
REFERENCES,0.45153061224489793,"[22] Rohit Babbar and Bernhard Schölkopf. Dismec: Distributed sparse machines for extreme multi-label
classification. In Proceedings of the tenth ACM international conference on web search and data mining,
pages 721–729, 2017."
REFERENCES,0.45408163265306123,"[23] Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Varma. Parabel: Partitioned
label trees for extreme classification with application to dynamic search advertising. In Proceedings of the
2018 World Wide Web Conference, pages 993–1002, 2018."
REFERENCES,0.45663265306122447,"[24] Ting Jiang, Deqing Wang, Leilei Sun, Huayi Yang, Zhengyang Zhao, and Fuzhen Zhuang. Lightxml:
Transformer with dynamic negative sampling for high-performance extreme multi-label text classification.
In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 7987–7994, 2021."
REFERENCES,0.45918367346938777,"[25] Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. Attentionxml:
Label tree-based attention-aware deep model for high-performance extreme multi-label text classification.
Advances in neural information processing systems, 32, 2019."
REFERENCES,0.461734693877551,"[26] Siddhant Kharbanda, Atmadeep Banerjee, Erik Schultheis, and Rohit Babbar. Cascadexml: Rethinking
transformers for end-to-end multi-resolution training in extreme multi-label classification. Advances in
neural information processing systems, 35:2074–2087, 2022."
REFERENCES,0.4642857142857143,"[27] Kunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh, Sumeet Agarwal,
Purushottam Kar, and Manik Varma. Siamesexml: Siamese networks meet extreme classifiers with 100m
labels. In International conference on machine learning, pages 2330–2340. PMLR, 2021."
REFERENCES,0.46683673469387754,"[28] Kunal Dahiya, Nilesh Gupta, Deepak Saini, Akshay Soni, Yajun Wang, Kushal Dave, Jian Jiao, Gururaj K,
Prasenjit Dey, Amit Singh, et al. Ngame: Negative mining-aware mini-batching for extreme classification.
In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages
258–266, 2023."
REFERENCES,0.46938775510204084,"[29] Jiong Zhang, Wei-Cheng Chang, Hsiang-Fu Yu, and Inderjit Dhillon. Fast multi-resolution transformer
fine-tuning for extreme multi-label text classification. Advances in Neural Information Processing Systems,
34:7267–7280, 2021."
REFERENCES,0.4719387755102041,"[30] Erik Schultheis and Rohit Babbar. Speeding-up one-versus-all training for extreme classification via
mean-separating initialization. Machine Learning, 111(11):3953–3976, 2022."
REFERENCES,0.4744897959183674,"[31] Mohammadreza Qaraei and Rohit Babbar. Meta-classifier free negative sampling for extreme multilabel
classification. Machine Learning, 113(2):675–697, 2024."
REFERENCES,0.4770408163265306,"[32] Rohit Babbar and Bernhard Schölkopf. Data scarcity, robustness and extreme multi-label classification.
Machine Learning, 108(8):1329–1351, 2019."
REFERENCES,0.47959183673469385,"[33] Ian En-Hsu Yen, Xiangru Huang, Pradeep Ravikumar, Kai Zhong, and Inderjit Dhillon. Pd-sparse: A primal
and dual sparse approach to extreme multiclass and multilabel classification. In International conference on
machine learning, pages 3069–3077. PMLR, 2016."
REFERENCES,0.48214285714285715,"[34] Tharun Kumar Reddy Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava.
Extreme classification in log memory using count-min sketch: A case study of amazon search with 50m
products. Advances in Neural Information Processing Systems, 32, 2019."
REFERENCES,0.4846938775510204,"[35] Erik Schultheis and Rohit Babbar. Towards memory-efficient training for extremely large output spaces–
learning with 670k labels on a single commodity gpu. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pages 689–704. Springer, 2023."
REFERENCES,0.4872448979591837,"[36] Vidit Jain, Jatin Prakash, Deepak Saini, Jian Jiao, Ramachandran Ramjee, and Manik Varma. Renee:
End-to-end training of extreme classification models. Proceedings of Machine Learning and Systems, 5,
2023."
REFERENCES,0.4897959183673469,"[37] Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, Ajay Jaiswal, and Zhangyang Wang.
Sparsity may cry: Let us fail (current) sparse neural networks together! arXiv preprint arXiv:2303.02141,
2023."
REFERENCES,0.4923469387755102,"[38] Ajay Kumar Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. Compressing
LLMs: The truth is rarely pure and never simple. In The Twelfth International Conference on Learning
Representations, 2024."
REFERENCES,0.49489795918367346,"[39] HimanshuJain, YashotejaPrabhu, andManikVarma. Extrememulti-labellossfunctionsforrecommendation,
tagging, ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining, pages 935–944, 2016."
REFERENCES,0.49744897959183676,"[40] Anirudh Buvanesh, Rahul Chand, Jatin Prakash, Bhawna Paliwal, Mudit Dhawan, Neelabh Madan, Deepesh
Hada, Vidit Jain, SONU MEHTA, Yashoteja Prabhu, et al. Enhancing tail performance in extreme classifiers
by label variance reduction. In The Twelfth International Conference on Learning Representations, 2023."
REFERENCES,0.5,"[41] Erik Schultheis, Marek Wydmuch, Wojciech Kotlowski, Rohit Babbar, and Krzysztof Dembczynski. Gen-
eralized test utilities for long-tail performance in extreme multi-label classification. Advances in Neural
Information Processing Systems, 36, 2024."
REFERENCES,0.5025510204081632,"[42] Mohammadreza Qaraei, Erik Schultheis, Priyanshu Gupta, and Rohit Babbar. Convex surrogates for
unbiased loss functions in extreme classification with missing labels. In Proceedings of the Web Conference
2021, pages 3711–3720, 2021."
REFERENCES,0.5051020408163265,"[43] Erik Schultheis, Marek Wydmuch, Rohit Babbar, and Krzysztof Dembczynski. On missing labels, long-tails
and propensities in extreme multi-label classification. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, pages 1547–1557, 2022."
REFERENCES,0.5076530612244898,"[44] Erik Schultheis and Rohit Babbar. Unbiased loss functions for multilabel classification with missing labels.
arXiv preprint arXiv:2109.11282, 2021."
REFERENCES,0.5102040816326531,"[45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5127551020408163,"[46] V Sanh. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Proceedings of
Thirty-third Conference on Neural Information Processing Systems (NIPS2019), 2019."
REFERENCES,0.5153061224489796,"[47] Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. Dynamic sparse training with
structured sparsity. In The Twelfth International Conference on Learning Representations, 2024."
REFERENCES,0.5178571428571429,"[48] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with
review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165–172, 2013."
REFERENCES,0.5204081632653061,"[49] Anshul Mittal, Noveen Sachdeva, Sheshansh Agrawal, Sumeet Agarwal, Purushottam Kar, and Manik
Varma. Eclare: Extreme classification with label graph correlations. In Proceedings of the Web Conference
2021, pages 3721–3732, 2021."
REFERENCES,0.5229591836734694,"[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,
2017."
REFERENCES,0.5255102040816326,"[51] Eldar Kurtic, Torsten Hoefler, and Dan Alistarh. How to prune your language model: Recovering accuracy
on the “sparsity may cry” benchmark. In Conference on Parsimony and Learning, pages 542–553. PMLR,
2024."
REFERENCES,0.5280612244897959,"[52] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:
Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning
Research, 22(241):1–124, 2021."
REFERENCES,0.5306122448979592,"[53] Babak Hassibi and David Stork. Second order derivatives for network pruning: Optimal brain surgeon.
Advances in neural information processing systems, 5, 1992."
REFERENCES,0.5331632653061225,"[54] Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. Nest: A neural network synthesis tool based on a grow-and-
prune paradigm. IEEE Transactions on Computers, 68(10):1487–1497, 2019."
REFERENCES,0.5357142857142857,"[55] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing perfor-
mance. arXiv preprint arXiv:1907.04840, 2019."
REFERENCES,0.5382653061224489,"[56] Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with
feedback. In International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SJem8lSFwB."
REFERENCES,0.5408163265306123,"[57] Aleksandra Nowak, Bram Grooten, Decebal Constantin Mocanu, and Jacek Tabor. Fantastic weights and
how to find them: Where to prune in dynamic sparse training. Advances in Neural Information Processing
Systems, 36, 2024."
REFERENCES,0.5433673469387755,"[58] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning. In SC20:
International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–14.
IEEE, 2020."
REFERENCES,0.5459183673469388,"[59] Joo Hyung Lee, Wonpyo Park, Nicole Elyse Mitchell, Jonathan Pilault, Johan Samir Obando Ceron, Han-
Byul Kim, Namhoon Lee, Elias Frantar, Yun Long, Amir Yazdanbakhsh, Woohyun Han, Shivani Agrawal,
Suvinay Subramanian, Xin Wang, Sheng-Chun Kao, Xingyao Zhang, Trevor Gale, Aart J.C. Bik, Milen
Ferev, Zhonglin Han, Hong-Seok Kim, Yann Dauphin, Gintare Karolina Dziugaite, Pablo Samuel Castro,
and Utku Evci. Jaxpruner: A concise library for sparsity research. In Conference on Parsimony and Learning
(Proceedings Track), 2023. URL https://openreview.net/forum?id=H2rCZCfXkS."
REFERENCES,0.548469387755102,"[60] Roberto L Castro, Andrei Ivanov, Diego Andrade, Tal Ben-Nun, Basilio B Fraguela, and Torsten Hoefler.
Venom: A vectorized n: M format for unleashing the power of sparse tensor cores. In Proceedings of the
International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–14,
2023."
REFERENCES,0.5510204081632653,"[61] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu,
and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378,
2021."
REFERENCES,0.5535714285714286,"[62] Shiwei Liu and Zhangyang Wang. Ten lessons we have learned in the new ""sparseland"": A short handbook
for sparse neural network researchers, 2023. URL https://arxiv.org/abs/2302.02596."
REFERENCES,0.5561224489795918,"[63] Saurav Muralidharan. Uniform sparsity in deep neural networks. Proceedings of Machine Learning and
Systems, 5, 2023."
REFERENCES,0.5586734693877551,"[64] John R Rice and Ronald F Boisvert. Solving elliptic problems using ELLPACK. Springer Series in
Computational Mathematics. Springer New York, 1985."
REFERENCES,0.5612244897959183,"[65] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In
International Conference on Learning Representations, 2018."
REFERENCES,0.5637755102040817,"[66] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost.
arXiv preprint arXiv:1604.06174, 2016."
REFERENCES,0.5663265306122449,"[67] Siddhant Kharbanda, Atmadeep Banerjee, Devaansh Gupta, Akash Palrecha, and Rohit Babbar. Incep-
tionxml: A lightweight framework with synchronized negative sampling for short text extreme classification.
In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Informa-
tion Retrieval, pages 760–769, 2023."
REFERENCES,0.5688775510204082,"[68] Arkaitz Zubiaga. Enhancing navigation on wikipedia with social tags. arXiv preprint arXiv:1202.5469,
2012."
REFERENCES,0.5714285714285714,"[69] Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and complementary
products. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and
data mining, pages 785–794, 2015."
REFERENCES,0.5739795918367347,"[70] Yinhan Liu. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692,
364, 2019."
REFERENCES,0.576530612244898,"A
Evaluation Metrics"
REFERENCES,0.5790816326530612,"To evaluate the performance of our Extreme Multi-label Text Classification (XMC) model, which
incorporates Dynamic Sparse Training, we use a set of metrics designed to provide a comprehensive
analysis of both overall and label-specific model performance. The primary metrics we employ is
Precision at k (P@k), which assess the accuracy of the top-k predictions. Additionally, we incorporate
Propensity-Scored Precision at k (PSP@k), Macro Precision at k (Macro P@k) and Macro Recall at k
(Macro R@k) to gauge the uniformity of the model’s effectiveness across the diverse range of labels
typical in XMC problems."
REFERENCES,0.5816326530612245,"Precision at k (P@k):
Precision at k is the fundamental metric for evaluating the top-k predictions
in XMC applications such as e-commerce product recommendation and document tagging:"
REFERENCES,0.5841836734693877,"P@k(y,ˆy)= 1 k X"
REFERENCES,0.5867346938775511,"ℓ∈topk(ˆy)
yℓ
(1)"
REFERENCES,0.5892857142857143,"where y is the true label vector, ˆy is the predicted score vector, and topk(ˆy) identifies the indices with
the top-k highest predicted scores."
REFERENCES,0.5918367346938775,"Propensity-Scored Precision at k (PSP@k):
Given the long-tailed label distribution in many XMC
datasets, PSP@k incorporates a propensity score yl to weight the precision contribution of each label,
thereby emphasizing the tail labels’ performance:"
REFERENCES,0.5943877551020408,"PSP@k(y,ˆy)= 1 k X"
REFERENCES,0.5969387755102041,"ℓ∈topk(ˆy) yℓ
pℓ (2)"
REFERENCES,0.5994897959183674,where pl corresponds to the propensity score for the label yl [39].
REFERENCES,0.6020408163265306,"Macro Precision at k (Macro P@k):
To capture the average precision across all labels and mitigate
any label imbalance, Macro Precision at k is used:"
REFERENCES,0.6045918367346939,"Macro P@k= 1 L L
X i=1 P"
REFERENCES,0.6071428571428571,"ℓ∈topk(ˆyi)yiℓ
min(k, |topk(ˆyi)|) ! (3)"
REFERENCES,0.6096938775510204,"B
Baselines and Related Works"
REFERENCES,0.6122448979591837,"Despite the Dense and Same Capacity Dense baseline we also compare our approach with different
State of the Art XMC methods and DST methods."
REFERENCES,0.6147959183673469,"XMC Methods
We compare our method with deep XMC methods with mainly transformer encoder."
REFERENCES,0.6173469387755102,"• AttentionXML[25]: The model segments labels using a shallow and wide PLT with a depth
between 2 and 3, learning a specific context vector for each label to create label-adapted
datapoint representations."
REFERENCES,0.6198979591836735,"• LightXML [24]: The method employs a transformer encoder to concurrently train both the
retriever and ranker, which incorporates dynamic negative sampling to enhance the model’s
efficacy."
REFERENCES,0.6224489795918368,"• XR-Transformer [29]: XR-Transformer employs a multi-resolution training approach,
iteratively training and freezing the transformer before re-clustering and re-training classifiers
at various resolutions using fixed features."
REFERENCES,0.625,"• CascadeXML [26]: This method separates the feature learning of distinct tasks across various
layers of the Probabilistic Label Tree (PLT) and aligns them with corresponding layers of the
transformer encoder."
REFERENCES,0.6275510204081632,"• ECLARE [49]: This model utilizes label graphs to improve label representations, focusing
specifically on enhancing performance for rare labels. The label graph is generated through
random walks using the label vectors."
REFERENCES,0.6301020408163265,"• SiameseXML [27]: This approach combines Siamese networks with one-vs-all classifiers.
SiameseXML utilizes multiple ANNS structures to retrieve label shortlists. These shortlisted
labels are subsequently ranked based on scores from label-wise one-vs-all classifiers."
REFERENCES,0.6326530612244898,"• NGAME [28]: NGAME enhances transformer-based training for extreme classification by
introducing a negative mining-aware mini-batching technique, which supports larger batch
sizes and accelerates convergence by optimizing the handling of negative samples."
REFERENCES,0.6352040816326531,"• Renee [36]: The Renee model employs an integrated end-to-end training approach for extreme
classification, using a novel loss shortcut for memory optimization and a hybrid data-model
parallel architecture to enhance training efficiency and scalability."
REFERENCES,0.6377551020408163,"DST Methods
Existing DST methods vary in their pruning and growing criteria. Recent studies
[57] indicate that magnitude-based pruning is effective in DST, while dense weight information is
impractical for Extreme Multi-label Classification (XMC). We evaluate key methods on select datasets
for conceptual validation."
REFERENCES,0.6403061224489796,"• RigL [6]: RigL uses weight magnitude and dense gradient magnitudes for the pruning and
regrowth saliency criteria, respectively. While RigL only needs the dense gradient information
during network topology updates, this is a prohibitive requirement in the XMC setting due to
the large memory consumption of the final classification layer. RigL learns an unstructured
sparse network toplogy, which is challenging to accelerate on GPUs."
REFERENCES,0.6428571428571429,"• Structured DST [63, 47, 35]: In contrast to RigL, structured DST methods adds constraints
to the learned network topology such that the network is more amenable to acceleration on
commodity GPUs. In our case, we employ the fixed fan-in constraint which reduces both
the latency and memory consumption of the final classification layer in the XMC setting.
Further, since the dense gradient information is not available in the XMC task, we simply
randomly regrow weights as per to SET [5] which has proven to be a robust baseline in the
DST literature."
REFERENCES,0.6454081632653061,"C
Hyperparameter Settings"
REFERENCES,0.6479591836734694,"WepresentthehyperparametersettingsusedduringtraininginTable8. Fortheencoderandclassifier, we
employ two separate optimizers: AdamW for both components, except in the case of LF-AmazonTitles-
131K where Adam and SGD are utilized. All experiments are conducted using half-precision float16
types, except for Amazon-3M and LF-AmazonTitles-131K, which use the bfloat16 type. We apply a
cosine scheduler with warmup, as specified in the table. The weight decay values are set separately:
0.01 for the encoder and 1.0e-4 for the final classification layer. We use the squared hinge loss function
for all datasets except for LF-AmazonTitles-131K, where we use binary cross-entropy (BCE) loss with
positive labels."
REFERENCES,0.6505102040816326,"Table 8: Hyperparameters of our approach to facilitate reproducibility. ""LR"" stands for learning rate."
REFERENCES,0.6530612244897959,"Dataset
Encoder
Batch
Size
Dropout
Epochs
LR
Encoder
LR
Classifier
Warmup
Sequence
Length"
REFERENCES,0.6556122448979592,"Wiki10
31K"
REFERENCES,0.6581632653061225,"BERT
Base
32
0.5
100
1.0e-5
0.01
1000
128"
REFERENCES,0.6607142857142857,"LF
AmazonTtles
131K"
REFERENCES,0.6632653061224489,"Distil
BERT
512
0.7
200
1.0e-5
0.08
5000
32"
REFERENCES,0.6658163265306123,"Wiki
500K"
REFERENCES,0.6683673469387755,"BERT
Base
128
0.5
50
5.0e-5
0.05
1000
128"
REFERENCES,0.6709183673469388,"Amazon
670K"
REFERENCES,0.673469387755102,"BERT
Base
64
0.6
110
1.0e-5
0.01
1000
128"
REFERENCES,0.6760204081632653,"Amazon
3M"
REFERENCES,0.6785714285714286,"BERT
Base
128
0.35
50
5.0e-5
0.05
10000
128"
REFERENCES,0.6811224489795918,"Figure 5: Impact of intermediate layer size on overall and tail label performance. The plots show
precision, propensity-scored precision, and macro precision across epochs for different intermediate
layer sizes (1024, 2048, 4096, and 8192)."
REFERENCES,0.6836734693877551,"We also present DST and other related settings in a separate Table 9. The learning rates for the auxiliary
classifier and the intermediate layer are fixed at 5.01e-4 and 2.01e-4, respectively. We use a random
growth mode with zero initialization, updating the topology until 66% of the total training steps."
REFERENCES,0.6862244897959183,Table 9: DST and other related hyperparameter settings for different datasets.
REFERENCES,0.6887755102040817,"Dataset
Fan-in
(sparsity)
Prune
mode
Rewire
threshold
Rewire
fraction
Rewire
interval"
REFERENCES,0.6913265306122449,"Aux
classifier
Aux
cut-off"
REFERENCES,0.6938775510204082,"Inter.
layer
Layer
size"
REFERENCES,0.6964285714285714,"Wiki10
31K"
REFERENCES,0.6989795918367347,"64
(0.92)
fraction
-
0.25
300
No
-
Yes
4096"
REFERENCES,0.701530612244898,"LF
AmazonTitles
131K"
REFERENCES,0.7040816326530612,"128
(0.83)
fraction
-
0.15
1000
Yes
12
No
-"
REFERENCES,0.7066326530612245,"Wiki
500K"
REFERENCES,0.7091836734693877,"128
(0.83)
threshold
0.05
-
600
Yes
8
No
-"
REFERENCES,0.7117346938775511,"Amazon
670K"
REFERENCES,0.7142857142857143,"128
(0.83)
threshold
0.01
-
800
Yes
15
No
-"
REFERENCES,0.7168367346938775,"Amazon
3M"
REFERENCES,0.7193877551020408,"256
(0.67)
fraction
-
0.25
1500
Yes
8
No
-"
REFERENCES,0.7219387755102041,"D
Effect of Intermediate Layer Size on Overall and Tail Label Performance"
REFERENCES,0.7244897959183674,"We investigated the impact of varying sizes of the intermediate layer on both overall performance and
the performance of tail labels specifically as shown in Figure 5. It is important to highlight that although
precision values peaked at certain layer sizes, the performance on tail labels continued to improve even"
REFERENCES,0.7270408163265306,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Epoch 0.1 0.2 0.3 0.4 P@1"
REFERENCES,0.7295918367346939,Final Task
REFERENCES,0.7321428571428571,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Epoch 0.1 0.2 0.3 0.4 0.5 P@1"
REFERENCES,0.7346938775510204,Label Recalling
REFERENCES,0.7372448979591837,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Epoch 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 P@3"
REFERENCES,0.7397959183673469,Final Task
REFERENCES,0.7423469387755102,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Epoch 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 P@3"
REFERENCES,0.7448979591836735,Label Recalling
REFERENCES,0.7474489795918368,"Cluster Types
Original Clusters
Random Clusters 1
Random Clusters 2"
REFERENCES,0.75,Figure 6: Final task and meta level precision performance for Amazon-670K
REFERENCES,0.7525510204081632,"beyond this point. This observation suggests that optimizing the size of the intermediate layer could
play a crucial role in enhancing model effectiveness, particularly for tail labels which are often more
challenging to predict accurately."
REFERENCES,0.7551020408163265,"E
The Role of Random Cluster based Meta Classifiers in XMC Problems"
REFERENCES,0.7576530612244898,"To understand the impact of random clusters on meta classifier-based methods, we selected the
LightXML [24] approach and experimented with two large-scale datasets: Amazon-670K and Wiki-
500K. For our experiments, we used the original code from the official LightXML repository and the
original clusters provided by the authors. We randomized the original clusters by applying several
iterations of random.shuffle(), repeating the process twice to generate two sets of random clusters."
REFERENCES,0.7602040816326531,"To ensure randomness, we calculated the intersection of elements between each pair of clusters from
the original and random sets. We then took the maximum overlap value among all pairs, which was less
than 3.5% in both cases. Subsequently, we ran the LightXML code using the original clusters and the
two sets of random clusters."
REFERENCES,0.7627551020408163,"Our observations revealed that the final performance remained largely unaffected, although the learning
process slowed down initially, as shown in the upper row of Figure 6. The bottom row illustrates the
precision of the meta classifier, which is lower for the random clusters as expected. We replicated the
same experiment with the Wiki-500K dataset and observed similar results, which are also depicted in
Figure 7."
REFERENCES,0.7653061224489796,"F
Computational Resources"
REFERENCES,0.7678571428571429,"While we want to demonstrate the memory efficiency of our algorithms, in order to enable meaningful
comparison with existing methods, we run all our experiments on a NVidia A100 GPU, and measure the
memory consumption using torch.cuda.max_memory_allocated. On this GPU, the experiments"
REFERENCES,0.7704081632653061,"0
2
4
6
8
10
12
Epoch 0.575 0.600 0.625 0.650 0.675 0.700 0.725 0.750 P@1"
REFERENCES,0.7729591836734694,Final Task
REFERENCES,0.7755102040816326,"0
2
4
6
8
10
12
Epoch 0.55 0.60 0.65 0.70 0.75 0.80 P@1"
REFERENCES,0.7780612244897959,Label Recalling
REFERENCES,0.7806122448979592,"0
2
4
6
8
10
12
Epoch 0.400 0.425 0.450 0.475 0.500 0.525 0.550 0.575 P@3"
REFERENCES,0.7831632653061225,Final Task
REFERENCES,0.7857142857142857,"0
2
4
6
8
10
12
Epoch 0.40 0.45 0.50 0.55 P@3"
REFERENCES,0.7882653061224489,Label Recalling
REFERENCES,0.7908163265306123,"Cluster Types
Original Clusters
Random Clusters 1
Random Clusters 2"
REFERENCES,0.7933673469387755,Figure 7: Final task and meta level precision performance for Wiki-500K
REFERENCES,0.7959183673469388,"with Wiki31K take about 1 hour, Amazon-131K 8 hours, Amazon-670k 30 hours, Wikipedia-500k 36
hours and Amazon-3M 72 hours."
REFERENCES,0.798469387755102,NeurIPS Paper Checklist
REFERENCES,0.8010204081632653,"The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and precede the (optional) supplemental material. The checklist does NOT count
towards the page limit."
REFERENCES,0.8035714285714286,"Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:"
REFERENCES,0.8061224489795918,"• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the relevant
information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA)."
REFERENCES,0.8086734693877551,"The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper."
REFERENCES,0.8112244897959183,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found."
REFERENCES,0.8137755102040817,"IMPORTANT, please:"
REFERENCES,0.8163265306122449,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
• Keep the checklist subsection headings, questions/answers and guidelines below.
• Do not modify the questions and only use the provided macros for your answers."
CLAIMS,0.8188775510204082,1. Claims
CLAIMS,0.8214285714285714,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction provide a suffcient explanation of the main idea of
the paper. The problem setting and the contributions are explicitly stated in the introduction
section of the paper.
Guidelines:"
CLAIMS,0.8239795918367347,"• The answer NA means that the abstract and introduction do not include the claims made
in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or NA
answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.826530612244898,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]"
CLAIMS,0.8290816326530612,"Justification: A section discussing the limitations has been added towards the end of the
paper. Various computational considerations are the main part of the paper, and are adequately
discussed.
Guidelines:"
CLAIMS,0.8316326530612245,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only
tested on a few datasets or with a few runs. In general, empirical results often depend on
implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be used
reliably to provide closed captions for online lectures because it fails to handle technical
jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and
how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an important
role in developing norms that preserve the integrity of the community. Reviewers will be
specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8341836734693877,"Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [NA]
Justification: There are no formal results proved or claimed in this paper
Guidelines:"
CLAIMS,0.8367346938775511,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they
appear in the supplemental material, the authors are encouraged to provide a short proof
sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.8392857142857143,"Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The dataset details, architecture outline and hyper-parameter details are suffi-
ciently explained in the main body of the paper and appendices."
CLAIMS,0.8418367346938775,Guidelines:
CLAIMS,0.8443877551020408,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of whether
the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct the
dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case authors
are welcome to describe the particular way they provide for reproducibility. In the
case of closed-source models, it may be that access to the model is limited in some
way (e.g., to registered users), but it should be possible for other researchers to have
some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8469387755102041,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8494897959183674,"Question: Does the paper provide open access to the data and code, with sufficient instructions
to faithfully reproduce the main experimental results, as described in supplemental material?"
OPEN ACCESS TO DATA AND CODE,0.8520408163265306,"Answer: Data-[Yes] , and code - [No]"
OPEN ACCESS TO DATA AND CODE,0.8545918367346939,"Justification: We perform experiments on publicly available datasets. The code will be made
publicly available in the near future."
OPEN ACCESS TO DATA AND CODE,0.8571428571428571,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8596938775510204,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why."
OPEN ACCESS TO DATA AND CODE,0.8622448979591837,"• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8647959183673469,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.8673469387755102,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8698979591836735,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8724489795918368,"Justification: The experimental setting and hyperparameter details are sufficiently explained
in the main paper and appendices."
OPEN ACCESS TO DATA AND CODE,0.875,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8775510204081632,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8801020408163265,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8826530612244898,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8852040816326531,Answer: [No]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8877551020408163,"Justification: While the train, and test splits are standard, due to the scale of datasets, and com-
putations involved performing significance tests is not undertaken by the research community
in this domain."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8903061224489796,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8928571428571429,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the
main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call
to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of
the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.8954081632653061,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.8979591836734694,"Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the
experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9005102040816326,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9030612244897959,Justification: This has been sufficiently explained in the experiments section of the paper.
EXPERIMENTS COMPUTE RESOURCES,0.9056122448979592,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9081632653061225,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than
the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t
make it into the paper)."
CODE OF ETHICS,0.9107142857142857,9. Code Of Ethics
CODE OF ETHICS,0.9132653061224489,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9158163265306123,Answer: [Yes]
CODE OF ETHICS,0.9183673469387755,Justification: Ethical considerations
CODE OF ETHICS,0.9209183673469388,Guidelines:
CODE OF ETHICS,0.923469387755102,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special considera-
tion due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9260204081632653,10. Broader Impacts
BROADER IMPACTS,0.9285714285714286,"Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?"
BROADER IMPACTS,0.9311224489795918,Answer: [Yes]
BROADER IMPACTS,0.9336734693877551,Justification: A brief description of the societal impact of work in provided in the paper.
BROADER IMPACTS,0.9362244897959183,Guidelines:
BROADER IMPACTS,0.9387755102040817,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being
used as intended and functioning correctly, harms that could arise when the technology is
being used as intended but gives incorrect results, and harms following from (intentional
or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9413265306122449,11. Safeguards
SAFEGUARDS,0.9438775510204082,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9464285714285714,Answer: [NA]
SAFEGUARDS,0.9489795918367347,"Justification: We do not release any Language model, or image generators."
SAFEGUARDS,0.951530612244898,Guidelines:
SAFEGUARDS,0.9540816326530612,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best faith
effort."
LICENSES FOR EXISTING ASSETS,0.9566326530612245,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9591836734693877,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9617346938775511,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9642857142857143,Justification: The link to the repository hosting publicly available datasets has been provided.
LICENSES FOR EXISTING ASSETS,0.9668367346938775,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9693877551020408,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the
derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the
asset’s creators."
NEW ASSETS,0.9719387755102041,13. New Assets
NEW ASSETS,0.9744897959183674,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9770408163265306,Answer: [NA]
NEW ASSETS,0.9795918367346939,Justification: We do not release any new assests.
NEW ASSETS,0.9821428571428571,Guidelines:
NEW ASSETS,0.9846938775510204,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872448979591837,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897959183673469,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well
as details about compensation (if any)?
Answer: [NA]
Justification: No study involving humans was part of this work.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9923469387755102,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948979591836735,"Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution)
were obtained?
Answer: [NA]
Justification: No study involving humans was part of this work.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974489795918368,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
