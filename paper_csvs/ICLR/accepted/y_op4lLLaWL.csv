Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024630541871921183,"Variational Autoencoders (VAEs) are one of the most commonly used generative
models, particularly for image data. A prominent difﬁculty in training VAEs is
data that is supported on a lower dimensional manifold. Recent work by Dai
and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a
conjecture that in standard VAE training the generator will converge to a solution
with 0 variance which is correctly supported on the ground truth manifold. They
gave partial support for this conjecture by showing that some optima of the VAE
loss do satisfy this property, but did not analyze the training dynamics. In this
paper, we show that for linear encoders/decoders, the conjecture is true—that is
the VAE training does recover a generator with support equal to the ground truth
manifold—and does so due to an implicit bias of gradient descent rather than
merely the VAE loss itself. In the nonlinear case, we show that VAE training
frequently learns a higher-dimensional manifold which is a superset of the ground
truth manifold."
INTRODUCTION,0.0049261083743842365,"1
INTRODUCTION"
INTRODUCTION,0.007389162561576354,"Variational autoencoders (VAEs) have recently enjoyed a revived interest, both due to architectural
choices that have led to improvements in sample quality (Oord et al., 2017; Razavi et al., 2019b;
Vahdat & Kautz, 2020) and due to algorithmic insights (Dai et al., 2017; Dai & Wipf, 2019). Nev-
ertheless, ﬁne-grained understanding of the behavior of VAEs is lacking, both on the theoretical and
empirical level."
INTRODUCTION,0.009852216748768473,"In our paper, we study a common setting of interest where the data is supported on a low-dimensional
manifold — often argued to be the setting relevant to real-world image and text data due to the man-
ifold hypothesis (see e.g. Goodfellow et al. (2016)). In this setting, Dai and Wipf (2019) proposed
a two-stage training process for VAEs, based on a conjecture that for standard VAE training with
such data distributions: (1) the generator’s covariance will converge to 0, (2) the generator will learn
the correct manifold, but not the correct density on the manifold (3) the number of approximately
0 eigenvalues in the encoder covariance will equal the intrinsic dimensionality of the manifold (see
also Dai et al. (2017)). Formally, they showed that some optima of the VAE loss satisfy this conjec-
ture, but they did not attempt to analyze the training dynamics."
INTRODUCTION,0.012315270935960592,"In this paper, we revisit this setting and explore the behaviour of both the VAE loss, and the training
dynamics. Through a combination of theory and experiments we show that:"
INTRODUCTION,0.014778325123152709,"• In the case of the data manifold being linear (i.e. the data is Gaussian, supported on a linear
subspace—equivalently, it is produced as the pushforward of a Gaussian through a linear map),
and the encoder/decoder being parametrized as linear maps, we show that: a) the set of optima
includes parameters for which the generator’s support is a strict superset of the data manifold; b)"
INTRODUCTION,0.017241379310344827,*These authors contributed equally to this work.
INTRODUCTION,0.019704433497536946,Published as a conference paper at ICLR 2022
INTRODUCTION,0.022167487684729065,"the gradient descent dynamics are such that they converge to generators with support equal to the
support of the data manifold. This provides a full proof of the conjecture in Dai & Wipf (2019),
albeit we show the phenomenon is a combination of both the location of the minima of the loss as
well as an implicit bias of the training dynamics.
• In the case of the data manifold being nonlinear (i.e. the data distribution is the pushforward of
the Gaussian through a nonlinear map f : Rr →Rd, r ≤d), the gradient descent dynamics from
a random start often converges to generators G whose support strictly contains the support of the
underlying data distribution, while driving reconstruction error to 0 and driving the VAE loss to
−∞. This shows that the conjecture in Dai & Wipf (2019) does not hold for general nonlinear
data manifolds and architectures for the generator/encoder."
INTRODUCTION,0.024630541871921183,"Organization:
We will provide an informal overview of our ﬁndings in Section 3. The rigorous
discussion on the VAE landscape are in Section 4 and on the implicit bias of gradient descent in
Section 5."
SETUP,0.027093596059113302,"2
SETUP"
SETUP,0.029556650246305417,"We will study the behavior of VAE learning when data lies on a low-dimensional manifold—more
precisely, we study when the generator can recover the support of the underlying data distribution.
In order to have a well-deﬁned “ground truth”, both for our theoretical and empirical results, we will
consider synthetic dataset that are generated by a “ground truth” generator as follows."
SETUP,0.03201970443349754,"Data distribution:
To generate a sample point x for the data distribution, we will sample z ∼
N(0, Ir∗), and output x = f(z), for a suitably chosen f. In the linear case, f(z) = Az, for some
matrix A ∈Rd×r∗. In the nonlinear case, f(z) will be a nonlinear function f : Rr∗→Rd. We will
consider several choices for f.
Parameterization of the trained model:
For the model we are training, the generator will sample
z ∼N(0, Ir) and output x ∼N(f(z), ϵ2I), for trainable f, ϵ; the encoder given input x will output
z ∼N(g(x), D), where D ∈Rr×r is a diagonal matrix, and g, D are trainable. In the linear case,
f, g will be parameterized as matrices ˜A, ˜B; in the nonlinear case, several different parameterizations
will be considered. In either case the VAE Loss will be denoted by L(·), see (3)."
OUR RESULTS,0.034482758620689655,"3
OUR RESULTS"
OUR RESULTS,0.03694581280788178,"Linear VAEs: the correct distribution is not recovered.
Recall in the linear case, we train a
linear encoder and decoder to learn a Gaussian distribution consisting of data points x ∼N(0, Σ) —
equivalently, the data distribution is the pushforward of a standard Gaussian z ∼N(0, Ir) through
a linear generator x = Az with AAT = Σ; see also Section 2 above. In Theorem 1 of Lucas
et al. (2019), the authors proved that in a certain probabilistic PCA setting where Σ is full-rank, the
landscape of the VAE loss has no spurious local minima: at any global minima of the loss, the VAE
decoder exactly matches the ground truth distribution, i.e. ˜A ˜AT + ϵ2I = Σ."
OUR RESULTS,0.03940886699507389,"We revisit this problem in the setting where Σ has rank less than d so that the data lies on the lower-
dimensional manifold/subspace spanned by the columns of A or equivalently Σ, denoted span(A).
We show empirically (i.e. via simulations) in Section 6 that when Σ is rank-degenerate the VAE
actually fails to recover the correct distribution. More precisely, the recovered ˜A has the correct
column span but fails to recover the correct density — conﬁrming predictions made in Dai & Wipf
(2019). We then explain theoretically why this happens, where it turns out we ﬁnd some surprises."
OUR RESULTS,0.04187192118226601,"Landscape Analysis: Linear and Nonlinear VAE.
Dai & Wipf (2019) made their predictions on
the basis of the following observation about the loss landscape: there can exist sequences of VAE
solutions whose objective value approaches −∞(i.e. are asymptotic global minima), for which the
generator has the correct column span, but does not recover the correct density on the subspace.
They also informally argued that these are all of the asymptotic global minima of loss landscape (Pg
7 and Appendix I in Dai & Wipf (2019)), but did not give a formal theorem or proof of this claim."
OUR RESULTS,0.04433497536945813,"We settle the question by showing this is not the case:* namely, there exist many convergent se-
quences of VAE solutions which still go to objective value −∞but also do not recover the correct"
OUR RESULTS,0.046798029556650245,"*They also argued this would hold in the nonlinear case, but our simulations show this is generally false in
that setting, even for the solutions chosen by gradient descent with a standard initialization — see Section 6."
OUR RESULTS,0.04926108374384237,Published as a conference paper at ICLR 2022
OUR RESULTS,0.05172413793103448,"column span — instead, the span of such ˜A is a strictly larger subspace. More precisely, we obtain
a tight characterization of all asymptotic global minima of the loss landscape:"
OUR RESULTS,0.054187192118226604,"Theorem 1 (Optima of Linear VAE Loss, Informal Version of Theorem 3). Suppose that ˜A, ˜B are
ﬁxed matrices such that A = ˜A ˜BA and suppose that #{i : ˜Ai = 0} > r −d, i.e. the number
of zero columns of ˜A is strictly larger than r −d. Then there exists ˜ϵt →0 and positive diagonal
matrices ˜Dt such that limt→∞L( ˜A, ˜B, ˜Dt, ˜ϵt) = −∞. Also, these are all of the asymptotic global
minima: any convergent sequence of points ( ˜At, ˜Bt, ˜Dt, ˜ϵt) along which the loss L goes to −∞
satisﬁes ˜At →˜A, ˜Bt →˜B with A = ˜A ˜BA such that #{i : ˜Ai = 0} > r −d."
OUR RESULTS,0.05665024630541872,"To interpret the constraint #{i :
˜Ai = 0} > r −d, observe that if the data lies on a lower-
dimensional subspace of dimension r∗< d (i.e. r∗is the rank of Σ), then there exists a generator
which generates the distribution with r −r∗> r −d zero columns by taking an arbitrary low-
rank factorization LLT = Σ with L : d × r∗and deﬁning A : d × r by A = [L
0d×r−r∗]. The
larger the gap is between the manifold/intrinsic dimension r∗and the ambient dimension d, the more
ﬂexibility we have in constructing asymptotic global minima of the landscape. Also, we note there
is no constraint in the Theorem that r −d ≥0: the assumption is automatically satisﬁed if r < d."
OUR RESULTS,0.059113300492610835,"To summarize, the asymptotic global minima satisfy A = ˜A ˜BA, so the column span of ˜A contains
that of A, but in general it can be a higher dimensional space. For example, if d, r ≥r∗+ 2 and"
OUR RESULTS,0.06157635467980296,"and the ground truth generator is A =

Ir∗
0
0
0"
OUR RESULTS,0.06403940886699508,"
, then ˜A =

Ir∗+1
0
0
0"
OUR RESULTS,0.0665024630541872,"
and ˜B =

Ir∗+1
0
0
0 "
OUR RESULTS,0.06896551724137931,"is a perfectly valid asymptotic global optima of the landscape, even though decoder ˜A generates a"
OUR RESULTS,0.07142857142857142,"different higher-dimensional Gaussian distribution N

0,

Ir∗+1
0
0
0"
OUR RESULTS,0.07389162561576355,"
than the ground truth. In"
OUR RESULTS,0.07635467980295567,"the above result we showed that there are asymptotic global minima with higher dimensional spans
even with the common restriction that the encoder variance is diagonal; if we considered the case
where the encoder variance is unconstrained, as done in Dai & Wipf (2019), and/or can depend on
its input x, this can only increase the number of ways to drive the objective value to −∞."
OUR RESULTS,0.07881773399014778,"We also consider the analogous question in the nonlinear VAE setting where the data lies on a low-
dimensional manifold. We prove in Theorem 6 that even in a very simple example where we ﬁt a
VAE to generate data produced by a 1-layer ground truth generator, there exists a bad solution with
strictly larger manifold dimension which drives the reconstruction error to zero (and VAE loss to
−∞). The proof of this result does not depend strongly on the details of this setup and it can be
adapted to construct bad solutions for other nonlinear VAE settings."
OUR RESULTS,0.0812807881773399,"We note that the nature both of these result is asymptotic: that is, they consider sequences of solu-
tions whose loss converges to −∞— but not the rate at which they do so. In the next section, we
will consider the trajectories the optimization algorithm takes, when the loss is minimized through
gradient descent."
OUR RESULTS,0.08374384236453201,"Linear VAE: implicit regularization of gradient ﬂow.
The above theorem indicates that studying
the minima of the loss landscape alone cannot explain the empirical phenomenon of linear VAE
training recovering the support of the ground truth manifold in experiments; the only prediction that
can be made is that the VAE will recover a possibly larger manifold containing the data."
OUR RESULTS,0.08620689655172414,"We resolve this tension by proving that gradient ﬂow, the continuous time version of gradient de-
scent, has an implicit bias towards the low-rank global optima. Precisely, we measure the effective
rank quantitatively in terms of the singular values: namely, if σk( ˜A) denotes the k-th largest singular
value of matrix ˜A, we show that all but the largest dim(span A) singular values of ˜A decay at an
exponential rate, as long as: (1) the gradient ﬂow continues to exist* , and (2) the gradient ﬂow does
not go off to inﬁnity, i.e. neither ˜A or ˜ϵ go to inﬁnity (in simulations, the decoder ˜A converges to
a bounded point and ˜ϵ →0 so the latter assumption is true). To simplify the proof, we work with
a slightly modiﬁed loss which “eliminates” the encoder variance by setting it to its optimal value:"
OUR RESULTS,0.08866995073891626,"*We remind the reader that the gradient ﬂow on loss L(x) is a differential equation dx/dt = −∇L(x).
Unlike discrete-time gradient descent, gradient ﬂow in some cases (e.g. dx/dt = x2) has solutions which exist
only for a ﬁnite time (e.g. x = 1/(1 −t)), which “blows up” at t = 1), so we need to explicitly assume the
solution exists up to time T."
OUR RESULTS,0.09113300492610837,Published as a conference paper at ICLR 2022
OUR RESULTS,0.09359605911330049,"L1( ˜A, ˜B, ˜ϵ) := min ˜
D L( ˜A, ˜B, ˜ϵ, ˜D); this loss has a simpler closed form, and we believe the theo-
rems should hold for the standard loss as well. (Generally, gradient descent on the original loss L
will try to optimize ˜D in terms of the other parameters, and if it succeeds to keep ˜D well-tuned in
terms of ˜A, ˜B, ˜ϵ then L will behave like the simpliﬁed loss L1.)
Theorem 2 (Implicit Bias of Gradient Flow, Informal version of Theorem 5). Let A : d × r be ar-
bitrary and deﬁne W to be the span of the rows of A, let ˜Θ(0) = ( ˜A(0), ˜B(0), ˜ϵ(0)) be an arbitrary
initialization and deﬁne the gradient ﬂow ˜Θ(t) = ( ˜A(t), ˜B(t), ˜ϵ(t)) by the ordinary differential
equation (ODE)
d˜Θ(t)"
OUR RESULTS,0.0960591133004926,"dt
= −∇L1(˜Θ(t))
(1)"
OUR RESULTS,0.09852216748768473,"with initial condition Θ0. If the solution to this equation exists on the time interval [0, T] and satisﬁes
maxt∈[0,T ] maxj[∥( ˜At)j∥2 + ˜ϵ2
t] ≤K, then for all t ∈[0, T] we have d
X"
OUR RESULTS,0.10098522167487685,"k=dim(W )+1
σ2
k( ˜A(t)) ≤C(A, ˜A) e−t/K
(2)"
OUR RESULTS,0.10344827586206896,"where C(A, ˜A) := ∥PW ⊥˜AT (0)∥2
F and PW ⊥is the orthogonal projection onto the orthogonal
complement of W."
OUR RESULTS,0.10591133004926108,"Together, our Theorem 1 and Theorem 2 show that if gradient descent converges to a point while
driving the loss to −∞, then it successfully recovers the ground truth subspace/manifold span A.
This shows that, in the linear case, the conjecture of Dai & Wipf (2019) can indeed be validated
provided we incorporate training dynamics into the picture. The prediction of theirs we do not prove
is that the number of zero entries of the encoder covariance D converges to the intrinsic dimension;
as shown in Table 1, in a few experimental runs this does not occur — in contrast, Theorem 2 implies
that ˜A should have the right number of nonzero singular values and our experiments agree with this."
OUR RESULTS,0.10837438423645321,"Nonlinear VAE: Dynamics and Simulations.
In the linear case, we showed that the implicit bias
of gradient descent leads the VAE training to converge to a distribution with the correct support. In
the nonlinear case, we show that this does not happen—even in simple cases."
OUR RESULTS,0.11083743842364532,"Precisely, in the setup of the one-layer ground truth generator, where we proved (Theorem 6) there
exist bad global optima of the landscape, we verify experimentally (see Figure 1) that gradient
descent from a random start does indeed converge to such bad asymptotic minima. In particular,
this shows that whether or not gradient descent has a favorable implicit bias strongly depends on the
data distribution and VAE architecture."
OUR RESULTS,0.11330049261083744,"More generally, by performing experiments with synthetic data of known manifold dimension, we
make the following conclusions: (1) gradient descent training recovers manifolds approximately
containing the data, (2) these manifolds are generally not the same dimension as the ground truth
manifold, but larger (this is in contrast to the conjecture in Dai & Wipf (2019) that they should be
equal) even when the decoder and encoder are large enough to represent the ground truth and the
reconstruction error is driven to 0 (VAE loss is driven to −∞), and (3) of all manifolds containing
the data, gradient descent still seems to favor those with relatively low (but not always minimal)
dimension. Further investigating the precise role of VAE architecture and optimization algorithm,
as well as the interplay with the data distribution is an exciting direction for future work."
RELATED WORK,0.11576354679802955,"3.1
RELATED WORK"
RELATED WORK,0.11822660098522167,"Implicit regularization.
Interestingly, the implicit bias towards low-rank solutions in the VAE
which we discover is consistent with theoretical and experimental results in other settings, such as
deep linear networks/matrix factorization (e.g. Gunasekar et al. (2018); Li et al. (2018); Arora et al.
(2019); Li et al. (2020); Jacot et al. (2021)), although it seems to be for a different mathematical
reason — unlike those settings, initialization scale does not play a major role. Similar to the setting
of implicit margin maximization (see e.g. Ji & Telgarsky (2018); Schapire & Freund (2013); Soudry
et al. (2018)), in our VAE setting the optima are asymptotic (though approaching a ﬁnite point, not
off at inﬁnity) and the loss goes to −∞. Kumar & Poole (2020); Tang & Yang (2021) also explore
some implicit regularization effects tied to the Jacobian of the generator and the covariance of the
Gaussian noise."
RELATED WORK,0.1206896551724138,Published as a conference paper at ICLR 2022
RELATED WORK,0.12315270935960591,"Architectural and Algorithmic Advances for VAEs.
There has been a recent surge in activity
with the goal of understanding VAE training and improving its performance in practice. Much of
the work has been motivated by improving posterior modeling to avoid problems such as “posterior
collapse”, see e.g. (Dai et al., 2020; Razavi et al., 2019a; Pervez & Gavves, 2021; Lucas et al., 2019;
He et al., 2019; Oord et al., 2017; Razavi et al., 2019b; Vahdat & Kautz, 2020). Most relevant to the
current work are probably the works Dai & Wipf (2019) and Lucas et al. (2019) discussed earlier. A
relevant previous work to these is Dai et al. (2017); one connection to the current work is that they
also performed experiments with a ground truth manifold, in their case given as the pushforward of
a Gaussian through a ReLU network. In their case, they found that for a certain decoder and encoder
architectures that they could recover the intrinsic dimension using a heuristic related to the encoder
covariance eigenvalues from Dai & Wipf (2019); our results are complementary in that they show
that this phenomena is not universal and does not hold for other natural datasets (e.g. manifold data
on a sphere ﬁt with a standard VAE architecture)."
VAE LANDSCAPE ANALYSIS,0.12561576354679804,"4
VAE LANDSCAPE ANALYSIS"
VAE LANDSCAPE ANALYSIS,0.12807881773399016,"In this section, we analyze the landscape of a VAE, both in the linear and non-linear case."
VAE LANDSCAPE ANALYSIS,0.13054187192118227,"Preliminaries and notation.
We use a VAE to model a datapoint x ∈Rd as the pushforward of
z ∼N(0, Ir). We have the following standard VAE architecture:"
VAE LANDSCAPE ANALYSIS,0.1330049261083744,"p(x|z) = N(f(z), ϵ2I),
q(z|x) = N(g(x), D)
where ϵ2 > 0 is the decoder variance, D is a diagonal matrix with nonnegative entries, and f, g, D, ϵ
are all trainable parameters. (For simplicity, our D does not depend on x; this is the most common
setup in the linear VAE case we will primarily focus on.) The VAE objective (see Appendix A for
explicit derivation) is to minimize:"
VAE LANDSCAPE ANALYSIS,0.1354679802955665,"L(f, g, D, ϵ) := Ex∼p∗Ez′∼N(0,Ir)
h 1"
VAE LANDSCAPE ANALYSIS,0.13793103448275862,"2ϵ2 ∥x −f(g(x) + D1/2z′)∥2 + ∥g(x)∥2/2
i"
VAE LANDSCAPE ANALYSIS,0.14039408866995073,+ d log(ϵ) + Tr(D)/2 −1 2 X
VAE LANDSCAPE ANALYSIS,0.14285714285714285,"i
log Dii.
(3)"
VAE LANDSCAPE ANALYSIS,0.14532019704433496,"We also state a general fact about VAEs for the case that the objective value can be driven to −∞,
which was observed in (Dai & Wipf, 2019): they must satisfy ϵ →0 and achieve perfect limiting
reconstruction error. The ﬁrst claim in this Lemma is established in the proof of Theorem 4 and the
second claim is Theorem 5 in Dai & Wipf (2019). For completeness, we include a self-contained
proof in Appendix B.1.
Lemma 1 (Theorems 4 and 5 of Dai & Wipf (2019)). Suppose ft, gt, Dt, ϵt for t ≥1 are
a sequence such that limt→∞L(ft, gt, Dt, ϵt) = −∞.
Then:
1) limt→∞ϵt
= 0 and 2)
limt→∞Ex∼p∗Ez′∼N(0,Ir)∥x −ft(gt(x) + D1/2
t
z′)∥2 = 0."
VAE LANDSCAPE ANALYSIS,0.1477832512315271,"In fact, the reconstruction error and ϵ are closely linked in a simple way:
Lemma 2. If f, g, D are ﬁxed, then the optimal value of ϵ to minimize L is given by ϵ =
q"
VAE LANDSCAPE ANALYSIS,0.15024630541871922,"1
dEx∼p∗Ez′∼N(0,Ir)

∥x −f(g(x) + D1/2z′)∥2
."
LINEAR VAE,0.15270935960591134,"4.1
LINEAR VAE"
LINEAR VAE,0.15517241379310345,"Setup:
In the linear VAE case, we assume the data is generated from the model x = Az with
A ∈Rd×r∗and z ∼N(0, Ir∗). We will denote the training parameters by ˜A ∈Rd×r, ˜B ∈Rr×d,
˜D ∈Rr×r, and ˜ϵ > 0, where r ≥1 is a ﬁxed hyperparameter which corresponds to the latent
dimension in the trained generator, and we assume ˜D is a diagonal matrix. With this notation in
place, the implied VAE has generator/decoder ˜x ∼N( ˜Az, ˜ϵ2Id) and encoder ˜z ∼N( ˜Bx, ˜D). The
VAE objective as a function of parameters ˜Θ = ( ˜A, ˜B, ˜D, ˜ϵ) is (see Appendix A):"
LINEAR VAE,0.15763546798029557,"L(˜Θ) =
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
LINEAR VAE,0.16009852216748768,"2∥˜BA∥2
F + d log ˜ϵ + 1 2 X i"
LINEAR VAE,0.1625615763546798,"
˜Dii∥˜Ai∥2/˜ϵ2 + ˜Dii −log ˜Dii

(4)"
LINEAR VAE,0.16502463054187191,"Our analysis makes use of a simpliﬁed objective L1, which “eliminates” D out of the objective by
plugging in the optimal value of D for a choice of the other variables. We use this as a technical tool
when analyzing the landscape of the original loss L."
LINEAR VAE,0.16748768472906403,Published as a conference paper at ICLR 2022
LINEAR VAE,0.16995073891625614,"Lemma 3 (Deriving the simpliﬁed loss L1). Suppose that ˜A, ˜B, ˜ϵ are ﬁxed. Then the objective
L is minimized by choosing for all i that ˜Dii =
˜ϵ2"
LINEAR VAE,0.1724137931034483,"∥˜
Ai∥2+˜ϵ2 where ˜Ai is column i of ˜A, and for"
LINEAR VAE,0.1748768472906404,"L1( ˜A, ˜B, ˜ϵ) := min ˜
D L( ˜A, ˜B, ˜D, ˜ϵ) it holds that"
LINEAR VAE,0.17733990147783252,"L1( ˜A, ˜B, ˜ϵ) =
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
LINEAR VAE,0.17980295566502463,"2∥˜BA∥2
F + (d −r) log ˜ε +
X i"
LINEAR VAE,0.18226600985221675,"1 + log

∥˜Ai∥2 + ˜ϵ2"
LINEAR VAE,0.18472906403940886,"2
. (5)"
LINEAR VAE,0.18719211822660098,"Taking advantage of this simpliﬁed formula, we can then identify (for the original loss L) simple suf-
ﬁcient conditions on ˜A, ˜B which ensure they can be used to approach the population loss minimum
by picking suitable ˜ϵt, ˜Dt and prove matching necessary conditions."
LINEAR VAE,0.1896551724137931,"Theorem 3. First, suppose that ˜A : d × r, ˜B : r × d are ﬁxed matrices such that A = ˜A ˜BA and
suppose that #{i : ˜Ai = 0} > r −d, i.e. the number of zero columns of ˜A is strictly larger than
r −d. Then for any sequence of positive ˜ϵt →0 there exist a sequence of positive diagonal matrices
˜Dt such that:"
LINEAR VAE,0.1921182266009852,"1. For every i such that ˜Ai ̸= 0, i.e. column i of ˜A is nonzero, we have ( ˜Dt)ii →0."
LINEAR VAE,0.19458128078817735,"2. limt→∞L( ˜A, ˜B, ˜Dt, ˜ϵt) = −∞."
LINEAR VAE,0.19704433497536947,"Conversely,
suppose
that
that
˜At, ˜Bt, ˜Dt, ˜ϵt
is
an
arbitrary
sequence
such
that
limt→∞L( ˜At, ˜Bt, ˜Dt, ˜ϵt) = −∞. Then as t →∞, we must have that:"
LINEAR VAE,0.19950738916256158,"1. ˜ϵt →0 and ∥A −˜At ˜BtA∥2
F →0."
LINEAR VAE,0.2019704433497537,"2. maxi( ˜Dt)ii∥( ˜At)i∥2
F →0 where ( ˜At)i denotes the i-th column of ˜At."
LINEAR VAE,0.2044334975369458,"3. For any δ > 0, lim inft→∞#{i : ∥( ˜At)i∥2
2 < δ} > r −d, i.e. asymptotically ˜At has
strictly more than r −d columns arbitrarily close to zero."
LINEAR VAE,0.20689655172413793,"In particular, if ( ˜At, ˜Bt, ˜Dt, ˜ϵt) converge to a point ( ˜A, ˜B, ˜D, ˜ϵ) then ˜ϵ = 0, A = ˜A ˜BA, ˜Dii = 0
for every i such that ˜Ai ̸= 0, and #{i : ˜Ai = 0} > r −d."
LINEAR VAE,0.20935960591133004,"For the sufﬁciency direction, we observe that in (5) the ﬁrst term is zero if A = ˜A ˜BA and the sum
of the last two terms goes to −∞if ˜ϵ →0 and enough columns of ˜A are zero. Based upon similar
reasoning combined with Lemma 1, we show necessity. The full proof is in the Appendix."
NONLINEAR VAE,0.21182266009852216,"4.2
NONLINEAR VAE"
NONLINEAR VAE,0.21428571428571427,"In this section, we give a simple example of a nonlinear VAE architecture which can represent the
ground truth distribution perfectly, but has another asymptotic global minimum where it outputs data
lying on a manifold of a larger dimension (r∗+ s instead of r∗for any s ≥1). The ground truth
model is a one-layer network (“sigmoid dataset” in Section 6) and the bad decoder we construct
outputs a standard Gaussian in r∗+ s dimensions padded with zeros.
Theorem 4 (Theorem 6 in Appendix). Let s ≥1 be arbitrary and consider the sigmoid setup
from Section 6. There exists ˜A1, ˜A2, ˜B s.t. for ˜ϵt →0 there exists ˜Dt s.t. (1) the VAE loss
L( ˜A1, ˜A2, ˜B, ˜Dt, ˜ϵt) →−∞; (2) The output of the decoder ˜x = ˜A1˜z + σ( ˜A2˜z), ˜z ∼N(0, Ir)
is a standard Gaussian in the ﬁrst r∗+ s coordinates and zero in the remaining ones."
NONLINEAR VAE,0.21674876847290642,"Thus, the generator constructed has as support a manifold of larger dimension (r∗+ s). Moreover,
in Section 6, we show that this is not merely a theoretical possibility: we show through simulations
that gradient descent from a random initialization often converges to similar minima."
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.21921182266009853,"5
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.22167487684729065,"In this section, we prove that even though the landscape of the VAE loss contains generators with
strictly larger support than the ground truth, the gradient ﬂow is implicitly biased towards low-rank
solutions. We prove this for the simpliﬁed loss L1( ˜A, ˜B, ˜ϵ) = min ˜
D L( ˜A, ˜B, ˜ϵ, ˜D), which makes"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.22413793103448276,Published as a conference paper at ICLR 2022
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.22660098522167488,"the calculations more tractable, though we believe our results should hold for the original loss L as
well. The main result we prove is as follows:
Theorem 5 (Implicit bias of gradient descent). Let A : d×r be arbitrary and deﬁne W to be the span
of the rows of A, let ˜Θ(0) = ( ˜A(0), ˜B(0), ˜ϵ(0)) be an arbitrary initialization and deﬁne the gradient
ﬂow ˜Θ(t) = ( ˜A(t), ˜B(t), ˜ϵ(t)) by the differential equation (1). with initial condition ˜Θ0. If the
solution to this equation exists on the time interval [0, T] and satisﬁes maxt∈[0,T ] maxj[∥( ˜At)j∥2 +
˜ϵ2
t] ≤K, then for all t ∈[0, T] we have d
X"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.229064039408867,"k=dim(W )+1
σ2
k( ˜A(t)) ≤∥PW ⊥˜AT (t)∥2
F ≤e−t/K∥PW ⊥˜AT (0)∥2
F
(6)"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.2315270935960591,where PW ⊥is the orthogonal projection onto the orthogonal complement of W.
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.23399014778325122,"Towards showing the above result, we ﬁrst show how to reduce to matrices where A has d −
dim(rowspan(A)) rows that are all-zero. To do this, we observe that the linear VAE objective is
invariant to arbitrary rotations in the output space (i.e. x-space), so the gradient descent/ﬂow trajec-
tories transform naturally under rotations. Thus, we can “rotate” the ground truth parameters as well
as the training parameters."
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.23645320197044334,"This is formally captured as Lemma 6 in the Appendix. Recall that by the singular value decomposi-
tion A = USV T for some orthogonal matrices U, V and diagonal matrix S, and rotation invariance
in the x-space lets us reduce to analyzing the case where U = I, i.e. A = SV T . This matrix has a
zero row for every zero singular value."
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.23891625615763548,"Analysis when A has zero rows.
Having reduced our analysis to the case where A has zero rows,
the following key lemma shows that for every i such that row i of A (denoted A(i)) is zero, the
gradient descent step −∇L or −∇L1 will be negatively correlated with the corresponding row ˜A(i).
Lemma 4 (Gradient correlation). If row i of A is zero, then r
X"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.2413793103448276,"j=1
˜Aij
∂L
∂˜Aij
≥ r
X"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.2438423645320197,"j=1
˜Djj ˜A2
ij/˜ϵ2, r
X"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.24630541871921183,"j=1
˜Aij
∂L1
∂˜Aij
≥ r
X j=1"
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.24876847290640394,"˜A2
ij
∥˜Aj∥2 + ˜ϵ2 ."
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.2512315270935961,"The way we use it is to notice that since the negative gradient points towards zero, gradient descent
will shrink the size of ˜A(i). Since the size of the matrix ˜A stays bounded, this should mean that for
small step sizes the norm of row i of ˜A shrinks by a constant factor at every step of gradient descent
on loss L1. We formalize this in continuous time for the gradient ﬂow, i.e. the limit of gradient
descent as step size goes to zero: for the special case of Theorem 2 in the zero row setting, the
corresponding rows of ˜A decay exponentially fast."
IMPLICIT BIAS OF GRADIENT DESCENT IN LINEAR VAE,0.2536945812807882,"Lemma 5 (Exponential decay of extra rows). Let A be arbitrary, and let ˜Θ(0) = ( ˜A(0), ˜B(0), ˜ϵ(0))
be an arbitrary initialization and deﬁne the gradient ﬂow ˜Θ(t) = ( ˜A(t), ˜B(t), ˜ϵ(t)) to be a solution
of the differential equation (1) with initial condition ˜Θ(0). If the solution exists on the time interval
[0, T] and satisﬁes maxt∈[0,T ] maxj[∥( ˜A(t))j∥2 + ˜ϵ(t)2] ≤K for some K > 0, then for all i such
that row i of A is zero we have ∥˜A(i)(t)∥2 ≤e−t/K∥˜A(i)(0)∥2 for all t ∈[0, T]."
SIMULATIONS,0.2561576354679803,"6
SIMULATIONS"
SIMULATIONS,0.25862068965517243,"In this section, we provide extensive empirical support for the questions we addressed theoretically.
In particular we investigate the kinds of minima VAEs converge to when optimized via gradient
descent over the course of training."
SIMULATIONS,0.26108374384236455,"Linear VAEs:
First, we investigate whether linear VAEs are able to ﬁnd the correct support for
a distribution supported over a linear subspace. The setup is as follows. We choose a ground
truth linear transformation matrix A by concatenating an r∗× r∗matrix consisting of iid standard
Gaussian entries with a zero matrix of dimension (d −r∗) × r∗; the data is generated as Az, z ∼
N(0, Ir∗). Thus the data lies in a r∗-dimensional subspace embedded in a d-dimensional space.
We ran the experiment with various choices for r∗, d as well as the latent dimension of the trained
decoder (Table 1). Every result is the mean over three experiments run with the same dimensionality
and setup but a different random seed."
SIMULATIONS,0.26354679802955666,Published as a conference paper at ICLR 2022
SIMULATIONS,0.2660098522167488,"Intrinsic Dimension
3
3
6
6
9
9
12
Ambient Dimension
12
20
12
20
12
20
20"
SIMULATIONS,0.2684729064039409,"Mean #0’s in Encoder Variance
3.3
3.7
6
6
9.3
9
12
Mean # Decoder Rows Nonzero
3
3
6
6
9
9
12
Mean Normalized Eigenvalue Error
0.44
0.71
0.49
0.47
0.30
0.45
0.42"
SIMULATIONS,0.270935960591133,"Table 1: Optima found by training a linear VAE on data generated by a linear generator (i.e. a linearly trans-
formed standard multivariate gaussian embedded in a larger ambient dimension by padding with zeroes) via
gradient descent. The results reﬂect the predictions of Theorem 5: the number of nonzero rows of the decoder
always match the dimensionality of the input data distribution with no variance while the number of nonzero
dimensions of encoder variance is greater than or equal to the nonzero rows. All VAEs are trained with a
20-dimensional latent space. Clearly, the model fails to recover the correct eigenvalues and therefore has a
substantially wrong data density function."
SIMULATIONS,0.2733990147783251,"Results: From Table 1 we can see that the optima found by gradient descent capture the support
of the manifold accurately across all choices of d, r, with the correct number of nonzero decoder
rows. We also almost always see the correct number of zero dimensions in the diagonal matrix
corresponding to the encoder variance."
SIMULATIONS,0.27586206896551724,"However, gradient descent is unable to recover the density of the data on the learned manifold in
the linear setting — in sharp contrast to the full rank case (Lucas et al., 2019). We conclude this
by comparing the eigenvalues of the data covariance matrix and the learned generator covariance
matrix. In order to understand whether the distribution on the linear subspace has the right density,
we compute the eigenvalue error by forming matrices X, ˆX with n rows, for which each row is
sampled from the ground truth and learned generator distribution respectively. We then compute
the vector of eigenvalues λ, ˆλ for the ground truth covariance matrix AAT and empirical covariance
matrix (1/n) ˆXT ˆX respectively and compute the normalized eigenvalue error ||ˆλ −λ||/||λ||. In no
case does the density of the learned distribution come close to the ground truth."
SIMULATIONS,0.27832512315270935,"Nonlinear Dataset
In this section, we investigate whether VAEs are able to ﬁnd the correct support
in nonlinear settings. Unlike the linear setting, there is no “canonical” data distribution suited for a
nonlinear VAE, so we explore two setups:"
SIMULATIONS,0.28078817733990147,"• Sphere dataset: The data are generated from the unit sphere concatenated with zero padding at the
end. This can be interpreted as a unit sphere embedded in a higher dimensional space. We used 3
layers of 200 hidden units to parameterize our encoder and decoder networks.
To measure how well the VAE has learnt the support of the distribution, we evaluate the average
of (∥˜x:(r+1)∥2 −1)2, where ˜x are generated by the learnt generator. We will call this quantity
manifold error. We have also evaluated the padding error, which is deﬁned as ∥˜xr+2:∥2
2.
• Sigmoid Dataset: Let z ∼N(0, Ir), the sigmoid dataset concatenates z with σ(⟨a∗, z⟩) where
a∗∈Rr is generated according to N(0, Ir). We add additional zero paddings to embed the gen-
erated data in a higher dimensional ambient space. The decoder is parameterized by a nonlinear
function f(z) = ˜Az + σ( ˜Cz) and the encoder is parameterized by a linear function g(x) = ˜Bx .
The intrinsic dimension of the dataset is r.
To measure how well the VAE has learnt the support of the distribution, we evaluate the average
of (σ(⟨a∗, ˜x:r⟩) −˜xr+1)2, where ˜x are generated by the learnt decoder. We will call this quantity
manifold error. The padding error is deﬁned as similarly as the sphere dataset.
Results: Due to space constraints, results for the sphere dataset (Table 3 and Figure 3) are in Ap-
pendix D. In both of the nonlinear dataset experiments, we see that the number of zero entries in
the diagonal encoder variance is less reﬂective of the intrinsic dimension of the manifold than the
linear dataset. It is, however, at least as large as the intrinsic dimension (Table 3, 2). We consider a
coordinate to be 0 if it’s less than 0.1. We found that the magnitude of each coordinate to be well
separated, i.e. the smaller coordinates tend to be smaller than 0.1 and the larger tend to be bigger
than 0.5. Thus the threshold selection is not crucial. We did not include padding error in the tables
because it reaches zero in all experiments"
SIMULATIONS,0.2832512315270936,"We show the progression of manifold error, decoder variance and VAE loss during training for the
sphere data in Figure 3 and for the sigmoid data in Figure 2, which are included in Appendix D
due to space constraints. Datasets of all conﬁgurations of dimensions reached close to zero decoder"
SIMULATIONS,0.2857142857142857,Published as a conference paper at ICLR 2022
SIMULATIONS,0.2881773399014778,"0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250"
SIMULATIONS,0.29064039408866993,"10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5 1.0 0.5 0.0 0.5 1.0 1.5"
SIMULATIONS,0.29310344827586204,"Figure 1: A demonstration that in the nonlinear setting (both types of data padded with zeroes to embed in
higher ambient dimension, see Setup in Section 6) VAE training does not always recover a distribution with
the correct support.
Left ﬁgure: A histogram of the norms of samples generated from the VAE restricted to
the dimensions which are not zero, which shows many of the points have norm less than 1. (The ground-truth
distribution would output only samples of norm 1.) The particular example here is Column 2 in Table 3. Right
ﬁgure: Two-dimensional linear projection of data output by VAE generator trained on our sigmoid dataset.
The x-axis denotes ⟨a∗, ˜x:r⟩and the y-axis is ˜xr+1, the blue points are from the trained VAE and the orange
points are from the ground truth. In contrast to the ground truth data, which satisﬁes the sigmoidal constraint
xr+1 = σ(⟨a∗, x:r⟩), the trained VAE points do not and instead resemble a standard gaussian distribution. This
is a case that closely resembles the example provided in Theorem 6. Also similar to Theorem 6, the VAE model
plotted here (from Column 6 in Table 2) achieves nearly-perfect reconstruction error, approximately 0.001."
SIMULATIONS,0.2955665024630542,"Intrinsic Dimensions
3
3
5
5
7
7
Ambient Dimensions
7
17
11
22
15
28
VAE Latent Dimensions
6
8
10
16
13
24"
SIMULATIONS,0.29802955665024633,"Mean Manifold Error
0.09
0.13
0.23
0.24
0.18
0.28
Mean #0’s in Encoder Variance
3
3.6
6
6.3
7.3
8"
SIMULATIONS,0.30049261083743845,"Table 2: Optima found by training a VAE on the sigmoid dataset. The VAE training consistently yields encoder
variances with number of 0 entries greater than or equal to the intrinsic dimension."
SIMULATIONS,0.30295566502463056,"variances, meaning the VAE loss is approaching −∞. To demonstrate Theorem 6, we took examples
from both datasets to visualize their output."
SIMULATIONS,0.3054187192118227,"For the sphere dataset, we visualize the data generated from the model, with 8 latent dimensions,
trained on unit sphere with 2 intrinsic dimensions and 16 ambient dimensions (Column 2 in Table
3). Its training progression is shown as the orange curve in Figure 3 . We create a histogram of the
norm of its ﬁrst 3 dimensions (Figure 1 (a)) and found that more than half of the generated data falls
inside of the unit sphere. The generated data has one intrinsic dimension higher than its training
data, despite its decoder variance approaching zero, which is equivalent to its reconstruction error
approaching zero by Lemma 2."
SIMULATIONS,0.3078817733990148,"In the sigmoid dataset, the featured model has 24 latent dimension, and is trained on a 7-dimensional
manifold embedded in a 28-dimensional ambient space. We produced a scatter plot given 1000
generated data points ˜xr+1 from the decoder. The x-axis in the Figure 1(b) is ⟨a∗, ˜x:r⟩and the y-axis
is ˜xr+1. In contrast to the groundtruth data, whose scatter points roughly form a sigmoid function,
the scatter points of the generate data resemble a gaussian distribution. This closely resembles the
example provided in Theorem 6. Hence, despite its decoder variance and reconstruction error both
approaching zero and loss consistently decreasing, the generated data do not recover the training
data distribution and the data distribution recovered has higher intrinsic dimensions than the training
data. We also investigated the effect of lower bounding the decoder variance as a possible way to
improve the VAE performance (details are given in Appendix E). This enabled the VAE to recover
the correct manifold dimension in the sigmoid example, but not the sphere example; methods of
improvements to the VAE’s manifold recovery is an important direction for future work."
SIMULATIONS,0.3103448275862069,Published as a conference paper at ICLR 2022
REFERENCES,0.312807881773399,REFERENCES
REFERENCES,0.31527093596059114,"Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32:7413–7424, 2019."
REFERENCES,0.31773399014778325,"Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019."
REFERENCES,0.32019704433497537,"Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Hidden talents of the variational
autoencoder. arXiv preprint arXiv:1706.05148, 2017."
REFERENCES,0.3226600985221675,"Bin Dai, Ziyu Wang, and David Wipf. The usual suspects? reassessing blame for vae posterior
collapse. In International Conference on Machine Learning, pp. 2313–2322. PMLR, 2020."
REFERENCES,0.3251231527093596,"Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016."
REFERENCES,0.3275862068965517,"Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1–10. IEEE, 2018."
REFERENCES,0.33004926108374383,"Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534,
2019."
REFERENCES,0.33251231527093594,"Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012."
REFERENCES,0.33497536945812806,"Arthur Jacot, Franc¸ois Ged, Franck Gabriel, Berﬁn S¸ims¸ek, and Cl´ement Hongler. Deep linear
networks dynamics: Low-rank biases induced by initialization scale and l2 regularization. arXiv
preprint arXiv:2106.15933, 2021."
REFERENCES,0.3374384236453202,"Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018."
REFERENCES,0.3399014778325123,"Abhishek Kumar and Ben Poole. On implicit regularization in β-vaes. In International Conference
on Machine Learning, pp. 5480–5490. PMLR, 2020."
REFERENCES,0.34236453201970446,"Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning The-
ory, pp. 2–47. PMLR, 2018."
REFERENCES,0.3448275862068966,"Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent
for matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839, 2020."
REFERENCES,0.3472906403940887,"James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the elbo! a
linear vae perspective on posterior collapse. Advances in Neural Information Processing Systems,
32:9408–9418, 2019."
REFERENCES,0.3497536945812808,"Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. arXiv preprint arXiv:1711.00937, 2017."
REFERENCES,0.3522167487684729,"Adeel Pervez and Efstratios Gavves. Spectral smoothing unveils phase transitions in hierarchical
variational autoencoders. In International Conference on Machine Learning, pp. 8536–8545.
PMLR, 2021."
REFERENCES,0.35467980295566504,"Ali Razavi, A¨aron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-vaes. arXiv preprint arXiv:1901.03416, 2019a."
REFERENCES,0.35714285714285715,"Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with
vq-vae-2. In Advances in neural information processing systems, pp. 14866–14876, 2019b."
REFERENCES,0.35960591133004927,"Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. Kybernetes, 2013."
REFERENCES,0.3620689655172414,"Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822–2878, 2018."
REFERENCES,0.3645320197044335,Published as a conference paper at ICLR 2022
REFERENCES,0.3669950738916256,"Rong Tang and Yun Yang. On empirical bayes variational autoencoder: An excess risk bound. In
Conference on Learning Theory, pp. 4068–4125. PMLR, 2021."
REFERENCES,0.3694581280788177,"Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020."
REFERENCES,0.37192118226600984,"A
DERIVATIONS OF VAE LOSSES"
REFERENCES,0.37438423645320196,"We have (for some constants C1, C2, C3):"
REFERENCES,0.3768472906403941,log p(x|z) = −1
REFERENCES,0.3793103448275862,2ϵ2 ∥x −f(z)∥2 −d log(ϵ) + C1
REFERENCES,0.3817733990147783,log p(z) = −∥z∥2/2 + C2
REFERENCES,0.3842364532019704,log q(z|x) = −1
REFERENCES,0.3866995073891626,"2⟨z −g(x), D−1(z −g(x)⟩−log
√"
REFERENCES,0.3891625615763547,det D + C3
REFERENCES,0.3916256157635468,"where the ﬁrst line uses that log
√"
REFERENCES,0.39408866995073893,"det ϵ2I = log
√"
REFERENCES,0.39655172413793105,"ϵ2d = d log(ϵ). The VAE objective is to maximize
the expectation of log p(x|z) + log p(z) −log q(z|x) for x from the data p∗and z ∼q(z|x). This
means that explicitly the objective is to maximize"
REFERENCES,0.39901477832512317,Ex∼p∗Ez∼q(z|x) [log p(x|z) + log p(z) −log q(z|x)] −C
REFERENCES,0.4014778325123153,"= Ex∼p∗Ez∼q(z|x) 
−1"
REFERENCES,0.4039408866995074,2ϵ2 ∥x −f(z)∥2 −d log(ϵ) −∥z∥2/2 + 1
REFERENCES,0.4064039408866995,"2⟨z −g(x), D−1(z −g(x)⟩+ log
√"
REFERENCES,0.4088669950738916,"det D
"
REFERENCES,0.41133004926108374,"= Ex∼p∗Ez′∼N(0,Ir)
h
−1"
REFERENCES,0.41379310344827586,2ϵ2 ∥x −f(g(x) + D1/2z′)∥2 −d log(ϵ) −∥g(x) + D1/2z′∥2/2 + 1
REFERENCES,0.41625615763546797,"2⟨z′, z′⟩+ log
√"
REFERENCES,0.4187192118226601,"det D
i"
REFERENCES,0.4211822660098522,which simpliﬁes to (up to additive constant)
REFERENCES,0.4236453201970443,"Ex∼p∗Ez′∼N(0,Ir)
h
−1"
REFERENCES,0.42610837438423643,"2ϵ2 ∥x −f(g(x) + D1/2z′)∥2 −∥g(x)∥2/2
i
−d log(ϵ) −Tr(D)/2 + 1 2 X"
REFERENCES,0.42857142857142855,"i
log Dii."
REFERENCES,0.43103448275862066,and converting this to minimization form gives the VAE Loss (3).
REFERENCES,0.43349753694581283,"Linear VAE derivation.
Plugging in the linear VAE parameters into the loss function, we get"
REFERENCES,0.43596059113300495,"L( ˜A, ˜B, ˜D, ˜ϵ) := Ex∼p∗Ez′∼N(0,I˜r)
h 1"
REFERENCES,0.43842364532019706,"2˜ϵ2 ∥x −˜A( ˜Bx + ˜D1/2z′)∥2 + ∥˜Bx∥2/2
i
(7)"
REFERENCES,0.4408866995073892,+ d log(˜ϵ) + Tr( ˜D)/2 −1 2 X
REFERENCES,0.4433497536945813,"i
log ˜Dii
(8)"
REFERENCES,0.4458128078817734,We can write out the expectation as:
REFERENCES,0.4482758620689655,"Ez∼N(0,I)Ez′∼N(0,I˜r)
h 1"
REFERENCES,0.45073891625615764,"2˜ϵ2 ∥Az −˜A( ˜BAz + ˜D1/2z′)∥2 + ∥˜BAz∥2/2
i"
REFERENCES,0.45320197044334976,"= Ez∼N(0,I)Ez′∼N(0,I˜r)
h 1"
REFERENCES,0.45566502463054187,"2˜ϵ2 ∥(A −˜A ˜BA)z −˜A ˜D1/2z′∥2 + ∥˜BAz∥2/2
i"
REFERENCES,0.458128078817734,"=
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.4605911330049261,"2˜ϵ2 ∥˜A ˜D1/2∥2
F + 1"
REFERENCES,0.4630541871921182,"2∥˜BA∥2
F"
REFERENCES,0.46551724137931033,"where we used that z, z′ are independent and the identity Ez∼N(0,I)∥Mz∥2 = ⟨MM T , I⟩=
∥M∥2
F . Next, we can observe that"
REFERENCES,0.46798029556650245,"∥˜A ˜D1/2∥2
F =
X"
REFERENCES,0.47044334975369456,"i
˜Dii∥˜Ai∥2"
REFERENCES,0.4729064039408867,where ˜Ai is the i-th column of the matrix ˜A. Therefore we recover (4).
REFERENCES,0.4753694581280788,Published as a conference paper at ICLR 2022
REFERENCES,0.47783251231527096,"B
DEFERRED PROOFS FROM SECTION 4"
REFERENCES,0.4802955665024631,"B.1
GENERAL FACTS"
REFERENCES,0.4827586206896552,"Proof of Lemma 1. For completeness, we include the proof of these claims; they are similar to the
proofs of Theorems 4 and 5 in Dai & Wipf (2019)."
REFERENCES,0.4852216748768473,"First, consider the objective for ﬁxed f, g, D, ϵ and omit the subscript t. We have"
REFERENCES,0.4876847290640394,"Ex∼p∗Ez′∼N(0,Ir)
h 1"
REFERENCES,0.49014778325123154,"2ϵ2 ∥x −f(g(x) + D1/2z′)∥2 + ∥g(x)∥2/2
i
≥0"
REFERENCES,0.49261083743842365,"and
Tr(D)/2 −1 2 X"
REFERENCES,0.49507389162561577,"i
log Dii = 1 2 X"
REFERENCES,0.4975369458128079,"i
(Dii −log Dii) ≥r/2"
REFERENCES,0.5,"from the inequality x −log x ≥1 for x ≥0. Since these terms are both bounded below, the only
way the objective goes to negative inﬁnity is if d log ϵ →−∞which means ϵ →0."
REFERENCES,0.5024630541871922,"Now that we know ϵt →0, we claim that limt→∞Ex∼p∗Ez′∼N(0,Ir)∥x−ft(gt(x)+D1/2
t
z′)∥2 = 0.
Suppose otherwise: then this for inﬁnitely many t this quantity is lower bounded by some constant
c > 0, hence the objective for those t is lower bounded by c/ϵ2 + d log(ϵ) + r/2 and this goes to
+∞as ϵ →0, instead of −∞."
REFERENCES,0.5049261083743842,Proof of Lemma 2. Taking the partial derivative of (3) with respect to ϵ and setting it to zero gives
REFERENCES,0.5073891625615764,0 = −1
REFERENCES,0.5098522167487685,"ϵ3 Ex∼p∗Ez′∼N(0,Ir)∥x −f(g(x) + D1/2z′)∥2 + d ϵ"
REFERENCES,0.5123152709359606,and solving for ϵ gives the result.
REFERENCES,0.5147783251231527,"B.2
LINEAR VAE"
REFERENCES,0.5172413793103449,"Proof of Lemma 3. Taking the partial derivative with respect to ˜Dii gives 0 = ∥˜Ai∥2/˜ϵ2+1−1/ ˜Dii
which means
˜Dii =
1
∥˜Ai∥2/˜ϵ2 + 1
=
˜ϵ2"
REFERENCES,0.5197044334975369,∥˜Ai∥2 + ˜ϵ2
REFERENCES,0.5221674876847291,"hence
˜Dii∥˜Ai∥2/˜ϵ2 + ˜Dii −log ˜Dii = 1 −log
˜ϵ2"
REFERENCES,0.5246305418719212,∥˜Ai∥2 + ˜ϵ2 .
REFERENCES,0.5270935960591133,It follows that the objective value at the optimal D is
REFERENCES,0.5295566502463054,"L1( ˜A, ˜B, ˜ϵ) =
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.5320197044334976,"2∥˜BA∥2
F + d log ˜ϵ + 1 2 X i"
REFERENCES,0.5344827586206896,"
1 −log
˜ϵ2"
REFERENCES,0.5369458128078818,∥˜Ai∥2 + ˜ϵ2 
REFERENCES,0.5394088669950738,"=
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.541871921182266,"2∥˜BA∥2
F + (d −r) log ˜ε + 1 2 X i"
REFERENCES,0.5443349753694581,"
1 −log
1
∥˜Ai∥2 + ˜ϵ2 
."
REFERENCES,0.5467980295566502,"Proof of Theorem 3. First we prove the sufﬁciency direction, i.e. that if A = ˜A ˜BA and there exists
i such that ˜Ai = 0 then we show how to drive the loss to −∞. By Lemma 3, if we make the optimal
choice of D (which clearly satisﬁes the conditions on D described in the Lemma) the objective
simpliﬁes to"
REFERENCES,0.5492610837438424,"L1( ˜A, ˜B, ˜ϵ) =
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.5517241379310345,"2∥˜BA∥2
F + (d −r) log ˜ε + 1 2 X i"
REFERENCES,0.5541871921182266,"
1 + log

∥˜Ai∥2 + ˜ϵ2 = 1"
REFERENCES,0.5566502463054187,"2∥˜BA∥2
F + (d −r) log ˜ε + 1 2 X i"
REFERENCES,0.5591133004926109,"
1 + log

∥˜Ai∥2 + ˜ϵ2"
REFERENCES,0.5615763546798029,Published as a conference paper at ICLR 2022
REFERENCES,0.5640394088669951,"where in the second line we used the assumption A = ˜A ˜BA. Note that for each zero column ˜Ai = 0
we have (1/2) log(∥˜Ai∥2 + ˜ϵ2) = log ˜ϵ so the objective will go to −∞provided (d −r + #{i :
˜Ai = 0}) log ˜ϵ →−∞. Since ˜ϵ →0 this is equivalent to asking d −r + #{i : ˜Ai = 0} > 0, which
is exactly the assumption of the Theorem."
REFERENCES,0.5665024630541872,"Next we prove the converse direction, i.e. the necessary conditions. Note: we split the ﬁrst item
in the lemma into two conclusions in the proof below (so there are four conclusions instead of
three). The ﬁrst conclusion follows from the ﬁrst conclusion of Lemma 1. The second conclusion
of Lemma 1 tells us that"
REFERENCES,0.5689655172413793,"0 = lim
t→∞Ez∼N(0,I)Ez′∼N(0,I˜r)∥Az−˜At( ˜BtAz+ ˜D1/2
t
z′)∥2 = lim
t→∞∥A−˜At ˜BtA∥2
F +∥˜At ˜D1/2
t
∥2
F"
REFERENCES,0.5714285714285714,"which gives us the second and third conclusions above.
For the fourth conclusion, since
L1( ˜At, ˜Bt, ˜Dt) ≤L( ˜At, ˜Bt, ˜Dt, ˜ϵt) we know that limt→∞L1( ˜At, ˜Bt, ˜Dt) = −∞and recalling"
REFERENCES,0.5738916256157636,"L1( ˜A, ˜B, ˜ϵ) =
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.5763546798029556,"2∥˜BA∥2
F + (d −r) log ˜ϵ + 1 2 X i"
REFERENCES,0.5788177339901478,"
1 + log

∥˜Ai∥2 + ˜ϵ2"
REFERENCES,0.5812807881773399,"we see that, because the ﬁrst two terms are nonnegative, this is possible only if the sum of the last
two terms goes to −∞. Based on similar reasoning to the sufﬁciency case, this is only possible if
strictly more than r −d of the columns of ( ˜At) become arbitrarily close to zero; precisely, if there
exists δ such that at most r −d of the columns of ˜At have norm less than δ, then"
REFERENCES,0.583743842364532,(d −r) log ˜ϵ + 1 2 X i
REFERENCES,0.5862068965517241,"
1 + log

∥˜Ai∥2 + ˜ϵ2 ≥1 2 X"
REFERENCES,0.5886699507389163,"i:∥˜
Ai∥≥δ"
REFERENCES,0.5911330049261084,"
1 + log

∥˜Ai∥2 + ˜ϵ2 ≥1 2 X"
REFERENCES,0.5935960591133005,"i:∥˜
Ai∥≥δ"
REFERENCES,0.5960591133004927," 
1 + log
 
δ2 + ˜ϵ2"
REFERENCES,0.5985221674876847,which does not go to −∞as ˜ϵ →0 (and the other terms of L1 are nonnegative).
REFERENCES,0.6009852216748769,"B.3
NONLINEAR VAE"
REFERENCES,0.603448275862069,"We give the full details of the construction of the bad nonlinear VAE optimum and prove that it is
an asymptotic global minimum. (Note: in the notation of Section 6 we are considering a∗with 0/1
entries, but the proof generalizes straightforwardly for arbitrary a∗with the correct support.)"
REFERENCES,0.6059113300492611,"Setup:
Suppose s ≥1 is arbitrary and the ground truth x ∈Rd with d > r∗+s is generated in the
following way: (x1, . . . , xr∗) ∼N(0, Ir∗), xr∗+1 = σ(x1 +· · ·+xr∗) for an arbitrary nonlinearity
σ, and xr∗+2 = · · · = xd = 0. Furthermore, suppose the architecture for the decoder with latent
dimension r > r∗+ 1 is
f ˜
A1, ˜
A2(z) := ˜A1z + σ

˜A2z
"
REFERENCES,0.6083743842364532,"where σ(·) is applied as an entrywise nonlinearity, and the encoder is linear, g(x) := ˜Bx."
REFERENCES,0.6108374384236454,"Observe that the ground truth decoder can be expressed by taking ˜A2 to have a single nonzero row
in position r + 1 with entries (1, . . . , 1, 0, . . . , 0),"
REFERENCES,0.6133004926108374,"˜A1 =

Ir∗
0
0
0"
REFERENCES,0.6157635467980296,"
,
˜B =

Ir∗
0
0
0 
."
REFERENCES,0.6182266009852216,where ˜B is a ground truth encoder which achieves perfect reconstruction.
REFERENCES,0.6206896551724138,"On the other hand, the following VAE different from the ground truth achieves perfect reconstruc-
tion:
˜A1 =

Ir∗+s
0
0
0"
REFERENCES,0.6231527093596059,"
,
˜A2 = 0,
˜B =

Ir∗+1
0
0
0 
(9)"
REFERENCES,0.625615763546798,Published as a conference paper at ICLR 2022
REFERENCES,0.6280788177339901,"The output of this decoder is a Gaussian N

0,

Ir∗+s
0
0
0"
REFERENCES,0.6305418719211823,"
, which means it is strictly higher-"
REFERENCES,0.6330049261083743,"dimensional than the ground truth dimension r∗. (This also means that if we drew the corresponding
plot of to Figure 1 (b) for this model, we would get something that looks just like the experimentally
obtained result.) We prove in the Appendix that it this is an asymptotic global optima:
Theorem 6. Let s ≥1 be arbitrary and the ground truth and VAE architecture is as deﬁned as
above. For any sequence ˜ϵt →0, there exist diagonal matrices ˜Dt such that:"
REFERENCES,0.6354679802955665,"1. the VAE loss L( ˜A1, ˜A2, ˜B, ˜Dt, ˜ϵt) →−∞where ˜A1, ˜A2, ˜B are deﬁned by (9)"
REFERENCES,0.6379310344827587,2. The number of coordinates of ˜Dt which go to zero equals r∗+ s.
REFERENCES,0.6403940886699507,"Proof. We show how to pick ˜Dt as a function of ˜ϵt and that if ˜ϵt →0, the loss goes to −∞. From
now on we drop the subscripts."
REFERENCES,0.6428571428571429,"With these parameters, the VAE loss is"
REFERENCES,0.645320197044335,"Ex∼p∗Ez′∼N(0,Ir)
h 1"
REFERENCES,0.6477832512315271,"2˜ϵ2 ∥x −f(g(x) + ˜D1/2z′)∥2 + ∥g(x)∥2/2
i
+ d log(˜ϵ) + Tr( ˜D)/2 −1 2 X"
REFERENCES,0.6502463054187192,"i
log ˜Dii"
REFERENCES,0.6527093596059114,= (1/2˜ϵ2)
REFERENCES,0.6551724137931034,"r∗+1
X"
REFERENCES,0.6576354679802956,"i=1
˜Dii + Ex∼p∗
h
∥x1:r∗+1∥2/2
i
+ d log(˜ϵ) + Tr( ˜D)/2 −1 2 X"
REFERENCES,0.6600985221674877,"i
log ˜Dii."
REFERENCES,0.6625615763546798,"Taking the partial derivative with respect to ˜Dii for i ≤r∗+ s and optimizing gives 0 = (1/˜ϵ2) +
1 −1/ ˜Dii i.e."
REFERENCES,0.6650246305418719,"˜Dii =
1
1 + 1/˜ϵ2 =
˜ϵ2"
REFERENCES,0.6674876847290641,"˜ϵ2 + 1
and plugging this into the objective gives"
REFERENCES,0.6699507389162561,(1/2˜ϵ2)
REFERENCES,0.6724137931034483,"r∗+1
X"
REFERENCES,0.6748768472906403,"i=1
˜Dii + Ex∼p∗
h
∥x1:r∗+1∥2/2
i
+ d log(˜ϵ) + Tr( ˜D)/2 −1 2 X"
REFERENCES,0.6773399014778325,"i
log ˜Dii"
REFERENCES,0.6798029556650246,= (1/2)
REFERENCES,0.6822660098522167,"r∗+1
X i=1"
REFERENCES,0.6847290640394089,"1
˜ϵ2 + 1 + Ex∼p∗
h
∥x1:r∗+1∥2/2
i
+ (d −r∗−s) log(˜ϵ)"
REFERENCES,0.687192118226601,+ Tr( ˜D)/2 + r∗+ 1
REFERENCES,0.6896551724137931,"2
log(1 + ϵ2) + 1 2 r
X"
REFERENCES,0.6921182266009852,"i=r∗+2
log ˜Dii."
REFERENCES,0.6945812807881774,"Setting the remaining ˜Dii to 1, we see that using d > r∗+ s that the loss goes to −∞provided
˜ϵ →0, proving the result."
REFERENCES,0.6970443349753694,"C
DEFERRED PROOFS FROM SECTION 5"
REFERENCES,0.6995073891625616,"First, we formalize the rotation invariance of the objective."
REFERENCES,0.7019704433497537,"Lemma 6 (Rotational Invariance of Gradient Descent on Linear VAE). Let LA( ˜A, ˜B, ˜D, ˜ϵ) denote
the VAE population loss objective (4). Then for an arbitrary orthogonal matrix U, we have"
REFERENCES,0.7044334975369458,"LA( ˜A, ˜B, ˜D, ˜ϵ) = LUA(U ˜A, ˜BU T , ˜D, ˜ϵ).
Furthermore,
U∇˜
ALA( ˜A, ˜B, ˜D, ˜ϵ) = ∇U ˜
ALUA(U ˜A, ˜BU T , ˜D, ˜ϵ)
and
(∇˜
BLA( ˜A, ˜B, ˜D, ˜ϵ))U T = ∇U ˜
BLUA(U ˜A, ˜BU T , ˜D, ˜ϵ)."
REFERENCES,0.7068965517241379,"As a consequence, if for any η ≥0 we deﬁne ( ˜A1, ˜B1, ˜D1, ˜ϵ1) = ( ˜A, ˜B, ˜D, ˜ϵ) −η∇LA( ˜A, ˜B, ˜D, ˜ϵ)
then
(U ˜A1, ˜B1U T , ˜D1, ˜ϵ1) = (U ˜A, ˜BU T , ˜D, ˜ϵ) −η∇(U ˜
A, ˜
BU T , ˜
D,˜ϵ)LUA(U ˜A, ˜BU T , ˜D, ˜ϵ),"
REFERENCES,0.7093596059113301,"i.e. gradient descent preserves rotations by U. The same result holds for the gradient ﬂow (i.e.
continuous time gradient descent), or replacing everywhere the loss L by the simpliﬁed loss L1."
REFERENCES,0.7118226600985221,Published as a conference paper at ICLR 2022
REFERENCES,0.7142857142857143,"Proof of Lemma 6. We give the proof for L as stated, but it is exactly the same for the simpliﬁed
loss L1."
REFERENCES,0.7167487684729064,From the objective function (4) and U T = U −1 observe that
REFERENCES,0.7192118226600985,"LUA(U ˜A, ˜BU T , ˜D, ˜ϵ)"
REFERENCES,0.7216748768472906,"=
1
2˜ϵ2 ∥UA −U ˜A ˜BU −1UA∥2
F + 1"
REFERENCES,0.7241379310344828,"2∥˜BU −1UA∥2
F + d log ˜ϵ + 1 2 X i"
REFERENCES,0.7266009852216748,"
˜Dii∥U ˜Ai∥2/˜ϵ2 + ˜Dii −log ˜Dii
"
REFERENCES,0.729064039408867,"=
1
2˜ϵ2 ∥A −˜A ˜BA∥2
F + 1"
REFERENCES,0.7315270935960592,"2∥˜BA∥2
F + d log ˜ϵ + 1 2 X i"
REFERENCES,0.7339901477832512,"
˜Dii∥˜Ai∥2/˜ϵ2 + ˜Dii −log ˜Dii
"
REFERENCES,0.7364532019704434,"= LA( ˜A, ˜B, ˜D, ˜ϵ)."
REFERENCES,0.7389162561576355,Then from the above and the multivariate chain rule have
REFERENCES,0.7413793103448276,"∇˜
ALA( ˜A, ˜B, ˜D, ˜ϵ) = ∇˜
ALUA(U ˜A, ˜BU T , ˜D, ˜ϵ) = U T 
∇U ˜
ALUA(U ˜A, ˜BU −1, ˜D, ˜ϵ)
"
REFERENCES,0.7438423645320197,"so multiplying both sides on the left by U and using U T = U −1 gives the second claim, and similarly"
REFERENCES,0.7463054187192119,"∇˜
BLA( ˜A, ˜B, ˜D, ˜ϵ) = ∇˜
BLUA(U ˜A, ˜BU T , ˜D, ˜ϵ) = (∇˜
BU T LUA(U ˜A, ˜BU T , ˜D, ˜ϵ))U"
REFERENCES,0.7487684729064039,gives the third claim. Then the gradient descent claim follows immediately.
REFERENCES,0.7512315270935961,"Proof of Lemma 4. First we prove the conclusion for the original loss L.
Since ( ˜A ˜BA)iℓ=
P"
REFERENCES,0.7536945812807881,"j,k ˜Aij ˜BjkAkℓwe have that"
REFERENCES,0.7561576354679803,"∂∥A −˜A ˜BA∥2
F
∂˜Aij
=
∂
∂˜Aij X ℓ "
REFERENCES,0.7586206896551724,"Aiℓ−
X"
REFERENCES,0.7610837438423645,"j′,k
˜Aij′ ˜Bj′kAkℓ   2 =
X ℓ
2 "
REFERENCES,0.7635467980295566,"Aiℓ−
X"
REFERENCES,0.7660098522167488,"j′,k
˜Aij′ ˜Bj′kAkℓ    −
X"
REFERENCES,0.7684729064039408,"k
˜BjkAkℓ !"
REFERENCES,0.770935960591133,and if we know the corresponding row i in A is zero then this simpliﬁes to
REFERENCES,0.7733990147783252,"∂∥A −˜A ˜BA∥2
F
∂˜Aij
=
X ℓ
2  X"
REFERENCES,0.7758620689655172,"j′,k
˜Aij′ ˜Bj′kAkℓ   X"
REFERENCES,0.7783251231527094,"k
˜BjkAkℓ !"
REFERENCES,0.7807881773399015,which means that X
REFERENCES,0.7832512315270936,"j
˜Aij
∂∥A −˜A ˜BA∥2
F
∂˜Aij
=
X ℓ
2  X"
REFERENCES,0.7857142857142857,"j,k
˜Aij ˜BjkAkℓ   2"
REFERENCES,0.7881773399014779,= 2∥( ˜A ˜BA)(i)∥2
REFERENCES,0.7906403940886699,"where the notation A(i) denotes row i of matrix A. Thus, for this term the gradient with respect to
row ˜A(i) has nonnegative dot product with row ˜A(i)."
REFERENCES,0.7931034482758621,"Also,
∂
∂˜Aij
(1/2)
X"
REFERENCES,0.7955665024630542,"i
˜Djj∥˜Aj∥2/˜ϵ2 = ˜Djj ˜Aij/˜ϵ2"
REFERENCES,0.7980295566502463,"and so
X"
REFERENCES,0.8004926108374384,"j
˜Aij
∂
∂˜Aij
(1/2)
X"
REFERENCES,0.8029556650246306,"j
˜Dj∥˜Aj∥2/˜ϵ2 =
X"
REFERENCES,0.8054187192118226,"j
˜Djj ˜A2
ij/˜ϵ2"
REFERENCES,0.8078817733990148,which gives the ﬁrst result.
REFERENCES,0.8103448275862069,"For the second result with the simpliﬁed loss L1, observe that"
REFERENCES,0.812807881773399,"∂
∂˜Aij X"
REFERENCES,0.8152709359605911,"k
log(∥˜Ak∥2 + ˜ϵ2) =
2 ˜Aij
∥˜Aj∥2 + ˜ϵ2 so
X"
REFERENCES,0.8177339901477833,"j
˜Aij
∂
∂˜Aij X"
REFERENCES,0.8201970443349754,"k
log(∥˜Ak∥2 + ˜ϵ2) =
X j"
REFERENCES,0.8226600985221675,"2 ˜A2
ij
∥˜Aj∥2 + ˜ϵ2"
REFERENCES,0.8251231527093597,"and the other terms in the loss behave the same in the case of L. Including the factor of 1/2 from
the loss function gives the result."
REFERENCES,0.8275862068965517,Published as a conference paper at ICLR 2022
REFERENCES,0.8300492610837439,"Proof of Lemma 5. From Lemma 4 we have that for any such row i,"
REFERENCES,0.8325123152709359,"d
dt∥˜A(i)(t)∥2 = 2⟨˜A(i)(t), d"
REFERENCES,0.8349753694581281,"dt
˜A(i)(t)⟩"
REFERENCES,0.8374384236453202,"= 2⟨˜A(i)(t), −∇˜
A(t)(i)L1(Θt)⟩≤− r
X j=1"
REFERENCES,0.8399014778325123,"( ˜A(t))2
ij
∥( ˜A(t))j∥2 + ˜ϵ2
t
≤−(1/K)∥˜A(i)(t)∥2"
REFERENCES,0.8423645320197044,which by Gronwall’s inequality implies ∥˜A(i)(t)∥2 ≤e−t/K∥˜A(i)(0)∥2 as desired.
REFERENCES,0.8448275862068966,"Proof of Theorem 5. Before proceeding, we observe that the ﬁrst inequality in (6) is a consequence
of the general min-max characterization of singular values, see e.g. Horn & Johnson (2012). We
now prove the rest of the statement."
REFERENCES,0.8472906403940886,"As explained at the beginning of the section, we start by taking the Singular Value Decomposition
A = USV T where S is the diagonal matrix of singular values and U, V are orthogonal. We assume
the diagonal matrix S is sorted so that its top-left entry is the largest singular value and its bottom-
right is the smallest. Note that this means the ﬁrst dim(W) rows of U are an orthonormal basis
for W. Note that for any time t, ∥PW ⊥˜AT (t)∥2
F = Pd
i=dim(W )+1 ∥(U ˜A(t)T )i∥2 because the rows
(Udim(W )+1, . . . , Ud) are an orthonormal basis for W ⊥. Therefore we have that"
REFERENCES,0.8497536945812808,"∥PW ⊥˜AT (t)∥2
F = d
X"
REFERENCES,0.8522167487684729,"i=dim(W )+1
∥(U ˜A(t)T )i∥2"
REFERENCES,0.854679802955665,"≤e−t/K
d
X"
REFERENCES,0.8571428571428571,"i=dim(W )+1
∥(U ˜A(0)T )i∥2 = e−t/K∥PW ⊥˜AT (0)∥2
F ,"
REFERENCES,0.8596059113300493,"proving the result, provided we justify the middle inequality. Deﬁne A∗:= U T A = SV T , which
has a zero row for every zero singular value of A, and apply Lemma 5 (using that the deﬁnition of K
is invariant to left-multiplication of ˜A by an orthogonal matrix) and Lemma 6 to conclude that the
rows of U T ˜A(t), i.e. the columns of U ˜A(t)T , corresponding to zero rows of A∗shrink by a factor
of e−t/K. This directly gives the desired inequality, completing the proof."
REFERENCES,0.8620689655172413,"D
DEFERRED FIGURES AND PLOTS FROM SECTION 6"
REFERENCES,0.8645320197044335,"Eigenvalues of Linear Data.
As we’ve discussed, in our linear setting the VAE does not recover
the ground truth data density. Since our generative process for ground-truth data is x = Az for a
matrix A and z normally distributed, we can characterize the density function by the eigenvalues of
the true or estimated covariance matrix. We give ﬁgures for the normalized error of these eigenvalues
between the learned generator and the ground truth in Table 1. A concrete example of eigenvalue
mismatch for a problem with 6 nonzero dimensions is a ground-truth set of covariance eigenvalues"
REFERENCES,0.8669950738916257,"λ = [0.001
0.156
1.54
5.06
9.55
16.4]"
REFERENCES,0.8694581280788177,while the trained linear VAE distribution has covariance eigenvalues
REFERENCES,0.8719211822660099,"ˆλ = [0.035
0.166
1.49
4.24
5.97
7.85] ."
REFERENCES,0.874384236453202,"Here, the VAE was easily able to learn the support of the data but clearly is very off when it comes
to the structure of the covariances."
REFERENCES,0.8768472906403941,"E
EXPERIMENTS WITH DECODER VARIANCE CLIPPING"
REFERENCES,0.8793103448275862,"As was suggested by an anonymous ICLR 2022 reviewer, one potential way to evade the results
in our paper is to restrict the decoder variance from converging to 0. In this section, we examine
(empirically) the impact of clipping the decoder variance during training. We caveat though, that
our paper does not analyze the landscape of the resulting constrained optimization problem, so our
results don’t imply anything about this regime."
REFERENCES,0.8817733990147784,Published as a conference paper at ICLR 2022
REFERENCES,0.8842364532019704,"0
5
10
15
20
25
30
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
REFERENCES,0.8866995073891626,Manifold Error
REFERENCES,0.8891625615763546,"0
5
10
15
20
25
30 0.00 0.05 0.10 0.15 0.20 0.25"
REFERENCES,0.8916256157635468,Decoder Variance
REFERENCES,0.8940886699507389,"0
5
10
15
20
25
30
100 50 0 50 100 150 200 250 300"
REFERENCES,0.896551724137931,VAE Loss
REFERENCES,0.8990147783251231,"3 intrinsic dim; 7 ambient dim; 6 latent dim
3 intrinsic dim; 17 ambient dim; 8 latent dim
5 intrinsic dim; 22 ambient dim; 16 latent dim
5 intrinsic dim; 11 ambient dim; 10 latent dim
7 intrinsic dim; 15 ambient dim; 13 latent dim
7 intrinsic dim; 28 ambient dim; 24 latent dim"
REFERENCES,0.9014778325123153,"Figure 2: VAE training on 6 datasets with different choices of dimensions for sigmoidal dataset (see Setup
in Section 6). The x-axis represents every 5000 gradient updates during training. The left-most ﬁgure is
the manifold error (see Setup in Section 6), The middle and right ﬁgure conﬁrms that the decoder variance
approaches zero and the VAE loss is steadily decreasing during the ﬁnite training time."
REFERENCES,0.9039408866995073,"0
5
10
15
20
25
30 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
REFERENCES,0.9064039408866995,Manifold Error
REFERENCES,0.9088669950738916,"0
5
10
15
20
25
30 0.00 0.02 0.04 0.06 0.08 0.10 0.12"
REFERENCES,0.9113300492610837,Decoder Variance
REFERENCES,0.9137931034482759,"0
5
10
15
20
25
30 50 25 0 25 50 75 100"
REFERENCES,0.916256157635468,VAE Loss
REFERENCES,0.9187192118226601,"2 intrinsic dim; 6 ambient dim; 6 latent dim
2 intrinsic dim; 16 ambient dim; 8 latent dim
4 intrinsic dim; 21 ambient dim; 16 latent dim
4 intrinsic dim; 10 ambient dim; 10 latent dim
6 intrinsic dim; 14 ambient dim; 13 latent dim"
REFERENCES,0.9211822660098522,"Figure 3: VAE training on 5 datasets generated by appending zeros to uniformly random samples from a unit
sphere to embed in a higher dimensional ambient space. The x-axis represents each iteration of every 5000
gradient updates. The left-most ﬁgure is the manifold error ( see Setup in Section 6), The middle and right
ﬁgure conﬁrms that the decoder variance approaches zero and the VAE loss is steadily decreasing during the
ﬁnite training time."
REFERENCES,0.9236453201970444,"Intrinsic Dimensions
2
2
4
4
6
Ambient Dimensions
6
16
10
21
14
VAE Latent Dimensions
6
8
10
16
13"
REFERENCES,0.9261083743842364,"Mean Manifold Error
0.02
0.14
0.04
0.06
0.03
Mean #0’s in Encoder Variance
3
5
5
6
7"
REFERENCES,0.9285714285714286,"Table 3: Optima found by training a VAE on data generated by padding uniformly random samples from a
unit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated the
manifold error as described in the setup. The VAE training on this dataset has consistently yielded encoder
variances with number of 0 entries greater than the number of intrinsic dimension."
REFERENCES,0.9310344827586207,Published as a conference paper at ICLR 2022
REFERENCES,0.9334975369458128,"We conduct the same nonlinear experiments described in Section 6 where we ﬁt VAEs to data gen-
erated from spheres and linear sigmoid functions. The only change is to clip the decoder variance
when it goes below a certain threshold. In the ﬁgures below, the chosen threshold is e−4 ≈0.018,
though we tried also e−2 and e−3, with similar outcomes. We initialize the decoder variance with
e−3 for this set of experiments, so the optimization still can decrease it."
REFERENCES,0.9359605911330049,"With this change, the optimization process on the sigmoid dataset does yield encoder variances
with their number of zeros reﬂective of their intrinsic dimensions as in Table 4. For the sphere
experiment, this still does not happen, as in Table 5. In fact, the model consistently recovers one
more dimension than the true intrinsic dimension of the manifold and the smaller encoder variances
can be as large as 0.1. We also provide a ﬁgure (Figure 4) in the same style as Figure 1. We see
that training with a clipped decoder variance allows the model to better capture the general shape
of the sigmoid function, though the variance of the generated points is high for both of the sphere
and sigmoid datasets. Other training details, such as the general trend of manifold error, encoder
variance and VAE loss, can be referred to in Figure 5 and 6."
REFERENCES,0.9384236453201971,"Overall, the beneﬁt of clipping the decoder variance during training is inconclusive as we see in-
consistent results in the sphere and sigmoid datasets. Designing more algorithms to improve the
ability of VAE’s to recover data supported on a low dimensional manifold is an important direction
for future work—both empirical and theoretical."
REFERENCES,0.9408866995073891,"Figure 4: A demonstration of how the data points generated by the model trained with clipped decoder variance
is distributed.
Left ﬁgure: A histogram of the norms of samples generated from the VAE restricted to the
dimensions which are not zero, which shows many of the points have norm less than 1. (The ground-truth
distribution would output only samples of norm 1.) The particular example here is Column 2 in Table 5. The
data points that do not fall on the sphere tend to lie on both sides of it whereas the those generated without
decoder variance clipping tend to lie inside the sphere as in Figure 1. Right ﬁgure: Two-dimensional linear
projection of data output by VAE generator trained on our sigmoid dataset. The x-axis denotes ⟨a∗, ˜x:r⟩and
the y-axis is ˜xr+1, the blue points are from the trained VAE and the orange points are from the ground truth.
The generated data points roughly capture the shape of the sigmoid function."
REFERENCES,0.9433497536945813,"0
5
10
15
20
25
30
0.0 0.5 1.0 1.5"
REFERENCES,0.9458128078817734,Manifold Error
REFERENCES,0.9482758620689655,"0
5
10
15
20
25
30 0.05 0.10 0.15 0.20"
DECODER VARIANCE,0.9507389162561576,"0.25
Decoder Variance"
DECODER VARIANCE,0.9532019704433498,"0
5
10
15
20
25
30 0 200 400 600 800 1000"
DECODER VARIANCE,0.9556650246305419,VAE Loss
DECODER VARIANCE,0.958128078817734,"3 intrinsic dim; 7 ambient dim; 6 latent dim
3 intrinsic dim; 17 ambient dim; 8 latent dim
5 intrinsic dim; 22 ambient dim; 16 latent dim
5 intrinsic dim; 11 ambient dim; 10 latent dim
7 intrinsic dim; 15 ambient dim; 13 latent dim
7 intrinsic dim; 28 ambient dim; 24 latent dim"
DECODER VARIANCE,0.9605911330049262,"Figure 5: VAE training on 6 datasets with different choices of dimensions for sigmoidal dataset (see Setup
in Section 6). The x-axis represents every 5000 gradient updates during training. The left-most ﬁgure is the
manifold error (see Setup in Section 6), The middle and right ﬁgure shows that as the decoder variance is
bounded below, the VAE loss stops decreasing further."
DECODER VARIANCE,0.9630541871921182,Published as a conference paper at ICLR 2022
DECODER VARIANCE,0.9655172413793104,"0
5
10
15
20
25
30 0.05 0.10 0.15 0.20 0.25 0.30"
MANIFOLD ERROR,0.9679802955665024,"0.35
Manifold Error"
MANIFOLD ERROR,0.9704433497536946,"0
5
10
15
20
25
30 0.02 0.04 0.06 0.08 0.10 0.12"
MANIFOLD ERROR,0.9729064039408867,Decoder Variance
MANIFOLD ERROR,0.9753694581280788,"0
5
10
15
20
25
30 0 20 40 60 80"
VAE LOSS,0.9778325123152709,"100
VAE Loss"
VAE LOSS,0.9802955665024631,"2 intrinsic dim; 6 ambient dim; 6 latent dim
2 intrinsic dim; 16 ambient dim; 8 latent dim
4 intrinsic dim; 21 ambient dim; 16 latent dim
4 intrinsic dim; 10 ambient dim; 10 latent dim
6 intrinsic dim; 14 ambient dim; 13 latent dim"
VAE LOSS,0.9827586206896551,"Figure 6: VAE training on 5 datasets generated by appending zeros to uniformly random samples from a unit
sphere to embed in a higher dimensional ambient space. The x-axis represents each iteration of every 5000
gradient updates. The left-most ﬁgure is the manifold error ( see Setup in Section 6), The middle and right
ﬁgure shows that as the decoder variance is bounded below, the VAE loss stops decreasing further."
VAE LOSS,0.9852216748768473,"Intrinsic Dimensions
3
3
5
5
7
7
Ambient Dimensions
7
17
11
22
15
28
VAE Latent Dimensions
6
8
10
16
13
24"
VAE LOSS,0.9876847290640394,"Mean Manifold Error
0.15
0.15
0.23
0.23
0.24
0.24
Mean #0’s in Encoder Variance
3
3
5
5
7
7"
VAE LOSS,0.9901477832512315,"Table 4: Optima found by training a VAE on the sigmoid dataset. The VAE training yields encoder variances
with number of 0 entries equal to the intrinsic dimension."
VAE LOSS,0.9926108374384236,"Intrinsic Dimensions
2
2
4
4
6
Ambient Dimensions
6
16
10
21
14
VAE Latent Dimensions
6
8
10
16
13"
VAE LOSS,0.9950738916256158,"Mean Manifold Error
0.03
0.03
0.03
0.02
0.02
Mean #0.1’s in Encoder Variance
3
3
5
5
7"
VAE LOSS,0.9975369458128078,"Table 5: Optima found by training a VAE on data generated by padding uniformly random samples from a
unit r-sphere with zeroes, so that the sphere is embedded in a higher ambient dimension. We evaluated the
manifold error as described in the setup. The VAE training on this dataset has consistently yielded encoder
variances with number of 0.1 entries greater than the number of intrinsic dimension."
