Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004098360655737705,"Antimicrobial peptides (AMPs) have shown promising results in broad-spectrum
antibiotics and resistant infection treatments, making it a magnet for the ﬁeld of
drug discovery. Recently, an increasing number of researchers have been intro-
ducing deep generative models to AMP design. However, few studies consider
secondary structure information during the generation, though it has shown cru-
cial inﬂuence on antimicrobial activity in all AMP mechanism theories. This paper
proposes LSSAMP which uses the multi-scale VQ-VAE to learn the positional la-
tent spaces modeling the secondary structure. By Sampling in the latent secondary
structure space, we can generate peptides with ideal amino acids and secondary
structures simultaneously. Experimental results show that our LSSAMP can gener-
ate peptides with multiple ideal physical attributes and a high probability of being
predicted as AMPs by public AMP prediction models."
INTRODUCTION,0.00819672131147541,"1
INTRODUCTION"
INTRODUCTION,0.012295081967213115,"Developing neural networks for drug discovery has attracted increasing attention recently. It can fa-
cilitate the discovery of potential therapies and reduce the time and cost of drug development (Stokes
et al., 2020). Plenty of works have been done to employ deep generative models in searching for
drug-like molecules with desired properties and achieved great success (Jin et al., 2018; Shi et al.,
2019; Schwalbe-Koda & G´omez-Bombarelli, 2020; Xie et al., 2020). However, these works mainly
focus on small molecules, and more complicated biochemicals, such as proteins, are still rarely
explored."
INTRODUCTION,0.01639344262295082,"Antimicrobial peptides (AMPs), deﬁned as short proteins of less than 50 amino acids with potent
antimicrobial activity, are an emerging category of therapeutic agents. AMPs exist widely in the nat-
ural immune system for all species and kill bacteria in a physical way (Aronica et al., 2021; Cardoso
et al., 2020). They attach to the bacterial membrane and insert into the membrane to form pores,
which leads to the death of bacteria by allowing cytoplasmic leakage. This mechanism makes them
more promising for handling extensively drug-resistant bacteria than traditional antibiotics (Mahla-
puu et al., 2016). However, the theoretical chemical space of peptides is enormous and the sequence
number grows exponentially as the length increases. Thus, it is challenging to search for valid
peptides with antimicrobial properties from such a huge sequence space."
INTRODUCTION,0.020491803278688523,"Several factors can affect the antimicrobial activity of peptides (Boman, 2003). Amino acids with
positive charges are more likely to bind with bacterial membrane as most bacterial surfaces are
anionic, while those with high hydrophobicities tend to move from the solution environment to the
bacterial membrane. However, the mechanisms of antibacterial peptides need not only a reasonable
sequence but also an appropriate structure. For example, by forming the helix structure, a peptide
can gather the hydrophobic amino acids on one side and hydrophilic ones on the other. This ability,
named amphipathy, helps it insert into the membrane and maintain a stable hole with other peptide
molecules in the membrane, as shown in Figure 1. The hole will drain the cytoplasm and ﬁnally kill
the bacteria. This mechanism of killing the bacteria is called ‘barrel stave’. Amphipathy plays an
important role in deciding the antibacterial activity of peptides and is closely related to the secondary
structure of the peptide (Aronica et al., 2021)."
INTRODUCTION,0.02459016393442623,"According to the antimicrobial mechanism, a new AMP should meet the following criteria. C1: It
possesses several ideal physical attributes (e.g. positive charge and high hydrophobicity). C2: It"
INTRODUCTION,0.028688524590163935,"Under review as a conference paper at ICLR 2022 !""#$"
INTRODUCTION,0.03278688524590164,"%&&'()
%&&'()"
INTRODUCTION,0.036885245901639344,"Figure 1: An example of the ‘barrel stave’ antimicrobial mechanism. The blue indicates the hy-
drophobic amino acids, and the red ones are hydrophilic. On the left, although the peptides with
reasonable amino acids has attached to the bacterial membrane, they still can not insert into it. How-
ever, by folding into the helix structure, as shown on the right, the peptides maintain a stable hole
which breaks the membrane of the bacterium."
INTRODUCTION,0.040983606557377046,"has appropriate secondary structures (e.g. alpha-helix). C3: It differs from existing AMPs to some
extent."
INTRODUCTION,0.045081967213114756,"The existing works mainly focus on sequential features of amino acids and ignore the secondary
structure. The traditional methods replace subsequences with patterns from the pattern database in
a given template (Porto et al., 2018). Inspired by success in deep neural networks, many researchers
apply neural generative models to AMP discovery. They often use the physical attributes as the extra
input to control the generation phase (Das et al., 2018; Van Oort et al., 2021), or train classiﬁers on
each attribute to ﬁlter the peptides after the generation (Capecchi et al., 2021; Das et al., 2021). The
former ones usually generate peptides that have low correlation with the input attributes and the ﬁlter
phase of the latter ones make the sampling inefﬁcient."
INTRODUCTION,0.04918032786885246,"As described above, the antimicrobial activity is determined by both the amino acid composition
and secondary structure of the peptide. Thus, we propose LSSAMP to generate antimicrobial pep-
tides from the latent semantic and structure space. Taking the peptide sequence as the time series,
we assign a latent variable on each position. Since it is computationally intractable to sum con-
tinuous latent variables over all positions, we employ the vector quantized-variational autoencoder
(VQ-VAE) (van den Oord et al., 2017) to learn the discrete distribution for each position and fur-
ther design a multi-scale codebooks strategy to capture different local patterns to ﬁt various length
ranges for amino acid and structure sequences. During the generation process, LSSAMP will sample
a backbone from the secondary structure latent space and generate the amino acid sequence simul-
taneously. We evaluate LSSAMP and several baselines through physical properties that are closely
related to the antibacterial mechanism. Besides, we use some public AMP prediction models to
predict generated sequences being AMPs as previous works did (Das et al., 2020; Van Oort et al.,
2021)."
INTRODUCTION,0.05327868852459016,"To conclude, our contributions are as follows:"
INTRODUCTION,0.05737704918032787,"• We propose LSSAMP, a generative model which samples peptides from the latent secondary struc-
ture space to control the peptide properties."
INTRODUCTION,0.06147540983606557,"• We develop a multi-scale VQ-VAE to learn positional latent spaces from different aspects and
model semantic sequences and structural sequences in the same space."
INTRODUCTION,0.06557377049180328,"• Experimental results show that LSSAMP can generate peptides with multiple ideal features such
as positive charge, better hydrophobicity, and better amphipathicity. The results of public AMP
classiﬁers also verify that our model can generate peptides with high AMP probability."
RELATED WORK,0.06967213114754098,"2
RELATED WORK"
RELATED WORK,0.07377049180327869,"Antimicrobial Peptides Generation Traditional methods for AMP design can be divided into three
approaches (Torres & de la Fuente-Nunez, 2019): (i) The pattern recognition algorithms build a
sequential pattern database from existing AMPs, and then pick a template peptide and replace local
sequence with patterns (Loose et al., 2006; Porto et al., 2018). (ii) The genetic algorithms analyze
the AMP database and design some antibiotic activity functions (Maccari et al., 2013). (iii)The"
RELATED WORK,0.0778688524590164,Under review as a conference paper at ICLR 2022
RELATED WORK,0.08196721311475409,Encoder
RELATED WORK,0.0860655737704918,"ℎ! ℎ"" ℎ#
…
ℎ$"
RELATED WORK,0.09016393442622951,Generator
RELATED WORK,0.0942622950819672,Classifier ⨁
RELATED WORK,0.09836065573770492,"Sample seed
Condition"
RELATED WORK,0.10245901639344263,"GLFDI V K KV V G A
G S
F
L"
RELATED WORK,0.10655737704918032,"GLFDI V K KV V G A
G S
F
L ⨁ …"
RELATED WORK,0.11065573770491803,"Figure 2: The framework of LSSAMP. The input sequence is ﬁrst fed into the encoder to get the
hidden representation h. Then, we use N pattern selectors to select local patterns with different
scales and use the corresponding codebooks to obtain discrete latent variables for each position. The
codebooks are made to learn the latent space of the secondary structure by employing the secondary
structure prediction task on them. For inference, we generate peptides from the latent structure space
by sampling the index sequences on the codebooks. We also try to use some conditions to restrict
the structure motifs to further improve the generation performance."
RELATED WORK,0.11475409836065574,"molecular modeling and molecular dynamics methods build 3D models of peptides and analyze
activity (Matyus et al., 2007; Bolintineanu & Kaznessis, 2011). Deep generative models take a
rapid growth in recent years. Dean & Walper (2020) encodes the peptide into the latent space
and interpolates across a predictive vector between a known AMP and its scrambled version to
generate novel peptides. The PepCVAE (Das et al., 2018) and CLaSS (Das et al., 2021) employ
the variational auto-encoder model to generate sequences. The AMPGAN (Van Oort et al., 2021)
uses the generative adversarial network to generate new peptide sequences with conditions. To our
knowledge, this is the ﬁrst study to take secondary structure information into consideration during
the generative phase, which is conducive to effectively generate well-structured sequences with
desired properties ."
RELATED WORK,0.11885245901639344,"Sequence Generation via VQ-VAE The variational auto-encoders (VAEs) were ﬁrst proposed
by Kingma & Welling (2014) for image generation, and then widely applied to sequence genera-
tion tasks such as language modeling (Bowman et al., 2016), paraphrase generation (Gupta et al.,
2018), machine translation (Bao et al., 2019) and so on. Instead of mapping the input to a continuous
latent space in VAE, the vector quantized-variational autoencoder (VQ-VAE) (van den Oord et al.,
2017) learns the codebook to obtain a discrete latent representation. It can avoid issues of posterior
collapse while has comparable performance with VAEs. Based on it, Razavi et al. (2019) uses a
multi-scale hierarchical organization to capture global and local features for image generation. Bao
et al. (2021) learns implicit categorical information of target words with VQ-VAE and models the
categorical sequence with conditional random ﬁelds in non-autoregressive machine translation. In
this paper, we employ the multi-scale vector quantized technique to obtain the discrete representa-
tion for each position of the peptide."
METHOD,0.12295081967213115,"3
METHOD"
METHOD,0.12704918032786885,"Given a peptide sequence x = {a1, a2, · · · , aL}, where a belongs to the 20 common amino
acids and L is the sequence length, the corresponding secondary structure can be denoted as
y = {y1, y2, · · · , yL}. Following the deﬁnition in Kabsch & Sander (1983), there are 8 secondary
structure types, including one unknown label, so yi ∈{H, B, E, G, I, T, S, −}1. We ﬁrst employ
VQ-VAE for the sequence reconstruction task to learn the sequential latent space (Section 3.1).
Then, we enforce the latent space to model the structure information by the secondary structure task
(Section 3.2). Besides, we design the multi-scale codebooks to capture different local patterns (Sec-"
METHOD,0.13114754098360656,"1H, G, I denote the alpha, 3-10, and pi helix. E, T are the strand and turn. The others are coil structures."
METHOD,0.13524590163934427,Under review as a conference paper at ICLR 2022
METHOD,0.13934426229508196,"tion 3.3). Finally, we describe the training and inference phase in Section 3.4. The overview of our
model is shown in Figure 2."
MODELING PEPTIDE SEQUENCES,0.14344262295081966,"3.1
MODELING PEPTIDE SEQUENCES"
MODELING PEPTIDE SEQUENCES,0.14754098360655737,"For sequential information, we embed the input peptide x = {a1, a2, · · · , aL} to the latent space
via the encoder and use the generator to reconstruct x. We assume that each ai is determined
by a latent variable zi, and the input sequences x = a1:L will be assigned to a latent sequence
z = z1:L.
Since it is computationally intractable to sum continuous latent variables over the
sequence, we use VQ-VAE (van den Oord et al., 2017) to lookup the discrete embedding vector
zq = {zq(a1), · · · , zq(aL)} for each position by vector quantization."
MODELING PEPTIDE SEQUENCES,0.15163934426229508,"Speciﬁcally, the encoder output ze(ai) ∈Rd will be replaced by the codebook embedding zq(ai) ∈
Rd via a nearest neighbors lookup from the codebook B ∈RK×d :"
MODELING PEPTIDE SEQUENCES,0.1557377049180328,"zq(ai) = ek, and k = argminj∈{1,··· ,K} ∥ze(ai) −ej∥2 .
(1)"
MODELING PEPTIDE SEQUENCES,0.1598360655737705,"Here, K is the size of the codebook and d is the dimension of the codebook entry e. Then, the
generator will take zq(ai) as its input and reconstruct x. The training objective Lr is deﬁned as:"
MODELING PEPTIDE SEQUENCES,0.16393442622950818,"Lr = log p (ai|zq(ai)) + ∥sg [ze(ai)] −zq(ai)∥2
2 + β ∥ze(ai) −sg[zq(ai)]∥2
2 .
(2)"
MODELING PEPTIDE SEQUENCES,0.1680327868852459,"Here, sg(·) is the stop gradient operator, which becomes 0 at the backward pass. β is the commit
coefﬁcient to control the codebook loss."
MODELING SECONDARY STRUCTURES,0.1721311475409836,"3.2
MODELING SECONDARY STRUCTURES"
MODELING SECONDARY STRUCTURES,0.1762295081967213,"In order to model the categorical information of the secondary structure, we deﬁne an 8-category
sequence labeling task on the latent space, which takes x as the input and the structure label sequence
y as the target. Similar with sequence reconstruction, we use the same encoder to get ze(ai) and
employ VQ-VAE to obtain discrete representation. Then, z′
q(ai) is fed to a separate classiﬁer for the
secondary structure prediction:"
MODELING SECONDARY STRUCTURES,0.18032786885245902,"Ls = log p
 
yi|z′
q(ai)

+
sg [ze(ai)] −z′
q(ai)
2
2 + β
ze(ai) −sg[z′
q(ai)]
2
2 .
(3)"
MODELING SECONDARY STRUCTURES,0.18442622950819673,"Peptide sequences and structures have distinctive local features, which are often utilized in tradi-
tional design algorithms. The patterns of amino acids are often used for template-based design and
feature-based recognition. For structure motifs such as α-helix with at least 3.6 consecutive amino
acids, they will determine the position of amino acids in the 3D space and affect the function of
peptides. However, the structure motifs are often much longer than sequence patterns. Therefore,
we establish codebooks of multiple scales to learn latent spaces for different local patterns."
MULTI-SCALE VQ-VAE,0.1885245901639344,"3.3
MULTI-SCALE VQ-VAE"
MULTI-SCALE VQ-VAE,0.19262295081967212,"For the encoder output hi = z(0)
e (ai), we ﬁrst use N pattern selectors F (n) to extract local patterns
from different scales, which will get z(n)
e
(ai) = F (n)(hi). Then, we establish the codebook for each
pattern, and use Eqn. 1 to look up the nearest codebook embedding z(n)
q
(ai)."
MULTI-SCALE VQ-VAE,0.19672131147540983,"We share the codebooks between the sequence reconstruction and secondary structure prediction to
capture common features and relationships between the amino acid and its structure. Speciﬁcally,
we make Nr codebook for the sequence information and Ns for the structure. The zqr(ai) is the
concatenation of Nr codebook embeddings and is fed to the sequence generator:"
MULTI-SCALE VQ-VAE,0.20081967213114754,"zqr(ai) = ∥n∈Nrz(n)
q
(ai),
(4)"
MULTI-SCALE VQ-VAE,0.20491803278688525,"Based on Eqn. 2, the reconstruction training objective for multi-scale VQ-VAE can be denoted as:"
MULTI-SCALE VQ-VAE,0.20901639344262296,"Lr = log p (ai|zqr(ai)) +
X n∈Nr"
MULTI-SCALE VQ-VAE,0.21311475409836064,"sg
h
z(n)
e
(ai)
i
−z(n)
q
(ai)

2"
MULTI-SCALE VQ-VAE,0.21721311475409835,"2 + β
z(n)
e
(ai) −sg[z(n)
q
(ai)]

2 2 
. (5)"
MULTI-SCALE VQ-VAE,0.22131147540983606,Under review as a conference paper at ICLR 2022
MULTI-SCALE VQ-VAE,0.22540983606557377,"Algorithm 1 Training phase of LSSAMP
Require: A pre-training dataset Dr, a peptide dataset with secondary structure Ds, and the AMP dataset
Damp. The model Mθ.
1: Pre-train sequence reconstruction on Dr and update Mθ via Eqn. 5.
2: Finetune sequence reconstruction as well as secondary structure prediction on Ds and update the Mθ via
Eqn. 6.
3: Further Finetune Mθ on Damp.
4: for n = 1, 2, · · · , N do
5:
Create an empty dataset Cn.
6:
for xi ∈Damp do
7:
Save the n-th codebook index of xi via Eqn. 1 to Cn
8:
end for
9:
Train an auto-regressive language model Mpriorn on Cn.
10: end for"
MULTI-SCALE VQ-VAE,0.22950819672131148,"The loss of secondary structure prediction task Ls can be formulated in the same way from Eqn. 3.
And the total training loss is:"
MULTI-SCALE VQ-VAE,0.2336065573770492,"L = Lr + γLs,
(6)"
MULTI-SCALE VQ-VAE,0.23770491803278687,where the γ is the weight of the secondary structure prediction.
TRAINING AND INFERENCE,0.24180327868852458,"3.4
TRAINING AND INFERENCE"
TRAINING AND INFERENCE,0.2459016393442623,"We discuss the training and sampling process in this section. Since the AMP dataset Damp is very
small, we use two extra datasets Dr and Ds to pre-train the reconstruction and prediction task. The
whole training process is described in Algorithm 1."
TRAINING AND INFERENCE,0.25,"Training To enable the generator to generate valid peptide sequences, we ﬁrst pre-train our model
on a large protein dataset Dr for the sequence construction task. Then, we jointly train the se-
quence reconstruction and the structure prediction on a peptide dataset Ds with secondary structure
information. This phase maps the positional latent space to an entangled sequential and structural
distribution. Finally, we ﬁnetune our model on the small AMP dataset Damp to capture the charac-
teristics of AMPs."
TRAINING AND INFERENCE,0.2540983606557377,"Following Kaiser et al. (2018), we use Exponential Moving Average (EMA) to update the embedding
vectors in the codebooks. Speciﬁcally, we keep a count ck measuring the number of times that the
embedding vector ek is chosen as the nearest neighbor of ze(ai) via Eqn. 1. Thus, the counts are
updated with a sort of momentum: ck ←λck + (1 −λ) P"
TRAINING AND INFERENCE,0.2581967213114754,"i I[zq(ai) = ek], with the embedding ek
being updated as: ek ←λek + (1 −λ) P
i
I[zq(ai)=ek]ze(ai)"
TRAINING AND INFERENCE,0.26229508196721313,"ck
. Here, λ is the decay parameter."
TRAINING AND INFERENCE,0.26639344262295084,"Prior Model The prior distribution over the codebook is a categorical distribution and can be made
auto-regressive by the extra prior model. In order to model the dependency between z1:L, we train
Transformer-based language models on the embedding entries. We extract the index sequences
generated by Eqn. 1 for each codebook n and then train Mpriorn on them."
TRAINING AND INFERENCE,0.27049180327868855,"Inference We sample several index sequences from the prior models for each codebook n, and then
lookup the codebook to get the embedding vector z(n)
q
. Finally, z(n)
q
is fed to the generator and
classiﬁer to generate the sequence with its secondary structure. We also try to control the secondary
structure by existing AMP structure patterns to further improve the generation quality."
EXPERIMENT,0.27459016393442626,"4
EXPERIMENT"
EXPERIMENT SETUP,0.2786885245901639,"4.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.2827868852459016,"Dataset The Universal Protein Resource (UniProt)2 is a comprehensive protein dataset. We down-
load reviewed protein sequences (550k) with the limitation of 100 in length as Dr (57k examples).
Then we use ProSPr3 (Billings et al., 2019) to predict the secondary structure for Dr and ﬁlter some"
EXPERIMENT SETUP,0.28688524590163933,"2https://www.uniprot.org/
3https://github.com/dellacortelab/prospr/tree/prospr1"
EXPERIMENT SETUP,0.29098360655737704,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP,0.29508196721311475,"low-quality examples. Therefore, we get Ds with 46k examples, which have both sequence and sec-
ondary structure information. For antimicrobial peptide dataset, we download from Antimicrobial
Peptide Database (APD3)4 (Wang et al., 2016) and ﬁlter repeated ones to get 3222 AMPs as Damp.
We randomly extract 3,000 examples as validation and 3,000 as test on Dr and Ds. For Damp, the
size of validation and test is both 100. Following Veltri et al. (2018), we establish a decoy set with
negative examples that do not have antimicrobial activities to examine the evaluation metric. It ﬁrst
removes peptide sequences with anti-microbe activity from Uniprot, and then removes sequences
with length < 10 and sequences with > 40 sequence identity with AMP sequences, resulting in
2021 non-AMP sequences."
EXPERIMENT SETUP,0.29918032786885246,"Baseline We implement several baselines for our experiment. Traditional methods usually randomly
substitute several amino acids on the existing AMPs and conducting biological experiments on them.
Thus, we use the Random replacement with probability p as the baseline. Following Dean & Walper
(2020), we use VAE to embed the peptides into the latent space and sample latent variable z from the
standard Gaussian distribution p ∼N(0, 1). For a fair comparison, we use the same Transformer
architecture as our model LSSAMP and train on the Uniprot Dr and APD dataset Damp. AMP-
GAN is proposed by Van Oort et al. (2021), which uses a BiCGAN architecture with convolution
layers. It consists of three components: the generator, discriminator, and encoder. The generator and
discriminator share the same encoder. It is trained on 49k false negative sequences from UniProt
and 7k positive AMP sequences. PepCVAE (Das et al., 2018) is a VAE-based generative model
with a structured variable c to control the attribute of the sequence. Since the authors did not release
their code, we use the model architecture from Hu et al. (2017) and modify the reproduced code5 for
AMPs. The original paper use 93k sequences from UniProt and 7960/6948 positive/negative AMPs
for training. For comparison, we use our UniProt dataset Dr and ADP dataset Damp to train it.
MLPeptide (Capecchi et al., 2021) is RNN-based generator. It is ﬁrst trained on 3580 AMPs and
then transfer against speciﬁc bacteria."
EXPERIMENT SETUP,0.30327868852459017,"Implementation Details There are three main modules for LSSAMP. The encoder and decoder are
based on 2-layer Transformer (Vaswani et al., 2017) with dmodel = 128 and head = 8. The size
of FFN projection is dffn = 512 and all drouput rate are 0.1. For the classiﬁer, we use the same
CNN block as Billings et al. (2019) with 32 input channels and a dilation scale of [1, 2, 4, 8, 10]. For
multi-scale codebooks, we ﬁrst apply CNN as F (n) to extract features. We set n = 4 and kernel
width ranging in [1, 2, 4, 8]. The features will be padded to the same length as the input sequence.
Then, we use 4 codebooks with K = 8 and d = 128. The reconstruction and prediction share the
same codebooks, which means Nr = Ns = 4. The commit coefﬁcient is set to β = 0.05."
EXPERIMENT SETUP,0.3073770491803279,"We use PyTorch for implementation and train on the single Tesla-V100-32GB. We optimize the pa-
rameter with Adam Optimizer (Kingma & Ba, 2015). During pre-training for sequence construction
on Dr, we limit the max length to 100 and set the maximum tokens in a batch bz as 30,000, learning
rate lr as 0.01 with 8,000 warmup steps, and decoy weight for EMA as λ = 0.8. For secondary
structure prediction on Ds, the max length is limited to 32, bz = 10, 000, lr = 0.003, λ = 0.95, and
the prediction loss coefﬁcient γ = 1. Finally, we transfer to Damp with the same hyperparameters
except the lr = 0.001."
EVALUATION METRIC,0.3114754098360656,"4.2
EVALUATION METRIC"
EVALUATION METRIC,0.3155737704918033,"To evaluate the antimicrobial activity of the generated peptide S = {a1, a2, · · · , aL}, we use several
physical features according to the mechanism of AMPs. Besides, we also apply some open-source
AMP predictive models to estimate the probability of the generated peptides being AMPs."
PHYSICAL ATTRIBUTES,0.319672131147541,"4.2.1
PHYSICAL ATTRIBUTES"
PHYSICAL ATTRIBUTES,0.3237704918032787,"Charge The bacterial membrane usually takes the negative charge, so peptides with the positive
charge prefer to bind with the bacteria. The whole net charge of the peptide sequence S is deﬁned
as the sum of the charge of all its amino acids C(ai) at pH 7.4, which is C(S) = P"
PHYSICAL ATTRIBUTES,0.32786885245901637,"ai∈S C(ai)
Hydrophobicity The hydrophobicity reﬂects the tendency to bind lipids on the bacterial membrane.
It is more likely for the peptide with a high hydrophobicity to move from the solution environment"
PHYSICAL ATTRIBUTES,0.3319672131147541,"4https://aps.unmc.edu/
5https://github.com/wiseodd/controlled-text-generation"
PHYSICAL ATTRIBUTES,0.3360655737704918,Under review as a conference paper at ICLR 2022
PHYSICAL ATTRIBUTES,0.3401639344262295,"to the bacterial membrane. We use the hydrophobicity scale H(ai) in Eisenberg et al. (1984) to
calculate the hydrophobicity of a sequence, which is H(S) = P"
PHYSICAL ATTRIBUTES,0.3442622950819672,ai∈S H(ai)
PHYSICAL ATTRIBUTES,0.3483606557377049,"Amphipathicity / Hydrophobic Momentum The amphipathicity measures the ability of the pep-
tide to bind water and lipid at the same time, which is a deﬁnitive feature of antimicrobial pep-
tides(Hancock & Rozek, 2002). It can be quantiﬁed by the hydrophobic momentum uH(S, θ),
deﬁned by Eisenberg et al. (1984). The hydrophobic momentum is determined by the hydrophobic-
ity H(ai) of each amino acid ai, along with the angle θ between amino acids. The angle can be
estimated by the secondary structure. For the α-helix structure, θ is 100◦and for β-sheet, θ is 180◦."
PHYSICAL ATTRIBUTES,0.3524590163934426,"uH(S, θ) =
s (
X"
PHYSICAL ATTRIBUTES,0.35655737704918034,"ai∈S
H(ai) ∗cos(i ∗θ))2 + (
X"
PHYSICAL ATTRIBUTES,0.36065573770491804,"ai∈S
H(ai)sin(i ∗θ))2
(7)"
PHYSICAL ATTRIBUTES,0.36475409836065575,"For each peptide, we calculate the above properties to measure its antimicrobial activity. For com-
parison, we draw the distribution on APD and decoy dataset and choose a range for each property6.
We use the percentage of peptides in the range for each attribute to leverage the generation perfor-
mance and use Combination to measure the percentage of those that satisfy three conditions at the
same time. The detailed information is attached in the appendix."
PHYSICAL ATTRIBUTES,0.36885245901639346,"Uniq
C
H
uH
Combination"
PHYSICAL ATTRIBUTES,0.3729508196721312,"APD
3222
68.75%
27.96%
4.72%
6.15%
Decoy
2020
21.83%
8.81%
1.98%
0.10%"
PHYSICAL ATTRIBUTES,0.3770491803278688,"Random p = 0.1
4978 66.16% ± 0.21% 26.92% ± 0.24% 23.12% ± 0.58% 4.40% ± 0.16%
Random p = 0.2
5000 61.70% ± 0.39% 24.87% ± 0.29% 20.79% ± 0.76% 2.47% ± 0.17%
VAE
4988 38.54% ± 0.36% 21.37% ± 0.58% 12.60% ± 0.67% 0.34% ± 0.11%
AMP-GAN
4976 88.07% ± 0.42% 17.39% ± 0.75% 23.55% ± 0.72% 1.93% ± 0.05%
PepCVAE
1346 58.89% ± 1.05% 14.54% ± 0.55% 11.65% ± 0.23% 2.75% ± 0.25%
MLPeptide
4486 86.59% ± 0.55%
9.01% ± 0.33%
36.56% ± 0.62% 3.22% ± 0.19%"
PHYSICAL ATTRIBUTES,0.38114754098360654,"LSSAMP
4886 75.06% ± 0.37% 39.51% ± 0.64% 27.18% ± 0.41% 9.96% ± 0.07%
LSSAMP w/o condition 4893 60.88% ± 0.44% 38.75% ± 0.26% 24.05% ± 0.71% 6.42% ± 0.29%"
PHYSICAL ATTRIBUTES,0.38524590163934425,"Table 1: Physical attributes of generated sequences. We use the percentage of peptides meeting the
range to measure the performance. Uniq is the number of unique generated sequences. C, H, uH
correspond to charge, hydrophobicity, hydrophobic moment described in Section 4.2.1. Combina-
tion is the percentage satisfying three ranges at the same time. The best results are bold."
AMP CLASSIFIERS,0.38934426229508196,"4.2.2
AMP CLASSIFIERS"
AMP CLASSIFIERS,0.39344262295081966,"Following previous works (Das et al., 2020; Van Oort et al., 2021), we also use several open-source
AMP prediction tools to predict the AMP probability of generated sequence. Since these prediction
tools are trained and report results in different AMP datasets, we ﬁrst use them to predict sequences
in APD and decoy dataset as a reference of their performance."
AMP CLASSIFIERS,0.3975409836065574,"Thomas et al. (2010) trained on the AMP database of 3782 sequences with random forest (RF),
discriminant analysis (DA), support vector machines (SVM)7, and artiﬁcial neural network (ANN)8
respectively. AMP Scanner v29 (Veltri et al., 2018), short as Scanner, is a CNN-&LSTM-based
deep neural network trained on 1778 AMPs picked from APD. AMPMIC10 (Witten & Witten, 2019)
trained a CNN-based regression model on 6760 unique sequences and 51345 MIC measurement to
predict MIC values. IAMPE11 (Kavousi et al., 2020) is a model based on Xtreme Gradient Boosting.
It achieves the highest correct prediction rate on a a set of ten more recent AMPs (Aronica et al.,
2021). ampPEP12 (Lawrence et al., 2021) is a random forest based model which is trained on 3268
AMP sequences. It has the best performance across multiple data sets (Aronica et al., 2021)."
AMP CLASSIFIERS,0.4016393442622951,"6The requirements are C ∈[2, 10], H ∈[0.25, ∞], and uH ∈[0.5, 0.75] ∪[1.75, ∞].
7http://www.camp3.bicnirrh.res.in/prediction.php
8We drop the ANN model becasue its accuracy on APD is low (82.83%).
9https://www.dveltri.com/ascan/v2/ascan.html
10https://github.com/zswitten/Antimicrobial-Peptides
11http://cbb1.ut.ac.ir/AMPClassiﬁer/Index
12https://github.com/tlawrence3/amPEPpy"
AMP CLASSIFIERS,0.4057377049180328,Under review as a conference paper at ICLR 2022
RESULTS AND ANALYSIS,0.4098360655737705,"4.3
RESULTS AND ANALYSIS"
RESULTS AND ANALYSIS,0.4139344262295082,"We generate 5000 sequences for each baseline and ﬁlter the repeated ones. During the generation,
we add some structural restrictions based on the antimicrobial mechanism on the secondary structure
to improve the performance. Speciﬁcally, we reject peptides with more than 30% coil structure (‘-’),
which can hardly fold in the solution environment and insert into the bacterial membrane. Besides,
we limit the minimum length of a helix structure (‘H’) to 4 based on physical rules. We name our
model with structural control as LSSAMP and the model without extra conditions as LSSAMP w/o
condition. We discuss the structure conditions and give some generated peptides in the appendix."
RESULTS AND ANALYSIS,0.4180327868852459,"SVM
RF
DA
Scanner
AMPMIC
IAMPE
amPEP
Average"
RESULTS AND ANALYSIS,0.42213114754098363,"APD
87.78%
91.24%
86.24%
94.66%
98.42%
97.83%
91.50%
92.52%
Decoy
17.43%
13.71%
16.04%
0.25%
18.07%
23.53%
52.92%
20.28%"
RESULTS AND ANALYSIS,0.4262295081967213,"Random p = 0.1
86.06%
86.12%
84.01%
93.23%
79.14%
95.60%
91.74%
87.99%
Random p = 0.2
76.66%
76.64%
74.83%
86.95%
68.57%
91.14%
87.89%
80.38%
VAE
24.90%
15.30%
13.83%
15.12%
15.25%
40.31%
24.30%
21.29%
AMP-GAN
78.62%
87.29%
83.82%
82.17%
89.58%
93.88%
80.52%
85.13%
PepCVAE
82.84%
85.96%
93.33%
85.44%
98.44%
98.14%
80.77%
89.27%
MLPeptide
90.43%
92.55%
93.08%
93.72%
96.34%
97.05%
91.37%
93.51%"
RESULTS AND ANALYSIS,0.430327868852459,"LSSAMP
91.78%
89.86%
91.09%
90.51%
93.40%
90.42%
93.23%
91.47%
LSSAMP w/o condition
77.98%
78.31%
78.00%
85.72%
76.71%
90.05%
84.32%
81.58%"
RESULTS AND ANALYSIS,0.4344262295081967,"Table 2: The percentage of generated sequences being predicted as AMP. The classiﬁers are de-
scribed in Section 4.2.2. The ﬁrst part is the prediction results on AMP and non-AMP dataset as the
reference. The bold ones are the best model results."
RESULTS AND ANALYSIS,0.4385245901639344,"Physical Attributes As listed in Table 1, LSSAMP outperforms on the combination percentage by
a large margin (5.56%), which indicates that our model can generate sequences satisfying multiple
properties at the same time. If we do not control the structure, the combination percentage is similar
to APD, which implies our model learns the latent distribution of APD. For the speciﬁc attribute,
LSSAMP tends to generate peptides with higher hydrophobicity, while AMP-GAN and MLPeptide
have more positive sequences. Compared with other models, PepCVAE tends to generate redundant
sequences, which results in a signiﬁcant decline in the number of unique sequences. Also, we can
ﬁnd that by control the secondary structure, all metrics can be improved. This veriﬁes that secondary
structure information has a great inﬂuence on peptide properties and it is beneﬁcial to take it into
consideration during the peptide generation."
RESULTS AND ANALYSIS,0.4426229508196721,"AMP Prediction The results of prediction tools are shown in Table 2.
By the comparison of
LSSAMP and LSSAMP w/o condition, we can draw the same conclusion as above that control-
ling the secondary structure can further improve the generation performance. For AMPMIC and
IAMPE, PepCVAE has a signiﬁcant advantage over other methods, but it has a poor performance
in other classiﬁers. LSSAMP performs better than MLPeptide in SVM and amPEP but does not
perform as well as it in other classiﬁers. On the classiﬁer with the largest performance gap IAMPE,
we can ﬁnd that the results of LSSAMP and LSSAMP w/o condition are similar, which implies that
different secondary structures do not affect the results of IAMPE. However, in real scenarios, the
structure is very important for the antimicrobial mechanism, which makes the classiﬁer less reliable."
RESULTS AND ANALYSIS,0.44672131147540983,PPL ↓Loss ↓AA Acc.↑SS Acc.↑
RESULTS AND ANALYSIS,0.45081967213114754,"LSSAMP
3.12
1.14
99.93
86.76
w/o Dr
11.56
2.45
66.06
82.78
w/o Ds
3.83
1.34
99.58
85.87
w/o subbook
3.49
1.25
99.86
86.61"
RESULTS AND ANALYSIS,0.45491803278688525,"Table 3: Ablation Study on validation set of
Damp. ‘w/o’ means that we remove the mod-
ule from LSSAMP. ↓means lower is better,
while ↑means higher is better. The detailed
descriptions are in Section 4.3."
RESULTS AND ANALYSIS,0.45901639344262296,Codebook PPL ↓Loss ↓AA Acc.↑SS Acc.↑
RESULTS AND ANALYSIS,0.46311475409836067,"[1]
19.04
2.94
65.49
83.41
[1, 2]
3.84
1.35
99.40
85.39
[1, 2, 4]
3.32
1.20
100.00
85.95
[1, 2, 4, 8]
3.24
1.17
99.79
87.20"
RESULTS AND ANALYSIS,0.4672131147540984,"Table 4: The inﬂuence of the number of code-
books. ‘[1,2,4,8]’ indicates that we use 4 code-
books to capture local features with window
sizes of 1,2,4,8. The meanings of symbols are
the same as Table 3."
RESULTS AND ANALYSIS,0.4713114754098361,Under review as a conference paper at ICLR 2022
RESULTS AND ANALYSIS,0.47540983606557374,A C D E F G H I K L M N P Q R S T V W Y
RESULTS AND ANALYSIS,0.47950819672131145,Amino Acids 0.00 0.02 0.04 0.06 0.08 0.10 0.12
RESULTS AND ANALYSIS,0.48360655737704916,Fraction
RESULTS AND ANALYSIS,0.48770491803278687,"APD
LSSAMP_wo_cond
LSSAMP
Decoy"
RESULTS AND ANALYSIS,0.4918032786885246,"5
0
5
10
15
Global Charge 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
RESULTS AND ANALYSIS,0.4959016393442623,Fraction
RESULTS AND ANALYSIS,0.5,"APD
LSSAMP_wo_cond
LSSAMP
Decoy APD"
RESULTS AND ANALYSIS,0.5040983606557377,LSSAMP_wo_cond
RESULTS AND ANALYSIS,0.5081967213114754,LSSAMP Decoy 5 10 15 20 25 30
RESULTS AND ANALYSIS,0.5122950819672131,Sequence Length APD
RESULTS AND ANALYSIS,0.5163934426229508,LSSAMP_wo_cond
RESULTS AND ANALYSIS,0.5204918032786885,LSSAMP Decoy 1.0 0.5 0.0 0.5 1.0
RESULTS AND ANALYSIS,0.5245901639344263,Global Hydrophobicity APD
RESULTS AND ANALYSIS,0.5286885245901639,LSSAMP_wo_cond
RESULTS AND ANALYSIS,0.5327868852459017,LSSAMP Decoy 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
RESULTS AND ANALYSIS,0.5368852459016393,Global Hydrophobic Moment
RESULTS AND ANALYSIS,0.5409836065573771,Charge
RESULTS AND ANALYSIS,0.5450819672131147,"10
5
0
5
10
15 H 1.0 0.5 0.0 0.5 1.0 uH 0 1 2 3"
RESULTS AND ANALYSIS,0.5491803278688525,"APD
LSSAMP_wo_cond
LSSAMP
Decoy"
RESULTS AND ANALYSIS,0.5532786885245902,"Figure 3: The distribution of amino acids, charge, sequence length, hydrophobicity, hydrophobic
momentum, and a 3D visualization for three physical attributes."
RESULTS AND ANALYSIS,0.5573770491803278,"Ablation Study We conduct the ablation study for our LSSAMP and show the results in Table 3.
PPL is the perplexity of generated sequences that can measure ﬂuency. Loss is the model loss
on the validation set. AA Acc. is the reconstruction accuracy of amino acids sequences and SS
Acc. is the prediction accuracy of the secondary structure. We can ﬁnd that without the ﬁrst pre-
training phase on Dr for sequence reconstruction, the model can hardly generate valid sequences.
The second phase to train the model on the large-scale secondary structure dataset Ds will affect the
prediction performance on the target AMP dataset. If we remove multiple sub-codebooks and use a
single codebook with the same size, the performance will decline a little."
RESULTS AND ANALYSIS,0.5614754098360656,"Codebook Number We explore the effect of different numbers of codebooks on generation perfor-
mance. From Table 4, we ﬁnd that a single small codebook can hardly learn enough information to
reconstruct the sequences. The PPL, loss, and secondary structure accuracy become better with the
increase of codebook items. However, the reconstruction accuracy achieves the best performance
when the codebook number is 3. This may be due to the relatively short local patterns for the amino
acid sequence, which makes the window of size 8 too long for it."
RESULTS AND ANALYSIS,0.5655737704918032,"Visualization We plot the distribution of amino acids, charge, sequence length, hydrophobicity,
and hydrophobic momentum for APD, Decoy, and our models in Figure 3. Without condition, the
distribution of LSSAMP is similar to APD, which indicates that LSSAMP successfully learns the
latent space of APD. However, if we control the secondary structure, it is more likely to generate
sequences with a longer length and more positive charges. For hydrophobicity and hydrophobic
momentum, the distribution of generated sequences is more concentrated."
CONCLUSION,0.569672131147541,"5
CONCLUSION"
CONCLUSION,0.5737704918032787,"In this paper, we propose a multi-scale VQ-VAE to generate peptides. Motivated by the important
role of structure in the antimicrobial mechanism, LSSAMP learns the latent spaces for each position
with both sequential and structural features. Our model shows excellent performance on physical
attributes related to antimicrobial activities and has a high probability to be predicted as AMPs by
public classiﬁers. As the structural feature determines the function of macromolecules, LSSAMP
has great potential for macromolecules design, including proteins and RNAs."
CONCLUSION,0.5778688524590164,Under review as a conference paper at ICLR 2022
REFERENCES,0.5819672131147541,REFERENCES
REFERENCES,0.5860655737704918,"Pietro GA Aronica, Lauren M Reid, Nirali Desai, Jianguo Li, Stephen J Fox, Shilpa Yadahalli,
Jonathan W Essex, and Chandra S Verma. Computational methods and tools in antimicrobial
peptide research. Journal of Chemical Information and Modeling, 61(7):3172–3196, 2021."
REFERENCES,0.5901639344262295,"Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, and Jiajun
Chen. Generating sentences from disentangled syntactic and semantic spaces. In Proc. of ACL,
July 2019."
REFERENCES,0.5942622950819673,"Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, and Jiajun Chen. Non-autoregressive
translation by learning target categorical codes. In Proc. of NAACL-HLT, pp. 5749–5759, 2021."
REFERENCES,0.5983606557377049,"Wendy M Billings, Bryce Hedelius, Todd Millecam, David Wingate, and Dennis Della Corte.
Prospr: democratized implementation of alphafold protein distance prediction network. bioRxiv,
pp. 830273, 2019."
REFERENCES,0.6024590163934426,"Dan S Bolintineanu and Yiannis N Kaznessis. Computational studies of protegrin antimicrobial
peptides: a review. Peptides, 32(1):188–201, 2011."
REFERENCES,0.6065573770491803,"HG Boman. Antibacterial peptides: basic facts and emerging concepts. Journal of internal medicine,
254(3):197–215, 2003."
REFERENCES,0.610655737704918,"Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. In 20th SIGNLL Conference on Compu-
tational Natural Language Learning, CoNLL 2016, pp. 10–21. Association for Computational
Linguistics (ACL), 2016."
REFERENCES,0.6147540983606558,"Alice Capecchi, Xingguang Cai, Hippolyte Personne, Thilo Kohler, Christian van Delden, and Jean-
Louis Reymond.
Machine learning designs non-hemolytic antimicrobial peptides.
Chemical
Science, 2021."
REFERENCES,0.6188524590163934,"Marlon H Cardoso, Raquel Q Orozco, Samilla B Rezende, Gisele Rodrigues, Karen GN Oshiro,
Elizabete S Cˆandido, and Oct´avio L Franco. Computer-aided design of antimicrobial peptides:
are we generating effective drug candidates? Frontiers in microbiology, 10:3097, 2020."
REFERENCES,0.6229508196721312,"Payel Das, Kahini Wadhawan, Oscar Chang, Tom Sercu, Cicero Dos Santos, Matthew Riemer, Vijil
Chenthamarakshan, Inkit Padhi, and Aleksandra Mojsilovic. Pepcvae: Semi-supervised targeted
design of antimicrobial peptide sequences. arXiv preprint arXiv:1810.07743, 2018."
REFERENCES,0.6270491803278688,"Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan, Vijil
Chenthamarakshan, Hendrik Strobelt, Cicero dos Santos, Pin-Yu Chen, et al. Accelerating an-
timicrobial discovery with controllable deep generative models and molecular dynamics. arXiv
preprint arXiv:2005.11248, 2020."
REFERENCES,0.6311475409836066,"Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan, Vijil
Chenthamarakshan, Hendrik Strobelt, Cicero Dos Santos, Pin-Yu Chen, et al. Accelerated an-
timicrobial discovery via deep generative models and molecular dynamics simulations. Nature
Biomedical Engineering, 5(6):613–623, 2021."
REFERENCES,0.6352459016393442,"Scott N Dean and Scott A Walper. Variational autoencoder for generation of antimicrobial peptides.
ACS omega, 5(33):20746–20754, 2020."
REFERENCES,0.639344262295082,"David Eisenberg, Robert M Weiss, and Thomas C Terwilliger. The hydrophobic moment detects
periodicity in protein hydrophobicity. Proceedings of the National Academy of Sciences, 81(1):
140–144, 1984."
REFERENCES,0.6434426229508197,"Ankush Gupta, Arvind Agarwal, Prawaan Singh, and Piyush Rai. A deep generative framework for
paraphrase generation. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proc. of AAAI, pp.
5149–5156, 2018."
REFERENCES,0.6475409836065574,"Robert EW Hancock and Annett Rozek.
Role of membranes in the activities of antimicrobial
cationic peptides. FEMS microbiology letters, 206(2):143–149, 2002."
REFERENCES,0.6516393442622951,Under review as a conference paper at ICLR 2022
REFERENCES,0.6557377049180327,"Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled
generation of text. In International Conference on Machine Learning, pp. 1587–1596. PMLR,
2017."
REFERENCES,0.6598360655737705,"Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-
graph translation for molecule optimization. In International Conference on Learning Represen-
tations, 2018."
REFERENCES,0.6639344262295082,"Wolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern recog-
nition of hydrogen-bonded and geometrical features.
Biopolymers:
Original Research on
Biomolecules, 22(12):2577–2637, 1983."
REFERENCES,0.6680327868852459,"Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. Fast decoding in sequence models using discrete latent variables. In International Con-
ference on Machine Learning, pp. 2390–2399. PMLR, 2018."
REFERENCES,0.6721311475409836,"Kaveh Kavousi, Mojtaba Bagheri, Saman Behrouzi, Safar Vafadar, Fereshteh Fallah Atanaki, Ba-
hareh Teimouri Lotfabadi, Shohreh Ariaeenejad, Abbas Shockravi, and Ali Akbar Moosavi-
Movahedi. Iampe: Nmr-assisted computational prediction of antimicrobial peptides. Journal
of Chemical Information and Modeling, 60(10):4691–4701, 2020."
REFERENCES,0.6762295081967213,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), Proc. of ICLR, 2015."
REFERENCES,0.680327868852459,Diederik P Kingma and Max Welling. Auto-encoding variational bayes. 2014.
REFERENCES,0.6844262295081968,"Travis J Lawrence, Dana L Carper, Margaret K Spangler, Alyssa A Carrell, Tom´as A Rush, Stephen J
Minter, David J Weston, and Jessy L Labb´e. ampeppy 1.0: a portable and accurate antimicrobial
peptide prediction tool. Bioinformatics, 37(14):2058–2060, 2021."
REFERENCES,0.6885245901639344,"Christopher Loose, Kyle Jensen, Isidore Rigoutsos, and Gregory Stephanopoulos.
A linguistic
model for the rational design of antimicrobial peptides. Nature, 443(7113):867–869, 2006."
REFERENCES,0.6926229508196722,"Giuseppe Maccari, Mariagrazia Di Luca, Riccardo Nifos´ı, Francesco Cardarelli, Giovanni Signore,
Claudia Boccardi, and Angelo Bifone. Antimicrobial peptides design by evolutionary multiob-
jective optimization. PLoS computational biology, 9(9):e1003212, 2013."
REFERENCES,0.6967213114754098,"Margit Mahlapuu, Joakim H˚akansson, Lovisa Ringstad, and Camilla Bj¨orn. Antimicrobial peptides:
an emerging category of therapeutic agents. Frontiers in cellular and infection microbiology, 6:
194, 2016."
REFERENCES,0.7008196721311475,"Edit Matyus, Christian Kandt, and D Peter Tieleman. Computer simulation of antimicrobial pep-
tides. Current medicinal chemistry, 14(26):2789–2798, 2007."
REFERENCES,0.7049180327868853,"William F Porto, Isabel CM Fensterseifer, Suzana M Ribeiro, and Octavio L Franco. Joker: An
algorithm to insert patterns into sequences for designing antimicrobial peptides. Biochimica et
Biophysica Acta (BBA)-General Subjects, 1862(9):2043–2052, 2018."
REFERENCES,0.7090163934426229,"Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with
vq-vae-2. In Advances in neural information processing systems, pp. 14866–14876, 2019."
REFERENCES,0.7131147540983607,"Schr¨odinger, LLC.
The AxPyMOL molecular graphics plugin for Microsoft PowerPoint, ver-
sion 1.8. November 2015a."
REFERENCES,0.7172131147540983,"Schr¨odinger, LLC. The JyMOL molecular graphics development component, version 1.8. November
2015b."
REFERENCES,0.7213114754098361,"Schr¨odinger, LLC. The PyMOL molecular graphics system, version 1.8. November 2015c."
REFERENCES,0.7254098360655737,"Daniel Schwalbe-Koda and Rafael G´omez-Bombarelli. Generative models for automatic chemical
design. In Machine Learning Meets Quantum Physics, pp. 445–467. Springer, 2020."
REFERENCES,0.7295081967213115,"Yimin Shen, Julien Maupetit, Philippe Derreumaux, and Pierre Tuffery. Improved pep-fold approach
for peptide and miniprotein structure prediction. Journal of chemical theory and computation, 10
(10):4745–4758, 2014."
REFERENCES,0.7336065573770492,Under review as a conference paper at ICLR 2022
REFERENCES,0.7377049180327869,"Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
ﬂow-based autoregressive model for molecular graph generation. In International Conference on
Learning Representations, 2019."
REFERENCES,0.7418032786885246,"Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al.
A deep learning approach to antibiotic discovery. Cell, 180(4):688–702, 2020."
REFERENCES,0.7459016393442623,"Shaini Thomas, Shreyas Karnik, Ram Shankar Barai, Vaidyanathan K Jayaraman, and Susan
Idicula-Thomas. Camp: a useful resource for research on antimicrobial peptides. Nucleic acids
research, 38(suppl 1):D774–D780, 2010."
REFERENCES,0.75,"Marcelo Der Torossian Torres and Cesar de la Fuente-Nunez. Toward computer-made artiﬁcial
antibiotics. Current opinion in microbiology, 51:30–38, 2019."
REFERENCES,0.7540983606557377,"Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In Proceedings of the 31st International Conference on Neural Information Processing Sys-
tems, pp. 6309–6318, 2017."
REFERENCES,0.7581967213114754,"Colin M Van Oort, Jonathon B Ferrell, Jacob M Remington, Safwan Wshah, and Jianing Li. Ampgan
v2: Machine learning-guided design of antimicrobial peptides. Journal of Chemical Information
and Modeling, 2021."
REFERENCES,0.7622950819672131,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Proc. of NeurIPS, pp. 5998–6008, 2017."
REFERENCES,0.7663934426229508,"Daniel Veltri, Uday Kamath, and Amarda Shehu. Deep learning improves antimicrobial peptide
recognition. Bioinformatics, 34(16):2740–2747, 2018."
REFERENCES,0.7704918032786885,"Guangshun Wang, Xia Li, and Zhe Wang. Apd3: the antimicrobial peptide database as a tool for
research and education. Nucleic acids research, 44(D1):D1087–D1093, 2016."
REFERENCES,0.7745901639344263,"Jacob Witten and Zack Witten. Deep learning regression model for antimicrobial peptide design.
BioRxiv, pp. 692681, 2019."
REFERENCES,0.7786885245901639,"Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars:
Markov molecular sampling for multi-objective drug discovery. In International Conference on
Learning Representations, 2020."
REFERENCES,0.7827868852459017,Under review as a conference paper at ICLR 2022
REFERENCES,0.7868852459016393,"A
APPENDIX"
REFERENCES,0.7909836065573771,"A.1
ATTRIBUTE DISTRIBUTION"
REFERENCES,0.7950819672131147,"To determine the threshold of net charge, hydrophobicity, and hydrophobic moment, we analyze the
distribution of sequences in APD and decoy in Figure 4. For charge, we follow the rule summarized
by specialists and choose sequences whose net charge is +2 to +10. For two characters left, we plot
the histogram and compare the proportion in every bin. If the proportion of APD is larger than that
in decoy, we add the bin into the acceptation range of the evaluation metric. The ﬁnal ranges are
C ∈[2, 10], H ∈[0.25, ∞], and uH ∈[0.5, 0.75] ∪[1.75, ∞]."
REFERENCES,0.7991803278688525,"5
0
5
10
15
Global Charge 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200"
REFERENCES,0.8032786885245902,Fraction
REFERENCES,0.8073770491803278,"APD
decoy"
REFERENCES,0.8114754098360656,"1.0
0.5
0.0
0.5
1.0
Global Hydrophobic 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14"
REFERENCES,0.8155737704918032,"APD
decoy"
REFERENCES,0.819672131147541,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Global Hydrophobic Moment 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175"
"APD
DECOY",0.8237704918032787,"0.200
APD
decoy"
"APD
DECOY",0.8278688524590164,"Figure 4: The histogram of charge, hydrophobicity and hydrophobic moment on APD and decoy
dataset."
"APD
DECOY",0.8319672131147541,"A.2
GENERATED AMPS"
"APD
DECOY",0.8360655737704918,"We show 10 peptides generated by LSSAMP in Table 5, and build 3D models for four of them by
PEPFold 3 (Shen et al., 2014) and draw the picture by PyMOL (Schr¨odinger, LLC, 2015c;a;b) in
Figure 5. We can ﬁnd that all these peptides have long helix structures, which make them more likely
to have the antimicrobial ability. At the same time, although the model predicts a long continuous
helix structure for peptide ID = 4 and ID = 9, in fact, they have a small coil structure between two
helix structures. It indicates that our model tends to predict a long continuous secondary structure
instead of several discontinuous small fragments."
"APD
DECOY",0.8401639344262295,"ID
Sequence
Secondary Structure
C
H
uH"
FLPLVRVWAKLI,0.8442622950819673,"1
FLPLVRVWAKLI
–HHHHHHHHHH
2.0
0.471
0.723
2
FLSTVPYVAFKVVPTLFCPIAKTC
–HHHHHHHHHHHHHHHHHHHT–
2.0
0.446
1.812
3
FFGVLARGIKSVVKHVMGLLMG
–HHHHHHHHHHHHHHHHHH–
3.0
0.420
0.549
4
GVLPAFKQYLPGIMKIIVKF
–HHHHHHHHHHHHHHH—
3.0
0.419
0.523
5
VFTLLGAIIHHLGNFVKRFSHVF
-HHHHHHHHHHHHHHHHHHHH–
2.0
0.416
0.514
6
FVPGLIKAAVGIGYTIFCKISKACYQ
–HHHHHHHHHHHHHHHHHHHT—-
3.0
0.394
1.815
7
ALWCQMLTGIGKLAGKA
–HHHHHHHHHHHHHHH
2.0
0.344
0.506
8
LLTRIIVGAISAVTSLIKKS
–HHHHHHHHHHHHHHHH–
3.0
0.334
0.531
9
FLSVIKGVWAASLPKQFCAVTAKC
–HHHHHHHHHHHHHHHHHHHT–
3.0
0.334
0.660
10
FLNPIIKIATQILVTAIKCFLKKC
–HHHHHHHHHHHHHHHHHHHT–
4.0
0.334
1.940"
FLPLVRVWAKLI,0.8483606557377049,"Table 5: Ten generated peptides and their physical attributes and predicted structures. ‘H’ is the
α-helix, ‘T’ is the Turn and ‘-’ is the coil."
FLPLVRVWAKLI,0.8524590163934426,"A.3
STRUCTURE CONDITION"
FLPLVRVWAKLI,0.8565573770491803,"As described above, controlling the secondary structure can affect the properties of generated pep-
tides. Thus we limit the percentage of the coil structure with different ratio and calculate the physical
attributes of generated peptides. The results are shown in Figure 6. We can ﬁnd that with the de-
crease of number of coil structures, the percentage of positive peptides keep growing. However, for"
FLPLVRVWAKLI,0.860655737704918,Under review as a conference paper at ICLR 2022
FLPLVRVWAKLI,0.8647540983606558,"(a) ID=1
(b) ID=4
(c) ID=9
(d) ID=10"
FLPLVRVWAKLI,0.8688524590163934,"Figure 5:
3D structures for sequence FLPLVRVWAKLI, GVLPAFKQYLPGIMKIIVKF,
FLSVIKGVWAASLPKQFCAVTAKC, FLNPIIKIATQILVTAIKCFLKKC. ID corresponds to
Table 5."
FLPLVRVWAKLI,0.8729508196721312,"Uniq
C
H
uH
Combination"
FLPLVRVWAKLI,0.8770491803278688,"APD
3222
68.75%
27.96%
4.72%
6.15%
Decoy
2020
21.83%
8.81%
1.98%
0.10%"
FLPLVRVWAKLI,0.8811475409836066,"Random p = 0.1 2055 73.54% ± 0.69% 37.93% ± 0.44% 27.82% ± 0.31% 8.74% ± 0.41%
Random p = 0.2 1831 69.00% ± 0.31% 34.39% ± 0.31% 22.70% ± 1.06% 4.66% ± 0.66%
VAE
475
56.99% ± 2.92% 24.05% ± 3.28%
9.82% ± 1.64%
0.63% ± 0.74%
AMP-GAN
1966 90.86% ± 0.50% 19.55% ± 0.34% 21.26% ± 0.53% 2.10% ± 0.35%
PepCVAE
208
62.76% ± 1.58% 12.61% ± 1.61% 12.66% ± 2.80% 6.68% ± 1.82%
MLPeptide
2106 84.11% ± 0.39% 11.02% ± 0.57% 45.80% ± 1.22% 4.34% ± 0.38%"
FLPLVRVWAKLI,0.8852459016393442,"LSSAMP
4886 75.06% ± 0.37% 39.51% ± 0.64% 27.18% ± 0.41% 9.96% ± 0.07%"
FLPLVRVWAKLI,0.889344262295082,"Table 6: Physical attributes of sequences ﬁltered by secondary structures. The notations are the
same with Table 1."
FLPLVRVWAKLI,0.8934426229508197,"hydrophobicity and hydrophobic moment, the percentage drop after 0.3. Therefore, we limit the
length of the coil structure to 30% in our main experiments. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90%"
FLPLVRVWAKLI,0.8975409836065574,"1.0
0.7
0.4
0.3
0.2"
FLPLVRVWAKLI,0.9016393442622951,"C
H
uH
Combine"
FLPLVRVWAKLI,0.9057377049180327,"Figure 6: The physical attributes of peptides with different percentage of the coil structure. The
x-axis is the maximum percentage and y-axis is the percentage of peptides that meet the attribute
range."
FLPLVRVWAKLI,0.9098360655737705,"A.4
FILTER STRATEGY"
FLPLVRVWAKLI,0.9139344262295082,"Different from the existing work, we control the secondary structures during the generation phase
instead of ﬁltering sequences after the generation. But does this ﬁltering mechanism make sense
for other methods as the post-process? To answer this question, we use the same rules to ﬁlter the
generated peptides of comparison partners. The results are shown in Table 6. Comparing Table 1"
FLPLVRVWAKLI,0.9180327868852459,Under review as a conference paper at ICLR 2022
FLPLVRVWAKLI,0.9221311475409836,tSNE for Amino Acid
FLPLVRVWAKLI,0.9262295081967213,"APD
LSSAMP_wo_cond
LSSAMP
Decoy"
FLPLVRVWAKLI,0.930327868852459,Figure 7: The tSNE plot for the distribution of amino acid in each sequence on four datasets.
FLPLVRVWAKLI,0.9344262295081968,"and 6, we can ﬁnd that all results are improved by limited sequences to the alpha-helical structures.
It implies that by controlling the structure, the physical attributes can be improved, which further
veriﬁes the importance of secondary structures. However, the sequence number have signiﬁcantly
declined, indicating that this generate-then-ﬁlter pipeline is inefﬁcient."
FLPLVRVWAKLI,0.9385245901639344,"A.5
DISTRIBUTION ON AMINO ACIDS"
FLPLVRVWAKLI,0.9426229508196722,"To illustrate the distribution of amino acids in generated peptides, we use the tSNE plot shown in
Figure 7. We convert use the vector with each dimension representing the probability of a certain
amino acid to represent the peptide. Then we use tSNE to convert the high-dimensional vector to 2D
and visualize them. From Figure 7, we can obverse that there is a large overlap between LSSAMP
w/o condition and APD, which indicates that our model has captured the global distribution of APD
instead of collapsing to a local mode. Furthermore, LSSAMP covers APD and has some outliers. It
implies that with the secondary structure condition, our model can not only learn the existing AMP
space but also explore more possible space."
FLPLVRVWAKLI,0.9467213114754098,"A.6
NOVELTY"
FLPLVRVWAKLI,0.9508196721311475,"To measure the novelty of generated peptides, we deﬁne three evaluation metrics: Uniqueness,
Diversity, and Similarity. Uniqueness is the percentage of the unique peptides. Diversity measures
the similarity among generated peptides. We calculate the Levenshtein distance between every two
sequences and normalize it by the sequence length. Then we average the normalized distance to get
the mean as its diversity. The higher the diversity, the more dissimilar the generated peptides are.
Similarity is the similarity between the generated peptides and the training AMP set (APD). For
each generated sequence, we ﬁnd a peptide from the training set which has the smallest Levenshtein
distance with it and normalize the distance by its length. And we calculate the average length as the
similarity."
FLPLVRVWAKLI,0.9549180327868853,"From Table 7, we obverse that VAE has the highest diversity and lowest similarity. However, in
Table 1 and 2, VAE performs the worst, which implies we can not measure the generation quality
based on the novelty alone. Besides, the limitation on secondary structures will cause a signiﬁcant
decline in diversity. However, it does not result in the increase of the number of redundant peptides
since the uniqueness does not increase. It implies that the restrictions make the model capture similar
local patterns but not generate the exact same sequence."
FLPLVRVWAKLI,0.9590163934426229,Under review as a conference paper at ICLR 2022
FLPLVRVWAKLI,0.9631147540983607,"Uniqueness ↑
Diversity ↑
Similarity ↓"
FLPLVRVWAKLI,0.9672131147540983,"Random p = 0.1
99.54% ± 0.02%
0.871 ± 0.021
0.078 ± 0.001
Random p = 0.2
99.99% ± 0.02%
0.971 ± 0.022
0.160 ± 0.001
VAE
98.60% ± 0.05%
1.011 ± 0.038
0.584 ± 0.002
AMP-GAN
99.54% ± 0.07%
0.907 ± 0.023
0.565 ± 0.007
PepCVAE
26.50% ± 0.58%
0.367 ± 0.007
0.423 ± 0.005
MLPeptide
90.02% ± 0.30%
0.850 ± 0.016
0.416 ± 0.010"
FLPLVRVWAKLI,0.9713114754098361,"LSSAMP
98.10% ± 0.09%
0.664 ± 0.018
0.528 ± 0.005
LSSAMP w/o condition
97.64% ± 0.24%
0.828 ± 0.013
0.580 ± 0.008"
FLPLVRVWAKLI,0.9754098360655737,Table 7: The novelty of generated sequences.
FLPLVRVWAKLI,0.9795081967213115,"Codebook
PPL ↓
Loss ↓
AA Acc.↑
SS Acc.↑"
FLPLVRVWAKLI,0.9836065573770492,"[1]
19.04 ± 2.84 2.94 ± 0.14
65.49 ± 3.49
83.41 ± 2.34
[1, 2]
3.84 ± 0.09
1.35 ± 0.02
99.40 ± 0.45
85.39 ± 0.26
[1, 2, 4]
3.32 ± 0.03
1.20 ± 0.01 100.00 ± 0.00 85.95 ± 0.42
[1, 2, 4, 8]
3.24 ± 0.16
1.17 ± 0.05
99.79 ± 0.20
87.20 ± 0.62"
FLPLVRVWAKLI,0.9877049180327869,"Table 8: The mean and standard deviation of results on codebooks. The meanings of symbols are
the same as Table 4."
FLPLVRVWAKLI,0.9918032786885246,"A.7
REPRODUCTION"
FLPLVRVWAKLI,0.9959016393442623,"We describe the architecture of our model and the hyperparameters used in the experiments in detail
in Section 4.1. We run the model for several times and calculate the mean and variance. In Table 8,
we add the mean and the standard deviation of each setting in Table 4. We will also release our code
after the anonymous period."
