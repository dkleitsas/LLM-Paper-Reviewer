Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0005235602094240838,"We study the problem of transfer learning and fine-tuning in linear models for
both regression and binary classification. In particular, we consider the use of
stochastic gradient descent (SGD) on a linear model initialized with pretrained
weights and using a small training data set from the target distribution. In the
asymptotic regime of large models, we provide an exact and rigorous analysis and
relate the generalization errors (in regression) and classification errors (in binary
classification) for the pretrained and fine-tuned models. In particular, we give
conditions under which the fine-tuned model outperforms the pretrained one. An
important aspect of our work is that all the results are ""universal"", in the sense that
they depend only on the first and second order statistics of the target distribution.
They thus extend well beyond the standard Gaussian assumptions commonly made
in the literature. Furthermore, our universality results extend beyond standard SGD
training to the test error of a classification task trained using ridge regression."
INTRODUCTION,0.0010471204188481676,"1
Introduction"
INTRODUCTION,0.0015706806282722514,"Deep neural networks have revolutionized the way data processing and statistical inference are
conducted. Despite their ground-breaking performance, these models often require a plethora of
training samples, which can make the process of data acquisition expensive. Moreover, with the
advent of increasingly complex deep networks and especially Large Language Models (LLMs), the
process of training from scratch has also become prohibitively expensive. To alleviate the scarcity of
prepared data and the high training costs, various strategies have been proposed. One such method is
fine-tuning in which a network previously trained on a source dataset/task different from the target
dataset/task is leveraged as the initialization point for training on the target dataset/task. In many
practical applications, especially for networks containing billions of parameters, only a subset of
weights are updated to adapt the model to the new task. A particularly attractive method involves
fine-tuning only the last layer of the network. A foundational question would be: How effective is
this procedure? Are there fundamental limits to how much one can achieve by utilization of a model
pretrained on a different distribution?"
INTRODUCTION,0.0020942408376963353,âˆ—Equal Contribution
INTRODUCTION,0.002617801047120419,"In this work, we would like to investigate this problem rigorously through the lens of linear regression
and binary classification in the overparametrized regime, where the number of weights/parameters
exceeds the number of data points. To do so, we analyze the performance of regressors/classifiers
obtained through performing SGD initialized at a weight w0 âˆˆRğ‘‘acquired through training on
the source domain. It was shown in Gunasekar et al. [2018] and Azizan and Hassibi [2018] that
gradient descent (GD) and stochastic gradient descent(SGD) iterations converge to the optimal
solution w âˆˆRğ‘‘of the following optimization problem:"
INTRODUCTION,0.0031413612565445027,"min
w âˆ¥w âˆ’w0âˆ¥2
2
(1)"
INTRODUCTION,0.0036649214659685864,"ğ‘ .ğ‘¡
y = Xw"
INTRODUCTION,0.004188481675392671,"where w0 âˆˆRğ‘‘is the initialization point for SGD, X âˆˆRğ‘›Ã—ğ‘‘is the design/data matrix which is
comprised of independent rows reflecting the fact that datapoints (xğ‘–, ğ‘¦ğ‘–) are sampled independently,
and y âˆˆRğ‘›is the vector of labels. This property of SGD is known as ""Implicit Regularization""
and will serve as the basis for our analysis as the pretraining step is captured by the vector w0.
Understanding this ""interpolating regime"" is key to the theoretical analysis of machine learning
models as most of the deep neural networks, on account of their highly overparametrized nature,
operate in a setting where they are able to attain negligible training error. It is noteworthy that,
even for linear models, characterizing the exact performance of fine-tuning approach has been
somewhat limited and is inhibited furthermore by the Gaussianity assumption on X. One of our
main contributions is to overcome this limitation. In fact, to showcase the ubiquity of our results, we
establish a general universality theorem that applies to a large class of data distributions and extends
beyond the context of Transfer Learning. To go into more detail, we say that Gaussian universality
holds for a data distribution P and a training algorithm ğ‘‡if the test error obtained by training on
data sampled from P using ğ‘‡is the same as the test error obtained by training on data sampled
from the Gaussian distribution N (ÂµP, ğšºP) using ğ‘‡, where ÂµP and ğšºP stand for the mean and the
covariance matrix of P respectively. In the context of binary classification, we establish universality
of the classification error with respect to the distribution of each class. In light of the the latter result,
the problem reduces to analyzing the case of the Gaussian design matrices. We allow classes with
different non-scalar covariance matrices ğšº1 and ğšº2 and build on the results of Akhtiamov et al. [2024]
to address the problem in this specific scenario."
RELATED WORKS,0.004712041884816754,"1.1
Related Works"
RELATED WORKS,0.005235602094240838,"Transfer Learning has been an active topic of research at least since the 1970â€™s Bozinovski [2020]. It
is mainly applied in the situations where obtaining data from the true distribution, which we will refer
as the target distribution, is costly but there is a cheap way to access data from a source distribution,
which bears resemblance to the target distribution. The two most popular approaches to transfer
learning consist of instance-based transfer learning and fine-tuning."
RELATED WORKS,0.005759162303664921,"Instance-based transfer learning incorporates the source dataset, along with the target dataset, and
trains the model on this amalgamated dataset. The key insight is that, provided that the source
distribution is close enough to the target distibution and a suitable training scheme is chosen, the final
performance enjoys an improvement. We refer the reader to the landmark workDai et al. [2007] as
well as to the comprehensive overviews covering the empirical advances in this area Tan et al. [2018],
Zhuang et al. [2020]. For theoretical analysis, we would like to emphasize a recent work Jain et al.
[2024] that characterized the scaling laws for ridge regression over the union of the source and target
data in the high dimensional regime."
RELATED WORKS,0.0062827225130890054,"The present manuscript focuses on fine-tuning in which a model is first pretrained on the source
distribution and then fine-tuned using the target data. Dar et al. [2021] analyzes transfer learning for
linear regression assuming Gaussianity of the data. The results obtained in this paper, in particular,
imply that the generalization error obtained in their work is universal, in the sense that they can be
immediately extended to a much broader set of target distributions, which we elaborate on in much
greater detail in the main body of the paper."
RELATED WORKS,0.006806282722513089,"Gerace et al. [2022] consider the problem of binary classification via a two-layer neural network in
a synthetic setting. The source data and the source labels are sampled according to x = ğ‘…ğ‘’ğ‘™ğ‘¢(Ga)
and ğ‘¦= ğ‘†ğ‘–ğ‘”ğ‘›(ağ‘‡z) respectively where G, a and z are i.i.d. Gaussian. The target data and labels are
generated by perturbing G and a and then applying the same rules as for the source data. Gerace
et al. [2022] trained the network on the source dataset and then proceeded by only fine-tuning the"
RELATED WORKS,0.007329842931937173,"last layer on the target domain using the cross-entropy loss with an â„“2 regularizer. The analysis was
conducted using the Replica method. The authors of Gerace et al. [2022] compared their results with
an equivalent random feature model and observed empirically a certain kind of Gaussian universality
for real-world datasets, such as MNIST."
RELATED WORKS,0.007853403141361256,"Extension of results obtained under Gaussianity assumptions to non-Gaussian distributions remains a
pertinent research direction in which the idea of Gaussian universality plays a central role. Montanari
and Nguyen [2017] proved the universality of the generalization error for the elastic net, assuming the
design matrix A is i.i.d subgaussian. Panahi and Hassibi [2017] generalized the result to the problem
of regularized regression with a quadratic loss and a convex separable regularizer which is either
ğ‘“(Â·) = âˆ¥Â· âˆ¥1 (LASSO) or strongly convex. Han and Shen [2023] generalized Panahi and Hassibi
[2017]â€™s results to non-separable Lipschitz test functions and provided non-asymptotic bounds for
the concentration of solutions. In the random geometry literature, Oymak and Tropp [2018] showed
universality of the embedding dimension of randomized dimension reduction linear maps with i.i.d
entries satisfying certain moment conditions. Abbasi et al. [2019] proved universality of the recovery
threshold for minimizing strongly convex functions under linear measurements constraint, assuming
the rows are iid and the norm of the mean is asymptotically negligible compared to the noise. Lahiry
and Sur [2023], proved the universality of generalization error for ridge regression and LASSO when
the rows are distributed iid with a specific block structure dependence per row, AD where A has zero
mean iid random subgaussian vectors per row and D being diagonal."
RELATED WORKS,0.008376963350785341,"Leveraging universality is not limited to the signal recovery literature. As an example, Hu and Lu
[2022], Bosch et al. [2023], SchrÃ¶der et al. [2023], have analyzed the performance of the random
feature models by replacing nonlinear functions of Gaussians with the corresponding Gaussian
distribution having the same mean and covariance in single-layer and multiple-layer scenarios,
respectively, through a universality argument, referred to as the Gaussian equivalence principle. In
a more general setting, under subgaussianity assumption on the data, Montanari and Saeed [2022]
proved the universality of the training error for general loss and regularizer and test error when the
regularizer is strongly convex and the loss is convex. Hastie et al. [2022] proved universality for the
the minimal â„“2-norm linear interpolators for the data generated via a very specific rule x = ğœ(Wğšº
1
2 z),
where z is i.i.d zero mean and satisfies several other technical properties and ğšºis a deterministic PSD
matrix. Dandi et al. [2024] considered the universality of mixture distributions in a similar setting to
Montanari and Saeed [2022] and proved the universality of the free energy of the test error."
OUTLINE OF THE PAPER,0.008900523560209424,"1.2
Outline of the Paper"
OUTLINE OF THE PAPER,0.009424083769633508,"In Section 2.1, we formally define the optimization problem for which we aim to establish universality.
Section 2.2 outlines our primary contributions. In Section 3, we present our main universality result,
accompanied by several insightful remarks. Section 4 focuses on our findings related to fine-tuning
in the contexts of linear regression and binary classification. In Section 5, we validate our theoretical
results through empirical experiments. Finally, we conclude with a summary and discussion in
Section 6."
PROBLEM FORMULATION,0.009947643979057591,"2
Problem Formulation"
THE SETTING,0.010471204188481676,"2.1
The Setting"
THE SETTING,0.01099476439790576,"We use bold letters for vectors and matrices. We denote the ğ‘–â€™th largest singular value of matrix A by
ğ‘ ğ‘–(A). We consider the proportional regime where ğ‘‘= Î˜(ğ‘›) and ğ‘‘"
THE SETTING,0.011518324607329843,"ğ‘›â†’ğœ…> 1. We refer to Section
A.1 for the other notations and definitions necessary for the rest of this exposition."
THE SETTING,0.012041884816753926,"Given a training dataset {(xğ‘–, ğ‘¦ğ‘–)}ğ‘›
ğ‘–=1 and a model with a fixed architecture, the conventional approach
of learning the weights for the model consists of choosing a loss function L and finding ğ‘¤minimizing
1
ğ‘›
Ãğ‘›
ğ‘–=1 L(xğ‘–, wğ‘–, ğ‘¦ğ‘–). In this exposition we focus on linear models and loss functions of the form
L(xğ‘–, wğ‘–, ğ‘¦ğ‘–) = â„“(ğ‘¦ğ‘–âˆ’xğ‘‡
ğ‘–wğ‘–), where â„“is differentiable and â„“(0) = 0. Azizan and Hassibi [2018]
characterized behavior of a broad family of optimizers, called stochastic mirror descent (SMD)
algorithms, for linear models in the over-parametrized regime. For a strictly convex function
ğ‘”: Rğ‘‘â†’R, the update rule of the SMD with a mirror ğ‘”and a learning rate ğœ‚> 0 is defined as"
THE SETTING,0.012565445026178011,"âˆ‡ğ‘”(wğ‘¡) = âˆ‡ğ‘”(wğ‘¡âˆ’1) âˆ’ğœ‚âˆ‡Lğ‘¡(wğ‘¡âˆ’1),
ğ‘¡â‰¥1,
(2)"
THE SETTING,0.013089005235602094,"where ğœ‚> 0 is the learning rate and Lğ‘¡is the loss function L evaluated at a point chosen at random
in the dataset corresponding to the ğ‘¡â€™th iteration. Due to strict convexity, âˆ‡ğ‘”(Â·) defines an invertible
transformation, which is why (2) is indeed a well-defined update rule. Note also that this includes
SGD as a special case, which corresponds to taking ğ‘”(Â·) = âˆ¥Â· âˆ¥2
2. Azizan and Hassibi [2018] show
that applying SMD initialized at w0 to minimize 1"
THE SETTING,0.013612565445026177,"ğ‘›
Ãğ‘›
ğ‘–=1 â„“(ğ‘¦ğ‘–âˆ’xğ‘‡
ğ‘–w) yields a weight vector defined
by the following optimization problem:"
THE SETTING,0.014136125654450262,"min
w ğ·ğ‘”(w, w0)"
THE SETTING,0.014659685863874346,"ğ‘ .ğ‘¡
ğ‘¦ğ‘–= xğ‘‡
ğ‘–w,
1 â‰¤ğ‘–â‰¤ğ‘›"
THE SETTING,0.015183246073298429,"We construct the random matrix A âˆˆRğ‘›Ã—ğ‘‘by setting its rows equal to xğ‘–. Hence, stating in a more
general fashion, we are interested in the analysis of the following optimization problem, with the
main case of interest being when ğ‘“is a quadratic:"
THE SETTING,0.015706806282722512,"min
w
ğ‘“(w)"
THE SETTING,0.016230366492146597,"ğ‘ .ğ‘¡
y = Aw"
THE SETTING,0.016753926701570682,"As the objective is comprised of minimizing a strongly convex function ğ‘“over a closed convex set, it
has a unique minimizer. By using a Lagrange multiplier ğœ†âˆˆR, this convex optimization problem can
be written in the unconstrained form:"
THE SETTING,0.017277486910994764,"Î¦(A) := sup
ğœ†>0
min
ğ‘¤
ğœ†
2 âˆ¥Aw âˆ’yâˆ¥2
2 + ğ‘“(w)
(3)"
THE SETTING,0.01780104712041885,"Considering the objective without the supğœ†>0 yields the following regularized linear regression
problem for which we will also establish a universality theorem:"
THE SETTING,0.01832460732984293,"Î¦ğœ†(A) := min
ğ‘¤
ğœ†
2 âˆ¥Aw âˆ’yâˆ¥2
2 + ğ‘“(w)
(4)"
THE SETTING,0.018848167539267015,"Note that (4) captures the case of explicit regularization, which is of highest importance whenever
there is a high amount of noise or label corruption present. Furthermore, by deliberately choosing
the regularizer ğ‘“, it is possible to obtain solutions that exhibit certain behavior, viz being sparse or
compressed. In order to tackle optimizations such as 3, 4, we prove their equivalence to a problem
with a suitable Gaussian design G which is described in Definition 4 in Section A.1. Our approach
for proving universality will be the Lindeberg approach Lindeberg [1922] Chatterjee [2006]."
OUR CONTRIBUTIONS,0.0193717277486911,"2.2
Our Contributions"
TRANSFER LEARNING,0.019895287958115182,"2.2.1
Transfer Learning"
TRANSFER LEARNING,0.020418848167539267,"Linear Regression We extend the results from Dar et al. [2021] by providing precise expressions for
the generalization error in linear regression tasks. In addition, we demonstrate that the test error is
always lower-bounded by a quantity that is attained only when the covariance matrix of the data is
scalar. Furthermore, we identify specific conditions under which fine-tuning is successful."
TRANSFER LEARNING,0.020942408376963352,"Binary Classification We present the first precise characterization of the classification error for
linear models trained using stochastic gradient descent (SGD) on data drawn from a general mixture
distribution with arbitrary covariance matrices. Moreover, we delineate the regimes in which fine-
tuning proves effective."
UNIVERSALITY,0.021465968586387434,"2.2.2
Universality"
UNIVERSALITY,0.02198952879581152,"Proof of universality for implicit regularization. In the context of SGD and, more generally,
SMD (see 2) with a mirror ğ‘“satisfying the assumptions of Theorem 1, we prove universality of the
generalization error as well as of the value of the corresponding implicit regularization objective for a
wide range of data distributions characterized by Assumptions 1. Note that this cannot be reduced to
any known results on universality of constrained objectives as the latter assume that the constraints
are deterministic, i.e the optimization variables belong to some determinitic set C chatacterized by
the constraints. While to analyze SMD one has to deal with constraints of the form Aw = y which
represents a random polytope. To the best of our knowledge, the only other paper dealing with
universality in the context of implicit regularization is Hastie et al. [2022]. They study minimal"
UNIVERSALITY,0.022513089005235604,"â„“2-norm linear interpolators, but they work with a very specific (random feature) model for data
distributions defined by x = ğœ(Wğšº
1
2 z), where z is i.i.d zero mean and W is i.i.d. Gaussian. Our
paper generalizes theirs in two non-trivial ways, as it allows for arbitrary smooth convex strongly
convex mirrors ğ‘“(w) as well as more general data distributions."
UNIVERSALITY,0.023036649214659685,"Relaxing assumptions for explicit regularization. To the best of the authorsâ€™ knowledge, all
previous results on universality assumed either that the data points x âˆˆRğ‘‘have i.i.d. entries or
that the rows are independent and x is subgaussian. We relax these assumptions. That is, we
show that, for the quadratic loss function and any strongly convex not necessarily differentiable
regularizer universality holds as long as the rows are i.i.d., all moments of the x up to the 6th satisfy"
UNIVERSALITY,0.02356020942408377,"Ex|(x âˆ’Ex)ğ‘‡v|ğ‘â‰¤ğ¾
âˆ¥vâˆ¥ğ‘
2
ğ‘‘ğ‘/2 for v âˆˆRğ‘‘and ğ‘‰ğ‘ğ‘Ÿ(xğ‘‡Cx) â†’0 for any C of bounded operator norm."
UNIVERSALITY,0.024083769633507852,"Extending universality to mixtures of distributions. Since the main motivation behind this work is
the study of generalization in classification tasks, we focus on data matrices sampled from mixture
distributions. Note that most previous papers on universality, such as Montanari and Saeed [2022],
do not apply to this setting directly. As an illustration to why results of Montanari and Saeed [2022]
cannot just be applied to the mixture distributions directly, consider P = 1"
UNIVERSALITY,0.024607329842931937,"2N (Âµ, I) + 1"
UNIVERSALITY,0.025130890052356022,"2N (âˆ’Âµ, I)
corresponding to a mixture of two classes with antipodal means and the resulting classification
error depends on âˆ¥Âµâˆ¥, while P has mean 0 and covariance I meaning that the matching gaussian
distribution N (0, I) does not contain any information about the classes."
UNIVERSALITY,0.025654450261780103,"However, there is one prior paper Dandi et al. [2024] that studies universality specifically in the
context of mixture distributions. It is worth mentioning that their definition of universality is different.
Namely, they prove universality of the expectation with respect to the Gibbs distribution, which
suffices to show universality of the train but not of the test error."
UNIVERSALITY,0.02617801047120419,"Allowing for non-vanishing means. In Definition 2, we assume only that Eaâˆ¥a âˆ’Âµâˆ¥2
2 = ğ‘‚(1),
whereas in Abbasi et al. [2019], Montanari and Saeed [2022] and Dandi et al. [2024] they have
âˆ¥Âµâˆ¥2
2
Eaâˆ¥aâˆ’Âµâˆ¥2
2 â†’0, implying that the norms of the means are negligible compared to the norms of the"
UNIVERSALITY,0.026701570680628273,"data points on average. To our knowledge, our work is the first to tackle it."
MAIN RESULT,0.027225130890052355,"3
Main Result"
MAIN RESULT,0.02774869109947644,"In this section, we present our main universality theorem and before doing so, we list the required
technical assumptions. Consider the following convex optimization problem"
MAIN RESULT,0.028272251308900525,"Î¦ğœ†(A) := min
ğ‘¤
ğœ†
2ğ‘›âˆ¥Aw âˆ’yâˆ¥2
2 + 1"
MAIN RESULT,0.028795811518324606,"ğ‘›ğ‘“(w)
(5)"
MAIN RESULT,0.02931937172774869,"Denote the solution to the optimization problem 5 above by wÎ¦ğœ†(A). Then we assume the following:
Assumptions 1.
1. A is a regular block matrix and G is its matching Gaussian matrix as in
Section A.1."
MAIN RESULT,0.029842931937172777,"2. The regularizer ğ‘“(Â·) is regular also as Section A.1 or there exists a sequence of regular ğ‘“ğ‘šâ€™s
converging uniformly to ğ‘“."
MAIN RESULT,0.030366492146596858,3. ğ‘“(Â·) is ğ‘€-strongly convex.
MAIN RESULT,0.030890052356020943,"4. Eyâˆ¥yâˆ¥2
2 = ğ‘‚(ğ‘‘) with E|ğ‘¦ğ‘—|ğ‘= ğ‘‚(1) for ğ‘—âˆˆ[ğ‘›] and ğ‘âˆˆ[6] and is independent of A."
MAIN RESULT,0.031413612565445025,"5. ğ‘ min(AAğ‘‡) = Î©(1) with high probability,"
MAIN RESULT,0.03193717277486911,"6. ğ‘‰ğ‘ğ‘Ÿ(âˆ¥yâˆ¥2
2) â†’0"
MAIN RESULT,0.032460732984293195,"7. âˆ¥âˆ‡ğ‘“(wÎ¦ğœ†(A))âˆ¥2 â‰¤ğ¾ğ‘“
âˆšğ‘›for a constant ğ¾ğ‘“independent of ğœ†.
Theorem 1. If Î¨ = ğš½= supğœ†>0 Î¦ğœ†, suppose parts (1) - (7) of Assumptions 1 hold and for Î¨ = Î¦ğœ†,
only assume parts (1) - (4) of Assumptions 1. Then"
MAIN RESULT,0.032984293193717276,"1. (Universality of the training error) For any L-Lipschitz ğ‘”: Rğ‘‘â†’R, and every ğ‘¡1 > 0, ğ‘¡2 >
ğ‘¡1, ğ‘âˆˆR the following holds:"
MAIN RESULT,0.033507853403141365,â€¢ If P(|ğ‘”(Î¨(G)) âˆ’ğ‘| > ğ‘¡1) â†’0 then P(|ğ‘”(Î¨(A)) âˆ’ğ‘| > ğ‘¡2) â†’0.
MAIN RESULT,0.034031413612565446,"â€¢ Furthermore, if ğ‘”has bounded second derivative, then"
MAIN RESULT,0.03455497382198953,"lim
ğ‘›â†’âˆ"
MAIN RESULT,0.03507853403141361,"EA,yğ‘”(Î¨(A)) âˆ’EG,yğ‘”(Î¨(G))
 = 0"
MAIN RESULT,0.0356020942408377,"2. (Universality of the solution) For a function ğœ“with âˆ‡2ğœ“âª¯ğ‘I if either ğœ“is regular
(Definition 1) or there is a sequence of regular functions converging uniformly to ğœ“,"
MAIN RESULT,0.03612565445026178,"â€¢ If for every ğ‘¡> 0, P(|Î¨(G) âˆ’ğ‘| > ğ‘¡) â†’0 then for B âˆˆ{A, G}, there exists ğ‘1 âˆˆR
such that for any ğ‘¡1 > 0 we have P(|ğœ“(wÎ¨(B)) âˆ’ğ‘1| > ğ‘¡1) â†’0.
â€¢ Furthermore, if ğœ“is bounded"
MAIN RESULT,0.03664921465968586,"lim
ğ‘›â†’âˆ"
MAIN RESULT,0.03717277486910995,"EA,yğœ“(wÎ¨(A)) âˆ’EG,yğœ“(wÎ¨(G))
 = 0"
MAIN RESULT,0.03769633507853403,"To sum up, Theorem 1 says that the vector of weights wA trained on data sampled from a non-gaussian
distribution A either via SMD with a mirror ğ‘“or by minimizing the least squares objective with
regularizer ğ‘“, shares many similar characteristics with the vector of weights wG trained on data
sampled from the matching GMM G via the same optimization procedure under certain technical
assumptions on A and ğ‘“. By ""similar characteristics"", we mean that for any ğœ“: Rğ‘‘â†’R with
bounded Hessian, ğœ“(wA) = ğœ“(wG) holds in the limit."
MAIN RESULT,0.03821989528795811,"We would like to point out a few remarks.
Remark 1. Note that ğœ“need not be convex, in fact any linear combination of regular ğœ“is also
regular. An immediate result is the universality of the empirical distribution of the coordinates of
solutions.
Remark 2. The assumption âˆ¥âˆ‡ğ‘“(wÎ¦ğœ†(A))âˆ¥2 = ğ‘‚(âˆšğ‘›) in Theorem 1 is satisfied for any function ğ‘“
with a locally lipschitz gradient, i.e âˆ¥âˆ‡ğ‘“(u)âˆ¥2 â‰¤ğ¾(1+ âˆ¥uâˆ¥2). In particular it applies to ğ‘“(Â·) = âˆ¥Â· âˆ¥2
2.
Remark 3. In both parts of Theorem 1, the regularizer is allowed to be an ğ‘€-strongly convex uniform
limit of differentiable convex functions, which implies that ğ‘“(w) = ğ‘¡âˆ¥wâˆ¥1 + ğ‘€âˆ¥wâˆ¥2
2, ğ‘¡> 0, ğ‘€> 0,
known as the elastic net, is also included in our results.
Remark 4. In Theorem 1, the optimal value of Î¦ğœ†is only achieved when ğœ†â†’âˆand is not attained
at any finite ğœ†. This means that proving the results for ğš¿= Î¦ in Theorem 1 requires additional ideas
apart from the case Î¨ = Î¦ğœ†in Theorem 1, as the latter makes use of the boundedness of ğœ†extensively.
To tackle this issue, we present a uniform convergence result in ğœ†which might be of independent
interest.
Remark 5. The condition ğ‘ min(AAğ‘‡) = Î©(1) can be satisfied for a variety of random matrix models.
If for each block of A, Ağ‘–= ËœAğ‘–ğšº1/2
ğ‘–
where ËœAğ‘–has iid entries and ğ‘ min(ğšºğ‘–) = Î©(1), then by the
Bai-Yinâ€™s law Bai and Yin [2008] the condition is satisfied. The second family is comprised of blocks
with independent and identical rows where the norm of rows have exponentrial concentration. We
prove this in the Appendix. One particular instance will be the distributions satsifying LCP from
Definition 3.
Remark 6. The second assumption in Assumptions 1 is not too restrictive and is in particular
satisfied for ğœ“(w) = P(ağ‘‡w > ğ‘). We have with high probability for Î¨ âˆˆ{Î¦ğœ†, Î¦}"
MAIN RESULT,0.0387434554973822,"lim
ğ‘›â†’âˆ"
MAIN RESULT,0.03926701570680628,"P(ağ‘‡wÎ¨(A) > ğ‘) âˆ’P(ağ‘‡wÎ¨(G) > ğ‘)
 = 0"
MAIN RESULT,0.039790575916230364,"For a independent of A, G but sampled from the same distribution as the rows of A, respectively. Note
that by this argument we have reduced the problem of verifying CLT for ağ‘‡wÎ¨(A) to its Gaussian
counterpart, ağ‘‡wÎ¨(G). Then, specifically for the applications presented in this paper, we provide a
proof that ağ‘‡wÎ¨(G) satisfies CLT w.r.t. to randomness in a with high probability in G. However, in
general the latter CLT condition has to be verified on a case by case basis."
MAIN RESULT,0.04031413612565445,"4
Applications: Transfer Learning"
REGRESSION,0.040837696335078534,"4.1
Regression"
REGRESSION,0.041361256544502616,"We consider the classical problem of recovering the best linear regressor for the following linear
model
y = Xwâˆ—+ z
(6)"
REGRESSION,0.041884816753926704,"Here, wâˆ—is the ground truth, the rows of the data matrix X are i.i.d with Exğ‘–= 0 and Exğ‘–xğ‘‡
ğ‘–=: Rğ‘¥,
z is centered and is independent of X and satisfies Ezâˆ¥zâˆ¥2
2 = ğœ2ğ‘›. For a given Ë†w, its generalization
error is defined by:"
REGRESSION,0.042408376963350786,ğ‘’ğ‘”ğ‘’ğ‘›( Ë†w) := Eğ‘¥(xğ‘‡wâˆ—âˆ’xğ‘‡Ë†w)2 = ( Ë†w âˆ’wâˆ—)ğ‘‡Rğ‘¥( Ë†w âˆ’wâˆ—)
REGRESSION,0.04293193717277487,"Now in order to recover wâˆ—given observations (y, X), we choose to optimize the least squares
objective, minw âˆ¥y âˆ’Xwâˆ¥2
2, and to do so, we leverage SGD. We would like to investigate how useful
having a pretrained classifier w0 can be for the recovery of wâˆ—. As pointed out earlier, SGD initialized
from w = w0, by its implicit regularization property, converges to the solution Ë†w of (1). We denote
ğ‘’ğ‘:= ğ‘’ğ‘”ğ‘’ğ‘›(w0); hence the name ""a priori error"", ğ‘’ğ‘. We provide the assumptions necessary for our
results on linear regression in Section A.2. In what follows we give a precise characterization of the
posterior error, ğ‘’ğ‘, of the model after training with SGD, in terms of ğ‘’ğ‘and the other parameters of
the problem. In order to leverage Theorem 1 to establish the universality of the generalization error,
it is sufficient to apply a change of variable wâ€² := Râˆ’1/2
ğ‘¥
w and use ğœ“(w) = âˆ¥wâˆ¥2
2 as the test function.
Theorem 2. Under Assumptions 3 in Section A.2, the generalization error of the SGD solution
initialized from w0 converges in probability to"
REGRESSION,0.043455497382198956,ğ‘’ğ‘= 2 âˆ’ğœ…(1 âˆ’ğ‘¡)
REGRESSION,0.04397905759162304,"ğœ…(1 âˆ’ğ‘¡) âˆ’1ğœ2 +
ğ‘¡
ğœ…(1 âˆ’ğ‘¡) âˆ’1ğ‘’ğ‘
(7)"
REGRESSION,0.04450261780104712,"With ğ‘¡=
âˆ«
4ğ‘(ğ‘Ÿ)
(2+ğ‘Ÿğœƒ)2 ğ‘‘ğ‘Ÿwhere ğœƒis found through ğœ…âˆ’1 ğœ…
= 1"
REGRESSION,0.04502617801047121,ğœƒğ‘†ğ‘(âˆ’1
REGRESSION,0.04554973821989529,ğœƒ) (12). And ğœ…> 1 is the proportional
REGRESSION,0.04607329842931937,"constant, i.e for X âˆˆRğ‘›Ã—ğ‘‘, ğ‘‘"
REGRESSION,0.04659685863874346,"ğ‘›â†’ğœ…. Moreover, for any distribution ğ‘(ğ‘Ÿ) we have"
REGRESSION,0.04712041884816754,"ğ‘’ğ‘â‰¥
1
ğœ…âˆ’1ğœ2 + ğœ…âˆ’1"
REGRESSION,0.04764397905759162,"ğœ…
ğ‘’ğ‘
(8)"
REGRESSION,0.048167539267015703,The lower bound is attained if and only if ğ‘(ğ‘Ÿ) = ğ›¿(ğ‘Ÿâˆ’ğ‘Ÿ0) for some ğ‘Ÿ0 > 0
REGRESSION,0.04869109947643979,"The following remark is immediate:
Remark 7. Theorem 2 entails that, depending on the noise level present in the data, training could
be even potentially harmful. Indeed, if ğœ2 â‰¥ğ‘’ğ‘, then ğ‘’ğ‘â‰¥
1
ğœ…âˆ’1ğœ2 + ğœ…âˆ’1"
REGRESSION,0.049214659685863874,"ğœ…ğ‘’ğ‘â‰¥ğœ(2âˆšğ‘’ğ‘âˆ’ğœ) â‰¥ğ‘’ğ‘
for any ğœ…> 1 and it is therefore more appropriate to use vector w0 instead of performing fine-tuning.
Moreover, if the covariance Rx is scalar, then the converse is true. Namely, if ğ‘’ğ‘â‰¥ğœ2, then the
best achievable error corresponds to ğœ…âˆ—=
âˆšğ‘’ğ‘
âˆšğ‘’ğ‘âˆ’ğœand equals ğœ(2âˆšğ‘’ğ‘âˆ’ğœ), which is less than ğ‘’ğ‘.
Therefore, transfer learning contributes to improving the test performance when the variance of the
noise is not too high, but the model has to be fine-tuned on the correct amount of target data."
CLASSIFICATION,0.049738219895287955,"4.2
Classification"
PROBLEM SETTING,0.050261780104712044,"4.2.1
Problem setting"
PROBLEM SETTING,0.050785340314136125,"Consider a binary classification task and let X stand for the data matrix and y denote the vector
of labels where each ğ‘¦ğ‘–= Â±1 depending on what class the ğ‘–-th point xğ‘–falls into. We denote
Âµâ„“:= Exğ‘–and ğšºâ„“:= Exğ‘–xğ‘‡
ğ‘–âˆ’Âµâ„“Âµğ‘‡
â„“, for the mean and covariance of each datapoint xğ‘–, respectively
and â„“âˆˆ{1, 2} depending on the class of xğ‘–. After learning a linear classifier w, we assign labels to
new previously unseen points according to yğ‘›ğ‘’ğ‘¤= ğ‘ ğ‘–ğ‘”ğ‘›(wğ‘‡xğ‘›ğ‘’ğ‘¤). Without loss of generality we
will assume that the first ğ‘›"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05130890052356021,"2 rows of X are sampled from the first class and the remaining rows are
sampled from the second as we can permute the rows otherwise. Modulo such permutation, it is
straightforward to see that X satisfies parts (1) - (3) of Definition 2. Since the topic of the present
paper is fine-tuning, we assume that the following steps are performed:"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.051832460732984295,"1. Obtain a pre-trained classifier w0
2. Renormalize w0 obtained during the previous step using the target data via"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05235602094240838,"min
ğ›¼âˆ¥y âˆ’ğ›¼Xw0âˆ¥2"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05287958115183246,This yields:
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05340314136125655,ğ›¼= yğ‘‡Xw
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05392670157068063,"âˆ¥Xwâˆ¥2
(9)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05445026178010471,"After finding ğ›¼take wâ€²
0 := ğ›¼w0. This transform preserves the direction of w0, while setting"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0549738219895288,its squared magnitude to ğ›¼2âˆ¥w0âˆ¥2 = (yğ‘‡Xw0)2âˆ¥w0âˆ¥2
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05549738219895288,"âˆ¥Xw0âˆ¥4
, which depends only on the direction of
w0 but not on its magnitude anymore. We find applying this transform very meaningful, as
it does not change the classification error for the source data but simplifies learning for the
regression problem defined by the target data.
3. Learn the final classifier from the target data X âˆˆRğ‘›Ã—ğ‘‘and labels y âˆˆRğ‘›using SGD
initialized at wâ€²
0, obtained from the previous step."
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05602094240837696,"We will use the assumptions in Section A.3 to define further details of the classification task we
consider. Note that in practice the main difficulty of the classification task in the over-parametrized
regime (ğ‘›< ğ‘‘) arises due to the fact that Âµ1, Âµ2, ğšº1 and ğšº2 are not known and cannot be estimated
reliably. Nevertheless, we would like to start with characterizing the optimal performance in the
scenario where these are provided to us by an oracle. Lemma 1 in Section A.3 provides such a
characterization under certain symmetry assumptions. In view of Lemma 1 it is natural to introduce
the following assumption:
Assumptions 2. The initialization point w0 satisfies"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05654450261780105,"w0 = ğ‘¡âˆ—wâˆ—+ ğ‘¡ğœ‚Î·
(10)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05706806282722513,"where wâˆ—= (ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2) is the optimal classifier defined as in Lemma 1. Î· is a noise vector
with âˆ¥Î·âˆ¥2 = 1 âˆ’ğ‘Ÿand for any deterministic matrix C of bounded operator norm it holds that Î·ğ‘‡CÎ·
converges in probability to Tr(C) (1âˆ’ğ‘Ÿ)"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05759162303664921,"ğ‘‘
. Note that the smaller the ratio ğ‘¡ğœ‚"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0581151832460733,"ğ‘¡âˆ—is, the better performance
w0 has."
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05863874345549738,"Indeed, 10 captures the closeness of the initilization point w0 to the optimal classifier wâˆ—. For
example, in the isotropic case ğšº1 = ğšº2 = ğœ2I, if w0 has the classification error ğ‘’ğ‘, this means that"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.059162303664921465,we should take ğ‘¡ğœ‚
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.05968586387434555,"ğ‘¡âˆ—=
ğ‘‘
2ğœ2
âˆšï¸ƒ"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.060209424083769635,"ğ‘‘(1âˆ’ğ‘Ÿ)
ğœ2ğ‘„âˆ’1(ğ‘’ğ‘)2 âˆ’2. Finally, we would like to remark that, under notation and
assumptions from Theorem 1, the following corollary is implied by Theorem 1 modulo Theorem 4
presented in the Appendix along with the explanation of the implication:
Corollary 1 (Universality of the classification error for SGD and ridge regression objectives). If
ğ‘“(w) = âˆ¥w âˆ’w0âˆ¥2
2, when âˆ¥w0âˆ¥2
2 = ğ‘‚(ğ‘‘) then for a, g independent of A, G but sampled from the
same distribution as their rows, respectively, we have with high probability for Î¨ âˆˆ{Î¦ğœ†, Î¦}"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.060732984293193716,"lim
ğ‘›â†’âˆ"
ROWS OF X ARE SAMPLED FROM THE FIRST CLASS AND THE REMAINING ROWS ARE,0.0612565445026178,"P(ağ‘‡wÎ¨(A) > 0) âˆ’P(gğ‘‡wÎ¨(G) > 0)
 = 0"
ANALYSIS OF THE CLASSIFICATION ERROR,0.061780104712041886,"4.2.2
Analysis of the Classification Error"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06230366492146597,"Theorem 3. Let w0 be an initialization point that satisfies Assumption 2 and X be a data matrix
satisfying Assumptions 4 in Section A.3. Then the classification error of the SGD solution initialized"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06282722513089005,"at ğ›¼w0 for ğ›¼defined by (9) and trained on X, y is given by ğ‘’ğ‘= ğ‘„  âˆ’ğ›¾+2 2
âˆšï¸ƒ ğœ2âˆ’ğ›¾2 4 !"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06335078534031413,where ğ›¾and ğœare
ANALYSIS OF THE CLASSIFICATION ERROR,0.06387434554973823,"determined in terms of a ğœƒsolving 2
âˆš"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06439790575916231,"2
ğœƒâˆšğ‘›ğ‘†ğšº1+ğšº2(âˆ’2
âˆš"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06492146596858639,"2
ğœƒâˆšğ‘›) = 1 âˆ’1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06544502617801047,ğœ…whose expressions are provided in the
ANALYSIS OF THE CLASSIFICATION ERROR,0.06596858638743455,"Appendix (cf. Section I, equation 40) with ğ‘‘"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06649214659685863,ğ‘›â†’ğœ…> 1.
ANALYSIS OF THE CLASSIFICATION ERROR,0.06701570680628273,"The expressions from Theorem 3 can be simplified drastically in the case of scalar covariance
matrices."
ANALYSIS OF THE CLASSIFICATION ERROR,0.06753926701570681,"Corollary 2. Under the notation from Theorem 3, assume Î£1 = Î£2 = ğœ2Iğ‘‘"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06806282722513089,"ğ‘‘
and define ğœŒ:= ğ‘‘(1âˆ’ğ‘Ÿ)"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06858638743455497,"ğœ2
.
Let ğ‘’ğ‘be the classification error of w0 initialized according to 10. Then"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06910994764397906,"ğ‘’ğ‘(ğœ…, ğœŒ)
Pâˆ’â†’ğ‘„"
ANALYSIS OF THE CLASSIFICATION ERROR,0.06963350785340314,"
ğ‘„âˆ’1(ğ‘’ğ‘)2(2ğœ…+ ğœŒâˆ’2) + ğœŒ
âˆšï¸"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07015706806282722,"ğœ…(ğœ…âˆ’1)
âˆšï¸ƒ"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07068062827225131,(2ğœ…+ ğœŒ)((4ğœ…âˆ’2)ğ‘„âˆ’1(ğ‘’ğ‘)4 + ğ‘„âˆ’1(ğ‘’ğ‘)2  2ğœ…3 + ğœ…2ğœŒâˆ’2ğœ…(ğœŒâˆ’1) + ğœŒ + 2ğœ…2) ! (11)
ANALYSIS OF THE CLASSIFICATION ERROR,0.0712041884816754,We arrive at the following conclusion summarizing the derivations above.
ANALYSIS OF THE CLASSIFICATION ERROR,0.07172774869109948,"Remark 8.
â€¢ If ğœŒ= ğœ”(1), then ğ‘’ğ‘(ğœ…, âˆ)
Pâˆ’â†’ğ‘„

ğ‘„âˆ’1(ğ‘’ğ‘) +
1
ğ‘„âˆ’1(ğ‘’ğ‘)"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07225130890052356,"âˆšï¸
ğœ…
ğœ…âˆ’1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07277486910994764,"
< ğ‘’ğ‘and fine-"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07329842931937172,"tuning always succeeds. It is observed that for ğ‘’ğ‘> ğ‘„(1), the worse ğ‘’ğ‘is, the better ğ‘’ğ‘
will be."
ANALYSIS OF THE CLASSIFICATION ERROR,0.07382198952879582,"â€¢ If ğœŒ= Î˜(1), then the fine-tuning step may or may not succeed depending on whether
ğ‘’ğ‘(ğœ…, ğœŒ) < ğ‘’ğ‘or not. See Section 5.2 for further (empirical) explorations on the usefulness
of the fine-tuning of the pre-trained solution for this regime."
ANALYSIS OF THE CLASSIFICATION ERROR,0.0743455497382199,"â€¢ If ğœŒ= ğ‘œ(1), then the classification error of any linear classifier goes to 1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07486910994764398,2 as ğ‘›â†’âˆsince it
ANALYSIS OF THE CLASSIFICATION ERROR,0.07539267015706806,"is lower bounded by ğ‘„(
âˆšï¸ƒ"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07591623036649214,"ğœŒ
2 ) according to Lemma 1. Thus, any kind of learning will fail in
this regime."
ANALYSIS OF THE CLASSIFICATION ERROR,0.07643979057591622,"An interesting regime is where the number of samples is much lower than the number of parameters,
which naturally rises in the context of fine-tuning large models and corresponds to letting ğœ…â‰«1. For
this regime, we have ğ‘’ğ‘â‰ˆğ‘„ "
ANALYSIS OF THE CLASSIFICATION ERROR,0.07696335078534032,ğ‘„âˆ’1(ğ‘’ğ‘) + 1 2
ANALYSIS OF THE CLASSIFICATION ERROR,0.0774869109947644,"
ğœŒâˆ’1
ğ‘„âˆ’1(ğ‘’ğ‘) âˆ’3ğ‘„âˆ’1(ğ‘’ğ‘)
 1 ğœ… !"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07801047120418848,"We observe that if ğœŒ< 1, transfer learning always fails independent of value of ğ‘’ğ‘for ğœ…â‰«1 and"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07853403141361257,"training would not help with improving performance. On the other hand, if ğ‘’ğ‘> ğ‘„(
âˆšï¸ƒ ğœŒâˆ’1"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07905759162303665,"3 ) > ğ‘„(
âˆšï¸ƒ"
ANALYSIS OF THE CLASSIFICATION ERROR,0.07958115183246073,"ğœŒ
2 ),
for large enough ğœ…, transfer learning always achieves an error less than ğ‘’ğ‘."
NUMERICAL EXPERIMENTS,0.08010471204188482,"5
Numerical experiments"
REGRESSION,0.0806282722513089,"5.1
Regression"
REGRESSION,0.08115183246073299,"To corroborate our findings, we plotted the generalization error of the weight obtained through
running SGD according to the Assumptions 3 in Section 4.1 with respect to ğœ…= ğ‘‘"
REGRESSION,0.08167539267015707,"ğ‘›. To do so, we
fixed ğ‘‘= 1000 and varied ğ‘›across different values. We used CVXPY (Grant and Boyd [2014],
Agrawal et al. [2018]) to solve (1) efficiently on a Laptop CPU. To verify the universality of our
results, we initially constructed a centered random matrix Xâ€² with i.i.d components according
to the distributions N (0, 1), ğµğ‘’ğ‘Ÿ(0.5), and ğœ’2(1) and using a correlation matrix Rğ‘¥we defined
X := Xâ€²R1/2
ğ‘¥. On the other hand, we generated Rğ‘¥according to the following three distributions:
single level ğ‘(ğ‘Ÿ) := ğ›¿(ğ‘Ÿâˆ’1), bilevel ğ‘(ğ‘Ÿ) := 0.3ğ›¿(ğ‘Ÿâˆ’1) + 0.7ğ›¿(ğ‘Ÿâˆ’5) and uniform on the interval
[1, 5]. We specifically consider these cases as they are common in the literature and we use the
parameter ğœ2 for the component-wise variance of z in 6. Additionally, w0 is chosen according to
Assumptions 3 in such a manner that ğ‘’ğ‘= 1. In both Figures 1, 2 the blue line represents the
prediction 7 made by Theorem 2, the red line depicts the lower bound 8. The markers showcase the
performance of weights obtained under different distributions as described earlier. It can be seen that
from Figures 1, 2, for the bilevel and uniform distributions, depending on the value of ğœ, transfer
learning might not be beneficial as discussed in Remark 7. In particular, in Figures 1c and 2c, the
generalization error is always lower bounded by ğ‘’ğ‘= 1 and only in ğœ…â†’âˆcan get close to 1. Finally,
the single-level distribution on Rğ‘¥is always a lower bound for the generalization error of various
distributions on Rğ‘¥."
CLASSIFICATION,0.08219895287958115,"5.2
Classification"
CLASSIFICATION,0.08272251308900523,"Similar to the preceding subsection, we experimented with sampling the entries of X independently
from three different centered distributions: normal, Bernoulli, and ğœ’2. We also sampled the means
Âµ1 and Âµ2 from N (0, 1"
CLASSIFICATION,0.08324607329842931,"ğ‘‘Iğ‘‘) with a cross-correlation ğ‘Ÿ= E[Âµ1ğ‘–Âµ2ğ‘–] = 0.9 and added them to
the corresponding rows of X. For Figures 3, we fixed ğœŒ= 0.8, 2, 5 respectively and plotted the
classification error predicted by Corollary 2 as a solid red line, empirically observed classification
errors for the normal, Bernoulli and ğœ’2 entries as black squares, green circles and red triangles
respectively. The blue lines depict the classification error at the initialization. It can be seen that there
is a close match between the empirical errors between points from different distributions as well as
with the theoretical prediction, thus validating both Theorem 1 and Theorem 3. Note that fixing ğœŒ"
CLASSIFICATION,0.08376963350785341,in this setting corresponds to fixing ğœ2 as ğœŒ= ğ‘‘(1âˆ’ğ‘Ÿ)
CLASSIFICATION,0.08429319371727749,"ğœ2
. It is also worth mentioning that fine-tuning
improves performance in the setting of Figure 3c but hurts it for Figure 3a. Moreover, note that
in Figure 3b, although for smaller ğœ…transfer learning hurts, past a certain ğœ…, training improves the
performance. Also we observe that by increasing ğœŒacross the three plots, the classification task
becomes easier and fine-tuning improves performance as supported by Remark 8."
CLASSIFICATION,0.08481675392670157,"2
4
6
8
10 0 0.2 0.4 0.6 0.8 1 ğœ…"
CLASSIFICATION,0.08534031413612565,Generalization Error N
CLASSIFICATION,0.08586387434554973,"ğµ
ğœ’2
ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.08638743455497382,(a) ğœ= 0.01
CLASSIFICATION,0.08691099476439791,"2
4
6
8
10 0.4 0.6 0.8 1 ğœ…"
CLASSIFICATION,0.08743455497382199,Generalization Error N
CLASSIFICATION,0.08795811518324607,"ğµ
ğœ’2
ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.08848167539267016,(b) ğœ= 0.15
CLASSIFICATION,0.08900523560209424,"2
4
6
8
10 0 50 100 150 ğœ…"
CLASSIFICATION,0.08952879581151832,Generalization Error N
CLASSIFICATION,0.09005235602094241,"ğµ
ğœ’2(1)"
CLASSIFICATION,0.0905759162303665,"ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.09109947643979058,(c) ğœ= 2
CLASSIFICATION,0.09162303664921466,Figure 1: Generalization error for the bilevel distribution on the covariance of the data
CLASSIFICATION,0.09214659685863874,"2
4
6
8
10 0 0.2 0.4 0.6 0.8 1 ğœ…"
CLASSIFICATION,0.09267015706806282,Generalization Error N
CLASSIFICATION,0.09319371727748692,"ğµ
ğœ’2
ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.093717277486911,(a) ğœ= 0.01
CLASSIFICATION,0.09424083769633508,"2
4
6
8
10 0.4 0.6 0.8 ğœ…"
CLASSIFICATION,0.09476439790575916,Generalization Error N
CLASSIFICATION,0.09528795811518324,"ğµ
ğœ’2
ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.09581151832460733,(b) ğœ= 0.15
CLASSIFICATION,0.09633507853403141,"2
4
6
8
10 0 50 100 ğœ…"
CLASSIFICATION,0.0968586387434555,Generalization Error N
CLASSIFICATION,0.09738219895287958,"ğµ
ğœ’2
ğ‘’ğ‘
Lower Bound"
CLASSIFICATION,0.09790575916230367,(c) ğœ= 2
CLASSIFICATION,0.09842931937172775,Figure 2: Generalization error for the uniform distribution on the covariance of the data
CLASSIFICATION,0.09895287958115183,"0
5
10
15
20 0.3 0.35 0.4 0.45 ğœ…"
CLASSIFICATION,0.09947643979057591,Classification Error N
CLASSIFICATION,0.1,"ğµ
ğœ’2
ğ‘’ğ‘"
CLASSIFICATION,0.10052356020942409,"(a) ğœŒ= 0.8, ğ‘Ÿ= 0.9, ğ‘’ğ‘= 0.3"
CLASSIFICATION,0.10104712041884817,"0
5
10
15
20 0.3 0.32 0.34 0.36 0.38 ğœ…"
CLASSIFICATION,0.10157068062827225,"Classification Error ğ‘’ğ‘
N"
CLASSIFICATION,0.10209424083769633,"ğµ
ğœ’2
ğ‘’ğ‘"
CLASSIFICATION,0.10261780104712041,"(b) ğœŒ= 2, ğ‘Ÿ= 0.9, ğ‘’ğ‘= 0.3"
CLASSIFICATION,0.10314136125654451,"0
5
10
15
20 0.2 0.22 0.24 0.26 0.28 0.3 ğœ…"
CLASSIFICATION,0.10366492146596859,Classification Error N
CLASSIFICATION,0.10418848167539267,"ğµ
ğœ’2
ğ‘’ğ‘"
CLASSIFICATION,0.10471204188481675,"(c) ğœŒ= 5, ğ‘Ÿ= 0.9, ğ‘’ğ‘= 0.3"
CLASSIFICATION,0.10523560209424083,Figure 3: Classification error
CONCLUSION AND FUTURE WORK,0.10575916230366492,"6
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.10628272251308901,"We presented a novel Gaussian universality result and used it to study the problem of transfer learning
in linear models, for both regression and binary classification. In particular, we were able to precisely
relate the performance of the pretrained model to that of the fine-tuned model trained via SGD and,
as a result, identified situations where transfer learning helps and where it or does not. Possible future
directions include investigating other problems where the universality result may be useful, extending
the results to potential functions that are not necessarily convex nor separable, as well as exploring
the implications of universality for objectives with explicit regularization."
REFERENCES,0.1068062827225131,References
REFERENCES,0.10732984293193717,"E. Abbasi, F. Salehi, and B. Hassibi. Universality in learning from linear measurements. Advances in
Neural Information Processing Systems, 32, 2019."
REFERENCES,0.10785340314136126,"A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd. A rewriting system for convex optimization
problems. Journal of Control and Decision, 5(1):42â€“60, 2018."
REFERENCES,0.10837696335078534,"D. Akhtiamov, D. Bosch, R. Ghane, K. N. Varma, and B. Hassibi. A novel gaussian min-max theorem
and its applications. arXiv preprint arXiv:2402.07356, 2024."
REFERENCES,0.10890052356020942,"N. Azizan and B. Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit
regularization. In International Conference on Learning Representations, 2018."
REFERENCES,0.1094240837696335,"Z.-D. Bai and Y.-Q. Yin. Limit of the smallest eigenvalue of a large dimensional sample covariance
matrix. In Advances In Statistics, pages 108â€“127. World Scientific, 2008."
REFERENCES,0.1099476439790576,"S. G. Bobkov. On concentration of distributions of random weighted sums. Annals of probability,
pages 195â€“215, 2003."
REFERENCES,0.11047120418848168,"D. Bosch, A. Panahi, and B. Hassibi. Precise asymptotic analysis of deep random feature models. In
The Thirty Sixth Annual Conference on Learning Theory, pages 4132â€“4179. PMLR, 2023."
REFERENCES,0.11099476439790576,"S. Bozinovski. Reminder of the first paper on transfer learning in neural networks, 1976. Informatica,
44(3), 2020."
REFERENCES,0.11151832460732984,S. Chatterjee. A generalization of the lindeberg principle. 2006.
REFERENCES,0.11204188481675392,"W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In Proceedings of the 24th
international conference on Machine learning, pages 193â€“200, 2007."
REFERENCES,0.112565445026178,"Y. Dandi, L. Stephan, F. Krzakala, B. Loureiro, and L. ZdeborovÃ¡. Universality laws for gaussian
mixtures in generalized linear models. Advances in Neural Information Processing Systems, 36,
2024."
REFERENCES,0.1130890052356021,"Y. Dar, D. LeJeune, and R. G. Baraniuk. The common intuition to transfer learning can win or lose:
Case studies for linear regression. arXiv preprint arXiv:2103.05621, 2021."
REFERENCES,0.11361256544502618,"F. Gerace, L. Saglietti, S. S. Mannelli, A. Saxe, and L. ZdeborovÃ¡. Probing transfer learning with a
model of synthetic correlated datasets. Machine Learning: Science and Technology, 3(1):015030,
2022."
REFERENCES,0.11413612565445026,"M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1, Mar.
2014."
REFERENCES,0.11465968586387434,"S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of opti-
mization geometry. In J. Dy and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pages 1832â€“1841. PMLR, 10â€“15 Jul 2018. URL https://proceedings.mlr.press/
v80/gunasekar18a.html."
REFERENCES,0.11518324607329843,"Q. Han and Y. Shen. Universality of regularized regression estimators in high dimensions. The
Annals of Statistics, 51(4):1799â€“1823, 2023."
REFERENCES,0.11570680628272251,"T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least
squares interpolation. Annals of statistics, 50(2):949, 2022."
REFERENCES,0.1162303664921466,"H. Hu and Y. M. Lu. Universality laws for high-dimensional learning with random features. IEEE
Transactions on Information Theory, 69(3):1932â€“1964, 2022."
REFERENCES,0.11675392670157068,"A. Jain, A. Montanari, and E. Sasoglu. Scaling laws for learning with real and surrogate data. arXiv
preprint arXiv:2402.04376, 2024."
REFERENCES,0.11727748691099477,"S. Lahiry and P. Sur. Universality in block dependent linear models with applications to nonparametric
regression. arXiv preprint arXiv:2401.00344, 2023."
REFERENCES,0.11780104712041885,"F. Liese and K.-J. Miescke. Statistical decision theory. In Statistical Decision Theory: Estimation,
Testing, and Selection, pages 1â€“52. Springer, 2008."
REFERENCES,0.11832460732984293,"J. W. Lindeberg. Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung.
Mathematische Zeitschrift, 15(1):211â€“225, 1922."
REFERENCES,0.11884816753926701,"A. Montanari and P.-M. Nguyen. Universality of the elastic net error. In 2017 IEEE International
Symposium on Information Theory (ISIT), pages 2338â€“2342. IEEE, 2017."
REFERENCES,0.1193717277486911,"A. Montanari and B. N. Saeed. Universality of empirical risk minimization. In Conference on
Learning Theory, pages 4310â€“4312. PMLR, 2022."
REFERENCES,0.11989528795811519,"S. Oymak and J. A. Tropp. Universality laws for randomized dimension reduction, with applications.
Information and Inference: A Journal of the IMA, 7(3):337â€“446, 2018."
REFERENCES,0.12041884816753927,"A. Panahi and B. Hassibi. A universal analysis of large-scale regularized least squares solutions.
Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.12094240837696335,"D. SchrÃ¶der, H. Cui, D. Dmitriev, and B. Loureiro. Deterministic equivalent and error universality
of deep random features learning. In International Conference on Machine Learning, pages
30285â€“30320. PMLR, 2023."
REFERENCES,0.12146596858638743,"M. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet. Random matrix theory proves that deep
learning representations of gan-data behave as gaussian mixtures. In International Conference on
Machine Learning, pages 8573â€“8582. PMLR, 2020."
REFERENCES,0.12198952879581151,"C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu. A survey on deep transfer learning. In
Artificial Neural Networks and Machine Learningâ€“ICANN 2018: 27th International Conference
on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27, pages
270â€“279. Springer, 2018."
REFERENCES,0.1225130890052356,"R. Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.12303664921465969,"F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on
transfer learning. Proceedings of the IEEE, 109(1):43â€“76, 2020."
REFERENCES,0.12356020942408377,"A
Notations, Definitions and Assumptions"
REFERENCES,0.12408376963350785,"A.1
Notations and Definitions"
REFERENCES,0.12460732984293194,"We call a convex function ğ‘“: Rğ‘‘â†’R separable if ğ‘“(w) = Ãğ‘‘
ğ‘–=1 ğ‘“ğ‘–(ğ‘¤ğ‘–), where ğ‘“ğ‘–: R â†’R are
convex. Examples of such functions include âˆ¥Â· âˆ¥ğ‘
ğ‘for every ğ‘â‰¥1. A function ğ‘“: Rğ‘‘â†’R is
ğ‘€-strongly convex for ğ‘€> 0 if for every w âˆˆRğ‘‘, ğ‘“(w) âˆ’ğ‘€âˆ¥wâˆ¥2
2 is convex."
REFERENCES,0.12513089005235603,"Functions satisfying the following definition will be instrumental in the statements of our results:
Definition 1. We call a function ğ‘“: Rğ‘‘â†’R regular if it satisfies the following conditions"
REFERENCES,0.1256544502617801,1. ğ‘“is convex with ğ‘“(0) = ğ‘‚(ğ‘‘).
REFERENCES,0.1261780104712042,2. ğ‘“is three times differentiable.
REFERENCES,0.12670157068062826,"3. ğ‘“satisfies the following third-order condition for some ğ¶ğ‘“> 0: ğ‘‘
âˆ‘ï¸"
REFERENCES,0.12722513089005236,"ğ‘–, ğ‘—,ğ‘˜=1"
REFERENCES,0.12774869109947645,"ğœ•3 ğ‘“
ğœ•ğ‘¤ğ‘–ğœ•ğ‘¤ğ‘—ğœ•ğ‘¤ğ‘˜
ğ‘£ğ‘–ğ‘£ğ‘—ğ‘£ğ‘˜â‰¤ğ¶ğ‘“ ğ‘‘
âˆ‘ï¸"
REFERENCES,0.12827225130890052,"ğ‘–=1
|ğ‘£ğ‘–|3"
REFERENCES,0.12879581151832462,"Note that separable functions with bounded third order derivatives satisfy the third assumption of
Definition 1. For the description of the main results on regression and classification we recall several
probability theory concepts."
REFERENCES,0.12931937172774868,"Given a real-valued measure ğœ‡, its Stieltjes transform is defined as"
REFERENCES,0.12984293193717278,"ğ‘†ğœ‡(ğ‘§) :=
âˆ« R"
REFERENCES,0.13036649214659685,"1
ğ‘Ÿâˆ’ğ‘§ğœ‡(ğ‘‘ğ‘Ÿ),
ğ‘§âˆˆH+ âˆªR \ ğ‘†ğ‘¢ğ‘ğ‘(ğœ‡)
(12)"
REFERENCES,0.13089005235602094,"With H+ = {ğ‘§: ğ¼ğ‘š(ğ‘§) > 0}. Moreover, we denote convergence in probability and weak convergence"
REFERENCES,0.13141361256544504,"for measures by
Pâˆ’â†’and â‡respectively. For a random variable ğ‘‹, we sometimes use ğ‘‰ğ‘ğ‘Ÿ(ğ‘‹) for the
variance of ğ‘‹. For a convex differentiable function ğ‘”, the Bregman divergence is defined by"
REFERENCES,0.1319371727748691,"ğ·ğ‘”(w, w0) := ğ‘”(w) âˆ’ğ‘”(w0) âˆ’âˆ‡ğ‘”(w0)ğ‘‡(w âˆ’w0)"
REFERENCES,0.1324607329842932,"Next, we will provide a description of the design matrices investigated in this paper. It will be clear
soon that these assumptions are often satisfied in practice.
Definition 2.
2 We call a random matrix A âˆˆRğ‘›Ã—ğ‘‘block-regular if it satisfies the following
properties:"
REFERENCES,0.13298429319371727,"1. Ağ‘‡=  Ağ‘‡
1
Ağ‘‡
2
. . .
Ağ‘‡
ğ‘˜
 with ğ‘˜being finite and for each Ağ‘–âˆˆRğ‘›ğ‘–Ã—ğ‘‘, ğ‘›ğ‘–= Î˜(ğ‘›)"
REFERENCES,0.13350785340314136,"2. For 1 â‰¤ğ‘–â‰¤ğ‘˜, the rows of Ağ‘–are distributed independently and identically."
REFERENCES,0.13403141361256546,"3. EAğ‘–=: 1Âµğ‘‡
ğ‘–"
REFERENCES,0.13455497382198953,"4. Let a be any row of A and Âµ be its mean. Then for any deterministic vector v âˆˆRğ‘‘, and"
REFERENCES,0.13507853403141362,"ğ‘âˆˆN, ğ‘â‰¤6, there exists a constant ğ¾> 0 such that Ea|(a âˆ’Âµ)ğ‘‡v|ğ‘â‰¤ğ¾
âˆ¥vâˆ¥ğ‘
2
ğ‘‘ğ‘/2"
REFERENCES,0.1356020942408377,"5. For any deterministic matrix C âˆˆRğ‘‘Ã—ğ‘‘of bounded operator norm we have ğ‘‰ğ‘ğ‘Ÿ(ağ‘‡Ca) â†’0
as ğ‘‘â†’âˆ."
REFERENCES,0.13612565445026178,"6. Each âˆ¥Âµğ‘–âˆ¥2
2 = ğ‘‚(1)"
REFERENCES,0.13664921465968585,"Note that assumptions 1-3 from Definition 2 are satisfied for the design matrix both in the classification
and regression scenarios, and the assumption âˆ¥Âµğ‘–âˆ¥2
2 = ğ‘‚(1) is just a matter of normalization. While
assumptions 4 and 5 might appear obscure at first, any a âˆˆRğ‘‘satisfying the Lipschitz Concentration
Property (LCP) also satisfies the aforementioned assumptions. Recall that the LCP is defined as
follows."
REFERENCES,0.13717277486910995,"2We would like to point out that in this definition and throughout this work in general, mathematically
speaking, we are dealing with sequences of vectors and matrices with increasing dimensions. So, for example,
assumption 5 can be formally stated as follows: given any sequence of Cğ‘–âˆˆRğ‘‘ğ‘–Ã—ğ‘‘ğ‘–satisfying âˆ¥Cğ‘–âˆ¥ğ‘œğ‘â‰¤ğ¾for
some constant ğ¾> 0 and all ğ‘–> 0, we have that ğ‘‰ğ‘ğ‘Ÿ(ağ‘–Cğ‘–ağ‘–) â†’0 as ğ‘–â†’âˆ"
REFERENCES,0.13769633507853404,"Definition 3. We say that a distribution a âˆˆRğ‘‘satisfies LCP if the following inequality holds for
any Lipschitz function ğœ™: Rğ‘‘â†’R"
REFERENCES,0.1382198952879581,"P(|ğœ™(a) âˆ’Eğœ™(a)| > ğ‘¡) â‰¤2 exp (âˆ’
ğ‘‘ğ‘¡2"
REFERENCES,0.1387434554973822,"2âˆ¥ğœ™âˆ¥2
ğ¿ğ‘–ğ‘
) ."
REFERENCES,0.13926701570680627,"It is also worth mentioning that, as shown in Seddik et al. [2020], any data distribution from
which one can sample using a GAN satisfies Definition 3. Since it is known that many natural
datasets can be approximated well by GAN-generated data in practice, we thus find this assumption
realistic. These assumptions also bear significance for the instance-based approach to transfer
learning as utilizing GANs can remedy the dearth of data. What is more, these assumptions are not
limited to the subgaussians and include many other subexponential distributions, including ğœ’2 which
appears naturally in many signal processing applications such as Phase Retrieval. In order to tackle
optimizations such as 3, 4, we prove their equivalence to a problem with a suitable Gaussian design
G. For this purpose we will need the following definition:"
REFERENCES,0.13979057591623037,"Definition 4. We call a Gaussian matrix G matching a block-regular A if G is block-regular as well,
EGğ‘–= EAğ‘–= 1Âµğ‘‡
ğ‘–and for any row g of Gğ‘–and any row a of Ağ‘–, it holds that Eggğ‘‡= Eaağ‘‡."
REFERENCES,0.14031413612565444,"A.2
Assumptions for Theorem 2"
REFERENCES,0.14083769633507853,"Assumptions 3.
1. Rğ‘¥â‰»0 is diagonal without loss of generality."
REFERENCES,0.14136125654450263,"2. Let Ë†ğ‘ğ‘be the empirical spectral density of Rğ‘¥, then Ë†ğ‘ğ‘â‡ğ‘for some probability distribu-
tion ğ‘."
REFERENCES,0.1418848167539267,"3. w0 is a white perturbation of true wâˆ—in the Rğ‘¥basis. That is, w0 = wâˆ—+ Râˆ’1/2
ğ‘¥
Î¾ where
EÎ¾ = 0, EÎ¾Î¾ğ‘‡= ğ‘’ğ‘"
REFERENCES,0.1424083769633508,ğ‘‘I for a fixed ğ‘’ğ‘> 0.
REFERENCES,0.14293193717277486,"The third assumption from Assumptions 3 represents the fact that w0 is a perturbation of the ground
truth wâˆ—in the eigenbasis of ğ‘…ğ‘¥."
REFERENCES,0.14345549738219895,"A.3
Assumptions for Theorem 3"
REFERENCES,0.14397905759162305,"Assumptions 4.
1. The data matrix X satisfies parts (4)-(6) of Definition 2"
REFERENCES,0.14450261780104712,"2. The means Âµ1, Âµ2 are of norm 1 and for any deterministic matrix C of bounded operator
norm it holds that Âµğ‘‡
1 CÂµ1, Âµğ‘‡
2 CÂµ2 and Âµğ‘‡
1 CÂµ2 converge in probability to Tr(C)"
REFERENCES,0.1450261780104712,"ğ‘‘
, Tr(C)"
REFERENCES,0.14554973821989528,"ğ‘‘
and
ğ‘ŸTr(C)"
REFERENCES,0.14607329842931938,"ğ‘‘
respectively."
REFERENCES,0.14659685863874344,"3. ğšº1, ğšº2 are diagonal."
REFERENCES,0.14712041884816754,"4. Let Ë†ğ‘ğ‘be the joint empirical spectral density of ğšº1, ğšº2, then Ë†ğ‘ğ‘â‡ğ‘, where the distribu-
tion ğ‘is such that ğ‘(ğ‘ 1, ğ‘ 2) = ğ‘(ğ‘ 2, ğ‘ 1) holds for all ğ‘ 1, ğ‘ 2."
REFERENCES,0.14764397905759163,"For a discussion of the first assumption from Assumptions 4, see the paragraph after Definition 2.
The second assumption from Assumptions 4 is satisfied, for example, for any random vector of
the form ğšº1/2z, where ğšºis an arbitrary ğ‘ƒğ‘†ğ·matrix and z is i.i.d. The third assumption postulates
that the means are normalized generic ğ‘‘-dimensional vectors with an angle arccos ğ‘Ÿbetween them.
The fourth assumption says that ğšº1 and ğšº2 are simultaneously diagonalizable and, finally, the fifth
assumption simply introduces notation for the joint density function of the eigenvalues of ğšº1 and ğšº2."
REFERENCES,0.1481675392670157,"Lemma 1. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2
and Assumptions 4 hold. Then the optimal classifier is given by wâˆ—= (ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2) and its
classification error is equal to the following, where ğ‘„(Â·) is the integral of the tail of the standard
normal distribution. 1
2ğ‘„"
REFERENCES,0.1486910994764398,Tr(ğšº1 + ğšº2)âˆ’1âˆš
REFERENCES,0.14921465968586387,"1 âˆ’ğ‘Ÿ
âˆšï¸"
REFERENCES,0.14973821989528796,2Tr(ğšº1(ğšº1 + ğšº2)âˆ’2) ! + 1 2ğ‘„
REFERENCES,0.15026178010471203,Tr(ğšº1 + ğšº2)âˆ’1âˆš
REFERENCES,0.15078534031413612,"1 âˆ’ğ‘Ÿ
âˆšï¸"
REFERENCES,0.15130890052356022,2Tr(ğšº2(ğšº1 + ğšº2)âˆ’2) !
REFERENCES,0.1518324607329843,"B
Proof of Theorem 1"
REFERENCES,0.15235602094240838,In this section we provide the complete proof of Theorem 1.
REFERENCES,0.15287958115183245,"B.1
Proof of part 1 of Theorem 1 when Î¨ = Î¦ğœ†"
REFERENCES,0.15340314136125655,"We prove the universality of the objective value and then use it to prove the universality of a ğœ“applied
to the optimal solutions. To do so, we construct the perturbed objective. Let"
REFERENCES,0.15392670157068064,"Î¦ğœ†,ğœ–(A) := 1"
REFERENCES,0.1544502617801047,"ğ‘›min
ğ‘¤âˆˆSğ‘¤
ğœ†
2 âˆ¥Aw âˆ’yâˆ¥2 + ğ‘“(w) + ğœ–ğœ“(w)"
REFERENCES,0.1549738219895288,"Where |ğœ–| is small enough that ğœ–ğœ“(w) + ğ‘“(w) is ğœŒ-strongly convex. For such ğœ–â€™s, we will show that
Î¦ğœ†,ğœ–(A) and Î¦ğœ†,ğœ–(B) converge in probability to the same value and use that to deduce a similar
result involving ğœ“(wÎ¦ğœ†,0(A)), ğœ“(wÎ¦ğœ†,0(B)) . A key step is to reduce the proof to upper bounding the
difference of expectations, to do so, the following proposition will be instrumental whose proof is
given in Appendix D."
REFERENCES,0.15549738219895287,"Proposition 1. If for any Lipschitz function ğ‘”: R â†’R and ğ‘¡1 > 0, P

|ğ‘”(Î¦ğœ†,ğœ–(B)) âˆ’ğ‘| > ğ‘¡1

â†’0
and for every twice differentiable Ëœğ‘”: R â†’R with bounded second derivative, we have that
limğ‘›â†’âˆ
EA,y Ëœğ‘”(Î¦ğœ†,ğœ–(B)) âˆ’EB,y Ëœğ‘”(Î¦ğœ†,ğœ–(A))
 â†’0 then for any ğ‘¡2 > ğ‘¡1."
REFERENCES,0.15602094240837697,"lim
ğ‘›â†’âˆP

|ğ‘”(Î¦ğœ†,ğœ–(A)) âˆ’ğ‘| > ğ‘¡2

= 0"
REFERENCES,0.15654450261780103,"Thus it suffices to prove limğ‘›â†’âˆ
EA,yğ‘”(Î¦ğœ†(B)) âˆ’EB,yğ‘”(Î¦ğœ†(A))
 = 0 for every twice differentiable
ğ‘”with bounded second derivative. As stated earlier, we proceed by Lindebergâ€™s approach. Fixing ğœ†
and ğœ–, to simplify notation we use Î¦(A) for Î¦ğœ†,ğœ–(A). For 0 â‰¤ğ‘—â‰¤ğ‘›, let ğ‘âˆˆN be the largest such
that ğ‘—â‰¥Ãğ‘
ğ‘™=1 ğ‘›ğ‘™, and let ğ‘Ÿ= ğ‘—âˆ’Ãğ‘
ğ‘™=1 ğ‘›ğ‘™if , then Ë†A ğ‘—="
REFERENCES,0.15706806282722513,"(
a1,1
a1,2
. . .
ağ‘,ğ‘Ÿ
bğ‘,ğ‘Ÿ+1
. . .
bğ‘˜,ğ‘›ğ‘˜
ğ‘‡
ğ‘Ÿ< ğ‘›ğ‘+1

a1,1
a1,2
. . .
ağ‘,ğ‘Ÿ
bğ‘+1,1
. . .
bğ‘˜,ğ‘›ğ‘˜
ğ‘‡
ğ‘Ÿ= ğ‘›ğ‘+1"
REFERENCES,0.15759162303664923,It follwos that A = Ë†A0 and B = Ë†Ağ‘›. Then we have by a telescopic sum
REFERENCES,0.1581151832460733,"EA,yğ‘”(Î¦(A)) âˆ’EB,yğ‘”(Î¦(B))
 =
EA,B,y"
REFERENCES,0.1586387434554974,"ğ‘›âˆ’1
âˆ‘ï¸"
REFERENCES,0.15916230366492146,"ğ‘—=0
ğ‘”(Î¦( Ë†A ğ‘—)) âˆ’ğ‘”(Î¦( Ë†A ğ‘—âˆ’1)) â‰¤"
REFERENCES,0.15968586387434555,"ğ‘›âˆ’1
âˆ‘ï¸ ğ‘—=0"
REFERENCES,0.16020942408376965,"E Ë†A ğ‘—,yğ‘”(Î¦( Ë†A ğ‘—)) âˆ’E Ë†A ğ‘—âˆ’1,yğ‘”(Î¦( Ë†A ğ‘—âˆ’1))"
REFERENCES,0.16073298429319371,Now we define a new matrix by dropping the ğ‘—â€™th row. ËœAğ‘—=
REFERENCES,0.1612565445026178,"(
a1,1
a1,2
. . .
ağ‘,ğ‘Ÿâˆ’1
bğ‘,ğ‘Ÿ+1
. . .
bğ‘˜,ğ‘›ğ‘˜
ğ‘‡
ğ‘Ÿ< ğ‘›ğ‘+1

a1,1
a1,2
. . .
ağ‘,ğ‘Ÿâˆ’1
bğ‘+1,1
. . .
bğ‘˜,ğ‘›ğ‘˜
ğ‘‡
ğ‘Ÿ= ğ‘›ğ‘+1"
REFERENCES,0.16178010471204188,Note that
REFERENCES,0.16230366492146597,Î¦( Ë†Ağ‘—) = 1
REFERENCES,0.16282722513089004,"ğ‘›min
ğ‘¤âˆˆSğ‘¤
ğœ†
2 âˆ¥Ë†A ğ‘—w âˆ’yâˆ¥2 + ğ‘“(w) + ğœ–ğœ“(w) = 1"
REFERENCES,0.16335078534031414,"ğ‘›min
wâˆˆSğ‘¤
ğœ†
2 âˆ¥ËœA ğ‘—w âˆ’Ëœyâˆ¥2+ğœ†"
REFERENCES,0.16387434554973823,"2 (ağ‘‡
ğ‘—w âˆ’ğ‘¦ğ‘—)2 + ğ‘“(w) + ğœ–ğœ“(w) =: M(ağ‘—)"
REFERENCES,0.1643979057591623,Î¦( Ë†A ğ‘—âˆ’1) = 1
REFERENCES,0.1649214659685864,"ğ‘›min
ğ‘¤âˆˆSğ‘¤
ğœ†
2 âˆ¥Ë†A ğ‘—w âˆ’yâˆ¥2 + ğ‘“(w) + ğœ–ğœ“(w) = 1"
REFERENCES,0.16544502617801046,"ğ‘›min
wâˆˆSğ‘¤
ğœ†
2 âˆ¥ËœA ğ‘—w âˆ’Ëœyâˆ¥2+ğœ†"
REFERENCES,0.16596858638743456,"2 (bğ‘‡
ğ‘—w âˆ’ğ‘¦ğ‘—)2 + ğ‘“(w) + ğœ–ğœ“(w) =: M(b ğ‘—)"
REFERENCES,0.16649214659685863,"And we define M(a)
:=
minwâˆˆSğ‘¤ğ“‚(a, w).
So, we need to bound
E Ë†A ğ‘—,yğ‘”(Î¦( Ë†A ğ‘—)) âˆ’"
REFERENCES,0.16701570680628272,"E Ë†A ğ‘—âˆ’1,yğ‘”(Î¦( Ë†A ğ‘—âˆ’1))
, to do so, we condition on ËœA ğ‘—:
E Ë†A ğ‘—,yğ‘”(Î¦( Ë†A ğ‘—)) âˆ’E Ë†A ğ‘—âˆ’1,yğ‘”(Î¦( Ë†A ğ‘—âˆ’1))
 =
E ËœA ğ‘—,ËœyEa ğ‘—,bğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.16753926701570682,"
ğ‘”(M(ağ‘—)) âˆ’ğ‘”(M(b ğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.16806282722513088,Let us define the optimization whose terms are shared by both of M(ağ‘—) and M(bğ‘—)
REFERENCES,0.16858638743455498,Î˜ := 1
REFERENCES,0.16910994764397905,"ğ‘›min
wâˆˆSğ‘¤
ğœ†
2 âˆ¥ËœA ğ‘—w âˆ’Ëœyâˆ¥2 + ğ‘“(w) + ğœ–ğœ“(w)"
REFERENCES,0.16963350785340314,"Now note that since the second derivative of ğ‘”is bounded, we have
ğ‘”(Î˜) âˆ’ğ‘”(M(ağ‘—)) âˆ’ğ‘”â€²(Î˜)(M(ağ‘—) âˆ’Î˜)
 â‰¤âˆ¥ğ‘”â€²â€²âˆ¥âˆ(Î˜ âˆ’M(ağ‘—))2
ğ‘”(Î˜) âˆ’ğ‘”(M(bğ‘—)) âˆ’ğ‘”â€²(Î˜)(M(bğ‘—) âˆ’Î˜)
 â‰¤âˆ¥ğ‘”â€²â€²âˆ¥âˆ(Î˜ âˆ’M(bğ‘—))2
(13)"
REFERENCES,0.17015706806282724,"Adding and subtracting ğ‘”(Î˜) and taking expectation and moving inside, we have
E ËœA ğ‘—,ËœyEağ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.1706806282722513,"
ğ‘”(M(ağ‘—)) âˆ’ğ‘”(M(bğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.1712041884816754,"â‰¤
E ËœA ğ‘—,ËœyEağ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17172774869109947,"
ğ‘”(Î˜) âˆ’ğ‘”(M(ağ‘—)) âˆ’ğ‘”â€²(Î˜)(M(ağ‘—) âˆ’Î˜)
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.17225130890052356,"+
E ËœA ğ‘—,ËœyEa ğ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17277486910994763,"
ğ‘”(Î˜) âˆ’ğ‘”(M(bğ‘—)) âˆ’ğ‘”â€²(Î˜)(M(bğ‘—) âˆ’Î˜)
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.17329842931937173,"+
E ËœA ğ‘—,ËœyEa ğ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17382198952879582,"
ğ‘”â€²(Î˜)(M(ağ‘—) âˆ’M(bğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.1743455497382199,"Using 13, and by the independence of Î˜ and ağ‘—, bğ‘—, ğ‘¦ğ‘—, we have
E ËœA ğ‘—,ËœyEağ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17486910994764399,"
ğ‘”(M(ağ‘—)) âˆ’ğ‘”(M(bğ‘—))
 ËœA ğ‘—, Ëœy
 â‰¤
E ËœAğ‘—,Ëœyğ‘”â€²(Î˜)Ea ğ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17539267015706805,"
(M(ağ‘—) âˆ’M(b ğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.17591623036649215,"+ âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA ğ‘—,Ëœy,ağ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(ağ‘—))2 + âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA ğ‘—,Ëœy,bğ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(bğ‘—))2"
REFERENCES,0.17643979057591622,"â‰¤E ËœA ğ‘—,Ëœy|ğ‘”â€²(Î˜)|
Eağ‘—,b ğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.1769633507853403,"
(M(ağ‘—) âˆ’M(bğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.1774869109947644,"+ âˆ¥ğ‘”â€²â€²âˆ¥âˆ

E ËœA ğ‘—,Ëœy,ağ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(ağ‘—))2 + E ËœA ğ‘—,Ëœy,b ğ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(bğ‘—))2"
REFERENCES,0.17801047120418848,"â‰¤âˆ¥ğ‘”â€²âˆ¥âˆE ËœA ğ‘—,Ëœy"
REFERENCES,0.17853403141361257,"Ea ğ‘—,bğ‘—,ğ‘¦ğ‘—"
REFERENCES,0.17905759162303664,"
(M(ağ‘—) âˆ’M(bğ‘—))
 ËœA ğ‘—, Ëœy
"
REFERENCES,0.17958115183246073,"+ âˆ¥ğ‘”â€²â€²âˆ¥âˆ

E ËœA ğ‘—,Ëœy,ağ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(ağ‘—))2 + E ËœAğ‘—,Ëœy,bğ‘—,ğ‘¦ğ‘—(Î˜ âˆ’M(bğ‘—))2
(14)"
REFERENCES,0.18010471204188483,"Thus we focus on M(a ğ‘—), M(ğ‘ğ‘—) conditioned on ËœA ğ‘—. From now on, we drop the index ğ‘—from ËœA ğ‘—,
ağ‘—, bğ‘—, ğ‘¦ğ‘—. Intuitively, we show that by dropping (ağ‘‡w âˆ’ğ‘¦)2 and (bğ‘‡w âˆ’ğ‘¦)2 and approximating
ğ‘“, ğœ“by their second-order Taylor expansion, the objective values Î¦( Ë†A ğ‘—), Î¦( Ë†A ğ‘—âˆ’1) would not change
much. Which implies Î¦( Ë†A ğ‘—) and Î¦( Ë†A ğ‘—) are also close in value."
REFERENCES,0.1806282722513089,Let ğ‘¢:= arg min ğœ†
REFERENCES,0.181151832460733,"2 âˆ¥ËœAw âˆ’Ëœyâˆ¥2 + ğ‘“(w) + ğœ–ğœ“(w) and it is unique because of strong convexity. Now we
use the second order Taylor expansion of ğ‘“+ ğœ–ğœ“:"
REFERENCES,0.18167539267015706,ğ‘“(w) + ğœ–ğœ“(w) = ğ‘“(u) + ğœ–ğœ“(u) + âˆ‡( ğ‘“(u) + ğœ–ğœ“(u))ğ‘‡(w âˆ’u) + 1
REFERENCES,0.18219895287958116,2 (w âˆ’u)ğ‘‡âˆ‡2( ğ‘“(u) + ğœ–ğœ“(u))(w âˆ’u) + ğ‘…(w âˆ’u)
REFERENCES,0.18272251308900522,"Where limwâ†’u
ğ‘…(wâˆ’u)"
REFERENCES,0.18324607329842932,"âˆ¥wâˆ’uâˆ¥2 = 0. Let g := âˆ‡( ğ‘“(u) + ğœ–ğœ“(u)), H := âˆ‡2( ğ‘“(u) + ğœ–ğœ“(u)). Moreover, let"
REFERENCES,0.1837696335078534,"L(a) := min
wâˆˆSğ‘¤â„“(a, w) := 1"
REFERENCES,0.18429319371727748,"ğ‘›min
wâˆˆSğ‘¤
ğœ†
2 âˆ¥ËœAw âˆ’Ëœyâˆ¥2 + ğœ†"
REFERENCES,0.18481675392670158,2 (ağ‘‡w âˆ’ğ‘¦)2
REFERENCES,0.18534031413612564,+ ğ‘“(u) + ğœ–ğœ“(u) + gğ‘‡(w âˆ’u) + 1
REFERENCES,0.18586387434554974,2 (w âˆ’u)ğ‘‡H(w âˆ’u)
REFERENCES,0.18638743455497384,"Note that a and b match in distribution up to the second moment, which implies that"
REFERENCES,0.1869109947643979,"Ea,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)2 = Eb,ğ‘¦(bğ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.187434554973822,"Eaağ‘‡ğ›€âˆ’1a = Ebbğ‘‡ğ›€âˆ’1b
By triangle inequality we obtain"
REFERENCES,0.18795811518324607,"E ËœA,Ëœy"
REFERENCES,0.18848167539267016,"Ea,b,ğ‘¦"
REFERENCES,0.18900523560209423,"
(M(a) âˆ’M(b))
 ËœA, Ëœy
"
REFERENCES,0.18952879581151832,"â‰¤E ËœA,Ëœy Ea,ğ‘¦"
REFERENCES,0.19005235602094242,"
(M(a) âˆ’L(a))
 ËœA, Ëœy
 + E ËœA,Ëœy Eb,ğ‘¦"
REFERENCES,0.1905759162303665,"
(M(b) âˆ’L(b))
 ËœA, Ëœy
"
REFERENCES,0.19109947643979058,"+ E ËœA,Ëœy Ea,ğ‘¦"
REFERENCES,0.19162303664921465,"
L(a) âˆ’Î˜ âˆ’ğœ†"
REFERENCES,0.19214659685863875,"2ğ‘›
Ea,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.19267015706806281,1 + ğœ†Eaağ‘‡ğ›€âˆ’1a
REFERENCES,0.1931937172774869,"ËœA, Ëœy
 + E ËœA,Ëœy Eb,ğ‘¦"
REFERENCES,0.193717277486911,"
L(b) âˆ’Î˜ âˆ’ğœ†"
REFERENCES,0.19424083769633507,"2ğ‘›
Eb,ğ‘¦(bğ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.19476439790575917,1 + ğœ†Ebbğ‘‡ğ›€âˆ’1b
REFERENCES,0.19528795811518324,"ËœA, Ëœy
"
REFERENCES,0.19581151832460733,"â‰¤E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| + E ËœA,Ëœy,b,ğ‘¦|M(b) âˆ’L(b)|"
REFERENCES,0.19633507853403143,"+ E ËœA,Ëœy Ea,ğ‘¦"
REFERENCES,0.1968586387434555,"
L(a) âˆ’Î˜ âˆ’ğœ†"
REFERENCES,0.1973821989528796,"2ğ‘›
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.19790575916230366,1 + ğœ†Eaağ‘‡ğ›€âˆ’1a
REFERENCES,0.19842931937172775,"ËœA, Ëœy
 + E ËœA,Ëœy Eb,ğ‘¦"
REFERENCES,0.19895287958115182,"
L(b) âˆ’Î˜ âˆ’ğœ†"
REFERENCES,0.19947643979057592,"2ğ‘›
(bğ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.2,1 + ğœ†Ebbğ‘‡ğ›€âˆ’1b
REFERENCES,0.20052356020942408,"ËœA, Ëœy
"
REFERENCES,0.20104712041884817,"â‰¤E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| + E ËœA,Ëœy,b,ğ‘¦|M(b) âˆ’L(b)| + ğœ†"
REFERENCES,0.20157068062827224,"2ğ‘›E ËœA,Ëœy,a,ğ‘¦"
REFERENCES,0.20209424083769634,(ağ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.20261780104712043,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
âˆ’
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.2031413612565445,1 + ğœ†Eaağ‘‡ğ›€âˆ’1a + ğœ†
REFERENCES,0.2036649214659686,"2ğ‘›E ËœA,Ëœy,b,ğ‘¦"
REFERENCES,0.20418848167539266,(bğ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.20471204188481676,"1 + ğœ†bğ‘‡ğ›€âˆ’1b
âˆ’
(bğ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.20523560209424083,1 + ğœ†Ebbğ‘‡ğ›€âˆ’1b  (15)
REFERENCES,0.20575916230366492,"First we will provide an upper bound for the third and fourth terms in 15. As the function ğ‘¥â†¦â†’
1
1+ğ‘¥is
1-Lipschitz and using Cauchy Schwartz we arrive at"
REFERENCES,0.20628272251308902,"ğœ†
2ğ‘›E ËœA,Ëœy,a,ğ‘¦"
REFERENCES,0.20680628272251309,(ağ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.20732984293193718,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
âˆ’
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.20785340314136125,1 + ğœ†Eaağ‘‡ğ›€âˆ’1a â‰¤ğœ†2
REFERENCES,0.20837696335078534,"2ğ‘›E ËœA,Ëœy,a,ğ‘¦
(ağ‘‡u âˆ’ğ‘¦)2(ağ‘‡ğ›€âˆ’1a âˆ’Eaağ‘‡ğ›€âˆ’1a) (16) â‰¤ğœ†2 2ğ‘› âˆšï¸ƒ"
REFERENCES,0.2089005235602094,"E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)4E ËœA,Ëœy,a,ğ‘¦(ağ‘‡ğ›€âˆ’1a âˆ’Eaağ‘‡ğ›€âˆ’1a)2 (17)"
REFERENCES,0.2094240837696335,"By Lemma 7, there exists a constant ğ¶1 < âˆsuch that E ËœA,Ëœy,a,ğ‘¦(ağ‘‡uâˆ’ğ‘¦)4 < ğ¶1 for any ğ‘›. Furthermore,
by the assumptions, limğ‘›â†’âˆE ËœA,Ëœy,a,ğ‘¦(ağ‘‡ğ›€âˆ’1a âˆ’Eaağ‘‡ğ›€âˆ’1a)2 = 0."
REFERENCES,0.2099476439790576,"Now for the first and second terms in (15), using Lemma 5, we obtain with probability 1,"
REFERENCES,0.21047120418848167,|M(a) âˆ’L(a)| â‰¤8ğ¶ğ‘“+ğœ–ğœ“
REFERENCES,0.21099476439790577,"ğ‘›
âˆ¥u âˆ’wLâˆ¥3
31{âˆ¥u âˆ’wLâˆ¥3 â‰¤
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
} + ğœ†"
REFERENCES,0.21151832460732983,"2ğ‘›(ağ‘‡u âˆ’ğ‘¦)21{âˆ¥u âˆ’wLâˆ¥3 >
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
}"
REFERENCES,0.21204188481675393,Which implies
REFERENCES,0.21256544502617802,"E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| â‰¤8ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.2130890052356021,"ğ‘›
E ËœA,Ëœy,a,ğ‘¦"
REFERENCES,0.2136125654450262,"
âˆ¥u âˆ’wLâˆ¥3
31{âˆ¥u âˆ’wLâˆ¥3 â‰¤
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
}
 + ğœ†"
REFERENCES,0.21413612565445025,"2ğ‘›E ËœA,Ëœy,a,ğ‘¦"
REFERENCES,0.21465968586387435,"
(ağ‘‡u âˆ’ğ‘¦)21{âˆ¥u âˆ’wLâˆ¥3 >
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
}
"
REFERENCES,0.21518324607329842,We apply Cauchy-Schwarz to each term:
REFERENCES,0.2157068062827225,"E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| â‰¤8ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.2162303664921466,"ğ‘›
E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3 + ğœ† 2ğ‘› âˆšï¸‚"
REFERENCES,0.21675392670157068,"E ËœA,Ëœy,a,ğ‘¦12{âˆ¥u âˆ’wLâˆ¥3 >
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
}E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)4"
REFERENCES,0.21727748691099477,= 8ğ¶ğ‘“+ğœ–ğœ“
REFERENCES,0.21780104712041884,"ğ‘›
E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3 + ğœ† 2ğ‘› âˆšï¸‚"
REFERENCES,0.21832460732984293,"P ËœA,Ëœy,a,ğ‘¦(âˆ¥u âˆ’wLâˆ¥3 >
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“
)E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)4"
REFERENCES,0.218848167539267,"To deal with the terms involving P, we leverage Markovâ€™s inequality"
REFERENCES,0.2193717277486911,"E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| â‰¤8ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.2198952879581152,"ğ‘›
E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3 + ğœ†"
REFERENCES,0.22041884816753926,"2ğ‘›
 18ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.22094240837696336,"ğœŒ
3/2âˆšï¸ƒ"
REFERENCES,0.22146596858638742,"E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)4"
REFERENCES,0.22198952879581152,"By Lemma 7, there exists a ğ¶2 < âˆsuch that E ËœA,Ëœy,a,ğ‘¦âˆ¥uâˆ’wLâˆ¥3
3 â‰¤ğ¶2"
REFERENCES,0.22251308900523561,"ğ‘‘, and recall that E ËœA,Ëœy,a,ğ‘¦(ağ‘‡uâˆ’
ğ‘¦)4 < ğ¶1 for any ğ‘›. Thus"
REFERENCES,0.22303664921465968,"E ËœA,Ëœy,a,ğ‘¦|M(a) âˆ’L(a)| â‰¤8ğ¶2ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.22356020942408378,"ğ‘›ğ‘‘
+ ğœ†âˆšğ¶1 2ğ‘›
âˆš ğ‘‘"
REFERENCES,0.22408376963350785, 18ğ¶ğ‘“+ğœ–ğœ“
REFERENCES,0.22460732984293194,"ğœŒ
3/2 â‰¤ğ¶ğ‘ ğ‘›
âˆš"
REFERENCES,0.225130890052356,"ğ‘‘
(18)"
REFERENCES,0.2256544502617801,For some constant Ëœğ¶= ğœ†âˆšğ¶1
REFERENCES,0.2261780104712042,"2
  18ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.22670157068062827,"ğœŒ
3/2 + 8ğ¶2ğ¶ğ‘“+ğœ–ğœ“. Plugging (16) and (18) in (15), yields"
REFERENCES,0.22722513089005236,"E ËœA,Ëœy"
REFERENCES,0.22774869109947643,"Ea,b,ğ‘¦"
REFERENCES,0.22827225130890053,"
(M(a) âˆ’M(b))
 ËœA, Ëœy
 â‰¤
Ëœğ¶+ Ëœğ¶â€² ğ‘›
âˆš"
REFERENCES,0.22879581151832462,"ğ‘‘
+ ğœ†2 Ëœğ¶ 2ğ‘› âˆšï¸ƒ"
REFERENCES,0.2293193717277487,"E ËœA,Ëœyğ‘‰ğ‘ğ‘Ÿa(ağ‘‡ğ›€âˆ’1a) + ğœ†2 Ëœğ¶â€² 2ğ‘› âˆšï¸ƒ"
REFERENCES,0.22984293193717278,"E ËœA,Ëœyğ‘‰ğ‘ğ‘Ÿb(bğ‘‡ğ›€âˆ’1b)"
REFERENCES,0.23036649214659685,"For the other terms in (14), we have by Lemmas 4, 7:"
REFERENCES,0.23089005235602095,"âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA,Ëœy,a,ğ‘¦(Î˜ âˆ’M(a))2+âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA,Ëœy,b,ğ‘¦(Î˜ âˆ’M(b))2"
REFERENCES,0.23141361256544501,"â‰¤âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA,Ëœy,a,ğ‘¦
ğœ†2"
REFERENCES,0.2319371727748691,"4ğ‘›2 (ağ‘‡u âˆ’ğ‘¦)2 + âˆ¥ğ‘”â€²â€²âˆ¥âˆE ËœA,Ëœy,b,ğ‘¦
ğœ†2"
REFERENCES,0.2324607329842932,4ğ‘›2 (bğ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.23298429319371727,"â‰¤
ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶1 + Ëœğ¶â€²
1)"
REFERENCES,0.23350785340314137,"4ğ‘›2
Plugging in (14), we obtain
E ËœA,ËœyEa,b,ğ‘¦"
REFERENCES,0.23403141361256544,"
ğ‘”(M(a)) âˆ’ğ‘”(M(b))
 ËœA, Ëœy
"
REFERENCES,0.23455497382198953,â‰¤âˆ¥ğ‘”â€²âˆ¥âˆ
REFERENCES,0.2350785340314136," Ëœğ¶+ Ëœğ¶â€² ğ‘›
âˆš"
REFERENCES,0.2356020942408377,"ğ‘‘
+ ğœ†2 Ëœğ¶"
REFERENCES,0.2361256544502618,"2ğ‘›E ËœA,Ëœyğ‘‰ğ‘ğ‘Ÿa(ağ‘‡ğ›€âˆ’1a) + ğœ†2 Ëœğ¶â€²"
REFERENCES,0.23664921465968586,"2ğ‘›E ËœA,Ëœyğ‘‰ğ‘ğ‘Ÿb(bğ‘‡ğ›€âˆ’1b)

+ ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²) 4ğ‘›2"
REFERENCES,0.23717277486910995,"Therefore,
EA,yğ‘”(Î¦(A)) âˆ’EB,yğ‘”(Î¦(B))
 â‰¤"
REFERENCES,0.23769633507853402,"ğ‘›âˆ’1
âˆ‘ï¸ ğ‘—=0"
REFERENCES,0.23821989528795812,"E Ë†A ğ‘—,yğ‘”(Î¦( Ë†A ğ‘—)) âˆ’E Ë†A ğ‘—âˆ’1,yğ‘”(Î¦( Ë†A ğ‘—âˆ’1)) â‰¤"
REFERENCES,0.2387434554973822,"ğ‘›âˆ’1
âˆ‘ï¸"
REFERENCES,0.23926701570680628,"ğ‘—=0
âˆ¥ğ‘”â€²âˆ¥âˆ"
REFERENCES,0.23979057591623038," Ëœğ¶+ Ëœğ¶â€² ğ‘›
âˆš"
REFERENCES,0.24031413612565444,"ğ‘‘
+ ğœ†2 Ëœğ¶"
REFERENCES,0.24083769633507854,"2ğ‘›E ËœA ğ‘—,Ëœyğ‘—ğ‘‰ğ‘ğ‘Ÿa ğ‘—(ağ‘‡
ğ‘—ğ›€âˆ’1ağ‘—) + ğœ†2 Ëœğ¶â€²"
REFERENCES,0.2413612565445026,"2ğ‘›E ËœA ğ‘—,Ëœyğ‘—ğ‘‰ğ‘ğ‘Ÿb ğ‘—(bğ‘‡
ğ‘—ğ›€âˆ’1bğ‘—)

+ ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²) 4ğ‘›2"
REFERENCES,0.2418848167539267,"= âˆ¥ğ‘”â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²)
âˆš"
REFERENCES,0.2424083769633508,"ğ‘‘
+ ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²) 4ğ‘› +"
REFERENCES,0.24293193717277486,"ğ‘›âˆ’1
âˆ‘ï¸"
REFERENCES,0.24345549738219896,"ğ‘—=0
âˆ¥ğ‘”â€²âˆ¥âˆ"
REFERENCES,0.24397905759162303,ğœ†2 Ëœğ¶ 2ğ‘› âˆšï¸ƒ
REFERENCES,0.24450261780104712,"E ËœA ğ‘—,Ëœyğ‘—ğ‘‰ğ‘ğ‘Ÿağ‘—(ağ‘‡
ğ‘—ğ›€âˆ’1a ğ‘—) + ğœ†2 Ëœğ¶â€² 2ğ‘› âˆšï¸ƒ"
REFERENCES,0.2450261780104712,"E ËœA ğ‘—,Ëœyğ‘—ğ‘‰ğ‘ğ‘Ÿb ğ‘—(bğ‘‡
ğ‘—ğ›€âˆ’1bğ‘—)
"
REFERENCES,0.24554973821989529,Now note that since âˆ¥ğ›€âˆ¥ğ‘œğ‘â‰¤1
REFERENCES,0.24607329842931938,"ğœŒwith probability 1, limğ‘›â†’âˆğ‘‰ğ‘ğ‘Ÿa ğ‘—(ağ‘‡
ğ‘—ğ›€âˆ’1ağ‘—) = 0 from assumptions,
thus for every class [ ğ‘—], there exists a function ğœ[ ğ‘—](ğ‘›) that ğ‘‰ğ‘ğ‘Ÿağ‘—(ağ‘‡
ğ‘—ğ›€âˆ’1ağ‘—) â‰¤ğœ2
[ ğ‘—](ğ‘›) for every
ğ‘›âˆˆN and limğ‘›â†’âˆğœ[ ğ‘—](ğ‘›) = 0. Thus
EA,yğ‘”(Î¦(A)) âˆ’EB,yğ‘”(Î¦(B))
 â‰¤âˆ¥ğ‘”â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²)
âˆš"
REFERENCES,0.24659685863874345,"ğ‘‘
+ ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²)"
REFERENCES,0.24712041884816754,"4ğ‘›
+ ğœ†2âˆ¥ğ‘”â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²) ğ‘˜
âˆ‘ï¸ ğ‘–=1"
REFERENCES,0.2476439790575916,"ğ‘›ğ‘–
2ğ‘›ğœğ‘–(ğ‘›)"
REFERENCES,0.2481675392670157,"â‰¤âˆ¥ğ‘”â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²)
âˆš"
REFERENCES,0.2486910994764398,"ğ‘‘
+ ğœ†2âˆ¥ğ‘”â€²â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²)"
REFERENCES,0.24921465968586387,"4ğ‘›
+ ğœ†2âˆ¥ğ‘”â€²âˆ¥âˆ( Ëœğ¶+ Ëœğ¶â€²) max
1â‰¤ğ‘–â‰¤ğ‘˜
ğ‘›ğ‘–
2ğ‘›ğœğ‘–(ğ‘›)"
REFERENCES,0.24973821989528797,"Now since ğ‘˜is finite and ğ‘›ğ‘–= Î˜(ğ‘›) for every 1 â‰¤ğ‘–â‰¤ğ‘˜, we have that limğ‘›â†’âˆmax1â‰¤ğ‘–â‰¤ğ‘˜
ğ‘›ğ‘–
2ğ‘›ğœğ‘–(ğ‘›) = 0.
Thus for every ğœ†, ğœ–> 0"
REFERENCES,0.25026178010471206,"lim
ğ‘›â†’âˆ"
REFERENCES,0.25078534031413613,"EA,yğ‘”(Î¦ğœ†,ğœ–(A)) âˆ’EB,yğ‘”(Î¦ğœ†,ğœ–(B))
 = 0"
REFERENCES,0.2513089005235602,and the proof of part 1 of Theorem 1 is concluded.
REFERENCES,0.2518324607329843,"B.2
Proof of part 1 of Theorem 1 when Î¨ = Î¦"
REFERENCES,0.2523560209424084,"Now to prove the part 1 of Theorem 1, we will combine the following lemma with the previous
section. For the ease of notation, we drop the dependency on ğœ–. Recall that since ğ‘“is ğœŒ-strongly
convex, we can write"
REFERENCES,0.25287958115183246,"Î¦ğ‘›
ğœ†(ğ´) = min
w
ğœ†
2 âˆ¥Aw âˆ’yâˆ¥2
2 + ğœŒ"
REFERENCES,0.2534031413612565,"2 âˆ¥wâˆ¥2
2 + â„(w)"
REFERENCES,0.25392670157068065,For some convex â„: Rğ‘‘â†’R.
REFERENCES,0.2544502617801047,"Lemma 2. If for every ğœ†> 0, we have Î¦ğ‘›
ğœ†(ğ´)
Pâˆ’â†’ğ‘ğœ†, then for every Lipschitz function ğ‘”: R â†’R,"
REFERENCES,0.2549738219895288,"we have ğ‘”(supğœ†>0 Î¦ğ‘›
ğœ†(ğ´))
Pâˆ’â†’ğ‘”(supğœ†>0 ğ‘ğœ†) and ğ‘”(supğœ†>0 Î¦ğ‘›
ğœ†(ğµ))
Pâˆ’â†’ğ‘”(supğœ†>0 ğ‘ğœ†)"
REFERENCES,0.2554973821989529,"Proof. First note that for a given ğ‘¡> 0, we have for every Lipschitz function ğ‘”"
REFERENCES,0.256020942408377,"P(|ğ‘”(Î¦ğ‘›
ğœ†) âˆ’ğ‘”(ğ‘ğœ†)| > ğ‘¡) â‰¤P(|Î¦ğ‘›
ğœ†âˆ’ğ‘ğœ†| > ğ‘¡ ğ¿)"
REFERENCES,0.25654450261780104,Letting ğ›¿:= ğ‘¡
REFERENCES,0.2570680628272251,"ğ¿, Ëœğ›¿1 > 0 , we know that for every ğœ†â‰¥0, there exists ğ‘1 âˆˆN such that for every ğ‘›â‰¥ğ‘1"
REFERENCES,0.25759162303664923,"P(|Î¦ğ‘›
ğœ†âˆ’ğ‘ğœ†| > ğ›¿) < Ëœğ›¿1"
REFERENCES,0.2581151832460733,"We will show that an ğ‘…1 > 0 can be chosen in such a way that there exists ğ‘2 âˆˆN such that for every
ğ‘›â‰¥ğ‘2"
REFERENCES,0.25863874345549737,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) â‰¤Ëœğ›¿2"
REFERENCES,0.2591623036649215,"To see this, by the monotonicity of Î¦ğ‘›
ğœ†in ğœ†, we have by the fundamental theorem of calculus for a
given ğ‘…1"
REFERENCES,0.25968586387434556,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) = P( lim
ğœ†â†’âˆÎ¦ğ‘›
ğœ†âˆ’Î¦ğ‘›
ğ‘…1 > ğ›¿) = P
âˆ«âˆ ğ‘…1"
REFERENCES,0.2602094240837696,"ğœ•Î¦ğ‘›
ğœ†
ğœ•ğœ†ğ‘‘ğ‘ > ğ›¿
"
REFERENCES,0.2607329842931937,"By Danskinâ€™s theorem,"
REFERENCES,0.2612565445026178,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) = P
1 ğ‘› âˆ«âˆ"
REFERENCES,0.2617801047120419,"ğ‘…1
âˆ¥Aw(ğ‘ ) âˆ’yâˆ¥2ğ‘‘ğ‘ > ğ›¿

(19)"
REFERENCES,0.26230366492146595,"Now for the gradient of the main optimization we have, by ğ‘”:= âˆ‡â„(w)"
REFERENCES,0.2628272251308901,ğœ†Ağ‘‡(Aw âˆ’y) + ğœŒw + g = 0
REFERENCES,0.26335078534031414,"Which implies w = (ğœ†Ağ‘‡A + ğœŒI)âˆ’1(ğœ†Ağ‘‡y âˆ’g). Hence by matrix inversion lemma,"
REFERENCES,0.2638743455497382,Aw âˆ’y = âˆ’(I âˆ’A(ğœ†Ağ‘‡A + ğœŒI)âˆ’1ğœ†Ağ‘‡)y âˆ’A(ğœ†Ağ‘‡A + ğœŒI)âˆ’1g
REFERENCES,0.2643979057591623,= âˆ’ğœŒ(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1y âˆ’A(ğœ†Ağ‘‡A + ğœŒI)âˆ’1g = âˆ’ğœŒ(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1y âˆ’(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1Ag
REFERENCES,0.2649214659685864,By triangle inequality and definition of the operator norm we have
REFERENCES,0.26544502617801047,"âˆ¥Aw + yâˆ¥2
2 â‰¤4ğœŒ2âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘âˆ¥yâˆ¥2
2 + 4âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘âˆ¥gâˆ¥2
2
Whence plugging back in 19 yields"
REFERENCES,0.26596858638743454,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) â‰¤P
4 ğ‘› âˆ«âˆ"
REFERENCES,0.26649214659685866,"ğ‘…1
ğœŒ2âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘âˆ¥yâˆ¥2
2 + âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘âˆ¥g(ğ‘ )âˆ¥2
2ğ‘‘ğ‘ > ğ›¿
"
REFERENCES,0.2670157068062827,"â‰¤P
4ğœŒ2âˆ¥yâˆ¥2
2
ğ‘› âˆ«âˆ"
REFERENCES,0.2675392670157068,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘> ğ›¿ 2 "
REFERENCES,0.2680628272251309,"+ P
4 ğ‘› âˆ«âˆ"
REFERENCES,0.268586387434555,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘âˆ¥g(ğ‘ )âˆ¥2
2ğ‘‘ğ‘ > ğ›¿ 2 "
REFERENCES,0.26910994764397905,"For the norm of g, by assumption, there exists a ğ¶ğ‘”independent of ğœ†, such that
âˆ¥g(ğ‘ ) âˆ¥2
2
ğ‘›
â‰¤ğ¶ğ‘”with
high probability."
REFERENCES,0.2696335078534031,Consider the following the events
REFERENCES,0.27015706806282724,"Tğ‘¦:=
n âˆ¥yâˆ¥2
2
ğ‘›
â‰¤ğ¶ğ‘¦
o"
REFERENCES,0.2706806282722513,"Tğ‘”:=
n âˆ¥g(ğ‘ )âˆ¥2
2
ğ‘›
â‰¤ğ¶ğ‘”
o"
REFERENCES,0.2712041884816754,"Tğ´:=
n
ğ‘ min(AAğ‘‡) â‰¥ğ¶ğ´
o"
REFERENCES,0.2717277486910995,We further have
REFERENCES,0.27225130890052357,"P
4ğœŒ2âˆ¥yâˆ¥2
2
ğ‘› âˆ«âˆ"
REFERENCES,0.27277486910994764,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘ğ‘‘ğ‘ > ğ›¿ 2"
REFERENCES,0.2732984293193717,"
â‰¤P(T ğ‘
ğ‘¦) + P

4ğœŒ2ğ¶ğ‘¦ âˆ«âˆ"
REFERENCES,0.27382198952879583,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘ğ‘‘ğ‘ > ğ›¿ 2  P
4 ğ‘› âˆ«âˆ"
REFERENCES,0.2743455497382199,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘âˆ¥g(ğ‘ )âˆ¥2
2ğ‘‘ğ‘ > ğ›¿ 2"
REFERENCES,0.27486910994764396,"
â‰¤P(T ğ‘
ğ‘”) + P

4ğ¶ğ‘” âˆ«âˆ"
REFERENCES,0.2753926701570681,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘ğ‘‘ğ‘ > ğ›¿ 2  (20)"
REFERENCES,0.27591623036649215,"Thus we need to focus on the operator norm of each term in 20. After diagonalizing and using the
fact that for any two rectangular matrices X, Y with matching dimensions, ğ‘ ğ‘ğ‘’ğ‘(XY) = ğ‘ ğ‘ğ‘’ğ‘(YX)"
REFERENCES,0.2764397905759162,"âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘= max
1â‰¤ğ‘–â‰¤ğ‘›
1
(ğœ†ğ‘ ğ‘–(AAğ‘‡) + ğœŒ)2 =
1
(ğœ†ğ‘ min(AAğ‘‡) + ğœŒ)2"
REFERENCES,0.2769633507853403,"âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘= max
1â‰¤ğ‘–â‰¤ğ‘›
ğ‘ ğ‘–(AAğ‘‡)
(ğœ†ğ‘ ğ‘–(AAğ‘‡) + ğœŒ)2"
REFERENCES,0.2774869109947644,"Now by the event Tğ´, ğ‘ min(AAğ‘‡) â‰¥ğ¶ğ´, then for a large enough ğ‘…1 such that ğœŒ"
REFERENCES,0.2780104712041885,"ğ‘…1 â‰¤ğ¶ğ´, then for every
ğœ†â‰¥ğ‘…1"
REFERENCES,0.27853403141361255,"âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘â‰¤
1
(ğœ†ğ¶ğ´+ ğœŒ)2"
REFERENCES,0.27905759162303667,"âˆ¥(ğœ†AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘â‰¤
ğ¶ğ´
(ğœ†ğ¶ğ´+ ğœŒ)2"
REFERENCES,0.27958115183246074,"Bounding the integrals implies
âˆ«âˆ"
REFERENCES,0.2801047120418848,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1âˆ¥2
ğ‘œğ‘ğ‘‘ğ‘ â‰¤1 ğ¶ğ´"
REFERENCES,0.2806282722513089,"1
ğ¶ğ´ğ‘…1 + ğœŒ
âˆ«âˆ"
REFERENCES,0.281151832460733,"ğ‘…1
âˆ¥(ğ‘ AAğ‘‡+ ğœŒğ¼)âˆ’1Aâˆ¥2
ğ‘œğ‘ğ‘‘ğ‘ â‰¤
1
ğ¶ğ´ğ‘…1 + ğœŒ"
REFERENCES,0.28167539267015707,"Summarizing,"
REFERENCES,0.28219895287958113,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) â‰¤P

4ğœŒ2ğ¶ğ‘¦
ğ¶ğ´(ğ¶ğ´ğ‘…1 + ğœŒ) > ğ›¿ 2"
REFERENCES,0.28272251308900526,"
+ P

ğ¶ğ‘”
ğ¶ğ´ğ‘…1 + ğœŒ> ğ›¿ 2"
REFERENCES,0.2832460732984293,"
+ P(T ğ‘
ğ‘¦) + P(T ğ‘
ğ‘”) + 2P(T ğ‘
ğ´) (21)"
REFERENCES,0.2837696335078534,"Now we choose ğ‘…1 to be large enough such that the first two terms vanish. Therefore there exists an
ğ‘2 âˆˆN such that for every ğ‘›â‰¥ğ‘2, P(T ğ‘
ğ‘¦) + P(T ğ‘
ğ‘”) + 2P(T ğ‘
ğ´) â‰¤Ëœğ›¿2. Thus for such ğ‘…1 and ğ‘2, we
have"
REFERENCES,0.28429319371727746,"P(sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’
sup
0<ğœ†<ğ‘…1
Î¦ğ‘›
ğœ†> ğ›¿) â‰¤Ëœğ›¿2"
REFERENCES,0.2848167539267016,Now we can let ğ‘…2 > 0 be chosen in such a way that:
REFERENCES,0.28534031413612565,"sup
ğœ†>0
ğ‘ğœ†âˆ’
sup
0<ğœ†<ğ‘…2
ğ‘ğœ†< Ëœğ›¿"
REFERENCES,0.2858638743455497,"Take the maximum of ğ‘…1 and ğ‘…2 as ğ‘…. By triangle inequality we observe
P(| sup
ğœ†>0
Î¦ğ‘›
ğœ†âˆ’sup
ğœ†>0
ğ‘ğœ†| > ğ›¿) â‰¤P(| sup
0<ğœ†<ğ‘…
Î¦ğ‘›
ğœ†âˆ’sup
ğœ†>0
ğ‘ğœ†| > ğ›¿) + P(sup
0<ğœ†
Î¦ğ‘›
ğœ†âˆ’sup
0<ğœ†<ğ‘…
Î¦ğ‘›
ğœ†> ğ›¿)"
REFERENCES,0.28638743455497384,"â‰¤P(| sup
0<ğœ†<ğ‘…
Î¦ğ‘›
ğœ†âˆ’sup
0<ğœ†<ğ‘…
ğ‘ğœ†| > ğ›¿) + P(sup
ğœ†>0
ğ‘ğœ†âˆ’sup
0<ğœ†<ğ‘…
ğ‘ğœ†> ğ›¿) + P(sup
0<ğœ†
Î¦ğ‘›
ğœ†âˆ’sup
0<ğœ†<ğ‘…
Î¦ğ‘›
ğœ†> ğ›¿)"
REFERENCES,0.2869109947643979,"â‰¤P(| sup
0<ğœ†<ğ‘…
Î¦ğ‘›
ğœ†âˆ’sup
0<ğœ†<ğ‘…
ğ‘ğœ†| > ğ›¿) + 0 + Ëœğ›¿1"
REFERENCES,0.287434554973822,"â‰¤Ëœğ›¿1 + P( sup
0<ğœ†<ğ‘…
|Î¦ğ‘›
ğœ†âˆ’ğ‘ğœ†| > ğ›¿)"
REFERENCES,0.2879581151832461,"â‰¤Ëœğ›¿1 + P( sup
0â‰¤ğœ†â‰¤ğ‘…
|Î¦ğ‘›
ğœ†âˆ’ğ‘ğœ†| > ğ›¿)"
REFERENCES,0.28848167539267017,"First we prove ğ‘ğœ†is concave in ğœ†. Then by appealing to the Convexity Lemma (lemma 7.75 in
Liese and Miescke [2008]), the result follows. Note that Î¦ğ‘›
ğœ†is concave in ğœ†as it is the pointwise-
minimum of a concave function in ğœ†everywhere. Now to prove the concavity of ğ‘ğœ†, for a given pair of
ğœ†1, ğœ†2 âˆˆ[0, ğ‘…] and 0 < ğœƒ< 1, we prove the deterministic event {ğ‘ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 âˆ’ğœƒğ‘ğœ†1 âˆ’(1âˆ’ğœƒ)ğ‘ğœ†2 < 0}
has probability zero. First note that
Î¦ğ‘›
ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 âˆ’ğœƒÎ¦ğ‘›
ğœ†1 âˆ’(1 âˆ’ğœƒ)Î¦ğ‘›
ğœ†2 â‰¥0"
REFERENCES,0.28900523560209423,Thus by union bound for ğœ–> 0
REFERENCES,0.2895287958115183,"P

ğœƒğ‘ğœ†1+(1 âˆ’ğœƒ)ğ‘ğœ†2 âˆ’ğ‘ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 > ğœ–
"
REFERENCES,0.2900523560209424,"â‰¤P

ğœƒ(ğ‘ğœ†1 âˆ’Î¦ğ‘›
ğœ†1) + (1 âˆ’ğœƒ)(ğ‘ğœ†2 âˆ’Î¦ğ‘›
ğœ†2) + Î¦ğ‘›
ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 âˆ’ğ‘ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 > ğœ–
"
REFERENCES,0.2905759162303665,"â‰¤P

ğœƒ|ğ‘ğœ†1 âˆ’Î¦ğ‘›
ğœ†1| + (1 âˆ’ğœƒ)|ğ‘ğœ†2 âˆ’Î¦ğ‘›
ğœ†2| + |Î¦ğ‘›
ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 âˆ’ğ‘ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2| > ğœ–
"
REFERENCES,0.29109947643979056,"â‰¤P

ğœƒ|ğ‘ğœ†1 âˆ’Î¦ğ‘›
ğœ†1| > ğœ– 3"
REFERENCES,0.2916230366492147,"
+ P

(1 âˆ’ğœƒ)|ğ‘ğœ†2 âˆ’Î¦ğ‘›
ğœ†2| > ğœ– 3"
REFERENCES,0.29214659685863875,"
+ P

|Î¦ğ‘›
ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2 âˆ’ğ‘ğœƒğœ†1+(1âˆ’ğœƒ)ğœ†2| > ğœ– 3 "
REFERENCES,0.2926701570680628,"Now by taking ğ‘›to be large enough, the RHS can be made arbitrarily small for every ğœ–and ğœƒ, thus ğ‘ğœ†
is concave. Furthermore, since [0, ğ‘…] is a compact set, we can appeal to the convexity lemma 7.75 in
Liese and Miescke [2008] and the first claim follows. For the second claim, note that so far we have
proved"
REFERENCES,0.2931937172774869,"Î¦ğœ†(ğ´)
Pâˆ’â†’ğ‘ğœ†=â‡’sup
ğœ†>0
Î¦ğœ†(ğ´)
Pâˆ’â†’sup
ğœ†>0
ğ‘ğœ†"
REFERENCES,0.293717277486911,"Then since |Î¦ğœ†(ğµ) âˆ’Î¦ğœ†(ğ´)|
Pâˆ’â†’0 thus Î¦ğœ†(ğµ)
Pâˆ’â†’ğ‘ğœ†, repeating the same argument this time for
Î¦ğœ†(ğµ) yields"
REFERENCES,0.2942408376963351,"sup
ğœ†>0
Î¦ğœ†(ğµ)
Pâˆ’â†’sup
ğœ†>0
ğ‘ğœ† â–¡"
REFERENCES,0.29476439790575915,"B.2.1
Sequence of Regularizers"
REFERENCES,0.29528795811518327,"We prove that by constraining ourselves to a large enough compact set Sw, if ğ‘“ğ‘šâ†’ğ‘“uniformly,
then the universality results also hold. Note for the Î¦ğœ†case, we have that by setting w = 0 in the
optimization, we have âˆ¥wÎ¦ğœ†(A) âˆ¥2 â‰¤ğ¶ğ‘¤
âˆšğ‘›for some ğ¶ğ‘¤> 0 with high probability, thus we can
instead set Sw := ğ¶ğ‘¤
âˆšğ‘›and consider the following equivalent constrained optimization problem"
REFERENCES,0.29581151832460734,"ËœÎ¦ğ‘š
ğœ†,ğœ–(A) := 1"
REFERENCES,0.2963350785340314,"ğ‘›min
wâˆˆSw
ğœ†
2 âˆ¥Aw âˆ’yâˆ¥2
2 + ğ‘“ğ‘š(w) + ğœ–ğœ“(w)"
REFERENCES,0.29685863874345547,"Now consider a sequence of regular functions ğ‘“ğ‘š, converging uniformly to ğ‘“on Sw. Take ğ‘šlarge
enough such that âˆ¥ğ‘“âˆ’ğ‘“ğ‘šâˆ¥âˆâ‰¤ğ‘›ğ›¿for any ğ‘›which implies | ËœÎ¦ğ‘š
ğœ†,ğœ–(A) âˆ’ËœÎ¦ğœ†,ğœ–(A)| â‰¤ğ›¿. Thus for any
ğ‘¡> 0"
REFERENCES,0.2973821989528796,"P
 ËœÎ¦ğœ†,ğœ–(A) âˆ’ğ‘ğœ†,ğœ–
 > ğ‘¡

â‰¤P
 ËœÎ¦ğœ†,ğœ–(A) âˆ’ËœÎ¦ğ‘š
ğœ†,ğœ–(A)
 > ğ‘¡ 2"
REFERENCES,0.29790575916230366,"
+ P
 ËœÎ¦ğ‘š
ğœ†,ğœ–(A) âˆ’ğ‘ğ‘š
ğœ†,ğœ–
 > ğ‘¡ 2"
REFERENCES,0.29842931937172773,"
(22)"
REFERENCES,0.29895287958115185,"If ËœÎ¦ğ‘š
ğœ†,ğœ–(A)
Pâˆ’â†’ğ‘ğ‘š
ğœ†,ğœ–, then by (22), we have ËœÎ¦ğœ†,ğœ–(A)
Pâˆ’â†’ğ‘ğœ†,ğœ–. A similar argument holds for"
REFERENCES,0.2994764397905759,"ËœÎ¦ğ‘š
ğœ–(A) := 1"
REFERENCES,0.3,"ğ‘›
min
Aw=y,wâˆˆSw ğ‘“ğ‘š(w) + ğœ–ğœ“(w)"
REFERENCES,0.30052356020942406,"Furthermore, subsequent results also hold for ğœ“(wÎ¦ğ‘š
ğœ†(A)) and ğœ“(wÎ¦ğœ†(A)), ğœ“(wÎ¦ğ‘š(A)) and ğœ“(wÎ¦(A))"
REFERENCES,0.3010471204188482,"B.3
Proof of Part 2 of Theorem 1"
REFERENCES,0.30157068062827225,"B.3.1
Proof of Part 2 of Theorem 1 for a regular test function when Î¨ = Î¦ğœ†"
REFERENCES,0.3020942408376963,"Now we prove the second part of Theorem 1. Since we assumed Î¦ğœ†,0(A) converges to some ğ‘ğœ†,0 in"
REFERENCES,0.30261780104712044,"probability, then for a small enough ğœ–, we also have Î¦ğœ†,ğœ–(A)
Pâˆ’â†’ğ‘ğœ†,ğœ–. Therefore we may write by an
application of triangle inequality and the union bound"
REFERENCES,0.3031413612565445,"P

Î¦ğœ†,ğœ–(A) âˆ’Î¦ğœ†,0(A)"
REFERENCES,0.3036649214659686,"ğœ–
âˆ’ğ‘ğœ†,ğœ–âˆ’ğ‘ğœ†,0 ğœ–"
REFERENCES,0.3041884816753927,"> ğ‘¡

â‰¤P
Î¦ğœ†,ğœ–(A) âˆ’ğ‘ğœ†,ğœ–
 > ğœ–ğ‘¡ 2"
REFERENCES,0.30471204188481676,"
+ P
Î¦ğœ†,0(A) âˆ’ğ‘ğœ†,0
 > ğœ–ğ‘¡ 2  (23)"
REFERENCES,0.30523560209424083,"For
every
ğœ–, ğ‘¡
>
0,
the
RHS
of
(23)
goes
to
zero.
Similarly
we
have"
REFERENCES,0.3057591623036649,"P

Î¦ğœ†,0(A)âˆ’Î¦ğœ†,âˆ’ğœ–(A)"
REFERENCES,0.306282722513089,"ğœ–
âˆ’ğ‘ğœ†,0âˆ’ğ‘ğœ†,âˆ’ğœ– ğœ–"
REFERENCES,0.3068062827225131,"> ğ‘¡

â†’0."
REFERENCES,0.30732984293193716,Now we observe that by triangle inequality
REFERENCES,0.3078534031413613,"P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.30837696335078535,"> ğ‘¡

â‰¤P

ğœ“(wÎ¦ğœ†(A)) > ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.3089005235602094,"ğœ–=0+ğ‘¡

+ P

ğœ“(wÎ¦ğœ†(A)) < ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.3094240837696335,"ğœ–=0âˆ’ğ‘¡
"
REFERENCES,0.3099476439790576,"We take Ë†ğœ–to be small enough such that

ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.3104712041884817,"ğœ–=0âˆ’ğ‘ğœ†, Ë†ğœ–âˆ’ğ‘ğœ†,0 Ë†ğœ– < ğ‘¡"
REFERENCES,0.31099476439790574,"2,

ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.31151832460732987,"ğœ–=0âˆ’ğ‘ğœ†,0 âˆ’ğ‘ğœ†,âˆ’Ë†ğœ– Ë†ğœ– < ğ‘¡ 2"
REFERENCES,0.31204188481675393,This implies
REFERENCES,0.312565445026178,"P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.31308900523560207,"> ğ‘¡

â‰¤P

ğœ“(wÎ¦ğœ†(A)) > ğ‘ğœ†,0 âˆ’ğ‘ğœ†,âˆ’Ë†ğœ–"
REFERENCES,0.3136125654450262,"Ë†ğœ–
+ ğ‘¡ 2"
REFERENCES,0.31413612565445026,"
+ P

ğœ“(wÎ¦ğœ†(A)) < ğ‘ğœ†, Ë†ğœ–âˆ’ğ‘ğœ†,0 Ë†ğœ–
âˆ’ğ‘¡ 2 "
REFERENCES,0.3146596858638743,"And by Danskinâ€™s theorem, Î¦ğœ†,ğœ–(A) is minimum of a concave function ğœ–, therefore it is also concave
and it is differentiable with respect to ğœ–, we observe that ğœ“(ğ‘¤Î¦ğœ†(A)) = ğ‘‘Î¦ğœ†,ğœ–(A) ğ‘‘ğœ–"
REFERENCES,0.31518324607329845,"ğœ–=0. Moreover, by
concavity over Ë†ğœ–:"
REFERENCES,0.3157068062827225,"Î¦ğœ†, Ë†ğœ–(A) âˆ’Î¦ğœ†,0(A)"
REFERENCES,0.3162303664921466,"Ë†ğœ–
â‰¤ğœ“(ğ‘¤Î¦ğœ†(A)) â‰¤Î¦ğœ†,0(A) âˆ’Î¦ğœ†,âˆ’Ë†ğœ–(A)"
REFERENCES,0.31675392670157065,"Ë†ğœ–
Whence"
REFERENCES,0.3172774869109948,"P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.31780104712041884,"> ğ‘¡

â‰¤P
Î¦ğœ†,0(A) âˆ’Î¦ğœ†,âˆ’Ë†ğœ–(A)"
REFERENCES,0.3183246073298429,"Ë†ğœ–
> ğ‘ğœ†,0 âˆ’ğ‘ğœ†,âˆ’Ë†ğœ–"
REFERENCES,0.31884816753926704,"Ë†ğœ–
+ ğ‘¡ 2 "
REFERENCES,0.3193717277486911,"+ P
Î¦ğœ†, Ë†ğœ–(A) âˆ’Î¦ğœ†,0(A)"
REFERENCES,0.31989528795811517,"Ë†ğœ–
< ğ‘ğœ†, Ë†ğœ–âˆ’ğ‘ğœ†,0 Ë†ğœ–
âˆ’ğ‘¡ 2 "
REFERENCES,0.3204188481675393,"â‰¤P

Î¦ğœ†,0(A) âˆ’Î¦ğœ†,âˆ’Ë†ğœ–(A)"
REFERENCES,0.32094240837696336,"Ë†ğœ–
âˆ’ğ‘ğœ†,0 âˆ’ğ‘ğœ†,âˆ’Ë†ğœ– Ë†ğœ– > ğ‘¡ 2 "
REFERENCES,0.32146596858638743,"+ P

Î¦ğœ†, Ë†ğœ–(A) âˆ’Î¦ğœ†,0(A)"
REFERENCES,0.3219895287958115,"Ë†ğœ–
âˆ’ğ‘ğœ†, Ë†ğœ–âˆ’ğ‘ğœ†,0 Ë†ğœ– > ğ‘¡ 2 "
REFERENCES,0.3225130890052356,"By (23), the RHS goes to zero, thus ğœ“(wÎ¦ğœ†(A))
Pâˆ’â†’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.3230366492146597,"ğœ–=0. Furthermore, we have"
REFERENCES,0.32356020942408376,"P
ğœ“(wÎ¦ğœ†(B)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.3240837696335079,"> ğ‘¡

â‰¤P

Î¦ğœ†,0(B) âˆ’Î¦ğœ†,âˆ’Ë†ğœ–(B)"
REFERENCES,0.32460732984293195,"Ë†ğœ–
âˆ’ğ‘ğœ†,0 âˆ’ğ‘ğœ†,âˆ’Ë†ğœ– Ë†ğœ– > ğ‘¡ 2 "
REFERENCES,0.325130890052356,"+ P

Î¦ğœ†, Ë†ğœ–(B) âˆ’Î¦ğœ†,0(B)"
REFERENCES,0.3256544502617801,"Ë†ğœ–
âˆ’ğ‘ğœ†, Ë†ğœ–âˆ’ğ‘ğœ†,0 Ë†ğœ– > ğ‘¡ 2 "
REFERENCES,0.3261780104712042,"â‰¤P
Î¦ğœ†,ğœ–(B) âˆ’ğ‘ğœ†,ğœ–
 > ğœ–ğ‘¡ 2"
REFERENCES,0.3267015706806283,"
+ P
Î¦ğœ†,âˆ’ğœ–(B) âˆ’ğ‘ğœ†,âˆ’ğœ–
 > ğœ–ğ‘¡ 2 "
REFERENCES,0.32722513089005234,"+ 2P
Î¦ğœ†,0(B) âˆ’ğ‘ğœ†,0
 > ğœ–ğ‘¡ 2"
REFERENCES,0.32774869109947646,"
(24)"
REFERENCES,0.32827225130890053,"By the result of the first part, we know that the RHS (24) goes to zero as ğ‘›â†’âˆ. Hence, ğœ“(wÎ¦ğœ†(B))
Pâˆ’â†’
ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0."
REFERENCES,0.3287958115183246,"Moreover, if ğœ“is bounded, by definition of convergence in distribution, |EAğœ“(wÎ¦ğœ†(A)) âˆ’
EBğœ“(wÎ¦ğœ†(B))| â†’0."
REFERENCES,0.32931937172774867,"B.3.2
Proof of Part 2 of Theorem 1 for a regular test function for Î¨ = Î¦"
REFERENCES,0.3298429319371728,Defining
REFERENCES,0.33036649214659686,ËœÎ¦ğœ–(A) := 1
REFERENCES,0.3308900523560209,"ğ‘›
min
Aw=y,wâˆˆSw ğ‘“(w) + ğœ–ğœ“(w)"
REFERENCES,0.33141361256544505,"Since ğ‘ ğ‘¢ğ‘ğœ†>0 ËœÎ¦ğœ†,ğœ–(A)
Pâˆ’â†’ğ‘ ğ‘¢ğ‘ğœ†>0ğ‘ğœ†,ğœ–, or equivalently ËœÎ¦ğœ–(A)
Pâˆ’â†’ğ‘ğœ–, we can use the same argu-"
REFERENCES,0.3319371727748691,"ment from previous section and Danskinâ€™s theorem, to observe that ğœ“(w ËœÎ¦ğœ†(A))
Pâˆ’â†’
ğ‘‘ğ‘ğœ– ğ‘‘ğœ–"
REFERENCES,0.3324607329842932,ğœ–=0 and
REFERENCES,0.33298429319371725,"ğœ“(w ËœÎ¦ğœ†(B))
Pâˆ’â†’ğ‘‘ğ‘ğœ– ğ‘‘ğœ– ğœ–=0."
REFERENCES,0.3335078534031414,"B.3.3
Sequences of regular test functions"
REFERENCES,0.33403141361256544,"By a similar approach, it follows that for a sequence of regular functions ğœ“ğ‘šconverging uniformly to"
REFERENCES,0.3345549738219895,"ğœ“, one also has ğœ“(wÎ¦ğœ†)
Pâˆ’â†’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.33507853403141363,"ğœ–=0:= limğ‘šâ†’âˆ
ğ‘‘ğ‘ğ‘š
ğœ–
ğ‘‘ğœ–"
REFERENCES,0.3356020942408377,"ğœ–=0. Indeed,"
REFERENCES,0.33612565445026177,"P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.3366492146596859,"> ğ‘¡

â‰¤P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğœ“ğ‘š(wÎ¦ğœ†(A))
 > ğ‘¡ 2"
REFERENCES,0.33717277486910996,"
+ P
ğœ“ğ‘š(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0 > ğ‘¡ 2 "
REFERENCES,0.337696335078534,"Since we have uniform convergence, for ğ‘¡"
REFERENCES,0.3382198952879581,"2, there exists an ğ‘š0 such that for all ğ‘šâ‰¥ğ‘š0 and all ğ‘¥âˆˆRğ‘‘,"
REFERENCES,0.3387434554973822,we have |ğœ“ğ‘š(ğ‘¥) âˆ’ğœ“(ğ‘¥)| < ğ‘¡
REFERENCES,0.3392670157068063,"2. Thus for such ğ‘šâ€™s, P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğœ“ğ‘š(wÎ¦ğœ†(A))
 > ğ‘¡ 2"
REFERENCES,0.33979057591623035,"
= 0. Then by"
REFERENCES,0.3403141361256545,triangle inequality
REFERENCES,0.34083769633507854,"P
ğœ“(wÎ¦ğœ†(A)) âˆ’ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ– ğœ–=0"
REFERENCES,0.3413612565445026,"> ğ‘¡

â‰¤P
ğœ“ğ‘š(wÎ¦ğœ†(A)) âˆ’
ğ‘‘ğ‘ğ‘š
ğœ†,ğœ–
ğ‘‘ğœ– ğœ–=0 > ğ‘¡ 4"
REFERENCES,0.3418848167539267,"
+ P

ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.3424083769633508,"ğœ–=0âˆ’
ğ‘‘ğ‘ğ‘š
ğœ†,ğœ–
ğ‘‘ğœ– ğœ–=0 > ğ‘¡ 4 "
REFERENCES,0.34293193717277487,"By assumption P
ğœ“ğ‘š(wÎ¦ğœ†(A)) âˆ’
ğ‘‘ğ‘ğ‘š
ğœ†,ğœ–
ğ‘‘ğœ– ğœ–=0 > ğ‘¡ 4"
REFERENCES,0.34345549738219894,"
â†’0. And by the definition of ğ‘‘ğ‘ğœ†,ğœ– ğ‘‘ğœ–"
REFERENCES,0.34397905759162306,ğœ–=0 the claim
REFERENCES,0.34450261780104713,follows.
REFERENCES,0.3450261780104712,"C
Proof of Corollary 1"
REFERENCES,0.34554973821989526,We will utilize the result of the following Theorem.
REFERENCES,0.3460732984293194,"Theorem 4. (Bobkov [2003]) For an isotropic random vector X âˆˆRğ‘‘, and an isotropic gaussian
g âˆˆRğ‘‘if P(| âˆ¥Xâˆ¥2
âˆš"
REFERENCES,0.34659685863874345,"2 âˆ’1| â‰¥ğœ–ğ‘‘) â‰¤ğœ–ğ‘‘, then for all ğ›¿> 0"
REFERENCES,0.3471204188481675,"ğœˆ

sup
ğ‘¡âˆˆR"
REFERENCES,0.34764397905759165,"P(Xğ‘‡Î¸ â‰¤ğ‘¡) âˆ’P(gğ‘‡Î¸ â‰¤ğ‘¡)
 â‰¥4ğœ–ğ‘‘+ ğ›¿

â‰¤4ğ‘‘3/8ğ‘’âˆ’ğ‘ğ‘‘ğ›¿4"
REFERENCES,0.3481675392670157,Where Î¸ âˆ¼ğœˆthe uniform measure on the unit sphere Sğ‘‘âˆ’1
REFERENCES,0.3486910994764398,"Essentially, through CGMT, we observe that when we use a quadratic regularizer or run SGD, the
solution, wÎ¨(G) converges weakly in distribution to a Gaussian vector, gğ´ğ‘‚. Whence we conclude
that wÎ¨(G) behaves the same as a generic vector. Therefore, by choosing X := a âˆ’Ea, a row of our
data matrix A, Theorem 4 enables us to prove the universality of the classification error. Note that in
general EwÎ¨(G) â‰ 0, but by Assumptions 4, the means of the classes are generic which allows us to
employ Theorem 4 to conclude the proof."
REFERENCES,0.34921465968586385,"D
Lemmata for the proof of Theorem 1"
REFERENCES,0.34973821989528797,"Proposition 2. If for any Lipschitz function ğ‘”: R â†’R and ğ‘¡1 > 0, P

|ğ‘”(Î¦ğœ†,ğœ–(B)) âˆ’ğ‘| > ğ‘¡1

â†’0
and for every twice differentiable Ëœğ‘”: R â†’R with bounded second derivative, we have that
limğ‘›â†’âˆ
EA,y Ëœğ‘”(Î¦ğœ†,ğœ–(B)) âˆ’EB,y Ëœğ‘”(Î¦ğœ†,ğœ–(A))
 â†’0 then for any ğ‘¡2 > ğ‘¡1."
REFERENCES,0.35026178010471204,"lim
ğ‘›â†’âˆP

|ğ‘”(Î¦ğœ†,ğœ–(A)) âˆ’ğ‘| > ğ‘¡2

= 0"
REFERENCES,0.3507853403141361,"Proof. First note that P(|Î¦ğœ†,ğœ–(B) âˆ’ğ‘| > ğ‘¡) â†’0, implies P(|ğ‘”(Î¦ğœ†,ğœ–(B)) âˆ’ğ‘”(ğ‘)| > ğ¿ğ‘¡) â†’0.
Thus it suffices to verify P(|Î¦ğœ†,ğœ–(B) âˆ’ğ‘| > ğ‘¡) â†’0. This follows similar to Panahi and Hassibi
[2017]. Let ğœ‰(ğ‘¥) := 21(âˆ’âˆ,âˆ’ğ‘¡2]âˆª[ğ‘¡2,âˆ) (ğ‘¥) +   ğ‘¡1+ğ‘¡2"
REFERENCES,0.35130890052356023,"2
âˆ’(|ğ‘¥| âˆ’ğ‘¡2)21(âˆ’ğ‘¡2,âˆ’ğ‘¡1+ğ‘¡2"
REFERENCES,0.3518324607329843,"2
]âˆª[ ğ‘¡1+ğ‘¡2"
REFERENCES,0.35235602094240837,"2
,ğ‘¡2) (ğ‘¥) + (|ğ‘¥| âˆ’"
REFERENCES,0.35287958115183243,ğ‘¡1)21(âˆ’ğ‘¡1+ğ‘¡2
REFERENCES,0.35340314136125656,"2
,âˆ’ğ‘¡1]âˆª[ğ‘¡1, ğ‘¡1+ğ‘¡2"
REFERENCES,0.3539267015706806,"2
) (ğ‘¥). Then we have"
REFERENCES,0.3544502617801047,"P

|Î¦ğœ†,ğœ–(A) âˆ’ğ‘| > ğ‘¡2

= P

ğœ‰ Î¦ğœ†,ğœ–(A) âˆ’ğ‘ > 2
"
REFERENCES,0.3549738219895288,Then by Markovâ€™s and triangle inequality
REFERENCES,0.3554973821989529,"P

|Î¦ğœ†,ğœ–(A) âˆ’ğ‘| > ğ‘¡2

â‰¤1"
REFERENCES,0.35602094240837695,"2EA,yğœ‰ Î¦ğœ†,ğœ–(A) âˆ’ğ‘ â‰¤1 2"
REFERENCES,0.3565445026178011,"EA,yğœ‰ Î¦ğœ†,ğœ–(A) âˆ’ğ‘ âˆ’EB,yğœ‰ Î¦ğœ†,ğœ–(B) âˆ’ğ‘ + 1"
REFERENCES,0.35706806282722514,"2EB,yğœ‰ Î¦ğœ†,ğœ–(B) âˆ’ğ‘"
REFERENCES,0.3575916230366492,"Note that ğœ‰(ğ‘¥) â‰¤21(âˆ’âˆ,âˆ’ğ‘¡1]âˆª[ğ‘¡1,âˆ). Thus 1"
REFERENCES,0.3581151832460733,"2EB,yğœ‰ Î¦ğœ†,ğœ–(B) âˆ’ğ‘ â‰¤P

|Î¦ğœ†,ğœ–(B) âˆ’ğ‘| > ğ‘¡1

. Moreover,
we know that ğœ‰(.) has bounded second derivative derivatives almost everywhere and by assumption
on ğ‘”â€², we would have"
REFERENCES,0.3586387434554974,"P

|Î¦ğœ†,ğœ–(A) âˆ’ğ‘| > ğ‘¡2

â‰¤1 2"
REFERENCES,0.35916230366492147,"EA,yğœ‰ Î¦ğœ†,ğœ–(A) âˆ’ğ‘ âˆ’EB,yğœ‰ Î¦ğœ†,ğœ–(B) âˆ’ğ‘ + P

|Î¦ğœ†,ğœ–(B) âˆ’ğ‘| > ğ‘¡1

â†’0 â–¡"
REFERENCES,0.35968586387434553,Lemma 3. We have the following:
REFERENCES,0.36020942408376966,1. L(a) âˆ’Î˜ = ğœ†
REFERENCES,0.3607329842931937,"2ğ‘›
(ağ‘‡uâˆ’ğ‘¦)2"
REFERENCES,0.3612565445026178,1+ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.36178010471204186,"2. wL âˆ’u = ğœ†
ğ‘¦âˆ’ağ‘‡u
1+ğœ†ağ‘‡ğ›€âˆ’1ağ›€âˆ’1a"
REFERENCES,0.362303664921466,"Proof. We know for Î˜, taking derivative yields"
REFERENCES,0.36282722513089005,ğœ†ËœAğ‘‡ËœAu âˆ’ğœ†ËœAğ‘‡Ëœy + g = 0
REFERENCES,0.3633507853403141,This implies âˆ’ğœ†ËœAğ‘‡Ëœy + g = âˆ’ğœ†ËœAğ‘‡ËœAu.
REFERENCES,0.36387434554973824,"Note that for L(a), since the objective is quadratic in w, we solve the optimization in closed form,
we have"
REFERENCES,0.3643979057591623,L(a) = 1
REFERENCES,0.3649214659685864,"ğ‘›min
w
 wğ‘‡
1 
1
2 (ğœ†ËœAğ‘‡ËœA + ğœ†aağ‘‡+ H)
1
2 (âˆ’ğœ†ËœAğ‘‡Ëœy âˆ’ğœ†ğ‘¦a + g âˆ’Hu)
1
2 (âˆ’ğœ†ËœAğ‘‡Ëœy âˆ’ğœ†ğ‘¦a + g âˆ’Hu)ğ‘‡
ğ‘“(u) + ğœ–ğœ“(u) âˆ’gğ‘‡u + 1"
REFERENCES,0.36544502617801045,2uğ‘‡Hu + ğœ†
REFERENCES,0.36596858638743457,2 (âˆ¥Ëœyâˆ¥2 + ğ‘¦2)
REFERENCES,0.36649214659685864," 
w
1 "
REFERENCES,0.3670157068062827,Let ğ›€:= ğœ†ËœAğ‘‡ËœA + H. hence
REFERENCES,0.3675392670157068,L(a) = 1 ğ‘›
REFERENCES,0.3680628272251309,"
ğ‘“(u) + ğœ–ğœ“(u) âˆ’gğ‘‡u + 1"
REFERENCES,0.36858638743455496,2ğ‘¢ğ‘‡Hu + ğœ†
REFERENCES,0.36910994764397903,2 (âˆ¥Ëœyâˆ¥2 + ğ‘¦2) âˆ’1
REFERENCES,0.36963350785340315,"2 (ğ›€u + ğœ†ğ‘¦ğ‘)ğ‘‡(ğ›€+ ğœ†ğ‘ağ‘‡)âˆ’1(ğ›€u + ğœ†ğ‘¦a)
"
REFERENCES,0.3701570680628272,Using Sherman-Morrison as ğ›€â‰»0:
REFERENCES,0.3706806282722513,"(ğ›€u + ğœ†ğ‘¦a)ğ‘‡
ğ›€+ ğœ†aağ‘‡âˆ’1
(ğ›€u + ğœ†ğ‘¦a) = (ğ›€u + ğœ†ğ‘¦a)ğ‘‡
ğ›€âˆ’1 âˆ’ğœ†ğ›€âˆ’1aağ‘‡ğ›€âˆ’1"
REFERENCES,0.3712041884816754,1 + ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.3717277486910995,"
(ğ›€u + ğœ†ğ‘¦a) ="
REFERENCES,0.37225130890052355,"= (ğ›€u + ğœ†ğ‘¦a)ğ‘‡
u âˆ’ğœ†ağ‘‡u
ğ›€âˆ’1a"
REFERENCES,0.37277486910994767,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
+ ğœ†ğ‘¦ğ›€âˆ’1a âˆ’ğœ†2ğ‘¦ağ‘‡ğ›€âˆ’1a
ğ›€âˆ’1a"
REFERENCES,0.37329842931937174,"1 + ğœ†ağ‘‡ğ›€âˆ’1a 
="
REFERENCES,0.3738219895287958,"= (ğ›€u + ğœ†ğ‘¦a)ğ‘‡

u + ğœ†

ğ‘¦âˆ’ğœ†ğ‘¦ağ‘‡ğ›€âˆ’1a + ağ‘‡u"
REFERENCES,0.3743455497382199,1 + ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.374869109947644,"
ğ›€âˆ’1a

= (ğ›€u + ğœ†ğ‘¦a)ğ‘‡
u + ğœ†
ğ‘¦âˆ’ağ‘‡u"
REFERENCES,0.37539267015706806,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
ğ›€âˆ’1a
"
REFERENCES,0.37591623036649213,From here we obtain for the solution Thus for the first claim:
REFERENCES,0.37643979057591626,"wL = (ğ›€+ ğœ†aağ‘‡)âˆ’1(ğ›€u + ğœ†ğ‘¦a) = u + ğœ†
ğ‘¦âˆ’ağ‘‡u"
REFERENCES,0.3769633507853403,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
ğ›€âˆ’1a"
REFERENCES,0.3774869109947644,Hence we have for the first claim
REFERENCES,0.37801047120418846,"wL âˆ’u = ğœ†
ğ‘¦âˆ’ağ‘‡u"
REFERENCES,0.3785340314136126,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
ğ›€âˆ’1a"
REFERENCES,0.37905759162303665,Now we continue with the calculation of the objective value
REFERENCES,0.3795811518324607,"uğ‘‡ğ›€u + ğœ†
ğ‘¦âˆ’ağ‘‡u"
REFERENCES,0.38010471204188484,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
ağ‘‡u + ğœ†ğ‘¦ağ‘‡u + ğœ†2ğ‘¦
ğ‘¦âˆ’ağ‘‡u"
REFERENCES,0.3806282722513089,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
ağ‘‡ğ›€âˆ’1a ="
REFERENCES,0.381151832460733,"= uğ‘‡ğ›€u +
ğœ†"
REFERENCES,0.38167539267015704,1 + ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.38219895287958117,"
ğ‘¦ağ‘‡u âˆ’(ağ‘‡u)2 + ğ‘¦ağ‘‡u + ğœ†ğ‘¦(ağ‘‡u)(ağ‘‡ğ›€âˆ’1a) + ğœ†ğ‘¦2ağ‘‡ğ›€âˆ’1a âˆ’ğœ†ğ‘¦(ağ‘‡u)(ağ‘‡ğ›€âˆ’1a)

="
REFERENCES,0.38272251308900523,"= uğ‘‡ğ›€u +
ğœ†"
REFERENCES,0.3832460732984293,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
(2ğ‘¦ağ‘‡u âˆ’(ağ‘‡u)2 + ğœ†ğ‘¦2ağ‘‡ğ›€âˆ’1a) Thus"
REFERENCES,0.3837696335078534,L(a) = 1 ğ‘›
REFERENCES,0.3842931937172775,"
ğ‘“(u) + ğœ–ğœ“(u) âˆ’gğ‘‡u + 1"
REFERENCES,0.38481675392670156,2uğ‘‡Hu + ğœ†
REFERENCES,0.38534031413612563,"2 (âˆ¥Ëœyâˆ¥2 + ğ‘¦2) âˆ’1 2uğ‘‡ğ›€u âˆ’1 2
ğœ†"
REFERENCES,0.38586387434554975,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
(2ğ‘¦ağ‘‡u âˆ’(ağ‘‡u)2 + ğœ†ğ‘¦2ağ‘‡ğ›€âˆ’1a)
 = 1 ğ‘›"
REFERENCES,0.3863874345549738,"
ğ‘“(u) + ğœ–ğœ“(u) âˆ’gğ‘‡u âˆ’ğœ†"
REFERENCES,0.3869109947643979,2uğ‘‡ËœAğ‘‡ËœAu + ğœ†
REFERENCES,0.387434554973822,2 âˆ¥Ëœyâˆ¥2 + ğœ†
REFERENCES,0.3879581151832461,"2
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.38848167539267014,1 + ğœ†ağ‘‡ğ›€âˆ’1a 
REFERENCES,0.38900523560209427,"Since ğ‘”= ğœ†ËœAğ‘‡Ëœy âˆ’ğœ†ËœAğ‘‡ËœAğ‘¢, then gğ‘‡u = ğœ†Ëœyğ‘‡ËœAu âˆ’ğœ†uğ‘‡ËœA ËœAğ‘¢, therefore"
REFERENCES,0.38952879581151834,L(a) = 1 ğ‘›
REFERENCES,0.3900523560209424,"
ğ‘“(u) + ğœ–ğœ“(u) âˆ’ğœ†Ëœyğ‘‡ËœAu + ğœ†uğ‘‡ËœA ËœAu âˆ’ğœ†"
REFERENCES,0.39057591623036647,2uğ‘‡ËœAğ‘‡ËœAu + ğœ†
REFERENCES,0.3910994764397906,2 âˆ¥Ëœyâˆ¥2 + ğœ†
REFERENCES,0.39162303664921466,"2
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.39214659685863873,1 + ğœ†ağ‘‡ğ›€âˆ’1a  = 1 ğ‘›
REFERENCES,0.39267015706806285,"
ğ‘“(u) + ğœ–ğœ“(u) + ğœ†"
REFERENCES,0.3931937172774869,2 âˆ¥ËœAu âˆ’Ëœyâˆ¥2 + ğœ†
REFERENCES,0.393717277486911,"2
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.39424083769633506,1 + ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.3947643979057592,"
= Î˜ + ğœ†"
REFERENCES,0.39528795811518325,"2ğ‘›
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.3958115183246073,"1 + ğœ†ağ‘‡ğ›€âˆ’1a
â–¡"
REFERENCES,0.39633507853403144,Lemma 4. We have the following:
REFERENCES,0.3968586387434555,"1. M(a) âˆ’Î˜ â‰¤
ğœ†
2ğ‘›(ağ‘‡u âˆ’y)2"
REFERENCES,0.3973821989528796,"2. â„“(a, w) âˆ’L(a) â‰¥
ğœŒ
2ğ‘›âˆ¥w âˆ’wÂ¯ Lâˆ¥2
2"
REFERENCES,0.39790575916230364,"3. |ğ“‚(a, w) âˆ’â„“(a, w)| â‰¤ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.39842931937172776,"ğ‘›
âˆ¥w âˆ’uâˆ¥3
3"
REFERENCES,0.39895287958115183,"Proof. 1. By definition,"
REFERENCES,0.3994764397905759,"M(a) â‰¤ğ“‚(a, u) = Î˜ + ğœ†"
REFERENCES,0.4,2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.4005235602094241,"Thus M(a) âˆ’Î˜ â‰¤
ğœ†
2ğ‘›(ağ‘‡u âˆ’y)2"
REFERENCES,0.40104712041884816,"2. The idea of the proof is by strong convexity. By assumption, ğ‘“(w) + ğœ–ğœ“(w) for small enough ğœ–is
ğœŒ-strongly convex. This implies from defintion:"
REFERENCES,0.4015706806282722,"â„“(a, w) â‰¥L(a) + ğœŒ"
REFERENCES,0.40209424083769635,"2ğ‘›âˆ¥w âˆ’wLâˆ¥2
2"
REFERENCES,0.4026178010471204,3. By Taylor remainder theorem for some 0 < ğ‘¡< 1:
REFERENCES,0.4031413612565445,"ğ“‚(a, w) âˆ’â„“(a, w) = 1 ğ‘›"
REFERENCES,0.4036649214659686,"
ğ‘“(w) + ğœ–ğœ“(w) âˆ’ğ‘“(u) âˆ’ğœ–ğœ“(u) âˆ’ğ‘”ğ‘‡(w âˆ’u) âˆ’1"
REFERENCES,0.4041884816753927,"2 (w âˆ’u)ğ‘‡H(w âˆ’u)
 = 1 ğ‘› âˆ‘ï¸"
REFERENCES,0.40471204188481674,"| ğ›¼|=3
ğœ•ğ›¼( ğ‘“+ ğœ–ğœ“)((1 âˆ’ğ‘¡)u + ğ‘¡w) (w âˆ’u) ğ›¼ ğ›¼!"
REFERENCES,0.40523560209424087,"Since ğ‘“and ğœ“are separable, ğœ•ğ›¼( ğ‘“+ ğœ–ğœ“) = 0 unless ğ›¼= 3ğ‘’ğ›¼(ğ‘’ğ›¼is the standard basis vector in Rğ‘‘.
Then by assumption on the third derivative of ğ‘“+ ğœ–ğœ“,"
REFERENCES,0.40575916230366493,"|ğ“‚(a, w) âˆ’â„“(a, w)| = 1 ğ‘› âˆ‘ï¸"
REFERENCES,0.406282722513089,"| ğ›¼|=3
ğœ•ğ›¼( ğ‘“+ ğœ–ğœ“)((1 âˆ’ğ‘¡)u + ğ‘¡w) (w âˆ’u) ğ›¼ ğ›¼!  = 1 ğ‘›  ğ‘‘
âˆ‘ï¸ ğ‘–=1"
REFERENCES,0.40680628272251307,ğœ•3( ğ‘“+ ğœ–ğœ“)((1 âˆ’ğ‘¡)ğ‘¢ğ‘–+ ğ‘¡ğ‘¤ğ‘–)
REFERENCES,0.4073298429319372,"ğœ•ğ‘¤3
ğ‘–
(wğ‘–âˆ’ğ‘¢ğ‘–)3 â‰¤ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.40785340314136126,"ğ‘›
âˆ¥w âˆ’uâˆ¥3
3 â–¡"
REFERENCES,0.4083769633507853,Now we are ready to prove the following key lemma.
REFERENCES,0.40890052356020945,Lemma 5. We have |M(a) âˆ’L(a)| â‰¤min{ ğœ†
REFERENCES,0.4094240837696335,"2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2, 8ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.4099476439790576,"ğ‘›
âˆ¥wL âˆ’uâˆ¥3
3}"
REFERENCES,0.41047120418848165,"Proof. Let wM = arg min ğ“‚(a, w). Note that since Î˜ â‰¤M(a) and by Lemma 4 we have"
REFERENCES,0.4109947643979058,L(a) âˆ’M(a) = L(a) âˆ’Î˜ + Î˜ âˆ’M(a) â‰¤ğœ†
REFERENCES,0.41151832460732984,"2ğ‘›
(ağ‘‡u âˆ’ğ‘¦)2"
REFERENCES,0.4120418848167539,1 + ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.41256544502617803,"On the other hand, note that ğ“‚(a, u) = Î˜ + ğœ†"
REFERENCES,0.4130890052356021,2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2 and Î˜ â‰¤L(a)
REFERENCES,0.41361256544502617,"M(a) âˆ’L(a) = M(a) âˆ’ğ“‚(a, u) + ğ“‚(a, u) âˆ’Î˜ + Î˜ âˆ’L(a) â‰¤ğœ†"
REFERENCES,0.41413612565445024,2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.41465968586387436,Thus as ğœ†
REFERENCES,0.41518324607329843,"2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2 â‰¥
ğœ†
2ğ‘›
(ağ‘‡uâˆ’ğ‘¦)2"
REFERENCES,0.4157068062827225,1+ğœ†ağ‘‡ğ›€âˆ’1a
REFERENCES,0.4162303664921466,|M(a) âˆ’L(a)| â‰¤ğœ†
REFERENCES,0.4167539267015707,2ğ‘›(ağ‘‡u âˆ’ğ‘¦)2
REFERENCES,0.41727748691099475,"But as we will see, (ağ‘‡u âˆ’ğ‘¦)2 is of order ğ‘‚(1), which will not suffice. Thus we break down
M(a) âˆ’L(a) depending on the distance of ğ‘¢and wL"
REFERENCES,0.4178010471204188,"M(a) âˆ’L(a) = ğ“‚(a, wM) âˆ’ğ“‚(a, wL) + ğ“‚(a, wL) âˆ’â„“(a, wL)"
REFERENCES,0.41832460732984295,"â‰¤ğ“‚(a, wL) âˆ’â„“(a, wL) â‰¤ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.418848167539267,"ğ‘›
âˆ¥wL âˆ’uâˆ¥3
3"
REFERENCES,0.4193717277486911,For the other direction
REFERENCES,0.4198952879581152,"L(a) âˆ’M(a) = â„“(a, wL) âˆ’â„“(a, wM) + â„“(a, wM) âˆ’ğ“‚(a, wM) â‰¤ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.42041884816753927,"ğ‘›
âˆ¥wM âˆ’uâˆ¥3
3"
REFERENCES,0.42094240837696334,"Let ğ‘…:= âˆ¥wL âˆ’uâˆ¥3 and consider the â„“3 ball centered at wL. Now if âˆ¥wM âˆ’wLâˆ¥3 â‰¤ğ‘…then
âˆ¥wM âˆ’uâˆ¥3 â‰¤2ğ‘…by a straightforward application of triangle inequality. Which implies
L(a) âˆ’M(a) â‰¤8ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.4214659685863874,"ğ‘›
ğ‘…3. Now when does âˆ¥wM âˆ’wLâˆ¥3 â‰¤ğ‘…hold? We show that for every point
on the boundary, w, ğ“‚(a, w) â‰¥ğ“‚(a, wL), this implies that wM is indeed inside the ball. We have"
REFERENCES,0.42198952879581153,"ğ“‚(a, w) âˆ’ğ“‚(a, wL) â‰¥â„“(a, w) âˆ’ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.4225130890052356,"ğ‘›
âˆ¥w âˆ’uâˆ¥3
3 âˆ’â„“(a, wL) âˆ’ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.42303664921465967,"ğ‘›
âˆ¥wL âˆ’uâˆ¥3
3 â‰¥ğœŒ"
REFERENCES,0.4235602094240838,"2ğ‘›âˆ¥w âˆ’wLâˆ¥2
2 âˆ’ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.42408376963350786,"ğ‘›
(âˆ¥w âˆ’uâˆ¥3
3 + ğ‘…3)"
REFERENCES,0.4246073298429319,Then using Lemma 4
REFERENCES,0.42513089005235605,"ğ“‚(a, w) âˆ’ğ“‚(a, wL) â‰¥ğœŒ"
REFERENCES,0.4256544502617801,"2ğ‘›âˆ¥w âˆ’wLâˆ¥2
3 âˆ’ğ¶ğ‘“+ğœ–ğœ“"
REFERENCES,0.4261780104712042,"ğ‘›
(âˆ¥w âˆ’uâˆ¥3
3 + ğ‘…3) â‰¥ğœŒ"
REFERENCES,0.42670157068062825,2ğ‘›ğ‘…2 âˆ’ğ¶ğ‘“+ğœ–ğœ“
REFERENCES,0.4272251308900524,"ğ‘›
(8ğ‘…3 + ğ‘…3) = ğ‘…2( ğœŒ"
REFERENCES,0.42774869109947644,"2ğ‘›âˆ’9ğ¶ğ‘“+ğœ–ğœ“ ğ‘›
ğ‘…)"
REFERENCES,0.4282722513089005,"Now if ğ‘…â‰¤
ğœŒ
18ğ¶ğ‘“+ğœ–ğœ“, then ğ“‚(a, w) âˆ’ğ“‚(a, wL) â‰¥0. Thus all in all"
REFERENCES,0.42879581151832463,|L(a) âˆ’M(a)| â‰¤8ğ¶ğ‘“+ğœ–ğœ“
REFERENCES,0.4293193717277487,"ğ‘›
âˆ¥wL âˆ’uâˆ¥3
3 â–¡"
REFERENCES,0.42984293193717277,Lemma 6. We have the following upper bounds
REFERENCES,0.43036649214659684,"1. âˆ¥uâˆ¥2
2 = ğ‘‚(ğ‘›) with high probabilty."
REFERENCES,0.43089005235602096,"2. E ËœA,ËœyÂµğ‘‡u = ğ‘‚(1)"
REFERENCES,0.431413612565445,"Proof. 1. Since ğ‘“+ ğœ–ğœ“is ğœŒ-strongly convex, one can decompose ğ‘“(w) + ğœ–ğœ“(w) = ğœŒ"
REFERENCES,0.4319371727748691,"2 âˆ¥wâˆ¥2
2 + â„(w)
for some convex function â„(Â·). Then"
REFERENCES,0.4324607329842932,"ğœ†
2 âˆ¥ËœAu âˆ’Ëœyâˆ¥2
2 + ğœŒ"
REFERENCES,0.4329842931937173,2 âˆ¥uâˆ¥2 + â„(u) â‰¤ğœ†
REFERENCES,0.43350785340314135,"2 âˆ¥Ëœyâˆ¥2
2 + â„(0) = ğ‘‚(ğ‘›)"
REFERENCES,0.4340314136125654,"Thus this implies âˆ¥uâˆ¥2
2 = ğ‘‚(ğ‘›)."
REFERENCES,0.43455497382198954,"2. In a similar fashion, we have for E ËœA,ËœyÂµğ‘‡u Î˜ = 1 ğ‘› ğœ†"
REFERENCES,0.4350785340314136,"2 âˆ¥ËœAu âˆ’Ëœyâˆ¥2
2 + ğ‘“(u) + ğœ–ğœ“(u)

â‰¤1 ğ‘› ğœ†"
REFERENCES,0.4356020942408377,"2 âˆ¥Ëœyâˆ¥2 + ğ‘“(0) + ğœ–ğœ“(0)
"
REFERENCES,0.4361256544502618,"Let ËœAğ‘‡=
 Ë†ËœAğ‘‡
1
Ë†ËœAğ‘‡
2
. . .
Ë†ËœAğ‘‡
ğ‘˜"
REFERENCES,0.43664921465968587,"
. Now we use the definition of ËœA to obtain a lower bound:"
REFERENCES,0.43717277486910994,"ğœ†
2 âˆ¥ËœAu âˆ’Ëœyâˆ¥2
2 + ğ‘“(u) + ğœ–ğœ“(u) â‰¥ğœ† 2 ğ‘˜
âˆ‘ï¸"
REFERENCES,0.437696335078534,"ğ‘–=1
âˆ¥Ë†ËœAğ‘–u + 1Âµğ‘‡
ğ‘–u âˆ’Ë†Ëœyğ‘–âˆ¥2
2 â‰¥ğœ†"
REFERENCES,0.4382198952879581,"2 âˆ¥Ë†ËœAğ‘–u + 1Âµğ‘‡
ğ‘–u âˆ’Ë†Ëœyğ‘–âˆ¥2
2 â‰¥ğ‘›ğ‘–ğœ†"
REFERENCES,0.4387434554973822,"2 (Âµğ‘‡
ğ‘–u)2 + ğœ†(Âµğ‘‡
ğ‘–u)1ğ‘‡( Ë†ËœAğ‘–u âˆ’Ë†Ëœyğ‘–)"
REFERENCES,0.43926701570680626,"Thus plugging in w = 0 in the optimization yields an upper bound, which is a quadratic inequality in
Âµğ‘‡
ğ‘–u: ğ‘›ğ‘–ğœ†"
REFERENCES,0.4397905759162304,"2 (Âµğ‘‡
ğ‘–u)2 + ğœ†(Âµğ‘‡
ğ‘–u)1ğ‘‡( Ë†ËœAğ‘–u âˆ’Ë†Ëœyğ‘–) âˆ’ğœ†"
REFERENCES,0.44031413612565445,"2 âˆ¥Ëœyâˆ¥2 âˆ’ğ‘“(0) âˆ’ğœ–ğœ“(0) â‰¤0
(25)"
REFERENCES,0.4408376963350785,Now note that for a quadratic optimization with ğ‘â‰¥0
REFERENCES,0.44136125654450264,"ğ‘¥2 âˆ’ğ‘ğ‘¥âˆ’ğ‘â‰¤0 â‡â‡’ğ‘âˆ’
âˆš"
REFERENCES,0.4418848167539267,ğ‘2 + 4ğ‘
REFERENCES,0.4424083769633508,"2
â‰¤ğ‘¥â‰¤ğ‘+
âˆš"
REFERENCES,0.44293193717277485,ğ‘2 + 4ğ‘
WHENCE,0.44345549738219897,"2
Whence"
WHENCE,0.44397905759162304,"|ğ‘¥| â‰¤|ğ‘| +
âˆš"
WHENCE,0.4445026178010471,"ğ‘2 + 4ğ‘
2
â‰¤|ğ‘| + |ğ‘| + 2
âˆš"
WHENCE,0.44502617801047123,"ğ‘
2
= |ğ‘| +
âˆš ğ‘"
WHENCE,0.4455497382198953,"We apply this result to 25,"
WHENCE,0.44607329842931936,"|Âµğ‘‡
ğ‘–u| â‰¤2"
WHENCE,0.44659685863874343,"ğ‘›ğ‘–
|1ğ‘‡( Ë†ËœAğ‘–u âˆ’Ë†Ëœyğ‘–)| + 2 ğœ†ğ‘›ğ‘– âˆšï¸‚"
WHENCE,0.44712041884816756,"ğœ†
2 âˆ¥Ëœyâˆ¥2 + ğ‘“(0) + ğœ–ğœ“(0) â‰¤2"
WHENCE,0.4476439790575916,"ğ‘›ğ‘–
|1ğ‘‡Ë†Ëœyğ‘–| + 2"
WHENCE,0.4481675392670157,"ğ‘›ğ‘–
âˆ¥1ğ‘‡Ë†ËœAğ‘–âˆ¥2âˆ¥uâˆ¥2 + 2 ğœ†ğ‘›ğ‘– âˆšï¸‚"
WHENCE,0.4486910994764398,"ğœ†
2 âˆ¥Ëœyâˆ¥2 + ğ‘“(0) + ğœ–ğœ“(0)"
WHENCE,0.4492146596858639,Now taking expectation and using a combination Cauchy-Schwarz and Jensen inequalities yields
WHENCE,0.44973821989528795,"E ËœA,Ëœy|Âµğ‘‡
ğ‘–u| â‰¤2"
WHENCE,0.450261780104712,"ğ‘›ğ‘–
E ËœA,Ëœy|1ğ‘‡Ë†Ëœyğ‘–| + 2 ğ‘›ğ‘– âˆšï¸ƒ"
WHENCE,0.45078534031413614,"E ËœA,Ëœyâˆ¥1ğ‘‡Ë†ËœAğ‘–âˆ¥2
2E ËœA,Ëœyâˆ¥uâˆ¥2
2 + 2 ğœ†ğ‘›ğ‘– âˆšï¸‚"
WHENCE,0.4513089005235602,"ğœ†
2E ËœA,Ëœyâˆ¥Ëœyâˆ¥2
2 + ğ‘“(0) + ğœ–ğœ“(0)"
WHENCE,0.4518324607329843,"Now note that E Ë†ËœAğ‘–= 0, and by the independence of the rows:"
WHENCE,0.4523560209424084,"E ËœA,Ëœyâˆ¥1ğ‘‡Ë†ËœAğ‘–âˆ¥2
2 = ğ‘‘
âˆ‘ï¸ ğ‘—1=1"
WHENCE,0.45287958115183247,"  ğ‘›
âˆ‘ï¸"
WHENCE,0.45340314136125653,"ğ‘—2=1
( Ë†ËœAğ‘–) ğ‘—2 ğ‘—1
2 = ğ‘‘
âˆ‘ï¸ ğ‘—1=1 ğ‘›
âˆ‘ï¸"
WHENCE,0.4539267015706806,"ğ‘—2, ğ‘—3=1
E ËœA( Ë†ËœAğ‘–) ğ‘—2 ğ‘—1( Ë†ËœAğ‘–) ğ‘—3 ğ‘—1 = ğ‘‘
âˆ‘ï¸"
WHENCE,0.4544502617801047,"ğ‘—2=1
E ËœA ğ‘›
âˆ‘ï¸"
WHENCE,0.4549738219895288,"ğ‘—1=1
( Ë†ËœAğ‘–)2
ğ‘—2 ğ‘—1"
WHENCE,0.45549738219895286,"By assumption, for each row Ë†ağ‘–of Ë†ËœAğ‘–, we have that EË†ağ‘–âˆ¥Ë†ağ‘–âˆ¥2
2 = ğ‘‚(1), which implies E ËœA,Ëœyâˆ¥1ğ‘‡Ë†ËœAğ‘–âˆ¥2
2 =
ğ‘‚(ğ‘‘). Therefore, by the previous part, since âˆ¥uâˆ¥2
2 â‰¤
ğœ†
ğœŒâˆ¥Ëœyâˆ¥2
2 + 2"
WHENCE,0.456020942408377,"ğœŒâ„(0), then EËœyâˆ¥uâˆ¥2
2 â‰¤
2
ğœŒâ„(0) +
ğœ†
ğœŒEËœyâˆ¥Ëœyâˆ¥2
2 = ğ‘‚(ğ‘‘) by assumption on the Ëœy. Hence it can be seen that E ËœA,ËœyÂµğ‘‡u = ğ‘‚(1)
â–¡"
WHENCE,0.45654450261780105,"Now we are ready to conclude the proof with the following lemma.
Lemma 7. We have the following"
WHENCE,0.4570680628272251,"1. E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)2ğ‘= ğ‘‚(1) for ğ‘âˆˆ[3]"
WHENCE,0.45759162303664924,"2. E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3 = ğ‘‚( 1 ğ‘‘)"
WHENCE,0.4581151832460733,Proof. 1. First note that
WHENCE,0.4586387434554974,"E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)2ğ‘= E ËœA,Ëœy,a,ğ‘¦ 2ğ‘
âˆ‘ï¸ ğ‘—=1 2ğ‘
ğ‘—"
WHENCE,0.45916230366492145,"
((a âˆ’Âµ + Âµ)ğ‘‡u) ğ‘—(âˆ’ğ‘¦)2ğ‘âˆ’ğ‘¦"
WHENCE,0.45968586387434557,"Moreover,"
WHENCE,0.46020942408376964,"E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)2ğ‘â‰¤ 2ğ‘
âˆ‘ï¸ ğ‘—=1 2ğ‘
ğ‘—"
WHENCE,0.4607329842931937,"
E ËœA,Ëœy,a,ğ‘¦"
WHENCE,0.4612565445026178,"
|(a âˆ’Âµ + Âµ)ğ‘‡u| ğ‘—|ğ‘¦|2ğ‘âˆ’ğ‘¦
 â‰¤ 2ğ‘
âˆ‘ï¸ ğ‘—=1 2ğ‘
ğ‘—"
WHENCE,0.4617801047120419,"
E ËœA,Ëœy,a,ğ‘¦"
WHENCE,0.46230366492146596,"
|ğ‘¦|2ğ‘âˆ’ğ‘¦2ğ‘—max{|Âµğ‘‡u|, |(a âˆ’Âµ)ğ‘‡u|} ğ‘—
 â‰¤ 2ğ‘
âˆ‘ï¸"
WHENCE,0.46282722513089003,"ğ‘—=1
2 ğ‘—
2ğ‘
ğ‘—"
WHENCE,0.46335078534031415,"
E ËœA,Ëœy,a,ğ‘¦"
WHENCE,0.4638743455497382,"
|ğ‘¦|2ğ‘âˆ’ğ‘¦(|Âµğ‘‡u| ğ‘—+ |(a âˆ’Âµ)ğ‘‡u| ğ‘—)
 â‰¤ 2ğ‘
âˆ‘ï¸"
WHENCE,0.4643979057591623,"ğ‘—=1
2 ğ‘—
2ğ‘
ğ‘— âˆšï¸ƒ"
WHENCE,0.4649214659685864,"E|ğ‘¦|4ğ‘âˆ’2ğ‘¦
âˆšï¸"
WHENCE,0.4654450261780105,"E|Âµğ‘‡u|2 ğ‘—+
âˆšï¸"
WHENCE,0.46596858638743455,"E|(a âˆ’Âµ)ğ‘‡u|2ğ‘—
"
WHENCE,0.4664921465968586,"By assumption E|ğ‘¦|2ğ‘âˆ’ğ‘¦â‰¤ğ¶for some constant ğ¶> 0 that is dimension-independent. Moreover, by"
WHENCE,0.46701570680628274,"our assumption, E|(a âˆ’Âµ)ğ‘‡u| ğ‘—â‰¤Ëœğ¶
âˆ¥uâˆ¥ğ‘—
2
ğ‘‘ğ‘—/2 . Also, from the previous lemma, E|Âµğ‘‡u|2ğ‘—= ğ‘‚(1). The
claim follows."
WHENCE,0.4675392670157068,"2. First note that âˆ¥u âˆ’wLâˆ¥3
3 = ğœ†3
ğ‘¦âˆ’ağ‘‡u
1+ğœ†ağ‘‡ğ›€âˆ’1a"
WHENCE,0.4680628272251309,"3
âˆ¥ğ›€âˆ’1aâˆ¥3
3 and since ğ›€â‰»0, 1 + ğœ†ağ‘‡ğ›€âˆ’1a > 0 hence by
Minkowskiâ€™s inequality"
WHENCE,0.468586387434555,"E ËœA,Ëœy,a,ğ‘¦âˆ¥u âˆ’wLâˆ¥3
3 â‰¤ğœ†3E ËœA,Ëœy,a,ğ‘¦|ğ‘ğ‘‡u âˆ’ğ‘¦|3âˆ¥ğ›€âˆ’1aâˆ¥3
3
â‰¤ğœ†3E ËœA,Ëœy,a,ğ‘¦|ğ‘ğ‘‡u âˆ’ğ‘¦|3âˆ¥ğ›€âˆ’1(a âˆ’Âµ + Âµ)âˆ¥3
3
â‰¤8ğœ†3E ËœA,Ëœy,a,ğ‘¦|ğ‘ğ‘‡u âˆ’ğ‘¦|3 max{âˆ¥ğ›€âˆ’1(a âˆ’Âµ)âˆ¥3, âˆ¥ğ›€âˆ’1Âµâˆ¥3}3"
WHENCE,0.46910994764397906,"â‰¤8ğœ†3E ËœA,Ëœy,a,ğ‘¦|ğ‘ğ‘‡u âˆ’ğ‘¦|3âˆ¥ğ›€âˆ’1(a âˆ’Âµ)âˆ¥3
3 + 8ğœ†3E ËœA,Ëœy,a,ğ‘¦|ğ‘ğ‘‡u âˆ’ğ‘¦|3|ğ›€âˆ’1Âµâˆ¥3
3"
WHENCE,0.46963350785340313,â‰¤8ğœ†3âˆšï¸ƒ
WHENCE,0.4701570680628272,"E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)6E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1(a âˆ’Âµ)âˆ¥6
3"
WHENCE,0.4706806282722513,+ 8ğœ†3âˆšï¸ƒ
WHENCE,0.4712041884816754,"E ËœA,Ëœy,a,ğ‘¦(ağ‘‡u âˆ’ğ‘¦)6E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1Âµâˆ¥6
3"
WHENCE,0.47172774869109946,"Now since we assumed that the objective is ğœŒ-strongly convex, or there exists ğœŒ> 0 that âˆ¥ğ›€âˆ’1âˆ¥ğ‘œğ‘â‰¤
1
ğœŒ, denoting Ï‰ğ‘—as the ğ‘—th row of ğ›€âˆ’1, we have by the inequality between Frobenius and Operator
norm, âˆ¥Ï‰ğ‘˜âˆ¥2 â‰¤1"
WHENCE,0.4722513089005236,ğœŒ. Now by the assumptions we have
WHENCE,0.47277486910994765,"E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1(a âˆ’Âµ)âˆ¥6
3 = E ËœA,Ëœy,a,ğ‘¦
 ğ‘‘
âˆ‘ï¸"
WHENCE,0.4732984293193717,"ğ‘—=1
|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘—|32"
WHENCE,0.4738219895287958,"= E ËœA,Ëœy ğ‘‘
âˆ‘ï¸"
WHENCE,0.4743455497382199,"ğ‘—=1
Eğ‘,ğ‘¦|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘—|6 + ğ‘‘
âˆ‘ï¸"
WHENCE,0.474869109947644,"ğ‘—,ğ‘™=1, ğ‘—â‰ ğ‘™
E|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘—|3|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘™|3 â‰¤ğ¶ ğ‘‘
âˆ‘ï¸ ğ‘—=1"
WHENCE,0.47539267015706804,"âˆ¥Ï‰ğ‘—âˆ¥6
2
ğ‘‘3
+ ğ¶ ğ‘‘
âˆ‘ï¸"
WHENCE,0.47591623036649217,"ğ‘—,ğ‘™=1, ğ‘—â‰ ğ‘™ âˆšï¸ƒ"
WHENCE,0.47643979057591623,"Ea|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘—|6Ea|(a âˆ’Âµ)ğ‘‡Ï‰ğ‘™|6 â‰¤ğ¶ğ‘‘
1
ğœŒ6ğ‘‘3 + ğ¶(ğ‘‘2 âˆ’ğ‘‘)
1
ğœŒ6ğ‘‘3 â‰¤
ğ¶
ğœŒ6ğ‘‘"
WHENCE,0.4769633507853403,We also have for the other term:
WHENCE,0.4774869109947644,"E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1Âµâˆ¥6
3 â‰¤E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1Âµâˆ¥6
2"
WHENCE,0.4780104712041885,"Note that âˆ¥ğ›€âˆ’1Âµâˆ¥2 â‰¤âˆ¥ğ›€âˆ’1/2âˆ¥ğ‘œğ‘âˆ¥ğ›€âˆ’1/2Âµâˆ¥2. Thus it is sufficient to analyze Âµğ‘‡ğ›€âˆ’1Âµğ‘‡. Let ğ‘¡âˆˆR
be an upper bound for Âµğ‘‡ğ›€âˆ’1Âµğ‘‡, that is Âµğ‘‡ğ›€âˆ’1Âµğ‘‡â‰¤ğ‘¡. Note that by the Schur complement
property, we need to find the smallest ğ‘¡> 0 such that ğ›€âˆ’1"
WHENCE,0.47853403141361256,"ğ‘¡ÂµÂµğ‘‡âª°0 with holds with high probability.
Recall that ğ›€:= ğœ†ËœAğ‘‡ËœA + H and by assumption H âª°ğœŒI, thus it suffices to find the largest ğ‘¡> 0
that ğœ†ËœAğ‘‡ËœA + ğœŒI âˆ’1"
WHENCE,0.4790575916230366,"ğ‘¡ÂµÂµğ‘‡âª°0. Let ËœAâ€² := ËœA âˆ’ËœM be centered. This problem is equivalent to having
âˆ¥( ËœAâ€² + ËœM)vâˆ¥2
2 + ğœŒâˆ¥vâˆ¥2
2 âˆ’1"
WHENCE,0.47958115183246075,ğ‘¡(Âµğ‘‡v)2 â‰¥0 for every v âˆˆRğ‘‘.
WHENCE,0.4801047120418848,Thus we should have
WHENCE,0.4806282722513089,"ğ‘¡â‰¥
(Âµğ‘‡v)2"
WHENCE,0.481151832460733,"âˆ¥( ËœAâ€² + ËœM)vâˆ¥2
2 + ğœŒâˆ¥vâˆ¥2
2"
WHENCE,0.4816753926701571,"Let ğ‘¡âˆ—:= supv
(Âµğ‘‡v)2"
WHENCE,0.48219895287958114,"âˆ¥( ËœAâ€²+ ËœM)vâˆ¥2
2+ğœŒâˆ¥vâˆ¥2
2 . We show ğ‘¡âˆ—= ğ‘‚(ğ‘›âˆ’ğ‘ ) for some ğ‘ > 0 with high probability. Let"
WHENCE,0.4827225130890052,"vâˆ—:= arg max
(Âµğ‘‡v)2"
WHENCE,0.48324607329842934,"âˆ¥( ËœAâ€²+ ËœM)vâˆ¥2
2+ğœŒâˆ¥vâˆ¥2
2 . This implies"
WHENCE,0.4837696335078534,"âˆ¥( ËœAâ€² + ËœM)vâˆ—âˆ¥2
2 â‰¤1"
WHENCE,0.48429319371727747,"ğ‘¡âˆ—(Âµğ‘‡vâˆ—)2 â‰¤
âˆ¥Âµâˆ¥2
2
ğ‘¡âˆ—
âˆ¥vâˆ—âˆ¥2
2"
WHENCE,0.4848167539267016,"On the other hand, by triangle inequality we have"
WHENCE,0.48534031413612566,(Âµğ‘‡vâˆ—)2 â‰¤1
WHENCE,0.48586387434554973,"ğ‘›â„“
âˆ¥ËœMvâˆ—âˆ¥2
2 â‰¤4 ğ‘›â„“"
WHENCE,0.4863874345549738,"
âˆ¥( ËœAâ€² + ËœM)vâˆ—âˆ¥2
2 + âˆ¥ËœAâ€²vâˆ—âˆ¥2
2 
â‰¤4 ğ‘›â„“"
WHENCE,0.4869109947643979," âˆ¥Âµâˆ¥2
2
ğ‘¡âˆ—
âˆ¥vâˆ—âˆ¥2
2 + âˆ¥ËœAâ€²âˆ¥2
ğ‘œğ‘âˆ¥vâˆ—âˆ¥2
2 "
WHENCE,0.487434554973822,. Now we have
WHENCE,0.48795811518324606,"ğ‘¡âˆ—=
(Âµğ‘‡vâˆ—)2"
WHENCE,0.4884816753926702,"âˆ¥( ËœAâ€² + ËœM)vâˆ—âˆ¥2
2 + ğœŒâˆ¥vâˆ—âˆ¥2
2
â‰¤ 4
ğ‘›â„“"
WHENCE,0.48900523560209425,"
âˆ¥Âµâˆ¥2
2
ğ‘¡âˆ—âˆ¥vâˆ—âˆ¥2
2 + âˆ¥ËœAâ€²âˆ¥2
ğ‘œğ‘âˆ¥vâˆ—âˆ¥2
2 "
WHENCE,0.4895287958115183,"ğœŒâˆ¥vâˆ—âˆ¥2
2
="
WHENCE,0.4900523560209424,"4âˆ¥Âµâˆ¥2
2
ğ‘¡âˆ—
+ 4âˆ¥ËœAâ€²âˆ¥2
ğ‘œğ‘
ğœŒğ‘›â„“"
WHENCE,0.4905759162303665,We obtain a quadratic inequality in ğ‘¡âˆ—
WHENCE,0.49109947643979057,"ğ‘¡âˆ—2 âˆ’
4âˆ¥ËœAâ€²âˆ¥2
ğ‘œğ‘
ğœŒğ‘›â„“
ğ‘¡âˆ—âˆ’
4âˆ¥Âµâˆ¥2
2
ğœŒğ‘›â„“
â‰¤0"
WHENCE,0.49162303664921464,Which implies the following upper bound
WHENCE,0.49214659685863876,"Âµğ‘‡ğ›€âˆ’1Âµğ‘‡â‰¤min{
4âˆ¥ËœAâ€²âˆ¥2
ğ‘œğ‘
ğœŒğ‘›â„“
+ 4 âˆšï¸„"
WHENCE,0.49267015706806283,"âˆ¥Âµâˆ¥2
2
ğœŒğ‘›â„“
, 1 ğœŒ}"
WHENCE,0.4931937172774869,"According to Lemma (8) âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰¤ğ‘›
1
3 +ğ‘ with probability 1 âˆ’ğ¶log(ğ‘‘+ğ‘›)"
WHENCE,0.493717277486911,"ğ‘›ğ‘ 
.Thus using the law of total
expectation, we also observe that"
WHENCE,0.4942408376963351,"E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1Âµâˆ¥6
3 â‰¤E ËœA,Ëœy,a,ğ‘¦âˆ¥ğ›€âˆ’1Âµâˆ¥6
2 â‰¤E ËœA,Ëœy,a,ğ‘¦(Âµğ‘‡ğ›€âˆ’1Âµ)3 â‰¤
4ğ‘›
2
3 +2ğ‘ "
WHENCE,0.49476439790575916,"ğœŒğ‘›â„“
+ 4 âˆšï¸„"
WHENCE,0.4952879581151832,"âˆ¥Âµâˆ¥2
2
ğœŒğ‘›â„“"
WHENCE,0.49581151832460735,"3
+ ğ¶log(ğ‘‘+ ğ‘›) ğœŒğ‘›ğ‘ "
WHENCE,0.4963350785340314,For 0 < ğ‘ < 1
WHENCE,0.4968586387434555,"6, as âˆ¥Âµâˆ¥2
2 = ğ‘‚(1) by assumption, the RHS goes to zero.
â–¡"
WHENCE,0.4973821989528796,"Lemma 8. Let ËœAâ€² be a centered random matrix following the assumptions. We have âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰¤ğ¶ğ‘›
1
3 +ğ‘ "
WHENCE,0.4979057591623037,"with probability at least 1 âˆ’ğ¶â€² log(ğ‘‘+ğ‘›) ğ‘›ğ‘ 
."
WHENCE,0.49842931937172774,"Proof. First, we provide an upper bound for the expectation E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘using the matrix Bernstein
inequality and the symmetrization technique. First we construct a corresponding hermitian matrix:"
WHENCE,0.4989528795811518,âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘=
WHENCE,0.49947643979057593,"
0
ËœAâ€²
ËœAâ€²ğ‘‡
0"
WHENCE,0.5,"
ğ‘œğ‘
= ğ‘›
âˆ‘ï¸"
WHENCE,0.5005235602094241,"ğ‘–=1
Xğ‘– ğ‘œğ‘"
WHENCE,0.5010471204188481,"With Xğ‘–:=

0
Eğ‘–ğ‘–ËœAâ€²
ËœAâ€²ğ‘‡Eğ‘–ğ‘–
0"
WHENCE,0.5015706806282723,"
where Eğ‘–ğ‘–is the all-zero matrix except (ğ‘–, ğ‘–)-entry where it is equal to 1."
WHENCE,0.5020942408376964,"Note that I = Ãğ‘›
ğ‘–=1 Eğ‘–ğ‘–and Eğ‘–ğ‘–ËœAâ€² essentially picks the iâ€™th row of ËœAâ€², which is Ëœa
â€²ğ‘‡
ğ‘–. Let {ğœ–ğ‘–}ğ‘›
ğ‘–=1 be an
iid sequence of symmetric Bernoulli random variables supported on {Â±1}, independent of Xğ‘–â€™s. Then
by the symmetrization lemma Vershynin [2018], we have"
WHENCE,0.5026178010471204,"E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘= E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5031413612565445,"ğ‘–=1
Xğ‘–"
WHENCE,0.5036649214659686,"ğ‘œğ‘
â‰¤E ËœAâ€²,Ïµ  ğ‘›
âˆ‘ï¸"
WHENCE,0.5041884816753927,"ğ‘–=1
ğœ–ğ‘–Xğ‘– ğ‘œğ‘"
WHENCE,0.5047120418848168,"Now conditioning on ËœAâ€² or equivalently Xğ‘–â€™s, since ğœ–Xğ‘–â€™s are independent zero-mean symmetric
matrices of size (ğ‘‘+ ğ‘›) Ã— (ğ‘‘+ ğ‘›) with bounded operator norm, we can leverage Matrix Bernstein
inequality Vershynin [2018] to obtain (â‰²means up to some constant)"
WHENCE,0.5052356020942408,"E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰²
âˆšï¸"
WHENCE,0.5057591623036649,"log(ğ‘‘+ ğ‘›)E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.506282722513089,"ğ‘–=1
Eğœ–ğ‘–
 ğœ–ğ‘–Xğ‘–
2 1/2"
WHENCE,0.506806282722513,"ğ‘œğ‘
+ log(ğ‘‘+ ğ‘›)E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5073298429319372,"Xğ‘–

ğ‘œğ‘ =
âˆšï¸"
WHENCE,0.5078534031413613,"log(ğ‘‘+ ğ‘›)E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5083769633507853,"ğ‘–=1
X2
ğ‘–  1/2"
WHENCE,0.5089005235602094,"ğ‘œğ‘
+ log(ğ‘‘+ ğ‘›)E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5094240837696336,"Xğ‘–

ğ‘œğ‘ =
âˆšï¸"
WHENCE,0.5099476439790576,log(ğ‘‘+ ğ‘›)E ËœAâ€² 
WHENCE,0.5104712041884817,"Ãğ‘›
ğ‘–=1 Eğ‘–ğ‘–ËœAâ€² ËœAâ€²ğ‘‡Eğ‘–ğ‘–
0
0
Ãğ‘›
ğ‘–=1 Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–  1/2"
WHENCE,0.5109947643979058,"ğ‘œğ‘
+ log(ğ‘‘+ ğ‘›)E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5115183246073298,"Xğ‘–

ğ‘œğ‘"
WHENCE,0.512041884816754,"Note that for
Xğ‘–

ğ‘œğ‘, we have
Xğ‘–

ğ‘œğ‘=
Eğ‘–ğ‘–ËœAâ€²
ğ‘œğ‘= max
âˆ¥vâˆ¥2=1"
WHENCE,0.512565445026178,"Eğ‘–ğ‘–ËœAâ€²v
2
2 = max
âˆ¥vâˆ¥2=1(Ëœa
â€²ğ‘‡
ğ‘–v)2 = âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2"
WHENCE,0.5130890052356021,"Moreover, using triangle inequality"
WHENCE,0.5136125654450262,"Ãğ‘›
ğ‘–=1 Eğ‘–ğ‘–ËœAâ€² ËœAâ€²ğ‘‡Eğ‘–ğ‘–
0
0
Ãğ‘›
ğ‘–=1 Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.5141361256544502,"
ğ‘œğ‘
â‰¤ ğ‘›
âˆ‘ï¸"
WHENCE,0.5146596858638743,"ğ‘–=1
Eğ‘–ğ‘–ËœAâ€² ËœAâ€²ğ‘‡Eğ‘–ğ‘– ğ‘œğ‘
+ ğ‘›
âˆ‘ï¸"
WHENCE,0.5151832460732985,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘ = ğ‘›
âˆ‘ï¸"
WHENCE,0.5157068062827225,"ğ‘–=1
âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2Eğ‘–ğ‘– ğ‘œğ‘
+ ğ‘›
âˆ‘ï¸"
WHENCE,0.5162303664921466,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5167539267015707,"= max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ‘›
âˆ‘ï¸"
WHENCE,0.5172774869109947,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5178010471204189,Combining these results along with Jensenâ€™s inequality yields
WHENCE,0.518324607329843,"E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰²
âˆšï¸"
WHENCE,0.518848167539267,"log(ğ‘‘+ ğ‘›)

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5193717277486911,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5198952879581152,"1/2
+ log(ğ‘‘+ ğ‘›)E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2"
WHENCE,0.5204188481675392,Now to analyze E ËœAâ€²
WHENCE,0.5209424083769634,"Ãğ‘›
ğ‘–=1 Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.5214659685863874,"ğ‘œğ‘
, we use the symmetrization trick again with iid Bernoulli {ğœ–â€²
ğ‘–}ğ‘›
ğ‘–=1"
WHENCE,0.5219895287958115,"and Bernsteinâ€™s similar to Vershyninâ€™s E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5225130890052356,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.5230366492146596,"ğ‘œğ‘
â‰¤E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5235602094240838,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–
âˆ’EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘
+ ğ‘›
âˆ‘ï¸"
WHENCE,0.5240837696335079,"ğ‘–=1
EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5246073298429319,"â‰¤E ËœAâ€²,Ïµâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.525130890052356,"ğ‘–=1
ğœ–â€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘
+ ğ‘›
âˆ‘ï¸ ğ‘–=1"
WHENCE,0.5256544502617801,"EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘ â‰²
âˆšï¸"
WHENCE,0.5261780104712042,"log ğ‘›E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5267015706806283,"ğ‘–=1
Eğœ–â€²
ğ‘–
 ğœ–â€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–
2 1/2"
WHENCE,0.5272251308900524,"ğ‘œğ‘
+ log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5277486910994764,"Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–âˆ¥ğ‘œğ‘+ ğ‘›
âˆ‘ï¸ ğ‘–=1"
WHENCE,0.5282722513089005,"EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘ =
âˆšï¸"
WHENCE,0.5287958115183246,"log ğ‘›E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5293193717277487,"ğ‘–=1
âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–  1/2"
WHENCE,0.5298429319371728,"ğ‘œğ‘
+ log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5303664921465968,"Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ‘›
âˆ‘ï¸ ğ‘–=1"
WHENCE,0.5308900523560209,"EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘ â‰²
âˆšï¸"
WHENCE,0.5314136125654451,"log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›"
WHENCE,0.5319371727748691,"Ëœaâ€²
ğ‘–âˆ¥2  ğ‘›
âˆ‘ï¸"
WHENCE,0.5324607329842932,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–  1/2"
WHENCE,0.5329842931937173,"ğ‘œğ‘
+ log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ‘›
âˆ‘ï¸ ğ‘–=1"
WHENCE,0.5335078534031413,"EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5340314136125655,"Now for the first term, applying Cauchy Schwartz yields"
WHENCE,0.5345549738219896,"E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2  ğ‘›
âˆ‘ï¸"
WHENCE,0.5350785340314136,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–  1/2"
WHENCE,0.5356020942408377,"ğ‘œğ‘
â‰¤

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5361256544502618,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘ 1/2"
WHENCE,0.5366492146596858,"Moreover, for the last term we have from our assumptions
EËœaâ€²
ğ‘–Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.53717277486911,"ğ‘œğ‘
= max
âˆ¥vâˆ¥2=1 EËœaâ€²
ğ‘–(Ëœaâ€²ğ‘‡
ğ‘–v)2 â‰¤max
âˆ¥vâˆ¥2=1 ğ¶
âˆ¥vâˆ¥2
2
ğ‘‘
= ğ¶"
WHENCE,0.537696335078534,"ğ‘‘
Thus we have E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5382198952879581,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.5387434554973822,"ğ‘œğ‘
â‰²
âˆšï¸"
WHENCE,0.5392670157068062,"log ğ‘›

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5397905759162304,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5403141361256545,"1/2
+ log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ¶"
WHENCE,0.5408376963350785,"Solving for

E ËœAâ€²"
WHENCE,0.5413612565445026,"Ãğ‘›
ğ‘–=1 Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.5418848167539267,"1/2
yields"
WHENCE,0.5424083769633508,"
E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5429319371727749,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘– ğ‘œğ‘"
WHENCE,0.543455497382199,"1/2
â‰²
âˆšï¸"
WHENCE,0.543979057591623,"log ğ‘›

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2"
WHENCE,0.5445026178010471,"1/2
+ 2

log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ¶
1/2 Thus E ËœAâ€²  ğ‘›
âˆ‘ï¸"
WHENCE,0.5450261780104712,"ğ‘–=1
Ëœaâ€²
ğ‘–Ëœaâ€²ğ‘‡
ğ‘–"
WHENCE,0.5455497382198953,"ğ‘œğ‘
â‰²log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ¶"
WHENCE,0.5460732984293194,"Summarising, we have"
WHENCE,0.5465968586387434,"E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰²
âˆšï¸"
WHENCE,0.5471204188481675,"log(ğ‘‘+ ğ‘›)

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + log ğ‘›E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 + ğ¶
1/2
+ log(ğ‘‘+ ğ‘›)E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2"
WHENCE,0.5476439790575917,"Thus it remains to provide an upper bound for E ËœAâ€² max1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2, we have by Jensenâ€™s

E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2"
WHENCE,0.5481675392670157,"3
â‰¤E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥6
2 â‰¤ ğ‘›
âˆ‘ï¸"
WHENCE,0.5486910994764398,"ğ‘–=1
EËœaâ€²
ğ‘–âˆ¥Ëœaâ€²
ğ‘–âˆ¥6
2"
WHENCE,0.5492146596858639,Now by Definition 2 and Holderâ€™s we have
WHENCE,0.5497382198952879,"EËœaâ€²
ğ‘–âˆ¥Ëœaâ€²
ğ‘–âˆ¥6
2 =
âˆ‘ï¸"
WHENCE,0.550261780104712,"ğ›¼1,ğ›¼2,ğ›¼3
EËœaâ€²
ğ‘–Ëœa
â€²2
ğ‘–ğ›¼1 Ëœa
â€²2
ğ‘–ğ›¼2 Ëœa
â€²2
ğ‘–ğ›¼3 â‰¤
âˆ‘ï¸"
WHENCE,0.5507853403141362,"ğ›¼1,ğ›¼2,ğ›¼3"
WHENCE,0.5513089005235602,"
EËœaâ€²
ğ‘–Ëœa
â€²6
ğ‘–ğ›¼1"
WHENCE,0.5518324607329843,"1/3
EËœaâ€²
ğ‘–Ëœa
â€²6
ğ‘–ğ›¼2"
WHENCE,0.5523560209424084,"1/3
EËœaâ€²
ğ‘–Ëœa
â€²6
ğ‘–ğ›¼3"
WHENCE,0.5528795811518324,"1/3
â‰¤
âˆ‘ï¸"
WHENCE,0.5534031413612566,"ğ›¼1,ğ›¼2,ğ›¼3"
WHENCE,0.5539267015706806,"ğ¶
ğ‘‘3 â‰¤ğ¶"
WHENCE,0.5544502617801047,"Thus
E ËœAâ€² max
1â‰¤ğ‘–â‰¤ğ‘›âˆ¥Ëœaâ€²
ğ‘–âˆ¥2
2 â‰¤ğ¶ğ‘›
1
3"
WHENCE,0.5549738219895288,"All in all
E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰²log(ğ‘‘+ ğ‘›)ğ‘›
1
3
Thus by Markovâ€™s inequality for ğ‘ > 0"
WHENCE,0.5554973821989528,"P âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘â‰¥ğ¶ğ‘›
1
3 +ğ‘  â‰¤E ËœAâ€² âˆ¥ËœAâ€²âˆ¥ğ‘œğ‘"
WHENCE,0.556020942408377,"ğ¶ğ‘›
1
3 +ğ‘ 
â‰¤ğ¶â€² log(ğ‘‘+ ğ‘›)ğ‘›
1
3"
WHENCE,0.5565445026178011,"ğ‘›
1
3 +ğ‘ 
= ğ¶â€² log(ğ‘‘+ ğ‘›) ğ‘›ğ‘ 
â–¡"
WHENCE,0.5570680628272251,"Lemma 9. For any function ğœ“: R â†’R with bounded third derivative, Ëœğœ“(w) := Eağœ“(ağ‘‡w) is regular"
WHENCE,0.5575916230366492,"Proof. We only need to verify the third order Taylor remainder bound,
âˆ‘ï¸"
WHENCE,0.5581151832460733,"ğ›¼1,ğ›¼2,ğ›¼3"
WHENCE,0.5586387434554974,"ğœ•3 Ëœğœ“
ğœ•ğ‘¤ğ›¼1ğœ•ğ‘¤ğ›¼2ğœ•ğ‘¤ğ›¼3
ğ‘£ğ›¼1ğ‘£ğ›¼2ğ‘£ğ›¼3 =
âˆ‘ï¸"
WHENCE,0.5591623036649215,"ğ›¼1,ğ›¼2,ğ›¼3
Eağœ“â€²â€²â€²(ağ‘‡w)ğ‘ğ›¼1ğ‘ğ›¼2ğ‘ğ›¼3ğ‘£ğ›¼1ğ‘£ğ›¼2ğ‘£ğ›¼3 = Ea(ağ‘‡v)3ğœ“â€²â€²â€²(ağ‘‡w)"
WHENCE,0.5596858638743456,"By Cauchy Schwartz, we have
âˆ‘ï¸"
WHENCE,0.5602094240837696,"ğ›¼1,ğ›¼2,ğ›¼3"
WHENCE,0.5607329842931937,"ğœ•3 Ëœğœ“
ğœ•ğ‘¤ğ›¼1ğœ•ğ‘¤ğ›¼2ğœ•ğ‘¤ğ›¼3
ğ‘£ğ›¼1ğ‘£ğ›¼2ğ‘£ğ›¼3 â‰¤
âˆšï¸ƒ"
WHENCE,0.5612565445026177,"Eağœ“â€²â€²â€²(ağ‘‡w)2
âˆšï¸ƒ"
WHENCE,0.5617801047120419,Ea(ağ‘‡v)6
WHENCE,0.562303664921466,"By boundedness of the third derivative of ğœ“, and assumptions
âˆ‘ï¸"
WHENCE,0.56282722513089,"ğ›¼1,ğ›¼2,ğ›¼3"
WHENCE,0.5633507853403141,"ğœ•3 Ëœğœ“
ğœ•ğ‘¤ğ›¼1ğœ•ğ‘¤ğ›¼2ğœ•ğ‘¤ğ›¼3
ğ‘£ğ›¼1ğ‘£ğ›¼2ğ‘£ğ›¼3 â‰¤ğ¶ğœ“
âˆ¥vâˆ¥3
2
ğ‘‘3/2 â‰¤ğ¶ğœ“âˆ¥vâˆ¥3
3 â–¡"
WHENCE,0.5638743455497383,"E
Generalized CGMT"
WHENCE,0.5643979057591623,"After reducing the analysis to the equivalent gaussian model, one can leverage a new generalization
of the CGMT Akhtiamov et al. [2024] to analyze Î¦(G) which is stated as follows and recover the
precise asymptotic properties of the solutions."
WHENCE,0.5649214659685864,"Theorem 5 (Generalized CGMT). Let Sğ‘¤âŠ‚Rğ‘‘, Sğ‘£1 âŠ‚Rğ‘›1, . . . , Sğ‘£ğ‘˜âŠ‚Rğ‘›ğ‘˜be compact convex
sets. Denote Sğ‘£:= Sğ‘£1 Ã— Â· Â· Â· Ã— Sğ‘£ğ‘˜, let v âˆˆSğ‘£stand for (v1, . . . , vğ‘˜) âˆˆSğ‘£1 Ã— Â· Â· Â· Ã— Sğ‘£ğ‘˜and
ğœ“(w, v) : Sğ‘¤Ã— Sğ‘£â†’R be convex on Sğ‘¤and concave on Sğ‘£. Also let ğšº1, . . . , ğšºğ‘˜âˆˆRğ‘‘Ã—ğ‘‘
be arbitrary PSD matrices. Furthermore, let G1 âˆˆRğ‘›1Ã—ğ‘‘, . . . , Gğ‘˜âˆˆRğ‘›ğ‘˜Ã—ğ‘‘, g1, . . . , gğ‘˜âˆˆRğ‘‘,
h1 âˆˆRğ‘›1, . . . , hğ‘˜âˆˆRğ‘›ğ‘˜all have i.i.d N (0, 1) components and G = (G1, . . . , Gğ‘˜), g = (g1, . . . , gğ‘˜),
h = (h1, . . . , hğ‘˜) be the corresponding ğ‘˜-tuples. Define"
WHENCE,0.5654450261780105,"Î¨(G) := min
wâˆˆSğ‘¤max
vâˆˆSğ‘£ ğ‘˜
âˆ‘ï¸"
WHENCE,0.5659685863874345,"â„“=1
vğ‘‡
â„“Gâ„“ğšº"
WHENCE,0.5664921465968586,"1
2
â„“w + ğœ“(w, v)
(26)"
WHENCE,0.5670157068062828,"ğœ™(g, h) := min
wâˆˆSğ‘¤max
vâˆˆSğ‘£ ğ‘˜
âˆ‘ï¸ â„“=1"
WHENCE,0.5675392670157068,"
âˆ¥vâ„“âˆ¥2gğ‘‡
â„“ğšº"
WHENCE,0.5680628272251309,"1
2
â„“w + âˆ¥ğšº"
WHENCE,0.5685863874345549,"1
2
â„“wâˆ¥2hğ‘‡
â„“vâ„“"
WHENCE,0.569109947643979,"
+ ğœ“(w, v)
(27)"
WHENCE,0.5696335078534032,"Let S be an arbitrary open subset of Sğ‘¤and Sğ‘= Sğ‘¤\ S. Let ğœ™S be the optimal value of the
optimization 27 when Sğ‘¤is restricted to S. Assume also that there exist ğœ–, ğ›¿> 0, Â¯ğœ™, Â¯ğœ™Sğ‘such that"
WHENCE,0.5701570680628272,â€¢ Â¯ğœ™Sğ‘â‰¥Â¯ğœ™+ 3ğ›¿
WHENCE,0.5706806282722513,"â€¢ ğœ™(ğ‘”, â„) < Â¯ğœ™+ ğ›¿with probability at least 1 âˆ’ğœ–"
WHENCE,0.5712041884816754,â€¢ ğœ™Sğ‘> Â¯ğœ™Sğ‘âˆ’ğ›¿with probability at least 1 âˆ’ğœ–
WHENCE,0.5717277486910994,"Then for the optimal solution w of the optimization 26, we have P(wÎ¨(G) âˆˆS) â‰¥1 âˆ’2ğ‘˜+1ğœ–"
WHENCE,0.5722513089005236,"F
Proof of Theorem 2"
WHENCE,0.5727748691099477,"By leveraging our universality theorem 1 we are ready to analyze the linear regression problem
descrided in 2 in full details"
WHENCE,0.5732984293193717,"Proof. First note that for the ground truth we have ğ‘¤âˆ—= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›Eğ‘ƒ(ğ‘¦âˆ’xğ‘‡w)2 = Râˆ’1
ğ‘¥Rğ‘¥ğ‘¦. By a
simple transformation, the constraint could also be written as"
WHENCE,0.5738219895287958," X
y 
w
âˆ’1 
= 0"
WHENCE,0.5743455497382199,"We would like to write for a G with i.i.d entries:
 X
y = GS"
WHENCE,0.574869109947644,Indeed
WHENCE,0.5753926701570681,"Sğ‘‡EGğ‘‡GS = Sğ‘‡S = E

Xğ‘‡ yğ‘‡"
WHENCE,0.5759162303664922,"  X
y = E

Rğ‘¥
Rğ‘¥ğ‘¦
Rğ‘¦ğ‘¥
ğ‘…ğ‘¦ "
WHENCE,0.5764397905759162,Now we can denote S =
WHENCE,0.5769633507853403,"R1/2
ğ‘¥
Râˆ’ğ‘‡/2
ğ‘¥
Rğ‘¥ğ‘¦
0
ğ‘…1/2
Î” !"
WHENCE,0.5774869109947643,"with ğ‘…Î” := ğ‘…ğ‘¦âˆ’Rğ‘¦ğ‘¥Râˆ’1
ğ‘¥Rğ‘¥ğ‘¦. And Râˆ’ğ‘‡/2
ğ‘¥
Rğ‘¥ğ‘¦= R1/2
ğ‘¥wâˆ—."
WHENCE,0.5780104712041885,Thus we can write the optimization as
WHENCE,0.5785340314136126,"min
w âˆ¥w âˆ’w0âˆ¥2 ğ‘ .ğ‘¡
G"
WHENCE,0.5790575916230366,"R1/2
ğ‘¥(w âˆ’wâˆ—)
âˆ’ğ‘…1/2
Î” ! = 0"
WHENCE,0.5795811518324607,we can write the optimization as
WHENCE,0.5801047120418849,"min
w âˆ¥w âˆ’w0âˆ¥2 ğ‘ .ğ‘¡
G"
WHENCE,0.5806282722513089,"R1/2
ğ‘¥(w âˆ’wâˆ—)
âˆ’ğ‘…1/2
Î” ! = 0"
WHENCE,0.581151832460733,"Let u := R1/2
ğ‘¥(w âˆ’wâˆ—) and x = wâˆ—âˆ’w0, then"
WHENCE,0.5816753926701571,"min
w âˆ¥Râˆ’1/2
ğ‘¥
u + xâˆ¥2"
WHENCE,0.5821989528795811,"ğ‘ .ğ‘¡
G

u
âˆ’ğ‘…1/2
Î” 
= 0"
WHENCE,0.5827225130890052,We Lagrange multiplier to bring in the constraint:
WHENCE,0.5832460732984294,"min
u max
v
âˆ¥Râˆ’1/2
ğ‘¥
u + xâˆ¥2 + vğ‘‡G

u
âˆ’ğ‘…1/2
Î” "
WHENCE,0.5837696335078534,Now we can appeal to CGMT to obtain the AO:
WHENCE,0.5842931937172775,"min
u max
v
âˆ¥Râˆ’1/2
ğ‘¥
u + xâˆ¥2 + âˆ¥vâˆ¥(gğ‘‡u + ğ‘”â€²ğ‘…1/2
Î” ) + âˆ¥

u
âˆ’ğ‘…1/2
Î”"
WHENCE,0.5848167539267015,"
âˆ¥hğ‘‡v"
WHENCE,0.5853403141361256,"Dropping the ğ‘”â€²ğ‘…1/2
Î”
term and doing the optimization over the direction of v, yields"
WHENCE,0.5858638743455498,"min
u max
ğ›½â‰¥0 âˆ¥Râˆ’1/2
ğ‘¥
u + xâˆ¥2 + ğ›½âˆ¥hâˆ¥
âˆšï¸"
WHENCE,0.5863874345549738,âˆ¥uâˆ¥2 + ğ‘…Î” + ğ›½gğ‘‡u
WHENCE,0.5869109947643979,Using the identity âˆšğ‘¥= minğœâ‰¥0 ğœ
WHENCE,0.587434554973822,2 + ğ‘¥2 2ğœ
WHENCE,0.587958115183246,"min
ğœâ‰¥0,u max
ğ›½â‰¥0 âˆ¥Râˆ’1/2
ğ‘¥
u + xâˆ¥2 + ğ›½âˆ¥hâˆ¥ğœ"
WHENCE,0.5884816753926702,"2
+ ğ›½âˆ¥hâˆ¥"
WHENCE,0.5890052356020943,2ğœ(âˆ¥uâˆ¥2 + ğ‘…Î”) + ğ›½gğ‘‡u
WHENCE,0.5895287958115183,Now we obtain a quadratic optimization over ğ‘¢which could be rewritten as
WHENCE,0.5900523560209424," uğ‘‡
1
 
Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
WHENCE,0.5905759162303665,"2ğœI
Râˆ’1/2
ğ‘¥
x + ğ›½"
G,0.5910994764397905,"2 g
xğ‘‡Râˆ’ğ‘‡/2
ğ‘¥
+ ğ›½"
G,0.5916230366492147,"2 gğ‘‡
âˆ¥xâˆ¥2"
G,0.5921465968586388,"! 
u
1 "
G,0.5926701570680628,"Doing the optimization over u, we are left with"
G,0.5931937172774869,"min
ğœâ‰¥0 max
ğ›½â‰¥0 âˆ¥xâˆ¥2 + ğ›½ğœâˆšğ‘›"
G,0.5937172774869109,"2
+ ğ›½âˆšğ‘›"
G,0.5942408376963351,"2ğœğ‘…Î” âˆ’(Râˆ’1/2
ğ‘¥
x + ğ›½"
G,0.5947643979057592,"2 g)ğ‘‡(Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
G,0.5952879581151832,"2ğœI)âˆ’1(Râˆ’1/2
ğ‘¥
x + ğ›½ 2 g)"
G,0.5958115183246073,Expanding the third term yields
G,0.5963350785340314,"min
ğœâ‰¥0 max
ğ›½â‰¥0 âˆ¥xâˆ¥2 + ğ›½ğœâˆšğ‘›"
G,0.5968586387434555,"2
+ ğ›½âˆšğ‘›"
G,0.5973821989528796,"2ğœğ‘…Î” âˆ’xğ‘‡Râˆ’ğ‘‡/2
ğ‘¥
(Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
G,0.5979057591623037,"2ğœI)âˆ’1Râˆ’1/2
ğ‘¥
x âˆ’ğ›½2"
G,0.5984293193717277,"4 gğ‘‡(Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
G,0.5989528795811518,2ğœI)âˆ’1g
G,0.599476439790576,"Let z := R1/2
ğ‘¥x, we assume Ezzğ‘‡= âˆ¥zâˆ¥2"
G,0.6,"ğ‘‘ğ¼, and ğ‘’ğ‘= xğ‘‡Rğ‘¥x = âˆ¥zâˆ¥2 using concentration:"
G,0.6005235602094241,"min
ğœâ‰¥0 max
ğ›½â‰¥0 âˆ¥ğ‘¥âˆ¥2 + ğ›½ğœâˆšğ‘›"
G,0.6010471204188481,"2
+ ğ›½âˆšğ‘›"
G,0.6015706806282722,2ğœğ‘…Î” âˆ’ğ‘‘1
G,0.6020942408376964,"ğ‘‘TrRâˆ’ğ‘‡
ğ‘¥(Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
G,0.6026178010471204,"2ğœI)âˆ’1Râˆ’1
ğ‘¥âˆ’ğ‘‘ğ›½2"
G,0.6031413612565445,"4
1
ğ‘‘Tr(Râˆ’1
ğ‘¥+ ğ›½âˆšğ‘›"
G,0.6036649214659686,2ğœI)âˆ’1
G,0.6041884816753926,Therefore
G,0.6047120418848168,"min
ğœâ‰¥0 max
ğ›½â‰¥0 âˆ¥xâˆ¥2 + ğ›½ğœâˆšğ‘›"
G,0.6052356020942409,"2
+ ğ›½âˆšğ‘›"
G,0.6057591623036649,2ğœğ‘…Î” âˆ’ğ‘’ğ‘
G,0.606282722513089,"âˆ«
ğ‘(ğ‘Ÿ)
1"
G,0.6068062827225131,ğ‘Ÿ(1 + ğ›½âˆšğ‘›
G,0.6073298429319371,"2ğœğ‘Ÿ)
ğ‘‘ğ‘Ÿâˆ’ğ‘‘ğ›½2 4"
G,0.6078534031413613,"âˆ«
ğ‘(ğ‘Ÿ)
ğ‘Ÿ"
G,0.6083769633507854,1 + ğ›½âˆšğ‘›
G,0.6089005235602094,"2ğœğ‘Ÿ
ğ‘‘ğ‘Ÿ"
G,0.6094240837696335,"Dropping the constant term âˆ¥xâˆ¥2, we arrive at the following scalarized optimization"
G,0.6099476439790575,"min
ğœâ‰¥0 max
ğ›½â‰¥0
ğ›½ğœâˆšğ‘›"
G,0.6104712041884817,"2
+ ğ›½âˆšğ‘›"
G,0.6109947643979058,"2ğœğ‘…Î” âˆ’ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğ‘’ğ‘ ğ‘‘+ ğ›½2 4 ğ‘Ÿ2"
G,0.6115183246073298,ğ‘Ÿ(1 + ğ›½âˆšğ‘›
G,0.6120418848167539,"2ğœğ‘Ÿ)
ğ‘‘ğ‘Ÿ"
G,0.612565445026178,"Let ğ‘’â€²
ğ‘:= ğ‘’ğ‘"
G,0.6130890052356021,"ğ‘‘. To analyze the optimization further, noting that the objective is differentiable w.r.t ğ›½, ğœ,
by taking derivatives we have at the optimal point: ğœâˆšğ‘›"
G,0.6136125654450262,"2
+
âˆšğ‘›ğ‘…Î”"
G,0.6141361256544503,"2ğœ
âˆ’ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ(âˆšğ‘›ğ‘Ÿ2ğ›½2 + 4ğ‘Ÿğœğ›½âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›)
2(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2
ğ‘‘ğ‘Ÿ= 0 ğ›½âˆšğ‘›"
G,0.6146596858638743,"2
âˆ’ğ›½ğ‘…Î”
âˆšğ‘›
2ğœ2
âˆ’ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğ›½âˆšğ‘›(ğ›½2ğ‘Ÿ2 + 4ğ‘’â€²
ğ‘)
2(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2 ğ‘‘ğ‘Ÿ= 0"
G,0.6151832460732984,Rearranging
G,0.6157068062827226,"ğœ2âˆšğ‘›+ âˆšğ‘›ğ‘…Î” = ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ2(âˆšğ‘›ğ‘Ÿ2ğ›½2 + 4ğ‘Ÿğœğ›½âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›)
(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2
ğ‘‘ğ‘Ÿ"
G,0.6162303664921466,"ğœ2âˆšğ‘›âˆ’âˆšğ‘›ğ‘…Î” = ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ2(âˆšğ‘›ğ›½2ğ‘Ÿ2 + âˆšğ‘›4ğ‘’â€²
ğ‘)
(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2
ğ‘‘ğ‘Ÿ"
G,0.6167539267015707,Summing and subtracting:
G,0.6172774869109947,"ğ‘›= ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğ‘›ğ‘Ÿ2ğ›½2 + 2âˆšğ‘›ğ‘Ÿğœğ›½"
G,0.6178010471204188,"(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2 ğ‘‘ğ‘Ÿ= ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ)(1 âˆ’2âˆšğ‘›ğ‘Ÿğœğ›½+ 4ğœ2"
G,0.618324607329843,(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2 )ğ‘‘ğ‘Ÿ
G,0.618848167539267,"âˆšğ‘›ğ‘…Î” = ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ2(2ğ‘Ÿğœğ›½âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›)
(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2
ğ‘‘ğ‘Ÿ Thus ğ‘‘âˆ’ğ‘›"
G,0.6193717277486911,"ğ‘‘
=
âˆ«
ğ‘(ğ‘Ÿ) 2âˆšğ‘›ğ‘Ÿğœğ›½+ 4ğœ2"
G,0.6198952879581152,"(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2 ğ‘‘ğ‘Ÿ=
âˆ«
ğ‘(ğ‘Ÿ)
2ğœ
2ğœ+ ğ‘Ÿğ›½âˆšğ‘›ğ‘‘ğ‘Ÿ= 2ğœ"
G,0.6204188481675392,ğ›½âˆšğ‘›ğ‘†ğ‘ƒ(âˆ’2ğœ
G,0.6209424083769634,ğ›½âˆšğ‘›+ ğ‘–0)
G,0.6214659685863875,"âˆšğ‘›ğ‘…Î” = ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ2(2ğ‘Ÿğœğ›½âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›)
(2ğœ+ ğ‘Ÿğ›½âˆšğ‘›)2
ğ‘‘ğ‘Ÿ"
G,0.6219895287958115,If one can solve ğ‘‘âˆ’ğ‘›
G,0.6225130890052356,"ğ‘‘
=
2ğœ
ğ›½âˆšğ‘›ğ‘†ğ‘ƒ(âˆ’2ğœ"
G,0.6230366492146597,ğ›½âˆšğ‘›+ ğ‘–0) to find ğœƒ:= ğ›½
G,0.6235602094240837,"ğœ, then plugging into the second equation
yields"
G,0.6240837696335079,"âˆšğ‘›ğ‘…2
Î” = ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) ğœ2(2ğ‘Ÿğœƒğœ2 âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›)
(2ğœ+ ğ‘Ÿğœƒğœâˆšğ‘›)2
ğ‘‘ğ‘Ÿ= ğ‘‘
âˆ«
ğ‘(ğ‘Ÿ) 2ğ‘Ÿğœƒğœ2 âˆ’4ğ‘’â€²
ğ‘
âˆšğ‘›
(2 + ğ‘Ÿğœƒâˆšğ‘›)2
ğ‘‘ğ‘Ÿ"
G,0.624607329842932,Which yields the following equation for ğœ2
G,0.625130890052356,"ğœ2 =
âˆšğ‘›"
G,0.6256544502617801,"2ğœƒğ‘‘
âˆ«
ğ‘(ğ‘Ÿ)
ğ‘Ÿ
(2+ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ"
G,0.6261780104712041,"
ğ‘…Î” + 4ğ‘‘ğ‘’â€²
ğ‘"
G,0.6267015706806283,"âˆ«
ğ‘(ğ‘Ÿ)
(2 + ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ
"
G,0.6272251308900524,"Now note that
ğ‘‘âˆ’ğ‘›"
G,0.6277486910994764,"ğ‘‘
=
âˆ«
ğ‘(ğ‘Ÿ)
2ğœ
2ğœ+ ğ‘Ÿğ›½âˆšğ‘›ğ‘‘ğ‘Ÿ=
âˆ«
ğ‘(ğ‘Ÿ)
2
2 + ğ‘Ÿğœƒâˆšğ‘›ğ‘‘ğ‘Ÿ=
âˆ«
ğ‘(ğ‘Ÿ) 2(2 + ğ‘Ÿğœƒâˆšğ‘›)"
G,0.6282722513089005,(2 + ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ=
G,0.6287958115183246,"= 2ğœƒâˆšğ‘›
âˆ«
ğ‘(ğ‘Ÿ)
ğ‘Ÿ
(2 + ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ+ 4
âˆ«
ğ‘(ğ‘Ÿ)
(2 + ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ"
G,0.6293193717277487,"Let ğ›¼:= 4
âˆ«
ğ‘(ğ‘Ÿ)
(2+ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ, ğ›¼â€² := 2ğœƒâˆšğ‘›
âˆ«
ğ‘(ğ‘Ÿ)
ğ‘Ÿ
(2+ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿand . Then the expression for ğœcould be
written as"
G,0.6298429319371728,"ğœ2 =
ğ‘›
ğ‘‘ğ›¼â€² (ğ‘…Î” + ğµğ‘’ğ‘)"
G,0.6303664921465969,As ğ›¼+ ğ›¼â€² = ğ‘‘âˆ’ğ‘›
G,0.6308900523560209,"ğ‘‘, one can write"
G,0.631413612565445,"ğœ2 =
ğ‘›"
G,0.6319371727748692,ğ‘‘( ğ‘‘âˆ’ğ‘›
G,0.6324607329842932,"ğ‘‘
âˆ’ğ›¼)
(ğ‘…Î” + ğ›¼ğ‘’ğ‘) =
ğ‘›
ğ‘‘âˆ’ğ‘›âˆ’ğ›¼ğ‘‘(ğ‘…Î” + ğ›¼ğ‘’ğ‘) =
ğ‘›
ğ‘‘(1 âˆ’ğ›¼) âˆ’ğ‘›ğ‘…Î” +
ğ›¼ğ‘›
ğ‘‘(1 âˆ’ğ›¼) âˆ’ğ‘›ğ‘’ğ‘"
G,0.6329842931937173,Now the generalization error is
G,0.6335078534031413,ğœ2 âˆ’ğ‘…Î” = 2ğ‘›âˆ’ğ‘‘(1 âˆ’ğ›¼)
G,0.6340314136125654,"ğ‘‘(1 âˆ’ğ›¼) âˆ’ğ‘›ğ‘…Î” +
ğ‘›ğµ
ğ‘‘(1 âˆ’ğ›¼) âˆ’ğ‘›ğ‘’ğ‘= (2ğ‘›âˆ’ğ‘‘)ğ‘…Î” + (ğ‘‘ğ‘…Î” + ğ‘›ğ‘’ğ‘)ğ›¼"
G,0.6345549738219896,ğ‘‘âˆ’ğ‘›âˆ’ğ‘‘ğ›¼
G,0.6350785340314136,"With ğ›¼= 4
âˆ«
ğ‘(ğ‘Ÿ)
(2+ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ. Now note that using Cauchy Schwarz applied to
âˆšï¸"
G,0.6356020942408377,"ğ‘(ğ‘Ÿ) and
2âˆš"
G,0.6361256544502618,"ğ‘(ğ‘Ÿ)
2+ğ‘Ÿğœƒâˆšğ‘› ( ğ‘‘âˆ’ğ‘›"
G,0.6366492146596858,"ğ‘‘
)2 = (
âˆ«
ğ‘(ğ‘Ÿ)
2
2 + ğ‘Ÿğœƒâˆšğ‘›ğ‘‘ğ‘Ÿ)2 â‰¤
âˆ«
ğ‘(ğ‘Ÿ)ğ‘‘ğ‘Ÿ
âˆ«
4ğ‘(ğ‘Ÿ)
(2 + ğ‘Ÿğœƒâˆšğ‘›)2 ğ‘‘ğ‘Ÿ= ğ›¼"
G,0.63717277486911,Thus ğ›¼â‰¥( ğ‘‘âˆ’ğ‘›
G,0.6376963350785341,"ğ‘‘)2. Now for the scalar distribution ğ‘(ğ‘Ÿ) = ğ›¿(ğ‘Ÿâˆ’ğ‘Ÿ0), since 2+ğ‘Ÿ0ğœƒâˆšğ‘›= 2+ 2ğ‘›"
G,0.6382198952879581,"ğ‘‘âˆ’ğ‘›=
2ğ‘‘
ğ‘‘âˆ’ğ‘›,
then ğ›¼= ( ğ‘‘âˆ’ğ‘›"
G,0.6387434554973822,"ğ‘‘)2. Thus ğ›¼achieves its lower bound (uniquely) for the scalar distribution. Now
note that assuming ğ‘›, ğ‘‘, ğ‘’ğ‘, ğ‘…Î” are fixed, the generalization error is increasing in ğ›¼, thus the error
expression for the generalization error in the scalar distribution case is a lower bound.
â–¡"
G,0.6392670157068063,"G
Useful Results"
G,0.6397905759162303,"Lemma 10. Given a weight vector w, and assuming the feature vectors are equally likely to be drawn
from class 1 or class 2 and satisfy part 2 of Assumptions 4, the corresponding classification error is
given by"
G,0.6403141361256545,"1
2ğ‘„(
Âµğ‘‡
1 w
âˆšï¸"
G,0.6408376963350786,"wğ‘‡ğšº1w
) + 1"
G,0.6413612565445026,"2ğ‘„(âˆ’
Âµğ‘‡
2 w
âˆšï¸"
G,0.6418848167539267,"wğ‘‡ğšº2w
)
(28)"
G,0.6424083769633507,"Lemma 11. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2,
Assumptions 4 hold and w0 is defined as in Assumption 2. Then the classification error corresponding
to w0 is equal to"
G,0.6429319371727749,"1
2ğ‘„
Â©Â­Â­Â­Â­
Â« âˆš"
G,0.643455497382199,"1 âˆ’ğ‘ŸTr(ğšº1 + ğšº2)âˆ’1
âˆšï¸‚"
G,0.643979057591623,"2Tr(ğšº1(ğšº1 + ğšº2)âˆ’2) +
ğ‘¡2ğœ‚
ğ‘¡2âˆ—Trğšº1"
G,0.6445026178010471,"ÂªÂ®Â®Â®Â®
Â¬ + 1"
G,0.6450261780104712,"2ğ‘„
Â©Â­Â­Â­Â­
Â« âˆš"
G,0.6455497382198953,"1 âˆ’ğ‘ŸTr(ğšº1 + ğšº2)âˆ’1
âˆšï¸‚"
G,0.6460732984293194,"2Tr(ğšº2(ğšº1 + ğšº2)âˆ’2) +
ğ‘¡2ğœ‚
ğ‘¡2âˆ—Trğšº2"
G,0.6465968586387435,"ÂªÂ®Â®Â®Â®
Â¬"
G,0.6471204188481675,"H
Proof of Lemma 1"
G,0.6476439790575916,Let us proceed to prove Lemma 1. We will take the gradient of (28) w.r.t. ğ‘¤and set it to 0:
G,0.6481675392670158,Proof. âˆ‡w
G,0.6486910994764398,"""
1
2ğ‘„(âˆ’
Âµğ‘‡
2 w
âˆšï¸"
G,0.6492146596858639,"wğ‘‡ğšº2w
) # =
1 4
âˆš 2ğœ‹ """
G,0.6497382198952879,"exp (âˆ’
wğ‘‡Âµ2Âµğ‘‡
2 w"
G,0.650261780104712,2wğ‘‡ğšº2w ) âˆšï¸ wğ‘‡ğšº2w
G,0.6507853403141362,"Âµğ‘‡
2 w"
G,0.6513089005235602,"wğ‘‡ğšº2wÂµ2Âµğ‘‡
2 w âˆ’wğ‘‡Âµ2Âµğ‘‡
2 wğšº2w"
G,0.6518324607329843,"(wğ‘‡ğšº2w)2 # = 1 4
âˆš 2ğœ‹"
G,0.6523560209424084,"""
1
ğ‘ 2
ğ‘’ğ‘¥ğ‘(âˆ’
ğ‘ 2
2
2 )(Âµ2ğ‘¡2 âˆ’ğ‘ 2
2ğšº2w) #"
G,0.6528795811518324,"where ğ‘ 2 =
Âµğ‘‡
2 w
âˆšï¸"
G,0.6534031413612565,"wğ‘‡ğšº2w
, ğ‘¡2 =
Âµğ‘‡
2 w wğ‘‡ğšº2w âˆ‡w"
G,0.6539267015706807,"""
1
2ğ‘„(
Âµğ‘‡
1 w
âˆšï¸"
G,0.6544502617801047,"wğ‘‡ğšº1w
) # = âˆ’
1 4
âˆš 2ğœ‹ """
G,0.6549738219895288,"exp (âˆ’
wğ‘‡Âµ1Âµğ‘‡
1 w"
G,0.6554973821989529,2wğ‘‡ğšº1w ) âˆšï¸ wğ‘‡ğšº1w
G,0.6560209424083769,"Âµğ‘‡
1 w"
G,0.6565445026178011,"wğ‘‡ğšº1wÂµ1Âµğ‘‡
1 w âˆ’wğ‘‡Âµ1Âµğ‘‡
1 wğšº1w"
G,0.6570680628272252,"(wğ‘‡ğšº1w)2 # = âˆ’
1 4
âˆš 2ğœ‹"
G,0.6575916230366492,"""
1
ğ‘ 1
exp (âˆ’
ğ‘ 2
1
2 )(Âµ1ğ‘¡1 âˆ’ğ‘ 2
1ğšº1w) #"
G,0.6581151832460733,"where ğ‘ 1 =
Âµğ‘‡
1 w
âˆšï¸"
G,0.6586387434554973,"wğ‘‡ğšº1w
, ğ‘¡1 =
Âµğ‘‡
1 w wğ‘‡ğšº1w"
G,0.6591623036649215,"Hence, the following equality holds at any optimal w:"
G,0.6596858638743456,"1
ğ‘ 2
exp (âˆ’
ğ‘ 2
2
2 )(Âµ2ğ‘¡2 âˆ’ğ‘ 2
2ğšº2w) = 1"
G,0.6602094240837696,"ğ‘ 1
exp (âˆ’
ğ‘ 2
1
2 )(Âµ1ğ‘¡1 âˆ’ğ‘ 2
1ğšº1w)"
G,0.6607329842931937,"Therefore, w = (Ëœğ‘ 1ğšº1 + Ëœğ‘ 2ğšº2)âˆ’1(Ëœğ‘¡1Âµ1 âˆ’Ëœğ‘¡2Âµ2), where we have"
G,0.6612565445026178,"Ëœğ‘ 1 = ğ‘ 1 exp (âˆ’
ğ‘ 2
1
2 ),
Ëœğ‘ 2 = âˆ’ğ‘ 2 exp (âˆ’
ğ‘ 2
2
2 ),"
G,0.6617801047120419,Ëœğ‘¡1 = ğ‘¡1
G,0.662303664921466,"ğ‘ 1
exp (âˆ’
ğ‘ 2
1
2 ),
Ëœğ‘¡2 = ğ‘¡2"
G,0.6628272251308901,"ğ‘ 2
exp (âˆ’
ğ‘ 2
2
2 )"
G,0.6633507853403141,Now we can consider the following optimization over 4 scalar variables:
G,0.6638743455497382,"min
Ëœğ‘ 1, Ëœğ‘ 2,Ëœğ‘¡1,Ëœğ‘¡2
1
2ğ‘„(
Âµğ‘‡
1 w
âˆšï¸"
G,0.6643979057591624,"wğ‘‡ğšº1w
) + 1"
G,0.6649214659685864,"2ğ‘„(âˆ’
Âµğ‘‡
2 w
âˆšï¸"
G,0.6654450261780105,"wğ‘‡ğšº2w
)
(29)"
G,0.6659685863874345,"Note that since the optimization (29) is low-dimensional, by the Convexity Lemma Liese and Miescke
[2008], we can interchange the limit in ğ‘›and min, implying that we can analyze the concentrated
optimization instead. We then arrive at the following fixed point equations using Assumptions 4:"
G,0.6664921465968586,"ğ‘ 2
1
ğ‘¡2
1
= Tr(ğšº1(Ëœğ‘ 1ğšº1 + Ëœğ‘ 2ğšº2)âˆ’2)(Ëœğ‘¡2
1 + Ëœğ‘¡2
2 âˆ’2Ëœğ‘¡1Ëœğ‘¡2ğ‘Ÿ)ğ‘‘
(30)"
G,0.6670157068062827,"ğ‘ 2
1
ğ‘¡1
= Tr(Ëœğ‘ 1ğšº1 + Ëœğ‘ 2ğšº2)âˆ’1(Ëœğ‘¡1 âˆ’Ëœğ‘¡2ğ‘Ÿ)ğ‘‘
(31)"
G,0.6675392670157068,"ğ‘ 2
2
ğ‘¡2
2
= Tr(ğšº2(Ëœğ‘ 1ğšº1 + Ëœğ‘ 2ğšº2)âˆ’2)(Ëœğ‘¡2
1 + Ëœğ‘¡2
2 âˆ’2Ëœğ‘¡1Ëœğ‘¡2ğ‘Ÿ)ğ‘‘
(32)"
G,0.6680628272251309,"ğ‘ 2
2
ğ‘¡2
= Tr(Ëœğ‘ 1ğšº1 + Ëœğ‘ 2ğšº2)âˆ’1(Ëœğ‘¡2 âˆ’Ëœğ‘¡1ğ‘Ÿ)ğ‘‘
(33)"
G,0.668586387434555,"Note that, since we assume ğ‘is exchangeable in its arguments, the system of equations (30) is
invariant under the transformation (ğ‘ 1, ğ‘¡1, ğ‘ 2, ğ‘¡2) â†’(âˆ’ğ‘ 2, ğ‘¡2, âˆ’ğ‘ 1, ğ‘¡1). As such, ğ‘ 1 = âˆ’ğ‘ 2 and ğ‘¡1 = ğ‘¡2
at the optimal point, implying that wâˆ—= (ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2) is the optimal classifier, as for the
chosen linear classification procedure the performance in invariant to the rescalings of the classifier
vector and therefore we can completely drop ğ‘ 1, ğ‘ 2, ğ‘¡1, ğ‘¡2 from the equations. â–¡"
G,0.669109947643979,"I
Proof of Theorem 3"
G,0.6696335078534031,"By Gaussian universality proved in Theorem 1, we analyze the binary classification problem as
follows"
G,0.6701570680628273,Proof. Define
G,0.6706806282722513,"R0 := EÂµ1,Âµ2w0wğ‘‡
0 = EÂµ1,Âµ2(ğ‘¡âˆ—wâˆ—+ ğ‘¡ğœ‚Î·)(ğ‘¡âˆ—wâˆ—+ ğ‘¡ğœ‚Î·)ğ‘‡=
ğ‘¡2
ğœ‚(1 âˆ’ğ‘Ÿ)"
G,0.6712041884816754,"ğ‘‘
ğ¼ğ‘‘+ ğ‘¡2
âˆ—EÂµ1,Âµ2wâˆ—wğ‘‡
âˆ—"
G,0.6717277486910995,"=
ğ‘¡2
ğœ‚(1 âˆ’ğ‘Ÿ)"
G,0.6722513089005235,"ğ‘‘
ğ¼ğ‘‘+ 2ğ‘¡2
âˆ—
ğ‘‘(1 âˆ’ğ‘Ÿ)(ğšº1 + ğšº2)âˆ’2"
G,0.6727748691099477,"Note that the eigenvalue density of R0 is captured by ğœ™(ğ‘ 1, ğ‘ 2) =
ğ‘¡2
ğœ‚(1âˆ’ğ‘Ÿ)"
G,0.6732984293193718,"ğ‘‘
+ 2ğ‘¡2
âˆ—(1âˆ’ğ‘Ÿ)"
G,0.6738219895287958,"ğ‘‘
(ğ‘ 1 + ğ‘ 2)âˆ’2,
assuming (ğ‘ 1, ğ‘ 2) âˆ¼ğ‘, where ğ‘is the joint EPD function of ğšº1 and ğšº2."
G,0.6743455497382199,"We would like to start with finding ğ›¼. Note that the data matrix X can be decomposed as follows
where G1, G2 are i.i.d. standard normal: X ="
G,0.6748691099476439,"G1ğšº1/2
1
G2ğšº1/2
2 !"
G,0.675392670157068,"+

1
0
0
1"
G,0.6759162303664922," 
Âµğ‘‡
1
Âµğ‘‡
2 "
G,0.6764397905759162,We then have: Xw0 =
G,0.6769633507853403,"G1ğšº1/2
1 w0
G2ğšº1/2
2 w0 !"
G,0.6774869109947644,"+

1
0
0
1"
G,0.6780104712041884," 
Âµğ‘‡
1 w0
Âµğ‘‡
2 w0"
G,0.6785340314136126,"
(34)"
G,0.6790575916230367,"It is immediate to see that the first term of (34) is a centered normally distributed random vector
with covariance diag(wğ‘‡
0 ğšº1w0, . . . , wğ‘‡
0 ğšº1w0, wğ‘‡
0 ğšº2w0, . . . , wğ‘‡
0 ğšº2w0), and therefore its norm con-"
G,0.6795811518324607,"centrates to
âˆšï¸ƒ"
G,0.6801047120418848,"ğ‘›
2 wğ‘‡
0 (ğšº1 + ğšº2)w0 = Î˜(
âˆšï¸"
G,0.680628272251309,Tr(ğšº1 + ğšº2)âˆ¥w0âˆ¥) according to our assumptions. The second
G,0.681151832460733,"term of (34) is a deterministic vector of norm Î˜(âˆšğ‘›âˆ¥w0âˆ¥
âˆš"
G,0.6816753926701571,1 âˆ’ğ‘Ÿ).
G,0.6821989528795811,Similarly:
G,0.6827225130890052,"yğ‘‡Xw0 =  1ğ‘‡
âˆ’1ğ‘‡  
G1ğšº1/2
1 w0
G2ğšº1/2
2 w0 !"
G,0.6832460732984293,"+

1
0
0
1"
G,0.6837696335078534," 
Âµğ‘‡
1 w0
Âµğ‘‡
2 w0"
G,0.6842931937172775,"
=  1ğ‘‡
âˆ’1ğ‘‡
 
G1ğšº1/2
1 w0
G2ğšº1/2
2 w0 ! + ğ‘›"
G,0.6848167539267016,2 (Âµ1 âˆ’Âµ2)ğ‘‡w0 (35)
G,0.6853403141361256,"Note that we can drop the first term of (35) compared to the second term. Indeed, the first term is a
centered normal random variable with variance ğ‘›"
G,0.6858638743455497,"2 wğ‘‡
0 (ğšº1 + ğšº2)w0 = Î˜(âˆ¥w0âˆ¥2Tr(ğšº1 + ğšº2)) and the
second term is a deterministic quantity of order Î˜(ğ‘›âˆ¥w0âˆ¥
âˆš"
G,0.6863874345549739,1 âˆ’ğ‘Ÿ). Thus
G,0.6869109947643979,ğ›¼= yğ‘‡Xw0
G,0.687434554973822,âˆ¥Xw0âˆ¥2 =
G,0.6879581151832461,"ğ‘›
2 (Âµ1 âˆ’Âµ2)ğ‘‡w0
ğ‘›
2 wğ‘‡
0 (ğšº1 + ğšº2)w0 + ğ‘›"
G,0.6884816753926701,"2 (Âµğ‘‡
1 w0)2 + ğ‘›"
G,0.6890052356020943,"2 (Âµğ‘‡
2 w0)2"
G,0.6895287958115183,"Moreover, by independence of Î· from Âµğ‘–we can drop the first term in (Âµ1 âˆ’Âµ2)ğ‘‡w0 = ğ‘¡ğœ‚(Âµ1 âˆ’
Âµ2)ğ‘‡Î· + ğ‘¡âˆ—(Âµ1 âˆ’Âµ2)ğ‘‡wâˆ—and have"
G,0.6900523560209424,"(Âµ1 âˆ’Âµ2)ğ‘‡wâˆ—= (Âµ1 âˆ’Âµ2)ğ‘‡(ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2)
Pâˆ’â†’2(1 âˆ’ğ‘Ÿ)Tr(ğšº1 + ğšº2)âˆ’1"
G,0.6905759162303665,On the other hand:
G,0.6910994764397905,"Âµğ‘‡
1 w0 = ğ‘¡ğœ‚Âµğ‘‡
1 Î· + ğ‘¡âˆ—Âµğ‘‡
1 wâˆ—= ğ‘¡ğœ‚Âµğ‘‡
1 Î· + ğ‘¡âˆ—Âµğ‘‡
1 (ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2)"
G,0.6916230366492147,"Which implies that we can drop the cross term in the expansion of (Âµğ‘‡
1 w0)2:"
G,0.6921465968586388,"(Âµğ‘‡
1 w0)2 â‰ˆğ‘¡2
ğœ‚(Âµğ‘‡
1 Î·)2 + ğ‘¡2
âˆ—

Âµğ‘‡
1 (ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2)
2 Pâˆ’â†’1 âˆ’ğ‘Ÿ"
G,0.6926701570680628,"ğ‘‘
ğ‘¡2
ğœ‚+ ğ‘¡2
âˆ—(1 âˆ’ğ‘Ÿ)2"
G,0.6931937172774869,"ğ‘‘2
Tr2(ğšº1 + ğšº2)âˆ’1"
G,0.693717277486911,"We can drop the first without loss of generality as ğ‘¡ğœ‚usually does not grow with ğ‘‘. This implies that
we have for ğ›¼ ğ›¼="
G,0.694240837696335,"ğ‘›
2
ğ‘¡âˆ—(1âˆ’ğ‘Ÿ)"
G,0.6947643979057592,"ğ‘‘
2Tr(ğšº1 + ğšº2)âˆ’1"
G,0.6952879581151833,"ğ‘›
2 Tr(R0(ğšº1 + ğšº2)) + ğ‘›ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)2"
G,0.6958115183246073,"ğ‘‘2
Tr2(ğšº1 + ğšº2)âˆ’1 ="
G,0.6963350785340314,"1
2
ğ‘¡âˆ—(1âˆ’ğ‘Ÿ)"
G,0.6968586387434555,"ğ‘‘
2Tr(ğšº1 + ğšº2)âˆ’1"
G,0.6973821989528796,"1
2Tr(R0(ğšº1 + ğšº2)) + ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)2"
G,0.6979057591623037,"ğ‘‘2
Tr2(ğšº1 + ğšº2)âˆ’1"
G,0.6984293193717277,Then SGD converges to
G,0.6989528795811518,"min
w âˆ¥w âˆ’ğ›¼w0âˆ¥2"
G,0.6994764397905759,"ğ‘ .ğ‘¡
y = Xw"
G,0.7,Introducing a Lagrange multiplier v âˆˆRğ‘›:
G,0.7005235602094241,"min
w max
v
âˆ¥w âˆ’ğ›¼w0âˆ¥2 + vğ‘‡G"
G,0.7010471204188482,"ğšº1/2
1
ğšº1/2
2 !"
G,0.7015706806282722,w + vğ‘‡Mw âˆ’vğ‘‡y
G,0.7020942408376963,"Using Theorem 4 from Akhtiamov et al. [2024],"
G,0.7026178010471205,"min max âˆ¥âˆ’ğ›¼0âˆ¥2 + 2
âˆ‘ï¸"
G,0.7031413612565445,"ğ‘–=1
âˆ¥ğ‘£ğ‘–âˆ¥ğ‘‡
ğ‘–ğšº1/2
ğ‘–
+ âˆ¥ğšº1/2
ğ‘–
âˆ¥hğ‘‡
ğ‘–ğ‘–+ ğ‘‡
ğ‘–1Âµğ‘‡
ğ‘–+ ğ‘‡
ğ‘–1(âˆ’1)ğ‘–"
G,0.7036649214659686,Denoting ğ›½ğ‘–:= âˆ¥vğ‘–âˆ¥and performing optimization over the direction of ğ‘£ğ‘–:
G,0.7041884816753927,"min max
ğ›½1,ğ›½2â‰¥0 âˆ¥âˆ’ğ›¼0âˆ¥2 + 2
âˆ‘ï¸"
G,0.7047120418848167,"ğ‘–=1
ğ›½ğ‘–(ğ‘‡
ğ‘–ğšº1/2
ğ‘–
+ âˆ¥hğ‘–âˆ¥ğšº1/2
ğ‘–
âˆ¥+ Âµğ‘‡
ğ‘–1 + (âˆ’1)ğ‘–1âˆ¥)"
G,0.7052356020942409,We have
G,0.7057591623036649,"min
w
max
ğ›½1,ğ›½2â‰¥0 âˆ¥w âˆ’ğ›¼w0âˆ¥2 + 2
âˆ‘ï¸"
G,0.706282722513089,"ğ‘–=1
ğ›½ğ‘–(gğ‘‡
ğ‘–ğšº1/2
ğ‘–
w +
âˆšï¸‚ ğ‘›
2 âˆšï¸ƒ"
G,0.7068062827225131,"âˆ¥ğšº1/2
ğ‘–
wâˆ¥2 + (Âµğ‘‡
ğ‘–w + (âˆ’1)ğ‘–)2)"
G,0.7073298429319371,Applying the square root trick to the norm inside the objective we arrive at:
G,0.7078534031413612,"min
ğœ1,ğœ2â‰¥0,w max
ğ›½1,ğ›½2â‰¥0 âˆ¥w âˆ’ğ›¼w0âˆ¥2 + 2
âˆ‘ï¸"
G,0.7083769633507854,"ğ‘–=1
ğ›½ğ‘–gğ‘‡
ğ‘–ğšº1/2
ğ‘–
w +
âˆšï¸‚"
G,0.7089005235602094,"ğ‘›
2
ğ›½ğ‘–ğœğ‘–"
G,0.7094240837696335,"2
+ ğ›½ğ‘– 2ğœğ‘– âˆšï¸‚"
G,0.7099476439790576,"ğ‘›
2 (âˆ¥ğšº1/2
ğ‘–
wâˆ¥2 + (Âµğ‘‡
ğ‘–w + (âˆ’1)ğ‘–)2)"
G,0.7104712041884816,"Introducing ğ›¾ğ‘–â€™s as the Fenchel duals for (Âµğ‘‡
ğ‘–w + (âˆ’1)ğ‘–)2 we have:"
G,0.7109947643979058,"min
ğœ1,ğœ2â‰¥0,w
max
ğ›½1,ğ›½2â‰¥0,ğ›¾1,ğ›¾2 âˆ¥w âˆ’ğ›¼w0âˆ¥2 + 2
âˆ‘ï¸"
G,0.7115183246073299,"ğ‘–=1
ğ›½ğ‘–gğ‘‡
ğ‘–ğšº1/2
ğ‘–
w +
âˆšï¸‚"
G,0.7120418848167539,"ğ‘›
2
ğ›½ğ‘–ğœğ‘–"
G,0.712565445026178,"2
+ ğ›½ğ‘– 2ğœğ‘– âˆšï¸‚"
G,0.7130890052356021,"ğ‘›
2 (âˆ¥ğšº1/2
ğ‘–
wâˆ¥2 + Âµğ‘‡
ğ‘–wğ›¾ğ‘–+ (âˆ’1)ğ‘–ğ›¾ğ‘–âˆ’ğ›¾2
ğ‘–
4 )"
G,0.7136125654450262,Performing optimization over w yields:
G,0.7141361256544503,"min
ğœ1,ğœ2â‰¥0
max
ğ›½1,ğ›½2â‰¥0,ğ›¾1,ğ›¾2 ğ›¼2âˆ¥w0âˆ¥2 + 2
âˆ‘ï¸ ğ‘–=1 âˆšï¸‚"
G,0.7146596858638743,"ğ‘›
2
ğ›½ğ‘–ğœğ‘–"
G,0.7151832460732984,"2
+
âˆšï¸‚"
G,0.7157068062827225,"ğ‘›
2
ğ›½ğ‘–
2ğœğ‘–
(ğ›¾ğ‘–(âˆ’1)ğ‘–âˆ’ğ›¾2
ğ‘–
4 )"
G,0.7162303664921466,"âˆ’

âˆ’ğ›¼w0 + 1"
G,0.7167539267015707,"2 (ğ›½1ğšº1/2
1 g1 + ğ›½2ğšº1/2
2 g2) + ğ›½1 4ğœ1 âˆšï¸‚"
G,0.7172774869109948,"ğ‘›
2ğ›¾1Âµ1 + ğ›½2 4ğœ2 âˆšï¸‚"
G,0.7178010471204188,"ğ‘›
2ğ›¾2Âµ2
ğ‘‡
ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7183246073298429,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7188481675392671,"ğ‘›
2ğšº2
âˆ’1"
G,0.7193717277486911,"Â·

âˆ’ğ›¼w0 + 1"
G,0.7198952879581152,"2 (ğ›½1ğšº1/2
1 g1 + ğ›½2ğšº1/2
2 g2) + ğ›½1 4ğœ1 âˆšï¸‚"
G,0.7204188481675393,"ğ‘›
2ğ›¾1Âµ1 + ğ›½2 4ğœ2 âˆšï¸‚"
G,0.7209424083769633,"ğ‘›
2ğ›¾2Âµ2
"
G,0.7214659685863875,"We drop the ğ›¼2âˆ¥w0âˆ¥2 from the objective because it does not contain any of the variables the
optimization is performed over and therefore dropping it will not change the optimal values of
ğ›½1, ğ›½2, ğ›¾1, ğ›¾2, ğœ1, ğœ2. We obtain:"
G,0.7219895287958115,"min
ğœ1,ğœ2â‰¥0
max
ğ›½1,ğ›½2â‰¥0,ğ›¾1,ğ›¾2 2
âˆ‘ï¸ ğ‘–=1 âˆšï¸‚"
G,0.7225130890052356,"ğ‘›
2
ğ›½ğ‘–ğœğ‘–"
G,0.7230366492146597,"2
+
âˆšï¸‚"
G,0.7235602094240837,"ğ‘›
2
ğ›½ğ‘–
2ğœğ‘–
(ğ›¾ğ‘–(âˆ’1)ğ‘–âˆ’ğ›¾2
ğ‘–
4 ) âˆ’ğ›¼2wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7240837696335078,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.724607329842932,"ğ‘›
2ğšº2)âˆ’1w0âˆ’"
G,0.725130890052356,"âˆ’
ğ›½2
1
4 gğ‘‡
1 ğšºğ‘‡/2
1
(ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7256544502617801,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7261780104712042,"ğ‘›
2ğšº2)âˆ’1ğšº1/2
1 g1 âˆ’
ğ›½2
2
4 gğ‘‡
2 ğšºğ‘‡/2
2
(ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7267015706806282,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7272251308900524,"ğ‘›
2ğšº2)âˆ’1ğšº1/2
2 g2âˆ’"
G,0.7277486910994765,"âˆ’
ğ‘›ğ›½2
1ğ›¾2
1
32ğœ2
1
Âµğ‘‡
1 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7282722513089005,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7287958115183246,"ğ‘›
2ğšº2)âˆ’1Âµ1 âˆ’
ğ‘›ğ›½2
2ğ›¾2
2
32ğœ2
2
Âµğ‘‡
2 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7293193717277487,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7298429319371728,"ğ‘›
2ğšº2)âˆ’1Âµ2+ + ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7303664921465969,"ğ‘›
2ğ›¾1ğ›¼wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7308900523560209,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.731413612565445,"ğ‘›
2ğšº2)âˆ’1Âµ1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7319371727748691,"ğ‘›
2ğ›¾2ğ›¼wğ‘‡
0 (ğ¼+ ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7324607329842932,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7329842931937173,"ğ‘›
2ğšº2)âˆ’1Âµ2âˆ’"
G,0.7335078534031414,âˆ’ğ‘›ğ›½1ğ›½2ğ›¾1ğ›¾2
G,0.7340314136125654,"16ğœ1ğœ2
Âµğ‘‡
1 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7345549738219895,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7350785340314137,"ğ‘›
2ğšº2)âˆ’1Âµ2"
G,0.7356020942408377,The objective above can be rewritten in the integral form in the following way:
G,0.7361256544502618,"min
ğœ1,ğœ2â‰¥0
max
ğ›½1,ğ›½2â‰¥0,ğ›¾1,ğ›¾2 2
âˆ‘ï¸ ğ‘–=1 âˆšï¸‚"
G,0.7366492146596859,"ğ‘›
2
ğ›½ğ‘–ğœğ‘–"
G,0.7371727748691099,"2
+
âˆšï¸‚"
G,0.737696335078534,"ğ‘›
2
ğ›½ğ‘–
2ğœğ‘–
(ğ›¾ğ‘–(âˆ’1)ğ‘–âˆ’ğ›¾2
ğ‘–
4 ) âˆ’ğ›¼2wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7382198952879581,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7387434554973822,"ğ‘›
2ğšº2)âˆ’1w0âˆ’"
G,0.7392670157068063,"âˆ’ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.7397905759162303,"ğ‘ 1ğ›½2
1+ğ‘ 2ğ›½2
2
4
1 + ğ›½1"
G,0.7403141361256544,"2ğœ1
âˆšï¸ğ‘›"
G,0.7408376963350786,2 ğ‘ 1 + ğ›½2
G,0.7413612565445026,"2ğœ2
âˆšï¸ğ‘›"
G,0.7418848167539267,"2 ğ‘ 2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 âˆ’
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.7424083769633508,"ğ‘›ğ›½2
1 ğ›¾2
1
32ğœ2
1 +
ğ‘›ğ›½2
2 ğ›¾2
2
32ğœ2
2
1 + ğ›½1"
G,0.7429319371727748,"2ğœ1
âˆšï¸ğ‘›"
G,0.743455497382199,2 ğ‘ 1 + ğ›½2
G,0.7439790575916231,"2ğœ2
âˆšï¸ğ‘›"
G,0.7445026178010471,"2 ğ‘ 2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2+ + ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7450261780104712,"ğ‘›
2ğ›¾1ğ›¼wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7455497382198953,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7460732984293194,"ğ‘›
2ğšº2)âˆ’1Âµ1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7465968586387435,"ğ‘›
2ğ›¾2ğ›¼wğ‘‡
0 (ğ¼+ ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7471204188481675,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7476439790575916,"ğ‘›
2ğšº2)âˆ’1Âµ2"
G,0.7481675392670157,"âˆ’2ğ‘Ÿ
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.7486910994764397,ğ‘›ğ›½1ğ›½2ğ›¾1ğ›¾2
G,0.7492146596858639,"32ğœ1ğœ2
1 + ğ›½1"
G,0.749738219895288,"2ğœ1
âˆšï¸ğ‘›"
G,0.750261780104712,2 ğ‘ 1 + ğ›½2
G,0.7507853403141361,"2ğœ2
âˆšï¸ğ‘›"
G,0.7513089005235603,"2 ğ‘ 2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.7518324607329843,Note that
G,0.7523560209424084,"ğ›¼2wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7528795811518325,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7534031413612565,"ğ‘›
2ğšº2)âˆ’1w0 â†’ğ›¼2Tr(R0(ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7539267015706806,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7544502617801047,"ğ‘›
2ğšº2)âˆ’1)"
G,0.7549738219895288,Moreover
G,0.7554973821989529,Tr(R0(ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚
G,0.7560209424083769,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.756544502617801,"ğ‘›
2ğšº2)âˆ’1) â†’ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğœ™(ğ‘ 1, ğ‘ 2)"
G,0.7570680628272252,1 + ğ›½1
G,0.7575916230366492,"2ğœ1
âˆšï¸ğ‘›"
G,0.7581151832460733,2 ğ‘ 1 + ğ›½2
G,0.7586387434554974,"2ğœ2
âˆšï¸ğ‘›"
G,0.7591623036649214,"2 ğ‘ 2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 And"
G,0.7596858638743456,"wğ‘‡
0 (ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7602094240837697,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7607329842931937,"ğ‘›
2ğšº2)âˆ’1Âµ1 â†’ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)"
G,0.7612565445026178,"ğ‘‘
Tr((ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7617801047120419,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.762303664921466,"ğ‘›
2ğšº2)âˆ’1(ğšº1 + ğšº2)âˆ’1)"
G,0.7628272251308901,The latter term can be derived in the integral form as:
G,0.7633507853403141,"Tr

(ğ¼+ ğ›½1 2ğœ1 âˆšï¸‚"
G,0.7638743455497382,"ğ‘›
2ğšº1 + ğ›½2 2ğœ2 âˆšï¸‚"
G,0.7643979057591623,"ğ‘›
2ğšº2)âˆ’1(ğšº1 + ğšº2)âˆ’1

â†’ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)âˆ’1"
G,0.7649214659685863,1 + ğ›½1
G,0.7654450261780105,"2ğœ1
âˆšï¸ğ‘›"
G,0.7659685863874346,2 ğ‘ 1 + ğ›½2
G,0.7664921465968586,"2ğœ2
âˆšï¸ğ‘›"
G,0.7670157068062827,"2 ğ‘ 2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.7675392670157068,"Since ğ‘(ğ‘ 1, ğ‘ 2) is exchangeable in its arguments, we see that the latter objective remains invariant
under the transformation that swaps (ğ›½1, ğœ1, ğ›¾1) with (ğ›½2, ğœ2, âˆ’ğ›¾2). Therefore, ğ›½:= ğ›½1 = ğ›½2,
ğœ:= ğœ1 = ğœ2 and ğ›¾:= ğ›¾2 = âˆ’ğ›¾1 holds at the optimal point. The objective then reduces to the
following:"
G,0.7680628272251309,"min
ğœâ‰¥0 max
ğ›½â‰¥0,ğ›¾ âˆšï¸‚"
G,0.768586387434555,"ğ‘›
2 ğ›½ğœ+
âˆšï¸‚"
G,0.7691099476439791,"ğ‘›
2
ğ›½
ğœ(ğ›¾âˆ’ğ›¾2"
G,0.7696335078534031,"4 ) âˆ’
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğ‘‘ğ›¼2ğœ™(ğ‘ 1, ğ‘ 2) + ğ‘‘(ğ‘ 1+ğ‘ 2)ğ›½2"
G,0.7701570680628272,"4
+ ğ‘›ğ›½2ğ›¾2"
G,0.7706806282722513,16ğœ2 (1 âˆ’ğ‘Ÿ) 1 + ğ›½
G,0.7712041884816754,2ğœ(ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.7717277486910995,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 (36)"
G,0.7722513089005235,"âˆ’2ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¼
âˆšï¸‚ ğ‘›
2"
G,0.7727748691099476,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.7732984293193718,"ğ›½ğ›¾
2ğœ(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğ›½"
G,0.7738219895287958,"2ğœ
âˆšï¸ğ‘›"
G,0.7743455497382199,"2 (ğ‘ 1 + ğ‘ 2)
(37)"
G,0.774869109947644,"Note that ğ›¾can be found explicitly in terms of ğ›½and ğœas the optimization over ğ›¾is quadratic. Denote
ğ‘’0 = âˆ¥ğ‘¤0âˆ¥2"
G,0.775392670157068,"ğ‘‘
. To facilitate derivations, let us introduce a few notations. Let ğ¹1( ğ›½"
G,0.7759162303664922,"ğœ, ğ›¾) :=
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğ‘‘ğ›¼2ğœ™(ğ‘ 1, ğ‘ 2) + ğ‘›ğ›½2ğ›¾2"
G,0.7764397905759163,16ğœ2 (1 âˆ’ğ‘Ÿ) + 2ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¼âˆšï¸ğ‘›
G,0.7769633507853403,"2
ğ›½ğ›¾
2ğœ(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğ›½"
G,0.7774869109947644,2ğœ(ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.7780104712041885,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 ğ¹2( ğ›½"
G,0.7785340314136125,"ğœ, ğ›½) := ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.7790575916230367,(ğ‘ 1+ğ‘ 2)ğ›½2
G,0.7795811518324607,"4
1 + ğ›½"
G,0.7801047120418848,"2ğœ
âˆšï¸ğ‘›"
G,0.7806282722513089,"2 (ğ‘ 1 + ğ‘ 2)
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.7811518324607329,Thus the objective is
G,0.7816753926701571,"min
ğœâ‰¥0 max
ğ›½â‰¥0,ğ›¾âˆ’ğ¹1( ğ›½"
G,0.7821989528795812,"ğœ, ğ›¾) âˆ’ğ¹2( ğ›½"
G,0.7827225130890052,"ğœ, ğ›½) +
âˆšï¸‚"
G,0.7832460732984293,"ğ‘›
2 ğ›½ğœ+
âˆšï¸‚"
G,0.7837696335078534,"ğ‘›
2
ğ›½
ğœ(ğ›¾âˆ’ğ›¾2 4 )"
G,0.7842931937172775,Now taking derivatives w.r.t ğœyields
G,0.7848167539267016,ğœ•ğœğ¹1( ğ›½
G,0.7853403141361257,"ğœ, ğ›¾) = âˆ’ğ›½"
G,0.7858638743455497,"ğœ2 ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾)"
G,0.7863874345549738,ğœ•ğœğ¹2( ğ›½
G,0.7869109947643979,"ğœ, ğ›½) = âˆ’ğ›½"
G,0.787434554973822,"ğœ2 ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½)"
G,0.7879581151832461,Now for ğ›½:
G,0.7884816753926701,ğœ•ğ›½ğ¹1( ğ›½
G,0.7890052356020942,"ğœ, ğ›¾) = 1"
G,0.7895287958115184,"ğœğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾)"
G,0.7900523560209424,ğœ•ğ›½ğ¹2( ğ›½
G,0.7905759162303665,"ğœ, ğ›½) = 2"
G,0.7910994764397906,ğ›½ğ¹2( ğ›½
G,0.7916230366492146,"ğœ, ğ›½) + 1"
G,0.7921465968586388,"ğœğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½)"
G,0.7926701570680629,Setting the derivative of the objective w.r.t ğœto 0:
G,0.7931937172774869,"ğ›½
ğœ2 ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) + ğ›½"
G,0.793717277486911,"ğœ2 ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) +
âˆšï¸‚"
G,0.7942408376963351,"ğ‘›
2 ğ›½âˆ’
âˆšï¸‚"
G,0.7947643979057591,"ğ‘›
2
ğ›½
ğœ2 (ğ›¾âˆ’ğ›¾2"
G,0.7952879581151833,4 ) = 0
G,0.7958115183246073,Multiplying by ğœ2 and dropping ğ›½yields:
G,0.7963350785340314,"ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) + ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) +
âˆšï¸‚"
G,0.7968586387434555,"ğ‘›
2ğœ2 âˆ’
âˆšï¸‚"
G,0.7973821989528795,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.7979057591623037,"4 ) = 0
(38)"
G,0.7984293193717278,Derivative w.r.t. ğ›½ âˆ’1
G,0.7989528795811518,"ğœğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) âˆ’2"
G,0.7994764397905759,ğ›½ğ¹2( ğ›½
G,0.8,"ğœ, ğ›½) âˆ’1"
G,0.8005235602094241,"ğœğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) +
âˆšï¸‚"
G,0.8010471204188482,"ğ‘›
2ğœ+
âˆšï¸‚"
G,0.8015706806282723,"ğ‘›
2
1
ğœ(ğ›¾âˆ’ğ›¾2"
G,0.8020942408376963,4 ) = 0
G,0.8026178010471204,"Multiplying by ğœ,"
G,0.8031413612565445,"âˆ’ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) âˆ’2ğœ"
G,0.8036649214659686,ğ›½ğ¹2( ğ›½
G,0.8041884816753927,"ğœ, ğ›½) âˆ’ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) +
âˆšï¸‚"
G,0.8047120418848167,"ğ‘›
2ğœ2 +
âˆšï¸‚"
G,0.8052356020942408,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.805759162303665,4 ) = 0
G,0.806282722513089,"Therefore the set of equations is:
(
ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) + ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) + âˆšï¸ğ‘›"
G,0.8068062827225131,2 ğœ2 âˆ’âˆšï¸ğ‘›
G,0.8073298429319372,2 (ğ›¾âˆ’ğ›¾2
G,0.8078534031413612,4 ) = 0
G,0.8083769633507853,"âˆ’ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) âˆ’2ğœ"
G,0.8089005235602095,ğ›½ğ¹2( ğ›½
G,0.8094240837696335,"ğœ, ğ›½) âˆ’ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) + âˆšï¸ğ‘›"
G,0.8099476439790576,2 ğœ2 + âˆšï¸ğ‘›
G,0.8104712041884817,2 (ğ›¾âˆ’ğ›¾2
G,0.8109947643979057,4 ) = 0
G,0.8115183246073299,Summing the equations yields âˆ’2ğœ
G,0.8120418848167539,ğ›½ğ¹2( ğ›½
G,0.812565445026178,"ğœ, ğ›½) + 2
âˆšï¸‚"
G,0.8130890052356021,"ğ‘›
2ğœ2 = 0 â†’2"
G,0.8136125654450261,ğ›½ğ¹2( ğ›½
G,0.8141361256544503,"ğœ, ğ›½) = 2
âˆšï¸‚ ğ‘›
2ğœ"
G,0.8146596858638744,Which implies
G,0.8151832460732984,"ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.8157068062827225,(ğ‘ 1+ğ‘ 2)ğ›½
G,0.8162303664921466,"2
1 + ğ›½"
G,0.8167539267015707,"2ğœ
âˆšï¸ğ‘›"
G,0.8172774869109948,"2 (ğ‘ 1 + ğ‘ 2)
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 = 2
âˆšï¸‚ ğ‘›
2ğœ"
G,0.8178010471204189,"Multiplying both sides by
âˆšğ‘›"
G,0.8183246073298429,"2
ğ‘‘ğœand introducing ğœƒ= ğ›½"
G,0.818848167539267,ğœwe obtain:
G,0.819371727748691,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.8198952879581152,"ğœƒ
2
âˆšï¸ğ‘›"
G,0.8204188481675393,"2 (ğ‘ 1 + ğ‘ 2) 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.8209424083769633,"2 (ğ‘ 1 + ğ‘ 2)
)ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 = ğ‘› ğ‘‘"
G,0.8214659685863874,"This can be rewritten as:
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)(1 âˆ’
2
âˆš 2 2
âˆš"
G,0.8219895287958116,"2 + ğœƒâˆšğ‘›(ğ‘ 1 + ğ‘ 2)
)ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 = ğ‘› ğ‘‘"
G,0.8225130890052356,We arrive at the following equation for ğœƒ: ğ‘‘âˆ’ğ‘›
G,0.8230366492146597,"ğ‘‘
=
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
2
âˆš 2 2
âˆš"
G,0.8235602094240838,"2 + ğœƒâˆšğ‘›(ğ‘ 1 + ğ‘ 2)
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
(39)"
G,0.8240837696335078,"The last equation above can be reformulated in terms of the Stiltjes transform as follows: 2
âˆš"
G,0.824607329842932,"2
ğœƒâˆšğ‘›ğ‘†ğšº1+ğšº2(âˆ’2
âˆš"
G,0.8251308900523561,"2
ğœƒâˆšğ‘›) = ğ‘‘âˆ’ğ‘› ğ‘‘"
G,0.8256544502617801,"After finding ğœƒfrom the equation above, we proceed to identify the optimal value of ğ›¾. We will do so
by taking the derivative of (36) by ğ›¾and setting it to 0: âˆšï¸‚"
G,0.8261780104712042,"ğ‘›
2ğœƒ(1 âˆ’ğ›¾"
G,0.8267015706806282,"2 ) âˆ’
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘›ğœƒ2ğ›¾"
G,0.8272251308900523,"8
(1 âˆ’ğ‘Ÿ) 1 + ğœƒ"
G,0.8277486910994765,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8282722513089005,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 âˆ’ğ›¼
âˆšï¸‚ ğ‘›
2"
G,0.8287958115183246,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğœƒğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.8293193717277487,"2 (ğ‘ 1 + ğ‘ 2)
= 0"
G,0.8298429319371727,"Dividing both sides by ğœƒ, factoring âˆšğ‘›out and using (39) we obtain:"
G,0.8303664921465969,"ğ›¾âˆšğ‘›
 1 2
âˆš"
G,0.830890052356021,"2
+
âˆšğ‘›ğœƒ(1 âˆ’ğ‘Ÿ) 8
ğ‘‘âˆ’ğ‘› ğ‘‘"
G,0.831413612565445,"
=
âˆšï¸‚"
G,0.8319371727748691,"ğ‘›
2 âˆ’ğ›¼
âˆšï¸‚ ğ‘›
2"
G,0.8324607329842932,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.8329842931937173,2 (ğ‘ 1 + ğ‘ 2)
G,0.8335078534031414,"Hence, we can find ğ›¾via ğ›¾= 1
2 + âˆš"
G,0.8340314136125655,2ğ‘›ğœƒ(1 âˆ’ğ‘Ÿ)(ğ‘‘âˆ’ğ‘›) 8ğ‘‘ !âˆ’1
G,0.8345549738219895,"1 âˆ’ğ›¼
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.8350785340314136,2 (ğ‘ 1 + ğ‘ 2) !
G,0.8356020942408376,"Finally, to find ğœ, recall (40)"
G,0.8361256544502618,"ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) + ğœ•ğ‘¥ğ¹2(ğ‘¥, ğ›½) +
âˆšï¸‚"
G,0.8366492146596859,"ğ‘›
2ğœ2 âˆ’
âˆšï¸‚"
G,0.8371727748691099,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.837696335078534,4 ) = 0
G,0.8382198952879581,Using ğ¹2 is quadratic in its second argument we have:
G,0.8387434554973822,"ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) + ğœ2(ğœ•ğ‘¥ğ¹2(ğ‘¥, ğœƒ) +
âˆšï¸‚"
G,0.8392670157068063,"ğ‘›
2) =
âˆšï¸‚"
G,0.8397905759162304,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2 4 )"
G,0.8403141361256544,"ğœ2(ğœ•ğ‘¥ğ¹2(ğ‘¥, ğœƒ) +
âˆšï¸‚"
G,0.8408376963350785,"ğ‘›
2) =
âˆšï¸‚"
G,0.8413612565445027,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.8418848167539267,"4 ) âˆ’ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾)"
G,0.8424083769633508,"ğœ2 = (ğœ•ğ‘¥ğ¹2(ğ‘¥, ğœƒ) +
âˆšï¸‚"
G,0.8429319371727748,"ğ‘›
2)âˆ’1(
âˆšï¸‚"
G,0.8434554973821989,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.8439790575916231,"4 ) âˆ’ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾))"
G,0.8445026178010471,We have
G,0.8450261780104712,"ğ¹1(ğ‘¥, ğ›¾) :=
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğ‘‘ğ›¼2ğœ™(ğ‘ 1, ğ‘ 2) + ğ‘›ğ‘¥2ğ›¾2"
G,0.8455497382198953,"16
(1 âˆ’ğ‘Ÿ) + 2ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¼âˆšï¸ğ‘› 2
ğ‘¥ğ›¾"
G,0.8460732984293193,2 (ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğ‘¥
G,0.8465968586387435,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8471204188481676,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 Then"
G,0.8476439790575916,"ğœ•ğ‘¥ğ¹1(ğ‘¥, ğ›¾) = âˆ’ğ‘‘ğ›¼2 2 âˆšï¸‚ ğ‘›
2"
G,0.8481675392670157,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğœ™(ğ‘ 1, ğ‘ 2)(ğ‘ 1 + ğ‘ 2)"
G,0.8486910994764398,(1 + ğ‘¥
G,0.8492146596858638,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.849738219895288,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.8502617801047121,+ğ‘›ğ‘¥ğ›¾2(1 âˆ’ğ‘Ÿ) 8
G,0.8507853403141361,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1 1 + ğ‘¥"
G,0.8513089005235602,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8518324607329842,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.8523560209424084,"âˆ’ğ‘›ğ‘¥2ğ›¾2(1 âˆ’ğ‘Ÿ) 32 âˆšï¸‚ ğ‘›
2"
G,0.8528795811518325,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)"
G,0.8534031413612565,(1 + ğ‘¥
G,0.8539267015706806,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8544502617801047,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.8549738219895288,"âˆ’ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¾
âˆšï¸‚ ğ‘›
2"
G,0.8554973821989529,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğ‘¥"
G,0.856020942408377,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.856544502617801,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.8570680628272251,"+ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ‘¥ğ›¾ 2
ğ‘›
2"
G,0.8575916230366493,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1"
G,0.8581151832460733,(1 + ğ‘¥
G,0.8586387434554974,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8591623036649214,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.8596858638743455,Moreover
G,0.8602094240837697,"ğ¹2(ğ‘¥, ğœƒ) = ğ‘‘
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.8607329842931937,(ğ‘ 1+ğ‘ 2) ğœƒ2
G,0.8612565445026178,"4
1 + ğ‘¥ 2
âˆšï¸ğ‘›"
G,0.8617801047120419,"2 (ğ‘ 1 + ğ‘ 2)
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 Then"
G,0.8623036649214659,"ğœ•ğ‘¥ğ¹2(ğ‘¥, ğœƒ) = âˆ’ğ‘‘ğœƒ2 8 âˆšï¸‚ ğ‘›
2"
G,0.86282722513089,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)2"
G,0.8633507853403142,"(1 + ğ‘¥ 2
âˆšï¸ğ‘›"
G,0.8638743455497382,2 (ğ‘ 1 + ğ‘ 2))2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.8643979057591623,"Thus, all in all:"
G,0.8649214659685864,"ğœ2 =
âˆšï¸‚"
G,0.8654450261780104,"ğ‘›
2 âˆ’ğ‘‘ğœƒ2 8 âˆšï¸‚ ğ‘›
2"
G,0.8659685863874346,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)2"
G,0.8664921465968587,"(1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.8670157068062827,2 (ğ‘ 1 + ğ‘ 2))2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 âˆ’1 âˆšï¸‚
G,0.8675392670157068,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.8680628272251308,"4 ) + ğ‘‘ğ›¼2 2 âˆšï¸‚ ğ‘›
2"
G,0.868586387434555,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğœ™(ğ‘ 1, ğ‘ 2)(ğ‘ 1 + ğ‘ 2)"
G,0.8691099476439791,(1 + ğœƒ
G,0.8696335078534031,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8701570680628272,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.8706806282722513,âˆ’ğ‘›ğœƒğ›¾2(1 âˆ’ğ‘Ÿ) 8
G,0.8712041884816754,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1 1 + ğœƒ"
G,0.8717277486910995,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8722513089005236,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.8727748691099476,"+ ğ‘›ğœƒ2ğ›¾2(1 âˆ’ğ‘Ÿ) 32 âˆšï¸‚ ğ‘›
2"
G,0.8732984293193717,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)"
G,0.8738219895287959,(1 + ğœƒ
G,0.8743455497382199,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.874869109947644,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.875392670157068,"âˆ’ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¾
âˆšï¸‚ ğ‘›
2"
G,0.8759162303664921,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ"
G,0.8764397905759163,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8769633507853403,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.8774869109947644,+ ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğœƒğ›¾ğ‘› 4
G,0.8780104712041885,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1"
G,0.8785340314136125,(1 + ğœƒ
G,0.8790575916230366,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.8795811518324608,"2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2  ğ›¾= 1
2 + âˆš"
G,0.8801047120418848,2ğ‘›ğœƒ(1 âˆ’ğ‘Ÿ)(ğ‘‘âˆ’ğ‘›) 8ğ‘‘ !âˆ’1
G,0.8806282722513089,"1 âˆ’ğ›¼
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.881151832460733,2 (ğ‘ 1 + ğ‘ 2) ! (40)
G,0.881675392670157,"The generalization error can be found through ğ‘„

1âˆ’ğ›¾ 2
âˆš"
G,0.8821989528795812,"ğœ2âˆ’( ğ›¾ 2 )2 
."
G,0.8827225130890053,"I.1
Single direction case."
G,0.8832460732984293,In the special case when Î£1 = Î£2 = ğœ2 Iğ‘‘
G,0.8837696335078534,"ğ‘‘we obtain as R0 =
ğ‘¡2
ğœ‚(1âˆ’ğ‘Ÿ)"
G,0.8842931937172774,"ğ‘‘
ğ¼ğ‘‘+ 2 ğ‘¡2
âˆ—
ğ‘‘(1 âˆ’ğ‘Ÿ)(ğšº1 + ğšº2)âˆ’2:"
G,0.8848167539267016,ğ›¼= yğ‘‡Xw0
G,0.8853403141361257,âˆ¥Xw0âˆ¥2 =
G,0.8858638743455497,ğ‘¡âˆ—(1âˆ’ğ‘Ÿ)
G,0.8863874345549738,"ğ‘‘
Tr(ğšº1 + ğšº2)âˆ’1"
G,0.8869109947643979,"1
2Tr(R0(ğšº1 + ğšº2)) + ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)2"
G,0.887434554973822,"ğ‘‘2
Tr2(ğšº1 + ğšº2)âˆ’1"
G,0.8879581151832461,"=
ğ‘¡âˆ—
ğ‘‘
2ğœ2"
G,0.8884816753926702,"1
ğ‘‘ğœ2(ğ‘¡2ğœ‚+ ğ‘¡2âˆ—
ğ‘‘2
2ğœ4 ) + ğ‘¡2âˆ—(1 âˆ’ğ‘Ÿ) ğ‘‘2 4ğœ4"
G,0.8890052356020942,We also have
G,0.8895287958115183,"ğ›¼=
ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2"
G,0.8900523560209425,ğ‘¡2ğœ‚(1âˆ’ğ‘Ÿ)
G,0.8905759162303665,"2ğ‘‘
Tr 2ğœ2"
G,0.8910994764397906,ğ‘‘I + ğ‘¡2âˆ—
G,0.8916230366492146,ğ‘‘(1 âˆ’ğ‘Ÿ)Tr( 2ğœ2
G,0.8921465968586387,ğ‘‘I)âˆ’1 + ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)2
G,0.8926701570680629,"ğ‘‘2
Tr2( 2ğœ2 ğ‘‘I)âˆ’1"
G,0.8931937172774869,"=
ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2"
G,0.893717277486911,ğ‘¡2ğœ‚(1âˆ’ğ‘Ÿ) ğœ2
G,0.8942408376963351,"ğ‘‘
+ ğ‘‘ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)"
G,0.8947643979057591,"2ğœ2
+ ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)2ğ‘‘2"
G,0.8952879581151832,"4ğœ4
=
ğ‘¡âˆ—
ğ‘‘
2ğœ2 ğ‘¡2ğœ‚ğœ2"
G,0.8958115183246074,"ğ‘‘
+ ğ‘‘ğ‘¡2âˆ—"
G,0.8963350785340314,2ğœ2 + ğ‘¡2âˆ—(1âˆ’ğ‘Ÿ)ğ‘‘2
G,0.8968586387434555,"4ğœ4
=
ğ‘¡âˆ—
ğ‘‘
2ğœ2 ğœ2"
G,0.8973821989528796,"ğ‘‘(ğ‘¡2ğœ‚+ ğ‘¡2âˆ—
ğ‘‘2
2ğœ4 ) + ğ‘¡2âˆ—(1 âˆ’ğ‘Ÿ) ğ‘‘2 4ğœ4"
G,0.8979057591623036,"Multiplying both sides by ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2 we arrive at:"
G,0.8984293193717278,Î“ := ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ) ğ‘‘
G,0.8989528795811519,"2ğœ2 =
1"
G,0.8994764397905759,"ğœ2
ğ‘‘(1âˆ’ğ‘Ÿ) (
ğ‘¡2ğœ‚
ğ‘¡2âˆ—
4ğœ4"
G,0.9,"ğ‘‘2 + 2) + 1
(41)"
G,0.900523560209424,Note that plugging in w0 = ğ‘¡âˆ—(ğšº1 + ğšº2)âˆ’1(Âµ1 âˆ’Âµ2) + ğ‘¡ğœ‚Î· for ğšº1 = ğšº2 = ğœ2
G,0.9010471204188482,"ğ‘‘Iğ‘‘into the expression
for the generalization error yields"
G,0.9015706806282723,"ğ‘’ğ‘= ğ‘„
Â©Â­Â­
Â«"
G,0.9020942408376963,"Âµğ‘‡
1 w0
âˆšï¸ƒ"
G,0.9026178010471204,"wğ‘‡
0 ğšº1w0 ÂªÂ®Â®
Â¬"
G,0.9031413612565445,"= ğ‘„
Â©Â­Â­
Â«"
G,0.9036649214659686,"ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2
âˆšï¸ƒ"
G,0.9041884816753927,"ğ‘¡2âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2 + ğœ2"
G,0.9047120418848168,ğ‘‘ğ‘¡2ğœ‚(1 âˆ’ğ‘Ÿ)
G,0.9052356020942408,"ÂªÂ®Â®
Â¬
Taking ğ‘„âˆ’1 of both sides and then squaring them we arrive at:"
G,0.9057591623036649,"ğ‘„âˆ’1(ğ‘’ğ‘)2 =
ğ‘¡2
âˆ—(1 âˆ’ğ‘Ÿ)2 ğ‘‘2 4ğœ4"
G,0.9062827225130891,"ğ‘¡2âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2 + ğœ2"
G,0.9068062827225131,"ğ‘‘ğ‘¡2ğœ‚(1 âˆ’ğ‘Ÿ)
=
ğ‘¡2
âˆ—(1 âˆ’ğ‘Ÿ)2 ğ‘‘2 4ğœ4"
G,0.9073298429319372,"ğ‘¡2âˆ—(1 âˆ’ğ‘Ÿ)
ğ‘‘
2ğœ2 + ğœ2"
G,0.9078534031413612,"ğ‘‘ğ‘¡2ğœ‚(1 âˆ’ğ‘Ÿ)
=
1"
G,0.9083769633507853,"ğœ2
(1âˆ’ğ‘Ÿ)ğ‘‘(2 + 4 ğœ4"
G,0.9089005235602095,"ğ‘‘2
ğ‘¡2ğœ‚
ğ‘¡2âˆ—)"
G,0.9094240837696335,This yields that by taking ğœŒ:= ğ‘‘(1âˆ’ğ‘Ÿ) ğœ2 ğ‘¡ğœ‚
G,0.9099476439790576,"ğ‘¡âˆ—
=
ğ‘‘
2ğœ2"
G,0.9104712041884817,"âˆšï¸‚
ğœŒ
ğ‘„âˆ’1(ğ‘’ğ‘)2 âˆ’2 =
ğœŒ
2(1 âˆ’ğ‘Ÿ)"
G,0.9109947643979057,"âˆšï¸‚
ğœŒ
ğ‘„âˆ’1(ğ‘’ğ‘)2 âˆ’2"
G,0.9115183246073298,We thus obtain from (41):
G,0.912041884816754,"Î“ =
1
ğ‘„âˆ’1(ğ‘’ğ‘)âˆ’2 + 1 =
1
ğ‘„âˆ’1(ğ‘’ğ‘)âˆ’2 + 1 =
ğ‘„âˆ’1(ğ‘’ğ‘)2"
G,0.912565445026178,"ğ‘„âˆ’1(ğ‘’ğ‘)2 + 1 = 1 âˆ’
1
ğ‘„âˆ’1(ğ‘’ğ‘)2 + 1
(42)"
G,0.9130890052356021,"We proceed to find ğœƒnext. From the general case we had 2
âˆš"
G,0.9136125654450262,"2
ğœƒâˆšğ‘›ğ‘†ğšº1+ğšº2(âˆ’2
âˆš"
G,0.9141361256544502,"2
ğœƒâˆšğ‘›) = ğ‘‘âˆ’ğ‘› ğ‘‘"
G,0.9146596858638744,"Now plugging for the distribution of ğšº1, ğšº2 yields"
G,0.9151832460732985,"1 + ğœƒ
âˆšï¸‚"
G,0.9157068062827225,"ğ‘›
2
ğœ2"
G,0.9162303664921466,"ğ‘‘=
ğ‘‘
ğ‘‘âˆ’ğ‘› ğœƒ
âˆšï¸‚"
G,0.9167539267015706,"ğ‘›
2
ğœ2"
G,0.9172774869109948,"ğ‘‘=
ğ‘›
ğ‘‘âˆ’ğ‘›"
G,0.9178010471204189,"ğœƒ=
ğ‘‘
âˆš"
G,0.9183246073298429,"2ğ‘›
(ğ‘‘âˆ’ğ‘›)ğœ2"
G,0.918848167539267,"Using (41) we can simplify the last expression above: ğ›¾= 1
2 + âˆš"
G,0.9193717277486911,2ğ‘›ğœƒ(1 âˆ’ğ‘Ÿ)(ğ‘‘âˆ’ğ‘›) 8ğ‘‘ !âˆ’1
G,0.9198952879581151,"1 âˆ’ğ›¼
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.9204188481675393,"2 (ğ‘ 1 + ğ‘ 2) ! = = 1
2 + âˆš"
G,0.9209424083769634,"2ğ‘›ğœƒ(1 âˆ’ğ‘Ÿ)(ğ‘‘âˆ’ğ‘›) 8ğ‘‘ !âˆ’1 1 âˆ’
Î“ 1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.9214659685863874,"2 (ğ‘ 1 + ğ‘ 2) ! = Â©Â­
Â« 1
2 + âˆš"
G,0.9219895287958115,"2ğ‘›
ğ‘‘
âˆš"
G,0.9225130890052357,"2ğ‘›
(ğ‘‘âˆ’ğ‘›) ğœ2 (1 âˆ’ğ‘Ÿ)(ğ‘‘âˆ’ğ‘›)"
G,0.9230366492146597,"8ğ‘‘
ÂªÂ®
Â¬"
G,0.9235602094240838,"âˆ’1
Â©Â­
Â«
1 âˆ’
Î“"
G,0.9240837696335078,"1 +
ğ‘‘
âˆš"
G,0.9246073298429319,"2ğ‘›
2(ğ‘‘âˆ’ğ‘›) ğœ2
âˆšï¸ğ‘› 2
2ğœ2 ğ‘‘)"
G,0.925130890052356,"ÂªÂ®
Â¬
="
G,0.9256544502617801,Thus we can write for ğ›¾ ğ›¾=
G,0.9261780104712042,"1
2 +
ğ‘›2"
G,0.9267015706806283,ğœ2 (1 âˆ’ğ‘Ÿ) 8
G,0.9272251308900523,"!âˆ’1 
1 âˆ’
Î“
1 +
ğ‘›
ğ‘‘âˆ’ğ‘›  =
1"
G,0.9277486910994764,2 + ğ‘›(1 âˆ’ğ‘Ÿ) 4ğœ2
G,0.9282722513089006,"âˆ’1 
1 âˆ’ğ‘‘âˆ’ğ‘›"
G,0.9287958115183246,"ğ‘‘
(1 âˆ’
1
ğ‘„âˆ’1(ğ‘’ğ‘)2 + 1)

="
G,0.9293193717277487,"ğ‘›
ğ‘‘+ ğ‘‘âˆ’ğ‘›"
G,0.9298429319371728,"ğ‘‘
1
ğ‘„âˆ’1(ğ‘’ğ‘)2+1
1
2 + ğœŒ"
G,0.9303664921465968,"4ğœ…
where ğœŒ:= ğ‘‘(1 âˆ’ğ‘Ÿ) ğœ2"
G,0.930890052356021,"Furthermore, we have for ğœ2"
G,0.9314136125654451,"ğœ2 =
âˆšï¸‚"
G,0.9319371727748691,"ğ‘›
2 âˆ’ğ‘‘ğœƒ2 8 âˆšï¸‚ ğ‘›
2"
G,0.9324607329842932,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)2"
G,0.9329842931937172,"(1 + ğœƒ 2
âˆšï¸ğ‘›"
G,0.9335078534031414,2 (ğ‘ 1 + ğ‘ 2))2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 âˆ’1 âˆšï¸‚
G,0.9340314136125655,"ğ‘›
2 (ğ›¾âˆ’ğ›¾2"
G,0.9345549738219895,"4 ) + ğ‘‘ğ›¼2 2 âˆšï¸‚ ğ‘›
2"
G,0.9350785340314136,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
ğœ™(ğ‘ 1, ğ‘ 2)(ğ‘ 1 + ğ‘ 2)"
G,0.9356020942408377,(1 + ğœƒ
G,0.9361256544502617,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.9366492146596859,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.93717277486911,âˆ’ğ‘›ğœƒğ›¾2(1 âˆ’ğ‘Ÿ) 8
G,0.937696335078534,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1 1 + ğœƒ"
G,0.9382198952879581,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.9387434554973823,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.9392670157068063,"+ ğ‘›ğœƒ2ğ›¾2(1 âˆ’ğ‘Ÿ) 32 âˆšï¸‚ ğ‘›
2"
G,0.9397905759162304,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)"
G,0.9403141361256544,(1 + ğœƒ
G,0.9408376963350785,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.9413612565445026,2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2
G,0.9418848167539267,"âˆ’ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¾
âˆšï¸‚ ğ‘›
2"
G,0.9424083769633508,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
(ğ‘ 1 + ğ‘ 2)âˆ’1 1 + ğœƒ"
G,0.9429319371727749,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.9434554973821989,"2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.943979057591623,+ ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğœƒğ›¾ğ‘› 4
G,0.9445026178010472,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1"
G,0.9450261780104712,(1 + ğœƒ
G,0.9455497382198953,2 (ğ‘ 1 + ğ‘ 2)âˆšï¸ğ‘›
G,0.9460732984293194,"2 )2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 
="
G,0.9465968586387434,Simplifying furthermore yields
G,0.9471204188481676,"ğœ2 =

1 âˆ’ğ‘‘ ğ‘›"
G,0.9476439790575916,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)"
G,0.9481675392670157,"ğ‘›2
(ğ‘‘âˆ’ğ‘›)2"
G,0.9486910994764398,"ğ‘‘2
(ğ‘‘âˆ’ğ‘›)2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 âˆ’1"
G,0.9492146596858638,"
(ğ›¾âˆ’ğ›¾2"
G,0.949738219895288,"4 ) +
âˆ«âˆ«
ğ›¼2ğ‘(ğ‘ 1, ğ‘ 2)(1 âˆ’ğ‘Ÿ)2(ğ‘¡2
ğœ‚+ ğ‘¡2
âˆ—
ğ‘‘2
2ğœ4 )ğœ2"
G,0.9502617801047121,"ğ‘‘(1 âˆ’ğ‘Ÿ)(
ğ‘‘
ğ‘‘âˆ’ğ‘›)2
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.9507853403141361,"âˆ’
ğ‘›
2ğ‘‘
(ğ‘‘âˆ’ğ‘›) ğœ2 ğ›¾2(1 âˆ’ğ‘Ÿ) 8"
G,0.9513089005235602,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) 1"
G,0.9518324607329843,"ğ‘‘
ğ‘‘âˆ’ğ‘›
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.9523560209424083,"+
ğ‘›
2ğ‘›ğ‘‘2
(ğ‘‘âˆ’ğ‘›)2 ğœ4 ğ›¾2(1 âˆ’ğ‘Ÿ) 32"
G,0.9528795811518325,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) 2ğœ2"
G,0.9534031413612566,"ğ‘‘
(
ğ‘‘
ğ‘‘âˆ’ğ‘›)2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.9539267015706806,"âˆ’ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)ğ›¾
âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2) ğ‘‘
2ğœ2"
G,0.9544502617801047,"ğ‘‘
ğ‘‘âˆ’ğ‘›
ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2"
G,0.9549738219895288,"+
ğ›¼ğ‘¡âˆ—(1 âˆ’ğ‘Ÿ)
2ğ‘‘
(ğ‘‘âˆ’ğ‘›) ğœ2 ğ›¾ğ‘› 4"
G,0.9554973821989529,"âˆ«âˆ«
ğ‘(ğ‘ 1, ğ‘ 2)
1"
G,0.956020942408377,"(
ğ‘‘
ğ‘‘âˆ’ğ‘›)2 ğ‘‘ğ‘ 1ğ‘‘ğ‘ 2 
="
G,0.956544502617801,Further manipulation leads to
G,0.9570680628272251,"ğœ2 =
ğœ…
ğœ…âˆ’1"
G,0.9575916230366492,"
ğ›¾âˆ’ğ›¾2"
G,0.9581151832460733,"4 +
Î“2"
G,0.9586387434554974,ğ‘„âˆ’1(ğ‘’ğ‘)2 (1 âˆ’1
G,0.9591623036649215,ğœ…)2 âˆ’ğœŒğ›¾2
G,0.9596858638743455,4ğœ…+ ğœŒğ›¾2
G,0.9602094240837696,8ğœ…2 âˆ’Î“ğ›¾(1 âˆ’1
G,0.9607329842931938,"ğœ…) + (1 âˆ’1 ğœ…) 1 ğœ…Î“ğ›¾
"
G,0.9612565445026178,"=
ğœ…
ğœ…âˆ’1"
G,0.9617801047120419,"
ğ›¾âˆ’ğ›¾2"
G,0.962303664921466,"4 +
Î“2"
G,0.96282722513089,ğ‘„âˆ’1(ğ‘’ğ‘)2 (1 âˆ’1
G,0.9633507853403142,ğœ…)2 âˆ’ğœŒğ›¾2
G,0.9638743455497382,4ğœ…+ ğœŒğ›¾2
G,0.9643979057591623,8ğœ…2 âˆ’(1 âˆ’1
G,0.9649214659685864,"ğœ…)2Î“ğ›¾
"
G,0.9654450261780104,"=
ğœ…
ğœ…âˆ’1  ğœŒ"
G,0.9659685863874345,8ğœ…2 âˆ’ğœŒ 4ğœ…âˆ’1 4
G,0.9664921465968587,"
ğ›¾2 +

1 âˆ’(1 âˆ’1"
G,0.9670157068062827,"ğœ…)2Î“

ğ›¾+
Î“2"
G,0.9675392670157068,"ğ‘„âˆ’1(ğ‘’ğ‘)2 (1 âˆ’1 ğœ…)2
"
G,0.9680628272251309,"Recall that Î“ = 1 âˆ’
1
ğ‘„âˆ’1(ğ‘’ğ‘)2+1 and
Î“
ğ‘„âˆ’1(ğ‘’ğ‘)2 = 1 âˆ’Î“. Hence, we obtain:"
G,0.9685863874345549,"ğœ2 =
ğœ…
ğœ…âˆ’1  ğœŒ"
G,0.9691099476439791,8ğœ…2 âˆ’ğœŒ 4ğœ…âˆ’1 4
G,0.9696335078534032,"
ğ›¾2 +

1 âˆ’(1 âˆ’1"
G,0.9701570680628272,"ğœ…)2Î“

ğ›¾+ Î“(1 âˆ’Î“)(1 âˆ’1"
G,0.9706806282722513,"ğœ…)2

(43)"
G,0.9712041884816754,"On the other hand, ğ›¾=
1âˆ’(1âˆ’1"
G,0.9717277486910995,"ğœ…)Î“
1
2 + ğœŒ"
G,0.9722513089005236,"4ğœ…
and Î“ = 1 âˆ’
1
ğ‘„âˆ’1(ğ‘’ğ‘)2+1 combining these fact with 43 yields ğ‘’ğ‘= ğ‘„"
G,0.9727748691099476,"
ğ‘„âˆ’1(ğ‘’ğ‘)2(2ğœ…+ ğœŒâˆ’2) + ğœŒ
âˆšï¸"
G,0.9732984293193717,"ğœ…(ğœ…âˆ’1)
âˆšï¸ƒ"
G,0.9738219895287958,(2ğœ…+ ğœŒ)((4ğœ…âˆ’2)ğ‘„âˆ’1(ğ‘’ğ‘)4 + ğ‘„âˆ’1(ğ‘’ğ‘)2  2ğœ…3 + ğœ…2ğœŒâˆ’2ğœ…(ğœŒâˆ’1) + ğœŒ + 2ğœ…2) ! â–¡
G,0.9743455497382199,NeurIPS Paper Checklist
CLAIMS,0.974869109947644,1. Claims
CLAIMS,0.9753926701570681,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The main results are stated in Sections 3, 4 along with numerical evidence in
Section 5.
Guidelines:"
CLAIMS,0.9759162303664921,"â€¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.9764397905759162,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of our approach and assumptions are explained in Section 6.
Guidelines:"
CLAIMS,0.9769633507853404,"â€¢ The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
â€¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢ The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢ If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9774869109947644,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9780104712041885,"Justification: The theoretical results, assumptions, and definitions are provided in sections
2, 3, 4. Moreover, the complete proof of arguments in the main body is presented in the
Appendices B, F, H, I."
CLAIMS,0.9785340314136126,Guidelines:
CLAIMS,0.9790575916230366,"â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢ Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9795811518324608,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9801047120418848,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9806282722513089,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.981151832460733,"Justification: All the necessary details are given in Section 5. Moreover, the code is attached
to the supplementary materials."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.981675392670157,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9821989528795811,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢ While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9827225130890053,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9832460732984293,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is attached to the supplementary material.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9837696335078534,"â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
â€¢ While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
â€¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
OPEN ACCESS TO DATA AND CODE,0.9842931937172775,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All the necessary details are given in Section 5.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9848167539267015,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
OPEN ACCESS TO DATA AND CODE,0.9853403141361257,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: As the main focus of the paper is the theoretical contribution, the experiments
are conducted using specific data distributions whose statistics are exactly known. (ğœ’2,
Bernoulli, Gaussian)
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9858638743455498,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
OPEN ACCESS TO DATA AND CODE,0.9863874345549738,"â€¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
OPEN ACCESS TO DATA AND CODE,0.9869109947643979,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: All the necessary details are given in Section 5.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.987434554973822,"â€¢ The answer NA means that the paper does not include experiments.
â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9. Code Of Ethics"
OPEN ACCESS TO DATA AND CODE,0.987958115183246,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research conducted in this paper conforms with Neurips Code of Ethics.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9884816753926702,"â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
OPEN ACCESS TO DATA AND CODE,0.9890052356020942,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
OPEN ACCESS TO DATA AND CODE,0.9895287958115183,"â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
OPEN ACCESS TO DATA AND CODE,0.9900523560209424,"â€¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9905759162303664,11. Safeguards
SAFEGUARDS,0.9910994764397906,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9916230366492147,Answer: [NA]
SAFEGUARDS,0.9921465968586387,Justification: Since this paper is concerned with the mathematical foundations of AI.
SAFEGUARDS,0.9926701570680628,Guidelines:
SAFEGUARDS,0.993193717277487,"â€¢ The answer NA means that the paper poses no such risks.
â€¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.993717277486911,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9942408376963351,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9947643979057592,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9952879581151832,Justification: All the necessary details are given in Section 5.
LICENSES FOR EXISTING ASSETS,0.9958115183246073,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9963350785340314,"â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢ If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
â€¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.9968586387434555,"â€¢ If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9973821989528796,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9979057591623036,"â€¢ The answer NA means that the paper does not release new assets.
â€¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢ At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9984293193717277,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9989528795811519,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Since this paper is concerned with the mathematical foundations of AI.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9994764397905759,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢ For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
