Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008271298593879239,"Despite the rise to dominance of deep learning in unstructured data domains, tree-
1"
ABSTRACT,0.0016542597187758478,"based methods such as Random Forests (RF) and Gradient Boosted Decision Trees
2"
ABSTRACT,0.0024813895781637717,"(GBDT) are still the workhorses for handling discriminative tasks on tabular data.
3"
ABSTRACT,0.0033085194375516956,"We explore generative extensions of these popular algorithms with a focus on
4"
ABSTRACT,0.0041356492969396195,"explicitly modeling the data density (up to a normalization constant), thus enabling
5"
ABSTRACT,0.004962779156327543,"other applications besides sampling. As our main contribution we propose an
6"
ABSTRACT,0.005789909015715467,"effective energy-based generative boosting algorithm that is analogous to the second
7"
ABSTRACT,0.006617038875103391,"order boosting algorithm implemented in popular packages like XGBoost. We
8"
ABSTRACT,0.007444168734491315,"show that, despite producing a generative model capable of handling inference tasks
9"
ABSTRACT,0.008271298593879239,"over any input variable, our proposed algorithm can achieve similar discriminative
10"
ABSTRACT,0.009098428453267164,"performance to GBDT algorithms on a number of real world tabular datasets and
11"
ABSTRACT,0.009925558312655087,"outperform competing approaches for sampling.
12"
INTRODUCTION,0.010752688172043012,"1
Introduction
13"
INTRODUCTION,0.011579818031430935,"Generative models have achieved tremendous success in computer vision and natural language
14"
INTRODUCTION,0.01240694789081886,"processing, where the ability to generate synthetic data guided by user prompts opens up many
15"
INTRODUCTION,0.013234077750206782,"exciting possibilities. While generating synthetic table records does not necessarily enjoy the same
16"
INTRODUCTION,0.014061207609594707,"wide appeal, this problem has still received considerable attention as a potential avenue for bypassing
17"
INTRODUCTION,0.01488833746898263,"privacy concerns when sharing data. Estimating the data density, p(x), is another typical application
18"
INTRODUCTION,0.015715467328370553,"of generative models which enables a host of different use cases that can be particularly interesting
19"
INTRODUCTION,0.016542597187758478,"for tabular data. Unlike discriminative models which are trained to perform inference over a single
20"
INTRODUCTION,0.017369727047146403,"target variable, density models can be used more flexibly for inference over different variables or for
21"
INTRODUCTION,0.018196856906534328,"out of distribution detection. They can also handle inference with missing data in a principled way by
22"
INTRODUCTION,0.01902398676592225,"marginalizing over unobserved variables.
23"
INTRODUCTION,0.019851116625310174,"The development of generative models for tabular data has mirrored its progression in computer
24"
INTRODUCTION,0.0206782464846981,"vision with many of its Deep Learning (DL) approaches being adapted to the tabular domain [Jordon
25"
INTRODUCTION,0.021505376344086023,"et al., 2018, Xu et al., 2019, Engelmann and Lessmann, 2020, Fan et al., 2020, Zhao et al., 2021,
26"
INTRODUCTION,0.022332506203473945,"Kotelnikov et al., 2022]. Unfortunately, these methods are only useful for sampling as they either
27"
INTRODUCTION,0.02315963606286187,"don’t model the density explicitly or can’t evaluate it due to untractable marginalization over high
28"
INTRODUCTION,0.023986765922249794,"dimensional latent variable spaces. Furthermore, despite growing in popularity, DL has still failed to
29"
INTRODUCTION,0.02481389578163772,"displace tree-based ensemble methods as the tool of choice for handling tabular discriminative tasks
30"
INTRODUCTION,0.02564102564102564,"with gradient boosting still being found to outperform neural-network-based methods in many real
31"
INTRODUCTION,0.026468155500413565,"world datasets [Grinsztajn et al., 2022, Borisov et al., 2022a].
32"
INTRODUCTION,0.02729528535980149,"While there have been recent efforts to extend the success of tree-based models to generative modeling
33"
INTRODUCTION,0.028122415219189414,"[Correia et al., 2020, Wen and Hang, 2022, Nock and Guillame-Bert, 2022, Watson et al., 2023,
34"
INTRODUCTION,0.028949545078577336,"Nock and Guillame-Bert, 2023, Jolicoeur-Martineau et al., 2023], we find that direct extensions of
35"
INTRODUCTION,0.02977667493796526,"Random Forests (RF) and Gradient Boosted Decision Tree (GBDT) are still missing. It is this gap
36"
INTRODUCTION,0.030603804797353185,"that we try to address, seeking to keep the general algorithmic structure of these popular algorithms
37"
INTRODUCTION,0.03143093465674111,"Training Data
NRGBoost
TVAE
TabDDPM"
INTRODUCTION,0.03225806451612903,Figure 1: Downsampled MNIST samples generated by NRGBoost and two tabular DL methods.
INTRODUCTION,0.033085194375516956,"but replacing the optimization of their discriminative objective with a generative counterpart. Our
38"
INTRODUCTION,0.03391232423490488,"main contributions in this regard are:
39"
INTRODUCTION,0.034739454094292806,"• Proposing NRGBoost, a novel energy-based generative boosting model that, analogously to
40"
INTRODUCTION,0.03556658395368073,"the boosting algorithms implemented in popular GBDT packages, is trained to maximize a
41"
INTRODUCTION,0.036393713813068655,"local second order approximation to the likelihood at each boosting round.
42"
INTRODUCTION,0.03722084367245657,"• Proposing an approximate sampling algorithm to speed up the training of any tree-based
43"
INTRODUCTION,0.0380479735318445,"multiplicative generative boosting model.
44"
INTRODUCTION,0.03887510339123242,"• Exploring the use of bagged ensembles of Density Estimation Trees (DET) [Ram and Gray,
45"
INTRODUCTION,0.03970223325062035,"2011] with feature subsampling as the generative counterpart to RF.
46"
INTRODUCTION,0.04052936311000827,"The longstanding popularity of GBDT models in machine learning practice can, in part, be attributed
47"
INTRODUCTION,0.0413564929693962,"to the strength of its empirical results and the efficiency of its existing implementations. We therefore
48"
INTRODUCTION,0.04218362282878412,"focus on an experimental evaluation in real world datasets spanning a range of use cases, number
49"
INTRODUCTION,0.043010752688172046,"of samples and features. We find that, on smaller datasets, our implementation of NRGBoost can
50"
INTRODUCTION,0.043837882547559964,"be trained in a few minutes on a mid-range consumer CPU and achieve similar discriminative
51"
INTRODUCTION,0.04466501240694789,"performance to a standard GBDT model while also being able to generate samples that are generally
52"
INTRODUCTION,0.045492142266335814,"harder to distinguish from real data than state of the art neural-network-based models.
53"
ENERGY BASED MODELS,0.04631927212572374,"2
Energy Based Models
54"
ENERGY BASED MODELS,0.04714640198511166,"An Energy-Based Model (EBM) parametrizes the logarithm of a probability density function directly
55"
ENERGY BASED MODELS,0.04797353184449959,"(up to an unspecified normalizing constant):
56"
ENERGY BASED MODELS,0.04880066170388751,qf(x) = exp (f(x))
ENERGY BASED MODELS,0.04962779156327544,"Z[f]
.
(1)"
ENERGY BASED MODELS,0.050454921422663356,"Here f(x) : X →R is a real function over the input domain.1 We will avoid introducing any
57"
ENERGY BASED MODELS,0.05128205128205128,"parametrization, instead treating the function f ∈F(X) lying in an appropriate function space over
58"
ENERGY BASED MODELS,0.052109181141439205,"the input space as our model parameter directly. Z[f] = P
x∈X exp (f(x)), known as the partition
59"
ENERGY BASED MODELS,0.05293631100082713,"function, is then a functional of f giving us the necessary normalizing constant.
60"
ENERGY BASED MODELS,0.053763440860215055,"This is the most flexible way one could represent a probability density function making essentially
61"
ENERGY BASED MODELS,0.05459057071960298,"no compromises on its structure. The downside to this is that for most interesting choices of F,
62"
ENERGY BASED MODELS,0.055417700578990904,"computing or estimating this normalizing constant is untractable which makes training these models
63"
ENERGY BASED MODELS,0.05624483043837883,"difficult. Their unnormalized nature however does not prevent EBMs from being useful in a number
64"
ENERGY BASED MODELS,0.05707196029776675,"of applications besides sampling. Performing inference over a small enough subset of variables
65"
ENERGY BASED MODELS,0.05789909015715467,"requires only normalizing over the set of their possible values and for anomaly or out of distribution
66"
ENERGY BASED MODELS,0.058726220016542596,"detection, knowledge of the normalizing constant is not necessary.
67"
ENERGY BASED MODELS,0.05955334987593052,"One common way to train an energy-based model to approximate a data generating distribution, p(x),
68"
ENERGY BASED MODELS,0.060380479735318446,"is to minimize the Kullback-Leibler divergence between p and qf, or equivalently, maximize the
69"
ENERGY BASED MODELS,0.06120760959470637,"expected log likelihood functional:
70"
ENERGY BASED MODELS,0.062034739454094295,"L[f] = Ex∼p log qf(x) = Ex∼pf(x) −log Z[f]
(2)"
WE WILL ASSUME THAT X IS FINITE AND DISCRETE TO SIMPLIFY THE NOTATION AND EXPOSITION BUT EVERYTHING IS,0.06286186931348221,"1We will assume that X is finite and discrete to simplify the notation and exposition but everything is
applicable to bounded continuous input spaces, replacing the sums with integrals as appropriate."
WE WILL ASSUME THAT X IS FINITE AND DISCRETE TO SIMPLIFY THE NOTATION AND EXPOSITION BUT EVERYTHING IS,0.06368899917287014,"This optimization is typically carried out by gradient descent over the parameters of f, but due to
71"
WE WILL ASSUME THAT X IS FINITE AND DISCRETE TO SIMPLIFY THE NOTATION AND EXPOSITION BUT EVERYTHING IS,0.06451612903225806,"the untractability of the partition function, one must rely on Markov Chain Monte Carlo (MCMC)
72"
WE WILL ASSUME THAT X IS FINITE AND DISCRETE TO SIMPLIFY THE NOTATION AND EXPOSITION BUT EVERYTHING IS,0.065343258891646,"sampling to estimate the gradients [Song and Kingma, 2021].
73"
NRGBOOST,0.06617038875103391,"3
NRGBoost
74"
NRGBOOST,0.06699751861042183,"Expanding the increase in log-likelihood in equation 2 due to a variation δf around an energy function
75"
NRGBOOST,0.06782464846980976,"f up to second order we have
76"
NRGBOOST,0.06865177832919768,L[f + δf] −L[f] ≈Ex∼pδf(x) −Ex∼qf δf(x) −1
NRGBOOST,0.06947890818858561,"2Varx∼qf δf(x) =: ∆Lf[δf] .
(3)"
NRGBOOST,0.07030603804797353,"The δf that maximizes this quadratic approximation should thus have a large positive difference
77"
NRGBOOST,0.07113316790736146,"between the expected value under the data and under qf while having low variance under qf. We
78"
NRGBOOST,0.07196029776674938,"note that just like the original log-likelihood, this Taylor expansion is invariant to adding an overall
79"
NRGBOOST,0.07278742762613731,"constant to δf. This means that, in maximizing equation 3 we can consider only functions that have
80"
NRGBOOST,0.07361455748552523,"zero expectation under qf in which case we can simplify ∆Lf[δf] as
81"
NRGBOOST,0.07444168734491315,∆Lf[δf] = Ex∼pδf(x) −1
NRGBOOST,0.07526881720430108,"2Ex∼qf δf 2(x) .
(4)"
NRGBOOST,0.076095947063689,"We thus formulate our boosting algorithm as modelling the data density with an additive energy
82"
NRGBOOST,0.07692307692307693,"function. At each boosting iteration we improve upon the current energy function ft by finding an
83"
NRGBOOST,0.07775020678246485,"optimal step δf ∗
t that maximizes ∆Lft[δf]
84"
NRGBOOST,0.07857733664185278,"δf ∗
t = arg max
δf∈Ht ∆Lft[δf] ,
(5)"
NRGBOOST,0.0794044665012407,"where Ht is an appropriate space of functions (satisfying Ex∼qftδf(x) = 0 if equation 4 is used).
85"
NRGBOOST,0.08023159636062861,"The solution to this problem can be interpreted as a Newton step in the space of energy functions.
86"
NRGBOOST,0.08105872622001654,"Because for an energy-based model, the Fisher Information matrix with respect to the energy function
87"
NRGBOOST,0.08188585607940446,"and the hessian of the expected log-likelihood are the same, we can also interpret the solution to
88"
NRGBOOST,0.0827129859387924,"equation 5 as a natural gradient step (see the Appendix A). This approach is essentially analogous
89"
NRGBOOST,0.08354011579818031,"to the second order step implemented in modern discriminative gradient boosting libraries such as
90"
NRGBOOST,0.08436724565756824,"XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] and which can be traced back
91"
NRGBOOST,0.08519437551695616,"to Friedman et al. [2000].
92"
NRGBOOST,0.08602150537634409,"In updating the current iterate, ft+1 = ft + αt · δf ∗
t , we scale δf ∗
t by an additional scalar step-size
93"
NRGBOOST,0.08684863523573201,"αt. This can be interpreted as a globalization strategy to account for the fact that the quadratic
94"
NRGBOOST,0.08767576509511993,"approximation in equation 3 is not necessarily valid over large steps in function space. A common
95"
NRGBOOST,0.08850289495450786,"strategy in nonlinear optimization would be to select αt via a line search based on the original
96"
NRGBOOST,0.08933002481389578,"log-likelihood. Common practice in discriminative boosting however is to interpret this step size
97"
NRGBOOST,0.09015715467328371,"as a regularization parameter and to select a fixed value in ]0, 1] with (more) smaller steps typically
98"
NRGBOOST,0.09098428453267163,"outperforming fewer larger ones when it comes to generalization. We choose to adopt a hybrid
99"
NRGBOOST,0.09181141439205956,"strategy, first selecting an optimal step size by line search and then shrinking it by a fixed factor. We
100"
NRGBOOST,0.09263854425144748,"find that this typically accelerates convergence allowing the algorithm to take comparatively larger
101"
NRGBOOST,0.0934656741108354,"steps that increase the likelihood in the initial phase of boosting. For a starting point, f0, we can
102"
NRGBOOST,0.09429280397022333,"choose the logarithm of any probability distribution over X as long as it is easy to evaluate. Sensible
103"
NRGBOOST,0.09511993382961124,"choices are a uniform distribution (i.e., f ≡0), the product of marginals for the training set, or any
104"
NRGBOOST,0.09594706368899918,"mixture distribution between these two.
105"
WEAK LEARNERS,0.0967741935483871,"3.1
Weak Learners
106"
WEAK LEARNERS,0.09760132340777503,"As a weak learner we will consider functions defined by trees over the input space. I.e., letting
107
SJ
j=1 Xj = X be the partitioning of the input space induced by the leaves of a binary tree whose
108"
WEAK LEARNERS,0.09842845326716294,"internal nodes represent a split along one dimension into two disjoint partitions, we take as H the set
109"
WEAK LEARNERS,0.09925558312655088,"of functions such as
110"
WEAK LEARNERS,0.1000827129859388,"δf(x) = J
X"
WEAK LEARNERS,0.10090984284532671,"j=1
wj1Xj(x) ,
(6)"
WEAK LEARNERS,0.10173697270471464,"where 1X denotes the indicator function of a subset X and wj are values associated with each
111"
WEAK LEARNERS,0.10256410256410256,"leaf j ∈[1..J]. In a standard decision tree these values would typically encode an estimate of
112"
WEAK LEARNERS,0.10339123242349049,"p(y|x ∈Xj), with y being a special target variable that is never considered for splitting. In our
113"
WEAK LEARNERS,0.10421836228287841,"generative approach they encode unconditional densities (or more precisely energies) over each leaf’s
114"
WEAK LEARNERS,0.10504549214226634,"support and every variable can be used for splitting. Note that our functions δf are thus parametrized
115"
WEAK LEARNERS,0.10587262200165426,"by the values wj as well the structure of the tree and the variables and values for the split at each
116"
WEAK LEARNERS,0.10669975186104218,"node which ultimately determine the Xj. We omit these dependencies for brevity.
117"
WEAK LEARNERS,0.10752688172043011,"Replacing the definition in equation 6 in our objective (equation 4) we get the following optimization
118"
WEAK LEARNERS,0.10835401157981803,"problem to find the optimal decision tree:
119"
WEAK LEARNERS,0.10918114143920596,"max
w1,...,wJ,X1,...,XJ J
X j=1"
WEAK LEARNERS,0.11000827129859388,"
wjP(Xj) −1"
WEAK LEARNERS,0.11083540115798181,"2w2
jQf(Xj)
 s.t. J
X"
WEAK LEARNERS,0.11166253101736973,"j=1
wjQf(Xj) = 0 , (7)"
WEAK LEARNERS,0.11248966087675766,"where P(Xj) and Qf(Xj) denote the probability of the event x ∈Xj under the respective distribution
120"
WEAK LEARNERS,0.11331679073614558,"and the constraint ensures that δf has zero expectation under qf. With respect to the leaf weights this
121"
WEAK LEARNERS,0.1141439205955335,"is a quadratic program whose optimal solution and objective values are respectively given by
122"
WEAK LEARNERS,0.11497105045492143,"w∗
j = P(Xj)"
WEAK LEARNERS,0.11579818031430934,"Qf(Xj) −1 ,
∆L∗
f (X1, . . . , XJ) = 1 2  
J
X j=1"
WEAK LEARNERS,0.11662531017369727,"P 2(Xj)
Qf(Xj) −1 "
WEAK LEARNERS,0.11745244003308519,".
(8)"
WEAK LEARNERS,0.11827956989247312,"Because carrying out the maximization of this optimal value over the tree structure that determines
123"
WEAK LEARNERS,0.11910669975186104,"the Xj is hard, we approximate its solution by greedily growing a tree that maximizes it when
124"
WEAK LEARNERS,0.11993382961124896,"considering how to split each node individually. A parent leaf with support XP is thus split into 2
125"
WEAK LEARNERS,0.12076095947063689,"child leaves, with disjoint support, XL ∪XR = XP , so as to maximize over all possible partitionings
126"
WEAK LEARNERS,0.12158808933002481,"along a single dimension, P (XP ), the following objective:
127"
WEAK LEARNERS,0.12241521918941274,"max
XL,XR∈P(XP )
P 2(XL)
Qf(XL) + P 2(XR)"
WEAK LEARNERS,0.12324234904880066,Qf(XR) −P 2(XP )
WEAK LEARNERS,0.12406947890818859,"Qf(XP ) .
(9)"
WEAK LEARNERS,0.12489660876757651,"Note that when using parametric weak learners, computing a second order step would typically
128"
WEAK LEARNERS,0.12572373862696443,"involve solving a linear system with a full Hessian. As we can see, this is not the case when the
129"
WEAK LEARNERS,0.12655086848635236,"weak learners are decision trees where the optimal value to assign to a leaf j does not depend on
130"
WEAK LEARNERS,0.1273779983457403,"any information from other leaves and, likewise, the optimal objective value is a sum of terms, each
131"
WEAK LEARNERS,0.1282051282051282,"depending only on information from a single leaf. This would have not been the case had we tried to
132"
WEAK LEARNERS,0.12903225806451613,"optimize the likelihood functional in Equation 2 directly instead of its quadratic approximation.
133"
SAMPLING,0.12985938792390406,"3.2
Sampling
134"
SAMPLING,0.130686517783292,"To compute the leaf values in equation 8 and the splitting criterion in equation 9 we would have to
135"
SAMPLING,0.1315136476426799,"know P(X) and be able to compute Qf(X) which is infeasible due to the untractable normalization
136"
SAMPLING,0.13234077750206782,"constant. We therefore estimate these quantities, with recourse to empirical data for P(X), and to
137"
SAMPLING,0.13316790736145576,"samples approximately drawn from the model with MCMC. Because even if the input space is not
138"
SAMPLING,0.13399503722084366,"partially discrete, f is still discontinuous and constant almost everywhere we can’t use gradient based
139"
SAMPLING,0.1348221670802316,"samplers and therefore rely on Gibbs sampling instead. This only requires evaluating each ft along
140"
SAMPLING,0.13564929693961952,"one dimension at a time, while keeping all others fixed which can be computed efficiently for a tree
141"
SAMPLING,0.13647642679900746,"by traversing it only once. However, since at boosting iteration t our energy function is a sum of t
142"
SAMPLING,0.13730355665839536,"trees, this computation scales linearly with the iteration number. This makes the overall time spent
143"
SAMPLING,0.1381306865177833,"sampling quadratic in the number of iterations and thus precludes us from training models with a
144"
SAMPLING,0.13895781637717122,"large number of trees.
145"
SAMPLING,0.13978494623655913,"In order to reduce the burden associated with this sampling, which can dominate the runtime of
146"
SAMPLING,0.14061207609594706,"training the model, we propose a new sampling approach that leverages the cumulative nature of
147"
SAMPLING,0.141439205955335,"boosting. The intuition behind this approach is that the set of samples used in the previous boosting
148"
SAMPLING,0.14226633581472292,"round are (approximately) drawn from a distribution that is already close to the new model distribution.
149"
SAMPLING,0.14309346567411083,"It could therefore be helpful to keep some of those samples, especially those that conform the best to
150"
SAMPLING,0.14392059553349876,"the new model. Rejection sampling allows us to do just that. The boosting update in terms of the
151"
SAMPLING,0.1447477253928867,"densities takes the following multiplicative form:
152"
SAMPLING,0.14557485525227462,"qt(x) = kt qt−1(x) exp (αtδft(x)) .
(10)"
SAMPLING,0.14640198511166252,"Here, k is an unknown multiplicative constant and since δft is given by a tree, we can easily bound
153"
SAMPLING,0.14722911497105046,"the exponential factor by finding the leaf with the largest value. We can therefore use the previous
154"
SAMPLING,0.1480562448304384,"model, qt−1(x), as a proposal distribution for which we already have a set of samples and keep each
155"
SAMPLING,0.1488833746898263,"sample, x, with an acceptance probability of:
156"
SAMPLING,0.14971050454921422,"paccept(x) = exp
h
αt

δft(x) −max
x
δft(x)
i
.
(11)"
SAMPLING,0.15053763440860216,"We note that knowledge of the constant kt is not necessary to compute this acceptance probability.
157"
SAMPLING,0.1513647642679901,"After removing samples from the pool, we can use Gibbs sampling to draw a new set of samples in
158"
SAMPLING,0.152191894127378,"order to keep a fixed total number of samples per round of boosting. Note also that q0 is typically a
159"
SAMPLING,0.15301902398676592,"simple model for which we can both directly evaluate the desired quantities (i.e., Q0(X) for a given
160"
SAMPLING,0.15384615384615385,"partition X) and cheaply draw exact samples from. As such, no sampling is required for the first
161"
SAMPLING,0.15467328370554176,"iteration of boosting and for the second we can draw exact samples from q1 with rejection sampling
162"
SAMPLING,0.1555004135649297,"using q0 as a proposal distribution.
163"
SAMPLING,0.15632754342431762,"This approach works better when either the range of ft is small or when the step sizes αt are small as
164"
SAMPLING,0.15715467328370555,"this leads to larger acceptance probabilities. Note that in practice it can be helpful to independently
165"
SAMPLING,0.15798180314309346,"refresh a fixed fraction samples, prefresh, at each round of boosting in order to encourage more
166"
SAMPLING,0.1588089330024814,"diverse samples between rounds. This can be accomplished by keeping each sample with a probability
167"
SAMPLING,0.15963606286186932,"paccept(x)(1 −prefresh) instead.
168"
REGULARIZATION,0.16046319272125723,"3.3
Regularization
169"
REGULARIZATION,0.16129032258064516,"The simplest way to regularize a boosting model is to stop training when overfitting is detected by
170"
REGULARIZATION,0.1621174524400331,"monitoring a suitable performance metric on a validation set. For NRGBoost this could be the increase
171"
REGULARIZATION,0.16294458229942102,"in log-likelihood at each boosting round. However, estimating this quantity would require drawing
172"
REGULARIZATION,0.16377171215880892,"additional validation samples from the model (see Appendix A). An alternative viable validation
173"
REGULARIZATION,0.16459884201819686,"strategy which needs no additional samples is to simply monitor a discriminative performance metric
174"
REGULARIZATION,0.1654259718775848,"(over one or more variables). This essentially amounts to monitoring the quality of qf(xi|x−i) instead
175"
REGULARIZATION,0.1662531017369727,"of the full qf(x).
176"
REGULARIZATION,0.16708023159636062,"Besides early stopping, the decision trees themselves can be regularized by limiting the depth or total
177"
REGULARIZATION,0.16790736145574855,"number of leaves of each tree. Additionally we can rely on other strategies such as disregarding splits
178"
REGULARIZATION,0.1687344913151365,"that would result in a leaf with too little training data, P(X), model data, Qf(X), volume V (X) or
179"
REGULARIZATION,0.1695616211745244,"too high of a ratio between training and model data P (X)/Qf (X). We found the latter to be the most
180"
REGULARIZATION,0.17038875103391232,"effective of these, not only yielding better generalization performance than other approaches, but also
181"
REGULARIZATION,0.17121588089330025,"having the added benefit of allowing us to lower bound the acceptance probability of our rejection
182"
REGULARIZATION,0.17204301075268819,"sampling scheme.
183"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1728701406120761,"4
Density Estimation Trees and Density Estimation Forests
184"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17369727047146402,"Density Estimation Trees (DET) were proposed by Ram and Gray [2011] as an alternative to
185"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17452440033085195,"histograms and kernel density estimation but have received little attention as generative models
186"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17535153019023986,"for sampling or other applications. They model the density function as a constant value over the
187"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1761786600496278,"support of each leaf in a binary tree, q = PJ
j=1
ˆ
P (Xj)
V (Xj)1Xj, with ˆP(X) being an empirical estimate
188"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17700578990901572,"of probability of the event x ∈X and V (X) denoting the volume of X. Note that it is possible
189"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17783291976840365,"to draw an exact sample from this type of model by randomly selecting a leaf, j ∈[1..J], given
190"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.17866004962779156,"probabilities ˆP(Xj), and then drawing a sample from a uniform distribution over Xj.
191"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1794871794871795,"To fit a DET, Ram and Gray [2011] propose optimizing the Integrated Squared Error (ISE) between the
192"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18031430934656742,"data and model distributions which, following a similar approach to Section 3.1, leads the following
193"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18114143920595532,"optimization problem when considering how to split a leaf node:
194"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18196856906534326,"max
XL,XR∈P(XP ) D(P(XL), V (XL)) + D(P(XR), V (XR)) −D(P(XP ), V (XP )) .
(12)"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1827956989247312,"For the ISE, D should be taken as the function DISE(P, V ) = P 2/V which leads to a similar splitting
195"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18362282878411912,"criterion to Equation 12 but replacing the previous model’s distribution with the volume measure V
196"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18444995864350702,"which can be interpreted as the uniform distribution on X (up to a multiplicative constant).
197"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18527708850289495,"Maximum Likelihood
Often generative models are trained to maximize the likelihood of the
198"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18610421836228289,"observed data. This was left for future work in Ram and Gray [2011] but, as we show in Appendix
199"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1869313482216708,"B, can be accomplished by replacing the D in Equation 12 with DKL(P, V ) = P log (P/V ).This
200"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18775847808105872,"choice of minimization criterion can be seen as analogous to the choice between Gini impurity and
201"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18858560794044665,"Shannon entropy in the computation of the information gain in decision trees.
202"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.18941273779983459,"Bagging and Feature Subsampling
Following the common approach in decision trees, Ram and
203"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1902398676592225,"Gray [2011] suggest the use of pruning for regularization of DET models. Practice has however
204"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19106699751861042,"evolved to prefer bagging as a form of regularization rather than relying on single decision trees. We
205"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19189412737799835,"employ same principle to DETs by fitting many trees on bootstrap samples of the data. We also adopt
206"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19272125723738626,"the common practice from Random Forests of randomly sampling a subset of features to consider
207"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.1935483870967742,"when splitting any leaf node in order to encourage independence between the different trees in the
208"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19437551695616212,"ensemble. The ensemble model, which we call Density Estimation Forests (DEF) in the sequence,
209"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19520264681555005,"is thus an additive mixture of DETs with uniform weights, therefore still allowing for normalized
210"
DENSITY ESTIMATION TREES AND DENSITY ESTIMATION FORESTS,0.19602977667493796,"density computation and exact sampling.
211"
RELATED WORK,0.1968569065343259,"5
Related Work
212"
RELATED WORK,0.19768403639371382,"Generative Boosting
Most prior work on generative boosting focuses on unstructured data and
213"
RELATED WORK,0.19851116625310175,"the use of parametric weak learners and is split between two approaches: (i) Additive methods that
214"
RELATED WORK,0.19933829611248965,"model the density function as an additive mixture of weak learners such as Rosset and Segal [2002],
215"
RELATED WORK,0.2001654259718776,"Tolstikhin et al. [2017]. (ii) Those that take a multiplicative approach modeling the density function as
216"
RELATED WORK,0.20099255583126552,"an unnormalized product of weak learners. The latter is equivalent to the energy based approach that
217"
RELATED WORK,0.20181968569065342,"writes the energy function (log density) as an additive sum of weak learners. Welling et al. [2002] in
218"
RELATED WORK,0.20264681555004135,"particular also approach boosting from the point of view of functional optimization of the likelihood
219"
RELATED WORK,0.20347394540942929,"or the logistic loss of an energy-based model. However, they rely on a first order local approximation
220"
RELATED WORK,0.20430107526881722,"of the objective since they focus on parametric weak learners such as restricted boltzman machines
221"
RELATED WORK,0.20512820512820512,"for which a second order step would be impractical.
222"
RELATED WORK,0.20595533498759305,"Greedy Multiplicative Boosting
Another more direct multiplicative boosting framework was first
223"
RELATED WORK,0.20678246484698098,"proposed by Tu [2007]. At each boosting round a discriminative classifier is trained to distinguish
224"
RELATED WORK,0.2076095947063689,"between empirical data and data generated by the current model by estimating the likelihood ratio
225"
RELATED WORK,0.20843672456575682,"p(x)/qt(x). This estimated ratio is used as a direct multiplicative factor to update the current model
226"
RELATED WORK,0.20926385442514475,"qt (after being raised to an appropriate step size). In ideal conditions this greedy procedure would
227"
RELATED WORK,0.21009098428453268,"converge in a single iteration if a step size of 1 would be used. While Tu [2007] does not prescribe a
228"
RELATED WORK,0.2109181141439206,"particular choice of classifier to use, Grover and Ermon [2017] proposes a similar concept where the
229"
RELATED WORK,0.21174524400330852,"ratio is estimated based on an adversarial bound for an f-divergence and Cranko and Nock [2019]
230"
RELATED WORK,0.21257237386269645,"provides additional analysis on this method. In Appendix C we dive deeper into the differences
231"
RELATED WORK,0.21339950372208435,"between NRGBoost and this approach when it is adapted to use trees as weak learners. We note, how-
232"
RELATED WORK,0.2142266335814723,"ever, that the main difference is that NRGBoost attempts to update the current density proportionally
233"
RELATED WORK,0.21505376344086022,"to an exponential of the ratio, exp (αt · p(x)/qt(x)), instead of the ratio directly.
234"
RELATED WORK,0.21588089330024815,"Tree-Based Density Modelling
Other authors have proposed tree-based density models similar to
235"
RELATED WORK,0.21670802315963605,"DET [Nock and Guillame-Bert, 2022] or additive mixtures of tree-based models [Correia et al., 2020,
236"
RELATED WORK,0.21753515301902399,"Wen and Hang, 2022, Watson et al., 2023] but perhaps surprisingly, the natural idea of creating an
237"
RELATED WORK,0.21836228287841192,"ensemble of DET models through bagging has not been explored before as far as we are aware. Two
238"
RELATED WORK,0.21918941273779982,"distinguishing features of some of these alternative approaches are: (i) Unlike DETs, the partitioning
239"
RELATED WORK,0.22001654259718775,"of each tree is not driven directly by a density estimation goal. Correia et al. [2020] leverages
240"
RELATED WORK,0.22084367245657568,"a standard discriminative Random Forest, therefore giving special treatment to a particular input
241"
RELATED WORK,0.22167080231596362,"variable whose conditional estimation drives the choice of partitions and Wen and Hang [2022]
242"
RELATED WORK,0.22249793217535152,"proposes using a mid-point random tree partitioning. (ii) Besides modelling the density function as
243"
RELATED WORK,0.22332506203473945,"uniform at the leaf of each tree, other authors propose leveraging more complex models [Correia
244"
RELATED WORK,0.22415219189412738,"et al., 2020, Watson et al., 2023] which can allow for the use of trees that are more representative
245"
RELATED WORK,0.22497932175351532,"with a smaller number of leaves. (iii) Nock and Guillame-Bert [2022] and Watson et al. [2023] both
246"
RELATED WORK,0.22580645161290322,"propose generative adversarial frameworks where the generator and discriminator are both a tree or
247"
RELATED WORK,0.22663358147229115,"an ensemble of trees respectively. Note that, unlike with boosting, in these approaches the new model
248"
RELATED WORK,0.22746071133167908,"doesn’t add to the previous one but replaces it instead.
249"
RELATED WORK,0.228287841191067,"Table 1: Single variable inference results. The reported values are the averages over 5 cross-validation
folds. The corresponding sample standard deviations are reported in Appendix G."
RELATED WORK,0.22911497105045492,"R2 ↑
AUC ↑
Accuracy ↑"
RELATED WORK,0.22994210090984285,"AB
CH
PR
AD
MBNE
MNIST
CT"
RELATED WORK,0.23076923076923078,"XGBoost
0.552
0.849
0.678
0.927
0.987
0.976
0.972"
RELATED WORK,0.23159636062861869,"RFDE
0.071
0.340
0.059
0.862
0.668
0.302
0.681
DEF (ISE)
0.467
0.737
0.566
0.854
0.653
0.206
0.790
DEF (KL)
0.482
0.801
0.639
0.892
0.939
0.487
0.852"
RELATED WORK,0.23242349048800662,"NRGBoost
0.547
0.850
0.676
0.920
0.974
0.966
0.949"
RELATED WORK,0.23325062034739455,"Other Recent Tree-Based approaches
Nock and Guillame-Bert [2023] proposes a different
250"
RELATED WORK,0.23407775020678245,"ensemble approach where each tree does not have their own leaf values that get added or multiplied
251"
RELATED WORK,0.23490488006617039,"to produce the final density, but instead serve to collectively define the partitioning of the input space.
252"
RELATED WORK,0.23573200992555832,"To train such models the authors propose a boosting framework where, rather than adding a new tree
253"
RELATED WORK,0.23655913978494625,"to the ensemble at every iteration, the model is initialized with a fixed number of tree root nodes and
254"
RELATED WORK,0.23738626964433415,"each iteration adds a split to an existing leaf node. Finally Jolicoeur-Martineau et al. [2023] propose
255"
RELATED WORK,0.23821339950372208,"a diffusion model where a tree-based model (e.g., GBDT) is used to regress the score function. Being
256"
RELATED WORK,0.23904052936311002,"a diffusion model, however, means that computing densities is untractable.
257"
EXPERIMENTS,0.23986765922249792,"6
Experiments
258"
EXPERIMENTS,0.24069478908188585,"For our experiments we use 5 tabular datasets from the UCI Machine Learning Repository [Dheeru
259"
EXPERIMENTS,0.24152191894127378,"and Karra Taniskidou, 2017]: Abalone (AB), Physicochemical Properties of Protein Tertiary Structure
260"
EXPERIMENTS,0.24234904880066171,"(PR), Adult (AD), MiniBooNE (MBNE) and Covertype (CT) as well as the California Housing (CH)
261"
EXPERIMENTS,0.24317617866004962,"available through the Scikit-Learn package [Pedregosa et al., 2011]. We also include a downsampled
262"
EXPERIMENTS,0.24400330851943755,"version of MNIST (by 2x along each dimension) which allows us to visually assess the quality of
263"
EXPERIMENTS,0.24483043837882548,"individual samples, something that is generally not possible with structured tabular data, and provides
264"
EXPERIMENTS,0.2456575682382134,"an example of the performance that can be achieved in an unstructured dataset with many features
265"
EXPERIMENTS,0.24648469809760132,"that are correlated among themselves. More details about these datasets are given in Appendix E.
266"
EXPERIMENTS,0.24731182795698925,"We split our experiments into two sections, the first to evaluate the quality of density models directly
267"
EXPERIMENTS,0.24813895781637718,"on a single variable inference task and the second to investigate the performance of our proposed
268"
EXPERIMENTS,0.24896608767576509,"models when used for sampling.
269"
SINGLE VARIABLE INFERENCE,0.24979321753515302,"6.1
Single Variable Inference
270"
SINGLE VARIABLE INFERENCE,0.2506203473945409,"In this section we test the ability of a generative model, trained to learn the density over all input
271"
SINGLE VARIABLE INFERENCE,0.25144747725392885,"variables, q(x), to infer the value of a single one. I.e., we wish to test how good is its estimate of
272"
SINGLE VARIABLE INFERENCE,0.2522746071133168,"q(xi|x−i). For this purpose we pick xi = y as the original target of the dataset, noting that the
273"
SINGLE VARIABLE INFERENCE,0.2531017369727047,"models that we train do not treat this variable in any special way, except for the selection of the best
274"
SINGLE VARIABLE INFERENCE,0.25392886683209265,"model in validation. As such, we would expect that the model’s performance in inference over this
275"
SINGLE VARIABLE INFERENCE,0.2547559966914806,"particular variable is indicative of its strength on any other single variable inference task and also
276"
SINGLE VARIABLE INFERENCE,0.2555831265508685,"indicative of the quality of the full q(x) from which the conditional probability estimate is derived.
277"
SINGLE VARIABLE INFERENCE,0.2564102564102564,"We use XGBoost [Chen and Guestrin, 2016] as a baseline for what should be achievable by a very
278"
SINGLE VARIABLE INFERENCE,0.2572373862696443,"strong discriminative model. Note that this model is trained to maximize the discriminative likelihood,
279"
SINGLE VARIABLE INFERENCE,0.25806451612903225,"Ex∼p log q(xi|x−i), directly, not wasting model capacity in learning other aspects of the full data
280"
SINGLE VARIABLE INFERENCE,0.2588916459884202,"distribution. As another generative baseline we use our own implementation of RFDE [Wen and
281"
SINGLE VARIABLE INFERENCE,0.2597187758478081,"Hang, 2022] which allows us to gauge the impact of the guided partitioning used in the DEF models
282"
SINGLE VARIABLE INFERENCE,0.26054590570719605,"over a random partitioning of the input space.
283"
SINGLE VARIABLE INFERENCE,0.261373035566584,"We use random search to tune the hyperparameters of the XGBoost model and a grid search to tune the
284"
SINGLE VARIABLE INFERENCE,0.26220016542597185,"most important hyperparameters of the generative density models. We employ 5-fold cross-validation,
285"
SINGLE VARIABLE INFERENCE,0.2630272952853598,"repeating the hyperparameter tuning on each fold for all datasets except for the largest one (CT) for
286"
SINGLE VARIABLE INFERENCE,0.2638544251447477,"which we report results on a single fold. For the full details of the experimental protocol please refer
287"
SINGLE VARIABLE INFERENCE,0.26468155500413565,"to Appendix F.
288"
SINGLE VARIABLE INFERENCE,0.2655086848635236,"Table 2: ML Efficiency results. The reported values are the averages over 5 different datasets
generated by the same model. The best methods for each dataset are in bold and methods whose
difference is < 2σ away from zero are underlined. The performance of XGBoost trained on the real
data is also reported for reference."
SINGLE VARIABLE INFERENCE,0.2663358147229115,"R2 ↑
AUC ↑
Accuracy ↑"
SINGLE VARIABLE INFERENCE,0.26716294458229944,"AB
CH
PR
AD
MBNE
MNIST
CT"
SINGLE VARIABLE INFERENCE,0.2679900744416873,"XGBoost
0.554
0.838
0.682
0.927
0.987
0.976
0.972"
SINGLE VARIABLE INFERENCE,0.26881720430107525,"TVAE
0.483
0.758
0.365
0.898
0.975
0.688
0.724
TabDDPM
0.539
0.807
0.596
0.910
0.984
0.579
0.818"
SINGLE VARIABLE INFERENCE,0.2696443341604632,"DEF (KL)
0.450
0.762
0.498
0.892
0.943
0.230
0.753
NRGBoost
0.528
0.801
0.573
0.914
0.977
0.959
0.895"
SINGLE VARIABLE INFERENCE,0.2704714640198511,"We find that NRGBoost performs better than the additive ensemble models (see Table 1) despite
289"
SINGLE VARIABLE INFERENCE,0.27129859387923905,"producing more compact ensembles. It often achieves comparable performance to XGBoost on the
290"
SINGLE VARIABLE INFERENCE,0.272125723738627,"smaller datasets and with a small gap on the three larger ones. We note also that for the regression
291"
SINGLE VARIABLE INFERENCE,0.2729528535980149,"datasets the generative models provide an estimate of the full conditional distribution over the target
292"
SINGLE VARIABLE INFERENCE,0.2737799834574028,"variable rather than a point estimate like XGBoost. While there are other variants of discriminative
293"
SINGLE VARIABLE INFERENCE,0.2746071133167907,"boosting that also provide an estimate of the aleatoric uncertainty [Duan et al., 2020], they rely on a
294"
SINGLE VARIABLE INFERENCE,0.27543424317617865,"parametric assumption about p(y|x) that needs to hold for any x.
295"
SAMPLING,0.2762613730355666,"6.2
Sampling
296"
SAMPLING,0.2770885028949545,"In this section, we compare the sampling performance of our proposed methods to neural-network-
297"
SAMPLING,0.27791563275434245,"based methods TVAE [Xu et al., 2019] and TabDDPM [Kotelnikov et al., 2022] on two metrics.
298"
SAMPLING,0.2787427626137304,"Machine Learning Efficiency
The Machine Learning (ML) efficiency has been a popular way
299"
SAMPLING,0.27956989247311825,"to measure the quality of generative models for sampling [Xu et al., 2019, Kotelnikov et al., 2022,
300"
SAMPLING,0.2803970223325062,"Borisov et al., 2022b]. It relies on using samples from the model to train a discriminative model which
301"
SAMPLING,0.2812241521918941,"is then evaluated on the real data. Note that this is similar to the single variable inference performance
302"
SAMPLING,0.28205128205128205,"from Section 6.1. In fact, if the density model’s support covers that of the full data, one would expect
303"
SAMPLING,0.28287841191067,"the discriminative model to recover the generator’s q(y|x), and therefore its performance, in the limit
304"
SAMPLING,0.2837055417700579,"where infinite generated data is used to train it.
305"
SAMPLING,0.28453267162944584,"We use an XGBoost model (with the hyperparameters tuned in real data) as the discriminative model
306"
SAMPLING,0.2853598014888337,"and train it using a similar number of training and validation samples as in the original data. For
307"
SAMPLING,0.28618693134822165,"the density models, we generate samples from the best model found in the previous section and
308"
SAMPLING,0.2870140612076096,"for non-density models we select their hyperparameters by evaluating the ML Efficiency in the
309"
SAMPLING,0.2878411910669975,"real validation set. Note that this leaves the sampling models at a potential advantage since the
310"
SAMPLING,0.28866832092638545,"hyperparameter selection is based on the metric that is being evaluated rather than the direct inference
311"
SAMPLING,0.2894954507857734,"performance of the previous section.
312"
SAMPLING,0.2903225806451613,"Discriminator Measure
Similar to Borisov et al. [2022b] we test the capacity of a discriminative
313"
SAMPLING,0.29114971050454924,"model to distinguish between real and generated data. We use the original validation set as the real
314"
SAMPLING,0.2919768403639371,"part of the training data in order to avoid benefiting generative methods that overfit their original
315"
SAMPLING,0.29280397022332505,"training set. A new validation set is carved out of the original test set (20%) and used to tune the
316"
SAMPLING,0.293631100082713,"hyperparameters of an XGBoost model which we use as our choice of discriminator, evaluating its
317"
SAMPLING,0.2944582299421009,"AUC on the remainder of the real test data.
318"
SAMPLING,0.29528535980148884,"We repeat all experiments 5 times, with 5 different generated datatsets from each model. Results are
319"
SAMPLING,0.2961124896608768,"reported in Tables 2 and 3 showing that (i) NRGBoost outperforms all other methods by substantial
320"
SAMPLING,0.2969396195202647,"margins in the discriminator measure except for the PR and the MBNE datasets. (ii) On the ML
321"
SAMPLING,0.2977667493796526,"Efficiency metric, TabDDPM outperforms NRGBoost by small margins on the small datasets which
322"
SAMPLING,0.2985938792390405,"could in part be explained by the denser hyperparameter tuning favouring models that perform
323"
SAMPLING,0.29942100909842845,"particularly well at inferring the target variable at the expense of the others. Nevertheless, NRGBoost
324"
SAMPLING,0.3002481389578164,"still significantly outperforms all other models on MNIST and CT. Its samples also look visually
325"
SAMPLING,0.3010752688172043,"similar to the real data in both the MNIST and California datasets (see Figures 1 and 2).
326"
SAMPLING,0.30190239867659224,"Table 3: Discriminator measure results. All results are the AUC of an XGBoost model trained to
distinguish real from generated data an therefore lower means better. The reported values are the
averages over 5 different datasets generated by the same model."
SAMPLING,0.3027295285359802,"AB
CH
PR
AD
MBNE
MNIST
CT"
SAMPLING,0.30355665839536805,"TVAE
0.971
0.834
0.940
0.898
1.000
1.000
0.999
TabDDPM
0.818
0.667
0.628
0.604
0.789
1.000
0.915"
SAMPLING,0.304383788254756,"DEF (KL)
0.823
0.751
0.877
0.956
1.000
1.000
0.999
NRGBoost
0.625
0.574
0.631
0.559
0.993
0.943
0.724"
SAMPLING,0.3052109181141439,Longitude
SAMPLING,0.30603804797353185,Latitude
SAMPLING,0.3068651778329198,Training Data
SAMPLING,0.3076923076923077,Longitude
SAMPLING,0.30851943755169564,NRGBoost
SAMPLING,0.3093465674110835,Longitude
SAMPLING,0.31017369727047145,DEF (KL)
SAMPLING,0.3110008271298594,Longitude
SAMPLING,0.3118279569892473,TabDDPM
SAMPLING,0.31265508684863524,Longitude TVAE
SAMPLING,0.3134822167080232,Figure 2: Joint histogram for the latitude and longitude for the California Housing dataset.
DISCUSSION,0.3143093465674111,"7
Discussion
327"
DISCUSSION,0.315136476426799,"While the additive tree models like DEF require no sampling to train and are easy to sample from, we
328"
DISCUSSION,0.3159636062861869,"find that in practice they require very deep trees to model the data well which, in turn, also requires
329"
DISCUSSION,0.31679073614557485,"using a large number of trees in the ensemble to regularize. In our experiments we found that their
330"
DISCUSSION,0.3176178660049628,"performance was often capped by the maximum number of leaves we allowed them to grow to (214).
331"
DISCUSSION,0.3184449958643507,"In contrast, we find that NRGBoost is able to model the data better while using shallower trees
332"
DISCUSSION,0.31927212572373864,"and in fewer number. Its main downside is that it can only be sampled from approximately using
333"
DISCUSSION,0.3200992555831266,"more expensive MCMC and also requires sampling during the training process. While our fast
334"
DISCUSSION,0.32092638544251445,"Gibbs sampling implementation coupled with our proposed sampling approach were able to mitigate
335"
DISCUSSION,0.3217535153019024,"the slow training, making these models much more usable in practice they are still cumbersome to
336"
DISCUSSION,0.3225806451612903,"use for sampling due to autocorrelation between samples from the same Markov Chain. We argue
337"
DISCUSSION,0.32340777502067825,"however that unlike in image or text generation where fast sampling is necessary for an interactive
338"
DISCUSSION,0.3242349048800662,"user experience, this can be less of a concern for the task of generating synthetic datasets where the
339"
DISCUSSION,0.3250620347394541,"one time cost of sampling is not as important as faithfully capturing the data generating distribution.
340"
DISCUSSION,0.32588916459884204,"We also find that tuning the hyperparameters of tree-based models is easier and less crucial than DL
341"
DISCUSSION,0.3267162944582299,"models for which many trials fail to produce a reasonable model. In particular we found NRGBoost
342"
DISCUSSION,0.32754342431761785,"to be rather robust, with different hyperparameters leading to small differences in performance.
343"
DISCUSSION,0.3283705541770058,"Finally, we note that like any other machine learning models, generative models are susceptible to
344"
DISCUSSION,0.3291976840363937,"overfitting and are thus liable to leak information about their training data when generating synthetic
345"
DISCUSSION,0.33002481389578164,"samples. In this respect, we believe that NRGBoost offers better tools to monitor and control
346"
DISCUSSION,0.3308519437551696,"overfitting than other alternatives (see Section 3.3) but, still, due consideration for this risk must be
347"
DISCUSSION,0.3316790736145575,"taken into account when sharing synthetic data.
348"
CONCLUSION,0.3325062034739454,"8
Conclusion
349"
CONCLUSION,0.3333333333333333,"In this work, we extend the two most popular tree-based discriminative methods for use in generative
350"
CONCLUSION,0.33416046319272125,"modeling. We find that our boosting approach, in particular, offers generally good discriminative
351"
CONCLUSION,0.3349875930521092,"performance and better overall sampling performance than alternatives. We hope that these results
352"
CONCLUSION,0.3358147229114971,"encourage further research into generative boosting approaches for tabular data, in particular exploring
353"
CONCLUSION,0.33664185277088504,"other applications besides sampling that are enabled by density models.
354"
REFERENCES,0.337468982630273,"References
355"
REFERENCES,0.33829611248966085,"Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.
356"
REFERENCES,0.3391232423490488,"Deep Neural Networks and Tabular Data: A Survey. IEEE Transactions on Neural Networks and Learning
357"
REFERENCES,0.3399503722084367,"Systems, pages 1–21, 2022a. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2022.3229161. URL
358"
REFERENCES,0.34077750206782464,"http://arxiv.org/abs/2110.01889. arXiv:2110.01889 [cs].
359"
REFERENCES,0.3416046319272126,"Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language Mod-
360"
REFERENCES,0.3424317617866005,"els are Realistic Tabular Data Generators, October 2022b. URL http://arxiv.org/abs/2210.06280.
361"
REFERENCES,0.34325889164598844,"arXiv:2210.06280 [cs].
362"
REFERENCES,0.34408602150537637,"Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM
363"
REFERENCES,0.34491315136476425,"SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785–794, August 2016.
364"
REFERENCES,0.3457402812241522,"doi: 10.1145/2939672.2939785. URL http://arxiv.org/abs/1603.02754. arXiv:1603.02754 [cs].
365"
REFERENCES,0.3465674110835401,"Alvaro Correia, Robert Peharz, and Cassio P de Campos. Joints in random forests. In H. Larochelle, M. Ranzato,
366"
REFERENCES,0.34739454094292804,"R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
367"
REFERENCES,0.348221670802316,"pages 11404–11415. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_
368"
REFERENCES,0.3490488006617039,"files/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf.
369"
REFERENCES,0.34987593052109184,"Zac Cranko and Richard Nock. Boosted Density Estimation Remastered. In Proceedings of the 36th International
370"
REFERENCES,0.3507030603804797,"Conference on Machine Learning, pages 1416–1425. PMLR, May 2019. URL https://proceedings.mlr.
371"
REFERENCES,0.35153019023986765,"press/v97/cranko19b.html. ISSN: 2640-3498.
372"
REFERENCES,0.3523573200992556,"Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing
373"
REFERENCES,0.3531844499586435,"Magazine, 29(6):141–142, 2012.
374"
REFERENCES,0.35401157981803144,"Dua Dheeru and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http://archive.ics.
375"
REFERENCES,0.3548387096774194,"uci.edu/ml.
376"
REFERENCES,0.3556658395368073,"Tony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, and Alejandro Schuler.
377"
REFERENCES,0.3564929693961952,"NGBoost: Natural Gradient Boosting for Probabilistic Prediction, June 2020. URL http://arxiv.org/
378"
REFERENCES,0.3573200992555831,"abs/1910.03225. arXiv:1910.03225 [cs, stat].
379"
REFERENCES,0.35814722911497104,"Justin Engelmann and Stefan Lessmann. Conditional Wasserstein GAN-based Oversampling of Tabular Data for
380"
REFERENCES,0.358974358974359,"Imbalanced Learning, August 2020. URL http://arxiv.org/abs/2008.09202. arXiv:2008.09202 [cs].
381"
REFERENCES,0.3598014888337469,"Ju Fan, Junyou Chen, Tongyu Liu, Yuwei Shen, Guoliang Li, and Xiaoyong Du. Relational data synthesis using
382"
REFERENCES,0.36062861869313484,"generative adversarial networks: a design space exploration. Proceedings of the VLDB Endowment, 13(12):
383"
REFERENCES,0.36145574855252277,"1962–1975, August 2020. ISSN 2150-8097. doi: 10.14778/3407790.3407802. URL https://dl.acm.org/
384"
REFERENCES,0.36228287841191065,"doi/10.14778/3407790.3407802.
385"
REFERENCES,0.3631100082712986,"Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
Additive logistic regression:
a statisti-
386"
REFERENCES,0.3639371381306865,"cal view of boosting (With discussion and a rejoinder by the authors).
The Annals of Statis-
387"
REFERENCES,0.36476426799007444,"tics, 28(2):337–407, April 2000.
ISSN 0090-5364, 2168-8966.
doi:
10.1214/aos/1016218223.
388"
REFERENCES,0.3655913978494624,"URL
https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/
389"
REFERENCES,0.3664185277088503,"Additive-logistic-regression--a-statistical-view-of-boosting-With/10.1214/aos/
390"
REFERENCES,0.36724565756823824,"1016218223.full. Publisher: Institute of Mathematical Statistics.
391"
REFERENCES,0.3680727874276261,"Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep learning
392"
REFERENCES,0.36889991728701405,"on tabular data?, July 2022. URL http://arxiv.org/abs/2207.08815. arXiv:2207.08815 [cs, stat].
393"
REFERENCES,0.369727047146402,"Aditya Grover and Stefano Ermon. Boosted Generative Models, December 2017. URL http://arxiv.org/
394"
REFERENCES,0.3705541770057899,"abs/1702.08484. arXiv:1702.08484 [cs, stat].
395"
REFERENCES,0.37138130686517784,"Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau,
396"
REFERENCES,0.37220843672456577,"Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
397"
REFERENCES,0.3730355665839537,"Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peter-
398"
REFERENCES,0.3738626964433416,"son, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
399"
REFERENCES,0.3746898263027295,"Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357–362, September
400"
REFERENCES,0.37551695616211744,"2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.
401"
REFERENCES,0.3763440860215054,"Alexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via diffusion
402"
REFERENCES,0.3771712158808933,"and flow-based gradient-boosted trees, 2023.
403"
REFERENCES,0.37799834574028124,"James Jordon, Jinsung Yoon, and Mihaela van der Schaar. PATE-GAN: Generating Synthetic Data with Differ-
404"
REFERENCES,0.37882547559966917,"ential Privacy Guarantees. December 2018. URL https://openreview.net/forum?id=S1zk9iRqF7.
405"
REFERENCES,0.37965260545905705,"Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
406"
REFERENCES,0.380479735318445,"LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In Advances in Neural Information
407"
REFERENCES,0.3813068651778329,"Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.
408"
REFERENCES,0.38213399503722084,"cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html.
409"
REFERENCES,0.3829611248966088,"Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. TabDDPM: Modelling Tabular Data
410"
REFERENCES,0.3837882547559967,"with Diffusion Models, September 2022. URL http://arxiv.org/abs/2209.15421. arXiv:2209.15421
411"
REFERENCES,0.38461538461538464,"[cs].
412"
REFERENCES,0.3854425144747725,"Richard Nock and Mathieu Guillame-Bert. Generative trees: Adversarial and copycat. In Kamalika Chaudhuri,
413"
REFERENCES,0.38626964433416044,"Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th
414"
REFERENCES,0.3870967741935484,"International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,
415"
REFERENCES,0.3879239040529363,"pages 16906–16951. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/nock22a.
416"
REFERENCES,0.38875103391232424,"html.
417"
REFERENCES,0.38957816377171217,"Richard Nock and Mathieu Guillame-Bert. Generative forests, 2023.
418"
REFERENCES,0.3904052936311001,"Melissa E. O’Neill. Pcg: A family of simple fast space-efficient statistically good algorithms for random number
419"
REFERENCES,0.391232423490488,"generation. Technical Report HMC-CS-2014-0905, Harvey Mudd College, Claremont, CA, September 2014.
420"
REFERENCES,0.3920595533498759,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
421"
REFERENCES,0.39288668320926384,"V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
422"
REFERENCES,0.3937138130686518,"Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
423"
REFERENCES,0.3945409429280397,"Parikshit Ram and Alexander G. Gray. Density estimation trees. In Proceedings of the 17th ACM SIGKDD
424"
REFERENCES,0.39536807278742764,"international conference on Knowledge discovery and data mining, pages 627–635, San Diego California
425"
REFERENCES,0.39619520264681557,"USA, August 2011. ACM. ISBN 978-1-4503-0813-7. doi: 10.1145/2020408.2020507. URL https:
426"
REFERENCES,0.3970223325062035,"//dl.acm.org/doi/10.1145/2020408.2020507.
427"
REFERENCES,0.3978494623655914,"Saharon Rosset and Eran Segal. Boosting Density Estimation. In Advances in Neural Information Processing
428"
REFERENCES,0.3986765922249793,"Systems, volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_files/paper/2002/
429"
REFERENCES,0.39950372208436724,"hash/3de568f8597b94bda53149c7d7f5958c-Abstract.html.
430"
REFERENCES,0.4003308519437552,"Yang Song and Diederik P. Kingma. How to Train Your Energy-Based Models. arXiv:2101.03288 [cs, stat],
431"
REFERENCES,0.4011579818031431,"January 2021. URL http://arxiv.org/abs/2101.03288. arXiv: 2101.03288.
432"
REFERENCES,0.40198511166253104,"Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Schölkopf. AdaGAN:
433"
REFERENCES,0.40281224152191897,"Boosting Generative Models, May 2017. URL http://arxiv.org/abs/1701.02386. arXiv:1701.02386
434"
REFERENCES,0.40363937138130684,"[cs, stat].
435"
REFERENCES,0.4044665012406948,"Zhuowen Tu. Learning Generative Models via Discriminative Approaches. In 2007 IEEE Conference on
436"
REFERENCES,0.4052936311000827,"Computer Vision and Pattern Recognition, pages 1–8, June 2007. doi: 10.1109/CVPR.2007.383035. ISSN:
437"
REFERENCES,0.40612076095947064,"1063-6919.
438"
REFERENCES,0.40694789081885857,"David S. Watson, Kristin Blesch, Jan Kapar, and Marvin N. Wright. Adversarial random forests for density
439"
REFERENCES,0.4077750206782465,"estimation and generative modeling. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors,
440"
REFERENCES,0.40860215053763443,"Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of
441"
REFERENCES,0.4094292803970223,"Proceedings of Machine Learning Research, pages 5357–5375. PMLR, 25–27 Apr 2023. URL https:
442"
REFERENCES,0.41025641025641024,"//proceedings.mlr.press/v206/watson23a.html.
443"
REFERENCES,0.4110835401157982,"Max Welling, Richard Zemel, and Geoffrey E Hinton. Self Supervised Boosting. In Advances in Neural
444"
REFERENCES,0.4119106699751861,"Information Processing Systems, volume 15. MIT Press, 2002. URL https://papers.nips.cc/paper_
445"
REFERENCES,0.41273779983457404,"files/paper/2002/hash/cd0cbcc668fe4bc58e0af3cc7e0a653d-Abstract.html.
446"
REFERENCES,0.41356492969396197,"Hongwei Wen and Hanyuan Hang. Random forest density estimation. In Kamalika Chaudhuri, Stefanie Jegelka,
447"
REFERENCES,0.4143920595533499,"Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International
448"
REFERENCES,0.4152191894127378,"Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 23701–
449"
REFERENCES,0.4160463192721257,"23722. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/wen22c.html.
450"
REFERENCES,0.41687344913151364,"Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.
Modeling Tabu-
451"
REFERENCES,0.41770057899090157,"lar data using Conditional GAN.
In Advances in Neural Information Processing Systems, vol-
452"
REFERENCES,0.4185277088502895,"ume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
453"
REFERENCES,0.41935483870967744,"254ed7d2de3b23ab10936522dd547b78-Abstract.html.
454"
REFERENCES,0.42018196856906537,"Zilong Zhao, Aditya Kunar, Hiek Van der Scheer, Robert Birke, and Lydia Y. Chen. CTAB-GAN: Effective
455"
REFERENCES,0.42100909842845324,"Table Data Synthesizing, May 2021. URL http://arxiv.org/abs/2102.08369. arXiv:2102.08369 [cs].
456"
REFERENCES,0.4218362282878412,"A
Additional Derivations
457"
REFERENCES,0.4226633581472291,"The expected log-likelihood for an energy-based model (EBM),
458"
REFERENCES,0.42349048800661704,qf(x) = exp (f(x))
REFERENCES,0.42431761786600497,"Z[f]
,
(13)"
REFERENCES,0.4251447477253929,"is given by
459"
REFERENCES,0.42597187758478083,"L[f] = Ex∼p log qf(x) = Ex∼pf(x) −log Z[f] .
(14)"
REFERENCES,0.4267990074441687,"The first variation of L can be computed as
460"
REFERENCES,0.42762613730355664,δL[f; g] := dL[f + ϵg] dϵ
REFERENCES,0.4284532671629446,"ϵ=0
= Ex∼p g(x) −δ log Z[f; g] = Ex∼p g(x) −Ex∼qf g(x) .
(15)"
REFERENCES,0.4292803970223325,"This is a linear functional of its second argument, g, and can be regarded as a directional derivative
461"
REFERENCES,0.43010752688172044,"of L at f along a variation g. The last equality comes from the following computation of the first
462"
REFERENCES,0.43093465674110837,"variation of the log-partition function:
463"
REFERENCES,0.4317617866004963,δ log Z[f; g] = δZ[f; g]
REFERENCES,0.4325889164598842,"Z[f]
(16)"
REFERENCES,0.4334160463192721,"=
1
Z[f] X"
REFERENCES,0.43424317617866004,"x
exp′ (f(x)) g(x)
(17) =
X x"
REFERENCES,0.43507030603804797,exp (f(x))
REFERENCES,0.4358974358974359,"Z[f]
g(x)
(18)"
REFERENCES,0.43672456575682383,"= Ex∼qf g(x) .
(19)"
REFERENCES,0.43755169561621177,"Analogous to a Hessian, we can differentiate Equation 15 again along a second independent variation
464"
REFERENCES,0.43837882547559964,"h of f yielding a symmetric bilinear functional which we will write as δ2L[f; g, h]. Note that the
465"
REFERENCES,0.4392059553349876,"first term in equation 2 is linear in f and thus has no curvature, so we only have to consider the log
466"
REFERENCES,0.4400330851943755,"partition function itself:
467"
REFERENCES,0.44086021505376344,"δ2L[f; g, h] := ∂2L[f + ϵg + εh] ∂ϵ∂ε"
REFERENCES,0.44168734491315137,"(ϵ,ε)=0
(20)"
REFERENCES,0.4425144747725393,"= −δ2 log Z[f; g, h] = −δ {δ log Z[f; g]} [f; h]
(21) = −δ"
REFERENCES,0.44334160463192723,"(
1
Z[f] X"
REFERENCES,0.4441687344913151,"x
exp (f(x)) g(x) )"
REFERENCES,0.44499586435070304,"[f; h]
(22)"
REFERENCES,0.445822994210091,= δZ[f; h] Z2[f] X
REFERENCES,0.4466501240694789,"x
exp (f(x)) g(x) −
1
Z[f] X"
REFERENCES,0.44747725392886684,"x
exp′ (f(x)) g(x)h(x)
(23)"
REFERENCES,0.44830438378825477,= δZ[f; h]
REFERENCES,0.4491315136476427,"Z[f]
· Ex∼qf g(x) −
1
Z[f] X"
REFERENCES,0.44995864350703063,"x
exp (f(x)) g(x)h(x)
(24)"
REFERENCES,0.4507857733664185,"= Ex∼qf h(x) · Ex∼qf g(x) −Ex∼qf h(x)g(x)
(25)"
REFERENCES,0.45161290322580644,"= −Covx∼qf (g(x), h(x)) .
(26)"
REFERENCES,0.45244003308519437,"Note that this functional is negative semi-definite for all f, i.e. δ2L[f; h, h] ≤0, meaning that the
468"
REFERENCES,0.4532671629445823,"log-likelihood is a concave functional of f.
469"
REFERENCES,0.45409429280397023,"Using these results, we can now compute the Taylor expansion of the increment in log-likelihood L
470"
REFERENCES,0.45492142266335817,"from a change f →f + δf up to second order in δf:
471"
REFERENCES,0.4557485525227461,∆Lf[δf] = δL[f; δf] + 1
REFERENCES,0.456575682382134,"2δ2L[f; δf, δf]
(27)"
REFERENCES,0.4574028122415219,= Ex∼pδf(x) −Ex∼qf δf(x) −1
REFERENCES,0.45822994210090984,"2Varx∼qf δf(x) .
(28)"
REFERENCES,0.45905707196029777,"As an aside, defining the functional derivative, δJ[f]"
REFERENCES,0.4598842018196857,"δf(x), of a functional J implicitly by:
472 X x"
REFERENCES,0.46071133167907363,"δJ[f]
δf(x)g(x) = δJ[f; g] ,
(29)"
REFERENCES,0.46153846153846156,"we can formally define, by analogy with the parametric case, the Fisher Information ""Matrix"" (FIM)
473"
REFERENCES,0.46236559139784944,"at f as the following bilinear functional of two independent variations g and h:
474"
REFERENCES,0.46319272125723737,"F[f; g, h] := −
X y,z"
REFERENCES,0.4640198511166253,"
Ex∼qf
δ2 log qf(x)
δf(y)δf(z)"
REFERENCES,0.46484698097601324,"
g(y)h(z)
(30) =
X y,z"
REFERENCES,0.46567411083540117,"δ2 log Z[f]
δf(y)δf(z)g(y)h(z)
(31)"
REFERENCES,0.4665012406947891,"= δ2 log Z[f; g, h] .
(32)"
REFERENCES,0.46732837055417703,"The only difference to the second-order variation of 2 computed in equation 20 would be that the
475"
REFERENCES,0.4681555004135649,"expectation is taken under the model distribution, qf, instead of the data distribution p. However,
476"
REFERENCES,0.46898263027295284,"because the only term in log qf(x) that is non-linear in f is the log-partition functional, which is not
477"
REFERENCES,0.46980976013234077,"a function of x, this expectation plays no role in the computation and we get the result that the FIM is
478"
REFERENCES,0.4706368899917287,"the same as the negative Hessian of the log-likelihood for these models.
479"
REFERENCES,0.47146401985111663,"A.1
Application to Piecewise Constant Functions
480"
REFERENCES,0.47229114971050457,"Considering a weak learner such as
481"
REFERENCES,0.4731182795698925,"δf(x) = J
X"
REFERENCES,0.4739454094292804,"j=1
wj1Xj(x) ,
(33)"
REFERENCES,0.4747725392886683,"where the subsets Xj are disjoint and cover the entire input space, X, we have that
482"
REFERENCES,0.47559966914805624,"Ex∼qδf(x) =
X"
REFERENCES,0.47642679900744417,"x∈X
q(x) J
X"
REFERENCES,0.4772539288668321,"j=1
wj1Xj(x)
(34) = J
X"
REFERENCES,0.47808105872622003,"j=1
wj
X"
REFERENCES,0.47890818858560796,"x∈Xj
q(x) = J
X"
REFERENCES,0.47973531844499584,"j=1
wjQ(Xj) .
(35)"
REFERENCES,0.48056244830438377,"Similarly, making use of the fact that 1Xi(x)1Xj(x) = δij1Xi(x), we can compute
483"
REFERENCES,0.4813895781637717,"Ex∼qδf 2(x) =
X"
REFERENCES,0.48221670802315963,"x∈X
q(x)  
J
X"
REFERENCES,0.48304383788254757,"j=1
wj1Xj(x)   2 = J
X"
REFERENCES,0.4838709677419355,"j=1
w2
jQ(Xj) .
(36)"
REFERENCES,0.48469809760132343,"In fact, we can extend this to any ordinary function of δf:
484"
REFERENCES,0.4855252274607113,"Ex∼q g (δf(x)) =
X"
REFERENCES,0.48635235732009924,"x∈X
q(x) J
X"
REFERENCES,0.48717948717948717,"j=1
1Xj(x)g (δf(x))
(37) = J
X j=1 X"
REFERENCES,0.4880066170388751,"x∈Xj
q(x)g(wj)
(38) = J
X"
REFERENCES,0.48883374689826303,"j=1
g(wj)Q(Xj) ,
(39)"
REFERENCES,0.48966087675765096,"where we made use of the fact that the 1Xj constitute a partition of unity:
485 1 = J
X"
REFERENCES,0.4904880066170389,"j=1
1Xj(x) .
(40)"
REFERENCES,0.4913151364764268,"Finally, we can compute the increase in likelihood from a step f →f + α · δf as
486"
REFERENCES,0.4921422663358147,"L[f + α · δf] −L[f] = Ex∼p [α · δf(x)] −log Z[f + α · δf] + log Z[f]
(41)
= αEx∼pδf(x) −log Ex∼qf exp(αδf(x))
(42) = α J
X"
REFERENCES,0.49296939619520264,"j=1
wjP (Xj) −log J
X"
REFERENCES,0.49379652605459057,"j=1
Qf (Xj) exp (αwj) ,
(43)"
REFERENCES,0.4946236559139785,"where in equation 42 we made use of the equality:
487"
REFERENCES,0.49545078577336643,"log Z[f + α · δf] −log Z[f] = log
P"
REFERENCES,0.49627791563275436,x exp(f(x) + αδf(x))
REFERENCES,0.49710504549214224,"Z[f]
= log
X"
REFERENCES,0.49793217535153017,"x
qf(x) exp(αδf(x)) ,"
REFERENCES,0.4987593052109181,"(44)
and of the result in equation 39 in the final step.
488"
REFERENCES,0.49958643507030603,"This result can be used to conduct a line search over the step size using training data and to estimate
489"
REFERENCES,0.5004135649296939,"an increase in likelihood at each round of boosting for the purpose of early stopping, using validation
490"
REFERENCES,0.5012406947890818,"data.
491"
REFERENCES,0.5020678246484698,"B
Training Density Estimation Trees
492"
REFERENCES,0.5028949545078577,"Density Estimation Trees (DET) [Ram and Gray, 2011] model the density function as a piecewise
493"
REFERENCES,0.5037220843672456,"constant function,
494"
REFERENCES,0.5045492142266336,"q(x) = J
X"
REFERENCES,0.5053763440860215,"j=1
vj1Xj(x) ,
(45)"
REFERENCES,0.5062034739454094,"where Xj are given by a partitioning of the input space X induced by a binary tree and the vj are the
495"
REFERENCES,0.5070306038047974,"density values associated with each leaf that, for the time being, we will only require to be such that
496"
REFERENCES,0.5078577336641853,"q(x) sums to one.
497"
REFERENCES,0.5086848635235732,"Ram and Gray [2011] proposes fitting DET models to directly minimize a generative objective, the
498"
REFERENCES,0.5095119933829612,"Integrated Squared Error (ISE) between the data generating distribution, p(x) and the model:
499"
REFERENCES,0.5103391232423491,"min
q∈Q X"
REFERENCES,0.511166253101737,"x∈X
(p(x) −q(x))2 .
(46)"
REFERENCES,0.5119933829611248,"Noting that q is a function as in Equation 45 and that SJ
j=1 Xj = X, we can rewrite this as
500"
REFERENCES,0.5128205128205128,"min
v1,...,vJ,X1,...,XJ X"
REFERENCES,0.5136476426799007,"x∈X
p2(x) + J
X j=1 X x∈Xj"
REFERENCES,0.5144747725392886," 
v2
j −2vjp(x)
 s.t. J
X j=1 X"
REFERENCES,0.5153019023986766,"x∈Xj
vj = 1 . (47)"
REFERENCES,0.5161290322580645,"Since the first term in the objective does not depend on the model this optimization problem can be
501"
REFERENCES,0.5169561621174524,"further simplified as
502"
REFERENCES,0.5177832919768404,"min
v1,...,vJ,X1,...,XJ J
X j=1"
REFERENCES,0.5186104218362283," 
v2
j V (Xj) −2vjP(Xj)
 s.t. J
X"
REFERENCES,0.5194375516956162,"j=1
vjV (Xj) = 1 , (48)"
REFERENCES,0.5202646815550042,"where V (X) denotes the volume of a subset X. Solving this quadratic program for the vj we obtain
503"
REFERENCES,0.5210918114143921,"the following optimal leaf values and objective:
504"
REFERENCES,0.52191894127378,"v∗
j = P(Xj)"
REFERENCES,0.522746071133168,"V (Xj) ,
ISE∗(X1, . . . , XJ) = − J
X j=1"
REFERENCES,0.5235732009925558,P 2(Xj)
REFERENCES,0.5244003308519437,"Vf(Xj) .
(49)"
REFERENCES,0.5252274607113316,"One can therefore grow a tree by greedily choosing to split a parent leaf with support XP into two
505"
REFERENCES,0.5260545905707196,"leaves with supports XL and XR so as to maximize the following criterion:
506"
REFERENCES,0.5268817204301075,"max
XL,XR∈P(XP )
P 2(XL)"
REFERENCES,0.5277088502894954,V (XL) + P 2(XR)
REFERENCES,0.5285359801488834,V (XR) −P 2(XP )
REFERENCES,0.5293631100082713,"V (XP ) .
(50)"
REFERENCES,0.5301902398676592,"B.1
Maximum Likelihood
507"
REFERENCES,0.5310173697270472,"To maximize the likelihood,
508"
REFERENCES,0.5318444995864351,"max
q
Ex∼p log q(x) ,
(51)"
REFERENCES,0.532671629445823,"rather than the ISE one can use the same approach. Here the optimization problem to solve is:
509"
REFERENCES,0.533498759305211,"max
v1,...,vJ,X1,...,XJ J
X"
REFERENCES,0.5343258891645989,"j=1
P(Xj) log vj s.t. J
X"
REFERENCES,0.5351530190239868,"j=1
vjV (Xj) = 1 . (52)"
REFERENCES,0.5359801488833746,"This is, again, easy to solve for vj since it is separable over j after removing the constraint using
510"
REFERENCES,0.5368072787427626,"Lagrange multipliers. The optimal leaf values and objective are in this case:
511"
REFERENCES,0.5376344086021505,"v∗
j = P(Xj)"
REFERENCES,0.5384615384615384,"V (Xj) ,
L∗(X1, . . . , XJ) = J
X"
REFERENCES,0.5392886683209264,"j=1
P(Xj) log P(Xj)"
REFERENCES,0.5401157981803143,"Vf(Xj) .
(53)"
REFERENCES,0.5409429280397022,"The only change is therefore to the splitting criterion which should become:
512"
REFERENCES,0.5417700578990902,"max
XL,XR∈P(XP ) P(XL) log P(XL)"
REFERENCES,0.5425971877584781,V (XL) + P(XR) log P(XR)
REFERENCES,0.543424317617866,V (XR) −P(XP ) log P(XP )
REFERENCES,0.544251447477254,"V (XP ) .
(54)"
REFERENCES,0.5450785773366419,"C
Greedy Tree Based Multiplicative Boosting
513"
REFERENCES,0.5459057071960298,"In multiplicative generative boosting an unnormalized current density model, ˜qt−1(x), is updated at
514"
REFERENCES,0.5467328370554178,"each boosting round by multiplication with a new factor δqαt
t (x):
515"
REFERENCES,0.5475599669148056,"˜qt(x) = ˜qt−1(x) · δqαt
t (x) .
(55)"
REFERENCES,0.5483870967741935,"For our proposed NRGBoost, this factor is chosen in order to maximize a local quadratic approx-
516"
REFERENCES,0.5492142266335814,"imation of the log likelihood around qt−1 as a functional of the log density (see Section 3). The
517"
REFERENCES,0.5500413564929694,"motivation behind the greedy approach of Tu [2007] or Grover and Ermon [2017] is to instead make
518"
REFERENCES,0.5508684863523573,"the update factor δqt(x) proportional to the likelihood ratio rt(x) := p(x)/qt−1(x) directly, which
519"
REFERENCES,0.5516956162117452,"under ideal conditions would mean that the method converges immediately when choosing a step size
520"
REFERENCES,0.5525227460711332,"αt = 1. In more realistic setting, however, this method has been shown to converge under conditions
521"
REFERENCES,0.5533498759305211,"on the performance of the individual δqt as discriminators between real and generated data [Tu, 2007,
522"
REFERENCES,0.554177005789909,"Grover and Ermon, 2017, Cranko and Nock, 2019].
523"
REFERENCES,0.555004135649297,"While in principle this desired rt(x) could be derived from any binary classifier that is trained to
524"
REFERENCES,0.5558312655086849,"predict a probability of a datapoint being generated (e.g., by training it to minimize a strictly proper
525"
REFERENCES,0.5566583953680728,"loss) and Tu [2007] does not prescribe any particular choice, Grover and Ermon [2017] propose
526"
REFERENCES,0.5574855252274608,"relying on the following variational bound of an f-divergence to derive an estimator for this ratio:
527"
REFERENCES,0.5583126550868487,"Df(P∥Qt−1) ≥sup
u∈Ut"
REFERENCES,0.5591397849462365,"
Ex∼p u(x) −Ex∼qt−1f ∗(u(x))

.
(56)"
REFERENCES,0.5599669148056244,"Here f ∗denotes the convex conjugate of f. This bound is tight, with the optimum being achieved for
528"
REFERENCES,0.5607940446650124,"u∗
t (x) = f ′(p(x)/qt−1(x)), if Ut is capable of representing this function. (f ′)−1 (u∗
t (x)) can thus be
529"
REFERENCES,0.5616211745244003,"interpreted as an approximation of rt(x).
530"
REFERENCES,0.5624483043837882,"Adapting this method to use trees as weak learners can be accomplished by considering Ut in Equation
531"
REFERENCES,0.5632754342431762,"56 to be defined by tree functions u = 1/J PJ
j=1 wj1Xj with leaf values wj and leaf supports Xj.
532"
REFERENCES,0.5641025641025641,"At each boosting iteration a new tree, u∗
t can thus be grown to greedily optimize the lower bound in
533"
REFERENCES,0.564929693961952,"the r.h.s. of Equation 56 and setting δqt(x) = (f ′)−1 (u∗
t (x)) which is thus also a tree with the same
534"
REFERENCES,0.56575682382134,"leaf supports and leaf values given by vj := (f ′)−1 (wj). This leads to the seaprable optimization
535"
REFERENCES,0.5665839536807279,"problem:
536"
REFERENCES,0.5674110835401158,"max
w1,...,wJ,X1,...,XJ J
X"
REFERENCES,0.5682382133995038,"j
[P(Xj)wj −Q(Xj)f ∗(wj)] .
(57)"
REFERENCES,0.5690653432588917,Table 4: Comparison of splitting criterion and leaf weights for the different versions of boosting.
REFERENCES,0.5698924731182796,"Splitting Criterion
Leaf Values (Density)"
REFERENCES,0.5707196029776674,"DiscBGM (KL)
P log (P/Q)
P/Q
DiscBGM (χ2)
P 2/Q
P/Q
NRGBoost
P 2/Q
exp (P/Q −1)"
REFERENCES,0.5715467328370554,"Note that we drop the iteration indices from this point onward for brevity. Maximizing over wj with
537"
REFERENCES,0.5723738626964433,"the Xj fixed we have that w∗
j = f ′ (P (Xj)/Q(Xj)) which yields the optimal value
538"
REFERENCES,0.5732009925558312,"J∗(X1, . . . , Xj) =
X j"
REFERENCES,0.5740281224152192,"
P(Xj)f ′
P(Xj) Q(Xj)"
REFERENCES,0.5748552522746071,"
−Q(Xj)(f ∗◦f ′)
P(Xj) Q(Xj)"
REFERENCES,0.575682382133995,"
(58)"
REFERENCES,0.576509511993383,"that in turn determines the splitting criterion as a function of the choice of f. Finally, the optimal
539"
REFERENCES,0.5773366418527709,"density values for the leaves are given by
540"
REFERENCES,0.5781637717121588,"v∗
j = (f ′)−1 (w∗
j ) = P(Xj)"
REFERENCES,0.5789909015715468,"Q(Xj) .
(59)"
REFERENCES,0.5798180314309347,"It is interesting to note two particular choices of f-divergences. For the KL divergence, f(t) = t log t
541"
REFERENCES,0.5806451612903226,"and f ′(t) = 1 + log t = (f ∗)−1 (t). This leads to
542"
REFERENCES,0.5814722911497106,"JKL(X1, . . . , Xj) =
X"
REFERENCES,0.5822994210090985,"j
P(Xj) log P(Xj)"
REFERENCES,0.5831265508684863,"Q(Xj)
(60)"
REFERENCES,0.5839536807278742,"as the splitting criterion. The Pearson χ2 divergence, with f(t) = (t −1)2, leads to the same splitting
543"
REFERENCES,0.5847808105872622,"criterion as NRGBoost. Note however that for NRGBoost the leaf values for the multiplicative update
544"
REFERENCES,0.5856079404466501,"of the density are given by exp (P (Xj)/Q(Xj) −1) instead of the ratio directly. Table 4 summarizes
545"
REFERENCES,0.586435070306038,"these results.
546"
REFERENCES,0.587262200165426,"Another interesting observation is that a DET model can be interpreted as a single round of greedy
547"
REFERENCES,0.5880893300248139,"multiplicative boosting starting from a uniform initial model. The choice of the ISE as the criterion to
548"
REFERENCES,0.5889164598842018,"optimize the DET corresponds to the choice of Pearson’s χ2 divergence and likelihood to the choice
549"
REFERENCES,0.5897435897435898,"of KL divergence.
550"
REFERENCES,0.5905707196029777,"D
Implementation Details
551"
REFERENCES,0.5913978494623656,"Discretization
In our practical implementation of tree based methods we first discretize the input
552"
REFERENCES,0.5922249793217536,"space by binning continuous numerical variables by quantiles. Furthermore we also bin discrete
553"
REFERENCES,0.5930521091811415,"numerical variables in order to keep their cardinalities smaller than 256. This can also be interpreted
554"
REFERENCES,0.5938792390405294,"as establishing a priori a set of discrete values to consider when splitting on each numerical variable
555"
REFERENCES,0.5947063688999172,"and is done for computational efficiency, being inspired by LightGBM [Ke et al., 2017].
556"
REFERENCES,0.5955334987593052,"Categorical Splitting
For splitting on a categorical variable we once again take inspiration from
557"
REFERENCES,0.5963606286186931,"LightGBM. Rather than relying on one-vs-all splits we found it better to first order the possible
558"
REFERENCES,0.597187758478081,"categorical values at a leaf according to a pre-defined sorting function and then choose the optimal
559"
REFERENCES,0.598014888337469,"many-vs-many split as if the variable was numerical. The function used to sort the values is the leaf
560"
REFERENCES,0.5988420181968569,"value function. E.g., for splitting on a categorical variable xi we order each possible categorical value
561"
REFERENCES,0.5996691480562448,"k by ˆ
P (xi=k,X−i)/ ˆ
Q(xi=k,X−i) in the case of NRGBoost where X−i denotes the leaf support over the
562"
REFERENCES,0.6004962779156328,"remaining variables.
563"
REFERENCES,0.6013234077750207,"Tree Growth Strategy
We always grow trees in best first order. I.e., we always split the current
564"
REFERENCES,0.6021505376344086,"leaf node that yields the maximum gain in the chosen objective value.
565"
REFERENCES,0.6029776674937966,"Line Search
As mentioned in Section 3, we perform a line search to find the optimal step size after
566"
REFERENCES,0.6038047973531845,"each round of boosting in order to maximize the likelihood gain in Equation 43. Because evaluating
567"
REFERENCES,0.6046319272125724,"multiple possible step sizes, αt, is inexpensive, we simply do a grid search over 101 different step
568"
REFERENCES,0.6054590570719603,"sizes in the range [10−3, 10] with their logarithm uniformly distributed.
569"
REFERENCES,0.6062861869313482,"Table 5: Dataset Information. We respect the original test sets of each dataset when provided,
otherwise we set aside 20% of the original dataset as a test set. 20% of the remaining data is set aside
as a validation set used for hyperparameter tuning."
REFERENCES,0.6071133167907361,"Abbr
Name
Train + Val
Test
Num
Cat
Target
Cardinality"
REFERENCES,0.607940446650124,"AB
Abalone
3342
835
7
1
Num
29
CH
California Housing
16512
4128
8
0
Num
Continuous
PR
Protein
36584
9146
9
0
Num
Continuous
AD
Adult
32560
16280∗
6
8
Cat
2
MBNE
MiniBooNE
104051
26013
50
0
Cat
2
MNIST
MNIST (downsampled)
60000
10000∗
196
0
Cat
10
CT
Covertype
464810
116202
10
2
Cat
7"
REFERENCES,0.608767576509512,∗Original test set was respected.
REFERENCES,0.6095947063688999,"Random Forest Density Estimation (RFDE)
We implement the RFDE method [Wen and Hang,
570"
REFERENCES,0.6104218362282878,"2022] after quantile discretization of the dataset and therefore split at the midpoint of the discretized
571"
REFERENCES,0.6112489660876758,"dimension instead of the original one. When a leaf support has odd cardinality over the splitting
572"
REFERENCES,0.6120760959470637,"dimension a random choice is made over the two possible splitting values. Finally, the original paper
573"
REFERENCES,0.6129032258064516,"does not mention how to split over categorical domains. We therefore choose to randomly split the
574"
REFERENCES,0.6137303556658396,"possible categorical values for a leaf evenly as we found that this yielded slightly better results than a
575"
REFERENCES,0.6145574855252275,"random one vs all split.
576"
REFERENCES,0.6153846153846154,"Code
Our implementation of the proposed tree-based methods is mostly Python code using the
577"
REFERENCES,0.6162117452440034,"NumPy library [Harris et al., 2020]. We implement the tree evaluation and Gibbs sampling in C,
578"
REFERENCES,0.6170388751033913,"making use of the PCG library [O’Neill, 2014] for random number generation.
579"
REFERENCES,0.6178660049627791,"E
Datasets
580"
REFERENCES,0.618693134822167,"We use 5 datasets from the UCI Machine Learning Repository [Dheeru and Karra Taniskidou, 2017]:
581"
REFERENCES,0.619520264681555,"Abalone, Physicochemical Properties of Protein Tertiary Structure (referred to as Protein in the
582"
REFERENCES,0.6203473945409429,"sequence), Adult, MiniBooNE and Covertype. We also use the California Housing dataset which was
583"
REFERENCES,0.6211745244003308,"downloaded through the Scikit-Learn package Pedregosa et al. [2011] and a downsampled version of
584"
REFERENCES,0.6220016542597188,"the MNIST dataset Deng [2012]. Table 5 summarizes the main details of these datasets as well as the
585"
REFERENCES,0.6228287841191067,"approximate number of samples used for train/validation/test for each cross-validation fold.
586"
REFERENCES,0.6236559139784946,"F
Experimental Setup
587"
REFERENCES,0.6244830438378826,"F.1
XGBoost Hyperparameter Tuning
588"
REFERENCES,0.6253101736972705,"To tune the hyperparameters of XGBoost we use 100 trials of random search with the search space
589"
REFERENCES,0.6261373035566584,"defined in Table 6.
590"
REFERENCES,0.6269644334160464,Table 6: XGBoost hyperparameter tuning search space. δ(0) denotes a point mass distribution at 0.
REFERENCES,0.6277915632754343,"Parameter
Distribution or Value"
REFERENCES,0.6286186931348222,"learning_rate
LogUniform
 
10−3, 1.0
"
REFERENCES,0.62944582299421,"max_leaves
Uniform ({16, 32, 64, 128, 256, 512, 1024})
min_child_weight
LogUniform
 
10−1, 103"
REFERENCES,0.630272952853598,"reg_lambda
0.5 · δ(0) + 0.5 · LogUniform
 
10−3, 10
"
REFERENCES,0.6311000827129859,"reg_alpha
0.5 · δ(0) + 0.5 · LogUniform
 
10−3, 10
"
REFERENCES,0.6319272125723738,"max_leaves
0 (we already limit the number of leaves)
grow_policy
lossguide
tree_method
hist"
REFERENCES,0.6327543424317618,"Each model was trained for 1000 boosting rounds on regression and binary classification tasks. For
591"
REFERENCES,0.6335814722911497,"multi-class classification tasks a maximum number of 200 rounds of boosting was used due to the
592"
REFERENCES,0.6344086021505376,"larger size of the datasets and because a separate tree is built at every round for each class. The
593"
REFERENCES,0.6352357320099256,"best model was selected based on the validation set, together with the boosting round where the best
594"
REFERENCES,0.6360628618693135,"performance was attained. The test metrics reported correspond to the performance of the selected
595"
REFERENCES,0.6368899917287014,"model at that boosting round on the test set.
596"
REFERENCES,0.6377171215880894,"F.2
TVAE Hyperparameter Tuning
597"
REFERENCES,0.6385442514474773,"To tune the hyperparameters of TVAE we use 50 trials of random search with the search spaces
598"
REFERENCES,0.6393713813068652,"defined in Table 7.
599"
REFERENCES,0.6401985111662531,"The TVAE implementations used are from the latest version of the SDV package (https://github.
600"
REFERENCES,0.6410256410256411,"com/sdv-dev/SDV) available at the time.
601"
REFERENCES,0.6418527708850289,"Table 7:
TVAE hyperparameter tuning search space.
We set both compress_dims and
decompress_dims to have the number of layers specified by num_layers, with hidden_dim
hidden units in each layer. We use larger batch sizes and smaller number of epochs for the larger
datasets (MBNE, MNIST, CO)."
REFERENCES,0.6426799007444168,"Parameter
Datasets
Distribution or Value"
REFERENCES,0.6435070306038048,"epochs
AB, CH, PR, AD
Uniform ([100..500])
MBNE, MNIST, CO
Uniform ([50..200])
batch_size
AB, CH, PR, AD
Uniform ({100, 200, . . . , 500})
MBNE, MNIST, CO
Uniform ({500, 1000, . . . , 2500})
embedding_dim
all
Uniform ({32, 64, 128, 256, 512})
hidden_dim
all
Uniform ({32, 64, 128, 256, 512})
num_layers
all
Uniform ({1, 2, 3})
compress_dims
all
(hidden_dim,) * num_layers
decompress_dims
all
(hidden_dim,) * num_layers"
REFERENCES,0.6443341604631927,"F.3
TabDDPM Hyperparameter Tuning
602"
REFERENCES,0.6451612903225806,"To tune the hyperparameters of TabDDPM we use 50 trials of random search with the same search
603"
REFERENCES,0.6459884201819686,"space that the original authors use in their paper [Kotelnikov et al., 2022].
604"
REFERENCES,0.6468155500413565,"We use the official implementation (https://github.com/yandex-research/tab-ddpm)
605"
REFERENCES,0.6476426799007444,"adapted to use our datasets and validation setup.
606"
REFERENCES,0.6484698097601324,"F.4
Random Forest Density Estimation
607"
REFERENCES,0.6492969396195203,"For RFDE models we train a total of 1000 trees. The only hyperparameter that we tune is the
608"
REFERENCES,0.6501240694789082,"maximum number of leaves per tree for which we test the values [26, 27, . . . , 214]. For the Adult
609"
REFERENCES,0.6509511993382961,"dataset, due to limitations of our tree evaluation implementation we only values test up to 213.
610"
REFERENCES,0.6517783291976841,"F.5
Density Estimation Forests Hyperparameter Tuning
611"
REFERENCES,0.652605459057072,"We train ensembles with 1000 DET models. Only three hyperparameters are tuned, using three nested
612"
REFERENCES,0.6534325889164598,"loops. Every loop runs over the possible values of a single parameter in a pre-defined order with early
613"
REFERENCES,0.6542597187758478,"stopping triggering if a value fails to improve the validation metric over the previous one. The tuned
614"
REFERENCES,0.6550868486352357,"parameters along with their possible values are reported in Table 8
615"
REFERENCES,0.6559139784946236,"F.6
NRGBoost
616"
REFERENCES,0.6567411083540116,"We train NRGBoost models for a maximum of 200 rounds of boosting. The starting point of each
617"
REFERENCES,0.6575682382133995,"NRGBoost model was selected as a mixture model between a uniform distribution (10%) and the
618"
REFERENCES,0.6583953680727874,"Table 8: DEF models grid search space. Rows are in order of outermost loop to innermost loop.
Note that for the Adult dataset, due to limitations of the implementation a maximum number of 8192
leaves is used instead of 16384."
REFERENCES,0.6592224979321754,"Parameter
Description"
REFERENCES,0.6600496277915633,"max_leaves
The maximum number of leaves per tree
[16384, 4096, 1024, 256]
feature_frac
The fraction of features to consider when splitting a node
as a function of the total number of features d
[d−1/2, d−1/4, 1]"
REFERENCES,0.6608767576509512,"min_data_in_leaf
The minimum number of data points that need to be left
in each leaf for a split to be considered
[0, 1, 3, 10, 30]"
REFERENCES,0.6617038875103392,"product of training marginals (90%) on the discretized input space. We observed that this mixture
619"
REFERENCES,0.6625310173697271,"coefficient does not have much impact on the results however.
620"
REFERENCES,0.663358147229115,"We only tune two parameters for NRGBoost Models:
621"
REFERENCES,0.664185277088503,"• The maximum number of leaves for which we try the values [64, 256, 1024, 4096] in order,
622"
REFERENCES,0.6650124069478908,"stopping if performance fails to improve from one value to the next. For the CT dataset we
623"
REFERENCES,0.6658395368072787,"also include 16384 in the values to test.
624"
REFERENCES,0.6666666666666666,"• The constant factor by which the optimal step size determined by the line search is shrunk
625"
REFERENCES,0.6674937965260546,"at each round of boosting. This is essentially the ""learning rate"" parameter. To tune it we
626"
REFERENCES,0.6683209263854425,"perform a Golden-section search for the log of its value using a total of 6 evaluations. The
627"
REFERENCES,0.6691480562448304,"range we use is [0.01, 0.5].
628"
REFERENCES,0.6699751861042184,"This means that at maximum we train only 24 NRGBoost models (30 for CT).
629"
REFERENCES,0.6708023159636063,"All other relevant parameters are fixed and their values, along with a short description, is given in
630"
REFERENCES,0.6716294458229942,"Table 9.
631"
REFERENCES,0.6724565756823822,Table 9: NRGBoost fixed parameters.
REFERENCES,0.6732837055417701,"Parameter
Description"
REFERENCES,0.674110835401158,"num_rounds
Total number of rounds of boosting
200
splitter
How the next leaf to split is determined
best
line_search
Whether to use a line search in determining the step size
True
max_ratio_leaf
Maximum ratio between training data and model data in each leaf
2"
REFERENCES,0.674937965260546,"num_samples
Total number of samples in the sample pool
80000
320000 (CT)
p_refresh
Indepdendent probability that a sample from the pool is replaced
0.1
burn_in
Number of samples to discard from the beginning of each chain
100
num_chains
Number of independent chains used for sampling
16
64 (CT)"
REFERENCES,0.6757650951199339,"F.7
Evaluation Setup
632"
REFERENCES,0.6765922249793217,"Single variable inference
For the single variable inference evaluation, the best models are selected
633"
REFERENCES,0.6774193548387096,"by their discriminative performance on a validation set. The entire setup is repeated five times with
634"
REFERENCES,0.6782464846980976,"different cross-validation folds and with different seeds for all sources of randomness except on the
635"
REFERENCES,0.6790736145574855,"CT dataset due to its large size. For the Adult and MNIST datasets the test set is fixed but training
636"
REFERENCES,0.6799007444168734,"and validation splits are still rotated.
637"
REFERENCES,0.6807278742762614,"Sampling
For the sampling evaluation we use a single train/validation/test split of the real data
638"
REFERENCES,0.6815550041356493,"(corresponding to the first fold in the previous setup) for training the generative models. The density
639"
REFERENCES,0.6823821339950372,"models used are those previously selected based on their single variable inference performance
640"
REFERENCES,0.6832092638544252,"on the validation set. For the sampling models (TVAE and TabDDPM) we directly evaluate their
641"
REFERENCES,0.6840363937138131,"ML Efficiency using the validation data by training an XGBoost model on generated data. The
642"
REFERENCES,0.684863523573201,"hyperparameters used for this XGBoost model are those selected on the real data in the previous
643"
REFERENCES,0.685690653432589,"experiment. We only use a generated validation set in order to select the best stopping point for
644"
REFERENCES,0.6865177832919769,"XGBoost.
645"
REFERENCES,0.6873449131513648,"ML Efficiency
For each selected model we sample a train and validation sets with the same number
646"
REFERENCES,0.6881720430107527,"of samples as those used on the original data. For NRGBoost we generate these samples by running
647"
REFERENCES,0.6889991728701406,"64 chains in parallel with 100 steps of burn in and downsampling their outputs by 30 (for the smaller
648"
REFERENCES,0.6898263027295285,"datasets) or 10 (for MBNE, MNIST and CT). The setup is repeated 5 times with 5 different datasets
649"
REFERENCES,0.6906534325889164,"generated for each method.
650"
REFERENCES,0.6914805624483044,"Discriminator Measure
We create the training, validation and test sets to train an XGBoost model
651"
REFERENCES,0.6923076923076923,"to discriminate between real and generated data using the following process:
652"
REFERENCES,0.6931348221670802,"• The original validation set is used as the real part of the training set in order to avoid
653"
REFERENCES,0.6939619520264682,"benefitting generative methods that overfit their training set.
654"
REFERENCES,0.6947890818858561,"• The original test set is split 20%/80%. The 20% portion is used as the real part of the
655"
REFERENCES,0.695616211745244,"validation set and the 80% portion as the real part of the test set.
656"
REFERENCES,0.696443341604632,"• To form the generated part of the training, validation and test sets for the smaller datasets
657"
REFERENCES,0.6972704714640199,"we sample data according to the original number of samples in the train, validation and
658"
REFERENCES,0.6980976013234078,"test splits on the real data. Note that this makes the ratio of real to synthetic data 1:4 in the
659"
REFERENCES,0.6989247311827957,"training set. This is deliberate because for these smaller datasets the original validation has
660"
REFERENCES,0.6997518610421837,"few samples and adding extra synthetic data helps the discriminator.
661"
REFERENCES,0.7005789909015715,"• For the larger datasets we generate the same number of synthetic samples as there are real
662"
REFERENCES,0.7014061207609594,"samples on each split, therefore making every ratio 1:1 because the discriminator is typically
663"
REFERENCES,0.7022332506203474,"already too powerful and doesn’t need extra data.
664"
REFERENCES,0.7030603804797353,"Because, in contrast to the previous metric, having a lower number of effective samples helps rather
665"
REFERENCES,0.7038875103391232,"than hurts we take extra precautions to not generate correlated data with NRGBoost. We draw each
666"
REFERENCES,0.7047146401985112,"sample by running its own independent chain for 100 steps starting from an independent sample from
667"
REFERENCES,0.7055417700578991,"the initial model which is a rather slow process. The setup is repeated 5 times with 5 different sets of
668"
REFERENCES,0.706368899917287,"generated samples from each method.
669"
REFERENCES,0.707196029776675,"F.8
Computational Resources
670"
REFERENCES,0.7080231596360629,"The experiments were run on a machine equipped with an AMD Ryzen 7 7700X 8 core CPU and 32
671"
REFERENCES,0.7088502894954508,"GB of RAM. The comparisons with TVAE and TabDDPM further made use of a GeForce RTX 3060
672"
REFERENCES,0.7096774193548387,"GPU with 12 GB of VRAM.
673"
REFERENCES,0.7105045492142267,"G
Additional Results
674"
REFERENCES,0.7113316790736146,"G.1
Standard Errors
675"
REFERENCES,0.7121588089330024,"In Tables 10, 11 and 12 we report the sample standard deviations obtained for the main tables
676"
REFERENCES,0.7129859387923904,"presented in the paper.
677"
REFERENCES,0.7138130686517783,"G.2
Samples
678"
REFERENCES,0.7146401985111662,"In Figure G.2 we show the convergence of a Gibbs sampler sampling from a NRGBoost model. In
679"
REFERENCES,0.7154673283705542,"only a few samples each chain appears to have converged to the data manifold after starting at a
680"
REFERENCES,0.7162944582299421,"random sample from the initial model (a mixture between the product of training marginals and a
681"
REFERENCES,0.71712158808933,"uniform). Note how consecutive samples are autocorrelated. In particular it can be rare for a chain
682"
REFERENCES,0.717948717948718,"to switch between two different modes of the distribution (e.g., switching digits) even though a few
683"
REFERENCES,0.7187758478081059,"such transitions can be observed.
684"
REFERENCES,0.7196029776674938,Table 10: Single variable inference sample standard deviations.
REFERENCES,0.7204301075268817,"R2
AUC
Accuracy"
REFERENCES,0.7212572373862697,"AB
CH
PR
AD
MBNE
MNIST"
REFERENCES,0.7220843672456576,"XGBoost
0.0354
0.0092
0.0036
0.0004
0.0005
0.0017"
REFERENCES,0.7229114971050455,"RFDE
0.0963
0.0039
0.0071
0.0023
0.0078
0.0101
DEF (ISE)
0.0373
0.0080
0.0023
0.0026
0.0108
0.0107
DEF (KL)
0.0271
0.0083
0.0038
0.0005
0.0009
0.0073"
REFERENCES,0.7237386269644334,"NRGBoost
0.0358
0.0113
0.0087
0.0006
0.0007
0.0009"
REFERENCES,0.7245657568238213,Table 11: ML Efficiency results sample standard deviations.
REFERENCES,0.7253928866832092,"R2
AUC
Accuracy"
REFERENCES,0.7262200165425972,"AB
CH
PR
AD
MBNE
MNIST
CT"
REFERENCES,0.7270471464019851,"TVAE
0.0059
0.0054
0.0054
0.0011
0.0002
0.0088
0.0013
TabDDPM
0.0182
0.0049
0.0072
0.0007
0.0000
0.0250
0.0012"
REFERENCES,0.727874276261373,"DEF (KL)
0.0131
0.0063
0.0073
0.0011
0.0022
0.0283
0.0029
NRGBoost
0.0161
0.0010
0.0076
0.0009
0.0009
0.0008
0.0011"
REFERENCES,0.728701406120761,Table 12: Discriminator measure sample standard deviations.
REFERENCES,0.7295285359801489,"AB
CH
PR
AD
MBNE
MNIST
CT"
REFERENCES,0.7303556658395368,"TVAE
0.0039
0.0055
0.0017
0.0012
0.0001
0.0000
0.0001
TabDDPM
0.0146
0.0045
0.0043
0.0022
0.0024
0.0000
0.0074"
REFERENCES,0.7311827956989247,"DEF (KL)
0.0129
0.0081
0.0022
0.0016
0.0000
0.0000
0.0001
NRGBoost
0.0167
0.0115
0.0059
0.0032
0.0005
0.0026
0.0058"
REFERENCES,0.7320099255583127,"Figure 3: Downsampled MNIST samples generated by Gibbs sampling from a NRGBoost model.
Each row corresponds to an independent chain initialized with a sample from the initial model f0
(first column). Each column represents a consecutive sample from the chain."
REFERENCES,0.7328370554177006,"Table 13: Best NRGBoost model parameters per dataset and the wall time taken to train it. The
format is minutes:seconds."
REFERENCES,0.7336641852770885,"AB
CH
PR
AD
MBNE
MNIST
CT"
REFERENCES,0.7344913151364765,"max_leaves
64
1024
1024
256
1024
4096
16384
shrinkage
0.14
0.063
0.14
0.09
0.199
0.199
0.098
Time
1:18
4:17
5:27
3:54
20:36
149:30
179:11"
REFERENCES,0.7353184449958643,"G.3
Time
685"
REFERENCES,0.7361455748552522,"In Table 13 we report the best hyperparameters found for NRGBoost for the first cross-validation
686"
REFERENCES,0.7369727047146402,"fold together with the time taken to train this best model.
687"
REFERENCES,0.7377998345740281,"NeurIPS Paper Checklist
688"
CLAIMS,0.738626964433416,"1. Claims
689"
CLAIMS,0.739454094292804,"Question: Do the main claims made in the abstract and introduction accurately reflect the
690"
CLAIMS,0.7402812241521919,"paper’s contributions and scope?
691"
CLAIMS,0.7411083540115798,"Answer: [Yes]
692"
CLAIMS,0.7419354838709677,"Justification: Claims about proposal of novel methods are justified in Sections 3 and 4.
693"
CLAIMS,0.7427626137303557,"Claims about empirical results are justified in Section 6 and Appendix G.
694"
CLAIMS,0.7435897435897436,"Guidelines:
695"
CLAIMS,0.7444168734491315,"• The answer NA means that the abstract and introduction do not include the claims
696"
CLAIMS,0.7452440033085195,"made in the paper.
697"
CLAIMS,0.7460711331679074,"• The abstract and/or introduction should clearly state the claims made, including the
698"
CLAIMS,0.7468982630272953,"contributions made in the paper and important assumptions and limitations. A No or
699"
CLAIMS,0.7477253928866832,"NA answer to this question will not be perceived well by the reviewers.
700"
CLAIMS,0.7485525227460711,"• The claims made should match theoretical and experimental results, and reflect how
701"
CLAIMS,0.749379652605459,"much the results can be expected to generalize to other settings.
702"
CLAIMS,0.750206782464847,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
703"
CLAIMS,0.7510339123242349,"are not attained by the paper.
704"
LIMITATIONS,0.7518610421836228,"2. Limitations
705"
LIMITATIONS,0.7526881720430108,"Question: Does the paper discuss the limitations of the work performed by the authors?
706"
LIMITATIONS,0.7535153019023987,"Answer: [Yes]
707"
LIMITATIONS,0.7543424317617866,"Justification: We discuss limitations of our proposed method both in the section that intro-
708"
LIMITATIONS,0.7551695616211745,"duces it (Section 3) as well as in the experiments (Section 6) and discussion (Section 7)
709"
LIMITATIONS,0.7559966914805625,"sections.
710"
LIMITATIONS,0.7568238213399504,"Guidelines:
711"
LIMITATIONS,0.7576509511993383,"• The answer NA means that the paper has no limitation while the answer No means that
712"
LIMITATIONS,0.7584780810587263,"the paper has limitations, but those are not discussed in the paper.
713"
LIMITATIONS,0.7593052109181141,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
714"
LIMITATIONS,0.760132340777502,"• The paper should point out any strong assumptions and how robust the results are to
715"
LIMITATIONS,0.76095947063689,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
716"
LIMITATIONS,0.7617866004962779,"model well-specification, asymptotic approximations only holding locally). The authors
717"
LIMITATIONS,0.7626137303556658,"should reflect on how these assumptions might be violated in practice and what the
718"
LIMITATIONS,0.7634408602150538,"implications would be.
719"
LIMITATIONS,0.7642679900744417,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
720"
LIMITATIONS,0.7650951199338296,"only tested on a few datasets or with a few runs. In general, empirical results often
721"
LIMITATIONS,0.7659222497932175,"depend on implicit assumptions, which should be articulated.
722"
LIMITATIONS,0.7667493796526055,"• The authors should reflect on the factors that influence the performance of the approach.
723"
LIMITATIONS,0.7675765095119934,"For example, a facial recognition algorithm may perform poorly when image resolution
724"
LIMITATIONS,0.7684036393713813,"is low or images are taken in low lighting. Or a speech-to-text system might not be
725"
LIMITATIONS,0.7692307692307693,"used reliably to provide closed captions for online lectures because it fails to handle
726"
LIMITATIONS,0.7700578990901572,"technical jargon.
727"
LIMITATIONS,0.770885028949545,"• The authors should discuss the computational efficiency of the proposed algorithms
728"
LIMITATIONS,0.771712158808933,"and how they scale with dataset size.
729"
LIMITATIONS,0.7725392886683209,"• If applicable, the authors should discuss possible limitations of their approach to
730"
LIMITATIONS,0.7733664185277088,"address problems of privacy and fairness.
731"
LIMITATIONS,0.7741935483870968,"• While the authors might fear that complete honesty about limitations might be used by
732"
LIMITATIONS,0.7750206782464847,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
733"
LIMITATIONS,0.7758478081058726,"limitations that aren’t acknowledged in the paper. The authors should use their best
734"
LIMITATIONS,0.7766749379652605,"judgment and recognize that individual actions in favor of transparency play an impor-
735"
LIMITATIONS,0.7775020678246485,"tant role in developing norms that preserve the integrity of the community. Reviewers
736"
LIMITATIONS,0.7783291976840364,"will be specifically instructed to not penalize honesty concerning limitations.
737"
THEORY ASSUMPTIONS AND PROOFS,0.7791563275434243,"3. Theory Assumptions and Proofs
738"
THEORY ASSUMPTIONS AND PROOFS,0.7799834574028123,"Question: For each theoretical result, does the paper provide the full set of assumptions and
739"
THEORY ASSUMPTIONS AND PROOFS,0.7808105872622002,"a complete (and correct) proof?
740"
THEORY ASSUMPTIONS AND PROOFS,0.7816377171215881,"Answer: [Yes]
741"
THEORY ASSUMPTIONS AND PROOFS,0.782464846980976,"Justification: All results presented in the main paper are justified in Appendices A and B.
742"
THEORY ASSUMPTIONS AND PROOFS,0.7832919768403639,"Guidelines:
743"
THEORY ASSUMPTIONS AND PROOFS,0.7841191066997518,"• The answer NA means that the paper does not include theoretical results.
744"
THEORY ASSUMPTIONS AND PROOFS,0.7849462365591398,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
745"
THEORY ASSUMPTIONS AND PROOFS,0.7857733664185277,"referenced.
746"
THEORY ASSUMPTIONS AND PROOFS,0.7866004962779156,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
747"
THEORY ASSUMPTIONS AND PROOFS,0.7874276261373035,"• The proofs can either appear in the main paper or the supplemental material, but if
748"
THEORY ASSUMPTIONS AND PROOFS,0.7882547559966915,"they appear in the supplemental material, the authors are encouraged to provide a short
749"
THEORY ASSUMPTIONS AND PROOFS,0.7890818858560794,"proof sketch to provide intuition.
750"
THEORY ASSUMPTIONS AND PROOFS,0.7899090157154673,"• Inversely, any informal proof provided in the core of the paper should be complemented
751"
THEORY ASSUMPTIONS AND PROOFS,0.7907361455748553,"by formal proofs provided in appendix or supplemental material.
752"
THEORY ASSUMPTIONS AND PROOFS,0.7915632754342432,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7923904052936311,"4. Experimental Result Reproducibility
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932175351530191,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.794044665012407,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948717948717948,"of the paper (regardless of whether the code and data are provided or not)?
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7956989247311828,"Answer: [Yes]
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7965260545905707,"Justification: Additional implementation details of our method are provided in Appendix D
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7973531844499586,"and the full experimental setup is described in detail in Appendix F.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7981803143093466,"Guidelines:
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7990074441687345,"• The answer NA means that the paper does not include experiments.
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998345740281224,"• If the paper includes experiments, a No answer to this question will not be perceived
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8006617038875103,"well by the reviewers: Making the paper reproducible is important, regardless of
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014888337468983,"whether the code and data are provided or not.
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8023159636062862,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8031430934656741,"to make their results reproducible or verifiable.
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8039702233250621,"• Depending on the contribution, reproducibility can be accomplished in various ways.
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80479735318445,"For example, if the contribution is a novel architecture, describing the architecture fully
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8056244830438379,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
770"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8064516129032258,"be necessary to either make it possible for others to replicate the model with the same
771"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8072787427626137,"dataset, or provide access to the model. In general. releasing code and data is often
772"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8081058726220016,"one good way to accomplish this, but reproducibility can also be provided via detailed
773"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089330024813896,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
774"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8097601323407775,"of a large language model), releasing of a model checkpoint, or other means that are
775"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8105872622001654,"appropriate to the research performed.
776"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8114143920595533,"• While NeurIPS does not require releasing code, the conference does require all submis-
777"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8122415219189413,"sions to provide some reasonable avenue for reproducibility, which may depend on the
778"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8130686517783292,"nature of the contribution. For example
779"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8138957816377171,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
780"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8147229114971051,"to reproduce that algorithm.
781"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.815550041356493,"(b) If the contribution is primarily a new model architecture, the paper should describe
782"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8163771712158809,"the architecture clearly and fully.
783"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8172043010752689,"(c) If the contribution is a new model (e.g., a large language model), then there should
784"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8180314309346567,"either be a way to access this model for reproducing the results or a way to reproduce
785"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8188585607940446,"the model (e.g., with an open-source dataset or instructions for how to construct
786"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8196856906534326,"the dataset).
787"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8205128205128205,"(d) We recognize that reproducibility may be tricky in some cases, in which case
788"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8213399503722084,"authors are welcome to describe the particular way they provide for reproducibility.
789"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8221670802315963,"In the case of closed-source models, it may be that access to the model is limited in
790"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8229942100909843,"some way (e.g., to registered users), but it should be possible for other researchers
791"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8238213399503722,"to have some path to reproducing or verifying the results.
792"
OPEN ACCESS TO DATA AND CODE,0.8246484698097601,"5. Open access to data and code
793"
OPEN ACCESS TO DATA AND CODE,0.8254755996691481,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
794"
OPEN ACCESS TO DATA AND CODE,0.826302729528536,"tions to faithfully reproduce the main experimental results, as described in supplemental
795"
OPEN ACCESS TO DATA AND CODE,0.8271298593879239,"material?
796"
OPEN ACCESS TO DATA AND CODE,0.8279569892473119,"Answer: [No]
797"
OPEN ACCESS TO DATA AND CODE,0.8287841191066998,"Justification: Unfortunately we did not have time to clean up the code and document it
798"
OPEN ACCESS TO DATA AND CODE,0.8296112489660876,"so that it could be useful at the time of the paper deadline. But we intend to make our
799"
OPEN ACCESS TO DATA AND CODE,0.8304383788254756,"implementations of the proposed algorithms available as a python library as soon as possible
800"
OPEN ACCESS TO DATA AND CODE,0.8312655086848635,"and will also open source the full experimental setup on Github.
801"
OPEN ACCESS TO DATA AND CODE,0.8320926385442514,"Guidelines:
802"
OPEN ACCESS TO DATA AND CODE,0.8329197684036393,"• The answer NA means that paper does not include experiments requiring code.
803"
OPEN ACCESS TO DATA AND CODE,0.8337468982630273,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
804"
OPEN ACCESS TO DATA AND CODE,0.8345740281224152,"public/guides/CodeSubmissionPolicy) for more details.
805"
OPEN ACCESS TO DATA AND CODE,0.8354011579818031,"• While we encourage the release of code and data, we understand that this might not be
806"
OPEN ACCESS TO DATA AND CODE,0.8362282878411911,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
807"
OPEN ACCESS TO DATA AND CODE,0.837055417700579,"including code, unless this is central to the contribution (e.g., for a new open-source
808"
OPEN ACCESS TO DATA AND CODE,0.8378825475599669,"benchmark).
809"
OPEN ACCESS TO DATA AND CODE,0.8387096774193549,"• The instructions should contain the exact command and environment needed to run to
810"
OPEN ACCESS TO DATA AND CODE,0.8395368072787428,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
811"
OPEN ACCESS TO DATA AND CODE,0.8403639371381307,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
812"
OPEN ACCESS TO DATA AND CODE,0.8411910669975186,"• The authors should provide instructions on data access and preparation, including how
813"
OPEN ACCESS TO DATA AND CODE,0.8420181968569065,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
814"
OPEN ACCESS TO DATA AND CODE,0.8428453267162944,"• The authors should provide scripts to reproduce all experimental results for the new
815"
OPEN ACCESS TO DATA AND CODE,0.8436724565756824,"proposed method and baselines. If only a subset of experiments are reproducible, they
816"
OPEN ACCESS TO DATA AND CODE,0.8444995864350703,"should state which ones are omitted from the script and why.
817"
OPEN ACCESS TO DATA AND CODE,0.8453267162944582,"• At submission time, to preserve anonymity, the authors should release anonymized
818"
OPEN ACCESS TO DATA AND CODE,0.8461538461538461,"versions (if applicable).
819"
OPEN ACCESS TO DATA AND CODE,0.8469809760132341,"• Providing as much information as possible in supplemental material (appended to the
820"
OPEN ACCESS TO DATA AND CODE,0.847808105872622,"paper) is recommended, but including URLs to data and code is permitted.
821"
OPEN ACCESS TO DATA AND CODE,0.8486352357320099,"6. Experimental Setting/Details
822"
OPEN ACCESS TO DATA AND CODE,0.8494623655913979,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
823"
OPEN ACCESS TO DATA AND CODE,0.8502894954507858,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
824"
OPEN ACCESS TO DATA AND CODE,0.8511166253101737,"results?
825"
OPEN ACCESS TO DATA AND CODE,0.8519437551695617,"Answer: [Yes]
826"
OPEN ACCESS TO DATA AND CODE,0.8527708850289496,"Justification: The experimental setup is described in as much detail as the space allows in
827"
OPEN ACCESS TO DATA AND CODE,0.8535980148883374,"Section 6. The full setup is described in Appendix F.
828"
OPEN ACCESS TO DATA AND CODE,0.8544251447477254,"Guidelines:
829"
OPEN ACCESS TO DATA AND CODE,0.8552522746071133,"• The answer NA means that the paper does not include experiments.
830"
OPEN ACCESS TO DATA AND CODE,0.8560794044665012,"• The experimental setting should be presented in the core of the paper to a level of detail
831"
OPEN ACCESS TO DATA AND CODE,0.8569065343258891,"that is necessary to appreciate the results and make sense of them.
832"
OPEN ACCESS TO DATA AND CODE,0.8577336641852771,"• The full details can be provided either with the code, in appendix, or as supplemental
833"
OPEN ACCESS TO DATA AND CODE,0.858560794044665,"material.
834"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8593879239040529,"7. Experiment Statistical Significance
835"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8602150537634409,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
836"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8610421836228288,"information about the statistical significance of the experiments?
837"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8618693134822167,"Answer: [Yes]
838"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8626964433416047,"Justification: Sample standard deviations for all experiments are reported in Appendix G.
839"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8635235732009926,"Guidelines:
840"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8643507030603805,"• The answer NA means that the paper does not include experiments.
841"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8651778329197684,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
842"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8660049627791563,"dence intervals, or statistical significance tests, at least for the experiments that support
843"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8668320926385442,"the main claims of the paper.
844"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8676592224979321,"• The factors of variability that the error bars are capturing should be clearly stated (for
845"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8684863523573201,"example, train/test split, initialization, random drawing of some parameter, or overall
846"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.869313482216708,"run with given experimental conditions).
847"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8701406120760959,"• The method for calculating the error bars should be explained (closed form formula,
848"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8709677419354839,"call to a library function, bootstrap, etc.)
849"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8717948717948718,"• The assumptions made should be given (e.g., Normally distributed errors).
850"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8726220016542597,"• It should be clear whether the error bar is the standard deviation or the standard error
851"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8734491315136477,"of the mean.
852"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8742762613730356,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
853"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8751033912324235,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
854"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8759305210918115,"of Normality of errors is not verified.
855"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8767576509511993,"• For asymmetric distributions, the authors should be careful not to show in tables or
856"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8775847808105872,"figures symmetric error bars that would yield results that are out of range (e.g. negative
857"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8784119106699751,"error rates).
858"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8792390405293631,"• If error bars are reported in tables or plots, The authors should explain in the text how
859"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.880066170388751,"they were calculated and reference the corresponding figures or tables in the text.
860"
EXPERIMENTS COMPUTE RESOURCES,0.8808933002481389,"8. Experiments Compute Resources
861"
EXPERIMENTS COMPUTE RESOURCES,0.8817204301075269,"Question: For each experiment, does the paper provide sufficient information on the com-
862"
EXPERIMENTS COMPUTE RESOURCES,0.8825475599669148,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
863"
EXPERIMENTS COMPUTE RESOURCES,0.8833746898263027,"the experiments?
864"
EXPERIMENTS COMPUTE RESOURCES,0.8842018196856907,"Answer: [Yes]
865"
EXPERIMENTS COMPUTE RESOURCES,0.8850289495450786,"Justification: This information is provided in Appendix G.
866"
EXPERIMENTS COMPUTE RESOURCES,0.8858560794044665,"Guidelines:
867"
EXPERIMENTS COMPUTE RESOURCES,0.8866832092638545,"• The answer NA means that the paper does not include experiments.
868"
EXPERIMENTS COMPUTE RESOURCES,0.8875103391232424,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
869"
EXPERIMENTS COMPUTE RESOURCES,0.8883374689826302,"or cloud provider, including relevant memory and storage.
870"
EXPERIMENTS COMPUTE RESOURCES,0.8891645988420182,"• The paper should provide the amount of compute required for each of the individual
871"
EXPERIMENTS COMPUTE RESOURCES,0.8899917287014061,"experimental runs as well as estimate the total compute.
872"
EXPERIMENTS COMPUTE RESOURCES,0.890818858560794,"• The paper should disclose whether the full research project required more compute
873"
EXPERIMENTS COMPUTE RESOURCES,0.891645988420182,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
874"
EXPERIMENTS COMPUTE RESOURCES,0.8924731182795699,"didn’t make it into the paper).
875"
CODE OF ETHICS,0.8933002481389578,"9. Code Of Ethics
876"
CODE OF ETHICS,0.8941273779983457,"Question: Does the research conducted in the paper conform, in every respect, with the
877"
CODE OF ETHICS,0.8949545078577337,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
878"
CODE OF ETHICS,0.8957816377171216,"Answer: [Yes]
879"
CODE OF ETHICS,0.8966087675765095,"Justification: As far as we are aware there are no violations of the Code of Ethics.
880"
CODE OF ETHICS,0.8974358974358975,"Guidelines:
881"
CODE OF ETHICS,0.8982630272952854,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
882"
CODE OF ETHICS,0.8990901571546733,"• If the authors answer No, they should explain the special circumstances that require a
883"
CODE OF ETHICS,0.8999172870140613,"deviation from the Code of Ethics.
884"
CODE OF ETHICS,0.9007444168734491,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
885"
CODE OF ETHICS,0.901571546732837,"eration due to laws or regulations in their jurisdiction).
886"
BROADER IMPACTS,0.902398676592225,"10. Broader Impacts
887"
BROADER IMPACTS,0.9032258064516129,"Question: Does the paper discuss both potential positive societal impacts and negative
888"
BROADER IMPACTS,0.9040529363110008,"societal impacts of the work performed?
889"
BROADER IMPACTS,0.9048800661703887,"Answer: [Yes]
890"
BROADER IMPACTS,0.9057071960297767,"Justification: We discuss the main potential misuse of our work in Section 7.
891"
BROADER IMPACTS,0.9065343258891646,"Guidelines:
892"
BROADER IMPACTS,0.9073614557485525,"• The answer NA means that there is no societal impact of the work performed.
893"
BROADER IMPACTS,0.9081885856079405,"• If the authors answer NA or No, they should explain why their work has no societal
894"
BROADER IMPACTS,0.9090157154673284,"impact or why the paper does not address societal impact.
895"
BROADER IMPACTS,0.9098428453267163,"• Examples of negative societal impacts include potential malicious or unintended uses
896"
BROADER IMPACTS,0.9106699751861043,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
897"
BROADER IMPACTS,0.9114971050454922,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
898"
BROADER IMPACTS,0.91232423490488,"groups), privacy considerations, and security considerations.
899"
BROADER IMPACTS,0.913151364764268,"• The conference expects that many papers will be foundational research and not tied
900"
BROADER IMPACTS,0.9139784946236559,"to particular applications, let alone deployments. However, if there is a direct path to
901"
BROADER IMPACTS,0.9148056244830438,"any negative applications, the authors should point it out. For example, it is legitimate
902"
BROADER IMPACTS,0.9156327543424317,"to point out that an improvement in the quality of generative models could be used to
903"
BROADER IMPACTS,0.9164598842018197,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
904"
BROADER IMPACTS,0.9172870140612076,"that a generic algorithm for optimizing neural networks could enable people to train
905"
BROADER IMPACTS,0.9181141439205955,"models that generate Deepfakes faster.
906"
BROADER IMPACTS,0.9189412737799835,"• The authors should consider possible harms that could arise when the technology is
907"
BROADER IMPACTS,0.9197684036393714,"being used as intended and functioning correctly, harms that could arise when the
908"
BROADER IMPACTS,0.9205955334987593,"technology is being used as intended but gives incorrect results, and harms following
909"
BROADER IMPACTS,0.9214226633581473,"from (intentional or unintentional) misuse of the technology.
910"
BROADER IMPACTS,0.9222497932175352,"• If there are negative societal impacts, the authors could also discuss possible mitigation
911"
BROADER IMPACTS,0.9230769230769231,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
912"
BROADER IMPACTS,0.923904052936311,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
913"
BROADER IMPACTS,0.9247311827956989,"feedback over time, improving the efficiency and accessibility of ML).
914"
SAFEGUARDS,0.9255583126550868,"11. Safeguards
915"
SAFEGUARDS,0.9263854425144747,"Question: Does the paper describe safeguards that have been put in place for responsible
916"
SAFEGUARDS,0.9272125723738627,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
917"
SAFEGUARDS,0.9280397022332506,"image generators, or scraped datasets)?
918"
SAFEGUARDS,0.9288668320926385,"Answer: [NA]
919"
SAFEGUARDS,0.9296939619520265,"Justification: We do not believe our proposed models have a high risk of misuse but will
920"
SAFEGUARDS,0.9305210918114144,"nonetheless highlight the potential risks in the documentation when we release the code.
921"
SAFEGUARDS,0.9313482216708023,"Guidelines:
922"
SAFEGUARDS,0.9321753515301903,"• The answer NA means that the paper poses no such risks.
923"
SAFEGUARDS,0.9330024813895782,"• Released models that have a high risk for misuse or dual-use should be released with
924"
SAFEGUARDS,0.9338296112489661,"necessary safeguards to allow for controlled use of the model, for example by requiring
925"
SAFEGUARDS,0.9346567411083541,"that users adhere to usage guidelines or restrictions to access the model or implementing
926"
SAFEGUARDS,0.9354838709677419,"safety filters.
927"
SAFEGUARDS,0.9363110008271298,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
928"
SAFEGUARDS,0.9371381306865177,"should describe how they avoided releasing unsafe images.
929"
SAFEGUARDS,0.9379652605459057,"• We recognize that providing effective safeguards is challenging, and many papers do
930"
SAFEGUARDS,0.9387923904052936,"not require this, but we encourage authors to take this into account and make a best
931"
SAFEGUARDS,0.9396195202646815,"faith effort.
932"
LICENSES FOR EXISTING ASSETS,0.9404466501240695,"12. Licenses for existing assets
933"
LICENSES FOR EXISTING ASSETS,0.9412737799834574,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
934"
LICENSES FOR EXISTING ASSETS,0.9421009098428453,"the paper, properly credited and are the license and terms of use explicitly mentioned and
935"
LICENSES FOR EXISTING ASSETS,0.9429280397022333,"properly respected?
936"
LICENSES FOR EXISTING ASSETS,0.9437551695616212,"Answer: [Yes]
937"
LICENSES FOR EXISTING ASSETS,0.9445822994210091,"Justification: As far as we are aware we cite all the sources of the data used in our experiments
938"
LICENSES FOR EXISTING ASSETS,0.9454094292803971,"as well the main software packages used.
939"
LICENSES FOR EXISTING ASSETS,0.946236559139785,"Guidelines:
940"
LICENSES FOR EXISTING ASSETS,0.9470636889991728,"• The answer NA means that the paper does not use existing assets.
941"
LICENSES FOR EXISTING ASSETS,0.9478908188585607,"• The authors should cite the original paper that produced the code package or dataset.
942"
LICENSES FOR EXISTING ASSETS,0.9487179487179487,"• The authors should state which version of the asset is used and, if possible, include a
943"
LICENSES FOR EXISTING ASSETS,0.9495450785773366,"URL.
944"
LICENSES FOR EXISTING ASSETS,0.9503722084367245,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
945"
LICENSES FOR EXISTING ASSETS,0.9511993382961125,"• For scraped data from a particular source (e.g., website), the copyright and terms of
946"
LICENSES FOR EXISTING ASSETS,0.9520264681555004,"service of that source should be provided.
947"
LICENSES FOR EXISTING ASSETS,0.9528535980148883,"• If assets are released, the license, copyright information, and terms of use in the
948"
LICENSES FOR EXISTING ASSETS,0.9536807278742763,"package should be provided. For popular datasets, paperswithcode.com/datasets
949"
LICENSES FOR EXISTING ASSETS,0.9545078577336642,"has curated licenses for some datasets. Their licensing guide can help determine the
950"
LICENSES FOR EXISTING ASSETS,0.9553349875930521,"license of a dataset.
951"
LICENSES FOR EXISTING ASSETS,0.9561621174524401,"• For existing datasets that are re-packaged, both the original license and the license of
952"
LICENSES FOR EXISTING ASSETS,0.956989247311828,"the derived asset (if it has changed) should be provided.
953"
LICENSES FOR EXISTING ASSETS,0.9578163771712159,"• If this information is not available online, the authors are encouraged to reach out to
954"
LICENSES FOR EXISTING ASSETS,0.9586435070306039,"the asset’s creators.
955"
NEW ASSETS,0.9594706368899917,"13. New Assets
956"
NEW ASSETS,0.9602977667493796,"Question: Are new assets introduced in the paper well documented and is the documentation
957"
NEW ASSETS,0.9611248966087675,"provided alongside the assets?
958"
NEW ASSETS,0.9619520264681555,"Answer: [NA]
959"
NEW ASSETS,0.9627791563275434,"Justification: We don’t release any new assets at the time of submission. We plan to release
960"
NEW ASSETS,0.9636062861869313,"the code later and will fully document it.
961"
NEW ASSETS,0.9644334160463193,"Guidelines:
962"
NEW ASSETS,0.9652605459057072,"• The answer NA means that the paper does not release new assets.
963"
NEW ASSETS,0.9660876757650951,"• Researchers should communicate the details of the dataset/code/model as part of their
964"
NEW ASSETS,0.9669148056244831,"submissions via structured templates. This includes details about training, license,
965"
NEW ASSETS,0.967741935483871,"limitations, etc.
966"
NEW ASSETS,0.9685690653432589,"• The paper should discuss whether and how consent was obtained from people whose
967"
NEW ASSETS,0.9693961952026469,"asset is used.
968"
NEW ASSETS,0.9702233250620348,"• At submission time, remember to anonymize your assets (if applicable). You can either
969"
NEW ASSETS,0.9710504549214226,"create an anonymized URL or include an anonymized zip file.
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718775847808105,"14. Crowdsourcing and Research with Human Subjects
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9727047146401985,"Question: For crowdsourcing experiments and research with human subjects, does the paper
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9735318444995864,"include the full text of instructions given to participants and screenshots, if applicable, as
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9743589743589743,"well as details about compensation (if any)?
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9751861042183623,"Answer: [NA]
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760132340777502,"Justification: We don’t conduct any experiments involving human subjects.
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768403639371381,"Guidelines:
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776674937965261,"• The answer NA means that the paper does not involve crowdsourcing nor research with
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978494623655914,"human subjects.
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9793217535153019,"• Including this information in the supplemental material is fine, but if the main contribu-
980"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801488833746899,"tion of the paper involves human subjects, then as much detail as possible should be
981"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809760132340778,"included in the main paper.
982"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9818031430934657,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
983"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826302729528535,"or other labor should be paid at least the minimum wage in the country of the data
984"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834574028122415,"collector.
985"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842845326716294,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
986"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851116625310173,"Subjects
987"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9859387923904053,"Question: Does the paper describe potential risks incurred by study participants, whether
988"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867659222497932,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
989"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875930521091811,"approvals (or an equivalent approval/review based on the requirements of your country or
990"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884201819685691,"institution) were obtained?
991"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989247311827957,"Answer: [NA]
992"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900744416873449,"Justification: We don’t conduct any experiments involving human subjects.
993"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9909015715467329,"Guidelines:
994"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917287014061208,"• The answer NA means that the paper does not involve crowdsourcing nor research with
995"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925558312655087,"human subjects.
996"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933829611248967,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
997"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942100909842845,"may be required for any human subjects research. If you obtained IRB approval, you
998"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950372208436724,"should clearly state this in the paper.
999"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958643507030603,"• We recognize that the procedures for this may vary significantly between institutions
1000"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966914805624483,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1001"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975186104218362,"guidelines for their institution.
1002"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983457402812241,"• For initial submissions, do not include any information that would break anonymity (if
1003"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991728701406121,"applicable), such as the institution conducting the review.
1004"
