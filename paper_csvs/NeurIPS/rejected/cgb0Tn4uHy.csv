Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00102880658436214,"The transition matrix methods have garnered sustained attention as a class of
1"
ABSTRACT,0.00205761316872428,"techniques for label-noise learning due to their simplicity and statistical consis-
2"
ABSTRACT,0.0030864197530864196,"tency. However, existing methods primarily focus on class-dependent noise and
3"
ABSTRACT,0.00411522633744856,"lack applicability for instance-dependent noise, while some methods specifically
4"
ABSTRACT,0.0051440329218107,"designed for instance-dependent noise tend to be relatively complex. To address
5"
ABSTRACT,0.006172839506172839,"this issue, we propose an extended model based on transition matrix in this paper,
6"
ABSTRACT,0.00720164609053498,"which preserves simplicity while extending its applicability to handle a broader
7"
ABSTRACT,0.00823045267489712,"range of noisy data beyond class-dependent noise. The proposed algorithm’s con-
8"
ABSTRACT,0.009259259259259259,"vergence and generalization properties are theoretically analyzed under certain
9"
ABSTRACT,0.0102880658436214,"assumptions. Experimental evaluations conducted on various synthetic and real-
10"
ABSTRACT,0.01131687242798354,"world noisy datasets demonstrate significant improvements over existing transition
11"
ABSTRACT,0.012345679012345678,"matrix-based methods. Upon acceptance of our paper, the code will be open
12"
ABSTRACT,0.013374485596707819,"sourced.
13"
INTRODUCTION,0.01440329218106996,"1
Introduction
14"
INTRODUCTION,0.015432098765432098,"Deep neural networks have achieved remarkable success in various fields in recent years, especially
15"
INTRODUCTION,0.01646090534979424,"in classification problems with labeled data [32, 2]. Compared to traditional methods, deep neural
16"
INTRODUCTION,0.01748971193415638,"networks have greatly improved performance but their effects heavily depend on the accuracy of the
17"
INTRODUCTION,0.018518518518518517,"provided labels. Bringing data with corrupted labels into the neural network model without special
18"
INTRODUCTION,0.01954732510288066,"treatment can severely affect the prediction performance [8, 50]. However, acquiring accurately
19"
INTRODUCTION,0.0205761316872428,"annotated data in reality can be very expensive, so a larger amount of data comes from the Internet or
20"
INTRODUCTION,0.021604938271604937,"annotations by non-professional annotators. Therefore, it is currently worth studying and promoting
21"
INTRODUCTION,0.02263374485596708,"how to alleviate the damage caused to the model when using noisy labels and make the model more
22"
INTRODUCTION,0.023662551440329218,"robust, which is known as the problem of label-noise learning or called learning with noisy labels
23"
INTRODUCTION,0.024691358024691357,"[29, 36, 10, 43, 41, 1, 35].
24"
INTRODUCTION,0.0257201646090535,"Various methods have been proposed for label-noise learning. Existing methods can be classified into
25"
INTRODUCTION,0.026748971193415638,"several categories. One of them is to design novel loss functions or network structures [53, 39, 28],
26"
INTRODUCTION,0.027777777777777776,"which reduce the impact of noisy labels to make the model more robust. Another category is sample
27"
INTRODUCTION,0.02880658436213992,"selection based on sample loss or feature extracted, dividing samples into the clean dataset and the
28"
INTRODUCTION,0.029835390946502057,"noisy dataset [4, 10, 13, 19]. Then they relabel the noisy labels [33, 15], or clear the noisy labels
29"
INTRODUCTION,0.030864197530864196,"and use semi-supervised methods for learning [3, 19]. These methods are common recently and
30"
INTRODUCTION,0.03189300411522634,"have achieved some good results. However, the process of sample selection is relatively subjective,
31"
INTRODUCTION,0.03292181069958848,"and statistical consistency is lost after the selection, and most of them lack theoretical support.
32"
INTRODUCTION,0.033950617283950615,"In contrast, transition matrix methods [9, 43, 22, 14, 59] have statistical consistency and usually
33"
INTRODUCTION,0.03497942386831276,"have corresponding theoretical analysis as support, attracting continued attention and occupying an
34"
INTRODUCTION,0.0360082304526749,"important position in various learning algorithms with label noise.
35"
INTRODUCTION,0.037037037037037035,"The core idea of transition matrix methods is to use a matrix measuring the transition probability from
36"
INTRODUCTION,0.03806584362139918,"the distribution of true label to the distribution of observed noisy label. If an accurate transition matrix
37"
INTRODUCTION,0.03909465020576132,"can be estimated and combined with observable data to obtain the noisy class-posterior probability, the
38"
INTRODUCTION,0.040123456790123455,"distribution of clean label can be inferred for network learning. Therefore, estimating the transition
39"
INTRODUCTION,0.0411522633744856,"matrix is the key to this type of method. However, it is infeasible to estimate an individual transition
40"
INTRODUCTION,0.04218106995884774,"matrix for each sample without additional conditions [26]. Previous methods mostly focus on class-
41"
INTRODUCTION,0.043209876543209874,"dependent and instance-independent label noise problems [43, 22, 51], assuming that the transition
42"
INTRODUCTION,0.044238683127572016,"matrix is fixed for all samples. Among these methods, some [31, 43] assume the existence of anchor
43"
INTRODUCTION,0.04526748971193416,"points to estimate the transition matrix, while other methods obtain the optimal estimation by adding
44"
INTRODUCTION,0.046296296296296294,"a regularization term for matrix structure to weaken the anchor points assumption [22, 51]. However,
45"
INTRODUCTION,0.047325102880658436,"these methods are not suitable for instance-dependent label noise and complex real-world data because
46"
INTRODUCTION,0.04835390946502058,"they estimate only one matrix for all samples. Moreover, when the estimation of noisy class-posterior
47"
INTRODUCTION,0.04938271604938271,"distribution is inaccurate, the estimation of the transition matrix may be easily affected [47], thereby
48"
INTRODUCTION,0.050411522633744855,"affecting the estimation of the clean label distribution. Although some methods [42, 58, 52, 20] have
49"
INTRODUCTION,0.051440329218107,"recently been designed to use special networks or structures for instance-dependent noise situations,
50"
INTRODUCTION,0.05246913580246913,"the estimation errors for them are still large, and the computational cost is too high to lose the concise
51"
INTRODUCTION,0.053497942386831275,"characteristic of transition matrix methods.
52"
INTRODUCTION,0.05452674897119342,"Addressing the limitations of current transition matrix-based methods, this paper introduces an
53"
INTRODUCTION,0.05555555555555555,"extended model for transition matrix that extends their applicability from class-dependent noise to
54"
INTRODUCTION,0.056584362139917695,"a broader range of label-noise data without requiring additional techniques such as clustering or
55"
INTRODUCTION,0.05761316872427984,"self-supervised learning. Inspired by methods that handle noise using sparse structures [57, 25], our
56"
INTRODUCTION,0.05864197530864197,"model combines a global transition matrix with a sparse implicit regularization term [31, 25] for
57"
INTRODUCTION,0.059670781893004114,"fitting the distribution of noisy labels across instances, replacing the need for estimating a separate
58"
INTRODUCTION,0.060699588477366256,"transition matrix for each sample. This approach allows us to incorporate instance-level information
59"
INTRODUCTION,0.06172839506172839,"into the model, expanding its capability beyond class-dependent noise scenarios while avoiding the
60"
INTRODUCTION,0.06275720164609054,"unidentifiability and computational complexity of estimating instance-dependent matrices.
61"
INTRODUCTION,0.06378600823045268,"The structure of the following sections is as follows. In Section 2, we give relevant definitions and
62"
INTRODUCTION,0.06481481481481481,"propose our method. In section 3 we conduct a theoretical analysis of the proposed method on a
63"
INTRODUCTION,0.06584362139917696,"simplified model. In Section 4, we conduct experiments on various synthetic and real-world noisy
64"
INTRODUCTION,0.0668724279835391,"datasets, comparing with other transition matrix-based methods. We conclude the paper in Section 5.
65"
INTRODUCTION,0.06790123456790123,"In addition, we provide a more specific review of related works in Appendix A, proofs of theorems in
66"
INTRODUCTION,0.06893004115226338,"Appendix B, and experimental details in Appendix C.
67"
INTRODUCTION,0.06995884773662552,"The main contributions of this paper are:
68"
INTRODUCTION,0.07098765432098765,"• We propose a novel extended model for transition matrix, incorporating sparse implicit regu-
69"
INTRODUCTION,0.0720164609053498,"larization, which enables the extension of transition matrix methods from class-dependent
70"
INTRODUCTION,0.07304526748971193,"noise to a broader range of noisy label data while maintaining simplicity, without the need
71"
INTRODUCTION,0.07407407407407407,"for excessive additional framework design or sophisticated techniques.
72"
INTRODUCTION,0.07510288065843622,"• Under certain assumptions, we provide theoretical analysis on the convergence and gener-
73"
INTRODUCTION,0.07613168724279835,"alization results of the algorithm on a simplified model. We prove the theorems proposed
74"
INTRODUCTION,0.07716049382716049,"accordingly, giving support for the effectiveness of the proposed method.
75"
INTRODUCTION,0.07818930041152264,"• Our proposed method achieves significant improvements compared to previous transition ma-
76"
INTRODUCTION,0.07921810699588477,"trix methods on both synthetic and real-world noisy label datasets, and produces competitive
77"
INTRODUCTION,0.08024691358024691,"results without the need for additional auxiliary techniques.
78"
METHODOLOGY,0.08127572016460906,"2
Methodology
79"
METHODOLOGY,0.0823045267489712,"In this section, we give relevant definitions and propose a novel model that extends the transition
80"
METHODOLOGY,0.08333333333333333,"matrix with implicit regularization (TMR) from class-dependent noise to more label-noise. It is a
81"
METHODOLOGY,0.08436213991769548,"convenient and end-to-end model. We will formulate the method in detail and illustrate it theoretically.
82"
PRELIMINARIES,0.08539094650205761,"2.1
Preliminaries
83"
PRELIMINARIES,0.08641975308641975,"Let X ⊂Rd be the feature space, Y = {1, 2, · · · , C} be the label space, where C is the number
84"
PRELIMINARIES,0.0874485596707819,"of classes. Random variables (X, Y ), (X, ˜Y ) ∈X × Y denote the underlying data distributions
85"
PRELIMINARIES,0.08847736625514403,"with true and noisy labels respectively. In general, we can not observe the latent true data samples
86"
PRELIMINARIES,0.08950617283950617,"D(N) = {(xi, yi)}N
i=1, but can only obtain the corrupted data ˜D(N) = {(xi, ˜yi)}N
i=1, where ˜y ∈Y is
87"
PRELIMINARIES,0.09053497942386832,"the noisy label corrupted from the true label y, while denote corresponding one-hot label as y and ˜y.
88"
PRELIMINARIES,0.09156378600823045,"Transition matrix methods use a matrix T (x) ∈[0, 1]C×C to represent the probability from clean
89"
PRELIMINARIES,0.09259259259259259,"label to noisy label, where the ij-th entry of the transition matrix is the probability that the instance x
90"
PRELIMINARIES,0.09362139917695474,"with the clean label i corrupted to a noisy label j. The matrix satisfies the requirement that the sum
91"
PRELIMINARIES,0.09465020576131687,"of each row PC
j=1 Tij(x) is 1, and usually has the requirement for Tii(x) > Tij(x), ∀j ̸= i. The
92"
PRELIMINARIES,0.09567901234567901,"set of possible values for T is denoted as T =
n
T ∈[0, 1]C×C| PC
j=1 Tij = 1, Tii > Tij, ∀j ̸= i
o
.
93"
PRELIMINARIES,0.09670781893004116,"Let P(Y |X = x) = [P(Y = 1|X = x), · · · , P(Y = C|X = x)]⊤be the clean class-posterior
94"
PRELIMINARIES,0.09773662551440329,"probability and P( ˜Y |X = x) = [P( ˜Y = 1|X = x), · · · , P( ˜Y = C|X = x)]⊤be the noisy
95"
PRELIMINARIES,0.09876543209876543,"class-posterior probability, the formula can be write as:
96"
PRELIMINARIES,0.09979423868312758,"P( ˜Y |X = x) = T (x)⊤P(Y |X = x).
(1)"
PRELIMINARIES,0.10082304526748971,"Though estimating the transition matrix and the noisy class-posterior probability, the clean class-
97"
PRELIMINARIES,0.10185185185185185,"posterior probability can be inferred by P(Y |X = x) = T (x)−⊤P( ˜Y |X = x), where the symbol
98"
PRELIMINARIES,0.102880658436214,"−⊤denotes the transpose of the inverse matrix. Alternatively, the neural network can be utilized to
99"
PRELIMINARIES,0.10390946502057613,"fit the clean label distribution by the loss function:
100 L = 1 N N
X"
PRELIMINARIES,0.10493827160493827,"i=1
ℓ
 
T (xi)⊤fθ(xi), ˜yi

,
(2)"
PRELIMINARIES,0.10596707818930041,"where fθ(·) : X →∆C−1 (∆C−1 ⊂[0, 1]C is the C-dimensional simplex) is a differentiable
101"
PRELIMINARIES,0.10699588477366255,"function represented by a neural network with parameters θ and ℓis a loss function usually using
102"
PRELIMINARIES,0.10802469135802469,"cross-entropy (CE) loss. Therefore, the key to addressing the problem in this class of methods lies in
103"
PRELIMINARIES,0.10905349794238683,"how to estimate the transition matrix.
104"
PRELIMINARIES,0.11008230452674897,"Since it is difficult to estimate the transition matrix T (x) individually for each sample, the majority
105"
PRELIMINARIES,0.1111111111111111,"of existing methods [31, 10, 22] focus on studying the class-dependent and instance-independent
106"
PRELIMINARIES,0.11213991769547325,"transition matrix, i.e., T (x) = T for ∀x. However, these methods are limited by the assumption
107"
PRELIMINARIES,0.11316872427983539,"of class-dependence and cannot be directly applied to instance-dependent label noise with good
108"
PRELIMINARIES,0.11419753086419752,"effectiveness. Our objective is to make improvement and extension based on this limitation.
109"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.11522633744855967,"2.2
Transition Matrix with Implicit Regularization
110"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.11625514403292181,"The main issue with directly applying class-dependent transition matrix methods to instance-
111"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.11728395061728394,"dependent noise lies in using a fixed matrix T , multiplying with clean class-posterior probability
112"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.1183127572016461,"P(Y |X), i.e., T ⊤P(Y |X) is not always equal to the noisy class-posterior probability P( ˜Y |X),
113"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.11934156378600823,"even if the probability values P(Y |X) and P( ˜Y |X) are correctly estimated. Therefore, for a broader
114"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12037037037037036,"range of label-noise scenarios, relying solely on a fixed matrix T is insufficient.
115"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12139917695473251,"The core idea of our proposed model is to introduce a residual term r(X) to fit the distribution
116"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12242798353909465,"difference between P( ˜Y |X) and T ⊤P(Y |X), where r(X) is a C-dimensional vector for each X.
117"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12345679012345678,"It can be transformed into using T ⊤P(Y |X) + r(X) to fit P( ˜Y |X).
118"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12448559670781893,"Intuitively, if an overall relatively suitable transition matrix T is applied to T ⊤P(Y |X), then the
119"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12551440329218108,"difference between it and the probability P( ˜Y |X) should be small. Inspired by methods that handle
120"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12654320987654322,"noise using sparse structures [57, 25], we utilize a sparse structure to model the residual term r.
121"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12757201646090535,"Follow the works [30, 31, 25], using implicit regularization to represent sparse structures is a method
122"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.1286008230452675,"that facilitates updates and provides more stable learning performance. We exploit this technique
123"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.12962962962962962,"to model the residual term as ri = ui ⊙ui −vi ⊙vi with respect to training sample xi, where
124"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.13065843621399176,"ui, vi are all C-dimensional vectors and ⊙denotes an entry-wise Hadamard product. As usual, we
125"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.13168724279835392,"use a deep neural network fθ(·) to learn the true label probability yi w.r.t xi. So for the noisy label
126"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.13271604938271606,"probability distribution ˜yi given by the data, the model use T ⊤fθ (xi) + ui ⊙ui −vi ⊙vi to fit it.
127"
TRANSITION MATRIX WITH IMPLICIT REGULARIZATION,0.1337448559670782,"Bring it into the loss function as:
128"
N,0.13477366255144033,"1
N N
X"
N,0.13580246913580246,"i=1
ℓ
 
T ⊤fθ(xi) + ui ⊙ui −vi ⊙vi, ˜yi

.
(3)"
N,0.1368312757201646,"Due to the potential existence of different T and P(Y |X = x) such that P( ˜Y |X = x) =
129"
N,0.13786008230452676,"T ⊤
1 P1(Y |X = x) = T ⊤
2 P2(Y |X = x), we add a regularization term of the volume of the
130"
N,0.1388888888888889,"matrix Vol(T ) = log det(T ) to loss function as [22] to ensure the transition matrix is identifiable.
131"
N,0.13991769547325103,"The total loss function applied in our proposed method is:
132"
N,0.14094650205761317,"L(θ, T , {ui, vi}N
i=1) = 1 N N
X"
N,0.1419753086419753,"i=1
ℓ
 
T ⊤fθ(xi) + ui ⊙ui −vi ⊙vi, ˜yi

+ λ · log det(T ),
(4)"
N,0.14300411522633744,"where we estimate parameters according to:
133"
N,0.1440329218106996,"ˆθ, ˆT , {ˆui, ˆvi}N
i=1 =
arg min
θ,T ,{ui,vi}N
i=1
L(θ, T , {ui, vi}N
i=1).
(5)"
N,0.14506172839506173,"We use the gradient descent method to update the parameters to be learned above. This method
134"
N,0.14609053497942387,"constitutes our proposed extended Transition Matrix model with sparse implicit Regularization
135"
N,0.147119341563786,"(TMR).The method steps are summarized in Algorithm 1 in Appendix B.1.
136"
N,0.14814814814814814,"Through our model, the estimation of individual transition matrices for each sample is replaced
137"
N,0.14917695473251028,"by the estimation of the global matrix and the sparse residual term. In this way, the number of
138"
N,0.15020576131687244,"parameters for the transition matrix is reduced from O(NC2) to O(NC), which greatly reduces the
139"
N,0.15123456790123457,"difficulty of matrix estimation and computational consumption when C is large. In addition, the
140"
N,0.1522633744855967,"incorporation of sparse implicit regularization in combination with the transition matrix makes the
141"
N,0.15329218106995884,"learning optimization process concise and efficient.
142"
INTEGRATION WITH CONTRASTIVE LEARNING,0.15432098765432098,"2.3
Integration with Contrastive Learning
143"
INTEGRATION WITH CONTRASTIVE LEARNING,0.15534979423868311,"To further improve the effectiveness of our approach, we first utilize contrastive learning as a pre-
144"
INTEGRATION WITH CONTRASTIVE LEARNING,0.15637860082304528,"trained feature extractor, followed by label learning. In this work, we also examine the enhancement
145"
INTEGRATION WITH CONTRASTIVE LEARNING,0.1574074074074074,"of the TMR method by incorporating the SimCLR method from contrastive learning as a feature
146"
INTEGRATION WITH CONTRASTIVE LEARNING,0.15843621399176955,"learner as pre-trained encoder, then resulting in TMR+.
147"
THEORETICAL ANALYSIS,0.15946502057613168,"3
Theoretical Analysis
148"
THEORETICAL ANALYSIS,0.16049382716049382,"In this section, we want to analyze the effectiveness of the proposed method theoretically under
149"
THEORETICAL ANALYSIS,0.16152263374485595,"specific conditions related to label-noise generation. However, it is difficult to give a direct analysis
150"
THEORETICAL ANALYSIS,0.16255144032921812,"of the deep neural network model. So we follow the theoretical analysis method of [25] to simplify
151"
THEORETICAL ANALYSIS,0.16358024691358025,"the proposed model and study on an approximately linear structure to demonstrate the effectiveness
152"
THEORETICAL ANALYSIS,0.1646090534979424,"of our proposed model.
153"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.16563786008230452,"3.1
Model Simplification and Convergence Analysis
154"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.16666666666666666,"The first to solve is the construction of an approximate simplified model for theoretical analysis of
155"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.16769547325102882,"our algorithm. Based on [12], we use first-order Taylor expansion to approximate the deep neural
156"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.16872427983539096,"network fθ(·), which is highly over-parameterized:
157"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1697530864197531,"fθ(x) ≈fθ0(x) +
∂f ⊤
θ (x)
∂θ θ=θ0"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17078189300411523,"⊤
· (θ −θ0),
(6)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17181069958847736,"where fθ(x) is a C-dimensional vector, θ ∈Rp (p ≫N) denotes the parameters of the neural
158"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1728395061728395,"network, ∂f ⊤
θ (x)
∂θ

θ=θ0 is a p × C matrix, θ0 is the initialization of θ, symbol · represents matrix
159"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17386831275720166,"multiplication. For simplicity, we drop the constant term in the derivation and abbreviate ∂f ⊤
θ (x)
∂θ

θ=θ0
160"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1748971193415638,"as ∇θ0f(x). The approximate formula becomes:
161"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17592592592592593,"fθ(x) ≈∇θ0f(x)⊤· θ.
(7)
Through this processing, we simplify the deep neural network into an approximately linear structure,
162"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17695473251028807,"and we use fθ(x) = ∇θ0f(x) · θ in the following theoretical analysis. We use a N × C matrix F to
163"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1779835390946502,"represent the neural network predictions on the overall training dataset {(xi, yi)}N
i=1:
164 F =  "
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.17901234567901234,"f ⊤
θ (x1)
...
f ⊤
θ (xN) "
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1800411522633745,".
(8)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18106995884773663,"In order to be written in matrix form, we rewrite the formula (7) in vector expansion form:
165"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18209876543209877,"f ⊤
θ (x) = [fθ(x)1, · · · , fθ(x)C] = vec(∇θ0f(x))⊤· Θ,
(9)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1831275720164609,"where vec(A) denotes matrix expansion of a m × n matrix A by column vectors:
166"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18415637860082304,"vec(A) = [A1,1, · · · , Am,1, · · · , A1,n, · · · , Am,n]⊤,
(10)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18518518518518517,"and Θ is a CP × C matrix, denoting the Kronecker product of C × C identity matrix IC with θ, i.e.,
167"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18621399176954734,Θ = IC ⊗θ =  
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18724279835390947,"θ
0
· · ·
0
0
θ
· · ·
0
...
...
...
...
0
0
· · ·
θ   CP ×C"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1882716049382716,".
(11)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.18930041152263374,"We use a Jacobian matrix G ∈RN×CP to denote the partial derivatives of the network for each
168"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19032921810699588,"sample:
169 G =  "
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19135802469135801,"vec(∇θ0f(x1))⊤
...
vec(∇θ0f(xN))⊤ "
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19238683127572018,".
(12)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.1934156378600823,"Then, an aggregate form of formula (7) is:
170"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19444444444444445,"F = G · Θ.
(13)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19547325102880658,"Now we give a simplified model assumption that there exists an underlying ground truth parameter
171"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19650205761316872,"θ∗such that corresponding F∗generated by equation (13) fits the true label distribution for sample.
172"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19753086419753085,"Meanwhile, there exist potentially true transition matrix T∗and sparse residual matrix R∗=
173"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19855967078189302,"[r(x1), · · · , r(xN)]⊤made up of the residual terms r(x) for sample defined in Section 2.2. We
174"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.19958847736625515,"assume that the N × C observed noisy label matrix ˜Y = [˜y1, · · · , ˜yN]⊤is generated by:
175"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2006172839506173,"˜Y = F∗· T∗+ R∗.
(14)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20164609053497942,"Expanded form after bringing in G and θ∗is:
176"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20267489711934156,"˜Y = G · (IC ⊗θ∗) · T∗+ R∗.
(15)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2037037037037037,"The problem to be studied is transformed into given G and observed ˜Y generated by formula (15),
177"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20473251028806586,"how to estimate the underlying θ∗, T∗and R∗. At this time, our proposed loss function (4) to be
178"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.205761316872428,"optimized transforms into:
179"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20679012345679013,"L(θ, T , U, V ) = L

G · (IC ⊗θ) · T + U ⊙U −V ⊙V , ˜Y

+ λ · log det(T ),
(16)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20781893004115226,"where L is matrix form from ℓin formula (4), U = [u1, · · · , uN]⊤, V = [v1, · · · , vN]⊤, R =
180"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2088477366255144,"U ⊙U −V ⊙V .
181"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.20987654320987653,"Intuitively, the parameters θ, T , R are unidentifiable without other conditions due to the model (15)
182"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2109053497942387,"is over-parameterized. We need to add some conditional assumptions to ensure the convergence
183"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21193415637860083,"of parameters. The required conditions are summarized in the Appendix B.2, such as the low rank
184"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21296296296296297,"condition of G, sparsity of R∗, special small initialization setting, sufficiently scattered assumption
185"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2139917695473251,"[22] of clean class-posterior probability distribution, etc. Under these conditions, we try to analyze
186"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21502057613168724,"the effectiveness of our algorithm. For the simplicity of proof, we use square loss in formula (16),
187"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21604938271604937,"which can be analogized to cross-entropy loss. The parameter optimization problem (5) becomes:
188"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21707818930041153,"ˆθ, ˆT , ˆU, ˆV = arg min
θ,T ,U,V"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.21810699588477367,"1
2∥G · (IC ⊗θ) · T + U ⊙U −V ⊙V −˜Y ∥2
2 + λ · log det(T ).
(17)"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2191358024691358,"Based on this, the convergence result of parameters estimation is as follows:
189"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.22016460905349794,"Theorem 3.1. (Convergence) Under the conditions in B.2, the estimated parameters ˆθ, ˆT , ˆR for
190"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.22119341563786007,"optimization problem (17) based on Algorithm 1 converge to the ground truth solution θ∗, T∗, R∗.
191"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2222222222222222,"The proof can be seen in Appendix B.3. Theorem 3.1 shows that under a simplified linear model and
192"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.22325102880658437,"some conditions, one can use our proposed algorithm to obtain the consistent estimation of network
193"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.2242798353909465,"parameters θ∗applicable to learning with clean label data. At the same time, we can estimate the
194"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.22530864197530864,"overall transition probability T∗from the correct label to the noisy label that we observed. Theorem
195"
MODEL SIMPLIFICATION AND CONVERGENCE ANALYSIS,0.22633744855967078,"3.1 provides theoretical support for the effectiveness of our proposed method.
196"
GENERALIZATION ANALYSIS,0.2273662551440329,"3.2
Generalization Analysis
197"
GENERALIZATION ANALYSIS,0.22839506172839505,"In addition to convergence, the generalization of the proposed result is also worth exploring. It is
198"
GENERALIZATION ANALYSIS,0.2294238683127572,"finite to the amount of noisy label training data ˜D(N) = {(xi, ˜yi)}N
i=1 we can observe, which is
199"
GENERALIZATION ANALYSIS,0.23045267489711935,"considered to be randomly sampled from the overall infinite noisy data ˜D. We want to explore how
200"
GENERALIZATION ANALYSIS,0.23148148148148148,"well the parameters ˆθ(N), ˆT(N) estimated by the proposed algorithm with finite data ˜D(N) fit when
201"
GENERALIZATION ANALYSIS,0.23251028806584362,"applied to the overall data ˜D.
202"
GENERALIZATION ANALYSIS,0.23353909465020575,"We define a function class about the data as
203"
GENERALIZATION ANALYSIS,0.2345679012345679,"F :=

ℓ(T ⊤fθ(·) + γ(·), ·) : X × Y →R+, ∀θ ∈Rp, T ∈T
	
,
(18)"
GENERALIZATION ANALYSIS,0.23559670781893005,"where γ(·) is the true residual term for each sample. Each element in F is a function about
204"
GENERALIZATION ANALYSIS,0.2366255144032922,"data sample. It is worth mentioning that the term of log det(T ) can be incorporated into the
205"
GENERALIZATION ANALYSIS,0.23765432098765432,"loss function ℓ, without explicitly writing it separately for simplicity. Denote the ϵ-cover of F
206"
GENERALIZATION ANALYSIS,0.23868312757201646,"as NF = N (ϵ, F, ∥· ∥∞), the average losses on ˜D(N) and ˜D are L(θ(N), T(N), R(N); ˜D(N)) and
207"
GENERALIZATION ANALYSIS,0.2397119341563786,"L(θ, T , R; ˜D) respectively. According to Theorem 3.1, for any fixed ϵ > 0, there exists estimated
208"
GENERALIZATION ANALYSIS,0.24074074074074073,"parameters ˆθ(N), ˆT(N), ˆR(N) obtained by our algorithm such that:
209"
GENERALIZATION ANALYSIS,0.2417695473251029,"L(ˆθ(N), ˆT(N), ˆR(N); ˜D(N)) ≤L(θ(N), T(N), R∗
(N); ˜D(N)) + ϵ, ∀θ(N) ∈Rp, T(N) ∈T
(19)"
GENERALIZATION ANALYSIS,0.24279835390946503,"where R∗
(N) is the true residual terms for ˜D(N). If we know the ground truth R∗, we have the
210"
GENERALIZATION ANALYSIS,0.24382716049382716,"following result:
211"
GENERALIZATION ANALYSIS,0.2448559670781893,"Theorem 3.2. Suppose the loss function is bounded by 0 ≤ℓ(·, ·) ≤M. For any δ > 0, then with
212"
GENERALIZATION ANALYSIS,0.24588477366255143,"probability at least 1 −δ we have
213"
GENERALIZATION ANALYSIS,0.24691358024691357,"L(ˆθ(N), ˆT(N), R∗; ˜D) ≤
inf
θ∈Rp,T ∈T L(θ, T , R∗; ˜D) + M r"
GENERALIZATION ANALYSIS,0.24794238683127573,ln(2NF/δ)
N,0.24897119341563786,"2n
+ M r"
N,0.25,ln(2/δ)
N,0.25102880658436216,"2n
+ 3ϵ. (20)"
N,0.25205761316872427,"The proof can be found in Appendix B.4, using Theorem 2 in [48] as a reference. For any fixed ϵ > 0,
214"
N,0.25308641975308643,"as n continues to increase, the terms
q"
N,0.25411522633744854,ln(2NF/δ)
"N
AND
Q",0.2551440329218107,"2n
and
q"
"N
AND
Q",0.25617283950617287,ln(2/δ)
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.257201646090535,"2n
on the right side of the inequality
215"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.25823045267489714,"(20) tend to 0. Since the ϵ can be arbitrarily small, the right side of the inequality (20) can be bounded.
216"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.25925925925925924,"Looking back at the optimization target (17), we can find that the Theorem 3.2 states the estimators
217"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.2602880658436214,"ˆθ(N), ˆT(N) based on finite data ˜D(N) can also be applied relatively effectively to wider data ˜D as
218"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.2613168724279835,"long as they are randomly generated from the same pattern. It shows the generalization result of our
219"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.2623456790123457,"algorithm, indicating that the estimation ˆθ(N), ˆT(N) can be applied to new data and only the residual
220"
"N
ON THE RIGHT SIDE OF THE INEQUALITY",0.26337448559670784,"terms R need to be estimated separately.
221"
EXPERIMENTS,0.26440329218106995,"4
Experiments
222"
EXPERIMENTS,0.2654320987654321,"In this section, we present experimental findings to showcase the effectiveness of our proposed
223"
EXPERIMENTS,0.2664609053497942,"method compared to other methods. We evaluate our approach on both synthetic instance-dependent
224"
EXPERIMENTS,0.2674897119341564,"noisy datasets and real-world noisy datasets. More experimental details can be found in the Appendix
225"
EXPERIMENTS,0.26851851851851855,"C.
226"
DATASETS,0.26954732510288065,"4.1
Datasets
227"
DATASETS,0.2705761316872428,"We conduct experiments on following image classification datasets: CIFAR-10 and CIFAR-100 [16],
228"
DATASETS,0.2716049382716049,"CIFAR-10N and CIFAR-100N [40], Clothing1M [44], Webvision and ILSVRC12 [21]. Among
229"
DATASETS,0.2726337448559671,"them, CIFAR-10 and CIFAR-100 both have 32 × 32 × 3 color images including 50,000 training
230"
DATASETS,0.2736625514403292,"images and 10,000 test images. CIFAR-10 has 10 classes while CIFAR-100 has 100 classes. We
231"
DATASETS,0.27469135802469136,"generate instance-dependent noisy data on CIFAR-10 and CIFAR-100 with noise rates ranging from
232"
DATASETS,0.2757201646090535,"10% to 50%, following the same generation method as in [42]. CIFAR-10N and CIFAR-100N are
233"
DATASETS,0.2767489711934156,"manually annotated by human annotators, existing noisy labels within them. Clothing1M is a real-
234"
DATASETS,0.2777777777777778,"world dataset consisting of 1 million training images, consisting of 14 categories. WebVision contains
235"
DATASETS,0.2788065843621399,"2.4 million images crawled from the websites using the 1,000 concepts in ImageNet ILSVRC12, but
236"
DATASETS,0.27983539094650206,"only the first 50 classes of the Google image subset are used in our experiments. For the validation
237"
DATASETS,0.2808641975308642,"set selection in our TMR method, we randomly sampled 10 samples from each observed class for
238"
DATASETS,0.28189300411522633,"each dataset to form the validation set, while the remaining samples were used for the training set.
239"
EXPERIMENTAL SETUP,0.2829218106995885,"4.2
Experimental Setup
240"
EXPERIMENTAL SETUP,0.2839506172839506,"We conduct the experiments using NVIDIA 3090Ti graphics cards. During the training process, we
241"
EXPERIMENTAL SETUP,0.28497942386831276,"update the transition matrix using the Adam optimization method, the initialization is consistent
242"
EXPERIMENTAL SETUP,0.28600823045267487,"with [22]. While the updates for other parameters are performed using the stochastic gradient
243"
EXPERIMENTAL SETUP,0.28703703703703703,"descent (SGD) optimization method. More specifically, for CIFAR-10/10N, we use ResNet-18 as
244"
EXPERIMENTAL SETUP,0.2880658436213992,"the backbone network with 300 epochs, batch size 128, learning rate for network is 0.05, 0.0005 for
245"
EXPERIMENTAL SETUP,0.2890946502057613,"transition matrix and divided by 10 after the 30th and 60th epoch. For CIFAR-100/100N, we use
246"
EXPERIMENTAL SETUP,0.29012345679012347,"ResNet-34 network with the same 300 epochs, batch size 128, while learning rate for network is 0.05,
247"
EXPERIMENTAL SETUP,0.2911522633744856,"0.0002 for transition matrix and divided by 10 after the 30th and 60th epoch. For clothing1M, we
248"
EXPERIMENTAL SETUP,0.29218106995884774,"use a ResNet-50 pre-trained with 10 epochs, batch size 64, learning rate 0.002 for network, 0.0001
249"
EXPERIMENTAL SETUP,0.2932098765432099,"for transition matrix and divided by 10 after the 5th epoch. We use InceptionResNetV2 network
250"
EXPERIMENTAL SETUP,0.294238683127572,"on Webvision, with 100 epochs, batch size 32, learning rate 0.02 for network, 0.0005 for transition
251"
EXPERIMENTAL SETUP,0.2952674897119342,"matrix and divided by 10 after the 30th and 60th epoch. For ILSVRC12, we directly use the model
252"
EXPERIMENTAL SETUP,0.2962962962962963,"trained on Webvision, following the common setting in other papers in this field.
253"
COMPARISON METHODS,0.29732510288065844,"4.3
Comparison Methods
254"
COMPARISON METHODS,0.29835390946502055,"In our experiments, we included the following commonly used baseline methods for instance-
255"
COMPARISON METHODS,0.2993827160493827,"dependent transition matrix estimation and comparison: (1) GCE [53], (2) Forward [31], (3) DMI
256"
COMPARISON METHODS,0.3004115226337449,"[45], (4) VolMinNet [22], (5) PeerLoss [27] (6) BLTM [46], (7) PartT [42], (8) MEIDTM [6], (9)
257"
COMPARISON METHODS,0.301440329218107,"SOP [25] as an implicit regularization method for comparison, as well as state-of-the-art methods
258"
COMPARISON METHODS,0.30246913580246915,"for comparison purposes: (10) Co-teaching [10], (11) ELR+ [24], (12) DivideMix [19], (13) SOP+
259"
COMPARISON METHODS,0.30349794238683125,"[25], (14) CC [54], (15) PGDF [5], (16) DISC [23].
260"
COMPARISON METHODS,0.3045267489711934,Table 1: Test accuracy with instance-dependent noise on CIFAR-10/100.
COMPARISON METHODS,0.3055555555555556,"CIFAR-10
IDN-10%
IDN-20%
IDN-30%
IDN-40%
IDN-50%
CE
88.86±0.23
86.93±0.17
82.42±0.44
76.68±0.23
58.93±1.54
GCE
90.82±0.05
88.89±0.08
82.90±0.51
74.18±3.10
58.93±2.67
Forward
91.71±0.08
89.62±0.14
86.93±0.15
80.29±0.27
65.91±1.22
DMI
91.43±0.18
89.99±0.15
86.87±0.34
80.74±0.44
63.92±3.92
VolMinNet
89.97±0.57
87.01±0.64
83.80±0.67
79.52±0.83
61.90±1.06
PeerLoss
90.89±0.07
89.21±0.63
85.70±0.56
78.51±1.23
59.08±1.05
BLTM
90.45±0.72
88.14±0.66
84.55±0.48
79.71±0.95
63.33±2.75
PartT
90.32±0.15
89.33±0.70
85.33±1.86
80.59±0.41
64.58±2.86
MEIDTM
92.91±0.07
92.26±0.25
90.73±0.34
85.94±0.92
73.77±0.82
SOP
93.58±0.31
93.07±0.45
92.42±0.43
89.83±0.77
82.52±0.97
TMR
94.45±0.17
93.90±0.21
93.14±0.20
91.82±0.31
87.04±0.42
CIFAR-100
IDN-10%
IDN-20%
IDN-30%
IDN-40%
IDN-50%
CE
66.55±0.23
63.94±0.51
61.97±1.16
58.70±0.56
56.63±0.69
GCE
69.18±0.14
68.35±0.33
66.35±0.13
62.09±0.09
56.68±0.75
Forward
67.81±0.48
67.23±0.29
65.42±0.63
62.18±0.26
58.61±0.44
DMI
67.06±0.46
64.72±0.64
62.80±1.46
60.24±0.63
56.52±1.18
VolMinNet
67.78±0.62
66.13±0.47
61.08±0.90
57.35±0.83
52.60±1.31
PeerLoss
65.64±1.07
63.83±0.48
61.64±0.67
58.30±0.80
55.41±0.28
BLTM
68.42±0.42
66.62±0.85
64.72±0.64
59.38±0.65
55.68±1.43
PartT
67.33±0.33
65.33±0.59
64.56±1.55
59.73±0.76
56.80±1.32
MEIDTM
69.88±0.45
69.16±0.16
66.76±0.30
63.46±0.48
59.18±0.16
SOP
74.09±0.52
73.13±0.46
72.14±0.46
68.98±0.58
64.24±0.86
TMR
76.96±0.25
75.94±0.32
74.87±0.45
72.56±0.60
69.85±0.56"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3065843621399177,"4.4
Experimental Results on Synthetic Datasets
261"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.30761316872427985,"We primarily validated our TMR method against previous instance-based transition matrix methods
262"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.30864197530864196,"on synthetic CIFAR-10/100 noise datasets. These methods mainly focus on estimating the transition
263"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3096707818930041,"matrix and do not leverage advanced self-supervised or semi-supervised techniques. We performed 5
264"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31069958847736623,"independent runs for each experimental configuration, and the average values and standard deviations
265"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3117283950617284,"of each experiment are presented in Table 1.
266"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31275720164609055,"The results demonstrate that our proposed TMR method outperforms other methods of the same
267"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31378600823045266,"category across various noise rates. It is evident that traditional transition matrix methods such as
268"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3148148148148148,"Forward and VolMinNet exhibit subpar performance when handling instance-dependent noise. On
269"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31584362139917693,"the other hand, specialized transition matrix methods designed for instance-dependent noise, such as
270"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3168724279835391,"ParT and MEIDTM, still show significant gaps compared to our method.
271"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31790123456790126,"Furthermore, as the noise rates increase, the test accuracy of existing transition matrix methods
272"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31893004115226337,"significantly decline. This is particularly pronounced in the case of CIFAR-100 with 50% instance-
273"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.31995884773662553,"dependent noise (IDN) data, where all transition matrix methods achieve test accuracy below 60%.
274"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.32098765432098764,"In contrast, our proposed TMR method achieves a remarkable test accuracy of 69.85%, showcasing
275"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3220164609053498,"its exceptional performance. That demonstrates relatively robust performance of TMR with only a
276"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3230452674897119,"slight decrease as the noise rate increases.
277"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.32407407407407407,"It is worth mentioning that SOP [25], as a method that also applies implicit regularization based
278"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.32510288065843623,"on sparsity assumptions, achieves comparable performance to our method when the noise rates are
279"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.32613168724279834,"low. However, it still falls short of our method’s performance. As the noise rate increases, SOP is
280"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3271604938271605,"more adversely affected by the noise due to its reliance on the sparsity assumption. In contrast, our
281"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3281893004115226,"proposed TMR method effectively estimates the overall trend by utilizing the transition matrix and
282"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3292181069958848,"combines it with sparsity, thereby demonstrating robustness even in the presence of higher noise
283"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.33024691358024694,"rates. For instance, on CIFAR-10/100 with a 10% noise rate, TMR outperforms SOP by 0.87 and
284"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.33127572016460904,"2.87 percentage points, respectively. When the noise rate increases to 50%, TMR surpasses SOP by
285"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3323045267489712,"4.52 and 5.61 percentage points, respectively. This clearly demonstrates the general effectiveness of
286"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3333333333333333,"our method in handling label noise learning across various noise rates.
287"
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.3343621399176955,Table 2: Test accuracy on CIFAR-10N and CIFAR-100N.
EXPERIMENTAL RESULTS ON SYNTHETIC DATASETS,0.33539094650205764,"CIFAR-10N
CIFAR-100N
Aggregate
Random 1
Random 2
Random 3
Worst
Noisy
CE
87.77±0.38 85.02±0.65 86.46±1.79 85.16±0.61 77.69±1.55
50.50±0.66
Forward
88.24±0.22 86.88±0.50 86.14±0.21 87.04±0.35 79.49±0.46
57.01±1.03
Co-teaching 91.20±0.13 90.33±0.13 90.30±0.17 90.15±0.18 83.83±0.13
60.37±0.27
ELR+
94.83±0.10 94.43±0.41 94.20±0.24 94.34±0.22 91.09±1.60
66.72±0.07
DivideMix
95.01±0.71 95.16±0.19 94.89±0.23 95.03±0.20 92.56±0.42
71.13±0.48
SOP+
95.61±0.13 95.28±0.13 95.31±0.10 95.39±0.11 93.24±0.21
67.81±0.23
PGDF
95.35±0.12 94.95±0.21 94.78±0.34 94.92±0.28 94.22±0.29
67.76±0.35
TMR+
96.06±0.21 95.96±0.17 95.74±0.31 95.88±0.14 94.91±0.22
70.31±0.28"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.33641975308641975,"4.5
Experimental Results on Real-world Datasets
288"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3374485596707819,"In addition to comparing with transition matrix methods, we also enhanced our method, TMR, by
289"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.338477366255144,"incorporating SimCLR for feature learning, as TMR+. We compared TMR+ with other state-of-the-
290"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3395061728395062,"art methods on multiple real-world noisy datasets, and the results are presented in Table 2 and Table
291"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3405349794238683,"3.
292"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.34156378600823045,"Table 3: Test accuracy on Clothing1M, Webvision and ILSVRC12."
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3425925925925926,"Clothing1M
Webvision
ILSVRC12
CE
69.1
-
-
Forward
69.8
61.1
57.3
Co-teaching
69.2
63.6
61.5
ELR+
74.81
77.78
70.29
DivideMix
74.76
77.32
75.20
SOP+
74.98
77.60
75.29
CC
75.40
79.36
76.08
PGDF
75.19
81.47
75.45
DISC
73.72
80.28
77.44
TMR+
75.42
82.06
77.65"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3436213991769547,"The results demonstrate that regardless of the type of noise labels, whether it is aggregated, random,
293"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3446502057613169,"or the worst-case scenario in CIFAR-10N, as well as in CIFAR-100N with more label categories, our
294"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.345679012345679,"method consistently achieves the best results in handling real-world noise. When dealing with large
295"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.34670781893004116,"datasets like Clothing1M and complex image datasets like Webvision, TMR+ also achieves excellent
296"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3477366255144033,"results compared to to other SOTA methods like CC, PGDF and DISC.
297"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3487654320987654,"Through extensive experiments on five real-world datasets, we demonstrate that our TMR method
298"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3497942386831276,"can significantly benefit from combining with self-supervised methods such as contrastive learning,
299"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3508230452674897,"indicating that high-quality features can greatly enhance our original TMR method. TMR is a
300"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.35185185185185186,"plug-and-play model, where the feature extraction part can be unrelated to TMR itself and be replaced
301"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.35288065843621397,"with other similar methods without requiring additional special handling.
302"
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.35390946502057613,"Table 4: Ablation study of TMR, IR represents implicit regularization and TM represents transition
matrix."
EXPERIMENTAL RESULTS ON REAL-WORLD DATASETS,0.3549382716049383,"CIFAR-10
CIFAR-100
IDN-0.2
IDN-0.4
IDN-0.2
IDN-0.4
w/o IR
90.25
83.31
66.09
62.47
w/o TM
93.36
89.67
72.78
68.59
TMR
93.90
91.82
75.94
72.56"
ABLATION STUDY,0.3559670781893004,"4.6
Ablation Study
303"
ABLATION STUDY,0.35699588477366256,"Besides the aforementioned experiments, we conducted ablation studies on proposed TMR method to
304"
ABLATION STUDY,0.35802469135802467,"assess the importance of each component. Table 4 presents the comparative results under 20% and
305"
ABLATION STUDY,0.35905349794238683,"40% instance-dependent noise rates, where ""w/o"" denotes ""without"", ""TM"" represents the transition
306"
ABLATION STUDY,0.360082304526749,"matrix, and ""IR"" the represents implicit regularization. From the results, it can be observed that the
307"
ABLATION STUDY,0.3611111111111111,"absence of either IR or TM significantly affects the performance of our TMR method. Removing IR
308"
ABLATION STUDY,0.36213991769547327,"has a greater impact, particularly in the case of instance-dependent noise, resulting in a substantial
309"
ABLATION STUDY,0.3631687242798354,"decrease compared to TMR. While removing TM yields similar results on CIFAR-10 with a 20%
310"
ABLATION STUDY,0.36419753086419754,"noise rate, the difference becomes apparent when the noise rate increases to 40% or when applied
311"
ABLATION STUDY,0.36522633744855965,"to more complex datasets like CIFAR-100. These results indicate that both the transition matrix
312"
ABLATION STUDY,0.3662551440329218,"and implicit regularization term are crucial components in our model, highlighting the innovation of
313"
ABLATION STUDY,0.36728395061728397,"combining these two aspects in our method.
314"
CONCLUSION,0.3683127572016461,"5
Conclusion
315"
CONCLUSION,0.36934156378600824,"We propose an extended model for transition matrix that firstly combines it with sparse implicit
316"
CONCLUSION,0.37037037037037035,"regularization, enabling the extension of transition matrix methods from class-dependent noise to a
317"
CONCLUSION,0.3713991769547325,"broader range of noise scenarios while maintaining the simplicity of the model. The effectiveness of
318"
CONCLUSION,0.3724279835390947,"our method is theoretically analyzed under certain assumptions and validated through experiments
319"
CONCLUSION,0.3734567901234568,"on various noisy datasets. Additionally, our method can be enhanced by combining with pre-trained
320"
CONCLUSION,0.37448559670781895,"feature extractor such as contrastive learning, achieving state-of-the-art performance.
321"
REFERENCES,0.37551440329218105,"References
322"
REFERENCES,0.3765432098765432,"[1] Görkem Algan and Ilkay Ulusoy. Image classification with deep learning in the presence of
323"
REFERENCES,0.3775720164609053,"noisy labels: A survey. Knowledge-Based Systems, 215:106771, 2021.
324"
REFERENCES,0.3786008230452675,"[2] Md Zahangir Alom, Tarek M Taha, Chris Yakopcic, Stefan Westberg, Paheding Sidike,
325"
REFERENCES,0.37962962962962965,"Mst Shamima Nasrin, Mahmudul Hasan, Brian C Van Essen, Abdul AS Awwal, and Vi-
326"
REFERENCES,0.38065843621399176,"jayan K Asari. A state-of-the-art survey on deep learning theory and architectures. Electronics,
327"
REFERENCES,0.3816872427983539,"8(3):292, 2019.
328"
REFERENCES,0.38271604938271603,"[3] Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised
329"
REFERENCES,0.3837448559670782,"label noise modeling and loss correction. In International Conference on Machine Learning,
330"
REFERENCES,0.38477366255144035,"pages 312–321. PMLR, 2019.
331"
REFERENCES,0.38580246913580246,"[4] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
332"
REFERENCES,0.3868312757201646,"Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.
333"
REFERENCES,0.38786008230452673,"A closer look at memorization in deep networks. In International Conference on Machine
334"
REFERENCES,0.3888888888888889,"Learning, pages 233–242. PMLR, 2017.
335"
REFERENCES,0.389917695473251,"[5] Wenkai Chen, Chuang Zhu, and Mengting Li. Sample prior guided robust model learning to
336"
REFERENCES,0.39094650205761317,"suppress noisy labels. In Joint European Conference on Machine Learning and Knowledge
337"
REFERENCES,0.39197530864197533,"Discovery in Databases, pages 3–19. Springer, 2023.
338"
REFERENCES,0.39300411522633744,"[6] De Cheng, Tongliang Liu, Yixiong Ning, Nannan Wang, Bo Han, Gang Niu, Xinbo Gao,
339"
REFERENCES,0.3940329218106996,"and Masashi Sugiyama. Instance-dependent label-noise learning with manifold-regularized
340"
REFERENCES,0.3950617283950617,"transition matrix estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision
341"
REFERENCES,0.39609053497942387,"and Pattern Recognition, pages 16630–16639, 2022.
342"
REFERENCES,0.39711934156378603,"[7] De Cheng, Yixiong Ning, Nannan Wang, Xinbo Gao, Heng Yang, Yuxuan Du, Bo Han, and
343"
REFERENCES,0.39814814814814814,"Tongliang Liu. Class-dependent label-noise learning with cycle-consistency regularization.
344"
REFERENCES,0.3991769547325103,"Advances in Neural Information Processing Systems, 35:11104–11116, 2022.
345"
REFERENCES,0.4002057613168724,"[8] Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate
346"
REFERENCES,0.4012345679012346,"description length. Advances in Neural Information Processing Systems, 32, 2019.
347"
REFERENCES,0.4022633744855967,"[9] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta-
348"
REFERENCES,0.40329218106995884,"tion layer. In International Conference on Learning Representations, 2016.
349"
REFERENCES,0.404320987654321,"[10] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
350"
REFERENCES,0.4053497942386831,"Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.
351"
REFERENCES,0.4063786008230453,"Advances in Neural Information Processing Systems, 31, 2018.
352"
REFERENCES,0.4074074074074074,"[11] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The
353"
REFERENCES,0.40843621399176955,"collected works of Wassily Hoeffding, pages 409–426, 1994.
354"
REFERENCES,0.4094650205761317,"[12] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
355"
REFERENCES,0.4104938271604938,"generalization in neural networks. Advances in Neural Information Processing Systems, 31,
356"
REFERENCES,0.411522633744856,"2018.
357"
REFERENCES,0.4125514403292181,"[13] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
358"
REFERENCES,0.41358024691358025,"data-driven curriculum for very deep neural networks on corrupted labels. In International
359"
REFERENCES,0.41460905349794236,"Conference on Machine Learning, pages 2304–2313. PMLR, 2018.
360"
REFERENCES,0.4156378600823045,"[14] Zhimeng Jiang, Kaixiong Zhou, Zirui Liu, Li Li, Rui Chen, Soo-Hyun Choi, and Xia Hu. An
361"
REFERENCES,0.4166666666666667,"information fusion approach to learning with instance-dependent label noise. In International
362"
REFERENCES,0.4176954732510288,"Conference on Learning Representations, 2021.
363"
REFERENCES,0.41872427983539096,"[15] Jan Kremer, Fei Sha, and Christian Igel. Robust active label correction. In International
364"
REFERENCES,0.41975308641975306,"Conference on Artificial Intelligence and Statistics, pages 308–316. PMLR, 2018.
365"
REFERENCES,0.4207818930041152,"[16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
366"
REFERENCES,0.4218106995884774,"2009.
367"
REFERENCES,0.4228395061728395,"[17] Seong Min Kye, Kwanghee Choi, Joonyoung Yi, and Buru Chang. Learning with noisy labels
368"
REFERENCES,0.42386831275720166,"by efficient transition matrix estimation to combat label miscorrection. In European Conference
369"
REFERENCES,0.42489711934156377,"on Computer Vision, pages 717–738. Springer, 2022.
370"
REFERENCES,0.42592592592592593,"[18] Jiangyuan Li, Thanh Nguyen, Chinmay Hegde, and Ka Wai Wong. Implicit sparse regularization:
371"
REFERENCES,0.4269547325102881,"The impact of depth and early stopping. Advances in Neural Information Processing Systems,
372"
REFERENCES,0.4279835390946502,"34:28298–28309, 2021.
373"
REFERENCES,0.42901234567901236,"[19] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as
374"
REFERENCES,0.43004115226337447,"semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.
375"
REFERENCES,0.43106995884773663,"[20] Shikun Li, Xiaobo Xia, Hansong Zhang, Yibing Zhan, Shiming Ge, and Tongliang Liu. Esti-
376"
REFERENCES,0.43209876543209874,"mating noise transition matrix with label correlations for noisy multi-label learning. Advances
377"
REFERENCES,0.4331275720164609,"in Neural Information Processing Systems, 35:24184–24198, 2022.
378"
REFERENCES,0.43415637860082307,"[21] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database:
379"
REFERENCES,0.4351851851851852,"Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.
380"
REFERENCES,0.43621399176954734,"[22] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end
381"
REFERENCES,0.43724279835390945,"label-noise learning without anchor points. In International Conference on Machine Learning,
382"
REFERENCES,0.4382716049382716,"pages 6403–6413. PMLR, 2021.
383"
REFERENCES,0.43930041152263377,"[23] Yifan Li, Hu Han, Shiguang Shan, and Xilin Chen. Disc: Learning from noisy labels via dynamic
384"
REFERENCES,0.4403292181069959,"instance-specific selection and correction. In Proceedings of the IEEE/CVF Conference on
385"
REFERENCES,0.44135802469135804,"Computer Vision and Pattern Recognition, pages 24070–24079, 2023.
386"
REFERENCES,0.44238683127572015,"[24] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-
387"
REFERENCES,0.4434156378600823,"learning regularization prevents memorization of noisy labels. Advances in Neural Information
388"
REFERENCES,0.4444444444444444,"Processing Systems, 33:20331–20342, 2020.
389"
REFERENCES,0.4454732510288066,"[25] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-
390"
REFERENCES,0.44650205761316875,"parameterization. In International Conference on Machine Learning, pages 14153–14172.
391"
REFERENCES,0.44753086419753085,"PMLR, 2022.
392"
REFERENCES,0.448559670781893,"[26] Yang Liu, Hao Cheng, and Kun Zhang. Identifiability of label noise transition matrix. In
393"
REFERENCES,0.4495884773662551,"International Conference on Machine Learning, pages 21475–21496. PMLR, 2023.
394"
REFERENCES,0.4506172839506173,"[27] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing
395"
REFERENCES,0.45164609053497945,"noise rates. In International Conference on Machine Learning, pages 6226–6236. PMLR, 2020.
396"
REFERENCES,0.45267489711934156,"[28] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey.
397"
REFERENCES,0.4537037037037037,"Normalized loss functions for deep learning with noisy labels. In International Conference on
398"
REFERENCES,0.4547325102880658,"Machine Learning, pages 6543–6553. PMLR, 2020.
399"
REFERENCES,0.455761316872428,"[29] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning
400"
REFERENCES,0.4567901234567901,"with noisy labels. Advances in Neural Information Processing Systems, 26, 2013.
401"
REFERENCES,0.45781893004115226,"[30] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:
402"
REFERENCES,0.4588477366255144,"On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
403"
REFERENCES,0.45987654320987653,"[31] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.
404"
REFERENCES,0.4609053497942387,"Making deep neural networks robust to label noise: A loss correction approach. In Proceedings
405"
REFERENCES,0.4619341563786008,"of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1944–1952, 2017.
406"
REFERENCES,0.46296296296296297,"[32] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-
407"
REFERENCES,0.46399176954732513,"Ling Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms,
408"
REFERENCES,0.46502057613168724,"techniques, and applications. ACM Computing Surveys (CSUR), 51(5):1–36, 2018.
409"
REFERENCES,0.4660493827160494,"[33] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples
410"
REFERENCES,0.4670781893004115,"for robust deep learning. In International Conference on Machine Learning, pages 4334–4343.
411"
REFERENCES,0.46810699588477367,"PMLR, 2018.
412"
REFERENCES,0.4691358024691358,"[34] Jun Shu, Qian Zhao, Zongben Xu, and Deyu Meng. Meta transition adaptation for robust deep
413"
REFERENCES,0.47016460905349794,"learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.
414"
REFERENCES,0.4711934156378601,"[35] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from
415"
REFERENCES,0.4722222222222222,"noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and
416"
REFERENCES,0.4732510288065844,"Learning Systems, 2022.
417"
REFERENCES,0.4742798353909465,"[36] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
418"
REFERENCES,0.47530864197530864,"convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
419"
REFERENCES,0.4763374485596708,"[37] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
420"
REFERENCES,0.4773662551440329,"sparse recovery. Advances in Neural Information Processing Systems, 32, 2019.
421"
REFERENCES,0.4783950617283951,"[38] Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In
422"
REFERENCES,0.4794238683127572,"Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages
423"
REFERENCES,0.48045267489711935,"526–536, 2021.
424"
REFERENCES,0.48148148148148145,"[39] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
425"
REFERENCES,0.4825102880658436,"entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International
426"
REFERENCES,0.4835390946502058,"Conference on Computer Vision, pages 322–330, 2019.
427"
REFERENCES,0.4845679012345679,"[40] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning
428"
REFERENCES,0.48559670781893005,"with noisy labels revisited: A study using real-world human annotations. arXiv preprint
429"
REFERENCES,0.48662551440329216,"arXiv:2110.12088, 2021.
430"
REFERENCES,0.4876543209876543,"[41] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang.
431"
REFERENCES,0.4886831275720165,"Robust early-learning: Hindering the memorization of noisy labels. In International Conference
432"
REFERENCES,0.4897119341563786,"on Learning Representations, 2020.
433"
REFERENCES,0.49074074074074076,"[42] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu,
434"
REFERENCES,0.49176954732510286,"Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent
435"
REFERENCES,0.492798353909465,"label noise. Advances in Neural Information Processing Systems, 33:7597–7610, 2020.
436"
REFERENCES,0.49382716049382713,"[43] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi
437"
REFERENCES,0.4948559670781893,"Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in Neural
438"
REFERENCES,0.49588477366255146,"Information Processing Systems, 32, 2019.
439"
REFERENCES,0.49691358024691357,"[44] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive
440"
REFERENCES,0.49794238683127573,"noisy labeled data for image classification. In Proceedings of the IEEE Conference on Computer
441"
REFERENCES,0.49897119341563784,"Vision and Pattern Recognition, pages 2691–2699, 2015.
442"
REFERENCES,0.5,"[45] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: A novel information-theoretic
443"
REFERENCES,0.5010288065843621,"loss function for training deep nets robust to label noise. Advances in Neural Information
444"
REFERENCES,0.5020576131687243,"Processing Systems, 32, 2019.
445"
REFERENCES,0.5030864197530864,"[46] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating
446"
REFERENCES,0.5041152263374485,"instance-dependent bayes-label transition matrix using a deep neural network. In International
447"
REFERENCES,0.5051440329218106,"Conference on Machine Learning, pages 25302–25312. PMLR, 2022.
448"
REFERENCES,0.5061728395061729,"[47] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi
449"
REFERENCES,0.507201646090535,"Sugiyama. Dual t: Reducing estimation error for transition matrix in label-noise learning.
450"
REFERENCES,0.5082304526748971,"Advances in Neural Information Processing Systems, 33:7260–7271, 2020.
451"
REFERENCES,0.5092592592592593,"[48] LIN Yong, Renjie Pi, Weizhong Zhang, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu,
452"
REFERENCES,0.5102880658436214,"and Bo Han. A holistic view of label noise transition matrix in deep learning and beyond. In
453"
REFERENCES,0.5113168724279835,"The Eleventh International Conference on Learning Representations, 2022.
454"
REFERENCES,0.5123456790123457,"[49] Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant
455"
REFERENCES,0.5133744855967078,"learning rates for double over-parameterization. Advances in Neural Information Processing
456"
REFERENCES,0.51440329218107,"Systems, 33:17733–17744, 2020.
457"
REFERENCES,0.5154320987654321,"[50] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
458"
REFERENCES,0.5164609053497943,"deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
459"
REFERENCES,0.5174897119341564,"115, 2021.
460"
REFERENCES,0.5185185185185185,"[51] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only
461"
REFERENCES,0.5195473251028807,"noisy labels via total variation regularization. In International Conference on Machine Learning,
462"
REFERENCES,0.5205761316872428,"pages 12501–12512. PMLR, 2021.
463"
REFERENCES,0.5216049382716049,"[52] Yivan Zhang and Masashi Sugiyama. Approximating instance-dependent noise via instance-
464"
REFERENCES,0.522633744855967,"confidence embedding. arXiv preprint arXiv:2103.13569, 2021.
465"
REFERENCES,0.5236625514403292,"[53] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
466"
REFERENCES,0.5246913580246914,"with noisy labels. Advances in Neural Information Processing Systems, 31, 2018.
467"
REFERENCES,0.5257201646090535,"[54] Ganlong Zhao, Guanbin Li, Yipeng Qin, Feng Liu, and Yizhou Yu. Centrality and consistency:
468"
REFERENCES,0.5267489711934157,"two-stage clean samples identification for learning with instance-dependent noisy labels. In
469"
REFERENCES,0.5277777777777778,"European Conference on Computer Vision, pages 21–37. Springer, 2022.
470"
REFERENCES,0.5288065843621399,"[55] Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over-
471"
REFERENCES,0.529835390946502,"parametrization in high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2(4):8,
472"
REFERENCES,0.5308641975308642,"2019.
473"
REFERENCES,0.5318930041152263,"[56] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit
474"
REFERENCES,0.5329218106995884,"regularization. Biometrika, 109(4):1033–1046, 2022.
475"
REFERENCES,0.5339506172839507,"[57] Xiong Zhou, Xianming Liu, Chenyang Wang, Deming Zhai, Junjun Jiang, and Xiangyang
476"
REFERENCES,0.5349794238683128,"Ji. Learning with noisy labels via sparse regularization. In Proceedings of the IEEE/CVF
477"
REFERENCES,0.5360082304526749,"International Conference on Computer Vision, pages 72–81, 2021.
478"
REFERENCES,0.5370370370370371,"[58] Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points
479"
REFERENCES,0.5380658436213992,"when learning with noisy labels. In International Conference on Machine Learning, pages
480"
REFERENCES,0.5390946502057613,"12912–12923. PMLR, 2021.
481"
REFERENCES,0.5401234567901234,"[59] Zhaowei Zhu, Jialu Wang, and Yang Liu. Beyond images: Label noise transition matrix
482"
REFERENCES,0.5411522633744856,"estimation for tasks with lower-quality features. In International Conference on Machine
483"
REFERENCES,0.5421810699588477,"Learning, pages 27633–27653. PMLR, 2022.
484"
REFERENCES,0.5432098765432098,"A
Related Works
485"
REFERENCES,0.5442386831275721,"A.1
Transition Matrix Methods
486"
REFERENCES,0.5452674897119342,"Most previous transition matrix methods focus on class-dependent label noise to simplify the esti-
487"
REFERENCES,0.5462962962962963,"mation difficulty. Some of the early methods [31, 43, 47] usually assume the existence of anchor
488"
REFERENCES,0.5473251028806584,"points and make the transition matrix identifiable by finding anchor points or approximate anchor
489"
REFERENCES,0.5483539094650206,"points. To mitigate the anchor point assumption, VolMinNet [22] and TVD [51] add different forms
490"
REFERENCES,0.5493827160493827,"of regularization for the transition matrix respectively to make it identifiable. While other methods
491"
REFERENCES,0.5504115226337448,"[7, 17] try setting up unique network structure to estimate the transition matrix. Besides, [34, 48]
492"
REFERENCES,0.551440329218107,"utilize structures like meta-learning to estimate the transition matrix, but may require more clean data
493"
REFERENCES,0.5524691358024691,"and computational consumption. Although the above methods are designed to handle class-dependent
494"
REFERENCES,0.5534979423868313,"label noise, it is not suitable when encountering instance-dependent noise or real-world noisy data.
495"
REFERENCES,0.5545267489711934,"However, it is not feasible to estimate a transition matrix individually for each sample without other
496"
REFERENCES,0.5555555555555556,"assumptions or multiple noisy labels [26]. In order to achieve an approximate estimation of the
497"
REFERENCES,0.5565843621399177,"instance-dependent transition matrix, [9] uses an adaptation layer to estimate the transition matrix
498"
REFERENCES,0.5576131687242798,"based on each sample’s output, but the error is large due to the influence of the initial value. While
499"
REFERENCES,0.558641975308642,"[46] uses a separate network to estimate the transition matrix based on the Bayesian label. Some
500"
REFERENCES,0.5596707818930041,"methods [42, 38, 58, 59] learn a part-dependent or group-dependent matrix through clustering, which
501"
REFERENCES,0.5606995884773662,"is a compromise estimation method lies between instance-dependent and class-dependent methods.
502"
REFERENCES,0.5617283950617284,"Other methods [6, 14] utilize similarity in feature space to assist transition matrix learning. Although
503"
REFERENCES,0.5627572016460906,"these instance-dependent transition matrix methods achieve identifiability through special treatments,
504"
REFERENCES,0.5637860082304527,"they are usually relatively complex and have larger errors, which is contrary to the convenient and
505"
REFERENCES,0.5648148148148148,"simple characteristics of transition matrix methods.
506"
REFERENCES,0.565843621399177,"A.2
Implicit Regularization
507"
REFERENCES,0.5668724279835391,"Implicit regularization can be regarded as a statistical method for sparsity, playing the role of
508"
REFERENCES,0.5679012345679012,"minimizing L1 loss in sparse noise learning and being currently used in various models [55, 37, 49,
509"
REFERENCES,0.5689300411522634,"18, 56]. Among these methods, SOP [25] is the one worthy of special attention, which is related
510"
REFERENCES,0.5699588477366255,"to our method. SOP also uses implicit regularization for noisy label learning, which gives a sparse
511"
REFERENCES,0.5709876543209876,"representation of the residual term between prediction and observed noisy label. However, it does not
512"
REFERENCES,0.5720164609053497,"take advantage of the overall transfer probability of noise and the noise sparsity assumption does not
513"
REFERENCES,0.573045267489712,"apply to high noise rates situation, so its performance on large noise rates data is relatively weak. We
514"
REFERENCES,0.5740740740740741,"will compare it with our proposed method by experimental results specifically in Section 4.
515"
REFERENCES,0.5751028806584362,"B
Algorithm and proofs
516"
REFERENCES,0.5761316872427984,"B.1
Algorithm
517"
REFERENCES,0.5771604938271605,"The steps of our TMR algorithm are shown in detail in Algorithm 1.
518"
REFERENCES,0.5781893004115226,"B.2
Conditions
519"
REFERENCES,0.5792181069958847,"Condition 1. For optimization problem (17), initialize parameters in the algorithm 1 with ui = t1,
520"
REFERENCES,0.5802469135802469,"v = t1, where 1 are vectors of all 1, t is a small value scalar. There exists a given α0 > 0 such that
521"
REFERENCES,0.581275720164609,"the learning rates of gradient descent satisfy lr(u) = lr(v) = αlr(θ), α < α0.
522"
REFERENCES,0.5823045267489712,"Condition 2. Denote the rank of G in formula (15) as r, the number of sparse nonzero entries of R∗
523"
REFERENCES,0.5833333333333334,"is k, P is the matrix of row vectors in SVD decomposition of G. Define s = N"
REFERENCES,0.5843621399176955,"r max1≤i≤N∥P ⊤ei∥2
2.
524"
REFERENCES,0.5853909465020576,"Then k, r, s satisfy 4k2rs < N.
525"
REFERENCES,0.5864197530864198,"Condition 3. The row vectors of matrix F in formula (14) are sufficiently scattered, which is a
526"
REFERENCES,0.5874485596707819,"weakened requirement of the anchor points assumption can be found in Definition 2 of [22].
527"
REFERENCES,0.588477366255144,"B.3
Proof of Theorem 3.1
528"
REFERENCES,0.5895061728395061,"Proof. Denote Q = (IC ⊗θ) · T , the optimization problem in (17) can be written as:
529 min 1"
REFERENCES,0.5905349794238683,"2∥G · Q + U ⊙U −V ⊙V −˜Y ∥2
2 + λ · log det(T ).
(21)"
REFERENCES,0.5915637860082305,Algorithm 1 Extended Transition Matrix Model with Sparse Implicit Regularization (TMR)
REFERENCES,0.5925925925925926,"Input: Training data {(xi, yi)}N
i=1, network fθ(·), coefficient λ, learning rate τθ, τu, τv, τT , batch
size m, epoch number E, transition matrix update frequency k.
Initialization: Transition matrix T with an identity matrix, draw entries of {ui, vi}N
i=1 from i.i.d.
Gaussian distribution with zero-mean and s.t.d. 1e-8.
for t = 1 to E do"
REFERENCES,0.5936213991769548,for b = 1 to N/m do
REFERENCES,0.5946502057613169,"Get a sample batch B ⊆{1, . . . , N} with |B| = m
Calculate loss L by 4 with batch B
for i in B do"
REFERENCES,0.595679012345679,"Update ui ←ui −τu · ∂L/∂ui
Update vi ←vi −τv · ∂L/∂vi
end for
Update θ ←θ −τθ · ∂L/∂θ
if b/k is 0 then"
REFERENCES,0.5967078189300411,"Update T ←T −τT · ∂L/∂T
end if
end for
end for
Output: Network parameters ˆθ, variables {ˆui, ˆvi}N
i=1 and transition matrix ˆT ."
REFERENCES,0.5977366255144033,"Since implicit regularization can minimize the L1 loss and according to Proposition 3.3 in [25],
530"
REFERENCES,0.5987654320987654,"the first half of formula (21) will converge to a global solution for any fixed T under Condition 1.
531"
REFERENCES,0.5997942386831275,"Furthermore, it can be converted into the following optimization problem:
532"
REFERENCES,0.6008230452674898,"min
Q,R
1
2∥Q∥2
2 + β∥R∥1,
s.t.
˜Y = G · Q + R,
(22)"
REFERENCES,0.6018518518518519,where β = −log t
REFERENCES,0.602880658436214,"2α as defined in 1. When Condition 2 is true, the solution to problem (22) are Q∗and
533"
REFERENCES,0.6039094650205762,"R∗, where ˜Y is produced by G · Q∗+ R∗. This conclusion can be deduced from the analogy of
534"
REFERENCES,0.6049382716049383,"Proposition 3.5 in [25]. Combining formula (15), we can get:
535"
REFERENCES,0.6059670781893004,"Q∗= (IC ⊗θ∗) · T∗.
(23)"
REFERENCES,0.6069958847736625,"Therefore, problem (21) transform into an optimization problem with parameter θ, T :
536"
REFERENCES,0.6080246913580247,"min
θ,T log det(T ),
s.t.
(IC ⊗θ) · T = Q∗.
(24)"
REFERENCES,0.6090534979423868,"The above optimization problem has the same form as the optimization problem in [22], similar with
537"
REFERENCES,0.6100823045267489,"Theorem 1 in this paper, under Condition 3, the solution to problem (24) is:
538"
REFERENCES,0.6111111111111112,"ˆθ = θ∗,
ˆT = T∗.
(25)"
REFERENCES,0.6121399176954733,"To sum up, when all conditions in Appendix B.2 are met, we can get the ground truth solution θ∗, the
539"
REFERENCES,0.6131687242798354,"estimators by our algorithm converge to T∗, R∗as mentioned in Theorem 3.1.
540"
REFERENCES,0.6141975308641975,"B.4
Proof of Theorem 3.2
541"
REFERENCES,0.6152263374485597,"Proof. We use the inequality we use Hoeffding inequality [11] to help us complete the proof. Since
542"
REFERENCES,0.6162551440329218,"ˆθ(N), ˆT(N) are not independent of the samples, we use ϵ-cover as mentioned in Section 3.2 to deal
543"
REFERENCES,0.6172839506172839,"with the problem. In addition, the parameter R is omitted in the following proof for convenience and
544"
REFERENCES,0.6183127572016461,"does not affect the understanding of the results.
545"
REFERENCES,0.6193415637860082,"According to the definition of ϵ covering, We can find a pair of parameters θk, Tk in the covering set
546"
REFERENCES,0.6203703703703703,"such that:
547"
REFERENCES,0.6213991769547325,"|ℓ(θk, Tk; X, Y ) −ℓ(ˆθ(N), ˆT(N); X, Y )| ≤ϵ, ∀(X, Y ) ∈X × Y.
(26)"
REFERENCES,0.6224279835390947,"Average the loss over samples, we have:
548"
REFERENCES,0.6234567901234568,"L(ˆθ(N), ˆT(N); ˜D) ≤L(θk, Tk; ˜D) + ϵ.
(27)"
REFERENCES,0.6244855967078189,"To meet the requirement of probability 1 −δ in Theorem 3.2, we take the probability value as δ/2NF
549"
REFERENCES,0.6255144032921811,"in Hoeffding inequality due to the randomness of k. Thus, with probability at least 1 −δ/2NF,
550"
REFERENCES,0.6265432098765432,"L(θk, Tk; ˜D) ≤L(θk, Tk; ˜D(N)) + M r"
REFERENCES,0.6275720164609053,ln(2NF/δ)
N,0.6286008230452675,"2n
.
(28)"
N,0.6296296296296297,"By the definition of formula (26),
551"
N,0.6306584362139918,"L(θk, Tk; ˜D(N)) ≤L(ˆθ(N), ˆT(N); ˜D(N)) + ϵ.
(29)"
N,0.6316872427983539,"According to the property of ˆθ(N), ˆT(N) in formula (19), for any θ ∈Rp, T ∈T,
552"
N,0.6327160493827161,"L(ˆθ(N), ˆT(N); ˜D(N)) ≤L(θ, T ; ˜D(N)) + ϵ.
(30)"
N,0.6337448559670782,"Using the Hoeffding inequality again with probability δ/2, with probability at least 1 −δ/2 we have:
553"
N,0.6347736625514403,"L(θ, T ; ˜D(N)) ≤L(θ, T ; ˜D) + M r"
N,0.6358024691358025,ln(2/δ)
N,0.6368312757201646,"2n
.
(31)"
N,0.6378600823045267,"Combining inequalities (27), (28), (29), (30), (31) and adding the probability values, we get the
554"
N,0.6388888888888888,"conclusion that with probability at least 1 −δ,
555"
N,0.6399176954732511,"L(ˆθ(N), ˆT(N); ˜D) ≤L(θ, T , ; ˜D) + M r"
N,0.6409465020576132,ln(2NF/δ)
N,0.6419753086419753,"2n
+ M r"
N,0.6430041152263375,ln(2/δ)
N,0.6440329218106996,"2n
+ 3ϵ, ∀θ ∈Rp, T ∈T. (32) 556"
N,0.6450617283950617,"C
Experiment details
557"
N,0.6460905349794238,"C.1
Experimental Setup
558"
N,0.647119341563786,"We conduct experiments on a single NVIDIA 3090Ti graphics card. For software, we use Python 3.11
559"
N,0.6481481481481481,"and PyTorch 1.10 to build the models. Throughout the training process, transition matrix updates are
560"
N,0.6491769547325102,"carried out using the Adam optimization method, while updates for other parameters are performed
561"
N,0.6502057613168725,"using the stochastic gradient descent (SGD) optimization method. The experimental setup involves a
562"
N,0.6512345679012346,"few training hyper-parameters, including the backbone network used, batch size, learning rate for
563"
N,0.6522633744855967,"parameters, and weight of the regularization term. For specific experimental configurations, please
564"
N,0.6532921810699589,"refer to Table 5 in Appendix C.2.
565"
N,0.654320987654321,"C.2
Hyper-parameters Setting
566"
N,0.6553497942386831,"The backbone network and hyper-parameters of the experiments on each dataset are listed in the table
567"
N,0.6563786008230452,"5.
568"
N,0.6574074074074074,"Table 5: Hyper-parameters on CIFAR-10/100, Clothing-1M and Webvision."
N,0.6584362139917695,"CIFAR-10
CIFAR-100
Clothing1M
Webvision
Network
ResNet18
ResNet34
ResNet-50
InceptionResNetV2
Batch size
128
128
64
32
Training samples
50,000
50,000
1,000,000
65,944
Epochs
300
300
10
100
Learning rate(lr) for network
0.05
0.05
0.002
0.02
lr decay for network
Cosine
Cosine
5th
50th
Weight decay for network
5e-4
5e-4
1e-3
5e-4
lr for T
0.0005
0.0002
0.0001
0.0005
lr decay for T
30th, 60th
30th, 60th
5th
50th
Initialization for T
-2
-4.5
-2.5
-4
lr for u, v
10, 10
1, 100
0.1, 1
0.1, 1
lr decay for u, v
Cosine
Cosine
5th
50th
Coefficient λ
0.001
0.001
0.001
0.001"
N,0.6594650205761317,Table 6: Test accuracy with symmetric and flip noise on CIFAR-10/100.
N,0.6604938271604939,"CIFAR-10
Symmetric
Flip
20%
50%
20%
45%
CE
85.68±0.18
77.35±0.21
86.32±0.16
75.22±0.43
GCE
87.83±0.54
79.54±0.23
89.75±1.53
75.75±0.36
Forward
85.20±0.80
74.82±0.78
88.21±0.48
77.44±6.89
DMI
87.54±0.20
82.68±0.21
89.89±0.45
73.15±7.31
VolMinNet
89.58±0.26
83.37±0.25
90.37±0.30
88.54±0.21
PeerLoss
87.97±0.33
81.06±0.47
89.11±0.42
76.89±1.83
BLTM
88.30±0.38
82.04±0.29
90.77±0.45
80.53±1.51
PartT
89.97±0.36
83.72±0.56
90.81±0.43
86.15±0.87
MEIDTM
90.89±0.20
84.61±0.39
91.01±0.19
88.45±1.07
SOP
93.18±0.57
88.98±0.43
94.02±0.30
89.58±0.86
TMR
94.36±0.22
91.63±0.30
94.55±0.19
93.17±0.53
CIFAR-100
Symmetric
Flip
20%
50%
20%
45%
CE
51.43±0.58
41.31±0.67
53.19±0.42
40.56±0.89
GCE
63.22±0.45
53.16±0.72
64.15±0.44
40.58±0.49
Forward
54.90±0.74
41.85±0.71
56.12±0.54
36.88±2.32
DMI
62.65±0.39
52.42±0.64
59.56±0.73
38.17±2.02
VolMinNet
64.94±0.40
53.89±1.26
68.45±0.69
58.90±0.89
PeerLoss
62.92±0.48
50.25±0.52
64.14±0.39
43.53±0.75
BLTM
63.46±0.58
52.43±0.47
67.10±0.22
48.68±0.77
PartT
65.76±0.28
54.88±0.93
69.40±0.39
56.12±0.61
MEIDTM
66.90±0.32
57.24±1.01
70.16±0.52
58.53±0.50
SOP
74.42±0.42
66.46±0.65
73.93±0.55
63.32±0.87
TMR
76.20±0.24
71.53±0.41
76.53±0.22
70.96±0.52"
N,0.661522633744856,"C.3
Supplementary experiments on class-dependent noise
569"
N,0.6625514403292181,"In addition to conducting experiments on instance-dependent noisy data, we further evaluated the
570"
N,0.6635802469135802,"general effectiveness of our method compared to other approaches by introducing class-dependent
571"
N,0.6646090534979424,"scenarios on CIFAR-10/100 datasets. Table 6 presents the comparative results on CIFAR-10/100
572"
N,0.6656378600823045,"datasets with symmetric noise rates of 20% and 50%, as well as flip noise rates of 20% and 45%. It can
573"
N,0.6666666666666666,"be observed that for class-dependent noise, which serves as a simplified case of instance-dependent
574"
N,0.6676954732510288,"noise, our proposed method TMR outperforms other comparative methods, including transition
575"
N,0.668724279835391,"matrix methods specifically designed for class-dependent noise, such as VolMinNet. Specifically, the
576"
N,0.6697530864197531,"transition matrix methods specifically designed for handling instance-dependent noise, such as BLTM,
577"
N,0.6707818930041153,"PartT and MEIDTM, do not show significant improvements when applied to class-dependent noise
578"
N,0.6718106995884774,"scenarios compared to the transition matrix methods designed only for class-dependent noise, such as
579"
N,0.6728395061728395,"VolMinNet. However, our proposed method, TMR, achieves significant improvements even when
580"
N,0.6738683127572016,"applied to class-dependent noise scenarios compared to VolMinNet. This indicates that our method
581"
N,0.6748971193415638,"has universal applicability and yields favorable results in both class-dependent and instance-dependent
582"
N,0.6759259259259259,"noise scenarios.
583"
N,0.676954732510288,"NeurIPS Paper Checklist
584"
CLAIMS,0.6779835390946503,"1. Claims
585"
CLAIMS,0.6790123456790124,"Question: Do the main claims made in the abstract and introduction accurately reflect the
586"
CLAIMS,0.6800411522633745,"paper’s contributions and scope?
587"
CLAIMS,0.6810699588477366,"Answer: [Yes]
588"
CLAIMS,0.6820987654320988,"Justification: The main content and contributions of the work are reflected in the abstract
589"
CLAIMS,0.6831275720164609,"and introduction.
590"
CLAIMS,0.684156378600823,"Guidelines:
591"
CLAIMS,0.6851851851851852,"• The answer NA means that the abstract and introduction do not include the claims
592"
CLAIMS,0.6862139917695473,"made in the paper.
593"
CLAIMS,0.6872427983539094,"• The abstract and/or introduction should clearly state the claims made, including the
594"
CLAIMS,0.6882716049382716,"contributions made in the paper and important assumptions and limitations. A No or
595"
CLAIMS,0.6893004115226338,"NA answer to this question will not be perceived well by the reviewers.
596"
CLAIMS,0.6903292181069959,"• The claims made should match theoretical and experimental results, and reflect how
597"
CLAIMS,0.691358024691358,"much the results can be expected to generalize to other settings.
598"
CLAIMS,0.6923868312757202,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
599"
CLAIMS,0.6934156378600823,"are not attained by the paper.
600"
LIMITATIONS,0.6944444444444444,"2. Limitations
601"
LIMITATIONS,0.6954732510288066,"Question: Does the paper discuss the limitations of the work performed by the authors?
602"
LIMITATIONS,0.6965020576131687,"Answer: [Yes]
603"
LIMITATIONS,0.6975308641975309,"Justification: In the theoretical analysis section and experimental section, we analyze the
604"
LIMITATIONS,0.698559670781893,"applicability and limitations of our method.
605"
LIMITATIONS,0.6995884773662552,"Guidelines:
606"
LIMITATIONS,0.7006172839506173,"• The answer NA means that the paper has no limitation while the answer No means that
607"
LIMITATIONS,0.7016460905349794,"the paper has limitations, but those are not discussed in the paper.
608"
LIMITATIONS,0.7026748971193416,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
609"
LIMITATIONS,0.7037037037037037,"• The paper should point out any strong assumptions and how robust the results are to
610"
LIMITATIONS,0.7047325102880658,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
611"
LIMITATIONS,0.7057613168724279,"model well-specification, asymptotic approximations only holding locally). The authors
612"
LIMITATIONS,0.7067901234567902,"should reflect on how these assumptions might be violated in practice and what the
613"
LIMITATIONS,0.7078189300411523,"implications would be.
614"
LIMITATIONS,0.7088477366255144,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
615"
LIMITATIONS,0.7098765432098766,"only tested on a few datasets or with a few runs. In general, empirical results often
616"
LIMITATIONS,0.7109053497942387,"depend on implicit assumptions, which should be articulated.
617"
LIMITATIONS,0.7119341563786008,"• The authors should reflect on the factors that influence the performance of the approach.
618"
LIMITATIONS,0.7129629629629629,"For example, a facial recognition algorithm may perform poorly when image resolution
619"
LIMITATIONS,0.7139917695473251,"is low or images are taken in low lighting. Or a speech-to-text system might not be
620"
LIMITATIONS,0.7150205761316872,"used reliably to provide closed captions for online lectures because it fails to handle
621"
LIMITATIONS,0.7160493827160493,"technical jargon.
622"
LIMITATIONS,0.7170781893004116,"• The authors should discuss the computational efficiency of the proposed algorithms
623"
LIMITATIONS,0.7181069958847737,"and how they scale with dataset size.
624"
LIMITATIONS,0.7191358024691358,"• If applicable, the authors should discuss possible limitations of their approach to
625"
LIMITATIONS,0.720164609053498,"address problems of privacy and fairness.
626"
LIMITATIONS,0.7211934156378601,"• While the authors might fear that complete honesty about limitations might be used by
627"
LIMITATIONS,0.7222222222222222,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
628"
LIMITATIONS,0.7232510288065843,"limitations that aren’t acknowledged in the paper. The authors should use their best
629"
LIMITATIONS,0.7242798353909465,"judgment and recognize that individual actions in favor of transparency play an impor-
630"
LIMITATIONS,0.7253086419753086,"tant role in developing norms that preserve the integrity of the community. Reviewers
631"
LIMITATIONS,0.7263374485596708,"will be specifically instructed to not penalize honesty concerning limitations.
632"
THEORY ASSUMPTIONS AND PROOFS,0.727366255144033,"3. Theory Assumptions and Proofs
633"
THEORY ASSUMPTIONS AND PROOFS,0.7283950617283951,"Question: For each theoretical result, does the paper provide the full set of assumptions and
634"
THEORY ASSUMPTIONS AND PROOFS,0.7294238683127572,"a complete (and correct) proof?
635"
THEORY ASSUMPTIONS AND PROOFS,0.7304526748971193,"Answer: [Yes]
636"
THEORY ASSUMPTIONS AND PROOFS,0.7314814814814815,"Justification: We conduct theoretical analysis of our method and provide proofs for the
637"
THEORY ASSUMPTIONS AND PROOFS,0.7325102880658436,"theorems in the paper.
638"
THEORY ASSUMPTIONS AND PROOFS,0.7335390946502057,"Guidelines:
639"
THEORY ASSUMPTIONS AND PROOFS,0.7345679012345679,"• The answer NA means that the paper does not include theoretical results.
640"
THEORY ASSUMPTIONS AND PROOFS,0.73559670781893,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
641"
THEORY ASSUMPTIONS AND PROOFS,0.7366255144032922,"referenced.
642"
THEORY ASSUMPTIONS AND PROOFS,0.7376543209876543,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
643"
THEORY ASSUMPTIONS AND PROOFS,0.7386831275720165,"• The proofs can either appear in the main paper or the supplemental material, but if
644"
THEORY ASSUMPTIONS AND PROOFS,0.7397119341563786,"they appear in the supplemental material, the authors are encouraged to provide a short
645"
THEORY ASSUMPTIONS AND PROOFS,0.7407407407407407,"proof sketch to provide intuition.
646"
THEORY ASSUMPTIONS AND PROOFS,0.7417695473251029,"• Inversely, any informal proof provided in the core of the paper should be complemented
647"
THEORY ASSUMPTIONS AND PROOFS,0.742798353909465,"by formal proofs provided in appendix or supplemental material.
648"
THEORY ASSUMPTIONS AND PROOFS,0.7438271604938271,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7448559670781894,"4. Experimental Result Reproducibility
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7458847736625515,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7469135802469136,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7479423868312757,"of the paper (regardless of whether the code and data are provided or not)?
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489711934156379,"Answer: [Yes]
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"Justification: We provide a detailed description of the experimental setup in the experimental
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510288065843621,"section, and specific settings for hyperparameters are provided in the appendix.
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520576131687243,"Guidelines:
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7530864197530864,"• The answer NA means that the paper does not include experiments.
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7541152263374485,"• If the paper includes experiments, a No answer to this question will not be perceived
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7551440329218106,"well by the reviewers: Making the paper reproducible is important, regardless of
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7561728395061729,"whether the code and data are provided or not.
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757201646090535,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7582304526748971,"to make their results reproducible or verifiable.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7592592592592593,"• Depending on the contribution, reproducibility can be accomplished in various ways.
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7602880658436214,"For example, if the contribution is a novel architecture, describing the architecture fully
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613168724279835,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623456790123457,"be necessary to either make it possible for others to replicate the model with the same
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633744855967078,"dataset, or provide access to the model. In general. releasing code and data is often
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.76440329218107,"one good way to accomplish this, but reproducibility can also be provided via detailed
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654320987654321,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664609053497943,"of a large language model), releasing of a model checkpoint, or other means that are
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7674897119341564,"appropriate to the research performed.
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7685185185185185,"• While NeurIPS does not require releasing code, the conference does require all submis-
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7695473251028807,"sions to provide some reasonable avenue for reproducibility, which may depend on the
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7705761316872428,"nature of the contribution. For example
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7716049382716049,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.772633744855967,"to reproduce that algorithm.
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7736625514403292,"(b) If the contribution is primarily a new model architecture, the paper should describe
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7746913580246914,"the architecture clearly and fully.
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757201646090535,"(c) If the contribution is a new model (e.g., a large language model), then there should
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7767489711934157,"either be a way to access this model for reproducing the results or a way to reproduce
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7777777777777778,"the model (e.g., with an open-source dataset or instructions for how to construct
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7788065843621399,"the dataset).
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.779835390946502,"(d) We recognize that reproducibility may be tricky in some cases, in which case
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7808641975308642,"authors are welcome to describe the particular way they provide for reproducibility.
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7818930041152263,"In the case of closed-source models, it may be that access to the model is limited in
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829218106995884,"some way (e.g., to registered users), but it should be possible for other researchers
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7839506172839507,"to have some path to reproducing or verifying the results.
688"
OPEN ACCESS TO DATA AND CODE,0.7849794238683128,"5. Open access to data and code
689"
OPEN ACCESS TO DATA AND CODE,0.7860082304526749,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
690"
OPEN ACCESS TO DATA AND CODE,0.7870370370370371,"tions to faithfully reproduce the main experimental results, as described in supplemental
691"
OPEN ACCESS TO DATA AND CODE,0.7880658436213992,"material?
692"
OPEN ACCESS TO DATA AND CODE,0.7890946502057613,"Answer: [Yes]
693"
OPEN ACCESS TO DATA AND CODE,0.7901234567901234,"Justification: We provide partial code in the supplementary materials, and the complete code
694"
OPEN ACCESS TO DATA AND CODE,0.7911522633744856,"will be open-sourced upon acceptance of the paper.
695"
OPEN ACCESS TO DATA AND CODE,0.7921810699588477,"Guidelines:
696"
OPEN ACCESS TO DATA AND CODE,0.7932098765432098,"• The answer NA means that paper does not include experiments requiring code.
697"
OPEN ACCESS TO DATA AND CODE,0.7942386831275721,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
698"
OPEN ACCESS TO DATA AND CODE,0.7952674897119342,"public/guides/CodeSubmissionPolicy) for more details.
699"
OPEN ACCESS TO DATA AND CODE,0.7962962962962963,"• While we encourage the release of code and data, we understand that this might not be
700"
OPEN ACCESS TO DATA AND CODE,0.7973251028806584,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
701"
OPEN ACCESS TO DATA AND CODE,0.7983539094650206,"including code, unless this is central to the contribution (e.g., for a new open-source
702"
OPEN ACCESS TO DATA AND CODE,0.7993827160493827,"benchmark).
703"
OPEN ACCESS TO DATA AND CODE,0.8004115226337448,"• The instructions should contain the exact command and environment needed to run to
704"
OPEN ACCESS TO DATA AND CODE,0.801440329218107,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
705"
OPEN ACCESS TO DATA AND CODE,0.8024691358024691,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
706"
OPEN ACCESS TO DATA AND CODE,0.8034979423868313,"• The authors should provide instructions on data access and preparation, including how
707"
OPEN ACCESS TO DATA AND CODE,0.8045267489711934,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
708"
OPEN ACCESS TO DATA AND CODE,0.8055555555555556,"• The authors should provide scripts to reproduce all experimental results for the new
709"
OPEN ACCESS TO DATA AND CODE,0.8065843621399177,"proposed method and baselines. If only a subset of experiments are reproducible, they
710"
OPEN ACCESS TO DATA AND CODE,0.8076131687242798,"should state which ones are omitted from the script and why.
711"
OPEN ACCESS TO DATA AND CODE,0.808641975308642,"• At submission time, to preserve anonymity, the authors should release anonymized
712"
OPEN ACCESS TO DATA AND CODE,0.8096707818930041,"versions (if applicable).
713"
OPEN ACCESS TO DATA AND CODE,0.8106995884773662,"• Providing as much information as possible in supplemental material (appended to the
714"
OPEN ACCESS TO DATA AND CODE,0.8117283950617284,"paper) is recommended, but including URLs to data and code is permitted.
715"
OPEN ACCESS TO DATA AND CODE,0.8127572016460906,"6. Experimental Setting/Details
716"
OPEN ACCESS TO DATA AND CODE,0.8137860082304527,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
717"
OPEN ACCESS TO DATA AND CODE,0.8148148148148148,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
718"
OPEN ACCESS TO DATA AND CODE,0.815843621399177,"results?
719"
OPEN ACCESS TO DATA AND CODE,0.8168724279835391,"Answer: [Yes]
720"
OPEN ACCESS TO DATA AND CODE,0.8179012345679012,"Justification: We provided a detailed description of the experimental setup in the experimen-
721"
OPEN ACCESS TO DATA AND CODE,0.8189300411522634,"tal section, and specific settings for hyperparameters are provided in the appendix.
722"
OPEN ACCESS TO DATA AND CODE,0.8199588477366255,"Guidelines:
723"
OPEN ACCESS TO DATA AND CODE,0.8209876543209876,"• The answer NA means that the paper does not include experiments.
724"
OPEN ACCESS TO DATA AND CODE,0.8220164609053497,"• The experimental setting should be presented in the core of the paper to a level of detail
725"
OPEN ACCESS TO DATA AND CODE,0.823045267489712,"that is necessary to appreciate the results and make sense of them.
726"
OPEN ACCESS TO DATA AND CODE,0.8240740740740741,"• The full details can be provided either with the code, in appendix, or as supplemental
727"
OPEN ACCESS TO DATA AND CODE,0.8251028806584362,"material.
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8261316872427984,"7. Experiment Statistical Significance
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8271604938271605,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8281893004115226,"information about the statistical significance of the experiments?
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8292181069958847,"Answer: [Yes]
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8302469135802469,"Justification: We conducted multiple repeated experiments to validate our approach and
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.831275720164609,"performed ablation experiments.
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8323045267489712,"Guidelines:
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8333333333333334,"• The answer NA means that the paper does not include experiments.
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8343621399176955,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8353909465020576,"dence intervals, or statistical significance tests, at least for the experiments that support
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8364197530864198,"the main claims of the paper.
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374485596707819,"• The factors of variability that the error bars are capturing should be clearly stated (for
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.838477366255144,"example, train/test split, initialization, random drawing of some parameter, or overall
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8395061728395061,"run with given experimental conditions).
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8405349794238683,"• The method for calculating the error bars should be explained (closed form formula,
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8415637860082305,"call to a library function, bootstrap, etc.)
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425925925925926,"• The assumptions made should be given (e.g., Normally distributed errors).
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8436213991769548,"• It should be clear whether the error bar is the standard deviation or the standard error
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8446502057613169,"of the mean.
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845679012345679,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8467078189300411,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8477366255144033,"of Normality of errors is not verified.
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8487654320987654,"• For asymmetric distributions, the authors should be careful not to show in tables or
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8497942386831275,"figures symmetric error bars that would yield results that are out of range (e.g. negative
752"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8508230452674898,"error rates).
753"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8518518518518519,"• If error bars are reported in tables or plots, The authors should explain in the text how
754"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.852880658436214,"they were calculated and reference the corresponding figures or tables in the text.
755"
EXPERIMENTS COMPUTE RESOURCES,0.8539094650205762,"8. Experiments Compute Resources
756"
EXPERIMENTS COMPUTE RESOURCES,0.8549382716049383,"Question: For each experiment, does the paper provide sufficient information on the com-
757"
EXPERIMENTS COMPUTE RESOURCES,0.8559670781893004,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
758"
EXPERIMENTS COMPUTE RESOURCES,0.8569958847736625,"the experiments?
759"
EXPERIMENTS COMPUTE RESOURCES,0.8580246913580247,"Answer: [Yes]
760"
EXPERIMENTS COMPUTE RESOURCES,0.8590534979423868,"Justification: We list the relevant details in the experimental section.
761"
EXPERIMENTS COMPUTE RESOURCES,0.8600823045267489,"Guidelines:
762"
EXPERIMENTS COMPUTE RESOURCES,0.8611111111111112,"• The answer NA means that the paper does not include experiments.
763"
EXPERIMENTS COMPUTE RESOURCES,0.8621399176954733,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
764"
EXPERIMENTS COMPUTE RESOURCES,0.8631687242798354,"or cloud provider, including relevant memory and storage.
765"
EXPERIMENTS COMPUTE RESOURCES,0.8641975308641975,"• The paper should provide the amount of compute required for each of the individual
766"
EXPERIMENTS COMPUTE RESOURCES,0.8652263374485597,"experimental runs as well as estimate the total compute.
767"
EXPERIMENTS COMPUTE RESOURCES,0.8662551440329218,"• The paper should disclose whether the full research project required more compute
768"
EXPERIMENTS COMPUTE RESOURCES,0.8672839506172839,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
769"
EXPERIMENTS COMPUTE RESOURCES,0.8683127572016461,"didn’t make it into the paper).
770"
CODE OF ETHICS,0.8693415637860082,"9. Code Of Ethics
771"
CODE OF ETHICS,0.8703703703703703,"Question: Does the research conducted in the paper conform, in every respect, with the
772"
CODE OF ETHICS,0.8713991769547325,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
773"
CODE OF ETHICS,0.8724279835390947,"Answer: [Yes]
774"
CODE OF ETHICS,0.8734567901234568,"Justification: We submitted the paper following the NeurIPS Code of Ethics.
775"
CODE OF ETHICS,0.8744855967078189,"Guidelines:
776"
CODE OF ETHICS,0.8755144032921811,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
777"
CODE OF ETHICS,0.8765432098765432,"• If the authors answer No, they should explain the special circumstances that require a
778"
CODE OF ETHICS,0.8775720164609053,"deviation from the Code of Ethics.
779"
CODE OF ETHICS,0.8786008230452675,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
780"
CODE OF ETHICS,0.8796296296296297,"eration due to laws or regulations in their jurisdiction).
781"
BROADER IMPACTS,0.8806584362139918,"10. Broader Impacts
782"
BROADER IMPACTS,0.8816872427983539,"Question: Does the paper discuss both potential positive societal impacts and negative
783"
BROADER IMPACTS,0.8827160493827161,"societal impacts of the work performed?
784"
BROADER IMPACTS,0.8837448559670782,"Answer: [Yes]
785"
BROADER IMPACTS,0.8847736625514403,"Justification: We discuss the positive implications of our work and ensure it does not have
786"
BROADER IMPACTS,0.8858024691358025,"any negative societal impact.
787"
BROADER IMPACTS,0.8868312757201646,"Guidelines:
788"
BROADER IMPACTS,0.8878600823045267,"• The answer NA means that there is no societal impact of the work performed.
789"
BROADER IMPACTS,0.8888888888888888,"• If the authors answer NA or No, they should explain why their work has no societal
790"
BROADER IMPACTS,0.8899176954732511,"impact or why the paper does not address societal impact.
791"
BROADER IMPACTS,0.8909465020576132,"• Examples of negative societal impacts include potential malicious or unintended uses
792"
BROADER IMPACTS,0.8919753086419753,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
793"
BROADER IMPACTS,0.8930041152263375,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
794"
BROADER IMPACTS,0.8940329218106996,"groups), privacy considerations, and security considerations.
795"
BROADER IMPACTS,0.8950617283950617,"• The conference expects that many papers will be foundational research and not tied
796"
BROADER IMPACTS,0.8960905349794238,"to particular applications, let alone deployments. However, if there is a direct path to
797"
BROADER IMPACTS,0.897119341563786,"any negative applications, the authors should point it out. For example, it is legitimate
798"
BROADER IMPACTS,0.8981481481481481,"to point out that an improvement in the quality of generative models could be used to
799"
BROADER IMPACTS,0.8991769547325102,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
800"
BROADER IMPACTS,0.9002057613168725,"that a generic algorithm for optimizing neural networks could enable people to train
801"
BROADER IMPACTS,0.9012345679012346,"models that generate Deepfakes faster.
802"
BROADER IMPACTS,0.9022633744855967,"• The authors should consider possible harms that could arise when the technology is
803"
BROADER IMPACTS,0.9032921810699589,"being used as intended and functioning correctly, harms that could arise when the
804"
BROADER IMPACTS,0.904320987654321,"technology is being used as intended but gives incorrect results, and harms following
805"
BROADER IMPACTS,0.9053497942386831,"from (intentional or unintentional) misuse of the technology.
806"
BROADER IMPACTS,0.9063786008230452,"• If there are negative societal impacts, the authors could also discuss possible mitigation
807"
BROADER IMPACTS,0.9074074074074074,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
808"
BROADER IMPACTS,0.9084362139917695,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
809"
BROADER IMPACTS,0.9094650205761317,"feedback over time, improving the efficiency and accessibility of ML).
810"
SAFEGUARDS,0.9104938271604939,"11. Safeguards
811"
SAFEGUARDS,0.911522633744856,"Question: Does the paper describe safeguards that have been put in place for responsible
812"
SAFEGUARDS,0.9125514403292181,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
813"
SAFEGUARDS,0.9135802469135802,"image generators, or scraped datasets)?
814"
SAFEGUARDS,0.9146090534979424,"Answer: [NA]
815"
SAFEGUARDS,0.9156378600823045,"Justification: There are no concerns in this regard regarding this work.
816"
SAFEGUARDS,0.9166666666666666,"Guidelines:
817"
SAFEGUARDS,0.9176954732510288,"• The answer NA means that the paper poses no such risks.
818"
SAFEGUARDS,0.918724279835391,"• Released models that have a high risk for misuse or dual-use should be released with
819"
SAFEGUARDS,0.9197530864197531,"necessary safeguards to allow for controlled use of the model, for example by requiring
820"
SAFEGUARDS,0.9207818930041153,"that users adhere to usage guidelines or restrictions to access the model or implementing
821"
SAFEGUARDS,0.9218106995884774,"safety filters.
822"
SAFEGUARDS,0.9228395061728395,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
823"
SAFEGUARDS,0.9238683127572016,"should describe how they avoided releasing unsafe images.
824"
SAFEGUARDS,0.9248971193415638,"• We recognize that providing effective safeguards is challenging, and many papers do
825"
SAFEGUARDS,0.9259259259259259,"not require this, but we encourage authors to take this into account and make a best
826"
SAFEGUARDS,0.926954732510288,"faith effort.
827"
LICENSES FOR EXISTING ASSETS,0.9279835390946503,"12. Licenses for existing assets
828"
LICENSES FOR EXISTING ASSETS,0.9290123456790124,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
829"
LICENSES FOR EXISTING ASSETS,0.9300411522633745,"the paper, properly credited and are the license and terms of use explicitly mentioned and
830"
LICENSES FOR EXISTING ASSETS,0.9310699588477366,"properly respected?
831"
LICENSES FOR EXISTING ASSETS,0.9320987654320988,"Answer: [Yes]
832"
LICENSES FOR EXISTING ASSETS,0.9331275720164609,"Justification: The data and code used in our work are all publicly available and open-source.
833"
LICENSES FOR EXISTING ASSETS,0.934156378600823,"Guidelines:
834"
LICENSES FOR EXISTING ASSETS,0.9351851851851852,"• The answer NA means that the paper does not use existing assets.
835"
LICENSES FOR EXISTING ASSETS,0.9362139917695473,"• The authors should cite the original paper that produced the code package or dataset.
836"
LICENSES FOR EXISTING ASSETS,0.9372427983539094,"• The authors should state which version of the asset is used and, if possible, include a
837"
LICENSES FOR EXISTING ASSETS,0.9382716049382716,"URL.
838"
LICENSES FOR EXISTING ASSETS,0.9393004115226338,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
839"
LICENSES FOR EXISTING ASSETS,0.9403292181069959,"• For scraped data from a particular source (e.g., website), the copyright and terms of
840"
LICENSES FOR EXISTING ASSETS,0.941358024691358,"service of that source should be provided.
841"
LICENSES FOR EXISTING ASSETS,0.9423868312757202,"• If assets are released, the license, copyright information, and terms of use in the
842"
LICENSES FOR EXISTING ASSETS,0.9434156378600823,"package should be provided. For popular datasets, paperswithcode.com/datasets
843"
LICENSES FOR EXISTING ASSETS,0.9444444444444444,"has curated licenses for some datasets. Their licensing guide can help determine the
844"
LICENSES FOR EXISTING ASSETS,0.9454732510288066,"license of a dataset.
845"
LICENSES FOR EXISTING ASSETS,0.9465020576131687,"• For existing datasets that are re-packaged, both the original license and the license of
846"
LICENSES FOR EXISTING ASSETS,0.9475308641975309,"the derived asset (if it has changed) should be provided.
847"
LICENSES FOR EXISTING ASSETS,0.948559670781893,"• If this information is not available online, the authors are encouraged to reach out to
848"
LICENSES FOR EXISTING ASSETS,0.9495884773662552,"the asset’s creators.
849"
NEW ASSETS,0.9506172839506173,"13. New Assets
850"
NEW ASSETS,0.9516460905349794,"Question: Are new assets introduced in the paper well documented and is the documentation
851"
NEW ASSETS,0.9526748971193416,"provided alongside the assets?
852"
NEW ASSETS,0.9537037037037037,"Answer: [NA]
853"
NEW ASSETS,0.9547325102880658,"Justification: The paper currently does not include any new assets.
854"
NEW ASSETS,0.9557613168724279,"Guidelines:
855"
NEW ASSETS,0.9567901234567902,"• The answer NA means that the paper does not release new assets.
856"
NEW ASSETS,0.9578189300411523,"• Researchers should communicate the details of the dataset/code/model as part of their
857"
NEW ASSETS,0.9588477366255144,"submissions via structured templates. This includes details about training, license,
858"
NEW ASSETS,0.9598765432098766,"limitations, etc.
859"
NEW ASSETS,0.9609053497942387,"• The paper should discuss whether and how consent was obtained from people whose
860"
NEW ASSETS,0.9619341563786008,"asset is used.
861"
NEW ASSETS,0.9629629629629629,"• At submission time, remember to anonymize your assets (if applicable). You can either
862"
NEW ASSETS,0.9639917695473251,"create an anonymized URL or include an anonymized zip file.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9650205761316872,"14. Crowdsourcing and Research with Human Subjects
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660493827160493,"Question: For crowdsourcing experiments and research with human subjects, does the paper
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670781893004116,"include the full text of instructions given to participants and screenshots, if applicable, as
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681069958847737,"well as details about compensation (if any)?
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691358024691358,"Answer: [NA]
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970164609053498,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711934156378601,"Guidelines:
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722222222222222,"• The answer NA means that the paper does not involve crowdsourcing nor research with
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732510288065843,"human subjects.
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742798353909465,"• Including this information in the supplemental material is fine, but if the main contribu-
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753086419753086,"tion of the paper involves human subjects, then as much detail as possible should be
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763374485596708,"included in the main paper.
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.977366255144033,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783950617283951,"or other labor should be paid at least the minimum wage in the country of the data
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794238683127572,"collector.
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804526748971193,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814814814814815,"Subjects
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825102880658436,"Question: Does the paper describe potential risks incurred by study participants, whether
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835390946502057,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845679012345679,"approvals (or an equivalent approval/review based on the requirements of your country or
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98559670781893,"institution) were obtained?
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866255144032922,"Answer: [NA]
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876543209876543,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886831275720165,"Guidelines:
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897119341563786,"• The answer NA means that the paper does not involve crowdsourcing nor research with
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907407407407407,"human subjects.
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917695473251029,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992798353909465,"may be required for any human subjects research. If you obtained IRB approval, you
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938271604938271,"should clearly state this in the paper.
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948559670781894,"• We recognize that the procedures for this may vary significantly between institutions
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958847736625515,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969135802469136,"guidelines for their institution.
895"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979423868312757,"• For initial submissions, do not include any information that would break anonymity (if
896"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989711934156379,"applicable), such as the institution conducting the review.
897"
