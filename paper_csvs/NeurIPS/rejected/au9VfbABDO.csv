Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019723865877712033,"Imitation learning addresses the challenge of learning by observing an expert’s
1"
ABSTRACT,0.0039447731755424065,"demonstrations without access to reward signals from environments. Most existing
2"
ABSTRACT,0.005917159763313609,"imitation learning methods that do not require interacting with environments either
3"
ABSTRACT,0.007889546351084813,"model the expert distribution as the conditional probability p(a|s) (e.g., behavioral
4"
ABSTRACT,0.009861932938856016,"cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). De-
5"
ABSTRACT,0.011834319526627219,"spite its simplicity, modeling the conditional probability with BC usually struggles
6"
ABSTRACT,0.013806706114398421,"with generalization. While modeling the joint probability can lead to improved
7"
ABSTRACT,0.015779092702169626,"generalization performance, the inference procedure can be time-consuming and it
8"
ABSTRACT,0.01775147928994083,"often suffers from manifold overfitting. This work proposes an imitation learning
9"
ABSTRACT,0.01972386587771203,"framework that benefits from modeling both the conditional and joint probability
10"
ABSTRACT,0.021696252465483234,"of the expert distribution. Our proposed diffusion model-augmented behavioral
11"
ABSTRACT,0.023668639053254437,"cloning (DBC) employs a diffusion model trained to model expert behaviors and
12"
ABSTRACT,0.02564102564102564,"learns a policy to optimize both the BC loss (conditional) and our proposed diffu-
13"
ABSTRACT,0.027613412228796843,"sion model loss (joint). DBC outperforms baselines in various continuous control
14"
ABSTRACT,0.029585798816568046,"tasks in navigation, robot arm manipulation, dexterous manipulation, and locomo-
15"
ABSTRACT,0.03155818540433925,"tion. We design additional experiments to verify the limitations of modeling either
16"
ABSTRACT,0.03353057199211045,"the conditional probability or the joint probability of the expert distribution as well
17"
ABSTRACT,0.03550295857988166,"as compare different generative models.
18"
INTRODUCTION,0.03747534516765286,"1
Introduction
19"
INTRODUCTION,0.03944773175542406,"Recently, the success of deep reinforcement learning (DRL) [Mnih et al., 2015, Lillicrap et al., 2016,
20"
INTRODUCTION,0.04142011834319527,"Arulkumaran et al., 2017] has inspired the research community to develop DRL frameworks to
21"
INTRODUCTION,0.04339250493096647,"control robots, aiming to automate the process of designing sensing, planning, and control algorithms
22"
INTRODUCTION,0.045364891518737675,"by letting the robot learn in an end-to-end fashion. Yet, acquiring complex skills through trial and
23"
INTRODUCTION,0.047337278106508875,"error can still lead to undesired behaviors even with sophisticated reward design [Christiano et al.,
24"
INTRODUCTION,0.04930966469428008,"2017, Leike et al., 2018, Lee et al., 2019]. Moreover, the exploring process could damage expensive
25"
INTRODUCTION,0.05128205128205128,"robotic platforms or even be dangerous to humans [Garcıa and Fernández, 2015, Levine et al., 2020].
26"
INTRODUCTION,0.05325443786982249,"To overcome this issue, imitation learning (i.e., learning from demonstration) [Schaal, 1997, Osa et al.,
27"
INTRODUCTION,0.055226824457593686,"2018] has received growing attention, whose aim is to learn a policy from expert demonstrations,
28"
INTRODUCTION,0.05719921104536489,"which are often more accessible than appropriate reward functions for reinforcement learning. Among
29"
INTRODUCTION,0.05917159763313609,"various imitation learning directions, adversarial imitation learning [Ho and Ermon, 2016, Zolna
30"
INTRODUCTION,0.0611439842209073,"et al., 2021, Kostrikov et al., 2019] and inverse reinforcement learning [Ng and Russell, 2000, Abbeel
31"
INTRODUCTION,0.0631163708086785,"and Ng, 2004] have achieved encouraging results in a variety of domains. Yet, these methods require
32"
INTRODUCTION,0.0650887573964497,"interacting with environments, which can still be expensive or unsafe.
33"
INTRODUCTION,0.0670611439842209,"On the other hand, behavioral cloning (BC) [Pomerleau, 1989, Bain and Sammut, 1995] does not
34"
INTRODUCTION,0.06903353057199212,"require interacting with environments. BC formulates imitation learning as a supervised learning
35"
INTRODUCTION,0.07100591715976332,"problem — given an expert demonstration dataset, an agent policy takes states sampled from the
36"
INTRODUCTION,0.07297830374753451,"dataset as input and learns to replicate the corresponding expert actions. One can view a BC policy as
37"
INTRODUCTION,0.07495069033530571,"a discriminative model p(a|s) that models the conditional probability of an action a given a state s.
38"
INTRODUCTION,0.07692307692307693,"Due to its simplicity and training stability, BC has been widely adopted for various applications.
39"
INTRODUCTION,0.07889546351084813,"However, BC struggles at generalizing to states unobserved during training [Nguyen et al., 2023].
40"
INTRODUCTION,0.08086785009861933,"To address this issue, implicit behavioral cloning (IBC) [Florence et al., 2022] aims to model the
41"
INTRODUCTION,0.08284023668639054,"joint probability of the expert state-action pairs p(s, a) with energy-based models. IBC demonstrates
42"
INTRODUCTION,0.08481262327416174,"superior performance when generalization is required. Yet, imitation learning methods in a similar
43"
INTRODUCTION,0.08678500986193294,"vein [Ganapathi et al., 2022] that model the joint probability of state-action pairs p(s, a) instead
44"
INTRODUCTION,0.08875739644970414,"of directly predicting actions p(a|s) require time-consuming actions sampling and optimization to
45"
INTRODUCTION,0.09072978303747535,"retrieve a desired action arg max
a∈A
p(s, a) during inference despite the choice of models.
46"
INTRODUCTION,0.09270216962524655,"This work proposes an imitation learning framework that combines both the efficiency of modeling the
47"
INTRODUCTION,0.09467455621301775,"conditional probability and the generalization ability of modeling the joint probability. Specifically,
48"
INTRODUCTION,0.09664694280078895,"we propose to model the expert state-action pairs using a state-of-the-art generative model, a diffusion
49"
INTRODUCTION,0.09861932938856016,"model, which learns to estimate how likely a state-action pair is sampled from the expert dataset.
50"
INTRODUCTION,0.10059171597633136,"Then, we train a policy to optimize both the BC objective and the estimate produced by the learned
51"
INTRODUCTION,0.10256410256410256,"diffusion model. Therefore, our proposed framework not only can efficiently predict actions given
52"
INTRODUCTION,0.10453648915187377,"states via capturing the conditional probability p(a|s) but also enjoys the generalization ability
53"
INTRODUCTION,0.10650887573964497,"induced by modeling the joint probability p(s, a) and utilizing it to guide policy learning.
54"
INTRODUCTION,0.10848126232741617,"We evaluate our proposed framework and baselines in various continuous control domains, including
55"
INTRODUCTION,0.11045364891518737,"navigation, robot arm manipulation, and locomotion. The experimental results show that the proposed
56"
INTRODUCTION,0.11242603550295859,"framework outperforms all the baselines or achieves competitive performance on all tasks. Extensive
57"
INTRODUCTION,0.11439842209072978,"ablation studies compare our proposed method to its variants, justifying our design choices, such as
58"
INTRODUCTION,0.11637080867850098,"different generative models, and investigating the effect of hyperparameters.
59"
RELATED WORK,0.11834319526627218,"2
Related Work
60"
RELATED WORK,0.1203155818540434,"Imitation learning addresses the challenge of learning by observing expert demonstrations without
61"
RELATED WORK,0.1222879684418146,"access to reward signals from environments. It has various applications such as robotics [Schaal,
62"
RELATED WORK,0.1242603550295858,"1997], autonomous driving [Ly and Akhloufi, 2020], and game AI [Harmer et al., 2018].
63"
RELATED WORK,0.126232741617357,"Behavioral Cloning (BC). BC [Pomerleau, 1989, Torabi et al., 2018] formulate imitating an expert
64"
RELATED WORK,0.1282051282051282,"as a supervised learning problem. Due to its simplicity and effectiveness, it has been widely adopted
65"
RELATED WORK,0.1301775147928994,"in various domains. Yet, it often struggles at generalizing to states unobserved from the expert
66"
RELATED WORK,0.13214990138067062,"demonstrations [Ross et al., 2011, Florence et al., 2022]. In this work, we augment BC by employing
67"
RELATED WORK,0.1341222879684418,"a diffusion model that learns to capture the joint probability of expert state-action pairs.
68"
RELATED WORK,0.13609467455621302,"Adversarial Imitation Learning (AIL). AIL methods aim to match the state-action distributions of
69"
RELATED WORK,0.13806706114398423,"an agent and an expert via adversarial training. Generative adversarial imitation learning (GAIL) [Ho
70"
RELATED WORK,0.14003944773175542,"and Ermon, 2016] and its extensions [Torabi et al., 2019, Kostrikov et al., 2019, Zolna et al., 2021]
71"
RELATED WORK,0.14201183431952663,"resemble the idea of generative adversarial networks [Goodfellow et al., 2014], which trains a
72"
RELATED WORK,0.14398422090729784,"generator policy to imitate expert behaviors and a discriminator to distinguish between the expert
73"
RELATED WORK,0.14595660749506903,"and the learner’s state-action pair distributions. While modeling state-action distributions often leads
74"
RELATED WORK,0.14792899408284024,"to satisfactory performance, adversarial learning can be unstable and inefficient [Chen et al., 2020].
75"
RELATED WORK,0.14990138067061143,"Moreover, AIL methods require online interaction with environments, which can be costly or even
76"
RELATED WORK,0.15187376725838264,"dangerous. In contrast, our work does not require interacting with environments.
77"
RELATED WORK,0.15384615384615385,"Inverse Reinforcement Learning (IRL). IRL methods [Ng and Russell, 2000, Abbeel and Ng,
78"
RELATED WORK,0.15581854043392504,"2004, Fu et al., 2018, Lee et al., 2021] are designed to infer the reward function that underlies the
79"
RELATED WORK,0.15779092702169625,"expert demonstrations and then learn a policy using the inferred reward function. This allows for
80"
RELATED WORK,0.15976331360946747,"learning tasks whose reward functions are difficult to specify manually. However, due to its double-
81"
RELATED WORK,0.16173570019723865,"loop learning procedure, IRL methods are typically computationally expensive and time-consuming.
82"
RELATED WORK,0.16370808678500987,"Additionally, obtaining accurate estimates of the expert’s reward function can be difficult, especially
83"
RELATED WORK,0.16568047337278108,"when the expert’s behavior is non-deterministic or when the expert’s demonstrations are sub-optimal.
84"
RELATED WORK,0.16765285996055226,"Diffusion Policies. Recently, Pearce et al. [2023], Chi et al. [2023], Reuss et al. [2023] propose to
85"
RELATED WORK,0.16962524654832348,"represent and learn an imitation learning policy using a conditional diffusion model, which produces
86"
RELATED WORK,0.17159763313609466,"a predicted action conditioning on a state and a sampled noise vector. These methods achieve
87"
RELATED WORK,0.17357001972386588,"encouraging results in modeling stochastic and multimodal behaviors from human experts or play
88"
RELATED WORK,0.1755424063116371,"data. In contrast, instead of representing a policy using a diffusion model, our work employs a
89"
RELATED WORK,0.17751479289940827,"diffusion model trained on expert demonstrations to guide a policy as a learning objective.
90"
PRELIMINARIES,0.1794871794871795,"3
Preliminaries
91"
IMITATION LEARNING,0.1814595660749507,"3.1
Imitation Learning
92"
IMITATION LEARNING,0.1834319526627219,"Without loss of generality, the reinforcement learning problem can be formulated as a Markov decision
93"
IMITATION LEARNING,0.1854043392504931,"process (MDP), which can be represented by a tuple M = (S, A, R, P, ρ, γ) with states S, actions
94"
IMITATION LEARNING,0.1873767258382643,"A, reward function R(S, A) ∈(0, 1), transition distribution P(s
′|s, a) : S × A × S →[0, 1], initial
95"
IMITATION LEARNING,0.1893491124260355,"state distribution ρ, and discounted factor γ. Based on the rewards received while interacting with
96"
IMITATION LEARNING,0.1913214990138067,"the environment, the goal is to learn a policy π(·|s) to maximize the expectation of the cumulative
97"
IMITATION LEARNING,0.1932938856015779,"discounted return (i.e., value function): V (π) = E[
TP"
IMITATION LEARNING,0.1952662721893491,"t=0
γtR(st, at)|s0 ∼ρ(·), at ∼π(·|st), st+1 ∼
98"
IMITATION LEARNING,0.19723865877712032,"P(st+1|st, at)], where T denotes the episode length. Instead of interacting with the environment and
99"
IMITATION LEARNING,0.1992110453648915,"receiving rewards, imitation learning aims to learn an agent policy from an expert demonstration
100"
IMITATION LEARNING,0.20118343195266272,"dataset, containing M trajectories, D = {τ1, ..., τM}, where τi represents a sequence of ni state-
101"
IMITATION LEARNING,0.20315581854043394,"action pairs {si
1, ai
1, ..., si
ni, ai
ni}.
102"
IMITATION LEARNING,0.20512820512820512,"3.2
Behavioral Cloning: Modeling Conditional Probability p(a|s)
103"
IMITATION LEARNING,0.20710059171597633,"To learn a policy π, behavioral cloning (BC) directly estimates the expert policy πE with maximum
104"
IMITATION LEARNING,0.20907297830374755,"likelihood estimation (MLE). Given a state-action pair (s, a) sampled from the dataset D, BC
105"
IMITATION LEARNING,0.21104536489151873,"optimizes max
θ
P"
IMITATION LEARNING,0.21301775147928995,"(s,a)∈D
log(πθ(a|s)), where θ denotes the parameters of the policy π. One can view a
106"
IMITATION LEARNING,0.21499013806706113,"BC policy as a discriminative model p(a|s), capturing the conditional probability of an action a given
107"
IMITATION LEARNING,0.21696252465483234,"a state s. Despite its success in various applications, BC tends to overfit and struggle at generalizing
108"
IMITATION LEARNING,0.21893491124260356,"to states unseen during training [Ross et al., 2011, Codevilla et al., 2019, Wang et al., 2022].
109"
IMITATION LEARNING,0.22090729783037474,"3.3
Modeling Joint Probability p(s, a)
110"
IMITATION LEARNING,0.22287968441814596,"Aiming for improved generalization ability, implicit behavioral cloning [Florence et al., 2022] and
111"
IMITATION LEARNING,0.22485207100591717,"methods in a similar vein [Ganapathi et al., 2022] model the joint probability p(s, a) of expert state-
112"
IMITATION LEARNING,0.22682445759368836,"action pairs. These methods demonstrate superior generalization performance in diverse domains. Yet,
113"
IMITATION LEARNING,0.22879684418145957,"without directly modeling the conditional probability p(a|s), the action sampling and optimization
114"
IMITATION LEARNING,0.23076923076923078,"procedure to retrieve a desired action arg maxa∈A p(s, a) during inference is often time-consuming.
115"
IMITATION LEARNING,0.23274161735700197,"Moreover, explicit generative models such as energy-based models [Du and Mordatch, 2019, Song and
116"
IMITATION LEARNING,0.23471400394477318,"Kingma, 2021], variational autoencoder [Kingma and Welling, 2014], and flow-based models Rezende
117"
IMITATION LEARNING,0.23668639053254437,"and Mohamed [2015], Dinh et al. [2017] are known to struggle with modeling observed high-
118"
IMITATION LEARNING,0.23865877712031558,"dimensional data that lies on a low-dimensional manifold (i.e., manifold overfitting) [Wu et al., 2021,
119"
IMITATION LEARNING,0.2406311637080868,"Loaiza-Ganem et al., 2022]. As a result, these methods often perform poorly when learning from
120"
IMITATION LEARNING,0.24260355029585798,"demonstrations produced by script policies or PID controllers, as discussed in Section 5.4.
121"
IMITATION LEARNING,0.2445759368836292,"We aim to develop an imitation learning framework that enjoys the advantages of modeling the
122"
IMITATION LEARNING,0.2465483234714004,"conditional probability p(a|s) and the joint probability p(s, a). Specifically, we propose to model the
123"
IMITATION LEARNING,0.2485207100591716,"joint probability of expert state-action pairs using an explicit generative model ϕ, which learns to
124"
IMITATION LEARNING,0.2504930966469428,"produce an estimate indicating how likely a state-action pair is sampled from the expert dataset. Then,
125"
IMITATION LEARNING,0.252465483234714,"we train a policy to model the conditional probability p(a|s) by optimizing the BC objective and
126"
IMITATION LEARNING,0.25443786982248523,"the estimate produced by the learned generative model ϕ. Hence, our method can efficiently predict
127"
IMITATION LEARNING,0.2564102564102564,"actions given states, generalize better to unseen states, and suffer less from manifold overfitting.
128"
DIFFUSION MODELS,0.2583826429980276,"3.4
Diffusion Models
129"
DIFFUSION MODELS,0.2603550295857988,"As described in the previous sections, this work aims to combine the advantages of modeling both
130"
DIFFUSION MODELS,0.26232741617357,"the conditional probability p(a|s) and the joint probability p(s, a). To this end, we leverage diffusion
131"
DIFFUSION MODELS,0.26429980276134124,"models to model the joint probability of expert state-action pairs. The diffusion model is a recently
132"
DIFFUSION MODELS,0.26627218934911245,"developed class of generative models and has achieved state-of-the-art performance on various
133"
DIFFUSION MODELS,0.2682445759368836,"tasks Sohl-Dickstein et al. [2015], Nichol and Dhariwal [2021], Dhariwal and Nichol [2021].
134"
DIFFUSION MODELS,0.2702169625246548,q(xn|xn−1)
DIFFUSION MODELS,0.27218934911242604,"ϕ(xn−1|xn)
xn−1
xn
xN
x0"
DIFFUSION MODELS,0.27416173570019725,Forward diffusion process
DIFFUSION MODELS,0.27613412228796846,"Reverse diffusion process
Figure 1:
Denoising Diffusion Probabilistic
Model (DDPM). Latent variables x1, ..., xN are
produced from the data point x0 via the forward
diffusion process, i.e., gradually adding noises to
the latent variables. The diffusion model ϕ learns
to reverse the diffusion process by denoising the
noisy data to reconstruct the original data point x0."
DIFFUSION MODELS,0.2781065088757396,"In this work, we utilize Denoising Diffusion
135"
DIFFUSION MODELS,0.28007889546351084,"Probabilistic Models (DDPMs) J Ho [2020] to
136"
DIFFUSION MODELS,0.28205128205128205,"model expert state-action pairs. Specifically,
137"
DIFFUSION MODELS,0.28402366863905326,"DDPM models gradually add noise to data sam-
138"
DIFFUSION MODELS,0.2859960552268245,"ples (i.e., concatenated state-action pairs) until
139"
DIFFUSION MODELS,0.2879684418145957,"they become isotropic Gaussian (forward diffu-
140"
DIFFUSION MODELS,0.28994082840236685,"sion process), and then learn to denoise each
141"
DIFFUSION MODELS,0.29191321499013806,"step and restore the original data samples (re-
142"
DIFFUSION MODELS,0.2938856015779093,"verse diffusion process), as illustrated in Figure
143"
DIFFUSION MODELS,0.2958579881656805,"1. In other words, DDPM learns to recognize a
144"
DIFFUSION MODELS,0.2978303747534517,"data distribution by learning to denoise noisy
145"
DIFFUSION MODELS,0.29980276134122286,"sampled data. More discussion on diffusion
146"
DIFFUSION MODELS,0.30177514792899407,"models can be found in the Section G.
147"
APPROACH,0.3037475345167653,"4
Approach
148"
APPROACH,0.3057199211045365,"Our goal is to design an imitation learning framework that enjoys both the advantages of modeling
149"
APPROACH,0.3076923076923077,"the conditional probability and the joint probability of expert behaviors. To this end, we first adopt
150"
APPROACH,0.3096646942800789,"behavioral cloning (BC) for modeling the conditional probability from expert state-action pairs, as
151"
APPROACH,0.3116370808678501,"described in Section 4.1. To capture the joint probability of expert state-action pairs, we employ
152"
APPROACH,0.3136094674556213,"a diffusion model which learns to produce an estimate indicating how likely a state-action pair is
153"
APPROACH,0.3155818540433925,"sampled from the expert state-action pair distribution, as presented in Section 4.2.1. Then, we propose
154"
APPROACH,0.3175542406311637,"to guide the policy learning by optimizing this estimate provided by a learned diffusion model,
155"
APPROACH,0.31952662721893493,"encouraging the policy to produce actions similar to expert actions, as discussed in Section 4.2.2.
156"
APPROACH,0.3214990138067061,"Finally, in Section 4.3, we introduce the framework that combines the BC loss and our proposed
157"
APPROACH,0.3234714003944773,"diffusion model loss, allowing for learning a policy that benefits from modeling both the conditional
158"
APPROACH,0.3254437869822485,"probability and the joint probability of expert behaviors. An overview of our proposed framework is
159"
APPROACH,0.32741617357001973,"illustrated in Figure 2, and the algorithm is detailed in Section B.
160"
BEHAVIORAL CLONING LOSS,0.32938856015779094,"4.1
Behavioral Cloning Loss
161"
BEHAVIORAL CLONING LOSS,0.33136094674556216,"The behavioral cloning (BC) model aims to imitate expert behaviors with supervision learning. BC
162"
BEHAVIORAL CLONING LOSS,0.3333333333333333,"learns to capture the conditional probability p(a|s) of expert state-action pairs. Given a sampled
163"
BEHAVIORAL CLONING LOSS,0.33530571992110453,"expert state-action pair (s, a), a policy π learns to predict an action ˆa ∼π(s) by optimizing
164"
BEHAVIORAL CLONING LOSS,0.33727810650887574,"LBC = d(a, ˆa),
(1)"
BEHAVIORAL CLONING LOSS,0.33925049309664695,"where d(·, ·) denotes a distance measure between a pair of actions. For example, we can adapt the
165"
BEHAVIORAL CLONING LOSS,0.34122287968441817,"mean-square error (MSE) loss ||a −ˆa||2 for most continuous control tasks.
166"
LEARNING A DIFFUSION MODEL AND GUIDING POLICY LEARNING,0.3431952662721893,"4.2
Learning a Diffusion Model and Guiding Policy Learning
167"
LEARNING A DIFFUSION MODEL AND GUIDING POLICY LEARNING,0.34516765285996054,"Instead of directly learning the conditional probability p(a|s), this section discusses how to model
168"
LEARNING A DIFFUSION MODEL AND GUIDING POLICY LEARNING,0.34714003944773175,"the joint probability p(s, a) of expert behaviors with a diffusion model in Section 4.2.1 and presents
169"
LEARNING A DIFFUSION MODEL AND GUIDING POLICY LEARNING,0.34911242603550297,"how to leverage the learned diffusion model to guide policy learning in Section 4.2.2.
170"
LEARNING A DIFFUSION MODEL,0.3510848126232742,"4.2.1
Learning a Diffusion Model
171"
LEARNING A DIFFUSION MODEL,0.3530571992110454,"We propose to model the joint probability of expert state-action pairs with a diffusion model ϕ.
172"
LEARNING A DIFFUSION MODEL,0.35502958579881655,"Specifically, we create a joint distribution by simply concatenating a state vector s and an action
173"
LEARNING A DIFFUSION MODEL,0.35700197238658776,"vector a from a state-action pair (s, a). To model such distribution by learning a denoising diffusion
174"
LEARNING A DIFFUSION MODEL,0.358974358974359,"probabilistic model (DDPM) J Ho [2020], we inject noise ϵ(n) into sampled state-action pairs, where
175"
LEARNING A DIFFUSION MODEL,0.3609467455621302,"n indicates the number of steps of the Markov procedure, which can be viewed as a variable of the
176"
LEARNING A DIFFUSION MODEL,0.3629191321499014,"level of noise. Then, we train the diffusion model ϕ to predict the injected noises by optimizing
177"
LEARNING A DIFFUSION MODEL,0.36489151873767256,"Ldiff(s, a, ϕ) = ||ˆϵ(s, a, n) −ϵ(n)||2 = ||ϕ(s, a, ϵ(n)) −ϵ(n)||2,
(2)"
LEARNING A DIFFUSION MODEL,0.3668639053254438,"where ˆϵ is the noise predicted by the diffusion model ϕ. Once optimized, the diffusion model can
178"
LEARNING A DIFFUSION MODEL,0.368836291913215,"recognize the expert distribution by perfectly predicting the noise injected into state-action pairs
179"
LEARNING A DIFFUSION MODEL,0.3708086785009862,"sampled from the expert distribution. On the other hand, predicting the noise injected into state-
180"
LEARNING A DIFFUSION MODEL,0.3727810650887574,"action pairs sampled from any other distribution should yield a higher loss value. Therefore, we
181"
LEARNING A DIFFUSION MODEL,0.3747534516765286,"(a) Learning a Diffusion Model
(b) Learning a Policy with the Learned Diffusion Model s a"
LEARNING A DIFFUSION MODEL,0.3767258382642998,"ϵ
Diffusion 
Model ϕ
̂ϵ"
LEARNING A DIFFUSION MODEL,0.378698224852071,"Expert’s Demonstrations D
τ1 s1 1 a1 1 s1 2 a1 2 s1 n1 a1 n1 τ2 s2 1 a2 1 s2 2 a2 2 s2 n2 a2 n2 τM sM 1 aM 1 sM 2 a3 2 sM nM aM nM"
LEARNING A DIFFUSION MODEL,0.3806706114398422,Sample ℒdiff
LEARNING A DIFFUSION MODEL,0.3826429980276134,Learning objective
LEARNING A DIFFUSION MODEL,0.38461538461538464,Learnable mapping
LEARNING A DIFFUSION MODEL,0.3865877712031558,"Frozen mapping
s ̂a ϵ"
LEARNING A DIFFUSION MODEL,0.388560157790927,"Diffusion 
Model ϕ
s a ϵ"
LEARNING A DIFFUSION MODEL,0.3905325443786982,ℒagent diff
LEARNING A DIFFUSION MODEL,0.39250493096646943,ℒexpert diff ℒDM
LEARNING A DIFFUSION MODEL,0.39447731755424065,Policy
LEARNING A DIFFUSION MODEL,0.39644970414201186,"π
a
s
̂a
ℒBC"
LEARNING A DIFFUSION MODEL,0.398422090729783,Agent’s
LEARNING A DIFFUSION MODEL,0.40039447731755423,action
LEARNING A DIFFUSION MODEL,0.40236686390532544,Expert’s
LEARNING A DIFFUSION MODEL,0.40433925049309666,"action
State"
LEARNING A DIFFUSION MODEL,0.40631163708086787,"Figure 2: Diffusion Model-Augmented Behavioral Cloning. Our proposed method DBC augments behavioral
cloning (BC) by employing a diffusion model. (a) Learning a Diffusion Model: the diffusion model ϕ learns to
model the distribution of concatenated state-action pairs sampled from the demonstration dataset D. It learns
to reverse the diffusion process (i.e., denoise) by optimizing Ldiff in Eq. 2. (b) Learning a Policy with the
Learned Diffusion Model: we propose a diffusion model objective LDM for policy learning and jointly optimize
it with the BC objective LBC. Specifically, LDM is computed based on processing a sampled state-action pair
(s, a) and a state-action pair (s, ˆa) with the action ˆa predicted by the policy π with Ldiff."
LEARNING A DIFFUSION MODEL,0.40828402366863903,"propose to view Ldiff(s, a, ϕ) as an estimate of how well the state-action pair (s, a) fits the state-action
182"
LEARNING A DIFFUSION MODEL,0.41025641025641024,"distribution that ϕ learns from.
183"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.41222879684418146,"4.2.2
Learning a Policy with Diffusion Model Loss
184"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.41420118343195267,"A diffusion model ϕ trained on the expert distribution can produce an estimate Ldiff(s, a, ϕ) indicating
185"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4161735700197239,"how well a state-action pair (s, a) fits the expert distribution. We propose to leverage this signal to
186"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4181459566074951,"guide a policy to imitate the expert. Specifically, given a state-action (s, a) sampled from D, the π
187"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.42011834319526625,"predicts an action given the state ˆa ∼π(s) by optimizing
188"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.42209072978303747,"Lagent
diff
= Ldiff(s, ˆa, ϕ) = ||ˆϵ(s, ˆa, n) −ϵ||2.
(3)
Intuitively, the policy learns to predict actions that are indistinguishable from the expert actions for
189"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4240631163708087,"the diffusion model conditioning on the same set of states.
190"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4260355029585799,"We hypothesize that learning a policy to optimize Eq. 3 can be unstable, especially for state-action
191"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4280078895463511,"pairs that are not well-modeled by the diffusion model, which yield a high value of Ldiff even with
192"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.42998027613412226,"expert state-action pairs. Therefore, we propose to normalize the agent diffusion loss Lagent
diff
with an
193"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4319526627218935,"expert diffusion loss Lexpert
diff , which can be computed with expert state-action pairs (s, a) as follows:
194"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4339250493096647,"Lexpert
diff
= Ldiff(s, a, ϕ) = ||ˆϵ(s, a, n) −ϵ||2.
(4)"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4358974358974359,"We propose to optimize the diffusion model loss LDM based on calculating the difference between
195"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.4378698224852071,"the above agent and expert diffusion losses:
196"
LEARNING A POLICY WITH DIFFUSION MODEL LOSS,0.43984220907297833,"LDM = max(Lagent
diff −Lexpert
diff , 0).
(5)"
COMBINING THE TWO OBJECTIVES,0.4418145956607495,"4.3
Combining the Two Objectives
197"
COMBINING THE TWO OBJECTIVES,0.4437869822485207,"Our goal is to learn a policy that benefits from both modeling the conditional probability and the joint
198"
COMBINING THE TWO OBJECTIVES,0.4457593688362919,"probability of expert behaviors. To this end, we propose to augment a BC policy that optimizes the
199"
COMBINING THE TWO OBJECTIVES,0.4477317554240631,"BC loss LBC in Eq. 1 by jointing optimizing the proposed diffusion model loss LDM in Eq. 5, which
200"
COMBINING THE TWO OBJECTIVES,0.44970414201183434,"encourages the policy to predict actions that fit the expert joint probability captured by a diffusion
201"
COMBINING THE TWO OBJECTIVES,0.4516765285996055,"model. To learn from both the BC loss and the diffusion model loss, we train the policy to optimize
202"
COMBINING THE TWO OBJECTIVES,0.4536489151873767,"Ltotal = LBC + λLDM,
(6)
where λ is a coefficient that determines the importance of the diffusion model loss relative to the BC
203"
COMBINING THE TWO OBJECTIVES,0.4556213017751479,"loss. We analyze the effect of the coefficient in Section 5.6.1.
204"
COMBINING THE TWO OBJECTIVES,0.45759368836291914,"(a) MAZE
(b) FETCHPICK
(c) FETCHPUSH
(d) HANDROTATE
(e) WALKER"
COMBINING THE TWO OBJECTIVES,0.45956607495069035,"Figure 3: Environments & Tasks. (a) MAZE: A point-mass agent (green) in a 2D maze learns to
navigate from its start location to a goal location (red). (b)-(c) FETCHPICK and FETCHPUSH: The
robot arm manipulation tasks employ a 7-DoF Fetch robotics arm. FETCHPICK requires picking up
an object (yellow cube) from the table and moving it to a target location (red); FETCHPUSH requires
the arm to push an object (black cube) to a target location (red). (d) HANDROTATE: This dexterous
manipulation task requires a Shadow Dexterous Hand to in-hand rotate a block to a target orientation.
(e) WALKER: This locomotion task requires learning a bipedal walker policy to walk as fast as
possible while maintaining its balance."
EXPERIMENTS,0.46153846153846156,"5
Experiments
205"
EXPERIMENTS,0.4635108481262327,"We design experiments in various continuous control domains, including navigation, robot arm
206"
EXPERIMENTS,0.46548323471400394,"manipulation, dexterous manipulation, and locomotion, to compare our proposed framework (DBC)
207"
EXPERIMENTS,0.46745562130177515,"to its variants and baselines.
208"
EXPERIMENTAL SETUP,0.46942800788954636,"5.1
Experimental Setup
209"
EXPERIMENTAL SETUP,0.4714003944773176,"This section describes the environments, tasks, and expert demonstrations used for learning and
210"
EXPERIMENTAL SETUP,0.47337278106508873,"evaluation. More details can be found in Section A.
211"
EXPERIMENTAL SETUP,0.47534516765285995,"Navigation. To evaluate our method on a navigation task, we choose MAZE, a maze environment
212"
EXPERIMENTAL SETUP,0.47731755424063116,"proposed in Fu et al. [2020] (maze2d-medium-v2), as illustrated in Figure 3a. This task features
213"
EXPERIMENTAL SETUP,0.47928994082840237,"a point-mass agent in a 2D maze learning to navigate from its start location to a goal location by
214"
EXPERIMENTAL SETUP,0.4812623274161736,"iteratively predicting its x and y acceleration. The agent’s beginning and final locations are chosen
215"
EXPERIMENTAL SETUP,0.4832347140039448,"randomly. We collect 100 demonstrations with 18,525 transitions using a controller.
216"
EXPERIMENTAL SETUP,0.48520710059171596,"Robot Arm Manipulation. We evaluate our method in a robot arm manipulation domain with
217"
EXPERIMENTAL SETUP,0.48717948717948717,"two 7-DoF Fetch tasks: FETCHPICK and FETCHPUSH, as illustrated in Figure 3c and Figure
218"
EXPERIMENTAL SETUP,0.4891518737672584,"3b. FETCHPICK requires picking up an object from the table and lifting it to a target location;
219"
EXPERIMENTAL SETUP,0.4911242603550296,"FETCHPUSH requires the arm to push an object to a target location. We use the demonstrations
220"
EXPERIMENTAL SETUP,0.4930966469428008,"provided in Lee et al. [2021] for these tasks. Each dataset contains 10k transitions (303 trajectories
221"
EXPERIMENTAL SETUP,0.49506903353057197,"for FETCHPICK and 185 trajectories for FETCHPUSH).
222"
EXPERIMENTAL SETUP,0.4970414201183432,"Dexterous Manipulation. In HANDROTATE, we further evaluate our method on a challenging
223"
EXPERIMENTAL SETUP,0.4990138067061144,"environment proposed in Plappert et al. [2018], where a 24-DoF Shadow Dexterous Hand learns
224"
EXPERIMENTAL SETUP,0.5009861932938856,"to in-hand rotate a block to a target orientation, as illustrated in Figure 3d. This environment has
225"
EXPERIMENTAL SETUP,0.5029585798816568,"a high-dimensional state space (68D) and action space (20D). We collected 10k transitions (515
226"
EXPERIMENTAL SETUP,0.504930966469428,"trajectories) from a SAC [Haarnoja et al., 2018] expert policy trained for 10M environment steps.
227"
EXPERIMENTAL SETUP,0.5069033530571992,"Locomotion. For locomotion, we leverage the WALKER environment Brockman et al. [2016], which
228"
EXPERIMENTAL SETUP,0.5088757396449705,"requires a bipedal agent to walk as fast as possible while maintaining its balance, as illustrated in
229"
EXPERIMENTAL SETUP,0.5108481262327417,"Figure 3e. We use the demonstrations provided by Kostrikov [2018], which contains 5 trajectories
230"
EXPERIMENTAL SETUP,0.5128205128205128,"with 5k state-action pairs.
231"
BASELINES,0.514792899408284,"5.2
Baselines
232"
BASELINES,0.5167652859960552,"We compare our method DBC with the following baselines.
233"
BASELINES,0.5187376725838264,"• BC learns to imitate an expert by modeling the conditional probability p(a|s) of the expert
234"
BASELINES,0.5207100591715976,"behaviors via optimizing the BC loss LBC in Eq. 1.
235"
BASELINES,0.5226824457593688,"Table 1: Experimental Result. We report the mean and the standard deviation of success rate (MAZE,
FETCHPICK, FETCHPUSH, HANDROTATE) and return (WALKER), evaluated over three random
seeds. Our proposed method (DBC) outperforms the baselines on MAZE, FETCHPICK, FETCHPUSH,
HANDROTATE, and performs competitively against the best performing baseline on WALKER."
BASELINES,0.52465483234714,"Method
MAZE
FETCHPICK
FETCHPUSH
HANDROTATE
WALKER"
BASELINES,0.5266272189349113,"BC
79.35% ± 5.05%
69.15% ± 5.00%
66.02% ± 6.88%
55.48% ± 3.97%
7066.61 ± 22.79
Implicit BC
81.43% ± 4.88%
72.27% ± 6.71%
77.70% ± 4.42%
14.52% ± 3.04%
685.92 ± 150.26
Diffusion Policy
73.34% ± 5.30%
74.37% ± 3.80%
86.93% ± 3.26%
58.59% ± 2.85%
6429.87 ± 356.70
DBC
86.99% ± 2.84%
88.71% ± 6.46%
89.50% ± 3.99%
60.34% ± 4.60%
7057.42 ± 36.19"
BASELINES,0.5285996055226825,"• Implicit BC (IBC) [Florence et al., 2022] models expert state-action pairs with an energy-based
236"
BASELINES,0.5305719921104537,"model. For inference, we implement the derivative-free optimization algorithm proposed in IBC,
237"
BASELINES,0.5325443786982249,"which samples actions iteratively to select the desired action with the minimum predicted energy.
238"
BASELINES,0.534516765285996,"This baseline serves a representative of the methods that solely model the joint probability p(s, a)
239"
BASELINES,0.5364891518737672,"of the expert behaviors.
240"
BASELINES,0.5384615384615384,"• Diffusion policy refers to the methods that learn a conditional diffusion model as a policy [Chi
241"
BASELINES,0.5404339250493096,"et al., 2023, Reuss et al., 2023]. Specifically, we implement this baseline based on Pearce et al.
242"
BASELINES,0.5424063116370809,"[2023]. We include this baseline to analyze the effectiveness of using diffusion models as a policy
243"
BASELINES,0.5443786982248521,"or as a learning objective (ours).
244"
EXPERIMENTAL RESULTS,0.5463510848126233,"5.3
Experimental Results
245"
EXPERIMENTAL RESULTS,0.5483234714003945,"We report the experimental results in terms of success rate (MAZE, FETCHPICK, FETCHPUSH,
246"
EXPERIMENTAL RESULTS,0.5502958579881657,"HANDROTATE), and return (WALKER) in Table 1. The details of model architecture can be found
247"
EXPERIMENTAL RESULTS,0.5522682445759369,"in Section C. Training and evaluation details can be found in Section D. Additional analysis and
248"
EXPERIMENTAL RESULTS,0.5542406311637081,"experimental results can be found in Section E and Section F.
249"
EXPERIMENTAL RESULTS,0.5562130177514792,"Overall Task Performance. Our proposed method DBC achieves the highest success rates, out-
250"
EXPERIMENTAL RESULTS,0.5581854043392505,"performing our baselines in all the goal-directed tasks (MAZE, FETCHPICK, FETCHPUSH, and
251"
EXPERIMENTAL RESULTS,0.5601577909270217,"HANDROTATE) and perform competitively in WALKER compared to the best-performing baseline
252"
EXPERIMENTAL RESULTS,0.5621301775147929,"(BC). We hypothesize the improvement in the goal-directed tasks can be mostly attributed to the
253"
EXPERIMENTAL RESULTS,0.5641025641025641,"better generalization ability since starting positions and the goals are randomized during evaluation
254"
EXPERIMENTAL RESULTS,0.5660749506903353,"and therefore requires the policy to deal with unseen situation. To verify this hypothesis, we further
255"
EXPERIMENTAL RESULTS,0.5680473372781065,"evaluate the baselines and our method in FETCHPICK and FETCHPUSH with different levels of
256"
EXPERIMENTAL RESULTS,0.5700197238658777,"randomization in Section E.
257"
EXPERIMENTAL RESULTS,0.571992110453649,"Locomotion. Unlike the goal-directed tasks, we do not observe significant improvement but competi-
258"
EXPERIMENTAL RESULTS,0.5739644970414202,"tive results from DBC compared to the best-performing baseline (BC). We hypothesize that this is
259"
EXPERIMENTAL RESULTS,0.5759368836291914,"because locomotion tasks such as WALKER, with sufficient expert demonstrations and little random-
260"
EXPERIMENTAL RESULTS,0.5779092702169625,"ness, do not require generalization during inference. The agent can simply follow the closed-loop
261"
EXPERIMENTAL RESULTS,0.5798816568047337,"progress of the expert demonstrations, resulting in both BC (7066.61) and DBC (7057.42) performing
262"
EXPERIMENTAL RESULTS,0.5818540433925049,"similarly to the expert with an average return of 7063.72. On the other hand, we hypothesize that
263"
EXPERIMENTAL RESULTS,0.5838264299802761,"Diffusion Policy performs slightly worse due to its design for modeling multimodal behaviors, which
264"
EXPERIMENTAL RESULTS,0.5857988165680473,"is contradictory to learning from this single-mode simulated locomotion task.
265"
EXPERIMENTAL RESULTS,0.5877712031558185,"Action Space Dimension. While Implicit BC models the joint distribution and generalizes better,
266"
EXPERIMENTAL RESULTS,0.5897435897435898,"it requires time-consuming actions sampling and optimization during inference. Moreover, such
267"
EXPERIMENTAL RESULTS,0.591715976331361,"procedure may not scale well to high-dimensional action spaces. Our Implicit BC baseline with
268"
EXPERIMENTAL RESULTS,0.5936883629191322,"a derivative-free optimizer struggles in HANDROTATE and WALKER environments, whose action
269"
EXPERIMENTAL RESULTS,0.5956607495069034,"dimensions are 20 and 6, respectively. This is consistent with Florence et al. [2022], which reports
270"
EXPERIMENTAL RESULTS,0.5976331360946746,"that the optimizer failed to solve tasks with an action dimension larger than 5. In contrast, our
271"
EXPERIMENTAL RESULTS,0.5996055226824457,"proposed DBC can handle high-dimensional action spaces.
272"
EXPERIMENTAL RESULTS,0.6015779092702169,"Inference Efficiency. To evaluate the inference efficiency, we measure and report the number of
273"
EXPERIMENTAL RESULTS,0.6035502958579881,"evaluation episodes per second (↑) for IBC (9.92), Diffusion Policy (1.38), and DBC (30.79) on
274"
EXPERIMENTAL RESULTS,0.6055226824457594,"an NVIDIA RTX 3080 Ti GPU in MAZE. This can be attributed to the fact that DBC and BC
275"
EXPERIMENTAL RESULTS,0.6074950690335306,"model the conditional probability p(a|s) and can directly map states to actions during inference. In
276"
EXPERIMENTAL RESULTS,0.6094674556213018,"contrast, Implicit BC requires action sampling and optimization, while Diffusion Policy is required to
277"
EXPERIMENTAL RESULTS,0.611439842209073,"iteratively denoise sampled noises. This verifies the efficiency of modeling the conditional probability.
278"
EXPERIMENTAL RESULTS,0.6134122287968442,"(a)
(b)
(c)"
EXPERIMENTAL RESULTS,0.6153846153846154,"Figure 4: Comparing Modeling Conditional Probability and Joint Probability. (a) Generaliza-
tion. We collect expert trajectories from a PPO policy learning to navigate to goals sampled from the
green regions. Then, we learn a policy πBC to optimize LBC, and another policy πDM to optimize
LDM with a diffusion model trained on the expert distribution. We evaluate the two policies by
sampling goals from the red regions, which requires the ability to generalize. πBC (orange) struggles
at generalizing to unseen goals, whereas πDM (blue) can generalize (i.e., extrapolate) to some extent.
(b)-(c) Manifold overfitting. We collect the green spiral trajectories from a script policy, whose
actions are visualized as red crosses. We then train and evaluate πBC and πDM. The trajectories
of πBC (orange) can closely follow the expert trajectories (green), while the trajectories of πDM
(blue) drastically deviates from expert’s. This is because the diffusion model struggles at modeling
such expert action distribution with a lower intrinsic dimension, which can be observed from poorly
predicted actions (blue dots) produced by the diffusion model."
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6173570019723866,"5.4
Comparing Modeling Conditional Probability and Joint Probability
279"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6193293885601578,"This section aims to empirically identify the limitations of modeling either the conditional or the
280"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.621301775147929,"joint probability in an open maze environment implemented with [Fu et al., 2020].
281"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6232741617357002,"Generalization. We aim to investigate if learning from the BC loss alone struggles at generalization
282"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6252465483234714,"(conditional) and examine if guiding the policy using the diffusion model loss yields improved
283"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6272189349112426,"generalization ability (joint). We collect trajectories of a PPO policy learning to navigate from
284"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6291913214990138,"(5, 3) to goals sampled around (1, 2) and (1, 4) (green), as shown in Figure 4a. Given these expert
285"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.631163708086785,"trajectories, we learn a policy πBC to optimize Eq. 1 and another policy πDM to optimize Eq. 5. Then,
286"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6331360946745562,"we evaluate the two policies by sampling goals around (1, 1), (1, 3), and (1, 5) (red), which requires
287"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6351084812623274,"the ability to generalize. Visualized trajectories of the two policies in Figure 4a show that πBC
288"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6370808678500987,"(orange) fails to generalize to unseen goals, whereas πDM (blue) can generalize (i.e., extrapolate) to
289"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6390532544378699,"some extent. This verifies our motivation to augment BC with the diffusion model loss.
290"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6410256410256411,"Manifold overfitting. We aim to examine if modeling the joint probability is difficult when observed
291"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6429980276134122,"high-dimensional data lies on a low-dimensional manifold (i.e., manifold overfitting). We collect
292"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6449704142011834,"trajectories from a script policy that executes actions (0.5, 0), (0, 0.5), (−0.7, 0), and (0, −0.7) (red
293"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6469428007889546,"crosses in Figure 4b), each for 40 consecutive time steps, resulting the green spiral trajectories
294"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6489151873767258,"visualized in Figure 4c.
295"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.650887573964497,"Given these expert demonstrations, we learn a policy πBC to optimize Eq. 1, and another policy
296"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6528599605522682,"πDM to optimize Eq. 5 with a diffusion model trained on the expert distribution. Figure 4b shows
297"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6548323471400395,"that the diffusion model struggles at modeling such expert action distribution with a lower intrinsic
298"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6568047337278107,"dimension. As a result, Figure 4c show that the trajectories of πDM (blue) drastically deviates from
299"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6587771203155819,"the expert trajectories (green) as the diffusion model cannot provide effective loss. On the other hand,
300"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6607495069033531,"the trajectories of πBC (orange) is able to closely follow expert’s. This verifies our motivation to
301"
COMPARING MODELING CONDITIONAL PROBABILITY AND JOINT PROBABILITY,0.6627218934911243,"complement modeling the joint probability with modeling the conditional probability (i.e., BC).
302"
COMPARING DIFFERENT GENERATIVE MODELS,0.6646942800788954,"5.5
Comparing Different Generative Models
303"
COMPARING DIFFERENT GENERATIVE MODELS,0.6666666666666666,"Our proposed framework employs a diffusion model (DM) to model the joint probability of expert
304"
COMPARING DIFFERENT GENERATIVE MODELS,0.6686390532544378,"state-action pairs and utilizes it to guide policy learning. To justify our choice, we explore using other
305"
COMPARING DIFFERENT GENERATIVE MODELS,0.6706114398422091,"popular generative models to replace the diffusion model in MAZE. We consider energy-based models
306"
COMPARING DIFFERENT GENERATIVE MODELS,0.6725838264299803,"(EBMs) [Du and Mordatch, 2019, Song and Kingma, 2021], variational autoencoder (VAEs) [Kingma
307"
COMPARING DIFFERENT GENERATIVE MODELS,0.6745562130177515,"and Welling, 2014], and generative adversarial networks (GANs) Goodfellow et al. [2014]. Each
308"
COMPARING DIFFERENT GENERATIVE MODELS,0.6765285996055227,"Table 2: Generative Models. We compare using different
generative models to model the expert distribution and
guide policy learning in MAZE."
COMPARING DIFFERENT GENERATIVE MODELS,0.6785009861932939,"Method
without BC
with BC"
COMPARING DIFFERENT GENERATIVE MODELS,0.6804733727810651,"BC
N/A
79.35% ± 5.05%
EBM
49.09% ± 15.15%
80.00% ± 4.06%
VAE
48.47% ± 7.57%
82.31% ± 5.84%
GAN
50.29% ± 8.27%
71.64% ± 5.50%
DM
53.51% ± 4.20%
86.99% ± 2.84%"
COMPARING DIFFERENT GENERATIVE MODELS,0.6824457593688363,"Table 3: Effect of λ. We experiment
with different values of λ in MAZE, each
evaluated over three random seeds."
COMPARING DIFFERENT GENERATIVE MODELS,0.6844181459566075,"λ
Success Rate"
COMPARING DIFFERENT GENERATIVE MODELS,0.6863905325443787,"1
85.40% ± 4.37%
2
85.64% ± 3.69%
5
86.99% ± 2.84%
10
85.46% ± 4.47%
20
85.17% ± 2.61%"
COMPARING DIFFERENT GENERATIVE MODELS,0.6883629191321499,"generative model learns to model expert state-action pairs. To guide policy learning, given a predicted
309"
COMPARING DIFFERENT GENERATIVE MODELS,0.6903353057199211,"state-action pair (s, ˆa) we use the estimated energy of an EBM, the reconstruction error of a VAE,
310"
COMPARING DIFFERENT GENERATIVE MODELS,0.6923076923076923,"and the discriminator output of a GAN to optimize a policy with or without the BC loss. Training
311"
COMPARING DIFFERENT GENERATIVE MODELS,0.6942800788954635,"details can be found in Section D.3.
312"
COMPARING DIFFERENT GENERATIVE MODELS,0.6962524654832347,"Table 2 compares using different generative models to model the expert distribution and guide
313"
COMPARING DIFFERENT GENERATIVE MODELS,0.6982248520710059,"policy learning. All the generative model-guide policies can be improved by adding the BC loss,
314"
COMPARING DIFFERENT GENERATIVE MODELS,0.7001972386587771,"justifying our motivation to complement modeling the joint probability with modeling the conditional
315"
COMPARING DIFFERENT GENERATIVE MODELS,0.7021696252465484,"probability. With or without the BC loss, the diffusion model-guided policy achieves the best
316"
COMPARING DIFFERENT GENERATIVE MODELS,0.7041420118343196,"performance compared to other generative models, verifying our choice of the generative model.
317"
ABLATION STUDY,0.7061143984220908,"5.6
Ablation Study
318"
ABLATION STUDY,0.7080867850098619,"In this section, we investigate the effect of the diffusion model loss coefficient λ (Section 5.6.1) and
319"
ABLATION STUDY,0.7100591715976331,"examine the effect of the normalization term Lexpert
diff
in the diffusion model loss LDM (Section 5.6.2).
320"
ABLATION STUDY,0.7120315581854043,"5.6.1
Effect of the Diffusion Model Loss Coefficient λ
321"
ABLATION STUDY,0.7140039447731755,"We examine the impact of varying the coefficient of the diffusion model loss λ in Eq. 6 in MAZE.
322"
ABLATION STUDY,0.7159763313609467,"The result presented in Table 3 shows that λ = 5 yields the best performance. A higher or lower λ
323"
ABLATION STUDY,0.717948717948718,"leads to worse performance, demonstrating how modeling the conditional probability (LBC) and the
324"
ABLATION STUDY,0.7199211045364892,"joint probability (LDM) can complement each other.
325"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.7218934911242604,"5.6.2
Effect of the Normalization Term Lexpert
diff
326"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.7238658777120316,"We aim to investigate whether normalizing the diffusion model loss LDM with the expert diffusion
327"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.7258382642998028,"model loss Lexpert
diff
yields improved performance in MAZE. We train a variant of DBC where only
328"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.727810650887574,"Lagent
diff
in Eq. 3 instead of LDM in Eq. 5 is used to augment BC. This variant learning from an
329"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.7297830374753451,"unnormalized diffusion model loss achieves an average success rate of 80.20%, worse than the full
330"
"EFFECT OF THE NORMALIZATION TERM LEXPERT
DIFF",0.7317554240631163,"DBC (86.99%). This justifies the effectiveness of the proposed normalization term Lexpert
diff
in LDM.
331"
CONCLUSION,0.7337278106508875,"6
Conclusion
332"
CONCLUSION,0.7357001972386588,"We propose an imitation learning framework that benefits from modeling both the conditional
333"
CONCLUSION,0.73767258382643,"probability p(a|s) and the joint probability p(s, a) of the expert distribution. Our proposed diffusion
334"
CONCLUSION,0.7396449704142012,"model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert
335"
CONCLUSION,0.7416173570019724,"behaviors and learns a policy to optimize both the BC loss and our proposed diffusion model loss.
336"
CONCLUSION,0.7435897435897436,"Specifically, the BC loss captures the conditional probability p(a|s) from expert state-action pairs,
337"
CONCLUSION,0.7455621301775148,"which directly guides the policy to replicate the expert’s action. On the other hand, the diffusion
338"
CONCLUSION,0.747534516765286,"model loss models the joint distribution of expert’s state-action pairs p(s, a), which provides an
339"
CONCLUSION,0.7495069033530573,"evaluation of how well the predicted action aligned with the expert distribution. DBC outperforms
340"
CONCLUSION,0.7514792899408284,"baselines or achieves competitive performance in various continuous control tasks in navigation,
341"
CONCLUSION,0.7534516765285996,"robot arm manipulation, dexterous manipulation, and locomotion. We design additional experiments
342"
CONCLUSION,0.7554240631163708,"to verify the limitations of modeling either the conditional probability or the joint probability of the
343"
CONCLUSION,0.757396449704142,"expert distribution as well as compare different generative models. Ablation studies investigate the
344"
CONCLUSION,0.7593688362919132,"effect of hyperparameters and justify the effectiveness of our design choices.
345"
REFERENCES,0.7613412228796844,"References
346"
REFERENCES,0.7633136094674556,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
347"
REFERENCES,0.7652859960552268,"Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
348"
REFERENCES,0.7672583826429981,"through deep reinforcement learning. Nature, 2015.
349"
REFERENCES,0.7692307692307693,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
350"
REFERENCES,0.7712031558185405,"David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
351"
REFERENCES,0.7731755424063116,"International Conference on Learning Representations, 2016.
352"
REFERENCES,0.7751479289940828,"Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep
353"
REFERENCES,0.777120315581854,"reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 2017.
354"
REFERENCES,0.7790927021696252,"Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
355"
REFERENCES,0.7810650887573964,"reinforcement learning from human preferences. In Advances in Neural Information Processing
356"
REFERENCES,0.7830374753451677,"Systems, 2017.
357"
REFERENCES,0.7850098619329389,"Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent
358"
REFERENCES,0.7869822485207101,"alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.
359"
REFERENCES,0.7889546351084813,"Youngwoon Lee, Shao-Hua Sun, Sriram Somasundaram, Edward S. Hu, and Joseph J. Lim. Compos-
360"
REFERENCES,0.7909270216962525,"ing complex skills by learning transition policies. In Proceedings of International Conference on
361"
REFERENCES,0.7928994082840237,"Learning Representations, 2019.
362"
REFERENCES,0.7948717948717948,"Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning.
363"
REFERENCES,0.796844181459566,"Journal of Machine Learning Research, 2015.
364"
REFERENCES,0.7988165680473372,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
365"
REFERENCES,0.8007889546351085,"review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
366"
REFERENCES,0.8027613412228797,"Stefan Schaal. Learning from demonstration. In Advances in Neural Information Processing Systems,
367"
REFERENCES,0.8047337278106509,"1997.
368"
REFERENCES,0.8067061143984221,"Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al.
369"
REFERENCES,0.8086785009861933,"An algorithmic perspective on imitation learning. Foundations and Trends® in Robotics, 2018.
370"
REFERENCES,0.8106508875739645,"Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
371"
REFERENCES,0.8126232741617357,"Information Processing Systems, 2016.
372"
REFERENCES,0.814595660749507,"Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan
373"
REFERENCES,0.8165680473372781,"Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning.
374"
REFERENCES,0.8185404339250493,"In Conference on Robot Learning, 2021.
375"
REFERENCES,0.8205128205128205,"Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.
376"
REFERENCES,0.8224852071005917,"Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation
377"
REFERENCES,0.8244575936883629,"learning. In International Conference on Learning Representations, 2019.
378"
REFERENCES,0.8264299802761341,"Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In International
379"
REFERENCES,0.8284023668639053,"Conference on Machine Learning, 2000.
380"
REFERENCES,0.8303747534516766,"Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
381"
REFERENCES,0.8323471400394478,"International Conference on Machine Learning, 2004.
382"
REFERENCES,0.834319526627219,"Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in Neural
383"
REFERENCES,0.8362919132149902,"Information Processing Systems, 1989.
384"
REFERENCES,0.8382642998027613,"Michael Bain and Claude Sammut. A framework for behavioural cloning. In Machine Intelligence
385"
REFERENCES,0.8402366863905325,"15, 1995.
386"
REFERENCES,0.8422090729783037,"Tung Nguyen, Qinqing Zheng, and Aditya Grover. Reliable conditioning of behavioral cloning for
387"
REFERENCES,0.8441814595660749,"offline reinforcement learning. arXiv preprint arXiv:2210.05158, 2023.
388"
REFERENCES,0.8461538461538461,"Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian
389"
REFERENCES,0.8481262327416174,"Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In
390"
REFERENCES,0.8500986193293886,"Conference on Robot Learning, 2022.
391"
REFERENCES,0.8520710059171598,"Aditya Ganapathi, Pete Florence, Jake Varley, Kaylee Burns, Ken Goldberg, and Andy Zeng. Implicit
392"
REFERENCES,0.854043392504931,"kinematic policies: Unifying joint and cartesian action spaces in end-to-end robot learning. In
393"
REFERENCES,0.8560157790927022,"International Conference on Robotics and Automation, 2022.
394"
REFERENCES,0.8579881656804734,"Abdoulaye O Ly and Moulay Akhloufi. Learning to drive by imitation: An overview of deep behavior
395"
REFERENCES,0.8599605522682445,"cloning methods. IEEE Transactions on Intelligent Vehicles, 2020.
396"
REFERENCES,0.8619329388560157,"Jack Harmer, Linus Gisslén, Jorge del Val, Henrik Holst, Joakim Bergdahl, Tom Olsson, Kristoffer
397"
REFERENCES,0.863905325443787,"Sjöö, and Magnus Nordin. Imitation learning with concurrent actions in 3d games. In IEEE
398"
REFERENCES,0.8658777120315582,"Conference on Computational Intelligence and Games, 2018.
399"
REFERENCES,0.8678500986193294,"Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In International
400"
REFERENCES,0.8698224852071006,"Joint Conference on Artificial Intelligence, 2018.
401"
REFERENCES,0.8717948717948718,"Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
402"
REFERENCES,0.873767258382643,"prediction to no-regret online learning. In International Conference on Artificial Intelligence and
403"
REFERENCES,0.8757396449704142,"Statistics, 2011.
404"
REFERENCES,0.8777120315581854,"Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation.
405"
REFERENCES,0.8796844181459567,"ICML, 2019.
406"
REFERENCES,0.8816568047337278,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
407"
REFERENCES,0.883629191321499,"Aaron Courville, and Yoshua Bengio.
Generative adversarial nets.
In Advances in Neural
408"
REFERENCES,0.8856015779092702,"Information Processing Systems, 2014.
409"
REFERENCES,0.8875739644970414,"Minshuo Chen, Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, and Tuo Zhao.
410"
REFERENCES,0.8895463510848126,"On computation and generalization of generative adversarial imitation learning. In International
411"
REFERENCES,0.8915187376725838,"Conference on Learning Representations, 2020.
412"
REFERENCES,0.893491124260355,"Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforce-
413"
REFERENCES,0.8954635108481263,"ment learning. In International Conference on Learning Representations, 2018.
414"
REFERENCES,0.8974358974358975,"Youngwoon Lee, Andrew Szot, Shao-Hua Sun, and Joseph J. Lim. Generalizable imitation learning
415"
REFERENCES,0.8994082840236687,"from observation via inferring goal proximity. In Neural Information Processing Systems, 2021.
416"
REFERENCES,0.9013806706114399,"Tim Pearce, Tabish Rashid, Anssi Kanervisto, David Bignell, Mingfei Sun, Raluca Georgescu,
417"
REFERENCES,0.903353057199211,"Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin.
418"
REFERENCES,0.9053254437869822,"Imitating human behaviour with diffusion models. In International Conference on Learning
419"
REFERENCES,0.9072978303747534,"Representations, 2023.
420"
REFERENCES,0.9092702169625246,"Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran
421"
REFERENCES,0.9112426035502958,"Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and
422"
REFERENCES,0.9132149901380671,"Systems, 2023.
423"
REFERENCES,0.9151873767258383,"Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation
424"
REFERENCES,0.9171597633136095,"learning using score-based diffusion policies. arXiv preprint arXiv:2304.02532, 2023.
425"
REFERENCES,0.9191321499013807,"Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon. Exploring the limitations of
426"
REFERENCES,0.9211045364891519,"behavior cloning for autonomous driving. In International Conference on Computer Vision, 2019.
427"
REFERENCES,0.9230769230769231,"Lingguang Wang, Carlos Fernandez, and Christoph Stiller. High-level decision making for automated
428"
REFERENCES,0.9250493096646942,"highway driving via behavior cloning. IEEE Transactions on Intelligent Vehicles, 2022.
429"
REFERENCES,0.9270216962524654,"Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Neural
430"
REFERENCES,0.9289940828402367,"Information Processing Systems, 2019.
431"
REFERENCES,0.9309664694280079,"Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint
432"
REFERENCES,0.9329388560157791,"arXiv:2101.03288, 2021.
433"
REFERENCES,0.9349112426035503,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
434"
REFERENCES,0.9368836291913215,"on Learning Representations, 2014.
435"
REFERENCES,0.9388560157790927,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
436"
REFERENCES,0.9408284023668639,"Conference on Machine Learning, 2015.
437"
REFERENCES,0.9428007889546351,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In
438"
REFERENCES,0.9447731755424064,"International Conference on Learning Representations, 2017.
439"
REFERENCES,0.9467455621301775,"Qitian Wu, Rui Gao, and Hongyuan Zha. Bridging explicit and implicit deep generative models via
440"
REFERENCES,0.9487179487179487,"neural stein estimators. In Neural Information Processing Systems, 2021.
441"
REFERENCES,0.9506903353057199,"Gabriel Loaiza-Ganem, Brendan Leigh Ross, Jesse C Cresswell, and Anthony L Caterini. Diagnosing
442"
REFERENCES,0.9526627218934911,"and fixing manifold overfitting in deep generative models. Transactions on Machine Learning
443"
REFERENCES,0.9546351084812623,"Research, 2022.
444"
REFERENCES,0.9566074950690335,"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
445"
REFERENCES,0.9585798816568047,"learning using nonequilibrium thermodynamics. In International Conference on Machine Learning.
446"
REFERENCES,0.960552268244576,"PMLR, 2015.
447"
REFERENCES,0.9625246548323472,"Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
448"
REFERENCES,0.9644970414201184,"In International Conference on Machine Learning, 2021.
449"
REFERENCES,0.9664694280078896,"Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Neural
450"
REFERENCES,0.9684418145956607,"Information Processing Systems, 2021.
451"
REFERENCES,0.9704142011834319,"A Jain J Ho. Denoising diffusion probabilistic models. In Advances in Neural Information Processing
452"
REFERENCES,0.9723865877712031,"Systems, 2020.
453"
REFERENCES,0.9743589743589743,"Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
454"
REFERENCES,0.9763313609467456,"data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
455"
REFERENCES,0.9783037475345168,"Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
456"
REFERENCES,0.980276134122288,"Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learn-
457"
REFERENCES,0.9822485207100592,"ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
458"
REFERENCES,0.9842209072978304,"2018.
459"
REFERENCES,0.9861932938856016,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy max-
460"
REFERENCES,0.9881656804733728,"imum entropy deep reinforcement learning with a stochastic actor. In Proceedings of International
461"
REFERENCES,0.9901380670611439,"Conference on Machine Learning (ICML), 2018.
462"
REFERENCES,0.9921104536489151,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
463"
REFERENCES,0.9940828402366864,"Wojciech Zaremba. Openai gym, 2016.
464"
REFERENCES,0.9960552268244576,"Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://github.
465"
REFERENCES,0.9980276134122288,"com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
466"
