Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001455604075691412,"In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). As observed previously, long after per-
formance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the structure in
the gradient noise, and the Hessian matrix at the end of training that explains this
anomalous diffusion. To build this understanding, we ﬁrst derive a continuous-
time model for SGD with ﬁnite learning rates and batch sizes as an underdamped
Langevin equation. We study this equation in the setting of linear regression,
where we can derive exact, analytic expressions for the phase space dynamics of
the parameters and their instantaneous velocities from initialization to stationar-
ity. Using the Fokker-Planck equation, we show that the key ingredient driving
these dynamics is not the original training loss, but rather the combination of a
modiﬁed loss, which implicitly regularizes the velocity, and probability currents,
which cause oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on Im-
ageNet. Through the lens of statistical physics, we uncover a mechanistic origin
for the anomalous limiting dynamics of deep neural networks trained with SGD."
"INTRODUCTION
DEEP NEURAL NETWORKS HAVE DEMONSTRATED REMARKABLE GENERALIZATION ACROSS A VARIETY OF DATASETS AND",0.002911208151382824,"1
INTRODUCTION
Deep neural networks have demonstrated remarkable generalization across a variety of datasets and
tasks. Essential to their success has been a collection of good practices on how to train these models
with stochastic gradient descent (SGD). Yet, despite their importance, these practices are mainly
based on heuristic arguments and trial and error search. Without a general theory connecting the
hyperparameters of optimization, the architecture of the network, and the geometry of the dataset,
theory-driven design of deep learning systems is impossible. Existing theoretical works studying
this interaction have leveraged the random structure of neural networks at initialization [1, 2, 3]
and in their inﬁnite width limits in order to study their dynamics [4, 5, 6, 7, 8]. Here we take a
different approach and study the training dynamics of pre-trained networks that are ready to be used
for inference. By leveraging the mathematical structures found at the end of training, we uncover an
intricate interaction between the hyperparameters of optimization, the structure in the gradient noise,
and the Hessian matrix that corroborates previously identiﬁed empirical behavior such as anomalous
limiting dynamics. Not only is understanding the limiting dynamics of SGD a critical stepping stone
to building a complete theory for the learning dynamics of neural networks, but recently there have
been a series of works demonstrating that the performance of pre-trained networks can be improved
through averaging and ensembling [9, 10, 11]. Combining empirical exploration and theoretical
tools from statistical physics, we identify and uncover a mechanistic explanation for the limiting
dynamics of neural networks trained with SGD."
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.004366812227074236,"2
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD
A network that has converged in performance will continue to move through parameter space [12,
13, 14, 15]. To demonstrate this behavior, we resume training of pre-trained convolutional networks
while tracking the network trajectory through parameter space. Let θ∗∈Rm be the parameter vector
for a pre-trained network and θk ∈Rm be the parameter vector after k steps of resumed training."
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.005822416302765648,Under review as a conference paper at ICLR 2022
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.00727802037845706,"We track two metrics of the training trajectory, namely the local parameter displacement δk between
consecutive steps, and the global displacement ∆k after k steps from the pre-trained initialization: …"
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.008733624454148471,VGG-16 with BN
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.010189228529839884,VGG-11 with BN Step
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.011644832605531296,ResNet-34
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.013100436681222707,ResNet-18
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.01455604075691412,VGG-16
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.01601164483260553,c = 1.401
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.017467248908296942,c = 1.075
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.018922852983988356,c = 1.370
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.020378457059679767,c = 1.616 Step
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.021834061135371178,c=1.085
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.023289665211062592,"x104
1
2
3
4
0.5
Figure 1: Despite performance con-
vergence, the network continues to
move through parameter space. We
plot the squared Euclidean norm for the
local and global displacement (δk and
∆k) of ﬁve classic convolutional neu-
ral network architectures. The networks
are standard Pytorch models pre-trained
on ImageNet [16]. Their training is re-
sumed for 10 additional epochs.
We
show the global displacement on a log-
log plot where the slope of the least
squares line c is the exponent of the
power law ∥∆k∥2
2
∝
kc.
See ap-
pendix H for experimental details."
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.024745269286754003,"δk = θk −θk−1,
∆k = θk −θ∗.
(1)"
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.026200873362445413,"As shown in Fig. 1, neither of these differences converge
to zero across a variety of architectures, indicating that
despite performance convergence, the networks continue
to move through parameter space, both locally and glob-
ally. The squared norm of the local displacement ∥δk∥2
2
remains near a constant value, indicating the network
is essentially moving at a constant instantaneous speed.
This observation is quite similar to the “equilibrium"" phe-
nomenon or “constant angular update"" observed in Li
et al. [17] and Wan et al. [13] respectively. However, these
works only studied the displacement for parameters im-
mediately preceding a normalization layer. The constant
instantaneous speed behavior we observe is for all param-
eters in the model and is even present in models without
normalization layers."
DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD,0.027656477438136828,"While the squared norm of the local displacement is es-
sentially constant, the squared norm of the global dis-
placement ∥∆k∥2
2 is monotonically growing for all net-
works, implying even once trained, the network continues
to diverge from where it has been. Indeed Fig. 1 indicates
a power law relationship between global displacement
and number of steps, given by ∥∆k∥2
2 ∝kc. As we’ll
see in section 8, this relationship is indicative of anoma-
lous diffusion where c corresponds to the anomalous dif-
fusion exponent. Standard Brownian motion corresponds
to c = 1. Similar observation were made by Baity-Jesi
et al. [14] who noticed distinct phases of the training tra-
jectory evident in the dynamics of the global displace-
ment and Chen et al. [15] who found that the exponent
of diffusion changes through the course of training. A
parallel observation is given by Hoffer et al. [18] for the
beginning of training, where they measure the global dis-
placement from the initialization of an untrained network and observe a rate ∝log(k), a form of
ultra-slow diffusion. These empirical observations raise the natural questions, where is the network
moving to and why? To answer these questions we will build a diffusion based theory of SGD, study
these dynamics in the setting of linear regression, and use lessons learned in this fundamental setting
to understand the limiting dynamics of neural networks."
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.02911208151382824,"3
RELATED WORK
There is a long line of literature studying both theoretically and empirically the learning dynamics
of deep neural networks trained with SGD. Our analysis and experiments build upon this literature."
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.03056768558951965,"Continuous models for SGD. Many works consider how to improve the classic gradient ﬂow model
for SGD to more realistically reﬂect momentum [19], discretization due to ﬁnite learning rates
[20, 21], and stochasticity due to random batches [22, 23]. One line of work has studied the dynam-
ics of networks in their inﬁnite width limits through dynamical mean ﬁeld theory [24, 25, 26, 27],
while a different approach has used stochastic differential equations (SDEs) to model SGD directly,
the approach we take in this work. However, recently, the validity of this approach has been ques-
tioned. The main argument, as nicely explained in Yaida [28], is that most SDE approximations
simultaneously assume that ∆t →0+, while maintaining that the learning rate η = ∆t is ﬁnite.
The works Simsekli et al. [29] and Li et al. [30] have questioned the correctness of the using the
central limit theorem (CLT) to model the gradient noise as Gaussian, arguing respectively that the
heavy-tailed structure in the gradient noise and the weak dependence between batches leads the
CLT to break down. In our work, we maintain the CLT assumption holds, which we discuss fur-"
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.03202328966521106,Under review as a conference paper at ICLR 2022
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.033478893740902474,"ther in appendix A, but importantly we avoid the pitfalls of many previous SDE approximations by
simultaneously modeling the effect of ﬁnite learning rates and stochasticity."
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.034934497816593885,"Limiting dynamics. A series of works have applied SDE models of SGD to study the limiting
dynamics of neural networks. In the seminal work by Mandt et al. [31], the limiting dynamics were
modeled with a multivariate Ornstein-Uhlenbeck process by combining a ﬁrst-order SDE model for
SGD with assumptions on the geometry of the loss and covariance matrix for the gradient noise.
This analysis was extended by Jastrz˛ebski et al. [12] through additional assumptions on the covari-
ance matrix to gain tractable insights and applied by Ali et al. [32] to the simpler setting of linear
regression, which has a quadratic loss. A different approach was taken by Chaudhari and Soatto
[33], which did not formulate the dynamics as an OU process, nor assume directly a structure on the
loss or gradient noise. Rather, this analysis studied the same ﬁrst-order SDE via the Fokker-Planck
equation to propose the existence of a modiﬁed loss and probability currents driving the limiting
dynamics, but did not provide explicit expressions. Our analysis deepens and combines ideas from
all these works, where our key insight is to lift the dynamics into phase space. By studying the
dynamics of the parameters and their velocities, and by applying the analysis ﬁrst in the setting of
linear regression where assumptions are provably true, we are able to identify analytic expressions
and explicit insights which lead to concrete predictions and testable hypothesis."
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.036390101892285295,"Stationary dynamics. A different line of work avoids modeling the limiting dynamics of SGD
with an SDE and instead chooses to leverage the property of stationarity. These works [28, 34,
35, 36] assume that eventually the probability distribution governing the model parameters reaches
stationarity such that the discrete SGD process is simply sampling from this distribution. Yaida [28]
used this approach to derive ﬂuctuation-dissipation relations that link measurable quantities of the
parameters and hyperparameters of SGD. Liu et al. [35] used this approach to derive properties for
the stationary distribution of SGD with a quadratic loss. Similar to our analysis, this work identiﬁes
that the stationary distribution for the parameters reﬂects a modiﬁed loss function dependent on
the relationship between the covariance matrix of the gradient noise and the Hessian matrix for the
original loss."
"RELATED WORK
THERE IS A LONG LINE OF LITERATURE STUDYING BOTH THEORETICALLY AND EMPIRICALLY THE LEARNING DYNAMICS",0.03784570596797671,"Empirical exploration. Another set of works analyzing the limiting dynamics of SGD has taken
a purely empirical approach. Building on the intuition that ﬂat minima generalize better than sharp
minima, Keskar et al. [37] demonstrated empirically that the hyperparameters of optimization in-
ﬂuence the eigenvalue spectrum of the Hessian matrix at the end of training. Many subsequent
works have studied the Hessian eigenspectrum during and at the end of training. Jastrz˛ebski et al.
[38], Cohen et al. [39] studied the dynamics of the top eigenvalues during training. Sagun et al.
[40], Papyan [41], Ghorbani et al. [42] demonstrated the spectrum has a bulk of values near zero
plus a small number of larger outliers. Gur-Ari et al. [43] demonstrated that the learning dynamics
are constrained to the subspace spanned by the top eigenvectors, but found no special properties of
the dynamics within this subspace. In our work we also determine that the top eigensubspace of the
Hessian plays a crucial role in the limiting dynamics and by projecting the dynamics into this sub-
space in phase space, we see that the motion is not random, but consists of incoherent oscillations
leading to anomalous diffusion."
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.039301310043668124,"4
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION
Following the route of previous works [31, 12, 33] studying the limiting dynamics of neural
networks, we ﬁrst seek to model SGD as a continuous stochastic process. We consider a net-
work parameterized by θ ∈Rm, a training dataset {x1, . . . , xN} of size N, and a training loss
L(θ) =
1
N
PN
i=1 ℓ(θ, xi) with corresponding gradient g(θ) = ∂L"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.040756914119359534,"∂θ . The state of the network at the
kth step of training is deﬁned by the position vector θk and velocity vector vk of the same dimension.
The gradient descent update with learning rate η, momentum β, and weight decayλ is given by
vk+1 = βvk −g(θk) −λθk,
θk+1 = θk + ηvk+1,
(2)
where we initialize the network such that v0 = 0 and θ0 is the parameter initialization. In order to
understand the dynamics of the network through position and velocity space, which we will refer to
as phase space, we express these discrete recursive equations as the discretization of some unknown
ordinary differential equation (ODE), sometimes referred to as a modiﬁed equation as in [44, 20].
While this ODE models the gradient descent process even at ﬁnite learning rates, it fails to account
for the stochasticity introduced by choosing a random batch B of size S drawn uniformly from the
set of N training points. This sampling yields the stochastic gradient gB(θ) = 1 S
P"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.042212518195050945,"i∈B ∇ℓ(θ, xi).
To model this effect, we make the following assumption:"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.043668122270742356,Under review as a conference paper at ICLR 2022
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.04512372634643377,"Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such
that gB(θ) −g(θ) is a Gaussian random variable with mean 0 and covariance 1"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.046579330422125184,S Σ(θ).
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.048034934497816595,"Incorporating this model of stochastic gradients into the previous ﬁnite difference equation and
applying the stochastic counterparts to Euler discretizations, results in the standard drift-diffusion
stochastic differential equation (SDE), referred to as an underdamped Langevin equation,"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.049490538573508006,"d

θ
v"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.050946142649199416,"
=

v
−
2
η(1+β) (g(θ) + λθ + (1 −β)v)"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.05240174672489083,"
dt +
0
0
0
2
√ηS(1+β)
p Σ(θ)"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.053857350800582245,"
dWt,
(3)"
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.055312954876273655,"where Wt is a standard Wiener process. This is the continuous model we will study in this work:
Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the
trajectory of the network driven by SGD through phase space such that θ(ηk) ≈θk and v(ηk) ≈vk."
MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION,0.056768558951965066,See appendix A for further discussion on the nuances of modeling SGD with an SDE.
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.05822416302765648,"5
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS
Equipped with a model for SGD, we seek to understand its dynamics in the fundamental setting of
linear regression, one of the few cases where we have a complete model for the interaction of the
dataset, architecture, and optimizer. Let X ∈RN×d be the input data, Y ∈RN be the output labels,
and θ ∈Rd be our vector of regression coefﬁcients. The least squares loss is the convex quadratic
loss L(θ) =
1
2N ∥Y −Xθ∥2 with gradient g(θ) = Hθ−b, where H = X⊺X"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.05967976710334789,"N
and b = X⊺Y"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.0611353711790393,"N . Plugging
this expression for the gradient into the underdamped Langevin equation (3), and rearranging terms,
results in the multivariate Ornstein-Uhlenbeck (OU) process,"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06259097525473072,"d

θt
vt"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06404657933042213,"
= −

0
−I
2
η(1+β)(H + λI)
2(1−β)
η(1+β)I "
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06550218340611354,"|
{z
}
A"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06695778748180495,"
θt
vt"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06841339155749636,"
−

µ
0"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.06986899563318777,"
dt +
√"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07132459970887918,"2κ−1
v
u
u
u
t"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07278020378457059,"0
0
0
2(1−β)
η(1+β)Σ(θ) "
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07423580786026202,"|
{z
}
D dWt, (4) Step"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07569141193595343,Velocity
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07714701601164484,Position Step
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.07860262008733625,Velocity
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08005822416302766,Position
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08151382823871907,Eq. (7)
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08296943231441048,"Figure 2:
Oscillatory dynamics in
linear regression.
We train a linear
network to perform regression on the
CIFAR-10 dataset by using an MSE
loss on the one-hot encoding of the la-
bels.
We compute the hessian of the
loss, as well as its top eigenvectors.
The position and velocity trajectories
are projected onto the ﬁrst eigenvector
of the hessian and visualized in black.
The theoretically derived mean, equa-
tion (5), is shown in red. The top and
bottom panels demonstrate the effect of
varying momentum on the oscillation
mode."
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08442503639010189,"where A and D are the drift and diffusion matrices re-
spectively, κ = S(1 −β2) is an inverse temperature con-
stant, and µ = (H + λI)−1b is the ridge regression solu-
tion. The solution to an OU process is a Gaussian process.
By solving for the temporal dynamics of the ﬁrst and sec-
ond moments of the process, we can obtain an analytic
expression for the trajectory at any time t. In particular,
we can decompose the trajectory as the sum of a deter-
ministic and stochastic component deﬁned by the ﬁrst and
second moments respectively."
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.0858806404657933,"Deterministic component. Using the form of A we can
decompose the expectation as a sum of harmonic oscilla-
tors in the eigenbasis {q1, . . . , qm} of the Hessian,"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08733624454148471,"E

θt
vt"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.08879184861717612,"
=

µ
0 
+ m
X i=1"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09024745269286755,"
ai(t)

qi
0"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09170305676855896,"
+ bi(t)

0
qi"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09315866084425037,"
. (5)"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09461426491994178,"Here the coefﬁcients ai(t) and bi(t) depend on the op-
timization hyperparameters η, β, λ, S and the respective
eigenvalue of the Hessian ρi as further explained in
appendix F. We verify this expression nearly perfectly
matches empirics on complex datasets under various hy-
perparameter settings as shown in Fig. 2."
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09606986899563319,"Stochastic component. The cross-covariance of the pro-
cess between two points in time t ≤s, is"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.0975254730713246,"Cov

θt
vt"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.09898107714701601,"
,

θs
vs"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.10043668122270742,"
=κ−1
B−e−AtBe−A⊺t
eA⊺(t−s), (6)"
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.10189228529839883,Under review as a conference paper at ICLR 2022
LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS,0.10334788937409024,"where B solves the Lyapunov equation AB + BA⊺= 2D. In order to gain analytic expressions for
B in terms of the optimization hyperparameters, eigendecomposition of the Hessian, and covariance
of the gradient noise, we must introduce the following assumption:
Assumption 3 (Simultaneously Diagonalizable). We assume the covariance of the gradient noise is
spatially independent Σ(θ) = Σ and commutes with the Hessian HΣ = ΣH, therefore sharing a
common eigenbasis."
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.10480349344978165,"6
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION
The OU process is unique in that it is one of the few SDEs which we can solve exactly. As shown in
section 5, we were able to derive exact expressions for the dynamics of linear regression trained with
SGD from initialization to stationarity by simply solving for the ﬁrst and second moments. While the
expression for the ﬁrst moment provides an understanding of the intricate oscillatory relationship in
the deterministic component of the process, the second moment, driving the stochastic component,
is much more opaque. An alternative route to solving the OU process that potentially provides more
insight is the Fokker-Planck equation."
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.10625909752547306,"The Fokker-Planck (FP) equation is a PDE describing the time evolution for the probability distribu-
tion of a particle governed by Langevin dynamics. For an arbitrary potential Φ and diffusion matrix
D, the Fokker-Planck equation (under an Itô integration prescription) is
∂tp = ∇·
 
∇Φp + ∇·
 
κ−1Dp
"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.10771470160116449,"|
{z
}
−J ,
(7)"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.1091703056768559,"where p represents the time-dependent probability distribution, and J is a vector ﬁeld commonly
referred to as the probability current. The FP equation is especially useful for explicitly solving for
the stationary solution, assuming one exists, of the Langevin dynamics. The stationary solution pss
by deﬁnition obeys ∂tpss = 0 or equivalently ∇· Jss = 0. From this second deﬁnition we see that
there are two distinct settings of stationarity: detailed balance when Jss = 0 everywhere, or broken
detailed balance when ∇· Jss = 0 and Jss ̸= 0."
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11062590975254731,"For a general OU process, the potential is a convex quadratic function Φ(x) = x⊺Ax deﬁned by
the drift matrix A. When the diffusion matrix is isotropic (D ∝I) and spatially independent (∇·
D = 0) the resulting stationary solution is a Gibbs distribution pss(x) ∝e−κΦ(x) determined
by the original loss Φ(x) and is in detailed balance. Lesser known properties of the OU process
arise when the diffusion matrix is anisotropic or spatially dependent [45, 46]. In this setting the
solution is still a Gaussian process, but the stationary solution, if it exists, is no longer deﬁned by
the Gibbs distribution of the original loss Φ(x), but actually a modiﬁed loss Ψ(x). Furthermore,
the stationary solution may be in broken detailed balance leading to a non-zero probability current
Jss(x). Depending on the relationship between the drift matrix A and the diffusion matrix D the
resulting dynamics of the OU process can have very nontrivial behavior."
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11208151382823872,"In the setting of linear regression, anisotropy in the data distribution will lead to anisotropy in the
gradient noise and thus an anisotropic diffusion matrix. This implies that for most datasets we should
expect that the SGD trajectory is not driven by the original least squares loss, but by a modiﬁed loss
and converges to a stationary solution with broken detailed balance, as predicted by Chaudhari and
Soatto [33]. Using the explicit expressions for the drift A and diffusion D matrices we can compute
analytically the modiﬁed loss and stationary probability current,"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11353711790393013,"Ψ(θ, v) =

θ
v"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11499272197962154,"
−

µ
0 ⊺U 2"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11644832605531295," 
θ
v"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11790393013100436,"
−

µ
0"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.11935953420669577,"
,
Jss(θ, v) = −QU

θ
v"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12081513828238719,"
−

µ
0"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.1222707423580786,"
pss,
(8)"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12372634643377002,"where Q is a skew-symmetric matrix and U is a positive deﬁnite matrix deﬁned as,"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12518195050946143,"Q =

0
−Σ(θ)
Σ(θ)
0"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12663755458515283,"
,
U =

2
η(1+β)Σ(θ)−1 (H + λI)
0
0
Σ(θ)−1"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12809315866084425,"
.
(9)"
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.12954876273653565,"These new fundamental matrices, Q and U, relate to the original drift A and diffusion D matrices
through the unique decomposition A = (D + Q)U, introduced by Ao [47] and Kwon et al. [48].
Using this decomposition we can easily show that B = U −1 solves the Lyapunov equation and
indeed the stationary solution pss is the Gibbs distribution deﬁned by the modiﬁed loss Ψ(θ, v)
in equation (8). Further, the stationary cross-covariance solved in section 5 reﬂects the oscillatory
dynamics introduced by the stationary probability currents Jss(θ, v) in equation (8). Taken together,
we gain the intuition that the limiting dynamics of SGD in linear regression are driven by a modiﬁed
loss subject to oscillatory probability currents."
UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION,0.13100436681222707,Under review as a conference paper at ICLR 2022
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1324599708879185,"7
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1339155749636099,"Does the theory derived in the linear regression setting (sections 5, 6) help explain the empirical
phenomena observed in the non-linear setting of deep neural networks (section 2)? In order for the
theory built in the previous sections to apply to the limiting dynamics of neural networks, we must
introduce simplifying assumptions on the loss landscape and gradient noise at the end of training:"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.13537117903930132,"Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network
can be approximated by the quadratic loss L(θ) = (θ −µ)⊺  H"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.13682678311499272,"2

(θ −µ), where H ⪰0 is the
training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.13828238719068414,"Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-
tional to the Hessian of the quadratic loss Σ(θ) = σ2H where σ ∈R+ is some unknown scalar."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.13973799126637554,"Under these simpliﬁcations, then the expressions derived in the linear regression setting would apply
to the limiting dynamics of deep neural networks and depend only on quantities that we can easily
estimate empirically. Of course, these simpliﬁcations are quite strong, but without arguing their the-
oretical validity, we can empirically test their qualitative implications: (1) a modiﬁed isotropic loss
driving the limiting dynamics through parameter space, (2) implicit regularization of the velocity
trajectory, and (3) oscillatory phase space dynamics determined by the Hessian eigen-structure."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14119359534206696,"Modiﬁed loss. As discussed in section 6, due to the anisotropy of the diffusion matrix, the loss
landscape driving the dynamics at the end of training is not the original training loss L(θ), but a
modiﬁed loss Ψ(θ, v) in phase space. As shown in equation (8), the modiﬁed loss decouples into a
term Ψθ that only depends on the parameters θ and a term Ψv that only depends on the velocities v.
Under assumption 5, the parameter dependent component is proportional to the convex quadratic,"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14264919941775836,"Ψθ ∝(θ −µ)⊺
H−1(H + λI)"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14410480349344978,η(1 + β)
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14556040756914118,"
(θ −µ) .
(10)"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1470160116448326,"This quadratic function has the same mean µ as the training loss, but a different curvature. Using
this expression, notice that when λ ≈0, the modiﬁed loss is isotropic in the column space of H,
regardless of what the nonzero eigenspectrum of H is. This striking prediction suggests that no
matter how anisotropic the original training loss – as reﬂected by poor conditioning of the Hessian
eigenspectrum – the training trajectory of the network will behave isotropically, since it is driven not
by the original anisotropic loss, but a modiﬁed isotropic loss."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14847161572052403,"We test this prediction by studying the limiting dynamics of a pre-trained ResNet-18 model with
batch normalization that we continue to train on ImageNet according to the last setting of its hyper-
parameters [49]. Let θ∗represent the initial pre-trained parameters of the network, depicted with the
white dot in ﬁgures 3 and 4."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.14992721979621543,"Training loss
Test loss
Modified loss
Ltrain
Ltest
Figure 3: The training trajectory behaves isotropically, regardless of the training loss. We
resume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajec-
tory (black line) onto the space spanned by the eigenvectors of its pre-trained Hessian q1, q30 (with
eigenvalue ratio ρ1/ρ30 ≃6). We sample the training and test loss within the same 2D subspace
and visualize them as a heatmap in the left and center panels respectively. We visualize the mod-
iﬁed loss computed from the eigenvalues (ρ1, ρ30) and optimization hyperparameters according to
equation (10) in the right plot. Note the projected trajectory is isotropic, despite the anisotropy of
the training and test loss."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.15138282387190685,Under review as a conference paper at ICLR 2022
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.15283842794759825,"Figure 4: Implicit velocity regulariza-
tion deﬁned by the inverse Hessian.
The shape of the projected velocity tra-
jectory closely resembles the contours
of the modiﬁed loss Ψv."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.15429403202328967,"We estimate1 the top thirty eigenvectors q1, . . . , q30 of the
Hessian matrix H∗evaluated at θ∗and project the limit-
ing trajectory for the parameters onto the plane spanned
by the top q1 and bottom q30 eigenvectors to maximize the
illustrated anisotropy with our estimates. We sample the
train and test loss in this subspace for a region around the
projected trajectory. Additionally, using the hyperparam-
eters of the optimization, the eigenvalues ρ1 and ρ30, and
the estimate for the mean µ = θ∗−H−1
∗g∗(g∗is the gra-
dient evaluated at θ∗), we also sample from the modiﬁed
loss equation (10) in the same region. Figure 3 shows the
projected parameter trajectory on the sampled train, test
and modiﬁed losses. Contour lines of both the train and
test loss exhibit anisotropic structure, with sharper curva-
ture along eigenvector q1 compared to eigenvector q30, as
expected. However, as predicted, the trajectory appears
to cover both directions equally. This striking isotropy
of the trajectory within a highly anisotropic slice of the
loss landscape indicates qualitatively that the trajectory
evolves in a modiﬁed isotropic loss landscape. Step"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.15574963609898107,Velocity
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1572052401746725,Position Step
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1586608442503639,Velocity
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16011644832605532,Position
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1615720524017467,"Figure 5: Phase space oscillations are
determined by the eigendecomposi-
tion of the Hessian. We visualize the
projected position and velocity trajecto-
ries in phase space. The top and bottom
panels show the projections onto q1 and
q30 respectively. Oscillations at differ-
ent rates are distinguishable for the dif-
ferent eigenvectors and were veriﬁed by
comparing the dominant frequencies in
the fast Fourier transform of the trajec-
tories."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16302765647743814,"Implicit velocity regularization. A second qualitative
prediction of the theory is that the velocity is regulated
by the inverse Hessian of the training loss. Of course
there are no explicit terms in either the train or test losses
that depend on the velocity. Yet, the modiﬁed loss con-
tains a component, Ψv ∝v⊺H−1v, that only depends on
the velocities This additional term can be understood as a
form of implicit regularization on the velocity trajectory.
Indeed, when we project the velocity trajectory onto the
plane spanned by the q1 and q30 eigenvectors, as shown
in Fig. 4, we see that the trajectory closely resembles the
curvature of the inverse Hessian H−1. The modiﬁed loss
is effectively penalizing SGD for moving in eigenvectors
of the Hessian with small eigenvalues. A similar quali-
tative effect was recently proposed by Barrett and Dherin
[21] as a consequence of the discretization error due to
ﬁnite learning rates."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16448326055312956,"Phase space oscillations. A ﬁnal implication of the the-
ory is that at stationarity the network is in broken detailed
balance leading to non-zero probability currents ﬂowing
through phase space:"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16593886462882096,"Jss(θ, v) =

v
−
2
η(1+β) (H + λI) (θ −µ)"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16739446870451238,"
pss.
(11)"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.16885007278020378,"These probability currents encourage oscillatory dynamics in the phase space planes characterized
by the eigenvectors of the Hessian, at rates proportional to their eigenvalues. We consider the same
projected trajectory of the ResNet-18 model visualized in ﬁgures 3 and 4, but plot the trajectory in
phase space for the two eigenvectors q1 and q30 separately. Shown in Fig. 5, we see that both trajec-
tories look like noisy clockwise rotations. Qualitatively, the trajectories for the different eigenvectors
appear to be rotating at different rates."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1703056768558952,"The integral curves of the stationary probability current are one-dimensional paths conﬁned to level
sets of the modiﬁed loss. These paths might cross themselves, in which case they are limit cycles, or
they could cover the entire surface of the level sets, in which case they are space-ﬁlling curves. This
distinction depends on the relative frequencies of the oscillations, as determined by the pairwise"
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.1717612809315866,"1To estimate the eigenvectors of H∗we use subspace iteration, and limit ourselves to 30 eigenvectors to
constrain computation time. See appendix H for details."
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.17321688500727803,Under review as a conference paper at ICLR 2022
EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING,0.17467248908296942,"ratios of the eigenvalues of the Hessian. For real-world datasets, with a large spectrum of incom-
mensurate frequencies, we expect to be in the latter setting, thus contradicting the suggestion that
SGD in deep networks converges to limit cycles, as claimed in Chaudhari and Soatto [33]."
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.17612809315866085,"8
UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS
Taken together the empirical results shown in section 7 indicate that many of the same qualitative
behaviors of SGD identiﬁed theoretically for linear regression are evident in the limiting dynamics
of neural networks. Can this theory quantitatively explain the results we identiﬁed in section 2?"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.17758369723435224,"Constant instantaneous speed. As noted in section 2, we observed that at the end of training,
across various architectures, the squared norm of the local displacement ∥δt∥2
2 remains essentially
constant. Assuming the limiting dynamics are described by the stationary solution the expectation
of the local displacement is"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.17903930131004367,"Ess

∥δt∥2
=
η2"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.1804949053857351,"S(1 −β2)σ2tr (H) ,
(12)"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.1819505094614265,"as derived in appendix G. We cannot test this prediction directly as we do not know σ2 and comput-
ing tr(H) is computationally prohibitive. However, we can estimate σ2tr(H) by resuming training
for a model, measuring the average ∥δt∥2, and then inverting equation (12). Using this single es-
timate, we ﬁnd that for a sweep of models with varying hyperparameters, equation (12) accurately
predicts their instantaneous speed. Indeed, Fig. 6 shows an exact match between the empirics and
theory, which strongly suggests that despite changing hyperparameters at the end of training, the
model remains in the same quadratic basin."
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.18340611353711792,"Learning rate
Batch size
Momentum"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.1848617176128093,Diffusion c
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.18631732168850074,Eq. (11)
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.18777292576419213,c = 1.0
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.18922852983988356,"Figure 6: Understanding how the hyperparameters of optimization inﬂuence the diffusion. We
resume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batch
sizes, and momentum coefﬁcients, tracking ∥δt∥2 and ∥∆t∥2. Starting from the default hyperparam-
eters, namely η = 1e −4, S = 256, and β = 0.9, we vary each one while keeping the others ﬁxed.
The top row shows the measured ∥δt∥2 in color, with the default hyperparameter setting highlighted
in black. The dotted line depicts the predicted value from equation (12). The bottom row shows the
estimated exponent c found by ﬁtting a line to the ∥∆t∥2 trajectories on a log-log plot. The dotted
line shows c = 1, corresponding to standard diffusion."
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.19068413391557495,"Exponent of anomalous diffusion. The expected value for the global displacement under the sta-
tionary solution can also be analytically expressed in terms of the optimization hyperparameters and
the eigendecomposition of the Hessian as,"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.19213973799126638,"Ess

∥∆t∥2
=
η2"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.19359534206695778,S(1 −β2)σ2
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.1950509461426492,"tr (H) t + 2t t
X k=1"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.1965065502183406,"
1 −k t  m
X"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.19796215429403202,"l=1
ρlCl(k) !"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.19941775836972345,",
(13)"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.20087336244541484,"where Cl(k) is a trigonometric function describing the velocity of a harmonic oscillator with damp-
ing ratio ζl = (1 −β)/
p"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.20232896652110627,"2η(1 + β) (pl + λ), see appendix G for details. As shown empirically in
section 2, the squared norm ∥∆t∥2 monotonically increases as a power law in the number of steps,
suggesting its expectation is proportional to tc for some unknown, constant c. The exponent c de-
termines the regime of diffusion for the process. When c = 1, the process corresponds to standard
Brownian diffusion. For c > 1 or c < 1 the process corresponds to anomalous super-diffusion
or sub-diffusion respectively. Unfortunately, it is not immediately clear how to extract the explicit"
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.20378457059679767,Under review as a conference paper at ICLR 2022
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.2052401746724891,"exponent c from equation (13). However, by exploring the functional form of Cl(k) and its relation-
ship to the hyperparameters of optimization through the damping ratio ζl, we can determine overall
trends in the diffusion exponent c."
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.2066957787481805,"Akin to how the exponent c determines the regime of diffusion, the damping ratio ζl determines the
regime for the harmonic oscillator describing the stationary velocity-velocity correlation in the lth
eigenvector of the Hessian. When ζl = 1, the oscillator is critically damped implying the velocity
correlations converge to zero as quickly as possible. In the extreme setting of Cl(k) = 0 for all
l, k, then equation (13) simpliﬁes to standard Brownian diffusion, Ess

∥∆t∥2
∝t. When ζl > 1,
the oscillator is overdamped implying the velocity correlations dampen slowly and remain positive
even over long temporal lags. Such long lasting temporal correlations in velocity lead to faster
global displacement. Indeed, in the extreme setting of Cl(k) = 1 for all l, k, then equation (13)
simpliﬁes to a form of anomalous super-diffusion, Ess

∥∆t∥2
∝t2. When ζl < 1, the oscillator is
underdamped implying the velocity correlations will oscillate quickly between positive and negative
values. Indeed, the only way equation (13) could describe anomalous sub-diffusion is if Cl(k) took
on negative values for certain l, k."
"UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS",0.2081513828238719,"Using the same sweep of models described previously, we can empirically conﬁrm that the opti-
mization hyperparameters each inﬂuence the diffusion exponent c. As shown in Fig. 6, the learning
rate, batch size, and momentum can each independently drive the exponent c into different regimes
of anomalous diffusion. Notice how the inﬂuence of the learning rate and momentum on the dif-
fusion exponent c closely resembles their respective inﬂuences on the damping ratio ζl. Interest-
ingly, a larger learning rate leads to underdamped oscillations, and the resultant temporal velocities’
anti-correlations reduce the exponent of anomalous diffusion. Thus contrary to intuition, a larger
learning rate actually leads to slower global transport in parameter space. The batch size on the other
hand, has no inﬂuence on the damping ratio, but leads to an interesting, non-monotonic inﬂuence
on the diffusion exponent. Overall, the hyperparameters of optimization and eigenspectrum of the
Hessian all conspire to govern the degree of anomalous diffusion at the end of training."
DISCUSSION,0.2096069868995633,"9
DISCUSSION
Through combined empirics and theory based on statistical physics, we uncovered an intricate in-
terplay between the optimization hyperparameters, structure in the gradient noise, and the Hessian
matrix at the end of training."
DISCUSSION,0.21106259097525473,"Signiﬁcance. The signiﬁcance of our work lies in (1) the identiﬁcation/veriﬁcation of multiple
empirical phenomena (constant instantaneous speed, anomalous diffusion in global displacement,
isotropic parameter exploration despite anisotopic loss, velocity regularization, and slower global
parameter exploration with faster learning rates) present in the limiting dynamics of deep neural
networks, (2) the emphasis on studying the dynamics in velocity space in addition to parameter
space, and (3) concrete quantitative as well as qualitative predictions of an SDE based theory that
we empirically veriﬁed in deep networks trained on large scale datasets (indeed some of the above
nontrivial phenomena were predictions of this theory). Of course, these contributions directly build
upon a series of related works studying the immensely complex process of deep learning. To this
end, we further clarify the originality of our contributions with respect to some relevant works."
DISCUSSION,0.21251819505094613,"Originality. The empirical phenomena we present provide novel insight with respect to the works
of Wan et al. [13], Hoffer et al. [18], and Chen et al. [15]. We observe that all parameters in the
network (not just those with scale symmetry) move at a constant instantaneous speed at the end
of training and diffuse anomalously at rates determined by the hyperparameters of optimization.
In contrast to the work by Liu et al. [35], we modeled the entire SGD process as an OU process
which allows us to provide insight into the transient dynamics and identify oscillations in parameter
and velocity space. We build on the theoretical framework used by Chaudhari and Soatto [33] and
provide explicit expressions for the limiting dynamics in the simpliﬁed linear regression setting and
conclude that the oscillations present in the limiting dynamics are more likely to be space-ﬁlling
curves (and not limit cycles) in deep learning due to many incommensurate oscillations."
DISCUSSION,0.21397379912663755,"Overall, by identifying key phenomena, explaining them in a simpler setting, deriving predictions
of new phenomena, and providing evidence for these predictions at scale, we are furthering the
scientiﬁc study of deep learning. We hope our newly derived understanding of the limiting dynamics
of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and
momentum, can serve as a basis for future work that can turn these insights into algorithmic gains."
DISCUSSION,0.21542940320232898,Under review as a conference paper at ICLR 2022
REFERENCES,0.21688500727802038,REFERENCES
REFERENCES,0.2183406113537118,"[1] Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks,
pages 29–53. Springer, 1996."
REFERENCES,0.2197962154294032,"[2] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep infor-
mation propagation. arXiv preprint arXiv:1611.01232, 2016."
REFERENCES,0.22125181950509462,"[3] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein.
Deep neural networks as gaussian processes.
arXiv preprint
arXiv:1711.00165, 2017."
REFERENCES,0.22270742358078602,"[4] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.22416302765647744,"[5] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha
Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear
models under gradient descent. arXiv preprint arXiv:1902.06720, 2019."
REFERENCES,0.22561863173216884,"[6] Mei Song, Andrea Montanari, and P Nguyen. A mean ﬁeld view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, 115:E7665–E7671, 2018."
REFERENCES,0.22707423580786026,"[7] Grant M Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time
convergence and asymptotic error scaling of neural networks. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pages 7146–7155, 2018."
REFERENCES,0.22852983988355166,"[8] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018."
REFERENCES,0.22998544395924309,"[9] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wil-
son. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pages 8803–8812, 2018."
REFERENCES,0.2314410480349345,"[10] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018."
REFERENCES,0.2328966521106259,"[11] Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. arXiv preprint arXiv:1902.02476,
2019."
REFERENCES,0.23435225618631733,"[12] Stanisław Jastrz˛ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey.
Three factors inﬂuencing minima in sgd.
arXiv preprint
arXiv:1711.04623, 2017."
REFERENCES,0.23580786026200873,"[13] Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics of deep
neural networks with batch normalization and weight decay. arXiv preprint arXiv:2006.08419,
2020."
REFERENCES,0.23726346433770015,"[14] Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gérard Ben Arous, Chiara
Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep
neural networks versus glassy systems. In International Conference on Machine Learning,
pages 314–323. PMLR, 2018."
REFERENCES,0.23871906841339155,"[15] Guozhang Chen, Cheng Kevin Qu, and Pulin Gong. Anomalous diffusion dynamics of learning
in deep neural networks. arXiv preprint arXiv:2009.10588, 2020."
REFERENCES,0.24017467248908297,"[16] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. Neural Information Processing Systems Workshop, 2017."
REFERENCES,0.24163027656477437,"[17] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with tradi-
tional optimization analyses: The intrinsic learning rate. In Advances in Neural Information
Processing Systems, volume 33, pages 14544–14555. Curran Associates, Inc., 2020."
REFERENCES,0.2430858806404658,Under review as a conference paper at ICLR 2022
REFERENCES,0.2445414847161572,"[18] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc., 2017."
REFERENCES,0.24599708879184862,"[19] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks,
12(1):145–151, 1999."
REFERENCES,0.24745269286754004,"[20] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori
Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dy-
namics. arXiv preprint arXiv:2012.04728, 2020."
REFERENCES,0.24890829694323144,"[21] David GT Barrett and Benoit Dherin.
Implicit gradient regularization.
arXiv preprint
arXiv:2009.11162, 2020."
REFERENCES,0.25036390101892286,"[22] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pages 2101–2110.
PMLR, 2017."
REFERENCES,0.25181950509461426,"[23] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021."
REFERENCES,0.25327510917030566,"[24] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Dynam-
ical mean-ﬁeld theory for stochastic gradient descent in gaussian mixture classiﬁcation. arXiv
preprint arXiv:2006.06098, 2020."
REFERENCES,0.2547307132459971,"[25] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and gen-
eralization of shallow neural networks with quadratic activation functions.
arXiv preprint
arXiv:2006.15459, 2020."
REFERENCES,0.2561863173216885,"[26] Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová. Stochasticity helps to nav-
igate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval
problem. Machine Learning: Science and Technology, 2021."
REFERENCES,0.2576419213973799,"[27] Stefano Sarao Mannelli and Pierfrancesco Urbani. Just a momentum: Analytical study of
momentum-based acceleration methods in paradigmatic high-dimensional non-convex prob-
lems. arXiv preprint arXiv:2102.11755, 2021."
REFERENCES,0.2590975254730713,"[28] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018."
REFERENCES,0.26055312954876275,"[29] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic
gradient noise in deep neural networks. In International Conference on Machine Learning,
pages 5827–5837. PMLR, 2019."
REFERENCES,0.26200873362445415,"[30] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochas-
tic differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021."
REFERENCES,0.26346433770014555,"[31] Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gra-
dient algorithms. In International conference on machine learning, pages 354–363. PMLR,
2016."
REFERENCES,0.264919941775837,"[32] Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic
gradient ﬂow for least squares. In International Conference on Machine Learning, pages 233–
244. PMLR, 2020."
REFERENCES,0.2663755458515284,"[33] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer-
ence, converges to limit cycles for deep networks. In 2018 Information Theory and Applica-
tions Workshop (ITA), pages 1–10. IEEE, 2018."
REFERENCES,0.2678311499272198,"[34] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl,
Chris Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes?
insights from a noisy quadratic model. In Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.2692867540029112,Under review as a conference paper at ICLR 2022
REFERENCES,0.27074235807860264,"[35] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and ﬂuctuation of ﬁnite learning rate
stochastic gradient descent. In International Conference on Machine Learning, pages 7045–
7056. PMLR, 2021."
REFERENCES,0.27219796215429404,"[36] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. On minibatch noise: Discrete-
time sgd, overparametrization, and bayes. arXiv preprint arXiv:2102.05375, 2021."
REFERENCES,0.27365356622998543,"[37] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp min-
ima. arXiv preprint arXiv:1609.04836, 2016."
REFERENCES,0.27510917030567683,"[38] Stanisław Jastrz˛ebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and
Amos Storkey. On the relation between the sharpest directions of dnn loss and the sgd step
length. arXiv preprint arXiv:1807.05031, 2018."
REFERENCES,0.2765647743813683,"[39] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.
Gra-
dient descent on neural networks typically occurs at the edge of stability.
arXiv preprint
arXiv:2103.00065, 2021."
REFERENCES,0.2780203784570597,"[40] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017."
REFERENCES,0.2794759825327511,"[41] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training
and sample size. arXiv preprint arXiv:1811.07062, 2018."
REFERENCES,0.28093158660844253,"[42] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net opti-
mization via hessian eigenvalue density. In International Conference on Machine Learning,
pages 2232–2241. PMLR, 2019."
REFERENCES,0.2823871906841339,"[43] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
arXiv preprint arXiv:1812.04754, 2018."
REFERENCES,0.2838427947598253,"[44] Nikola B Kovachki and Andrew M Stuart. Analysis of momentum methods. arXiv preprint
arXiv:1906.04285, 2019."
REFERENCES,0.2852983988355167,"[45] Crispin W Gardiner et al. Handbook of stochastic methods, volume 3. springer Berlin, 1985."
REFERENCES,0.2867540029112082,"[46] Hannes Risken.
Fokker-planck equation.
In The Fokker-Planck Equation, pages 63–95.
Springer, 1996."
REFERENCES,0.28820960698689957,"[47] Ping Ao. Potential in stochastic differential equations: novel construction. Journal of physics
A: mathematical and general, 37(3):L25, 2004."
REFERENCES,0.28966521106259097,"[48] Chulan Kwon, Ping Ao, and David J Thouless. Structure of stochastic dynamics near ﬁxed
points. Proceedings of the National Academy of Sciences, 102(37):13029–13033, 2005."
REFERENCES,0.29112081513828236,"[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 770–778, 2016."
REFERENCES,0.2925764192139738,"[50] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier
Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-
overﬁtting puzzle. arXiv preprint arXiv:1801.00173, 2017."
REFERENCES,0.2940320232896652,"[51] Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning:
Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016."
REFERENCES,0.2954876273653566,"[52] Valentin Thomas, Fabian Pedregosa, Bart Merriënboer, Pierre-Antoine Manzagol, Yoshua
Bengio, and Nicolas Le Roux. On the interplay between noise and curvature and its effect
on optimization and generalization. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 3503–3513. PMLR, 2020."
REFERENCES,0.29694323144104806,"[53] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker–
planck equation. SIAM journal on mathematical analysis, 29(1):1–17, 1998."
REFERENCES,0.29839883551673946,"[54] Richard Jordan, David Kinderlehrer, and Felix Otto. Free energy and the fokker-planck equa-
tion. Physica D: Nonlinear Phenomena, 107(2-4):265–271, 1997."
REFERENCES,0.29985443959243085,Under review as a conference paper at ICLR 2022
REFERENCES,0.30131004366812225,"A
MODELING SGD WITH AN SDE"
REFERENCES,0.3027656477438137,"As explained in section 4, in order to understand the dynamics of stochastic gradient descent we
build a continuous Langevin equation in phase space modeling the effect of discrete updates and
stochastic batches simultaneously."
REFERENCES,0.3042212518195051,"A.1
MODELING DISCRETIZATION"
REFERENCES,0.3056768558951965,"To model the discretization effect we assume that the system of update equations (2) is actually a
discretization of some unknown ordinary differential equation. To uncover this ODE, we combine
the two update equations in (2), by incorporating a previous time step θk−1, and rearrange into
the form of a ﬁnite difference discretization, as shown in equation (??). Like all discretizations,
the Euler discretizations introduce error terms proportional to the step size, which in this case is
the learning rate η. Taylor expanding θk+1 and θk−1 around θk, its easy to show that both Euler
discretizations introduce a second-order error term proportional to η 2 ¨θ."
REFERENCES,0.3071324599708879,θk+1 −θk
REFERENCES,0.30858806404657935,"η
= ˙θ + η"
REFERENCES,0.31004366812227074,"2
¨θ + O(η2),
θk −θk−1"
REFERENCES,0.31149927219796214,"η
= ˙θ −η"
REFERENCES,0.3129548762736536,"2
¨θ + O(η2)."
REFERENCES,0.314410480349345,"Notice how the momentum coefﬁcient β ∈[0, 1] regulates the amount of backward Euler incorpo-
rated into the discretization. When β = 0, we remove all backward Euler discretization leaving
just the forward Euler discretization. When β = 1, we have equal amounts of backward Euler as
forward Euler resulting in a central second-order discretization2 as noticed in [19]."
REFERENCES,0.3158660844250364,"A.2
MODELING STOCHASTICITY"
REFERENCES,0.3173216885007278,"In order to model the effect of stochastic batches, we ﬁrst model a batch gradient with the following
assumption:"
REFERENCES,0.31877729257641924,"Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that
gB(θ) −g(θ) is a Gaussian random variable with mean 0 and covariance 1"
REFERENCES,0.32023289665211063,S Σ(θ).
REFERENCES,0.32168850072780203,"The two conditions needed for the CLT to hold are not exactly met in the setting of SGD. Indepen-
dent and identically distributed. Generally we perform SGD by making a complete pass through the
entire dataset before using a sample again which introduces a weak dependence between samples.
While the covariance matrix without replacement more accurately models the dependence between
samples within a batch, it fails to account for the dependence between batches. Finite variance.
A different line of work has questioned the Gaussian assumption entirely because of the need for
ﬁnite variance random variables. This work instead suggests using the generalized central limit the-
orem implying the noise would be a heavy-tailed α-stable random variable [29]. Thus, the previous
assumption is implicitly assuming the i.i.d. and ﬁnite variance conditions apply for large enough
datasets and small enough batches."
REFERENCES,0.3231441048034934,"Under the CLT assumption, we must also replace the Euler discretizations with Euler–Maruyama
discretizations. For a general stochastic process, dXt = µdt + σdWt, the Euler–Maruyama method
extends the Euler method for ODEs to SDEs, resulting in the update equation Xk+1 = Xk + ∆tµ +
√"
REFERENCES,0.3245997088791849,"∆tσξ, where ξ ∼N(0, 1). Notice, the key difference is that if the temporal step size is ∆t = η,
then the noise is scaled by the square root √η. In fact, the main argument against modeling SGD
with an SDE, as nicely explained in Yaida [28], is that most SDE approximations simultaneously
assume that ∆t →0+, while maintaining that the square root of the learning rate √η is ﬁnite.
However, by modeling the discretization and stochastic effect simultaneously we can avoid this
argument, bringing us to our second assumption:"
REFERENCES,0.3260553129548763,"Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the
trajectory of the network driven by SGD through phase space such that θ(ηk) ≈θk and v(ηk) ≈vk."
REFERENCES,0.32751091703056767,"This approach of modeling discretization and stochasticity simultaneously is called stochastic mod-
iﬁed equations, as further explained in Li et al. [22]."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3289665211062591,"2The difference between a forward Euler and backward Euler discretization is a second-order central dis-
cretization,

θk+1−θk"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3304221251819505,"η

−

θk−θk−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3318777292576419,"η

= η

θk+1−2θk+θk−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3333333333333333,"η2

= η¨θ + O(η2)."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.33478893740902477,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.33624454148471616,"B
STRUCTURE IN THE COVARIANCE OF THE GRADIENT NOISE"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.33770014556040756,"As we’ve mentioned before, SGD introduces highly structured noise into an optimization process,
often assumed to be an essential ingredient for its ability to avoid local minima."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.33915574963609896,"Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-
tional to the Hessian of the quadratic loss Σ(θ) = σ2H where σ ∈R+ is some unknown scalar."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3406113537117904,"In the setting of linear regression, this is a very natural assumption. If we assume the classic genera-
tive model for linear regression data yi = x⊺
i ¯θ+σϵ where, ¯θ ∈Rd is the true model and ϵ ∼N(0, 1),
then provably Σ(θ) ≈σ2H."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3420669577874818,"Proof. We can estimate the covariance as Σ(θ) ≈
1
N
PN
i=1 gig⊺
i −gg⊺. Near stationarity gg⊺≪
1
N
PN
i=1 gig⊺
i , and thus,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3435225618631732,"Σ(θ) ≈1 N N
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.34497816593886466,"i=1
gig⊺
i ."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.34643377001455605,"Under the generative model yi = x⊺
i ¯θ + σϵ where ϵ ∼N(0, 1) and σ ∈R+, then the gradient gi is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.34788937409024745,"gi = (x⊺
i (θ −¯θ) −σϵ)xi,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.34934497816593885,"and the matrix gig⊺
i is
gig⊺
i = (x⊺
i (θ −¯θ) −σϵ)2(xix⊺
i )."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3508005822416303,"Assuming θ ≈¯θ at stationarity, then (x⊺
i (θ −¯θ) −σϵ)2 ≈σ2. Thus,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3522561863173217,"Σ(θ) ≈σ2 N N
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3537117903930131,"i=1
xix⊺
i = σ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3551673944687045,N X⊺X = σ2H
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.35662299854439594,"Also notice that weight decay is independent of the data or batch and thus simply shifts the gradient
distribution, but leaves the covariance of the gradient noise unchanged."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.35807860262008734,"While the above analysis is in the linear regression setting, for deep neural networks it is reasonable
to make the same assumption. See the appendix of Jastrz˛ebski et al. [12] for a discussion on this
assumption in the non-linear setting."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.35953420669577874,"Recent work by Ali et al. [32] also studies the dynamics of SGD (without momentum) in the setting
of linear regression. This work, while studying the classic ﬁrst-order stochastic differential equation,
made a point to not introduce an assumption on the diffusion matrix. In particular, they make the
point that even in the setting of linear regression, a constant covariance matrix will fail to capture
the actual dynamics. To illustrate this point they consider the univariate responseless least squares
problem,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3609898107714702,"minimize
θ∈R
1
2n n
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3624454148471616,"i=1
(xiθ)2."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.363901018922853,"As they explain, the SGD update for this problem would be"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3653566229985444,θk+1 = θk −η S X
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.36681222707423583,"i∈B
xi ! θk = k
Y"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3682678311499272,"i=1
(1 −η( 1 S
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3697234352256186,"i∈B
xi))θ0,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.37117903930131,"from which they conclude for a small enough learning rate η, then with probability one θk →
0. They contrast this with the Ornstein-Uhlenbeck process given by a constant covariance matrix
where while the mean for θk converges to zero its variance converges to a positive constant. So
is this discrepancy evidence that an Ornstein-Uhlenbeck process with a constant covariance matrix
fails to capture the updates of SGD? In many ways this problem is not a simple example, rather a
pathological edge case. Consider the generative model that would give rise to this problem,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3726346433770015,y = 0x + 0ξ = 0.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.37409024745269287,"In otherwords, the true model ¯θ = 0 and the standard deviation for the noise σ = 0. This would
imply by the assumption used in our paper that there would be zero diffusion and the resulting SDE
would simplify to a deterministic ODE that exponentially converges to zero."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.37554585152838427,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.37700145560407566,"C
A QUADRATIC LOSS AT THE END OF TRAINING"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3784570596797671,"Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network
can be approximated by the quadratic loss L(θ) = (θ −µ)⊺  H"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3799126637554585,"2

(θ −µ), where H ⪰0 is the
training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3813682678311499,"This assumption has been amply used in previous works such as Mandt et al. [31], Jastrz˛ebski et al.
[12], and Poggio et al. [50]. Particularly, Mandt et al. [31] discuss how this assumption makes sense
for smooth loss functions for which the stationary solution to the stochastic process reaches a deep
local minimum from which it is difﬁcult to escape."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.38282387190684136,"It is a well-studied fact, both empirically and theoretically, that the Hessian is low-rank near local
minima as noted by Sagun et al. [51], and Kunin et al. [20]. This degeneracy results in ﬂat directions
of equal loss. Kunin et al. [20] discuss how differentiable symmetries, architectural features that
keep the loss constant under certain weight transformations, give rise to these ﬂat directions. Im-
portantly, the Hessian and the covariance matrix share the same null space, and thus we can always
restrict ourselves to the image space of the Hessian, where the drift and diffusion matrix will be full
rank. Further discussion on the relationship between the Hessian and the covariance matrix can be
found in Thomas et al. [52]."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.38427947598253276,"It is also a well known empirical fact that even at the end of training the Hessian can have negative
eigenvalues [41]. This empirical observation is at odds with our assumption that the Hessian is
positive semi-deﬁnite H ⪰0. Further analysis is needed to alleviate this inconsistency."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.38573508005822416,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.38719068413391555,"D
SOLVING AN ORNSTEIN-UHLENBECK PROCESS WITH ANISOTROPIC
NOISE"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.388646288209607,"We will study the multivariate Ornstein-Uhlenbeck process described by the stochastic differential
equation
dXt = A(µ −Xt)dt +
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3901018922852984,"2κ−1DdWt
X0 = x0,
(14)
where A ∈Sm
++ is a positive deﬁnite drift matrix, µ ∈Rm is a mean vector, κ ∈R+ is some positive
constant, and D ∈Sm
++ is a positive deﬁnite diffusion matrix. This OU process is unique in that it
is one of the few SDEs we can solve explicitly. We can derive an expression for XT as,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3915574963609898,"XT = e−AT x0 +
 
I −e−AT 
µ +
Z T"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3930131004366812,"0
eA(t−T )√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.39446870451237265,"2κ−1DdWt.
(15)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.39592430858806404,"Proof. Consider the function f(t, x) = eAtx where eA is a matrix exponential. Then by Itô’s
Lemma3 we can evaluate the derivative of f(t, Xt) as"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.39737991266375544,"df(t, Xt) =
 
AeAtXt + eAtA(µ −Xt)

dt + eAt√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.3988355167394469,2κ−1DdWt
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4002911208151383,= AeAtµdt + eAt√
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4017467248908297,"2κ−1DdWt
Integrating this expression from t = 0 to t = T gives"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4032023289665211,"f(T, XT ) −f(0, X0) =
Z T"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.40465793304221254,"0
AeAtµdt +
Z T"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.40611353711790393,"0
eAt√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.40756914119359533,2κ−1DdWt
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4090247452692867,"eAT XT −x0 =
 
eAT −I

µ +
Z T"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4104803493449782,"0
eAt√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4119359534206696,2κ−1DdWt
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.413391557496361,which rearranged gives the expression for XT .
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4148471615720524,"From this expression it is clear that XT is a Gaussian process. The mean of the process is
E [XT ] = e−AT x0 +
 
I −e−AT 
µ,
(16)
and the covariance and cross-covariance of the process are"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4163027656477438,"Var(XT ) = κ−1
Z T"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4177583697234352,"0
eA(t−T )2DeA⊺(t−T )dt,
(17)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4192139737991266,"Cov(XT , XS) = κ−1
Z min(T,S)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.42066957787481807,"0
eA(t−T )2DeA⊺(t−S)dt.
(18)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.42212518195050946,These last two expressions are derived by Itô Isometry4.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.42358078602620086,"D.1
THE LYAPUNOV EQUATION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.42503639010189226,"We can explicitly solve the integral expressions for the covariance and cross-covariance exactly by
solving for the unique matrix B ∈Sm
++ that solves the Lyapunov equation,
AB + BA⊺= 2D.
(19)
If B solves the Lyapunov equation, notice
d
dt"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4264919941775837,"
eA(t−T )BeA⊺(t−S)
= eA(t−T )ABeA⊺(t−S) + eA(t−T )BA⊺eA⊺(t−S)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4279475982532751,= eA(t−T )2DeA⊺(t−S)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4294032023289665,"Using this derivative, the integral expressions for the covariance and cross-covariance simplify as,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.43085880640465796,"Var(XT ) = κ−1 
B −e−AT Be−A⊺T 
,
(20)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.43231441048034935,"Cov(XT , XS) = κ−1 
B −e−AT Be−A⊺T 
eA⊺(T −S),
(21)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.43377001455604075,where we implicitly assume T ≤S.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.43522561863173215,3Itô’s Lemma states that for any Itô drift-diffusion process dXt = µtdt + σtdWt and twice differentiable
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4366812227074236,"scalar function f(t, x), then df(t, Xt) =

ft + µtfx +
σ2
t
2 fxx

dt + σtfxdWt."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.438136826783115,"4Itô Isometry states for any standard Itô process Xt, then E
R t
0 XtdWt
2
= E
hR t
0 X2
t dt
i
."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4395924308588064,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4410480349344978,"D.2
DECOMPOSING THE DRIFT MATRIX"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.44250363901018924,"While the Lyapunov equation simpliﬁes the expressions for the covariance and cross-covariance, it
does not explain how to actually solve for the unknown matrix B. Following a method proposed by
Kwon et al. [48], we will show how to solve for B explicitly in terms of the drift A and diffusion D."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.44395924308588064,"The drift matrix A can be uniquely decomposed as,
A = (D + Q)U
(22)
where D is our symmetric diffusion matrix, Q is a skew-symmetric matrix (i.e. Q = −Q⊺), and U is
a positive deﬁnite matrix. Using this decomposition, then B = U −1, solves the Lyapunov equation."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.44541484716157204,"Proof. Plug B = U −1 into the left-hand side of equation (19),
AU −1 + U −1A⊺= (D + Q)UU −1 + U −1U(D −Q)
= (D + Q) + (D −Q)
= 2D
Here we used the symmetry of A, D, U and the skew-symmetry of Q."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4468704512372635,"All that is left is to do is solve for the unknown matrices Q and U. First notice the following identity,
AD −DA = QA + AQ
(23)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4483260553129549,"Proof. Multiplying A = (D + Q)U on the right by (D −Q) gives,
A(D −Q) = (D + Q)U(D −Q)
= (D + Q)A⊺,
which rearranged and using A = A⊺gives the desired equation."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4497816593886463,"Let V ΛV ⊺be the eigendecomposition of A and deﬁne the matrices eD = V ⊺DV and eQ = V ⊺QV .
These matrices observe the following relationship,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4512372634643377,eQij = λi −λj
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.45269286754002913,"ρi + λj
eDij.
(24)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.45414847161572053,"Proof. Replace A in the previous equality with its eigendecompsoition,
V ΛV ⊺D −DV ΛV ⊺= QV ΛV ⊺+ V ΛV ⊺Q.
Multiply this equation on the right by V and on the left by V ⊺,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4556040756914119,"Λ eD −eDΛ = eQΛ + Λ eQ.
Looking at this equality element-wise and using the fact that Λ is diagonal gives the scalar equality
for any i, j,
(λi −λj) eDij = (λi + λj) eQij,
which rearranged gives the desired expression."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4570596797671033,"Thus, Q and U are given by,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4585152838427948,"Q = V eQV ⊺,
U = (D + Q)−1A.
(25)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.45997088791848617,"This decomposition always holds uniquely when A, D ≻0, as λi−λj"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.46142649199417757,"λi+λj exists and (D + Q) is invert-
ible. See [48] for a discussion on the singularities of this decomposition."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.462882096069869,"D.3
STATIONARY SOLUTION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4643377001455604,"Using the Lyapunov equation and the drift decomposition, then XT ∼pT , where"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4657933042212518,"pT = N

e−AT x0 +
 
I −e−AT 
µ, κ−1 
U −1 −e−AT U −1e−A⊺T 
.
(26)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4672489082969432,"In the limit as T →∞, then e−AT →0 and pT →pss where
pss = N
 
µ, κ−1U −1
.
(27)
Similarly, the cross-covariance converges to the stationary cross-covariance,
Covss(XT , XS) = κ−1BeA⊺(T −S).
(28)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.46870451237263466,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.47016011644832606,"E
A VARIATIONAL FORMULATION OF THE OU PROCESS WITH
ANISOTROPIC NOISE"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.47161572052401746,"In this section we will describe an alternative, variational, route towards solving the dynamics of the
OU process studied in appendix D."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.47307132459970885,"Let Φ : Rn →R be an arbitrary, non-negative potential and consider the stochastic differential
equation describing the Langevin dynamics of a particle in this potential ﬁeld,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4745269286754003,"dXt = −∇Φ(Xt)dt +
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4759825327510917,"2κ−1D(Xt)dWt,
X0 = x0,
(29)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4774381368267831,"where D(Xt) is an arbitrary, spatially-dependent, diffusion matrix, κ is a temperature constant, and
x0 ∈Rm is the particle’s initial position. The Fokker-Planck equation describes the time evolution
for the probability distribution p of the particle’s position such that p(x, t) = P(Xt = x). The FP
equation is the partial differential equation5,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.47889374090247455,"∂tp = ∇·
 
∇Φ(Xt)p + κ−1∇· (D(Xt)p)

,
p(x, 0) = δ(x0),
(30)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.48034934497816595,"where ∇· denotes the divergence and δ(x0) is a dirac delta distribution centered at the initialization
x0. To assist in the exploration of the FP equation we deﬁne the vector ﬁeld,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.48180494905385735,"J(x, t) = −∇Φ(Xt)p −∇· (D(Xt)p) ,
(31)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.48326055312954874,"which is commonly referred to as the probability current. Notice, that this gives an alternative ex-
pression for the FP equation, ∂tp = −∇·J, demonstrating that J(x, t) deﬁnes the ﬂow of probability
mass through space and time. This interpretation is especially useful for solving for the stationary
solution pss, which is the unique distribution that satisﬁes,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4847161572052402,"∂tpss = −∇· Jss = 0,
(32)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4861717612809316,"where Jss is the probability current for pss. The stationary condition can be obtained in two distinct
ways:"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.487627365356623,"1. Detailed balance. This is when Jss(x) = 0 for all x ∈Ω. This is analogous to reversibility
for discrete Markov chains, which implies that the probability mass ﬂowing from a state i
to any state j is the same as the probability mass ﬂowing from state j to state i.
2. Broken detailed balance. This is when ∇· Jss(x) = 0 but Jss(x) ̸= 0 for all x ∈Ω. This
is analogous to irreversibility for discrete Markov chains, which only implies that the total
probability mass ﬂowing out of state i equals to the total probability mass ﬂowing into state
i."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4890829694323144,"The distinction between these two cases is critical for understanding the limiting dynamics of the
process."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.49053857350800584,"E.1
THE VARIATIONAL FORMULATION OF THE FOKKER-PLANCK EQUATION WITH
ISOTROPIC DIFFUSION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.49199417758369723,"We will now consider the restricted setting of standard, isotropic diffusion (D = I). It is easy
enough to check that in this setting the stationary solution is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.49344978165938863,pss(x) = e−κΦ(x)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4949053857350801,"Z
,
Z =
Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4963609898107715,"Ω
e−κΦ(x)dx,
(33)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4978165938864629,"where pss is called a Gibbs distribution and Z is the partition function. Under this distribution,
the stationary probability current is zero (Jss(x) = 0) and thus the process is in detailed balance.
Interestingly, the Gibbs distribution pss has another interpretation as the unique minimizer of the the
Gibbs free energy functional,
F(p) = E [Φ] −κ−1H(p),
(34)
where E [Φ] is the expectation of the potential Φ under the distribution p and H(p)
=
−
R"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.4992721979621543,Ωp(x)log(p(x))dx is the Shannon entropy of p.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5007278020378457,5This PDE is also known as the Forward Kolmogorov equation.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5021834061135371,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5036390101892285,"Proof. To prove that indeed pss is the unique minimizer of the Gibbs free energy functional, consider
the following equivalent expression"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.50509461426492,"F(p) =
Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5065502183406113,"Ω
p(x)Φ(x)dx + κ−1
Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5080058224163028,"Ω
p(x)log(p(x))dx"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5094614264919942,"= κ−1
Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5109170305676856,"Ω
p(x) (log(p(x)) −log(pss(x))) dx −κ−1
Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.512372634643377,"Ω
log(Z)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5138282387190685,= κ−1DKL(p ∥pss) −κ−1log(Z)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5152838427947598,"From this expressions, it is clear that the Kullback–Leibler divergence is uniquely minimized when
p = pss."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5167394468704513,"In other words, with isotropic diffusion the stationary solution pss can be thought of as the limiting
distribution given by the Fokker-Planck equation or the unique minimizer of an energetic-entropic
functional."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5181950509461426,"Seminal work by Jordan et al. [53] deepened this connection between the Fokker-Planck equation
and the Gibbs free energy functional. In particular, their work demonstrates that the solution p(x, t)
to the Fokker-Planck equation is the Wasserstein gradient ﬂow trajectory on the Gibbs free energy
functional."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.519650655021834,"Steepest descent is always deﬁned with respect to a distance metric. For example, the update equa-
tion, xk+1 = xk −η∇Φ(xk), for classic gradient descent on a potential Φ(x), can be formu-
lated as the solution to the minimization problem xk+1 = argminxηΦ(x) + 1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5211062590975255,"2d(x, xk)2 where
d(x, xk) = ∥x −xk∥is the Euclidean distance metric. Gradient ﬂow is the continuous-time limit of
gradient descent where we take η →0+. Similarly, Wasserstein gradient ﬂow is the continuous-time
limit of steepest descent optimization deﬁned by the Wasserstein metric. The Wasserstein metric is
a distance metric between probability measures deﬁned as,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5225618631732168,"W 2
2 (µ1, µ2) =
inf
p∈Π(µ1,µ2) Z"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5240174672489083,"Rn×Rn |x −y|2p(dx, dy),
(35)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5254730713245997,"where µ1 and µ2 are two probability measures on Rn with ﬁnite second moments and Π(µ1, µ2)
deﬁnes the set of joint probability measures with marginals µ1 and µ2. Thus, given an initial distri-
bution and learning rate η, we can use the Wasserstein metric to derive a sequence of distributions
minimizing some functional in the sense of steepest descent. In the continuous-time limit as η →0+
this sequence deﬁnes a continuous trajectory of probability distributions minimizing the functional.
Jordan et al. [54] proved, through the following theorem, that this process applied to the Gibbs free
energy functional converges to the solution to the Fokker-Planck equation with the same initializa-
tion:
Theorem 1 (JKO). Given an initial condition p0 with ﬁnite second moment and an η > 0, deﬁne
the iterative scheme pη with iterates deﬁned by"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5269286754002911,"pk = argminpη
 
E [Φ] −κ−1H(p)

+ W 2
2 (p, pk−1)."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5283842794759825,"As η →0+, then pη →p weakly in L1 where p is the solution to the Fokker-Planck equation with
the same initial condition."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.529839883551674,See [54] for further explanation and [53] for a complete derivation.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5312954876273653,"E.2
EXTENDING THE VARIATIONAL FORMULATION TO THE SETTING OF ANISOTROPIC
DIFFUSION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5327510917030568,"While the JKO theorem provides a very powerful lens through which to view solutions to the Fokker-
Planck equation, and thus distributions for particles governed by Langevin dynamics, it only applies
in the very restricted setting of isotropic diffusion. In this section we will review work by Chaudhari
and Soatto [33] extending the variational interpretation to the setting of anisotropic diffusion."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5342066957787481,"Consider when D(Xt) is an anisotropic, spatially-dependent diffusion matrix. In this setting, the
original Gibbs distribution given in equation (33) does not necessarily satisfy the stationarity con-
dition equation (32). In fact, it is not immediately clear what the stationary solution is or if the
dynamics even have one. Thus, Chaudhari and Soatto [33] make the following assumption:"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5356622998544396,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.537117903930131,"Stationary Assumption. Assume there exists a unique distribution pss that is the stationary solution
to the Fokker-Planck equation irregardless of initial conditions."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5385735080058224,"Under this assumption we can implicitly deﬁne the potential Ψ(x) = −κ−1log(pss(x)). Using this
modiﬁed potential we can express the stationary solution as a Gibbs distribution,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5400291120815138,"pss(x) ∝e−κΨ(x).
(36)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5414847161572053,"Under this implicit deﬁnition we can deﬁne the stationary probability current as Jss(x) =
j(x)pss(x) where
j(x) = −∇Φ(x) −κ−1∇· D(x) + D(x)∇Ψ(x).
(37)
The vector ﬁeld j(x) reﬂects the discrepancy between the original potential Φ and the modiﬁed
potential Ψ according to the diffusion D(x). Notice that in the isotropic case, when D(x) = I,
then Φ = Ψ and j(x) = 0. Chaudhari and Soatto [33] introduce another property of j(x) through
assumption,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5429403202328966,Conservative Assumption. Assume that the force j(x) is conservative (i.e. ∇· j(x) = 0).
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5443959243085881,"Using this assumption, Chaudhari and Soatto [33] extends the variational formulation provided by
the JKO theorem to the anisotropic setting,
Theorem 2 (CS). Given an initial condition p0 with ﬁnite second moment, then the energetic-
entropic functional,
F(p) = Ep [Ψ(x)] −κ−1H(p)
monotonically decreases throughout the trajectory given by the solution to the Fokker-Planck equa-
tion with the given initial condition."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5458515283842795,"In other words, the Fokker-Plank equation (30) with anisotropic diffusion can be interpreted as mini-
mizing the expectation of a modiﬁed loss Ψ, while being implicitly regularized towards distributions
that maximize entropy. The derivation requires we assume a stationary solution pss exists and that
the force j(x) implicitly deﬁned by pss is conservative. However, rather than implicitly deﬁne
Ψ(x) and j(x) through assumption, if we can explicitly construct a modiﬁed loss Ψ(x) such that
the resulting j(x) satisﬁes certain conditions, then the stationary solution exists and the variational
formulation will apply as well. We formalize this statement with the following theorem,
Theorem 3 (Explicit Construction). If there exists a potential Ψ(x) such that either j(x) = 0 or
∇· j(x) = 0 and ∇Ψ(x) ⊥j(x), then pss is the Gibbs distribution ∝e−κΨ(x) and the variational
formulation given in Theorem 2 applies."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5473071324599709,"E.3
APPLYING THE VARIATIONAL FORMULATION TO THE OU PROCESS"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5487627365356623,"Through explicit construction we now seek to ﬁnd analytic expressions for the modiﬁed loss Ψ(x)
and force j(x) hypothesised by Chaudhari and Soatto [33] in the fundamental setting of an OU
process with anisotropic diffusion, as described in section D. We assume the diffusion matrix is
anisotropic, but spatially independent, ∇· D(x) = 0. For the OU process the original potential
generating the drift is
Φ(x) = (x −µ)⊺A"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5502183406113537,"2 (x −µ).
(38)
Recall, that in order to extend the variational formulation we must construct some potential Ψ(x)
such that ∇· j(x) = 0 and ∇Ψ ⊥j(x). It is possible to construct Ψ(x) using the unique decompo-
sition of the drift matrix A = (D + Q)U discussed in appendix D. Deﬁne the modiﬁed potential,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5516739446870451,Ψ(x) = (x −µ)⊺U
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5531295487627366,"2 (x −µ).
(39)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5545851528384279,"Using this potential, the force j(x) is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5560407569141194,"j(x) = −A(x −µ) + DU(x −µ) = −QU(x −µ).
(40)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5574963609898108,"Notice that j(x) is conservative, ∇· j(x) = ∇· −QU (x −µ) = 0 because Q is skew-symmetric.
Additionally, j(x) is orthogonal, j(x)⊺∇Ψ(x) = (x −µ)⊺U ⊺QU (x −µ) = 0, again because Q is
skew-symmetric. Thus, we have determined a modiﬁed potential Ψ(x) that results in a conservative
orthogonal force j(x) satisfying the conditions for Theorem 3. Indeed the stationary Gibbs distri-
bution given by Theorem 3 agrees with equation (27) derived via the ﬁrst and second moments in
appendix D,
e−κΨ(x) ∝N
 
µ, κ−1U −1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5589519650655022,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5604075691411936,"In addition to the variational formulation, this interpretation further details explicitly the stationary
probability current, Jss(x) = j(x)pss, and whether or not the the stationary solution is in broken
detailed balance."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5618631732168851,"F
EXPLICIT EXPRESSIONS FOR THE OU PROCESS GENERATED BY SGD"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5633187772925764,"We will now consider the speciﬁc OU process generated by SGD with linear regression. Here we
repeat the setup as explained in section 5."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5647743813682679,"Let X ∈RN×d, Y ∈RN be the input data, output labels respectively and θ ∈Rd be our vector of
regression coefﬁcients. The least squares loss is the convex quadratic loss L(θ) =
1
2N ∥Y −Xθ∥2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5662299854439592,"with gradient g(θ) = Hθ −b, where H =
X⊺X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5676855895196506,"N
and b =
X⊺Y"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5691411935953421,"N . Plugging this expression for
the gradient into the underdamped Langevin equation (3), and rearranging terms, results in the
multivariate Ornstein-Uhlenbeck (OU) process,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5705967976710334,"d

θt
vt"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5720524017467249,"
= A

µ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5735080058224163,"
−

θt
vt"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5749636098981077,"
dt +
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5764192139737991,"2κ−1DdWt,
(41)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5778748180494906,"where A and D are the drift and diffusion matrices respectively,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5793304221251819,"A =

0
−I
2
η(1+β)(H + λI)
2(1−β)
η(1+β)I"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5807860262008734,"
,
D =
0
0
0
2(1−β)
η(1+β)Σ(θ)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5822416302765647,"
,
(42)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5836972343522562,"κ = S(1 −β2) is a temperature constant, and µ = (H + λI)−1b is the ridge regression solution."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5851528384279476,"F.1
SOLVING FOR THE MODIFIED LOSS AND CONSERVATIVE FORCE"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.586608442503639,"In order to apply the expressions derived for a general OU process in appendix D and E, we must
ﬁrst decompose the drift as A = (D + Q)U. Under the simpliﬁcation Σ(θ) = σ2H discussed in
appendix B, then the matrices Q and U, as deﬁned below, achieve this,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5880640465793304,"Q =

0
−σ2H
σ2H
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5895196506550219,"
,
U =

2
η(1+β)σ2 H−1 (H + λI)
0
0
1
σ2 H−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5909752547307132,"
.
(43)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5924308588064047,"Using these matrices we can now derive explicit expressions for the modiﬁed loss Ψ(θ, v) and con-
servative force j(θ, v). First notice that the least squares loss with L2 regularization is proportional
to the convex quadratic,
Φ(θ) = (θ −µ)⊺(H + λI)(θ −µ).
(44)
The modiﬁed loss Ψ is composed of two terms, one that only depends on the position,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5938864628820961,"Ψθ(θ) = (θ −µ)⊺
H−1(H + λI)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5953420669577875,η(1 + β)σ2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5967976710334789,"
(θ −µ) ,
(45)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5982532751091703,"and another that only depends on the velocity,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.5997088791848617,"Ψv(v) = v⊺
H−1 σ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6011644832605532,"
v.
(46)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6026200873362445,"The conservative force j(θ, v) is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.604075691411936,"j(θ, v) =

v
−
2
η(1+β) (H + λI) (θ −µ)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6055312954876274,"
,
(47)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6069868995633187,"and thus the stationary probability current is Jss(θ, v) = j(θ, v)pss."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6084425036390102,"F.2
DECOMPOSING THE TRAJECTORY INTO THE EIGENBASIS OF THE HESSIAN"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6098981077147017,"As shown in appendix D, the temporal distribution for the OU process at some time T ≥0 is, pT"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.611353711790393,"
θ
v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6128093158660844,"
= N

e−AT

θ0
v0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6142649199417758,"
+
 
I −e−AT  
µ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6157205240174672,"
, κ−1 
U −1 −e−AT U −1e−A⊺T 
."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6171761280931587,"Here we will now use the eigenbasis {q1, . . . , qm} of the Hessian with eigenvalues {ρ1, . . . , ρm} to
derive explicit expressions for the mean and covariance of the process through time."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.61863173216885,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6200873362445415,Deterministic component. We can rearrange the expectation as
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6215429403202329,"E

θ
v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6229985443959243,"
=

µ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6244541484716157,"
+ e−AT

θ0 −µ
v0 
."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6259097525473072,"Notice that the second, time-dependent term is actually the solution to the system of ODEs"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6273653566229985,"˙

θ
v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.62882096069869,"
= −A

θ
v "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6302765647743813,"with initial condition [θ0 −µ
v0]⊺. This system of ODEs can be block diagonalized by factorizing
A = OSO⊺where O is orthogonal and S is block diagonal deﬁned as O =  "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6317321688500728,"q1
0
. . .
qm
0
...
0
q1
. . .
0
qm "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6331877729257642,"
S = "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6346433770014556," 0
−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.636098981077147,"2
η(1+β)(ρ1 + λ)
2(1−β)
η(1+β)
...
...
...
0
−1
2
η(1+β)(ρm + λ)
2(1−β)
η(1+β) "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6375545851528385,
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6390101892285298,"In otherwords in the plane spanned by [qi
0]⊺and [0
qi]⊺the system of ODEs decouples into the
2D system
˙

ai
bi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6404657933042213,"
=

0
1
−
2
η(1+β)(ρi + λ)
−2(1−β)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6419213973799127,η(1+β)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6433770014556041," 
ai
bi "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6448326055312955,"This system has a simple physical interpretation as a damped harmonic oscillator. If we let bi = ˙ai,
then we can unravel this system into the second order ODE"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6462882096069869,¨ai + 2 1 −β
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6477438136826783,"η(1 + β) ˙ai +
2
η(1 + β)(ρi + λ)ai = 0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6491994177583698,"which is in standard form (i.e. ¨x + 2γ ˙x + ω2x = 0) for γ =
1−β
η(1+β) and ωi =
q"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6506550218340611,"2
η(1+β)(ρi + λ)."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6521106259097526,"Let ai(0) = ⟨θ0 −µ, qi⟩and bi(0) = ⟨v0, qi⟩, then the solution in terms of γ and ωi is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.653566229985444,ai(t) =
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6550218340611353,"




"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6564774381368268,"



"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6579330422125182,"e−γt

ai(0) cosh
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6593886462882096,"γ2 −ω2
i t

+ γai(0)+bi(0)
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.660844250363901,"γ2−ω2
i
sinh
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6622998544395924,"γ2 −ω2
i t

γ > ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6637554585152838,"e−γt(ai(0) + (γai(0) + bi(0))t)
γ = ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6652110625909753,"e−γt

ai(0) cos
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6666666666666666,"ω2
i −γ2t

+ γai(0)+bi(0)
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6681222707423581,"ω2
i −γ2
sin
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6695778748180495,"ω2
i −γ2t

γ < ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6710334788937409,Differentiating these equations gives us solutions for bi(t)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6724890829694323,bi(t) =
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6739446870451238,"




"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6754002911208151,"



"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6768558951965066,"e−γt

bi(0) cosh
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6783114992721979,"γ2 −ω2
i t

−ω2
i ai(0)+γbi(0)
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6797671033478894,"γ2−ω2
i
sinh
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6812227074235808,"γ2 −ω2
i t

γ > ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6826783114992722,"e−γt  
bi(0) −
 
ω2
i ai(0) + γbi(0)

t

γ = ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6841339155749636,"e−γt

bi(0) cos
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6855895196506551,"ω2
i −γ2t

−ω2
i ai(0)+γbi(0)
√"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6870451237263464,"ω2
i −γ2
sin
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6885007278020379,"ω2
i −γ2t

γ < ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6899563318777293,"Combining all these results, we can now analytically decompose the expectation as the sum,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6914119359534207,"E

θ
v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6928675400291121,"
=

µ
0 
+ m
X i=1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6943231441048034,"
ai(t)

qi
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6957787481804949,"
+ bi(t)

0
qi 
."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6972343522561864,"Intuitively, this equation describes a damped rotation (spiral) around the OLS solution in the planes
deﬁned by the the eigenvectors of the Hessian at a rate proportional to the respective eigenvalue."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.6986899563318777,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7001455604075691,"Stochastic component. Using the previous block diagonal decomposition A = OSO⊺we can
simplify the variance as"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7016011644832606,"Var

θ
v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7030567685589519,"
= κ−1 
U −1 −e−AT U −1e−A⊺T "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7045123726346434,"= κ−1 
U −1 −e−OSO⊺T U −1e−OS⊺O⊺T "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7059679767103348,"= κ−1O

O⊺U −1O −e−ST (O⊺U −1O)e−ST ⊺
O⊺"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7074235807860262,"Interestingly, the matrix O⊺U −1O is also block diagonal,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7088791848617176,"O⊺U −1O = O⊺
 η(1+β)σ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.710334788937409,"2
(H + λI)−1 H
0
0
σ2H 
O = "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7117903930131004,
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7132459970887919,η(1+β)σ2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7147016011644832,"2
ρ1
ρ1+λ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7161572052401747,"0
σ2ρ1
...
...
..."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7176128093158661,η(1+β)σ2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7190684133915575,"2
ρm
ρm+λ
0
0
σ2ρm "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7205240174672489,
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7219796215429404,"Thus, similar to the mean, we can simply consider the variance in each of the planes spanned by
[qi
0]⊺and [0
qi]⊺. If we deﬁne the block matrices, Di = ""
ησ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7234352256186317,"2S(1−β)
ρi
ρi+λ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7248908296943232,"0
σ2
S(1−β2)ρi #"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7263464337700145,"Si =

0
1
−
2
η(1+β)(ρi + λ)
−2(1−β)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.727802037845706,η(1+β) 
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7292576419213974,then the projected variance matrix in this plane simpliﬁes as
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7307132459970888,"Var

q⊺
i θ
q⊺
i v"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7321688500727802,"
= Di −e−SiT Die−SiT ⊺"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7336244541484717,"Using the solution to a damped harmonic osccilator discussed previously, we can express the matrix
exponential e−SiT explicitly in terms of γ =
1−β
η(1+β) and ωi =
q"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.735080058224163,"2
η(1+β)(ρi + λ). If we let αi =
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7365356622998545,"|γ2 −ω2
i |, then the matrix exponential is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7379912663755459,e−Sit =
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7394468704512372,"








"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7409024745269287,"







"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.74235807860262,"e−γt
""
cosh (αit) + γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7438136826783115,"αi sinh (αit)
1
αi sinh (αit)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.745269286754003,"−ω2
i
αi sinh (αit)
cosh (αit) −γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7467248908296943,αi sinh (αit) #
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7481804949053857,γ > ωi
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7496360989810772,"e−γt
1 + γt
t
−ω2
i t
1 −γt"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7510917030567685,"
γ = ωi"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.75254730713246,"e−γt
""
cos (αit) + γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7540029112081513,"αi sin (αit)
1
αi sin (αit)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7554585152838428,"−ω2
i
αi sin (αit)
cos (αit) −γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7569141193595342,αi sin (αit) #
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7583697234352256,γ < ωi
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.759825327510917,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7612809315866085,"G
ANALYZING PROPERTIES OF THE STATIONARY SOLUTION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7627365356622998,"Assuming the stationary solution is given by equation (??) we can solve for the expected value of
the norm of the local displacement and gain some intuition for the expected value of the norm of
global displacement."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7641921397379913,"G.1
INSTANTANEOUS SPEED"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7656477438136827,"Ess

∥δk∥2
= Ess

∥θk+1 −θk∥2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7671033478893741,"= η2Ess

∥vk+1∥2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7685589519650655,"= η2tr
 
Ess

vk+1v⊺
k+1
"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7700145560407569,= η2tr (Varss (vk+1) + Ess [vk+1] Ess [vk+1]⊺)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7714701601164483,"= η2tr
 
κ−1U −1 =
η2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7729257641921398,"S(1 −β2)tr
 
σ2H
"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7743813682678311,"Note that this follows directly from the deﬁnition of δk in equation (1) and the mean and variance of
the stationary solution in equation ( ??), as well as the follow-up derivation in appendix F."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7758369723435226,"G.2
ANOMALOUS DIFFUSION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.777292576419214,"Notice, that the global movement ∆t = θt−θ0 can be broken up into the sum of the local movements
∆t = Pt
i=1 δi, where δi = θi −θi−1. Applying this decomposition,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7787481804949054,"Ess

∥∆t∥2
= Ess     t
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7802037845705968,"i=1
δi   2  = t
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7816593886462883,"i=1
Ess

∥δi∥2
+ t
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7831149927219796,"i̸=j
Ess [⟨δi, δj⟩]"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.784570596797671,"As we solved for previously,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7860262008733624,"Ess

∥δi∥2
= η2Ess

∥vi∥2
= η2tr (Varss(vi)) =
η2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7874818049490538,"S(1 −β2)tr
 
σ2H

."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7889374090247453,"By a similar simpliﬁcation, we can express the second term in terms of the stationary cross-
covariance,
Ess [⟨δi, δj⟩] = η2Ess [⟨vi, vj⟩] = η2tr (Covss(vi, vj)) .
Thus, to simplify this expression we just need to consider the velocity-velocity covariance
Covss(vi, vj). At stationarity, the cross-covariance for the system in phase space, zi = [θi
vi]
is
Covss(zi, zj) = κ−1U −1e−A⊺|i−j|"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7903930131004366,"where κ = S(1 −β2), and"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7918486171761281,"U =

2
η(1+β)σ2 H−1 (H + λI)
0
0
1
σ2 H−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7933042212518195,"
A =

0
−I
2
η(1+β)(H + λI)
2(1−β)
η(1+β)I "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7947598253275109,"As discussed when solving for the mean of the OU trajectory, the drift matrix A can be block
diagonalized as A = OSO⊺where O is orthogonal and S is block diagonal deﬁned as O =  "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7962154294032023,"q1
0
. . .
qm
0
...
0
q1
. . .
0
qm "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7976710334788938,"
,
S = "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.7991266375545851," 0
−1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8005822416302766,"2
η(1+β)(ρ1 + λ)
2(1−β)
η(1+β)
...
...
...
0
−1
2
η(1+β)(ρm + λ)
2(1−β)
η(1+β) "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8020378457059679, .
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8034934497816594,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8049490538573508,"Notice also that O diagonalizes U −1 such that,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8064046579330422,Λ = O⊺U −1O = 
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8078602620087336,
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8093158660844251,η(1+β)σ2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8107714701601164,"2
ρ1
ρ1+λ
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8122270742358079,"0
σ2ρ1
...
...
..."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8136826783114993,η(1+β)σ2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8151382823871907,"2
ρm
ρm+λ
0
0
σ2ρm "
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8165938864628821, .
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8180494905385735,"Applying these decompositions, properties of matrix exponentials, and the cyclic invariance of the
trace, allows us to express the trace of the cross-covariance as"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8195050946142649,"tr (Covss(zi, zj)) = κ−1tr

U −1e−A⊺|i−j|"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8209606986899564,"= κ−1tr

U −1Oe−S⊺|i−j|O⊺"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8224163027656477,"= κ−1tr

Λe−S⊺|i−j|"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8238719068413392,"= κ−1
n
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8253275109170306,"k=1
tr

Λke−S⊺
k |i−j|"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.826783114992722,"where Λk and Sk are the blocks associated with each eigenvector of H. As solved for previously in
the variance of the OU process, we can express the matrix exponential e−Sk|i−j| explicitly in terms
of γ =
1−β
η(1+β) and ωk =
q"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8282387190684134,"2
η(1+β)(ρk + λ). If we let τ = |i −j| and αk =
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8296943231441049,"|γ2 −ω2
k|, then the
matrix exponential is"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8311499272197962,e−Sk|i−j| =
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8326055312954876,"








"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.834061135371179,"







"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8355167394468704,"e−γτ
""
cosh (αkτ) +
γ
αk sinh (αkτ)
1
αk sinh (αkτ)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8369723435225619,"−ω2
k
αk sinh (αkτ)
cosh (αkτ) −
γ
αk sinh (αkτ) #"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8384279475982532,γ > ωk
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8398835516739447,"e−γτ
1 + γτ
τ
−ω2
kτ
1 −γτ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8413391557496361,"
γ = ωk"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8427947598253275,"e−γτ
""
cos (αkτ) +
γ
αk sin (αkτ)
1
αk sin (αkτ)"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8442503639010189,"−ω2
k
αk sin (αkτ)
cos (αkτ) −
γ
αk sin (αkτ) #"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8457059679767104,γ < ωk
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8471615720524017,"Plugging in these expressions into previous expression and restricting to just the kth velocity com-
ponent, we see"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8486171761280932,"Covss(vi,k, vj,k) = κ−1 h
Λke−S⊺
k |i−j|i 1,1 ="
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8500727802037845,"


 

"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.851528384279476,"κ−1σ2ρke−γτ 
cosh (αkτ) −
γ
αk sinh (αkτ)

γ > ωk
κ−1σ2ρke−γτ (1 −γτ)
γ = ωk
κ−1σ2ρke−γτ 
cos (αkτ) −
γ
αk sin (αkτ)

γ < ωk"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8529839883551674,"Pulling it all together,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8544395924308588,"Ess

∥∆t∥2
=
η2σ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8558951965065502,S(1 −β2) 
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8573508005822417,"tr (H) t + 2t t
X k=1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.858806404657933,"
1 −k t  m
X"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8602620087336245,"l=1
ρlCl(k) !"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8617176128093159,where Cl(k) is deﬁned as
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8631732168850073,Cl(k) =
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8646288209606987,"


 

"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.86608442503639,"e−γk 
cosh (αlk) −γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8675400291120815,"αl sinh (αlk)

γ > ωl
e−γk (1 −γk)
γ = ωl
e−γk 
cos (αlk) −γ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.868995633187773,"αl sin (αlk)

γ < ωl"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8704512372634643,"for γ =
1−β
η(1+β), ωl =
q"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8719068413391557,"2
η(1+β)(ρl + λ), and αl =
p"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8733624454148472,"|γ2 −ω2
l |."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8748180494905385,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.87627365356623,"G.3
TRAINING LOSS AND EQUIPARTITION THEOREM"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8777292576419214,"In addition to solving for the expected values of the local and global displacements, we can consider
the expected training loss and ﬁnd an interesting relationship to the equipartition theorem from
classical statistical mechanics."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8791848617176128,The regularized training loss is Lλ(θ) = 1
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8806404657933042,2(θ −µ)⊺H(θ −µ) + λ
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8820960698689956,"2 ∥θ∥2, where H is the Hessian
matrix and µ is the true mean. Taking the expectation with respect to the stationary distribution,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.883551673944687,Ess [Lλ(θ)] = 1
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8850072780203785,2tr ((H + λI)Ess [θθ⊺]) −µ⊺HEss [θ] + 1 2µ⊺Hµ
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8864628820960698,The ﬁrst and second moments of the stationary solution are
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8879184861717613,"Ess [θ] = µ
Ess [θθ⊺] =
η
S(1 −β)
σ2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8893740902474527,2 (H + λI)−1H + µµ⊺
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8908296943231441,Plugging these expressions in and canceling terms we get
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8922852983988355,"Ess [Lλ(θ)] =
η
4S(1 −β)tr
 
σ2H

+ λ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.893740902474527,2 ∥µ∥2
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8951965065502183,"Deﬁne the kinetic energy of the network as K(v) =
1
2m∥v∥2, where m =
η
2(1 + β) is the per-
parameter “mass"" of the network according to our previously derived Langevin dynamics. At sta-
tionarity,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8966521106259098,Ess [K(v)] = η(1 + β)
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8981077147016011,"4
tr (Ess [vv⊺]) =
η
4S(1 −β)tr
 
σ2H
"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.8995633187772926,"where we used the fact that Ess [vv⊺] =
1
S(1−β2)σ2H. In otherwords, at stationarity,"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.901018922852984,Ess [Lλ(θ)] = Ess [K(v)] + λ
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9024745269286754,2 ∥µ∥2.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9039301310043668,"This relationship between the expected potential and kinetic energy can be understood as a form of
the equipartition theorem."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9053857350800583,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9068413391557496,"H
EXPERIMENTAL DETAILS"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9082969432314411,"H.1
COMPUTING THE HESSIAN EIGENDECOMPOSITION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9097525473071325,"Computing the full Hessian of the loss with respect to the parameters is computationally intractable
for large models. However, equipped with an autograd engine, we can compute Hessian-vector prod-
ucts. We use the subspace iteration on Hessian-vector products computed on a variety of datasets.
For Cifar-10 we use the entire train dataset to compute the Hessian-vector products. For Imagenet,
we use a subset 40,000 images sampled from the train dataset to keep the computation within rea-
sonable limits."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9112081513828238,"For experiments on linear regression, the Hessian is independent of the model (it only depends on
the data) and can be computed using any model checkpoint. For all other experiments, the Hessian
eigenvectors were computed using the model at its initial pre-trained state."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9126637554585153,"H.2
FIGURE 1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9141193595342066,"We resumed training for a variety of ImageNet pre-trained models from Torchvision [16] for 10
epochs, with the hyperparameters used at the end of training, shown in table 1."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9155749636098981,"We kept track of the norms of the local and global displacement, ∥δk∥2
2 and ∥∆k∥2
2, every 250
steps in the training process, to keep the length of the trajectories within reasonable limits. ∥δk∥2
2 is
visualized directly, along with its 15 step moving average. We then ﬁt a power law of the form αkc
to the ∥∆k∥2
2 trajectories for each model, using the last 2/3 of the saved trajectories. We visualize
the ∥∆k∥2
2 trajectories along with their ﬁts on a log-log plot."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9170305676855895,"Model
Dataset
Opt.
Epochs
Batch size S
LR η
Mom. β
WD λ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9184861717612809,"VGG-16
ImageNet
SGDM
10
256
10−5
0.9
5 × 10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9199417758369723,"VGG-11 w/BN
ImageNet
SGDM
10
256
10−5
0.9
5 × 10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9213973799126638,"VGG-16 w/BN
ImageNet
SGDM
10
256
10−5
0.9
5 × 10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9228529839883551,"ResNet-18
ImageNet
SGDM
10
256
10−4
0.9
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9243085880640466,"ResNet-34
ImageNet
SGDM
10
256
10−4
0.9
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.925764192139738,Table 1: Figure 1 experiments training hyperparameters.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9272197962154294,"H.3
FIGURE 2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9286754002911208,"For this ﬁgure we trained a linear regression model on Cifar-10, using MSE loss on the one-hot
encoded labels. The hyperparameters used during training are shown in table 2."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9301310043668122,"At every step in training the full set of model weights and velocities were stored. The top 30 eigen-
vectors of the hessian were computed as described in appendix H.1, using 10 subspace iterations."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9315866084425036,"The saved weight and velocity trajectories were then projected onto the top eigenvector of the hessian
and were visualized in black. Using the initial weights and velocities, the red trajectories were
computed according to equation (5)."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9330422125181951,"Model
Dataset
Opt.
Epochs
Batch size S
LR η
Mom. β
WD λ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9344978165938864,"Linear Regression
Cifar-10
SGDM
4
512
10−5
0.9
0
Linear Regression
Cifar-10
SGDM
4
512
10−5
0.99
0"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9359534206695779,Table 2: Figure 2 experiments training hyperparameters.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9374090247452693,"H.4
FIGURE 3"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9388646288209607,"For this ﬁgure we constructed an arbitrary Ornstein-Uhlenbeck process with anisotropic noise which
would help contrast the original and modiﬁed potentials. We sampled from a 2 dimensional OU
process of the form"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9403202328966521,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9417758369723436,"d

x1
x2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9432314410480349,"
= A

b −

x1
x2"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9446870451237264,"
dt + 10−4√ DdWt."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9461426491994177,"where we set b = [−0.1, 0.05]⊺and arbitrarily construct A such that it’s eigenvectors are aligned
with q1 = [−1, 1]⊺and q2 = [1, 1]⊺and it’s eigenvalues are 4 and 1 as follows:"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9475982532751092,"D =

4
0
0
1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9490538573508006,"
,
V =

−1
1
1
1"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.950509461426492,"
,
A = V −1DV"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9519650655021834,"The background for the left panel was computed from the convex quadratic potential Φ(x) =
1
2x⊺Ax −bx. The background for the right panel was computed from the modiﬁed quadratic
Ψ(x) =
1
2x⊺Ux −ux, with U = (D + Q)−1A and u = UA−1b (see equation (25)). Both
were sampled in a regular 40 × 40 grid in [−0.1, 0.1] × [−0.1, 0.1]."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9534206695778749,"H.5
FIGURES 4, 5, AND 6"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9548762736535662,"Starting with the ImageNet pre-trained ResNet-18 from Torchvision [16], we resumed training for
5 epochs with the hyperparameters used at the end of training, shown in table3. The top 30 Hessian
eigenvectors were computed as described in appendix H.1, using 10 subspace iterations. During
training, we tracked the projection of the weights and velocities onto eigenvectors q1 and q30."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9563318777292577,"For Figure 3, we show the projection of the position trajectory onto eigenvectors q1 and q30 in 2D in
black. The background for the left and center panels was computed taking the ImageNet pre-trained
ResNet-18 from Torchvision [16] and perturbing its weights in the q1 and q30 directions in a region
close to the projected trajectory. The training and test loss were computed for a grid of 20 × 20
perturbed models. The background for the right panel was computed according to equation (10)."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9577874818049491,"Model
Dataset
Opt.
Epochs
Batch size S
LR η
Mom. β
WD λ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9592430858806404,"ResNet-18
ImageNet
SGDM
5
256
10−4
0.9
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9606986899563319,"Table 3: Figures 4,5,6 experiments training hyperparameters."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9621542940320232,"H.6
FIGURE 7"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9636098981077147,"We resumed training for an ImageNet pre-trained ResNet-18 from Torchvision [16] for 2 epochs,
using the sweeps of hyperparameters shown in table 4. We indicate a sweep in a particular hyperpa-
rameter by [A, B], which denotes 20 evenly spaced numbers between A and B, inclusive."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9650655021834061,"We kept track of the norms of the local and global displacement, ∥δk∥2
2 and ∥∆k∥2
2, every step in
the training process. The value for ∥δk∥2
2 at the end of the two epochs is shown in the top row of
the ﬁgure. We ﬁtted power law of the form αkc to the ∥∆k∥2
2 trajectories for each model on the full
trajectories. The ﬁtted exponent c for each model is plotted in the bottom row of the ﬁgure."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9665211062590975,"Model
Dataset
Opt.
Epochs
Batch size S
LR η
Mom. β
WD λ"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9679767103347889,"ResNet-18
ImageNet
SGDM
2
[32, 1024]
10−4
0.9
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9694323144104804,"ResNet-18
ImageNet
SGDM
2
256
[10−3, 10−5]
0.9
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9708879184861717,"ResNet-18
ImageNet
SGDM
2
256
10−4
[0.8, 0.99]
10−4"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9723435225618632,Table 4: Figure 7 experiments training hyperparameters.
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9737991266375546,"H.7
INCREASING RATE OF ANOMALOUS DIFFUSION"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.975254730713246,"Upon further experimentation with the ﬁtting procedure for the rate of anomalous diffusion ex-
plained in Figure 1, we observed an interesting phenomenon. The ﬁtted exponent c for the power
law relationship ∥∆k∥2
2 ∝kc increases as a function of the length of the trajectory we ﬁt to. As can"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9767103347889374,Under review as a conference paper at ICLR 2022
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9781659388646288,"be seen in Figure 7, c increases at a diminishing rate with the length of the trajectory. This could
be indicative of ∥∆k∥2
2 being governed by a sum of power laws where the leading term becomes
dominant for longer trajectories."
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9796215429403202,c = 1.069
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9810771470160117,c = 1.190
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.982532751091703,"c = 1.268
c = 1.288"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9839883551673945,c = 1.219
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9854439592430859,"c = 1.117
c = 1.157"
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9868995633187773,c = 1.245
THE DIFFERENCE BETWEEN A FORWARD EULER AND BACKWARD EULER DISCRETIZATION IS A SECOND-ORDER CENTRAL DIS-,0.9883551673944687,c = 1.306
EPOCHS,0.9898107714701602,"20 epochs
30 epochs
40 epochs"
EPOCHS,0.9912663755458515,"70 epochs
60 epochs"
EPOCHS,0.992721979621543,"90 epochs
100 epochs
80 epochs"
EPOCHS,0.9941775836972343,50 epochs
EPOCHS,0.9956331877729258,"Step
Epochs"
EPOCHS,0.9970887918486172,Diffusion c
EPOCHS,0.9985443959243085,"Figure 7: The ﬁtted rate of anomalous diffusion increases with the length of trajectory ﬁtted.
The left panel shows the ﬁtted power law on training trajectories of increasing length from the pre-
trained ResNet-18 model. The right panel shows the ﬁtted exponent c as a function of the length of
the trajectory."
