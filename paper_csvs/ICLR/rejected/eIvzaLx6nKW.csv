Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035587188612099642,"Contrastive self-supervised learning has recently gained signiﬁcant attention ow-
ing to its ability to learn improved feature representations without the use of label
information. Current contrastive learning approaches, however, are only effective
when trained on a particular dataset, limiting their utility in diverse multi-domain
settings. In fact, training these methods on a combination of several domains of-
ten degrades the quality of learned representations compared to the models trained
on a single domain. In this paper, we propose a Multi-Domain Self-Supervised
Learning (MDSSL) approach that can effectively perform representation learning
on multiple, diverse datasets. In MDSSL, we propose a three-level hierarchi-
cal loss for measuring the agreement between augmented views of a given sam-
ple, agreement between samples within a dataset and agreement between samples
across datasets. We show that MDSSL when trained on a mixture of CIFAR-10,
STL-10, SVHN and CIFAR-100 produces powerful representations, achieving up
to a 25% increase in top-1 accuracy on a linear classiﬁer compared to single-
domain self-supervised encoders. Moreover, MDSSL encoders can generalize
more effectively to unseen datasets compared to both single-domain and multi-
domain baselines. MDSSL is also highly efﬁcient in terms of the resource usage
as it stores and trains a single model for multiple datasets leading up to 17% reduc-
tion in training time. Finally, for multi-domain datasets where domain labels are
unknown, we propose a modiﬁed approach that alternates between clustering and
MDSSL. Thus, for diverse multi-domain datasets (even without domain labels),
MDSSL provides an efﬁcient and generalizable self-supervised encoder without
sacriﬁcing the quality of representations in individual domains."
INTRODUCTION,0.0071174377224199285,"1
INTRODUCTION"
INTRODUCTION,0.010676156583629894,"Self-supervised contrastive training (Chen et al., 2020; He et al., 2020; Misra & van der Maaten,
2020; Caron et al., 2020b) has become a popular paradigm for unsupervised representation learning
as it shows impressive results on linear classiﬁcation tasks, almost matching the performance of
a supervised model trained from scratch. However, we ﬁnd that current self-supervised models are
only effective when trained on a single-domain. This can hinder their deployment in large scale real-
world settings where data almost always comes from multiple diverse domains. We illustrate this
issue in Table 1, where we show that a popular self-supervised model, SimCLR (Chen et al., 2020),
trained on CIFAR-10 (Krizhevsky et al., a) does not generalize to other domains at the test time.
We observe that the top-1 accuracy of a linear classiﬁer signiﬁcantly drops on unseen datasets. This
means that a different self-supervised model needs to be trained for every new dataset, which can
add signiﬁcant computational overheads given that training these models often require large batch
sizes and a large number of training epochs (Chen et al., 2020; He et al., 2020; Wu et al., 2018)."
INTRODUCTION,0.014234875444839857,"One potential solution for self-supervised learning on multi-domain datasets is to train the mod-
els on the union of all input domains. Unfortunately, this solution performs poorly and fails to
obtain a good performance on every individual dataset and does not generalize well to unseen do-
mains. To illustrate this, we trained SimCLR on the union of multiple datasets including CIFAR-10
(Krizhevsky et al., a), CIFAR-100 (Krizhevsky et al., b), SVHN (Netzer et al., 2011) and STL-10
(Coates et al.). The trained model is unfavorable as it signiﬁcantly decreases the top-1 accuracy in
all training datasets compared to the single-domain baselines (see Table 1)."
INTRODUCTION,0.017793594306049824,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.021352313167259787,"Figure 1: Framework of Multi-Domain Self-Supervised Learning: Let us consider our input as a
mixture of domains, containing datasets from various sources. We introduce MDSSL, a three-level
hierarchical self-supervised learning approach to perform representation learning on all of these
domains at the same time. Using a standard ResNet-50 encoder, we learn latent representations
that are optimized using the MDSSL objective. We support MDSSL under two setups - with and
without domain labels. When domain labels are not available, we ﬁrst cluster representations to
identify pseudo-domain-labels and then train MDSSL. We use a robust clustering approach, while
recomputing clusters at regular intervals."
INTRODUCTION,0.02491103202846975,"To tackle these issues, we propose Multi-Domain Self-Supervised Learning (MDSSL), a tech-
nique for obtaining a uniﬁed embedder that can be trained on multiple domains. In MDSSL, we
train the model over the union of multiple datasets using a three-level hierarchical loss involving:"
INTRODUCTION,0.028469750889679714,"• Embedding similarities of two views of a sample: In the ﬁrst level, we maximize agree-
ment (i.e. the cosine similarity between l2-normalized vectors) between embeddings of two
augmented views of a given sample."
INTRODUCTION,0.03202846975088968,"• Embedding similarities of samples from a given dataset: In the second level, we mini-
mize the pairwise agreements between embeddings of all samples within a dataset."
INTRODUCTION,0.03558718861209965,"• Embedding similarities of samples from different datasets: In the third level, we mini-
mize the pairwise agreement between samples across all training datasets."
INTRODUCTION,0.03914590747330961,"The ﬁrst two levels ensure that the model learns high quality representations for each individual
domain. The third level of the MDSSL loss encourages the model to learn distinguishable represen-
tations between domains. This approach assumes that domain labels are known during training."
INTRODUCTION,0.042704626334519574,"We also extend MDSSL to more realistic multi-domain setups where domain labels are unknown. In
such scenarios, we present an iterative approach that alternates between clustering and MDSSL at
ﬁxed intervals. We use clustering to detect pseudo-domain-labels for each training dataset and use
these labels in the MDSSL loss. We also propose a robust version of clustering by reducing outlier
noise which further improves the performance of MDSSL in an entirely unsupervised setup."
INTRODUCTION,0.046263345195729534,"In summary, the goal of MDSSL is to compute improved latent representations of samples from
multiple diverse datasets using a single self-supervised model (See Figure 1). We summarize our
contributions as follows:"
INTRODUCTION,0.0498220640569395,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05338078291814947,"• We show that current self-supervised learning techniques such as SimCLR, under multi-
domain setups, show degraded performance on downstream linear classiﬁcation tasks and
do not generalize well to unseen domains."
INTRODUCTION,0.05693950177935943,"• We propose Multi-Domain Self-Supervised Learning (MDSSL) that uses a new loss function
for self-supervised learning that supports training over multiple domains at once and pushes
the model to learn distinguishable representations across datasets."
INTRODUCTION,0.060498220640569395,"• We show that MDSSL trained on a mixture of CIFAR-10, STL-10, SVHN and CIFAR-100,
shows a 25% increase in top-1 accuracy and is more efﬁcient (See Table 1)."
INTRODUCTION,0.06405693950177936,"• We also experiment over DTD and Tiny-ImageNet and show that MDSSL generalizes bet-
ter to unseen domains of varying diversity compared to both single-domain SimCLR and
multi-domain SimCLR."
INTRODUCTION,0.06761565836298933,"• We propose an iterative approach combining MDSSL with clustering to train over multi-
domain datasets without the use of domain labels."
INTRODUCTION,0.0711743772241993,"• We further improve our clustering approach by introducing robust clustering that prevents
outlier noise from affecting domain labels."
INTRODUCTION,0.07473309608540925,Table 1: Comparing SimCLR and MDSSL on single and multi-domain setups
INTRODUCTION,0.07829181494661921,"Train Dataset
Top-1 Accuracy
CIFAR-10
STL-10
SVHN
CIFAR-100
Average"
INTRODUCTION,0.08185053380782918,Single-Domain Training
INTRODUCTION,0.08540925266903915,SimCLR
INTRODUCTION,0.08896797153024912,"CIFAR-10
92.35
56.71
55.97
75.37
70.10
STL-10
71.05
77.58
46.06
63.81
64.62
SVHN
62.83
46.77
92.42
48.27
62.57
CIFAR-100
79.58
55.27
61.16
90.29
71.57"
INTRODUCTION,0.09252669039145907,Multi-Domain Training
INTRODUCTION,0.09608540925266904,SimCLR
INTRODUCTION,0.099644128113879,"CIFAR-10, CIFAR-100,
82.30
61.41
66.65
73.41
70.94
SVHN, STL-10 MDSSL"
INTRODUCTION,0.10320284697508897,"CIFAR-10, CIFAR-100,"
INTRODUCTION,0.10676156583629894,"88.45
65.95
75.35
83.05
78.20
SVHN, STL-10
(λ1 = 1, λ2 = 0.1)"
RELATED WORK,0.1103202846975089,"2
RELATED WORK"
RELATED WORK,0.11387900355871886,"Supervised classiﬁcation techniques involve minimizing a loss function (e.g. the cross-entropy loss)
to match model predictions to true labels. Unsupervised classiﬁcation methods, on the other hand,
learn to classify data without the use of training labels, usually with the use of clustering techniques
(Bojanowski & Joulin, 2017; Dosovitskiy et al., 2014; YM. et al., 2020; Bautista et al., 2016; Caron
et al., 2018; 2019; Huang et al., 2019)."
RELATED WORK,0.11743772241992882,"More recently, new unsupervised techniques called self-supervised representation learning have
been proposed. A self-supervised model learns by observing every instance of the given data and
assigns its own labels, and then performs a classiﬁcation task (Bojanowski & Joulin, 2017; Dosovit-
skiy et al., 2014; Wu et al., 2018; Dosovitskiy et al., 2016). To simplify the complexity of instance-
level classiﬁcation, a memory bank (Wu et al., 2018; He et al., 2020) can be used with the help of
contrastive learning (Gutmann & Hyv¨arinen, 2010; Hjelm et al., 2019; van den Oord et al., 2019;
Grill et al., 2020). Contrastive learning (Arora et al., 2019; Tosh et al., 2021; Bachman et al., 2019)
is a temperature-controlled cross-entropy loss between positive pairs of similar samples and nega-
tive pairs of dissimilar samples. Positive pairs are usually considered as multiple transformations
(views) (Tian et al., 2020) of a given sample using stochastic data augmentation. SimCLR (Chen
et al., 2020) shows that contrastive learning can be done without the use of a memory bank, using
the samples within a batch, if we have large enough batches. SwAV (Caron et al., 2020a) uses a"
RELATED WORK,0.12099644128113879,Under review as a conference paper at ICLR 2022
RELATED WORK,0.12455516014234876,"mixture of contrastive learning and clustering to form a swapped prediction problem that can learn
even with very small batch sizes. Finally, contrastive learning can beneﬁt from training labels, if
available with a simple modiﬁcation of contrasting between samples within a class and taking sam-
ples of other classes as negatives (Khosla et al., 2020). Each of these approaches show remarkable
linear classiﬁcation accuracy on single-domain setups."
RELATED WORK,0.12811387900355872,"Extending self-supervised learning to multiple diverse domains, other than ImageNet (Russakovsky
et al., 2015), is a relatively less explored topic (Wallace & Hariharan, 2020). When multiple related
domains are available during training, a possible approach is to use mutual information to simultane-
ously encode common invariant information and domain-speciﬁc information of each image (Feng
et al., 2019). In our paper, we focus on a general setup where we combine diverse unrelated domains
and evaluate individual domain-speciﬁc tasks."
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.13167259786476868,"3
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS"
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.13523131672597866,"In this section, we deﬁne the Multi-Domain Self-Supervised Learning (MDSSL) paradigm for D
training datasets where domain labels are known. We deﬁne xd
i ∈Rr and ˜xi
d ∈Rr as two trans-
formed views of the ith sample from the dth dataset, d ∈{1, ..., D}. Similar to SimCLR, we use a
base encoder f(.) and a two-layer MLP projection head g(.) to map a given sample into the latent
space. We deﬁne the latent representations of the two views of the ith sample from the dth dataset as
zd
i = f(g(xd
i )) ∈Rr′ and ˜zi
d = f(g( ˜xi
d)) ∈Rr′ where r′ is the size of each latent representation.
We represent mini-batches containing 2N samples (2 views per sample) from D datasets as a matrix
X ∈R2ND×r, whose corresponding latent representation is denoted by Z ∈R2ND×r′."
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.1387900355871886,"We then calculate a similarity matrix S ∈R(2ND)×(2ND) that contains the exponential cosine
similarity scaled by a temperature parameter τ, between all the latent representations in a given
batch. The (i, j)th element of S is:"
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.1423487544483986,"S(i,j) := exp
1"
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.14590747330960854,"τ
zT
i zj
∥zi∥∥zj∥ 
(1)"
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.1494661921708185,"where zi ∈Rr′ and zj ∈Rr′ are the ith and jth row of Z, respectively."
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.15302491103202848,"Sd
i represents the cosine similarity between zd
i and ˜zi
d. Sd,d′"
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITH DOMAIN LABELS,0.15658362989323843,"ij
represents the cross-dataset cosine
similarity between zd
i and zd′
j where d, d′ ∈{1, ..., D}. MDSSL aims to solve the following opti-
mization problem: max
θ"
ND,0.1601423487544484,"1
ND D
X d=1 N
X"
ND,0.16370106761565836,"i=1
log Sd
i ! (2) −λ1 1
2ND D
X d=1"
"N
X",0.16725978647686832,"2N
X"
"N
X",0.1708185053380783,"i=1
log"
"N
X",0.17437722419928825,"2N
X"
"N
X",0.17793594306049823,"j=1
1j̸=iSd,d
ij ! (3) −λ2 1
2ND D
X d=1"
"N
X",0.18149466192170818,"2N
X"
"N
X",0.18505338078291814,"i=1
log D
X d′=1"
"N
X",0.18861209964412812,"2N
X"
"N
X",0.19217081850533807,"j=1
1d′̸=dSd,d′ ij ! (4)"
"N
X",0.19572953736654805,"where θ is the set of model parameters and 0 < λ1 ≤1 and λ2 ≥0 are tunable regularization
parameters. This is a three-level hierarchical loss. (2) maximizes the similarity between two trans-
formed views (xd
i and ˜xi
d) in the latent space. (3) minimizes the similarity between every pair of
samples (xd
i and xd
j) within a dataset. (4) minimizes the similarity between pairs of samples (xd
i and
xd′
j ) across different datasets. When λ1 = 1 and λ2 = 0, this optimization is simpliﬁed to SimCLR
(Chen et al., 2020). Therefore, we use λ1 = 1 and λ2 = 0 as the baseline in all our experiments."
"N
X",0.199288256227758,"λ1 helps us control the extent to which we want to minimize the similarity within a domain. We
empirically observe that relaxing λ1 from the SimCLR baseline (λ1 = 0) to a value slightly less
than 1, in fact generates better structure in the latent space by clustering samples within a domain
relatively closer compared to samples outside a domain (See Appendix Section A.3). λ2 should
always be non-negative as we always want to minimize the agreement between samples of different"
"N
X",0.20284697508896798,Under review as a conference paper at ICLR 2022
"N
X",0.20640569395017794,"datasets. We can implement the MDSSL loss efﬁciently as it only involves calculating the similarity
matrix S once and then selecting elements according to each term mask. As the number of datasets
increases, the size of S increases, gradually increasing the running time of MDSSL."
EXPERIMENTAL SETUP,0.2099644128113879,"3.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.21352313167259787,"We use ResNet-50 (He et al., 2016) as the base encoder (f(.)) and a 2-layer MLP projection head
(g(.)) for all of our experiments. For data augmentation, we use a combination of random crop,
random horizontal ﬂip, random color distortion and random Gaussian blur. In all experiments, the
latent representations are in a 128-dimensional space and τ = 0.1. We optimize our loss using LARS
optimizer (You et al., 2017) with a learning rate of 4 and weight decay of 10−6. We train with a batch
size of 1024 and train over 48, 000 iterations. We experiment with the following datasets: CIFAR-
10 (Krizhevsky et al., a), CIFAR-100 (Krizhevsky et al., b), STL-10 (Coates et al.), SVHN (Netzer
et al., 2011), Tiny-ImageNet (Le & Yang, 2015) and DTD (Describable Textures) (Cimpoi et al.,
2014). We resize all images to 32x32 in all our experiments. We use Nvidia GeForce RTX 2080
GPUs. We measure the quality of representations using the linear evaluation protocol (Kolesnikov
et al., 2019; Bachman et al., 2019; van den Oord et al., 2019) where we train a linear classiﬁer
on top of frozen MDSSL representations and compute the top-1 accuracy of each domain-speciﬁc
classiﬁcation task. Since we train over multiple domains, we compute the top-1 accuracy over each
domain to evaluate the overall model performance."
EXPERIMENTAL SETUP,0.21708185053380782,"Table 2: Resource utilization of SimCLR and MDSSL when trained on CIFAR-10, STL-10, SVHN
and CIFAR-100"
EXPERIMENTAL SETUP,0.2206405693950178,"Resource
Single-Domain SimCLR
MDSSL"
EXPERIMENTAL SETUP,0.22419928825622776,"Training Time (hours)
42.41
34.95 (-17.59%)
Disk Memory (MB)
968
242 (-75%)
Compute (GPUs)
4
2 (-50%)"
MDSSL PERFORMANCE COMPARED TO SIMCLR BASELINE,0.2277580071174377,"3.2
MDSSL PERFORMANCE COMPARED TO SIMCLR BASELINE"
MDSSL PERFORMANCE COMPARED TO SIMCLR BASELINE,0.2313167259786477,"In this section, we analyze the performance of MDSSL and compare it to SimCLR trained on single
domains (referred to as the single-domain SimCLR) and multiple domains (referred to as the multi-
domain SimCLR). Table 1 summarizes our results on CIFAR-10, STL-10, SVHN and CIFAR-100.
We observe that, single-domain SimCLR models generalize poorly on unseen datasets. For example,
SimCLR trained on CIFAR-10 achieves 92.35% top-1 accuracy on CIFAR-10 samples but only
55.97% on SVHN samples."
MDSSL PERFORMANCE COMPARED TO SIMCLR BASELINE,0.23487544483985764,"We also observe that the multi-domain SimCLR model shows a degraded performance when eval-
uated on each individual training domain. For example, SimCLR trained on the union of samples
from CIFAR-10, STL-10, SVHN and CIFAR-100 achieves 82.30% top-1 accuracy on CIFAR-10,
signiﬁcantly lower than the performance of the single-domain SimCLR model trained on CIFAR-
10. Our method, MDSSL, shows a signiﬁcant improvement compared to the multi-domain SimCLR
and almost matches the baseline accuracy of single-domain SimCLR models on some of the training
domains. Among the average top-1 accuracy, we observe up to 25% improvement from the single-
domain SimCLR and a 10% improvement from the multi-domain SimCLR (See Table 1). SimCLR
would require us to train 4 different single-domain models for these datasets and therefore requires
more compute, memory and time. MDSSL, being a uniﬁed model, signiﬁcantly outperforms Sim-
CLR in terms of resource utilization as shown in Table 2. This makes MDSSL an efﬁcient solution
in limited resource environments."
GENERALIZATION TO UNSEEN DATASETS,0.23843416370106763,"3.3
GENERALIZATION TO UNSEEN DATASETS"
GENERALIZATION TO UNSEEN DATASETS,0.24199288256227758,"In this section we evaluate the generalization capacity of MDSSL to unseen domains. We consider
two setups: in the ﬁrst case, we use CIFAR-10, STL-10 and SVHN as training datasets (domains
containing less diverse datasets as their number of classes are ≤10) and evaluate the model perfor-
mance on unseen datasets of CIFAR-100, DTD and Tiny ImageNet (highly diverse datasets whose
number of classes are > 10). In the second case, we use CIFAR-100, DTD and Tiny ImageNet as"
GENERALIZATION TO UNSEEN DATASETS,0.24555160142348753,Under review as a conference paper at ICLR 2022
GENERALIZATION TO UNSEEN DATASETS,0.2491103202846975,Table 3: Generalization of SimCLR and MDSSL to unseen domains
GENERALIZATION TO UNSEEN DATASETS,0.2526690391459075,Train Dataset
GENERALIZATION TO UNSEEN DATASETS,0.25622775800711745,Top-1 Accuracy
GENERALIZATION TO UNSEEN DATASETS,0.2597864768683274,"CIFAR-10
STL-10
SVHN
CIFAR-100
DTD
Tiny-
Average
ImageNet"
GENERALIZATION TO UNSEEN DATASETS,0.26334519572953735,SimCLR
GENERALIZATION TO UNSEEN DATASETS,0.2669039145907473,"CIFAR-10
92.35
56.71
55.97
75.37
40.50
19.95
56.80
STL-10
71.05
77.58
46.06
63.81
39.22
21.41
53.18
SVHN
62.83
46.77
92.42
48.27
36.47
13.42
50.03
CIFAR-100
79.58
55.27
61.16
90.29
42.24
21.36
58.31
DTD
64.95
51.68
49.86
55.98
50.43
19.40
48.71
Tiny-ImageNet
81.67
63.20
53.75
82.69
44.03
37.99
60.55
ImageNet (250K)
68.16
75.43
49.09
50.03
50.57
21.00
52.38"
GENERALIZATION TO UNSEEN DATASETS,0.2704626334519573,"Multi-Domain Training
Average
Average
(Training domains)
(Unseen domains)"
GENERALIZATION TO UNSEEN DATASETS,0.27402135231316727,SimCLR
GENERALIZATION TO UNSEEN DATASETS,0.2775800711743772,"CIFAR-10, STL-10,
83.96
63.23
72.10
71.72
47.94
22.67
73.09
47.44
SVHN MDSSL"
GENERALIZATION TO UNSEEN DATASETS,0.28113879003558717,"CIFAR-10, STL-10,"
GENERALIZATION TO UNSEEN DATASETS,0.2846975088967972,"87.50
65.58
88.05
76.48
49.36
24.49
80.37
50.11
SVHN
(λ1 = 0.9, λ2 = 0.1)"
GENERALIZATION TO UNSEEN DATASETS,0.28825622775800713,SimCLR
GENERALIZATION TO UNSEEN DATASETS,0.2918149466192171,"CIFAR-100, DTD,
77.27
59.22
68.06
75.72
51.82
28.40
51.98
68.18
Tiny ImageNet MDSSL"
GENERALIZATION TO UNSEEN DATASETS,0.29537366548042704,"CIFAR-100, DTD,"
GENERALIZATION TO UNSEEN DATASETS,0.298932384341637,"81.92
62.89
72.35
83.93
54.77
30.18
56.29
72.38
Tiny ImageNet
(λ1 = 0.9, λ2 = 0.05)"
GENERALIZATION TO UNSEEN DATASETS,0.302491103202847,SimCLR
GENERALIZATION TO UNSEEN DATASETS,0.30604982206405695,"ImageNet (250K),
76.95
74.87
59.82
69.95
52.11
27.99
64.88
57.98
CIFAR-100, SVHN MDSSL"
GENERALIZATION TO UNSEEN DATASETS,0.3096085409252669,"ImageNet (250K),"
GENERALIZATION TO UNSEEN DATASETS,0.31316725978647686,"81.86
77.01
78.64
80.16
55.30
33.04
79.40
61.80
CIFAR-100, SVHN
(λ1 = 0.9, λ2 = 0.1)"
GENERALIZATION TO UNSEEN DATASETS,0.3167259786476868,"our training datasets and assess the performances on CIFAR-10, STL-10 and SVHN. We also add
results on ImageNet (250K) which contains 1000 classes, each including 250 samples resized to
32x32."
GENERALIZATION TO UNSEEN DATASETS,0.3202846975088968,"Table 3 summarizes our results. Among the single-domain SimCLR models, we observe that Sim-
CLR trained on Tiny-ImageNet generalizes relatively better than other single-domain models since
Tiny-ImageNet is comparatively larger and most diverse. However, the drop in top-1 accuracy of
unseen domains from the baseline is very signiﬁcant even for the single-domain SimCLR trained
on Tiny-ImageNet (42% drop for SVHN). Similarly, ImageNet (250K) also poorly generalizes to
unseen domains."
GENERALIZATION TO UNSEEN DATASETS,0.3238434163701068,"In our ﬁrst multi-domain setup (with training datasets of CIFAR-10, STL-10 and SVHN), we ob-
serve that although these training datasets are relatively less diverse, MDSSL generalizes remarkably
well on more diverse datasets like CIFAR-100, DTD and Tiny-ImageNet. MDSSL also outperforms
the multi-domain SimCLR in all domains (training and unseen). We observe a similar improve-
ment when we train MDSSL on CIFAR-100, DTD and Tiny-ImageNet and on ImageNet (250K),
CIFAR-100 and SVHN. MDSSL outperforms both single and multi-domain SimCLR in terms of
generalization capacity. These results highlight that MDSSL is a favorable solution that achieves
good accuracy on training domains and generalizes well to unseen domains."
EFFECT OF NUMBER OF TRAINING DATASETS,0.3274021352313167,"3.4
EFFECT OF NUMBER OF TRAINING DATASETS"
EFFECT OF NUMBER OF TRAINING DATASETS,0.3309608540925267,"In this section, we discuss the behavior of MDSSL as we increase the number of train-
ing domains.
We train MDSSL on CIFAR-10 and CIFAR-100 (2-domain baseline).
We
then add STL-10, SVHN, Tiny-ImageNet and DTD datasets one by one and train MDSSL."
EFFECT OF NUMBER OF TRAINING DATASETS,0.33451957295373663,Under review as a conference paper at ICLR 2022
EFFECT OF NUMBER OF TRAINING DATASETS,0.33807829181494664,"2
3
4
5
6
Number of Training Datasets 70 75 80 85 90 95"
EFFECT OF NUMBER OF TRAINING DATASETS,0.3416370106761566,Top-1 Accuracy
EFFECT OF NUMBER OF TRAINING DATASETS,0.34519572953736655,"SimCLR (CIFAR-10) Baseline
SimCLR (CIFAR-100) Baseline
CIFAR-10
CIFAR-100"
EFFECT OF NUMBER OF TRAINING DATASETS,0.3487544483985765,"Figure 2:
Effect of number of
training datasets. In this plot, we
show that when the number of train-
ing datasets increases in MDSSL,
the top-1 accuracy increases and
eventually beats the SimCLR single-
domain baseline, marked by dotted
lines. We train MDSSL on CIFAR-
10, CIFAR-100, STL-10, SVHN,
Tiny-ImageNet and DTD."
EFFECT OF NUMBER OF TRAINING DATASETS,0.35231316725978645,"In Figure 2, we observe that as the number of datasets in-
creases, the top-1 accuracy also increases and ﬁnally beats
the single-domain baseline.
Therefore, MDSSL beneﬁts
from training over a large number of datasets."
HYPERPARAMETER SELECTION,0.35587188612099646,"3.5
HYPERPARAMETER SELECTION"
HYPERPARAMETER SELECTION,0.3594306049822064,"The MDSSL loss is controlled by two regularizers λ1 and λ2,
as shown in Section 3. When λ1 = 1 and λ2 = 0, MDSSL
boils down to our baseline, SimCLR (Chen et al., 2020). As
we decrease λ1 while ﬁxing λ2 = 0, we observe that the in-
distribution similarity increases (See Appendix Section A.3)
and eventually, all samples show a mutual similarity of 1.
Consequently, the top-1 accuracy quickly degrades from the
baseline as shown in the ﬁrst plot in Figure 3. This behavior
can be explained by Term 3 of the MDSSL loss in Section
3 which measures the mutual similarity between all samples
within a dataset. Therefore, we ﬁx λ1 ≥0.9 so that it mildly
increases in-distribution similarity without signiﬁcantly af-
fecting the top-1 accuracy. We utilize Term 4 of the MDSSL
loss by controlling λ2, to ensure that domains are more dis-
tinguishable in the latent space. In Figure 3, the second plot
shows the top-1 accuracy as we increase λ2. The top-1 accu-
racy rises steadily at ﬁrst, and then drops at around λ2 = 0.2.
This is because after a certain threshold, the in-distribution
representations become too similar which makes them harder to classify. Therefore, a good balance
should be found between λ1 and λ2 such that we achieve favourable top-1 accuracy."
HYPERPARAMETER SELECTION,0.36298932384341637,"0.0
0.2
0.4
0.6
0.8
1.0 1 20 40 60 80"
HYPERPARAMETER SELECTION,0.3665480427046263,Top-1 Accuracy 2 = 0
HYPERPARAMETER SELECTION,0.3701067615658363,"CIFAR-10
SVHN"
HYPERPARAMETER SELECTION,0.3736654804270463,"0.0
0.1
0.2
0.3
0.4
0.5 2 20 40 60 80"
HYPERPARAMETER SELECTION,0.37722419928825623,Top-1 Accuracy
HYPERPARAMETER SELECTION,0.3807829181494662,1 = 0.9
HYPERPARAMETER SELECTION,0.38434163701067614,"CIFAR-10
SVHN"
HYPERPARAMETER SELECTION,0.3879003558718861,"Figure 3: Top-1 accuracy of MDSSL trained on CIFAR-10 and SVHN with varying λ1 (similar-
ity within dataset) and λ2 (similarity across datasets). In the ﬁrst plot, we observe that increasing
λ1 from −1 (SimCLR Chen et al. (2020)) quickly drops the top-1 accuracy since samples within a
dataset become more and more indistinguishable. In the second plot, when λ1 = −0.9, the top-1
accuracy steadily improves with λ2 until a threshold (λ2 = 0.2) and then drops. These plots show
that there is a sweet-spot in selecting λ1 and λ2 such that we achieve high top-1 accuracy."
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.3914590747330961,"4
MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.39501779359430605,"Most real-world multi-domain datasets are unlabelled (i.e., domain label information is not avail-
able). In this section, we develop an extension of MDSSL for such setups by identifying pseudo
domain labels via a clustering approach in the latent space. As it is common in clustering, we
assume the number of domains (denoted by M) is known."
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.398576512455516,Under review as a conference paper at ICLR 2022
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.40213523131672596,"In the MDSSL loss (especially in Term 4), we need domain labels to compute pairwise similarities
of samples from two different domains. To achieve this, we ﬁrst treat the problem as single-domain
self-supervised learning and warm up the MDSSL encoder for the ﬁrst few training iterations using
the optimization described in Section 3 with =λ1 = 1, λ2 = 0 and D = 1 (i.e., SimCLR training
on one domain). This warm up helps us get somewhat distinguishable representations for samples
between M domains and the number of iterations to warm up is determined empirically. At the
end of the warm up, we cluster the latent representations of the entire multi-domain dataset into
M clusters using K-Means clustering (Hartigan & Wong, 1979). Using these clusters as pseudo-
domain-labels, we continue training the encoder under the MDSSL loss with λ1 ≤1, λ2 > 0
and D = M. As the training progresses, MDSSL improves the latent structure and therefore, we
recompute clusters multiple times (determined empirically) as the training progresses to ensure that
improved domain labels are used."
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.40569395017793597,"In practice, we observe that clustering does not provide 100% accurate domain labels, especially for
datasets that are distributionally similar such as CIFAR-10 and STL-10. In such cases, we propose
to use a robust clustering approach coupled with MDSSL to prevent outlier clustering noise from
affecting the MDSSL training. Let us consider a MDSSL encoder that is warmed up on a multi-
domain dataset containing M domains. We cluster the representations of this dataset into M clusters
with centroids c1, c2, . . . , cM. Before assigning pseudo-domain-labels to each representation, we
ﬁrst determine if they are outliers or not. If so, we ignore these samples in training MDSSL in the
next round. We say a latent sample zi is not an outlier if it is signiﬁcantly closer to one of the
clustering centroids compared to another. Concretely, zi is not an outlier if"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4092526690391459,"max
∥zi −cm∥2"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4128113879003559,"∥zi −cn∥2
: 1 ≤m ≤M, 1 ≤n ≤M

> 1 + ϵ
(5)"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.41637010676156583,"where ϵ ≥0 is deﬁned as an outlier threshold. When ϵ is high, it means that the given sample is close
to its respective centroid. When ϵ approaches 0, it indicates that the sample is almost equidistant
from at least two centroids and therefore, may not be reliably clustered into one. We ignore such
samples in MDSSL training. When we perform clustering for the ﬁrst time, we start with ϵ = 1 and
each time we repeat clustering, we decay its value exponentially such that it approaches 0 by the end
of training to ensure that at the end, all samples contribute to the MDSSL training."
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4199288256227758,"75
50
25
0
25
50
75
60 40 20 0 20 40 60"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4234875444839858,MDSSL with Clustering
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.42704626334519574,"CIFAR-10
SVHN"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4306049822064057,"100
75
50
25
0
25
50
75 60 40 20 0 20 40 60 80"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.43416370106761565,MDSSL with Robust Clustering
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4377224199288256,"CIFAR-10
SVHN"
"MULTI-DOMAIN SELF-SUPERVISED LEARNING WITHOUT DOMAIN
LABELS",0.4412811387900356,"Figure 4: Latent Space of MDSSL with Clustering: We use TSNE to visualize the latent space of
MDSSL with clustering on CIFAR-10 and SVHN. We observe that clustering helps us distinguish
between domains and this improves when we apply robust clustering as shown above."
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.44483985765124556,"4.1
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.4483985765124555,"Our experimental setup for training MDSSL without domain labels remains the same as the one we
explain in Section 3.1. We perform the SimCLR warmup for 480 iterations and update the clus-
ters every 2, 400 iterations going forward. In this section, we consider two mixtures for training
datasets: (i) CIFAR-10 and SVHN (containing visually dissimilar samples), (ii) CIFAR-10, STL-10"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.45195729537366547,Under review as a conference paper at ICLR 2022
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.4555160142348754,"(containing visually similar samples). In Figure 4, we plot the TSNE of CIFAR-10 and SVHN sam-
ple embeddings while using either clustering or robust clustering approaches. We observe that with
clustering, we achieve a reasonable separation between domains although there are several outliers.
These outliers are signiﬁcantly reduced while using robust clustering as described in Equation 5. As
a result, the clusters are quite well deﬁned and easily distinguishable."
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.45907473309608543,"In Figure 5, we plot the top-1 accuracy of 5 training setups: single-domain SimCLR, multi-domain
SimCLR, MDSSL (with domain labels), MDSSL (with clustering) and MDSSL (with robust cluster-
ing). When trained on CIFAR-10 and SVHN, we observe that MDSSL with clustering outperforms
SimCLR on both datasets and on CIFAR-100 which is an unseen domain. We also observe that
MDSSL with clustering seems to generalize better to CIFAR-100 compared to MDSSL. MDSSL
with clustering also outperforms SimCLR when trained on CIFAR-10 and STL-10 which are more
visually similar. We also observe that applying robust clustering shows an improvement on all do-
mains including unseen domains (CIFAR-100). These results highlight that clustering is a useful
approach to identify pseudo-domain-labels and when coupled with MDSSL, it helps us learn better
representations for seen and unseen domains."
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.4626334519572954,"CIFAR-10
SVHN
CIFAR-100
50 55 60 65 70 75 80 85 90 95"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.46619217081850534,Top-1 Accuracy
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.4697508896797153,"Train Datasets - CIFAR-10, SVHN"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.47330960854092524,"CIFAR-10
STL-10
CIFAR-100
50 55 60 65 70 75 80 85 90 95"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.47686832740213525,Top-1 Accuracy
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.4804270462633452,"Train Datasets - CIFAR-10, STL-10"
PERFORMANCE OF MDSSL TRAINED WITHOUT DOMAIN LABELS,0.48398576512455516,"Figure 5: MDSSL trained with Clustering: We train MDSSL using clustering and robust cluster-
ing on CIFAR-10 and SVHN (left) and CIFAR-10 and STL-10 (right). We observe that, although
domain labels are not used in clustering, we are able to improve the performance of MDSSL com-
pared to SimCLR."
DISCUSSION,0.4875444839857651,"5
DISCUSSION"
DISCUSSION,0.49110320284697506,"We propose Multi-Domain Self-Supervised Learning (MDSSL), as a unifying approach to compute
self-supervised representations for a range of datasets. We support training MDSSL under two se-
tups: with domain labels and without domain labels. We show that MDSSL achieves up to a 25%
increase in top-1 accuracy with linear evaluation compared to the SimCLR baseline on a combi-
nation of CIFAR-10, STL-10, SVHN and CIFAR-100. We also show that MDSSL is signiﬁcantly
more efﬁcient than SimCLR in terms of resource (time, compute and memory) utilization, gener-
alizes better than SimCLR in multi-domain setting, and beneﬁts from an increase in the number
of training datasets. In addition, we propose two versions of clustering that can be coupled with
MDSSL when training over multiple domains without the use of domain labels. MDSSL achieves
good performance even under these entirely unsupervised setups. Our uniﬁed approach, MDSSL, is
general-purpose, enables training on diverse multi-domain settings, and can obtain meaningful em-
beddings achieving state-of-the-art results both on seen (training) and unseen benchmark datasets."
DISCUSSION,0.49466192170818507,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.498220640569395,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.501779359430605,"We share our code in the supplementary materials. We also provide several implementation details
to ensure reproducibility of all our experiments. In Sections 3.1 and A.5, we provide a detailed
explanation of our training setup including the architecture of our encoder, optimizers, learning
rate schedule and training hyperparameters. We explain the process of hyperparameter selection in
Sections 3.5 and A.3."
ETHICS STATEMENT,0.505338078291815,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.5088967971530249,"We use only publicly available datasets which involve classiﬁcation tasks on general objects, vehi-
cles, animals, etc. To the best of our knowledge, our work does not have a negative impact on our
society or any societal group. However, as with all machine learning models, MDSSL should not be
used on datasets that are inherently biased or involve harmful tasks that target or affect any particular
regional, cultural or societal group. Therefore, before running MDSSL, one must select datasets and
downstream tasks such that they are safe and do not amplify any social biases."
ETHICS STATEMENT,0.5124555160142349,Under review as a conference paper at ICLR 2022
REFERENCES,0.5160142348754448,REFERENCES
REFERENCES,0.5195729537366548,"Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning, 2019."
REFERENCES,0.5231316725978647,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximiz-
ing mutual information across views. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 32. Curran Associates, Inc., 2019.
URL https://proceedings.neurips.cc/
paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf."
REFERENCES,0.5266903914590747,"Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn Ommer. Cliquecnn:
Deep unsupervised exemplar learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf."
REFERENCES,0.5302491103202847,"Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In Doina Precup
and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learning Research, pp. 517–526. PMLR, 06–11 Aug
2017. URL http://proceedings.mlr.press/v70/bojanowski17a.html."
REFERENCES,0.5338078291814946,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), September 2018."
REFERENCES,0.5373665480427047,"Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of
image features on non-curated data. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), October 2019."
REFERENCES,0.5409252669039146,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin.
Unsupervised learning of visual features by contrasting cluster assignments.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems,
volume 33,
pp. 9912–9924. Curran Asso-
ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/
70feb62b69f16e0238f741fab228fec2-Paper.pdf."
REFERENCES,0.5444839857651246,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin.
Unsupervised learning of visual features by contrasting cluster assignments.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems,
volume 33,
pp. 9912–9924. Curran Asso-
ciates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
70feb62b69f16e0238f741fab228fec2-Paper.pdf."
REFERENCES,0.5480427046263345,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations.
In Hal Daum´e III and Aarti Singh (eds.),
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro-
ceedings of Machine Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020. URL http:
//proceedings.mlr.press/v119/chen20j.html."
REFERENCES,0.5516014234875445,"M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014."
REFERENCES,0.5551601423487544,"Adam Coates, Honglak Lee, and Andrew Y. Ng. Stanford stl-10 image dataset. URL https:
//cs.stanford.edu/˜acoates/stl10/."
REFERENCES,0.5587188612099644,"Alexey
Dosovitskiy,
Jost
Tobias
Springenberg,
Martin
Riedmiller,
and
Thomas
Brox.
Discriminative unsupervised feature learning with convolutional neural networks.
In
Z. Ghahramani, M. Welling,
C. Cortes,
N. Lawrence,
and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems,
volume 27. Curran Associates,
Inc.,
2014.
URL
https://proceedings.neurips.cc/paper/2014/file/
07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf."
REFERENCES,0.5622775800711743,Under review as a conference paper at ICLR 2022
REFERENCES,0.5658362989323843,"Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas
Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(9):1734–1747, 2016. doi:
10.1109/TPAMI.2015.2496141."
REFERENCES,0.5693950177935944,"William
Falcon
et
al.
Pytorch
lightning.
GitHub.
Note:
https://github.com/PyTorchLightning/pytorch-lightning, 3, 2019."
REFERENCES,0.5729537366548043,"Zeyu Feng, Chang Xu, and Dacheng Tao.
Self-supervised representation learning from multi-
domain data. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3244–
3254, 2019."
REFERENCES,0.5765124555160143,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent -
a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
21271–21284. Curran Associates, Inc., 2020.
URL https://proceedings.neurips.
cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf."
REFERENCES,0.5800711743772242,"Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Yee Whye Teh and Mike Titterington (eds.), Proceedings
of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 9 of
Proceedings of Machine Learning Research, pp. 297–304, Chia Laguna Resort, Sardinia, Italy,
13–15 May 2010. PMLR. URL http://proceedings.mlr.press/v9/gutmann10a.
html."
REFERENCES,0.5836298932384342,"J. A. Hartigan and M. A. Wong. A k-means clustering algorithm. JSTOR: Applied Statistics, 28(1):
100–108, 1979."
REFERENCES,0.5871886120996441,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778, 2016. doi: 10.1109/CVPR.2016.90."
REFERENCES,0.5907473309608541,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 9726–9735, 2020. doi: 10.1109/CVPR42600.2020.00975."
REFERENCES,0.594306049822064,"Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset
and deep learning benchmark for land use and land cover classiﬁcation. IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing, 2019."
REFERENCES,0.597864768683274,"R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio.
Learning deep representations by mutual information estima-
tion and maximization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=Bklr3j0cKX."
REFERENCES,0.6014234875444839,"Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian Zhu. Unsupervised deep learning by neigh-
bourhood discovery. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 2849–2858. PMLR, 09–15 Jun 2019. URL http://proceedings.
mlr.press/v97/huang19b.html."
REFERENCES,0.604982206405694,"Daniel S. Kermany, Michael Goldbaum, Wenjia Cai, Carolina C.S. Valentim, Huiying Liang,
Sally L. Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, Justin Dong, Made K.
Prasadha, Jacqueline Pei, Magdalene Y.L. Ting, Jie Zhu, Christina Li, Sierra Hewett, Jason Dong,
Ian Ziyar, Alexander Shi, Runze Zhang, Lianghong Zheng, Rui Hou, William Shi, Xin Fu, Yaou
Duan, Viet A.N. Huu, Cindy Wen, Edward D. Zhang, Charlotte L. Zhang, Oulan Li, Xiaobo
Wang, Michael A. Singer, Xiaodong Sun, Jie Xu, Ali Tafreshi, M. Anthony Lewis, Huimin Xia,
and Kang Zhang. Identifying medical diagnoses and treatable diseases by image-based deep
learning. Cell, 172(5):1122–1131.e9, 2018. ISSN 0092-8674. doi: https://doi.org/10.1016/j.cell.
2018.02.010.
URL https://www.sciencedirect.com/science/article/pii/
S0092867418301545."
REFERENCES,0.608540925266904,Under review as a conference paper at ICLR 2022
REFERENCES,0.6120996441281139,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,
Aaron Maschinot, Ce Liu, and Dilip Krishnan.
Supervised contrastive learning.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 18661–18673. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf."
REFERENCES,0.6156583629893239,"Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual repre-
sentation learning. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1920–1929, 2019. doi: 10.1109/CVPR.2019.00202."
REFERENCES,0.6192170818505338,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search). a. URL http://www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.6227758007117438,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced
research). b. URL http://www.cs.toronto.edu/˜kriz/cifar.html."
REFERENCES,0.6263345195729537,Ya Le and X. Yang. Tiny imagenet visual recognition challenge. 2015.
REFERENCES,0.6298932384341637,"Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-
tions. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.
6706–6716, 2020. doi: 10.1109/CVPR42600.2020.00674."
REFERENCES,0.6334519572953736,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/
housenumbers/nips2011_housenumbers.pdf."
REFERENCES,0.6370106761565836,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y."
REFERENCES,0.6405693950177936,"Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
6827–6839. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf."
REFERENCES,0.6441281138790036,"Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models, 2021."
REFERENCES,0.6476868327402135,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding, 2019."
REFERENCES,0.6512455516014235,"Bram Wallace and Bharath Hariharan. Extending and analyzing self-supervised learning across
domains. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Com-
puter Vision – ECCV 2020, pp. 717–734, Cham, 2020. Springer International Publishing. ISBN
978-3-030-58574-7."
REFERENCES,0.6548042704626335,"Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018."
REFERENCES,0.6583629893238434,"Asano YM., Rupprecht C., and Vedaldi A.
Self-labelling via simultaneous clustering and rep-
resentation learning.
In International Conference on Learning Representations, 2020.
URL
https://openreview.net/forum?id=Hyx-jyBFPr."
REFERENCES,0.6619217081850534,"Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv:
Computer Vision and Pattern Recognition, 2017."
REFERENCES,0.6654804270462633,Under review as a conference paper at ICLR 2022
REFERENCES,0.6690391459074733,"A
APPENDIX"
REFERENCES,0.6725978647686833,"A.1
EXPERIMENTS ON NON-OBJECT-FOCUSED DATASETS"
REFERENCES,0.6761565836298933,"In this section we discuss the results on 3 datasets that are not object-focused i.e, EuroSAT (Helber
et al., 2019), Chest X-Ray (Pneumonia) (Kermany et al., 2018) and DTD (Cimpoi et al., 2014).
These datasets are understandably not generalizable to unseen domains as shown in Table 4. How-
ever, under a multi-domain setup where we combine all of these domains, MDSSL shows a modest
improvement compared to multi-domain SimCLR and almost matches the single-domain baselines."
REFERENCES,0.6797153024911032,Table 4: Comparing SimCLR and MDSSL on diverse non-object-focused datasets
REFERENCES,0.6832740213523132,"Train Dataset
Top-1 Accuracy
EuroSAT
ChestXRay
DTD
Average"
REFERENCES,0.6868327402135231,Single-Domain Training
REFERENCES,0.6903914590747331,"EuroSAT
88.95
93.57
45.75
76.09
ChestXRay
85.03
95.29
46.20
75.50
DTD
86.11
93.41
50.43
76.65"
REFERENCES,0.693950177935943,Multi-Domain Training
REFERENCES,0.697508896797153,SimCLR
REFERENCES,0.701067615658363,"EuroSAT, ChestXRay,
86.02
94.27
46.78
75.69
DTD MDSSL"
REFERENCES,0.7046263345195729,"EuroSAT, ChestXRay,"
REFERENCES,0.708185053380783,"87.10
94.43
50.23
77.25
DTD
(λ1 = 0.9, λ2 = 0.15)"
REFERENCES,0.7117437722419929,"A.2
COMPARING MDSSL WITH SIMCLR PRE-TRAINED ON IMAGENET"
REFERENCES,0.7153024911032029,"In this section, we use a pre-trained SimCLR encoder from Pytorch Lightning Bolts (Falcon et
al., 2019) and train a linear classiﬁer on several unseen datasets. We resize all images to 32x32
during linear classiﬁcation to maintain consistency with the rest of our experiments. We realize this
may be an unfair comparison since the encoder is pre-trained on 224x224 images. Nevertheless,
we observe a signiﬁcant improvement in generalization of MDSSL over SimCLR pre-trained on
full-sized ImageNet on all datasets (See Table 5)."
REFERENCES,0.7188612099644128,Table 5: Comparing SimCLR pre-trained on full ImageNet with MDSSL
REFERENCES,0.7224199288256228,Train Dataset
REFERENCES,0.7259786476868327,Top-1 Accuracy
REFERENCES,0.7295373665480427,"CIFAR-10
STL-10
SVHN
CIFAR-100
DTD
Tiny-
Average
ImageNet"
REFERENCES,0.7330960854092526,"ImageNet
68.21
58.72
49.05
50.11
47.36
20.85
49.05"
REFERENCES,0.7366548042704626,Multi-Domain Training MDSSL
REFERENCES,0.7402135231316725,"CIFAR-10, STL-10,"
REFERENCES,0.7437722419928826,"87.50
65.58
88.05
76.48
49.36
24.49
65.24
SVHN
(λ1 = 0.9, λ2 = 0.1) MDSSL"
REFERENCES,0.7473309608540926,"CIFAR-100, DTD,"
REFERENCES,0.7508896797153025,"81.92
62.89
72.35
83.93
54.77
30.18
64.34
Tiny ImageNet
(λ1 = 0.9, λ2 = 0.05)"
REFERENCES,0.7544483985765125,Under review as a conference paper at ICLR 2022
REFERENCES,0.7580071174377224,"A.3
HYPERPARAMETER SELECTION"
REFERENCES,0.7615658362989324,"In Figure A.1, we plot the similarity matrices of class-averaged samples of MDSSL trained on
CIFAR-10 and SVHN by ﬁxing λ2 = 0 and varying λ1. As explained in Section 3, higher λ1 in-
creases the similarity of samples within a domain. We observe that when λ1 = −1 and λ2 = 0
(SimCLR), the similarity within a domain is comparable with the similarity across domains, mean-
ing that, domains are indistinguishable. As we increase λ1, we see that the similarity within domains
increases and eventually, all samples show a mutual similarity of 1."
REFERENCES,0.7651245551601423,"In Figure A.1, although similarity within domains increases, we still cannot distinguish between
domains. To achieve this, we utilize Term 4 of the MDSSL loss by controlling λ2. In Figure A.2,
we vary λ2 and ﬁx λ1 = −1 (ﬁrst row) and λ1 = −0.9 (second row). As λ2 increases, the similarity
across domains decreases and each domain become clearly distinguishable. When λ1 = −0.9, the
effect is seen even at lower values of λ2, as MDSSL learns to simultaneously increase similarity
within domains while decreasing similarity across domains."
REFERENCES,0.7686832740213523,CIFAR-10 SVHN
REFERENCES,0.7722419928825622,CIFAR-10 SVHN 1 = 1 2 = 0 0.2 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7758007117437722,CIFAR-10 SVHN
REFERENCES,0.7793594306049823,CIFAR-10 SVHN
REFERENCES,0.7829181494661922,1 = 0.9 2 = 0 0.2 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.7864768683274022,CIFAR-10 SVHN
REFERENCES,0.7900355871886121,CIFAR-10 SVHN
REFERENCES,0.7935943060498221,1 = 0.7 2 = 0 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.797153024911032,CIFAR-10 SVHN
REFERENCES,0.800711743772242,CIFAR-10 SVHN
REFERENCES,0.8042704626334519,1 = 0.5 2 = 0 0.2 0.0 0.2 0.4 0.6 0.8 1.0
REFERENCES,0.8078291814946619,CIFAR-10 SVHN
REFERENCES,0.8113879003558719,CIFAR-10 SVHN 1 = 0 2 = 0 3.5 3.0 2.5 2.0 1.5 1.0 0.5
REFERENCES,0.8149466192170819,"0.0
1e
7+1"
REFERENCES,0.8185053380782918,CIFAR-10 SVHN
REFERENCES,0.8220640569395018,CIFAR-10 SVHN
REFERENCES,0.8256227758007118,"1 =
0.1 2 = 0 1.5 1.0 0.5 0.0 0.5 1.0"
E,0.8291814946619217,"1e
7+1"
E,0.8327402135231317,"Figure A.1: MDSSL trained on CIFAR-10 and SVHN with λ2 = 0 and varying λ1. These
heatmaps represent the similarity matrices between class-averaged representations of CIFAR-10 and
SVHN. The ﬁrst matrix in the ﬁrst row represents our baseline, SimCLR (Chen et al., 2020). As λ1
increases, the mutual similarity between samples within a domain increases. When λ1 goes over 0
the mutual similarity between all training samples effectively reaches 1."
E,0.8362989323843416,"A.4
RUNNING TIME OF MDSSL"
E,0.8398576512455516,"The MDSSL optimization, as discussed in Section 3, is solved by iterating over the number of
training datasets (D) in each term. Therefore, as the number of datasets increases, the running time
of MDSSL will increase accordingly. We use a large batch size of 1024 which also accounts for
increased running time for larger datasets. In Figure A.3, we see that as the number of training
datasets increases, number of training hours of MDSSL also increases."
E,0.8434163701067615,"A.5
IMPLEMENTATION DETAILS"
E,0.8469750889679716,"Table 6 summarizes the entire architecture of each component of MDSSL with the ﬁlter and output
dimensions for input image size 3 × 32 × 32. Our implementation of MDSSL is on PyTorch. We"
E,0.8505338078291815,Under review as a conference paper at ICLR 2022
E,0.8540925266903915,CIFAR-10 SVHN
E,0.8576512455516014,CIFAR-10 SVHN 1 = 1
E,0.8612099644128114,2 = 0.05 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
E,0.8647686832740213,CIFAR-10 SVHN
E,0.8683274021352313,CIFAR-10 SVHN 1 = 1
E,0.8718861209964412,2 = 0.1 0.2 0.0 0.2 0.4 0.6 0.8 1.0
E,0.8754448398576512,CIFAR-10 SVHN
E,0.8790035587188612,CIFAR-10 SVHN 1 = 1
E,0.8825622775800712,2 = 0.2 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
E,0.8861209964412812,CIFAR-10 SVHN
E,0.8896797153024911,CIFAR-10 SVHN
E,0.8932384341637011,1 = 0.9
E,0.896797153024911,2 = 0.05 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0
E,0.900355871886121,CIFAR-10 SVHN
E,0.9039145907473309,CIFAR-10 SVHN
E,0.9074733096085409,1 = 0.9
E,0.9110320284697508,2 = 0.1 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
E,0.9145907473309609,CIFAR-10 SVHN
E,0.9181494661921709,CIFAR-10 SVHN
E,0.9217081850533808,1 = 0.9
E,0.9252669039145908,2 = 0.2 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00
E,0.9288256227758007,"Figure A.2: MDSSL trained on CIFAR-10 and SVHN with λ1 = −1 (ﬁrst row) and λ1 = −0.9
(second row) and varying λ2. These heatmaps represent the similarity matrices between class-
averaged representations of CIFAR-10 and SVHN. As λ2 increases, the mutual similarity between
samples across domains decreases. When λ1 = −0.9 MDSSL learns to push samples within a
domain closer while simultaneously reducing the similarity of samples across domains."
E,0.9323843416370107,"2
3
4
5
6
Number of Training Datasets 0 20 40 60 80"
E,0.9359430604982206,Running Time (hours)
E,0.9395017793594306,"Figure A.3: Running Time of MDSSL. In this plot, we show how the running time increases when
the number of training datasets increases in MDSSL. We train MDSSL on the following datasets in
the given order: CIFAR-10, SVHN, STL-10, CIFAR-100, DTD, Tiny-ImageNet."
E,0.9430604982206405,"use ResNet-50 (He et al., 2016) as the base encoder for all our experiments. Since we have multiple
datasets during training, we prepare a DataLoader for each dataset and load batches of size 1024
from each dataset. We refer to these as dataset batches. When the number of training datasets is
low, we concatenate all dataset batches (X) and pass it through the encoder (f(.)) and projection
head (g(.)) to get Z. However, when the number of training datasets increases, X becomes too large
and may require more memory to encode. In this case, we ﬁrst separately encode every dataset batch
and then concatenate all dataset embeddings to get Z. This trick helps us efﬁciently train MDSSL
on 2 GPUs with 4 training datasets and a high batch size of 1024."
E,0.9466192170818505,"In Section 3.1, we discuss the experimental setup with hyperparameters for MDSSL training. We
summarize these parameters in Table 7. We evaluate MDSSL using the linear evaluation protocol"
E,0.9501779359430605,Under review as a conference paper at ICLR 2022
E,0.9537366548042705,"Table 6: Architecture of MDSSL encoder, projection head and linear classiﬁer
MDSSL Component
Layer
Output Size
Filters"
E,0.9572953736654805,ResNet-50 Encoder
E,0.9608540925266904,"Conv2d
64 × 16 × 16
7 × 7, 64, stride 2, padding 3
BatchNorm
64 × 16 × 16
64
RelU
64 × 16 × 16
-
MaxPool2d
64 × 8 × 8
3 × 3, stride 2, padding 1
Bottleneck
256 × 8 × 8
planes 64, blocks 3
Bottleneck
512 × 4 × 4
planes 128, blocks 4
Bottleneck
1024 × 2 × 2
planes 256, blocks 6
Bottleneck
2048 × 1 × 1
planes 512, blocks 3
AdaptiveAvgPool2d
2048 × 1 × 1
1 × 1"
E,0.9644128113879004,Projection Head
E,0.9679715302491103,"Linear
2048
2048
RelU
2048
-
Linear
128
128"
E,0.9715302491103203,"Linear Classiﬁer
Linear
10
10"
E,0.9750889679715302,"Table 7: Hyperparameter details for MDSSL encoder, projection head and linear classiﬁer"
E,0.9786476868327402,"MDSSL Component
Parameter
Value"
E,0.9822064056939501,Encoder and Projection Head
E,0.9857651245551602,"Latent Dimension
128
Temperature
0.1
Optimizer
LARS
LR Scheduler
Warmup-Anneal
Learning Rate
4
Weight Decay
10−6
Batch Size
1024
Number of Training Iterations
48,000
GPU
Nvidia GeForce RTX 2080"
E,0.9893238434163701,Linear Classiﬁer
E,0.9928825622775801,"Input Dimension
128
Optimizer
SGD
LR Scheduler
-
Learning Rate
0.1
Weight Decay
-
Batch Size
1024
Number of Training Iterations
30000
GPU
Nvidia GeForce RTX 2080"
E,0.99644128113879,"(Kolesnikov et al., 2019; Bachman et al., 2019; van den Oord et al., 2019). At test time, we discard
the projection head (g(.)) and keep only the ResNet encoder (f(.)). We freeze the encoder and deﬁne
a trainable linear layer that maps 128-dimensional features from the encoder to class probabilities.
This is our linear classiﬁer. We train this classiﬁer over the frozen embeddings from the ResNet
encoder for 100 epochs with a batch size of 1024. We use the SGD optimizer with an initial learning
rate of 0.1. We summarize all of these parameters in Table 7. We optimize the linear classiﬁer using
the cross-entropy loss and calculate the top-1 accuracy at the end of training."
