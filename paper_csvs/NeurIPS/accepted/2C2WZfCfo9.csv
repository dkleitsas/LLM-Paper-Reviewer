Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002506265664160401,"Speech enhancement (SE) aims to improve the intelligibility and quality of speech
in the presence of non-stationary additive noise. Deterministic deep learning
models have traditionally been used for SE, but recent studies have shown that
generative approaches, such as denoising diffusion probabilistic models (DDPMs),
can also be effective. However, incorporating condition information into DDPMs
for SE remains a challenge. We propose a model-agnostic method called DOSE that
employs two efficient condition-augmentation techniques to address this challenge,
based on two key insights: (1) We force the model to prioritize the condition
factor when generating samples by training it with dropout operation; (2) We inject
the condition information into the sampling process by providing an informative
adaptive prior. Experiments demonstrate that our approach yields substantial
improvements in high-quality and stable speech generation, consistency with
the condition factor, and inference efficiency. Codes are publicly available at
https://github.com/ICDM-UESTC/DOSE."
INTRODUCTION,0.005012531328320802,"1
Introduction"
INTRODUCTION,0.007518796992481203,"Speech enhancement (SE) aims to improve the intelligibility and quality of speech, particularly in
scenarios where degradation is caused by non-stationary additive noise. It has significant practical
implications in various fields such as telecommunications [1], medicine [2], and entertainment [3].
Modern deep learning models are often used to learn a deterministic mapping from noisy to clean
speech. While deterministic models have long been regarded as more powerful in the field of SE,
recent advancements in generative models [4, 5] have significantly closed this gap."
INTRODUCTION,0.010025062656641603,"One such generative approach is based on using denoising diffusion probabilistic models (DDPMs) [6,
7], which have been shown to effectively synthesize natural-sounding speech. Several diffusion
enhancement models have been developed [4, 5, 8], which try to learn a probability distribution over
the data and then generate clean speech conditioned on the noisy input. A key challenge in using
diffusion enhancement models is how to effectively incorporate condition information into learning
and generating faithful speech [9, 10, 8]. Previous works address this issue through designing specific
condition-injecting strategies [4, 9, 11] or devising complex network architectures [10, 5]."
INTRODUCTION,0.012531328320802004,"We conduct a thorough examination to understand the limitation of diffusion-based SE methods and
find that diffusion enhancement models are susceptible to condition collapse, where the primary cause
of inconsistent generation is the non-dominant position of the condition factor. We thus introduce
a new paradigm to effectively incorporate condition information into the diffusion enhancement
models. Specifically, we propose a Diffusion-drOpout Speech Enhancement method (DOSE), which
is a model-agnostic SE method (Figure 1) that employs two efficient condition-augmentation tech-"
INTRODUCTION,0.015037593984962405,∗Corresponding author: fan.zhou@uestc.edu.cn
INTRODUCTION,0.017543859649122806,"Figure 1: An illustration of the proposed DOSE. DOSE consists two primary procedures: (1) training
a condition diffusion model using dropout operation, and (2) generating speech using a conditional
diffusion model equipped with the adaptive prior."
INTRODUCTION,0.020050125313283207,"niques: (1) During training, we randomly drop out intermediate-generated samples. This dropout
mechanism guides the model’s attention toward the condition factors; (2) Instead of letting the model
generate samples from scratch (Gaussian distribution), we employ an adaptive prior derived from
the conditional factor to generate samples. Experiments on benchmark datasets demonstrate that
our method surpasses recent diffusion enhancement models in terms of both accuracy and efficiency.
Additionally, DOSE produces more natural-sounding speech and exhibits stronger generalization
capabilities compared to deterministic mapping-based methods using the same network architecture."
RELATED WORKS,0.022556390977443608,"2
Related works"
RELATED WORKS,0.02506265664160401,"There are two main categories of diffusion-based SE methods: (1) designing specific condition-
injecting strategies [4, 9, 11, 5], or (2) generating speech with an auxiliary condition optimizer[12,
10, 8]. The first category considerates noisy speech in the diffusion (or reverse) process, either by
linearly interpolating between clean and noisy speech along the process [4, 9], or by defining such a
transformation within the drift term of a stochastic differential equation (SDE) [11, 5]."
RELATED WORKS,0.02756892230576441,"Works from the second category rely on an auxiliary condition optimizer – a generator (diffusion
model) synthesizes clean speech and a condition optimizer informs what to generate [12, 10, 8].
Both the generator and condition optimizer have the ability to denoise, with the latter undertaking
the core part. Given the challenges in leveraging condition information [8], diffusion-based SE
methods within this category often necessitate specific network architecture design to guarantee the
participation of condition factors."
RELATED WORKS,0.03007518796992481,"In a paradigm sense, our method is quite similar but different to the second branch – unlike previous
approaches that require additional auxiliary networks, DOSE is an end-to-end diffusion-based SE
method. In addition, DOSE is model-agnostic that does not need any specific network design to
guarantee consistency between the generated sample and its corresponding condition factor."
PRELIMINARIES,0.03258145363408521,"3
Preliminaries"
PRELIMINARIES,0.03508771929824561,"We now provide a brief introduction to the diffusion probabilistic model (diffusion models, for short),
the definition of speech enhancement, and the condition collapse problem."
DIFFUSION MODELS,0.03759398496240601,"3.1
Diffusion models"
DIFFUSION MODELS,0.040100250626566414,"A diffusion model [13, 6] consists of a forward (or, diffusion) process and a reverse process. Given
a data point x0 with probability distribution p(x0), the forward process gradually destroys its data
structure by repeated application of the following Markov diffusion kernel:"
DIFFUSION MODELS,0.042606516290726815,"p(xt|xt−1) = N(xt;
p"
DIFFUSION MODELS,0.045112781954887216,"1 −βtxt−1, βtI),
t ∈{1, 2, · · · , T},
(1)"
DIFFUSION MODELS,0.047619047619047616,"where β1, · · · , βT is a pre-defined noise variance schedule. With enough diffusion step T, p(xT )
converges to the unit spherical Gaussian distribution. Based on the Markov chain, the marginal
distribution at arbitrary timestep t has the following analytical form:"
DIFFUSION MODELS,0.05012531328320802,"p(xt|x0) = N(xt;
√"
DIFFUSION MODELS,0.05263157894736842,"¯αtx0, (1 −¯αt)I),
t ∈{1, 2, · · · , T},
(2)"
DIFFUSION MODELS,0.05513784461152882,"where ¯αt = Qt
s=1(1 −βs)."
DIFFUSION MODELS,0.05764411027568922,"Figure 2: Investigation of the condition collapse problem. From left to right: (1) comparison of
loss curves between unconditional and conditional diffusion models; (2) three variants; (3) PESQ
performance of different variants, (-) represent the unprocessed speech."
DIFFUSION MODELS,0.06015037593984962,"As for the reverse process, it aims to learn a transition kernel from xt to xt−1, which is defined as
the following Gaussian distribution [6]:"
DIFFUSION MODELS,0.06265664160401002,"pθ(xt−1|xt) = N(xt−1; µθ(xt, t), Σ(xt, t)),
(3)"
DIFFUSION MODELS,0.06516290726817042,"where θ is the learnable parameter and µθ(xt, t) =
1
√1−βt (xt −
βt
√1−¯αt ϵθ(xt, t)) denotes the mean
of xt−1, which is obtained by subtracting the estimated Gaussian noise ϵθ(xt, t) in the xt. With
such a learned transition kernel, one can approximate the data distribution p(x0) via:"
DIFFUSION MODELS,0.06766917293233082,"pθ(x0) =
Z
pθ(xT ) T
Y"
DIFFUSION MODELS,0.07017543859649122,"t=1
pθ(xt−1|xt)dx1:T ,
(4)"
DIFFUSION MODELS,0.07268170426065163,"where pθ(xT ) = N(xT ; 0, I)."
PROBLEM FORMULATION,0.07518796992481203,"3.2
Problem formulation"
PROBLEM FORMULATION,0.07769423558897243,"Speech enhancement refers to methods that try to reduce distortions, make speech sounds more
pleasant, and improve intelligibility. In real environments, the monaural noisy speech y in the time
domain can be modeled as:
y = x + n
(5)
where x and n denote clean and noise signals, respectively. For human perception, the primary goal
of speech enhancement is to extract x from y. Mapping-based speech enhancement methods directly
optimize pθ(x|y), while diffusion enhancement methods generate clean samples through a Markov
process pθ(x0:T −1|x1:T , y)."
CONDITION COLLAPSE IN DIFFUSION ENHANCEMENT MODELS,0.08020050125313283,"3.3
Condition Collapse in diffusion enhancement models"
CONDITION COLLAPSE IN DIFFUSION ENHANCEMENT MODELS,0.08270676691729323,"The condition collapse problem in speech enhancement was first proposed in [8] and it refers to
the limited involvement of the condition factor during conditional diffusion training, resulting in
inconsistencies between the generated speech and its condition factor."
CONDITION COLLAPSE IN DIFFUSION ENHANCEMENT MODELS,0.08521303258145363,"In this work, we argue that the condition factor y indeed participates and helps the intermediate-
generated sample xt approximate p(xt−1|xt, x0). Our assertion is supported by the experiment
depicted in the left part of Figure 2 – the diffusion model equipped with the condition factor exhibits
a lower loss curve compared to the unconditional one2. To better understand the condition collapse
phenomenon, we devise two variants that explicitly modify the mutual information between the
condition factor and the model’s output (Figure 2 (middle)). We use skip connections to add the
condition factor to multiple layers, forcing the likelihood of maintaining a strong connection between
the condition factor and output features. Since the dependence of the output on any hidden state
in the hierarchy becomes weaker as one moves further away from the output in that hierarchy
(cf. [14]), using skip connections can explicitly enhance connections between the generated sample
and condition factor."
CONDITION COLLAPSE IN DIFFUSION ENHANCEMENT MODELS,0.08771929824561403,"2We use DiffWave [7] as basic architecture and use the same experimental settings as [4, 9] – the only
difference being the change in the way of condition-injecting since most speech enhancement methods will
directly use noisy speech as the condition factor, rather than Mel-spectrogram."
CONDITION COLLAPSE IN DIFFUSION ENHANCEMENT MODELS,0.09022556390977443,"As shown in Figure 2 (right), an increase in mutual information (connections) leads to a significant
improvement in the consistency between the generated sample and the condition factor (a →b).
However, it requires a meticulously designed model to guarantee its effectiveness (b →c). While
previous studies [5, 10, 8] focus on explicitly enhancing the consistency between the output speech
and condition factor through specific network architecture design, we explore the possibility of a
solution independent of the model architecture. This would broaden the applicability of our method,
as it enables slight modifications to existing deterministic mapping-based models to transform them
into diffusion enhancement models."
METHODOLOGY,0.09273182957393483,"4
Methodology"
METHODOLOGY,0.09523809523809523,"Considering the diffusion model provides a transition function from xt to xt−1, typical condition
generation process is represented as:"
METHODOLOGY,0.09774436090225563,"pθ(x0|y) =
Z
p(xT )
| {z }
Prior T
Y"
METHODOLOGY,0.10025062656641603,"t=1
pθ(xt−1|xt, y)
|
{z
}
Condition"
METHODOLOGY,0.10275689223057644,"dx1:T ,
xT ∼N(xT ; 0, I).
(6)"
METHODOLOGY,0.10526315789473684,"Our experiments above indicate that pθ(xt−1|xt, y) will easily collapse to pθ(xt−1|xt), resulting in
the condition generation process degenerating into a vanilla unconditional process:
Z
pθ(xT ) T
Y"
METHODOLOGY,0.10776942355889724,"t=1
pθ(xt−1|xt, y)dx1:T ⇒
Z
pθ(xT ) T
Y"
METHODOLOGY,0.11027568922305764,"t=1
pθ(xt−1|xt)dx1:T .
(7)"
METHODOLOGY,0.11278195488721804,"As a result, facilitating automatic learning of the joint distribution for both clean and noisy speech
samples does not work well for the speech enhancement task."
METHODOLOGY,0.11528822055137844,"4.1
Condition augmentation I: Adaptive Prior"
METHODOLOGY,0.11779448621553884,"Let’s revisit Eq. (6): since we cannot easily inject the condition factor into the condition term, how
about the prior term? For example, we can modify the condition generation process as:"
METHODOLOGY,0.12030075187969924,"pθ(x0|y) =
Z
p(xτ|y)
| {z }
Conditional τY"
METHODOLOGY,0.12280701754385964,"t=1
pθ(xt−1|xt)
|
{z
}
Unconditional"
METHODOLOGY,0.12531328320802004,"dx1:τ,
(8)"
METHODOLOGY,0.12781954887218044,"where p(xτ|y) is formulated as p(xτ|y) = N(xτ; √¯ατy, (1 −¯ατ)I). The following propositions
verify the feasibility of our proposal.
Proposition 1. For any ξ > 0 such that 0 < ξ < M for some finite positive value M, there exists a
positive value τ ∈{0, · · · , T} that satisfies:"
METHODOLOGY,0.13032581453634084,"DKL (p(xt|x)∥p(xt|y)) ≤ξ,
∀τ ≤t ≤T,
(9)"
METHODOLOGY,0.13283208020050125,"where p(xt|c) = N(xt; √¯αtc, (1 −¯αt)I).
Remark 1. This proposition indicates that, given a tolerable margin of error ξ and a well-trained
diffusion model, we can always find a suitable τ such that we are able to recover the clean speech x
from its noisy one y using Eq. (8)."
METHODOLOGY,0.13533834586466165,"While Proposition 1 allows us to generate clean speech x given the noisy speech y using Eq. (8), it
does not guarantee that our model will achieve successful recovery with a high probability.
Proposition 2. Let x be the clean sample, y be it’s corresponding noisy one, and x′ be any neighbor
from the neighbor set S(x). Then diffusion enhancement models can recover x with a high probability
if the following inequality is satisfied:"
METHODOLOGY,0.13784461152882205,"log
 p(x) p(x′)"
METHODOLOGY,0.14035087719298245,"
>
1
2σ2
t"
METHODOLOGY,0.14285714285714285," 
∥x −y∥2
2 −∥x′ −y∥2
2

,
∀x′ ∈S(x),
(10)"
METHODOLOGY,0.14536340852130325,"where σ2
t = 1−¯αt ¯αt ."
METHODOLOGY,0.14786967418546365,"Remark 2. Assuming that the condition factor y is closer to x than to x′, we obtain a non-positive
right-hand side (RHS). For a given x, the left-hand side (LHS) value is fixed, and to ensure the
inequality always holds, a smaller σ2
t is preferred."
METHODOLOGY,0.15037593984962405,"0
100
200
Timestep t -2.5 0.0 2.5"
METHODOLOGY,0.15288220551378445,"log10(σ2
t)"
METHODOLOGY,0.15538847117794485,"50, 1e-4, 0.035
200, 1e-4, 0.05"
METHODOLOGY,0.15789473684210525,"Figure 3: The change curves of
log10 σ2
t . Elements in legend are
T, β1, βT respectively."
METHODOLOGY,0.16040100250626566,"As shown in Figure 3, σ2
t will increase as the timestep t increases.
Thus, according to Proposition 2, we should choose a small τ
for Eq. (8) to maximize the probability of successfully recovering
the clean speech from the noisy one. However, constrained by
Proposition 1, τ cannot be too small. In other words, the clean
speech distribution p(xτ) and the noisy speech distribution p(yτ)
will get closer over the forward diffusion process, and the gap
|n| = |y −x| between the noisy speech and the clean one will
indeed be “washed out” by the increasingly added noise. Since
the original semantic information will also be removed if τ is too
large, there should be a trade-off when we set τ for the diffusion
enhancement model."
METHODOLOGY,0.16290726817042606,"Condition optimizer. We find that both propositions are corre-
lated with the condition factor y. If we can reduce the gap between the condition factor and clean
speech, we can choose a smaller τ, effectively increasing the likelihood of recovering clean speech.
One simple idea is to employ a neural network fψ to optimize the condition factor, as demonstrated
in [15]. Accordingly, we can rewrite Eq. (8) as:"
METHODOLOGY,0.16541353383458646,"pθ,ψ(x0|y) =
Z
pψ(xτ|y) τY"
METHODOLOGY,0.16791979949874686,"t=1
pθ(xt−1|xt)dx1:τ,
(11)"
METHODOLOGY,0.17042606516290726,"where pψ(xτ|y) = N(xτ; √¯ατfψ(y), (1 −¯ατ)I)."
METHODOLOGY,0.17293233082706766,"In practice, we should also consider failure cases of the condition optimizer in complex scenarios,
especially the issue of excessive suppression that has been reported in recent literature [16, 17, 18].
To mitigate this issue, we use 0.5c + 0.5y (like a simple residual layer) as a mild version of the
condition factor:"
METHODOLOGY,0.17543859649122806,"pψ(xτ|y) = N(xτ; 0.5√¯ατ (fψ(y) + y) , (1 −¯ατ)I).
(12)"
METHODOLOGY,0.17794486215538846,We call pψ(xτ|y) the adaptive prior as it varies with different noisy samples y.
METHODOLOGY,0.18045112781954886,"4.2
Condition augmentation II: Diffusion Dropout"
METHODOLOGY,0.18295739348370926,"Aside from changing the prior p(xT ) to conditional prior pψ(xτ|y) , we also optimize the condition
term pθ(xt−1|xt, y). Instead of designing specific condition-injecting strategies [4, 9, 11] or devising
complicated network architecture [8, 10, 5], we attempt to “do subtraction” by discarding some shared
(intermediate-generated samples & condition factor) and important (target-related) information from
intermediate-generated samples. Naturally, if we discard some information from xt, then the diffusion
enhancement model is forced to use the condition factor y to recover the speech. Taking a further
step, we can even discard the entire xt, as the condition factor y alone is sufficient for recovering
the clean speech x0 (this is what deterministic models do). To this end, we define a neural network
fθ(d(xt, p), y, t) to approximate p(xt−1|xt, x0):"
METHODOLOGY,0.18546365914786966,"pθ(xt−1|xt, y) = N(xt−1; fθ(d(xt, p), y, t), Σ(xt, t)),
(13)"
METHODOLOGY,0.18796992481203006,"where d(xt, p) is the dropout operation:"
METHODOLOGY,0.19047619047619047,"d(xt, p) =
xt
if r = 1
ϵ
if r = 0 ,
r ∼Bernoulli(1 −p).
(14)"
DOSE TRAINING,0.19298245614035087,"4.3
DOSE training"
DOSE TRAINING,0.19548872180451127,"Ho et al. [6] and much of the following work choose to parameterize the denoising model through
directly predicting ϵ with a neural network ϵθ(xt, t), which implicitly sets:"
DOSE TRAINING,0.19799498746867167,"ˆx0 =
1
√¯αt"
DOSE TRAINING,0.20050125313283207," 
xt −
√"
DOSE TRAINING,0.20300751879699247,"1 −¯αtϵθ

.
(15)"
DOSE TRAINING,0.20551378446115287,"In this case, the training loss is also usually defined as the mean squared error in the ϵ-space
∥ϵ −ϵθ(xt, t)∥2
2. Although this standard specification works well for training an unconditional
diffusion model, it is not suited for DOSE – for two reasons."
DOSE TRAINING,0.20802005012531327,Algorithm 1 DOSE Training
DOSE TRAINING,0.21052631578947367,"1: choose p
2: repeat
3:
x0 ∼p(x)
4:
t ∼Uniform({1, . . . , T})
5:
ϵ ∼N(0, I)
6:
Take gradient descent step on
7:
▽θ∥x0 −xθ(d (xt, p) , y, t)∥2
2
8: until converged"
DOSE TRAINING,0.21303258145363407,Algorithm 2 DOSE Sampling
DOSE TRAINING,0.21553884711779447,"1: choose τ1, τ2, τ2 < τ1 ≤T
2: Step 1: Generate ˆxτ2
3:
yτ1 ∼N(√¯ατ1y, (1 −¯ατ1)I)
4:
ˆx0 = fθ
 
yτ1, y, τ1
"
DOSE TRAINING,0.21804511278195488,"5:
ˆxτ2 ∼N(0.5√¯ατ2(ˆx0 + y), (1 −¯ατ2)I)
6: Step 2: Generate ˆx0
7:
ˆx0 = fθ (ˆxτ2, y, τ2)
8: return ˆx0"
DOSE TRAINING,0.22055137844611528,"First, we cannot estimate ϵ without the help of xt because ϵ and y are independent. Second, as
discussed earlier, we want DOSE to start with a small timestep and we strive to make τ small.
However, as τ approaches zero, small changes in x-space have an increasingly amplified effect on the
implied prediction in ϵ-space (Eq. (15)). In other words, the efforts made by diffusion enhancement
models become so negligible that diffusion models lose their ability to calibrate the speech at small
timesteps."
DOSE TRAINING,0.22305764411027568,"So, we need to ensure that the estimation of ˆx0 remains flexible as the timestep t gets smaller.
Considering the equivalently of the ϵ-space loss ∥ϵ −ϵθ(xt, t)∥2
2 to a weighted reconstruction loss
in x-space
1
σ2
t ∥x0 −xθ(xt, t)∥2
2, we can directly estimate the clean speech x0 at each timestep t:"
DOSE TRAINING,0.22556390977443608,"L = Ex0∼p(x),t∈{1,...,T }

∥x0 −fθ(d(xt, p), y, t)∥2
2

(16)"
DOSE INFERENCE,0.22807017543859648,"4.4
DOSE inference"
DOSE INFERENCE,0.23057644110275688,"After training, the ideal scenario is that pθ(xt−1|xt, y) approximates p(xt−1|xt, x0) precisely,
enabling us to generate clean speech using Eq. (6). However, when applied in practice, it is difficult
to completely eliminate errors (both sample error and true error). If these errors are not effectively
managed or corrected during the generation process, the quality of the generated samples may
deteriorate, leading to artifacts, blurriness, etc [19, 20]. This issue is particularly pronounced when
using diffusion models for fine-grained conditional generation tasks, as diffusion models require a
large number of steps to generate samples, which will significantly reduce the consistency between
the generated sample and its condition factor (see §5.3, Figure 6)."
DOSE INFERENCE,0.23308270676691728,"The adaptive prior (Sec 4.1) provides an opportunity to address the error accumulation issue. Specifi-
cally, we can select a suitable τ smaller than T, conditioned on an adaptive prior, and generate speech
in fewer steps. We can extend Eq. 11 by transforming the unconditional diffusion enhancement model
into a conditional one:"
DOSE INFERENCE,0.23558897243107768,"pθ,ψ(x0|y) =
Z
pψ(xτ|y) τY"
DOSE INFERENCE,0.23809523809523808,"t=1
pθ(xt−1|xt, y)dx1:τ,
(17)"
DOSE INFERENCE,0.24060150375939848,and the number of sampling steps is reduced from T to τ + 1.
DOSE INFERENCE,0.24310776942355888,"Readers familiar with diffusion models may recall that the standard process repeatedly applies a
“single-step” denoising operation xt−1 = denoise(xt; t) that aims to convert a noisy sample at some
timestep t to a (slightly less) noisy sample at the previous timestep t −1. In fact, each application of
the one-step denoiser consists of two steps: (1) an estimation of the fully denoised sample x0 from
the current timestep t, and (2) computing a (properly weighted, according to the diffusion model)
average between this estimated denoised sample and the noisy sample at the previous timestep t −1.
Thus, instead of performing the entire τ-step diffusion process to denoise a sample, it is also possible
to run denoise once and simply output the estimated sample in one shot [21]. Accordingly, Eq. (17)
can be further rewritten as:"
DOSE INFERENCE,0.24561403508771928,"pθ,ψ(x0|y) =
Z
pψ(xτ|y)pθ(x0|xτ, y)dxτ
(18)"
DOSE INFERENCE,0.24812030075187969,"We can even achieve DOSE without the condition optimizer fψ(·) – using conditional diffusion
enhancement model instead. For example, we can generate clean speech via:"
DOSE INFERENCE,0.2506265664160401,"pθ(x0|y) =
Z Z
pθ(ˆxτ2|yτ1, y)pθ(x0|ˆxτ2, y)dˆxτ2dyτ1,
(19)"
DOSE INFERENCE,0.2531328320802005,"where τ1, τ2 (τ2 < τ1 ≤T) are two pre-defined hyper-parameters. The motivation behind Eq. (19) is
that, once we have trained a neural network fθ(xt, y, t) that can accurately estimate x0 (Eq. (16)),
according to the theoretical analysis in Sec 4.1, we can first choose a suitable value for τ1 to ensure a
relatively good approximation of x0:"
DOSE INFERENCE,0.2556390977443609,"ˆx0 = fθ(yτ1, y, τ1) ≈fθ(xτ1, y, τ1)
(20)"
DOSE INFERENCE,0.2581453634085213,"In the second step, once we have obtained a good condition factor, we can choose a smaller timestep
τ2 < τ1 to get a better estimation of x0 than ˆx0 generated in the first step."
DOSE INFERENCE,0.2606516290726817,"Summary. DOSE has three important benefits: (1) By dropping xt entirely, we make the condition
factor y the “protagonist”, automatically enhancing the consistency between the generated sample
and the condition factor. (2) By training the model with this modified training objective, DOSE can
perform well not only on Gaussian noise (xt →x0) but also on various types of non-Gaussian noise
(y →x0). (3) DOSE is efficient (2 steps), faster than existing diffusion enhancement models."
EXPERIMENTS,0.2631578947368421,"5
Experiments"
EXPERIMENTS,0.2656641604010025,"We compare DOSE with prevailing diffusion enhancement methods and deterministic mapping-
based enhancement methods in §5.1. We conduct a counterfactual verification to understand the
intrinsic mechanism of DOSE in §5.2. We show two visual cases of excessive suppression and error
accumulation in §5.3. While providing a self-contained version of our main results, we note that we
also have additional quantitative observations reported in the Appendices. Specifically, we compare
DOSE with other baselines via subjective evaluation (Appendix A.2); We investigate the significance
of the proposed adaptive prior and explain why we need to use a mild version of the condition factor
(Appendix A.3); We examine the effect of our new training objective and demonstrate the necessity
of using it (Appendix A.4); We explain why we use two steps in speech generation (Appendix
A.5); We provide parameter sensitivity experiments (Appendix A.6); We show plenty of visual
cases of excessive suppression and error accumulation (Appendix A.7 and A.8). To help readers
better understand our research, we include a discussion subsection in Appendix A.9. Specifically,
we: (1) Analyze the reasons behind the superior generalizability of diffusion enhancement models
compared to deterministic mapping-based models (from the robust training perspective); (2) Explain
why we use 0.5 in the mild version of the condition factor; (3) Discuss the broader impacts of speech
enhancement methods."
EXPERIMENTS,0.2681704260651629,"Dataset and baselines. Following previous works [4, 9, 8], we use the VoiceBank-DEMAND
dataset [22, 23] for performance evaluations. To investigate the generalization ability of models, we
use CHiME-4 [24] as another test dataset following [9], i.e., the models are trained on VoiceBank-
DEMAND and evaluated on CHiME-4. We compare our model with recent open-sourced diffusion
enhancement models such as DiffuSE [4], CDiffuSE [9], SGMSE [11], SGMSE+ [5], and DR-
DiffuSE [8]. Since the only difference between SGMSE+ and SGMSE is their network architecture,
we compare our model with just one of them."
EXPERIMENTS,0.2706766917293233,"Evaluation metrics. We use the following metrics to evaluate SE performance: the perceptual
evaluation of speech quality (PESQ) [25], short-time objective intelligibility (STOI) [26], segmental
signal-to-noise ratio (SSNR), the mean opinion score (MOS) prediction of the speech signal distortion
(CSIG) [27], the MOS prediction of the intrusiveness of background noise (CBAK) [27] and the
MOS prediction of the overall effect (COVL) [27]. Besides these metrics, we also design two MOS
metrics (MOS and Similarity MOS) for subjective evaluation."
EXPERIMENTS,0.2731829573934837,"Configurations. To ensure a fair comparison, we keep the model architecture exactly the same as
that of the DiffWave model [7] for all methods3. DiffWave takes 50 steps with the linearly spaced
training noise schedule βt ∈

1 × 10−4, 0.035

[4]. We train all methods for 300,000 iterations using
1 NVIDIA RTX 3090 GPU with a batch size of 16 audios. We select the best values for τ1 and τ2
according to the performance on a validation dataset, a small subset (10%) extracted from the training
data. More experiment settings can be found in Appendix A.10."
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2756892230576441,"3Since the focus of our work is on studying the capabilities of diffusion dropout and adaptive prior for
consistency enhancement, we use off-the-shelf architectures to avoid confounding our findings with model
improvements. This decision (using DiffWave) rests on both its widely validated effectiveness and the minimal
changes it required in the baseline experimental setup."
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2781954887218045,Table 1: Comparison of different diffusion enhancement methods.
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2807017543859649,"Method
Year
Efficiency
Dataset
STOI(%)↑
PESQ↑
CSIG↑
CBAK↑
COVL↑"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2832080200501253,"Unprocessed
–
– VB"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2857142857142857,"92.1
1.97
3.35
2.44
2.63
DiffWave
2021
1 step (dis)
93.3
2.51
3.72
3.27
3.11
DiffuSE
2021
6 steps
93.5+0.20"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2882205513784461,"±0.05
2.39−0.12"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2907268170426065,"±0.01
3.71−0.01"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2932330827067669,"±0.01
3.04−0.23"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2957393483709273,"±0.01
3.03−0.08"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.2982456140350877,"±0.01
CDiffuSE
2022
6 steps
93.7+0.40"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3007518796992481,"±0.05
2.43−0.08"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3032581453634085,"±0.01
3.77+0.05"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3057644110275689,"±0.01
3.09−0.18"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3082706766917293,"±0.01
3.09−0.02"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3107769423558897,"±0.01
SGMSE
2022
50 steps
93.3+0.00
±0.08
2.34−0.17"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3132832080200501,"±0.01
3.69−0.03"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3157894736842105,"±0.01
2.90−0.37"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3182957393483709,"±0.01
3.00−0.11"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3208020050125313,"±0.01
DR-DiffuSE
2023
6 steps
92.9−0.04"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3233082706766917,"±0.06
2.50−0.01"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3258145363408521,"±0.02
3.68−0.04"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3283208020050125,"±0.02
3.27+0.00
±0.02
3.08−0.03"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3308270676691729,"±0.02
DOSE
–
2 steps
93.6+0.30"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3333333333333333,"±0.05
2.56+0.05"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3358395989974937,"±0.01
3.83+0.11"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3383458646616541,"±0.01
3.27+0.00
±0.01
3.19+0.08 ±0.01"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3408521303258145,"Unprocessed
–
–"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3433583959899749,CHIME-4
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3458646616541353,"71.5
1.21
2.18
1.97
1.62
DiffWave
2021
1 step (dis)
72.3
1.22
2.21
1.95
1.63
DiffuSE
2021
6 steps
83.7+11.4"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3483709273182957,"±0.05
1.59+0.36"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3508771929824561,"±0.01
2.91+0.70"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3533834586466165,"±0.01
2.19+0.24"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3558897243107769,"±0.01
2.19+0.56"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3583959899749373,"±0.01
CDiffuSE
2022
6 steps
82.8+10.5"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3609022556390977,"±0.05
1.58+0.36"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3634085213032581,"±0.01
2.88+0.67"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3659147869674185,"±0.01
2.15+0.20"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.3684210526315789,"±0.01
2.18+0.55"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.37092731829573933,"±0.01
SGMSE
2022
50 steps
84.5+12.2"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.37343358395989973,"±0.05
1.57+0.34"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.37593984962406013,"±0.02
2.92+0.71"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.37844611528822053,"±0.01
2.18+0.23"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.38095238095238093,"±0.02
2.18+0.55"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.38345864661654133,"±0.01
DR-DiffuSE
2023
6 steps
77.6+5.30"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.38596491228070173,"±0.06
1.29+0.07"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.38847117794486213,"±0.04
2.40+0.19"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.39097744360902253,"±0.02
2.04+0.09"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.39348370927318294,"±0.01
1.78+0.15"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.39598997493734334,"±0.01
DOSE
–
2 steps
86.6+14.3"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.39849624060150374,"±0.05
1.52+0.30"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.40100250626566414,"±0.01
2.71+0.50"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.40350877192982454,"±0.01
2.15+0.20"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.40601503759398494,"±0.01
2.06+0.43 ±0.01"
SINCE THE FOCUS OF OUR WORK IS ON STUDYING THE CAPABILITIES OF DIFFUSION DROPOUT AND ADAPTIVE PRIOR FOR,0.40852130325814534,"Figure 4: Counterfactual visualization. The first two columns are associated with a dropout probability
of 0.1, while the last two columns are associated with a dropout probability of 0.9. In each row, the
blue waveforms in the first and third columns are the counterfactual samples, and the blue waveforms
in the second and fourth columns are the normal samples. The orange waveforms are generated
samples from the model."
PERFORMANCE COMPARISON,0.41102756892230574,"5.1
Performance comparison"
PERFORMANCE COMPARISON,0.41353383458646614,"We compare our method with previous diffusion enhancement methods and summarize our ex-
perimental results in Table 1. We observe that: (1) Diffusion enhancement methods have better
generalizability than deterministic methods. (2) Methods with specific condition-injecting strategies,
such as DiffuSE, CDiffuSE, and SGMSE, have strong generalization but perform slightly worse
than deterministic mapping-based methods in matched scenarios. (3) Method (DR-DiffuSE) with
auxiliary condition optimizer, performs better in matched scenarios and shows a slight improvement
in mismatched scenarios. (4) Our method performs well in both matched and mismatched scenarios
and is on par with state-of-the-art diffusion enhancement models while requiring fewer steps."
COUNTERFACTUAL VERIFICATION,0.41604010025062654,"5.2
Counterfactual verification"
COUNTERFACTUAL VERIFICATION,0.41854636591478694,"We perform a counterfactual verification to gain insights into the underlying mechanism of DOSE.
To verify whether dropout can increase the “discourse power” of the conditional factor, we keep the
condition factor y fixed and reverse the intermediate-generated speech at a specific step (reverse(xt)).
This reversed intermediate-generated speech is called a counterfactual sample. Notably, if the final
generated speech is more similar to the condition factor than the counterfactual speech, we can
conclude that the condition factor plays a dominant role in the generation process. Otherwise, we can
say that the condition factor is less influential."
COUNTERFACTUAL VERIFICATION,0.42105263157894735,"As shown in Figure 4, we compare the performance of two models with different dropout probabilities
(0.1 vs. 0.9). We have two findings here: (1) A higher dropout probability encourages the model to
prioritize the condition factor even with a small timestep t. (2) When timestep t is large, DOSE ef-"
COUNTERFACTUAL VERIFICATION,0.42355889724310775,"Figure 5: Excessive suppression visualization (unconditional diffusion enhancement model on
CHIME-4). From left to right: (1) DiffWave (dis); (2) adaptive prior with the estimated condition; (3)
adaptive prior with the mild condition; (4) clean speech."
COUNTERFACTUAL VERIFICATION,0.42606516290726815,"Figure 6: Error accumulation visualization (VB, DOSE). From left to right: (1) noisy speech; (2) full
(50) steps; (3) 2 steps; (4) clean."
COUNTERFACTUAL VERIFICATION,0.42857142857142855,"fectively captures condition information, ensuring the model’s robustness to noise and maintaining
consistency in the early stages of inference."
EXCESSIVE SUPPRESSION & ERROR ACCUMULATION,0.43107769423558895,"5.3
Excessive suppression & error accumulation"
EXCESSIVE SUPPRESSION & ERROR ACCUMULATION,0.43358395989974935,"We provide a visual case of excessive suppression in Figure 5 and a visual case of error accumulation
in Figure 6. From Figure 5, we can see that: (1) The deterministic model fails in mismatched
scenarios and generates samples that lose speech details; (2) The diffusion enhancement model
generate defective speech when directly using the estimated speech as the condition factor; (3) The
diffusion enhancement model equipped with a mild version of the condition factor can recover
clean speech effectively. From Figure 6, we notice that: (1) Full-step generation can remove noise
and generate natural-sounding speech. However, it can’t guarantee the consistency between the
generated speech and condition factor; (2) Two-step speech generation with adaptive prior can
promise consistency and high quality simultaneously."
CONCLUSIONS,0.43609022556390975,"6
Conclusions"
CONCLUSIONS,0.43859649122807015,"In this work, we present a new approach DOSE that effectively incorporates condition information
into diffusion models for speech enhancement. DOSE uses two efficient condition-augmentation
techniques to address the condition collapse problem. Comprehensive experiments on benchmark
datasets demonstrate the efficiency and effectiveness of our method."
CONCLUSIONS,0.44110275689223055,"In our method, there are two groups of hyper-parameters: the dropout probability p for the dropout
operation and two timesteps τ1, τ2 for the adaptive prior. These parameters are critical to model
performance. For example, if the dropout probability is set too high, the diffusion enhancement
model will rely solely on the condition factor to estimate the speech. Then our diffusion enhancement
model will degenerate into a deterministic model, losing its generalizability. We also need to make a
trade-off when choosing the timestep τ (especially τ1): On one hand, a large τ is needed to reduce
the gap between the clean speech and condition factor. On the other hand, the original semantic
information will also be removed if τ is set too large."
CONCLUSIONS,0.44360902255639095,"In practice, it is necessary to evaluate the model on a subset of data and then empirically set the
hyperparameters. These manually defined hyper-parameters are selected based on the Empirical
Risk Minimization (ERM) principle and may not be optimal for every individual sample. Thus,
an important direction for future research is to develop methods that can adaptively choose hyper-
parameters for different samples. It is also expected that the model can adaptively select appropriate
coefficients when forming a mild version of the conditioning factor."
CONCLUSIONS,0.44611528822055135,Acknowledgement
CONCLUSIONS,0.44862155388471175,"This work was supported in part by National Natural Science Foundation of China (Grant
No.62176043 and No.62072077), Natural Science Foundation of Sichuan Province (Grant
No.2022NSFSC0505), Kashgar Science and Technology Bureau (Grant No.KS2023025), and Na-
tional Science Foundation SWIFT (Grant No.2030249)."
REFERENCES,0.45112781954887216,References
REFERENCES,0.45363408521303256,"[1] Steven L Gay and Jacob Benesty. Acoustic signal processing for telecommunication, volume
551. Springer Science & Business Media, 2012."
REFERENCES,0.45614035087719296,"[2] Tim Van den Bogaert, Simon Doclo, Jan Wouters, and Marc Moonen. Speech enhancement with
multichannel wiener filter techniques in multimicrophone binaural hearing aids. The Journal of
the Acoustical Society of America, 125(1):360–371, 2009."
REFERENCES,0.45864661654135336,"[3] Ivan Tashev. Recent advances in human-machine interfaces for gaming and entertainment.
International journal of information technologies and security, 3(3):69–76, 2011."
REFERENCES,0.46115288220551376,"[4] Yen-Ju Lu, Yu Tsao, and Shinji Watanabe. A study on speech enhancement based on diffusion
probabilistic model. In 2021 Asia-Pacific Signal and Information Processing Association Annual
Summit and Conference (APSIPA ASC), pages 659–666. IEEE, 2021."
REFERENCES,0.46365914786967416,"[5] Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann.
Speech enhancement and dereverberation with diffusion-based generative models.
arXiv
preprint arXiv:2208.05830, 2022."
REFERENCES,0.46616541353383456,"[6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances
in Neural Information Processing Systems, 33:6840–6851, 2020."
REFERENCES,0.46867167919799496,"[7] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. In International Conference on Learning Representations,
2021."
REFERENCES,0.47117794486215536,"[8] Wenxin Tai, Fan Zhou, Goce Trajcevski, and Ting Zhong. Revisiting denoising diffusion
probabilistic models for speech enhancement: Condition collapse, efficiency and refinement. In
AAAI, 2023."
REFERENCES,0.47368421052631576,"[9] Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao.
Conditional diffusion probabilistic model for speech enhancement. In ICASSP 2022-2022
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
7402–7406. IEEE, 2022."
REFERENCES,0.47619047619047616,"[10] Joan Serrà, Santiago Pascual, Jordi Pons, R Oguz Araz, and Davide Scaini. Universal speech
enhancement with score-based diffusion. arXiv preprint arXiv:2206.03065, 2022."
REFERENCES,0.47869674185463656,"[11] Simon Welker, Julius Richter, and Timo Gerkmann. Speech enhancement with score-based
generative models in the complex STFT domain. In Proc. Interspeech 2022, pages 2928–2932,
2022. doi: 10.21437/Interspeech.2022-10653."
REFERENCES,0.48120300751879697,"[12] Jianwei Zhang, Suren Jayasuriya, and Visar Berisha. Restoring degraded speech via a modified
diffusion model. In 22nd Annual Conference of the International Speech Communication
Association, INTERSPEECH 2021, pages 2753–2757. International Speech Communication
Association, 2021."
REFERENCES,0.48370927318295737,"[13] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International conference on machine
learning, pages 2256–2265. PMLR, 2015."
REFERENCES,0.48621553884711777,"[14] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999."
REFERENCES,0.48872180451127817,"[15] Zongsheng Yue and Chen Change Loy. Difface: Blind face restoration with diffused error
contraction. arXiv preprint arXiv:2212.06512, 2022."
REFERENCES,0.49122807017543857,"[16] Andong Li, Chengshi Zheng, Cunhang Fan, Renhua Peng, and Xiaodong Li. A recursive
network with dynamic attention for monaural speech enhancement. In Proc. Interspeech 2020,
pages 2422–2426, 2020. doi: 10.21437/Interspeech.2020-1513. URL http://dx.doi.org/
10.21437/Interspeech.2020-1513."
REFERENCES,0.49373433583959897,"[17] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad
Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2022."
REFERENCES,0.49624060150375937,"[18] Santiago López-Tapia and Nicolás Pérez de la Blanca. Fast and robust cascade model for
multiple degradation single image super-resolution. IEEE Transactions on Image Processing,
30:4747–4759, 2021."
REFERENCES,0.49874686716791977,"[19] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic
models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021."
REFERENCES,0.5012531328320802,"[20] Jean-Marie Lemercier, Julius Richter, Simon Welker, and Timo Gerkmann. Storm: A diffusion-
based stochastic regeneration model for speech enhancement and dereverberation. arXiv preprint
arXiv:2212.11851, 2022."
REFERENCES,0.5037593984962406,"[21] Nicholas Carlini, Florian Tramer, J Zico Kolter, et al. (certified!!) adversarial robustness for
free! In International Conference on Learning Representations, 2023."
REFERENCES,0.506265664160401,"[22] Christophe Veaux, Junichi Yamagishi, and Simon King. The voice bank corpus: Design,
collection and data analysis of a large regional accent speech database. In International
Conference Oriental COCOSDA, pages 1–4. IEEE, 2013."
REFERENCES,0.5087719298245614,"[23] Joachim Thiemann, Nobutaka Ito, and Emmanuel Vincent. The diverse environments multi-
channel acoustic noise database (demand): A database of multichannel environmental noise
recordings. In Proceedings of Meetings on Acoustics, volume 19, page 035081. Acoustical
Society of America, 2013."
REFERENCES,0.5112781954887218,"[24] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer.
An analysis of environment, microphone and data simulation mismatches in robust speech
recognition. Computer Speech Language, 46:535–557, 2017."
REFERENCES,0.5137844611528822,"[25] Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual
evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone
networks and codecs. In IEEE international conference on acoustics, speech, and signal
processing (ICASSP), pages 749–752. IEEE, 2001."
REFERENCES,0.5162907268170426,"[26] Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. A short-time objec-
tive intelligibility measure for time-frequency weighted noisy speech. In IEEE international
conference on acoustics, speech, and signal processing (ICASSP), pages 4214–4217. IEEE,
2010."
REFERENCES,0.518796992481203,"[27] Yi Hu and Philipos C Loizou. Evaluation of objective quality measures for speech enhancement.
IEEE Transactions on audio, speech, and language processing, 16(1):229–238, 2007."
REFERENCES,0.5213032581453634,"[28] Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. Prodiff:
Progressive fast diffusion model for high-quality text-to-speech. In Proceedings of the 30th
ACM International Conference on Multimedia, pages 2595–2605, 2022."
REFERENCES,0.5238095238095238,"A
Appendix"
REFERENCES,0.5263157894736842,"In the supplemental material,"
REFERENCES,0.5288220551378446,• A.1: We provide the proofs for Propositions 1 and 2.
REFERENCES,0.531328320802005,• A.2: We compare DOSE with other baselines via subjective evaluation.
REFERENCES,0.5338345864661654,"• A.3: We investigate the significance of the proposed adaptive prior and explain why we need to use
a mild version of the condition factor."
REFERENCES,0.5363408521303258,• A.4: We examine the effect of our new training objective and demonstrate the necessity of using it.
REFERENCES,0.5388471177944862,• A.5: We explain why we use two steps in speech generation.
REFERENCES,0.5413533834586466,• A.6: We provide parameter sensitivity experiments.
REFERENCES,0.543859649122807,• A.7: We present several visual cases of excessive suppression.
REFERENCES,0.5463659147869674,• A.8: We present several visual cases of error accumulation.
REFERENCES,0.5488721804511278,"• A.9: We analyze the reasons behind the superior generalizability of diffusion enhancement models
compared to deterministic mapping-based models (from the robust training perspective), explain
why we use 0.5 in the mild version of the condition factor, and discuss the broader impacts of
speech enhancement methods."
REFERENCES,0.5513784461152882,• A.10: We include more information about speech processing and basic architecture.
REFERENCES,0.5538847117794486,"A.1
Mathematical proofs"
REFERENCES,0.556390977443609,We now present the proofs of the Propositions stated in the main text.
REFERENCES,0.5588972431077694,"Proposition 1. (cf. §4.1): For any ξ > 0 such that 0 < ξ < M for some finite positive value M,
there exists a positive value τ ∈{0, · · · , T} that satisfies:"
REFERENCES,0.5614035087719298,"DKL (p(xt|x)∥p(xt|y)) ≤ξ,
∀τ ≤t ≤T,
(9)"
REFERENCES,0.5639097744360902,"where p(xt|c) = N(xt; √¯αtc, (1 −¯αt)I)."
REFERENCES,0.5664160401002506,"Proof. Given two Gaussian distributions P, Q defined over a vector space Rd, the KL divergence of
multivariate Gaussian distributions is defined as follows:"
REFERENCES,0.568922305764411,DKL (P∥Q) = 1 2
REFERENCES,0.5714285714285714,"
Tr
 
Σ−1
2 Σ1

+ (µ2 −µ1)T Σ−1
2 (µ2 −µ1) −d + ln
det Σ2"
REFERENCES,0.5739348370927319,det Σ1
REFERENCES,0.5764411027568922,"
.
(21)"
REFERENCES,0.5789473684210527,"Here, µ1 ∈Rd and Σ1 ∈Rd×d are the mean and covariance matrix of distribution P, and µ2 ∈Rd
and Σ2 ∈Rd×d are the mean and covariance matrix of distribution Q. d is the dimensionality of the
vectors (i.e., the number of dimensions in the vector space), and Tr denotes the trace operator."
REFERENCES,0.581453634085213,"Note that when the two Gaussian distributions have diagonal covariance matrices (i.e., when the
different dimensions are independent), the above formula simplifies to the sum of the KL divergences
of each univariate Gaussian distribution. Thus, given two Gaussian distributions p(xt|x), p(xt|y)
and Eq. (5), the KL divergence between these two distributions can be calculated as follows:"
REFERENCES,0.5839598997493735,"DKL (p(xt|x)∥p(xt|y)) =
¯αt
1 −¯αt
∥y −x∥2
2 = 1"
REFERENCES,0.5864661654135338,"σ2
t
∥n∥2
2.
(22)"
REFERENCES,0.5889724310776943,"According to the definition of the diffusion model and Figure 3, DKL (p(xt|x)∥p(xt|y)) is a
monotonically decreasing function. Ideally, for a bounded error n with (almost) infinite timestep T,
we have:"
REFERENCES,0.5914786967418546,"lim
t→0 DKL (p(xt|x)∥p(xt|y)) = +∞,
lim
t→T DKL (p(xt|x)∥p(xt|y)) = 0.
(23)"
REFERENCES,0.5939849624060151,"According to Bolzano’s theorem, there exists at least one point τ in the interval {0, · · · , T} such that
DKL (p(xτ|x)∥p(xτ|y)) = ξ. Then, Eq. (9) holds for τ ≤t ≤T."
REFERENCES,0.5964912280701754,Figure 7: Screenshot of MOS test.
REFERENCES,0.5989974937343359,"Proposition 2. (cf. §4.1): Let x be the clean sample, y be it’s corresponding noisy one, and x′ be
any neighbor from the neighbor set S(x). Then diffusion enhancement models can recover x with a
high probability if the following inequality is satisfied:"
REFERENCES,0.6015037593984962,"log
 p(x) p(x′)"
REFERENCES,0.6040100250626567,"
>
1
2σ2
t"
REFERENCES,0.606516290726817," 
∥x −y∥2
2 −∥x′ −y∥2
2

,
∀x′ ∈S(x),
(10)"
REFERENCES,0.6090225563909775,"where σ2
t = 1−¯αt"
REFERENCES,0.6115288220551378,"¯αt
is the variance of the Gaussian noise added at timestep t in the forward diffusion
process."
REFERENCES,0.6140350877192983,"Proof. The main idea is to prove that any point x′ quite similar but different to the ground-true speech
x should have a lower density than x in the conditional distribution so that the diffusion enhancement
models can recover x with a high probability. In other words, we should have:"
REFERENCES,0.6165413533834586,"p(x0 = x|xt = yt) > p(x0 = x′|xt = yt)
(24)"
REFERENCES,0.6190476190476191,"According to Bayes’ theorem, we have:"
REFERENCES,0.6215538847117794,"p(x0 = x|xt = yt) = p(x0 = x, xt = yt)"
REFERENCES,0.6240601503759399,"p(xt = yt)
(25)"
REFERENCES,0.6265664160401002,= p(x0 = x) · p(xt = yt|x0 = x)
REFERENCES,0.6290726817042607,"p(xt = yt)
."
REFERENCES,0.631578947368421,"Applying Eq. (25) to Eq. (24), we obtain:"
REFERENCES,0.6340852130325815,"p(x) ·
1
p"
REFERENCES,0.6365914786967418,"(2πσ2
t )d exp −∥x −y∥2
2
2σ2
t
> p(x′) ·
1
p"
REFERENCES,0.6390977443609023,"(2πσ2
t )d exp −∥x′ −y∥2
2
2σ2
t
(26)"
REFERENCES,0.6416040100250626,"⇔log
 p(x) p(x′)"
REFERENCES,0.6441102756892231,"
>
1
2σ2
t"
REFERENCES,0.6466165413533834," 
∥x −y∥2
2 −∥x′ −y∥2
2

,
∀x′ ∈S(x),"
REFERENCES,0.6491228070175439,and the proof is now complete.
REFERENCES,0.6516290726817042,"A.2
Subjective evaluation"
REFERENCES,0.6541353383458647,"We conduct two types of Mean Opinion Score (MOS) tests to verify the quality of synthesized audio
through human evaluation."
REFERENCES,0.656641604010025,Figure 8: Screenshot of Similarity MOS test.
REFERENCES,0.6591478696741855,Table 2: MOS tests under different scenarios.
REFERENCES,0.6616541353383458,"Method
Scenarios
MOS↑
Similarity MOS↑
Scenarios
MOS↑
Similarity MOS↑"
REFERENCES,0.6641604010025063,Unprocessed
REFERENCES,0.6666666666666666,Matched
REFERENCES,0.6691729323308271,"3.60±0.31
3.33±0.34"
REFERENCES,0.6716791979949874,Mismatched
REFERENCES,0.6741854636591479,"3.30±0.30
3.10±0.32
DiffWave (dis)
3.80+0.20"
REFERENCES,0.6766917293233082,"±0.21
3.75+0.42"
REFERENCES,0.6791979949874687,"±0.24
2.10−1.20"
REFERENCES,0.681704260651629,"±0.28
1.00−2.10"
REFERENCES,0.6842105263157895,"±0.24
DiffuSE
3.40−0.20"
REFERENCES,0.6867167919799498,"±0.20
4.17+1.84"
REFERENCES,0.6892230576441103,"±0.26
3.00−0.30"
REFERENCES,0.6917293233082706,"±0.27
3.33+0.23"
REFERENCES,0.6942355889724311,"±0.21
CDiffuSE
3.85+0.25"
REFERENCES,0.6967418546365914,"±0.25
4.12+1.79"
REFERENCES,0.6992481203007519,"±0.31
2.55−0.75"
REFERENCES,0.7017543859649122,"±0.25
3.42+0.32"
REFERENCES,0.7042606516290727,"±0.29
SGMSE
3.65+0.05"
REFERENCES,0.706766917293233,"±0.28
3.95+0.62"
REFERENCES,0.7092731829573935,"±0.23
2.83−0.47"
REFERENCES,0.7117794486215538,"±0.33
3.41+0.31"
REFERENCES,0.7142857142857143,"±0.25
DR-DiffuSE
3.80+0.20"
REFERENCES,0.7167919799498746,"±0.25
3.84+0.51"
REFERENCES,0.7192982456140351,"±0.27
2.48−0.82"
REFERENCES,0.7218045112781954,"±0.25
1.45−1.65"
REFERENCES,0.7243107769423559,"±0.33
DOSE
4.05+0.45"
REFERENCES,0.7268170426065163,"±0.29
4.35+1.02"
REFERENCES,0.7293233082706767,"±0.21
3.48+0.18"
REFERENCES,0.731829573934837,"±0.26
3.17+0.07 ±0.23"
REFERENCES,0.7343358395989975,"Naturalness. For audio quality evaluation, we conduct the MOS (mean opinion score) tests and
explicitly instruct the raters to “focus on examining the audio quality and naturalness, and ignore the
differences of style (timbre, emotion, and prosody)”. The testers present and rate the samples, and
each tester is asked to evaluate the subjective naturalness on a 1-5 Likert scale."
REFERENCES,0.7368421052631579,"Consistency. For audio consistency evaluation, we explicitly instruct the raters to “focus on the
similarity of the speech (content, timbre, emotion, and prosody) to the reference, and ignore the
differences of audio quality”. This is slightly different from the original definition of SMOS for
speech synthesis. In the SMOS (similarity mean opinion score) tests, we pair each synthesized
utterance with a ground truth utterance to evaluate how well the synthesized speech matches that of
the target speaker. The testers present and rate the samples, and each tester is asked to evaluate the
subjective consistency on a 1-5 Likert scale."
REFERENCES,0.7393483709273183,"0
10
20
30
40
50
Timestep 0.01 0.02 0.03 Loss"
REFERENCES,0.7418546365914787,"0
10
20
30
40
50
Timestep 0.01 0.02 0.03 Loss 1.00 1.10 1.20 1.30 1.40 1.50 PESQ"
REFERENCES,0.7443609022556391,"1.21
1.22 1.34 1.15"
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7468671679197995,"1.46
Unprocessed
Dis
Noisy
Estimated
Mild
Clean"
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7493734335839599,"Figure 9: Performance of the unconditional diffusion enhancement model with adaptive prior. From
left to right: (1) each step loss on matched VB; (2) each step loss on mismatched CHIME-4; (3)
PESQ comparison for different priors on mismatched CHIME-4."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7518796992481203,"Our subjective evaluation tests are crowd-sourced and conducted by 15 volunteers. The screenshots
of instructions for testers have been shown in Figure 7 and Figure 8. We paid $10 to participants
hourly and totally spent about $300 on participant compensation."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7543859649122807,"The MOS results with the 95% confidence interval are shown in Table 2. we observe that: (1)
Our method surpasses all baselines, demonstrating the strong ability of the proposed framework in
synthesizing natural speech; (2) Our model can synthesize consistent speech to the golden speech,
which is aligned with our motivation for algorithm design. It’s also exciting to see that DOSE yields
similar scores to methods with specific condition-injecting strategies (i.e., DiffuSE, CDiffuSE, and
SGMSE) on Similarity MOS."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7568922305764411,"A.3
Adaptive prior analysis"
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7593984962406015,"We now investigate the significance of the proposed adaptive prior (§4.1) and show why we need to
use a mild version (Eq. (12)) of the condition factor."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7619047619047619,"We design three variants to investigate the effect of different condition optimizer settings on denoising
performance. These variants are: (a) applying adaptive prior with the noisy speech; (b) applying
adaptive prior with the estimated one (from the deterministic model); (c) applying adaptive prior with
the mild condition (Eq. (12)). To control variables, we use an unsupervised diffusion model with
adaptive priors. We conduct experiments on the matched VB dataset and mismatched CHIME-4
dataset respectively, and our results are shown in Figure 9."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7644110275689223,"As shown in Figure 9 (left), we plot the one-step loss on matched VB and obtain the following
observations: (1) The trend of loss curves is in line with our analysis in §4.1 that we need to find a
trade-off timestep for better performance. (2) Equipping the unconditional diffusion enhancement
model with the adaptive prior technique has a certain denoising ability but is inferior to its counterpart
discriminative model in matched scenarios. We attribute the second phenomenon to the limited
denoising capacity of the unconditional diffusion enhancement models (cf. [9, 21])."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7669172932330827,"Although the performance of the unconditional diffusion enhancement model equipped with adaptive
prior is mediocre in matched scenarios, as illustrated in Figure 9 (mid), it exhibits greater stability
than discriminative models in mismatched scenarios. To verify the influence of different priors, we
compare their PESQ on mismatched CHIME-4, shown in Figure 9 (right). We see that: (1) The
deterministic model fails in mismatched scenarios and generates samples that are even worse than the
unprocessed ones; (2) The diffusion enhancement model has strong generalizability and performs
significantly better than the deterministic model; (3) The diffusion enhancement model loses its
capability when using the estimated speech as the condition factor; and (4) Although the estimated
speech is worse than the unprocessed one, the diffusion enhancement model equipped with a mild
version of the condition factor achieves the best performance. This implies that the estimated speech
can provide additional complementary guidance to the diffusion model, and the model can adaptively
“separate the wheat from the chaff”. Thus, using a mild version of the condition factor is important
and necessary."
"UNPROCESSED
DIS
NOISY
ESTIMATED
MILD
CLEAN",0.7694235588972431,"In summary, our research shows the strong generalizability of the diffusion enhancement model.
However, we also find that the unconditional diffusion enhancement model has mediocre performance.
This suggests that relying solely on the adaptive prior technique is not be sufficient, further emphasiz-"
K,0.7719298245614035,"20k
40k
60k
80k
100k
120k"
K,0.7744360902255639,"Step
0 0.02 0.04 0.06 0.08 0.1 0.12 Loss"
K,0.7769423558897243,"curve
Condition_ori "
K,0.7794486215538847,"Uncondition_ori
Uncondition_new "
K,0.7819548872180451,Condition_new
K,0.7844611528822055,Figure 11: Investigation of the training objective. we compare the ϵ-space and x-space loss curves.
K,0.7869674185463659,"ing the importance of the diffusion dropout operation – training conditional diffusion enhancement
models in a supervised manner."
K,0.7894736842105263,"A.4
Training objective investigation"
K,0.7919799498746867,We now examine the effect of our new training objective and demonstrate the necessity of using it.
K,0.7944862155388471,"0
20
40
Timestep t 100 101 102"
K,0.7969924812030075,"Figure 10: The relationship between
∆x0 = |ˆx0 −x0| (orange) and ∆ϵ =
|ˆϵ −ϵ| (blue)."
K,0.7994987468671679,"Let’s recall the sampling process of DOSE. In the first
step, we generate a relatively good estimation of the clean
speech. In the second step, we use DOSE with a small
timestep to generate a better one. We plot the relationship
between ∆ϵ and ∆x0 in Figure 10. Specifically, we fix
the ∆x0 as a constant and use Eq. (15) to calculate the
corresponding ∆ϵ. This experiment aims to show how
effort for calibration in x-space is equivalent to that in
ϵ-space. From Figure 10 we see that, as t approaches zero,
small changes in x-space amplify the implied prediction in
ϵ-space. There is a nearly 100-fold difference between the
values of ∆ϵ and ∆x0. This implies that the efforts of the
diffusion enhancement model at small timesteps become
negligible, causing diffusion models to lose their ability to
recover natural-sounding speech from the defective speech
estimated in the small timestep."
K,0.8020050125313283,"We also plot the training loss curves in Figure 11. After substituting the training objective from ϵ-
space to x-space, we observe that the loss change becomes more significant. This demonstrates more
effective participation of conditioning factors in diffusion training: training diffusion enhancement
model with this new objective allows for easier and more effective exploitation of conditioning
factors."
K,0.8045112781954887,"A.5
Complexity analysis & why use two-steps generation?"
K,0.8070175438596491,"In this subsection, we explain why we use two steps to generate speech."
K,0.8095238095238095,"Why not one-step speech generation? According to §4.4, we can generate speech in one shot
using a trained conditional diffusion enhancement model to reduce error accumulation [21]. This
one-step speech generation provides an appealing performance (Table 3). However, if we consider
the diffusion model training as a multi-task paradigm, the denoising task at a smaller timestep t is
easier than that at a larger timestep [21]. Correspondingly, the primary estimation error occurs at the
large timestep area and directly estimating clean speech at a large timestep will result in sub-optimal
performance [28]. Meanwhile, we can’t choose a small timestep t as it will lead to the mismatched
problem discussed in §4.1."
K,0.8120300751879699,"Since the conditional diffusion model can learn vital information from both the intermediate-generated
and noisy speech – the estimated speech can provide complementary guidance to the diffusion model
(Appendix A.3) – allowing us to further improve the result by generating speech with multiple steps."
K,0.8145363408521303,Table 3: Ablation study for two-step speech generation.
K,0.8170426065162907,"Method
Scenarios
PESQ↑
STOI(%)↑
Scenarios
PESQ↑
STOI(%)↑"
K,0.8195488721804511,Unprocessed
K,0.8220551378446115,Matched
K,0.8245614035087719,"1.97
92.1"
K,0.8270676691729323,Mismatched
K,0.8295739348370927,"1.21
71.5
DOSE (fixed 1 step)
2.47+0.50"
K,0.8320802005012531,"±0.01
93.0+0.90"
K,0.8345864661654135,"±0.05
1.38+0.17"
K,0.8370927318295739,"±0.01
82.8+11.3"
K,0.8395989974937343,"±0.05
DOSE (handpicked 1 step)
2.50+0.53"
K,0.8421052631578947,"±0.01
93.4+1.30"
K,0.8446115288220551,"±0.05
1.51+0.31"
K,0.8471177944862155,"±0.01
86.4+15.7"
K,0.849624060150376,"±0.05
DOSE (fixed 2 steps)
2.48+0.51"
K,0.8521303258145363,"±0.01
93.1+1.00"
K,0.8546365914786967,"±0.05
1.44+0.23"
K,0.8571428571428571,"±0.01
83.6+12.1"
K,0.8596491228070176,"±0.05
DOSE (handpicked 2 steps)
2.56+0.59"
K,0.8621553884711779,"±0.01
93.6+1.50"
K,0.8646616541353384,"±0.05
1.52+0.32"
K,0.8671679197994987,"±0.01
86.6+15.1 ±0.05"
K,0.8696741854636592,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 Loss"
K,0.8721804511278195,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 Loss"
K,0.87468671679198,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 Loss"
K,0.8771929824561403,"Unprocessed
Dis
Noisy
Esti
Mild
Clean"
K,0.8796992481203008,"Figure 12: Performance of the conditional diffusion enhancement model with different dropout
probability in VB. From left to right: (1) without dropout; (2) dropout 50% ; (3) dropout 90%."
K,0.8822055137844611,"Hyper-parameter selection. Basically, we need to set optimal hyper-parameters by evaluating the
performance with a small batch of data. Suppose we choose the number of sampling steps as K
(K < T), and the amount of test data as N, then the computational complexity of the grid search
is O (NT!/(T −K)!). Since T is always set as a large number in the diffusion model’s setup, the
complexity introduced by choosing a large K is often unmanageable."
K,0.8847117794486216,"Trivial solution. We have a simple alternative solution: defining the hyper-parameters empirically
(e.g., equal intervals [7]). We present speech quality comparisons between empirically defined (fixed)
and handpicked hyper-parameters in Table 3. As shown, the conditional diffusion enhancement
model with handpicked hyper-parameters performs better than that with empirically defined hyper-
parameters."
K,0.8872180451127819,"Although fixed hyper-parameters have inferior performance compared to handpicked ones, they still
show appealing performance compared to prevailing diffusion enhancement baselines. Therefore,
in situations where we can’t evaluate the model in advance, we can use empirically defined hyper-
parameters instead."
K,0.8897243107769424,"A.6
Parameter sensitivity"
K,0.8922305764411027,"There are two groups of hyper-parameters that are critical to model performance: the dropout
probability p for model training and two timesteps τ1, τ2 for model inference. In this subsection,
we conduct parameter sensitivity experiments to investigate how these hyper-parameters affect the
model’s performance."
K,0.8947368421052632,"Dropout probability p. We vary the dropout probability p, setting it to {0.0, 0.5, 0.9, 1.0}, and plot
the one-step loss on matched VB (Figure 12) and mismatched CHIME-4 (Figure 13) respectively.
Please note that the figure with p = 1 has been excluded since it degenerates to a deterministic
mapping-based model and the performance remains unchanged across all timesteps."
K,0.8972431077694235,"From Figure 12, we see that: (1) The proposed conditional diffusion enhancement model works.
Compared to the deterministic mapping-based model, our model performs slightly better in matched
scenarios and significantly better in mismatched scenarios; (2) When intermediate-generated speech
xt is unreliable (large t), the condition factor y plays a dominant role. (3) As the dropout probability
increases, the model focuses more on the condition factor, while when the dropout probability is
small, the loss curve oscillates when t gets large."
K,0.899749373433584,"From Figure 13, We find that: when using a higher dropout probability (such as p = 0.5 and p = 0.9),
the model can generally achieve better generalizability. Note that if the dropout probability is set too
high, the diffusion enhancement model will rely solely on the condition factor to estimate the clean"
K,0.9022556390977443,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 0.020 Loss"
K,0.9047619047619048,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 0.020 Loss"
K,0.9072681704260651,"0
10
20
30
40
50
Timestep 0.005 0.010 0.015 0.020 Loss"
K,0.9097744360902256,"Unprocessed
Dis
Noisy
Esti
Mild
Clean"
K,0.9122807017543859,"Figure 13: Performance of the conditional diffusion enhancement model with different dropout
probability in CHIME-4. From left to right: (1) without dropout; (2) dropout 50% ; (3) dropout 90%."
K,0.9147869674185464,"50 40 30 20 10
1"
K,0.9172932330827067,"50
40
30
20
10
1"
K,0.9197994987468672,"50 40 30 20 10
1"
K,0.9223057644110275,"50
40
30
20
10
1"
K,0.924812030075188,"50 40 30 20 10
1"
K,0.9273182957393483,"50
40
30
20
10
1"
K,0.9298245614035088,"50 40 30 20 10
1"
K,0.9323308270676691,"50
40
30
20
10
1 2.35 2.40 2.45 2.50 2.55 1.10 1.20 1.30 1.40 1.50 0.70 0.75 0.80 0.85 0.930 0.932 0.934"
K,0.9348370927318296,"Figure 14: Performance of two-step speech generation. From left to right: (1) PESQ on matched VB
dataset; (2) STOI on matched VB dataset; (3) PESQ on mismatched CHIME-4 dataset; (4) STOI on
mismatched CHIME-4 dataset."
K,0.9373433583959899,"speech – and the diffusion enhancement model will degenerate into a deterministic model, losing its
generalizability."
K,0.9398496240601504,"Timesteps τ1 and τ2. Considering the computational complexity of the “one-step” grid search, we
search optimal hyperparameters with a slightly larger step. Specifically, we select both optimal τ1
and τ2 from the predefined set {1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50}. We show the PESQ and STOI
performance of different combinations in Figure 14. We can observe that τ1 > τ2 (excluding STOI on
matched VB dataset, and the difference is negligible: 0.930 ∼0.934) will lead to better performance,
which is in line with our analysis (§4.4)."
K,0.9423558897243107,"A.7
Visual case of excessive suppression"
K,0.9448621553884712,We present several visual cases of excessive suppression.
K,0.9473684210526315,"Figure 15: Excessive suppression visualization (CHIME-4, DOSE). From left to right: (1) estimated
condition; (2) 2 steps; (3) clean; (4) noisy speech."
K,0.949874686716792,"A.8
Visual case of error accumulation"
K,0.9523809523809523,We present several visual cases of error accumulation.
K,0.9548872180451128,"Figure 16: Error accumulation visualization (VB, DOSE). From left to right: (1) full (50) steps; (2) 2
steps; (3) clean; (4) noisy speech."
K,0.9573934837092731,"A.9
Discussion"
K,0.9598997493734336,"To help readers better understand our approach, we analyze the reasons behind the better generaliz-
ability of diffusion enhancement models compared to deterministic mapping-based models (from the
robust training perspective), explain why we use 0.5 in the mild version of the condition factor, and
discuss the broader impacts of speech enhancement methods."
K,0.9624060150375939,"A.9.1
Generalization analysis"
K,0.9649122807017544,"Given that full-step diffusion enhancement models have better generalizability than deterministic
models, we now delve into the following question: if we are just using diffusion models as one-step
(or two-step) denoisers, what accounts for their enhanced performance compared to deterministic
mapping-based models?"
K,0.9674185463659147,"To answer this question, we need to point out that training a diffusion model can be considered
equivalent to training a multi-task paradigm. This involves training a model with shared parameters
on multiple levels of Gaussian noise concurrently. Recent research [21] has demonstrated that the full
training process of diffusion models leads to significantly improved one-shot denoising capabilities,
which are more generalizable compared to previous works that trained standalone denoisers on a
single noise level. Please refer to [21] (§5.2) for more details."
K,0.9699248120300752,"A.9.2
Why use 0.5 in the mild version of the condition factor? (reviewer boQC)"
K,0.9724310776942355,"Employing an equal weight provides stability, yet the performance during instances with low SNR
would be compromised (albeit still superior to direct utilization of y as a condition factor, unless the
condition optimizer falters). One prospective solution involves introducing an additional adaptive
strategy, i.e., c = αfθ(y) + (1 −α)y. We can design an adaptive alpha predictor and hope it
can output α based on the quality of raw condition and samples from the condition optimizer. For
example, when the condition optimizer produces lower-quality samples, giving more weight to the
original condition factor would make sense. Conversely, if the raw condition factor has a low SNR,
emphasizing the generated counterpart could be more effective. However, implementing this idea
practically is intricate."
K,0.974937343358396,"Given our strong reliance on diffusion-enhanced models to enhance generalization, any new adaptive
strategy must be generalizable. For instance, training an adaptive alpha predictor on the (seen)
VB-dataset (high SNR & consistent condition optimizer performance) could lead the model to
consistently output higher α values for fusion. Unfortunately, this auxiliary model might not
effectively adapt to variations when evaluating the mismatched (unseen) CHIME-4 dataset (low
SNR & potential condition optimizer challenges). To this end, we might need other techniques such
as data augmentation and adversarial training to improve its generalizability and robustness. This
creates a dilemma: harnessing the speech diffusion model for overarching speech noise reduction
generalization while simultaneously necessitating a pre-established generalized model to facilitate its
implementation. So far, despite our efforts to train a strong alpha predictor, progress has been limited
(the alpha predictor is still not generalizable, and the new system has no significant performance
improvement over DOSE)."
K,0.9774436090225563,"A.9.3
Broader impacts"
K,0.9799498746867168,"As speech enhancement technology continues to advance and become more prevalent, it’s important
to consider its broader impacts."
K,0.9824561403508771,"Positive impacts. The impact of speech enhancement technology on real-life situations, particularly
for individuals with hearing impairments, cannot be overstated. Hearing aids have long been the
primary solution for those with hearing loss, but they are not always effective in noisy environments
or for certain types of hearing loss. Speech enhancement technology can greatly improve speech
intelligibility and communication for hearing aid users. For example, some hearing aids have
AI-powered speech enhancement that boosts speech quality."
K,0.9849624060150376,"In addition to the benefits for individuals with hearing impairments, speech enhancement technol-
ogy also has significant implications for various applications. In transportation, clearer and more
intelligible speech can improve communication between pilots and air traffic control, leading to
safer and more efficient air travel. In industry, speech enhancement can improve communication on"
K,0.9874686716791979,"noisy factory floors, leading to increased productivity and safety. In educational settings, speech
enhancement can improve student comprehension and engagement during lectures and presentations."
K,0.9899749373433584,"Negative impacts. While speech enhancement technology has the potential to greatly improve
communication and speech intelligibility, one potential concern is that speech enhancement could
modify the semantic content of speech, potentially misleading listeners. Thus, it’s important for
developers of speech enhancement technology to consider this potential negative effect and work
towards creating trustworthy systems."
K,0.9924812030075187,"A.10
Experimental details"
K,0.9949874686716792,"Speech preprocessing. We process the speech waveform at a 16 kHz sampling rate. To maintain
dimensionality consistency within mini-batches, we pad each utterance to 2 seconds (32000 points)
using a zero-padding technique."
K,0.9974937343358395,"Basic architecture. To make a fair comparison, we use DiffWave [7] as the basic architecture
following [4, 9] – the only difference being the change in the way of condition-injecting since most
speech enhancement methods will directly use noisy speech as the condition factor, rather than
Mel-spectrogram. We concatenate the condition factor with the intermediate-generated sample along
the channel dimension as the model’s input. Specifically, the network is composed of 30 residual
layers with residual channels 128. We use a bidirectional dilated convolution (Bi-DilConv) with
kernel size 3 in each layer. We sum the skip connections from all residual layers. The total number of
trainable parameters is 2.31M, slightly smaller than naive DiffWave (2.64M). Please refer to [7] and
our code for more details."
