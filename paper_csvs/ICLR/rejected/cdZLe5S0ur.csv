Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010111223458038423,"The development and deployment of federated learning (FL) have been bottle-
necked by the heavy communication overheads of high-dimensional models be-
tween the distributed client nodes and the central server. To achieve better error-
communication tradeoffs, recent efforts have been made to either adaptively re-
duce the communication frequency by skipping unimportant updates, a.k.a. lazily-
aggregated quantization (LAQ), or adjust the quantization bits for each communi-
cation. In this paper, we propose a unifying communication efficient framework
for FL based on adaptive quantization of lazily-aggregated gradients (AQUILA),
which adaptively adjusts two mutually-dependent factors, the communication fre-
quency and the quantization level, in a synergistic way. Specifically, we start from
a careful investigation on the classical LAQ scheme and formulate AQUILA as
an optimization problem where the optimal quantization level per communication
is selected by minimizing the gradient loss caused by updates skipping. Mean-
while, we adjust the LAQ strategy to better fit the novel quantization criterion
and thus keep the communication frequency at an appropriate level. The effec-
tiveness and convergence of the proposed AQUILA framework are theoretically
verified. The experimental results demonstrate that AQUILA can reduce around
50% of overall transmitted bits compared to existing methods while achieving
the same level of model accuracy in a number of non-homogeneous FL scenarios,
including Non-IID data distribution and heterogeneous model architecture. The
proposed AQUILA is highly adaptive and compatible to existing FL settings."
INTRODUCTION,0.0020222446916076846,"1
INTRODUCTION"
INTRODUCTION,0.003033367037411527,"With the deployment of ubiquitous sensing and computing devices, the Internet of things (IoT) as
well as many other distributed systems have gradually grown from concept to reality, bringing dra-
matic convenience to people’s daily life (Du et al., 2020; Liu et al., 2020a; Hard et al., 2018). To fully
utilize such distributed computing resources, distributed learning provides a promising framework
that can achieve comparable performance with the traditional centralized learning scheme. How-
ever, the privacy and security of sensitive data during the updating and transmitting processes in dis-
tributed learning have been a growing concern. In this context, federated learning (FL) (McMahan
et al., 2017; Yang et al., 2019) has been developed, enabling distributed devices to collaboratively
learn a global model without privacy leakage by keeping private data sets isolated and masking
transmitted information with secure approaches like differential privacy (Abadi et al., 2016), secret
sharing techniques (Bonawitz et al., 2017) and homomorphic encryption (Liu et al., 2020b). Due
to its privacy and security preserving property and great potentials in some distributed but privacy-
sensitive fields like finance and health, FL has attracted tremendous attentions from both academia
and industry in recent years."
INTRODUCTION,0.004044489383215369,"Unfortunately, in many FL applications like image classification and objective recognition, the mod-
els to be trained tend to be high-dimensional, which lead to heavy communication overheads, for
example, a Resnet-152 network has over 58 million parameters (He et al., 2016). Hence, com-
munication efficiency has become one of the key bottlenecks of FL. To this end, recent researches
have tried to reduce the communication frequency, for example, Sun et al. (2020) proposes the
lazily-aggregated quantization (LAQ) method to reduce communication rounds by skipping some"
INTRODUCTION,0.005055611729019211,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.006066734074823054,"unnecessary parameter uploads. To further reduce transmitted bits per communication, LAQ can
be used jointly with gradient compression techniques, e.g. quantization and sparsification (Strom,
2015; Wangni et al., 2018; Lin et al., 2018; Han et al., 2020). Moreover, Mao et al. (2021) devel-
ops the Adaptive Quantized Gradient (AQG) for LAQ to adjust the quantization bit among multiple
given levels during training. However, the AQG is not sufficiently adaptive, for example, in the
two-level AQG with 4 bit and 2 bit, the situation of 3 bit and 1 bit is not covered at all. In a sepa-
rate line of work, Jhunjhunwala et al. (2021) develops an adaptive quantization rule (AdaQuantFL)
for FL which can search in a given range for an optimal quantization level and achieve a better
error-communication tradeoff."
INTRODUCTION,0.007077856420626896,"Previous work has investigated how to optimize communication frequency or adjust quantization
levels in a highly adaptive fashion, but not both. Intuitively, we ask the question, can we adaptively
adjust the quantization level in LAQ to further reduce communication rounds and transmitted bits
simultaneously? A straightforward approach is to train LAQ jointly with state-of-the-art adaptive
quantization methods like AdaQuantFL. However, quantization level and communication frequency
are mutually dependent in establishing the model convergence and must be adjusted together. For
example, lazy aggregation leads to the skip of some gradients that no longer require quantization,
while the choice of quantization levels directly affects the quality of a gradient update and thus
whether it is selected for transmission and aggregation. Therefore, the key question of our work is
how to jointly leverage these two complementary yet mutually-dependent degrees of freedom for
further optimizing communication efficiency in FL."
INTRODUCTION,0.008088978766430738,"The key idea of this paper is to select the optimal quantization bits for each communication round
in LAQ by optimizing the gradient loss caused by skipping quantized updates, which gives a novel
quantization criterion that can cooperate with LAQ strategy to further reduce overall transmitted
bits while maintaining the desired convergence properties of LAQ. The contributions of this paper
are trifold and summarized as follows. 1) We propose a FL framework with adaptive quantization
of lazily-aggregated gradients termed AQUILA, which simultaneously adjusts the communication
frequency and quantization level in a synergistic fashion. 2) We formulate AQUILA as an optimiza-
tion problem and develop an upperbound for the gradient loss caused by communication skipping,
which gives a novel adaptive quantization criterion which is theoretically proven to be more efficient
compared to AdaQuantFL while maintaining the same convergence properties. 3) We experimen-
tally evaluate the performance of AQUILA in a number of non-homogeneous FL settings, including
Non-IID data distribution and various heterogeneous model architecture. The experimental results
show that AQUILA can significantly mitigate the communication overhead over a number of base-
lines including fixed-bit LAQ and the naive combination of LAQ and AdaQuantFL. Our approach is
highly adaptive and compatible to existing FL settings."
BACKGROUND AND RELATED WORK,0.00910010111223458,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.010111223458038422,"Consider a FL system with one central server and a clients set M of M distributed clients to collab-
oratively train a global model parameterized by θ∗. At iteration k, each client m ∈M first trains
the global model θk on its local data Dk
m, and sends the local gradient gk
m = ∇fm(Dk
m; θk) to the
central server. Then the server aggregates the parameters and updates the global parameter by:"
BACKGROUND AND RELATED WORK,0.011122345803842264,θk+1 = θk −α M X
BACKGROUND AND RELATED WORK,0.012133468149646108,"m∈M
gk
m.
(1)"
BACKGROUND AND RELATED WORK,0.01314459049544995,"To reduce communication overheads with gradient quantization, the stochastic uniform quantizer
(Alistarh et al., 2017) is usually adopted. For any local gradient g ∈Rd, the quantized value of its
i-th dimension with quantization level b is defined as:"
BACKGROUND AND RELATED WORK,0.014155712841253791,"Qb (gi) = ∥g∥2 · sign (gi) · ξi(g, b),
(2)"
BACKGROUND AND RELATED WORK,0.015166835187057633,"where ξi(g, b) is a random variable defined as follows. Let l ∈{0, 1, 2, ..., b−1} be an integer
satisfying |gi| /∥g∥2 ∈[ l/b, (l + 1)/b ), then:"
BACKGROUND AND RELATED WORK,0.016177957532861477,"ξi(g, b) =
 (ℓ+ 1)/b
with probability (b · ∥gi∥)/∥g∥2 −l
ℓ/b
otherwise.
(3)"
BACKGROUND AND RELATED WORK,0.017189079878665317,Under review as a conference paper at ICLR 2022
BACKGROUND AND RELATED WORK,0.01820020222446916,"It is clear that with quantization level b, the number of bits for transmitting a quantized gradient from
a client to the central server is Cb = d ⌈log2(b + 1)⌉+ d + 32, with 32 bits for ∥g∥2, 1 bit for each
sign (gi), and log2(b + 1) bits for each ξi(g, b)."
BACKGROUND AND RELATED WORK,0.019211324570273004,"For communication rounds reduction, the LAQ proposes to let the client m ∈M upload its newly-
quantized local gradient Qb(gk
m) at iteration k only when the change in local gradient is sufficiently
large, i.e.,"
BACKGROUND AND RELATED WORK,0.020222446916076844,"Qb(gk
m) −Qb(ˆgk−1
m
)

2 2 ≥"
BACKGROUND AND RELATED WORK,0.021233569261880688,"PD
d=1 ξd
θk+1−d −θk−d
2"
BACKGROUND AND RELATED WORK,0.022244691607684528,"2
α2M 2
+ 3(∥εb(ˆgk−1
m
)∥2
2 +
εb(gk
m)
2"
BACKGROUND AND RELATED WORK,0.023255813953488372,"2), (4)"
BACKGROUND AND RELATED WORK,0.024266936299292215,"where Qb(ˆgk−1
m
) is the last quantized upload from client m, εb(ˆgk−1
m
) and εb(gk
m) denote quantiza-
tion errors, and {ξd}D
d=1 are some predetermined constant weights. Notice here a fixed quantization
level b is used. In LAQ, if the difference between client m’s newly-quantized local gradient Qb(gk
m)
and the last upload is smaller than a threshold involving quantization errors and global model’s in-
novation, client m will skip the upload of Qb(gk
m) at iteration k and the central server will reuses
Qb(ˆgk−1
m
) for such lazy aggregation:"
BACKGROUND AND RELATED WORK,0.025278058645096056,"ˆθ
k+1 = θk −α M X"
BACKGROUND AND RELATED WORK,0.0262891809908999,"m∈M
Qb(ˆgk
m) = θk −α M X"
BACKGROUND AND RELATED WORK,0.027300303336703743,"m∈M\Mk
0"
BACKGROUND AND RELATED WORK,0.028311425682507583,"Qb(gk
m) −α M X"
BACKGROUND AND RELATED WORK,0.029322548028311426,"m∈Mk
0"
BACKGROUND AND RELATED WORK,0.030333670374115267,"Qb(ˆgk−1
m
),
(5)"
BACKGROUND AND RELATED WORK,0.03134479271991911,"where Mk
0 denotes the subset of clients that skip the new gradient update and reuse the old quantized
gradient at iteration k. Besides, ˆgk
m represents the actual gradient for aggregation from client m,
which is gk
m for m ∈M \ Mk
0, while ˆgk−1
m
for m ∈Mk
0."
BACKGROUND AND RELATED WORK,0.032355915065722954,"Recently, AdaQuantFL is proposed to achieve a better error-communication tradeoff by adaptively
adjusting the quantization levels during FL training (Jhunjhunwala et al., 2021).
Specifically,
AdaQuantFL computes iteration k’s optimal quantization level b∗
k based on the following criterion
involving training loss and initial quantization level b0:"
BACKGROUND AND RELATED WORK,0.033367037411526794,"b∗
k = r"
BACKGROUND AND RELATED WORK,0.034378159757330634,"f
 
θ0
/f

θk
· b0,
(6)"
BACKGROUND AND RELATED WORK,0.03538928210313448,"where f(θ0) and f(θk) are the global training loss at iteration 0 and k, respectively."
BACKGROUND AND RELATED WORK,0.03640040444893832,"However, AdaQuantFL transmits quantized gradients at every iteration. In order to skip unneces-
sary communication rounds and adaptively adjust quantization level for each communication jointly,
an naive approach is to quantize lazily aggregated gradients with AdaQuantFL. Nevertheless, it fails
to achieve efficient communication due to a number of reasons. Firstly, given the descending trend of
training loss, AdaQuantFL’s criterion (6) may lead to high quantization bit number even exceeding
32 late in the training process, which is too much for cases where the global convergence is already
approaching. Secondly, higher quantization level results in smaller quantization error, which will
lead to lower communication threshold in LAQ’s criterion (4) and thus higher frequency of trans-
mission. Therefore it is desirable to develop more efficient adaptive quantization method in the
lazily-aggregated setting to systematically improve communication efficiency in FL."
METHOD,0.03741152679474216,"3
METHOD"
METHOD,0.03842264914054601,"3.1
ADAPTIVE QUANTIZATION OF LAZILY-AGGREGATED GRADIENTS (AQUILA)"
METHOD,0.03943377148634985,"Given the above limitations of the naive joint use of existing adaptive quantization criterion and lazy
aggregation strategy, this paper aims to design a unifying framework for communication efficiency
optimization where the quantization level and communication frequency are adjusted in a synergistic
and interactive way. Based on a careful investigation on LAQ, we design a novel quantization
criterion where the optimal quantization level is selected by minimizing the expected gradient loss
caused by skipping quantized updates. The rationale behind such strategy is, by formulating the
adaptive quantization problem of lazily-aggregated gradients as optimizing the expected gradient
loss with respect to the number of quantization bits, we can get an adaptive quantization criterion
based on local gradient updates while maintaining or even improving the convergence property of"
METHOD,0.04044489383215369,Under review as a conference paper at ICLR 2022
METHOD,0.041456016177957536,"LAQ. To get the optimization target, we first derive an upperbound for the expected gradient loss
in terms of quantization bits as elaborated in Section 3.2, which gives a novel adaptive quantization
criterion (7) that selects the quantization level for client m at iteration k based on initial quantization
level b0 and the change between client m’s newly computed gradient gk
m and the last uploaded
gradient ˆgk−1
m
:"
METHOD,0.042467138523761376,"(bk
m)∗= b0 ·
qg1m −ˆg0
m
2
2/
gkm −ˆgk−1
m
2
2.
(7)"
METHOD,0.043478260869565216,"The superiority of (7) comes from the following two aspects. Firstly, the gradient updates tend
to fluctuate along with the training process instead of keeping descending like the loss value, and
thus prevent the quantization level from increasing tremendously compared with the initial level.
Secondly, with lazy aggregation criterion based on gradient updates like (4), the transmitted bits
in AQUILA is further controlled, since the gradient update for actual transmission in (7) is lower
bounded by the lazy aggregation criterion, and therefore high-bit transmission for small update is
more likely to be skipped."
METHOD,0.044489383215369056,"To better fit the larger quantization errors induced by fewer quantization bits in (7), we modify the
communication criterion as follows to avoid the potential expansion of clients group to be skipped:"
METHOD,0.0455005055611729,"Qbkm(gk
m)−Qˆbk−1
m (ˆgk−1
m
)

2 2 ≥"
METHOD,0.046511627906976744,"PD
d=1ξd
θk+1−d−θk−d
2"
METHOD,0.047522750252780584,"2
α2M 2
+2
εbkm(gk
m)−εˆbk−1
m (ˆgk−1
m
)

2"
METHOD,0.04853387259858443,"2, (8)"
METHOD,0.04954499494438827,"where all the notations are the same as in (4) except the heterogeneous quantization level bk
m and
ˆbk−1
m
for each client. For detailed development of (8), please refer to the Appendix.  ࢍ૚
࢑"
METHOD,0.05055611729019211,"濷
Client 1
Client 2
Client 3
Client m"
METHOD,0.05156723963599596,Server 濷
METHOD,0.0525783619817998,"ࢍ૛
࢑
ࢍ૜
࢑
ࢍ࢓
࢑ skip"
METHOD,0.05358948432760364,"ܳ௕ೖࢍ૛
࢑
ܳ௕ೖࢍ૜
࢑
ܳ௕ೖࢍ࢓
࢑"
METHOD,0.054600606673407485,"ࣂ࢑ା૚= ࣂ࢑െߙ෍
௠ୀଵ"
METHOD,0.055611729019211326,"ெ
ܳ෠௕ೖ(ෝࢍ࢓
࢑)"
METHOD,0.056622851365015166,"ܾ௞
AdaQuantFL
LAQ
ܾ௞=
݂(ߠ଴)/݂(ߠ௞) ȉ ܾ଴"
METHOD,0.057633973710819006,"AdaQuantFL + LAQ
AQUILA ࢍ૚
࢑"
METHOD,0.05864509605662285,"ܳ௕మೖࢍ૛
࢑
ܳ௕యೖࢍ૜
࢑"
METHOD,0.05965621840242669,"ࣂ࢑ା૚= ࣂ࢑െߙ෍
௠ୀଵ"
METHOD,0.06066734074823053,"ெ
ܳ෠௕೘
ೖ(ෝࢍ࢓
࢑)"
METHOD,0.06167846309403438,"Client 1
Client 2
Client 3
Client m"
METHOD,0.06268958543983821,Server 濷
METHOD,0.06370070778564206,"ܾ௠
௞
Quantization level selection
Lazy aggregation strategy skip"
METHOD,0.06471183013144591,"ࢍ૛
࢑
ࢍ૜
࢑
ࢍ࢓
࢑ skip 濷"
METHOD,0.06572295247724974,"suppress transmission with high ܾ௠
௞"
METHOD,0.06673407482305359,minimize gradient loss caused by skipping
METHOD,0.06774519716885744,"(a) 
(b)"
METHOD,0.06875631951466127,"Figure 1: The schematic illustration of the communication efficient FL with AQUILA in comparison
with the naive combination of AdaQuantFL and LAQ. The blue lines indicating the transmission of
quantized gradients in AQUILA are drawn in different thicknesses to represent different quantization
levels selected by various clients."
METHOD,0.06976744186046512,"The cooperation of the novel adaptive quantization criterion (7) and the modified lazy aggrega-
tion strategy (8) is illustrated in Fig. 1a. Compared to the naive combination of AdaQuantFL and
LAQ where the mutual influence between adaptive quantization and lazy aggregation has not been
considered as shown in Fig. 1b, our AQUILA framework adaptively optimizes the allocation of
quantization bits throughout training to promote the convergence of lazy aggregation, and at the
same time utilizes the lazy aggregation strategy to improve the efficiency of adaptive quantization
by skipping high-bit transmission. The proposed AQUILA’s effect of suppressing the transmission
of high quantization bits has been verified in our experiments, as shown in Fig. 20 in the Appendix.
Besides, with the adjusted lazy aggregation strategy (8), AQUILA well addresses the problem of
high communication frequency in the late training process of naive combination of AdaQuantFL
and LAQ, as indicated by Fig. 2."
METHOD,0.07077856420626896,"The proposed AQUILA is summarized as follows in Algorithm 1. At iteration k = 0, all clients are
forced to transmit local gradients quantized with the initial level b0. At iteration k ∈{1, 2, ..., K},
the server first broadcasts the global model θk to all clients. Each client m computes gk
m with
local training data, and then uses it to select an optimal quantization level by (7). Then, each client"
METHOD,0.0717896865520728,Under review as a conference paper at ICLR 2022
METHOD,0.07280080889787664,"computes its gradient update after quantization and determines whether to upload the update or not
based on the communication criterion (8). Finally, the server updates the new global model θk+1"
METHOD,0.07381193124368049,"with up-to-date quantized gradients Qbkm(gk
m) for those clients who send the uploads at iteration k,
while reusing the old quantized gradients Qˆbk−1
m (ˆgk−1
m
) for those who skip the uploads."
METHOD,0.07482305358948432,"(a) 
(b)"
METHOD,0.07583417593528817,"Figure 2: Comparison of AQUILA and AdaQuantFL+LAQ on the number of unskipped clients per
step in three experiment settings with homogeneous model architecture. AdaQuantFL clashes with
the threshold condition in LAQ and results in high communication frequency due to the increasing
quantization level late in the training process, but AQUILA keeps the communication frequency at
an appropriate level throughout the training."
METHOD,0.07684529828109202,Algorithm 1 Communication Efficient FL with AQUILA
METHOD,0.07785642062689585,"Input: the number of communication rounds K, the learning rate α, the maximum communication
level bmax
Initialize: the initial global model parameter θ0 and the initial quantization level b0."
METHOD,0.0788675429726997,"1: Server broadcasts θ0 to all clients.
2: for each client m ∈M in parallel do
3:
Calculates local gradient g0
m and sends the quantized gradient Qb0(g0
m).
4:
Set ˆg0
m = g0
m and ˆb0
m = b0 on both sides.
5: end for
6: for k = 1, 2, ..., K do
7:
Server broadcasts θk to all clients.
8:
for each client m ∈M in parallel do
9:
Calculates local gradient gk
m.
10:
Computes the optimal local quantization level bk
m by (7).
11:
if bk
m ≥bmax then
12:
bk
m = bmax.
13:
end if
14:
if (8) holds for client m then
15:
Client m computes and sends the quantized gradient Qbkm(gk
m)."
METHOD,0.07987866531850354,"16:
Set ˆgk
m = gk
m and ˆbk
m = bk
m on both sides.
17:
else
18:
Client m sends nothing.
19:
Set ˆgk
m = ˆgk−1
m
and ˆbk
m = ˆbk−1
m
on both sides.
20:
end if
21:
end for"
METHOD,0.08088978766430738,"22:
Server updates θk+1 by θk −α
XM"
METHOD,0.08190091001011122,"m=1 Qˆbkm(ˆgk
m)."
METHOD,0.08291203235591507,23: end for
METHOD,0.0839231547017189,Under review as a conference paper at ICLR 2022
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.08493427704752275,"3.2
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.0859453993933266,"As mentioned before, in this work we bound the expected gradient loss caused by skipping
updates with respect to quantization bits. Specifically, if the communication criterion (8) holds
for client m at iteration k, it does not contribute to iteration k’s gradient loss, otherwise, the loss
caused by client m will be minimized with the optimal quantization criterion (7). In this sec-
tion, the theoretical derivation of the target upper bound is based on following standard assumptions:"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.08695652173913043,"Assumption 1. Loss function f(θ) =
X"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.08796764408493428,"m∈M fm(θ) is L-smooth, and fm(θ) is Lm-smooth."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.08897876643073811,"Assumption 2. The quantization operation is unbiased with E[Qb(w)|w] = w, and its variance
satisfy: ∀w ∈Rd, E[∥Qb(w) −w∥2
2 |w] ≤qb ∥w∥2
2, where qb is a positive constant."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.08998988877654196,"Here we adopt the definition of B in AdaQuantFL, but modified it as follows:
Definition 1 (Number of Bits sent from client m to the server, Bm). The total number of bits that
has been sent from client m to the central server until a given time instant is denoted by Bm.
With the definition of Bm, the upper bound of the loss induced by gradient skipping is indicated by
the following theorem:
Theorem 1. Under Assumption 1 and 2, the expected gradient loss caused by lazy aggregation has
the following upper bound:"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.0910010111223458,"E[
ˆθ
k+1 −θk+1
2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09201213346814964,2] ≤3α2 M X m∈M
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09302325581395349,"dmd

log2(4bk
m)
 Bm"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09403437815975733,"gk
m −ˆgk−1
m

2 2 +3α2 M X m∈M 
dσ2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09504550050556117,"(ˆbk−1
m
)2 + dσ2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09605662285136501,(bkm)2
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09706774519716886,"
+ 3α2 M X m∈M"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.0980788675429727,dm(d + 32) Bm
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.09908998988877654,"gk
m −ˆgk−1
m

2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10010111223458039,"2 ,
(9)"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10111223458038422,"where σ2 denotes the maximum ∥w∥2
2, and dm is a positive constant determined by Lm. See the
Appendix for proof details."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10212335692618807,"For each client m, the optimal bk
m is selected by setting the first derivative of the upper bound (9) to
zero:"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10313447927199192,"(bk
m)∗="
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10414560161779575,"r
2σ2Bmloge(2)

/

dm
gkm −ˆgk−1
m

2 2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.1051567239635996,"
.
(10)"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10616784630940344,"Therefore, an adaptive quantization criterion which does not rely on unknown parameters such as σ
and dm can be achieved through dividing bk
m by b1
m, which gives the adaptive selection criterion in
(7) if b1
m for each client m is chosen as the initial quantization level b0."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10717896865520728,"Comparison with AdaQuantFL. In AdaQuantFL, the optimal quantization level is selected by
minimizing the upper bound of the expected squared norm of the gradient ∇f(θk) of a non-convex
objective function, and thus the convergence of AdaQuantFL is guaranteed. However, the proposed
AQUILA is proven to be more efficient with sufficiently small learning rates as shown in Theorem
2. Besides, it is theoretically guaranteed that the squared norm of the global gradient ∇f(θk) is also
able to converge under AQUILA’ s adaptive quantization criterion developed for lazy aggregation."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10819009100101112,"Theorem 2. Under Assumption 1 and 2, if the learning rate α satisfies α ≤(1/ML) ·
p"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.10920121334681497,"2Pξ/L,
the following inequality holds:"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.1102123356926188,"(bk
m)∗= b0"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11122345803842265,"v
u
u
u
t"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.1122345803842265,"g1m −ˆg0
m
2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11324570273003033,"2
gkm −ˆgk−1
m

2 2 ≤b0"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11425682507583418,"v
u
u
tα2M 2L3 θ1 −θ02"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11526794742163801,"2
2ξ[f(θk) −f(θ∗)]
≤b0 s"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11627906976744186,"f(θ0) −f(θ∗)
f(θk) −f(θ∗)
,
(11)"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.1172901921132457,where ξ and P are both positive constants. See the Appendix for proof details.
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11830131445904954,"Theorem 2 indicates that with appropriate learning rate, the proposed AQUILA uses less number of
bits per communication compared to AdaQuantFL."
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.11931243680485339,Under review as a conference paper at ICLR 2022
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.12032355915065723,"Convergence analysis. The Lyapunov function of AQUILA is defined in the same way as LAQ and
AQG (Mao et al., 2021):"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.12133468149646107,"V(θk) = f(θk) −f(θ∗) + D
X d=1 D
X j=d ξj α"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.12234580384226491,"θk+1−d −θk−d
2"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.12335692618806876,"2 ,
(12)"
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.1243680485338726,where θ∗is the optimal solution of minθf(θ).
THEORETICAL DERIVATION AND ANALYSIS OF AQUILA,0.12537917087967643,"Convergence guarantee of federated learning with lazy aggregation has been well discussed in Sun
et al. (2020). More specifically, both the objective residual f(θk) −f(θ∗) and the parameter dif-
ferences term in Lyapunov function are guaranteed to descend along with the training process.
Therefore, the squared norm of the gradient ∇f(θk) is also guaranteed to converge based on the
L-smoothness assumption which results in: ∥∇f(θk)∥2
2 ≤2L[f(θk) −f(θ∗)]."
EXPERIMENTS AND DISCUSSION,0.1263902932254803,"4
EXPERIMENTS AND DISCUSSION"
EXPERIMENT SETUP,0.12740141557128412,"4.1
EXPERIMENT SETUP"
EXPERIMENT SETUP,0.12841253791708795,"Dataset. In this paper, we evaluate our method with MNIST and CIFAR10 dataset, considering both
IID and Non-IID data distribution. To simulate Non-IID situation, each client is assigned with two
classes of data at most and the amount of data for each class is balanced."
EXPERIMENT SETUP,0.12942366026289182,"Parameters. We set total client number M = 10, and follow the settings in LAQ, where D = 10
and ξ1 = ξ2 = · · · = ξD = 0.8/D. In terms of initial quantization level, a low level b0 = 2 is
chosen for simpler tasks with MNIST, and a larger level b0 = 6 is selected for more complex tasks
with CIFAR10. For both AdaQuantFL and AQUILA, we set an upperbound for quantization level
as bmax = 16 in case that the level grows too high."
EXPERIMENT SETUP,0.13043478260869565,"Training. We train a CNN with MNIST and a Resnet18 network with CIFAR10. The hyperparam-
eters of our experiments are shown in Table 1 in Appendix."
EXPERIMENT SETUP,0.13144590495449948,"We first evaluate our proposed AQUILA with homogeneous settings where all the local models share
the same architecture as the global model. The performance of AQUILA is compared with several
state-of-the-art methods including FedAvg (McMahan et al., 2017), QSGD (Alistarh et al., 2017),
AdaQuantFL, fixed-bit LAQ and the naive combination of AdaQuantFL with LAQ. For the choice
of quantization level for fixed-bit LAQ, since AQUILA’s initial level b0 is 2 for MNIST and 6 for
CIFAR10, we compare AQUILA with LAQ-2 and LAQ-6 for MNIST and CIFAR 10 respectively.
Besides, the performance of LAQ with the upperbound bmax = 16 is also evaluated."
EXPERIMENT SETUP,0.13245702730030334,"We also evaluate our proposed AQUILA with HeteroFL (Diao et al., 2020), where the local models
trained at clients’ side are heterogeneous. Assume the global model at iteration k is θk
g and its size
is dg · hg, then the local model of each client m can be selected by θk
m = θk
g [: dm, : hm], where
dm = rmdg and hm = rmhg respectively. In this paper, we choose three various model complexity
levels r = {a, b, c} = {1, 0.5, 0.25}."
EXPERIMENT SETUP,0.13346814964610718,"Fig. 3 shows the training loss vs total transmitted bits curve of experiments with IID MNIST / CI-
FAR10 for homogeneous model architecture, IID MNIST / CIFAR10 for 100%-50% heterogeneous
model architecture, and Non-IID MNIST / CIFAR10 for 100%-25% heterogeneous model architec-
ture. The corresponding transmitted bits vs steps curves of the above experiment settings are shown
in Fig. 4, which represents how many bits are transmitted in each step. Methods without adaptive
quantization and lazy aggregation like FedAvg and QSGD are not included in Fig. 4 for simplicity.
All the other experiment results are provided in Appendix, with Fig. 5 to Fig. 9 for homogeneous
models, and Fig. 10 to Fig. 19 for heterogeneous models."
PERFORMANCE ANALYSIS,0.134479271991911,"4.2
PERFORMANCE ANALYSIS"
PERFORMANCE ANALYSIS,0.13549039433771487,"In this part, we analyze the performance of AQUILA with various experiment settings including
Non-IID data distribution and heterogeneous model architecture. From figures in this paper, we can
observe that:"
PERFORMANCE ANALYSIS,0.1365015166835187,Under review as a conference paper at ICLR 2022
PERFORMANCE ANALYSIS,0.13751263902932254,"• AQUILA achieves a significant transmission reduction on both MNIST and CIFAR10 as
compared to AdaQuantFL and the naive combination of LAQ and AdaQuantFL. For in-
stance, AQUILA saves 65.98% of transmitted bits compared with AdaQuantFL and 62.05%
compared with the naive combination of LAQ and AdaQuantFL in the IID scenario with CI-
FAR10 dataset as shown in Fig. 3d, and other figures all show an obvious reduction in terms
of the total transmitted bits required for convergence. Fig. 4 verifies that the quantization
level selected by AQUILA will not continuously increase during training like AdaQuantFL
and LAQ with AdaQuantFL."
PERFORMANCE ANALYSIS,0.1385237613751264,"• Similarly, AQUILA outperforms all fixed-level LAQ in terms of overall transmitted bits
for both MNIST and CIFAR10, as shown in Fig. 3. For example, Fig. 3a indicates that
AQUILA reduces 42.6% of total transmitted bits compared with LAQ-2 and 92.32% com-
pared with LAQ-16 in MNIST. The transmission reduction is 38.76% for LAQ-6 and
78.09% for LAQ-16 in CIFAR10 as shown in Fig. 3e. Besides, Fig. 3 and Fig. 4 indicate
that although LAQ with fixed but low quantization level like LAQ-2 and LAQ-4 sometimes
transmit smaller amount of bits per step compared with AQUILA, they suffer from lower
accuracy and slower convergence. It further verifies the necessity and effectiveness of our
well-designed adaptive quantization criterion which achieves fast convergence with similar
low-bit transmission but without degradation of the model performance."
PERFORMANCE ANALYSIS,0.13953488372093023,"• AQUILA also works well with heterogeneous local models. With the two various ways
of distributing the heterogeneous local models in this paper, our proposed AQUILA still
outperforms other methods by significantly reducing overall transmitted bits while main-
taining the same convergence property and model accuracy. Please refer to Fig. 10 to
Fig. 19 in the Appendix for more detailed information. These experimental results in non-
homogeneous FL settings prove that our proposed AQUILA can be used in a more general
and complicated federated learning scenarios."
PERFORMANCE ANALYSIS,0.14054600606673406,"(a) 
(b) 
(c)"
PERFORMANCE ANALYSIS,0.14155712841253792,"(d) 
(e) 
(f)"
PERFORMANCE ANALYSIS,0.14256825075834176,"Figure 3: Training Loss vs Total Transmitted Bits. In this figure, LAQ-2 and LAQ-16 represent
LAQ with fixed quantization level of 2 and 16 respectively. AdaQuantFL+LAQ represents the naive
combination of AdaQuantFL and LAQ. For heterogeneous model architecture, 100%-50% implies
that half of clients have 100% of global model, whereas the other half of clients just share 50%∗50%
of the global model. Similarly, 100%-25% means the other half of clients just share 25% ∗25% of
the global model. Particularly, we zoom in the end of the curves to better compare AQUILA with
other methods."
PERFORMANCE ANALYSIS,0.1435793731041456,Under review as a conference paper at ICLR 2022
CONCLUSIONS AND FUTURE WORK,0.14459049544994945,"5
CONCLUSIONS AND FUTURE WORK"
CONCLUSIONS AND FUTURE WORK,0.14560161779575329,"This paper proposes a communication efficient FL framework to simultaneously adjust two
mutually-dependent degrees of freedom: communication frequency and quantization level. With the
close cooperation of the novel adaptive quantization and adjusted lazy aggregation strategy devel-
oped in this paper, the proposed AQUILA has been proven to be capable of reducing the transmitted
bits while maintaining the same convergence property and model performance compared against ex-
isting methods both theoretically and experimentally. The evaluation with Non-IID data distribution
and various heterogeneous model architectures demonstrates that AQUILA is compatible to existing
FL settings. Future works include further improvements and theoretical guarantee for FL systems
with heterogeneity."
CONCLUSIONS AND FUTURE WORK,0.14661274014155712,"(f) 
(e) (d)"
CONCLUSIONS AND FUTURE WORK,0.14762386248736098,"(b) 
(a) (c)"
CONCLUSIONS AND FUTURE WORK,0.1486349848331648,"Figure 4: Transmitted Bits vs Steps. For better illustration, the results have been down-sampled.
The solid lines represent values after down-sampling and the shadows around them represent the
true values."
CONCLUSIONS AND FUTURE WORK,0.14964610717896865,Under review as a conference paper at ICLR 2022
REFERENCES,0.1506572295247725,REFERENCES
REFERENCES,0.15166835187057634,"Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308–318, 2016."
REFERENCES,0.15267947421638017,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
QSGD:
Communication-efficient sgd via gradient quantization and encoding. In Proceedings of Advances
in Neural Information Processing Systems 30, pp. 1709–1720, 2017."
REFERENCES,0.15369059656218403,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175–1191, 2017."
REFERENCES,0.15470171890798787,"Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efficient
federated learning for heterogeneous clients. In Proceedings of the 8th International Conference
on Learning Representations, 2020."
REFERENCES,0.1557128412537917,"Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng Ji, and Jie Li.
Federated learning for vehicular Internet of things: Recent advances and open issues.
IEEE
Computer Graphics and Applications, pp. 45–61, 2020."
REFERENCES,0.15672396359959556,"Pengchao Han, Shiqiang Wang, and Kin K Leung. Adaptive gradient sparsification for efficient
federated learning: An online learning approach. In Proceedings of the 40th IEEE International
Conference on Distributed Computing Systems, pp. 300–310, 2020."
REFERENCES,0.1577350859453994,"Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc¸oise Beaufays, Sean
Augenstein, Hubert Eichner, Chlo´e Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018."
REFERENCES,0.15874620829120323,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778, 2016."
REFERENCES,0.1597573306370071,"Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C Eldar. Adaptive quantization
of model updates for communication-efficient federated learning. In Proceedings of the 2021
IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3110–3114,
2021."
REFERENCES,0.16076845298281092,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In Proceedings of the 6th International
Conference on Learning Representations, 2018."
REFERENCES,0.16177957532861476,"Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian
Chen, Han Yu, and Qiang Yang. Fedvision: An online visual object detection platform powered
by federated learning. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, pp.
13172–13179, 2020a."
REFERENCES,0.16279069767441862,"Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang. A secure federated transfer
learning framework. IEEE Intelligent Systems, pp. 70–82, 2020b."
REFERENCES,0.16380182002022245,"Yuzhu Mao, Zihao Zhao, Guangfeng Yan, Yang Liu, Tian Lan, Linqi Song, and Wenbo
Ding. Communication efficient federated learning with adaptive quantization. arXiv preprint
arXiv:2104.06023, 2021."
REFERENCES,0.16481294236602628,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273–1282, 2017."
REFERENCES,0.16582406471183014,"Nikko Strom. Scalable distributed DNN training using commodity GPU cloud computing. In Pro-
ceedings of 16th Annual Conference of the International Speech Communication Association, pp.
1488–1492, 2015."
REFERENCES,0.16683518705763398,Under review as a conference paper at ICLR 2022
REFERENCES,0.1678463094034378,"Jun Sun, Tianyi Chen, Georgios B Giannakis, Qinmin Yang, and Zaiyue Yang. Lazily aggregated
quantized gradient innovation for communication-efficient federated learning. IEEE Transactions
on Pattern Analysis & Machine Intelligence, pp. 1–15, 2020."
REFERENCES,0.16885743174924167,"Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Proceedings of Advances in Neural Information Processing
Systems 31, pp. 1299–1309, 2018."
REFERENCES,0.1698685540950455,"Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, pp. 1–19, 2019."
REFERENCES,0.17087967644084934,Under review as a conference paper at ICLR 2022
REFERENCES,0.1718907987866532,"A
APPENDIX"
REFERENCES,0.17290192113245703,"A.1
SUPPLEMENTARY EXPERIMENTAL RESULTS"
REFERENCES,0.17391304347826086,"The appendix includes supplementary experimental results, mathematical proof of Theorem 1 and
Theorem 2, and detailed derivation of the novel adaptive quantization criterion and lazy aggregation
strategy. Compared to Fig. 3 and Fig. 4 in the main text, result figures in appendix show a more
comprehensive evaluation with AQUILA, which contain more detailed information including but
not limited to accuracy vs steps and training loss vs steps curves."
REFERENCES,0.17492416582406473,Table 1: Hyperparameters in training process
REFERENCES,0.17593528816986856,"Dataset
MNIST
CIFAR10
Model
CNN
ResNet18
Hidden Size
[64, 128, 256, 512]
[64, 128, 256, 512]
Data Distribution
IID
Non-IID
IID
Non-IID
Global Epoch E
300
100
3000
100
Local Batch Size B
200
10
100
10
Optimizer
SGD
SGD
SGD
SGD
Momentum
0.9
0.9
0.9
0.9
Weight Decay
5.00E-04
5.00E-04
5.00E-04
5.00E-04
Learning Rate η
0.01
0.01
0.1
0.1"
REFERENCES,0.1769464105156724,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.17795753286147623,"T
otal
T
ransmitted
Bits(GB) 20 40 60 80"
REFERENCES,0.1789686552072801,Accuracy(%)
REFERENCES,0.17997977755308392,"Homo -MNIST
-IID Parameters"
REFERENCES,0.18099089989888775,FedAvg QSGD
REFERENCES,0.1820020222446916,L AQ -2
REFERENCES,0.18301314459049545,L AQ -16
REFERENCES,0.18402426693629928,AdaQuantFL
REFERENCES,0.18503538928210314,AdaQuantFL+L AQ
REFERENCES,0.18604651162790697,AQUIL A
REFERENCES,0.1870576339737108,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.18806875631951467,"0
50
100
150
200
250
300 Steps 20 40 60 80"
REFERENCES,0.1890798786653185,Accuracy(%)
REFERENCES,0.19009100101112233,"Homo -MNIST
-IID Parameters"
REFERENCES,0.1911021233569262,FedAvg QSGD
REFERENCES,0.19211324570273003,L AQ -2
REFERENCES,0.19312436804853386,L AQ -16
REFERENCES,0.19413549039433772,AdaQuantFL
REFERENCES,0.19514661274014156,AdaQuantFL+L AQ
REFERENCES,0.1961577350859454,AQUIL A
REFERENCES,0.19716885743174925,(b) Accuracy vs Steps
REFERENCES,0.19817997977755308,"0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5"
REFERENCES,0.19919110212335692,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0"
REFERENCES,0.20020222446916078,"T
raining Loss"
REFERENCES,0.2012133468149646,"Homo -MNIST
-IID Parameters"
REFERENCES,0.20222446916076844,FedAvg QSGD
REFERENCES,0.2032355915065723,L AQ -2
REFERENCES,0.20424671385237614,L AQ -16
REFERENCES,0.20525783619817997,AdaQuantFL
REFERENCES,0.20626895854398383,AdaQuantFL+L AQ
REFERENCES,0.20728008088978767,AQUIL A
REFERENCES,0.2082912032355915,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.20930232558139536,"0
50
100
150
200
250
300 Steps 0.5 1.0 1.5 2.0"
REFERENCES,0.2103134479271992,"T
raining Loss"
REFERENCES,0.21132457027300303,"Homo -MNIST
-IID Parameters"
REFERENCES,0.2123356926188069,FedAvg QSGD
REFERENCES,0.21334681496461072,L AQ -2
REFERENCES,0.21435793731041455,L AQ -16
REFERENCES,0.21536905965621841,AdaQuantFL
REFERENCES,0.21638018200202225,Adaptive+L AQ
REFERENCES,0.21739130434782608,AQUIL A
REFERENCES,0.21840242669362994,(d) Training Loss vs Steps
REFERENCES,0.21941354903943378,Figure 5: Homo-MNIST-IID
REFERENCES,0.2204246713852376,Under review as a conference paper at ICLR 2022
REFERENCES,0.22143579373104147,"0
1
2
3
4
5
6"
REFERENCES,0.2224469160768453,"T
otal
T
ransmitted
Bits(GB) 0 20 40 60 80 100"
REFERENCES,0.22345803842264914,Accuracy(%)
REFERENCES,0.224469160768453,"Homo -MNIST
-Non-IID Parameters"
REFERENCES,0.22548028311425683,FedAvg QSGD
REFERENCES,0.22649140546006066,L AQ -2
REFERENCES,0.2275025278058645,L AQ -16
REFERENCES,0.22851365015166836,AdaQuantFL
REFERENCES,0.2295247724974722,AdaQuantFL+L AQ
REFERENCES,0.23053589484327602,AQUIL A
REFERENCES,0.23154701718907988,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.23255813953488372,"0
20
40
60
80
100 Steps 0 20 40 60 80 100"
REFERENCES,0.23356926188068755,Accuracy(%)
REFERENCES,0.2345803842264914,"Homo -MNIST
-Non-IID Parameters"
REFERENCES,0.23559150657229525,FedAvg QSGD
REFERENCES,0.23660262891809908,L AQ -2
REFERENCES,0.23761375126390294,L AQ -16
REFERENCES,0.23862487360970677,AdaQuantFL
REFERENCES,0.2396359959555106,AdaQuantFL+L AQ
REFERENCES,0.24064711830131447,AQUIL A
REFERENCES,0.2416582406471183,(b) Accuracy vs Steps
REFERENCES,0.24266936299292213,"0
1
2
3
4
5
6"
REFERENCES,0.243680485338726,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.24469160768452983,"T
raining Loss"
REFERENCES,0.24570273003033366,"Homo -MNIST
-Non-IID Parameters"
REFERENCES,0.24671385237613752,FedAvg QSGD
REFERENCES,0.24772497472194135,L AQ -2
REFERENCES,0.2487360970677452,L AQ -16
REFERENCES,0.24974721941354905,AdaQuantFL
REFERENCES,0.25075834175935285,AdaQuantFL+L AQ
REFERENCES,0.2517694641051567,AQUIL A
REFERENCES,0.2527805864509606,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.2537917087967644,"0
20
40
60
80
100 Steps 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.25480283114256824,"T
raining Loss"
REFERENCES,0.2558139534883721,"Homo -MNIST
-Non-IID Parameters"
REFERENCES,0.2568250758341759,FedAvg QSGD
REFERENCES,0.25783619817997977,L AQ -2
REFERENCES,0.25884732052578363,L AQ -16
REFERENCES,0.25985844287158744,AdaQuantFL
REFERENCES,0.2608695652173913,Adaptive+L AQ
REFERENCES,0.26188068756319516,AQUIL A
REFERENCES,0.26289180990899896,(d) Training Loss vs Steps
REFERENCES,0.2639029322548028,Figure 6: Homo-MNIST-Non-IID
REFERENCES,0.2649140546006067,"0
200
400
600
800
1000
1200"
REFERENCES,0.2659251769464105,"T
otal
T
ransmitted
Bits(GB) 20 40 60 80"
REFERENCES,0.26693629929221435,Accuracy(%)
REFERENCES,0.2679474216380182,"Homo -CIF
AR10-IID Parameters"
REFERENCES,0.268958543983822,FedAvg QSGD
REFERENCES,0.2699696663296259,L AQ -4
REFERENCES,0.27098078867542974,L AQ -6
REFERENCES,0.27199191102123355,L AQ -16
REFERENCES,0.2730030333670374,AdaQuantFL
REFERENCES,0.27401415571284127,AdaQuantFL+L AQ
REFERENCES,0.2750252780586451,AQUIL A
REFERENCES,0.27603640040444893,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.2770475227502528,"0
500
1000
1500
2000
2500
3000 Steps 20 40 60 80"
REFERENCES,0.2780586450960566,Accuracy(%)
REFERENCES,0.27906976744186046,"Homo -CIF
AR10-IID Parameters"
REFERENCES,0.2800808897876643,FedAvg QSGD
REFERENCES,0.2810920121334681,L AQ -4
REFERENCES,0.282103134479272,L AQ -6
REFERENCES,0.28311425682507585,L AQ -16
REFERENCES,0.28412537917087965,AdaQuantFL
REFERENCES,0.2851365015166835,AdaQuantFL+L AQ
REFERENCES,0.2861476238624874,AQUIL A
REFERENCES,0.2871587462082912,(b) Accuracy vs Steps
REFERENCES,0.28816986855409504,"0
200
400
600
800
1000
1200"
REFERENCES,0.2891809908998989,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0"
REFERENCES,0.2901921132457027,"T
raining Loss"
REFERENCES,0.29120323559150657,"Homo -CIF
AR10-IID Parameters"
REFERENCES,0.29221435793731043,FedAvg QSGD
REFERENCES,0.29322548028311424,L AQ -4
REFERENCES,0.2942366026289181,L AQ -6
REFERENCES,0.29524772497472196,L AQ -16
REFERENCES,0.29625884732052576,AdaQuantFL
REFERENCES,0.2972699696663296,AdaQuantFL+L AQ
REFERENCES,0.2982810920121335,AQUIL A
REFERENCES,0.2992922143579373,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.30030333670374115,"0
500
1000
1500
2000
2500
3000 Steps 0.5 1.0 1.5 2.0"
REFERENCES,0.301314459049545,"T
raining Loss"
REFERENCES,0.3023255813953488,"Homo -CIF
AR10-IID Parameters"
REFERENCES,0.3033367037411527,FedAvg QSGD
REFERENCES,0.30434782608695654,L AQ -4
REFERENCES,0.30535894843276035,L AQ -6
REFERENCES,0.3063700707785642,L AQ -16
REFERENCES,0.30738119312436807,AdaQuantFL
REFERENCES,0.3083923154701719,Adaptive+L AQ
REFERENCES,0.30940343781597573,AQUIL A
REFERENCES,0.3104145601617796,(d) Training Loss vs Steps
REFERENCES,0.3114256825075834,Figure 7: Homo-CIFAR-IID
REFERENCES,0.31243680485338726,Under review as a conference paper at ICLR 2022
REFERENCES,0.3134479271991911,"0
10
20
30
40"
REFERENCES,0.31445904954499493,"T
otal
T
ransmitted
Bits(GB) 20 30 40 50 60 70"
REFERENCES,0.3154701718907988,Accuracy(%)
REFERENCES,0.31648129423660265,"Homo -CIF
AR10-Non-IID Parameters"
REFERENCES,0.31749241658240646,FedAvg QSGD
REFERENCES,0.3185035389282103,L AQ -4
REFERENCES,0.3195146612740142,L AQ -6
REFERENCES,0.320525783619818,L AQ -16
REFERENCES,0.32153690596562184,AdaQuantFL
REFERENCES,0.3225480283114257,AdaQuantFL+L AQ
REFERENCES,0.3235591506572295,AQUIL A
REFERENCES,0.32457027300303337,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.32558139534883723,"0
20
40
60
80
100 Steps 20 30 40 50 60 70"
REFERENCES,0.32659251769464104,Accuracy(%)
REFERENCES,0.3276036400404449,"Homo -CIF
AR10-Non-IID Parameters"
REFERENCES,0.32861476238624876,FedAvg QSGD
REFERENCES,0.32962588473205257,L AQ -4
REFERENCES,0.3306370070778564,L AQ -6
REFERENCES,0.3316481294236603,L AQ -16
REFERENCES,0.3326592517694641,AdaQuantFL
REFERENCES,0.33367037411526795,AdaQuantFL+L AQ
REFERENCES,0.3346814964610718,AQUIL A
REFERENCES,0.3356926188068756,(b) Accuracy vs Steps
REFERENCES,0.3367037411526795,"0
10
20
30
40"
REFERENCES,0.33771486349848334,"T
otal
T
ransmitted
Bits(GB) 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.33872598584428715,"T
raining Loss"
REFERENCES,0.339737108190091,"Homo -CIF
AR10-Non-IID Parameters"
REFERENCES,0.34074823053589487,FedAvg QSGD
REFERENCES,0.3417593528816987,L AQ -4
REFERENCES,0.34277047522750254,L AQ -6
REFERENCES,0.3437815975733064,L AQ -16
REFERENCES,0.3447927199191102,AdaQuantFL
REFERENCES,0.34580384226491406,AdaQuantFL+L AQ
REFERENCES,0.3468149646107179,AQUIL A
REFERENCES,0.34782608695652173,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.3488372093023256,"0
20
40
60
80
100 Steps 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.34984833164812945,"T
raining Loss"
REFERENCES,0.35085945399393326,"Homo -CIF
AR10-Non-IID Parameters"
REFERENCES,0.3518705763397371,FedAvg QSGD
REFERENCES,0.3528816986855409,L AQ -4
REFERENCES,0.3538928210313448,L AQ -6
REFERENCES,0.35490394337714865,L AQ -16
REFERENCES,0.35591506572295245,AdaQuantFL
REFERENCES,0.3569261880687563,Adaptive+L AQ
REFERENCES,0.3579373104145602,AQUIL A
REFERENCES,0.358948432760364,(d) Training Loss vs Steps
REFERENCES,0.35995955510616784,Figure 8: Homo-CIFAR-Non-IID
REFERENCES,0.3609706774519717,"0
50
100
150
200
250
300
Steps 0 5 10 15 20 25 30"
REFERENCES,0.3619817997977755,Transmitted Bits(MB)
REFERENCES,0.36299292214357937,Homo-MNIST-IID Parameters
REFERENCES,0.3640040444893832,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.36501516683518703,(a) Transmitted Bits vs Steps
REFERENCES,0.3660262891809909,"0
20
40
60
80
100
Steps 0 5 10 15 20 25 30"
REFERENCES,0.36703741152679475,Transmitted Bits(MB)
REFERENCES,0.36804853387259856,Homo-MNIST-Non-IID Parameters
REFERENCES,0.3690596562184024,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.3700707785642063,(b) Transmitted Bits vs Steps
REFERENCES,0.3710819009100101,"0
500
1000
1500
2000
2500
3000
Steps 0 50 100 150 200"
REFERENCES,0.37209302325581395,Transmitted Bits(MB)
REFERENCES,0.3731041456016178,Homo-CIFAR10-IID Parameters
REFERENCES,0.3741152679474216,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.3751263902932255,(c) Transmitted Bits vs Steps
REFERENCES,0.37613751263902934,"0
20
40
60
80
100
Steps 0 50 100 150 200"
REFERENCES,0.37714863498483314,Transmitted Bits(MB)
REFERENCES,0.378159757330637,Homo-CIFAR10-Non-IID Parameters
REFERENCES,0.37917087967644086,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.38018200202224467,(d) Transmitted Bits vs Steps
REFERENCES,0.38119312436804853,Figure 9: Homo-Transmitted Bits vs Steps
REFERENCES,0.3822042467138524,Under review as a conference paper at ICLR 2022
REFERENCES,0.3832153690596562,"0
2
4
6
8
10"
REFERENCES,0.38422649140546006,"T
otal
T
ransmitted
Bits(GB) 20 40 60 80"
REFERENCES,0.3852376137512639,Accuracy(%)
REFERENCES,0.3862487360970677,"100%-50%-MNIST
-IID Parameters"
REFERENCES,0.3872598584428716,FedAvg QSGD
REFERENCES,0.38827098078867545,L AQ -2
REFERENCES,0.38928210313447925,L AQ -16
REFERENCES,0.3902932254802831,AdaQuantFL
REFERENCES,0.391304347826087,AdaQuantFL+L AQ
REFERENCES,0.3923154701718908,AQUIL A
REFERENCES,0.39332659251769464,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.3943377148634985,"0
50
100
150
200
250
300 Steps 20 40 60 80"
REFERENCES,0.3953488372093023,Accuracy(%)
REFERENCES,0.39635995955510617,"100%-50%-MNIST
-IID Parameters"
REFERENCES,0.39737108190091003,FedAvg QSGD
REFERENCES,0.39838220424671383,L AQ -2
REFERENCES,0.3993933265925177,L AQ -16
REFERENCES,0.40040444893832156,AdaQuantFL
REFERENCES,0.40141557128412536,AdaQuantFL+L AQ
REFERENCES,0.4024266936299292,AQUIL A
REFERENCES,0.4034378159757331,(b) Accuracy vs Steps
REFERENCES,0.4044489383215369,"0
2
4
6
8
10"
REFERENCES,0.40546006066734075,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0"
REFERENCES,0.4064711830131446,"T
raining Loss"
REFERENCES,0.4074823053589484,"100%-50%-MNIST
-IID Parameters"
REFERENCES,0.4084934277047523,FedAvg QSGD
REFERENCES,0.40950455005055614,L AQ -2
REFERENCES,0.41051567239635994,L AQ -16
REFERENCES,0.4115267947421638,AdaQuantFL
REFERENCES,0.41253791708796766,AdaQuantFL+L AQ
REFERENCES,0.41354903943377147,AQUIL A
REFERENCES,0.41456016177957533,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.4155712841253792,"0
50
100
150
200
250
300 Steps 0.5 1.0 1.5 2.0"
REFERENCES,0.416582406471183,"T
raining Loss"
REFERENCES,0.41759352881698686,"100%-50%-MNIST
-IID Parameters"
REFERENCES,0.4186046511627907,FedAvg QSGD
REFERENCES,0.4196157735085945,L AQ -2
REFERENCES,0.4206268958543984,L AQ -16
REFERENCES,0.42163801820020225,AdaQuantFL
REFERENCES,0.42264914054600605,Adaptive+L AQ
REFERENCES,0.4236602628918099,AQUIL A
REFERENCES,0.4246713852376138,(d) Training Loss vs Steps
REFERENCES,0.4256825075834176,Figure 10: 100%-50%-MNIST-IID
REFERENCES,0.42669362992922144,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5"
REFERENCES,0.4277047522750253,"T
otal
T
ransmitted
Bits(GB) 0 20 40 60 80 100"
REFERENCES,0.4287158746208291,Accuracy(%)
REFERENCES,0.42972699696663297,"100%-50%-MNIST
-Non-IID Parameters"
REFERENCES,0.43073811931243683,FedAvg QSGD
REFERENCES,0.43174924165824063,L AQ -2
REFERENCES,0.4327603640040445,L AQ -16
REFERENCES,0.43377148634984836,AdaQuantFL
REFERENCES,0.43478260869565216,AdaQuantFL+L AQ
REFERENCES,0.435793731041456,AQUIL A
REFERENCES,0.4368048533872599,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.4378159757330637,"0
20
40
60
80
100 Steps 0 20 40 60 80 100"
REFERENCES,0.43882709807886755,Accuracy(%)
REFERENCES,0.4398382204246714,"100%-50%-MNIST
-Non-IID Parameters"
REFERENCES,0.4408493427704752,FedAvg QSGD
REFERENCES,0.4418604651162791,L AQ -2
REFERENCES,0.44287158746208294,L AQ -16
REFERENCES,0.44388270980788674,AdaQuantFL
REFERENCES,0.4448938321536906,AdaQuantFL+L AQ
REFERENCES,0.44590495449949447,AQUIL A
REFERENCES,0.44691607684529827,(b) Accuracy vs Steps
REFERENCES,0.44792719919110213,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5"
REFERENCES,0.448938321536906,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.4499494438827098,"T
raining Loss"
REFERENCES,0.45096056622851366,"100%-50%-MNIST
-Non-IID Parameters"
REFERENCES,0.45197168857431747,FedAvg QSGD
REFERENCES,0.4529828109201213,L AQ -2
REFERENCES,0.4539939332659252,L AQ -16
REFERENCES,0.455005055611729,AdaQuantFL
REFERENCES,0.45601617795753285,AdaQuantFL+L AQ
REFERENCES,0.4570273003033367,AQUIL A
REFERENCES,0.4580384226491405,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.4590495449949444,"0
20
40
60
80
100 Steps 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.46006066734074824,"T
raining Loss"
REFERENCES,0.46107178968655205,"100%-50%-MNIST
-Non-IID Parameters"
REFERENCES,0.4620829120323559,FedAvg QSGD
REFERENCES,0.46309403437815977,L AQ -2
REFERENCES,0.4641051567239636,L AQ -16
REFERENCES,0.46511627906976744,AdaQuantFL
REFERENCES,0.4661274014155713,Adaptive+L AQ
REFERENCES,0.4671385237613751,AQUIL A
REFERENCES,0.46814964610717896,(d) Training Loss vs Steps
REFERENCES,0.4691607684529828,Figure 11: 100%-50%-MNIST-Non-IID
REFERENCES,0.47017189079878663,Under review as a conference paper at ICLR 2022
REFERENCES,0.4711830131445905,"0
100
200
300
400
500
600
700
800"
REFERENCES,0.47219413549039435,"T
otal
T
ransmitted
Bits(GB) 20 40 60 80"
REFERENCES,0.47320525783619816,Accuracy(%)
REFERENCES,0.474216380182002,"100%-50%-CIF
AR10-IID Parameters"
REFERENCES,0.4752275025278059,FedAvg QSGD
REFERENCES,0.4762386248736097,L AQ -4
REFERENCES,0.47724974721941354,L AQ -6
REFERENCES,0.4782608695652174,L AQ -16
REFERENCES,0.4792719919110212,AdaQuantFL
REFERENCES,0.48028311425682507,AdaQuantFL+L AQ
REFERENCES,0.48129423660262893,AQUIL A
REFERENCES,0.48230535894843274,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.4833164812942366,"0
500
1000
1500
2000
2500
3000 Steps 20 40 60 80"
REFERENCES,0.48432760364004046,Accuracy(%)
REFERENCES,0.48533872598584427,"100%-50%-CIF
AR10-IID Parameters"
REFERENCES,0.4863498483316481,FedAvg QSGD
REFERENCES,0.487360970677452,L AQ -4
REFERENCES,0.4883720930232558,L AQ -6
REFERENCES,0.48938321536905965,L AQ -16
REFERENCES,0.4903943377148635,AdaQuantFL
REFERENCES,0.4914054600606673,AdaQuantFL+L AQ
REFERENCES,0.4924165824064712,AQUIL A
REFERENCES,0.49342770475227504,(b) Accuracy vs Steps
REFERENCES,0.49443882709807885,"0
100
200
300
400
500
600
700
800"
REFERENCES,0.4954499494438827,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0"
REFERENCES,0.49646107178968657,"T
raining Loss"
REFERENCES,0.4974721941354904,"100%-50%-CIF
AR10-IID Parameters"
REFERENCES,0.49848331648129424,FedAvg QSGD
REFERENCES,0.4994944388270981,L AQ -4
REFERENCES,0.5005055611729019,L AQ -6
REFERENCES,0.5015166835187057,L AQ -16
REFERENCES,0.5025278058645096,AdaQuantFL
REFERENCES,0.5035389282103134,AdaQuantFL+L AQ
REFERENCES,0.5045500505561172,AQUIL A
REFERENCES,0.5055611729019212,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.506572295247725,"0
500
1000
1500
2000
2500
3000 Steps 0.5 1.0 1.5 2.0"
REFERENCES,0.5075834175935288,"T
raining Loss"
REFERENCES,0.5085945399393327,"100%-50%-CIF
AR10-IID Parameters"
REFERENCES,0.5096056622851365,FedAvg QSGD
REFERENCES,0.5106167846309403,L AQ -4
REFERENCES,0.5116279069767442,L AQ -6
REFERENCES,0.512639029322548,L AQ -16
REFERENCES,0.5136501516683518,AdaQuantFL
REFERENCES,0.5146612740141557,Adaptive+L AQ
REFERENCES,0.5156723963599595,AQUIL A
REFERENCES,0.5166835187057633,(d) Training Loss vs Steps
REFERENCES,0.5176946410515673,Figure 12: 100%-50%-CIFAR-IID
REFERENCES,0.5187057633973711,"0
5
10
15
20
25"
REFERENCES,0.5197168857431749,"T
otal
T
ransmitted
Bits(GB) 30 35 40 45 50 55 60 65 70"
REFERENCES,0.5207280080889788,Accuracy(%)
REFERENCES,0.5217391304347826,"100%-50%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.5227502527805864,FedAvg QSGD
REFERENCES,0.5237613751263903,L AQ -4
REFERENCES,0.5247724974721941,L AQ -6
REFERENCES,0.5257836198179979,L AQ -16
REFERENCES,0.5267947421638018,AdaQuantFL
REFERENCES,0.5278058645096056,AdaQuantFL+L AQ
REFERENCES,0.5288169868554095,AQUIL A
REFERENCES,0.5298281092012134,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.5308392315470172,"0
20
40
60
80
100 Steps 30 35 40 45 50 55 60 65 70"
REFERENCES,0.531850353892821,Accuracy(%)
REFERENCES,0.5328614762386249,"100%-50%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.5338725985844287,FedAvg QSGD
REFERENCES,0.5348837209302325,L AQ -4
REFERENCES,0.5358948432760364,L AQ -6
REFERENCES,0.5369059656218402,L AQ -16
REFERENCES,0.537917087967644,AdaQuantFL
REFERENCES,0.538928210313448,AdaQuantFL+L AQ
REFERENCES,0.5399393326592518,AQUIL A
REFERENCES,0.5409504550050556,(b) Accuracy vs Steps
REFERENCES,0.5419615773508595,"0
5
10
15
20
25"
REFERENCES,0.5429726996966633,"T
otal
T
ransmitted
Bits(GB) 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.5439838220424671,"T
raining Loss"
REFERENCES,0.544994944388271,"100%-50%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.5460060667340748,FedAvg QSGD
REFERENCES,0.5470171890798786,L AQ -4
REFERENCES,0.5480283114256825,L AQ -6
REFERENCES,0.5490394337714863,L AQ -16
REFERENCES,0.5500505561172901,AdaQuantFL
REFERENCES,0.5510616784630941,AdaQuantFL+L AQ
REFERENCES,0.5520728008088979,AQUIL A
REFERENCES,0.5530839231547017,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.5540950455005056,"0
20
40
60
80
100 Steps 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.5551061678463094,"T
raining Loss"
REFERENCES,0.5561172901921132,"100%-50%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.5571284125379171,FedAvg QSGD
REFERENCES,0.5581395348837209,L AQ -4
REFERENCES,0.5591506572295247,L AQ -6
REFERENCES,0.5601617795753286,L AQ -16
REFERENCES,0.5611729019211324,AdaQuantFL
REFERENCES,0.5621840242669363,Adaptive+L AQ
REFERENCES,0.5631951466127402,AQUIL A
REFERENCES,0.564206268958544,(d) Training Loss vs Steps
REFERENCES,0.5652173913043478,Figure 13: 100%-50%-CIFAR-Non-IID
REFERENCES,0.5662285136501517,Under review as a conference paper at ICLR 2022
REFERENCES,0.5672396359959555,"0
50
100
150
200
250
300
Steps 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.5682507583417593,Transmitted Bits(MB)
REFERENCES,0.5692618806875632,100%-50%-MNIST-IID Parameters
REFERENCES,0.570273003033367,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.5712841253791708,(a) Transmitted Bits vs Steps
REFERENCES,0.5722952477249748,"0
20
40
60
80
100
Steps 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"
REFERENCES,0.5733063700707786,Transmitted Bits(MB)
REFERENCES,0.5743174924165824,100%-50%-MNIST-Non-IID Parameters
REFERENCES,0.5753286147623863,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.5763397371081901,(b) Transmitted Bits vs Steps
REFERENCES,0.5773508594539939,"0
500
1000
1500
2000
2500
3000
Steps 0 20 40 60 80 100 120"
REFERENCES,0.5783619817997978,Transmitted Bits(MB)
REFERENCES,0.5793731041456016,100%-50%-CIFAR10-IID Parameters
REFERENCES,0.5803842264914054,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.5813953488372093,(c) Transmitted Bits vs Steps
REFERENCES,0.5824064711830131,"0
20
40
60
80
100
Steps 0 20 40 60 80 100 120"
REFERENCES,0.583417593528817,Transmitted Bits(MB)
REFERENCES,0.5844287158746209,100%-50%-CIFAR10-Non-IID Parameters
REFERENCES,0.5854398382204247,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.5864509605662285,(d) Transmitted Bits vs Steps
REFERENCES,0.5874620829120324,Figure 14: 100%-50%-Transmitted Bits vs Steps
REFERENCES,0.5884732052578362,"0
2
4
6
8"
REFERENCES,0.58948432760364,"T
otal
T
ransmitted
Bits(GB) 10 20 30 40 50 60 70 80 90"
REFERENCES,0.5904954499494439,Accuracy(%)
REFERENCES,0.5915065722952477,"100%-25%-MNIST
-IID Parameters"
REFERENCES,0.5925176946410515,FedAvg QSGD
REFERENCES,0.5935288169868554,L AQ -2
REFERENCES,0.5945399393326593,L AQ -16
REFERENCES,0.5955510616784631,AdaQuantFL
REFERENCES,0.596562184024267,AdaQuantFL+L AQ
REFERENCES,0.5975733063700708,AQUIL A
REFERENCES,0.5985844287158746,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.5995955510616785,"0
50
100
150
200
250
300 Steps 10 20 30 40 50 60 70 80 90"
REFERENCES,0.6006066734074823,Accuracy(%)
REFERENCES,0.6016177957532861,"100%-25%-MNIST
-IID Parameters"
REFERENCES,0.60262891809909,FedAvg QSGD
REFERENCES,0.6036400404448938,L AQ -2
REFERENCES,0.6046511627906976,L AQ -16
REFERENCES,0.6056622851365016,AdaQuantFL
REFERENCES,0.6066734074823054,AdaQuantFL+L AQ
REFERENCES,0.6076845298281092,AQUIL A
REFERENCES,0.6086956521739131,(b) Accuracy vs Steps
REFERENCES,0.6097067745197169,"0
2
4
6
8"
REFERENCES,0.6107178968655207,"T
otal
T
ransmitted
Bits(GB) 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.6117290192113246,"T
raining Loss"
REFERENCES,0.6127401415571284,"100%-25%-MNIST
-IID Parameters"
REFERENCES,0.6137512639029322,FedAvg QSGD
REFERENCES,0.6147623862487361,L AQ -2
REFERENCES,0.6157735085945399,L AQ -16
REFERENCES,0.6167846309403437,AdaQuantFL
REFERENCES,0.6177957532861477,AdaQuantFL+L AQ
REFERENCES,0.6188068756319515,AQUIL A
REFERENCES,0.6198179979777553,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.6208291203235592,"0
50
100
150
200
250
300 Steps 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.621840242669363,"T
raining Loss"
REFERENCES,0.6228513650151668,"100%-25%-MNIST
-IID Parameters"
REFERENCES,0.6238624873609707,FedAvg QSGD
REFERENCES,0.6248736097067745,L AQ -2
REFERENCES,0.6258847320525783,L AQ -16
REFERENCES,0.6268958543983822,AdaQuantFL
REFERENCES,0.627906976744186,Adaptive+L AQ
REFERENCES,0.6289180990899899,AQUIL A
REFERENCES,0.6299292214357938,(d) Training Loss vs Steps
REFERENCES,0.6309403437815976,Figure 15: 100%-25%-MNIST-IID
REFERENCES,0.6319514661274014,Under review as a conference paper at ICLR 2022
REFERENCES,0.6329625884732053,"0.0
0.5
1.0
1.5
2.0
2.5
3.0"
REFERENCES,0.6339737108190091,"T
otal
T
ransmitted
Bits(GB) 0 20 40 60 80"
REFERENCES,0.6349848331648129,Accuracy(%)
REFERENCES,0.6359959555106168,"100%-25%-MNIST
-Non-IID Parameters"
REFERENCES,0.6370070778564206,FedAvg QSGD
REFERENCES,0.6380182002022244,L AQ -2
REFERENCES,0.6390293225480284,L AQ -16
REFERENCES,0.6400404448938322,AdaQuantFL
REFERENCES,0.641051567239636,AdaQuantFL+L AQ
REFERENCES,0.6420626895854399,AQUIL A
REFERENCES,0.6430738119312437,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.6440849342770475,"0
20
40
60
80
100 Steps 0 20 40 60 80"
REFERENCES,0.6450960566228514,Accuracy(%)
REFERENCES,0.6461071789686552,"100%-25%-MNIST
-Non-IID Parameters"
REFERENCES,0.647118301314459,FedAvg QSGD
REFERENCES,0.6481294236602629,L AQ -2
REFERENCES,0.6491405460060667,L AQ -16
REFERENCES,0.6501516683518705,AdaQuantFL
REFERENCES,0.6511627906976745,AdaQuantFL+L AQ
REFERENCES,0.6521739130434783,AQUIL A
REFERENCES,0.6531850353892821,(b) Accuracy vs Steps
REFERENCES,0.654196157735086,"0.0
0.5
1.0
1.5
2.0
2.5
3.0"
REFERENCES,0.6552072800808898,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.6562184024266936,"T
raining Loss"
REFERENCES,0.6572295247724975,"100%-25%-MNIST
-Non-IID Parameters"
REFERENCES,0.6582406471183013,FedAvg QSGD
REFERENCES,0.6592517694641051,L AQ -2
REFERENCES,0.660262891809909,L AQ -16
REFERENCES,0.6612740141557129,AdaQuantFL
REFERENCES,0.6622851365015167,AdaQuantFL+L AQ
REFERENCES,0.6632962588473206,AQUIL A
REFERENCES,0.6643073811931244,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.6653185035389282,"0
20
40
60
80
100 Steps 0.5 1.0 1.5 2.0 2.5"
REFERENCES,0.6663296258847321,"T
raining Loss"
REFERENCES,0.6673407482305359,"100%-25%-MNIST
-Non-IID Parameters"
REFERENCES,0.6683518705763397,FedAvg QSGD
REFERENCES,0.6693629929221436,L AQ -2
REFERENCES,0.6703741152679474,L AQ -16
REFERENCES,0.6713852376137512,AdaQuantFL
REFERENCES,0.6723963599595552,Adaptive+L AQ
REFERENCES,0.673407482305359,AQUIL A
REFERENCES,0.6744186046511628,(d) Training Loss vs Steps
REFERENCES,0.6754297269969667,Figure 16: 100%-25%-MNIST-Non-IID
REFERENCES,0.6764408493427705,"0
100
200
300
400
500
600"
REFERENCES,0.6774519716885743,"T
otal
T
ransmitted
Bits(GB) 20 40 60 80"
REFERENCES,0.6784630940343782,Accuracy(%)
REFERENCES,0.679474216380182,"100%-25%-CIF
AR10-IID Parameters"
REFERENCES,0.6804853387259858,FedAvg QSGD
REFERENCES,0.6814964610717897,L AQ -4
REFERENCES,0.6825075834175935,L AQ -6
REFERENCES,0.6835187057633973,L AQ -16
REFERENCES,0.6845298281092013,AdaQuantFL
REFERENCES,0.6855409504550051,AdaQuantFL+L AQ
REFERENCES,0.6865520728008089,AQUIL A
REFERENCES,0.6875631951466128,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.6885743174924166,"0
500
1000
1500
2000
2500
3000 Steps 20 40 60 80"
REFERENCES,0.6895854398382204,Accuracy(%)
REFERENCES,0.6905965621840243,"100%-25%-CIF
AR10-IID Parameters"
REFERENCES,0.6916076845298281,FedAvg QSGD
REFERENCES,0.6926188068756319,L AQ -4
REFERENCES,0.6936299292214358,L AQ -6
REFERENCES,0.6946410515672397,L AQ -16
REFERENCES,0.6956521739130435,AdaQuantFL
REFERENCES,0.6966632962588474,AdaQuantFL+L AQ
REFERENCES,0.6976744186046512,AQUIL A
REFERENCES,0.698685540950455,(b) Accuracy vs Steps
REFERENCES,0.6996966632962589,"0
100
200
300
400
500
600"
REFERENCES,0.7007077856420627,"T
otal
T
ransmitted
Bits(GB) 0.5 1.0 1.5 2.0"
REFERENCES,0.7017189079878665,"T
raining Loss"
REFERENCES,0.7027300303336703,"100%-25%-CIF
AR10-IID Parameters"
REFERENCES,0.7037411526794742,FedAvg QSGD
REFERENCES,0.704752275025278,L AQ -4
REFERENCES,0.7057633973710818,L AQ -6
REFERENCES,0.7067745197168858,L AQ -16
REFERENCES,0.7077856420626896,AdaQuantFL
REFERENCES,0.7087967644084934,AdaQuantFL+L AQ
REFERENCES,0.7098078867542973,AQUIL A
REFERENCES,0.7108190091001011,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.7118301314459049,"0
500
1000
1500
2000
2500
3000 Steps 0.5 1.0 1.5 2.0"
REFERENCES,0.7128412537917088,"T
raining Loss"
REFERENCES,0.7138523761375126,"100%-25%-CIF
AR10-IID Parameters"
REFERENCES,0.7148634984833164,FedAvg QSGD
REFERENCES,0.7158746208291203,L AQ -4
REFERENCES,0.7168857431749242,L AQ -6
REFERENCES,0.717896865520728,L AQ -16
REFERENCES,0.7189079878665319,AdaQuantFL
REFERENCES,0.7199191102123357,Adaptive+L AQ
REFERENCES,0.7209302325581395,AQUIL A
REFERENCES,0.7219413549039434,(d) Training Loss vs Steps
REFERENCES,0.7229524772497472,Figure 17: 100%-25%-CIFAR-IID
REFERENCES,0.723963599595551,Under review as a conference paper at ICLR 2022
REFERENCES,0.7249747219413549,"0
5
10
15
20"
REFERENCES,0.7259858442871587,"T
otal
T
ransmitted
Bits(GB) 20 30 40 50 60 70"
REFERENCES,0.7269969666329625,Accuracy(%)
REFERENCES,0.7280080889787665,"100%-25%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.7290192113245703,FedAvg QSGD
REFERENCES,0.7300303336703741,L AQ -4
REFERENCES,0.731041456016178,L AQ -6
REFERENCES,0.7320525783619818,L AQ -16
REFERENCES,0.7330637007077856,AdaQuantFL
REFERENCES,0.7340748230535895,AdaQuantFL+L AQ
REFERENCES,0.7350859453993933,AQUIL A
REFERENCES,0.7360970677451971,(a) Accuracy vs Total Transmitted Bits
REFERENCES,0.737108190091001,"0
20
40
60
80
100 Steps 20 30 40 50 60 70"
REFERENCES,0.7381193124368048,Accuracy(%)
REFERENCES,0.7391304347826086,"100%-25%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.7401415571284126,FedAvg QSGD
REFERENCES,0.7411526794742164,L AQ -4
REFERENCES,0.7421638018200202,L AQ -6
REFERENCES,0.7431749241658241,L AQ -16
REFERENCES,0.7441860465116279,AdaQuantFL
REFERENCES,0.7451971688574317,AdaQuantFL+L AQ
REFERENCES,0.7462082912032356,AQUIL A
REFERENCES,0.7472194135490394,(b) Accuracy vs Steps
REFERENCES,0.7482305358948432,"0
5
10
15
20"
REFERENCES,0.7492416582406471,"T
otal
T
ransmitted
Bits(GB) 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.750252780586451,"T
raining Loss"
REFERENCES,0.7512639029322548,"100%-25%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.7522750252780587,FedAvg QSGD
REFERENCES,0.7532861476238625,L AQ -4
REFERENCES,0.7542972699696663,L AQ -6
REFERENCES,0.7553083923154702,L AQ -16
REFERENCES,0.756319514661274,AdaQuantFL
REFERENCES,0.7573306370070778,AdaQuantFL+L AQ
REFERENCES,0.7583417593528817,AQUIL A
REFERENCES,0.7593528816986855,(c) Training Loss vs Total Transmitted Bits
REFERENCES,0.7603640040444893,"0
20
40
60
80
100 Steps 0.75 1.00 1.25 1.50 1.75 2.00 2.25"
REFERENCES,0.7613751263902933,"T
raining Loss"
REFERENCES,0.7623862487360971,"100%-25%-CIF
AR10-Non-IID Parameters"
REFERENCES,0.7633973710819009,FedAvg QSGD
REFERENCES,0.7644084934277048,L AQ -4
REFERENCES,0.7654196157735086,L AQ -6
REFERENCES,0.7664307381193124,L AQ -16
REFERENCES,0.7674418604651163,AdaQuantFL
REFERENCES,0.7684529828109201,Adaptive+L AQ
REFERENCES,0.7694641051567239,AQUIL A
REFERENCES,0.7704752275025278,(d) Training Loss vs Steps
REFERENCES,0.7714863498483316,Figure 18: 100%-25%-CIFAR-Non-IID
REFERENCES,0.7724974721941354,"0
50
100
150
200
250
300
Steps 0 2 4 6 8 10 12 14 16"
REFERENCES,0.7735085945399394,Transmitted Bits(MB)
REFERENCES,0.7745197168857432,100%-25%-MNIST-IID Parameters
REFERENCES,0.775530839231547,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.7765419615773509,(a) Transmitted Bits vs Steps
REFERENCES,0.7775530839231547,"0
20
40
60
80
100
Steps 0 2 4 6 8 10 12 14 16"
REFERENCES,0.7785642062689585,Transmitted Bits(MB)
REFERENCES,0.7795753286147624,100%-25%-MNIST-Non-IID Parameters
REFERENCES,0.7805864509605662,"LAQ-2
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.78159757330637,(b) Transmitted Bits vs Steps
REFERENCES,0.782608695652174,"0
500
1000
1500
2000
2500
3000
Steps 0 20 40 60 80 100"
REFERENCES,0.7836198179979778,Transmitted Bits(MB)
REFERENCES,0.7846309403437816,100%-25%-CIFAR10-IID Parameters
REFERENCES,0.7856420626895855,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.7866531850353893,(c) Transmitted Bits vs Steps
REFERENCES,0.7876643073811931,"0
20
40
60
80
100
Steps 0 20 40 60 80 100"
REFERENCES,0.788675429726997,Transmitted Bits(MB)
REFERENCES,0.7896865520728008,100%-25%-CIFAR10-Non-IID Parameters
REFERENCES,0.7906976744186046,"LAQ-4
LAQ-6
LAQ-16
AdaQuantFL
AdaQuantFL+LAQ
AQUILA"
REFERENCES,0.7917087967644085,(d) Transmitted Bits vs Steps
REFERENCES,0.7927199191102123,Figure 19: 100%-25%-Transmitted Bits vs Steps
REFERENCES,0.7937310414560161,Under review as a conference paper at ICLR 2022
REFERENCES,0.7947421638018201,"0
20
40
60
80
100 Steps 5.0 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.7957532861476239,Local Quantization Level
REFERENCES,0.7967644084934277,"Hetero -CIF
AR10-IID"
REFERENCES,0.7977755308392316,Communication skipping
REFERENCES,0.7987866531850354,(a) 1st client in system
REFERENCES,0.7997977755308392,"0
20
40
60
80
100 Steps 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0"
REFERENCES,0.8008088978766431,Local Quantization Level
REFERENCES,0.8018200202224469,"Hetero -CIF
AR10-IID"
REFERENCES,0.8028311425682507,Communication skipping
REFERENCES,0.8038422649140546,(b) 2nd client in system
REFERENCES,0.8048533872598584,"0
20
40
60
80
100 Steps 5.0 5.5 6.0 6.5 7.0 7.5 8.0"
REFERENCES,0.8058645096056622,Local Quantization Level
REFERENCES,0.8068756319514662,"Hetero -CIF
AR10-IID"
REFERENCES,0.80788675429727,Communication skipping
REFERENCES,0.8088978766430738,(c) 3rd client in system
REFERENCES,0.8099089989888777,"Figure 20: The illustration about AQUILA’s ability to suppress high-bit transmission. Each figure
shows the relationship between local quantization level and communication skipping of one local
client during training with heterogeneous models in the IID scenario with CIFAR10 dataset. These
figures imply a trend that communications with relatively high quantization level (e.g. higher than
the initial level) chosen by (7) are mostly skipped by (8)."
REFERENCES,0.8109201213346815,"A.2
MATHEMATICAL PROOF"
REFERENCES,0.8119312436804853,"A.2.1
PROOF OF THEOREM 1"
REFERENCES,0.8129423660262892,"Without lazy aggregation, the aggregated model at iteration k should be:"
REFERENCES,0.813953488372093,θk+1 = θk −α M X
REFERENCES,0.8149646107178968,"m∈M
Qbk
m(gk
m).
(13)"
REFERENCES,0.8159757330637007,"With lazy aggregation, the actual aggregated model at iteration k is:"
REFERENCES,0.8169868554095046,"ˆθ
k+1 = θk −α M X"
REFERENCES,0.8179979777553084,"m∈M\Mk
0"
REFERENCES,0.8190091001011123,"Qbkm(gk
m) −α M X"
REFERENCES,0.8200202224469161,"m∈Mk
0"
REFERENCES,0.8210313447927199,"Qˆbk−1
m (ˆgk−1
m
).
(14)"
REFERENCES,0.8220424671385238,"With (13) and (14), the gradient loss caused by skipping can be written as:"
REFERENCES,0.8230535894843276,"ˆθ
k+1 −θk+1
2 2 =  α
M X"
REFERENCES,0.8240647118301314,"m∈Mk
0"
REFERENCES,0.8250758341759353,"Qbkm(gk
m) −Qˆbk−1
m (ˆgk−1
m
)  2 2 ≤α2M M 2
X m∈M"
REFERENCES,0.8260869565217391,"Qbkm(gk
m) −Qˆbk−1
m (ˆgk−1
m
)

2 2 = α2 M X m∈M"
REFERENCES,0.8270980788675429,"[Qbkm(gk
m) −gk
m] −[Qˆbk−1
m (ˆgk−1
m
) −(ˆgk−1
m
)] + (gk
m −ˆgk−1
m
)

2 2 ≤3α2 M X m∈M"
REFERENCES,0.8281092012133469,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8291203235591507,2 + 3α2 M X
REFERENCES,0.8301314459049545,"m∈M
(
εbk
m(gk
m)
2"
REFERENCES,0.8311425682507584,"2 +
εˆbk−1
m (ˆgk−1
m
)

2"
REFERENCES,0.8321536905965622,"2),
(15)"
REFERENCES,0.833164812942366,"where the two inequalities follow from:  n
X"
REFERENCES,0.8341759352881699,"i=1
ai  2 2
≤n n
X"
REFERENCES,0.8351870576339737,"i=1
∥ai∥2
2."
REFERENCES,0.8361981799797775,Take expectation of both sides of (15):
REFERENCES,0.8372093023255814,"E[
ˆθ
k+1 −θk+1
2"
REFERENCES,0.8382204246713852,2] ≤3α2 M X
REFERENCES,0.839231547017189,"m∈M
E[
gk
m −ˆgk−1
m

2"
REFERENCES,0.840242669362993,2] + 3α2 M X
REFERENCES,0.8412537917087968,"m∈M
(E[
εbkm(gk
m)
2"
REFERENCES,0.8422649140546006,"2] + E[
εˆbk−1
m (ˆgk−1
m
)

2 2)] ≤3α2 M X"
REFERENCES,0.8432760364004045,"m∈M
E[
gk
m −ˆgk−1
m

2"
REFERENCES,0.8442871587462083,2] + 3α2 M X
REFERENCES,0.8452982810920121,"m∈M
(
dσ2"
REFERENCES,0.846309403437816,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.8473205257836198,(bkm)2 ) ≤3α2 M X m∈M dm K
REFERENCES,0.8483316481294236,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8493427704752275,2 + 3α2 M X
REFERENCES,0.8503538928210314,"m∈M
(
dσ2"
REFERENCES,0.8513650151668352,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.8523761375126391,"(bkm)2 ),
(16)"
REFERENCES,0.8533872598584429,Under review as a conference paper at ICLR 2022
REFERENCES,0.8543983822042467,"where the second inequality follows from Assumption 2 with σ2 denoting the maximum ∥w∥2
2 and
qb = d/b2, and the last inequality is resulted from the conclusion in LAQ (Sun et al., 2020), which
indicates that the client m communicates with the server at most dm rounds in total K iterations,
where dm is a constant related to Lm."
REFERENCES,0.8554095045500506,"With the definition of Bm and Ck
m = d

log2(bk
m + 1)

+ d + 32, the expected gradient loss can be
written as:"
REFERENCES,0.8564206268958544,"E[
ˆθ
k+1 −θk+1
2"
REFERENCES,0.8574317492416582,2] ≤3α2 M X m∈M
REFERENCES,0.8584428715874621,"dmCk
m
Bm"
REFERENCES,0.8594539939332659,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8604651162790697,2 + 3α2 M X
REFERENCES,0.8614762386248737,"m∈M
(
dσ2"
REFERENCES,0.8624873609706775,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.8634984833164813,(bkm)2 ) = 3α2 M X m∈M
REFERENCES,0.8645096056622852,"dmd

log2(bk
m + 1)
 Bm"
REFERENCES,0.865520728008089,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8665318503538928,2 + 3α2 M X
REFERENCES,0.8675429726996967,"m∈M
(
dσ2"
REFERENCES,0.8685540950455005,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.8695652173913043,(bkm)2 ) + 3α2 M X m∈M
REFERENCES,0.8705763397371082,dm(d + 32) Bm
REFERENCES,0.871587462082912,"gk
m −ˆgk−1
m

2 2 ≤3α2 M X m∈M"
REFERENCES,0.8725985844287159,"dmd

log2(4bk
m)
 Bm"
REFERENCES,0.8736097067745198,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8746208291203236,2 + 3α2 M X
REFERENCES,0.8756319514661274,"m∈M
(
dσ2"
REFERENCES,0.8766430738119313,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.8776541961577351,(bkm)2 ) + 3α2 M X m∈M
REFERENCES,0.8786653185035389,dm(d + 32) Bm
REFERENCES,0.8796764408493428,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8806875631951466,"2 .
(17)"
REFERENCES,0.8816986855409504,This completes the proof for Theorem 1.
REFERENCES,0.8827098078867543,"A.2.2
DERIVATION DETAILS OF ADAPTIVE QUANTIZATION CRITERION (7)"
REFERENCES,0.8837209302325582,Let H be the upper bound to be minimized:
REFERENCES,0.884732052578362,H = 3α2 M X m∈M
REFERENCES,0.8857431749241659,"dmd

log2(4bk
m)
 Bm"
REFERENCES,0.8867542972699697,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8877654196157735,2 + 3α2 M X
REFERENCES,0.8887765419615774,"m∈M
(
dσ2"
REFERENCES,0.8897876643073812,"(ˆbk−1
m
)2 + dσ2"
REFERENCES,0.890798786653185,(bkm)2 ) + 3α2 M X m∈M
REFERENCES,0.8918099089989889,dm(d + 32) Bm
REFERENCES,0.8928210313447927,"gk
m −ˆgk−1
m

2"
REFERENCES,0.8938321536905965,"2 .
(18)"
REFERENCES,0.8948432760364005,The first derivative is:
REFERENCES,0.8958543983822043,"∂H
∂bkm
=
3α2did
gk
m −ˆgk−1
m

2"
REFERENCES,0.8968655207280081,2 ⌈log2 e⌉
REFERENCES,0.897876643073812,"BmMbkm
−6α2dσ2"
REFERENCES,0.8988877654196158,"M(bkm)3 .
(19)"
REFERENCES,0.8998988877654196,Let ∂H
REFERENCES,0.9009100101112234,"∂bkm
= 0, there is:"
REFERENCES,0.9019211324570273,"(bk
m)∗="
REFERENCES,0.9029322548028311,"v
u
u
t"
REFERENCES,0.9039433771486349,2σ2Bmloge(2)
REFERENCES,0.9049544994944388,"di
gkm −ˆgk−1
m

2 2"
REFERENCES,0.9059656218402427,".
(20)"
REFERENCES,0.9069767441860465,"Dividing (bk
m)∗by (b1
m):"
REFERENCES,0.9079878665318504,"(bk
m)∗= b1
m"
REFERENCES,0.9089989888776542,"v
u
u
u
t"
REFERENCES,0.910010111223458,"g1m −ˆg0
m
2"
REFERENCES,0.9110212335692619,"2
gkm −ˆgk−1
m

2 2"
REFERENCES,0.9120323559150657,".
(21)"
REFERENCES,0.9130434782608695,"For iteration k = 1, we set b1
m for each client m as the intial quantization level b0, and thus we get
the adaptive quantization criterion (7) for iteration k = 1, 2, ..., K:"
REFERENCES,0.9140546006066734,"(bk
m)∗= b0"
REFERENCES,0.9150657229524772,"v
u
u
u
t"
REFERENCES,0.916076845298281,"g1m −ˆg0
m
2"
REFERENCES,0.917087967644085,"2
gkm −ˆgk−1
m

2 2"
REFERENCES,0.9180990899898888,".
(22)"
REFERENCES,0.9191102123356926,Under review as a conference paper at ICLR 2022
REFERENCES,0.9201213346814965,"A.2.3
PROOF OF THEOREM 2"
REFERENCES,0.9211324570273003,"With the adjusted communication criterion (8), all transmitted gradient updates are larger than a
threshold after quantization. Therefore, all transmitted gradient updates satisfy:"
REFERENCES,0.9221435793731041,"gk
m −ˆgk−1
m

2"
REFERENCES,0.923154701718908,"2 =
gk
m −Qbkm
 
gk
m

+ Qˆbk−1
m"
REFERENCES,0.9241658240647118,"
ˆgk−1
m

−ˆgk−1
m
+ Qbkm
 
gk
m

−Qˆbk−1
m"
REFERENCES,0.9251769464105156,"
ˆgk−1
m

2 2"
REFERENCES,0.9261880687563195,"≈
Qbkm
 
gk
m

−Qˆbk−1
m"
REFERENCES,0.9271991911021233,"
ˆgk−1
m

2 2 ≥"
REFERENCES,0.9282103134479271,"PD
d=1 ξd
θk+1−d−θk−d
2"
REFERENCES,0.9292214357937311,"2
α2M 2
+3
εbkm(gk
m)
2"
REFERENCES,0.9302325581395349,"2+
εˆbk−1
m (ˆgk−1
m
)

2 2"
REFERENCES,0.9312436804853387,"
.
(23) then,"
REFERENCES,0.9322548028311426,"(bk
m)∗= b0"
REFERENCES,0.9332659251769464,"v
u
u
u
t"
REFERENCES,0.9342770475227502,"g1m −ˆg0
m
2"
REFERENCES,0.9352881698685541,"2
gkm −ˆgk−1
m

2 2 ≤b0"
REFERENCES,0.9362992922143579,"v
u
u
u
u
t"
REFERENCES,0.9373104145601617,"g1m −ˆg0
m
2 2"
REFERENCES,0.9383215369059656,"1
α2M 2
PD
d=1 ξd
θk+1−d −θk−d
2"
REFERENCES,0.9393326592517695,"2 + 3
εbkm(gkm)
2"
REFERENCES,0.9403437815975733,"2 +
εˆbk−1
m (ˆgk−1
m
)

2 2  ≤b0"
REFERENCES,0.9413549039433772,"v
u
u
u
u
t"
REFERENCES,0.942366026289181,"g1m −ˆg0
m
2 2"
REFERENCES,0.9433771486349848,"2
α2M 2L
PD
d=1 ξd(f(θk+1−d) −f(θk−d)) + 3
εbkm(gkm)
2"
REFERENCES,0.9443882709807887,"2 +
εˆbk−1
m (ˆgk−1
m
)

2 2  ≤b0"
REFERENCES,0.9453993933265925,"v
u
u
u
u
t"
REFERENCES,0.9464105156723963,"g1m −ˆg0
m
2 2"
REFERENCES,0.9474216380182002,"2ξd
α2M 2L[f(θk) −f(θk−D)] + 3
εbkm(gkm)
2"
REFERENCES,0.948432760364004,"2 +
εˆbk−1
m (ˆgk−1
m
)

2 2  ≤b0"
REFERENCES,0.9494438827098078,"v
u
u
t"
REFERENCES,0.9504550050556118,"g1m −ˆg0
m
2"
REFERENCES,0.9514661274014156,"2
2ξd
α2M 2L[f(θk) −f(θ∗)] ≤b0"
REFERENCES,0.9524772497472194,"v
u
u
t
(Lm)2 θ1 −θ02"
REFERENCES,0.9534883720930233,"2
2ξd
α2M 2L[f(θk) −f(θ∗)]
,
(24)"
REFERENCES,0.9544994944388271,where the second inequality comes from Assumption 1. Let P = f(θ0) −f(θ∗)
REFERENCES,0.9555106167846309,"L
θ1 −θ02"
REFERENCES,0.9565217391304348,"2
. Therefore, if"
REFERENCES,0.9575328614762386,"α ≤(1/ML)
p"
REFERENCES,0.9585439838220424,"2Pξ/L holds, then in the experiment setting where ξ1 = ξ2 = ... = ξD = ξ and
L ≈Lm for all m ∈M, there is:"
REFERENCES,0.9595551061678463,"(bk
m)∗≤b0"
REFERENCES,0.9605662285136501,"v
u
u
t
(Lm)2 θ1 −θ02"
REFERENCES,0.961577350859454,"2
2ξd
α2M 2L[f(θk) −f(θ∗)]
≤b0 s"
REFERENCES,0.9625884732052579,"f(θ0) −f(θ∗)
f(θk) −f(θ∗)
.
(25)"
REFERENCES,0.9635995955510617,Under review as a conference paper at ICLR 2022
REFERENCES,0.9646107178968655,"A.2.4
DEVELOPMENT OF LAZY AGGREGATION CRITERION (8)"
REFERENCES,0.9656218402426694,"Qbkm
 
gk
m

−Qˆbk−1
m"
REFERENCES,0.9666329625884732,"
ˆgk−1
m

2 2"
REFERENCES,0.967644084934277,"=
Qbkm
 
gk
m

−gk
m −Qˆbk−1
m"
REFERENCES,0.9686552072800809,"
ˆgk−1
m

+ ˆgk−1
m
+ gk
m −ˆgk−1
m

2 2"
REFERENCES,0.9696663296258847,"≤2
gk
m −ˆgk−1
m

2"
REFERENCES,0.9706774519716885,"2 + 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2 2"
REFERENCES,0.9716885743174924,"≤2L2
m
θk −θk−d′
2"
REFERENCES,0.9726996966632963,"2 + 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2 2"
REFERENCES,0.9737108190091001,"= 2L2
m  d′
X"
REFERENCES,0.974721941354904,"d=1
θk+1−d −θk−d  2 2"
REFERENCES,0.9757330637007078,"+ 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2 2"
REFERENCES,0.9767441860465116,"≤2L2
md′
d′
X d=1"
REFERENCES,0.9777553083923155,"θk+1−d −θk−d
2"
REFERENCES,0.9787664307381193,"2 + 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2"
REFERENCES,0.9797775530839231,"2 ,
(26)"
REFERENCES,0.980788675429727,where the second inequality comes from:
REFERENCES,0.9817997977755308,"ˆgk−1
m
= ∇fm(θk−d′).
(27)"
REFERENCES,0.9828109201213346,"gk
m = ∇fm(θk).
(28)
∇fm(θk) −∇fm(θk−d′)

2"
REFERENCES,0.9838220424671386,"2 ≤L2
m
θk −θk−d′
2"
REFERENCES,0.9848331648129424,"2 .
(29)"
REFERENCES,0.9858442871587462,"Following LAQ’s definition, redefine dm, m ∈M as:"
REFERENCES,0.9868554095045501,"dm := max
d

d|L2
m ≤ξd/(2α2M 2D), d ∈1, 2, ..., D
	
.
(30)"
REFERENCES,0.9878665318503539,"With the definition of dm and ξ1 ≥ξ2 ≥... ≥ξD, there is:"
REFERENCES,0.9888776541961577,"L2
m ≤
ξd′
2α2M 2D, for all d′ satisfying1 ≤d′ ≤dm.
(31)"
REFERENCES,0.9898887765419616,"With (26) and (31), we have:"
REFERENCES,0.9908998988877654,"Qbkm
 
gk
m

−Qˆbk−1
m"
REFERENCES,0.9919110212335692,"
ˆgk−1
m

2 2"
REFERENCES,0.9929221435793731,"≤
1
α2M 2 d′
X"
REFERENCES,0.993933265925177,"d=1
ξd′
θk+1−d −θk−d
2"
REFERENCES,0.9949443882709808,"2 + 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2 2"
REFERENCES,0.9959555106167847,"≤
1
α2M 2 D
X"
REFERENCES,0.9969666329625885,"d=1
ξd
θk+1−d −θk−d
2"
REFERENCES,0.9979777553083923,"2 + 2
εbkm(gk
m) −εˆbk−1
m (ˆgk−1
m
)

2"
REFERENCES,0.9989888776541962,"2 .
(32)"
