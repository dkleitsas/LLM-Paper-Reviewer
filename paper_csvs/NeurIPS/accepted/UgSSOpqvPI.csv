Section,Section Appearance Order,Paragraph
GEORGIA INSTITUTE OF TECHNOLOGY,0.0,"1Georgia Institute of Technology
2Microsoft Research"
ABSTRACT,0.005076142131979695,Abstract
ABSTRACT,0.01015228426395939,"Construction of a universal detector poses a crucial question: How can we most
effectively train a model on a large mixture of datasets? The answer lies in learning
dataset-specific features and ensembling their knowledge but do all this in a single
model. Previous methods achieve this by having separate detection heads on a
common backbone but that results in a significant increase in parameters. In this
work, we present Mixture-of-Experts as a solution, highlighting that MoEs are
much more than a scalability tool. We propose Dataset-Aware Mixture-of-Experts,
DAMEX where we train the experts to become an ‘expert’ of a dataset by learning
to route each dataset tokens to its mapped expert. Experiments on Universal
Object-Detection Benchmark show that we outperform the existing state-of-the-art
by average +10.2 AP score and improve over our non-MoE baseline by average
+2.0 AP score. We also observe consistent gains while mixing datasets with (1)
limited availability, (2) disparate domains and (3) divergent label sets. Further, we
qualitatively show that DAMEX is robust against expert representation collapse.
Code is available at https://github.com/jinga-lala/DAMEX."
INTRODUCTION,0.015228426395939087,"1
Introduction"
INTRODUCTION,0.02030456852791878,"Human visual perception has naturally evolved to excel at recognizing and localizing objects in a
wide range of real-world scenarios [25, 33]. However, the field of computer vision has traditionally
concentrated on training and evaluating object detection models on specific datasets [42, 3, 43, 7,
20, 28, 34]. These models have shown limited adaptability to new environments [1, 32], leading
to decreased accuracy and practicality in real-world settings [30, 27]. Recognizing the challenges
posed by the single dataset models and the availability of diverse datasets collected over time, unified
models are being developed that merge information from multiple datasets into a single framework
[35, 45, 11, 41, 24, 15]."
INTRODUCTION,0.025380710659898477,"The process of combining datasets presents several challenges that need to be addressed. Firstly,
these datasets have been collected over time [5] and for different purposes, resulting in variations
across domains. For example, one dataset may contain images from indoor environments [9], while
another may be focused on the medical domain [38]. Secondly, imbalances in training images and
tail distributions may exist across the datasets. This means that certain datasets and classes might
have a substantial number of training examples, while others have only a few training examples.
To illustrate, the popular Universal Object-Detection Benchmark (UODB) dataset for robust object
detection demonstrates this diversity: the Clipart [13] data subset comprises 500 training images,
whereas the COCO dataset [22] includes 35,000 training images. Figure 1 shows such diversities in
the real world data, encompassing various aspects such as domains, the number of training images,
camera views, scale and label set, taken from the multiple data sources. Can a universal object
detection method be designed that effectively addresses the above issues associated with mixing
multiple datasets?"
INTRODUCTION,0.030456852791878174,"Kitchen (Indoor)
COCO (Natural)
VOC (Natural)"
INTRODUCTION,0.03553299492385787,"Widerface (Natural)
KITTI (Traffic)
LISA (Traffic)"
INTRODUCTION,0.04060913705583756,"Deeplesion (Medical)
DOTA (Aerial)
Clipart (Style)
Comic (Style)
Watercolor (Style)"
INTRODUCTION,0.04568527918781726,"Figure 1: Samples from Universal Object Detection Benchmark (UODB): UODB set comprises
of a group of 11 datasets which vary in domains, labels, camera view and scale."
INTRODUCTION,0.050761421319796954,"To tackle multi-datasets based object detection problem, several approaches have been proposed. [16]
manually unifies taxonomy of 7 semantic segmentation datasets using Mechanical Turk whereas our
objective is to learn a single universal detector on any mixture of datasets. [45] provides a way to
combine multiple separate detectors by learning a unified label space for multiple datasets. Their
work proposed a new loss term to unify the label space but requires training separate partitioned
detectors heads which are later unified with a drop in performance. [35] trains a universal detector
on 11 datasets of Universal Object Detection Benchmark (UODB) and gain robustness by joining
different sources of supervision. However, their method requires knowledge of the input dataset
domain during test time whereas our objective universal detector should be agnostic to the input data
domain making the problem more challenging."
INTRODUCTION,0.05583756345177665,"In this work, we propose a Mixture-of-Experts (MoE) [14] based object detection method that can
handle all the desired properties of a universal detector. Traditionally, MoEs have been used as a
scaling tool in Natural Language Processing [6, 18, 31, 21, 4, 2] and image classification problem
[12, 29]. However, we propose that Mixture-of-Experts are more than just scalable learners, instead
MoEs are an effective and efficient solution to build a universal model for mixture of datasets.
Building over vanilla MoE, we introduce a novel Dataset-aware Mixture-of-Experts model, DAMEX
which learns to disentangle dataset-specific features at the MoE layer while pooling information at
non-MoE layers. DAMEX learns to route tokens to their corresponding expert so that during inference
it automatically chooses the best pathway for the test image through the network. We use DINO [42]
based detection architecture for developing our approach."
INTRODUCTION,0.06091370558375635,"We perform our evaluation on UODB dataset which comprises of 11 diverse datasets as shown in
Figure 1. We compare against previously reported state-of-the-art on UODB [35] and recent multi-
dataset object-detection method [45]. Moreover, we also compare our method against individual
dataset trained DINO detectors along with complete UODB trained DINO model. We report an
average +10.2 AP score improvement over previous baseline and a +2.0 AP gain over non-MoE
based method. Additionally, we mix dataset pairs that highlight that MoE based detectors are better"
INTRODUCTION,0.06598984771573604,"in handling, (1) Mixing with limited data availability, (2) Domain Adaptation, (3) Mixing divergent
label sets within same domain, and show consistent gains on DAMEX. We qualitatively show that
DAMEX results in better expert utilization and avoid representation collapse which is a common
problem with MoE training [2]."
INTRODUCTION,0.07106598984771574,Our contributions can be summarized as:
INTRODUCTION,0.07614213197969544,"• We introduce a novel Dataset-aware Mixture-of-Experts layer, DAMEX which disentangles
dataset-specific features within the MoE layer, effectively tackling mixing of heterogeneous
datasets. We experimentally show that DAMEX facilitates superior expert utilization and
avoid the common issue of representation collapse in vanilla MoE."
INTRODUCTION,0.08121827411167512,"• Compared to the baselines, DAMEX does not require test-time dataset labels as it learns to
route appropriate input dataset to its corresponding expert during training itself, which is a
more challenging setup."
INTRODUCTION,0.08629441624365482,"• To the best of our knowledge, we are the first work to explore the potential of MoE as being
more than a scalability tool but an effective learner for a mixture of datasets by serving as a
knowledge-ensembling strategy within the model architecture with a marginal increase in
number of parameters wrt. dense architecture."
INTRODUCTION,0.09137055837563451,"• We establish a new state-of-the-art on UODB set by beating previously reported baseline by
an average of +10.2 AP score and outperforming non-MoE baseline by +2.0 AP score on
average. Further, we observe consistent improvements in various dataset mixing scenarios,
like (1) Limited data availability, (2) Disparate domains, and (3) Divergent label sets."
RELATED WORK,0.09644670050761421,"2
Related Work"
MIXING MULTIPLE DATASETS IN OBJECT-DETECTION,0.10152284263959391,"2.1
Mixing Multiple Datasets in Object-detection"
MIXING MULTIPLE DATASETS IN OBJECT-DETECTION,0.1065989847715736,"Training on multiple datasets has emerged as a robust way to improve task performance of a deep
learning model as shown in stereo matching [39], depth estimation [17, 10] and detection performance
[35, 16, 45]. Previously, [16] manually unifies taxonomy of 7 semantic segmentation datasets using
Mechanical Turk whereas [44] manually merge the label mapping of datasets at the detection head.
However, our objective is to build a universal object-detection model which should be able to adapt
to any mixture of detection datasets. [37] trains a partitioned detector on three datasets and combine
their information through an inter-dataset graph-based attention module. [45] goes a step forward and
provides a way to combine multiple separate detectors by learning a unified label space for multiple
datasets. Their work proposed a new loss term to unify the label space but requires training separate
partitioned detectors heads which are later unified with a drop in performance. Our approach, on the
other hand does not rely on unification of labels and does not need specialized partitioned detectors
as it utilizes the advantages of a transformer based end-to-end detection pipeline and simply increases
the number of classes in the last layer, resulting in marginal increase in parameters. Similar to [37],
[35] trains a universal detector on 11 datasets of Universal Object Detection Benchmark (UODB)
by having separate detection heads for each dataset with a domain attention module conditioned on
datasets for supervision of backbone during training. However, like [37] their method also requires
knowledge of the input dataset domain during test time whereas our objective universal detector is
agnostic to the input data domain making the detection more challenging."
MIXTURE-OF-EXPERTS,0.1116751269035533,"2.2
Mixture-of-Experts"
MIXTURE-OF-EXPERTS,0.116751269035533,"Mixture-of-experts (MoE) [14] is a machine learning concept that employs multiple expert layers,
each of which specializes in solving a specific subtask. The experts then work together to solve
the entire task at hand. Recently, MoE has been widely applied to large-scale distributed Deep
Learning models by using a cross-GPU layer that exchanges hidden features from different GPUs
[12, 6, 29, 21, 4, 18, 31]. The MoE approach is differentiated from existing scale-up approaches for
DNNs, such as increasing the depth or width of DNNs, in terms of its high cost-efficiency. Specifically,
adding more model parameters (experts) in MoE layers does not increase the computational cost
per token. Thus, MoE has been studied for scaling for the models of trillion-size parameters in NLP
[6, 21, 4, 18]."
MIXTURE-OF-EXPERTS,0.1218274111675127,"In machine vision, [12, 29] primarily showed benefits of MoE on ImageNet classification task. In
this work, we further pursue this direction by modelling MoE on object detection task and highlight
the effectiveness, and robustness of MoEs in learning from a mixture of datasets. Moreover, we
improve over expert utilization problem of MoEs [2] in a mixture of dataset setting by introducing a
novel Dataset-Aware Mixture-of-Experts, DAMEX layer that learns to make each expert an ‘expert’
of its corresponding dataset while pooling shared information between the datasets in the rest of the
network."
MIXTURE-OF-EXPERTS,0.12690355329949238,"3
Preliminaries: Mixture-of-Experts (MoE)"
MIXTURE-OF-EXPERTS,0.1319796954314721,"An MoE layer consists of two parts: (1) a set of experts distributed across different GPUs, and (2) a
gating function (router). The distribution across GPUs accelerate the inference speed of MoE while
keeping the parameter count same. The gating function or router determines the destination GPU
of each input token, then the tokens are dispatched to their target expert. These processed token
representations are then combined such that the tokens are returned to their original location."
ROUTING OF TOKENS,0.13705583756345177,"3.1
Routing of tokens"
ROUTING OF TOKENS,0.14213197969543148,"Let us denote the input tokens with x ∈RD, a set E of experts by {ei}|E|
i=1 and router variable with
Wr ∈RE×D. The probability of each expert pi will then become:"
ROUTING OF TOKENS,0.14720812182741116,"gx = Wr · x
(1)"
ROUTING OF TOKENS,0.15228426395939088,"pi(x) =
exp (gxi)
P|E|
j=1 exp (gxj)
.
(2)"
ROUTING OF TOKENS,0.15736040609137056,"After calculating the probability of each expert pi(x), [31] used top-k experts for routing the token.
[31] calculates the output y as a weighted combination of the processed tokens with their selection
probability as: y =
X"
ROUTING OF TOKENS,0.16243654822335024,"i∈top-k
pi(x)ei(x)
(3)"
ROUTING OF TOKENS,0.16751269035532995,"We use k = 1 as it results in a marginal degradation in performance but a huge gain in throughput [6]
and [12]. The output y is then passed forward to the remaining layers of the neural net."
LOAD BALANCING AMONG THE EXPERTS,0.17258883248730963,"3.2
Load balancing among the experts"
LOAD BALANCING AMONG THE EXPERTS,0.17766497461928935,"Sparsity in the activation of experts is desirable for the MoE layer with respect to a given token. A
simplistic approach would be to select the top-k experts based on the softmax probability distribution.
In practice however, this approach causes majority of the tokens to be assigned to a small number of
busy experts causing a load imbalance during training. This leads to a significant input buffer for a
few busy experts, causing a delay in the training process, while other experts remain untrained [6].
Additionally, many experts may not receive adequate training. Therefore, a better gating function
design is needed to distribute the processing burden equally across all experts."
LOAD BALANCING AMONG THE EXPERTS,0.18274111675126903,"To ensure better distribution of tokens, an auxiliary loss on experts is added along with the main
task loss [18, 12]. The load balancing auxiliary loss Lload-balancing can be written as an average of
Limportance and Lload."
LOAD BALANCING AMONG THE EXPERTS,0.18781725888324874,The importance loss Limportance for expert ei is calculated by
LOAD BALANCING AMONG THE EXPERTS,0.19289340101522842,"Ii =
X"
LOAD BALANCING AMONG THE EXPERTS,0.19796954314720813,"x∈Im
pi(x)
(4)"
LOAD BALANCING AMONG THE EXPERTS,0.20304568527918782,"Limportance =
Var(I)
Mean(I)2
(5)"
LOAD BALANCING AMONG THE EXPERTS,0.20812182741116753,"Let us denote the normal distribution as N(0, σ2I), where σ = gate noise"
LOAD BALANCING AMONG THE EXPERTS,0.2131979695431472,"|E|
. The CDF of this normal
distribution is denoted as Φ(.). For load Li for expert ei, we sample the CDF at probabilities pi(x)"
LOAD BALANCING AMONG THE EXPERTS,0.2182741116751269,Multi-Head
LOAD BALANCING AMONG THE EXPERTS,0.2233502538071066,Attention MoE
LOAD BALANCING AMONG THE EXPERTS,0.22842639593908629,Add & Norm
LOAD BALANCING AMONG THE EXPERTS,0.233502538071066,Add & Norm
LOAD BALANCING AMONG THE EXPERTS,0.23857868020304568,Output Tokens
LOAD BALANCING AMONG THE EXPERTS,0.2436548223350254,Input Tokens
LOAD BALANCING AMONG THE EXPERTS,0.24873096446700507,MoE Transformer module
LOAD BALANCING AMONG THE EXPERTS,0.25380710659898476,Input images in batch
LOAD BALANCING AMONG THE EXPERTS,0.25888324873096447,"COCO
Watercolor"
LOAD BALANCING AMONG THE EXPERTS,0.2639593908629442,"Expert-N
Expert-1"
LOAD BALANCING AMONG THE EXPERTS,0.26903553299492383,Router ...
LOAD BALANCING AMONG THE EXPERTS,0.27411167512690354,Expert-2
LOAD BALANCING AMONG THE EXPERTS,0.27918781725888325,Dataset-aware expert selection
LOAD BALANCING AMONG THE EXPERTS,0.28426395939086296,DAMEX: Dataset-aware MoE
LOAD BALANCING AMONG THE EXPERTS,0.2893401015228426,Expert-3
LOAD BALANCING AMONG THE EXPERTS,0.29441624365482233,COCO tokens routed
LOAD BALANCING AMONG THE EXPERTS,0.29949238578680204,to expert-1
LOAD BALANCING AMONG THE EXPERTS,0.30456852791878175,Watercolor tokens
LOAD BALANCING AMONG THE EXPERTS,0.3096446700507614,routed to expert-N
LOAD BALANCING AMONG THE EXPERTS,0.3147208121827411,"Expert-N
Expert-1 ..."
LOAD BALANCING AMONG THE EXPERTS,0.3197969543147208,Expert-2
LOAD BALANCING AMONG THE EXPERTS,0.3248730964467005,Load balancing among experts
LOAD BALANCING AMONG THE EXPERTS,0.3299492385786802,Vanilla Mixture-of-Experts
LOAD BALANCING AMONG THE EXPERTS,0.3350253807106599,Expert-3
LOAD BALANCING AMONG THE EXPERTS,0.3401015228426396,Router
LOAD BALANCING AMONG THE EXPERTS,0.34517766497461927,"Figure 2: Overview of DAMEX: The dense FFN of a transformer block is replaced by an MoE layer.
We show the difference between functioning of vanilla MoE layer vs DAMEX. Given two images
from COCO and Watercolor dataset, the vanilla MoE router tends to balance the distribution of input
tokens to each expert while DAMEX learns to route dataset tokens to their corresponding expert (Here
expert-1 is assigned to COCO dataset and expert-N to Watercolor dataset). During inference, the
trained router choose the appropriate expert depending on the input token without any information of
dataset source."
LOAD BALANCING AMONG THE EXPERTS,0.350253807106599,"as Φ(pi(x)) and sum it over all the tokens x in the image Im. The load Li denotes the number of
assignments among the experts. The loss Lload becomes"
LOAD BALANCING AMONG THE EXPERTS,0.3553299492385787,"Li =
X"
LOAD BALANCING AMONG THE EXPERTS,0.3604060913705584,"x∈Im
Φ(pi(x))
(6)"
LOAD BALANCING AMONG THE EXPERTS,0.36548223350253806,"Lload =
Var(L)
Mean(L)2 .
(7)"
LOAD BALANCING AMONG THE EXPERTS,0.37055837563451777,"Finally, the load balancing auxiliary loss Lload-balancing is calculated by"
LOAD BALANCING AMONG THE EXPERTS,0.3756345177664975,Lload-balancing = Limportance + Lload
LOAD BALANCING AMONG THE EXPERTS,0.38071065989847713,"2
.
(8)"
METHOD,0.38578680203045684,"4
Method"
METHOD,0.39086294416243655,"Our method Dataset-aware Mixture-of-Experts, DAMEX is based on Mixture of Experts (MoE)
applied over transformer-based object detection pipeline DINO (DETR with Improved denoising
anchor boxes) [42] such that each expert is trained to align to a corresponding dataset, subtly making
it an actual ‘expert’ of that dataset-specific features. In this section, we will explain our method in
detail."
SETUP,0.39593908629441626,"4.1
Setup"
SETUP,0.4010152284263959,"We replace alternate non-MoE transformer modules from decoder of DINO architecture with MoE
transformer modules. A common problem that occurs with mixing of datasets in object-detection is to
have separate detection heads for each dataset which increases the number of parameters significantly
in a two-stage detection pipeline [35, 45]. We leverage the transformer architecture of DINO in
handling the number of classes by increasing the dimension of the last classification layer which
results in marginal increase in number of parameters (≈10, 000) compared to total model parameters,
46M. In all experiments, we use one expert per GPU which keeps our parameter count same as that
of non-MoE method (with an addition of a tiny router linear layer)."
SETUP,0.40609137055837563,"In vanilla MoE unlike image classification problem where every visual token is used for loss calcula-
tion [12, 2, 29], we apply the load-balancing loss Equation 8 only on the foreground tokens in the
detection task. We find that the foreground-only loss balancing results in better gradient updates and
mitigate expert representation collapse on detection task."
SETUP,0.41116751269035534,"4.2
DAMEX: Dataset-aware Mixture-of-Experts"
SETUP,0.41624365482233505,"Vanilla MoE balances the experts usage across GPUs to utilize maximum benefit from ensembling
features from input tokens. However, it can lead to inefficiency in knowledge sharing among experts
due to distribution shift. For e.g. , the expert domain knowledge required for a natural images dataset
like COCO is very different from a surveillance-style dataset like DOTA and we would want to
learn their dataset-specific characteristics. Thus, to solve this inefficiency with using MoE on a
mixture-of-datasets we introduce Dataset-aware Mixture-of-Experts (DAMEX) where we train the
MoE router to route tokens depending on their dataset origin."
SETUP,0.4213197969543147,"Given a set D of datasets {dm}|D|
m=1, such that input token x ∈dm. We define a mapping function
h : {D} →{E} such that each dataset dm is assigned a specific expert ei, then auxiliary loss LDAMEX
is calculated by a cross-entropy loss between logits pi (probability of selection of expert ei) and
labels h(dm) (target expert for token x from dataset dm)."
SETUP,0.4263959390862944,"LDAMEX = − |E|
X"
SETUP,0.43147208121827413,"i=1
1(h(dm) = i) log pi(x)
(9)"
SETUP,0.4365482233502538,"DAMEX trains the MoE router to dispatch all the visual tokens of the dataset to its corresponding
expert as shown in Figure 2. Selecting specific expert ensure efficient usage of MoE and in-turn avoid
their representation collapse."
EXPERIMENTS,0.4416243654822335,"5
Experiments"
EXPERIMENTS,0.4467005076142132,"Datasets.
We evaluate the presented approach on Universal Object Detection Benchmark (UODB)
[35]. UODB comprises of 11 datasets: Pascal VOC [5], WiderFace [40], KITTI [8], LISA [26],
DOTA [36], COCO [22], Watercolor, Clipart, Comic [13], Kitchen [9] and DeepLesions [38], shown
in Figure 1. Pascal VOC and COCO have natural everyday object images while Kitchen features
indoor kitchen items. WiderFace is a huge human-face web images dataset covering a large variety
of regions across the globe. Both KITTI and LISA datasets are based on traffic scenes captured with
cameras mounted on moving vehicles. KITTI includes categories such as vehicles, pedestrians, and
cyclists, whereas LISA is primarily composed of traffic signs. DOTA is a surveillance dataset with
aerial images annotating planes, ships, harbors etc.. Watercolor, Clipart and Comic are cross-domain
datasets in their separate styles of images. Comic have the same label set as Pascal VOC, while
Watercolor and Clipart are a subset of that label set. DeepLesions is a medical CT images dataset
highlighting lesions. Altogether, the UODB offers a wide array of domains, label sets, camera views
and image styles, providing a comprehensive suite for evaluating multi-dataset object detection
models."
EXPERIMENTS,0.4517766497461929,"Implementation details.
Following recent end-to-end object-detection pipelines [23, 19] we
adapted MoE on current state-of-the-art, DINO [42]. We use pre-trained ImageNet ResNet-50
backbone with 4 scale features as input across all methods. For hyper-parameters, as in DINO, we
use a 6-layer Transformer encoder and a 6-layer Transformer decoder and 256 as the hidden feature
dimension. We altered DINO’s decoder with alternate MoE layers following [12] using TUTEL
library [12]. We use a capacity factor f of 1.25 and an auxiliary expert-balancing loss weight of
0.1 with top-1 selection of experts. We kept one expert per GPU and train on 8 RTX6000 GPUs
with a batch-size of 2 per GPU, unless mentioned otherwise. Note that the number of parameters
of model remains the same (except the addition of marginal router parameters) as each expert re-
side on a separate GPU replacing the existing feed-forward layer. For DAMEX, we keep the 1:1
dataset-mapping by assigning single expert to each dataset, except for full UODB run where we
assign a single expert to COCO, VOC and Clipart dataset while KITTI and LISA shared a expert
to fit 8 experts (on 8-GPU machine) on 11 datasets. We use a learning-rate of 1.4e-4 and kept other
DINO-specific hyperparameters same as [42]."
EXPERIMENTS,0.45685279187817257,"Table 1: Multi-dataset object detection results on UODB benchmark: We report mean AP score
on UODB set. Mixing refers to whether the model is trained with all the datasets mixed together or
individually. The top two rows denote single domain models which are separately trained on each set.
Observe that MoE hardly increases the number of parameters but show a jump in performance."
EXPERIMENTS,0.4619289340101523,"Method
Mixing
Params
KITTI
VOC
WiderFace
LISA
Kitchen
COCO
DOTA
DeepLesion
Comic
Clipart
Watercolor
Mean"
EXPERIMENTS,0.467005076142132,"DINO [42]
-
46.67M x 11
68.8
57.6
35.2
78.8
46.4
43.1
40.7
39.6
11.1
10.1
14.8
40.6
Single dataset DINO-MoE
-
46.68M x 11
68.2
58.0
35.3
79.2
47.0
43.4
41.0
40.6
8.4
10.0
14.8
40.5"
EXPERIMENTS,0.4720812182741117,"Wang et al. [35]
✓
44.75M
21.5
51.4
23.0
59.2
49.8
28.5
30.1
26.0
25.7
29.7
30.7
34.1
Zhou et al.[45]
✓
70.18M
47.5
30.7
26.3
60.7
42.6
16.5
28.8
22.4
20.4
24.4
21.2
31.1
DINO
✓
46.73M
58.4
53.9
34.7
73.0
48.4
39.9
44.7
42.9
21.2
27.8
20.1
42.3
DINO-MoE
✓
46.74M
57.9
56.4
35.0
74.0
49.8
40.2
45.2
43.0
24.1
27.3
19.9
43.0
DAMEX(Ours)
✓
46.74M
61.8
56.0
35.1
74.9
49.4
41.3
46.5
43.3
25.4
29.7
23.5
44.3"
EXPERIMENTS,0.47715736040609136,"Baselines and metrics.
We compare our approach against [35] that is the previous state-of-the-art
on UODB benchmark. [35] gains a further advantage as during inference, the object detector knows
from which dataset each test image comes from and thus makes predictions in the corresponding label
space. On the other hand, our setup is more challenging as during inference the method is agnostic
to the source dataset. We also compare our method against Partitioned detector of [45] as their
work is complimentary to our approach and establish a universal detector with a common backbone.
ResNet-50 is the common backbone across all the methods in this paper, including baselines. For fair
comparison on the architectural gains, we prepare competitive baselines by running DINO on each
dataset individually as well as mixing all the datasets together. All the reported numbers in this work
are mean Average Precision (AP) scores evaluated on the available test or val set of corresponding
dataset."
MULTI-DATASET OBJECT DETECTION ON UODB,0.48223350253807107,"5.1
Multi-dataset Object Detection on UODB"
MULTI-DATASET OBJECT DETECTION ON UODB,0.4873096446700508,"In this section, we highlight our main results where we compare our DINO baseline against MoE
DINO setup along with our proposed approach DAMEX on complete UODB datasets. Moreover, we
re-evaluate the AP scores of [35] by running their best released model against COCO AP evaluation.
Table 1 shows the AP scores on individual dataset and average performance on UODB. We observe
that the proposed method improve accuracy by an average of +10.2 AP score with previous state-of-
the-art and shows an average improvement of +2.0 AP on Mixed DINO baseline. Individually, we
observe an increase in performance with DAMEX on out-of-domain datasets, like DOTA (surveillance
aerial images) and DeepLesion (medical CT images) due to experts learning dataset specific features.
While, KITTI suffer from a performance drop due to the reduced representation in the batch. Overall,
dataset-aware MoE, DAMEX outperforms vanilla-MoE by +1.3 AP score highlighting the benefit of
our proposed dataset-specific experts in multi-dataset training."
ANALYSIS,0.49238578680203043,"5.2
Analysis"
ANALYSIS,0.49746192893401014,"We analyze the benefit of DAMEX in mixing datasets by quantitatively and qualitatively evaluating it
for the desired properties associated with a universal object detector such as, limited data mixing,
domain adaptation and same domain but different annotation set mixing. Further, we visualize the
deeper routing decisions made by DAMEX and vanilla MoE across mixed dataset classes to highlight
the efficiency and effectiveness of DAMEX."
ANALYSIS,0.5025380710659898,"Limited data mixing.
In this setup, we demonstrate that the MoE based object detection model is
better in handling imbalanced data. For this, we take KITTI and Kitchen dataset from the UODB set
and create several few-shot scenarios by taking all the original images of Kitchen dataset and pair it
against 50-shot, 100-shot, 1000-shot and complete KITTI dataset. The results have been presented in
the Table 2. We demonstrate that the DAMEX is better in handling imbalanced even as we decrease
the number of training images with a relative percentage gain of 15.7% in 50-shot and 24.3% in
100-shot setting over non-MoE baseline. We observe that the gap between DAMEX and vanilla MoE
decreases with increase in data points. We believe that vanilla MoE is able to find a similar optimal
distribution at the class-level with large amount of data."
ANALYSIS,0.5076142131979695,"Domain adaptation.
Another common scenario of mixing dataset is when there are two datasets
with the same annotation set but different domains. To replicate this setting, we conduct our analysis
on a mixture of Watercolor and Comic dataset. Both Watercolor and Comic have the same label set"
ANALYSIS,0.5126903553299492,"Table 2: Limited Data Mixing: We mix Kitchen dataset with n-shot examples of KITTI dataset. On
right, we can observe the relative percentage gain of MoE and DAMEX over non-MoE method. In
low-resource setting, DAMEX outperforms vanilla MoE due to efficient expert distribution whereas
the gap diminishes as MoE found a similar distribution at the class-level with large amount of data."
ANALYSIS,0.5177664974619289,"# of examples
50-shot
100-shot
1000-shot
Full"
ANALYSIS,0.5228426395939086,"Method
KITTI
Kitchen
KITTI
Kitchen
KITTI
Kitchen
KITTI
Kitchen"
ANALYSIS,0.5279187817258884,"Mixed DINO
14.6
46.3
18.9
48.1
40.0
48.5
63.0
48.1
Mixed DINO-MoE
16.4
47.6
20.9
46.3
45.2
47.0
68.7
48.3
DAMEX(Ours)
16.9
47.7
23.5
46.0
46.1
47.5
68.5
48.3"
ANALYSIS,0.5329949238578681,"50
100
1000
full dataset
# Shots (Log Scale) 0 5 10 15 20 25 30"
ANALYSIS,0.5380710659898477,Percentage Gain
ANALYSIS,0.5431472081218274,"vanilla MoE
DAMEX"
ANALYSIS,0.5482233502538071,"but have very different styles of images. Watercolor has unique characteristics of watercolor art, such
as transparent, fluid color washes, and visible brush strokes while Comic have bold, outlined objects
and figures, with high-contrast color fills. The performance comparison of DAMEX, vanilla MoE, and
non-MoE baseline is demonstrated in Table 3. We note a significant +2.9 AP increase on Watercolor
and Comic datasets, underscoring that specialized experts can effectively learn dataset-specific traits,
while shared network components can aggregate shared knowledge of the datasets."
ANALYSIS,0.5532994923857868,"Table 3: Domain adaptation: Watercolor
and Comic both have same label sets but
different style image domains. By learning
dataset-specific features, DAMEX efficiently
adapts to both dataset compared to vanilla
MoE and non-MoE baseline."
ANALYSIS,0.5583756345177665,"Method
Watercolor
Comic"
ANALYSIS,0.5634517766497462,"Mixed DINO
15.3
10.7
Mixed DINO-MoE
16.6
12.9
DAMEX(Ours)
18.2
13.6"
ANALYSIS,0.5685279187817259,"Table 4: Addressing Divergent Label Sets
in Mixed Datasets: KITTI and LISA, though
both consist of traffic imagery, possess en-
tirely disparate annotation sets, leading to po-
tentially conflicting gradient updates. Never-
theless, DAMEX effectively circumvents this
issue by learning dataset-specific features."
ANALYSIS,0.5736040609137056,"Method
KITTI
LISA"
ANALYSIS,0.5786802030456852,"Mixed DINO
67.7
76.6
Mixed DINO-MoE
69.2
77.9
DAMEX(Ours)
69.4
78.5"
ANALYSIS,0.583756345177665,"Table 5: Effect of dataset-expert mapping in DAMEX: We compare the effect of dataset-expert
mapping on UODB set. We observe that random assignment is still better than vanilla MoE which is
trained on load-balancing loss while DAMEX experts learns dataset-specific features. Further, adding
a human prior (or domain knowledge) in mapping can lead to further performance improvements in
DAMEX."
ANALYSIS,0.5888324873096447,"Method
KITTI
VOC
WiderFace
LISA
Kitchen
COCO
DOTA
DeepLesion
Comic
Clipart
Watercolor
Mean"
ANALYSIS,0.5939086294416244,"Mixed DINO-MoE
57.9
56.4
35.0
74.0
49.8
40.2
45.2
43.0
24.1
27.3
19.9
43.0
DAMEX: Random mapping
60.8
56.2
35.2
73.9
48.8
41.8
46.0
43.6
24.3
28.5
21.6
43.7
DAMEX: Human-prior mapping (ours)
61.8
56.0
35.1
74.9
49.4
41.3
46.5
43.3
25.4
29.7
23.5
44.3"
ANALYSIS,0.5989847715736041,"Mixing divergent label sets but within same domain.
Having two datasets within the same
domain but having different annotation set is a common problem with mixing datasets. A universal
detector should be robust against such missing labels scenario. We conduct our analysis on the
mixture of KITTI and LISA datasets. Both are traffic images dataset, however while KITTI label
set contains pedestrians, cars and cyclists, LISA label set contains only traffic signs such as speed
limit, stop, no turn and warning sign. Note that these classes are present in both the datasets which
make this a more challenging and realistic scenario for a universal detector. Table 4 depicts the
performance of DAMEX against vanilla MoE and non-MoE baseline. We observe that dataset-aware
MoE outperforms the baseline with +1.7 AP score on KITTI and +2.9 AP score on LISA dataset,
highlighting that universal detector can benefit from dataset-specific experts."
ANALYSIS,0.6040609137055838,"Deeper routing decisions.
We analyze the expert utilization of MoE layer in vanilla MoE against
DAMEX. Figure 3 illustrates the distribution of expert selection weights over mixture of datasets. The
plots were produced by running Mixed DINO-MoE and DAMEX from Table 1 on UODB set."
ANALYSIS,0.6091370558375635,"The datasets belonging to same domain are mapped to same expert. Figure 3 show that experts are
equally distributed across datasets with shallow MoE layer suffering from poor expert utilization.
Vanilla MoE fail to specialize in discriminating among datasets. DAMEX on the other hand efficiently
use experts across datasets and conclusively learn to select appropriate dataset-specific expert during
inference. The router selection improves in deeper MoE layers due to dataset-specific features learnt
by earlier MoE layers. Moreover, DAMEX ensures fair expert utilization across all MoE layers."
ANALYSIS,0.6142131979695431,"Effect of human-prior in dataset-expert mapping.
A central part of DAMEX is the dataset-expert
mapping h : {D} →{E} that the router learns during training. However, a question remains on
how this mapping assignment affect the visual understanding of the model overall? We conduct an
experiment where we randomly assign UODB datasets to experts and found a decrease in performance,
as shown in Table 5. DAMEX allows the user to incorporate human-prior or domain knowledge in
mapping datasets to experts. This in-turn helps in sharing similar features from larger datasets, e.g.
COCO and VOC belong to the same domain, natural images. In our experience, assigning datasets
with similar domains to same expert tend to help performance while keeping disparate domains to
separate experts."
ANALYSIS,0.6192893401015228,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.6243654822335025,UODB Datasets
ANALYSIS,0.6294416243654822,MoE Layer 1
ANALYSIS,0.6345177664974619,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.6395939086294417,MoE Layer 2
ANALYSIS,0.6446700507614214,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.649746192893401,MoE Layer 3
ANALYSIS,0.6548223350253807,"0.240
0.245
0.250
0.255
0.260"
ANALYSIS,0.6598984771573604,Vanilla MoE Expert Utilization
ANALYSIS,0.6649746192893401,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.6700507614213198,UODB Datasets
ANALYSIS,0.6751269035532995,MoE Layer 1
ANALYSIS,0.6802030456852792,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.6852791878172588,MoE Layer 2
ANALYSIS,0.6903553299492385,"0
1
2
3
4
5
6
7
Expert ID D0 D1 D2 D3 D4 D5 D6 D7"
ANALYSIS,0.6954314720812182,MoE Layer 3
ANALYSIS,0.700507614213198,"0.2
0.4
0.6
0.8"
ANALYSIS,0.7055837563451777,DAMEX Expert Utilization
ANALYSIS,0.7106598984771574,"Figure 3: Expert Utilization in DAMEX across datasets: We show distribution of expert selection
weights across dataset assignments. Here, Dn on y-axis refer to datasets mapped to expert ID n while
x-axis correspond to 8 experts. The (expert e, dataset d) pair denote the average routing weights of
the tokens corresponding to bounding box of examples in dataset d. We can observe that DAMEX
learns to route tokens to their mapped experts and is able to replicate it during inference without the
knowledge of dataset-source. On the other hand, vanilla MoE struggles to efficiently utilize experts
and suffer with mode collapse at Layer 1."
ANALYSIS,0.7157360406091371,"Table 6: Effect of number of experts
on DAMEX"
ANALYSIS,0.7208121827411168,"# Experts
Clipart
DOTA
KITTI
VOC
Mean"
ANALYSIS,0.7258883248730964,"2
33.3
44.6
58.9
57.4
48.6
4
34.0
45.3
62.0
57.7
49.7
8
32.6
44.9
60.9
56.5
48.7"
ANALYSIS,0.7309644670050761,"Effect of Number of Experts on DAMEX.
We study the
effect of number of experts on DAMEX by experimenting
on a mixture of four disparate domain datasets, namely
KITTI, DOTA, VOC and Clipart to maximize the effect of
dataset-aware experts. Table 6 shows the variation of mean
AP score across the four datasets. We can observe that the
best performance is obtained when number of experts are
same as number of datasets. This observation is different
from vanilla MoEs where performance increases with increase in number of experts [12, 29]. It
highlights that DAMEX is an efficient method for mixing datasets both in terms of compute and
number of parameters as less number of experts require lesser compute. Note that for 8 experts, we
are mapping two experts per dataset with equal probability during training while expert selection
during inference is done by the trained router."
CONCLUSION,0.7360406091370558,"6
Conclusion"
CONCLUSION,0.7411167512690355,"In this work, we have presented a novel Dataset-aware Mixture-of-Experts (DAMEX) layer for the
object-detection task. We demonstrate the potential of MoE models for mixing datasets on object-
detection task. The introduced DAMEX model leverages the MoE architecture to learn dataset-specific
features, thereby enhancing expert utilization and significantly improving the performance on a
universal object-detection benchmark with marginal increase in parameters. Our experimental results
establish a new state-of-the-art performance on UODB by achieving an average increase of +10.2 AP
score over the previous baseline and +2.0 AP score over non-MoE method."
LIMITATIONS & SOCIAL IMPACT,0.7461928934010152,"7
Limitations & Social Impact"
LIMITATIONS & SOCIAL IMPACT,0.751269035532995,"One limitation of our approach is that we are currently concatenating the classes of each dataset
which results in a duplicate recognition of overlapping classes. A future work in unifying these labels
over our setup can be an interesting avenue."
LIMITATIONS & SOCIAL IMPACT,0.7563451776649747,"Going forward it is important that we ensure that the universal object detectors are unbiased and
maintain fairness in their predictions. Here unbiased refers to classes with very few training examples
or domain shift which can be called underrepresented classes. The detectors should be able to work
on both classes with large, medium and few training samples in any environment. Similarly, for
fairness, a universal object detector should be able to handle objects from different geographical
regions, races, genders, etc. These are issues that we believe are ameliorated by methods focused on
learning under imbalance or domain shift."
LIMITATIONS & SOCIAL IMPACT,0.7614213197969543,"As a result, we feel DAMEX could be a method that takes a step towards this where each dataset,
even with very few samples can be represented by different experts. We can potentially divide a
large dataset into different sets which cater to at least one aspect of fairness and bias. However,
we understand the need for further analysis and research before we have a truly unbiased and fair
universal object detector and implore the community to do so too."
REFERENCES,0.766497461928934,References
REFERENCES,0.7715736040609137,"[1] Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to
small image transformations? arXiv preprint arXiv:1805.12177, 2018."
REFERENCES,0.7766497461928934,"[2] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, Xian-Ling Mao, et al. On the representation collapse of sparse mixture
of experts. Advances in Neural Information Processing Systems, 35:34600–34613, 2022."
REFERENCES,0.7817258883248731,"[3] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei
Zhang. Dynamic head: Unifying object detection heads with attentions. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 7373–7382, 2021."
REFERENCES,0.7868020304568528,"[4] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P
Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,
Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng
Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,
editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pages 5547–5569. PMLR, 17–23 Jul 2022."
REFERENCES,0.7918781725888325,"[5] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and
Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International
journal of computer vision, 111:98–136, 2015."
REFERENCES,0.7969543147208121,"[6] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity. The Journal of Machine Learning Research,
23(1):5232–5270, 2022."
REFERENCES,0.8020304568527918,"[7] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in
2021. arXiv preprint arXiv:2107.08430, 2021."
REFERENCES,0.8071065989847716,"[8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving?
the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern
recognition, pages 3354–3361. IEEE, 2012."
REFERENCES,0.8121827411167513,"[9] Georgios Georgakis, Md Alimoor Reza, Arsalan Mousavian, Phi-Hung Le, and Jana Košecká.
Multiview rgb-d dataset for object instance detection. In 2016 Fourth International Conference
on 3D Vision (3DV), pages 426–434. IEEE, 2016."
REFERENCES,0.817258883248731,"[10] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan
Misra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 16102–16112, 2022."
REFERENCES,0.8223350253807107,"[11] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general pur-
pose vision systems: An end-to-end task-agnostic vision-language architecture. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16399–16409,
2022."
REFERENCES,0.8274111675126904,"[12] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael
Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang
Xiong. Tutel: Adaptive mixture-of-experts at scale. CoRR, abs/2206.03382, June 2022."
REFERENCES,0.8324873096446701,"[13] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain
weakly-supervised object detection through progressive domain adaptation. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 5001–5009, 2018."
REFERENCES,0.8375634517766497,"[14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures
of local experts. Neural computation, 3(1):79–87, 1991."
REFERENCES,0.8426395939086294,"[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
preprint arXiv:2304.02643, 2023."
REFERENCES,0.8477157360406091,"[16] John Lambert, Zhuang Liu, Ozan Sener, James Hays, and Vladlen Koltun. Mseg: A composite
dataset for multi-domain semantic segmentation. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 2879–2888, 2020."
REFERENCES,0.8527918781725888,"[17] Katrin Lasinger, René Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monoc-
ular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. arXiv preprint
arXiv:1907.01341, 2019."
REFERENCES,0.8578680203045685,"[18] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-
tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020."
REFERENCES,0.8629441624365483,"[19] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate
detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 13619–13627, 2022."
REFERENCES,0.868020304568528,"[20] X Li, W Wang, X Hu, J Li, J Tang, and J Yang. Generalized focal loss v2: Learning reliable
localization quality estimation for dense object detection. 2020 ieee. In CVF Conference on
Computer Vision and Pattern Recognition (CVPR), Seattle, pages 13–19, 2020."
REFERENCES,0.8730964467005076,"[21] Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang,
Yong Li, Wei Lin, et al. M6-10t: A sharing-delinking paradigm for efficient multi-trillion
parameter pretraining. arXiv preprint arXiv:2110.03888, 2021."
REFERENCES,0.8781725888324873,"[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13, pages 740–755. Springer, 2014."
REFERENCES,0.883248730964467,"[23] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang.
DAB-DETR: Dynamic anchor boxes are better queries for DETR. In International Conference
on Learning Representations, 2022."
REFERENCES,0.8883248730964467,"[24] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint
arXiv:2206.08916, 2022."
REFERENCES,0.8934010152284264,"[25] David Marr. Vision: A computational investigation into the human representation and processing
of visual information. MIT press, 2010."
REFERENCES,0.8984771573604061,"[26] Andreas Mogelmose, Mohan Manubhai Trivedi, and Thomas B Moeslund. Vision-based traffic
sign detection and analysis for intelligent driver assistance systems: Perspectives and survey.
IEEE Transactions on Intelligent Transportation Systems, 13(4):1484–1497, 2012."
REFERENCES,0.9035532994923858,"[27] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet
classifiers generalize to imagenet? In International conference on machine learning, pages
5389–5400. PMLR, 2019."
REFERENCES,0.9086294416243654,"[28] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time
object detection with region proposal networks. Advances in neural information processing
systems, 28, 2015."
REFERENCES,0.9137055837563451,"[29] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André
Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.
Advances in Neural Information Processing Systems, 34:8583–8595, 2021."
REFERENCES,0.9187817258883249,"[30] Amir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. arXiv preprint
arXiv:1808.03305, 2018."
REFERENCES,0.9238578680203046,"[31] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538, 2017."
REFERENCES,0.9289340101522843,"[32] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pages
1521–1528, 2011."
REFERENCES,0.934010152284264,"[33] Shimon Ullman, Michel Vidal-Naquet, and Erez Sali. Visual features of intermediate complexity
and their use in classification. Nature neuroscience, 5(7):682–687, 2002."
REFERENCES,0.9390862944162437,"[34] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling
cross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and
pattern recognition, pages 13029–13038, 2021."
REFERENCES,0.9441624365482234,"[35] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vasconcelos. Towards universal object
detection by domain attention. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 7289–7298, 2019."
REFERENCES,0.949238578680203,"[36] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu,
Marcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial
images. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 3974–3983, 2018."
REFERENCES,0.9543147208121827,"[37] Hang Xu, Linpu Fang, Xiaodan Liang, Wenxiong Kang, and Zhenguo Li. Universal-rcnn:
Universal object detector via transferable graph r-cnn. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 12492–12499, 2020."
REFERENCES,0.9593908629441624,"[38] Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam P Harrison, Mohammadhadi Bagheri, and
Ronald M Summers. Deep lesion graphs in the wild: relationship learning and organization of
significant radiology image findings in a diverse large-scale lesion database. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 9261–9270, 2018."
REFERENCES,0.9644670050761421,"[39] Gengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep
stereo matching on high-resolution images. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 5515–5524, 2019."
REFERENCES,0.9695431472081218,"[40] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection
benchmark. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 5525–5533, 2016."
REFERENCES,0.9746192893401016,"[41] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for
computer vision. arXiv preprint arXiv:2111.11432, 2021."
REFERENCES,0.9796954314720813,"[42] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-
Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection.
arXiv preprint arXiv:2203.03605, 2022."
REFERENCES,0.9847715736040609,"[43] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sunderhauf. Varifocalnet: An iou-aware
dense object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 8514–8523, 2021."
REFERENCES,0.9898477157360406,"[44] Xiangyun Zhao, Samuel Schulter, Gaurav Sharma, Yi-Hsuan Tsai, Manmohan Chandraker, and
Ying Wu. Object detection with a unified label space from multiple datasets. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XIV 16, pages 178–193. Springer, 2020."
REFERENCES,0.9949238578680203,"[45] Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
7571–7580, 2022."
