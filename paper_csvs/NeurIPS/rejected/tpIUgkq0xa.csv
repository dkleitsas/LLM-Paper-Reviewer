Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015037593984962407,"Language models (LMs) proficiency in handling deterministic symbolic reasoning
1"
ABSTRACT,0.0030075187969924814,"and rule-based tasks remains limited due to their dependency implicit learning
2"
ABSTRACT,0.004511278195488722,"on textual data. To enable fully rule comprehension ability, we explore how to
3"
ABSTRACT,0.006015037593984963,"incorporate compiled neural networks (CoNNs) which weight is specially designed
4"
ABSTRACT,0.007518796992481203,"into the architecture of LMs, to achieve high accuracy and robust performance.
5"
ABSTRACT,0.009022556390977444,"CoNNs are transformer-based neural networks that execute rules through artificially
6"
ABSTRACT,0.010526315789473684,"generated attention weights. Our method, which call ""Neural Comprehension"", by
7"
ABSTRACT,0.012030075187969926,"incorporating CoNN modules into the LM, the framework effectively tackles rule-
8"
ABSTRACT,0.013533834586466165,"intensive challenges. Our experiments on symbolic reasoning tasks and real-world
9"
ABSTRACT,0.015037593984962405,"arithmetic reasoning tasks demonstrate the superior performance of our method
10"
ABSTRACT,0.016541353383458645,"compared to existing techniques. Furthermore, our LM achieves flawless execution
11"
ABSTRACT,0.01804511278195489,"on symbolic operations tasks, highlighting the potential of our method in enabling
12"
ABSTRACT,0.019548872180451128,"LMs to possess true symbolic comprehension capabilities.
13"
INTRODUCTION,0.021052631578947368,"1
Introduction
14"
INTRODUCTION,0.022556390977443608,"3
10
20
30
Enter Text Length (Digits) 0 20 40 60 80 100"
INTRODUCTION,0.02406015037593985,Solve Rate (%)
INTRODUCTION,0.02556390977443609,<      In-Dist      > <                         Out-of-Dist                         >
INTRODUCTION,0.02706766917293233,"GPT-4
GPT-3.5
T5-base"
INTRODUCTION,0.02857142857142857,"Figure 1: The length generalization of T5
(with fine-tune) [Raffel et al., 2020], GPT-
3.5 (with few-shot) [Ouyang et al., 2022] and
GPT-4 (with few-shot) on symbolic opera-
tions (Additional) tasks. The tasks included
examples such as ""15673 + 3186"" (length =
10). To evaluate the model’s proficiency, we
conducted tests on tasks ranging from 3 to
30 digits, with longer than 10 digits being
out-of-distribution of training data."
INTRODUCTION,0.03007518796992481,"Language models (LMs), particularly large language
15"
INTRODUCTION,0.031578947368421054,"models (LLMs), have exhibited impressive perfor-
16"
INTRODUCTION,0.03308270676691729,"mance on complex reasoning tasks [Brown et al.,
17"
INTRODUCTION,0.03458646616541353,"2020, Zhang et al., 2022a, Chowdhery et al., 2022,
18"
INTRODUCTION,0.03609022556390978,"Wei et al., 2022d,a, Suzgun et al., 2022]. Despite this,
19"
INTRODUCTION,0.03759398496240601,"the proficiency of LMs in tackling deterministic sym-
20"
INTRODUCTION,0.039097744360902256,"bolic reasoning and rule-based tasks is still limited
21"
INTRODUCTION,0.0406015037593985,"[Welleck et al., Razeghi et al., 2022]. For example,
22"
INTRODUCTION,0.042105263157894736,"GPT-3’s arithmetic performance declines with higher
23"
INTRODUCTION,0.04360902255639098,"digit numbers [Brown et al., 2020], and its mathe-
24"
INTRODUCTION,0.045112781954887216,"matical accuracy is influenced by word frequency in
25"
INTRODUCTION,0.04661654135338346,"training data [Razeghi et al., 2022]. Moreover, length
26"
INTRODUCTION,0.0481203007518797,"generalization [Anil et al., 2022] remains a challenge
27"
INTRODUCTION,0.04962406015037594,"even for 100-billion-parameter models, such as GPT-
28"
INTRODUCTION,0.05112781954887218,"4 [Bubeck et al., 2023]. We hypothesize that these
29"
INTRODUCTION,0.05263157894736842,"limitations stem from LMs’ dependency on implicitly
30"
INTRODUCTION,0.05413533834586466,"learning rules from textual data. During the training
31"
INTRODUCTION,0.055639097744360905,"process, the primary objective of implicitly learning
32"
INTRODUCTION,0.05714285714285714,"based on gradient Updating is to minimize the loss associated with the given textual dataset. As
33"
INTRODUCTION,0.058646616541353384,"illustrated in Figure 1, a simple length generalization experiment using addition tasks with varying
34"
INTRODUCTION,0.06015037593984962,"numbers of digits highlights this limitation. Performance deteriorates as test length increases, indicat-
35"
INTRODUCTION,0.061654135338345864,"ing that these models strongly rely on statistical patterns in the data rather than capturing fundamental
36"
INTRODUCTION,0.06315789473684211,"logical structures. This reliance on implicit learning constrains LMs’ accuracy in executing symbolic
37"
INTRODUCTION,0.06466165413533835,"operations tasks. As a result, their performance suffers when confronted with out-of-distribution and
38"
INTRODUCTION,0.06616541353383458,"rule-intensive tasks that require a more profound understanding of abstract rules.
39"
INTRODUCTION,0.06766917293233082,"Neural
Comprehension"
INTRODUCTION,0.06917293233082707,"We propose a transformer-based language model framework, termed ""Neu-
40"
INTRODUCTION,0.07067669172932331,"ral Comprehension"", which synergistically integrates a pre-trained LM [Li
41"
INTRODUCTION,0.07218045112781955,"et al., 2021b] and compiled neural networks (CoNNs) [Weiss et al., 2021] to
42"
INTRODUCTION,0.07368421052631578,"achieve high accuracy and robust performance. CoNNs are neural networks
43"
INTRODUCTION,0.07518796992481203,"but the rules are explicitly coded through transformer-liked structures and
44"
INTRODUCTION,0.07669172932330827,"attention. Therefore CoNN is human-controllable, executing rules through
45"
INTRODUCTION,0.07819548872180451,"artificially generated attention weights, and can achieve perfect accuracy
46"
INTRODUCTION,0.07969924812030076,"once compiled network is done. Neural Comprehension relying solely on
47"
INTRODUCTION,0.081203007518797,"neural networks without requiring additional tools. It employs a token-by-
48"
INTRODUCTION,0.08270676691729323,"token generation method, analogous to GPT-3, where each token can be
49"
INTRODUCTION,0.08421052631578947,"generated by either the pre-trained LM or one of the CoNNs. We comprises a pre-trained LM and
50"
INTRODUCTION,0.08571428571428572,"multiple sets of CoNNs. The implementation of the Neural Comprehension framework facilitates
51"
INTRODUCTION,0.08721804511278196,"the integration of rule-intensive abilities and reasoning capabilities into LMs, endowing them with
52"
INTRODUCTION,0.0887218045112782,"genuine symbolic comprehension skills.
53"
INTRODUCTION,0.09022556390977443,"In this work, we conduct extensive experiments to evaluate the performance of our proposed Neural
54"
INTRODUCTION,0.09172932330827067,"Comprehension method on a variety of rule-intensive tasks. Our experimental results demonstrate the
55"
INTRODUCTION,0.09323308270676692,"effectiveness of our approach in comparison with existing state-of-the-art techniques, such as vanilla
56"
INTRODUCTION,0.09473684210526316,"fine-tuning, few-shot learning, and Chain-of-Thought reasoning. Specifically, Neural Comprehension
57"
INTRODUCTION,0.0962406015037594,"outperforms these methods in terms of accuracy, efficiency, and interpretability, showcasing its
58"
INTRODUCTION,0.09774436090225563,"superiority in handling rule-intensive tasks. Our study presents a strong case for the deployment of
59"
INTRODUCTION,0.09924812030075188,"Neural Comprehension in language models, highlighting its potential to transform the landscape of
60"
INTRODUCTION,0.10075187969924812,"symbolic reasoning and language understanding capabilities.
61"
INTRODUCTION,0.10225563909774436,"Contributions
Our main contributions are as follows:
62"
INTRODUCTION,0.1037593984962406,"• We pioneer the development and implementation of flawless execution rule-intensive sym-
63"
INTRODUCTION,0.10526315789473684,"bolic operations for language models that rely on neural networks. By employing a versatile
64"
INTRODUCTION,0.10676691729323308,"and interpretable method, we successfully integrate CoNNs, which are explicitly coded and
65"
INTRODUCTION,0.10827067669172932,"human-controllable, into the language model. Our method facilitates direct rule deduction
66"
INTRODUCTION,0.10977443609022557,"without the need for learning from conditional probabilities, leading to a more robust and
67"
INTRODUCTION,0.11127819548872181,"effective approach. (Section 3)
68"
INTRODUCTION,0.11278195488721804,"• To expand the application field, we leverage the In-context learning ability of large language
69"
INTRODUCTION,0.11428571428571428,"models to auto generate CoNN. Our method can be easily extended to various symbolic
70"
INTRODUCTION,0.11578947368421053,"operations tasks. (Appendix C)
71"
INTRODUCTION,0.11729323308270677,"• Our experimental results on controllable symbolic reasoning tasks and real-world numerical
72"
INTRODUCTION,0.11879699248120301,"calculation tasks demonstrate the superior performance of our method in comparison to
73"
INTRODUCTION,0.12030075187969924,"existing techniques. Notably, our language model achieves flawless execution on symbolic
74"
INTRODUCTION,0.12180451127819548,"reasoning tasks. (Section 5.1 5.2 5.3)
75"
INTRODUCTION,0.12330827067669173,"• We also studied the potential of combining multiple CoNNs and found that adding correlated
76"
INTRODUCTION,0.12481203007518797,"CoNNs can continuously increase performance, while adding uncorrelated CoNNs rarely
77"
INTRODUCTION,0.12631578947368421,"leads to performance degradation. This provides a new approach for model fusion, enabling
78"
INTRODUCTION,0.12781954887218044,"the model to easily acquire new knowledge. (Section 5.4)
79"
RELATED WORKS,0.1293233082706767,"2
Related Works
80"
RELATED WORKS,0.13082706766917293,"As model parameters, training calculations, and dataset sizes have increased, language models have
81"
RELATED WORKS,0.13233082706766916,"gained new capabilities [Srivastava et al., 2022, Wei et al., 2022a], such as coding [Li et al., 2022b,
82"
RELATED WORKS,0.13383458646616542,"Nijkamp et al., 2022], medical diagnosis [Li et al., 2021a, Xia et al., 2022], complex question-
83"
RELATED WORKS,0.13533834586466165,"answering [Zhu et al., 2022, Daull et al., 2023], cross-language translation [Fan et al., 2021, Li et al.,
84"
RELATED WORKS,0.1368421052631579,"2022a], few-shot learning [Brown et al., 2020, Perez et al., 2021], and thought chaining [Wei et al.,
85"
RELATED WORKS,0.13834586466165413,"2022c, Weng et al., 2022]. However, these models also exhibit limitations as they generally learn
86"
RELATED WORKS,0.13984962406015036,"superficial patterns rather than the innate logic and rules of language. Consequently, humans often
87"
RELATED WORKS,0.14135338345864662,"find it challenging to trust the results provided by language models [Sarker et al., 2021, Moore, 2022].
88"
RELATED WORKS,0.14285714285714285,"Pre-trained Language Models encompass those trained on general-purpose corpora [Lewis et al.,
89"
RELATED WORKS,0.1443609022556391,"2019, Scao et al., 2022] and specialized symbolic tasks [Geva et al., 2020, Lewkowycz et al., 2022].
90"
RELATED WORKS,0.14586466165413534,"They primarily aim to capture statistical patterns in language, which limits their capacity for symbolic
91"
RELATED WORKS,0.14736842105263157,"reasoning. Symbolic reasoning involves manipulating abstract symbols and logical rules to derive
92"
RELATED WORKS,0.14887218045112782,"new knowledge [Shindo et al., 2021, Yang and Deng, 2021] and necessitates the ability to extrapolate
93"
RELATED WORKS,0.15037593984962405,"to novel situations and reason about concepts absent in the training data [Fujisawa and Kanai, 2022].
94"
RELATED WORKS,0.1518796992481203,"Due to the constraints of gradient learning, neural networks face challenges in wholly solving
95"
RELATED WORKS,0.15338345864661654,"symbolic reasoning problems.
96"
RELATED WORKS,0.1548872180451128,"In-Context Learning has emerged as a promising approach to address these challenges [Dong et al.,
97"
RELATED WORKS,0.15639097744360902,"2022] and closely approximate the predictors computed by gradient descent [Akyürek et al., 2022].
98"
RELATED WORKS,0.15789473684210525,"By prompting the language model to generate an explanation before generating an answer, the chain
99"
RELATED WORKS,0.1593984962406015,"of thought [Wei et al., 2022c, Kojima et al., 2022, Zhang et al., 2022b, Zhou et al., 2022a] encourages
100"
RELATED WORKS,0.16090225563909774,"the model to think sequentially. This technique has been employed in various numerical and symbolic
101"
RELATED WORKS,0.162406015037594,"reasoning tasks, such as scratchpad prompting [Nye et al., 2021] for length generalization [Anil
102"
RELATED WORKS,0.16390977443609023,"et al., 2022] and utilizing the chain of thought to perform arithmetic operations like summing pairs of
103"
RELATED WORKS,0.16541353383458646,"single digits with carry [Zhou et al., 2022b]. However, this approach often necessitates substantial
104"
RELATED WORKS,0.16691729323308271,"computational resources, and achieving perfect accuracy remains challenging.
105"
RELATED WORKS,0.16842105263157894,"Augmented Language Models have been proposed as an alternative, supplementing language
106"
RELATED WORKS,0.1699248120300752,"models with external tools [Mialon et al., 2023]. Examples include generating Python code for
107"
RELATED WORKS,0.17142857142857143,"numerical reasoning [Gao et al., 2022, Chen et al., 2022] or incorporating tool usage as a pre-training
108"
RELATED WORKS,0.17293233082706766,"task [Schick et al., 2023]. However, using external tools lacks a unified framework with language
109"
RELATED WORKS,0.17443609022556392,"models and instead relies on the normativity of program generation. Consequently, if a task demands
110"
RELATED WORKS,0.17593984962406015,"higher-level abstraction or intricate and robust capabilities, such as Redefine [Wei et al., 2022b],
111"
RELATED WORKS,0.1774436090225564,"Autoformalization [Wu et al., 2022], and Theorem Proving [Wu et al., 2020], the language model
112"
RELATED WORKS,0.17894736842105263,"may struggle to solve it, even if it possesses the ability to operate external tools [Zhou et al., 2022b].
113"
METHODS,0.18045112781954886,"3
Methods
114"
PRELIMINARIES,0.18195488721804512,"3.1
Preliminaries
115"
PRELIMINARIES,0.18345864661654135,"In-Context Learning (ICL), Recent studies on ICL algorithms have shown that the learning process
116"
PRELIMINARIES,0.1849624060150376,"of language models within the ICL framework is analogous to gradient descent [Akyürek et al., 2022].
117"
PRELIMINARIES,0.18646616541353384,"Specifically, transformer-based in-context learners implicitly implement standard learning algorithms
118"
PRELIMINARIES,0.18796992481203006,"by encoding smaller models in their activations and updating these implicit models as new examples
119"
PRELIMINARIES,0.18947368421052632,"appear in the context. However, these models face challenges in rule-intensive questions, as the
120"
PRELIMINARIES,0.19097744360902255,"rules represent abstract, high-dimensional knowledge that cannot be directly learned from the data,
121"
PRELIMINARIES,0.1924812030075188,"resulting in difficulties with implicit learning.
122"
PRELIMINARIES,0.19398496240601504,"Compiled Neural Network (CoNN). The flexibility of neural networks to adjust their weights is
123"
PRELIMINARIES,0.19548872180451127,"a unique characteristic not found in the human brain. We propose incorporating CoNNs into LLM
124"
PRELIMINARIES,0.19699248120300752,"architectures to leverage this feature. The CoNN is a transformer-based neural network leveraging
125"
PRELIMINARIES,0.19849624060150375,"artificially compiled attention weights to execute rules. A transformer model comprises multiple
126"
PRELIMINARIES,0.2,"attention layers and Multi-Layer Perceptron (MLP) layers. Each attention layer facilitates interactions
127"
PRELIMINARIES,0.20150375939849624,"between tokens, with the multiplication of query and key elements representing a ""Select"" operation
128"
PRELIMINARIES,0.20300751879699247,"in CoNN. Subsequent multiplication with value elements indicates an ""Aggregate"" operation. The
129"
PRELIMINARIES,0.20451127819548873,"MLP layer is responsible for the token itself and is referred to as the ""Zipmap"" operation [Weiss
130"
PRELIMINARIES,0.20601503759398496,"et al., 2021]. Utilizing the three operations (Select, Aggregate, and Zipmap) to represent the sequence-
131"
PRELIMINARIES,0.2075187969924812,"to-sequence process, we can convert this information into transformer weights [Lindner et al., 2023].
132"
PRELIMINARIES,0.20902255639097744,"By stacking multiple attention layers, CoNN can address various human-defined rule understanding
133"
PRELIMINARIES,0.21052631578947367,"problems, such as mathematical calculations and symbol operations 1.
134"
NEURAL COMPREHENSION,0.21203007518796993,"3.2
Neural Comprehension
135"
NEURAL COMPREHENSION,0.21353383458646616,"Language models excel in language understanding tasks, while CoNNs achieve absolut accuracy
136"
NEURAL COMPREHENSION,0.21503759398496242,"in rule-intensive operation tasks using attention weights guided by abstract rules. To combine the
137"
NEURAL COMPREHENSION,0.21654135338345865,"language understanding capabilities of existing language models with accurate problem-solving for
138"
NEURAL COMPREHENSION,0.21804511278195488,"rule-based tasks (e.g., computation), we propose the Neural Comprehension, which integrates the
139"
NEURAL COMPREHENSION,0.21954887218045113,"language model’s implicit learning parameters and CoNNs’ explicit learning parameters. In Neural
140"
NEURAL COMPREHENSION,0.22105263157894736,1Appendix B provides a more detailed description of CoNN.
NEURAL COMPREHENSION,0.22255639097744362,"To find out how much farther Stanley ran than walked, we 
need to subtract the distance he walked from the distance he ran 
364425 - 216582 = 147843  meters. Therefore, Stanley ran 147843 
meters farther than he walked."
NEURAL COMPREHENSION,0.22406015037593985,"Compiled 
Neural Network"
NEURAL COMPREHENSION,0.22556390977443608,Language Models with Compiled Neural Networks
NEURAL COMPREHENSION,0.22706766917293233,Input:  The iWatch show that Stanley ran 364425 meters and walked 216582 meters a month. How much farther did Stanley run than walk ?
NEURAL COMPREHENSION,0.22857142857142856,"Pre-training 
Neural Network"
NEURAL COMPREHENSION,0.23007518796992482,...                                               ...
NEURAL COMPREHENSION,0.23157894736842105,Both neural networks are similar to the decoder structure of transformer …
NEURAL COMPREHENSION,0.23308270676691728,"…
216582
=
147843
meters"
NEURAL COMPREHENSION,0.23458646616541354,"-
=
147843
216582 … …"
NEURAL COMPREHENSION,0.23609022556390977,Output
NEURAL COMPREHENSION,0.23759398496240602,Neural
NEURAL COMPREHENSION,0.23909774436090225,"Comprehension
Neural"
NEURAL COMPREHENSION,0.24060150375939848,"Comprehension
Neural"
NEURAL COMPREHENSION,0.24210526315789474,"Comprehension
Neural"
NEURAL COMPREHENSION,0.24360902255639097,Comprehension Input
NEURAL COMPREHENSION,0.24511278195488723,"——
——
——
——
147843
meters
=
216582
-
148843"
-,0.24661654135338346,"216582
-
=
147843
meters … …"
-,0.24812030075187969,Figure 2: The architecture of Neural Comprehension.
-,0.24962406015037594,"Comprehension, CoNNs represent high-dimensional rules explicitly using multiple attention matrices
141"
-,0.2511278195488722,"and incorporate these with the original LM’s attention matrix.
142"
-,0.25263157894736843,"As illustrated in Figure 2, we maintain the use of a decoder architecture to iteratively generate
143"
-,0.25413533834586466,"the subsequent context step by step. In particular, the language model encodes the context and
144"
-,0.2556390977443609,"produces the textual and reasoning process context D(x) step by step, while CoNNs handle sequence
145"
-,0.2571428571428571,"transformations involving rules. When a rule-required operation emerges, CoNN’s attention is utilized
146"
-,0.2586466165413534,"to calculate specific values. The structure of Neural Comprehension is similar to MoE [Shazeer
147"
-,0.26015037593984963,"et al., 2017]. For example, when calculating 364425-216582, the pre-trained language model output
148"
-,0.26165413533834586,"148843, which is incorrect. However, the Subtraction CoNN can correct the result to 147843 in
149"
-,0.2631578947368421,"the neural comprehension framework. This process encoded into context dynamically, improving
150"
-,0.2646616541353383,"intermediate results interpretability and final result accuracy.
151"
-,0.2661654135338346,"Neural Comprehension combines LM and CoNNs in a piecewise function to perform gradient update.
152"
-,0.26766917293233083,"LLM hidden state output is HL =

HL1 · · · HLdL"
-,0.26917293233082706,"⊤
∈RdL,
HLi ∈(0, 1), and CoNN output
153"
-,0.2706766917293233,"is HC =

HC1 · · · HCdC"
-,0.2721804511278195,"⊤
∈RdC,
HCi ∈(0, 1) 2. Specifically, we perform model fusion by
154"
-,0.2736842105263158,"adding the mapping from the last hidden layer representation to the vocabulary.
155"
-,0.27518796992481204,"ˆi = argmax
i"
-,0.27669172932330827,"
IdL, 0
0, βIdC"
-,0.2781954887218045," 
HL, 0
0, HC"
-,0.2796992481203007,"
,
β ∈{0, 1}
(1)"
-,0.281203007518797,"Within the Neural Comprehension, CoNNs manage sequence transformations involving rules. When
156"
-,0.28270676691729324,"the model encounters a rule-required operation, a gating mechanism determines whether to use
157"
-,0.28421052631578947,"CoNN’s attention for computation. The gating mechanism assesses whether to maintain the initial
158"
-,0.2857142857142857,"output, provided by the pretrained language model, or modify it using the CoNN. where the model
159"
-,0.28721804511278193,"corrects the answer by applying a gradient to the in-context learning function through β. In Equation
160"
-,0.2887218045112782,"1, since the hidden state output HCi elements of CoNN are {0, 1}, when β = 0, the model adopts
161"
-,0.29022556390977444,"the original decoding token of LM. When encountering a rule calculation problem, β = 1, the
162"
-,0.29172932330827067,"model calculates the result by taking the maximum value of CoNN’s hidden layer output HC and
163"
-,0.2932330827067669,"decodes the result from CoNN’s vocabulary. Regarding the selection of β, since the CoNN involved
164"
-,0.29473684210526313,"in this paper is relatively simple, it is determined by the forward computation results of CoNN. For
165"
-,0.2962406015037594,"example, when we set up an Addition CoNN, we specify that the final result should be output when
166"
-,0.29774436090225564,"2It is worth noting that dL and dC here refer to the vocabulary size of the Model’s decode output. In this
paper, for ease of implementation, the output vocabulary size of CoNNs’ decode dC is generally less than 100
due to limitations in computing resources (detailed information is shown in Appendix Table 1). The Neural
Comprehension combines the Pre-trained LM’s hidden state output, HL, and CoNN’s output, HC, using identity
matrices IdL (for dL) and IdC (for dC) to concatenate them for model fusion."
-,0.2992481203007519,"encountering ’=’, so when encountering ’=’, β = 1. However, for larger-scale CoNN, we recommend
167"
-,0.3007518796992481,"that a learnable gating network determine β.
168"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3022556390977444,"3.3
Gradient Modification in Neural Comprehension
169"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3037593984962406,"To better appreciate the benefits of our method in handling rule-intensive tasks and improving
170"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.30526315789473685,"accuracy, it is crucial to understand the gradient perspective of ICL. The optimization process in
171"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3067669172932331,"ICL can be viewed as a search for suitable gradients to minimize the loss function. Due to the
172"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3082706766917293,"implicit learning nature of standard ICL methods, gradients learned from data may not always be
173"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3097744360902256,"ideal for addressing rule-intensive tasks. Therefore, our proposed method introduces an explicit
174"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3112781954887218,"learning component to provide more appropriate gradient updates for such tasks, ultimately leading
175"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.31278195488721805,"to enhanced overall performance. In this section, we focus on elucidating the changes in the gradient
176"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3142857142857143,"introduced by the Neural Comprehension model.
177"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3157894736842105,"The gradient of the model during the execution of ICL can be partitioned into two categories based
178"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3172932330827068,"on the origin of the gradients:
179"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.318796992481203,"Gradient =

Id1
Text
Id2
Rule
(2)"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.32030075187969925,"Here, Id1 represents the gradients derived implicitly from the language model (LM) and corresponds
180"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3218045112781955,"to the text-based learning aspect of the model. Conversely, Id2 represents the gradients explicitly
181"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3233082706766917,"derived from the CoNNs, encoding rule-based knowledge. The Neural Comprehension model
182"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.324812030075188,"integrates both gradient sources to optimize the ICL process.
183"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3263157894736842,"In linear regression problems, the loss function can be expressed as a piecewise function according
184"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.32781954887218046,"to 1, here P1(x) is the LLM and P2(x) is CONN, the In-context-learner can be separate into two
185"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3293233082706767,"process :
186"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3308270676691729,"L =
y −β⊤x
2
(3) ="
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3323308270676692,"( y −β⊤
1 x
2
x ∈P1(x)
y −β⊤
2 x
2
x ∈P2(x)
(4)"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.33383458646616543,"Based on the partitioned gradient as defined in Equation 2, the overall gradient of the Neural
187"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.33533834586466166,"Comprehension model can be obtained by computing their individual gradients concerning the
188"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3368421052631579,"respective β:
189"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3383458646616541,"∂L
∂β
|{z}
Gradient ="
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3398496240601504,"(
∂L
∂β1
x ∈P1(x)
∂L
∂β2
x ∈P2(x)
(5)"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.34135338345864663,"This partitioning allows the Neural Comprehension model to specifically address the gradient require-
190"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.34285714285714286,"ments of both implicit learning via LM and explicit learning via CoNNs. It is crucial to note that
191"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3443609022556391,"CoNNs are designed to minimize the loss associated with rule-based tasks, essentially providing an
192"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3458646616541353,"optimal gradient for tasks involving rule-intensive operations. This leads to a substantial improvement
193"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3473684210526316,"in the model’s accuracy for rule-based tasks, as the gradient updates provided by CoNNs are more
194"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.34887218045112783,"suitable for rule learning compared to the initially available gradients from the LM. By amalgamating
195"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.35037593984962406,"the both of gradient sources, the Neural Comprehension model achieves a more refined optimization
196"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3518796992481203,"of in-context learning. Additionally, from the perspective of gradients, our approach surpasses
197"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3533834586466165,"conventional data-driven implicit learning techniques as it integrates explicit rule-based learning
198"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.3548872180451128,"mechanisms that exhibit more suitable gradient updates for rule-intensive questions. The Neural
199"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.35639097744360904,"Comprehension model effectively balances the need for implicit and explicit learning within the ICL
200"
GRADIENT MODIFICATION IN NEURAL COMPREHENSION,0.35789473684210527,"framework, leading to an enhanced overall performance in terms of accuracy and interpretability.
201"
EXPERIMENTAL SETTINGS,0.3593984962406015,"4
Experimental Settings
202"
EXPERIMENTAL SETTINGS,0.3609022556390977,"In this study, we primarily explore the capacity of language models to address symbolic reason-
203"
EXPERIMENTAL SETTINGS,0.362406015037594,"ing tasks, concentrating on three areas: symbolic operations, symbolic reasoning, and arithmetic
204"
EXPERIMENTAL SETTINGS,0.36390977443609024,"reasoning.
205"
EXPERIMENTAL SETTINGS,0.36541353383458647,"Symbolic Operations
Building upon the approaches developed by Anil et al. [2022] and Qian
206"
EXPERIMENTAL SETTINGS,0.3669172932330827,"et al. [2022], we examine the following tasks: Parity, Reverse, Addition and Subtraction. These
207"
EXPERIMENTAL SETTINGS,0.3684210526315789,"tasks do not require complex text understanding, but only require faithfully implementing symbolic
208"
EXPERIMENTAL SETTINGS,0.3699248120300752,"operations and outputting the corresponding results.
209"
EXPERIMENTAL SETTINGS,0.37142857142857144,"Symbolic Reasoning
We employ the experimental framework of Wei et al. [2022c] for the two
210"
EXPERIMENTAL SETTINGS,0.37293233082706767,"tasks, Last Letter Concatenation and Coin Flip. These tasks require a combination of language
211"
EXPERIMENTAL SETTINGS,0.3744360902255639,"understanding and rule comprehension abilities.
212"
EXPERIMENTAL SETTINGS,0.37593984962406013,"Arithmetic Reasoning
To evaluate the method’s generalization ability from symbolic operations
213"
EXPERIMENTAL SETTINGS,0.3774436090225564,"to arithmetic reasoning in addition and subtraction tasks, we use five established arithmetic reasoning
214"
EXPERIMENTAL SETTINGS,0.37894736842105264,"datasets: AddSub [Hosseini et al., 2014], SingleEq [Koncel-Kedziorski et al., 2015], MultiArith [Roy
215"
EXPERIMENTAL SETTINGS,0.3804511278195489,"and Roth, 2016], GSM8K [Cobbe et al., 2021], and SVAMP [Arkil et al., 2021]. Additionally, we
216"
EXPERIMENTAL SETTINGS,0.3819548872180451,"introduce the AddSub+ dataset, containing tasks of varying complexity based on the number of digits
217"
EXPERIMENTAL SETTINGS,0.38345864661654133,"involved in arithmetic operations, ranging from 1-digit addition to 20-digit addition/subtraction tasks.
218"
ECPERIMENT AND RESULT,0.3849624060150376,"5
Ecperiment and Result
219"
SYMBOLIC TASKS,0.38646616541353385,"5.1
Symbolic Tasks
220"
SYMBOLIC TASKS,0.3879699248120301,"1
5
10
15
20
25
30
35
40
(A) Parity 0 20 40 60 80 100"
SYMBOLIC TASKS,0.3894736842105263,Solve Rate (%)
SYMBOLIC TASKS,0.39097744360902253,<     Out-of-Dist     > <         In-Dist         > <                        Out-of-Dist                        >
SYMBOLIC TASKS,0.3924812030075188,"T5-small
T5-base"
SYMBOLIC TASKS,0.39398496240601505,"T5-large
GPT-3.5"
SYMBOLIC TASKS,0.3954887218045113,"GPT-3
GLM"
SYMBOLIC TASKS,0.3969924812030075,"Scratchpad (T5-base)
Neural Comprehension"
SYMBOLIC TASKS,0.39849624060150374,"1
5
10
15
20
25
30
35
40
(B) Reverse 0 20 40 60 80 100"
SYMBOLIC TASKS,0.4,Solve Rate (%)
SYMBOLIC TASKS,0.40150375939849625,<     Out-of-Dist     > <         In-Dist         > <                        Out-of-Dist                        >
SYMBOLIC TASKS,0.4030075187969925,"T5-small
T5-base"
SYMBOLIC TASKS,0.4045112781954887,"T5-large
GPT-3.5"
SYMBOLIC TASKS,0.40601503759398494,"GPT-3
GLM"
SYMBOLIC TASKS,0.4075187969924812,Neural Comprehension
SYMBOLIC TASKS,0.40902255639097745,"3
5
10
15
20
25
30
35
40
(C) Addition 0 20 40 60 80 100"
SYMBOLIC TASKS,0.4105263157894737,Solve Rate (%)
SYMBOLIC TASKS,0.4120300751879699,<Out-of-Dist> <         In-Dist         > <                        Out-of-Dist                        >
SYMBOLIC TASKS,0.41353383458646614,"T5-small
T5-base"
SYMBOLIC TASKS,0.4150375939849624,"T5-large
GPT-3.5"
SYMBOLIC TASKS,0.41654135338345866,"GPT-3
GLM"
SYMBOLIC TASKS,0.4180451127819549,"Algorithm (GPT-3.5)
Neural Comprehension"
SYMBOLIC TASKS,0.4195488721804511,"3
5
10
15
20
25
30
35
40
(D) Subtraction 0 20 40 60 80 100"
SYMBOLIC TASKS,0.42105263157894735,Solve Rate (%)
SYMBOLIC TASKS,0.42255639097744363,<Out-of-Dist> <         In-Dist         > <                        Out-of-Dist                        >
SYMBOLIC TASKS,0.42406015037593986,"T5-small
T5-base"
SYMBOLIC TASKS,0.4255639097744361,"T5-large
GPT-3.5"
SYMBOLIC TASKS,0.4270676691729323,"GPT-3
GLM"
SYMBOLIC TASKS,0.42857142857142855,Neural Comprehension
SYMBOLIC TASKS,0.43007518796992483,"Figure 3: Comparison of Neural Comprehension and other implicit learning-based methods in symbolic
operations tasks to test length generalization performance. In this, the T5 model uses the Vanilla Fine-tune
method for learning, and LLMs use the Few-shot learning method. In Neural Comprehension, each task has a
different CoNN, namely Parity, Reverse, Addition, and Subtraction."
SYMBOLIC TASKS,0.43157894736842106,"Techniques
In-distribution
Out-of-distribution
Time and Space Complexity
Interpretability"
SYMBOLIC TASKS,0.4330827067669173,"Vanilla Fine-tune (For LM)
✓✓
✗
✓✓
✗
Vanilla Few-shot (For LLM)
✓
✓
✓✓
✗
Scratchpad [Anil et al., 2022]
✓✓
✓
✗
✓
Algorithmic [Zhou et al., 2022b]
✓✓
✓
✗
✓
Neural Comprehension (Ours)
✓✓
✓✓
✓✓
✓✓"
SYMBOLIC TASKS,0.4345864661654135,"Table 1: Performance on Symbolic operations tasks of five techniques that language models admit: (1) Vanilla
Finetuning, (2) Vanilla Few-shot, (3) Scratchpad (Chain-of-Thought reasoning), (4) Algorithmic (Chain-of-
Thought reasoning) and (5) Neural Comprehension. We find that the first four learning-based methods have
different modes of failure regarding in and out-of-distribution coverage for symbolic operations. However,
Neural Comprehension has strong advantages in terms of length generalization, efficiency, and interpretability.
✗signifies poor ✓signifies nontrivial, ✓✓signifies near-perfect performance. (*) Refers to task-dependency."
SYMBOLIC TASKS,0.43609022556390975,"In this study, we conduct a length generalization experiment [Anil et al., 2022] to examine the
221"
SYMBOLIC TASKS,0.43759398496240604,"distinctions between the Neural Comprehension and learning-based methods, as depicted in Figure 3.
222"
SYMBOLIC TASKS,0.43909774436090226,"Our experimental design encompasses 1000 × 40 independent test sets, comprising problems with
223"
SYMBOLIC TASKS,0.4406015037593985,"varying digit lengths from 1 to 40 digits. 10 to 20 digits within the range are provided by us for
224"
SYMBOLIC TASKS,0.4421052631578947,"methods based on implicit learning for training; during the testing phase, this range is called In-Dist.
225"
SYMBOLIC TASKS,0.44360902255639095,"Furthermore, we present results for both Scratchpad [Anil et al., 2022] and Algorithmic [Zhou et al.,
226"
SYMBOLIC TASKS,0.44511278195488724,"2022b] approaches.
227"
SYMBOLIC TASKS,0.44661654135338347,"The results of our experiment demonstrate that the Vanilla Fine-tune (red lines) method performs
228"
SYMBOLIC TASKS,0.4481203007518797,"optimally on the in-domain (10-20 digit) training set, while its performance deteriorates for both
229"
SYMBOLIC TASKS,0.4496240601503759,"more simplistic and more intricate. This finding suggests that the absence of relevant samples in the
230"
SYMBOLIC TASKS,0.45112781954887216,"training set may cause gradient descent-based language models to underperform on both simpler and
231"
SYMBOLIC TASKS,0.45263157894736844,"more complex tasks. As further discussed in the appendix D.1, this phenomenon can be attributed to
232"
SYMBOLIC TASKS,0.45413533834586467,"the inherent generalization limitations of statistical models and the position bias of language models.
233"
SYMBOLIC TASKS,0.4556390977443609,"Considering the Vanilla Few-shot method (green lines), we determine that its performance is less
234"
SYMBOLIC TASKS,0.45714285714285713,"impacted by the prompt sample range compared to Vanilla Fine-tune. Large language models, which
235"
SYMBOLIC TASKS,0.45864661654135336,"are trained on extensive text corpora, excel at solving more straightforward problems such as symbolic
236"
SYMBOLIC TASKS,0.46015037593984964,"operations within a ten-digit range. Nevertheless, performance remains below par for test sets with
237"
SYMBOLIC TASKS,0.4616541353383459,"more than ten digits, even when prompted with 10-20 digit samples.
238"
SYMBOLIC TASKS,0.4631578947368421,"Observing CoT-like methods (we use GPT-3.5), including Scratchpad and Algorithmic, unveils their
239"
SYMBOLIC TASKS,0.46466165413533833,"robust length generalization capabilities. Scratchpad works by requiring large language models
240"
SYMBOLIC TASKS,0.46616541353383456,"to record intermediate steps, while Algorithmic employs a similar approach to record the carry
241"
SYMBOLIC TASKS,0.46766917293233085,"operations involved in the addition process. This can be primarily attributed to their proficiency in
242"
SYMBOLIC TASKS,0.4691729323308271,"decomposing complex problems into smaller incremental steps and maintaining intermediate states.
243"
SYMBOLIC TASKS,0.4706766917293233,"However, these methods necessitate substantial computational resources, and extending the length
244"
SYMBOLIC TASKS,0.47218045112781953,"beyond the input limit of the model becomes challenging.
245"
SYMBOLIC TASKS,0.47368421052631576,"Our study reveals that Neural Comprehension attains remarkably high accuracy in symbolic operations.
246"
SYMBOLIC TASKS,0.47518796992481205,"This implies that Neural Comprehension, unlike conventional methods, does not rely on training data
247"
SYMBOLIC TASKS,0.4766917293233083,"and remains unaffected by discrepancies in input lengths for in-distribution and out-of-distribution
248"
SYMBOLIC TASKS,0.4781954887218045,"data. Consequently, it alleviates the requirement for step-by-step work tracking, and language
249"
SYMBOLIC TASKS,0.47969924812030074,"models with CoNNs only need relatively fewer computational steps to execute sequence operations
250"
SYMBOLIC TASKS,0.48120300751879697,"directly. Encoding rules into neural network modules endows us with greater interpretability, enabling
251"
SYMBOLIC TASKS,0.48270676691729325,"language models to flawlessly perform purely symbolic operation tasks.
252"
SYMBOLIC REASONING,0.4842105263157895,"5.2
Symbolic Reasoning
253"
SYMBOLIC REASONING,0.4857142857142857,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.48721804511278194,Coin Flip
SYMBOLIC REASONING,0.48872180451127817,Iterations
SYMBOLIC REASONING,0.49022556390977445,T5 Small:60M
SYMBOLIC REASONING,0.4917293233082707,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.4932330827067669,Iterations
SYMBOLIC REASONING,0.49473684210526314,T5 Base:220M
SYMBOLIC REASONING,0.49624060150375937,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.49774436090225566,Iterations
SYMBOLIC REASONING,0.4992481203007519,T5 Large:770M
SYMBOLIC REASONING,0.5007518796992482,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.5022556390977444,Last Letter Concatenation
SYMBOLIC REASONING,0.5037593984962406,Iterations
SYMBOLIC REASONING,0.5052631578947369,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.5067669172932331,Iterations
SYMBOLIC REASONING,0.5082706766917293,"0
6K
12K
18K
24K
0% 20% 40% 60% 80% 100%"
SYMBOLIC REASONING,0.5097744360902255,Iterations
SYMBOLIC REASONING,0.5112781954887218,"Vanilla Fine-tune
Neural Comprehension"
SYMBOLIC REASONING,0.512781954887218,"Direct Fine-tune
Improve Performance"
SYMBOLIC REASONING,0.5142857142857142,"Figure 4: In the iterative process of gradient descent
during training. The bleu line represents a language
model that incorporates neural comprehension, and the
red line represents the original language model. Addi-
tionally, we provide Direct, which is a direct prediction
of the final result, as a reference."
SYMBOLIC REASONING,0.5157894736842106,"In this section, we investigate the performance
254"
SYMBOLIC REASONING,0.5172932330827068,"of Neural Comprehension in terms of sym-
255"
SYMBOLIC REASONING,0.518796992481203,"bolic reasoning capabilities. Our hypothesis
256"
SYMBOLIC REASONING,0.5203007518796993,"is that, although pretrained Language Models
257"
SYMBOLIC REASONING,0.5218045112781955,"(LMs) demonstrate strong language understand-
258"
SYMBOLIC REASONING,0.5233082706766917,"ing abilities, they lack the capacity to deduce
259"
SYMBOLIC REASONING,0.524812030075188,"and comprehend rules regarding symbolic rea-
260"
SYMBOLIC REASONING,0.5263157894736842,"soning tasks. Thus, we aim to evaluate whether
261"
SYMBOLIC REASONING,0.5278195488721804,"the incorporation of compiled neural networks
262"
SYMBOLIC REASONING,0.5293233082706766,"in the form of CoNNs can address this limita-
263"
SYMBOLIC REASONING,0.530827067669173,"tion and improve the LM’s symbolic reasoning
264"
SYMBOLIC REASONING,0.5323308270676692,"abilities.
265"
SYMBOLIC REASONING,0.5338345864661654,"To assess the performance of the rule com-
266"
SYMBOLIC REASONING,0.5353383458646617,"prehension component (CoNNs) in symbolic
267"
SYMBOLIC REASONING,0.5368421052631579,"reasoning, we devise an experiment that mea-
268"
SYMBOLIC REASONING,0.5383458646616541,"sures the model’s accuracy using intermediate
269"
SYMBOLIC REASONING,0.5398496240601504,"processes and represents them in a ""Chain of
270"
SYMBOLIC REASONING,0.5413533834586466,"Thought""-like manner. In doing so, the experi-
271"
SYMBOLIC REASONING,0.5428571428571428,"ment decomposes language understanding and
272"
SYMBOLIC REASONING,0.544360902255639,"rule comprehension explicitly into simpler out-
273"
SYMBOLIC REASONING,0.5458646616541354,"puts, avoiding the complexities of reasoning and
274"
SYMBOLIC REASONING,0.5473684210526316,"additional error propagation in the models. Ex-
275"
SYMBOLIC REASONING,0.5488721804511278,"ample outputs from this approach can be found
276"
SYMBOLIC REASONING,0.5503759398496241,"in Appendix F. We observed that neural com-
277"
SYMBOLIC REASONING,0.5518796992481203,"prehension improves the symbolic reasoning capabilities of pre-trained language models in most
278"
SYMBOLIC REASONING,0.5533834586466165,"cases (Neural Comprehension almost always outperforms Vanilla Fine-tune in Figure 4), and can fit
279"
SYMBOLIC REASONING,0.5548872180451128,"faster. This observation suggests that the introduction of compiled neural networks has a positive
280"
SYMBOLIC REASONING,0.556390977443609,"impact on pretrained LMs, addressing rule comprehension limitations in symbolic reasoning tasks.
281"
ARITHMETIC REASONING,0.5578947368421052,"5.3
Arithmetic Reasoning
282"
ARITHMETIC REASONING,0.5593984962406015,"1
5
10
15
20
The number of digits 0 20 40 60 80 100"
ARITHMETIC REASONING,0.5609022556390978,Solve Rate (%)
ARITHMETIC REASONING,0.562406015037594,GPT3.5: 175B
ARITHMETIC REASONING,0.5639097744360902,"Vanilla CoT
PAL
Neural Comprehension
Improve Performance"
ARITHMETIC REASONING,0.5654135338345865,"1
5
10
15
20
The number of digits 0 20 40 60 80 100"
ARITHMETIC REASONING,0.5669172932330827,Solve Rate (%)
ARITHMETIC REASONING,0.5684210526315789,GPT3: 175B
ARITHMETIC REASONING,0.5699248120300752,"1
5
10
15
20
The Number of Digits 0 20 40 60 80 100"
ARITHMETIC REASONING,0.5714285714285714,Solve Rate (%)
ARITHMETIC REASONING,0.5729323308270676,GLM: 130B
ARITHMETIC REASONING,0.5744360902255639,"Figure 5: We conducted simulations of the AddSub dataset with varying digits by modifying the ""lEquations""
parameter. We then tested the performance of three LLMs with and without Neural Comprehension in generating
CoT outputs for AddSub+. And we reported the solve rates of three LLMs and compared the solve rates of
using additional tools (PAL [Gao et al., 2022])."
ARITHMETIC REASONING,0.5759398496240602,"Arithmetic reasoning serves as a suitable testbed for evaluating language models and their ability to
283"
ARITHMETIC REASONING,0.5774436090225564,"address real-world problems. In this study, we examine the AddSub+ dataset variants that involve
284"
ARITHMETIC REASONING,0.5789473684210527,"different digit lengths, utilizing the Addition and Subtraction models from the CoNNs family.
285"
ARITHMETIC REASONING,0.5804511278195489,"Notably, the capabilities of Neural Comprehension extend beyond these tasks, as CoNNs can also
286"
ARITHMETIC REASONING,0.5819548872180451,"simulate calculators that support multiplication and division operations, and potentially perform
287"
ARITHMETIC REASONING,0.5834586466165413,"linear algebra computations or even in-context learning algorithms that employ backpropagation
288"
ARITHMETIC REASONING,0.5849624060150376,"[Giannou et al., 2023].
289"
ARITHMETIC REASONING,0.5864661654135338,"To evaluate the impact of Neural Comprehension on arithmetic reasoning, we compare the output
290"
ARITHMETIC REASONING,0.58796992481203,"of vanilla CoT language models and those incorporating Neural Comprehension, using the vanilla
291"
ARITHMETIC REASONING,0.5894736842105263,"CoT baseline as a reference. As demonstrated in Figure 5, the vanilla CoT model struggles to
292"
ARITHMETIC REASONING,0.5909774436090226,"extrapolate and solve arithmetic problems involving longer digit lengths. However, integrating
293"
ARITHMETIC REASONING,0.5924812030075188,"Neural Comprehension significantly improves the performance of language models on such complex
294"
ARITHMETIC REASONING,0.5939849624060151,"arithmetic tasks. Since we only incorporated the Addition and Subtraction CoNNs, we attribute
295"
ARITHMETIC REASONING,0.5954887218045113,"the observed performance enhancement to the increased computational accuracy of the language
296"
ARITHMETIC REASONING,0.5969924812030075,"model. For further evidence, we present additional experimental results on widely-used arithmetic
297"
ARITHMETIC REASONING,0.5984962406015037,"reasoning datasets in Appendix D.2, which reinforce the benefits of using Neural Comprehension
298"
ARITHMETIC REASONING,0.6,"over the vanilla CoT model.
299"
ARITHMETIC REASONING,0.6015037593984962,"In comparison to language models employing external tools like PAL [Gao et al., 2022], our findings
300"
ARITHMETIC REASONING,0.6030075187969924,"suggest that generating accurate code for the less code-trained GLM-130B model might be challenging
301"
ARITHMETIC REASONING,0.6045112781954888,"for PAL, resulting in performance levels inferior to those of the vanilla CoT. This outcome indicates
302"
ARITHMETIC REASONING,0.606015037593985,"that language models offer greater flexibility, whereas external tools may have difficulties in more
303"
ARITHMETIC REASONING,0.6075187969924812,"complex or unique situations. The integration of compiled neural networks appears to be a more
304"
ARITHMETIC REASONING,0.6090225563909775,"promising approach, as evidenced by the performance improvements observed in our experiments.
305"
ARITHMETIC REASONING,0.6105263157894737,"Specifically, when language models encounter intricate arithmetic tasks that involve nested operations
306"
ARITHMETIC REASONING,0.6120300751879699,"or multi-step calculations, the integrated CoNNs can efficiently handle these operations, allowing the
307"
ARITHMETIC REASONING,0.6135338345864662,"language model to focus on higher-level reasoning. In contrast, the use of external tools often requires
308"
ARITHMETIC REASONING,0.6150375939849624,"explicit coding and may not generalize effectively to more complicated scenarios. In conclusion, our
309"
ARITHMETIC REASONING,0.6165413533834586,"results demonstrate that incorporating compiled neural networks into language models provides a
310"
ARITHMETIC REASONING,0.6180451127819548,"more robust and versatile solution for arithmetic reasoning and related challenges, underlining the
311"
ARITHMETIC REASONING,0.6195488721804512,"superiority of this approach over external tools such as PAL.
312"
ARITHMETIC REASONING,0.6210526315789474,"5.4
Ablation and Analyses: Module Combination for Neural Comprehension
313"
ARITHMETIC REASONING,0.6225563909774436,"Efficiently deploying multiple CoNNs is crucial for achieving exceptional Neural Comprehension
314"
ARITHMETIC REASONING,0.6240601503759399,"performance. As depicted in Figure 4, the amalgamation of distinct CoNNs, tailored for both symbolic
315"
ARITHMETIC REASONING,0.6255639097744361,"� +  1 �
� +  � �"
ARITHMETIC REASONING,0.6270676691729323,"A Pre-trained Model with a CoNN
A Pre-trained Model with some CoNNs"
ARITHMETIC REASONING,0.6285714285714286,(Increase from left to right)
ARITHMETIC REASONING,0.6300751879699248,"T5-small:60M
T5-base:220M
T5-large:770M
0 20 40 60 80 100"
ARITHMETIC REASONING,0.631578947368421,Solve Rate (%)
ARITHMETIC REASONING,0.6330827067669172,Last Letter Concatenation 5.2 43.8 53.5 13.7 53.8 82.0 11.0 48.7 74.3 5.2 43.8 53.5 5.2 43.8 53.5 5.2 43.8 53.5
ARITHMETIC REASONING,0.6345864661654136,"Vanilla Fine-tune
(Correlated) Last-Word
(Correlated) Copy-Letter
(Uncorrelated) Parity
(Uncorrelated) Add
(Uncorrelated) Sub"
ARITHMETIC REASONING,0.6360902255639098,"T5-small:60M
T5-base:220M
T5-large:770M
0 20 40 60 80 100"
ARITHMETIC REASONING,0.637593984962406,Solve Rate (%) 5.2 43.8 53.5 13.7 53.8
ARITHMETIC REASONING,0.6390977443609023,"82.0
88.9"
ARITHMETIC REASONING,0.6406015037593985,"96.8
99.3 88.9"
ARITHMETIC REASONING,0.6421052631578947,"96.8
99.3 88.9"
ARITHMETIC REASONING,0.643609022556391,"96.8
99.3 88.9"
ARITHMETIC REASONING,0.6451127819548872,"96.8
99.3"
ARITHMETIC REASONING,0.6466165413533834,"GPT3.5:175B
GPT3:175B
GLM:130B
0 20 40 60 80 100"
ARITHMETIC REASONING,0.6481203007518797,Solve Rate (%)
-DISTS OF ADDSUB,0.649624060150376,"20-Dists of AddSub 6.9 0
0 44.9 19.5 5.5 25.4 17.4"
-DISTS OF ADDSUB,0.6511278195488722,"2.1
6.9"
-DISTS OF ADDSUB,0.6526315789473685,"0
0
6.9"
-DISTS OF ADDSUB,0.6541353383458647,"0
0
6.9 0
0"
-DISTS OF ADDSUB,0.6556390977443609,"Vanilla CoT
(Correlated) Add
(Correlated) Sub
(Uncorrelated) Parity
(Uncorrelated) Last-Word
(Uncorrelated) Copy-Letter"
-DISTS OF ADDSUB,0.6571428571428571,"GPT3.5:175B
GPT3:175B
GLM:130B
0 20 40 60 80 100 6.9 0
0 44.9 19.5 5.5 64.4 36.9 7.6 64.4 36.9 7.6 64.4 36.9 7.6 64.4 36.9 7.6"
-DISTS OF ADDSUB,0.6586466165413534,"Figure 6: In Neural Comprehension framework, the performance of multiple different module combination is
demonstrated. The left side shows the effect of combining a pre-trained language model with a CoNN, while
the right side shows the impact of combining a language model with multiple CoNNs. For different tasks, we
categorize CoNNs as Correlated (green) and Uncorrelated (red), indicating whether the CoNN is related to the
current task or not."
-DISTS OF ADDSUB,0.6601503759398496,"and arithmetic reasoning tasks within the language model framework, can lead to remarkable benefits.
316"
-DISTS OF ADDSUB,0.6616541353383458,"It is observed that integrating pertinent CoNNs bolsters the performance of the initial language model,
317"
-DISTS OF ADDSUB,0.6631578947368421,"whereas the inclusion of unrelated language models rarely causes detrimental effects, regardless of
318"
-DISTS OF ADDSUB,0.6646616541353384,"whether single or multiple CoNNs are combined.
319"
-DISTS OF ADDSUB,0.6661654135338346,"This can be ascribed to the refined design of the Neural Comprehension framework, which ensures
320"
-DISTS OF ADDSUB,0.6676691729323309,"the precise execution of assigned tasks by CoNNs without interference from irrelevant modules. Each
321"
-DISTS OF ADDSUB,0.6691729323308271,"CoNN module is adept at generating the appropriate output when needed, thereby preventing the
322"
-DISTS OF ADDSUB,0.6706766917293233,"emergence of erroneous results from unrelated components. Importantly, as seen in Appendix B.3,
323"
-DISTS OF ADDSUB,0.6721804511278195,"the parameter count for each CoNN module ranges from 1/1000 to 1/1000000 of that for GPT-3,
324"
-DISTS OF ADDSUB,0.6736842105263158,"and the experiments in Appendix D.3 show that the inference latency in the neural understanding
325"
-DISTS OF ADDSUB,0.675187969924812,"framework only increases by 1%-3% compared to Vanilla.
326"
-DISTS OF ADDSUB,0.6766917293233082,"This observation underscores the remarkable scalability of the Neural Comprehension framework,
327"
-DISTS OF ADDSUB,0.6781954887218045,"which possesses the capability to not only accommodate existing knowledge concepts but also
328"
-DISTS OF ADDSUB,0.6796992481203008,"assimilate novel ones as the number of CoNNs expands. Theoretically, the integration of tens of
329"
-DISTS OF ADDSUB,0.681203007518797,"thousands of CoNN modules within language models holds the potential to foster a comprehensive
330"
-DISTS OF ADDSUB,0.6827067669172933,"understanding of concepts.
331"
CONCLUSION,0.6842105263157895,"6
Conclusion
332"
CONCLUSION,0.6857142857142857,"We have observed that pretrained language models lack an intrinsic comprehension of rule-based
333"
CONCLUSION,0.687218045112782,"concepts and explored how Neural Comprehension can integrate compiled neural networks into the
334"
CONCLUSION,0.6887218045112782,"language model framework in a simple and generic manner. We demonstrated the superiority of our
335"
CONCLUSION,0.6902255639097744,"approach over existing learning-based method, Without external tools, our approach enables language
336"
CONCLUSION,0.6917293233082706,"models to perform nearly perfect symbolic operations and can be applied to more realistic arithmetic
337"
CONCLUSION,0.6932330827067669,"reasoning tasks.
338"
CONCLUSION,0.6947368421052632,"Our study opens new avenues for language models, such as the investigation of more complex CoNNs
339"
CONCLUSION,0.6962406015037594,"related to higher-order abstract reasoning, the development of more advanced gating mechanisms for
340"
CONCLUSION,0.6977443609022557,"smoother integration, and the exploration of other domains in which Neural Comprehension could
341"
CONCLUSION,0.6992481203007519,"exhibit significant advantages. Furthermore, our framework provides a foundation for future work on
342"
CONCLUSION,0.7007518796992481,"unifying both implicit and explicit learning in language models and facilitating the seamless.
343"
REFERENCES,0.7022556390977444,"References
344"
REFERENCES,0.7037593984962406,"E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context
345"
REFERENCES,0.7052631578947368,"learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.
346"
REFERENCES,0.706766917293233,"C. Anil, Y. Wu, A. J. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari,
347"
REFERENCES,0.7082706766917293,"E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. In A. H.
348"
REFERENCES,0.7097744360902256,"Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing
349"
REFERENCES,0.7112781954887218,"Systems, 2022. URL https://openreview.net/forum?id=zSkYVeX7bC4.
350"
REFERENCES,0.7127819548872181,"P. Arkil, B. Satwik, and G. Navin. Are nlp models really able to solve simple math word problems?
351"
REFERENCES,0.7142857142857143,"2021.
352"
REFERENCES,0.7157894736842105,"T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
353"
REFERENCES,0.7172932330827068,"G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
354"
REFERENCES,0.718796992481203,"processing systems, 33:1877–1901, 2020.
355"
REFERENCES,0.7203007518796992,"S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,
356"
REFERENCES,0.7218045112781954,"S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general
357"
REFERENCES,0.7233082706766917,"intelligence: Early experiments with gpt-4, 2023.
358"
REFERENCES,0.724812030075188,"W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling
359"
REFERENCES,0.7263157894736842,"computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588,
360"
REFERENCES,0.7278195488721805,"2022.
361"
REFERENCES,0.7293233082706767,"A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
362"
REFERENCES,0.7308270676691729,"C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
363"
REFERENCES,0.7323308270676692,"arXiv:2204.02311, 2022.
364"
REFERENCES,0.7338345864661654,"K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training
365"
REFERENCES,0.7353383458646616,"verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
366"
REFERENCES,0.7368421052631579,"X. Daull, P. Bellot, E. Bruno, V. Martin, and E. Murisasco. Complex qa and language models hybrid
367"
REFERENCES,0.7383458646616541,"architectures, survey. arXiv preprint arXiv:2302.09051, 2023.
368"
REFERENCES,0.7398496240601504,"Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey for in-context
369"
REFERENCES,0.7413533834586467,"learning. arXiv preprint arXiv:2301.00234, 2022.
370"
REFERENCES,0.7428571428571429,"A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal, M. Baines, O. Celebi, G. Wenzek,
371"
REFERENCES,0.7443609022556391,"V. Chaudhary, et al. Beyond english-centric multilingual machine translation. The Journal of
372"
REFERENCES,0.7458646616541353,"Machine Learning Research, 22(1):4839–4886, 2021.
373"
REFERENCES,0.7473684210526316,"I. Fujisawa and R. Kanai. Logical tasks for measuring extrapolation and rule comprehension. arXiv
374"
REFERENCES,0.7488721804511278,"preprint arXiv:2211.07727, 2022.
375"
REFERENCES,0.750375939849624,"L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided
376"
REFERENCES,0.7518796992481203,"language models. arXiv preprint arXiv:2211.10435, 2022.
377"
REFERENCES,0.7533834586466165,"M. Geva, A. Gupta, and J. Berant. Injecting numerical reasoning skills into language models. arXiv
378"
REFERENCES,0.7548872180451128,"preprint arXiv:2004.04487, 2020.
379"
REFERENCES,0.7563909774436091,"A. Giannou, S. Rajput, J. yong Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers
380"
REFERENCES,0.7578947368421053,"as programmable computers, 2023.
381"
REFERENCES,0.7593984962406015,"M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. Learning to solve arithmetic word
382"
REFERENCES,0.7609022556390977,"problems with verb categorization. empirical methods in natural language processing, 2014.
383"
REFERENCES,0.762406015037594,"T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot rea-
384"
REFERENCES,0.7639097744360902,"soners. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information
385"
REFERENCES,0.7654135338345864,"Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf.
386"
REFERENCES,0.7669172932330827,"R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni, and S. D. Ang. Parsing algebraic
387"
REFERENCES,0.7684210526315789,"word problems into equations. Transactions of the Association for Computational Linguistics,
388"
REFERENCES,0.7699248120300752,"2015.
389"
REFERENCES,0.7714285714285715,"M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettle-
390"
REFERENCES,0.7729323308270677,"moyer.
Bart: Denoising sequence-to-sequence pre-training for natural language generation,
391"
REFERENCES,0.7744360902255639,"translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.
392"
REFERENCES,0.7759398496240602,"A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,
393"
REFERENCES,0.7774436090225564,"I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models.
394"
REFERENCES,0.7789473684210526,"arXiv preprint arXiv:2206.14858, 2022.
395"
REFERENCES,0.7804511278195488,"B. Li, E. Chen, H. Liu, Y. Weng, B. Sun, S. Li, Y. Bai, and M. Hu. More but correct: Generating
396"
REFERENCES,0.7819548872180451,"diversified and entity-revised medical response. arXiv e-prints, pages arXiv–2108, 2021a.
397"
REFERENCES,0.7834586466165413,"B. Li, Y. Weng, B. Sun, and S. Li. A multi-tasking and multi-stage chinese minority pre-trained
398"
REFERENCES,0.7849624060150376,"language model. In T. Xiao and J. Pino, editors, Machine Translation, pages 93–105, Singapore,
399"
REFERENCES,0.7864661654135339,"2022a. Springer Nature Singapore. ISBN 978-981-19-7960-6.
400"
REFERENCES,0.7879699248120301,"J. Li, T. Tang, W. X. Zhao, and J.-R. Wen. Pretrained language models for text generation: A survey,
401"
REFERENCES,0.7894736842105263,"2021b.
402"
REFERENCES,0.7909774436090226,"Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
403"
REFERENCES,0.7924812030075188,"A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):
404"
REFERENCES,0.793984962406015,"1092–1097, 2022b.
405"
REFERENCES,0.7954887218045112,"D. Lindner, J. Kramár, M. Rahtz, T. McGrath, and V. Mikulik. Tracr: Compiled transformers as a
406"
REFERENCES,0.7969924812030075,"laboratory for interpretability. arXiv preprint arXiv:2301.05062, 2023.
407"
REFERENCES,0.7984962406015037,"G. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick,
408"
REFERENCES,0.8,"J. Dwivedi-Yu, A. Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint
409"
REFERENCES,0.8015037593984963,"arXiv:2302.07842, 2023.
410"
REFERENCES,0.8030075187969925,"J. Moore. Language models understand us, poorly. arXiv preprint arXiv:2210.10684, 2022.
411"
REFERENCES,0.8045112781954887,"E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen:
412"
REFERENCES,0.806015037593985,"An open large language model for code with multi-turn program synthesis.
arXiv preprint
413"
REFERENCES,0.8075187969924812,"arXiv:2203.13474, 2022.
414"
REFERENCES,0.8090225563909774,"M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan,
415"
REFERENCES,0.8105263157894737,"A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate
416"
REFERENCES,0.8120300751879699,"computation with language models. arXiv preprint arXiv:2112.00114, 2021.
417"
REFERENCES,0.8135338345864662,"L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
418"
REFERENCES,0.8150375939849624,"A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,
419"
REFERENCES,0.8165413533834587,"J. Leike, and R. Lowe. Training language models to follow instructions with human feedback.
420"
REFERENCES,0.8180451127819549,"2022.
421"
REFERENCES,0.8195488721804511,"E. Perez, D. Kiela, and K. Cho. True few-shot learning with language models. Advances in neural
422"
REFERENCES,0.8210526315789474,"information processing systems, 34:11054–11070, 2021.
423"
REFERENCES,0.8225563909774436,"J. Qian, H. Wang, Z. Li, S. Li, and X. Yan. Limitations of language models in arithmetic and symbolic
424"
REFERENCES,0.8240601503759398,"induction. arXiv preprint arXiv:2208.05051, 2022.
425"
REFERENCES,0.825563909774436,"C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.
426"
REFERENCES,0.8270676691729323,"Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of
427"
REFERENCES,0.8285714285714286,"Machine Learning Research, 21(1):5485–5551, 2020.
428"
REFERENCES,0.8300751879699249,"Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies on
429"
REFERENCES,0.8315789473684211,"few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
430"
REFERENCES,0.8330827067669173,"S. Roy and D. Roth. Solving general arithmetic word problems. arXiv: Computation and Language,
431"
REFERENCES,0.8345864661654135,"2016.
432"
REFERENCES,0.8360902255639098,"K. Sarker, L. Zhou, A. Eberhart, and P. Hitzler. Neuro-symbolic artificial intelligence: Current trends.
433"
REFERENCES,0.837593984962406,"arXiv: Artificial Intelligence, 2021.
434"
REFERENCES,0.8390977443609022,"T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon,
435"
REFERENCES,0.8406015037593985,"M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv
436"
REFERENCES,0.8421052631578947,"preprint arXiv:2211.05100, 2022.
437"
REFERENCES,0.843609022556391,"T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and
438"
REFERENCES,0.8451127819548873,"T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint
439"
REFERENCES,0.8466165413533835,"arXiv:2302.04761, 2023.
440"
REFERENCES,0.8481203007518797,"N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously
441"
REFERENCES,0.849624060150376,"large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017.
442"
REFERENCES,0.8511278195488722,"URL http://arxiv.org/abs/1701.06538.
443"
REFERENCES,0.8526315789473684,"H. Shindo, D. S. Dhami, and K. Kersting. Neuro-symbolic forward reasoning. arXiv preprint
444"
REFERENCES,0.8541353383458646,"arXiv:2110.09383, 2021.
445"
REFERENCES,0.8556390977443609,"A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. San-
446"
REFERENCES,0.8571428571428571,"toro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray,
447"
REFERENCES,0.8586466165413534,"A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain,
448"
REFERENCES,0.8601503759398497,"A. Askell, A. Dsouza, A. Slone, A. Rahane, A. S. Iyer, A. Andreassen, A. Madotto, A. Santilli,
449"
REFERENCES,0.8616541353383459,"A. Stuhlmüller, A. Dai, A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta,
450"
REFERENCES,0.8631578947368421,"A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubara-
451"
REFERENCES,0.8646616541353384,"jan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karaka{¸s}, B. R. Roberts,
452"
REFERENCES,0.8661654135338346,"B. S. Loe, B. Zoph, B. Bojanowski, B. Özyurt, B. Hedayatnia, B. Neyshabur, B. Inden, B. Stein,
453"
REFERENCES,0.8676691729323308,"B. Ekmekci, B. Y. Lin, B. Howald, C. Diao, C. Dour, C. Stinson, C. Argueta, C. F. Ramírez,
454"
REFERENCES,0.869172932330827,"C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch, C. Waites, C. Voigt, C. D.
455"
REFERENCES,0.8706766917293233,"Manning, C. Potts, C. Ramirez, C. E. Rivera, C. Siro, C. Raffel, C. Ashcraft, C. Garbacea, D. Sileo,
456"
REFERENCES,0.8721804511278195,"D. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Freeman, D. Khashabi, D. Levy, D. M.
457"
REFERENCES,0.8736842105263158,"González, D. Perszyk, D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard,
458"
REFERENCES,0.8751879699248121,"D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hup-
459"
REFERENCES,0.8766917293233083,"kes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, E. Shutova, E. D. Cubuk, E. Segal,
460"
REFERENCES,0.8781954887218045,"E. Hagerman, E. Barnes, E. Donoway, E. Pavlick, E. Rodola, E. Lam, E. Chu, E. Tang, E. Er-
461"
REFERENCES,0.8796992481203008,"dem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak, E. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia,
462"
REFERENCES,0.881203007518797,"F. Siar, F. Martínez-Plumed, F. Happé, F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo,
463"
REFERENCES,0.8827067669172932,"G. Kruszewski, G. Parascandolo, G. Mariani, G. Wang, G. Jaimovitch-López, G. Betz, G. Gur-Ari,
464"
REFERENCES,0.8842105263157894,"H. Galijasevic, H. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. Shevlin, H. Schütze,
465"
REFERENCES,0.8857142857142857,"H. Yakura, H. Zhang, H. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton,
466"
REFERENCES,0.8872180451127819,"J. Lee, J. F. Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Koco´n, J. Thompson, J. Kaplan,
467"
REFERENCES,0.8887218045112782,"J. Radom, J. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh,
468"
REFERENCES,0.8902255639097745,"J. Kim, J. Taal, J. Engel, J. Alabi, J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U. Balis,
469"
REFERENCES,0.8917293233082707,"J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Jones, J. B. Tenenbaum, J. S.
470"
REFERENCES,0.8932330827067669,"Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan, K. Ignatyeva, K. Markert,
471"
REFERENCES,0.8947368421052632,"K. D. Dhole, K. Gimpel, K. Omondi, K. Mathewson, K. Chiafullo, K. Shkaruta, K. Shridhar,
472"
REFERENCES,0.8962406015037594,"K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-
473"
REFERENCES,0.8977443609022556,"Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. O. Colón, L. Metz,
474"
REFERENCES,0.8992481203007519,"L. K. {¸S}enel, M. Bosma, M. Sap, M. ter Hoeve, M. Farooqi, M. Faruqui, M. Mazeika, M. Baturan,
475"
REFERENCES,0.9007518796992481,"M. Marelli, M. Maru, M. J. R. Quintana, M. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast,
476"
REFERENCES,0.9022556390977443,"M. L. Leavitt, M. Hagen, M. Schubert, M. O. Baitemirova, M. Arnaud, M. McElrath, M. A.
477"
REFERENCES,0.9037593984962407,"Yee, M. Cohen, M. Gu, M. Ivanitskiy, M. Starritt, M. Strube, M. Sw{˛e}drowski, M. Bevilacqua,
478"
REFERENCES,0.9052631578947369,"M. Yasunaga, M. Kale, M. Cain, M. Xu, M. Suzgun, M. Tiwari, M. Bansal, M. Aminnaseri,
479"
REFERENCES,0.9067669172932331,"M. Geva, M. Gheini, M. V. T, N. Peng, N. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts,
480"
REFERENCES,0.9082706766917293,"N. Doiron, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S. Iyer, N. Constant, N. Fiedel,
481"
REFERENCES,0.9097744360902256,"N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. A. M. Casares, P. Doshi, P. Fung,
482"
REFERENCES,0.9112781954887218,"P. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. Chang, P. Eckersley, P. M. Htut,
483"
REFERENCES,0.912781954887218,"P. Hwang, P. Mi{ł}kowski, P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade,
484"
REFERENCES,0.9142857142857143,"R. E. Rudolph, R. Gabriel, R. Habacker, R. R. Delgado, R. Millière, R. Garg, R. Barnes, R. A.
485"
REFERENCES,0.9157894736842105,"Saurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. LeBras,
486"
REFERENCES,0.9172932330827067,"R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang,
487"
REFERENCES,0.9187969924812031,"S. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. R.
488"
REFERENCES,0.9203007518796993,"Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey,
489"
REFERENCES,0.9218045112781955,"S. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi,
490"
REFERENCES,0.9233082706766917,"S. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, S. S. Debnath, S. Shakeri,
491"
REFERENCES,0.924812030075188,"S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar, S. Dehaene,
492"
REFERENCES,0.9263157894736842,"S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. T. Piantadosi, S. M. Shieber, S. Misherghi,
493"
REFERENCES,0.9278195488721804,"S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. Ali, T. Hashimoto, T.-L. Wu,
494"
REFERENCES,0.9293233082706767,"T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick, T. Kornev, T. Telleen-Lawton,
495"
REFERENCES,0.9308270676691729,"T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot, T. Shultz, U. Shaham, V. Misra, V. Dem-
496"
REFERENCES,0.9323308270676691,"berg, V. Nyamai, V. Raunak, V. Ramasesh, V. U. Prabhu, V. Padmakumar, V. Srikumar, W. Fedus,
497"
REFERENCES,0.9338345864661655,"W. Saunders, W. Zhang, W. Vossen, X. Ren, X. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh,
498"
REFERENCES,0.9353383458646617,"Y. Lakretz, Y. Song, Y. Bahri, Y. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou,
499"
REFERENCES,0.9368421052631579,"Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J. Wang, Z. Wang, and Z. Wu. Beyond the imitation game:
500"
REFERENCES,0.9383458646616541,"Quantifying and extrapolating the capabilities of language models. 2022.
501"
REFERENCES,0.9398496240601504,"M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H.
502"
REFERENCES,0.9413533834586466,"Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them.
503"
REFERENCES,0.9428571428571428,"arXiv preprint arXiv:2210.09261, 2022.
504"
REFERENCES,0.9443609022556391,"J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,
505"
REFERENCES,0.9458646616541353,"D. Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682,
506"
REFERENCES,0.9473684210526315,"2022a.
507"
REFERENCES,0.9488721804511279,"J. Wei, Y. Tay, and Q. V. Le. Inverse scaling can become u-shaped. 2022b.
508"
REFERENCES,0.9503759398496241,"J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting
509"
REFERENCES,0.9518796992481203,"elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022c.
510"
REFERENCES,0.9533834586466166,"J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-
511"
REFERENCES,0.9548872180451128,"thought prompting elicits reasoning in large language models. In Advances in Neural Information
512"
REFERENCES,0.956390977443609,"Processing Systems, 2022d.
513"
REFERENCES,0.9578947368421052,"G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In International Conference on
514"
REFERENCES,0.9593984962406015,"Machine Learning, pages 11080–11090. PMLR, 2021.
515"
REFERENCES,0.9609022556390977,"S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with
516"
REFERENCES,0.9624060150375939,"unlikelihood training. In International Conference on Learning Representations.
517"
REFERENCES,0.9639097744360903,"Y. Weng, M. Zhu, S. He, K. Liu, and J. Zhao. Large language models are reasoners with self-
518"
REFERENCES,0.9654135338345865,"verification. arXiv preprint arXiv:2212.09561, 2022.
519"
REFERENCES,0.9669172932330827,"Y. Wu, A. X. Jiang, J. Ba, and R. Grosse. Int: An inequality benchmark for evaluating generalization
520"
REFERENCES,0.968421052631579,"in theorem proving. arXiv: Artificial Intelligence, 2020.
521"
REFERENCES,0.9699248120300752,"Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy. Autoformalization
522"
REFERENCES,0.9714285714285714,"with large language models. 2022.
523"
REFERENCES,0.9729323308270676,"F. Xia, B. Li, Y. Weng, S. He, K. Liu, B. Sun, S. Li, and J. Zhao. Medconqa: Medical conversational
524"
REFERENCES,0.9744360902255639,"question answering system based on knowledge graphs. In Proceedings of the The 2022 Conference
525"
REFERENCES,0.9759398496240601,"on Empirical Methods in Natural Language Processing: System Demonstrations, pages 148–158,
526"
REFERENCES,0.9774436090225563,"2022.
527"
REFERENCES,0.9789473684210527,"K. Yang and J. Deng. Learning symbolic rules for reasoning in quasi-natural language. arXiv preprint
528"
REFERENCES,0.9804511278195489,"arXiv:2111.12038, 2021.
529"
REFERENCES,0.9819548872180451,"S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,
530"
REFERENCES,0.9834586466165414,"et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
531"
REFERENCES,0.9849624060150376,"2022a.
532"
REFERENCES,0.9864661654135338,"Z. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language
533"
REFERENCES,0.98796992481203,"models. arXiv preprint arXiv:2210.03493, 2022b.
534"
REFERENCES,0.9894736842105263,"D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and
535"
REFERENCES,0.9909774436090225,"E. Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv
536"
REFERENCES,0.9924812030075187,"preprint arXiv:2205.10625, 2022a.
537"
REFERENCES,0.9939849624060151,"H. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur, and H. Sedghi. Teaching algorithmic
538"
REFERENCES,0.9954887218045113,"reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022b.
539"
REFERENCES,0.9969924812030075,"M. Zhu, Y. Weng, S. He, K. Liu, and J. Zhao. Reasonchainqa: Text-based complex question answering
540"
REFERENCES,0.9984962406015038,"with explainable evidence chains. arXiv preprint arXiv:2210.08763, 2022.
541"
