Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037735849056603774,"We investigate the possibility of using the embeddings produced by a lightweight
network more effectively with a nonlinear classiﬁcation layer. Although conven-
tional deep networks use an abundance of nonlinearity for representation (embed-
ding) learning, they almost universally use a linear classiﬁer on the learned embed-
dings. This could be suboptimal for a network with a limited-capacity backbone
since better nonlinear classiﬁers could exist in the same embedding vector space.
We advocate a nonlinear kernelized classiﬁcation layer for deep networks to tackle
this problem. We theoretically show that our classiﬁcation layer optimizes over
all possible radial kernel functions on the space of embeddings to learn an optimal
nonlinear classiﬁer. We then demonstrate the usefulness of this layer in learn-
ing more model-efﬁcient classiﬁers in a number of computer vision and natural
language processing tasks."
INTRODUCTION,0.007547169811320755,"1
INTRODUCTION"
INTRODUCTION,0.011320754716981131,"A traditional classiﬁcation deep network consists of two parts: a representation learner that maps
an input to a vector-valued representation called the embedding, and a classiﬁer that classiﬁes this
embedding into the correct class. For example, in the text classiﬁcation setting, the input text may
be sent through a transformer encoder network with CLS-pooling (the representation learner) to
obtain an embedding vector for the text (Devlin et al., 2018). A fully-connected layer (the classiﬁer)
is then operated on this embedding. A classiﬁer learned in a usual fully connected layer with the
softmax loss is linear in the space of embeddings. Therefore, the representation learner has to learn
linearly-separable embeddings to do well in the classiﬁcation task."
INTRODUCTION,0.01509433962264151,"When the representation learner backbone is unable to learn linearly separable embeddings to satis-
factorily solve a given classiﬁcation task, the usual ﬁx is to use a deeper and/or wider backbone to
generate better embeddings (Turc et al., 2019; He et al., 2016). However, bigger backbones demand
more resources in terms of compute, memory, and model storage during both training and infer-
ence. Therefore, it is natural to ask whether it is instead possible to use embeddings produced by a
capacity-limited network more effectively by using a more sophisticated classiﬁcation layer."
INTRODUCTION,0.018867924528301886,"In this work we address this problem by proposing a nonlinear, kernelized classiﬁcation layer. This
classiﬁcation layer ﬁnds an optimal nonlinear classiﬁer for the embeddings by mapping them into a
Reproducing Kernel Hilbert Space (RKHS) that optimally separates them into different classes. We
borrow the key idea from kernel methods in the classic machine learning literature (Cortes & Vapnik,
1995; Sch¨olkopf & Smola, 2002): instead of running a classiﬁer directly on the embeddings, they
are ﬁrst mapped to much higher dimensional vectors in an RKHS using a positive deﬁnite kernel
function. A linear classiﬁer is then run on this high-dimensional RKHS. Since the dimensionality
of the embeddings is dramatically increased via this mapping, a linear classiﬁer in the RKHS cor-
responds to a powerful nonlinear classiﬁer in the original embedding space. Such a classiﬁer can
therefore utilize even linearly inseparable embeddings to satisfactorily solve a classiﬁcation task as
demonstrated in Figure 1."
INTRODUCTION,0.022641509433962263,"A common issue with traditional kernel methods is the choice of the kernel function used to obtain
the RKHS mapping. While a collection of well-known kernels such as the linear kernel, polynomial
kernels, and the Gaussian RBF kernel is available, it is often unclear which kernel would work best
for a given problem. We tackle this issue by sweeping over all possible kernel functions within the
deep network itself to ﬁnd the optimal one via backpropagation and stochastic gradient descent."
INTRODUCTION,0.026415094339622643,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03018867924528302,"(a) Training data
(b) Softmax classiﬁer
(b) Kernelized classiﬁer
Figure 1: Beneﬁts of kernelized classiﬁcation. In the second and third images, regions identiﬁed by each
classiﬁer are shown in blue and orange colors. Note that the usual softmax classiﬁer can only separate cap-like
“linear” regions on the sphere, whereas our kernelized classiﬁer can do more complex nonlinear classiﬁcation
thanks to the higher dimensional RKHS embedding of the sphere. See § 6.1 for the experiment details."
INTRODUCTION,0.033962264150943396,"There is a wealth of literature on making deep learning models more efﬁcient by using techniques
such as pruning, quantization, low-rank factorization, and distillation (Cheng et al., 2017; Deng
et al., 2020). We approach the model efﬁciency from an orthogonal angle by asking whether it
is possible to utilize embeddings produced by a given representation learner more effectively by
doing nonlinear classiﬁcation. Therefore, our approach is complementary to these existing model
compression techniques."
INTRODUCTION,0.03773584905660377,"In summary, we propose a method for model-efﬁcient classiﬁcation in a deep network with three
contributions: (i) we introduce a kernelized classiﬁcation layer with built-in kernel learning that can
utilize a given representation learning model more effectively. (ii) we theoretically establish that it
is possible to optimize, within the deep network itself, over all possible kernel functions we care
about to ﬁnd the best RKHS that optimally separates embeddings. (iii) we empirically show that
the kernelized classiﬁcation layer is a viable alternative to using a larger backbone to improve the
classiﬁcation accuracy in a number of computer vision and natural language processing tasks.
2
RELATED WORK"
INTRODUCTION,0.04150943396226415,"There have been several explorations of loss functions other than the usual softmax crossentropy
loss in CNN settings, specially in the open-set classiﬁcation setting (Wen et al., 2019; Cevikalp &
Saglamlar, 2021; Deng et al., 2019; Wang et al., 2018). An example is the Center Loss (Wen et al.,
2019), which encourages low intra-class variance in the feature vectors. Other methods such as
Deng et al. (2019) achieve higher performance by leaving additional margins in the softmax loss.
Our work differs from these since we work in the closed-set classiﬁcation setting and employ an
automatically learned kernel to obtain nonlinear classiﬁcation boundaries."
INTRODUCTION,0.045283018867924525,"Second order pooling methods (Lin et al., 2015; Li et al., 2017; Wang et al., 2020) propose a way
to perform nonlinear classiﬁcation in the embedding space. In has been shown that second order
pooling is equivalent to using a second-degree polynomial kernel (Gao et al., 2016; Cai et al., 2017).
Cui et al. (2017) extended second-order pooling to higher orders while learning the coefﬁcients of
higher order interactions. Their method requires explicit calculation of feature maps, which they
tackle using Fast Fourier Transforms. Mahmoudi et al. (2020), use the kernel trick in the dense layer
along with the polynomial kernel. Our work differs from the above works in that we never calculate
explicit feature maps and we theoretically show that our method learns over the space of all possible
positive deﬁnite kernels on the hyper-sphere, which includes all polynomial kernels."
INTRODUCTION,0.04905660377358491,"Some methods focus on extending the linear convolution in CNNs to a nonlinear operation. Con-
volutional kernel networks (Mairal et al., 2014) provide a kernel approximation to interpret convo-
lutions. Zoumpourlis et al. (2017) used Volterra series approximations to extend convolutions to a
nonlinear operation. Wang et al. (2019b) proposed a kernelized version of the convolution opera-
tion and demonstrated that it can learn more complicated features than the usual convolution. Our
work differs from theirs in a number of ways: Some kernels used in their work, such as the Lp-norm
kernels, are not positive deﬁnite (Berg et al., 1984) and therefore do not represent a valid RKHS map-
ping (Aronszajn, 1950). In contrast, we strictly work with positive deﬁnite kernels, which represent
valid mappings to an RKHS. Furthermore, learning of hyperparameters of pre-deﬁned kernels advo-
cated in their work is principally different from the kernel learning method presented in this paper –
we theoretically show that our method optimizes over the space of all radial positive deﬁnite kernels
on the unit sphere to ﬁnd the best kernel, instead of limiting the optimization to the hyperparameters
of pre-deﬁned kernels."
INTRODUCTION,0.052830188679245285,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05660377358490566,"Prior to the dominance of deep learning methods, picking the right kernel for a given problem has
been studied extensively in works such as Howley & Madden (2005); Ali & Smith-Miles (2006);
Jayasumana et al. (2014); G¨onen et al. (2011). In particular, Multiple Kernel Learning (MKL)
approaches (G¨onen et al., 2011; Varma & Ray, 2007) were popular in conjunction with SVM. Un-
fortunately, these methods scale poorly with the size of the training dataset. In this work, we au-
tomatically learn the kernel within a deep network. This not only allows automatic representation
learning, but also scales well for large training sets. Kernels have also been considered for deep
learning to reduce the memory footprint of CNNs. This was accomplished by achieving an end-
to-end training of a Fastfood kernel layer (Yang et al., 2015), which uses approximations of kernel
functions using Fastfood transforms (Le et al., 2013). Other related methods involving both kernels
and deep learning include stochastic kernel machines (Dai et al., 2015), deep SimNets (Cohen et al.,
2015), scalable deep kernels (Wilson et al., 2015), KerNET (Lauriola et al., 2020), and deep belief
network based work of Le et al. (2016)."
NONLINEAR SOFTMAX CLASSIFICATION,0.06037735849056604,"3
NONLINEAR SOFTMAX CLASSIFICATION"
NONLINEAR SOFTMAX CLASSIFICATION,0.06415094339622641,"In this section we discuss the usual classiﬁcation inside a deep network and its nonlinear extension.
Let us consider a classiﬁcation problem with a training set {(xi, yi)}N
i=1, where each xi ∈X, each
yi ∈[L]
.= {1, 2, . . . , L}, X is a nonempty set, L is the number of labels, and N is the number of
training examples. For instance, each training datum (xi, yi) can be an image with its class label."
NONLINEAR SOFTMAX CLASSIFICATION,0.06792452830188679,"A deep neural network that solves this task has two components: a representation learner and a
classiﬁer. In the case of image classiﬁcation, the representation learner consists of modules such as
convolution layers, max-pooling layers, and fully-connected layers. The classiﬁer is the last fully-
connected layer operating on the learned representations (embeddings). This layer is endowed with
a loss function during training."
NONLINEAR SOFTMAX CLASSIFICATION,0.07169811320754717,"Let r(Θ) : X →Rd denote the representation learner, where d is the dimensionality of the embed-
dings and Θ represents all the parameters in this part of the network. The classiﬁer is characterized
by a function g(Ω) : Rd →[L], where Ωdenotes all the parameters in the last layer of the net-
work. Usually, Ωconsists of weight vectors w1, w2, . . . , wL with each wj ∈Rd, and bias terms
b1, b2, . . . , bL with each bj ∈R. The function g(Ω) then takes the form:"
NONLINEAR SOFTMAX CLASSIFICATION,0.07547169811320754,"g(Ω)(z) = argmax
j
wT
j z,
(1)"
NONLINEAR SOFTMAX CLASSIFICATION,0.07924528301886792,"where z = r(Θ)(x) ∈Rd is the embedding for input the x. Note that we have dropped the additive
bias term bj, with no loss of generality, to keep the notation uncluttered. During inference, the
deep network’s class prediction ˆy∗for an input x∗is the composite of these two functions: ˆy∗=
 
g(Ω) ◦r(Θ)
(x∗)."
NONLINEAR SOFTMAX CLASSIFICATION,0.0830188679245283,"Although conceptually there are two components of the deep network, their parameters Θ and Ω
are learned jointly during training. The de facto standard way of training a classiﬁcation network is
minimizing the softmax loss applied to the classiﬁcation layer. The softmax loss is the combination
of the softmax function and the cross-entropy loss. More speciﬁcally, for a single training example
(x, y) with the embedding z = r(Θ)(x), the softmax loss is calculated as:"
NONLINEAR SOFTMAX CLASSIFICATION,0.08679245283018867,"l(y, z) = −log"
NONLINEAR SOFTMAX CLASSIFICATION,0.09056603773584905,"exp(wT
y z)
PL
j=1 exp(wT
j z) ! .
(2)"
NONLINEAR SOFTMAX CLASSIFICATION,0.09433962264150944,"Note that the classiﬁer g(Ω) trained is this manner is completely linear in Rd, the space of the
embeddings zs, as is evident from (1)."
NONLINEAR SOFTMAX CLASSIFICATION,0.09811320754716982,"From the classic knowledge in kernel methods, we are aware that more powerful nonlinear classiﬁers
on Rd can be obtained using the kernel trick. The key idea here is to ﬁrst map the embeddings zs into
a high-dimensional RKHS H and perform classiﬁcation there. Although the classiﬁcation is linear
in the high-dimensional H, it is nonlinear in the original embedding space Rd. Let φ : Rd →H
represent this RKHS embedding. Performing classiﬁcation in H is then equivalent to training the
neural network with the following modiﬁed version of the softmax loss:"
NONLINEAR SOFTMAX CLASSIFICATION,0.1018867924528302,"lnl(y, z) = −log"
NONLINEAR SOFTMAX CLASSIFICATION,0.10566037735849057,"exp
 
⟨φ(wy), φ(z)⟩H
"
NONLINEAR SOFTMAX CLASSIFICATION,0.10943396226415095,"PL
j=1 exp
 
⟨φ(wj), φ(z)⟩H
 ! ,
(3)"
NONLINEAR SOFTMAX CLASSIFICATION,0.11320754716981132,Under review as a conference paper at ICLR 2022
NONLINEAR SOFTMAX CLASSIFICATION,0.1169811320754717,"where ⟨., .⟩H denotes the inner product in the Hilbert space H. The key difference between (2) and
(3) is that the dot products between wjs and z have been replaced with the inner products between
φ(wj)s and φ(z). The more general notion of inner product is used instead of dot product because
the Hilbert space H can be inﬁnite dimensional."
NONLINEAR SOFTMAX CLASSIFICATION,0.12075471698113208,"For a network trained with this nonlinear softmax function, predictions can be obtained using a
modiﬁed version of the predictor:"
NONLINEAR SOFTMAX CLASSIFICATION,0.12452830188679245,"g(Ω)
nl (z) = argmax
j
⟨φ(wj), φ(z)⟩H .
(4)"
NONLINEAR SOFTMAX CLASSIFICATION,0.12830188679245283,"Note that the Hilbert space embeddings φ(.)s can be very-high, even inﬁnite, dimensional. There-
fore, computing and storing them can be problematic. We can use the kernel trick from classic
machine learning (Sch¨olkopf & Smola, 2002; Shawe-Taylor & Cristianini, 2004) to overcome this
problem: explicit computation of φ(.)s can be avoided by directly evaluating the inner product be-
tween them using a kernel function k : Rd × Rd →R. That is:"
NONLINEAR SOFTMAX CLASSIFICATION,0.1320754716981132,"⟨φ(w), φ(z)⟩H = k(w, z).
(5)"
NONLINEAR SOFTMAX CLASSIFICATION,0.13584905660377358,"Intuitively, mapping d-dimensional embeddings into a much higher dimensional RKHS using a
kernel would help in ﬁnding complex, nonlinear patterns in the embeddings that the usual softmax
classiﬁcation is unable to ﬁnd due to its linear nature. We therefore expect kernelized classiﬁcation
to use embeddings provided by a given representation learner more effectively."
KERNELS ON THE UNIT SPHERE,0.13962264150943396,"4
KERNELS ON THE UNIT SPHERE"
KERNELS ON THE UNIT SPHERE,0.14339622641509434,"It was shown in the previous section that, given a kernel function on the embedding space, we
can obtain a nonlinear classiﬁer in the last layer of a deep network by modifying the softmax loss
function during training and the predictor during inference. However, only positive deﬁnite kernels
allow this trick (Aronszajn, 1950; Berg et al., 1984). There are various choices for kernel functions
in the classic machine learning literature. Some popular choices include the polynomial kernel,
the Gaussian RBF kernel (squared exponential kernel), and the Laplacian kernel. Nevertheless,
it is often difﬁcult to decide the optimal kernel for a given problem. Furthermore, many of the
kernels have hyperparameters that need to be manually tuned. The generally accepted solution to
this problem in classic kernel methods is the MKL framework (G¨onen et al., 2011), where a better
kernel is learned as a linear combination of some pre-deﬁned kernels. Unfortunately, like SVM,
MKL methods do not scale well with the train set size. Furthermore, usual MKL provides no
guarantee to explore all possible kernel functions to ﬁnd the optimal one."
KERNELS ON THE UNIT SPHERE,0.1471698113207547,"In this section, we present some theoretical results that will pave the way to deﬁne a neural network
layer that can automatically learn the optimal kernel from data. By formulating kernel learning as a
neural network layer, we inherit the desirable properties of deep learning, including scalability and
automatic representation learning. Importantly, we show that our method can sweep over the entire
space of positive deﬁnite kernels applicable to our problem setting to ﬁnd the best kernel. We start
the discussion with the following deﬁnition of positive deﬁnite kernels (Berg et al., 1984)."
KERNELS ON THE UNIT SPHERE,0.1509433962264151,"Deﬁnition 4.1. Let U be a nonempty set. A function k : (U × U) →R is called a positive deﬁnite
kernel if k(u, v) = k(v, u) for all u, v ∈U and PN
j=1
PN
i=1 cicjk(ui, uj) ≥0, for all N ∈
N, {u1, . . . , uN} ⊆U and {c1, . . . , cN} ⊆R."
KERNELS ON THE UNIT SPHERE,0.15471698113207547,"Properties of positive deﬁnite kernels have been studied extensively in mathematics (Berg et al.,
1984). The following summarizes some important closure properties of this class of functions."
KERNELS ON THE UNIT SPHERE,0.15849056603773584,"Proposition 4.2. The family of all positive deﬁnite kernels on a given nonempty set forms a convex
cone that is closed under pointwise multiplication and pointwise convergence."
KERNELS ON THE UNIT SPHERE,0.16226415094339622,"Proof. To intuitively understand this result, it is helpful to recall that the geometry of the family of
the all positive deﬁnite kernels on a given nonempty set is closely related to that of the space of the
d × d symmetric positive deﬁnite matrices, which forms a convex cone. The formal proof of this
proposition can be found in Remark 3.1.11 and Theorem 3.1.12 of Berg et al. (1984)."
KERNELS ON THE UNIT SPHERE,0.1660377358490566,"To simplify the problem setting, we assume that both the embeddings zs and the weight vectors
wjs are L2-normalized. Not only this simpliﬁes the mathematics, but also it is a practice in use"
KERNELS ON THE UNIT SPHERE,0.16981132075471697,Under review as a conference paper at ICLR 2022
KERNELS ON THE UNIT SPHERE,0.17358490566037735,"for stabilizing the training of neural networks (Yi et al., 2019; Liu et al., 2017; Hoffer et al., 2018).
Due to this assumption, we are interested in positive deﬁnite kernels on the unit sphere in Rd. From
now on, we use Sn, where n = d −1, to denote this space. We also restrict our discussion to
radial kernels on Sn. Radial kernels, kernels that only depend on the distance between the two input
points, have the desirable property of translation invariance. Furthermore, all the commonly used
kernels on Sn, such as the linear kernel, the polynomial kernel, the Gaussian RBF kernel, and the
Laplacian kernel are radial kernels. The following theorem, origins of which can be traced back to
Schoenberg (1942), fully characterizes radial kernels on Sn.
Theorem 4.3. A radial kernel k : Sn × Sn →R is positive deﬁnite for any n if and only if it admits
a unique series representation of the form"
KERNELS ON THE UNIT SPHERE,0.17735849056603772,"k(u, v) = α−2J⟨u, v⟩∈{−1, 1}K+ α−1(J⟨u, v⟩= 1K−J⟨u, v⟩= −1K)+ ∞
X"
KERNELS ON THE UNIT SPHERE,0.1811320754716981,"m=0
αm ⟨u, v⟩m , (6)"
KERNELS ON THE UNIT SPHERE,0.18490566037735848,"where each αm ≥0 , P∞
m=−2 αm < ∞, and J.K depicts the Iversion bracket."
KERNELS ON THE UNIT SPHERE,0.18867924528301888,"Proof. The kernel k1 : Sn × Sn →[−1, 1] : k1(u, v) = ⟨u, v⟩is positive deﬁnite on Sn for any
n since P j
P"
KERNELS ON THE UNIT SPHERE,0.19245283018867926,"i cicj ⟨ui, uj⟩= ∥P"
KERNELS ON THE UNIT SPHERE,0.19622641509433963,"i ciui∥2 ≥0. Therefore, from the closure properties in Propo-
sition 4.2, the kernel km : (u, v) 7→⟨u, v⟩m is also positive deﬁnite for any m ∈N. Furthermore,
km is positive deﬁnite for m = 0 since P j
P"
KERNELS ON THE UNIT SPHERE,0.2,"i cicj ⟨ui, uj⟩0 = ∥P"
KERNELS ON THE UNIT SPHERE,0.2037735849056604,i ci∥2 ≥0.
KERNELS ON THE UNIT SPHERE,0.20754716981132076,"Let us now consider the two sequences of kernels sodd = k1, k3, . . . , k2m+1, . . . and seven =
k2, k4, . . . , k2m, . . . . Since −1 ≤⟨u, v⟩≤1, it is clear that sodd and seven converge pointwise
to kodd(u, v) = J⟨u, v⟩= 1K −J⟨u, v⟩= −1K and keven(u, v) = J⟨u, v⟩∈{−1, 1}K, respec-
tively. From the last closure property of Proposition 4.2, both kodd and keven are positive deﬁnite
on Sn. Invoking Proposition 4.2 again, we conclude that any ﬁnite conic combination of the kernels
keven, kodd, k0, k1, . . . is positive deﬁnite on Sn for any n. This completes the forward direction of
the proof. The proof of the converse is found in Chapter 5 of Berg et al. (1984)."
KERNELS ON THE UNIT SPHERE,0.21132075471698114,"Equipped with a complete characterization of the positive deﬁnite radial kernels on Sn, we now dis-
cuss how we can combine this result with the nonlinear softmax formulation in § 3 to automatically
learn the best kernel classiﬁer within a deep network."
THE KERNELIZED CLASSIFICATION LAYER,0.21509433962264152,"5
THE KERNELIZED CLASSIFICATION LAYER"
THE KERNELIZED CLASSIFICATION LAYER,0.2188679245283019,"We now introduce a kernelized classiﬁcation layer that acts as a drop-in replacement for the usual
softmax classiﬁcation layer in a deep network. This new layer classiﬁes embeddings in a high-
dimensional RKHS while automatically choosing the optimal positive deﬁnite kernel that enables
the mapping into the RKHS. As a result, we do not have to hand-pick a kernel or its hyperparameters.
Our nonlinear classiﬁcation layer can utilize embeddings more efﬁciently than a softmax classiﬁer,
which is linear in the embedding space. We also show that the kernelized classiﬁcation layers comes
with negligible added cost during both training and inference."
MECHANICS OF THE LAYER,0.22264150943396227,"5.1
MECHANICS OF THE LAYER
Our classiﬁcation layer is parameterized by the usual weight vectors: w1, w2, . . . , wL, and some
additional learnable coefﬁcients: α−2, α−1, . . . , αM, where M ∈N and each αm ≥0. During
training, this classiﬁer maps embeddings zs into a high-dimensional RKHS Hopt that optimally
separates embeddings belonging to different classes, and learns a linear classiﬁer in Hopt. During
inference, the classiﬁer maps embeddings of previously unseen inputs to the RKHS it learned during
training and performs classiﬁcation in that space. This is achieved by using the nonlinear softmax
loss deﬁned in (3) during training and the nonlinear predictor deﬁned in (4) during testing, with the
inner product in H given by:⟨φ(w), φ(z)⟩H = ⟨φ(w), φ(z)⟩Hopt = kopt(w, z), where kopt(., .)
is the reproducing kernel of Hopt. The optimal RKHS Hopt for a given classiﬁcation problem is
learned by ﬁnding the optimal kernel kopt during training as discussed in the following."
MECHANICS OF THE LAYER,0.22641509433962265,"Theorem 4.3 states that any positive deﬁnite radial kernel on Sn admits the series representation
shown in (6). Therefore, the optimal kernel kopt must also have such a series representation. We
approximate this series with a ﬁnite summation by cutting off the terms beyond the order M:"
MECHANICS OF THE LAYER,0.23018867924528302,"kopt(w, z) ≈α−2keven(w, z) + α−1kodd(w, z) + M
X"
MECHANICS OF THE LAYER,0.2339622641509434,"m=0
αmkm(w, z),
(7)"
MECHANICS OF THE LAYER,0.23773584905660378,Under review as a conference paper at ICLR 2022
MECHANICS OF THE LAYER,0.24150943396226415,"where, keven, kodd, k0, k1, . . . , kM have meanings deﬁned § 4 and α−2, α−1, . . . , αM ≥0. Using
Proposition 4.2 and the discussion in the proof of Theorem 4.3, one can easily verify that this ap-
proximation does not violate the positive deﬁniteness of kopt. A rigorous analysis of the accuracy
of this approximation is provided in Appendix E."
MECHANICS OF THE LAYER,0.24528301886792453,"With this, kopt is learned automatically from data by making the coefﬁcients α−2, α−1, . . . , αMs
learnable parameters of the classiﬁcation layer. Let α = [α−2, α−1, . . . , αM]T . The gradient of the
loss function with respect to α can be calculated via the backpropagation algorithm using (3) and
(7). Therefore, it can be optimized along with w1, w2, . . . , wL during the gradient descent based
optimization of the network. This procedure is equivalent to automatically ﬁnding the RKHS that
optimally separates the embeddings belonging to different classes."
MECHANICS OF THE LAYER,0.2490566037735849,"The constraint α−2, α−1, . . . , αM ≥0 in (7) can be imposed with a ReLU function (see Ap-
pendix D.5 for more discussion). As shown later in § 7.3, the exact value of M is not critical as
long as it is sufﬁciently large. This is because, as discussed in the proof of Theorem 4.3, the higher
order terms that are truncated approach either kodd or keven, both of which are already included in
the ﬁnite summation. On the other hand, if the terms beyond some order M ′ < M are not important,
the network can automatically learn to make the corresponding α coefﬁcients vanish. We observed
that M = 10 works well enough in practice and stick to this number in all our experiments."
MECHANICS OF THE LAYER,0.2528301886792453,"Importantly, the kernelized classiﬁcation layer described above can pass on the loss gradients to its
inputs. Therefore, the kernelized classiﬁcation layer is fully compatible with end-to-end training
and can act as a drop-in replacement for an existing softmax classiﬁcation layer."
ADDITIONAL COMPLEXITY,0.25660377358490566,"5.2
ADDITIONAL COMPLEXITY"
ADDITIONAL COMPLEXITY,0.26037735849056604,"Since we propose kernelized classiﬁcation as a replacement for the usual softmax classiﬁcation to
improve model efﬁciency, one might wonder about the added cost of the kernelized classiﬁcation
layer. It uses (M + 3) extra learnable parameters. During both training and inference, the added
computational complexity is O(M + d) per datum, assuming a commonly-available constant-time
operation for taking powers. Additional memory footprint is O(M) during training to account for
cached gradients. Note that M = 10 and d ranges from 64 to 768 in our experiments. Therefore, the
kernelized classiﬁcation comes with a negligible added cost over the softmax classiﬁcation in terms
of compute, memory, and trained model storage."
EXPERIMENTS,0.2641509433962264,"6
EXPERIMENTS"
EXPERIMENTS,0.2679245283018868,"We now present experimental evaluation of our method. For all experiments, the main baseline is
the standard softmax classiﬁer. Where appropriate, we show three additional baseline results based
on the linear kernel, second order pooling (Lin et al., 2015), and kervolutional networks (Wang
et al., 2019b). Note that the focus of our experiments is to demonstrate the beneﬁts of kernelized
classiﬁcation in efﬁciently utilizing embeddings learned with various representation learners, not
to claim state-of-the-art results on already well-explored benchmark datasets. Details about our
experimental setup is in Appendix A."
SYNTHETIC DATA,0.27169811320754716,"6.1
SYNTHETIC DATA"
SYNTHETIC DATA,0.27547169811320754,"We ﬁrst evaluate the kernelized classiﬁcation layer as an isolated unit by demonstrating its capabil-
ities to learn nonlinear patterns on Sn. We train a softmax classiﬁer and our kernelized classiﬁer
on the synthetic dataset described in Appendix B. Results on the test set are shown in Table 1. We
also report the theoretical maximum accuracy, the accuracy of the Bayes optimal classiﬁer. The
accuracy of our kernelized classiﬁcation layer signiﬁcantly outperforms the baseline and gets close
to theoretical best. This can be attributed to the layer’s capabilities to learn nonlinear patterns on the
sphere by embedding the data into an RKHS that optimally separates the classes."
SYNTHETIC DATA,0.2792452830188679,"We visualize the outcomes of the classiﬁers in Figure 1. Note that the softmax classiﬁer can only
separate cap-like regions on S2, this is a result of its being a linear classiﬁer with respect to the em-
beddings. Our kernelized classiﬁer, on the other hand, can do a more complex nonlinear separation
of the embeddings."
SYNTHETIC DATA,0.2830188679245283,Under review as a conference paper at ICLR 2022
SYNTHETIC DATA,0.28679245283018867,"Method
Accuracy
Softmax classiﬁer (baseline)
85.51
Kernelized classiﬁer (ours)
94.20
Bayes optimal classiﬁer
95.06"
SYNTHETIC DATA,0.29056603773584905,Table 1: Classiﬁcation results on the synthetic dataset.
SYNTHETIC DATA,0.2943396226415094,"Dataset
Accuracy
Baseline
Ours
CIFAR-10
76.06
79.85
CIFAR-100
44.38
46.48"
SYNTHETIC DATA,0.2981132075471698,Table 2: Results in the distillation setting.
IMAGE CLASSIFICATION,0.3018867924528302,"6.2
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.30566037735849055,"We now report results on CIFAR-10 and CIFAR-100 real world image benchmarks1 (Krizhevsky,
2009). To demonstrate better model-efﬁciency with the kernelized classiﬁcation, we experiment
with several CIFAR-ResNet architectures (He et al., 2016) with increasing model capacity. We
consider four different baselines: (1) Softmax: the standard softmax loss, (2) LIN: normalized
embeddings and weights with only the linear kernel along with a learnable coefﬁcient. This is similar
to the approach discussed in Hoffer et al. (2018), but with additional freedom to learn the weight
vectors, (3) SOP: second order pooling (Lin et al., 2015), which is also equivalent to Mahmoudi
et al. (2020) with a second degree polynomial, and (4) KERVO: kervolutional networks (Wang
et al., 2019b) with the best out of the Gaussian RBF kernel and the polynomial kernel."
IMAGE CLASSIFICATION,0.30943396226415093,"Increased model-efﬁciency obtained with the kernelized classiﬁer is evident from the accuracies
summarized in Table 3. For example, the same accuracy of ResNet-56 (540K parameters) with the
standard softmax classiﬁer can be obtained with a much smaller ResNet-32 (300K parameters) when
the kernelized classiﬁer is used. Our method signiﬁcantly outperforms the other baselines as well.
This shows the beneﬁts of optimizing over the entire space of positive deﬁnite kernels instead of
restricting ourselves to linear methods or pre-deﬁned kernels."
IMAGE CLASSIFICATION,0.3132075471698113,"Backbone
# params
Softmax
LIN
SOP
KERVO
Ours
ResNet-8
61K
83.73 / 53.82
82.45 / 54.00
84.03 / 55.80
85.15 / 56.92
86.93 / 58.27
ResNet-14
121K
89.87 / 63.85
90.16 / 63.67
90.47 / 63.53
90.43 / 64.14
91.48 / 66.67
ResNet-20
181K
91.14 / 65.99
91.01 / 65.79
91.75 / 67.97
91.34 / 67.31
92.88 / 69.33
ResNet-32
300K
92.22 / 68.96
92.21 / 69.16
92.31 / 70.39
92.42 / 69.40
93.70 / 71.30
ResNet-44
420K
92.10 / 70.16
93.10 / 70.54
92.42 / 71.13
92.88 / 71.09
94.05 / 73.20
ResNet-56
540K
93.01 / 71.23
93.13 / 72.11
93.33 / 73.12
93.10 / 72.39
94.15 / 74.10"
IMAGE CLASSIFICATION,0.3169811320754717,Table 3: Results on the CIFAR-10/CIFAR-100 datasets.
NATURAL LANGUAGE UNDERSTANDING,0.32075471698113206,"6.3
NATURAL LANGUAGE UNDERSTANDING"
NATURAL LANGUAGE UNDERSTANDING,0.32452830188679244,"In this section, we show the beneﬁts of the kernelized classiﬁcation layer in solving four different
text classiﬁcation tasks in the GLUE benchmark (Wang et al., 2019a). We use mask-LM pretrained
BERT models of different capacities (Turc et al., 2019) and ﬁnetune them on each classiﬁcation task.
Note that we do not use distillation and directly ﬁnetune the models with the dataset labels. Since
detailed analyses on GLUE test datasets are not allowed (Wang et al., 2019a), we tune hyperparam-
eters on subsets of train sets and report results on the validation sets in Table 4. More details about
the experiment setup are provided in Appendix A."
NATURAL LANGUAGE UNDERSTANDING,0.3283018867924528,"Table 4 provides evidence that kernelized classiﬁcation layers helps in extracting more gains out of
a given representation learner. For example, across all the datasets, BERT-Mini (11.3M parameters)
with kernelized classiﬁcation can get similar results as the BERT-Small (29.1M parameters) with
softmax classiﬁcation. Therefore, using the nonlinear, kernelized classiﬁer is an effective alternative
to using a bigger backbone for increasing classiﬁcation performance."
KNOWLEDGE DISTILLATION,0.3320754716981132,"6.4
KNOWLEDGE DISTILLATION"
KNOWLEDGE DISTILLATION,0.33584905660377357,"We now evaluate our method in the distillation setting to show that it is complementary to existing
model compression techniques. We used the CIFAR-10 and CIFAR-100 datasets, the softmax CI-
FAR ResNet-56 models from Tables 3 as the teacher models, and the LeNet-5 network (Lecun et al.,
1998) as the student model. Note that it is straightforward to utilize the kernelized classiﬁcation
layer in the distillation setting described in Hinton et al. (2015) by replacing usual logits with their"
KNOWLEDGE DISTILLATION,0.33962264150943394,"1We use the standard data augmentation in CIFAR-10/100 (He et al., 2016). Some authors refer to these
datasets as CIFAR-10+/100+ when data augmentation is used. We omit the + to keep the text uncluttered."
KNOWLEDGE DISTILLATION,0.3433962264150943,Under review as a conference paper at ICLR 2022
KNOWLEDGE DISTILLATION,0.3471698113207547,"Dataset
Method
BERT-Tiny
BERT-Mini
BERT-Small
BERT-Medium
BERT-Base
# params
4.4M
11.3M
29.1M
41.7M
110.1M"
KNOWLEDGE DISTILLATION,0.35094339622641507,MPRC (F1/Acc)
KNOWLEDGE DISTILLATION,0.35471698113207545,"Softmax
82.77 / 74.50
85.20 / 79.25
88.41 / 83.75
89.72 / 85.50
92.58 / 89.50
LIN
82.00 / 73.78
84.99 / 79.10
88.51 / 83.79
89.88 / 85.62
92.34 / 89.48
SOP
83.81 / 76.21
86.21 / 80.42
89.21 / 84.38
89.99 / 85.78
92.68 / 89.73
Ours
84.97 / 78.25
88.89 / 84.50
90.12 / 85.75
91.60 / 87.75
93.24 / 90.50"
KNOWLEDGE DISTILLATION,0.3584905660377358,QQP (F1/Acc)
KNOWLEDGE DISTILLATION,0.3622641509433962,"Softmax
81.91 / 86.35
84.09 / 88.10
85.18 / 88.87
86.12 / 89.73
87.18 / 90.30
LIN.
81.07 / 86.11
84.05 / 88.09
85.19 / 88.87
86.05 / 89.69
87.20 / 90.31
SOP
82.01 / 87.00
84.50 / 88.56
85.99 / 90.00
86.15 / 89.99
87.21 / 90.34
Ours
82.78 / 87.13
85.00 / 88.84
85.83 / 89.46
86.83 / 90.17
87.97 / 91.04"
KNOWLEDGE DISTILLATION,0.3660377358490566,RTE (Acc)
KNOWLEDGE DISTILLATION,0.36981132075471695,"Softmax
63.53
65.74
66.01
66.84
71.46
LIN
62.32
65.69
65.97
66.02
71.50
SOP
63.59
65.78
66.00
68.02
72.96
Ours.
65.96
66.93
67.72
67.94
73.28"
KNOWLEDGE DISTILLATION,0.37358490566037733,SST-2 (Acc)
KNOWLEDGE DISTILLATION,0.37735849056603776,"Softmax
82.38
87.11
87.19
89.86
91.97
LIN.
82.04
86.92
87.29
89.50
91.72
SOP
82.89
87.19
88.52
90.12
91.98
Ours
84.07
87.14
89.42
90.83
92.69"
KNOWLEDGE DISTILLATION,0.38113207547169814,Table 4: Results on several natural language understanding tasks in the GLUE benchmark.
KNOWLEDGE DISTILLATION,0.3849056603773585,"kernelized counterparts. We use the cross-entropy loss with the teacher scores with the temperature
set to 20 in all cases. Results are shown in Table 2. Signiﬁcant gains are observed with the kernel-
ized classiﬁcation layer. This can be attributed to the layer’s capabilities to approximate complex
teacher probabilities even with weak embeddings due to the nonlinear classiﬁer."
ACTIVE LEARNING,0.3886792452830189,"6.5
ACTIVE LEARNING"
ACTIVE LEARNING,0.39245283018867927,"Active learning focuses on reducing human annotation costs by selecting a subset of images to label
that are more likely to yield the best model (Settles, 2009). We used different sampling methods such
as random, margin (Lewis & Gale, 1994; Scheffer et al., 2001), and k-center (Sener & Savarese,
2017; Wolf, 2011) to generate subsets of various sizes. The setup is detailed in Appendix C. As
shown in Table 5, our results on random subsets outperform the softmax ResNet-56 models on
margin and k-center based subsets, and we achieve even better results using improved sampling
methods. This demonstrates that the kernelized classiﬁcation layer can produce better models in
limited-data settings as well."
ABLATION STUDIES,0.39622641509433965,"7
ABLATION STUDIES"
ABLATION STUDIES,0.4,We now present ablation experiments using CIFAR-100 and the ResNet-56 backbone.
KERNEL LEARNING,0.4037735849056604,"7.1
KERNEL LEARNING"
KERNEL LEARNING,0.4075471698113208,"To investigate the beneﬁts of automatic kernel learning compared to using a pre-deﬁned kernel in
the kernelized classiﬁcation layer, we compare our kernel learning method with two pre-deﬁned
kernels in the kernelized classiﬁcation layer: the polynomial kernel of order 10 and the Gaussian
RBF kernel. We also show results with an MKL baseline with linear, 2nd order, and Gaussian"
KERNEL LEARNING,0.41132075471698115,"%
Baseline
Ours
rnd
mgn
k-ctr
rnd
mgn
k-ctr
30
58.03
58.88
58.41
61.66
61.80
63.08
40
61.05
61.81
62.02
65.25
66.28
66.35
50
64.81
65.36
65.47
67.17
68.14
69.41
60
66.26
67.03
68.25
69.17
70.61
70.10
70
67.47
69.16
69.84
70.06
70.90
71.50
80
69.59
69.47
71.25
71.66
72.21
72.64
90
70.25
71.41
71.11
72.60
73.90
73.14"
KERNEL LEARNING,0.41509433962264153,"Table 5:
Active learning on CIFAR-100. Terms rnd, mgn,
and k-ctr refer to random, margin, and k-center, respectively."
KERNEL LEARNING,0.4188679245283019,"Figure 2: Accuracy versus the order of the
approximation (M) on CIFAR-100."
KERNEL LEARNING,0.4226415094339623,Under review as a conference paper at ICLR 2022
KERNEL LEARNING,0.42641509433962266,"kernels. Results are shown in Table 6. It is evident that sweeping over all possible kernels in light
of Theorem 4.3 to ﬁnd the best kernel yields signiﬁcant practical beneﬁts. This is not surprising
because the polynomial and Gaussian kernels are members of the large search space of kernels that
we optimize over in our method."
KERNEL LEARNING,0.43018867924528303,"Method
Acc
Gaussian RBF with ﬁxed σ
73.21
Gaussian RBF with learned σ
73.23
Polynomial kernel
73.16
MKL
73.25
Ours
74.10"
KERNEL LEARNING,0.4339622641509434,Table 6: Beneﬁts of learning the best kernel.
KERNEL LEARNING,0.4377358490566038,"Method
Acc
No normalization
71.16
Embeddings normalized
71.23
Embeddings & weights normalized with ﬁxed T
71.19
Embeddings & weights normalized with learned T
72.11
Ours
74.10"
KERNEL LEARNING,0.44150943396226416,Table 7: Effects of normalization.
EMBEDDING NORMALIZATION,0.44528301886792454,"7.2
EMBEDDING NORMALIZATION"
EMBEDDING NORMALIZATION,0.4490566037735849,"As discussed previously, we L2-normalize both embeddings and weights in the classiﬁcation layer.
We study the effect of this normalization for the baseline softmax loss in Table 7. Note the setting
where both embeddings and weights are normalized with an appropriate temperature T is equivalent
to the cosine softmax loss (Liu et al., 2017; Chen et al., 2019; Wang et al., 2020). Note also that the
LIN baseline considered in Tables 3 and 4 uses normalized embeddings and weights with an auto-
matically learned T, same as the fourth row in Table 7. In our softmax baselines we use normalized
embeddings since it works consistently better than the unnormalized version."
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.4528301886792453,"7.3
SENSITIVITY TO THE ORDER OF THE APPROXIMATION"
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.45660377358490567,"As discussed in § 5.1, intuitively, the order M of the approximation should not matter as long as it
is large enough. We verify this in Figure 2 with the CIFAR-100 dataset, where we show the changes
in accuracy with increasing M. We use M = 10 in all our experiments."
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.46037735849056605,"7.4
DO MORE FULLY-CONNECTED LAYERS HELP?"
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.4641509433962264,"One could wonder whether more fully-connected layers at the end of the network would improve
the classiﬁcation. To address this, we added an additional fully-connected layer with d units to
ResNet-56. The accuracy improved only marginally from 71.23 to 71.29, as opposed to 74.10 with
our method. This observation is in-line with the discussion in Rendle et al. (2020): MLP scorers are
somewhat difﬁcult to train. An explanation for this could be the common observation that, although
MLPs can theoretically approximate any function, learning one from data is difﬁcult. This has
motivated inductive-bias based models such as CNNs and Transformers. Kernelized classiﬁcation
can also be viewed as a way of presenting an inductive bias motivated by the classic kernel method
theory. Note also that, unlike the kernelized classiﬁcation layer, added MLP layers come with a
signiﬁcant increase in the model’s computational and memory complexity."
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.4679245283018868,"7.5
DIFFERENT LOSS FUNCTIONS, ACTIVATION FOR COEFFICIENT, ETC."
SENSITIVITY TO THE ORDER OF THE APPROXIMATION,0.4716981132075472,"We also provide a number of other ablations studies on different loss functions, effects of embedding
rectiﬁcation, kernel coefﬁcient activation, and other settings in Appendix D."
CONCLUSION,0.47547169811320755,"8
CONCLUSION"
CONCLUSION,0.47924528301886793,"We presented a kernelized classiﬁcation layer for deep neural networks aiming to extract the best
possible classiﬁer with embeddings produced by a given representation learner. This classiﬁcation
layer classiﬁes embeddings in a high dimensional RKHS while automatically learning the optimal
kernel that enables this high-dimensional mapping. We showed that a classiﬁcation network with
a lightweight representation learning backbone can be made more effective by replacing the usual
softmax classiﬁer with the kernelized classiﬁer. We showed consistent and substantial accuracy im-
provements in image classiﬁcation, natural language understanding, distillation, and active learning
settings. These accuracy improvements strongly support the usefulness of kernelized classiﬁcation
layer in ﬁnding nonlinear patterns in the embeddings."
CONCLUSION,0.4830188679245283,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4867924528301887,ETHICS STATEMENT
ETHICS STATEMENT,0.49056603773584906,"This work concerns mathematical and empirical analysis of deep learning based classiﬁcation tech-
niques with applications in image recognition and natural language understanding. We do not fore-
see our work having undue societal effects. Our work does not explicitly consider issues of fairness
in classiﬁcation, which is an important yet under-studied dimension. We do not foresee our tech-
niques as unduly amplifying biases in existing algorithms."
REFERENCES,0.49433962264150944,REFERENCES
REFERENCES,0.4981132075471698,"Shawkat Ali and Kate A. Smith-Miles. A meta-learning approach to automatic kernel selection for
support vector machines. Neurocomputing, 2006. Neural Networks."
REFERENCES,0.5018867924528302,"Nachman Aronszajn. Theory of Reproducing Kernels. Transactions of the American Mathematical
Society, 1950."
REFERENCES,0.5056603773584906,"Tensorﬂow Authors. TensorFlow Datasets: a collection of ready-to-use datasets. URL https:
//www.tensorflow.org/datasets/. [Online; accessed Feb-04-2021]."
REFERENCES,0.5094339622641509,"C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups. Springer, 1984."
REFERENCES,0.5132075471698113,"S. Cai, W. Zuo, and L. Zhang. Higher-order integration of hierarchical convolutional activations for
ﬁne-grained visual categorization. In ICCV, 2017."
REFERENCES,0.5169811320754717,"H. Cevikalp and H. Saglamlar. Polyhedral Conic Classiﬁers for Computer Vision Applications and
Open Set Recognition. TPAMI, 2021."
REFERENCES,0.5207547169811321,"Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classiﬁcation. CoRR, abs/1904.04232, 2019. URL http://arxiv.org/
abs/1904.04232."
REFERENCES,0.5245283018867924,"Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
for deep neural networks. CoRR, abs/1710.09282, 2017. URL http://arxiv.org/abs/
1710.09282."
REFERENCES,0.5283018867924528,"Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. CoRR, abs/1506.03059, 2015."
REFERENCES,0.5320754716981132,"C. Cortes and V. Vapnik. Support Vector Networks. Machine Learning, 1995."
REFERENCES,0.5358490566037736,"Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie. Kernel Pooling for Convolutional Neural
Networks. In CVPR, 2017."
REFERENCES,0.539622641509434,"Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, and Le Song. Scalable
kernel methods via doubly stochastic gradients, 2015."
REFERENCES,0.5433962264150943,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009."
REFERENCES,0.5471698113207547,"J. Deng, J. Guo, N. Xue, and S. Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face
Recognition. In CVPR, 2019."
REFERENCES,0.5509433962264151,"Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware
acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):
485–532, 2020. doi: 10.1109/JPROC.2020.2976475."
REFERENCES,0.5547169811320755,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5584905660377358,"Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact Bilinear Pooling. In CVPR, 2016."
REFERENCES,0.5622641509433962,"Priya Goyal, Piotr Doll´ar, Ross B. Girshick, P. Noordhuis, L. Wesolowski, Aapo Kyrola, Andrew
Tulloch, Y. Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.
ArXiv, abs/1706.02677, 2017."
REFERENCES,0.5660377358490566,Under review as a conference paper at ICLR 2022
REFERENCES,0.569811320754717,"R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale
inference with anisotropic vector quantization. In ICML, 2020. URL https://arxiv.org/
abs/1908.10396."
REFERENCES,0.5735849056603773,"Mehmet G¨onen, Ethem Alpaydın, and Francis Bach. Multiple kernel learning algorithms. JMLR,
2011."
REFERENCES,0.5773584905660377,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
The Elements of Statistical Learning.
Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001."
REFERENCES,0.5811320754716981,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016."
REFERENCES,0.5849056603773585,"Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural Network.
In NIPS Deep Learning and Representation Learning Workshop, 2015. URL http://arxiv.
org/abs/1503.02531."
REFERENCES,0.5886792452830188,"Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classiﬁer: the marginal value of training the
last weight layer. In ICLR, 2018."
REFERENCES,0.5924528301886792,"Tom Howley and Michael G. Madden. The genetic kernel support vector machine: Description and
evaluation. Artif. Intell. Rev., 2005."
REFERENCES,0.5962264150943396,"Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-
entropy in classiﬁcation tasks. 2020."
REFERENCES,0.6,"Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, and Mehrtash Harandi.
Optimizing Over Radial Kernels on Compact Manifolds. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2014."
REFERENCES,0.6037735849056604,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained
categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013."
REFERENCES,0.6075471698113207,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009."
REFERENCES,0.6113207547169811,"Ivano Lauriola, Claudio Gallicchio, and Fabio Aiolli. Enhancing deep neural networks via multiple
kernel learning. Pattern Recognition, 2020."
REFERENCES,0.6150943396226415,"Linh Le, Jie Hao, Ying Xie, and Jennifer Priestley. Deep kernel: Learning kernel function from
data using deep neural network. In 2016 IEEE/ACM 3rd International Conference on Big Data
Computing Applications and Technologies (BDCAT), 2016."
REFERENCES,0.6188679245283019,"Quoc Le, Tam´as Sarl´os, and Alex Smola. Fastfood: Approximating kernel expansions in loglinear
time. In ICML, 2013."
REFERENCES,0.6226415094339622,"Yann Lecun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278–2324, 1998."
REFERENCES,0.6264150943396226,"D. D. Lewis and W. A. Gale. A sequential algorithm for training text classiﬁers. In SIGIR, 1994."
REFERENCES,0.630188679245283,"P. Li, J. Xie, Q. Wang, and W. Zuo. Is Second-Order Information Helpful for Large-Scale Visual
Recognition? In ICCV, 2017."
REFERENCES,0.6339622641509434,"T. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN Models for Fine-Grained Visual Recognition.
In ICCV, 2015."
REFERENCES,0.6377358490566037,"W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. SphereFace: Deep Hypersphere Embedding for
Face Recognition. In CVPR, 2017."
REFERENCES,0.6415094339622641,"Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
Hyperspherical Learning. In Advances in Neural Information Processing Systems, 2017."
REFERENCES,0.6452830188679245,"I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations (ICLR) 2017 Conference Track, April 2017."
REFERENCES,0.6490566037735849,Under review as a conference paper at ICLR 2022
REFERENCES,0.6528301886792452,"M. A. Mahmoudi, A. Chetouani, F. Boufera, and H. Tabia. Kernelized Dense Layers For Facial
Expression Recognition. In 2020 IEEE International Conference on Image Processing (ICIP),
pp. 2226–2230, 2020. doi: 10.1109/ICIP40778.2020.9190694."
REFERENCES,0.6566037735849056,"Julien Mairal, Piotr Koniusz, Za¨ıd Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
CoRR, abs/1406.3332, 2014."
REFERENCES,0.660377358490566,"O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In 2012 IEEE Conference
on Computer Vision and Pattern Recognition, 2012."
REFERENCES,0.6641509433962264,"Steffen Rendle, Walid Krichene, Li Zhang, and John R. Anderson. Neural Collaborative Filtering
vs. Matrix Factorization Revisited. In Fourteenth ACM Conference on Recommender Systems,
2020."
REFERENCES,0.6679245283018868,"T. Scheffer, C. Decomain, and S. Wrobel. Active hidden markov models for information extraction.
In Advances in Intelligent Data Analysis, 2001."
REFERENCES,0.6716981132075471,"I. J. Schoenberg. Positive Deﬁnite Functions on Spheres. Duke Mathematical Journal, 1942."
REFERENCES,0.6754716981132075,"Bernhard Sch¨olkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT Press, 2002."
REFERENCES,0.6792452830188679,"O. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set approach,
2017."
REFERENCES,0.6830188679245283,"B. Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison,
2009."
REFERENCES,0.6867924528301886,"John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-
sity Press, 2004."
REFERENCES,0.690566037735849,"Ingo Steinwart. Sparseness of Support Vector Machines. JMLR, 2003."
REFERENCES,0.6943396226415094,"Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019."
REFERENCES,0.6981132075471698,"Manik Varma and D. Ray. Learning the discriminative power-invariance trade-off. In IN ICCV,
2007."
REFERENCES,0.7018867924528301,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural In-
formation Processing Systems, 2017."
REFERENCES,0.7056603773584905,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019a.
In the Proceedings of ICLR."
REFERENCES,0.7094339622641509,"Chen Wang, Jianfei Yang, Lihua Xie, and Junsong Yuan. Kervolutional neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 31–40, 2019b."
REFERENCES,0.7132075471698113,"H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu. CosFace: Large Margin
Cosine Loss for Deep Face Recognition. In CVPR, 2018."
REFERENCES,0.7169811320754716,"Q. Wang, J. Xie, W. Zuo, L. Zhang, and P. Li. Deep CNNs Meet Global Covariance Pooling: Better
Representation and Generalization. TPAMI, 2020."
REFERENCES,0.720754716981132,"Xin Wang, Thomas E. Huang, Trevor Darrell, Joseph E. Gonzalez, and Fisher Yu. Frustratingly
simple few-shot object detection, 2020."
REFERENCES,0.7245283018867924,"Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A Comprehensive Study on Center Loss for Deep Face
Recognition. IJCV, 2019."
REFERENCES,0.7283018867924528,"Andrew Gordon Wilson and Hannes Nickisch. Kernel Interpolation for Scalable Structured Gaussian
Processes (KISS-GP). In ICML, 2015."
REFERENCES,0.7320754716981132,Under review as a conference paper at ICLR 2022
REFERENCES,0.7358490566037735,"Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning.
CoRR, abs/1511.02222, 2015."
REFERENCES,0.7396226415094339,"Gert W. Wolf. Facility location: Concepts, models, algorithms and case studies. series: Contribu-
tions to management science. 2011."
REFERENCES,0.7433962264150943,"Z. Yang, M. Moczulski, M. Denil, N. De Freitas, L. Song, and Z. Wang. Deep fried convnets. In
2015 IEEE International Conference on Computer Vision (ICCV), 2015."
REFERENCES,0.7471698113207547,"Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe
Zhao, Li Wei, and Ed Chi. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item
Recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems, New
York, NY, USA, 2019. Association for Computing Machinery."
REFERENCES,0.7509433962264151,"Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, and Petros Daras. Non-linear
convolution ﬁlters for cnn-based learning. CoRR, abs/1708.07038, 2017."
REFERENCES,0.7547169811320755,Under review as a conference paper at ICLR 2022
REFERENCES,0.7584905660377359,"A
EXPERIMENTAL SETUP"
REFERENCES,0.7622641509433963,"In all experiments, we use M = 10 for the kernelized classiﬁcation layer. As discussed in § 5.1 and
Appendix D.5, we use ReLU activation and a weight decay of 0.0001 on the α parameter vector.
This is the same amount of weight decay used in the other parts of the network, when applicable.
The α parameter vector is initialized with all ones."
REFERENCES,0.7660377358490567,"For all image classiﬁcation experiments, we use SGD with 0.9 momentum, linear learning rate
warmup (Goyal et al., 2017), cosine learning rate decay (Loshchilov & Hutter, 2017), and decide the
base learning by cross validation. When a better learning rate schedule is available for the baseline
(e.g. the CIFAR schedule in He et al. 2016), we experimented with both that and our schedule and
report the best accuracy of the two. The maximum number of epochs was 450 in all cases. Mini-
batch size was 128 for the synthetic and CIFAR datasets and 64 other datasets with larger images
used in Appendix D.2. We used the standard CIFAR data augmentation method in He et al. 2016
for CIFAR-10 and CIFAR-100 datasets, and the Imagenet data augmentation in the same paper for
other image datasets. Some ResNet models use ReLU activation on embeddings. We also remove
this activation to utilize the full surface of Sn. The same is done to the baseline models to enable a
fair comparison (see AppendixD.4 for more details)."
REFERENCES,0.769811320754717,"For the natural language understanding tasks in § 6.3, we use the publicly-available, MLM-
pretrained BERT/Small-BERT models from TensorFlow Hub (Turc et al., 2019). Usual CLS pool-
ing is done at the end of the Transformer encoder to obtain an embedding for each input sen-
tence/sentence pair. We use the AdamW optimizer with linear ramp-up and decay as is standard
for BERT model ﬁnetuning. Note that we do not distill the ﬁnal task from a bigger model and di-
rectly ﬁnetune with the one-hot labels in the original datasets. Hyperparameter search followed the
procedure described in Turc et al. (2019). Since detailed analysis of different methods on GLUE test
tests are not allowed, we tune hyperparameters on a 10% subset of the train set and report results on
the validation sets. Once the hyperparamers are decided, the full train set is used to train the ﬁnal
model. Since GLUE validation sets are small, we report the average accuracy over 5 different runs
for each method. We do not however notice signiﬁcant variance in accuracy across different runs."
REFERENCES,0.7735849056603774,"B
SYNTHETIC DATA GENERATION"
REFERENCES,0.7773584905660378,"We generated the binary classiﬁcation dataset used in §6.1 using a mixture of Gaussians, inspired
by the blue-orange dataset in Hastie et al. (2001). More speciﬁcally, we ﬁrst generated 10 cluster
centers for each class by sampling from an isotropic Gaussian distribution with covariance 0.5 I3 and
mean [1, 0, 0]T for the blue class and [0, 1, 0]T for the orange class. We then generated 5,000 train
observations for each class using the following method: for each observation, we uniform-randomly
picked a cluster center of the corresponding class and then generated a sample from an isotropic
Gaussian distribution centered at that cluster center with covariance 0.02 I3. All the observations
were projected on to S2 by L2-normalizing them. The test set was generated in the same manner
using the same cluster centers as the train set."
REFERENCES,0.7811320754716982,"C
ACTIVE LEARNING"
REFERENCES,0.7849056603773585,"The goal of the active learning experiment is to show that the kernelized classiﬁcation layer can
produce accurate models even with less data. In order to study this, we produce subsets of datasets
under various budgets using several sampling techniques, and evaluate the models trained on this.
The simplest one is random sampling, where images are selected randomly under a given budget.
Other methods include margin (Lewis & Gale, 1994; Scheffer et al., 2001), and k-center (Sener &
Savarese, 2017; Wolf, 2011) where class prediction scores and embeddings from the images are used
in the subset selection."
REFERENCES,0.7886792452830189,"We do not rely on the actual labels of the images in the dataset during the subset selection, since
active learning is driven toward reducing label annotation costs. We used a 10% random subset of
the original CIFAR-100 dataset with labels to ﬁrst learn an initial seed model, which was then used
to generate predictions and embeddings. Note that only embeddings and class prediction scores from
this initial seed model are used in subset selection, and we do not access the original class labels of
the images. We use a batch setting where we do not incrementally update the model after selecting"
REFERENCES,0.7924528301886793,Under review as a conference paper at ICLR 2022
REFERENCES,0.7962264150943397,"every image, and we directly select entire subsets under a given budget. In all our experiments,
we used the CIFAR ResNet-56 model. The learning rate, batch size and the number of epochs are
provided in Appendix A. The embeddings are of dimension 64. For the k-center method, we need
distances between the embeddings, and we used cosine distances computed using fast similarity
search of (Guo et al., 2020)."
REFERENCES,0.8,"D
ADDITIONAL EXPERIMENTS"
REFERENCES,0.8037735849056604,We report a number of additional experimental results in this section.
REFERENCES,0.8075471698113208,"D.1
ADDITIONAL BACKBONES"
REFERENCES,0.8113207547169812,"Image classiﬁcation results with VGG-16 and DenseNet-40-12 backbones are reported in Table 8.
Since the original VGG-16 is designed for 224 × 224 images, we used a modiﬁed CIFAR version
with 256 dimensional hidden size at the end."
REFERENCES,0.8150943396226416,"Backbone
Softmax
LIN
SOP
KER
Ours
VGG-16
92.58 / 71.48
92.59 / 71.52
93.20 / 72.10
93.21 / 72.04
94.39 / 73.32
DenseNet
94.76 / 75.58
94.73 / 75.67
94.98 / 75.00
94.92 / 75.08
95.31 / 76.87"
REFERENCES,0.8188679245283019,Table 8: Results on the CIFAR-10/CIFAR-100 datasets with different backbones.
REFERENCES,0.8226415094339623,"D.2
IMAGE CLASSIFICATION TRANSFER LEARNING"
REFERENCES,0.8264150943396227,"Here, we evaluate our method in a image classiﬁcation transfer learning setting. To this end, we take
a ResNet-50 network pre-trained on the Imagenet ILSVRC 2012 classiﬁcation dataset (Deng et al.,
2009) and ﬁnetune it on Oxford-IIIT Pets (Parkhi et al., 2012) and Stanford Cars (Krause et al., 2013)
datasets. For each dataset, we use the train/test splits provided by the standard Tensorﬂow Datasets
implementation (Authors). Results are summarized in Table 9. Note that the KERVO baseline is not
possible in this setting as it involves changes to the backbone network. On both datasets, kernelized
classiﬁcation layer results in signiﬁcant gains over the baselines. This is intuitive to understand since
the embeddings learned from the source task (Imagenet) might not linearly separate the new classes
in the target task. We can therefore beneﬁt from a nonlinear classiﬁer in the transfer learning setting."
REFERENCES,0.8301886792452831,"Dataset
Accuracy
SM
LIN
SOP
Ours
Oxford-IIIT Pets
92.06
91.99
92.28
93.56
Stanford Cars
90.78
90.83
91.04
92.60"
REFERENCES,0.8339622641509434,Table 9: Results on the transfer learning datasets.
REFERENCES,0.8377358490566038,"D.3
DIFFERENT LOSS FUNCTIONS"
REFERENCES,0.8415094339622642,"To evaluate the kernelized classiﬁcation layer under a loss function other than the cross-entory loss,
we report CIFAR-10/100 results with the squared loss (Hui & Belkin, 2020) in Table 10. Note that
the application of squared loss to the kernelized classiﬁcation layer’s outputs is straightforward since
it outputs logits in the usual sense."
REFERENCES,0.8452830188679246,"D.4
EFFECT OF EMBEDDING RECTIFICATION"
REFERENCES,0.8490566037735849,"As discussed previously, different to the usual image classiﬁcation networks in He et al. (2016), we
remove the ReLU activation from the embeddings. This is to utilize the full surface of Sn without
restricting ourselves to only the nonnegative orthant. As is evident from Table 11, removing ReLU
has only a marginal effect on the standard softmax baseline. It is however an important factor for
our method. We consistently used embeddings without the ReLU activation in all our experiments
in the previous sections."
REFERENCES,0.8528301886792453,Under review as a conference paper at ICLR 2022
REFERENCES,0.8566037735849057,"Backbone
CIFAR-10
CIFAR-100
Sq. Loss Ours+Sq.Loss Sq. Loss Ours+Sq. Loss
ResNet-8
83.70
86.58
51.55
55.99
ResNet-14
89.86
91.67
62.94
65.06
ResNet-20
91.20
92.75
64.00
68.61
ResNet-32
92.19
93.65
68.10
71.35
ResNet-44
92.16
94.12
69.48
72.25
ResNet-56
93.19
94.22
70.34
73.13"
REFERENCES,0.8603773584905661,Table 10: Results on the CIFAR-10/100 datasets with the square loss.
REFERENCES,0.8641509433962264,"Method
Accuracy
Softmax classiﬁer with:"
REFERENCES,0.8679245283018868,"rectiﬁed embeddings
70.96
unrectiﬁed embeddings
71.23
Our classiﬁer with:"
REFERENCES,0.8716981132075472,"rectiﬁed embeddings
71.61
unrectiﬁed embeddings
74.10"
REFERENCES,0.8754716981132076,Table 11: Effect of rectiﬁcation of the embeddings.
REFERENCES,0.879245283018868,"D.5
EFFECT OF ACTIVATION ON THE KERNEL COEFFICIENTS"
REFERENCES,0.8830188679245283,"Following the discussion in § 5.1, the constraint α−2, α−1, . . . , αM ≥0 is important to preserve
the positive deﬁniteness of kopt. This can be imposed by using α = ReLU(α′), where α′ is the
learnable parameter vector. However, ReLU has no upper-bound and allowing the scale of α to grow
unboundedly causes issues in optimization: Assume we have an instantiation α0 of the vector α. By
replacing α0 with λα0, where λ > 1, we scale all the inner product terms in (3) and (4) by the same
λ. As a result, we improve the loss of the already correctly classiﬁed training examples, but without
making any effective change to the predictor. Therefore, under this setting, once the majority of
the training examples are correctly classiﬁed, the neural network can easily improve the loss just by
increasing the norm of α, which is not useful. We therefore advocate an L2-regularization term on
α when ReLU activation is used."
REFERENCES,0.8867924528301887,"Alternatively, one could also use α = sigmoid(α′) or α = softmax(α′), both of which not only
guarantee α−2, α−1, . . . , αM ≥0, but also produce bounded α. Therefore, no regularization on α
is needed for these options. The softmax activation here should not be confused with the softmax
loss discussed in § 3. The usage of the softmax activation in this context is similar to that in the
self-attention literature (Vaswani et al., 2017), where it is used to normalize the coefﬁcients of a
linear combination. We experimented with these different activations on α′ and summerized the
results in Table 12."
REFERENCES,0.8905660377358491,"We used a weight decay of 0.0001 on the coefﬁcient vector whenever ReLU activation is used.
Although sigmoid and softmax activations eliminate the need for weight decay, they put a hard
constraint on | ⟨φ(w), φ(f)⟩H |. To overcome this limitation, it is helpful to use a temperature hy-
perparameter in (3), where each inner product is divided by T before taking the exponential. We
used a temperature of 0.1 and 0.005, with sigmoid and softmax, respectively. Although sigmoid
gives the best performance in Table 12, we occasionally observed optimization issues with it, which
could be due to the vanishing gradient issue associated with this activation function. We therefore
stick to ReLU in all other experiments. We however note that, in most cases, competitive results can
be obtained with softmax as well, when used with a temperature of 0.005."
REFERENCES,0.8943396226415095,"It is also interesting to note that using no activation function on α′ causes frequent divergence in
training. This is consistent with the theory: The summation in (7) is not guaranteed to be posi-
tive deﬁnite when ams are allowed to be negative (see Proposition 4.2). Therefore, the theory of
kernelized classiﬁcation is not valid in this case."
REFERENCES,0.8981132075471698,Under review as a conference paper at ICLR 2022
REFERENCES,0.9018867924528302,"Activation function
Accuracy
sigmoid
74.96
softmax
73.69
ReLU
74.10
None (linear)
unstable"
REFERENCES,0.9056603773584906,"Table 12: Different activation functions on the coefﬁcient vector. Note that the kernelized classiﬁer
is unstable when no activation function is used, this agrees with the theoretical analysis."
REFERENCES,0.909433962264151,"E
APPROXIMATION ERROR BOUNDS"
REFERENCES,0.9132075471698113,"In this section, we analyze error bounds for the approximation in Eq. (7). We start by proving the
following theorem, which establishes a rigorous upper bound for the approximation error.
Theorem E.1. Let k : Sn × Sn →R be any positive deﬁnite radial kernel on Sn with the series
expansion k(u, v) = α−2keven(u, v) + α−1kodd(u, v) + P∞
m=0 αm ⟨u, v⟩m, and kM : Sn ×
Sn →R be its M th partial sum. Deﬁne ψ : [−1, 1] →R as ψ(x) := P∞
m=0 αmxm. Then the
approximation error bound for the partial sum of the kernel is given by"
REFERENCES,0.9169811320754717,"|k(u, v) −kM(u, v)| ≤
1
(M + 1)!
max
x∈(−1,1) |ψ(M+1)(x)|,"
REFERENCES,0.9207547169811321,"for all (u, v) ∈Sn × Sn."
REFERENCES,0.9245283018867925,"Proof. Since k is positive deﬁnite, from Theorem 4.3, it has a series expansion of the form:"
REFERENCES,0.9283018867924528,"k(u, v) = α−2J⟨u, v⟩∈{−1, 1}K + α−1(J⟨u, v⟩= 1K −J⟨u, v⟩= −1K) + ∞
X"
REFERENCES,0.9320754716981132,"m=0
αm ⟨u, v⟩m"
REFERENCES,0.9358490566037736,"= α−2J⟨u, v⟩∈{−1, 1}K + α−1(J⟨u, v⟩= 1K −J⟨u, v⟩= −1K) + ψ(⟨u, v⟩)."
REFERENCES,0.939622641509434,"Note that ψ is an analytic function. Furthermore,"
REFERENCES,0.9433962264150944,"kM(u, v) = α−2J⟨u, v⟩∈{−1, 1}K + α−1(J⟨u, v⟩= 1K −J⟨u, v⟩= −1K) + M
X"
REFERENCES,0.9471698113207547,"m=0
αm ⟨u, v⟩m"
REFERENCES,0.9509433962264151,"= α−2J⟨u, v⟩∈{−1, 1}K + α−1(J⟨u, v⟩= 1K −J⟨u, v⟩= −1K) + ψM(⟨u, v⟩),"
REFERENCES,0.9547169811320755,"where ψM(x) := PM
m=0 αmxm. Therefore, ψM(x) is the M th order Maclaurin polynomial approx-
imation of ψ(x). Since ψ(x) is analytic, and therefore inﬁnitely differentiable, we can obtain the
Lagrange form of the approximation error as:"
REFERENCES,0.9584905660377359,ψ(x) −ψM(x) = ψ(M+1)(ξ)
REFERENCES,0.9622641509433962,"(M + 1)! x(M+1),"
REFERENCES,0.9660377358490566,"for some ξ ∈(−x, x), for all x ∈[−1, 1], where ψ(M+1) is the (M + 1)th order derivative of ψ. It
follows that, for all (u, v) ∈Sn × Sn,"
REFERENCES,0.969811320754717,"|k(u, v) −kM(u, v)| ≤
max
x∈[−1,1] |ψ(x) −ψM(x)|"
REFERENCES,0.9735849056603774,"≤
1
(M + 1)!
max
x∈(−1,1) |ψ(M+1)(x)|."
REFERENCES,0.9773584905660377,"The above theorem states that the absolute error made by cutting off the terms beyond order M is
less than or equal to the maximum absolute value of the (M + 1)th order derivative of k, attenuated
by a factor of 1/(M + 1)!. To put this into context, since we use M = 10, the attenuation factor is
around 2.5 × 10−8. Therefore, to make the error signiﬁcant, the 11th order derivative of the kernel
would have to be very high, suggesting a kernel function with abrupt changes looking almost like
discontinuities. Since such functions are unlikely to be useful to learn a generalizable model, we
believe that the error caused by this approximation is indeed negligible. Note also that, we set kM to"
REFERENCES,0.9811320754716981,Under review as a conference paper at ICLR 2022
REFERENCES,0.9849056603773585,"be the Maclaurin polynomial in the proof of Theorem E.1 to make the derivations easier. However,
since we have the freedom to learn the coefﬁcients of the summation, it is theoretically possible to
approximate k even better. In particular, even when k has abrupt changes causing a non-negligible
error in the Maclaurin approximation, it will be possible to capture some of the residuals using the
kernels kodd and keven in kM’s expansion. This is because the higher order kernels reach one of these
kernels in the limit as discussed in the proof of Theorem 4.3."
REFERENCES,0.9886792452830189,"F
COMPARISON TO NON-PARAMETRIC KERNEL METHODS"
REFERENCES,0.9924528301886792,"We use a parametric model with kernels in our method. This is in contrast to the more popu-
lar usage of kernels with non-parametric models, such as support vector machines and Gaussian
processes. Non-parametric models are usually more ﬂexible. They are also more interpretable
than deep network-based parametric models. However, unfortunately, non-parametric models scale
poorly with the train set size. For example, the number of support vectors grow linearly with the
train set size (Steinwart, 2003), and Gaussian processes methods scale with the cube of the train set
size, or linearly after some optimizations (Wilson & Nickisch, 2015)."
REFERENCES,0.9962264150943396,"In contrast, parametric models, such as the one proposed in this work, scale well with the training set
size since they use a constant number of parameters regardless of the number of training examples.
In fact, this is one of the main reasons why deep learning methods have become extremely popular
in the recent years. Non-parametric models also allow faster inference, making them suitable for
developing models for compute-limited scenarios, which is the primary focus of this work."
