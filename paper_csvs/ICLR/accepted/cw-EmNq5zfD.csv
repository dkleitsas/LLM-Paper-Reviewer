Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002680965147453083,"The recent trend of using large-scale deep neural networks (DNN) to boost per-
formance has propelled the development of the parallel pipelining technique for
efﬁcient DNN training, which has resulted in the development of several prominent
pipelines such as GPipe, PipeDream, and PipeDream-2BW. However, the current
leading pipeline PipeDream-2BW still suffers from two major drawbacks, i.e., the
excessive memory redundancy and the delayed weight updates across all stages.
In this work, we propose a novel pipeline named WPipe, which achieves better
memory efﬁciency and fresher weight updates. WPipe uses a novel pipelining
scheme that divides model partitions into two groups. It moves the forward pass of
the next period of weight updates to the front of the backward pass of the current
period of weight updates in the ﬁrst group, retains the order in the second group,
and updates each group alternatively. This scheme can eliminate half of the delayed
gradients and memory redundancy compared to PipeDream-2BW. The experiments,
which train large BERT language models, show that compared to PipeDream-2BW,
WPipe achieves 1.4× acceleration and reduces the memory footprint by 36%,
without nearly sacriﬁcing any ﬁnal model accuracy."
INTRODUCTION,0.005361930294906166,"1
INTRODUCTION"
INTRODUCTION,0.00804289544235925,"Several recent lines of research (Liu et al., 2019; Yang et al., 2019; Lan et al., 2019; Raffel et al., 2019;
Brown et al., 2020; Lin et al., 2021) on various application domains have collectively demonstrated
that larger DNN models can yield better performance. This creates an emerging trend in scaling up
the number of model parameters, resulting in very large DNNs, the memory footprint of which will
be beyond the limit of a single accelerator."
INTRODUCTION,0.010723860589812333,"To resolve the aforementioned problem, researchers and practitioners have focused on model paral-
lelism, which allows to further scale up model parameters signiﬁcantly by partitioning a large model
over multiple accelerators/workers. However, the conventional model parallelism, which includes
inter-layer model parallelism (Narayanan et al., 2021) and intra-layer model parallelism (Shoeybi
et al., 2019), either suffers from low resource utilization or high communication overhead. Recently,
some studies proposed the pipeline-parallel technique to accelerate conventional model-parallel
training. The pipelining technique can effectively improve the utilization of computing resources by
scheduling different workers to consume different mini-batches in parallel. However, naive pipelining
cannot be directly used for DNN training due to two problems: (1) staleness of weight updates
(Harlap et al., 2018), which is caused by the inconsistency of the weight versions used in a forward
pass and its corresponding backward pass, and (2) excessive in-ﬂight activations, which are heaped
up by continuous forward activations waiting for their corresponding backward passes in pipelining."
INTRODUCTION,0.013404825737265416,"To address the staleness problem, some workable pipelines have been proposed with different
emphases on low memory footprint and high throughput. GPipe proposes the periodic pipeline
ﬂushing technique, which maintains only one weight version. This technique can achieve a low
memory footprint and almost the same weight update semantics as data parallelism, but at the cost of
introducing bubble overhead (Huang et al., 2019), thus limiting throughput. PipeDream proposes the
weights stashing (Harlap et al., 2018) technique to eliminate bubble overhead, resulting in higher"
INTRODUCTION,0.0160857908847185,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01876675603217158,"throughput; however, the multiple weight versions incur a high memory footprint. To further reduce
the weight versions maintained in PipeDream, PipeDream-2BW was proposed in which the double-
buffered weight update technique (Narayanan et al., 2021) is used, which largely reduces the number
of weight versions and enables similar weight update semantics as that in data parallelism."
INTRODUCTION,0.021447721179624665,"To reduce in-ﬂight activations, GPipe and PipeDream-2BW adopt activation recomputation (Chen
et al., 2016; Jain et al., 2019). Recomputation enables the forward pass to execute without retaining
the internal activations, which are recomputed in the corresponding backward pass. This method
of being recomputed when needed can effectively alleviate the stacking up of in-ﬂight activations
and greatly reduce the memory footprint. Although both GPipe and PipeDream-2BW utilize the
recomputation technique, PipeDream-2BW has fewer in-ﬂight activations than GPipe owing to the
more timely execution of the backward pass in its pipeline. Regarding Pipedream, unfortunately, it
doesn’t take effective measures on this issue, which prevents it from training a larger model."
INTRODUCTION,0.024128686327077747,"Currently, PipeDream-2BW is the pipeline-parallel system with the best overall performance. However,
it still suffers from two main problems: delayed weight updates and excessive memory redundancy on
weight versions and activations (Narayanan et al., 2021). In this paper, we propose a novel pipeline,
named WPipe, to solve these two problems. In WPipe, model partitions are divided into two groups,
namely G0 and G1, which share the same workers. In the execution pipeline, the forward pass of
the next update period is moved to the front of the backward pass of the current update period in
G0, and the execution order in G1 is retained, and then, G0 and G1 are updated alternatively. We
named the alternate update technique for the grouping weights as double-grouped weight updates
(2GW). Compared to 2BW, 2GW has two main improvements: (1) 2GW realizes the same weight update
semantics as that in data parallelism and only maintains one weight version without introducing
bubble overhead in G1. (2) 2GW reduces half of the in-ﬂight activations since only half of the weights
are involved in the training at a time."
INTRODUCTION,0.02680965147453083,"Additionally, we incorporate some conventional but effective communication optimization techniques
including the overlap of computation and communication, hybrid parallelism, and heterogeneous
communication to further reduce communication overhead. Finally, our throughput experiments
show that WPipe achieves acceleration of 7.1× compared to that of PipeDream and 2.1× compared
to that of GPipe for Bert192 with 1.4 billion parameters. When training models with up to 5.5
billion parameters, which cannot be trained by GPipe and PipeDream, WPipe is 1.4× faster than
PipeDream-2BW. Memory experiments show that compared to PipeDream-2BW, WPipe reduces the
memory footprint by 35.8% for Bert96. Convergence experiments show that WPipe can achieve
similar ﬁnal accuracy to data parallelism."
BACKGROUND,0.029490616621983913,"2
BACKGROUND"
BACKGROUND,0.032171581769437,"In this section, we brieﬂy introduce the related techniques of model parallelism."
BACKGROUND,0.03485254691689008,"Model parallelism. Model parallelism (Chen et al., 2018; Chilimbi et al., 2014; Dean et al., 2012;
Jia et al., 2018) partitions a large model into multiple parts and assigns them to different workers.
Each worker is responsible for their own weight updates and sending and receiving intermediate
activations and gradients. The conventional model parallelism includes inter-layer model parallelism
and intra-layer model parallelism (Shoeybi et al., 2019). The former suffers from underutilized
resources as only one worker is active at a time, as shown in Figure 1a. The latter requires all-to-all
aggregation with intermediate activations/gradients for each layer. The all-to-all communication
overhead, which grows in proportion to the number of layers, makes it difﬁcult to expand to deeper
layers, especially in heterogeneous network interconnections. 1
1 1
1 Time"
BACKGROUND,0.03753351206434316,Worker0
BACKGROUND,0.040214477211796246,Worker1
BACKGROUND,0.04289544235924933,"Forward Pass
Backward Pass
Idle"
BACKGROUND,0.045576407506702415,Worker0
BACKGROUND,0.04825737265415549,Worker1
BACKGROUND,0.05093833780160858,"Time
(a)
(b)"
BACKGROUND,0.05361930294906166,"1
2
1
3
2
4
3"
BACKGROUND,0.05630026809651475,"1
1
2
2
3
3
4"
BACKGROUND,0.058981233243967826,Figure 1: The execution timelines of inter-layer model parallelism and naive pipeline parallelism.
BACKGROUND,0.06166219839142091,Published as a conference paper at ICLR 2022
BACKGROUND,0.064343163538874,"Pipeline Parallelism. Pipeline parallelism can effectively improve the utilization of computing
resources of inter-layer model parallelism by scheduling different workers to consume different
mini-batches simultaneously. However, naive pipeline parallelism suffers from a staleness issue for
weight updates. As shown in Figure 1b, when the backward pass of the latter mini-batch is executed,
its corresponding weights could be updated by the prior mini-batch. For example, for mini-batch 2,
the weights used in the backward pass were updated by mini-batch 1."
BACKGROUND,0.06702412868632708,"Existing solutions tackle the issue by trading off throughput and memory footprint. GPipe proposes
the periodic pipeline ﬂushing technique (Huang et al., 2019). As shown in Figure 2a, it splits a mini-
batch into multiple smaller micro-batches, which are injected into the model continuously to achieve
pipelining. Within each pipeline period, GPipe can accumulate gradients across micro-batches and
ﬂush synchronously. In this way, it achieves the same weight update semantics as data parallelism
and only maintains one weight version. However, in GPipe, bubble overhead (Huang et al., 2019)
limits throughput, especially when a large mini-batch cannot be supported. PipeDream-ﬂush is
essentially the same as GPipe (Narayanan et al., 2021), and its main improvement is to move forward
the backward pass, thereby releasing in-ﬂight activations as soon as possible and reducing memory
footprint. Time"
BACKGROUND,0.06970509383378017,Worker0
BACKGROUND,0.07238605898123325,Worker1
BACKGROUND,0.07506702412868632,"1
2
2
1
Update"
BACKGROUND,0.0777479892761394,"1
2
2
1
Update"
BACKGROUND,0.08042895442359249,"𝑊!! 𝑊! "" 𝑊""!"
BACKGROUND,0.08310991957104558,"𝑊!! 𝑊!"""
BACKGROUND,0.08579088471849866,"𝑊""! 𝑊"""""
BACKGROUND,0.08847184986595175,"Weight 
stashing"
BACKGROUND,0.09115281501340483,"Forward Pass
Backward Pass
Idle"
BACKGROUND,0.0938337801608579,Worker0
BACKGROUND,0.09651474530831099,Worker1
BACKGROUND,0.09919571045576407,Worker0
BACKGROUND,0.10187667560321716,Worker1 Time
BACKGROUND,0.10455764075067024,"Time
2BW"
BACKGROUND,0.10723860589812333,"(a)
(c) (d)"
BACKGROUND,0.10991957104557641,"1
2
1
3
2
4
3
5
4"
BACKGROUND,0.1126005361930295,"1
1
2
2
3
3
4
4
5"
BACKGROUND,0.11528150134048257,"1
2
1
3
2
4
3
5
4"
BACKGROUND,0.11796246648793565,"1
1
2
2
3
3
4
4
5"
BACKGROUND,0.12064343163538874,Update
BACKGROUND,0.12332439678284182,"1
2
1
2
Update"
BACKGROUND,0.1260053619302949,"1
1
2
2
Update Time"
BACKGROUND,0.128686327077748,Worker0
BACKGROUND,0.13136729222520108,Worker1 (b)
BACKGROUND,0.13404825737265416,"Figure 2: Timelines of various pipeline-parallel executions: (a) GPipe updates the weights in a
mini-batch. (b) PipeDream-ﬂush moves backward pass forward compared to GPipe. (c) PipeDream
implements weight stashing to update the weight immediately by default. (d) PipeDream-2BW
realizes periodic updates through gradient accumulation. For example, when the update period is 2,
micro-batches 2 and 4 update weights in the ﬁgure."
BACKGROUND,0.13672922252010725,"PipeDream uses the weight stashing technique to tackle the staleness issue without introducing
the bubble overhead, and thus achieves higher throughput (Harlap et al., 2018). However, it must
maintain a large number of weight versions. From Figure 2b worker i(i = 0, 1, ...) comprises N −i
weight versions, where N is the number of model partitions. This results in a high memory footprint,
especially when N is large. Moreover, in PipeDream, the weight update semantics has different delay
terms at different stages."
BACKGROUND,0.13941018766756033,"PipeDream-2BW (Narayanan et al., 2021), as an upgraded version of PipeDream, has higher through-
put and more memory efﬁciency. As shown in Figure 2c, it uses double-buffered weight updates
(2BW), which is combined with gradient accumulation, to reduce effectively the number of weight
versions maintained on each worker from N −i to 2. Notably, in 2BW, its weight update semantics
has only one weight delay term at each stage (Narayanan et al., 2021)."
SYSTEM DESIGN OF WPIPE,0.14209115281501342,"3
SYSTEM DESIGN OF WPIPE"
SYSTEM DESIGN OF WPIPE,0.1447721179624665,"In this section, we ﬁrst analyze the problems of 2BW. Then, we propose double-grouped weight
updates (2GW) and analyze 2GW from three aspects: model partitions grouping, effective learning,
and memory footprint. Finally, we introduce several communication optimization techniques."
INTRODUCTION,0.14745308310991956,"3.1
INTRODUCTION"
INTRODUCTION,0.15013404825737264,"To simplify the description, we denote the naive model-parallel timeline as V2 (where “2” represents
the number of model partitions), as shown in Figure 1a. Then we denote the process of cross-splicing
V2s to form a pipeline as P(nV2) (where n represents the number of V2), as shown in Figure 1b.
Analyzing the execution regular of the pipeline in Figure 1b, we can ﬁnd that P(nV2) can ensure that"
INTRODUCTION,0.15281501340482573,Published as a conference paper at ICLR 2022
INTRODUCTION,0.1554959785522788,"Forward Pass
Backward Pass
Idle"
INTRODUCTION,0.1581769436997319,"1
2
1
2
3
4
3
4
5
6"
INTRODUCTION,0.16085790884718498,"1
1
2
2
3
3
4
4
5
5"
INTRODUCTION,0.16353887399463807,Introduced Idle
INTRODUCTION,0.16621983914209115,Figure 3: Separate two adjacent update periods in PipeDream-2BW by introducing idle time blocks.
INTRODUCTION,0.16890080428954424,"the backward pass is executed immediately after the forward pass is completed so that the forward
activations can be released in time to alleviate the stacking up of the activations. However, the P(nV2)
cannot be used directly due to the inconsistency issue discussed above. To address the issue, 2BW
uses two weight versions and combines gradient accumulation. Why does 2BW work effectively?
The point is that 2BW effectively handles the intersection of adjacent update periods. As shown in
Figure 2c, the forward pass for micro-batch 3 is executed before the backward pass for micro-batch
2, but its backward pass is executed after the backward pass for micro-batch 2. At this time, it is
necessary to continue to use the old weight version in the period of micro-batches 3 and 4, and the
new weight version (updated by micro-batch 2) will be used in the next period (micro-batches 5
and 6). Thus, if we want to further reduce the number of weight versions, we need to eliminate the
intersections of micro-batches 2 and 3, 4 and 5, etc. To achieve this, we temporarily introduce idle
time blocks, as shown in Figure 3 (its pipeline is the same as PipeDream-ﬂush)."
INTRODUCTION,0.17158176943699732,"If these idle time blocks cannot be eliminated, can we ﬁll them? Through the statistical analysis, we
found that for a pipeline of PipeDream-2BW with n model partitions, at least the idle time blocks
shaped like P(xVn), x >= (n −1) are required to eliminate the intersections. In addition, a pipeline
accumulation period has at least execution time blocks shaped like P(xVn), x >= n. Obviously,
when x >= n, they have the same shape. Thus, can we ﬁll these idle time blocks with the pipelining
of another part of the weights of the same model?"
INTRODUCTION,0.1742627345844504,"3.2
DOUBLE-GROUPED WEIGHT UPDATES (2GW)"
INTRODUCTION,0.1769436997319035,"In this subsection, we introduce the 2GW technique from two points: model partitions grouping and
weight update semantics."
INTRODUCTION,0.17962466487935658,"1
1
1
1"
INTRODUCTION,0.18230563002680966,"1
1
1
1"
INTRODUCTION,0.18498659517426275,"1
1
1
2
1"
INTRODUCTION,0.1876675603217158,"1
1
1
2
1"
INTRODUCTION,0.1903485254691689,"(a)
(b) (c) G0,F"
INTRODUCTION,0.19302949061662197,"G1, F&B G0,B"
INTRODUCTION,0.19571045576407506,"G1,F1&B1
G0,F2&B1"
INTRODUCTION,0.19839142091152814,"𝐺! Forward Pass
𝐺! Backward Pass
Idle
𝐺"" Forward Pass
𝐺"" Backward Pass"
INTRODUCTION,0.20107238605898123,"1
2
1
2
1
3
2
4
1
3
2
4
3
5
4"
INTRODUCTION,0.2037533512064343,"1
2
1
1
2
2
3
1
4
2
3
3
4
4
5"
INTRODUCTION,0.2064343163538874,"Figure 4: Derivation of WPipe pipeline: (a) Further partitioning and grouping, (b) moving the G0
forward pass of the next step to the front of the backward pass of G0 of the current, (c) expansion of
the pipelining for (b)."
INTRODUCTION,0.20911528150134048,"Model Partitions Grouping. As shown in Figure 4a, based on Figure 1a, we further split each
model partition into two and obtain twice the number of model partitions. Then, we divide them
into two groups, G0 and G1, and train them on the same devices. For G1, it is a typical V2 structure,
which can realize continuous pipeline expansion through P. But for G0, since its forward pass and
backward pass are separated, a continuous pipeline cannot be directly achieved by cross-splicing
(P). Therefore, we ﬁrst need to convert it into a V2 structure. By analyzing the execution rules of the
pipeline after grouping, we found that moving the forward pass of G0 of the next step to the front
of the backward pass of G0 of the current step can form a V2, as shown in Figure 4b. After doing
this, we can cross-splice G0 and G1 respectively to obtain P(G0) and P(G1), and then continue to
cross-splice their results to obtain P(P(G0), P(G1)). For G1, its micro-batches 2 and 3 can be well"
INTRODUCTION,0.21179624664879357,Published as a conference paper at ICLR 2022
INTRODUCTION,0.21447721179624665,"Table 1: Details of the memory footprint of pipeline systems of GPipe, PipeDream, PipeDream-2BW,
and WPipe, where we ignore the intermediate activations for recomputation."
INTRODUCTION,0.21715817694369974,"PIPELINE
BUBBLE
RATIO
THE SIZE OF
ALL BUFFERS
ACTIVATIONS
(NON-RECOMPUTATION)
ACTIVATIONS
(RECOMPUTATION)"
INTRODUCTION,0.21983914209115282,"GPIPE
N−1
M+N−1
0
MSa
Sa
PIPEDREAM
0
N+1"
SM,0.2225201072386059,"2
Sm
2M−N+1"
"SA
NONE",0.225201072386059,"2
Sa
NONE
PIPEDREAM-2BW
0
2Sm
2M−N+1"
"SA
SA
WPIPE",0.22788203753351208,"2
Sa
Sa
WPIPE
0
Sm
4M−N+1"
SA,0.23056300268096513,"4
Sa
0.5Sa"
SA,0.23324396782841822,"separated by the pipelining of G0 (P(G0)), as shown in Figure 4c. This also answers the question
mentioned in section 3.1. The pipelining of G0 can ﬁll in the idle time blocks in Figure 3. Time"
SA,0.2359249329758713,Stage0
SA,0.2386058981233244,Stage1
SA,0.24128686327077747,Stage2
SA,0.24396782841823056,Stage3
SA,0.24664879356568364,"𝐺! Forward Pass
𝐺! Backward Pass
Idle
𝐺"" Forward Pass
𝐺"" Backward Pass"
SA,0.24932975871313673,"𝑊!! 𝑊! "" 𝑊"" ! 𝑊"" "" 𝑊# ! 𝑊# "" 𝑊$ ! 𝑊$ "" 𝐺!"
SA,0.2520107238605898,"1
2
3
4
1
2
3
4
1
5
2
6
3
7
4
8
1
5
2
6
3
7
4"
SA,0.2546916890080429,"1
2
3
4
1
2
3
1
4
2
5
3
6
4
7
1
8
2
5
3
6
4
7"
SA,0.257372654155496,"1
2
3
4
1
2
1
3
2
4
3
5
4
6
1
7
2
8
3
5
4
6
5"
SA,0.26005361930294907,"1
2
3
4
1
1
2
2
3
3
4
4
5
1
6
2
7
3
8
4
5
5
6"
SA,0.26273458445040215,"Figure 5: Timeline of execution of 2GW, where only G0 needs double weight versions."
SA,0.26541554959785524,"Weight Update Semantics. As shown in Figure 4c, G1 only maintains one weight version and has
the same weight update semantics as data parallelism. For G0, due to the movement operation, the
intersections naturally exist, such as micro-batches 3, 4, and 2 in Figure 4c. Nevertheless, G0 only
needs to maintain two weight versions and has only one delay term in the weight update semantics,
which is the same as 2BW. If using SGD optimizer, the weight update semantics of 2GW are as
follows:
wt+1
Gi = wt
Gi −ν · ▽f(w(t−1+i)
Gi1
, w(t−1+i)
Gi2
, ..., w(t−1+i)
Gin
), i ∈{0, 1}.
(1)"
SA,0.2680965147453083,"Other optimizers can be similarly analyzed. Figure 5 shows a more detailed pipeline-parallel timeline
of 2GW. In each pipeline stage, the model partitions of G0 and G1 are executed alternately without
introducing bubbles after the system is warmed up."
MEMORY FOOTPRINT ANALYSIS,0.2707774798927614,"3.3
MEMORY FOOTPRINT ANALYSIS"
MEMORY FOOTPRINT ANALYSIS,0.2734584450402145,"The memory footprint of most pipelines is mainly divided into four parts, i.e., naive model weights,
weight buffers (saving history weight versions), in-ﬂight activations, and optimizers. Since the
naive model weights and optimizers are roughly the same for each pipeline method, we mainly
analyze weight buffers and in-ﬂight activations. In WPipe, G1 has no weight buffer, and the size of
G0’s weight buffer is 2. We use Sp to represent the size of the entire model parameters, MG0 and
MG1 represent the sizes of G0 and G1, respectively. Supposing that the model is evenly divided
(MG1 = MG0), the size of weight buffer of WPipe is M = 2MG1 = MG0 + MG1 = Sp. For
in-ﬂight activations, we use Sa to indicate the activations of the entire model for a micro-batch. From
Figure 5, we statistically conclude that the in-ﬂight activation amount of stagei is equal to M ∗Sa"
N,0.2761394101876676,"2N
for G0 and (M −i) ∗Sa"
N,0.27882037533512066,"2N for G1, and of all stages is equal to:"
N,0.28150134048257375,"i=N−1
X"
N,0.28418230563002683,"i=0
(2M −i) ∗Sa"
N,0.2868632707774799,2N = 4M −N + 1
N,0.289544235924933,"4
∗Sa,
(2)"
N,0.29222520107238603,"where the N and M indicate the number of pipeline stages and micro-batches, respectively, and
M >= N. The same statistical method is used for GPipe, PipeDream, and PipeDream-2BW to obtain
the size of all weight buffers and in-ﬂight activations, respectively, as shown in Table 1."
N,0.2949061662198391,"With Activation Recomputation. From Table 1, the in-ﬂight activations increase linearly with the
number of pipeline stages, which will become a memory bottleneck when training a large model. Thus,
a pipeline system must take measures to reduce in-ﬂight activations. The activation recomputation is"
N,0.2975871313672922,Published as a conference paper at ICLR 2022
N,0.3002680965147453,"an effective method widely accepted, especially for 2GW. It can eliminate the stacking up of historical
in-ﬂight activations, leaving only the activations in an active state in the system, as shown in Table 1.
From the table, we can summarize the following points: (a) compared to PipeDream-2BW, the weight
buffer size of WPipe is reduced by half, and the superiority is more obvious compared to PipeDream.
Although GPipe has no weight buffer, it has to introduce
N−1
M+N−1 bubble overhead; (b) recomputation
can signiﬁcantly reduce in-ﬂight activations for GPipe, PipeDream-2BW, and especially WPipe."
COMMUNICATION OPTIMIZATION,0.30294906166219837,"3.4
COMMUNICATION OPTIMIZATION"
COMMUNICATION OPTIMIZATION,0.30563002680965146,"We use Ca to represent the cost of intermediate activations/gradients in one communication, and then
the model-parallel communication overheads of different pipeline systems are CWPipe = 2(2Ns −
1)Ca, C2BW = 2(Ns −1)Ca, and CGPipe = 2(Ns −1)Ca, respectively, where Ns indicates the
number of stages. They show that with the same number of stages, the communication overhead of
WPipe is twice that of GPipe and PipeDream-2BW. Nevertheless, in most cases, Ca is small and can
be ignored, especially when training some language models, such as Transformer (Vaswani et al.,
2017). When Ca is large, We have the following countermeasures:"
COMMUNICATION OPTIMIZATION,0.30831099195710454,stage0
COMMUNICATION OPTIMIZATION,0.3109919571045576,stage1
COMMUNICATION OPTIMIZATION,0.3136729222520107,stage2
COMMUNICATION OPTIMIZATION,0.3163538873994638,stage3
COMMUNICATION OPTIMIZATION,0.3190348525469169,"G00
G10"
COMMUNICATION OPTIMIZATION,0.32171581769436997,"G01
G11"
COMMUNICATION OPTIMIZATION,0.32439678284182305,"G02
G12"
COMMUNICATION OPTIMIZATION,0.32707774798927614,"G03
G13"
COMMUNICATION OPTIMIZATION,0.3297587131367292,"G00
G10"
COMMUNICATION OPTIMIZATION,0.3324396782841823,"G01
G11"
COMMUNICATION OPTIMIZATION,0.3351206434316354,"G02
G12"
COMMUNICATION OPTIMIZATION,0.3378016085790885,"G03
G13"
COMMUNICATION OPTIMIZATION,0.34048257372654156,"G00
G10"
COMMUNICATION OPTIMIZATION,0.34316353887399464,"G01
G11"
COMMUNICATION OPTIMIZATION,0.34584450402144773,"G02
G12"
COMMUNICATION OPTIMIZATION,0.3485254691689008,"G03
G13"
COMMUNICATION OPTIMIZATION,0.3512064343163539,"G00
G10"
COMMUNICATION OPTIMIZATION,0.353887399463807,"G01
G11"
COMMUNICATION OPTIMIZATION,0.35656836461126007,"G02
G12"
COMMUNICATION OPTIMIZATION,0.35924932975871315,"G03
G13"
COMMUNICATION OPTIMIZATION,0.36193029490616624,"Model Parallelism Depth
Data Parallelism Width"
COMMUNICATION OPTIMIZATION,0.3646112600536193,"Figure 6: Layouts of model parallelism and data
parallelism."
COMMUNICATION OPTIMIZATION,0.3672922252010724,"Combination with Data Parallelism.
Nor-
mally, too large Ca is caused by too large a
micro-batch. we can proportionally reduce the
depth of the pipeline while reducing the size of
the micro-batch, and then we proportionally in-
crease the width of data parallelism to maintain
the same global batch size, as shown in Fig-
ure 6. As a result, the size of the micro-batch be-
comes smaller, and the Ca also decreases, while
the number of accelerators and the global batch
size remain unchanged. Of course, we need
to weigh the communication overhead between
model parallelism (CWPipe) and data parallelism
(CDP) to choose the appropriate ratio of depth
(d) and width (w). When CWPipe is greater than CDP, we reduce the value of d : w, d ∗w = NGP U.
Otherwise, we increase the value of d : w."
COMMUNICATION OPTIMIZATION,0.3699731903485255,"Overlap of Computation and Communication & Heterogeneous Network Communication.
The former can almost offset the overhead of activation recomputation. The latter can effectively use
heterogeneous networks to balance communication overhead. Please refer to the appendix for details.
In addition, some communication compression techniques are also shown in the appendix."
EXPERIMENTS,0.3726541554959786,"4
EXPERIMENTS"
EXPERIMENTS,0.3753351206434316,"WPipe is implemented with PyTorch-1.4 (Edward Z. Yang, 2021) and executes on two environments,
i.e., a single machine with eight 16-GB V100 GPUs (Env-1) and a private cluster with 8 × 8V100
GPUs (Env-2)."
QUALITY OF CONVERGENCE,0.3780160857908847,"4.1
QUALITY OF CONVERGENCE"
QUALITY OF CONVERGENCE,0.3806970509383378,"In this section, we compare the convergences of WPipe and PipeDream-2BW by comparing the
accuracy when training the same model on the same dataset with the same hyperparameters."
QUALITY OF CONVERGENCE,0.38337801608579086,"Text Classiﬁcation. We ﬁnetuned BERTBASE (Devlin et al., 2018) and BERTLARGE (Devlin et al.,
2018) for WPipe, PipeDream-2BW, and data parallelism on the QQP and MNLI tasks (Wang et al.,
2018). We used respectively bert-base-uncase and bert-large-uncase pre-training
weights from transformers-3.5.0 (Wolf et al., 2020). We used Adam optimizer, a learning
rate of 8 × 10−5(ν = 8 × 10−5) with 1000 steps warmup(ws = 1000) and a mini-batch size of
256(b = 256) for BERTBASE and the same optimizer, ν = 4 × 10−5 with ws = 2000 and b = 128
for BERTLARGE. From Table 2, WPipe, PipeDream-2BW, and data parallelism have similar ﬁnal
accuracy. For further analysis, we continue to perform image classiﬁcation experiments."
QUALITY OF CONVERGENCE,0.38605898123324395,Published as a conference paper at ICLR 2022
QUALITY OF CONVERGENCE,0.38873994638069703,"Table 2: The results of the convergence experiment of WPipe. We train the models three times with
different seeds. Then, we calculated the means and standard deviations of the results. Where DP
represents data parallelism and PD-2BW represents PipeDream-2BW."
QUALITY OF CONVERGENCE,0.3914209115281501,"TASKS
MODEL
METRIC
DP
WPIPE
PD-2BW"
QUALITY OF CONVERGENCE,0.3941018766756032,"QQP
BERTBASE
ACC
87.66 ± 0.06
87.68 ± 0.06
87.63 ± 0.03
F1
83.39 ± 0.02
83.39± 0.04
83.34± 0.05"
QUALITY OF CONVERGENCE,0.3967828418230563,"BERTLARGE
ACC
87.63 ± 0.05
87.59± 0.07
87.38 ± 0.02
F1
83.26± 0.08
83.21± 0.11
82.96 ± 0.05"
QUALITY OF CONVERGENCE,0.39946380697050937,"MNLI
BERTBASE
ACC
82.81± 0.01
83.05± 0.19
82.98± 0.14
M-ACC
83.16± 0.03
83.04± 0.29
82.82 ± 0.26"
QUALITY OF CONVERGENCE,0.40214477211796246,"BERTLARGE
ACC
86.16 ± 0.12
86.25 ± 0.21
86.29 ± 0.12
M-ACC
86.05±0.26
86.15 ± 0.26
86.08 ± 0.24"
QUALITY OF CONVERGENCE,0.40482573726541554,"OXFORD
FLOWERS102
RESNEXT50 32X4D
ACC
99.55 ± 0.14
99.47± 0.14
99.51 ± 0.12
RESNEXT101 32X8D
ACC
99.39 ± 0.21
99.19± 0.19
99.19 ± 0.19"
QUALITY OF CONVERGENCE,0.4075067024128686,"CIFAR10
RESNEXT50 32X4D
ACC
97.15± 0.10
97.28± 0.20
97.26± 0.09
RESNEXT101 32X8D
ACC
98.15 ± 0.07
98.18 ± 0.07
98.15 ± 0.07"
QUALITY OF CONVERGENCE,0.4101876675603217,"CIFAR100
RESNEXT50 32X4D
ACC
85.67 ± 0.12
85.62± 0.09
85.80 ± 0.21
RESNEXT101 32X8D
ACC
87.90 ± 0.30
88.00 ± 0.21
87.93 ± 0.18"
QUALITY OF CONVERGENCE,0.4128686327077748,"Image Classiﬁcation. We ﬁnetuned, respectively, the ResNeXt50 (32x4d) (Xie et al., 2017) and
ResNeXt101 (32x8d) (Xie et al., 2017) for WPipe, PipeDream-2BW, and data parallelism on the
three datasets of CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and
Oxford 102 Flowers (Nilsback & Zisserman, 2008). We used the pre-training weights from the
torchvision (Francisco Massa, 2021), SDG optimizer, ν = 1 × 10−2 with 0.05 warmup ratio
and b = 256. From Table 2, there is still no obvious difference in their ﬁnal accuracy. Thus, we
continue to analyze the changes in loss of the three methods during training, while supplementing the
training experiments from scratch."
QUALITY OF CONVERGENCE,0.4155495978552279,"Intermediate Metrics Analysis. As shown in Figures 7a-7c, although WPipe, PipeDream-2BW, and
data parallelism have similar ﬁnal loss and accuracy, the curve of WPipe is closer to data parallelism.
Among them, the loss of data parallelism drops the fastest, and its accuracy rises the fastest, and
WPipe is second only to data parallelism, which beneﬁts from the fact that the weight update delay of
WPipe is reduced by half, compared to PipeDream-2BW. For more loss curves and accuracy curves,
please refer to the appendix."
QUALITY OF CONVERGENCE,0.41823056300268097,"0
3
6
9
12
15
Epochs 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Loss"
QUALITY OF CONVERGENCE,0.42091152815013405,"Data Parallelism
PipeDream-2BW
WPipe"
QUALITY OF CONVERGENCE,0.42359249329758714,(a) QQP:BERTLARGE.
QUALITY OF CONVERGENCE,0.4262734584450402,"0
3
6
9
12
15
Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Loss"
QUALITY OF CONVERGENCE,0.4289544235924933,"Data Parallelism
PipeDream-2BW
WPipe"
QUALITY OF CONVERGENCE,0.4316353887399464,(b) MNLI:BERTLARGE.
QUALITY OF CONVERGENCE,0.4343163538873995,"0
5
10
15
20
25
30
Epochs 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ACC"
QUALITY OF CONVERGENCE,0.43699731903485256,"WPipe
PipeDream-2BW
Data Parallelism"
QUALITY OF CONVERGENCE,0.43967828418230565,(c) Cifar10:ResNeXt50.
QUALITY OF CONVERGENCE,0.44235924932975873,"Figure 7: Part of the training loss from Table 2 and the accuracy when training ResNeXt50 from
scratch with WPipe, PipeDream-2BW, and Data Parallelism (SGD with a ﬁxed learning rate of 0.01)."
THROUGHPUT,0.4450402144772118,"4.2
THROUGHPUT"
THROUGHPUT,0.4477211796246649,"In this section, we measure the throughputs of WPipe through training the large-scale DNN models
on Env-1 and Env-2 and compare them with other pipeline systems. The main factors that affect
throughput are batch size (B), the ratio of model parallelism to data parallelism (d : w), and the
possibility to recompute (Sa) as presented below:"
THROUGHPUT,0.450402144772118,"T = max(Ts∈Sa×Sd:w×SM ); B = b ∗M ∗w, M >= d; Sa = {True, False};
Sd:w = {H(d : w)|d ∗w = NGP U}; SM = {d, d + 1, ...};
(3)"
THROUGHPUT,0.45308310991957107,Published as a conference paper at ICLR 2022
THROUGHPUT,0.45576407506702415,"where T is the optimal result in an optimization space Sa × Sd:w × SM. H(d : w) represents all
isomers in the same d : w. SM represents the gradient accumulation period. With regards to the
optimal solution of T, the study in (Narayanan et al., 2021) gives a workable solution, we do not do
further research. Note that since B directly affects the accuracy of the model, it is often a given value."
COMPARISON WITH PIPELINED APPROACHES,0.4584450402144772,"4.2.1
COMPARISON WITH PIPELINED APPROACHES"
COMPARISON WITH PIPELINED APPROACHES,0.46112600536193027,"As shown in Figure 8, throughput experiments are carried out through training different layers of
BERT and ResNeXt models on the Env-1 and Env-2. For more throughput experiment data, please
refer to the appendix. In general, the communication overhead of data parallelism is larger for BERT,
and model parallelism is larger for ResNeXt."
COMPARISON WITH PIPELINED APPROACHES,0.46380697050938335,"Comparison to PipeDream-2BW. In general, the suitable batch size is between 29 and 211. If it is too
large or too small, it is not conducive to the convergence of the model. However, in most cases, when
training large models, some methods cannot reach this batch size, as shown in Figure 8a. When in this
range, as shown in Figure 8c, when batch = 29, WPipe is 1.4 times faster than PipeDream-2BW for
Bert768 with 5.5 billion parameters. Compared to 2BW, WPipe not only reduces the cost of switching
between weighted versions by half but also has a larger solution space when the batch is larger. As
shown in Figure 8d, when batch = 29, WPipe can run in the conﬁguration of d : w = 4 : 16 in
Env-2, and 2BW due to memory limitations, the model-parallel depth is at least 8, that is, d >= 8, so
in this case, the throughput of WPipe is higher (for convolutional networks, in general, Ca is larger
and CDP is smaller, so d : w needs to be smaller to improve throughput). From Figures 8b and 8d, it
can be concluded that after communication optimization, even on a convolutional network with a
larger Ca, WPipe has a throughput not lower than PiprDream-2BW."
COMPARISON WITH PIPELINED APPROACHES,0.46648793565683644,"Comparison to GPipe. For GPipe, its biggest problem is that there are idle time blocks on its
execution timeline. In addition, its forward activations cannot be released in time. These factors
directly or indirectly affect its throughput. Compared to GPipe, from Figure 8a- 8d, WPipe is 2.1×
faster for Bert192 and 2.2× faster for ResNeXt302."
COMPARISON WITH PIPELINED APPROACHES,0.4691689008042895,"Comparison to PipeDream. Due to poor memory efﬁciency, the larger models cannot be trained
using PipeDream, such as Bert384 and Bert768. Practice shows that for ResNeXt200, ResNeXt302,
ResNeXt500, and ResNeXt800, PipeDream cannot automatically split them. According to existing
data, WPipe is 7.1× faster for Bert192."
COMPARISON WITH PIPELINED APPROACHES,0.4718498659517426,"3
5
7
9
Global mini-batch size (ln2) 10 20 30 40 50"
COMPARISON WITH PIPELINED APPROACHES,0.4745308310991957,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.4772117962466488,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.47989276139410186,"WPipe
2BW
GPipe
PipeDream"
COMPARISON WITH PIPELINED APPROACHES,0.48257372654155495,(a) Bert192(8V100s).
COMPARISON WITH PIPELINED APPROACHES,0.48525469168900803,"3
5
7
Global mini-batch size (ln2) 25 50 75 100 125 150 175"
COMPARISON WITH PIPELINED APPROACHES,0.4879356568364611,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.4906166219839142,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.4932975871313673,"WPipe
2BW
GPipe"
COMPARISON WITH PIPELINED APPROACHES,0.4959785522788204,(b) ResNeXt302(8V100s).
COMPARISON WITH PIPELINED APPROACHES,0.49865951742627346,"6
8
10
Global mini-batch size (ln2) 20 40 60 80 100"
COMPARISON WITH PIPELINED APPROACHES,0.5013404825737265,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.5040214477211796,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.5067024128686327,"WPipe
2BW"
COMPARISON WITH PIPELINED APPROACHES,0.5093833780160858,(c) Bert768(64V100s).
COMPARISON WITH PIPELINED APPROACHES,0.5120643431635389,"6
8
10
Global mini-batch size (ln2) 0 100 200 300 400"
COMPARISON WITH PIPELINED APPROACHES,0.514745308310992,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.517426273458445,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.5201072386058981,"WPipe
2BW"
COMPARISON WITH PIPELINED APPROACHES,0.5227882037533512,(d) ResNeXt800(64V100s).
COMPARISON WITH PIPELINED APPROACHES,0.5254691689008043,"6
8
10
12
Global mini-batch size (ln2) 50 100 150 200"
COMPARISON WITH PIPELINED APPROACHES,0.5281501340482574,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.5308310991957105,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.5335120643431636,"WPipe
WPipe-R
2BW
2BW-R"
COMPARISON WITH PIPELINED APPROACHES,0.5361930294906166,"(e) Bert384(16:4, 64V100s)."
COMPARISON WITH PIPELINED APPROACHES,0.5388739946380697,"6
8
10
12
Global mini-batch size (ln2) 0 100 200 300 400 500 600"
COMPARISON WITH PIPELINED APPROACHES,0.5415549597855228,Throughput
COMPARISON WITH PIPELINED APPROACHES,0.5442359249329759,(samples/second)
COMPARISON WITH PIPELINED APPROACHES,0.546916890080429,"WPipe-R(4:16)
WPipe-R(8:8)
WPipe-R(16:4)
WPipe-R(32:2)
WPipe-R(64:1)"
COMPARISON WITH PIPELINED APPROACHES,0.5495978552278821,(f) ResNeXt500(64V100s).
COMPARISON WITH PIPELINED APPROACHES,0.5522788203753352,"Figure 8: Optimal throughput for different batches in the Env-1 and Env-2. Where SM:N = {2 :
4, 4 : 2, 8 : 1} in Env-1, SM:N = {4 : 16, 8 : 8, 16 : 4, 32 : 2, 64 : 1} in Env-2. 8e-8f show the
throughput changes with different conﬁgurations."
COMPARISON WITH PIPELINED APPROACHES,0.5549597855227882,Published as a conference paper at ICLR 2022
COMMUNICATION OPTIMIZATION ANALYSIS,0.5576407506702413,"4.2.2
COMMUNICATION OPTIMIZATION ANALYSIS"
COMMUNICATION OPTIMIZATION ANALYSIS,0.5603217158176944,"Recomputation. The activation recomputation can greatly reduce memory footprint and expand
mini-batch size for pipeline systems. As shown in Figure 8e, PipeDream-2BW can expand the
maximum mini-batch by 16 times, and WPipe can even expand by 44 times. WPipe has a more
obvious memory advantage with activation recomputation. Although the batch size is not as large as
possible, usually when training large models, the batch size must be within a certain range to have
better convergence. In many cases, due to memory limitations, the effective range of the batch size
cannot be reached. At this time the advantage of WPipe is crucial."
COMMUNICATION OPTIMIZATION ANALYSIS,0.5630026809651475,"Table 3: The impact of network
communication on throughput."
COMMUNICATION OPTIMIZATION ANALYSIS,0.5656836461126006,"Env-2(4:16)
ResNeXt500"
COMMUNICATION OPTIMIZATION ANALYSIS,0.5683646112600537,"WPipe-R(A)
607.8/3328
WPipe-R(B)
497.3/3328"
COMMUNICATION OPTIMIZATION ANALYSIS,0.5710455764075067,"The Ratio between Model and Data parallelism. As shown
in Figure 8f, as M : N decreases, the corresponding throughput
increases for ResNeXt500. This happens because the commu-
nication overhead of data parallelism is smaller than that of
model parallelism in the convolutional network. In this case,
M : N can be adjusted more widely, which is more conducive
to improving throughput, e.g. batch = 211 in Figure 8f."
COMMUNICATION OPTIMIZATION ANALYSIS,0.5737265415549598,"Heterogeneous Network Settings. We change the communi-
cation placement of model parallelism and data parallelism in
the same M : N to achieve higher throughput. A means that data-parallel communication takes place
between machines and model-parallel communication takes place inside machines, while B is the
opposite. As shown in Table 3, in the case of A, the throughput of WPipe is higher, because the
model-parallel communication is the bottleneck for ResNeXt. Compared with the inter-machine, the
bandwidth inside the machine is higher and the communication speed is faster. The model-parallel
communication bottleneck has been better alleviated."
MEMORY OPTIMIZATION ANALYSIS,0.5764075067024129,"4.3
MEMORY OPTIMIZATION ANALYSIS 0 2 4 6 8 10 12 14 16"
MEMORY OPTIMIZATION ANALYSIS,0.579088471849866,"Bert96(8)
Bert96(64)
ResNeXt200(32)
ResNeXt200(64)"
MEMORY OPTIMIZATION ANALYSIS,0.5817694369973191,Memory footprint(GB)
MEMORY OPTIMIZATION ANALYSIS,0.5844504021447721,"DP
GPipe
GPipe-R
PipeDream
PipeDream-2BW
PipeDream-2BW-R
WPipe
WPipe-R OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM"
MEMORY OPTIMIZATION ANALYSIS,0.5871313672922251,"Figure 9: The Bert96 and ResNeXt200 memory
footprint vary with batch size. We set M : N as 8 :
1 for Bert96 and M : N as 2 : 4 for ResNeXt200,
which is the fastest conﬁguration. We measured
the average maximum memory footprint per GPU."
MEMORY OPTIMIZATION ANALYSIS,0.5898123324396782,"In the throughput experiment, WPipe can exe-
cute on a larger batch, which also reﬂects the ad-
vantage of the lower memory footprint of WPipe.
Here, we conduct a more detailed analysis: (1)
The results of experiments show that the mem-
ory footprint of various pipeline-parallel meth-
ods is signiﬁcantly reduced with activations re-
computation, especially for WPipe. As shown in
Figure 9, WPipe reduces the memory footprint
by 65% for Bert96 on the per-GPU micro-batch
of 64, and 56% for RestNeXt200 on the per-
GPU micro-batch of 32. They are 47% and 29%
respectively for PipeDream-2BW; (2) with the in-
crease of batch size, the effect of recomputation
on WPipe-R is more signiﬁcant. From Figure 9,
for Bert96, WPipe-R is reduced by -14% com-
pared to GPipe-R on the per-GPU micro-batch
of 8 (because GPipe has no weight buffer, and
its initial memory footprint is lower), and on the per-GPU micro-batch of 64, it increases to 28%.
Compared to PipeDream-2BW-R, under the same conditions, WPipe-R increases from 21% to 36%."
CONCLUSIONS,0.5924932975871313,"5
CONCLUSIONS"
CONCLUSIONS,0.5951742627345844,"In this work, we proposed and implemented WPipe, a system for group-based interleaved pipeline-
parallel training. Compared to the state-of-the-art approach, PipeDream-2BW, WPipe achieves better
memory efﬁciency, higher throughput, and fresher weight updates through the double-grouped
weight updates. Speciﬁcally, (1) throughput: WPipe achieves 1.4× acceleration; (2) memory
footprint: WPipe-R reduces the memory footprint by 36%; and (3) convergence: although WPipe and
PipeDream-2BW have similar ﬁnal accuracy when training, WPipe has a weight update semantics
closer to data parallelism."
CONCLUSIONS,0.5978552278820375,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.6005361930294906,ACKNOWLEDGMENTS
CONCLUSIONS,0.6032171581769437,"This work is supported by the Computational Intelligence Department of Ant Group. We thank
Jiyan Jiang for his helpful discussions. We would like to thank the Aliyun EFLOPS team for their
substantial support in designing the industry-leading training platform to facilitate fast trials in this
work. We also thank anonymous reviewers for their insightful and valuable comments."
REFERENCES,0.6058981233243967,REFERENCES
REFERENCES,0.6085790884718498,Nvlink. URL https://www.nvidia.com/en-us/data-center/nvlink/.
REFERENCES,0.6112600536193029,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.613941018766756,"Chi-Chung Chen, Chia-Lin Yang, and Hsiang-Yun Cheng. Efﬁcient and robust parallel dnn training
through model parallelism on multi-gpu platform. arXiv preprint arXiv:1809.02839, 2018."
REFERENCES,0.6166219839142091,"Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174, 2016."
REFERENCES,0.6193029490616622,"Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efﬁcient and scalable deep learning training system. In 11th {USENIX} Symposium
on Operating Systems Design and Implementation ({OSDI} 14), pp. 571–582, 2014."
REFERENCES,0.6219839142091153,"Thor Johnsen. Christian Sarofeen. Nvidia/apex. https://github.com/NVIDIA/apex, 2021."
REFERENCES,0.6246648793565683,"Jeffrey Dean, Greg S Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V Le, Mark Z Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, et al. Large scale distributed deep networks.
2012."
REFERENCES,0.6273458445040214,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.6300268096514745,"Soumith Chintala. Edward Z. Yang.
pytorch/pytorch.
https://github.com/pytorch/
pytorch, 2021."
REFERENCES,0.6327077747989276,"NicolasHug. Francisco Massa. pytorch/vision. https://github.com/pytorch/vision,
2021."
REFERENCES,0.6353887399463807,"Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger,
and Phil Gibbons. Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv preprint
arXiv:1806.03377, 2018."
REFERENCES,0.6380697050938338,"Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong
Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient training of giant neural
networks using pipeline parallelism. In Advances in neural information processing systems, pp.
103–112, 2019."
REFERENCES,0.6407506702412868,"Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion
Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor
rematerialization. arXiv preprint arXiv:1910.02653, 2019."
REFERENCES,0.6434316353887399,"Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep neural
networks. arXiv preprint arXiv:1807.05358, 2018."
REFERENCES,0.646112600536193,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.6487935656836461,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019."
REFERENCES,0.6514745308310992,Published as a conference paper at ICLR 2022
REFERENCES,0.6541554959785523,"Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang
Wang, Le Jiang, Xianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint
arXiv:2103.00823, 2021."
REFERENCES,0.6568364611260054,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.6595174262734584,"Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efﬁcient
pipeline-parallel dnn training. In International Conference on Machine Learning, pp. 7937–7947.
PMLR, 2021."
REFERENCES,0.6621983914209115,"M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In
Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec
2008."
REFERENCES,0.6648793565683646,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.6675603217158177,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model
parallelism. arXiv preprint arXiv:1909.08053, 2019."
REFERENCES,0.6702412868632708,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.6729222520107239,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018."
REFERENCES,0.675603217158177,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language
processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6."
REFERENCES,0.67828418230563,"Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.6809651474530831,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V
Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019."
REFERENCES,0.6836461126005362,Published as a conference paper at ICLR 2022
REFERENCES,0.6863270777479893,"A
APPENDIX"
REFERENCES,0.6890080428954424,"A.1
OTHER EXPERIMENTAL DATA OF CONVERGENCE."
REFERENCES,0.6916890080428955,"Figures 10a-10h show the training loss curves of the remaining experiment in Table 2. Figure 10i
shows the F1 curve of BERTBASE when training from scratch. Analyzing the trend of the curve, we
found that WPipe and data parallelism can converge normally, but PipeDream-2BW does not."
REFERENCES,0.6943699731903485,"0
3
6
9
12
15
Epochs 0.0 0.1 0.2 0.3 0.4 0.5 Loss"
REFERENCES,0.6970509383378016,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.6997319034852547,(a) QQP:BERTBASE.
REFERENCES,0.7024128686327078,"0
3
6
9
12
15
Epochs 0.0 0.2 0.4 0.6 Loss"
REFERENCES,0.7050938337801609,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.707774798927614,(b) MNLI:BERTBASE.
REFERENCES,0.710455764075067,"0
23
47
71
95
Epochs 0 1 2 3 4 Loss"
REFERENCES,0.7131367292225201,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.7158176943699732,(c) Flowers102:ResNeXt50.
REFERENCES,0.7184986595174263,"0
23
47
71
95
Epochs 0 1 2 3 4 Loss"
REFERENCES,0.7211796246648794,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.7238605898123325,(d) Flowers102:ResNeXt101.
REFERENCES,0.7265415549597856,"0
8
17
26
35
Epochs 0.5 1.0 1.5 2.0 Loss"
REFERENCES,0.7292225201072386,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.7319034852546917,(e) Cifar10:ResNeXt50.
REFERENCES,0.7345844504021448,"0
8
17
26
35
Epochs 0.5 1.0 1.5 2.0 Loss"
REFERENCES,0.7372654155495979,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.739946380697051,(f) Cifar10:ResNeXt101.
REFERENCES,0.7426273458445041,"0
8
17
26
35
Epochs 1 2 3 4 Loss"
REFERENCES,0.7453083109919572,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.7479892761394102,(g) Cifar100:ResNeXt50.
REFERENCES,0.7506702412868632,"0
8
17
26
35
Epochs 1 2 3 4 Loss"
REFERENCES,0.7533512064343163,"Data Parallelism
PipeDream-2BW
WPipe"
REFERENCES,0.7560321715817694,(h) Cifar100:ResNeXt101.
REFERENCES,0.7587131367292225,"0
5
10
15
20
25
30
Epochs 0.575 0.600 0.625 0.650 0.675 0.700 0.725 0.750 F1"
REFERENCES,0.7613941018766756,"WPipe
PipeDream-2BW
DataParallel"
REFERENCES,0.7640750670241286,(i) QQP:BERTBASE.
REFERENCES,0.7667560321715817,"Figure 10: The remaining part of the training loss from Table2 and the F1 when training BERTBASE
from scratch with WPipe, PipeDream-2BW, and DataParallel (Adam with a ﬁxed learning rate of
5 × 10−6)."
REFERENCES,0.7694369973190348,"A.2
THROUGHPUT"
REFERENCES,0.7721179624664879,"In the experiment, our settings of models are hidden size=768, num attention heads=12,
seq len=128 of all Bert models, and groups=32, width per group=4 of all ResNeXt mod-
els. Regarding cluster conﬁguration, there are 8 machines in our private cluster, and each machine
has 8 GPUs with a memory size of 16G, Intel(R)Xeon(R) Platinum 8163 CPU, 512GB of RAM with
a 25Gbps Ethernet interface, and 300GBps NVLink (nvl), which is 96 times the Ethernet bandwidth.
In addition, the version of PyTorch we used was 1.4."
REFERENCES,0.774798927613941,"As shown in Figure 11a-11d, we continue to train Bert96 and ResNeXt200 on a single machine with
WPipe, PipeDream-2BW, PipeDream, and GPipe, and train Bert384 and ResNeXt500 on multiple
machines with WPipe and PipeDream-2BW. The overall conclusion is the same as the above. WPipe
has a more obvious acceleration effect on Transformer series networks (Bert, GDP, etc.), but has a
slight acceleration on convolutional networks, compared to PipeDream-2BW."
REFERENCES,0.7774798927613941,Published as a conference paper at ICLR 2022
REFERENCES,0.7801608579088471,"3
5
7
9
Global mini-batch size (ln2) 20 40 60 80 100"
REFERENCES,0.7828418230563002,Throughput
REFERENCES,0.7855227882037533,(samples/second)
REFERENCES,0.7882037533512064,"WPipe
2BW
GPipe
PipeDream"
REFERENCES,0.7908847184986595,(a) Bert96(8V100s).
REFERENCES,0.7935656836461126,"3
5
7
9
Global mini-batch size (ln2) 50 100 150 200 250"
REFERENCES,0.7962466487935657,Throughput
REFERENCES,0.7989276139410187,(samples/second)
REFERENCES,0.8016085790884718,"WPipe
2BW
GPipe"
REFERENCES,0.8042895442359249,(b) ResNeXt200(8V100s).
REFERENCES,0.806970509383378,"6
8
10
12
Global mini-batch size (ln2) 50 100 150 200"
REFERENCES,0.8096514745308311,Throughput
REFERENCES,0.8123324396782842,(samples/second)
REFERENCES,0.8150134048257373,"WPipe
2BW"
REFERENCES,0.8176943699731903,(c) Bert384(64V100s).
REFERENCES,0.8203753351206434,"6
8
10
12
Global mini-batch size (ln2) 0 100 200 300 400 500 600"
REFERENCES,0.8230563002680965,Throughput
REFERENCES,0.8257372654155496,(samples/second)
REFERENCES,0.8284182305630027,"WPipe
2BW"
REFERENCES,0.8310991957104558,(d) ResNeXt500(64V100s).
REFERENCES,0.8337801608579088,"6
8
10
Global mini-batch size (ln2) 0 100 200 300 400"
REFERENCES,0.8364611260053619,Throughput
REFERENCES,0.839142091152815,(samples/second)
REFERENCES,0.8418230563002681,"WPipe-R-APC
WPipe-R-NonAPC
PipeDream-2BW-R"
REFERENCES,0.8445040214477212,"(e) ResNeXt800(4:16, 64V100s)."
REFERENCES,0.8471849865951743,"6
8
10
12
Global mini-batch size (ln2) 20 40 60 80 100"
REFERENCES,0.8498659517426274,Throughput
REFERENCES,0.8525469168900804,(samples/second)
REFERENCES,0.8552278820375335,"WPipe-R-APC
WPipe-R-NonAPC
PipeDream-2BW-R"
REFERENCES,0.8579088471849866,"(f) Bert768(4:16, 64V100s)."
REFERENCES,0.8605898123324397,"Figure 11: Optimal throughput for different batches in the Env-1 and Env-2. Where SM:N = {2 :
4, 4 : 2, 8 : 1} in Env-1, SM:N = {4 : 16, 8 : 8, 16 : 4, 32 : 2, 64 : 1} in Env-2. Figures 11e-11f
show the effect of compressed model-parallel communication on different models."
REFERENCES,0.8632707774798928,"A.3
COMMUNICATION OPTIMIZATION"
REFERENCES,0.8659517426273459,"In this section, we use a speciﬁc example to analyze the effectiveness of the model-parallel communi-
cation compression. In addition, we add a detailed description of overlap and heterogeneous network
communication."
REFERENCES,0.868632707774799,Sender
REFERENCES,0.871313672922252,Dynamic Scale
REFERENCES,0.8739946380697051,Activations/Gradients
REFERENCES,0.8766756032171582,Receiver
REFERENCES,0.8793565683646113,"Unscale
tensor_fp16,"
REFERENCES,0.8820375335120644,scaler
REFERENCES,0.8847184986595175,Activations/Gradients
REFERENCES,0.8873994638069705,"Figure 12: The communication process of the inter-
mediate activations and gradients using automatic
precision compression."
REFERENCES,0.8900804289544236,"Communication Compression. Referring to
the automatic mixed precision algorithm of
the apex (Christian Sarofeen, 2021), we im-
plemented the automatic precision compression
technique for intermediate activations/gradients,
which reduced the communication overhead by
nearly half across the pipeline. This offsets the
communication overhead introduced by further
splitting model partitions. As shown in Fig-
ure 12, the sender divides the intermediate acti-
vations/gradients by appropriate scalers (pow-
ers of 2) and converts them into half-precision
tensors. If the conversion succeeds, the sender will transmit the half-precision tensors together
with the scalers to the receiver. After receiving the data, the receiver restores their original pre-
cision by the scaler. If the conversion fails, the sender will transmit the full precision tensors."
REFERENCES,0.8927613941018767,"Table 4: The impact of network commu-
nication on throughput."
REFERENCES,0.8954423592493298,"Env-2(4:16)
ResNeXt500"
REFERENCES,0.8981233243967829,"WPipe-R-APC
732.6/3328
WPipe-R-NonAPC
607.8/3328"
REFERENCES,0.900804289544236,"Automatic Precision Compression (APC). As shown in
Table 4, WPipe is 1.17× faster with APC when training
ResNeXt500 and the batch size equals 3328. The loss
(the negative value) of accuracy, which is brought by APC,
does not exceed 0.1%, as shown in Table 5. However,
when the communication overhead is not large, the use
of APC will not speed up but will be slower. As shown
in Figure 11f, when batch <= 211, Ca is small, and
the acceleration beneﬁt cannot exceed the compression
overhead. When batch > 211, there is positive feedback.
For convolutional networks, Ca is large, and APC has always had a positive effect, but the effect
is only obvious when the batch size is large enough, as shown in Figure 11e. In summary, when"
REFERENCES,0.903485254691689,Published as a conference paper at ICLR 2022
REFERENCES,0.9061662198391421,"Table 5: The accuracy difference between using APC and not using APC with the same hyperparam-
eters. We set ν = 8 × 10−5, b = 32 × 8, ws = 200, epoch = 1 for BERTBASE and ν = 4 × 10−5,
b = 16 × 8, ws = 100, epoch = 1 for BERTLARGE and ν = 0.01, b = 32 × 8, wr = 0.05 for
ResNeXt50 32x4d and ResNeXt101 32x8d."
REFERENCES,0.9088471849865952,"BERTBASE
BERTLARGE"
REFERENCES,0.9115281501340483,"▽F1
-0.0003 ± 0.0011
0.0035 ± 0.0127"
REFERENCES,0.9142091152815014,"RESNEXT50
RESNEXT101"
REFERENCES,0.9168900804289544,"▽ACC
-0.0008±0.0014
0.0012± 0.0012"
REFERENCES,0.9195710455764075,"the batch is large, we use the communication compression technique to have a positive beneﬁt, but
in many cases, we do not need to use such a large batch size to train the model, so in most cases,
model-parallel communication will not be a bottleneck."
REFERENCES,0.9222520107238605,"Overlap of Computation and Communication. As shown in Figure 13, the activations or gradients
communication can overlap with the forward pass, backward pass, and activation recomputation.
Especially for activation recomputation, its time overhead can be offset mostly."
REFERENCES,0.9249329758713136,"Forward
Backward
Recompute"
REFERENCES,0.9276139410187667,Forward
REFERENCES,0.9302949061662198,Activation Send
REFERENCES,0.9329758713136729,Gradient Recv
REFERENCES,0.935656836461126,Forward
REFERENCES,0.938337801608579,Activation Recv
REFERENCES,0.9410187667560321,Gradient Send
REFERENCES,0.9436997319034852,Figure 13: The overlap of model execution and activations/gradients.
REFERENCES,0.9463806970509383,"Heterogeneous Network Communication. Generally, the intra-machine bandwidth (NVLink) is
higher than the inter-machine bandwidth. In WPipe, the communication tasks mainly include
model-parallel communication and data-parallel communication. Thus, WPipe allows tasks with
large communication overhead to use the higher intra-machine bandwidth, and tasks with small
communication overhead to use inter-machine bandwidth, to balance communication overhead."
REFERENCES,0.9490616621983914,"A.4
MODEL PARTITIONS GROUPING"
REFERENCES,0.9517426273458445,"Model partitions
P1
P2
P3
P4
P1
P2
P3
P4
G0
G1"
REFERENCES,0.9544235924932976,"Pipeline stages
P1 P2 P3 P4 P1 P2"
REFERENCES,0.9571045576407506,Stage0
REFERENCES,0.9597855227882037,Stage1
REFERENCES,0.9624664879356568,Figure 14: The relationship between model partitions and pipeline stages.
REFERENCES,0.9651474530831099,"A.5
EXPANSION"
REFERENCES,0.967828418230563,"GPipe Grouping. As shown in Figure 15, the pipeline grouping technique can also be applied to
GPipe, thereby reducing GPipe’s bubbles. In Figure 15, the two model partitions are further divided"
REFERENCES,0.9705093833780161,Published as a conference paper at ICLR 2022
REFERENCES,0.9731903485254692,"into four and divided into two groups. At this time, the bubble is reduced by half. For further
discussion, when the number of groups is N, the bubble will be reduced to 1"
REFERENCES,0.9758713136729222,"N , but the model-parallel
communication overhead will increase by N times. However, if NVLink can be used, the impact of
increased communication overhead will be greatly reduced. F10 F20 F11"
REFERENCES,0.9785522788203753,"F21
B21
B20"
REFERENCES,0.9812332439678284,"B11
B10 F10 F20 F11 F21 F30 F40 F31"
REFERENCES,0.9839142091152815,"F41
B41
B40"
REFERENCES,0.9865951742627346,"B31
B30
B21
B20"
REFERENCES,0.9892761394101877,"B11
B10"
REFERENCES,0.9919571045576407,Bubble
REFERENCES,0.9946380697050938,Bubble
REFERENCES,0.9973190348525469,Figure 15: The pipeline grouping is applied to GPipe.
