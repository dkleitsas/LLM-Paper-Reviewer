Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0017543859649122807,"We consider the ofﬂine constrained reinforcement learning (RL) problem, in
which the agent aims to compute a policy that maximizes expected return while
satisfying given cost constraints, learning only from a pre-collected dataset. This
problem setting is appealing in many real-world scenarios, where direct interac-
tion with the environment is costly or risky, and where the resulting policy should
comply with safety constraints. However, it is challenging to compute a policy that
guarantees satisfying the cost constraints in the ofﬂine RL setting, since the off-
policy evaluation inherently has an estimation error. In this paper, we present an
ofﬂine constrained RL algorithm that optimizes the policy in the space of the sta-
tionary distribution. Our algorithm, COptiDICE, directly estimates the stationary
distribution corrections of the optimal policy with respect to returns, while con-
straining the cost upper bound, with the goal of yielding a cost-conservative policy
for actual constraint satisfaction. Experimental results show that COptiDICE at-
tains better policies in terms of constraint satisfaction and return-maximization,
outperforming baseline algorithms."
INTRODUCTION,0.0035087719298245615,"1
INTRODUCTION"
INTRODUCTION,0.005263157894736842,"Reinforcement learning (RL) has shown great promise in a wide range of domains, such as com-
plex games (Mnih et al., 2015; Silver et al., 2017) and robotic control (Lillicrap et al., 2016;
Haarnoja et al., 2018). However, the need to interact with the environment during learning hinders
its widespread application to many real-world problems for which executing exploratory behavior
in the environment is costly or dangerous. Ofﬂine RL (also known as batch RL) (Lange et al., 2012;
Levine et al., 2020) algorithms sidestep this problem and perform policy optimization solely from a
set of pre-collected data. The use of existing (ofﬂine) data can make ofﬂine reinforcement learning
applicable to real world systems and and has led to a sharp increase in interest in this paradigm."
INTRODUCTION,0.007017543859649123,"Recent works on ofﬂine RL, however, mostly assume that the environment is modeled as a Markov
decision process (MDP), and standard ofﬂine RL algorithms focus on reward-maximization only
(Fujimoto et al., 2019; Wu et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Wang et al., 2020;
Kumar et al., 2020). In contrast, in real-world domains it is common that the behavior of the agent
is subject to additional constraints beyond the reward. Consider, for example, autonomous driving
or an industrial robot in a factory. Some behaviors may damage the agent itself or its surroundings,
and safety constraints should thus be considered as part of the objective of a suitable reinforce-
ment learning system. One of the ways to mathematically characterize a constrained RL problem
is through the formalism of constrained Markov Decision Processes (CMDP) (Altman, 1999). In
CMDPs taking an action incurs a cost as well as a reward, and the goal is to maximize the expected
long-term reward while satisfying a bound on the expected long-term cost. In this work, we aim to
solve the constrained decision making problem in the ofﬂine RL setting, to enable deployment in
various safety-critical domains where direct learning interactions are infeasible."
INTRODUCTION,0.008771929824561403,"Ofﬂine constrained RL inherits the difﬁculties of ofﬂine unconstrained RL, while introducing ad-
ditional challenges. First, since the target policy being optimized deviates from the data-collection"
INTRODUCTION,0.010526315789473684,∗Work done during an internship at DeepMind.
INTRODUCTION,0.012280701754385965,Published as a conference paper at ICLR 2022
INTRODUCTION,0.014035087719298246,"policy with no further data collection, distribution shift becomes the central difﬁculty. To mitigate
the distributional shift, existing ofﬂine RL methods frequently adopt the pessimism principle: either
by explicit policy (and critic) regularization that penalizes deviation from the data-collection policy
(Jaques et al., 2019; Wu et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Wang et al., 2020; Lee
et al., 2020; Kostrikov et al., 2021) or by reward penalty to the uncertain state-action regions (Kumar
et al., 2020; Kidambi et al., 2020; Yu et al., 2020). Second, in ofﬂine constrained RL, the computed
policy should satisfy the given cost constraints when it is deployed to the real environment. Unfor-
tunately off-policy policy evaluation inherently has estimation errors, and it is therefore difﬁcult to
ensure that a policy estimated from a ﬁnite dataset will satisfy the constraint when executed in the en-
vironment. In addition, constrained policy optimization usually involves an additional optimization
for the Lagrange multiplier associated with the cost constraints. Actor-critic-based constrained RL
algorithms thus have to solve triple (i.e. critic, actor, Lagrange multiplier) intertwined optimization
problems (Borkar, 2005; Tessler et al., 2019), which can be very unstable in practice."
INTRODUCTION,0.015789473684210527,"In this paper, we present an ofﬂine constrained RL algorithm that optimizes the state-action sta-
tionary distribution directly, rather than the Q-function or the policy. We show that such treatment
obviates the need for multiple estimators for the value and the policy, yielding a single optimiza-
tion objective that is practically solvable. Still, naively constraining the cost value may result in
severe constraint violation in the real environment, as we demonstrate empirically. We thus propose
a method to constrain the upper bound of the cost value, aiming to compute a policy more robust
in constraint violation, where the upper bound is computed in a way motivated by a recent advance
in off-policy conﬁdence interval estimation (Dai et al., 2020). Our algorithm, Ofﬂine Constrained
Policy Optimization via stationary DIstribution Correction Estimation (COptiDICE), estimates the
stationary distribution corrections of the optimal policy that maximizes rewards while constraining
the cost upper bound, with the goal of yielding a cost-conservative policy for better actual constraint
satisfaction. COptiDICE computes the upper bound of the cost efﬁciently by solving an additional
minimization problem. Experimental results show that COptiDICE attains a better policy in terms
of constraint satisfaction and reward-maximization, outperforming several baseline algorithms."
BACKGROUND,0.017543859649122806,"2
BACKGROUND"
BACKGROUND,0.01929824561403509,"A Constrained Markov Decision Process (CMDP) (Altman, 1999) is an extension of an MDP, for-
mally deﬁned by a tuple M = ⟨S, A, T, R, C = {Ck}1..K, ˆc = {ˆck}1..K, p0, γ⟩, where S is the set
of states, A is the set of actions, T : S × A →∆(S) is a transition probability, R : S × A →R
is the reward function, Ck : S × A →R is the k-th cost function with its corresponding threshold
ˆck ∈R, p0 ∈∆(S) is the initial state distribution, and γ ∈(0, 1] is the discount factor. A policy
π : S →∆(A) is a mapping from state to distribution over actions. We will express solving CMDPs
in terms of stationary distribution. For a given policy π, the stationary distribution dπ is deﬁned by:"
BACKGROUND,0.021052631578947368,"dπ(s, a) ="
BACKGROUND,0.02280701754385965,"


 

"
BACKGROUND,0.02456140350877193,"(1 −γ)
∞
P"
BACKGROUND,0.02631578947368421,"t=0
γt Pr(st = s, at = a)
if γ < 1,"
BACKGROUND,0.028070175438596492,"lim
T →∞
1
T +1 TP"
BACKGROUND,0.02982456140350877,"t=0
Pr(st = s, at = a) if γ = 1,
(1)"
BACKGROUND,0.031578947368421054,"where s0 ∼p0, at ∼π(st), st+1 ∼T(st, at) for all timesteps t ≥0. We deﬁne the value of the
policy as VR(π) := E(s,a)∼dπ[R(s, a)] ∈R and VC(π) := E(s,a)∼dπ[C(s, a)] ∈RK. Constrained
RL aims to learn an optimal policy that maximizes the reward while bounding the costs up to the
thresholds by interactions with the environment:"
BACKGROUND,0.03333333333333333,"max
π
VR(π) s.t. VCk(π) ≤ˆck
∀k = 1, . . . , K
(2)"
BACKGROUND,0.03508771929824561,"Lagrangian relaxation is typically employed to solve Eq. (2), leading to the unconstrained problem:"
BACKGROUND,0.03684210526315789,"min
λ≥0 max
π
VR(π) −λ⊤(VC(π) −ˆc)
(3)"
BACKGROUND,0.03859649122807018,"where λ ∈RK is the Lagrange multiplier and ˆc ∈RK is the vector-valued cost threshold. The inner
maximization in Eq. (3) corresponds to computing an optimal policy that maximizes scalarized
rewards R(s, a) −λ⊤C(s, a), due to the linearity of the value function with respect to the reward
function. The outer minimization corresponds to balancing the cost penalty in the scalarized reward"
BACKGROUND,0.04035087719298246,Published as a conference paper at ICLR 2022
BACKGROUND,0.042105263157894736,"function: if the current policy is violating the k-th cost constraint, λk increases so that the cost is
penalized more in the scalarized reward, and vice versa."
BACKGROUND,0.043859649122807015,"In the ofﬂine RL setting, online interaction with the environment is not allowed, and the policy is
optimized using the ﬁxed ofﬂine dataset D = {(s0, s, a, r, c, s′)i}N
i=1 collected with one or more
(unknown) data-collection policies. The empirical distribution of the dataset is denoted as dD, and
we will abuse the notation dD for s ∼dD, (s, a) ∼dD, (s, a, s′) ∼dD. We abuse (s0, s, a, s′) ∼
dD for s0 ∼p0, (s, a, s′) ∼dD. We denote the space of data samples (s0, s, a, s′) as X."
BACKGROUND,0.0456140350877193,"A naive way to solve (3) in an ofﬂine manner is to adopt an actor-critic based ofﬂine RL algorithm for
maxπ VR−λ⊤C(π) while jointly optimizing λ. However, the intertwined training procedure of off-
policy actor-critic algorithms often suffers from instability due to the compounding error incurred
by bootstrapping out-of-distribution action values in an ofﬂine RL setting (Kumar et al., 2019). The
instability would be exacerbated when the nested optimization for λ is added."
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.04736842105263158,"3
OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.04912280701754386,"In this section, we present our ofﬂine constrained RL algorithm, Constrained Policy Optimization
via stationary DIstribution Correction Estimation (COptiDICE). The derivation of our algorithm
starts by augmenting the standard linear program for CMDP (Altman, 1999) with an additional
f-divergence regularization:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.05087719298245614,"max
d
E(s,a)∼d[R(s, a)] −αDf(d||dD)
(4)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.05263157894736842,"s.t. E(s,a)∼d[Ck(s, a)] ≤ˆck
∀k = 1, . . . , K
(5)
P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.054385964912280704,"a′ d(s′, a′) = (1 −γ)p0(s′) + γ P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.056140350877192984,"s,a
d(s, a)T(s′|s, a)
∀s′
(6)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.05789473684210526,"d(s, a) ≥0
∀s, a,
(7)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.05964912280701754,"where Df(d||dD) := E(s,a)∼dD

f
  d(s,a)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.06140350877192982,"dD(s,a)

is the f-divergence between the distribution d and the
dataset distribution dD, and α > 0 is the hyperparameter that controls the degree of pessimism, i.e.
how much we penalize the distribution shift, a commonly adopted principle for ofﬂine RL (Nachum
et al., 2019b; Kidambi et al., 2020; Yu et al., 2020; Lee et al., 2021). We assume that dD > 0
and f is a strictly convex and continuously differentiable function with f(1) = 0. Note that when
α = 0, the optimization (4-7) reduces to the standard linear program for CMDPs. The Bellman-ﬂow
constraints (6-7) ensure that d is the stationary distribution of a some policy, where d(s, a) can be
interpreted as a normalized discounted occupancy measure of (s, a). Thus, we seek the stationary
distribution of an optimal policy that maximizes the reward value (4) while bounding the cost values
(5). Once the optimal solution d∗has been estimated, its corresponding optimal policy is obtained
by π∗(a|s) =
d∗(s,a)
P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.06315789473684211,"a′ d∗(s,a′)."
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.06491228070175438,"Now, consider the Lagrangian for the constrained optimization problem (4-7):"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.06666666666666667,"min
λ≥0,ν max
d≥0 E(s,a)∼d[R(s, a)] −αDf(d||dD) −
K
P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.06842105263157895,"k=1
λk
 
E(s,a)∼d[Ck(s, a)] −ˆck
 −P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07017543859649122,"s′ ν(s′)
h P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07192982456140351,"a′ d(s′, a′) −(1 −γ)p0(s′) −γ P"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07368421052631578,"s,a
d(s, a)T(s′|s, a)
i
(8)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07543859649122807,"where λ ∈RK
+ is the Lagrange multiplier for the cost constraints (5), and ν(s) ∈R is the Lagrange
multiplier for the Bellman ﬂow constraints (6). Solving (8) in its current form requires evaluation of
T(s′|s, a) for (s, a) ∼d, which is not accessible in the ofﬂine RL setting. To make the optimization
tractable, we rearrange the terms so that the direct dependence on d is eliminated, while introducing
new optimization variables w that represent stationary distribution corrections:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07719298245614035,"min
λ≥0,ν max
d≥0 E (s,a)∼d"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.07894736842105263,"s′∼T (s,a)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.08070175438596491,"
R(s, a) −λ⊤C(s, a) + γν(s′) −ν(s)

−αE(s,a)∼dD
h
f

d(s,a)
dD(s,a)
i"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.0824561403508772,+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.08421052631578947,"= min
λ≥0,ν max
w≥0 E(s,a)∼dD"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.08596491228070176,"
w(s, a)eλ,ν(s, a) −αf(w(s, a))

+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc
(9)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.08771929824561403,Published as a conference paper at ICLR 2022
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.08947368421052632,"where eλ,ν(s, a) := R(s, a) −λ⊤C(s, a) + γEs′∼T (s,a)[ν(s′)] −ν(s) is the advantage function by
regarding ν as a state value function, and w(s, a) :=
d(s,a)
dD(s,a) is the stationary distribution correction.
Every term in Eq. (9) can be estimated from samples in the ofﬂine dataset D:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.0912280701754386,"min
ν,λ≥0 max
w≥0 E(s0,s,a,s′)∼dD

w(s, a)ˆeλ,ν(s, a, s′) −αf(w(s, a)) + (1 −γ)ν(s0)

+ λ⊤ˆc
(10)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.09298245614035087,"where ˆeλ,ν(s, a, s′) := R(s, a)−λ⊤C(s, a)+γν(s′)−ν(s) is the advantage estimate using a single
sample. As a consequence, (10) can be optimized in a fully ofﬂine manner. Moreover, exploiting
the strict convexity of f, we can further derive a closed-form solution for the inner maximization in
(9) as follows. All the proofs can be found in Appendix B.
Proposition 1. For any ν and λ, the closed-form solution of the inner maximization problem in (9)
is given by:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.09473684210526316,"w∗
λ,ν(s, a) = (f ′)−1
1
αeλ,ν(s, a)
"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.09649122807017543,"+ where x+ = max(0, x)
(11)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.09824561403508772,"Finally, by plugging the closed-form solution (11) into (9), we obtain the following convex mini-
mization problem:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.1,"min
λ≥0,ν L(λ, ν) = E(s,a)∼dD

w∗
λ,ν(s, a)eλ,ν(s, a) −αf(w∗
λ,ν(s, a))

+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc (12)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.10175438596491228,"To sum up, by operating in the space of stationary distributions, constrained (ofﬂine) RL can in
principle be solved by solving a single convex minimization (12) problem. This is in contrast to
existing constrained RL algorithms that manipulate both Q-function and policy, and thus require
solving triple optimization problems for the actor, the critic, and the cost Lagrange multiplier with
three different objective functions. Note also that when λ is ﬁxed and treated as a constant, (12)
reduces to OptiDICE (Lee et al., 2021) for unconstrained RL with the scalarized rewards R(s, a) −
λ⊤C(s, a), without considering the cost constraints. In order to meet the constraints, λ should also
be optimized, and the procedure of (12) can be understood as joint optimization of:"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.10350877192982456,"ν ←arg min
ν
L(λ, ν)
(OptiDICE for R −λ⊤C)
(13)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.10526315789473684,"λ ←arg min
λ≥0
λ⊤ 
ˆc −E(s,a)∼dD[w∗
λ,ν(s, a)C(s, a)]
|
{z
}
≈VC(π)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.10701754385964912,"
(Cost Lagrange multiplier)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.10877192982456141,"Once the optimal solution of (12), (λ∗, ν∗), is computed, w∗
λ∗,ν∗(s, a) = dπ∗(s,a)"
"OFFLINE CONSTRAINED RL VIA STATIONARY DISTRIBUTION
CORRECTION ESTIMATION",0.11052631578947368,"dD(s,a) is also derived by
(11), which is the stationary distribution correction between the stationary distribution of the optimal
policy for the CMDP and the dataset distribution."
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.11228070175438597,"3.1
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.11403508771929824,"Our ﬁrst method based on (12) relies on off-policy evaluation (OPE) using DICE to ensure cost
constraint satisfaction, i.e. E(s,a)∼dD[wλ,ν(s, a)C(s, a)] ≈E(s,a)∼dπ[C(s, a)] ≤ˆc. However, as
we will see later, constraining the cost value estimate naively can result in constraint violation when
deployed to the real environment. This is due to the fact that an off-policy value estimate based on
a ﬁnite dataset inevitably has estimation error. Reward estimation error may be tolerated as long as
the value estimates are useful as policy improvement signals: it may be sufﬁcient to maintain the
relative order of action values, while the absolute values matter less. For the cost value constraint,
we instead rely on the estimated value directly."
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.11578947368421053,"To make a policy robust against cost constraint violation in an ofﬂine setting, we consider the con-
strained policy optimization scheme that exploits the upper bound of the cost value estimate:"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.11754385964912281,"max
π
ˆVR(π) s.t. UpperBound( ˆVCk(π)) ≤ˆck
∀k
(14)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.11929824561403508,"Then, the key question is how to estimate the upper bound of the policy value. One natural way
is to exploit bootstrap conﬁdence interval (Efron & Tibshirani, 1993; Hanna et al., 2017). We can"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.12105263157894737,Published as a conference paper at ICLR 2022
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.12280701754385964,"construct bootstrap datasets Di by resampling from D and run an OPE algorithm on each Di, which
yields population statistics for conﬁdence interval estimation { ˆVC(π)i}m
i=1. However, this procedure
is computationally very expensive since it requires solving m OPE tasks. Instead, we take a different
approach in a more computationally efﬁcient way motivated by CoinDICE (Dai et al., 2020), a
recently proposed DICE-family algorithm for off-policy conﬁdence interval estimation. Speciﬁcally,
given that our method estimates the stationary distribution corrections w(s, a) ≈
dπ(s,a)
dD(s,a) of the
target policy π, we consider the following optimization problem for each cost function Ck:"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.12456140350877193,"max
˜p∈∆(X) E(s0,s,a,s′)∼˜p[w(s, a)Ck(s, a)]
(15)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.12631578947368421,"s.t. DKL(˜p(s0, s, a, s′)||dD(s0, s, a, s′)) ≤ϵ
(16)
P"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1280701754385965,"a′ ˜p(s′, a′)w(s′, a′) = (1 −γ)˜p0(s′) + γ P"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.12982456140350876,"s,a
˜p(s, a)w(s, a)˜p(s′|s, a) ∀s′
(17)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.13157894736842105,"where ˜p(s0, s, a, s′) = ˜p0(s0)˜p(s, a)˜p(s′|s, a) is the distribution over data samples (s0, s, a, s′) ∈X
which lies in the simplex ∆(X), and ϵ > 0 is the hyperparameter. In essence, we want to adversari-
ally optimize the distribution over data samples ˜p so that it overestimates the cost value by (15). At
the same time, we enforce that the distribution ˜p should not be perturbed too much from the empirical
data distribution dD by the KL constraint (16). Lastly, the perturbation of distribution should be done
in a way that maintains compatibility with the Bellman ﬂow constraint. The constraint (17) is anal-
ogous to the Bellman ﬂow constraint (6) by noting that ˜p(s, a)w(s, a) = ˜p(s, a) dπ(s,a)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.13333333333333333,"dD(s,a) ≈dπ(s, a).
In this optimization, when ϵ = 0, the optimal solution is simply given by ˜p∗= dD, which yields
the vanilla OPE result via DICE, i.e. E(s,a)∼dD[w(s, a)Ck(s, a)]. For ϵ > 0, the cost value will be
overestimated more as ϵ increases. Through a derivation similar to that for obtaining (12), we can
simplify the constrained optimization into a single unconstrained minimization problem as follows.
We denote (s0, s, a, s′) as x for notational brevity.
Proposition 2. The constrained optimization problem (15-17) can be reduced to solving the follow-
ing unconstrained minimization problem:"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.13508771929824562,"min
τ≥0,χ ℓk(τ, χ; w) = τ log Ex∼dD
h
exp

1
τ
 
w(s, a)(Ck(s, a) + γχ(s′) −χ(s)) + (1 −γ)χ(s0)
i
+ τϵ (18)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1368421052631579,"where τ ∈R+ corresponds to the Lagrange multiplier for the constraint (16), and χ(s) ∈R
corresponds to the Lagrange multiplier for the constraint (17). In other words, minτ≥0,χ ℓ(τ, χ) =
E(s,a)∼˜p∗[w(s, a)Ck(s, a)] where ˜p∗is the optimal perturbed distribution of the problem (15-17).
Also, for the optimal solution (τ ∗, χ∗), ˜p∗is given by:"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.13859649122807016,"˜p∗(x) ∝dD(x) exp

1
τ ∗
 
w(s, a)(Ck(s, a) + γχ∗(s′) −χ∗(s)) + (1 −γ)χ∗(s0)
"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.14035087719298245,"|
{z
}
=: ω∗(x) (unnormalized weight for x = (s0, s, a, s′)) (19)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.14210526315789473,"Note that every term in (18) can be estimated only using samples of the ofﬂine dataset D, thus it can
be optimized in a fully ofﬂine manner. This procedure can be understood as computing the weights
for each sample while adopting reweighting in the DICE-based OPE, i.e. UpperBound( ˆVC(π)) =
Ex∼dD[˜ω∗(x)w(s, a)C(s, a)] where ˜ω∗(x) = (normalized ω∗(x) of (19)). The weights are given
non-uniformly so that the cost value is overestimated to the extent controlled by ϵ.
Remark. CoinDICE (Dai et al., 2020) solves the similar optimization problem to estimate an upper
cost value of the target policy π as follows:"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.14385964912280702,"max
w≥0 min
τ≥0,ν τ log E x∼dD"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1456140350877193,a0∼π(s0)
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.14736842105263157,a′∼π(s′)
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.14912280701754385,"h
exp

1
τ
 
w(s, a)(Ck(s, a) + γν(s′, a′) −ν(s′, a′)) + (1 −γ)ν(s0, a0)
i
+ τϵ (20)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.15087719298245614,"It is proven that (20) provides an asymptotic (1−α)-upper-conﬁdence-interval of the policy value if
ϵ := ξα"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.15263157894736842,"N where ξα is the (1 −α)-quantile of the χ2-distribution with 1 degree of freedom (Dai et al.,
2020). Compared to our optimization problem (18), CoinDICE’s (20) involves the additional outer
maximization, which is for estimating w(s, a) =
˜dπ(s,a)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1543859649122807,"˜p(s,a) , the stationary distribution corrections
of the target policy π. In contrast, we consider the case when w is given, thus solving the inner
minimization alone is enough."
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.156140350877193,Published as a conference paper at ICLR 2022
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.15789473684210525,"Finally, we are ready to present our ﬁnal algorithm COptiDICE, an ofﬂine constrained RL algorithm
that maximizes rewards while bounding the upper cost value, with the goal of computing a policy
robust against cost violation. COptiDICE addresses (14) by solving the following joint optimization.
ν ←arg min
ν
L(λ, ν)
(OptiDICE for R −λ⊤C)
(21)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.15964912280701754,"τ, χ ←arg min
τ≥0,χ"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.16140350877192983,"PK
k=1 ℓk(τk, χk; w∗
λ,ν)
(Upper cost value estimation)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1631578947368421,"λ ←arg min
λ≥0
λ⊤ 
ˆc −ℓ(τ, χ; w∗
λ,ν)
|
{z
}"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.1649122807017544,≈UpperBound( ˆVC(π))
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.16666666666666666,"
(Cost Lagrange multiplier)"
COST-CONSERVATIVE CONSTRAINED POLICY OPTIMIZATION,0.16842105263157894,"Compared to (13), the additional minimization for (τ, χ) is introduced to estimate the upper bound
of cost value."
POLICY EXTRACTION,0.17017543859649123,"3.2
POLICY EXTRACTION"
POLICY EXTRACTION,0.17192982456140352,"Our algorithm estimates the stationary distribution corrections of the optimal policy, rather than
directly obtaining the policy itself. Since the stationary distribution corrections do not provide a"
POLICY EXTRACTION,0.1736842105263158,"direct way to sample an action, we need to extract the optimal policy π∗from w∗(s, a) = dπ∗(s,a)"
POLICY EXTRACTION,0.17543859649122806,"dD(s,a) ,
in order to select actions when deployed. For ﬁnite CMDPs, it is straightforward to obtain π∗"
POLICY EXTRACTION,0.17719298245614035,"by π∗(a|s) =
dπ∗(s,a)
P"
POLICY EXTRACTION,0.17894736842105263,"a′ dπ∗(s,a′) =
dD(s,a)w∗(s,a)
P"
POLICY EXTRACTION,0.18070175438596492,"a′ dD(s,a)w∗(s,a). However, the same method cannot directly be
applied to continuous CMDPs due to the intractability of computing the normalization constant. For
continuous CMDPs, we instead extract the policy using importance-weighted behavioral cloning:
max
π
E(s,a)∼dπ∗[log π(a|s)] = E(s,a)∼dD[w∗(s, a) log π(a|s)]
(22)"
POLICY EXTRACTION,0.1824561403508772,which maximizes the log-likelihood of actions to be selected by the optimal policy π∗.
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.18421052631578946,"3.3
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.18596491228070175,"For continuous or large CMDPs, we represent our optimization variables using neural networks.
The Lagrange multipliers ν and χ are networks parameterized by θ and φ respectively: νθ : S →R
is a feedforward neural network that takes a state as an input and outputs a scalar value, and χφ :
S →RK is deﬁned similarly. λ ∈RK
+ and τ ∈RK
+ are represented by K-dimensional vectors. For
the policy πψ, we use a mixture density network (Bishop, 1994) where the parameters of a Gaussian
mixture model are output by the neural network. The parameters of the νθ network are trained by
minimizing the loss:
min
θ
Jν(θ) =Ex∼dD

ˆw(s, a, s′)(R(s, a) −λ⊤C(s, a) + γνθ(s′) −νθ(s))
(23)"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.18771929824561404,"−αf( ˆw(s, a, s′)) + (1 −γ)νθ(s0)

+ λ⊤ˆc"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.18947368421052632,"where ˆw(s, a, s′) := (f ′)−1  1"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.1912280701754386,"α(R(s, a) −λ⊤C(s, a) + γνθ(s′) −νθ(s))
"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.19298245614035087,"+. While Jλ
ν can be a
biased estimate of L(λ, ν) in (12) in general due to (f ′)−1(E[·]) ̸= E[(f ′)−1(·)], we can show that
Jλ
ν is an upper bound of L(ν, λ) (i.e. we minimize the upper bound), and Jλ
ν = L(ν, λ) holds if
transition dynamics are deterministic (e.g. Mujoco control tasks) (Lee et al., 2021). The parameters
of the χφ network and τ can be trained by:"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.19473684210526315,"min
τ≥0,φ K
P"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.19649122807017544,"k=1
τ log Ex∼dD
h
exp
  1"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.19824561403508772,"τ
 
ˆw(s, a, s′)(Ck(s, a) + γχφ,k(s′) −χφ,k(s))
(24)"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.2,"+ (1 −γ)χφ,k(s0)
i
+ τkϵ"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.20175438596491227,"This involves a logarithm outside of the expectation, which implies that mini-batch approximations
would introduce a bias. Still, we adopt the simple mini-batch approximation for computational
efﬁciency, with a moderately large batch size (e.g. 1024), which worked well in practice. The
empirical form of the loss we use is given by:"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.20350877192982456,"min
τ≥0,φ Jτ,χ(τ, φ) =Ebatch(D)∼D
h K
P"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.20526315789473684,"k=1
τ log Ex∼batch(D)

exp
  1"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.20701754385964913,"τ
 
ˆw(s, a, s′)·
(25)"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.20877192982456141,"(Ck(s, a) + γχφ,k(s′) −χφ,k(s)) + (1 −γ)χφ,k(s0)

+ τkϵ
i"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.21052631578947367,Published as a conference paper at ICLR 2022
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.21228070175438596,"Lastly, λ and the policy parameter ψ are optimized by:"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.21403508771929824,"min
λ≥0 Jλ(λ) = λ⊤(ˆc −Jτ,χ(τ, φ))
(26)"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.21578947368421053,"min
ψ Jπ(ψ) = −Ex∼dD[ ˆw(s, a, s′) log πψ(a|s)]
(27)"
PRACTICAL ALGORITHM WITH FUNCTION APPROXIMATION,0.21754385964912282,"The complete pseudo-code is described in Appendix C, where every parameter is optimized jointly."
EXPERIMENTS,0.21929824561403508,"4
EXPERIMENTS"
EXPERIMENTS,0.22105263157894736,"4.1
TABULAR CMDPS (RANDOMLY GENERATED CMDPS)"
EXPERIMENTS,0.22280701754385965,"We ﬁrst probe how COptiDICE can improve the reward performance beyond the data-collection
policy while satisfying the given cost constraint via repeated experiments. We follow a proto-
col similar to the random MDP experiment in (Laroche et al., 2019; Lee et al., 2021) but with
cost as an additional consideration.
We conduct repeated experiments for 10K runs, and for
each run, a CMDP M is generated randomly with the cost threshold ˆc = 0.1.
We test with
two types of data-collection policy πD, a constraint-satisfying policy (i.e.
VC(πD) = 0.09)
and a constraint-violating policy (i.e.
VC(πD) = 0.11).
Then, a varying number of trajecto-
ries N ∈{10, 20, 50, 100, 200, 500, 1000, 2000} are collected from the sampled CMDP using the
constructed data-collection policy πD, which constitutes the ofﬂine dataset D. Finally, the of-
ﬂine dataset D is given to each ofﬂine constrained RL algorithm, and its reward and cost perfor-
mance is evaluated. For the reward, we evaluate the normalized performance of the policy π by
VR(π)−VR(πD)
VR(π∗)−VR(πD) ∈(−∞, 1] to see the performance improvement of π over πD intuitively, where π∗"
EXPERIMENTS,0.22456140350877193,is the optimal policy of the underlying CMDP. More details can be found in Appendix E.
EXPERIMENTS,0.22631578947368422,"Ofﬂine constrained RL has been mostly unexplored, thus lacks published baseline algorithms. We
consider the following three baselines. First, BC denotes the simple behavior cloning algorithm to
see whether the proposed method is just remembering the dataset. Second, Baseline denotes
the algorithm that constructs a maximum-likelihood estimation (MLE) CMDP ˆ
M using D and then
solves the MLE CMDP using a tabular CMDP solver (LP solver) (Altman, 1999). Third, C-SPIBB
is the variant of SPIBB (an ofﬂine RL method for tabular MDPs) (Laroche et al., 2019), where we
modiﬁed SPIBB to deal with the cost constraint by Lagrange relaxation (Appendix E)."
EXPERIMENTS,0.22807017543859648,"Figure 1a presents the result when the data-collection policy is constraint-satisfying. The perfor-
mance of BC approaches the performance of the πD as the size of the dataset increases, as expected.
When the size of the ofﬂine dataset is very small, Baseline severely violates the cost constraint when
its computed policy is deployed to the real environment (cost red curve in Figure 1a), and it even
fails to improve the reward performance over πD (reward red curve in Figure 1a). This result is ex-
pected since Baseline overﬁts to the MLE CMDP, exploiting the highly uncertain state-actions. This
can cause a signiﬁcant gap between the stationary distribution of the optimized policy computed in
M and the one computed in ˆ
M, leading to failure in both reward performance improvement and cost
constraint satisfaction. To prevent such distributional shift, ofﬂine RL algorithms commonly adopt
the pessimism principle, encouraging staying close to the data support. We can observe that such
pessimism principle is also effective in ofﬂine constrained RL: both C-SPIBB (orange) and the naive
version of COptiDICE (green) show consistent policy improvement over the data-collection policy
while showing better constraint satisfaction. Still, the pessimism principle alone is not sufﬁcient
to ensure constraint satisfaction, raising the need for additional treatment for cost-conservativeness.
Finally, our COptiDICE (blue) shows much stricter cost satisfaction than other baseline algorithms
while outperforming Baseline and C-SPIBB in terms of reward performance."
EXPERIMENTS,0.22982456140350876,"Figure 1b presents the result when the data-collection policy is constraint-violating, where all the
baseline algorithms exhibit severe cost violation. This result shows that if the agent is encouraged
to stay close to the constraint-violating policy, it may negatively affect the constraint satisfaction,
although the pessimism principle was beneﬁcial in terms of reward maximization. In this situation,
only COPtiDICE (blue) could meet the given constraint in general, which demonstrates the effective-
ness of our proposed method of constraining the upper bound of cost value. Although COptiDICE
(blue) shows reward performance degradation when the size of dataset is very small, this is natural in
that it sacriﬁces the reward performance to lower the cost value to meet the constraint conservatively,
and it still outperforms the baseline in this low-data regime."
EXPERIMENTS,0.23157894736842105,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.23333333333333334,"10
1
10
2
10
3"
EXPERIMENTS,0.23508771929824562,number of trajectories in D 0.09 0.10 0.11 Cost
EXPERIMENTS,0.23684210526315788,"10
1
10
2
10
3"
EXPERIMENTS,0.23859649122807017,number of trajectories in D 0.0 0.5 1.0
EXPERIMENTS,0.24035087719298245,(a) when the data-collection policy is cost-satisfying
EXPERIMENTS,0.24210526315789474,Reward
EXPERIMENTS,0.24385964912280703,"10
1
10
2
10
3"
EXPERIMENTS,0.24561403508771928,number of trajectories in D 0.10 0.12 Cost
EXPERIMENTS,0.24736842105263157,"10
1
10
2
10
3"
EXPERIMENTS,0.24912280701754386,number of trajectories in D 0 1
EXPERIMENTS,0.25087719298245614,(b) when the data-collection policy is cost-violating
EXPERIMENTS,0.25263157894736843,Reward
EXPERIMENTS,0.2543859649122807,"Cost threshold ˆc
Baseline"
EXPERIMENTS,0.256140350877193,"Data-collection policy
C-SPIBB"
EXPERIMENTS,0.2578947368421053,"Optimal policy
COptiDICE (by (12))"
EXPERIMENTS,0.2596491228070175,"BC
COptiDICE"
EXPERIMENTS,0.2614035087719298,"Figure 1: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs for the
varying number of trajectories and two types of data-collection policies. Plots of (a) correspond to
the case when the data-collection policy is constraint-satisfying, while the data-collection policy is
constraint-violating in the last two plots of (b). The mean of normalized reward performance and
the mean of cost value are reported for 10K runs, where the error bar denotes the standard error."
EXPERIMENTS,0.2631578947368421,"4.2
CONTINUOUS CONTROL TASKS (RWRL SUITE)"
EXPERIMENTS,0.2649122807017544,"We also evaluate COptiDICE on domains from the Real-World RL (RWRL) suite (Dulac-Arnold
et al., 2020), where the safety constraints are employed. The cost of 1 is given if the task-speciﬁc
safety constraint is violated at each time step, and the goal is to compute a policy that maximizes re-
wards while bounding the average cost up to ˆc. Per-domain safety constraints and the cost constraint
thresholds are given in Appendix E. Due to lack of an algorithm that addresses ofﬂine constrained
RL in continuous action space, we compare COptiDICE with simple baseline algorithms, i.e. BC:
A simple behavior-cloning agent, CRR (Wang et al., 2020): a state-of-the-art (unconstrained) ofﬂine
RL algorithm, and C-CRR: the constrained variant of CRR with Lagrangian relaxation where a cost
critic and a Lagrange multiplier for the cost constraint are introduced (Appendix E)."
EXPERIMENTS,0.26666666666666666,"Since there is no standard dataset for ofﬂine constrained RL, we collected data using online con-
strained RL agents (C-DMPO; the constrained variant of Distributional MPO) (Abdolmaleki et al.,
2018; Mankowitz et al., 2021). We trained the online C-DMPO with various cost thresholds ˆc and
saved checkpoints at regular intervals, which constitutes the pool of policy checkpoints. Then, we
generated datasets, where each of them consists of β × 100% constraint-satisfying trajectories and
the rest constraint-violating trajectories. The trajectories were sampled by policies in the pool of
policy checkpoints. Since there is a trade-off between reward and cost in general, the dataset is a
mixture of low-reward-low-cost trajectories and high-reward-high-cost trajectories."
EXPERIMENTS,0.26842105263157895,"Figure 2 presents our results in RWRL tasks, where the dataset contains mostly constraint-satisfying
trajectories (β = 0.8), with some cost-violating trajectories. This type of dataset is representative
of many practical scenarios: the data-collecting agent behaves safely in most cases, but sometimes
exhibit exploratory behaviors which can be leveraged for potential performance improvement. Due
to the characteristics of these datasets, BC (orange) generally yields constraint-satisfying policy, but
its reward performance is also very low. CRR (red) signiﬁcantly improves reward performance over
BC, but it does not ensure that the constraints are satisﬁed. C-CRR (brown) incurs relatively lower
cost value than BC in Walker, Quadruped, and Humanoid, but its reward performance is clearly
worse than BC. The naive COptiDICE algorithm (green) takes the cost constraint into account but
nevertheless frequently violates the constraint due to limitations of OPE. Finally, COptiDICE (blue)
computes a policy that is more robust to cost violation than other algorithms, while signiﬁcantly
outperforming BC in terms of reward performance. We observe that the hyperparameter ϵ in (16)
controls the degree of cost-conservativeness as expected: larger values of ϵ lead to overestimates of
the cost value, yielding a more conservative policy."
EXPERIMENTS,0.27017543859649124,"To study the dependence on the characteristics of the dataset we experiment with different values
of β. Figure 3a show the result for β = 0.8 (low-reward-low-cost data), Figure 3b for β = 0.5,
and Figure 3c for β = 0.2 (high-reward-high-cost data). These results show the expected trend:
more high-reward-high-cost data leads to a joint increase in rewards and costs of all agents. A
simple modiﬁcation of the unconstrained CRR to the constrained one (brown) was not effective
enough to satisfy the constraint. Our vanilla ofﬂine constrained RL algorithm (green), encouraged
to stay close to the data, also suffers from severe constraint violation when most of the trajectories
are given as the constraint-violating ones, which is similar to the result of Figure 1c-1d. Finally,"
EXPERIMENTS,0.2719298245614035,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2736842105263158,"0
2.5
5
iterations
1e5 0.00 0.25"
AVERAGE COST,0.2754385964912281,"0.50
Average cost"
AVERAGE COST,0.2771929824561403,"0
2.5
5
iterations
1e5 0 500"
AVERAGE COST,0.2789473684210526,(a) Cartpole Swingup
AVERAGE COST,0.2807017543859649,Reward
AVERAGE COST,0.2824561403508772,"0
2.5
5
iterations
1e5 0.0 0.2"
AVERAGE COST,0.28421052631578947,Average cost
AVERAGE COST,0.28596491228070176,"0
2.5
5
iterations
1e5 0 500"
AVERAGE COST,0.28771929824561404,(b) Walker
AVERAGE COST,0.2894736842105263,Reward
AVERAGE COST,0.2912280701754386,"0
1
2
3
iterations
1e6 0.0 0.5"
AVERAGE COST,0.2929824561403509,Average cost
AVERAGE COST,0.29473684210526313,"0
1
2
3
iterations
1e6 500 1000"
AVERAGE COST,0.2964912280701754,(c) Quadruped
AVERAGE COST,0.2982456140350877,Reward
AVERAGE COST,0.3,"0
1
2
3
iterations
1e6 0.00 0.25"
AVERAGE COST,0.3017543859649123,"0.50
Average cost"
AVERAGE COST,0.30350877192982456,"0
1
2
3
iterations
1e6 200 300 400"
AVERAGE COST,0.30526315789473685,(d) Humanoid
AVERAGE COST,0.30701754385964913,Reward
AVERAGE COST,0.3087719298245614,"Cost threshold ˆc
COptiDICE (by (12))"
AVERAGE COST,0.3105263157894737,"BC
COptiDICE (ϵ = 0.01)"
AVERAGE COST,0.312280701754386,"CRR
COptiDICE (ϵ = 0.05)"
AVERAGE COST,0.3140350877192982,"C-CRR
COptiDICE (ϵ = 0.1)"
AVERAGE COST,0.3157894736842105,"Figure 2: Result of RWRL control tasks. For each task, we report the reward return and the average
cost. The results are averaged over 5 runs, and the shaded area represents the standard error."
AVERAGE COST,0.3175438596491228,"0
5
10
iterations 1e5 0.0 0.1 0.2 0.3"
AVERAGE COST,0.3192982456140351,(a) Walker (β = 0.8)
AVERAGE COST,0.32105263157894737,Average cost
AVERAGE COST,0.32280701754385965,"0
5
10
iterations 1e5 0 500"
REWARD,0.32456140350877194,"1000
Reward"
REWARD,0.3263157894736842,"0
5
10
iterations 1e5 0.0 0.1 0.2 0.3"
REWARD,0.3280701754385965,(b) Walker (β = 0.5)
REWARD,0.3298245614035088,Average cost
REWARD,0.33157894736842103,"0
5
10
iterations 1e5 0 500"
REWARD,0.3333333333333333,"1000
Reward"
REWARD,0.3350877192982456,"0
5
10
iterations 1e5 0.0 0.1 0.2 0.3"
REWARD,0.3368421052631579,(c) Walker (β = 0.2)
REWARD,0.3385964912280702,Average cost
REWARD,0.34035087719298246,"0
5
10
iterations 1e5 0 500"
REWARD,0.34210526315789475,"1000
Reward"
REWARD,0.34385964912280703,"Cost threshold ˆc
COptiDICE (by (12))"
REWARD,0.3456140350877193,"BC
COptiDICE (ϵ = 0.01)"
REWARD,0.3473684210526316,"CRR
COptiDICE (ϵ = 0.05)"
REWARD,0.34912280701754383,"C-CRR
COptiDICE (ϵ = 0.1)"
REWARD,0.3508771929824561,"Figure 3: Result on RWRL walker using three dataset conﬁgurations with different levels of con-
straint satisfaction β: for (a) the data is obtained with β = 0.8 (low-reward-low-cost data), (b)
with β = 0.5, and (c) with β = 0.2 (high-reward-high-cost data)."
REWARD,0.3526315789473684,"COptiDICE (blue) demonstrates more robust behavior to avoid constraint violations across dataset
conﬁgurations, highlighting the effectiveness of our method constraining the cost upper bound."
DISCUSSION AND CONCLUSION,0.3543859649122807,"5
DISCUSSION AND CONCLUSION"
DISCUSSION AND CONCLUSION,0.356140350877193,"The notion of safety in RL has been captured in various forms such as risk-sensitivity (Chow et al.,
2015; Urp´ı et al., 2021; Yang et al., 2021), Robust MDP (Iyengar, 2005; Tamar et al., 2014), and
Constrained MDP (Altman, 1999), among which we focus on CMDP as it provides a natural for-
malism to encode safety speciﬁcations (Ray et al., 2019). Most of the existing constrained RL
algorithms (Achiam et al., 2017; Tessler et al., 2019; Satija et al., 2020) are on-policy algorithms,
which cannot be applied to the ofﬂine setting directly. A recent exception is the work by Le et al.
(2019) that also aims to solve constrained RL in an ofﬂine setting, though its method is limited to
discrete action spaces and relies on solving an MDP completely as an inner optimization, which is
inefﬁcient. It also relies on the vanilla OPE estimate of the policy cost, which could result in severe
constraint violation when deployed. Lastly, in a work done concurrently to ours, Xu et al. (2021)
also exploits overestimated cost value to deal with the cost constraint, but their approach relies on
an actor-critic algorithm, while ours relies on stationary distribution optimization."
DISCUSSION AND CONCLUSION,0.35789473684210527,"In this work, we have presented a DICE-based ofﬂine constrained RL algorithm, COptiDICE. DICE-
family algorithms have been proposed for off-policy evaluation (Nachum et al., 2019a; Zhang et al.,
2020a;b; Yang et al., 2020b; Dai et al., 2020), imitation learning (Kostrikov et al., 2019), ofﬂine
policy selection (Yang et al., 2020a), and RL (Nachum et al., 2019b; Lee et al., 2021), but none of
them is for constrained RL. Our ﬁrst contribution was a derivation that constrained ofﬂine RL can be
tackled by solving a single minimization problem. We demonstrated that such approach, in its sim-
plest form, suffers from constraint violation in practice. To mitigate the issue, COptiDICE instead
constrains the cost upper bound, which is estimated in a way that exploits the distribution correc-
tion w obtained by solving the RL problem. Such reuse of w eliminates the nested optimization in
CoinDICE (Dai et al., 2020), and COptiDICE can be optimized efﬁciently as a result. Experimental
results demonstrated that our algorithm achieved better trade-off between reward maximization and
constraint satisfaction than several baselines, across domains and conditions."
DISCUSSION AND CONCLUSION,0.35964912280701755,Published as a conference paper at ICLR 2022
DISCUSSION AND CONCLUSION,0.36140350877192984,ACKNOWLEDGMENTS
DISCUSSION AND CONCLUSION,0.3631578947368421,"The authors would like to thank Rui Zhu for technical support and Sandy Huang for paper feed-
back. Kee-Eung Kim was supported by the National Research Foundation (NRF) of Korea (NRF-
2019R1A2C1087634, NRF-2021M3I1A1097938) and the Ministry of Science and Information
communication Technology (MSIT) of Korea (IITP No.2019-0-00075, IITP No.2020-0-00940, IITP
No.2021-0-02068)."
REFERENCES,0.3649122807017544,REFERENCES
REFERENCES,0.36666666666666664,"Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learn-
ing Representations (ICLR), 2018."
REFERENCES,0.3684210526315789,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research, pp. 22–31. PMLR, 06–11 Aug 2017."
REFERENCES,0.3701754385964912,"Eitan Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999."
REFERENCES,0.3719298245614035,"Christopher M. Bishop. Mixture density networks. Technical report, 1994."
REFERENCES,0.3736842105263158,"V.S. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & Control
Letters, 54(3):207–213, 2005."
REFERENCES,0.37543859649122807,"Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-
making: a cvar optimization approach. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28, 2015."
REFERENCES,0.37719298245614036,"Bo Dai, Oﬁr Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans.
CoinDICE: Off-policy conﬁdence interval estimation. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), volume 33, pp. 9398–9411, 2020."
REFERENCES,0.37894736842105264,"Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,
and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learn-
ing. 2020."
REFERENCES,0.38070175438596493,"Bradley Efron and Robert J. Tibshirani. An Introduction to the Bootstrap. Number 57 in Monographs
on Statistics and Applied Probability. Chapman & Hall/CRC, Boca Raton, Florida, USA, 1993."
REFERENCES,0.3824561403508772,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the 36th International Conference on Machine Learning (ICML),
2019."
REFERENCES,0.38421052631578945,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems
(NeurIPS), volume 30, 2017."
REFERENCES,0.38596491228070173,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning (ICML), 2018."
REFERENCES,0.387719298245614,"Josiah P. Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Conﬁdence intervals
for off-policy evaluation. In Proceedings of the 16th Conference on Autonomous Agents and
MultiAgent Systems, AAMAS, pp. 538–546. ACM, 2017."
REFERENCES,0.3894736842105263,"Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara
Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex
Novikov, Sergio G´omez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew
Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed
reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. URL https://arxiv.org/
abs/2006.00979."
REFERENCES,0.3912280701754386,Published as a conference paper at ICLR 2022
REFERENCES,0.3929824561403509,"Garud N. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 2005."
REFERENCES,0.39473684210526316,"Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog, 2019."
REFERENCES,0.39649122807017545,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL : Model-
based ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems
(NeurIPS), 2020."
REFERENCES,0.39824561403508774,"Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu-
tion matching. In Proceedings of the 7th International Conference on Learning Representations
(ICLR), 2019."
REFERENCES,0.4,"Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Ofﬂine reinforcement learning
with ﬁsher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceed-
ings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of
Machine Learning Research, pp. 5774–5783. PMLR, 18–24 Jul 2021."
REFERENCES,0.4017543859649123,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019."
REFERENCES,0.40350877192982454,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for ofﬂine
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.4052631578947368,"Sascha Lange, Thomas Gabel, and Martin Riedmiller. Reinforcement learning: State-of-the-art.
Springer Berlin Heidelberg, 2012."
REFERENCES,0.4070175438596491,"Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning
(ICML), 2019."
REFERENCES,0.4087719298245614,"Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 3703–3712. PMLR, 09–15 Jun 2019."
REFERENCES,0.4105263157894737,"Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim. Batch reinforcement
learning with hyperparameter gradients. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 5725–5735. PMLR, 13–18 Jul 2020."
REFERENCES,0.41228070175438597,"Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. OptiDICE: Ofﬂine
policy optimization via stationary distribution correction estimation. In Proceedings of the 38th
International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pp. 6120–6130. PMLR, 18–24 Jul 2021."
REFERENCES,0.41403508771929826,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems, 2020."
REFERENCES,0.41578947368421054,"Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR, 2016."
REFERENCES,0.41754385964912283,"Daniel J. Mankowitz, Dan A. Calian, Rae Jeong, Cosmin Paduraru, Nicolas Heess, Sumanth
Dathathri, Martin Riedmiller, and Timothy Mann. Robust constrained reinforcement learning
for continuous control with model misspeciﬁcation, 2021."
REFERENCES,0.4192982456140351,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015."
REFERENCES,0.42105263157894735,Published as a conference paper at ICLR 2022
REFERENCES,0.42280701754385963,"Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation
of discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems (NeurIPS), 2019a."
REFERENCES,0.4245614035087719,"Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. AlgaeDICE:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b."
REFERENCES,0.4263157894736842,"Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019."
REFERENCES,0.4280701754385965,"Harsh Satija, Philip Amortila, and Joelle Pineau. Constrained Markov decision processes via back-
ward value functions. In Proceedings of the 37th International Conference on Machine Learning
(ICML), volume 119 of Proceedings of Machine Learning Research, pp. 8502–8511, 13–18 Jul
2020."
REFERENCES,0.4298245614035088,"Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavioral modelling priors for ofﬂine reinforcement learning, 2020."
REFERENCES,0.43157894736842106,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of Go without human knowledge. Nature, 550:354–359, 2017."
REFERENCES,0.43333333333333335,"Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation.
In Proceedings of the 31st International Conference on International Conference on Machine
Learning (ICML), 2014."
REFERENCES,0.43508771929824563,"Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
International Conference on Learning Representations (ICLR), 2019."
REFERENCES,0.4368421052631579,"N´uria Armengol Urp´ı, Sebastian Curi, and Andreas Krause. Risk-averse ofﬂine reinforcement learn-
ing. In International Conference on Learning Representations (ICLR), 2021."
REFERENCES,0.43859649122807015,"Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas.
Critic regularized regression. In Advances in Neural Information Processing Systems (NeurIPS),
volume 33, pp. 7768–7778, 2020."
REFERENCES,0.44035087719298244,"Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning,
2019."
REFERENCES,0.4421052631578947,"Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe ofﬂine
reinforcement learning, 2021."
REFERENCES,0.443859649122807,"Mengjiao Yang, Bo Dai, Oﬁr Nachum, George Tucker, and Dale Schuurmans. Ofﬂine policy selec-
tion under uncertainty, 2020a."
REFERENCES,0.4456140350877193,"Mengjiao Yang, Oﬁr Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized lagrangian. In Advances in Neural Information Processing Systems (NeurIPS),
2020b."
REFERENCES,0.4473684210526316,"Qisong Yang, Thiago D. Sim˜ao, Simon H Tindemans, and Matthijs T. J. Spaan. Wcsac: Worst-
case soft actor critic for safety-constrained reinforcement learning.
Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, 35(12):10639–10646, May 2021."
REFERENCES,0.44912280701754387,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. MOPO: Model-based ofﬂine policy optimization. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2020."
REFERENCES,0.45087719298245615,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized ofﬂine estimation
of stationary values. In Proceedings of the 8th International Conference on Learning Represen-
tations (ICLR), 2020a."
REFERENCES,0.45263157894736844,Published as a conference paper at ICLR 2022
REFERENCES,0.4543859649122807,"Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized ofﬂine
estimation of stationary values. In Proceedings of the 35th International Conference on Machine
Learning (ICML), 2020b."
REFERENCES,0.45614035087719296,Published as a conference paper at ICLR 2022
REFERENCES,0.45789473684210524,"A
ALGORITHM FOR UNDISCOUNTED CMDP"
REFERENCES,0.45964912280701753,"For γ = 1, the optimization problem (4-7) should be modiﬁed by adding an additional normalization
constraint P"
REFERENCES,0.4614035087719298,"s,a d(s, a) = 1."
REFERENCES,0.4631578947368421,"max
d
E(s,a)∼d[R(s, a)] −αDf(d||dD)
(28)"
REFERENCES,0.4649122807017544,"s.t. E(s,a)∼d[Ck(s, a)] ≤ˆck
∀k = 1, . . . , K
(29)
P"
REFERENCES,0.4666666666666667,"a′ d(s′, a′) = (1 −γ)p0(s′) + γ P"
REFERENCES,0.46842105263157896,"s,a
d(s, a)T(s′|s, a)
∀s′
(30)"
REFERENCES,0.47017543859649125,"d(s, a) ≥0
∀s, a
(31)
P"
REFERENCES,0.47192982456140353,"s,a
d(s, a) = 1
(32)"
REFERENCES,0.47368421052631576,"Then, we consider the Lagrangian:"
REFERENCES,0.47543859649122805,"min
λ≥0,ν,µ max
d≥0 E(s,a)∼d[R(s, a)] −αDf(d||dD) −
K
P"
REFERENCES,0.47719298245614034,"k=1
λk
 
E(s,a)∼d[Ck(s, a)] −ˆck
 −P"
REFERENCES,0.4789473684210526,"s′ ν(s′)
h P"
REFERENCES,0.4807017543859649,"a′ d(s′, a′) −(1 −γ)p0(s′) −γ P"
REFERENCES,0.4824561403508772,"s,a
d(s, a)T(s′|s, a)
i
−µ
h P"
REFERENCES,0.4842105263157895,"s,a
d(s, a) −1
i
(33)"
REFERENCES,0.48596491228070177,"where µ ∈R is the Lagrange multiplier for the normalization constraint (32). Then, we rearrange the
terms so that the direct dependence on d is eliminated, while introducing new optimization variables
w(s, a) =
d(s,a)
dD(s,a) that represent stationary distribution corrections:"
REFERENCES,0.48771929824561405,"min
λ≥0,ν,µ max
d≥0 E (s,a)∼d
s′∼T (s,a)"
REFERENCES,0.48947368421052634,"
R(s, a) −λ⊤C(s, a) + γν(s′) −ν(s) −µ

−αE(s,a)∼dD
h
f

d(s,a)
dD(s,a)
i"
REFERENCES,0.49122807017543857,+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc + µ
REFERENCES,0.49298245614035086,"= min
λ≥0,ν,µ max
w≥0 E(s,a)∼dD

w(s, a)(eλ,ν(s, a) −µ) −αf(w(s, a))
"
REFERENCES,0.49473684210526314,"+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc + µ
(34)"
REFERENCES,0.4964912280701754,"which yields the minmax optimization problem that can be optimized in a fully ofﬂine manner. Due
to the strict convexity of f, we can also derive the closed form solution of the inner maximization
problem:"
REFERENCES,0.4982456140350877,"w∗
λ,ν,µ(s, a) = (f ′)−1
1
α(eλ,ν(s, a) −µ)
"
REFERENCES,0.5,"+
(35)"
REFERENCES,0.5017543859649123,which simplify the the minmax problem (34) into the following single minimization problem:
REFERENCES,0.5035087719298246,"min
λ≥0,ν,µ L(λ, ν, µ) = E(s,a)∼dD

w∗
λ,ν,µ(s, a)(eλ,ν(s, a) −µ) −αf(w∗
λ,ν,µ(s, a))
"
REFERENCES,0.5052631578947369,"+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc + µ
(36)"
REFERENCES,0.5070175438596491,"Once the optimal solution (λ∗, ν∗, µ∗) are obtained, we can also compute the stationary distribution
corrections of the optimal policy using (35). Still, naively using (36) would similarly result in
cost violation when the resulting policy is deployed to the real environment. We therefore adopt
constraining the upper bound of cost value:"
REFERENCES,0.5087719298245614,"ν, µ ←arg min
ν,µ
L(λ, ν, µ)
(RL for R −λ⊤C using (36))
(37)"
REFERENCES,0.5105263157894737,"τ, χ ←arg min
τ≥0,χ"
REFERENCES,0.512280701754386,"PK
k=1 ℓk(τk, χk; w∗
λ,ν,µ)
(Upper cost value estimation using (18))"
REFERENCES,0.5140350877192983,"λ ←arg min
λ≥0
λ⊤ 
ˆc −ℓ(τ, χ; w∗
λ,ν,µ)
|
{z
}"
REFERENCES,0.5157894736842106,≈UpperBound( ˆVC(π))
REFERENCES,0.5175438596491229,"
(Cost Lagrange multiplier)"
REFERENCES,0.519298245614035,which completes the brief description of COptiDICE for γ = 1.
REFERENCES,0.5210526315789473,Published as a conference paper at ICLR 2022
REFERENCES,0.5228070175438596,"B
PROOFS"
REFERENCES,0.5245614035087719,"Proposition 1. For any ν and λ, the closed-form solution of"
REFERENCES,0.5263157894736842,"max
w≥0 L(w, λ, ν) := E(s,a)∼dD

w(s, a)eλ,ν(s, a) −αf(w(s, a))

+ (1 −γ)Es0∼p0[ν(s0)] + λ⊤ˆc (38)"
REFERENCES,0.5280701754385965,is given by:
REFERENCES,0.5298245614035088,"w∗
λ,ν(s, a) = (f ′)−1
1
αeλ,ν(s, a)
"
REFERENCES,0.531578947368421,"+ where x+ = max(0, x)
(39)"
REFERENCES,0.5333333333333333,"Proof. For a ﬁxed λ and ν, we write the dual of the maximization problem maxw≥0 L(w, λ, ν):"
REFERENCES,0.5350877192982456,"max
w
min
κ≥0 L(w, λ, ν) +
X"
REFERENCES,0.5368421052631579,"s,a
κ(s, a)w(s, a)"
REFERENCES,0.5385964912280702,"⇔max
w
min
κ≥0 L(w, λ, ν) +
X"
REFERENCES,0.5403508771929825,"s,a
κ(s, a)dD(s, a)w(s, a)
(∵dD > 0)."
REFERENCES,0.5421052631578948,"By the strong duality, it is sufﬁcient to consider KKT condition for (w∗, κ∗)."
REFERENCES,0.543859649122807,"Condition 1 (Primal feasibility) w∗(s, a) ≥0 ∀s, a"
REFERENCES,0.5456140350877193,"Condition 2 (Dual feasibility) κ∗(s, a) ≥0 ∀s, a"
REFERENCES,0.5473684210526316,"Condition 3 (Stationarity) dD(s, a)(eλ,ν(s, a) −αf ′(w∗(s, a)) + κ∗(s, a)) = 0 ∀s, a"
REFERENCES,0.5491228070175439,"⇔f ′(w∗(s, a)) = 1"
REFERENCES,0.5508771929824562,"α(eλ,ν(s, a) + κ∗(s, a))"
REFERENCES,0.5526315789473685,"⇔w∗(s, a) = (f ′)−1   1"
REFERENCES,0.5543859649122806,"α(eλ,ν(s, a) + κ∗(s, a))

(40)"
REFERENCES,0.5561403508771929,"Condition 4 (Complementary slackness) w∗(s, a)κ∗(s, a) = 0 ∀s, a"
REFERENCES,0.5578947368421052,"Then, we will show that:"
REFERENCES,0.5596491228070175,"w∗
λ,ν(s, a) = (f ′)−1   1"
REFERENCES,0.5614035087719298,"αeλ,ν(s, a)
"
REFERENCES,0.5631578947368421,"+
(41)"
REFERENCES,0.5649122807017544,"satisﬁes the KKT conditions for all (s, a). First, Primal feasibility is always satisﬁed by deﬁnition
of w∗
λ,ν(s, a). Then, we consider either 1"
REFERENCES,0.5666666666666667,"αeλ,ν(s, a) > f ′(0) or 1"
REFERENCES,0.5684210526315789,"αeλ,ν(s, a) ≤f ′(0) for each (s, a).
 
Case 1: 1"
REFERENCES,0.5701754385964912,"αeλ,ν(s, a) > f ′(0)

: In this case, κ∗(s, a) = 0, where Dual feasibility and Complemen-
tary slackness is satisﬁed. Stationarity also holds by:"
REFERENCES,0.5719298245614035,"w∗
λ,ν(s, a) = (f ′)−1   1"
REFERENCES,0.5736842105263158,"αeλ,ν(s, a)
"
REFERENCES,0.5754385964912281,"+
= (f ′)−1   1"
REFERENCES,0.5771929824561404,"αeλ,ν(s, a)

(by assumption)"
REFERENCES,0.5789473684210527,= (f ′)−1   1
REFERENCES,0.5807017543859649,"α(eλ,ν(s, a) + κ∗(s, a))

⇔(40)"
REFERENCES,0.5824561403508772,"Therefore, KKT conditions (Conditions 1-4) are satisﬁed."
REFERENCES,0.5842105263157895,(Case 2: 1
REFERENCES,0.5859649122807018,"αeλ,ν(s, a) ≤f ′(0)): In this case, κ∗(s, a) = αf ′(0) −eλ,ν(s, a), where Dual feasibility
holds by assumption. Also, w∗
λ,ν(s, a) = 0 by assumption, which implies Complementary slackness
also holds. Finally, Stationarity also holds by:"
REFERENCES,0.5877192982456141,"w∗
λ,ν(s, a) = (f ′)−1   1"
REFERENCES,0.5894736842105263,"αeλ,ν(s, a)
"
REFERENCES,0.5912280701754385,"+
= 0
(by assumption)"
REFERENCES,0.5929824561403508,= (f ′)−1 (f ′(0))
REFERENCES,0.5947368421052631,= (f ′)−1   1
REFERENCES,0.5964912280701754,"α(eλ,ν(s, a) + κ∗(s, a))

⇔(40)"
REFERENCES,0.5982456140350877,"As a consequence, KKT conditions (Conditions 1-4) are always satisﬁed with w∗
λ,ν(s, a) =
(f ′)−1   1"
REFERENCES,0.6,"αeλ,ν(s, a)
"
REFERENCES,0.6017543859649123,"+, which concludes the proof."
REFERENCES,0.6035087719298246,Published as a conference paper at ICLR 2022
REFERENCES,0.6052631578947368,"Proposition 2. The constrained optimization problem (15-17) can be reduced to solving the follow-
ing unconstrained minimization problem:"
REFERENCES,0.6070175438596491,"min
τ≥0,χ ℓk(τ, χ; w) = τ log Ex∼dD
h
exp

1
τ
 
w(s, a)(Ck(s, a) + γχ(s′) −χ(s)) + (1 −γ)χ(s0)
i
+ τϵ (18)"
REFERENCES,0.6087719298245614,"where τ ∈R+ corresponds to the Lagrange multiplier for the constraint (16), and χ(s) ∈R
corresponds to the Lagrange multiplier for the constraint (17). In other words, minτ≥0,χ ℓ(τ, χ) =
E(s,a)∼˜p∗[w(s, a)Ck(s, a)] where ˜p∗is the optimal perturbed distribution of the problem (15-17).
Also, for the optimal solution (τ ∗, χ∗), ˜p∗is given by:"
REFERENCES,0.6105263157894737,"˜p∗(x) ∝dD(x) exp

1
τ ∗
 
w(s, a)(Ck(s, a) + γχ∗(s′) −χ∗(s)) + (1 −γ)χ∗(s0)
"
REFERENCES,0.612280701754386,"|
{z
}
=: ω∗(x) (unnormalized weight for x = (s0, s, a, s′)) (19)"
REFERENCES,0.6140350877192983,Proof. For the given constrained optimization problem:
REFERENCES,0.6157894736842106,"max
˜p∈∆(X) E(s0,s,a,s′)∼˜p[w(s, a)Ck(s, a)]
(42)"
REFERENCES,0.6175438596491228,"s.t. DKL(˜p(s0, s, a, s′)||dD(s0, s, a, s′)) ≤ϵ
(43)
P"
REFERENCES,0.6192982456140351,"a′ ˜p(s′, a′)w(s′, a′) = (1 −γ)˜p0(s′) + γ P"
REFERENCES,0.6210526315789474,"s,a
˜p(s, a)w(s, a)˜p(s′|s, a) ∀s′
(44)"
REFERENCES,0.6228070175438597,We consider the Lagrangian:
REFERENCES,0.624561403508772,"min
τ≥0,χ,ζ max
˜p≥0
P"
REFERENCES,0.6263157894736842,"x
˜p(s0, s, a, s′)[w(s, a)Ck(s, a)] −τ
 P"
REFERENCES,0.6280701754385964,"x
˜p(s0, s, a, s′)
h
log
˜p(s0,s,a,s′)
dD(s0,s,a,s′)
i
−ϵ
 −P"
REFERENCES,0.6298245614035087,"s′ χ(s′)
h P"
REFERENCES,0.631578947368421,"a′ ˜p(s′, a′)w(s′, a′) −(1 −γ)˜p0(s′) −γ P"
REFERENCES,0.6333333333333333,"s,a
˜p(s, a)w(s, a)˜p(s′|s, a)
i"
REFERENCES,0.6350877192982456,"−ζ
h P"
REFERENCES,0.6368421052631579,"x
˜p(s0, s, a, s′) −1
i
(45)"
REFERENCES,0.6385964912280702,"where τ ∈R+ is the Lagrange multiplier for KL constraint (43), χ(s′) ∈R is the Lagrange mul-
tiplier for (44), and ζ ∈R is the Lagrange multiplier for the normalization constraint that ensures
P"
REFERENCES,0.6403508771929824,"x ˜p(x) = 1. Then, we rearrange (45) by:"
REFERENCES,0.6421052631578947,"min
τ≥0,χ,ζ max
˜p≥0 X"
REFERENCES,0.643859649122807,"x
˜p(x)

w(s, a)
 
Ck(s, a) + γχ(s′) −χ(s)

+ (1 −γ)χ(s0) −τ log
˜p(x)
dD(x)
"
REFERENCES,0.6456140350877193,"+ τϵ −ζ
h P"
REFERENCES,0.6473684210526316,"x
˜p(x) −1
i
=: g(τ, χ, ζ, ˜p)
(46)"
REFERENCES,0.6491228070175439,"Then, we can compute the non-parametric closed form solution for each sample x = (s0, s, a, s′) for
the inner-maximization problem. Thanks to convexity of KL-divergence, it is sufﬁcient to consider
∂g(τ,χ,ζ,˜p)"
REFERENCES,0.6508771929824562,"∂˜p(x)
= 0 for each x. Then,"
REFERENCES,0.6526315789473685,"∂g(τ,χ,ζ,˜p)"
REFERENCES,0.6543859649122807,"∂˜p(x)
= w(s, a)
 
Ck(s, a) + γχ(s′) −χ(s)

+ (1 −γ)χ(s0) −τ log
˜p(x)
dD(x) + τ −ζ = 0"
REFERENCES,0.656140350877193,"⇒˜p(x) ∝dD(x) exp

1
τ
 
w(s, a)
 
Ck(s, a) + γχ(s′) −χ(s)

+ (1 −γ)χ(s0)

(47)"
REFERENCES,0.6578947368421053,with some normalization constant that ensures P
REFERENCES,0.6596491228070176,"x ˜p(x) = 1, which is described with respect to ζ.
Then, by plugging (47) into (46), we obtain the result. Also, (19) is the direct result of (47)."
REFERENCES,0.6614035087719298,Published as a conference paper at ICLR 2022
REFERENCES,0.6631578947368421,"C
PSEUDOCODE OF COPTIDICE"
REFERENCES,0.6649122807017543,"Algorithm 1 COptiDICE
Input: An ofﬂine dataset D = {(s0, s, a, r, c, s′)i}N
i=1, a learning rate η.
1: Initialize parameter vectors θ, φ, λ, τ, ψ.
2: for each gradient step do
3:
Sample mini-batches from D.
4:
Compute gradients and perform SGD update:
5:
θ ←θ −η∇θJν(θ)
(Eq. (23))
φ ←φ −η∇φJτ,χ(τ, φ) (Eq. (25))
6:
τ ←

τ −η∇τJτ,χ(τ, φ)
"
REFERENCES,0.6666666666666666,"+ (Eq. (25))
λ ←

λ −ηJλ(λ)
"
REFERENCES,0.6684210526315789,"+
(Eq. (26))
7:
ψ ←ψ −η∇ψJπ(ψ)
(Eq. (27))
8: end for"
REFERENCES,0.6701754385964912,"D
COMPARISON WITH COINDICE"
REFERENCES,0.6719298245614035,"CoinDICE (Dai et al., 2020) is a DICE-family algorithm for off-policy conﬁdence interval estima-
tion. For a given policy π, CoinDICE essentially solves the following constrained optimization
problem to estimate the upper conﬁdence interval of the cost value:"
REFERENCES,0.6736842105263158,"max
˜p∈∆(X) max
w≥0E(s0,s,a,s′)∼˜p[w(s, a)Ck(s, a)]
(48)"
REFERENCES,0.6754385964912281,"s.t. DKL(˜p(s0, s, a, s′)||dD(s0, s, a, s′)) ≤ϵ
(49)"
REFERENCES,0.6771929824561403,"˜p(s′, a′)w(s′, a′) = (1 −γ)˜p0(s′)π(a′|s′) + γ P"
REFERENCES,0.6789473684210526,"s,a
˜p(s, a)w(s, a)˜p(s′|s, a)π(a′|s′) ∀s′, a′
(50)"
REFERENCES,0.6807017543859649,The constraint (50) is analogous to the π-dependent Bellman ﬂow constraint:
REFERENCES,0.6824561403508772,"dπ(s′, a′) = (1 −γ)p0(s′)π(a′|s′) + γ P s,a"
REFERENCES,0.6842105263157895,"dπ(s, a)T(s′|s, a)π(a′|s′) ∀s′, a′
(51)"
REFERENCES,0.6859649122807018,"It is well known that the transposed Bellman equation (51) always has a unique solution dπ, i.e. the
stationary distribution of the given policy π. Note that for a ﬁxed ˜p, the constrained optimization
problem (48-50) is over-constrained for w(s, a) by (50), and therefore the optimal solution will sim-
ply be given by w∗(s, a) = dπ(s,a)"
REFERENCES,0.6877192982456141,"˜p(s,a) to satisfy the transposed Bellman equation (51) on the empirical
MDP deﬁned by ˜p. Then, we want to adversarially optimize the distribution ˜p so that it overestimates
the cost value by (48). At the same time, we enforce that the distribution ˜p should not be perturbed
too much from the empirical dsta distribution dD by the KL constraint (49). Finally, following the
similar derivation in Proposition 2, we can reduce the constrained optimization problem (48-50) to
solving the following unconstrained max-min optimization."
REFERENCES,0.6894736842105263,"max
w≥0 min
τ≥0,ν τ log E x∼dD"
REFERENCES,0.6912280701754386,a0∼π(s0)
REFERENCES,0.6929824561403509,a′∼π(s′)
REFERENCES,0.6947368421052632,"h
exp

1
τ
 
w(s, a)(Ck(s, a) + γν(s′, a′) −ν(s′, a′)) + (1 −γ)ν(s0, a0)
i
+ τϵ"
REFERENCES,0.6964912280701754,"where maxw≥0 minν(·) is to estimate w(s, a) = dπ(s,a)"
REFERENCES,0.6982456140350877,"˜p(s,a) ."
REFERENCES,0.7,"In contrast, we consider the case when w is given and aim to solve the following constrained opti-
mization problem."
REFERENCES,0.7017543859649122,"max
˜p∈∆(X) E(s0,s,a,s′)∼˜p[w(s, a)Ck(s, a)]
(15)"
REFERENCES,0.7035087719298245,"s.t. DKL(˜p(s0, s, a, s′)||dD(s0, s, a, s′)) ≤ϵ
(16)
P"
REFERENCES,0.7052631578947368,"a′ ˜p(s′, a′)w(s′, a′) = (1 −γ)˜p0(s′) + γ P"
REFERENCES,0.7070175438596491,"s,a
˜p(s, a)w(s, a)˜p(s′|s, a) ∀s′
(17)"
REFERENCES,0.7087719298245614,"This can be reduced to unconstrained minimization problem, without requiring nested optimization
to estimate w:"
REFERENCES,0.7105263157894737,"min
τ≥0,χ τ log Ex∼dD
h
exp

1
τ
 
w(s, a)(Ck(s, a) + γχ(s′) −χ(s)) + (1 −γ)χ(s0)
i
+ τϵ"
REFERENCES,0.712280701754386,Published as a conference paper at ICLR 2022
REFERENCES,0.7140350877192982,"E
EXPERIMENTAL SETTINGS"
REFERENCES,0.7157894736842105,"E.1
RANDOM CMDPS"
REFERENCES,0.7175438596491228,"For random CMDP experiments, We follow a similar experimental protocol as (Laroche et al., 2019;
Lee et al., 2021) with additional consideration of cost constraint."
REFERENCES,0.7192982456140351,"Random CMDP generation
For each run, we constructed a random CMDP with |S| = 50, |A| =
4, γ = 0.95, and a ﬁxed initial state s0. The transition probability is constructed randomly with con-
nectivity of 4, i.e. for each (s, a), we sample 4 states uniformly, and then, the transition probabilities
to those states are determined by Dirichlet(1, 1, 1, 1). The reward of 1 is given to a single state
that minimizes the optimal policy’s reward value at s0, and 0 is given anywhere else. This reward
design can be roughly understood as choosing a goal state that is most difﬁcult to reach from s0.
The cost function is generated randomly, i.e. C(s, a) ∼Beta(0.2, 0.2) for ∀s ∈S, a ∈{a2, a3, a4}
and C(s, a1) = 0 to ensure existence of a feasible policy of the CMDP. Lastly, the cost threshold
ˆc = 0.1 is used."
REFERENCES,0.7210526315789474,"Data-collection policy construction
The pseudo-code for the data-collection policy construction
is presented in Algorithm 2, where M is the underlying true CMDP, and ˆcD ∈{0.09, 0.11} is the
hyperparameter that determines the cost value of πD. Starting from πD = π∗, the policy is softened"
REFERENCES,0.7228070175438597,"Algorithm 2 Data-collection policy construction
Input: CMDP M = ⟨S, A, T, R, C, ˆc, p0, γ⟩, target cost value of the data-collection policy ˆcD"
REFERENCES,0.724561403508772,"Compute the optimal policy π∗and its reward value function Qπ∗
R (s, a) on the given CMDP M.
Initialize πsoft ←π∗and πD ←π∗
Initialize a temperature parameter τ ←10−6
# Compute 0.9-optimal behavior policy in terms of reward performance.
while V πD
R (s0) > 0.9V π∗
R (s0) + 0.1V πunif
R
(s0) do
Set πsoft to πsoft(a|s) ∝exp
  1"
REFERENCES,0.7263157894736842,"τ Qπ∗
R (s, a)

∀s, a
πD ←arg minπ Df(dπ||dπsoft) s.t. E(s,a)∼dπ[C(s, a)] ≤ˆcD
τ ←τ/0.9
end while
Output: The data-collection policy πD"
REFERENCES,0.7280701754385965,"via πsoft(a|s) ∝exp(Q∗
R(s, a)/τ) while projecting it into the cost-satisfying one by solving πD ←
arg minπ Df(dπ||dπsoft) s.t. E(s,a)∼dπ[C(s, a)] ≤ˆcD. This process is repeated until the reward
performance of πD reaches 0.9-optimality while increasing the temperature τ, i.e. V πD
R (s0) =
0.9V π∗
R (s0) + 0.1V πunif
R
(s0)."
REFERENCES,0.7298245614035088,"After the data-collection policy πD is constructed, we sample trajectories using πD, which con-
stitutes the ofﬂine dataset D. We conducted experiments for a varying number of trajectories, i.e.
(the number of trajectories) ∈{10, 20, 50, 100, 200, 500, 1000, 2000}. Each episode is terminated
either when 50 time steps have reached or the agent reached the goal state that yields a non-zero
reward."
REFERENCES,0.7315789473684211,"Hyperparameters
For tabular COptiDICE, we used α =
1
N and ϵ = 0.1"
REFERENCES,0.7333333333333333,"N , where N denotes the
number of trajectories in D. We also used f(x) = 1"
REFERENCES,0.7350877192982456,"2(x −1)2, which corresponds to χ2-divergence."
REFERENCES,0.7368421052631579,"C-SPIBB: Constrained variant of SPIBB
C-SPIBB solves the ofﬂine RL problem with respect
to R −λC via SPIBB while updating λ in the direction of ( ˆVC(πSPIBB) −ˆc), where ˆVC(πSPIBB) is
evaluated using the MLE CMDP. For C-SPIBB, we used N∧= 5 as the hyperparameter of SPIBB."
REFERENCES,0.7385964912280701,Published as a conference paper at ICLR 2022
REFERENCES,0.7403508771929824,"E.2
RWRL CONTROL TASKS"
REFERENCES,0.7421052631578947,"Network architecture and hyperparameters
We used the ACME framework (Hoffman et al.,
2020). For the νθ network and the χφ network, we used LayerNormMLP with hidden sizes of
[512, 512, 256]. For the policy network πψ, we used the network architecture used in CRR (Wang
et al., 2020). Speciﬁcally, we the πψ network consists of 4 ResidualMLP blocks with hidden size
of 1024 and a mixture of Gaussians policy head with 5 mixture components. We used the batch size
of 1024. We use Adam optimizer with learning rate 3e-4. Similar to ValueDICE (Kostrikov et al.,
2021), we additionally adopt gradient penalties (Gulrajani et al., 2017) for the νθ network and the
χφ network with coefﬁcient 10e-5. We performed grid search for α ∈{0.01, 0.05, 0.1} for each
domain. We used the following f as in OptiDICE (Lee et al., 2021):"
REFERENCES,0.743859649122807,"f(x) =
x log x −x + 1
if 0 < x < 1
1
2(x −1)2
if x ≥1"
REFERENCES,0.7456140350877193,"Task speciﬁcation
We conduct experiments on domains using the RWRL suite with safety-spec.
The safety coefﬁcient is a ﬂag in the RWRL suite with safety-spec, and its value can be between 0.0
and 1.0. Lowering the value of the ﬂag incurs more safety-constraint violation (cost of 1). Originally,
each domain has multiple types of safety constraints, but we use only one of them, which was the
hardest safety constraint to be satisﬁed by an online constrained RL agent. In summary, we used the
following task speciﬁcations (safety coefﬁcients, name of the used safety constraint, cost threshold
ˆc)."
REFERENCES,0.7473684210526316,"• Cartpole (realworld-swingup): safety-coeff=0.3, slider pos, ˆc = 0.1,"
REFERENCES,0.7491228070175439,"• Walker (realworld-walk): safety-coeff=0.3, joint velocity constraint ˆc = 0.1."
REFERENCES,0.7508771929824561,"• Quadruped (realworld-walk): safety-coeff=0.5, joint angle constraint ˆc = 0.3."
REFERENCES,0.7526315789473684,"• Humanoid (realworld-walk): safety-coeff=0.5, joint angle constraint, ˆc = 0.3."
REFERENCES,0.7543859649122807,"The ofﬂine dataset consists trajectories of 1000 episodes for Cartpole and Walker, and 5000 episodes
for Quadruped and Humanoid."
REFERENCES,0.756140350877193,"C-CRR: Constrained variant of CRR
C-CRR additionally introduces the cost critic QC and the
Lagrange multiplier λ for the cost constraint. Then, the reward critic and the cost critic are trained
by minimizing TD-error for reward and cost respectively. The actor is trained by weighted behavior-
cloning: maxπ E(s,a)∼dD[exp( ˆA(s, a)/β) log π(a|s)] where ˆA(s, a) = (QR(s, a) −λQC(s, a)) −
1
m
Pm
j=1(QR(s, aj) −λQC(s, aj)), with aj ∼π(·|s). This corresponds to optimizing the policy
with respect to the scalarized reward R −λC. The Lagrange multiplier λ is updated in the direction
of (E(s,a)∼dD[QC(s, a)] −ˆc)."
REFERENCES,0.7578947368421053,Published as a conference paper at ICLR 2022
REFERENCES,0.7596491228070176,"F
DISCUSSION ON THE DATASET COVERAGE ASSUMPTION"
REFERENCES,0.7614035087719299,"Although we have made an assumption dD > 0 in Section 3, this is not strictly required for the
ofﬂine RL algorithm to work in practice. We adopted this assumption same as OptiDICE (Lee
et al., 2021) for the simplicity of describing the algorithm derivation from Eq (4-7) to Eq (10): the
assumption makes Eq. (10) correspond to solving the underlying true CMDP of Eq. (4-7). If we
take this assumption off, Eq. (10) then becomes equivalent to solving the reduced CMDP where the
state and action spaces are limited to the support of dD."
REFERENCES,0.7631578947368421,"To see this, we consider the following constrained optimization problem, where the optimization
variables d(s, a) are deﬁned only for the (s, a) who are within the support of dD. We denote ˆp0 and
ˆT as the empirical initial state distribution and the empirical transition function respectively."
REFERENCES,0.7649122807017544,"max
d
P"
REFERENCES,0.7666666666666667,"(s,a)∈Supp(dD)
d(s, a)R(s, a) −αDf(d||dD)
(52)"
REFERENCES,0.7684210526315789,"s.t.
P"
REFERENCES,0.7701754385964912,"(s,a)∈Supp(dD)
d(s, a)[Ck(s, a)] ≤ˆck
∀k = 1, . . . , K P"
REFERENCES,0.7719298245614035,"a′∈Supp(dD)
d(s′, a′) = (1 −γ)ˆp0(s′) + γ
P"
REFERENCES,0.7736842105263158,"(s,a)∈Supp(dD)
d(s, a) ˆT(s′|s, a)
∀s′ ∈Supp(dD)"
REFERENCES,0.775438596491228,"d(s, a) ≥0
∀s, a ∈Supp(dD)"
REFERENCES,0.7771929824561403,"Then, by considering the Lagrangian for (52) and following the similar derivation of Eq. (8-9),
we can ﬁnally arrive at Eq. (10) without any assumption on the coverage of dD, due to the fact
that P"
REFERENCES,0.7789473684210526,"(s,a)∈Supp(dD)[d(s, a)(·)] = P"
REFERENCES,0.7807017543859649,"(s,a)∈Supp(dD) dD(s, a) d(s,a)"
REFERENCES,0.7824561403508772,"dD(s,a)(·) = E(s,a)∼dD
 d(s,a)"
REFERENCES,0.7842105263157895,"dD(s,a)(·)
"
REFERENCES,0.7859649122807018,"always holds. In other words, our ofﬂine RL algorithms that rely on Eq. (10) essentially solve the
reduced CMDP that is limited to the support of dD."
REFERENCES,0.787719298245614,Published as a conference paper at ICLR 2022
REFERENCES,0.7894736842105263,"G
ABLATION EXPERIMENTS ON DIFFERENT COST THRESHOLDS"
REFERENCES,0.7912280701754386,"In order to see the sensitivity of the proposed method to different cost thresholds, we conduct abla-
tion experiments on different cost thresholds using randomly generated CMDPs. The experimental
setup is identical to the one described in Section 4.1."
REFERENCES,0.7929824561403509,"10
1
10
2
10
3"
REFERENCES,0.7947368421052632,number of trajectories in D 0.08 0.10 0.12 Cost
REFERENCES,0.7964912280701755,"10
1
10
2
10
3"
REFERENCES,0.7982456140350878,number of trajectories in D 0.5 0.0 0.5 1.0
REFERENCES,0.8,(a) when VC(πD) = 0.09
REFERENCES,0.8017543859649123,Reward
REFERENCES,0.8035087719298246,"10
1
10
2
10
3"
REFERENCES,0.8052631578947368,number of trajectories in D 0.08 0.10 0.12 0.14 Cost
REFERENCES,0.8070175438596491,"10
1
10
2
10
3"
REFERENCES,0.8087719298245614,number of trajectories in D 1 0 1
REFERENCES,0.8105263157894737,(b) when VC(πD) = 0.11
REFERENCES,0.8122807017543859,Reward
REFERENCES,0.8140350877192982,"10
1
10
2
10
3"
REFERENCES,0.8157894736842105,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.8175438596491228,"10
1
10
2
10
3"
REFERENCES,0.8192982456140351,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.8210526315789474,"10
1
10
2
10
3"
REFERENCES,0.8228070175438597,number of trajectories in D 0.08 0.10 0.12 0.14
REFERENCES,0.8245614035087719,"10
1
10
2
10
3"
REFERENCES,0.8263157894736842,number of trajectories in D 1 0 1
REFERENCES,0.8280701754385965,"10
1
10
2
10
3"
REFERENCES,0.8298245614035088,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.8315789473684211,"10
1
10
2
10
3"
REFERENCES,0.8333333333333334,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.8350877192982457,"10
1
10
2
10
3"
REFERENCES,0.8368421052631579,number of trajectories in D 0.08 0.10 0.12 0.14
REFERENCES,0.8385964912280702,"10
1
10
2
10
3"
REFERENCES,0.8403508771929824,number of trajectories in D 0.5 0.0 0.5 1.0
REFERENCES,0.8421052631578947,"10
1
10
2
10
3"
REFERENCES,0.843859649122807,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.8456140350877193,"10
1
10
2
10
3"
REFERENCES,0.8473684210526315,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.8491228070175438,"10
1
10
2
10
3"
REFERENCES,0.8508771929824561,number of trajectories in D 0.08 0.10 0.12 0.14
REFERENCES,0.8526315789473684,"10
1
10
2
10
3"
REFERENCES,0.8543859649122807,number of trajectories in D 0.5 0.0 0.5 1.0
REFERENCES,0.856140350877193,"10
1
10
2
10
3"
REFERENCES,0.8578947368421053,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.8596491228070176,"10
1
10
2
10
3"
REFERENCES,0.8614035087719298,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.8631578947368421,"10
1
10
2
10
3"
REFERENCES,0.8649122807017544,number of trajectories in D 0.08 0.10 0.12 0.14
REFERENCES,0.8666666666666667,"10
1
10
2
10
3"
REFERENCES,0.868421052631579,number of trajectories in D 0.5 0.0 0.5 1.0
REFERENCES,0.8701754385964913,"Cost threshold ˆc
Baseline"
REFERENCES,0.8719298245614036,"Data-collection policy
C-SPIBB"
REFERENCES,0.8736842105263158,"Optimal policy
COptiDICE (by (12))"
REFERENCES,0.875438596491228,"BC
COptiDICE"
REFERENCES,0.8771929824561403,"Figure 4: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs. Plots for
the ﬁrst two columns in (a) correspond to the case when the cost value of the data-collection policy
is 0.09 (corresponding to Figure 1a). Plots for the last two columns in (b) correspond to the case
when the cost value of the data-collection policy is 0.11 (corresponding to Figure 1b). The mean
of normalized reward performance and the mean of cost value are reported for 10K runs, where the
error bar denotes the standard error."
REFERENCES,0.8789473684210526,"On the same data-collection policy (whose cost value is either 0.09 or 0.11) and the same of-
ﬂine dataset as in Figure 1, we tested each algorithm against different target cost thresholds
ˆc ∈{0.08, 0.09, 0.10, 0.11, 0.12}. Each row in Figure 4 presents the result for each target cost
threshold, ranging from ˆc = 0.08 to ˆc = 0.12. Our COptiDICE (blue) shows consistent robustness
to constraint violation on varying cost thresholds, while other algorithms fail to meet the constraints
especially when the threshold is set to low values (e.g. rows 1 and 2)."
REFERENCES,0.8807017543859649,Published as a conference paper at ICLR 2022
REFERENCES,0.8824561403508772,"H
ADDITIONAL EXPERIMENTS USING MIXTURE DATASET"
REFERENCES,0.8842105263157894,"In Section 4.1, we demonstrated the results when the ofﬂine dataset was collected by a single data-
collection policy. However, in real-world situations, it would be common that data-collecting agents
act safely in most cases but have some unsafe attempts. To simulate this scenario, we conduct addi-
tional experiments on the use of a mixture dataset, where the dataset is collected by both constraint-
satisfying and constraint-violating policy."
REFERENCES,0.8859649122807017,"10
1
10
2
10
3"
REFERENCES,0.887719298245614,number of trajectories in D 0.10 0.12 Cost
REFERENCES,0.8894736842105263,"10
1
10
2
10
3"
REFERENCES,0.8912280701754386,number of trajectories in D 0.50 0.55
REFERENCES,0.8929824561403509,"(a) 80% of trajectories are by cost-satisfying policy,"
REFERENCES,0.8947368421052632,20% of trajectories are by cost-violating policy
REFERENCES,0.8964912280701754,Reward
REFERENCES,0.8982456140350877,"10
1
10
2
10
3"
REFERENCES,0.9,number of trajectories in D 0.10 0.12 Cost
REFERENCES,0.9017543859649123,"10
1
10
2
10
3"
REFERENCES,0.9035087719298246,number of trajectories in D 0.50 0.55
REFERENCES,0.9052631578947369,"(b) 20% of trajectories are by cost-satisfying policy,"
REFERENCES,0.9070175438596492,80% of trajectories are by cost-violating policy
REFERENCES,0.9087719298245615,Reward
REFERENCES,0.9105263157894737,"Cost threshold ˆc
C-SPIBB"
REFERENCES,0.9122807017543859,"Optimal policy
COptiDICE (by (12))"
REFERENCES,0.9140350877192982,"BC
COptiDICE"
REFERENCES,0.9157894736842105,Baseline
REFERENCES,0.9175438596491228,"Figure 5: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs, using
mixture dataset. Plots (a) correspond to the case when the dataset is collected generally by the
cost-satisfying policy, i.e. 80% of trajectories are by the cost-satisfying policy and 20% are by the
cost-violating policy. Plots (b) correspond to the case when the dataset is collected generally by
the cost-violating policy, i.e. 20% of trajectories are by the cost-satisfying policy and 80% are by the
cost-violating policy. The mean (unnormalized) reward value and the mean cost value are reported
for 10K runs, where the error bar denotes the standard error."
REFERENCES,0.9192982456140351,"Figure 5 presents the result, where the dataset is collected by two policies, the cost-satisfying one
(i.e. its cost value is 0.09) and the cost-violating one (i.e. its cost value is 0.11). The overall trend
remains the same as in Figure 1 (Figure 5a ≈Figure 1a, Figure 5b ≈Figure 1b): when the dataset
consists of mostly cost-satisfying trajectories, baseline algorithms exhibits less constraint violation,
while they show more constraint violation when the dataset consists of mostly cost-violating ones.
Our COptiDICE (blue) still shows much stricter cost satisfaction than other baseline algorithms,
demonstrating a better trade-off between reward maximization and constraint satisfaction than base-
lines."
REFERENCES,0.9210526315789473,"Furthermore, solving the reduced CMDP is a valid method in the ofﬂine RL setting since we may
want to optimize the policy only within the dataset support in order to prevent unexpected perfor-
mance degradation by out-of-distribution actions."
REFERENCES,0.9228070175438596,Published as a conference paper at ICLR 2022
REFERENCES,0.9245614035087719,"I
ADDITIONAL EXPERIMENTS ON DATA-COLLECTION POLICY WITH
LIMITED EXPLORATION POWER"
REFERENCES,0.9263157894736842,"For the random CMDP experiment, we conducted additional experiments to see the results when
the data-collection policy πD does not cover the entire state-action space well. Speciﬁcally, we limit
the support of πD(·|s): πD(a|s) will have non-zero probabilities only for a ∈˜A where ˜A ⊂A
is the subset of A = {a1, a2, a3, a4}. By doing so, we ensure that πD covers only a part of the
entire state-action space. The detailed construction procedure of πD is described in Algorithm 3.
We conduct experiments for ˜A = {a1}, ˜A = {a1, a2}, and ˜A = {a1, a2, a3}, and the results are
presented in Figure 6."
REFERENCES,0.9280701754385965,Algorithm 3 Constructing a data-collection policy with limited action support
REFERENCES,0.9298245614035088,"Input: CMDP M = ⟨S, A, T, R, C, ˆc, p0, γ⟩, the subset of the entire actions, ˜A ⊂A, to be consid-
ered by the data-collection policy, ˆcD: the target threshold of the data-collection policy.
1: for each s ∈S and a ∈A do
2:"
REFERENCES,0.9315789473684211,"˜C(s, a) ←
C(s, a)
if a ∈˜A
∞
if a /∈˜A
# Eliminate action a /∈˜A by assigning a large cost."
REFERENCES,0.9333333333333333,"˜πunif(a|s) ←
1/| ˜A|
if a ∈˜A
0
if a /∈˜A"
REFERENCES,0.9350877192982456,"3: end for
4: ρ ←1
5: while true do
6:
# Artiﬁcially lower the cost threshold (ˆcD · ρ) until πD meets the target constraint.
7:
˜π∗←SolveCMDP(S, A, T, R, ˜C, ˆcD · ρ, p0, γ)
8:
πD ←0.9 · ˜π∗+ 0.1 · ˜πunif
# To make a stochastic policy within ˜A.
9:
if VC(πD) > ˆcD then
10:
ρ ←ρ ∗0.99
11:
else
12:
break
13:
end if
14: end while
Output: πD: the data-collection policy with limited action support."
REFERENCES,0.9368421052631579,Published as a conference paper at ICLR 2022
REFERENCES,0.9385964912280702,"10
1
10
2
10
3"
REFERENCES,0.9403508771929825,number of trajectories in D 0.00 0.05 0.10 Cost
REFERENCES,0.9421052631578948,"10
1
10
2
10
3"
REFERENCES,0.9438596491228071,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.9456140350877194,(a) when the data-collection policy is cost-satisfying
REFERENCES,0.9473684210526315,Reward
REFERENCES,0.9491228070175438,"10
1
10
2
10
3"
REFERENCES,0.9508771929824561,number of trajectories in D 0.00 0.05 0.10 Cost
REFERENCES,0.9526315789473684,"10
1
10
2
10
3"
REFERENCES,0.9543859649122807,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.956140350877193,(b) when the data-collection policy is cost-violating
REFERENCES,0.9578947368421052,Reward
REFERENCES,0.9596491228070175,"10
1
10
2
10
3"
REFERENCES,0.9614035087719298,number of trajectories in D 0.07 0.08 0.09 0.10 0.11
REFERENCES,0.9631578947368421,"10
1
10
2
10
3"
REFERENCES,0.9649122807017544,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.9666666666666667,"10
1
10
2
10
3"
REFERENCES,0.968421052631579,number of trajectories in D 0.07 0.08 0.09 0.10 0.11
REFERENCES,0.9701754385964912,"10
1
10
2
10
3"
REFERENCES,0.9719298245614035,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.9736842105263158,"10
1
10
2
10
3"
REFERENCES,0.9754385964912281,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.9771929824561404,"10
1
10
2
10
3"
REFERENCES,0.9789473684210527,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.980701754385965,"10
1
10
2
10
3"
REFERENCES,0.9824561403508771,number of trajectories in D 0.08 0.10 0.12
REFERENCES,0.9842105263157894,"10
1
10
2
10
3"
REFERENCES,0.9859649122807017,number of trajectories in D 0.0 0.5 1.0
REFERENCES,0.987719298245614,"Cost threshold ˆc
Baseline"
REFERENCES,0.9894736842105263,"Data-collection policy
C-SPIBB"
REFERENCES,0.9912280701754386,"Optimal policy
COptiDICE (by (12))"
REFERENCES,0.9929824561403509,"BC
COptiDICE"
REFERENCES,0.9947368421052631,"Figure 6: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs when
the data-collection policy has limited exploration power. Plots for the ﬁrst two columns in (a)
correspond to the case when the data-collection policy is cost-satisfying. Plots for the last two
columns in (b) correspond to the case when the data-collection policy is cost-violating (except for
the ﬁrst row). The ﬁrst row denotes the case when ˜A = {a1}, the second row denotes the case
when ˜A = {a1, a2}, and the third row denotes the case when ˜A = {a1, a2, a3}. The mean of
normalized reward performance and the mean of cost value are reported for 10K runs, where the
error bar denotes the standard error."
REFERENCES,0.9964912280701754,"The ﬁrst row of Figure 6 shows the result when ˜A = {a1}. Since a1 is the zero-cost action as
described in Appendix E, the cost value of πD is always 0. Besides, since the dataset contains
only a single action a1 for each state due to the deterministic data-collection policy, no ofﬂine RL
algorithms can improve the performance beyond the data-collection policy: there must be more than
one action in some states in the dataset, in order for ofﬂine RL algorithms to have room to improve
the performance in general."
REFERENCES,0.9982456140350877,"The second row and the third row of Figure 6 shows the result when ˜A = {a1, a2} and ˜A =
{a1, a2, a3} respectively. As the size of ˜A increases from 2 to 3, ofﬂine RL algorithms further im-
prove the performance over the data-collection policy. This is natural since the larger ˜A implies that
there is more room for ofﬂine RL algorithms to optimize. Finally, even when the exploration power
of the data-collection policy is limited, our COptiDICE (blue) shows a consistent advantage over
baseline algorithms, in terms of the better trade-off between reward maximization and constraint
satisfaction."
