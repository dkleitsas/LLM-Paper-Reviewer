Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002004008016032064,"Contextual Bandits with Knapsacks (CBwK) is a fundamental and essential frame-
1"
ABSTRACT,0.004008016032064128,"work for modeling a dynamic decision-making scenario with resource constraints.
2"
ABSTRACT,0.006012024048096192,"Under this framework, an agent selects an action in each round upon observing a
3"
ABSTRACT,0.008016032064128256,"request, leading to a reward and resource consumption that are further associated
4"
ABSTRACT,0.01002004008016032,"with an unknown external factor. The agent’s target is to maximize the total reward
5"
ABSTRACT,0.012024048096192385,"under the initial inventory. While previous research has already established an
6 eO(
√"
ABSTRACT,0.014028056112224449,"T) worst-case regret for this problem, this work offers two results that go
7"
ABSTRACT,0.01603206412825651,"beyond the worst-case perspective, one for worst-case locations, and another for
8"
ABSTRACT,0.018036072144288578,"logarithmic regret rates. We start by demonstrating that the unique-optimality and
9"
ABSTRACT,0.02004008016032064,"degeneracy of the fluid LP problem, which is both succinct and easily verifiable,
10"
ABSTRACT,0.022044088176352707,"is a sufficient condition for the existence of an Ω(
√"
ABSTRACT,0.02404809619238477,"T) regret lower bound. To
11"
ABSTRACT,0.026052104208416832,"supplement this worst-case location result, we merge the re-solving heuristic with
12"
ABSTRACT,0.028056112224448898,"distribution estimation skills and propose an algorithm that achieves an eO(1) regret
13"
ABSTRACT,0.03006012024048096,"as long as the fluid LP has a unique and non-degenerate solution. This condition
14"
ABSTRACT,0.03206412825651302,"is mild as it is satisfied for most problem instances. Furthermore, we prove our
15"
ABSTRACT,0.03406813627254509,"algorithm maintains a near-optimal eO(
√"
ABSTRACT,0.036072144288577156,"T) regret even in the worst cases, and
16"
ABSTRACT,0.03807615230460922,"extend these results to the setting where request and external factor are continuous.
17"
ABSTRACT,0.04008016032064128,"Regarding information, our regret results are obtained under two feedback models,
18"
ABSTRACT,0.04208416833667335,"respectively, where the algorithm accesses the external factor at the end of each
19"
ABSTRACT,0.04408817635270541,"round and at the end of a round only when a non-null action is executed.
20"
INTRODUCTION,0.04609218436873747,"1
Introduction
21"
INTRODUCTION,0.04809619238476954,"In the contextual bandits problem with knapsack constraints (CBwK problem for short), an agent is
22"
INTRODUCTION,0.050100200400801605,"required to make sequential decisions over a finite time horizon to maximize the accumulated reward
23"
INTRODUCTION,0.052104208416833664,"under initial resource constraints. To be more specific, in each round t = 1, · · · , T, a request θt and
24"
INTRODUCTION,0.05410821643286573,"an external factor γt are independently generated from two distributions, and only θt is revealed to
25"
INTRODUCTION,0.056112224448897796,"the agent. Based on the request, the agent should irrevocably choose an action at, which would result
26"
INTRODUCTION,0.05811623246492986,"in a reward r(θt, at, γt) and a consumption vector c(θt, at, γt) of resources. The agent’s target is to
27"
INTRODUCTION,0.06012024048096192,"optimize the sum of rewards PT
t=1 r(θt, at, γt) before the resources are depleted.
28"
INTRODUCTION,0.06212424849699399,"The CBwK problem presents two key challenges when compared to closely related problems (e.g.,
29"
INTRODUCTION,0.06412825651302605,"network revenue management problem) : (1) choices are made without observing external factors,
30"
INTRODUCTION,0.06613226452905811,"and (2) distributions of requests and external factors are unknown. However, the complexity of CBwK
31"
INTRODUCTION,0.06813627254509018,"makes it a suitable mathematical abstraction for many real-life scenarios, such as dynamic bidding in
32"
INTRODUCTION,0.07014028056112225,"repeated second-price auctions with budgets [Balseiro et al., 2021, Balseiro and Gur, 2019]. In this
33"
INTRODUCTION,0.07214428857715431,"circumstance, an advertiser (the agent) acquires the value of the ad slot (the request) at the start of
34"
INTRODUCTION,0.07414829659318638,"each auction, and would choose a bid (the action) accordingly. The agent’s utility and payment in
35"
INTRODUCTION,0.07615230460921844,"this auction, as a consequence, are collaboratively determined by the value, the bid, and the highest
36"
INTRODUCTION,0.0781563126252505,"competing bid (the external factor). It is to be noted that the highest competing bid is inaccessible
37"
INTRODUCTION,0.08016032064128256,"to the agent before committing to the bid, as all advertisers bid simultaneously. Meanwhile, its
38"
INTRODUCTION,0.08216432865731463,"distribution is decided by other advertisers, which is also unknown to the agent before the auctions.
39"
INTRODUCTION,0.0841683366733467,"The CBwK model can also capture other well-discussed problems including multi-secretary, online
40"
INTRODUCTION,0.08617234468937876,"linear programming, online matching, as discussed in Balseiro et al. [2021].
41"
INTRODUCTION,0.08817635270541083,"Previous studies of the CBwK problem have shown that the worst-case regret of any online strategy
42"
INTRODUCTION,0.09018036072144289,"is eO(
√"
INTRODUCTION,0.09218436873747494,"T) when the initial resources are linearly proportional to the horizon length T [Slivkins and
43"
INTRODUCTION,0.09418837675350701,"Foster, 2022, Han et al., 2022]. 1 However, it is still unclear where worst-case scenarios occur,
44"
INTRODUCTION,0.09619238476953908,"meaning under which condition(s) an eΩ(
√"
INTRODUCTION,0.09819639278557114,"T) regret is inevitable. Furthermore, we do not know
45"
INTRODUCTION,0.10020040080160321,"whether we can achieve a better regret guarantee for the CBwK problem beyond worst-case scenarios.
46"
INTRODUCTION,0.10220440881763528,"In particular, can we design algorithms to obtain an o(
√"
INTRODUCTION,0.10420841683366733,"T) regret only under mild assumptions that
47"
INTRODUCTION,0.1062124248496994,"hold for almost all possible CBwK instances? This work takes the first step in addressing these
48"
INTRODUCTION,0.10821643286573146,"questions.
49"
OUR CONTRIBUTIONS,0.11022044088176353,"1.1
Our Contributions
50"
OUR CONTRIBUTIONS,0.11222444889779559,"This work mainly makes three contributions, summarized as follows.
51"
OUR CONTRIBUTIONS,0.11422845691382766,"A precise sufficient condition for an eΩ(
√"
OUR CONTRIBUTIONS,0.11623246492985972,"T) regret lower bound.
To move beyond worst-case
52"
OUR CONTRIBUTIONS,0.11823647294589178,"analysis, we establish a precise sufficient condition for the eΩ(
√"
OUR CONTRIBUTIONS,0.12024048096192384,"T) regret lower bound to hold.
53"
OUR CONTRIBUTIONS,0.12224448897795591,"Specifically, we demonstrate that when the fluid benchmark (also known as the deterministic LP)
54"
OUR CONTRIBUTIONS,0.12424849699398798,"has a unique and degenerate solution, then an Ω(
√"
OUR CONTRIBUTIONS,0.12625250501002003,"T) regret is unavoidable for any online decision
55"
OUR CONTRIBUTIONS,0.1282565130260521,"strategy (Theorem 2.1). While Han et al. [2022] have also provided a regret lower bound result
56"
OUR CONTRIBUTIONS,0.13026052104208416,"for the CBwK problem, their condition depends on the inseparability of the possible expected
57"
OUR CONTRIBUTIONS,0.13226452905811623,"reward/consumption function set. In other words, their condition may not perform well when this
58"
OUR CONTRIBUTIONS,0.1342685370741483,"feasible set is small. Furthermore, their condition is rather complicated to verify. In contrast, our
59"
OUR CONTRIBUTIONS,0.13627254509018036,"condition only depends on the underlying problem instance, is concise, and is easy to check. The
60"
OUR CONTRIBUTIONS,0.13827655310621242,"proof of our result extends the approach of Vera and Banerjee [2021] to the CBwK problem.
61"
OUR CONTRIBUTIONS,0.1402805611222445,"An eO(1) regret via re-solving under mild assumptions with full/partial information feedback.
62"
OUR CONTRIBUTIONS,0.14228456913827656,"With the above result, we investigate how well an online algorithm can perform beyond worst cases,
63"
OUR CONTRIBUTIONS,0.14428857715430862,"by applying the re-solving heuristic in conjunction with distribution estimation techniques, as given
64"
OUR CONTRIBUTIONS,0.1462925851703407,"in Algorithm 1. Although this method has been considered in the problem of bandits with knapsacks
65"
OUR CONTRIBUTIONS,0.14829659318637275,"(BwK) [Flajolet and Jaillet, 2015], to the best of our knowledge, we are the first to extend this
66"
OUR CONTRIBUTIONS,0.15030060120240482,"method to the CBwK problem, which poses new challenge as decisions should be made according
67"
OUR CONTRIBUTIONS,0.1523046092184369,"to the request. To avoid worst cases, we explicitly suppose that the fluid problem has a unique and
68"
OUR CONTRIBUTIONS,0.15430861723446893,"non-degenerate solution (Assumption 3.1). This assumption is mild in three aspects: (1) it captures
69"
OUR CONTRIBUTIONS,0.156312625250501,"almost all CBwK problem instances, as slightly perturbing any LP can help it satisfy the unique
70"
OUR CONTRIBUTIONS,0.15831663326653306,"optimality and non-degeneracy conditions; (2) it is almost necessary for an o(
√"
OUR CONTRIBUTIONS,0.16032064128256512,"T) regret bound to
71"
OUR CONTRIBUTIONS,0.1623246492985972,"establish by Theorem 2.1 as we just discussed, only left the case that the fluid problem has multiple
72"
OUR CONTRIBUTIONS,0.16432865731462926,"optimal solutions; and (3) it is far less restrictive than the assumptions given in Sankararaman and
73"
OUR CONTRIBUTIONS,0.16633266533066132,"Slivkins [2021], which require that there are at most two resources and the best-arm optimality, and
74"
OUR CONTRIBUTIONS,0.1683366733466934,"almost surely excludes all problem instances. Under the assumption, our main results show that the
75"
OUR CONTRIBUTIONS,0.17034068136272545,"re-solving heuristic reaches an O(1) regret with full information (Theorem 3.1) and an O(log T)
76"
OUR CONTRIBUTIONS,0.17234468937875752,"regret with partial information (Theorem 4.1). To our knowledge, these are the first eO(1) regret
77"
OUR CONTRIBUTIONS,0.1743486973947896,"results in the CBwK problem beyond the worst case with only mild assumptions. Importantly, these
78"
OUR CONTRIBUTIONS,0.17635270541082165,"regret bounds are also independent to the number of actions, unlike previous results.
79"
OUR CONTRIBUTIONS,0.17835671342685372,"Within our results, the full information model assumes that the agents sees the external factor at
80"
OUR CONTRIBUTIONS,0.18036072144288579,"the end of each round, while in the partial information model, the agent acquires the external factor
81"
OUR CONTRIBUTIONS,0.18236472945891782,"only when a non-null action is adopted. Other state-of-the-art results consider bandit information
82"
OUR CONTRIBUTIONS,0.1843687374749499,"feedback, in which the agent only sees the reward and the consumption rather than the external factor.
83"
OUR CONTRIBUTIONS,0.18637274549098196,"1In this work, a strategy’s regret is defined as the gap between its expected total reward and the fluid
benchmark (to be introduced in Section 2), which has known to be an upper bound of the former. Such
a definition is implicitly yet widely adopted in the literature [Slivkins and Foster, 2022, Han et al., 2022,
Sivakumar et al., 2022]."
OUR CONTRIBUTIONS,0.18837675350701402,"However, they explicitly assume a specific (e.g., linear) relationship between the conditional expected
84"
OUR CONTRIBUTIONS,0.1903807615230461,"reward-consumption pair and the request [Agrawal and Devanur, 2016, Sankararaman and Slivkins,
85"
OUR CONTRIBUTIONS,0.19238476953907815,"2021, Han et al., 2022, Slivkins and Foster, 2022], whereas our results do not impose any underlying
86"
OUR CONTRIBUTIONS,0.19438877755511022,"distribution structures. On this side, our information model are comparable to those in existing work.
87"
OUR CONTRIBUTIONS,0.1963927855711423,"A near-optimal regret even in worst cases with full/partial information feedback, and an
88"
OUR CONTRIBUTIONS,0.19839679358717435,"extension to continuous randomness.
We further explore how well our Algorithm 1 performs
89"
OUR CONTRIBUTIONS,0.20040080160320642,"even in worst-case scenarios. With full information feedback, we show that an O(√T log T) regret
90"
OUR CONTRIBUTIONS,0.20240480961923848,"is achieved (Theorem 5.1). This bound is asymptotically equal to the state-of-the-arts with this
91"
OUR CONTRIBUTIONS,0.20440881763527055,"information model [Han et al., 2022, Slivkins and Foster, 2022]. Even with partial information,
92"
OUR CONTRIBUTIONS,0.20641282565130262,"we can still guarantee a universal O(
√"
OUR CONTRIBUTIONS,0.20841683366733466,"T log T) regret (Theorem 5.2), which is optimal up to a
93"
OUR CONTRIBUTIONS,0.21042084168336672,"logarithmic factor. These results demonstrate the applicability of the re-solving heuristic in CBwK
94"
OUR CONTRIBUTIONS,0.2124248496993988,"problems, regardless of the specific instance. For completeness, we also extend our algorithm and
95"
OUR CONTRIBUTIONS,0.21442885771543085,"analysis to the situation in which the randomness of request and external factor are continuous, and
96"
OUR CONTRIBUTIONS,0.21643286573146292,"derive corresponding regret results (Theorems A.1 and A.2).
97"
LITERATURE REVIEW,0.218436873747495,"1.2
Literature Review
98"
LITERATURE REVIEW,0.22044088176352705,"Contextual bandits with knapsacks.
The contextual bandits with knapsacks framework was
99"
LITERATURE REVIEW,0.22244488977955912,"introduced by Agrawal and Devanur [2016]. Along this research line, two main methodologies
100"
LITERATURE REVIEW,0.22444889779559118,"have been proposed to solve the problem. The first approach aims to select the best probabilistic
101"
LITERATURE REVIEW,0.22645290581162325,"strategy within the policy set [Badanidiyuru et al., 2014], and Agrawal et al. [2016] adopts this
102"
LITERATURE REVIEW,0.22845691382765532,"approach to achieve an eO(
√"
LITERATURE REVIEW,0.23046092184368738,"T) regret. This heuristic originates from the subject of contextual bandits
103"
LITERATURE REVIEW,0.23246492985971945,"[Dudik et al., 2011, Agarwal et al., 2014], and requires a cost-sensitive classification oracle to achieve
104"
LITERATURE REVIEW,0.23446893787575152,"computation efficiency.
105"
LITERATURE REVIEW,0.23647294589178355,"On the other hand, another approach views the problem from the perspective of the Lagrangian
106"
LITERATURE REVIEW,0.23847695390781562,"dual space. It uses a dual update method that reduces the CBwK problem to the online convex
107"
LITERATURE REVIEW,0.24048096192384769,"optimization (OCO) problem. In particular, some work [Agrawal and Devanur, 2016, Sankararaman
108"
LITERATURE REVIEW,0.24248496993987975,"and Slivkins, 2021, Sivakumar et al., 2022, Liu and Grigas, 2022] assumes a linear relationship
109"
LITERATURE REVIEW,0.24448897795591182,"between the conditional expectation of the reward-consumption pair and the request-action pair. This
110"
LITERATURE REVIEW,0.24649298597194388,"line adopts techniques for estimating linear function classes [Abbasi-Yadkori et al., 2011, Auer, 2002,
111"
LITERATURE REVIEW,0.24849699398797595,"Sivakumar et al., 2020, Elmachtoub and Grigas, 2022] and combines them with OCO methods to
112"
LITERATURE REVIEW,0.250501002004008,"achieve sub-linear regret. Among these works, [Sankararaman and Slivkins, 2021] shows that when
113"
LITERATURE REVIEW,0.25250501002004005,"there are only two resources and a best-arm, this method can obtain an O(log T) regret. Compared
114"
LITERATURE REVIEW,0.2545090180360721,"with their results, our assumptions are much milder, as we only assume non-degeneracy.
115"
LITERATURE REVIEW,0.2565130260521042,"From another angle, depending on the difficulty of overcoming the lack of distribution knowledge on
116"
LITERATURE REVIEW,0.25851703406813625,"the external factor, there are two types of feedback models in the literature: full or bandit information.
117"
LITERATURE REVIEW,0.2605210420841683,"In the former [Liu and Grigas, 2022], the agent sees the external factor at the end of each round and can
118"
LITERATURE REVIEW,0.2625250501002004,"derive the reward and consumption of each possible decision in the round ex-post. Meanwhile, in the
119"
LITERATURE REVIEW,0.26452905811623245,"latter, the agent can only observe the reward-consumption pair brought by the decision. Apparently,
120"
LITERATURE REVIEW,0.2665330661322645,"the bandit information feedback is harder to deal with since less information can be accessed. Our
121"
LITERATURE REVIEW,0.2685370741482966,"work further considers a partial feedback model, in which the agent observes the external factor when
122"
LITERATURE REVIEW,0.27054108216432865,"a non-null action is chosen. This model acts as an intermediate between full and partial information
123"
LITERATURE REVIEW,0.2725450901803607,"feedback models.
124"
LITERATURE REVIEW,0.2745490981963928,"Apart from the above work, two results [Han et al., 2022, Slivkins and Foster, 2022] concurrent with
125"
LITERATURE REVIEW,0.27655310621242485,"this work are not restricted to linear expectation functions. To deal with more general problems with
126"
LITERATURE REVIEW,0.2785571142284569,"bandit feedback, they plug model-reliable online regression methods [Foster et al., 2018, Foster and
127"
LITERATURE REVIEW,0.280561122244489,"Rakhlin, 2020] into the dual update framework. As a result, the regret of their algorithms is the sum
128"
LITERATURE REVIEW,0.28256513026052105,"of the regret on online regression and online convex optimization, respectively. Nevertheless, the
129"
LITERATURE REVIEW,0.2845691382765531,"online regression technique still limits the conditionally expected reward-consumption functions.
130"
LITERATURE REVIEW,0.2865731462925852,"Network revenue management and the re-solving heuristic.
Unlike the above approaches, our
131"
LITERATURE REVIEW,0.28857715430861725,"work adopts the re-solving method, also known as the “certainty equivalence” (CE) heuristic. Under
132"
LITERATURE REVIEW,0.2905811623246493,"this approach, the agent frequently solves the fluid optimization problem with the remaining resources
133"
LITERATURE REVIEW,0.2925851703406814,"to obtain a probability control in each round. This method comes from the literature on the network
134"
LITERATURE REVIEW,0.29458917835671344,"revenue management problem, which can be seen as a simplification of the CBwK problem without
135"
LITERATURE REVIEW,0.2965931863727455,"the existence of external factors, or that the external factor not getting involved in the resource
136"
LITERATURE REVIEW,0.2985971943887776,"consumption [Wu et al., 2015]. Some work in this setting also assumes known request distributions.
137"
LITERATURE REVIEW,0.30060120240480964,"This line of research originates from Jasin and Kumar [2012], and also includes Jasin [2015], Ferreira
138"
LITERATURE REVIEW,0.3026052104208417,"et al. [2018], Bumpensanti and Wang [2020], Li and Ye [2021], Chen et al. [2022], Besbes et al.
139"
LITERATURE REVIEW,0.3046092184368738,"[2022]. They show that the re-solving method can obtain constant regret under certain non-degeneracy
140"
LITERATURE REVIEW,0.3066132264529058,"assumptions and can generally obtain square root regret [Chen et al., 2022]. Recently, the re-solving
141"
LITERATURE REVIEW,0.30861723446893785,"method is also extended to the general dynamic resource-constrained reward collection (DRCRC)
142"
LITERATURE REVIEW,0.3106212424849699,"problem in Balseiro et al. [2021], which assumes the knowledge of request and external factor
143"
LITERATURE REVIEW,0.312625250501002,"distributions and achieves O(1) to O(log T) regret for different action space cardinalities.
144"
LITERATURE REVIEW,0.31462925851703405,"We should mention that the re-solving technique has also been adopted to the bandits with knapsacks
145"
LITERATURE REVIEW,0.3166332665330661,"(BwK) problem [Flajolet and Jaillet, 2015] to achieve an O(log T) regret. However, CBwK is a more
146"
LITERATURE REVIEW,0.3186372745490982,"challenging problem than BwK in the sense that the decision has to be made based on the received
147"
LITERATURE REVIEW,0.32064128256513025,"request. Thus, there is no optimal static action mode that is irrelevant to the round, which adds a layer
148"
LITERATURE REVIEW,0.3226452905811623,"of complexity to the re-solving method.
149"
PRELIMINARIES,0.3246492985971944,"2
Preliminaries
150"
PRELIMINARIES,0.32665330661322645,"We consider an agent interacting with the environment for T rounds. There are n kinds of resources,
151"
PRELIMINARIES,0.3286573146292585,"with an average amount of ρi for resource i in each round, resulting in a total of ρiT amount of
152"
PRELIMINARIES,0.3306613226452906,"resource i. We suppose that 0 < ρ = ρ1 = (ρi)i∈[n] ≤1 is independent of T, with a maximum
153"
PRELIMINARIES,0.33266533066132264,"entry of ρmax and a minimum entry of ρmin. At the beginning of each round t ≥1, the agent observes
154"
PRELIMINARIES,0.3346693386773547,"a request θt ∈Θ drawn i.i.d. from a distribution U, and should choose an action at from a set of
155"
PRELIMINARIES,0.3366733466933868,"actions A. Given the request θt and the action at, the agent receives a random reward rt ∈[0, 1] and
156"
PRELIMINARIES,0.33867735470941884,"consumption vector of resources ct ∈[0, 1]n, both of which are related to an external factor γt ∈Γ
157"
PRELIMINARIES,0.3406813627254509,"drawn i.i.d. from a distribution V. In other words, there is a reward function r : Θ × A × Γ →[0, 1]
158"
PRELIMINARIES,0.342685370741483,"and a consumption vector function c : Θ × A × Γ →[0, 1]n, such that rt = r(θt, at, γt) and
159"
PRELIMINARIES,0.34468937875751504,"ct = c(θt, at, γt). We suppose these two functions are pre-known to the agent. We further define
160"
PRELIMINARIES,0.3466933867735471,"R(θ, a) := Eγ[r(θ, a, γ)], and C(θ, a) := Eγ[c(θ, a, γ)].
161"
PRELIMINARIES,0.3486973947895792,"We impose minimum restrictions on the distributions U and V. Specifically, in the main body of this
162"
PRELIMINARIES,0.35070140280561124,"work, we suppose that both distributions are discrete without any further assumptions. In other words,
163"
PRELIMINARIES,0.3527054108216433,"Θ and Γ are finite. We denote the mass function of U and V by u(θ) and v(γ), respectively. We will
164"
PRELIMINARIES,0.35470941883767537,"extend to the situation that these two distributions can be continuous in Appendix A.
165"
PRELIMINARIES,0.35671342685370744,"The agent’s objective is to maximize her cumulative rewards over the period under initial resource
166"
PRELIMINARIES,0.3587174348697395,"constraints, which is a sequential decision-making problem. To ensure feasibility, we assume the
167"
PRELIMINARIES,0.36072144288577157,"existence of a null action (denoted by 0) in the action set A. Under the null action, the reward and the
168"
PRELIMINARIES,0.3627254509018036,"consumption of any resource are zero, regardless of the request and the external factor. In other words,
169"
PRELIMINARIES,0.36472945891783565,"we have r(θt, 0, γt) = 0 and c(θt, 0, γt) = 0 for any (θt, γt) ∈Θ × Γ. We use A+ := A \ {0} to
170"
PRELIMINARIES,0.3667334669338677,"denote the set of non-null actions, and let m := |A+| be its size.
171"
PRELIMINARIES,0.3687374749498998,"We consider the set of non-anticipating strategies Π. In particular, let Ht be the history the agent
172"
PRELIMINARIES,0.37074148296593185,"could access at the start of round t. Then, for any non-anticipating strategy π ∈Π, at should depend
173"
PRELIMINARIES,0.3727454909819639,"only on e
Ht := (θt, Ht), that is, at = aπ
t (θt, Ht). For abbreviation, we write aπ
t = aπ
t (θt, Ht) when
174"
PRELIMINARIES,0.374749498997996,"there is no confusion.
175"
PRELIMINARIES,0.37675350701402804,"Therefore, we can define the agent’s optimization problem as below:
176"
PRELIMINARIES,0.3787575150300601,"V ON := max
π∈Π Eθ∼UT ,γ∼VT "" T
X"
PRELIMINARIES,0.3807615230460922,"t=1
r(θt, aπ
t , γt) # , s.t. T
X"
PRELIMINARIES,0.38276553106212424,"t=1
c(θt, aπ
t , γt) ≤ρT,
∀θ ∈ΘT , γ ∈ΓT ."
PRELIMINARIES,0.3847695390781563,"Benchmark.
In practice, however, computing the expected reward of the optimal online strategy
177"
PRELIMINARIES,0.3867735470941884,"would require high-dimension (probably infinite) dynamic programming, which is intractable. Hence,
178"
PRELIMINARIES,0.38877755511022044,"we turn to consider the fluid benchmark to measure the performance of a strategy, which is defined as
179"
PRELIMINARIES,0.3907815631262525,Full information
PRELIMINARIES,0.3927855711422846,Agent’s observation
PRELIMINARIES,0.39478957915831664,"t = 1
t = 2
t = 3
t = 4
t = 5"
PRELIMINARIES,0.3967935871743487,"γ1
γ2
γ3
γ4
γ5"
PRELIMINARIES,0.39879759519038077,"a1 ̸= 0
a2 ̸= 0
a3 = 0
a4 = 0
a5 ̸= 0"
PRELIMINARIES,0.40080160320641284,Partial information
PRELIMINARIES,0.4028056112224449,Agent’s observation
PRELIMINARIES,0.40480961923847697,"t = 1
t = 2
t = 3
t = 4
t = 5"
PRELIMINARIES,0.40681362725450904,"γ1
γ2
γ5"
PRELIMINARIES,0.4088176352705411,"a1 ̸= 0
a2 ̸= 0
a3 = 0
a4 = 0
a5 ̸= 0"
PRELIMINARIES,0.41082164328657317,Figure 1: An illustration of the two information feedback models we consider in this work.
PRELIMINARIES,0.41282565130260523,"follows:
180"
PRELIMINARIES,0.4148296593186373,"V FL := T ·
max
ϕ:Θ×A+→R Eθ∼U "" X"
PRELIMINARIES,0.4168336673346693,"a∈A+
R(θ, a)ϕ(θ, a) # ,"
PRELIMINARIES,0.4188376753507014,"s.t.
Eθ∼U "" X"
PRELIMINARIES,0.42084168336673344,"a∈A+
C(θ, a)ϕ(θ, a) # ≤ρ, X"
PRELIMINARIES,0.4228456913827655,"a∈A+
ϕ(θ, a) ≤1,
∀θ ∈Θ,"
PRELIMINARIES,0.4248496993987976,"ϕ(θ, a) ≥0,
∀(θ, a) ∈Θ × A+."
PRELIMINARIES,0.42685370741482964,"For a better understanding, V FL reflects the maximum expected total rewards an agent can win
181"
PRELIMINARIES,0.4288577154308617,"when a static strategy is adopted and the resource constraints are only to be satisfied in expectation.
182"
PRELIMINARIES,0.4308617234468938,"Therefore, this optimization problem is a linear programming, in which the decision variable ϕ(θ, a)
183"
PRELIMINARIES,0.43286573146292584,"represents the probability that the agent chooses action a upon seeing request θ. It is a well-known
184"
PRELIMINARIES,0.4348697394789579,"result that V FL gives an upper bound on V ON.
185"
PRELIMINARIES,0.43687374749499,"Proposition 2.1 ([Balseiro et al., 2021]). V FL ≥V ON.
186"
PRELIMINARIES,0.43887775551102204,"Thus, we evaluate the performance of a non-anticipating strategy π by comparing its expected
187"
PRELIMINARIES,0.4408817635270541,"accumulated reward Rewπ with the fluid benchmark V FL. We call their difference the regret of π for
188"
PRELIMINARIES,0.44288577154308617,"convenience. In this context, we prove that an Ω(
√"
PRELIMINARIES,0.44488977955911824,"T) regret lower bound is inevitable as long as
189"
PRELIMINARIES,0.4468937875751503,"V FL is degenerate.
190"
PRELIMINARIES,0.44889779559118237,"Theorem 2.1 (worst-case location). When V FL has a unique and degenerate optimal solution,
191"
PRELIMINARIES,0.45090180360721444,"V FL −V ON = Ω(
√"
PRELIMINARIES,0.4529058116232465,"T).
192"
PRELIMINARIES,0.45490981963927857,"Despite the worst-case lower bound, we prove in this work that for any CBwK instance in which V FL
193"
PRELIMINARIES,0.45691382765531063,"has a unique non-degenerate optimal solution (Assumption 3.1), we can obtain an O(1) regret via the
194"
PRELIMINARIES,0.4589178356713427,"re-solving approach.
195"
PRELIMINARIES,0.46092184368737477,"Information feedback model.
In this work, we consider two types of information feedback models,
196"
PRELIMINARIES,0.46292585170340683,"with increasing levels of difficulty in obtaining a sample of the external factor γ.
197"
PRELIMINARIES,0.4649298597194389,"• [Full information feedback.] The agent is able to observe γt at the end of each round t.
198"
PRELIMINARIES,0.46693386773547096,"• [Partial information feedback.] The agent can observe γt at the end of round t only if at ̸= 0.
199"
PRELIMINARIES,0.46893787575150303,"The above two information feedback models are illustrated in Figure 1. In general, with full
200"
PRELIMINARIES,0.4709418837675351,"information feedback, the agent can observe an i.i.d. sample from V each round, which is the optimal
201"
PRELIMINARIES,0.4729458917835671,"scenario for learning the distribution. Nevertheless, such an assumption may be overly strong since
202"
PRELIMINARIES,0.4749498997995992,"the reward and consumption vector are irrelevant to the external factor when the agent chooses the
203"
PRELIMINARIES,0.47695390781563124,Algorithm 1: Re-Solving with Empirical Estimation.
PRELIMINARIES,0.4789579158316633,"Input: ρ, T.
Initialization: I1 ←∅, B1 ←ρT."
PRELIMINARIES,0.48096192384769537,1 for t ←1 to T do
PRELIMINARIES,0.48296593186372744,"2
Observe θt;"
PRELIMINARIES,0.4849699398797595,"/* Solve a linear programming with estimates.
*/"
PRELIMINARIES,0.48697394789579157,"3
ρt ←Bt/(T −t + 1);"
PRELIMINARIES,0.48897795591182364,"4
bϕ∗
t ←the solution to bJ(ρt, Ht);"
PRELIMINARIES,0.4909819639278557,"5
Choose at ∈A randomly such that for a ∈A+, Pr[at = a] = bϕ∗
t (θt, a), and
Pr[at = 0] = 1 −P
a∈A+ bϕ∗
t (θt, a);"
PRELIMINARIES,0.49298597194388777,"/* Observe the sample.
*/"
PRELIMINARIES,0.49498997995991983,"6
if (FULL-INFO) ∨(PARTIAL-INFO ∧at ̸= 0) then"
PRELIMINARIES,0.4969939879759519,"7
Observe γt;"
PRELIMINARIES,0.49899799599198397,"8
It+1 ←It ∪{t};"
END,0.501002004008016,"9
end"
ELSE,0.503006012024048,"10
else"
ELSE,0.5050100200400801,"11
It+1 ←It;"
END,0.5070140280561122,"12
end"
END,0.5090180360721442,"/* Update the remaining budget vector.
*/"
END,0.5110220440881763,"13
Bt+1 ←Bt −ct;"
IF BI,0.5130260521042084,"14
if Bi
t+1 < 1 for some i ∈[n] then"
IF BI,0.5150300601202404,"15
break;"
END,0.5170340681362725,"16
end"
END,0.5190380761523046,17 end
END,0.5210420841683366,"null action a = 0. Thereby, a more realistic information model is partial feedback, where the external
204"
END,0.5230460921843687,"factor is only accessible when a ̸= 0. This limitation also increases the difficulty of learning the
205"
END,0.5250501002004008,"distribution V since the agent observes fewer samples under this model than under full information
206"
END,0.5270541082164328,"feedback. It is important to note that the partial information model represents a transition from full to
207"
END,0.5290581162324649,"bandit information feedback, under which only the reward and consumption vector are accessible in
208"
END,0.531062124248497,"each round, rather than the external factor.
209"
THE RE-SOLVING HEURISTIC,0.533066132264529,"3
The Re-Solving Heuristic
210"
THE RE-SOLVING HEURISTIC,0.5350701402805611,"In this work, we introduce the re-solving heuristic to the CBwK problem. The resulting algorithm is
211"
THE RE-SOLVING HEURISTIC,0.5370741482965932,"presented in Algorithm 1.
212"
THE RE-SOLVING HEURISTIC,0.5390781563126252,"To briefly describe the algorithm, we start by defining an optimization problem that captures the
213"
THE RE-SOLVING HEURISTIC,0.5410821643286573,"optimal fluid control for each round, assuming complete knowledge of U and V. For any κ ∈[0, 1]n,
214"
THE RE-SOLVING HEURISTIC,0.5430861723446894,"we define J(κ) be the following optimization problem:
215"
THE RE-SOLVING HEURISTIC,0.5450901803607214,"J(κ) :=
max
ϕ:Θ×A+→R Eθ∼U "" X"
THE RE-SOLVING HEURISTIC,0.5470941883767535,"a∈A+
R(θ, a)ϕ(θ, a) # ,"
THE RE-SOLVING HEURISTIC,0.5490981963927856,"s.t.
Eθ∼U "" X"
THE RE-SOLVING HEURISTIC,0.5511022044088176,"a∈A+
C(θ, a)ϕ(θ, a) # ≤κ, X"
THE RE-SOLVING HEURISTIC,0.5531062124248497,"a∈A+
ϕ(θ, a) ≤1,
∀θ ∈Θ,"
THE RE-SOLVING HEURISTIC,0.5551102204408818,"ϕ(θ, a) ≥0,
∀(θ, a) ∈Θ × A+."
THE RE-SOLVING HEURISTIC,0.5571142284569138,"Evidently, we have V FL = T · J(ρ) = T · J(ρ1) by definition. Intuitively, in each round t, the best
216"
THE RE-SOLVING HEURISTIC,0.5591182364729459,"fluid choice of the agent is given by the optimal solution ϕ∗
t of LP J(ρt), where ρt is the average
217"
THE RE-SOLVING HEURISTIC,0.561122244488978,"budget of the remaining rounds, including round t. Nevertheless, since the agent lacks full knowledge
218"
THE RE-SOLVING HEURISTIC,0.56312625250501,"of the exact distributions U and V, she can only solve an estimated programming bJ(ρt, Ht) as
219"
THE RE-SOLVING HEURISTIC,0.5651302605210421,"outlined in Algorithm 1, with the following realization:
220"
THE RE-SOLVING HEURISTIC,0.5671342685370742,"bJ(ρt, Ht) :=
max
ϕ:Θ×A+→R Eθ∼b
Ut "" X"
THE RE-SOLVING HEURISTIC,0.5691382765531062,"a∈A+
Eγ∼bVt [r(θ, a, γ)] ϕ(θ, a) # ,"
THE RE-SOLVING HEURISTIC,0.5711422845691383,"s.t.
Eθ∼b
Ut "" X"
THE RE-SOLVING HEURISTIC,0.5731462925851704,"a∈A+
Eγ∼bVt [c(θ, a, γ)] ϕ(θ, a) # ≤ρt, X"
THE RE-SOLVING HEURISTIC,0.5751503006012024,"a∈A+
ϕ(θ, a) ≤1,
∀θ ∈Θ,"
THE RE-SOLVING HEURISTIC,0.5771543086172345,"ϕ(θ, a) ≥0,
∀(θ, a) ∈Θ × A+."
THE RE-SOLVING HEURISTIC,0.5791583166332666,"Here, bUt and bVt represent the empirical distribution of θ and γ, respectively, according to the sample
221"
THE RE-SOLVING HEURISTIC,0.5811623246492986,"history given by Ht. Specifically, the mass functions of these two estimated distributions are standard
222"
THE RE-SOLVING HEURISTIC,0.5831663326653307,"as follows:
223"
THE RE-SOLVING HEURISTIC,0.5851703406813628,but(θ) := #[θ appears in previous t −1 rounds]
THE RE-SOLVING HEURISTIC,0.5871743486973948,"t −1
;"
THE RE-SOLVING HEURISTIC,0.5891783567134269,bvt(γ) := #[γ appears in It]
THE RE-SOLVING HEURISTIC,0.591182364729459,"|It|
."
THE RE-SOLVING HEURISTIC,0.593186372745491,"It is worth noting that the estimated distribution of θ, bUt, is always based t −1 samples since the
224"
THE RE-SOLVING HEURISTIC,0.5951903807615231,"agent received an independent sample from U at the beginning of each round. On the other hand,
225"
THE RE-SOLVING HEURISTIC,0.5971943887775552,"the empirical distribution of the external factor γ, bVt, is estimated from |It| independent samples.
226"
THE RE-SOLVING HEURISTIC,0.5991983967935872,"With full information feedback, |It| = t −1; whereas with partial information feedback, |It| ≤t −1
227"
THE RE-SOLVING HEURISTIC,0.6012024048096193,"equals the number of times the agent chooses an action a ̸= 0 before round t. For brevity, for the
228"
THE RE-SOLVING HEURISTIC,0.6032064128256514,"estimated programming, we write b
Ct(θ, a) := Eγ∼bVt[c(θ, a, γ)] and bRt(θ, a) := Eγ∼bVt[r(θ, a, γ)].
229"
THE RE-SOLVING HEURISTIC,0.6052104208416834,"As per Algorithm 1, the agent’s decision mode in round t is given by the optimal solution bϕ∗
t
230"
THE RE-SOLVING HEURISTIC,0.6072144288577155,"of programming bJ(ρt, Ht). The algorithm stops when the resources are near depletion, that is,
231"
THE RE-SOLVING HEURISTIC,0.6092184368737475,"Bi ≤1 for some resource i ∈[n], and we use T0 to denote the stopping time of Algorithm 1, i.e.,
232"
THE RE-SOLVING HEURISTIC,0.6112224448897795,"T0 := min{T, min{t : ∃i ∈[n], Bi
t+1 < 1}}.
233"
THE RE-SOLVING HEURISTIC,0.6132264529058116,"For an analysis beyond the worst-case scenario, a crucial assumption we will make is that the fluid
234"
THE RE-SOLVING HEURISTIC,0.6152304609218436,"problem possesses good regularity properties, i.e., it is an LP with a unique and non-degenerate
235"
THE RE-SOLVING HEURISTIC,0.6172344689378757,"solution.
236"
THE RE-SOLVING HEURISTIC,0.6192384769539078,"Assumption 3.1. The optimal solution to J(ρ1) is unique and non-degenerate.
237"
THE RE-SOLVING HEURISTIC,0.6212424849699398,"The regularity assumption made Assumption 3.1 is commonplace in the linear programming literature
238"
THE RE-SOLVING HEURISTIC,0.6232464929859719,"[Chen et al., 2022, Li and Ye, 2021]. Further, any LP can easily avoid non-uniqueness or degeneracy
239"
THE RE-SOLVING HEURISTIC,0.625250501002004,"through a slight perturbation [Megiddo and Chandrasekaran, 1989].
240"
THE RE-SOLVING HEURISTIC,0.627254509018036,"With the assumption, below we present the main result of this work, which is proved in Appendix C.1.
241"
THE RE-SOLVING HEURISTIC,0.6292585170340681,"Theorem 3.1. Under Assumption 3.1, with full information feedback, the expected accumulated
242"
THE RE-SOLVING HEURISTIC,0.6312625250501002,"reward Rew brought by Algorithm 1 when T →∞satisfies:
243"
THE RE-SOLVING HEURISTIC,0.6332665330661322,"V FL −Rew = O(1),
T →∞,"
THE RE-SOLVING HEURISTIC,0.6352705410821643,"which is independent of T.
244"
THE RE-SOLVING HEURISTIC,0.6372745490981964,"One of the key implications of Theorem 3.1 is that the re-solving heuristic’s regret is independent of
245"
THE RE-SOLVING HEURISTIC,0.6392785571142284,"the number of rounds beyond the worst-case with full information. This result represents a significant
246"
THE RE-SOLVING HEURISTIC,0.6412825651302605,"improvement over previous state-of-the-art results under mild assumptions, surpassing the solutions
247"
THE RE-SOLVING HEURISTIC,0.6432865731462926,"proposed by Slivkins and Foster [2022], Han et al. [2022]. In particular, their solutions come from
248"
THE RE-SOLVING HEURISTIC,0.6452905811623246,"the bandits with knapsacks (BwK) literature and rely on dual update and upper confidence bound
249"
THE RE-SOLVING HEURISTIC,0.6472945891783567,"(UCB) heuristics, which only provide a worst-case regret of O(√T log T) even with full information
250"
THE RE-SOLVING HEURISTIC,0.6492985971943888,"feedback. Furthermore, the reduction proposed by Sankararaman and Slivkins [2021] can only grant
251"
THE RE-SOLVING HEURISTIC,0.6513026052104208,"an O(log T) regret for linear CBwK problems, and relies on the strong assumption that there is a
252"
THE RE-SOLVING HEURISTIC,0.6533066132264529,Θ(log T)
THE RE-SOLVING HEURISTIC,0.655310621242485,"O(T)
Overall O(1) frequency"
THE RE-SOLVING HEURISTIC,0.657314629258517,Uniform O(1/ log T) frequency
THE RE-SOLVING HEURISTIC,0.6593186372745491,Figure 2: An illustration of Lemma 4.1.
THE RE-SOLVING HEURISTIC,0.6613226452905812,"universal best action and only n = 2 resources. In contrast, our assumption is more common and less
253"
THE RE-SOLVING HEURISTIC,0.6633266533066132,"restrictive.
254"
THE RE-SOLVING HEURISTIC,0.6653306613226453,"Additionally, as pointed out in Theorem 2.1, an Ω(
√"
THE RE-SOLVING HEURISTIC,0.6673346693386774,"T) lower bound is established when the primal
255"
THE RE-SOLVING HEURISTIC,0.6693386773547094,"LP J(ρ1) has a unique and degenerate optimal solution, while Theorem 3.1 provides an O(1) upper
256"
THE RE-SOLVING HEURISTIC,0.6713426853707415,"bound on the optimal regret of CBwK with full information under the uniqueness and non-degeneracy
257"
THE RE-SOLVING HEURISTIC,0.6733466933867736,"condition. It is interesting to consider the regret bound in the remaining cases when J(ρ1) has
258"
THE RE-SOLVING HEURISTIC,0.6753507014028056,"multiple optimal solutions.
259"
THE RE-SOLVING HEURISTIC,0.6773547094188377,"It is worth noting the relationship between our theoretical regret and the number of resources n and
260"
THE RE-SOLVING HEURISTIC,0.6793587174348698,"number of actions m. Generally, our analysis shows that the regret scales with (at most) the square
261"
THE RE-SOLVING HEURISTIC,0.6813627254509018,"of n. Further, a surprising result is that the regret is not explicitly related to m. This is superior
262"
THE RE-SOLVING HEURISTIC,0.6833667334669339,"to existing results, which report an eO(√m) reliance [Slivkins and Foster, 2022, Han et al., 2022,
263"
THE RE-SOLVING HEURISTIC,0.685370741482966,"Badanidiyuru et al., 2014, Agrawal et al., 2016]. As an intuitive reason, the number of actions does
264"
THE RE-SOLVING HEURISTIC,0.687374749498998,"not explicitly appear in the re-solving algorithm but only contributes to the dimension of the linear
265"
THE RE-SOLVING HEURISTIC,0.6893787575150301,"programming. However, this is not the case for other existing algorithms, which explicitly incorporate
266"
THE RE-SOLVING HEURISTIC,0.6913827655310621,"the number of actions m into their algorithms, resulting in a correlation between the regret and m.
267"
PARTIAL INFORMATION FEEDBACK,0.6933867735470942,"4
Partial Information Feedback
268"
PARTIAL INFORMATION FEEDBACK,0.6953907815631263,"We now shift to consider the re-solving method’s performance with partial information feedback,
269"
PARTIAL INFORMATION FEEDBACK,0.6973947895791583,"under which the agent only sees the external factor γt when her choice is non-null in round t, i.e.,
270"
PARTIAL INFORMATION FEEDBACK,0.6993987975951904,"at ̸= 0. Apparently, with less information, the learning speed of the distribution V decreases,
271"
PARTIAL INFORMATION FEEDBACK,0.7014028056112225,"hindering the re-solving procedure’s quick convergence to an optimal solution. Nevertheless, we
272"
PARTIAL INFORMATION FEEDBACK,0.7034068136272545,"demonstrate that the performance of the re-solving method only faces an O(log T) multiplicative
273"
PARTIAL INFORMATION FEEDBACK,0.7054108216432866,"degradation under partial information feedback. Our primary theorem in this section is as follows:
274"
PARTIAL INFORMATION FEEDBACK,0.7074148296593187,"Theorem 4.1. Under Assumption 3.1, with partial information feedback, the expected accumulated
275"
PARTIAL INFORMATION FEEDBACK,0.7094188376753507,"reward Rew brought by Algorithm 1 satisfies:
276"
PARTIAL INFORMATION FEEDBACK,0.7114228456913828,"V FL −Rew = O(log T),
T →∞."
PARTIAL INFORMATION FEEDBACK,0.7134268537074149,"Before we come to the technical parts, we first place Theorem 4.1 within the literature. As previously
277"
PARTIAL INFORMATION FEEDBACK,0.7154308617234469,"mentioned, Ω(
√"
PARTIAL INFORMATION FEEDBACK,0.717434869739479,"T) is a worst-case lower bound on the regret even with full information feedback,
278"
PARTIAL INFORMATION FEEDBACK,0.7194388777555111,"and thus also serves as a lower bound with partial information feedback. However, Theorem 4.1 steps
279"
PARTIAL INFORMATION FEEDBACK,0.7214428857715431,"beyond the worst-case by providing an O(log T) upper bound for most regular problem instances.
280"
PARTIAL INFORMATION FEEDBACK,0.7234468937875751,"This result outperforms the universal O(√T log T) regret by Slivkins and Foster [2022], Han et al.
281"
PARTIAL INFORMATION FEEDBACK,0.7254509018036072,"[2022]. Although the result is asymptotically equivalent to that of Sankararaman and Slivkins [2021],
282"
PARTIAL INFORMATION FEEDBACK,0.7274549098196392,"it imposes fewer restrictions on the problem structure, as previously discussed. Moreover, the
283"
PARTIAL INFORMATION FEEDBACK,0.7294589178356713,"regret result’s dependence on the number of resources n and number of actions m is inherited from
284"
PARTIAL INFORMATION FEEDBACK,0.7314629258517034,"Theorem 3.1.
285"
PARTIAL INFORMATION FEEDBACK,0.7334669338677354,"We now provide an intuitive understanding of the proof of Theorem 4.1. The crux lies in analyzing
286"
PARTIAL INFORMATION FEEDBACK,0.7354709418837675,"the frequency that Algorithm 1 can access an independent sample of the external factor. To this end,
287"
PARTIAL INFORMATION FEEDBACK,0.7374749498997996,"we use Yt = |It| ≤t −1 to denote the times of choosing action a ̸= 0 before time t, or equivalently,
288"
PARTIAL INFORMATION FEEDBACK,0.7394789579158316,"the number of i.i.d. samples from V observed by the agent before time t, under partial information
289"
PARTIAL INFORMATION FEEDBACK,0.7414829659318637,"feedback. We have the following important lemma that presents a lower bound on Yt.
290"
PARTIAL INFORMATION FEEDBACK,0.7434869739478958,"Lemma 4.1. There is a constant 0 < Cb < 1/2, such that with probability 1−O(1/T), the following
291"
PARTIAL INFORMATION FEEDBACK,0.7454909819639278,"hold for Algorithm 1:
292"
PARTIAL INFORMATION FEEDBACK,0.7474949899799599,"1. For any Θ(log T) ≤t ≤Cb · T, Yt ≥Cf · (t −1)/ log T for some constant Cf;
293"
PARTIAL INFORMATION FEEDBACK,0.749498997995992,"2. For any t > Cb · T, Yt ≥Cr · T for some constant Cr.
294"
PARTIAL INFORMATION FEEDBACK,0.751503006012024,"The proof of Lemma 4.1 is deferred to Appendix D.2, and an illustration is displayed in Figure 2. In
295"
PARTIAL INFORMATION FEEDBACK,0.7535070140280561,"simple terms, during the first Θ(log T) rounds (the shaded segment), the re-solving method cannot
296"
PARTIAL INFORMATION FEEDBACK,0.7555110220440882,"guarantee the accessing frequency since the learning of the request distribution U has not converged
297"
PARTIAL INFORMATION FEEDBACK,0.7575150300601202,"sufficiently. However, after Θ(log T) rounds, Algorithm 1 ensures a constant probability of obtaining
298"
PARTIAL INFORMATION FEEDBACK,0.7595190380761523,"a new example in each round, provided that the remaining resources are sufficient. As a consequence,
299"
PARTIAL INFORMATION FEEDBACK,0.7615230460921844,"before O(T) rounds, we can guarantee an O(1/ log T) accessing frequency at any time step and an
300"
PARTIAL INFORMATION FEEDBACK,0.7635270541082164,"overall O(1) frequency with high probability, by a concentration inequality. The remaining proof of
301"
PARTIAL INFORMATION FEEDBACK,0.7655310621242485,"Theorem 4.1 is provided in Appendix D.1.
302"
PARTIAL INFORMATION FEEDBACK,0.7675350701402806,"5
Relaxing the Regularity Assumption – A Worst-Case Guarantee
303"
PARTIAL INFORMATION FEEDBACK,0.7695390781563126,"In Sections 3 and 4, we have proved that can achieve an eO(1) regret for CBwK problems under full or
304"
PARTIAL INFORMATION FEEDBACK,0.7715430861723447,"partial information feedbacks, assuming certain regular conditions (Assumption 3.1). Put differently,
305"
PARTIAL INFORMATION FEEDBACK,0.7735470941883767,"the re-solving heuristic nicely deals with regular scenarios. In this section, we complement this by
306"
PARTIAL INFORMATION FEEDBACK,0.7755511022044088,"showing that this method can also attain nearly optimal regret in the worst cases. Furthermore, we
307"
PARTIAL INFORMATION FEEDBACK,0.7775551102204409,"extend our analysis to cases where the context and external factor distributions can be continuous in
308"
PARTIAL INFORMATION FEEDBACK,0.779559118236473,"Appendix A.
309"
PARTIAL INFORMATION FEEDBACK,0.781563126252505,"Our main results are given below, and their proofs are provided in Appendices E.1 and E.2, respec-
310"
PARTIAL INFORMATION FEEDBACK,0.7835671342685371,"tively.
311"
PARTIAL INFORMATION FEEDBACK,0.7855711422845691,"Theorem 5.1. With full information feedback, the expected accumulated reward Rew brought by
312"
PARTIAL INFORMATION FEEDBACK,0.7875751503006012,"Algorithm 1 satisfies:
313"
PARTIAL INFORMATION FEEDBACK,0.7895791583166333,"V FL −Rew = O(
p"
PARTIAL INFORMATION FEEDBACK,0.7915831663326653,"T log T),
T →∞."
PARTIAL INFORMATION FEEDBACK,0.7935871743486974,"Theorem 5.2. With partial information feedback, the expected accumulated reward Rew brought by
314"
PARTIAL INFORMATION FEEDBACK,0.7955911823647295,"Algorithm 1 satisfies:
315"
PARTIAL INFORMATION FEEDBACK,0.7975951903807615,"V FL −Rew = O(
√"
PARTIAL INFORMATION FEEDBACK,0.7995991983967936,"T log T),
T →∞."
PARTIAL INFORMATION FEEDBACK,0.8016032064128257,"As given by Theorem 2.1, the worst-case regret of any online CBwK algorithm is Ω(
√"
PARTIAL INFORMATION FEEDBACK,0.8036072144288577,"T), while
316"
PARTIAL INFORMATION FEEDBACK,0.8056112224448898,"Theorems 5.1 and 5.2 indicate that the re-solving heuristic reaches near-optimality in such cases.
317"
PARTIAL INFORMATION FEEDBACK,0.8076152304609219,"Further, state-of-the-art algorithms [Han et al., 2022, Slivkins and Foster, 2022]) can at most obtain
318"
PARTIAL INFORMATION FEEDBACK,0.8096192384769539,"an eO(
√"
PARTIAL INFORMATION FEEDBACK,0.811623246492986,"T) regret with full/partial information feedback. Our algorithm also achieves this regret
319"
PARTIAL INFORMATION FEEDBACK,0.8136272545090181,"bound in worst cases.
320"
CONCLUDING REMARKS,0.8156312625250501,"6
Concluding Remarks
321"
CONCLUDING REMARKS,0.8176352705410822,"This work establishes the effectiveness of the re-solving heuristic in the contextual bandits with
322"
CONCLUDING REMARKS,0.8196392785571143,"knapsacks problem. We first prove that any online algorithm incurs a regret of Ω(
√"
CONCLUDING REMARKS,0.8216432865731463,"T) when the
323"
CONCLUDING REMARKS,0.8236472945891784,"fluid LP has a unique and degenerate optimal solution. Building on this, we demonstrate that the
324"
CONCLUDING REMARKS,0.8256513026052105,"re-solving method reaches an O(1) regret with full information and an O(log T) regret with partial
325"
CONCLUDING REMARKS,0.8276553106212425,"information when the fluid LP has a unique and non-degenerate optimal solution. Considering the
326"
CONCLUDING REMARKS,0.8296593186372746,"sufficient condition for the Ω(
√"
CONCLUDING REMARKS,0.8316633266533067,"T) lower bound, our non-degeneracy assumption is mild, especially
327"
CONCLUDING REMARKS,0.8336673346693386,"when combined with the two-resource and best-arm-optimality condition required in Sankararaman
328"
CONCLUDING REMARKS,0.8356713426853707,"and Slivkins [2021].
329"
CONCLUDING REMARKS,0.8376753507014028,"Further, we show that even in the worst-case, the re-solving method achieves an O(√T log T) regret
330"
CONCLUDING REMARKS,0.8396793587174348,"with full information feedback and an O(
√"
CONCLUDING REMARKS,0.8416833667334669,"T log T) regret with partial information feedback. These
331"
CONCLUDING REMARKS,0.843687374749499,"results are comparable to start-of-the-art results [Slivkins and Foster, 2022, Han et al., 2022]. We
332"
CONCLUDING REMARKS,0.845691382765531,"also extend our analysis to the continuous randomness case for completeness.
333"
REFERENCES,0.8476953907815631,"References
334"
REFERENCES,0.8496993987975952,"Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic
335"
REFERENCES,0.8517034068136272,"bandits. Advances in Neural Information Processing Systems, 24, 2011.
336"
REFERENCES,0.8537074148296593,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
337"
REFERENCES,0.8557114228456913,"the monster: A fast and simple algorithm for contextual bandits. In International Conference on
338"
REFERENCES,0.8577154308617234,"Machine Learning, pages 1638–1646. PMLR, 2014.
339"
REFERENCES,0.8597194388777555,"Shipra Agrawal and Nikhil Devanur. Linear contextual bandits with knapsacks. Advances in Neural
340"
REFERENCES,0.8617234468937875,"Information Processing Systems, 29, 2016.
341"
REFERENCES,0.8637274549098196,"Shipra Agrawal, Nikhil R Devanur, and Lihong Li. An efficient algorithm for contextual bandits
342"
REFERENCES,0.8657314629258517,"with knapsacks, and an extension to concave objectives. In Conference on Learning Theory, pages
343"
REFERENCES,0.8677354709418837,"4–18. PMLR, 2016.
344"
REFERENCES,0.8697394789579158,"Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
345"
REFERENCES,0.8717434869739479,"Learning Research, 3(Nov):397–422, 2002.
346"
REFERENCES,0.87374749498998,"Ashwinkumar Badanidiyuru, John Langford, and Aleksandrs Slivkins. Resourceful contextual bandits.
347"
REFERENCES,0.875751503006012,"In Conference on Learning Theory, pages 1109–1134. PMLR, 2014.
348"
REFERENCES,0.8777555110220441,"Santiago Balseiro, Omar Besbes, and Dana Pizarro. Survey of dynamic resource constrained reward
349"
REFERENCES,0.8797595190380761,"collection problems: Unified model and analysis. Available at SSRN 3963265, 2021.
350"
REFERENCES,0.8817635270541082,"Santiago R Balseiro and Yonatan Gur. Learning in repeated auctions with budgets: Regret minimiza-
351"
REFERENCES,0.8837675350701403,"tion and equilibrium. Management Science, 65(9):3952–3968, 2019.
352"
REFERENCES,0.8857715430861723,"Omar Besbes, Yash Kanoria, and Akshit Kumar. The multisecretary problem with many types. arXiv
353"
REFERENCES,0.8877755511022044,"preprint arXiv:2205.09078, 2022.
354"
REFERENCES,0.8897795591182365,"Pornpawee Bumpensanti and He Wang. A re-solving heuristic with uniformly bounded loss for
355"
REFERENCES,0.8917835671342685,"network revenue management. Management Science, 66(7):2993–3009, 2020.
356"
REFERENCES,0.8937875751503006,"Guanting Chen, Xiaocheng Li, and Yinyu Ye. An improved analysis of lp-based control for revenue
357"
REFERENCES,0.8957915831663327,"management. Operations Research, 2022.
358"
REFERENCES,0.8977955911823647,"Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and
359"
REFERENCES,0.8997995991983968,"Tong Zhang. Efficient optimal learning for contextual bandits. In Proceedings of the Twenty-Seventh
360"
REFERENCES,0.9018036072144289,"Conference on Uncertainty in Artificial Intelligence, pages 169–178, 2011.
361"
REFERENCES,0.9038076152304609,"Adam N Elmachtoub and Paul Grigas. Smart “predict, then optimize”. Management Science, 68(1):
362"
REFERENCES,0.905811623246493,"9–26, 2022.
363"
REFERENCES,0.9078156312625251,"Kris Johnson Ferreira, David Simchi-Levi, and He Wang. Online network revenue management using
364"
REFERENCES,0.9098196392785571,"thompson sampling. Operations research, 66(6):1586–1602, 2018.
365"
REFERENCES,0.9118236472945892,"Arthur Flajolet and Patrick Jaillet. Logarithmic regret bounds for bandits with knapsacks. arXiv
366"
REFERENCES,0.9138276553106213,"preprint arXiv:1510.01800, 2015.
367"
REFERENCES,0.9158316633266533,"Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with
368"
REFERENCES,0.9178356713426854,"regression oracles. In International Conference on Machine Learning, pages 3199–3210. PMLR,
369"
REFERENCES,0.9198396793587175,"2020.
370"
REFERENCES,0.9218436873747495,"Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical
371"
REFERENCES,0.9238476953907816,"contextual bandits with regression oracles. In International Conference on Machine Learning,
372"
REFERENCES,0.9258517034068137,"pages 1539–1548. PMLR, 2018.
373"
REFERENCES,0.9278557114228457,"Yuxuan Han, Jialin Zeng, Yang Wang, Yang Xiang, and Jiheng Zhang. Optimal contextual bandits
374"
REFERENCES,0.9298597194388778,"with knapsacks under realizibility via regression oracles. arXiv preprint arXiv:2210.11834, 2022.
375"
REFERENCES,0.9318637274549099,"Stefanus Jasin. Performance of an lp-based control for revenue management with unknown demand
376"
REFERENCES,0.9338677354709419,"parameters. Operations Research, 63(4):909–915, 2015.
377"
REFERENCES,0.935871743486974,"Stefanus Jasin and Sunil Kumar. A re-solving heuristic with bounded revenue loss for network
378"
REFERENCES,0.9378757515030061,"revenue management with customer choice. Mathematics of Operations Research, 37(2):313–345,
379"
REFERENCES,0.9398797595190381,"2012.
380"
REFERENCES,0.9418837675350702,"Xiaocheng Li and Yinyu Ye. Online linear programming: Dual convergence, new algorithms, and
381"
REFERENCES,0.9438877755511023,"regret bounds. Operations Research, 2021.
382"
REFERENCES,0.9458917835671342,"Heyuan Liu and Paul Grigas. Online contextual decision-making with a smart predict-then-optimize
383"
REFERENCES,0.9478957915831663,"method. arXiv preprint arXiv:2206.07316, 2022.
384"
REFERENCES,0.9498997995991983,"Olvi L Mangasarian and T-H Shiau. Lipschitz continuity of solutions of linear inequalities, programs
385"
REFERENCES,0.9519038076152304,"and complementarity problems. SIAM Journal on Control and Optimization, 25(3):583–595, 1987.
386"
REFERENCES,0.9539078156312625,"Nimrod Megiddo and Ramaswamy Chandrasekaran. On the ε-perturbation method for avoiding
387"
REFERENCES,0.9559118236472945,"degeneracy. Operations Research Letters, 8(6):305–308, 1989.
388"
REFERENCES,0.9579158316633266,"Karthik Abinav Sankararaman and Aleksandrs Slivkins. Bandits with knapsacks beyond the worst
389"
REFERENCES,0.9599198396793587,"case. Advances in Neural Information Processing Systems, 34:23191–23204, 2021.
390"
REFERENCES,0.9619238476953907,"Gerard Sierksma. Linear and integer programming: theory and practice. CRC Press, 2001.
391"
REFERENCES,0.9639278557114228,"Vidyashankar Sivakumar, Steven Wu, and Arindam Banerjee. Structured linear contextual bandits: A
392"
REFERENCES,0.9659318637274549,"sharp and geometric smoothed analysis. In International Conference on Machine Learning, pages
393"
REFERENCES,0.9679358717434869,"9026–9035. PMLR, 2020.
394"
REFERENCES,0.969939879759519,"Vidyashankar Sivakumar, Shiliang Zuo, and Arindam Banerjee. Smoothed adversarial linear con-
395"
REFERENCES,0.9719438877755511,"textual bandits with knapsacks. In International Conference on Machine Learning, pages 20253–
396"
REFERENCES,0.9739478957915831,"20277. PMLR, 2022.
397"
REFERENCES,0.9759519038076152,"Aleksandrs Slivkins and Dylan Foster. Efficient contextual bandits with knapsacks via regression.
398"
REFERENCES,0.9779559118236473,"arXiv preprint arXiv:2211.07484, 2022.
399"
REFERENCES,0.9799599198396793,"Alberto Vera and Siddhartha Banerjee. The bayesian prophet: A low-regret framework for online
400"
REFERENCES,0.9819639278557114,"decision making. Management Science, 67(3):1368–1391, 2021.
401"
REFERENCES,0.9839679358717435,"Larry Wasserman. Density estimation – lecture note for 36-708 statistical methods for machine
402"
REFERENCES,0.9859719438877755,"learning, Carnegie Mellon University, 2019.
403"
REFERENCES,0.9879759519038076,"Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger.
404"
REFERENCES,0.9899799599198397,"Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep,
405"
REFERENCES,0.9919839679358717,"2003.
406"
REFERENCES,0.9939879759519038,"Huasen Wu, Rayadurgam Srikant, Xin Liu, and Chong Jiang. Algorithms with logarithmic or
407"
REFERENCES,0.9959919839679359,"sublinear regret for constrained contextual bandits. Advances in Neural Information Processing
408"
REFERENCES,0.9979959919839679,"Systems, 28, 2015.
409"
