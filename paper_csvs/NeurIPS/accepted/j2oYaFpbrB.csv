Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0015408320493066256,"In this work, we investigate Active Vision Reinforcement Learning (ActiveVision-
RL), where an embodied agent simultaneously learns action policy for the task
while also controlling its visual observations in partially observable environments.
We denote the former as motor policy and the latter as sensory policy. For example,
humans solve real world tasks by hand manipulation (motor policy) together
with eye movements (sensory policy). ActiveVision-RL poses challenges on
coordinating two policies given their mutual influence. We propose SUGARL,
Sensorimotor Understanding Guided Active Reinforcement Learning, a framework
that models motor and sensory policies separately, but jointly learns them using with
an intrinsic sensorimotor reward. This learnable reward is assigned by sensorimotor
reward module, incentivizes the sensory policy to select observations that are
optimal to infer its own motor action, inspired by the sensorimotor stage of humans.
Through a series of experiments, we show the effectiveness of our method across
a range of observability conditions and its adaptability to existed RL algorithms.
The sensory policies learned through our method are observed to exhibit effective
active vision strategies."
INTRODUCTION,0.0030816640986132513,"1
Introduction"
INTRODUCTION,0.004622496147919877,"Although Reinforcement Learning (RL) has demonstrated success across challenging tasks and games
in both simulated and real environments [2, 9, 11, 89, 97], the observation spaces for visual RL tasks
are typically predefined to offer the most advantageous views based on prior knowledge and can not
be actively adjusted by the agent itself. For instance, table-top robot manipulators often utilize a
fixed overhead camera view [51]. While such fixed viewpoints can potentially stabilize the training
of an image feature encoder [14], this form of perception is different from humans who actively
adjust their perception system to finish the task, e.g. eye movements [24]. The absence of active
visual perception poses challenges on learning in highly dynamic environments [59, 61], open-world
tasks [30] and partially observable environments with occlusions, limited field-of-views, or multiple
view angles [40, 83]."
INTRODUCTION,0.0061633281972265025,"We study Active Reinforcement Learning (Active-RL) [102], the RL process that allows the embodied
agent to actively acquire new perceptual information in contrast to the standard RL, where the new
information could be reward signals [3, 23, 27, 50, 56, 62], visual observations [32, 33], and other
forms. Specifically, we are interested in visual Active-RL tasks, i.e. ActiveVision-RL, that an agent
controls its own views of visual observation, in an environment with limited visual observability [33].
Therefore, the goal of ActiveVision-RL is to learn two policies that still maximize the task return: the
motor policy to finish the task and the sensory policy to control the observation."
INTRODUCTION,0.007704160246533128,"Our project page, code, and library are available at this link"
INTRODUCTION,0.009244992295839754,"Standard RL
ActiveVision-RL
ActiveVision-RL Example on Atari-Boxing"
INTRODUCTION,0.01078582434514638,"Sensory Action
Motor Action"
INTRODUCTION,0.012326656394453005,"no-op
hit"
INTRODUCTION,0.01386748844375963,non-observable
INTRODUCTION,0.015408320493066256,observable
INTRODUCTION,0.01694915254237288,"Figure 1: ActiveVision-RL with limited visual observability in comparison with standard RL, with
exemplary process in Atari game Boxing. Red arrows stand for additional relationships considered
in ActiveVision-RL. In the example on the right, the highlighted regions are the actual observations
visible to the agent at each step. The rest of the pixels are not visible to the agent."
INTRODUCTION,0.01848998459167951,"ActiveVision-RL tasks present a considerable challenge due to the coordination between motor
and sensory policies, given their mutual influence [102]. The motor policy requires clear visual
observation for decision-making, while the sensory policy should adapt accordingly to the motor
action. Depending on the sensory policy, transitioning to a new view could either aid or hinder the
motor policy learning [14, 40, 102]. One notable impediment is the perceptual aliasing mentioned
by Whitehead and Ballard [102]. An optimal strategy for sensory policy should be incorporating
crucial visual information while eliminating any distractions. In the real world, humans disentangle
their sensory actions, such as eye movements, from their motor actions, such as manipulation, and
subsequently learn to coordinate them [59, 61]. Despite being modeled separately, these two action
policies and the coordination can be learned jointly through the interaction during sensorimotor
stage [25, 71, 72, 103]."
INTRODUCTION,0.020030816640986132,"Taking inspiration from human capabilities, we propose SUGARL: Sensorimotor Understanding
Guided Active Reinforcement Learning, an Active-RL framework designed to jointly learn sensory
and motor policies by maximizing extra intrinsic sensorimotor reward together with environmental
reward. We model the ActiveVision-RL agent with separate sensory and motor policies by extending
existing RL algorithms with two policy/value branches. Inspired by sensorimotor stage [71, 72, 103],
we use the intrinsic sensorimotor reward to guide the joint learning of two policies, imposing penalties
on the agent for selecting sub-optimal observations. We use a learned sensorimotor reward module to
assign the intrinsic reward. The module is trained using inverse dynamics prediction task [52, 95],
with the same experiences as policy learning without additional data or pre-training."
INTRODUCTION,0.02157164869029276,"In our experiments, we use modified Atari [9] and DeepMind Control suite (DMC) [97] with limited
observability to comprehensively evaluate our proposed method. We also test on Robosuite tasks
to demonstrate the effectiveness of active agent in 3D manipulation. Through the challenging
benchmarks, we experimentally show that SUGARL is an effective and generic approach for Active-
RL with minimum modification on top of existed RL algorithms. The learned sensory policy also
exhibit active vision skills by analogy with humans’ fixation and tracking."
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.023112480739599383,"2
Active Vision Reinforcement Learning Settings"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.02465331278890601,"Consider a vanilla RL setting based on a Markov Decision Process (MDP) described by (S, A, r, P, γ),
where S is the state space, A is the action space, r is the reward function, P describes state transition
which is unknown and γ is the discount factor. In this work, we study ActiveVision-RL under limited
visual observability, described by (S, O, As, Ao, r, P, γ), as shown in Figure 1. O is the actual
partial observation space the agent perceives. In particular, we are interested in visual tasks, so each
observation o is an image contains partial information of an environmental state s, like an image
crop in 2D space or a photo from a viewpoint in 3D space. To emulate the human ability, there
are two action spaces for the agent in Active-RL formulation. As is the motor action space that
causes state change p(s′|s, as). Ao is the sensory action space that only changes the observation of
an environmental state p(o|s, ao). In this setting, the agent needs to take (as, ao) for each step, based
on observation(s) only. An example is shown in Figure 1."
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.026194144838212634,"Our goal is to learn the motor and sensory action policies (πs, πo) that still maximize the return P rt.
Note that the agent is never exposed to the full environmental state s. Both policies are completely
based on the partial observations: as = πs(·|o), ao = πo(·|o). Therefore the overall policy learning
is challenging due to the limited information per step and the non-stationary observations."
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.02773497688751926,"Sensory
Actor"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.029275808936825885,"Critic
Actor"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.030816640986132512,"Original Algorithm
SUGARL"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.032357473035439135,"Sensory and Motor Policies
Sensorimotor Reward Module Enc"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.03389830508474576,Policy
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.03543913713405239,Learning
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.03697996918335902,"Policy
Env Enc"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.03852080123266564,"Sensory
Critic Enc"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.040061633281972264,"Motor
Critic Enc"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.04160246533127889,"Motor
Actor"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.04314329738058552,Policy Learning
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.04468412942989214,"Policies
Env +"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.046224961479198766,"+
Loss"
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.04776579352850539,Sensorimotor Reward Module Learning
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.04930662557781202,"Figure 2: Overview of SUGARL and the comparison with original RL algorithm formulation.
SUGARL introduces an extra sensory policy head, and jointly learns two policies together with
the extra sensorimotor reward. We use the formulation of SAC [36] as an example. We introduce
sensorimotor reward module to assign the reward. The reward indicates the quality of the sensory
policy through the prediction task. The sensorimotor reward module is trained independently to the
policies by action prediction error."
ACTIVE VISION REINFORCEMENT LEARNING SETTINGS,0.05084745762711865,"3
SUGARL: Sensorimotor Understanding Guided Active-RL"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.05238828967642527,"3.1
Active-RL Algorithm with Sensorimotor Understanding"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.053929121725731895,"We implement Active-RL algorithms based on the normal vision-based RL algorithms with simple
modifications regarding separated motor and sensory policies (πs, πo), and the sensorimotor reward
rsugarl. We use DQN [65] and SAC [36] as examples to show the modifications are generally
applicable. The example diagram of SAC is shown in Figure 2. We first introduce the policy then
describe the sensorimotor reward in Section 3.2"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.05546995377503852,"Network Architecture The architectural modification is branching an additional head for sensory
policy, and both policies share a visual encoder stem. For DQN [65], two heads output Qs, Qo for each
policy respectively. This allows the algorithm to select as = arg maxas Qs and ao = arg maxao Qo
for each step. Similarly for SAC [36], the value and actor networks also have two separate heads for
motor and sensory policies. The example in the form of SAC is in Figure 2."
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.05701078582434515,"Joint Learning of Motor and Sensory Policies
Though two types of actions are individually
selected or sampled from network outputs, we find that the joint learning of two policies benefits
the whole learning [41, 111]. The joint learning here means both policies are trained using a shared
reward function. Otherwise, the sensory policy usually fails to learn with intrinsic reward signal only.
Below we give the formal losses for DQN and SAC."
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.05855161787365177,"For DQN, we take the sum of Q-values from two policies Q = Qs + Qo and train both heads jointly.
The loss is the following where we indicate our modifications by blue:"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.060092449922958396,"LQ
i (θi) = E(ot,as
t,ao
t )∼D
h 
yi −
 
Qs
θi(ot, as
t) + Qo
θi(ot, ao
t) 2i"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.061633281972265024,yi = Eot+1
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.06317411402157165,"
renv
t
+ βrsugarl
t
+ γ

max
as
t+1
Qs
θi−1(ot+1, as
t+1) + max
ao
t+1
Qo
θi−1(ot+1, ao
t+1) 
,"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.06471494607087827,"where LQ is the loss for Q-networks, θ stands for the parameters of both heads and the encoder stem,
i is the iteration, and D is the replay buffer. βrsugarl
t
is the extra sensorimotor reward with balancing
scale which will be described in Section 3.2."
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.0662557781201849,"For SAC, we do the joint learning similarly. The soft-Q loss is in the similar form of above DQN
which is omitted for simplicity. The soft-value loss LV and is"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.06779661016949153,LV (ψ) = Eot∼D 1
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.06933744221879815,"2
 
V s
ψ(ot) + V o
ψ(ot) −"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.07087827426810478,"Eas
t∼πs
ϕ,ao
t ∼πo
ϕ

Qs
ψ(ot, as
t) + Qo
ψ(ot, ao
t) −log πs
ϕ(as
t|ot) −log πo
ϕ(ao
t|ot)"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.0724191063174114,"2
,"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.07395993836671803,"and the actor loss Lπ is
Lπ(ϕ) = Eot∼D

log πs
ϕ(as
t|ot) + log πo
ϕ(ao
t|ot) −Qs
ψ(ot, as
t) −Qo
ψ(ot, ao
t) 
,"
ACTIVE-RL ALGORITHM WITH SENSORIMOTOR UNDERSTANDING,0.07550077041602465,"where ψ, ϕ are parameters for the critic and the actor respectively, and reparameterization is omitted
for clearness."
SENSORIMOTOR REWARD,0.07704160246533127,"3.2
Sensorimotor Reward"
SENSORIMOTOR REWARD,0.07858243451463791,"The motor and sensory policies are jointly trained using a shared reward function, which is the
combination of environmental reward and our sensorimotor reward. We first introduce the assignment
of sensorimotor reward and then describe the combination of two rewards."
SENSORIMOTOR REWARD,0.08012326656394453,"The sensorimotor reward is assigned by the sensorimotor reward module uξ(·). The module is trained
to have the sensorimotor understanding, and is used to indicate the goodness of the sensory policy,
tacking inspiration of human sensorimotor learning [72]. The way we obtain such reward module is
similar to learning an inverse dynamics model [95]. Given a transition (ot, as
t, ot+1), the module
predicts the motor action as only, based on an observation transition tuple (ot, ot+1). When the
module is (nearly) fully trained, the higher prediction error indicates the worse quality of visual
observations. For example, if the agent is absent from observations, it is hard to infer the motor action.
Such sub-optimal observations also confuse agent’s motor policy. Since the sensory policy selects
those visual observations, the quality of visual observations is tied to the sensory policy. As a result,
we can employ the negative error of action prediction as the sensorimotor reward:"
SENSORIMOTOR REWARD,0.08166409861325115,"rsugarl
t
= −(1 −p(as
t|ot, ot+1; uξ)) .
(1)
This non-positive intrinsic reward penalizes the sensory policy for selecting sub-optimal views that do
not contribute to the accuracy of action prediction and confuse motor policy learning. In Section 5.4
we show that naive positive rewarding does not guide the policy well. Note that the reward is less
noisy when the module is fully trained. However, it is not harmful for being noisy at the early stage
of training, as the noisy signal may encourage the exploration of policies. We use the sensorimotor
reward though the whole learning."
SENSORIMOTOR REWARD,0.08320493066255778,"The sensorimotor reward module is implemented by an independent neural network. The loss is a
simple prediction error:
Lu(ξ) = Eot,ot+1,as
t∼D [Error (as
t −uξ(ot, ot+1))] ,
(2)"
SENSORIMOTOR REWARD,0.0847457627118644,"where the Error(·) can be a cross-entropy loss for discrete action space or L2 loss for continuous
action space. Though being implemented and optimized separately, the sensorimotor reward module
uses the same experience data as policy learning, with no extra data or prior knowledge introduced."
SENSORIMOTOR REWARD,0.08628659476117104,"Combining Sensorimotor Reward and Balancing The sensorimotor reward rsugarl is added densely
on a per-step basis, on top of the environmental reward renv in a balanced form:"
SENSORIMOTOR REWARD,0.08782742681047766,"rt = renv
t
+ βrsugarl
t
,
(3)
where β is the balance parameter varies across environments. The reward balance is very important
to make both motor and sensory policies work as expected, without heavy bias towards one side [28],
which will be discussed in our ablation study in Section 5.4. Following the studies in learning
with intrinsic rewards and rewards of many magnitudes [20, 38, 75, 96, 99], we empirically set
β = Eτ[PT
t=1 renv
t /T], which is the average environmental return normalized by the length of the
trajectory. We get these referenced return data from the baseline agents trained on normal, fully
observable environments, or from the maximum possible environmental return of one episode."
PERSISTENCE-OF-VISION MEMORY,0.08936825885978428,"3.3
Persistence-of-Vision Memory"
PERSISTENCE-OF-VISION MEMORY,0.09090909090909091,"To address ActiveVision-RL more effectively, we introduce a Persistence-of-Vision Memory (PVM)
to spatio-temporally combine multiple recent partial observations into one, mimicking the nature Full"
PERSISTENCE-OF-VISION MEMORY,0.09244992295839753,Partial
PERSISTENCE-OF-VISION MEMORY,0.09399075500770417,Padded
PERSISTENCE-OF-VISION MEMORY,0.09553158705701079,Partial PVM Obs.
PERSISTENCE-OF-VISION MEMORY,0.0970724191063174,"t=0
t=1
t=2
t=3"
PERSISTENCE-OF-VISION MEMORY,0.09861325115562404,"t=0
t=1
t=2
t=3"
PERSISTENCE-OF-VISION MEMORY,0.10015408320493066,"LSTM
PVM 
Obs."
PERSISTENCE-OF-VISION MEMORY,0.1016949152542373,"CNN
CNN
CNN
CNN"
PERSISTENCE-OF-VISION MEMORY,0.10323574730354391,"Stitching PVM
Neural Representation PVM
Figure 3: Examples for different instantiations of PVM with B = 3."
PERSISTENCE-OF-VISION MEMORY,0.10477657935285054,"Wipe 
Door
NutAssembly"
PERSISTENCE-OF-VISION MEMORY,0.10631741140215717,"Front View
Agent View
Eye-in-hand View
Side View"
PERSISTENCE-OF-VISION MEMORY,0.10785824345146379,"Lift
Stack"
PERSISTENCE-OF-VISION MEMORY,0.10939907550077041,"Front View
Agent View
Eye-in-hand View
Side View
Front View
Agent View
Eye-in-hand View
Side View"
PERSISTENCE-OF-VISION MEMORY,0.11093990755007704,"Front View
Agent View
Eye-in-hand View
Side View
Front View
Agent View
Eye-in-hand View
Side View
Figure 4: Five selected Robosuite tasks with examples on four hand-coded views."
PERSISTENCE-OF-VISION MEMORY,0.11248073959938366,"of human eyes and the memory. PVM aims to expand the effective observable area, even though
some visual observations may become outdated or be superseded by more recent observations.
PVM stores the observations from B past steps in a buffer and combines them into a single PVM
observation according to there spatial positions. This PVM observation subsequently replaces the
original observation at each step:"
PERSISTENCE-OF-VISION MEMORY,0.1140215716486903,"PVM(ot) = f(ot−B+1, . . . , ot),"
PERSISTENCE-OF-VISION MEMORY,0.11556240369799692,"where f(·) is a combination operation. In the context of ActiveVision-RL, it is reasonable to assume
that the agent possesses knowledge of its focus point, as it maintains the control over the view.
Therefore, the viewpoint or position information can be used. In our implementations of f(·) shown
in Figure 3, we show a 2D case of PVM using stitching, and a PVM using LSTM which can be used
in both 2D and 3D environments. In stitching PVM, the partial observations are combined like a
Jiasaw puzzle. It is worth noting that combination function f can be implemented by other pooling
operations, sequence representations [15, 44, 85, 86], neural memories [37, 39, 76, 78], or neural 3D
representations [74, 107], depending on the input modalities, tasks, and approaches."
ENVIRONMENTS AND SETTINGS,0.11710323574730354,"4
Environments and Settings"
ENVIRONMENTS AND SETTINGS,0.11864406779661017,"4.1
Active-Gym: An Environment for ActiveVision-RL"
ENVIRONMENTS AND SETTINGS,0.12018489984591679,"We present Active-Gym, an open-sourced customized environment wrapper designed to transform
RL environments into ActiveVision-RL constructs. Our library currently supports active vision agent
on Robosuite [114], a robot manipulation environment in 3D, as well as Atari games [9] and the
DeepMind Control Suite (DMC) [97] offering 2D active vision cases. These 2D environments were
chosen due to the availability of full observations for establishing baselines and upper bounds, and
the availability to manipulate observability for systematic study."
ROBOSUITE SETTINGS,0.12172573189522343,"4.2
Robosuite Settings"
ROBOSUITE SETTINGS,0.12326656394453005,"Observation Space In the Robosuite environment, the robot controls a movable camera. The image
captured by that camera is a partial observation of the 3D space."
ROBOSUITE SETTINGS,0.12480739599383667,"Action Spaces Each step in Active-Gym requires motor and sensory actions (as, ao). The motor
action space As is the same as the base environment. The sensory action space Ao is a 5-DoF control:
relative (x, y, z, yaw, pitch). The maximum linear and angular velocities are constrained to 0.01/step
and 5 degrees/step, respectively."
ROBOSUITE SETTINGS,0.1263482280431433,"4.3
2D Benchmark Settings"
ROBOSUITE SETTINGS,0.12788906009244994,"Observation Space In the 2D cases of Active-Gym, given a full observation O with dimensions
(H, W), only a crop of it is given to the agent’s input. Examples are highlighted in red boxes in
Figure 5. The sensory action decides an observable area by a location (x, y), corresponding to the
top-left corner of the bounding box, and the size of the bounding box (h, w). The pixels within the
observable area becomes the foveal observation, defined as of = oc = O[x : x + h, y : y + w].
Optionally, the foveal observation can be interpolated to other resolutions of = Interp(oc; (h, w) →
(rf
h, rf
w)), where (rf
h, rf
w) is the foveal resolution. This design allows for flexibility in altering
the observable area size while keeping the effective foveal resolution constant. Typically we set
(rf
h, rf
w) = (h, w) and fixed them during a task. The peripheral observation can be optionally provided
as well, obtained by interpolating the non-foveal part op = Interp(O \ oc; (H, W) →(rp
h, rp
w)),
where (rp
h, rp
w) is the peripheral resolution. The examples are at the even columns of Figure 5. If the
peripheral observation is not provided, op = 0."
ROBOSUITE SETTINGS,0.12942989214175654,"Full
w/o F
P: 20x20"
ROBOSUITE SETTINGS,0.13097072419106318,"F: 20x20 
w/o P"
ROBOSUITE SETTINGS,0.1325115562403698,"F: 20x20 
w/ P"
ROBOSUITE SETTINGS,0.13405238828967642,"F: 30x30 
w/o P"
ROBOSUITE SETTINGS,0.13559322033898305,"F: 30x30 
w/ P"
ROBOSUITE SETTINGS,0.13713405238828968,"F: 50x50 
w/o P"
ROBOSUITE SETTINGS,0.1386748844375963,"F: 50x50 
w/ P"
ROBOSUITE SETTINGS,0.14021571648690292,4x4 discretized sensory actions
ROBOSUITE SETTINGS,0.14175654853620956,(for foveal observation)
ROBOSUITE SETTINGS,0.14329738058551617,"(0,0)
(0,1) (1,0) (2,0) (3,0)"
ROBOSUITE SETTINGS,0.1448382126348228,"(0,2)
(0,3) …… (3,3)"
ROBOSUITE SETTINGS,0.14637904468412943,"Figure 5: Left: observations from Active-Gym with different foveal resolutions (F) and peripheral
settings (P) in Atari and DMC. The red bounding boxes show the observable area (foveal) for clarity.
Right: 4x4 discrete sensory action options in our experiments."
ROBOSUITE SETTINGS,0.14791987673343607,"Table 1: Results on Robosuite. We report the IQM of raw rewards from 30 evaluations. Highlighted
task names are harder tasks. Bold numbers are the best scores of each task and underscored numbers
are the second best."
ROBOSUITE SETTINGS,0.14946070878274267,"Approach
Wipe
Door
NutAssemblySquare
Lift
Stack"
ROBOSUITE SETTINGS,0.1510015408320493,"SUGARL-DrQ (Stacking PVM)
56.0
274.8
78.0
79.2
12.7
SUGARL-DrQ (LSTM PVM)
58.5
266.9
108.6
88.8
31.5
SUGARL-DrQ (3D Transformation+LSTM PVM)
74.1
291.0
65.2
87.5
32.4"
ROBOSUITE SETTINGS,0.15254237288135594,"SUGARL-DrQ w/o Joint Learning
43.6
175.4
58.0
107.2
12.0
SUGARL-DrQ w/o PVM
52.8
243.3
37.9
55.6
7.7"
ROBOSUITE SETTINGS,0.15408320493066255,"Single Policy
12.4
22.8
8.42
10.7
0.53
DrQ w/ Object Detection (DETR)
15.2
43.1
54.8
15.4
7.5
DrQ w/ End-to-End Attention
14.2
141.4
28.5
33.0
13.6"
ROBOSUITE SETTINGS,0.15562403697996918,"Eye-in-hand View (hand-coded, moving camera)
16.1
114.6
102.9
233.9
73.0
Front View (hand-coded, fixed camera)
49.4
240.6
39.6
69.0
13.8
Agent View (hand-coded, fixed camera)
12.7
190.3
49.9
122.6
14.7
Side View (hand-coded, fixed camera)
25.9
136.2
34.5
56.6
12.8"
ROBOSUITE SETTINGS,0.15716486902927582,"Action Spaces The sensory action space Ao includes all the possible (pixel) locations on the full
observation, but can be further formulated to either continuous or discrete spaces according to specific
task designs. In our experiments, we simplify the space by a 4x4 discrete grid-like anchors for (x, y)
(Figure 5 right). Each anchor corresponds to the top-left corner of the observable area. The sensory
policy chooses to place the observable area among one of 16 anchors (absolute control), or moves it
from one to the four neighbor locations (relative control)."
LEARNING SETTINGS,0.15870570107858242,"4.4
Learning Settings"
LEARNING SETTINGS,0.16024653312788906,"In our study, we primarily use DQN [65] and SAC [36] as the backbone algorithms of SUGARL to
address tasks with discrete action spaces (Atari), and use DrQv2 [108] for continuous action spaces
(DMC and Robosuite). All the visual encoders are standardized as the convolutional networks
utilized in DQN [65]. To keep the network same, we resize all inputs to 84x84. For the sensorimotor
understanding model, we employ the similar visual encoder architecture with a linear head to predict
as
t. Each agent is trained with one million transitions for each of the 26 Atari games, or trained with
0.1 million transitions for each of the 6 DMC tasks and 5 Robosuite tasks. The 26 games are selected
following Atari-100k benchmark [47]. We report the results using Interquartile Mean (IQM), with
the scores normalized by the IQM of the base DQN agent under full observation (except Robosuite),
averaged across five seeds (three for Robosuite) and all games/tasks per benchmark. Details on
architectures and hyperparameters can be found in the Appendix."
RESULTS,0.1617873651771957,"5
Results"
ROBOSUITE RESULTS,0.1633281972265023,"5.1
Robosuite Results"
ROBOSUITE RESULTS,0.16486902927580893,"We selected five of available tasks in Robosuite [114], namely block lifting (Lift), block stacking
(Stack), nut assembling (NutAssembleSquare), door opening (Door), wiping the table (Wipe). The
first two are easier compared to the later three. Example observations are available in Figure 4. We
compare against a straightforward baseline that a single policy is learned to govern both motor and
sensory actions. We also compare to baselines including RL with object detection (a replication
of Cheng et al. [17]), learned attention [93], and standard RL with hand-coded views. Results are
in 1. We confirm that our SUGARL works outperforms baselines all the time, and also outperforms"
ROBOSUITE RESULTS,0.16640986132511557,"20x20
30x30
50x50
Observable Area Size 0.0 0.2 0.4 0.6 0.8 1.0"
ROBOSUITE RESULTS,0.1679506933744222,Averaged IQM 0.307 0.241 0.367 0.287 0.246
ROBOSUITE RESULTS,0.1694915254237288,"0.487
0.475"
ROBOSUITE RESULTS,0.17103235747303544,"0.810
0.805"
ROBOSUITE RESULTS,0.17257318952234207,"Random View
Raster Scanning
SUGARL"
ROBOSUITE RESULTS,0.17411402157164868,(a) Without peripheral observation
ROBOSUITE RESULTS,0.17565485362095531,"20x20
30x30
50x50
Observable Area Size 0.0 0.2 0.4 0.6 0.8 1.0 1.2"
ROBOSUITE RESULTS,0.17719568567026195,Averaged IQM
ROBOSUITE RESULTS,0.17873651771956856,"0.764
0.768 0.885"
ROBOSUITE RESULTS,0.1802773497688752,"0.800
0.794
0.807 1.010 1.116 1.289"
ROBOSUITE RESULTS,0.18181818181818182,"Random View
Raster Scanning
SUGARL"
ROBOSUITE RESULTS,0.18335901386748846,(b) With peripheral observation
ROBOSUITE RESULTS,0.18489984591679506,"20x20
30x30
50x50
Observable Area Size 0.0 0.2 0.4 0.6 0.8 1.0"
ROBOSUITE RESULTS,0.1864406779661017,Averaged IQM 0.258 0.350 0.588 0.236 0.310 0.359 0.184 0.384 0.695 0.475
ROBOSUITE RESULTS,0.18798151001540833,"0.810
0.805"
ROBOSUITE RESULTS,0.18952234206471494,"Center
Upper Left
Bottom Right
SUGARL"
ROBOSUITE RESULTS,0.19106317411402157,(c) Static policies comparison
ROBOSUITE RESULTS,0.1926040061633282,"Figure 6: Results with different observation size and peripheral observation settings. The green
bars are results from SUGARL. The red dashed lines stand for the DQN baseline trained on full
observations using the same amount of data. We compare SUGARL against two dynamic views:
Random View and Raster Scanning, and three static view baselines. In (b) with peripheral observation,
we compare a baseline using peripheral observation only indicated by the cyan line."
ROBOSUITE RESULTS,0.1941448382126348,"the hand-coded views most of the time. Specifically, for the harder tasks including Wipe, Door, and
NutAssemblySquare, SUGARL gets the best scores."
ROBOSUITE RESULTS,0.19568567026194145,"Designs of PVM
We compare different instantiations of proposed PVMs including: Stacking:
naively stacking multiple frames; LSTM: Each image is first encoded by CNN and fed into LSTM;
3D Transformation + LSTM: we use camera parameters to align pixels from different images to the
current camera frame. Then an LSTM encodes the images after going through CNN. We find that 3D
Transformation + LSTM works the best, because it tackles spatial aligning and temporal merging
together. LSTM also works well in general."
ROBOSUITE RESULTS,0.19722650231124808,"5.2
2D Benchmark Results"
ROBOSUITE RESULTS,0.1987673343605547,"We evaluate the policy learning on Atari under two primary visual settings: with and without
peripheral observation. In each visual setting we explore three sizes of observable area (set
equivalent to foveal resolution): 20x20, 30x30, and 50x50. In with peripheral observation setting, the
peripheral resolution is set to 20x20 for all tasks. We use DQN [65]-based SUGARL (SUGARL-
DQN) and compare it against two variants by replacing the learnable sensory policy in SUGARL with:
Random View and Raster Scanning. Random View always uniformly samples from all possible
crops. Raster Scanning uses a pre-designed sensory policy that chooses observable areas from left
to right and top to down sequentially. Raster Scanning yields relatively stable observation patterns
and provides maximum information under PVM. We also provide a DQN baseline trained on full
observations (84x84) as a soft oracle. In with peripheral observation settings, another baseline trained
on peripheral observation only (20x20) is compared as a soft lower bound."
ROBOSUITE RESULTS,0.20030816640986132,"In Figure 6a and 6b, we find that SUGARL performs the best in all settings, showing that SUG-
ARL learns effective sensory and motor policies jointly. More importantly, SUGARL with peripheral
observation achieves higher overall scores (+0.01∼0.2) than the full observation baselines. In details,
SUGARL gains higher scores than the full observation baseline in 13 out of 26 games with 50x50
foveal resolution (details are available in the Appendix). This finding suggests the untapped potential
of ActiveVision-RL agents which leverage partial observations better than full observations. By ac-
tively selecting views, the agent can filter out extraneous information and concentrate on task-centric
information."
ROBOSUITE RESULTS,0.20184899845916796,"Compare against Static Sensory Policies We also examine baselines with static sensory policies,
which consistently select one region to observe throughout all steps. The advantage of this type
baseline lies in the stability of observation. We select 3 regions: Center, Upper Left, and Bottom
Right, and compare them against SUGARL in environment w/o peripheral observation. As shown
in Figure 6c, we observe that SUGARL still surpasses all three static policies. The performance
gaps between SUGARL and Center and Bottom Right are relatively small when the observation
size is larger (50x50), as the most valuable information is typically found at these locations in Atari"
ROBOSUITE RESULTS,0.2033898305084746,"Table 2: Evaluation results on different conditions and algorithm backbones
(a) Action modeling"
ROBOSUITE RESULTS,0.2049306625577812,"Model
20
30
50"
ROBOSUITE RESULTS,0.20647149460708783,"SUGARL (abs) 0.475 0.810 0.805
SUGARL (rel) 0.367 0.745 0.945
Single Policy
0.132 0.222 0.171"
ROBOSUITE RESULTS,0.20801232665639446,(b) Train more steps
ROBOSUITE RESULTS,0.20955315870570107,"Steps
Model
20
30
50"
M,0.2110939907550077,"1M
SUGARL 0.475 0.810 0.805
Single Policy 0.132 0.222 0.171"
M,0.21263482280431434,"5M
SUGARL 1.170 1.121 1.553
Single Policy 0.332 0.640 1.145"
M,0.21417565485362094,(c) SUGARL with SAC
M,0.21571648690292758,"Model
20
30
50"
M,0.2172573189522342,"SUGARL
0.424 0.730 0.785
SUGARL w/o rsugarl 0.300 0.307 0.504
SAC-raster scanning 0.117 0.195 0.136
SAC-random view
0.155 0.104 0.134"
M,0.21879815100154082,(d) Different PVMs
M,0.22033898305084745,"Model
20
30
50"
M,0.2218798151001541,"Stitching PVM 0.475 0.815 0.810
LSTM PVM
0.397 0.448 0.470"
M,0.22342064714946072,Frequency
M,0.22496147919876733,"“Fixation”
“Movement”"
M,0.22650231124807396,"boxing
battle_zone"
M,0.2280431432973806,"Figure 7: Examples of learned sensory policies from SUGARL-DQN. Top: frequency heat maps of
pixels being observed. Bottom: sequences of observations showing fixation-like and tracking-like
behaviors. More videos are available at this link"
M,0.2295839753466872,"environments. Static observation is therefore quite beneficial for decision-making. However, as the
observation size decreases, an active policy becomes essential for identifying optimal observations,
leading SUGARL to outperform others by a significant margin."
M,0.23112480739599384,"Sensory-Motor Action Spaces Modeling SUGARL models the sensory and motor action spaces
separately inspired by the human abilities. An alternative approach is jointly modeling two action
spaces A = As × Ao and learning one single action policy, with environmental reward only (referred
as Single Policy). We compare Single Policy and display the results in Table 2a, which reveal that
modeling one large action space fails to learn the policy properly due to the expanded action space.
Additionally, we examine two types of sensory action spaces: absolute (abs) and relative (rel).
Absolute modeling allows the view to jump across the entire space while relative modeling restricts
the view to moving only to the neighboring options at each step. Results in Table 2a indicate that the
absolute modeling performs better in smaller observable regions, as it can select the desired view
more quickly than relative modeling. In contrast, relative modeling demonstrates better performance
in larger observable region setting as it produces more stable observations across steps."
M,0.23266563944530047,"Training More Steps We subsequently explore SUGARL performance when trained with more
environmental steps, comparing outcomes between 1M and 5M steps. Results in Table 2b confirm
that SUGARL continuous to improve with more steps and consistently outperforms the single
policy baseline, suggesting that it does not merely stagnate at a trivial policy. However, due to the
challenges posed by limited observability, the learning still proceeds slower than for the agent with
full observations, which achieves the score 4.106 at 5M steps."
M,0.23420647149460708,"Generalization of SUGARL We apply SUGARL to Soft Actor-Critic (SAC) [36] and evaluate its
performance on environments without peripheral observation. As before, we compare it with Random
View, Raster Scanning, and SUGARL without intrinsic reward. According to Table 2c, we find that
SUGARL-SAC outperforms the naive version without the guidance of rsugarl, further emphasizing
the significance of our method. Moreover, SUGARL-SAC also surpasses the random view and raster
scanning policies. However, when employing max-entropy-based methods like SAC, it is necessary
to reduce the weight of the entropy term to ensure consistency in the sensory policy. We will further
discuss it in Section 5.3 based on the analysis of the learned sensory policy behavior."
M,0.2357473035439137,Table 3: SUGARL on DMC
M,0.23728813559322035,"Model
20
30
50
SUGARL-DrQ
0.686 0.717 1.052
DrQ-Single Policy
0.540 0.570 0.776
DrQ-Raster Scanning
0.609 0.566 0.913
DrQ-Random View
0.569 0.591 0.768
SUGARL-DrQ w/o PVM
0.672 0.620 0.930
SUGARL w/o Joint Learning 0.377 0.446 0.355"
SENSORY POLICY ANALYSIS,0.23882896764252695,"5.3
Sensory Policy Analysis"
SENSORY POLICY ANALYSIS,0.24036979969183359,"Sensory Policy Patterns
In Figure 7, we present examples of learned sensory policies from
SUGARL-DQN in four Atari games, in settings w/o peripheral observations. These policies are
visualized as heat maps based on the frequency of observed pixels. We discover that the sensory
policies learn both fixation and movement (similar to tracking) behaviours [12, 110] depending on
the specific task requirements. In the first two examples, the sensory policy tends to concentrate on
fixed regions. In battle_zone, the policy learns to focus on the regions where enemies appear and the
fixed front sight needed for accurate firing. By contrast, in highly dynamic environments like boxing
and freeway, the sensory policies tend to observe broader areas in order to get timely observations.
Though not being a perfect tracker, the sensory policy learns to track the agent or the object of interest
in these two environments, demonstrating the learned capability akin to humans’ Smooth Pursuit
Movements. Recorded videos for entire episodes are available at our project page."
SENSORY POLICY ANALYSIS,0.24191063174114022,"20x20
30x30
50x50"
SENSORY POLICY ANALYSIS,0.24345146379044685,"Figure 8: KL divergence distributions of
learned sensory policies."
SENSORY POLICY ANALYSIS,0.24499229583975346,"Sensory Policy Distribution We quantitatively assess the
distributions of learned sensory policies. There are 16 sen-
sory actions, i.e. the observable area options in our setup
(Figure 5). We compare the distributions against uniform
distribution using KL-divergence, across 26 games x 10
eval runs x 5 seeds. The resulting histogram are shown in
Figure 8. We observe that the learned policies consistently
deviate from the uniform distribution, suggesting that sen-
sory policies prefer specific regions in general. The high
peak at the high KL end supports the “fixation” behavior identified in the previous analysis. As the
observation size increases, the divergence distribution shifts towards smaller KL end, while remaining
> 0.5 for all policies. This trend indicates that with larger observable sizes, the policy does not need
to adjust its attention frequently, corroborating the benefit of using relative control shown in Table 2a."
SENSORY POLICY ANALYSIS,0.2465331278890601,"Table
4:
Varing
α
of
SUGARL-SAC."
SENSORY POLICY ANALYSIS,0.24807395993836673,"Model
20
30
50
autotune α
0.271 0.358 0.444
fixed-α = 0.2 0.424 0.730 0.785"
SENSORY POLICY ANALYSIS,0.24961479198767333,"Pitfalls on Max-Entropy Based Methods
In the previous analysis,
we both qualitatively and quantitatively demonstrate that sensory
policies are not uniformly dispersed across the entire sensory action
space. This observation implies that sensory policy should exercise
caution when adopting the max-entropy-based methods. We conduct
an experiment on varying the usage of α, the entropy coefficient in
SAC in all three foveal observation size settings. Results in Table 4
show that simply disabling autotune and setting a small value to the sensory policy’s α improves.
This finding tells that max-entropy may not be a suitable assumption in modeling sensory policies."
ABLATION STUDIES,0.25115562403698,"5.4
Ablation Studies"
ABLATION STUDIES,0.2526964560862866,"We conduct ablation studies in the setting without peripheral observation and with 50x50 observation
size. Five crucial design components are investigated to validate their effectiveness by incrementally
adding or removing them from the full model. The results are presented in Table 5. From the
results, we demonstrate the importance of penalizing the agent for inaccurate self-understanding
predictions rather than rewarding accurate predictions (rsugarl(positive) →rsugarl(negative)). By
imposing penalties, the maximum return is bounded by the original maximum possible return per
episode, allowing motor and sensory policies to better coordinate each other and achieve optimal
task performance. Reward balance significantly improves the policy, indicating its effectiveness in
coordinating two policies as well. PVM also considerably enhances the algorithm by increasing the
effective observable area as expected."
ABLATION STUDIES,0.2542372881355932,"Table 5: Ablation study results based on DQN 50x50 foveal res w/o peripheral observation. The
blank fields means there are no such modifications for that model."
ABLATION STUDIES,0.25577812018489987,"Model
rsugarl
Joint Learning
PVM
Reward Balance (β)
IQM"
ABLATION STUDIES,0.2573189522342065,"Random View
✓
0.367"
ABLATION STUDIES,0.2588597842835131,SUGARL =
ABLATION STUDIES,0.26040061633281975,"




"
ABLATION STUDIES,0.26194144838212635,"



"
ABLATION STUDIES,0.26348228043143296,"Base RL algorithm
-
+Naive positive intrinsic reward
positive
0.281
+Joint learning
positive
✓
0.322
Positive →negative rsugarl
negative
✓
0.360
+PVM
negative
✓
✓
0.423
+Reward Balance
negative
✓
✓
✓
0.805"
ABLATION STUDIES,0.2650231124807396,"SUGARL w/o rsugarl
✓
✓
0.421
SUGARL w/o rsugarl and w/o PVM
✓
0.231"
RELATED WORK,0.2665639445300462,"6
Related Work"
RELATED WORK,0.26810477657935283,"Active Learning is the concept that an agent decides which data are taken into its learning and may ask
for external information in comparison to fitting a fixed data distribution [1, 6, 10, 21, 22, 31, 46, 49,
55, 57, 60, 64, 66, 73, 81, 88, 90, 101, 105, 113]. Active Vision focuses on continuously acquiring
new visual observations that is helpful for the vision task like object classification, recognition
and detection [4, 5, 7, 8, 18, 28, 29, 48, 63, 98, 106, 109], segmentation [13, 58, 70], and action
recognition [45]. The active vision is usually investigated under a robot vision scenario that a robot
moves around in a scene. However, the policy is usually not required to accomplish a task with
physical interactions such as manipulating objects compared to reinforcement learning."
RELATED WORK,0.2696456086286595,"Active Reinforcement Learning (Active-RL), at a high level, is that the agent is allowed to actively
gather new perceptual information of interest simultaneously through an RL task, which can also be
called active perception [102]. The extra perceptual information could be reward signal [3, 23, 27, 50,
56, 62], visual observations from new viewpoints [33, 54, 67, 87], other input modalities [16], and
language instructions [19, 68, 69, 94]. Though these work may not explicitly use the term Active-RL,
we find that they can be uniformly organized in the general Active-RL formulation and we coin
the term here. In our work, we study the ActiveVision-RL task in a limited visual observability
environment, where at each step the agent is only able to partially observe the environment. The agent
should actively seek the optimal observation at each step. Therefore, our setting is more close to [32,
33] and Active Vision problems, unlike research incorporating attention-like inductive bias given a
full observation [34, 42, 43, 79, 91, 93, 104]. The ActiveVision-RL agent must learn an observation
selection policy, called sensory policy, to effectively choose the optimal partial observation for
executing the task-specific policy (motor policy). The unique challenge for ActiveVision-RL is the
coordination between sensory and motor policies given there mutual influence. In recent works, the
sensory policy can be either trained in the task-agnostic way [33] with enormous exploration data, or
trained jointly with the task [32] with naive environmental reward only. In this work we investigate
the joint learning case because of the high cost and availability concern of pre-training tasks [33]."
RELATED WORK,0.2711864406779661,"Robot Learning with View Changes Viewpoint changes and gaps in visual observations are the
common challenges for robot learning [40, 53, 77, 84, 92, 112], especially for the embodied agents
that uses its first-person view [30, 35, 80]. To address those challenges, previous works proposed to
map visual observation from different viewpoints to a common representation space by contrastive
encoding [26, 82, 83] or build implicit neural representations [53, 107]. In many first-person view
tasks, the viewpoint control is usually modeled together with the motor action like manipulation and
movement [30, 80]. In contrast, in our ActiveVision-RL setting, we explore the case where the agent
can choose where to observe independently to the motor action inspired by the humans’ ability."
LIMITATIONS,0.2727272727272727,"7
Limitations"
LIMITATIONS,0.27426810477657937,"In this work, we assume that completely independent sensory and motor actions are present in an
embodied agent. But in a real-world case, the movement of the sensors may depend on the motor
actions. For example, a fixed camera attached to the end-effector of a robot manipulator, or to a
mobility robot. To address the potential dependence and conflicts between two policies in this case,
extensions like voting or weighing across two actions to decide the final action may be required.
The proposed algorithm also assumes a chance to adjust viewpoints at every step. This could be
challenging for applications where the operational or latency costs for adjusting the sensors are high
like remote control. To resolve this, additional penalties on sensory action and larger memorization
capability are potentially needed. Last, the intrinsic reward currently only considers the accuracy of
agent-centric prediction. Other incentives like gathering novel information or prediction accuracy
over other objects in the environment can be further explored."
CONCLUSION,0.275808936825886,"8
Conclusion"
CONCLUSION,0.2773497688751926,"We present SUGARL, a framework based on existed RL algorithms to jointly learn sensory and
motor policies through the ActiveVision-RL task. In SUGARL, an intrinsic reward determined
by sensorimotor understanding effectively guides the learning of two policies. Our framework is
validated in both 3D and 2D benchmarks with different visual observability settings. Through the
analysis on the learned sensory policy, it shows impressive active vision skills by analogy with
human’s fixation and tracking that benefit the overall policy learning. Our work paves the initial way
towards reinforcement learning using active agents for open-world tasks."
CONCLUSION,0.27889060092449924,Acknowledgments and Disclosure of Funding
CONCLUSION,0.28043143297380585,"We appreciate the fruitful discussions on the methodology with Xiang Li and Wensheng Cheng, on
experimental designs with Kumara Kahatapitiya and Ryan Burgert. We also appreciate the inspiring
feedbacks from Kanchana Ranasinghe and Varun Belagali."
REFERENCES,0.28197226502311246,References
REFERENCES,0.2835130970724191,"[1] S. Agarwal, H. Arora, S. Anand, and C. Arora. Contextual diversity for active learning. In
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XVI 16, pages 137–153. Springer, 2020."
REFERENCES,0.2850539291217257,"[2] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M. Plappert, G. Powell, R. Ribas, et al. Solving rubik’s cube with a robot hand. arXiv preprint
arXiv:1910.07113, 2019."
REFERENCES,0.28659476117103233,"[3] R. Akrour, M. Schoenauer, and M. Sebag. April: Active preference learning-based reinforce-
ment learning. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23,
pages 116–131. Springer, 2012."
REFERENCES,0.288135593220339,"[4] A. Andreopoulos and J. K. Tsotsos. A theory of active object localization. In 2009 IEEE 12th
International Conference on Computer Vision, pages 903–910. IEEE, 2009."
REFERENCES,0.2896764252696456,"[5] A. Andreopoulos and J. K. Tsotsos. A computational learning theory of active object recogni-
tion under uncertainty. International journal of computer vision, 101:95–142, 2013."
REFERENCES,0.29121725731895226,"[6] J. T. Ash, C. Zhang, A. Krishnamurthy, J. Langford, and A. Agarwal. Deep batch active
learning by diverse, uncertain gradient lower bounds. In International Conference on Learning
Representations, 2020."
REFERENCES,0.29275808936825887,"[7] N. Atanasov, B. Sankaran, J. Le Ny, G. J. Pappas, and K. Daniilidis. Nonmyopic view planning
for active object classification and pose estimation. IEEE Transactions on Robotics, 30(5):
1078–1090, 2014."
REFERENCES,0.2942989214175655,"[8] R. Bajcsy, Y. Aloimonos, and J. K. Tsotsos. Revisiting active perception. Autonomous Robots,
42:177–196, 2018."
REFERENCES,0.29583975346687214,"[9] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment:
An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253–279, jun 2013."
REFERENCES,0.29738058551617874,"[10] W. H. Beluch, T. Genewein, A. Nürnberger, and J. M. Köhler. The power of ensembles for
active learning in image classification. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 9368–9377, 2018."
REFERENCES,0.29892141756548535,"[11] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D˛ebiak, C. Dennison, D. Farhi, Q. Fischer,
S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint
arXiv:1912.06680, 2019."
REFERENCES,0.300462249614792,"[12] A. Borji and L. Itti. State-of-the-art in visual attention modeling. IEEE transactions on pattern
analysis and machine intelligence, 35(1):185–207, 2012."
REFERENCES,0.3020030816640986,"[13] A. Casanova, P. O. Pinheiro, N. Rostamzadeh, and C. J. Pal. Reinforced active learning for
image segmentation. arXiv preprint arXiv:2002.06583, 2020."
REFERENCES,0.3035439137134052,"[14] E. Cetin, P. J. Ball, S. Roberts, and O. Celiktutan. Stabilizing off-policy deep reinforcement
learning from pixels. In International Conference on Machine Learning, pages 2784–2810.
PMLR, 2022."
REFERENCES,0.3050847457627119,"[15] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and
I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In
Advances in Neural Information Processing Systems (NeurIPS), Dec. 2021."
REFERENCES,0.3066255778120185,"[16] S. Chen, H. Cao, Q. Ouyang, X. Wu, and Q. Qian. Alds: An active learning method for
multi-source materials data screening and materials design. Materials & Design, 223:111092,
2022. ISSN 0264-1275."
REFERENCES,0.3081664098613251,"[17] R. Cheng, A. Agarwal, and K. Fragkiadaki. Reinforcement learning of active vision for
manipulating objects under occlusions. In Conference on Robot Learning, pages 422–431.
PMLR, 2018."
REFERENCES,0.30970724191063176,"[18] R. Cheng, Z. Wang, and K. Fragkiadaki. Geometry-aware recurrent neural networks for active
visual recognition. Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.31124807395993837,"[19] T.-C. Chi, M. Shen, M. Eric, S. Kim, and D. Hakkani-tur. Just ask: An interactive learning
framework for vision and language navigation. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pages 2459–2466, 2020."
REFERENCES,0.31278890600924497,"[20] J. Choi and S.-e. Yoon. Intrinsic motivation driven intuitive physics learning using deep
reinforcement learning with intrinsic reward normalization. arXiv preprint arXiv:1907.03116,
2019."
REFERENCES,0.31432973805855163,"[21] J. Choi, K. M. Yi, J. Kim, J. Choo, B. Kim, J. Chang, Y. Gwon, and H. J. Chang. Vab-al:
Incorporating class imbalance and difficulty with variational bayes for active learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
6749–6758, 2021."
REFERENCES,0.31587057010785824,"[22] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal
of artificial intelligence research, 4:129–145, 1996."
REFERENCES,0.31741140215716485,"[23] C. Daniel, M. Viering, J. Metz, O. Kroemer, and J. Peters. Active reward learning. In Robotics:
Science and systems, volume 98, 2014."
REFERENCES,0.3189522342064715,"[24] R. Dodge. Five types of eye movement in the horizontal meridian plane of the field of regard.
American journal of physiology-legacy content, 8(4):307–329, 1903."
REFERENCES,0.3204930662557781,"[25] P. W. Duncan, M. Propst, and S. G. Nelson. Reliability of the fugl-meyer assessment of
sensorimotor recovery following cerebrovascular accident. Physical therapy, 63(10):1606–
1610, 1983."
REFERENCES,0.3220338983050847,"[26] D. Dwibedi, J. Tompson, C. Lynch, and P. Sermanet. Learning actionable representations from
visual observations. In IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pages 1577–1584. IEEE, 2018."
REFERENCES,0.3235747303543914,"[27] A. Epshteyn, A. Vogel, and G. DeJong. Active reinforcement learning. In Proceedings of the
25th international conference on Machine learning, pages 296–303, 2008."
REFERENCES,0.325115562403698,"[28] L. Fan and Y. Wu. Avoiding lingering in learning active recognition by adversarial disturbance.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
(WACV), pages 4612–4621, January 2023."
REFERENCES,0.3266563944530046,"[29] L. Fan, P. Xiong, W. Wei, and Y. Wu. Flar: A unified prototype framework for few-sample
lifelong active recognition. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), pages 15394–15403, October 2021."
REFERENCES,0.32819722650231126,"[30] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,
and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale
knowledge. In Advances in Neural Information Processing Systems, volume 35, pages 18343–
18362. Curran Associates, Inc., 2022."
REFERENCES,0.32973805855161786,"[31] Y. Gal, R. Islam, and Z. Ghahramani. Deep bayesian active learning with image data. In
International conference on machine learning, pages 1183–1192. PMLR, 2017."
REFERENCES,0.3312788906009245,"[32] R. Göransson. Deep reinforcement learning with active vision on atari environments. In
Master’s Thesis. Lund University, 2022."
REFERENCES,0.33281972265023113,"[33] M. K. Grimes, J. V. Modayil, P. W. Mirowski, D. Rao, and R. Hadsell. Learning to look by
self-prediction. Transactions on Machine Learning Research, 2023. ISSN 2835-8856."
REFERENCES,0.33436055469953774,"[34] S. S. Guo, R. Zhang, B. Liu, Y. Zhu, D. Ballard, M. Hayhoe, and P. Stone. Machine versus
human attention in deep reinforcement learning tasks. Advances in Neural Information
Processing Systems, 34:25370–25385, 2021."
REFERENCES,0.3359013867488444,"[35] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov.
MineRL: A large-scale dataset of Minecraft demonstrations. Twenty-Eighth International
Joint Conference on Artificial Intelligence, 2019. URL http://minerl.io."
REFERENCES,0.337442218798151,"[36] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. In Proceedings of the International
Conference on Machine Learning (ICML), 2018."
REFERENCES,0.3389830508474576,"[37] M. J. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In
Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2015."
REFERENCES,0.3405238828967643,"[38] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforce-
ment learning that matters. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018."
REFERENCES,0.3420647149460709,"[39] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735–1780, 1997."
REFERENCES,0.3436055469953775,"[40] K. Hsu, M. J. Kim, R. Rafailov, J. Wu, and C. Finn. Vision-based manipulators need to also
see from their hands. arXiv preprint arXiv:2203.12677, 2022."
REFERENCES,0.34514637904468415,"[41] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular
policies for agent-agnostic control. In International Conference on Machine Learning, pages
4455–4464. PMLR, 2020."
REFERENCES,0.34668721109399075,"[42] S. James and A. J. Davison. Q-attention: Enabling efficient learning for vision-based robotic
manipulation. IEEE Robotics and Automation Letters, 7(2):1612–1619, 2022."
REFERENCES,0.34822804314329736,"[43] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning
for visual robotic manipulation via discretisation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 13739–13748, 2022."
REFERENCES,0.349768875192604,"[44] M. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling
problem. In Advances in Neural Information Processing Systems (NeurIPS), Dec. 2021."
REFERENCES,0.35130970724191063,"[45] D. Jayaraman and K. Grauman. Look-ahead before you leap: end-to-end active recognition by
forecasting the effect of motion. In Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 489–505.
Springer, 2016."
REFERENCES,0.35285053929121724,"[46] A. J. Joshi, F. Porikli, and N. Papanikolopoulos.
Multi-class active learning for image
classification. In 2009 ieee conference on computer vision and pattern recognition, pages
2372–2379. IEEE, 2009."
REFERENCES,0.3543913713405239,"[47] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan,
C. Finn, P. Kozakowski, S. Levine, A. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski.
Model based reinforcement learning for atari. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020."
REFERENCES,0.3559322033898305,"[48] S. Kasaei, J. Sock, L. S. Lopes, A. M. Tomé, and T.-K. Kim. Perceiving, learning, and
recognizing 3d objects: An approach to cognitive service robots. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 32, 2018."
REFERENCES,0.3574730354391371,"[49] T. Kim, I. Hwang, H. Lee, H. Kim, W.-S. Choi, J. J. Lim, and B.-T. Zhang. Message
passing adaptive resonance theory for online active semi-supervised learning. In International
Conference on Machine Learning, pages 5519–5529. PMLR, 2021."
REFERENCES,0.35901386748844377,"[50] D. Krueger, J. Leike, O. Evans, and J. Salvatier. Active reinforcement learning: Observing
rewards at a cost. arXiv preprint arXiv:2011.06709, 2020."
REFERENCES,0.3605546995377504,"[51] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination
for robotic grasping with deep learning and large-scale data collection. The International
journal of robotics research, 37(4-5):421–436, 2018."
REFERENCES,0.362095531587057,"[52] X. Li, J. Shang, S. Das, and M. Ryoo. Does self-supervised learning really improve reinforce-
ment learning from pixels? In Advances in Neural Information Processing Systems, volume 35,
pages 30865–30881, 2022."
REFERENCES,0.36363636363636365,"[53] Y. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for
visuomotor control. In Conference on Robot Learning, pages 112–123. PMLR, 2022."
REFERENCES,0.36517719568567025,"[54] L. Liu, S. Fryc, L. Wu, T. L. Vu, G. Paul, and T. Vidal-Calleja. Active and interactive mapping
with dynamic gaussian process implicit surfaces for mobile manipulators. IEEE Robotics and
Automation Letters, 6(2):3679–3686, 2021."
REFERENCES,0.3667180277349769,"[55] Z. Liu, H. Ding, H. Zhong, W. Li, J. Dai, and C. He. Influence selection for active learning. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9274–9283,
2021."
REFERENCES,0.3682588597842835,"[56] M. Lopes, F. Melo, and L. Montesano. Active learning for reward estimation in inverse rein-
forcement learning. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II
20, pages 31–46. Springer Berlin Heidelberg, 2009."
REFERENCES,0.3697996918335901,"[57] W. Luo, A. Schwing, and R. Urtasun. Latent structured active learning. Advances in Neural
Information Processing Systems, 26, 2013."
REFERENCES,0.3713405238828968,"[58] D. Mahapatra, B. Bozorgtabar, J.-P. Thiran, and M. Reyes. Efficient active learning for image
classification and segmentation using a sample selection and conditional generative adversarial
network. In International Conference on Medical Image Computing and Computer-Assisted
Intervention, pages 580–588. Springer, 2018."
REFERENCES,0.3728813559322034,"[59] J. S. Matthis, J. L. Yates, and M. M. Hayhoe. Gaze and the control of foot placement when
walking in natural terrain. Current Biology, 28(8):1224–1233, 2018."
REFERENCES,0.37442218798151,"[60] C. Mayer and R. Timofte. Adversarial sampling for active learning. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3071–3079, 2020."
REFERENCES,0.37596302003081666,"[61] M. K. McBeath, D. M. Shaffer, and M. K. Kaiser. How baseball outfielders determine where
to run to catch fly balls. Science, 268(5210):569–573, 1995."
REFERENCES,0.37750385208012327,"[62] P. Ménard, O. D. Domingues, A. Jonsson, E. Kaufmann, E. Leurent, and M. Valko. Fast
active learning for pure exploration in reinforcement learning. In International Conference on
Machine Learning, pages 7599–7608. PMLR, 2021."
REFERENCES,0.3790446841294299,"[63] M. B. Mirza, R. A. Adams, C. D. Mathys, and K. J. Friston. Scene construction, visual
foraging, and active inference. Frontiers in computational neuroscience, 10:56, 2016."
REFERENCES,0.38058551617873654,"[64] S. Mittal, M. Tatarchenko, Ö. Çiçek, and T. Brox. Parting with illusions about deep active
learning. arXiv preprint arXiv:1912.05361, 2019."
REFERENCES,0.38212634822804314,"[65] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning, 2013. arXiv:1312.5602."
REFERENCES,0.38366718027734975,"[66] P. Munjal, N. Hayat, M. Hayat, J. Sourati, and S. Khan. Towards robust and reproducible active
learning using neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 223–232, 2022."
REFERENCES,0.3852080123266564,"[67] A. Narr, R. Triebel, and D. Cremers. Stream-based active learning for efficient and adap-
tive classification of 3d objects. In 2016 IEEE International Conference on Robotics and
Automation (ICRA), pages 227–233. IEEE, 2016."
REFERENCES,0.386748844375963,"[68] K. Nguyen and H. Daumé III.
Help, anna! visual navigation with natural multimodal
assistance via retrospective curiosity-encouraging imitation learning.
In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
684–695, 2019."
REFERENCES,0.3882896764252696,"[69] K. Nguyen, D. Dey, C. Brockett, and B. Dolan. Vision-based navigation with language-based
assistance via imitation learning with indirect intervention. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 12527–12537, 2019."
REFERENCES,0.3898305084745763,"[70] D. Nilsson, A. Pirinen, E. Gärtner, and C. Sminchisescu. Embodied visual active learning
for semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 2373–2383, 2021."
REFERENCES,0.3913713405238829,"[71] J. K. O’regan and A. Noë. A sensorimotor account of vision and visual consciousness.
Behavioral and brain sciences, 24(5):939–973, 2001."
REFERENCES,0.3929121725731895,"[72] J. Piaget. Piaget’s theory, 1976."
REFERENCES,0.39445300462249616,"[73] R. Pinsler, J. Gordon, E. Nalisnick, and J. M. Hernández-Lobato. Bayesian batch active
learning as sparse subset approximation. Advances in neural information processing systems,
32, 2019."
REFERENCES,0.39599383667180277,"[74] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer. D-nerf: Neural radiance fields
for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 10318–10327, 2021."
REFERENCES,0.3975346687211094,"[75] A. Rangel and J. A. Clithero. Value normalization in decision making: theory and evidence.
Current opinion in neurobiology, 22(6):970–981, 2012."
REFERENCES,0.39907550077041604,"[76] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error
propagation. In J. W. Shavlik and T. G. Dietterich, editors, Readings in Machine Learning,
pages 115–137, San Mateo, CA, 1990. Kaufmann."
REFERENCES,0.40061633281972264,"[77] M. S. Ryoo, T. J. Fuchs, L. Xia, J. K. Aggarwal, and L. Matthies. Robot-centric activity
prediction from first-person videos: What will they do to me? In Proceedings of the tenth
annual ACM/IEEE international conference on human-robot interaction, pages 295–302,
2015."
REFERENCES,0.40215716486902925,"[78] M. S. Ryoo, K. Gopalakrishnan, K. Kahatapitiya, T. Xiao, K. Rao, A. Stone, Y. Lu, J. Ibarz,
and A. Arnab. Token turing machines. arXiv preprint arXiv:2211.09119, 2022."
REFERENCES,0.4036979969183359,"[79] S. Salter, D. Rao, M. Wulfmeier, R. Hadsell, and I. Posner. Attention-privileged reinforcement
learning. In Conference on Robot Learning, pages 394–408. PMLR, 2021."
REFERENCES,0.4052388289676425,"[80] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun,
J. Malik, D. Parikh, and D. Batra. Habitat: A platform for embodied ai research. In 2019
IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Oct 2019."
REFERENCES,0.4067796610169492,"[81] O. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017."
REFERENCES,0.4083204930662558,"[82] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain. Time-
contrastive networks: Self-supervised learning from video. In IEEE International Conference
on Robotics and Automation (ICRA), pages 1134–1141. IEEE, 2018."
REFERENCES,0.4098613251155624,"[83] J. Shang and M. S. Ryoo. Self-supervised disentangled representation learning for third-person
imitation learning. In 2021 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pages 214–221, 2021."
REFERENCES,0.41140215716486905,"[84] J. Shang, S. Das, and M. Ryoo.
Learning viewpoint-agnostic visual representations by
recovering tokens in 3d space. Advances in Neural Information Processing Systems, 35:
31031–31044, 2022."
REFERENCES,0.41294298921417566,"[85] J. Shang, K. Kahatapitiya, X. Li, and M. S. Ryoo. Starformer: Transformer with state-action-
reward representations for visual reinforcement learning. In Proceedings of the European
Conference on Computer Vision (ECCV), 2022."
REFERENCES,0.41448382126348227,"[86] J. Shang, X. Li, K. Kahatapitiya, Y.-C. Lee, and M. S. Ryoo. Starformer: Transformer with
state-action-reward representations for robot learning. IEEE transactions on pattern analysis
and machine intelligence, 2022."
REFERENCES,0.41602465331278893,"[87] J. Shields, O. Pizarro, and S. B. Williams. Towards adaptive benthic habitat mapping. In 2020
IEEE International Conference on Robotics and Automation (ICRA), pages 9263–9270. IEEE,
2020."
REFERENCES,0.41756548536209553,"[88] C. Shui, F. Zhou, C. Gagné, and B. Wang. Deep active learning: Unified and principled method
for query and training. In International Conference on Artificial Intelligence and Statistics,
pages 1308–1318. PMLR, 2020."
REFERENCES,0.41910631741140214,"[89] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforce-
ment learning algorithm. arXiv preprint arXiv:1712.01815, 2017."
REFERENCES,0.4206471494607088,"[90] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 5972–5981, 2019."
REFERENCES,0.4221879815100154,"[91] I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov, and A. Ignateva. Deep attention recurrent
q-network, 2015."
REFERENCES,0.423728813559322,"[92] B. C. Stadie, P. Abbeel, and I. Sutskever. Third-person imitation learning. arXiv preprint
arXiv:1703.01703, 2017."
REFERENCES,0.4252696456086287,"[93] Y. Tang, D. Nguyen, and D. Ha. Neuroevolution of self-interpretable agents. In Proceedings
of the 2020 Genetic and Evolutionary Computation Conference, pages 414–424, 2020."
REFERENCES,0.4268104776579353,"[94] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer. Vision-and-dialog navigation. In
Conference on Robot Learning, pages 394–406. PMLR, 2020."
REFERENCES,0.4283513097072419,"[95] F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954, 2018."
REFERENCES,0.42989214175654855,"[96] A. Tucker, A. Gleave, and S. Russell. Inverse reinforcement learning for video games. arXiv
preprint arXiv:1810.10593, 2018."
REFERENCES,0.43143297380585516,"[97] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap,
N. Heess, and Y. Tassa. dm_control: Software and tasks for continuous control. Software
Impacts, 6:100022, 2020. ISSN 2665-9638."
REFERENCES,0.43297380585516176,"[98] T. Van de Maele, T. Verbelen, O. Çatal, and B. Dhoedt. Embodied object representation
learning and recognition. Frontiers in Neurorobotics, 16:840658, 2022."
REFERENCES,0.4345146379044684,"[99] H. P. van Hasselt, A. Guez, M. Hessel, V. Mnih, and D. Silver. Learning values across many
orders of magnitude. Advances in neural information processing systems, 29, 2016."
REFERENCES,0.43605546995377503,"[100] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Rothörl, T. Lampe,
and M. Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics
problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017."
REFERENCES,0.43759630200308164,"[101] F. Wan, T. Yuan, M. Fu, X. Ji, Q. Huang, and Q. Ye. Nearest neighbor classifier embedded
network for active learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 10041–10048, 2021."
REFERENCES,0.4391371340523883,"[102] S. D. Whitehead and D. H. Ballard. Active perception and reinforcement learning. In Machine
Learning Proceedings 1990, pages 179–188. Elsevier, 1990."
REFERENCES,0.4406779661016949,"[103] D. M. Wolpert, J. Diedrichsen, and J. R. Flanagan. Principles of sensorimotor learning. Nature
reviews neuroscience, 12(12):739–751, 2011."
REFERENCES,0.4422187981510015,"[104] H. Wu, K. Khetarpal, and D. Precup. Self-supervised attention-aware reinforcement learning.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 10311–
10319, 2021."
REFERENCES,0.4437596302003082,"[105] Y. Xie, H. Lu, J. Yan, X. Yang, M. Tomizuka, and W. Zhan. Active finetuning: Exploiting
annotation budget in the pretraining-finetuning paradigm. arXiv preprint arXiv:2303.14382,
2023."
REFERENCES,0.4453004622496148,"[106] J. Yang, Z. Ren, M. Xu, X. Chen, D. J. Crandall, D. Parikh, and D. Batra. Embodied
amodal recognition: Learning to move to perceive objects. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 2040–2050, 2019."
REFERENCES,0.44684129429892144,"[107] R. Yang, G. Yang, and X. Wang. Neural volumetric memory for visual locomotion control. In
Conference on Computer Vision and Pattern Recognition, 2023."
REFERENCES,0.44838212634822805,"[108] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Improved
data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021."
REFERENCES,0.44992295839753466,"[109] T. Yuan, F. Wan, M. Fu, J. Liu, S. Xu, X. Ji, and Q. Ye. Multiple instance active learning
for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 5330–5339, 2021."
REFERENCES,0.4514637904468413,"[110] G. Zelinsky, W. Zhang, B. Yu, X. Chen, and D. Samaras. The role of top-down and bottom-up
processes in guiding eye movements during visual search. Advances in neural information
processing systems, 18, 2005."
REFERENCES,0.4530046224961479,"[111] T. Zhang, Y. Li, C. Wang, G. Xie, and Z. Lu. Fop: Factorizing optimal joint policy of maximum-
entropy multi-agent reinforcement learning. In International Conference on Machine Learning,
pages 12491–12500. PMLR, 2021."
REFERENCES,0.45454545454545453,"[112] A. Zhou, M. J. Kim, L. Wang, P. Florence, and C. Finn. Nerf in the palm of your hand: Cor-
rective augmentation for robotics via novel-view synthesis. arXiv preprint arXiv:2301.08556,
2023."
REFERENCES,0.4560862865947612,"[113] J.-J. Zhu and J. Bento. Generative adversarial active learning. arXiv preprint arXiv:1702.07956,
2017."
REFERENCES,0.4576271186440678,"[114] Y. Zhu, J. Wong, A. Mandlekar, R. Martín-Martín, A. Joshi, S. Nasiriany, and Y. Zhu. robo-
suite: A modular simulation framework and benchmark for robot learning. In arXiv preprint
arXiv:2009.12293, 2020."
REFERENCES,0.4591679506933744,"A
Appendix"
REFERENCES,0.46070878274268107,"A.1
Learning Curves"
REFERENCES,0.4622496147919877,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 250 500 750 1000 1250 1500 1750"
REFERENCES,0.4637904468412943,up_n_down
REFERENCES,0.46533127889060094,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.46687211093990755,ms_pacman
REFERENCES,0.46841294298921415,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 500 1000 1500 2000 2500 3000 3500 4000"
REFERENCES,0.4699537750385208,battle_zone
REFERENCES,0.4714946070878274,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20 25"
REFERENCES,0.47303543913713403,freeway
REFERENCES,0.4745762711864407,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.4761171032357473,bank_heist
REFERENCES,0.4776579352850539,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600"
REFERENCES,0.47919876733436056,frostbite
REFERENCES,0.48073959938366717,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.48228043143297383,assault
REFERENCES,0.48382126348228044,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 50 100 150 200 250 300"
REFERENCES,0.48536209553158705,seaquest
REFERENCES,0.4869029275808937,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 700 800 900 qbert"
REFERENCES,0.4884437596302003,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.4899845916795069,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.4915254237288136,amidar
REFERENCES,0.4930662557781202,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500"
REFERENCES,0.4946070878274268,kangaroo
REFERENCES,0.49614791987673346,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.49768875192604006,breakout
REFERENCES,0.49922958397534667,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500"
REFERENCES,0.5007704160246533,chopper_command
REFERENCES,0.50231124807396,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600 700 800"
REFERENCES,0.5038520801232665,jamesbond
REFERENCES,0.5053929121725732,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000 10000"
REFERENCES,0.5069337442218799,private_eye
REFERENCES,0.5084745762711864,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.5100154083204931,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.5115562403697997,demon_attack
REFERENCES,0.5130970724191063,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200"
REFERENCES,0.514637904468413,asterix
REFERENCES,0.5161787365177196,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.5177195685670262,road_runner
REFERENCES,0.5192604006163328,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 alien"
REFERENCES,0.5208012326656395,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 40 20 0 20 40 60"
REFERENCES,0.522342064714946,boxing
REFERENCES,0.5238828967642527,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400 1600"
REFERENCES,0.5254237288135594,gopher
REFERENCES,0.5269645608628659,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.5285053929121726,kung_fu_master
REFERENCES,0.5300462249614792,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.5315870570107858,crazy_climber
REFERENCES,0.5331278890600925,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 2500 3000 krull"
REFERENCES,0.5346687211093991,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View"
REFERENCES,0.5362095531587057,"Figure 9: Learning curves of 26 Atari games, under the setting of 50x50 foveal observation size and
20x20 peripheral observation."
REFERENCES,0.5377503852080123,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 250 500 750 1000 1250 1500 1750 2000"
REFERENCES,0.539291217257319,up_n_down
REFERENCES,0.5408320493066255,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000"
REFERENCES,0.5423728813559322,ms_pacman
REFERENCES,0.5439137134052389,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000 8000"
REFERENCES,0.5454545454545454,battle_zone
REFERENCES,0.5469953775038521,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20"
REFERENCES,0.5485362095531587,freeway
REFERENCES,0.5500770416024653,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.551617873651772,bank_heist
REFERENCES,0.5531587057010786,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.5546995377503852,frostbite
REFERENCES,0.5562403697996918,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.5577812018489985,assault
REFERENCES,0.559322033898305,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 50 100 150 200"
REFERENCES,0.5608628659476117,seaquest
REFERENCES,0.5624036979969184,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 qbert"
REFERENCES,0.5639445300462249,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.5654853620955316,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.5670261941448382,amidar
REFERENCES,0.5685670261941448,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600"
REFERENCES,0.5701078582434514,kangaroo
REFERENCES,0.5716486902927581,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.5731895223420647,breakout
REFERENCES,0.5747303543913713,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500"
REFERENCES,0.576271186440678,chopper_command
REFERENCES,0.5778120184899846,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600 700 800"
REFERENCES,0.5793528505392912,jamesbond
REFERENCES,0.5808936825885979,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.5824345146379045,private_eye
REFERENCES,0.5839753466872111,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.5855161787365177,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.5870570107858244,demon_attack
REFERENCES,0.588597842835131,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.5901386748844376,asterix
REFERENCES,0.5916795069337443,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.5932203389830508,road_runner
REFERENCES,0.5947611710323575,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 alien"
REFERENCES,0.5963020030816641,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 40 20 0 20 40 60"
REFERENCES,0.5978428351309707,boxing
REFERENCES,0.5993836671802774,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 2500"
REFERENCES,0.600924499229584,gopher
REFERENCES,0.6024653312788906,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.6040061633281972,kung_fu_master
REFERENCES,0.6055469953775039,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.6070878274268104,crazy_climber
REFERENCES,0.6086286594761171,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 2500 3000 3500 krull"
REFERENCES,0.6101694915254238,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View"
REFERENCES,0.6117103235747303,"Figure 10: Learning curves of 26 Atari games, under the setting of 30x30 foveal observation size and
20x20 peripheral observation."
REFERENCES,0.613251155624037,"A.2
Hyper-parameter Settings"
REFERENCES,0.6147919876733436,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 250 500 750 1000 1250 1500 1750"
REFERENCES,0.6163328197226502,up_n_down
REFERENCES,0.6178736517719569,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 700 800 900"
REFERENCES,0.6194144838212635,ms_pacman
REFERENCES,0.6209553158705701,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000"
REFERENCES,0.6224961479198767,battle_zone
REFERENCES,0.6240369799691834,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20"
REFERENCES,0.6255778120184899,freeway
REFERENCES,0.6271186440677966,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.6286594761171033,bank_heist
REFERENCES,0.6302003081664098,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600"
REFERENCES,0.6317411402157165,frostbite
REFERENCES,0.6332819722650231,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.6348228043143297,assault
REFERENCES,0.6363636363636364,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 25 50 75 100 125 150 175 200 225"
REFERENCES,0.637904468412943,seaquest
REFERENCES,0.6394453004622496,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 qbert"
REFERENCES,0.6409861325115562,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.6425269645608629,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.6440677966101694,amidar
REFERENCES,0.6456086286594761,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600"
REFERENCES,0.6471494607087828,kangaroo
REFERENCES,0.6486902927580893,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.650231124807396,breakout
REFERENCES,0.6517719568567026,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500"
REFERENCES,0.6533127889060092,chopper_command
REFERENCES,0.6548536209553159,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800"
REFERENCES,0.6563944530046225,jamesbond
REFERENCES,0.6579352850539292,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.6594761171032357,private_eye
REFERENCES,0.6610169491525424,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.662557781201849,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.6640986132511556,demon_attack
REFERENCES,0.6656394453004623,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 700"
REFERENCES,0.6671802773497689,asterix
REFERENCES,0.6687211093990755,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000 8000"
REFERENCES,0.6702619414483821,road_runner
REFERENCES,0.6718027734976888,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 alien"
REFERENCES,0.6733436055469953,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 40 20 0 20 40 60"
REFERENCES,0.674884437596302,boxing
REFERENCES,0.6764252696456087,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400 1600"
REFERENCES,0.6779661016949152,gopher
REFERENCES,0.6795069337442219,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.6810477657935285,kung_fu_master
REFERENCES,0.6825885978428351,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.6841294298921418,crazy_climber
REFERENCES,0.6856702619414484,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 2500 3000 3500 4000 krull"
REFERENCES,0.687211093990755,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View"
REFERENCES,0.6887519260400616,"Figure 11: Learning curves of 26 Atari games, under the setting of 20x20 foveal observation size and
20x20 peripheral observation."
REFERENCES,0.6902927580893683,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000 1200"
REFERENCES,0.6918335901386748,up_n_down
REFERENCES,0.6933744221879815,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.6949152542372882,ms_pacman
REFERENCES,0.6964560862865947,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000"
REFERENCES,0.6979969183359014,battle_zone
REFERENCES,0.699537750385208,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20"
REFERENCES,0.7010785824345146,freeway
REFERENCES,0.7026194144838213,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.7041602465331279,bank_heist
REFERENCES,0.7057010785824345,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.7072419106317411,frostbite
REFERENCES,0.7087827426810478,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.7103235747303543,assault
REFERENCES,0.711864406779661,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 25 50 75 100 125 150 175 200"
REFERENCES,0.7134052388289677,seaquest
REFERENCES,0.7149460708782742,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 700 qbert"
REFERENCES,0.7164869029275809,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.7180277349768875,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.7195685670261941,amidar
REFERENCES,0.7211093990755008,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 0 100 200 300 400 500 600 700"
REFERENCES,0.7226502311248074,kangaroo
REFERENCES,0.724191063174114,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.7257318952234206,breakout
REFERENCES,0.7272727272727273,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500"
REFERENCES,0.7288135593220338,chopper_command
REFERENCES,0.7303543913713405,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
REFERENCES,0.7318952234206472,jamesbond
REFERENCES,0.7334360554699538,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.7349768875192604,private_eye
REFERENCES,0.736517719568567,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.7380585516178737,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.7395993836671803,demon_attack
REFERENCES,0.7411402157164869,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.7426810477657936,asterix
REFERENCES,0.7442218798151001,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000 8000"
REFERENCES,0.7457627118644068,road_runner
REFERENCES,0.7473035439137135,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 alien"
REFERENCES,0.74884437596302,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 60 40 20 0 20 40 60"
REFERENCES,0.7503852080123267,boxing
REFERENCES,0.7519260400616333,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400"
REFERENCES,0.7534668721109399,gopher
REFERENCES,0.7550077041602465,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.7565485362095532,kung_fu_master
REFERENCES,0.7580893682588598,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.7596302003081664,crazy_climber
REFERENCES,0.7611710323574731,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 krull"
REFERENCES,0.7627118644067796,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View
DQN-Center
DQN A^s x A^o"
REFERENCES,0.7642526964560863,"Figure 12: Learning curves of 26 Atari games, under the setting of 50x50 foveal observation size and
w/o peripheral observation."
REFERENCES,0.765793528505393,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
REFERENCES,0.7673343605546995,up_n_down
REFERENCES,0.7688751926040062,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.7704160246533128,ms_pacman
REFERENCES,0.7719568567026194,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 500 1000 1500 2000 2500 3000 3500 4000"
REFERENCES,0.773497688751926,battle_zone
REFERENCES,0.7750385208012327,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20"
REFERENCES,0.7765793528505393,freeway
REFERENCES,0.7781201848998459,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.7796610169491526,bank_heist
REFERENCES,0.7812018489984591,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.7827426810477658,frostbite
REFERENCES,0.7842835130970724,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.785824345146379,assault
REFERENCES,0.7873651771956857,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 25 50 75 100 125 150 175 200"
REFERENCES,0.7889060092449923,seaquest
REFERENCES,0.7904468412942989,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600 qbert"
REFERENCES,0.7919876733436055,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.7935285053929122,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.7950693374422187,amidar
REFERENCES,0.7966101694915254,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000"
REFERENCES,0.7981510015408321,kangaroo
REFERENCES,0.7996918335901386,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.8012326656394453,breakout
REFERENCES,0.802773497688752,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400"
REFERENCES,0.8043143297380585,chopper_command
REFERENCES,0.8058551617873652,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600 700 800"
REFERENCES,0.8073959938366718,jamesbond
REFERENCES,0.8089368258859785,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000"
REFERENCES,0.810477657935285,private_eye
REFERENCES,0.8120184899845917,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.8135593220338984,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.8151001540832049,demon_attack
REFERENCES,0.8166409861325116,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.8181818181818182,asterix
REFERENCES,0.8197226502311248,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000 8000"
REFERENCES,0.8212634822804314,road_runner
REFERENCES,0.8228043143297381,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 alien"
REFERENCES,0.8243451463790447,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 40 20 0 20 40 60"
REFERENCES,0.8258859784283513,boxing
REFERENCES,0.827426810477658,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400"
REFERENCES,0.8289676425269645,gopher
REFERENCES,0.8305084745762712,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.8320493066255779,kung_fu_master
REFERENCES,0.8335901386748844,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.8351309707241911,crazy_climber
REFERENCES,0.8366718027734977,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400 1600 1800 krull"
REFERENCES,0.8382126348228043,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View
DQN-Center
DQN A^s x A^o"
REFERENCES,0.8397534668721109,"Figure 13: Learning curves of 26 Atari games, under the setting of 30x30 foveal observation size and
w/o peripheral observation."
REFERENCES,0.8412942989214176,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 200 400 600 800 1000 1200 1400"
REFERENCES,0.8428351309707242,up_n_down
REFERENCES,0.8443759630200308,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.8459167950693375,ms_pacman
REFERENCES,0.847457627118644,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 500 1000 1500 2000 2500 3000 3500"
REFERENCES,0.8489984591679507,battle_zone
REFERENCES,0.8505392912172574,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15 20 25"
REFERENCES,0.8520801232665639,freeway
REFERENCES,0.8536209553158706,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.8551617873651772,bank_heist
REFERENCES,0.8567026194144838,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600"
REFERENCES,0.8582434514637904,frostbite
REFERENCES,0.8597842835130971,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.8613251155624037,assault
REFERENCES,0.8628659476117103,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 25 50 75 100 125 150 175 200"
REFERENCES,0.864406779661017,seaquest
REFERENCES,0.8659476117103235,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 qbert"
REFERENCES,0.8674884437596302,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 20 15 10 5 0 5 10 15 pong"
REFERENCES,0.8690292758089369,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 20 40 60 80 100"
REFERENCES,0.8705701078582434,amidar
REFERENCES,0.8721109399075501,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 0 100 200 300 400 500"
REFERENCES,0.8736517719568567,kangaroo
REFERENCES,0.8751926040061633,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 10 20 30 40 50"
REFERENCES,0.8767334360554699,breakout
REFERENCES,0.8782742681047766,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500"
REFERENCES,0.8798151001540832,chopper_command
REFERENCES,0.8813559322033898,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 100 200 300 400 500 600 700 800"
REFERENCES,0.8828967642526965,jamesbond
REFERENCES,0.884437596302003,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000 10000 12000"
REFERENCES,0.8859784283513097,private_eye
REFERENCES,0.8875192604006163,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 hero"
REFERENCES,0.889060092449923,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800"
REFERENCES,0.8906009244992296,demon_attack
REFERENCES,0.8921417565485362,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600"
REFERENCES,0.8936825885978429,asterix
REFERENCES,0.8952234206471494,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000 8000"
REFERENCES,0.8967642526964561,road_runner
REFERENCES,0.8983050847457628,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 100 200 300 400 500 600 700 alien"
REFERENCES,0.8998459167950693,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 40 20 0 20 40 60"
REFERENCES,0.901386748844376,boxing
REFERENCES,0.9029275808936826,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 500 1000 1500 2000 2500"
REFERENCES,0.9044684129429892,gopher
REFERENCES,0.9060092449922958,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 1000 2000 3000 4000 5000 6000 7000"
REFERENCES,0.9075500770416025,kung_fu_master
REFERENCES,0.9090909090909091,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 2000 4000 6000 8000 10000 12000 14000"
REFERENCES,0.9106317411402157,crazy_climber
REFERENCES,0.9121725731895224,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 200 400 600 800 1000 1200 1400 1600 krull"
REFERENCES,0.9137134052388289,"DQN-Full
SUGARL-DQN
SUGARL w/o r^sugarl
DQN-Raster Scanning
DQN-Random View
DQN-Center
DQN A^s x A^o"
REFERENCES,0.9152542372881356,"Figure 14: Learning curves of 26 Atari games, under the setting of 20x20 foveal observation size and
w/o peripheral observation."
REFERENCES,0.9167950693374423,"0
25000 50000 75000 100000 50 100 150 200 250 300 350"
REFERENCES,0.9183359013867488,walker-walk
REFERENCES,0.9198767334360555,"0
25000 50000 75000 100000 50 100 150 200 250"
REFERENCES,0.9214175654853621,fish-swim
REFERENCES,0.9229583975346687,"0
25000 50000 75000 100000 5 10 15 20 25 30"
REFERENCES,0.9244992295839753,dog-fetch
REFERENCES,0.926040061633282,"0
25000 50000 75000 100000 50 100 150 200 250 300 350 400 450"
REFERENCES,0.9275808936825886,cartpole-swingup
REFERENCES,0.9291217257318952,"0
25000 50000 75000 100000
0 100 200 300 400 500 600 700"
REFERENCES,0.9306625577812019,ball_in_cup-catch
REFERENCES,0.9322033898305084,"0
25000 50000 75000 100000 50 100 150 200 250"
REFERENCES,0.9337442218798151,"cheetah-run
DrQ-Full
SUGARL-DrQ
DrQ-Raster Scanning
DrQ-Random View
DrQ A^s x A^o"
REFERENCES,0.9352850539291218,"Figure 15: Learning curves of 6 DMC environments, under the setting of 50x50 foveal observation
size and w/o peripheral observation."
REFERENCES,0.9368258859784283,"0
25000 50000 75000 100000 50 100 150 200 250 300"
REFERENCES,0.938366718027735,walker-walk
REFERENCES,0.9399075500770416,"0
25000 50000 75000 100000 50 75 100 125 150 175 200 225"
REFERENCES,0.9414483821263482,fish-swim
REFERENCES,0.9429892141756548,"0
25000 50000 75000 100000 2 4 6 8 10 12 14"
REFERENCES,0.9445300462249615,dog-fetch
REFERENCES,0.9460708782742681,"0
25000 50000 75000 100000 50 100 150 200 250 300 350 400 450"
REFERENCES,0.9476117103235747,cartpole-swingup
REFERENCES,0.9491525423728814,"0
25000 50000 75000 100000
0 100 200 300 400 500 600 700"
REFERENCES,0.9506933744221879,ball_in_cup-catch
REFERENCES,0.9522342064714946,"0
25000 50000 75000 100000 50 100 150 200 250"
REFERENCES,0.9537750385208013,"cheetah-run
DrQ-Full
SUGARL-DrQ
DrQ-Raster Scanning
DrQ-Random View
DrQ A^s x A^o"
REFERENCES,0.9553158705701078,"Figure 16: Learning curves of 6 DMC environments, under the setting of 30x30 foveal observation
size and w/o peripheral observation."
REFERENCES,0.9568567026194145,"0
25000 50000 75000 100000 50 100 150 200 250 300"
REFERENCES,0.9583975346687211,walker-walk
REFERENCES,0.9599383667180277,"0
25000 50000 75000 100000 50 75 100 125 150 175 200 225"
REFERENCES,0.9614791987673343,fish-swim
REFERENCES,0.963020030816641,"0
25000 50000 75000 100000 2 4 6 8 10 12"
REFERENCES,0.9645608628659477,dog-fetch
REFERENCES,0.9661016949152542,"0
25000 50000 75000 100000 50 100 150 200 250 300 350 400 450"
REFERENCES,0.9676425269645609,cartpole-swingup
REFERENCES,0.9691833590138675,"0
25000 50000 75000 100000
0 100 200 300 400 500 600 700"
REFERENCES,0.9707241910631741,ball_in_cup-catch
REFERENCES,0.9722650231124808,"0
25000 50000 75000 100000 50 100 150 200 250"
REFERENCES,0.9738058551617874,"cheetah-run
DrQ-Full
SUGARL-DrQ
DrQ-Raster Scanning
DrQ-Random View
DrQ A^s x A^o"
REFERENCES,0.975346687211094,"Figure 17: Learning curves of 6 DMC environments, under the setting of 20x20 foveal observation
size and w/o peripheral observation."
REFERENCES,0.9768875192604006,Table 6: Hyper-parameters for DQN / SUGARL-DQN (on Atari)
REFERENCES,0.9784283513097073,"Total steps
1,000,000 or 5,000,000
Replay buffer size
100,000
ϵ start
1.0
ϵ end
0.01
min ϵ step
100,000
γ
0.99
Learning start
80,000
Q network train frequency
4
Target network update frequency
1,000
Learning rate
10−4
Batch size
32
Self-understanding module train frequency
4
Self-understanding module learning rate
10−4"
REFERENCES,0.9799691833590138,Table 7: Hyper-parameters for SAC (on Atari)
REFERENCES,0.9815100154083205,"Total steps
1,000,000
Replay buffer size
100,000
γ
0.99
Learning start
80,000
Actor train frequency
4
Critic train frequency
4
Target network update frequency
8,000
Actor Learning rate
3 × 10−4"
REFERENCES,0.9830508474576272,"Critic Learning rate
3 × 10−4
Batch size
64
Self-understanding module train frequency
4
Self-understanding module learning rate
3 × 10−4
Visual policy alpha
0.2
Physical policy alpha
autotune
Physical policy target entropy scale
0.2"
REFERENCES,0.9845916795069337,Table 8: Hyper-parameters for DrQv2 (on DMC)
REFERENCES,0.9861325115562404,"Total steps
100,000
Replay buffer size
100,000
γ
0.99
Standard deviation start
1.0
Standard deviation end
0.1
Standard deviation end step
50,000
Standard deviation clip
0.3
Learning start
2,000
Actor train frequency
2
Critic train frequency
2
Target network update frequency
2
Target network exponential moving average weight
0.01
Actor Learning rate
10−4"
REFERENCES,0.987673343605547,"Critic Learning rate
10−4
Batch size
256
Self-understanding module train frequency
2
Self-understanding module learning rate
10−4
Multiple-step reward
3"
REFERENCES,0.9892141756548536,Table 9: Hyper-parameters for DrQv2 (on Robosuite)
REFERENCES,0.9907550077041603,"Total steps
100,000
Replay buffer size
100,000
γ
0.99
Standard deviation start
1.0
Standard deviation end
0.1
Standard deviation end step
50,000
Standard deviation clip
0.3
Learning start
8,000
Actor train frequency
2
Critic train frequency
2
Target network update frequency
2
Target network exponential moving average weight
0.01
Actor Learning rate
10−4"
REFERENCES,0.9922958397534669,"Critic Learning rate
10−4
Batch size
256
Self-understanding module train frequency
2
Self-understanding module learning rate
10−4
Multiple-step reward
3"
REFERENCES,0.9938366718027735,Table 10: Environment Settings
REFERENCES,0.9953775038520801,"Atari
Gray-scale
True
Full observation size
84x84
Frame stacking
4
Action repeat (frame skipping) 4
Observable area initial location
(0, 0)
Sensory action options
4 × 4 grid
Sensory action space size
16 (abs) or 5 (rel)
PVM number of steps
3"
REFERENCES,0.9969183359013868,"DMC
Gray-scale
True
Full observation size
84x84
Frame stacking
3
Action repeat (frame skipping)
2
Observable area initial location
(0, 0)
Sensory action options
4 × 4 grid
Sensory action space size
5 (rel)
PVM number of steps
3"
REFERENCES,0.9984591679506933,"Robosuite
Gray-scale
False
Full observation size
84x84
Frame stacking
3
Action repeat (frame skipping)
2
Observable area initial location
Side View
Sensory action space
continuous relative 5-DoF control
PVM number of steps
3"
