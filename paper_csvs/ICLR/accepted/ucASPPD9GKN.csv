Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0015174506828528073,"Graph neural networks (GNNs) have shown great prowess in learning representa-
tions suitable for numerous graph-based machine learning tasks. When applied to
semi-supervised node classiﬁcation, GNNs are widely believed to work well due
to the homophily assumption (“like attracts like”), and fail to generalize to het-
erophilous graphs where dissimilar nodes connect. Recent works have designed
new architectures to overcome such heterophily-related limitations. However, we
empirically ﬁnd that standard graph convolutional networks (GCNs) can actually
achieve strong performance on some commonly used heterophilous graphs. This
motivates us to reconsider whether homophily is truly necessary for good GNN
performance. We ﬁnd that this claim is not quite accurate, and certain types of
“good” heterophily exist, under which GCNs can achieve strong performance.
Our work carefully characterizes the implications of different heterophily condi-
tions, and provides supporting theoretical understanding and empirical observa-
tions. Finally, we examine existing heterophilous graphs benchmarks and recon-
cile how the GCN (under)performs on them based on this understanding."
INTRODUCTION,0.0030349013657056147,"1
INTRODUCTION 0 0 0 0 1 1 1 1"
INTRODUCTION,0.004552352048558422,"Figure 1: A het-
erophilous graph
on which GCN
achieves perfect
performance."
INTRODUCTION,0.006069802731411229,"Graph neural networks (GNNs) are a prominent approach for learning represen-
tations for graph structured data. Thanks to their great capacity in jointly lever-
aging attribute and graph structure information, they have been widely adopted
to promote improvements for numerous graph-related learning tasks (Kipf and
Welling, 2016; Hamilton et al., 2017; Ying et al., 2018; Fan et al., 2019; Zit-
nik et al., 2018), especially centered around node representation learning and
semi-supervised node classiﬁcation (SSNC). GNNs learn node representations
by a recursive neighborhood aggregation process, where each node aggregates
and transforms features from its neighbors. The node representations can then
be utilized for downstream node classiﬁcation or regression tasks. Due to this
neighborhood aggregation mechanism, several existing works posit that many
GNNs implicitly assume strong homophily and homophily is critical for GNNs
to achieve strong performance on SSNC (Zhu et al., 2020b;a; Chien et al., 2021;
Maurya et al., 2021; Halcrow et al., 2020; Lim et al., 2021). In general, homophily describes the
phenomenon that nodes tend to connect with “similar” or “alike” others. Homophily is observed
in a wide range of real-world graphs including friendship networks (McPherson et al., 2001), po-
litical networks (Gerber et al., 2013; Newman, 2018), citation networks (Ciotti et al., 2016) and
more. Under the homophily assumption, through the aggregation process, a node’s representation
is “smoothed” via its neighbors’ representations, since each node is able to receive additional infor-
mation from neighboring nodes, which are likely to share the same label. Several recent works (Zhu
et al., 2020b;a) claim that GNNs are implicitly (or explicitly) designed with homophily in mind, are
not suitable for graphs exhibiting heterophily, where connected nodes are prone to have different
properties or labels, e.g dating networks or molecular networks (Zhu et al., 2020b). Such works
accordingly design and modify new architectures and demonstrate outperformance over other GNN
models on several heterophilous graphs."
INTRODUCTION,0.007587253414264037,Published as a conference paper at ICLR 2022
INTRODUCTION,0.009104704097116844,"Present work. In our work, we empirically ﬁnd that the graph convolutional network (GCN) Kipf
and Welling (2016), a fundamental, representative GNN model (which we focus on in this work)
is actually able to outperform such heterophily-speciﬁc models on some heterophilous graphs after
careful hyperparameter tuning. This motivates us to reconsider the popular notion in the literature
that GNNs exhibit a homophilous inductive bias, and more speciﬁcally that strong homophily is cru-
cial to strong GNN performance. Counter to this idea, we ﬁnd that GCN model has the potential to
work well for heterophilous graphs under suitable conditions. We demonstrate intuition with the fol-
lowing toy example: Consider the perfectly heterophilous graph (with all inter-class edges) shown in
Figure 1, where the color indicates the node label. Blue-labeled and orange-labeled nodes are asso-
ciated with the scalar feature 0 and 1, respectively. If we consider a single-layer GCN by performing
an averaging feature aggregation over all neighboring nodes, it is clear that all blue nodes will have
a representation of 1, while the orange nodes will have that of 0. Additional layers/aggregations
will continue to alternate the features between the two types of nodes. Regardless of the number of
layers, the two classes can still be perfectly separated. In this toy example, each blue (orange) node
only connects orange (blue) nodes, and all blue (orange) nodes share similar neighborhood patterns
in terms of their neighbors’ label/feature distributions."
INTRODUCTION,0.010622154779969651,"Our work elucidates this intuition and extends it to a more general case: put simply, given a (ho-
mophilous or heterophilous) graph, GCN has the potential to achieve good performance if nodes
with the same label share similar neighborhood patterns. We theoretically support this argument
by investigating the learned node embeddings from the GCN model. We ﬁnd that homophilous
graphs always satisfy such assumptions, which explains why GCN typically works well for them.
On other hand, there exist both “good” and “bad” heterophily, and GCNs can actually achieve strong
performance for “good” heterophily settings while they usually fail on “bad” heterophily settings.
Our work characterizes these settings, and provides a new perspective and solid step towards deeper
understanding for heterophilous graphs. In short:"
INTRODUCTION,0.012139605462822459,"Our contributions. (1) We reveal that strong homophily is not a necessary assumption for the GCN
model. The GCN model can perform well over some heterophilous graphs under certain condi-
tions. (2) We carefully characterize these conditions and provide theoretical understandings on how
GCNs can achieve good SSNC performance under these conditions by investigating their embed-
ding learning process. (3) We carefully investigate commonly used homophilous and heterophilous
benchmarks and reason about GCN’s performs on them utilizing our theoretical understanding."
PRELIMINARIES,0.013657056145675266,"2
PRELIMINARIES"
PRELIMINARIES,0.015174506828528073,"Let G = {V, E} denote a graph, where V and E are the sets of nodes and edges, respectively. The
graph connection information can also be represented as an adjacency matrix A ∈{0, 1}|V|×|V|,
where |V| is the number of nodes in the graph. The i, j-th element of the adjacency matrix A[i, j] is
equal to 1 if and only if nodes i and j are adjacent to each other, otherwise A[i, j] = 0. Each node
i is associated with a l-dimensional vector of node features xi ∈Rl; the features for all nodes can
be summarized as a matrix X ∈R|V|×l. Furthermore, each node i is associated with a label yi ∈C,
where C denotes the set of labels. We also denote the set of nodes with a given label c ∈C as Vc.
We assume that labels are only given for a subset of nodes Vlabel ⊂V. The goal of semi-supervised
node classiﬁcation (SSNC) is to learn a mapping f : V →C utilizing the graph G, the node features
X and the labels for nodes in Vlabel."
HOMOPHILY IN GRAPHS,0.01669195751138088,"2.1
HOMOPHILY IN GRAPHS
In this work, we focus on investigating performance in the context of graph homophily and het-
erophily properties. Homophily in graphs is typically deﬁned based on similarity between con-
nected node pairs, where two nodes are considered similar if they share the same node label. The
homophily ratio is deﬁned based on this intuition following Zhu et al. (2020b)."
HOMOPHILY IN GRAPHS,0.018209408194233688,"Deﬁnition 1 (Homophily). Given a graph G = {V, E} and node label vector y, the edge homophily
ratio is deﬁned as the fraction of edges that connect nodes with the same labels. Formally, we have:"
HOMOPHILY IN GRAPHS,0.019726858877086494,"h(G, {yi; i ∈V}) = 1 |E| X"
HOMOPHILY IN GRAPHS,0.021244309559939303,"(j,k)∈E
1(yj = yk),
(1)"
HOMOPHILY IN GRAPHS,0.02276176024279211,where |E| is the number of edges in the graph and 1(·) is the indicator function.
HOMOPHILY IN GRAPHS,0.024279210925644917,"A graph is typically considered to be highly homophilous when h(·) is large (typically, 0.5 ≤h(·) ≤
1), given suitable label context. On the other hand, a graph with a low edge homophily ratio is
considered to be heterophilous. In future discourse, we write h(·) as h when discussing given a
ﬁxed graph and label context."
HOMOPHILY IN GRAPHS,0.025796661608497723,Published as a conference paper at ICLR 2022
"GRAPH NEURAL NETWORKS
GRAPH NEURAL NETWORKS LEARN NODE REPRESENTATIONS BY AGGREGATING AND TRANSFORMING INFORMATION OVER",0.027314112291350532,"2.2
GRAPH NEURAL NETWORKS
Graph neural networks learn node representations by aggregating and transforming information over
the graph structure. There are different designs and architectures for the aggregation and transforma-
tion, which leads to different graph neural network models (Scarselli et al., 2008; Kipf and Welling,
2016; Hamilton et al., 2017; Veliˇckovi´c et al., 2017; Gilmer et al., 2017; Zhou et al., 2020)."
"GRAPH NEURAL NETWORKS
GRAPH NEURAL NETWORKS LEARN NODE REPRESENTATIONS BY AGGREGATING AND TRANSFORMING INFORMATION OVER",0.028831562974203338,"One of the most popular and widely adopted GNN models is the graph convolutional network
(GCN). A single GCN operation takes the following form H′ = D−1AHW, where H and H′"
"GRAPH NEURAL NETWORKS
GRAPH NEURAL NETWORKS LEARN NODE REPRESENTATIONS BY AGGREGATING AND TRANSFORMING INFORMATION OVER",0.030349013657056147,"denote the input and output features of layer, W(k) ∈Rl×l is a parameter matrix to transform the
features, and D is a diagonal matrix and D[i, i] = deg(i) with deg(i) denoting the degree of node
i. From a local perspective for node i, the process can be written as a feature averaging process
hi =
1
deg(i)
P"
"GRAPH NEURAL NETWORKS
GRAPH NEURAL NETWORKS LEARN NODE REPRESENTATIONS BY AGGREGATING AND TRANSFORMING INFORMATION OVER",0.03186646433990895,"j∈N(i) Wxj, where N(i) denotes the neighbors of node i. The neighborhood N(i)
may contain the node i itself. Usually, when building GCN model upon GCN operations, nonlinear
activation functions are added between consecutive GCN operations."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.03338391502276176,"3
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.03490136570561457,"Considerable prior literature posits that graph neural networks (such as GCN) work by assuming
and exploiting homophily assumptions in the underlying graph (Maurya et al., 2021; Halcrow et al.,
2020; Wu et al., 2018). To this end, researchers have determined that such models are considered
to be ill-suited for heterophilous graphs, where the homophily ratio is low (Zhu et al., 2020b;a;
Chien et al., 2021). To deal with this limitation, researchers proposed several methods including
H2GNN (Zhu et al., 2020b), CPGNN (Zhu et al., 2020a) and GPRGNN (Chien et al., 2021), which
are explicitly designed to handle heterophilous graphs via architectural choices (e.g. adding skip-
connections, carefully choosing aggregators, etc.)
Table 1: SSNC accuracy on two het-
erophilous datasets."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.036418816388467376,"Chameleon
Squirrel
Method
(h = 0.23)
(h = 0.22)"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.03793626707132018,"GCN
67.96 ± 1.82 54.47 ± 1.17
H2GCN-1
57.11 ± 1.58 36.42 ± 1.89
H2GCN-2
59.39 ± 1.98 37.90 ± 2.02
CPGNN-MLP
54.53 ± 2.37 29.13 ± 1.57
CPGNN-Cheby 65.17 ± 3.17 29.25 ± 4.17
GPRGNN
66.31 ± 2.05 50.56 ± 1.51
MLP
48.11 ± 2.23 31.68 ± 1.90"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.03945371775417299,"In this section, we revisit the claim that GCNs have
fundamental homophily assumptions and are not suited
for heterophilous graphs.
To this end, we ﬁrst ob-
serve empirically that the GCN model achieves fairly
good performance on some of the commonly used het-
erophilous graphs; speciﬁcally, we present SSNC per-
formance on two commonly used heterophilous graph
datasets, Chameleon and Squirrel in Table 1 (see
Appendix D for further details about the datasets and
models). Both Chameleon and Squirrel are highly
heterophilous (h≈0.2). We ﬁnd that with some hyperparameter tuning, GCN can outperform al-
ternative methods uniquely designed to operate on some certain heterophilous graphs. This obser-
vation suggests that GCN does not always “underperform” on heterophilous graphs, and it leads
us to reconsider the prevalent assumption in literature. Hence, we next examine how GCNs learn
representations, and how this information is used in downstream SSNC tasks."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.0409711684370258,"3.1
WHEN DOES GCN LEARN SIMILAR EMBEDDINGS FOR NODES WITH THE SAME LABEL?
GCN is considered to be unable to tackle heterophilous graphs due to its feature averaging pro-
cess (Zhu et al., 2020b; Chien et al., 2021). Namely, a node’s newly aggregated features are con-
sidered “corrupted” by those neighbors that do not share the same label, leading to the intuition
that GCN embeddings are noisy and un-ideal for SSNC. However, we ﬁnd that crucially, for some
heterophilous graphs, the features of nodes with the same label are “corrupted in the same way.”
Hence, the obtained embeddings still contain informative characteristics and thus facilitate SSNC.
We next illustrate when GCN learns similar embeddings for nodes with the same label, beginning
with a toy example and generalizing to more practical cases.
a
b"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.042488619119878605,"Figure 2: Two nodes share the same
neighborhood distribution; GCN learns
equivalent embeddings for a and b."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.04400606980273141,"GCNs have been shown to be able to capture the local
graph topological and structural information (Xu et al.,
2019; Morris et al., 2019). Speciﬁcally, the aggregation
step in the GCN model is able to capture and discriminate
neighborhood distribution information, e.g. the mean of
the neighborhood features (Xu et al., 2019). Let us con-
sider the two nodes a and b shown in Figure 2, where we use color to indicate the label of each
node. If we further assume that all nodes sharing the same label are associated with exactly the
same features, then clearly, after 1-step aggregation, the GCN operation will output exactly the
same embedding for nodes a and b. Accordingly, Xu et al. (2019) reasons that the GCN model lacks
expressiveness due to its inability to differentiate the two nodes in the embedding space. However, in"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.04552352048558422,Published as a conference paper at ICLR 2022
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.04704097116843703,"the SSNC task, mapping a and b to the same location in the embedding space is explicitly desirable.
Intuitively, if all nodes with the same label are mapped to the same embedding and embeddings for
different labels are distinct, SSNC is effortless (Zhao et al., 2020)."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.048558421851289835,"Such assumptions are hard to meet in practice. Thus, to consider a more practical scenario, we
assume that both features and neighborhood patterns for nodes with a certain label are sampled from
some ﬁxed distributions. Under these conditions, same-label nodes may not share ﬁxed embeddings,
but we can aim to characterize their closeness. Intuitively, if the learned embeddings for same-label
nodes are close and embeddings for other-label nodes are far, we expect strong SSNC performance
to be good, given class separability (low intra-class variance and high inter-class variance) (Fisher,
1936). We prove that, for graphs meeting suitable conditions the distance between GCN-learned
embeddings of any same-label node pair is bounded by a small quantity with high probability."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.05007587253414264,"Assumptions on Graphs. We consider a graph G, where each node i has features xi ∈Rl and
label yi. We assume that (1) The features of node i are sampled from feature distribution Fyi, i.e,
xi ∼Fyi, with µ(Fyi) denoting its mean; (2) Dimensions of xi are independent to each other; (3)
The features in X are bounded by a positive scalar B, i.e, maxi,j |X[i, j]| ≤B; (4) For node i,
its neighbor’s labels are independently sampled from neighbor distribution Dyi. The sampling is
repeated for deg(i) times to sample the labels for deg(i) neighbors."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.051593323216995446,"We denote a graph following these assumptions (1)-(4) as G = {V, E, {Fc, c ∈C}, {Dc, c ∈C}}.
Note that we use the subscripts in Fyi and Dyi to indicate that these two distributions are shared
by all nodes with the same label as node i. Next, we analyze the embeddings obtained after a GCN
operation. Following previous works (Li et al., 2018; Chen et al., 2020; Baranwal et al., 2021), we
drop the non-linearity in the analysis.
Theorem 1. Consider a graph G = {V, E, {Fc, c ∈C}, {Dc, c ∈C}}, which follows Assump-
tions (1)-(4). For any node i ∈V, the expectation of the pre-activation output of a single GCN
operation is given by
E[hi] = W
 
Ec∼Dyi ,x∼Fc[x]

.
(2)
and for any t > 0, the probability that the distance between the observation hi and its expectation
is larger than t is bounded by"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.05311077389984825,"P (∥hi −E[hi]∥2 ≥t) ≤2 · l · exp

−
deg(i)t2"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.054628224582701064,2ρ2(W)B2l
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.05614567526555387,"
,
(3)"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.057663125948406675,where l denotes the feature dimensionality and ρ(W) denotes the largest singular value of W.
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.05918057663125948,"The detailed proof can be found in Appendix A. Theorem 1 demonstrates two key ideas. First, in
expectation, all nodes with the same label have the same embedding (Eq. (2)). Second, the distance
between the output embedding of a node and its expectation is small with a high probability. Specif-
ically, this probability is related to the node degree and higher degree nodes have higher probability
to be close to the expectation. Together, these results show that the GCN model is able to map nodes
with the same label to an area centered around the expectation in the embedding space under given
assumptions. Then, the downstream classiﬁer in the GCN model is able to assign these nodes to
the same class with high probability. To ensure that the classiﬁer achieves strong performance, the
centers (or the expectations) of different classes must be distant from each other; if we assume that
µ(Fyi) are distinct from each other (as is common), then the neighbor distributions {Dc, c ∈C}
must be distinguishable to ensure good SSNC performance. Based on these understandings and
discussions, we have the following key (informal) observations on GCN’s performance for graphs
with homophily and heterophily.
Observation 1 (GCN under Homophily). In homophilous graphs, the neighborhood distribution of
nodes with the same label (w.l.o.g c) can be approximately regarded as a highly skewed discrete Dc,
with most of the mass concentrated on the category c. Thus, different labels clearly have distinct
distributions. Hence, the GCN model typically in SSNC on such graph, with high degree nodes
beneﬁting more, which is consistent with previous work (Tang et al., 2020b).
Observation 2 (GCN under Heterophily). In heterophilous graphs, if the neighborhood distribution
of nodes with the same label (w.l.o.g. c) is (approximately) sampled from a ﬁxed distribution Dc,
and different labels have distinguishable distributions, then GCN can excel at SSNC, especially
when node degrees are large. Otherwise, GCNs may fail for heterophilous graphs."
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.06069802731411229,"Notably, our ﬁndings illustrate that disruptions of certain conditions inhibit GCN performance on
heterophilous graphs, but heterophily is not a sufﬁcient condition for poor GCN performance. GCNs
are able to achieve reasonable performance for both homophilous and heterophilous graphs if they
follow certain assumptions as discussed in the two observations. In Section 3.2, we theoretically"
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.0622154779969651,Published as a conference paper at ICLR 2022
GRAPH CONVOLUTIONAL NETWORKS UNDER HETEROPHILY,0.0637329286798179,"demonstrate these observations for graphs sampled from the Contextual Stochastic Block Model
(CSBM) (Deshpande et al., 2018) with two classes, whose distinguishablilty of neighborhood dis-
tributions can be explicitly characterized. Furthermore, in Section 3.3, we empirically demonstrate
these observations on graphs with multiple classes. We note that although our derivations are for
GCN, a similar line of analysis can be used for more general message-passing neural networks."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.06525037936267071,"3.2
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES
The CSBM model.
To clearly control assumptions, we study the contextual stochastic block
model(CSBM), a generative model for random graphs; such models have been previously adopted
for benchmarking graph clustering (Fortunato and Hric, 2016) and GNNs (Tsitsulin et al., 2021).
Speciﬁcally, we consider a CSBM model consisting of two classes c1 and c2. In this case, the nodes
in the generated graphs consist of two disjoint sets C1 and C2 corresponding to the two classes,
respectively. Edges are generated according to an intra-class probability p and an inter-class prob-
ability q. Speciﬁcally, any two nodes in the graph, are connected by an edge with probability p,
if they are from the same class, otherwise, the probability is q. For each node i, its initial fea-
tures xi ∈Rl are sampled from a Gaussian distribution xi ∼N(µ, I), where µ = µk ∈Rl for
i ∈Ck with k ∈{1, 2} and µ1 ̸= µ2. We denote a graph generated from such an CSBM model as
G ∼CSBM(µ1, µ2, p, q). We denote the features for node i obtained after a GCN operation as hi."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.06676783004552352,"Linear separability under GCN. To better evaluate the effectiveness of GCN operation, we study
the linear classiﬁers with the largest margin based on {xi, i ∈V} and {hi, i ∈V} and compare
their performance. Since the analysis is based on linear classiﬁers, we do not consider the linear
transformation in the GCN operation as it can be absorbed in the linear model, i.e, we only consider
the process hi =
1
deg(i)
P"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.06828528072837632,"j∈N(i) xj. For a graph G ∼CSBM(µ1, µ2, p, q), we can approximately
regard that for each node i, its neighbor’s labels are independently sampled from a neighborhood
distribution Dyi, where yi denotes the label of node i. Speciﬁcally, the neighborhood distributions
corresponding to c1 and c2 are Dc1 = [
p
p+q,
q
p+q] and Dc2 = [
q
p+q,
p
p+q], respectively."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.06980273141122914,"Based on the neighborhood distributions, the features obtained from GCN operation follow Gaussian
distributions: hi ∼N"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07132018209408195,pµ1 + qµ2
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07283763277693475,"p + q
,
I
p"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07435508345978756,deg(i) !
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07587253414264036,", for i ∈C1; and hi ∼N"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07738998482549317,qµ1 + pµ2
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.07890743550834597,"p + q
,
I
p"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.08042488619119878,deg(i) !
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.0819423368740516,", for i ∈C2.
(4)"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.0834597875569044,"Based on the properties of Gaussian distributions, it is easy to see that Theorem 1 holds. We denote
the expectation of the original features for nodes in the two classes as Ec1[xi] and Ec2[xi]. Similarly,
we denote the expectation of the features obtained from GCN operation as Ec1[hi] and Ec2[hi]. The
following proposition describes their relations.
Proposition 1. (Ec1[xi], Ec2[xi]) and (Ec1[hi], Ec2[hi]) share the same middle point. Ec1[xi] −
Ec2[xi] and Ec1[hi] −Ec2[hi] share the same direction. Speciﬁcally, the middle point m and the
shared direction w are as follows: m = (µ1 + µ2)/2, and w = (µ1 −µ2)/∥µ1 −µ2∥2.
This proposition follows from direct calculations. Given that the feature distributions of these two
classes are systematic to each other (for both xi and hi), the hyperplane that is orthogonal to w
and goes through m deﬁnes the decision boundary of the optimal linear classiﬁer for both types of
features. We denote this decision boundary as P = {x|w⊤x −w⊤(µ1 + µ2)/2}."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.08497723823975721,"Next, to evaluate how GCN operation affects the classiﬁcation performance, we compare the proba-
bility that this linear classiﬁer misclassiﬁes a certain node based on the features before and after the
GCN operation. We summarize the results in the following theorem.
Theorem 2. Consider a graph G ∼CSBM(µ1, µ2, p, q). For any node i in this graph, the linear
classiﬁer deﬁned by the decision boundary P has a lower probability to misclassify hi than xi when
deg(i) > (p + q)2/(p −q)2."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.08649468892261002,"The detailed proof can be found in Appendix B. Note that the Euclidean distance between the two
discrete neighborhood distributions Dc0 and Dc1 is
√"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.08801213960546282,2 |p−q|
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.08952959028831563,"(p+q). Hence, Theorem 2 demonstrates
that the node degree deg(i) and the distinguishability (measured by the Euclidean distance) of the
neighborhood distributions both affect GCN’s performance. Speciﬁcally, we can make the following
conclusions: (1) When p and q are ﬁxed, the GCN operation is more likely to improve the linear
separability of the high-degree nodes than low-degree nodes, which is consistent with observations
in (Tang et al., 2020b). (2) The more distinguishable the neighborhood distributions are (or the
larger the Euclidean distance is), the more nodes can be beneﬁted from the GCN operation. For"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09104704097116843,Published as a conference paper at ICLR 2022
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09256449165402124,0.81 0.74 0.68 0.63 0.59 0.52 0.46 0.42 0.38 0.32 0.28 0.25
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09408194233687406,Homophily Ratio 0.4 0.5 0.6 0.7 0.8
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09559939301972686,Test Accuracy(%) =1 =0.8 =0.6 =0.4 = 0.2 = 0 MLP
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09711684370257967,(a) Synthetic graphs generated from Cora
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.09863429438543247,"0.74 0.65 0.58 0.53 0.48 0.41 0.36 0.32 0.28 0.24
0.2
0.18
Homophily Ratio 0.3 0.4 0.5 0.6 0.7 0.8 0.9"
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.10015174506828528,Test Accuracy(%) =1 =0.8 =0.6 =0.4 = 0.2 = 0 MLP
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.10166919575113809,"(b) Synthetic graphs generated from Citeseer
Figure 3: SSNC accuracy of GCN on synthetic graphs with various homophily ratios."
ANALYSIS BASED ON CSBM MODEL WITH TWO CLASSES,0.10318664643399089,"example, when p = 9q or 9p = q, (p + q)2/(p −q)2 ≈1.23, thus nodes with degree larger than 1
can beneﬁt from the GCN operation. These two cases correspond to extremely homophily (h = 0.9)
and extremely heterophily (h = 0.1), respectively. However, GCN model behaves similarly on these
two cases and is able to improve the performance for most nodes. This clearly demonstrates that
heterophily is not a sufﬁcient condition for poor GCN performance. Likewise, when p≈q, the two
neighborhood distributions are hardly distinguishable, and only nodes with extremely large degrees
can beneﬁt. In the extreme case, where p = q, the GCN operation cannot help any nodes at all. Note
that Theorem 2 and the followed analysis can be extended to a multi-class CSBM scenario as well –
see Appendix F for an intuitive explanation and proof sketch."
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.1047040971168437,"3.3
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES"
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.1062215477996965,Alg. 1: Hetero. Edge Addition
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.10773899848254932,"input : G = {V, E}, K, {Dc}|C|−1
c=0
and {Vc}|C|−1
c=0
output: G′ = {V, E′}
Initialize G′ = {V, E}, k = 1 ;
while 1 ≤k ≤K do"
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.10925644916540213,"Sample node i ∼Uniform(V);
Obtain the label, yi of node i;
Sample a label c ∼Dyi;
Sample node j ∼Uniform(Vc);
Update edge set E′ = E′ ∪{(i, j)};
k ←k + 1;"
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.11077389984825493,"return G′ = {V, E′}"
EMPIRICAL INVESTIGATIONS ON GRAPHS WITH MULTIPLE CLASSES,0.11229135053110774,"We conduct experiments to substantiate our claims in Ob-
servations 1 and 2 in graphs with multiple classes. We
evaluate how SSNC performance changes as we make a
homophilous graph more and more heterophilous under
two settings: (1) different labels have distinct distribu-
tions, and (2) different labels’ distributions are muddled."
TARGETED HETEROPHILOUS EDGE ADDITION,0.11380880121396054,"3.3.1
TARGETED HETEROPHILOUS EDGE ADDITION
Graph generation strategy. We start with common, real-
world benchmark graphs, and modify their topology by
adding synthetic, cross-label edges that connect nodes
with different labels. Following our discussion in Obser-
vation 2, we construct synthetic graphs that have similar
neighborhood distributions for same-label nodes. Specif-
ically, given a real-world graph G, we ﬁrst deﬁne a discrete neighborhood target distribution Dc for
each label c ∈C. We then follow these target distributions to add cross-label edges. The process of
generating new graphs by adding edges to G is shown in Algorithm 1. Speciﬁcally, we add a total K
edges to the given graph G: to add each edge, we ﬁrst uniformly sample a node i from V with label
yi, then we sample a label c from C according to Dyi, and ﬁnally, we uniformly sample a node j from
Vc and add the edge (i, j) to the graph. We generate synthetic graphs based on several real-world
graphs. We present the results based on Cora and Citeseer (Sen et al., 2008). The results for
other datasets can be found in Appendix C. Both Cora and Citeseer exhibit strong homophily.
For both datasets, we ﬁx Dc for all labels. Although many suitable Dc could be speciﬁed in line
with Observation 2, we ﬁx one set for illustration and brevity. For both datasets, we vary K over
11 values and thus generate 11 graphs. Notably, as K increases, the homophily h decreases. More
detailed information about the {Dc, c ∈C} and K for both datasets is included in Appendix C."
TARGETED HETEROPHILOUS EDGE ADDITION,0.11532625189681335,"Observed results. Figure 3(a-b) show SSNC results (accuracy) on graphs generated based on Cora
and Citeseer, respectively. The black line in both ﬁgures shows results for the presented setting
(we introduce γ in next subsection). Without loss of generality, we use Cora (a) to discuss our ﬁnd-
ings, since observations are similar over these datasets. Each point on the black line in Figure 3(a)
represents the performance of GCN model on a certain generated graph and the corresponding value
in x-axis denotes the homophily ratio of this graph. The point with homophily ratio h = 0.81 de-
notes the original Cora graph, i.e, K = 0. We observe that as K increases, h decreases, and while
the classiﬁcation performance ﬁrst decreases, it eventually begins to increase, showing a V -shape
pattern. For instance, when h = 0.25 (a rather heterophilous graph), the GCN model achieves an
impressive 86% accuracy, even higher than that achieved on the original Cora graph. We note that"
TARGETED HETEROPHILOUS EDGE ADDITION,0.11684370257966616,Published as a conference paper at ICLR 2022
TARGETED HETEROPHILOUS EDGE ADDITION,0.11836115326251896,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
TARGETED HETEROPHILOUS EDGE ADDITION,0.11987860394537178,0.92 0.31 0.42 0.04 0.04 0.46 0.29
TARGETED HETEROPHILOUS EDGE ADDITION,0.12139605462822459,"0.31 0.94 0.37
0.4
0.02 0.02 0.46"
TARGETED HETEROPHILOUS EDGE ADDITION,0.12291350531107739,"0.42 0.37 0.92
0.5
0.41 0.01 0.01"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1244309559939302,"0.04
0.4
0.5
0.85 0.49 0.39 0.03"
TARGETED HETEROPHILOUS EDGE ADDITION,0.125948406676783,0.04 0.02 0.41 0.49 0.91 0.36 0.45
TARGETED HETEROPHILOUS EDGE ADDITION,0.1274658573596358,0.46 0.02 0.01 0.39 0.36 0.94 0.26
TARGETED HETEROPHILOUS EDGE ADDITION,0.12898330804248861,0.29 0.46 0.01 0.03 0.45 0.26 0.97 0.0 0.2 0.4 0.6 0.8 1.0
TARGETED HETEROPHILOUS EDGE ADDITION,0.13050075872534142,"(a) γ = 0, Acc: 86%"
TARGETED HETEROPHILOUS EDGE ADDITION,0.13201820940819423,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
TARGETED HETEROPHILOUS EDGE ADDITION,0.13353566009104703,0.85 0.54 0.58 0.36 0.39 0.63 0.52
TARGETED HETEROPHILOUS EDGE ADDITION,0.13505311077389984,"0.54 0.91 0.58 0.55 0.39
0.4
0.66"
TARGETED HETEROPHILOUS EDGE ADDITION,0.13657056145675264,0.58 0.58 0.85 0.64 0.57 0.39 0.37
TARGETED HETEROPHILOUS EDGE ADDITION,0.13808801213960548,0.36 0.55 0.64 0.79 0.65 0.54 0.36
TARGETED HETEROPHILOUS EDGE ADDITION,0.13960546282245828,"0.39 0.39 0.57 0.65 0.84 0.58
0.6"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1411229135053111,"0.63
0.4
0.39 0.54 0.58 0.89
0.5"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1426403641881639,"0.52 0.66 0.37 0.36
0.6
0.5
0.93 0.0 0.2 0.4 0.6 0.8 1.0"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1441578148710167,"(b) γ = 0.4, Acc: 66%"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1456752655538695,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1471927162367223,"0.8
0.76 0.74 0.64
0.7
0.77 0.76"
TARGETED HETEROPHILOUS EDGE ADDITION,0.14871016691957512,0.76 0.85 0.77 0.68 0.73 0.76 0.83
TARGETED HETEROPHILOUS EDGE ADDITION,0.15022761760242792,"0.74 0.77
0.8
0.71 0.72 0.72 0.74"
TARGETED HETEROPHILOUS EDGE ADDITION,0.15174506828528073,0.64 0.68 0.71 0.72 0.71 0.67 0.67
TARGETED HETEROPHILOUS EDGE ADDITION,0.15326251896813353,"0.7
0.73 0.72 0.71 0.79 0.76 0.77"
TARGETED HETEROPHILOUS EDGE ADDITION,0.15477996965098634,0.77 0.76 0.72 0.67 0.76 0.83 0.76
TARGETED HETEROPHILOUS EDGE ADDITION,0.15629742033383914,0.76 0.83 0.74 0.67 0.77 0.76 0.89 0.0 0.2 0.4 0.6 0.8 1.0
TARGETED HETEROPHILOUS EDGE ADDITION,0.15781487101669195,"(c) γ = 0.8, Acc: 44%"
TARGETED HETEROPHILOUS EDGE ADDITION,0.15933232169954475,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
TARGETED HETEROPHILOUS EDGE ADDITION,0.16084977238239756,"0.8
0.82 0.78
0.7
0.77 0.81 0.83"
TARGETED HETEROPHILOUS EDGE ADDITION,0.16236722306525037,"0.82 0.85
0.8
0.73
0.8
0.83 0.85"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1638846737481032,"0.78
0.8
0.79 0.67 0.75
0.8
0.82"
TARGETED HETEROPHILOUS EDGE ADDITION,0.165402124430956,"0.7
0.73 0.67 0.73 0.69 0.72 0.74"
TARGETED HETEROPHILOUS EDGE ADDITION,0.1669195751138088,"0.77
0.8
0.75 0.69 0.77 0.78 0.81"
TARGETED HETEROPHILOUS EDGE ADDITION,0.16843702579666162,"0.81 0.83
0.8
0.72 0.78 0.83 0.84"
TARGETED HETEROPHILOUS EDGE ADDITION,0.16995447647951442,0.83 0.85 0.82 0.74 0.81 0.84 0.88 0.0 0.2 0.4 0.6 0.8 1.0
TARGETED HETEROPHILOUS EDGE ADDITION,0.17147192716236723,"(d) γ = 1, Acc: 41%
Figure 4: Cross-class neighborhood similarity on synthetic graphs generated from Cora; all graphs
have h = 0.25, but with varying neighborhood distributions as per the noise parameter γ."
TARGETED HETEROPHILOUS EDGE ADDITION,0.17298937784522003,"performance continues to increase as K increases further to the right (we censor due to space limi-
tations; see Appendix C for details). This clearly demonstrates that the GCN model can work well
on heterophilous graphs under certain conditions. Intuitively, the V -shape arises due to a “phase
transition”, where the initial topology is overridden by added edges according to the associated Dc
target neighbor distributions. In the original graph, the homophily ratio is quite high (h = 0.81),
and classiﬁcation behavior is akin to that discussed in Observation 1, where same-label nodes have
similar neighborhood patterns. As we add edges to the graph, the originally evident neighborhood
patterns are perturbed by added edges and gradually become less informative, which leads to the
performance decrease in the decreasing segment of the V -shape in Figure 3(a). Then, as we keep
adding more edges, the neighborhood pattern gradually approaches Dc for all c, corresponding to
the increasing segment of the V -shape."
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.17450682852807284,"3.3.2
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS
Graph generation strategy. In Section 3.3.1, we showed that the GCN model can achieve reason-
able performance on heterophilous graphs constructed following distinct, pre-deﬁned neighborhood
patterns. As per Observation 2, our theoretical understanding suggests that performance should de-
grade under heterophily if the distributions of different labels get more and more indistinguishable.
Hence, we next demonstrate this empirically by introducing controllable noise levels into our edge
addition strategy. We adopt a strategy similar to that described in Algorithm 1, but with the key
difference being that we introduce an additional parameter γ, which controls the probability that
we add cross-label edges randomly rather than following the pre-deﬁned distributions. A detailed
description of this approach is demonstrated in Algorithm 2 in Appendix C. For nodes of a given
class c (w.l.o.g), compared to the edges added according to Dc, the randomly added edges can be
regarded as noise. Speciﬁcally, by increasing the noise parameter γ, we increase the similarity be-
tween Dc, Dc′ for any pair of labels c, c′. If γ = 1, then all neighborhood distributions will be
indistinguishable (they will all be approximately Uniform(|C|). By ﬁxing K and varying γ, we can
generate graph variants with the same homophily ratio but different similarities between Dc and
Dc′. As in Section 3.3.1, we create graphs by adding edges at various K, but also vary γ ∈[0, 1] in
increments of 0.2 on both Cora and Citeseer."
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.17602427921092564,"Observed results. We report the SSNC performance on these graphs in Figure 3. Firstly, we observe
that noise affects the performance signiﬁcantly when the homophily ratio is low. For example,
observing Figure 3(a) vertically at homophily ratio h = 0.25, higher γ clearly results in worse
performance. This indicates that not only the ﬁxed-ness of the neighborhood distributions, but their
similarities are important for the SSNC task (aligned with Observation 2. It also indicates that
there are “good” and “bad” kinds of heterophily. On the other hand, high γ does not too-negatively
impact when K is small, since noise is minimal and the original graph topology is yet largely
homophilous. At this stage, both “good” (ﬁxed and disparate patterns) and “bad” (randomly added
edges) heterophilous edges introduce noise to the dominant homophilous patterns. When the noise
level γ is not too large, we can still observe the V -shape: e.g. γ = 0.4 in Figure 3(a) and γ = 0.2
in Figure 3 (b); this is because the designed pattern is not totally dominated by the noise. However,
when γ is too high, adding edges will constantly decrease the performance, as nodes of different
classes have indistinguishably similar neighborhoods."
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.17754172989377845,"To further demonstrate how γ affects the neighborhood distributions in the generated graph, we
examine the cross-class neighborhood similarity, which we deﬁne as follows:"
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.17905918057663125,"Deﬁnition 2 (Cross-Class Neighborhood Similarity (CCNS)). Given graph G and labels y for all
nodes, the CCNS between classes c, c′ ∈C is s(c, c′) =
1
|Vc||Vc′ |
P"
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18057663125948406,"i∈Vc,j∈Vc′ cos (d(i), d(j)) where"
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18209408194233687,Published as a conference paper at ICLR 2022
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18361153262518967,"Vc indicates the set of nodes in class c and d(i) denotes the empirical histogram (over |C| classes)
of node i’s neighbors’ labels, and the function cos(·, ·) measures the cosine similarity."
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18512898330804248,"When c = c′, s(c, c′) calculates the intra-class similarity, otherwise, it calculates the inter-class
similarity from a neighborhood label distribution perspective. Intuitively, if nodes with the same
label share the same neighborhood distributions, the intra-class similarity should be high. Likewise,
to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter-
class similarity should be low. To illustrate how various γ values affect the neighborhood patterns,
we illustrate the intra-class and inter-class similarities in Figure 4 for γ = 0, 0.4, 0.8, 1 on graphs
generated from Cora with homophily ratio h = 0.25. The diagonal cells in each heatmap indicate
the intra-class similarity while off-diagonal cells indicate inter-class similarity. Clearly, when γ is
small, the intra-class similarity is high while the inter-class similarity is low, which demonstrates
the existence of strongly discriminative neighborhood patterns in the graph. As γ increases, the
intra-class and inter-class similarity get closer, becoming more and more indistinguishable, leading
to bad performance due to indistinguishable distributions as referenced in Observation 2."
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18664643399089528,"4
REVISITING GCN’S PERFORMANCE ON REAL-WORLD GRAPHS"
INTRODUCING NOISE TO NEIGHBORHOOD DISTRIBUTIONS,0.18816388467374812,"In this section, we ﬁrst give more details on the experiments we run to compare GCN and MLP. We
next investigate why the GCN model does or does not work well on certain datasets utilizing the
understanding developed in earlier sections."
QUANTITATIVE ANALYSIS,0.18968133535660092,"4.1
QUANTITATIVE ANALYSIS
Following previous work (Pei et al., 2020; Zhu et al., 2020b), we evaluate the performance of
the GCN model on several real-world graphs with different levels of homophily. We include the
citation networks Cora, Citeseer and Pubmed (Kipf and Welling, 2016), which are highly
homophilous. We also adopt several heterophilous benchmark datasets including Chameleon,
Squirrel, Actor, Cornell, Wisconsin and Texas (Rozemberczki et al., 2021; Pei et al.,
2020). Appendix D.1 gives descriptions and summary statistics of these datasets. For all datasets,
we follow the experimental setting provided in (Pei et al., 2020), which consists of 10 random splits
with proportions 48/32/20% corresponding to training/validation/test for each graph. For each
split, we use 10 random seeds, and report the average performance and standard deviation across
100 runs. We compare the GCN model with the MLP model, which does not utilize the graph
structure. With this comparison, we aim to check whether the GCN model always fails for for
heterophilous graphs (perform even worse than MLP). We also compare GCN with state-of-the-art
methods and their descriptions and performance are included in the Appendix D. The node clas-
siﬁcation performance (accuracy) of these models is reported in Table 2. Notably, GCN achieves
better performance than MLP on graphs with high homophily (Cora, Citeseer, and Pubmed),
as expected. For the heterophilous graphs, the results are comparatively mixed. The GCN model
outperforms MLP on Squirrel and Chameleon (it even outperforms methods speciﬁcally de-
signed for heterophilous graphs as shown in Appendix D.5.), while underperforming on the other
datasets (Actor, Cornell, Wisconsin, and Texas). In the next section, we provide explana-
tions for GCN’s distinct behaviors on these graphs based on the understanding developed in earlier
sections."
QUALITATIVE ANALYSIS,0.19119878603945373,"4.2
QUALITATIVE ANALYSIS
Our work so far illustrates that the popular notion of GCNs not being suitable for heterophily, or
homophily being a mandate for good GCN performance is not accurate. In this subsection, we
aim to use the understanding we developed in Section 3 to explain why GCN does (not) work
well on real-world graphs. As in Section 3.3.2, we inspect cross-class neighborhood similarity
(Deﬁnition 2) for each dataset; due to the space limit, we only include representative ones here
(Cora, Chameleon, Actor and Cornell); see Figure 5). Heatmaps for the other datasets
can be found in Appendix E. From Figure 5(a), it is clear that the intra-class similarity is much
higher than the inter-similarity ones, hence Cora contains distinct neighborhood patterns, consistent
with Observation 1. In Figure 5(b), we can observe that in Chameleon, intra-class similarity is
generally higher than inter-class similarity, though not as strong as in Figure 5(a). Additionally,
there is an apparent gap between labels 0, 1 and 2, 3, 4, which contributes to separating nodes of the
former 2 from the latter 3 classes, but potentially increasing misclassiﬁcation within each of the two
groupings. These observations also help substantiate why GCN can achieve reasonable performance
(much higher than MLP) on Chameleon. The GCN model underperforms MLP in Actor and
we suspect that the graph does not provide useful information. The heatmap for Actor in Figure 5
shows that the intra-class and inter-class similarities are almost equivalent, making the neighborhood"
QUALITATIVE ANALYSIS,0.19271623672230653,Published as a conference paper at ICLR 2022
QUALITATIVE ANALYSIS,0.19423368740515934,Table 2: Node classiﬁcation performance (accuracy) on homophilous and heterophilous graphs.
QUALITATIVE ANALYSIS,0.19575113808801214,"Cora
Citeseer
Pubmed
Chameleon
Squirrel
Actor
Cornell
Wisconsin
Texas"
QUALITATIVE ANALYSIS,0.19726858877086495,"GCN
87.12 ± 1.38
76.50 ± 1.61
88.52 ± 0.41
67.96 ± 1.82
54.47 ± 1.17
30.31 ± 0.98
59.35 ± 4.19
61.76 ± 6.15
63.81 ± 5.27
MLP
75.04 ± 1.97
72.40 ± 1.97
87.84 ± 0.30
48.11 ± 2.23
31.68 ± 1.90
36.17 ± 1.09
84.86 ± 6.04
86.29 ± 4.50
83.30 ± 4.54"
QUALITATIVE ANALYSIS,0.19878603945371776,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
QUALITATIVE ANALYSIS,0.20030349013657056,0.84 0.07 0.03 0.15 0.12 0.11 0.16
QUALITATIVE ANALYSIS,0.20182094081942337,"0.07 0.87 0.09
0.1
0.04 0.06 0.01"
QUALITATIVE ANALYSIS,0.20333839150227617,0.03 0.09 0.95 0.05 0.01 0.05 0.01
QUALITATIVE ANALYSIS,0.20485584218512898,"0.15
0.1
0.05
0.9
0.11 0.07 0.04"
QUALITATIVE ANALYSIS,0.20637329286798178,0.12 0.04 0.01 0.11 0.89 0.04 0.02
QUALITATIVE ANALYSIS,0.2078907435508346,0.11 0.06 0.05 0.07 0.04 0.87 0.12
QUALITATIVE ANALYSIS,0.2094081942336874,0.16 0.01 0.01 0.04 0.02 0.12 0.89 0.0 0.2 0.4 0.6 0.8 1.0
QUALITATIVE ANALYSIS,0.2109256449165402,(a) Cora
QUALITATIVE ANALYSIS,0.212443095599393,"0
1
2
3
4 0 1 2 3 4"
QUALITATIVE ANALYSIS,0.21396054628224584,"0.58
0.58
0.48
0.48
0.47"
QUALITATIVE ANALYSIS,0.21547799696509864,"0.58
0.59
0.51
0.51
0.47"
QUALITATIVE ANALYSIS,0.21699544764795145,"0.48
0.51
0.7
0.68
0.67"
QUALITATIVE ANALYSIS,0.21851289833080426,"0.48
0.51
0.68
0.73
0.7"
QUALITATIVE ANALYSIS,0.22003034901365706,"0.47
0.47
0.67
0.7
0.74 0.0 0.2 0.4 0.6 0.8 1.0"
QUALITATIVE ANALYSIS,0.22154779969650987,(b) Chameleon
QUALITATIVE ANALYSIS,0.22306525037936267,"0
1
2
3
4 0 1 2 3 4"
QUALITATIVE ANALYSIS,0.22458270106221548,"0.51
0.51
0.51
0.5
0.51"
QUALITATIVE ANALYSIS,0.22610015174506828,"0.51
0.51
0.51
0.51
0.51"
QUALITATIVE ANALYSIS,0.2276176024279211,"0.51
0.51
0.51
0.5
0.51"
QUALITATIVE ANALYSIS,0.2291350531107739,"0.5
0.51
0.5
0.5
0.5"
QUALITATIVE ANALYSIS,0.2306525037936267,"0.51
0.51
0.51
0.5
0.51 0.0 0.2 0.4 0.6 0.8 1.0"
QUALITATIVE ANALYSIS,0.2321699544764795,(c) Actor
QUALITATIVE ANALYSIS,0.2336874051593323,"0
1
2
3
4 0 1 2 3 4"
QUALITATIVE ANALYSIS,0.23520485584218512,"0.6
0.69
0.53
0.57
0.51"
QUALITATIVE ANALYSIS,0.23672230652503792,"0.69
1.0
0.57
0.57
0.56"
QUALITATIVE ANALYSIS,0.23823975720789076,"0.53
0.57
0.48
0.52
0.47"
QUALITATIVE ANALYSIS,0.23975720789074356,"0.57
0.57
0.52
0.58
0.5"
QUALITATIVE ANALYSIS,0.24127465857359637,"0.51
0.56
0.47
0.5
0.45 0.0 0.2 0.4 0.6 0.8 1.0"
QUALITATIVE ANALYSIS,0.24279210925644917,"(d) Cornell
Figure 5: Cross-class neighborhood similarity on homophilous graphs and heterophilous graphs."
QUALITATIVE ANALYSIS,0.24430955993930198,"distributions for different classes hard to distinguish and leading to bad GCN performance. Similar
observations are made for Cornell. Note that Cornell only consists of 183 nodes and 280
edges, hence, the similarities shown in Figure 5 are impacted signiﬁcantly (e.g. there is a single
node with label 1, leading to perfect intra-class similarity for label 1)."
RELATED WORK,0.24582701062215478,"5
RELATED WORK"
RELATED WORK,0.2473444613050076,"Graph neural networks (GNNs) are powerful models for graph representation learning. They have
been widely adopted to tackle numerous applications from various domains (Kipf and Welling,
2016; Fan et al., 2019; Bastings et al., 2017; Shi et al., 2019). (Scarselli et al., 2008) proposed
the ﬁrst GNN model, to tackle both node and graph level tasks. Subsequently, Bruna et al. (2013)
and Defferrard et al. (2016) generalized convolutional neural networks to graphs from the graph
spectral perspective. Kipf and Welling (2016) simpliﬁed the spectral GNN model and proposed
graph convolutional networks (GCNs). Since then, numerous GNN variants, which follow speciﬁc
forms of feature transformation (linear layers) and aggregation have been proposed (Veliˇckovi´c et al.,
2017; Hamilton et al., 2017; Gilmer et al., 2017; Klicpera et al., 2019). The aggregation process can
be usually understood as feature smoothing (Li et al., 2018; Ma et al., 2020; Jia and Benson, 2021;
Zhu et al., 2021). Hence, several recent works claim (Zhu et al., 2020b;a; Chien et al., 2021), assume
(Halcrow et al., 2020; Wu et al., 2018; Zhao et al., 2020) or remark upon (Abu-El-Haija et al., 2019;
Maurya et al., 2021; Hou et al., 2020) GNN models homophily-reliance or unsuitability in capturing
heterophily. Several recent works speciﬁcally develop GNN models choices to tackle heterophilous
graphs by carefully designing or modifying model architectures such as Geom-GCN (Pei et al.,
2020), H2GCN (Zhu et al., 2020b), GPR-GNN (Chien et al., 2021), and CPGNN (Zhu et al., 2020a).
Some other works aim to modify/construct graphs to be more homophilous (Suresh et al., 2021).
There is concurrent work (Luan et al., 2021) also pointing out that GCN can potentially achieve
strong performance on heterophilous graphs. A major focus of this paper is to propose a new model
to handle heterophilous graphs. However, our work aims to empirically and theoretically understand
whether homophily is a necessity for GNNs and how GNNs work for heterophilous graphs."
CONCLUSION,0.2488619119878604,"6
CONCLUSION"
CONCLUSION,0.2503793626707132,"It is widely believed that GNN models inherently assume strong homophily and hence fail to gener-
alize to graphs with heterophily. In this paper, we revisit this popular notion and show it is not quite
accurate. We investigate one representative model, GCN, and show empirically that it can achieve
good performance on some heterophilous graphs under certain conditions. We analyze theoretically
the conditions required for GCNs to learn similar embeddings for same-label nodes, facilitating the
SSNC task; put simply, when nodes with the same label share similar neighborhood patterns, and
different classes have distinguishable patterns, GCN can achieve strong class separation, regard-
less of homophily or heterophily properties. Empirical analysis supports our theoretical ﬁndings.
Finally, we revisit several existing homophilous and heterophilous SSNC benchmark graphs, and
investigate GCN’s empirical performance in light of our understanding. Note that while there exist
graphs with “good heterophily”, “bad heterophily” still poses challenges to GNN models, which
calls for dedicated efforts. We discuss the limitation of the current work in Appendix I."
CONCLUSION,0.251896813353566,Published as a conference paper at ICLR 2022
REFERENCES,0.2534142640364188,REFERENCES
REFERENCES,0.2549317147192716,"Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. MixHop: Higher-order graph convolutional
architectures via sparsiﬁed neighborhood mixing. In Kamalika Chaudhuri and Ruslan Salakhutdi-
nov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 21–29. PMLR, 09–15 Jun 2019. URL
http://proceedings.mlr.press/v97/abu-el-haija19a.html."
REFERENCES,0.2564491654021244,"Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath.
Graph convolution for semi-
supervised classiﬁcation: Improved linear separability and out-of-distribution generalization.
arXiv preprint arXiv:2102.06966, 2021."
REFERENCES,0.25796661608497723,"Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. Graph convolu-
tional encoders for syntax-aware neural machine translation. arXiv preprint arXiv:1704.04675,
2017."
REFERENCES,0.25948406676783003,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013."
REFERENCES,0.26100151745068284,"Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In International Conference on Machine Learning, pages 1725–1735.
PMLR, 2020."
REFERENCES,0.26251896813353565,"Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank
graph neural network. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=n6jl7fLxrP."
REFERENCES,0.26403641881638845,"Valerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro Panzarasa, and Vito Latora. Ho-
mophily and missing links in citation networks. EPJ Data Science, 5:1–14, 2016."
REFERENCES,0.26555386949924126,"Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks with
limited sensitive attribute information. In Proceedings of the 14th ACM International Conference
on Web Search and Data Mining, pages 680–688, 2021."
REFERENCES,0.26707132018209406,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. arXiv preprint arXiv:1606.09375, 2016."
REFERENCES,0.26858877086494687,"Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic
block models. In NeurIPS, 2018."
REFERENCES,0.2701062215477997,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pages 417–426, 2019."
REFERENCES,0.2716236722306525,"Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7
(2):179–188, 1936."
REFERENCES,0.2731411229135053,"Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics reports,
659:1–44, 2016."
REFERENCES,0.2746585735963581,"Elisabeth R Gerber, Adam Douglas Henry, and Mark Lubell. Political homophily and collaboration
in regional planning networks. American Journal of Political Science, 57(3):598–610, 2013."
REFERENCES,0.27617602427921095,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pages
1263–1272. PMLR, 2017."
REFERENCES,0.27769347496206376,"Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan Perozzi. Grale: Designing networks for
graph learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, pages 2523–2532, 2020."
REFERENCES,0.27921092564491656,"William L Hamilton, Rex Ying, and Jure Leskovec.
Inductive representation learning on large
graphs. arXiv preprint arXiv:1706.02216, 2017."
REFERENCES,0.28072837632776937,Published as a conference paper at ICLR 2022
REFERENCES,0.2822458270106222,"Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard T. B. Ma, Hongzhi Chen, and Ming-Chang
Yang.
Measuring and improving the use of graph information in graph neural networks.
In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=rkeIIkHKvS."
REFERENCES,0.283763277693475,"Ankit Jain and Piero Molino. Enhancing recommendations on uber eats with graph convolutional
networks."
REFERENCES,0.2852807283763278,"Junteng Jia and Austin R Benson. A unifying generative model for graph learning algorithms: Label
propagation, graph convolutions, and combinations. arXiv preprint arXiv:2101.07730, 2021."
REFERENCES,0.2867981790591806,"Weiwei Jiang and Jiayun Luo. Graph neural network for trafﬁc forecasting: A survey. arXiv preprint
arXiv:2101.11174, 2021."
REFERENCES,0.2883156297420334,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.2898330804248862,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann.
Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations (ICLR), 2019."
REFERENCES,0.291350531107739,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 32, 2018."
REFERENCES,0.2928679817905918,"Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and
Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong
simple methods. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.2943854324734446,"Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
Chang, and Doina Precup. Is heterophily a real nightmare for graph neural networks to do node
classiﬁcation? arXiv preprint arXiv:2109.05641, 2021."
REFERENCES,0.2959028831562974,"Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A uniﬁed view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020."
REFERENCES,0.29742033383915023,"Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving graph neural networks with simple
architecture design. arXiv preprint arXiv:2105.07634, 2021."
REFERENCES,0.29893778452200304,"Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social
networks. Annual review of sociology, 27(1):415–444, 2001."
REFERENCES,0.30045523520485584,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4602–4609,
2019."
REFERENCES,0.30197268588770865,"Mark Newman. Networks. Oxford university press, 2018."
REFERENCES,0.30349013657056145,"Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=S1e2agrFvS."
REFERENCES,0.30500758725341426,"Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021."
REFERENCES,0.30652503793626706,"Aravind Sankar, Yozen Liu, Jun Yu, and Neil Shah. Graph neural networks for friend ranking in
large-scale social platforms. 2021."
REFERENCES,0.30804248861911987,"Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008."
REFERENCES,0.3095599393019727,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008."
REFERENCES,0.3110773899848255,Published as a conference paper at ICLR 2022
REFERENCES,0.3125948406676783,"Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with directed
graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 7912–7921, 2019."
REFERENCES,0.3141122913505311,"Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the limit of
graph neural networks by improving the assortativity of graphs with local mixing patterns. arXiv
preprint arXiv:2106.06586, 2021."
REFERENCES,0.3156297420333839,"Xianfeng Tang, Yozen Liu, Neil Shah, Xiaolin Shi, Prasenjit Mitra, and Suhang Wang. Knowing
your fate: Friendship, action and temporal explanations for user engagement prediction on so-
cial apps. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 2269–2279, 2020a."
REFERENCES,0.3171471927162367,"Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal, Prasenjit Mitra,
and Suhang Wang. Investigating and mitigating degree-related biases in graph convoltuional net-
works. In Proceedings of the 29th ACM International Conference on Information & Knowledge
Management, pages 1435–1444, 2020b."
REFERENCES,0.3186646433990895,"Anton Tsitsulin, Benedek Rozemberczki, John Palowitch, and Bryan Perozzi. Synthetic graph gen-
eration to benchmark graph learning. 2021."
REFERENCES,0.3201820940819423,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017."
REFERENCES,0.3216995447647951,"Xuan Wu, Lingxiao Zhao, and Leman Akoglu. A quest for structure: jointly learning the graph struc-
ture and semi-supervised classiﬁcation. In Proceedings of the 27th ACM international conference
on information and knowledge management, pages 87–96, 2018."
REFERENCES,0.3232169954476479,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
How powerful are graph neural
networks?
In International Conference on Learning Representations, 2019.
URL https:
//openreview.net/forum?id=ryGs6iA5Km."
REFERENCES,0.32473444613050073,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
974–983, 2018."
REFERENCES,0.3262518968133536,"Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aug-
mentation for graph neural networks. In AAAI, 2020."
REFERENCES,0.3277693474962064,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-
tions. AI Open, 1:57–81, 2020."
REFERENCES,0.3292867981790592,"Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai
Koutra. Graph neural networks with heterophily. arXiv preprint arXiv:2009.13566, 2020a."
REFERENCES,0.330804248861912,"Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. Advances in
Neural Information Processing Systems, 33, 2020b."
REFERENCES,0.3323216995447648,"Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural
networks with an optimization framework. arXiv preprint arXiv:2101.11859, 2021."
REFERENCES,0.3338391502276176,"Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects with
graph convolutional networks. Bioinformatics, 34(13):i457–i466, 2018."
REFERENCES,0.3353566009104704,Published as a conference paper at ICLR 2022
REFERENCES,0.33687405159332323,"A
PROOF OF THEOREM 1"
REFERENCES,0.33839150227617604,"To prove Theorem 1, we ﬁrst introduce the celebrated Hoeffding inequality below."
REFERENCES,0.33990895295902884,"Lemma 1 (Hoeffding’s Inequality). Let Z1, . . . , Zn be independent bounded random variables with
Zi ∈[a, b] for all i, where −∞< a ≤b < ∞. Then P"
N,0.34142640364188165,"1
n n
X"
N,0.34294385432473445,"i=1
(Zi −E [Zi]) ≥t !"
N,0.34446130500758726,"≤exp

−
2nt2"
N,0.34597875569044007,(b −a)2  and P
N,0.34749620637329287,"1
n n
X"
N,0.3490136570561457,"i=1
(Zi −E [Zi]) ≤−t !"
N,0.3505311077389985,"≤exp

−
2nt2"
N,0.3520485584218513,(b −a)2 
N,0.3535660091047041,for all t ≥0.
N,0.3550834597875569,"Theorem 1. Consider a graph G = {V, E, {Fc, c ∈C}, {Dc, c ∈C}}, which follows Assump-
tions (1)-(4). For any node i ∈V, the expectation of the pre-activation output of a single GCN
operation is given by
E[hi] = W
 
Ec∼Dyi,x∼Fc[x]

.
(5)
and for any t > 0, the probability that the distance between the observation hi and its expectation
is larger than t is bounded by"
N,0.3566009104704097,"P (∥hi −E[hi]∥2 ≥t) ≤2 · l · exp

−
deg(i)t2"
N,0.3581183611532625,2ρ2(W)B2l
N,0.3596358118361153,"
,
(6)"
N,0.3611532625189681,where l is the feature dimensionality and ρ(W) denotes the largest singular value of W.
N,0.3626707132018209,Proof. The expectation of hi can be derived as follows.
N,0.36418816388467373,E [hi] = E  X
N,0.36570561456752654,j∈N(i)
N,0.36722306525037934,"1
deg(i)Wxj  "
N,0.36874051593323215,"=
1
deg(i) X"
N,0.37025796661608495,"j∈N(i)
WEc∼Dyi,x∼Fc[x]"
N,0.37177541729893776,"= W
 
Ec∼Dyi,x∼Fc[x]

."
N,0.37329286798179057,"We utilize Hoeffding’s Inequality to prove the bound in Eq. (6). Let xi[k], k = 1, . . . , l denote the
i-th element of x. Then, for any dimension k, {xj[k], j ∈N(i)} is a set of independent bounded
random variables. Hence, directly applying Hoeffding’s inequality, for any t1 ≥0, we have the
following bound: P    X"
N,0.37481031866464337,"j∈N(i)
(xj[k] −E [xj[k]]) ≥t1 "
N,0.37632776934749623,"≤2 exp

−(deg(i))t2
1
2B2  If P"
N,0.37784522003034904,"j∈N(i)
(xj −E [xj]) 2
≥
√"
N,0.37936267071320184,"lt1, then at least for one k
∈
{1, . . . , l}, the inequality

P"
N,0.38088012139605465,"j∈N(i)
(xj[k] −E [xj[k]])"
N,0.38239757207890746,"≥t1 holds. Hence, we have P    X"
N,0.38391502276176026,"j∈N(i)
(xj −E [xj]) 2 ≥
√ lt1  ≤P  
l[ k=1 
   X"
N,0.38543247344461307,"j∈N(i)
(xj[k] −E [xj[k]]) ≥t1 
    ≤ l
X k=1
P    X"
N,0.38694992412746587,"j∈N(i)
(xj[k] −E [xj[k]]) ≥t1  "
N,0.3884673748103187,"= 2 · l · exp

−(deg(i))t2
1
2B2 "
N,0.3899848254931715,Published as a conference paper at ICLR 2022
N,0.3915022761760243,"Let t1 = t2
√"
N,0.3930197268588771,"l, then we have P    X"
N,0.3945371775417299,"j∈N(i)
(xj −E [xj]) 2 ≥t2 "
N,0.3960546282245827,"≤2 · l · exp

−(deg(i))t2
2
2B2l "
N,0.3975720789074355,"Furthermore, we have"
N,0.3990895295902883,∥hi −E[hi]∥2 = W  X
N,0.4006069802731411,"j∈N(i)
(xj −E [xj])   2 ≤∥W∥2  X"
N,0.40212443095599393,"j∈N(i)
(xj −E [xj]) 2"
N,0.40364188163884673,= ρ(W)  X
N,0.40515933232169954,"j∈N(i)
(xj −E [xj]) 2 ,"
N,0.40667678300455234,where ∥W∥2 is the matrix 2-norm of W. Note that the last line uses the identity ∥W∥2 = ρ(W).
N,0.40819423368740515,"Then, for any t > 0, we have"
N,0.40971168437025796,P (∥hi −E[hi]∥2 ≥t) ≤P  ρ(W)  X
N,0.41122913505311076,"j∈N(i)
(xj −E [xj]) 2 ≥t   = P    X"
N,0.41274658573596357,"j∈N(i)
(xj −E [xj]) 2"
N,0.4142640364188164,"≥
t
ρ(W)  "
N,0.4157814871016692,"≤2 · l · exp

−(deg(i))t2"
N,0.417298937784522,"2ρ2(W)B2l 
,"
N,0.4188163884673748,which completes the proof.
N,0.4203338391502276,"B
PROOF OF THEOREM 2"
N,0.4218512898330804,"Theorem 2. Consider a graph G ∼CSBM(µ1, µ2, p, q). For any node i in this graph, the linear
classiﬁer deﬁned by the decision boundary P has a lower probability to mis-classify hi than xi
when deg(i) > (p + q)2/(p −q)2."
N,0.4233687405159332,"Proof. We only prove for nodes from classes c0 since the case for nodes from classes c1 is symmetric
and the proof is exactly the same. For a node i ∈C0, we have the follows
P(xi is mis-classiﬁed) = P(w⊤xi + b ≤0) for i ∈C0
(7)"
N,0.424886191198786,"P(hi is mis-classiﬁed) = P(w⊤hi + b ≤0) for i ∈C0,
(8)
where w and b = −w⊤(µ1 + µ1) /2 is the parameters of the decision boundary P. we have that
P(w⊤hi + b ≤0) = P(w⊤p"
N,0.4264036418816389,"deg(i)hi +
p"
N,0.4279210925644917,"deg(i)b ≤0).
(9)
We denote the scaled version of hi as h′
i =
p"
N,0.4294385432473445,"deg(i)hi. Then, h′
i follows"
N,0.4309559939301973,"h′
i =
p"
N,0.4324734446130501,deg(i)hi ∼N p
N,0.4339908952959029,deg(i) (pµ0 + qµ1)
N,0.4355083459787557,"p + q
, I !"
N,0.4370257966616085,", for i ∈C0.
(10)"
N,0.4385432473444613,"Because of the scale in Eq. (9), the decision boundary for h′
i is correspondingly moved to w⊤h′ +
p"
N,0.4400606980273141,"deg(i)b = 0. Now, since xi and h′
i share the same variance, to compare the mis-classiﬁcation
probabilities, we only need to compare the distance from their expected value to their corresponding
decision boundary. Speciﬁcally, the two distances are as follows:"
N,0.44157814871016693,disxi = ∥µ0 −µ1∥2 2
N,0.44309559939301973,"dish′
i = p"
N,0.44461305007587254,deg(i)|p −q|
N,0.44613050075872535,"(p + q)
· ∥µ0 −µ1∥2"
N,0.44764795144157815,"2
.
(11)"
N,0.44916540212443096,Published as a conference paper at ICLR 2022
N,0.45068285280728376,"The larger the distance is the smaller the mis-classiﬁcation probability is. Hence, when dish′
i <
disxi, h′
i has a lower probability to be mis-classiﬁed than xi. Comparing the two distances, we"
N,0.45220030349013657,"conclude that when deg(i) >

p+q
p−q
2
, h′
i has a lower probability to be mis-classiﬁed than xi.
Together with Eq. 9, we have that"
N,0.4537177541729894,"P(hi is mis-classiﬁed) < P(xi is mis-classiﬁed) if deg(i) >
p + q p −q"
N,0.4552352048558422,"2
,
(12)"
N,0.456752655538695,which completes the proof.
N,0.4582701062215478,"C
ADDITIONAL DETAILS AND RESULTS FOR SECTION 3.3.1"
N,0.4597875569044006,"C.1
DETAILS ON THE GENERATED GRAPHS"
N,0.4613050075872534,"In this subsection, we present the details of the graphs that we generate in Section 3.3.1. Speciﬁcally,
we detail the distributions {Dc, c ∈C} used in the examples, the number of added edges K, and the
homophily ratio h. We provide the details for Cora and Citeseer in the following subsections.
Note that the choices of distributions shown here are for illustrative purposes, to coincide with
Observations 1 and 2. We adapted circulant matrix-like designs due to their simplicity."
N,0.4628224582701062,"C.1.1
CORA"
N,0.464339908952959,"There are 7 labels, which we denote as {0, 1, 2, 3, 4, 5, 6}. The distributions {Dc, c ∈C} are listed
as follows. The values of K and the homophily ratio of their corresponding generated graphs are
shown in Table 3.
D0 : Categorical([0, 0.5, 0, 0, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0, 0, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5, 0, 0]),
D4 : Categorical([0, 0, 0, 0.5, 0, 0.5, 0]),
D5 : Categorical([0, 0, 0, 0, 0.5, 0, 0.5]),
D6 : Categorical([0.5, 0, 0, 0, 0, 0.5, 0])."
N,0.4658573596358118,Table 3: # of added edges (K) and homophily ratio (h) values for generated graphs based on Cora.
N,0.4673748103186646,"K
1003
2006
3009
4012
6018
8024
10030
12036
16048
20060
24072"
N,0.46889226100151743,"h
0.740
0.681
0.630
0.587
0.516
0.460
0.415
0.378
0.321
0.279
0.247"
N,0.47040971168437024,"C.1.2
CITESEER"
N,0.47192716236722304,"There are 6 labels, which we denote as {0, 1, 2, 3, 4, 5}. The distributions {Dc, c ∈C} are listed
as follows. The values of K and the homophily ratio of their corresponding generated graphs are
shown in Table 4.
D0 : Categorical([0, 0.5, 0, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5, 0]),
D4 : Categorical([0, 0, 0, 0.5, 0, 0.5]),
D5 : Categorical([0.5, 0, 0, 0, 0.5, 0])."
N,0.47344461305007585,Published as a conference paper at ICLR 2022
N,0.47496206373292865,"Table 4: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Citeseer"
N,0.4764795144157815,"K
1204
2408
3612
4816
7224
9632
12040
14448
19264
24080
28896"
N,0.4779969650986343,"h
0.650
0.581
0.527
0.481
0.410
0.357
0.317
0.284
0.236
0.202
0.176"
N,0.4795144157814871,"C.2
RESULTS ON MORE DATASETS: CH A M E L E O N AND SQ U I R R E L"
N,0.48103186646433993,"We conduct similar experiments as those in Section 3.3.1 based on Chameleon and Squirrel.
Note that both Squirrel and Chameleon have 5 labels, which we denote as {0, 1, 2, 3, 4}. We
pre-deﬁne the same distributions for them as listed as follows. The values of K and the homophily
ratio of their corresponding generated graphs based on Squirrel and Chameleon are shown in
Table 5 and Table 6, respectively.
D0 : Categorical([0, 0.5, 0, 0, 0.5]),
D1 : Categorical([0.5, 0, 0.5, 0, 0]),
D2 : Categorical([0, 0.5, 0, 0.5, 0]),
D3 : Categorical([0, 0, 0.5, 0, 0.5]),
D4 : Categorical([0.5, 0, 0, 0.5, 0])."
N,0.48254931714719274,"Table 5: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Squirrel."
N,0.48406676783004554,"K
12343
24686
37030
49374
61716
74060
86404
98746
111090
12434
135776"
N,0.48558421851289835,"h
0.215
0.209
0.203
0.197
0.192
0.187
0.182
0.178
0.173
0.169
0.165"
N,0.48710166919575115,"Table 6: # of added edges (K) and homophily ratio (h) values for generated graphs based on
Chameleon."
N,0.48861911987860396,"K
1932
3866
5798
7730
9964
11596
13528
15462
17394
19326
21260"
N,0.49013657056145676,"h
0.223
0.217
0.210
0.205
0.199
0.194
0.189
0.184
0.180
0.176
0.172"
N,0.49165402124430957,"0.222
0.215
0.209
0.203
0.198
0.192
0.187
0.182
0.178
0.174
0.169
0.165
Homophily Ratio 0.5 0.6 0.7 0.8 0.9"
N,0.4931714719271624,Test Accuracy = 0
N,0.4946889226100152,(a) Synthetic graphs generated from Squirrel
N,0.496206373292868,"0.23
0.223
0.217
0.21
0.205
0.199
0.194
0.189
0.184
0.18
0.176
0.172
Homophily Ratio 0.60 0.65 0.70 0.75 0.80 0.85"
N,0.4977238239757208,Test Accuracy = 0
N,0.4992412746585736,(b) Synthetic graphs generated from Chameleon
N,0.5007587253414264,Figure 6: Performance of GCN on synthetic graphs with various homophily ratio.
N,0.5022761760242792,"The performance of the GCN model on these two sets of graphs (generated from Squirrel and
Chameleon) is shown in Figure 6. The observations are similar to what we found for those gen-
erated graphs based on Cora and Citeseer in Section 3.3.1. Note that the original Squirrel
and Chameleon graphs already have very low homophily, but we still observe a V -shape from
the ﬁgures. This is because there are some neighborhood patterns in the original graphs, which are
distinct from those that we designed for addition. Hence, when we add edges in the early stage,
the performance decreases. As we add more edges, the designed pattern starts to mask the original
patterns and the performance starts to increase."
N,0.503793626707132,Published as a conference paper at ICLR 2022
N,0.5053110773899848,"C.3
GCN’S PERFORMANCE IN THE LIMIT (AS K →∞)"
N,0.5068285280728376,"In this subsection, we illustrate that as K →∞, the accuracy of the GCN model approaches 100%.
Speciﬁcally, we set K to a set of larger numbers as listed in Table 7. Ideally, when K →∞,
the homophily ratio will approach 0 and the model performance will approach 100% (for diverse-
enough Dc). The performance of the GCN model on the graphs described in Table 3 and Table 7
are shown in Figure 7. Clearly, the performance of the GCN model approaches the maximum as we
continue to increase K."
N,0.5083459787556904,"Table 7: Extended # of added edges (K) and homophily ratio (h) values for generated graphs based
on Cora."
N,0.5098634294385432,"K
28084
32096
36108
40120
44132
48144
52156
56168
80240"
N,0.511380880121396,"h
0.272
0.248
0.228
0.211
0.196
0.183
0.172
0.162
0.120"
N,0.5128983308042488,0.81 0.74 0.68 0.63 0.59 0.52 0.46 0.42 0.38 0.32 0.28 0.25 0.22 0.2 0.18 0.17 0.16 0.15 0.14 0.13 0.09
N,0.5144157814871017,Homophily Ratio 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
N,0.5159332321699545,Test Accuracy = 0
N,0.5174506828528073,"Figure 7: As the number of edges approaches K →∞, the homophily ratio h →0, and GCN’s
performance approaches 100%."
N,0.5189681335356601,"C.4
DETAILS OF ALGORITHM 2"
N,0.5204855842185129,"The pseudo code to describe the process to generate graphs with a noise level γ is shown in Al-
gorithm 2. The only difference from Algorithm 1 is in Line 6-7, we randomly add edges if the
generated random number r is smaller than the pre-deﬁned γ (with probability γ)."
N,0.5220030349013657,Alg. 2: Heterophilous Edge Addition with Noise
N,0.5235204855842185,"input : G = {V, E}, K, {Dc}|C|−1
c=0
and {Vc}|C|−1
c=0
output: G′ = {V, E′}
Initialize G′ = {V, E}, k = 1 ;
while 1 ≤k ≤K do"
N,0.5250379362670713,"Sample node i ∼Uniform(V);
Obtain the label, yi of node i;
Sample a number r ∼Uniform(0,1) ;
// Uniform(0,1) denotes the continuous
standard uniform distribution
if r ≤γ then"
N,0.5265553869499241,"Sample a label c ∼Uniform(C \ {yi});
else"
N,0.5280728376327769,"Sample a label c ∼Dyi;
Sample node j ∼Uniform(Vc);
Update edge set E′ = E′ ∪{(i, j)};
k ←k + 1;"
N,0.5295902883156297,"return G′ = {V, E′}"
N,0.5311077389984825,Published as a conference paper at ICLR 2022
N,0.5326251896813353,"D
EXPERIMENTAL DETAILS: DATASETS, MODELS, AND RESULTS"
N,0.5341426403641881,"We compare the standard GCN model (Kipf and Welling, 2016) with several recently proposed
methods speciﬁcally designed for heterophilous graphs including H2GCN (Zhu et al., 2020b), GPR-
GNN (Chien et al., 2021), and CPGNN (Zhu et al., 2020a). A brief introduction of these methods
can be found in Appendix D.2."
N,0.5356600910470409,"D.1
DATASETS"
N,0.5371775417298937,"We give the number of nodes, edges, homophily ratios and distinct classes of datasets we used in
this paper in Table 8."
N,0.5386949924127465,Table 8: Benchmark dataset summary statistics.
N,0.5402124430955993,"Cora
Citeseer
Pubmed
Chameleon
Squirrel
Actor
Cornell
Wisconsin
Texas"
N,0.5417298937784522,"# Nodes (|V|)
2708
3327
19717
2277
5201
7600
183
251
183
# Edges (|E|)
5278
4676
44327
31421
198493
26752
280
466
295
Homophily Ratio (h)
0.81
0.74
0.80
0.23
0.22
0.22
0.3
0.21
0.11
# Classes (|C|)
7
6
3
5
5
5
5
5
5"
N,0.543247344461305,"D.2
MODELS"
N,0.5447647951441578,"• H2GCN (Zhu et al., 2020b) speciﬁcally designed several architectures to deal with heterophilous
graphs, which include ego- and neighbor-embedding separation (skip connection), aggregation
from higher-order neighborhoods, and combination of intermediate representations. We include
two variants H2GCN-1 and H2GCN-2 with 1 or 2 steps of aggregations, respectively. We adopt
the code published by the authors at https://github.com/GemsLab/H2GCN.
• GPR-GNN (Chien et al., 2021) performs feature aggregation for multiple steps and then lin-
early combines the features aggregated with different steps.
The weights of the linear com-
bination are learned during the model training.
Note that it also includes the original fea-
tures before aggregation in the combination. We adopt the code published by the authors at
https://github.com/jianhao2016/GPRGNN.
• CPGNN (Zhu et al., 2020a) incorporates the label compatibility matrix to capture the connec-
tion information between classes. We adopted two variants of CPGNN that utilize MLP and
ChebyNet (Defferrard et al., 2016) as base models to pre-calculate the compatibility matrix, re-
spectively. We use two aggregation layers for both variants. We adopt the code published by the
authors at https://github.com/GemsLab/CPGNN."
N,0.5462822458270106,"D.3
MLP+GCN"
N,0.5477996965098634,"We implement a simple method to linearly combine the learned features from the GCN model and
an MLP model. Let H(2)
GCN ∈R|V|×|C| denote the output features from a 2-layer GCN model, where
|V| and |C| denote the number of nodes and the number of classes, respectively. Similarly, we use
H(2)
MLP ∈R|V|×|C| to denote the features output from a 2-layer MLP model. We then combine them
for classiﬁcation. The process can be described as follows.
H = α · H(2)
GCN + (1 −α) · H(2)
MLP ,
(13)
where α is a hyperparameter balancing the two components. We then apply a row-wise softmax to
each row of H to perform the classiﬁcation."
N,0.5493171471927162,"D.4
PARAMETER TUNING AND RESOURCES USED"
N,0.5508345978755691,"We tune parameters for GCN, GPR-GCN, CPGNN, and MLP+GCN from the following options:"
N,0.5523520485584219,"• learning rate: {0.002, 0.005, 0.01, 0.05}
• weight decay {5e−04, 5e−05, 5e−06, 5e−07, 5e−08, 1e−05, 0}
• dropout rate: {0, 0.2, 0.5, 0.8}."
N,0.5538694992412747,"For GPR-GNN, we use the “PPR” as the initialization for the coefﬁcients. For MLP+GCN, we tune
α from {0.2, 0.4, 0.6, 0.8, 1}. Note that the parameter search range encompasses the range adopted
in the original papers to avoid unfairness issues."
N,0.5553869499241275,Published as a conference paper at ICLR 2022
N,0.5569044006069803,"All experiments are run on a cluster equipped with Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
CPUs and NVIDIA Tesla K80 GPUs."
N,0.5584218512898331,"D.5
MORE RESULTS"
N,0.5599393019726859,"Cora
Citeseer
Pubmed
Chameleon
Squirrel
Actor
Cornell
Wisconsin
Texas"
N,0.5614567526555387,"GCN
87.12 ± 1.38
76.50 ± 1.61
88.52 ± 0.41
67.96 ± 1.82
54.47 ± 1.17
30.31 ± 0.98
59.35 ± 4.19
61.76 ± 6.15
63.81 ± 5.27
MLP
75.04 ± 1.97
72.40 ± 1.97
87.84 ± 0.30
48.11 ± 2.23
31.68 ± 1.90
36.17 ± 1.09
84.86 ± 6.04
86.29 ± 4.50
83.30 ± 4.54
MLP + GCN
87.01 ± 1.35
76.35 ± 1.85
89.77 ± 0.39
68.04 ± 1.86
54.48 ± 1.11
36.24 ± 1.09
84.82 ± 4.87
86.43 ± 4.00
83.60 ± 6.04"
N,0.5629742033383915,"H2GCN-1
86.92 ± 1.37
77.07 ±1.64
89.40 ± 0.34
57.11 ± 1.58
36.42 ± 1.89
35.86 ±1.03
82.16 ± 6.00
86.67 ± 4.69
84.86 ± 6.77
H2GCN-2
87.81 ± 1.35
76.88 ± 1.77
89.59 ± 0.33
59.39 ± 1.98
37.90 ± 2.02
35.62 ± 1.30
82.16 ± 6.00
85.88 ± 4.22
82.16 ± 5.28
CPGNN-MLP
85.84 ± 1.20
74.80 ± 0.92
86.58 ± 0.37
54.53 ± 2.37
29.13 ± 1.57
35.76 ± 0.92
79.93 ± 6.12
84.58 ± 2.72
82.62 ± 6.88
CPGNN-Cheby
87.23 ± 1.31
76.64 ± 1.43
88.41 ± 0.33
65.17 ± 3.17
29.25 ± 4.17
34.28 ± 0.77
75.08 ± 7.51
79.19 ± 2.80
75.96 ± 5.66
GPR-GNN
86.79 ± 1.27
75.55 ± 1.56
86.79 ± 0.55
66.31 ± 2.05
50.56 ± 1.51
33.94 ± 0.95
79.27 ± 6.03
83.73 ± 4.02
84.43 ± 4.10"
N,0.5644916540212443,"E
HEATMAPS FOR OTHER BENCHMARKS"
N,0.5660091047040972,"We provide the heatmaps for Citeseer and Pubmed in Figure 8 and those for Squirrel,
Texas, and Wisconsin in Figure 9. For the Citeseer and Pubmed, which have high ho-
mophily, the observations are similar to those of Cora as we described in Section 4.2.
For
Squirrel, there are some patterns; the intra-class similarity is generally higher than inter-class
similarities. However, these patterns are not very strong, i.e, the differences between them are not
very large, which means that the neighborhood patterns of different labels are not very distinguish-
able from each other. This substantiates the middling performance of GCN on Squirrel. Both
Texas and Wisconsin are very small, with 183 nodes, 295 edges and 251 nodes, 466 edges,
respectively. The average degree is extremely small (< 2). Hence, the similarities presented in
the heatmap may present strong bias. Especially, in Texas, there is only 1 node with label 1. In
Wisconsin, there are only 10 nodes with label 0."
N,0.56752655538695,"0
1
2
3
4
5 0 1 2 3 4 5"
N,0.5690440060698028,"0.65
0.26
0.12
0.14
0.17
0.07"
N,0.5705614567526556,"0.26
0.79
0.18
0.08
0.09
0.05"
N,0.5720789074355084,"0.12
0.18
0.85
0.17
0.06
0.09"
N,0.5735963581183612,"0.14
0.08
0.17
0.85
0.06
0.05"
N,0.575113808801214,"0.17
0.09
0.06
0.06
0.87
0.1"
N,0.5766312594840668,"0.07
0.05
0.09
0.05
0.1
0.87 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.5781487101669196,(a) Citeseer
N,0.5796661608497724,"0.5
0.0
0.5
1.0
1.5
2.0
2.5 0.5 0.0 0.5 1.0 1.5 2.0 2.5"
N,0.5811836115326252,"0.82
0.16
0.24"
N,0.582701062215478,"0.16
0.92
0.19"
N,0.5842185128983308,"0.24
0.19
0.88 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.5857359635811836,(b) Pubmed
N,0.5872534142640364,"Figure 8: Cross-class neighborhood similarity on Citeseer and Pubmed. On both graphs, the
intra-class similarity is clearly higher than the inter-class ones."
N,0.5887708649468892,"F
EXTENDING THEOREM 2 TO MULTIPLE CLASSES"
N,0.590288315629742,"Below, we provide a proof sketch and illustration for an extension to Theorem 2’s main results in a
multi-class context. Speciﬁcally, we consider a special case for more tractable analysis."
N,0.5918057663125948,"Consider a K-class CSBM. The nodes in the generated graphs consist of K disjoint sets of the same
size C1, . . . , CK corresponding to the K classes, respectively. Edges are generated according to an
intra-class probability p and an inter-class probability q. Speciﬁcally, for any two nodes in the graph,
if they are from the same class, then an edge is generated to connect them with probability p, other-
wise, the probability is q. For each node i, its initial associated features xi ∈Rl are sampled from
a Gaussian distribution xi ∼N(µ, I), where µ = µk ∈Rl for i ∈Ck with k ∈{1, . . . , K} and
µz ̸= µw∀z, w ∈{1, . . . , K}. We further assume that the distance between the mean of distribu-
tions corresponding to any two classes is equivalent, i.e, ∥uz −uw∥2 = D, ∀z, w ∈{1, . . . , K},
where D is a positive constant. We illustrate the 3-classes case in Figure 10, where we use the dashed
circles to demonstrate the standard deviation. Note that the K classes of the CSBM model are sym-
metric to each other. Hence, the optimal decision boundary of the K classes are deﬁned by a set"
N,0.5933232169954477,Published as a conference paper at ICLR 2022
N,0.5948406676783005,"0
1
2
3
4 0 1 2 3 4"
N,0.5963581183611533,"0.61
0.63
0.64
0.65
0.66"
N,0.5978755690440061,"0.63
0.7
0.7
0.71
0.72"
N,0.5993930197268589,"0.64
0.7
0.75
0.74
0.75"
N,0.6009104704097117,"0.65
0.71
0.74
0.78
0.77"
N,0.6024279210925645,"0.66
0.72
0.75
0.77
0.8 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.6039453717754173,(a) Squirrel
N,0.6054628224582701,"0
1
2
3
4 0 1 2 3 4"
N,0.6069802731411229,"0.65
0.03
0.61
0.17
0.21"
N,0.6084977238239757,"0.03
1.0
0.1
0.33
0.31"
N,0.6100151745068285,"0.61
0.1
0.68
0.31
0.41"
N,0.6115326251896813,"0.17
0.33
0.31
0.58
0.58"
N,0.6130500758725341,"0.21
0.31
0.41
0.58
0.64 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.6145675265553869,(b) Texas
N,0.6160849772382397,"0
1
2
3
4 0 1 2 3 4"
N,0.6176024279210925,"0.75
0.25
0.27
0.36
0.51"
N,0.6191198786039454,"0.25
0.75
0.71
0.64
0.35"
N,0.6206373292867982,"0.27
0.71
0.72
0.63
0.45"
N,0.622154779969651,"0.36
0.64
0.63
0.69
0.51"
N,0.6236722306525038,"0.51
0.35
0.45
0.51
0.72 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.6251896813353566,(c) Wisconsin
N,0.6267071320182094,"Figure 9: Cross-class neighborhood similarity on Squirrel, Texas and Wisconsin. The inter-
class similarity on Squirrel is slightly higher than intra-class similarity for most classes, which
substantiates the middling performance of GCN. Both Texas and Wisconsin are quite small,
hence the cross-class similarity in these two graphs present severe bias and may not provide precise
information about these graphs."
N,0.6282245827010622,"of
 K
2

hyperplanes (see Figure 10 for an example), where each hyperplane equivalently separates
two classes. Similar to the analysis in the binary case (see the description below Proposition 1),
for any given two classes cz and cw, the hyperplane is P = {x|w⊤x −w⊤(µz + µw)/2} with
w = (uz −uw)/∥uz −uw)∥2, which is orthogonal to (µz −µw) and going through (µz + µw)/2.
For example, in Figure 10, the decision boundaries separate the entire space to 3 areas correspond-
ing to the 3 classes. Each decision boundary is deﬁned by a hyperplane equally separating two
classes. For example, the descision bounadry between c1 and c2 is orthogonal to (µ1 −µ2) and
going through the middle point (µ1 + µ2)/2. Clearly, the linear separability is dependent on the
distance D between the classes and also the standard deviations of each class’ node features. More
speciﬁcally, linear separability is favored by a larger distance D and smaller standard deviation."
N,0.629742033383915,"Figure 10: 3-class CSBM in 2-dimensional space. The dashed circles demonstrate the standard
deviation for each class. The entire space is split to three areas (indicated by different colors) corre-
sponding to the three classes for optimal decision making. The equilateral triangle indicates that the
distance between the means of any two classes is the same."
N,0.6312594840667678,"Next, we discuss how the GCN operation affects the linear separability. Overall, we want to demon-
strate the following: (i) after the GCN operation, the classes are still symmetric to each other; (ii)
the GCN operation will reduce the distance between the classes (in terms of the expectation of the
output embeddings) and reduce the standard deviation of the classes; and (iii) reducing the distance
impairs the separability while reducing the standard deviation improves the separability. Hence, we"
N,0.6327769347496206,Published as a conference paper at ICLR 2022
N,0.6342943854324734,"analyze the two effect and provide a threshold. We describe these three items in a more detailed way
as follows."
N,0.6358118361153262,"• Preservation of Symmetry. For any class cz, z ∈{1, . . . , K}, its neighborhood label distribution
Dcz can be described by a vector where only the z-th element equals
p
p+(K−1)q and all other
elements equal to
q
p+(K−1)q. We consider the aggression process hi =
1
deg(i)
P"
N,0.637329286798179,"j∈N(i)
xj. Then,"
N,0.6388467374810318,"for a node i with label cz, its features obtained after this process follow the following Gaussian
distribution. hi ∼N  
"
N,0.6403641881638846,"pµz +
P"
N,0.6418816388467374,"k∈{1,...K},k̸=z
qµk"
N,0.6433990895295902,"p + (K −1)q
,
I
p"
N,0.644916540212443,deg(i) 
N,0.6464339908952959,"
, for i ∈Cz
(14)"
N,0.6479514415781487,"Speciﬁcally, we denote the expectation of class cz after the GCN operation as Ecz[h] =
pµz+
P"
N,0.6494688922610015,"k∈{1,...K},k̸=z
qµk"
N,0.6509863429438544,"p+(K−1)q
. We next show that the distance between any two classes is the same.
Speciﬁcally, for any two classes cz and cw, after the GCN operation, the distance between their
expectation is as follows."
N,0.6525037936267072,∥Ecz[h] −Eck[h]∥2 = 
N,0.65402124430956,"pµz +
P"
N,0.6555386949924128,"k∈{1,...K},k̸=z
qµk"
N,0.6570561456752656,"p + (K −1)q
−"
N,0.6585735963581184,"pµw +
P"
N,0.6600910470409712,"k∈{1,...K},k̸=w
qµk"
N,0.661608497723824,p + (K −1)q 2
N,0.6631259484066768,"=

(p −q)(µz −µw)"
N,0.6646433990895296,p + (K −1)q
N,0.6661608497723824,"2
=
|p −q|
p + (K −1)q ∥µz −µw∥2
(15)"
N,0.6676783004552352,"Note that we have ∥µz −µw∥2 = D for any pair of classes cz and cw. Hence, after the GCN
operation, the distance between any two classes is still the same. Thus, the classes are symmetric
two each other.
• Reducing inter-class distance and intra-class standard deviation. According to Eq (15), after
the graph convolution operation, the distance between any two classes is reduced by a factor of
|p−q|
p+(K−1)q. On the other hand, according to Eq. (14), the standard deviation depends on the degree
of nodes. Speciﬁcally, for node i with degree deg(i), its standard deviation is reduced by a factor
of
p"
N,0.669195751138088,"deg(i).
• Implications for separability. For a node i, the probability of being mis-classiﬁed is the prob-
ability of its embedding falling out of its corresponding decision area. This probability depends
on both the inter-class distance between classes (in terms of means) and the intra-class standard
deviation. To compare the linear separability before and after the graph convolution operation, we
scale the distance between classes before and after the graph convolution operation to be the same.
Speciﬁcally, we scale hi as h′
i = p+(K−1)q"
N,0.6707132018209409,"|p−q|
hi. Note that classifying hi is equivalent to classifying
h′
i. Based on h′, the distance between any two classes cz, cw is scaled to ∥µz −µw∥2 = D, which
is equivalent to the class distance before the graph convolution operation. Correspondingly, the
standard deviation for h′
i equals to p+(K−1)q"
N,0.6722306525037937,"|p−q|
·
I
√"
N,0.6737481031866465,"deg(i). Now, to compare the mis-classiﬁcation"
N,0.6752655538694993,"probability before and after the graph convolution, we only need to compare the standard devia-
tions as the distances between classes in these two scenarios has been scaled to the same. More
speciﬁcally, when p+(K−1)q"
N,0.6767830045523521,"|p−q|
·
I
√"
N,0.6783004552352049,"deg(i) < 1, the mis-classiﬁcation probability for node i is re-"
N,0.6798179059180577,"duced after the GCN model, otherwise, the mis-classiﬁcation probability is increased after the
GCN model. In other words, for nodes with degree larger than (p+(K−1)q)2"
N,0.6813353566009105,"(p−q)2
, the mis-classiﬁcation
rate can be reduced after the graph convolution operation. This thereold is similar to the one we
developed in Theorem 2 and similar analysis/discussions as those for Theorem 2 follows for the
this multiple-class case."
N,0.6828528072837633,"G
OTHER METRICS THAN COSINE SIMILARITY"
N,0.6843702579666161,"The choice of similarity measure would not affect the results and conclusions signiﬁcantly. We
empirically demonstrate this argument by investigating two other metrics: Euclidean distance and
Hellinger distance. The heatmaps based on these two metrics for Chamelon dataset is shown in
Figure 11 in Appendix G. The patterns demonstrated in these two heatmaps are similar to those"
N,0.6858877086494689,Published as a conference paper at ICLR 2022
N,0.6874051593323217,"observed in Figure 5(b), where cosine similarity is adopted. Note that larger distance means lower
similarity. Hence, the numbers in Figure 11 should be interpreted in the opposite way as those in
Figure 5."
N,0.6889226100151745,"0
1
2
3
4 0 1 2 3 4"
N,0.6904400606980273,"0.59
0.59
0.65
0.64
0.69"
N,0.6919575113808801,"0.59
0.55
0.61
0.61
0.67"
N,0.6934749620637329,"0.65
0.61
0.46
0.47
0.51"
N,0.6949924127465857,"0.64
0.61
0.47
0.42
0.48"
N,0.6965098634294385,"0.69
0.67
0.51
0.48
0.47 0.40 0.45 0.50 0.55 0.60 0.65 0.70"
N,0.6980273141122914,(a) Euclidean Distance
N,0.6995447647951442,"0
1
2
3
4 0 1 2 3 4"
N,0.701062215477997,"1.03
1.05
1.21
1.21
1.25"
N,0.7025796661608498,"1.05
0.98
1.13
1.14
1.2"
N,0.7040971168437026,"1.21
1.13
0.85
0.89
0.93"
N,0.7056145675265554,"1.21
1.14
0.89
0.8
0.88"
N,0.7071320182094082,"1.25
1.2
0.93
0.88
0.86 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25"
N,0.708649468892261,(b) Hellinger distance
N,0.7101669195751138,"Figure 11: Cross-class neighborhood patterns on Chameleon based on different metrics than co-
sine similarity metric."
N,0.7116843702579666,"H
ADDITIONAL EXPERIMENTS WITH DIFFERENT SETTINGS"
N,0.7132018209408194,"In this section, we discuss additional experiments for Section 3.3.1."
N,0.7147192716236722,"H.1
GENERATING GRAPHS WITH OTHER NEIGHBORHOOD DISTRIBUTIONS"
N,0.716236722306525,"In this section, we extend the experiments in Section 3.3.1 by including more patterns for neighbor-
hood distributions. We aim to illustrate if the neighborhood distribution for different classes (labels)
are sufﬁciently distinguishable from each other, we should generally observe similar V -shape curves
as in Figure 3 in Section 3.3.1. However, it is impractical to enumerate all possible neighborhood
distributions. Hence, in this section, we include two additional neighborhood distribution patterns
in Section H.1.1 and two extreme patterns in Section H.1.2."
N,0.7177541729893778,"H.1.1
ADDITIONAL PATTERNS"
N,0.7192716236722306,"Here, for both Cora and Citeseer, we adopt two additional sets of neighborhood distributions
for adding new edges. For convenience, for Cora, we name the two neighborhood distribution
patterns as Cora Neighborhood Distribution Pattern 1 and Cora Neighborhood Distribution Pat-
tern 2. Similarly, for Citeseer, we name the two neighborhood distribution patterns as Citeseer
Neighborhood Distribution Pattern 1 and Citeseer Neighborhood Distribution Pattern 2. We follow
Algorithm 1 to generate graphs while utilizing these neighborhood distributions as the {Dc}|C|−1
c=0
for Algorithm 1."
N,0.7207890743550834,"The two neighborhood distribution patterns for Cora are listed as bellow. Figure 12 and Figure 13
show GCN’s performance on graphs generated from Cora following Cora Neighborhood Distribu-
tion Pattern 1 and Cora Neighborhood Distribution Pattern 2, respectively."
N,0.7223065250379362,Cora Neighborhood Distribution Pattern 1:
N,0.723823975720789,Published as a conference paper at ICLR 2022
N,0.7253414264036419,"D0 : Categorical([0, 1 3, 1 3, 1"
N,0.7268588770864947,"3, 0, 0, 0]),"
N,0.7283763277693475,D1 : Categorical([1
N,0.7298937784522003,"3, 0, 0, 0, 1 3, 1"
N,0.7314112291350531,"3, 0]),"
N,0.7329286798179059,D2 : Categorical([1
N,0.7344461305007587,"3, 0, 0, 0, 0, 1 3, 1 3]),"
N,0.7359635811836115,D3 : Categorical([1
N,0.7374810318664643,"3, 0, 0, 0, 1"
N,0.7389984825493171,"3, 0, 1 3]),"
N,0.7405159332321699,"D4 : Categorical([0, 1"
N,0.7420333839150227,"3, 0, 1"
N,0.7435508345978755,"3, 0, 1"
N,0.7450682852807283,"3, 0]),"
N,0.7465857359635811,"D5 : Categorical([0, 1 3, 1"
N,0.7481031866464339,"3, 0, 1"
N,0.7496206373292867,"3, 0, 0]),"
N,0.7511380880121397,"D6 : Categorical([0, 0, 1 2, 1"
N,0.7526555386949925,"2, 0, 0, 0])."
N,0.7541729893778453,"0.74 0.68 0.63 0.59 0.52 0.46 0.42 0.38 0.32 0.28 0.25 0.22
0.2
0.17 0.12
Homophily Ratio 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
N,0.7556904400606981,Test Accuracy
N,0.7572078907435509,"Figure 12:
Performance of GCN on synthetic
graphs from Cora. Graphs are generated follow-
ing Cora Neighborhood Distribution Pattern 1"
N,0.7587253414264037,Cora Neighborhood Distribution Pattern 2:
N,0.7602427921092565,"D0 : Categorical([0, 1"
N,0.7617602427921093,"2, 0, 1"
N,0.7632776934749621,"2, 0, 0, 0]),"
N,0.7647951441578149,D1 : Categorical([1
N,0.7663125948406677,"2, 0, 0, 1"
N,0.7678300455235205,"2, 0, 0, 0]),"
N,0.7693474962063733,"D2 : Categorical([0, 0, 0, 1, 0, 0, 0]),"
N,0.7708649468892261,"D3 : Categorical([1 5, 1 5, 1"
N,0.7723823975720789,"5, 0, 1 5, 1"
N,0.7738998482549317,"5], 0),"
N,0.7754172989377845,"D4 : Categorical([0, 0, 0, 1"
N,0.7769347496206374,"2, 0, 1"
N,0.7784522003034902,"2, 0]),"
N,0.779969650986343,"D5 : Categorical([0, 1 3, 1"
N,0.7814871016691958,"3, 0, 1"
N,0.7830045523520486,"3, 0, 0]),"
N,0.7845220030349014,"D6 : Categorical([0, 0, 0, 0, 0, 0, 1])."
N,0.7860394537177542,"0.74 0.68 0.63 0.59 0.52 0.46 0.42 0.38 0.32 0.28 0.25 0.22
0.2
0.17 0.12
Homophily Ratio 0.70 0.75 0.80 0.85 0.90 0.95"
N,0.787556904400607,Test Accuracy
N,0.7890743550834598,"Figure 13:
Performance of GCN on synthetic
graphs from Cora. Graphs are generated follow-
ing Cora Neighborhood Distribution Pattern 2
Clearly, the results demonstrated in Figure 12 and Figure 13 are similar to the black curve (γ=0)
in 3a. More speciﬁcally, they present V -shape curves."
N,0.7905918057663126,"The two neighborhood distribution patterns for Citeseer are listed as bellow. Figure 14 and Fig-
ure 15 show GCN’s performance on graphs generated from Citeseer following Citeseer Neigh-
borhood Distribution Pattern 1 and Citeseer Neighborhood Distribution Pattern 2, respectively."
N,0.7921092564491654,Citeseer Neighborhood Distribution Pattern 1:
N,0.7936267071320182,"D0 : Categorical([0, 1 2, 1"
N,0.795144157814871,"2, 0, 0, 0]),"
N,0.7966616084977238,D1 : Categorical([1
N,0.7981790591805766,"3, 0, 0, 0, 1 3, 1 3]),"
N,0.7996965098634294,D2 : Categorical([1
N,0.8012139605462822,"2, 0, 0, 0, 0, 1 2]),"
N,0.802731411229135,"D3 : Categorical([0, 0, 0, 0, 1, 0),"
N,0.8042488619119879,"D4 : Categorical([0, 1"
N,0.8057663125948407,"3, 0, 1"
N,0.8072837632776935,"3, 0, 1 3]),"
N,0.8088012139605463,"D5 : Categorical([0, 1 3, 1"
N,0.8103186646433991,"3, 0, 1"
N,0.8118361153262519,"3, 0]),"
N,0.8133535660091047,"0.65
0.58
0.53
0.48
0.41
0.36
0.32
0.28
0.24
0.2
0.18
0.16
0.14
0.12
Homophily Ratio 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
N,0.8148710166919575,Test Accuracy
N,0.8163884673748103,"Figure 14:
Performance of GCN on synthetic
graphs from Citeseer.
Graphs are gener-
ated following Citeseer Neighborhood Distribu-
tion Pattern 1"
N,0.8179059180576631,Citeseer Neighborhood Distribution Pattern 2:
N,0.8194233687405159,Published as a conference paper at ICLR 2022
N,0.8209408194233687,"D0 : Categorical([0, 1"
N,0.8224582701062215,"2, 0, 1"
N,0.8239757207890743,"2, 0, 0]),"
N,0.8254931714719271,D1 : Categorical([1
N,0.8270106221547799,"2, 0, 0, 1"
N,0.8285280728376327,"32, 0]),"
N,0.8300455235204856,"D2 : Categorical([0, 0, 0, 1, 0, 0]),"
N,0.8315629742033384,"D3 : Categorical([1 5, 1 5, 1"
N,0.8330804248861912,"5, 0, 1 5, 1 5),"
N,0.834597875569044,"D4 : Categorical([0, 0, 0, 1"
N,0.8361153262518968,"2, 0, 1 2]),"
N,0.8376327769347496,"D5 : Categorical([0, 0, 0, 1 2, 1"
N,0.8391502276176024,"2, 0]),"
N,0.8406676783004552,"0.65
0.58
0.53
0.48
0.41
0.36
0.32
0.28
0.24
0.2
0.18
0.16
0.14
0.12
Homophily Ratio 0.65 0.70 0.75 0.80 0.85"
N,0.842185128983308,Test Accuracy
N,0.8437025796661608,"Figure 15:
Performance of GCN on synthetic
graphs from Citeseer.
Graphs are gener-
ated following Citeseer Neighborhood Distribu-
tion Pattern 2"
N,0.8452200303490136,"Clearly, the results demonstrated in Figure 14 and Figure 15 are similar to the black curve (γ=0)
in 3b. More speciﬁcally, they present V -shape curves."
N,0.8467374810318664,"H.1.2
EXTREME NEIGHBORHOOD DISTRIBUTION PATTERNS"
N,0.8482549317147192,"In this subsection, we further extend the experiments in Section 3.3.1 by investigating “extreme”
neighborhood distribution patterns suggested by WXGg. More speciﬁcally, these two patterns are
“a given label are connected to a single different label” and “a given label are connected to all
other labels other excluding its own label”. For convenience, we denote these two types of neigh-
borhood distributions as single and all, respectively. We utilize these neighborhood distributions
as {Dc}|C|−1
c=0
for generating synthetic graphs. Next, we ﬁrst present the results for Cora with
analysis for both the single and all neighborhood distribution patterns. Then, we present the
results for Citeseer but omit analysis and discussion since the observations are similar to
those we make for Cora."
N,0.849772382397572,"Note that to ensure “a label is only connected to a single different label”, we need to group the labels
into pairs and “connect them”. Since there are different ways to group labels into pairs, there exists
various ways to formulate the single neighborhood distributions. We demonstrate one of the single
neighborhood distributions as all the other possible are symmetric to each other. For Cora, the
single neighborhood distribution we adopted is as follows. Note that there are 7 labels in Cora, and
we could not pair all the labels. Thus, we leave one of labels (label 4 in our setting) untouched, i.e,
it is not connect to other labels during the edge addition process.
D0 : Categorical([0, 1, 0, 0, 0, 0, 0]),"
N,0.8512898330804249,"D1 : Categorical([1, 0, 0, 0, 0, 0, 0]),"
N,0.8528072837632777,"D2 : Categorical([0, 0, 0, 1, 0, 0, 0]),"
N,0.8543247344461306,"D3 : Categorical([0, 0, 1, 0, 0, 0, 0]),"
N,0.8558421851289834,"D4 : Categorical([0, 0, 0, 0, 0, 0, 0]),"
N,0.8573596358118362,"D5 : Categorical([0, 0, 0, 0, 0, 0, 1]),"
N,0.858877086494689,"D6 : Categorical([0, 0, 0, 0, 0, 1, 0])."
N,0.8603945371775418,"The GCN’s performance on these graphs generated from Cora following the single neighborhood
distribution pattern is shown in Figure 16a. It clearly presents a V -shape curve. Almost perfect
performance can be achieved when the homophily ratio gets close to 0. This is because the single
neighborhood distributions for different labels are clearly distinguishable from each other. We fur-
ther demonstrate the heatmap of cross-class similarity for the generated graph with homophily ratio
0.07 (the right most point in Figure 16a) in Figure 16b. Clearly, this graph has very high intra-class
similarity and very low inter-class similarity, which explains the good performance."
N,0.8619119878603946,"For Cora, the all neighborhood distribution patterns can be described as follows."
N,0.8634294385432474,"D0 : Categorical([0, 1 6, 1 6, 1 6, 1 6, 1 6, 1 6]),"
N,0.8649468892261002,D1 : Categorical([1
N,0.866464339908953,"6, 0, 1 6, 1 6, 1 6, 1 6, 1 6]),"
N,0.8679817905918058,"D2 : Categorical([1 6, 1"
N,0.8694992412746586,"6, 0, 1 6, 1 6, 1 6, 1 6]),"
N,0.8710166919575114,Published as a conference paper at ICLR 2022
N,0.8725341426403642,"0.81
0.74
0.68
0.59
0.52
0.46
0.38
0.3
0.28
0.22
0.18
0.12
0.09
0.08
0.07
Homophily Ratio 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00"
N,0.874051593323217,Test Accuracy
N,0.8755690440060698,"(a) Performance of GCN on synthetic graphs from Cora. Graphs are gen-
erated following single neighborhood distribution."
N,0.8770864946889226,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
N,0.8786039453717754,"1.0
0.08 0.01
0.0
0.02
0.0
0.0"
N,0.8801213960546282,"0.08
1.0
0.0
0.01 0.06
0.0
0.0"
N,0.881638846737481,"0.01
0.0
1.0
0.14 0.07
0.0
0.0"
N,0.8831562974203339,"0.0
0.01 0.14 0.99 0.01
0.0
0.0"
N,0.8846737481031867,0.02 0.06 0.07 0.01 0.89 0.01 0.01
N,0.8861911987860395,"0.0
0.0
0.0
0.0
0.01
1.0
0.06"
N,0.8877086494688923,"0.0
0.0
0.0
0.0
0.01 0.06
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.8892261001517451,"(b)
Cross-class
neighborhood
similarity for the graph generated
from Cora following single with
homophily ratio 0.07 (the right
most point in Figure 16a)."
N,0.8907435508345979,Figure 16: Cora: single neighborhood distribution pattern
N,0.8922610015174507,"D3 : Categorical([1 6, 1 6, 1"
N,0.8937784522003035,"6, 0, 1 6, 1 6, 1 6]),"
N,0.8952959028831563,"D4 : Categorical([1 6, 1 6, 1 6, 1"
N,0.8968133535660091,"6, 0, 1 6, 1 6]),"
N,0.8983308042488619,"D5 : Categorical([1 6, 1 6, 1 6, 1 6, 1"
N,0.8998482549317147,"6, 0, 1 6]),"
N,0.9013657056145675,"D6 : Categorical([1 6, 1 6, 1 6, 1 6, 1 6, 1"
N,0.9028831562974203,"6, 0])."
N,0.9044006069802731,"The GCN’s performance on these graphs generated from Cora following the all neighborhood
distribution pattern is shown in Figure 17a. Again, it clearly presents a V -shape curve. However, the
performance is not perfectly good even when we add extremely large number of edges. For example,
for the right most point in Figure 17a, we add almost as 50 times many as edges into the graph and
the homophily ratio is reduced to 0.03 while GCN’s performance for it is only around 70%. This
is because the all neighborhood distributions for different labels are not easily distinguishable from
each other. More speciﬁcally, any two neighborhood distributions for different labels in all are very
similar each other (they share “4 labels”). We further empirically demonstrate this by providing the
heatmap of cross-class similarity for the generated graph with homophily ratio 0.03 (the right most
point in Figure 17a) in Figure 17b. As per our discussion in Observation 2, this graph is with not
such “good” heterophily and GCNs cannot produce perfect performance for it. This observation
further demonstrates our key argument that the distinguishability of the distributions for different
labels are important for performance."
N,0.9059180576631259,"For Citeseer, the single neighborhood distribution we adopted is as follows. The GCN’s perfor-
mance on these graphs generated from Citeseer following the single neighborhood distribution
pattern is shown in Figure 18a. The heatmap of cross-class similarity for the generated graph with
homophily ratio 0.06 (the right most point in Figure 18a) in Figure 18b.
D0 : Categorical([0, 1, 0, 0, 0, 0]),"
N,0.9074355083459787,"D1 : Categorical([1, 0, 0, 0, 0, 0]),"
N,0.9089529590288316,"D2 : Categorical([0, 0, 0, 1, 0, 0]),"
N,0.9104704097116844,"D3 : Categorical([0, 0, 1, 0, 0, 0]),"
N,0.9119878603945372,"D4 : Categorical([0, 0, 0, 0, 0, 1]),"
N,0.91350531107739,"D5 : Categorical([0, 0, 0, 0, 1, 0])."
N,0.9150227617602428,"For Citeseer, the all neighborhood distribution patterns can be described as follows. The GCN’s
performance on these graphs generated from Citeseer following the all neighborhood distribu-
tion pattern is shown in Figure 19a. The heatmap of cross-class similarity for the generated graph
with homophily ratio 0.01 (the right most point in Figure 19a) in Figure 19b.
D0 : Categorical([0, 1/5, 1/5, 1/5, 1/5, 1/5]),"
N,0.9165402124430956,Published as a conference paper at ICLR 2022
N,0.9180576631259484,"0.81
0.68
0.59
0.46
0.38
0.28
0.22
0.16
0.15
0.11
0.09
0.08
0.07
0.06
0.05
0.044
0.04
0.037
0.03
Homophily Ratio 0.4 0.5 0.6 0.7 0.8"
N,0.9195751138088012,Test Accuracy
N,0.921092564491654,"(a) Performance of GCN on synthetic graphs from Cora. Graphs are gen-
erated following all neighborhood distribution."
N,0.9226100151745068,"0
1
2
3
4
5
6 0 1 2 3 4 5 6"
N,0.9241274658573596,0.98 0.85 0.85 0.85 0.85 0.85 0.85
N,0.9256449165402124,0.85 0.98 0.85 0.85 0.85 0.86 0.87
N,0.9271623672230652,0.85 0.85 0.97 0.85 0.85 0.85 0.85
N,0.928679817905918,0.85 0.85 0.85 0.95 0.85 0.85 0.85
N,0.9301972685887708,0.85 0.85 0.85 0.85 0.97 0.85 0.85
N,0.9317147192716236,0.85 0.86 0.85 0.85 0.85 0.98 0.86
N,0.9332321699544764,0.85 0.87 0.85 0.85 0.85 0.86 0.99 0.0 0.2 0.4 0.6 0.8 1.0
N,0.9347496206373292,"(b)
Cross-class
neighborhood
similarity for the graph generated
from Cora following all with
homophily ratio 0.03 (the right
most point in Figure 17a)."
N,0.936267071320182,Figure 17: Cora: all neighborhood distribution pattern
N,0.9377845220030349,"0.74
0.58
0.48
0.41
0.36
0.32
0.28
0.26
0.24
0.2
0.18
0.16
0.13
0.12
0.08
0.06
Homophily Ratio 0.6 0.7 0.8 0.9 1.0"
N,0.9393019726858877,Test Accuracy
N,0.9408194233687405,"(a) Performance of GCN on synthetic graphs from Citeseer. Graphs
are generated following single neighborhood distribution."
N,0.9423368740515933,"0
1
2
3
4
5 0 1 2 3 4 5"
N,0.9438543247344461,"1.0
0.07
0.01
0.0
0.0
0.0"
N,0.9453717754172989,"0.07
1.0
0.0
0.01
0.0
0.0"
N,0.9468892261001517,"0.01
0.0
0.99
0.15
0.0
0.0"
N,0.9484066767830045,"0.0
0.01
0.15
1.0
0.0
0.0"
N,0.9499241274658573,"0.0
0.0
0.0
0.0
1.0
0.11"
N,0.9514415781487102,"0.0
0.0
0.0
0.0
0.11
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.952959028831563,"(b)
Cross-class
neighborhood
similarity for the graph generated
from
Citeseer
following
single with homophily ratio 0.06."
N,0.9544764795144158,Figure 18: Citeseer: single neighborhood distribution pattern
N,0.9559939301972686,"D1 : Categorical([1/5, 0, 1/5, 1/5, 1/5, 1/5]),"
N,0.9575113808801214,"D2 : Categorical([1/5, 1/5, 0, 1/5, 1/5, 1/5]),"
N,0.9590288315629742,"D3 : Categorical([1/5, 1/5, 1/5, 0, 1/5, 1/5]),"
N,0.960546282245827,"D4 : Categorical([1/5, 1/5, 1/5, 1/5, 0, 1/5]),"
N,0.9620637329286799,"D5 : Categorical([1/5, 1/5, 1/5, 1/5, 1/5, 0])."
N,0.9635811836115327,"Similar observations as those we made for Cora can be made for Citeseer, hence we do not
repeat the analysis here."
N,0.9650986342943855,"I
LIMITATION"
N,0.9666160849772383,"Though we provide new perspectives and understandings of GCN’s performance on heterophilous
graphs, our work has some limitations. To make the theoretical analysis more feasible, we make a
few assumptions. We dropped the non-linearity in the analysis since the main focus of this paper is
the aggregation part of GCN. While the experiment results empirically demonstrate that our analysis
seems to hold with non-linearity, more formal investigation for GCN with non-linearity is valuable.
Our analysis in Theorem 2 assumes the independence between features, which limits the generality
of the analysis and we would like to conduct further instigation to more general case. We provide
theoretical understanding on GCN’s performance based on CSBM, which stands for a type of graphs"
N,0.9681335356600911,Published as a conference paper at ICLR 2022
N,0.9696509863429439,"0.74
0.58
0.48
0.36
0.28
0.24
0.2
0.16
0.14
0.11
0.09
0.07
0.05
0.04
0.03
0.02
0.02
0.01
0.01
Homophily Ratio 0.3 0.4 0.5 0.6 0.7"
N,0.9711684370257967,Test Accuracy
N,0.9726858877086495,"(a) Performance of GCN on synthetic graphs from Citeseer. Graphs
are generated following all neighborhood distribution."
N,0.9742033383915023,"0
1
2
3
4
5 0 1 2 3 4 5"
N,0.9757207890743551,"1.0
0.81
0.82
0.81
0.82
0.83"
N,0.9772382397572079,"0.81
0.99
0.81
0.8
0.8
0.8"
N,0.9787556904400607,"0.82
0.81
0.99
0.8
0.81
0.81"
N,0.9802731411229135,"0.81
0.8
0.8
0.99
0.81
0.8"
N,0.9817905918057663,"0.82
0.8
0.81
0.81
0.99
0.8"
N,0.9833080424886191,"0.83
0.8
0.81
0.8
0.8
0.99 0.0 0.2 0.4 0.6 0.8 1.0"
N,0.9848254931714719,"(b)
Cross-class
neighborhood
similarity for the graph generated
from Citeseer following all
with homophily ratio 0.01."
N,0.9863429438543247,Figure 19: Citeseer: all neighborhood distribution pattern
N,0.9878603945371776,"attracting increasing attention in the research community. However, it is not ideal for modeling
sparse graphs, which are commonly observed in the real-world. Hence, it is important to devote more
efforts to analyzing more general graphs. We believe our results established a solid initial study for
further investigation. Finally, our current theoretical analysis majorly focuses on the GCN model;
we hope to extend this analysis in the future to more general message-passing neural networks."
N,0.9893778452200304,"J
BROADER IMPACT"
N,0.9908952959028832,"Graph neural networks (GNNs) are a prominent architecture for modeling and understanding graph-
structured data in a variety of practical applications. Most GNNs have a natural inductive bias
towards leveraging graph neighborhood information to make inferences, which can exacerbate unfair
or biased outcomes during inference, especially when such neighborhoods are formed according
to inherently biased upstream processes, e.g. rich-get-richer phenomena and other disparities in
the opportunities to “connect” to other nodes: For example, older papers garner more citations
than newer ones, and are hence likely to have a higher in-degree in citation networks and hence
beneﬁt more from neighborhood information; similar analogs can be drawn for more established
webpages attracting more attention in search results. Professional networking (“ability to connect”)
may be easier for those individuals (nodes) who are at top-tier, well-funded universities compared
to those who are not. Such factors inﬂuence network formation, sparsity, and thus GNN inference
quality simply due to network topology (Tang et al., 2020b). Given these acknowledged issues,
GNNs are still used in applications including ranking (Sankar et al., 2021), recommendation(Jain
and Molino), engagement prediction (Tang et al., 2020a), trafﬁc modeling(Jiang and Luo, 2021),
search and discovery (Ying et al., 2018) and more, and when unchecked, suffer traditional machine
learning unfairness issues (Dai and Wang, 2021)."
N,0.992412746585736,"Despite these practical impacts, the prominent notion in prior literature in this space has been that
such methods are inapplicable or perform poorly on heterophilous graphs, and this may have miti-
gated practitioners’ interests in applying such methods for ML problems in those domains conven-
tionally considered heterophily-dominant (e.g. dating networks). Our work shows that this notion is
misleading, and that heterophily and homophily are not themselves responsible for good or bad in-
ference performance. We anticipate this ﬁnding to be helpful in furthering research into the capacity
of GNN models to work in diverse data settings, and emphasize that our work provides an under-
standing, rather than a new methodology or approach, and thus do not anticipate negative broader
impacts from our ﬁndings."
N,0.9939301972685888,ACKNOWLEDGEMENTS
N,0.9954476479514416,"This research is supported by the National Science Foundation (NSF) under grant numbers
IIS1714741, CNS1815636, IIS1845081, IIS1907704, IIS1928278, IIS1955285, IOS2107215, and"
N,0.9969650986342944,Published as a conference paper at ICLR 2022
N,0.9984825493171472,"IOS2035472, the Army Research Ofﬁce (ARO) under grant number W911NF-21-1-0198, the Home
Depot, Cisco Systems Inc and Snap Inc."
