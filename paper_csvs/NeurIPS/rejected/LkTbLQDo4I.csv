Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00196078431372549,"The ability to design novel proteins with higher ﬁtness on a given task would be
1"
ABSTRACT,0.00392156862745098,"revolutionary for many ﬁelds of medicine. However, brute-force search through
2"
ABSTRACT,0.0058823529411764705,"the combinatorially large space of sequences is infeasible. Prior methods constrain
3"
ABSTRACT,0.00784313725490196,"search to a small mutational radius from a reference sequence, but such heuristics
4"
ABSTRACT,0.00980392156862745,"drastically limit the design space. Our work seeks to remove the restriction on
5"
ABSTRACT,0.011764705882352941,"mutational distance while enabling efﬁcient exploration. We propose Bi-level
6"
ABSTRACT,0.013725490196078431,"Gibbs sampling with Graph-based Smoothing (BiGGS), which uses the gradients
7"
ABSTRACT,0.01568627450980392,"of a trained ﬁtness predictor to sample many mutations towards higher ﬁtness.
8"
ABSTRACT,0.01764705882352941,"Bi-level Gibbs ﬁrst samples sequence locations then sequence edits. We introduce
9"
ABSTRACT,0.0196078431372549,"graph-based smoothing to remove noisy gradients that lead to false positives. Our
10"
ABSTRACT,0.021568627450980392,"method is state-of-the-art in discovering high-ﬁtness proteins with up to 8 mutations
11"
ABSTRACT,0.023529411764705882,"from the training set. We study the GFP and AAV design problems, ablations, and
12"
ABSTRACT,0.025490196078431372,"baselines to elucidate the results.
13"
INTRODUCTION,0.027450980392156862,"1
Introduction
14"
INTRODUCTION,0.029411764705882353,"In protein design, ﬁtness is loosely deﬁned as performance on a desired property or function. Ex-
15"
INTRODUCTION,0.03137254901960784,"amples of ﬁtness include catalytic activity for enzymes [1, 20] and ﬂuorescence for biomarkers [27].
16"
INTRODUCTION,0.03333333333333333,"Protein engineering seeks to design proteins with high ﬁtness by altering the underlying sequences of
17"
INTRODUCTION,0.03529411764705882,"amino acids. However, the number of possible proteins increases exponentially with sequence length,
18"
INTRODUCTION,0.03725490196078431,"rendering it infeasible to perform brute-force search to engineer novel functions which often requires
19"
INTRODUCTION,0.0392156862745098,"many mutations (i.e. at least 3 [11]). Directed evolution [3] has been successful in improving protein
20"
INTRODUCTION,0.041176470588235294,"ﬁtness, but it requires substantial labor and time to gradually explore many mutations.
21"
INTRODUCTION,0.043137254901960784,"We aim to ﬁnd shortcuts to generate high-ﬁtness proteins that are many mutations away from what is
22"
INTRODUCTION,0.045098039215686274,"known but face several challenges. Proteins are notorious for highly non-smooth ﬁtness landscapes:1
23"
INTRODUCTION,0.047058823529411764,"ﬁtness can change dramatically with just a single mutation, and most protein sequences have zero
24"
INTRODUCTION,0.049019607843137254,"ﬁtness [29]. As a result, machine learning (ML) methods are susceptible to learning noisy ﬁtness
25"
INTRODUCTION,0.050980392156862744,"landscapes with false positives [18] and local optimums [6] which poses problems to optimization
26"
INTRODUCTION,0.052941176470588235,"and search. The 3D protein structure, if available, can help provide helpful constraints in navigating
27"
INTRODUCTION,0.054901960784313725,"the noisy ﬁtness landscape, but it cannot be assumed in the majority of cases – current protein folding
28"
INTRODUCTION,0.056862745098039215,"methods typically cannot predict the effects on structure of point mutations [25].
29"
INTRODUCTION,0.058823529411764705,"Our work proposes a sequence-based method that can optimize over a noisy ﬁtness landscape and
30"
INTRODUCTION,0.060784313725490195,"efﬁciently sample large mutational edits. We introduce two methodological advances summarized in
31"
INTRODUCTION,0.06274509803921569,"Figure 1. The ﬁrst is Graph-based Smoothing (GS) that regularizes the noisy landscape. We consider
32"
INTRODUCTION,0.06470588235294118,"it as a noisy graph signal and apply L1 graph Laplacian regularization. This encourages sparsity and
33"
INTRODUCTION,0.06666666666666667,"local consistency in the landscape; most protein sequences have zero ﬁtness, and similar sequences
34"
INTRODUCTION,0.06862745098039216,1Landscape refers to the mapping from sequence to ﬁtness.
INTRODUCTION,0.07058823529411765,"Figure 1: BiGGS overview. (A) Protein engineering is often challenged with a noisy ﬁtness landscape
on which the starting dataset (unblurred) is a fraction of landscape with the highest ﬁtness sequences
hidden (blurred). (B) We develop Graph-based Smoothing (GS) to estimate a smoothed ﬁtness
landscape from the starting data. Intuitively, the gradients allow extrapolation towards higher ﬁtness
sequences. (C) A ﬁtness predictor is trained on the smoothed ﬁtness landscape. (D) Gradients from
the ﬁtness predictor are used in an iterative sampling procedure called Iterative Extrapolation (IE)
where Bi-level Gibbs sampling (BiG) is performed on each step with renewed gradient computations.
(E) Each round of IE samples mutations towards higher ﬁtness."
INTRODUCTION,0.07254901960784314,"have similar ﬁtness [42]. The effect is a smooth ﬁtness landscape learned by the ML model on which
35"
INTRODUCTION,0.07450980392156863,"gradients accurately approximate the direction towards high-ﬁtness sequences. To reach high-ﬁtness
36"
INTRODUCTION,0.07647058823529412,"sequences requiring many mutations, we use the improved gradients in our second advancement,
37"
INTRODUCTION,0.0784313725490196,"Bi-level Gibbs (BiG), to approximate the proposal distribution in a Gibbs sampling procedure – as
38"
INTRODUCTION,0.0803921568627451,"inspired by Gibbs with Gradients (GWG) [12]. BiG uses bi-level sampling to propose up to 5 indices
39"
INTRODUCTION,0.08235294117647059,"to mutate simultaneously. Local improvements from the gradients help select beneﬁcial mutations to
40"
INTRODUCTION,0.08431372549019608,"guide low-ﬁtness sequences towards higher ﬁtness while sampling allows exploration. Following the
41"
INTRODUCTION,0.08627450980392157,"intuition of directed evolution, we apply multiple rounds of sampling over clustered sequences in a
42"
INTRODUCTION,0.08823529411764706,"procedure we call Iterative Extrapolation (IE).
43"
INTRODUCTION,0.09019607843137255,"We ﬁnd BiG and GS are complementary in enabling long-range exploration while avoiding the pitfalls
44"
INTRODUCTION,0.09215686274509804,"of a noisy ﬁtness landscape; the combination of both is referred to as BiGGS. We introduce a set
45"
INTRODUCTION,0.09411764705882353,"of tasks using the Green Fluorescent Proteins (GFP) dataset [30] to simulate challenging protein
46"
INTRODUCTION,0.09607843137254903,"design scenarios by starting with low-ﬁtness sequences that require many (5 or more) mutations to
47"
INTRODUCTION,0.09803921568627451,"the best ﬁtness. We primarily study GFP because of (1) its difﬁculty as one of the longest proteins in
48"
INTRODUCTION,0.1,"ﬁtness datasets and (2) its comprehensive ﬁtness measurements of up to 15 mutations. To assess the
49"
INTRODUCTION,0.10196078431372549,"generalizability of our method, we additionally study the Adeno-Associated Virus (AAV) dataset [7]
50"
INTRODUCTION,0.10392156862745099,"based on gene delivery ﬁtness. We evaluate BiGGS and prior works on our proposed benchmarks
51"
INTRODUCTION,0.10588235294117647,"to show that BiGGS is state-of-the-art in GFP and AAV ﬁtness optimization. Our contributions are
52"
INTRODUCTION,0.10784313725490197,"summarized as follows:
53"
INTRODUCTION,0.10980392156862745,"• We develop a novel sequence-based protein ﬁtness optimization algorithm, BiGGS, based on
54"
INTRODUCTION,0.11176470588235295,"BiG to efﬁciently sample multiple mutations, GS to regularize the ﬁtness landscape, and IE to
55"
INTRODUCTION,0.11372549019607843,"progressively mutate towards higher-ﬁtness (Section 2).
56"
INTRODUCTION,0.11568627450980393,"• We study GFP by proposing a set of design benchmarks of different difﬁculty with varying
57"
INTRODUCTION,0.11764705882352941,"starting sequence distribution (Section 3). While our focus is GFP, we develop benchmarks on
58"
INTRODUCTION,0.11960784313725491,"AAV to evaluate a new ﬁtness criteria (Appendix C).
59"
INTRODUCTION,0.12156862745098039,"• We show BiGGS is state-of-the-art in GFP and AAV ﬁtness optimization while exhibiting diversity
60"
INTRODUCTION,0.12352941176470589,"and novelty from the training set. We analyze the contributions of BiG and GS towards successful
61"
INTRODUCTION,0.12549019607843137,"ﬁtness optimization over challenging ﬁtness landscapes (Section 5).
62"
METHOD,0.12745098039215685,"2
Method
63"
METHOD,0.12941176470588237,"We begin with the problem formulation in Section 2.1. Our method uses two components bi-level
64"
METHOD,0.13137254901960785,"Gibbs sampling (Section 2.2) and graph-based smoothing (Section 2.3). Together they are part of
65"
METHOD,0.13333333333333333,"a iterative sampling method called iterative extrapolation (Section 2.4) as a way to progressively
66"
METHOD,0.13529411764705881,"extrapolate towards novel sequences. The full algorithm, BiGGS, is presented in Algorithm 1.
67"
PROBLEM FORMULATION,0.13725490196078433,"2.1
Problem formulation
68"
PROBLEM FORMULATION,0.1392156862745098,"Let the starting set of length L protein sequences and their ﬁtness measurements be denoted as
69"
PROBLEM FORMULATION,0.1411764705882353,"D0 = (X0, Y0) where X0 ⇢VL with vocabulary V = {1, . . . , 20} and Y0 ⇢R. We use subscripts
70"
PROBLEM FORMULATION,0.14313725490196078,"to distinguish sequences, xi 2 VL, while a paranthetical subcript denotes the token, (xi)j 2 V where
71"
PROBLEM FORMULATION,0.1450980392156863,"j 2 {1, . . . , L}. Note our method can readily be extended to other modalities, e.g. nucleic acids.
72"
PROBLEM FORMULATION,0.14705882352941177,"For in-silico evaluation, we denote the set of all known sequences and ﬁtness measurements as
73"
PROBLEM FORMULATION,0.14901960784313725,"D⇤= (X ⇤, Y⇤). We assume there exists a black-box function g : VL ! R such that g(x⇤) = y⇤,
74"
PROBLEM FORMULATION,0.15098039215686274,"which is approximated by an oracle gφ. In practice, the oracle is a model trained with weights φ to
75"
PROBLEM FORMULATION,0.15294117647058825,"minimize prediction error on D⇤. The starting dataset only includes low ﬁtness sequences and is a
76"
PROBLEM FORMULATION,0.15490196078431373,"strict subset of the oracle dataset D0 ⇢D⇤to simulate ﬁtness optimization scenarios. Given D0, our
77"
PROBLEM FORMULATION,0.1568627450980392,"task is to generate a set of sequences ˆ
X = {ˆxi}Nsamples"
PROBLEM FORMULATION,0.1588235294117647,"i=1
with higher ﬁtness than the starting set.
78"
PROBLEM FORMULATION,0.1607843137254902,"2.2
BiG: Bi-level Gibbs (with Gradients)
79"
PROBLEM FORMULATION,0.1627450980392157,"To generate new sequences, we propose a modiﬁed version of Gibbs With Gradients (GWG) [12].
80"
PROBLEM FORMULATION,0.16470588235294117,"The ﬁrst step is to train a ﬁtness predictor, f✓: VL ! R, using D0 to act as the learned unnormalized
81"
PROBLEM FORMULATION,0.16666666666666666,"probability (i.e. negative energy) from sequence to ﬁtness. We use the Mean-Squared Error (MSE)
82"
PROBLEM FORMULATION,0.16862745098039217,"loss to train the predictor which we parameterize as a deep neural network. We found it beneﬁcial to
83"
PROBLEM FORMULATION,0.17058823529411765,"employ negative data augmentation since both the dataset and the range of ﬁtness values are small.
84"
PROBLEM FORMULATION,0.17254901960784313,"Speciﬁcally, we double the size of the dataset by sampling random sequences, xneg"
PROBLEM FORMULATION,0.17450980392156862,"i
⇠Uniform(VL),
85"
PROBLEM FORMULATION,0.17647058823529413,"and assigning them the lowest possible ﬁtness value, µ.
86"
PROBLEM FORMULATION,0.1784313725490196,"Our goal is to sample from log p(x) = f✓(x) −log Z where Z is the normalization constant. Higher
87"
PROBLEM FORMULATION,0.1803921568627451,"ﬁtness sequences will be more likely under this distribution while sampling over many mutations will
88"
PROBLEM FORMULATION,0.18235294117647058,"induce diversity and novelty. GWG uses Gibbs sampling with locally informed proposals:
89"
PROBLEM FORMULATION,0.1843137254901961,qr(x0|x) / e
PROBLEM FORMULATION,0.18627450980392157,(x0)>d✓(x)
PROBLEM FORMULATION,0.18823529411764706,"2
1(x0 2 H(x)),
d✓(x)ij = rxf✓(x)ij −xT"
PROBLEM FORMULATION,0.19019607843137254,"i rf✓(x)i,
(1)"
PROBLEM FORMULATION,0.19215686274509805,"where d✓(x)ij is a ﬁrst order Taylor approximation of the log-likelihood ratio of mutating the ith
90"
PROBLEM FORMULATION,0.19411764705882353,"index of x to token j. Treating x, x0 as one-hot, (x0)>d✓(x) = P i(x0"
PROBLEM FORMULATION,0.19607843137254902,"i)>d✓(x)i is the sum over the
91"
PROBLEM FORMULATION,0.1980392156862745,"local differences where x0 differs from x. The proposal q(x0|x) can be efﬁciently computed when
92"
PROBLEM FORMULATION,0.2,"H(·) is the 1-Hamming ball2: a single backward pass is needed to compute the Jacobian in eq. (1).
93"
PROBLEM FORMULATION,0.2019607843137255,"Sampling M > 1 mutations in the same fashion would require estimating the gradients for each
94"
PROBLEM FORMULATION,0.20392156862745098,"mutation individually resulting in exponentially more computations. Instead, we ﬁnd a simple bi-level
95"
PROBLEM FORMULATION,0.20588235294117646,"sampling scheme to be effective. The ﬁrst level samples mutation indices, `m, with a categorical
96"
PROBLEM FORMULATION,0.20784313725490197,"tempered-softmax distribution over the column-wise maxima, d✓(x)i = maxj2{1,...,L} d✓(x)ij. The
97"
PROBLEM FORMULATION,0.20980392156862746,"second level samples token-wise mutations (x0)`m over the vocabulary the same way as the ﬁrst level
98"
PROBLEM FORMULATION,0.21176470588235294,"using d✓(x)`mj.
99"
PROBLEM FORMULATION,0.21372549019607842,First level: `m
PROBLEM FORMULATION,0.21568627450980393,"iid
⇠q(·|x) = Cat "
PROBLEM FORMULATION,0.21764705882352942,Softmax
PROBLEM FORMULATION,0.2196078431372549,⇢d✓(x)i ⌧ $L i=1 !!
PROBLEM FORMULATION,0.22156862745098038,",
m 2 {1, . . . , M}"
PROBLEM FORMULATION,0.2235294117647059,"Second level: (x0)`m ⇠q(·|x, `m) = Cat "
PROBLEM FORMULATION,0.22549019607843138,Softmax
PROBLEM FORMULATION,0.22745098039215686,⇢d✓(x)`mj ⌧ $|V| j=1
PROBLEM FORMULATION,0.22941176470588234,"!!
(2)"
PROBLEM FORMULATION,0.23137254901960785,"where ⌧is a temperature hyperparameter. Indices are sampled iid which means the same index may
100"
PROBLEM FORMULATION,0.23333333333333334,"get sampled twice. An improvement left for future work is to model conditional dependencies across
101"
PROBLEM FORMULATION,0.23529411764705882,2Deﬁned as a ball using the hamming distance.
PROBLEM FORMULATION,0.2372549019607843,"locations. Each proposed sequence is accepted or rejected using Metropolis-Hasting (MH)
102 min "
PROBLEM FORMULATION,0.23921568627450981,exp(f✓(x0) −f✓(x)) QM
PROBLEM FORMULATION,0.2411764705882353,"m=1 q((x0)`m|x, `m)q(`m|x)
QM"
PROBLEM FORMULATION,0.24313725490196078,"m=1 q((x)`m|x0, `m)q(`m|x0) , 1 ! .
(3)"
PROBLEM FORMULATION,0.24509803921568626,"To summarize, our method Bi-level Gibbs (BiG) ﬁrst samples Nprop sequences each with up to M
103"
PROBLEM FORMULATION,0.24705882352941178,"mutations from eq. (2) then returns a set of accepted sequences, X 0, according to eq. (3). Forcing BiG
104"
PROBLEM FORMULATION,0.24901960784313726,"to make M mutations may make it skip sequences that are less than M mutations away. We found it
105"
PROBLEM FORMULATION,0.25098039215686274,"best to run BiG over all values leading up to M. The full algorithm is provided in algorithm 2.
106"
PROBLEM FORMULATION,0.2529411764705882,"A concern is the accuracy of the 1st order Taylor approximation, d✓(x)ij, for M > 1. We observed
107"
PROBLEM FORMULATION,0.2549019607843137,"the performance of BiG is highly dependent on the performance of the predictor for gradients that
108"
PROBLEM FORMULATION,0.2568627450980392,"correlate with higher ﬁtness. The next two sections focus on the development of a robust predictor
109"
PROBLEM FORMULATION,0.25882352941176473,"(Section 2.3) and an iterative framework to improve the Gibbs sampling approximation (Section 2.4).
110"
PROBLEM FORMULATION,0.2607843137254902,"2.3
GS: Graph-based smoothing
111"
PROBLEM FORMULATION,0.2627450980392157,"The efﬁcacy of the gradients in BiG to guide sampling towards high ﬁtness sequences depends on
112"
PROBLEM FORMULATION,0.2647058823529412,"the smoothness of the mapping from sequence to ﬁtness learned by the predictor. Unfortunately, the
113"
PROBLEM FORMULATION,0.26666666666666666,"high-dimensional sequence space coupled with few data points and noisy labels results in a noisy
114"
PROBLEM FORMULATION,0.26862745098039215,"predictor that is prone to sampling false positives [18] or getting stuck in local optima [6]. To address
115"
PROBLEM FORMULATION,0.27058823529411763,"this, we use techniques from graph signal processing to smooth the learned mapping by promoting
116"
PROBLEM FORMULATION,0.2725490196078431,"similar sequences to have similar ﬁtness [42] while penalizing noisy predictions [17].
117"
PROBLEM FORMULATION,0.27450980392156865,"Suppose we have trained a noisy predictor with weights ✓0 on the initial dataset D0. To construct
118"
PROBLEM FORMULATION,0.27647058823529413,"our graph G = (V, E), we ﬁrst construct the nodes V by iteratively applying pointwise mutations
119"
PROBLEM FORMULATION,0.2784313725490196,"to each sequence in the initial set X0 to simulate a local landscape around each sequence. We call
120"
PROBLEM FORMULATION,0.2803921568627451,"this routine Perturb with a hyperparameter Nperturb for the number of perturbations per sequence
121"
PROBLEM FORMULATION,0.2823529411764706,"(see Algorithm 5). The edges, E, are a nearest neighbor graph with Nneigh neighbors where edge
122"
PROBLEM FORMULATION,0.28431372549019607,"weights are inversely proportional to their sequence distance, !ij = !((vi, vj)) = 1/dist(vi, vj);
123"
PROBLEM FORMULATION,0.28627450980392155,"edge weights are stored in a similarity matrix W = {!ij 8vi, vj 2 V }.
124"
PROBLEM FORMULATION,0.28823529411764703,"The normalized Laplacian matrix of G is L = I −D−1/2WD−1/2 where I is the identity and D
125"
PROBLEM FORMULATION,0.2901960784313726,is a diagonal matrix with i-th diagonal element Dii = P
PROBLEM FORMULATION,0.29215686274509806,"j !ij. An eigendecomposition of L gives
126"
PROBLEM FORMULATION,0.29411764705882354,"L = U⌃U T where ⌃is a diagonal matrix with sorted eigenvalues along the diagonal and U is a
127"
PROBLEM FORMULATION,0.296078431372549,"matrix of corresponding eigenvectors along the columns. An equivalent eigendecomposition with
128"
PROBLEM FORMULATION,0.2980392156862745,"symmetric matrix B (and edges E arranged into an adjacency matrix) is
129"
PROBLEM FORMULATION,0.3,"L = (⌃1/2U T )T ⌃1/2U T = BT B,
B = ⌃1/2U T .
Next, we formulate smoothing as an optimization problem. For each node, we predict its ﬁtness
130"
PROBLEM FORMULATION,0.30196078431372547,"S = {f✓0(v) 8v 2 V }, also called the graph signal, which we assume to have noisy values. Our goal
131"
PROBLEM FORMULATION,0.30392156862745096,"is to solve the following where S is arranged as a vector and S⇤is the smoothed signal,
132"
PROBLEM FORMULATION,0.3058823529411765,"S⇤= arg min ˆ
S"
PROBLEM FORMULATION,0.307843137254902,"kB ˆSk1 + γk ˆS −Sk1
(4)"
PROBLEM FORMULATION,0.30980392156862746,"Equation (4) is a form of graph Laplacian regularization that has been studied for image segmentation
133"
PROBLEM FORMULATION,0.31176470588235294,"with weak labels [17]. B has eigenvalue weighted eigenvectors as rows. Due to the L1-norm kB ˆSk1
134"
PROBLEM FORMULATION,0.3137254901960784,"is small if ˆS is primarily aligned with slowly varying eigenvectors whose eigenvalues are small. This
135"
PROBLEM FORMULATION,0.3156862745098039,"term penalizes large jumps in ﬁtness between neighboring nodes hence we call it smoothness sparsity
136"
PROBLEM FORMULATION,0.3176470588235294,"constraint. The second term, k ˆS −Sk1, is the signal sparsity constraint that remove noisy predictions
137"
PROBLEM FORMULATION,0.3196078431372549,"with hyperparameter γ. The L1-norm is applied to reﬂect that most sequences have zero ﬁtness.
138"
PROBLEM FORMULATION,0.3215686274509804,"At a high level, eq. (4) is solved by introducing auxiliary variables which allows for an approximate
139"
PROBLEM FORMULATION,0.3235294117647059,"solution by solving multiple LASSO regularization problems [34]. Technical details and algorithm
140"
PROBLEM FORMULATION,0.3254901960784314,"are described in Appendix B. Once we have S⇤, we retrain our predictor with the smoothed dataset
141"
PROBLEM FORMULATION,0.32745098039215687,"D = (V, S⇤) on which the learned predictor is smoother with gradients much more amenable for
142"
PROBLEM FORMULATION,0.32941176470588235,"gradient-based sampling, BiG. We refer to our smoothing algorithm as Graph-based Smoothing (GS).
143"
PROBLEM FORMULATION,0.33137254901960783,"2.4
IE: Iterative Extrapolation
144"
PROBLEM FORMULATION,0.3333333333333333,"The 1st order Taylor approximation of eq. (1) deteriorates the more we mutate from the parent
145"
PROBLEM FORMULATION,0.3352941176470588,"sequence. Inspired by directed evolution [3], we propose to alleviate this by performing multiple
146"
PROBLEM FORMULATION,0.33725490196078434,"rounds of sampling where successive rounds use sequences from the previous round. Each round
147"
PROBLEM FORMULATION,0.3392156862745098,"re-centers the Taylor approximation and extrapolates from the previous round. We ﬁrst train a
148"
PROBLEM FORMULATION,0.3411764705882353,"predictor f✓using GS (Section 2.3). Prior to sampling, we observe the number of sequences may be
149"
PROBLEM FORMULATION,0.3431372549019608,"large and redundant. To reduce the number of sequences, we perform hierarchical clustering [22] and
150"
PROBLEM FORMULATION,0.34509803921568627,"take the sequence of each cluster with the highest ﬁtness using f✓. Let C be the number of clusters.
151"
PROBLEM FORMULATION,0.34705882352941175,Reduce( {X c}C
PROBLEM FORMULATION,0.34901960784313724,c=1; ✓) = C[ c=1
PROBLEM FORMULATION,0.3509803921568627,{arg max
PROBLEM FORMULATION,0.35294117647058826,"x2X c
f✓(x)} where {X c}C"
PROBLEM FORMULATION,0.35490196078431374,c=1 = Cluster(X; C).
PROBLEM FORMULATION,0.3568627450980392,"Each round r reduces the sequences from the previous round and performs BiG sampling.
152 X 0 r+1 = ["
PROBLEM FORMULATION,0.3588235294117647,"x2 ˜
Xr"
PROBLEM FORMULATION,0.3607843137254902,"BiG(x; ✓),
˜
Xr = Reduce({X c r }C"
PROBLEM FORMULATION,0.3627450980392157,"c=1; ✓),
{X c r }C"
PROBLEM FORMULATION,0.36470588235294116,c=1 = ClusterX 0
PROBLEM FORMULATION,0.36666666666666664,r+1(X 0
PROBLEM FORMULATION,0.3686274509803922,r; C).
PROBLEM FORMULATION,0.37058823529411766,"One cycle of clustering, reducing, and sampling is a round of extrapolation,
153 X 0"
PROBLEM FORMULATION,0.37254901960784315,r+1 = Extrapolate(X 0
PROBLEM FORMULATION,0.37450980392156863,"r; ✓, C)
(5)"
PROBLEM FORMULATION,0.3764705882352941,where the initial round r = 0 starts with X 0
PROBLEM FORMULATION,0.3784313725490196,"0 = X0. After R rounds, we select our candidate sequences
154"
PROBLEM FORMULATION,0.3803921568627451,"by taking the Top-Nsamples sequences based on ranking with f✓. We call this procedure Iterative
155"
PROBLEM FORMULATION,0.38235294117647056,"Extrapolation (IE). While IE is related to previous directed evolution methods [31], it differs by
156"
PROBLEM FORMULATION,0.3843137254901961,"taking larger mutational edits on each round with BiG and encouraging diversity by mutating the best
157"
PROBLEM FORMULATION,0.3862745098039216,"sequence of each cluster. The full candidate generation, Bi-level Gibbs with Graph-based Smoothing
158"
PROBLEM FORMULATION,0.38823529411764707,"(BiGGS), with IE is presented in Algorithm 1.
159"
PROBLEM FORMULATION,0.39019607843137255,"Algorithm 1 BiGGS: Bi-level Gibbs with Graph-based Smoothing
Require: Starting dataset: D0 = (X0, Y0)
Require: BiG hyperparameters: Nprop, ⌧, M
Require: GS hyperparameters: Nneigh, Nperturb, γ
Require: IE hyperparameters: Nsamples, R, C"
PROBLEM FORMULATION,0.39215686274509803,1: D  D0 [ {(xneg
PROBLEM FORMULATION,0.3941176470588235,"i , µ)}|D0|"
PROBLEM FORMULATION,0.396078431372549,"i=1
. Construct negative data
2: ✓0  arg max˜✓E(x,y)⇠D ⇥"
PROBLEM FORMULATION,0.3980392156862745,(y −f˜✓(x))2⇤
PROBLEM FORMULATION,0.4,". Initial training.
3: ✓ Smooth(X0; ✓0)
. GS Algorithm 3.
4: {X0}C"
PROBLEM FORMULATION,0.4019607843137255,"c=1  Cluster(X0; C)
. Initial round of IE
5:
˜
X c"
PROBLEM FORMULATION,0.403921568627451,0  Reduce({X0}C
PROBLEM FORMULATION,0.40588235294117647,"c=1; ✓)
6: X 0"
PROBLEM FORMULATION,0.40784313725490196,"0  [x2 ˜
X c"
PROBLEM FORMULATION,0.40980392156862744,"0 BiG(x; ✓)
. BiG algorithm 2
7: for r = 1, . . . , R do
8:
X 0"
PROBLEM FORMULATION,0.4117647058823529,r  Extrapolate(X 0
PROBLEM FORMULATION,0.4137254901960784,"r−1; ✓)
. Remaining rounds of IE eq. (5)
9: end for
10:
ˆ
X  TopK([R"
PROBLEM FORMULATION,0.41568627450980394,r=1X 0
PROBLEM FORMULATION,0.4176470588235294,"r)
. Return Top-Nsamples sequences based on predicted ﬁtness f✓.
11: Return ˆ
X"
BENCHMARKS,0.4196078431372549,"3
Benchmarks
160"
BENCHMARKS,0.4215686274509804,"We use the Green Fluoresent Protein (GFP) dataset from Sarkisyan et al. [30] containing over 56,806
161"
BENCHMARKS,0.4235294117647059,"log ﬂuorescent ﬁtness measurements, with 51,715 unique amino-acid sequences due to sequences
162"
BENCHMARKS,0.42549019607843136,"having multiple measurements. We quantify the difﬁculty of a protein ﬁtness optimization task by
163"
BENCHMARKS,0.42745098039215684,"introducing the concept of a mutational gap, which we deﬁne as the minimum Levenshtein distance
164"
BENCHMARKS,0.4294117647058823,"between any sequence in the training set to any sequence in the 99th percentile:
165"
BENCHMARKS,0.43137254901960786,"Gap(X0; X 99th) = min({dist(x, ˜x) : x 2 X, ˜x 2 X 99th})"
BENCHMARKS,0.43333333333333335,"A mutational gap of 0 means that the training set, D0 may contain sequences that are in the 99th
166"
BENCHMARKS,0.43529411764705883,"percentile of ﬁtness. Solving such tasks is easy because methods may sample high-ﬁtness sequences
167"
BENCHMARKS,0.4372549019607843,"from the training set. Prior work commonly uses the GFP task introduced by design-bench (DB)
168"
BENCHMARKS,0.4392156862745098,"evaluation framework [36] which has a mutational gap of 0 (see Appendix A). To compare to previous
169"
BENCHMARKS,0.4411764705882353,"work, we include the DB task as ""easy"" difﬁculty in our experiments, but we introduce ""medium""
170"
BENCHMARKS,0.44313725490196076,"and ""hard"" optimization tasks which have lower starting ﬁtness ranges in the 20-40th and 10-30th
171"
BENCHMARKS,0.44509803921568625,"percentile of known ﬁtness measurements alongside much higher mutational gaps. Our proposed
172"
BENCHMARKS,0.4470588235294118,"difﬁculties are summarized in Table 1 and visualized in Figure 5.
173"
BENCHMARKS,0.44901960784313727,Table 1: Proposed GFP tasks
BENCHMARKS,0.45098039215686275,"Difﬁculty
Range (%)
|D0|
Gap"
BENCHMARKS,0.45294117647058824,"Medium
20th-40th
2828
6
Hard
10th-30th
1636
7"
BENCHMARKS,0.4549019607843137,"The oracle in design-bench (DB) uses a Transformer-
174"
BENCHMARKS,0.4568627450980392,"based architecture from Rao et al. [26]. When using this
175"
BENCHMARKS,0.4588235294117647,"oracle, we noticed a concerning degree of false positives
176"
BENCHMARKS,0.46078431372549017,"and a thresholding effect of its predictions. We propose
177"
BENCHMARKS,0.4627450980392157,"a simpler CNN architecture as the oracle that achieves
178"
BENCHMARKS,0.4647058823529412,"superior performance in terms of Spearman correlation
179"
BENCHMARKS,0.4666666666666667,"and fewer false positives as seen in Figure 6. Our CNN
180"
BENCHMARKS,0.46862745098039216,"consists of a 1D convolutional layer that takes in a one-hot encoded sequence, followed by max-
181"
BENCHMARKS,0.47058823529411764,"pooling and a dense layer to a single node that outputs a scalar value. It uses 256 channels throughout
182"
BENCHMARKS,0.4725490196078431,"for a total of 157,000 parameters – 15 fold fewer than DB oracle.
183"
BENCHMARKS,0.4745098039215686,"Our experiments in Section 5 benchmark on GFP easy, medium, and hard with our CNN oracle. In
184"
BENCHMARKS,0.4764705882352941,"Appendix C we summarize an additional benchmark using Adeno-Associated Virus (AAV) dataset
185"
BENCHMARKS,0.47843137254901963,"[7] which focuses on optimizing a 28-amino acid segment for DNA delivery. We use the same task
186"
BENCHMARKS,0.4803921568627451,"set-up and train our CNN oracle on AAV.
187"
RELATED WORK,0.4823529411764706,"4
Related work
188"
RELATED WORK,0.4843137254901961,"Optimization in protein design. Approaches in protein design can broadly be categorized in
189"
RELATED WORK,0.48627450980392156,"using sequence, structure or both [9]. Advances in structure-based protein design have been driven
190"
RELATED WORK,0.48823529411764705,"by a combination of geometric deep learning and generative models [37, 13, 39, 8]. Sequence-
191"
RELATED WORK,0.49019607843137253,"based protein design has been explored through the lens of reinforcement learning [2, 16], latent
192"
RELATED WORK,0.492156862745098,"space optimization [32, 16, 19], GFlowNets [14], bayesian optimization [38], generative models
193"
RELATED WORK,0.49411764705882355,"[6, 5, 23, 21], and model-based directed evolution [31, 4, 24, 28, 35]. Together they face the common
194"
RELATED WORK,0.49607843137254903,"issue of a noisy landscape to optimize over. Moreover, ﬁtness labels are problem-dependent and
195"
RELATED WORK,0.4980392156862745,"scarce, apart from well-studied proteins [5]. Our method addresses small amounts of starting data
196"
RELATED WORK,0.5,"and noisy landscape by regularization with GS. We focus on sequence-based methods where we use
197"
RELATED WORK,0.5019607843137255,"locally informed Markov Chain Monte Carlo (MCMC) methods [40] method based on Gibbs With
198"
RELATED WORK,0.503921568627451,"Gradients (GWG) [12] which requires a smooth energy function for strong performance guarantees.
199"
RELATED WORK,0.5058823529411764,"Concurrently, Emami et al. [10] used GWG to sample higher ﬁtness sequences by optimizing over
200"
RELATED WORK,0.5078431372549019,"a product of experts distribution, a mixture of a protein language model and a ﬁtness predictor.
201"
RELATED WORK,0.5098039215686274,"However, they eschewed the need for a smooth energy function which we address with GS.
202"
RELATED WORK,0.5117647058823529,"Discrete MCMC. High-dimensional discrete MCMC can be inefﬁcient with slow mixing times.
203"
RELATED WORK,0.5137254901960784,"GWG showed discrete MCMC becomes practical by utilizing learned gradients in the sampling
204"
RELATED WORK,0.515686274509804,"distribution, but GWG in its published form was limited to sampling in a proposal window of size 1.
205"
RELATED WORK,0.5176470588235295,"Zhang et al. [41] proposed to modify GWG with langevin dynamics to allow for the whole sequence to
206"
RELATED WORK,0.5196078431372549,"mutate on every step while Sun et al. [33] augmented GWG with a path auxiliary proposal distribution
207"
RELATED WORK,0.5215686274509804,"to propose a series of local moves before accepting or rejecting. We ﬁnd that BiGGS with bi-level
208"
RELATED WORK,0.5235294117647059,"sampling is simpler and effective in achieving a proposal window size beyond 1.
209"
EXPERIMENTS,0.5254901960784314,"5
Experiments
210"
EXPERIMENTS,0.5274509803921569,"We study the performance of BiGGS on the GFP tasks from Section 3. Furthermore, to ensure
211"
EXPERIMENTS,0.5294117647058824,"that we did not over-optimize to the GFP dataset, we benchmark BiGGS using AAV benchmark in
212"
EXPERIMENTS,0.5313725490196078,"Appendix C. In the subsequent sections, we outline our experiments on GFP, while corresponding
213"
EXPERIMENTS,0.5333333333333333,"results for AAV are in Appendix C. Section 5.1 compares the performance of BiGGS on GFP to
214"
EXPERIMENTS,0.5352941176470588,"a representative set of baselines while Section 5.2 performs ablations on components of BiGGS.
215"
EXPERIMENTS,0.5372549019607843,"Finally, Section 5.3 analyzes BiGGS’s performance.
216"
EXPERIMENTS,0.5392156862745098,"BiGGS training and sampling. Following section 3, we use the oracle CNN architecture for our
217"
EXPERIMENTS,0.5411764705882353,"predictor (but trained on different data). To ensure a fair comparison, we use the same predictor
218"
EXPERIMENTS,0.5431372549019607,"across all model-based baselines. We use the following hyperparameters as input to Algorithm 1
219"
EXPERIMENTS,0.5450980392156862,"across all tasks: Nprop = 100, ⌧= 0.01, M = 5, Nneigh = 500, Nperturb = 1000 Nsamples = 128
220"
EXPERIMENTS,0.5470588235294118,"R = 3, C = 500. We were unable to perform extensive exploration of hyperparameters. Reducing
221"
EXPERIMENTS,0.5490196078431373,"the number of hyperparameters and ﬁnding optimal values is an important future direction. Training
222"
EXPERIMENTS,0.5509803921568628,"is performed with batch size 1024, ADAM optimizer [15] (with β1 = 0.9, β2 = 0.999), learning
223"
EXPERIMENTS,0.5529411764705883,"rate 0.0001, and 1000 epochs using a single A6000 Nvidia GPU. Initial predictor training takes 10
224"
EXPERIMENTS,0.5549019607843138,"minutes while graph-based smoothing takes around 30 minutes depending on convergence of the
225"
EXPERIMENTS,0.5568627450980392,"numerical solvers. Training with the smoothed data takes 4 to 8 hours. Sampling takes under 30
226"
EXPERIMENTS,0.5588235294117647,"minutes and can be parallelized.
227"
EXPERIMENTS,0.5607843137254902,"Baselines. We choose a representative set of prior works with publicly available code: GFlowNets
228"
EXPERIMENTS,0.5627450980392157,"(GFN-AL) [14], model-based adaptive sampling (CbAS) [6], greedy search (AdaLead) [31], bayesian
229"
EXPERIMENTS,0.5647058823529412,"optimization with quasi-expected improvement acquisition function (BO-qei) [38], conservative
230"
EXPERIMENTS,0.5666666666666667,"model-based optimization (CoMs) [35], and proximal exploration (PEX) [28].
231"
EXPERIMENTS,0.5686274509803921,"Metrics. Each method generates Nsamples = 128 samples ˆ
X = {ˆxi}Nsamples"
EXPERIMENTS,0.5705882352941176,"i=1
to evaluate. Here, dist is
232"
EXPERIMENTS,0.5725490196078431,"the Levenshtein distance. We report three metrics:
233"
EXPERIMENTS,0.5745098039215686,• (Normalized) Fitness = median({⇠(ˆxi; Y⇤)}Nsamples
EXPERIMENTS,0.5764705882352941,"i=1
) where ⇠(ˆx; Y⇤) =
gφ(ˆxi)−min(Y⇤)
max(Y⇤)−min(Y⇤) is the
234"
EXPERIMENTS,0.5784313725490197,"min-max normalized ﬁtness.
235"
EXPERIMENTS,0.5803921568627451,"• Diversity = mean({dist(x, ˜x) : x, ˜x 2 ˆ
X, x 6= ˜x}) is the average sample similarity.
236"
EXPERIMENTS,0.5823529411764706,• Novelty = median({⌘(ˆxi; X0)}Nsamples
EXPERIMENTS,0.5843137254901961,"i=1
) where ⌘(x; X0) = min({dist(x, ˜x) : ˜x 2 X ⇤, ˜x 6= x})
237"
EXPERIMENTS,0.5862745098039216,"is the minimum distance of sample x to any of the starting sequences X0.
238"
EXPERIMENTS,0.5882352941176471,"We use median for outlier robustness. Diversity and novelty were introduced in Jain et al. [14]. We
239"
EXPERIMENTS,0.5901960784313726,"emphasize that higher diversity and novelty is not equivalent to better performance. For instance, a
240"
EXPERIMENTS,0.592156862745098,"random algorithm would achieve maximum diversity and novelty.
241"
RESULTS,0.5941176470588235,"5.1
Results
242"
RESULTS,0.596078431372549,"All methods are evaluated on 128 generated candidates, as done in design-bench. We run 5 seeds and
243"
RESULTS,0.5980392156862745,"report the average metric across all seeds including the standard deviation in parentheses. Results
244"
RESULTS,0.6,"using our GFP oracle are summarized in table 2. Results using the DB oracle are in appendix C.
245"
RESULTS,0.6019607843137255,Table 2: GFP optimization results (our oracle).
RESULTS,0.6039215686274509,"GFP Task
Method"
RESULTS,0.6058823529411764,"Difﬁculty Metric GFN-AL
CbAS
Adalead
BO-qei
CoMs
PEX
BiGGS Easy"
RESULTS,0.6078431372549019,"Fit.
0.16 (0.0) 0.81 (0.0) 0.92 (0.0) 0.77 (0.0) 0.06 (0.3)
0.71 (0.0) 0.92 (0.0)
Div.
27.9 (2.0)
4.5 (0.4)
2.1 (0.2)
5.9 (0.0)
129 (16)
2.2 (0.1)
2.2 (0.0)
Nov.
215 (2.9)
1.4 (0.5)
1.0 (0.0)
0.0 (0.0)
164 (80)
1.0 (0.0)
1.0 (0.0)"
RESULTS,0.6098039215686275,Medium
RESULTS,0.611764705882353,"Fit.
0.13 (0.0) 0.21 (0.0) 0.53 (0.0) 0.17 (0.0) -0.1 (0.0)
0.51 (0.0) 0.86 (0.0)
Div.
30.9 (2.7)
9.2 (1.5)
9.3 (0.1)
20.1 (7.1) 142 (15.5)
2.0 (0.0)
4.0 (0.2)
Nov.
214 (3.3)
7.0 (0.7)
1.0 (0.0)
0.0 (0.0)
190 (10.5)
1.0 (0.0)
5.9 (0.2) Hard"
RESULTS,0.6137254901960785,"Fit.
0.17 (0.0) -0.08 (0.0) 0.03 (0.0) 0.01 (0.0) -0.1 (0.2) -0.11 (0.0) 0.43 (0.0)
Div.
29.3 (2.2)
98.7 (16)
6.6 (0.6)
84.0 (7.1)
140 (7.1)
2.0 (0.0)
4.1 (0.1)
Nov.
212 (2.0)
46.2 (9.4)
1.0 (0.0)
0.0 (0.0)
198 (2.9)
1.0 (0.0)
7.0 (0.0)"
RESULTS,0.615686274509804,"BiGGS substantially outperforms other baselines on the medium and hard difﬁculties, consistently
246"
RESULTS,0.6176470588235294,"navigating the mutational to achieve high ﬁtness, while maintaining diversity and novelty from the
247"
RESULTS,0.6196078431372549,"training set. The unique extrapolation capabilities of BiGGS on the hardest difﬁculty level warranted
248"
RESULTS,0.6215686274509804,"additional analysis, and we investigate this further in Section 5.3. Adalead overall performed second-
249"
RESULTS,0.6235294117647059,"best, matching the performance of BiGGS on the easy difﬁculty with PEX only slightly worse.
250"
RESULTS,0.6254901960784314,"Notably, both Adalead and PEX suffer from a low novelty in the medium and hard settings.
251"
RESULTS,0.6274509803921569,"Regarding the other baselines, GFN-AL exhibits subpar performance across all difﬁculty levels. We
252"
RESULTS,0.6294117647058823,"were unable to reproduce their published results.3 Its performance notably deteriorates on medium
253"
RESULTS,0.6313725490196078,"and hard difﬁculty levels, a trend common amongst all baselines. CbAS explores very far, making on
254"
RESULTS,0.6333333333333333,"average 46 mutations, resulting in poor ﬁtness. BO-qei is unable to extrapolate beyond the training
255"
RESULTS,0.6352941176470588,"set, and CoMs presents instability, as indicated by their high standard deviations, and collapse.4
256"
RESULTS,0.6372549019607843,"We further analyze the distribution of novelty and ﬁtness among CbAS, Adalead, and our method,
257"
RESULTS,0.6392156862745098,"BiGGS, in Figure 2. Adalead tends to be conservative, while CbAS is excessively liberal. BiGGS, on
258"
RESULTS,0.6411764705882353,"3We contacted the authors but there was no resolution. Lee et al. [16] also were unable to reproduce GFN-AL.
4CoMs managed to generate only between 7 and 65 unique sequences."
RESULTS,0.6431372549019608,"Figure 2: Comparison of GFP novelty and ﬁtness on samples from AdaLead, BiGGS, and CbAS.
From left to right, we observe increasing exploration behaviour from the respective methods. However,
only BiGGS maintains high ﬁtness while exploring the novel sequences. Nearly all samples from
CbAS on hard are beyond 10 novelty and have very low ﬁtness."
RESULTS,0.6450980392156863,"the other hand, manages to ﬁnd the middle ground, displaying high ﬁtness in its samples while also
259"
RESULTS,0.6470588235294118,"effectively exploring across the mutational gap at each difﬁculty level.
260"
ABLATIONS,0.6490196078431373,"5.2
Ablations
261"
ABLATIONS,0.6509803921568628,"We perform ablations on each component of BiGGS on the hard difﬁculty task. In the ﬁrst ablation,
262"
ABLATIONS,0.6529411764705882,"we replace BiG with GWG but use an equivalent number of samples by running R = 15 of IE for a
263"
ABLATIONS,0.6549019607843137,"fair comparison. The second ablation removes GS and starts sampling after initial predictor training.
264"
ABLATIONS,0.6568627450980392,"The last ablation removes iterative extrapolation by setting R = 1, M = 15, Nsample = 300 which
265"
ABLATIONS,0.6588235294117647,maintains the number of samples but without iterative rounds. Our results are shown in Table 3. We
ABLATIONS,0.6607843137254902,Table 3: Ablation results (our oracle).
ABLATIONS,0.6627450980392157,"Difﬁculty Metric
BiGGS
with GWG without IE without GS Hard"
ABLATIONS,0.6647058823529411,"Fitness
0.43 (0.0)
0.38 (0.0)
0.21 (0.0)
0.0 (0.0)
Diversity
4.1 (0.1)
4.0 (0.1)
8.3 (0.1)
18.4 (0.6)
Novelty
7.0 (0.0)
7.1 (0.2)
4.0 (0.0)
6.0 (0.0) 266"
ABLATIONS,0.6666666666666666,"see GS is crucial for BiGGS on the hard difﬁculty level. Additional analysis is provided in section 5.3.
267"
ABLATIONS,0.6686274509803921,"Removing IE also results in a large decrease in performance. Unsurprisingly, GWG greatly beneﬁts
268"
ABLATIONS,0.6705882352941176,"from GS and IE due to its similarity with BiG. However, using BiG results in improved ﬁtness. We
269"
ABLATIONS,0.6725490196078432,"conclude each component of BiGGS contributes to its performance.
270"
ANALYSIS,0.6745098039215687,"5.3
Analysis
271"
ANALYSIS,0.6764705882352942,"We analyze BiGGS in the hard GFP task and demonstrate that (1) GS results in gradients from BiG
272"
ANALYSIS,0.6784313725490196,"that point towards higher ﬁtness sequences and (2) BiG’s ability to sample large mutations (M ≥3)
273"
ANALYSIS,0.6803921568627451,"enables efﬁcient traversal of large mutational distances in a high dimensional space.
274"
ANALYSIS,0.6823529411764706,"Figure 3A, B shows how GS leads to a smooth ﬁtness landscape, enabling BiG to sample high-ﬁtness
275"
ANALYSIS,0.6843137254901961,"mutations. Often but not always, GS allows BiGGS to assign high probability to higher-ﬁtness
276"
ANALYSIS,0.6862745098039216,"mutations that are low-probability without GS. We use the GFP wildtype (WT) as a representative of
277"
ANALYSIS,0.6882352941176471,"high-ﬁtness sequences in the 99th percentile. The smoothed f✓(ﬁg. 3A) assigns high probability to
278"
ANALYSIS,0.6901960784313725,"the mutation that changes the current residue to the WT residue at a given proposed position, while
279"
ANALYSIS,0.692156862745098,"Figure 3: Analysis of BiGGS for Hard Task. (A, B) Proposed mutation probability of WT residue
vs. non-WT residues for subsequently accepted mutations with and without GS. The non-smoothed
predictor gives the WT residue only slightly higher probability than other residues. (C) Single vs.
quadruple mutations accepted by BiGGS. Quadruple mutations lead to more large improvements."
ANALYSIS,0.6941176470588235,"giving low probability to other (lower ﬁtness) mutations. The non-smoothed predictor proposes to
280"
ANALYSIS,0.696078431372549,"mutate the current residue to the WT residue only slightly more often than other mutations (ﬁg. 3B).
281"
ANALYSIS,0.6980392156862745,"In Figure 3C, we show that BiGGS’s ability to consider large mutations (M ≥3) facilitates efﬁcient
282"
ANALYSIS,0.7,"exploration. We use the oracle to analyze all single (M = 1) and quadruple (M = 4) mutations
283"
ANALYSIS,0.7019607843137254,"accepted during the course of running BiGGS. We choose M = 4 as it represents the largest portion
284"
ANALYSIS,0.703921568627451,"of BiGGS-accepted mutations among large mutations (M ≥3). According to the oracle, the largest
285"
ANALYSIS,0.7058823529411765,"quadruple mutation ﬁtness increases are bigger than the largest single mutation ﬁtness increases.
286"
ANALYSIS,0.707843137254902,"Quadruple mutations also result in a greater number of substantial ﬁtness increases. We note a
287"
ANALYSIS,0.7098039215686275,"somewhat larger count of substantially negative mutations for quadruple mutations vs. for single
288"
ANALYSIS,0.711764705882353,"mutations. This is expected given BiGGS’s stochasticity, and the tendency of large mutations to be
289"
ANALYSIS,0.7137254901960784,"more deleterious than small ones. Similar analysis for M up to 5 is in Appendix D.
290"
DISCUSSION,0.7156862745098039,"6
Discussion
291"
DISCUSSION,0.7176470588235294,"In this work, we presented BiGGS, a method for optimizing protein ﬁtness by incorporating ideas
292"
DISCUSSION,0.7196078431372549,"from MCMC, graph Laplacian regularization, and directed evolution. We outlined a new benchmark
293"
DISCUSSION,0.7215686274509804,"on GFP that introduces the challenge of starting with poor-ﬁtness sequences, many edits from the top
294"
DISCUSSION,0.7235294117647059,"ﬁtness sequences. BiGGS discovered higher ﬁtness sequences than in the starting set, even in the hard
295"
DISCUSSION,0.7254901960784313,"difﬁculty of our benchmark where prior methods struggled. We analyzed the two methodological
296"
DISCUSSION,0.7274509803921568,"advancements, Graph-based Smoothing (GS) and Bi-level Gibbs (BiG) (which includes Iterative
297"
DISCUSSION,0.7294117647058823,"Extrapolation), as well as ablations to conclude each of these techniques aided BiGGS’s performance.
298"
DISCUSSION,0.7313725490196078,"There are multiple extensions of BiGGS. The ﬁrst is to improve BiG by removing the independence
299"
DISCUSSION,0.7333333333333333,"assumption across residues and instead modeling joint probabilities of epistatic interactions. One
300"
DISCUSSION,0.7352941176470589,"possibility for learning epistatic interactions is to incorporate 3D structure information (if available) to
301"
DISCUSSION,0.7372549019607844,"bias the sampling distribution. Secondly, the effectiveness of GS in our ablations warrants additional
302"
DISCUSSION,0.7392156862745098,"exploration into better regularization techniques for protein ﬁtness predictors. Our formulation of GS
303"
DISCUSSION,0.7411764705882353,"is slow due to the nearest neighbor graph construction and its L1 optimization. Lastly, investigating
304"
DISCUSSION,0.7431372549019608,"BiGGS to handle variable length sequences, multiple objectives, and multiple rounds of optimization
305"
DISCUSSION,0.7450980392156863,"is of high importance towards real protein engineering problems. Our code is included in the
306"
DISCUSSION,0.7470588235294118,"supplementary data and will be publicly available upon acceptance.
307"
REFERENCES,0.7490196078431373,"References
308"
REFERENCES,0.7509803921568627,"[1] Dave W Anderson, Florian Baier, Gloria Yang, and Nobuhiko Tokuriki. The adaptive landscape
309"
REFERENCES,0.7529411764705882,"of a metallo-enzyme is shaped by environment-dependent epistasis. Nature Communications,
310"
REFERENCES,0.7549019607843137,"12(1):3867, 2021.
311"
REFERENCES,0.7568627450980392,"[2] Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and
312"
REFERENCES,0.7588235294117647,"Lucy Colwell. Model-based reinforcement learning for biological sequence design. 2020.
313"
REFERENCES,0.7607843137254902,"[3] Frances H Arnold. Design by directed evolution. Accounts of chemical research, 31(3):125–131,
314"
REFERENCES,0.7627450980392156,"1998.
315"
REFERENCES,0.7647058823529411,"[4] Frances H Arnold. Directed evolution: bringing new chemistry to life. Angewandte Chemie
316"
REFERENCES,0.7666666666666667,"International Edition, 57(16):4143–4148, 2018.
317"
REFERENCES,0.7686274509803922,"[5] Surojit Biswas, Grigory Khimulya, Ethan C Alley, Kevin M Esvelt, and George M Church.
318"
REFERENCES,0.7705882352941177,"Low-n protein engineering with data-efﬁcient deep learning. Nature methods, 18(4):389–396,
319"
REFERENCES,0.7725490196078432,"2021.
320"
REFERENCES,0.7745098039215687,"[6] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling
321"
REFERENCES,0.7764705882352941,"for robust design. In International conference on machine learning, pages 773–782. PMLR,
322"
REFERENCES,0.7784313725490196,"2019.
323"
REFERENCES,0.7803921568627451,"[7] Drew H Bryant, Ali Bashir, Sam Sinai, Nina K Jain, Pierce J Ogden, Patrick F Riley, George M
324"
REFERENCES,0.7823529411764706,"Church, Lucy J Colwell, and Eric D Kelsic. Deep diversiﬁcation of an aav capsid protein by
325"
REFERENCES,0.7843137254901961,"machine learning. Nature Biotechnology, 39(6):691–696, 2021.
326"
REFERENCES,0.7862745098039216,"[8] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F
327"
REFERENCES,0.788235294117647,"Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep
328"
REFERENCES,0.7901960784313725,"learning–based protein sequence design using proteinmpnn. Science, 378(6615):49–56, 2022.
329"
REFERENCES,0.792156862745098,"[9] Wenze Ding, Kenta Nakai, and Haipeng Gong. Protein design via deep learning. Brieﬁngs in
330"
REFERENCES,0.7941176470588235,"bioinformatics, 23(3):bbac102, 2022.
331"
REFERENCES,0.796078431372549,"[10] Patrick Emami, Aidan Perreault, Jeffrey Law, David Biagioni, and Peter St John. Plug & play
332"
REFERENCES,0.7980392156862746,"directed evolution of proteins with gradient-based discrete mcmc. Machine Learning: Science
333"
REFERENCES,0.8,"and Technology, 4(2):025014, 2023.
334"
REFERENCES,0.8019607843137255,"[11] Mahan Ghafari and Daniel B. Weissman. The expected time to cross extended ﬁtness plateaus.
335"
REFERENCES,0.803921568627451,"Theoretical Population Biology, 129:54–67, 2019. ISSN 0040-5809. doi: https://doi.org/
336"
REFERENCES,0.8058823529411765,"10.1016/j.tpb.2019.03.008. URL https://www.sciencedirect.com/science/article/
337"
REFERENCES,0.807843137254902,"pii/S0040580918301011. Special issue in honor of Marcus Feldman’s 75th birthday.
338"
REFERENCES,0.8098039215686275,"[12] Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris Maddison. Oops
339"
REFERENCES,0.8117647058823529,"i took a gradient: Scalable sampling for discrete distributions. In International Conference on
340"
REFERENCES,0.8137254901960784,"Machine Learning, pages 3831–3841. PMLR, 2021.
341"
REFERENCES,0.8156862745098039,"[13] John Ingraham, Max Baranov, Zak Costello, Vincent Frappier, Ahmed Ismail, Shan Tie, Wujie
342"
REFERENCES,0.8176470588235294,"Wang, Vincent Xue, Fritz Obermeyer, Andrew Beam, et al. Illuminating protein space with a
343"
REFERENCES,0.8196078431372549,"programmable generative model. bioRxiv, pages 2022–12, 2022.
344"
REFERENCES,0.8215686274509804,"[14] Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure
345"
REFERENCES,0.8235294117647058,"F. P. Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang,
346"
REFERENCES,0.8254901960784313,"Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. In
347"
REFERENCES,0.8274509803921568,"Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,
348"
REFERENCES,0.8294117647058824,"editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of
349"
REFERENCES,0.8313725490196079,"Proceedings of Machine Learning Research, pages 9786–9801. PMLR, 17–23 Jul 2022. URL
350"
REFERENCES,0.8333333333333334,"https://proceedings.mlr.press/v162/jain22a.html.
351"
REFERENCES,0.8352941176470589,"[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
352"
REFERENCES,0.8372549019607843,"arXiv:1412.6980, 2014.
353"
REFERENCES,0.8392156862745098,"[16] Minji Lee, Luiz Felipe Vecchietti, Hyunkyu Jung, Hyunjoo Ro, Meeyoung Cha, and Ho Min
354"
REFERENCES,0.8411764705882353,"Kim. Protein sequence design in a latent space via model-based reinforcement learning.
355"
REFERENCES,0.8431372549019608,"[17] Zhiwu Lu, Zhenyong Fu, Tao Xiang, Peng Han, Liwei Wang, and Xin Gao. Learning from
356"
REFERENCES,0.8450980392156863,"weak and noisy labels for semantic segmentation. IEEE transactions on pattern analysis and
357"
REFERENCES,0.8470588235294118,"machine intelligence, 39(3):486–500, 2016.
358"
REFERENCES,0.8490196078431372,"[18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
359"
REFERENCES,0.8509803921568627,"Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
360"
REFERENCES,0.8529411764705882,"2017.
361"
REFERENCES,0.8549019607843137,"[19] Natalie Maus, Haydn Jones, Juston Moore, Matt J Kusner, John Bradshaw, and Jacob Gardner.
362"
REFERENCES,0.8568627450980392,"Local latent space bayesian optimization over structured inputs. Advances in Neural Information
363"
REFERENCES,0.8588235294117647,"Processing Systems, 35:34505–34518, 2022.
364"
REFERENCES,0.8607843137254902,"[20] Stanislav Mazurenko, Zbynek Prokop, and Jiri Damborsky. Machine learning in enzyme
365"
REFERENCES,0.8627450980392157,"engineering. ACS Catalysis, 10(2):1210–1223, 2019.
366"
REFERENCES,0.8647058823529412,"[21] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language
367"
REFERENCES,0.8666666666666667,"models enable zero-shot prediction of the effects of mutations on protein function. Advances in
368"
REFERENCES,0.8686274509803922,"Neural Information Processing Systems, 34:29287–29303, 2021.
369"
REFERENCES,0.8705882352941177,"[22] Daniel Müllner. Modern hierarchical, agglomerative clustering algorithms. arXiv preprint
370"
REFERENCES,0.8725490196078431,"arXiv:1109.2378, 2011.
371"
REFERENCES,0.8745098039215686,"[23] Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez,
372"
REFERENCES,0.8764705882352941,"Debora Marks, and Yarin Gal. Tranception: protein ﬁtness prediction with autoregressive
373"
REFERENCES,0.8784313725490196,"transformers and inference-time retrieval. In International Conference on Machine Learning,
374"
REFERENCES,0.8803921568627451,"pages 16990–17017. PMLR, 2022.
375"
REFERENCES,0.8823529411764706,"[24] Vishakh Padmakumar, Richard Yuanzhe Pang, He He, and Ankur P Parikh. Extrapolative
376"
REFERENCES,0.884313725490196,"controlled sequence generation via iterative reﬁnement. arXiv preprint arXiv:2303.04562, 2023.
377"
REFERENCES,0.8862745098039215,"[25] Marina A Pak, Karina A Markhieva, Mariia S Novikova, Dmitry S Petrov, Ilya S Vorobyev,
378"
REFERENCES,0.888235294117647,"Ekaterina S Maksimova, Fyodor A Kondrashov, and Dmitry N Ivankov. Using alphafold
379"
REFERENCES,0.8901960784313725,"to predict the impact of single mutations on protein stability and function. Plos one, 18(3):
380"
REFERENCES,0.8921568627450981,"e0282689, 2023.
381"
REFERENCES,0.8941176470588236,"[26] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter
382"
REFERENCES,0.8960784313725491,"Abbeel, and Yun S Song. Evaluating protein transfer learning with tape. In Advances in Neural
383"
REFERENCES,0.8980392156862745,"Information Processing Systems, 2019.
384"
REFERENCES,0.9,"[27] S James Remington. Green ﬂuorescent protein: a perspective. Protein Science, 20(9):1509–
385"
REFERENCES,0.9019607843137255,"1519, 2011.
386"
REFERENCES,0.903921568627451,"[28] Zhizhou Ren, Jiahan Li, Fan Ding, Yuan Zhou, Jianzhu Ma, and Jian Peng. Proximal exploration
387"
REFERENCES,0.9058823529411765,"for model-guided protein sequence design. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
388"
REFERENCES,0.907843137254902,"Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International
389"
REFERENCES,0.9098039215686274,"Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,
390"
REFERENCES,0.9117647058823529,"pages 18520–18536. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/
391"
REFERENCES,0.9137254901960784,"v162/ren22a.html.
392"
REFERENCES,0.9156862745098039,"[29] Philip A Romero and Frances H Arnold. Exploring protein ﬁtness landscapes by directed
393"
REFERENCES,0.9176470588235294,"evolution. Nature reviews Molecular cell biology, 10(12):866–876, 2009.
394"
REFERENCES,0.9196078431372549,"[30] Karen S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S
395"
REFERENCES,0.9215686274509803,"Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov,
396"
REFERENCES,0.9235294117647059,"Onuralp Soylemez, et al. Local ﬁtness landscape of the green ﬂuorescent protein. Nature, 533
397"
REFERENCES,0.9254901960784314,"(7603):397–401, 2016.
398"
REFERENCES,0.9274509803921569,"[31] Sam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric D
399"
REFERENCES,0.9294117647058824,"Kelsic. Adalead: A simple and robust adaptive greedy search algorithm for sequence design.
400"
REFERENCES,0.9313725490196079,"arXiv preprint arXiv:2010.02141, 2020.
401"
REFERENCES,0.9333333333333333,"[32] Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton
402"
REFERENCES,0.9352941176470588,"Greenside, and Andrew Gordon Wilson. Accelerating bayesian optimization for biological
403"
REFERENCES,0.9372549019607843,"sequence design with denoising autoencoders. arXiv preprint arXiv:2203.12742, 2022.
404"
REFERENCES,0.9392156862745098,"[33] Haoran Sun, Hanjun Dai, Wei Xia, and Arun Ramamurthy. Path auxiliary proposal for mcmc in
405"
REFERENCES,0.9411764705882353,"discrete space. In International Conference on Learning Representations, 2022.
406"
REFERENCES,0.9431372549019608,"[34] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
407"
REFERENCES,0.9450980392156862,"Statistical Society: Series B (Methodological), 58(1):267–288, 1996.
408"
REFERENCES,0.9470588235294117,"[35] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective
409"
REFERENCES,0.9490196078431372,"models for effective ofﬂine model-based optimization. In Marina Meila and Tong Zhang,
410"
REFERENCES,0.9509803921568627,"editors, Proceedings of the 38th International Conference on Machine Learning, volume 139
411"
REFERENCES,0.9529411764705882,"of Proceedings of Machine Learning Research, pages 10358–10368. PMLR, 18–24 Jul 2021.
412"
REFERENCES,0.9549019607843138,"URL https://proceedings.mlr.press/v139/trabucco21a.html.
413"
REFERENCES,0.9568627450980393,"[36] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Bench-
414"
REFERENCES,0.9588235294117647,"marks for data-driven ofﬂine model-based optimization. CoRR, abs/2202.08450, 2022. URL
415"
REFERENCES,0.9607843137254902,"https://arxiv.org/abs/2202.08450.
416"
REFERENCES,0.9627450980392157,"[37] Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E.
417"
REFERENCES,0.9647058823529412,"Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M.
418"
REFERENCES,0.9666666666666667,"Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Shefﬂer, Jue Wang, Preetham
419"
REFERENCES,0.9686274509803922,"Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile
420"
REFERENCES,0.9705882352941176,"Mathieu, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David
421"
REFERENCES,0.9725490196078431,"Baker. Broadly applicable and accurate protein design by integrating structure prediction
422"
REFERENCES,0.9745098039215686,"networks and diffusion generative models. bioRxiv, 2022.
423"
REFERENCES,0.9764705882352941,"[38] James T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparame-
424"
REFERENCES,0.9784313725490196,"terization trick for acquisition functions. arXiv preprint arXiv:1712.00424, 2017.
425"
REFERENCES,0.9803921568627451,"[39] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina
426"
REFERENCES,0.9823529411764705,"Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone
427"
REFERENCES,0.984313725490196,"generation. arXiv preprint arXiv:2302.02277, 2023.
428"
REFERENCES,0.9862745098039216,"[40] Giacomo Zanella. Informed proposals for local mcmc in discrete spaces. Journal of the
429"
REFERENCES,0.9882352941176471,"American Statistical Association, 115(530):852–865, 2020.
430"
REFERENCES,0.9901960784313726,"[41] Ruqi Zhang, Xingchao Liu, and Qiang Liu. A langevin-like sampler for discrete distributions.
431"
REFERENCES,0.9921568627450981,"International Conference on Machine Learning, 2022.
432"
REFERENCES,0.9941176470588236,"[42] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Schölkopf.
433"
REFERENCES,0.996078431372549,"Learning with local and global consistency. Advances in neural information processing systems,
434"
REFERENCES,0.9980392156862745,"16, 2003.
435"
