Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037313432835820895,"Despite their success and popularity, deep neural networks (DNNs) are vulnerable
when facing backdoor attacks. This impedes their wider adoption, especially in
mission critical applications. This paper tackles the problem of Trojan detection,
namely, identifying Trojaned models – models trained with poisoned data. One
popular approach is reverse engineering, i.e., recovering the triggers on a clean
image by manipulating the model’s prediction. One major challenge of reverse
engineering approach is the enormous search space of triggers. To this end, we
propose innovative priors such as diversity and topological simplicity to not only
increase the chances of ﬁnding the appropriate triggers but also improve the quality
of the found triggers. Moreover, by encouraging a diverse set of trigger candidates,
our method can perform effectively in cases with unknown target labels. We
demonstrate that these priors can signiﬁcantly improve the quality of the recovered
triggers, resulting in substantially improved Trojan detection accuracy as validated
on both synthetic and publicly available TrojAI benchmarks."
INTRODUCTION,0.007462686567164179,"1
INTRODUCTION"
INTRODUCTION,0.011194029850746268,"Deep learning has achieved superior performance in various computer vision tasks, such as image
classiﬁcation (Krizhevsky et al., 2012), image segmentation (Long et al., 2015), object detection (Gir-
shick et al., 2014), etc. However, the vulnerability of DNNs against backdoor attacks raises serious
concerns. In this paper, we address the problem of Trojan attacks, where during training, an attacker
injects polluted samples. While resembling normal samples, these polluted samples contain a speciﬁc
type of perturbation (called triggers). These polluted samples are assigned with target labels, which
are usually different from the expected class labels. Training with this polluted dataset results in a
Trojaned model. At the inference stage, a Trojaned model behaves normally given clean samples. But
when the trigger is present, it makes unexpected, yet consistently incorrect predictions."
INTRODUCTION,0.014925373134328358,"One major constraint for Trojan detection is the limited access to the polluted training data. In
practice, the end-users, who need to detect the Trojaned models, often only have access to the weights
and architectures of the trained DNNs. State-of-the-art (SOTA) Trojan detection methods generally
adopt a reverse engineering approach (Guo et al., 2019; Wang et al., 2019; Huster & Ekwedike,
2021; Wang et al., 2020b; Chen et al., 2019c; Liu et al., 2019). They start with a few clean samples,
using either gradient descent or careful stimuli crafting, to ﬁnd a potential trigger that alters model
prediction. Characteristics of the recovered triggers along with the associated network activations are
used as features to determine whether a model is Trojaned or not."
INTRODUCTION,0.018656716417910446,"Trojan triggers can be of arbitrary patterns (e.g., shape, color, texture) at arbitrary locations of an input
image (e.g., Fig. 1). As a result, one major challenge of the reverse engineering-based approach is the
enormous search space for potential triggers. Meanwhile, just like the trigger is unknown, the target
label (i.e., the class label to which a triggered model predicts) is also unknown in practice. Gradient
descent may ﬂip a model’s prediction to the closest alternative label, which may not necessarily be the
target label. This makes it even more challenging to recover the true trigger. Note that many existing
methods (Guo et al., 2019; Wang et al., 2019; 2020b) require a target label. These methods achieve"
INTRODUCTION,0.022388059701492536,∗Email: Xiaoling Hu (xiaolhu@cs.stonybrook.edu).
INTRODUCTION,0.026119402985074626,Published as a conference paper at ICLR 2022
INTRODUCTION,0.029850746268656716,"(a)
(b)
(c)
(d)
(e)
(f)"
INTRODUCTION,0.033582089552238806,"Figure 1: Illustration of recovered triggers: (a) clean image, (b) poisoned image, (c) image with a
trigger recovered without topological prior, (d)-(f) images with candidate triggers recovered with
the proposed method. Topological prior contributes to improved compactness. We run the trigger
reconstruction for multiple rounds with a diversity prior to ensure a diverse set of trigger candidates."
INTRODUCTION,0.03731343283582089,"target label independence by enumerating through all possible labels, which can be computationally
prohibitive especially when the label space is huge."
INTRODUCTION,0.041044776119402986,"We propose a novel target-label-agnostic reverse engineering method. First, to improve the quality of
the recovered triggers, we need a prior that can localize the triggers, but in a ﬂexible manner. We,
therefore, propose to enforce a topological prior to the optimization process of reverse engineering,
i.e., the recovered trigger should have fewer connected components. This prior is implemented
through a topological loss based on the theory of persistent homology (Edelsbrunner & Harer, 2010).
It allows the recovered trigger to have arbitrary shape and size. Meanwhile, it ensures the trigger is
not scattered and is reasonably localized. See Fig. 1 for an example – comparing (d)-(f) vs. (c)."
INTRODUCTION,0.04477611940298507,Target Class
INTRODUCTION,0.048507462686567165,Clean !
INTRODUCTION,0.05223880597014925,"Decision 
Boundary R1 R2 R3"
INTRODUCTION,0.055970149253731345,"Figure 2: Illustration of generating a diverse set of
trigger candidates to increase the chance of ﬁnd-
ing the true trigger, especially for scenarios with
unknown target labels."
INTRODUCTION,0.05970149253731343,"As a second contribution, we propose to reverse
engineer multiple diverse trigger candidates. In-
stead of running gradient decent once, we run
it for multiple rounds, each time producing one
trigger candidate, e.g., Fig. 1 (d)-(f). Further-
more, we propose a trigger diversity loss to en-
sure the trigger candidates to be sufﬁciently dif-
ferent from each other (see Fig. 2). Generating
multiple diverse trigger candidates can increase
the chance of ﬁnding the true trigger. It also
mitigates the risk of unknown target labels. In
the example of Fig. 2, the ﬁrst trigger candidate
ﬂips the model prediction to a label different
from the target, while only the third candidate
hits the true target label."
INTRODUCTION,0.06343283582089553,"Generating multiple trigger candidates, however,
adds difﬁculties in ﬁltering out already-subtle
cues for Trojan detection. We also note reverse
engineering approaches often suffer from false positive triggers such as adversarial perturbations
or direct modiﬁcation of the crucial objects of the image1. In practice, we systematically extract a
rich set of features to describe the characteristics of the reconstructed trigger candidates based on
geometry, color, and topology, as well as network activations. A Trojan-detection network is then
trained to detect Trojaned models based on these features. Our main contributions are summarized as
follows:"
INTRODUCTION,0.06716417910447761,"• We propose a topological prior to regularize the optimization process of reverse engineering.
The prior ensures the locality of the recovered triggers, while being sufﬁciently ﬂexible
regarding the appearance. It signiﬁcantly improves the quality of the reconstructed triggers.
• We propose a diversity loss to generate multiple diverse trigger candidates. This increases
the chance of recovering the true trigger, especially for cases with unknown target labels.
• Combining the topological prior and diversity loss, we propose a novel Trojan detection
framework. On both synthetic and public TrojAI benchmarks, our method demonstrates
substantial improvement in both trigger recovery and Trojan detection."
INTRODUCTION,0.0708955223880597,"1Modiﬁcation of crucial objects is usually not a valid trigger strategy; it is too obvious to end-users and is
against the principle of adversaries."
INTRODUCTION,0.07462686567164178,Published as a conference paper at ICLR 2022
RELATED WORK,0.07835820895522388,"2
RELATED WORK"
RELATED WORK,0.08208955223880597,"Trojan detection. Many Trojan detection methods have been proposed recently. Some focus on
detecting poisoned inputs via anomaly detection (Chou et al., 2020; Gao et al., 2019; Liu et al., 2017;
Ma & Liu, 2019). For example, SentiNet (Chou et al., 2020) tries to identify adversarial inputs, and
uses the behaviors of these adversarial inputs to detect Trojaned models. Others focus on analyzing
the behaviors of the trained models (Chen et al., 2019a; Guo et al., 2019; Shen et al., 2021; Sun
et al., 2020). Speciﬁcally, Chen et al. (2019a) propose the Activation Clustering (AC) methodology
to analyze the activations of neural networks to determine if a model has been poisoned or not."
RELATED WORK,0.08582089552238806,"While early works require all training data to detect Trojans (Chen et al., 2019a; Gao et al., 2019;
Tran et al., 2018), recent approaches have been focusing on a more realistic setting – when one has
limited access to the training data. A particular promising direction is reverse engineering approaches,
which recover Trojan triggers with only a few clean samples. Neural cleanse (NC) (Wang et al.,
2019) develops a Trojan detection method by identifying if there is a trigger that would produce
misclassiﬁed results when added to an input. However, as pointed out by (Guo et al., 2019), NC
becomes futile when triggers vary in terms of size, shape, and location."
RELATED WORK,0.08955223880597014,"Since NC, different approaches have been proposed, extending the reverse engineering idea. Using a
conditional generative model, DeepInspect (Chen et al., 2019c) learns the probability distribution of
potential triggers from a model of interest. Kolouri et al. (2020) propose to learn universal patterns
that change predictions of the model (called Universal Litmus Patterns (ULPs)). The method is
efﬁcient as it only involves forward passes through a CNN and avoids backpropagation. ABS (Liu
et al., 2019) analyzes inner neuron behaviors by measuring how extra stimulation can change the
network’s prediction. Wang et al. (2020b) propose a data-limited TrojanNet detector (TND) by
comparing the impact of per-sample attack and universal attack. Guo et al. (2019) cast Trojan
detection as a non-convex optimization problem and it is solved through optimizing an objective
function. Huster & Ekwedike (2021) solve the problem by observing that, compared with clean
models, adversarial perturbations transfer from image to image more readily in poisoned models.
Zheng et al. (2021) inspect neural network structure using persistent homology and identify structural
cues differentiating Trojaned and clean models."
RELATED WORK,0.09328358208955224,"Existing methods are generally demanding on training data access, neural network architectures,
types of triggers, target class, etc. This limits their deployment to real-world applications. As for
reverse engineering approaches, it remains challenging, if not entirely infeasible, to recover the true
triggers. We propose a novel reverse engineering approach that can recover the triggers with high
quality using the novel diversity and topological prior. Our method shares the common beneﬁt of
reverse engineering methods; it only needs a few clean input images per model. Meanwhile, our
approach is agnostic of model architectures, trigger types, and target labels."
RELATED WORK,0.09701492537313433,"Topological data analysis and persistent homology. Topological data analysis (TDA) is a ﬁeld in
which one analyzes datasets using topological tools such as persistent homology (Edelsbrunner &
Harer, 2010; Edelsbrunner et al., 2000). The theory has been applied to different applications (Wu
et al., 2017; Kwitt et al., 2015; Wong et al., 2016; Chazal et al., 2013; Ni et al., 2017; Adams et al.,
2017; Bubenik, 2015; Varshney & Ramamurthy, 2015; Hu et al., 2019; 2021; Zhao et al., 2020; Yan
et al., 2021; Wu et al., 2020)."
RELATED WORK,0.10074626865671642,"As the advent of deep learning, some works have tried to incorporate the topological information into
deep neural networks, and the differentiable property of persistent homology make it possible. The
main idea is that the persistence diagram/barcodes can capture all the topological changes, and it is
differentiable to the original data. Hu et al. (2019) ﬁrst propose a topological loss to learn to segment
images with correct topology, by matching persistence diagrams in a supervised manner. Similarly,
Clough et al. (2020) use the persistence barcodes to enforce a given topological prior of the target
object. These methods achieve better results especially in structural accuracy. Persistent-homology-
based losses have been applied to other imaging (Abousamra et al., 2021; Wang et al., 2020a) and
learning problems (Hofer et al., 2019; 2020; Carrière et al., 2020; Chen et al., 2019b)."
RELATED WORK,0.1044776119402985,"The aforementioned methods use topological priors in supervised learning tasks (namely, segmenta-
tion). Instead, in this work, we propose to leverage the topological prior in an unsupervised setting;
we use a topological prior for the reverse engineering pipeline to reduce the search space of triggers
and enforce the recovered triggers to have fewer connected components."
RELATED WORK,0.10820895522388059,Published as a conference paper at ICLR 2022
METHOD,0.11194029850746269,"3
METHOD"
METHOD,0.11567164179104478,"Our reverse engineering framework is illustrated in Fig. 3. Given a trained DNN model, either clean
or Trojaned, and a few clean images, we use gradient descent to reconstruct triggers that can ﬂip the
model’s prediction. To increase the quality of reconstructed triggers, we introduce novel diversity
loss and topological prior. They help recover multiple diverse triggers of high quality."
METHOD,0.11940298507462686,"The common hypothesis of reverse engineering approaches is that the reconstructed triggers will
appear different for Trojaned and clean models. To fully exploit the discriminative power of the
reconstructed triggers for Trojan detection, we extract features based on trigger characteristics and
associated network activations. These features are used to train a classiﬁer, called the Trojan-detection
network, to classify a given model as Trojaned or clean."
METHOD,0.12313432835820895,"We note the discriminative power of the extracted trigger features, and thus the Trojan-detection
network, are highly dependent on the quality of the reconstructed triggers. Empirical results will show
the proposed diversity loss and topological prior are crucial in reconstructing high quality triggers,
and ensures a high quality Trojan-detection network. We will show that our method can learn to
detect Trojaned models even when trained with a small amount of labeled DNN models."
METHOD,0.12686567164179105,"For the rest of this section, we mainly focus on the reverse engineering module. We also add details
of the Trigger feature extraction to Sec. 3.3 and details of the Trojan-detection network to Sec. A.1."
METHOD,0.13059701492537312,"Binary 
classification: 
Trojaned or Not"
METHOD,0.13432835820895522,Clean Image
METHOD,0.13805970149253732,Trained models min
METHOD,0.1417910447761194,"𝐦,𝜽𝐿(𝐦, 𝜽; 𝐱, 𝑓, 𝑐∗)"
METHOD,0.1455223880597015,Features
METHOD,0.14925373134328357,"Diversity
Topo. Prior"
METHOD,0.15298507462686567,Reversed Image
METHOD,0.15671641791044777,"Trojan-detection Network
Reverse Engineering"
METHOD,0.16044776119402984,Figure 3: Our Trojan detection framework.
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.16417910447761194,"3.1
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES + 1 −𝐦𝐱 𝐦𝜽"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.16791044776119404,"𝜙(𝐱, 𝐦, 𝜽)"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.17164179104477612,"Figure 4: m and θ convert an input im-
age x into an altered one φ(x, m, θ).
The ⊙is omitted here for simpliﬁcation."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.17537313432835822,"Our method is based on the existing reverse engineering
pipeline ﬁrst proposed by Neural Cleanse (Wang et al.,
2019). Given a trained DNN model, let f(·) be the map-
ping from an input clean image x ∈R3×M×N to the
output y ∈RK with K classes, where M and N denote
the height and width of the image, respectively. Denote by
fk(·) the k-th output of f. The predicted label c∗is given
by c∗= arg maxk fk(x), 1 ≤k ≤K. We introduce
parameters θ and m to convert x into an altered sample"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.1791044776119403,"φ(x, m, θ) = (1 −m) ⊙x + m ⊙θ,
(1)"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.1828358208955224,"where the binary mask m ∈{0, 1}M×N and the pattern
θ ∈RM×N determine the trigger. 1 denotes an all-one
matrix. The symbol “⊙” denotes Hadamard product. See Fig. 4 for an illustration. We intend to ﬁnd
a triggered image ˆx = φ(x, ˆm, ˆθ) so that the model prediction ˆc = arg maxk fk(ˆx) is different from
the prediction on the original image c∗."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.1865671641791045,"We ﬁnd the triggered image, ˆx, by minimizing a loss over the space of m and θ:"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.19029850746268656,"L(m, θ; x, f, c∗) = Lflip(. . .) + λ1Ldiv(. . .) + λ2Ltopo(. . .) + R(m),
(2)"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.19402985074626866,"where Lflip, Ldiv, and Ltopo denote the label-ﬂipping loss, diversity loss, and topological loss,
respectively. We temporarily dropped their arguments for convenience. λ1, λ2 are the weights to"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.19776119402985073,Published as a conference paper at ICLR 2022
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.20149253731343283,"balance the loss terms. R(m) is a regularization term penalizing the size and range of the mask (more
details will be provided in the Sec. A.1 of Appendix)."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.20522388059701493,"To facilitate optimization, we relax the constraint on the mask m and allow it to be a continuous-
valued function, ranging between 0 and 1 and deﬁned over the image domain, m ∈[0, 1]M×N. Next,
we introduce the three loss terms one-by-one."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.208955223880597,"Label-ﬂipping loss Lflip: The label-ﬂipping loss Lflip penalizes the prediction of the model regard-
ing the ground truth label, formally:"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.2126865671641791,"Lflip(m, θ; x, f, c∗) = fc∗(φ(x, m, θ)).
(3)"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.21641791044776118,"Minimizing Lflip means minimizing the probability that the altered image φ(x, m, θ) is predicted as
c∗. In other words, we are pushing the input image out of its initial decision region."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.22014925373134328,"Note that we do not specify which label we would like to ﬂip the prediction to. This makes the
optimization easier. Existing approaches often run optimization to ﬂip the label to a target label and
enumerate through all possible target labels (Wang et al., 2019; 2020b). This can be rather expensive
in computation, especially with large label space."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.22388059701492538,"The downside of not specifying a target label during optimization is we will potentially miss the
correct target label, i.e., the label which the Trojaned model predicts on a triggered image. To this end,
we propose to reconstruct multiple candidate triggers with diversity constraints. This will increase
the chance of hitting the correct target label. See Fig. 2 for an illustration."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.22761194029850745,"Diversity loss Ldiv: With the label-ﬂipping loss Lflip, we ﬂip the label to a different one from the
original clean label and recover the corresponding triggers. The new label, however, may not be the
same as the true target label. Also considering the huge trigger search space, it is difﬁcult to recover
the triggers with only one attempt. Instead, we propose to search for multiple trigger candidates to
increase the chance of capturing the true trigger."
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.23134328358208955,"We run our algorithm for NT rounds, each time reconstructing a different trigger candidate. To avoid
ﬁnding similar trigger candidates, we introduce the diversity loss Ldiv to encourage different trigger
patterns and locations. Let mj and θj denote the trigger mask and pattern found in the j-th round.
At the i-th round, we compare the current candidates with triggers from all previous founds in terms
of L2 norm. Formally:
Ldiv(m, θ) = −
Xi−1"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.23507462686567165,"j=1 ||m ⊙θ −mj ⊙θj||2.
(4)"
REVERSE ENGINEERING OF MULTIPLE DIVERSE TRIGGER CANDIDATES,0.23880597014925373,"Minimizing Ldiv ensures the eventual trigger mi ⊙θi to be different from triggers from previous
rounds. Fig. 1(d)-(f) demonstrates the multiple candidates recovered with sufﬁcient diversity."
TOPOLOGICAL PRIOR,0.24253731343283583,"3.2
TOPOLOGICAL PRIOR"
TOPOLOGICAL PRIOR,0.2462686567164179,"Quality control of the trigger reconstruction remains a major challenge in reverse engineering methods,
due to the huge search space of triggers. Even with the regularizor R(m), the recovered triggers can
still be scattered and unrealistic. See Fig. 1(c) for an illustration. We propose a topological prior to
improve the locality of the reconstructed trigger. We introduce a topological loss enforcing that the
recovered trigger mask m to have as few number of connected components as possible. The loss is
based on the theory of persistent homology (Edelsbrunner et al., 2000; Edelsbrunner & Harer, 2010),
which models the topological structures of a continuous signal in a robust manner."
TOPOLOGICAL PRIOR,0.25,"Persistent homology. We introduce persistent homology in the context of 2D images. A more
comprehensive treatment of the topic can be found in (Edelsbrunner & Harer, 2010; Dey & Wang,
2021). Recall we relaxed the mask function m to a continuous-valued function deﬁned over the
image domain (denoted by Ω). Given any threshold α, we can threshold the image domain with
regard to m and obtain the superlevel set, Ωα := {p ∈Ω|m(p) ≥α}. A superlevel set can have
different topological structures, e.g., connected components and holes. If we continuously decrease
the value α, we have a continuously growing superlevel set Ωα. This sequence of superlevel set is
called a ﬁltration. The topology of Ωα continuously changes through the ﬁltration. New connected
components are born and later die (are merged with others). New holes are born and later die (are
sealed up). For each topological structure, the threshold at which it is born is called its birth time.
The threshold at which it dies is called its death time. The difference between birth and death time is
called the persistence of the topological structure."
TOPOLOGICAL PRIOR,0.2537313432835821,Published as a conference paper at ICLR 2022
TOPOLOGICAL PRIOR,0.2574626865671642,"We record the lifespan of all topological structures over the ﬁltration and encode them via a 2D point
set called persistence diagram, denoted by Dgm(m). Each topological structure is represented by
a 2D point within the diagram, p ∈Dgm(m), called a persistent dot. We use the birth and death
times of the topological structure to deﬁne the coordinates of the corresponding persistent dot. For
each dot p ∈Dgm(m), we abuse the notation and call the birth/death time of its corresponding
topological structure as birth(p) and death(p). Then we have p = (death(p), birth(p)). See Fig. 5
for an example function m (viewed as a terrain function) and its corresponding diagram. There are
ﬁve dots in the diagram, corresponding to ﬁve peaks in the landscape view."
TOPOLOGICAL PRIOR,0.26119402985074625,"To compute persistence diagram, we use the classic algorithm (Edelsbrunner & Harer, 2010; Edels-
brunner et al., 2000) with an efﬁcient implementation (Chen & Kerber, 2011; Wagner et al., 2012).
The image is ﬁrst discretized into a cubical complex consisting of vertices (pixels), edges and squares.
A boundary matrix is then created to encode the adjacency relationship between these elements. The
algorithm essentially carries out a matrix reduction algorithm over the boundary matrix, and the
reduced matrix reads out the persistence diagram. 𝛼"" 𝛼# 𝛼$ 𝛼% 𝛼&
𝛼' Birth"
TOPOLOGICAL PRIOR,0.26492537313432835,"Death
𝛼"""
TOPOLOGICAL PRIOR,0.26865671641791045,"𝛼#
𝛼%
𝛼&
𝛼'"
TOPOLOGICAL PRIOR,0.27238805970149255,"𝛼$
𝑐)(𝑝) 𝑐-(𝑝) 𝑆 𝛼#"
TOPOLOGICAL PRIOR,0.27611940298507465,"𝛼""
(a)
(b)
(c)
(d)
(e)"
TOPOLOGICAL PRIOR,0.2798507462686567,"Figure 5: From the left to right: (a) a sample landscape for a continuous function. The values at the
peaks α0 < α1 < α2 < α3 < α4 < α5. As we decrease the threshold, the topological structures
of the superlevel set change, (b) and (c) correspond to topological structures captured by different
thresholds, (d) highlighted region in (a), (e) the changes are captured by the persistence diagram
(right ﬁgure). We focus on the 0-dimensional topological structures (connected components). Each
persistent dot in the persistence diagram denotes a speciﬁc connected component. The topological
loss is introduced to reduce the connected components, which means pushing most of the persistent
dots to the diagonal (along the green lines).
Topological loss Ltopo: We formulate our topological loss based on persistent homology described
above. Minimizing our loss reduces the number of connected components of triggers. We will focus
on zero-dimensional topological structure, i.e., connected components. Intuitively speaking, each dot
in the diagram corresponds to a connected component. The ones far away from the diagonal line are
considered salient as its birth and death times are far apart. And the ones close to the diagonal line are
considered trivial. In Fig. 5, there is one salient dot far away from the diagonal line. It corresponds to
the highest peak. The other four dots are closer to the diagonal line and correspond to the smaller
peaks. The topological loss will reduce the number of connected components by penalizing the
distance of all dots from the diagonal line, except for the most salient one. Formally, the loss Ltopo is
deﬁned as:
Ltopo(m) =
X"
TOPOLOGICAL PRIOR,0.2835820895522388,"p∈Dgm(m)\{p∗}[birth(p) −death(p)]2,
(5)"
TOPOLOGICAL PRIOR,0.2873134328358209,"where p∗denotes the persistent dot that is farthest away from the diagonal (with the highest persis-
tence). Minimizing this loss will keep p∗intact, while pushing all other dots to the diagonal line, thus
making their corresponding components either disappear or merged with the main component."
TOPOLOGICAL PRIOR,0.291044776119403,"Differentiability and the gradient: The loss function (Eq. (5)) is differentiable almost everywhere
in the space of functions. To see this, we revisit the ﬁltration, i.e., the growing superlevel set as we
continuously decrease the threshold α. The topological structures change at speciﬁc locations of
the image domain. A component is born at the corresponding local maximum. It dies merging with
another component at the saddle point between the two peaks. In fact, these locations correspond
to critical points of the function. And the function values at these critical points correspond to the
birth and death times of these topological structures. For a persistent dot, p, we call the critical point
corresponding to its birth, cb(p), and the critical point corresponding to its death, cd(p). Then we
have birth(p) = m(cb(p)) and death(p) = m(cd(p)). The loss function (Eq. 5) can be rewritten as
a polynomial function of the function m at different critical points."
TOPOLOGICAL PRIOR,0.2947761194029851,"Ltopo(m) =
X"
TOPOLOGICAL PRIOR,0.29850746268656714,"p∈Dgm(m)\{p∗}[m(cb(p)) −m(cd(p))]2.
(6)"
TOPOLOGICAL PRIOR,0.30223880597014924,Published as a conference paper at ICLR 2022
TOPOLOGICAL PRIOR,0.30597014925373134,"The gradient can be computed naturally. Ltopo is a piecewise differentiable loss function over the
space of all possible functions m. In a gradient decent step, for all dots except for p∗, we push up the
function at the death critical point cd(p) (the saddle), and push down the function value at the birth
critical point cb(p) (the local maximum). This is illustrated by the arrows in Fig. 5(Middle-Right).
This will kill the non-salient components and push them towards the diagonal."
TRIGGER FEATURE EXTRACTION AND TROJAN DETECTION NETWORK,0.30970149253731344,"3.3
TRIGGER FEATURE EXTRACTION AND TROJAN DETECTION NETWORK"
TRIGGER FEATURE EXTRACTION AND TROJAN DETECTION NETWORK,0.31343283582089554,"Next we summarize the features we extract from recovered triggers. The recovered Trojan triggers
can be characterized via their capability in ﬂipping model predictions (i.e., the label-ﬂipping loss).
Moreover, they are different from adversarial noise as they tend to be more regularly shaped and
are also distinct from actual objects which can be recognized by a trained model. We introduce
appearance-based features to differentiate triggers from adversarial noise and actual objects, ."
TRIGGER FEATURE EXTRACTION AND TROJAN DETECTION NETWORK,0.31716417910447764,"Speciﬁcally, for label ﬂipping capability, we directly use the label-ﬂipping loss Lflip and diversity
loss Ldiv as features. For appearance features, we use trigger size and topological statistics as their
features: 1) The number of foreground pixels divided by total number of pixels in mask m; 2) To
capture the size of the triggers in the horizontal and vertical directions, we ﬁt a Gaussian distribution
to the mask m and record mean and std in both directions; 3) The trigger we ﬁnd may have multiple
connected components. The ﬁnal formulated topological descriptor includes the topological loss
Ltopo, the number of connected components, mean and std in terms of the size of each component."
TRIGGER FEATURE EXTRACTION AND TROJAN DETECTION NETWORK,0.3208955223880597,"After the features are extracted, we build a neural network for Trojan detection, which takes the bag
of features of the generated triggers as inputs, and outputs a scalar score of whether the model is
Trojaned or not. More details are provided in Sec. A.1 of Appendix."
EXPERIMENTS,0.3246268656716418,"4
EXPERIMENTS"
EXPERIMENTS,0.3283582089552239,"We evaluate our method on both synthetic datasets and publicly available TrojAI benchmarks. We
provide quantitative and qualitative results, followed by ablation studies, to demonstrate the efﬁcacy
of the proposed method. All clean/Trojaned models are DNNs trained for image classiﬁcation."
EXPERIMENTS,0.332089552238806,"Synthetic datasets (Trojaned-MNIST and Trojaned-CIFAR10): We adopt the codes provided by
NIST2 to generate 200 DNNs (50% of them are Trojaned) trained to classify MNIST and CIFAR10
data, respectively. The Trojaned models are trained with images poisoned by square triggers. The
poison rate is set as 0.2."
EXPERIMENTS,0.3358208955223881,"TrojAI benchmarks (TrojAI-Round1, Round2, Round3 and Round4): These datasets are pro-
vided by US IARPA/NIST3, who recently organized a Trojan AI competition. Polygon triggers are
generated randomly with variations in shape, size, and color. Filter-based triggers are generated by
randomly choosing from ﬁve distinct ﬁlters. Trojan detection is more challenging on these TrojAI
datasets as compared to Triggered-MNIST due to the use of deeper DNNs and larger variations in
appearances of foreground/background objects, trigger patterns etc. Round1, Round2, Round3 and
Round4 have 1000, 1104, 1008 and 1008 models, respectively. Descriptions of the difference among
these rounds are provided in Sec. A.2 of Appendix."
EXPERIMENTS,0.33955223880597013,Table 1: Comparison on Trojaned-MNIST/CIFAR10.
EXPERIMENTS,0.34328358208955223,Method Metric Trojaned-MNIST Trojaned-CIFAR10
EXPERIMENTS,0.34701492537313433,"NC
AUC
0.57 ± 0.07
0.75 ± 0.07
ABS
AUC
0.63 ± 0.04
0.67 ± 0.06
TABOR AUC
0.65 ± 0.07
0.71 ± 0.05
ULP
AUC
0.59 ± 0.03
0.55 ± 0.03
DLTND AUC
0.62 ± 0.05
0.52 ± 0.08
Ours
AUC
0.88 ± 0.04
0.91 ± 0.05
NC
ACC
0.60 ± 0.04
0.73 ± 0.06
ABS
ACC
0.65 ± 0.02
0.69 ± 0.04
TABOR ACC
0.62 ± 0.04
0.69 ± 0.08
ULP
ACC
0.57 ± 0.02
0.59 ± 0.06
DLTND ACC
0.64 ± 0.07
0.55 ± 0.07
Ours
ACC
0.89 ± 0.02
0.92 ± 0.04"
EXPERIMENTS,0.35074626865671643,"Baselines:
We
choose
recently
pub-
lished
methods
including
NC
(Neural
Cleanse) (Wang et al., 2019), ABS (Liu
et
al.,
2019),
TABOR
(Guo
et
al.,
2019), ULP (Kolouri et al., 2020), and
DLTND (Wang et al., 2020b) as baselines."
EXPERIMENTS,0.35447761194029853,"Implementation details: We set λ1 = 1,
λ2 = 10 and NT = 3 for all our experi-
ments (i.e., we generate 3 trigger candidates
for each input image and each model). The
parameters of Trojan detection network are
learned using a set of clean and Trojaned
models with ground truth labeling. We train"
EXPERIMENTS,0.3582089552238806,"2https://github.com/trojai/trojai
3https://pages.nist.gov/trojai/docs/data.html"
EXPERIMENTS,0.3619402985074627,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.3656716417910448,"(a)
(b)
(c)
(d)
(e)
(f)
Figure 6: Examples of recovered triggers overlaid on clean images. From left to right: (a) clean
image, (b) triggers recovered by (Wang et al., 2019), (c) triggers recovered by (Liu et al., 2019), (d)
triggers recovered by (Guo et al., 2019), (e) triggers recovered by our method without topological
prior, and (f) triggers recovered by our method with topological prior."
EXPERIMENTS,0.3694029850746269,"the detection network by optimizing cross entropy loss using the Adam optimizer (Kingma & Ba,
2014). The hidden state size, number of layers of MLPα, MLPβ, as well as optimizer learning rate,
weight decay and number of epochs are optimized using Bayesian hyperparameter search4 for 500
rounds on 8-fold cross-validation."
EXPERIMENTS,0.373134328358209,"Evaluation metrics: We follow the settings in (Sikka et al., 2020). We report the mean and standard
deviation of two metrics: area under the ROC curve (AUC) and accuracy (ACC). Speciﬁcally, we
evaluate our approach on the whole set by doing an 8-fold cross validation. For each fold, we use
80% of the models for training, 10% for validation, and the rest 10% for testing."
EXPERIMENTS,0.376865671641791,"Results: Tables 1 and 2 show the quantitative results on the Trojaned-MNIST/CIFAR10 and TrojAI
datasets, respectively. The reported performances of baselines are reproduced using source codes
provided by the authors or quoted from related papers. The best performing numbers are highlighted
in bold. From Tab. 1 and 2, we observe that our method performs substantially better than the
baselines. It is also worth noting that, compared with these baselines, our proposed method extracts
ﬁx-sized features for each model, independent of the number of classes, architectures, trigger types,
etc. By using the extracted features, we are able to train a separate Trojan detection network, which
is salable and model-agnostic."
EXPERIMENTS,0.3805970149253731,"Table 2: Performance comparison on the TrojAI dataset.
Method
Metric
TrojAI-Round1
TrojAI-Round2
TrojAI-Round3
TrojAI-Round4
NC
AUC
0.50 ± 0.03
0.63 ± 0.04
0.61 ± 0.06
0.58 ± 0.05
ABS
AUC
0.68 ± 0.05
0.61 ± 0.06
0.57 ± 0.04
0.53 ± 0.06
TABOR
AUC
0.71 ± 0.04
0.66 ± 0.07
0.50 ± 0.07
0.52 ± 0.04
ULP
AUC
0.55 ± 0.06
0.48 ± 0.02
0.53 ± 0.06
0.54 ± 0.02
DLTND
AUC
0.61 ± 0.07
0.58 ± 0.04
0.62 ± 0.07
0.56 ± 0.05
Ours
AUC
0.90 ± 0.02
0.87 ± 0.05
0.89 ± 0.04
0.92 ± 0.06
NC
ACC
0.53 ± 0.04
0.49 ± 0.02
0.59 ± 0.07
0.60 ± 0.04
ABS
ACC
0.70 ± 0.04
0.59 ± 0.05
0.56 ± 0.03
0.51 ± 0.05
TABOR
ACC
0.70 ± 0.03
0.68 ± 0.08
0.51 ± 0.05
0.55 ± 0.06
ULP
ACC
0.58 ± 0.07
0.51 ± 0.03
0.56 ± 0.04
0.57 ± 0.04
DLTND
ACC
0.59 ± 0.04
0.61 ± 0.05
0.65± 0.04
0.59 ± 0.06
Ours
ACC
0.91 ± 0.03
0.89 ± 0.04
0.90 ± 0.03
0.91 ± 0.04"
EXPERIMENTS,0.3843283582089552,"Fig. 6 shows a few examples of recovered triggers. We observe that, compared with the baselines,
the triggers found by our method are more compact and of better quality. This is mainly due to the
introduction of topological constraints. The improved quality of recovered triggers directly results in
improved performance of Trojan detection."
EXPERIMENTS,0.3880597014925373,4https://github.com/hyperopt/hyperopt
EXPERIMENTS,0.3917910447761194,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.39552238805970147,"Ablation study of loss weights: For the loss weights λ1 and λ2, we empirically choose the weights
which make reverse engineering converge the fastest. This is a reasonable choice as in practice, time
is one major concern for reverse engineering pipelines."
EXPERIMENTS,0.39925373134328357,"Despite the seemingly ad hoc choice, we have observed that our performances are quite robust to all
these loss weights. As topological loss is a major contribution of this paper, we conduct an ablation
study in terms of its weight (λ2) on TrojAI-Round4 dataset. The results are reported in Fig. 7. We
observe that the proposed method is quite robust to λ2, and when λ = 10, it achieves slightly better
performance (AUC: 0.92 ± 0.06) than other choices."
EXPERIMENTS,0.40298507462686567,Figure 7: Ablation study results for λ2.
EXPERIMENTS,0.40671641791044777,"Ablation study of number of training model samples:
The trigger features and Trojan detection network are
important in achieving SOTA performance. To further
demonstrate the efﬁcacy of the proposed diversity and
topological loss terms, we conduct another ablation study
to investigate the case with less training model samples,
and thus a weaker Trojan detection network."
EXPERIMENTS,0.41044776119402987,"The ablation study in terms of number of training samples
on TrojAI-Round4 data is illustrated in Tab. 3. We observe
that the proposed topological loss and diversity loss will boost the performance with/without a fully
trained Trojan-detection network. These two losses improve the quality of the recovered trigger, in
spite of how the trigger information is used. Thus even with a limited number of training samples
(e.g., 25), the proposed method could still achieve signiﬁcantly better performance than the baselines."
EXPERIMENTS,0.4141791044776119,Table 3: Ablation study for # of training samples.
EXPERIMENTS,0.417910447761194,"# of samples
Ours
w/o topo
w/o diversity
25
0.77 ± 0.04 0.73 ± 0.03 0.68 ± 0.04
50
0.81 ± 0.03 0.76 ± 0.05 0.73 ± 0.02
100
0.84 ± 0.05 0.78 ± 0.06 0.76 ± 0.03
200
0.86 ± 0.04 0.82 ± 0.04 0.79 ± 0.05
400
0.90 ± 0.05 0.85 ± 0.03 0.82 ± 0.04
800
0.92 ± 0.06 0.89 ± 0.04 0.85 ± 0.02"
EXPERIMENTS,0.4216417910447761,"Ablation study for loss terms: We investi-
gate the individual contribution of different loss
terms used to search for the latent triggers.
Tab. 4 lists the corresponding performance on
the TrojAI-Round4 dataset. We observe a de-
crease in AUC (from 0.92 to 0.89) if the topo-
logical loss is removed. This drop is expected as
the topological loss helps to ﬁnd more compact
triggers. Also, the performance drops signiﬁ-
cantly (from 0.92 to 0.85 in AUC) if the diversity loss is removed. We also report the performance by
setting NT = 2; when NT = 2, the performance increases from 0.85 to 0.89 in AUC. The reason is
that with diversity loss, we are able to generate multiple diverse trigger candidates, which increases
the probability of recovering the true trigger when the target class is unknown. Our ablation study
justiﬁes the use of both diversity and topological losses."
EXPERIMENTS,0.4253731343283582,Table 4: Ablation results of loss terms.
EXPERIMENTS,0.4291044776119403,"Method
TrojAI-Round4
w/o topological loss
0.89 ± 0.04
w/o diversity loss (NT = 1)
0.85 ± 0.02
NT = 2
0.89 ± 0.05
with all loss terms (NT = 3)
0.92 ± 0.06"
EXPERIMENTS,0.43283582089552236,"In practice, we found that topological loss can im-
prove the convergence of trigger search. Without
topological loss, it takes ≈50 iterations to ﬁnd a rea-
sonable trigger (Fig. 6(e)). In contrast, with the topo-
logical loss, it takes only ≈30 iterations to converge
to a better recovered trigger (Fig. 6(f)). The rationale
is that, as the topological loss imposes strong constraints on the number of connected components, it
largely reduces the search space of triggers, consequently, making the convergence of trigger search
much faster. This is worth further investigation."
EXPERIMENTS,0.43656716417910446,"Unsupervised vs supervised: Our technical contributions are agnostic of whether the detection is
supervised (i.e., using annotated models) or unsupervised. The proposed diversity and topological
losses are used to improve the quality of a reconstructed trigger, using only a single model and input
data. More discussions and results in terms of (un)supervised settings are included in Sec. A.5."
CONCLUSION,0.44029850746268656,"5
CONCLUSION"
CONCLUSION,0.44402985074626866,"In this paper, we propose a diversity loss and a topological prior to improve the quality of the trigger
reverse engineering for Trojan detection. These loss terms help ﬁnding high quality triggers efﬁciently.
They also avoid the dependant of the method to the target label. On both synthetic datasets and
publicly available TrojAI benchmarks, our approach recovers high quality triggers and achieves
SOTA Trojan detection performance."
CONCLUSION,0.44776119402985076,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.45149253731343286,"Ethics Statement: As we have developed better Trojan detection algorithm and introduce the method
in details, the attackers may inversely create Trojaned models that are more difﬁcult to detect based on
the limitations of current method. Attack and defense will always coexist, which pushes researchers
to keep developing more efﬁcient algorithms."
REPRODUCIBILITY STATEMENT,0.4552238805970149,"Reproducibility Statement: The implementation details are mentioned in Sec. 4. The details of the
data are provided in Sec. A.2 of Appendix. The details of Trojan detection classiﬁer are described in
Sec. A.1 of Appendix. The used computation resources are speciﬁed in Sec. A.7 of Appendix."
REPRODUCIBILITY STATEMENT,0.458955223880597,"Acknowledgement: The authors thank anonymous reviewers for their constructive feedback. This
effort was partially supported by the Intelligence Advanced Research Projects Agency (IARPA) under
the contract W911NF20C0038. The content of this paper does not necessarily reﬂect the position or
the policy of the Government, and no ofﬁcial endorsement should be inferred."
REFERENCES,0.4626865671641791,REFERENCES
REFERENCES,0.4664179104477612,"Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with
topological constraints. In Proceedings of AAAI Conference on Artiﬁcial Intelligence, 2021."
REFERENCES,0.4701492537313433,"Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman,
Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence images: A
stable vector representation of persistent homology. The Journal of Machine Learning Research,
18(1):218–252, 2017."
REFERENCES,0.47388059701492535,"Peter Bubenik. Statistical topological data analysis using persistence landscapes. The Journal of
Machine Learning Research, 16(1):77–102, 2015."
REFERENCES,0.47761194029850745,"Mathieu Carrière, Frédéric Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, and Yuhei Umeda.
Perslay: A neural network layer for persistence diagrams and new graph topological signatures. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 2786–2796. PMLR, 2020."
REFERENCES,0.48134328358208955,"Frédéric Chazal, Leonidas J Guibas, Steve Y Oudot, and Primoz Skraba. Persistence-based clustering
in riemannian manifolds. Journal of the ACM (JACM), 60(6):41, 2013."
REFERENCES,0.48507462686567165,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. In SafeAI@ AAAI, 2019a."
REFERENCES,0.48880597014925375,"Chao Chen and Michael Kerber. Persistent homology computation with a twist. In Proceedings 27th
European Workshop on Computational Geometry, volume 11, pp. 197–200, 2011."
REFERENCES,0.4925373134328358,"Chao Chen, Xiuyan Ni, Qinxun Bai, and Yusu Wang. A topological regularizer for classiﬁers via
persistent homology. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics,
pp. 2573–2582. PMLR, 2019b."
REFERENCES,0.4962686567164179,"Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan
detection and mitigation framework for deep neural networks. In IJCAI, pp. 4658–4664, 2019c."
REFERENCES,0.5,"Edward Chou, Florian Tramèr, and Giancarlo Pellegrino. Sentinet: Detecting localized universal
attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pp.
48–54. IEEE, 2020."
REFERENCES,0.503731343283582,"James Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A Zimmer, Julia A Schnabel, and Andrew
King. A topological loss function for deep-learning based image segmentation using persistent
homology. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."
REFERENCES,0.5074626865671642,"Tamal K. Dey and Yusu Wang. Computational Topology for Data Analysis. Cambridge University
Press, 2021."
REFERENCES,0.5111940298507462,"Herbert Edelsbrunner and John Harer. Computational topology: an introduction. American Mathe-
matical Soc., 2010."
REFERENCES,0.5149253731343284,Published as a conference paper at ICLR 2022
REFERENCES,0.5186567164179104,"Herbert Edelsbrunner, David Letscher, and Afra Zomorodian. Topological persistence and simpliﬁca-
tion. In Proceedings 41st Annual Symposium on Foundations of Computer Science, pp. 454–463.
IEEE, 2000."
REFERENCES,0.5223880597014925,"Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual
Computer Security Applications Conference, pp. 113–125, 2019."
REFERENCES,0.5261194029850746,"Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 580–587, 2014."
REFERENCES,0.5298507462686567,"Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach
to inspecting and restoring trojan backdoors in ai systems. 20th IEEE International Conference on
Data Mining, 2019."
REFERENCES,0.5335820895522388,"Christoph Hofer, Roland Kwitt, Marc Niethammer, and Mandar Dixit. Connectivity-optimized
representation learning via persistent homology. In International Conference on Machine Learning,
pp. 2751–2760. PMLR, 2019."
REFERENCES,0.5373134328358209,"Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph ﬁltration
learning. In International Conference on Machine Learning, pp. 4314–4323. PMLR, 2020."
REFERENCES,0.5410447761194029,"Xiaoling Hu, Fuxin Li, Dimitris Samaras, and Chao Chen. Topology-preserving deep image segmen-
tation. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.5447761194029851,"Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, and Chao Chen. Topology-aware segmentation
using discrete morse theory. International Conference on Learning Representations, 2021."
REFERENCES,0.5485074626865671,"Todd Huster and Emmanuel Ekwedike. Top: Backdoor detection in neural networks via transferability
of perturbation. arXiv preprint arXiv:2103.10274, 2021."
REFERENCES,0.5522388059701493,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5559701492537313,"Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns:
Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 301–310, 2020."
REFERENCES,0.5597014925373134,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.5634328358208955,"Roland Kwitt, Stefan Huber, Marc Niethammer, Weili Lin, and Ulrich Bauer. Statistical topological
data analysis-a kernel perspective. In Advances in neural information processing systems, pp.
3070–3078, 2015."
REFERENCES,0.5671641791044776,"Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs:
Scanning neural networks for back-doors by artiﬁcial brain stimulation. In Proceedings of the 2019
ACM SIGSAC Conference on Computer and Communications Security, pp. 1265–1282, 2019."
REFERENCES,0.5708955223880597,"Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International Conference
on Computer Design (ICCD), pp. 45–48. IEEE, 2017."
REFERENCES,0.5746268656716418,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431–3440, 2015."
REFERENCES,0.5783582089552238,"Shiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant
checking. In Proceedings of the 26th Network and Distributed System Security Symposium (NDSS
2019), 2019."
REFERENCES,0.582089552238806,"Xiuyan Ni, Novi Quadrianto, Yusu Wang, and Chao Chen. Composing tree graphical models
with persistent homology features for clustering mixed-type data. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 2622–2631. JMLR. org, 2017."
REFERENCES,0.585820895522388,Published as a conference paper at ICLR 2022
REFERENCES,0.5895522388059702,"Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma,
and Xiangyu Zhang. Backdoor scanning for deep neural networks through k-arm optimization.
arXiv preprint arXiv:2102.05123, 2021."
REFERENCES,0.5932835820895522,"Karan Sikka, Indranil Sur, Susmit Jha, Anirban Roy, and Ajay Divakaran. Detecting trojaned dnns
using counterfactual attributions. arXiv preprint arXiv:2012.02275, 2020."
REFERENCES,0.5970149253731343,"Mingjie Sun, Siddhant Agarwal, and J Zico Kolter. Poisoned classiﬁers are not only backdoored, they
are fundamentally broken. arXiv preprint arXiv:2010.09080, 2020."
REFERENCES,0.6007462686567164,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in
neural information processing systems, 2018."
REFERENCES,0.6044776119402985,"Kush R Varshney and Karthikeyan Natesan Ramamurthy. Persistent topology of decision boundaries.
In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.
3931–3935. IEEE, 2015."
REFERENCES,0.6082089552238806,"Hubert Wagner, Chao Chen, and Erald Vuçini. Efﬁcient computation of persistent homology for
cubical data. In Topological methods in data analysis and visualization II, pp. 91–106. Springer,
2012."
REFERENCES,0.6119402985074627,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019."
REFERENCES,0.6156716417910447,"Fan Wang, Huidong Liu, Dimitris Samaras, and Chao Chen. Topogan: A topology-aware generative
adversarial network. In European Conference on Computer Vision, pp. 118–136. Springer, 2020a."
REFERENCES,0.6194029850746269,"Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases. European Conference on
Computer Vision (ECCV), 2020b."
REFERENCES,0.6231343283582089,"Eleanor Wong, Sourabh Palande, Bei Wang, Brandon Zielinski, Jeffrey Anderson, and P Thomas
Fletcher. Kernel partial least squares regression for relating functional brain network topology to
clinical measures of behavior. In 2016 IEEE 13th International Symposium on Biomedical Imaging
(ISBI), pp. 1303–1306. IEEE, 2016."
REFERENCES,0.6268656716417911,"Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2019."
REFERENCES,0.6305970149253731,"Pengxiang Wu, Chao Chen, Yusu Wang, Shaoting Zhang, Changhe Yuan, Zhen Qian, Dimitris
Metaxas, and Leon Axel. Optimal topological cycles and their application in cardiac trabeculae
restoration. In International Conference on Information Processing in Medical Imaging, pp. 80–92.
Springer, 2017."
REFERENCES,0.6343283582089553,"Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A topological
ﬁlter for learning with label noise. Advances in neural information processing systems, 33:
21382–21393, 2020."
REFERENCES,0.6380597014925373,"Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. Link prediction with persistent
homology: An interactive view. In International Conference on Machine Learning, pp. 11659–
11669. PMLR, 2021."
REFERENCES,0.6417910447761194,"Qi Zhao, Ze Ye, Chao Chen, and Yusu Wang. Persistence enhanced graph neural network. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 2896–2906. PMLR, 2020."
REFERENCES,0.6455223880597015,"Songzhu Zheng, Yikai Zhang, Hubert Wagner, Mayank Goswami, and Chao Chen. Topological
detection of trojaned neural networks. Advances in Neural Information Processing Systems, 34,
2021."
REFERENCES,0.6492537313432836,Published as a conference paper at ICLR 2022
REFERENCES,0.6529850746268657,"A
APPENDIX"
REFERENCES,0.6567164179104478,"A.1
LEARNING-BASED TROJAN-DETECTION NETWORK"
REFERENCES,0.6604477611940298,"While bottom-up trigger generation uses appearance heuristics to search for possible triggers, we
cannot guarantee the recovered triggers are true triggers, even for Trojaned models. Many other
perturbations can create label ﬂipping effects, such as adversarial samples and modiﬁcations speciﬁc
on the most semantically critical region. See Figure 8 for illustrations. These examples can easily
become false positives for a Trojan detector; they can be good trigger candidate generated by the
reverse engineer pipeline."
REFERENCES,0.664179104477612,"To fully address these issues, we propose a top-down Trojan detector learned using clean and Trojaned
models. It helps separate true Trojan triggers from other false positives. To this end, we propose
to extract features from the reverse engineered Trojan triggers and train a separate shallow neural
network for Trojan detection."
REFERENCES,0.667910447761194,"For each model, we generate a diverse set of NT possible triggers for each of its K output classes to
scan for possible triggers. As described above, we extract one feature for each generated trigger. As a
result, for each model we have NT × K sets of features."
REFERENCES,0.6716417910447762,"Regularizer: The regularizer R(m) consists of a mass term and a size term. For mass, we use ¯m as
the average value of m. For size, we normalize the mask m into a distribution p(x, y) over x and y.
To capture the spatial extent of mask m, we compute the standard deviation of X ∼p(x) as δX and
Y ∼p(y) as δY . As a result, R(m) = ¯m + δX + δY is the regularizer."
REFERENCES,0.6753731343283582,"Trojan 
trigger"
REFERENCES,0.6791044776119403,Adversarial noise
REFERENCES,0.6828358208955224,"Object 
modification"
REFERENCES,0.6865671641791045,"Changes 
Prediction
Localized
Well 
Connected"
REFERENCES,0.6902985074626866,Preserves
REFERENCES,0.6940298507462687,Content
REFERENCES,0.6977611940298507,Bottom-up Reverse Engineering
REFERENCES,0.7014925373134329,with Topological Prior
REFERENCES,0.7052238805970149,Top-down Trojan
REFERENCES,0.7089552238805971,Classification
REFERENCES,0.7126865671641791,"Figure 8: Our Trojan detection method combines
bottom-up Trigger reverse engineering under topo-
logical constraints, with top-down classiﬁcation.
Such a combination allows us to accurately isolate
Trojan triggers from non-Trojan patterns such as
adversarial noise and object modiﬁcations."
REFERENCES,0.7164179104477612,"Classiﬁcation network: After the features are
extracted, we build a neural network for Trojan
detection, which takes as input for a given image
classiﬁer, the bag of features of its generated
triggers and outputs a scalar score of whether
the model is Trojaned or not."
REFERENCES,0.7201492537313433,"Since different models may have different num-
bers of output classes K, the number of features
varies for each model. Therefore, a sequence of
modeling architectures – such as bag-of-words,
Recurrent Neural Networks and Transformers
– could be employed to aggregate the NT × K
features into a 0/1 Trojan classiﬁcation output."
REFERENCES,0.7238805970149254,"Given a set of annotated models, clean or in-
fected, supervised learning can be applied to
train the classiﬁcation network. Empirically,
we found that a simple bag-of-words technique
achieved the best Trojan detection performance
while also being fast to run and data efﬁcient.
Speciﬁcally, let the bag of features be {vi}, i = 1, . . . , NT K. The features are ﬁrst transformed
individually using an MLP, followed by average pooling across the features and another MLP to
output the Trojan classiﬁcation:"
REFERENCES,0.7276119402985075,"⃗hi = MLPα(⃗vi),
⃗h =
1
NT K X"
REFERENCES,0.7313432835820896,"i
⃗hi,
s = MLPβ(⃗h),
i = 1, . . . NT K.
(7)"
REFERENCES,0.7350746268656716,"A.2
DETAILS ABOUT THE TROJAI DATASETS"
REFERENCES,0.7388059701492538,"TrojAI-Round1, Round2, Round3, Round4 datasets:
These datasets are provided by US
IARPA/NIST5 and contain trained models for trafﬁc sign classiﬁcation (for each round, 50% of
total models are Trojaned). All the models are trained on synthetically created image data of non-
real trafﬁc signs superimposed on road background scenes. Trojan detection is more difﬁcult on
Round2/Round3/Round4 compared to Round1 due to following reasons:"
REFERENCES,0.7425373134328358,5https://pages.nist.gov/trojai/docs/data.html
REFERENCES,0.746268656716418,Published as a conference paper at ICLR 2022
REFERENCES,0.75,"• Round2/Round3 have more number of classes: Round1 has 5 classes while Round2/Round3
have 5-25 classes."
REFERENCES,0.753731343283582,"• Round2/Round3 have more trigger types: Round1 only has polygon triggers, while
Round2/Round3 have both polygon and Instagram ﬁlter based triggers."
REFERENCES,0.7574626865671642,"• The number of source classes are different: all classes are poisoned in Round1, while 1, 2,
or all classes are poisoned in Round2/Round3."
REFERENCES,0.7611940298507462,"• Round2/Round3 have more type of model architectures: Round1 has 3 architectures, while
Round2/Round3 have 23 architectures."
REFERENCES,0.7649253731343284,"Round3 experimental design is identical to Round2 with the addition of Adversarial Training. Two
different Adversarial Training approaches: Projected Gradient Descent (PGD), Fast is Better than
Free (FBF) (Wong et al., 2019) are used."
REFERENCES,0.7686567164179104,"Unlike the previous rounds, Round4 can have multiple concurrent triggers. Additionally, triggers can
have conditions attached to their ﬁring. The differences are listed as follows:"
REFERENCES,0.7723880597014925,"• All triggers in Round4 are one to one mappings, which means a trigger ﬂips a single source
class to a single target class."
REFERENCES,0.7761194029850746,"• Three possible conditionals, spatial, spectral and class are attached to triggers within this
dataset."
REFERENCES,0.7798507462686567,• Round4 has remove the very large model architectures to reduce the training time.
REFERENCES,0.7835820895522388,"Round1, Round2, Round3 and Round4 have 1000, 1104, 1008 and 1008 models respectively."
REFERENCES,0.7873134328358209,"A.3
SUPPORTING ADDITIONAL TRIGGER CLASSES"
REFERENCES,0.7910447761194029,"Different classes of Trojan triggers are being actively explored in Trojan detection benchmarks. For
image classiﬁcation, the TrojAI datasets include localized triggers which can be directly applied to
objects along with global ﬁlter-based Trojans, where the idea is that a color ﬁlter could be attached to
the lens of camera and results in a global image transformation."
REFERENCES,0.7947761194029851,"Our top-down bottom-up Trojan detection framework is designed to support multiple classes of
Trojan triggers, where each trigger class gets its dedicated reverse engineering approach and pathway
in the Trojan detection network. Adding support to a new class of triggers, e.g. color ﬁlters, amounts
to adding a reverse engineering approach for color ﬁlters and adding a appearance feature descriptor
for Trojan classiﬁcation."
REFERENCES,0.7985074626865671,"Reverse engineering color ﬁlter triggers.
The pipeline of reverse engineering color ﬁlter triggers
is illustrated in Fig. 9. Comparing to reverse engineering local triggers in Fig.4, the ﬁlter editor and
the loss functions are adjusted to ﬁnd color ﬁlter triggers."
REFERENCES,0.8022388059701493,"We model a color ﬁlter trigger using a per-pixel position-dependent color transformation. Let
[rij, gij, bij] be the color of a pixel of input image x at location (i, j), and we model a color ﬁlter
trigger using an MLP E(·; θﬁlter) with parameters θﬁlter which performs position-dependent color
mapping:"
REFERENCES,0.8059701492537313,"[ ˆ
rij, ˆ
gij, ˆ
bij] = E([rij, gij, bij, i, j]; θﬁlter).
(8)"
REFERENCES,0.8097014925373134,"Here [ ˆ
rij, ˆ
gij, ˆ
bij] is the color of pixel (i, j) of the triggered image ˆx. For TrojAI datasets we use
a 2-layer 16 hidden neurons MLP to model the color ﬁlters, as it learns sufﬁciently complex color
transforms while being fast to run:"
REFERENCES,0.8134328358208955,"Lﬁlter = Lﬁlter
flip + λﬁlter
1
Lﬁlter
div + λﬁlter
2
Rﬁlter(θﬁlter).
(9)"
REFERENCES,0.8171641791044776,"The label ﬂipping loss Lﬁlter
flip remains identical:"
REFERENCES,0.8208955223880597,"Lﬁlter
flip = ˆyc.
(10)"
REFERENCES,0.8246268656716418,Published as a conference paper at ICLR 2022
REFERENCES,0.8283582089552238,"Trained models
Filter Editor"
REFERENCES,0.832089552238806,Diversity
REFERENCES,0.835820895522388,"min
$%&'()* 𝐿,-./"
REFERENCES,0.8395522388059702,",.-012 + 𝐿4.5"
REFERENCES,0.8432835820895522,",.-012 + 𝑅,.-012"
REFERENCES,0.8470149253731343,"𝑓(𝐸$(𝑥))
𝐸$(𝑥)
𝑥"
REFERENCES,0.8507462686567164,Figure 9: Reverse engineering of global color ﬁlter triggers.
REFERENCES,0.8544776119402985,"The diversity loss Lﬁlter
div is designed to induce diverse color transforms. We use how a color ﬁlter
transforms 16 random (r, g, b, i, j) tuples to characterize a color ﬁlter E(·; θﬁlter). We record the 16
output (ˆr, ˆg,ˆb) tuples as a 48-dim descriptor of the color ﬁlter, denoted as uθﬁlter. The diversity loss is
the L2 norm of the current candidate to previously found triggers:"
REFERENCES,0.8582089552238806,"Lﬁlter
div = − NT
X j=1 j−1
X"
REFERENCES,0.8619402985074627,"i=1
||uθﬁlter
i
−uθﬁlter
j ||2.
(11)"
REFERENCES,0.8656716417910447,"The regularizer term Rﬁlter(θﬁlter) is simply an L2 regularizer on the MLP parameters to reduce the
complexity on the color transforms:"
REFERENCES,0.8694029850746269,"Rﬁlter(θﬁlter) = ||θﬁlter||2
2.
(12)"
REFERENCES,0.8731343283582089,"Feature extractor for color ﬁlter triggers.
For each model we use reverse engineering to generate
K classes by N filter
T
diverse color ﬁlter triggers. For each color ﬁlter trigger, we combine an
appearance descriptor with label ﬂipping loss Lﬁlter
flip and diversity loss Lﬁlter
div as its combined feature
descriptor. For the appearance descriptor, we use the same descriptor discussed in diversity: how
a color ﬁlter E(·; θﬁlter) transforms 16 random (r, g, b, i, j) tuples. We record the 16 output (ˆr, ˆg,ˆb)
tuples as a 48-dim appearance descriptor."
REFERENCES,0.8768656716417911,"As a result for each model we have N filter
T
K features for color ﬁlter triggers."
REFERENCES,0.8805970149253731,"Trojan classiﬁer with color ﬁlter triggers.
A bag-of-words model is used to aggregate those
features. The aggregated features across multiple trigger classes, e.g. color ﬁlters and local triggers,
are concatenated and fed through an MLP for ﬁnal Trojan classiﬁcation as below:"
REFERENCES,0.8843283582089553,"⃗hfilter
i
= MLP filter
α
(⃗vfilter
i
),
⃗hfilter =
1"
REFERENCES,0.8880597014925373,"N filter
T
K X"
REFERENCES,0.8917910447761194,"i
⃗hfilter
i
,
i = 1, . . . N filter
T
K
(13)"
REFERENCES,0.8955223880597015,"⃗hlocal
i
= MLP local
α
(⃗vlocal
i
),
⃗hlocal =
1
NT K X"
REFERENCES,0.8992537313432836,"i
⃗hlocal
i
,
i = 1, . . . NT K
(14)"
REFERENCES,0.9029850746268657,"s = MLPβ([⃗hfilter;⃗hlocal]),
(15)"
REFERENCES,0.9067164179104478,"For the TrojAI datasets, color ﬁlter reverse engineering is conducted using Adam optimizer with
learning rate 3 × 10−2 for 10 iterations. Hyperparameters are set to λﬁlter
1
= 0.05 and λﬁlter
2
= 10−4.
We also set NT = 2 and N filter
T
= 8."
REFERENCES,0.9104477611940298,"A.4
COMPARED WITH ADDITIONAL SOTA METHOD"
REFERENCES,0.914179104477612,"We also compare our proposed method with another parallel work (Shen et al., 2021). Here we
directly quote the numbers (Accuracy) from the paper for comparison. Note that the eval protocols are
different, but only subtly. Additionally, we provide the comparison of recovered triggers with (Shen
et al., 2021) in Fig. 10 to demonstrate that the proposed method could also improve the quality of
recovered triggers."
REFERENCES,0.917910447761194,Published as a conference paper at ICLR 2022 h
REFERENCES,0.9216417910447762,"Table 5: Comparison with (Shen et al., 2021).
Method
Round1
Round2
Round3
Round4
Shen et al. (2021)
0.90
0.89
0.91
0.89
Ours
0.91
0.89
0.90
0.91"
REFERENCES,0.9253731343283582,"(a) Original image
(b) (Shen et al., 2021)
(c) The proposed method"
REFERENCES,0.9291044776119403,"Figure 10: Recovered triggers compared with (Shen et al., 2021)"
REFERENCES,0.9328358208955224,"A.5
UNSUPERVISED SETTING FOR TROJAN DETECTION"
REFERENCES,0.9365671641791045,"Table 6:
Unsupervised perfor-
mances on Trojan."
REFERENCES,0.9402985074626866,"Method
AUC
ACC
NC
0.58
0.60
ABS
0.53
0.51
TABOR
0.52
0.55
ULP
0.54
0.57
DLTND
0.56
0.59
Ours
0.63
0.65"
REFERENCES,0.9440298507462687,"Indeed, our method outperforms existing methods in different
settings: fully supervised setting with annotated models, and
unsupervised setting. In the main text (Tab. 3), we have already
demonstrated that with a limited number of annotated models,
our method outperformed others. To make a fair comparison,
following Neural Cleanse and DLTND, we also use the simple
technique based on Median Absolute Deviation (MAD) for
trojan detection. We report the performance of Round 4 data
of TrojAI in Tab. 6."
REFERENCES,0.9477611940298507,"Because of the better trigger quality, due to the proposed losses,
our method outperforms baselines such as Neural Cleanse. We also note that, in Tab. 6, all methods
(including ours) perform unsatisfactorily in the unsupervised setting. This brings us back to the
discussion as to whether a supervised setting is justiﬁed in Trojan detection (although this is not
directly relevant to our method)."
REFERENCES,0.9514925373134329,"From the research point of view, we believe that data-driven methods for Trojan detection are
unavoidable as the attack techniques continue to develop. Like in many other security research
problems, Trojan attack and defense are two sides of the same problem that are supposed to advance
together. When the problem was ﬁrst studied, classic unsupervised methods such as Neural Cleanse
are sufﬁcient. In recent years, the attack techniques have continued to develop, exploiting the
entire dataset and leveraging techniques like adversarial training. Meanwhile, detection methods are"
REFERENCES,0.9552238805970149,Published as a conference paper at ICLR 2022
REFERENCES,0.9589552238805971,"conﬁned with only the given model and a few sample data. For defense methods to move forward
and to catch up with the attack methods, it seems only natural and necessary to exploit supervised
approaches, e.g., learning patterns from public datasets such as the TrojAI benchmarks."
REFERENCES,0.9626865671641791,"A.6
IMPLEMENTATION OF BASELINES"
REFERENCES,0.9664179104477612,"We carefully choose the methods with available codes from the authors as our baselines and follow
the instructions to obtain the reported results. And we list all the available repositories here:"
REFERENCES,0.9701492537313433,Neural Cleanse: https://github.com/bolunwang/backdoor
REFERENCES,0.9738805970149254,ABS: https://github.com/naiyeleo/ABS
REFERENCES,0.9776119402985075,TABOR: https://github.com/UsmannK/TABOR
REFERENCES,0.9813432835820896,ULP: https://github.com/UMBCvision/Universal-Litmus-Patterns
REFERENCES,0.9850746268656716,DLTND: https://github.com/wangren09/TrojanNetDetector
REFERENCES,0.9888059701492538,"Part of the reason for the strong beneﬁt of our method, as well as the ablated version (without the
new losses), is because of the availability of the annotated training models."
REFERENCES,0.9925373134328358,"A.7
COMPUTATION RESOURCES"
REFERENCES,0.996268656716418,"We implement our method on a server with an Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz and 1
Tesla V100 GPUs (32GB Memory)."
